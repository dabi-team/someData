1
2
0
2

c
e
D
7
2

]
L
M

.
t
a
t
s
[

2
v
2
2
2
0
0
.
2
1
1
2
:
v
i
X
r
a

GANs Training: A Game and Stochastic Control Approach

Xin Guo∗ Othmane Mounjid∗

December 28, 2021

Abstract

Training generative adversarial networks (GANs) is known to be diﬃcult, especially for
ﬁnancial time series. This paper ﬁrst analyzes the well-posedness problem in GANs minimax
games and the convexity issue in GANs objective functions.
It then proposes a stochastic
control framework for hyper-parameters tuning in GANs training. The weak form of dynamic
programming principle and the uniqueness and the existence of the value function in the
viscosity sense for the corresponding minimax game are established. In particular, explicit
forms for the optimal adaptive learning rate and batch size are derived and are shown to
depend on the convexity of the objective function, revealing a relation between improper
choices of learning rate and explosion in GANs training. Finally, empirical studies demonstrate
that training algorithms incorporating this adaptive control approach outperform the standard
ADAM method in terms of convergence and robustness.

From GANs training perspective, the analysis in this paper provides analytical support for
the popular practice of “clipping”, and suggests that the convexity and well-posedness issues
in GANs may be tackled through appropriate choices of hyper-parameters.

1 Introduction

Generative adversarial networks (GANs) belong to the class of generative models. The key idea
behind GANs (Goodfellow et al., 2014) is to add a discriminator network in order to improve the
data generation process. GANs can therefore be viewed as competing games between two neural
networks: a generator network and a discriminator network. The generator network attempts to
fool the discriminator network by converting random noise into sample data, while the discrimi-
nator network tries to identify whether the sample data is fake or real. This powerful idea behind
GANs leads to a versatile class of generative models with a wide range of successful applications
from image generation to natural language processing (Denton et al., 2015; Radford et al., 2015;
Yeh et al., 2016; Ledig et al., 2017; Zhu et al., 2016; Reed et al., 2016; Vondrick et al., 2016; Luc
et al., 2016; Kulharia et al., 2017).

Inspired by the success of GANs for computer vision, there is a surge of interest in applying GANs
for ﬁnancial time series data generation. In such a context, the key contributions consist of adapting
divergence functions and network architectures in order to cope with the time series nature and the
dependence structure of ﬁnancial data. For example, Quant-GAN (Wiese et al., 2019, 2020) uses
a TCN structure, C-Wasserstein GAN (Li et al., 2020) uses the Wasserstein distance as (Arjovsky
et al., 2017) and (Gulrajani et al., 2017), FIN-GAN (Takahashi et al., 2019) generates synthetic
ﬁnancial data, Corr-GAN (Marti, 2020) adapts DCGAN structure of (Radford et al., 2015) for
correlation matrix of asset returns, C-GAN (Fu et al., 2019) follows (Mirza and Osindero, 2014)
to simulate realistic conditional scenarios, Sig-Wasserstein-GAN (Ni et al., 2020) generates orders
and transactions, and Tail-GAN (Dionelis et al., 2020) deals with risk management. Furthermore,
activities from industrial AI labs such as JP Morgan (Storchan et al., 2020) and American Express
(Eﬁmov et al., 2020) have further elevated GANs as promising tools for synthetic data generation
and for model testing. (See also reviews by (Cao and Guo, 2021) and (Eckerli and Osterrieder,
2021) for more details).

Despite this empirical success, there are well-recognized issues in GANs training, including the
vanishing gradient when there is an imbalance between the generator and the discriminator training
(Arjovsky and Bottou, 2017), and the mode collapse when the generator learns to produce only

∗Department of Industrial Engineering and Operations Research University of California, Berkeley, Berkeley, CA

94704

1

 
 
 
 
 
 
a speciﬁc range of samples (Salimans et al., 2016). In particular, it is hard to reproduce realistic
ﬁnancial time series due to well-documented stylized facts (Chakraborti et al., 2011; Cont, 2001)
such as heavy-tailed-and-skewed distribution of asset returns, volatility clustering, the leverage
eﬀect, and the Zumbach eﬀect (Zumbach and Lynch, 2001). For instance, (Takahashi et al., 2019)
shows that batch normalization tends to yield large ﬂuctuations in the generated time series as
well as strong auto-correlation. Moreover, Figure 1 below highlights the diﬃculty of convergence
for GANs when a convolutional neural network is used to generate ﬁnancial time series: the
discriminator does not reach the desired accuracy level of 50% and the generator exhibits a non-
vanishing loss. (See Section 6 for a detailed description of data sources).

(a) Discriminator accuracy

(b) Generator loss

Figure 1: Discriminator accuracy in (a), and generator loss in (b).

In response, there have been a number of theoretical studies for GANs training. (Berard et al., 2020)
proposes a visualization method for the GANs training process through the gradient vector ﬁeld of
loss functions, (Mescheder et al., 2018) demonstrates that regularization improves the convergence
performance of GANs, (Conforti et al., 2020) and (Domingo-Enrich et al., 2020) analyze general
minimax games including GANs, and connect the mixed Nash equilibrium of the game with the
invariant measure of Langevin dynamics.
In the same spirit, (Hsieh et al., 2019) proposes a
sampling algorithm that converges towards mixed Nash equilibria, and then (Kamalaruban et al.,
2020) shows how this algorithm escapes saddle points. Recently, (Cao and Guo, 2020) establishes
the continuous-time approximation for the discrete-time GANs training by coupled stochastic
diﬀerential equations, enabling the convergence analysis of GANs training via stochastic tools.

Our work. The focus of this paper is to analyze GANs training in the stochastic control and
game framework. It starts by revisiting vanilla GANs from the original work of (Goodfellow et al.,
2014) and identiﬁes through detailed convexity analysis one of the culprits behind the convergence
issue for GANs: the lack of convexity in GANs objective function hence the general well-posedness
issue in GANs minimax games. It then reformulates GANs problem as a stochastic game and uses
it to study the optimal control of learning rate (and its equivalence to the optimal choice of time
scale) and optimal batch size.

To facilitate the analysis of this type of minimax games, this paper ﬁrst establishes the weak form
of dynamics programming principle for a general class of stochastic games in the spirit of (Bouchard
and Touzi, 2011); it then focuses on the particular minimax games of GANs training, by analyzing
the existence and the uniqueness of viscosity solutions to Issac-Bellman equations. In particular,
it obtains an explicit form for the optimal adaptive learning rate and optimal batch size which
depend on the convexity of the objective function for the game. Finally, by experimenting on syn-
thetic data drawn either from the Gaussian distribution or the Student t-distribution and ﬁnancial
data collected from the Quandl Wiki Prices database, it demonstrates that training algorithms
incorporating our adaptive control methodology outperform the standard ADAM optimizer, in
terms of both convergence and robustness.

Note that the dynamic programming principle for stochastic games has been proved in a determin-
istic setting by (Evans and Souganidis, 1984), and recently extended to the stochastic framework
for diﬀusions under boundedness and regularity conditions (Krylov, 2014; Bayraktar and Yao,
2013; Sirbu, 2014a,b). The dynamic programming principle established in this paper is without
the continuity assumption and is for a more general class of stochastic diﬀerential games beyond
diﬀusions. In addition, upper and lower bounds of the value function are obtained.

2

25507510012515017520020304050602550751001251501752000.4000.4250.4500.4750.5000.5250.5500.5750.600In terms of GANs training, our analysis provides analytical support for the popular practice of “clip-
ping” in GANs (Arjovsky and Bottou, 2017); and suggests that the convexity and well-posedness
issues associated with GANs problems may be resolved by appropriate choices of hyper-parameters.
In addition, our study presents a precise relation between explosion in GANs training and improper
choices of the learning rate. It also reveals an interesting connection between the optimal learning
rate and the standard Newton algorithm.

Notations. Throughout this paper, the following notations are used:

• For any vector x ∈ Rd with d ∈ N∗, denote by the operator ∇x the gradient with respect to
the coordinates of x. When there is no subscript x, the operator ∇ refers to the standard
gradient operator.

• For any d ∈ N∗, the set MR(d) is the space of d × d matrices with real coeﬃcients.

• For any vector m ∈ Rd and symmetric positive-deﬁnite matrix A ∈ MR(d) with d ∈ N∗,

denote by N (m, A) the Gaussian distribution with mean m and covariance matrix A.

• EX emphasizes on the dependence of expectation with respect to the distribution of X.

2 GANs: Well-posedness and Convexity

GANs as generative models. GANs fall into the category of generative models. The pro-
cedure of generative modeling is to approximate an unknown true distribution PX of a random
variable X from a sample space X by constructing a class of suitable parametrized probability
distributions Pθ. That is, given a latent space Z, deﬁne a latent variable Z ∈ Z with a ﬁxed
probability distribution and a family of functions Gθ : Z → X parametrized by θ. Then, Pθ can
be seen as the probability distribution of Gθ(Z).

To approximate PX , GANs use two competing neural networks: a generator network for the
function Gθ, and a discriminator network Dw parametrized by w. The discriminator Dw assigns a
score between 0 and 1 to each sample. A score closer to 1 indicates that the sample is more likely
to be from the true distribution. GANs are trained by optimizing Gθ and Dw iteratively until Dw
can no longer distinguish between true and generated samples and assigns a score close to 0.5.

Equilibrium of GANs as minimax games. Under a ﬁxed network architecture, the parametrized
GANs optimization problem can be viewed as the following minimax game:

min
θ∈RN

max
w∈RM

g(w, θ),

with g : RM × RN → R the objective function. In vanilla GANs,

g(w, θ) = EX

(cid:2) log(Dw(X))(cid:3) + EZ

(cid:2) log (cid:0)1 − Dw(Gθ(Z))(cid:1)(cid:3),

(1)

(2)

where Dw : RM → R is the discriminator network, Gθ : RN → R is the generator network, X
represents the unknown data distribution, and Z is the latent variable.

From a game theory viewpoint, the objective in (1), when attained, is in fact the upper value of the
two-player zero-sum game of GANs. If there exists a locally optimal pair of parameters (θ∗, w∗)
for (1), then (θ∗, w∗) is a Nash equilibrium, i.e., no player can do better by unilaterally deviating
from her strategy.

To guarantee the existence of a Nash equilibrium, convexity or concavity conditions are required at
least locally. From an optimization perspective, these convexity/concavity conditions also ensure
the absence of a duality gap, as suggested by Sion’s generalized minimax theorem in (Sion, 1958)
and (Von Neumann, 1959).

3

2.1 GANs Training and Convexity

GANs are trained by stochastic gradient algorithms (SGA). In GANs training, if g is convex (resp.
concave) in θ (resp. ω), then it is possible to decrease (resp. increase) the objective function g by
moving in the opposite (resp. same) direction of the gradient.

It is well known that SGA may not converge without suitable convexity properties on g, even when
the Nash equilibrium is unique for a minimax game. For instance, g(x, y) = xy clearly admits the
point (0, 0) as a unique Nash equilibrium. However, as illustrated in Figure 2, SGA fails to con-
verge since g is neither strictly concave nor strictly convex in x or y.

Figure 2: Plot of the parameters values when using SGA to solve the minimax problem
miny∈R maxx∈R xy. The label “Start” (resp. “End”) indicates the initial (resp. ﬁnal) value of
the parameters.

Moreover, convexity properties are easy to violate with the composition of multiple layers in a
neural network even for a simple GANs model, as illustrated in the following example.

Counterexample. Take the vanilla GANs with g in (2). Take two normal distributions for X
and Z such that X ∼ N (m, σ2) and Z ∼ N (0, 1) with (m, σ) ∈ R × R+.

Now take the following parametrization of the discriminator and the generator networks:






Dw(x) = D(w1,w2,w3)(x) =
Gθ(z) = G(θ1,θ2)(z) = θ2z + θ1,

1
1 + e−(w3/2·x2+w2x+w1)

,

(3)

where w = (w1, w2, w3) ∈ R3, and θ = (θ1, θ2) ∈ R × R+. Note that this parametrization of the
discriminator and the generator networks are standard since the generator is a simple linear layer
and the discriminator can be seen as the composition of the sigmoid activation function with a
linear layer in the variable (x, x2).

To ﬁnd the optimal choice for the parameters w and θ, denote by fX and fG respectively the
density functions of X and Gθ(Z). Then, the density fG∗ of the optimal generator is given by
fG∗ = fX , meaning θ∗ = (m, σ). Moreover, Proposition 1 in (Goodfellow et al., 2014) shows that
the optimal value for the discriminator is

Dw∗ (x) =

fX (x)
fX (x) + fG∗ (x)

=

1
1 + (fG∗ /fX )(x)

=

1
2

,

for any x ∈ R which gives w∗ = (0, 0, 0).

We now demonstrate that the function g may not satisfy the convexity/concavity requirement.

• To see this, let us ﬁrst study the concavity of the function gθ0 : w → g(w, θ0) with θ0 ﬁxed.

We will show that gθ0 is concave with respect to w.

4

1.000.330.331.001.000.330.331.001.000.330.331.00StartEndTo this end, write Dw as the composition of the two following functions D1 and L:

D1(x) = 1/(1 + e−x), L(w; x) = w3/2 · x2 + w2x + w1,

for any x ∈ R, and w = (w1, w2, w3) ∈ R3. Note that a straightforward computation of the
second derivatives shows that the functions

g1 : x → log(D1(x)),

g2 : x → log(1 − D1(x)),

with x ∈ R are both concave. Thus, by linearity and composition, the function gθ0 : w →
EX [g1(L(w; X))] + EZ[g2(L(w; Gθ0 (Z)))] remains concave.

• Next, we investigate the convexity of the function gw0 : θ → g(w0, θ) with w0 ﬁxed. We will

show that gw0 is not necessarily convex with respect to θ.

To this end, ﬁrst note that the mapping θ :→ EX [log(Dw(X))] does not depend on the pa-
rameter θ. Therefore, one can simply focus on the function g3 : θ → EZ[log(1−Dw0 (Gθ(Z)))],
which is not necessarily convex with respect to θ. To see this, let us take θ2 = 0 for simplicity1
and study the function

g3|θ2=0 : θ1 → log(1 − Dw0 (G(θ1,0)(z)))
3/2·θ2

= log (cid:0)1 − 1/(1 + e−(w0

1 +w0

2θ1+w0

1))(cid:1),

with θ1 ∈ R. On one hand, the convexity of g3|θ2=0 depends on the choice of the parameter
w0
3, as demonstrated in Figure 3. On the other hand, simple computation of the second
derivative gives

(g3|θ2=0)(2)(θ1) = −

eh(θ1)+w1(cid:0)w3(eh(θ1)+w1 + 2h(θ1) + 1) + w2
((eh(θ1)+w1 + 1)2

2

(cid:1)

,

with θ1 ∈ R, and h(θ1) = θ1(w0
3/2 × θ1 + w0
of the parameter w0. Thus, gw0 is not necessarily convex with respect to θ.

2). The sign of this function depends on the choice

(a)

(b)

Figure 3: Plot of g3|θ2=0 with (w1, w2, w3) = (1, 1, 2) in (a) and (w1, w2, w3) = (1, 1, −2) in (b).

The analysis indicates analytically one of the culprits behind the convergence issue of GANs: the
lack of convexity in the objective function, hence the well-posedness problem of GANs models.

2.2 GANs Training and Parameters Tuning

In addition to the convexity and well-posedness issue of GANs as minimax games, appropriate
choices and speciﬁcations for parameters’ tuning aﬀect GANs training as well.

Hyper-parameters in GANs training. There are three key hyper-parameters in stochastic
gradient algorithms for GANs training: the learning rate, the batch size, and the time scale.

• The learning rate determines how far to move along the gradient direction. A higher learning
rate accelerates the convergence, yet with a higher chance of explosion; while a lower learning
rate yields slower convergence.

1This enables us to get rid of the expectation with respect to Z.

5

642024302520151050420241.41.21.00.80.60.40.20.0• The time scale parameters monitor the number of updates of the variables w and θ. In the
context of GANs training, there are generally two diﬀerent time scales: a ﬁner time scale for
the discriminator and a coarser one for the generator or conversely. Note that more updates
means faster convergence, however, also computationally more costly.

• The batch size refers to the number of training samples used in the estimate of the gradient.
The more training samples used in the estimate, the more accurate this estimate is, yet with
a higher computational cost. Smaller batch size, while providing a cruder estimate, oﬀers a
regularizing eﬀect and lowers the chance of overﬁtting.

Example of improper learning rate. A simple example below demonstrates the importance of
an appropriate choice of learning rate for the convergence of SGA. Consider the R-valued function

f (x) = (a/2) x2 + b x,

∀x ∈ R,

where (a, b) ∈ R+ × R. Finding the minimum x∗ = −(b/a) of f via the gradient algorithm consists
of updating an initial guess x0 ∈ R as follows:

xn+1 = xn − η(axn + b),

∀n ≥ 0,

(4)

with η the learning rate. Let us study the behavior of the error en = |xn − x∗|2. By (4) and
ax∗ + b = 0,

en+1 = |xn+1 − x∗|2 = |xn − x∗|2 + 2(xn+1 − xn)en + |xn+1 − xn|2

= (cid:0)1 − ηa(2 − ηa)(cid:1)|xn − x∗|2.

(5)

Thus, when η > 2/a, the factor ηa(2 − ηa) < 0 which means r = (cid:0)1 − ηa(2 − ηa)(cid:1) > 1. In such a
case, Equation (5) becomes

en+1 = r en,

ensuring en → +∞ when n goes to inﬁnity, and leading to the failure of the convergence for the
gradient algorithm. This example highlights the importance of the learning rate parameter. Such
an issue of improper learning rates for GANs training will be revisited in a more general setting
(see Section 5.1).

Note that there are earlier works on optimal learning rate policies to improve the performance of
gradient-like algorithms (see for instance (Moulines and Bach, 2011), (Gadat and Panloup, 2017),
and (Mounjid and Lehalle, 2019)).

Time scale for GANs training. Equation (7) corresponds to a speciﬁc implementation where
updates of the parameters w and θ are alternated. However, it also is possible to consider an
asynchronous update of the following form (see for example Algorithm 1 in (Arjovsky et al.,
2017)):

Algorithm 1 Asynchronous gradient algorithm

1: for i = 1 . . . nθ
2:
3:

for j = 1 . . . nw

max do

w ← w + ηwgw(w, θ)

max do

end for
θ ← θ − ηθgθ(w, θ)

4:
5:
6: end for

max and nθ

max are respectively the maximum number of iterations for the upper and the
In such a context, one naturally deals with two time scales: a ﬁner time scale for

where nw
inner loop.
the discriminator and a coarser one for the generator. The time scale parameters nw
max
monitor the number of updates of the variables w and θ. As mentioned earlier, more updates
means faster convergence but at the cost of more gradient computations. It is therefore necessary
to select these parameters carefully in order to perform updates only when needed.

max and nθ

6

Batch size for GANs training. To better understand the batch size impact, we consider here
the vanilla objective function g, i.e.,

g(w, θ) = EX

(cid:2) log(Dw(X))(cid:3) + EZ

(cid:2) log (cid:0)1 − Dw(Gθ(Z))(cid:1)(cid:3).

In general, the function g is approximated by the empirical mean

gNM (w, θ) =

(cid:80)N

i=1

(cid:80)M

j=1 gi,j(w, θ)
,
N · M

(6)

with

gi,j(w, θ) = log(Dw(xi)) + log (cid:0)1 − Dw

(cid:0)Gθ(zj)(cid:1)(cid:1),

where (xi)i≤N are i.i.d. samples from PX the distribution of X, (zj)j≤M are i.i.d. samples from PZ
the distribution of Z, and N (resp. M ) is the number of PX (resp. PZ) samples. The quantity N ·M
here represents the batch size. It is clear from Equation (6) that enlarging the batch size oﬀers a
better estimate of gNM (w, θ) since it reduces the variance. However, such an improvement requires
a higher computational power. Sometimes, it is better to spend such a power on performing more
gradient updates rather than reducing the variance.

3 Control and Game Formulation of GANs Training

Clearly, hyper-parameters introduced in Section 2.2 are not independent for GANs training, which
often involves choices between adjusting learning rates and changing sample sizes. In this section,
we show how hyper-parameters’ tuning can be formulated and analyzed as stochastic control prob-
lems, and how the popular practice of “clipping” in GANs training can be understood analytically
in this framework.

3.1 Stochastic Control of Learning Rate

In this part, we present an optimal selection methodology for the learning rate. To start, let us
recall the continuous-time stochastic diﬀerential equation used to represent GANs training.

SDE approximation of GANs training.
In GANs training, gradient algorithms for the opti-
mal parameters θ∗ and w∗ start with an initial guess (w0, θ0) and apply at time step t the following
(simultaneous) update rule:

(cid:26) wt+1 = wt + ηgw(wt, θt),
θt+1 = θt − ηgθ(wt, θt),

(7)

with gw = ∇wg, gθ = ∇θg, and η ∈ R+ the learning rate.

The continuous-time approximation of GANs training via functional central limit theorem (Cao
and Guo, 2020) replaces the update rule in (7) with coupled stochastic diﬀerential equations (SDEs)

(cid:26) dw(t) = gw(q(t))dt +
dθ(t) = −gθ(q(t))dt +

ησw(q(t))dW 1(t),
√
ησθ(q(t))dW 2(t),

√

(8)

where q(t) = (w(t), θ(t)), the functions σw : RM × RN → MR(M ) and σθ : RM × RN → MR(N )
are approximated by the covariances of gw and gθ, and the Brownian motions W 1 and W 2 are
independent. Note that in this SDE approximation, the learning rates for the generator and the
discriminator (η, η) are ﬁxed constants. Based on this approximation, optimal choices of learning
rate can be formulated as a stochastic control problem.

Adaptive learning rate. The idea goes as follows. Consider the following decomposition for
the learning rate η(t) at time t:

η(t) = (cid:0)ηw(t), ηθ(t)(cid:1) = (cid:0)uw(t) × ¯ηw(t), uθ(t) × ¯ηθ(t)(cid:1) = u(t) • ¯η(t),

∀t ≥ 0,

(9)

where the ﬁrst component ¯η(t) = (¯ηw(t), ¯ηθ(t)) ∈ [¯ηmin, 1]2 is a predeﬁned base learning rate ﬁxed
by the controller using her favorite learning rate selection method, and the second component
u(t) = (uw(t), uθ(t)) is a [umin, umax]2-valued process representing an adjustment around ¯η(t).
The symbol • here is the component-wise product between vectors. Furthermore, the positive
constants ¯ηmin, umin, and umax are “clipping” parameters introduced to handle the convexity issue

7

discussed earlier for GANs training, to establish ellipticity conditions needed for the regularity of
the value function, and to avoid explosion. This explosion aspect will be clariﬁed shortly. With the
incorporation of the adaptive learning rate (9), the corresponding SDE for GANs training becomes






dw(t) = uw(t)gw(q(t))dt + (cid:0)uw√

¯ηw(cid:1)(t)σw(q(t))dW 1(t),

dθ(t) = −uθ(t)gθ(q(t))dt + (cid:0)uθ(cid:112)¯ηθ(cid:1)(t)σθ(q(t))dW 2(t),

(10)

with q(t) = (w(t), θ(t)) for any t ≥ 0.

Optimal control of adaptive learning rate. Let T < ∞ be a ﬁnite time horizon, and deﬁne
the objective function J as

J(T, t, q; u) = E(cid:2)g(cid:0)q(T )(cid:1)(cid:12)

(cid:12)q(t) = q],

where q = (w, θ) ∈ RM × RN is the value of the process q(t) at t ∈ [0, T ]. Note that the function
J here is similar to the mapping g used in vanilla GANs (see Equation (1)). The main diﬀerence
consists of replacing the constant parameters (w, θ) in (1) by q(T ) their value at the end of the
training. Moreover, since q(T ) is a random variable, an expectation is added to estimate the aver-
age value of g(cid:0)q(T )(cid:1).

Then, the control problem for the adaptive learning rate is formulated as

v(t, q) = min
uθ∈U θ

max
uw∈U w

J(T, t, q; u),

(11)

for any (t, q) ∈ [0, T ] × RM × RN , with U w and U θ the set of appropriate admissible controls for
uw and uθ. For a ﬁxed uθ ∈ U θ, U w is deﬁned as

U w = (cid:8)u : u c`adl`ag in [umin, umax] adapted to F(W 1,W 2),

E[g(cid:0)q(T )(cid:1) | q(0) ] < ∞(cid:9),

where umax ≥ umin > 0 are the upper bounds introduced earlier. Then, we write U θ as follows:

U θ = (cid:8)u : u c`adl`ag in [umin, umax] adapted to F(W 1,W 2),
E[g(cid:0)q(T )(cid:1) | q(0) ] < ∞(cid:9).

sup
u∈U w

3.2 Stochastic Control of Time Scales

Let us consider the following expression for nmax:

nmax = (nw

max, nθ

max) = (cid:0)cw × ¯nw

max, cθ × ¯nθ

max

(cid:1) = c • ¯nmax,

max, ¯nθ

with ¯nmax = (¯nw
max) a base time scale parameter initially ﬁxed by the controller and c =
(cw, cθ) a constant adjustment around ¯nmax. Under suitable conditions (see for example (Fatkullin
and Vanden-Eijnden, 2004; Weinan et al., 2005)) one can show that the asynchronous Algorithm
1 converges towards






dw(t) =

dθ(t) = −

1
cw(cid:15)1gw(q(t))dt,
1
cθgθ(q(t))dt,

(12)

where q(t) = (w(t), θ(t)) and (cid:15)1 is a small parameter measuring the separation between time scales.
Thus, the SDE version of (12) is






dw(t) = ˜ηwgw(q(t))dt + ˜ηw√
dθ(t) = −˜ηθgθ(q(t))dt + ˜ηθ√

¯ησw(q(t))dW 1(t),

¯ησθ(q(t))dW 2(t),

with ˜ηw = 1/(cw(cid:15)1), and ˜ηθ = 1/cθ. Comparing the dynamics of (w(t), θ(t))t≥0 with the one of
Section 3.1 suggests that the time scale and the learning rate control problems are equivalent.

8

3.3 Stochastic Control of Batch Size

To understand the impact of the batch size, let us introduce a scaling factor mθ ≥ 1, the terminal
time T , a maximum number of iterations tmax, and compare the two following algorithms:

(cid:26) w1
t+1 = w1
t+1 = θ1
θ1

w (w1
t + ηgNM
(w1
t − ηgNM
θ

t , θ1
t ),
t , θ1
t ),

∀t ≤ tmax,

and

(cid:40)

w2
t+1 = w2
t+1 = θ2
θ2

t + ηgmθ(NM )
t − ηgmθ(NM )

w

θ

(w2
(w2

t , θ2
t ),
t , θ2
t ),

∀t ≤ tmax/mθ.

Note that the second algorithm is simulated less to ensure computational costs for the two methods
are the same, i.e., with the same number of gradient calculations. Following (Cao and Guo, 2020)
the continuous-time approximation for both algorithms can be written as

and











dw1(t) = gw(q1(t))dt +

√

ησw(q1(t))dW 1(t),

dθ1(t) = −gθ(q1(t))dt +

√

ησθ(q1(t))dW 2(t),

dw2(t) = gw(q2(t))dt + (cid:112)η/mθσw(q2(t))dW 1(t),

dθ2(t) = −gθ(q2(t))dt + (cid:112)η/mθσθ(q2(t))dW 2(t),

with q1(t) = (w1(t), θ1(t)), q2(t) = (w2(t), θ2(t)), gw = E[gNM
(resp. gNM
proportional to the variance of gNM
w
since samples are i.i.d., the variances of gmθNM
w ) = mθVar(gmθNM
gNM
θ
the variance.

w (resp. σ2
θ )
), and η ≥ 0 a constant learning rate. Note that,
and gmθNM
and
θ
), implying that enlarging the batch size reduces

are mθ times smaller than gNM

variances, i.e., Var(gNM

w ], gθ = E[gNM

], σ2

w

w

w

θ

θ

Meanwhile, comparing the objective functions E[g(q1(T ))] and E[g(q2(T /mθ))] of both implemen-
tations suggests that reducing the batch size leads to a larger time horizon which means more
parameters updates. Therefore, the question of ﬁnding the right trade-oﬀ between reducing the
variance and performing more updates arises naturally.

Now, one can select the optimal batch size in the same spirit of the learning rate control problem
(see Section 3.1). That is to consider

˜vm(t, q) = min

mθ∈Mθ

E(cid:2)g(cid:0)˜qm(T )(cid:1)(cid:12)

(cid:12)˜qm(t) = q],

(13)

for any (t, q) ∈ R+ × RM +N , where the process ˜qm(t) = ( ˜wm(t), ˜θm(t)) represents the trained
parameters at time t, and Mθ is the set of admissible controls for mθ. The process ˜qm satisﬁes
the SDE below





d ˜wm(t) =

√

gw(˜qm(t))
mθ

dt +

ησw(˜qm(t))
mθ

d ˜W 1(t),

d˜θm(t) = −

gθ(˜qm(t))
mθ

dt +

√

ησθ(˜qm(t))
mθ

d ˜W 2(t),

(14)

with ˜W 1 and ˜W 2 two independent Brownian motions. The derivation of (14) is detailed in Ap-
pendix A. Moreover, the set Mθ is deﬁned as

Mθ = (cid:8)m : m c`adl`ag in [1, mmax] adapted to F( ˜W 1, ˜W 2),

E[g(cid:0)˜qm(T (cid:1) | ˜qm(0) ] < ∞(cid:9).

We allow here mθ to be a process in order to handle a more general control problem.

4 Stochastic Diﬀerential Games

The minimax game of GANs with adaptive learning rate and batch size in the previous section
can be analyzed in a more general framework of stochastic diﬀerential games.
In this section,
we ﬁrst establish a weak form of dynamic programming principle (DPP) for a class of stochastic

9

diﬀerential games where the underlying process is not necessarily a controlled Markov diﬀusion and
where the value function is not a priori continuous. We will then apply this weak form of dynamic
programming principle to stochastic games with controlled Markov diﬀusion, and show that the
value of such games is the unique viscosity solution to the associated Isaac-Bellman equation, under
suitable technical conditions.

4.1 Formulation of Stochastic Diﬀerential Games

Let d ≥ 1 be a ﬁxed integer and (Ω, F, P) be a probability space supporting a c`adl`ag Rd-valued
process Y with independent increments. Given T ∈ R∗
+, we write F = {Ft, 0 ≤ t ≤ T } for the
completion of Y natural ﬁltration on [0, T ]. Here F satisﬁes the usual condition (see for instance
(Jacod and Shiryaev, 2013)). We suppose that F0 is trivial and that FT = F. Moreover, for every
t ≥ 0, set Ft = {F t
s is the completion of σ(Yr − Yt, t ≤ r ≤ s ∨ t) by null sets of F.

s, s ≥ 0} where F t

The set T refers to the collection of all F-stopping times. For any (τ1, τ2) ∈ T 2 such that τ1 ≤ τ2,
the subset T[τ1,τ2] is the collection of all τ ∈ T verifying τ ∈ [τ1, τ2] a.s. When τ1 = 0, we simply
to denote the corresponding sets of Ft-stopping
write Tτ2. We use the notations T t
[τ1,τ2] and T t
τ2
times.

For every τ ∈ T and a subset A of a ﬁnite-dimensional space, we denote by L0
τ (A) the collection
of all Fτ -measurable random variables with values in A. The set H0(A) is the collection of all
F-progressively measurable processes with values in A, and H0
rcll(A) is the subset of all processes
in H0(A) which are right continuous with ﬁnite left limits. We ﬁrst introduce the sets

S = [0, T ] × Rd, S0 = {(τ, (cid:15)); τ ∈ TT , (cid:15) ∈ L0

τ (Rd)}.

Then we take the two sets of control processes U 1
k2 ≥ 1 two integers, such that the controlled state process deﬁned as the mapping

) and U 2

0 ⊂ H0(Rk2

0 ⊂ H0(Rk1

), with k1 ≥ 1 and

(τ, (cid:15); ν1, ν2) ∈ S × U 1

0 × U 2

0 −→ X (ν1,ν2)

τ,(cid:15)

,

is well deﬁned and

(θ, X ν

τ,(cid:15)(θ)) ∈ S,

∀(τ, (cid:15)) ∈ S, ∀θ ∈ T[τ,T ].

Here, X (ν1,ν2)
one can take S = {(τ, (cid:15)) ∈ S0; E[|(cid:15)|2] < ∞}. In the sequel, we write U0 for the set U0 = U 1

refers to the controlled process and S is a set satisfying S ⊂ S ⊂ S0. For instance,
0 × U 2
0 .

τ,(cid:15)

Let f : Rd → R be a Borel function, and the reward function J be

J(t, x; ν) = E[f (X ν

t,x(T ))],

∀(t, x) ∈ S, ∀ν = (ν1, ν2) ∈ U 1 × U 2,

(15)

with U 1 (resp. U 2) the set of admissible controls for ν1 (resp. ν2). Given ν2, we deﬁne U 1 as

U 1 = (cid:8)ν1 ∈ U 1

0 ; E[|f (X (ν1,ν2)

t,x

(T ))|] < ∞(cid:9).

Then, we denote by U 2 the set

U 2 = (cid:8)ν2 ∈ U 2
0 :

sup
ν1∈U 1

E[|f (X (ν1,ν2)

t,x

(T ))|] < ∞(cid:9).

t (resp. U 2

We write U 1
progressively measurable. We denote by Ut the set Ut = U 1
stochastic control problem can be written as

t ) for the collection of processes ν1 ∈ U 1 (resp. ν2 ∈ U 2) that are Ft-
t . The value function V of the

t × U 2

V (t, x) = inf

ν2∈U 2
t

sup
ν1∈U 1
t

J(t, x; ν),

∀(t, x) ∈ S.

4.2 Dynamic Programming Principle for Stochastic Diﬀerential Games

To establish the weak form of the dynamic programming principle, we work under the following
mild assumptions, as in (Bouchard and Touzi, 2011).

Assumption A. For all (t, x) ∈ S, and ν ∈ Ut, the controlled state process satisﬁes

10

A.1 Independence. The process X ν

t,x is Ft-progressively measurable.

A.2 Causality. For any ˜ν ∈ Ut, τ ∈ T t

[t,T ], and A ∈ F t

τ , if ν = ˜ν on [t, τ ] and ν1A = ˜ν1A on

(τ, T ], then X ν

t,x1A = X ˜ν

t,x1A.

A.3 Stability under concatenation. For every ˜ν ∈ Ut, and θ ∈ T t

[t,T ], we have

ν1[0,θ] + ˜ν1(θ,T ] ∈ Ut.

A.4 Consistency with deterministic initial data. For all θ ∈ T t

[t,T ], we have the following:

a. For P−a.e. ω ∈ Ω, there exists ˜νω ∈ Uθ(ω) such that

E[f (X ν

t,x(T ))|Fθ](ω) = J(θ(ω), X ν

t,x(θ)(ω); ˜νω).

b. For t ≤ s ≤ T , θ ∈ T t

[t,s], ˜ν ∈ Us, and ¯ν = ν1[0,θ] + ˜ν1(θ,T ], we have

E[f (X ¯ν

t,x(T ))|Fθ](ω) = J(θ(ω), X ν

t,x(θ)(ω); ˜ν),

for P − a.e. ω ∈ Ω.

We also need the boundedness and regularity assumptions below.

Assumption B. The value function V is locally bounded.

Assumption C. The reward function J(.; ν) is continuous for every ν ∈ U0.

We are now ready to derive the weak form of dynamic programming principle.

Theorem 1. Assume Assumptions A, B, and C. Then

• For any function φ upper-semicontinuous such that V ≥ φ, we have

V (t, x) ≥ inf

ν2∈U 2
t

sup
ν1∈U 1
t

E[φ(θν, X ν

t,x(θν)].

• For any function φ lower-semicontinuous such that V ≤ φ, we have

V (t, x) ≤ inf

ν2∈U 2
t

sup
ν1∈U 1
t

E[φ(θν, X ν

t,x(θν)].

(16)

(17)

Proof. (For Theorem 1). Let us ﬁrst prove (16). For any admissible process ν2 ∈ U 2

t , deﬁne ¯V as

¯V (t, x; ν2) = sup
ν1∈U 1
t

J(t, x; (ν1, ν2)),

∀(t, x) ∈ S.

(18)

Under Assumptions A, B, and C, one can apply (Bouchard and Touzi, 2011, Theorem 3.5) to get

¯V (t, x; ν2) ≥ sup
ν1∈U 1
t

E[φ(θν, X ν

t,x(θν)].

(19)

Since, V (t, x) = inf ν2∈U 2

t

¯V (t, x; ν2) by deﬁnition, we use (19) to deduce that

V (t, x) ≥ inf

ν2∈U 2
t

sup
ν1∈U 1
t

E[φ(θν, X ν

t,x(θν)].

We move now to the proof of (17). Fix ν = (ν1, ν2) ∈ Ut and set θ ∈ T t
[t,T ]. For any t ≥ 0, we
write ¯t for the time ¯t = t ∧ θ(ω). Let (cid:15) > 0 and ¯V be the function introduced in (18). Then, there
is a family of (ν(s,y),(cid:15),2)(s,y)∈S ⊂ U0 such that

ν(s,y),(cid:15),2 ∈ U 2
s ,

J(s, y; (νs,1, ν(s,y),(cid:15),2)) − (cid:15) ≤ ¯V (s, y; ν(s,y),(cid:15),2) − (cid:15) ≤ V (s, y),

(20)

for any (s, y) ∈ S and νs,1 ∈ U 1
and J is upper-semicontinuous, there exists a family (r(s,y))(s,y)∈S of positive scalars such that

s . Let ν(s,y),(cid:15) = (ν1, ν(s,y),(cid:15),2) ∈ Us. Since φ is lower-semicontinuous,

φ(s, y) − φ(s(cid:48), y(cid:48)) ≤ (cid:15),

J(s, y; ν(s,y),(cid:15)) − J(s(cid:48), y(cid:48); ν(s,x),(cid:15)) ≥ −(cid:15),

∀(s(cid:48), y(cid:48)) ∈ B(s, y; r(s,y)),

(21)

11

with (s, y) ∈ S and

B(s, y; r) = {(s(cid:48), y(cid:48)) ∈ S; s(cid:48) ∈ (s − r, s], (cid:107)y − y(cid:48)(cid:107) < r},

∀r > 0.

Now follow the same approach of (Bouchard and Touzi, 2011, Theorem 3.5, step 2) to construct a
countable sequence (ti, yi, ri)i≥1 of elements of S × R, with 0 < ri ≤ r(ti,yi) for all i ≥ 1, such that
S ⊂ {0} × Rd ∪ {∪i≥1B(ti, yi; ri)}. Set A0 = {T } × Rd, C−1 = ∅, and deﬁne the sequence

Ai+1 = B(ti+1, yi+1; ri+1) \ Ci,

Ci = Ci−1 ∪ Ai,

∀i ≥ 0.

With this construction, it follows from (20), (21), and the fact that V ≤ φ, that the countable
family (Ai)i≥0 satisﬁes

(cid:26) (cid:0)θ, X ν

t,x(θ)(cid:1) ∈ (cid:0) ∪i≥0 Ai

J(.; νi,(cid:15)) ≤ φ + 3(cid:15),

(cid:1) P-a.s., Ai ∩ Aj = ∅,

for i (cid:54)= j,

on Ai, for i ≥ 1,

(22)

with νi,(cid:15) = ν(ti,yi),(cid:15) for any i ≥ 1. We are now ready to prove (17). To this end, set An = ∪0≤i≤nAi
for any n ≥ 1 and deﬁne

ν(cid:15),n,2
s

= 1[t,θ](s)ν2

s + 1(θ,T ]

(cid:0)ν2

s 1(An)c

(cid:0)θ, X ν

t,x(θ)(cid:1) +

n
(cid:88)

i=1

1Ai

(cid:0)θ, X ν

t,x(θ)(cid:1)νi,(cid:15),2

s

(cid:1),

∀s ∈ [t, T ],

and ¯ν(cid:15),n = (ν1, ν(cid:15),n,2) for any i ≥ 1. Note that {(θ, X ν
θ as a consequence of
Assumption A.1. Then, it follows from Assumption A.3 that ¯ν(cid:15),n ∈ Ut. Moreover, by deﬁnition of
B(ti, yi; ri), we have θ = ¯ti ≤ ti on {(θ, X ν
t,x(θ) ∈ Ai}. Then, using Assumption A.4, Assumption
A.2, and (22), we deduce

t,x(θ)) ∈ Ai} ∈ F t

E[f (cid:0)X ¯ν(cid:15),n

t,x (T )(cid:1)|Fθ]1An

(cid:0)θ, X ν

t,x(θ)(cid:1)

=E[f (cid:0)X ¯ν(cid:15),n

t,x (T )(cid:1)|Fθ]1A0

(cid:0)θ, X ν

t,x(θ)(cid:1) +

n
(cid:88)

i=1

E[f (cid:0)X ¯ν(cid:15),n

t,x (T )(cid:1)|F¯ti]1Ai

(cid:0)θ, X ν

t,x(θ)(cid:1)

=V (cid:0)T, X ¯ν(cid:15),n

t,x (T )(cid:1)1A0

(cid:0)θ, X ν

t,x(θ)(cid:1) +

n
(cid:88)

i=1

J(¯ti, X ν

t,x(¯ti); νi,(cid:15))1Ai

(cid:0)θ, X ν

t,x(θ)(cid:1)

≤

n
(cid:88)

i=0

(cid:0)φ(θ, X ν

t,x(θ)) + 3(cid:15)(cid:1)1Ai

(cid:0)θ, X ν

t,x(θ)(cid:1) = (cid:0)φ(θ, X ν

t,x(θ)) + 3(cid:15)(cid:1)1An

(cid:0)θ, X ν

t,x(θ)(cid:1),

Using the tower property of conditional expectations, we get

E(cid:2)f (cid:0)X ¯ν(cid:15),n

t,x (T )(cid:1)(cid:3) = E(cid:2)E[f (cid:0)X ¯ν(cid:15),n
≤ E(cid:2)(cid:0)φ(θ, X ν

t,x (T )(cid:1)|Fθ](cid:3)
t,x(θ)) + 3(cid:15)(cid:1)1An

(cid:0)θ, X ν

t,x(θ)(cid:1)(cid:3) + E(cid:2)f (cid:0)X ¯ν(cid:15),n

t,x (T )(cid:1)1(An)c

(cid:0)θ, X ν

t,x(θ)(cid:1)(cid:3).

Since f (cid:0)X ¯ν(cid:15),n

t,x (T ) ∈ L1, it follows from the dominated convergence theorem

t,x (T )(cid:1)(cid:3)

E(cid:2)f (cid:0)X ¯ν(cid:15),n
≤3(cid:15) + lim inf
n→∞

=3(cid:15) + lim
n→∞
=3(cid:15) + E(cid:2)φ(θ, X ν

E(cid:2)φ(θ, X ν
E(cid:2)φ(θ, X ν
t,x(θ))(cid:3),

t,x(θ))1An
t,x(θ))+1An

(cid:0)θ, X ν
(cid:0)θ, X ν

t,x(θ)(cid:1)(cid:3)
t,x(θ)(cid:1)(cid:3) − lim

n→∞

E(cid:2)φ(θ, X ν

t,x(θ))−1An

(cid:0)θ, X ν

t,x(θ)(cid:1)(cid:3)

(23)

where the last inequality follows from the left-hand side of (22) and from the monotone convergence
t,x(θ))−(cid:3) < ∞. Finally, by deﬁnition of V ,
theorem since either E(cid:2)φ(θ, X ν
deﬁnition of ¯V , the arbitrariness of ν1, and (23), we deduce

t,x(θ))+(cid:3) < ∞ or E(cid:2)φ(θ, X ν

V (t, x) ≤ ¯V (t, x; ν(cid:15),n,2) = sup
ν1∈U 1
t

E(cid:2)f (cid:0)X ¯ν(cid:15),n

t,x (T )(cid:1)(cid:3) ≤ 3(cid:15) + sup

ν1∈U 1
t

E(cid:2)φ(θ, X ν

t,x(θ))(cid:3),

which completes the proof of (17) by the arbitrariness of ν2 ∈ U 2

t and (cid:15) > 0.

12

4.3 Stochastic Diﬀerential Games under Controlled Markov Diﬀusions

Now considering a particular class of controlled Markov dynamics, where for any control process
ν ∈ U0,

dX ν

t = b(t, X ν

t , νt)dt + ˜σ(t, X ν

t , νt)dWt,

(24)

× Rk2

with W a d-dimensional Brownian motion, b : R+ × Rd × Rk1
→ Rd and ˜σ : R+ × Rd ×
Rk1
→ MR(d) two continuous functions, and MR(d) the space of d × d matrices with real
coeﬃcients. Here we assume that b and ˜σ satisfy the usual Lipschitz continuity conditions to
ensure that Equation (24) admits a unique strong solution X ν
(t0) = x0 with
(t0, x0) ∈ S. Moreover, the function f : Rd → R associated with the reward function J in (15) is
continuous, and there exists a constant K such that

such that X ν

× Rk2

t0,x0

t0,x0

|f (x)| ≤ K(cid:0)1 + (cid:107)x(cid:107)2(cid:1),

∀x ∈ Rd.

Then we can characterize the value of the game after deﬁning the operator H as follows:

H(t, x, p, A) = max
u1∈U1

min
u2∈U2

H (u1,u2)(t, x, p, A),

∀(t, x, p, q) ∈ S × Rd × MR(d),

with U1 (resp. U2) a closed subset of Rk1

(resp. Rk2

) and

H (u1,u2)(t, x, p, A) = −b(cid:62)p −

1
Tr[˜σ˜σ(cid:62)A],
2

∀(t, x, p, q) ∈ S × Rd × MR(d).

Proposition 1. Assume that V is locally bounded. Then

• V∗ is a viscosity supersolution of

− V∗t + H(., V∗, V∗x, V∗xx) ≥ 0,

on [0, T ) × Rd,

(25)

with

• V ∗ is a viscosity subsolution of

V∗(t, x) = lim inf

V (t, x).

(t(cid:48),x(cid:48))→(t,x)

− V ∗

t + H(., V ∗, V ∗

x , V ∗

xx) ≤ 0,

on [0, T ) × Rd,

(26)

with

V ∗(t, x) = lim sup

V (t, x).

(t(cid:48),x(cid:48))→(t,x)

• V is the unique viscosity solution of

− Vt + H(., V, Vx, Vxx) = 0,

on [0, T ) × Rd.

(27)

Proof. (For Proposition 1). Let us ﬁrst prove the supersolution property (25).

1. For this, assume to the contrary that there is (t0, x0) ∈ S = [0, T ] × Rd together with a

smooth function φ : S → R satisfying

0 = (V∗ − φ)(t0, x0) < (V∗ − φ)(t, x),

∀(t, x) ∈ [0, T ] × Rd,

(t, x) (cid:54)= (t0, x0),

such that

For ˜(cid:15) > 0, deﬁne ψ by

(cid:0) − ∂tφ + H(., φ, φx, φxx)(cid:1)(t0, x0) < 0.

ψ(t, x) = φ(t, x) − ˜(cid:15)(cid:0)|t − t0|2 + (cid:107)x − x0(cid:107)4(cid:1),

and note that ψ converges uniformly on compact sets to φ as ˜(cid:15) → 0. Since H is upper-
semicontinuous and (ψ, ψt, ψx, ψxx)(t0, x0) = (φ, φt, φx, φxx)(t0, x0), we can choose ˜(cid:15) > 0
small enough so that there exist r > 0, with t0 + r < T , such that for any u1 ∈ U1 one can
ﬁnd some ¯u2 ∈ U2 satisfying

(cid:0) − ∂tψ + H (u1,¯u2)(., ψ, ψx, ψxx)(cid:1)(t0, x0) < 0,

∀(t, x) ∈ Br(t0, x0),

(28)

13

with Br(t0, x0) the open ball of radius r and center (t0, x0). Let (tn, xn)n≥1 be a sequence
in Br(t0, x0) such that (cid:0)tn, xn, V (tn, xn)(cid:1) → (cid:0)t0, x0, V∗(t0, x0)(cid:1), ν1 ∈ U 1
, ν2 be the constant
tn
control ν2 = ¯u2, and ν = (ν1, ν2). Now write X n
(.) for the solution of (24) with
control ν and initial condition X n
tn

. = X ν
= xn, and consider the stopping time

tn,xn

θn = inf{s > tn; (s, X n

s ) (cid:54)∈ Br(t0, x0)}.

Note that θn < T since t0 + r < T . Using (28) gives

(cid:90) θn

E[

tn

[−∂tψ + H u(, ψ, ψx, ψxx)](s, X n

s ) ds] < 0.

By the arbitrariness of ν1, we get

inf
ν2∈U 2
tn

sup
ν1∈U 1
tn

(cid:90) θn

E[

tn

[−∂tψ + H u(, ψ, ψx, ψxx)](s, X n

s ) ds] < 0.

(29)

Applying Itˆo’s formula to ψ and using (29), we deduce that

ψ(tn, xn) < inf
ν2∈U 2
tn

sup
ν1∈U 1
tn

E[ψ(θn, X n
θn

)].

Now observe that φ > ψ + η on (cid:0)[0, T ] × Rd(cid:1) \ Br(t0, x0) for some η > 0. Hence, the
)] − η. Since (ψ −
above inequality implies that ψ(tn, xn) < inf ν2∈U 2
tn
V )(tn, xn) → 0, we can then ﬁnd n large enough so that

E[φ(θn, X n
θn

supν1∈U 1
tn

V (tn, xn) < inf
ν2∈U 2
tn

sup
ν1∈U 1
tn

E[φ(θn, X n
θn

)] − η/2.

Meanwhile, Theorem 1 ensures that

V (tn, xn) ≥ inf
ν2∈U 2
tn

sup
ν1∈U 1
tn

E[φ(θn, X ν

tn,Xn

(θn))].

which gives the required contradiction.

2. We now move to the proof of (26), which is similar to that of (25). We assume to the contrary

that there is (t0, x0) ∈ S together with a smooth function φ : S → R satisfying

0 = (V ∗ − φ)(t0, x0) > (V ∗ − φ)(t, x),

∀(t, x) ∈ [0, T ] × Rd,

(t, x) (cid:54)= (t0, x0),

such that

For ˜(cid:15) > 0, deﬁne ψ by

(cid:0) − ∂tφ + H(., φ, φx, φxx)(cid:1)(t0, x0) > 0.

ψ(t, x) = φ(t, x) + ˜(cid:15)(cid:0)|t − t0|2 + (cid:107)x − x0(cid:107)4(cid:1),

and note that ψ converges uniformly on compact sets to φ as ˜(cid:15) → 0. Since H is lower-
semicontinuous and (ψ, ψt, ψx, ψxx)(t0, x0)) = (φ, φt, φx, φxx)(t0, x0), we can choose ˜(cid:15) > 0
small enough so that there exist r > 0, with t0 + r < T , such that for any u1 ∈ U1 one can
ﬁnd some ¯u2 ∈ U2 satisfying

(cid:0) − ∂tψ + H (u1,¯u2)(., ψ, ψx, ψxx)(cid:1)(t0, x0) > 0,

∀(t, x) ∈ Br(t0, x0),

(30)

with Br(t0, x0) the open ball of radius r and center (t0, x0). Let (tn, xn)n≥1 be a sequence in
Br(t0, x0) such that (cid:0)tn, xn, V (tn, xn)(cid:1) → (cid:0)t0, x0, V ∗(t0, x0)(cid:1), ν1 ∈ U 1
, ν2 be the constant
tn
control ν2 = ¯u2, and ν = (ν1, ν2). Now write X n
(.) for the solution of (24) with
control ν and initial condition X n
tn

. = X ν
= xn, and consider the stopping time

tn,xn

θn = inf{s > tn; (s, X n

s ) (cid:54)∈ Br(t0, x0)}.

Note that θn < T since t0 + r < T . Using (30) gives

(cid:90) θn

E[

tn

[−∂tψ + H u(, ψ, ψx, ψxx)](s, X n

s ) ds] > 0.

14

By the arbitrariness of ν1, we get

inf
ν2∈U 2
tn

sup
ν1∈U 1
tn

(cid:90) θn

E[

tn

[−∂tψ + H u(, ψ, ψx, ψxx)](s, X n

s ) ds] > 0.

(31)

Applying Itˆo’s formula to ψ and using (31), we deduce that

ψ(tn, xn) > inf
ν2∈U 2
tn

sup
ν1∈U 1
tn

E[ψ(θn, X n
θn

)].

Now observe that ψ > φ + η on (cid:0)[0, T ] × Rd(cid:1) \ Br(t0, x0) for some η > 0. Hence, the
)] + η. Since (ψ −
above inequality implies that ψ(tn, xn) > inf ν2∈U 2
tn
V )(tn, xn) → 0, we can then ﬁnd n large enough so that

E[φ(θn, X n
θn

supν1∈U 1
tn

V (tn, xn) > inf
ν2∈U 2
tn

sup
ν1∈U 1
tn

E[φ(θn, X n
θn

)] + η/2.

Meanwhile, Theorem 1 ensures that

V (tn, xn) ≤ inf
ν2∈U 2
tn

sup
ν1∈U 1
tn

E[φ(θn, X ν

tn,Xn

(θn))],

which gives the required contradiction.

3. Since V∗ ≤ V ≤ V ∗, the comparison principle in Lemma 1 below (see (Pham, 2009)) shows

that V is the unique viscosity solution of (27) which completes the proof.

Lemma 1 (Comparision principle). Assume the same conditions as in Proposition 1. Let u and v
be respectively an upper-semicontinuous viscosity subsolution and a lower-semicontinuous viscosity
supersolution of (27) such that u(T, .) ≤ v(T, .) on Rd. Then, u ≤ v on [0, T ) × Rd.

5 Analysis of Optimal Adaptive Learning Rate and Batch

Size

This section will be devoted to analyzing the optimal learning rate and batch size but also discussing
their implications for GANs training.

5.1 Optimal Learning Rate

Note that problem (11) is a special case of the more general framework in Section 5.5 since the
dynamic of the process q in (8) is a diﬀusion of the same form as Equation (24). Indeed, one can
simply take the variables X ν

t , ν, b, and ˜σ in (24) as






X ν

t = q(t),

b(t, x, v) =

ν = u,

˜σ(t, x, v) =

(cid:18) vwgw(x)
−vθgw(x)

(cid:19)

,

(cid:18) vw√

¯ηwσw(x)

0

0
vθ(cid:112)¯ηθσθ(x)

(cid:19)

,

for any x and v = (vw, vθ), and apply the general results of Section 5.5 to analyze the optimal
adaptive learning rate, assuming

Assumption D. D.1 There exists a constant L1 such that for φ = gw, gθ, σw, σθ, we have

(cid:107)φ(w, θ) − φ(w(cid:48), θ(cid:48))(cid:107) ≤ L1(cid:0)(cid:107)w − w(cid:48)(cid:107) + (cid:107)θ − θ(cid:48)(cid:107)(cid:1),
(cid:107)φ(w, θ)(cid:107) ≤ L1(cid:0)1 + (cid:107)w(cid:107) + (cid:107)θ(cid:107)(cid:1),

for any (w, w(cid:48), θ, θ(cid:48)) ∈ (cid:0)RM (cid:1)2

× (cid:0)RN (cid:1)2

.

D.2 There exists a constant K 1 such that

|g(w, θ)| ≤ K 1(cid:0)1 + (cid:107)w(cid:107)2 + (cid:107)θ(cid:107)2(cid:1), ∀(w, θ) ∈ RM × RN .

15

In particular, it is clear that the value function v is a solution to the following Isaac-Bellman
equation:

vt + max min(uw,uθ∈[umin,umax])

w vw − uθg(cid:62)
θ vθ
(cid:2)(uw)2( ¯Σw : vww) + (uθ)2( ¯Σθ : vθθ)(cid:3)(cid:9) = 0,

(cid:8)(cid:0)uwg(cid:62)

(cid:1)

+ 1
2





v(T, ·) = g(·),

(32)

with A : B = Tr[A(cid:62)B] for any real matrices A and B. More precisely, we have

Proposition 2. Assume Assumption D. Then

• The value function v deﬁned in (11) is the unique viscosity solution of (32).

• When v ∈ C1,2([0, T ], RM × RN ), the optimal learning rate ¯uw and ¯uθ are given by

¯uw(t) =






umin ∨ (uw∗(t) ∧ umax) ,

if (cid:0) ¯Σw : vww

(cid:1) (cid:0)t, q(t)(cid:1) < 0,

umax,

umin,

if |umax − uw∗(t)| ≥ |umin − uw∗(t)|,

otherwise,

umin ∨ (cid:0)uθ∗(t) ∧ umax(cid:1) ,

if (cid:0) ¯Σθ : vθθ

(cid:1) (cid:0)t, q(t)(cid:1) > 0,

and

¯uθ(t) =






umax,

umin,

with uw∗(t) =

(cid:32)

(cid:33)

− g(cid:62)
w vw
¯Σw : vww

(cid:0)t, q(t)(cid:1), uθ∗(t) =




¯Σw(t, q) = {¯σw

t (¯σw

t )(cid:62)}(q),

if |umax − uθ∗(t)| ≥ |umin − uθ∗(t)|,

otherwise,

(cid:32)

(cid:33)

g(cid:62)
θ vθ
¯Σθ : vθθ

(cid:0)t, q(t)(cid:1), and ¯Σw and ¯Σθ as

¯Σθ(t, q) = {¯σθ

t (¯σθ)(cid:62)

t }(q),

t (q) = (cid:112)¯ηw(t)σw(q),
¯σw
for any t ∈ R+, and q = (w, θ) ∈ RM × RN .



t (q) = (cid:112)¯ηθ(t)σθ(q),
¯σθ

Proof. (For Proposition 2). Since Assumption D holds, one can simply use Proposition 1 to show
that v is the unique viscosity solution of (32). Moreover, the control ¯uw(t) maximizes the quadratic
function u :→ u(cid:0)g(cid:62)
t minimizes
u :→ −u(cid:0)g(cid:62)

(cid:1)(t, q(t)). Direct computation completes the proof.

(cid:1)(t, q(t)), and similarly the control ¯uθ

w vw
(cid:1)(t, q(t)) + 1

2 u2(cid:0) ¯Σw : vww

(cid:1)(t, q(t)) + 1

2 u2(cid:0) ¯Σθ : vθθ

θ vθ

Learning rate and GANs training. Now we can see the explicit dependency of optimal
learning rate on the convexity of the objective function, and its relation to Newton’s algorithm.
Speciﬁcally,

• Proposition 2 provides a two-step scheme for the selection of the optimal learning rate

Step 1. Use the Isaac-Bellman equation to get ¯u = (¯uw, ¯uθ)
Step 2. Given ¯u, apply the gradient algorithm with the optimal learning rate ¯u • ¯η.

• To get the expression of the optimal adaptive learning rate in Proposition 2, we need some
regularity conditions on the value function v such as v ∈ C1,2([0, T ], RM × RN ). Conditions
for such a regularity can be found in (Pimentel, 2019).

• When the value function v does not satisfy the regularity requirement v ∈ C1,2([0, T ], RM ×
RN ), it is standard to use discrete time approximations (Barles and Souganidis, 1991; Kushner
et al., 2001; Wang and Forsyth, 2008). Note that these approximations are shown to converge
towards the value function.

• The introduction of the clipping parameter umax is closely related to the convexity issue
discussed for GANs in Section 2.2. When the convexity condition ¯Σw : vww < 0 is violated,
the learning rate takes the maximum value umax or minimum value umin to escape as quickly
as possible from this non-concave region. The clipping parameter umax is also used to pre-
vent explosion in GANs training. Conditions under which explosion occurs are detailed in
Proposition 3.

16

• The control (¯uw, ¯uθ) of Proposition 2 is closely related to the standard Newton algorithm. To
see this, take ¯η = (1, 1), M = N = 1, σw = gw, σθ = gθ, and replace the value function v by
the suboptimal choice g. For such a conﬁguration of the parameters and with the convexity
conditions

(cid:0) ¯Σw : vww

(cid:1) = |gw|2gww < 0,

(cid:0) ¯Σθ : vθθ

(cid:1) = |gθ|2gθθ > 0,

the controls ¯uw and ¯uθ become

¯uw(t) = umin ∨

− 1
¯ηw(t)gww(q(t))

∧ umax,

¯uθ(t) = umin ∨

1
¯ηθ(t)gθθ(q(t))

∧ umax.

(33)

In absence of the clipping parameters umax and umin, the variables ¯uw and ¯uθ are exactly
the ones used for Newton’s algorithm.

Learning rate and GANs convergence revisited. We can now further analyze the impact
of the learning rate on the convergence of GANs, generalizing the example of Section 2.2 in which
poor choices of the learning rate destroy the convergence.

Let (cid:15) > 0, and ˜u = (˜uw, ˜uθ) be

(cid:32)

˜u(t) =

− 2|gw|2
¯Σw : gww

,

2|gθ|2
¯Σθ : gθθ

(cid:33)

(cid:0)t, q(t)(cid:1),

∀t ≥ 0.

We assume the existence of γ > 0 such that

γ ≤ − (cid:0) ¯Σw : gww

(cid:1) (t, q),

γ ≤ (cid:0) ¯Σθ : gθθ

(cid:1) (t, q),

for every t ≥ 0, and q = (w, θ) ∈ RM × RN . Then,

Proposition 3. For any control process u = (uw, uθ) ∈ U w × U θ such that

• uw ≥ (˜uw ∨ 1) + (cid:15), there exists ˜(cid:15) > 0 satisfying

J(T, 0, q0; u) ≤ −˜(cid:15) × T + E

(cid:34)(cid:90) T

0

(cid:8) − uθ|gθ|2 +

1
2

(uθ)2( ¯Σθ : gθθ)(cid:9)(cid:0)s, q(s)(cid:1) ds|q0

(cid:35)

= −˜(cid:15) × T + L1(T, q0; u),

for any (T, q0) ∈ R+ × RM +N .

• uθ ≥ (¯uθ ∨ 1) + (cid:15), there exists ˜(cid:15) > 0 such that

J(T, 0, q0; u) ≥ ˜(cid:15) × T + E

(cid:34)(cid:90) T

0

(cid:8)uw|gw|2 +

1
2

(uw)2( ¯Σw : gww)(cid:9)(cid:0)s, q(s)(cid:1) ds|q0

(cid:35)

= ˜(cid:15) × T + L2(T, q0; u),

for any (T, q0) ∈ R+ × RM +N .

Note that inequality (34) shows that

for any q0 ∈ RM +N satisfying lim supT →∞ L1(T, q0; u) < +∞. Similarly, inequality (35) gives

J(T, 0, q0; u) →

T →∞

−∞,

J(T, 0, q0; u) →
t→∞

+∞,

(34)

(35)

(36)

(37)

for every q0 ∈ RM +N such that lim inf T →∞ L2(T, q0; u) > −∞. Thus, Equations (36) and (37)
guarantee the explosion of the reward function without proper choices of the learning rate.

17

Proof. (For Proposition 3). Since the proofs of inequalities (34) and (35) are similar, we will only
prove (34). Let (T, q0) ∈ R+ × RM +N . By Itˆo’s formula and the SDE (10) for the process (q(t))t≥0,
we get

∂T J(T, 0, q0; u) = E(cid:2)(cid:8)(cid:0)uw|gw|2 − uθ|gθ|2(cid:1)

1
2

(cid:2)(uw)2( ¯Σw : gww) + (uw)2( ¯Σθ : gθθ)(cid:3) (cid:9)(cid:0)T, q(T )(cid:1)|q0

+
= E(cid:2)h1(cid:0)T, q(T ); uw(T )(cid:1)(cid:3) + E(cid:2)h2(cid:0)T, q(T ); uθ(T )(cid:1)(cid:3),

(cid:3)

with

and

h1(t, q; u) = u|gw|2(q) +

1
2

(u)2( ¯Σw : gww)(t, q),

h2(t, q; u) = −u|gθ|2(q) +

1
2

(u)2( ¯Σθ : gθθ)(t, q),

for any (t, q, u) ∈ R+ × RM +N × [0, umax]. Using

γ ≤ −( ¯Σw : gww)(t, q),

for every (t, q) ∈ R+ × RM +N , and uw ≥ (˜uw ∨ 1) + (cid:15) almost surely, we obtain2

E(cid:2)h1(cid:0)T, q(T ); uw(T )(cid:1)(cid:3) = E(cid:2)(cid:0)a uw(uw − ˜uw)(cid:1)(T )(cid:3)

≤ −(γ/2)E[uw(T )](cid:15) ≤ −(γ/2)(1 + (cid:15))(cid:15) = −˜(cid:15),

with a(T ) =

( ¯Σw : gww)(T, q(T ))
2

. Thus,

∂T J(T, 0, q0; u) ≤ −˜(cid:15) + E(cid:2)h2(cid:0)T, q(T ); uθ(T )(cid:1)(cid:3),

which ensures that

for all T ≥ 0.

5.2 Optimal Batch Size

J(T, 0, q0; u) ≤ −˜(cid:15)T + Lθ,

The optimal batch size problem can be formulated and analyzed by following the same approach
of Section 5.1.

Proposition 4. Under Assumption D,

• The value function v solving the batch size problem (13) is the unique viscosity solution of

the following equation:






vt + minmθ∈[1,mmax]

v(T, ·) = g(·),

(cid:40) (cid:0)g(cid:62)

w vw − g(cid:62)
mθ

θ vθ

(cid:1)

+

(cid:2)( ¯Σw : vww) + ( ¯Σθ : vθθ)(cid:3)
2(mθ)2

(cid:41)

= 0,

(38)

where mθ controls the generator batch size and mmax is an upper bound representing the
maximum batch size.

• When v ∈ C1,2([0, T ], RM × RN ), the optimal control ¯mθ is given by

¯mθ(t) =






(cid:32)

with m∗(t) =

1 ∨ (m∗(t) ∧ mmax) ,

if (cid:0)( ¯Σw : vww) + ( ¯Σθ : vθθ)(cid:1) (cid:0)t, q(t)(cid:1) > 0,

− (cid:0)m∗(t)(cid:1)−1(cid:12)

(cid:12) ≥ (cid:12)

(cid:12)1 − (cid:0)m∗(t)(cid:1)−1(cid:12)
(cid:12),

mmax,

1,

if (cid:12)
(cid:12)

(cid:0)mmax(cid:1)−1

otherwise,

( ¯Σw : vww) + ( ¯Σθ : vθθ)
w vw − g(cid:62)
g(cid:62)

θ vθ

(cid:33)

(cid:0)t, q(t)(cid:1).

The proof of Proposition 4 is omitted since it is very similar to the one of Proposition 2.

2Note that the convexity condition ( ¯Σw : gww) ≤ 0 is in force.

18

Some remarks. We observe that

• Proposition 4, in the same spirit of Proposition 2, suggests a two-step approach for the
implementation of the optimal batch size. First, estimate ¯mθ using Equation (38). Second,
apply the gradient algorithm with ¯mθ.

• The expression of the optimal batch size is given under the regularity condition v ∈ C1,2([0, T ], RM ×

RN ). It is possible to show that v ∈ C1,2([0, T ], RM × RN ) when g and the volatility ¯σ satisfy
simple Lipschitz continuity conditions (see (Evans, 1983) for more details). Finally, when v
does not satisfy the regularity assumption v ∈ C1,2([0, T ], RM × RN ), it is standard to use
discrete time approximations (see Section 5.1 for more details).

• The convexity condition ( ¯Σw : vww) + ( ¯Σθ : vθθ) involved in the expression of ¯mθ is the
aggregation of the generator component ( ¯Σw : vww) and the discriminator one ( ¯Σθ : vθθ).
Moreover, the clipping parameters 1 and mmax are used to prevent either gradient explosion
or vanishing gradient.

6 Numerical Experiment

In this section, we compare a well-known and established algorithm, the ADAM optimizer, with
its adaptive learning rate counterpart which we call LADAM. The implementation of the adaptive
learning rate is detailed in Section 6.1. We use three numerical examples to compare the con-
vergence speed of these algorithms: generation of Gaussian distributions, generation of Student
t-distributions, and ﬁnancial time series generation.

6.1 Optimizer Design

For computational eﬃciency, instead of directly applying the two-step resolution scheme introduced
in Section 5.1 for high dimensional Isaac-Bellman equation, we exploit the connection between the
optimal learning rate and Newton’s algorithm by following the methodology illustrated in Equation
(33). The main idea is to approximate the function v, introduced in Equation (11), by g.

Furthermore, we divide all the parameters into M sets. For example, for neural networks, one
may associate a unique set to each layer of the network. For each set i, denote by xi
t the value at
iteration t ∈ N of the set’s parameters and write gxi for the gradient of the loss g with respect to
the parameters of the set i. Then, we replace for each set i the update rule

t+1 = xi
xi

t − ¯ηtgxi(xi

t),

with ¯ηt a base reference learning rate that depends on the optimizer, by the following update rule:

where the adjustment ui

t, derived from Proposition 2, is deﬁned as follows:

t+1 = xi
xi

t − ui
t

(cid:0)¯ηtgxi (xi

t)(cid:1),

t = umin ∨
ui

(cid:107)gxi(xi

t)(cid:107)2
xi)gxixi](xi
t)

Tr[(¯gxi ¯g(cid:62)

∧ umax

xi)gxixi](cid:1)(xi

t) > 0; otherwise ui

if (cid:0)Tr[(¯gxi ¯g(cid:62)
¯ηtgxi and umax (resp. umin) is
the maximum (resp. minimum) allowed adjustment. Note that the same adjustment ui
t is used
for all parameters of the set i. The most expensive operation here is the computation of the term
(cid:0)Tr[(gxig(cid:62)
t) since we need to approximate the Hessian matrix gxixi .

t = umax. Here ¯gxi =

xi)gxixi](cid:1)(xi

√

6.2 Vanilla GANs Revisited

We use the vanilla GANs in Section 2 to show the relevance of our adaptive learning rate and the
inﬂuence of the batch size on GANs training.

19

Data. The numerical samples of X are drawn either from the Gaussian distribution N (m, σ2)
with (m, σ) = (3, 1) or the translated Student t-distribution a + T (n) where a = 3, and T (n) is
a standard Student t-distribution with n degrees of freedom. The noise Z always follows N (0, 1).
These samples are then decomposed into two sets: a training set and a test set. In the experi-
ment, one epoch refers to the number of gradient updates needed to pass the entire training dataset.

At the end of the training, the discriminator accuracy is expected to be around 50%, meaning that
the generator manages to produce samples that fool the discriminator who is unable to diﬀerentiate
the original data from the fake ones.

Network architecture description. We work here under the same setting of Section 2, with
discriminator detailed in Equation (3) while the generator is composed of the following two layers:
the ﬁrst layer is linear, and the second one is convolutional. Both layers use the ReLU activation
function.

6.2.1 Gaussian Distribution Generation

Here the input samples X are drawn from N (3, 1). We ﬁrst compare the accuracy of the discrim-
inator for two choices of the optimizers, namely the standard ADAM optimizer and the ADAM
optimizer with adaptive learning rates (i.e., LADAM). Figure 4 plots the accuracy of the discrimi-
nator when the base learning rate varies for ADAM and LADAM. Evidently, a bigger learning rate
yields faster convergence for both optimizers; LADAM outperforms ADAM due to the introduction
of the adaptive learning rate, and more importantly, LADAM performance is more robust with
respect to the initial learning rate since it can adjust using the additional adaptive component.

(a) Discriminator accuracy ADAM

(b) Discriminator accuracy LADAM

Figure 4: Discriminator accuracy for ADAM with a base learning rate in (a) and LADAM with
an adaptive learning rate in (b)

Next, we analyze the generator loss for ADAM and LADAM optimizers. Figure 5 shows the
variations of the generator loss when moving the base learning rate for these optimizers. First,
one can see that the loss decreases faster when using LADAM. Second, Figure 5 conﬁrms the
robustness of LADAM with respect to the choice of the initial learning rate, again thanks to the
additional adaptive learning rate component.

6.2.2 Student T- Distribution Generation

We repeat the same experiments of Section 6.2.1 but with diﬀerent inputs data. Here, the input
samples X are drawn from the translated Student t-distribution 3 + T (3) where T (3) is a standard
Student t-distribution with 3 degrees of freedom. Figures 6 and 7 display the discriminator accuracy
and generator loss for both ADAM and LADAM. One can see that conclusions of Section 6.2.1 still
hold, namely LADAM oﬀers better performance, and is more robust with respect to the choice of
the initial learning rate.

6.3 Financial Time Series Generation

Data. Data used here is taken from Quandl Wiki Prices database, which oﬀers stock prices,
dividends, and splits for 3000 US publicly-traded companies. In the numerical analysis, we will
focus on the following six stocks: Boeing, Caterpillar, Walt Disney, General Electric, IBM, and

20

01020304050epoch405060708090accuracylr = 1e-03lr = 5e-04lr = 1e-0401020304050epoch405060708090accuracylr = 1e-03lr = 5e-04lr = 1e-04(a) Generator loss ADAM

(b) Generator loss LADAM

Figure 5: Generator loss for ADAM with a base learning rate in (a) and LADAM with an adaptive
learning rate in (b).

(a) Discriminator accuracy ADAM

(b) Discriminator accuracy LADAM

Figure 6: Discriminator accuracy for ADAM with a base learning rate in (a) and LADAM with
an additional adaptive learning rate component in (b).

(a) Generator loss ADAM

(b) Generator loss LADAM

Figure 7: Generator loss for ADAM with a base learning rate in (a) and LADAM with an additional
adaptive learning rate component in (b).

Coca-Cola. For each stock, key quantities such as the traded volume, the open and close prices
are daily recorded. The studied time period starts from January, 2000 and ends in March, 2018.

Network architecture description. The neural network architecture is borrowed from (Yoon
et al., 2019). It consists of four network components: an embedding function, a recovery function, a
sequence generator, and a sequence discriminator. The key idea is to jointly train the auto-encoding
components (i.e., the embedding and the recovery functions) with the adversarial components (i.e.,
the discriminator and the generator networks) in order to learn how to encode features and to
generate data at the same time.

Numerical results. Figure 8 plots the discriminator accuracy for the two following optimizers:
the standard ADAM optimizer, and the same ADAM optimizer with an adaptive learning rate,
which we call LADAM. We ﬁx the base learning rate here to be 5 · 10−4 since the best performance

21

01020304050epoch0.0020.0030.0040.0050.0060.0070.0080.009loss generatorlr = 1e-03lr = 5e-04lr = 1e-0401020304050epoch0.0020.0030.0040.0050.0060.0070.0080.009loss generatorlr = 1e-03lr = 5e-04lr = 1e-0401020304050epoch405060708090accuracylr = 1e-03lr = 5e-04lr = 1e-0401020304050epoch405060708090accuracylr = 1e-03lr = 5e-04lr = 1e-0401020304050epoch0.0020.0030.0040.0050.0060.0070.0080.009loss generatorlr = 1e-03lr = 5e-04lr = 1e-0401020304050epoch0.0020.0030.0040.0050.0060.0070.0080.009loss generatorlr = 1e-03lr = 5e-04lr = 1e-04for ADAM is obtained with this value.

Figure 8 clearly shows that both accuracy and loss of LADAM converge faster than those of ADAM.
This is mainly due to the adaptive learning component which controls how fast the algorithm moves
in the gradient direction. Moreover, Figure 8.b reveals that the loss from LADAM is more stable
(i.e., with less ﬂuctuations) than that from ADAM. Such a behavior is expected since LADAM
incorporates the convexity of the loss function in its choice of the learning rate.

(a) Discriminator accuracy

(b) Generator loss

Figure 8: Discriminator accuracy for ADAM and LADAM in (a), and generator loss for ADAM
and LADAM in (b).

22

2550751001251501752000102030405060LadamAdam2550751001251501752000.000250.000500.000750.001000.001250.001500.001750.00200LadamAdamA Proof of Equation (14)

To derive (14), we follow the same approach of Section 3.1 and consider the value function below

˜v(t, q) = min

mθ∈ ˜Mθ

E(cid:2)g(cid:0)˜q(T /mθ(cid:1)(cid:12)

(cid:12)˜q(t) = q],

for any (t, q) ∈ R+ × RM +N , with ˜Mθ the set of admissible controls for mθ, and






d ˜w(t) = gw(˜q(t))dt + (cid:112)η/(mθ)σw(˜q(t))dW 1(t),

d˜θ(t) = −gθ(˜q(t))dt + (cid:112)η/(mθ)σθ(˜q(t))dW 2(t),

where ˜q(t) = ( ˜w(t), ˜θ(t)). The set ˜Mθ is deﬁned as

˜Mθ = (cid:8)m : m c`adl`ag in [1, mmax] adapted to F(W 1,W 2),

E[g(cid:0)˜q(T /m(cid:1) | ˜q(0) ] < ∞(cid:9).

(39)

(40)

Since the batch size is a single parameter, one can either choose to maximize or minimize the
objective function in (39). We choose here to reduce the loss as the ultimate goal of GANs is to
generate realistic data.

For any t ≥ 0 and mθ ∈ [1, mmax], write ˜qm = ( ˜wm, ˜θm) for the process ˜qm(t) = ˜q(mθ t). By Itˆo’s
formula and (40), we get

d ˜wm(t) =

gw(˜qm(t))
mθ

dt +

√

ησw(˜qm(t))
mθ

d ˜W 1(t),

d˜θm(t) = −

gθ(˜qm(t))
mθ

dt +

√

ησθ(˜qm(t))
mθ

d ˜W 2(t),




√

mθ(W 1, W 2)(cid:0)t/(mθ)(cid:1). The scaling property of the Brownian motion ensures
with ( ˜W 1, ˜W 2)(t) =
that ( ˜W 1, ˜W 2) is a Brownian motion as well. Thus, we replace the value function in (39) by the
quantity below

E(cid:2)g(cid:0)˜qm(T )(cid:1)(cid:12)

(cid:12)˜q(t) = q],

˜vm(t, q) = min

mθ∈Mθ

to complete the proof.

References

Arjovsky, M. and Bottou, L. (2017). Towards principled methods for training generative adversarial

networks. arXiv preprint arXiv:1701.04862.

Arjovsky, M., Chintala, S., and Bottou, L. (2017). Wasserstein generative adversarial networks. In

International Conference on Machine Learning, pages 214–223. PMLR.

Barles, G. and Souganidis, P. E. (1991). Convergence of approximation schemes for fully nonlinear

second order equations. Asymptotic Analysis, 4(3):271–283.

Bayraktar, E. and Yao, S. (2013). A weak dynamic programming principle for zero-sum stochas-
tic diﬀerential games with unbounded controls. SIAM Journal on Control and Optimization,
51(3):2036–2080.

Berard, H., Gidel, G., Almahairi, A., Vincent, P., and Lacoste-Julien, S. (2020). A closer look at
the optimization landscape of generative adversarial networks. In International Conference on
Learning Representations.

Bouchard, B. and Touzi, N. (2011). Weak dynamic programming principle for viscosity solutions.

SIAM Journal on Control and Optimization, 49(3):948–962.

Cao, H. and Guo, X. (2020). Approximation and convergence of GANs training: an SDE approach.

arXiv preprint arXiv:2006.02047.

Cao, H. and Guo, X. (2021). Generative dversarial network: some analytical perspectives. Machine

Learning And Data Sciences For Financial Markets: A Guide To Contemporary Practices.

23

Chakraborti, A., Toke, I. M., Patriarca, M., and Abergel, F. (2011). Econophysics review: I.

empirical facts. Quantitative Finance, 11(7):991–1012.

Conforti, G., Kazeykina, A., and Ren, Z. (2020). Game on random environment, mean-ﬁeld

Langevin system and neural networks. arXiv preprint arXiv:2004.02457.

Cont, R. (2001). Empirical properties of asset returns: stylized facts and statistical issues. Quan-

titative ﬁnance, 1(2):223.

Denton, E. L., Chintala, S., Szlam, A., and Fergus, R. (2015). Deep generative image models using
In Advances in Neural Information Processing

a Laplacian pyramid of adversarial networks.
Systems, pages 1486–1494.

Dionelis, N., Yaghoobi, M., and Tsaftaris, S. A. (2020). Tail of distribution GAN (TailGAN):
Generativeadversarial-network-based boundary formation. In 2020 Sensor Signal Processing for
Defence Conference (SSPD), pages 1–5. IEEE.

Domingo-Enrich, C., Jelassi, S., Mensch, A., Rotskoﬀ, G. M., and Bruna, J. (2020). A mean-ﬁeld

analysis of two-player zero-sum games. arXiv preprint arXiv:2002.06277.

Eckerli, F. and Osterrieder, J. (2021). Generative adversarial networks in ﬁnance: an overview.

Available at SSRN 3864965.

Eﬁmov, D., Xu, D., Kong, L., Nefedov, A., and Anandakrishnan, A. (2020). Using generative
adversarial networks to synthesize artiﬁcial ﬁnancial datasets. arXiv preprint arXiv:2002.02271.

Evans, L. C. (1983). Classical solutions of the Hamilton-Jacobi-Bellman equation for uniformly

elliptic operators. Transactions of the American Mathematical Society, 275(1):245–255.

Evans, L. C. and Souganidis, P. E. (1984). Diﬀerential games and representation formulas for solu-
tions of Hamilton-Jacobi-Isaacs equations. Indiana University Mathematics Journal, 33(5):773–
797.

Fatkullin, I. and Vanden-Eijnden, E. (2004). A computational strategy for multiscale systems with

applications to Lorenz 96 model. Journal of Computational Physics, 200(2):605–638.

Fu, R., Chen, J., Zeng, S., Zhuang, Y., and Sudjianto, A. (2019). Time series simulation by

conditional generative adversarial net. arXiv preprint arXiv:1904.11419.

Gadat, S. and Panloup, F. (2017). Optimal non-asymptotic bound of the Ruppert-Polyak averaging

without strong convexity. arXiv preprint arXiv:1709.03342.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville,
In Advances in Neural Information

A., and Bengio, Y. (2014). Generative adversarial nets.
Processing Systems, pages 2672–2680.

Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A. (2017). Improved training

of Wasserstein GANs. arXiv preprint arXiv:1704.00028.

Hsieh, Y.-P., Liu, C., and Cevher, V. (2019). Finding mixed Nash equilibria of generative ad-
versarial networks. In Chaudhuri, K. and Salakhutdinov, R., editors, Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research, pages 2810–2819. PMLR.

Jacod, J. and Shiryaev, A. (2013). Limit Theorems for Stochastic Processes, volume 288. Springer

Science & Business Media.

Kamalaruban, P., Huang, Y.-T., Hsieh, Y.-P., Rolland, P., Shi, C., and Cevher, V. (2020). Ro-
bust reinforcement learning via adversarial training with langevin dynamics. arXiv preprint
arXiv:2002.06063.

Krylov, N. (2014). On the dynamic programming principle for uniformly nondegenerate stochastic
diﬀerential games in domains and the Isaacs equations. Probability Theory and Related Fields,
158(3-4):751–783.

Kulharia, V., Ghosh, A., Mukerjee, A., Namboodiri, V., and Bansal, M. (2017). Contextual
RNN-GANs for abstract reasoning diagram generation. Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, 31(1).

24

Kushner, H. J. K., Kushner, H. J., Dupuis, P. G., and Dupuis, P. (2001). Numerical Methods
for Stochastic Control Problems in Continuous Time, volume 24. Springer Science & Business
Media.

Ledig, C., Theis, L., Husz´ar, F., Caballero, J., Cunningham, A., Acosta, A., Aitken, A., Tejani, A.,
Totz, J., Wang, Z., et al. (2017). Photo-realistic single image super-resolution using a generative
adversarial network. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 4681–4690.

Li, J., Wang, X., Lin, Y., Sinha, A., and Wellman, M. (2020). Generating realistic stock market
order streams. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 34(01):727–734.

Luc, P., Couprie, C., Chintala, S., and Verbeek, J. (2016). Semantic segmentation using adversarial

networks. arXiv preprint arXiv:1611.08408.

Marti, G. (2020). CorrGan: sampling realistic ﬁnancial correlation matrices using generative
adversarial networks. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pages 8459–8463. IEEE.

Mescheder, L., Geiger, A., and Nowozin, S. (2018). Which training methods for GANs do actually
converge? In Dy, J. and Krause, A., editors, Proceedings of the 35th International Conference on
Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3481–3490.
PMLR.

Mirza, M. and Osindero, S. (2014). Conditional generative adversarial nets. arXiv preprint

arXiv:1411.1784.

Moulines, E. and Bach, F. R. (2011). Non-asymptotic analysis of stochastic approximation al-
gorithms for machine learning. In Advances in Neural Information Processing Systems, pages
451–459.

Mounjid, O. and Lehalle, C.-A. (2019).

Improving reinforcement learning algorithms: towards

optimal learning rate policies. arXiv preprint arXiv:1911.02319.

Ni, H., Szpruch, L., Wiese, M., Liao, S., and Xiao, B. (2020). Conditional Sig-Wasserstein GANs

for time series generation. arXiv preprint arXiv:2006.05421.

Pham, H. (2009). Continuous-time Stochastic Control and Optimization with Financial Applica-

tions, volume 61. Springer Science & Business Media.

Pimentel, E. A. (2019). Regularity theory for the Isaacs equation through approximation meth-
ods. In Annales de l’Institut Henri Poincar´e C, Analyse non Lin´eaire, volume 36, pages 53–74.
Elsevier.

Radford, A., Metz, L., and Chintala, S. (2015). Unsupervised representation learning with deep

convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434.

Reed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B., and Lee, H. (2016). Generative Adver-
sarial Text to Image Synthesis. In 33rd International Conference on Machine Learning, pages
1060–1069.

Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X. (2016). Im-
proved techniques for training GANs. In Advances in Neural Information Processing Systems,
pages 2234–2242.

Sion, M. (1958). On general minimax theorems. Paciﬁc Journal of Mathematics, 8(1):171–176.

Sirbu, M. (2014a). On martingale problems with continuous-time mixing and values of zero-sum
games without the Isaacs condition. SIAM Journal on Control and Optimization, 52(5):2877–
2890.

Sirbu, M. (2014b). Stochastic Perron’s method and elementary strategies for zero-sum diﬀerential

games. SIAM Journal on Control and Optimization, 52(3):1693–1711.

Storchan, V., Vyetrenko, S., and Balch, T. (2020). MAS-GAN: adversarial calibration of multi-

agent market simulators.

25

Takahashi, S., Chen, Y., and Tanaka-Ishii, K. (2019). Modeling ﬁnancial time-series with generative

adversarial networks. Physica A: Statistical Mechanics and its Applications, 527:121261.

Von Neumann, J. (1959). On the theory of games of strategy. Contributions to the Theory of

Games, 4:13–42.

Vondrick, C., Pirsiavash, H., and Torralba, A. (2016). Generating videos with scene dynamics. In

Advances in Neural Information Processing Systems, pages 613–621.

Wang, J. and Forsyth, P. A. (2008). Maximal use of central diﬀerencing for Hamilton–Jacobi–

Bellman PDEs in ﬁnance. SIAM Journal on Numerical Analysis, 46(3):1580–1601.

Weinan, E., Liu, D., and Vanden-Eijnden, E. (2005). Analysis of multiscale methods for stochastic
diﬀerential equations. Communications on Pure and Applied Mathematics, 58(11):1544–1585.

Wiese, M., Bai, L., Wood, B., Morgan, J. P., and Buehler, H. (2019). Deep hedging: learning to

simulate equity option markets. arXiv preprint arXiv:1911.01700.

Wiese, M., Knobloch, R., Korn, R., and Kretschmer, P. (2020). Quant GANs: deep generation of

ﬁnancial time series. Quantitative Finance, 20(9):1419–1440.

Yeh, R., Chen, C., Lim, T. Y., Hasegawa-Johnson, M., and Do, M. N. (2016). Semantic image

inpainting with perceptual and contextual losses. arXiv preprint arXiv:1607.07539, 2(3).

Yoon, J., Jarrett, D., and Van der Schaar, M. (2019). Time-series generative adversarial networks.

Neural Information Processing Systems.

Zhu, J.-Y., Kr¨ahenb¨uhl, P., Shechtman, E., and Efros, A. A. (2016). Generative visual manipulation
on the natural image manifold. In European Conference on Computer Vision, pages 597–613.
Springer.

Zumbach, G. and Lynch, P. (2001). Heterogeneous volatility cascade in ﬁnancial markets. Physica

A: Statistical Mechanics and its Applications, 298(3-4):521–529.

26

