1
2
0
2

g
u
A
9

]

V
C
.
s
c
[

1
v
4
9
8
3
0
.
8
0
1
2
:
v
i
X
r
a

FIFA: Fast Inference Approximation for Action
Segmentation

Yaser Souri1, Yazan Abu Farha1, Fabien Despinoy2, Gianpiero Francesca2, and
Juergen Gall1

1 University of Bonn
2 Toyota Motor Europe
{souri, abufarha, gall}@iai.uni-bonn.de
{fabien.despinoy, gianpiero.francesca}@toyota-motor.com

Abstract. We introduce FIFA, a fast approximate inference method for
action segmentation and alignment. Unlike previous approaches, FIFA
does not rely on expensive dynamic programming for inference. Instead,
it uses an approximate diﬀerentiable energy function that can be min-
imized using gradient-descent. FIFA is a general approach that can re-
place exact inference improving its speed by more than 5 times while
maintaining its performance. FIFA is an anytime inference algorithm
that provides a better speed vs. accuracy trade-oﬀ compared to exact in-
ference. We apply FIFA on top of state-of-the-art approaches for weakly
supervised action segmentation and alignment as well as fully supervised
action segmentation. FIFA achieves state-of-the-art results on most met-
rics on two action segmentation datasets.

Keywords: Action Segmentation · Approximate Inference.

1

Introduction

Action segmentation is the task of predicting the action label for each frame
in the input video. Action segmentation is usually studied in the context of
activities performed by a single person, where temporal smoothness of actions
are assumed. Fully supervised approaches for action segmentation [25,1,30,40]
already achieve good performance on this task. Most approaches for fully su-
pervised action segmentation make frame-wise predictions [25,1,30] while trying
to model the temporal relationship between the action labels. These approaches
usually suﬀer from over-segmentation. Recent works [40,15] try to overcome
the over-segmentation problem by ﬁnding the action boundaries and temporally
smoothing the predictions inside each action segment. But these post-processing
approaches still can not guarantee temporal smoothness.

Action segmentation inference is the problem of making segment-wise smooth
predictions from frame-wise probabilities given a known grammar of the actions
and their average lengths [35]. The typical inference in action segmentation in-
volves solving an expensive Viterbi-like dynamic programming problem that
ﬁnds the best action sequence and its corresponding lengths. In the literature

 
 
 
 
 
 
2

Y. Souri et al.

usually weakly supervised action segmentation approaches [21,34,35,29,37] use
inference at test time. Despite being very useful for action segmentation, the
inference problem remains the main computational bottleneck in the action seg-
mentation pipeline [37].

In this paper, we propose FIFA, a fast anytime approximate inference pro-
cedure that achieves comparable performance with respect to the dynamic pro-
gramming based Viterbi decoding inference at a fraction of the computational
time. Instead of relying on dynamic programming, we formulate the energy func-
tion as an approximate diﬀerentiable function of segment lengths parameters and
use gradient-descent-based methods to search for a conﬁguration that minimizes
the approximate energy function. Given a transcript of actions and the corre-
sponding initial lengths conﬁguration, we deﬁne the energy function as a sum
over segment level energies. The segment level energy consists of two terms: a
length energy term that penalizes the deviations from a global length model and
an observation energy term that measures the compatibility between the current
conﬁguration and the predicted frame-wise probabilities. A naive approach to
model the observation energy would be, to sum up the negative log probabilities
of the action labels that are deﬁned based on the length conﬁguration. Neverthe-
less, such an approach is not diﬀerentiable with respect to the segment lengths.
In order to optimize the energy using gradient descent-based methods, the ob-
servation energy has to be diﬀerentiable with respect to the segment lengths. To
this end, we construct a plateau-shaped mask for each segment which temporally
locates the segment within the video. This mask is parameterized by the segment
lengths, the position in the video, and a sharpness parameter. The observation
energy is then deﬁned as a product of a segment mask and the predicted frame-
wise negative log probabilities, followed by a sum pooling operation. Finally, a
gradient descent-based method is used to ﬁnd a conﬁguration for the segment
lengths that minimizes the total energy.

FIFA is a general inference approach and can be applied at test time on top
of diﬀerent action segmentation approaches for fast inference. We evaluate our
approach on top of the state-of-the-art methods for weakly supervised temporal
action segmentation, weakly supervised action alignment, and fully supervised
action segmentation. Results on the Breakfast [19] and Hollywood extended [4]
datasets show that FIFA achieves state-of-the-art results on most metrics. Com-
pared to the exact inference using the Viterbi decoding, FIFA is at least 5 times
faster. Furthermore, FIFA is an anytime algorithm which can be stopped after
each step of the gradient-based optimization, therefore it provides a better speed
vs. accuracy trade-oﬀ compared to exact inference.

2 Related Work

In this section we highlight relevant works addressing fully and weakly supervised
action segmentation that have been recently achieved.

Fully Supervised Action Segmentation. In fully supervised action segmentation,
frame-level labels are used for training. Initial attempts for action segmenta-

FIFA: Fast Inference Approximation for Action Segmentation

3

tion applied action classiﬁers on a sliding window over the video frames [36,17].
However, these approaches did not capture the dependencies between the action
segments. With the objective of capturing the context over long video sequences,
context free grammars [39,33] or hidden Markov models (HMMs) [26,20,23]
are typically combined with frame-wise classiﬁers. Recently, temporal convolu-
tional networks showed good performance for the temporal action segmentation
task using encoder-decoder architectures [25,28] or even multi-stage architec-
tures [1,30]. Many approaches further improve the multi-stage architectures by
applying post-processing based on boundary-aware pooling operation [40,15] or
graph-based reasoning [14]. Without any inference most of the fully-supervised
approaches suﬀer from oversegmentation at test time.

Weakly Supervised Action Segmentation. To reduce the annotation cost, many
approaches that rely on a weaker form of supervision have been proposed. Ear-
lier approaches apply discriminative clustering to align video frames to movie
scripts [8]. Bojanowski et al . [5] proposed to use as supervision the transcripts in
the form of ordered lists of actions. Indeed, many approaches rely on this form
of supervision to train a segmentation model using connectionist temporal clas-
siﬁcation [13], dynamic time warping [6] or energy-based learning [29]. In [7], an
iterative training procedure is used to reﬁne the transcript. A soft labeling mech-
anism is further applied at the boundaries between action segments. Kuehne et
al . [22] applied a speech recognition system based on a HMM and Gaussian mix-
ture model (GMM) to align video frames to transcripts. The approach generates
pseudo ground truth labels for the training videos and iteratively reﬁne them.
A similar idea has been recently used in [34,23]. Richard et al . [35] combined
the frame-wise loss function with the Viterbi algorithm to generate the target
labels. At inference time, these approaches iterate over the training transcripts
and select the one that matches best the testing video. By contrast, Souri et
al . [37] predict the transcript besides the frame-wise scores at inference time.
State of the art weakly supervised action segmentation approaches require time
consuming dynamic programming based inference at test time.

Energy-Based Inference. In energy-based inference methods, gradient descent is
used at inference time as described in [27]. The goal is to minimize an energy
function that measures the compatibility between the input variables and the
predicted variables. This idea has been exploited for many structured prediction
tasks such as image generation [9,16], machine translation [12] and structured
prediction energy networks [3]. Belanger and McCallum [2] relaxed the discrete
output space for multi-label classiﬁcation tasks to a continuous space and used
gradient descent to approximate the solution. Gradient-based methods have also
been used for other applications such as generating adversarial examples [11] and
learning text embeddings [24].

4

Y. Souri et al.

3 Background

The following sections introduce all the concepts and notations required to un-
derstand the proposed FIFA methodology.

3.1 Action Segmentation

In action segmentation, we want to temporally localize all the action segments
occurring in a video. In this paper, we consider the case where the actions are
from a predeﬁned set of M classes (a background class is used to cover uninter-
esting parts of a video). The input video of length T is usually represented as
a set of d dimensional features vectors x1:T = (x1, . . . , xT ). These features are
extracted oﬄine and are assumed to be the input to the action segmentation
model. The output of action segmentation can be represented in two ways:

– Frame-wise representation y1:T = (y1, . . . , yT ) where yt represents the action

label at time t.

– Segment-wise representation s1:N = (s1, . . . , sN ) where segment sn is rep-
resented by both the action label of the segment cn and its corresponding
length (cid:96)n i.e. sn = (cn, (cid:96)n). The ordered list of actions c1:N is usually referred
to as the transcript.

These two representations are equal and redundant i.e. it is possible to compute
one from the other. In order to transfer from the segment-wise to the frame-wise
representation, we introduce a mapping α(t; c1:N , (cid:96)1:N ) which outputs the action
label at frame t given the segment-wise labeling.

The target labels to train a segmentation model, depend on the level of
supervision. In fully supervised action segmentation [1,30,40], the target label
for each frame is provided. However, in weakly supervised approaches [35,29,37]
only the ordered list of action labels are provided during training while their
lengths are unknown.

Recent fully supervised approaches for action segmentation like MSTCN [1]
and its variants directly predict the frame-wise representation y1:T by choosing
the action label with the highest probability for each frame independently. This
results in predictions that are sometimes oversegmented.

Conversely, recent weakly supervised action segmentation approaches like
NNV [35] and follow-up work include an inference stage during testing where
they explicitly predict the segment-wise representation. This inference stage in-
volves a dynamic programming algorithm for solving an optimization problem
which is a computational bottleneck for these approaches.

3.2 Inference in Action Segmentation

During testing, the inference stage involves an optimization problem to ﬁnd the
most likely segmentation for the input video i.e.,

c1:N , (cid:96)1:N = argmax
ˆc1:N ,ˆ(cid:96)1:N

(cid:111)
(cid:110)
p(ˆc1:N , ˆ(cid:96)1:N |x1:T )
.

(1)

FIFA: Fast Inference Approximation for Action Segmentation

5

Given the transcript c1:N , the inference stage boils down to ﬁnding the seg-

ment lengths (cid:96)1:N by aligning the transcript to the input video i.e.,

(cid:96)1:N = argmax

(cid:111)
(cid:110)
p(ˆ(cid:96)1:N |x1:T , c1:N )
.

ˆ(cid:96)1:N

(2)

In approaches like NNV [35] and CDFL [29], the transcript is found by iterat-
ing over the transcripts seen during training and selecting the transcript that
achieves the most likely alignment by optimizing (2). In MuCon [37], the tran-
script is predicted by a sequence to sequence network.

The probability deﬁned in (2) is broken down by making independences as-

sumption between frames

p(ˆ(cid:96)1:N |x1:T , c1:N ) =

T
(cid:89)

t=1

p(cid:0)α(t; c1:N , ˆ(cid:96)1:N )|xt

(cid:1) ·

N
(cid:89)

n=1

p(cid:0)ˆ(cid:96)n|cn

(cid:1)

(3)

(cid:1) is referred to as the observation model and p(cid:0)(cid:96)n|cn

where p(cid:0)α(t)|xt
(cid:1) as the
length model. Here α(t) is the mapping from time t to the action label given
the segmentwise labeling. The observation model estimates the frame-wise ac-
tion probabilities and is implemented using a neural network. The length model
is used to constrain the inference deﬁned in (2) with the assumption that the
length of segments for the same action follow a particular probability distribu-
tion. The segment length is usually modelled by a Poisson distribution with a
class dependent mean parameter λcn i.e.,

p(cid:0)(cid:96)n|cn

(cid:1) =

λ(cid:96)n
cn

exp(−λcn )

(cid:96)n!

.

(4)

This optimization is solved using an expensive dynamic programming based
Viterbi decoding [35]. For details on how to solve this optimization problem
using Viterbi decoding please refer to the supplementary material.

4 FIFA: Fast Inference Approximation

Our goal is to introduce a fast inference algorithm for action segmentation. We
want the fast inference to be applicable in both weakly supervised and fully
supervised action segmentation. We also want the fast inference to be ﬂexible
enough to work with diﬀerent action segmentation methods. To this end, we
introduce FIFA, a novel approach for fast inference for action segmentation.

In the following for brevity we write the mapping α(t; c1:N , (cid:96)1:N ) simply as
α(t). Maximizing probability (2) can be rewritten as minimizing the negative
log of that probability

(cid:26)

argmax

p(ˆ(cid:96)1:N |x1:T , c1:N )

(cid:27)

(cid:26)

= argmin

− log (cid:0)p(ˆ(cid:96)1:N |x1:T , c1:N )(cid:1)

(cid:27)

(5)

6

Y. Souri et al.

Fig. 1. Overview of the FIFA optimization process. At each step in the optimization,
using the current length estimates a set of masks are generated. Using the generated
masks and the frame-wise negative log probabilities, the observation energy is calcu-
lated in an approximate but diﬀerentiable manner. The length energy is calculated
from the current length estimate and added to the observation energy to calculate the
total energy value. Taking the gradient of the total energy with respect to the length
estimates we can update it using a gradient step.

which we refer to as the energy E((cid:96)1:N ). Using (3) the energy is rewritten as

E((cid:96)1:N ) = − log

p((cid:96)1:N |x1:T , c1:N )

= − log

(cid:18)

(cid:19)

(cid:18) T
(cid:89)

t=1

p(cid:0)α(t)|xt

(cid:1) ·

(cid:19)

p(cid:0)(cid:96)n|cn

(cid:1)

N
(cid:89)

n=1

=

T
(cid:88)

t=1
(cid:124)

− log p(cid:0)α(t)|xt

(cid:1)

+

(cid:123)(cid:122)
Eo

(cid:125)

N
(cid:88)

n=1
(cid:124)

− log p(cid:0)(cid:96)n|cn

(cid:1)

.

(cid:123)(cid:122)
E(cid:96)

(cid:125)

(6)
The ﬁrst term in (6), Eo is referred to as the observation energy. This term
calculates the cost of assigning the labels for each frame and will be calculated
from the frame-wise probability estimates. The second term E(cid:96) is referred to as
the length energy. This term is the cost of each segment having a length given
that we assume some average length for actions of a speciﬁc class.

We proposed to optimize the energy deﬁned in (6) using gradient based opti-
mization in order to avoid the need for time-consuming dynamic programming.
We start with an initial estimate of the lengths (obtained from the length model
of each approach or calculated from training data when available) and update
our estimate to minimize the energy function.

As the energy function E((cid:96)1:N ) is not diﬀerentiable with respect to the
lengths, we have to calculate a relaxed and approximate energy function E∗((cid:96)1:N )
that respects this mathematical property.

4.1 Approximate Diﬀerentiable Energy E∗

The energy function E as deﬁned in (6) is not diﬀerentiable in two parts. First
the observation energy term Eo is not diﬀerentiable because of the α(t) function.

X+LengthModelFIFA: Fast Inference Approximation for Action Segmentation

7

Second, the length energy term E(cid:96) is not diﬀerentiable because it expects natural
numbers as input and cannot be computed on real values which are dealt with in
gradient-based optimization. Below we describe how we approximate and make
each of the terms diﬀerentiable.

Approximate Diﬀerentiable Observation Energy Consider a N ×T matrix
P containing negative log probabilities, i.e.

Imagine a mask matrix M with the same size N × T where

P [n, t] = − log p(cn|xt).

M [n, t] =

(cid:40)

0
1

if α(t) (cid:54)= cn
if α(t) = cn

.

Using the mask matrix we can rewrite the observation energy term as

Eo =

T
(cid:88)

N
(cid:88)

t=1

n=1

M [n, t] · P [n, t].

(7)

(8)

(9)

In order to make the observation energy term diﬀerentiable with respect to
the length, we propose to construct an approximate diﬀerentiable mask matrix
M ∗. We use the following smooth and parametric plateau function

f (t|λc, λw, λs) =

1
(eλs(t−λc−λw) + 1)(eλs(−t+λc−λw) + 1)

(10)

from [31]. This plateau function has three parameters and it is diﬀerentiable
with respect to them: λc controls the center of the plateau, λw is the width and
λs is the sharpness of the plateau function.

While the sharpness of the plateau functions λs used to construct the ap-
proximate mask M ∗ is ﬁxed as a hyper-parameter of our approach, the center
λc and the width λw are computed from the lengths (cid:96)1:N . First we calculate the
starting position of each plateau function bn as

b1 = 0, bn =

n−1
(cid:88)

n(cid:48)=1

(cid:96)n(cid:48).

(11)

We can then deﬁne both the center and the width parameters of each plateau
function as

λc
n = bn + (cid:96)n/2,

λw
n = (cid:96)n/2

and deﬁne each row of the approximate mask as
n, λw

M ∗[n, t] = f (t|λc

n , λs).

(12)

(13)

Now we can calculate a diﬀerentiable approximate observation energy similar to
(9) as

E∗

o =

T
(cid:88)

N
(cid:88)

t=1

n=1

M ∗[n, t] · P [n, t].

(14)

8

Y. Souri et al.

Approximate Diﬀerentiable Length Energy For the gradient-based op-
timization, we must relax the length values to be positive real values instead
of natural numbers. As the Poisson distribution (4) is only deﬁned on natural
numbers, we propose to use a substitute distribution deﬁned on real numbers.
As a replacement, we experiment with a Laplace distribution and a Gaussian
distribution. In both cases, the scale or the width parameter of the distribution
is assumed to be ﬁxed.

We can rewrite the length energy E(cid:96) as the approximate length energy

E∗

(cid:96) ((cid:96)1:N ) =

N
(cid:88)

n=1

− log p((cid:96)n|λ(cid:96)
cn

),

(15)

where λ(cid:96)
cn
In case of the Laplace distribution this length energy will be

is the expected value for the length of a segment from the action cn.

E∗

(cid:96) ((cid:96)1:N ) =

1
Z

N
(cid:88)

n=1

|(cid:96)n − λ(cid:96)
cn

|,

(16)

where Z is the constant normalization factor. This means that the length energy
will penalize any deviation from the expected average length linearly. Similarly,
for the Gaussian distribution, the length energy will be

E∗

(cid:96) ((cid:96)1:N ) =

1
Z

N
(cid:88)

n=1

|(cid:96)n − λ(cid:96)
cn

|2,

(17)

which means that the Gaussian length energy will penalize any deviation from
the expected average length quadratically.

With the objective to maintain a positive value for the length during the op-
timization process, we estimate the length in log space and convert it to absolute
space only in order to compute both the approximate mask matrix M ∗ and the
approximate length energy E∗
(cid:96) .

Approximate Energy Optimization The total approximate energy func-
tion is deﬁned as a weighted sum of both the approximate observation and the
approximate length energy functions

E∗((cid:96)1:N ) = E∗

o ((cid:96)1:N , Y ) + βE∗

(cid:96) ((cid:96)1:N )

(18)

where β is the multiplier for the length energy.

Given an initial length estimate (cid:96)0

1:N , we iteratively update this estimate
to minimize the total energy. Figure 1 illustrates the optimization step for our
approach. During each optimization step, we ﬁrst calculate the energy E∗ and
then calculate the gradients of the energy with respect to the length values.
Using the calculated gradients, we update the length estimate using a gradient
descent update rule such as SGD or Adam. After a certain number of gradient
steps (50 steps in our experiments) we will ﬁnally predict the segment length.

FIFA: Fast Inference Approximation for Action Segmentation

9

Fig. 2. Speed vs. accuracy trade-oﬀ of
diﬀerent inference approaches applied to
the MuCon method. Using FIFA we can
achieve a better speed vs. accuracy trade-
oﬀ compared to frame sampling or hy-
pothesis pruning in exact inference.

Fig. 3. Eﬀect of the length energy multi-
plier for Laplace and Gaussian length en-
ergy. Accuracy is calculated on the break-
fast dataset using FIFA applied to the
MuCon approach trained in the weakly
supervised action segmentation setting.

During testing, if the transcript is provided then it is used (e.g. using the
MuCon [37] approach or in a weakly supervised action alignment setting). How-
ever, if the latter is not known (e.g. in a fully supervised approach or CDFL
[29] for weakly supervised action segmentation) we perform the optimization for
each of the transcripts seen during training and select the most likely one based
on the ﬁnal energy value at the end of the optimization.

The initial length estimates are calculated from the length model of each
approach in case of weakly supervised setting whereas in fully supervised setting
the average length of each action class is calculated from the training data and
used as the initial length estimates. The initial length estimates are also used as
the expected length parameters for the length energy calculations.

The hyper-parameters like the choice of the optimizer, number of steps, learn-
ing rate, and the mask sharpness, remain as the hyper-parameters of our ap-
proach.

5 Experiments

5.1 Evaluation Protocols and Datasets

We evaluate FIFA on 3 diﬀerent tasks: weakly supervised action segmentation,
fully supervised action segmentation, and weakly supervised action alignment.
Results for action alignment are included in the supplementary material. We
obtain the source code for the state-of-the-art approaches on each of these tasks
and train a model using the standard training conﬁguration of each model. Then
we apply FIFA as a replacement for an existing inference stage or as an additional
inference stage.

We evaluate our model using the Breakfast [19] and Hollywood extended [4]
datasets on the 3 diﬀerent tasks. Details of the datasets are included in the
supplementary material.

0.10.20.51.02.04.08.016.0Time (minutes)363840424446485052Accuracy025102030405060EI10%15%EI234510EIFIFAPruningFrame samplingExact inference0.00.20.40.60.81.0Energy Multiplier4546474849505152AccuracyLaplace length energyGaussian length energy10

Y. Souri et al.

Inference Method initialization MoF

Exact

MuCon [37]
Equal

50.7
48.8 (-1.9)

MuCon [37]
Equal
Table 1. Impact of the Length Model initialization for MuCon using exact inference
and FIFA for weakly supervised action segmentation on the Breakfast dataset.

51.3
50.2 (-1.1)

FIFA

5.2 Results and Discussions

In this section, we study the speed-accuracy trade-oﬀ and the impact of the
length model. Additional ablation experiments are included in the supplemen-
tary material.

Speed vs. Accuracy Trade-oﬀ. One of the major beneﬁts of FIFA is the ﬂexibil-
ity of choosing the number of optimization steps. The number of steps of the
optimization can be a tool to trade-oﬀ speed vs. accuracy. In exact inference,
we can use frame-sampling i.e. lowering the resolution of the input features, or
hypothesis pruning i.e. beam search for speed vs. accuracy trade-oﬀ.

Figure 2 plots the speed vs. accuracy trade-oﬀ of exact inference compared to
FIFA. We observe that FIFA provides a much better speed-accuracy trade-oﬀ as
compared to frame-sampling for exact inference. The best performance after 50
steps with 5.9% improvement on the MoF accuracy compared to not performing
any inference.

Impact of the Length Energy. For the length energy, we assume that the segment
lengths follow a Laplace distribution. Figure 3 shows the impact of the length
energy multiplier on the performance. While the best accuracy is achieved with a
multiplier of 0.05, our approach is robust to the choice of these hyper-parameters.
We further experimented with a Gaussian length energy. However, as shown in
the ﬁgure, the performance is much worse compared to the Laplace energy. This
is due to the quadratic penalty that dominates the total energy, which makes
the optimization biased towards the initial estimate and ignores the observation
energy.

Impact of Length Model Initialization. Since FIFA starts with an initial esti-
mate for the lengths, the choice of initialization might have an impact on the
performance. Table 1 shows the eﬀect of initializing the lengths with equal values
compared to using the length model of MuCon [37] for the weakly supervised
action segmentation on the Breakfast dataset. As shown in the table, FIFA is
more robust to initialization compared to the exact inference as the drop in
performance is approximately half of the exact inference.

5.3 Comparison to State of the Art

In this section, we compare FIFA to other state-of-the-art approaches.

FIFA: Fast Inference Approximation for Action Segmentation

11

Fig. 4. Visualization of the FIFA optimization process. On the right the values of the
total approximate energy is plotted. On the left, negative log probability values, ground
truth segmentation, optimization initialization, the masks and the segmentation after
inference is ploted.

Weakly Supervised Action Segmentation. We apply FIFA on top of two state-
of-the-art approaches for weakly supervised action segmentation namely Mu-
Con [37] and CDFL [29] on the Breakfast dataset [19] and report the results in
Table2. FIFA applied on CDFL achieves a 12 times faster inference speed while
obtaining results comparable to exact inference. FIFA applied to MuCon achieves
a 5 times faster inference speed and obtains a new state-of-the-art performance
on the Breakfast dataset on most of the metrics.

Method

MoF MoF-BG IoU IoD Time (min)

ISBA [7]
NNV [35]
D3TW [6]

38.4
43.0
45.7

50.2
CDFL [29]
CDFL∗
49.4
FIFA + CDFL∗ 47.9

38.4
-
-

48.0
47.5
46.3

24.2 40.6

-
-

-
-

0.01
234
-

33.7 45.4
35.2 46.4
34.7 48.0 20.4 (×12.8)

-
260

-

-

-
4.1

40.9 54.0
41.1 53.3 0.8 (×5.1)

-
MuCon [37]
47.1
MuCon∗
50.3
50.7
FIFA + MuCon∗ 51.3
50.7
Table 2. Results for weakly supervised
action segmentation on the Breakfast
dataset. ∗ indicates results obtained by
running the code on our machine.

Method

BCN [40]
ASRF [15]

F1@{10, 25, 50} Edit MoF

68.7 65.5 55.0 66.2 70.4
74.3 68.9 56.1 72.4 67.6

MS-TCN++ [30]
64.1 58.6 45.9 65.6 67.6
FIFA + MS-TCN++ 74.3 69.0 54.3 77.3 67.9

52.6 48.1 37.9 61.7 66.3
75.5 70.2 54.8 78.5 68.6

MS-TCN [1]
FIFA + MS-TCN
Table 3. Results for fully supervised ac-
tion segmentation setup on the Breakfast
dataset.

Similarly for the Hollywood extended dataset [4] we apply FIFA to Mu-
Con [37] and report the results in Table 4. FIFA applied on MuCon achieves
a 4 times faster inference speed while obtaining results comparable to exact
inference.

Fully Supervised Action Segmentation. In the fully supervised action segmen-
tation we apply FIFA on top of MS-TCN [1] and its variant MS-TCN++ [30]

0100200300400500Framewise negative log probabilities[ground truth][optimization initialization] MoF:0.5301masks[approximate inference] MoF:0.7505101520253035404550step60657075808590total energy12

Y. Souri et al.

on the Breakfast dataset [19] and report the results in Table 3. MS-TCN and
MS-TCN++ are approaches that do not perform any inference at test time. This
usually results in over-segmentation and low F1 and Edit scores. Applying FIFA
on top of these approaches improves the F1 and Edit scores signiﬁcantly. FIFA
applied on top of MS-TCN achieves state-of-the-art performance and sets new
state-of-the-art performance on most metrics.

For the Hollywood extended dataset [4], we train MS-TCN [1] and report
results comparing exact inference (EI) compared to FIFA in Table 5. We ob-
serve that MS-TCN using an inference algorithm achieves new state-of-the-art
results on this dataset. FIFA is comparable or better than exact inference on
this dataset.

Method

MoF-BG IoU Time (speedup)

ISBA [6]
D3TW [x]
CDFL [x]
MuCon [33]

12.6

19.5

34.5
33.6
40.6
41.6

-
-
-
-

13.9
13.7

40.1
41.2

53
13 (×4.1)

MuCon∗
MuCon + FIFA∗
Table 4. Results for weakly supervised
action segmentation on the Hollywood ex-
tended dataset. Time is reported in sec-
onds. ∗ indicates results obtained by run-
ning the code on our machine.

Method

MoF MoF-BG IoU IoD

HTK [19]
ED-TCN [6]
ISBA [6]

39.5
36.7
54.8

8.4
10.9 13.1
20.4 28.8

27.3
33.1

35.0
34.8

22.6 33.2
MSTCN [1] (+ EI) 64.9
MSTCN + FIFA 66.2
23.9 35.8
Table 5. Results for fully supervised ac-
tion segmentation on the Hollywood ex-
tended dataset. EI stands for Exact In-
ference.

5.4 Qualitative Example

A qualitative example of the FIFA optimization process is depicted in Figure 4.
For further qualitative examples, failure cases, and details please refer to the
supplementary material.

6 Conclusion

In this paper, we proposed FIFA a fast approximate inference procedure for ac-
tion segmentation and alignment. Unlike previous methods, the proposed method
does not rely on any expensive Viterbi decoding for inference. Instead, FIFA
optimizes a diﬀerentiable energy function that can be minimized using gradient-
descent which allows for a fast but also accurate inference during testing. We
evaluated FIFA on top of fully and weakly supervised methods trained on the
Breakfast dataset. The results show that FIFA is able to achieve comparable or
better performance, while being at least 5 time faster than exact inference.

FIFA: Fast Inference Approximation for Action Segmentation

13

References

1. Abu Farha, Y., Gall, J.: Ms-tcn: Multi-stage temporal convolutional network for

action segmentation. CVPR (2019)

2. Belanger, D., McCallum, A.: Structured prediction energy networks. In: ICML

(2016)

3. Belanger, D., Yang, B., McCallum, A.: End-to-end learning for structured predic-

tion energy networks. In: ICML (2017)

4. Bojanowski, P., Lajugie, R., Bach, F., Laptev, I., Ponce, J., Schmid, C., Sivic, J.:
Weakly supervised action labeling in videos under ordering constraints. In: ECCV
(2014)

5. Bojanowski, P., Lajugie, R., Bach, F., Laptev, I., Ponce, J., Schmid, C., Sivic, J.:
Weakly supervised action labeling in videos under ordering constraints. In: ECCV
(2014)

6. Chang, C., Huang, D., Sui, Y., Fei-Fei, L., Niebles, J.C.: D3TW: Discriminative
diﬀerentiable dynamic time warping for weakly supervised action alignment and
segmentation. CVPR (2019)

7. Ding, L., Xu, C.: Weakly-supervised action segmentation with iterative soft bound-

ary assignment. In: CVPR (2018)

8. Duchenne, O., Laptev, I., Sivic, J., Bach, F., Ponce, J.: Automatic annotation of

human actions in video. In: ICCV (2009)

9. Gatys, L.A., Ecker, A.S., Bethge, M.: A neural algorithm of artistic style. arXiv

preprint arXiv:1508.06576 (2015)

10. Ghoddoosian, R., Sayed, S., Athitsos, V.: Action duration prediction for segment-

level alignment of weakly-labeled videos. In: WACV (2021)

11. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial

examples. In: ICLR (2015)

12. Hoang, C.D.V., Haﬀari, G., Cohn, T.: Towards decoding as continuous optimiza-

tion in neural machine translation. In: EMNLP (2017)

13. Huang, D.A., Fei-Fei, L., Niebles, J.C.: Connectionist temporal modeling for weakly

supervised action labeling. In: ECCV (2016)

14. Huang, Y., Sugano, Y., Sato, Y.: Improving action segmentation via graph-based

temporal reasoning. In: CVPR (2020)

15. Ishikawa, Y., Kasai, S., Aoki, Y., Kataoka, H.: Alleviating over-segmentation errors

by detecting action boundaries. In: WACV (2021)

16. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and

super-resolution. In: ECCV (2016)

17. Karaman, S., Seidenari, L., Del Bimbo, A.: Fast saliency based pooling of ﬁsher

encoded dense trajectories. In: ECCV THUMOS Workshop (2014)

18. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR

(2015)

19. Kuehne, H., Arslan, A., Serre, T.: The language of actions: Recovering the syntax

and semantics of goal-directed human activities. In: CVPR (2014)

20. Kuehne, H., Gall, J., Serre, T.: An end-to-end generative framework for video

segmentation and recognition. In: WACV (2016)

21. Kuehne, H., Richard, A., Gall, J.: Weakly supervised learning of actions from

transcripts. CVIU (2017)

22. Kuehne, H., Richard, A., Gall, J.: Weakly supervised learning of actions from

transcripts. Computer Vision and Image Understanding 163, 78–89 (2017)

14

Y. Souri et al.

23. Kuehne, H., Richard, A., Gall, J.: A Hybrid RNN-HMM approach for weakly

supervised temporal action segmentation. PAMI 42(04), 765–779 (2020)

24. Le, Q., Mikolov, T.: Distributed representations of sentences and documents. In:

ICML (2014)

25. Lea, C., Flynn, M.D., Vidal, R., Reiter, A., Hager, G.D.: Temporal convolutional

networks for action segmentation and detection. In: CVPR (2017)

26. Lea, C., Reiter, A., Vidal, R., Hager, G.D.: Segmental spatiotemporal cnns for

ﬁne-grained action segmentation. In: ECCV (2016)

27. LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., Huang, F.: A tutorial on energy-

based learning. Predicting structured data 1 (2006)

28. Lei, P., Todorovic, S.: Temporal deformable residual networks for action segmen-

tation in videos. In: CVPR (2018)

29. Li, J., Lei, P., Todorovic, S.: Weakly supervised energy-based learning for action

segmentation. In: ICCV (2019)

30. Li, S., Abu Farha, Y., Liu, Y., Cheng, M.M., Gall, J.: MS-TCN++: Multi-stage

temporal convolutional network for action segmentation. PAMI (2020)

31. Moltisanti, D., Fidler, S., Damen, D.: Action recognition from single timestamp

supervision in untrimmed videos. In: CVPR (2019)

32. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T.,
Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z.,
Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.:
Pytorch: An imperative style, high-performance deep learning library. In: NeurIPS
(2019)

33. Pirsiavash, H., Ramanan, D.: Parsing videos of actions with segmental grammars.

In: CVPR (2014)

34. Richard, A., Kuehne, H., Gall, J.: Weakly supervised action learning with rnn

based ﬁne-to-coarse modeling. In: CVPR (2017)

35. Richard, A., Kuehne, H., Iqbal, A., Gall, J.: Neuralnetwork-viterbi: A framework

for weakly supervised video learning. In: CVPR (2018)

36. Rohrbach, M., Amin, S., Andriluka, M., Schiele, B.: A database for ﬁne grained

activity detection of cooking activities. In: CVPR (2012)

37. Souri, Y., Fayyaz, M., Minciullo, L., Francesca, G., Gall, J.: Fast weakly supervised

action segmentation using mutual consistency. In: arXiv (2019)

38. Souri, Y., Richard, A., Minciullo, L., Gall, J.: On evaluating weakly supervised

action segmentation methods. In: arXiv (2020)

39. Vo, N.N., Bobick, A.F.: From stochastic grammar to bayes network: Probabilistic

parsing of complex activity. In: CVPR (2014)

40. Wang, Z., Gao, Z., Wang, L., Li, Z., Wu, G.: Boundary-aware cascade networks

for temporal action segmentation. In: ECCV (2020)

FIFA: Fast Inference Approximation for Action Segmentation

15

A Detials of Exact Inference

The NNV approach [35] proposes an exact solution to the inference problem
using a Viterbi-like dynamic programming method. This dynamic programming
approach was later adopted by CDFL [29] and MuCon [37]. First an auxiliary
function Q(t, (cid:96), n) is deﬁned that yields the best probability score for a segmen-
tation up to frame t satisfying the following conditions:

– the length of the last segment is (cid:96),
– the last segment was the nth segment with label cn.

The function Q can be computed recursively. The following two cases are distin-
guished. The ﬁrst case deﬁnes when no new segment is hypothesized, i.e (cid:96) > 1.
Then,

Q(t, (cid:96), n) = Q(t − 1, (cid:96) − 1, n) · p(cn|xt),

(19)

with the current frame probability being multiplied with the value of the aux-
iliary function at the previous frame. The second case is a new segment being
hypothesized at frame t, i.e. (cid:96) = 1. Then,

Q(t, (cid:96) = 1, n) =

(cid:26)

max
ˆ(cid:96)

Q(t − 1, ˆ(cid:96), n − 1) · p(cn|xt) · p(ˆ(cid:96)|cn−1))

(cid:27)

,

(20)

where the optimization being calculated over all possible previous segments with
length ˆ(cid:96) and label cn−1. Here the probability of the previous segment having
length ˆ(cid:96) and label cn−1 is being multiplied to the previous value of the auxiliary
function.

The most likely alignment is given by

(cid:26)

max
(cid:96)

Q(T, (cid:96), N ) · p((cid:96)|cN )

.

(cid:27)

(21)

The optimal lengths can be obtained by keeping track of the maximizing argu-
ments ˆ(cid:96) from (20).

B Time Complexity Comparison

B.1 Time Complexity of Exact Inference

The time complexity of the above exact inference is quadratic in the length
of the video T and linear in the number of segments N . As input videos for
action segmentation are usually long, it becomes computationally expensive to
calculate. In practice, [35,29,37] limit the maximum size of each segment to a
ﬁxed value of L = 2000. The ﬁnal time complexity of exact inference is O(LN T ).
Furthermore, this optimization process is inherently not parallelizable. This is
due to the max operation in (20). Experiments have shown [37,38] that this
inference stage is the main computational bottleneck of action segmentation
approaches.

16

Y. Souri et al.

B.2 Time Complexity of FIFA

At each optimization step, the time complexity is O(N T ), where N is the number
of segments and T is the length of the video because we must create the M ∗
matrix and calculate the element-wise multiplication. Overall, the FIFA time
complexity is O(M N T ), where M is the number of optimization steps. Compared
to the exact inference which has a time complexity of O(LN T ), where L is the
ﬁxed value of 2000, our time complexity is lower since M is usually 50 steps and
N is on average 10.

We also want to mention that the proposed approach is inherently a paral-
lelizable optimization method (i.e. values of the mask, the element-wise multipli-
cation, and the calculation of the gradient for each time step can be calculated
in parallel) and is independent of any other time step values. This is in contrast
to the dynamic programming approaches where the intermediate optimization
values for each time step depend on the value of the previous time steps.

C Details of the Datasets

The Breakfast dataset [19] is the most popular and largest dataset typically
used for action segmentation. It contains more than 1.7k videos of diﬀerent
cooking activities. The dataset consists of 48 diﬀerent ﬁne-grained actions. In
our experiments, we follow the 4 train/test splits provided with the dataset and
report the average.

The Hollywood extended dataset [4] contains 937 videos taken from Hol-
lywood movies. The videos contain 16 diﬀerent action classes. We follow the
train/test split strategy of [7,35,29].

The main performance metrics used for weakly supervised action segmenta-
tion and alignment are the same as the previous approaches. The input features
are also kept the same depending on the approach we use FIFA with.

D Implementation Details

We implement our approach using the PyTorch[32] library. For all experiments
we set the number of FIFA’s gradient-based optimization steps to 50 and we
use the Adam [18] optimizer. Mask sharpness and the optimization learning rate
is chosen depending on the approach that FIFA is applied on top of. When
applying FIFA on top of MuCon [37] we use 0.3 as the learning rate and set the
mask sharpness to 1.75. For CDFL [29], we set the mask sharpness to 0.1 and
the learning rate to 0.15. Looking at the visualization in Figure G.9 it is clear
that CDFL provides noisy framewize probability estimates. For this reason a
lower mask sharpness is prefered. When applying FIFA on top of fully supervised
approaches like MS-TCN [1] we use mask sharpness value of 15 and learning rate
of 0.02. Looking at the visualization in Figure G.15 we see that fully supervised
approaches provide clean smooth framewise probabilities and having a sharp
mask is recommented in these settings.

FIFA: Fast Inference Approximation for Action Segmentation

17

Num. Steps MoF MoF-BG IoU IoD Time (min)

No inference

45.4

44.7

37.3 51.2

2 steps
5 steps
10 steps
30 steps
50 steps
60 steps

47.9
49.1
50.1
51.2
51.3
51.3

47.1
48.3
49.4
50.6
50.7
50.7

39.8 53.0
40.0 52.8
40.2 52.9
41.0 53.2
41.1 53.3
41.1 53.3

1.0

1.2
1.5
2.0
4.2
6.5
7.7

Exact Inference 50.7

50.3
Table E.1. Impact of the number of optimization steps for FIFA+MuCon for weakly
supervised action segmentation on the Breakfast dataset.

40.9 54.0

32.85

E Ablation Experiments

E.1 Number of Optimization Steps

In Table E.1 we report the results for weakly supervised action segmentation
on the Breakfast dataset [19] using the MuCon[37] approach. The proposed
approach achieves the best performance after 50 steps with 5.9% improvement
on the MoF accuracy compared to not performing any inference. Moreover, it is
more than 5 times faster than the exact inference.

E.2 Optimizer and Its Learning Rate

The choice of the optimizer used to update the length estimates using the calcu-
lated gradients is one of the hyper-parameters of our approach. We have exper-
imented with two optimizers SGD and Adam. As shown in Figure E.1, the best
performing value for the learning rate hyper-parameter depends on the optimizer
used. For SGD a low value of 0.001 achieves the best performance with higher
values causing major drops in performance. On the other hand, Adam optimizer
works well with a range of learning rate values as it has an internal mechanism
to adjust the learning rate. The best performance for Adam is observed at 0.3.
We further investigate and notice that the reason SGD performs so poorly for
large values of the learning rate is that it ﬂuctuates and is not able to optimize
the energy eﬀectively. Figure E.2 shows the value of the approximate energy
during the optimization for Adam and SGD for the same inference. We observe
that a large learning rate causes SGD to ﬂuctuate while Adam is stable and
achieves a lower energy value at the end of the optimization.

F Weakly Supervised Action Alignment

Similar to weakly supervised action segmentation, we apply FIFA on top of
CDFL and MuCon for weakly supervised action alignment task on the Breakfast

18

Y. Souri et al.

Fig. E.1. Eﬀect of the learning rate on the performance of weakly supervised action
segmentation using FIFA applied on the MuCon approach. Accuracy is calculated on
the Breakfast dataset.

dataset and report the results in Table F.2. Our experiments show that FIFA
applied on top of CDFL achieves state-of-the-art or better than state-of-the-art
results on MoF and Mof-BG metrics, whereas FIFA applied on top of MuCon
achieves state-of-the-art results for IoD and IoU metrics.

G Qualitative Examples

In this section we show various qualitative results of applying FIFA for action
segmentation. In each ﬁgure on the right, the approximate total energy value is
plotted as a function of number of steps. On the left, at the top, the framewise
negative log probabilities (P ) are visualized. The ground truth segmentation,

Method

MoF MoF-BG IoU IoD

ISBA [7]
D3TW [6]
CDFL [29]
ADP [10]

53.5
57.0
63.0
64.1

FIFA + CDFL∗ 65.3
FIFA + MuCon∗ 61.4

51.7
-
61.4
65.5

64.3
61.2

-

35.3 52.3
56.3
45.8 63.9
43.0

-

46.3 61.3
48.4 64.1

Table F.2. Results for weakly supervised action alignment on the Breakfast dataset.

0.0010.010.020.10.20.5Learning rate3035404550AccuracyAdamSGDFIFA: Fast Inference Approximation for Action Segmentation

19

Fig. E.2. The value of the approximate energy during FIFA optimization for SGD and
Adam optimizer for the same inference.

optimization initialization, the generated masks and the segmentation obtained
after approximate inference using FIFA is visualized in rows 2 to 5. The MoF
metric is also calculated for a single video and reported for the optimization
initialization and the approximate decoding.

An animation form of the same ﬁgures is also provided in the supplementary

material as a single video ﬁle.

Figures G.3-G.7 show qualitative examples of applying FIFA on top of Mu-
Con [37] for weakly supervised action segmentation. Figures G.9-G.13 show qual-
itative examples of applying FIFA on top of CDFL [29] for weakly supervised
action segmentation. Figures G.15-G.19 show qualitative examples of applying
FIFA on top of MSTCN [1] for fully supervised action segmentation.

G.1 Failure Cases

Figures G.8, G.14 and, G.20 show failure cases for FIFA + MuCon, FIFA +
CDFL and, FIFA + MSTCN respectively. We observe that the major failure case
is when the optimization is initialized with an incorrect transcript (Figures G.8
and G.14). Another failure mode is when the predicted negative log probabilities
are not correct (Figure G.20) which causes the boundaries of actions to be in
the wrong location.

01020304050step406080100120total energyAdam01020304050step406080100120SGD20

Y. Souri et al.

Fig. G.3. Qualitativ Result: Weakly supervised action segmentation, FIFA + MuCon

Fig. G.4. Qualitativ Result: Weakly supervised action segmentation, FIFA + MuCon

Fig. G.5. Qualitativ Result: Weakly supervised action segmentation, FIFA + MuCon

050100150200250300350400Framewise negative log probabilities[ground truth][optimization initialization] MoF:0.7101masks[approximate inference] MoF:0.7805101520253035404550step505560657075total energy02004006008001000120014001600Framewise negative log probabilities[ground truth][optimization initialization] MoF:0.6401masks[approximate inference] MoF:0.7405101520253035404550step150200250300total energy0100200300400500600700Framewise negative log probabilities[ground truth][optimization initialization] MoF:0.7001masks[approximate inference] MoF:0.8505101520253035404550step35.037.540.042.545.047.550.0total energyFIFA: Fast Inference Approximation for Action Segmentation

21

Fig. G.6. Qualitativ Result: Weakly supervised action segmentation, FIFA + MuCon

Fig. G.7. Qualitativ Result: Weakly supervised action segmentation, FIFA + MuCon

Fig. G.8. Qualitativ Result: Weakly supervised action segmentation, FIFA + MuCon,
Failure Case

0200400600800100012001400Framewise negative log probabilities[ground truth][optimization initialization] MoF:0.5701masks[approximate inference] MoF:0.7905101520253035404550step708090100110120130140total energy0200400600800Framewise negative log probabilities[ground truth][optimization initialization] MoF:0.5701masks[approximate inference] MoF:0.7605101520253035404550step100120140160total energy05001000150020002500Framewise negative log probabilities[ground truth][optimization initialization] MoF:0.0201masks[approximate inference] MoF:0.0505101520253035404550step160180200220240260280300total energy22

Y. Souri et al.

Fig. G.9. Qualitativ Result: Weakly supervised action segmentation, FIFA + CDFL

Fig. G.10. Qualitativ Result: Weakly supervised action segmentation, FIFA + CDFL

Fig. G.11. Qualitativ Result: Weakly supervised action segmentation, FIFA + CDFL

050010001500200025003000Framewise negative log probabilities[ground truth][optimization initialization] MoF:0.6201masks[approximate decoding] MoF:0.6905101520253035404550step1060010700108001090011000111001120011300total energy0200400600800100012001400Framewise negative log probabilities[ground truth][optimization initialization] MoF:0.7201masks[approximate decoding] MoF:0.7305101520253035404550step425043004350440044504500total energy0500100015002000Framewise negative log probabilities[ground truth][optimization initialization] MoF:0.6601masks[approximate decoding] MoF:0.7505101520253035404550step7400760078008000820084008600total energyFIFA: Fast Inference Approximation for Action Segmentation

23

Fig. G.12. Qualitativ Result: Weakly supervised action segmentation, FIFA + CDFL

Fig. G.13. Qualitativ Result: Weakly supervised action segmentation, FIFA + CDFL

Fig. G.14. Qualitativ Result: Weakly supervised action segmentation, FIFA + CDFL,
Failure Case

020040060080010001200Framewise negative log probabilities[ground truth][optimization initialization] MoF:0.5001masks[approximate decoding] MoF:0.7805101520253035404550step360038004000420044004600total energy020040060080010001200Framewise negative log probabilities[ground truth][optimization initialization] MoF:0.7501masks[approximate decoding] MoF:0.8405101520253035404550step2700280029003000total energy0200400600800100012001400Framewise negative log probabilities[ground truth][optimization initialization] MoF:0.0501masks[approximate decoding] MoF:0.1405101520253035404550step4800490050005100520053005400total energy24

Y. Souri et al.

Fig. G.15. Qualitativ Result: Fully supervised action segmentation, FIFA + MSTCN

Fig. G.16. Qualitativ Result: Fully supervised action segmentation, FIFA + MSTCN

Fig. G.17. Qualitativ Result: Fully supervised action segmentation, FIFA + MSTCN

0200400600800100012001400Framewise negative log probabilities[ground truth][optimization initialization] MoF:0.6601masks[approximate decoding] MoF:0.7905101520253035404550step400450500550600650total energy050100150200250300Framewise negative log probabilities[ground truth][optimization initialization] MoF:0.7201masks[approximate decoding] MoF:0.8105101520253035404550step400450500550600650700750total energy050100150200250300350400Framewise negative log probabilities[ground truth][optimization initialization] MoF:0.7201masks[approximate decoding] MoF:0.9605101520253035404550step300400500600700total energyFIFA: Fast Inference Approximation for Action Segmentation

25

Fig. G.18. Qualitativ Result: Fully supervised action segmentation, FIFA + MSTCN

Fig. G.19. Qualitativ Result: Fully supervised action segmentation, FIFA + MSTCN

Fig. G.20. Qualitativ Result: Fully supervised action segmentation, FIFA + MSTCN,
Failure Case

0100200300400500600700Framewise negative log probabilities[ground truth][optimization initialization] MoF:0.7501masks[approximate decoding] MoF:0.9605101520253035404550step20040060080010001200total energy050010001500200025003000Framewise negative log probabilities[ground truth][optimization initialization] MoF:0.8001masks[approximate decoding] MoF:0.8605101520253035404550step1250150017502000225025002750total energy0100200300400500600700800Framewise negative log probabilities[ground truth][optimization initialization] MoF:0.4801masks[approximate decoding] MoF:0.4005101520253035404550step2004006008001000total energy