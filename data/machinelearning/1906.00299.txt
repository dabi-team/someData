9
1
0
2

n
u
J

6

]

G
L
.
s
c
[

3
v
9
9
2
0
0
.
6
0
9
1
:
v
i
X
r
a

Quantitative Overﬁtting Management for Human-in-the-loop
ML Application Development with ease.ml/meter
Towards Data Management for Statistical Generalization

Frances Ann Hubis† Wentao Wu‡
†ETH Zurich

Ce Zhang†
‡Microsoft Research, Redmond

{hubisf, ce.zhang}@inf.ethz.ch wentao.wu@microsoft.com

June 7, 2019

Abstract

Simplifying machine learning (ML) application development, including distributed computation, pro-
gramming interface, resource management, model selection, etc, has attracted intensive interests recently.
These research efforts have signiﬁcantly improved the efﬁciency and the degree of automation of developing
ML models.

In this paper, we take a ﬁrst step in an orthogonal direction towards automated quality management for
human-in-the-loop ML application development. We build ease.ml/meter, a system that can automatically
detect and measure the degree of overﬁtting during the whole lifecycle of ML application development.
ease.ml/meter returns overﬁtting signals with strong probabilistic guarantees, based on which developers
In particular, ease.ml/meter provides principled guidelines to simple yet
can take appropriate actions.
nontrivial questions regarding desired validation and test data sizes, which are among commonest questions
raised by developers. The fact that ML application development is typically a continuous procedure further
worsens the situation: The validation and test data sets can lose their statistical power quickly due to
multiple accesses, especially in the presence of adaptive analysis. ease.ml/meter addresses these challenges
by leveraging a collection of novel techniques and optimizations, resulting in practically tractable data sizes
without compromising the probabilistic guarantees. We present the design and implementation details of
ease.ml/meter, as well as detailed theoretical analysis and empirical evaluation of its effectiveness.

1 Introduction

The fast advancement of machine learning technologies has triggered tremendous interest in their adoption
in a large range of applications, both in science and business. Developing robust machine learning (ML) so-
lutions to real-world, mission-critical applications, however, is challenging. It is a continuous procedure that
requires many iterations of training, tuning, validating, and testing various machine learning models before
a good one can be found, which is tedious, time-consuming, and error-prone. This dilemma has inspired
lots of recent work towards simplifying ML application development, covering aspects such as distributed
computation [14, 16], resource management [15, 27], AutoML [2, 11], etc.

During the last couple of years, we have been working together with many developers, most of whom are
not computer science experts, in building a range of scientiﬁc and business applications [4, 8, 24, 25, 28, 32]
and in the meanwhile, observe the challenges that they are facing. On the positive side, recent research on
efﬁcient model training and AutoML deﬁnitely improves their productivity signiﬁcantly. However, as training
machine learning model becomes less of an issue, new challenges arise — in the iterative development
process of an ML model, users are left with a powerful tool but not enough principled guidelines regarding
many design decisions that cannot be automated by AutoML.

1

 
 
 
 
 
 
Figure 1: Interaction with ease.ml/meter.

(Motivating Example) In this paper, we focus on two of the most commonly asked questions from our users1
—

(Q1) How large does my validation set need to be?

(Q2) How large does my test set need to be?

Both questions essentially ask about the generalization property of these two datasets — they need to be large
enough such that they form a representative sample of the (unknown) underlying true distribution. However,
giving meaningful answers to these two questions (rather than answers like “as large as possible” or “maybe
a million”) is not an easy task. The reasons are three fold. First, the answers depend on the error tolerance
of the ML application — a mission-critical application deﬁnitely needs a larger validation/test set. Second,
the answers depend on the history of how these data were used — when a dataset (especially the validation
set) is used multiple times, it loses its statistical power and as a result its size relies on the set of all historical
operations ever conducted. Third, the answers need to be practically feasible/affordable — simply applying
standard concentration inequalities can lead to answers that require millions of (or more) labels that may be
intractable.

A Data Management System for Generalization In this paper, we present ease.ml/meter, a system that
takes the ﬁrst step in (not completely) tackling the above challenges. Speciﬁcally, ease.ml/meter is a data
management system designed to manage the statistical power of the validation and test data sets. Figure 1
illustrates the functionality of ease.ml/meter, which interacts with its user in the following way:

1. The user inputs a set of parameters specifying the error tolerance of an ML application (deﬁned and

scoped later).

2. The system returns |Dval| and |Dtest|, the required sizes of the validation and test sets that can satisfy

the error tolerance.

3. The user provides a validation set and a test set.

4. The user queries the validation and test set. The system returns the answer that satisﬁes the user-
deﬁned error tolerance in Step 1. The system monitors and constrains the usage of the given validation
set and test set.

1As an anecdotal note, THE most popularly asked question is actually “how large does my training set need to be?”

2

Gap betweendev & testGap betweentest& realYou really have to stop.Hmm, you need a new dev set. I wouldn’t bet my money on it.Enjoy, with a li<le grain of salt.Yay! Overfitting is under control.Commit a new ML modelReceive SignalValidation SetError AnalysisAbstract Insights0%10%20%30%40%50%5. When the system discovers that the validation set or test set loses its statistical power, it asks the user

for another validation or test set of size |Dval| or |Dtest|, respectively.

(Scope) In this paper, we focus on a very speciﬁc (yet typical) special case of the above generic interaction
framework — the user has full access to the validation set. She starts from the current machine learning
model, Hi, conducts error analysis by looking at the error that Hi is making on the validation set, and
proposes a new machine learning model Hi+1 (e.g., by adding new feature extractors, trying a different ML
model, or adding more training examples). However, as the validation set is open to the user, after many
development iterations the user’s decision might start to overﬁt to this speciﬁc validation set. Meanwhile,
although the test set is hidden from the user, the signals returned by the system inevitably carries some
information about the test set as well. As a result, the user’s decision might also overﬁt to this particular test
set, which undermines its plausibility as a delegate of the underlying true data distribution. The goal of our
system is to (1) inform the user whenever she needs to use a new validation set for error analysis, and (2) inform
the user whenever she needs to use a new test set to measure validation set’s overﬁtting behavior.

(System Overview) In the above workload, the “error tolerance” (cid:15)tot is the generialization power of the
current validation set. Speciﬁcally, let Dtest ∼ Dtest be the test set and Dval be the validation set. Let H
be a machine learning model and l(H, −) returns the loss of H on Dval, Dtest, and Dtest, respectively.2 We
assume that the user hopes to be alerted whenever

|l(H, Dval) − l(H, Dtest)| > (cid:15)tot.

One critical design decision in ease.ml/meter is to decompose the LHS of the above inequality into two
terms:

1. Empirical Overﬁtting: |l(H, Dval) − l(H, Dtest)|;

2. Distributional Overﬁtting: |l(H, Dtest) − l(H, Dtest))|.

The rationale behind this decomposition is that it separates the roles of the validation set and the test set
when overﬁtting occurs — when the empirical overﬁtting term is large, the validation set “diverges from” the
test set, and therefore, the system should ask for a new validation set; when the distributional overﬁtting
term is large, the test set “diverges from” the true distribution, and therefore, the system should ask for a
new test set. Moreover, empirical overﬁtting can be directly computed, whereas distributional overﬁtting has
to be estimated using nontrivial techniques, as we will see.

ease.ml/meter provides a “meter” to communicate the current level of empirical overﬁtting and distri-
butional overﬁtting to the user. For each model Hi the user developed, the system returns one out of ﬁve
possible signals as illustrated in Figure 1 — the solid color bar represents the range of the empirical overﬁt-
ting, and the gray color bar represents the upper bound of distributional overﬁtting. The user decides whether
to replace a validation set according to the signals returned by the system, and the system asks for a new test set
whenever it cannot guarantee that the distributional overﬁtting is smaller than a user-deﬁned upper bound, (cid:15).

Technical Challenges and Contributions The key technical challenge in building ease.ml/meter is to
estimate distributional overﬁtting, which is nontrivial due to the fact that subsequent versions of the ML
application (i.e., Hi) can be dependent on prior versions (i.e., H1, ..., Hi−1). As was demonstrated by recent
work [6], such kind of adaptivity can accelerate the degradation of test set, fading its statistical power
quickly. To accommodate dependency between successive versions of the ML application, one may have to
use a larger test set (compared with the case where all versions are independent of each other) to provide
the same guarantee on distributional overﬁtting.

C1. The ﬁrst technical contribution of this work is to adapt techniques developed recently by the theory and
ML community on adaptive statistical querying [5, 6] to our scenario. Although the underlying method (i.e.,

2l(H, Dtest) represents the expected loss over the true distribution, which is unknown.

3

bounding the description length) is the same, it is the ﬁrst time that such techniques are applied to enabling
a scenario that is similar to what ease.ml/meter tries to support. Compared with the naive approach
that draws a new test set for each new version of the application, the amount of test samples required by
ease.ml/meter can be an order of magnitude smaller, which makes it more practical.

C2. The second technical contribution of this work is a set of simple, but novel optimizations that further
reduce the amount of test samples required. These optimizations go beyond traditional adaptive analytics
techniques [5, 6] by taking into consideration different speciﬁc operation modes that ease.ml/meter pro-
vides to its user, namely (1) non-uniform error tolerance, (2) multi-tenant isolation, (3) non-adversarial
developers, and (4) “time travel.” Each of these techniques focuses on one speciﬁc application scenario of
ease.ml/meter, and can further reduce the (expected) size of the test set signiﬁcantly.

Relationship with Previous Work The most similar work to ease.ml/meter is our recent paper
ease.ml/ci [22]. ci is a “continuous integration” system designed for the ML development process —
given a new model provided by the user, ci checks whether certain statistical property holds (e.g., the new
model is better than the old model, tested with ((cid:15), δ)). At a very high level, meter shares a similar goal as
ci, however, from the technical perspective, is signiﬁcantly more difﬁcult to build, for two reasons. First,
ease.ml/meter cannot use the properties of the continuous integration process (e.g., the new model will
not change too much) to optimize for the sample complexity. As we will see, to achieve practical sample
complexity, meter relies on a completely different set of optimizations. Second, ease.ml/meter needs to
support multiple signals, instead of a binary pass/fail signal as in ci. As a result, we see ci and meter fall
into the same “conceptual umbrella” of data management for statistical generalization, but focus on different
scenarios and thus require different technical optimizations and system designs.

Limitations We believe that ease.ml/meter is an “innovative system” that provides functionalities that
we have not seen in current ML ecosystems. Although ease.ml/meter works reasonably well for our tar-
get workload in this paper, it has several limitations/assumptions that require future investment. First,
ease.ml/meter requires new test data points as well as their labels from developers. Although recent work
on automated labeling (e.g., Snorkel [20]) alleviates the dependency on human labor for generating labeled
training data points, it does not address our problem as we require labels for test data rather than training
data. Second, the question of what actions should be taken upon receiving overﬁtting signals is left to devel-
opers. A speciﬁc reaction strategy can lead to a more speciﬁc type of adaptive analysis that may have further
impact on reducing the size of the test set. Moreover, while this work focuses on monitoring overﬁtting,
there are other aspects regarding quality control in ML application development. For instance, one may wish
to ensure that there is no performance regression, i.e., the next version of the ML application must improve
over the current version [22]. We believe that quality control and lifecycle management in ML application
development is a promising area that deserves further research.

2 Preliminaries

The core of ease.ml/meter is based on the theory of answering adaptive statistical queries [6].
In this
section, we start by introducing the traditional data model in supervised machine learning, and the existing
theory of supporting adaptive statistical queries which will serve as the baseline that we will compare with in
ease.ml/meter. As we will see in Section 5, with a collection of simple, but novel optimization techniques,
we are able to signiﬁcantly bring down the requirement of the number of human labels, sometimes by several
orders of magnitude.

2.1 Training, Validation, and Testing

In the current version of ease.ml/meter, we focus on the supervised machine learning setting, which consists
of three data distribution: (1) the training distribution Dtrain, (2) the validation distribution Dval, and (3)

4

the test distribution Dtest, each of which deﬁnes a probability distribution PD over (x, y), where x ∈ Rd is
the feature vector of dimension d and y ∈ R is the label.

(Application Scenarios) In traditional supervised learning setting, one often assumes that all three distri-
butions are the same, i.e., Dtrain = Dval = Dtest. In ease.ml/meter, we intentionally distinguish between
these three distributions to incorporate two emerging scenarios that we see from our users. First, in weakly
supervised learning paradigm such as data programming [21] or distant supervision [17], the training dis-
tribution Dtrain is a noisy version of the real distribution. As a result Dtrain (cid:54)= Dtest. The second example
is motivated by one anomaly detection application we built together with a telecommunication company,
in which the validation and training distributions are injected with anomalies and the real distribution is an
empirical distribution composed of/from real anomalies collected over history. As a result, Dval (cid:54)= Dtest.
The functionality provided by ease.ml/meter does not rely on the assumption that these three distributions
are the same.

(Sampling from Distribution) When building ML applications, in many, if not all, cases user does not have
access to the data distributions. Instead, user only has access to a ﬁnite set of samples from each distribution:
Dtrain = {(xt
i )} ∼ Dtest. As noted in the
introduction, one common question from our users is: How large does the training/validation/test set need to
be? The goal of ease.ml/meter is to provide one way of deciding the test set size |Dtest|, as well as when to
draw a new test set.

i )} ∼ Dval, and Dtest = {(xr

i)} ∼ Dtrain, Dval = {(xv

i , yv

i , yr

i, yt

2.2 Human-in-the-loop ML Development

An ML application is a function H : Rd (cid:55)→ R that maps a feature vector x to its predicted label f(x). Coming
up with this function is not a one-shot process, as indicated in previous work [13, 12, 30, 31, 33]. Instead, it
often involves human developers who (1) start from a baseline application H0, (2) conduct error analysis by
looking at the prediction of the current application Ht and summarize a taxonomy of errors the application
is making, and (3) try out a “ﬁx” to produce a new application Ht+1. A potential “ﬁx” could be (1) adding a
new feature, (2) using a different noise model of data, and (3) using a different model family, architecture,
or hyperparamter.

There are different frameworks of modeling human behavior. In this paper, we adopt one that is com-
monly used by previous work on answering adaptive statistical queries [5, 6, 9, 29]. Speciﬁcally, we assume
that the user, at step t, is a deterministic mapping U that maps the current application Ht into

We explain the parameterization of Ht in the following:

Ht+1 := U(Ht, Dval, Dtrain, g(Ht, Dtest), ξt)

1. The ﬁrst three parameters Ht, Dval, and Dtrain captures the scenario that the human developer has
full access to the training and validation sets, as well as the current version Ht of the ML application,
and can use them to develop the next version Ht+1 of the application.

2. The fourth parameter g(Ht, Dtest) captures the scenario in which the human developer only has lim-
ited access to the test set. Here g is a set function mapping from Ht and Dtest to a set of feedback
returned by ease.ml/meter to the developer. As a special case, if g(·, ·) = ∅, it models the scenario in
which the developer does not have access to the test set at all (i.e., does not have any feedback) during
development.

3. The ﬁfth parameter ξt models the “environment effect” that is orthogonal to the developer. ξt is a

variable that does not rely on past decisions, and is only a function of the step ID t.

When it is clear from the context that Dval, Dtrain, and ξt are given, we abbreviate the notation as

Ht+1 = U(Ht, g(Ht, Dtest)).

5

(Limitations and Assumptions) There are multiple limitations that are inherent to the above model of
human behaviours. Some we believe are ﬁxable with techniques similar to what we propose in this paper,
while others are more fundamental. The most fundamental assumption is that human decision does not have
In other words, Ht+1 is only a function of H0 and all past feedback signals
impact on the environment.
g(Ht, Dtest). There are also other potential extensions that one could develop. For example, instead of
treating human behavior as a deterministic function U, one can extend it to a class of deterministic functions
following some (known or unknown) probabilistic distribution.

2.3 Generalization and Test Set Size

The goal of ML is to learn a model over a ﬁnite sample that can be generalized to the underlying distribution
that the user does not have access to. In this paper, we focus on the following loss function l : Rd × R × R →
{0, 1}, (x, y, f(x)) (cid:55)→ l(x, y, H(x)). which maps each data point, along with its prediction, to either 0 or 1. (We
refer to this loss as “0-1 loss.”) We also focus on the following notion of “generialization” for a given model
H ([26]):

Pr

(cid:34)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

(cid:88)

Dtest

l(x, y, H(x)) − EDtest l(x, y, H(x))

> (cid:15)

< δ,

(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where (x, y) ∈ Dtest in the ﬁrst l(x, y, H(x)) and (x, y) ∼ Dtest in the second l(x, y, H(x)). Given an ML
model H, checking whether H generializes according to this deﬁnition is simple if one only uses the test set
Dtest once. In this case, one can simply apply Hoeffding’s inequality (Appendix A.1) to obtain:

|Dtest| ≥

ln 2/δ
2(cid:15)2 .

This provides a way of deciding the required number of samples in the test set. It becomes tricky, however,
when the test set is used multiple times, and the goal of ease.ml/meter is to automatically manage this
scenario and decrease the required size of Dtest.

2.4 Adaptive Analysis and Statistical Queries

In recent years, there is an emerging research ﬁeld regarding the so-called adaptive analysis or reusable
holdout [6] that focuses on ML scenarios where the test set Dtest can be accessed multiple times. In our
setting, consider T ML models H1, ... HT where

The goal is to make sure that

Ht+1 = U(Ht, g(Ht, Dtest)).

(cid:34)

Pr

∃t,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

(cid:88)

Dtest

l(x, y, ft(x)) − EDl(x, y, ft(x))

(cid:35)

> (cid:15)

< δ.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

When g(·, ·) is non-trivial, simply applying union bound and requiring a test set of size (see Appendix A.2.2
for details)

|Dtest| ≥

ln(2T/δ)
2(cid:15)2

(1)

does not provide the desired probabilistic guarantee because of the dependency between Ht and Ht+1. We
will discuss adaptive analysis in detail when we discuss the overﬁtting meter in Section 4.
(Baseline: Resampling) If a new test set D(t)
test is sampled from the distribution Dtest in each step t, to
make sure that with probability 1 − δ all T models return a generalized loss, one only needs to apply union

6

bound to make sure that each sampled test set generalizes with probability 1 − δ/T . Thus, to support T
adaptive steps, one needs a test set of size (see Appendix A.2.3 for details)

This gives us a simple baseline approach that provides the above generalization guarantee. Unfortunately, it
usually requires a huge amount of samples, as there is essentially no “reuse” of the test set.

|Dtest| ≥

T ln(2T/δ)
2(cid:15)2

.

(2)

3 System Design

We describe in detail (1) the interaction model between a user and ease.ml/meter, (2) different system
components of ease.ml/meter, and (3) the formalization of the guarantee that ease.ml/meter provides.
Last but not least, we provide a concrete example illustrating how ease.ml/meter would operate using real
ML development traces.

3.1 User Interactions

ease.ml/meter assumes that there are two types of users — (1) developers, who develop ML models, and
(2) labelers, who can provide labels for data points in the validation set or the test set. We assume that
the developers and labelers do not have ofﬂine communication that ease.ml/meter is not aware of (e.g.,
labelers cannot send developers the test set via email).

Access Control The separation between developers and labelers is to allow ease.ml/meter to manage the
access of data:

1. Developers have full access to the validation set;

2. Developers have no access to the test set (ease.ml/meter encrypts the test set and only labelers can

decrypt it);

3. Labelers have full access to the test set;

4. Labelers have full access to the validation set.

The rationale for the above protocol is that ease.ml/meter can measure the amount of information that
is “leaked” from the test set to the developers, which, as we will see, is the key to bounding the degree of
overﬁtting over the test set.

Interaction with Developers A meter (as illustrated in Figure 2) is speciﬁed by a set of triples

ri, ¯ri, (cid:15)i)}m
{(
¯

rj, ¯rj] = ∅.
ri, ¯ri] ∩ [
ri, ¯ri] = 1; ∀i (cid:54)= j, [
i=1 s.t. ∪i [
¯
¯
¯

ri, ¯ri, (cid:15)i) deﬁnes one of the m possible overﬁtting signals (e.g., m = 5 in Figure 1) returned by
Each (
¯
ease.ml/meter to the developer: (1) Each [
ri, ¯ri] speciﬁes the range of empirical overﬁtting that this signal
¯
covers (i.e., the solid color bar in Figure 2); and (2) (cid:15)i speciﬁes the upper bound of distributional overﬁtting
that this signal guarantees (i.e., the gray color bar in Figure 2).

We assume in ease.ml/meter that

ri < ¯ri =
¯

ri+1 < ¯ri+1, and (cid:15)1 ≤ (cid:15)2 ≤ ... ≤ (cid:15)m.
¯

The rationale behind the non-decreasing (cid:15)i is because of the intuition that when the empirical overﬁtting is
already quite large, the developer often cares less about small distributional overﬁtting.

7

Figure 2: An example meter deﬁned by four triples, i.e., four possible signals (see Section 3.1): Signal i is
deﬁned by (ri, ¯ri, (cid:15)i). If the system sends Signal 1 to the user, it means that the gap between the validation
set accuracy and the test set accuracy falls into [ri = 0, ¯ri = 0.05], while the test set accuracy is at most
(cid:15)1 = 0.01 away from the (unknown) accuracy on the real data distribution.

Interaction with Developers There are two phases when the developer interacts with ease.ml/meter,
initialization and model development:

1. Initialization Phase (user initiates). The developer initializes an ease.ml/meter session by specifying
the length of development cycle, T , the number of iterative development steps the developer hopes that
this session can support; the developer also provides the performance metric, l, a loss function (e.g.,
accuracy) whose output is bounded by [0, 1]. The developer further submits the current validation set
to ease.ml/meter.

2. Initialization Phase (system response). Given T , the meter returns |Dtest|, the number of examples
required to support T development steps from the developer. The system will then request |Dtest|
labels from the labeler.

The developer starts development after the initialization phase:

1. Model Development Phase (developer initiates).

The developer submits the new ML model to

ease.ml/meter.

2. Model Development Phase (system response). Given the model H, the system calculates the empirical
overﬁtting, |l(H, Dval) − l(H, Dtest)| (i.e., the gap between the losses over the validation and test sets)
and ﬁnds the response i s.t.

and returns the value i to the developer. In the meantime, the system guarantees that the distributional
overﬁtting, i.e.,

ri ≤ |l(H, Dval) − l(H, Dtest)| ≤ ¯ri
¯

|l(H, Dtest) − l(H, Dtest)|

is smaller than (cid:15)i, with (high) probability 1 − δ.

ri, ¯ri].
3. Model Development Phase (developer). The developer receives the response i and decodes it to [
¯
The developer then decides whether the empirical overﬁtting is too large. If so, she might choose to
collect a new validation set.

After T development cycles (i.e., after the developer has checked in T models) the system terminates.

The developer may then initiate a new ease.ml/meter session.

8

Signal 1Signal 2Signal 3Signal 4Empirical Overﬁtting: Gapbetween Val set and Test set Distributional Overﬁtting: Gap betweentest set and (unknown) test distributionOne example meter withfour possible signals00.10.20.30.4Interaction with Labelers Labelers are responsible for providing labeled test data. Whenever the preset
budget T is used up, that is, the developer has submitted T versions of the ML application, ease.ml/meter is-
sues a new request to the labeler to ask for a new, independent test set. The old test set can be replaced by
and released to the developer for development use.

3.2 Overﬁtting Signals

In standard ML settings, overﬁtting is connected with model training – not testing. This is due to the (im-
plicit) assumption that the test set will only be accessed once by the ML model. In the context of continuous
ML application development, this assumption is no longer valid and the test set is subject to overﬁtting as
well. The presence of adaptive analysis further accelerates the process towards overﬁtting. We now formally
deﬁne the semantics of the overﬁtting signals returned by ease.ml/meter. Without loss of generality, we
assume that (cid:15)1 = ... = (cid:15)m = (cid:15) and only discuss the case of them being different in Section 5.1.

3.2.1 Formalization of User Behavior

To formalize this notion of overﬁtting, we need precise characterization of the behavior of the developer:

1. The developer does not have access to Dtest.

2. At the beginning, the developer speciﬁes: (1) T , the number of development iterations; (2) (cid:15), the
tolerance of distributional overﬁtting (deﬁned later); and (3) 1 − δ, the conﬁdence. In response, the
system returns the required size |Dtest|.

3. At every single step t, the system returns to the user an indicator It ∈ {1, · · · , m} which is a function
of Dtest. As we will see, It = i indicates that degree of overﬁtting is bounded by [
ri − (cid:15), ¯ri + (cid:15)], with
¯
probability at least 1 − δ.

3.2.2 Formal Semantics of Overﬁtting

There could be various deﬁnitions and semantics of overﬁtting. It is not our goal to investigate all those
alternatives in this work, which is itself an interesting topic. Instead, we settle on the following deﬁnitions
that we believe are useful via our conversations with ML application developers.

“Distributional Overﬁtting” and “Empirical Overﬁtting”. Formally, let H be an ML application and l be
a performance measure (e.g., a loss function). For a given data set Dtest drawn i.i.d. from Dtest, we use
l(H, Dtest) to represent the performance of H over Dtest. We also use ED∼Dtest [l(H, D)] to represent the
expected performance of H over the distribution Dtest. We deﬁne the degree of overﬁtting of the validation set
as

OVFT(Dval, Dtest) = l(H, Dval) − ED∼Dtest [l(H, D)].

We decompose this term into two terms – the empirical difference between Dval and Dtest, i.e.,

∆H(Dval, Dtest) = l(H, Dval) − l(H, Dtest)

and the “quality” of estimator relying on Dtest, i.e.,

∆H(Dtest, Dtest) := l(H, Dtest) − ED∼Dtest [l(H, D)].

We call the ﬁrst term ∆H(Dval, Dtest) empirical overﬁtting as it is measured in terms of the empirical loss, and
call the second term ∆H(Dtest, Dtest) distributional overﬁtting as it measures the gap between the current
test set and the (unknown) true distribution. One crucial design decision we made in ease.ml/meter is to
decouple these two terms and only report the empirical overﬁtting to the user while treating distributional
overﬁtting as a hard error tolerance constraint.

9

(a) System X (SemEval 2019)

(b) System Y (SemEval 2018)

(c) System Y (SemEval 2019) for δ = 0.1 and (cid:15) = 0.01

Figure 3: (a, b) Real development history traces of two ML applications; (c) Signals that developers would
get when applying ease.ml/meter to these two development traces.

Distributional Overﬁtting (Overﬁtting). As the empirical overﬁtting term can be measured directly
by calculating the difference between the validation set and the test set, the technical challenge of
ease.ml/meter hinges on the measurement/control of distributional overﬁtting. In the rest of this paper,
we use the term “Dtest overﬁts by (cid:15)” to speciﬁcally refer to distributional overﬁtting. When the context of
Dtest and Dtest is clear, we use

∆H ≡ ∆H(Dtest, Dtest)

to denote distributional overﬁtting.

We want to measure ∆H not just for a single model H, but in the context of a series H0, ..., HT of models.

Deﬁnition 1. We say that Dtest overﬁts by (cid:15) with respect to a submission history H0, ..., HT and performance
measure l, if and only if ∃H ∈ H(T ), |∆H| > (cid:15). Here H(T ) = {H0, ..., HT }.

Intuitively, this guarantees that, as long as the test set Dtest does not overﬁt up to step t, all decisions
made by the developer are according to a test set that closely matches the real distribution – at least in terms
of some aggregated statistics (e.g., accuracy).

3.3 Example Use Cases

To illustrate how ease.ml/meter can be used in the development process of ML models, we use the devel-
opment trace data from two real-world ML applications we developed in the past and showcase the signals
ease.ml/meter would return to its user.

Development Trace 1: Emotion Detection in Text As our ﬁrst case study, we took the development history
of System X, which is a participant of the “EmoContext” task in SemEval 2019.3 This task aims for detecting
emotions from text leveraging contextual information, which is deemed challenging due to the lack of facial
expressions and voice modulations.4 It took developers eight iterations before delivering the ﬁnal version of
System X. Changes in each individual step include adding various word representations such as ELMo [19]
and GloVe [18], which lead to signiﬁcant performance increase/drop. Figure 3(a) plots the accuracy of
System X on the validation set and the test set (assuming that the accuracy on the test set was reported to
the user in every development step), respectively, in each of the eight development steps.

3https://www.humanizing-ai.com/emocontext.html
4https://competitions.codalab.org/competitions/19790

10

Development Trace 2: Relation Extraction Our second case study comes from System Y [23], which is
a participant of the task “Semantic Relation Extraction and Classiﬁcation in Scientiﬁc Papers” in SemEval
2018.5 This task aims for identifying concepts from scientiﬁc documents and recognizing the semantic rela-
tion that holds between the concepts. In particular, it requires semantic relation extraction and classiﬁcation
into six categories speciﬁc to scientiﬁc literature. The development history of System Y indicates that it
involves eight steps before reaching the ﬁnal (version of the) system. Figure 3(b) presents the accuracy of
System Y on the training set (using 5-fold cross validation) and the test set, respectively, for each step in the
development cycle.

Meter in Action Figure 3 illustrates the signals that developers would receive when applying
ease.ml/meter to these two development traces. At each step, the current empirical overﬁtting is visi-
ble while the distributional overﬁtting is guaranteed to be smaller than (cid:15) = 0.01 with probability 1 − δ = 0.9
(i.e., δ = 0.1).

Figure 3 also reveals two working modes of ease.ml/meter: (1) the regular meter and (2) the incremental
meter. The regular meter simply returns an overﬁtting signal for each submission that indicates its degree of
overﬁtting, as we have been discussing so far. However, this is often unnecessary in practice, as developers
usually only care about the maximum (or worst) degree of overﬁtting of all the models that have been
submitted. The rationale is that the tolerance for overﬁtting usually only depends on the application, not a
particular model — a submitted model is acceptable as long as its overﬁtting is below the application-wide
tolerance threshold. The incremental meter is designed for this observation.

It is worth mentioning some tradeoffs if developers choose to use the incremental meter. On the pos-
itive side, it can signiﬁcantly reduce the number of human labels, compared with the regular meter (see
Sections 4.3 and 4.4). For instance, for the particular setting here ((cid:15) = 0.01 and δ = 0.1), the incremental
meter would have required only 50K labels, compared with the 80K labels required by the regular meter, to
support T = 8 steps as in Figure 3(c). On the negative side, developers may lose clue on the performance
of an individual submission, if the incremental meter does not march — in such case developers only know
that this submission is better than the worst one in the history.

3.4 Discussion

One may wonder why not taking a more straightforward approach that bounds | OVFT(Dval, Dtest)| directly,
rather than the decomposition strategy ease.ml/meter uses. The rationale is that the former becomes much
more challenging, if not impossible, when the validation distribution drifts from the true distribution (i.e.,
Dval (cid:54)= Dreal). When there is no distribution drift, we can indeed apply the same techniques in Section 4
below to Dval (in lieu of Dreal) to derive a lower bound for |Dval|. However, given that the developer has
full access to Dval and Dval is often used for hyper-parameter tuning that involves lots of iterations (i.e.,
very large T ’s), the required |Dval| can easily blow up.

In fact, it is even not our goal to bound | OVFT(Dval, Dtest)|. Recall that, ease.ml/meter aims for
understanding sizes of both the validation set and the test set. Even if directly bounding | OVFT(Dval, Dtest)|
were possible, it would only give us an answer to the question of desired validation set size, and the question
about desired test set size remains unanswered. Our decision of decomposing OVFT(Dval, Dtest) is indeed
a design choice, not a compromise.
Instead of providing a speciﬁc number about the validation set size,
ease.ml/meter answers the question in probably the strongest sense: Pick whatever size — the validation
set size no longer matters! In practice, one can simply take a conservative, progressive approach: Start with
a validation set with moderate size, and let the meter tell the degree of overﬁtting (via explicit control
over the test set size); If the degree of overﬁtting exceeds the tolerance, replace the validation set (e.g., by
adding more samples). Therefore, our decomposition design is indeed a “two birds, one stone” approach
that simultaneously addresses the two concerns regarding both validation and test set sizes.

Given that we do not explicitly bound | OVFT(Dval, Dtest)|, one may raise the question about the seman-
tics of overﬁtting signals by ease.ml/meter, in terms of | OVFT(Dval, Dtest)|. When ease.ml/meter re-

5https://competitions.codalab.org/competitions/17422

11

Figure 4: An illustration of adaptive analysis. The performance of the tth model submission Ht({Xi}) on the
test set {Xi} is reported back to the developer indirectly via a feedback function g(·).

ri ≤
turns an overﬁtting signal i,
¯
|∆H(Dval, Dtest)| ≤ ¯ri and |∆H(Dtest, Dtest)| ≤ (cid:15), it follows that, with probability (i.e., conﬁdence) at
least 1 − δ,

ri, ¯ri] on the meter.
it indicates the corresponding range [
¯

Since

ri − (cid:15) ≤ | OVFT(Dval, Dtest)| ≤ ¯ri + (cid:15).
¯

Limitations and Assumptions Although the above setup is quite generic, a range of limitations remain.
One important limitation is that ease.ml/meter assumes that user is able to draw samples from each distribu-
tion at any time/step. This is not always true, especially in many medical-related applications and anomaly
detection applications in physical systems. Another limitation is that ease.ml/meter assumes that each data
distribution is stationary, i.e., all three distributions do not change or drift over time, though in many ap-
plications concept/domain drift is inevitable [7, 35]. We believe that these limitations are all interesting
directions to explore. However, as one of the early efforts on overﬁtting management, we leave these as
future work in this paper.

4 Monitoring Overﬁtting

We now present techniques in ease.ml/meter that monitor the degree of (distributional) overﬁtting. We
ﬁrst piggyback on techniques recently developed in adaptive analysis [5, 6] and apply them to our new
scenario. We have also developed multiple simple, but novel, optimizations, which we will discuss later in
Section 5. The main technical question we aim to answer is that, given the error tolerance parameters (cid:15), δ,
and the length of development cycles T , how large should the test set Dtest be?

4.1 Recap: Adaptive Analysis

As we have discussed in Section 2.4, we cannot simply draw a fresh test set for each new submission (i.e.,
the Resampling baseline), as it would become prohibitively expensive in practice for reasonable choices of (cid:15)
and δ in many circumstances. For instance, if we set (cid:15) = 0.01 and δ = 0.01, it would require 380K examples
to test just T = 10 models (by Equation 2).

To reduce this sample complexity, it is natural to consider reusing the same test set for subsequent submis-
sions. As one special case (which is unrealistic for ease.ml/meter), if all submissions are independent (i.e.,
the next submission does not depend on the overﬁtting signal returned by ease.ml/meter for the present
submission), then we can simply apply the union bound combined with the Hoeffding’s inequality to con-
clude a sample complexity as shown in Equation 1. Using the previous setting again ((cid:15) = 0.01, δ = 0.01, and
T = 10), we now need only 38K examples in the test set.

However, this independence assumption seldom holds in practice, as the developers would always receive
overﬁtting signals returned by ease.ml/meter, which in the worst case, would always have impact on her
choice of the next model (see Figure 4). It then implies that the models submitted by developers can be

12

i.i.d samplesX1X2...  Xn(test set) (encryption barrier)  H0 g(,{})H0Xig(U(,g(,{}),{})H0H0XiXiU(,g(,{})H0H0Xidependent. We now formally examine this kind of adaptive analysis during ML application development in
more detail, studying its impact on the size of the test set.

4.2 Cracking Model Dependency

The basic technique remains similar to when the submissions are independent: We can (1) apply Hoeffding’s
inequality to each submission and then (2) apply union bound to all possible submissions. While (1) is the
same as the independent case, (2) requires additional work as the set of all possible submissions expands
signiﬁcantly under adaptive analysis. We use a technique based on description length, which is similar to
those used by other adaptive analysis work [5, 6, 22].

Speciﬁcally, consider step t.

If the submission ft+1 is independent of ft, the number of all possible
submissions is simply T after T steps.
If, on the other hand, ft+1 depends on both ft and It (i.e., the
indicator returned by the meter, which speciﬁes the range i ∈ [m] that the degree of overﬁtting of ft falls
into), then for each different value of It we can have a different ft+1. To count the total number of possible
submissions, we can naturally use a tree T (m, T ) to capture dependencies between ft+1 and ft.

In more detail, the tree T (m, T ) contains T + 1 levels, where level t represents the corresponding step
Ht in the submission history H(T ) = {H0, ..., HT }.
In particular, the root represents H0. Each node at
level t represents a particular realization ft of Ht, i.e., a possible submission made by developers at step
t. Meanwhile, the children of ft represent all possible ft+1’s that are realizations of Ht+1 given that the
submission at step t is ft.

Example 1 (Dependency Tree). Figure 5(a) showcases the corresponding tree T (2, 3) for a regular meter when
m = 2 and T = 2. It contains T + 1 = 3 levels. The root represents H0, the initial submission. Since m = 2, the
meter contains two overﬁtting ranges and therefore can return one of the two possible overﬁtting signals, Signal
1 or Signal 2. Depending on which signal is returned for H0, developers may come up with different realizations
for H1. This is why the root has two children at level 1. The same reasoning applies to these two nodes at level 1
as well, which results in four nodes at level 2.

The problem of applying union bound to all possible submissions under adaptive analysis therefore boils
down to computing the size |T (m, T )| of the tree T (m, T ). We next analyze |T (m, T )| for the regular meter
and the incremental meter, respectively.

4.3 Regular Meter

In the regular meter, each signal can take m values ({1...m}) for a meter with m possible signals. One can
then easily see that, in general, the number of nodes in the (model) dependency tree T (m, T ) is |T (m, T )| =
(cid:80)
T
. This leads to the following result on sample complexity for the regular meter (the

t=1 mt = m(mT −1)

m−1

complete proof is in Appendix A.3.1):

Theorem 1 (Regular Meter). The test set size (in the adaptive setting) of the regular meter satisﬁes

2 · |T (m, T )| · exp (cid:0)−2|Dtest|(cid:15)2(cid:1) < δ,

where |T (m, T )| = m(mT −1)

m−1

. As a result, it follows that

|Dtest| >

ln (cid:0)2|T (m, T )|/δ(cid:1)
2(cid:15)2

T ln m + ln (cid:0)2m/((m − 1)δ)(cid:1)
2(cid:15)2

,

≈

(3)

by using the approximation mT − 1 ≈ mT .

(Comparison to Baseline) Compared to the Resampling baseline (Equation 2), the regular meter (with
m = 5) can reduce the number of test examples from 380K to 108K when (cid:15) = 0.01, δ = 0.01, and T = 10, a
3.5× improvement.

13

(a) Regular Meter

(b) Incremental Meter

Figure 5: Illustration of regular and incremental meters.

4.4 Incremental Meter

As we have discussed in Section 3.3, the incremental meter reports the worst degree of overﬁtting for all
models that have been submitted so far. Formally, at step t it returns

I(t) = max{Ik|1 ≤ k ≤ t, Ik ∈ {1, · · · , m}},

where Ik is the overﬁtting signal that would have been returned by the regular meter at step k. As a result,
the incremental meter can only move (indeed, increase) towards one direction. This constraint allows us to
further reduce the required amount of test examples, often signiﬁcantly (compared to the regular meter):

Theorem 2 (Incremental Meter). The test set size (in the adaptive setting) of the incremental meter satisﬁes
2 · |T (m, T )| · exp (cid:0)−2|Dtest|(cid:15)2(cid:1) < δ ,

where

As a result, it follows that

|T (m, T )| =

m(cid:88)

T(cid:88)

k

t

(cid:19)

(cid:18)k + t − 2
t − 1

=

(cid:19)

(cid:18)m + T
m

− 1.

|Dtest| >

ln(2|T (m, T )|/δ)
2(cid:15)2

=

ln(2 ·

(cid:17)
(cid:1) − 1

(cid:16)(cid:0)m+T
m
2(cid:15)2

/δ)

.

(4)

The proof is in Appendix A.3.2. 6 Compared to the regular meter, the size of the (model) dependency
tree can be further pruned. Figure 5(b) illustrates this for the incremental meter when m = 2 and T = 2:
Shadowed nodes are pruned with respect to the tree of the corresponding regular meter (Figure 5(a)).

(Comparison to Baseline) Compared to the Resampling baseline (Equation 2), the incremental meter (with
m = 5) can reduce the number of test examples from 380K to 66K ((cid:15) = 0.01, δ = 0.01, and T = 10), a 5.8×
improvement.

5 Optimizations

In the previous section, we adapt existing techniques directly to ease.ml/meter. However, these tech-
niques are developed for general adaptive analysis without considering the speciﬁc application scenario
that ease.ml/meter is designed for. We now describe a set of simple, but novel optimizations that further
decrease the requirement of labels by ease.ml/meter.

6The proof is quite engaged but the idea is simple: Count the number of tree nodes h(k, t) with respect to (1) the overﬁtting
T
t=1 h(k, t). We can further show that

signal k returned by ease.ml/meter and (2) the level t, and observe that |T (m, T )| =
h(k, t) = (cid:0)k+t−2
t−1

m
k=1

(cid:80)

(cid:80)

(cid:1).

14

5.1 Nonuniform Error Tolerance

Our ﬁrst observation is that a uniform error tolerance (cid:15) for all signals, as was assumed by all previous
work [5, 22], is perhaps an “overkill” – when empirical overﬁtting is large, user might have higher error
tolerance (e.g., when validation accuracy and test accuracy are already off by 20 points, user might not
need to control distributional overﬁtting to a single precision point). Our ﬁrst optimization is then to extend
existing result to support different (cid:15)k’s for different signals k ∈ [m], such that (cid:15)1 ≤ · · · ≤ (cid:15)m. This leads to
the following extensions of the results in Sections 4.3 and 4.4.

Corollary 1 (Nonuniform, Regular Meter). The test set size (in the adaptive setting) of the regular meter with
non-uniform (cid:15) satisﬁes

(cid:88)m

k=1

1
m

2 · |T (m, T )| · exp (cid:0)−2|Dtest|(cid:15)2

k

(cid:1) < δ,

(5)

where |T (m, T )| = m(mT −1)

m−1

remains the same as in Theorem 1.

The proof can be found in Appendix A.3.3. The basic idea is the same as that in Section 4.2: We can
use a tree T (m, T ) to capture the dependencies between historical submissions and count the number of
tree nodes (i.e., possible submissions). However, in the nonuniform-(cid:15) case, we have to count the number of
tree nodes for each individual overﬁtting signal k ∈ [m] separately, since we need to apply the Hoeffding’s
inequality for each group of nodes T (k)(m, T ) corresponding to a particular k, with respect to (cid:15)k. For the
|T (m, T )|.
regular meter, it turns out that |T (k)(m, T )| = 1
m

Corollary 2 (Nonuniform, Incremental Meter). The test set size (in the adaptive setting) of the incremental
meter with non-uniform (cid:15) satisﬁes

(cid:88)m

k=1

2 · |T (k)(m, T )| · exp (cid:0)−2|Dtest|(cid:15)2

k

(cid:1) < δ,

(6)

where

|T (k)(m, T )| =

T(cid:88)

t=1

(cid:19)

(cid:18)k + t − 2
t − 1

=

(cid:18)k + T − 1
k

(cid:19)
.

The previous remark on the proof of Corollary 1 can be applied to the nonuniform, incremental meter,
too: We can compute |T (k)(m, T )| and then apply the Hoeffding’s inequality with respect to (cid:15)k, for each
k ∈ [m] separately. The complete proof can be found in Appendix A.3.4. Note that the tree size |T (m, T )| =
(cid:80)

m
k=1

|T (k)(m, T )| is the same as that of Theorem 2.

Impact on Sample Complexity The difﬁculty of ﬁnding a closed form solution for non-uniform (cid:15) makes
it challenging to directly compare this optimization with those that we derived in Section 4. To better
understand sample complexity of nonuniform meters, in the following we conduct an analysis based on the
assumption that |Dtest| is dominated by (cid:15)1:7
(Sample Complexity assuming (cid:15)1-Dominance) Speciﬁcally, for the nonuniform, regular meter, we have

|Dtest| >

1
2(cid:15)2
1

(cid:16)

ln

2(mT − 1)
δ(m − 1)

(cid:17)

≈

T ln m + ln
2(cid:15)2
1

2
δ(m−1)

.

On the other hand, for the nonuniform, incremental meter, we have

|Dtest| >

1
2(cid:15)2
1

(cid:16)

ln

(cid:17)

.

2T
δ

(7)

(8)

7Given that (cid:15)1 ≤ · · · ≤ (cid:15)m and the exponential terms that enclose the (cid:15)k’s, one can expect that the LHS sides of Equations 5 and 6

are dominated by the terms that contain (cid:15)1.

15

Figure 6: Multitenancy in ease.ml/meter.

(Improvement over Section 4) We now compare the sample complexity of the nonuniform meters to their
uniform counterparts. The nonuniform, regular meter can reduce sample complexity to 100K when setting
((cid:15)1, (cid:15)2, (cid:15)3, (cid:15)4, (cid:15)5) to (0.01, 0.02, 0.03, 0.04, 0.05), δ = 0.01, and T = 10, compared to 108K with a uniform
error tolerance (cid:15) = 0.01 ≡ (cid:15)1. We do not observe signiﬁcant improvement, though. In fact, if we compare
Equation 7 with Equation 3, the improvement is upper bounded by 1 + ln m
ln(2/δ) . For m = 5 and δ = 0.01, it
means that the best improvement would be 1.3× regardless of T . Nonetheless, one can increase either m or
δ to boost the expected improvement. On the other hand, the nonuniform, incremental meter can further
reduce sample complexity from 66K to 38K (which matches Equation 1, the ideal sample complexity when
all submissions are independent), a 1.75× improvement.

5.2 Multitenancy

Our second observation is that, when multiple users are having access to the same meter, it is possible
to decrease the requirement on the number of examples if we assume that these users do not communicate
with each other. Multitenancy is a natural requirement in practice given that developing ML applications is
usually team work. We implemented a multitenancy management subsystem to enable concurrent access to
the meter from different users.

Figure 6 illustrates the speciﬁc multitenancy scenario targeted by ease.ml/meter. Unlike traditional
multitenancy setting where multiple users access system simultaneously, our multitenancy scenario captures
more of a collaboration pattern between two developers where one starts from the checkpoint made by the
other, similar to the well-known “git branch” and “git merge” development pattern (if we have to make
an analogy).

An interesting, perhaps counter-intuitive observation is that multitenancy can further reduce the desired
size of the test set. We illustrate this using a simple, two-tenant case where there are only two developers,
each working on T
2 steps. We provide the result for more than two developers in Appendix B. We focus on
nonuniform meters in our discussion, as uniform meters (i.e., with a single (cid:15)) can be viewed as special cases.
(Two Tenants in Nonuniform, Regular Meter) Consider the regular meter ﬁrst. Since each tenant has T
2
development cycles, it follows from Corollary 1 that, in the presence of two tenants,

4 · |T k(m, T/2)| · exp(−2|Dtest|(cid:15)2
k)

(9)

(cid:88)m

1
m
(cid:88)m

k=1

δ >

=

4 ·

mT/2 − 1
m − 1

· exp(−2|Dtest|(cid:15)2

k) .

k=1

(Two Tenants in Nonuniform, Incremental Meter) Similarly, for the incremental meter, it follows from
Corollary 2 that, in the presence of two tenants,

δ >

=

(cid:88)m

1
m
(cid:88)m

k=1

4 ·

k=1

4 · |T k(m, T/2)| · exp(−2|Dtest|(cid:15)2
k)
(cid:18)k + T
2 − 1
k

· exp(−2|Dtest|(cid:15)2

k) .

(cid:19)

(10)

Intuition This result may be counter-intuitive at a ﬁrst glance. One might wonder what the fundamen-
tal difference is between multiple developers and a single developer, given our two-tenancy setting. The

16

First 50% IssuesSecond 50% IssuesAuto"Merge"TODOs(Issues) ofMLapplications(a) Single Tenant

(b) Two Tenants

Figure 7: Comparison between single-/multi- tenancy.

observation here is that the second developer can forget about the “development history” made by the ﬁrst
developer, since it is irrelevant to her own development (see Figure 7 for an example when m = 2 and
T = 3).

Impact on Sample Complexity We can illustrate the impact of multi-tenancy by again analyzing sample
complexity under the (cid:15)1-dominance assumption:

(Sample Complexity Assuming (cid:15)1-Dominance) Speciﬁcally, for the nonuniform, regular meter, we have

|Dtest| >

1
2(cid:15)2
1

(cid:16)

ln

4(mT/2 − 1)
δ(m − 1)

(cid:17)

≈

T
2 ln m + ln
2(cid:15)2
1

4
δ(m−1)

.

On the other hand, for the nonuniform, incremental meter, we have

|Dtest| >

1
2(cid:15)2
1

(cid:16)

ln

(cid:17)

.

2T
δ

(11)

(12)

(Improvement over Section 5.1) We now compare the sample complexity of the two-tenancy meters to
the single-tenancy ones. The two-tenancy, regular meter can reduce the number of test examples from
100K to 71K when setting ((cid:15)1, ..., (cid:15)5) = (0.01, 0.02, 0.03, 0.04, 0.05), δ = 0.01, and T = 10, a 1.4×
improvement. One can indeed achieve asymptotically 2× improvement as T increases (see Section 6.2.2).
On the other hand, the two-tenancy, incremental meter cannot further improve the sample complexity. This
is not surprising, as Equation 12 and Equation 8 are exactly the same. In fact, both have matched Equation 1,
which represents the ideal sample complexity when all submissions are independent.

5.3 “Time Travel”

Our third observation is that it is a natural action for developers to revert to a previous step once an overﬁtting
signal is observed, i.e., “traveling back in timeline.” In practice, it makes little sense to revert to older steps
except for the latest one prior to the one that resulted in overﬁtting. Therefore, we only consider the case of
taking one step back, just like what the “git revert HEAD” command does. Intuitively, “time travel” permits
“regret” in development history. As a result, we can use a smaller test set to support the same number of
development steps. In the following, we present a formal analysis that veriﬁes this intuition.

Analysis For ease of exposition, we start by assuming one single budget for “time travel,” i.e., only one
reversion is allowed in development history. Again, we are interested in the total number of possible model
submissions. Suppose that user decides to revert at step t. One can decompose the entire procedure into
three phases: (1) keep submitting models until step t; (2) revert and go back to step t − 1; (3) continue

17

submitting T − t times until step T . For each (cid:15)k, the number of possible submissions in the three phases are
then (1) |T (k)(m, t)|, (2) −|T (k)(m, t − 1)|, and (3) |T (k)(m, (t − 1) + (T − t))| = |T (k)(m, T − 1)|.

It is straightforward to generalize the results to B ≥ 1 budgets. Suppose that user decides to revert at
steps t1 ≤ t2 ≤ · · · ≤ tB. Both phases (1) and (2) can repeat for each ti, though each ti should be replaced
by t (cid:48)
i = ti − (i − 1) to accommodate the “time shift” effect due to “time travel.” Phase (3) follows afterwards.
Therefore, the total number of possible submissions is
(cid:88)B

(cid:16)

i=1

|T (k)(m, t (cid:48)

i)| − |T (k)(m, t (cid:48)

(cid:17)
i − 1)|

+ |T (k)(m, T − B)|.

(“Time Travel” in Nonuniform, Regular Meter) By Corollary 1, the total number of possible submissions is

|R(k)| =

(mT −B − 1)
m − 1

+

(cid:88)B

i=1

mt (cid:48)

i−1.

The test set size therefore satisﬁes

(cid:88)m

k=1

2 · |R(k)| · exp (cid:0)−2|Dtest|(cid:15)2

k

(cid:1) < δ.

(13)

(14)

(“Time Travel” in Nonuniform, Incremental Meter) By Corollary 2, the total number of possible submis-
sions is

|I (k)| =

The test set size therefore satisﬁes

(cid:18)k + (T − B) − 1
k

(cid:19)

+

(cid:88)B

(cid:18)k + t (cid:48)

i − 2

(cid:19)

i=1

k − 1

(cid:88)m

k=1

2 · |I (k)| · exp (cid:0)−2|Dtest|(cid:15)2

k

(cid:1) < δ.

.

(15)

(16)

Impact on Sample Complexity As before, we illustrate the impact of “time travel” by analyzing sample
complexity under the (cid:15)1-dominance assumption.

(Sample Complexity Assuming (cid:15)1-Dominance) Speciﬁcally, for the nonuniform, regular meter, we have

|Dtest| >

1
2(cid:15)2
1

ln

(cid:16) 2
δ

·

(cid:16) mT −B − 1
m − 1

+

(cid:88)B

i=1

i−1(cid:17)(cid:17)

mt (cid:48)

.

(17)

On the other hand, for the nonuniform, incremental meter, regardless of the choice of B and t1 to tB, we
have

|Dtest| >

1
2(cid:15)2
1

ln

(cid:17)

(cid:16) 2T
δ

.

(18)

(Improvement over Section 5.1) We now compare the sample complexity of the “time travel” meters to the
ones in Section 5.1. We set B = 3 and (t1, t2, t3) = (1, 2, 3). The “time travel” regular meter can reduce
sample complexity from 100K to 76K when setting ((cid:15)1, ..., (cid:15)5) = (0.01, 0.02, 0.03, 0.04, 0.05), δ = 0.01,
and T = 10, a 1.32× improvement. We can further improve the sample complexity by increasing the “time
travel” budget — for T = 10 we can achieve an improvement up to 2.64× (see Section 6.2.3). We do not
see improvement for the incremental meters under the (cid:15)1-dominance assumption, though, as Equation 18
already matches the lower bound in Equation 1.

6 Experimental Evaluation

We report experimental evaluation results in this section. Our evaluation covers the following aspects of
ease.ml/meter:

18

• How effective are the regular meter and incremental meter in Section 4, compared with the Resam-

pling baseline in Section 2.4? — see Section 6.1.

• How effective are the optimization techniques in Section 5, compared with the basic regular meter and

incremental meter in Section 4? — see Section 6.2.

• How can ease.ml/meter be ﬁt into real ML application development lifecycle management? — see

Section 6.3.

We compare the participated techniques in terms of their induced sample complexity (i.e., the amount of
human labels required), under various parameter settings that are typical in practice.

6.1 Meters vs. Baselines

Baseline We have presented the Resampling baseline in Section 2.4 in the context of a single (cid:15), which can
only serve as a baseline for uniform meters. It is straightforward to extend it to the nonuniform case, though:
Given that (cid:15)1 ≤ · · · ≤ (cid:15)m, the required sample size in each single step is bounded by ln(2T/δ)/(2(cid:15)2
1). Hence,
the total number of samples ntot satisﬁes

|Dtest,tot| > T ·

ln(2T/δ)
2(cid:15)2
1

.

(19)

Computation of Sample Size for Nonuniform Meters Equations 5 and 6 are algebraically unsolvable for
the sample size n = |Dtest| unless m = 1. It is, however, possible to ﬁnd the correct n = |Dtest| by bounding
1 < n < ∞ and using binary search to ﬁnd the desired δ and n.

In our experiments, we choose δ from the set {0.001, 0.005, 0.01, 0.05, 0.1}, which con-
Parameter Settings
sists of common conﬁdence thresholds encountered in practice. We choose m from {5, 10, 20, 50} and vary T
from 10 to 100.

Evaluation Results We choose (cid:15) from {0.01, 0.05, 0.1}. Figure 8 compares the uniform meters with the base-
line approaches when setting m = 5, δ = {0.01, 0.05}, and (cid:15) = {0.01, 0.05}. In each of the four subﬁgures, we
plot the number of samples required by the baseline (“Resampling”), the regular meter (“Regular Meter”),
the incremental meter (“Incremental Meter”), and the ideal case where the submissions are independent
(“No Adaptivity”), respectively. Note that the y-axis is in log scale.

(Impact of T ) We have the following observations on the impact of T , regardless of the choices for δ and (cid:15):

• As T increases, the number of samples required by each participant approach increases.
• However, the speeds of growth differ dramatically: Both Regular Meter and Incremental Meter grow
much more slowly than Resampling, and moreover, Incremental Meter grows much more slowly than
Regular Meter. In fact, based on Equations 2, 3, and 4, we can show that the sample size required by
Resampling, Regular Meter, and Incremental Meter are O(T ln T ), O(T ), and O(ln T ), respectively,
with respect to T .8

• Although both Incremental Meter and No Adaptivity grow at the rate of O(ln T ), there is still visible

gap between them, which indicates opportunity for further improvement.

(Impact of δ) By comparing Figure 8(a) and Figure 8(b), where we keep (cid:15) = 0.05 unchanged but vary δ
from 0.05 to 0.01, we observe that the sample complexity only slightly increases (not quite noticible given
that the y-axis is in log scale). Comparing Figure 8(c) and Figure 8(d) leads to the same observation. This
is understandable, as the impact of δ on the sample complexity of all participant approaches is (the same)
O(ln 1

δ ).

(Impact of (cid:15)) On the other hand, the impact of (cid:15) on the sample complexity of all participant approaches
is much more signiﬁcant. This can be evidenced by comparing Figure 8(a) and Figure 8(c), where we keep

8We need to apply Stirling’s approximation (to Equation 4) to obtain the O(ln T ) result for Incremental Meter.

19

(a) δ = 0.05 and (cid:15) = 0.05

(b) δ = 0.01 and (cid:15) = 0.05

(c) δ = 0.05 and (cid:15) = 0.01

(d) δ = 0.01 and (cid:15) = 0.01

Figure 8: Comparison of uniform meters with baseline approaches when setting m = 5 and varying T from
10 to 100.

20

1E+31E+41E+51E+61E+7102030405060708090100Number of SamplesNumber of Steps TResamplingRegular MeterIncremental MeterNo Adaptivity1E+31E+41E+51E+61E+7102030405060708090100Number of SamplesNumber of Steps TResamplingRegular MeterIncremental MeterNo Adaptivity1E+31E+41E+51E+61E+7102030405060708090100Number of SamplesNumber of Steps TResamplingRegular MeterIncremental MeterNo Adaptivity1E+31E+41E+51E+61E+7102030405060708090100Number of SamplesNumber of Steps TResamplingRegular MeterIncremental MeterNo Adaptivity(a) m = 5

(b) m = 10

Figure 9: Comparison of uniform meters with different m’s when setting δ = 0.01 and (cid:15) = 0.01, and varying
T from 10 to 100.

δ = 0.05 but change (cid:15) from 0.05 to 0.01. As we can see, the sample complexity increases by around 25×!
This observation remains true if we compare Figure 8(b) and Figure 8(d). The rationale is simple – the
impact of (cid:15) on the sample complexity is (the same) O( 1

(cid:15)2 ) for all approaches.

(Impact of m) Figure 9 further compares the sample complexity of the uniform meters when ﬁxing δ = 0.01
and (cid:15) = 0.01. Figures 9(a) and 9(b) depict results when m = 5 and m = 10, respectively. (The y-axis is
in log scale.) We see that sample complexity increases for both Regular Meter and Incremental Meter. In
fact, we can show that both meters actually grow at the (same) rate O(ln m) with respect to m.9

6.2 Optimizations for Meters

We next evaluate the effectiveness of the optimization techniques presented in Section 5: (1) nonuniform
(cid:15)’s and (2) multitenancy.

6.2.1 Nonuniform Error Tolerance

Figure 10 compares nonuniform meters when setting ((cid:15)1, ..., (cid:15)5) = (0.01,0.02,0.03,0.04,0.05) with uniform
ones. For uniform meters and the two baselines Resampling and No Adaptivity, we set (cid:15) = (cid:15)1 = 0.01. We
observe the following regardless of δ:

• The nonuniform regular meter (“Regular, Nonuniform”) slightly improves over Regular Meter, but
not much. This is not surprising, though, if we compare Equation 7 with Equation 5 — for a given m,
the sample complexity of both meters grows at the rate O(T ).

• On the other hand, the nonuniform incremental meter (“Incremental, Nonuniform”) signiﬁcantly
improves over its uniform counterpart Incremental Meter. Again, we can verify this by comparing
Equation 8 with Equation 6 — the uniform version has sample complexity O(m ln T ) whereas the
nonuniform version has sample complexity O(ln T ).

• The sample complexity of the nonuniform incremental meter is close to that of No Adaptivity (i.e., the
ideal case). In fact, their sample complexity would be the same if we assume (cid:15)1-dominance for the
nonuniform incremental meter. To verify, compare Equation 8 with Equation 1.

In our experiments, we have tested other settings for ((cid:15)1, ..., (cid:15)5) and the observations remain valid.

9Again, we need to apply Stirling’s approximation (to Equation 4) to obtain the O(ln m) result for Incremental Meter.

21

1E+31E+41E+51E+61E+7102030405060708090100Number of SamplesNumber of Steps TResamplingRegular MeterIncremental MeterNo Adaptivity1E+31E+41E+51E+61E+7102030405060708090100Number of SamplesNumber of Steps TResamplingRegular MeterIncremental MeterNo Adaptivity(a) δ = 0.05

(b) δ = 0.01

Figure 10:
(0.01, 0.02, 0.03, 0.04, 0.05).

Comparison of nonuniform meters with uniform ones when setting ((cid:15)1, ..., (cid:15)5) =

Figure 11: Comparison of two-tenancy vs. single-tenancy meters, ((cid:15)1, ..., (cid:15)5) = (0.01,0.02,0.03,0.04,0.05)
and δ = 0.01.

6.2.2 Multitenancy

We next evaluate the sample complexity of the multitenancy setting (speciﬁcally, two-tenancy setting) pre-
sented in Section 5.2. We focus on nonuniform meters by setting ((cid:15)1, ..., (cid:15)5) = (0.01, 0.02, 0.03, 0.04, 0.05)
and δ = 0.01.

Figure 11 presents the results. We observe that the two-tenancy regular meter further improves signif-
In fact, it roughly improves by 2×, which can be veriﬁed by
icantly over its single-tenancy counterpart.
comparing Equation 11 with Equation 7. In contrast, the two-tenancy incremental meter does not improve
over the single-tenancy one. This is understandable, though, if we look at Equation 12 and Equation 8 —
they are just the same. In fact, both of them have matched the sample complexity of No Adaptivity (i.e.,
Equation 1), which represents the ideal case where all submissions are independent of each other.

6.2.3 “Time Travel”

We further evaluate the sample complexity of the “time travel” setting presented in Section 5.3 where users
are allowed to take reversions during development history. Again, we focus on nonuniform meters by setting
((cid:15)1, ..., (cid:15)5) = (0.01, 0.02, 0.03, 0.04, 0.05) and δ = 0.01. In our evaluation, we tested different settings for T

22

1E+41E+51E+61E+7102030405060708090100Number of SamplesNumber of Steps TResamplingRegular MeterRegular, NonuniformIncremental MeterIncremental, NonuniformNo Adaptivity1E+41E+51E+61E+7102030405060708090100Number of SamplesNumber of Steps TResamplingRegular MeterRegular, NonuniformIncremental MeterIncremental, NonuniformNo Adaptivity1E+41E+51E+61E+7102030405060708090100Number of SamplesNumber of Steps TResamplingRegular, Single-TenancyRegular, Two-TenancyIncremental, Single-TenancyIncremental, Two-TenancyNo Adaptivity(a) T = 5

(b) T = 10

Figure 12:
(0.01, 0.02, 0.03, 0.04, 0.05) and δ = 0.01.

Comparison of meters with and without

“time

travel,”

setting ((cid:15)1, ..., (cid:15)5) =

Figure 13:
tion 3.3.

Integration of ease.ml/meter into real-world ML application development lifecycles in Sec-

and B. For a given T , we varied the “time travel” budget B from 1 to T , and set (t1, ..., tB) = (1, ..., B).

Figure 12 summarizes the results when setting (1) T = 5; and (2) T = 10. We observe that the “time
travel” regular meter further improves signiﬁcantly over its normal counterpart. Moreover, the improvement
In fact, the sample complexity converges to its lower
increases as we increase the “time travel” budget.
bound – the (ideal) sample complexity of No Adaptivity (i.e., Equation 1) – as B increases to T . In contrast,
the “time travel” incremental meter does not improve over the normal one. This is understandable, though,
as both of them have already matched the lower bound.

6.3 Meter In Action: A Revisit

We now revisit the two ML applications presented in Section 3.3 and study the sample complexity if
ease.ml/meter were integrated into their development lifecycles, under various parameter settings. We
set T = 8 as both applications involve 8 development steps. For nonuniform meters, we further set
((cid:15)1, ..., (cid:15)5) = (0.01, 0.02, 0.03, 0.04, 0.05), and we set (cid:15) = (cid:15)1 = 0.01 for the other approaches. We vary δ
from 0.1 to 0.01, which corresponds to varying reliability/conﬁdence 1 − δ from 0.9 to 0.99.

Figure 13 summarizes the sample complexity of various approaches. As we can see, the Resampling
approach is perhaps too expensive for practical use, as it requires around 200K to 300K human labels even

23

1E+41E+51E+612345Number of Samples"Time Travel" BudgetResampleRegular, NonuniformRegular, Time-TravelIncremental, NonuniformIncremental, Time-TravelNo Adaptivity1E+41E+51E+612345678910Number of Samples"Time Travel" BudgetResamplingRegular, NonuniformRegular, Time-TravelIncremental, NonuniformIncremental, Time-TravelNo Adaptivity2E+42E+50.90.9250.950.9750.99Number of SamplesReliability (1 -δ)ResamplingRegular MeterRegular, NonuniformIncremental MeterIncremental, NonuniformNo Adaptivityfor a short development history with only 8 steps. All versions of ease.ml/meter can signiﬁcantly improve
sample complexity over Resampling, by 2.5× to 8×. Moreover, the incremental meters outperform the
regular meters, by 1.6× to 3×. The most signiﬁcant improvement comes from the nonuniform incremental
meter, whose sample complexity is close to that of the ideal case represented by No Adaptivity: For example,
it would require only 25K labels when setting reliability to 0.9; even when reliability increases to 0.99, it
would require just 37K labels. These observations are in line with what we have observed in previous
evaluations, and they demonstrate the practicality of integrating the meters into real-world ML application
development activities.

7 Related Work

AutoML Systems There is a ﬂurry of recent work on developing automatic machine learning (AutoML) sys-
tems that aims for alleviating development effort by providing “declarative” machine learning services. In a
typical AutoML system, users only need to upload their datasets and provide high-level speciﬁcations of their
machine learning tasks (e.g., schemata of inputs/outputs, task categories such as binary classiﬁcation/multi-
class classiﬁcation/regression, loss functions to be minimized, etc.), and the system can take over the rest via
automated pipeline execution (e.g., training/validating/testing), infrastructure support (e.g., resource allo-
cation, job scheduling), and performance-critical functionality such as model selection and hyperparameter
tuning. Examples of such systems include industrial efforts made by major cloud service providers such as
Amazon [1], Microsoft [3], and Google [2], as well as ones from the academia, such as the Northstar system
developed at MIT [11] and our own recent effort on the ease.ml service [10, 15, 34]. Our work in this
paper is in line with but orthogonal to these AutoML systems: While they have focused on automation (and
therefore efﬁciency) of the ML application development cycle itself, we seek an automated way of verifying
the efﬁcacy of the ML application developed.

Adaptive Analysis Recent theoretical work has revealed the increased risk of drawing false conclusion
(known as “false discovery” in the literature) via statistical methods, due to the presence of adaptive analy-
sis [6]. Moreover, it has been shown that achieving statistical validity is often computationally intractable in
adaptive settings [9, 29]. Intuitively, adaptive analysis makes it more likely to draw conclusions tied to the
speciﬁc data set used in a statistical study, rather than conclusions that can be generalized to the underlying
distribution that governs data generation. Our contribution in this paper can be viewed as an application
of this theoretical observation to a novel, important scenario emerging from the ﬁeld of ML application
development lifecycle management and quality control.

8 Conclusion

We have presented ease.ml/meter, an overﬁtting management system for modern, continuous ML applica-
tion development. We discussed its system architecture, design principles, as well as implementation details.
We focused on addressing the primary challenge that could prevent ease.ml/meter from being practical,
i.e., the sample complexity of the test set in the presence of adaptive analysis. We further proposed various
optimization techniques that can signiﬁcantly take down the required amount of test labels, by applying
recent developments from the theory community. Evaluation results demonstrate that ease.ml/meter can
outperform resampling-based approaches with test set sizes that are an order of magnitude smaller, while
providing the same, stringent probabilistic guarantees of the overﬁtting signals.

24

References

[1] Amazon sage maker. https://aws.amazon.com/blogs/aws/sagemaker-automatic-model-tuning/.

[2] Google cloud automl. https://cloud.google.com/automl/.

[3] Microsoft azure machine learning. https://azure.microsoft.com/en-us/blog/announcing-automated-

ml-capability-in-azure-machine-learning/.

[4] S. Ackermann et al. Using transfer learning to detect galaxy mergers. MNRAS, 2018.

[5] A. Blum and M. Hardt. The ladder: A reliable leaderboard for machine learning competitions.

In

International Conference on Machine Learning, pages 1006–1014, 2015.

[6] C. Dwork, V. Feldman, M. Hardt, T. Pitassi, O. Reingold, and A. L. Roth. Preserving statistical validity

in adaptive data analysis. In STOC, pages 117–126, 2015.

[7] J. Gama, P. Medas, G. Castillo, and P. P. Rodrigues. Learning with drift detection.

In SBIA, pages

286–295, 2004.

[8] I. Girardi et al. Patient risk assessment and warning symptom detection using deep attention-based

neural networks. LOUHI, 2018.

[9] M. Hardt and J. Ullman. Preventing false discovery in interactive data analysis is hard. In FOCS, 2014.

[10] B. Karlas, J. Liu, W. Wu, and C. Zhang. Ease.ml in action: Towards multi-tenant declarative learning

services. PVLDB, 11(12):2054–2057, 2018.

[11] T. Kraska. Northstar: An interactive data science system. PVLDB, 11(12):2150–2164, 2018.

[12] A. F. Lara. Continuous delivery for ml models. https://medium.com/onﬁdo-tech/continuous-delivery-

for-ml-models-c1f9283aa971.

[13] A. F. Lara. Continuous integration for ml projects. https://medium.com/onﬁdo-tech/continuous-

integration-for-ml-projects-e11bc1a4d34f.

[14] M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V. Josifovski, J. Long, E. J. Shekita, and B. Su.
Scaling distributed machine learning with the parameter server. In OSDI, pages 583–598, 2014.

[15] T. Li, J. Zhong, J. Liu, W. Wu, and C. Zhang. Ease.ml: Towards multi-tenant resource sharing for

machine learning workloads. PVLDB, 11(5):607–620, 2018.

[16] X. Meng, J. K. Bradley, B. Yavuz, E. R. Sparks, S. Venkataraman, D. Liu, J. Freeman, D. B. Tsai, M. Amde,
S. Owen, D. Xin, R. Xin, M. J. Franklin, R. Zadeh, M. Zaharia, and A. Talwalkar. Mllib: Machine learning
in apache spark. Journal of Machine Learning Research, 17:34:1–34:7, 2016.

[17] M. Mintz, S. Bills, R. Snow, and D. Jurafsky. Distant supervision for relation extraction without labeled

data. In ACL, pages 1003–1011, 2009.

[18] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In EMNLP,

pages 1532–1543, 2014.

[19] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep contextu-

alized word representations. In NAACL-HLT, pages 2227–2237, 2018.

[20] A. Ratner, S. H. Bach, H. R. Ehrenberg, J. A. Fries, S. Wu, and C. R´e. Snorkel: Rapid training data

creation with weak supervision. PVLDB, 11(3):269–282, 2017.

25

[21] A. J. Ratner, C. D. Sa, S. Wu, D. Selsam, and C. R´e. Data programming: Creating large training sets,

quickly. In NIPS, pages 3567–3575, 2016.

[22] C. Renggli, B. Karlas, B. Ding, F. Liu, K. Schawinski, W. Wu, and C. Zhang. Continuous integration
In SysML

of machine learning models with ease.ml/ci: Towards a rigorous yet practical treatment.
Conference, 2019.

[23] J. Rotsztejn, N. Hollenstein, and C. Zhang. Eth-ds3lab at semeval-2018 task 7: Effectively com-
bining recurrent and convolutional neural networks for relation classiﬁcation and extraction.
In
SemEval@NAACL-HLT, pages 689–696, 2018.

[24] K. Schawinski et al. Generative adversarial networks recover features in astrophysical images of galax-

ies beyond the deconvolution limit. MNRAS, 2017.

[25] K. Schawinski et al. Exploring galaxy evolution with generative models. Astronomy & Astrophysics,

2018.

[26] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algorithms.

Cambridge University Press, 2014.

[27] E. R. Sparks, A. Talwalkar, D. Haas, M. J. Franklin, M. I. Jordan, and T. Kraska. Automating model

search for large scale machine learning. In SoCC, pages 368–380, 2015.

[28] D. Stark et al. PSFGAN: a generative adversarial network system for separating quasar point sources

and host galaxy light. MNRAS, 2018.

[29] T. Steinke and J. Ullman. Interactive ﬁngerprinting codes and the hardness of preventing false discov-

ery. In COLT, pages 1588–1628, 2015.

[30] R. Stojnic. Continuous integration for machine learning. https://medium.com/@rstojnic/continuous-

integration-for-machine-learning-6893aa867002.

[31] R. Stojnic. Continuous integration for machine learning. https://www.reddit.com/r/MachineLearning/

comments/8bq5la/d continuous integration for machine learning/.

[32] M. Su et al. Generative adversarial networks as a tool to recover structural information from cryo-

electron microscopy data. BioRxiv, 2018.

[33] D. Tran. Continuous integration for data science. http://engineering.pivotal.io/post/continuous-

integration-for-data-science/.

[34] C. Zhang, W. Wu, and T. Li. An overreaction to the broken machine learning abstraction: The ease.ml

vision. In HILDA@SIGMOD 2017, pages 3:1–3:6, 2017.

[35] I. Zliobaite. Learning under concept drift: an overview. CoRR, abs/1010.4784, 2010.

26

A Theoretical Results

This section includes theoretical results referenced by the paper, the details of which have been omitted due
to space limitation.

A.1 Hoeffding’s Inequality

Throughout this paper, we use the following form of the Hoeffding’s inequality. (A more general form is in
Lemma 4.5 of [26].)

Theorem 3 (Hoeffding’s Inequality). For a bounded function 0 ≤ h(x) ≤ 1 on the i.i.d. random variables
x1, ..., xn ∈ X and for all (cid:15) > 0 we have

Pr

(cid:34)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

(cid:88)

i

h(xi) − E[h(x)]

(cid:35)

> (cid:15)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

< 2 exp (cid:0)−2n(cid:15)2(cid:1) .

(20)

A.2 Basics for Sample Complexity Analysis

Here we cover basics for sample complexity analysis. In particular, we present analysis for two basic cases:
(1) one single submission; and (2) multiple, independent submissions. For comparison purpose, we also
include analysis for the Resampling baseline.

A.2.1 The Case of One Single Model

Deﬁnition 2. We say that Dtest overﬁts by (cid:15) with respect to model H and performance measure l, if and only
if |∆H| > (cid:15).

We can simply leverage the Hoeffding’s inequality (Appendix A.1) to derive |Dtest|, assuming data points

in Dtest are i.i.d. samples from Dtest. Speciﬁcally,

Theorem 4. Suppose that we use a loss function l bounded by [0, 1]. Given a model H ∈ H, the required test set
size satisﬁes

Pr[|∆H| > (cid:15)] < 2 exp (cid:0)−2|Dtest|(cid:15)2(cid:1) < δ.

(21)

As a result, it follows that |Dtest| > ln(2/δ)
2(cid:15)2 .

Proof. Suppose that we use a loss function l bounded by [0, 1]. Given a model H ∈ H, we have

Pr

(cid:34)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

(cid:88)

xi∈Dtest

l(H, xi) − E[l(H, x)]

(cid:35)

> (cid:15)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

< 2 exp (cid:0)−2n(cid:15)2(cid:1) .

Since

and

it follows that

l(H, Dtest) =

1
n

(cid:88)

xi∈Dtest

l(H, xi)

l(H, Dtest) = E [l(H, x)] ,

Pr [|l(H, Dtest) − l(H, Dtest)| > (cid:15)] < 2 exp (cid:0)−2n(cid:15)2(cid:1) ,

where n = |Dtest|. Setting 2 exp (cid:0)−2n(cid:15)2(cid:1) < δ yields

This completes the proof of the theorem.

|Dtest| >

ln(2/δ)
2(cid:15)2

.

27

For example, if the user sets (cid:15) = 0.1 and δ = 0.05, then the test set needs at least 185 data examples.
On the other hand, if the user wishes a more ﬁne-grained overﬁtting indicator with higher conﬁdence, e.g.,
(cid:15) = 0.01 and δ = 0.01, then the size of the test set would blow up to 26,492.

A.2.2 The Case of Independent Models

Suppose that all versions submitted by the developer for testing are independent (i.e., the next submission
does not depend on the overﬁtting signal returned by ease.ml/meter for the present submission). Then
there are just T possible submissions in H(T ). By Deﬁnition 1, applying the union bound (combined with the
Hoeffding’s inequality) we obtain

Pr[∃H ∈ H(T ),

|∆H| > (cid:15)] < 2T exp (cid:0)−2|Dtest|(cid:15)2(cid:1) < δ,

where H = {H1, ..., HT }. The required |Dtest| is then simply

|Dtest| >

ln(2T/δ)
2(cid:15)2

.

For instance, if we set (cid:15) = 0.01 and δ = 0.01, we need 38K examples to test T = 10 (independent) submis-
sions.

A.2.3 The Resampling Baseline

Let the test set in the t-th submission be D(t)
we are interested in the total number of test examples

test, for 1 ≤ t ≤ T . We require |D(t)

test

| = · · · = |D(T )
test

| = n, and

|Dtest| = |D(t)
test

| + ... + |D(T )
test

| = T · n.

Again, applying the union bound (combined with the Hoeffding’s inequality) we obtain

Pr[∃H ∈ H(T ),

|∆H| > (cid:15)] < 2T exp (cid:0)−2n(cid:15)2(cid:1) < δ,

where H = {H1, ..., HT }. It then follows that

|Dtest| = T · n > T ·

ln(2T/δ)
2(cid:15)2

.

Use the same setting where (cid:15) = 0.01 and δ = 0.01, it would now require 380K examples to test just T = 10
models.

A.3 Proofs of Theorems

A.3.1 Proof of Theorem 1

Proof. Following the idea when proving the Ladder mechanism [5], let T be the tree that captures the
dependencies between submissions (in the adaptive setting). It then follows that
|∆H| > (cid:15)] < 2|T | exp (cid:0)−2|Dtest|(cid:15)2(cid:1) ,

Pr[∃H ∈ H(T ),

using the union bound over all submissions in T and applying the Hoeffding’s inequality for each submission.
For the uniform, regular meter, T is a perfect m-ary tree with T levels (except for the root that represents

H0), where each internal tree node has exactly m children. Therefore,

|T | =

T(cid:88)

t=1

mt =

m(mT − 1)
m − 1

= |T (m, T )|.

This completes the proof of the theorem.

28

A.3.2 Proof of Theorem 2

Proof. Again, the idea is to compute the tree size |T |. Due to additional constraints in the incremental meter,
|T | ≤ |T (m, T )| in Theorem 1. Speciﬁcally, we use h(k, t) to represent the number of tree nodes with value
k and depth t, for 1 ≤ k ≤ m and 1 ≤ t ≤ T . It then follows that

|T | =

(cid:88)m

(cid:88)T

k=1

t=1

h(k, t).

(22)

We now divide the whole proof procedure into two phases: (I) derive h(k, t) and (II) derive |T | based on
Equation 22.

I. Derive h(k, t). We have the following observations:

h(k, 1) = 1 =

h(k, 2) = k =

(cid:19)

(cid:19)

(cid:19)

(cid:19)

(cid:18)k − 1
0
(cid:18)k
1
(cid:18)k + 1
2
(cid:18)k + 2
3

,

,

,

,

∀k ∈ [m];

∀k ∈ [m];

∀k ∈ [m];

∀k ∈ [m].

h(k, 3) = k ·

h(k, 4) = k ·

(k + 1)
2

·

(k + 1)
2
(k + 2)
3

=

=

In general, using induction (on t) we can prove that

h(k, t) =

(cid:19)

(cid:18)k + t − 2
t − 1

.

(23)

To see this, suppose that h(k, t) = (cid:0)k+t−2
h(k, t + 1) =
by appending nodes with value k only behind nodes with values i ≤ k in T (t). As a result,

(cid:1) for all 1 ≤ t ≤ n. Now consider t = n + 1. Observe that
k
i=1 h(i, t) for all t ≥ 1, as at the step t + 1 we can grow T (t) – the snapshot of T at time t –

(cid:80)

t−1

k(cid:88)

h(k, n + 1) =

h(i, n) =

i=1

k(cid:88)

i=1

(cid:18)i + n − 2
n − 1

(cid:19)
.

(24)

We can get a closed-form solution to h(k, n + 1) by using the recursive deﬁnition of the binomial coefﬁcient:

(cid:19)

(cid:18)n
k

=

(cid:19)

(cid:18)n − 1
k − 1

+

(cid:18)n − 1
k

(cid:19)
.

(25)

Speciﬁcally, writing Equation 24 explicitly and further noticing that (cid:0)1+n−2

(cid:1) = 0, we obtain

n
(cid:18)k + n − 2
n − 1

(cid:19)

(24) =

=

(cid:19)

+

(cid:19)

(cid:18)1 + n − 2
n − 1

+ · · · +

(cid:18)1 + n − 2
n
(cid:18)k + n − 1
n

(cid:19)
.

This ﬁnishes the induction on proving Equation 23.
II. Derive |T |. By Equations 22 and 23 we have

|T | =

(cid:88)m

(cid:88)T

k=1

t=1

(cid:19)

(cid:18)k + t − 2
t − 1

.

29

Deﬁne

(cid:18)k + t − 2
t − 1
Using the same technique as in proving Equation 24, we can prove

|T (k)(m, T )| =

t=1

(cid:88)T

(cid:19)
.

Speciﬁcally, notice that (cid:0)k+2−2

(cid:1) = (cid:0)k+1−2

0

0

(cid:19)

=

(cid:18)k + T − 1
T − 1
(cid:1) = 1. It follows that
(cid:19)
(cid:18)k + 2 − 2
1
(cid:18)k + 2 − 2
1

+

+

(cid:19)

(cid:19)

(cid:19)

(cid:19)

(cid:18)k + T − 1
k

.

+ · · · +

+ · · · +

(cid:19)

(cid:19)

(cid:18)k + T − 2
T − 1
(cid:18)k + T − 2
T − 1

|T (k)(m, T )| =

(26) =

=

=

(cid:18)k + 1 − 2
0
(cid:18)k + 2 − 2
0
(cid:18)k + T − 1
T − 1

(cid:19)

.

(26)

(27)

Applying the same technique once again, we can show that
(cid:19)

|T | =

(cid:18)m + T
m

− 1.

(28)

Speciﬁcally, notice that (cid:0)1+T −1

0

S =

=

=

(cid:1) = 1. Let S = |T | + 1. We have

(cid:19)

(cid:18)k + T − 1
k

(cid:88)m

k=1
(cid:18)1 + T − 1
1

(cid:19)

+ · · · +

(cid:18)m + T − 1
m

(cid:19)

+

+

(cid:19)

(cid:19)

(cid:18)1 + T − 1
0
(cid:18)1 + T − 1
0
(cid:18)m + T
m

(cid:19)
.

This ﬁnishes the proof of Theorem 2.

A.3.3 Proof of Corollary 1

Proof. We need to count the number of tree nodes in T for each different k ∈ [m]. By symmetry, each k
contributes equally to the number of tree nodes. Therefore, the number of tree nodes for each k is simply
(the same) |T (m, T )|/m.

A.3.4 Proof of Corollary 2

Proof. Similarly, we count the number of tree nodes in T for each individual k ∈ [m]. Following the proof
of Theorem 2 in Appendix A.3.2, it is easy to see that this count is simply |T (k)(m, T )| = (cid:0)k+T −1
(cid:1), i.e.,
Equation 27.

k

B Multi-tenant Meter

We can extend the results in the two-tenant setting to l tenants. Under the assumption that every user
submits an equal number of models T

l we have
(cid:88)m

δ > l

δ > l

k=1

(cid:88)m

k=1

2 ·

2 ·

mT/l − 1
m − 1
(cid:18)k + T
l − 1
k

(cid:19)

· exp(−2|Dtest|(cid:15)2

k),

· exp(−2|Dtest|(cid:15)2
k)

30

(29)

(30)

for the regular and incremental meters, respectively. We can further generalize the results to an unequal
number of model submissions among tenants. Speﬁﬁcally, given tenants i = 1, ..., l, each of whom con-
tributes Ti models successively, we have

δ >

δ >

l(cid:88)

m(cid:88)

i=1
l(cid:88)

k=1
m(cid:88)

i=1

k=1

2 ·

mTi − 1
m − 1

· exp(−2|Dtest|(cid:15)2

k),

2 ·

(cid:18)k + Ti − 1
k

(cid:19)

· exp(−2|Dtest|(cid:15)2

k).

31

