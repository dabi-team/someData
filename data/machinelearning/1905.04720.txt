Rotation Invariant Householder Parameterization for Bayesian PCA

Rajbir S. Nirwan 1 Nils Bertschinger 1 2

9
1
0
2

y
a
M
2
1

]
L
M

.
t
a
t
s
[

1
v
0
2
7
4
0
.
5
0
9
1
:
v
i
X
r
a

Abstract

We consider probabilistic PCA and related fac-
tor models from a Bayesian perspective. These
models are in general not identiﬁable as the like-
lihood has a rotational symmetry. This gives rise
to complicated posterior distributions with contin-
uous subspaces of equal density and thus hinders
efﬁciency of inference as well as interpretation
of obtained parameters. In particular, posterior
averages over factor loadings become meaning-
less and only model predictions are unambigu-
ous. Here, we propose a parameterization based
on Householder transformations, which remove
the rotational symmetry of the posterior. Further-
more, by relying on results from random matrix
theory, we establish the parameter distribution
which leaves the model unchanged compared to
the original rotationally symmetric formulation.
In particular, we avoid the need to compute the
Jacobian determinant of the parameter transfor-
mation. This allows us to efﬁciently implement
probabilistic PCA in a rotation invariant fashion
in any state of the art toolbox. Here, we imple-
mented our model in the probabilistic program-
ming language Stan and illustrate it on several
examples.

1. Introduction

Modern algorithms and computational tools have vastly
expanded the scope of Bayesian modeling over the last
decades. In particular, Hamiltonian Monte-Carlo (HMC)
sampling (see Betancourt (2017) for a thorough introduc-
tion) and variational inference (Bishop, 2006) allow to ap-
proximate high-dimensional posterior distributions. In ad-
dition, software tools such as probabilistic programming

1Department of Computer Science, Goethe University,
Frankfurt, Germany 2Frankfurt Institute for Advanced Stud-
ies, Frankfurt, Germany.
Correspondence to: Rajbir S.
Nirwan <nirwan@ﬁas.uni-frankfurt.de>, Nils Bertschinger
<bertschinger@ﬁas.uni-frankfurt.de>.

Proceedings of the 36 th International Conference on Machine
Learning, Long Beach, California, PMLR 97, 2019. Copyright
2019 by the author(s).

languages, e.g. Stan (Carpenter et al., 2017), or libraries
for automatic differentiation, e.g. Tensorﬂow (Abadi et al.,
2015), simplify the implementation. Thanks to these ad-
vances, researchers can focus on the statistical modeling as
model implementation often requires just a few lines of code.
Nevertheless, some models remain challenging. Well known
examples include Probabilistic PCA (PPCA) and related
factor models which are widely used in exploratory and con-
ﬁrmatory data analysis. These models are non-identiﬁable,
which poses major problems when ﬁtted parameters, e.g.
factor loadings, are being interpreted. In this context, the
non-identiﬁability arises from a rotational symmetry of the
likelihood model. While identiﬁcation can be restored by
imposing constraints on the factor loadings (J¨oreskog, 1969;
Peeters, 2012), maximum likelihood estimation can be bi-
ased by the imposed constraints (Millsap, 2001).

Here, we consider PPCA from a Bayesian perspective. In
this context, the rotational symmetry gives rise to a compli-
cated posterior with continuous subspaces of equal density.
Continuous symmetries can severely reduce sampling ef-
ﬁciency as many samples are required in order to explore
the corresponding subspaces. Furthermore, interpretation
of the sampled parameters becomes impossible as marginal
posterior averages are meaningless when many different
parameter combinations give rise to the same likelihood.
This is akin to the discrete label switching symmetry ob-
served in mixture models. In this case, the parameters of the
individual mixture components cannot be averaged as the
arbitrary component labels between different samples could
be switched. Thus, a major advantage of Bayesian mod-
eling, namely the ability to access estimation uncertainty,
cannot be utilized as posterior means and variances not just
reﬂect variation within, but also between components.

Therefore, it is desirable to remove the rotational symmetry
of PPCA. Formally, the symmetry arises due to the choice
of coordinate system for the principal components being
arbitrary. In particular, a rotation of the coordinate axes
leaves the model likelihood unchanged. Mathematically,
the rotation symmetry is made explicit by formulating the
model in terms of an orthogonal matrix (transforming into
a ﬁxed coordinate system) and a diagonal matrix contain-
ing the explained variances. To this end, we employ the
singular value decomposition (SVD) and parameterize the
Stiefel manifold of orthogonal matrices in terms of non-

 
 
 
 
 
 
Rotation Invariant Householder Parameterization for Bayesian PCA

redundant unconstrained parameters. The mapping between
parameters and orthogonal matrices is provided in terms of
a sequence of Householder transformations.

removed via singular value decomposition (SVD). Finally,
we discuss some results from random matrix theory in order
to construct a suitable prior on the transformed parameters.

1.1. Related work

2.1. Probabilistic Principal Component Analysis

Special sampling methods for inference on statistical mani-
folds have been developed, in which the geometric structure
of the manifold is respected and geodesic trajectories are
traced out with respect to the metric of the manifold (Byrne
& Girolami, 2013). In general, the geometry will be non-
Euclidean, e.g. spherical for unit vectors. Also the geometry
of the Stiefel manifold of orthogonal matrices can be han-
dled in this fashion. Unfortunately, as the geometry needs
to be build into the sampling algorithm, this approach re-
quires substantive work. This also applies to approaches
using analytic results for the matrix von Mises-Fisher dis-
tribution on the Stiefel manifold. Exploiting conditional
conjugacy (Hoff, 2009) and ( ˇSm´ıdl & Quinn, 2007) have
employed this distribution to implement Gibbs sampling
and variational inference for Bayesian PPCA respectively.
An alternative, which allows to utilize general purpose tool
boxes, is to reparameterize the manifold in terms of uncon-
strained parameters1. Pourzanjani et al. (2017) achieved this
via Givens rotations. They also demonstrate that the model
can then be implemented in Stan without any changes to the
underlying sampling routine and unrestricted by conjugacy
requirements.

Ideally, reparameterizing a Bayesian model should not
change the joint density deﬁned by the model. In case of the
transformation via Givens rotations, the parameter density
corresponding to a uniform measure on the Stiefel mani-
fold is not known. Thus, by change of measure the density
needs to be corrected by the Jacobian determinant, which
poses a major computational bottleneck in high-dimensional
models. Here, we propose a different parameterization in
terms of Householder transformations. In this case, results
from random matrix theory allow us to obtain the induced
parameter density corresponding to a Gaussian prior on the
original parameters of PPCA, which include the rotational
symmetry. In section 2 we recall the necessary mathemati-
cal results to do so. Then, we illustrate the resulting model
on several examples (section 3 and 4) before we conclude
in section 5.

2. Background

Here, we quickly review PPCA and explain why the model
is rotationally symmetric, and how the symmetry can be

1In general, such a reparameterization cannot be achieved glob-
ally. Instead, the non-Euclidean geometry gives rise to singularities
in the parameter transformation, e.g. at the north pole when map-
ping a sphere onto a plane, or a different mapping, i.e. chart, needs
to be employed at different points.

(PPCA)

PPCA (Tipping & Bishop, 1999; Bishop, 2006) relates a D-
dimensional observation y to a Q-dimensional latent vector
x via a linear mapping W ∈ RD×Q

y = W x + µ + (cid:15),

(2.1)

where µ allows the model to have a non-zero mean and
(cid:15) states the variance not explained by the ﬁrst two terms
as noise. PPCA assumes a standard Gaussian distribu-
tion on the latent space, x ∼ N (0, I) and a zero mean
Gaussian noise with variance σ2 for all dimensions, i.e.
(cid:15) ∼ N (0, σ2I). The likelihood for y given W , µ and σ2
takes the form

p(y|W ) = N (µ, W W T + σ2I).

(2.2)

For N observations, denoted as Y = (y1, y2, ..., yN )T ∈
RN ×D, equation (2.1) becomes

Y = XW T + (cid:15),

(2.3)

where X = (x1, x2, ..., xN )T ∈ RN ×Q and (cid:15) ∈ RN ×D.
The likelihood of Y given W then becomes

p(Y |W ) =

N
(cid:89)

N (Y n,:|µ, W W T + σ2I),

ln p(Y |W ) =

n=1
N D
2

ln(2π) −

N
2

ln |K| −

1
2

tr(K−1 ˜Y ˜Y

T

),

(2.4)

where K = W W T + σ2I and ˜Y are the centered obser-
vations, i.e. ˜Y n,: = Y n,: − µ. This expression can be
maximized analytically and yields the solution (Tipping &
Bishop, 1999)

µML =

1
N

(cid:88)

n

Y n,:,

W ML = U (cid:0)Λ − σ2I(cid:1)1/2

R,

(2.5)

T

˜Y ˜Y

where U ∈ RD×Q is an orthogonal matrix containing the
principal eigenvectors of the empirical covariance matrix
1
, Λ ∈ RQ×Q is a diagonal matrix containing the
N
corresponding eigenvalues (λ1, ..., λQ) of 1
on the
N
diagonal and R ∈ RQ×Q is an arbitrary rotation matrix.
The maximum likelihood estimation for σ2 is given by

˜Y ˜Y

T

σ2
ML =

1
D − Q

D
(cid:88)

λi,

i=Q+1

(2.6)

Rotation Invariant Householder Parameterization for Bayesian PCA

which is basically the variance of Y not picked up by the
model. By projecting the data points y into the latent space,
a representation of reduced dimensionality Q < D can be
obtained. The posterior mean of the latent vectors is given
by (Bishop, 2006)

(cid:16)

E[x|y] =

W T

MLW ML + σ2I

(cid:17)−1

W T

ML(y − µML) .

(2.7)

T

Note that the rotation symmetry in the likelihood caused by
R in (2.5) makes the likelihood non-unique. Two different
solutions W and ˜W = W R will lead to the same likeli-
hood, since ˜W ˜W
= W RRT W T = W W T . For this
reason, numerical optimization algorithms often converge
to different results when started from different initial condi-
tions. While model predictions are unaffected by this non-
uniqueness, interpretation of parameters and the projected
latent vectors becomes impossible. Especially the latter is
problematic if PPCA is employed as a pre-processing step
to reduce the dimensionality of the data and further analysis
might be sensitive to the arbitrary choice of latent space ro-
tation. Figure 2.1 (left side) shows the maximum likelihood
result for 1000 different initial conditions for W . One can
clearly see the rotation symmetry.

PPCA is closely related to classical PCA. In particular, as
shown by Tipping & Bishop (1999), the classical solution is
recovered by letting σ2 → 0. In this case, the dimensionality
reduction is achieved by the linear projection

x = Λ− 1

2 U T (y − µML)

(2.8)

where µML, Λ and U are deﬁned as in (2.5). The eigen-
values λq are interpreted as the variance explained by the
q-th principal component U :,q in this context. From (2.7)
the classical solution is obtained by assuming vanishing
noise variance σ2 and ﬁxing the latent space rotation of the
maximum likelihood solution at R = I.

Factor analysis is closely related to PPCA as well. In this
case, PPCA is slightly generalized by assuming that the
noise is distributed as (cid:15) ∼ N (0, Ψ) with diagonal covari-
ance matrix Ψ. In particular, the likelihood has the same
rotational symmetry as PPCA. Here, we just note that all
our derivations in the following sections apply equally to
this model.

Figure 2.1. Data are created by sampling 50 5-dimensional points
from the standard normal distribution and map them via a random
linear mapping W ∈ R5×2 to the data-space Y . Elements of
W are drawn from a standard normal distribution. Left side:
Maximum likelihood solution for 1000 runs with different initial
conditions of W . Right side: 4000 samples from the posterior
distribution. Different colors show different rows of W .

a rotation invariant prior p(W ) (e.g. an isotropic Gaus-
sian) the posterior will have the same rotation symmetry as
well. Thus, the posterior has a complicated shape with a
continuous subspace of equal density posing a considerable
challenge for sampling algorithms.

Figure 2.1 (right side) shows 4000 posterior samples. The
likelihood is given by (2.4) and the prior on W is a standard
normal. We can clearly see the rotation symmetry again.
Samples were drawn using the NUTS sampler of Stan (Car-
penter et al., 2017), which is able to fully explore the chal-
lenging posterior. Yet, in higher dimensions D > Q (cid:29) 1
the problem becomes more severe and a lot of samples are
required to fully explore the symmetric solutions. Further-
more, statistics of the posterior samples, e.g. computing the
mean or variance, do not have any signiﬁcance in this case.

The question arises: Can we somehow take out the rotation
symmetry without changing the model (i.e. likelihood or
the mass of the posterior distribution)? The answer is yes:
We can achieve this by reparametrizing W . That way the
results are identiﬁed uniquely. The aim is not to change
the model by the reparameterization. Therefore, we need
speciﬁc corresponding distributions for the parameters on
the reparameterized model. In the next sections we explain
how to do that.

2.2. Bayesian Approach to PPCA

2.3. Singular Value Decomposition

The fully Bayesian way to solve (2.4) would be to impose
prior distributions p(µ, W , σ2) and solve for the posterior

p(µ, W , σ2|Y ) =

p(Y |µ, W , σ2)p(µ, W , σ2)
p(Y )

,

(2.9)

It is well known, that every positive-deﬁnite matrix S
can be diagonalized by an orthogonal transformation U
as S = U ΛU T , where Λ contains the eigenvalues of S on
the diagonal and the columns of U are the corresponding
normalized eigenvectors.

which is not tractable anymore. Therefore, we have to re-
sort to other techniques, e.g. sampling the posterior. For

This is not true anymore for other matrices, but there ex-
ists a more general decomposition called the singular value

−2−1012W1−2−1012W2−2−1012W1−2−1012W2Rotation Invariant Householder Parameterization for Bayesian PCA

decomposition (SVD). A matrix A ∈ RD×Q can be decom-
posed as

A = U ΣV T ,
(2.10)
where U = (u1, ..., uD)T , V = (v1, ..., vQ) are orthogo-
nal matrices and Σ = diag(σ1, ..., σQ) ∈ RD×Q is a diago-
nal matrix containing the singular values. When A has rank
P < Q, only P of them will be non-zero. It is easy to show
that the square of the diagonal elements (σ2
P ) are the
non-zero eigenvalues of AAT and AT A, which are similar
matrices, and all uj’s and vj’s obey the relation

1, ..., σ2

Avj = σjuj, AT uj = σjvj, ∀ j = 1, ..., P
Avj = 0,

AT uj = 0,

∀ j > P

(2.11)

We will decompose our mapping W ∈ RD×Q via SVD as
W = U ΣV T . The likelihood only depends on the outer
product of W . Since the outer product does not depend on
V ,

W W T = U ΣV T (U ΣV T )T = U Σ2U T ,

(2.12)

we can neglect V 2, i.e., we assume it to be the identity
matrix. That ﬁxes the coordinate frame of the latent space
and takes out the rotation symmetry.

By reparameterizing W by U and Σ, we obtain a unique
representation3. However, in the fully Bayesian case, we
have a prior on W . After reparameterization by U and Σ,
we have to make a Jacobian adjustment to the probability
density, since otherwise the prior density mass on W will
change as well. In the next sections, we consider a zero
mean Gaussian prior on W , and how to adjust the prior on
U and Σ, such that we do not change the distribution on
W .

Similarly, SVD provides a numerically stable way for clas-
sical PCA. In this case, the data matrix Y is decomposed
as Y = U P CAΣP CAV T
P CA and the desired dimensional-
ity reduction is achieved by the linear map ΣP CAU T
P CA.
Again, we see that the matrix of right singular vectors
V P CA, corresponding to an arbitrary choice of latent coor-
dinate rotation, is dropped.

2.4. Random Matrix Theory, Haar Measure

SVD decomposes our W into an orthogonal matrix U , a
diagonal matrix Σ and another orthogonal matrix V , that
we are neglecting in our analysis, since the outer product
W W T does not depend on V .

2Note, V is precisely the rotation symmetry that we want to

get rid of.

For a standard Gaussian prior on W , i.e. all elements of W
are zero mean Gaussian with unit variance, W W T has a
Wishart distribution and the following theorem holds (James
& Lee, 2014):

Theorem 1 Let the entries of W ∈ RD×Q be i.i.d. Gaus-
sian with zero mean and unit variance. The joint probability
density of the ordered strictly positive eigenvalues of the
Wishart matrix W T W , λ1 ≥ ... ≥ λQ, equals

p(λ) = ce− 1

2

(cid:80)Q

q=1 λq

Q
(cid:89)

q=1



D−Q−1
2

λ
q

Q
(cid:89)

q(cid:48)=q+1



|λq − λq(cid:48)|

 ,

(2.13)
where λ = (λ1, ..., λQ) and c is a constant that depends on
D and Q.

Note that the non-zero eigenvalues of W T W are the same
as for W W T , and therefore they have the same probability
density function. The singular values σi of W are the square
roots of λ’s, σi =
λi. Thus, by change of measure we
get4

√

p(σ1, ..., σQ) =

ce− 1

2

(cid:80)Q

q=1 σ2

q

Q
(cid:89)

q=1


σD−Q−1
q

Q
(cid:89)



|σ2

q − σ2
q(cid:48)|



q(cid:48)=q+1

Q
(cid:89)

q=1

2σq,

(2.14)

where the last product in the above term is the Jacobian
correction. Provided this density function as the prior distri-
bution on the singular value matrix Σ, we can easily sample
from its posterior.

To calculate the prior distribution for U , we need to dig
deeper into random matrix theory. As mentioned earlier, a
standard Gaussian prior on W gives rise to a Wishart dis-
tribution on W W T . Equation (2.12) shows, that a Wishart
matrix can be decomposed into a product of an orthogo-
nal matrix U and a diagonal matrix Σ. Furthermore, it is
known that the eigenvectors U are distributed uniformly in
the space of orthogonal matrices5. The set of orthogonal
matrices is known as the Stiefel manifold VQ,D

VQ,D =

(cid:110)

U ∈ RD×Q|U U T = I

(cid:111)

.

(2.15)

The dimension of this manifold is DQ − 1
2 Q(Q + 1), ac-
counting for the fact that the orthogonality constraint re-
duces the number of independent degrees of freedom. The

4Given a probability density px(x) on x and an invertible map
g, so that y = g(x), the density py(y) on y is given by py(y) =
(cid:17)(cid:12)
px(g−1(y))
(cid:12)
(cid:12) . Here the absolute determinant of
the Jacobian accounts for the change of volume under g.

(cid:16) dg−1(y)
dy

(cid:12)
(cid:12)
(cid:12)det

3In general, the SVD is not unique. While the singular values
can be ordered to ensure uniqueness of Σ, the left singular vectors
U are only determined up to the sign. As explained later, we
enforce a sign convention on them.

5Bai et al. (2007) and Uhlig (1994) mention that the eigenvector
matrix of a Wishart distribution is Haar-distributed, which is also
true for singular Wishart matrices, but we could not ﬁnd a proof
anywhere.

Rotation Invariant Householder Parameterization for Bayesian PCA

Stiefel manifold can be equipped with a uniform measure.
This measure is an example of a Haar measure, as it is invari-
ant under the action of the orthogonal group O(D) (cid:39) VD,D,
i.e.

p(U ) = p(RU ) ∀R ∈ O(D) .

(2.16)

Theorem 2 Let vD, vD−1, ..., v1 be uniformly distributed
on the unit spheres SD−1, ..., S0 respectively, where Sn−1 is
the unit sphere in Rn. Furthermore, let H n(vn) be the n-th
Householder transformation as deﬁned in equation (2.20)
The product

To proceed, we need to ﬁnd an unconstrained parameteri-
zation for orthogonal matrices along with a density on the
parameters, such that the resulting matrix is Haar distributed.
Shepard et al. (2014); Pourzanjani et al. (2017) and Mez-
zadri (2007) suggest ways to do that. The procedure is
explained in detail in the next section.

2.5. QR-decomposition and Householder

Transformations

Given a matrix Z ∈ RD×Q the (thin) QR-decomposition
decomposes Z into an orthogonal matrix Q ∈ VQ,D and
an upper triangular matrix R ∈ RQ×Q. If the elements of
Z are i.i.d Gaussian with mean zero and unit variance, Q
is Haar distributed (Mezzadri, 2007). Note that this is only
the case if a unique QR-decomposition is used. In practice,
this is usually achieved by enforcing the convention that the
diagonal elements of R are positive.

To compute the QR-decomposition of Z, the so-called
Householder Transformations H can be used. These trans-
formations are reﬂections on the plane spanned by a vector
vn ∈ Rn. To decompose a D-by-Q matrix, Q of such trans-
formations are needed and the resulting orthogonal matrix
Q can be written as

Q = H D(vD)H D−1(vD−1)...H D−Q+1(vD−Q+1).

To construct H n, we deﬁne ˜Hn(vn) as
˜Hn(vn) = −sgn(vn1) (cid:0)I − 2unuT

n

(cid:1) ∈ Rn×n,

where

un =

vn + sgn(vn1)(cid:107)vn(cid:107)e1
(cid:107)vn + sgn(vn1)(cid:107)vn(cid:107)e1(cid:107)

and construct H n by

H n =

(cid:18)I
0

(cid:19)

.

0
˜Hn

(2.17)

(2.18)

(2.19)

(2.20)

The algorithm for the QR-decomposition constructs a suit-
able sequence of vectors vD, . . . , vD−Q+1 based on the en-
tries of the successively rotated columns of Z. The choice
of the sign fulﬁlls the requirements mentioned earlier, i.e.
the diagonal elements of R are positive.

Here, we are only interested in the resulting orthogonal
matrix Q. In particular, by randomly drawing vectors v, we
obtain a random orthogonal matrix via (2.17). The following
theorem tells us which distribution the vs need to have in
order to get an orthogonal matrix distributed according to
the Haar measure (Mezzadri, 2007).

Q = H D(vD)H D−1(vD−1)...H 1(v1)

(2.21)

is a random orthogonal matrix with distribution given by
the Haar measure on O(D).

A corresponding draw from the Stiefel manifold VQ,D can
be obtained by just taking the ﬁrst Q columns of Q. Alterna-
tively, this is achieved by drawing vectors vD, . . . , vD−Q+1
uniformly from the respective unit sphere and construct
the unitary matrix from the corresponding Householder
transformations. By the theorem above, this gives the
Haar measure on VQ,D as the Householder transformations
H D−Q, . . . , H 1 only effect the columns D − Q, . . . , 1 by
(2.20).

3. Unique PPCA

Here, we connect the results from the previous section to
obtain a model for PPCA without the latent space symmetry
that plaques the original formulation. We start by sampling
uniformly from the unit spheres Sn−1. This is most eas-
ily achieved by drawing an i.i.d. standard Gaussian vector
of dimension n and normalizing its length. Note that this
spends an additional parameter compared to the dimension-
ality n − 1 of the unit sphere Sn−1. Yet, the n-dimensional
standard Gaussian distribution is easy to sample from and
the vector length is sufﬁciently constrained by the Gaus-
sian prior such that the sampler can move around the unit
sphere effectively. The same parameterization is employed,
for example, by Stan in order to support a unit_vector
data type (Stan Development Team, 2018). Furthermore,
the Householder transformation does not require the vec-
tors v to be of unit length as they are normalized anyways
via (2.19). Thus, we construct the orthogonal matrix U dis-
tributed with the Haar measure by successively transforming
standard Gaussian random vectors.

Next, we sample the ordered singular values from the joint
distribution (2.14). Again, this can be accomplished by
transforming the ordered vector (σ1, . . . , σQ) to an uncon-
strained space and correcting the joint density by the Jaco-
bian determinant of the transformation. We refer to Stan
Development Team (2018) for details of this transformation.
Finally, from the orthogonal matrix U and the diagonal ma-
trix Σ, we construct W by W = U Σ, which we then use
in the likelihood of the PPCA model. Overall, we obtain the

Rotation Invariant Householder Parameterization for Bayesian PCA

following generative model:

vD, . . . , vD−Q+1 ∼ N (0, I)

σ ∼ p(σ) ∝ eq. (2.14)
µ ∼ p(µ), e.g. a broad Gaussian

U =

Q
(cid:89)

q=1

H D−q+1(vD−q+1)

Σ = diag(σ)
W = U Σ
σnoise ∼ p(σnoise)

Y ∼

(cid:16)

N

N
(cid:89)

n=1

Y n,:|µ, W W T + σ2

noiseI

(cid:17)

.

Note that this model deﬁnes the same distribution as
the corresponding model with a standard Gaussian prior
W ∼ N (0, I). In both cases, the sampling distribution is
governed by the Wishart distributed matrix W W T , even
though the distribution on W is actually different. We im-
plemented both models in the probabilistic programming
language Stan. The code for the simulations is available on
Github: https://github.com/RSNirwan/HouseholderBPCA.

Compared to previous approaches based on parameterizing
the Stiefel manifold in terms of Givens rotations (Pourzan-
jani et al., 2017), our model has the following advantages:
First, our Householder parameters v are unconstrained, in
contrast to the angular parameters of Givens rotations where
the sampler might hit the boundary of the space. Secondly,
we avoid the computationally demanding computation of
the Jacobian determinant (Shepard et al., 2014). Compared
to other approaches employing the matrix von Mises-Fisher
distribution (Hoff, 2009; ˇSm´ıdl & Quinn, 2007), our model
has the following advantages: First, we do not require condi-
tional conjugacy allowing non-linear extensions of Bayesian
PPCA as illustrated in section 4. Secondly, we do not need
to resort to rejection sampling or variational approxima-
tions. Similar to these approaches, our parameterization
introduces a combinatorial symmetry by the sign ambiguity
of the SVD. This is akin to the label switching problem in
Gaussian mixture models which usually poses few problems
with the sampler simply getting stuck in a speciﬁc mode.
Here, in order to compare results across different modes, we
postprocess each sample such that the ﬁrst entry of each col-
umn of U is made positive. In the following, we illustrate
our model on some data sets and discuss possible extensions
to non-linear models where similar symmetries arise.

3.1. Model Comparison

3.1.1. SYNTHETIC DATASET

Figure 3.1. Results for synthetic dataset. Left: In the background,
we see samples from the posterior of the standard parameterization.
Samples from the U and Σ mapped to W = U Σ are shown
in a darker color. Right: Comparison of the suggested model
to classical PCA. White dots are the classical PCA solution and
orange dots are the true values. 4000 samples for each row of W
are shown in different colors.

For (N, D, Q) = (150, 5, 2) we sample X ∈ RN ×D from
a standard normal distribution and construct W by W =
U Σ ∈ RD×Q, where U is sampled from the Stiefel mani-
fold with Haar measure and we specify Σ = diag(σ1, σ2),
where (σ1, σ2) = (3.0, 1.0). Then, we get the observation
Y = XW T + (cid:15), where (cid:15) denotes the noise sampled from
a zero mean Gaussian with a standard deviation of 0.01.

The left plot in Figure 3.1 compares the Bayesian inference
for the standard model, where we directly assume a standard
Gaussian prior on W with our suggested model, where
we parameterize W by U and Σ and infer the posterior
distribution of both. As expected, the standard model has
the rotational symmetry in W , whereas our model takes out
the rotation6. On the right side (Figure 3.1), we compare
the posterior distribution of our model with the classical
PCA solution and as expected with the low observation
noise, the solutions are quite similar. In contrast to classical
PCA, the Bayesian approach provides a distribution for our
parameters, accessing estimation uncertainty of the solution
as well.

Figure 3.2 shows the posterior distribution of Σ, that we
ﬁxed to diag(3.0, 1.0). As we can see, the true values are
recovered quite well.

3.1.2. BREAST CANCER WISCONSIN DATASET

We tested the model on the Breast Cancer Wisconsin dataset
as well. The dataset was downloaded from the Python tool-
box scikit-learn (Pedregosa et al., 2011) and contains 569
labeled datapoints with 30 features. We neglect the labels
and take the standardized 569 × 30 matrix as the input to our
model. For visualization purposes, we again map the data
to Q = 2. Figure 3.3 shows the performance of different
models. In the left plot, we see the posterior samples for the

Here, we build our own synthetic dataset with known pa-
rameters and the goal is to reconstruct the parameter values.

6The additional sign ambiguity of the columns of U is removed

afterwards by post processing each sample.

−2−1012W1−2−1012W2−0.50.00.51.0W1−2−1012W2Rotation Invariant Householder Parameterization for Bayesian PCA

which is implemented in Stan, needs fewer leap-frog steps
per sample because the rotational symmetry is removed
from the posterior and does not need to be explored. Numer-
ically, this leads to slightly lower wall clock times overall.
From this perspective, our implementation is as efﬁcient as
other sampling methods proposed for probabilistic PCA or
Bayesian factor analysis. If scalability becomes an issue,
our model can easily be used together with more scalable
approximation methods such as variational Bayes.

Pourzanjani et al. (2017) proposed an alternative parameter-
ization which removes the rotational symmetry by means of
Givens rotations. This approach requires a Jacobian adjust-
ment to account for the change of measure in order to ensure
the uniform Haar measure. This requires the computation
of the absolute determinant of an D · Q dimensional matrix.
Thus, compared to their approach we achieve a substantial
speedup as, according to theorem 2, no Jacobian adjustment
is required in our model.

3.3. Other than Gaussian Priors

The prior in the paper has been chosen to aid compari-
son with standard (classical) PCA. Using the SVD, we de-
compose the prior into a rotation, i.e. the principal axis,
and a diagonal matrix, containing the principal compo-
nents/variances. Thus, the parameters of our model are
easily and well interpretable. For the Gaussian prior, which
is often chosen in Bayesian factor analysis to support Gibbs
sampling, we show that the rotation and variances are inde-
pendent. Furthermore, the rotation is uniformly distributed
according to the Haar measure.

Other priors of this structure can be constructed without
issues. In particular, the interpretation of the variance pa-
rameters σ2 as the principal components helps to this end.
For instance, automatic relevance determination is readily
accomplished by putting a shrinkage/sparsity prior on the
principle components. Interestingly, such a prior would be
very different from priors arising from imposing sparsity on
the factor loadings as suggested in sparse Bayesian factor
analysis (Bhattacharya & Dunson, 2011). In this case, the in-
duced distribution on the principal components is still sparse
- at least numerically, as we are not aware of any theoretical
results. More importantly though, the induced rotation is
not distributed uniformly anymore. Thus, such a prior gives
rise to an implicit a-priori preference for certain principal
axes. Note that this does not break the rotational symmetry
of the model which arises from the rotation of the latent
space. Instead it imposes a preference for certain directions
in data space which, as we believe, is actually undesirable
in many cases. We view it as a strength of our model that
interpretable priors can be choosen independently for the
rotation and variances.

Figure 3.2. Histogram of the samples from the posterior distribu-
tion of Σ = diag(σ1, σ2). The vertical solid black lines are the
classic PCA solution and dashed lines are the true values (3.0, 1.0),
which generated the data.

Figure 3.3. Results for the Breast Cancer Wisconsin dataset. Left:
Posterior samples of the standard parameterization of W (in the
background) and posterior samples of W parameterized by U and
Σ (foreground). Right: Posterior samples of our parameterization
and white dots are the classical PCA solution. 4000 samples for
each row of W are shown in different colors. For clarity, we only
show 10 randomly chosen rows instead of all 30.

direct parameterization of W and the parameterization of
W by U and Σ and again, our model is able to uniquely
identify W . The right plot contains the uniquely identiﬁed
W and the classical PCA solution (white dots). Again, our
model enriches the classical PCA solution with uncertainty
estimates. Samples were drawn from 4 independent chains
which all converged to the same solution.

3.2. Computational Complexity

Compared to probabilistic PCA we compute the DxQ load-
ing matrix W from parameters v for the principal rotation
and σ2 for the principal variances. This is achieved via a
sequence of Q Householder transformations which, as de-
tailed in Shepard et al. (2014), can be achieved in O(DQ2)
steps. The same complexity applies when computing gra-
dients with respect to the model parameters. Overall, this
increases the total computation time of probabilistic PCA
by a factor of Q, i.e. the number of latent dimensions, com-
pared to the standard parameterization. The scaling with
respect to the number of data points is unchanged.

When sampling from the model, the adaptive NUTS sampler,

1.01.52.02.53.03.54.0σ0100200300400500600700samples from p(σ|Y) σ - classical PCAσ - True values−1.0−0.50.00.51.0W1−1.0−0.50.00.51.0W2−1.0−0.50.00.5W10.00.20.40.60.81.0W2Rotation Invariant Householder Parameterization for Bayesian PCA

4. Extension to Non-linear Models

4.1. Gaussian Process Latent Variable Model

Since our method works well for the linear PPCA model,
the question arises, whether we could as well improve the
posterior sampling of non-linear dimensionality reduction
techniques. Lawrence (2005) introduced an extension of
PPCA called the Gaussian Process Latent Variable Model
(GP-LVM). The model starts with the dual formulation of
PPCA. In this case, instead of marginalizing over X in
equation (2.3), we marginalize over W ∼ N (0, I). The
resulting likelihood then takes the form

p(Y |X) =

D
(cid:89)

d=1

N (Y :,d|µ, K + σ2I),

(4.1)

where K = XX T and Kij = X T
i,:X j,:. Identifying this
with the covariance kernel of Gaussian process (Rasmussen,
2006), the model is generalized by replacing the linear ker-
nel Kij with a non-linear one. In this way, a non-linear
dimensionality reduction method is obtained, the GP-LVM.
One of the most used covariance kernels is the exponentiated
quadratic kernel (Rasmussen, 2006)

kSE(x, x(cid:48)) = σ2

SE exp(−0.5||x − x(cid:48)||2

2/l2),

(4.2)

where σ2
will use this covariance function in our analysis as well.

SE is the kernel variance and l the lengthscale. We

As suggested by Lawrence (2005), we can optimize equa-
tion (4.1) log p(Y |X) with respect to the latent positions
and the hyperparameters. Optimization can easily lead to
overﬁtting and a fully Bayesian treatment of the model
would be preferable. Therefore, we need a prior distribution
p(X) on X and using Bayes rule, the posterior is given as
p(X|Y ) = p(Y |X)p(X)/p(Y ). As in the case of PPCA
the posterior is not analytically tractable and, as before, we
resort to sampling in order to approximate it. Note that the
kernel kSE only depends on the distance between points
x and x(cid:48) and thus the model likelihood is again invariant
under rotations of the latent space X.

4.2. Model Comparison

To test our model in the non-linear case, we again take the
Breast Cancer Wisconsin dataset. For the GP-LVM the input
is the transposed matrix Y ∈ RN ×D, where N = 30 and
D = 569. We standardized the data and set σSE and l to one
and only sample the latent space. First, we ﬁt the data with
the standard parameterization, where we directly have the
X as parameters and then we reparameterize X by U and
Σ and sample v and σ as described in sections 2.4 and 2.5.

Figure 4.1 shows the results for the exponentiated quadratic
kernel for Q = 2. As before, we used Stan to sample from

Figure 4.1. Posterior samples from for the GP-LVM. The top row
shows the results for the standard parameterization for X and the
bottom row shows the results of our suggested parameterization.
2000 samples per chain for each row of X are shown in different
colors. Again, for clarity, we only show 10 randomly chosen rows
instead of all 30.

the posterior on three independent chains. The top row
shows the samples from the different chains for the stan-
dard parameterization. One can clearly see the rotational
symmetry. The bottom row shows the samples with our sug-
gested parameterization. As expected, there is no rotational
symmetry anymore. However, due to the models complex-
ity/ﬂexibility many local minima arise and the chains with
both (the standard and our suggested) parameterizations do
not converge to the same posterior.

5. Conclusion

We suggested a new parameterization for the mapping W in
PPCA taking latent to observed points, which uniquely iden-
tiﬁes the principal components even though the likelihood
and the posterior (for the Bayesian case) are rotationally
symmetric. We have shown how to parameterize the model
via the singular vectors and values of W and how to set
the prior on the new parameters such that the model is
not changed compared to a standard Gaussian prior on W
directly. Furthermore, we provided an efﬁcient implementa-
tion via Householder transformations.

Thereafter, we tested the model on a synthetic and the Breast
Cancer Wisconsin dataset. Our model was able to uniquely
reconstruct the true parameters that created the synthetic
data. On both datasets our model provided similar results as
classical PCA, additionally containing the uncertainty of the
estimates as well. In the end, we showed that our method
can also be used to remove the symmetry in the latent space
of a non-linear GP-LVM model. Overall, we believe that
our approach, thanks to its known prior distribution and
computational efﬁciency, can be quite useful for latent space
models with rotation symmetric likelihoods.

−505X1−5.0−2.50.02.55.0X2standard - chain: 1−505X1standard - chain: 2−505X1standard - chain: 3−202X1−5.0−2.50.02.55.0X2unique - chain: 1−202X1unique - chain: 2−202X1unique - chain: 3Rotation Invariant Householder Parameterization for Bayesian PCA

Acknowledgements

The authors thank Dr. h.c. Maucher for funding their posi-
tions.

References

Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,
Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M.,
Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard,
M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Lev-
enberg, J., Man´e, D., Monga, R., Moore, S., Murray, D.,
Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever,
I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan,
V., Vi´egas, F., Vinyals, O., Warden, P., Wattenberg, M.,
Wicke, M., Yu, Y., and Zheng, X. TensorFlow: Large-
scale machine learning on heterogeneous systems, 2015.
Software available from tensorﬂow.org.

Bai, Z. D., Miao, B. Q., and Pan, G. M. On asymptotics
of eigenvectors of large sample covariance matrix. Ann.
Probab., 35(4):1532–1572, 07 2007.

Betancourt, M. A Conceptual Introduction to Hamiltonian

Monte Carlo. ArXiv e-prints, January 2017.

Mezzadri, F. How to generate random matrices from the
classical compact groups. Notices of the American Math-
ematical Society, 54(5):592 – 604, 5 2007. ISSN 0002-
9920.

Millsap, R. E. When trivial constraints are not trivial: The
choice of uniqueness constraints in conﬁrmatory factor
analysis. Structural Equation Modeling: A Multidisci-
plinary Journal, 8(1):1–17, 2001.

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-
napeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
Scikit-learn: Machine learning in Python. Journal of
Machine Learning Research, 12:2825–2830, 2011.

Peeters, C. F. W. Rotational uniqueness conditions under
oblique factor correlation metric. Psychometrika, 77(2):
288–292, Apr 2012. ISSN 1860-0980.

Pourzanjani, A. A., Jiang, R. M., Mitchell, B., Atzberger,
P. J., and Petzold, L. R. General Bayesian Inference over
the Stiefel Manifold via the Givens Transform. ArXiv
e-prints, October 2017.

Bhattacharya, A. and Dunson, D. B. Sparse bayesian inﬁnite

Rasmussen, C. E. Gaussian processes for machine learning.

factor models. Biometrika, 98 2:291–306, 2011.

MIT Press, 2006.

Shepard, R., Gidofalvi, G., and Brozell, S. R. The multifacet
graphically contracted function method. II. a general pro-
cedure for the parameterization of orthogonal matrices
and its application to arc factors. The Journal of Chemical
Physics, 141(6):064106, 2014.

Stan Development Team. Stan modeling language users

guide and reference manual, version 2.18.0, 2018.

Tipping, M. E. and Bishop, C. M. Probabilistic principal
component analysis. Journal of the Royal Statistical
Society, Series B, 61:611–622, 1999.

Uhlig, H. On singular wishart and singular multivariate beta
distributions. Ann. Statist., 22(1):395–405, 03 1994.

ˇSm´ıdl, V. and Quinn, A. On bayesian principal component
analysis. Computational Statistics & Data Analysis, 51
(9):4101–4123, 2007.

Bishop, C. M. Pattern Recognition and Machine Learning.

Springer, New York, 1st edition, 2006.

Byrne, S. and Girolami, M. Geodesic monte carlo on em-
bedded manifolds. Scandinavian Journal of Statistics, 40
(4):825–845, 2013.

Carpenter, B., Gelman, A., Hoffman, M., Lee, D., Goodrich,
B., Betancourt, M., Brubaker, M., Guo, J., Li, P., and
Riddell, A. Stan: A probabilistic programming language.
Journal of Statistical Software, Articles, 76(1):1–32, 2017.
ISSN 1548-7660.

Hoff, P. D. Simulation of the matrix Bingham–von Mises–
Fisher distribution, with applications to multivariate and
relational data. Journal of Computational and Graphical
Statistics, 18(2):438–456, 2009.

James, O. and Lee, H.-N. Concise Probability Distributions
of Eigenvalues of Real-Valued Wishart Matrices. ArXiv
e-prints, February 2014.

J¨oreskog, K. G. A general approach to conﬁrmatory max-
imum likelihood factor analysis. Psychometrika, 34(2):
183–202, Jun 1969. ISSN 1860-0980.

Lawrence, N. Probabilistic non-linear principal component
analysis with gaussian process latent variable models. J.
Mach. Learn. Res., 6:1783–1816, December 2005. ISSN
1532-4435.

