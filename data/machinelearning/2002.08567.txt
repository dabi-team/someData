ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

1

Multi-Agent Meta-Reinforcement Learning for
Self-Powered and Sustainable Edge Computing
Systems

Md. Shirajum Munir, Graduate Student Member, IEEE, Nguyen H. Tran, Senior Member, IEEE,
Walid Saad, Fellow, IEEE, and Choong Seon Hong, Senior Member, IEEE

1
2
0
2

b
e
F
0
1

]

G
L
.
s
c
[

3
v
7
6
5
8
0
.
2
0
0
2
:
v
i
X
r
a

Abstract—The stringent requirements of mobile edge comput-
ing (MEC) applications and functions fathom the high capacity
and dense deployment of MEC hosts to the upcoming wireless
networks. However, operating such high capacity MEC hosts
can signiﬁcantly increase energy consumption. Thus, a base
station (BS) unit can act as a self-powered BS. In this paper, an
effective energy dispatch mechanism for self-powered wireless
networks with edge computing capabilities is studied. First, a
two-stage linear stochastic programming problem is formulated
with the goal of minimizing the total energy consumption cost of
the system while fulﬁlling the energy demand. Second, a semi-
distributed data-driven solution is proposed by developing a novel
multi-agent meta-reinforcement learning (MAMRL) framework
to solve the formulated problem. In particular, each BS plays
the role of a local agent that explores a Markovian behavior for
both energy consumption and generation while each BS transfers
time-varying features to a meta-agent. Sequentially, the meta-
agent optimizes (i.e., exploits) the energy dispatch decision by
accepting only the observations from each local agent with its
own state information. Meanwhile, each BS agent estimates its
own energy dispatch policy by applying the learned parameters
from meta-agent. Finally, the proposed MAMRL framework
is benchmarked by analyzing deterministic, asymmetric, and
stochastic environments in terms of non-renewable energy usages,
energy cost, and accuracy. Experimental results show that the
proposed MAMRL model can reduce up to 11% non-renewable
energy usage and by 22.4% the energy cost (with 95.8% prediction
accuracy), compared to other baseline methods.

Index Terms—Mobile edge computing (MEC), stochastic op-
timization, meta-reinforcement learning, self-powered, demand
response.

This work was partially supported by the National Research Founda-
tion of Korea(NRF) grant funded by the Korea government(MSIT) (No.
2020R1A4A1018607) and by Institute of Information & communications
Technology Planning & Evaluation (IITP) grant funded by the Korea govern-
ment(MSIT) (No.2019-0-01287, Evolvable Deep Learning Model Generation
Platform for Edge Computing).

Md. Shirajum Munir, and Choong Seon Hong are with the Department of
Computer Science and Engineering, Kyung Hee University, Yongin-si 17104,
Republic of Korea (e-mail: munir@khu.ac.kr; cshong@khu.ac.kr).

Nguyen H. Tran is with the School of Computer Science, The University of
Sydney, Sydney, 2006, NSW, Australia. (e-mail: nguyen.tran@sydney.edu.au).
Walid Saad is with the Wireless@VT Group, Bradley Department of Elec-
trical and Computer Engineering, Virginia Tech, Blacksburg, VA 24061 USA,
and also with the Department of Computer Science and Engineering, Kyung
Hee University, Yongin-si 17104, Republic of Korea (e-mail: walids@vt.edu).
Corresponding author: Choong Seon Hong (e-mail: cshong@khu.ac.kr).
©2021 IEEE. Personal use of this material is permitted. Permission from
IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or promotional
purposes, creating new collective works, for resale or redistribution to servers
or lists, or reuse of any copyrighted component of this work in other works.

I. INTRODUCTION

Next-generation wireless networks are expected to signif-
icantly rely on edge applications and functions that include
edge computing and edge artiﬁcial
intelligence (edge AI)
[1]–[7]. To successfully support such edge services within
a wireless network with mobile edge computing (MEC) ca-
pabilities, energy management (i.e., demand and supply) is
one of the most critical design challenges. In particular, it
is imperative to equip next-generation wireless networks with
alternative energy sources, such as renewable energy, in order
to provide extremely reliable energy dispatch with less energy
consumption cost [8]–[15]. An efﬁcient energy dispatch design
requires energy sustainability, which not only saves energy
consumption cost, but also fulﬁlls the energy demand of
the edge computing by enabling its own renewable energy
sources. Speciﬁcally, sustainable energy is the practice of
seamless energy ﬂow to the MEC system that emerges to
meet the energy demand without compromising the ability of
future energy generation. Furthermore, to ensure a sustainable
MEC operation, the retrogressive penetration of uncertainty for
energy consumption and generation is essential. A summary
of the challenges that are solved by the literature to enable
renewable energy sources for the wireless network is presented
in Table I.

To provide sustainable edge computing for next-generation
wireless systems, each base station (BS) with MEC capabilities
unit can be equipped with renewable energy sources. Thus, the
energy source of such a BS unit not only relies solely on the
power grid, but also on the equipped renewable energy sources.
In particular, in a self-powered network, wireless BSs with
MEC capabilities is equipped with its own renewable energy
sources that can generate renewable energy, consume, store,
and share energy with other BS units.

Delivering seamless energy ﬂow with a low energy con-
sumption cost in a self-powered wireless network with MEC
capabilities can lead to uncertainty in both energy demand and
generation. In particular, the randomness of the energy demand
is induced by the uncertain resources (i.e., computation and
communication) request by the edge services and applications.
Meanwhile, the energy generation of a renewable source (i.e.,
a solar panel) at each self-powered BS unit varies on the time
of a day. In other words, the pattern of energy demand and gen-
eration will differ from one self-powered BS unit to another.
Thus, such ﬂuctuating energy demand and generation pattern

 
 
 
 
 
 
ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

2

induces a non-independent and identically distributed (non-
i.i.d.) of energy dispatch at each BS over time. To overcome
this non-i.i.d. energy demand and generation, characterizing
the expected amount of uncertainty is crucial to ensure a
seamless energy ﬂow to the self-powered wireless network.
As such, when designing self-powered wireless networks, it is
necessary to take into account this uncertainty in the energy
patterns.

A. Related Works

The problem of energy management for MEC-enabled
wireless networks has been studied in [16]–[22] (summary
in Table II). In [16], the authors proposed a joint mechanism
for radio resource management and users task ofﬂoading with
the goal of minimizing the long-term power consumption
for both mobile devices and the MEC server. The authors
in [17] proposed a heuristic to solve the joint problem of
computational resource allocation, uplink transmission power,
and user task ofﬂoading problem. The work in [18] studied the
tradeoff between communication and computation for a MEC
system and the authors proposed a MEC server CPU scaling
mechanism for reducing the energy consumption. Further, the
work in [19] proposed an energy-aware mobility management
scheme for MEC in ultra-dense networks, and they addressed
the problem using Lyapunov optimization and multi-armed
bandits. Recently, the authors in [21] proposed a distributed
power control scheme for a small cell network by using the
concept of a multi-agent calibrate learning. Further, the authors
in [22] studied the problem of energy storage and energy
harvesting (EH) for a wireless network using deviation theory
and Markov processes. However, all of these existing works
assume that the consumed energy is available from the energy
utility source to the wireless network system [16]–[22]. Since
the assumed models are often focused on energy management
and user task ofﬂoading on network resource allocations, the
random demand for computational (e.g., CPU computation,
memory, etc.) and communication requirements of the edge
applications and services are not considered. In fact, even if
enough energy supply is available, the energy cost related to
network operation can be signiﬁcant because of the usage of
non-renewable (e.g., coal, petroleum, natural gas). Indeed, it
is necessary to include renewable energy sources towards the
next-generation wireless networking infrastructure.

Recently, some of the challenges of renewable energy
powered wireless networks have been studied in [8]–[14], [23].
In [8], the authors proposed an online optimization framework
to analyze the activation and deactivation of BSs in a self-
powered network. In [9], proposed a hybrid power source
infrastructure to support heterogeneous networks (HetNets), a
model-free deep reinforcement learning (RL) mechanism was
proposed for user scheduling and network resource manage-
ment. In [10], the authors developed an RL scheme for edge
resource management while incorporating renewable energy
in the edge network. In particular, the goal of [10] is to
minimize a long-term system cost by load balancing between
the centralized cloud and edge server. The authors in [11]
introduced a microgrid enabled edge computing system. A

joint optimization problem is studied for MEC task assignment
and energy demand-response (DR) management. The authors
in [11] developed a model-based deep RL framework to tackle
the joint problem. In [12], the authors proposed a risk-sensitive
energy proﬁling for microgrid-powered MEC network to en-
sure a sustainable energy supply for green edge computing by
capturing the conditional value at risk (CVaR) tail distribution
of the energy shortfall. The authors in [12] proposed a multi-
agent RL system to solve the energy scheduling problem. In
[13], the authors proposed a self-sustainable mobile networks,
using graph-based approach for intelligent energy management
with a microgrid. The authors in [14] proposed a smart
grid-enabled wireless network and minimized grid energy
consumption by applying energy sharing among the BSs.
Furthermore, in [23], the authors addressed challenges of non-
coordinated energy shedding and mis-aligned incentives for
mixed-use building (i.e., buildings and data centers) using auc-
tion theory to reduce energy usage. However, these works [9]–
[14], [23] do not investigate the problem of energy dispatch
nor do they account for the energy cost of MEC-enabled,
self-powered networks when the demand and generation of
each self-powered BS are non-i.i.d.. Dealing with non-i.i.d.
energy demand and generation among self-powered BSs is
challenging due to the intrinsic energy requirements of each
BS evolve the uncertainty. In order to overcome this unique
energy dispatch challenge, we propose to develop a multi-
agent meta-reinforcement learning framework that can adapt
new uncertain environment without considering the entire past
experience.

Some interesting problems related to meta-RL and multi-
agent deep RL are studied in [24]–[28] (summary in Table
II). In [24], the authors focused on studying the challenges
of the tradeoff between effectiveness and available amounts
of training data for a deep-RL based learning system. To
the authors in [24] tackled those challenges by
this end,
learning architecture.
exploring a deep meta-reinforcement
This learning architecture comprises of two learning systems:
1) lower-level system that can learn each new task very
quickly, 2) higher-level system is responsible to improve the
performance of each lower-level system task. In particular, this
learning mechanism is involved with one lower-level system
that can learn relatively quickly as compared with a higher-
level system. This lower-level system can adapt to a new
task while a higher-level system performs ﬁne-tuning so as
to improve the performance of the lower-level system. In
particular, in deep meta-reinforcement learning, a lower-level
system quantiﬁes a reward based on the desired action and
feeds back that reward to a higher-level system to tune the
weights of a recurrent network. However, the authors in [24]
do not consider a stochastic environment nor do they extend
their work for a multi-agent scenario. The authors in [25]
proposed a stochastic gradient-based meta-parameter learning
scheme for tuning reinforcement learning parameters to the
physical environmental dynamics. Particularly, the experiment
in [25] performed in both animal and robot environments,
where an animal must recognize food before it starves and
a robot must recharge before the battery is empty. Thus, the
proposed scheme can effectively ﬁnd meta-parameter values

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

3

TABLE I: Summary of the challenges that are solved by the literature for enabling renewable energy sources in the wireless
network.

Ref.

Energy sources

[8]

[9]
[10]

[11]

[12]

[13]
[14]

[15]

This work

Renewable

Hybrid energy
Hybrid energy

Microgrid

Microgrid

Renewable
Smart grid enabled
hybrid energy
Hybrid energy

Smart grid enabled
self-powered
renewable energy

MEC ca-
pabilities
No

Non-i.i.d.
dataset
No

Energy
dispatch
No

Energy
cost
No

No
Yes

Yes

Yes

No
No

No

Yes

No
No

No

No

No
No

No

Yes

No
No

Yes

Yes

Yes
Yes

Yes

Yes

No
No

No

No

No
No

No

Yes

Remarks

Activation and deactivation of BSs in a self-powered net-
work
User scheduling and network resource management
Load balancing between the centralized cloud and edge
server
MEC task assignment and energy demand-response (DR)
management
Risk-sensitive energy proﬁling for microgrid-powered MEC
network
Energy load balancing among the SBSs with a microgrid
Joint network resource allocation and energy sharing among
the BSs
Overall system architecture for edge computing and renew-
able energy resources
An effective energy dispatch mechanism for self-powered
wireless networks with edge computing capabilities

and controls the meta-parameter in both static and dynamic
environments. In [26], the authors investigated a learning to
learn (i.e., meta-learning) mechanism with the recurrent neural
networks, where the meta-learning problem was designed
as a generalized transfer learning scheme. In particular, the
authors in [26] considered a parametrized optimizer that can
transfer the neural network parameters update to an opti-
mizee. Meanwhile, the optimizee can determine the gradients
without relying on the optimizer parameters. Moreover, the
optimizee sends the error to the optimizer, and updates its
own parameters based on the transferred parameters. This
mechanism allows an agent to learn new tasks for a similar
structure. An asynchronous multi-agent RL framework was
studied in [27], where the authors investigated how parallel
actor learners of asynchronous advantage actor-critic (A3C)
can achieve better stability during the neural network training
comparted to asynchronous RL schemes. Such schemes in-
clude asynchronous one-step Q-learning, one-step Sarsa, and
n-step Q-learning. The authors in [28] proposed a general-
purpose multi-agent scheme by adopting the framework of
centralized training with decentralized execution. In particular,
the authors in [28] proposed an extension of the actor-critic
policy gradient mechanism by modifying the role of the critic.
This critic is augmented with an additional policy information
from the other actors (agents). Sequentially, each local actor
executes in a decentralized manner and sends its own policy
to the centralized critic for further investigation. However, the
environment (i.e., state information) of this model remains
the same for all of the local actors while in our setting the
environment of each BS agent is deferent from others based on
its own energy demand and generation. Moreover, the works in
[24]–[28], do not consider a multi-agent environment in which
the policy of each agent relies on its own state information.
In particular, such state information belongs to a non-i.i.d.
learning environment when environmental dynamics become
distinct among the agents.

Fig. 1: Multi-agent meta-reinforcement learning framework of
self-powered energy dispatch for sustainable edge computing.

B. Contributions

The main contribution of this paper is a novel energy
management framework for next-generation MEC in self-
powered wireless network that
is reliable against extreme
uncertain energy demand and generation. We formulate a two-
stage stochastic energy cost minimization problem that can
balance renewable, non-renewable, and storage energy without
knowing the actual demand. In fact, the formulated problem
also investigates the realization of renewable energy generation
after receiving the uncertain energy demand from the MEC
applications and service requests. To solve this problem, we
propose a multi-agent meta-reinforcement learning (MAMRL)
framework that dynamically observes the non-i.i.d. behavior of
time-varying features in both energy demand and generation
at each BS and, then transfers those observations to obtain
an energy dispatch decision and execute the energy dispatch
policy to the self-powered BS. Fig. 1 illustrates how we pro-
pose to dispatch energy to ensure sustainable edge computing
over a self-powered network using MAMRL framework. As
we can see, each BS that includes small cell base stations
(SBSs) and a macro base station (MBS) will act as a local

Meta Agent Learning…LSTM for Meta-Policy Optimizer Energy Environment BS 0Local Agent BS 0StateRewardActionEnergy Environment BS BLocal Agent BS BStateRewardActionObservation MemoryMeta-AdvantageFunctionMeta-PolicyParameter UpdateObservationsEach Agent ObservationLocal AdvantageFunctionShared Neural Networks ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

4

TABLE II: Summary of the related works [16]–[28].

Ref.
[16]

[17]

task

resource

Contributions
Radio
management
and
users
ofﬂoading
Computational
resource allocation,
uplink transmission
power,
user
task ofﬂoading

and

[18] MEC server CPU
scaling mechanism
for reducing the en-
ergy consumption
Energy-aware mo-
bility management
scheme for MEC

[19]

[20]

Energy
efﬁcient
green-IoT network

[21] Distributed power
control scheme for
a small cell net-
work
Energy storage and
energy
harvesting
(EH) for a wireless
network
[23] Non-coordinated

[22]

[24]

[25]

shedding
energy
mis-aligned
and
incentives
for
mixed-use building
between
Tradeoff
and
effectiveness
available
amounts
of training data
Controling
meta-parameter
in
both
and
environments
Learning to learn
mechanism
with
the recurrent neural
networks
[27] Asynchronous

static
dynamic

[26]

the

multi-agent
framework

RL

[28] General-purpose

multi-agent scheme

Limitation

Method
Optimization Usage of non-
renewable, deter-
ministic environ-
ment
Usage of non-
renewable,
energy dispatch,
performance
guarantee

Heuristic

Optimization Usage of non-

renewable,
energy dispatch

Energy dispatch,
i.i.d.
energy
demand-
response
Edge computing,
Energy dispatch,
deterministic en-
vironment
Usage of non-
renewable,
energy dispatch

MEC
capabilities, i.i.d.
energy demand-
response
MEC
capabilities, i.i.d.
energy demand-
response

Stochastic envi-
ronment and a
multi-agent sce-
nario
Single-
agent,
environment

same

Lyapunov
and multi-
armed
bandits
Heuristic

Multi-agent
calibrate
learning

Deviation
theory and
Markov
processes
Auction
theory

Deep meta-
RL

SGD-based
meta-
parameter
learning

Generalized
transfer
learning

Deterministic
environment,
single-agent

One-step
Q-learning,
one-step
Sarsa, and
n-step
Q-learning
Extension
of
the
actor-critic
policy
gradient

Deterministic en-
vironment

Same
environment
for
all of
local actors

the

quickly. Thus,
the proposed MAMRL framework ensures
autonomous decision making under an uncertain and unknown
environment. Our key contributions include:

• We formulate a self-powered energy dispatch problem for
MEC-supported wireless network, in which the objective
is to minimize the total energy consumption cost of
network while considering the uncertainty of both energy
consumption and generation. The formulated problem
is, thus, a two-stage linear stochastic programming. In
particular, the ﬁrst stage makes a decision when energy
demand is unknown, and the second stage discretizes the
realization of renewable energy generation after knowing
energy demand of the network.

• To solve the formulated problem, we propose a new
multi-agent meta-reinforcement learning framework by
considering the skill transfer mechanism [24], [25] be-
tween each local agent (i.e., self-powered BS) and meta-
agent. In this MAMRL scheme, each local agent ex-
plores its own energy dispatch decision using Markovian
properties for capturing the time-varying features of both
energy demand and generation. Meanwhile, the meta-
agent evaluates (exploits) that decision for each local
agent and optimizes the energy dispatch decision. In
particular, we design a long short-term memory (LSTM)
as a meta-agent (i.e., run at MBS) that is capable of
avoiding the incompetent decision from each local agent
and learns the right features more quickly by maintaining
its own state information.

• We develop the proposed MAMRL energy dispatch
framework in a semi-distributed manner. Each local agent
(i.e., self-powered BS) estimates its own energy dis-
patch decision using local energy data (i.e., demand and
generation), and provides observations to the meta-agent
individually. Consequently, the meta-agent optimizes the
decision centrally and assists the local agent toward a
globally optimized decision. Thus, this approach not only
reduces the computational complexity and communica-
tion overhead but it also mitigates the curse of dimension-
ality under the uncertainty by utilizing non-i.i.d. energy
demand and generation from each local agent.

• Experimental results using real datasets establish a signif-
icant performance gain of the energy dispatch under the
deterministic, asymmetric, and stochastic environments.
Particularly, the results show that the proposed MAMRL
model saves up to 22.44% of energy consumption cost
over a baseline approach while achieving an average
accuracy of around 95.8% in a stochastic environment.
Our approach also decreases the usage of non-renewable
energy up to 11% of total consumed energy.

agent and transfer their own decision (reward and action) to
the meta-agent. Then, the meta-agent accumulates all of the
non-i.i.d. observations from each local agent (i.e., SBSs and
MBS) and optimizes the energy dispatch policy. The proposed
MAMRL framework then provides feedback to each BS agent
for exploring efﬁciently that acquire the right decision more

The rest of the paper is organized as follows. Section II
presents the system model of self-powered edge computing.
The problem formulation is described in Section III. Section
IV provides MAMRL framework for solving energy dispatch
problem. Experimental results are analyzed in Section V.
Finally, conclusions are drawn in Section VI.

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

5

TABLE III: Summary of notations.

Description
Set of BSs (SBSs and MBS)
Set of active servers under the BS 𝑖 ∈ B
Set of user tasks at BS 𝑖 ∈ B
Set of renewable energy sources
Server utilization in BS 𝑖 ∈ B
No. of CPU cores
Average downlink data of BS 𝑖
Fixed channel bandwidth of BS 𝑖 for user task 𝑗
Transmission power of BS 𝑖
Downlink channel gain between user task 𝑗 to BS 𝑖
Co-channel interference for user task 𝑗 at BS 𝑖
Energy coefﬁcient for BS 𝑖 ∈ B
MEC server CPU frequency for a single core
Server switching capacitance
MEC server static energy consumption
MEC server idle state power consumption
Scaling factor of heterogeneous MEC CPU core
Static energy consumption of BS
Renewable energy cost per unit
Non-renewable energy cost per unit
Storage energy cost per unit
Amount of renewable energy
Amount of non-renewable energy
Amount of surplus energy
Energy demand at time slot 𝑡
Random variable for energy demand
Maximum capacity of renewable energy at BS 𝑖 ∈ B
Set of observation at BS 𝑖 ∈ B
Big 𝑂 notation to represent complexity
Entropy regularization coefﬁcient
Discount factor
Learning parameters for BS 𝑖 ∈ B
Energy dispatch policy with parameters 𝜃𝑖 at BS 𝑖 ∈ B
Meta-agent learning parameters

(𝑡)
(𝑡)

Notation
B
K𝑖
J𝑖
R
𝜌𝑖 (𝑡)
𝐿
𝑅𝑖 (𝑡)
𝑊𝑖 𝑗
𝑃𝑖
𝑔𝑖 𝑗 (𝑡)
𝐼𝑖 𝑗 (𝑡)
𝛿𝑖
𝑓
𝜏
𝜂MEC
st
𝜂MEC
idle
𝜛𝑘
st (𝑡)
𝜂net
𝑐ren
𝑡
𝑐non
𝑡
𝑐sto
𝑡
𝜉 ren
𝑡
𝜉 non
𝑡
𝜉 sto
𝑡
𝜉 d
𝑡
𝜉 D
𝑡
𝜉 renmax
𝑡
O𝑖
𝑂 (.)
𝛽
𝛾
𝜃𝑖
𝜋𝜃𝑖
𝜙

average trafﬁc size 𝑆𝑖 (𝑡) (bits) at time slot 𝑡. The average trafﬁc
arrival rate is deﬁned as 𝜆𝑖 (𝑡) = 1
𝑆𝑖 (𝑡) . Therefore, an M/M/K
queuing model is suitable to model these 𝐽𝑖 user tasks using 𝐾𝑖
MEC servers at BS 𝑖 and time 𝑡 [31], [32]. The task size of this
queuing model is exponentially distributed since the average
trafﬁc size 𝑆𝑖 (𝑡) is already known. Hence, the service rate of
the BS 𝑖 is determined by 𝜇𝑖 (𝑡) =
𝑢𝑘𝑖 (𝑡) ] . At any given
time 𝑡, we assume that all of the tasks in J𝑖 are uniformly
distributed at each BS 𝑖. Thus, for a given MEC server task
association indicator Υ 𝑗 𝑘𝑖 (𝑡) = 1 if task 𝑗 is assigned to server
𝑘 at BS 𝑖, and 0 otherwise, the average MEC server utilization
is deﬁned as follows [11]:
(cid:40) (cid:205) 𝑗 ∈J𝑖
0,

(cid:205)𝑘𝑖 ∈K𝑖 Υ 𝑗 𝑘𝑖 (𝑡) 𝜆𝑖 (𝑡)
𝜇𝑖 (𝑡) 𝐾𝑖

, if Υ 𝑗 𝑘𝑖 (𝑡) = 1,

1
E[(cid:205)𝑘𝑖 ∈K𝑖

otherwise.

𝜌𝑖 (𝑡) =

(1)

1) MEC Server Energy Consumption:

In case of MEC
server energy consumption, the computational energy con-
sumption (dynamic energy) will be dependent on the CPU
activity for executing computational tasks [16], [17], [33].
Further, such dynamic energy is also accounted with the
thermal design power (TDP), memory, and disk I/O operations
of the MEC server [16], [17], [33] and we denote as 𝜂MEC
(𝑡).
Meanwhile, static energy 𝜂MEC
(𝑡) includes the idle state power
idle
of CPU activities [16], [18]. We consider, a single core CPU
with a processor frequency 𝑓
(cycles/s), an average server
utilization 𝜌𝑖 (𝑡) (using (1)) at time slot 𝑡, and a switching

st

Fig. 2: System model for a self-powered wireless network with
MEC capabilities.

II. SYSTEM MODEL OF SELF-POWERED EDGE
COMPUTING

Consider a self-powered wireless network that is connected
with a smart grid controller as shown in Fig. 2. Such a wireless
network enables edge computing services for various MEC
applications and services. The energy consumption of the
network depends on network operations energy consumption
along with the task loads of the MEC applications. Meanwhile,
the energy supply of the network relies on the energy gener-
ation from renewable sources that are attached to the BSs, as
well as both renewable and non-renewable sources of the smart
grid. Furthermore, the smart grid controller is a representative
of the main power grid (i.e, smart grid), where an additional
amount of energy can be supplied via the smart grid controller
to the network. Therefore, we will ﬁrst discuss the energy
demand model that includes MEC server energy consumption,
and network communication energy consumption. We will
then describe the energy generation model that consists of the
non-renewable energy generation cost, surplus energy storage
cost, and total energy generation cost. Table III illustrates the
summary of notations.

A. Energy Demand Model

Consider a set B = {0, 1, 2, . . . , 𝐵} of 𝐵 + 1 (0 for MBS)
BSs that encompass 𝐵 SBSs overlaid over a single MBS.
Each BS 𝑖 ∈ B includes a set K𝑖 = {1, 2, . . . , 𝐾𝑖 } of 𝐾𝑖
MEC application servers. We consider a ﬁnite time horizon
T = 1, 2, . . . , 𝑇 with each time slot being indexed by 𝑡 and
having a duration of 15 minutes [29]. The observational period
of each time slot 𝑡 ends at the 15-th minute and is capable of
capturing the changes of network dynamics [11], [12], [30].
A set J𝑖 of 𝐽𝑖 heterogeneous MEC application task requests
from users will arrive to BS 𝑖 with an average task arrival
rate 𝜆𝑖 (𝑡) (bits/s) at time 𝑡. The task arrival rate 𝜆𝑖 (𝑡) at BS
𝑖 ∈ B follows a Poisson process at time slot 𝑡. BS 𝑖 integrates
𝐾𝑖 heterogeneous active MEC application servers that has
𝑢𝑘𝑖 (𝑡) (bits/s) processing capacity. Thus, 𝐽𝑖 computational task
requests will be accumulated into the service pool with an

Smart Grid ControllerMBS : Macro base stationSBS: Small cell base stationAT : Application typeMEC : Mobile edge computingMBS SBS with MEC capabilitiesAT1AT2:ATkAT1AT2:ATkSBS with MEC capabilitiesSmart grid to BS (MBS, SBS) connection Local renewable energy to BS connection User to MEC server communicationACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

6

capacitance 𝜏 = 5 × 10−27 (farad) [17]. The dynamic power
consumption of such single core CPU can be calculated by
applying a cubic formula 𝜏𝜌𝑖 (𝑡) 𝑓 3 [18], [34]. Thus, energy
consumption of 𝐾𝑖 MEC servers with 𝐿 CPU cores at BS 𝑖 is
deﬁned as follows:
(cid:26) (cid:205)𝑘 ∈𝐾𝑖
𝜂MEC
idle

(cid:205)𝑙 ∈𝐿 𝜏𝜌𝑖 (𝑡) 𝑓 3
𝑘𝑖
(𝑡),
otherwise,

(2)
where 𝜛𝑘𝑖𝑙 denotes a scaling factor of heterogeneous CPU core
of the MEC server. Thus, the value of 𝜛𝑘𝑖𝑙 is dependent on
the processor architecture [35] that assures the heterogeneity
of the MEC server.

𝜛𝑘𝑖𝑙 + 𝜂MEC

𝜉MEC
𝑖

(𝑡) =

st

(𝑡), if 𝜌𝑖 (𝑡) > 0,

2) Base Station Energy Consumption: The energy con-
sumption needed for the operation of the network base stations
(i.e., SBSs and MBS) includes two types of energy: dynamic
and static energy consumption [36]. On one hand, a static
energy consumption 𝜂net
st (𝑡) includes the energy for maintain-
ing the idle state of any BS, a constant power consumption
for receiving packet from users, and the energy for wired
transmission among the BSs. On the other hand, the dynamic
energy consumption of the BSs depends on the amount of
data transfer from BSs to users which essentially relates to the
downlink [37] transmit energy. Thus, we consider that each BS
𝑖 ∈ B operates at a ﬁxed channel bandwidth 𝑊𝑖 𝑗 and constant
transmission power 𝑃𝑖 [37]. Then the average downlink data
of BS 𝑖 will be given by [11]:

𝑅𝑖 (𝑡) =

∑︁

∑︁

𝑖 ∈B

𝑗 ∈J𝑖

𝑊𝑖 𝑗 log2

(cid:16)

1 +

𝑃𝑖𝑔𝑖 𝑗 (𝑡)
𝜎2 + 𝐼𝑖 𝑗 (𝑡)

(cid:17)

(3)

where 𝑔𝑖 𝑗 (𝑡) represents downlink channel gain between user
to BS 𝑖, 𝜎2 determines a variance of an Additive
task 𝑗
White Gaussian Noise (AWGN), and 𝐼𝑖 𝑗 (𝑡) denotes the co-
channel interference [38], [39] among the BSs. Here, the co-
channel interference 𝐼𝑖 𝑗 (𝑡) = (cid:205)𝑖(cid:48) ∈B,𝑖(cid:48)≠𝑖 𝑃𝑖(cid:48)𝑔𝑖(cid:48) 𝑗 (𝑡) relates to
the transmissions from other BSs 𝑖(cid:48) ∈ B that use the same
subchannels of 𝑊𝑖 𝑗 . 𝑃𝑖(cid:48) and 𝑔𝑖(cid:48) 𝑗 (𝑡) represent, respectively, the
transmit power and the channel gain of the BS 𝑖(cid:48) ≠ 𝑖 ∈ B.
Therefore, downlink energy consumption of the data transfer
of BS 𝑖 ∈ B is deﬁned by
[watt-seconds or joule], where
𝑆𝑖 (𝑡)
𝑅𝑖 (𝑡) [seconds] determines the duration of transmit power 𝑃𝑖
[watt]. Thus, the network energy consumption for BS 𝑖 at time
𝑡 is deﬁned as follows [19], [36]:

𝑃𝑖 𝑆𝑖 (𝑡)
𝑅𝑖 (𝑡)

𝜉net
𝑖

(𝑡) =

∑︁

(cid:16)

𝑗 ∈J𝑖

𝛿net
𝑖

𝑃𝑖𝑆𝑖 (𝑡)
𝑅𝑖 (𝑡)

+ 𝜂net

st (𝑡)

(cid:17)

,

(4)

𝑖

where 𝛿net
data through the network. In fact, the value of 𝛿net
on the type of the network device (e.g., 𝛿net
transceiver remote radio head [36]).

determines the energy coefﬁcient for transferring
depends
𝑖
𝑖 = 2.8 for a 6 unit

3) Total Energy Demand: The total energy consumption
(demand) of the network consists of both MEC server compu-
tational energy (in (2)) consumption, and network the opera-
tional energy (i.e., BSs energy consumption in (4)). Thus, the
overall energy demand of the network at time slot 𝑡 is given
as follows:

𝜉d
𝑡 =

𝜉net
𝑖

(𝑡) + 𝜉MEC

𝑖

(𝑡)

.

(5)

(cid:17)

∑︁

(cid:16)

𝑖 ∈B

The demand 𝜉d
𝑡
on the computational tasks load of the MEC servers.

is random over time and completely depends

B. Energy Generation Model

𝑖

𝑖

time 𝑡

The energy supply of the self-powered wireless network
with MEC capabilities relates to the network’s own renewable
(e.g., solar, wind, biofuels, etc.) sources as well as the main
grid’s non-renewable (e.g., diesel generator, coal power, and so
on) energy sources [8], [9]. In this energy generation model,
we consider a set R = {R0, R1, . . . , R𝐵} of renewable energy
sources of the network, with each element R𝑖 representing
the set of renewable energy sources of BS 𝑖 ∈ B. Each
renewable energy source 𝑞 ∈ R𝑖 at BS 𝑖 ∈ B can generate
𝑖𝑞 (𝑡) of renewable energy at time 𝑡. Therefore,
an amount 𝜉ren
(𝑡) at
the total amount of renewable energy generation 𝜉ren
(𝑡) = (cid:205)𝑞 ∈R𝑖
BS 𝑖 ∈ B will be 𝜉ren
𝜉ren
𝑖𝑞 (𝑡) for time slot 𝑡.
Thus, the total renewable energy generation for the considered
is deﬁned as 𝜉ren
(𝑡). The
network at
maximum limit of this renewable energy 𝜉ren
is less than or
equal to the maximum capacity 𝜉renmax
of renewable energy
generation at time period 𝑡. Thus, we consider a maximum
storage limit that is equal to the maximum capacity 𝜉renmax
of the renewable energy generation [40]–[42]. Further, the
self-powered wireless network is able to get an additional
non-renewable energy amount 𝜉non
from the main grid at
time 𝑡. The per unit renewable and non-renewable energy
cost are deﬁned by 𝑐ren
, respectively. In general, the
𝑡
renewable energy cost only depends on the maintenance cost
of the renewable energy sources [40]–[42]. Therefore, the per
unit non-renewable energy cost is greater than the renewable
energy cost 𝑐non
𝑡 > 𝑐ren
. Additionally, the surplus amount of the
energy 𝜉sto
at time 𝑡 can be stored in energy storage medium
𝑡
for the future usages [41], [42] and the energy storage cost of
per unit energy store is denoted by 𝑐sto

= (cid:205)𝑖 ∈B 𝜉ren

and 𝑐non

.

𝑡

𝑡

𝑡

𝑡

𝑡

𝑡

𝑡

𝑖

𝑡

1) Non-renewable Energy Generation Cost:

the energy demand 𝜉d
𝑡 when it
𝑡

In order to
is greater than the
fulﬁll
generated renewable energy 𝜉ren
, the main grid can provide
an additional amount of energy 𝜉non
from its non-renewable
sources. Thus, the non-renewable energy generation cost 𝐶non
of the network is determined as follows:
(cid:26) 𝑐non
], if 𝜉d
𝑡
otherwise,
0,

𝑡 > 𝜉ren

𝑡 − 𝜉ren

𝐶non
𝑡

[𝜉d

(6)

=

,

𝑡

𝑡

𝑡

𝑡

where 𝑐non

𝑡

represents a unit energy cost.

2) Surplus Energy Storage Cost: The surplus amount of
energy is stored in a storage medium when 𝜉d
𝑡 < 𝜉ren
(i.e., energy demand is smaller than the renewable energy
generation) at time 𝑡. We consider the per unit energy storage
cost 𝑐sto
. This storage cost depends on the storage medium
and amount of the energy store at time 𝑡 [23], [41], [43], [44].
With the per unit energy storage cost 𝑐sto
, the total storage
cost at time 𝑡 is deﬁned as follows:

𝑡

𝑡

𝑡

𝐶sto

𝑡 =

(cid:26) 𝑐sto
𝑡
0,

[𝜉ren

𝑡 − 𝜉d

𝑡 < 𝜉ren

𝑡

𝑡 ], if 𝜉d
otherwise.

,

(7)

3) Total Energy Generation Cost: The total energy gen-
eration cost includes renewable, non-renewable, and storage
energy cost. Naturally, this total energy generation cost will

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

7

depend on the energy demand 𝜉d
𝑡 of the network at time 𝑡.
Therefore, the total energy generation cost at time 𝑡 is deﬁned
as follows:

𝑄(𝜉ren
𝑡

, 𝜉d

𝑡 ) = 𝑐ren

𝑡 𝜉ren
𝑡

+ 𝑐non
𝑡
+𝑐sto
𝑡

𝑡 − 𝜉ren
[𝜉d
]+
𝑡
,
𝑡 − 𝜉d
[𝜉ren
𝑡 ]+

(8)

𝑡

𝑡

[𝜉d

[𝜉ren

, 𝑐non
𝑡

𝑡 − 𝜉d

𝑡 − 𝜉ren

𝑡 𝜉ren
𝑡
𝑡 ]+, respectively. In (8), energy demand 𝜉d

where the energy cost of the renewable, non-renewable, and
storage energy are given by 𝑐ren
]+, and
𝑐sto
𝑡 and
𝑡
renewable energy generation 𝜉ren
are stochastic in nature. The
𝑡
energy cost of non-renewable energy (6) and storage energy (7)
completely rely on energy demand 𝜉d
𝑡 and renewable energy
generation 𝜉ren
. Hence, to address the uncertainty of both
energy demand and renewable energy generation in a self-
powered wireless network, we formulate a two-stage stochastic
programing problem. In particular, the ﬁrst stage makes a
decision of the energy dispatch without knowing the actual
demand of the network. Then we make further energy dispatch
decisions by analyzing the uncertainty of the network demand
in the second stage. A detailed discussion of the problem
formulation is given in the following section.

III. PROBLEM FORMULATION WITH A TWO-STAGE
STOCHASTIC MODEL

We now consider the case in which the non-renewable en-
𝑡 > 𝑐ren
ergy cost is greater than the renewable energy cost, 𝑐non
that is often the case in a practical smart grid as discussed in
[40], [41], [42], and [45]. Here, 𝜉ren
𝑡 are the continuous
variables over the observational duration 𝑡. The objective is
to minimize the total energy consumption cost 𝑄(𝜉ren
𝑡 ).
𝜉ren
is
𝑡
a parameter. When the energy demand 𝜉d
the
𝑡
optimization problem will be:

, 𝜉d
is the decision variable and the energy demand 𝜉d
𝑡
is known,

and 𝜉d

𝑡

𝑡

𝑡

𝜒 = min
𝜉 ren
𝑡 ≥0

𝑄(𝜉ren
𝑡

, 𝜉d

𝑡 ).

(9)

In problem (9), after removing the non-negativity constraints
𝜉ren
𝑡 ≥ 0, we can rewrite the objective function in the form of
piecewise linear functions as follows:

𝑄(𝜉ren
𝑡

, 𝜉d

𝑡 ) = max
𝜉 ren
𝑡

(cid:110)(cid:16)

(𝑐ren

𝑡 − 𝑐non

𝑡

)𝜉ren

𝑡 + 𝑐non

𝑡

(cid:17)

,

𝜉d
𝑡

(cid:16)

(𝑐ren

𝑡 + 𝑐sto

𝑡 )𝜉ren

𝑡 − 𝑐sto

𝑡 𝜉d
𝑡

(10)

(cid:17)(cid:111)

.

𝑡

𝑡

𝑡

)𝜉ren

𝑡 < 𝜉ren

𝑡 + 𝑐non

𝑡 − 𝑐non

𝑡 and (𝑐ren
𝜉d

𝑡 − 𝑐sto
𝑡

𝑡 )𝜉ren
𝑡 > 𝜉ren

Where (𝑐ren
𝑡 + 𝑐sto
𝑡 𝜉d
𝑡
determine the cost of non-renewable (i.e., 𝜉d
) and
storage (i.e., 𝜉d
) energy, respectively. Therefore, we
have to choose one out of the two cases. In fact, if the energy
demand 𝜉d
𝑡 is known and also the amount of renewable energy
𝜉ren
is the same as the energy demand, then problem (10)
𝑡
provides the optimal decision in order to exact amount of
demand 𝜉d
𝑡 . However, the challenge here is to make a decision
about the renewable energy 𝜉ren
usage before the demand
𝑡
becomes known. To overcome this challenge, we consider the
energy demand 𝜉D
𝑡 as a random variable whose probability
distribution can be estimated from the previous history of

the energy demand. We can re-write problem (9) using the
expectation of the total cost as follows:

E[𝑄(𝜉ren

𝑡

, 𝜉D

𝑡 )].

min
𝜉 ren
𝑡 ≥0

(11)

The solution of problem (11) will provide an optimal result on
average. However, in the practical scenario, we need to solve
problem (11) repeatedly over the uncertain energy demand 𝜉D
𝑡 .
Thus, this solution approach does not signiﬁcantly affect our
model in terms of scalability while 𝐵 + 1 number of BSs gen-
erates a large variety of energy demand over the observational
period of 𝑡. In fact, energy demand and generation can change
over time for each BS 𝑖 ∈ B, and they can also induce large
variations of demand-generation among the BSs. Hence, the
solution to problem (11) cannot rely on an iterative scheme
due to a lake of the adaptation for uncertain change of energy
demand and generation over time.

We consider the moment of random variable 𝜉D
that has
𝑡
a ﬁnitely supported distribution and takes values 𝜉D
, . . . , 𝜉D
𝑡 𝐵
𝑡0
with respective probabilities 𝑝0, . . . , 𝑝 𝐵 of BSs 𝐵 + 1. The
cumulative distribution function (CDF) 𝐻 (𝜉D
𝑡 ) of energy de-
is a step function and jumps of size 𝑝𝑖 at each
mand 𝜉D
𝑡
demand 𝜉d
𝑡𝑖. Therefore, the probability distribution of each BS
energy demand 𝜉d
𝑡 ) of historical
observation of energy demand 𝜉D
𝑡 . In this case, we can convert
problem (11) into a deterministic optimization problem and the
expectation of energy usage cost E[𝑄(𝜉ren
𝑡 )] is determined
𝑡
by (cid:205)𝑖 ∈B 𝑝𝑖𝑄(𝜉ren
𝑡 ). Thus, we can rewrite the problem (9)
as a linear programming problem using the representation in
(10) as follows:

𝑡𝑖 belongs to the CDF 𝐻 (𝜉D

, 𝜉D

, 𝜉d

𝑡

𝜒

min
𝜉 ren
, 𝜒
𝑡
s.t. 𝜒 ≥ (𝑐ren
𝜒 ≥ (𝑐ren
𝜉renmax
𝑡

𝜉d
𝑡 + 𝑐non
)𝜉ren
𝑡 − 𝑐non
𝑡 ,
𝑡
𝑡
𝑡 + 𝑐sto
𝑡 ,
𝑡 𝜉d
𝑡 − 𝑐sto
𝑡 )𝜉ren
≥ 𝜉ren

𝑡 ≥ 0.

(12)

(12a)

(12b)

(12c)

𝑡

𝑡

, 𝜉d

For a ﬁxed value of the renewable energy 𝜉ren
, problem (12)
is an equivalent of problem (10). Meanwhile, problem (12) is
equal to 𝑄(𝜉ren
𝑡 ). We have converted the piecewise linear
function from problem (10) into the inequality constraints
(12a) and (12b). Constraint (12c) ensures a limit on the
maximum allowable renewable energy usage. We consider
𝑝𝑖 as a highest probability of energy demand at each BS
𝑖 ∈ B. Therefore, for 𝐵 + 1 BSs, we deﬁne 𝑝0, . . . , 𝑝 𝐵
as the probability of energy demand with respect
to BSs
𝑖 = 0, . . . , 𝐵. Thus, we can rewrite the problem (11) for 𝐵 + 1
BSs 𝝃D
𝑡 𝐵) is as follows:

, . . . , 𝜉D

𝑡 = (𝜉D
𝑡0

min
, 𝜒0,...,𝜒𝐵

𝜉 ren
𝑡

∑︁

𝑝𝑖 𝜒𝑖,

𝑖 ∈B
s.t. 𝜒𝑖 ≥ (𝑐ren
𝜒𝑖 ≥ (𝑐ren
𝜉renmax
𝑡

𝜉D
𝑡𝑖, ∀𝑖 ∈ B,
𝑡 + 𝑐non
)𝜉ren
𝑡 − 𝑐non
𝑡
𝑡
𝑡𝑖, ∀𝑖 ∈ B,
𝑡 𝜉D
𝑡 − 𝑐sto
𝑡 )𝜉ren
𝑡 + 𝑐sto
≥ 𝜉ren

𝑡 ≥ 0,

(13)

(13a)

(13b)

(13c)

where 𝑝𝑖 represents the highest probability of the energy
demand 𝜉D
𝑡𝑖 is a random variable and 𝜉d
𝑡𝑖
denotes a realization of energy demand on BS 𝑖 ∈ B at time 𝑡.
The value of 𝑝𝑖 belongs to the empirical CDF 𝐻 (𝝃D
𝑡𝑖) of the

𝑡𝑖, in which 𝜉D

𝑡𝑖 = 𝜉d

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

8

energy demand 𝜉D
𝑡𝑖 for BS 𝑖 ∈ B. This CDF is calculated from
the historical observation of the energy demand at BS 𝑖 ∈ B. In
fact, for a ﬁxed value of non-renewable energy 𝜉ren
, problem
(13) is separable. As a result, we can decompose this problem
with a structure of two-stage linear stochastic programming
problem [46], [47].

𝑡

To ﬁnd an approximation for a random variable with a ﬁnite
probability distribution, we decompose problem (13) in a two-
stage linear stochastic programming under uncertainty. The
decision is made using historical data of energy demand, which
is fully independent from the future observation. As a result,
the ﬁrst stage of self-powered energy dispatch problem for
sustainable edge computing is formulated as follows:

min
𝜉 ren
𝑡 ≥0
s.t.

(𝑐ren
𝑡

)(cid:62)𝜉ren

𝑡 + E[𝑄(𝜉ren

𝑡

𝜉renmax
𝑡

≥ 𝜉ren

𝑡 ≥ 0,

, 𝝃D

𝑡 )],

(14)

(14a)

𝑡

𝑡

, 𝝃D

where 𝑄(𝜉ren
𝑡 ) determines an optimal value of the second
stage problem. In problem (14), the decision variable 𝜉ren
is
calculated before the realization of uncertain energy demand
𝝃D
𝑡 . Meanwhile, at the ﬁrst stage of the formulated problem
(14), the cost (𝑐ren
is minimized for the decision variable
𝜉ren
𝑡 which then allows us to estimate the expected energy cost
E[𝑄(𝜉ren
𝑡 )] for the second stage decision. Constraint (14a)
provides a boundary for the maximum allowable renewable
energy usage. Thus, based on the decision of the ﬁrst stage
problem, the second stage problem can be deﬁned as follows:

)(cid:62)𝜉ren
𝑡

, 𝝃D

𝑡

𝑡

min
, 𝜉 sto
𝑡

𝜉 non
𝑡

(𝑐non
𝑡

)(cid:62)𝜉non

𝑡 − (𝑐sto

𝑡 )(cid:62)𝜉sto

𝑡

s.t.

𝑡 − 𝜉non
|,
𝑡
𝑡 )(cid:62),
𝑡 ≤ (𝝃D

𝜉sto
𝑡 = |𝜉ren
0 ≤ 𝜉non
𝑡 ≥ 0.
𝜉non

,

(15)

(15a)

(15b)

(15c)

𝑡

𝑡

𝑡

In the second stage problem (15), the decision variables 𝜉non
𝑡
and 𝜉sto
𝑡 depend on the realization of the energy demand 𝝃D
𝑡 of
the ﬁrst stage problem (14), where, 𝜉ren
determines the amount
of renewable energy usage at time 𝑡. The ﬁrst constraint (15a)
is an equality constraint that determines the surplus amount
of energy 𝜉sto
𝑡 must be equal to the absolute value difference
and non-renewable 𝜉non
between the usage of renewable 𝜉ren
energy amount. The second constraint (15b) is an inequality
constraint that uses the optimal demand value from the ﬁrst
stage realization. In particular, the value of demand comes
from (5) that is the historical observation of energy demand.
Finally, the constraint (15c) protects from the non-negativity
for the non-renewable energy 𝜉non

𝑡
The formulated problems (14) and (15) can characterize the
uncertainty between network energy demand and renewable
energy generation. Particularly, the second stage problem (15)
contains random demand 𝝃D
leads the optimal cost
𝑡
E[𝑄(𝜉ren
𝑡 )] as a random variable. As a result, we can
rewrite the problems (14) and (15) in a one large linear pro-
gramming problem for 𝐵 + 1 BSs and the problem formulation

usage.

, 𝝃D

that

𝑡

is as follows:

∑︁

(cid:16)

(𝑐ren
𝑡

)(cid:62)𝜉ren

𝑡 +

min
, 𝜉 non
𝑡

𝜉 ren
𝑡

𝑡 ∈T

, 𝜉 sto
𝑡
∑︁

𝑝𝑖 [(𝑐non

𝑡

)(cid:62)𝜉non

𝑡𝑖 − (𝑐sto

𝑡 )(cid:62)𝜉sto
𝑡𝑖 ]

(16)

(cid:17)

,

(16a)

(16b)

(16c)

s.t.

|, ∀𝑖 ∈ B,

𝑡𝑖, ∀𝑖 ∈ B,

𝑖 ∈B
𝑡𝑖 − 𝜉non
𝑡𝑖

𝑡𝑖 = |𝜉ren
𝜉sto
𝑡𝑖 ≤ 𝜉D
0 ≤ 𝜉non
𝑡𝑖 ≥ 0, ∀𝑖 ∈ B,
𝜉non
𝜉renmax
≥ 𝜉ren
𝑡

𝑡

𝑡

𝑡

(16d)

, 𝜉non
𝑡

, 𝜉non
𝑡

and 𝜉sto

and 𝜉sto

𝑡𝑖 ≥ 0, ∀𝑖 ∈ B.
In problem (16), for 𝐵 + 1 BSs, energy demand 𝜉D
. . . 𝜉D
𝑡 𝐵
𝑡0
happens with positive probabilities 𝑝0 . . . 𝑝 𝐵 and (cid:205)𝑖 ∈B 𝑝𝑖 = 1.
The decision variables are 𝜉ren
, which represent
the amount of renewable, non-renewable, and storage energy,
respectively. Constraint (16a) deﬁnes a relationship among
all of the decision variables 𝜉ren
. In essence,
𝑡
this constraint discretizes the surplus amount of energy for
storage. Hence, constraint (16b) ensures the utilization of non-
renewable energy based on the energy demand of the network.
Constraint (16c) ensures that the decision variable 𝜉non
𝑡 will
not be a negative value. Finally, constraint (16d) restricts the
usages in to maximum capacity 𝜉renmax
renewable energy 𝜉ren
at time 𝑡. Problem (16) is an integrated form of the ﬁrst-
stage problem in (14) and the second-stage problem in (15),
where the solution of 𝜉non
completely depends on
𝑡
𝑡𝑖 for all 𝐵 + 1 BSs. The decision
realization of demand 𝜉D
comes before the realization of demand 𝜉D
of the 𝜉ren
𝑡𝑖 and,
thus, the estimation of renewable energy generation 𝜉ren
𝑡 will
be independent and random. Therefore, problem (16) holds
the property of relatively complete recourse. In problem (16),
the number of variables and constraints is proportional to
the numbers of BSs, 𝐵 + 1. Additionally, the complexity of
the decision problem (16) leads to O(2 |T |×|B |) due to the
combinatorial properties of the decisions and constraints [46]–
[48].

and 𝜉sto

𝑡

𝑡

𝑡

𝑡

𝑡𝑖 , non-renewable 𝜉non
𝑡𝑖

The goal of the self-powered energy dispatch problem
(16) is to ﬁnd an optimal energy dispatch policy that in-
cludes amount of renewable 𝜉ren
, and
𝑡𝑖 energy of each BS 𝑖 ∈ B while minimizing the
storage 𝜉sto
energy consumption cost. Meanwhile, such energy dispatch
policy relies on an empirical probability distribution 𝐻 (𝝃D
𝑡 )
of historical demand at each BS 𝑖 ∈ B at time 𝑡. In order
to solve problem (16), we choose an approach that does
not rely on the conservativeness of a theoretical probability
distribution of energy demand in problem (16), and also will
capture the uncertainty of renewable energy generation from
the historical data. In contrast, we can construct a theoretical
probability distribution of energy demand 𝝃D
𝑡 when we know
what the exact distribution is as well as what its parameters
will be (e.g., mean, variance, and standard deviation). In
fact, in practice, the distribution of energy demand 𝝃D
is
𝑡
unknown and instead, a certain amount of historical energy
demand data are available. As a result, we cannot rely on this
distribution to measure uncertainty while the renewable energy
generation and energy demand are random over time. Hence,
we can obtain time-variant features of both energy demand and
generation by characterizing the Markovian properties from

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

9

Algorithm 1 State Space Generation of BS 𝑖 ∈ B in MAMRL
Framework
Input: 𝑊𝑖 𝑗 , 𝑃𝑖, 𝑔𝑖 𝑗 (𝑡), 𝜎2, 𝐼𝑖 𝑗 (𝑡), Υ 𝑗 𝑘𝑖 (𝑡), 𝜏, 𝑓𝑘𝑖 , 𝜛𝑘𝑖𝑙 , 𝜂MEC
Input: 𝛿net
, 𝜂net
𝑖
Output: 𝒔𝑡𝑖 : (𝜉d

), ∀𝒔𝑡𝑖 ∈ S𝑖 ∈ S, ∀𝑡 ∈ T

st (𝑡), 𝑐non
𝑡
𝑖 , 𝜉ren
𝑖

, 𝑐sto
𝑡
(𝑡), 𝐶sto

𝑡𝑖 , 𝐶non
𝑡𝑖

st

Initialization: R𝑖, K𝑖, J𝑖,S𝑖, 𝜆𝑖 (𝑡), 𝜇𝑖 (𝑡), 𝜌𝑖 (𝑡), 𝑅𝑖 (𝑡)

(𝑡), 𝑆𝑖 (𝑡)

for each 𝑖 ∈ B do

1: for each 𝑡 ∈ T do
2:
3:
4:
5:
6:

for each 𝑗 ∈ J𝑖 do
Calculate: 𝜉MEC
Calculate: 𝜉net

𝑖

𝑖

(𝑡) using eq. (2)

(𝑡) using eq. (4)

𝑖

end for
𝑡 = 𝜉net
Calculate: 𝜉d
(𝑡) + 𝜉MEC
𝑖
𝑡 = (cid:205)𝑞 ∈R 𝜉ren
Calculate: 𝜉ren
𝑖𝑞 (𝑡)
Calculate: 𝐶non
using eq. (6)
𝑡
Calculate: 𝐶sto
using eq. (7)
(𝑡), 𝐶sto
𝑖 , 𝜉ren
Assign: 𝒔𝑡𝑖 : (𝜉d
𝑖

𝑡

𝑡𝑖 , 𝐶non
𝑡𝑖

)

7:
8:
9:
10:
11:
12:

end for
Append: 𝒔𝑡𝑖 ∈ S𝑖

13:
14: end for
15: return ∀S𝑖 ∈ S

(𝑡) using eq. (5)

for energy dispatch in the considered network. The proposed
MAMRL framework includes two types of agents: A local
agent that acts as a local learner at each self-powered with
MEC capabilities BS and a meta-agent that learns the global
energy dispatch policy. In particular, each local BS agent
can discretize the Markovian dynamics for energy demand-
generation of each BS (i.e., both SBSs and MBS) separately
by applying deep-reinforcement learning. Meanwhile, we train
a long short-term memory (LSTM) [49], [50] as a meta-
agent at the MBS that optimizes [26] the accumulated energy
dispatch of the local agents. As a result, the meta-agent can
handle the non-i.i.d. energy demand-generation of the each
local agent with own state information of the LSTM. To this
end, MAMRL mitigates the curse of dimensionality for the
uncertainty of energy demand and generation while providing
an energy dispatch solution with a less computational and
communication complexity (i.e., less message passing between
the local agents and meta-agent).

A. Preliminary Setup

(𝑡), 𝐶sto

𝑡𝑖 , 𝐶non
𝑡𝑖

In the MAMRL setting, each BS 𝑖 ∈ B acts as a local agent
and the number of local agents is equal to 𝐵 + 1 BSs (i.e., 1
MBS and 𝐵 SBSs). We deﬁne a set S = {S0, S1, . . . , S𝐵} of
state spaces and a set A = {A0, A1, . . . , A𝐵} of actions for
the 𝐵 + 1 agents. The state space of a local agent 𝑖 is deﬁned
by 𝒔𝑡𝑖 : (𝜉d
𝑖 , 𝜉ren
(𝑡), 𝐶sto
𝑡𝑖 ,
𝑖
and 𝐶non
represent the amount of energy demand, renewable
𝑡𝑖
generation, storage cost, and non-renewable energy cost, re-
spectively, at time 𝑡. We execute Algorithm 1 to generate the
state space for every BSs 𝑖 ∈ B, individually. In Algorithm
1, lines 3 to 6 calculate the individual energy consumption of
the MEC computation and network operation using (2) and
(4), respectively. Overall, the energy demand of the BS 𝑖 is
computed in line 7 and the self-powered energy generation is

) ∈ S𝑖, where 𝜉d

𝑖 , 𝜉ren
𝑖

Fig. 3: Multi-agent meta-reinforcement learning framework.

the historical observation over time. In particular, we capture
the dynamics of Markovian by considering a data-driven
approach. This approach can overcome the conservativeness
of theoretical probability distribution as historical observation
goes to ﬁnitely many.

To prevalence the aforementioned contemporary, we pro-
pose a multi-agent meta-reinforcement
learning framework
that can explore the Markovian behavior from historical energy
demand and generation of each BS 𝑖 ∈ B. Meanwhile, meta-
agent can cope with such time-varying features to a globally
optimal energy dispatch policy for each BS 𝑖 ∈ B.

We design an MAMRL framework by converting the cost
minimization problem (16) to a reward maximization prob-
lem that we then solve with a data-driven approach. In the
MAMRL setting, each agent works as a local agent for each
BS 𝑖 ∈ B and determines an observation (i.e., exploration)
for the decision variables, renewable 𝜉ren
𝑡𝑖 , non-renewable 𝜉non
,
𝑡𝑖
and storage 𝜉sto
𝑡𝑖 energy. The goal of this exploration is to ﬁnd
time-varying features from the local historical data so that the
energy demand 𝜉d
𝑡𝑖 of the network is satisﬁed. Furthermore,
using these observations and current state information, a meta-
agent is used to determine a stochastic energy dispatch policy.
Thus, to obtain such dispatch policy, the meta-agent only
requires the observations (behavior) from each local agent.
Then, the meta-agent can evaluate (exploit) behavior toward an
optimal decision for dispatching energy. Further, the MAMRL
approach is capable of capturing the exploration-exploitation
tradeoff in a way that the meta-agent optimizes decisions of the
each self-powered BS under uncertainty. A detailed discussion
of the MAMRL framework is given in the following section.

IV. ENERGY DISPATCH WITH MULTI-AGENT
META-REINFORCEMENT LEARNING FRAMEWORK

In this section, we developed our proposed multi-agent
meta-reinforcement learning framework (as seen in Fig. 3)

Meta Agent Learning…LSTM for Meta-Policy Optimizer Energy Environment BS i=0Local Agent BS i=0StateRewardActionEnergy Environment BS i=BLocal Agent BS i=BStateObservation MemoryMeta-AdvantageFunctionShared Neural Networks Meta-PolicyRNN State Parameter UpdateObservationsValue FunctionRewardActionValue FunctionLocal Agent Policy GradientLocal Agent Policy GradientLocal AdvantageFunctionACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

10

estimated by line 8 in Algorithm 1. Non-renewable and storage
energy costs are calculated in lines 9 and 10 for time slot 𝑡.
Finally, line 11 creates state space tuple (i.e., 𝒔𝑡𝑖) for time 𝑡
in Algorithm 1.

B. Local Agent Design

𝑖

𝑖

Consider each local BS agent 𝑖 ∈ B that can take two
types of actions 𝜉sto
(𝑡) and 𝜉non
(𝑡) which are the amount
𝑖
of storage energy 𝜉sto
(𝑡), and the amount of non-renewable
𝑖
energy 𝜉non
(𝑡) at time 𝑡. We consider a discrete set of actions
(𝑡)) ∈ A𝑖 for
that consists of two actions 𝒂𝑡𝑖 : (𝜉sto
𝑖
each BS unit 𝑖 ∈ B. Since the state 𝒔𝑡𝑖 and action 𝒂𝑡𝑖 both
contain a time varying information of the agent 𝑖 ∈ B, we
consider the dynamics of Markovian and represent problem
(16) as a discounted reward maximization problem for each
agent 𝑖 (i.e., each BS). Thus, the objective function of the
discounted reward maximization problem of agent 𝑖 is deﬁned
as follows [28]:

(𝑡), 𝜉non
𝑖

𝑟𝑖 (𝒂𝑡𝑖, 𝒔𝑡𝑖) = max
𝒂𝑡𝑖 ∈A𝑖

E𝒂𝑡𝑖∼𝒔𝑡𝑖 [

∞
∑︁

𝑡(cid:48)=𝑡

𝛾𝑡(cid:48)−𝑡 Υ𝑡 (𝒂𝑡𝑖, 𝒔𝑡𝑖)],

(17)

is a discount factor and each reward

where 𝛾 ∈ (0, 1)
Υ𝑡 𝒂𝑡𝑖, 𝒔𝑡𝑖) is considered as,
(cid:40)

Υ𝑡 (𝒂𝑡𝑖, 𝒔𝑡𝑖) =

1, if
0,

𝜉 ren
𝑡𝑖
𝜉 d
𝑡𝑖

> 1,
otherwise.

𝑖

𝑖

𝑖

(𝑡)) = 𝑃(𝒂𝑡𝑖 = 𝜉sto

Here, the optimal value function (20) learns a parameterized
policy 𝜋 𝜃𝑖 (𝒂𝑡𝑖 |𝒔𝑡𝑖; 𝜃𝑖) by using an LSTM-based Q-networks
for the parameters 𝜃𝑖. Thus, each BS agent 𝑖 ∈ B determines
its parametrized energy dispatch policy 𝜋 𝜃𝑖 (𝒂𝑡𝑖 |𝒔𝑡𝑖; 𝜃𝑖) =
𝑃(𝒂𝑡𝑖 |𝒔𝑡𝑖; 𝜃𝑖), where 𝑃(𝜉sto
(𝑡)|𝒔𝑡𝑖; 𝜃𝑖) and
𝑃(𝜉non
(𝑡)) for the parameters 𝜃𝑖. The decision
(𝑡)) = 1− 𝑃(𝜉sto
𝑖
of each BS agent 𝑖 ∈ B relies on 𝜃𝑖. In particular, energy
dispatch policy 𝜋 𝜃𝑖 is the probability of taking action 𝒂𝑡𝑖 for
a given state 𝒔𝑡𝑖 with parameters 𝜃𝑖. In this setting, each local
agent 𝑖 ∈ B is comprised of an actor and a critic [27], [51].
The policy of energy dispatch is determined by choosing an
action in (20) that can be seen as an actor of BS agent 𝑖.
Meanwhile, the value function (19) is estimated by a critic of
each local BS agent 𝑖. The critic can criticize actions that are
made by the actor of each BS agent 𝑖. Therefore, each BS
agent 𝑖 ∈ B can determine a temporal difference (TD) error
[51] based on the current energy dispatch policy of the actor
and value estimation by the critic. The TD error is considered
as an advantage function and the advantage function of agent
𝑖 is deﬁned as follows:

Λ 𝜋𝜃𝑖 (𝒔𝑡𝑖, 𝒂𝑡𝑖) =
(cid:16)

𝑟𝑖 (𝒂𝑡𝑖, 𝒔𝑡𝑖) +

∞
∑︁

𝑡(cid:48)=𝑡

𝛾𝑡(cid:48)−𝑡𝑉 𝜋𝜃𝑖 (𝒔𝑡(cid:48)𝑖)

(cid:17)

− 𝑉 𝜋𝜃𝑖 (𝒔𝑡𝑖).

(21)

(18)

Thus, the policy gradient of each BS agent 𝑖 ∈ B is determined
as,

𝜉 ren
𝑡𝑖
𝜉 d
𝑡𝑖

determines a ratio between renewable energy
In (18),
generation and energy demand (supply-demand ratio) of the
BS agent 𝑖 ∈ B at time 𝑡. When renewable energy generation-
is larger than 1 then the BS agent 𝑖 achieves
demand ratio
a reward of 1 because the amount of renewable energy exceeds
the demand that can be stored in the storage unit.

𝜉 ren
𝑡𝑖
𝜉 d
𝑡𝑖

Each action 𝒂𝑡𝑖 of BS agent 𝑖 ∈ B determines a stochastic
policy 𝜋 𝜃𝑖 . 𝜃𝑖 is a parameter of 𝜋 𝜃𝑖 and the energy dispatch
policy is deﬁned by 𝜋 𝜃𝑖 : S𝑖 × A𝑖 ↦→ [0, 1]. Policy 𝜋 𝜃𝑖 decides
a state transition function Γ : S𝑖 × A𝐵 ↦→ S𝑖 for the next
state 𝒔𝑡(cid:48)𝑖 ∈ S𝑖. Thus, the state transition function Γ of BS
agent 𝑖 ∈ B is determined by a reward function (18), where
↦→ R. Further, each BS agent 𝑖 ∈ B
Υ𝑡 (𝒂𝑡𝑖, 𝒔𝑡𝑖) : S𝑖 × A𝑖
chooses an action 𝒂𝑡𝑖 from a parametrized energy dispatch
policy 𝜋 𝜃𝑖 (𝒂𝑡𝑖 |𝒔𝑡𝑖; 𝜃𝑖). Therefore, for a given state 𝒔𝑡𝑖, the state
value function with a cumulative discounted reward will be:

𝑉 𝜋𝜃𝑖 (𝒔𝑡𝑖) = E𝒂𝑡𝑖∼ 𝜋𝜃𝑖 (𝒂𝑡𝑖 |𝒔𝑡𝑖;𝜃𝑖) [

∞
∑︁

𝛾𝑡(cid:48)−𝑡 Υ𝑡+𝑡(cid:48) (𝒂𝑡𝑖, 𝒔𝑡𝑖)|𝒔𝑡𝑖, 𝒂𝑡𝑖],

𝑡(cid:48)=𝑡

(19)
where 𝛾𝑡(cid:48)−𝑡 is a discount factor and ensures the convergence
of state value function 𝑉 𝜋𝜃𝑖 (𝒔𝑡𝑖) over the inﬁnity time horizon.
Thus, for a given state 𝒔𝑡𝑖, the optimal policy 𝜋∗
(𝒂𝑡𝑖 |𝒔𝑡𝑖) for
𝜃𝑖
the next state 𝒔𝑡(cid:48)𝑖 can be determined by an optimal state value
function while a Markovian property is imposed. Therefore,
the optimal value function is given as follows:

∇𝜃𝑖 Λ 𝜋𝜃𝑖 (𝒔𝑡𝑖, 𝒂𝑡𝑖) =
∞
∑︁

E 𝜋𝜃𝑖 [

𝑡(cid:48)=𝑡

𝛾𝑡(cid:48)−𝑡 ∇𝜃𝑖 log 𝜋 𝜃𝑖 (𝒂𝑡𝑖 |𝒔𝑡𝑖; 𝜃𝑖)Λ 𝜋𝜃𝑖 (𝒔𝑡𝑖, 𝒂𝑡𝑖)],

(22)

where log 𝜋 𝜃𝑖 (𝒂𝑡𝑖 |𝒔𝑡𝑖; 𝜃𝑖), and Λ 𝜋𝜃𝑖 (𝒔𝑡𝑖, 𝒂𝑡𝑖) represent the ac-
tor and critic, respectively, for each local BS agent 𝑖 ∈ B.

𝑖

(𝑡), 𝜉non
𝑖

Using (22), we can discretize the energy dispatch decision
𝒂𝑡𝑖 : (𝜉sto
(𝑡)) for each self-powered BS 𝑖 ∈ B in the
network. In fact, we can achieve a centralized solution for
∀𝑖 ∈ B when all of the BSs state information (i.e., demand
and generation) are known. However, the space complexity
for computation increases as 𝑂 (2 |S𝑖 |×|A𝑖 |×|B |×𝑇 ) and also the
computational complexity becomes 𝑂 (|S𝑖 | × |A𝑖 | × |B|2 × 𝑇)
[21]. Further,
the exploration-
the solution does not meet
exploitation dilemma since the centralized (i.e., single agent)
method ignores the interactions and energy dispatch decision
strategies of other agents (i.e., BSs) which creates an imbal-
ance between exploration and exploitation. In other words, this
learning approach optimizes the action policy by exploring
its own state information. Therefore, when we change the
energy environment (i.e., demand and generation), this method
cannot cope with an unknown environment due to the lack of
diverse state information during the training. Next, we propose
an approach that not only reduces the complexity but also
explores alternative energy dispatch decision to achieve the
highest expected reward in (17).

𝑉 𝜋∗

𝜃𝑖 (𝒔𝑡𝑖) =

max
𝑎𝑡 ∈A

E 𝜋∗
𝜃𝑖

[

∑︁

𝑖 ∈B

𝑟𝑖 (𝒂𝑡(cid:48)𝑖, 𝒔𝑡(cid:48)𝑖) +

∞
∑︁

𝑡(cid:48)=𝑡

𝛾𝑡(cid:48)−𝑡𝑉 𝜋𝜃𝑖
𝑡(cid:48)

(𝒔𝑡(cid:48)𝑖)|𝒔𝑡𝑖; 𝜃𝑖, 𝒂𝑡𝑖].

(20)

C. Multi-Agent Meta-Reinforcement Learning Modeling

We consider a set O = {O0, O1, . . . , O𝐵} of 𝐵 + 1 ob-
servations [24], [52] and for an BS agent 𝑖 ∈ B, a single

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

11

observation tuple is given by 𝒐𝑖 ∈ O𝑖. For a given state
𝒔𝑡𝑖,
the observation 𝒐𝑖 of the next state 𝒔𝑡(cid:48)𝑖 consists of
𝒐𝑖 : (𝑟𝑖 (𝒂𝑡(cid:48)𝑖, 𝒔𝑡(cid:48)𝑖), 𝑟𝑖 (𝒂𝑡𝑖, 𝒔𝑡𝑖), 𝒂𝑡𝑖, 𝒂𝑡(cid:48)𝑖, 𝑡 (cid:48), Λ 𝜋𝜃𝑖 (𝒔𝑡𝑖, 𝒂𝑡𝑖)), where
𝑟𝑖 (𝒂𝑡(cid:48)𝑖, 𝒔𝑡(cid:48)𝑖), 𝑟𝑖 (𝒂𝑡𝑖, 𝒔𝑡𝑖), 𝒂𝑡𝑖, 𝒂𝑡(cid:48)𝑖, 𝑡 (cid:48) and Λ 𝜋𝜃𝑖 (𝒔𝑡𝑖, 𝒂𝑡𝑖) are next-
state discounted rewards, current state discounted rewards,
next action, current action, time slot, and TD error, respec-
tively. Here, a complete information of the observation 𝒐𝑖 is
correlated with the state space 𝒐𝑖 : S𝑖 ↦→ O𝑖 while observation
𝒐𝑖 does not require the complete state information of the
previous states.

Thus, the space complexity for computation at each BS
𝑖 ∈ B leads to 𝑂 ((|S𝑖 | + |A𝑖 |)2 × 𝑇). Meanwhile,
agent
the computational complexity for each time slot 𝑡 becomes
𝑂 (|S𝑖 |2×A𝑖 ×𝜃𝑡 +𝐻), where 𝜃𝑡 is the learning parameter and 𝐻
represents the numbers of LSTM units. Each BS agent 𝑖 ∈ B
requires to send an amount of |O𝑖 | observational data (i.e.,
the communication
payload) to the meta-agent. Therefore,
overhead for each BS agent 𝑖 ∈ B leads to 𝑂 ( |O |×𝑇
𝐵+1 ). On
the other hand, the computational complexity of the meta-
agent leads to 𝑂 (|O|2 × 𝜙 + 𝐻) while 𝜙 represents learning
parameter at meta-agent. In particular, for a ﬁxed number of
output memory 𝜙, the meta-agent’s update complexity at each
time slot 𝑡 becomes 𝑂 (𝜙2) [53]. Further, when transferring
the learned parameters 𝜃𝑡(cid:48) from the meta-agent to all local
agents ∀𝑖 ∈ B,
the communication overhead goes to the
𝑂 (𝜃𝑡(cid:48) ×(𝐵+1)) at each time slot 𝑡. Here, the size of 𝜃𝑡(cid:48) depends
on the memory size of the LSTM cell at the meta-agent [see
Appendix A].

In the MAMRL framework, the local agents work as an
optimizee and the meta-agent performs the role of optimizer
[26]. To model our meta-agent, we consider an LSTM archi-
tecture [49], [50] that stores its own state information (i.e.,
parameters) and the local agent (i.e., optimizee) only provides
the observation of a current state. In the proposed MAMRL
framework, a policy 𝜋 𝜃𝑖 is determined by updating the parame-
ters 1 𝜃𝑖. Therefore, we can represent the state value function
(20) for time 𝑡 is as follows: 𝑉 𝜋∗
𝜃𝑖 (𝒔𝑡𝑖) ≈ 𝑉 𝜋𝜃𝑖 (𝒔𝑡𝑖; 𝜃𝑡 ), and
the advantage (temporal difference) function (21) is presented
by, Λ 𝜋𝜃𝑖 (𝒔𝑡𝑖, 𝒂𝑡𝑖) ≈ Λ 𝜋𝜃𝑖 (𝒔𝑡𝑖, 𝒂𝑡𝑖; 𝜃𝑡 ). As a result, the param-
eterized policy is deﬁned by, 𝜋 𝜃𝑖 (𝒂𝑡𝑖 |𝒔𝑡𝑖) ≈ 𝜋 𝜃𝑖 (𝒂𝑡𝑖 |𝒔𝑡𝑖; 𝜃𝑡 ).
Considering all of the BS agents 𝐵 + 1 and the advantage
function (21) is rewritten as,

Λ

𝜋∗
𝜃𝑖 (𝒔𝑡𝑖, 𝒂𝑡0, . . . , 𝒂𝑡 𝐵; 𝜃𝑡 ) = 𝑟𝑖 (𝒔𝑡𝑖, 𝒂𝑡0, . . . , 𝒂𝑡 𝐵) +
∞
∑︁

𝛾𝑡(cid:48)−𝑡 Γ(𝒔𝑡(cid:48)𝑖 |𝒔𝑡𝑖, 𝒂𝑡0, . . . , 𝒂𝑡 𝐵)𝑉 𝜋𝜃𝑖 (𝒔𝑡(cid:48)𝑖, 𝜋∗
𝜃0

, . . . , 𝜋∗

𝜃𝐵 ) −

𝒔𝑡(cid:48)𝑖 ∈S𝑖 ,𝑡(cid:48)=𝑡

𝑉 𝜋𝜃𝑖 (𝒔𝑡𝑖, 𝜋∗
𝜃0

, . . . , 𝜋∗

𝜃𝐵 ),

(23)

, . . . , 𝜋∗

𝜃 : (𝜋∗
𝜃0

where 𝜋∗
) is a joint energy dispatch policy and
Γ(𝒔𝑡(cid:48)𝑖 |𝒔𝑡𝑖, 𝒂𝑡0, . . . , 𝒂𝑡 𝐵) ↦→ [0, 1] represents state transition
probability. Using (23), we can get the value loss function for

𝜃𝐵

agent 𝑖 and the objective is to minimize the temporal difference
[27],

𝐿 (𝜃𝑖) =

(cid:18) (cid:16)

min
𝜋𝜃𝑖

1
|B|

∑︁

𝑖 ∈B

1
2

𝑟𝑖 (𝒂𝑡𝑖, 𝒔𝑡𝑖) +

∞
∑︁

𝑡(cid:48)=𝑡

𝛾𝑡(cid:48)−𝑡𝑉 𝜋∗

𝜃𝑖 (𝒔𝑡(cid:48)𝑖 |𝜃𝑡 )

(cid:17)

− 𝑉 𝜋∗

𝜃𝑖 (𝒔𝑡𝑖)

(cid:19) 2

.

(24)

To improve the exploration with a low bias, we consider
an entropy regularization 2 𝛽ℎ(𝜋 𝜃𝑖 (𝒂𝑡𝑖 |𝒔𝑡𝑖; 𝜃𝑡 )) that cope with
the non-i.i.d. energy demand and generation for all of the BS
agents ∀𝑖 ∈ B. Here, 𝛽 is a coefﬁcient for the magnitude of
regularization and ℎ(𝜋 𝜃𝑖 (𝒂𝑡𝑖 |𝒔𝑡𝑖; 𝜃𝑡 )) determines the entropy of
the policy 𝜋 𝜃𝑖 for the parameter 𝜃𝑖. Additionally, a larger value
of 𝛽ℎ(𝜋 𝜃𝑖 (𝒂𝑡𝑖 |𝒔𝑡𝑖; 𝜃𝑡 )) encourages the agents to have a more
diverse exploration to estimate the energy dispatch policy.
Thus, we can redeﬁne the policy loss function as follows:

𝐿(𝜃𝑖) = −E𝒔𝑡𝑖 ,𝒂𝑡𝑖 [𝜋 𝜃𝑖 (𝒂𝑡𝑖 |𝒔𝑡𝑖) + 𝛽ℎ(𝜋 𝜃𝑖 (𝒂𝑡𝑖 |𝒔𝑡𝑖; 𝜃𝑡 ))].

(25)

Therefore, the policy gradient of the loss function (25) is
deﬁned in terms of temporal difference and entropy. The policy
gradient of the loss function is deﬁned as follows:

∇𝜃𝑖 𝐿(𝜃𝑖) =

1
|B|

∑︁

∞
∑︁

∇𝜃𝑖 log 𝜋 𝜃𝑖 (𝒂𝑡𝑖 |𝒔𝑡𝑖)Λ 𝜋𝜃𝑖 (𝒔𝑡𝑖, 𝒂𝑡𝑖 |𝜃𝑡 )

𝑖 ∈B

𝑡(cid:48)=𝑡
+ 𝛽ℎ(𝜋 𝜃𝑖 (𝒂𝑡𝑖 |𝒔𝑡𝑖; 𝜃𝑡 )).

(26)

𝑖 ∈ B at

To design our meta-agent, we consider meta-agent param-
eters 𝜙 and optimized parameters 𝜃∗ of the optimizee (i.e.,
is deﬁned as 𝑀𝑡 (O𝑡 ; 𝜙) (cid:66)
local agent). The meta-agent
𝑀𝑡 (∇𝜃𝑡 𝐿(𝜃𝑡 ); 𝜙), where 𝑀𝑡 (.)
is modeled by an LSTM.
Consider an observational vector O𝑖𝑡(cid:48) ∈ O of a local
time 𝑡 (cid:48) and each observation is
BS agent
𝒐𝑖 : (𝑟𝑖 (𝒂𝑡(cid:48)𝑖, 𝒔𝑡(cid:48)𝑖), 𝑟𝑖 (𝒂𝑡𝑖, 𝒔𝑡𝑖), 𝒂𝑡𝑖, 𝒂𝑡(cid:48)𝑖, 𝑡 (cid:48), Λ 𝜋𝜃𝑖 (𝒔𝑡𝑖, 𝒂𝑡𝑖)) ∈ O𝑖𝑡(cid:48).
The LSTM-based meta-agent takes the observational vector
O𝑖𝑡(cid:48) as an input. Meanwhile,
the meta-agent holds long-
term dependencies by generating its own state with param-
eters 𝜙. To do this, the LSTM model creates several gates
to determine an optimal policy 𝜋∗
𝜃𝑖 and advantage function
𝜋∗
𝜃𝑖 (𝒔𝑡𝑖, 𝒂𝑡0, . . . , 𝒂𝑡 𝐵; 𝜃𝑡 ) for the next state 𝒔𝑡(cid:48)𝑖. As a result,
Λ
the structure of the recurrent neural network for the meta-agent
is the same as the LSTM model [49], [50]. In particular, each
LSTM unit for the meta-agent consists of four gate layers such
as forget gate 𝑭𝑡(cid:48), input gate 𝑰𝑡(cid:48), cell state ˆ𝑬𝑡(cid:48), and output 𝒁𝑡(cid:48)
layer. The cell state gate ˆ𝑬𝑡(cid:48) usages a tanh activation function
and other gates are used sigmoid 𝜎(.) as an activation function.

1We consider recurrent neural networks (RNNs) state parameters for the
parameterization of energy dispatch policy. In particular, we consider a long
short-term memory (LSTM) for RNN, in which cell state and hidden state
are considered as parameters.

2Entropy [54]–[57] can allow us to manage non-i.i.d. datasets when changes
in the environment over time lead to an uncertainty. Therefore, we use entropy
regularization to handle the non-i.i.d. energy demand and generation over time
by managing with the uncertainty for each BS agent 𝑖 ∈ B.

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

12

Thus, the outcome of the meta policy for a single unit LSTM
cell is presented as follows:

𝑀𝑡(cid:48) (O𝑡(cid:48); 𝜙) = softmax

(cid:16)

(𝑯𝑡(cid:48))(cid:62)(cid:17)

,

(cid:16)

where 𝑭𝑡(cid:48) = 𝜎
(cid:16)

𝑰𝒕(cid:48) = 𝜎

𝜙𝐹 𝑂 (O𝑖𝑡(cid:48))(cid:62) + 𝜙𝐹 𝐻 (𝑯𝑡 )(cid:62) + 𝒃𝐹
(cid:17)

𝜙𝐼 𝑂 (O𝑖𝑡(cid:48))(cid:62) + 𝜙𝐼 𝐻 (𝑯𝑡 )(cid:62) + 𝒃𝐼

,

(cid:17)

,

𝜙𝐸𝑂 (O𝑖𝑡(cid:48))(cid:62) + 𝜙𝐸 𝐻 (𝑯𝑡 )(cid:62) + 𝒃𝐸

(cid:16)

ˆ𝑬𝒕(cid:48) = tanh
𝑬𝑡(cid:48) = ˆ𝑬𝒕(cid:48) (cid:12) 𝑰𝑡(cid:48) + 𝑭𝑡(cid:48) (cid:12) 𝑬𝑡 ,
𝒁𝑡(cid:48) = 𝜎
𝑯𝑡(cid:48) = tanh(𝑬𝑡(cid:48)) (cid:12) 𝒁𝑡(cid:48).

(cid:16)

𝜙𝑍 𝑂 (O𝑖𝑡(cid:48))(cid:62) + 𝜙𝑍 𝐻 (𝑯𝑡 )(cid:62) + 𝒃𝑍

(cid:17)

,

(27)

(27a)

(27b)

(cid:17)

, (27c)

(27d)

(27e)

(27f)

In the meta-agent policy formulation (27), the forget gate
vector (27a) determines what information is needed to throw
away. Input gate vector (27b) helps to decide which informa-
tion is needed to update, the cell state (27c) creates a vector
of new candidate values using tanh(·) function, and updates
the cell state information by applying (27d). The output layer
(27e) that determines what parts of the cell state are going
to output and calculate the cell outputs using the equation
(27f). Further, the cell state through the tanh(·) will restrict
the values between −1 and +1. This entire process is followed
for each LSTM block and ﬁnally, (27) determines the meta-
policy for 𝜋∗
𝜃𝑖 of the state 𝑠𝑡(cid:48). In addition, optimized RNN
state parameters 𝜃∗ are obtained from the cell state (27d) and
hidden state (27f) of an LSTM unit. Thus, the loss function
𝐿 (𝜙) = E𝐿 ( 𝜃) [𝐿 (𝜃∗(𝐿(𝜃); 𝜙))] of meta-agent depends on the
distribution of 𝐿(𝜃𝑡 ) and the expectation of the meta-agent
loss function is deﬁned as follows [26]:

𝐿(𝜙) = E𝐿 ( 𝜃) [

𝑇
∑︁

𝑡=1

𝐿 (𝜃𝑡 )].

(28)

In the proposed MAMRL framework, we transfer the learned
parameters (i.e., cell state and hidden state) of meta-agent to
the local agents so that each local agent will be estimated an
optimal energy dispatch policy by updating its own learning
parameters. Thus, the parameters of each agent (i.e., BS) is
updated with 𝜃𝑡(cid:48) = 𝜃∗ while 𝜋∗
= 𝑀𝑡 (∇𝜃𝑡 𝐿 (𝜃𝑡 ); 𝜙) to decide
𝜃𝑖
the energy dispatch policy.

We consider an LSTM-based recurrent neural network
(RNN) for the both local agents and the meta-agent. This
LSTM RNN consists of 48 LSTM units for each LSTM cell as
shown in Fig. 4. In particular, the conﬁguration of the LSTM
for the meta-agent and each local agent is the same while
the objective of the loss functions differ from local agent
to meta-agent. In which, local BS agent determines its own
energy dispatch policy by exploring its own environmental
state information for reducing the TD error. Meanwhile,
meta-agent deals with the observations of each local BS agent
by exploiting its own RNN states information using entropy
based loss function to capture non-i.i.d. energy demand and
generation of each local BS. Therefore, having different
loss functions for local and meta agent leads the proposed
MAMRL model to learn a domain speciﬁc generalized model
so that it can cope with an unknown environment. Further,

this RNN consists of a branch of two fully connected output
layers on top of the LSTM cell. In particular, fully connected
layer with a softmax activation is considered for energy
dispatch policy determination, and another fully connected
output
layer without activation function is deployed for
value function estimation. Thus, the advantage is calculated
based on value function estimation from the second fully
connected layer. Each local LSTM-based RNN receives a
current reward 𝑟𝑖 (𝒂𝑡𝑖, 𝒔𝑡𝑖), current action 𝒂𝑡𝑖, and next time
slot 𝑡 (cid:48) as an input for each BS agent 𝑖 ∈ B. Meanwhile,
this local LSTM model estimates a policy 𝜋 𝜃𝑖 and value
𝑉 𝜋𝜃𝑖 (𝒔𝑡𝑖) for BS agent 𝑖 ∈ B. On the other hand, meta
agent LSTM-based RNN feeds input as an observational
𝒐𝑖 : (𝑟𝑖 (𝒂𝑡(cid:48)𝑖, 𝒔𝑡(cid:48)𝑖), 𝑟𝑖 (𝒂𝑡𝑖, 𝒔𝑡𝑖), 𝒂𝑡𝑖, 𝒂𝑡(cid:48)𝑖, 𝑡 (cid:48), Λ 𝜋𝜃𝑖 (𝒔𝑡𝑖, 𝒂𝑡𝑖))
tuple
from each BS agent 𝑖 ∈ B. This observation consists of the
current and next reward, current and next action, next time
slot, and TD error for each BS agent 𝑖. Thus,
this meta
agent estimates parameters 𝜃𝑡(cid:48)
to ﬁnd a globally optimal
𝜃𝑖 for each BS 𝑖 ∈ B. The learned
energy dispatch policy 𝜋∗
parameters of the meta-agent are transferred to each local BS
agent 𝑖 ∈ B asynchronously while this local agent updates its
own parameters for estimating the globally optimal energy
dispatch policy via the local LSTM-based RNN. In particular,
the learned parameters (i.e., RNN states) are transfered from
meta-agent to each local agent 𝑖 ∈ B. Additionally, these
RNN state parameters include cell state and hidden state of
the LSTM cell, which do not depend on any of the fully
the proposed RNN architecture.
connected out
Meanwhile, each local agent 𝑖 ∈ B updates its own RNN
states using the transferred parameters by the meta-agent. We
consider a cellular network for exchanging observations and
parameters between local BS agent and meta-agent.

layers of

We run the proposed Algorithm 2 at each self-powered
BS 𝑖 ∈ B with MEC capabilities as local agent
𝑖. The
input of Algorithm 2 is the state information S𝑖 of local
agent 𝑖, which is the output from Algorithm 1. The cumu-
lative discounted reward (17) and state value in (19) are
calculated in lines 5 and 6, respectively (in Algorithm 2)
for each step (until the maximum step size 3 for time step
𝑡). Consequently, based on a chosen action 𝒂𝑡𝑖
from the
estimated policy 𝜋 𝜃𝑖 (𝒂𝑡𝑖 |𝒔𝑡𝑖) (in line 7), episode buffer is
generated and appended in line 8. Advantage function (21)
of local agent 𝑖 is evaluated in line 12 and the policy gradient
(22) is calculated in line 13 using an LSTM-based local
neural network. Algorithm 2 generates observational
tuple
𝒐𝑖 : (𝑟𝑖 (𝒂𝑡(cid:48)𝑖, 𝒔𝑡(cid:48)𝑖), 𝑟𝑖 (𝒂𝑡𝑖, 𝒔𝑡𝑖), 𝒂𝑡𝑖, 𝒂𝑡(cid:48)𝑖, 𝑡 (cid:48), Λ 𝜋𝜃𝑖 (𝒔𝑡𝑖, 𝒂𝑡𝑖)) in line
15. Here, we transfer the knowledge of local BS agent 𝑖 ∈ B
to the meta-agent learner (deployed in MBS) in Algorithm 3
so as to optimize the energy dispatch decision (in Algorithm
2 line 16). Hence, the observation tuple 𝒐𝑖 of local BS agent 𝑖
consists of only the decision from BS 𝑖, where does not require
to send all of the state information to meta-agent learner.
Employing the meta-agent policy gradient, each local agent
is capable of updating the energy dispatch decision policy in

3To capture the heterogeneity for energy demand and generation of each
BS separately, we consider the same number of user tasks that are executed
by each BS agent 𝑖 ∈ B during one observational period 𝑡 as the steps size.

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

13

Fig. 4: Recurrent neural network architecture for the proposed multi-agent meta-reinforcement learning framework.

line 17 in Algorithm 2. Finally, the energy dispatch policy is
executed in line 20 at the BS 𝑖 ∈ B by local agent 𝑖.

The meta-agent

learner (Algorithm 3 in MBS) receives
the observations O𝑖 ∈ O from each local BS agent 𝑖 ∈ B
asynchronously. Then the meta-agent asynchronously updates
the meta policy gradient of the each BS agent 𝑖 ∈ B. Lines
from 4 to 12 of Algorithm 3 represent the LSTM block for
the meta-agent. In Algorithm 3, entropy loss (25) and gradient
of the loss (26) are estimated in lines 6 and 7, respectively. In
order to estimate this, Algorithm 3 deploys a fully connected
output layer without activation function, so that advantage loss
can be calculated without affecting the value that is calculated
by the value function of the proposed MAMRL framework.
The meta-agent energy dispatch policy is updated in line 10
of Algorithm 3. Before that, a fully connected output layer
with a softmax activation function of the LSTM cell assists
to determine the energy dispatch policy and meta policy loss
in lines 8 and 9 (in Algorithm 3), respectively, for the meta-
agent. Additionally, the meta-agent utilizes the observations
of the local agents and determines its own state information
that helps to estimate the energy dispatch policy of the meta-
agent. In line 11, the meta-agent RNN states 𝜃∗ (i.e., cell and
hidden states) are received from the considered LSTM cell in
Algorithm 3. Finally, the meta-agent policy and RNN states
are transfered to each BS agent for updating the parameters
(i.e., RNN states) of each local BS agent. To this end, a
meta-agent learner deployed at center node (i.e., MBS) in the
considered network and sends the learning parameters of the
optimal energy dispatch policy to each local BS (i.e., MBS
and SBS) through the network.

The proposed MAMRL framework established a guarantee

to converge with an optimal energy dispatch policy. In fact,
the MAMRL framework can be reduced to a |B|-player
Markovian game [58], [59] as a base problem that establishes
more insight into convergence and optimality. The proposed
MAMRL model has at least one Nash equilibrium point that
ensures an optimal energy dispatch policy. This argument is
similar from the previous studies of |B|-player Markovian
game [58], [59]. Hence, we can conclude with the following
proposition:
Proposition 1. 𝜋∗
𝜃𝑖
that
𝑉 𝜋𝜃𝑖 (𝒔𝑡𝑖, 𝜋∗
𝜃0

is an optimal energy dispatch policy
is an equilibrium point with an equilibrium value

) for BS 𝑖 [see Appendix B].

, . . . , 𝜋∗

𝜃𝐵

We can justify the convergence of MAMRL framework via

the following Proposition:

𝑖

Proposition 2. Consider a stochastic environment with a state
space 𝒔𝑡𝑖 ∈ S, 𝑖 ∈ ∀B of |B| BS agents such that all BS
agents are initialized with an equal probability of 0.5 for a
(𝑡)) = 𝜃𝑖 ≈ 0.5, where
(𝑡)) = 𝑃(𝜉non
binary actions, 𝑃(𝜉sto
(𝑡)) ∈ A𝑖, ∀𝑖 ∈ B, and 𝑟𝑖 (𝒔𝑡𝑖, 𝒂𝑡0, . . . , 𝒂𝑡 𝐵).
𝒂𝑡𝑖 : (𝜉sto
Therefore, to estimate the gradient of loss function (24), we can
establish a relationship among the gradient of approximation
ˆ∇𝜃𝑖 𝐿 (𝜃𝑖) and true gradient ∇𝜃𝑖 𝐿 (𝜃𝑖),
(cid:17)
(cid:16) ˆ∇𝜃𝑖 𝐿(𝜃𝑖), ∇𝜃𝑖 𝐿 (𝜃𝑖) > 0

∝ (0.5) |B | .

(𝑡), 𝜉non
𝑖

(29)

𝑃

𝑖

𝑖

[See Appendix C].

Propositions 1 and 2 validate the optimality and conver-
gence, respectively for the proposed MAMRL framework.
Proposition 1 guarantees an optimal energy dispatch policy.
Meanwhile, Proposition 2 ensures that the proposed MAMRL

RNN of local BS agent i =BLSTMLSTMLSTM…Energy environment BS agent i = 0Energy environment BS agent i = BPeriodic meta parameters (i.e., RNN states) transfer to local agent i =0Periodic meta parameters (i.e., RNN states) transfer to local agent i =BLocal policy and observation of BS agent i = BLocal policy and observation of BS agent i = 0Current reward and action of BS agent i = 0Current reward and action of BS agent i = BMeta agent RNN states (cell state and hidden state)LSTMLSTMLSTMLSTMLSTMLSTMMeta agent fully-connected output layer for policy (softmax activation)Meta agent fully-connected output layer for value (without activation)Local fully-connected output layer for policy (softmax activation)Local fully-connected output layer for value (without activation)Local fully-connected output layer for policy (softmax activation)Local fully-connected output layer for value (without activation)LSTMLSTMLSTMMeta agent LSTM cell consists of 48 LSTM unitsLocal agent LSTM cell consists of 48 LSTM unitsLSTMLSTMLSTMCell and hidden states from meta agentCell and hidden states from meta agentInput to local BS agent i = 0RNNInput to local BS agent i = BRNNOutput of local BS agent i =0RNN and input for meta RNN Meta agent RNNOutput of meta RNNOutput of local BS agent i =BRNN and input for meta RNN ………RNN of local BS agent i =0ActionLocal BS agent i = 0Local BS agent i = BMeta agent at MBSActionACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

14

Algorithm 2 Local Agent Training of Energy Dispatch of BS
𝑖 ∈ B in MAMRL Framework
Input: 𝒔𝑡𝑖 : (𝜉d
𝑡𝑖 , 𝐶non
𝑡𝑖
Output: 𝒐𝑖 : (𝑟𝑖 (𝒂𝑡(cid:48)𝑖, 𝒔𝑡(cid:48)𝑖), 𝑟𝑖 (𝒂𝑡𝑖, 𝒔𝑡𝑖), 𝒂𝑡𝑖, 𝒂𝑡(cid:48)𝑖, 𝑡 (cid:48), Λ 𝜋𝜃𝑖 (𝒔𝑡𝑖, 𝒂𝑡𝑖)),

), ∀𝒔𝑡𝑖 ∈ S𝑖, ∀𝑡 ∈ 𝑇

(𝑡), 𝐶sto

𝑖 , 𝜉ren
𝑖

𝒐𝑖 ∈ O𝑖, ∇𝜃𝑡 𝐿 (𝜃𝑡 )
Initialization: 𝐿𝑜𝑐𝑎𝑙 𝐿𝑆𝑇 𝑀 (.), 𝜃𝑖, 𝑖 ∈ B, 𝛾, O𝑖

Initialization: 𝑒 𝑝𝑐𝐵𝑢 𝑓 []
for each 𝑡 ∈ T do

1: for episode = 1 to maximum episodes do
2:
3:
4:
5:

for 𝑠𝑡𝑒 𝑝 = 1 to 𝑀𝑎𝑥𝑆𝑡𝑒 𝑝 do

4:
5:

6:

7:

8:
9:
10:

11:

Calculate: 𝑟𝑖 (𝒂𝑡𝑖, 𝒔𝑡𝑖) using eq. (17)
Calculate: 𝑉 𝜋𝜃𝑖 (𝒔𝑡𝑖) using eq. (19)
Choose Action: 𝒂𝑡𝑖 ∼ 𝜋 𝜃𝑖 (𝒂𝑡𝑖 |𝒔𝑡𝑖)
Append: 𝑒 𝑝𝑐𝐵𝑢 𝑓 [𝒂𝑡𝑖, 𝑟𝑖 (𝒂𝑡𝑖, 𝒔𝑡𝑖), 𝑡, 𝑠𝑡𝑒 𝑝, 𝑉 𝜋𝜃𝑖 (𝒔𝑡𝑖)]

end for
𝐿𝑜𝑐𝑎𝑙 𝐿𝑆𝑇 𝑀 (𝑟𝑖 (𝒂𝑡𝑖, 𝒔𝑡𝑖), 𝒂𝑡𝑖, 𝑡 (cid:48) = 𝑡 + 1)
{LSTM-based local RNN block}
{
Evaluate: Λ 𝜋𝜃𝑖 (𝒔𝑡𝑖, 𝒂𝑡𝑖) using eq. (21)
Local agent policy gradient: ∇𝜃𝑖 Λ 𝜋𝜃𝑖 (𝒔𝑡𝑖, 𝒂𝑡𝑖) using
eq. (22)
{In (22), 𝜋 𝜃𝑖 (𝒂𝑡𝑖 |𝒔𝑡𝑖; 𝜃𝑖) is determined by a fully
layer with a softmax activation
connected output
function and Λ 𝜋𝜃𝑖 (𝒔𝑡𝑖, 𝒂𝑡𝑖) is calculated through a
fully connected output layer without activation func-
tion}
}
Append:
𝒐𝑖 : (𝑟𝑖 (𝒂𝑡(cid:48)𝑖, 𝒔𝑡(cid:48)𝑖), 𝑟𝑖 (𝒂𝑡𝑖, 𝒔𝑡𝑖), 𝒂𝑡𝑖, 𝒂𝑡(cid:48)𝑖, 𝑡 (cid:48), Λ 𝜋𝜃𝑖 (𝒔𝑡𝑖, 𝒂𝑡𝑖)),
𝒐𝑖 ∈ O𝑖
Get Meta-agent policy 𝜋∗
𝑀𝑡 (O𝑡 ; 𝜙) using Algorithm 3
Update: 𝜃𝑡(cid:48) = 𝜃∗ {RNN states update}

𝜃𝑖 and RNN states 𝜃∗:

6:
7:
8:
9:
10:

11:
12:
13:

14:

15:

16:

17:

end for

18:
19: end for
20: return new_state(𝒔𝑡(cid:48)𝑖 = argmax 𝜋∗
𝜃𝑖

(𝒂𝑡𝑖)), 𝑖 ∈ B

model can meet the convergence for a single state 𝒔𝑡𝑖 ∈ S, 𝑖 ∈
∀B. That implies this model is also able to converge for
∀𝒔𝑡𝑖 ∈ S, 𝑖 ∈ ∀B.

The signiﬁcance of the proposed MAMRL model are ex-

plained as follows:

• First, each BS (i.e., local agent) can explore its own
energy dispatch policy based on individual requirements
for the energy generation and consumption. Meanwhile,
the meta-agent exploits each BS energy dispatch decision
from its own recurrent neural networks state information.
As a result, individual BS anticipates its own energy
demand and generation while meta-agent handles the non-
i.i.d. energy demand and generation for all BS agents to
efﬁciently meet the exploration-exploitation tradeoff of
the proposed MAMRL.

• Second, the proposed MAMRL model can effectively
handle distinct environment dynamics for non-i.i.d. en-
ergy demand and generation among the agents.

• Third, the proposed MAMRL model ensures less infor-

Algorithm 3 Meta-Agent Learner of Energy Dispatch in
MAMRL Framework
Input: 𝒐𝑖 : (𝑟𝑖 (𝒂𝑡(cid:48)𝑖, 𝒔𝑡(cid:48)𝑖), 𝑟𝑖 (𝒂𝑡𝑖, 𝒔𝑡𝑖), 𝒂𝑡𝑖, 𝒂𝑡(cid:48)𝑖, 𝑡 (cid:48), Λ 𝜋𝜃𝑖 (𝒔𝑡𝑖, 𝒂𝑡𝑖)),

Initialization: 𝑀𝑒𝑡𝑎𝐿𝑆𝑇 𝑀 (.), 𝜙, 𝜋 𝜃𝑖 , 𝛾

∀𝒐𝑖 ∈ O𝑖, 𝑡 ∈ T , 𝑖 ∈ B

Output: 𝜙

1: for each 𝑡 ∈ T do
2:
3:

for each 𝑖 ∈ B do

𝒐𝑖 : (𝑟𝑖 (𝒂𝑡(cid:48)𝑖, 𝒔𝑡(cid:48)𝑖), 𝑟𝑖 (𝒂𝑡𝑖, 𝒔𝑡𝑖), 𝒂𝑡𝑖, 𝒂𝑡(cid:48)𝑖, 𝑡 (cid:48), Λ 𝜋𝜃𝑖 (𝒔𝑡𝑖, 𝒂𝑡𝑖)),
𝒐𝑖 ∈ O𝑖
𝑀𝑒𝑡𝑎𝐿𝑆𝑇 𝑀 (O𝑖, 𝜋 𝜃𝑖 ) {LSTM-based RNN block}
{
{Lines from 6 to 7 using fully connected output layer
without activation function}
Entropy loss: 𝐿(𝜃𝑖) using eq. (25)
Gradient of the loss: ∇𝜃𝑡 𝐿 (𝜃𝑡 ) using eq. (26)
{Policy is estimated using a fully connected output
layer with softmax activation function}
Calculate: 𝜋 𝜃𝑖 = 𝑀𝑡 (∇𝜃𝑡 𝐿(𝜃𝑡 ); 𝜙) using eq. (27)
Get meta policy loss 𝐿 (𝜙) using eq. (28)
Update: 𝜋∗
= 𝜋 𝜃𝑖
𝜃𝑖
Get RNN states: 𝜃∗
{cell state and hidden state from the LSTM cell}
}
end for
Send: Meta-agent policy 𝜋∗

𝜃𝑖 and RNN states 𝜃∗

12:
13:
14:
15: end for
16: return

mation exchange between the local agents and meta-
agent. In particular, each local BS agent only sends an
observational vector to meta-agent and received neural
network parameters at the end of 15 minutes observation
period. Additionally, the proposed MAMRL model does
not require sending an entire environment state from each
local agent to the meta-agent.

• Finally, the meta-agent can learn a generalized model
toward the energy dispatch decision and transfer its skill
to each local BS agent. This, in turn, can signiﬁcantly
increase the learning accuracy as well as reduce the com-
putational time for each local BS agent thus enhancing
the robustness of the energy dispatch decision.

We benchmark the proposed MAMRL framework by per-
forming an extensive experimental analysis, and the experi-
mental analysis and discussion are given in the later section.

V. EXPERIMENTAL RESULTS AND ANALYSIS

In our experiment, we use the CRAWDAD nyupoly/video
packet delivery dataset [60] to discretize the self-powered SBS
network’s energy consumption. Further, we choose a state-of-
the-art UMass solar panel dataset [61] to evaluate renewable
energy generation. We create deterministic, asymmetric, and
stochastic environments by selecting different days of the
same solar unit for the generation. Meanwhile, usage several
session from the network packet delivery dataset. We train
our proposed meta-reinforcement learning (Meta-RL)-based

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

15

TABLE IV: Summary of experimental setup

Description
No. of SBSs (no. of local agents)
No. of MEC servers in each SBS
No. of MBS (meta-agent)
Channel bandwidth
System bandwidth
Transmission power
Channel gain
A variance of an AWGN
Energy coefﬁcient for data transfer 𝛿net
MEC server CPU frequency 𝑓
Server switching capacitance 𝜏
(𝑡)
MEC static energy 𝜂MEC

𝑖

st

𝑡

Task sizes (uniformly distributed)
No. of task requests at BS 𝑖
Unit cost renewal energy 𝑐ren
𝑡
Unit cost non-renewal energy 𝑐non
𝑡
Unit cost storage energy 𝑐sto
Initial discount factor 𝛾
Initial action selection probability
One observation period 𝑡
No. of episodes
No. of epochs 𝑇 for each day
No. of steps for each epoch at each agent 𝑖
No. of actions
No. of LSTM units in one LSTM cell
No. of LSTM cells
LSTM cell API BasicLSTMCell(.)
Entropy regularization coefﬁcient 𝛽
Learning rate
Optimizer
Output layer activation function

Value
9
2
1
180 kHz [62]
20 MHz [17]
27 dB [16]
140.7 + 36.7 log 𝑑 [17]
-114 dBm/Hz [62]
2.8 [36]
2.5 GHz [16]
5 × 10−27 (farad) [17]
[7.5, 25] Watts [63]
[31, 1546060] bytes [60]
[1, 10, 000] [11]
$50 per MW-hour [45]
$102 per MW-hour [45]
10% additional [44]
0.9
[0.5, 0.5]
15 minutes
800
96
𝐽𝑖 = [1, 10, 000] [60]
(𝑡))

2 (i.e., 𝜉 sto

𝑖

𝑖

(𝑡), 𝜉 non
48
10 (i.e., B+1)
tf.contrib.rnn [64]
0.05
0.001
Adam [65]
Softmax [51]

(a) MEC network energy demand

(b) Renewable energy generation

Fig. 5: Histogram of energy demand and renewable energy
generation for 9 SBSs and each SBS consists of 96 time slots
after preprocessing using Algorithm 1.

MAMRL framework using deterministic environment and
evaluate the testing performance for the three environments.
Three environments4 are as follows: 1) In the deterministic
environment, both network energy consumption and renewable
generation are known, 2) network energy consumption is
known but renewable generation is unknown in the asymmetric
environment, and 3) the stochastic environment contains both
energy consumption and renewable generation are unknown.
To benchmark the proposed MAMRL framework intuitively,

4For example, we train and test the MAMRL model using the known
(i.e., deterministic environment) network energy consumption, and renewable
generation data of day 1. Then we have tested the trained model using day 2
data, where network energy consumption is known, and renewable generation
is unknown which represents an asymmetric environment. In a stochastic
environment, let us consider day 3 data, where both energy consumption and
renewable generation are unknown to the trained model.

Fig. 6: Reward value achieved for proposed Meta-RL training
of the meta-agent alone with other SBS agents.

we have considered a centralized single-agent deep-RL, multi-
agent centralized A3C deep-RL with a same neural networks
conﬁguration as the proposed MAMRL, and a pure greedy
model as baselines. These are as follows:

• We consider the neural advantage actor-critic (A2C) [51],
[66] method as a centralized single-agent deep-RL. In
particular, the learning environment encompasses the state
information of all BSs ∀𝑖 ∈ B and is learned by a neural
A2C [51], [66] scheme with the same conﬁguration as
the MAMRL model.

• An asynchronous advantage actor-critic (A3C) based
multi-agent RL framework [28] is considered a second
benchmark in a cooperative environment [27]. In partic-
ular, each local actor can ﬁnd its own policy in a decen-
tralized manner while a centralized critic is augmented
with additional policy information. Therefore, this model
is learned by a centralized training with decentralized
execution [28]. We call this model a multi-agent cen-
tralized A3C deep-RL [28]. The environment (i.e., state
information) of this model remains the same for all of the
local actor agents. To ensure a meaningful comparison
with the proposed MAMRL model, we employ this joint
energy dispatch policy using the same advantage function
(23) as the MAMRL model.

• We deploy a pure greedy-based algorithm [51] to ﬁnd the
best action-value mapping. In particular, this algorithm
never takes the risk to choose an unknown action. Mean-
while, it explores other strategies and learns from them so
as to infer more reasonable decisions. Thus, we choose
this upper conﬁdence bounded action selection mecha-
nism [51] as one of the baselines used for benchmarking
our proposed MAMRL model.

We implement our MAMRL framework using multi-
threading programming in Python platform 5, along with
TensorFlow APIs [68]. Table IV shows the key parameters
of this experiment setup.

We prepossess both of the datasets ( [60] and [61]) using
Algorithm 1 that generates the state space information. The
histograms of the network energy demand (in 5(a)) and a
renewable energy generation (in 5(b)) of the deterministic

5MAMRL

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

16

Fig. 7: Reward value achieved of proposed Meta-RL, single-
agent centralized, and multi-agent centralized methods.

environment are shown in Fig. 5. To the best of our knowledge,
there are no publicly available datasets that comprises both
of energy consumption and generation of a self-powered
network with MEC capabilities. Additionally, if we change
the environment using other datasets, the proposed MAMRL
framework can deal with the new, unknown environment by
using the skill transfer feature from the meta-agent to each
the MAMRL approach can
local BS agent. In particular,
readily deal with the case in which the BS agent achieves
a much lower reward due to more variability in consumption
and generation. As a result, the above experiment setup is
reasonable for the benchmarking of the proposed MAMRL
framework.

Fig. 6 illustrates the reward achieved by each local SBS
along with a meta-agent, where we take an average reward
for each 50 episodes. In the MAMRL setting, we design a
maximum reward of 96 (15 minute slot for 24 hours), where
meta-agent converges with a high reward value (around 90).
Hence, all of the local agents converge with around 80 − 85
reward value except the SBS 6 that achieves a reward of 70 at
convergence because its energy consumption and generation
vary more than the others. In fact, this variation of reward
among the BSs is leading to anticipate the non-i.i.d. energy
demand and generation of the considered network as well as
densiﬁcation of the exploration and exploitation tradeoff for
energy dispatch. The proposed approach can adapt the uncer-
tain energy demand and generation over time by characterizing
the expected amount of uncertainty in an energy dispatch
decision for each BS 𝑖 ∈ B individually. Meanwhile, the meta-
agent exploits the energy dispatch decision by employing a
joint policy toward the globally optimal energy dispatch for
each BS 𝑖 ∈ B. Therefore, the challenges of distinct energy
demand and generation of the state space among the BSs can
be efﬁciently handled by applying learned parameters from
the meta-agent to each BS 𝑖 ∈ B during the training that
establishes a balance between exploration and exploitation.

We compare the achieved reward of proposed MAMRL
model with single-agent centralized and multi-agent central-
ized models in Fig. 7. The single agent centralized (diamond
mark with red line) model converges faster than the other two
models but it achieves the lowest reward due to the lack of
exploitation as it has only one agent. Further, the multi-agent

(a) Proposed Meta-RL

(b) Single-agent centralized

(c) Multi-agent centralized

Fig. 8: Relationship among the entropy loss, value loss, and
policy loss in the training phase of proposed Meta-RL, single-
agent centralized, and multi-agent centralized methods.

centralized (circle mark with blue line) model converges with
a higher reward than the single agent method. The proposed
MAMRL (cross mark with green line) model outperforms the
other two models while converges with the highest reward
value. In addition, multi-agent centralized needs the entire
state information. In contrast, the meta-agent requires only the
observation from local agents, and it can optimize the neural
network parameters by using its own state information.

We analyze the relationship among the value loss, entropy
loss, and policy loss in Fig. 8, where the maximum policy
loss of the proposed MAMRL (in 8(a)) model
is around
0.06 whereas single-agent centralized (in 8(b)) and multi-
agent centralized (in 8(c)) methods gain about 1.88 and 0.12,
respectively. Therefore, the training accuracy increases due to
more variation between exploration and exploitation. Thus,
our MAMRL model is capable of incorporating the decision
of each local BS agent that solves the challenge of non-i.i.d.

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

17

Fig. 9: Testing accuracy of the proposed Meta-RL, single-
agent centralized, and multi-agent centralized methods with
deterministic, asymmetric, and stochastic environments of the
9 SBSs.

demand-generation for the other BSs.

𝑖

In Fig. 9, we examine the testing accuracy [69] of the stor-
age energy 𝜉sto
(𝑡) and the non-renewable energy generation
𝑖
(𝑡) for 96 time slots (1 days) of 9 SBSs under
decision 6 𝜉non
the deterministic, asymmetric, and stochastic environments. In
the experiment, we have used the well-known UMass solar
panel dataset [61] for renewable energy generation information
as well as, the CRAWDAD nyupoly/video dataset [60], for es-
timating the energy consumption of the self-powered network.
Further, we preprocess both of the datasets ( [60] and [61])
using Algorithm 1 that generates the state space information.
Thus, the ground truth comes from this state-space information
of the considered datasets, where the actions are depended
on the renewable energy generation and consumption of a
particular BS 𝑖 ∈ B. The proposed MAMRL (green box)
and multi-agent centralized (blue box) methods achieve a
maximum accuracy of around 95% and 92%, respectively,
under the stochastic environment (in Fig. 9). Further, Fig. 9
shows that the mean accuracy (88%) of the proposed method
is also higher than the centralized solution (86%). Similarly,
in the deterministic and asymmetric environment, the average
accuracy (around 87%) of the proposed low complexity semi-
distributed solution is almost the same as the baseline method.
The prediction results of renewable, storage and non-
renewable energy usage for a single SBS (SBS 2) for 24
hours (96 time slots) under the stochastic environment are
shown in Fig. 10. The proposed MAMRL outperforms all
other baselines and achieves an accuracy of around 95.8%.
In contrast, the accuracy of the other two methods is 75%
and 93.7% for the single-agent centralized and multi-agent
centralized, respectively.

In Figs. 11 and 12, we validate our approach with two stan-
dard regression model evaluation metric, explained variance

6Each BS agent 𝑖 ∈ B can calculate its action from a globally optimal
energy dispatch policy 𝜋∗
(𝒂𝑡𝑖)). In
which, at the end of 15 minutes duration of each time slot 𝑡, the each BS
agent 𝑖 ∈ B can choose one action (i.e., storage or non-renewable) from the
energy dispatch policy 𝜋∗

𝜃𝑖 by using argmax(.) (i.e., argmax 𝜋∗
𝜃𝑖

𝜃𝑖 .

Fig. 10: Prediction result of renewable, storage, and non-
renewable energy usages of a single SBS (SBS 2) for 24 hours
(96 time slots) under the stochastic environment.

Fig. 11: Explained variance score of a single SBS (SBS 2) for
24 hours (96 time slots) under the stochastic environment.

Fig. 12: Mean absolute error of a single SBS (SBS 2) for 24
hours (96 time slots) under the stochastic environment.

7 (i.e., explained variation) and mean absolute error (MAE)
[69], respectively. Fig. 11 shows that the explained variance
score of the proposed MAMRL method almost the same as
the multi-agent centralized. However, in the case of renewable
energy generation, MAMRL method signiﬁcantly performs
better (i.e., 1% more score) than the multi-agent centralized
solution. In particular,
the proposed MAMRL model has
pursued the uncertainty of renewable energy generation by
the dynamics of Markovian for each BS. Further, meta-agent
anticipates the energy dispatch by other BSs decisions and its

7We measure the discrepancy for energy dispatch decisions between the
proposed and baseline models on the ground truth of the datasets ( [60]
and [61]). We deploy the explained variance regression score function using
sklearn API [70] to measure and compare this discrepancy.

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

18

model in terms of the non-renewable energy usages into a
stochastic environment with other benchmarks. This ﬁgure
considers a kernel density analysis for 24 hours (96 time
slots) under a stochastic environment, where the median of
the non-renewable energy usages 0.15 (kWh), and 0.27 (kWh)
for the proposed MAMRL, and pure greedy, respectively, at
each 15 minutes time slot. Further, the proposed MAMRL
can signiﬁcantly reduce the usages of non-renewable energy
for the considered self-powered wireless network, where the
MAMRL can save up to 13.3% of the non-renewable energy
the meta agent of the MAMRL model can
usages. Here,
discretize uncertainty from each local BS agent and transfer
the knowledge (i.e., learning parameters) to each local agent
that can take a globally optimal energy dispatch decision.

Fig. 13: Kernel density analysis of non-renewable energy
usages for 24 hours (96 time slots) under the stochastic
environment.

Fig. 14: Energy consumption cost analysis of 9 SBSs for 24
hours (96 time slots) under deterministic, asymmetric, and
stochastic environments using the proposed Meta-RL method
over pure greedy method.

own state information. We analyze the MAE 8 for the three
environments (i.e., deterministic, asymmetric, and stochastic)
among the proposed MAMRL, single-agent centralized, and
multi-agent centralized methods in Fig. 12. The MAE of the
proposed MAMRL is 11%, 15%, and 4% for deterministic,
asymmetric, and stochastic, respectively since meta-agent has
the capability to adopt the uncertain environment very fast.
This adaptability is enhanced by the exploration mechanism
that is taken into account at each BS, and exploitation that
performs by capitalizing the non-i.i.d. explored information
of all BSs.

Fig. 13 illustrates the efﬁcacy of the proposed MAMRL

8This performance metric provides us with the average magnitude of errors
for the energy dispatch decision of a single SBS (SBS 2) for 24 hours (96 time
slots). Particularly, we analyze the average error over the 96 time slots of the
absolute differences between prediction and actual observation. To evaluate
this metric, we have used the mean absolute error regression loss function of
sklearn API [71].

Fig. 15: Amount of renewable, non-renewable, and storage
energy estimation for 24 hours (96 time slots) for proposed
meta-RL, single-agent RL, multi-agent RL, next ﬁt, ﬁrst ﬁt,
and ﬁrst ﬁt decreasing methods.

Fig. 14 presents the energy consumption cost analysis for 9
SBSs over 24 hours (96 time slots) under deterministic, asym-
metric, and stochastic environments using the proposed Meta-
RL method while comparing it to the pure greedy method.
The total energy cost achieved by the proposed approach for a
particular day will be $33.75, $28.29, and $25.83 for determin-
istic, asymmetric, and stochastic environments, respectively.
Fig. 14 also shows that the proposed method signiﬁcantly
reduces the energy consumption cost (by at least 22.4%) for all
three environments over the pure greedy method. The median
of the energy cost at each time slot is $0.04, $0.03, and $0.03
for the deterministic, asymmetric, and stochastic environments,
respectively. In contrast, Fig. 14 has shown that a median
energy cost for the pure greedy baseline is $0.05 at each time
slot that is due to a lack of the competence to cope with an
unknown environment for energy consumption and generation.
Therefore, the proposed MAMRL model can overcome the
challenges of an unknown environment as well as non-i.i.d.
characteristics for energy consumption and generation of a
self-powered MEC network.

In Fig. 15, we compare our proposed meta-RL model with
single-agent RL, multi-agent RL, next ﬁt, ﬁrst ﬁt, and ﬁrst
ﬁt decreasing methods in terms of amount of renewable,
non-renewable, and storage energy usages for 24 hours (96
time slots). Fig. 15 shows that the proposed MAMRL model

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

19

TABLE V: Comparison between the proposed method and other methods with ground truth for a single SBS (SBS 2) for 24
hours (96 time slots) under the stochastic environment.

Method

Non-
renewable
energy
usage
(kWh)

Storage
energy
usage
(kWh)

Renewable
energy
usage
(kWh)

Non-
renewable
energy
usage
cost ($)

Storage
energy
usage
cost ($)

Renewable
energy
usage
($)

cost

Total en-
ergy us-
age cost
($)

Ground truth (i.e., optimal)
MAMRL (proposed)
Single-agent RL
Multi-agent RL
Next Fit
First Fit
First Fit Decreasing
Without renewable

30.15
30.88
34.53
31.24
38.92
37.37
37.12
47.69

8.87
8.50
6.65
8.31
4.44
5.22
5.34
NA

8.67
8.32
6.50
8.14
4.34
5.10
5.23
NA

3.07
3.14
3.52
3.19
3.97
3.81
3.79
4.86

0.49
0.47
0.37
0.46
0.24
0.29
0.30
NA

0.43
0.42
0.33
0.41
0.21
0.26
0.26
NA

3.99
4.03
4.21
4.05
4.43
4.35
4.34
4.86

Cost dif-
ference
with
ground
truth
(%)
NA
0.90
5.43
1.36
10.86
8.94
8.63
21.72

demand and generation from the historical data while meta-
agent optimizes energy dispatch decisions by obtaining those
features with its own parameters of LSTM. In the case of
testing, a generalized MAMRL trained model is employed that
makes a fully independent and unbiased energy dispatch from
an unknown environment. To this end, the proposed MAMRL
framework shows the efﬁcacy of solving the energy dispatch of
a self-powered wireless network with MEC capabilities with
a higher degree of reliability.

Fig. 16: Competitive cost ratio of the proposed Meta-RL
method for 24 hours (96 time slots) under the deterministic,
asymmetric, and stochastic environments.

outperforms the others that achieves around 22% less non-
renewable energy usages than the next ﬁt scheduling algo-
rithm. Additionally, next ﬁt, ﬁrst ﬁt, and ﬁrst ﬁt decreasing
scheduling methods [72] cannot capture the uncertainty of
energy generation and consumption, as well as provide a near
optimal solution. Further, a comparison between the proposed
method and other methods with the ground truth for a single
SBS (SBS 2) for 24 hours (96 time slots) under the stochastic
environment is illustrated in Table V. The proposed method
can achieve signiﬁcant outcomes with respect to energy cost as
compared with the ground truth. In particular, the experiment
shows that
the energy usage cost difference between the
proposed method and ground truth is around 1% for a single
BS (in Table V). This leads to one of the evidence that the
proposed MAMRL can adopt the unknown environment and
can utilize it during the execution for each BS energy dispatch.
Finally, in Fig. 16, we examine the competitive cost ratio
[29] of the proposed MAMRL framework. From this ﬁgure,
we observe that the proposed MAMRL framework effectively
minimizes the energy consumption cost for each BS under
deterministic, asymmetric, and stochastic environments. In
fact, Fig. 16 ensures the robustness of the proposed MAMRL
framework that is performed a tremendous performance gain
by coping with non-i.i.d. energy consumption and generation
under the uncertainty. Furthermore, in MAMRL training, each
local agent has captured the time-variant features of energy

VI. CONCLUSIONS

In this paper, we have investigated an energy dispatch prob-
lem of a self-powered wireless network with MEC capabilities.
We have formulated a two-stage stochastic linear programming
energy dispatch problem for the considered network. To solve
the energy dispatch problem in a semi-distributed manner, we
have proposed a novel multi-agent meta-reinforcement learn-
ing framework. In particular, each local BS agent obtains the
time-varying features by capturing the Markovian properties of
the network’s energy consumption and renewable generation
for each BS unit, and predict its own energy dispatch policy.
Meanwhile, a meta-agent optimizes each BS agent’s energy
dispatch policy from its own state information, and it transfers
global learning parameters to each BS agent so that they can
update their energy dispatch policy into an optimal policy. We
have shown that the proposed MAMRL framework can capture
the uncertainty of non-i.i.d. energy demand and generation for
the self-powered wireless network with MEC capabilities. Our
experimental results have shown that the proposed MAMRL
framework can save a signiﬁcant amount of non-renewable
energy with higher accuracy prediction that ensures the energy
sustainability of the network. In particular, the performance of
energy dispatch over deterministic, asymmetric, and stochastic
environments outperform other baseline approaches, where
average accuracy achieves up to 95.8% and reduces the energy
cost about 22.4% of the self-powered wireless network. To
this end, the proposed MAMRL model can reduce by at least
11% of the non-renewable energy usage for the self-powered
wireless network.

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

20

APPENDIX A
EXAMPLE OF INFORMATION EXCHANGE BETWEEN LOCAL
BS AGENT AND META AGENT IN MAMRL FRAMEWORK

𝑖 , 𝜉ren
𝑖

(𝑡), 𝐶sto

𝑡𝑖 , 𝐶non
𝑡𝑖

For example, consider an LSTM cell with 48 LSTM units
[49], [64]. Thus, the dimension of forget gate, input gate,
gate/memory/activation gate, and output gate will be 48 for
𝑖 ∈ B that
each gate. Now consider a local BS agent
embedding a dimension of 3 inputs (𝑟𝑖 (𝒂𝑡𝑖, 𝒔𝑡𝑖), 𝒂𝑡𝑖, 𝑡 (cid:48)) to a
local LSTM cell. This input comes from the state information
) of a local BS agent 𝑖. As a result,
𝒔𝑡𝑖 : (𝜉d
inputs are appended to all gates during the training. Therefore,
the number of learning parameters will be 4× (48(48+3) +48)
(i.e., 𝑔𝑎𝑡𝑒𝑠 × [𝑢𝑛𝑖𝑡𝑠(𝑢𝑛𝑖𝑡𝑠 + 𝑖𝑛𝑝𝑢𝑡) + 𝑢𝑛𝑖𝑡𝑠]). Additionally, the
size of hidden state and cell state parameters remain 48 for
each due to an LSTM cell with 48 LSTM units. Further, on top
of the LSTM cell, we have two fully connected output layers, a
fully connected output layer with a Softmax activation to deter-
mine the local energy dispatch policy. Meanwhile, advantage is
determined from another fully connected output layer without
activation function by value function estimation. The hidden
and cell state of each local agent are updated by receiving the
state parameters with a 48×2 dimensional data from the meta-
agent. In case of the meta-agent, the conﬁguration of LSTM
cell is the same as each local LSTM cell. Therefore, at the
end of each time slot duration, the meta-agent sends a 48 × 2
dimensional state information to each local BS agent 𝑖 ∈ B.
Subsequently, the meta-agent receives a 6 dimensional obser-
vation 𝒐𝑖 : (𝑟𝑖 (𝒂𝑡(cid:48)𝑖, 𝒔𝑡(cid:48)𝑖), 𝑟𝑖 (𝒂𝑡𝑖, 𝒔𝑡𝑖), 𝒂𝑡𝑖, 𝒂𝑡(cid:48)𝑖, 𝑡 (cid:48), Λ 𝜋𝜃𝑖 (𝒔𝑡𝑖, 𝒂𝑡𝑖))
as an input from each local BS agent, where the number of
learning parameters at the meta-agent will be 4 × (48(48 + 6) +
48) for each iteration. The output layer of the meta-agent also
consists of two fully connected output layers for determining
meta-policy (i.e., joint policy) and meta advantage. Thus, these
output layers do not affect the dimension of hidden and cell
states for the meta agent’s LSTM cell. In fact, these RNN
states are used as an input to these fully connected layers. As
a result, for each epoch (i.e., end of a time slot duration), the
meta-agent sends 48 × 2 dimensional RNN states to each local
agent along with an energy dispatch policy, and each local
agent sends 6 dimensional observation to the meta-agent.

APPENDIX B
PROOF OF PROPOSITION 1

Proof. For a BS agent 𝑖, energy dispatch policy 𝜋∗
is the
𝜃𝑖
best response for the equilibrium responses from all other BS
agents. Thus, the BS agent 𝑖 can not be improved the value
𝑉 𝜋∗
𝜃𝑖 . Therefore, (24)
holds the following property,

𝜃𝑖 (𝒔𝑡𝑖) any more by deviating of policy 𝜋∗

𝑉 𝜋∗
∞
∑︁

𝒔𝑡(cid:48)𝑖 ∈S𝑖 ,𝑡(cid:48)=𝑡

𝜃𝑖 (𝒔𝑡𝑖) ≥ 𝑟𝑖 (𝒔𝑡𝑖, 𝒂𝑡0, . . . , 𝒂𝑡 𝐵) +

𝛾𝑡(cid:48)−𝑡 Γ(𝒔𝑡(cid:48)𝑖 |𝒔𝑡𝑖, 𝒂𝑡0, . . . , 𝒂𝑡 𝐵)𝑉 𝜋𝜃𝑖 (𝒔𝑡(cid:48)𝑖, 𝜋∗
𝜃0

, . . . , 𝜋∗

𝜃𝐵 ).

(30)

the meta-agent 𝑀𝑡 (O𝑡 ; 𝜙) of the |B|-agent energy
Hence,
dispatch model (i.e., MAMRL) reaches a Nash equilibrium

point for policy 𝜋∗
value of BS agent 𝑖 ∈ B can be as follows:

𝜃𝑖 with parameters 𝜃𝑖. As a result, the optimal

𝑉 𝜋∗

𝜃𝑖 (𝒔𝑡𝑖) = 𝑀𝑡 (∇𝜃𝑡 𝐿 (𝜃𝑡 ); 𝜙).

(31)

(31) implies that 𝜋∗
decisions. Thus, the optimal policy 𝜋∗
equilibrium point and holds the following inequality,

𝜃𝑖 is an optimal policy of energy dispatch
𝜃𝑖 belongs to a Nash

𝑉 𝜋∗

𝜃𝑖 (𝒔𝑡𝑖) ≥ E𝐿 ( 𝜃) [𝐿 (𝜃∗(𝐿(𝜃); 𝜙))]

(32)

(cid:4)

APPENDIX C
PROOF OF PROPOSITION 2

Proof. A probability of action 𝒂𝑡𝑖 of BS agent 𝑖 ∈ B at time
𝑡 can be presented as follows:

𝑃(𝒂𝑡𝑖) = 𝜃𝒂𝑡𝑖

𝑖

(1 − 𝜃𝑖)1−𝒂𝑡𝑖

= 𝒂𝑡𝑖 log 𝜃𝑖 + (1 − 𝒂𝑡𝑖) log(1 − 𝜃𝑖).

(33)

We consider a single state, and a policy gradient estimator can
be deﬁned as,

ˆ𝜕
𝜕𝜃𝑖

𝐿 (𝜃𝑖) = 𝑟𝑖 (𝒔𝑡𝑖, 𝒂𝑡0, . . . , 𝒂𝑡 𝐵)

log 𝑃(𝒂𝑡0, . . . , 𝒂𝑡 𝐵)

𝜕
𝜕𝜃𝑖

𝒂𝑡𝑖 log 𝜃𝑖 + (1 − 𝒂𝑡𝑖) log(1 − 𝜃𝑖)

(𝒂𝑡𝑖 log 𝜃𝑖 + (1 − 𝒂𝑡𝑖) log(1 − 𝜃𝑖))

∑︁

∀𝑖 ∈B

= 𝑟𝑖 (𝒔𝑡𝑖, 𝒂𝑡0, . . . , 𝒂𝑡 𝐵)

= 𝑟𝑖 (𝒔𝑡𝑖, 𝒂𝑡0, . . . , 𝒂𝑡 𝐵)

𝜕
𝜕𝜃𝑖
𝜕
𝜕𝜃𝑖
𝒂𝑡𝑖
𝜃𝑖
= 𝑟𝑖 (𝒔𝑡𝑖, 𝒂𝑡0, . . . , 𝒂𝑡 𝐵)(2𝒂𝑡𝑖 − 1), for 𝜃𝑖 = 0.5.

= 𝑟𝑖 (𝒔𝑡𝑖, 𝒂𝑡0, . . . , 𝒂𝑡 𝐵)(

(1 − 𝒂𝑡𝑖)
(1 − 𝜃𝑖)

−

)

(34)

Thus, an expected reward for |B| BS agents can be represented
as, E[𝑟𝑖] = (cid:205)
∀𝑖 ∈B 𝑟𝑖 (𝒔𝑡𝑖, 𝒂𝑡0, . . . , 𝒂𝑡 𝐵)(0.5) |B |, where by ap-
plying 𝑟𝑖 (𝒔𝑡𝑖, 𝒂𝑡0, . . . , 𝒂𝑡 𝐵) = 1|𝑟𝑖 (𝒔𝑡𝑖, 𝒂𝑡0, . . . , 𝒂𝑡 𝐵), we can
get E[𝑟𝑖] = (0.5) |B |. Now, we can deﬁne an expectation of
ˆ𝜕
a gradient estimation as, E[
𝐿 (𝜃𝑖) = (0.5) |B |.
𝜕𝜃𝑖
Therefore, a variance of the estimated gradient can be deﬁned
as,

𝐿 (𝜃𝑖)] = 𝜕
𝜕𝜃𝑖

V(cid:2)

ˆ𝜕
𝜕𝜃𝑖

𝐿(𝜃𝑖)(cid:3) = E(cid:2)

ˆ𝜕
𝜕𝜃𝑖
= (0.5) |B | − (0.5)2 |B | .

𝐿2 (𝜃𝑖)(cid:3) −

(cid:18)

E(cid:2)

ˆ𝜕
𝜕𝜃𝑖

(cid:19) 2

𝐿 (𝜃𝑖)(cid:3)

(35)

of
analyze
Now, we
𝑃(( ˆ∇𝜃𝑖 𝐿(𝜃𝑖), ∇𝜃𝑖 𝐿 (𝜃𝑖)) > 0) (in (29)), where

step

can

the

gradient

for

𝑃

(cid:16) ˆ∇𝜃𝑖 𝐿(𝜃𝑖), ∇𝜃𝑖 𝐿(𝜃𝑖)

(cid:17)

= (0.5) |B | ∑︁
∀𝑖 ∈B

ˆ𝜕
𝜕𝜃𝑖

𝐿 (𝜃𝑖).

(36)

As a result, 𝑃(( ˆ∇𝜃𝑖 𝐿 (𝜃𝑖), ∇𝜃𝑖 𝐿(𝜃𝑖)) > 0) = (0.5) |B | implies
that the gradient step not only moves in the correct direction
but also decreases exponentially with an increasing number of
(cid:4)
BS agents.

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

21

REFERENCES

[1] W. Saad, M. Bennis and M. Chen, “A Vision of 6G Wireless Sys-
tems: Applications, Trends, Technologies, and Open Research Prob-
lems," IEEE Network, vol. 34, no. 3, pp. 134-142, May/June 2020, doi:
10.1109/MNET.001.1900287.

[2] J. Park, S. Samarakoon, M. Bennis, and M. Debbah, “Wireless Network
Intelligence at the Edge," Proceedings of the IEEE, vol. 107, no. 11, pp.
2204-2239, Nov. 2019, doi: 10.1109/JPROC.2019.2941458.

[3] E. Dahlman, S. Parkvall, J. Peisa, H. Tullberg, H. Murai and M. Fujioka,
“Artiﬁcial Intelligence in Future Evolution of Mobile Communication,"
International Conference on Artiﬁcial Intelligence in Information and
Communication (ICAIIC), Okinawa, Japan, February 2019, pp. 102-106.
[4] M. S. Munir, S. F. Abedin and C. S. Hong, “Artiﬁcial Intelligence-
based Service Aggregation for Mobile-Agent in Edge Computing," 2019
20th Asia-Paciﬁc Network Operations and Management Symposium (AP-
NOMS), Matsue, Japan, September 2019, pp. 1-6.

[5] M. Chen, U. Challita, W. Saad, C. Yin and M. Debbah, “Artiﬁcial Neural
Networks-Based Machine Learning for Wireless Networks: A Tutorial,"
IEEE Communications Surveys & Tutorials, Early Access, July 2019.
[6] M. Chen, Z. Yang, W. Saad, C. Yin, H. V. Poor, and S. Cui, “A Joint
Learning and Communications Framework for Federated Learning over
Wireless Networks," IEEE Transactions on Wireless Communications,
vol. 20, no. 1, pp. 269-283, Jan. 2021, doi: 10.1109/TWC.2020.3024629.
[7] N. H. Tran, W. Bao, A. Zomaya, M. N. H. Nguyen and C. S. Hong,
“Federated Learning over Wireless Networks: Optimization Model Design
and Analysis," IEEE INFOCOM 2019 - IEEE Conference on Computer
Communications, Paris, France, 2019, pp. 1387-1395.

[8] G. Lee, W. Saad, M. Bennis, A. Mehbodniya and F. Adachi, “Online
Ski Rental for ON/OFF Scheduling of Energy Harvesting Base Stations,"
IEEE Transactions on Wireless Communications, vol. 16, no. 5, pp. 2976-
2990, May 2017.

[9] Y. Wei, F. R. Yu, M. Song and Z. Han, “User Scheduling and Resource
Allocation in HetNets With Hybrid Energy Supply: An Actor-Critic
Reinforcement Learning Approach," IEEE Transactions on Wireless Com-
munications, vol. 17, no. 1, pp. 680-692, Jan. 2018.

[10] J. Xu, L. Chen and S. Ren, “Online Learning for Ofﬂoading and
Autoscaling in Energy Harvesting Mobile Edge Computing," IEEE Trans-
actions on Cognitive Communications and Networking, vol. 3, no. 3, pp.
361-373, Sept. 2017.

[11] M. S. Munir, S. F. Abedin, N. H. Tran and C. S. Hong, “When
Edge Computing Meets Microgrid: A Deep Reinforcement Learning
Approach," in IEEE Internet of Things Journal, vol. 6, no. 5, pp. 7360-
7374, October 2019.

[12] M. S. Munir, S. F. Abedin, D. H. Kim, N. H. Tran, Z. Han, and C. S.
Hong, "A Multi-Agent System Toward the Green Edge Computing with
Microgrid," 2019 IEEE Global Communications Conference (GLOBE-
COM), Waikoloa, HI, USA, 9-13 December 2019.

[13] N. Piovesan, D. A. Temesgene, M. Miozzo and P. Dini, "Joint Load
Control and Energy Sharing for Autonomous Operation of 5G Mobile
Networks in Micro-Grids," IEEE Access, vol. 7, pp. 31140-31150, March
2019.

[14] X. Huang, T. Han and N. Ansari, “Smart Grid Enabled Mobile Networks:
Jointly Optimizing BS Operation and Power Distribution," IEEE/ACM
Transactions on Networking, vol. 25, no. 3, pp. 1832-1845, June 2017.
[15] W. Li, T. Yang, F. C. Delicato, P. F. Pires, Z. Tari, S. U. Khan, and A.
Y. Zomaya "On Enabling Sustainable Edge Computing with Renewable
Energy Resources," IEEE Communications Magazine, vol. 56, no. 5, pp.
94-101, May 2018.

[16] Y. Mao, J. Zhang, S. H. Song and K. B. Letaief, “Stochastic Joint Radio
and Computational Resource Management for Multi-User Mobile-Edge
Computing Systems," IEEE Transactions on Wireless Communications,
vol. 16, no. 9, pp. 5994-6009, September 2017.

[17] T. X. Tran and D. Pompili, “Joint Task Ofﬂoading and Resource
Allocation for Multi-Server Mobile-Edge Computing Networks," IEEE
Transactions on Vehicular Technology, vol. 68, no. 1, pp. 856-868,
January 2019.

[18] P. Chang and G. Miao, “Resource Provision for Energy-Efﬁcient Mobile
IEEE Global Communications Conference

Edge Computing Systems,"
(GLOBECOM), Abu Dhabi, United Arab Emirates, 2018, pp. 1-6.
[19] Y. Sun, S. Zhou and J. Xu, “EMM: Energy-Aware Mobility Management
for Mobile Edge Computing in Ultra Dense Networks," IEEE Journal on
Selected Areas in Communications, vol. 35, no. 11, pp. 2637-2646, Nov.
2017.

[20] S. F. Abedin, M. G. R. Alam, R. Haw and C. S. Hong, “A system model
for energy efﬁcient green-IoT network," 2015 International Conference
on Information Networking (ICOIN), Cambodia, 2015, pp. 177-182.

[21] X. Zhang, M. R. Nakhai, G. Zheng, S. Lambotharan and B. Ottersten,
“Calibrated Learning for Online Distributed Power Allocation in Small-
Cell Networks," IEEE Transactions on Communications, vol. 67, no. 11,
pp. 8124-8136, Nov. 2019, doi: 10.1109/TCOMM.2019.2938514.

[22] S. Akin and M. C. Gursoy, “On the Energy and Data Storage Manage-
ment in Energy Harvesting Wireless Communications," IEEE Transac-
tions on Communications, Early Access, August 2019.

[23] N. H. Tran, C. Pham, M. N. H. Nguyen, S. Ren and C. S. Hong,
“Incentivizing Energy Reduction for Emergency Demand Response in
Multi-Tenant Mixed-Use Buildings," IEEE Transactions on Smart Grid,
vol. 9, no. 4, pp. 3701-3715, July 2018.

[24] J. X. Wang et al., “Learning to reinforcement learn," CogSci, 2017. (In

London, UK).

[25] N. Schweighofera, and K. Doya, “Meta-learning in Reinforcement
Learning," Neural Networks, vol. 16, no. 1, pp. 5-9, January 2003.
[26] M. Andrychowicz et al., “Learning to learn by gradient descent by
gradient descent," Advances in Neural Information Processing Systems
29 (NIPS 2016), 2016.

[27] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
D. Silver, and K. Kavukcuoglu, “Asynchronous Methods for Deep Rein-
forcement Learning," Proceedings of The 33rd International Conference
on Machine Learning, vol. 48, pp. 1928-1937, Jan. 2016.

[28] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch, “Multi-
agent actor-critic for mixed cooperative-competitive environments," In
Advances in Neural Information Processing Systems, pp. 6379-6390.
2017.

[29] Y. Zhang, M. H. Hajiesmaili, S. Cai, M. Chen and Q. Zhu, “Peak-
Aware Online Economic Dispatching for Microgrids," IEEE Transactions
on Smart Grid, vol. 9, no. 1, pp. 323-335, Jan. 2018.

[30] T. Han and N. Ansari, "Network Utility Aware Trafﬁc Load Balancing in
Backhaul-Constrained Cache-Enabled Small Cell Networks with Hybrid
Power Supplies," in IEEE Transactions on Mobile Computing, vol. 16,
no. 10, pp. 2819-2832, 1 Oct. 2017.

[31] S. F. Abedin, A. K. Bairagi, M. S. Munir, N. H. Tran and C. S. Hong,
“Fog Load Balancing for Massive Machine Type Communications: A
Game and Transport Theoretic Approach," IEEE Access, vol. 7, pp. 4204-
4218, December 2018.

[32] Z. Chang, Z. Zhou, T. Ristaniemi, and Z. Niu, “Energy Efﬁcient
Optimization for Computation Ofﬂoading in Fog Computing System,"
IEEE Global Communications Conference, Singapore, December 2017,
pp. 1-6.

[33] A. Ndikumana, N. H. Tran, T. M. Ho, Z. Han, W. Saad, D. Niyato,
and C. S. Hong, “Joint Communication, Computation, Caching, and
Control in Big Data Multi-access Edge Computing," IEEE Transactions
on Mobile Computing, vol. 19, no. 6, pp. 1359-1374, 1 June 2020, doi:
10.1109/TMC.2019.2908403.

[34] T. Rauber, G. Runger, M. Schwind, H. Xu, and S. Melzner, “Energy
measurement, modeling, and prediction for processors with frequency
scaling," The Journal of Supercomputing, vol. 70, no. 3, pp. 1454-1476,
2014.

[35] R. Bertran, M. Gonzalez, X. Martorell, N. Navarro and E. Ayguade,
“A Systematic Methodology to Generate Decomposable and Responsive
Power Models for CMPs," IEEE Transactions on Computers, vol. 62, no.
7, pp. 1289-1302, July 2013. Firstquarter 2016.

[36] G. Auer et al., “How much energy is needed to run a wireless network?,"
IEEE Wireless Communications,vol. 18, no. 5, pp. 40-49, October 2011.
[37] ETSI TS, “5G; NR; Physical layer procedures for data", [Online]:

www.etsi.org/deliver/etsi_ts/138200_138299/138214/15.03.00_60/
ts_138214v150300p.pdf, 3GPP TS 38.214 version 15.3.0 Release 15,
October 2018 (Visited on 18 July, 2019).

[38] Y. Gu, W. Saad, M. Bennis, M. Debbah and Z. Han, “Matching
theory for future wireless networks: fundamentals and applications," IEEE
Communications Magazine, vol. 53, no. 5, pp. 52-59, May 2015.

[39] F. Pantisano, M. Bennis, W. Saad, S. Valentin and M. Debbah, “Matching
with externalities for context-aware user-cell association in small cell net-
works," 2013 IEEE Global Communications Conference (GLOBECOM),
Atlanta, GA, 2013, pp. 4483-4488.

[40] N.L. Panwar, S.C. Kaushik, and S. Kothari, “Role of renewable energy
sources in environmental protection: a review," Renewable and Sustain-
able Energy Reviews, vol. 15, no. 3, pp. 1513-1524, April, 2011.

[41] F. A. Chacra, P. Bastard, G. Fleury and R. Clavreul, “Impact of energy
storage costs on economical performance in a distribution substation,"
IEEE Transactions on Power Systems, vol. 20, no. 2, pp. 684-691, May
2005.

[42] H. Kanchev, D. Lu, F. Colas, V. Lazarov and B. Francois, “Energy
Management and Operational Planning of a Microgrid With a PV-Based

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

22

Active Generator for Smart Grid Applications," IEEE Transactions on
Industrial Electronics, vol. 58, no. 10, pp. 4583-4592, Oct. 2011.

[43] A. Mishra, D. Irwin, P. Shenoy, J. Kurose and T. Zhu, “GreenCharge:
Managing RenewableEnergy in Smart Buildings," IEEE Journal on
Selected Areas in Communications, vol. 31, no. 7, pp. 1281-1293, July
2013.

[44] X. Xu, Z. Yan, M. Shahidehpour, Z. Li, M. Yan and X. Kong,
“Data-Driven Risk-Averse Two-Stage Optimal Stochastic Scheduling of
Energy and Reserve with Correlated Wind Power," IEEE Transactions
on Sustainable Energy, vol. 11, no. 1, pp. 436-447, Jan. 2020, doi:
10.1109/TSTE.2019.2894693.

[45] Business Insider, “One simple chart shows why an energy revolution is
coming", [Online]: www.businessinsider.com/solar-power-cost-decrease-
2018-5, May 2018 (Visited on 23 July, 2019).

[46] Y. Liu, and N. K. C. Nair, “A Two-Stage Stochastic Dynamic Economic
Dispatch Model Considering Wind Uncertainty," IEEE Transactions on
Sustainable Energy, vol. 7, no. 2, pp. 819-829, April 2016.

[47] D. Zhou, M. Sheng, B. Li, J. Li and Z. Han, “Distributionally Robust
Planning for Data Delivery in Distributed Satellite Cluster Network,"
IEEE Transactions on Wireless Communications, vol. 18, no. 7, pp. 3642-
3657, July 2019.

[48] S. F. Abedin, M. G. R. Alam, S. M. A. Kazmi, N. H. Tran, D. Niyato
and C. S. Hong, “Resource Allocation for Ultra-reliable and Enhanced
Mobile Broadband IoT Applications in Fog Network," IEEE Transactions
on Communications, vol. 67, no. 1, pp. 489-502, January 2019.

[49] S. Hochreiter, and J. Schmidhuber„ “Long short-term memory," Neural

Computation, vol. 9, pp. 1735-1780, 1997.

[50] Z. M. Fadlullah et al., “State-of-the-Art Deep Learning: Evolving
Machine Intelligence Toward Tomorrow’s Intelligent Network Trafﬁc
Control Systems," IEEE Communications Surveys & Tutorials, vol. 19,
no. 4, pp. 2432-2455, Fourthquarter 2017.

[51] R. S. Sutton and A. G. Barto, "Reinforcement Learning: An Introduc-

tion," 2nd ed. Cambridge, MA, USA: MIT Press, vol. 1, 2017.

[52] M. L. Littman, “Markov games as a framework for multi-agent reinforce-
ment learning," In Proceedings of the eleventh international conference
on machine learning, pp. 157-163, 1994.

[53] R. J. Williams, and D. Zipser, "Gradient-based learning algorithms for
recurrent," Backpropagation: Theory, architectures, and applications, vol.
433, 1995.

[54] I. Bialynicki-Birula, and J. Mycielski, "Uncertainty relations for infor-
mation entropy," Commun. Math. Phys., vol. 44, pp. 129-132, 1975.
[55] T. Seidenfeld, "Entropy and uncertainty," Phil. Sci., vol. 53, pp. 467-491,

1986.

[56] H. R. Feyzmahdavian, A. Aytekin and M. Johansson, "An Asynchronous
Mini-Batch Algorithm for Regularized Stochastic Optimization," IEEE
Transactions on Automatic Control, vol. 61, no. 12, pp. 3740-3754, Dec.
2016, doi: 10.1109/TAC.2016.2525015.

[57] A. Agarwal and J. C. Duchi, "The Generalization Ability of Online Al-
gorithms for Dependent Data," IEEE Transactions on Information Theory,
vol. 59, no. 1, pp. 573-587, Jan. 2013, doi: 10.1109/TIT.2012.2212414.
[58] A. M. Fink, "Equilibrium in a stochastic 𝑛-person game," Journal of
Science of the Hiroshima University, Series A-I (Mathematics), vol. 28,
no. 1, pp. 89-93, 1964.

[59] P. J. Herings, and R. J. A. P. Peeters, "Stationary equilibria in stochastic
games: structure, selection, and computation," Journal of Economic
Theory, Elsevier, vol. 118, no. 1, pp. 32-60, September, 2004.

[60] S. Fu, and Y. Zhang,
2015-04-01),"

"CRAWDAD dataset due/packet-delivery
from www.crawdad.org/due/packet-

downloaded
(v.
delivery/20150401, Apr 2015 (Visited on 3 July, 2019).

[61] Online:“Solar

panel
www.traces.cs.umass.edu/index.php/Smart/Smart,
2019).

dataset,"

UMassTraceRepository
(Visited on 3 July,

[62] A. K. Bairagi, N. H. Tran, W. Saad, Z. Han and C. S. Hong, “A Game-
Theoretic Approach for Fair Coexistence Between LTE-U and Wi-Fi
Systems," IEEE Transactions on Vehicular Technology, vol. 68, no. 1,
pp. 442-455, Jan. 2019.

[63] Intel,

“Intel

Core

i7-6500U

Processor,"

[Online]:

www.ark.intel.com/content/www/us/en/ark/products/88194/intel-core-
i7-6500u-processor-4m-cache-up-to-3-10-ghz.html,
August, 2019).

(Visited

on

17

[64] Online:

"TensorFlow

Core

v2.2.0,"

TensorFlow,

www.tensorﬂow.org/api_docs/python/tf/compat/v1/nn/rnn_cell
/BasicLSTMCell (Visited on 27 May, 2020).

[65] D.P. Kingma, and J. Ba, "Adam: A Method for Stochastic Optimiza-
tion," in Proceedings of the 3rd International Conference on Learning
Representations (ICLR),pp. 1-41, San Diego, USA, May 2015.

[66] Y. Takahashi, G. Schoenbaum, and Y. Niv, "Silencing the critics:
Understanding the effects of cocaine sensitization on dorsolateral and
ventral striatum in the context of an Actor/Critic model," Front. Neurosci.,
vol. 2, no. JUL, pp. 86-89, 2009.

[67] C. G. Li, M. Wang and Q. N. Yuan, "A Multi-agent Reinforcement
Learning using Actor-Critic methods," 2008 International Conference on
Machine Learning and Cybernetics, Kunming, 2008, pp. 878-882, doi:
10.1109/ICMLC.2008.4620528.
symbols
“All

TensorFlow,"

[68] Online:

TensorFlow,

in

www.tensorﬂow.org/api_docs/python/ (Visited on 3 July, 2019).

[69] Online: “Model evaluation: quantifying the quality of predictions,"
scikit-learn, www.scikit-learn.org/stable/modules/model_evaluation.html
(Visited on 3 August, 2019).

"Explained

[70] Online:
score
learn.org/stable/modules/generated/sklearn.metrics.explained_variance
_score.html (Visited on 28 May, 2020).

scikit-learn,

regression
www.scikit-

function,"

variance

[71] Online: "Mean absolute error regression loss," scikit-learn, www.scikit-

learn.org/stable/modules/generated/sklearn.metrics.mean_absolute
_error.html (Visited on 28 May, 2020)

[72] D. S. Johnson, "Near-optimal bin packing algorithms," Massachusetts

Institute of Technology, 1973.

Md. Shirajum Munir (Graduate Student Member,
IEEE) received the B.S. degree in computer science
and engineering from Khulna University, Khulna,
Bangladesh, in 2010. He is currently pursuing the
Ph.D. degree in computer science and engineering
at Kyung Hee University, Seoul, South Korea. He
served as a Lead Engineer with the Solution Labora-
tory, Samsung Research and Development Institute,
Dhaka, Bangladesh, from 2010 to 2016. His current
research interests include IoT network management,
fog computing, mobile edge computing, software-

deﬁned networking, smart grid, and machine learning.

Nguyen H. Tran (S’10-M’11-SM’18) received the
B.S. degree from the Ho Chi Minh City University
of Technology, Ho Chi Minh City, Vietnam, in 2005,
and the Ph.D. degree in electrical and computer en-
gineering from Kyung Hee University, Seoul, South
Korea, in 2011. Since 2018, he has been with the
School of Computer Science, University of Sydney,
Sydney, NSW, Australia, where he is currently a
Senior Lecturer. He was an Assistant Professor with
the Department of Computer Science and Engineer-
ing, Kyung Hee University, from 2012 to 2017. His
current research interests include applying analytic techniques of optimization,
game theory, and stochastic modeling to cutting-edge applications, such
as cloud and mobile edge computing, data centers, heterogeneous wireless
networks, and big data for networks. Dr. Tran was a recipient of the Best
KHU Thesis Award in Engineering in 2011 and the Best Paper Award of
IEEE ICC 2016. He has been an Editor of the IEEE TRANSACTIONS ON
GREEN COMMUNICATIONS AND NETWORKING since 2016 and served
as the Editor of the 2017 Newsletter of Technical Committee on Cognitive
Networks on Internet of Things.

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

23

Walid Saad d (S’07, M’10, SM’15, F’19) received
his Ph.D degree from the University of Oslo in
2010. He is currently a Professor at the Department
of Electrical and Computer Engineering at Virginia
Tech, where he leads the Network sciEnce, Wireless,
and Security (NEWS) laboratory. His research in-
terests include wireless networks, machine learning,
game theory, security, unmanned aerial vehicles,
cyber-physical systems, and network science. Dr.
Saad is a Fellow of the IEEE and an IEEE Dis-
tinguished Lecturer. He is also the recipient of the
NSF CAREER award in 2013, the AFOSR summer faculty fellowship in
2014, and the Young Investigator Award from the Ofﬁce of Naval Research
(ONR) in 2015. He was the author/co-author of eight conference best paper
awards at WiOpt in 2009, ICIMP in 2010, IEEE WCNC in 2012, IEEE
PIMRC in 2015, IEEE SmartGridComm in 2015, EuCNC in 2017, IEEE
GLOBECOM in 2018, and IFIP NTMS in 2019. He is the recipient of the
2015 Fred W. Ellersick Prize from the IEEE Communications Society, of
the 2017 IEEE ComSoc Best Young Professional in Academia award, of the
2018 IEEE ComSoc Radio Communications Committee Early Achievement
Award, and of the 2019 IEEE ComSoc Communication Theory Technical
Committee. From 2015-2017, Dr. Saad was named the Stephen O. Lane
Junior Faculty Fellow at Virginia Tech and, in 2017, he was named College
of Engineering Faculty Fellow. He received the Dean’s award for Research
Excellence from Virginia Tech in 2019. He currently serves as an editor for
the IEEE Transactions on Wireless Communications, IEEE Transactions on
Mobile Computing, IEEE Transactions on Cognitive Communications and
Networking, and IEEE Transactions on Information Forensics and Security.
He is an Editor-at-Large for the IEEE Transactions on Communications.

Choong Seon Hong (S’95-M’97-SM’11) received
the B.S. and M.S. degrees in electronic engineering
from Kyung Hee University, Seoul, South Korea, in
1983 and 1985, respectively, and the Ph.D. degree
from Keio University, Japan, in 1997. In 1988, he
joined KT, where he was involved in broadband
networks as a Member of Technical Staff. Since
1993, he has been with Keio University. He was
with the Telecommunications Network Laboratory,
KT, as a Senior Member of Technical Staff and as
the Director of the Networking Research Team until
1999. Since 1999, he has been a Professor with the Department of Computer
Science and Engineering, Kyung Hee University. His research interests include
future Internet, ad hoc networks, network management, and network security.
He is a member of the ACM, the IEICE, the IPSJ, the KIISE, the KICS,
the KIPS, and the OSIA. Dr. Hong has served as the General Chair, the
TPC Chair/Member, or an Organizing Committee Member of international
conferences such as NOMS, IM, APNOMS, E2EMON, CCNC, ADSN,
ICPP, DIM, WISA, BcN, TINA, SAINT, and ICOIN. He was an Associate
Editor of the IEEE TRANSACTIONS ON NETWORK AND SERVICE
MANAGEMENT, and the IEEE JOURNAL OF COMMUNICATIONS AND
NETWORKS. He currently serves as an Associate Editor of the International
Journal of Network Management, and an Associate Technical Editor of the
IEEE Communications Magazine.

