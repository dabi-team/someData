ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

1

Multi-Agent Meta-Reinforcement Learning for
Self-Powered and Sustainable Edge Computing
Systems

Md. Shirajum Munir, Graduate Student Member, IEEE, Nguyen H. Tran, Senior Member, IEEE,
Walid Saad, Fellow, IEEE, and Choong Seon Hong, Senior Member, IEEE

1
2
0
2

b
e
F
0
1

]

G
L
.
s
c
[

3
v
7
6
5
8
0
.
2
0
0
2
:
v
i
X
r
a

Abstractâ€”The stringent requirements of mobile edge comput-
ing (MEC) applications and functions fathom the high capacity
and dense deployment of MEC hosts to the upcoming wireless
networks. However, operating such high capacity MEC hosts
can signiï¬cantly increase energy consumption. Thus, a base
station (BS) unit can act as a self-powered BS. In this paper, an
effective energy dispatch mechanism for self-powered wireless
networks with edge computing capabilities is studied. First, a
two-stage linear stochastic programming problem is formulated
with the goal of minimizing the total energy consumption cost of
the system while fulï¬lling the energy demand. Second, a semi-
distributed data-driven solution is proposed by developing a novel
multi-agent meta-reinforcement learning (MAMRL) framework
to solve the formulated problem. In particular, each BS plays
the role of a local agent that explores a Markovian behavior for
both energy consumption and generation while each BS transfers
time-varying features to a meta-agent. Sequentially, the meta-
agent optimizes (i.e., exploits) the energy dispatch decision by
accepting only the observations from each local agent with its
own state information. Meanwhile, each BS agent estimates its
own energy dispatch policy by applying the learned parameters
from meta-agent. Finally, the proposed MAMRL framework
is benchmarked by analyzing deterministic, asymmetric, and
stochastic environments in terms of non-renewable energy usages,
energy cost, and accuracy. Experimental results show that the
proposed MAMRL model can reduce up to 11% non-renewable
energy usage and by 22.4% the energy cost (with 95.8% prediction
accuracy), compared to other baseline methods.

Index Termsâ€”Mobile edge computing (MEC), stochastic op-
timization, meta-reinforcement learning, self-powered, demand
response.

This work was partially supported by the National Research Founda-
tion of Korea(NRF) grant funded by the Korea government(MSIT) (No.
2020R1A4A1018607) and by Institute of Information & communications
Technology Planning & Evaluation (IITP) grant funded by the Korea govern-
ment(MSIT) (No.2019-0-01287, Evolvable Deep Learning Model Generation
Platform for Edge Computing).

Md. Shirajum Munir, and Choong Seon Hong are with the Department of
Computer Science and Engineering, Kyung Hee University, Yongin-si 17104,
Republic of Korea (e-mail: munir@khu.ac.kr; cshong@khu.ac.kr).

Nguyen H. Tran is with the School of Computer Science, The University of
Sydney, Sydney, 2006, NSW, Australia. (e-mail: nguyen.tran@sydney.edu.au).
Walid Saad is with the Wireless@VT Group, Bradley Department of Elec-
trical and Computer Engineering, Virginia Tech, Blacksburg, VA 24061 USA,
and also with the Department of Computer Science and Engineering, Kyung
Hee University, Yongin-si 17104, Republic of Korea (e-mail: walids@vt.edu).
Corresponding author: Choong Seon Hong (e-mail: cshong@khu.ac.kr).
Â©2021 IEEE. Personal use of this material is permitted. Permission from
IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or promotional
purposes, creating new collective works, for resale or redistribution to servers
or lists, or reuse of any copyrighted component of this work in other works.

I. INTRODUCTION

Next-generation wireless networks are expected to signif-
icantly rely on edge applications and functions that include
edge computing and edge artiï¬cial
intelligence (edge AI)
[1]â€“[7]. To successfully support such edge services within
a wireless network with mobile edge computing (MEC) ca-
pabilities, energy management (i.e., demand and supply) is
one of the most critical design challenges. In particular, it
is imperative to equip next-generation wireless networks with
alternative energy sources, such as renewable energy, in order
to provide extremely reliable energy dispatch with less energy
consumption cost [8]â€“[15]. An efï¬cient energy dispatch design
requires energy sustainability, which not only saves energy
consumption cost, but also fulï¬lls the energy demand of
the edge computing by enabling its own renewable energy
sources. Speciï¬cally, sustainable energy is the practice of
seamless energy ï¬‚ow to the MEC system that emerges to
meet the energy demand without compromising the ability of
future energy generation. Furthermore, to ensure a sustainable
MEC operation, the retrogressive penetration of uncertainty for
energy consumption and generation is essential. A summary
of the challenges that are solved by the literature to enable
renewable energy sources for the wireless network is presented
in Table I.

To provide sustainable edge computing for next-generation
wireless systems, each base station (BS) with MEC capabilities
unit can be equipped with renewable energy sources. Thus, the
energy source of such a BS unit not only relies solely on the
power grid, but also on the equipped renewable energy sources.
In particular, in a self-powered network, wireless BSs with
MEC capabilities is equipped with its own renewable energy
sources that can generate renewable energy, consume, store,
and share energy with other BS units.

Delivering seamless energy ï¬‚ow with a low energy con-
sumption cost in a self-powered wireless network with MEC
capabilities can lead to uncertainty in both energy demand and
generation. In particular, the randomness of the energy demand
is induced by the uncertain resources (i.e., computation and
communication) request by the edge services and applications.
Meanwhile, the energy generation of a renewable source (i.e.,
a solar panel) at each self-powered BS unit varies on the time
of a day. In other words, the pattern of energy demand and gen-
eration will differ from one self-powered BS unit to another.
Thus, such ï¬‚uctuating energy demand and generation pattern

 
 
 
 
 
 
ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

2

induces a non-independent and identically distributed (non-
i.i.d.) of energy dispatch at each BS over time. To overcome
this non-i.i.d. energy demand and generation, characterizing
the expected amount of uncertainty is crucial to ensure a
seamless energy ï¬‚ow to the self-powered wireless network.
As such, when designing self-powered wireless networks, it is
necessary to take into account this uncertainty in the energy
patterns.

A. Related Works

The problem of energy management for MEC-enabled
wireless networks has been studied in [16]â€“[22] (summary
in Table II). In [16], the authors proposed a joint mechanism
for radio resource management and users task ofï¬‚oading with
the goal of minimizing the long-term power consumption
for both mobile devices and the MEC server. The authors
in [17] proposed a heuristic to solve the joint problem of
computational resource allocation, uplink transmission power,
and user task ofï¬‚oading problem. The work in [18] studied the
tradeoff between communication and computation for a MEC
system and the authors proposed a MEC server CPU scaling
mechanism for reducing the energy consumption. Further, the
work in [19] proposed an energy-aware mobility management
scheme for MEC in ultra-dense networks, and they addressed
the problem using Lyapunov optimization and multi-armed
bandits. Recently, the authors in [21] proposed a distributed
power control scheme for a small cell network by using the
concept of a multi-agent calibrate learning. Further, the authors
in [22] studied the problem of energy storage and energy
harvesting (EH) for a wireless network using deviation theory
and Markov processes. However, all of these existing works
assume that the consumed energy is available from the energy
utility source to the wireless network system [16]â€“[22]. Since
the assumed models are often focused on energy management
and user task ofï¬‚oading on network resource allocations, the
random demand for computational (e.g., CPU computation,
memory, etc.) and communication requirements of the edge
applications and services are not considered. In fact, even if
enough energy supply is available, the energy cost related to
network operation can be signiï¬cant because of the usage of
non-renewable (e.g., coal, petroleum, natural gas). Indeed, it
is necessary to include renewable energy sources towards the
next-generation wireless networking infrastructure.

Recently, some of the challenges of renewable energy
powered wireless networks have been studied in [8]â€“[14], [23].
In [8], the authors proposed an online optimization framework
to analyze the activation and deactivation of BSs in a self-
powered network. In [9], proposed a hybrid power source
infrastructure to support heterogeneous networks (HetNets), a
model-free deep reinforcement learning (RL) mechanism was
proposed for user scheduling and network resource manage-
ment. In [10], the authors developed an RL scheme for edge
resource management while incorporating renewable energy
in the edge network. In particular, the goal of [10] is to
minimize a long-term system cost by load balancing between
the centralized cloud and edge server. The authors in [11]
introduced a microgrid enabled edge computing system. A

joint optimization problem is studied for MEC task assignment
and energy demand-response (DR) management. The authors
in [11] developed a model-based deep RL framework to tackle
the joint problem. In [12], the authors proposed a risk-sensitive
energy proï¬ling for microgrid-powered MEC network to en-
sure a sustainable energy supply for green edge computing by
capturing the conditional value at risk (CVaR) tail distribution
of the energy shortfall. The authors in [12] proposed a multi-
agent RL system to solve the energy scheduling problem. In
[13], the authors proposed a self-sustainable mobile networks,
using graph-based approach for intelligent energy management
with a microgrid. The authors in [14] proposed a smart
grid-enabled wireless network and minimized grid energy
consumption by applying energy sharing among the BSs.
Furthermore, in [23], the authors addressed challenges of non-
coordinated energy shedding and mis-aligned incentives for
mixed-use building (i.e., buildings and data centers) using auc-
tion theory to reduce energy usage. However, these works [9]â€“
[14], [23] do not investigate the problem of energy dispatch
nor do they account for the energy cost of MEC-enabled,
self-powered networks when the demand and generation of
each self-powered BS are non-i.i.d.. Dealing with non-i.i.d.
energy demand and generation among self-powered BSs is
challenging due to the intrinsic energy requirements of each
BS evolve the uncertainty. In order to overcome this unique
energy dispatch challenge, we propose to develop a multi-
agent meta-reinforcement learning framework that can adapt
new uncertain environment without considering the entire past
experience.

Some interesting problems related to meta-RL and multi-
agent deep RL are studied in [24]â€“[28] (summary in Table
II). In [24], the authors focused on studying the challenges
of the tradeoff between effectiveness and available amounts
of training data for a deep-RL based learning system. To
the authors in [24] tackled those challenges by
this end,
learning architecture.
exploring a deep meta-reinforcement
This learning architecture comprises of two learning systems:
1) lower-level system that can learn each new task very
quickly, 2) higher-level system is responsible to improve the
performance of each lower-level system task. In particular, this
learning mechanism is involved with one lower-level system
that can learn relatively quickly as compared with a higher-
level system. This lower-level system can adapt to a new
task while a higher-level system performs ï¬ne-tuning so as
to improve the performance of the lower-level system. In
particular, in deep meta-reinforcement learning, a lower-level
system quantiï¬es a reward based on the desired action and
feeds back that reward to a higher-level system to tune the
weights of a recurrent network. However, the authors in [24]
do not consider a stochastic environment nor do they extend
their work for a multi-agent scenario. The authors in [25]
proposed a stochastic gradient-based meta-parameter learning
scheme for tuning reinforcement learning parameters to the
physical environmental dynamics. Particularly, the experiment
in [25] performed in both animal and robot environments,
where an animal must recognize food before it starves and
a robot must recharge before the battery is empty. Thus, the
proposed scheme can effectively ï¬nd meta-parameter values

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

3

TABLE I: Summary of the challenges that are solved by the literature for enabling renewable energy sources in the wireless
network.

Ref.

Energy sources

[8]

[9]
[10]

[11]

[12]

[13]
[14]

[15]

This work

Renewable

Hybrid energy
Hybrid energy

Microgrid

Microgrid

Renewable
Smart grid enabled
hybrid energy
Hybrid energy

Smart grid enabled
self-powered
renewable energy

MEC ca-
pabilities
No

Non-i.i.d.
dataset
No

Energy
dispatch
No

Energy
cost
No

No
Yes

Yes

Yes

No
No

No

Yes

No
No

No

No

No
No

No

Yes

No
No

Yes

Yes

Yes
Yes

Yes

Yes

No
No

No

No

No
No

No

Yes

Remarks

Activation and deactivation of BSs in a self-powered net-
work
User scheduling and network resource management
Load balancing between the centralized cloud and edge
server
MEC task assignment and energy demand-response (DR)
management
Risk-sensitive energy proï¬ling for microgrid-powered MEC
network
Energy load balancing among the SBSs with a microgrid
Joint network resource allocation and energy sharing among
the BSs
Overall system architecture for edge computing and renew-
able energy resources
An effective energy dispatch mechanism for self-powered
wireless networks with edge computing capabilities

and controls the meta-parameter in both static and dynamic
environments. In [26], the authors investigated a learning to
learn (i.e., meta-learning) mechanism with the recurrent neural
networks, where the meta-learning problem was designed
as a generalized transfer learning scheme. In particular, the
authors in [26] considered a parametrized optimizer that can
transfer the neural network parameters update to an opti-
mizee. Meanwhile, the optimizee can determine the gradients
without relying on the optimizer parameters. Moreover, the
optimizee sends the error to the optimizer, and updates its
own parameters based on the transferred parameters. This
mechanism allows an agent to learn new tasks for a similar
structure. An asynchronous multi-agent RL framework was
studied in [27], where the authors investigated how parallel
actor learners of asynchronous advantage actor-critic (A3C)
can achieve better stability during the neural network training
comparted to asynchronous RL schemes. Such schemes in-
clude asynchronous one-step Q-learning, one-step Sarsa, and
n-step Q-learning. The authors in [28] proposed a general-
purpose multi-agent scheme by adopting the framework of
centralized training with decentralized execution. In particular,
the authors in [28] proposed an extension of the actor-critic
policy gradient mechanism by modifying the role of the critic.
This critic is augmented with an additional policy information
from the other actors (agents). Sequentially, each local actor
executes in a decentralized manner and sends its own policy
to the centralized critic for further investigation. However, the
environment (i.e., state information) of this model remains
the same for all of the local actors while in our setting the
environment of each BS agent is deferent from others based on
its own energy demand and generation. Moreover, the works in
[24]â€“[28], do not consider a multi-agent environment in which
the policy of each agent relies on its own state information.
In particular, such state information belongs to a non-i.i.d.
learning environment when environmental dynamics become
distinct among the agents.

Fig. 1: Multi-agent meta-reinforcement learning framework of
self-powered energy dispatch for sustainable edge computing.

B. Contributions

The main contribution of this paper is a novel energy
management framework for next-generation MEC in self-
powered wireless network that
is reliable against extreme
uncertain energy demand and generation. We formulate a two-
stage stochastic energy cost minimization problem that can
balance renewable, non-renewable, and storage energy without
knowing the actual demand. In fact, the formulated problem
also investigates the realization of renewable energy generation
after receiving the uncertain energy demand from the MEC
applications and service requests. To solve this problem, we
propose a multi-agent meta-reinforcement learning (MAMRL)
framework that dynamically observes the non-i.i.d. behavior of
time-varying features in both energy demand and generation
at each BS and, then transfers those observations to obtain
an energy dispatch decision and execute the energy dispatch
policy to the self-powered BS. Fig. 1 illustrates how we pro-
pose to dispatch energy to ensure sustainable edge computing
over a self-powered network using MAMRL framework. As
we can see, each BS that includes small cell base stations
(SBSs) and a macro base station (MBS) will act as a local

Meta Agent Learningâ€¦LSTM for Meta-Policy Optimizer Energy Environment BS 0Local Agent BS 0StateRewardActionEnergy Environment BS BLocal Agent BS BStateRewardActionObservation MemoryMeta-AdvantageFunctionMeta-PolicyParameter UpdateObservationsEach Agent ObservationLocal AdvantageFunctionShared Neural Networks ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

4

TABLE II: Summary of the related works [16]â€“[28].

Ref.
[16]

[17]

task

resource

Contributions
Radio
management
and
users
ofï¬‚oading
Computational
resource allocation,
uplink transmission
power,
user
task ofï¬‚oading

and

[18] MEC server CPU
scaling mechanism
for reducing the en-
ergy consumption
Energy-aware mo-
bility management
scheme for MEC

[19]

[20]

Energy
efï¬cient
green-IoT network

[21] Distributed power
control scheme for
a small cell net-
work
Energy storage and
energy
harvesting
(EH) for a wireless
network
[23] Non-coordinated

[22]

[24]

[25]

shedding
energy
mis-aligned
and
incentives
for
mixed-use building
between
Tradeoff
and
effectiveness
available
amounts
of training data
Controling
meta-parameter
in
both
and
environments
Learning to learn
mechanism
with
the recurrent neural
networks
[27] Asynchronous

static
dynamic

[26]

the

multi-agent
framework

RL

[28] General-purpose

multi-agent scheme

Limitation

Method
Optimization Usage of non-
renewable, deter-
ministic environ-
ment
Usage of non-
renewable,
energy dispatch,
performance
guarantee

Heuristic

Optimization Usage of non-

renewable,
energy dispatch

Energy dispatch,
i.i.d.
energy
demand-
response
Edge computing,
Energy dispatch,
deterministic en-
vironment
Usage of non-
renewable,
energy dispatch

MEC
capabilities, i.i.d.
energy demand-
response
MEC
capabilities, i.i.d.
energy demand-
response

Stochastic envi-
ronment and a
multi-agent sce-
nario
Single-
agent,
environment

same

Lyapunov
and multi-
armed
bandits
Heuristic

Multi-agent
calibrate
learning

Deviation
theory and
Markov
processes
Auction
theory

Deep meta-
RL

SGD-based
meta-
parameter
learning

Generalized
transfer
learning

Deterministic
environment,
single-agent

One-step
Q-learning,
one-step
Sarsa, and
n-step
Q-learning
Extension
of
the
actor-critic
policy
gradient

Deterministic en-
vironment

Same
environment
for
all of
local actors

the

quickly. Thus,
the proposed MAMRL framework ensures
autonomous decision making under an uncertain and unknown
environment. Our key contributions include:

â€¢ We formulate a self-powered energy dispatch problem for
MEC-supported wireless network, in which the objective
is to minimize the total energy consumption cost of
network while considering the uncertainty of both energy
consumption and generation. The formulated problem
is, thus, a two-stage linear stochastic programming. In
particular, the ï¬rst stage makes a decision when energy
demand is unknown, and the second stage discretizes the
realization of renewable energy generation after knowing
energy demand of the network.

â€¢ To solve the formulated problem, we propose a new
multi-agent meta-reinforcement learning framework by
considering the skill transfer mechanism [24], [25] be-
tween each local agent (i.e., self-powered BS) and meta-
agent. In this MAMRL scheme, each local agent ex-
plores its own energy dispatch decision using Markovian
properties for capturing the time-varying features of both
energy demand and generation. Meanwhile, the meta-
agent evaluates (exploits) that decision for each local
agent and optimizes the energy dispatch decision. In
particular, we design a long short-term memory (LSTM)
as a meta-agent (i.e., run at MBS) that is capable of
avoiding the incompetent decision from each local agent
and learns the right features more quickly by maintaining
its own state information.

â€¢ We develop the proposed MAMRL energy dispatch
framework in a semi-distributed manner. Each local agent
(i.e., self-powered BS) estimates its own energy dis-
patch decision using local energy data (i.e., demand and
generation), and provides observations to the meta-agent
individually. Consequently, the meta-agent optimizes the
decision centrally and assists the local agent toward a
globally optimized decision. Thus, this approach not only
reduces the computational complexity and communica-
tion overhead but it also mitigates the curse of dimension-
ality under the uncertainty by utilizing non-i.i.d. energy
demand and generation from each local agent.

â€¢ Experimental results using real datasets establish a signif-
icant performance gain of the energy dispatch under the
deterministic, asymmetric, and stochastic environments.
Particularly, the results show that the proposed MAMRL
model saves up to 22.44% of energy consumption cost
over a baseline approach while achieving an average
accuracy of around 95.8% in a stochastic environment.
Our approach also decreases the usage of non-renewable
energy up to 11% of total consumed energy.

agent and transfer their own decision (reward and action) to
the meta-agent. Then, the meta-agent accumulates all of the
non-i.i.d. observations from each local agent (i.e., SBSs and
MBS) and optimizes the energy dispatch policy. The proposed
MAMRL framework then provides feedback to each BS agent
for exploring efï¬ciently that acquire the right decision more

The rest of the paper is organized as follows. Section II
presents the system model of self-powered edge computing.
The problem formulation is described in Section III. Section
IV provides MAMRL framework for solving energy dispatch
problem. Experimental results are analyzed in Section V.
Finally, conclusions are drawn in Section VI.

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

5

TABLE III: Summary of notations.

Description
Set of BSs (SBSs and MBS)
Set of active servers under the BS ğ‘– âˆˆ B
Set of user tasks at BS ğ‘– âˆˆ B
Set of renewable energy sources
Server utilization in BS ğ‘– âˆˆ B
No. of CPU cores
Average downlink data of BS ğ‘–
Fixed channel bandwidth of BS ğ‘– for user task ğ‘—
Transmission power of BS ğ‘–
Downlink channel gain between user task ğ‘— to BS ğ‘–
Co-channel interference for user task ğ‘— at BS ğ‘–
Energy coefï¬cient for BS ğ‘– âˆˆ B
MEC server CPU frequency for a single core
Server switching capacitance
MEC server static energy consumption
MEC server idle state power consumption
Scaling factor of heterogeneous MEC CPU core
Static energy consumption of BS
Renewable energy cost per unit
Non-renewable energy cost per unit
Storage energy cost per unit
Amount of renewable energy
Amount of non-renewable energy
Amount of surplus energy
Energy demand at time slot ğ‘¡
Random variable for energy demand
Maximum capacity of renewable energy at BS ğ‘– âˆˆ B
Set of observation at BS ğ‘– âˆˆ B
Big ğ‘‚ notation to represent complexity
Entropy regularization coefï¬cient
Discount factor
Learning parameters for BS ğ‘– âˆˆ B
Energy dispatch policy with parameters ğœƒğ‘– at BS ğ‘– âˆˆ B
Meta-agent learning parameters

(ğ‘¡)
(ğ‘¡)

Notation
B
Kğ‘–
Jğ‘–
R
ğœŒğ‘– (ğ‘¡)
ğ¿
ğ‘…ğ‘– (ğ‘¡)
ğ‘Šğ‘– ğ‘—
ğ‘ƒğ‘–
ğ‘”ğ‘– ğ‘— (ğ‘¡)
ğ¼ğ‘– ğ‘— (ğ‘¡)
ğ›¿ğ‘–
ğ‘“
ğœ
ğœ‚MEC
st
ğœ‚MEC
idle
ğœ›ğ‘˜
st (ğ‘¡)
ğœ‚net
ğ‘ren
ğ‘¡
ğ‘non
ğ‘¡
ğ‘sto
ğ‘¡
ğœ‰ ren
ğ‘¡
ğœ‰ non
ğ‘¡
ğœ‰ sto
ğ‘¡
ğœ‰ d
ğ‘¡
ğœ‰ D
ğ‘¡
ğœ‰ renmax
ğ‘¡
Oğ‘–
ğ‘‚ (.)
ğ›½
ğ›¾
ğœƒğ‘–
ğœ‹ğœƒğ‘–
ğœ™

average trafï¬c size ğ‘†ğ‘– (ğ‘¡) (bits) at time slot ğ‘¡. The average trafï¬c
arrival rate is deï¬ned as ğœ†ğ‘– (ğ‘¡) = 1
ğ‘†ğ‘– (ğ‘¡) . Therefore, an M/M/K
queuing model is suitable to model these ğ½ğ‘– user tasks using ğ¾ğ‘–
MEC servers at BS ğ‘– and time ğ‘¡ [31], [32]. The task size of this
queuing model is exponentially distributed since the average
trafï¬c size ğ‘†ğ‘– (ğ‘¡) is already known. Hence, the service rate of
the BS ğ‘– is determined by ğœ‡ğ‘– (ğ‘¡) =
ğ‘¢ğ‘˜ğ‘– (ğ‘¡) ] . At any given
time ğ‘¡, we assume that all of the tasks in Jğ‘– are uniformly
distributed at each BS ğ‘–. Thus, for a given MEC server task
association indicator Î¥ ğ‘— ğ‘˜ğ‘– (ğ‘¡) = 1 if task ğ‘— is assigned to server
ğ‘˜ at BS ğ‘–, and 0 otherwise, the average MEC server utilization
is deï¬ned as follows [11]:
(cid:40) (cid:205) ğ‘— âˆˆJğ‘–
0,

(cid:205)ğ‘˜ğ‘– âˆˆKğ‘– Î¥ ğ‘— ğ‘˜ğ‘– (ğ‘¡) ğœ†ğ‘– (ğ‘¡)
ğœ‡ğ‘– (ğ‘¡) ğ¾ğ‘–

, if Î¥ ğ‘— ğ‘˜ğ‘– (ğ‘¡) = 1,

1
E[(cid:205)ğ‘˜ğ‘– âˆˆKğ‘–

otherwise.

ğœŒğ‘– (ğ‘¡) =

(1)

1) MEC Server Energy Consumption:

In case of MEC
server energy consumption, the computational energy con-
sumption (dynamic energy) will be dependent on the CPU
activity for executing computational tasks [16], [17], [33].
Further, such dynamic energy is also accounted with the
thermal design power (TDP), memory, and disk I/O operations
of the MEC server [16], [17], [33] and we denote as ğœ‚MEC
(ğ‘¡).
Meanwhile, static energy ğœ‚MEC
(ğ‘¡) includes the idle state power
idle
of CPU activities [16], [18]. We consider, a single core CPU
with a processor frequency ğ‘“
(cycles/s), an average server
utilization ğœŒğ‘– (ğ‘¡) (using (1)) at time slot ğ‘¡, and a switching

st

Fig. 2: System model for a self-powered wireless network with
MEC capabilities.

II. SYSTEM MODEL OF SELF-POWERED EDGE
COMPUTING

Consider a self-powered wireless network that is connected
with a smart grid controller as shown in Fig. 2. Such a wireless
network enables edge computing services for various MEC
applications and services. The energy consumption of the
network depends on network operations energy consumption
along with the task loads of the MEC applications. Meanwhile,
the energy supply of the network relies on the energy gener-
ation from renewable sources that are attached to the BSs, as
well as both renewable and non-renewable sources of the smart
grid. Furthermore, the smart grid controller is a representative
of the main power grid (i.e, smart grid), where an additional
amount of energy can be supplied via the smart grid controller
to the network. Therefore, we will ï¬rst discuss the energy
demand model that includes MEC server energy consumption,
and network communication energy consumption. We will
then describe the energy generation model that consists of the
non-renewable energy generation cost, surplus energy storage
cost, and total energy generation cost. Table III illustrates the
summary of notations.

A. Energy Demand Model

Consider a set B = {0, 1, 2, . . . , ğµ} of ğµ + 1 (0 for MBS)
BSs that encompass ğµ SBSs overlaid over a single MBS.
Each BS ğ‘– âˆˆ B includes a set Kğ‘– = {1, 2, . . . , ğ¾ğ‘– } of ğ¾ğ‘–
MEC application servers. We consider a ï¬nite time horizon
T = 1, 2, . . . , ğ‘‡ with each time slot being indexed by ğ‘¡ and
having a duration of 15 minutes [29]. The observational period
of each time slot ğ‘¡ ends at the 15-th minute and is capable of
capturing the changes of network dynamics [11], [12], [30].
A set Jğ‘– of ğ½ğ‘– heterogeneous MEC application task requests
from users will arrive to BS ğ‘– with an average task arrival
rate ğœ†ğ‘– (ğ‘¡) (bits/s) at time ğ‘¡. The task arrival rate ğœ†ğ‘– (ğ‘¡) at BS
ğ‘– âˆˆ B follows a Poisson process at time slot ğ‘¡. BS ğ‘– integrates
ğ¾ğ‘– heterogeneous active MEC application servers that has
ğ‘¢ğ‘˜ğ‘– (ğ‘¡) (bits/s) processing capacity. Thus, ğ½ğ‘– computational task
requests will be accumulated into the service pool with an

Smart Grid ControllerMBS : Macro base stationSBS: Small cell base stationAT : Application typeMEC : Mobile edge computingMBS SBS with MEC capabilitiesAT1AT2:ATkAT1AT2:ATkSBS with MEC capabilitiesSmart grid to BS (MBS, SBS) connection Local renewable energy to BS connection User to MEC server communicationACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

6

capacitance ğœ = 5 Ã— 10âˆ’27 (farad) [17]. The dynamic power
consumption of such single core CPU can be calculated by
applying a cubic formula ğœğœŒğ‘– (ğ‘¡) ğ‘“ 3 [18], [34]. Thus, energy
consumption of ğ¾ğ‘– MEC servers with ğ¿ CPU cores at BS ğ‘– is
deï¬ned as follows:
(cid:26) (cid:205)ğ‘˜ âˆˆğ¾ğ‘–
ğœ‚MEC
idle

(cid:205)ğ‘™ âˆˆğ¿ ğœğœŒğ‘– (ğ‘¡) ğ‘“ 3
ğ‘˜ğ‘–
(ğ‘¡),
otherwise,

(2)
where ğœ›ğ‘˜ğ‘–ğ‘™ denotes a scaling factor of heterogeneous CPU core
of the MEC server. Thus, the value of ğœ›ğ‘˜ğ‘–ğ‘™ is dependent on
the processor architecture [35] that assures the heterogeneity
of the MEC server.

ğœ›ğ‘˜ğ‘–ğ‘™ + ğœ‚MEC

ğœ‰MEC
ğ‘–

(ğ‘¡) =

st

(ğ‘¡), if ğœŒğ‘– (ğ‘¡) > 0,

2) Base Station Energy Consumption: The energy con-
sumption needed for the operation of the network base stations
(i.e., SBSs and MBS) includes two types of energy: dynamic
and static energy consumption [36]. On one hand, a static
energy consumption ğœ‚net
st (ğ‘¡) includes the energy for maintain-
ing the idle state of any BS, a constant power consumption
for receiving packet from users, and the energy for wired
transmission among the BSs. On the other hand, the dynamic
energy consumption of the BSs depends on the amount of
data transfer from BSs to users which essentially relates to the
downlink [37] transmit energy. Thus, we consider that each BS
ğ‘– âˆˆ B operates at a ï¬xed channel bandwidth ğ‘Šğ‘– ğ‘— and constant
transmission power ğ‘ƒğ‘– [37]. Then the average downlink data
of BS ğ‘– will be given by [11]:

ğ‘…ğ‘– (ğ‘¡) =

âˆ‘ï¸

âˆ‘ï¸

ğ‘– âˆˆB

ğ‘— âˆˆJğ‘–

ğ‘Šğ‘– ğ‘— log2

(cid:16)

1 +

ğ‘ƒğ‘–ğ‘”ğ‘– ğ‘— (ğ‘¡)
ğœ2 + ğ¼ğ‘– ğ‘— (ğ‘¡)

(cid:17)

(3)

where ğ‘”ğ‘– ğ‘— (ğ‘¡) represents downlink channel gain between user
to BS ğ‘–, ğœ2 determines a variance of an Additive
task ğ‘—
White Gaussian Noise (AWGN), and ğ¼ğ‘– ğ‘— (ğ‘¡) denotes the co-
channel interference [38], [39] among the BSs. Here, the co-
channel interference ğ¼ğ‘– ğ‘— (ğ‘¡) = (cid:205)ğ‘–(cid:48) âˆˆB,ğ‘–(cid:48)â‰ ğ‘– ğ‘ƒğ‘–(cid:48)ğ‘”ğ‘–(cid:48) ğ‘— (ğ‘¡) relates to
the transmissions from other BSs ğ‘–(cid:48) âˆˆ B that use the same
subchannels of ğ‘Šğ‘– ğ‘— . ğ‘ƒğ‘–(cid:48) and ğ‘”ğ‘–(cid:48) ğ‘— (ğ‘¡) represent, respectively, the
transmit power and the channel gain of the BS ğ‘–(cid:48) â‰  ğ‘– âˆˆ B.
Therefore, downlink energy consumption of the data transfer
of BS ğ‘– âˆˆ B is deï¬ned by
[watt-seconds or joule], where
ğ‘†ğ‘– (ğ‘¡)
ğ‘…ğ‘– (ğ‘¡) [seconds] determines the duration of transmit power ğ‘ƒğ‘–
[watt]. Thus, the network energy consumption for BS ğ‘– at time
ğ‘¡ is deï¬ned as follows [19], [36]:

ğ‘ƒğ‘– ğ‘†ğ‘– (ğ‘¡)
ğ‘…ğ‘– (ğ‘¡)

ğœ‰net
ğ‘–

(ğ‘¡) =

âˆ‘ï¸

(cid:16)

ğ‘— âˆˆJğ‘–

ğ›¿net
ğ‘–

ğ‘ƒğ‘–ğ‘†ğ‘– (ğ‘¡)
ğ‘…ğ‘– (ğ‘¡)

+ ğœ‚net

st (ğ‘¡)

(cid:17)

,

(4)

ğ‘–

where ğ›¿net
data through the network. In fact, the value of ğ›¿net
on the type of the network device (e.g., ğ›¿net
transceiver remote radio head [36]).

determines the energy coefï¬cient for transferring
depends
ğ‘–
ğ‘– = 2.8 for a 6 unit

3) Total Energy Demand: The total energy consumption
(demand) of the network consists of both MEC server compu-
tational energy (in (2)) consumption, and network the opera-
tional energy (i.e., BSs energy consumption in (4)). Thus, the
overall energy demand of the network at time slot ğ‘¡ is given
as follows:

ğœ‰d
ğ‘¡ =

ğœ‰net
ğ‘–

(ğ‘¡) + ğœ‰MEC

ğ‘–

(ğ‘¡)

.

(5)

(cid:17)

âˆ‘ï¸

(cid:16)

ğ‘– âˆˆB

The demand ğœ‰d
ğ‘¡
on the computational tasks load of the MEC servers.

is random over time and completely depends

B. Energy Generation Model

ğ‘–

ğ‘–

time ğ‘¡

The energy supply of the self-powered wireless network
with MEC capabilities relates to the networkâ€™s own renewable
(e.g., solar, wind, biofuels, etc.) sources as well as the main
gridâ€™s non-renewable (e.g., diesel generator, coal power, and so
on) energy sources [8], [9]. In this energy generation model,
we consider a set R = {R0, R1, . . . , Rğµ} of renewable energy
sources of the network, with each element Rğ‘– representing
the set of renewable energy sources of BS ğ‘– âˆˆ B. Each
renewable energy source ğ‘ âˆˆ Rğ‘– at BS ğ‘– âˆˆ B can generate
ğ‘–ğ‘ (ğ‘¡) of renewable energy at time ğ‘¡. Therefore,
an amount ğœ‰ren
(ğ‘¡) at
the total amount of renewable energy generation ğœ‰ren
(ğ‘¡) = (cid:205)ğ‘ âˆˆRğ‘–
BS ğ‘– âˆˆ B will be ğœ‰ren
ğœ‰ren
ğ‘–ğ‘ (ğ‘¡) for time slot ğ‘¡.
Thus, the total renewable energy generation for the considered
is deï¬ned as ğœ‰ren
(ğ‘¡). The
network at
maximum limit of this renewable energy ğœ‰ren
is less than or
equal to the maximum capacity ğœ‰renmax
of renewable energy
generation at time period ğ‘¡. Thus, we consider a maximum
storage limit that is equal to the maximum capacity ğœ‰renmax
of the renewable energy generation [40]â€“[42]. Further, the
self-powered wireless network is able to get an additional
non-renewable energy amount ğœ‰non
from the main grid at
time ğ‘¡. The per unit renewable and non-renewable energy
cost are deï¬ned by ğ‘ren
, respectively. In general, the
ğ‘¡
renewable energy cost only depends on the maintenance cost
of the renewable energy sources [40]â€“[42]. Therefore, the per
unit non-renewable energy cost is greater than the renewable
energy cost ğ‘non
ğ‘¡ > ğ‘ren
. Additionally, the surplus amount of the
energy ğœ‰sto
at time ğ‘¡ can be stored in energy storage medium
ğ‘¡
for the future usages [41], [42] and the energy storage cost of
per unit energy store is denoted by ğ‘sto

= (cid:205)ğ‘– âˆˆB ğœ‰ren

and ğ‘non

.

ğ‘¡

ğ‘¡

ğ‘¡

ğ‘¡

ğ‘¡

ğ‘¡

ğ‘¡

ğ‘–

ğ‘¡

1) Non-renewable Energy Generation Cost:

the energy demand ğœ‰d
ğ‘¡ when it
ğ‘¡

In order to
is greater than the
fulï¬ll
generated renewable energy ğœ‰ren
, the main grid can provide
an additional amount of energy ğœ‰non
from its non-renewable
sources. Thus, the non-renewable energy generation cost ğ¶non
of the network is determined as follows:
(cid:26) ğ‘non
], if ğœ‰d
ğ‘¡
otherwise,
0,

ğ‘¡ > ğœ‰ren

ğ‘¡ âˆ’ ğœ‰ren

ğ¶non
ğ‘¡

[ğœ‰d

(6)

=

,

ğ‘¡

ğ‘¡

ğ‘¡

ğ‘¡

where ğ‘non

ğ‘¡

represents a unit energy cost.

2) Surplus Energy Storage Cost: The surplus amount of
energy is stored in a storage medium when ğœ‰d
ğ‘¡ < ğœ‰ren
(i.e., energy demand is smaller than the renewable energy
generation) at time ğ‘¡. We consider the per unit energy storage
cost ğ‘sto
. This storage cost depends on the storage medium
and amount of the energy store at time ğ‘¡ [23], [41], [43], [44].
With the per unit energy storage cost ğ‘sto
, the total storage
cost at time ğ‘¡ is deï¬ned as follows:

ğ‘¡

ğ‘¡

ğ‘¡

ğ¶sto

ğ‘¡ =

(cid:26) ğ‘sto
ğ‘¡
0,

[ğœ‰ren

ğ‘¡ âˆ’ ğœ‰d

ğ‘¡ < ğœ‰ren

ğ‘¡

ğ‘¡ ], if ğœ‰d
otherwise.

,

(7)

3) Total Energy Generation Cost: The total energy gen-
eration cost includes renewable, non-renewable, and storage
energy cost. Naturally, this total energy generation cost will

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

7

depend on the energy demand ğœ‰d
ğ‘¡ of the network at time ğ‘¡.
Therefore, the total energy generation cost at time ğ‘¡ is deï¬ned
as follows:

ğ‘„(ğœ‰ren
ğ‘¡

, ğœ‰d

ğ‘¡ ) = ğ‘ren

ğ‘¡ ğœ‰ren
ğ‘¡

+ ğ‘non
ğ‘¡
+ğ‘sto
ğ‘¡

ğ‘¡ âˆ’ ğœ‰ren
[ğœ‰d
]+
ğ‘¡
,
ğ‘¡ âˆ’ ğœ‰d
[ğœ‰ren
ğ‘¡ ]+

(8)

ğ‘¡

ğ‘¡

[ğœ‰d

[ğœ‰ren

, ğ‘non
ğ‘¡

ğ‘¡ âˆ’ ğœ‰d

ğ‘¡ âˆ’ ğœ‰ren

ğ‘¡ ğœ‰ren
ğ‘¡
ğ‘¡ ]+, respectively. In (8), energy demand ğœ‰d

where the energy cost of the renewable, non-renewable, and
storage energy are given by ğ‘ren
]+, and
ğ‘sto
ğ‘¡ and
ğ‘¡
renewable energy generation ğœ‰ren
are stochastic in nature. The
ğ‘¡
energy cost of non-renewable energy (6) and storage energy (7)
completely rely on energy demand ğœ‰d
ğ‘¡ and renewable energy
generation ğœ‰ren
. Hence, to address the uncertainty of both
energy demand and renewable energy generation in a self-
powered wireless network, we formulate a two-stage stochastic
programing problem. In particular, the ï¬rst stage makes a
decision of the energy dispatch without knowing the actual
demand of the network. Then we make further energy dispatch
decisions by analyzing the uncertainty of the network demand
in the second stage. A detailed discussion of the problem
formulation is given in the following section.

III. PROBLEM FORMULATION WITH A TWO-STAGE
STOCHASTIC MODEL

We now consider the case in which the non-renewable en-
ğ‘¡ > ğ‘ren
ergy cost is greater than the renewable energy cost, ğ‘non
that is often the case in a practical smart grid as discussed in
[40], [41], [42], and [45]. Here, ğœ‰ren
ğ‘¡ are the continuous
variables over the observational duration ğ‘¡. The objective is
to minimize the total energy consumption cost ğ‘„(ğœ‰ren
ğ‘¡ ).
ğœ‰ren
is
ğ‘¡
a parameter. When the energy demand ğœ‰d
the
ğ‘¡
optimization problem will be:

, ğœ‰d
is the decision variable and the energy demand ğœ‰d
ğ‘¡
is known,

and ğœ‰d

ğ‘¡

ğ‘¡

ğ‘¡

ğœ’ = min
ğœ‰ ren
ğ‘¡ â‰¥0

ğ‘„(ğœ‰ren
ğ‘¡

, ğœ‰d

ğ‘¡ ).

(9)

In problem (9), after removing the non-negativity constraints
ğœ‰ren
ğ‘¡ â‰¥ 0, we can rewrite the objective function in the form of
piecewise linear functions as follows:

ğ‘„(ğœ‰ren
ğ‘¡

, ğœ‰d

ğ‘¡ ) = max
ğœ‰ ren
ğ‘¡

(cid:110)(cid:16)

(ğ‘ren

ğ‘¡ âˆ’ ğ‘non

ğ‘¡

)ğœ‰ren

ğ‘¡ + ğ‘non

ğ‘¡

(cid:17)

,

ğœ‰d
ğ‘¡

(cid:16)

(ğ‘ren

ğ‘¡ + ğ‘sto

ğ‘¡ )ğœ‰ren

ğ‘¡ âˆ’ ğ‘sto

ğ‘¡ ğœ‰d
ğ‘¡

(10)

(cid:17)(cid:111)

.

ğ‘¡

ğ‘¡

ğ‘¡

)ğœ‰ren

ğ‘¡ < ğœ‰ren

ğ‘¡ + ğ‘non

ğ‘¡ âˆ’ ğ‘non

ğ‘¡ and (ğ‘ren
ğœ‰d

ğ‘¡ âˆ’ ğ‘sto
ğ‘¡

ğ‘¡ )ğœ‰ren
ğ‘¡ > ğœ‰ren

Where (ğ‘ren
ğ‘¡ + ğ‘sto
ğ‘¡ ğœ‰d
ğ‘¡
determine the cost of non-renewable (i.e., ğœ‰d
) and
storage (i.e., ğœ‰d
) energy, respectively. Therefore, we
have to choose one out of the two cases. In fact, if the energy
demand ğœ‰d
ğ‘¡ is known and also the amount of renewable energy
ğœ‰ren
is the same as the energy demand, then problem (10)
ğ‘¡
provides the optimal decision in order to exact amount of
demand ğœ‰d
ğ‘¡ . However, the challenge here is to make a decision
about the renewable energy ğœ‰ren
usage before the demand
ğ‘¡
becomes known. To overcome this challenge, we consider the
energy demand ğœ‰D
ğ‘¡ as a random variable whose probability
distribution can be estimated from the previous history of

the energy demand. We can re-write problem (9) using the
expectation of the total cost as follows:

E[ğ‘„(ğœ‰ren

ğ‘¡

, ğœ‰D

ğ‘¡ )].

min
ğœ‰ ren
ğ‘¡ â‰¥0

(11)

The solution of problem (11) will provide an optimal result on
average. However, in the practical scenario, we need to solve
problem (11) repeatedly over the uncertain energy demand ğœ‰D
ğ‘¡ .
Thus, this solution approach does not signiï¬cantly affect our
model in terms of scalability while ğµ + 1 number of BSs gen-
erates a large variety of energy demand over the observational
period of ğ‘¡. In fact, energy demand and generation can change
over time for each BS ğ‘– âˆˆ B, and they can also induce large
variations of demand-generation among the BSs. Hence, the
solution to problem (11) cannot rely on an iterative scheme
due to a lake of the adaptation for uncertain change of energy
demand and generation over time.

We consider the moment of random variable ğœ‰D
that has
ğ‘¡
a ï¬nitely supported distribution and takes values ğœ‰D
, . . . , ğœ‰D
ğ‘¡ ğµ
ğ‘¡0
with respective probabilities ğ‘0, . . . , ğ‘ ğµ of BSs ğµ + 1. The
cumulative distribution function (CDF) ğ» (ğœ‰D
ğ‘¡ ) of energy de-
is a step function and jumps of size ğ‘ğ‘– at each
mand ğœ‰D
ğ‘¡
demand ğœ‰d
ğ‘¡ğ‘–. Therefore, the probability distribution of each BS
energy demand ğœ‰d
ğ‘¡ ) of historical
observation of energy demand ğœ‰D
ğ‘¡ . In this case, we can convert
problem (11) into a deterministic optimization problem and the
expectation of energy usage cost E[ğ‘„(ğœ‰ren
ğ‘¡ )] is determined
ğ‘¡
by (cid:205)ğ‘– âˆˆB ğ‘ğ‘–ğ‘„(ğœ‰ren
ğ‘¡ ). Thus, we can rewrite the problem (9)
as a linear programming problem using the representation in
(10) as follows:

ğ‘¡ğ‘– belongs to the CDF ğ» (ğœ‰D

, ğœ‰D

, ğœ‰d

ğ‘¡

ğœ’

min
ğœ‰ ren
, ğœ’
ğ‘¡
s.t. ğœ’ â‰¥ (ğ‘ren
ğœ’ â‰¥ (ğ‘ren
ğœ‰renmax
ğ‘¡

ğœ‰d
ğ‘¡ + ğ‘non
)ğœ‰ren
ğ‘¡ âˆ’ ğ‘non
ğ‘¡ ,
ğ‘¡
ğ‘¡
ğ‘¡ + ğ‘sto
ğ‘¡ ,
ğ‘¡ ğœ‰d
ğ‘¡ âˆ’ ğ‘sto
ğ‘¡ )ğœ‰ren
â‰¥ ğœ‰ren

ğ‘¡ â‰¥ 0.

(12)

(12a)

(12b)

(12c)

ğ‘¡

ğ‘¡

, ğœ‰d

For a ï¬xed value of the renewable energy ğœ‰ren
, problem (12)
is an equivalent of problem (10). Meanwhile, problem (12) is
equal to ğ‘„(ğœ‰ren
ğ‘¡ ). We have converted the piecewise linear
function from problem (10) into the inequality constraints
(12a) and (12b). Constraint (12c) ensures a limit on the
maximum allowable renewable energy usage. We consider
ğ‘ğ‘– as a highest probability of energy demand at each BS
ğ‘– âˆˆ B. Therefore, for ğµ + 1 BSs, we deï¬ne ğ‘0, . . . , ğ‘ ğµ
as the probability of energy demand with respect
to BSs
ğ‘– = 0, . . . , ğµ. Thus, we can rewrite the problem (11) for ğµ + 1
BSs ğƒD
ğ‘¡ ğµ) is as follows:

, . . . , ğœ‰D

ğ‘¡ = (ğœ‰D
ğ‘¡0

min
, ğœ’0,...,ğœ’ğµ

ğœ‰ ren
ğ‘¡

âˆ‘ï¸

ğ‘ğ‘– ğœ’ğ‘–,

ğ‘– âˆˆB
s.t. ğœ’ğ‘– â‰¥ (ğ‘ren
ğœ’ğ‘– â‰¥ (ğ‘ren
ğœ‰renmax
ğ‘¡

ğœ‰D
ğ‘¡ğ‘–, âˆ€ğ‘– âˆˆ B,
ğ‘¡ + ğ‘non
)ğœ‰ren
ğ‘¡ âˆ’ ğ‘non
ğ‘¡
ğ‘¡
ğ‘¡ğ‘–, âˆ€ğ‘– âˆˆ B,
ğ‘¡ ğœ‰D
ğ‘¡ âˆ’ ğ‘sto
ğ‘¡ )ğœ‰ren
ğ‘¡ + ğ‘sto
â‰¥ ğœ‰ren

ğ‘¡ â‰¥ 0,

(13)

(13a)

(13b)

(13c)

where ğ‘ğ‘– represents the highest probability of the energy
demand ğœ‰D
ğ‘¡ğ‘– is a random variable and ğœ‰d
ğ‘¡ğ‘–
denotes a realization of energy demand on BS ğ‘– âˆˆ B at time ğ‘¡.
The value of ğ‘ğ‘– belongs to the empirical CDF ğ» (ğƒD
ğ‘¡ğ‘–) of the

ğ‘¡ğ‘–, in which ğœ‰D

ğ‘¡ğ‘– = ğœ‰d

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

8

energy demand ğœ‰D
ğ‘¡ğ‘– for BS ğ‘– âˆˆ B. This CDF is calculated from
the historical observation of the energy demand at BS ğ‘– âˆˆ B. In
fact, for a ï¬xed value of non-renewable energy ğœ‰ren
, problem
(13) is separable. As a result, we can decompose this problem
with a structure of two-stage linear stochastic programming
problem [46], [47].

ğ‘¡

To ï¬nd an approximation for a random variable with a ï¬nite
probability distribution, we decompose problem (13) in a two-
stage linear stochastic programming under uncertainty. The
decision is made using historical data of energy demand, which
is fully independent from the future observation. As a result,
the ï¬rst stage of self-powered energy dispatch problem for
sustainable edge computing is formulated as follows:

min
ğœ‰ ren
ğ‘¡ â‰¥0
s.t.

(ğ‘ren
ğ‘¡

)(cid:62)ğœ‰ren

ğ‘¡ + E[ğ‘„(ğœ‰ren

ğ‘¡

ğœ‰renmax
ğ‘¡

â‰¥ ğœ‰ren

ğ‘¡ â‰¥ 0,

, ğƒD

ğ‘¡ )],

(14)

(14a)

ğ‘¡

ğ‘¡

, ğƒD

where ğ‘„(ğœ‰ren
ğ‘¡ ) determines an optimal value of the second
stage problem. In problem (14), the decision variable ğœ‰ren
is
calculated before the realization of uncertain energy demand
ğƒD
ğ‘¡ . Meanwhile, at the ï¬rst stage of the formulated problem
(14), the cost (ğ‘ren
is minimized for the decision variable
ğœ‰ren
ğ‘¡ which then allows us to estimate the expected energy cost
E[ğ‘„(ğœ‰ren
ğ‘¡ )] for the second stage decision. Constraint (14a)
provides a boundary for the maximum allowable renewable
energy usage. Thus, based on the decision of the ï¬rst stage
problem, the second stage problem can be deï¬ned as follows:

)(cid:62)ğœ‰ren
ğ‘¡

, ğƒD

ğ‘¡

ğ‘¡

min
, ğœ‰ sto
ğ‘¡

ğœ‰ non
ğ‘¡

(ğ‘non
ğ‘¡

)(cid:62)ğœ‰non

ğ‘¡ âˆ’ (ğ‘sto

ğ‘¡ )(cid:62)ğœ‰sto

ğ‘¡

s.t.

ğ‘¡ âˆ’ ğœ‰non
|,
ğ‘¡
ğ‘¡ )(cid:62),
ğ‘¡ â‰¤ (ğƒD

ğœ‰sto
ğ‘¡ = |ğœ‰ren
0 â‰¤ ğœ‰non
ğ‘¡ â‰¥ 0.
ğœ‰non

,

(15)

(15a)

(15b)

(15c)

ğ‘¡

ğ‘¡

ğ‘¡

In the second stage problem (15), the decision variables ğœ‰non
ğ‘¡
and ğœ‰sto
ğ‘¡ depend on the realization of the energy demand ğƒD
ğ‘¡ of
the ï¬rst stage problem (14), where, ğœ‰ren
determines the amount
of renewable energy usage at time ğ‘¡. The ï¬rst constraint (15a)
is an equality constraint that determines the surplus amount
of energy ğœ‰sto
ğ‘¡ must be equal to the absolute value difference
and non-renewable ğœ‰non
between the usage of renewable ğœ‰ren
energy amount. The second constraint (15b) is an inequality
constraint that uses the optimal demand value from the ï¬rst
stage realization. In particular, the value of demand comes
from (5) that is the historical observation of energy demand.
Finally, the constraint (15c) protects from the non-negativity
for the non-renewable energy ğœ‰non

ğ‘¡
The formulated problems (14) and (15) can characterize the
uncertainty between network energy demand and renewable
energy generation. Particularly, the second stage problem (15)
contains random demand ğƒD
leads the optimal cost
ğ‘¡
E[ğ‘„(ğœ‰ren
ğ‘¡ )] as a random variable. As a result, we can
rewrite the problems (14) and (15) in a one large linear pro-
gramming problem for ğµ + 1 BSs and the problem formulation

usage.

, ğƒD

that

ğ‘¡

is as follows:

âˆ‘ï¸

(cid:16)

(ğ‘ren
ğ‘¡

)(cid:62)ğœ‰ren

ğ‘¡ +

min
, ğœ‰ non
ğ‘¡

ğœ‰ ren
ğ‘¡

ğ‘¡ âˆˆT

, ğœ‰ sto
ğ‘¡
âˆ‘ï¸

ğ‘ğ‘– [(ğ‘non

ğ‘¡

)(cid:62)ğœ‰non

ğ‘¡ğ‘– âˆ’ (ğ‘sto

ğ‘¡ )(cid:62)ğœ‰sto
ğ‘¡ğ‘– ]

(16)

(cid:17)

,

(16a)

(16b)

(16c)

s.t.

|, âˆ€ğ‘– âˆˆ B,

ğ‘¡ğ‘–, âˆ€ğ‘– âˆˆ B,

ğ‘– âˆˆB
ğ‘¡ğ‘– âˆ’ ğœ‰non
ğ‘¡ğ‘–

ğ‘¡ğ‘– = |ğœ‰ren
ğœ‰sto
ğ‘¡ğ‘– â‰¤ ğœ‰D
0 â‰¤ ğœ‰non
ğ‘¡ğ‘– â‰¥ 0, âˆ€ğ‘– âˆˆ B,
ğœ‰non
ğœ‰renmax
â‰¥ ğœ‰ren
ğ‘¡

ğ‘¡

ğ‘¡

ğ‘¡

(16d)

, ğœ‰non
ğ‘¡

, ğœ‰non
ğ‘¡

and ğœ‰sto

and ğœ‰sto

ğ‘¡ğ‘– â‰¥ 0, âˆ€ğ‘– âˆˆ B.
In problem (16), for ğµ + 1 BSs, energy demand ğœ‰D
. . . ğœ‰D
ğ‘¡ ğµ
ğ‘¡0
happens with positive probabilities ğ‘0 . . . ğ‘ ğµ and (cid:205)ğ‘– âˆˆB ğ‘ğ‘– = 1.
The decision variables are ğœ‰ren
, which represent
the amount of renewable, non-renewable, and storage energy,
respectively. Constraint (16a) deï¬nes a relationship among
all of the decision variables ğœ‰ren
. In essence,
ğ‘¡
this constraint discretizes the surplus amount of energy for
storage. Hence, constraint (16b) ensures the utilization of non-
renewable energy based on the energy demand of the network.
Constraint (16c) ensures that the decision variable ğœ‰non
ğ‘¡ will
not be a negative value. Finally, constraint (16d) restricts the
usages in to maximum capacity ğœ‰renmax
renewable energy ğœ‰ren
at time ğ‘¡. Problem (16) is an integrated form of the ï¬rst-
stage problem in (14) and the second-stage problem in (15),
where the solution of ğœ‰non
completely depends on
ğ‘¡
ğ‘¡ğ‘– for all ğµ + 1 BSs. The decision
realization of demand ğœ‰D
comes before the realization of demand ğœ‰D
of the ğœ‰ren
ğ‘¡ğ‘– and,
thus, the estimation of renewable energy generation ğœ‰ren
ğ‘¡ will
be independent and random. Therefore, problem (16) holds
the property of relatively complete recourse. In problem (16),
the number of variables and constraints is proportional to
the numbers of BSs, ğµ + 1. Additionally, the complexity of
the decision problem (16) leads to O(2 |T |Ã—|B |) due to the
combinatorial properties of the decisions and constraints [46]â€“
[48].

and ğœ‰sto

ğ‘¡

ğ‘¡

ğ‘¡

ğ‘¡

ğ‘¡ğ‘– , non-renewable ğœ‰non
ğ‘¡ğ‘–

The goal of the self-powered energy dispatch problem
(16) is to ï¬nd an optimal energy dispatch policy that in-
cludes amount of renewable ğœ‰ren
, and
ğ‘¡ğ‘– energy of each BS ğ‘– âˆˆ B while minimizing the
storage ğœ‰sto
energy consumption cost. Meanwhile, such energy dispatch
policy relies on an empirical probability distribution ğ» (ğƒD
ğ‘¡ )
of historical demand at each BS ğ‘– âˆˆ B at time ğ‘¡. In order
to solve problem (16), we choose an approach that does
not rely on the conservativeness of a theoretical probability
distribution of energy demand in problem (16), and also will
capture the uncertainty of renewable energy generation from
the historical data. In contrast, we can construct a theoretical
probability distribution of energy demand ğƒD
ğ‘¡ when we know
what the exact distribution is as well as what its parameters
will be (e.g., mean, variance, and standard deviation). In
fact, in practice, the distribution of energy demand ğƒD
is
ğ‘¡
unknown and instead, a certain amount of historical energy
demand data are available. As a result, we cannot rely on this
distribution to measure uncertainty while the renewable energy
generation and energy demand are random over time. Hence,
we can obtain time-variant features of both energy demand and
generation by characterizing the Markovian properties from

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

9

Algorithm 1 State Space Generation of BS ğ‘– âˆˆ B in MAMRL
Framework
Input: ğ‘Šğ‘– ğ‘— , ğ‘ƒğ‘–, ğ‘”ğ‘– ğ‘— (ğ‘¡), ğœ2, ğ¼ğ‘– ğ‘— (ğ‘¡), Î¥ ğ‘— ğ‘˜ğ‘– (ğ‘¡), ğœ, ğ‘“ğ‘˜ğ‘– , ğœ›ğ‘˜ğ‘–ğ‘™ , ğœ‚MEC
Input: ğ›¿net
, ğœ‚net
ğ‘–
Output: ğ’”ğ‘¡ğ‘– : (ğœ‰d

), âˆ€ğ’”ğ‘¡ğ‘– âˆˆ Sğ‘– âˆˆ S, âˆ€ğ‘¡ âˆˆ T

st (ğ‘¡), ğ‘non
ğ‘¡
ğ‘– , ğœ‰ren
ğ‘–

, ğ‘sto
ğ‘¡
(ğ‘¡), ğ¶sto

ğ‘¡ğ‘– , ğ¶non
ğ‘¡ğ‘–

st

Initialization: Rğ‘–, Kğ‘–, Jğ‘–,Sğ‘–, ğœ†ğ‘– (ğ‘¡), ğœ‡ğ‘– (ğ‘¡), ğœŒğ‘– (ğ‘¡), ğ‘…ğ‘– (ğ‘¡)

(ğ‘¡), ğ‘†ğ‘– (ğ‘¡)

for each ğ‘– âˆˆ B do

1: for each ğ‘¡ âˆˆ T do
2:
3:
4:
5:
6:

for each ğ‘— âˆˆ Jğ‘– do
Calculate: ğœ‰MEC
Calculate: ğœ‰net

ğ‘–

ğ‘–

(ğ‘¡) using eq. (2)

(ğ‘¡) using eq. (4)

ğ‘–

end for
ğ‘¡ = ğœ‰net
Calculate: ğœ‰d
(ğ‘¡) + ğœ‰MEC
ğ‘–
ğ‘¡ = (cid:205)ğ‘ âˆˆR ğœ‰ren
Calculate: ğœ‰ren
ğ‘–ğ‘ (ğ‘¡)
Calculate: ğ¶non
using eq. (6)
ğ‘¡
Calculate: ğ¶sto
using eq. (7)
(ğ‘¡), ğ¶sto
ğ‘– , ğœ‰ren
Assign: ğ’”ğ‘¡ğ‘– : (ğœ‰d
ğ‘–

ğ‘¡

ğ‘¡ğ‘– , ğ¶non
ğ‘¡ğ‘–

)

7:
8:
9:
10:
11:
12:

end for
Append: ğ’”ğ‘¡ğ‘– âˆˆ Sğ‘–

13:
14: end for
15: return âˆ€Sğ‘– âˆˆ S

(ğ‘¡) using eq. (5)

for energy dispatch in the considered network. The proposed
MAMRL framework includes two types of agents: A local
agent that acts as a local learner at each self-powered with
MEC capabilities BS and a meta-agent that learns the global
energy dispatch policy. In particular, each local BS agent
can discretize the Markovian dynamics for energy demand-
generation of each BS (i.e., both SBSs and MBS) separately
by applying deep-reinforcement learning. Meanwhile, we train
a long short-term memory (LSTM) [49], [50] as a meta-
agent at the MBS that optimizes [26] the accumulated energy
dispatch of the local agents. As a result, the meta-agent can
handle the non-i.i.d. energy demand-generation of the each
local agent with own state information of the LSTM. To this
end, MAMRL mitigates the curse of dimensionality for the
uncertainty of energy demand and generation while providing
an energy dispatch solution with a less computational and
communication complexity (i.e., less message passing between
the local agents and meta-agent).

A. Preliminary Setup

(ğ‘¡), ğ¶sto

ğ‘¡ğ‘– , ğ¶non
ğ‘¡ğ‘–

In the MAMRL setting, each BS ğ‘– âˆˆ B acts as a local agent
and the number of local agents is equal to ğµ + 1 BSs (i.e., 1
MBS and ğµ SBSs). We deï¬ne a set S = {S0, S1, . . . , Sğµ} of
state spaces and a set A = {A0, A1, . . . , Ağµ} of actions for
the ğµ + 1 agents. The state space of a local agent ğ‘– is deï¬ned
by ğ’”ğ‘¡ğ‘– : (ğœ‰d
ğ‘– , ğœ‰ren
(ğ‘¡), ğ¶sto
ğ‘¡ğ‘– ,
ğ‘–
and ğ¶non
represent the amount of energy demand, renewable
ğ‘¡ğ‘–
generation, storage cost, and non-renewable energy cost, re-
spectively, at time ğ‘¡. We execute Algorithm 1 to generate the
state space for every BSs ğ‘– âˆˆ B, individually. In Algorithm
1, lines 3 to 6 calculate the individual energy consumption of
the MEC computation and network operation using (2) and
(4), respectively. Overall, the energy demand of the BS ğ‘– is
computed in line 7 and the self-powered energy generation is

) âˆˆ Sğ‘–, where ğœ‰d

ğ‘– , ğœ‰ren
ğ‘–

Fig. 3: Multi-agent meta-reinforcement learning framework.

the historical observation over time. In particular, we capture
the dynamics of Markovian by considering a data-driven
approach. This approach can overcome the conservativeness
of theoretical probability distribution as historical observation
goes to ï¬nitely many.

To prevalence the aforementioned contemporary, we pro-
pose a multi-agent meta-reinforcement
learning framework
that can explore the Markovian behavior from historical energy
demand and generation of each BS ğ‘– âˆˆ B. Meanwhile, meta-
agent can cope with such time-varying features to a globally
optimal energy dispatch policy for each BS ğ‘– âˆˆ B.

We design an MAMRL framework by converting the cost
minimization problem (16) to a reward maximization prob-
lem that we then solve with a data-driven approach. In the
MAMRL setting, each agent works as a local agent for each
BS ğ‘– âˆˆ B and determines an observation (i.e., exploration)
for the decision variables, renewable ğœ‰ren
ğ‘¡ğ‘– , non-renewable ğœ‰non
,
ğ‘¡ğ‘–
and storage ğœ‰sto
ğ‘¡ğ‘– energy. The goal of this exploration is to ï¬nd
time-varying features from the local historical data so that the
energy demand ğœ‰d
ğ‘¡ğ‘– of the network is satisï¬ed. Furthermore,
using these observations and current state information, a meta-
agent is used to determine a stochastic energy dispatch policy.
Thus, to obtain such dispatch policy, the meta-agent only
requires the observations (behavior) from each local agent.
Then, the meta-agent can evaluate (exploit) behavior toward an
optimal decision for dispatching energy. Further, the MAMRL
approach is capable of capturing the exploration-exploitation
tradeoff in a way that the meta-agent optimizes decisions of the
each self-powered BS under uncertainty. A detailed discussion
of the MAMRL framework is given in the following section.

IV. ENERGY DISPATCH WITH MULTI-AGENT
META-REINFORCEMENT LEARNING FRAMEWORK

In this section, we developed our proposed multi-agent
meta-reinforcement learning framework (as seen in Fig. 3)

Meta Agent Learningâ€¦LSTM for Meta-Policy Optimizer Energy Environment BS i=0Local Agent BS i=0StateRewardActionEnergy Environment BS i=BLocal Agent BS i=BStateObservation MemoryMeta-AdvantageFunctionShared Neural Networks Meta-PolicyRNN State Parameter UpdateObservationsValue FunctionRewardActionValue FunctionLocal Agent Policy GradientLocal Agent Policy GradientLocal AdvantageFunctionACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

10

estimated by line 8 in Algorithm 1. Non-renewable and storage
energy costs are calculated in lines 9 and 10 for time slot ğ‘¡.
Finally, line 11 creates state space tuple (i.e., ğ’”ğ‘¡ğ‘–) for time ğ‘¡
in Algorithm 1.

B. Local Agent Design

ğ‘–

ğ‘–

Consider each local BS agent ğ‘– âˆˆ B that can take two
types of actions ğœ‰sto
(ğ‘¡) and ğœ‰non
(ğ‘¡) which are the amount
ğ‘–
of storage energy ğœ‰sto
(ğ‘¡), and the amount of non-renewable
ğ‘–
energy ğœ‰non
(ğ‘¡) at time ğ‘¡. We consider a discrete set of actions
(ğ‘¡)) âˆˆ Ağ‘– for
that consists of two actions ğ’‚ğ‘¡ğ‘– : (ğœ‰sto
ğ‘–
each BS unit ğ‘– âˆˆ B. Since the state ğ’”ğ‘¡ğ‘– and action ğ’‚ğ‘¡ğ‘– both
contain a time varying information of the agent ğ‘– âˆˆ B, we
consider the dynamics of Markovian and represent problem
(16) as a discounted reward maximization problem for each
agent ğ‘– (i.e., each BS). Thus, the objective function of the
discounted reward maximization problem of agent ğ‘– is deï¬ned
as follows [28]:

(ğ‘¡), ğœ‰non
ğ‘–

ğ‘Ÿğ‘– (ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–) = max
ğ’‚ğ‘¡ğ‘– âˆˆAğ‘–

Eğ’‚ğ‘¡ğ‘–âˆ¼ğ’”ğ‘¡ğ‘– [

âˆ
âˆ‘ï¸

ğ‘¡(cid:48)=ğ‘¡

ğ›¾ğ‘¡(cid:48)âˆ’ğ‘¡ Î¥ğ‘¡ (ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–)],

(17)

is a discount factor and each reward

where ğ›¾ âˆˆ (0, 1)
Î¥ğ‘¡ ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–) is considered as,
(cid:40)

Î¥ğ‘¡ (ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–) =

1, if
0,

ğœ‰ ren
ğ‘¡ğ‘–
ğœ‰ d
ğ‘¡ğ‘–

> 1,
otherwise.

ğ‘–

ğ‘–

ğ‘–

(ğ‘¡)) = ğ‘ƒ(ğ’‚ğ‘¡ğ‘– = ğœ‰sto

Here, the optimal value function (20) learns a parameterized
policy ğœ‹ ğœƒğ‘– (ğ’‚ğ‘¡ğ‘– |ğ’”ğ‘¡ğ‘–; ğœƒğ‘–) by using an LSTM-based Q-networks
for the parameters ğœƒğ‘–. Thus, each BS agent ğ‘– âˆˆ B determines
its parametrized energy dispatch policy ğœ‹ ğœƒğ‘– (ğ’‚ğ‘¡ğ‘– |ğ’”ğ‘¡ğ‘–; ğœƒğ‘–) =
ğ‘ƒ(ğ’‚ğ‘¡ğ‘– |ğ’”ğ‘¡ğ‘–; ğœƒğ‘–), where ğ‘ƒ(ğœ‰sto
(ğ‘¡)|ğ’”ğ‘¡ğ‘–; ğœƒğ‘–) and
ğ‘ƒ(ğœ‰non
(ğ‘¡)) for the parameters ğœƒğ‘–. The decision
(ğ‘¡)) = 1âˆ’ ğ‘ƒ(ğœ‰sto
ğ‘–
of each BS agent ğ‘– âˆˆ B relies on ğœƒğ‘–. In particular, energy
dispatch policy ğœ‹ ğœƒğ‘– is the probability of taking action ğ’‚ğ‘¡ğ‘– for
a given state ğ’”ğ‘¡ğ‘– with parameters ğœƒğ‘–. In this setting, each local
agent ğ‘– âˆˆ B is comprised of an actor and a critic [27], [51].
The policy of energy dispatch is determined by choosing an
action in (20) that can be seen as an actor of BS agent ğ‘–.
Meanwhile, the value function (19) is estimated by a critic of
each local BS agent ğ‘–. The critic can criticize actions that are
made by the actor of each BS agent ğ‘–. Therefore, each BS
agent ğ‘– âˆˆ B can determine a temporal difference (TD) error
[51] based on the current energy dispatch policy of the actor
and value estimation by the critic. The TD error is considered
as an advantage function and the advantage function of agent
ğ‘– is deï¬ned as follows:

Î› ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡ğ‘–) =
(cid:16)

ğ‘Ÿğ‘– (ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–) +

âˆ
âˆ‘ï¸

ğ‘¡(cid:48)=ğ‘¡

ğ›¾ğ‘¡(cid:48)âˆ’ğ‘¡ğ‘‰ ğœ‹ğœƒğ‘– (ğ’”ğ‘¡(cid:48)ğ‘–)

(cid:17)

âˆ’ ğ‘‰ ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–).

(21)

(18)

Thus, the policy gradient of each BS agent ğ‘– âˆˆ B is determined
as,

ğœ‰ ren
ğ‘¡ğ‘–
ğœ‰ d
ğ‘¡ğ‘–

determines a ratio between renewable energy
In (18),
generation and energy demand (supply-demand ratio) of the
BS agent ğ‘– âˆˆ B at time ğ‘¡. When renewable energy generation-
is larger than 1 then the BS agent ğ‘– achieves
demand ratio
a reward of 1 because the amount of renewable energy exceeds
the demand that can be stored in the storage unit.

ğœ‰ ren
ğ‘¡ğ‘–
ğœ‰ d
ğ‘¡ğ‘–

Each action ğ’‚ğ‘¡ğ‘– of BS agent ğ‘– âˆˆ B determines a stochastic
policy ğœ‹ ğœƒğ‘– . ğœƒğ‘– is a parameter of ğœ‹ ğœƒğ‘– and the energy dispatch
policy is deï¬ned by ğœ‹ ğœƒğ‘– : Sğ‘– Ã— Ağ‘– â†¦â†’ [0, 1]. Policy ğœ‹ ğœƒğ‘– decides
a state transition function Î“ : Sğ‘– Ã— Ağµ â†¦â†’ Sğ‘– for the next
state ğ’”ğ‘¡(cid:48)ğ‘– âˆˆ Sğ‘–. Thus, the state transition function Î“ of BS
agent ğ‘– âˆˆ B is determined by a reward function (18), where
â†¦â†’ R. Further, each BS agent ğ‘– âˆˆ B
Î¥ğ‘¡ (ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–) : Sğ‘– Ã— Ağ‘–
chooses an action ğ’‚ğ‘¡ğ‘– from a parametrized energy dispatch
policy ğœ‹ ğœƒğ‘– (ğ’‚ğ‘¡ğ‘– |ğ’”ğ‘¡ğ‘–; ğœƒğ‘–). Therefore, for a given state ğ’”ğ‘¡ğ‘–, the state
value function with a cumulative discounted reward will be:

ğ‘‰ ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–) = Eğ’‚ğ‘¡ğ‘–âˆ¼ ğœ‹ğœƒğ‘– (ğ’‚ğ‘¡ğ‘– |ğ’”ğ‘¡ğ‘–;ğœƒğ‘–) [

âˆ
âˆ‘ï¸

ğ›¾ğ‘¡(cid:48)âˆ’ğ‘¡ Î¥ğ‘¡+ğ‘¡(cid:48) (ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–)|ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡ğ‘–],

ğ‘¡(cid:48)=ğ‘¡

(19)
where ğ›¾ğ‘¡(cid:48)âˆ’ğ‘¡ is a discount factor and ensures the convergence
of state value function ğ‘‰ ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–) over the inï¬nity time horizon.
Thus, for a given state ğ’”ğ‘¡ğ‘–, the optimal policy ğœ‹âˆ—
(ğ’‚ğ‘¡ğ‘– |ğ’”ğ‘¡ğ‘–) for
ğœƒğ‘–
the next state ğ’”ğ‘¡(cid:48)ğ‘– can be determined by an optimal state value
function while a Markovian property is imposed. Therefore,
the optimal value function is given as follows:

âˆ‡ğœƒğ‘– Î› ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡ğ‘–) =
âˆ
âˆ‘ï¸

E ğœ‹ğœƒğ‘– [

ğ‘¡(cid:48)=ğ‘¡

ğ›¾ğ‘¡(cid:48)âˆ’ğ‘¡ âˆ‡ğœƒğ‘– log ğœ‹ ğœƒğ‘– (ğ’‚ğ‘¡ğ‘– |ğ’”ğ‘¡ğ‘–; ğœƒğ‘–)Î› ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡ğ‘–)],

(22)

where log ğœ‹ ğœƒğ‘– (ğ’‚ğ‘¡ğ‘– |ğ’”ğ‘¡ğ‘–; ğœƒğ‘–), and Î› ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡ğ‘–) represent the ac-
tor and critic, respectively, for each local BS agent ğ‘– âˆˆ B.

ğ‘–

(ğ‘¡), ğœ‰non
ğ‘–

Using (22), we can discretize the energy dispatch decision
ğ’‚ğ‘¡ğ‘– : (ğœ‰sto
(ğ‘¡)) for each self-powered BS ğ‘– âˆˆ B in the
network. In fact, we can achieve a centralized solution for
âˆ€ğ‘– âˆˆ B when all of the BSs state information (i.e., demand
and generation) are known. However, the space complexity
for computation increases as ğ‘‚ (2 |Sğ‘– |Ã—|Ağ‘– |Ã—|B |Ã—ğ‘‡ ) and also the
computational complexity becomes ğ‘‚ (|Sğ‘– | Ã— |Ağ‘– | Ã— |B|2 Ã— ğ‘‡)
[21]. Further,
the exploration-
the solution does not meet
exploitation dilemma since the centralized (i.e., single agent)
method ignores the interactions and energy dispatch decision
strategies of other agents (i.e., BSs) which creates an imbal-
ance between exploration and exploitation. In other words, this
learning approach optimizes the action policy by exploring
its own state information. Therefore, when we change the
energy environment (i.e., demand and generation), this method
cannot cope with an unknown environment due to the lack of
diverse state information during the training. Next, we propose
an approach that not only reduces the complexity but also
explores alternative energy dispatch decision to achieve the
highest expected reward in (17).

ğ‘‰ ğœ‹âˆ—

ğœƒğ‘– (ğ’”ğ‘¡ğ‘–) =

max
ğ‘ğ‘¡ âˆˆA

E ğœ‹âˆ—
ğœƒğ‘–

[

âˆ‘ï¸

ğ‘– âˆˆB

ğ‘Ÿğ‘– (ğ’‚ğ‘¡(cid:48)ğ‘–, ğ’”ğ‘¡(cid:48)ğ‘–) +

âˆ
âˆ‘ï¸

ğ‘¡(cid:48)=ğ‘¡

ğ›¾ğ‘¡(cid:48)âˆ’ğ‘¡ğ‘‰ ğœ‹ğœƒğ‘–
ğ‘¡(cid:48)

(ğ’”ğ‘¡(cid:48)ğ‘–)|ğ’”ğ‘¡ğ‘–; ğœƒğ‘–, ğ’‚ğ‘¡ğ‘–].

(20)

C. Multi-Agent Meta-Reinforcement Learning Modeling

We consider a set O = {O0, O1, . . . , Oğµ} of ğµ + 1 ob-
servations [24], [52] and for an BS agent ğ‘– âˆˆ B, a single

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

11

observation tuple is given by ğ’ğ‘– âˆˆ Oğ‘–. For a given state
ğ’”ğ‘¡ğ‘–,
the observation ğ’ğ‘– of the next state ğ’”ğ‘¡(cid:48)ğ‘– consists of
ğ’ğ‘– : (ğ‘Ÿğ‘– (ğ’‚ğ‘¡(cid:48)ğ‘–, ğ’”ğ‘¡(cid:48)ğ‘–), ğ‘Ÿğ‘– (ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–), ğ’‚ğ‘¡ğ‘–, ğ’‚ğ‘¡(cid:48)ğ‘–, ğ‘¡ (cid:48), Î› ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡ğ‘–)), where
ğ‘Ÿğ‘– (ğ’‚ğ‘¡(cid:48)ğ‘–, ğ’”ğ‘¡(cid:48)ğ‘–), ğ‘Ÿğ‘– (ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–), ğ’‚ğ‘¡ğ‘–, ğ’‚ğ‘¡(cid:48)ğ‘–, ğ‘¡ (cid:48) and Î› ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡ğ‘–) are next-
state discounted rewards, current state discounted rewards,
next action, current action, time slot, and TD error, respec-
tively. Here, a complete information of the observation ğ’ğ‘– is
correlated with the state space ğ’ğ‘– : Sğ‘– â†¦â†’ Oğ‘– while observation
ğ’ğ‘– does not require the complete state information of the
previous states.

Thus, the space complexity for computation at each BS
ğ‘– âˆˆ B leads to ğ‘‚ ((|Sğ‘– | + |Ağ‘– |)2 Ã— ğ‘‡). Meanwhile,
agent
the computational complexity for each time slot ğ‘¡ becomes
ğ‘‚ (|Sğ‘– |2Ã—Ağ‘– Ã—ğœƒğ‘¡ +ğ»), where ğœƒğ‘¡ is the learning parameter and ğ»
represents the numbers of LSTM units. Each BS agent ğ‘– âˆˆ B
requires to send an amount of |Oğ‘– | observational data (i.e.,
the communication
payload) to the meta-agent. Therefore,
overhead for each BS agent ğ‘– âˆˆ B leads to ğ‘‚ ( |O |Ã—ğ‘‡
ğµ+1 ). On
the other hand, the computational complexity of the meta-
agent leads to ğ‘‚ (|O|2 Ã— ğœ™ + ğ») while ğœ™ represents learning
parameter at meta-agent. In particular, for a ï¬xed number of
output memory ğœ™, the meta-agentâ€™s update complexity at each
time slot ğ‘¡ becomes ğ‘‚ (ğœ™2) [53]. Further, when transferring
the learned parameters ğœƒğ‘¡(cid:48) from the meta-agent to all local
agents âˆ€ğ‘– âˆˆ B,
the communication overhead goes to the
ğ‘‚ (ğœƒğ‘¡(cid:48) Ã—(ğµ+1)) at each time slot ğ‘¡. Here, the size of ğœƒğ‘¡(cid:48) depends
on the memory size of the LSTM cell at the meta-agent [see
Appendix A].

In the MAMRL framework, the local agents work as an
optimizee and the meta-agent performs the role of optimizer
[26]. To model our meta-agent, we consider an LSTM archi-
tecture [49], [50] that stores its own state information (i.e.,
parameters) and the local agent (i.e., optimizee) only provides
the observation of a current state. In the proposed MAMRL
framework, a policy ğœ‹ ğœƒğ‘– is determined by updating the parame-
ters 1 ğœƒğ‘–. Therefore, we can represent the state value function
(20) for time ğ‘¡ is as follows: ğ‘‰ ğœ‹âˆ—
ğœƒğ‘– (ğ’”ğ‘¡ğ‘–) â‰ˆ ğ‘‰ ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–; ğœƒğ‘¡ ), and
the advantage (temporal difference) function (21) is presented
by, Î› ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡ğ‘–) â‰ˆ Î› ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡ğ‘–; ğœƒğ‘¡ ). As a result, the param-
eterized policy is deï¬ned by, ğœ‹ ğœƒğ‘– (ğ’‚ğ‘¡ğ‘– |ğ’”ğ‘¡ğ‘–) â‰ˆ ğœ‹ ğœƒğ‘– (ğ’‚ğ‘¡ğ‘– |ğ’”ğ‘¡ğ‘–; ğœƒğ‘¡ ).
Considering all of the BS agents ğµ + 1 and the advantage
function (21) is rewritten as,

Î›

ğœ‹âˆ—
ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡0, . . . , ğ’‚ğ‘¡ ğµ; ğœƒğ‘¡ ) = ğ‘Ÿğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡0, . . . , ğ’‚ğ‘¡ ğµ) +
âˆ
âˆ‘ï¸

ğ›¾ğ‘¡(cid:48)âˆ’ğ‘¡ Î“(ğ’”ğ‘¡(cid:48)ğ‘– |ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡0, . . . , ğ’‚ğ‘¡ ğµ)ğ‘‰ ğœ‹ğœƒğ‘– (ğ’”ğ‘¡(cid:48)ğ‘–, ğœ‹âˆ—
ğœƒ0

, . . . , ğœ‹âˆ—

ğœƒğµ ) âˆ’

ğ’”ğ‘¡(cid:48)ğ‘– âˆˆSğ‘– ,ğ‘¡(cid:48)=ğ‘¡

ğ‘‰ ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğœ‹âˆ—
ğœƒ0

, . . . , ğœ‹âˆ—

ğœƒğµ ),

(23)

, . . . , ğœ‹âˆ—

ğœƒ : (ğœ‹âˆ—
ğœƒ0

where ğœ‹âˆ—
) is a joint energy dispatch policy and
Î“(ğ’”ğ‘¡(cid:48)ğ‘– |ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡0, . . . , ğ’‚ğ‘¡ ğµ) â†¦â†’ [0, 1] represents state transition
probability. Using (23), we can get the value loss function for

ğœƒğµ

agent ğ‘– and the objective is to minimize the temporal difference
[27],

ğ¿ (ğœƒğ‘–) =

(cid:18) (cid:16)

min
ğœ‹ğœƒğ‘–

1
|B|

âˆ‘ï¸

ğ‘– âˆˆB

1
2

ğ‘Ÿğ‘– (ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–) +

âˆ
âˆ‘ï¸

ğ‘¡(cid:48)=ğ‘¡

ğ›¾ğ‘¡(cid:48)âˆ’ğ‘¡ğ‘‰ ğœ‹âˆ—

ğœƒğ‘– (ğ’”ğ‘¡(cid:48)ğ‘– |ğœƒğ‘¡ )

(cid:17)

âˆ’ ğ‘‰ ğœ‹âˆ—

ğœƒğ‘– (ğ’”ğ‘¡ğ‘–)

(cid:19) 2

.

(24)

To improve the exploration with a low bias, we consider
an entropy regularization 2 ğ›½â„(ğœ‹ ğœƒğ‘– (ğ’‚ğ‘¡ğ‘– |ğ’”ğ‘¡ğ‘–; ğœƒğ‘¡ )) that cope with
the non-i.i.d. energy demand and generation for all of the BS
agents âˆ€ğ‘– âˆˆ B. Here, ğ›½ is a coefï¬cient for the magnitude of
regularization and â„(ğœ‹ ğœƒğ‘– (ğ’‚ğ‘¡ğ‘– |ğ’”ğ‘¡ğ‘–; ğœƒğ‘¡ )) determines the entropy of
the policy ğœ‹ ğœƒğ‘– for the parameter ğœƒğ‘–. Additionally, a larger value
of ğ›½â„(ğœ‹ ğœƒğ‘– (ğ’‚ğ‘¡ğ‘– |ğ’”ğ‘¡ğ‘–; ğœƒğ‘¡ )) encourages the agents to have a more
diverse exploration to estimate the energy dispatch policy.
Thus, we can redeï¬ne the policy loss function as follows:

ğ¿(ğœƒğ‘–) = âˆ’Eğ’”ğ‘¡ğ‘– ,ğ’‚ğ‘¡ğ‘– [ğœ‹ ğœƒğ‘– (ğ’‚ğ‘¡ğ‘– |ğ’”ğ‘¡ğ‘–) + ğ›½â„(ğœ‹ ğœƒğ‘– (ğ’‚ğ‘¡ğ‘– |ğ’”ğ‘¡ğ‘–; ğœƒğ‘¡ ))].

(25)

Therefore, the policy gradient of the loss function (25) is
deï¬ned in terms of temporal difference and entropy. The policy
gradient of the loss function is deï¬ned as follows:

âˆ‡ğœƒğ‘– ğ¿(ğœƒğ‘–) =

1
|B|

âˆ‘ï¸

âˆ
âˆ‘ï¸

âˆ‡ğœƒğ‘– log ğœ‹ ğœƒğ‘– (ğ’‚ğ‘¡ğ‘– |ğ’”ğ‘¡ğ‘–)Î› ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡ğ‘– |ğœƒğ‘¡ )

ğ‘– âˆˆB

ğ‘¡(cid:48)=ğ‘¡
+ ğ›½â„(ğœ‹ ğœƒğ‘– (ğ’‚ğ‘¡ğ‘– |ğ’”ğ‘¡ğ‘–; ğœƒğ‘¡ )).

(26)

ğ‘– âˆˆ B at

To design our meta-agent, we consider meta-agent param-
eters ğœ™ and optimized parameters ğœƒâˆ— of the optimizee (i.e.,
is deï¬ned as ğ‘€ğ‘¡ (Oğ‘¡ ; ğœ™) (cid:66)
local agent). The meta-agent
ğ‘€ğ‘¡ (âˆ‡ğœƒğ‘¡ ğ¿(ğœƒğ‘¡ ); ğœ™), where ğ‘€ğ‘¡ (.)
is modeled by an LSTM.
Consider an observational vector Oğ‘–ğ‘¡(cid:48) âˆˆ O of a local
time ğ‘¡ (cid:48) and each observation is
BS agent
ğ’ğ‘– : (ğ‘Ÿğ‘– (ğ’‚ğ‘¡(cid:48)ğ‘–, ğ’”ğ‘¡(cid:48)ğ‘–), ğ‘Ÿğ‘– (ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–), ğ’‚ğ‘¡ğ‘–, ğ’‚ğ‘¡(cid:48)ğ‘–, ğ‘¡ (cid:48), Î› ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡ğ‘–)) âˆˆ Oğ‘–ğ‘¡(cid:48).
The LSTM-based meta-agent takes the observational vector
Oğ‘–ğ‘¡(cid:48) as an input. Meanwhile,
the meta-agent holds long-
term dependencies by generating its own state with param-
eters ğœ™. To do this, the LSTM model creates several gates
to determine an optimal policy ğœ‹âˆ—
ğœƒğ‘– and advantage function
ğœ‹âˆ—
ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡0, . . . , ğ’‚ğ‘¡ ğµ; ğœƒğ‘¡ ) for the next state ğ’”ğ‘¡(cid:48)ğ‘–. As a result,
Î›
the structure of the recurrent neural network for the meta-agent
is the same as the LSTM model [49], [50]. In particular, each
LSTM unit for the meta-agent consists of four gate layers such
as forget gate ğ‘­ğ‘¡(cid:48), input gate ğ‘°ğ‘¡(cid:48), cell state Ë†ğ‘¬ğ‘¡(cid:48), and output ğ’ğ‘¡(cid:48)
layer. The cell state gate Ë†ğ‘¬ğ‘¡(cid:48) usages a tanh activation function
and other gates are used sigmoid ğœ(.) as an activation function.

1We consider recurrent neural networks (RNNs) state parameters for the
parameterization of energy dispatch policy. In particular, we consider a long
short-term memory (LSTM) for RNN, in which cell state and hidden state
are considered as parameters.

2Entropy [54]â€“[57] can allow us to manage non-i.i.d. datasets when changes
in the environment over time lead to an uncertainty. Therefore, we use entropy
regularization to handle the non-i.i.d. energy demand and generation over time
by managing with the uncertainty for each BS agent ğ‘– âˆˆ B.

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

12

Thus, the outcome of the meta policy for a single unit LSTM
cell is presented as follows:

ğ‘€ğ‘¡(cid:48) (Oğ‘¡(cid:48); ğœ™) = softmax

(cid:16)

(ğ‘¯ğ‘¡(cid:48))(cid:62)(cid:17)

,

(cid:16)

where ğ‘­ğ‘¡(cid:48) = ğœ
(cid:16)

ğ‘°ğ’•(cid:48) = ğœ

ğœ™ğ¹ ğ‘‚ (Oğ‘–ğ‘¡(cid:48))(cid:62) + ğœ™ğ¹ ğ» (ğ‘¯ğ‘¡ )(cid:62) + ğ’ƒğ¹
(cid:17)

ğœ™ğ¼ ğ‘‚ (Oğ‘–ğ‘¡(cid:48))(cid:62) + ğœ™ğ¼ ğ» (ğ‘¯ğ‘¡ )(cid:62) + ğ’ƒğ¼

,

(cid:17)

,

ğœ™ğ¸ğ‘‚ (Oğ‘–ğ‘¡(cid:48))(cid:62) + ğœ™ğ¸ ğ» (ğ‘¯ğ‘¡ )(cid:62) + ğ’ƒğ¸

(cid:16)

Ë†ğ‘¬ğ’•(cid:48) = tanh
ğ‘¬ğ‘¡(cid:48) = Ë†ğ‘¬ğ’•(cid:48) (cid:12) ğ‘°ğ‘¡(cid:48) + ğ‘­ğ‘¡(cid:48) (cid:12) ğ‘¬ğ‘¡ ,
ğ’ğ‘¡(cid:48) = ğœ
ğ‘¯ğ‘¡(cid:48) = tanh(ğ‘¬ğ‘¡(cid:48)) (cid:12) ğ’ğ‘¡(cid:48).

(cid:16)

ğœ™ğ‘ ğ‘‚ (Oğ‘–ğ‘¡(cid:48))(cid:62) + ğœ™ğ‘ ğ» (ğ‘¯ğ‘¡ )(cid:62) + ğ’ƒğ‘

(cid:17)

,

(27)

(27a)

(27b)

(cid:17)

, (27c)

(27d)

(27e)

(27f)

In the meta-agent policy formulation (27), the forget gate
vector (27a) determines what information is needed to throw
away. Input gate vector (27b) helps to decide which informa-
tion is needed to update, the cell state (27c) creates a vector
of new candidate values using tanh(Â·) function, and updates
the cell state information by applying (27d). The output layer
(27e) that determines what parts of the cell state are going
to output and calculate the cell outputs using the equation
(27f). Further, the cell state through the tanh(Â·) will restrict
the values between âˆ’1 and +1. This entire process is followed
for each LSTM block and ï¬nally, (27) determines the meta-
policy for ğœ‹âˆ—
ğœƒğ‘– of the state ğ‘ ğ‘¡(cid:48). In addition, optimized RNN
state parameters ğœƒâˆ— are obtained from the cell state (27d) and
hidden state (27f) of an LSTM unit. Thus, the loss function
ğ¿ (ğœ™) = Eğ¿ ( ğœƒ) [ğ¿ (ğœƒâˆ—(ğ¿(ğœƒ); ğœ™))] of meta-agent depends on the
distribution of ğ¿(ğœƒğ‘¡ ) and the expectation of the meta-agent
loss function is deï¬ned as follows [26]:

ğ¿(ğœ™) = Eğ¿ ( ğœƒ) [

ğ‘‡
âˆ‘ï¸

ğ‘¡=1

ğ¿ (ğœƒğ‘¡ )].

(28)

In the proposed MAMRL framework, we transfer the learned
parameters (i.e., cell state and hidden state) of meta-agent to
the local agents so that each local agent will be estimated an
optimal energy dispatch policy by updating its own learning
parameters. Thus, the parameters of each agent (i.e., BS) is
updated with ğœƒğ‘¡(cid:48) = ğœƒâˆ— while ğœ‹âˆ—
= ğ‘€ğ‘¡ (âˆ‡ğœƒğ‘¡ ğ¿ (ğœƒğ‘¡ ); ğœ™) to decide
ğœƒğ‘–
the energy dispatch policy.

We consider an LSTM-based recurrent neural network
(RNN) for the both local agents and the meta-agent. This
LSTM RNN consists of 48 LSTM units for each LSTM cell as
shown in Fig. 4. In particular, the conï¬guration of the LSTM
for the meta-agent and each local agent is the same while
the objective of the loss functions differ from local agent
to meta-agent. In which, local BS agent determines its own
energy dispatch policy by exploring its own environmental
state information for reducing the TD error. Meanwhile,
meta-agent deals with the observations of each local BS agent
by exploiting its own RNN states information using entropy
based loss function to capture non-i.i.d. energy demand and
generation of each local BS. Therefore, having different
loss functions for local and meta agent leads the proposed
MAMRL model to learn a domain speciï¬c generalized model
so that it can cope with an unknown environment. Further,

this RNN consists of a branch of two fully connected output
layers on top of the LSTM cell. In particular, fully connected
layer with a softmax activation is considered for energy
dispatch policy determination, and another fully connected
output
layer without activation function is deployed for
value function estimation. Thus, the advantage is calculated
based on value function estimation from the second fully
connected layer. Each local LSTM-based RNN receives a
current reward ğ‘Ÿğ‘– (ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–), current action ğ’‚ğ‘¡ğ‘–, and next time
slot ğ‘¡ (cid:48) as an input for each BS agent ğ‘– âˆˆ B. Meanwhile,
this local LSTM model estimates a policy ğœ‹ ğœƒğ‘– and value
ğ‘‰ ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–) for BS agent ğ‘– âˆˆ B. On the other hand, meta
agent LSTM-based RNN feeds input as an observational
ğ’ğ‘– : (ğ‘Ÿğ‘– (ğ’‚ğ‘¡(cid:48)ğ‘–, ğ’”ğ‘¡(cid:48)ğ‘–), ğ‘Ÿğ‘– (ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–), ğ’‚ğ‘¡ğ‘–, ğ’‚ğ‘¡(cid:48)ğ‘–, ğ‘¡ (cid:48), Î› ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡ğ‘–))
tuple
from each BS agent ğ‘– âˆˆ B. This observation consists of the
current and next reward, current and next action, next time
slot, and TD error for each BS agent ğ‘–. Thus,
this meta
agent estimates parameters ğœƒğ‘¡(cid:48)
to ï¬nd a globally optimal
ğœƒğ‘– for each BS ğ‘– âˆˆ B. The learned
energy dispatch policy ğœ‹âˆ—
parameters of the meta-agent are transferred to each local BS
agent ğ‘– âˆˆ B asynchronously while this local agent updates its
own parameters for estimating the globally optimal energy
dispatch policy via the local LSTM-based RNN. In particular,
the learned parameters (i.e., RNN states) are transfered from
meta-agent to each local agent ğ‘– âˆˆ B. Additionally, these
RNN state parameters include cell state and hidden state of
the LSTM cell, which do not depend on any of the fully
the proposed RNN architecture.
connected out
Meanwhile, each local agent ğ‘– âˆˆ B updates its own RNN
states using the transferred parameters by the meta-agent. We
consider a cellular network for exchanging observations and
parameters between local BS agent and meta-agent.

layers of

We run the proposed Algorithm 2 at each self-powered
BS ğ‘– âˆˆ B with MEC capabilities as local agent
ğ‘–. The
input of Algorithm 2 is the state information Sğ‘– of local
agent ğ‘–, which is the output from Algorithm 1. The cumu-
lative discounted reward (17) and state value in (19) are
calculated in lines 5 and 6, respectively (in Algorithm 2)
for each step (until the maximum step size 3 for time step
ğ‘¡). Consequently, based on a chosen action ğ’‚ğ‘¡ğ‘–
from the
estimated policy ğœ‹ ğœƒğ‘– (ğ’‚ğ‘¡ğ‘– |ğ’”ğ‘¡ğ‘–) (in line 7), episode buffer is
generated and appended in line 8. Advantage function (21)
of local agent ğ‘– is evaluated in line 12 and the policy gradient
(22) is calculated in line 13 using an LSTM-based local
neural network. Algorithm 2 generates observational
tuple
ğ’ğ‘– : (ğ‘Ÿğ‘– (ğ’‚ğ‘¡(cid:48)ğ‘–, ğ’”ğ‘¡(cid:48)ğ‘–), ğ‘Ÿğ‘– (ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–), ğ’‚ğ‘¡ğ‘–, ğ’‚ğ‘¡(cid:48)ğ‘–, ğ‘¡ (cid:48), Î› ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡ğ‘–)) in line
15. Here, we transfer the knowledge of local BS agent ğ‘– âˆˆ B
to the meta-agent learner (deployed in MBS) in Algorithm 3
so as to optimize the energy dispatch decision (in Algorithm
2 line 16). Hence, the observation tuple ğ’ğ‘– of local BS agent ğ‘–
consists of only the decision from BS ğ‘–, where does not require
to send all of the state information to meta-agent learner.
Employing the meta-agent policy gradient, each local agent
is capable of updating the energy dispatch decision policy in

3To capture the heterogeneity for energy demand and generation of each
BS separately, we consider the same number of user tasks that are executed
by each BS agent ğ‘– âˆˆ B during one observational period ğ‘¡ as the steps size.

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

13

Fig. 4: Recurrent neural network architecture for the proposed multi-agent meta-reinforcement learning framework.

line 17 in Algorithm 2. Finally, the energy dispatch policy is
executed in line 20 at the BS ğ‘– âˆˆ B by local agent ğ‘–.

The meta-agent

learner (Algorithm 3 in MBS) receives
the observations Oğ‘– âˆˆ O from each local BS agent ğ‘– âˆˆ B
asynchronously. Then the meta-agent asynchronously updates
the meta policy gradient of the each BS agent ğ‘– âˆˆ B. Lines
from 4 to 12 of Algorithm 3 represent the LSTM block for
the meta-agent. In Algorithm 3, entropy loss (25) and gradient
of the loss (26) are estimated in lines 6 and 7, respectively. In
order to estimate this, Algorithm 3 deploys a fully connected
output layer without activation function, so that advantage loss
can be calculated without affecting the value that is calculated
by the value function of the proposed MAMRL framework.
The meta-agent energy dispatch policy is updated in line 10
of Algorithm 3. Before that, a fully connected output layer
with a softmax activation function of the LSTM cell assists
to determine the energy dispatch policy and meta policy loss
in lines 8 and 9 (in Algorithm 3), respectively, for the meta-
agent. Additionally, the meta-agent utilizes the observations
of the local agents and determines its own state information
that helps to estimate the energy dispatch policy of the meta-
agent. In line 11, the meta-agent RNN states ğœƒâˆ— (i.e., cell and
hidden states) are received from the considered LSTM cell in
Algorithm 3. Finally, the meta-agent policy and RNN states
are transfered to each BS agent for updating the parameters
(i.e., RNN states) of each local BS agent. To this end, a
meta-agent learner deployed at center node (i.e., MBS) in the
considered network and sends the learning parameters of the
optimal energy dispatch policy to each local BS (i.e., MBS
and SBS) through the network.

The proposed MAMRL framework established a guarantee

to converge with an optimal energy dispatch policy. In fact,
the MAMRL framework can be reduced to a |B|-player
Markovian game [58], [59] as a base problem that establishes
more insight into convergence and optimality. The proposed
MAMRL model has at least one Nash equilibrium point that
ensures an optimal energy dispatch policy. This argument is
similar from the previous studies of |B|-player Markovian
game [58], [59]. Hence, we can conclude with the following
proposition:
Proposition 1. ğœ‹âˆ—
ğœƒğ‘–
that
ğ‘‰ ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğœ‹âˆ—
ğœƒ0

is an optimal energy dispatch policy
is an equilibrium point with an equilibrium value

) for BS ğ‘– [see Appendix B].

, . . . , ğœ‹âˆ—

ğœƒğµ

We can justify the convergence of MAMRL framework via

the following Proposition:

ğ‘–

Proposition 2. Consider a stochastic environment with a state
space ğ’”ğ‘¡ğ‘– âˆˆ S, ğ‘– âˆˆ âˆ€B of |B| BS agents such that all BS
agents are initialized with an equal probability of 0.5 for a
(ğ‘¡)) = ğœƒğ‘– â‰ˆ 0.5, where
(ğ‘¡)) = ğ‘ƒ(ğœ‰non
binary actions, ğ‘ƒ(ğœ‰sto
(ğ‘¡)) âˆˆ Ağ‘–, âˆ€ğ‘– âˆˆ B, and ğ‘Ÿğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡0, . . . , ğ’‚ğ‘¡ ğµ).
ğ’‚ğ‘¡ğ‘– : (ğœ‰sto
Therefore, to estimate the gradient of loss function (24), we can
establish a relationship among the gradient of approximation
Ë†âˆ‡ğœƒğ‘– ğ¿ (ğœƒğ‘–) and true gradient âˆ‡ğœƒğ‘– ğ¿ (ğœƒğ‘–),
(cid:17)
(cid:16) Ë†âˆ‡ğœƒğ‘– ğ¿(ğœƒğ‘–), âˆ‡ğœƒğ‘– ğ¿ (ğœƒğ‘–) > 0

âˆ (0.5) |B | .

(ğ‘¡), ğœ‰non
ğ‘–

(29)

ğ‘ƒ

ğ‘–

ğ‘–

[See Appendix C].

Propositions 1 and 2 validate the optimality and conver-
gence, respectively for the proposed MAMRL framework.
Proposition 1 guarantees an optimal energy dispatch policy.
Meanwhile, Proposition 2 ensures that the proposed MAMRL

RNN of local BS agent i =BLSTMLSTMLSTMâ€¦Energy environment BS agent i = 0Energy environment BS agent i = BPeriodic meta parameters (i.e., RNN states) transfer to local agent i =0Periodic meta parameters (i.e., RNN states) transfer to local agent i =BLocal policy and observation of BS agent i = BLocal policy and observation of BS agent i = 0Current reward and action of BS agent i = 0Current reward and action of BS agent i = BMeta agent RNN states (cell state and hidden state)LSTMLSTMLSTMLSTMLSTMLSTMMeta agent fully-connected output layer for policy (softmax activation)Meta agent fully-connected output layer for value (without activation)Local fully-connected output layer for policy (softmax activation)Local fully-connected output layer for value (without activation)Local fully-connected output layer for policy (softmax activation)Local fully-connected output layer for value (without activation)LSTMLSTMLSTMMeta agent LSTM cell consists of 48 LSTM unitsLocal agent LSTM cell consists of 48 LSTM unitsLSTMLSTMLSTMCell and hidden states from meta agentCell and hidden states from meta agentInput to local BS agent i = 0RNNInput to local BS agent i = BRNNOutput of local BS agent i =0RNN and input for meta RNN Meta agent RNNOutput of meta RNNOutput of local BS agent i =BRNN and input for meta RNN â€¦â€¦â€¦RNN of local BS agent i =0ActionLocal BS agent i = 0Local BS agent i = BMeta agent at MBSActionACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

14

Algorithm 2 Local Agent Training of Energy Dispatch of BS
ğ‘– âˆˆ B in MAMRL Framework
Input: ğ’”ğ‘¡ğ‘– : (ğœ‰d
ğ‘¡ğ‘– , ğ¶non
ğ‘¡ğ‘–
Output: ğ’ğ‘– : (ğ‘Ÿğ‘– (ğ’‚ğ‘¡(cid:48)ğ‘–, ğ’”ğ‘¡(cid:48)ğ‘–), ğ‘Ÿğ‘– (ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–), ğ’‚ğ‘¡ğ‘–, ğ’‚ğ‘¡(cid:48)ğ‘–, ğ‘¡ (cid:48), Î› ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡ğ‘–)),

), âˆ€ğ’”ğ‘¡ğ‘– âˆˆ Sğ‘–, âˆ€ğ‘¡ âˆˆ ğ‘‡

(ğ‘¡), ğ¶sto

ğ‘– , ğœ‰ren
ğ‘–

ğ’ğ‘– âˆˆ Oğ‘–, âˆ‡ğœƒğ‘¡ ğ¿ (ğœƒğ‘¡ )
Initialization: ğ¿ğ‘œğ‘ğ‘ğ‘™ ğ¿ğ‘†ğ‘‡ ğ‘€ (.), ğœƒğ‘–, ğ‘– âˆˆ B, ğ›¾, Oğ‘–

Initialization: ğ‘’ ğ‘ğ‘ğµğ‘¢ ğ‘“ []
for each ğ‘¡ âˆˆ T do

1: for episode = 1 to maximum episodes do
2:
3:
4:
5:

for ğ‘ ğ‘¡ğ‘’ ğ‘ = 1 to ğ‘€ğ‘ğ‘¥ğ‘†ğ‘¡ğ‘’ ğ‘ do

4:
5:

6:

7:

8:
9:
10:

11:

Calculate: ğ‘Ÿğ‘– (ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–) using eq. (17)
Calculate: ğ‘‰ ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–) using eq. (19)
Choose Action: ğ’‚ğ‘¡ğ‘– âˆ¼ ğœ‹ ğœƒğ‘– (ğ’‚ğ‘¡ğ‘– |ğ’”ğ‘¡ğ‘–)
Append: ğ‘’ ğ‘ğ‘ğµğ‘¢ ğ‘“ [ğ’‚ğ‘¡ğ‘–, ğ‘Ÿğ‘– (ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–), ğ‘¡, ğ‘ ğ‘¡ğ‘’ ğ‘, ğ‘‰ ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–)]

end for
ğ¿ğ‘œğ‘ğ‘ğ‘™ ğ¿ğ‘†ğ‘‡ ğ‘€ (ğ‘Ÿğ‘– (ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–), ğ’‚ğ‘¡ğ‘–, ğ‘¡ (cid:48) = ğ‘¡ + 1)
{LSTM-based local RNN block}
{
Evaluate: Î› ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡ğ‘–) using eq. (21)
Local agent policy gradient: âˆ‡ğœƒğ‘– Î› ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡ğ‘–) using
eq. (22)
{In (22), ğœ‹ ğœƒğ‘– (ğ’‚ğ‘¡ğ‘– |ğ’”ğ‘¡ğ‘–; ğœƒğ‘–) is determined by a fully
layer with a softmax activation
connected output
function and Î› ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡ğ‘–) is calculated through a
fully connected output layer without activation func-
tion}
}
Append:
ğ’ğ‘– : (ğ‘Ÿğ‘– (ğ’‚ğ‘¡(cid:48)ğ‘–, ğ’”ğ‘¡(cid:48)ğ‘–), ğ‘Ÿğ‘– (ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–), ğ’‚ğ‘¡ğ‘–, ğ’‚ğ‘¡(cid:48)ğ‘–, ğ‘¡ (cid:48), Î› ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡ğ‘–)),
ğ’ğ‘– âˆˆ Oğ‘–
Get Meta-agent policy ğœ‹âˆ—
ğ‘€ğ‘¡ (Oğ‘¡ ; ğœ™) using Algorithm 3
Update: ğœƒğ‘¡(cid:48) = ğœƒâˆ— {RNN states update}

ğœƒğ‘– and RNN states ğœƒâˆ—:

6:
7:
8:
9:
10:

11:
12:
13:

14:

15:

16:

17:

end for

18:
19: end for
20: return new_state(ğ’”ğ‘¡(cid:48)ğ‘– = argmax ğœ‹âˆ—
ğœƒğ‘–

(ğ’‚ğ‘¡ğ‘–)), ğ‘– âˆˆ B

model can meet the convergence for a single state ğ’”ğ‘¡ğ‘– âˆˆ S, ğ‘– âˆˆ
âˆ€B. That implies this model is also able to converge for
âˆ€ğ’”ğ‘¡ğ‘– âˆˆ S, ğ‘– âˆˆ âˆ€B.

The signiï¬cance of the proposed MAMRL model are ex-

plained as follows:

â€¢ First, each BS (i.e., local agent) can explore its own
energy dispatch policy based on individual requirements
for the energy generation and consumption. Meanwhile,
the meta-agent exploits each BS energy dispatch decision
from its own recurrent neural networks state information.
As a result, individual BS anticipates its own energy
demand and generation while meta-agent handles the non-
i.i.d. energy demand and generation for all BS agents to
efï¬ciently meet the exploration-exploitation tradeoff of
the proposed MAMRL.

â€¢ Second, the proposed MAMRL model can effectively
handle distinct environment dynamics for non-i.i.d. en-
ergy demand and generation among the agents.

â€¢ Third, the proposed MAMRL model ensures less infor-

Algorithm 3 Meta-Agent Learner of Energy Dispatch in
MAMRL Framework
Input: ğ’ğ‘– : (ğ‘Ÿğ‘– (ğ’‚ğ‘¡(cid:48)ğ‘–, ğ’”ğ‘¡(cid:48)ğ‘–), ğ‘Ÿğ‘– (ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–), ğ’‚ğ‘¡ğ‘–, ğ’‚ğ‘¡(cid:48)ğ‘–, ğ‘¡ (cid:48), Î› ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡ğ‘–)),

Initialization: ğ‘€ğ‘’ğ‘¡ğ‘ğ¿ğ‘†ğ‘‡ ğ‘€ (.), ğœ™, ğœ‹ ğœƒğ‘– , ğ›¾

âˆ€ğ’ğ‘– âˆˆ Oğ‘–, ğ‘¡ âˆˆ T , ğ‘– âˆˆ B

Output: ğœ™

1: for each ğ‘¡ âˆˆ T do
2:
3:

for each ğ‘– âˆˆ B do

ğ’ğ‘– : (ğ‘Ÿğ‘– (ğ’‚ğ‘¡(cid:48)ğ‘–, ğ’”ğ‘¡(cid:48)ğ‘–), ğ‘Ÿğ‘– (ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–), ğ’‚ğ‘¡ğ‘–, ğ’‚ğ‘¡(cid:48)ğ‘–, ğ‘¡ (cid:48), Î› ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡ğ‘–)),
ğ’ğ‘– âˆˆ Oğ‘–
ğ‘€ğ‘’ğ‘¡ğ‘ğ¿ğ‘†ğ‘‡ ğ‘€ (Oğ‘–, ğœ‹ ğœƒğ‘– ) {LSTM-based RNN block}
{
{Lines from 6 to 7 using fully connected output layer
without activation function}
Entropy loss: ğ¿(ğœƒğ‘–) using eq. (25)
Gradient of the loss: âˆ‡ğœƒğ‘¡ ğ¿ (ğœƒğ‘¡ ) using eq. (26)
{Policy is estimated using a fully connected output
layer with softmax activation function}
Calculate: ğœ‹ ğœƒğ‘– = ğ‘€ğ‘¡ (âˆ‡ğœƒğ‘¡ ğ¿(ğœƒğ‘¡ ); ğœ™) using eq. (27)
Get meta policy loss ğ¿ (ğœ™) using eq. (28)
Update: ğœ‹âˆ—
= ğœ‹ ğœƒğ‘–
ğœƒğ‘–
Get RNN states: ğœƒâˆ—
{cell state and hidden state from the LSTM cell}
}
end for
Send: Meta-agent policy ğœ‹âˆ—

ğœƒğ‘– and RNN states ğœƒâˆ—

12:
13:
14:
15: end for
16: return

mation exchange between the local agents and meta-
agent. In particular, each local BS agent only sends an
observational vector to meta-agent and received neural
network parameters at the end of 15 minutes observation
period. Additionally, the proposed MAMRL model does
not require sending an entire environment state from each
local agent to the meta-agent.

â€¢ Finally, the meta-agent can learn a generalized model
toward the energy dispatch decision and transfer its skill
to each local BS agent. This, in turn, can signiï¬cantly
increase the learning accuracy as well as reduce the com-
putational time for each local BS agent thus enhancing
the robustness of the energy dispatch decision.

We benchmark the proposed MAMRL framework by per-
forming an extensive experimental analysis, and the experi-
mental analysis and discussion are given in the later section.

V. EXPERIMENTAL RESULTS AND ANALYSIS

In our experiment, we use the CRAWDAD nyupoly/video
packet delivery dataset [60] to discretize the self-powered SBS
networkâ€™s energy consumption. Further, we choose a state-of-
the-art UMass solar panel dataset [61] to evaluate renewable
energy generation. We create deterministic, asymmetric, and
stochastic environments by selecting different days of the
same solar unit for the generation. Meanwhile, usage several
session from the network packet delivery dataset. We train
our proposed meta-reinforcement learning (Meta-RL)-based

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

15

TABLE IV: Summary of experimental setup

Description
No. of SBSs (no. of local agents)
No. of MEC servers in each SBS
No. of MBS (meta-agent)
Channel bandwidth
System bandwidth
Transmission power
Channel gain
A variance of an AWGN
Energy coefï¬cient for data transfer ğ›¿net
MEC server CPU frequency ğ‘“
Server switching capacitance ğœ
(ğ‘¡)
MEC static energy ğœ‚MEC

ğ‘–

st

ğ‘¡

Task sizes (uniformly distributed)
No. of task requests at BS ğ‘–
Unit cost renewal energy ğ‘ren
ğ‘¡
Unit cost non-renewal energy ğ‘non
ğ‘¡
Unit cost storage energy ğ‘sto
Initial discount factor ğ›¾
Initial action selection probability
One observation period ğ‘¡
No. of episodes
No. of epochs ğ‘‡ for each day
No. of steps for each epoch at each agent ğ‘–
No. of actions
No. of LSTM units in one LSTM cell
No. of LSTM cells
LSTM cell API BasicLSTMCell(.)
Entropy regularization coefï¬cient ğ›½
Learning rate
Optimizer
Output layer activation function

Value
9
2
1
180 kHz [62]
20 MHz [17]
27 dB [16]
140.7 + 36.7 log ğ‘‘ [17]
-114 dBm/Hz [62]
2.8 [36]
2.5 GHz [16]
5 Ã— 10âˆ’27 (farad) [17]
[7.5, 25] Watts [63]
[31, 1546060] bytes [60]
[1, 10, 000] [11]
$50 per MW-hour [45]
$102 per MW-hour [45]
10% additional [44]
0.9
[0.5, 0.5]
15 minutes
800
96
ğ½ğ‘– = [1, 10, 000] [60]
(ğ‘¡))

2 (i.e., ğœ‰ sto

ğ‘–

ğ‘–

(ğ‘¡), ğœ‰ non
48
10 (i.e., B+1)
tf.contrib.rnn [64]
0.05
0.001
Adam [65]
Softmax [51]

(a) MEC network energy demand

(b) Renewable energy generation

Fig. 5: Histogram of energy demand and renewable energy
generation for 9 SBSs and each SBS consists of 96 time slots
after preprocessing using Algorithm 1.

MAMRL framework using deterministic environment and
evaluate the testing performance for the three environments.
Three environments4 are as follows: 1) In the deterministic
environment, both network energy consumption and renewable
generation are known, 2) network energy consumption is
known but renewable generation is unknown in the asymmetric
environment, and 3) the stochastic environment contains both
energy consumption and renewable generation are unknown.
To benchmark the proposed MAMRL framework intuitively,

4For example, we train and test the MAMRL model using the known
(i.e., deterministic environment) network energy consumption, and renewable
generation data of day 1. Then we have tested the trained model using day 2
data, where network energy consumption is known, and renewable generation
is unknown which represents an asymmetric environment. In a stochastic
environment, let us consider day 3 data, where both energy consumption and
renewable generation are unknown to the trained model.

Fig. 6: Reward value achieved for proposed Meta-RL training
of the meta-agent alone with other SBS agents.

we have considered a centralized single-agent deep-RL, multi-
agent centralized A3C deep-RL with a same neural networks
conï¬guration as the proposed MAMRL, and a pure greedy
model as baselines. These are as follows:

â€¢ We consider the neural advantage actor-critic (A2C) [51],
[66] method as a centralized single-agent deep-RL. In
particular, the learning environment encompasses the state
information of all BSs âˆ€ğ‘– âˆˆ B and is learned by a neural
A2C [51], [66] scheme with the same conï¬guration as
the MAMRL model.

â€¢ An asynchronous advantage actor-critic (A3C) based
multi-agent RL framework [28] is considered a second
benchmark in a cooperative environment [27]. In partic-
ular, each local actor can ï¬nd its own policy in a decen-
tralized manner while a centralized critic is augmented
with additional policy information. Therefore, this model
is learned by a centralized training with decentralized
execution [28]. We call this model a multi-agent cen-
tralized A3C deep-RL [28]. The environment (i.e., state
information) of this model remains the same for all of the
local actor agents. To ensure a meaningful comparison
with the proposed MAMRL model, we employ this joint
energy dispatch policy using the same advantage function
(23) as the MAMRL model.

â€¢ We deploy a pure greedy-based algorithm [51] to ï¬nd the
best action-value mapping. In particular, this algorithm
never takes the risk to choose an unknown action. Mean-
while, it explores other strategies and learns from them so
as to infer more reasonable decisions. Thus, we choose
this upper conï¬dence bounded action selection mecha-
nism [51] as one of the baselines used for benchmarking
our proposed MAMRL model.

We implement our MAMRL framework using multi-
threading programming in Python platform 5, along with
TensorFlow APIs [68]. Table IV shows the key parameters
of this experiment setup.

We prepossess both of the datasets ( [60] and [61]) using
Algorithm 1 that generates the state space information. The
histograms of the network energy demand (in 5(a)) and a
renewable energy generation (in 5(b)) of the deterministic

5MAMRL

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

16

Fig. 7: Reward value achieved of proposed Meta-RL, single-
agent centralized, and multi-agent centralized methods.

environment are shown in Fig. 5. To the best of our knowledge,
there are no publicly available datasets that comprises both
of energy consumption and generation of a self-powered
network with MEC capabilities. Additionally, if we change
the environment using other datasets, the proposed MAMRL
framework can deal with the new, unknown environment by
using the skill transfer feature from the meta-agent to each
the MAMRL approach can
local BS agent. In particular,
readily deal with the case in which the BS agent achieves
a much lower reward due to more variability in consumption
and generation. As a result, the above experiment setup is
reasonable for the benchmarking of the proposed MAMRL
framework.

Fig. 6 illustrates the reward achieved by each local SBS
along with a meta-agent, where we take an average reward
for each 50 episodes. In the MAMRL setting, we design a
maximum reward of 96 (15 minute slot for 24 hours), where
meta-agent converges with a high reward value (around 90).
Hence, all of the local agents converge with around 80 âˆ’ 85
reward value except the SBS 6 that achieves a reward of 70 at
convergence because its energy consumption and generation
vary more than the others. In fact, this variation of reward
among the BSs is leading to anticipate the non-i.i.d. energy
demand and generation of the considered network as well as
densiï¬cation of the exploration and exploitation tradeoff for
energy dispatch. The proposed approach can adapt the uncer-
tain energy demand and generation over time by characterizing
the expected amount of uncertainty in an energy dispatch
decision for each BS ğ‘– âˆˆ B individually. Meanwhile, the meta-
agent exploits the energy dispatch decision by employing a
joint policy toward the globally optimal energy dispatch for
each BS ğ‘– âˆˆ B. Therefore, the challenges of distinct energy
demand and generation of the state space among the BSs can
be efï¬ciently handled by applying learned parameters from
the meta-agent to each BS ğ‘– âˆˆ B during the training that
establishes a balance between exploration and exploitation.

We compare the achieved reward of proposed MAMRL
model with single-agent centralized and multi-agent central-
ized models in Fig. 7. The single agent centralized (diamond
mark with red line) model converges faster than the other two
models but it achieves the lowest reward due to the lack of
exploitation as it has only one agent. Further, the multi-agent

(a) Proposed Meta-RL

(b) Single-agent centralized

(c) Multi-agent centralized

Fig. 8: Relationship among the entropy loss, value loss, and
policy loss in the training phase of proposed Meta-RL, single-
agent centralized, and multi-agent centralized methods.

centralized (circle mark with blue line) model converges with
a higher reward than the single agent method. The proposed
MAMRL (cross mark with green line) model outperforms the
other two models while converges with the highest reward
value. In addition, multi-agent centralized needs the entire
state information. In contrast, the meta-agent requires only the
observation from local agents, and it can optimize the neural
network parameters by using its own state information.

We analyze the relationship among the value loss, entropy
loss, and policy loss in Fig. 8, where the maximum policy
loss of the proposed MAMRL (in 8(a)) model
is around
0.06 whereas single-agent centralized (in 8(b)) and multi-
agent centralized (in 8(c)) methods gain about 1.88 and 0.12,
respectively. Therefore, the training accuracy increases due to
more variation between exploration and exploitation. Thus,
our MAMRL model is capable of incorporating the decision
of each local BS agent that solves the challenge of non-i.i.d.

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

17

Fig. 9: Testing accuracy of the proposed Meta-RL, single-
agent centralized, and multi-agent centralized methods with
deterministic, asymmetric, and stochastic environments of the
9 SBSs.

demand-generation for the other BSs.

ğ‘–

In Fig. 9, we examine the testing accuracy [69] of the stor-
age energy ğœ‰sto
(ğ‘¡) and the non-renewable energy generation
ğ‘–
(ğ‘¡) for 96 time slots (1 days) of 9 SBSs under
decision 6 ğœ‰non
the deterministic, asymmetric, and stochastic environments. In
the experiment, we have used the well-known UMass solar
panel dataset [61] for renewable energy generation information
as well as, the CRAWDAD nyupoly/video dataset [60], for es-
timating the energy consumption of the self-powered network.
Further, we preprocess both of the datasets ( [60] and [61])
using Algorithm 1 that generates the state space information.
Thus, the ground truth comes from this state-space information
of the considered datasets, where the actions are depended
on the renewable energy generation and consumption of a
particular BS ğ‘– âˆˆ B. The proposed MAMRL (green box)
and multi-agent centralized (blue box) methods achieve a
maximum accuracy of around 95% and 92%, respectively,
under the stochastic environment (in Fig. 9). Further, Fig. 9
shows that the mean accuracy (88%) of the proposed method
is also higher than the centralized solution (86%). Similarly,
in the deterministic and asymmetric environment, the average
accuracy (around 87%) of the proposed low complexity semi-
distributed solution is almost the same as the baseline method.
The prediction results of renewable, storage and non-
renewable energy usage for a single SBS (SBS 2) for 24
hours (96 time slots) under the stochastic environment are
shown in Fig. 10. The proposed MAMRL outperforms all
other baselines and achieves an accuracy of around 95.8%.
In contrast, the accuracy of the other two methods is 75%
and 93.7% for the single-agent centralized and multi-agent
centralized, respectively.

In Figs. 11 and 12, we validate our approach with two stan-
dard regression model evaluation metric, explained variance

6Each BS agent ğ‘– âˆˆ B can calculate its action from a globally optimal
energy dispatch policy ğœ‹âˆ—
(ğ’‚ğ‘¡ğ‘–)). In
which, at the end of 15 minutes duration of each time slot ğ‘¡, the each BS
agent ğ‘– âˆˆ B can choose one action (i.e., storage or non-renewable) from the
energy dispatch policy ğœ‹âˆ—

ğœƒğ‘– by using argmax(.) (i.e., argmax ğœ‹âˆ—
ğœƒğ‘–

ğœƒğ‘– .

Fig. 10: Prediction result of renewable, storage, and non-
renewable energy usages of a single SBS (SBS 2) for 24 hours
(96 time slots) under the stochastic environment.

Fig. 11: Explained variance score of a single SBS (SBS 2) for
24 hours (96 time slots) under the stochastic environment.

Fig. 12: Mean absolute error of a single SBS (SBS 2) for 24
hours (96 time slots) under the stochastic environment.

7 (i.e., explained variation) and mean absolute error (MAE)
[69], respectively. Fig. 11 shows that the explained variance
score of the proposed MAMRL method almost the same as
the multi-agent centralized. However, in the case of renewable
energy generation, MAMRL method signiï¬cantly performs
better (i.e., 1% more score) than the multi-agent centralized
solution. In particular,
the proposed MAMRL model has
pursued the uncertainty of renewable energy generation by
the dynamics of Markovian for each BS. Further, meta-agent
anticipates the energy dispatch by other BSs decisions and its

7We measure the discrepancy for energy dispatch decisions between the
proposed and baseline models on the ground truth of the datasets ( [60]
and [61]). We deploy the explained variance regression score function using
sklearn API [70] to measure and compare this discrepancy.

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

18

model in terms of the non-renewable energy usages into a
stochastic environment with other benchmarks. This ï¬gure
considers a kernel density analysis for 24 hours (96 time
slots) under a stochastic environment, where the median of
the non-renewable energy usages 0.15 (kWh), and 0.27 (kWh)
for the proposed MAMRL, and pure greedy, respectively, at
each 15 minutes time slot. Further, the proposed MAMRL
can signiï¬cantly reduce the usages of non-renewable energy
for the considered self-powered wireless network, where the
MAMRL can save up to 13.3% of the non-renewable energy
the meta agent of the MAMRL model can
usages. Here,
discretize uncertainty from each local BS agent and transfer
the knowledge (i.e., learning parameters) to each local agent
that can take a globally optimal energy dispatch decision.

Fig. 13: Kernel density analysis of non-renewable energy
usages for 24 hours (96 time slots) under the stochastic
environment.

Fig. 14: Energy consumption cost analysis of 9 SBSs for 24
hours (96 time slots) under deterministic, asymmetric, and
stochastic environments using the proposed Meta-RL method
over pure greedy method.

own state information. We analyze the MAE 8 for the three
environments (i.e., deterministic, asymmetric, and stochastic)
among the proposed MAMRL, single-agent centralized, and
multi-agent centralized methods in Fig. 12. The MAE of the
proposed MAMRL is 11%, 15%, and 4% for deterministic,
asymmetric, and stochastic, respectively since meta-agent has
the capability to adopt the uncertain environment very fast.
This adaptability is enhanced by the exploration mechanism
that is taken into account at each BS, and exploitation that
performs by capitalizing the non-i.i.d. explored information
of all BSs.

Fig. 13 illustrates the efï¬cacy of the proposed MAMRL

8This performance metric provides us with the average magnitude of errors
for the energy dispatch decision of a single SBS (SBS 2) for 24 hours (96 time
slots). Particularly, we analyze the average error over the 96 time slots of the
absolute differences between prediction and actual observation. To evaluate
this metric, we have used the mean absolute error regression loss function of
sklearn API [71].

Fig. 15: Amount of renewable, non-renewable, and storage
energy estimation for 24 hours (96 time slots) for proposed
meta-RL, single-agent RL, multi-agent RL, next ï¬t, ï¬rst ï¬t,
and ï¬rst ï¬t decreasing methods.

Fig. 14 presents the energy consumption cost analysis for 9
SBSs over 24 hours (96 time slots) under deterministic, asym-
metric, and stochastic environments using the proposed Meta-
RL method while comparing it to the pure greedy method.
The total energy cost achieved by the proposed approach for a
particular day will be $33.75, $28.29, and $25.83 for determin-
istic, asymmetric, and stochastic environments, respectively.
Fig. 14 also shows that the proposed method signiï¬cantly
reduces the energy consumption cost (by at least 22.4%) for all
three environments over the pure greedy method. The median
of the energy cost at each time slot is $0.04, $0.03, and $0.03
for the deterministic, asymmetric, and stochastic environments,
respectively. In contrast, Fig. 14 has shown that a median
energy cost for the pure greedy baseline is $0.05 at each time
slot that is due to a lack of the competence to cope with an
unknown environment for energy consumption and generation.
Therefore, the proposed MAMRL model can overcome the
challenges of an unknown environment as well as non-i.i.d.
characteristics for energy consumption and generation of a
self-powered MEC network.

In Fig. 15, we compare our proposed meta-RL model with
single-agent RL, multi-agent RL, next ï¬t, ï¬rst ï¬t, and ï¬rst
ï¬t decreasing methods in terms of amount of renewable,
non-renewable, and storage energy usages for 24 hours (96
time slots). Fig. 15 shows that the proposed MAMRL model

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

19

TABLE V: Comparison between the proposed method and other methods with ground truth for a single SBS (SBS 2) for 24
hours (96 time slots) under the stochastic environment.

Method

Non-
renewable
energy
usage
(kWh)

Storage
energy
usage
(kWh)

Renewable
energy
usage
(kWh)

Non-
renewable
energy
usage
cost ($)

Storage
energy
usage
cost ($)

Renewable
energy
usage
($)

cost

Total en-
ergy us-
age cost
($)

Ground truth (i.e., optimal)
MAMRL (proposed)
Single-agent RL
Multi-agent RL
Next Fit
First Fit
First Fit Decreasing
Without renewable

30.15
30.88
34.53
31.24
38.92
37.37
37.12
47.69

8.87
8.50
6.65
8.31
4.44
5.22
5.34
NA

8.67
8.32
6.50
8.14
4.34
5.10
5.23
NA

3.07
3.14
3.52
3.19
3.97
3.81
3.79
4.86

0.49
0.47
0.37
0.46
0.24
0.29
0.30
NA

0.43
0.42
0.33
0.41
0.21
0.26
0.26
NA

3.99
4.03
4.21
4.05
4.43
4.35
4.34
4.86

Cost dif-
ference
with
ground
truth
(%)
NA
0.90
5.43
1.36
10.86
8.94
8.63
21.72

demand and generation from the historical data while meta-
agent optimizes energy dispatch decisions by obtaining those
features with its own parameters of LSTM. In the case of
testing, a generalized MAMRL trained model is employed that
makes a fully independent and unbiased energy dispatch from
an unknown environment. To this end, the proposed MAMRL
framework shows the efï¬cacy of solving the energy dispatch of
a self-powered wireless network with MEC capabilities with
a higher degree of reliability.

Fig. 16: Competitive cost ratio of the proposed Meta-RL
method for 24 hours (96 time slots) under the deterministic,
asymmetric, and stochastic environments.

outperforms the others that achieves around 22% less non-
renewable energy usages than the next ï¬t scheduling algo-
rithm. Additionally, next ï¬t, ï¬rst ï¬t, and ï¬rst ï¬t decreasing
scheduling methods [72] cannot capture the uncertainty of
energy generation and consumption, as well as provide a near
optimal solution. Further, a comparison between the proposed
method and other methods with the ground truth for a single
SBS (SBS 2) for 24 hours (96 time slots) under the stochastic
environment is illustrated in Table V. The proposed method
can achieve signiï¬cant outcomes with respect to energy cost as
compared with the ground truth. In particular, the experiment
shows that
the energy usage cost difference between the
proposed method and ground truth is around 1% for a single
BS (in Table V). This leads to one of the evidence that the
proposed MAMRL can adopt the unknown environment and
can utilize it during the execution for each BS energy dispatch.
Finally, in Fig. 16, we examine the competitive cost ratio
[29] of the proposed MAMRL framework. From this ï¬gure,
we observe that the proposed MAMRL framework effectively
minimizes the energy consumption cost for each BS under
deterministic, asymmetric, and stochastic environments. In
fact, Fig. 16 ensures the robustness of the proposed MAMRL
framework that is performed a tremendous performance gain
by coping with non-i.i.d. energy consumption and generation
under the uncertainty. Furthermore, in MAMRL training, each
local agent has captured the time-variant features of energy

VI. CONCLUSIONS

In this paper, we have investigated an energy dispatch prob-
lem of a self-powered wireless network with MEC capabilities.
We have formulated a two-stage stochastic linear programming
energy dispatch problem for the considered network. To solve
the energy dispatch problem in a semi-distributed manner, we
have proposed a novel multi-agent meta-reinforcement learn-
ing framework. In particular, each local BS agent obtains the
time-varying features by capturing the Markovian properties of
the networkâ€™s energy consumption and renewable generation
for each BS unit, and predict its own energy dispatch policy.
Meanwhile, a meta-agent optimizes each BS agentâ€™s energy
dispatch policy from its own state information, and it transfers
global learning parameters to each BS agent so that they can
update their energy dispatch policy into an optimal policy. We
have shown that the proposed MAMRL framework can capture
the uncertainty of non-i.i.d. energy demand and generation for
the self-powered wireless network with MEC capabilities. Our
experimental results have shown that the proposed MAMRL
framework can save a signiï¬cant amount of non-renewable
energy with higher accuracy prediction that ensures the energy
sustainability of the network. In particular, the performance of
energy dispatch over deterministic, asymmetric, and stochastic
environments outperform other baseline approaches, where
average accuracy achieves up to 95.8% and reduces the energy
cost about 22.4% of the self-powered wireless network. To
this end, the proposed MAMRL model can reduce by at least
11% of the non-renewable energy usage for the self-powered
wireless network.

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

20

APPENDIX A
EXAMPLE OF INFORMATION EXCHANGE BETWEEN LOCAL
BS AGENT AND META AGENT IN MAMRL FRAMEWORK

ğ‘– , ğœ‰ren
ğ‘–

(ğ‘¡), ğ¶sto

ğ‘¡ğ‘– , ğ¶non
ğ‘¡ğ‘–

For example, consider an LSTM cell with 48 LSTM units
[49], [64]. Thus, the dimension of forget gate, input gate,
gate/memory/activation gate, and output gate will be 48 for
ğ‘– âˆˆ B that
each gate. Now consider a local BS agent
embedding a dimension of 3 inputs (ğ‘Ÿğ‘– (ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–), ğ’‚ğ‘¡ğ‘–, ğ‘¡ (cid:48)) to a
local LSTM cell. This input comes from the state information
) of a local BS agent ğ‘–. As a result,
ğ’”ğ‘¡ğ‘– : (ğœ‰d
inputs are appended to all gates during the training. Therefore,
the number of learning parameters will be 4Ã— (48(48+3) +48)
(i.e., ğ‘”ğ‘ğ‘¡ğ‘’ğ‘  Ã— [ğ‘¢ğ‘›ğ‘–ğ‘¡ğ‘ (ğ‘¢ğ‘›ğ‘–ğ‘¡ğ‘  + ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡) + ğ‘¢ğ‘›ğ‘–ğ‘¡ğ‘ ]). Additionally, the
size of hidden state and cell state parameters remain 48 for
each due to an LSTM cell with 48 LSTM units. Further, on top
of the LSTM cell, we have two fully connected output layers, a
fully connected output layer with a Softmax activation to deter-
mine the local energy dispatch policy. Meanwhile, advantage is
determined from another fully connected output layer without
activation function by value function estimation. The hidden
and cell state of each local agent are updated by receiving the
state parameters with a 48Ã—2 dimensional data from the meta-
agent. In case of the meta-agent, the conï¬guration of LSTM
cell is the same as each local LSTM cell. Therefore, at the
end of each time slot duration, the meta-agent sends a 48 Ã— 2
dimensional state information to each local BS agent ğ‘– âˆˆ B.
Subsequently, the meta-agent receives a 6 dimensional obser-
vation ğ’ğ‘– : (ğ‘Ÿğ‘– (ğ’‚ğ‘¡(cid:48)ğ‘–, ğ’”ğ‘¡(cid:48)ğ‘–), ğ‘Ÿğ‘– (ğ’‚ğ‘¡ğ‘–, ğ’”ğ‘¡ğ‘–), ğ’‚ğ‘¡ğ‘–, ğ’‚ğ‘¡(cid:48)ğ‘–, ğ‘¡ (cid:48), Î› ğœ‹ğœƒğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡ğ‘–))
as an input from each local BS agent, where the number of
learning parameters at the meta-agent will be 4 Ã— (48(48 + 6) +
48) for each iteration. The output layer of the meta-agent also
consists of two fully connected output layers for determining
meta-policy (i.e., joint policy) and meta advantage. Thus, these
output layers do not affect the dimension of hidden and cell
states for the meta agentâ€™s LSTM cell. In fact, these RNN
states are used as an input to these fully connected layers. As
a result, for each epoch (i.e., end of a time slot duration), the
meta-agent sends 48 Ã— 2 dimensional RNN states to each local
agent along with an energy dispatch policy, and each local
agent sends 6 dimensional observation to the meta-agent.

APPENDIX B
PROOF OF PROPOSITION 1

Proof. For a BS agent ğ‘–, energy dispatch policy ğœ‹âˆ—
is the
ğœƒğ‘–
best response for the equilibrium responses from all other BS
agents. Thus, the BS agent ğ‘– can not be improved the value
ğ‘‰ ğœ‹âˆ—
ğœƒğ‘– . Therefore, (24)
holds the following property,

ğœƒğ‘– (ğ’”ğ‘¡ğ‘–) any more by deviating of policy ğœ‹âˆ—

ğ‘‰ ğœ‹âˆ—
âˆ
âˆ‘ï¸

ğ’”ğ‘¡(cid:48)ğ‘– âˆˆSğ‘– ,ğ‘¡(cid:48)=ğ‘¡

ğœƒğ‘– (ğ’”ğ‘¡ğ‘–) â‰¥ ğ‘Ÿğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡0, . . . , ğ’‚ğ‘¡ ğµ) +

ğ›¾ğ‘¡(cid:48)âˆ’ğ‘¡ Î“(ğ’”ğ‘¡(cid:48)ğ‘– |ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡0, . . . , ğ’‚ğ‘¡ ğµ)ğ‘‰ ğœ‹ğœƒğ‘– (ğ’”ğ‘¡(cid:48)ğ‘–, ğœ‹âˆ—
ğœƒ0

, . . . , ğœ‹âˆ—

ğœƒğµ ).

(30)

the meta-agent ğ‘€ğ‘¡ (Oğ‘¡ ; ğœ™) of the |B|-agent energy
Hence,
dispatch model (i.e., MAMRL) reaches a Nash equilibrium

point for policy ğœ‹âˆ—
value of BS agent ğ‘– âˆˆ B can be as follows:

ğœƒğ‘– with parameters ğœƒğ‘–. As a result, the optimal

ğ‘‰ ğœ‹âˆ—

ğœƒğ‘– (ğ’”ğ‘¡ğ‘–) = ğ‘€ğ‘¡ (âˆ‡ğœƒğ‘¡ ğ¿ (ğœƒğ‘¡ ); ğœ™).

(31)

(31) implies that ğœ‹âˆ—
decisions. Thus, the optimal policy ğœ‹âˆ—
equilibrium point and holds the following inequality,

ğœƒğ‘– is an optimal policy of energy dispatch
ğœƒğ‘– belongs to a Nash

ğ‘‰ ğœ‹âˆ—

ğœƒğ‘– (ğ’”ğ‘¡ğ‘–) â‰¥ Eğ¿ ( ğœƒ) [ğ¿ (ğœƒâˆ—(ğ¿(ğœƒ); ğœ™))]

(32)

(cid:4)

APPENDIX C
PROOF OF PROPOSITION 2

Proof. A probability of action ğ’‚ğ‘¡ğ‘– of BS agent ğ‘– âˆˆ B at time
ğ‘¡ can be presented as follows:

ğ‘ƒ(ğ’‚ğ‘¡ğ‘–) = ğœƒğ’‚ğ‘¡ğ‘–

ğ‘–

(1 âˆ’ ğœƒğ‘–)1âˆ’ğ’‚ğ‘¡ğ‘–

= ğ’‚ğ‘¡ğ‘– log ğœƒğ‘– + (1 âˆ’ ğ’‚ğ‘¡ğ‘–) log(1 âˆ’ ğœƒğ‘–).

(33)

We consider a single state, and a policy gradient estimator can
be deï¬ned as,

Ë†ğœ•
ğœ•ğœƒğ‘–

ğ¿ (ğœƒğ‘–) = ğ‘Ÿğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡0, . . . , ğ’‚ğ‘¡ ğµ)

log ğ‘ƒ(ğ’‚ğ‘¡0, . . . , ğ’‚ğ‘¡ ğµ)

ğœ•
ğœ•ğœƒğ‘–

ğ’‚ğ‘¡ğ‘– log ğœƒğ‘– + (1 âˆ’ ğ’‚ğ‘¡ğ‘–) log(1 âˆ’ ğœƒğ‘–)

(ğ’‚ğ‘¡ğ‘– log ğœƒğ‘– + (1 âˆ’ ğ’‚ğ‘¡ğ‘–) log(1 âˆ’ ğœƒğ‘–))

âˆ‘ï¸

âˆ€ğ‘– âˆˆB

= ğ‘Ÿğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡0, . . . , ğ’‚ğ‘¡ ğµ)

= ğ‘Ÿğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡0, . . . , ğ’‚ğ‘¡ ğµ)

ğœ•
ğœ•ğœƒğ‘–
ğœ•
ğœ•ğœƒğ‘–
ğ’‚ğ‘¡ğ‘–
ğœƒğ‘–
= ğ‘Ÿğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡0, . . . , ğ’‚ğ‘¡ ğµ)(2ğ’‚ğ‘¡ğ‘– âˆ’ 1), for ğœƒğ‘– = 0.5.

= ğ‘Ÿğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡0, . . . , ğ’‚ğ‘¡ ğµ)(

(1 âˆ’ ğ’‚ğ‘¡ğ‘–)
(1 âˆ’ ğœƒğ‘–)

âˆ’

)

(34)

Thus, an expected reward for |B| BS agents can be represented
as, E[ğ‘Ÿğ‘–] = (cid:205)
âˆ€ğ‘– âˆˆB ğ‘Ÿğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡0, . . . , ğ’‚ğ‘¡ ğµ)(0.5) |B |, where by ap-
plying ğ‘Ÿğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡0, . . . , ğ’‚ğ‘¡ ğµ) = 1|ğ‘Ÿğ‘– (ğ’”ğ‘¡ğ‘–, ğ’‚ğ‘¡0, . . . , ğ’‚ğ‘¡ ğµ), we can
get E[ğ‘Ÿğ‘–] = (0.5) |B |. Now, we can deï¬ne an expectation of
Ë†ğœ•
a gradient estimation as, E[
ğ¿ (ğœƒğ‘–) = (0.5) |B |.
ğœ•ğœƒğ‘–
Therefore, a variance of the estimated gradient can be deï¬ned
as,

ğ¿ (ğœƒğ‘–)] = ğœ•
ğœ•ğœƒğ‘–

V(cid:2)

Ë†ğœ•
ğœ•ğœƒğ‘–

ğ¿(ğœƒğ‘–)(cid:3) = E(cid:2)

Ë†ğœ•
ğœ•ğœƒğ‘–
= (0.5) |B | âˆ’ (0.5)2 |B | .

ğ¿2 (ğœƒğ‘–)(cid:3) âˆ’

(cid:18)

E(cid:2)

Ë†ğœ•
ğœ•ğœƒğ‘–

(cid:19) 2

ğ¿ (ğœƒğ‘–)(cid:3)

(35)

of
analyze
Now, we
ğ‘ƒ(( Ë†âˆ‡ğœƒğ‘– ğ¿(ğœƒğ‘–), âˆ‡ğœƒğ‘– ğ¿ (ğœƒğ‘–)) > 0) (in (29)), where

step

can

the

gradient

for

ğ‘ƒ

(cid:16) Ë†âˆ‡ğœƒğ‘– ğ¿(ğœƒğ‘–), âˆ‡ğœƒğ‘– ğ¿(ğœƒğ‘–)

(cid:17)

= (0.5) |B | âˆ‘ï¸
âˆ€ğ‘– âˆˆB

Ë†ğœ•
ğœ•ğœƒğ‘–

ğ¿ (ğœƒğ‘–).

(36)

As a result, ğ‘ƒ(( Ë†âˆ‡ğœƒğ‘– ğ¿ (ğœƒğ‘–), âˆ‡ğœƒğ‘– ğ¿(ğœƒğ‘–)) > 0) = (0.5) |B | implies
that the gradient step not only moves in the correct direction
but also decreases exponentially with an increasing number of
(cid:4)
BS agents.

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

21

REFERENCES

[1] W. Saad, M. Bennis and M. Chen, â€œA Vision of 6G Wireless Sys-
tems: Applications, Trends, Technologies, and Open Research Prob-
lems," IEEE Network, vol. 34, no. 3, pp. 134-142, May/June 2020, doi:
10.1109/MNET.001.1900287.

[2] J. Park, S. Samarakoon, M. Bennis, and M. Debbah, â€œWireless Network
Intelligence at the Edge," Proceedings of the IEEE, vol. 107, no. 11, pp.
2204-2239, Nov. 2019, doi: 10.1109/JPROC.2019.2941458.

[3] E. Dahlman, S. Parkvall, J. Peisa, H. Tullberg, H. Murai and M. Fujioka,
â€œArtiï¬cial Intelligence in Future Evolution of Mobile Communication,"
International Conference on Artiï¬cial Intelligence in Information and
Communication (ICAIIC), Okinawa, Japan, February 2019, pp. 102-106.
[4] M. S. Munir, S. F. Abedin and C. S. Hong, â€œArtiï¬cial Intelligence-
based Service Aggregation for Mobile-Agent in Edge Computing," 2019
20th Asia-Paciï¬c Network Operations and Management Symposium (AP-
NOMS), Matsue, Japan, September 2019, pp. 1-6.

[5] M. Chen, U. Challita, W. Saad, C. Yin and M. Debbah, â€œArtiï¬cial Neural
Networks-Based Machine Learning for Wireless Networks: A Tutorial,"
IEEE Communications Surveys & Tutorials, Early Access, July 2019.
[6] M. Chen, Z. Yang, W. Saad, C. Yin, H. V. Poor, and S. Cui, â€œA Joint
Learning and Communications Framework for Federated Learning over
Wireless Networks," IEEE Transactions on Wireless Communications,
vol. 20, no. 1, pp. 269-283, Jan. 2021, doi: 10.1109/TWC.2020.3024629.
[7] N. H. Tran, W. Bao, A. Zomaya, M. N. H. Nguyen and C. S. Hong,
â€œFederated Learning over Wireless Networks: Optimization Model Design
and Analysis," IEEE INFOCOM 2019 - IEEE Conference on Computer
Communications, Paris, France, 2019, pp. 1387-1395.

[8] G. Lee, W. Saad, M. Bennis, A. Mehbodniya and F. Adachi, â€œOnline
Ski Rental for ON/OFF Scheduling of Energy Harvesting Base Stations,"
IEEE Transactions on Wireless Communications, vol. 16, no. 5, pp. 2976-
2990, May 2017.

[9] Y. Wei, F. R. Yu, M. Song and Z. Han, â€œUser Scheduling and Resource
Allocation in HetNets With Hybrid Energy Supply: An Actor-Critic
Reinforcement Learning Approach," IEEE Transactions on Wireless Com-
munications, vol. 17, no. 1, pp. 680-692, Jan. 2018.

[10] J. Xu, L. Chen and S. Ren, â€œOnline Learning for Ofï¬‚oading and
Autoscaling in Energy Harvesting Mobile Edge Computing," IEEE Trans-
actions on Cognitive Communications and Networking, vol. 3, no. 3, pp.
361-373, Sept. 2017.

[11] M. S. Munir, S. F. Abedin, N. H. Tran and C. S. Hong, â€œWhen
Edge Computing Meets Microgrid: A Deep Reinforcement Learning
Approach," in IEEE Internet of Things Journal, vol. 6, no. 5, pp. 7360-
7374, October 2019.

[12] M. S. Munir, S. F. Abedin, D. H. Kim, N. H. Tran, Z. Han, and C. S.
Hong, "A Multi-Agent System Toward the Green Edge Computing with
Microgrid," 2019 IEEE Global Communications Conference (GLOBE-
COM), Waikoloa, HI, USA, 9-13 December 2019.

[13] N. Piovesan, D. A. Temesgene, M. Miozzo and P. Dini, "Joint Load
Control and Energy Sharing for Autonomous Operation of 5G Mobile
Networks in Micro-Grids," IEEE Access, vol. 7, pp. 31140-31150, March
2019.

[14] X. Huang, T. Han and N. Ansari, â€œSmart Grid Enabled Mobile Networks:
Jointly Optimizing BS Operation and Power Distribution," IEEE/ACM
Transactions on Networking, vol. 25, no. 3, pp. 1832-1845, June 2017.
[15] W. Li, T. Yang, F. C. Delicato, P. F. Pires, Z. Tari, S. U. Khan, and A.
Y. Zomaya "On Enabling Sustainable Edge Computing with Renewable
Energy Resources," IEEE Communications Magazine, vol. 56, no. 5, pp.
94-101, May 2018.

[16] Y. Mao, J. Zhang, S. H. Song and K. B. Letaief, â€œStochastic Joint Radio
and Computational Resource Management for Multi-User Mobile-Edge
Computing Systems," IEEE Transactions on Wireless Communications,
vol. 16, no. 9, pp. 5994-6009, September 2017.

[17] T. X. Tran and D. Pompili, â€œJoint Task Ofï¬‚oading and Resource
Allocation for Multi-Server Mobile-Edge Computing Networks," IEEE
Transactions on Vehicular Technology, vol. 68, no. 1, pp. 856-868,
January 2019.

[18] P. Chang and G. Miao, â€œResource Provision for Energy-Efï¬cient Mobile
IEEE Global Communications Conference

Edge Computing Systems,"
(GLOBECOM), Abu Dhabi, United Arab Emirates, 2018, pp. 1-6.
[19] Y. Sun, S. Zhou and J. Xu, â€œEMM: Energy-Aware Mobility Management
for Mobile Edge Computing in Ultra Dense Networks," IEEE Journal on
Selected Areas in Communications, vol. 35, no. 11, pp. 2637-2646, Nov.
2017.

[20] S. F. Abedin, M. G. R. Alam, R. Haw and C. S. Hong, â€œA system model
for energy efï¬cient green-IoT network," 2015 International Conference
on Information Networking (ICOIN), Cambodia, 2015, pp. 177-182.

[21] X. Zhang, M. R. Nakhai, G. Zheng, S. Lambotharan and B. Ottersten,
â€œCalibrated Learning for Online Distributed Power Allocation in Small-
Cell Networks," IEEE Transactions on Communications, vol. 67, no. 11,
pp. 8124-8136, Nov. 2019, doi: 10.1109/TCOMM.2019.2938514.

[22] S. Akin and M. C. Gursoy, â€œOn the Energy and Data Storage Manage-
ment in Energy Harvesting Wireless Communications," IEEE Transac-
tions on Communications, Early Access, August 2019.

[23] N. H. Tran, C. Pham, M. N. H. Nguyen, S. Ren and C. S. Hong,
â€œIncentivizing Energy Reduction for Emergency Demand Response in
Multi-Tenant Mixed-Use Buildings," IEEE Transactions on Smart Grid,
vol. 9, no. 4, pp. 3701-3715, July 2018.

[24] J. X. Wang et al., â€œLearning to reinforcement learn," CogSci, 2017. (In

London, UK).

[25] N. Schweighofera, and K. Doya, â€œMeta-learning in Reinforcement
Learning," Neural Networks, vol. 16, no. 1, pp. 5-9, January 2003.
[26] M. Andrychowicz et al., â€œLearning to learn by gradient descent by
gradient descent," Advances in Neural Information Processing Systems
29 (NIPS 2016), 2016.

[27] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
D. Silver, and K. Kavukcuoglu, â€œAsynchronous Methods for Deep Rein-
forcement Learning," Proceedings of The 33rd International Conference
on Machine Learning, vol. 48, pp. 1928-1937, Jan. 2016.

[28] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch, â€œMulti-
agent actor-critic for mixed cooperative-competitive environments," In
Advances in Neural Information Processing Systems, pp. 6379-6390.
2017.

[29] Y. Zhang, M. H. Hajiesmaili, S. Cai, M. Chen and Q. Zhu, â€œPeak-
Aware Online Economic Dispatching for Microgrids," IEEE Transactions
on Smart Grid, vol. 9, no. 1, pp. 323-335, Jan. 2018.

[30] T. Han and N. Ansari, "Network Utility Aware Trafï¬c Load Balancing in
Backhaul-Constrained Cache-Enabled Small Cell Networks with Hybrid
Power Supplies," in IEEE Transactions on Mobile Computing, vol. 16,
no. 10, pp. 2819-2832, 1 Oct. 2017.

[31] S. F. Abedin, A. K. Bairagi, M. S. Munir, N. H. Tran and C. S. Hong,
â€œFog Load Balancing for Massive Machine Type Communications: A
Game and Transport Theoretic Approach," IEEE Access, vol. 7, pp. 4204-
4218, December 2018.

[32] Z. Chang, Z. Zhou, T. Ristaniemi, and Z. Niu, â€œEnergy Efï¬cient
Optimization for Computation Ofï¬‚oading in Fog Computing System,"
IEEE Global Communications Conference, Singapore, December 2017,
pp. 1-6.

[33] A. Ndikumana, N. H. Tran, T. M. Ho, Z. Han, W. Saad, D. Niyato,
and C. S. Hong, â€œJoint Communication, Computation, Caching, and
Control in Big Data Multi-access Edge Computing," IEEE Transactions
on Mobile Computing, vol. 19, no. 6, pp. 1359-1374, 1 June 2020, doi:
10.1109/TMC.2019.2908403.

[34] T. Rauber, G. Runger, M. Schwind, H. Xu, and S. Melzner, â€œEnergy
measurement, modeling, and prediction for processors with frequency
scaling," The Journal of Supercomputing, vol. 70, no. 3, pp. 1454-1476,
2014.

[35] R. Bertran, M. Gonzalez, X. Martorell, N. Navarro and E. Ayguade,
â€œA Systematic Methodology to Generate Decomposable and Responsive
Power Models for CMPs," IEEE Transactions on Computers, vol. 62, no.
7, pp. 1289-1302, July 2013. Firstquarter 2016.

[36] G. Auer et al., â€œHow much energy is needed to run a wireless network?,"
IEEE Wireless Communications,vol. 18, no. 5, pp. 40-49, October 2011.
[37] ETSI TS, â€œ5G; NR; Physical layer procedures for data", [Online]:

www.etsi.org/deliver/etsi_ts/138200_138299/138214/15.03.00_60/
ts_138214v150300p.pdf, 3GPP TS 38.214 version 15.3.0 Release 15,
October 2018 (Visited on 18 July, 2019).

[38] Y. Gu, W. Saad, M. Bennis, M. Debbah and Z. Han, â€œMatching
theory for future wireless networks: fundamentals and applications," IEEE
Communications Magazine, vol. 53, no. 5, pp. 52-59, May 2015.

[39] F. Pantisano, M. Bennis, W. Saad, S. Valentin and M. Debbah, â€œMatching
with externalities for context-aware user-cell association in small cell net-
works," 2013 IEEE Global Communications Conference (GLOBECOM),
Atlanta, GA, 2013, pp. 4483-4488.

[40] N.L. Panwar, S.C. Kaushik, and S. Kothari, â€œRole of renewable energy
sources in environmental protection: a review," Renewable and Sustain-
able Energy Reviews, vol. 15, no. 3, pp. 1513-1524, April, 2011.

[41] F. A. Chacra, P. Bastard, G. Fleury and R. Clavreul, â€œImpact of energy
storage costs on economical performance in a distribution substation,"
IEEE Transactions on Power Systems, vol. 20, no. 2, pp. 684-691, May
2005.

[42] H. Kanchev, D. Lu, F. Colas, V. Lazarov and B. Francois, â€œEnergy
Management and Operational Planning of a Microgrid With a PV-Based

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

22

Active Generator for Smart Grid Applications," IEEE Transactions on
Industrial Electronics, vol. 58, no. 10, pp. 4583-4592, Oct. 2011.

[43] A. Mishra, D. Irwin, P. Shenoy, J. Kurose and T. Zhu, â€œGreenCharge:
Managing RenewableEnergy in Smart Buildings," IEEE Journal on
Selected Areas in Communications, vol. 31, no. 7, pp. 1281-1293, July
2013.

[44] X. Xu, Z. Yan, M. Shahidehpour, Z. Li, M. Yan and X. Kong,
â€œData-Driven Risk-Averse Two-Stage Optimal Stochastic Scheduling of
Energy and Reserve with Correlated Wind Power," IEEE Transactions
on Sustainable Energy, vol. 11, no. 1, pp. 436-447, Jan. 2020, doi:
10.1109/TSTE.2019.2894693.

[45] Business Insider, â€œOne simple chart shows why an energy revolution is
coming", [Online]: www.businessinsider.com/solar-power-cost-decrease-
2018-5, May 2018 (Visited on 23 July, 2019).

[46] Y. Liu, and N. K. C. Nair, â€œA Two-Stage Stochastic Dynamic Economic
Dispatch Model Considering Wind Uncertainty," IEEE Transactions on
Sustainable Energy, vol. 7, no. 2, pp. 819-829, April 2016.

[47] D. Zhou, M. Sheng, B. Li, J. Li and Z. Han, â€œDistributionally Robust
Planning for Data Delivery in Distributed Satellite Cluster Network,"
IEEE Transactions on Wireless Communications, vol. 18, no. 7, pp. 3642-
3657, July 2019.

[48] S. F. Abedin, M. G. R. Alam, S. M. A. Kazmi, N. H. Tran, D. Niyato
and C. S. Hong, â€œResource Allocation for Ultra-reliable and Enhanced
Mobile Broadband IoT Applications in Fog Network," IEEE Transactions
on Communications, vol. 67, no. 1, pp. 489-502, January 2019.

[49] S. Hochreiter, and J. Schmidhuberâ€ â€œLong short-term memory," Neural

Computation, vol. 9, pp. 1735-1780, 1997.

[50] Z. M. Fadlullah et al., â€œState-of-the-Art Deep Learning: Evolving
Machine Intelligence Toward Tomorrowâ€™s Intelligent Network Trafï¬c
Control Systems," IEEE Communications Surveys & Tutorials, vol. 19,
no. 4, pp. 2432-2455, Fourthquarter 2017.

[51] R. S. Sutton and A. G. Barto, "Reinforcement Learning: An Introduc-

tion," 2nd ed. Cambridge, MA, USA: MIT Press, vol. 1, 2017.

[52] M. L. Littman, â€œMarkov games as a framework for multi-agent reinforce-
ment learning," In Proceedings of the eleventh international conference
on machine learning, pp. 157-163, 1994.

[53] R. J. Williams, and D. Zipser, "Gradient-based learning algorithms for
recurrent," Backpropagation: Theory, architectures, and applications, vol.
433, 1995.

[54] I. Bialynicki-Birula, and J. Mycielski, "Uncertainty relations for infor-
mation entropy," Commun. Math. Phys., vol. 44, pp. 129-132, 1975.
[55] T. Seidenfeld, "Entropy and uncertainty," Phil. Sci., vol. 53, pp. 467-491,

1986.

[56] H. R. Feyzmahdavian, A. Aytekin and M. Johansson, "An Asynchronous
Mini-Batch Algorithm for Regularized Stochastic Optimization," IEEE
Transactions on Automatic Control, vol. 61, no. 12, pp. 3740-3754, Dec.
2016, doi: 10.1109/TAC.2016.2525015.

[57] A. Agarwal and J. C. Duchi, "The Generalization Ability of Online Al-
gorithms for Dependent Data," IEEE Transactions on Information Theory,
vol. 59, no. 1, pp. 573-587, Jan. 2013, doi: 10.1109/TIT.2012.2212414.
[58] A. M. Fink, "Equilibrium in a stochastic ğ‘›-person game," Journal of
Science of the Hiroshima University, Series A-I (Mathematics), vol. 28,
no. 1, pp. 89-93, 1964.

[59] P. J. Herings, and R. J. A. P. Peeters, "Stationary equilibria in stochastic
games: structure, selection, and computation," Journal of Economic
Theory, Elsevier, vol. 118, no. 1, pp. 32-60, September, 2004.

[60] S. Fu, and Y. Zhang,
2015-04-01),"

"CRAWDAD dataset due/packet-delivery
from www.crawdad.org/due/packet-

downloaded
(v.
delivery/20150401, Apr 2015 (Visited on 3 July, 2019).

[61] Online:â€œSolar

panel
www.traces.cs.umass.edu/index.php/Smart/Smart,
2019).

dataset,"

UMassTraceRepository
(Visited on 3 July,

[62] A. K. Bairagi, N. H. Tran, W. Saad, Z. Han and C. S. Hong, â€œA Game-
Theoretic Approach for Fair Coexistence Between LTE-U and Wi-Fi
Systems," IEEE Transactions on Vehicular Technology, vol. 68, no. 1,
pp. 442-455, Jan. 2019.

[63] Intel,

â€œIntel

Core

i7-6500U

Processor,"

[Online]:

www.ark.intel.com/content/www/us/en/ark/products/88194/intel-core-
i7-6500u-processor-4m-cache-up-to-3-10-ghz.html,
August, 2019).

(Visited

on

17

[64] Online:

"TensorFlow

Core

v2.2.0,"

TensorFlow,

www.tensorï¬‚ow.org/api_docs/python/tf/compat/v1/nn/rnn_cell
/BasicLSTMCell (Visited on 27 May, 2020).

[65] D.P. Kingma, and J. Ba, "Adam: A Method for Stochastic Optimiza-
tion," in Proceedings of the 3rd International Conference on Learning
Representations (ICLR),pp. 1-41, San Diego, USA, May 2015.

[66] Y. Takahashi, G. Schoenbaum, and Y. Niv, "Silencing the critics:
Understanding the effects of cocaine sensitization on dorsolateral and
ventral striatum in the context of an Actor/Critic model," Front. Neurosci.,
vol. 2, no. JUL, pp. 86-89, 2009.

[67] C. G. Li, M. Wang and Q. N. Yuan, "A Multi-agent Reinforcement
Learning using Actor-Critic methods," 2008 International Conference on
Machine Learning and Cybernetics, Kunming, 2008, pp. 878-882, doi:
10.1109/ICMLC.2008.4620528.
symbols
â€œAll

TensorFlow,"

[68] Online:

TensorFlow,

in

www.tensorï¬‚ow.org/api_docs/python/ (Visited on 3 July, 2019).

[69] Online: â€œModel evaluation: quantifying the quality of predictions,"
scikit-learn, www.scikit-learn.org/stable/modules/model_evaluation.html
(Visited on 3 August, 2019).

"Explained

[70] Online:
score
learn.org/stable/modules/generated/sklearn.metrics.explained_variance
_score.html (Visited on 28 May, 2020).

scikit-learn,

regression
www.scikit-

function,"

variance

[71] Online: "Mean absolute error regression loss," scikit-learn, www.scikit-

learn.org/stable/modules/generated/sklearn.metrics.mean_absolute
_error.html (Visited on 28 May, 2020)

[72] D. S. Johnson, "Near-optimal bin packing algorithms," Massachusetts

Institute of Technology, 1973.

Md. Shirajum Munir (Graduate Student Member,
IEEE) received the B.S. degree in computer science
and engineering from Khulna University, Khulna,
Bangladesh, in 2010. He is currently pursuing the
Ph.D. degree in computer science and engineering
at Kyung Hee University, Seoul, South Korea. He
served as a Lead Engineer with the Solution Labora-
tory, Samsung Research and Development Institute,
Dhaka, Bangladesh, from 2010 to 2016. His current
research interests include IoT network management,
fog computing, mobile edge computing, software-

deï¬ned networking, smart grid, and machine learning.

Nguyen H. Tran (Sâ€™10-Mâ€™11-SMâ€™18) received the
B.S. degree from the Ho Chi Minh City University
of Technology, Ho Chi Minh City, Vietnam, in 2005,
and the Ph.D. degree in electrical and computer en-
gineering from Kyung Hee University, Seoul, South
Korea, in 2011. Since 2018, he has been with the
School of Computer Science, University of Sydney,
Sydney, NSW, Australia, where he is currently a
Senior Lecturer. He was an Assistant Professor with
the Department of Computer Science and Engineer-
ing, Kyung Hee University, from 2012 to 2017. His
current research interests include applying analytic techniques of optimization,
game theory, and stochastic modeling to cutting-edge applications, such
as cloud and mobile edge computing, data centers, heterogeneous wireless
networks, and big data for networks. Dr. Tran was a recipient of the Best
KHU Thesis Award in Engineering in 2011 and the Best Paper Award of
IEEE ICC 2016. He has been an Editor of the IEEE TRANSACTIONS ON
GREEN COMMUNICATIONS AND NETWORKING since 2016 and served
as the Editor of the 2017 Newsletter of Technical Committee on Cognitive
Networks on Internet of Things.

ACCEPTED ARTICLE BY IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, DOI: 10.1109/TNSM.2021.3057960

23

Walid Saad d (Sâ€™07, Mâ€™10, SMâ€™15, Fâ€™19) received
his Ph.D degree from the University of Oslo in
2010. He is currently a Professor at the Department
of Electrical and Computer Engineering at Virginia
Tech, where he leads the Network sciEnce, Wireless,
and Security (NEWS) laboratory. His research in-
terests include wireless networks, machine learning,
game theory, security, unmanned aerial vehicles,
cyber-physical systems, and network science. Dr.
Saad is a Fellow of the IEEE and an IEEE Dis-
tinguished Lecturer. He is also the recipient of the
NSF CAREER award in 2013, the AFOSR summer faculty fellowship in
2014, and the Young Investigator Award from the Ofï¬ce of Naval Research
(ONR) in 2015. He was the author/co-author of eight conference best paper
awards at WiOpt in 2009, ICIMP in 2010, IEEE WCNC in 2012, IEEE
PIMRC in 2015, IEEE SmartGridComm in 2015, EuCNC in 2017, IEEE
GLOBECOM in 2018, and IFIP NTMS in 2019. He is the recipient of the
2015 Fred W. Ellersick Prize from the IEEE Communications Society, of
the 2017 IEEE ComSoc Best Young Professional in Academia award, of the
2018 IEEE ComSoc Radio Communications Committee Early Achievement
Award, and of the 2019 IEEE ComSoc Communication Theory Technical
Committee. From 2015-2017, Dr. Saad was named the Stephen O. Lane
Junior Faculty Fellow at Virginia Tech and, in 2017, he was named College
of Engineering Faculty Fellow. He received the Deanâ€™s award for Research
Excellence from Virginia Tech in 2019. He currently serves as an editor for
the IEEE Transactions on Wireless Communications, IEEE Transactions on
Mobile Computing, IEEE Transactions on Cognitive Communications and
Networking, and IEEE Transactions on Information Forensics and Security.
He is an Editor-at-Large for the IEEE Transactions on Communications.

Choong Seon Hong (Sâ€™95-Mâ€™97-SMâ€™11) received
the B.S. and M.S. degrees in electronic engineering
from Kyung Hee University, Seoul, South Korea, in
1983 and 1985, respectively, and the Ph.D. degree
from Keio University, Japan, in 1997. In 1988, he
joined KT, where he was involved in broadband
networks as a Member of Technical Staff. Since
1993, he has been with Keio University. He was
with the Telecommunications Network Laboratory,
KT, as a Senior Member of Technical Staff and as
the Director of the Networking Research Team until
1999. Since 1999, he has been a Professor with the Department of Computer
Science and Engineering, Kyung Hee University. His research interests include
future Internet, ad hoc networks, network management, and network security.
He is a member of the ACM, the IEICE, the IPSJ, the KIISE, the KICS,
the KIPS, and the OSIA. Dr. Hong has served as the General Chair, the
TPC Chair/Member, or an Organizing Committee Member of international
conferences such as NOMS, IM, APNOMS, E2EMON, CCNC, ADSN,
ICPP, DIM, WISA, BcN, TINA, SAINT, and ICOIN. He was an Associate
Editor of the IEEE TRANSACTIONS ON NETWORK AND SERVICE
MANAGEMENT, and the IEEE JOURNAL OF COMMUNICATIONS AND
NETWORKS. He currently serves as an Associate Editor of the International
Journal of Network Management, and an Associate Technical Editor of the
IEEE Communications Magazine.

