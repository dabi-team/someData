0
2
0
2

c
e
D
4
2

]

G
L
.
s
c
[

1
v
6
4
3
3
1
.
2
1
0
2
:
v
i
X
r
a

Parallel-beam X-ray CT datasets of apples with
internal defects and label balancing for machine
learning

Sophia Bethany Coban1,†,*, Vladyslav Andriiashen1,†,*, Poulami Somanya Ganguly1,2,†,*,
Maureen van Eijnatten1,3, and Kees Joost Batenburg4

1Centrum Wiskunde & Informatica, Science Park 123, 1098 XG Amsterdam, The Netherlands.
2Mathematical Institute, Leiden University, Niels Bohrweg 1, 2333 CA Leiden, The Netherlands.
3Eindhoven University of Technology, Groene Loper 3, 5612 AE Eindhoven, The Netherlands.
4Leiden Institute of Advanced Computer Science, Niels Bohrweg 1, 2333 CA Leiden, The Netherlands.
*corresponding author(s): S.B. Coban (s.b.coban@cwi.nl), V. Andriiashen (vladyslav.andriiashen@cwi.nl), P.S. Ganguly
(poulami.ganguly@cwi.nl@cwi.nl).
†These authors contributed equally to this work.

ABSTRACT

We present three parallel-beam tomographic datasets of 94 apples with internal defects along with defect label ﬁles. The
datasets are prepared for development and testing of data-driven, learning-based image reconstruction, segmentation and
post-processing methods. The three versions are a noiseless simulation; simulation with added Gaussian noise, and with
scattering noise. The datasets are based on real 3D X-ray CT data and their subsequent volume reconstructions. The
ground truth images, based on the volume reconstructions, are also available through this project. Apples contain various
defects, which naturally introduce a label bias. We tackle this by formulating the bias as an optimization problem. In addition,
we demonstrate solving this problem with two methods: a simple heuristic algorithm and through mixed integer quadratic
programming. This ensures the datasets can be split into test, training or validation subsets with the label bias eliminated.
Therefore the datasets can be used for image reconstruction, segmentation, automatic defect detection, and testing the effects
of (as well as applying new methodologies for removing) label bias in machine learning.

Background & Summary

X-ray computed tomography (CT) is a versatile non-destructive imaging technique with a wide range of applications in clinical1,
industrial2 and scientiﬁc settings3–6. The basic working principle of CT is based on the acquisition of raw measurement data
from different angles, via which an image can be reconstructed by solving the resulting inverse problem. Various analytical and
iterative methods have been proposed for this task. However, when the acquired measurement data is limited, e.g. due to reduced
X-ray ﬂux (low-dose), few angles or a limited angular range, analytical methods result in inaccurate and noisy reconstructions,
while the computational load of more sophisticated iterative methods is often too demanding for high-throughput settings.

In such cases, data-driven methodologies such as deep neural networks have shown promising results in tackling the
shortcomings of traditional analytical and iterative reconstruction methods7–9. However, certain challenges remain, such as the
large amounts of labelled data needed to develop and train deep neural networks. Particularly the absence of segmentation labels
based on accurate ground truth images is a major limitation in practical settings, as such labels typically needs to be obtained
manually. Furthermore, it remains unclear which deep learning-based method results in the best reconstruction accuracy but
also generalizes well to new, unseen samples. This means that the method should not over-ﬁt on the training data, with the risk
of signiﬁcant under-performance when transferring the method to different types of imaging data and applications10. Since
it can be difﬁcult or even impossible to avoid all types of bias during data acquisition11, techniques need to be developed to
mitigate the impact of bias prior to or even during training of deep learning methods.

A common factor in the successful applications of deep learning is the abundance of labelled training data12. To enable
the integration of deep learning in image reconstruction methods, it is therefore essential to make large tomographic dataset
– including the ground truth reconstructions – publicly available to the scientiﬁc community. To date, a limited number of
medical imaging datasets has been made available in the context of Grand Challenges. Examples of such challenges are the
low-dose CT Challenge initiated by the Mayo clinic13 and the fast MRI challenge14. However, in most of these challenges only
dozens of images have been made available. A larger medical tomographic dataset that has recently been made available is the

 
 
 
 
 
 
LoDoPaB dataset citeleuschner2019lodopab, which contains over 40000 scan slices from around 800 patients selected from the
LIDC/IDRI Database with medical chest CT scans16. In addition, a non-medical dataset consisting of 42 walnuts acquired
using a cone-beam CT scanner has been published for high cone-angle artefact reduction17. Nevertheless, these datasets remain
limited to speciﬁc use cases and CT applications.

As a case that is representative for a broad range of CT applications, in which products are scanned in large numbers and
labelled data are difﬁcult to obtain, this work focuses on abnormality detection in fruit18. We describe a dataset of 72192
two-dimensional CT images of apples with internal defects, and 50 corresponding parallel-beam projections for each image.
Three different types of projection data are available: 1) noise-free projections; 2) projections with added Gaussian noise, and 3)
projections with added scattering based on the apple volume for a subset of the images. The projections can be used to simulate
various sparse or limited-view angular range tomographic problems, in which the volumetric apple reconstructions serve as
ground truth. We show an example of such a tomographic problem, namely limited-view angular sampling, in Technical
Validation. Due to the large number of available projections and corresponding ground truth reconstructions, our dataset is
well-suited to train deep neural networks for CT reconstruction tasks.

As an additional contribution of this work, we introduce two fast and effective methods to eliminate label bias when
splitting data for machine learning purposes. We describe an empirical splitting method and a splitting method through integer
programming that can be used to split a labelled dataset into balanced training, validation and test subsets. In this context,
balanced means that the internal defect percentages in the apples are equally distributed across the different splits, and therefore
there is no bias towards a speciﬁc apple, and more importantly, a speciﬁc defect within the apple.

Methods

Tomographic data acquisition and reconstruction of the apples
The tomographic datasets of 94 apples were acquired using the custom built and highly ﬂexible CT scanner at the FleX-
ray Laboratory, developed by TESCAN-XRE NV, managed by the Computational Imaging group at Centrum Wiskunde &
Informatica, Amsterdam19.

The scanner consists of a cone-beam microfocus X-ray point source that projects polychromatic X-rays onto a 1944 × 1536
pixels, 14-bit, ﬂat detector panel. The scanned apples were of similar sizes, thus the tube or geometry settings were determined
for the shape and material density of the ﬁrst apple, and remained unchanged for the rest of the scans. The tube voltage and
output power were 45kV/45W, respectively. Each apple data was captured at sample resolution of 54.2µm (magniﬁcation:
1.4) over a 360◦ in circular and continuous motion. A total of 1200 projections (or radiographs) were collected, which were
distributed evenly over the full circle (the angular increment was 0.3◦). Each projection was acquired with 100ms exposure
time. In addition, an open-shutter (ﬂat-ﬁeld) and closed-shutter (dark-ﬁeld) images were taken before each data collection in
order to normalise the acquired data and therefore accurately capture the material attenuation distribution free of any tube- or
detector-dependent artefacts. The total scanning time for each apple was just over 4 minutes.

Prior to any ground truth reconstruction, each dataset was normalised such a way that satisﬁes the Beer-Lambert Law20, 21,

P =

(cid:90)

L

µδ L = − log

I
I0

,

(1)

where P denotes the collected projections, which are the deﬁnite integral (or the sum) of material attenuation coefﬁcient, µ,
over the sample thickness, L, (the "ray-sum"); I0 the difference between each projection and the open-shutter images, I the
difference between closed-shutter and open-shutter images, and log the natural logarithm. The "ray-sum" in Equation 1 is
typically expressed using the Radon transform22, where the image is recovered via a back-projection (inverse Radon transform)
and an appropriate ﬁlter (often the ramp ﬁlter)23, 24. The image can also be recovered via an iterative reconstruction method
with an appropriate discretization of the geometry set-up. This allows the problem of image reconstruction to be expressed as a
set of linear equations, Ax = b, where x is the image to reconstruct, b is the vectorized projection data, and A is the geometry
set-up obtained with a ray tracing algorithm25, 26.

Adjustments to any rotational mismatch of the sample (also known as centre-of-rotation correction) is achieved via the
algorithm described in27, applied to the geometry prior to each reconstruction. The implementation of this correction algorithm
is included in28.

For the ground truth generation, we employed an iterative reconstruction method, namely, the conjugate gradient least
squares (CGLS)29, 30. The reconstructions were obtained using the CGLS algorithm with 15 iterations, employing the same
set-up as described in28, 31. Reconstructions were computed on a Linux machine with speciﬁcations 2.1GHz, 64-bit 16-core
CPU (2x 8-core Intel Xeon Silver 4110) using a single GPU with 11GB RAM (NVIDIA Geforce GTX 1080Ti). Data processing
and reconstruction for each apple took around 7 minutes to complete.

2/21

Ground truth generation
The ground truth slices were generated based on the cone-beam volumetric CGLS reconstruction of each apple. We note here
that when we mention an "apple slice", we refer to a horizontal 2D slice through the 3D reconstruction of an apple. Otherwise,
we specify that it is a vertical apple slice through the reconstruction. Figure 1 shows the cone-beam tomographic set-up in
which the apples were scanned, and the orientation of the apple slices in vertical and horizontal dimensions. Note that the voxel
grid in the ﬁgure is denoted by N × N × M, which in our case was 972 × 972 × 768.

Each apple slice was saved as an individual image, meaning we had 94 × 768 = 72192 reconstructed slices. However,
not all slices carried relevant information: some slices often showed the remains of sepal or stamens of the apple, or were
simply air. This meant that we had to cut out a portion at the bottom of the reconstructed volume in order to reduce the data to
contain only relevant apple information. Because the apples were of the same type (and thus of similar shape and sizes), we
decided to determine which slices to discard using the ﬁrst apple. Each horizontal slice with all pixels close to zero (with zero
denoting "air") were treated as empty slices. This was roughly equal to 50 slices at the bottom of a vertical apple slice. An
illustration of this is given in Figure 2 where the empty slices and their corresponding total grey-values are indicated with an
orange rectangular box.

We have also removed the top 50 slices as an attempt to exclude any high cone-beam artefact, which stemmed from the
sample nearing the border of the cone of X-ray beams. Apples that were bigger than the average apple showed more pronounced
high cone-beam artefacts as they were harder to ﬁt into the ﬁeld-of-view (without having to change the acquisition geometry).
Therefore discarding the very top slices also meant the exclusion of such effect. For the ﬁrst apple, the slices affected by the
high cone-beam artefact were determined by the narrow spike in the total grey-value distribution. This is indicated in Figure 2
with a red arrow, and the affected slices (which are later discarded) with a red box.

Parallel beam projection data
We simulated 1D parallel beam projections of each apple slice using the 2D Radon transform

Pθ (t) =

(cid:90) ∞

(cid:90) ∞

−∞

−∞

µ(x, y)δ (x cos θ + y sin θ − t)dxdy,

(2)

where µ(x, y) is the attenuation coefﬁcient at the voxel grid point (x, y), θ is the projection angle and Pθ (t) is the 1D projection
at angle θ . In practice, one has to discretise the Radon transform to apply it as a linear operator. Several discretised versions of
the Radon transform are available in open-source software packages, such as the ASTRA toolbox32 and SCIKIT-IMAGE33. We
used the Operator Discretisation Library (ODL)34 to deﬁne our projection geometry (Figure 3) and generated the data using the
CUDA projection kernel from the ASTRA toolbox as backend.

Using the same discretisation for simulated data generation and reconstruction amounts to committing an "inverse crime".
This is because data that is generated in this way lives in the range of the forward operator, and consequently, the (noiseless)
reconstruction problem is not ill-posed35. In order to avoid this, we upscaled all ground truth slices with bilinear interpolation
using SCIKIT-IMAGE. These upscaled images were used to generate projection data.

For each apple slice, projection data for 50 angles uniformly spaced in [0, π) were generated. These are henceforth referred
to as Dataset A. Dataset B was generated by corrupting Dataset A with 5% Gaussian noise, which has been used in the literature
to model electronic and thermal noise in the detector during data acquisition36. For each of the data in Dataset A, a zero-mean
Gaussian distribution with a standard deviation equal to 5% of the noiseless data mean was generated in the projection space
using odl.phantom.noise.white_noise. This distribution was then pointwise added to the noiseless data to generate
Dataset B.

Scattering
As an additional challenge, we imitated an effect of X-ray photon scattering for the noiseless data (Dataset A). An accurate
simulation can be done using Monte-Carlo methods. However this solution requires signiﬁcant computational time to reduce
relative error due to Poisson noise. Instead we have applied an approximate ﬁlter that does not precisely predict the physical
effect. The ﬁlter is based on the scattering kernel superposition method and uses a simpliﬁed model from37. The scattering
signal is calculated as a sum of contributions from pencil beams passing through the object. The scattering kernel is not constant
and depends on the thickness of the object along a pencil beam trajectory. Scattering intensity in a pixel u(cid:48) from the pencil
beam corresponding to the pixel u is
S(u(cid:48)) = T (u)G(u, u(cid:48)),

(3)
where T (u) is a forward scattering intensity, and G(u, u(cid:48)) is deﬁned by the angular distribution. This equation was written in a
simple form

S(u(cid:48)) = A(u) exp

(cid:21)

(cid:20)

−

(u − u(cid:48))2
2σ1(u)2

(cid:20)

+ B(u) exp

−

(u − u(cid:48))2
2σ2(u)2

(cid:21)

,

(4)

3/21

where |u − u(cid:48)| is a distance between pixels, and scattering is approximated as a combination of Gaussian blur with standard
deviations σ1 and σ2. Gaussian blur parameter depends on the X-ray absorption in the pixel u.

To sample functions A(u), B(u), σ1(u) and σ2(u), a Monte Carlo simulation based on Geant4 toolkit38 is performed for
different thicknesses of the material. In the simulation a pencil beam with an energy spectrum corresponding to the 45kV
tube passes through a cylinder made of water. Thickness of the cylinder can be varied to measure different values of intensity
functions A(u) and B(u). A distribution of photons around the cylinder is measured to approximate values of standard deviations
σ1(u) and σ2(u).

The approximate scattering effect was applied to the subset of slices for every apples in the dataset according to Equation 4.
We note that this ﬁlter is not expected to accurately represent the distribution of the physical effect since the scattering properties
of the ﬂesh of an apple differ from those of water. The main goal of this post-processing is to create an additional challenge for
reconstruction algorithms. The functions A(u) and B(u) were multiplied by a scattering intensity scaling factor, α, to enhance
the effect. Figure 4 shows the comparison of reconstructed slices with different values of α with α = 1 corresponding to the
scattering of water. Larger values of α naturally lead to stronger artefacts and noisier reconstructions.

Slices with the approximate scattering noise are henceforth referred to as Dataset C. We note that the scattering ﬁlter was
applied to Dataset A, and therefore, Dataset C contains only scattering and no other types of noise. We chose α = 5 as the
scattering intensity scaling factor for Dataset C. This effect was applied to a selection of apple slices to reduce the computation
time. These slices are 195–214, 365–404, 525–544 (80 slices in total).

Eliminating the label bias
For the purpose of developing or testing learning-based reconstruction algorithms, the Apple CT datasets will have to be split
into training and test subsets. In this paper, we consider a rough split of 20% – 80% for the test and training subsets, respectively.
To prevent any selection bias towards a speciﬁc apple, it is essential that slices from the same apple did not appear in both
subsets simultaneously. Therefore the split was performed on an "apple level". We prepared the data split such that 20 apples
were selected for the test subset, and the remaining 74 for the training.

The scanned apples contain four types of defects, namely bitterpit, holes, browning and rot. We note that these are not the
only defects that appear in this selection of apples, but they are the most common types found in the unhealthy parts of the
samples. Therefore we primarily focus on these four defects; the smaller or more uncommon defects are counted as healthy
parts of the apples.

Earlier we mentioned a "rough split of 20% – 80%" while going for a 20 apples – 74 apples split. In truth, the main criterion
of a good split is an equal percentage of the defects within the test or training subsets and not in the numbers of the apples. If
one defect has a higher representation in either of the subset, the selection bias might affect the ﬁnal results. Therefore, we need
an equal representation of each defect in both subsets. Our goal is to ﬁnd such a sequence of 20 apples that contain 20% of each
defect for the test subset, meaning the remaining 74 apples would have 80% of each defect for the training subset.

Along with the dataset, we include two apple label ﬁles that contain the number of pixels assigned for each defect, for all
apples. These ﬁles were obtained using a label ﬁle (on slice level) provided to us by our industrial partners. The slice-based
label information was then summed per apple in order to be able to calculate defect percentages for each apple. We use this
information to consider two methods of splitting the dataset into test or training subsets.

We note here that the target defect percentage and the number of apples in a subset are independent of one another, meaning
any target defect percentage with any number of apples can be achieved. However, as the percentage and the apple numbers
grow farther from one another, the problem of ﬁnding an optimal sequence becomes unstable with either method explained
below. With the code included in this submission, readers can perform a split of any size for any defect percentage suitable for
various levels of difﬁculty when assessing their machine learning based reconstruction methods.

Empirical data split
We ﬁrst consider a simple heuristic algorithm, in which a random sequence of apples is tested until the target defect percentage
for a selected defect is obtained while constraints are still satisﬁed. In our case, the defect we concentrated on was "browning"
as this was the most prominent of the four defects. Figure 5 shows an example of a reconstructed slice of a healthy apple and a
slice from an unhealthy apple with browning defect. Our initial aim was to reach the 20% threshold for the browning percentage
while the number of apples in sequence was exactly 20. At such a time these two conditions are satisﬁed, the tested sequence is
saved as "successful". We used the function randperm in MATLAB39, 40 (equivalent to numpy.random.permutation
for Python41), which takes a sequence of integers (in this case apple indices from 1 to 94) and performs a random permutation
without any repeats. We performed this permutation 10000 times and recorded all "successful" sequences of apples. It is
interesting to note that out of 10000 permutations, 545 were recorded as "successful". In fact, through testing, we noticed that
for any given number of permutations, roughly 5.4% of the sample size would be classiﬁed as a successful sequence. For the
interested reader, we include a table of the empirical splitting results for sample size 1eγ, for γ = 2, . . . , 8 in Table 1. For γ = 1
and γ ≥ 9 are excluded: for the former, chances of a successful sequence is very low (often not found), and for the latter, the

4/21

memory required exceeds the current limit. The table includes the number of successful sequences and the CPU time it took to
complete the samples.

We sorted the difference of the defect percentages for each successful sequence in ascending order, and proceeded with the
top result (i.e. the sequence with the least difference in defect percentages was selected). This sequence is given in the ﬁrst
column of Table 2, with the corresponding apple numbers in the adjacent column. The defect percentage results are given in the
stacked bar chart in Figure 6.

We note that this is an empirical method of achieving a split with defect bias eliminated but certainly not the only one. We
also note that the sequences for Dataset A, B and for Dataset C are different since the number of slices differ. The results of the
split for Dataset C are given in Table 3 with the corresponding defect percentage spread in Figure 7.

Data split through integer programming
The above method, introduced as "empirical", can be seen as an example of a greedy algorithm however one can also consider
the data splitting as a 0-1 knapsack problem. The knapsack problem is a combinatorial optimization problem where we select
items from a given set where each item has a certain weight and value. The aim is to maximize the total value while staying
below a given weight threshold. The 1-0 knapsack problem is where the items are integers and are simply labelled as "in" (1) or
"out" (0). The problem can be NP-hard, yet still be efﬁciently solved42.

Within the context of the apple data split into test and training subsets, we can formulate the problem as the following.

ci, jxi − b j(cid:107)2
2,

wixi = N, and

min (cid:107)∑
i
s.t. ∑
i
xi ∈ {0, 1},

i = 1, . . . , 94,

(5)

where b j is the target percentage of each defect j in the subset; ci, j ∈ R94×4 is the matrix containing defect percentage
information for each scanned apple, wi a vector of ones, N the constraint on the number of apples in the subset and (cid:107) · (cid:107)2
denotes the (Euclidean) 2-norm. The array xi is the binary-valued vector of apples, where the value 1 indicates an apple to
include in the subset; 0 the apple to exclude, and the index i of the array denotes the speciﬁc apple corresponding to the row
i of ci, j. We set our problem to obtain a sequence for the test subset, meaning we pick b j = 0.20, j = 1, . . . , 4, for the target
defect percentages, and N = 20 for the number of apples in the sequence. A clear advantage of this method over the empirical
algorithm is that it allows for the optimisation for all defects simultaneously.

We used CVX43, 44, a toolbox used for convex optimisation for MATLAB and Python, in conjunction with the MOSEK45, 46
solver in MATLAB, and ECOS_BB47–49 in Python. The former requires an academic license that takes minutes to obtain, and
latter being open-source. The precision for either solver was set to default, which is ε = 2.22e−850. The resulting sequences
for Dataset A, B and separately for Dataset C are given in Tables 2,3, and their corresponding defect percentage distribution in
Figure 6,7. For more information on these speciﬁc solvers, we refer the readers to51–53. We also detail how these solvers are
employed in Usage Notes and the relevant information for the scripts in Code Availability.

Data Records

Dataset are made available via Zenodo54, along with two label ﬁles containing pixel numbers for each defect for all apples.
Each dataset is uploaded as a compressed folder, which can be downloaded individually. Full data information can be found in
Table 4.

Technical Validation

The availability of large, publicly available datasets – comprising both the raw tomographic data and corresponding ground truth
image reconstructions – is a key enabling factor for the development of data-driven learning-based reconstruction algorithms.
Such algorithms are currently being developed for various applications of tomographic imaging, in which the acquired data are
limited in one or more ways, such as low-dose55, sparse-angle56, or limited-view57 CT. In this section, we will demonstrate
the feasibility of using our apple CT datasets to simulate one of these limited-data CT set-ups, namely limited-view angular
sampling. Limited-view image artefacts are very common in in high-throughput58 or conveyor belt59 CT, in which full angular
range data cannot be obtained due to limitations in acquisition time, apparatus set-up or hardware conﬁgurations.

We use our apple CT dataset to simulate two different limited-view settings. In both settings, we have data along 30
projection angles and a total missing wedge of 75.6°. In the ﬁrst setting (henceforth Sampling 1), we consider projection data
between 37.8° and 142.2°. In the second (Sampling 2), we consider data between 1.8° and 52.2° as well as between 127.8°
and 178.2°. These two conﬁgurations are shown in Figure 8 where the blue lines indicate an acquired view, and grey, missing.

5/21

We computed ﬁltered back-projection (FBP) reconstructions for Datasets A, B and C with data from the full angular range,
as well as the two limited-view settings (Figure 9). For this, we deﬁned a reconstruction space and projection geometry using
ODL, and used these to deﬁne a discretised ray transform and an FBP operator:

reco_space = odl.uniform_discr(min_pt, max_pt, reco_shape)
reco_geometry = odl.tomo.parallel_beam_geometry(reco_space, num_proj_angles)
reco_ray_trafo = odl.tomo.RayTransform(reco_space, reco_geometry)
fbp_op = odl.tomo.analytic.fbp_op(reco_ray_trafo).
The reconstruction could then be obtained by applying the FBP operator to data:

reco = fbp_op(data).
Reconstructions of noisy data using FBP were poor and those of limited-view data had severe artefacts. In the following,
we show that a convolutional neural network (CNN) can be trained to improve the image quality of these FBP reconstructions
by means of post-processing. A mixed-scale dense convolutional neural network (MS-D network) recently proposed by Pelt et
al.60 was subsequently trained to correct the limited-view artefacts. We used a standalone MS-D network implementation for
Python with a width of 1 and a depth of 100 layers. The MS-D networks for all datasets were trained for around 90-100 epochs,
and the parameters with the best validation loss were used for artefact removal. Networks for Dataset A were trained for 81
epochs (Sampling 1) and 67 epochs (Sampling 2); for Dataset B, 87 epochs (Sampling 1) and 83 epochs (Sampling 2), and for
Dataset C, 116 epochs (Sampling 1) and 109 epochs (Sampling 2). Validation subset size is 1000 images for Datasets A and B,
and 250 for Dataset C.

In order to demonstrate the results of the training, we include the FBP input and the output of post-processing for two
random apple slices. The ground truth reconstructions of these slices are given in Figure 10. The FBP reconstructions for
Datasets A-C (the input), and the corresponding MS-D post-processing results (the network output) for both limited-view
angular sampling are shown in Figures 11 and 12. Visually, the improvements on the input images are clear: it is evident that
the network has learned the general shape of the apples, and therefore manages to compensate for the missing views well. The
inner texture of the apples is not fully recovered, though the defects in either slice are recognisable by eye. In addition, we note
the shape of the core of the apples, which still exhibit some inﬂuence of the missing view artefacts (stretched out length-wise
for Sampling 1, width-wise for Sampling 2). This affect is particularly pronounced for Sampling 2, Dataset B in both slices. In
fact, in the cases of Dataset B, the location and shape of defects are still heavily inﬂuenced by the missing view artefacts. This
brings up the question how reliable MS-D network is in a practical setting with more unpredictable or ampliﬁed noise, or what
else can be done to improve the input image or the network performance in order to accurately identify the size, shape and
location of defects within an apple slice.

We employed peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) to assess the improvements
in image quality. These measures are included for both the input slices (the FBP reconstructions) and the MS-D network output.
The results given in Table 5 are the mean and standard deviation of PSNR and SSIM results based on 15360 slices for Datasets
A and B, and 1600 slices for Dataset C. We must note that these measures are not indicators of how accurate a physical result
can be extracted from these reconstructions, but rather how similar an image is (in terms of grey-value distribution and structural
similarity) to the ground truth image. The PSNR and SSIM results validate the improvements we saw in the apple slices: the
results improve drastically upon learned post-processing, with the output reconstructions for Dataset A and C approaching the
image quality of the ground truth, and those for Dataset B following closely behind.

Usage Notes

The results presented in this manuscript are obtained using scripts written in MATLAB and Python programming languages.
The scripts are tested with MATLAB 2017a,b to 2020a, and Python 3.6.10 to 3.7.7, and are distributed under the MIT license.
We make use of the following toolboxes: ASTRA32, ODL34, CVX43, 47 (MOSEK46 and ECOS_BB49) and Beads Dataset
Project28, 61.

ODL and the ASTRA toolbox can be installed in an appropriate Python environment using conda by running conda
install -c odlgroup odl and conda install -c astra-toolbox astra-toolbox, respectively. To
use the latest development version of the ASTRA toolbox, run conda install -c astra-toolbox/label/dev
astra-toolbox. Detailed installation instructions iii are available in the respective GitHub repositories.

CVX download and installation notes for MATLAB are found in iii iv. The steps can be summarised as the following: (1)
Download and unzip the appropriate package anywhere (we suggest the home directory); (2) Install any additional licenses
for solvers (in our case MOSEK, more on this later), (3) Add the full path to the unzipped location (the folder will be named

ihttps://github.com/astra-toolbox/astra-toolbox
iihttps://github.com/odlgroup/odl
iiihttp://cvxr.com/cvx/download/
ivhttp://cvxr.com/cvx/doc/install.html

6/21

"cvx"). You can add path using the addpath command in MATLAB. This is akin to importing a package in Python, and
is required for any external toolboxes. Finally, (4) install CVX by typing cvx_setup in the console (Command Window).
For Python, you can install CVXPY in a suitable environment by using pip or conda v, using pip install cvxpy or
conda install -c conda-forge cvxpy.

MOSEK solver is downloaded via the CVX toolbox however it requires an additional license. Readers can request a
Personal Academic Licensevi using an email address belonging to any academic institute. The license named "mosek.lic" is
emailed within minutes. Create a folder "mosek" and place the license ﬁle prior to installing CVX. In Python, the ECOS_BB
solvervii is installed along with the default installation of CVXPY. However, it is not the default solver and users are required to
specify solver=’ECOS_BB’ in prob.solveviii.

Code Availability

The latest versions of the scripts can be found on the Computational Imaging group GitHub repositoryix. The label ﬁles
apple_defects_full.csv and apple_defects_partial.csv contain the total number of labelled pixels for all 8 defects per apple,
as well as healthy pixels and the background. These ﬁles are required for some of the scripts, which we introduce below.
For ease of readability, the scripts are categorized into the following subsections: Dataset generation, bias elimination, and
reconstruction.

Dataset generation
Here, we list the scripts used to obtain all three versions of the simulated datasets.

generate_parallel_data.py
This script can be used to generate parallel-beam datasets similar to Datasets A and B from ground-truth reconstructions. It
contains the deﬁnition of our parallel beam geometry using ODL, with the ASTRA cuda projector as backend. The script
requires the user to specify the directory where the ground truth reconstructions are saved as well as the directory names for
storing the noiseless and noisy data. This can be done by setting the options in the function call as follows:

python generate_parallel_data.py -s ’source_dir’ -d ’noiseless_data_dir’
-n ’noisy_data_dir’

main.py
Scattering generation is performed using a Python script with C program utilized for scattering calculations. The C code can be
compiled using the attached Makeﬁle by simply running the following command:

make

Scattering can be calculated for a set of noiseless projections by executing the command

python main.py -i /path/to/input/ -o /path/to/output/ -a 5.0 -b 375 -e 385
which loads the projections in the folder speciﬁed by the "-i" option; multiplies the scattering effect by the intensity factor (in
the case of Dataset C, this was al pha = 5), and saves the results in the output folder. The output folder is expected to contain
two sub-folders, log_data and raw_data. Scattering is applied to a selected range of rows, which are set using the "-b" and "-e"
options (by default, this is set to rows 375–385).

Bias elimination
For the purpose of developing algorithms based on learning information from a training set or assess a result against a test or a
validation set, we have to split the data in such a way that we avoid introducing any bias towards a speciﬁc apple or a defect
within that apple. We described two methods in Eliminating bias: a simple heuristic method (referred to as "Empirical") and
mixed-integer quadratic programming using MOSEK via MATLAB ("MOSEK") and ECOS_BB via Python,("ECOS_BB").
Here we list the scripts used to solve this problem. Users can obtain the sequences given in Tables 2,3 and the defect percentages
in Figures 6,7 by following the instructions below.

vhttps://www.cvxpy.org/install/
vihttps://www.mosek.com/products/academic-licenses/
viihttps://github.com/embotech/ecos
viiihttps://www.cvxpy.org/tutorial/advanced/index.html
ixhttps://github.com/cicwi/applect-dataset-project

7/21

datasplit_empirical.m and datasplit_empirical.py
These scripts are equivalent, and therefore require the same input. The output differs due to the element of randomisation
but the problem is solved in the same way. The scripts require four input variables: Path location to the data ﬁle with defect
information for each apple (required); sample size for the number of times the apple index is permuted (optional, default is
10000 samples), target percentage for the browning defect within the subset, i.e. Subset 1 (optional, default is 0.2, as in 20%),
and required number of apples in the subset (optional, default is 20 apples). Once Subset 1 is determined, the remaining apples
are considered as part of Subset 2.
For a 30%, 10 apples test – 70%, 84 apples training data split, running the following commands in MATLAB and in Python
would return similar defect percentages given in Figure 6.

datasplit_empirical(filepath, 10000, 0.3, 10)
python datasplit_empirical.py -i filepath -s 10000 -p 0.3 -n 10

where ﬁlepath is the location of the defect data ﬁle "apple_defects_full.csv" is saved. This is valid for Dataset A and B. For
Dataset C, the required input ﬁle is "apple_defects_partial.csv".
The script returns two data ﬁles (in csv format): the apple defect information for the sequence determined as Subset 1, and the
remaining apples as Subset 2. Files are saved to the current folder. In addition, a text ﬁle named "empirical_split_results.txt" is
saved, which contains the number of successful sequences, the best sequence for Subset 1 and 2 and their defect percentages.
Script also prints the following messages to the console:

Results for split percentage 0.30 are saved in empirical_split_results_subset1.csv.
Results for split percentage 0.70 are saved in empirical_split_results_subset2.csv.

datasplit_mosek.m
The script solves the problem described in Equation 5 using CVX via MOSEK solver. The problem is solved by executing the
command

datasplit_mosek(filepath, 0.4, 30)

which would return a qualifying sequence for Subset 1 containing 30 apples with 40% defect percentage distribution, and the
remainder included in Subset 2. The only required input is the ﬁlepath: the target defect percentage value and number of apples
are optional, with the same default values as in datasplit_empirical. The script assumes CVX is added to the current
folder path, and will quit with an error otherwise. The script saves three ﬁles: defect data for both subsets and a text ﬁle named
"mosek_split_results.txt" with the solver termination details (optimal value and CPU time) as well as the defect percentage
spread and the sequence for both subsets. The console output for above command is

Results for split percentage 0.40 are saved in mosek_split_results_subset1.csv.
Results for split percentage 0.60 are saved in mosek_split_results_subset2.csv.

datasplit_ecos.py
The script solves the problem described in Equation 5 using CVX via ECOS_BB solver. The problem is solved by executing
the command

python datasplit_ecos.py -p 0.4 -n 25 -i filepath

which would return a qualifying sequence for Subset 1 containing 25 apples with 40% defect percentage distribution, and the
remainder in Subset 2. The default values are the same as for the previous scripts. The console and ﬁle output are as given
above, with the word "mosek" replaced by "ecos" in the ﬁle names.

Reconstruction
Lastly, we include the scripts used to obtain the results presented in Technical Validation.

reconstruct_fbp.py
This script was used to obtain the ﬁltered back-projection (FBP) reconstructions of all datasets for full angular range as well as
the two limited-view angular samplings (Figure 9). The script requires the user to specify the data directory with the option
"-d". The directories where the reconstructions will be saved must also be provided with option "-f". This can be done by
running the following command

python reconstruct_fbp.py -d ’data_dir’ -f ’full_range_reco_dir’
-m ’sampling1_reco_dir’ -n ’sampling2_reco_dir’

train.py
This script trains an MS-D network that corrects limited-angle artefacts, demonstrated in Technical Validation. This can be
done by running the command

python train.py -v 1000 -l 100 -i /path/to/input -t /path/to/target

which starts MS-D network training with 100 layers, 1000 images in the validation subset. Options "-i" and "-t" are used to set

8/21

the location of input and ground truth reconstructions, respectively. Training process runs until the script execution is stopped
by the user manually. Every 1000 images the script performs a validation step, and saves the network parameters if a smaller
loss value is achieved.

test.py
This script tests a trained MS-D network. This script should be executed in the same folder that was used for network training.
It uses network parameters from the ’regr_params.h5’ ﬁle. Test can be executed by running the command

python test.py -i /path/to/input -t /path/to/target -v

where "-i" and "-t" options are used to set the location of input and ground truth reconstructions, respectively. If the "-v" option
is included, the script will save network output and comparison images into the results/ folder. The script outputs average
values of PSNR and SSIM for the input and the network output reconstructions.

test_scattering.py
This script tests an MS-D network trained on limited-view reconstruction from Dataset C. The script is similar to test.py, but
only a subset of slices is taken from the ground truth reconstructions. It can be executed by running the command
python test_scattering.py -i /path/to/input -t /path/to/target -v

References

1. Liguori, C. et al. Emerging clinical applications of computed tomography. Med. Devices (Auckland, NZ) 8, 265 (2015).

2. De Chiffre, L., Carmignato, S., Kruth, J.-P., Schmitt, R. & Weckenmann, A. Industrial applications of computed tomography.

CIRP Annals 63, 655 – 677, https://doi.org/10.1016/j.cirp.2014.05.011 (2014).

3. Mees, F., Swennen, R., Van Geet, M. & Jacobs, P. Applications of x-ray computed tomography in the geosciences. Geol.

Soc. London, Special Publ. 215, 1–6 (2003).

4. Dierolf, M. et al. Ptychographic x-ray computed tomography at the nanoscale. Nature 467, 436–439 (2010).

5. Egan, C. et al. 3d chemical imaging in the laboratory by hyperspectral x-ray computed tomography. Sci. reports 5, 15979

(2015).

6. Morigi, M., Casali, F., Bettuzzi, M., Brancaccio, R. & d’Errico, V. Application of x-ray computed tomography to cultural

heritage diagnostics. Appl. Phys. A 100, 653–661 (2010).

7. Bubba, T. A. et al. Learning the invisible: A hybrid deep learning-shearlet framework for limited angle computed

tomography. Inverse Probl. 35, 064002 (2019).

8. Adler, J. & Öktem, O. Learned primal-dual reconstruction. IEEE transactions on medical imaging 37, 1322–1332 (2018).

9. Arridge, S., Maass, P., Öktem, O. & Schönlieb, C.-B. Solving inverse problems using data-driven models. Acta Numer. 28,

1–174 (2019).

10. Reitermanova, Z. Data splitting. In WDS, vol. 10, 31–36 (2010).

11. Biondetti, G. P., Gauriau, R., Bridge, C. P., Lu, C. & Andriole, K. P. " name that manufacturer". relating image acquisition
bias with task complexity when training deep learning models: experiments on head ct. arXiv preprint arXiv:2008.08525
(2020).

12. Deng, J. et al. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and

pattern recognition, 248–255 (Ieee, 2009).

13. McCollough, C. H. et al. Low-dose ct for the detection and classiﬁcation of metastatic liver lesions: Results of the 2016

low dose ct grand challenge. Med. physics 44, e339–e352 (2017).

14. Zbontar, J. et al. fastmri: An open dataset and benchmarks for accelerated mri. arXiv preprint arXiv:1811.08839 (2018).

15. Leuschner, J., Schmidt, M., Baguer, D. O. & Maaß, P. The lodopab-ct dataset: A benchmark dataset for low-dose ct

reconstruction methods. arXiv preprint arXiv:1910.01113 (2019).

16. Armato III, S. G. et al. The lung image database consortium (lidc) and image database resource initiative (idri): a completed

reference database of lung nodules on ct scans. Med. physics 38, 915–931 (2011).

17. Der Sarkissian, H. et al. A cone-beam x-ray computed tomography data collection designed for machine learning. Sci.

Data 6, 215, 10.1038/s41597-019-0235-y (2019).

9/21

18. Van De Looverbosch, T. et al. Nondestructive internal quality inspection of pear fruit by x-ray ct using machine learning.

Food Control. 113, 107170 (2020).

19. Coban, S. B., Lucka, F., Palenstijn, W. J., Van Loo, D. & Batenburg, K. J. Explorative imaging and its implementation at

the FleX-ray Laboratory. J. Imaging 6, 18, 10.3390/jimaging6040018 (2020).

20. Beer, A. "bestimmung der absorption des rothen lichts in farbigen ﬂüssigkeiten" [determination of the absorption of red
light in colored liquids] (in german). Annalen der Physik und Chemie 162, 78–88, 10.1002/andp.18521620505 (1852).

21. Lambert, J. "photometria sive de mensura et gradibus luminis, colorum et umbrae [photometry, or, on the measure
and gradations of light intensity, colors, and shade] (in latin). Augsburg, (Germany): Eberhardt Klett (1760). https:
//archive.org/details/TO0E039861_TO0324_PNI-2733_000000 [Last accessed 29-Jun-2020].

22. Radon, J. On the determination of functions from their integral values along certain manifolds. IEEE Transactions on Med.

Imaging 5, 170–176, https://doi.org/10.1109/TMI.1986.4307775 (1986).

23. Natterer, F. & Wübbeling, F. Mathematical Methods in Image Reconstruction (Society for Industrial and Applied

Mathematics, 2001).

24. Feldkamp, L., Davis, L. & Kress, J. Practical cone-beam algorithm. JOSA A 1, 612—619 (1984).

25. Jacobs, F., Sundermann, E., Sutter, B. D., Christiaens, M. & Lemahieu, I. A fast algorithm to calculate the exact radiological

path through a pixel or voxel space. J. Comput. Info. Technol. 6, 89–94, 10.1088/0031-9155/30/8/005 (1998).

26. Siddon, R. L. Prism representation: a 3d ray-tracing algorithm for radiotherapy applications. Phys Med Biol 30, 817–824,

10.1088/0031-9155/30/8/005 (1985).

27. Liu, T. Direct central ray determination in computed microtomography. Opt. Eng. 48, 1 – 6, 10.1117/1.3116707 (2009).

28. Coban, S. SophiaBeads Dataset Project Codes. Zenodo, 10.5281/zenodo.16539 (2015). DOI:10.5281/zenodo.16539.

http://sophilyplum.github.io/sophiabeads-datasets/; [Last accessed 10-Jun-2020].

29. Saad, Y. Iterative methods for sparse linear systems (Society for Industrial and Applied Mathematics, Philadelphia, PA,

2003), second edn.

30. Å. Björck, Elfving, T. & Strakos, Z. Stability of conjugate gradient and Lanczos methods for linear least squares problems.

SIAM J. Matrix Anal. Appl. 19(3), 720–736 (1998).

31. Coban, S., Lionheart, W. & Withers, P. Assessing the efﬁcacy of tomographic reconstruction methods through physical

quantiﬁcation techniques. Meas. Sci. Technol. (2021). (accepted; to appear).

32. van Aarle, W. et al. The ASTRA Toolbox: A platform for advanced algorithm development in electron tomography.

Ultramicroscopy 157, 35 – 47 (2015).

33. Van der Walt, S. et al. scikit-image: image processing in python. PeerJ 2, e453 (2014).

34. Adler, J., Kohr, H. & Öktem, O. Odl-a python framework for rapid prototyping in inverse problems. Royal Inst. Technol.

(2017).

35. Kaipio, J. & Somersalo, E. Statistical inverse problems: discretization, model reduction and inverse crimes. J. computational

applied mathematics 198, 493–504 (2007).

36. Oliver, B. Thermal and quantum noise. Proc. IEEE 53, 436–454 (1965).

37. Bhatia, N., Tisseur, D., Buyens, F. & Létang, J. M. Scattering correction using continuously thickness-adapted kernels.

NDT & E Int. 78, 52–60 (2016).

38. Agostinelli, S. et al. Geant4—a simulation toolkit. Nucl. instruments methods physics research section A: Accel.

Spectrometers, Detect. Assoc. Equip. 506, 250–303 (2003).

39. MATLAB. Version 9.8.0 (R2020a) Update 4 (The MathWorks Inc., Natick, Massachusetts, USA., 2020).

40. MATLAB. Documentation on randperm. https://uk.mathworks.com/help/matlab/ref/randperm.html (2011). [Last accessed

10-Jun-2020].

41. NumPy. NumPy v1.19 Manual: numpy.random.permutation. https://numpy.org/doc/stable/reference/random/generated/

numpy.random.permutation.html (2020). [Last accessed 29-Jun-2020].

42. Pia, A. D., Dey, S. S. & Molinaro, M. Mixed-integer quadratic programming is in np. Math. Program. 162, 225–240,

10.1007/s10107-016-1036-0 (2017).

43. Grant, M. & Boyd, S. CVX: Matlab software for disciplined convex programming, version 2.1. http://cvxr.com/cvx (2014).

10/21

44. Grant, M. & Boyd, S. Graph implementations for nonsmooth convex programs. In Blondel, V., Boyd, S. & Kimura, H. (eds.)
Recent Advances in Learning and Control, Lecture Notes in Control and Information Sciences, 95–110 (Springer-Verlag
Limited, 2008). http://stanford.edu/~boyd/graph_dcp.html.

45. ApS, M. The MOSEK optimization toolbox for MATLAB manual. Version 9.0. (2019).

46. Grant, M. & Boyd, S. CVX documentation on using mosek with cvx. http://cvxr.com/cvx/doc/mosek.html (2014). [Last

accessed 12-Nov-2020].

47. Diamond, S. & Boyd, S. CVXPY: A Python-embedded modeling language for convex optimization. J. Mach. Learn. Res.

17, 1–5 (2016).

48. Agrawal, A., Verschueren, R., Diamond, S. & Boyd, S. A rewriting system for convex optimization problems. J. Control.

Decis. 5, 42–60 (2018).

49. Domahidi, A., Chu, E. & Boyd, S. ECOS: An SOCP solver for embedded systems. In European Control Conference

(ECC), 3071–3076 (2013).

50. Grant, M. & Boyd, S. CVX documentation on solvers;controlling precision (2014). [Last accessed 12-Nov-2020].

51. ApS, M. Modeling cookbook 3.2.2 - mixed integer optimization. https://docs.mosek.com/modeling-cookbook/mio.html

(2020). [Last accessed 10-Jun-2020].

52. Nemirovski, A. Advances in convex optimization: Conic programming. In Proceedings of International Congress of

Mathematicians, vol. 1, 413–444 (European Mathematical Society Publishing House, 2006).

53. Kılınç-Karzan, F. & Yıldız, S. Two-term disjunctions on the second-order cone. Math. Program. 154, 463–491, 10.1007/

s10107-015-0903-4 (2015).

54. Coban, S. B., Ganguly, P. S. & Andriiashen, V. Simulated parallel beam tomographic dataset of 94 apples with defects,

10.5281/zenodo.4212301 (2020). http://doi.org/10.5281/zenodo.4212301. [Dataset].

55. Chen, H. et al. Low-dose ct via convolutional neural network. Biomed. optics express 8, 679–694 (2017).

56. Hamalainen, K. et al. Sparse tomography. SIAM J. on Sci. Comput. 35, B644–B665 (2013).

57. Ding, G., Liu, Y., Zhang, R. & Xin, H. L. A joint deep learning model to recover information and reduce artifacts in

missing-wedge sinograms for electron tomography and beyond. Sci. reports 9, 1–13 (2019).

58. Yang, W. et al. High-throughput measurement of rice tillers using a conveyor equipped with x-ray computed tomography.

Rev. Sci. Instruments 82, 025102 (2011).

59. Janssens, E. et al. Neural netwok based x-ray tomography for fast inspection of apples on a conveyor belt system. In 2015

IEEE International Conference on Image Processing (ICIP), 917–921 (IEEE, 2015).

60. Pelt, D. M., Batenburg, K. J. & Sethian, J. A. Improving tomographic reconstruction from limited data using mixed-scale

dense convolutional neural networks. J. Imaging 4, 128 (2018).

61. Coban, S. SophiaBeads Datasets Project Documentation and Tutorials. MIMS ePrints 26, 1–22 (2015).

Acknowledgements

Authors are grateful for the assistance of our industrial partners, GREEFA, especially Anna-Katharina Trull and Floris
Berendsen for providing the apple defect label ﬁles. S.B.C. would like to acknowledge the ﬁnancial support of the Netherlands
Organisation for Scientiﬁc Research (NWO; project number 639.073.506); P.S.G. the Marie Skłodowska-Curie Innovative
Training Network MUMMERING (grant agreement no. 765604) and M.v.E. and K.J.B. the ﬁnancial support by Holland High
Tech through the PPP allowance for research and development in the HTSM topsector.

Author contributions statement

Ground truth reconstructions were generated by S.B.C. Parallel beam data generation was performed by P.S.G. Scattering
was performed by V.A. Bias elimination methods were developed by S.B.C, V.A. and P.S.G. Technical validation results were
obtained by P.S.G, V.A. and M.v.E. All ﬁgures and tables were generated by S.B.C, P.S.G. and V.A. All authors have contributed
to text either by direct input or through comments.

Competing interests

The authors declare no competing interests, ﬁnancial or otherwise.

11/21

Figures & Tables

Sample Size

Successful Runs

Time (s)

Defect Percentage

Bitterpit

Holes

Rot

Browning

1e2

1e3

1e4

1e5

1e6

1e7

1e8

4

43

545

5418

54322

542670

0.0024

12.97% 35.04% 13.38% 20.02%

0.0291

22.01% 23.26% 17.31% 20.23%

0.060

19.24% 20.16% 16.72% 20.64%

0.4074

21.62% 19.79% 19.50% 20.10%

5.1885

20.00% 20.33% 20.86% 20.02%

410.3428

20.03% 20.23% 19.87% 20.46%

5431077

5236.5134

20.05% 20.05% 20.15% 20.02%

Table 1. The results of the test runs for the empirical data splitting method as the sample size is increased.

12/21

Empirical

MOSEK

ECOS_BB

Sequence

Apple No.

Sequence

Apple No.

Sequence

Apple No.

8

9

10

17

20

22

30

32

34

44

52

58

59

69

71

75

76

81

83

92

31108

31109

31110

31117

31120

31122

31208

31210

31212

31222

31308

31314

31315

32103

32105

32109

32110

32115

32117

32204

1

3

20

22

26

28

35

43

45

47

56

59

65

67

69

74

77

80

90

91

31101

31103

31120

31122

31204

31206

31213

31221

31301

31303

31312

31315

31321

32101

32103

32108

32111

32114

32202

32203

6

12

15

21

22

28

35

42

46

47

49

52

53

54

67

72

77

87

90

94

31106

31112

31115

31121

31122

31206

31213

31220

31302

31303

31305

31308

31309

31310

32101

32106

32111

32121

32202

32206

Table 2. Sequences and the corresponding apple numbers for the Test subset of Dataset A and B for each of the data splitting
method. The remaining apples are counted as part of the Training subset.

13/21

Empirical

MOSEK

ECOS_BB

Sequence

Apple No.

Sequence

Apple No.

Sequence

Apple No.

2

7

12

13

14

32

36

43

45

51

52

59

68

69

73

81

84

87

90

93

31102

31107

31112

31113

31114

31210

31214

31221

31301

31307

31308

31315

32102

32103

32107

32115

32118

32121

32202

32205

6

12

17

26

27

35

39

40

46

50

54

57

62

65

76

77

79

83

85

87

31106

31112

31117

31204

31205

31213

31217

31218

31302

31306

31310

31313

31318

31321

32110

32111

32113

32117

32119

32121

8

9

13

21

27

29

31

34

40

41

50

52

53

62

65

73

76

79

81

89

31108

31109

31113

31121

31205

31207

31209

31212

31218

31219

31306

31308

31309

31318

31321

32107

32110

32113

32115

32201

Table 3. Sequences and the corresponding apple numbers for the Test subset of Dataset C for each of the data splitting method.
The remaining apples are counted as part of the Training subset.

Dataset Name

Number of Apple Slices

Slice File Name

Dataset A

Dataset B

Dataset C

668 × 94 = 62792

data_appleNo_sliceNo.tif

668 × 94 = 62792

data_noisy_appleNo_sliceNo.tif

80 × 94 = 7520

data_appleNo_sliceNo.tif

Size

19GB

19GB

2GB

Ground Truth
Reconstructions

768 × 94 = 72192
(3D Volumes)

appleNo_sliceNo.tif

255GB

Table 4. Zenodo data upload information for each compressed folder. The datasets can be downloaded individually.

14/21

Dataset

Sampling

PSNR

SSIM

Name

Range

input

output

input

output

Dataset A

Dataset B

Dataset C

Sampling 1

15.1 ± 5.3

29.4 ± 3.1

0.29 ± 0.09

0.74 ± 0.04

Sampling 2

15.1 ± 5.3

29.4 ± 3.0

0.30 ± 0.09

0.75 ± 0.04

Sampling 1

1.6 ± 7.9

27.2 ± 3.1

0.01 ± 0.05

0.69 ± 0.05

Sampling 2

1.6 ± 7.9

27.5 ± 3.2

0.01 ± 0.05

0.72 ± 0.04

Sampling 1

-44.8 ± 0.7

28.8 ± 1.9

(3.5 ± 1.2)e−6

0.73 ± 0.02

Sampling 2

-44.8 ± 0.7

28.8 ± 1.9

(3.5 ± 1.1)e−6

0.75 ± 0.02

Table 5. Peak signal-to-noise (PSNR) and structural similarity index measure (SSIM) for the FBP reconstruction prior to
MS-D post-processing step (the input), and the MS-D network output after. The mean and standard deviation ﬁgures are based
on 15360 slices for Dataset A and B, and 1600 slices for Dataset C.

Figure 1. An illustration of the circular cone-beam CT geometry set up is shown on the left; example slices for the vertical
and horizontal directions in the reconstructed volumes on the right.

15/21

Figure 2. An example vertical volume slice is given on the left with high cone beam angle artefacts visible on top of the
reconstruction. The graph showing the total of grey-values per slice is given on the right. The red and orange bars highlight the
slices that were discarded (50 slices from top and bottom). The corresponding graph of grey-values for the ﬁrst 50 slices also
show the spike of high cone-beam artefact on top, and the bottom 50 the lack of any apple information. The middle 668 slices
were used for parallel beam simulations (Dataset A) and those with added Gaussian noise (Dataset B).

Figure 3. Parallel beam projection geometry used to generate Datasets A and B

16/21

Figure 4. The effect of scattering on an apple slice as the scattering intensity scaling factor, α is increased. Here, α = 0
signiﬁes no scattering, and α = 1 the scattering of water with no additional intensity.

Figure 5. A horizontal cross-section of a healthy apple is shown on the left, and another cross-section with browning defects
of the same apple shown on the right. The red arrows point to larger browning defects across the slice.

17/21

Figure 6. A bar chart that shows the defect percentage distribution for all defects within the Test subset. The percentage
distribution is determined using the resulting data split sequence obtained via the simple heuristic method (referred to as
"Empirical") and mixed-integer quadratic programming solvers MOSEK via MATLAB ("MOSEK") and ECOS_BB via
Python, ("ECOS_BB").

Figure 7. A bar chart that shows the defect percentage distribution for all defects within the Test subset. The percentage
distribution is determined using the resulting data split sequence obtained via the simple heuristic method (referred to as
"Empirical") and mixed-integer quadratic programming solvers MOSEK via MATLAB ("MOSEK") and ECOS_BB via
Python, ("ECOS_BB").

18/21

Figure 8. Different angular samplings of generated parallel beam data: (left to right) full angular range, missing wedge
sampling 1 and missing wedge sampling 2. Blue lines indicate the projection angles that were selected for the limited-view data
setup.

Figure 9. Baseline ﬁltered back-projection (FBP) reconstructions for full angular range and the two limited-view data setup.
The grey-values of the Dataset B and C reconstructions are set to the same range of Dataset A.

19/21

Figure 10. The target apple slices for the MS-D network.

Figure 11. MS-D post-processing results for the Target 1 shown in Figure 10. The FBP reconstructions used as inputs are
given for both missing wedge angular samplings for each data, and the resulting output by the network. The grey-values of the
Dataset B and C reconstructions are set to the same range of Dataset A.

20/21

Figure 12. MS-D post-processing results for the Target 2 shown in Figure 10. The FBP reconstructions used as inputs are
given for both missing wedge angular samplings for each data, and the resulting output by the network. The grey-values of the
Dataset B and C reconstructions are set to the same range of Dataset A.

21/21

