2
2
0
2

r
p
A
7

]

V
C
.
s
c
[

1
v
2
2
7
3
0
.
4
0
2
2
:
v
i
X
r
a

Automated Design of Salient Object Detection
Algorithms with Brain Programming

Gustavo Olaguea, Jose Armando Menendez-Clavijoa, Matthieu Olagued,
Arturo Ocampoc, Gerardo Ibarra-Vazquezb, Rocio Ochoae, Roberto Pinedaa

aEvoVisi´on Laboratory, CICESE Research Center. Carretera Ensenada-Tijuana 3918,
Zona Playitas, 22860, Ensenada, B.C. M´exico
bUniversidad Aut´onoma de San Luis Potos´ı, Facultad de Ingenier´ıa. Dr. Manuel Nava 8,
Col. Zona Universitaria Poniente, 78290, San Luis Potos´ı, S.L.P., M´exico
cUniversidad Nacional Aut´onoma de M´exico, Av Hacienda de Rancho Seco S/N, Impulsora
Popular Avicola, 57130 Nezahualc´oyotl, M´exico
dUniversidad Anahuac Quer´etaro, Calle Circuito Universidades I, Kil´ometro 7, Fracci´on 2,
El Marqu´es, Quer´etaro. C.P.76246, M´exico
eUniversidad Aut´onoma de Tlaxcala, Facultad de Ciencias B´asicas Ingenier´ıa y Tecnolog´ıa,
Carretera Apizaquito S/N, San Luis Apizaquito, C.P. 90401, Apizaco, Tlaxcala, M´exico

Abstract

Despite recent improvements in computer vision, artiﬁcial visual systems’ design

is still daunting since an explanation of visual computing algorithms remains

elusive. Salient object detection is one problem that is still open due to the

diﬃculty of understanding the brain’s inner workings. Progress on this research

area follows the traditional path of hand-made designs using neuroscience knowl-

edge. In recent years two diﬀerent approaches based on genetic programming

appear to enhance their technique. One follows the idea of combining previ-

ous hand-made methods through genetic programming and fuzzy logic. The

other approach consists of improving the inner computational structures of ba-

sic hand-made models through artiﬁcial evolution. This research work proposes

expanding the artiﬁcial dorsal stream using a recent proposal to solve salient

object detection problems. This approach uses the beneﬁts of the two main

aspects of this research area: ﬁxation prediction and detection of salient ob-

jects. We decided to apply the fusion of visual saliency and image segmentation

algorithms as a template. The proposed methodology discovers several critical

∗Corresponding author
Email address: olague@cicese.mx (Gustavo Olague)

Preprint submitted to Journal of LATEX Templates

April 11, 2022

 
 
 
 
 
 
structures in the template through artiﬁcial evolution. We present results on

a benchmark designed by experts with outstanding results in comparison with

the state-of-the-art.

Keywords: Visual Attention, Genetic Programming, Salient Object Detection

1. Introduction

Saliency is a property found in the animal kingdom whose purpose is to se-

lect the most prominent region on the ﬁeld of view. Elucidating the mechanism

of human attention including the learning of bottom-up and top-down processes

is of paramount importance for scientists working at the intersection of neuro-

science, computer science, and psychology. Giving a robot/machine this ability

will allow it to choose/diﬀerentiate the most relevant information. Learning the

algorithm for detecting and segmenting salient objects from natural scenes has

attracted great interest in computer vision and recently by people working with

genetic programming [1, 2]. While many models and applications have emerged,

a deep understanding of the inner workings remains lacking. This work develops

over a recent methodology that attempts to design brain-inspired models of the

visual system, including dorsal and ventral streams [3, 4]. The dorsal stream is

known as the “where” or “how” stream. This pathway is where the guidance of

actions and recognizing objects’ location in space is involved and where visual

attention occurs. The ventral stream is known as the “what” stream. This

pathway is mainly associated with object recognition and shape representation

tasks. This work deals with the optimization/improvement of an existing algo-

rithm (modeling the dorsal stream) and allows evolution to improve this initial

template method. The idea is to leverage the human designer with the whole

dorsal stream design’s responsibilities by focusing on the high-level concepts

while leaving the computer (genetic programming-GP) the laborious chore of

providing optimal variations to the template. Therefore, the human designer is

engaged in the more creative process of deﬁning a family of algorithms [5].

Figure 1 shows the template’s implementation (individual representation)

2

that emulates an artiﬁcial dorsal stream (ADS). As we can observe, the whole

algorithm represents a complex process based on two models. A neurophysiolog-

ical model called the two pathway cortical model–the two streams hypothesis–

and a psychological model called feature integration theory [6]. This last theory

states that human beings perform visual attention in two stages. The ﬁrst is

called the preattentive stage, where visual information is processed in parallel

over diﬀerent feature dimensions that compose the scene: color, orientation,

shape, intensity. The second stage, called focal attention, integrates the ex-

tracted features from the previous stage to highlight the scene’s region (salient

object). Hence, the image is decomposed into several dimensions to obtain a

set of conspicuity maps, which are then integrated–through a function known

as evolved feature integration (EFI)–into a single map called the saliency map.

Brain programming (BP) is based on the most popular theory of feature integra-

tion for the dorsal stream and the hierarchical representation of multiple layers

as in the ventral stream [7]. Note that the template’s design can be adapted

according to the visual task. In this work, we focus on designing an artiﬁcial

dorsal stream. Moreover, BP replaces the data-driven models with a function-

driven paradigm. In the function-driven process, a set of visual operators (V Os)

is fused by synthesis to describe the image’s properties to tackle object location

and recognition tasks.

This paper is organized as follows. First, we outline the related work brieﬂy

to highlight the research direction. Next, we detail the construction of the ADS

template using an adaptation of Graph-based Visual Saliency (GBVS) combined

with the Multiscale Combinatorial Grouping (MCG) to an evolutionary machine

learning algorithm. Then, we present the results of the evolutionary algorithm

to illustrate the beneﬁts of the new proposal. Finally, we ﬁnish the article with

our conclusions and future work on the automate design of brain models.

3

Color Image

Visual Operators
generated by the
evolutionary process

Color
VOC(Icolor)

Orientation
VOO(Icolor)

Shape
VOS(Icolor)

Intensity
VOInt=(Ir+Ig+Ib)
              3

Feature 
maps

Center
surround

Activation
maps

A

B

A

B

A

B

A

B

A

B

A

B

A

B

A

B

Conspicuity
maps

CMC

CMO

CMS

CMInt

Subsample

Feature 
extraction

Activation
Markov 
Chains

Normalization
Markov
Chains

Linear 
Combination

Figure 1: Brain programming implementation of the dorsal stream using the combination of

visual saliency and image segmentation algorithms. We propose to discover a set of visual
operators (V Os) and the evolutionary feature integration (EFI) within the template through

artiﬁcial evolution. The whole design makes a design balance between the human designer

and the computer.

4

2. Related Work

For a learning algorithm design technique to be well received, it needs to

solve several analysis levels. A major critique of deep learning is the opacity.

Scientists depend on complex computational systems that are often ineliminably

opaque, to the detriment of our ability to give scientiﬁc explanations and detect

artifacts. Here we follow a strategy for increasing transparency based on three

levels of explanation about what vision is and how it works and why we still

lack a general model, solution, or explanation for artiﬁcial vision [8]. The idea

follows a goal-oriented framework where learning is studied as an optimization

process [9]. The ﬁrst is theoretical transparency or knowledge of the visual

information processing whose design is the computation goal. The second is

algorithmic transparency or knowledge of visual processing coding. Finally, the

third level is execution transparency or knowledge of implementing the program

considering speciﬁc hardware and input data.

Visual attention has a long history, and we recommend the following recent

articles to the interested reader to learn more about the subject [10, 11, 12].

However, to really put it into practice, it is better to look for information about

benchmarks [13, 14, 15]. In the present work, we select Li et al. since it provides

an extensive evaluation of ﬁxation prediction and salient object segmentation

algorithms as well as statistics of major datasets. They provide a framework

focusing on the performance of GBVS against several state-of-the-art proposals.

The study also explains how to adapt ﬁxation prediction algorithms to salient

object detection by incorporating a segmentation stage. Fixation prediction

algorithms target at predicting where people look in images. Salient object de-

tection focus on a wide range of object-level computer vision applications. Since

ﬁxation prediction originated from cognitive and psychological communities, the

goal is to understand the biological mechanism. Salient object detection does

not necessarily need to understand biological phenomena.

Regarding the second level of explanation (algorithmic transparency) or the

knowledge of the visual processing coding. We can observe two diﬀerent ap-

5

proaches to incorporate learning into such study. The ﬁrst is exempliﬁed by a

deep learning technique (DHSNET–Deep Hierarchical Saliency Network) since

it is used as a building block in [2]. This method is a fully convolutional net-

work (FCN)-based method and it was designed to address the limitations of

multi-layer perceptron (MLP)-based methods [16]. FCN architectures leads to

end-to-end spatial saliency representation learning and fast saliency prediction,

within a single feed-forward process. FCN-based methods are now dominant in

the ﬁeld of computer vision.

The second methodology is represented by evolutionary computation ap-

plying genetic programming. We identify two representative works. In [2] the

contribution is oriented towards the automatic design of combination models by

using genetic programming. The proposed approach automatically selects the

algorithms to be combined and the combination operators uses as input a set of

candidate saliency detection methods and a set of combination operators. This

idea follows a long history in computer vision about combination models. To

achieve good results, authors rely on complex algorithms like DHSNET, using

it as building blocks to the detriment of transparency since the method does

not enhance the complex algorithms in the function set but only the output.

Since ﬁxation prediction algorithms are complex heuristics another alterna-

tive is to work directly with some key parts of the algorithm to attempt to im-

prove/discover the whole design. In [1] genetic programming serves to generate

visual attention models–ﬁxation prediction algorithms–to tackle salient object

segmentation. However, the authors took a step back, returning to the ﬁrst

stage–theoretical transparency–and revisited Koch et al. looking for a suitable

model susceptible to optimization [17]. They develop an optimization-based

approach to learn the complete model using a basic algorithm that serves the

purpose of a template. This algorithm uses as a foundation the code reported

in [18]. In this way, Dozal et al. attempt to fulﬁll the second stage–algorithmic

transparency–since they contemplate the diﬃculty of articulating the whole de-

sign exposed by Treisman and Gelade. To sum up, it is not easy to delegate

all practical aspects to the computer according to the genetic programming

6

paradigm. This way of looking for visual attention programs has already im-

pacted practical applications like visual tracking [19, 20]. This method searches

for new alternatives in the processes described in the feature integration theory

(FIT). That includes processes for the acquisition of visual features, the com-

putation of conspicuity maps, and the integration of features. Nevertheless, a

drawback is that the visual attention models are evolved to detect a particu-

lar/single object in the image.

In this work, we would like to identify all foreground regions and separate

them from the background. Note that the foreground can contain any object

on a particular database. This was the problem approached by Contreras et al.

and is known as salient object detection. The idea is to replace Itti’s algorithm

with the proposal published in [21] and the further adaptation and benchmark

described in [15].

Koch and coworkers adapt Treisman and Gelade’s theory into basic com-

putational reasoning. Itti’s algorithm accomplishes two stages 1) visual feature

acquisition and 2) feature integration. It consists of visual features extraction

and computation of visual and conspicuity maps, feature combination, and the

saliency map. GBVS is not diﬀerent from Itti’s implementation. However, it

makes a better description of the technique through Markov processes. The

idea is to adapt the GBVS algorithm to the symbolic framework of brain pro-

gramming. Figure 1 depicts the proposed algorithm where multiple functions

are discovered through artiﬁcial evolution. GBVS is a graph-based bottom-up

visual salience model. It consists of three steps: ﬁrst, extraction of features from

the image, second, creation of activation maps using the characteristic vectors,

and third, normalization of activation maps and combining these maps into a

master map. We adapt the algorithm described in [1] with the new proposal us-

ing four dimensions: color, orientation, shape, and intensity. In Koch’s original

work, there are three dimensions, each approached with a heuristic method, and

the same for the integration step. We apply the set of functions and terminals

provided in [22] with a few variants to discover optimal heuristic models for each

of these stages. This algorithm uses the Markov chains to generate the activation

7

maps. This approach is considered “organic” because, biologically, individual

“nodes” (neurons) exist in a connected, retinotopically organized network (the

visual cortex) and communicate with each other (synaptic activation) in a way

that results in emergent behavior, including quick decisions about which areas

of a scene require additional processing.

3. Methodology

BP aims to emulate the behavior of the brain through an evolutionary

paradigm using neuroscience knowledge for diﬀerent vision problems. The ﬁrst

jobs to introduce this technique [4, 1] focused on automating the design of visual

attention (VA) models and studied the way it surpasses previous human-made

systems developed by VA experts. To perceive salient visual features, the natu-

ral dorsal stream in the brain has developed VA as a skill through selectivity and

goal-driven behavior. The artiﬁcial dorsal stream (ADS) emulates this practice

by automating acquisition and integration steps. Handy applications for this

model are tracking objects from a video captured with a moving camera, as

shown in [19, 20].

BP is a long process consisting of several stages summarized in two central

ideas correlated with each other. First, the primary goal of BP is to discover

functions that are capable of optimizing complex models by adjusting the op-

erations within them. Second, a hierarchical structure inspired by the human

visual cortex uses function composition to extract features from images. It is

possible to adapt this model depending on the task at hand; e.g., the focus of at-

tention can be applied to saliency problems [1], or the complete artiﬁcial visual

cortex (AVC) can be used for categorization/classiﬁcation problems [4]. This

study uses the ADS, explained to a full extent in the following subsections, to

obtain as a ﬁnal result the design of optimal salient object detection programs

which satisfy the visual attention task.

8

Functions for EV OO

Terminals for EV OO

A + B, A

A + B

|
√A, k

A

−

,

|

−

B, A

B, A/B,

A
|
×
log2(A), A/2, A2,
|
A, A/k, A1/k, Ak, (1/k) + A,

−

B

A

,

|

|

,

×
(1/k), Gσ=1(A), Gσ=2(A),

Dx(A), Dy(A),

round(A),

,

A
⌉

inf (A, B), sup(A, B),

⌈
AttenuateBorders(A), ConvGabor(A)

,

A
⌊
⌋
thr(A),

Ir,

Ik,

Ig,

Ih,

Ib,

Ic,

Im,

Iy,

Is,

Iv, Dx(Icolor),

Dxx(Icolor),

Dyy(Icolor),

Dy(Icolor),

Dxy(Icolor),

AttenuateBorders(Icolor),

ConvGabor(Icolor),

Functions for EV OC

Terminals for EV OC

B, A

B, A/B, log2(A),
, A2, √A, (A)c, thr(A),

×

A, A/k, A1/k,

A + B, A

exp(A),

|
round(A),

−
A
|
A
⌋

⌊

,

⌈
Ak, (1/k) + A, A

−
Functions for EV OS

A
⌉

, k

×
(1/k)

A + B, A

B, A

B, A/B,

|
A/k, A1/k, Ak, (1/k) + A, A

×

−

, k

A
|

A,

×
(1/k),

−
SEd, A

, A

SEs,

round(A),

,

A
A
⌋
⌉
SEd, A

⌈

A

⊕

⊕
SEs, A

⌊
SEdm, A

⊕
SEdm,
Sk(A), P erim(A), A ⊛ SEd, A ⊛ SEs,
A ⊛ SEdm, That(A), Bhat(A), A ⊚ SEs,

⊖

⊖

⊖

A

⊙

SEs, thr(A)

Functions for EF I

A + B, A

B, A

−

A + B

,

A

|

|
|
Ak, (1/k) + A, A

−

|

B

, k

B, A/B,

A
|
A, A/k, A1/k,

,

|

×

×

(1/k),,Hist(A),

−
, thr(A), (A)2, √A,

round(A),

A
⌉
exp(A), Gσ=1(A), Gσ=2(A), Dx(A),

A
⌋

⌈

⌊

,

Ir, Ig, Ib, Ic, Im, Iy, Ik, Ih,

Is, Iv,DKLr, DKLΦ, DKLΘ,

Opr−g(I),Opb−y(I)

Terminals for EV OS

Ir, Ig, Ib, Ic, Im, Iy, Ik, Ih, Is,

Iv

Terminals for EF I

CMd, Dx(CMd), Dxx(CMd),

Dy(CMd),

Dxy(CMd)

Dyy(CMd),

Dy(A)

Table 1: Functions and terminals for the ADS.

9

3.1. Initialization

BP begins with a randomized generation, along an evolutionary process de-

ﬁned by a set of initialization variables such as population size, size of solutions

or individuals, or crossover-mutation probabilities. An individual represents a

computer program written with a group of syntactic trees embedded into hi-

erarchical structures. In this work, individuals within the population contain

functions corresponding to one of the four available visual operators (V O). Ta-

ble 1 shows the list of functions and terminals used for each V O or visual map

(V M ). The table includes arithmetic functions between two images A and B,

transcendental and square functions, square root function, image complement,

color opponencies (Red-Green and Blue-Yellow), dynamic threshold function,

arithmetic functions between an image A and a constant k.

It also includes

transcendental operations with a constant k and the spherical coordinates of

the DKL color space. The table also incorporates round, half, ﬂoor, and ceil

functions over an image A, dilation and erosion operators with the disk, square,

and diamond structure element (SE), skeleton operator over the image A, ﬁnd

the perimeter of objects in the image A, hit or miss transformation with the

disk, square, and diamond structures. Also, we include morphological top-hat

and bottom-hat ﬁltering over the image A, opening and closing morphological

operator on A, absolute value applied to A, and the addition and subtraction

operators. Finally, we add the inﬁmum and supremum functions between im-

ages A and B, the convolution of the image A, a Gaussian ﬁlter with σ = 1

or 2, and derivative of the image A along direction x and y. Note that all di-

mensions include elementary functions since these can be employed to compose

high-level properties (invariance to rotation, translation, scaling, and illumina-

tion ) through the usage of the template design.

3.2. Individual Representation

We represent individuals by using a set of functions for each V O deﬁned in

Section 3.3. Entities are encoded into a multi-tree architecture and optimized

through evolutionary operations of crossover and mutation.

10

The architecture uses four syntactic trees, one for each evolutionary visual

operator (EV OO, EV OC , EV OS) regarding orientation, color, and shape. We

then merge the CM s produced by the center-surround process–including feature

and activation maps–using an EFI tree, generating a saliency map (SM) as a

result. Section 3.3.1 provides details about the usage of these EV Os; addition-

ally, Figure 1 provides a graphical representation of the complete BP workﬂow.

After initializing the ﬁrst generation of individuals, the ﬁtness of each solution

is tested and used for creating a new population.

3.3. Artiﬁcial Dorsal Stream

The ADS models some components of the human visual cortex, where each

layer represents a function achieved by synthesis through a set of mathematical

operations; this constitutes a virtual bundle. We select visual features from the

image to build an abstract representation of the object of interest. Therefore,

the system looks for salient points (at diﬀerent dimensions) in the image to

construct a saliency map used in the detection process. The ADS comprises

two main stages: the ﬁrst acquires and transforms features in parallel that

highlight the object, while in the second stage, all integrated features serve the

goal of object detection.

3.3.1. Acquisition and Transformation of Features

In this stage, diﬀerent parts of the artiﬁcial brain automatically separate

basic features into dimensions. The entrance to the ADS is a color image I

deﬁned as the graph of a function as follows.

Deﬁnition 1. Image as the graph of a function. Let f be a function
R. The graph or image I of f is the subset of R3 that consist

R2

f : U

⊂

→

of the points (x, y, f (x, y)), in which the ordered pair (x, y) is a point in U and

f (x, y) is the value at that point. Symbolically, the image I =

R3

|

(x, y)

U

.

}

∈

(x, y, f (x, y))

{

∈

From this deﬁnition, we can highlight how images are variations in light

intensity along the two-dimensional plane of camera sensors. Regarding visual

11

processing for feature extraction of the input image, we consider multiple color

channels to build the set Icolor =

Ir, Ig, Ib, Ic, Im, Iy, Ik, Ih, Is, Iv}

{

, where each

element corresponds to the color components of the RGB (red, green, blue), HSV

(Hue, Saturation and Value) and CMYK (Cyan, Magenta, Yellow and black)

color spaces. We deﬁne the optimization process through the formulation of an

appropriate search space and evaluation functions.

3.3.2. Feature Dimensions

In this step, we obtain relevant characteristics from the image by decom-

posing it and analyzing key features. Three EV Os transform the input picture
Icolor through each V O deﬁned as EV Od : Icolor →
to emphasize speciﬁc characteristics of the object. Note that the fourth V MInt

V Md and applied in parallel

is not evolved and is calculated with the average of the RGB color bands. These

EV Os are operators generated in Section 3.2. Individuals-programs represent

possible conﬁgurations for feature extraction that describe input images and are

optimized using the evolutionary process. We perform these transformations to

recreate the process of extracting information following the FIT. When applying

each operator, a V M generated for each dimension represents a partial proce-

dure within the overall process. Each V M is a topographic map that represents,

in some way, an elementary characteristic of the image.

3.4. Creating the Activation Maps

After selecting the visual operators generated by the evolutionary process

created as follows. Suppose we are given a feature map M : [n]2

the feature maps complete the feature extraction. Next, activation maps are
R the
R such

goal is to compute an activation map for each dimension A : [n]2

→

that locations (i, j)

∈

[n]2 on the image, or as a proxy, M (i, j), is somehow

unusual in its neighborhood will correspond to high values of activation A. The

dissimilarity of M (i, j) and M (p, q) is given by:

d((i, j)

||

(p, q)) , log

M (i, j)
M (p, q)

.

(1)

12

→

3.5. A Markovian Approach

Now, consider a fully connected graph denoted as GA. For each node M

with its indexes (i, j)

∈

[n]2 connected to the other nodes. The edge point of

a node in the two-dimensional plane (i, j) to the node (p, q) will be the weight

and is deﬁned as follows:

w1((i, j), (p, q)) , d((i, j)
||

(p, q))

F (i

·

−

p, j

−

q) ,

where

F (a, b) , exp −

(a2 + b2)
2σ2

,

(2)

(3)

and σ is a free parameter of the algorithm. Thus, the weight of the edge from

node (i, j) to node (p, q) is proportional to their dissimilarity and their closeness

in the domain of M. It is possible then to deﬁne a Markov chain on GA by

normalizing the weights of the outbound edges of each node to 1, and drawing

an equivalence between nodes-states, and edges weights-transition probabilities.

3.6. Normalizing an Activation Map

This step is crucial to any saliency algorithm and remains a rich area of

study. GBVS proposes another Markovian algorithm and the goal of this step

is a mass-concentration in the activation maps. Authors construct a graph GN

with n2 nodes labeled with indices from [n]2. For each node (i, j) and (p, q)

connected, they introduce an edge from

w2((i, j), (p, q)) , A(p, q)

F (i

·

−

p, j

−

q) .

(4)

Once again, each node’s output edges are normalized to unity, and treating the

resulting graph as a Markov chain makes it possible to calculate the equilibrium

distribution over the nodes. The mass will ﬂow preferentially to those nodes with

high activation. The artiﬁcial evolutionary process works with the modiﬁed

version of GBVS. To improve the results, we can add the MCG during the

evolution or after since the image segmentation computational cost with this

algorithm is very high.

13

3.7. Genetic Operations

We follow the approach detailed in [1] where the template represents an

individual containing a set of V M s coded into an array of trees similar to a

chromosome and where each visual operator within the chromosome is a gene

code. In other words, the chromosome is a list of visual operators (V Os), and

each V O is a gene. Therefore, we apply four genetic operators:

• Chromosome-level crossover. The algorithm randomly selects a crossing

point from the list of trees. The process built a new oﬀspring by the union

of the left section of the ﬁrst parent with the right section of the second

parent.

• Gene level crossover: This operator selects two V Os, chosen randomly,

from a list of trees, and for each function of both trees (genes), the system

chooses a crossing point randomly, then the sub-trees under the crossing

point are exchanged to generate two new visual operators. Therefore, this

operation creates two new children-chromosomes.

• Chromosome-level mutation: The algorithm randomly selects a mutation

point within a parent’s chromosome and replaces the chosen operator com-

pletely with a randomly generated operator.

• Gene-level mutation: Within a visual operator, randomly chosen, the al-

gorithm selects a node, and the mutation operation randomly alters the

sub-tree that results below this point.

Once we generate the new population, the evolutionary process continues,

and we proceed to evaluate the new oﬀspring.

3.8. Evaluation Measures

Evolutionary algorithms usually apply a previously deﬁned ﬁtness function

to evaluate the individuals’ performance. BP designs algorithms using the gener-

ated EV Os to extract features from input images through the ADS hierarchical

14

structure depicted in Figure 1. Experts agree on the way to evaluate the var-

ious proposals for solutions to the problem of salient object detection. In this

work, we follow the protocol detailed with source code in [14] and apply two

main evaluation measures: Precision-Recall and F-measure. The ﬁrst is given

through Equation (5):

P recision = |

BM

∩
BM

|

, Recall = |

G
|

|

BM

∩
G
|

|

G
|

.

(5)

To compute a saliency map S, we convert it to a binary mask BM , and compute

P recision and Recall by comparing BM with ground-truth G. In this deﬁnition

binarization is a key step in the evaluation. The benchmark oﬀers a method

based on thresholds to generate a precision-recall curve. The second measure

(Equation (6)) is made with this information to obtain a ﬁgure of merit:

Fβ =

(1 + β2)P recision

Recall

β2P recision + Recall

×

.

(6)

This expression comprehensively evaluate the quality of a saliency map. The

F-measure is the weighted harmonic mean of precision and recall. In the bench-

mark β2 is set to 0.3 to increase the importance of the precision value.

We calculate both evaluations with two variants.

In our ﬁrst approach,

we obtain the maximum F-measure considering diﬀerent thresholds for each

image during the binarization process. Then, we calculate the average of all

photos in the training or testing set; see [1]. The second variant is the one used

in the benchmark, which consists of ﬁrst calculating the average that results

from varying the thresholds and then reporting the maximum resulting from

evaluating all the images. We will use both approaches during the experiments.

15

Parameters

Description

Generations

Population size

30

30

Initialization

Ramped half-and-half

Crossover at chromosome level

Crossover at gene level

Mutation at chromosome level

Mutation at gene level

0.8

0.8

0.2

0.2

Tree depth

Dynamic depth selection

Dynamic maximum depth

Maximum actual depth

7 levels

9 levels

Selection

Tournament selection

with lexicographic

parsimony pressure.

Elitism

Keep the best individual

Table 2: Main parameters settings of the BP algorithm.

4. Experiments and Results

Designing machine learning systems requires the deﬁnition of three diﬀerent

components: algorithm, data, and measure.

In this section, we evaluate the

proposed evolutionary algorithm with a standard test. Thus, the goal is to

benchmark our algorithm against external criteria. In this way, we need to run

a series of tests based on data and measures provided by well-known experts.

Finally, we contrast our results with several algorithms in the state-of-the-art.

In this research, we follow the protocol detailed in [15]. This benchmark is

of great help because it gives us the possibility of accessing the source code of

various algorithms to make a more exhaustive comparison. This benchmark also

analyzes blunt ﬂaws in the design of salience benchmarks known as database de-

sign bias produced by emphasizing stereotypical salience concepts. This bench-

16

mark makes an extensive evaluation of ﬁxation prediction and salient object

segmentation algorithms. We focus on the salient object detection part, con-

sisting of three databases FT, IMGSAL, and PASCAL-S. We present complete

results next. Also, we include a test of the best program with the databases

proposed in [2].

4.1. Image Databases

FT is a database with 800 images for training and 200 images for testing.

Authors of the benchmark reserve this last set of 200 images for comparison.

We use the training dataset to perform a k-fold technique (k = 5) to ﬁnd the

best individual. The training dataset is randomly partitioned into ﬁve subsets

of 160 images. A single subset of the k subsets is retained as the validation

data for testing the model, and the remaining k

1 subsets are used as training

−

data. Table 9 reports the best program results for 30 executions in the k-fold

technique. Each execution was run considering the parameters from Table 2.

We follow the same procedure in the PASCAL-S database, and the ﬁnal results

are given in Figure [23] while adding ocular ﬁxation information and object

segmentation labeling. The training set consists of 680 images, while the test

set consists of 170 images for comparison. We divide the training set into 5

subsets of 136 images to perform 5-fold cross-validation to discover the best

algorithm. Finally, we run the experiments on the IMGSAL dataset, which

contains 235 images. The database splits into 188 images for training and 47

images for testing. We select 185 out of the training set and split it into ﬁve

subsets of 37 images to perform 5-fold cross-validation.

FT contains a diverse representation of animate and inanimate objects with

image sizes ranging from 324

216 up to 400

×

×

300. PASCAL-S contains scenes

of domestic animals, persons, means of air and sea transport with image sizes

ranging from 200

300 up to 375

×

×

500. Finally, IMGSAL has wild animals

and ﬂora and diﬀerent objects and persons with image sizes of 480

640. All

×

images in the three datasets came with their corresponding ground truth. The

manual segmentation was carefully made in FT and PASCAL-S datasets to

17

obtain accurate ground truth, while IMGSAL provides a ground truth that was

purposefully segmented following a raw segmentation to illustrate a real case

where humans indicate an object’s location inaccurately.

4.2. Experiments with Dozal’s Fitness Function on GBVSBP

FT

Fold

Run 1

Run 2

Run 3

Run 4

Run 5

Run 6

Trng

Test

Trng

Test

Trng

Test

Trng

Test

Trng

Test

Trng

Test

75.34

76.10

75.07

73.97

77.57

76.75

76.98

75.29

77.34

75.74

77.08

75.36

74.88

71.60

72.92

69.81

75.10

72.97

74.65

71.27

73.62

71.87

76.41

72.25

78.18

76.58

76.69

71.77

77.03

72.97

76.75

72.18

76.68

72.39

77.01

72.21

75.30

77.05

74.78

74.25

74.90

71.47

75.86

77.67

74.60

76.13

77.03

74.98

75.15

74.54

73.14

73.45

74.74

74.97

74.62

74.17

75.18

75.06

76.09

74.37

1

2

3

4

5

Average

75,77

75.17

74,52

72.32

75,87

73.83

75,77

74.12

75,48

74.24

76,72

73.83

σ

1.36

2.21

1.54

1.97

1.33

2.05

1.12

2.54

1.52

1.97

0.45

1.51

Table 3: Performance of the best individuals of the GBVSBP model for all FT

database runs.

Table 3 details the experimental results after applying the k-fold cross-

validation method on the FT database. As we can observe, BP optimizes the

model scoring the highest value of 78.18% for the best program during training

at the forth run and the forth fold, while achieving 77.67% during testing also

in the ﬁrst run but at the fourth fold. σ remains low during the experimen-

tal runs. BP scores its highest value σ = 2.54 in the fourth testing run while

achieving σ = 0.45 and σ = 1.51 during the sixth run for training and testing,

respectively. On average, the methodology scores its highest ﬁtness of 76.72%

for training and 75.17% for testing. Table 4 presents the best solution. The

selected program corresponds to the training stage for this kind of table.

EV OO = Ik

EV OC = Ib + 1.00

EV Od and EF I operators

Fitness

Training = 0.7818

Testing = 0.7767

EV OS = top
−
EF I = (CMC )2

2

hat(Im x 0.31)

Table 4: Structure of the operators corresponding to the best solution for the

FT database.

18

IMGSAL

Fold

Run 1

Run 2

Run 3

Run 4

Run 5

Run 6

Trng

Test

Trng

Test

Trng

Test

Trng

Test

Trng

Test

Trng

Test

1

2

3

4

5

68.30

66.88

67.62

65.29

68.86

64.28

72.49

68.66

66.23

65.45

59.24

62.2

65.23

64.06

67.10

64.76

66.79

64.82

64.18

70.12

69.25

69.69

66.26

64.96

68.86

66.25

64.99

62.73

60.84

61.82

64.39

65.26

68.22

63.93

66.7

62.42

62.72

70.91

57.37

61.27

68.42

67.16

66.52

66.87

68.52

67.17

61.87

64.56

70.74

67.10

68.99

65.43

66.26

60.37

62.86

62.73

70.36

66.6

58.94

61.45

Average

67.17

67.04

65.21

63.90

66.23

63.69

66.09

66.73

68.52

66.57

σ

3.18

2.48

4.61

1.82

3.20

2.65

3.81

2.89

1.52

2.14

62.6

3.72

63.12

1.55

Table 5: Performance of the best individuals of the GBVSBP model for all

IMGSAL database runs.

Table 5 details the experimental results after applying the k-fold cross-

validation method on the IMGSAL database. The ﬁtness function results on this

dataset show high variability since, in training, the results range from 57.37%

in the second run–up to 70.74% in the ﬁrst run. The high dispersion is due

to the complexity of the problem generated by the poor manual segmentation.

The experiment shows an average oscillating between 62.60% and 68.52% for

training with σ = 3.72 and σ = 1.52 respectively. On the other hand, during

testing, the algorithm highest score in average is 67.04% with σ = 2.48. Table

6 reports the best solution.

EV OO = ((G(σ=1)(G(σ=1)(Iv))
EV OC = (I 0.44

)

c

EV Od and EF I operators

0.44)

−

−

0.69)

Fitness

Training = 0.7249

Testing = 0.7091

EV OS = Im

EF I = G(σ=2)(Dy(Dy(CMC )))

Table 6: Structure of the operators corresponding to the best solution for the

IMGSAL database.

19

PASCAL-S

Fold

Run 1

Run 2

Run 3

Run 4

Run 5

Run 6

Trng

Test

Trng

Test

Trng

Test

Trng

Test

Trng

Test

Trng

Test

63.58

62.31

62.92

61.83

65.19

65.33

62.27

61.34

62.84

63.12

63.79

62.25

61.53

58.92

64.70

57.51

63.02

56.88

64.64

59.80

65.29

58.81

64.50

58.57

61.58

64.61

63.47

65.46

62.18

64.13

63.71

65.22

63.20

67.66

59.71

64.90

59.60

62.82

58.20

59.77

59.23

60.87

60.45

63.68

59.72

61.78

59.84

60.63

63.20

65.81

64.46

63.25

63.90

63.47

66.03

64.55

66.22

66.79

66.39

66.78

1

2

3

4

5

Average

61.90

62.89

62.75

61.56

62.70

62.14

63.42

62.92

63.45

63.63

62.85

62.83

σ

1.58

2.63

2.64

3.07

2.24

3.36

2.15

2.28

2.52

3.64

2.96

2.99

Table 7: Performance of the best individuals of the GBVSBP model for all

PASCAL-S database runs.

The experimental results of the GBVSBP Model with the PASCAL-S database

show excellent stability as seen in Table 7 with a low standard deviation, espe-

cially in training with the ﬁrst run-scoring 1.58. During training, ﬁtness reaches

highest in the ﬁfth fold of run 6, scoring 66.39%, while on average, the ﬁfth run

scores ﬁrst place with 63.45%. Regarding the testing stage, the algorithm scores

the best individual at the ﬁfth fold with a 67.66%, and on average, the best run

was the ﬁfth with 63.63%. Table 8 presents the best set of trees.

EV OO =
⌊
EV OC = (((((Im −
EV OS = Im

(G(σ=1)(Dy(Is)))
⌋
0.62) + Iy) + (Iy + Iv)) + (Iy + Iv))

0.62)

−

EV Od and EF I operators

Fitness

Training = 0.6639

Testing = 0.6766

EF I = Dy(Dy(CMC ))

Table 8: Structure of the operators corresponding to the best solution for the

PASCAL-S database.

The experiment with our second model GBVSBP+MCG and the FT database

shows outstanding results compared to the previous model, see Table 9. Another

remarkable diﬀerence is the stability during training regarding the standard de-

viation, whose performance descend a little while testing the best models, where

four values score above σ = 3. Meanwhile, in the testing stage, the best indi-

vidual achieves 95.06% in the second run. On average, the algorithm discovered

the best individuals considering all folds in the ﬁrst run with 93.47%, while the

20

FT database

Fold

Run 1

Run 2

Run 3

Run 4

Run 5

Run 6

Trng

Test

Trng

Test

Trng

Test

Trng

Test

Trng

Test

Trng

Test

1

2

3

4

5

93.39

92.90

93.62

93.09

93.67

93.35

93.29

94.14

93.55

92.19

93.27

93.44

94.53

86.67

94.02

86.12

93.75

84.61

93.73

86.67

93.80

86.84

93.74

85.38

92.84

93.55

92.82

92.76

92.69

92.22

93.05

94.36

92.45

92.78

92.63

92.90

93.33

92.77

92.86

93.64

92.76

93.54

92.98

91.59

92.87

93.89

93.23

90.62

93.25

92.86

93.45

95.06

93.02

94.31

93.20

92.22

93.03

93.31

93.00

92.18

Average

93.47

91.75

93.35

92.13

93.18

91.61

93.25

91.80

93.14

91.80

93.17

90.90

σ

0.63

2.86

0.51

3.48

0.50

3.98

0.29

3.11

0.54

2.84

0.41

3.26

Table 9: Performance of the best individuals of the GBVSBP + MCG model

for all FT database runs.

second run reports the best results with an average of 92.13%. Table 10 shows

the best solution.

EV Od and EF I operators

EV OO = Gσ=1(Im)

EV OC = Complement(Complement(DKLr))

EV OS = ((bottom

hat(Ib) x (bottom

hat(Ib))

−

−

EF I =

(G(σ=1)(CMMM ) x 0.63) + ((Dy(Dy(CMC ))
|

−

Dy(Dy(CMMM )))

Dy(Dy(CMMM )))
|

−

Fitness

Training = 0.9453

Testing = 0.9506

Table 10: Structure of the operators corresponding to the best solution for the

FT database.

4.3. Experiments with Benchmark’s Score

As the second round of experiments, we adapt the algorithm to use as a ﬁt-

ness function the proposed benchmark’s score as explained earlier, see Section

3.8. From now on, all report experiments considered this way of evaluation.

Table 11 provides the results of the k-fold experimentation considering the GB-

VSBP algorithm with the FT database, while Table 17 provides partial results

with the GBVSBP+MCG model to illustrate the performance. As can be seen,

there is a decrease, but the ranking remains unaltered, as we veriﬁed with the

solutions. The results considering the datasets IMGSAL and PASCAL-S are in

Tables 13 and 15, respectively.

21

FT

Fold

Run 1

Run 2

Run 3

Run 4

Run 5

Run 6

Trng

Test

Trng

Test

Trng

Test

Trng

Test

Trng

Test

Trng

Test

1

2

3

4

5

71.27

71.94

70.23

70.39

72.04

72.23

70.73

70.99

71.22

70.80

72.59

73.65

70.00

63.05

67.04

65.73

68.66

68.86

67.57

69.01

69.5

66.58

68.18

67.78

71.71

64.82

71.81

67.92

71.16

68.15

71.22

64.15

72.14

68.02

71.08

66.86

69.23

73.45

70.69

66.13

71.32

68.11

68.15

73.81

68.77

71.13

70.87

69.42

68.08

67.68

68.52

67.20

68.88

66.03

69.38

64.25

69.17

68.19

68.73

67.59

Average

70.06

68,19

69.66

67,47

70.41

68,68

69.41

68,44

70.07

68,94

70.29

69,06

σ

1.48

4,47

1.88

1,85

1.54

2,25

1.58

4,23

1.51

1,95

1.81

2,73

Table 11: Performance of the best individuals of the GBVSBP model for all FT

database runs.

With this new F-measure, the results obtained show greater stability globally

according to Table 11 despite runs 1 and 4 reporting σ = 4.47 and σ = 4, 23.

All other values are below 2%. The ﬁtness of the individuals, despite that a

decrease is observed, remains with competitive results as the best individual

in the training stage achieves 72.59%, and the best in the test stage reaches

73.81%. In this experiment, the highest average ﬁtness was reﬂected in the last

run considering all folds and for both stages with values of 70.29% and 69.06%.

Table 12 gives the best set of visual operators.

EV Od and EF I operators

EV OO = Ir

EV OC = (((Ik + Im) + Im) + Im)

EV OS = threshold(Iy)

EF I = Dx(Dy(CMS))

Fitness

Training = 0.7259

Testing = 0.7381

Table 12: Structure of the operators corresponding to the best solution for the

FT database.

22

IMGSAL

Fold

Run 1

Run 2

Run 3

Run 4

Run 5

Run 6

Trng

Test

Trng

Test

Trng

Test

Trng

Test

Trng

Test

Trng

Test

1

2

3

4

5

57.19

60.16

60.09

60.17

65.35

63.45

62.05

60.31

65.09

62.20

60.24

60.8

56.12

59.94

55.55

60.25

63.47

63.34

63.94

63.44

60.41

60.51

59.84

60.55

57.56

55.90

57.96

57.96

63.78

61.93

63.54

59.49

57.45

56.34

56.97

60.30

57.44

59.83

57.37

58.09

64.21

62.94

60.03

62.23

61.60

56.69

59.87

62.26

59.49

58.57

57.41

59.30

62.01

60.54

62.86

62.13

58.28

57.48

62.08

57.26

Average

57.56

58.88

57.68

59.15

63.76

62.44

62.48

61.52

60.56

58.64

59.8

60.23

σ

1.22

1.78

1.63

1.10

1.21

1.22

1.55

1.59

3.02

2.58

1.83

1.83

Table 13: Performance of the best individuals of the GBVSBP model for all

IMGSAL database runs.

Fitness results with the GBVSBP model and IMGSAL database show very

similar behavior as we appreciate in outcomes. For example, the highest average

was 63.76% in the training stage with a σ = 1.21, while reaching a lowest average

of 58.64% with a σ = 2.58 in the testing stage. As we can appreciate, the results

were not as high as the other two datasets since IMGSAL presents diﬃculties

primarily due to poor segmentation when creating the ground truth. Also, the

results took a long time to be completed (several months) due to the bigger

image size. The algorithm reached the best solution on the third run-scoring

65.35% in training and 63.45% at testing. Table 14 provides the best solution.

EV OO = G(σ=1)(Iv)

EV OC = p(Im)

EV OS = Im

EF I =

G(σ=1)(Dy(CMS))
|
|

EV Od and EF I operators

Fitness

Training = 0.6535

Testing = 0.6345

Table 14: Structure of the operators corresponding to the best solution for the

IMGSAL database.

23

PASCAL-S

Fold

Run 1

Run 2

Run 3

Run 4

Run 5

Run 6

Trng

Test

Trng

Test

Trng

Test

Trng

Test

Trng

Test

Trng

Test

1

2

3

4

5

62.03

59.75

61.39

57.53

61.88

57.98

60.76

56.06

61.62

58.75

60.89

58.19

61.53

55.12

61.95

54.99

61.11

56.52

61.33

52.61

61.23

53.64

61.01

56.70

59.28

58.83

60.08

61.08

59.84

60.32

60.60

59.57

59.82

61.33

59.71

61.78

59.57

58.36

58.20

54.28

58.59

55.91

58.61

58.90

58.21

57.13

57.89

56.44

63.20

61.40

61.94

61.94

63.08

59.12

63.38

59.01

62.93

59.00

62.87

61.34

Average

61.12

58.69

60.71

57.96

60.90

57.97

60.94

57.23

60.76

57.97

60.47

58.87

σ

1.67

2.31

1.60

3.47

1.75

1.82

1.71

2.92

1.81

2.85

1.83

2.54

Table 15: Performance of the best individuals of the GBVSBP model for all

PASCAL-S database runs.

The experimental results with GBVSBP for the PASCAL-S database have

higher similarity because the F-measure has greater stability than previous re-

sults. The results show a stable standard deviation maintained between 1.60

and 3.47 for training and testing. We obtain similar behavior in the average

results between 60.47% and 61.12% for the training stage. The best solution

was reached in the ﬁfth fold at the fourth run-scoring 63.38% during training,

while in testing, the algorithm scored 61.94%. Table 16 shows the best trees.

EV Od and EF I operators

EV OO = G(σ=1)(G(σ=1)(Iv))

EV OC = Im

EV OS =

(dilationdisk(dilationsquare(erosiondisk(Ib))))
⌋

⌊

EF I =

G(σ=2)(Dy(CMMM ))
|
|

Fitness

Training = 0.6338

Testing = 0.6194

Table 16: Structure of the operators corresponding to the best solution for the

PASCAL-S database.

24

FT

Fold

Run 1

Run 2

Run 3

Run 4

Run 5

Run 6

Trng

Test

Trng

Test

Trng

Test

Trng

Test

Trng

Test

Trng

Test

1

88.41

88.51

88.07

86.00

87.92

80.83

88.32

85.75

87.52

89.02

87.04

86.69

Run 1

Average

σ

Minimum

Maximum

Mean

Trng

Test

Trng

Test

Trng

Test

Trng

Test

Trng

Test

Trng

Test

2

89.02

88.56

88.24

86.53

0.56

3.75

87.04

80.83

89.09

89.02

88.07

86.69

Table 17: Performance of the best individuals of the GBVSBP + MCG model

for all FT database runs.

The experimental results with GBVSBP + MCG for the FT database show

some loss of aptitude against its counterpart in the ﬁrst block of experiments

(5% in training and 3% in tests). However, the performance values are more

stable with σ = 0.56 for training and σ = 0.49 for testing.

In addition to

stability, we must bear in mind that the results of this experiment will be much

more consistent when the best individual is tested in the benchmark since we

are using the same F-measure. As a result, the best results were 89.02% in

training corresponding to the second run of the ﬁrst fold and 89.02% at testing

discovered in the ﬁrst run of the ﬁfth fold. Table 18 gives the best set of trees.

EV OO = G(σ=1)(Dy(Iy))
EV OC = ((DKL1/0.62

)

Φ

−

EV Od and EF I operators

(((Exp(Ib)1/0.62)1/0.62)

((DKLΦ −

−

DKLΦ)

(Ib −

−

Ib))))

Fitness

Training = 0.8902

Testing = 0.8902

EV OS = Iy

EF I = CMMM

Table 18: Structure of the operators corresponding to the best solution for the

FT database. Note that we can simplify the second tree. However, we report

the programs as returned by the computer.

4.4. Analysis of the Best Evolutionary Run

Typical experimental results that illustrate the inner workings of genetic

programming are those related to ﬁtness, diversity, number of nodes, and depth

of the tree. Figure 2 provides charts giving best ﬁtness, average ﬁtness, and

median ﬁtness. The purpose is to detail the performance and complexity of

25

solutions through the whole evolutionary run. As we can observe, artiﬁcial

evolution scores a high ﬁtness within the ﬁrst generations. On average, BP

converges around the seventh generation. The chart depicting diversity shows

the convergence of solutions in all of the four trees characterizing the program.

Compared to the ﬁtness plot, these data demonstrate that despite the diﬀer-

ences in diversity that occurred during the experiment, the model’s performance

remained constant. One of the biggest problems using genetic programming is

incrementing a program’s size without a rise in the program’s performance,

mainly when the ﬁnal result cannot generalize to new data. This problem is

called bloat and is usually associated with tree representation. As observed in

the last two graphs, the complexity is kept low with the number of nodes below

seven and depth below ﬁve regarding all trees. These numbers were consis-

tently below the proposed setup for all experiments. The hierarchical structure

allows an improvement in performance and the management of the algorithm’s

complexity.

4.5. Comparison with Other Approaches

Saliency Model

Score (F-measure)

GBVSBP + MCG

86.72

SF ([24])

GBVS + MCG ([15])

PCAS ([25])

GC ([26])

DHSNET ([2])

FT ([27])

GBVSBP

GBVS ([15])

FOA ([1])

85.38

85.33

83.93

80.64

74.06

71.23

69.08

65.25

60.05

Table 19: Comparison using the benchmark with other algorithms in the FT

database ([15]).

26

Figure 2: Brain programming statistics of the run corresponding to the best GBVSBP model

for the FT database.

The values presented above correspond to ﬁtness in the evolutionary cycle

of BP. The benchmark oﬀers two modalities: one which uses only 60% of the

database, and another containing all images; we keep the ﬁrst option. Table

19 shows ﬁnal results achieved on the testing set over 10 random splits with

our best program considering the FT dataset. Here, we appreciate ﬁnal results

considering the following algorithms for salient object detection: FT–Frequency-

tuned, GC–Global Contrast, SF–Saliency Filters, PCAS–Principal Component

Analysis, and DHSNET. Also, we include the original proposal of the artiﬁcial

dorsal stream named focus of attention (FOA) reported in [1]. Note that we

overpass all other algorithms in the benchmark. Figure 3 presents image results

of all algorithms in the three databases for visual comparison. Figure 4 provides

Precision-Recall curves for the FT, IMGSAL, and PASCAL-S databases of the

benchmark. Again we score highest in the IMGSAL and PASCAL-S datasets.

27

FT

PASCAL-S

IMGSAL

RGB

GBVSBP

MCG

SF

GBVS

MCG

PCAS

GC

FT

GBVSBP

GBVS

GT

RGB

GBVSBP

GBVS

MCG

MCG

PCAS

GC

GBVSBP

GBVS

SF

FT

GT

RGB

GBVSBP GBVS

PCAS

GC

GBVSBP

GBVS

MCG

MCG

SF

FT

GT

Figure 3: Example of outstanding maps obtained in experiments where our models observe

better performance.

Dataset

FT

HC MDC MBS

MIN

MAX

MSRA-BTest

0.5319

0.5663

0.7241

0.7116

0.6762

ECSSD

0.3775

0.3894

0.6127

0.5570

0.5177

SED2

iCoseg

0.6250

0.6079

0.6074

0.6654

0.6833

0.5545

0.5471

0.6348

0.6253

0.6210

0.6105

0.4344

0.6106

0.5587

AVG

0.6751

0.4994

0.6971

0.6105

Dataset

TOP2 CPSO GA

PSO GPMCC GPSED GBVSBP+MCG

MSRA-BTest

0.7423

0.7137

0.7579

0.7455

0.7711

ECSSD

0.5979

0.5588

0.6200

0.5988

0.6592

0.7662

0.6592

SED2

iCoseg

0.6658

0.6653

0.6914

0.6701

0.7148

0.7340

0.6556

0.6235

0.6678

0.6557

0.6865

0.7157

0.8308

0.7591

0.6919

0.7168

Table 20: Comparison with others models and databases published in [2].

28

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.2

0.4

0.6

0.8

1

Figure 4: The precision-Recall curve for FT, PASCAL-S, and IMGSAL datasets using the

following algorithms: GBVSBP + MCG, SF, GBVS + MCG, PCAS, GC, FT, GBVSBP, and

GBVS.

29

Note that even if the computer model is symbolic, the interpretation remains

numeric, and therefore exists computer errors. Anyway, the computation is

data-independent since the proposal follows a function-driven paradigm.

Finally, we test our best solution (GBVSBP + MCG) on four databases

studied in [2], and the results are in Table 20. The best solution designed by

Contreras-Cruz et al. was trained with MSRA-A and then it was tested with

MSRA-BTest, ECSSD, SED2, and iCoseg. We observe that we score highest on

three datasets MSRA-BTest, ECSSD, and iCoseg while achieving competitive

results on SED2. We provide such comparison since [2] does not test their

algorithms with the benchmark protocol. Therefore it is hard to make a clear

comparison between both approaches, and the results we provide here serve the

purpose of illustrating the methodologies’ performance. Figure 5 illustrates the

image processing through the whole GBVSBP+MCG program.

5. Conclusions

In this work, we propose a method to improve the ADS model presented by

[1]. This method consists of applying an algorithm called GBVS that surpasses

Itti’s previous model. GBVS uses a graph-based approach using Markov chains

while involving the same stages as the Itti model. Moreover, we follow the

idea of combining ﬁxation prediction with a segmentation algorithm to obtain

a new method called GBVSBP + MCG to tackle the problem of salient object

segmentation. As we show in the experiments, the novel design scores highest

in the FT, IMGSAL, and PASCAL-S datasets of a benchmark provided by [15].

These tests show the strength and generalization power of the discovered model

compared to others developed manually and current CNNs such as DHSNET,

surpassed by more than 12 percentage points in FT. Also, we give results on

four datasets described in [2] with outstanding results. The results are revealing

about the diﬃculty of solving this visual task.

In the FT and PASCAL-S,

objects are deﬁned with accurate ground truth, and the algorithm GBVSBP

+ MCG improves the results of the original algorithm slightly. However, in

30

Visual operators
generated by the 
evolutionary process

VOC

VOO

VOS

VOInt

A

B

A

B

A

B

A

B

Feature 
maps

Center
surround

Activation 
Maps

A

B

A

B

A

B

A

B

Normalized 
Maps

CMC

CMO

CMS

CMInt

Subsampling

Feature 
Extraction

Activation
Markov 
Chains

Normalization
Markov 
Chains

Linear 
Combination

EFI

Master Map

Saliency Map

MCG Candidates

Salient Object

Figure 5: Brain programming result of the dorsal stream using the best GBVS+MCG program.

31

                    
the IMGSAL database, the ground truth is poorly segmented, and GBVSBP

signiﬁcantly improves the score compared to the original proposal. Therefore,

we can say that there is a considerable beneﬁt in combining analytical methods

with heuristic approaches. We believe that this mixture of strategies can help

ﬁnd solutions to challenging problems in visual computing and beyond. One

advantage is that the overall process and ﬁnal designs are explainable, which is

considered a hot topic in today’s artiﬁcial intelligence. This research attempts

to advance studies conducted by experts (neuroscientists, psychologists, and

computer scientists) by adapting the symbolic paradigm for machine learning

to ﬁnd better ways of describing the brain’s inner workings.

References

[1] L. Dozal, G. Olague, E. Clemente, D. E. Hern´andez, Brain programming

for the evolution of an artiﬁcial dorsal stream, Cognitive Computation 6

(2014) 528–557. doi:10.1007/s12559-014-9251-6.

[2] M. A. Contreras-Cruz, D. E. Martinez-Rodriguez, U. H. Hernandez-

Belmonte, V. Ayala-Ramirez, A genetic programming framework in

the automatic design of combination models for salient object detec-

tion, Genetic Programming and Evolvable Machines 20 (2019) 285–325.

doi:10.1007/s10710-019-09345-5.

[3] E. Clemente, G. Olague, L. Dozal, M. Mancilla, Object recognition with

an optimized ventral stream model using genetic programming, in: Lecture

Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial

Intelligence and Lecture Notes in Bioinformatics), Vol. 7248 LNCS, 2012,

pp. 315–325. doi:10.1007/978-3-642-29178-4_32.

[4] G. Olague, E. Clemente, D. E. Hernandez, A. Barrera, M. Chan-

Ley,

S. Bakshi, Artiﬁcial

visual

cortex

and

random search

for

object

categorization,

IEEE Access

7

(2019)

54054–54072.

doi:10.1109/ACCESS.2019.2912792.

32

[5] J.

Rojas-Quintero,

M.

Rodr´ıguez-Li˜n´an,

A literature review of sensor heads for humanoid robots,

Robotics

and

Autonomous

Systems

143

(2021)

103834.

doi:https://doi.org/10.1016/j.robot.2021.103834.

URL https://www.sciencedirect.com/science/article/pii/S0921889021001196

[6] A. M.

Treisman,

G. Gelade,

A feature-integration

the-

ory

of

attention,

Cognitive

Psychology

12

(1980)

97–136.

doi:10.1016/0010-0285(80)90005-5.

[7] A. Khan, A. S. Qureshi, N. Wahab, M. Hussain, M. Y.

Hamza, A recent

survey on the applications of genetic program-

ming in image processing, Computational

Intelligence

(2021) 1–

34.doi:https://doi.org/10.1111/coin.12459.

[8] K. A. Creel, Transparency in complex computational systems, Philosophy

of Science 87 (2020) 1–37. doi:10.1086/709729.

[9] G. Olague, Evolutionary Computer Vision: The First Footprints, Springer,

2016. doi:10.1007/978-3-662-43693-6.

[10] N. Li, H. Bi, Z. Zhang, X. Kong, D. Lu, Performance compari-

son of saliency detection, Advances in Multimedia 2018 (2018) 1–13.

doi:10.1155/2018/9497083.

[11] A.

Borji, M.-M.

Cheng,

Q. Hou,

H.

Jiang,

J.

Li,

Salient object detection: A survey, Computational Visual Media 5 (2)

(2019) 117–150. doi:10.1007/s41095-019-0149-9.

URL https://doi.org/10.1007/s41095-019-0149-9

[12] W. Wang, Q. Lai, H. Fu, J. Shen, H. Ling, R. Yang, Salient object

detection in the deep learning era: An in-depth survey, IEEE Trans-

actions on Pattern Analysis & Machine Intelligence (01) (2021) 1–1.

doi:10.1109/TPAMI.2021.3051099.

33

[13] A. Borji, D. N. Sihite, L. Itti, Salient object detection: A benchmark,

in: Lecture Notes in Computer Science (including subseries Lecture Notes

in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics), Vol. 7573

LNCS, 2012, pp. 414–429. doi:10.1007/978-3-642-33709-3_30.

[14] A. Borji, M. Cheng, H. Jiang, J. Li, Salient object detection: A bench-

mark, IEEE Transactions on Image Processing 24 (12) (2015) 5706–5722.

doi:10.1109/TIP.2015.2487833.

[15] Y. Li, X. Hou, C. Koch, J. M. Rehg, A. L. Yuille, The secrets of salient

object segmentation, in: Proceedings of the IEEE Computer Society Con-

ference on Computer Vision and Pattern Recognition, 2014, pp. 280–287.

arXiv:1406.2807, doi:10.1109/CVPR.2014.43.

[16] N. Liu, J. Han, Dhsnet: Deep hierarchical saliency network for salient ob-

ject detection, in: 2016 IEEE Conference on Computer Vision and Pattern

Recognition (CVPR), 2016, pp. 678–686. doi:10.1109/CVPR.2016.80.

[17] C. Koch, S. Ullman, Shifts in selective visual attention: Towards the un-

derlying neural circuitry, in: Human Neurobiology, Vol. 4, Springer, 1985,

pp. 219–227. doi:10.1007/978-94-009-3833-5_5.

[18] L. Itti, C. Koch, E. Niebur, A model of saliency-based visual attention for

rapid scene analysis, IEEE Transactions on Pattern Analysis and Machine

Intelligence 20 (1998) 1254–1259. doi:10.1109/34.730558.

[19] G. Olague, D. E. Hernandez, E. Clemente, M. Chan-Ley, Evolving head

tracking routines with brain programming, IEEE Access 6 (2018) 26254–

26270. doi:10.1109/ACCESS.2018.2831633.

[20] G. Olague, D. E. Hern´andez, P. Llamas, E. Clemente, J. L. Brise˜no, Brain

programming as a new strategy to create visual routines for object track-

ing: Towards automation of video tracking design, Multimedia Tools and

Applications 78 (2019) 5881–5918. doi:10.1007/s11042-018-6634-9.

34

[21] J. Harel, C. Koch, P. Perona, Graph-based visual saliency,

in: Ad-

vances in Neural Information Processing Systems, 2007, pp. 545–552.

doi:10.7551/mitpress/7503.003.0073.

[22] E. Clemente, F. Chavez, F. Fernandez De Vega, G. Olague, Self-adjusting

focus of attention in combination with a genetic fuzzy system for improving

a laser environment control device system, Applied Soft Computing Journal

32 (2015) 250–265. doi:10.1016/j.asoc.2015.03.011.

[23] M. Everingham, L. Van Gool, C. K.

I. Williams,

J. Winn,

A.

Zisserman,

The pascal visual object classes (voc) challenge,

In-

ternational Journal of Computer Vision 88 (2)

(2010) 303–338.

doi:10.1007/s11263-009-0275-4.

URL https://doi.org/10.1007/s11263-009-0275-4

[24] F. Perazzi, P. Krahenbuhl, Y. Pritch, A. Hornung, Saliency ﬁlters: Con-

trast based ﬁltering for salient region detection, in: Proceedings of the IEEE

Computer Society Conference on Computer Vision and Pattern Recogni-

tion, 2012, pp. 733–740. doi:10.1109/CVPR.2012.6247743.

[25] R. Margolin, A. Tal, L. Zelnik-Manor, What makes a patch distinct?,

in: 2013 IEEE Conference on Computer Vision and Pattern Recognition,

IEEE, 2013, pp. 1139–1146. doi:10.1109/CVPR.2013.151.

[26] M. M. Cheng, G. X. Zhang, N. J. Mitra, X. Huang, S. M. Hu, Global con-

trast based salient region detection, in: Proceedings of the IEEE Computer

Society Conference on Computer Vision and Pattern Recognition, 2011, pp.

409–416. doi:10.1109/CVPR.2011.5995344.

[27] R. Achantay, S. Hemamiz, F. Estraday, S. S¨usstrunky, Frequency-

tuned salient

region detection,

in:

2009 IEEE Computer Society

Conference on Computer Vision and Pattern Recognition Workshops,

CVPR Workshops 2009, Vol. 2009 IEEE Computer Society Conference

on Computer Vision and Pattern Recognition, 2009, pp. 1597–1604.

doi:10.1109/CVPRW.2009.5206596.

35

