2
2
0
2

r
a

M
4
1

]
E
N
.
s
c
[

2
v
8
2
5
0
0
.
3
0
2
2
:
v
i
X
r
a

On genetic programming representations and fitness functions
for interpretable dimensionality reduction

Thomas Uriot
Leiden University Medical Center
Leiden, The Netherlands

Tanja Alderliesten
Leiden University Medical Center
Leiden, The Netherlands

Marco Virgolin
Centrum Wiskunde & Informatica
Amsterdam, The Netherlands

Peter A.N. Bosman
Centrum Wiskunde & Informatica
Amsterdam, The Netherlands

ABSTRACT
Dimensionality reduction (DR) is an important technique for data
exploration and knowledge discovery. However, most of the main
DR methods are either linear (e.g., PCA), do not provide an explicit
mapping between the original data and its lower-dimensional rep-
resentation (e.g., MDS, t-SNE, isomap), or produce mappings that
cannot be easily interpreted (e.g., kernel PCA, neural-based autoen-
coder). Recently, genetic programming (GP) has been used to evolve
interpretable DR mappings in the form of symbolic expressions.
There exists a number of ways in which GP can be used to this end
and no study exists that performs a comparison. In this paper, we
fill this gap by comparing existing GP methods as well as devising
new ones. We evaluate our methods on several benchmark datasets
based on predictive accuracy and on how well the original features
can be reconstructed using the lower-dimensional representation
only. Finally, we qualitatively assess the resulting expressions and
their complexity. We find that various GP methods can be compet-
itive with state-of-the-art DR algorithms and that they have the
potential to produce interpretable DR mappings.

CCS CONCEPTS
â€¢ Computing methodologies â†’ Genetic programming; Dimen-
sionality reduction and manifold learning.

KEYWORDS
Dimensionality reduction, genetic programming, interpretability,
unsupervised learning

1 INTRODUCTION
Dimensionality reduction (DR) is a key instrument for knowledge
discovery [8, 11]. Informally, we can define DR (see Section 2.1 for
a formal definition) to be the area of study concerned with find-
ing lower dimensional representations of the original data, such
that some meaningful properties of the original dataset are pre-
served. It is based on the fact that most real-world datasets only
have artificially high dimensionality due to, for instance, interde-
pendent or noisy features. However, current state-of-the-art DR
algorithms act as a black-box when going from high to low dimen-
sions (e.g., t-SNE [21], isomap [25], LLE [25]), and do not provide
any means to inspect the resulting lower dimensions in terms of
the original variables, i.e., they do not provide a functional map-
ping that describes how the original dimensions are compressed
into the lower-dimensional (or also called latent) representation.
Other methods, such as kernel PCA [37] and neural autoencoders,

do provide a functional mapping between the original data and
its lower dimensional representation, however these mappings are
hardly interpretable. Furthermore, in the absence of meaningful
labels or classes, it becomes even more challenging to assess and
inspect whether a lower-dimensional representation produced by
DR is truly meaningful. Finally, DR is often used for data explo-
ration, in which case the ability to draw insights from the data
is key. For these reasons, producing a functional mapping that is
human-interpretable can be very important, as that allows to assess
whether the compression the mapping performs may be reason-
able and, e.g., safe to use in high-stakes applications of machine
learning [36].

Recently, the use of genetic programming [14] (GP) has been
an increasingly popular way to produce interpretable models. For
instance, GP has been used to learn programs encoded by simple
symbolic expression representing physical laws [4, 9]. Furthermore,
recent works have investigated the use of such symbolic expressions
produced by GP as interpretable models [7, 17, 31, 33]. In this work,
we leverage the interpretability potential of GP and apply it to
non-linear DR.

GP has been shown to be a promising avenue to produce in-
terpretable lower-dimensional representations in several recent
works [17, 18, 20]. However, more representations and evaluation
schemes can be imagined, but no work has been done to provide a
comparison of these different options. We attempt to fill this gap
by designing new GP-based methods for DR and establish a ro-
bust evaluation protocol across two objectives (predictive power of
the newly constructed features and their ability to reconstruct the
original input). The main contributions of this paper are threefold:
(i) we investigate how GP can best be used for DR by considering
standard DR loss functions and GP representations (see Section
3), (ii) additionally, we perform experiments with several DR loss
functions that have previously never been applied to GP, such
as weighted rank-correlation, using a neural-based autoencoder
teacher, or directly using a GP-based autoencoder to reconstruct
the input, and (iii), we show that our GP-based methods are able
to learn non-linear and concise formulas of the lower-dimensional
latent space. We benchmark our methods against three mainstream
DR algorithms: principal components analysis [10] (PCA), locally
linear embedding [25] (LLE), and isomap [29].

 
 
 
 
 
 
2 BACKGROUND
2.1 Dimensionality Reduction
The goal of DR is to find a lower-dimensional representation of
the original data such that some properties of the original data,
such as total variance (e.g., PCA), distances between points (e.g.,
isomap), ability to reconstruct the original features (e.g., autoen-
coders) are preserved. Formally, let us denote the original dataset
by X âˆˆ Rğ‘›Ã—ğ‘ where ğ‘› and ğ‘ represent the number of instances
and features, respectively. Then, the goal is to find another repre-
sentation (cid:101)X âˆˆ Rğ‘›Ã—ğ‘˜ of X, such that ğ‘˜ â‰ª ğ‘, and such that the new
representation retains some meaningful properties of the original
data. Note that throughout this paper, we will refer to ğ‘‹ ğ‘— and (cid:101)ğ‘‹ ğ‘—
as the ğ‘— th dimension (feature), and ğ‘¥ğ‘– and Ëœğ‘¥ğ‘– as the ğ‘–th observation
of the original and lower-dimensional representation, respectively.

2.2 Genetic Programming for Dimensionality

Reduction

Recently, DR using GP has gained traction and has started being
more thoroughly explored. In [17], the authors use GP to perform
DR by using a fitness function that encourages the preservation of
local neighborhoods. In [20], the authors build on their previous
work [17] by taking a bi-objective approach where the first objective
encourages to preserve local structure, while the second objective
represents the number of dimensions in the latent representation.
However, both in [17] and [20], there is no constraint imposed on
the complexity of the evolved expressions, potentially rendering
them non-interpretable. This issue is addressed in [18] where the
authors use a bi-objective approach with the first objective being
the cost function used in the t-SNE algorithm [21] and the second
objective being a proxy for complexity. However, t-SNE is devised
to strictly preserve local neighborhoods and does not result in a
functional mapping, thus it is unclear whether the cost function of
t-SNE is well-suited to be used for GP.

In addition, GP has recently been used to perform feature ex-
traction and transfer learning by constructing non-linear combina-
tions of features to then be used in subsequent supervised learning
tasks [16, 31, 38]. This type of application requires to specify a
predictive task with ground truth labels. However, we may be inter-
ested in discovering intrinsic relationships between the variables
or find an interpretable functional mapping for the compression
of the original dataset without necessarily having an associated
predictive task. One way to tackle the aforementioned problem was
explored in [27], where the authors used GP to predict the value of
each feature by using the remaining ones as input. However, this
process does not belong to the realm of DR since the number of
features remains the same.

3.1 Computational Tree Representations
Single tree. The single-tree representation, as shown in Fig-
3.1.1
ure 1a, is one of the simplest way to represent a symbolic expression.
In the single-tree representation, a single tree is responsible to out-
put a unique single value. Formally, if we denote a single tree by
: Rğ‘ â†’ R, for ğ‘— = 1, . . . , ğ‘˜. In words, each
ğ‘š, we have that ğ‘š ğ‘—
tree maps the original dimensions (the terminals at the leaves) to a
single latent dimension (the root of the tree).

This representation is limited when it comes to modeling multi-
output functions which is the case in DR, if ğ‘˜ > 1. Indeed, each
latent dimension is evolved separately (see Figure 1a). Therefore,
this representation cannot efficiently model non-separable fitness
functions, where variables are dependent and should thus be jointly
optimized, as is the case in DR.

3.1.2 Classic multi-tree. Similarly to [17, 18, 20], we use a multi-
tree representation as illustrated in Figure 1b. Each tree of the
multi-tree takes the original data as input and outputs a single
dimension of the lower-dimensional representation. During evo-
lution, the trees of the multi-tree evolve simultaneously, different
from single-tree representation, where each tree evolves separately
during independent runs. This representation has two main advan-
tages: (i) it only requires a single evolution run, (ii) it is able to learn
non-separable fitness functions, where the variables are dependent,
which is the case in cost functions that are typically used in DR.

In this paper, we will denote the multi-tree representation by
MT (Â·) = {ğ‘š ğ‘— (Â·)}ğ‘˜
ğ‘—=1, where ğ‘š ğ‘— represents the ğ‘— th tree of the multi-
tree and ğ‘˜ is the number of dimensions in the lower-dimensional
representation.

3.1.3 Autoencoder multi-tree. In addition to the classic multi-tree,
we propose a new representation for DR (inspired from automati-
cally defined functions [13]), composed of two classic multi-trees,
with one multi-tree acting as encoder and one as decoder. Specifi-
cally, the encoder is a multi-tree composed of ğ‘˜ trees and is respon-
sible to map the original input to its lower-dimensional represen-
tation. On the other hand, the decoder is a multi-tree composed
of ğ‘ trees where ğ‘ is the number of original features. It is respon-
sible to map the output of the encoder to reconstruct the original
features. The encoder and the decoder are jointly evolved in a sin-
gle run. Formally, let us denote the encoder and decoder of the
autoencoder multi-tree by AMT (ğ‘’) and AMT (ğ‘‘) respectively,
then we have that AMT (ğ‘’) (Â·) = {ğ‘š (ğ‘’)
ğ‘—=1 and AMT (ğ‘‘) (Â·) =
(Â·)}ğ‘˜
ğ‘—
: Rğ‘ â†’ R and ğ‘š (ğ‘‘)
ğ‘—=1, where ğ‘š (ğ‘’)
{ğ‘š (ğ‘‘)
ğ‘—
The purpose of the autoencoder multi-tree is to test whether
a fully GP-based approach, i.e., one where both the encoder and
decoder are evolved at the same time, is capable of performing DR
effectively, as explained in the following subsection.

: Rğ‘˜ â†’ R.

(Â·)}

ğ‘

ğ‘—

ğ‘—

3 METHODOLOGY
In GP, the main way to represent symbolic expressions is by defin-
ing it as a computational tree [24]. In this paper, we follow the
literature and use trees to represent our GP programs. A detailed
description of the tree-based representations we consider is given
in the following subsection.

2

3.2 Fitness Functions
We investigate several fitness functions, denoted by F , to guide the
evolutionary process of the multi-tree GP. As mentioned in Section
2.1, we denote the lower-dimensional representation of the original
ğ‘¥ğ‘– = MT (ğ‘¥ğ‘– ),
dataset X by (cid:101)X, where the ğ‘–th data point is given by (cid:101)
ğ‘– = 1, . . . , ğ‘›, for a given multi-tree individual MT . Furthermore,
we denote the distance matrices of the original dataset and its

Figure 1: Three ways of mapping the original data to its lower-dimensional representation using computational trees. Note
that the trees are read top-down, from leaves to output. (a) Classic: A GP individual is represented by a single tree. Each tree
maps the input to a unique latent dimension, and is evolved separately. It thus requires ğ‘˜ independent runs. (b) Multi-tree:
A GP individual is a collection of ğ‘˜ trees that are evolved jointly in a single evolution run. (c) Autoencoder multi-tree multi-
tree: A GP individual is made of two parts: the encoder and the decoder. The encoder is a multi-tree composed of ğ‘˜ trees and is
responsible to map the original input to its lower-dimensional representation. The decoder is a multi-tree composed of ğ‘ trees,
where ğ‘ is the number of original features, and is responsible to map the output of the encoder to reconstruct the original
features. The encoder and the decoder evolve jointly.

lower-dimensional representation by ğ· and (cid:101)ğ·, respectively, where
ğ·ğ‘–,ğ‘— = ğ‘‘ğ‘–,ğ‘— and (cid:101)ğ·ğ‘–,ğ‘— = (cid:101)ğ‘‘ğ‘–,ğ‘— . For the distance preserving and rank
preserving fitness functions, we investigate two distance metrics:
(i) Euclidean distance, and, (ii) geodesic distance. The geodesic
distance is particularly suited to represent the local low-dimensional
geometry of the data manifold [29] that the Euclidean distance may
not be able to capture.

Formally, for a fitness function denoted by F (X, Â·), we seek

MT âˆ— such that:

MT âˆ— = argmin

F (X, Â·).

M T

(1)

Note that for the GP-based autoencoder fitness function, we use
an autoencoder multi-tree and the best individual would thus be
denoted by AMT âˆ—.

3.2.1 Distance preserving. This fitness function is based on preserv-
ing distances between pair of instances when mapping the original
data to its lower-dimensional representation. To this end, we use
Sammon mapping [26] which aims to minimize the following:

Fdist = F (X, (cid:101)X) =

1
(cid:205)ğ‘–< ğ‘— ğ‘‘ğ‘–,ğ‘—

âˆ‘ï¸

ğ‘–< ğ‘—

(ğ‘‘ğ‘–,ğ‘— âˆ’ (cid:101)ğ‘‘ğ‘–,ğ‘— )2
ğ‘‘ğ‘–,ğ‘—

.

(2)

The effect of ğ‘‘ğ‘–,ğ‘— in the denominator is to put emphasis on points
that are close to each-other in the original space. In other words,
we give more importance in preserving local structure than global.

3.2.2 Rank preserving. This fitness function is similar to preserving
distances except we are only interested in the ranks of the distances.
Essentially, this is a measure of the monotonicity of the relation-
ship between the distances in the original and lower-dimensional
representations. This has the advantage of being more flexible, less
sensitive to outliers (i.e., large distances) and thus more easily learn-
able than the distance preserving fitness. In [20], the authors use
Spearmanâ€™s rank correlation coefficient [35] as the fitness function
to perform dimensionality reduction using GP. In this paper, we use

Kendallâ€™s ğœ which is preferred to Spearmanâ€™s rank correlation coef-
ficient for smaller samples [12]. Our aim is to maximize Kendallâ€™s
ğœ:

,

ğœ =

ğ‘›ğ‘ âˆ’ ğ‘›ğ‘‘
ğ‘
2 (ğ‘ âˆ’ 1)
where ğ‘›ğ‘ is the number of concordant pairs, ğ‘›ğ‘‘ is the number of
discordant pairs, and ğ‘ is the total number of pairs. Let us denote the
ğ‘–th row of the distance matrices (cid:101)ğ· and ğ· by Ëœğ‘‘ğ‘– and ğ‘‘ğ‘– respectively.
Then, we have that a pair is concordant if sgn(ğ‘‘ğ‘–,ğ‘— âˆ’ğ‘‘ğ‘–,ğ‘™ ) = sgn( (cid:101)ğ‘‘ğ‘–,ğ‘— âˆ’
(cid:101)ğ‘‘ğ‘–,ğ‘™ ), and discordant if sgn(ğ‘‘ğ‘–,ğ‘— âˆ’ ğ‘‘ğ‘–,ğ‘™ ) = âˆ’sgn( (cid:101)ğ‘‘ğ‘–,ğ‘— âˆ’ (cid:101)ğ‘‘ğ‘–,ğ‘™ ), for ğ‘— â‰  ğ‘™
and sgn(Â·) being the standard signum function.

(3)

Taking the average across instances, the final fitness function

(to be minimized) is defined as:

Frank = F (X, (cid:101)X) = âˆ’

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğœğ‘– = âˆ’

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘›ğ‘ğ‘– âˆ’ ğ‘›ğ‘‘ğ‘–
ğ‘ğ‘–
2 (ğ‘ğ‘– âˆ’ 1)

,

(4)

where ğ‘›ğ‘ğ‘– , ğ‘›ğ‘‘ğ‘– , and ğ‘ğ‘– are computed using ğ‘‘ğ‘– and (cid:101)ğ‘‘ğ‘– .
However, this approach fails to take into account that discor-
dances between observations with high rank (a rank of 0 being the
highest rank) are more important than those between items with
low rank. This is because we want to emphasize preserving local
structure over global structure, and so, if the distance between two
points in the original space is large (i.e. the rank is low), we want it
to have a lesser importance in the final fitness function. This serves
similar purpose to the denominator in the Sammon mapping in
Equation (2).

Therefore, we use a weighted version of Kendallâ€™s ğœ proposed
in [30], where the weighting is defined by ranking the original
distances and assigning the highest rank of 0 to the shortest distance,
and so on. Then, a weighting function assigns a weight ğ‘¤ depending
on the rank ğ‘Ÿ . In this paper, we use a hyperbolic weighting function,
and thus, we have that ğ‘¤ = 1

ğ‘Ÿ +1 .

3

Autoencoder multi-tree3.2.3 Neural-based autoencoder teacher. This fitness function is
inspired by the student-teacher literature, where a (usually) simple
model (e.g., linear regression, low-depth trees, symbolic regression)
is trained to approximate the output of another (complex) model.
This is a two-step process where we first train a base model and then
train the surrogate model on the output (or hidden layer) produced
by the base model.

In our case, let us denote the latent layer of the neural-based
autoencoder by L = {ğ‘™ ğ‘— }ğ‘˜
ğ‘—=1, where ğ‘™ ğ‘— represents the ğ‘— th neuron and
ğ‘˜ is the number of latent dimensions (e.g., the number of neurons
in the latent layer). Our fitness function is then given by:

ğ‘›
âˆ‘ï¸

ğ‘˜
âˆ‘ï¸

1
ğ‘›ğ‘˜

FAE = F (X, (cid:101)X) =

(ğ‘™ ğ‘— (ğ‘¥ğ‘– ) âˆ’ (cid:101)

ğ‘¥ğ‘–,ğ‘— )2,

(5)

ğ‘–=1

ğ‘—=1
ğ‘¥ğ‘–,ğ‘— = ğ‘š ğ‘— (ğ‘¥ğ‘– ) is the output of the ğ‘— th tree of the multi-tree
where (cid:101)
MT = {ğ‘š ğ‘— }ğ‘˜
ğ‘—=1. In words, the fitness function is the mean-squared-
error between the output of the multi-tree and the latent layer of
the autoencoder.

3.2.4 GP-based autoencoder. This fitness function is similar to the
neural-based autoencoder teacher fitness function in that it also
makes use of an autoencoder. However, instead of evolving a classic
multi-tree GP to match the latent layer of a neural autoencoder
teacher, we let GP evolve a complete autoencoder, using the repre-
sentation shown in Figure 1c. As mentioned, the GP-based autoen-
coder is composed of two multi-trees representing the encoder and
the decoder of the autoencoder. In this case, the lower-dimensional
representation (cid:101)ğ‘‹ of the original dataset X is given by the output of
the encoding multi-tree. Formally, using the notation introduced in
ğ‘¥ğ‘– = AMT (ğ‘¥ğ‘– ) (ğ‘’) , ğ‘– = 1, . . . , ğ‘›, and the fitness
3.1.3, we have that (cid:101)
function is given by:

FGP = F (X, (cid:98)X) =

1
ğ‘›ğ‘

ğ‘›
âˆ‘ï¸

ğ‘
âˆ‘ï¸

ğ‘–=1

ğ‘—=1

(ğ‘¥ğ‘– âˆ’ (cid:98)

ğ‘¥ğ‘–,ğ‘— )2,

(6)

ğ‘¥ğ‘–,ğ‘— = ğ‘š (ğ‘‘)

ğ‘—

where (cid:98)ğ‘‹ğ‘–,ğ‘— = (cid:98)

(ğ‘¥ğ‘– ) is the output of the ğ‘— th tree of the
GPâ€™s decoder AMT (ğ‘‘) . In other words, the cost function is the
reconstruction error between the output of the decoder and the
original data, and the resulting lower-dimensional representation
is given by the output of the encoder.

3.3 Baseline Methods
We compare the performance of the GP-based methods with three
dimensionality reduction baselines: principal components analy-
sis (PCA), locally linear embedding (LLE), and isomap. These meth-
ods are chosen because they learn a mapping on the training data
that can be applied to unseen data, which is not possible with other
methods such as t-SNE or multi-dimensional scaling (MDS). This
enables a fair comparison since our GP-methods are trained to
perform dimensionality reduction, and the evaluation is done on
unseen data, as depicted in Figure 2. In addition, the baselines are
heterogeneous as PCA seeks a global orthogonal linear transforma-
tion of the data, while LLE is a local and non-linear method, and
isomap seeks to preserve the estimated intrinsic geometry of the
data manifold using geodesic distance.

4

3.4 Performance Metrics
In this section, we discuss the choices made to evaluate the per-
formance of the various fitness functions in guiding the evolution
process of GP. This amounts to evaluating the quality of the re-
sulting lower-dimensional representation of the original data. This
is still an open problem in data science and will depend on the
application and the questions that one wants to answer during the
analysis. In addition, the fitness function being optimized during
the dimensionality reduction process may be biased towards our
evaluation metric or even be misaligned with it.

3.4.1 Predictive performance. In some situations, there exists a
metric that truly represents our end goal, such as predictive per-
formance on a supervised downstream task. If there exists such an
objective (task) for a given dataset, then we do not have to worry
whether the metric (e.g., classification accuracy or regression error)
is biased towards any of our GP methods since it is a metric that
we are objectively interested in. Using the predictive power of the
low-dimensional representation on downstream supervised tasks
to evaluate dimensionality reduction algorithms has been done in
recent studies [17, 18, 20].

However, there are three main drawbacks with this approach.
Firstly, we have to assume that the response (e.g., class structure for
classification or target distribution for regression) is a significant
factor within the manifold structure. Arguably, this assumption
holds for the majority of curated datasets, and datasets for which
supervised learning algorithms can perform well (i.e., one can learn
labels from the data). Secondly, there may be some structure in the
data that is not captured by the labels but that may still be of interest
for knowledge discovery, and thus using predictive performance
as a proxy would be insufficient. Finally, and perhaps the most
critical drawback is that we may have datasets on which we want
to perform dimensionality reduction but that have no labels.

In this work, we use balanced accuracy [3] as a performance
metric. In particular, we follow [17, 20] and use a random forest
classifier as evaluation model for accuracy (denoted by ğ¸ in Section
4.4), trained on the low-dimensional representation to predict the
labels and compute the predictive performance.

3.4.2 Reconstruction error. Besides accuracy, we also consider the
reconstruction error, as an additional metric, and, in this case, we
use a neural-based decoder as the evaluation model. Note that,
however, the reconstruction error is biased to favor the methods
using the neural-based autoencoder teacher (FAE) and the GP-based
autoencoder (FGP) fitness functions since they both minimize the
reconstruction error during optimization.

4 EXPERIMENTAL SETUP
4.1 Datasets
We test our methods on 5 datasets from the UCI repository [6] for
which a brief description is provided in Table 1. Note that for the
purpose of our analysis, we only keep the continuous attributes,
which is why the number of features is different from the original
one for datasets that contain categorical variables. This is because
we only use arithmetic operators and do not use any conditional
or logical operators (e.g., if, and, or) in our GP representation (see
Section 4.2). In addition, some of our fitness functions are based on

the Euclidean distance, which would not be appropriate for a mix
of continuous and categorical variables.

index to specialize [1] and converge towards learning the same
latent dimension of the lower-dimensional representation.

Table 1: Datasets considered, where ğ‘›, ğ‘‘, and ğ‘ represent the
number of instances, features, and classes, respectively.

Name

Ionosphere
German Credit
Libras Movement
Segmentation
Telescope (MGT)

n

350
1000
359
2310
19019

d

34
24
90
19
10

c

2
2
15
7
2

4.1.1 Principal component initialization. Note that the fitness func-
tions described in Section 3.2 make use of all the variables from
the original data. This happens either at the time of computing the
distance matrices or when computing the reconstruction error. In
doing so, we may be using noisy variables as well as redundant
variables (e.g., co-linear variables) which could be dominating other
important variables during the optimization process. A simple and
well established solution is to perform PCA before applying any sort
of analysis that requires to compute distances between points, such
as clustering analysis [2, 5]. Thus, in this paper, we transform the
original dataset using PCA (retaining 99% of the original variance)
to compute the fitness functions in Section 3.2. This amounts to
replacing X with Xpca in F (X, Â·), where Xpca is the transformation
of X after applying PCA and keeping a sufficient number of princi-
pal components to account for 99% of the original variance. Note
that in order to keep the GP expressions interpretable in terms of
the original input, the input to the multi-tree GP is still the original
data X, as opposed to the PCA transformed data Xpca.

4.2 Genetic Programming Parameters
We use standard GP parameter settings, as shown in Table 2. A
basic tournament selection [24] process is used to chose the individ-
uals that will act as parents to produce offspring via the variation
operators. Each parent can be subject to up to three different ge-
netic operators: (i) crossover, (ii) subtree mutation, and (iii) one-point
mutation, with probabilities of ğ‘ğ‘ , ğ‘ğ‘  , and ğ‘ğ‘œ , respectively. These
can be easily experimented with, using our publicly available code1.
Note that the genetic operators, maximum tree size, and tree depth
are applied to single trees within a multi-tree. This means that, for
instance, each tree within a multi-tree has a ğ‘ğ‘  probability of having
a subtree mutation. For the crossover operator, where subtrees are
exchanged between two trees, one has to decide whether cross-
pollination between trees at different index from the two multi-tree
individuals is allowed. In [19], the authors randomly select a tree
from each individual and perform standard crossover between them
(random-index crossover), while in [1], the authors only allow trees
at the same position in the multi-tree to crossover with each other
(same-index crossover). Here, we use same-index crossover when
mixing trees from different multi-trees. The reason we make this
choice is that same-index crossover encourages trees at the same

1https://github.com/pinouche/gp_dr

Finally, the population is initialized using the popular ramped
half-and-half method [14], and ephemeral constants are drawn
from a N (0, 1). The terminal set (i.e., the set of all possible leaf
nodes) is T = {ğ‘‹, R}, where R represents the set of random con-
ğ‘
stants and ğ‘‹ = {ğ‘‹ ğ‘— }
ğ‘—=1 is the set of variables. The function set (i.e.,
the set of possible non-leaf nodes) is F = {âˆ’, +, Ã—}. This means
that GP will evolve polynomials, with arbitrarily complex interac-
tions only bounded by the depth of the tree. This results in simple
expressions that can be used as a starting point to further inter-
pretability studies on performing DR with GP. Furthermore, we
conducted the same experiments using the extended function set
= {âˆ’, +, Ã—, cos, logğ‘ }, where log(Â·)ğ‘ = log(| Â· | + ğœ€), but obtained
F
significantly worse results than when using F . Due to space limita-
â€²
tions, the results obtained with F
are not presented here but can
be found in the code repository.

â€²

Table 2: GP parameters and their settings.

Parameter
Crossover rate (ğ‘ğ‘ )
Subtree mutation rate (ğ‘ğ‘  )
Operator muration rate (ğ‘ğ‘œ )
Population size (ğ‘ƒ)
Generations
Tree depth: (Min, Max)
Tournament Size

Value

0.8
0.2
0.2
1000
100
(2, 7)
7

4.3 Mini-batch Training
Note that during the evolutionary process, for each generation,
we have to compute the distance matrix (cid:101)ğ· for all the multi-tree
individuals in the population, which has an overall computational
cost of O (ğ‘˜ğ‘›2 Ã— ğ‘ƒ), where ğ‘ƒ is the population size used by GP, ğ‘› is
the number of data points, and ğ‘˜ the number of latent variables of
the lower-dimensional representation. For large values of ğ‘›, this can
be prohibitively slow. Therefore, at each generation, we randomly
sample a mini-batch of ğ‘ < ğ‘› data points from the data and compute
the distance matrices on that mini-batch. In addition, mini-batch
training is thought to improve generalization by changing the loss
landscape at each batch, and thus escaping local optima [23].

4.4 Evaluation Protocol
A detailed account of the overall evaluation process, from the orig-
inal data to the final predictions, is given in Figure 2. First, the
original data is split into two subsets: (i) the training data for the
DR model ğ‘€, and, (ii) a held-out set to apply ğ‘€ on and obtain its
lower-dimensional representation. In turn, this lower-dimensional
data is split in two subsets: (i) the training data for the evaluation
model ğ¸, and, (ii) the hold-out set to apply the evaluation model
ğ¸ to compute the performance metrics. The evaluation model ğ¸
(e.g., a neural decoder for reconstruction error and a random forest
classifier for accuracy) is trained using a 10-fold cross validation
and we report the results on the test set (see Section 5). Note that

5

Figure 2: Evaluation protocol for our DR pipeline. First, we learn a DR model ğ‘€ on a subset of the original data. This model
is then applied on the held-out subset of the original data to obtain a lower-dimensional representation. We then learn an
evaluation model ğ¸ on a subset of the lower-dimensional data and apply it to the held-out subset in order to compute the final
performance metric.

the models used for the evaluation phase are described in Section
3.4.

5 RESULTS
5.1 Quantitative Analysis
In Table 3 (a), we display the average balanced accuracy and associ-
ated standard deviation computed across 30 independent runs for
all the GP-based methods and the baselines. Similarly, in Table 3 (b),
we display the results for the reconstruction error. In addition, we
compute statistical significance using the Mann-Whitney U rank
test [22], denoted by the asterisk notation.

We can see that the non-linear baseline methods LLE and isomap
seem to yield the better performances, in particular when ğ‘˜ = 2.
For ğ‘˜ = 3, Fdist (Euclidean) performs best on 2 of the datasets but is
not always statistically significant. Overall, it seems that for ğ‘˜ = 3,
the difference between the methods is less pronounced than for
ğ‘˜ = 2. This is intuitive since the more dimensions we keep the
easier it is to capture relevant information from the original data,
thus attenuating potential differences between methods. In fact,
for more complex datasets with a higher number of features, such
as Ionosphere and Libras Movement, we can still see a significant
difference between the best performing method (isomap) and the
other methods. Additionally, PCA (the only linear dimensionality
reduction method in our experiments) does not perform well on
these two datasets, suggesting that non-linearity allows for captur-
ing relevant relationships between the original variables.

In Table 3 (b), we can see that FAE and FGP have the lowest
reconstruction error out of the GP-based methods. This is expected
because these methods minimize the reconstruction error during
the optimization process. In addition, we can see that PCA and
isomap yield good results across several datasets. Interestingly,
while LLE performs well when balanced accuracy is used as met-
ric, it is arguably the worst performing method to reconstruct the

6

original data. This may be explained by the fact that co-linear fea-
tures are equally weighted when computing the distance matrices
ğ· and (cid:101)ğ·. This is detrimental to maximizing accuracy as there may
be independent variables (e.g., non co-linear) that are important
for the downstream classification task but which would be out-
weighted when computing the distance matrices. For this reason,
we performed PCA before computing distances, as mentioned in
Section 4.1.1. However, we only experimented with keeping 99% of
the original variance and one would need to experiment further to
identify the effect of performing PCA on the balanced accuracy and
reconstruction error. Finally, while FAE and FGP do perform well,
there exists drawbacks to these methods. For FAE we have to train
a neural network autoencoder, which adds a layer of complexity
to the overall method. That is, if our autoencoder teacher does not
perform well in the first place, the GP-based student is bound to
fail. On the other hand, while FGP does not require an autoencoder
teacher model, it does not scale well with the number of input
features since the GP-decoder requires a number of trees equal to
the number of input features, as depicted in Figure 1c.

Overall, our results show that GP-based dimensionality reduc-
tion methods can be on a par with proven baseline methods. In
addition, a major advantage of GP-based methods is that they have
the potential to produce interpretable mappings in the form of
symbolic expressions, which we discuss next.

5.2 Qualitative Analysis
In Table 4, we display the expressions that map the original data into
a lower-dimensional representation, for the various GP-based meth-
ods. We can see that the resulting embeddings are small enough
to have the potential of being interpretable. This is in contrast to
the baseline methods where only PCA provide a global functional
mapping from the original data to its lower-dimensional represen-
tation. However, PCA strictly returns a linear combination of all
the variables, limiting its complexity and rendering it unintelligible

Table 3: Score of (a) balanced accuracy and (b) reconstruction error (mean Â± standard deviation of 30 runs). For a confidence
level of 1 âˆ’ ğ›¼, statistical significance is denoted as follows: âˆ—,âˆ—âˆ— ,âˆ—âˆ—âˆ—, for ğ›¼ = 0.1, 0.05, 0.01, respectively. Statistical testing is
performed between the best method and the remaining 8 methods. If the mean performance is equal across several methods,
the method with the lowest standard deviation is chosen as the best method. The best method and those that are not statistically
significantly different are in bold.

(a) Balanced accuracy (to maximize)

Datasets

Segmentation Ionosphere Credit
0.67Â±0.01 âˆ—âˆ—âˆ—
0.82Â±0.01
0.78Â±0.03 âˆ—âˆ—âˆ—
0.75Â±0.07 âˆ—âˆ—âˆ—
0.77Â±0.06 âˆ—
0.77Â±0.06 âˆ—âˆ—
0.74Â±0.11 âˆ—
0.77Â±0.11
0.77Â±0.07 âˆ—

0.55Â±0.02
0.51Â±0.02 âˆ—âˆ—âˆ—
0.53Â±0.03 âˆ—âˆ—
0.55Â±0.02
0.54Â±0.02 âˆ—
0.53Â±0.03 âˆ—âˆ—
0.56Â±0.04
0.51Â±0.02 âˆ—âˆ—âˆ—
0.54Â±0.03 âˆ—

Dimensions Methods

ğ· = 2

ğ· = 3

PCA
LLE
Isomap
MT , Fdist (Euclidean)
MT , Fdist (geodesic)
MT , Frank (Euclidean)
MT , Frank (geodesic)
MT , FAE
AMT , FGP
PCA
LLE
Isomap
MT , Fdist (Euclidean)
MT , Fdist (geodesic)
MT , Frank (Euclidean)
MT , Frank (geodesic)
MT , FAE
AMT , FGP

0.83Â±0.01
0.85Â±0.01
0.84Â±0.02
0.82Â±0.07
0.85Â±0.04
0.83Â±0.06
0.85Â±0.05
0.85Â±0.05
0.83Â±0.05

0.74Â±0.03 âˆ—âˆ—âˆ—
0.79Â±0.06 âˆ—
0.82Â±0.04
0.78Â±0.04 âˆ—âˆ—
0.77Â±0.06 âˆ—âˆ—
0.77Â±0.06 âˆ—âˆ—
0.79Â±0.04 âˆ—âˆ—
0.79Â±0.05 âˆ—
0.77Â±0.05 âˆ—âˆ—
0.83Â±0.03 âˆ—âˆ—âˆ—
0.80Â±0.05 âˆ—âˆ—
0.84Â±0.03
0.81Â±0.05 âˆ—âˆ—
0.82Â±0.04 âˆ—âˆ—
0.83Â±0.04 âˆ—âˆ—
0.81Â±0.04 âˆ—âˆ—
0.83Â±0.05 âˆ—
0.83Â±0.04 âˆ—

0.57Â±0.03
0.51Â±0.02 âˆ—âˆ—âˆ—
0.53Â±0.03 âˆ—âˆ—
0.58Â±0.05
0.54Â±0.04
0.54Â±0.04 âˆ—
0.54Â±0.03 âˆ—
0.54Â±0.02 âˆ—
0.53Â±0.04 âˆ—âˆ—

Libras
0.28Â±0.04 âˆ—âˆ—âˆ—
0.44Â±0.07
0.46Â±0.04
0.29Â±0.04 âˆ—âˆ—âˆ—
0.33Â±0.04 âˆ—âˆ—âˆ—
0.34Â±0.05 âˆ—âˆ—âˆ—
0.34Â±0.06 âˆ—âˆ—âˆ—
0.35Â±0.04 âˆ—âˆ—âˆ—
0.35Â±0.06 âˆ—âˆ—âˆ—
0.43Â±0.05 âˆ—âˆ—âˆ—
0.49Â±0.06 âˆ—
0.52Â±0.04
0.45Â±0.05 âˆ—âˆ—âˆ—
0.42Â±0.06 âˆ—âˆ—âˆ—
0.48Â±0.07 âˆ—âˆ—
0.46Â±0.06 âˆ—âˆ—âˆ—
0.45Â±0.08 âˆ—âˆ—âˆ—
0.46Â±0.04 âˆ—âˆ—âˆ—

MGT

0.66Â±0.01
0.66Â±0.02
0.63Â±0.01 âˆ—âˆ—âˆ—
0.65Â±0.04
0.63Â±0.02 âˆ—âˆ—
0.64Â±0.05
0.64Â±0.03âˆ—
0.63Â±0.03 âˆ—âˆ—
0.63Â±0.03 âˆ—âˆ—

0.70Â±0.01
0.69Â±0.02
0.69Â±0.01
0.71Â±0.04
0.69Â±0.05
0.70Â±0.05
0.68Â±0.05
0.68Â±0.03
0.69Â±0.04

Dimensions Methods

Segmentation Ionosphere Credit

Libras

MGT

(b) Reconstruction error (to minimize)

Datasets

ğ· = 2

ğ· = 3

PCA
LLE
Isomap
MT , Fdist (Euclidean)
MT , Fdist (geodesic)
MT , Frank (Euclidean)
MT , Frank (geodesic)
MT , FAE
AMT , FGP
PCA
LLE
Isomap
MT , Fdist (Euclidean)
MT , Fdist (geodesic)
MT , Frank (Euclidean)
MT , Frank (geodesic)
MT , FAE
AMT , FGP

0.73Â±0.06
1.01Â±0.14 âˆ—âˆ—âˆ—
0.81Â±0.10 âˆ—âˆ—
0.79Â±0.12 âˆ—
0.79Â±0.12 âˆ—
0.79Â±0.15
0.86Â±0.19 âˆ—âˆ—
0.72Â±0.08
0.78Â±0.09 âˆ—

0.60Â±0.08
0.98Â±0.15 âˆ—âˆ—âˆ—
0.74Â±0.14 âˆ—âˆ—
0.69Â±0.09 âˆ—âˆ—
0.75Â±0.13 âˆ—âˆ—âˆ—
0.73Â±0.11 âˆ—âˆ—âˆ—
0.74Â±0.16 âˆ—âˆ—
0.67Â±0.08 âˆ—âˆ—
0.72Â±0.11 âˆ—âˆ—

1.27Â±0.13
1.30Â±0.13
1.26Â±0.14
1.29Â±0.13
1.30Â±0.13
1.32Â±0.14 âˆ—
1.33Â±0.15 âˆ—âˆ—
1.29Â±0.14
1.28Â±0.13

1.24Â±0.14
1.28Â±0.14
1.25Â±0.14
1.28Â±0.14
1.30Â±0.15
1.31Â±0.13 âˆ—
1.34Â±0.17 âˆ—
1.26Â±0.14
1.26Â±0.14

0.94Â±0.03 âˆ—âˆ—âˆ—
0.96Â±0.03 âˆ—âˆ—âˆ—
0.88Â±0.03
0.96Â±0.04 âˆ—âˆ—âˆ—
0.94Â±0.04 âˆ—âˆ—âˆ—
0.96Â±0.03 âˆ—âˆ—âˆ—
0.97Â±0.05 âˆ—âˆ—âˆ—
0.89Â±0.03
0.95Â±0.04 âˆ—âˆ—âˆ—
0.90Â±0.04 âˆ—âˆ—âˆ—
0.94Â±0.03 âˆ—âˆ—âˆ—
0.85Â±0.03
0.92Â±0.04 âˆ—âˆ—âˆ—
0.92Â±0.04 âˆ—âˆ—âˆ—
0.94Â±0.06 âˆ—âˆ—âˆ—
0.93Â±0.08 âˆ—âˆ—
0.88Â±0.03 âˆ—
0.91Â±0.03 âˆ—âˆ—âˆ—

0.95Â±0.09
1.01Â±0.09 âˆ—
0.97Â±0.10
0.97Â±0.11
0.98Â±0.10
0.96Â±0.10
0.96Â±0.10
0.95Â±0.09
0.94Â±0.09

0.89Â±0.10
0.95Â±0.10 âˆ—âˆ—
0.97Â±0.08 âˆ—âˆ—
0.89Â±0.11
0.92Â±0.10
0.89Â±0.10
0.87Â±0.07
0.86Â±0.09
0.86Â±0.09

0.63Â±0.02
0.81Â±0.06 âˆ—âˆ—âˆ—
0.63Â±0.03
0.66Â±0.04 âˆ—
0.68Â±0.04 âˆ—âˆ—âˆ—
0.69Â±0.05 âˆ—âˆ—âˆ—
0.72Â±0.05 âˆ—âˆ—âˆ—
0.66Â±0.04 âˆ—âˆ—
0.63Â±0.03
0.62Â±0.02 âˆ—
0.77Â±0.04 âˆ—âˆ—âˆ—
0.62Â±0.03
0.64Â±0.02 âˆ—âˆ—âˆ—
0.65Â±0.04 âˆ—âˆ—
0.65Â±0.04 âˆ—âˆ—âˆ—
0.69Â±0.04 âˆ—âˆ—âˆ—
0.63Â±0.02 âˆ—âˆ—âˆ—
0.60Â±0.03

7

Table 4: Expressions of the lower-dimensional representation of the best solution in terms of reconstruction error (out of the
30 independent runs), for all GP-based methods on the Libras and Segmentation datasets from UCI [6].

Method

Fdist (geodesic)

Fdist (Euclidean)

Dim
(cid:101)ğ‘‹ 1
(cid:101)ğ‘‹ 2
(cid:101)ğ‘‹ 1
(cid:101)ğ‘‹ 2
Frank (Euclidean) (cid:101)ğ‘‹ 1
(cid:101)ğ‘‹ 2
(cid:101)ğ‘‹ 1
(cid:101)ğ‘‹ 2
(cid:101)ğ‘‹ 1
(cid:101)ğ‘‹ 2
(cid:101)ğ‘‹ 1
(cid:101)ğ‘‹ 2

Frank (geodesic)

FAE

FGP

Segmentation
ğ‘¥18 âˆ’ ğ‘¥6 âˆ’ ğ‘¥8
ğ‘¥14 âˆ’ ğ‘¥17 âˆ’ ğ‘¥2
ğ‘¥0 + 3ğ‘¥14 + ğ‘¥7
ğ‘¥15ğ‘¥18 + ğ‘¥15 + ğ‘¥8
ğ‘¥18
ğ‘¥14 âˆ’ ğ‘¥17 (ğ‘¥5 + ğ‘¥9) âˆ’ ğ‘¥18 + ğ‘¥2
ğ‘¥11 âˆ’ ğ‘¥15 âˆ’ ğ‘¥16 âˆ’ ğ‘¥6
0.767ğ‘¥14 âˆ’ ğ‘¥18
âˆ’ğ‘¥10 âˆ’ ğ‘¥12 + ğ‘¥18 âˆ’ ğ‘¥6
ğ‘¥12ğ‘¥5
ğ‘¥11 + ğ‘¥8
2 + ğ‘¥6
ğ‘¥ 2

Datasets

Libras Movement
âˆ’ğ‘¥14 âˆ’ ğ‘¥26 + ğ‘¥45 + ğ‘¥55
ğ‘¥55 + ğ‘¥58
âˆ’ğ‘¥11ğ‘¥67 + ğ‘¥42 + ğ‘¥62 + ğ‘¥70
ğ‘¥36
ğ‘¥ 2
0
ğ‘¥11 + ğ‘¥15 + ğ‘¥16 + ğ‘¥42 + ğ‘¥52
ğ‘¥20 (ğ‘¥18 + ğ‘¥53 âˆ’ ğ‘¥55 + ğ‘¥7 âˆ’ ğ‘¥74) âˆ’ ğ‘¥85 + 0.561
ğ‘¥10ğ‘¥78 + ğ‘¥12 âˆ’ ğ‘¥80
ğ‘¥19 + ğ‘¥20 + ğ‘¥42 + ğ‘¥72 âˆ’ ğ‘¥74
ğ‘¥0 âˆ’ ğ‘¥20ğ‘¥86 + ğ‘¥7
ğ‘¥16ğ‘¥53 + ğ‘¥29 + ğ‘¥59
âˆ’ğ‘¥28 + ğ‘¥40 + ğ‘¥53 + ğ‘¥86
âˆ’ğ‘¥15 + ğ‘¥20 + ğ‘¥59 + ğ‘¥73

important; whereas to answer a predictive task as per classification,
only a subset of them may be.

7 CONCLUSION
In conclusion, we have considered different versions, i.e., represen-
tations and fitness functions of which two are novel (GP autoen-
coder representation and autoencoder teacher fitness), of genetic
programming for dimensionality reduction. We have found that
genetic programming is a competitive approach to obtain inter-
pretable mappings for dimensionality reduction. We thus believe
that genetic programming for dimensionality reduction is a promis-
ing research avenue.

when the number of input variables is large. On the other hand,
GP-based methods can construct non-linear expressions while only
using a small subset of the original variables.

6 DISCUSSION
In this paper, we have seen that GP can be competitive with widely
used DR methods, while producing small, non-linear, and poten-
tially interpretable functional mappings of how the original data can
be compressed into the latent dimensions. In particular, when the
fitness used for GP is the same as the objective we want to optimize
for, (e.g., reconstruction error), GP often outperforms baselines.

A limitation of this paper is that we adopted classic GP evolu-
tionary operators for variation and selection, yet state-of-the-art
GP methods include more interesting mechanisms. For instance,
one could use a GP version that is more expressive such as differ-
entiable Cartesian GP [9] (DCGP). This is because in DCGP, one
can optimize internal weights of the expression using gradient de-
scent. Another example could be to adopt GP-GOMEA [32] as it is
state-of-the-art to discover accurate yet small expressions [15].

Another limitation of this study is that we only used a fixed set
of parameters and did not study how different GP representations
and fitness functions inter-relate to different evolutionary budgets
and settings. For example, we used a classic population size of
1000. However, the population size is a key parameter because it
determines the supply of (high-order) building blocks available for
the evolution. Some recent works in GP such as [28, 32] suggest
that better performance is only achievable when the population
size is of tens of thousands or more. In light of this, future work on
understanding the potential of GP for DR should include an analysis
of the effect of important parameter settings. Moreover, future
work should also include more baselines and loss functions in the
comparison, including, e.g., the recenly introduced PaCMAP [34].
An interesting aspect emerging from our results is that DR meth-
ods achieving high accuracy often performed worse in terms of re-
construction error (and vice versa). This can happen because, when
performing reconstruction, each reconstructed feature is equally

8

[30] Vigna, S.: A weighted correlation index for rankings with ties. In: Proceedings of

the 24th international conference on World Wide Web. pp. 1166â€“1176 (2015)

[31] Virgolin, M., Alderliesten, T., Bosman, P.A.: On explaining machine learning
models by evolving crucial and compact features. Swarm and Evolutionary Com-
putation 53, 100640 (2020)

[32] Virgolin, M., Alderliesten, T., Witteveen, C., Bosman, P.A.: Improving model-
based genetic programming for symbolic regression of small expressions. Evolu-
tionary computation 29(2), 211â€“237 (2021)

[33] Virgolin, M., De Lorenzo, A., Medvet, E., Randone, F.: Learning a formula of
interpretability to learn interpretable formulas. arXiv preprint arXiv:2004.11170
(2020)

[34] Wang, Y., Huang, H., Rudin, C., Shaposhnik, Y.: Understanding how dimension
reduction tools work: an empirical approach to deciphering t-sne, umap, trimap,
and pacmap for data visualization. Journal of Machine Learning Research 22(201),
1â€“73 (2021)

[35] Zwillinger, D., Kokoska, S.: CRC standard probability and statistics tables and

formulae. Crc Press (1999)

[36] Adadi, A., Berrada, M.: Peeking Inside the Black-box: A Survey on eXplainable

Artificial Intelligence (XAI)

[37] SchÃ¶lkopf, B., Smola, A., MÃ¼ller, K.: Nonlinear component analysis as a kernel

eigenvalue problem Neural computation 10(5), 1299â€“1319 (1998)

[38] MuÃ±oz, L., Trujillo, L., Silva, S.: Transfer learning in constructive induction with
Genetic Programming Genetic Programming and Evolvable Machines 21(4),
529â€“569. Springer (2020)

REFERENCES
[1] Al-Helali, B., Chen, Q., Xue, B., Zhang, M.: Multi-tree genetic programming with
new operators for transfer learning in symbolic regression with incomplete data.
IEEE Transactions on Evolutionary Computation (2021)

[2] Ben-Hur, A., Guyon, I.: Detecting stable clusters using principal component

analysis. In: Functional genomics, pp. 159â€“182. Springer (2003)

[3] Brodersen, K.H., Ong, C.S., Stephan, K.E., Buhmann, J.M.: The balanced accuracy
and its posterior distribution. In: 2010 20th international conference on pattern
recognition. pp. 3121â€“3124. IEEE (2010)

[4] Cranmer, M., Sanchez-Gonzalez, A., Battaglia, P., Xu, R., Cranmer, K., Spergel, D.,
Ho, S.: Discovering symbolic models from deep learning with inductive biases.
arXiv preprint arXiv:2006.11287 (2020)

[5] Ding, C., He, X.: K-means clustering via principal component analysis. In: Pro-
ceedings of the twenty-first international conference on Machine learning. p. 29
(2004)

[6] Dua, D., Graff, C.: UCI machine learning repository (2017), http://archive.ics.uci.

edu/ml

[7] Evans, B.P., Xue, B., Zhang, M.: Whatâ€™s inside the black-box? a genetic program-
ming method for interpreting complex machine learning models. In: Proceedings
of the Genetic and Evolutionary Computation Conference. pp. 1012â€“1020 (2019)
[8] Huang, S.H., Wulsin, L.R., Li, H., Guo, J.: Dimensionality reduction for knowledge
discovery in medical claims database: application to antidepressant medication
utilization study. Computer methods and programs in biomedicine 93(2), 115â€“123
(2009)

[9] Izzo, D., Biscani, F., Mereta, A.: Differentiable genetic programming. In: European

conference on genetic programming. pp. 35â€“51. Springer (2017)

[10] Jolliffe, I.T., Cadima, J.: Principal component analysis: a review and recent de-
velopments. Philosophical Transactions of the Royal Society A: Mathematical,
Physical and Engineering Sciences 374(2065), 20150202 (2016)

[11] Katole, S.N., Karmore, S.P.: A new approach of microarray data dimension reduc-
tion for medical applications. In: 2015 2nd International Conference on Electronics
and Communication Systems (ICECS). pp. 409â€“413. IEEE (2015)

[12] Kendall, M.G.: A new measure of rank correlation. Biometrika 30(1/2), 81â€“93

(1938)

[13] Koza, J.R.: Genetic programming as a means for programming computers by

natural selection. Statistics and computing 4(2), 87â€“112 (1994)

[14] Koza, J.R., Koza, J.R.: Genetic programming: On the programming of computers

by means of natural selection, vol. 1. MIT press (1992)

[15] La Cava, W., Orzechowski, P., Burlacu, B., de FranÃ§a, F.O., Virgolin, M., Jin, Y.,
Kommenda, M., Moore, J.H.: Contemporary symbolic regression methods and
their relative performance. arXiv preprint arXiv:2107.14351 (2021)

[16] Lensen, A., Xue, B., Zhang, M.: Improving k-means clustering with genetic pro-
gramming for feature construction. In: Proceedings of the Genetic and Evolu-
tionary Computation Conference Companion. pp. 237â€“238 (2017)

[17] Lensen, A., Xue, B., Zhang, M.: Can genetic programming do manifold learning
too? In: European Conference on Genetic Programming. pp. 114â€“130. Springer
(2019)

[18] Lensen, A., Xue, B., Zhang, M.: Genetic programming for evolving a front of
interpretable models for data visualization. IEEE transactions on cybernetics
(2020)

[19] Lensen, A., Xue, B., Zhang, M.: Genetic programming for evolving similarity
functions for clustering: Representations and analysis. Evolutionary computation
28(4), 531â€“561 (2020)

[20] Lensen, A., Zhang, M., Xue, B.: Multi-objective genetic programming for mani-
fold learning: balancing quality and dimensionality. Genetic Programming and
Evolvable Machines pp. 1â€“33 (2020)

[21] Maaten, L.v.d., Hinton, G.: Visualizing data using t-sne. Journal of machine

learning research 9(Nov), 2579â€“2605 (2008)

[22] Mann, H.B., Whitney, D.R.: On a test of whether one of two random variables
is stochastically larger than the other. The annals of mathematical statistics pp.
50â€“60 (1947)

[23] Masters, D., Luschi, C.: Revisiting small batch training for deep neural networks.

arXiv preprint arXiv:1804.07612 (2018)

[24] Poli, R., Langdon, W.B., McPhee, N.F., Koza, J.R.: A field guide to genetic pro-

gramming. Lulu. com (2008)

[25] Roweis, S.T., Saul, L.K.: Nonlinear dimensionality reduction by locally linear

embedding. science 290(5500), 2323â€“2326 (2000)

[26] Sammon, J.W.: A nonlinear mapping for data structure analysis. IEEE Transactions

on computers 100(5), 401â€“409 (1969)

[27] Schofield, F., Lensen, A.: Evolving simpler constructed features for clustering
problems with genetic programming. In: 2020 IEEE Congress on Evolutionary
Computation (CEC). pp. 1â€“8. IEEE (2020)

[28] Schweim, D., Wittenberg, D., Rothlauf, F.: On sampling error in genetic program-

ming. Natural Computing pp. 1â€“14 (2021)

[29] Tenenbaum, J.B., De Silva, V., Langford, J.C.: A global geometric framework for
nonlinear dimensionality reduction. science 290(5500), 2319â€“2323 (2000)

9

