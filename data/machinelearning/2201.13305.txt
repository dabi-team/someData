2
2
0
2

n
a
J

1
3

]
E
N
.
s
c
[

1
v
5
0
3
3
1
.
1
0
2
2
:
v
i
X
r
a

Optimizing LLVM Pass Sequences with Shackleton:
A Linear Genetic Programming Framework

Hannah Peeler
hannah.peeler@arm.com
Arm Ltd., Cambridge, UK
USA

Shuyue Stella Li
sli136@jhu.edu
Department of Computer Science,
Johns Hopkins University
Baltimore, MD, USA

Andrew N. Sloss
andrew.sloss@arm.com
Arm Ltd., Cambridge, UK
USA

Kenneth N. Reid
ken@kenreid.co.uk
Department of Animal Science,
Michigan State University
East Lansing, MI, USA

Yuan Yuan‚àó
yyuan@msu.edu
Department of Computer Science and
Engineering, Michigan State
University
East Lansing, MI, USA

Wolfgang Banzhaf
banzhafw@msu.edu
Department of Computer Science and
Engineering, Michigan State
University
East Lansing, MI, USA

Abstract
In this paper we introduce Shackleton as a generalized frame-
work enabling the application of linear genetic programming
to a variety of use cases. We also explore here a novel ap-
plication for this class of methods: optimizing sequences
of LLVM optimization passes. The algorithm underpinning
Shackleton is discussed, with an emphasis on the effects of
different features unique to the framework when applied to
LLVM pass sequences. Combined with analysis of different
hyperparameter settings, we report the results on automat-
ically optimizing pass sequences using Shackleton for two
software applications at differing complexity levels. Finally,
we reflect on the advantages and limitations of our current
implementation and lay out a path for further improvements.
These improvements aim to surpass hand-crafted solutions
with an automatic discovery method for an optimal pass
sequence.

CCS Concepts: ‚Ä¢ Computing methodologies ‚Üí Genetic
programming; ‚Ä¢ Software and its engineering ‚Üí Com-
pilers.

Keywords: Evolutionary Algorithms, Genetic Programming,
Compiler Optimization, Parameter Tuning, Metaheuristics

1 INTRODUCTION
Linear Genetic Programming (LGP) [5] is categorized within
the super-set of Genetic Programming (GP) methodologies.
These methodologies were established in the early 1990s as
a means to generate algorithms and programs automatically,
steered by an evolutionary process [4, 11]. Historically, the
proliferation of LGP (and the wider umbrella of Evolutionary
Algorithms (EAs) it sits under) has been hindered by the
need to tailor each implementation to its target application
and the limited selection of potential applications [16]. As

‚àóNow at Beihang University, Beijing, China

a result, there has been and continues to be an interest in
frameworks designed to make EAs and LGP methods more
accessible to a wider audience. With such frameworks, ex-
perimentation demonstrates how these methods fare in new
problem spaces. However, the finer details of their implemen-
tation - their coding language, provided examples, overall
structure - bring trade-offs that tend toward complexity and
difficulty in comprehension [8]. In this paper, we introduce
the Shackleton Framework, a generic LGP framework that
intends to tackle some of these shortcomings.

Shackleton is a generalized framework that allows for
the exploration of applying LGP to novel use cases with
minimal background knowledge. The aim of the initial cre-
ation of Shackleton and this paper is two-fold: (i) to explore
the capabilities and usability of a generic LGP framework
written entirely in the C language, and (ii) to assess the
performance of such a framework on the optimization of a
practical and complex use-case: the optimization of LLVM
compiler optimization pass sequences. The choice of C as
the programming language for this project is motivated by
its efficiency and its rarity as a language for EA methods
(explored more in Section 2). Compiler optimization passes
are also a rare target for EAs despite the creation of effective
pass sequences being a problem-space exploration problem.
Through creation of the framework and hyperparameter ex-
perimentation, we explore the implications of these choices
and provide an analysis of the optimization sequences that
Shackleton‚Äôs evolutionary scheme generates.

This paper describes the framework in detail, with an em-
phasis on what distinguishes it from other LGP methods.
Section 2 discusses related work and important background
topics to understand. Section 3 introduces Shackleton in
full, outlines the problem space to be explored, and the hy-
perparameter tuning methodology used. Section 4 shares
the setup and methodology, results, and discussion on four

 
 
 
 
 
 
distinct experiments. Finally, section 5 concludes and sum-
marizes this paper, and outlines potential future applications.

2 Background and Related Work
The work presented in this paper is a combination of method,
implementation, and target application, but is built upon
structures created by other researchers and software devel-
opers.

Due to the often problem-specific nature of EAs, a com-
mon task explored is the creation of EA frameworks that
decrease barriers to experimentation. Some example frame-
works are generalized, covering many methods. No matter
the method, these frameworks offer advantages over writing
entire solutions from scratch. One such framework is the
Distributed EAs in Python (DEAP) package [8], which uti-
lizes the increasingly popular Python language with a wide
range of features. There are other frameworks that focus
specifically on GP but with more novel language choices,
such as PushGP [21] and its derivatives: PyshGP [17] writ-
ten in Python, and Clojush [20] written in Clojure. ‚ÄòPush‚Äô
itself is a language written specifically to support GP exper-
imentation and operators. Shackleton shares this attribute
with Push, but is written entirely in C to leverage its level of
memory control and runtime efficiency. Paired with Shack-
leton‚Äôs customizability through the integrated editor tool,
our framework is unique amongst GP frameworks as well as
being specifically a linear GP framework.

While Shackleton is by design a general framework that
can be used with multiple target applications, it was designed
with a target of compiler optimization sequences in mind.
A compiler completes the sole task of translating computer
code written in one source language to a target language in
an executable form. In contrast, an optimizing compiler tries
to also minimize or maximize attributes of the outputted
computer program in the target language to achieve some
benefit at execution time [1]. For much of the history of op-
timized compilation, decisions regarding optimization were
made based on a priori static information. Recent research
aims to improve this by targeting machine learning based
compilation, leveraging the exploration capabilities and in-
crease in popularity of machine learning techniques at the
compilation stage [25]. GP and GAs as methods are no excep-
tion to this. A genetic algorithm approach has been applied
to GNU Compiler Collection (GCC), a compiler similar to
LLVM but differentiated by its fixed number of supported
languages and reuse constraints [3, 10]. Further, there is
precedent for applying GP specifically to a compiler context
to leverage GP‚Äôs optimization potential for well-known com-
piler heuristics [22]. Shackleton builds on this motivation by
adding another layer of specificity: optimizing a particular
stage of the LLVM Compiler Project‚Äôs core framework.

2.1 Linear Genetic Programming

Shackleton as a system relies on a method called LGP which
descends from a long line of computational concepts and
techniques. To understand LGP, it is important to under-
stand the evolution of methods that resulted in its definition.
LGP shares a common ancestor with methods like cellular
automata and neural computation, that are nature-inspired or
bio-inspired computation. This term encompasses a wide va-
riety of methods ranging from those that simulate biological
natural phenomena in digital forms to methods that actively
employ natural materials in their execution. Within the class
of methods that takes inspiration from biology and aims to
simulate some biological phenomena, we find a subset of
methods known as Evolutionary Computation (EC) or EAs
[19].

GP is an ‚ÄúEC technique that automatically solves problems
without requiring the user to know or specify the form or
structure of the solution in advance‚Äù [18]. The Field Guide to
GP offers that ‚Äúat the most abstract level GP is a systematic,
domain-independent method for getting computers to solve
problems automatically starting from a high-level statement
of what needs to be done‚Äù. This compelling idea spawns a
myriad of different sub-methods and projects. LGP is one
such method where programs are linear sequences of instruc-
tions, leveraging the fact that computers often represent and
run programs in a linear fashion. This same concept is used
in Shackleton.

2.2 The LLVM Project

The LLVM Project is a collection of modular and reusable
compiler and tool-chain technologies. LLVM utilizes its own
intermediate representation (LLVM IR) in its core libraries
and processes, allowing its compilation process to be largely
source- and target-independent [14]. During the compila-
tion process, the LLVM IR is optimized using sequences of
LLVM passes, optimizations that traverse some portion of
the program to either collect information (analysis passes)
or transform the program (transform passes). This process
and specifically the included passes and their order in the
final pass sequence is central to the work discussed in this
paper. Creating effective pass sequences typically requires
prior knowledge of LLVM. Without this knowledge, the
next alternative is to use often sub-optimal default pass se-
quences. Shackleton can create custom pass sequences for
a specific application automatically, without the need for
domain-specific or LLVM-specific knowledge.

For the purposes of this work, Shackleton will not explore
the entirety of the LLVM optimization pass search space.
Analysis passes that are targeted for debugging or visual-
ization purposes are omitted by design. This omission is to
avoid conflicts with automation of the framework. All util-
ity passes are also omitted, due to their small number, their

2

considerable focus on visualization, and odd function com-
pared to analysis and transform passes. Additional passes are
omitted on a case-by-case basis depending on observed be-
havior in experimentation. Custom sequences created by the
framework are compared against all the default sequences
provided in a base LLVM installation.

3 Methods
3.1 Problem Space

Shackleton is designed to be applied to various use cases,
and here we focus on the LLVM use case as described above.
The framework takes in the source code of a target program,
and outputs the near-optimal optimization pass sequence
that the LLVM compiler can use to optimize this program.
In our experiments, Ant Colony Optimization for the Trav-
eling Salesman Problem (ACOTSP) [23] and the Backtrack
Algorithm for the Subset Sum Problem (SSP) [15] are used
as the target programs.

The Ant Colony Optimization (ACO) algorithm is inspired
by the foraging behavior of some ant species [7], in which
the path with more visits would be reinforced after each
iteration. In the traveling salesman problem, a graph of cities
and the distance between each pair of cities are given, and
the goal is to find a path in the topology that minimizes total
distance travelled.

In the SSP, we are given a set of ùëÅ numbers from 1 to
1,000,000, and we are asked to find a subset that sums up to a
random number ùëã (where 1, 000, 000 < ùëã < ùëÅ √ó 1, 000, 000)
[15][6][2][12]. Although it might run into halting states,
the backtracking algorithm approximates the solution in a
reasonable time. Both ACOTSP and SSP are NP-complete
problems, as they do not have polynomial-time solutions.
Therefore, optimizing the algorithms from the compiler level
is of great importance.

3.2 The Shackleton Framework

Shackleton is a generic GP framework that aims to make
GP easier for a myriad of uses. Currently, the main target
of Shackleton is to use the framework for optimization of
LLVM pass sequences that ultimately optimize executable
code. The source code for Shackleton is publicly available
on Arm‚Äôs GitHub page.

Shackleton is underpinned by a generic data structure that
represents what will be evolved in Shackleton. Any object
type can reside in this structure, an attractive attribute given
how often problem-specific implementations are needed to
achieve favorable results [13]. A unique offering of Shack-
leton is the editor tool that enables new use cases to be
formulated for the tool with ease: this may be explored in
more detail in supplemental materials.

3.2.1 Genetic Programming Design in Shackleton. We
now describe the implementation of LGP in the Shackleton
Framework in detail, which includes elitism, new individuals,

Figure 1. Novel Genetic Programming

and a special offspring selection mechanism as shown in FIG
1. In the LLVM flag use case of Shackleton, each chromosome
consists of a sequence of optimization flags. First, a popula-
tion of random individuals is generated. For each sequence,
its fitness is calculated by compiling the program with that se-
quence of passes and averaging the program runtime over 40
runs. In each generation, a portion of the individuals with the
best fitness scores are chosen as the elite group and copied
over to the next generation unchanged. Then, in the special
offspring selection process, a tournament-style selection ap-
proach is used to select parent individuals. Genetic operators
including one-point crossover and one-point substitution
mutation are applied to their genomes to produce a ‚Äònest‚Äô of
4 offspring individuals; in each nest, the two best performing
individuals out of the four offspring and the two parents are
selected to join the next generation. This special offspring
selection mechanism is inspired by Tacket‚Äôs brood selection
[24]. Finally, some new individuals are randomly generated
to be included in the next generation in order to preserve

3

Figure 2. Offspring Selection Process. This does not show a deterministic outcome that will always occur, but rather a possible
outcome from a single iteration of the stochastic process. The coloration represents the genetic identity of an individual: the
same color means identical genetic materials, and gradients represent a mixing of material from multiple individuals.

diversity in the population. The fitness of the new genera-
tion is then evaluated and evolution continues. Throughout
this process, the genes that would not contribute to faster
programs are gradually eliminated from the population, as
they are more likely to possess genomes with worse fitness
values. The program stops when the termination criteria are
met and it outputs individuals with the best fitness scores
as the final solution. For the experiments that are run in
this paper, the termination criterion is when the number of
generations reaches the set limit, but other implementations
(e.g., converging fitness values) are also available and can be
further explored.

3.3 Hyperparameter Tuning

Shackleton‚Äôs algorithm requires that a selection of hyperpa-
rameters be set ahead of runtime. These hyperparameters
affect the structure of the evolutionary process (i.e., number
of generations, population size, and tournament size) and
the balance of the application of evolutionary operators (i.e.,
mutation, and crossover). There are additional hyperparame-
ters such as the percentage of elite individuals carried to the
next generation, nest size in the special offspring selection
mechanism, length of each individual, as well as different
fitness metrics. It is often unclear what the optimal com-
bination of hyperparameters is for a given application or
implementation.

In the experimental section of this paper, we first test
out extreme combinations of the number of generations and

4

number of individuals to determine the effect of these param-
eters. Then, all hyperparameter values are fixed at a moder-
ate level to explore three algorithmic designs that consist of
different ways to calculate the fitness value of each individ-
ual. Lastly, the effects of crossover rate, mutation rate, and
tournament size are explored in the hyperparameter tuning
process using the Taguchi Method [9]. With the Taguchi De-
sign, as few as 16 sets of parameter combinations are needed
to explore 4 levels of the three parameters, which would re-
quire 64 experiments in a regular factorial design. We chose
these specific hyperparameters because they were found,
through preliminary testing, to influence the optimization
significantly.

The experiments were conducted on HPCC nodes running
CentOS Linux version 7 and Clang version 8.0.0. We utilized
the Backtrack Algorithm (from SSP), which contains recur-
sive loops that are potential sites of optimization. Preliminary
testing found this problem to be of adequate complexity for
our experimentation purposes, considering both the runtime
and human-verifiable testability.

4 Experiments
4.1 Number of Generations vs. Population Size
4.1.1 Experiment Setup. The number of generations and
population size directly determine the number of runtime
evaluations. These two hyperparameters are roughly pro-
portional to how many individuals will appear throughout
the evolutionary process. Therefore, in order to reduce the
overhead runtime of Shackleton, it is essential to determine

Table 1. Number of Generations vs. Population Size
Experiment Parameters

Experiment Nr. Number of Generations Population Size

0
1
2
3
4
5

50
250
200
10
8
4

40
8
10
200
250
500

how much these two factors influence the search process of
the optimal solution. In our first experiment, we tracked the
fitness of the best individual in each generation with extreme
settings of number of generations and population size. This
is to see whether more computing power and runtime should
be allocated to a large number of generations or a larger pop-
ulation size, when their product is fixed. The values are set
such that each parameter setting has a constant product, so
the same number of total individuals are represented in the
entire run of Shackleton.

Table 1 shows the combinations of parameters used for
this experiment. Six trials were run for each setting and other
hyperparameters are kept constant.

4.1.2 Results. FIG 3 plots the control set up with 50 gen-
erations and a population size of 40; FIG 4 and FIG 5 plot two
representative cases of a setting with a large num_generation
value and a setting with a large population_size value. The
ùë¶-axis is the runtime of the sample use case - the SSP with
the Backtrack Algorithm - in seconds, which we use as the
fitness of the individuals, and the ùë•-axis is the generation
number. The runtime of the program with no optimization
and with default LLVM optimization flags (first 8 data points
of each trend line) is plotted with the runtime of the in-
dividuals across generations. FIG 6 shows the percentage
improvement of the parameter settings compared to each of
the baseline optimization levels.

4.1.3 Discussion. By exploring the extreme ends of the
search space, this experiment shows that more computing
power and runtime should be allocated to the population size
rather than the number of generations. In the hyperparame-
ter setting plotted in FIG 4, a small population size of 10 is
allowed to undergo 200 generations in search for the optimal
solution, but the fitness of the best individual is noisy and
does not tend to converge, with a final percent improvement
of 4.08%. On the other hand, a large population size of 200
shows converging fitness within just 10 generations and a
final percent improvement of 5.25%. This result gives support
to the claim that the greater diversity in a large population
is crucial in the search for an optimal solution.

Figure 3. gen=50, pop=40

Figure 4. gen=250, pop=8

Figure 5. gen=10, pop=200

5

Table 3. Default Hyperparameter Settings

Hyperparameter

Value

number of generations
population size
crossover probability
mutation probability
tournament size
elite percentage
nest size

50
40
50
50
4
10
6

Figure 6. Improvement over Baseline

Table 2. Fitness Calculation Experiment Setup

Experiment Nr.
6
7
8

Fitness Hashing % Improvement
runtime
time+var
runtime

11.29
1.27
3.09

Yes
Yes
No

4.2 Fitness Calculation
4.2.1 Experiment Setup. The framework is implemented
with three algorithmic designs that feature different ways
of calculating the fitness scores, as well as re-evaluating
repeated individuals. In calculating the runtime of each in-
dividual, the optimization sequence is applied to the source
code and the runtime of the code is taken as the average
across 40 runs. Observations from preliminary testing indi-
cate that runtime across different runs can vary considerably.
Therefore, we first hypothesize that using the sum of the
average runtime and the variance as the fitness might prefer
individuals with more consistent runtime and lead to better
optimization. We also hypothesize that for the same optimiza-
tion sequence, storing the runtime into a hash table instead
of re-doing the 40 runs each time an individual appears in
the population could decrease overall runtime. This section
explores the impacts of these two potential improvements
in terms of algorithmic design.

For each fitness calculation, all hyperparameters are kept
at a moderate level as shown in Table 3. Table 2 highlights
the different designs.

4.2.2 Results. The fitness across generations and the over-
head runtime of Shackleton are recorded as the key metrics
of this experiment. FIG 7 shows the percent improvement
of fitness compared to the default LLVM optimization levels.
Fitness evaluation using only the runtime shows a signifi-
cant advantage compared to using both the runtime and the
variance across runs. FIG 8 shows the overhead runtime of
each design taken as the average across six trials.

Figure 7. Percent Improvement of Different Fitness Calcula-
tions

Figure 8. Overhead Runtime of Different Fitness Calcula-
tions

4.2.3 Discussion. We compared experiment #6 and exper-
iment #7, which differ only in that the fitness includes the
variance of the runs in experiment #7. Experiment #6 is able
to achieve a larger percentage improvement. This implies
that the variance amongst runs of a program is not relevant
to the optimization sequence applied to it, but rather due to
the inherent fluctuations of clock speed caused by sharing
of resources on the same computing cluster. Therefore, in-
cluding the variance into the fitness function would not help

6

Table 4. Parameter Tuning Experiment Setup

Table 5. Top Hyperparameter Combinations

Improvement

Baseline

Top 1

Top 2

Top 3

Exp. Crossover Mutation Tournament
Nr.

Size

09
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24

20
20
20
20
40
40
40
40
60
60
60
60
80
80
80
80

20
40
60
80
20
40
60
80
20
40
60
80
20
40
60
80

2
4
6
8
4
2
8
6
6
8
2
4
8
6
4
2

5.44%
0.50%
7.04%
1.91%
4.88%
3.49%
5.76%
4.90%
8.31%
4.18%
3.43%
10.88%
5.62%
5.53%
7.21%
6.83%

reduce the noise in the data and could distract the population
from moving toward a sequence with shorter runtime.

For experiment #8, where the fitness is re-evaluated and
averaged every time an individual appears in the population,
the more comprehensive fitness calculation greatly increases
Shackleton‚Äôs overhead runtime. However, as shown by the
percent improvement after the genetic algorithm, this did
not lead to a more effective evolution process. Therefore, the
improvement of looking up the fitness value from a hash
table helps considerably with the overall runtime and does
not sacrifice performance. In future experiments, the fitness
will be calculated the same way as it is in experiment #6.

4.3 Hyperparameter Tuning
4.3.1 Experiment Setup. In this experiment, the optimal
hyperparameter combination for percent crossover, percent
mutation, and tournament size is explored using a Taguchi
design [9]. Other hyperparameters are kept constant: 50
generations, population size = 40, elite percentage = 15, nest
size = 6, and individual size = 0 (When individual size is
set to 0, individuals will be randomly generated with length
between 10 and 90. Single-point mutation keeps the length
constant, and crossover changes the length of the offspring).
The three independent hyperparameters are tested across 4
equidistant levels; the Taguchi Method effectively reduces
the number of combinations needed to 16 compared to 43 =
64 in a regular factorial grid search design.

4.3.2 Results. Table 5 shows the three hyperparameter
combinations that yield the highest percent improvement in
terms of runtime when compared against each default LLVM
optimization level. From Table 4, it can be concluded that

No opt
Improvement
O0
Improvement
O1
Improvement
O2
Improvement
O3
Improvement
Os
Improvement
Oz
Improvement
Average
Improvement

(60,80,4)
19.12%
(60,80,4)
17.17%
(60,80,4)
18.31%
(60,80,4)
15.27%
(60,80,4)
14.53%
(60,80,4)
16.01%
(60,80,4)
18.83%
(60,80,4)
17.04%

(60,20,6)
18.20%
(40,20,4)
11.13%
(20,60,6)
10.83%
(80,60,4)
8.31%
(20,60,6)
8.24%
(80,80,2)
8.22%
(60,20,6)
8.19%
(60,20,6)
9.69%

(80,60,4)
10.43%
(60,20,6)
9.17%
(80,60,4)
10.46%
(80,80,2)
7.88%
(60,20,6)
7.18%
(60,20,6)
8.14%
(80,60,4)
8.00%
(80,60,4)
8.57%

Figure 9. Top 3 Hyperparameter Combinations

the combination (60,80,4) from experiment 20 - crossover
probability 60%, mutation probability 80%, and tournament
size of 4 - is the optimal hyperparameter combination as it
is consistently the top-performing combination compared
against any default optimization level. Examining the top
3 hyperparameter combinations, we can see that (60,20,6)
and (80,60,4) also give consistently good optimization. FIG 9
and 10 shows the percent improvement in terms of runtime
compared with the baseline optimizations levels using the
above three best hyperparameter combinations. The over-
head runtime averaged over six trials for each parameter
setting is plotted in FIG 11, with the top three performing
hyperparameter combinations in blue and others in orange.
Lastly, a Taguchi Analysis [9] is performed to analyze the
importance factor of the three hyperparameters, and the
results are shown in Table 6.

7

Figure 10. Top 3 Hyperparameter Combinations

Figure 11. Overhead Runtime of Shackleton Framework

Table 6. Taguchi Analysis - Improvement Signal-to-
Noise Ratio

Crossover Mutation Tournament Size

L1
L2
L3
L4
Œî

-7.92
3.26
5.01
3.87
12.93

2.14
-4.94
7.44
-0.41
12.38

7.09
-13.00
4.08
6.06
20.10

Table 7. Taguchi Analysis - Overhead Runtime Signal-
to-Noise Ratio

Crossover Mutation Tournament Size

L1
L2
L3
L4
Œî

48.84
46.63
43.39
43.88
5.45

49.71
47.53
43.78
41.71
8.00

44.25
48.60
42.20
47.68
6.40

4.3.3 Discussion. From Table 5, it can be observed that
the top hyperparameter configuration when compared to
any baseline optimization level is when crossover probability
equals 60, mutation probability equals 80, and tournament
size equals 4. Most hyperparameter combinations that made
it to the top 3 when compared to any baseline level have a
high rate of crossover, high rate of mutation, and a moder-
ate tournament size. Parents from the previous generation
are selected in a tournament style, where ùëõ individuals will
be randomly chosen from the entire population, and the
individual with the best fitness is taken as the origin of a
genetic operator. A higher tournament size will result in the
parent being selected to have a higher fitness, but setting
it too high will reduce population diversity as it results in
the same individuals being selected every time. Crossover
and mutation are genetic operators that make changes from
the original parents. High crossover and mutation probabili-
ties contributes to the diversity of the offspring population.
Therefore, a moderate tournament size ensures that ‚Äògood
enough‚Äô individuals are selected as parents. High crossover
and mutation probabilities maintain diversity in the popula-
tion so that the search can keep moving towards the optimal
solution.

From FIG 11, the overhead runtime roughly correlates to
the level of the mutation probability. However, the Taguchi
Analysis [9] in Table 6 shows that tournament size has the
largest effect on the optimization process. A larger range
means that a change of this hyperparameter can cause a
larger effect on the measurement output. Table 7 shows the
same analysis done with respect to the overhead runtime of
the Shackleton Framework. The range of the signal-to-noise
ratio for each hyperparameter is fairly similar, implying that
each hyperparameter has a similar level of impact on the
overhead runtime. The above information should also be
taken into consideration when choosing the best hyperpa-
rameter combination.

Looking at both the performance and runtime metrics
and the results from the Taguchi Analysis, experiment #17
(crossover probability = 60, mutation probability = 20, and
tournament size = 6) is among the top 3 hyperparameter
combinations with highest percent improvement and has a
reasonable runtime. The high crossover probability main-
tains the diversity in the population; low mutation proba-
bility contributes to the fast overhead runtime; and a large
tournament size selects efficient parents without reducing
diversity.

4.4 Robustness Analysis
4.4.1 Experiment Setup. Even though the hyperparame-
ter settings are problem-specific and can be set by the user,
it is important to make sure that most (if not all) hyperpa-
rameter combinations will speed up the execution of the
target source code. In this experiment, each of the 16 hyper-
parameter combinations from Table 4 is run 30 times, and

8

Figure 12. Percent Optimization of All Hyperparameter Set-
tings (compared to raw source code)

Figure 13. Percent Optimization of All Hyperparameter Set-
tings (compared to raw source code)

Table 8. Percent Improvement

Comparison

Average % Improvement Additional %

O2-raw
Shackleton-raw
Shackleton-O2

1.40
6.20
4.77

74.09
93.88
90.17

the data from each of these 16 √ó 30 = 480 runs are collected
to examine the robustness of Shackleton.

4.4.2 Results. Two metrics are of importance in this eval-
uation - percent optimization from the raw source code and
the percent improvement of the automatically-generated
sequence compared to the default optimization. We first
want to ensure that Shackleton will produce optimization
sequences that speed up the execution of the target source
code. Then, we want the automatically-generated sequences
to perform better than the default LLVM optimization.

In FIG 12, the percent optimization produced by the de-
fault LLVM -O2 sequence is plotted; 74% of these runs pro-
duce faster code compared to the unoptimized source code.
The histogram in FIG 13 shows the percent optimization
produced by the Shackleton-generated sequence; 94% of
these runs produce faster code compared to the unoptimized
source code. Lastly, the histogram in FIG 14 shows the per-
cent improvement compared to the default -O2 LLVM com-
piler flag; 90% of the Shackleton-generated optimization runs
are faster than the default optimization runs. Outlier values
(more than 3 standard deviations from the mean) are removed
from the analysis because these data points are not taken as
an average among trials, making it more prone to system
fluctuations. The experimental results are summarized in
Table 8.

Figure 14. Percent Improvement of All Hyperparameter
Settings (compared to O2)

4.4.3 Discussion. Shackleton‚Äôs use of stochastic elements
in its evolutionary scheme mean that speed improvements
over the raw source code after optimization are not always
guaranteed. However, the experiments shown here indicate
that Shackleton consistently produces optimized code that is
faster and offers a larger average improvement than that pro-
duced by the default LLVM optimization sequences. These
experiments are not exhaustive and notably omit some hyper-
parameters. More comprehensive analysis of these omitted
hyperparameters and additional scaling of the parameters
shown here could offer additional insights into benefits and
limiting factors. Based on our experiments, with fine-tuned
hyperparameters the optimization pass sequences automat-
ically generated by Shackleton can be expected to achieve
even better performance.

9

5 CONCLUSION AND FUTURE WORK
The existing predefined LLVM optimization levels (-Ox) are
used for general purpose program optimization and are not
problem specific. Targeted runtime optimization of a pro-
gram by a hand-crafted LLVM pass or a pass sequence would
require expert knowledge in both LLVM and the program
to be optimized. In this paper, we presented the Shackleton
Framework, which is able to automatically generate near-
optimal LLVM optimization flag sequences. The Shackleton
Framework, without any prior knowledge of the compiler,
the optimization passes, nor the program, consistently pro-
duces optimization pass sequences that achieve significant
runtime improvement over default optimization options. We
first proposed an elitist approach and a special offspring
selection mechanism for a more greedy genetic algorithm,
while a new random offspring mechanism ensures diversity
in the population. We also performed hyperparameter tun-
ing and explored the effect of each hyperparameter on the
optimization results and overhead runtime.

There are numerous opportunities to further improve and
analyze the Shackleton Framework. Shackleton currently
only supports a relatively small subset of LLVM optimization
passes and only compares against the default optimization
passes for analysis. Widening the scope of passes included
and allowing for comparison against bespoke sequences fit
for a specific use case could offer benefits beyond what is
shown here. LLVM also allows programmers to create their
own passes to leverage attributes of their specific applica-
tions more effectively than the base set of passes. This bene-
fit of the open-source framework could be integrated into
Shackleton to generate better sequences faster. In addition
to benefits for the LLVM use case, the generic Shackleton
Framework could be improved by allowing for hyperparam-
eter tuning beyond what is currently supported. Notably,
the brood/nest size and selection percentage for the spe-
cial offspring selection mechanism are currently fixed. Al-
lowing these parameters to vary or be configured by hand
would offer better control over the evolutionary process
and may provide more refined results. As an open source
framework, Shackleton is designed to be experimented with
and expanded upon. With further improvements, the pass
sequences automatically generated by the Shackleton Frame-
work could potentially surpass hand-crafted pass sequences
while simultaneously freeing developers‚Äô time to pursue
other advancements for compilers.

Acknowledgments
This work was generously funded by the EnSURE (Engineer-
ing Summer Undergraduate Research Experience) program
at Michigan State University (MSU) as well as the John R.
Koza Endowment. We also gratefully acknowledge The Insti-
tute of Cyber-Enabled Research (ICER) at MSU for providing

10

the hardware infrastructure that made the computation re-
quired to complete this work possible.

References
[1] Alfred V. Aho, Monica S. Lam, Ravi Sethi, and Jeffrey D. Ullman. 2006.
Compilers: Principles, Techniques, and Tools (2nd Edition). Addison
Wesley, 75 Arlington Street Suite 300 Boston MA 02116 USA.

[2] Michael Alekhnovich, Allan Borodin, Joshua Buresh-Oppenheim, Rus-
sell Impagliazzo, Avner Magen, and Toniann Pitassi. 2011. Toward a
model for backtracking and dynamic programming. Computational
Complexity 20, 4 (2011), 679‚Äì740.

[3] Prathibha A. Ballal, H. Sarojadevi, and Harsha P S. 2015. Article:
Compiler Optimization: A Genetic Algorithm Approach. International
Journal of Computer Applications 112, 10 (February 2015), 9‚Äì13.
[4] Wolfgang Banzhaf, Peter Nordin, Robert E. Keller, and Frank Francone.
1998. Genetic Programming ‚Äì An Introduction. Morgan Kaufmann,
Morgan Kaufmann Publishers 340 Pine Street, 6th Floor San Francisco,
CA 94104 USA.

[5] Markus F. Brameier and Wolfgang Banzhaf. 2007. Linear Genetic
Programming. Springer, Salmon Tower Building, New York City, New
York, USA.

[6] Pinar Civicioglu. 2013. Backtracking search optimization algorithm
for numerical optimization problems. Appl. Math. Comput. 219, 15
(2013), 8121‚Äì8144.

[7] Marco Dorigo, Mauro Birattari, and Thomas Stutzle. 2006. Ant colony
optimization. IEEE Computational Intelligence magazine 1, 4 (2006),
28‚Äì39.

[8] F√©lix-Antoine Fortin, Fran√ßois-Michel De Rainville, Marc-Andr√© Gard-
ner, Marc Parizeau, and Christian Gagn√©. 2012. DEAP: Evolutionary
Algorithms Made Easy. Journal of Machine Learning Research 13 (jul
2012), 2171‚Äì2175.

[9] Alessandro Freddi and Mario Salmon. 2019. Introduction to the Taguchi
Method: From Conceptualization to First Prototyping with Examples and
Case Studies. Springer, Salmon Tower Building New York City, New
York, USA, 159‚Äì180. https://doi.org/10.1007/978-3-319-95342-7_7

[10] William von Hagen. 2006. The Definitive Guide to GCC, Second Edition

(Definitive Guide). Apress, USA.

[11] John R. Koza. 1992. Genetic Programming. MIT Press, 12th floor of

One Broadway, in Cambridge, MA 02142.

[12] Jeffrey C Lagarias and Andrew M Odlyzko. 1985. Solving low-density
subset sum problems. Journal of the ACM (JACM) 32, 1 (1985), 229‚Äì246.
[13] Alexander Leonhardi, Wolfgang Reissenberger, Tim Schmelmer,
Karsten Weicker, and Nicole Weicker. 1998. Development of problem-
specific evolutionary algorithms. In Parallel Problem Solving from Na-
ture ‚Äî PPSN V, Agoston E. Eiben, Thomas B√§ck, Marc Schoenauer,
and Hans-Paul Schwefel (Eds.). Springer Berlin Heidelberg, Berlin,
Heidelberg, 388‚Äì397.

[14] llvm-admin team. 2021. The LLVM Compiler Infrastructure. https:

//llvm.org.

[15] Parth Shirish Nandedkar. 2019. SubsetSum-BacktrackAlgorithm. https:

//github.com/parthnan/SubsetSum-BacktrackAlgorithm.

[16] Mihai Oltean and Crina Grosan. 2003. A Comparison of Several Linear

GP Techniques. Complex Systems 14 (01 2003), 282‚Äì313.

[17] Eddie Pantridge. 2020. pyshgp. https://github.com/erp12/pyshgp.
[18] Riccardo Poli, William Langdon, and Nicholas Mcphee. 2008. A Field
Guide to Genetic Programming. Lulu Enterprises, UK Ltd, 73a High
Street, Egham, United Kingdom, TW20 9HE.

[19] Grzegorz Rozenberg, Thomas B√§ck, and Joost N. Kok (Eds.). 2012.
Handbook of Natural Computing. Springer-Verlag Berlin Heidelberg,
Berlin, Heidelberg.

[20] Lee Spector. 2015. Clojush. https://github.com/lspector/Clojush.
[21] Lee Spector. Last accessed 2022. Evolutionary Computing with Push.

http://faculty.hampshire.edu/lspector/push.html

[22] Mark Stephenson, Una-May O‚ÄôReilly, Martin C. Martin, and Saman
Amarasinghe. 2003. Genetic Programming Applied to Compiler Heuris-
tic Optimization. In Genetic Programming, Conor Ryan, Terence Soule,
Maarten Keijzer, Edward Tsang, Riccardo Poli, and Ernesto Costa (Eds.).
Springer Berlin Heidelberg, Berlin, Heidelberg, 238‚Äì253.

[23] Thomas St√ºtzle. 2002. ACOTSP: A software package of various ant
colony optimization algorithms applied to the symmetric traveling
salesman problem. URL http://www. aco-metaheuristic. org/aco-code

(2002).

[24] Walter A. Tackett and A. Carmi. 1994. The unique implications of
brood selection for genetic programming. In Proceedings of the First
IEEE Conference on Evolutionary Computation. IEEE World Congress on
Computational Intelligence, Vol. 1. IEEE, Orlando, Florida, 160‚Äì165.

[25] Zheng Wang and Michael O‚ÄôBoyle. 2018. Machine learning in compiler

optimization. Proc. IEEE 106, 11 (2018), 1879‚Äì1901.

11

