0
2
0
2

r
p
A
8
1

]

G
L
.
s
c
[

1
v
8
8
6
8
0
.
4
0
0
2
:
v
i
X
r
a

Published as a conference paper at ICLR 2020

LIPSCHITZ CONSTANT ESTIMATION OF NEURAL NET-
WORKS VIA SPARSE POLYNOMIAL OPTIMIZATION

Fabian Latorre, Paul Rolland and Volkan Cevher
EPFL, Switzerland
firstname.lastname@epfl.ch

ABSTRACT

We introduce LiPopt, a polynomial optimization framework for computing in-
creasingly tighter upper bounds on the Lipschitz constant of neural networks. The
underlying optimization problems boil down to either linear (LP) or semideﬁnite
(SDP) programming. We show how to use the sparse connectivity of a network,
to signiﬁcantly reduce the complexity of computation. This is specially useful for
convolutional as well as pruned neural networks. We conduct experiments on net-
works with random weights as well as networks trained on MNIST, showing that
in the particular case of the (cid:96)∞-Lipschitz constant, our approach yields superior
estimates, compared to baselines available in the literature.

1

INTRODUCTION

We consider a neural network fd deﬁned by the recursion:

f1(x) := W1x

fi(x) := Wi σ(fi−1(x)),

i = 2, . . . , d

(1)

for an integer d larger than 1, matrices {Wi}d
i=1 of appropriate dimensions and an activation function
σ, understood to be applied element-wise. We refer to d as the depth, and we focus on the case where
fd has a single real value as output.

In this work, we address the problem of estimating the Lipschitz constant of the network fd. A
function f is Lipschitz continuous with respect to a norm (cid:107)·(cid:107) if there exists a constant L such that
for all x, y we have |f (x) − f (y)| ≤ L(cid:107)x − y(cid:107). The minimum over all such values satisfying this
condition is called the Lipschitz constant of f and is denoted by L(f ).

The Lipschitz constant of a neural network is of major importance in many successful applications
of deep learning. In the context of supervised learning, Bartlett et al. (2017) show how it directly
correlates with the generalization ability of neural network classiﬁers, suggesting it as model com-
plexity measure. It also provides a measure of robustness against adversarial perturbations (Szegedy
et al., 2014) and can be used to improve such metric (Cisse et al., 2017). Moreover, an upper bound
on L(fd) provides a certiﬁcate of robust classiﬁcation around data points (Weng et al., 2018).

Another example is the discriminator network of the Wasserstein GAN (Arjovsky et al., 2017),
whose Lipschitz constant is constrained to be at most 1. To handle this constraint, researchers have
proposed different methods like heuristic penalties (Gulrajani et al., 2017), upper bounds (Miyato
et al., 2018), choice of activation function (Anil et al., 2019), among many others. This line of work
has shown that accurate estimation of such constant is key to generating high quality images.

Lower bounds or heuristic estimates of L(fd) can be used to provide a general sense of how robust a
network is, but fail to provide true certiﬁcates of robustness to input perturbations. Such certiﬁcates
require true upper bounds, and are paramount when deploying safety-critical deep reinforcement
learning applications (Berkenkamp et al., 2017; Jin & Lavaei, 2018). The trivial upper bound given
by the product of layer-wise Lipschitz constants is easy to compute but rather loose and overly
pessimistic, providing poor insight into the true robustness of a network (Huster et al., 2018).

Indeed, there is a growing need for methods that provide tighter upper bounds on L(fd), even at the
expense of increased complexity. For example Raghunathan et al. (2018a); Jin & Lavaei (2018); Fa-
zlyab et al. (2019) derive upper bounds based on semideﬁnite programming (SDP). While expensive

1

 
 
 
 
 
 
Published as a conference paper at ICLR 2020

to compute, these type of certiﬁcates are in practice surprisingly tight. Our work belongs in this vein
of research, and aims to overcome some limitations in the current state-of-the-art.

Our Contributions.

(cid:46) We present LiPopt, a general approach for upper bounding the Lipschitz constant of a neu-
ral network based on a relaxation to a polynomial optimization problem (POP) (Lasserre,
2015). This approach requires only that the unit ball be described with polynomial inequal-
ities, which covers the common (cid:96)2- and (cid:96)∞-norms.

(cid:46) Based on a theorem due to Weisser et al. (2018), we exploit the sparse connectivity of
neural network architectures to derive a sequence of linear programs (LPs) of considerably
smaller size than their vanilla counterparts. We provide an asymptotic analysis of the size
of such programs, in terms of the number of neurons, depth and sparsity of the network.
(cid:46) Focusing on the (cid:96)∞-norm, we experiment on networks with random weights and networks
trained on MNIST (Lecun et al., 1998). We evaluate different conﬁgurations of depth,
width and sparsity and we show that the proposed sequence of LPs can provide tighter
upper bounds on L(fd) compared to other baselines available in the literature.

Notation. We denote by ni the number of columns of the matrix Wi in the deﬁnition (1) of the
network. This corresponds to the size of the i-th layer, where we identify the input as the ﬁrst layer.
We let n = n1 + . . . + nd be the total number of neurons in the network. For a vector x, Diag(x)
denotes the square matrix with x in its diagonal and zeros everywhere else. For an array X, vec(X)
is the ﬂattened array. The support of a sequence supp(α) is deﬁned as the set of indices j such that
αj is nonzero. For x = [x1, . . . , xn] and a sequence of nonnegative integers γ = [γ1, . . . , γn] we
denote by xγ the monomial xγ1

n . The set of nonnegative integers is denoted by N.

2 . . . xγn

1 xγ2

Remark. The deﬁnition of network (1) covers typical architectures composed of dense and convo-
lutional layers. In general, our proposed approach can be readily extended with minor modiﬁcations
to any directed acyclic computation graph e.g., residual network architectures (He et al., 2016).

2 POLYNOMIAL OPTIMIZATION FORMULATION

In this section we derive an upper bound on L(fd) given by the value of a POP, i.e. the minimum
value of a polynomial subject to polynomial inequalities. Our starting point is the following theorem,
which casts L(f ) as an optimization problem:
Theorem 1. Let f be a differentiable and Lipschitz continuous function on an open, convex subset
X of an euclidean space. Let (cid:107)·(cid:107)∗ be the dual norm. The Lipschitz constant of f is given by

L(f ) = sup
x∈X

(cid:107)∇f (x)(cid:107)∗

(2)

For completeness, we provide a proof in appendix A. In our setting, we assume that the activation
function σ is Lipschitz continuous and differentiable. In this case, the assumptions of Theorem 1 are
fulﬁlled because fd is a composition of activations and linear transformations. The differentiability
assumption rules out the common ReLU activation σ(x) = max{0, x}, but allows many others such
as the exponential linear unit (ELU) (Clevert et al., 2015) or the softplus.

Using the chain rule, the compositional structure of fd yields the following formula for its gradient:

∇fd(x) = W T
1

d−1
(cid:89)

i=1

Diag(σ(cid:48)(fi(x)))W T

i+1

(3)

For every i = 1, . . . , d − 1 we introduce a variable si = σ(cid:48)(fi(x)) corresponding to the derivative
of σ at the i-th hidden layer of the network. For activation functions like ELU or softplus, their
derivative is bounded between 0 and 1, which implies that 0 ≤ si ≤ 1. This bound together with the
deﬁnition of the dual norm (cid:107)x(cid:107)∗ := sup(cid:107)t(cid:107)≤1 tT x implies the following upper bound of L(fd):

(cid:40)

L(fd) ≤ max

tT W T
1

d−1
(cid:89)

i=1

Diag(si)W T

i+1 : 0 ≤ si ≤ 1, (cid:107)t(cid:107) ≤ 1

(4)

(cid:41)

2

Published as a conference paper at ICLR 2020

We will refer to the polynomial objective of this problem as the norm-gradient polynomial of the
network fd, a central object of study in this work.

For some frequently used (cid:96)p-norms, the constraint (cid:107)t(cid:107)p ≤ 1 can be written with polynomial inequal-
ities. In the rest of this work, we use exclusively the (cid:96)∞-norm for which (cid:107)t(cid:107)∞ ≤ 1 is equivalent
to the polynomial inequalities −1 ≤ ti ≤ 1, for i = 1, . . . , n1. However, note that when p ≥ 2 is
a positive even integer, (cid:107)t(cid:107)p ≤ 1 is equivalent to a single polynomial inequality (cid:107)t(cid:107)p
p ≤ 1, and our
proposed approach can be adapted with minimal modiﬁcations.

In such cases, the optimization problem in the right-hand side of (4) is a POP. Optimization of
polynomials is a NP-hard problem and we do not expect to have efﬁcient algorithms for solving (4)
in this general form. In the next sections we describe LiPopt: a systematic way of obtaining an
upper bound on L(fd) via tractable approximation methods of the POP (4).

Local estimation.
In many practical escenarios, we have additional bounds on the input of the
network. For example, in the case of image classiﬁcation tasks, valid input is constrained in a
hypercube. In the robustness certiﬁcation task, we are interested in all possible input in a (cid:15)-ball
around some data point. In those cases, it is interesting to compute a local Lipschitz constant, that
is, the Lipschitz constant of a function restricted to a subset.

We can achieve this by deriving tighter bounds 0 ≤ li ≤ si ≤ ui ≤ 1, as a consequence of the
restricted input (see for example, Algorithm 1 in Wong & Kolter (2018)). By incorporating this
knowledge in the optimization problem (4) we obtain bounds on local Lipschitz constants of fd. We
study this setting and provide numerical experiments in section 7.3.

Choice of norm. We highlight the importance of computing good upper bounds on L(fd) with
respect to the (cid:96)∞-norm. It is one of the most commonly used norms to assess robustness in the
adversarial examples literature. Moreover, it has been shown that, in practice, (cid:96)∞-norm robust
networks are also robust in other more plausible measures of perceptibility, like the Wasserstein
distance (Wong et al., 2019). This motivates our focus on this choice.

3 HIERARCHICAL SOLUTION BASED ON A POLYNOMIAL POSITIVITY

CERTIFICATE

For ease of exposition, we rewrite (4) as a POP constrained in [0, 1]n using the substitution s0 := (t+
1)/2. Denote by p the norm-gradient polynomial, and let x = [s0, . . . , sd−1] be the concatenation
of all variables. Polynomial optimization methods (Lasserre, 2015) start from the observation that a
value λ is an upper bound for p over a set K if and only if the polynomial λ − p is positive over K.

In LiPopt, we will employ a well-known classical result in algebraic geometry, the so-called Kriv-
ine’s positivity certiﬁcate1, but in theory we can use any positivity certiﬁcate like sum-of-squares
(SOS). The following is a straightforward adaptation of Krivine’s certiﬁcate to our setting:
Theorem 2. (Adapted from Krivine (1964); Stengle (1974); Handelman (1988)) If the polynomial
λ − p is strictly positive on [0, 1]n, then there exist ﬁnitely many positive weights cαβ such that

λ − p =

(cid:88)

(α,β)∈N2n

cαβhαβ,

hαβ(x) :=

n
(cid:89)

j=1

xαj
j (1 − xj)βj

(5)

By truncating the degree of Krivine’s positivity certiﬁcate (Theorem 2) and minimizing over all
possible upper bounds λ we obtain a hierarchy of LP problems (Lasserre, 2015, Section 9):

θk := min
c≥0,λ




λ : λ − p =



(cid:88)

cαβhαβ

(α,β)∈N2n
k






(6)

where N2n
k is the set of nonnegative integer sequences of length 2n adding up to at most k. This
is indeed a sequence of LPs as the polynomial equality constraint can be implemented by equating
coefﬁcients in the canonical monomial basis. For this polynomial equality to be feasible, the degree

1also known as Krivine’s Positivstellensatz

3

Published as a conference paper at ICLR 2020

of the certiﬁcate has to be at least that of the norm-gradient polynomial p, which is equal to the depth
d. This implies that the ﬁrst nontrivial bound (θk < ∞) corresponds to k = d.
The sequence {θk}∞
k=1 is non-incresing and converges to the maximum of the upper bound (4). Note
that for any level of the hierarchy, the solution of the LP (6) provides a valid upper bound on L(fd).

An advantage of using Krivine’s positivity certiﬁcate over SOS is that one obtains an LP hierarchy
(rather than SDP), for which commercial solvers can reliably handle a large instances. Other pos-
itivity certiﬁcates offering a similar advantage are the DSOS and SDSOS hierarchies (Ahmadi &
Majumdar, 2019), which boil down to LP or second order cone programming (SOCP), respectively.

Drawback. The size of the LPs given by Krivine’s positivity certiﬁcate can become quite large. The
dimension of the variable c is |N2n
k | = O(nk). For reference, if we consider the MNIST dataset and
(cid:12)
a one-hidden-layer network with 100 neurons we have (cid:12)
(cid:12) ≈ 9.3 × 108.
To make this approach more scalable, in the next section we exploit the sparsity of the polynomial p
to ﬁnd LPs of drastically smaller size than (6), but with similar approximation properties.

(cid:12) ≈ 1.5 × 106 while (cid:12)
(cid:12)

(cid:12)N2n
2

(cid:12)N2n
3

Remark. In order to compute upper bounds for local Lipschitz constants, ﬁrst obtain tighter bounds
0 ≤ li ≤ si ≤ ui and then perform the change of variables (cid:101)si = (si − li)/(ui − li) to rewrite the
problem (4) as a POP constrained on [0, 1]n.

4 REDUCING THE NUMBER OF VARIABLES

Many neural network architectures, like those composed of convolutional layers, have a highly
sparse connectivity between neurons. Moreover, it has been empirically observed that up to 90% of
network weights can be pruned (set to zero) without harming accuracy (Frankle & Carbin, 2019). In
such cases their norm-gradient polynomial has a special structure that allows polynomial positivity
certiﬁcates of smaller size than the one given by Krivine’s positivity certiﬁcate (Theorem 2).

In this section, we describe an implementation of LiPopt (Algorithm 1) that exploits the sparsity of
the network to decrease the complexity of the LPs (6) given by the Krivine’s positivity certiﬁcate. In
this way, we obtain upper bounds on L(fd) that require less computation and memory. Let us start
with the deﬁnition of a valid sparsity pattern:
Deﬁnition 1. Let I = {1, . . . , n} and p be a polynomial with variable x ∈ Rn. A valid sparsity
pattern of p is a sequence {Ii}m

i=1 of subsets of I, called cliques, such that (cid:83)m

i=1 Ii = I and:

(cid:46) p = (cid:80)m

i=1 pi where pi is a polynomial that depends only on the variables {xj : j ∈ Ii}

(cid:46) for all i = 1, . . . , m − 1 there is an l ≤ i such that (Ii+1 ∩ (cid:83)i

r=1 Ir) ⊆ Il

When the polynomial objective p in a POP has a valid sparsity pattern, there is an extension of
Theorem 2 due to Weisser et al. (2018), providing a smaller positivity certiﬁcate for λ − p over
[0, 1]n. We refer to it as the sparse Krivine’s certiﬁcate and we include it here for completeness:
Theorem 3 (Adapted from Weisser et al. (2018)). Let a polynomial p have a valid sparsity pattern
i=1. Deﬁne Ni as the set of sequences (α, β) ∈ N2n where the support of both α and β is
{Ii}m
contained in Ii. If λ − p is strictly positive over K = [0, 1]n, there exist ﬁnitely many positive
weights cαβ such that

λ − p =

m
(cid:88)

i=1

hi,

(cid:88)

hi =

cαβhαβ

(α,β)∈Ni

(7)

where the polynomials hαβ are deﬁned as in (5).

The sparse Krivine’s certiﬁcate can be used like the general version (Theorem 2) to derive a sequence
of LPs approximating the upper bound on L(fd) stated in (4). However, the number of different
polynomials hαβ of degree at most k appearing in the sparse certiﬁcate can be drastically smaller,
the amount of which determines how good the sparsity pattern is.

We introduce a graph that depends on the network fd, from which we will extract a sparsity pattern
for the norm-gradient polynomial of a network.

4

Published as a conference paper at ICLR 2020

Figure 1: Sparsity pattern of Proposition 1 for
a network of depth three.

Figure 2: Structure of one set in the sparsity
pattern from Proposition 1 for a network with
2D convolutional layers with 3 × 3 ﬁlters.

Deﬁnition 2. Let fd be a network with weights {Wi}d

i=1. Deﬁne a directed graph Gd = (V, E) as:

V = {si,j : 0 ≤ i ≤ d − 1, 1 ≤ j ≤ ni}
E = {(si,j, si+1,k) : 0 ≤ i ≤ d − 2, [Wi]k,j (cid:54)= 0}

(8)

which we call the computational graph of the network fd.

In the graph Gd the vertex s(i,j) represents the j-th neuron in the i-th layer. There is a directed edge
between two neurons in consecutive layers if they are joined by a nonzero weight in the network.
The following result shows that for fully connected networks we can extract a valid sparsity pattern
from this graph. We relegate the proof to appendix B.
Proposition 1. Let fd be a dense network (all weights are nonzero). The following sets, indexed by
i = 1, . . . , nd, form a valid sparsity pattern for the norm-gradient polynomial of the network fd:
Ii := (cid:8)s(d−1,i)} ∪ {s(j,k) : there exists a directed path from s(j,k) to s(d−1,i) in Gd

(9)

(cid:9)

We refer to this as the sparsity pattern induced by Gd. An example is depicted in in Figure 1.

Remark. When the network is not dense, the the second condition (Deﬁnition 1) for the sparsity
pattern (9) to be valid might not hold. In that case we lose the guarantee that the values of the
corresponding LPs converge to the maximum of the POP (4). Nevertheless, it still provides a valid
positivity certiﬁcate that we use to upper bound L(fd). In Section 7 we show that in practice it
provides upper bounds of good enough quality. If needed, a valid sparsity pattern can be obtained
via a chordal completion of the correlative sparsity graph of the POP (Waki et al., 2006).

We now quantify how good this sparsity pattern is. Let s be the size of the largest clique in a sparsity
pattern, and let Ni,k be the subset of Ni (deﬁned in Theorem 3) composed of sequences summing
up to k. The number of different polynomials for the k-th LP in the hierarchy given by the sparse
Krivine’s certiﬁcate can be bounded as follows:

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

m
(cid:91)

i=1

Ni,k

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

m
(cid:88)

i=1

(cid:19)

(cid:18)2|Ii| + k
k

= O (cid:0)msk(cid:1)

(10)

We immediately see that the dependence on the number of cliques m is really mild (linear) but the
size of the cliques as well as the degree of the hierarchy can really impact the size of the optimization
problem. Nevertheless, this upper bound can be quite loose; polynomials hαβ that depend only on
variables in the intersection of two or more cliques are counted more than once.

The number of cliques given in the sparsity pattern induced by Gd is equal to the size of the last
layer m = nd and the size of each clique depends on the particular implementation of the net-
work. We now study different architectures that could arise in practice, and determine the amount
of polynomials in their sparse Krivine’s certiﬁcate.

Fully connected networks. Even in the case of a network with all nonzero connections, the sparsity
pattern induced by Gd decreases the size of the LPs when compared to Krivine’s certiﬁcate. In this
case the cliques have size n1 + . . . + nd−1 + 1 but they all have the same common intersection equal
to all neurons up to the second-to-last hidden layer. A straightforward counting argument shows that
the total number of polynomials is O(n(n1 + . . . + nd−1 + 1)k−1), improving the upper bound (10).

5

Published as a conference paper at ICLR 2020

Unstructured sparsity. In the case of networks obtained by pruning (Hanson & Pratt, 1989) or
generated randomly from a distribution over graphs (Xie et al., 2019), the sparsity pattern can be
arbitrary. In this case the size of the resulting LPs varies at runtime. Under the layer-wise assumption
that any neuron is connected to at most r neurons in the previous layer, the size of the cliques in (9)
is bounded as s = O(rd). This estimate has an exponential dependency on the depth but ignores
that many neurons might share connections to the same inputs in the previous layer, thus being
potentially loose. The bound (10) implies that the number of different polynomials is O(ndrdk).

2D Convolutional networks. The sparsity in the weight matrices of convolutional layers has a
certain local structure; neurons are connected to contiguous inputs in the previous layer. Adjacent
neurons also have many input pixels in common (see Figure 2). Assuming a constant number of
channels per layer, the size of the cliques in (9) is O(d3). Intuitively, such number is proportional to
the volume of the pyramid depicted in Figure 2 where each dimension depends linearly on d. Using
(10) we get that there are O(ndd3k) different polynomials in the sparse Krivine’s certiﬁcate. This is
a drastic decrease in complexity when compared to the unstructured sparsity case.

The use of sparsity in polynomial optimization preceeds Theorem 3 (Weisser et al., 2018). First stud-
ied in the context of sum-of-squares by Kojima et al. (2005) and further reﬁned in Waki et al. (2006);
Lasserre (2006) (and references therein), it has found applications in safety veriﬁcation (Yang et al.,
2016; Zhang et al., 2018), sensor localization Wang et al. (2006), optimal power ﬂow (Ghaddar
et al., 2015) and many others. Our work ﬁts precisely into this set of important applications.

Algorithm 1 LiPopt for ELU activations and sparsity pattern

Input: matrices {Wi}d

i=1, sparsity pattern {Ii}m
(cid:81)d−1
i=1 Diag(si)W T

i+1

i=1, hierarchy degree k.

(cid:46) compute norm-gradient polynomial

bγxγ

(cid:46) compute coefﬁcients of p in basis

γ∈Nn
k

k ] where p(x) = (cid:80)

1: p ← (2s0 − 1)T W T
1
2: x ← [s0, . . . , sd−1]
3: b ← [bγ : γ ∈ Nn
4: for i = 1, . . . , m do
5:
6: (cid:101)Nk ← ∪m
i=1Ni,k
7: h ← (cid:80)
(α,β)∈ (cid:101)N cαβhαβ
8: c ← [cαβ : (α, β) ∈ (cid:101)Nk]; y ← [λ, c]
where λ − h(x) = (cid:80)
9: Z ← [zγ]γ∈Nn

Ni,k ← {(α, β) ∈ N2n

k

return min{λ : b = Zy, y = [λ, c], c ≥ 0}

(zT

γ y)xγ

γ∈Nn
k

k : supp(α) ∩ supp(β) ⊆ Ii}

(cid:46) compute positivity certiﬁcate

(cid:46) linear program variables
(cid:46) compute coefﬁcients of λ − h in basis
(cid:46) solve LP

5 QCQP REFORMULATION AND SHOR’S SDP RELAXATION

Another way of upper bounding L(fd) comes from a further relaxation of (4) to an SDP. We consider
the following equivalent formulation where the variables si are normalized to lie in the interval
[−1, 1], and we rename t = s0:
(cid:40)

(cid:41)

L(fd) ≤ max

Diag(si + 1)W T

i+1 : −1 ≤ si ≤ 1

(11)

1
2d−1 sT

0 W T
1

d−1
(cid:89)

i=1

Any polynomial optimization problem like (11) can be cast as a (possibly non-convex) quadratically
constrained quadratic program (QCQP) by introducing new variables and quadratic constraints.
This is a well-known procedure described in Park & Boyd (2017, Section 2.1). When d = 2 problem
(11) is already a QCQP (for the (cid:96)∞ and (cid:96)2-norm cases) and no modiﬁcation is necessary.

QCQP reformulation. We illustrate the case d = 3 where we have the variables s1, s2 corre-
sponding to the ﬁrst and second hidden layer and a variable s0 corresponding to the input. The
norm-gradient polynomial in this case is cubic, and it can be rewritten as a quadratic polynomial by
introducing new variables corresponding to the product of the ﬁrst and second hidden layer variables.
More precisely the introduction of a variable s1,2 with quadratic constraint s1,2 = vec(s1sT
2 ) allows
us to write the objective (11) as a quadratic polynomial. The problem then becomes a QCQP with
variable y = [1, s0, s1, s2, s1,2] of dimension 1 + n + n1n2.

6

Published as a conference paper at ICLR 2020

SDP relaxation. Any quadratic objective and constraints can then be relaxed to linear constraints
on the positive semideﬁnite variable yyT = X (cid:60) 0 yielding the so-called Shor’s relaxation of (11)
(Park & Boyd, 2017, Section 3.3). When d = 2 the resulting SDP corresponds precisely to the one
studied in Raghunathan et al. (2018a). This resolves a common misconception (Raghunathan et al.,
2018b) that this approach is only limited to networks with one hidden layer.

Note that in our setting we are only interested in the optimal value rather than the optimizers, so
there is no need to extract a solution for (11) from that of the SDP relaxation.

Drawback. This approach includes a further relaxation step from (11), thus being fundamentally
limited in how tightly it can upper bound the value of L(fd). Moreover when compared to LP
solvers, off-the-shelf semideﬁnite programming solvers are, in general, much more limited in the
number of variables they can efﬁciently handle.

In the case d = 2 this relaxation provides a constant factor approximation to the original QCQP
(Ye, 1999). Further approximation quality results for such hierarchical optimization approaches to
NP-hard problems are out of the scope of this work.

Relation to sum-of-squares. The QCQP approach might appear fundamentaly different to the
hierarchical optimization approaches to POPs, like the one described in Section 3. However, it is
known that Shor’s SDP relaxation corresponds exactly to the ﬁrst degree of the SOS hierarchical
SDP solution to the QCQP relaxation (Lasserre, 2000). Thus, the approach in section 3 and the
one in this section are, in essence, the same; they only differ in the choice of polynomial positivity
certiﬁcate.

6 RELATED WORK

Estimation of L(fd) with (cid:96)2-norm is studied by Virmaux & Scaman (2018); Combettes & Pesquet
(2019); Fazlyab et al. (2019); Jin & Lavaei (2018). The method SeqLip proposed in Virmaux &
Scaman (2018) has the drawback of not providing true upper bounds. It is in fact a heuristic method
for solving (4) but which provides no guarantees and thus can not be used for robustness certiﬁcation.
In contrast the LipSDP method of Fazlyab et al. (2019) provides true upper bounds on L(fd) and in
practice shows superior performance over both SeqLip and CPLip (Combettes & Pesquet, 2019).

Despite the accurate estimation of LipSDP, its formulation is limited to the (cid:96)2-norm. The only
estimate available for other (cid:96)p-norms comes from the equivalence of norms in euclidean spaces. For
instance, we can obtain an upper bound for the (cid:96)∞-norm after multiplying the (cid:96)2 Lipschitz constant
upper bound by the square root of the input dimension. The resulting bound can be rather loose
and our experiments in section 7 conﬁrm the issue. In contrast, our proposed approach LiPopt can
acommodate any norm whose unit ball can be described via polynomial inequalities.

Let us point to one key advantage of LiPopt, compared to LipSDP (Jin & Lavaei, 2018; Fazlyab
et al., 2019). In the context of robustness certiﬁcation we are given a sample x(cid:92) and a ball of radius
(cid:15) around it. Computing an upper bound on the local Lipschitz constant in this subset, rather than
a global one, can provide a larger region of certiﬁed robustness. Taking into account the restricted
domain we can reﬁne the bounds in our POP (see remark in section 1). This potentially yields a
tighter estimate of the local Lipschitz constant. On the other hand, it is not clear how to include such
additional information in LipSDP, which only computes one global bound on the Lipschitz constant
for the unconstrained network.

Raghunathan et al. (2018a) ﬁnd an upper bound for L(fd) with (cid:96)∞ metric starting from problem
(4) but only in the context of one-hidden-layer networks (d = 2). To compute such bound they
use its corresponding Shor’s relaxation and obtain as a byproduct a differentiable regularizer for
training networks. They claim such approach is limited to the setting d = 2 but, as we remark in
section 5, it is just a particular instance of the SDP relaxation method for QCQPs arising from a
polynomial optimization problem. We ﬁnd that this method ﬁts into the LiPopt framework, using
SOS certiﬁcates instead of Krivine’s. We expect that the SDP-based bounds described in 5 can also
be used as regularizers promoting robustness.

Weng et al. (2018) provide an upper bound on the local Lipschitz constant for networks based on
a sequence of ad-hoc bounding arguments, which are particular to the choice of ReLU activation
function. In contrast, our approach applies in general to activations whose derivative is bounded.

7

Published as a conference paper at ICLR 2020

7 EXPERIMENTS

We consider the following estimators of L(fd) with respect to the (cid:96)∞ norm:

Name
SDP

Description
Upper bound arising from the solution of the SDP relaxation described in Sec-
tion 5

LipOpt-k Upper bound arising from the k-th degree of the LP hierarchy (6) based on the

sparse Krivine Positivstellenstatz.

Lip-SDP Upper bound from Fazlyab et al. (2019) multiplied

√

d where d is the input

UBP

LBS

dimension of the network.
Upper bound determined by the product of the layer-wise Lipschitz constants
with (cid:96)∞ metric
Lower bound obtained by sampling 50000 random points around zero, and
evaluating the dual norm of the gradient

7.1 EXPERIMENTS ON RANDOM NETWORKS

We compare the bounds obtained by the algorithms described above on networks with random
weights and either one or two hidden layers. We deﬁne the sparsity level of a network as the maxi-
mum number of neurons any neuron in one layer is connected to in the next layer. For example, the
network represented on Figure 1 has sparsity 2. The non-zero weights of network’s i-th layer are
sampled uniformly in [− 1√
ni

] where ni is the number of neurons in layer i.

1√
ni

,

For different conﬁgurations of width and sparsity, we generate 10 random networks and average the
obtained Lipschitz bounds. For better comparison, we plot the relative error. Since we do not know
the true Lipschitz constant, we cannot compute the true relative error. Instead, we take as reference
the lower bound given by LBS. Figures 3 and 5 show the relative error, i.e., ( ˆL − LLBS)/LLBS
where LLBS is the lower bound computed by LBS and ˆL is the estimated upper bound. Figures 9
and 10 in Appendix C we show the values of the computed Lipschitz bounds for 1 and 2 hidden
layers respectively.

When the chosen degree for LiPopt-k is the smallest as possible, i.e., equal to the depth of the
network, we observe that the method is already competitive with the SDP method, especially in the
case of 2 hidden layers. When we increment the degree by 1, LiPopt-k becomes uniformly better
than SDP over all tested conﬁgurations. We remark that the upper bounds given by UBP are too
large to be shown in the plots. Similarly, for the 1-hidden layer networks, the bounds from LipSDP
are too large to be plotted.

Finally, we measured the computation time of the different methods on each tested network (Fig-
ures 4 and 6). We observe that the computation time for LiPopt-k heavily depends on the network
sparsity, which reﬂects the fact that such structure is exploited in the algorithm. In contrast, the time
required for SDP does not depend on the sparsity, but only on the size of the network. Therefore as
the network size grows (with ﬁxed sparsity level), LipOpt-k obtains a better upper bound and runs
faster. Also, with our method, we see that it is possible to increase the computation power in order
to compute tighter bounds when required, making it more ﬂexible than SDP in terms of computa-
tion/accuracy tradeoff. LiPopt uses the Gurobi LP solver, while SDP uses Mosek. All methods run
on a single machine with Core i7 2.8Ghz quad-core processor and 16Gb of RAM.

(a) 40 × 40

(b) 80 × 80

(c) 160 × 160

(d) 320 × 320

Figure 3: Lipschitz approximated relative error for 1-hidden layer networks

8

5.07.510.012.515.017.520.0Sparsity0.20.40.60.81.0Lipschitz errorLiPopt_2LiPopt_3SDP5.07.510.012.515.017.520.0Sparsity0.20.40.60.81.01.2Lipschitz error5.07.510.012.515.017.520.0Sparsity0.20.40.60.81.01.2Lipschitz error5.07.510.012.515.017.520.0Sparsity0.20.40.60.81.01.2Lipschitz errorPublished as a conference paper at ICLR 2020

(a) 40 × 40

(b) 80 × 80

(c) 160 × 160

(d) 320 × 320

Figure 4: Computation times for 1-hidden layer networks (seconds

(a) 5 × 5 × 10

(b) 10 × 10 × 10

(c) 20 × 20 × 10

(d) 40 × 40 × 10

Figure 5: Lipschitz approximated relative error for 2-hidden layer networks

(a) 5 × 5 × 10

(b) 10 × 10 × 10

(c) 20 × 20 × 10

(d) 40 × 40 × 10

Figure 6: Computation times for 2-hidden layer networks (seconds

7.2 EXPERIMENTS ON TRAINED NETWORKS

Similarly, we compare these methods on networks trained on MNIST. The architecture we use is a
fully connected network with two hidden layers with 300 and 100 neurons respectively, and with
one-hot output of size 10. Since the output is multi-dimensional, we restrict the network to a single
output, and estimate the Lipschitz constant with respect to label 8.

Moreover, in order to improve the scalability of our method, we train the network using the pruning
strategy described in Han et al. (2015)2. After training the full network using a standard technique,
the weights of smallest magnitude are set to zero. Then, the network is trained for additional itera-
tions, only updating the nonzero parameters. Doing so, we were able to remove 95% of the weights,
while preserving the same test accuracy. We recorded the Lipschitz bounds for various methods in
Table 7.2. We observe clear improvement of the Lipschitz bound obtained from LiPopt-k com-
pared to SDP method, even when using k = 3. Also note that the input dimension is too large for
the method Lip-SDP to provide competitive bound, so we do not provide the obtained bound for
this method.

Algorithm
Lipschitz bound

LBS LiPopt-4 LiPopt-3
84.2

88.3

94.6

SDP UBP
691.5
98.8

7.3 ESTIMATING LOCAL LIPSCHITZ CONSTANTS WITH LIPOPT

In the of section 7.1, we study the improvement on the upper bound obtained by LiPopt, when
we incorporate tighter upper and lower bounds on the variables si of the polynomial optimization
problem (4). Such bounds arise from the limited range that the pre-activation values of the network
can take, when the input is limited to an (cid:96)∞-norm ball of radius (cid:15) centered at an arbitrary point x0.

2For training we used the code from this reference. It is publicly available in https://github.com/

mightydeveloper/Deep-Compression-PyTorch

9

468101214Sparsity0.00.10.20.30.4Computation timeLiPopt_2LiPopt_3SDP468101214Sparsity0.00.20.40.60.8Computation time468101214Sparsity0.00.51.01.5Computation time468101214Sparsity0246810Computation time468101214Sparsity0.00.10.20.30.40.5Lipschitz errorLiPopt_3LiPopt_4SDPLipSDP468101214Sparsity0.00.20.40.60.8Lipschitz error468101214Sparsity0.00.10.20.30.40.5Lipschitz error468101214Sparsity0.00.10.20.30.40.5Lipschitz error468101214Sparsity0.00.10.20.30.40.5Computation timeLiPopt_3LiPopt_4SDP468101214Sparsity0.00.51.01.52.02.53.0Computation time468101214Sparsity05101520Computation time468101214Sparsity0204060Computation timePublished as a conference paper at ICLR 2020

The algorithm that computes upper and lower bounds on the pre-activation values is fast (it has the
same complexity as a forward pass) and is described, for example, in Wong & Kolter (2018). The
variables si correspond to the value of the derivative of the activation function. For activations like
ELU or ReLU, their derivative is monotonically increasing, so we need only evaluate it at the upper
and lower bounds of the pre-activation values to obtain corresponding bounds for the variables si.

We plot the local upper bounds obtained by LiPopt-3 for increasing values of the radius (cid:15), the bound
for the global constant (given by LiPopt-3) and the lower bound on the local Lipschitz constant
obtained by sampling in the (cid:15)-neighborhood (LBS). We sample 15 random networks and plot the
average values obtained. We observe clear gap between both estimates, which shows that larger
certiﬁed balls could be obtained with such method in the robustness certiﬁcation applications.

(a) 40 × 40

(b) 80 × 80

(c) 160 × 160

(d) 320 × 320

Figure 7: Global vs local Lipschitz constant bounds for 1-hidden layer networks

(a) 10 × 10 × 10

(b) 20 × 20 × 10

(c) 40 × 40 × 10

(d) 80 × 80 × 10

Figure 8: Global vs local Lipschitz constant bounds for 2-hidden layer networks

8 CONCLUSION AND FUTURE WORK

In this work, we have introduced a general approach for computing an upper bound on the Lipschitz
constant of neural networks. This approach is based on polynomial positivity certiﬁcates and gen-
eralizes some existing methods available in the literature. We have empirically demonstrated that
it can tightly upper bound such constant. The resulting optimization problems are computationally
expensive but the sparsity of the network can reduce this burden.

In order to further scale such methods to larger and deeper networks, we are interested in several
possible directions: (i) divide-and-conquer approaches splitting the computation on sub-networks in
the same spirit of Fazlyab et al. (2019), (ii) exploiting parallel optimization algorithms leveraging
the structure of the polynomials, (iii) custom optimization algorithms with low-memory costs such
as Frank-wolfe-type methods for SDP (Yurtsever et al., 2019) as well as stochastic handling of
constraints (Fercoq et al., 2019) and (iv), exploting the symmetries in the polynomial that arise from
weight sharing in typical network architectures to further reduce the size of the problems.

ACKNOWLEDGMENTS

This project has received funding from the European Research Council (ERC) under the European
Union’s Horizon 2020 research and innovation programme (grant agreement 725594 - time-data)
and from the Swiss National Science Foundation (SNSF) under grant number 200021 178865. FL
is supported through a PhD fellowship of the Swiss Data Science Center, a joint venture between
EPFL and ETH Zurich. VC acknowledges the 2019 Google Faculty Research Award.

10

0.00.51.01.52.0radius0.850.900.951.001.051.101.15Lipschitz bound0.00.51.01.52.0radius0.900.951.001.051.101.151.20Lipschitz bound0.00.51.01.52.0radius0.951.001.051.101.151.20Lipschitz bound0.00.51.01.52.0radius1.001.051.101.151.20Lipschitz boundGlob-3Loc-3LBS0123456radius0.280.300.320.340.360.38Lipschitz bound0123456radius0.150.200.250.300.350.40Lipschitz bound0123456radius0.080.090.100.110.120.13Lipschitz bound0123456radius0.0300.0350.0400.0450.050Lipschitz boundGlob-3Loc-3LBSPublished as a conference paper at ICLR 2020

REFERENCES

A. Ahmadi and A. Majumdar. Dsos and sdsos optimization: More tractable alternatives to sum
of squares and semideﬁnite optimization. SIAM Journal on Applied Algebra and Geometry,
3(2):193–230, 2019. doi: 10.1137/18M118935X. URL https://doi.org/10.1137/
18M118935X.

Cem Anil, James Lucas, and Roger Grosse. Sorting out Lipschitz function approximation. In Pro-
ceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings
of Machine Learning Research, pp. 291–301, Long Beach, California, USA, 09–15 Jun 2019.
PMLR.

Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative adversarial networks.
In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Pro-
ceedings of Machine Learning Research, pp. 214–223, International Convention Centre, Sydney,
Australia, 06–11 Aug 2017. PMLR.

Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
In Advances in Neural Information Processing Systems 30, pp. 6240–6249.

neural networks.
Curran Associates, Inc., 2017.

Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause. Safe model-based
reinforcement learning with stability guarantees. In Advances in Neural Information Processing
Systems 30, pp. 908–918. Curran Associates, Inc., 2017.

Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval
networks: Improving robustness to adversarial examples. In Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
854–863, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.

Djork-Arn´e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and Accurate Deep Network
Learning by Exponential Linear Units (ELUs). arXiv e-prints, art. arXiv:1511.07289, Nov 2015.

Patrick L. Combettes and Jean-Christophe Pesquet. Lipschitz Certiﬁcates for Neural Network Struc-
tures Driven by Averaged Activation Operators. arXiv e-prints, art. arXiv:1903.01014, Mar 2019.

Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George J. Pappas. Efﬁ-
cient and Accurate Estimation of Lipschitz Constants for Deep Neural Networks. arXiv e-prints,
art. arXiv:1906.04893, Jun 2019.

Olivier Fercoq, Ahmet Alacaoglu, Ion Necoara, and Volkan Cevher. Almost surely constrained con-
In Proceedings of the 36th International Conference on Machine Learning,
vex optimization.
volume 97 of Proceedings of Machine Learning Research, pp. 1910–1919, Long Beach, Califor-
nia, USA, 09–15 Jun 2019. PMLR.

Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural

networks. In International Conference on Learning Representations, 2019.

Bissan Ghaddar, Jakub Marecek, and Martin Mevissen. Optimal power ﬂow as a polynomial opti-

mization problem. IEEE Transactions on Power Systems, 31(1):539–546, 2015.

Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In Advances in Neural Information Processing Systems 30,
pp. 5767–5777. Curran Associates, Inc., 2017.

Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.

David Handelman. Representing polynomials by positive linear functions on compact convex
polyhedra. Paciﬁc J. Math., 132(1):35–62, 1988. URL https://projecteuclid.org:
443/euclid.pjm/1102689794.

Stephen Jose Hanson and Lorien Y. Pratt. Comparing biases for minimal network construction
with back-propagation. In Advances in Neural Information Processing Systems 1, pp. 177–185.
Morgan-Kaufmann, 1989.

11

Published as a conference paper at ICLR 2020

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Conference on Computer Vision and Pattern Recognition, pp. 770–778, 06 2016. doi:
10.1109/CVPR.2016.90.

Todd Huster, Cho-Yu Jason Chiang, and Ritu Chadha. Limitations of the Lipschitz constant as a

defense against adversarial examples. arXiv e-prints, art. arXiv:1807.09705, Jul 2018.

Ming Jin and Javad Lavaei. Stability-certiﬁed reinforcement learning: A control-theoretic perspec-

tive. arXiv e-prints, art. arXiv:1810.11505, Oct 2018.

Masakazu Kojima, Sunyoung Kim, and Hayato Waki. Sparsity in sums of squares of poly-
doi:

nomials. Mathematical Programming, 103(1):45–62, May 2005.
10.1007/s10107-004-0554-3.

ISSN 1436-4646.

Jean-Louis Krivine. Anneaux pr´eordonn´es. Journal d’analyse math´ematique, 12:p. 307–326, 1964.

J. B. Lasserre. Convergent lmi relaxations for nonconvex quadratic programs. In Proceedings of the
39th IEEE Conference on Decision and Control (Cat. No.00CH37187), volume 5, pp. 5041–5046
vol.5, Dec 2000. doi: 10.1109/CDC.2001.914738.

Jean B Lasserre. Convergent sdp-relaxations in polynomial optimization with sparsity. SIAM Jour-

nal on Optimization, 17(3):822–843, 2006.

Jean Bernard Lasserre. An Introduction to Polynomial and Semi-Algebraic Optimization. Cam-
doi: 10.1017/

bridge Texts in Applied Mathematics. Cambridge University Press, 2015.
CBO9781107447226.

Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recog-

nition. Proceedings of the IEEE, 86(11):2278–2324, Nov 1998. doi: 10.1109/5.726791.

Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In International Conference on Learning Representations,
2018.

Jaehyun Park and Stephen Boyd. General heuristics for nonconvex quadratically constrained

quadratic programming. arXiv preprint arXiv:1703.07870, 2017.

Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certiﬁed defenses against adversarial exam-

ples. In International Conference on Learning Representations, 2018a.

Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. Semideﬁnite relaxations for certifying
robustness to adversarial examples. In Advances in Neural Information Processing Systems, pp.
10877–10887, 2018b.

Gilbert Stengle. A nullstellensatz and a positivstellensatz in semialgebraic geometry. Mathematische

Annalen, 207(2):87–97, Jun 1974. ISSN 1432-1807. doi: 10.1007/BF01362149.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In 2nd International Conference
on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference
Track Proceedings, 2014.

Aladin Virmaux and Kevin Scaman. Lipschitz regularity of deep neural networks: analysis and
efﬁcient estimation. In Advances in Neural Information Processing Systems 31, pp. 3835–3844.
Curran Associates, Inc., 2018.

H. Waki, S. Kim, M. Kojima, and M. Muramatsu. Sums of squares and semideﬁnite program
relaxations for polynomial optimization problems with structured sparsity. SIAM Journal on
Optimization, 17(1):218–242, 2006. doi: 10.1137/050623802.

Zizhuo Wang, Song Zheng, Stephen Boyd, and Yinyu Ye. Further relaxations of the sdp approach

to sensor network localization. Tech. Rep., 2006.

12

Published as a conference paper at ICLR 2020

Tillmann Weisser, Jean B Lasserre, and Kim-Chuan Toh. Sparse-bsos: a bounded degree sos hierar-
chy for large scale polynomial optimization with sparsity. Mathematical Programming Compu-
tation, 10(1):1–32, 2018.

Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning,
and Inderjit Dhillon. Towards fast computation of certiﬁed robustness for ReLU networks. In Pro-
ceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings
of Machine Learning Research, pp. 5276–5285, Stockholmsmssan, Stockholm Sweden, 10–15
Jul 2018. PMLR.

Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In Proceedings of the 35th International Conference on Machine Learning,
volume 80 of Proceedings of Machine Learning Research, pp. 5286–5295, Stockholmsmssan,
Stockholm Sweden, 10–15 Jul 2018. PMLR.

Eric Wong, Frank Schmidt, and Zico Kolter. Wasserstein adversarial examples via projected
In Proceedings of the 36th International Conference on Machine Learn-
Sinkhorn iterations.
ing, volume 97 of Proceedings of Machine Learning Research, pp. 6808–6817, Long Beach,
California, USA, 09–15 Jun 2019. PMLR.

Saining Xie, Alexander Kirillov, Ross Girshick, and Kaiming He. Exploring Randomly Wired
International Conference on Computer Vision, Apr

Neural Networks for Image Recognition.
2019.

Zhengfeng Yang, Chao Huang, Xin Chen, Wang Lin, and Zhiming Liu. A linear programming
relaxation based approach for generating barrier certiﬁcates of hybrid systems. In John Fitzgerald,
Constance Heitmeyer, Stefania Gnesi, and Anna Philippou (eds.), FM 2016: Formal Methods, pp.
721–738, Cham, 2016. Springer International Publishing. ISBN 978-3-319-48989-6.

Yinyu Ye. Approximating quadratic programming with bound and quadratic constraints. Mathemat-
ical Programming, 84(2):219–226, Feb 1999. ISSN 1436-4646. doi: 10.1007/s10107980012a.

Alp Yurtsever, Olivier Fercoq, and Volkan Cevher. A conditional-gradient-based augmented la-
grangian framework. In Proceedings of the 36th International Conference on Machine Learning,
volume 97 of Proceedings of Machine Learning Research, pp. 7272–7281, Long Beach, Califor-
nia, USA, 09–15 Jun 2019. PMLR.

Y. Zhang, Z. Yang, W. Lin, H. Zhu, X. Chen, and X. Li. Safety veriﬁcation of nonlinear hybrid
systems based on bilinear programming. IEEE Transactions on Computer-Aided Design of Inte-
grated Circuits and Systems, 37(11):2768–2778, Nov 2018. doi: 10.1109/TCAD.2018.2858383.

13

Published as a conference paper at ICLR 2020

A PROOF OF THEOREM 1

Theorem. Let f be a differentiable and Lipschitz continuous function on an open, convex subset X
of a euclidean space. Let (cid:107) · (cid:107) be the dual norm. The Lipschitz constant of f is given by

L(f ) = sup
x∈X

(cid:107)∇f (x)(cid:107)∗

(12)

Proof. First we show that L(f ) ≤ supx∈X (cid:107)∇f (x)(cid:107)∗.

|f (y) − f (x)| =

≤

≤

(cid:90) 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
0
(cid:90) 1

0
(cid:90) 1

(cid:12)
(cid:12)
∇f ((1 − t)x + ty)T (y − x) dt
(cid:12)
(cid:12)

(cid:12)∇f ((1 − t)x + ty)T (y − x)(cid:12)
(cid:12)

(cid:12) dt

(cid:107)∇f ((1 − t)x + ty)(cid:107)∗ dt (cid:107)y − x(cid:107)

0
≤ sup
x∈X

(cid:107)∇f (x)(cid:107)∗(cid:107)y − x(cid:107)

were we have used the convexity of X .

Now we show the reverse inequality L(f ) ≥ supx∈X (cid:107)∇f (x)(cid:107)∗. To this end, we show that for any
positive (cid:15), we have that L(f ) ≥ supx∈X (cid:107)∇f (x)(cid:107)∗ − (cid:15).
Let z ∈ X be such that (cid:107)∇f (z)(cid:107)∗ ≥ supx∈X (cid:107)∇f (x)(cid:107)∗ − (cid:15). Because X is open, there exists a
sequence {hk}∞

k=1 with the following properties:

1. (cid:104)hk, ∇f (z)(cid:105) = (cid:107)hk(cid:107)(cid:107)∇f (z)(cid:107)∗

2. z + hk ∈ X

3. limk→∞ hk = 0.

By deﬁnition of the gradient, there exists a function δ such that limh→0 δ(h) = 0 and the following
holds:

f (z + h) = f (z) + (cid:104)h, ∇f (z)(cid:105) + δ(h)(cid:107)h(cid:107)

For our previously deﬁned iterates hk we then have

⇒ |f (z + hk) − f (z)| = |(cid:107)hk(cid:107)(cid:107)∇f (z)(cid:107)∗ + δ(hk)(cid:107)hk(cid:107)|

Dividing both sides by (cid:107)hk(cid:107) and using the deﬁnition of L(f ) we ﬁnally get

⇒ L(f ) ≥

(cid:12)
(cid:12)
(cid:12)
(cid:12)

f (z + hk) − f (z)
(cid:107)hk(cid:107)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= |(cid:107)∇f (z)(cid:107)∗ + δ(hk)|

|(cid:107)f (z)(cid:107)∗ + δ(hk)| = (cid:107)∇f (z)(cid:107)∗

⇒ L(f ) ≥ lim
k→∞
⇒ L(f ) ≥ sup
x∈X

(cid:107)∇f (x)(cid:107)∗ − (cid:15)

14

Published as a conference paper at ICLR 2020

B PROOF OF PROPOSITION 1

Proposition. Let fd be a dense network (all weights are nonzero). The following sets, indexed by
i = 1, . . . , nd, form a valid sparsity pattern for the norm-gradient polynomial of the network fd:
Ii := (cid:8)s(d−1,i)} ∪ {s(j,k) : there exists a directed path from s(j,k) to s(d−1,i) in Gd

(13)

(cid:9)

Proof. First we show that ∪m
i=1Ii = I. This comes from the fact that any neuron in the network is
connected to at least one neuron in the last layer. Otherwise such neuron could be removed from the
network altogether.

Now we show the second property of a valid sparsity pattern. Note that the norm-gradient polyno-
mial is composed of monomials corresponding to the product of variables in a path from input to a
ﬁnal neuron. This imples that if we let pi be the sum of all the terms that involve the neuron s(d−1,i)
we have that p = (cid:80)

i pi, and pi only depends on the variables in Ii.

We now show the last property of the valid sparsity pattern. This is the only part where we use that
the network is dense. For any network architecture the ﬁrst two conditions hold. We will use the fact
that the maximal cliques of a chordal graph form a valid sparsity pattern (see for example Lasserre
(2006)).

Because the network is dense, we see that the clique Ii is composed of the neuron in the last layer
s(d−1,i) and all neurons in the previous layers. Now consider the extension of the computational
graph ˆGd = (V, ˆE) where

ˆE = E ∪ {(sj,k, sl,m) : j, l ≤ d − 2)}

which consists of adding all the edges between the neurons that are not in the last layer. We show
that this graph is chordal. Let (a1, . . . , ar, a1) be a cycle of length at least 4 (r ≥ 4). notice that
because neurons in the last layer are not connected between them in ˆG, no two consecutive neurons
in this cycle belong to the last layer. This implies that in the subsequence (a1, a2, a3, a4, a5) at most
three belong to the last layer. A simple analysis of all cases implies that it contains at least two
nonconsecutive neurons not in the last layer. Neurons not in the last layer are always connected in
ˆG. This constitutes a chord. This shows that ˆGd is a chordal graph. Its maximal cliques correspond
exactly to the sets in proposition.

15

Published as a conference paper at ICLR 2020

C EXPERIMENTS ON RANDOM NETWORKS

(a) 40 × 40

(b) 80 × 80

(c) 160 × 160

(d) 320 × 320

Figure 9: Lipschitz bound comparison for 1-hidden layer networks

(a) 5 × 5 × 10

(b) 10 × 10 × 10

(c) 20 × 20 × 10

(d) 40 × 40 × 10

Figure 10: Lipschitz bound comparison for 2-hidden layer networks

16

5.07.510.012.515.017.520.0Sparsity0.500.751.001.251.501.752.00Lipschitz boundLiPopt_2LiPopt_3SDPLBS5.07.510.012.515.017.520.0Sparsity0.51.01.52.0Lipschitz bound5.07.510.012.515.017.520.0Sparsity0.51.01.52.0Lipschitz bound5.07.510.012.515.017.520.0Sparsity1.01.52.0Lipschitz bound468101214Sparsity0.20.30.40.50.60.7Lipschitz boundLiPopt_3LiPopt_4SDPLipSDPLBS468101214Sparsity0.20.40.60.81.0Lipschitz bound468101214Sparsity0.20.40.6Lipschitz bound468101214Sparsity0.00.10.20.30.40.5Lipschitz bound