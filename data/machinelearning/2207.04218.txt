2
2
0
2

l
u
J

9

]
L
P
.
s
c
[

1
v
8
1
2
4
0
.
7
0
2
2
:
v
i
X
r
a

Subclasses of Class Function used to Implement Transformations of
Statistical Models

Lloyd Allison,
Faculty of Information Technology,
Monash University, Clayton, Victoria 3800, Australia
lloyd.allison@monash.edu

Abstract

A library of software for inductive inference guided
by the Minimum Message Length (MML) principle
was created previously. It contains various (object-
oriented-) classes and subclasses of statistical Model
and can be used to infer Models from given data sets
in machine learning problems. Here transformations
of statistical Models are considered and implemented
within the library so as to have desirable properties
from the object-oriented programming and mathe-
matical points of view. The subclasses of class Func-
tion needed to do such transformations are deﬁned.
transformation,
Statistical Model,
class Function, machine learning, inference, informa-
tion, MML

keywords:

1

Introduction

A library of software1 based on the Minimum Mes-
sage Length (MML) principle [1, 2] was created pre-
viously [3,4] for use in writing programs to solve ma-
chine learning problems, that is to infer statistical
Models from given data sets. It follows on from an
earlier prototype [5]. The software deﬁnes various
classes2 of statistical Model which can be used in-
dependently or combined to create structured Mod-
els. Here mathematical transformations of statistical

1See https://www.cantab.net/users/mmlist/MML/A/ for

source-code and documentation.

2In the object-oriented sense of the word “class”.

Models are added, and the properties of transforma-
tions are considered and they are implemented in the
library; this relies on deﬁning and using certain sub-
classes of class Function.

MML is a Bayesian method of inference devised
by Wallace and Boulton [1] in the 1960s, their initial
application being mixture modelling (clustering, un-
supervised classiﬁcation [6]). MML was subsequently
developed both theoretically and practically and has
been used on many and varied problems [2] includ-
ing, but not limited to, megalithic astronomical align-
ments [7], factor analysis [8], decision-trees [9] and
protein structural alignments [10]. In general “Strict”
MML inference is NP-hard [11,12] but there are good
and eﬃcient approximations [2, 13] for many cases.
MML can be seen as a realisation of Ockham’s ra-
zor [3, 14]. The fact that it measures the complexity
of statistical Models and of data in the same units
makes it particularly suitable for choosing between
competing models and for implementing structured
Models.

MML inference [1,2] relies on Bayes’s theorem [15]3

pr(m & ds) = pr(m) × pr(ds|m)

= pr(ds) × pr(m|ds)

(1)

and on Shannon’s mathematical theory of communi-

3Typical usage is ‘pr(x)’ for pr(X = x), ‘m’ for a parame-
terised Model, ‘sp’ for statistical parameters, ‘upm’ for an un-
parameterised Model, ‘ps’ for miscellaneous parameters, ‘D’
for a data space, ‘d ∈ D’ for a datum, and ‘ds ∈ D∗’ for a data
set of several data.

1

 
 
 
 
 
 
cation [16], hence “message”,

I(E) = − log(pr(E))

(2)

where I(E) is the information content, or message
length, of an event E. Base-two logarithms measure
information in ‘bits’ and natural logarithms measure
information in ‘nits’ (natural bits). Writing msg(.)
for I(.) it follows that

msg(m & ds) = msg(m) + msg(ds|m)

(3)

for Model (hypothesis) m and data set ds. The mes-
sage length of m and of ds together is the length
of a two-part message: ﬁrst transmit m and then
transmit ds assuming that m is true. Minimising
the total message length clariﬁes the trade-oﬀ be-
tween Model complexity msg(m) and its ﬁt to the
data msg(ds|m); the Model that achieves the mini-
mum is the best Model, the best answer to the infer-
ence problem being posed. (A one-part message may
be shorter, although by a surprisingly small amount,
but it does not provide an answer to any inference
problem.) Note that MML considers the accuracy
of measurement of continuous data (section 4) and
the optimal precision to which parameters should be
stated so every continuous datum, and parameter,
has a probability not just a probability density. This
is one of the reasons that, in general, MML is not
the same as maximum a posteriori (MAP) estima-
tion. Even a discrete valued parameter of a Model
may have an optimal precision that is less than its
data type would allow.

It is not the place of this paper to argue for the
usefulness of transformations of probability distribu-
tions or for a certain transformed distribution being
a good ﬁt for a particular kind of data – these have
been well-studied by statisticians. Rather it examines
a way of implementing such transformations more ex-
pressively in programming languages.

UPModel {estimator(ps); ...}

|
|--MultiState
|
etc.

|
|--Discretes
|
|
|
|
|
|--ByPdf
|
|
|--Continuous
|
|
|
|
|
|
|
|
|
|
|
|
|--R_D
|
etc.

|
|--NormalUPM
|
etc.

-- (Gaussian)

-- multivariate, R^D

Figure 1: Main UnParameterised Model classes

2 The Kinds of Model and As-

sociates

There are two stages of Model – unparameterised
Models which are instances of class UPModel4 and
parameterised Models which are instances of class
Model.

An unparameterised Model upm can be applied
to appropriate statistical parameter(s) sp to create
a parameterised Model m = upm(sp), for example,
N 01 = Normal ((cid:104)0, 1(cid:105)) is the Normal Model (Gaus-
sian distribution) with mean 0 and standard devi-
ation 1. An unparameterised Model has problem-
deﬁning parameters, for example, Bounded has the
bounds of its data space. Problem-deﬁning parame-
ters are given, not estimated. For some, such as the
Normal distribution, the problem-deﬁning parame-
ters are trivial, triv or ( ), and in such cases a single

4Note, on a suitable device, but probably not in an email
reader, hyperlinks in this pdf ﬁle such as UPModel←(click)
lead to online documentation and software within https://
www.cantab.net/users/mmlist/MML/A/

2

-- e.g. "fair coin"

Model {pr(d); nlPr(d); ...}

|
etc.

|
|--Discretes.M
|
|
|
|--ByPdf.M {pdf(d); ...}
|
|
|
|
|
|
|
etc.

|
|--Continuous.M
|
|
|
|--R_D.M

|
etc.

-- multivariate, R^D

Figure 2: Main (Parameterised) Model classes

instance of the unparameterised Model is suﬃcient.
An unparameterised Model (ﬁgure 1) can create pa-
rameterised Models (ﬁgure 2). Hopefully it can also
estimate (ﬁgure 3) a parameterised Model to ﬁt a
given data set ds.

A parameterised Model has statistical parameters,
for example, the (standard) Normal has its mean and
standard deviation. In some cases statistical param-
eters are trivial as, for example, in a Uniform distri-
bution, but statistical parameters are generally esti-
mated from a given data set although they can also
be given, as with N 01.
In accord with the MML
framework [1, 2], an estimated Model has a message
length, msg1, and the data set from which it was esti-
mated has a message length, msg2, calculated under
the assumption that the Model is true. An MML
estimator attempts to ﬁnd a Model to minimise the
two-part message length, msg = msg1 + msg2. (In
the case of given statistical parameters, msg1 is zero
as the parameters are common knowledge.)

The principal responsibility of a parameterised
Model m is to give the probability pr(d) and nega-
tive log probability 5 nlP r(d) of a datum d from m’s

5Many calculations in the implementation are actually done
in terms of negative log probabilities as those quantities are
typically more manageable than plain probabilities (similar

3

Estimator

{ ds2Model(ds);
...}

-- data set -> Model

Figure 3: Estimator class

data space. It may also do other things such as gen-
erate a random value, random(), from its probability
distribution.

An Estimator (ﬁgure 3) may have parameters to
control its actions, for example, to set the amount of
lookahead in a search algorithm, or to set a prior dis-
tribution on the statistical parameters of the Models
that it will estimate.

A further important class in the software is Func-
tion (ﬁgure 4). The library includes a simple inter-
preter for the λ-calculus [17] and Functions can be
deﬁned by λ-expressions. Functions can also be de-
ﬁned by Native code, that is Java code. Functions of
Continuous data Cts2Cts, in mathematics R → R,
and CtsD2CtsD, RD → RD, will be important later.
Note that a UPModel is a Function because it can
be applied to statistical parameters to return a pa-
rameterised Model and an Estimator is a Function
because it can be applied to a data set to return a
parameterised Model. Models, Functions, and hence
UPModels and Estimators, are ﬁrst class Values.

3 Model Transformations

Perhaps the most widely known transformed Model
is the log-Normal probability distribution: for a data
set ds = [d1, d2, ...], di ∈ (0, ∞), it assumes that the
values [log(d1), log(d2), ...] are modelled by a Normal
distribution. Conversely, to generate a random value
from a log-Normal, generate a random value x from
the underlying Normal distribution and apply log−1
to x, that is, return ex. We will see more of the log-
Normal in section 4.

In general, a Model can be transformed by a one-
to-one function, f , having an inverse, f −1. Both
unparameterised and parameterised Models can be
transformed; upmf =upm.transf orm(f ) remains an

considerations may apply to probability densities).

Function

| {apply(d); ...}
|
|--Lambda
|
|--Native
|
|--Cts2Cts {apply_x(x);
|
|
|--CtsD2CtsD{J();
|
|
|--UPModel
|
|--Estimator
|
etc.

d_dx(); ...} -- df/dx

-- Jacobian
nlJ(); ...} -- -log |J|

interface HasInverse{inverse();}

Figure 4: Main Function classes

Figure 5: Transforming and estimating

unparameterised Model and mf =m.transf orm(f ) is
a parameterised Model. We also have that, as distri-
butions, transforming with f and parameterising with
sp commute

upm(sp).transf orm(f ) = upm.transf orm(f )(sp);
(4)
the left and right sides of equation 4 have diﬀerent
histories but eﬀect the same probability distribution.
Similarly, as distributions, estimating (on appropri-
ate data) and transforming commute (ﬁgure 5)

upm.estimator(ps)(ds).transf orm(f )

= upm.transf orm(f ).estimator(ps)(ds.map(f −1)).
(5)

In addition,

upm.estimator(ps)(ds).msg()

= upm.transf orm(f )

(6)

.estimator(ps)(ds.map(f −1)).msg()

that is, the amounts of information in ds and in
ds.map(f −1) are the same. Conditions (4), (5) and
(6) are a kind of “invariance” of probability distribu-
tions (Models). Of course a general transformation
operation of an arbitrary Model by an arbitrary one-
to-one Function (of the right kind) cannot possibly
know what are equivalent transformations, if any, on
the statistical parameters of the arbitrary Model.

Given a data set, ds, upmf ’s estimator operates by
applying f to all members (map(f )) of ds and giving
the result to upm’s estimator. Model mf generates
a random() value by getting m to generate one and
applying f −1 to it. (It is a quirk of common usage
that when transforming a Model with Function f one
applies f to data and f −1 to random values generated
by the Model.)

Since the transforming function f must be one-to-
one, in the case of a discrete data space and its Mod-
els, such a function f must either permute the data
space in some simple way or set up a one to one corre-
spondence with a same sized space; continuous data
spaces, section 4 and section 5, are more interesting.

4

transform(f)transform(f)upmmupmfestimate on ds.map(f_inverse)estimate on dssult f (d) has an AoM of (cid:15) × |f (cid:48)(x)| where f (cid:48) = df /dx
is the derivative6 of f : If the exact value of d can be
somewhere in a range of (cid:15), f (d) can be somewhere
in a range of (cid:15) × |f (cid:48)(x)| (ﬁgure 6). For a continuous,
one-to-one Function f with an inverse f −1 the pdf of
m.transf orm(f ) is

m.pdf (f (x)) × |f (cid:48)(x)|.

(7)

The factor |f (cid:48)(x)| adjusts the pdf of f (d) and, in ef-
fect, adjusts the AoM of f (d) when pdf (f (d)) is used
by pr(f (d)). Having a pdf is suﬃcient to make the
transformed Model an instance of Continuous.

In the case of the log-Normal Model, f = log, f ’s
inverse is exp and f ’s derivative is 1/x. In the source
code:

logNormal = Normal.transform(log);

Naturally Function exp has the inverse log and
derivative exp, and transf orm(exp) turns a Model
of (0, ∞) into one of (−∞, ∞).

5 RD Models

Multivariate continuous data are members of RD for
some dimension, D, and an unparameterised Model
of such data is an instances of class R D.7 Note that
each component of a measured multivariate datum
has an accuracy of measurement – as in section 4.
As an example of transformation, consider Carte-
sian coordinates in the plane (cid:104)x, y(cid:105) ∈ R2 and po-
lar coordinates, (cid:104)r, θ(cid:105) ∈ R+ × [0, 2π) ⊂ R2. The
functions polar2cartesian and cartesian2polar eﬀect
mappings between these coordinate systems and are
If upmc is an unparame-
inverses of each other.
terised Model of cartesian coordinates then upmp =
upmc.transf orm(polar2cartesian) is a Model of po-
lar coordinates.

When transforming a univariate Continuous Model
with function f , the derivative of f was used to “ad-
just” the accuracy of measurement of a datum. With

6Mathematically, most functions in R → R are neither
continuous nor diﬀerentiable but in practice most of those that
we are interested in are both continuous and diﬀerentiable over
all or most of their domain.

7No(?) programming language allows RˆD (RD) as an
identiﬁer; R D is the closest we can get to it in program code.

Figure 6: AoM and Continuous

4 Continuous Models

First note that, due to the properties of object-
oriented programming, an unparameterised ‘Contin-
uous’ Model – one of continuous data – is a subclass
of UPModel. Hence an instance of Continuous can
be transformed by treating it just like any other UP-
Model (section 3), however, the transformed result
is then just a UPModel and is not an instance of
the Continuous subclass. If we want the transformed
Continuous itself to also be an instance of Continu-
ous, a little more work is required. In particular a
pdf (.) must be deﬁned for the parameterised trans-
formed Continuous: For example, we would like log-
Normal to be a Continuous not just a UPModel, see
ﬁgure 1.

Each continuous datum, d, has a nominal value
x and an accuracy of measurement (AoM ) (cid:15) and
stands for d = x ± (cid:15)
2 . Usually it is assumed that
(cid:15) is small and that a pdf varies little across x ± (cid:15)
2 so
that (cid:15) × pdf (x) is a good approximation for pr(x ± (cid:15)
2 ).
When a continuous function f is applied to d the re-

5

xff’AoM/2multivariate continuous data, the Jacobian matrix of
f , and its determinant, take on that role. A suitable
pdf (d) for m.transf orm(f ) is

d = x ± ∞ is to know nothing at all about d. A sub-
Model may need to know how much information is in
those columns of the data in which it deals.

m.pdf (f (d)) × |J(d)|.

For polar2cartesian

Jpc =

(cid:18)cos θ −r × sin θ
r × cos θ

sin θ

(cid:19)

and for cartesian2polar

Jcp =

(cid:18) x/r

y/r
−y/r2 x/r2

(cid:19)

(8)

(9)

(10)

giving |Jpc| = r, |Jcp| = 1/r, and Jpc × Jcp = I.

A Detail

The pdf (.) of a transformed R D Model calls upon
the pdf (.) of the Model being transformed and the de-
terminant of the Jacobian of the transforming Func-
tion. This does not necessarily require the AoM of
each component of a transformed datum – there is al-
ready provision for Vectors where the AoM of a Vec-
tor as a whole is known but not that of each compo-
nent (however every component of a measured datum
does have an AoM). However some structured Mod-
els, such as Dependent and Independent, apply sub-
Models to one or more components (columns, vari-
ables) of data. In such cases it may be be necessary
to attribute the AoM of a transformed datum among
its components. This particularly arises in Estima-
tors. Therefore the apply(.) method of a Function in
CtsD2CtsD (RD → RD) uses the Jacobian matrix
of the Function to set the ratios of the result’s compo-
nent’s AoM s and uses its determinant to scale them
to arrive at the correct total AoM area (volume, . . . )
in RD.

The matter is relevant to Estimators because the
AoM of a datum inﬂuences the amount of information
in the datum and an Estimator trades-oﬀ the com-
plexity (information) of an estimated Model against
the complexity of a data set. Other things being
equal, doubling the AoM of a datum reduces its in-
formation by one bit and, in the limit, to know that

6 Conclusion

Transformations have been implemented in the MML
software library for unparameterised Models and pa-
rameterised Models using a one-to-one Function f .
To make the transformed Model’s random() work
f must have an inverse. For Models of continuous
data f ’s derivative must be deﬁned, and for multi-
variate Models of continuous data f ’s Jacobian must
be deﬁned. As probability distributions, parameter-
ising and transforming a Model commute, and trans-
forming and estimating (on corresponding data) com-
mute. Applying such a Function f to all the mem-
bers of a data set leaves the information content
of the data set unchanged.
In the source code the
deﬁnition of the log-Normal distribution is simply
Normal.transform(log) (section 4).

The author is not aware of any widely used pro-
gramming language where all functions (subroutines,
procedures, methods), whether built-in (‘+’, ‘−’, sin,
cos and so on) or user-deﬁned, are instances of an ex-
plicit ‘class Function’ (the consensus [18] seems to be
that it would be possible in Haskell say). Every func-
tion does actually have at least one method, ‘apply’.
Applying apply is almost invariably implicit – f x or
f (x) – it is the space between the f and the x, and (x)
is just the same as x after all. Note that Haskell [19]
also has an explicit alternative (‘$’ as in f $ x) for
apply. Given a ‘class Function’, subclasses and inter-
faces such as ‘1−1’, continuous, diﬀerentiable, invert-
ible and so on are possible and, as suggested above,
interesting and useful.

References

[1] Wallace, C. S. and Boulton, D. M. (1968) An in-
formation measure for classiﬁcation. The Com-
puter Journal, 11, 185–194. https://doi.org/
10.1093/comjnl/11.2.185.

6

[11] Farr, G. E. and Wallace, C. S. (2002) The com-
plexity of strict minimum message length in-
ference. The Computer Journal, 45, 285–292.
https://doi.org/10.1093/comjnl/45.3.285.

[12] Dowty, J. G. (2015) SMML estimators for 1-
dimensional continuous data. The Computer
Journal, 58, 126–133. https://doi.org/10.
1093/comjnl/bxt145.

[13] Wallace, C. S. and Freeman, P. R. (1987)
Estimation and inference by compact coding.
Journal of the Royal Statistical Society, Se-
ries B, 49, 240–265. https://www.jstor.org/
stable/2985992.

[14] Spade, P. V. (1999) The Cambridge Companion
to Ockham. Cambridge University Press. https:
//doi.org/10.1017/CCOL052158244X.

[15] Bayes, T. (1763) An essay towards solving a
problem in the doctrine of chances. Philosophi-
cal Transactions of the Royal Society of London,
53, 370–418.
reprinted in Biometrika 45(3/4)
pp.293–315, 1958, https://doi.org/10.2307/
2333180.

[16] Shannon, C. E. (1948) A mathematical theory of
communication. Bell Systems Technical Journal,
27, 379–423, 623–656. https://doi.org/10.
1002/j.1538-7305.1948.tb01338.x.

[17] Church, A. (1941) The Calculi of Lambda Con-
version, Annals of Mathematical Studies, 6.
Princeton University Press.
https://press.
princeton.edu/titles/2390.html.

[18] various (2002). class Function? Discussion in
the Haskell mailing list (29 Oct. 2002); see ex-
tract at monash.edu.

[19] Peyton Jones, S. et al. (2003) Haskell 98 Lan-
guage and Libraries, the Revised Report. Cam-
bridge University Press. Also see haskell.org.

[2] Wallace, C. S.

ductive
Inference
Length. Springer.
1007/0-387-27656-4.

(2005) Statistical and In-
by Minimum Message
https://doi.org/10.

[3] Allison, L.

zor. Springer.
978-3-319-76433-7.

(2018) Coding Ockham’s Ra-
https://doi.org/10.1007/

[4] Allison, L. (2018, 2020). Documentation for and
source-code of MML software. https://www.
cantab.net/users/mmlist/MML/A/.

and data mining

[5] Allison, L. (2005) Models for machine learn-
in functional pro-
ing
gramming.
Journal of Functional Program-
ming, 15, 15–32. https://doi.org/10.1017/
S0956796804005301.

[6] Jorgensen, M. A. and McLachlan, G. J. (2008)
Wallace’s approach to unsupervised learning:
The Snob program. The Computer Journal, 51,
571–578. https://doi.org/10.1093/comjnl/
bxm121.

[7] Patrick, J. D. and Freeman, P. R. (1988) A clus-
ter analysis of astronomical orientations. In Rug-
gles, C. L. N. (ed.), Records in Stone, pp. 252–
261. Cambridge University Press.

[8] Wallace, C. S. and Freeman, P. R. (1992) Single-
factor analysis by minimum message length esti-
mation. Journal of the Royal Statistical Society,
Series B, 54, 195–209. https://www.jstor.
org/stable/2345956.

[9] Wallace, C. S. and Patrick, J. D. (1993) Cod-
ing decision trees. Machine Learning, 11, 7–22.
https://doi.org/10.1023/A:1022646101185.

[10] Collier, J., Allison, L., Lesk, A., Garcia de
La Banda, M., and Konagurthu, A. (2014) A
new statistical framework to assess structural
alignment quality using information compres-
sion. Bioinformatics, 30,
i512–i518. https:
//doi.org/10.1093/bioinformatics/btu460.

7

