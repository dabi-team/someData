Low-complexity Approximate Convolutional Neural Networks

R. J. Cintra*

Stefan Duffner†

Christophe Garcia†

André Leite‡

2
2
0
2

l
u
J

9
2

]

G
L
.
s
c
[

1
v
7
8
0
0
0
.
8
0
2
2
:
v
i
X
r
a

Abstract

In this paper, we present an approach for minimizing the computational complexity of trained Convolutional Neural Networks (ConvNet).
The idea is to approximate all elements of a given ConvNet and replace the original convolutional ﬁlters and parameters (pooling and bias
coefﬁcients; and activation function) with efﬁcient approximations capable of extreme reductions in computational complexity. Low-complexity
convolution ﬁlters are obtained through a binary (zero-one) linear programming scheme based on the Frobenius norm over sets of dyadic
rationals. The resulting matrices allow for multiplication-free computations requiring only addition and bit-shifting operations. Such low-
complexity structures pave the way for low-power, efﬁcient hardware designs. We applied our approach on three use cases of different complex-
ity: (i) a “light” but efﬁcient ConvNet for face detection (with around 1 000 parameters); (ii) another one for hand-written digit classiﬁcation
(with more than 180 000 parameters); and (iii) a signiﬁcantly larger ConvNet: AlexNet with ≈1.2 million matrices. We evaluated the overall
performance on the respective tasks for different levels of approximations. In all considered applications, very low-complexity approximations
have been derived maintaining an almost equal classiﬁcation performance.

Convolutional Neural Networks, Approximation, Optimization, Numerical Computation

Keywords

1 Introduction

Since their introduction in the 1990s by LeCun et al. [48], convolu-
tional neural networks (ConvNets) have proven to be very powerful
in many challenging computer vision tasks, such as hand-written
character recognition [14,48], embedded text detection and recogni-
tion [27,32], automatic facial analysis [35,63,82], trafﬁc sign recog-
nition [19], pedestrian detection [76], vision-based navigation [36],
and house numbers recognition [74], just to cite a few. Although
state-of-the-art results have been reached in many different ﬁelds,
ConvNets have become very popular only recently with the im-
pressive results obtained by Krizhevsky et al. [42] in the recogni-
tion task, followed by Simonyan and Zisserman who won the local-
ization challenge at the Large-Scale Visual Recognition Challenge
(ImageNet) 2014 [78].

The main property of ConvNets is their capability for automatic
extraction of complex and application-suitable features from raw
data (e.g., pixels in computer vision). To do so, they integrate a
pipeline of convolution and pooling layers generally followed by a
multi-layer perceptron, jointly performing local feature extraction
and classiﬁcation (or regression) in a single architecture where all
parameters are learnt using the classical error back-propagation
algorithm [69]. Traditionally, ConvNets are used for image process-
ing tasks and, to this end, they are often applied on small regions
of a bigger image in a sliding-window framework, for instance to
detect an object. Because of weight sharing, each layer essentially
performs convolution or pooling operations using small kernels in-
side a “retina”, and when applied to a large image, the replication
of the ConvNet operation over all positions in the image can be sig-
niﬁcantly optimized by performing each convolution over the full
image at once, efﬁciently implementing a full image transforma-
tion pipeline.

However, during training and when applying the trained

*R. J. Cintra is with Signal Processing Group, Departamento de Estatística, Univer-
sidade Federal de Pernambuco, Recife, PE, Brazil. At the time of writing, R. J. Cintra
was with the LIRIS, Institut National des Sciences Apliqueés (INSA), Lyon, France
(e-mail: rjdsc@de.ufpe.br).

†Stefan Duffner and Christophe Garcia are with the Université de
(e-mail:
INSA-Lyon, LIRIS, UMR5205, F-69621, France.

Lyon, CNRS,
{stefan.duffner, christophe.garcia}@liris.cnrs.fr

‡André Leite is with the Departamento de Estatística, Universidade Federal de

Pernambuco, Recife, PE, Brazil. (e-mail: leite@de.ufpe.br).

ConvNet, there is still a signiﬁcant number of ﬂoating-point com-
putations (multiplications and additions). In order to accelerate
these operations, most current approaches and available software
rely on parallel computing using GPUs [42, 75] facilitated to some
extent by the inherently parallel architectures of ConvNets. The
recent trend in “deep learning” to use more and more complex
models with millions of parameters requires enormous amounts
of computational resources, in particular for training the models
but also for applying the learnt ConvNets. Reducing the compu-
tational complexity of these models is thus of great interest to the
research community as well as to industry. Moreover, such reduc-
tion in complexity of ConvNets is necessary to implement them on
devices with limited resources (such as mobile devices) in order to
operate at acceptable speed, for example for real-time applications,
and reduce the overall power consumption.

In this paper, we focus on drastically reducing the computational
cost itself by proposing a post-training approximation scheme aim-
ing at replacing all parameters of a ConvNet with low-complexity
versions. That is, for the convolution ﬁlters, only additions and
bit-shifting operations are performed—no multiplication is neces-
sary. Additionally, the activation function is sought to be replaced
with low-complexity alternatives. Formulated as an optimization
problem, we adopt matrix approximation techniques based on the
Frobenius norm error and dyadic rational numbers represented in
terms of Canonical Signed Digit (CSD) encoding.

Indeed, a sound approximation theory is a necessary step to fa-
cilitated hardware development. This is illustrated in the case
of image compression where the most efﬁcient coding schemes
are based on approximate matrices realized in dedicated hard-
ware [16,26,40,60]. We aim at introducing an approach for approxi-
mating CNNs in order to pave the way for future efﬁcient dedicated
hardware design.

This work is organized as follows. Section 2 provides a litera-
ture review on the efﬁcient numerical implementation of neural
networks and, in particular, ConvNets. Section 3 details our ap-
proach for approximating the elements of a given ConvNet aiming
at designing low-complexity structures. In Section 4 we present the
results of our experimental evaluation of the proposed approach
for two typical ConvNet architectures. We assess the approximate
ConvNets relative to their exact counterparts in terms of several

1

 
 
 
 
 
 
ﬁgures of merit. We conclude the paper in Section 5.

2 Related Work

Although GPU implementations [71] allow for fast training and ap-
plication of ConvNets on sufﬁciently equipped platforms, their in-
tegration on embedded systems for real-time applications may be
more difﬁcult due to the limited amount of available resources on
these devices, which usually requires a good trade-off between per-
formance and code size. Several previous works have tackled this
problem. In early works [43, 55, 85] weight parameters of neural
networks have been represented as power-of-two integers. Thus all
multiplications can be operated as simple bit shifts. Direct train-
ing of these networks was also possible by keeping a ﬂoating-point
version of each weight parameter, or otherwise use a technique
called “weight dithering” [84]. Simard and Graf [77] extended
this idea by encoding all other parameters as powers of two ex-
cept the weights, i.e. neuron activations, gradients, and learning
rates. Later, Draghici [31] conducted a broader analysis on the
computational power of neural networks with reduced precision
weights. Recently, Machado et al. [52] proposed a speciﬁc approx-
imation scheme for sparse representation learning using only val-
ues of powers of two, and they integrated this quantization into the
learning process. Finally, in the work of Courbariaux et al. [24], all
weights are encoded as binary values (1-bit), and for the training a
ﬂoating-point version is still needed. Similarly, Kim and Paris [41],
proposes a completely binary neural network. However, an initial
real-valued training phase is required.

More recently, special attention has been paid to hardware im-
plementations of ConvNets, especially on FPGAs. In [54], for exam-
ple, a high-level optimization methodology is applied to the imple-
mentation of the CFF face detector [35]. They propose algorithmic
optimizations and advanced memory management and transform
the ﬂoating-point computation into ﬁxed-point arithmetic. Inter-
estingly such coarse approximation could furnish very similar de-
tection rates and low false-alarm rates on referenced datasets, for
a roughly sevenfold gain of speed. Later, this work has been ex-
tended in [34], where the authors present for the ﬁrst time several
implementations of the CFF algorithm on FPGA, with a parallel
architecture composed of a Processing Element ring and a FIFO
memory, which constitutes a generic architecture capable of pro-
cessing images of different sizes. Farabet and LeCun [33] also
propose a scalable hardware architecture to implement large-scale
ConvNets, with a modular vision engine for large image process-
ing, with FPGA and ASIC implementations. Chakradhar et al. [13]
present a dynamically conﬁgurable FPGA co-processor that adapts
to complex ConvNet architectures exploiting different types of par-
allelism. A very low-complexity ASIC design of ConvNets has been
developed by Chen et al. [15], allowing for very high execution
speeds and power consumption of state-of-the-art ConvNets. Fi-
nally, Zhang et al. [89] propose a FPGA design strategy and algo-
rithmic enhancements to optimize the computational throughput
and memory bandwidth for any given ConvNet architecture.

Other recent works have focused on the algorithmic and mem-
ory optimizations of large-scale ConvNets. For example, Ma-
malet et al. [53] proposed different strategies for simplifying the
convolutional ﬁlters (fusion of convolutional and pooling layers, 1D
separable ﬁlters), in order to modify the hypothesis space, and to
speed-up learning and processing times. These convolutions can
also effectively be performed by simple multiplications of the ﬁl-
ters with the respective input images in the frequency domain [57].
However, due to the overhead of the FFT, there is only a computa-

tional gain with larger ﬁlter sizes, and if a given ﬁlter can be reused
consecutively for many input images. Vanhoucke et al. [83] pre-
sented a set of different techniques to accelerate the computation of
ConvNets on CPU, mostly for Intel and AMD CPUs, exploiting for
example SIMD instructions, memory locality, and ﬁxed-point rep-
resentations. Also, many recent works [29, 30, 38, 45, 64, 70, 86, 88]
have focused on reducing the complexity of convolution or fully-
connected layers of large-scale ConvNets by replacing the high-
dimensional matrix or tensor multiplications with several low-rank
matrix multiplications using different low-rank factorization meth-
ods, either at test-time or both for training and testing. Although,
large gains in computational and memory resources can be ob-
tained on complex ConvNets, these optimizations do not focus on
hardware implementation and low-power constraints.

As opposed to many previous works that integrate the approx-
imation process into the learning [22, 23, 25, 61, 68], our approach
operates on existing fully-trained models, that originally may have
been aimed for standard PCs or more powerful architectures. Thus,
our approximation scheme allows to integrate these models into
hardware with much fewer resources.

3 Approximation Approach

3.1 General Goal

Our goal is to derive low-complexity structures capable of reducing
the computational costs of a given ConvNet. Ideally, the following
two conditions are simultaneously expected to be satisﬁed:

(i) the computational elements of the ConvNet (convolutional
ﬁlters, sub-sampling coefﬁcients, bias values, and sigmoid
function calls) are replaced by corresponding low-complexity
structures;

(ii) the performance of the ConvNet is not signiﬁcantly degraded.

However, addressing both above conditions proves to be a hard
In particular, the large number of variables, the non-
task.
linearities, and extremely long simulation times prevents such ap-
proach. Also, to the best of our knowledge, literature furnishes
no mathematical result linking the approximation of individual
ConvNet elements and the ﬁnal ConvNet performance. Thus, we
adopt a greedy-like heuristic which consists in individually sim-
plifying each computational structure of a ConvNet in the hope of
ﬁnding a resulting structure capable of good performance [21].

In a ConvNet two main types of mathematical elements are
found:
(i) matrix structures and (ii) activation functions. The
matrix structures are represented by convolution ﬁlter weights,
sub-sampling operations, and bias values; whereas the activation
function is usually a non-linear function, such as the threshold,
piecewise-linear, and sigmoid functions [37].

To approximate these two classes of elements, different tools are
required. For the matrix-based structures, we selected matrix ap-
proximation methods as a venue to derive low-complexity compu-
tational elements [17, 62, 73, 79]. For the activation, we separate
methods capable of approximating functions with efﬁcient digital
implementation [4, 81, 90].

3.2 Low-complexity Matrix Structures

In [8–11, 17, 79], several methods for deriving approximations of
discrete transform matrices—such as the discrete cosine trans-
form [1]—were proposed. Let M be an N × N given matrix. For
instance, M can be a convolutional ﬁlter. In this case, a computa-
tional instantiation of M applied to evaluate a single output pixel

2

requires in principle N2 ﬂoating point multiplications. A typical
ConvNet may contain thousands of convolutional ﬁlters. For ex-
ample, the classical architecture described in [42] contains 244,760
ﬁlters, which is nowadays considered a relatively small network.
Therefore, to minimize such a signiﬁcant computational cost, we
aim at obtaining a low-complexity matrix ˆM capable of satisfying
the following relation in an optimal sense: M ≈ ˆM. The matrix ˆM
is said to be an approximation for M. Such approximate convolu-
tional ﬁlters would allow the realization of computationally inten-
sive ConvNets in limited resources architectures.

In this paper, a low-complexity matrix is a matrix of dyadic ra-
tional entries. Dyadic rational numbers are fractions of the form
m/2n, where n is a positive integer and m is an odd integer. Such
numbers are suitable for binary arithmetic. Indeed, a multiplica-
tion by a dyadic rational consists of a multiplication by m followed
by a right shift of n bits. Because m is an integer, we can take
full advantage of ﬁxed-point arithmetic. Indeed m can be given
a binary representation with minimum number of adders, aiming
at multiplicative irreducibility. Multiplicative irreducibility is at-
tained whenever the minimum number of additions to implement
a multiplication by m is equal to the number of ones in the binary
representation of m [11]. Multiplicative irreducibility is often ob-
tained when the CSD representation is considered [28]. Therefore,
a multiplication by m can be converted into a sequence of additions
and bit-shifting operations. As a consequence, low-complexity ma-
trices are multiplierless, a very desirable property as ﬂoating-point
operations are much more costly than additions and bit-shifting
operations.

Standard methods for matrix approximation include:

inspec-
tion [18], matrix parametrization [10], and matrix factorization [5].
However, since a typical ConvNet may contain from thousands
to millions of ﬁlters, inspection-based approaches are not feasi-
ble. Methods based on the parametrization of the matrix elements
are also ineffective because (i) the elements of convolutional ﬁlters
are usually not clearly related, i.e., they do not satisfy identiﬁable
mathematical relationships and (ii) the elements are not repeated.
Additionally, ConvNet ﬁlters are not expected to satisfy properties,
such as symmetry and orthogonality, which favors the derivation of
approximations. Thus, methods based on matrix factorizations are
less adequate.

3.3 Matrix Approximation by Linear Programming

We adopted a general approach to the problem of obtaining ˆM ac-
cording to a optimization problem as described below:

ˆM = argmin
T

error (M,T) .

(1)

The above optimization problem can yield better approximate ma-
trices if an expansion factor α is introduced [11]. By adopting the
usual Frobenius norm [73] as an error measure, (1) can be recast
according to the following mixed integer nonlinear programming
(INLP) setup [50]:

to guarantee that the approximate ﬁltering structures (e.g., con-
volution kernels) are close to the exact counterpart. The Frobe-
nius norm satisﬁes the above rationale. This analysis is conﬁrmed
in [64].

To ensure that the candidate matrices T have low complexity,
we limited the search space of the above problem to the matrices
whose elements are deﬁned over a sets of dyadic rationals D. Some
particular sets are [18, 79]:

,0,

1
4

,

,1,2,3,4

,

1
2
1
2

3
4
1
4

,−

,0,

,

¾

D1 = {−1,0,1} ,
D2 = {−2,−1,0,1,2} ,
D3 = {−4,−3,−2,−1,0,1,2,3,4} ,
1
4

−4,−3,−2,−1,−

D4 =

3
4

1
2

,−

,−

½

D5 =

½

−7,−6,−5,−4,−3,−2,−1,−

3
4

,−

1
4

,

1
2

D6 =

−4,−

D7 =

D8 =

½

½

½

−5,−

−7,−

,

3
4
15
4
19
4
27
4

,1,2,3,4,5,6,7

,

,−

,... ,

,−

,−

,−

7
2
9
2
13
2

13
4
17
4
25
4

,−

,... ,

,−

,... ,

,

¾
13
4
17
4
25
4

,

7
2
9
2

,

,

15
4
19
4
13
2

,

,

,4

,

¾

,5

¾

27
4

,7

.

¾

Sets D6, D7, and D8 possess uniformly spaced rationals.

A straightforward way of addressing (2) is as follows. Consider-
ing a given set of dyadic rationals D, for each element of α · T, we
simply ﬁnd the closest neighbour of such element in D. Such ap-
proach can be efﬁciently implemented by means of binary search.
However, this approach is only effective as long as (2) remains an
unconstrained optimization problem. Alternatively, we can con-
sider a more ﬂexible approach based on integer linear program-
ming (ILP).

For ﬁxed values of α, the mixed INLP problem posed in (2) can
be efﬁciently solved by means of binary (zero-one) linear program-
ming. In other words, we aim at converting a nonlinear problem
into a linear one. Indeed, let mi, j , i, j = 1,2,... , N, denote the en-
tries of M and r ∈ D be a dyadic rational. We adopt the following
binary decision variables:

xi, j(r) =

1,

0,

(

if mi, j = r,
otherwise.

For binary (zero-one) variables we have y2 = y, where y is a dummy
variable. This fact paves the way for the linearization of the above-
mentioned optimization problem. Therefore, (2) can be re-written
according to the following binary linear programming problem [6,
51, 65]:

min
xi, j (r)

N

N

i=1
X

j=1
X

r∈D
X

¡

r − α · mi, j

2 · xi, j (r),
¢

(3)

(α

∗,T∗) = argmin
α,T

kM − α · Tk2,

(2)

subject to

where α > 0 is the real-valued expansion factor and k·k is the Frobe-
nius norm [73]. The choice of the Frobenius norm is justiﬁed by the
following argument. An approximate CNN must have its elements
numerically ‘close’ to elements from the exact CNN. Therefore, a
measure that takes into consideration distance in a energy-based
manner (euclidean distance sense) emerges naturally as a means

xi, j (r) = 1,

i, j = 1,2,... , N.

r∈D
X

The above constraint is to ensure that each element mi, j is approx-
imated by a unique dyadic rational in D. The solution of the above
problem is denoted as x(α)
, i, j = 1,2,... , N, r ∈ D, being linked to
i, j
the choice of α. Such binary (zero-one) solution can be employed to

3

compute the actual entries t(α)
i, j
ciated to the considered α according to:

of the low-complexity matrix asso-

Solving (2) for the above matrix, we obtain:

∗ = 0.30931,
α

t(α)
i, j

=

r∈D
X

r · x(α)

i, j (r).

(4)

The resulting low-complexity matrix is denoted by Tα. The approx-
imation error is implied by (2) and can be computed according to:

T∗ =










5
4.5
−2.25
−4
−4.75

3.25
7
2.5
−1.75
−4

2.5 −0.75 −0.75
2.75
6.5
3.75
5.5
2.5
0.5
−1
0.5

5
4
2.75
0.75










Error(α) = kM − α · Tαk2.

Because a sequence of values for α is selected, the above prob-
lem is solved for each instantiation; furnishing the sequence of er-
rors indexed by α: Error(α). Being a linear programming prob-
lem, each instantiation can be solved efﬁciently and very quickly
by contemporary computational packages [58, 67]. State-of-the-art
solvers can obtain solutions for ILP problems at an average compu-
tation complexity in O (N) [59] or O (N log N) [6, 51, 65]. Finally, we
determine the global optimum value α∗ according to:

∗ = arg min
α
α

Error(α),

(5)

which can be solved by simple minimization over a vector of val-
ues. Associated to α∗, we also obtain T∗ , Tα∗ , which is the global
optimal low-complexity matrix. Therefore, the sought approxima-
tion ˆM is given by:

10 −3 −3
13
20
11
20
26
28
18
−9
15
16
22
10
−16
−7
10
11
2
−19 −16 −4
2
3

.










=

1
4

·










Fig. 1(a) depicts the Frobenius norm error for varying values of
α (cf. 2). For very small α, the values of α · M are close to zero.
So the discrete entries of the candidate matrices T are unable to
provide a good approximation. As α increases, a similar effect hap-
pens. However, for intermediate values the minimum can be found.
Fig. 1(b) shows details in the vicinity of the optimum. The curves
shown in Fig. 1 are piecewise concatenations of parabolae. This is
due to the quadratic nature of the coefﬁcients (r − α · mi, j )2 of the
linear programming problem in (3). Each parabola is linked to a
particular approximate candidate T.

Notice that the low-complexity matrix T∗ is expressed in terms
of small integers, which can be given simple binary expansions
(e.g., 22 = 26 − 24 − 22). Similarly, we have that α∗ = 0.30931 ≈
2−2 + 2−4 − 2−8 = 0.30859375.

Therefore, considering (6), the actual fully multiplierless approx-

ˆM = α

∗ · T∗.

imation is furnished by:

(6)

The above ILP approach allows the user to easily include con-
straints to the optimization problem. This is relevant for fur-
ther investigation in this topic; in particular when speciﬁc math-
ematical properties are expected to be enforced on the resulting
low-complexity matrices (for instance, 2D ﬁlter normalization [12,
p. 115]).

We emphasize that the solving method for (2) is only required to
be efﬁcient enough to cope with the time constraints at the design
phase of the approximate neural network. In other words, solvers
available in contemporary optimization packages are suitable; and
the choice of the particular method for solving (2) is not a critical
for our approach. Additionally, we note that the optimization solver
is simply a step for obtaining the ﬁnal neural network. Once the
approximate structures are found, optimization solver are clearly
not required anymore.

3.4 Example

To illustrate the procedure, we selected D8 and considered the
search space of the expansion factor to be the interval [0.25,1]
with a step of 10−3. Additionally, we consider the following par-
ticular convolutional ﬁlter employed in the ConvNet described in
Section 4.2:

1.5200701 1.0317051 0.7906240 −0.2153791 −0.2340538

1.3982610 2.1860176 2.0152923 1.5620477 0.8270900

−0.6848867 0.7470516 1.6923728 1.2537112 1.1946758

−1.2387477 −0.5483563 0.1261987 0.8677799 0.7742613

−1.4691808 −1.2178997 −0.2924347 0.2172496 0.1325074

M0 = 






.









4

ˆM = (2−4 + 2−6 − 2−10) ·

10 −3 −3
13
20
11
20
26
28
18
−9
15
16
22
10
−16
−7
10
11
2
−19 −16 −4
2
3










.










3.5 Activation Function Approximations

Although there are several types of activation functions, we focus
our analyses on the continuous tanh-sigmoid function, which is de-
ﬁned according to the hyperbolic tangent function [37]. As indi-
cated in [37,47], the mathematical expression for the tanh-sigmoid
functon is given by:

φ(x) = a · tanh(b · x),

(7)

where a = 1.7159 and b = 2/3. This particular activation function
has been originally proposed by LeCun [46] and adopted in several
working models as the Convolutional Face Finder (CFF) [35].

In [4, 44, 72, 80, 81, 91], approximations for the related sigmoid
function given by y = 1/(1 + e−x) were examined, including the
Alippi and Storti-Gajani (ASG) approximation [2], the piecewise
linear approximation of a non-linear function (PLAN) [3], and sim-
ple linear [81] and quadratic [91] approximations. Based on these
approximations, we derived expressions for the tanh-sigmoid ap-
proximations as shown in Table 1. We adopted an 8-bit represen-
tation for a resulting in the following approximate value: ˆa = 7/4.

3.6 Complexity

As a consequence of the above approximations, we have substantial
savings in computation costs. Indeed, a single call of the original

0.2

0.15

r
o
r
r
E

0.1

0.05

0

0.25

0.08

0.06

r
o
r
r
E

0.04

0.02

0

0.29

0.5

0.75
Expansion factor

(a)

1

0.3

0.31
Expansion factor

0.32

0.33

0.34

(b)

Figure 1: Approximation error for the particular matrix M0: (a) error curve over the considered search interval for α and (b) detailed
view around the optimum value of α.

Table 1: Approximations for the tanh-sigmoid

−1,
x < −5,
x
128 , −5 ≤ x < − 19
16 − 89
8 ,
x
4 − 1
− 19
8 ≤ x < −1,
4 ,
x
−1 ≤ x < 1,
2 ,
x
1 ≤ x < 19
4 + 1
4 ,
8 ,
x
19
16 + 11
8 ≤ x < 5,
16 ,
x ≥ 5.
1,






σ4(x) = ˆa · 


−1,
x
2 ,
1,

x < −2,

−2 ≤ x < 2,
x ≥ 2.

ASG-based

σ1(x) = ˆa ·

1+ |⌊x⌋|−x
2
2|x|
1+ |⌊x⌋|−x
2
2|x|

,

−1 +




1 −



,

x < 0,

x ≥ 0.

PLAN-based

σ2(x) = ˆa ·

Linear I

Quadratic I

σ3(x) = ˆa · 


−1,
x
4 ,
1,

x < −4,

−4 ≤ x < 4,
x ≥ 4.

Linear II



x < −2,

−1,
( x
2 + 1)2 − 1, −2 ≤ x < 0,
1 − ( x
2 + 1)2, 0 ≤ x < 2,
1,

x ≥ 2.



x < −4,

−1,
( x
4 + 1)2 − 1, −4 ≤ x < 0,
σ6(x) = ˆa · 

1 − ( x
4 + 1)2, 0 ≤ x < 4,
1,


x ≥ 4.

Quadratic II

σ6(x) = ˆa · 



5

N × N matrix M requires N2 multiplications of ﬂoating-point en-
tries per pixel. On the other hand, the proposed approximation ˆM
contains only small integers that can be very efﬁciently encoded
with minimal number of adders [28]. Similarly, the expansion fac-
tor α∗ can be given a truncated rational approximation in the form
of dyadic rationals. The same rationale also applies to the remain-
ing computational structures of the original ConvNet. Thus, the
ﬁnal resulting structure is fully multiplierless—only additions and
bit-shifting operations are required. In terms of hardware realiza-
tion, the number of arithmetic operations translate into chip area
and power consumption [7, 66]. Thus, in limited resource scenar-
ios (e.g., embedded systems and wireless sensors), approximations
may provide an effective way of porting large ConvNets into physi-
cal realization.

To summarize, the proposed approximation approach consists of:

(i) ﬁnding approximate convolutional ﬁlter by solving (1) for each

exact convolutional ﬁlter from a given ConvNet;

(ii) converting scaling factors, sub-sampling coefﬁcients, and bias
values into CSD representation aiming at the minimization of
computation costs and multiplicative irreducibility;

(iii) approximating the activation function to a simple function.

4 Experiments

We studied the effectiveness of the proposed approximation ap-
proach on two classical computer vision problems: (i) a binary and
(ii) a multi-class classiﬁcation problem. The ﬁrst application is face
detection, where the ConvNet classiﬁes image regions as face or
non-face. The second one is handwritten digit recognition, where
the trained model is used to classify a given image patch into one
of the ten digits “0” to “9”. For each of the two applications, we
trained a ConvNet in a classical way and evaluated its performance
in terms of precison and recall, for the given application.

The ﬁrst ConvNet is relatively small, whereas the second model
(for digit recognition) contains much more parameters. We aim
at demonstrating that our proposed approach is able to effectively
process larger networks.

After approximating the parameters of the models, we com-
pared their performance with their respective original, exact ver-
sions. Note that we do not aim at improving the state-of-the-art
in face detection or hand-written digit recognition. Indeed, cur-
rent literature presents concrete example of complex models, such
as multi-view or part-based detectors for face detection [56] and
huge ensemble classiﬁers for digit recognition [20]. Our goal is to
demonstrate—based on common representative models—that the
complexity of a given trained ConvNet model can be reduced signif-
icantly by approximating its parameters while maintaining a very
similar performance.

Hereafter an approximate network based on dyadic set Di is re-

ferred to as A i.

4.1 Binary Classiﬁcation

Our ﬁrst set of experiments employs a ConvNet that was trained
for face detection in grey-scale images. Thus, such network is a bi-
nary classiﬁer that decides whether the given input image is a face
or not. As a working model, we selected the classical face detector
called Convolutional Face Finder (CFF) proposed by Garcia and De-
lakis [35]. This model is a relatively “light” ConvNet with an input
size of 32 × 36 and six layers: four layers alternating convolution

and average pooling operations, with 4, 4, 14, and 14 maps, respec-
tively, followed by 14 neurons and one single ﬁnal output neuron.
The ﬁrst convolution layer contains four ﬁlters of size 5×5, the sec-
ond one contains 20 ﬁlters of size 3 × 3, and the 14 neurons of the
ﬁrst neuron layer are treated as convolutions of size 6×7, each neu-
ron being connected to only one map. Pooling maps contain a single
coefﬁcient, and all maps and neurons have an additional bias. The
entire ConvNet has 951 trainable parameters in total. A thorough
description of this particular ConvNet is supplied in [35]. The em-
ployed activation function is the exact continuous tanh-sigmoidal
function as detailed in (7).

After training the ConvNet as described in [35], we approxi-
mated all the convolution ﬁlter matrices with low-complexity ver-
sions. We created several approximations using the different sets
of dyadic rationals described in the previous section: D1,D1,... ,D8.
We also replaced all average pooling coefﬁcients and bias terms
with their closest CSD representation using 8 bits, being 7 bits for
the fractional part.

Table 2 lists the arithmetic costs of the exact ConvNet compared
to its approximations. Floating-point multiplications, direct addi-
tions, additions due to the CSD expansion, and bit-shifting oper-
ations were counted. The exact structure requires both ﬂoating-
point multiplications and additions. In contrast, the approximate
methods completely eliminates the need for multiplications at the
expense of much simpler operations: additions and bit-shifting op-
erations. Because the approximate quantities can be easily rep-
resented in ﬁxed-point arithmetic representation, it is suitable for
hardware implementation. Additionally, the hardware implemen-
tation of bit-shifting operations require virtually no cost, because
it can be implemented by simple physical wiring. As a result, we
have a very favorable trade-off: multiplications are exchanged for
additions.

In order to analyse the effect of the approximation on the actual
performance of the ConvNet, we evaluated the different versions
on three standard face detection benchmarks: FDDB [39] (2 845
images), AFW [92] (205 images), and Pascal Faces dataset [87]
(851 images); and we used the improved annotation and evalua-
tion protocol proposed by Mathias et al. [56].

Tables 3-5 show the average precision rates for different combi-
nations of sigmoid and weight matrix approximations relative to
the exact model for the three datasets.

Overall, the “Linear II” sigmoid approximation provides the best
results, followed by “ASG”, “Quadratic I”, and “PLAN”. In terms
of weight matrix approximations, A1–A5 generally give unsatis-
factory results, and A7 performs best. Also, it is interesting to
note that the ﬁner approximation A8 gave worse results than A7.
We further evaluated some variants, where different layers of the
ConvNet have been approximated with different sets of dyadic ra-
tionals. For such mixed approximate structures, we have denoted
them by A i, j,k,l , where the subscripts indicate the selected dyadic
In other words, indices i, j indicate that the
set for each layer.
dyadic sets Di and D j, respectively, are employed in the ﬁrst two
convolution layers; and similarly, indices k, l correspond to the
adoption of the dyadic sets Dk and Dl , respectively, for the two ﬁ-
nal fully-connected layers. We found that the ﬁrst layer requires a
ﬁner approximation than the other layers. This allowed us to main-
tain a good performance with very low-complexity approximations
(e.g., A3, A4) for these later layers. This can be explained accord-
ing to the following: (i) by its own very nature, the layers have
different degrees of importance; (ii) errors in initial layers tend to
propagate through the succeeding layers; and (iii) error propaga-
tion can potentially be ampliﬁed along the layers. Fig. 2 shows the
receiver operating characteristic (ROC) curves of the best perform-

6

Table 2: Arithmetic cost for CFF-based models

Model

Exact
A1
A2
A3
A4
A5
A6
A7
A8

Operation

Mult. Add. CSD Add. Bit-shifting

882
0
0
0
0
0
0
0
0

843
843
843
843
843
843
843
843
843

-
235
251
377
457
506
756
842
1028

-
346
362
488
568
617
867
953
1139

Table 3: Average precision for CFF with the FDDB test set and different approximations relative to the exact model

Exact

ASG

PLAN Linear I

Linear II Quadratic I Quadratic II

Exact
A1
A2
A3
A4
A5
A6
A7
A8
A7,3,3,3
A7,4,4,4
A7,6,6,6

1.000
0.000
0.001
0.549
0.523
0.490
0.938
0.960
0.821
0.917
0.967
0.959

0.953
0.000
0.001
0.311
0.602
0.098
0.914
0.943
0.792
0.902
0.931
0.907

0.894
0.000
0.000
0.432
0.365
0.517
0.817
0.847
0.648
0.886
0.855
0.862

0.002
0.000
0.000
0.005
0.001
0.000
0.002
0.002
0.001
0.003
0.000
0.004

0.988
0.000
0.001
0.510
0.558
0.657
0.888
0.963
0.810
0.947
0.976
0.959

0.887
0.000
0.000
0.426
0.383
0.510
0.797
0.848
0.650
0.888
0.861
0.863

0.938
0.000
0.002
0.180
0.602
0.073
0.949
0.935
0.793
0.851
0.928
0.933

Table 4: Average precision for CFF with the AFW test set and different approximations relative to the exact model

Exact

ASG

PLAN Linear I

Linear II Quadratic I Quadratic II

1.041
0.000
0.000
0.199
0.457
0.217
0.893
0.985
0.565
0.914
0.954
0.979

0.797
0.000
0.000
0.248
0.336
0.251
0.694
0.755
0.544
0.816
0.761
0.788

0.383
0.000
0.000
0.014
0.275
0.002
0.700
0.614
0.169
0.184
0.576
0.827

Exact
A1
A2
A3
A4
A5
A6
A7
A8
A7,3,3,3
A7,4,4,4
A7,6,6,6

1.000
0.000
0.001
0.220
0.422
0.220
0.864
0.978
0.698
0.955
1.020
0.967

0.839
0.000
0.001
0.041
0.356
0.015
0.794
0.844
0.553
0.525
0.827
0.881

0.829
0.000
0.000
0.260
0.317
0.314
0.715
0.753
0.576
0.835
0.755
0.787

0.000
0.000
0.000
0.004
0.000
0.000
0.000
0.000
0.000
0.004
0.000
0.000

7

i

i

n
o
s
c
e
r
p

i

i

n
o
s
c
e
r
p

i

i

n
o
s
c
e
r
p

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

exact
A6 linear II
A7 linear II
A7,4,4,4 linear II
A7,6,6,6 linear II

 0.4

 0.45

 0.5

 0.55
recall

(a)

 0.6

 0.65

 0.7

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

 0.4

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

 0.4

exact
A6 linear II
A7 linear II
A7,4,4,4 linear II
A7,6,6,6 linear II

 0.45

 0.55

 0.6

 0.5
recall

(b)

exact
A6 linear II
A7 linear II
A7,4,4,4 linear II
A7,6,6,6 linear II

 0.45

 0.55

 0.6

 0.5
recall

(c)

Figure 2: ROC curves for the (a) FDDB, (b) AFW, and (c) Pascal datasets comparing the face detection performance of the original (exact)
ConvNet model with different approximations.

8

Table 5: Average precision for CFF with the “Pascal faces” test set and different approximations relative to the exact model

Exact

ASG

PLAN Linear I

Linear II Quadratic I Quadratic II

Exact
A1
A2
A3
A4
A5
A6
A7
A8
A7,3,3,3
A7,4,4,4
A7,6,6,6

1.000
0.001
0.006
0.234
0.357
0.433
0.862
0.917
0.725
0.899
0.930
0.931

0.933
0.001
0.006
0.128
0.375
0.143
0.844
0.879
0.700
0.752
0.889
0.906

0.774
0.000
0.004
0.239
0.259
0.386
0.656
0.707
0.520
0.755
0.710
0.725

0.001
0.000
0.000
0.000
0.001
0.000
0.000
0.001
0.001
0.004
0.000
0.004

1.039
0.001
0.006
0.234
0.361
0.449
0.894
0.971
0.697
0.899
0.967
0.970

0.747
0.001
0.003
0.230
0.258
0.345
0.638
0.702
0.517
0.752
0.708
0.722

0.893
0.001
0.009
0.098
0.370
0.101
0.847
0.889
0.617
0.645
0.906
0.916

ing approximations for the three datasets.

These results are quite impressive given the fact that we con-
siderably reduced the precision of each parameter of the ConvNet,
and given the highly non-linear classiﬁcation problem where the
frontier between the face and non-face classes can be very thin and
complex.

Fig. 3 shows some face detection results from the exact model
(top) and the approximation A7,3,3,3 (bottom), i.e. a ﬁner approxi-
mation for the ﬁrst layer and a very coarse one for the rest of the
layers. The results are almost identical.

4.2 Multi-class Classiﬁcation

We studied a second case where a ConvNet has been trained for
a classical multi-class classiﬁcation problem: the MNIST hand-
written digit recognition dataset [49]. To show that the proposed
approximations can also be applied to larger networks we trained a
ConvNet with a different architecture containing again six layers
but much more maps and around 180 000 parameters and more
than 5 300 matrices in total. The input is a 32 × 32 grey-scale im-
age, and the network is composed of ﬁve convolution maps (5 × 5
kernels) followed by ﬁve average pooling maps (connected one-to-
one), 50 convolution maps (3 × 3, fully connected), 50 average pool-
ing maps (connected one-to-one), 100 neurons (6 × 6 matrices, fully
connected), and the 10 ﬁnal output neurons corresponding to the
10 digits to classify.

After having trained this ConvNet model on the MNIST dataset,
we approximated all the convolution ﬁlters, the fully-connected
layer matrices and the activation functions which is based on the
tanh-sigmoid function. Again, all pooling coefﬁcients and bias
terms were replaced by their closest CSD representation using 8
bits. The computation cost of the exact and approximate struc-
tures is shown in Table 6. Similarly to the previous experiment,
the approximate models have totally eliminated the multiplicative
costs. Floating-point arithmetic is not required; being ﬁxed-point
arithmetic adequate. The cost of the extra additions due to the
CSD representation is very low compared to the multiplicative cost
required by the exact model. The cost of bit-shifting operations is
negligible.

Table 7 shows the relative classiﬁcation rates on the MNIST test
set for the different approximations, and Fig. 4 depicts the respec-
tive ROC curves of the best-performing approximations (combined
for the 10 classes).

The results show that the approximations, although very coarse,
have a very small effect on the overall performance of the ConvNet.

In a multi-class setting, the trained ConvNet, at least in this partic-
ular case, is much more robust to the loss in precision of the weights
induced by our approximation scheme compared to the binary clas-
siﬁer. For example, a very coarse approximation like A3,3,1,1 leads
to a relative performance decrease of less than 1%.

4.3 Large-Scale Deep Neural Networks

Finally, we applied our approximation approach to a deeper and
more complex network architecture, the well-known AlexNet pro-
posed by Krizhevsky et al. [42], and the ImageNet dataset [78] for
image classiﬁcation with 1000 classes. This model contains more
than 1.2 million matrices and 5096 vectors. We approximated all
convolution ﬁlter matrices of the fully-trained 8-layer ConvNet us-
ing two different sets of dyadic rationals for different layers, a very
coarse set D9 and a slightly ﬁner set D10:
1
2

−2,−1,−

D9 =

,1,2

,0,

,−

,

,

½

D10 =

−2,−1,−

½

1
2
1
2

1
8
1
4

1
8
1
8

,−

,−

,0,

¾

,

1
8

,

1
4

1
2

,1,2

.

¾

Again, all other coefﬁcients are approximated by their closest 8-bit
CSD representation. The pooling layers do not have any coefﬁcient
here, and only linear and Rectiﬁed Linear Units (ReLU) are used
as activation function, which are already of very low complexity
and thus do not require any approximation.

We used the ImageNet 2012 validation set to evaluate our dif-
ferent approximations. And, as usual in the literature, we com-
pute the classiﬁcation accuracy as well as the top-5 accuracy for
the 50000 test images. Table 8 shows the results. The approxi-
mation A10 with the set D10 gives the best performance, with a
relative decrease in accuracy of only 3.84% and 2.18% on the top-
5 accuracy. However, as the following line shows, we can achieve
almost the same performance using the coarser set D9 for all con-
volution layers except the ﬁrst one. This again suggests that a ﬁner
approximation of the ﬁrst layer is required to prevent a drastic per-
formance drop.

5 Conclusion

We presented a novel scheme for approximating the parameters of
a trained ConvNet, notably the convolution ﬁlters, neuron weights,
as well as pooling and bias coefﬁcients. Activation functions were
also approximated. The particularity of the matrix approxima-
tions is that they allow for an extremely efﬁcient implementation—
software or hardware—using only additions and bit-shifts, and

9

Figure 3: Some CFF face detection results on the AFW dataset. Top: exact model; bottom: approximation A7,3,3,3. Despite the very
coarse approximation, the results are very close. In the second last image, the approximate model even detects an additional face, missed
by the original CFF. However, in the last example a false detection is produced.

Table 6: Arithmetic cost for MNIST-based models

Model

Exact
A1
A2
A3
A4
A5
A6
A7
A8

Operation

Mult.

Add.

CSD Add. Bit-shifting

183375
0
0
0
0
0
0
0
0

178110
178110
178110
178110
178110
178110
178110
178110
178110

-
12740
12722
49127
61228
65211
141401
158595
188417

-
23325
23307
59712
71813
75796
151986
169180
199002

Table 7: Mean classiﬁcation rates for the MNIST test set and different approximations relative to the exact model.

Exact

ASG

PLAN Linear I

Linear II Quadratic I Quadratic II

Exact
A1
A2
A3
A4
A5
A6
A7
A8
A3,3,1,1
A3,1,1,1
A4,4,1,1
A4,1,1,1

1.0000
0.9684
0.9643
0.9961
0.9973
0.9976
0.9991
0.9992
0.9994
0.9931
0.9891
0.9937
0.9885

1.0000
0.9684
0.9643
0.9961
0.9973
0.9976
0.9991
0.9992
0.9994
0.9931
0.9891
0.9937
0.9885

0.9847
0.9588
0.9627
0.9848
0.9863
0.9866
0.9868
0.9846
0.9848
0.9749
0.9684
0.9780
0.9655

0.9680
0.9260
0.8805
0.9655
0.9700
0.9666
0.9701
0.9680
0.9675
0.9625
0.9580
0.9618
0.9572

0.9978
0.9615
0.9573
0.9944
0.9969
0.9969
0.9973
0.9977
0.9981
0.9924
0.9866
0.9943
0.9872

1.0000
0.9684
0.9643
0.9961
0.9973
0.9976
0.9991
0.9992
0.9994
0.9931
0.9891
0.9937
0.9885

1.0000
0.9684
0.9643
0.9961
0.9973
0.9976
0.9991
0.9992
0.9994
0.9931
0.9891
0.9937
0.9885

Table 8: Classiﬁcation accuracy and top-5 accuracy for ImageNet and different approximations relative to the exact AlexNet model

Absolute

Relative

Accuracy

Top-5

Accuracy

Top-5

Exact
A9
A10
A10,9,9,9,9,9
A10,10,9,9,9,9

0.5682
0.4862
0.5463
0.5423
0.5442

0.7995
0.7288
0.7820
0.7794
0.7796

1.0000
0.8558
0.9616
0.9544
0.9578

1.0000
0.9117
0.9782
0.9750
0.9751

10

i

i

n
o
s
c
e
r
p

 0.998

 0.996

 0.994

 0.992

 0.99

 0.988

 0.95

exact
A3 linear II
A4 linear II
A5 linear II
A6 linear II
A7 linear II
A8 linear II

 0.96

 0.97

 0.98

 0.99

 1

recall

Figure 4: ROC curves for the MNIST hand-written digit classiﬁcation test set comparing the performance of the original (exact) ConvNet
model with its approximations.

no multiplication. We thoroughly evaluated the impact of this
parameter approximation measuring the overall performance of
ConvNets on three different use cases: one smaller ConvNet for
face detection, a larger ConvNet for hand-written digit classiﬁca-
tion, and a much more complex, deep ConvNet for large-scale im-
age classiﬁcation.

For all three models, our proposed scheme was able to produce
low-complexity approximations without a signiﬁcant loss in perfor-
mance.

These results suggest that huge reductions in computational
complexity of trained ConvNet models can be obtained, and ex-
tremely efﬁcient hardware implementations can be realized. Fur-
ther studies need to be undertaken to analyse the impact of this
type of approximations for more use cases and different architec-
tures.

References

[1] N. AHMED, T. NATARAJAN, AND K. R. RAO, Discrete cosine transform,

IEEE Transactions on Computers, C-23 (1974), pp. 90–93.

[2] C. ALIPPI AND G. STORTI-GAJANI, Simple approximation of sig-
moidal functions: realistic design of digital neural networks capable
of learning, in Proc. of IEEE Int. Symp. on Circuits and Systems, Sin-
gapore, June 1991, pp. 1505–1508.

[3] H. AMIN, K. M. CURTIS, AND B. R. HAYES-GILL, Piecewise linear
approximation applied to nonlinear function of a neural network, IEE
Proc. Circuits, Devices Sys., 144 (1997), pp. 313–317.

[4] K. BASTERRETXEA, J. M. TARELA, AND I. DEL CAMPO, Approxima-
tion of sigmoid function and the derivative for hardware implemen-
tation of artiﬁcial neurons, IEE Proceedings - Circuits, Devices and
Systems, 151 (2004), pp. 18–24.

[5] F. M. BAYER AND R. J. CINTRA, DCT-like transform for image com-
pression requires 14 additions only, Electronics Letters, 48 (2012),
pp. 919–921.

[9]

, A multiplication-free transform for image compression, in 2nd
International Conference on Signals, Circuits and Systems, Monastir,
TN, 2008, pp. 1–4.

[10]

, A low-complexity parametric transform for image compression,
in IEEE International Symposium on Circuits and Systems, Rio de
Janeiro, BR, May 2011, pp. 2145–2148.

[11] V. BRITANAK, P. YIP, AND K. R. RAO, Discrete Cosine and Sine Trans-

forms, Academic Press, 2007.

[12] W. BURGER AND M. BURGE, Digital Image Processing: An Algorith-
mic Introduction Using Java, Texts in Computer Science, Springer
London, 2016.

[13] S. CHAKRADHAR, M. SANKARADAS, V. JAKKULA, AND S. CADAMBI,
A dynamically conﬁgurable coprocessor for convolutional neural net-
works, in Proceedings of the International Symposium on Computer
Architecture, 2010, pp. 247–257.

[14] K. CHELLAPILLA, S. PURI, AND P. SIMARD, High performance con-
volutional neural networks for document processing, in Proc. of the
Int. Workshop on Frontiers in Handwriting Recognition (IWFHR’06),
2006.

[15] T. CHEN, Z. DU, N. SUN, J. WANG, C. WU, Y. CHEN, AND O. TEMAM,
DianNao: A small-footprint high-throughput accelerator for ubiqui-
tous machine-learning, in Proceedings of the International Conference
on Architectural Support for Programming Languages and Operating
Systems (ASPLOS), 2014, pp. 269–284.

[16] Y.-J. CHEN, S. ORAINTARA, AND T. NGUYEN, Video compression us-
ing integer DCT, in Proceedings 2000 International Conference on Im-
age Processing (Cat. No.00CH37101), vol. 2, Sept. 2000, pp. 844–845
vol.2.

[17] R. J. CINTRA, An integer approximation method for discrete sinu-
soidal transforms, Circuits, Systems, and Signal Processing, 30 (2011),
pp. 1481–1501.

[18] R. J. CINTRA, F. M. BAYER, AND C. J. TABLADA, Low-complexity 8-
point DCT approximations based on integer functions, Signal Process-
ing, 99 (2014), pp. 201–214.

[19] D. C. CIRESAN, U. MEIER, J. MASCI, AND J. SCHMIDHUBER, Multi-
column deep neural network for trafﬁc sign classiﬁcation, Neural Net-
works, 32 (2012), pp. 333–338.

[6] D. BIENSTOCK AND G. NEMHAUSER, Integer Programming and
Combinatorial Optimization, Lecture Notes in Computer Science,
Springer, 2004.

[20] D. C. CIRESAN, U. MEIER, AND J. SCHMIDHUBER, Multi-column deep
neural networks for image classiﬁcation, in Proceedings of the Interna-
tional Conference onComputer Vision and Pattern Recognition, 2012.

[7] R. E. BLAHUT, Fast Algorithms for Digital Signal Processing, Cam-

[21] T. H. CORMEN, C. E. LEISERSON, R. L. RIVEST, AND C. STEIN, In-

bridge University Press, 2010.

troduction to Algorithms, The MIT Press, 3rd ed., July 2009.

[8] S. BOUGUEZEL, M. O. AHMAD, AND M. N. S. SWAMY, Low-complexity
8x8 transform for image compression, Electronics Letters, 44 (2008),
pp. 1249–1250.

[22] M. COURBARIAUX AND Y. BENGIO, Binarynet: Training deep neural
networks with weights and activations constrained to +1 or −1, CoRR,
abs/1602.02830 (2016).

11

[23] M. COURBARIAUX, Y. BENGIO, AND J. DAVID, Low precision arith-

metic for deep learning, CoRR, abs/1412.7024 (2014).

[24] M. COURBARIAUX, Y. BENGIO, AND J.-P. DAVID, BinaryConnect:
Training deep neural networks with binary weights during propaga-
tions, in Proceedings of theAnnual Conference on Neural Information
Processing Systems, 2015.

[25]

, Binaryconnect: Training deep neural networks with binary
weights during propagations, in Advances in Neural Information Pro-
cessing Systems, 2015, pp. 3123–3131.

[26] V. A. COUTINHO, R. J. CINTRA, F. M. BAYER, P. A. M. OLIVEIRA,
R. S. OLIVEIRA, AND A. MADANAYAKE, Pruned discrete Tchebichef
transform approximation for image compression, Circuits, Systems,
and Signal Processing, (2018).

[27] M. DELAKIS AND C. GARCIA, Text detection with convolutional neu-
ral networks, in VISAPP 2008: Proceedings of the Third International
Conference on Computer Vision Theory and Applications, vol. 2, Fun-
chal, Madeira, Portugal, Jan. 2008, pp. 290–294.

[28] A. G. DEMPSTER AND M. D. MACLEOD, Constant integer multiplica-
tion using minimum adders, IEE Proceedings - Circuits, Devices and
Systems, 141 (1994), pp. 407–413.

[29] M. DENIL, B. SHAKIBI, L. DINH, AND N. DE FREITAS, Predicting
parameters in deep learning, in Proceedings of theAnnual Conference
on Neural Information Processing Systems, 2013.

[30] E. DENTON, W. ZAREMBA, AND J. BRUNA, Exploiting linear structure
within convolutional networks for efﬁcient evaluation, in Proceedings
of theAnnual Conference on Neural Information Processing Systems,
2014.

[31] S. DRAGHICI, On the capabilities of neural networks using limited pre-

cision weights, Neural Networks, 15 (2002), pp. 395–414.

[32] K. ELAGOUNI, C. GARCIA, F. MAMALET, AND P. SÉBILLOT, Text
recognition in multimedia documents: a study of two neural-based
OCRs using and avoiding character segmentation, IJDAR, 17 (2014),
pp. 19–31.

[33] C. FARABET, B. MARTINI, P. AKSELROD, S. TALAY, Y. LECUN, AND
E. CULURCIELLO, Hardware accelerated convolutional neural net-
works for synthetic vision systems, in International Symposium on Cir-
cuits and Systems (ISCAS 2010), May 30 - June 2, 2010, Paris, France,
2010, pp. 257–260.

[34] N. FARRUGIA, F. MAMALET, S. ROUX, F. YANG, AND M. PAIN-
DAVOINE, Fast and robust face detection on a parallel optimized ar-
chitecture implemented on FPGA, IEEE Trans. Circuits Syst. Video
Techn., 19 (2009), pp. 597–602.

[35] C. GARCIA AND M. DELAKIS, Convolutional face ﬁnder: A neural ar-
chitecture for fast and robust face detection, IEEE Trans. Pattern Anal.
Mach. Intell., 26 (2004), pp. 1408–1423.

[36] R. HADSELL, P. SERMANET, M. SCOFFIER, A. ERKAN, K. KAVACK-
UOGLU, U. MULLER, AND Y. LECUN, Learning long-range vision for
autonomous off-road driving, Journal of Field Robotics, (2009).

[37] S. HAYKIN, Neural Networks, Prentice-Hall, Inc., Upper Saddle River,

NJ, 1999.

[38] M. JADERBERG, A. VEDALDI, AND A. ZISSERMAN, Speeding up con-
volutional neural networks with low rank expansions, in Proceedings
of theBritish Machine Vision Conference, 2014.

[39] V. JAIN AND E. LEARNED-MILLER, FDDB: A benchmark for face de-
tection in unconstrained settings, Tech. Rep. UM-CS-2010-009, Uni-
versity of Massachusetts, Amherst, 2010.

[40] A. M. JOSHI, V. MISHRA, AND R. M. PATRIKAR, Design of real-time
video watermarking based on integer dct for H.264 encoder, Interna-
tional Journal of Electronics, 102 (2015), pp. 141–155.

[41] M. KIM AND S. PARIS, Bitwise neural networks, in ICML Workshop

on Resource-Efﬁcient Machine Learning, 2015.

[42] A. KRIZHEVSKY, I. SUTSKEVER, AND G. E. HINTON, Imagenet classi-
ﬁcation with deep convolutional neural networks, in Advances in Neu-
ral Information Processing Systems 25: 26th Annual Conference on
Neural Information Processing Systems 2012. Proceedings of a meet-
ing held December 3-6, 2012, Lake Tahoe, Nevada, United States.,
2012, pp. 1106–1114.

[43] H. KWAN AND C. TANG, Multiplyerless multilayer feedforward neural
network design suitable for continuous input-output mapping, Elec-
tronic Letters, 29 (1993), pp. 1259–1260.

[44] D. LARKIN, A. KINANE, V. MURESAN, AND N. O’CONNOR, An efﬁ-
cient hardware architecture for a neural network activation function
generator, in Advances in Neural Networks – ISNN, 2006, pp. 1319–
1327.

[45] V. LEBEDEV, Y. GANIN, M. RAKHUBA, I. OSELEDETS, AND V. LEM-
PITSKY, Speeding up convolutional neural networks using ﬁne-tuned
CP decomposition, in Proceedings of theInternational Conference on
Learning Representations, 2015.

[46] Y. LECUN, Generalization and network design strategies, in Proceed-
ings of the International Conference Connectionism in Perspective,
University of Zürich, Oct. 1988, pp. 10–13.

[47] Y. LECUN, Generalization and network designs strategies, Technical

Report CRG-TR-89-4, Department of Computer Science, 1989.

[48] Y. LECUN, L. BOTTOU, Y. BENGIO, AND P. HAFFNER, Gradient-based
learning applied to document recognition, Proceeding of the IEEE,
(1998).

[49] Y. LECUN, C. CORTES, AND C. J. BURGES, The MNIST database of

handwritten digits. http://yann.lecun.com/exdb/mnist/, 2015.

[50] J. LEE AND S. LEYFFER, Mixed Integer Nonlinear Programming, The
IMA Volumes in Mathematics and its Applications, Springer New
York, 2011.

[51] J. K. LENSTRA AND A. H. G. R. KAN, Computational complexity of
discrete optimization problems, Annals of Discrete Mathematics, 4
(1979), pp. 121–140.

[52] E. L. MACHADO, C. J. MIOSSO, R. VON BORRIES, M. COUTINHO,
P. DE AZEVEDO BERGER, T. MARQUES, AND R. P. JACOBI, Com-
putational cost reduction in learned transform classiﬁcations, CoRR,
(2015).

[53] F. MAMALET AND C. GARCIA, Simplifying convnets for fast learn-
ing, in Artiﬁcial Neural Networks and Machine Learning - ICANN
2012 - 22nd International Conference on Artiﬁcial Neural Networks,
Lausanne, Switzerland, September 11-14, 2012, Proceedings, Part II,
2012, pp. 58–65.

[54] F. MAMALET, S. ROUX, AND C. GARCIA, Real-time video convolu-
tional face ﬁnder on embedded platforms, EURASIP J. Emb. Sys., 2007
(2007).

[55] M. MARCHESI, G. ORLANDO, F. PIAZZA, AND A. UNCINI, Fast neu-
ral networks without multipliers, IEEE Transactions on Neural Net-
works, 4 (1993), pp. 53–62.

[56] M. MATHIAS, R. BENENSON, M. PEDERSOLI, AND L. VAN GOOL,
Face detection without bells and whistles, in Proceesings of the Euro-
pean Conference on Computer Vision, 2014, pp. 720–735.

[57] M. MATHIEU, M. HENAFF, AND Y. LECUN, Fast training of convo-
lutional networks through FFTs, in Proceedings of theInternational
Conference on Learning Representations, 2014.

[58] MATLAB, version 7.10.0 (R2010a), The MathWorks Inc., Natick,

Massachusetts, 2010.

[59] N. MEGIDDO, Linear programming in linear time when the dimension

is ﬁxed, Journal of the ACM (JACM), 31 (1984), pp. 114–127.

[60] P. K. MEHER, S. Y. PARK, B. K. MOHANTY, K. S. LIM, AND C. YEO,
Efﬁcient integer DCT architectures for HEVC, IEEE Transactions on
Circuits and Systems for Video Technology, 24 (2014), pp. 168–178.

[61] D. MIYASHITA, E. H. LEE, AND B. MURMANN, Convolutional
neural networks using logarithmic data representation, CoRR,
abs/1603.01025 (2016).

[62] P. A. M. OLIVEIRA, R. J. CINTRA, F. M. BAYER, S. KULASEKERA,
AND A. MADANAYAKE, A discrete Tchebichef transform approximation
for image and video coding, IEEE Signal Processing Letters, 22 (2015),
pp. 1137–1141.

[63] M. OSADCHY, Y. LECUN, M. L. MILLER, AND P. PERONA, Synergistic
face detection and pose estimation with energy-based model, in Proc. of
Advances in Neural Information Processing Systems (NIPS’05), 2005.

12

[64] K. OSAWA AND R. YOKOTA, Evaluating the compression efﬁciency of
the ﬁlters in convolutional neural networks, in Artiﬁcial Neural Net-
works and Machine Learning - ICANN 2017, A. Lintas, S. Rovetta,
P. V. P., and A. Villa, eds., vol. 10614 of Lecture Notes in Computer
Science, Springer, 2017, pp. 459–466.

[65] C. H. PAPADIMITRIOU AND K. STEIGLITZ, Combinatorial Optimiza-

tion: Algorithms and Complexity, Dover Publications, 1998.

[66] U. S. POTLURI, A. MADANAYAKE, R. J. CINTRA, F. M. BAYER, S. KU-
LASEKERA, AND A. EDIRISURIYA, Improved 8-point approximate DCT
for image and video compression requiring only 14 additions, IEEE
Transactions on Circuits and Systems I, 61 (2014), pp. 1727–1740.

[67] R CORE TEAM, R: A Language and Environment for Statistical Com-
puting, R Foundation for Statistical Computing, Vienna, Austria,
2013.

[68] M. RASTEGARI, V. ORDONEZ, J. REDMON, AND A. FARHADI, XNOR-
Net: Imagenet classiﬁcation using binary convolutional neural net-
works, CoRR, abs/1603.05279 (2016).

[69] D. E. RUMELHART, G. E. HINTON, AND R. J. WILLIAMS, Learn-
ing representations by back-propagating errors, Nature, 323 (1986),
pp. 533–536.

[70] T. N. SAINATH, B. KINGSBURY, V. SINDHWANI, E. ARISOY, AND
B. RAMABHADRAN, Low-rank matrix factorization for deep neural net-
work training with high-dimensional output targets, in ICASSP, 2013,
pp. 6655–6659.

[71] D. SCHERER, H. SCHULZ, AND S. BEHNKE, Accelerating large-scale
convolutional neural networks with parallel graphics multiprocessors,
in Proceedings of theInternational Conference on Artiﬁcial Neural
Networks, 2010, pp. 82–91.

[72] J. SCHLESSMAN, Approximation of the sigmoid function and its
derivative using a minimax approach, Master’s thesis, Lehigh Uni-
versity, Aug. 2002.

[73] G. A. F. SEBER, A Matrix Handbook for Statisticians, Wiley-

Interscience, 2007.

[78] K. SIMONYAN AND A. ZISSERMAN, Very deep convolutional networks

for large-scale image recognition, CoRR, abs/1409.1556 (2014).

[79] C. J. TABLADA, F. M. BAYER, AND R. J. CINTRA, A class of DCT ap-
proximations based on the Feig-Winograd algorithm, Signal Process-
ing, 113 (2015), pp. 38–51.

[80] O. TEMAM, A defect-tolerant accelerator

emerging high-
performance applications, in Proceedings of the International Sympo-
sium on Computer Architecture, 2012, pp. 356–367.

for

[81] M. T. TOMMISKA, Efﬁcient digital implementation of the sigmoid func-
tion for reprogrammable logic, IEE Proceedings - Computers and Dig-
ital Techniques, 150 (2003), pp. 403–411.

[82] R. VAILLANT, C. MONROCQ, AND Y. LE CUN, An original approach
for the localization of objects in images, in IEE Proc on Vision, Image,
and Signal Processing, 1994, pp. 141(4):245–250.

[83] V. VANHOUCKE, A. SENIOR, AND M. MAO, Improving the speed of
neural networks on CPUs, in Proceedings of the Deep Learning and
Unsupervised Feature Learning NIPS Workshop, 2011.

[84] J. VINCENT AND D. MYERS, Weight dithering and wordlength selec-
tion for digital backpropagation networks, BT Technology Journal, 10
(1992), pp. 124–133.

[85] B. WHITE AND M. ELMASRY, The digi-neocognitron: A digital neocog-
nitron neural network model for VLSI, IEEE Transactions on Neural
Networks, 3 (1992), pp. 73–85.

[86] J. XUE, J. LI, AND Y. GONG, Restructuring of deep neural network
acoustic models with singular value decomposition, in Interspeech,
2013.

[87] J. YAN, X. ZHANG, Z. LEI, AND S. LI, Face detection by structural

models, Image and Vision Computing, 32 (2014), pp. 790–799.

[88] Z. YANG, M. MOCZULSKI, M. DENIL, N. D. FREITAS, A. SMOLA,
L. SONG, AND Z. WANG, Deep Fried Convnets, in Proceedings of theIn-
ternational Conference on Computer Vision, 2014.

[74] P. SERMANET, S. CHINTALA, AND Y. LECUN, Convolutional neural
networks applied to house numbers digit classiﬁcation, in Proceedings
of the 21st International Conference on Pattern Recognition, ICPR
2012, Tsukuba, Japan, November 11-15, 2012, 2012, pp. 3288–3291.

[89] C. ZHANG, P. LI, G. SUN, Y. GUAN, B. XIAO, AND J. CONG, Optimiz-
ing FPGA-based accelerator design for deep convolutional neural net-
works, in Proceedings of the ACM/SIGDA International Symposium
on Field-Programmable Gate Arrays - FPGA, 2015, pp. 161–170.

[75] P. SERMANET, D. EIGEN, X. ZHANG, M. MATHIEU, R. FERGUS, AND
Y. LECUN, OverFeat: Integrated recognition, localization and detec-
tion using convolutional networks, CoRR, abs/1312.6229 (2013).

[90] M. ZHANG, S. VASSILIADIS, AND J. G. DELGADO-FRIAS, Sigmoid
generators for neural computing using piecewise approximations,
IEEE Transactions on Computers, 45 (1996), pp. 1045–1049.

[76] P. SERMANET, K. KAVUKCUOGLU, S. CHINTALA, AND Y. LECUN,
Pedestrian detection with unsupervised multi-stage feature learning,
in 2013 IEEE Conference on Computer Vision and Pattern Recogni-
tion, Portland, OR, USA, June 23-28, 2013, 2013, pp. 3626–3633.

[77] P. SIMARD AND H. P. GRAF, Backpropagation without multiplication,
in Proceedings of theAnnual Conference on Neural Information Pro-
cessing Systems, 1994, pp. 232–239.

[91] M. ZHANG, S. VASSILIADIS, AND J. G. DELGADO-FRIAS, Sigmoid
generators for neural computing using piecewise approximations,
IEEE Transactions on Computers, 45 (1996), pp. 1045–1049.

[92] X. ZHU AND D. RAMANAN, Face detection, pose estimation, and land-
mark localization in the wild, in Proceedings of the International Con-
ference onComputer Vision and Pattern Recognition, 2012.

13

