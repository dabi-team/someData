0
2
0
2

b
e
F
8
1

]

O
R
.
s
c
[

2
v
6
4
2
5
0
.
7
0
9
1
:
v
i
X
r
a

DEEP REINFORCEMENT-LEARNING-BASED DRIVING POLICY
FOR AUTONOMOUS ROAD VEHICLES

A PREPRINT

Konstantinos Makantasis1,3, Maria Kontorinaki1,2, Ioannis Nikolos 1
1 School of Production Engineering and Management, Technical University of Crete, Chania, Greece
2 Department of Statistics and Operations Research, Faculty of Science, University of Malta, Msida, Malta
3 Institute of Digital Games, University of Malta, Msida, Malta
konst.makantasis@gmail.com, mkontorinaki@dssl.tuc.gr, jnikolo@dpem.tuc.gr

ABSTRACT

In this work the problem of path planning for an autonomous vehicle that moves on a freeway
is considered. The most common approaches that are used to address this problem are based on
optimal control methods, which make assumptions about the model of the environment and the
system dynamics. On the contrary, this work proposes the development of a driving policy based on
reinforcement learning. In this way, the proposed driving policy makes minimal or no assumptions
about the environment, since a priori knowledge about the system dynamics is not required. Driving
scenarios where the road is occupied both by autonomous and manual driving vehicles are considered.
To the best of our knowledge, this is one of the ﬁrst approaches that propose a reinforcement learning
driving policy for mixed driving environments. The derived reinforcement learning policy, ﬁrstly, is
compared against an optimal policy derived via dynamic programming, and, secondly, its efﬁciency
is evaluated under realistic scenarios generated by the established SUMO microscopic trafﬁc ﬂow
simulator. Finally, some initial results regarding the effect of autonomous vehicles’ behavior on the
overall trafﬁc ﬂow are presented.

Keywords Autonomous vehicles · Reinforcement learning · Driving policy

1

Introduction

In recent years, there has been a growing interest in self-driving vehicles. Building such autonomous systems has been
an active area of research [1, 2] for its high potential in leading to road networks that are much more safer and efﬁcient.
Although vehicle automation has already led to great achievements in supporting the driver in various monotonous
and challenging tasks, see for example [3], rising the level of automation to fully-automated driving is an extremely
challenging problem. This is mainly due to the complexity of real-world environments, including avoiding obstacles,
and human driving behavior aspects.

According to Donges [4], autonomous driving tasks can be roughly classiﬁed into three categories; navigation, guidance,
and stabilization. Navigation tasks are responsible for generating road-level routes. Tactical level guidance tasks
are responsible for guiding autonomous vehicles along these routes in complex environments by generating tactical
maneuver decisions. Finally, operational level stabilization tasks are responsible for translating tactical decisions into
reference trajectories and then low-level controls that need to be tracked by the vehicle.

Several methodologies have been proposed for addressing the problem of generating efﬁcient road-level routes. In the
works of [5] and [6] the authors propose a navigation algorithm based on a route reservation mechanism, in order to
generate road-level routes, and at the same time avoid trafﬁc congestion. In [7] a computationally efﬁcient algorithm
that can scale to very large transportation networks is presented. The authors of [8] focus on "green" navigation by
proposing a methodology to generate routes that minimize emissions. The work in [9] surveys exact algorithms for
addressing the routing problem under capacity and travel time constraints. Finally, a comprehensive review regarding
the generation of road-level routes in transportation networks can be found in [10]. Despite the research interest,

 
 
 
 
 
 
A PREPRINT

navigation examined by the autonomous driving perspective can be considered as a mature technology, since already
exist commercial and free applications for road-level route generation.

At the same time, vehicles are man-made products for which automotive industry has decades-long experience in
vehicle dynamics modeling, see for example [11] and [12]. Therefore, the operational level stabilization tasks, also
know as the acting part of autonomous driving, are well understood and modelled in control theory and robotics.

Tactical level guidance, referred also as driving policy, is crucial for enabling fully autonomous driving. On the contrary,
however, to navigation and stabilization, tactical level guidance methodologies cannot be considered mature enough, in
order to be applied to autonomous vehicles that move in unrestricted environments. A driving policy should be able to
make decisions in real-time and in complex environments, in order to plan and update a vehicle path, which should be
safe, collision-free, and user acceptable [13]. The requirement for real-time operation in highly complex environments,
as well as the safety constraints make current driving policies inadequate for fully autonomous driving.

Based on the discussion so far, this work aims to contribute towards the development of a robust driving policy for
autonomous vehicles that being capable of making decisions in real-time. The operation environment s restricted by
considering vehicles that move on a highway. Highways consist a speciﬁc and very important type of transportation
networks [14, 15]. Due to their high-capacity they can serve millions of people everyday, while at the same time, allow
users to travel with higher speed and less accelerations/decelerations compared to urban transportation networks. The
driving policy development problem is formulated from an autonomous vehicle perspective (ego vehicle), and, thus,
there is no need to make any assumptions regarding the kind of other vehicles (manual driving or autonomous) that
occupy the road.

Finally, the proposed methodology approaches the problem of driving policy development by exploiting recent advances
in Reinforcement Learning (RL) combined with the responsibility sensitive safety model, proposed in [16]. The
developed RL-based driving policy aims to avoid accidents (departures from the road and crashes with other vehicles),
move the vehicle with a desired speed, minimize accelerations/decelerations, and minimize lane changes. The latter two
criteria are also related to the comfort of vehicle passengers [17].

1.1 Related Work

The problem of path planning for autonomous vehicles can be seen as a trajectory generation problem corresponding to
the creation of a quasi-continuous sequence of states that must be tracked by the vehicle over a speciﬁc time horizon.
Trajectory generation has been widely studied in robotics [18]. Considering, however, road vehicles, path planning is a
much more critical task, since passengers’ safety must be guaranteed.

Under certain assumptions, simpliﬁcations and conservative estimates, heuristic, hand-designed rules can be used for
tactical decision making [19]. Such methods, however, are often tailored for speciﬁc non-complex environments and do
not generalize robustly [20]. Therefore, they are not able to cope with the complexity of real-world environments and
the diversity of driving conditions, let alone human driving behavior aspects. To overcome the limitations of rule-based
methods, approaches based on the careful design and exploitation of potential ﬁeld and optimal control methods have
also been proposed.

Potential ﬁeld methods generate a ﬁeld, or, in other words, an objective function the minimization of which corresponds
to the objectives of an agent. These methods are based on the design of potential functions for obstacles, road structures,
trafﬁc regulation and the goals to be achieved. Then, the overall objective function is expressed as the weighted sum of
the designed potential functions. The minimization is achieved via the generation of a vehicle trajectory moving towards
the descent direction of the overall objective function [21, 22, 23]. However, due to the fact that vehicle dynamics are
not considered during decision making, the generated trajectory may turn out to be non-feasible to be tracked by the
vehicle [24].

This drawback can be alleviated by formulating the trajectory generation problem as an optimal control problem,
which inherently takes into consideration system dynamics. Speciﬁcally, optimal control approaches allow for the
concurrent consideration of system dynamics and carefully designed potential ﬁelds [25]. In the work of [17] an optimal
control methodology for vehicles’ trajectory planning in the context of cooperative merging on highways, is presented.
The works of [26, 27] propose two optimal control based methodologies for trajectory planning, which incorporate
constraints for obstacles, so as to keep the automated vehicle robustly far from them. In the same spirit, the works in
[28] and in [29] design appropriate potential functions corresponding to the presence of obstacles, which, in turn, are
incorporated in the objective function to generate a collision-free path. Optimal control approaches usually map the
optimal control problem to a nonlinear programming (NLP) problem that can be solved using numerical NLP solvers,
see for example [30, 31, 28]. Although, potential ﬁeld and optimal control methods are quite popular due to the intuitive
problem formulation [32], there are still open issues regarding the decision making process.

2

A PREPRINT

First of all, mapping the optimal control problem to a NLP problem and solving it by employing numerical NLP solvers,
produces a locally optimal solution for which the guarantees of the globally optimal solution may not hold, and, thus,
the safety guarantees for the generated trajectory may be compromised [33]. For this reason dynamic programming
techniques have also been proposed for solving the optimal control problem. Although, dynamic programming
techniques produce a globally optimal solution, due to the curse of dimensionality [34], they are restricted to small
scale problems. Moreover, another problem faced with potential ﬁeld and optimal control approaches is the strong
dependency to a relatively simple environment model, usually with hand-crafted observation spaces, transition dynamics
and measurements mechanisms. These assumptions limit the generality of these methods to complex scenarios, since
they are not able to cope with environment uncertainties and measurements errors. Finally, optimal control methods are
not able to generalize, i.e., to associate a state of the environment with a decision without solving an optimal control
problem. This means that every time a sequence of decisions needs to be made an optimal control problem needs to
be solved, even if exactly the same problem has been solved in the past. This requirement signiﬁcantly increases the
computational cost of these methods.

Due to its recent success, supervised deep learning has also been considered as an alternative approach for developing
driving policies. In [35] a convolutional neural network is trained in a supervised manner to output continuous steering
actions. In the work of [36] a recurrent neural network is trained to output a steering angle, after a driving intention
has been estimated. The works in [37] and [38] also exploit end-to-end trainable neural networks that output feasible
driving actions and affordance indicators (such as distance between cars). The aforementioned approaches are based on
end-to-end trainable neural network architectures that are able to output low-level controls directly from input images.
Therefore, this kind of driving policies correspond to the outcome of a supervised learning algorithm, where deep
neural networks were trained to imitate the behavior of human drivers. However, such methods, ﬁrst, result to black-box
driving policies, which are susceptible to inﬂuence of drifted inputs, and second, are restricted to the limitations of
end-to-end learning [39].

Very recently, RL methods have also been proposed as challenging alternative approaches towards the development of
driving policies. RL-based approaches alleviate the strong dependency on hand-crafted simple environment models and
dynamics, and, at the same time, can fully exploit the recent advances in deep supervised machine learning [40]. Along
this line of research the work [41] utilizes a deep Q-Network to make decisions for intersection crossing, while the work
[42] exploits a similar architecture to make decisions about lane changing in freeways. In [43], the authors propose a
hierarchical RL-based approach for deriving a low-level driving policy capable of guiding a vehicle from an origin
point to a destination point. In [44] a policy gradient RL approach is used to develop a driving policy for cooperative
double merging scenarios. This approach combines a RL policy with a non-learnable mechanism to balance between
efﬁciency and safety. Finally, the work in [45] presents some elements of efﬁcient deep RL (empirically validated) for
decreasing the learning time and increasing the efﬁciency of RL-based driving policies.

Despite the fact that only very recently reinforcement learning was employed for developing driving policies, experi-
mental results appear very promising. The main drawback, however, of these approaches regards safety guarantees.
Due to the fact that the probability of an accident is very small, learning based approaches, as shown in [16], cannot
assure collision-free trajectories.

1.2 Proposed Work

This work proposes a RL-based approach towards the development of a driving policy for autonomous road vehicles.
The proposed RL-based method has several advantages over potential ﬁeld and optimal control methods. First of all,
RL-based approaches are model-free. They make the assumption that there is a state-transition model that describes the
system dynamics, which remains ﬁxed. However, the exact form of this model is not required to be a priori known
(typically such a model is considered unknown), but it is being inferred during training. Second, a driving policy based
on RL is able to generalize. After training, a RL-based policy has inferred a mapping for associating a given state of the
environment with a decision. In contrast to potential ﬁeld and optimal control methods, whenever a decision needs to
be made no problem needs to be solved; decision making can be done by simply evaluating the policy function. Third,
since a RL-based driving policy has been estimated, it can be shared across multiple autonomous vehicles, which in turn
can make decisions through the policy function evaluations. On the contrary, driving policy sharing is not possible when
potential ﬁeld and optimal control methods are used, since each vehicle needs to solve a decision making problem for
its own sake. Finally, since no learning-based driving policy can guarantee absolute safety, our work is motivated by the
formal responsibility sensitive safety model, proposed in [16], in order to derive and utilize ad-hoc rules that guarantee
responsibility-wise safety. That is, the ad-hoc rules guarantee that the autonomous vehicles will not be responsible for
any occurred accident. To the best of our knowledge, this work is one of the ﬁrst attempts that try to derive a RL driving
policy, combined with ad-hoc safety rules, targeting unrestricted highway environments, which are occupied by both
autonomous and manual driving vehicles.

3

A PREPRINT

Furthermore, the proposed RL-based driving policy is compared against an optimal policy derived using dynamic
programming, in terms of safety metrics, such as the number of collisions, and efﬁciency metrics, such as the average
time the autonomous vehicle moves with the desired speed. Although, dynamic programming techniques, due to the
curse of dimensionality [46], are restricted to small-scale problems, and are not suitable for real-time applications, they
produce globally optimal solutions to an optimal control problem, i.e., optimal driving policies. Thus, the comparison of
the proposed methodology against optimal driving policies, ﬁrst, will result to an objective evaluation for the RL-based
driving policy, and, second, can provide insights to the driving policy development problem.

The developed RL-based driving policy is also compared against manual driving using SUMO simulator. Through
this comparison, the generalization ability and stability of the proposed RL-based driving policy to ensure reliability is
evaluated; any learning system must generalize well to out-of-sample data, and be stable, i.e., small perturbations in
the input should slightly affect the output. Speciﬁcally, the RL-based driving policy is applied to randomly generated
driving scenarios (previously unseen driving conditions), with and without drivers’ imperfection and measurements
errors. Drivers’ imperfection and measurements errors can be seen as disturbances, and can be incorporated into driving
scenarios using appropriate settings in SUMO simulator.

Finally, preliminary results regarding the effect of autonomous vehicles on the overall trafﬁc ﬂow are provided. The
RL-based driving policy, seen by an autonomous vehicle perspective, is a selﬁsh policy. That is, each autonomous
vehicle that follows the RL policy tries to achieve its own goals disregarding the rest of the vehicles. Such a behavior
might have a negative effect on the overall trafﬁc ﬂow.

The rest of the paper is organized as follows: Section 2 describes the problem and the underlying assumptions, Section
3 gives a brief description of the RL framework, Section 3 presents in detail the development of the RL-based driving
policy and Section 5 the derivation of ad-hoc rules towards the design of a collision-free trajectory. Section 6 presents
the experimental setup and the experimental results, and Section 8 concludes this work.

2 Problem Description and Assumptions

The problem of path planning for an autonomous vehicle that moves on freeway, which is also occupied by manual
driving vehicles is considered. Without loss of generality, it is assumed that the freeway consists of three lanes. The
path planning algorithm, or in other, words the driving policy, should generate a collision-free trajectory for the
autonomous vehicle to follow. Moreover, the generated trajectory should permit the autonomous vehicle to move
forward with a desired speed, and, at the same time, minimize its longitudinal and lateral accelerations/decelerations.
The aforementioned three criteria are the objectives of the driving policy, and therefore, the goal that the RL algorithm
should achieve.

For the generation of an optimal trajectory using dynamic programming, the manual driving vehicles is required to
move with a constant speed following the kinematics equations. The generation of the optimal trajectory, via dynamic
programming, corresponds to the solution of a ﬁnite horizon optimal control problem. The aforementioned requirement
assures that the dynamics of the system will be a priori and fully known, and no disturbances will be present in the
system in order for the dynamic programming technique to produce the trajectory. However, for training the RL policy
the aforementioned system dynamics are not given to the algorithm, and, thus, are considered unknown.

Regarding the SUMO simulator, the manual driving vehicles move on the freeway using the Krauss car following model
[47]. It is assumed that letting the manual driving vehicles to move using the Krauss car following model will produce
realistic driving behaviors. Moreover, manual driving vehicles should move forward with a desired speed. In order to
generate realistic and customary trafﬁc conditions, we assume that at least two categories of manual driving vehicles
should be present at the freeway; manual driving vehicles that want to move faster than the autonomous vehicle, and
manual driving vehicles that want to move slower. At this point, it should be stressed that, although the manual vehicles
are moving using the Krauss model, this model is not given to the RL training algorithm, and, thus, from a RL point of
view it is considered unknown.

During the trajectory generation this work does not assume any communication between the autonomous vehicle and
other vehicles. Instead, the information available for the trajectory generation is obtained solely by sensors, such as
cameras, LiDAR and proximity sensors, installed on the autonomous vehicle. This work also assumes the availability
of a fusion module of the on-board sensors’ information, with the appropriate redundancy and cross-checking, to assure
the usefulness and accuracy of the provided information. Using such sensors, the autonomous vehicle can estimate the
position and the velocity of its surrounding vehicles. Therefore, the state representation of the autonomous vehicle and
its surrounding environment, includes information that is associated solely with the position and the velocity of the
vehicles present in the sensing area of the autonomous vehicle.

4

A PREPRINT

Furthermore, it is assumed that the freeway does not contain any turns. However, the generated vehicle trajectory
essentially reﬂects the vehicle longitudinal position, speed, and its traveling lane. The derived trajectory needs to
be tracked by the underlying vehicle control loops, based on high-deﬁnition maps. Therefore, for the trajectory
speciﬁcation, possible curvatures may be aligned to form an equivalent straight section [28].

Finally, the trajectory of the autonomous vehicle can be fully described by a sequence of goals that the vehicle should
achieve. Each one of the goals should be achieved within a speciﬁc time interval, and represents vehicle’s desires,
such as change lane, brake with a given deceleration, etc. These goals deﬁne the trajectory to be followed by the
autonomous vehicle in a higher level, and cannot be directly used by the vehicle control loops. Instead, it is assume that
the mechanism which translates these goals to low level controls and implements them is given.

Based on the aforementioned problem description and underlying assumptions, the main objective of this work is to
develop a driving policy. The driving policy will exploit the information coming from a set of sensors installed on the
autonomous vehicle, in order to set a goal for the vehicle to achieve, via a high-level action, during a speciﬁc time
interval. In other words, the objective is to derive a function that will map the information about the autonomous vehicle,
as well as, its surrounding environment to a speciﬁc goal and the corresponding high-level action for achieving it.

3 RL and Prioritized Experience Replay

In this work the development of a driving policy is being tackled as a RL problem, where the state-action value function
Q is approximated by a Double Deep Q-Network (DDQN) [48] using prioritized experience replay [49]. Therefore,
for the sake of completeness, in this section the RL framework and the algorithm of prioritized experience replay are
brieﬂy presented.

3.1 Reinforcement Learning

In the RL framework, an agent interacts with the environment in a sequence of actions (selected by following a speciﬁc
policy), observations, and rewards. In particular, at each time step t, the agent (in our case the autonomous vehicle)
observes the state of the environment st ∈ S and, based on a speciﬁc policy, it selects an action at ∈ A, where S
is the state space and A = {1, · · · , K} is the set of available actions. Then, the agent observes the new state of the
environment, st+1, which is the consequence of applying the action at at state st, and a scalar reward signal rt, which
is a quality measure of how good is to select action at at state st.

The goal of the agent is to interact with the environment by selecting actions in a way that maximizes the cumulative
future rewards, also known as future return. Future rewards are discounted by a factor 0 ≤ γ < 1 per time step, and the
future return at time t is deﬁned as

T
(cid:88)

Rt =

γt(cid:48)−trt(cid:48),

(1)

t(cid:48)=t

where parameter T denotes how many time steps ahead t are taken into consideration for calculating Rt. The non-
negative discount factor 0 ≤ γ < 1 determines the importance of future rewards. In other words, it weighs future
rewards, by giving higher weight to rewards received near than rewards received further in the future.

The interaction of the agent with the environment can be explicitly deﬁned by a policy function π : S → A that maps
states to actions. The maximum expected future reward achievable by following any policy after observing a state s and
selecting an action a is represented by the optimal action-value function Q∗(s, a), which is deﬁned as

Q∗(s, a) = max

π

E[Rt|st = s, at = a, π].

(2)

The optimal action-value function obeys a very important identity known as Bellman equation. That is, if the optimal
action-value function Q∗(st+1, at+1) of the state st+1 at the next time step was known for all possible actions at+1, then
the policy maximizing the future reward is to select the action maximizing the expected value of r + γQ∗(st+1, at+1),
and, thus, the following

Q∗(s, a) = Est+1[rt + γ max
at+1

Q∗(st+1, at+1)|st = s, at = a]

(3)

holds for the optimal action-value function when state st is observed and action at is selected. The expectation in
relation (3) is with respect to all possible states at the next time step.
The relation in (3) implies that the problem of estimating the optimal policy is equivalent to the estimation of Q∗(s, a)
for every pair (s, a) ∈ S × A. Although, Q∗(s, a) can be efﬁciently estimated when small scale problems need to be
addressed [50], for large state spaces estimating Q∗(s, a) for every possible (s, a) pair is practically implausible. For

5

A PREPRINT

such kind of problems, the optimal action-value function is approximated, ˜Q∗(s, a; θ) ≈ Q∗(s, a), using a learning
machine, such as linear regression of neural networks [40], parameterized by θ. Parameters θ are estimated by following
an iterative procedure for minimizing a sequence of loss functions

Li(θi) = Es,a[( ˜Q(s, a; θi−1) − ˜Q(s, a; θi))2],
where i stands for the iteration index. The (s, a, r, s(cid:48)) tuples used in relations (3) and (4) are generated by following an
(cid:15)-greedy policy that selects at a given state a greedy action with probability 1 − (cid:15) and a random action with probability
(cid:15).

(4)

The aforementioned procedure for estimating θ looks like a regression problem in the supervised learning paradigm.
However, there are two signiﬁcant differences. First, the learning machine sets itself and follows the targets ˜Q(s, a; θi−1),
which can lead to instabilities and divergence, and, second, the generated (s, a, r, s(cid:48)) tuples are not independently
generated; a property that is required by many learning machines.

To overcome the ﬁrst problem, two identical learning machines are used; one for setting the targets and one for following
them. The machine that sets the targets is freezed in time, i.e., its parameters are ﬁxed for several iterations. After a
predeﬁned number of iterations has passed, the parameters of the machine that sets the targets are updated by coping
the parameters from the machine that follows the targets. If we denote as ˆθ the parameters of the machine that sets the
targets, then the loss function Li(θi) in relation (4) is given by

Li(θi) = Es,a[( ˜Q(s, a; ˆθ) − ˜Q(s, a; θi))2].

(5)

3.2 Prioritized Experience Replay Algorithm

To overcome the latter problem, a Prioritized Experience Replay (PER) algorithm is employed to break the correlations
between the generated (s, a, r, s(cid:48)) tuples. The generated tuples are stored into a memory, and for minimizing (4) a
training set D = {(a, s, r, s(cid:48))j}n
j=1 is drawn from the memory according to a distribution that prefers tuples that do not
ﬁt well to the current estimate of the action-value function.

For estimating the sampling distribution, initially, the difference

d(s, a, r, s(cid:48)) = | ˜Q(s, a; ˆθ) − ˜Q(s, a; θi)|
is computed for each tuple in memory and is updated after each iteration i. Then, the difference is converted to priority
p = (d − (cid:15))a,

(6)

(7)

with (cid:15) > 0 to ensure that no tuple has zero probability of being drawn, and 0 ≤ a < 1 (when a = 0 the uniform
distribution over tuples is used). Finally, the priorities are translated into probabilities. In particular, a tuple k has a
probability

Pk =

pk
(cid:80)N
j=1 pj

(8)

of being drawn during the experience replay. Variable N in (8) stands for the cardinality of memory.

4 Driving Policy

Having described the RL framework and the the prioritized experience replay algorithm, in this section, the RL-based
approach utilized in this work is presented, along with the state and action representation, and the design of the scalar
reward signal. Finally, the architecture of the employed neural network, and details about the implementation, as well
as, the mechanism for generating (s, a, r, s(cid:48)) tuples for training the neural network are described.

4.1 State Representation

Autonomous vehicles are equipped with multiple sensors that enable them to capture heterogeneous and multimodal
information about their surrounding environment. This allows for a wide variety of state representations. The selection,
however, of the representation signiﬁcantly affects the ability of an agent to learn. In this work, a state representation
that, on the one hand, can be constructed using current sensing technologies, and, on the other, it allows the agent to
efﬁciently learn is utilized.

Speciﬁcally, this work considers autonomous vehicles that move on a freeway with three lanes. It is assumed that the
vehicle can sense the surrounding environment that spans 60 meters behind it and 100 meters ahead of it, as well as, its

6

A PREPRINT

(a)

(b)

Figure 1: State representation. The autonomous vehicle is represented by the red rectangle, while the green rectangle
represents another vehicle present on the road. (a) The purple shaded area corresponds to the sensed surrounding
environment of the autonomous vehicle. (b) Discretization of sensed environment.

two adjacent lanes. This means that the autonomous vehicle can estimate the relative positions and velocities of other
vehicles that are present in the aforementioned area. Note that with current LiDAR and camera sensing technologies
such an assumption can be considered valid. A schematic representation of the sensed surrounding environment of a
vehicle is presented in Figure 1(a).

In order to translate the information that can be sensed by the autonomous vehicle into a state vector, the sensed area is
discretized into tiles of one meter length, see Fig. 1(b). In order for this discretization to be useful, the accuracy of
the vehicle sensors must be in the order of centimeters, something that is feasible with current sensing technologies
[51, 53, 54]. The value of the longitudinal velocity of the autonomous vehicle is assigned to the tiles beneath of it. To
tiles occupied by other vehicles the value of their longitudinal velocity is assigned. The velocity of the other vehicles is
estimated by using their positions in two subsequent time instances. The value of zero is given to all non occupied tiles
that belong to the road and, ﬁnally, the value minus one to tiles outside of the road (the autonomous vehicle can sense
an area outside of the road in case it occupies the left-most or the right-most lane).

Using the representation above, the sensed environment is transformed into a matrix with three rows and one hundred
sixty columns. Moreover, this matrix contains information about the absolute velocities of vehicles, as well as, relative
positions of other vehicles with respect to the autonomous vehicle. Finally, the vectorized version of this matrix, that is
a vector with 480 elements, is used to represent the state of the environment at each speciﬁc time step.

The proposed state representation can be easily obtained by sensors installed on an autonomous vehicle. Despite the
fact that this representation is relatively simple, as it can be seen in Section 6, it contains adequate information for
obtaining robust driving policies. More realistic and informative state representations can be constructed. For example
current pattern and object recognition methods can be utilized to classify vehicles and, thus, incorporate into the state
representation information regarding the type of surrounding vehicles and their size. In addition, if we assume that
vehicles are equipped with communication enabling technologies, then vehicle-to-vehicle communication can be used
to enhance the state representation with information regarding vehicles’ longitudinal and lateral accelerations, while
vehicle-to-infrastructure communication can provide information regarding the state of the network. Although, more
accurate state representations can be constructed, using a simple state representation, like the proposed one, permits
to gain insights with respect to the behavior of the derived policy. Moreover, we deliberately do not assume any
communication between the vehicles, to make the training of the RL policy much harder, and, at the same time, be
able to evaluate its behavior under minimal assumptions. Finally, this work is based on the argument that RL based
techniques can be proved very valuable towards the developments of driving policies, even in mixed driving scenarios,
and thus, it can be seen as a preliminary proof-of-concept.

4.2 Action Representation

Seven available actions are deﬁned; i) change lane to the left, ii) change lane to the right, iii) accelerate with a constant
acceleration of 1m/s2 or 2m/s2, iv) decelerate with a constant deceleration of −1m/s2 or −2m/s2, and v) move with
the current speed at the current lane, see Table 1. For the acceleration and deceleration actions feasible acceleration
and deceleration values are used to ensure that the autonomous vehicle will be able to implement them. Moreover, the

7

A PREPRINT

Table 1: The available actions of the autonomous vehicle.

Action #1:
Action #2:
Action #3:
Action #4:
Action #5:
Action #6:
Action #7:

Change lane to the left
Change lane to the right
Constant acceleration of 1m/s2
Constant acceleration of 2m/s2
Constant deceleration of 1m/s2
Constant deceleration of 2m/s2
Move on current lane with current speed

autonomous vehicle is making decisions by selecting one action every one second, which implies that the ﬁrst two
actions are also feasible, that is, a moving car is able to change lane in a time interval of one second.

Using the aforementioned action representation, each action can be seen as a goal or desire of the autonomous vehicle
that should be achieved during one second. Practically, the ﬁrst six actions represent goals that are associated with the
avoidance of obstacles. The third to sixth actions represent also goals that are related to the fact that the autonomous
vehicle should move forward with a desired speed. Finally, the seventh action implies that the vehicle is moving with
the desired speed and there are no obstacles to avoid.

Note that the goal of this work is to develop a driving policy by approximating through RL the action-values Q(s, a)
for every possible (s, a) ∈ S × A pair. Therefore, adopting an action space with small cardinality can signiﬁcantly
simplify the problem leading to faster training. Moreover, the authors of [45] argue that low-level control tasks can be
less effective and/or robust for high-level driving policies. For these reasons an action space like the one presented
above is used, instead of lower level commands such as longitudinal and lateral accelerations.

Finally, by using an action set of goals, the RL-based driving policy makes high-level decisions for leading the
autonomous vehicle to a desired state. The implementation of these goals can efﬁciently take place by exploiting a
separate non-learnable module, such as dynamic programming. This low-level module will produce state trajectories by
translating each speciﬁc desire to lower level commands, such as longitudinal and lateral accelerations. These state
trajectories may then be used as a reference by the vehicle throttle and brake controllers, which are designed on the basis
of vehicle dynamics, to produce the actual vehicle movement on the road. As mentioned in Section 2, the development
of such a module is beyond the scope of this work, and, thus, it is assumed that is given.

4.3 Reward Signal Design

The reward signal is a measure of the quality of a selected action at a speciﬁc state, and is the only mean through which
a policy can be evaluated. So, designing appropriate rewards signals is the most important tool for shaping the driving
behavior of an autonomous vehicle.

For driving scenarios, the autonomous vehicle should be able to avoid collisions, move with a speciﬁc desired speed
and avoid unnecessary lane changes and accelerations. Therefore, the reward signal should reﬂect all these objectives
by employing one penalty function for collision avoidance, one that penalizes deviations from the desired speed and
two penalty functions for unnecessary lane changes and accelerations/decelerations.

The penalty function for collision avoidance should feature high values at the gross obstacle space, so that the
autonomous vehicle is repulsed, and potentially unsafe decisions are suppressed; and low (or virtually vanishing) values
outside that space. To this end, the exponential penalty function
(cid:26)e−(δi−δ0)

(9)

f (δi) =

0

if le = li
otherwise

is adopted. In (9) δi is the longitudinal distance between the automated vehicle and the i-th obstacle (the i-th vehicle in
its surrounding environment), δ0 stands for the minimum safe distance, and, le and li denote the lanes occupied by the
autonomous vehicle and the i-th obstacle, respectively. Note that this function is activated only when the automated
vehicle and an obstacle are at the same lane. Finally, if the value of (9) becomes greater or equal to one, then the driving
situation is considered very dangerous and it is treated as a collision.

The vehicle mission is to advance with a longitudinal speed close to a desired one. Thus, the quadratic term

h(v) = (v − vd)2
(10)
that penalizes the deviation between the vehicle speed and its desired speed, is incorporated in the reward. In (10) the
variable v stands for the longitudinal speed of the autonomous vehicle, while the constant vd represents its desired
longitudinal speed.

8

A PREPRINT

Two terms are also introduced; one for penalizing accelerations/decelerations, and one for penalizing unnecessary lane
changes. For penalizing accelerations the term

a(vt, vt−1) = (vt − vt−1)2

(11)

is used, while for penalizing lane changes the term

g(lt, lt−1) = I(lt (cid:54)= lt−1).
(12)
is used. Variables vt and lt correspond to the speed and lane of the autonomous vehicle at time step t, while I(·) is the
indicator function.

The total reward at time step t is the negative weighted sum of the aforementioned penalty terms, that is

rt = −w1

Ot(cid:88)

i=1

ft(δi) − w2ht(vt) − w3

Ot(cid:88)

i=1

I(ft(δi) ≥ 1 − w4a(vt, vt−1) − w5g(lt, lt−1)

(13)

In (13) the third term penalizes collisions and variable Ot corresponds to the total number of obstacles that can be sensed
by the autonomous vehicle at time step t. The selection of weights deﬁnes the importance of each penalty function to
the overall reward. In this work the weights were set, using a trial and error procedure, as follows: w1 = 1, w2 = 0.5,
w3 = 20, w4 = 0.01, w5 = 0.01. The largest weighting factors are associated with the terms that penalize collisions
and model obstacle avoidance, since the derived policy should generate collision free trajectories. The weighting term
associated with the desired speed of the vehicle deﬁnes how aggressive and/or how conservative will be the derived
driving policy. Using a small value for this weight will result to a conservative policy that will advance the vehicle with
very low speed or, even worse, keep the vehicle immobilized by setting its speed equal to zero. Finally, the values of the
weighting factors associated with lane changes accelerations/decelerations are small in order to enable the vehicle to
make maneuvers, such as overtaking other vehicles.

4.4 Neural Network Architecture

As mentioned before, the goal of this work is to develop a driving policy by approximating through RL the action-values
Q(s, a) for every possible (s, a) ∈ S × A pair. Towards this direction a fully connected feed forward neural network is
utilized, due to its universal function approximation property [52].

Speciﬁcally, the action-values Q(s, a) for each pair (s, a) ∈ S × A are approximated by using a neural network that
maps a speciﬁc state s ∈ S to the action-values Q(s, as,i), where {as,i}i is a non empty set that contains all actions
that can be selected by the policy when the agent is at state s. In this work, the DDQN approach is followed which
utilizes two identical neural networks with two hidden layers, consist of 256 and 128 neurons respectively. The ﬁrst
neural network is responsible for setting the targets, while the second one is responsible for following them. The
synchronization between the two neural networks is realized every 1000 epochs. For more information regarding the
DDQN model please refer to [48].

4.5 Training Set Generation and Policy Training

For generating (st, at, rt, st+1) tuples that will be used for training the DDQN, two different microscopic trafﬁc ﬂow
simulators are used. The ﬁrst one is a custom made simulator that moves the manual driving vehicles with constant speed
using the kinematics equations. The second simulator is the established SUMO1 microscopic trafﬁc ﬂow simulator. By
exploiting trafﬁc ﬂow simulators driving scenarios can be simulated. For each one of the simulation steps during a
simulated scenario, following the approach described in Section 4, one (st, at, rt, st+1) tuple can be generated using
information coming directly from the simulator.

After the collection of a set of (st, at, rt, st+1) tuples the training of the RL policy is starting following the procedures
described in Section 3. It should be mentioned that during policy training (and testing) we implemented a rule-based
action masking [45] for changing lanes. Our choice is justiﬁed by the fact that in some driving situations, undesirable
lane changes can be straightforward identiﬁed, e.g. lane changes that result to immediate collisions. In such cases
undesirable lane changes are ﬁltered out, instead of letting the agent to learn to avoid that actions. The beneﬁts from
action masking is twofold. First, it restricts the actions space, and, thus, it speeds up the learning process. Second,
selection of inferior actions caused by the variance in observation will be avoided resulting to a policy that is less prone
to false positives and easier to debug. Besides the aforementioned action masking, during training, no other safety
mechanisms are applied on the behavior of the autonomous vehicle. On the contrary, regarding the manual driving cars,

1www.sumo.dlr.de/

9

A PREPRINT

all safety mechanisms are enabled. Therefore, in case of a collision we are sure that the vehicle that caused the collision
is the autonomous one.

These are the general rules applied during the driving scenarios generation (for training and testing the RL-based driving
policy) using both of the aforementioned microscopic trafﬁc-ﬂow simulators. Depending on the speciﬁc characteristics
of each experiment extra rules may be applied. These are described in the corresponding subsections of Section 6.

4.6

Implementation details

For training the network we set the discount factor γ = 0.995 [see relation (1)], we used a memory of 2000 samples
capacity, a mini-batch of 64 samples and the ADAM optimizer with learning rate 0.003, β1 = 0.9 and β2 = 0.999. The
exploration factor (cid:15)t at each step is annealed by

(cid:15)k = 0.01 + 0.99e−λk,
(14)
where k stands for the index of the latest training step and λ was set equal to 7.5 · 10−6. Finally, the training process
started with (cid:15)1 = 1.0 and terminated when (cid:15)k = 0.01.

5 Safety Rules

As mentioned before, no learning based driving policy can guarantee a collision free trajectory. There will always
be corner cases (very rare events) that the learning algorithm will not encounter during its training phase. Therefore,
it cannot be assured that the decisions corresponding to such event will be correct [for a formal proof of this result
see [16] Lemma 2]. Moreover, a vehicle might be involved in an accident without being responsible for it. For these
reasons the authors of [16] derive ad-hoc rules to guarantee responsibility-sensitive safety, that is, to guarantee that an
autonomous vehicle will never cause an accident, even if it will be involved in one.

The derivation of safety rules in this work is motivated by the responsibility-sensitive framework. There is, however, a
main difference between the setting in [16] and our setting. The authors in [16] assume that the road is occupied only by
autonomous vehicles whose behaviour can be programmed. In our case, there is no such assumption. On the contrary,
mixed driving scenarios are considered, where the road is occupied both by autonomous and manual driving vehicles.
This implies that the behaviour of manual driving vehicles cannot be affected neither programmed. By restricting
attention on vehicles that move on a highway the aforementioned assumption can be removed. This allows to assume
that extreme events, such as vehicles that stop suddenly, will not occur.

Restricting attention on highways, permits also the simpliﬁcation of the responsibility-safety framework by considering
two types of collisions. An autonomous vehicle can cause an accident, ﬁrstly, if it moves faster than its leader and
violates a minimum time gap, and, secondly, during lane changes. In the following we derive rules for avoiding these
two types of collisions. Please note, that the information that can be used to derive such rules is only the information
available to the autonomous vehicle, that is the positions and the velocities of the vehicles surrounding it.

In order to avoid the ﬁrst type of collisions, the minimum safety time gap ρs that must be maintained between the
autonomous vehicles and its leader needs to be estimated. Obviously, the minimum safety time gap makes sense only
when the autonomous vehicle is moving faster that its leader. Let us denote as ve,t and as vl,t the longitudinal speeds
of the autonomous vehicle and its leader vehicle, respectively. Also, let us denote as dmax the maximum feasible
deceleration of the autonomous vehicle. In order to avoid the ﬁrst type of collisions after a time interval ρ, the following
inequality should hold:

vl,tρ − ve,tρ +

dmaxρ2 > 0.

1
2

Solving for ρ the minimum safety time gap ρs can be obtained by

(cid:40)

ρs = inf

ρ : ρ >

2(ve,t − vl,t)
dmax

(cid:41)
.

(15)

(16)

Based on relation (16), the autonomous vehicle before performing an action, different than lane change actions, evaluates
the minimum safety gap with respect to its leader. If the minimum time gap is violated, the autonomous vehicles
decelerates with dmax until its speed becomes equal to the speed of its leader. Otherwise, it performs the RL selected
action.

Regarding the second type of collisions that can be caused by lane changes, two different cases should be considered.
The autonomous vehicle should avoid collisions with its leader vehicle and with its follower vehicle in the newly
selected lane. In the ﬁrst case, estimates the minimum safety time gap ρs with respect to its leader in the newly selected

10

A PREPRINT

lane. If the minimum time gap is not violated the RL lane change action is performed. Otherwise, the autonomous
vehicle selects the last action of the action set A [see Section 4.2], that is to retain current lane and move with current
speed, and checks for the ﬁrst type of collisions. In order to avoid the collisions with its follower vehicle in the newly
selected lane, the autonomous vehicle is not permitted to change lane if the follower vehicle moves faster. In this case
again, the autonomous vehicle selects the last action of the action set A, and checks for the ﬁrst type of collisions. The
rule for avoiding collisions between the autonomous vehicle and its follower is very conservative. However, since the
RL-based driving policy cannot affect the behavior of the follower, and at the same time has no access to its maximum
feasible deceleration (in order to relax this rule by estimating a safety time gap), such a rule is the only way to guarantee
no collisions of the second type.

Although, the derived safety rules lead to a more conservative driving policy, as it can be seen in the experimental
validation of the proposed approach, they permit the autonomous vehicle to advance with its desired speed and at the
same time avoid collisions.

6 Experiments

In this work three different sets of experiments were conducted. In the ﬁrst set of experiments a simpliﬁed microscopic
trafﬁc ﬂow simulator is utilized in order to compare the behavior of the RL-based driving policy against an optimal
policy derived via Dynamic Programming. In the second set of experiments the established microscopic trafﬁc simulator
SUMO is used. Three different types of experiments are conducted. First, the behavior of the autonomous vehicle
is evaluated when it is controlled by the derived RL-based policy and when it is controlled by SUMO. Second, the
robustness of the derived policy with respect to measurement errors is evaluated. Finally, in the third set of experiments,
the effect of vehicles that move following the RL-based policy on trafﬁc ﬂow is investigated. In the following the details
of the experimental setup and the obtained results are presented.

6.1 RL-based driving policy and Dynamic Programming

Dynamic Programming techniques can produce optimal policies assuming that no disturbances occur in the system.
Due to this fact, for this set of experiments, a simpliﬁed custom made microscopic trafﬁc simulator was developed and
utilized. This simulator moves the manual driving vehicles with constant longitudinal velocity using the kinematics
equations. Moreover, the manual driving vehicles are not allowed to change lanes. Despite its simplifying setting,
this set of experiments allow the comparison of the RL driving policy against an optimal policy derived via Dynamic
Programming. At this point it should be mentioned that for this set of experiments the ad-hoc safety rules derived in
Section 5 are disabled, in order to gain insights regarding the safety aspects of the RL-based driving policy.

For training the DDQN, driving scenarios of 60 seconds length were generated. In these scenarios one vehicle enters
the road every two seconds, while the tenth vehicle that enters the road is the autonomous one. All vehicles enter the
road at a random lane, and their initial longitudinal velocity is randomly selected from a uniform distribution ranging
from 12m/s to 17m/s. Finally, the desired speed of the autonomous vehicle is set equal to 21m/s.

The RL driving policy is compared against an optimal policy derived via Dynamic Programming under four different
road density values. For each one of the different densities 100 scenarios of 60 seconds length were simulated. In these
scenarios, the simulator moves the manual driving vehicles, while the autonomous vehicle moves by following the RL
policy and by solving a Dynamic Programming problem with 60 seconds horizon (which utilizes the same objective
functions and actions as the RL algorithm). Finally, statistics regarding the number of collisions and lane changes,
and the percentage of time that the autonomous vehicle moves with its desired speed for both the RL and Dynamic
Programming policies are extracted. At this point it has to be mentioned that Dynamic Programming is not able to
produce the solution in real time, and it is just used for benchmarking and comparison purposes. On the contrary the RL
policy, at a given state can select an action very fast, since this selection corresponds to one evaluation of the neural
network function at the corresponding state.

Table 2 summarizes the results of this comparison. The four different densities are determined by the rate at which
the vehicles enter the road, that is, 1 vehicle enters the road every 8, 4, 2, and 1 seconds. The RL policy is able to
generate collision free trajectories, when the density is less than or equal to the density used to train the network. For
larger densities, however, the RL policy produced 2 collisions every 100 scenarios. In terms of efﬁciency, the optimal
Dynamic Programming policy is able to perform more lane changes and advance the vehicle faster.

11

Table 2: Driving behavior evaluation of the RL and DP driving policies, in terms of total number of collision and lane
changes for 100 scenarios and percentage of time that the vehicle moves with its desired speed.

A PREPRINT

1 veh./8 sec.
DP policy
RL policy
1 veh./4 sec.
DP policy
RL policy
1 veh./2 sec.
DP policy
RL policy
1 veh./1 sec.
DP policy
RL policy

Collisions Lane changes Desired speed (%)

0
0

0
0

0
0

0
2

84
81

127
115

120
108

70
62

85
73

83
64

87
62

72
56

6.2 RL-based driving policy and SUMO policy

In this set of experiments the behavior of the autonomous vehicle when it follows the RL policy and when it is controlled
by SUMO is evaluated. The training of the RL policy took place using scenarios generated by the SUMO simulator.
During the generation of scenarios, all SUMO safety mechanisms are enabled for the manual driving vehicles and
disabled for the autonomous vehicle. Furthermore, the manual driving cars is not permitted to implement cooperative
and strategic lane changes. Such a conﬁguration for the lane changing behavior, impels the autonomous vehicle to
implement maneuvers in order to achieve its objectives. Moreover, in order to simulate realistic scenarios two different
types of manual driving vehicles are used; vehicles that want to advance faster than the autonomous vehicle and vehicles
that want to advance slower. Finally, the density was equal to 600 veh/lane/hour. For the evaluation of the trained RL
policy, different driving scenarios, described in the following subsections, were simulated.

6.2.1 Evaluation of the derived RL driving policy and safety rules

In this set of experiments different driving scenarios were simulated; i) 100 driving scenarios during which the
autonomous vehicle follows the RL driving policy without the ad-hoc safety rules derived in Section 5, ii) 100 driving
scenarios during which the autonomous vehicle follows the RL driving policy with the ad-hoc safety rules, iii) 100
driving scenarios during which the default conﬁguration of SUMO was used to move forward the autonomous vehicle
(cooperative and strategic lane changes are enabled for the autonomous vehicle), and iv) 100 scenarios during which
the behavior of the autonomous vehicle is the same as the manual driving vehicles, i.e. it does not perform strategic
and cooperative lane changes. The duration of all simulated scenarios was 60 seconds. The aforementioned scenarios’
generation framework was applied for two different driving conditions. In the ﬁrst one the desired speed for the slow
manual driving vehicles was set to 18m/s, while in the second one to 16m/s. For both driving conditions the desired
speed for the fast manual driving vehicles was set to 25m/s. Furthermore, in order to investigate how the presence
of uncertainties affects the behavior of the autonomous vehicle, simulated scenarios where drivers’ imperfection was
introduced by appropriately setting the σ parameter in SUMO (0 ≤ σ ≤ 1 with σ = 0 to imply a perfect driver) were
also used. Finally, the behavior of the autonomous vehicles was evaluated in terms of i) collision rate, and ii) average
speed per scenario.

Table 3 summarizes the results of this comparison when the ad-hoc safety rules are disabled. In this way the safety
levels of the RL-based driving policy can be experimentally quantiﬁed. In Table 3, SUMO default corresponds to the
default SUMO conﬁguration for moving forward the autonomous vehicle, while SUMO manual to the case where
the behavior of the autonomous vehicle is the same as the manual driving vehicles. Irrespective of whether a perfect
(σ = 0) or an imperfect (σ = 0.5) driver is considered for the manual driving vehicles, the RL policy is able to move
forward the autonomous vehicle faster than the SUMO simulator, especially when slow vehicles are much slower than
the autonomous one. However, it results to a collision rate of 2%-4%, which is its main drawback. No guarantees for
collision-free trajectory is the price paid for deriving a learning based approach capable of generalizing to unknown
driving situations and inferring driving actions with minimal computational cost.

However, when the ad-hoc safety rules are enabled the derived RL driving policy achieves to provide collision free
trajectories. The average speed of the autonomous vehicle slightly decreases after the application of ad-hoc rules, but
again the derived policy advances the autonomous vehicle faster than the SUMO policies. Speciﬁcally, when the speed

12

Table 3: Driving behavior evaluation. SUMO default corresponds to the default SUMO conﬁguration, while SUMO
manual to the case where the behavior of the autonomous vehicle is the same as the manual driving vehicles.

A PREPRINT

Desired speed for slow vehicles 18m/s

RL policy with rules (σ = 0.0)
RL policy w/o rules (σ = 0.0)
SUMO default (σ = 0.0)
SUMO manual (σ = 0.0)
RL policy with rules (σ = 0.5)
RL policy w/o rules (σ = 0.5)
SUMO default (σ = 0.5)
SUMO manual (σ = 0.5)

Collisions
0%
2%
0%
0%
0%
3%
0%
0%

Desired speed for slow vehicles 16m/s

RL policy with rules (σ = 0.0)
RL policy w/o rules (σ = 0.0)
SUMO default (σ = 0.0)
SUMO manual (σ = 0.0)
RL policy with rules (σ = 0.5)
RL policy w/o rules (σ = 0.5)
SUMO default (σ = 0.5)
SUMO manual (σ = 0.5)

Collisions
0%
2%
0%
0%
0%
4%
0%
0%

Avg speed
20.62
20.71
20.22
19.48
20.08
20.09
19.57
19.05

Avg speed
19.87
20.04
18.41
17.47
19.81
19.87
17.67
17.26

of the slow vehicles is 18m/s the RL-based policy with the ad-hoc safety rules advances the autonomous vehicle 2%
and 2.6% faster than the SUMO default policy for σ = 0.0 and σ = 0.5 respectively. For the case where the speed of
the slow vehicles is 16m/s, the improvement, in terms of speed, of the RL-based policy over the SUMO default policy
is more signiﬁcant. In particular, the RL-based policy advances the autonomous vehicle 8% and 12% faster than the
SUMO default policy for σ = 0.0 and σ = 0.5 respectively.

The aforementioned results suggest that the RL-based driving policy is not signiﬁcantly more efﬁcient than the SUMO
default policy when the average speed of the manual driving vehicles is close to the desired speed of the autonomous
vehicle. However, when the deviation between the desired speed of the autonomous vehicle and the average speed of the
manual driving vehicles increases, the RL-based driving policy is able to advance the autonomous vehicle much faster.

6.2.2 Evaluation of the derived RL driving policy under measurement errors

In this set of experiments the robustness of the RL-based driving policy, with the application of ad-hoc safety rules,
is evaluated with respect to measurement errors regarding the position of the manual driving vehicles. At each time
step, measurement errors proportional to the distance between the autonomous vehicle and the manual driving vehicles
are introduced. Two different error magnitudes were used; ±5% and ±10%. The RL policy was evaluated in terms of
collisions and average speed in 100 driving scenarios of 60 seconds length for each error magnitude. In these scenarios
the desired speed of the slow vehicles is 16m/s. Finally, for these experiments, perfect and imperfect drivers were also
considered.

The results of this evaluation are presented in Table 4. Despite the introduction of noise, the RL-based driving policy is
able to produce collision-free trajectories, and at the same time, retain a high speed for the autonomous vehicle. In
particular, the introduction of 5% noise does not seem to affect the average speed of the vehicle. By increasing the noise
to 10% the average speed of the vehicle slightly decreases compared to the case with noiseless measurements. Fig. 2
presents the speed trajectories of the autonomous vehicle when different drivers’ imperfection and different magnitude
of noises are introduced. The solid green line represents the mean speed of the vehicle over all 100 scenarios, while
the shaded area represents 1 standard deviation of the speeds below and above their mean value. Irrespective of the
introduced uncertainties, during the ﬁrst steps of the simulation the autonomous vehicle increases its speed to reach a
speed close to its desired one, and then, it retains this speed. Moreover, by increasing the noise, the deviation of the
speeds over the 100 scenarios increases. This, however, is a rational behavior, since increasing the uncertainty, in terms
of noisy measurements, will increase the variance during the decision making process.

13

A PREPRINT

Figure 2: Speed trajectories for different measurements errors and driver imperfection. The solid green line represents
the mean speed of the vehicle over all 100 scenarios, while the shaded area represents 1 standard deviation of the speeds
below and above their mean value.

Table 4: Driving behavior evaluation with ad-hoc safety rules when different magnitudes of measurements errors are
introduced.

5% Noise

RL policy with rules (σ = 0.0)
RL policy with rules (σ = 0.5)

Collisions
0%
0%

10% Noise

RL policy with rules (σ = 0.0)
RL policy with rules (σ = 0.5)

Collisions
0%
0%

Avg speed
19.88
19.84

Avg speed
19.65
19.59

6.2.3 Evaluation of the derived RL driving policy with unknown vehicle types

In this set of experiments the robustness of the derived RL-based driving policy is evaluated when the road is occupied
by types of vehicles that were not present during the training phase. The RL-based driving policy was trained using
driving scenarios where the road was occupied by passenger manually driving vehicles that were moving faster and
slower than the autonomous vehicle. In this set of scenarios the road is occupied by the previously mentioned passenger
vehicles, but also by truck, buses and motorcycles. The percentage of these types of vehicles as well as their desired
speed are presented in Table 5. Under this experimental setting the robustness of the derived driving policy can be
evaluated when vehicles of different sizes and different desired speeds occupy the road. Towards this direction 100
driving scenarios considering perfect drivers and 100 scenarios considering drivers’ imperfections were simulated. All
driving scenarios were 60 seconds long. Finally, the RL-based driving policy was evaluated in terms of collisions and
average speed with which the autonomous vehicle moves forward.

Table 6 presents the RL driving policy evaluation results for the aforementioned set of experiments. By comparing these
results with the results in Table 3, it can be seen that the average speed of the autonomous vehicle is slightly decreases
by 0.13 m/s and 0.14 m/s, for σ = 0.0 and σ = 0.5 respectively, when types of vehicles not seen during the training
phase are present in the road. This decrease is mainly due to the randomness during driving scenarios generation and
not due to the presence of trucks, buses and motorcycles on the road. More importantly, the RL driving policy is able
to produce collision free trajectories despite the fact that the road is occupied by types of vehicles not seen during
the training phase. This is justiﬁed by two facts. First, the proposed state representation utilizes encodes about the

14

0102030405060Time (seconds)0510152025Speed (m/s)=0.0, noise=0%0102030405060Time (seconds)0510152025Speed (m/s)=0.0, noise=5%0102030405060Time (seconds)0510152025Speed (m/s)=0.0, noise=10%0102030405060Time (seconds)0510152025Speed (m/s)=0.5, noise=0%0102030405060Time (seconds)0510152025Speed (m/s)=0.5, noise=5%0102030405060Time (seconds)0510152025Speed (m/s)=0.5, noise=10%A PREPRINT

Table 5: Vehicle types present on the road.

Slow passenger vehicles
Fast passenger vehicles
Trucks
Buses
Motorcycles

Percentage
40%
40%
5%
5%
10%

Maximun Speed
16 m/s
25 m/s
14 m/s
16 m/s
21 m/s

Table 6: Driving behavior evaluation with ad-hoc safety rules when different types of vehicles occupy the road.

RL policy with rules (σ = 0.0)
RL policy with rules (σ = 0.5)

Collisions
0%
0%

Avg speed
19.74
19.67

position and the velocity of manual driving vehicles present on the road. This kind of information can be obtained and
encoded for any vehicle irrespective of it type. Second, the development and application of the proposed safety rules
compensates for the presence of manually driving vehicles of different sizes. It should be mentioned, however, that
more realistic and accurate state representations (see Section ??) can also be utilised to explicitly encode vehicles size
information in state representation.

6.2.4 Evaluation of the derived RL driving policy under rainy weather conditions

In this set of experiments the RL driving policy is evaluated under rainy weather driving conditions. Rainy weather
shifts the fundamental diagram to the left, which implies, on the one hand, that the vehicles move slower, and on
the other, that their maximum acceleration/deceleration becomes lower. In order to simulate rainy weather driving
scenarios, the desired speed of all vehicles, except the autonomous one, was decreased by 10%, and their maximum
acceleration/deceleration by 30%. Moreover, drivers’ imperfections were also included by setting σ = 0.5. Finally,
different types of vehicles, that is slow and fast passenger vehicles, truck, buses and motorcycles, were also present on
the road. 100 driving scenarios of 60 seconds long were simulated, and the derived driving policy was evaluated in terms
of number of collisions and average speed with which the autonomous vehicle moves forward. Table 7 presents the
result of this evaluation. The RL-based driving policy is able to produce collision free trajectories and, at the same time,
move forward the autonomous vehicle with a speed larger than 19 m/s. The longitudinal velocity of the autonomous
vehicle is slightly lower (0.19 m/s) than the previous experiments. This, however, is mainly caused by the decrease in
overall trafﬁc ﬂow due to weather conditions.

6.3 The effect of the RL-based driving policy on trafﬁc ﬂow

In this set of experiments preliminary results on the effect of autonomous vehicles on the overall trafﬁc ﬂow are
presented. Four different experiments are conducted by varying the percentage of autonomous vehicles that occupy
the road. In the ﬁrst experiment all vehicles are manual driving, that is the percentage of autonomous vehicles is zero.
For the rest three experiments percentages of 5%, 10% and 15%, respectively, are used. For each experiment 100
scenarios of 120 seconds length were simulated and for each scenario the average speed of all vehicles on the road is
computed, which is an indicator of the ﬂow; the higher the average speed the higher is the ﬂow. For all experiments
and all scenarios the desired speed of manual driving vehicles is 16m/s and the option for cooperative and strategic
maneuvers is disabled, while the desired speed for all the autonomous vehicles is 21m/s. In this way the behavior of
the manual driving vehicles can be seen as a moving bottleneck.

The results of these experiments are presented in Table 8. For each one of the experiments the average speed is
reported. As a baseline the case where the percentage of autonomous vehicles is zero is considered, and the relative
improvement of the rest of the cases (5%, 10%, and 15% autonomous vehicles) against this one is also reported. The
average speed for the baseline is 15.32m/s, a little bit lower than the desired speed of the manual driving vehicles.
This happens because the manual driving vehicles should satisfy the safety constraints imposed by SUMO. When the
percentage of autonomous vehicles increases to 5% the average speed of the vehicles is 15.41m/s resulting in a very
small improvement of 0.6% over the baseline. In this case the autonomous vehicles move faster than the manual driving
ones. Their percentage, however, is very small, and, thus, they only slightly improve the average speed compared to the
baseline. Increasing more the percentage of autonomous vehicles to 10% results to an average speed of 16.11m/s and
5.1% improvement compared to the baseline. Increasing, however, more the percentage of autonomous vehicles to 20%

15

A PREPRINT

Table 7: Driving behavior evaluation with ad-hoc safety rules rainy weather conditions.

RL policy with rules (σ = 0.5)

Collisions
0%

Avg speed
19.48

Table 8: Effect of autonomous vehicles on the overall trafﬁc ﬂow.

Autonomous vehicles 0 %
Autonomous vehicles 5 %
Autonomous vehicles 10%
Autonomous vehicles 20%

Avg speed
15.32 m/s
15.41 m/s
16.11 m/s
15.91 m/s

Improvement over 0%
0.0%
0.6%
5.1%
1.3%

results to an improvement of 1.3% over the baseline, which is smaller than the improvement of the previous case. This
mainly happens due to the selﬁsh behavior of the autonomous vehicles.

Autonomous vehicles want to move faster than the manual driving cars, and in order to achieve that they have to perform
maneuvers. Keeping the density of autonomous vehicles low permits the performance of maneuvers, and thus, the faster
advancement of the autonomous vehicles. Increasing, however, the density of autonomous vehicle above a threshold,
makes the performance of maneuvers more difﬁcult, since each one autonomous vehicle, present in a limited space,
wants to perform its own maneuvers in a selﬁsh way. This results in competitive behaviors among the autonomous
vehicles, which has a negative effect on the overall trafﬁc ﬂow.

These preliminary results show that selﬁsh and competitive behaviors deteriorate the overall trafﬁc ﬂow. Deriving an
RL-based driving policy trained on scenarios where the manually driven vehicles occupy a selﬁsh behavior will not
improve the overall trafﬁc ﬂow. Due to limited space and the large number of maneuvers performed by the manually
driven vehicles the RL training algorithm will result to a very conservative policy. In our view, the only way to
improve the overall trafﬁc ﬂow, under mixed driving scenarios, is to derive cooperative driving policies for clusters
of autonomous vehicles, in order to achieve not vehicle-centric, but overall trafﬁc ﬂow goals. This could be done by
introducing appropriate penalty terms regarding the overall trafﬁc ﬂow, such as minimum travel time or average trafﬁc
ﬂow, in the reward function. Deriving, however, cooperative RL-based driving policies for clusters of autonomous
vehicles is outside the scope of this work.

7 Discussion

The simulation results presented in Section 6 indicate that the derived RL-based driving policy is more efﬁcient, for
moving forward the autonomous vehicle, than the car following model used by SUMO simulator. At the same time, the
derived policy can produce collision free trajectories, and it seems to be robust under measurement errors, different
types of vehicles and weather conditions. Although, the current work makes the ﬁrst steps towards the exploitation of
deep RL techniques for autonomous vehicles’ path planning, the proposed methodology is not yet ready for real-world
adoption. More complicated scenarios should be generated and utilised during the training and testing phases, such as
scenarios where the autonomous vehicle is approaching a crash site ahead, heavy trafﬁcs, highway merging, emergency
lane switching and night driving.

Being able to identify the limitations of the current work motivates our ongoing and future work, which comprises i)
training and testing an RL-based driving policy under more complicated and realistic scenarios, ii) derive more accurate
state representations by exploiting vehicle-to-vehicle and vehicle-to-infrastructure communication technologies, and iii)
move from a selﬁsh driving policy to the derivation of a cooperative driving policy in order to achieve not vehicle-centric,
but overall trafﬁc ﬂow goals.

8 Conclusions

In this work, the problem of path planning for an autonomous vehicle that moves on a freeway is considered. For
addressing this problem RL techniques are employed to derive a driving policy. The driving policy is implemented
using a DDQN. Two different simulators to train and validate the derived driving policy are used; a custom made
microscopic trafﬁc ﬂow simulator and the established SUMO microscopic trafﬁc ﬂow simulator.

The custom made microscopic trafﬁc ﬂow simulator is utilized for comparing the RL-based driving policy against an
optimal policy derived via Dynamic Programming. The results of this comparison indicated that, although, Dynamic

16

A PREPRINT

Programming can advance the autonomous vehicle faster than the RL-based driving policy, it cannot produce the
trajectory in real time. Moreover, Dynamic Programming requires a priori and exact knowledge of the system dynamics
in a disturbance free environment to produce an optimal solution. Due to these facts, an RL-based driving policy that
incorporates the ad-hoc safety rules [see Section 5] can be proved a valuable approach for emerging driving behaviors
with very low computational cost, minimal or no assumptions about the environment, and the capability to generalize to
driving situations that are not known a priori.

The SUMO simulator is utilized in order to train and validate the RL-based driving policy under customary and realistic
trafﬁc scenarios. Since, no learning based approach can guarantee collision-free trajectories, ad-hoc safety rules are
derived motivated by the responsibility-safety framework presented in [16]. The derived RL-based driving policy is
compared against SUMO policies with and without the introduction of uncertainties. The results of this comparison
indicated that the autonomous vehicle following the RL-based policy is able to achieve higher scores.

Finally, preliminary results regarding the effect of selﬁsh autonomous vehicles behavior on the overall trafﬁc ﬂow are
presented. These results suggest that, although, an individual autonomous vehicle that follows a selﬁsh policy can
achieve its goals, when multiple autonomous vehicles follow a selﬁsh policy their impact on the overall trafﬁc ﬂow is
negative. Selﬁsh policies lead to competitive behaviors that deteriorate the overall trafﬁc ﬂow. This effect is known as
the user optimum versus system optimum trade-off.

9 Acknowledgement

This research is implemented through and has been ﬁnanced by the Operational Program ”Human Resources Develop-
ment, Education and Lifelong Learning” and is co-ﬁnanced by the European Union (European Social Fund) and Greek
national funds.

References

[1] Julius Ziegler, Philipp Bender, Thao Dang, and Christoph Stiller. Trajectory planning for bertha—a local, continuous

method. In Intelligent Vehicles Symposium (IV), pages 450–457. IEEE, 2014.

[2] Akansel Cosgun, Lichao Ma, Jimmy Chiu, Jiawei Huang, Mahmut Demir, Alexandre Miranda Anon, Thang Lian,
Hasan Taﬁsh, and Samir Al-Stouhi. Towards full automated drive in urban environments: A demonstration in
gomentum station, california. In Intelligent Vehicles Symposium (IV), pages 1811–1818. IEEE, 2017.

[3] Bryan Reimer, Bruce Mehler, and Joseph F Coughlin. An evaluation of driver reactions to new vehicle parking
assist technologies developed to reduce driver stress. Cambridge: New England University Transportation Center,
Massachusetts Institute of Technology, 2010.

[4] Edmund Donges. A conceptual framework for active safety in road trafﬁc. Vehicle System Dynamics, 32(2-3):113–

128, 1999.

[5] Charalambos Menelaou, Stelios Timotheou, Panayiotis Kolios, and Christos G Panayiotou. Improved road usage
through congestion-free route reservations. Transportation Research Record: Journal of the Transportation
Research Board, (2621):71–80, 2017.

[6] C Menelaou, P Kolios, S Timotheou, CG Panayiotou, and MP Polycarpou. Controlling road congestion via a
low-complexity route reservation approach. Transportation research part C: emerging technologies, 81:118–136,
2017.

[7] Hannah Bast, Erik Carlsson, Arno Eigenwillig, Robert Geisberger, Chris Harrelson, Veselin Raychev, and Fabien
Viger. Fast routing in very large public transportation networks using transfer patterns. In European Symposium on
Algorithms, pages 290–301. Springer, 2010.

[8] Miguel Figliozzi. Vehicle routing problem for emissions minimization. Transportation Research Record: Journal

of the Transportation Research Board, (2197):1–7, 2010.

[9] Roberto Baldacci, Aristide Mingozzi, and Roberto Roberti. Recent exact algorithms for solving the vehicle routing
problem under capacity and time window constraints. European Journal of Operational Research, 218(1):1–6,
2012.

[10] Hannah Bast, Daniel Delling, Andrew Goldberg, Matthias Müller-Hannemann, Thomas Pajor, Peter Sanders,
Dorothea Wagner, and Renato F Werneck. Route planning in transportation networks. In Algorithm engineering,
pages 19–80. Springer, 2016.

[11] Thomas D Gillespie. Vehicle dynamics. Warren dale, 1997.

17

A PREPRINT

[12] Rajesh Rajamani. Vehicle dynamics and control. Springer Science & Business Media, 2011.

[13] Sumin Zhang, Weiwen Deng, Qingrong Zhao, Hao Sun, and Bakhtiar Litkouhi. Dynamic trajectory planning for
vehicle autonomous driving. In Systems, Man, and Cybernetics (SMC), 2013 IEEE International Conference on,
pages 4161–4166. IEEE, 2013.

[14] Werner Brilon, Justin Geistefeldt, and Matthias Regler. Reliability of freeway trafﬁc ﬂow: a stochastic concept of
capacity. In Proceedings of the 16th International symposium on transportation and trafﬁc theory, volume 125143.
College Park Maryland, 2005.

[15] M Yazici, Camille Kamga, and Kaan Ozbay. Highway versus urban roads: Analysis of travel time and variability
patterns based on facility type. Transportation Research Record: Journal of the Transportation Research Board,
(2442):53–61, 2014.

[16] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. On a formal model of safe and scalable self-driving

cars. arXiv preprint arXiv:1708.06374, 2017.

[17] Ioannis A Ntousakis, Ioannis K Nikolos, and Markos Papageorgiou. Optimal vehicle trajectory planning in the
context of cooperative merging on highways. Transportation research part C: emerging technologies, 71:464–488,
2016.

[18] Chad Goerzen, Zhaodan Kong, and Bernard Mettler. A survey of motion planning algorithms from the perspective

of autonomous uav guidance. Journal of Intelligent and Robotic Systems, 57(1-4):65, 2010.

[19] Moritz Werling, Tobias Gindele, Daniel Jagszent, and Lutz Groll. A robust algorithm for handling moving trafﬁc

in urban scenarios. In Intelligent Vehicles Symposium (IV), pages 1108–1112. IEEE, 2008.

[20] Luke Fletcher, Seth Teller, Edwin Olson, David Moore, Yoshiaki Kuwata, Jonathan How, John Leonard, Isaac
Miller, Mark Campbell, Dan Huttenlocher, et al. The mit–cornell collision and why it happened. Journal of Field
Robotics, 25(10):775–807, 2008.

[21] Michael T Wolf and Joel W Burdick. Artiﬁcial potential functions for highway driving with collision avoidance.

In International Conference on Robotics and Automation (ICRA), pages 3731–3736. IEEE, 2008.

[22] Jianqiang Wang, Jian Wu, and Yang Li. The driving safety ﬁeld based on driver–vehicle–road interactions. IEEE

Transactions on Intelligent Transportation Systems, 16(4):2203–2214, 2015.

[23] Georg Schildbach and Francesco Borrelli. Scenario model predictive control for lane change assistance on

highways. In Intelligent Vehicles Symposium (IV), pages 611–616. IEEE, 2015.

[24] Stephen M Erlien. Shared vehicle control using safe driving envelopes for obstacle avoidance and stability. PhD

thesis, Stanford University, 2015.

[25] Yue J Zhang, Andreas A Malikopoulos, and Christos G Cassandras. Optimal control and coordination of
connected and automated vehicles at urban trafﬁc intersections. In 2016 American Control Conference (ACC),
pages 6227–6232. IEEE, 2016.

[26] Ashwin Carvalho, Yiqi Gao, Stéphanie Lefevre, and Francesco Borrelli. Stochastic predictive control of au-
tonomous vehicles in uncertain environments. In 12th International Symposium on Advanced Vehicle Control
(AVEC), 2014.

[27] Yiqi Gao, Andrew Gray, H Eric Tseng, and Francesco Borrelli. A tube-based robust nonlinear predictive control

approach to semiautonomous ground vehicles. Vehicle System Dynamics, 52(6):802–823, 2014.

[28] Konstantinos Makantasis and Markos Papageorgiou. Motorway path planning for automated road vehicles based

on optimal control methods. In Transportation Research Board 97th Annual Meeting, 2018.

[29] Yiqi Gao, Theresa Lin, Francesco Borrelli, Eric Tseng, and Davor Hrovat. Predictive control of autonomous
ground vehicles with obstacle avoidance on slippery roads. In ASME 2010 dynamic systems and control conference,
pages 265–272. American Society of Mechanical Engineers, 2010.

[30] Andrew Gray, Mohammad Ali, Yiqi Gao, J Hedrick, and Francesco Borrelli. Semi-autonomous vehicle control

for road departure and obstacle avoidance. IFAC control of transportation systems, pages 1–6, 2012.

[31] Moritz Werling and Darren Liccardo. Automatic collision avoidance using model-predictive online optimization.

In 51st Annual Conference on Decision and Control (CDC), pages 6309–6314. IEEE, 2012.

[32] Yadollah Rasekhipour, Amir Khajepour, Shih-Ken Chen, and Bakhtiar Litkouhi. A potential ﬁeld-based model
predictive path-planning controller for autonomous road vehicles. IEEE Transactions on Intelligent Transportation
Systems, 18(5):1255–1267, 2017.

18

A PREPRINT

[33] Markos Papageorgiou, Magalene Marinaki, Konstantinos Makantasis, and Typaldos Panagiotis. A feasible
direction algorithm for the numerical solution of optimal control problems. Dynamic Syst. Simulation Lab., Tech.
Univ. Crete, Chania, Greece, 2016.

[34] Richard Bellman. On the theory of dynamic programming. Proceedings of the National Academy of Sciences,

38(8):716–719, 1952.

[35] Mariusz Bojarski, Philip Yeres, Anna Choromanska, Krzysztof Choromanski, Bernhard Firner, Lawrence Jackel,
and Urs Muller. Explaining how a deep neural network trained with end-to-end learning steers a car. arXiv preprint
arXiv:1704.07911, 2017.

[36] Shitao Chen, Songyi Zhang, Jinghao Shang, Badong Chen, and Nanning Zheng. Brain-inspired cognitive model

with attention for self-driving cars. IEEE Transactions on Cognitive and Developmental Systems, 2017.

[37] Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong Xiao. Deepdriving: Learning affordance for direct
perception in autonomous driving. In 2015 IEEE International Conference on Computer Vision (ICCV), pages
2722–2730. IEEE, 2015.

[38] Huazhe Xu, Yang Gao, Fisher Yu, and Trevor Darrell. End-to-end learning of driving models from large-scale
video datasets. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
2174–2182, 2017.

[39] Tobias Glasmachers. Limits of end-to-end learning. arXiv preprint arXiv:1704.08305, 2017.
[40] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex
Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep
reinforcement learning. Nature, 518(7540):529, 2015.

[41] David Isele, Akansel Cosgun, Kaushik Subramanian, and Kikuo Fujimura. Navigating intersections with

autonomous vehicles using deep reinforcement learning. arXiv preprint arXiv:1705.01196, 2017.

[42] Mustafa Mukadam, Akansel Cosgun, Alireza Nakhaei, and Kikuo Fujimura. Tactical decision making for lane
changing with deep reinforcement learning. In submitted to International Conference on Learning Representations
(ICLR), 2017.

[43] Chris Paxton, Vasumathi Raman, Gregory D Hager, and Marin Kobilarov. Combining neural networks and tree

search for task and motion planning in challenging environments. arXiv preprint arXiv:1703.07887, 2017.

[44] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for

autonomous driving. arXiv preprint arXiv:1610.03295, 2016.

[45] Jingchu Liu, Pengfei Hou, Lisen Mu, Yinan Yu, and Chang Huang. Elements of effective deep reinforcement

learning towards tactical driving decision making. arXiv preprint arXiv:1802.00332, 2018.

[46] Richard Bellman. The theory of dynamic programming. Bulletin of the American Mathematical Society,

60(6):503–515, 1954.

[47] Venkatesan Kanagaraj, Gowri Asaithambi, CH Naveen Kumar, Karthik K Srinivasan, and R Sivanandan. Evalua-
tion of different vehicle following models under mixed trafﬁc conditions. Procedia-Social and Behavioral Sciences,
104:390–401, 2013.

[48] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In AAAI,

volume 2, page 5. Phoenix, AZ, 2016.

[49] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint

arXiv:1511.05952, 2015.

[50] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992.
[51] Naoki Akai and Luis Morales Yoichi and Takuma Yamaguchi and Eijiro Takeuchi and Yuki Yoshihara and
Hiroyuki Okuda and Tatsuya Suzuki and Yoshiki Ninomiya. Autonomous driving based on accurate localization
using multilayer LiDAR and dead reckoning. In IEEE 20th International Conference on Intelligent Transportation
Systems (ITSC), 2017.

[52] Balázs Csanád Csáji. Approximation with artiﬁcial neural networks. Faculty of Sciences, Etvs Lornd University,

Hungary, 24:48, 2001.

[53] Ryan W. Wolcott and Ryan M. Eustice. Robust LIDAR localization using multiresolution Gaussian mixture maps

for autonomous driving. The International Journal of Robotics Research, 36(3):292–319, 2017.

[54] Varuna De Silva and Jamie Roche and Ahmet Kondoz. Fusion of LiDAR and camera sensor data for environment

sensing in driverless vehicles. arXiv preprint arXiv:1511.05952, 2018.

19

