9
1
0
2

n
u
J

5
2

]
L
M

.
t
a
t
s
[

1
v
3
8
2
0
1
.
6
0
9
1
:
v
i
X
r
a

Certiﬁably Optimal Sparse Inverse Covariance Estimation

Dimitris Bertsimas

Jourdain Lamperski

Jean Pauphilet

Operations Research Center, Massachusetts Institute of Technology, Cambridge, MA.
{dbertsim, jourdain, jpauph}@mit.edu

June 2019

Abstract

We consider the maximum likelihood estimation of sparse inverse covariance matrices. We demon-

strate that current heuristic approaches primarily encourage robustness, instead of the desired sparsity.

We give a novel approach that solves the cardinality constrained likelihood problem to certiﬁable op-

timality. The approach uses techniques from mixed-integer optimization and convex optimization, and

provides a high-quality solution with a guarantee on its suboptimality, even if the algorithm is termi-

nated early. Using a variety of synthetic and real datasets, we demonstrate that our approach can solve

problems where the dimension of the inverse covariance matrix is up to 1, 000s. We also demonstrate that

our approach produces signiﬁcantly sparser solutions than Glasso and other popular learning procedures,

makes less false discoveries, while still maintaining state-of-the-art accuracy.

1 Introduction

Estimating inverse covariance (precision) matrices is a fundamental task in modern multivariate analysis.

Applications include undirected Gaussian graphical models [40], high dimensional discriminant analysis

[11], portfolio allocation [20, 25], complex data visualization [60], amongst many others, see [22] for a

review. For example, in the context of undirected Gaussian graphical models, estimating the precision

matrix corresponds to inferring the conditional independence structure on the related graphical model; zero

entries in the precision matrix indicate that variables are conditionally independent.

Sparsity of the true precision matrix is a prevailing assumption [65, 10, 39, 19, 52] for two reasons.

1. The covariance matrix is often estimated empirically using the maximum likelihood estimator:

Σ =

1
n

n
(cid:88)

i=1

(x(i) − ¯x)(x(i) − ¯x)T ,

(1)

where the number of samples n can be lower than the space dimension p. When this is the case, it is
known that the empirical covariance matrix1 Σ is singular, and thus does not accurately model the
true covariance matrix. Moreover, the empirical covariance matrix can not be inverted to obtain an

1Note that Σ is not the only estimate of the covariance matrix. In particular, n

n−1 Σ is a widely-used unbiased estimator of
the covariance matrix. In this paper, we will only consider Σ, which we might refer to as the empirical or sample covariance
matrix.

1

 
 
 
 
 
 
estimate of the precision matrix. Assuming sparsity of the true precision matrix is required for the

precision matrix estimation problem to be well-deﬁned.

2. In many applications, we use models to improve our knowledge of a given phenomenon and it is fair

to admit that humans are limited in their ability to understand complex models. As Rutherford D.

Roger said ‘We are drowning in information but starving for knowledge’. Models which only involve a

small number variables, i.e. sparse models, are inherently simple. Sparse models with high predictive

power can thus be extremely valuable in practice. We refer skeptic readers to the ﬁrst chapter of [32],

which makes a strong case for sparsity in statistical learning.

The most common method for encouraging sparsity in precision matrix estimation involves solving a (cid:96)1-
regularized maximum likelihood problem. The problem is convex and can be solved in high dimensions.

Though this approach is tractable, solutions suﬀer from similar drawbacks as Lasso solutions in linear
regression [7]. For example, one drawback is the (cid:96)1-penalty introduces extra bias when estimating nonzero
entries in the precision matrix with large absolute values [39].

In this paper, we seek to confront these drawbacks by solving the cardinality constrained optimization
problem for which the (cid:96)1-regularized problem is a convex surrogate. The cardinality constrained problem
parallels the relation the best subset selection (or feature selection) problem plays in linear regression with

Lasso. The main goal of this work is to solve the cardinality constrained problem for problem sizes of interest,

and compare the solutions with current approaches. A summary of the contributions in this paper is given

below.

1. Recent results in linear regression establish that Lasso can be viewed as a robust optimization problem

for an appropriately chosen uncertainty set [62, 5]. In a seminal paper on precision matrix estimation,
[3] already uncovered a similar connection, suggesting that the (cid:96)1-regularization approach is primarily
encouraging robustness and that sparsity is a fortunate by-product. We generalize their result and

show that a wide family of regularization can indeed be viewed as a robust version of the inverse

covariance estimation problem.

2. We formulate the cardinality constrained maximum likelihood problem for the inverse covariance matrix

as a binary optimization problem. We show that the resulting discrete optimization problem is non-

smooth in general, but that adding some well-chosen regularization penalty leads to a smooth convex

discrete optimization problem. In particular, we show that the well-known big-M formulation or the

Ridge regularization term satisfy this property.

3. We propose a combination of outer-approximation algorithm and ﬁrst-order methods to solve the

mixed-integer convex problem. To our knowledge, this is the ﬁrst time in which such a scheme is

used to solve a mixed-integer nonlinear optimization problem with semideﬁnite constraints. It is well-

known that problems of this type are notoriously hard to solve, and we observe that our approach

signiﬁcantly outperforms available mixed-integer nonlinear solvers. An advantage of our approach over

existing approaches is that it provides near optimal solutions fast, and a guarantee on the solutions

suboptimality if the method is terminated early.

4. We report computational results with both synthetic and real-world datasets that show that our pro-

posed approach can deliver near optimal solutions in a matter of seconds, and provably optimal solutions

in a matter of minutes for p in the 100s and k in the 10s. The algorithm also provides high-quality

2

solutions to problems in the 1, 000s, but a certiﬁcate of optimality is more computationally expensive

for those sizes.

5. We investigate empirically statistical properties of solutions for the cardinality constrained problem. We
compare solutions with (cid:96)1-regularized estimates and other popular learning procedures, and observe
that cardinality-constrained estimates recover the sparsity pattern of the true underlying precision

matrix with comparable accuracy as state-of-the-art but signiﬁcantly better false detection rate and

predictive power.

6. Finally, we show the modeling power of our framework and illustrate how it can be easily adapted to

estimate Gaussian graphical with more structural information.

The structure of the paper is as follows: In Section 2, we describe the problem of interest and provide

a more detailed overview of relevant results from the literature. We generalize existing results about the
equivalence between regularization and robustness. From this perspective, (cid:96)1-regularized approaches primar-
ily encourage robustness instead of sparsity, which could explain the known drawbacks of these techniques.

In Section 3 (supplemented by Appendix A), we provide a mixed-integer formulation for the cardinality-

constrained problem. Though non-smooth in general, we show that adding big-M constraints or a ridge

penalty term turns the problem into a smooth convex integer optimization problem, for which we propose

an eﬃcient cutting-plane procedure. We also discuss practical implementation and parameter tuning in

Section 3.4 and Appendix B. In Section 4, we describe and numerically compare ﬁrst-order and coordinate

descent methods to solve variants of the covariance selection problem, used in our algorithm to provide valid

cuts. We perform a variety of computational tests in Section 5 and Appendix C, and use synthetic and

real datasets to assess the algorithmic and statistical performance of our approach. Section 6 illustrates the

modeling power of our approach by discussing extensions to cases where structural information about the

correlation structure is available. In Section 7, we provide concluding remarks.

2 Overview and Preliminaries

In this section, we provide a description of the problem formulation and an overview of current approaches
for inducing sparsity in inverse covariance estimation. Previous work [3] showed that the (cid:96)1-regularization
approach is equivalent to a robust optimization problem with an appropriately chosen uncertainty set. We

generalize their result and discuss practical implications. In particular, this equivalence suggests that current

approaches are primarily encouraging robustness, not sparsity.

2.1 Problem Description

++, where Sp

Let us consider a Gaussian random variable X ∼ N (µ, Σ) with unknown mean µ ∈ Rp and covariance
Σ ∈ Sp
++ denotes the set of symmetric positive deﬁnite matrices in Rp×p. Given a random
sample x(1), ..., x(n) of X, we seek to estimate the precision matrix Σ−1. Let Σ ∈ Rp×p be the empirical
covariance matrix corresponding to the n observations as deﬁned in (1). The maximum likelihood estimate
of Σ−1 is the solution of the optimization problem

min
Θ(cid:31)0

(cid:104)Σ, Θ(cid:105) − log det Θ,

(2)

3

where the expression (cid:104)·, ·(cid:105) is the usual trace inner product (cid:104)Σ, Θ(cid:105) = tr(Σ

(cid:62)

Θ) and the objective function in

(2) is the negative Gaussian log-likelihood of the data [65].

As mentioned in introduction, a more interesting problem in practice is the cardinality-constrained version

of (2)

min
Θ(cid:31)0

(cid:104)Σ, Θ(cid:105) − log det Θ s.t.

(cid:107)Θ(cid:107)0 (cid:54) k,

(3)

where k ∈ Z+, and (cid:107)Θ(cid:107)0 := (cid:80)
part of Θ.

i>j 1Θij (cid:54)=0 counts the number of nonzero entries in the strictly lower triangular

Problem (3) parallels the role best subset selection plays in the context of linear regression. Like best

subset selection, the cardinality constraint makes it computationally challenging and indeed NP-hard [13].
There is also the extra diﬃculty that the problem is a minimization over positive deﬁnite matrices Sp
++. To
our knowledge, the problem has yet to be considered in the literature as a discrete optimization problem

over positive deﬁnite matrices. Thus, this paper provides the ﬁrst provably exact optimization approach

for solving Problem (3). Closest to our approach are recent works for approximately solving a variant of
Problem (3) with an (cid:96)0 penalty instead of a constraint.
[45] propose a coordinate descent method to ﬁnd
good stationary solutions. [41] approximate the (cid:96)0 pseudo-norm by a series of ridge penalties and implement
a variant of the alternating direction method of multipliers.

At the core of our methodology is the exploitation of novel techniques in discrete optimization. Recently,

best subset selection and other cardinality constrained problems have been solved in high dimensions, using

discrete optimization [8, 7, 9]. These approaches exploit the signiﬁcant progress in mixed-integer optimization

in the past decades and motivate our approach.

2.2 Notations

In the remaining of the paper, we will use bold characters to denote matrices or matrix-valued functions.

Unless otherwise stated, all norms on matrices are vector norms and matrices are p × p matrices.

Let us recall some linear algebra identities, which will be useful in Section 4.3. For any invertible matrix

A and vectors u, v, we can compute the determinant of A + uvT [?, ]Eqn. 6.2.3]meyer2000matrix

det(A + uvT ) = det(A) (1 + vT A−1u),

and its inverse [?, Woodbury-Sherman-Morrison Formula in ]Eqn. 3.8.2]meyer2000matrix

(A + uvT )−1 = A−1 −

1
1 + vT A−1u

A−1uvT A−1.

By default, all vectors are p-dimensional vectors. We will denote by ei, i = 1, . . . , p the unit vectors with

1 at the ith coordinate and zero elsewhere, and e the vector of all ones.

2.3 Current Approaches

A variety of convex and nonlinear based optimization methods have been proposed to induce sparsity using

the maximum likelihood problem [24]. Many of these methods can be interpreted as convex relaxation for
Problem (3), the most common of which being the (cid:96)1-regularized negative log-likelihood minimization

min
Θ(cid:31)0

(cid:104)Σ, Θ(cid:105) − log det Θ + λ(cid:107)Θ(cid:107)1,

(4)

4

where (cid:107)Θ(cid:107)1 := (cid:80)
i,j |Θij| is the (cid:96)1 vector norm. In practice, it has been observed that the penalty term
shrinks the coeﬃcients of Θ towards zero, and produces a sparse solution by setting many coeﬃcients equal to

zero. Problem (4) was originally motivated by the development and successes of Lasso as a convex surrogate

for the best subset selection problem [65]. The problem is well-studied in the literature [65, 3, 28, 53, 56]

and solved eﬃciently with a block coordinate descent procedure. [3] originally proposed the block coordinate
[28] then suggested a
descent schema and solved each sub-problem using Nesterov’s ﬁrst-order method.

modiﬁed version of the algorithm, commonly referred to as Graphical Lasso or Glasso for each sub-problem

is reformulated as a Lasso regression problem and solved as such. [47, 48] then further improved the Glasso

algorithm through smart feature screening rules. More recently, [38] used coordinate descent to solve each
sub-problem and released an R package which can solve (4) for a whole regularization path in a short
amount of time - within a minute for p = 1, 000. Coordinate descent [56], alternating linearization [55],

quadratic approximation and Newton’s method [35, 50, 36], and stochastic proximal methods [2] have also

been explored.

In earlier work, [49] proposed an eﬃcient algorithm to discover the sparsity pattern of Σ−1 by ﬁtting a
Lasso model to each variable, using the others as predictors. It has later been shown [3, 28] that their ap-

proach can be viewed as an approximation of Problem (4). More recently, [26] proposed a simple thresholding

heuristic and explored its connection with the graphical lasso (4)

Though the problem is tractable, it shares in the statistical shortcomings of its motivator, Lasso. Problem
(4) leads to biased estimates because the (cid:96)1-norm penalty term penalizes large entries more than the smaller
entries [39]. Accordingly, upon increasing the degree of regularization, (4) sets more entries of Θ to zero but

leaves true predictors outside of the support. Thus, as soon as certain regularity conditions on the data are

violated, Problem (4) becomes suboptimal as a variable selector and in terms of delivering a model with good

predictive performance. In contrast, Problem (3) chooses variables to enter the active set without shrinking

the entries in Θ. [39] discuss other statistical shortcomings of (4).

To address these shortcomings, other relaxation of (3) have been proposed using smooth nonconvex

penalties such as smoothly clipped absolute deviation (SCAD) [23] and minimax concave penalty (MCP)

[66], which are folded concave penalties that do not introduce extra bias for estimating nonzero entries with

large absolute values. Theoretical properties of these methods are well studied [53, 39]. However, these

formulations are nonconvex and cannot provide a guarantee on how close their optimal solution is to the

optimal solution of Problem (3).

Estimators and approaches other than using maximum likelihood have also been proposed for inducing
sparsity. Two such estimators are the constrained (cid:96)1-minimization for inverse matrix estimation (CLIME)
estimator [11] and the graphical Dantzig selector [64]. Rank and factor based methods have also been
proposed; for a more complete survey of the diﬀerent methods, see [24].

From an optimization perspective, mixed-integer semi-deﬁnite optimization (MI-SDP) has received a

lot of attention in recent years, for they naturally appear in robust optimization problems with ellipsoidal

uncertainty sets [4] or as reformulations of combinatorial problems [58]. Problem-speciﬁc MI-SDP strategies

have been developed for problems such as binary quadratic programming [33], robust truss topology [63]

or the max-cut problem [51]. More recently, rounding and Gomory cuts [12, 1], branch-and-bound [29]

and outer-approximation schemes [43] have also been developed, in an attempt to provide the same level

of general-purpose solvers for MI-SDP as there are for mixed-integer linear optimization. Our approach

is similar to the outer-approximation procedure described by [43] but leverages the speciﬁc dependency

between the binary and continuous variables in our problem. It also disconnects the combinatorial aspect

5

of the problem from its SDP component, allowing us to beneﬁt both from advances in mixed-integer linear

optimization and tailor-made semideﬁnite strategies.

2.4 Equivalence between Regularization and Robustness

As originally enunciated by [3], the (cid:96)1-regularization in (4) is the aftermath of a robust optimization problem.
Indeed, one can prove a clear equivalence between regularization and robustiﬁcation in the case of sparse

inverse covariance problems:

Theorem 1.A. For any vector norm (cid:107) · (cid:107),

min
Θ(cid:31)0

(cid:104)Σ, Θ(cid:105) − log det Θ + λ(cid:107)Θ(cid:107) = min
Θ(cid:31)0

max
U:(cid:107)U(cid:107)(cid:63)(cid:54)λ

(cid:104)Σ + U, Θ(cid:105) − log det Θ,

where (cid:107) · (cid:107)(cid:63) denotes the dual norm of (cid:107) · (cid:107).

Theorem 1.B. For any (p, q)-induced norm (cid:107) · (cid:107)(p,q),

min
Θ(cid:31)0

(cid:104)Σ, Θ(cid:105) − log det Θ + λ(cid:107)Θ(cid:107)(p,q) = min
Θ(cid:31)0

max
U∈U(p,q)

(cid:104)Σ + λU, Θ(cid:105) − log det Θ,

with U(p,q) := (cid:8)uvT : (cid:107)u(cid:107)p = 1, (cid:107)v(cid:107)q(cid:63) = 1(cid:9) and q(cid:63) deﬁned such that 1/q + 1/q(cid:63) = 1.

Let us recall that for any matrix A and p, q ∈ Z+ ∪ {∞}, the (p, q)-induced norm of A is deﬁned as

(cid:107)A(cid:107)(p,q) := max

u:(cid:107)u(cid:107)p=1

(cid:107)Au(cid:107)q.

In particular, the operator norm or the largest singular value of A is equal to its (2, 2)-induced norm.

Proof. Theorem 1.A follows directly from the deﬁnition of the dual norm

(cid:107)Θ(cid:107) = max

U:(cid:107)U(cid:107)(cid:63)(cid:54)1

(cid:104)U, Θ(cid:105).

Theorem 1.B follows from the fact that the dual norm of the (cid:96)q-norm is the (cid:96)q(cid:63) -norm, so that:

(cid:107)A(cid:107)(p,q) = max

u:(cid:107)u(cid:107)p=1

(cid:107)Au(cid:107)q = max

u:(cid:107)u(cid:107)p=1

max
v:(cid:107)v(cid:107)q(cid:63) =1

vT Au.

In the result above, the matrix U should be interpreted as the amount of noise on the covariance matrix

Σ one wishes to be protected against. Similar equivalence results have been proved in a wide range of other

statistical settings [6]. From a Bayesian perspective, regularization can also be derived by imposing some

prior distribution on the entries of Θ and there is a one-to-one correspondence between the class of prior

distributions, the corresponding uncertainty set in the robust perspective and the resulting penalty.

In addition to this robustness property, the (cid:96)1-norm is fortunately sparsity-inducing. Killing two birds
with one stone, (cid:96)1-regularization has naturally received a lot of attention from the statistical community.
Yet, it is fair to admit that the robustness interpretation of the (cid:96)1-norm has been neglected and that many
variants of (4) use the (cid:96)1-norm solely for sparsity, even though it makes little sense from a robust perspective.
For instance, diagonal entries of Θ should be nonzero - a consequence of Hadamard’s inequality and the

6

constraint Θ (cid:31) 0. This motivates the fact that diagonal entries are excluded from the cardinality constraint
in (3). Similarly, many derivatives of (4) exclude diagonal entries from the (cid:96)1-penalty, which, from a robust
point of view, is equivalent to considering that diagonal entries of Σ are noiseless. To avoid such unrealistic

assumptions, robustness and sparsity should, in our opinion, be considered as two distinct properties and be

treated as such.

3

Integer Optimization Perspective

We ﬁrst formulate Problem (3) as binary optimization problem in Section 3.1, and prove that it is non-

smooth in general. In practice, introducing big-M constants is a simple way to linearize such mixed-integer

bilinear problems. Yet, choosing the right big-M values is hard, making these reformulations not always

amenable for computation. We show in Section 3.2 that big-M formulations can be viewed as a special case

of regularization. With regularization as a unifying perspective, we prove that a certain class of penalty

functions leads to smooth convex integer optimization problems and propose a general cutting-plane algo-

rithm to solve them in Section 3.3. We believe our approach provides a novel perspective on the big-M

paradigm. In particular, we regard big-M more as a smoothing technique than a simple modeling trick and

reveal promising alternatives, such as ridge regularization.

3.1 Problem Formulation

Let us introduce binary variables Zij to encode the support of the inverse covariance matrix Θ. The set of
feasible supports is

S k

p =






Z ∈ {0, 1}p×p : ∀i, Zii = 1 and ∀i > j, Zij = Zji and

Zij (cid:54) k

(cid:88)

i,j>i






.

The ﬁrst set of constraints allows diagonal elements of Θ to take nonzero values. The second set of constraints

follows from the fact that Θ is symmetric. With these notations, we formulate the cardinality constrained

Problem (3) as the mixed-integer optimization problem

min
p ,Θ(cid:31)0

Z∈S k

(cid:104)Σ, Θ(cid:105) − log det Θ s.t. Θij = 0 if Zij = 0 ∀(i, j),

which can be considered as a binary-only optimization problem

with the objective function

min
Z∈S k
p

h(Z),

h(Z) := min
Θ(cid:31)0

(cid:104)Σ, Θ(cid:105) − log det Θ s.t. Θij = 0 if Zij = 0 ∀(i, j).

(5)

(6)

The inner-minimization problem deﬁning h(Z) is a so-called covariance selection problem [16], which is a

well-studied problem in the literature, and can be eﬃciently solved. In Section 4, we discuss more details of

how the problem can be solved using tailored ﬁrst-order methods [15] or coordinate descent schemes [56, 38].

Note that the problem is always feasible since the identity matrix satisﬁes all the constraints. Fortunately, as

7

a function of Z, h(Z) is convex (see proof in Appendix A). However, h(Z) is piece-wise constant and exhibits

strong discontinuities. In the following subsection, we explore techniques to reformulate or approximate h(Z)

in a smooth convex way, through the unifying lens of regularization.

3.2 Smoothing through regularization

In this section, we explore a regularized version of (6),

˜h(Z) := min
Θ(cid:31)0

(cid:104)Σ, Θ(cid:105) − log det Θ + Ω(Θ)

s.t. Θij = 0 if Zij = 0 ∀(i, j),

where Ω is regularizer, that is, a convex function of Θ. In particular, we are interested in two special cases:

Big-M regularization: A traditional way to express the dependency between Z and Θ in (6) is to use

big-M constraints

˜h(Z) := min
Θ(cid:31)0

(cid:104)Σ, Θ(cid:105) − log det Θ s.t. |Θij| (cid:54) MijZij ∀(i, j).

Mij ∈ R+ are constants chosen suﬃciently large such that if Θ∗ is a minimizer for Problem (3), then
|Θ∗

˜h(Z) = minZ h(Z), i.e., h and ˜h have the same minimum with

ij| (cid:54) Mijzij. In this case, minZ


0


Ω(Θ) =

if |Θij| (cid:54) Mij,



+∞ otherwise.

Ridge (or (cid:96)2

2) regularization: One can choose

Ω(Θ) =

1
2γ

(cid:107)Θ(cid:107)2

2 =

1
2γ

(cid:88)

i,j

Θ2
ij,

for some positive constant γ. Whatever γ > 0, Ω(Θ) > 0, so ˜h is not a reformulation but an upper-
Ideally, one would like to minimize ˜h for 1/γ → 0. However, as previously seen,
approximation of h.
regularization induces desirable robustness properties, so having 1/γ > 0 may be beneﬁcial from a statistical

perspective.

Under some weak assumptions on Ω, which are satisﬁed in the special cases of big-M and ridge regular-

ization, one can reformulate ˜h(Z) using strong duality:

Theorem 2. For any Z ∈ {0, 1}p×p such that Zii = 1 for all i = 1, . . . , p,

˜h(Z) := min
Θ(cid:31)0

(cid:104)Σ, Θ(cid:105) − log det Θ + Ω(Θ)

s.t. Θij = 0 if Zij = 0 ∀(i, j),

= max

R:Σ+R(cid:31)0

p + log det(Σ + R) − (cid:104)Z, Ω(cid:63)(R)(cid:105),

where Ω(cid:63) is some generalization of the Fenchel conjugate for Ω [?, see]chap. 3.3]boyd2004convex.

An explicit statement of the assumptions and proof of the theorem can be found in Appendix A. Theorem

2 calls for a few observations:

1. ˜h(Z) is a point-wise maximum of linear, hence convex, functions of Z. As a result, ˜h is a convex

function.

8

2. With the dual reformulation, it is easy to see that ˜h(Z) remains bounded.

3. For the big-M regularization, Theorem 2 reduces to

˜h(Z) = min
Θ(cid:23)0

(cid:104)Σ, Θ(cid:105) − log det Θ s.t. |Θij| (cid:54) MijZij,

= max

p + log det(Σ + R) −

R:Σ+R(cid:31)0

(cid:88)

i,j

MijZij|Rij|.

4. For the (cid:96)2

2-regularization, Theorem 2 reduces to

˜h(Z) = min
Θ(cid:31)0

(cid:104)Σ, Θ(cid:105) − log det Θ +

1
2γ

(cid:107)Θ(cid:107)2
2

s.t. Θij = 0 if Zij = 0,

= max

p + log det(Σ + R) −

R:Σ+R(cid:31)0

γ
2

(cid:88)

i,j

ZijR2
ij.

5. Given a feasible support Z, we denote by R(cid:63)(Z) the associated dual variable, i.e., ˜h(Z) = p+log det(Σ+

R(cid:63)(Z)) − (cid:104)Z, Ω(cid:63)(R(cid:63)(Z))(cid:105). Then for any feasible Z(cid:48), we have

˜h(Z(cid:48)) (cid:62) ˜h(Z) + (cid:104)Z(cid:48) − Z, Ω(cid:63)(R(cid:63)(Z))(cid:105).

(7)

The inequality above provides a linear lower-approximation of ˜h which coincides with ˜h at Z.
In
particular, it proves that −Ω(cid:63)(R(cid:63)(Z)) is a subgradient of ˜h at Z. This observation plays a central role
in devising a numerical strategy to solve (5).

3.3 Cutting-plane algorithm

Instead of solving the non-smooth integer optimization Problem (5), we consider its regularized proxy

with

˜h(Z),

min
Z∈S k
p

˜h(Z) = min
Θ(cid:31)0

(cid:104)Σ, Θ(cid:105) − log det Θ + Ω(Θ)

s.t. Θij = 0 if Zij = 0 ∀(i, j),

=

max
R:Σ+R(cid:31)0

p + log det(Σ + R) − (cid:104)Z, Ω(cid:63)(R)(cid:105),

(8)

(9)

as studied in the previous section. Our numerical approach substitutes ˜h in (8) by a piece-wise linear lower-
approximation and iteratively reﬁnes this approximation. This process is equivalent to constraint generation:
Applying the inequality (7) at all feasible supports, ˜h can indeed be seen as a piece-wise linear convex function
with an exponential number of pieces:

˜h(Z(cid:48)) = max

(cid:110)˜h(Z) + (cid:104)Z(cid:48) − Z, Ω(cid:63)(R(cid:63)(Z))(cid:105) : Z ∈ S k

p

(cid:111)

,

∀Z(cid:48) ∈ S k
p ,

and the algorithm iteratively includes new pieces. The method is referred to in the literature as outer-

approximation [18] or generalized Benders decomposition (GBD) and described in pseudo-code in Algorithm

3.1.

9

Algorithm 3.1 Cutting-plane algorithm

Require: Initial point Z(1) ∈ S k

p , sample covariance matrix Σ, sparsity parameter k, and tolerance (cid:15).

t ← 1
repeat

Compute Zt+1, ηt+1 solution of

min
Z∈Sp
k ,η

η

s.t. η (cid:62) ˜h(Zi) + (cid:104)Z − Zi, Ω(cid:63)(R(cid:63)(Zi))(cid:105), ∀i = 1, . . . , t.

(10)

Compute R(cid:63)(Zt+1), ˜h(Zt+1) by solving (9).
t ← t + 1

until ηt < ˜h(Zt) − ε
return Zt

We summarize some important observations, properties, and connections to the literature for the above

algorithm.

1. Generalized Benders decomposition is a method that can be used to solve convex mixed-integer op-

timization problems. In this context, Problem (10) is often referred to as the master problem, and

Problem (9) is referred to as the (separation) subproblem. The GBD algorithm converges in this

context in a ﬁnite number of steps because subproblems (9) are convex and satisfy Slater’s condition,
and the set S k
p is ﬁnite (see Theorem 2.4 in [30]). Thus, the above algorithm converges to an optimal
solution for the cardinality constrained Problem (8) in a ﬁnite number of steps.

2. Note that at each iteration the algorithm supplies a feasible solution Zt, an upper bound ˜h(Zt), and a
lower bound ηt on the optimal solution. Current heuristic approaches do not oﬀer such a certiﬁcate of
suboptimality.

3. Algorithm 3.1 requires to solve a large mixed-integer linear optimization problem each time a new

constraint is added. Thus, a branch and bound tree is built at each iteration of the algorithm. Lazy

constraint callbacks provide an alternative to building a new branch and bound tree at each iteration

of the algorithm. When a constraint is added, instead of resolving the problem, the constraint is added

to all active nodes in the current branch-and-bound tree. This enables the same tree to be used for all

iterations. This saves the rework of building a new tree every time a mixed-integer feasible solution is
found. Lazy constraint callbacks are a relatively new type of callback. CPLEX 12.3 introduced lazy

constraint callbacks in 2010 and Gurobi 5.0 introduced lazy constraint callbacks in 2012. To date, the

only mixed-integer solvers which provide lazy callback functionality are CPLEX [37], Gurobi [31], and
GLPK (see http://gnu.org/software/glpk/).

4. The algorithm can greatly beneﬁt from the choice of a good initial solution Z(1).

In practice, we

initialize the algorithm with the support returned by Glasso or Meinshausen and B¨uhlmann’s [49] local

neighborhood selection method.

3.4

Implementation considerations and cross-validation

In this section, we describe the grid-search procedure to tune the value of the sparsity level, k, and the

regularization parameter, M or γ.

10

Two alternatives have been considered in the literature for parameter tuning. The ﬁrst approach is cross-

validation: Before any computation, the data is divided into a training and a validation set, typically with

a ratio of 2 : 1. Inverse covariance matrices are computed using the training data only and evaluated out-of-

sample on the validation data. We pick the parameter values that lead to the best out-of-sample performance

in terms of negative log-likelihood. Though simple, cross-validation does not generally have consistency
Its“leave-one-out” or “multi-fold” variants are computationally more
properties for model selection [57].

expensive for they repeat this process on multiple training / validation splits. The second approach consists

in using an in-sample information criterion, such as the extended information criterion from [27]

BIC1/2(Θ) = n (cid:2)(cid:104)Σ, Θ(cid:105) − log det Θ(cid:3) + (cid:107)Θ(cid:107)0 log n + 2(cid:107)Θ(cid:107)0 log p,

which balances goodness of ﬁt and complexity of the model. This criterion is satisfying for it can be computed

in-sample and is asymptotically consistent. Consistency results, however, only hold asymptotically and under

some assumptions on the data. We will compare those two approaches numerically in Section 5.

We test diﬀerent values of k in a grid search manner. Let us remark that the sparsity k only impacts the
feasible set of Problem (8) and that all linear lower approximations of ˜h generated from solving a particular
instance of Problem (8) are valid for any value of k. Practically speaking, we solve a series of problems

(8) for decreasing values of k, where each new problem is constructed from the previous one by adding a

tighter cardinality constraint. In such a way, each new problem beneﬁts from the cuts generated for previous

problems.

Regarding the regularization parameter, we inspect values which are uniformly log-distributed, starting
from M0 = p/(cid:107)Σ(cid:107)1 for the big-M regularization and γ0 = 4p/(cid:107)Σ(cid:107)2
2 for the ridge regularization. Those values
follow from bounds on the norm of Θ(cid:63), the optimal solution of Problem (8), which we prove in Appendix
A.3. For the big-M formulation, we describe an optimization-based approach to ﬁnd valid M values from

any feasible solution in Appendix B.

4 Covariance selection problem

In this section, we investigate numerical strategies to eﬃciently solve separation subproblems of the form (9).

We provided both primal and dual formulations for the separation Problem (9). In Section 4.1, we discuss

the main advantages of solving the primal vs. the dual formulation. In Section 4.2 and 4.3 we describe two

families of numerical algorithms. In Section 4.4, we compare empirically those algorithms.

4.1 Comparisons between primal and dual approaches

The overall cutting-plane algorithm 3.1 requires at each iteration not only the optimal value h(Z) but also
the associated dual variables R(cid:63)(Z), which are eventually needed to obtain the subgradients −Ω(cid:63)(R(cid:63)(Z)).
For that matter, solving the dual formulation in (9) appears attractive.

In the end, the variables of interest are the primal ones, i.e., the sparse precision matrix. Optimal primal
and dual variables satify the KKT conditions Σ+R(cid:63)−(Θ(cid:63))−1 = 0 (see proof of Theorem 2 in Appendix A.2).
So, primal variables can be reconstructed from the dual variables at the cost of a p × p matrix inversion.
Due to numerical errors however, inverting R(cid:63)(Z) might not lead to a sparse matrix. To that extent, it
might be favorable to solve the primal formulation in (9), and obtain dual variables by inverting Θ(cid:63)(Z).
This computation might be computationally expensive (O(p3)) , but Θ(cid:63) is sparse, it involves at most p + 2k

11

nonzero coeﬃcients, a pattern which numerical algorithms could exploit.

All in all, the primal and dual formulations seem equally attractive. Moreover, both objective functions

involve the log-determinant. As a result, any gradient-based method will require updating the decision

variable, as well as its inverse. Matrix inversion is thus the computational bottleneck for both primal and

dual methods. Based on these observations, we identiﬁed two streams of relevant numerical strategies:

1. The ﬁrst stream of algorithms implements standard ﬁrst- or second-order methods to solve the primal
problem, leveraging the structure of the sparsity pattern deﬁned by Z to eﬃciently compute and update

the inverse of Θ [15].

2. The second stream consists in coordinate descent methods for either the primal [56] or the dual for-

mulation [38], where each iteration leads to low-rank update of the matrix and its inverse.

4.2 Gradient-based methods for the primal formulation

[15] proposed an eﬃcient gradient-based algorithm for solving the unregularized covariance selection Problem

(6). The gradient of the objective function is

Σ − Θ−1.

However, thanks to the constraints that Θij = 0 if Zij = 0, only the p+2k coordinates Θij with (i, j) such that
Zij = 1 are to be updated. In this context, [15] showed how a particular kind of sparsity patterns - patterns
whose clique graph is chordal [?, see]Section 3 for a deﬁnition]dahl2008covariance - could enable smart block
structure decomposition of both Θ and its inverse and fast computations of Θij and Θ−1
ij
(i, j) of interest. They also generalize their approach to sparsity patterns which are not chordal, through the

for the coordinates

use of so-called chordal embeddings. For large and sparse matrices, [15] report speedups in runtime of two

to three orders of magnitude for computing the inverse, and hence the gradient of the objective function. In
a similar fashion, their method can accelerate Hessian updates as well. They publicly released CHOMPACK, a
library which implements sparse matrix computations leveraging chordal sparsity patterns [61].

Lastly, [15] report that a limited-memory Broyden-Fletcher-Goldfarb-Reeves (BFGS) method signiﬁ-

cantly outperforms other ﬁrst order methods, such as conjugate gradient, for the covariance selection Prob-
lem (6). Surprisingly, the authors mention but do not numerically compare with coordinate descent methods,

which will be the topic of the next section.

In the case of the regularized covariance selection Problem (9), their approach can easily be adapted:

• For big-M regularization, one simply needs to project the iterates to ensure the constraints |Θij| (cid:54) Mij

are satisﬁed throughout the algorithm.

• Ridge regularization adds a 1

γ Θ term to the gradient, which raises no additional computational diﬃ-

culty.

4.3 Coordinate descent methods

Coordinate descent methods are one of the most widely used and highly scalable methods in statistical
learning problems. Indeed, as previously mentioned, the most successful methods for (cid:96)1-regularized inverse
covariance estimation (4) all involve a block coordinate descent strategy for the dual formulation and diﬀer

only in the algorithm used to solve the subproblem associated with each block. The caveat in coordinate

12

descent methods often resides in an eﬃcient update step, combined with a good rule for picking the coordinate

to update. As noted by many authors in similar contexts [15, 56, 38], the update step can be computed in

closed-form in our case, which makes coordinate descent methods very attractive.

For clarity, we illustrate the main ingredients of these methods on the primal formulation with (cid:96)2
2-
regularization only, but the same ideas can be applied to the dual formulation and to big-M regularization
as well. For a given feasible support Z, we solve

min
Θ(cid:31)0

(cid:104)Σ, Θ(cid:105) − log det Θ +

1
2γ

(cid:107)Θ(cid:107)2
2

s.t. Θij = 0 if Zij = 0.

4.3.1 Coeﬃcient updates

Given Θ (cid:31) 0, we ﬁrst consider the update of the (i, j)th coeﬃcient with i (cid:54)= j, that is, Θij ← Θij + t for
some t ∈ R. In matrix form, this can be written as Θ ← Θ + t(eieT
i ). Denoting W := Θ−1 the
inverse of Θ, we have

j + ejeT

log det(Θ + teieT

j + tejeT

i ) = log det Θ + log (cid:0)1 + 2Wijt + (W 2

ij − WiiWjj)t2(cid:1) ,

so that the best update is obtained by minimizing

2Σijt − log (cid:0)1 + 2Wijt + (W 2

ij − WiiWjj)t2(cid:1) + 1

γ (Θij + t)2.

Setting the derivative to zero, we ﬁnd the best update t(cid:63) as the unique solution of the equation

2Σij −

2Wij + 2(W 2
1 + 2Wijt + (W 2

ij − WiiWjj)t
ij − WiiWjj)t2 + 2

γ (Θij + t) = 0,

which satisﬁes 1 + 2Wijt + (W 2
in t.

ij − WiiWjj)t2 > 0. The above equation can be reduced into a cubic equation

Regarding diagonal coeﬃcients, the best update for the (i, i)th coeﬃcient, Θii ← Θii + 2t, can similarly

be found by minimizing

2Σiit − log (1 + 2Wiit) + 1

2γ (Θii + 2t)2,

over t such that 1 + 2Wiit > 0, which boils down to solving a quadratic equation.

In both cases, the value t(cid:63) for the best update Θ ← Θ + t(cid:63)(eieT

i ) can fortunately be computed in
closed-form, i.e., constant time. After updating Θ, W can be update in O(p2) steps only, using Woodbury-
ShermanMorrison formula.

j + ejeT

Observe that using these one-coordinate updates, the matrix Θ remains positive deﬁnite throughout the
algorithm. Indeed, using Shur complements [67], Θ + t(cid:63)(eieT
ij −
WiiWjj) > 0. If the algorithm is properly initialized by a positive deﬁnite matrix, positive deﬁniteness of
the subsequent iterates then follows by induction.

i ) (cid:31) 0 if Θ (cid:31) 0 and 1 + 2Wijt(cid:63) + (W 2

j + ejeT

4.3.2 Update rule and computational complexity:

In the case of Glasso, [56] successfully suggested a greedy rule: at each iteration, the algorithm scans through

all the coeﬃcients of Θ and compute the objective decrease resulting from their update. Then, only the

13

coeﬃcient leading to the largest improvement is updated, as described in Algorithm 4.1. All together, one
iteration of the algorithm updates one coeﬃcient and requires O(p2) operations, with the update of W as
the computational bottleneck. Note that this strategy is particularly eﬃcient on the primal formulation,

since there are only p + 2k potentially nonzero coeﬃcients, compared with p × (p + 1)/2 in the dual.

Algorithm 4.1 Greedy coordinate descent algorithm

Require: Support Z ∈ S k

p , sample covariance matrix Σ, regularization parameter γ.

repeat

For all (i, j) such that Zij = 1, compute the objective decrease resulting from the update of the (i, j)th
coeﬃcient.
Update Θ ← Θ + t(cid:63)eieT
Update W accordingly

for (i, j) which leads to the biggest improvement.

j + t(cid:63)ejeT
i

until Stopping criterion
return Θ

Since updating the inverse of Θ remains the challenging part, [38] suggested a block coordinate approach

for solving the dual formulation of the Lasso estimator (4). We can adapt their approach to our regularized

covariance selection problem, both in primal and dual formulation. From a high level perspective, at each
iteration, a whole row is updated instead of a single coeﬃcient. The computational cost remains O(p2) steps
per iteration, but one might expect fewer iterations in total. We refer to [38] for a detailed presentation of

the updates and the overall algorithm.

We terminate the algorithm as soon as the duality gap or the objective decrease is suﬃciently small.

4.4 Empirical performance and comparisons

In this section, we compare the computational time required to solve the covariance selection problem by

each method and see how they scale with the problem size p and the sparsity k. We also investigated how

the conditioning of the problem, through the number of samples n used to compute the empirical covariance

matrix Σ and the regularization parameter M or γ, impacted computational time. However, we observed

little eﬀect and decided not to report those experiments.

4.4.1

Instance generation

As in [65, 28], we consider a full precision matrix Θ0 with Θii = 2 and Θij = 1 for i (cid:54)= j, in short
Θ0 = Ip + eeT . We then generate n random samples from the normal distribution N (0, Θ−1
0 ) and compute
the empirical covariance matrix Σ. We randomly sample a feasible support Z from S k
p and solve Problem
(9).

The degrees of freedom in our simulations are the dimension p and the sparsity level t. Based on those

quantities, k and n are ﬁxed to

(cid:22)

k =

t

p(p − 1)
2

(cid:23)

,

n = p.

4.4.2 Methods implementation

For both the big-M and the (cid:96)2

2 regularization problem, we implement and compare ﬁve methods:

14

(a) p, with t = 1%.

(b) t, with p = 200.

Figure 1: Impact of dimension size p and sparsity level t on computational time, for the big-M regularization
with M = M0 = p/(cid:107)Σ(cid:107)1.

• a BFGS method on the primal formulation (BFGS_primal), using the library CHOMPACK for sparse

matrix computations [61],

• four (block) coordinate descent strategies, denoted CD_primal, CD_dual,

BCD_primal, and CD_dual.

All code is written in Julia 0.6.0 [42], with the exception of the BFGS algorithm, which is implemented
in Python 3.5.3 and integrated into the main Julia script using the PyCall package. We terminate the
algorithms when the duality gap falls below 10−4 or the objective improvement after one iteration is less
than 10−12.

4.4.3 Empirical results

Figures 1 and 2 report computational time as a p and t increase for the big-M and ridge regularization

respectively. From these experiments, we can make the following observations:

1. For (block) coordinate descent methods, solving the primal formulation is more eﬀective than solving

the dual problem.

2. Coordinate descent methods compete with block coordinate descent schemes when the sparsity level t

is very low (less than 1%) but do not scale as well as t increases.

3. As a result, BCD_primal is often the best method for solving Problem (9).

4. The BFGS_primal algorithm generally takes 50 − 100 times longer than BCD_primal. For p > 1000,

the algorithm did not terminate after a 12-hour time limit.

5 Computational Results

In this section, we present numerical results on both synthetic (Section 5.1) and real data (Section 5.2).

15

0500100015002000p02.0×10³4.0×10³6.0×10³8.0×10³1.0×101.2×10Time (in s)BFGS_primalCD_primalCD_dualBCD_primalBCD_dual0.000.050.100.150.20t20406080100120140Time (in s)BFGS_primalCD_primalCD_dualBCD_primalBCD_dual(a) p, with t = 1%.

(b) t, with p = 200.

Figure 2: Impact of dimension size p and sparsity level t on computational time, for the ridge regularization
with γ = γ0 = 4p/(cid:107)Σ(cid:107)2
2.

5.1 Synthetic experiments

We follow the methodology described in [3]. We sample precision matrices of the form Θ0 = δIp + 0.5Z0,
where Z0 ∈ S p
and δ is chosen so that the condition number is equal to p. We then randomly sample n
vectors from a multivariate normal distribution N (0, Θ−1
0 ), compute the empirical covariance matrix Σ and
standardize it. To evaluate the output of the algorithms out-of-sample, we generate similarly n/2 (resp. 5n)

ktrue

data points for the validation (resp. test) set.

In this setting, we can assess the feature selection ability of a method in terms of accuracy A, i.e., the
fraction of the ktrue nonzero upper-diagonal coeﬃcients of Θ0 correctly recovered, and false detection rate
F DR, deﬁned as the proportion of coeﬃcients in the support of the solution which are not in the support
of Θ0. We also compute the negative log-likelihood (−LL) of the returned precision matrix on the test set.
All discrete optimization problems are terminated once the tolerance gap falls below 10−4, where the
tolerance gap is the percentage diﬀerence between the ﬁnal lower and upper bounds, or after a 5-minute

time limit.

5.1.1

Impact of regularization and sparsity k

First, we consider one problem instance with p = 200, n/p = 1, and sparsity level ttrue = 1%. The discrete
formulation (8) involves two hyper-parameters, the sparsity k and the regularization parameter M or γ,
which needs to be tuned using grid-search as described in Section 3.4.

The value of the regularization parameter has a crucial impact on the overall computational time of the

cutting-plane algorithm. Figure 3 shows a steep increase in computational time (top) and in the number

of cuts (middle) as the regularization parameter, for both big-M and ridge regularization, increases. Un-

fortunately, for applications of interest in our experiments, we needed to use high values of M and γ and

had to stop the algorithm after a 5-minute time limit. Yet, this early stopping strategy did not harm the

overall performance of our approach. Indeed, the algorithm is able to ﬁnd optimal or near-optimal solutions

in a short amount of time but spends most of the time proving optimality. For moderate values of M/γ,

the optimality gap (Figure 3(c)) after ﬁve minute is indeed relatively small, and the algorithm spents a lot

of time closing that gap. For large regularization parameter value, on the other hand, the gap increases

signiﬁcantly (over 100%) and becomes uninformative. This corresponds to the regime of most of our subse-

16

0100020003000p05.0×10³1.0×101.5×102.0×10Time (in s)BFGS_primalCD_primalCD_dualBCD_primalBCD_dual0.000.050.100.150.20t0100200300Time (in s)BFGS_primalCD_primalCD_dualBCD_primalBCD_dualquent experiments for which we will not report optimality gaps. We provide extensive computational time

experiments on smaller-size problems as n, p and k vary in Appendix C.

At the end of the grid search, we select the best pair of parameters and compare the quality of the solution

in terms of sparsity, accuracy, false detection and out-of-sample log-likelihood with solutions returned by
Glasso [28] and Meinshausen and B¨uhlmann’s approximation scheme [49], implemented in the R package
glasso2. We tuned the hyper-parameter ρ in those formulations through a grid search, testing values which
led to similar sparsity level k as the discrete formulations. Table 1 (resp. Table 2) reports the results when

the hyper-parameters are tuned using the negative log-likelihood on a test set (resp. the information criterion

from [27]).

In both cases, we observe that discrete formulations outperform the other two methods in terms of

resulting sparsity (by at least 40%), false detection rate (by a factor 4-12) and out-of-sample likelihood (by

11-18%). On the other hand, Meinshausen and B¨uhlmann’s approximation (MB in short) is always the

fastest and most accurate method. Actually, we use its solution as a warm-start to our discrete optimization

method. Let us remark that the big-M and the ridge formulation perform almost identically and that their

performance is barely not impacted by the choice of the criterion. On the contrary, the model selected with

Glasso and MB highly depends on the cross-validation criterion: with negative log-likelihood, both methods
tend to select the less sparse model, whereas much sparser models are selected with BIC1/2.

Method

k(cid:63)
A
F DR
−LL
Time (in s)

big-M

Ridge

MB

Glasso

199 (0)
0.9508 (0.0080)
0.0492 (0.0080)
141.39 (3.05)
352.87 (11.12)

199 (0)
0.9508 (0.0080)
0.0492 (0.0080)
141.37 (3.05)
203.36 (39.00)

796 (0)
0.9960 (0.0020)
0.6791 (0.0030)
157.11 (2.47)
1.10 (0.04)

796 (0)
0.9945 (0.0023)
0.7514 (0.0006)
162.05 (1.89)
3.97 (0.31)

Table 1: Average performance on synthetic data with p = 200, n/p = 1, t = 1% (leading to ktrue = 199),
where the hyper-parameters of each formulation is chosen using the best negative log-likelihood over a
validation set. We report the average performance over 10 instances (and their standard deviation).

Method

k(cid:63)
A
F DR
−LLtest
Time (in s)

big-M

Ridge

MB

Glasso

194 (5)
0.9317 (0.0081)
0.0444 (0.0062)
141.78 (3.24)
349.5 (14.5)

194 (5)
0.9317 (0.0081)
0.0444 (0.0062)
141.78 (3.24)
225.2 (43.00)

276 (8)
0.9890 (0.0037)
0.2634 (0.0213)
167.16 (2.48)
0.90 (0.05)

542 (26)
0.9814 (0.0047)
0.6329 (0.0167)
170.22 (2.42)
2.77 (0.19)

Table 2: Average performance on synthetic data with p = 200, n/p = 1, t = 1% (leading to ktrue = 199),
where the hyper-parameters of each formulation are chosen using the best in-sample extended Bayesian
information criterion BIC1/2. We report the average performance over 10 instances (and their standard
deviation).

5.1.2

Impact of problem size

We now pursue the same comparison for problems with varying characteristics n/p, t and p.

2available at https://cran.r-project.org/web/packages/glasso/

17

(a) Computational time (in seconds).

(b) Number of cuts.

(c) Relative optimality gap.

Figure 3: Impact of the regularization parameter M/M0 for big-M (left), γ/γ0 for ridge (right) on computa-
tional time (top), number of cuts (middle) and relative optimality gap (bottom). For the big-M regulariza-
tion, M0 = p/(cid:107)Σ(cid:107)1. For ridge regularization, γ0 = 4p/(cid:107)Σ(cid:107)2
2.

18

102101100101102M/M0100200300400500600Time (in s)k=50k=100time_limit102101100101102/0100200300400500600700Time (in s)k=50k=100time_limit102101100101102M/M0102030405060Lazy constraintsk=50k=100102101100101102/01020304050Lazy constraintsk=50k=100102101100101102M/M00.00.51.01.52.02.5Optimality Gapk=50k=100102101100101102/0012345Optimality Gapk=50k=100(a) Accuracy A vs. n/p.

(b) False detection rate F DR vs. n/p.

Figure 4: Impact of the number of samples n/p on support recovery. Results are averaged over 10 instances
with p = 200, t = 1%. Hyper-parameters are tuned using out-of-sample negative log-likelihood.

Number of samples n Information-theoretic intuition suggests that the problem becomes easier as n

increases. For n < p, the empirical covariance matrix is always singular so its inverse cannot be properly

deﬁned without sparsity assumptions. On the other side of the spectrum, theoretical guarantees exists for

many algorithms [49, 54] in the limit n → ∞. As shown on Figure 4, this intuition is conﬁrmed experimentally

with accuracy (resp.

false detection rate) increasing (resp. decreasing) as n/p increases. In addition, we

observe that the conclusions drawn from the previous section hold consistently for various values of n: the

discrete optimization formulations lead to reduced false detection rate, while being of comparable accuracy

with the most accurate benchmark. They also demonstrate better out-of-sample negative log-likelihood

(Figure 6 in Appendix D) and their performance is robust to the cross-validation criterion used (Figure 7 in

Appendix D). Note that the other two methods, MB and Glasso, do not exhibit a decreasing false detection
rate when cross-validated using the BIC1/2 criterion.

Sparsity level t Recall that the sparsity level t relates to the number of nonzero upper-diagonal coeﬃcients
of Θ0 through the relationship

ktrue =

t

(cid:22)

p(p − 1)
2

(cid:23)

.

From Section 4.4, we observed that the separation Problem (9) is increasingly harder to solve as t increases.

grows exponentially with ktrue as long as ktrue (cid:54) p(p−1)

In addition, the combinatorics of the master Problem (8) also increases with t, since the size of the feasible set
(i.e., t (cid:54) 0.5). Figure 5 represents accuracy
S ktrue
p
and false detection rate as t increases, for all methods, using negative log-likelihood as a cross-validation
criterion. We report negative log-likelihood and results with BIC1/2 as the cross-validation criterion in
Appendix D (Figures 8 and 9 respectively).

4

19

0.51.01.52.02.53.0 n/p0.50.60.70.80.91.0Accuracy AMBGlassoBig MRidge0.51.01.52.02.53.0 n/p0.00.20.40.6False detection rate FDRMBGlassoBig MRidge(a) Accuracy A vs. t.

(b) False detection rate F DR vs. t.

Figure 5: Impact of the sparsity level t on support recovery. Results are averaged over 10 instances with
p = 200, n = p. Hyper-parameters are tuned using the out-of-sample negative log-likelihood.

Dimension p For n/p and t ﬁxed, the sparse precision matrix estimation problem should not be statistically

more diﬃcult as p increases, but computationally more expensive. We report results in Appendix D. Figures

10 and 11 report resulting accuracy and false detection rate as p increases, using negative log-likelihood
and BIC1/2 respectively as a cross-validation criterion. Figure 12 reports the impact of p on out-of-sample
Interestingly, the big-M formulation is harder to
negative log-likelihood, Figure 13 the impact on time.

scale than the ridge regularization, due to the additional constraints. As a result, fewer cuts were generated
within the 5-minute time limit and the resulting precision matrix shows a diﬀerent accuracy/false detection

trade-oﬀ with relatively poorer out-of-sample log-likelihood as p increases.

5.2 Analysis of a Breast Cancer Dataset

We apply our method on a real breast cancer dataset analyzed in [34]. The dataset can be found at
http://bioinformatics.mdanderson.org/. The dataset consists of 22,283 gene expression levels for 133
patients, including 34 with pathological complete response (pCR) and 99 with residual disease (RD). The

pCR subjects are considered to have a high chance of cancer-free survival in the long term, and thus it is of

interest to study the response states of the patients (pCR or RD) to preoperative chemotherapy. The main

objective of this analysis is to estimate the inverse covariance matrix of the gene expression levels and then

apply linear discriminant analysis (LDA) to predict whether or not a subject can achieve the pCR state.

The dataset has been studied in [21] using Glasso, revised Glasso, and SCAD. Later the same analysis

was performed with the CLIME estimator [11]. For the sake of consistency, we perform the same analysis,

but use our method to estimate inverse covariance matrices when needed. We ﬁrst brieﬂy describe how the

data is prepared and analyzed. We then present our results and compare with known results in [21, 11].

The data is ﬁrst randomly divided into testing and training sets using stratiﬁed sampling. 5 pCR subjects

and 16 RD subjects are randomly chosen to constitute the testing data. The remaining 112 subjects are

chosen to constitute the training data. This process is repeated 100 times and the following data preparation

techniques are used on each of the 100 instances of the training and testing data. A two-sample t-test is
performed between the two groups in the training dataset to determine the most signiﬁcant genes; we retain

the 113 genes with the smallest p-values as the variables for prediction and the rest are discarded. The data

for each variable (gene) is then standardized by dividing the data with the corresponding standard deviation,

estimated from the training dataset.

20

103.0102.5102.0101.5 t (log scale)0.60.70.80.91.0Accuracy AMBGlassoBig MRidge103.0102.5102.0101.5 t (log scale)0.00.10.20.30.40.50.6False detection rate FDRMBGlassoBig MRidgeWe next perform the linear discriminant analysis. We assume the normalized gene expression data are
normally distributed as N (µk, Σ), where the two groups have the same covariance Σ, but diﬀerent means,
µk (k = 1 for pCR and k = 2 for RD). The linear discriminant scores are as follows:

δk(x) = x(cid:62) ˆΣ−1 ˆµk −

1
2

ˆµ(cid:62)
k

ˆΣ−1 ˆµk + log πk,

where πk = nk/n is the proportion of the number of observations in the training data belonging to class k,
and the classiﬁcation rule is given by argmaxk δk(x). Based on each training dataset, we estimate the mean
ˆµk as,

ˆµk =

1
nk

(cid:88)

i∈class−k

xi

for k = 1, 2,

and the precision matrix ˆΣ−1 using the cardinality constrained problem. Since the sample size is less than
the dimension of the matrix, the empirical covariance is not invertible and can not be used in LDA.

Comparison Metrics

Description

Speciﬁcity

Sensitivity

MCC

T N
T N +F P

T P
T P +F N

√

T P ×T N −F P ×F N

(T P +F P )(T P +F N )(T N +F P )(T N +F N )

Table 3: Metrics used for prediction performance comparison for the breast cancer dataset. TP, TN, FP,
and FN are the number of true positives, true negatives, false positives and false negatives, respectively.
Positives correspond to pCR subjects and negatives correspond to RD subjects.

The classiﬁcation performance of δk is clearly associated with the estimation performance of ˆΣ−1. Let
true positive (TP) be the number of pCR subjects δk identiﬁes as pCR subjects and let true negative (TN)
be the number of RD subjects δk identiﬁes as RD Subjects. To compare prediction performance, we use
comparison metrics: speciﬁcity, sensitivity, and also Matthews Correlation Coeﬃcient (MCC). They are each

deﬁned in Table 3. MCC is widely used in machine learning for assessing the quality of a binary classiﬁer; it

takes true and false, positives and negatives, into account and is generally regarded as a balanced measure.

A larger MCC value indicates a better classiﬁer [21].

Method

Speciﬁcity

Sensitivity

MCC

NNZ

Glasso
Adaptive Lasso
SCAD
CLIME
big-M
Ridge

0.768 (0.009)
0.787 (0.009)
0.794 (0.009)
0.749 (0.009)
0.779 (0.011)
0.775 (0.011)

0.630 (0.021)
0.622 (0.022)
0.634 (0.022)
0.806 (0.017)
0.717 (0.019)
0.716 (0.020)

0.366 (0.018)
0.381 (0.018)
0.402 (0.020)
0.506 (0.020)
0.460 (0.019)
0.453 (0.021)

3923 (2)
1233 (1)
674 (1)
492 (7)
436 (3)
427 (3)

Table 4: Comparison of estimators on the breast cancer dataset. Data for Glasso, revised Glasso and SCAD
is from [21] and data for CLIME is from [11]. Average performance is reported on 100 instances of training
and testing data; standard deviations are included in parentheses. NNZ refers to the number of nonzero
entries in the estimate.

We perform the LDA for each of the 100 instances and report a summary of average performance in

21

Table 4. For each experiment, we calibrate the parameters k and M / γ using the extended Bayesian

information criterion on the training data. We observe that our proposed methods outperform Lasso-based

methods on all aspects. Our discrete optimization formulations are comparable to SCAD and Clime, yet

not dominated nor dominating by either of the two. Big-M and ridge formulations improve over SCAD in

terms of sensitivity and MCC, and over Clime in terms of speciﬁcity. On the contrary, SCAD ranks ﬁrst on
speciﬁcity and Clime on sensitivity and MCC. However, the biggest advantage of discrete formulations over

the others is that they produce sparser estimates. This is especially desirable in the context of graphical

models, when it is desirable to induce sparsity for explanatory and predictive power.

6 Extension to graphical model estimation with structural infor-

mation

In this section, we illustrate the modeling power of our mixed-integer formulation.

In graphical models

estimation, it is not unusual to have some information or intuition about the correlation structure between

variables [17], information which can easily be encoded in our framework by additional constraints on the

binary variables Z.

Sparsity In this paper, we focused on imposing sparsity on the precision matrix Θ. This requirement

translates into the linear constraint

Zij (cid:54) k.

(cid:88)

i>j

Partial knowledge of the support

In some settings, the modeler has some partial knowledge of the

correlation structure and can inform the optimization problem through the additional constraints

Zij = 0, if (i, j) ∈ S0,

Zij = 1, if (i, j) ∈ S1,

where S0 (resp. S1) is a set of indices for which Θijs are known to be 0 (resp. (cid:54)= 0).

Degree Information about the degree of each variable in the underlying structure (or graph) might also

be relevant [44]. In a protein contact graph for example, the degree of each node is upper bounded by some
constant. With our framework, the degree of any variable i is given by di := (cid:80)
j>i Zij, so that adding the
linear constraints

would enforce lower ((cid:96)i) and upper (ui) bounds on the node degrees. In a more ﬂexible fashion,

(cid:96)i (cid:54) di (cid:54) ui, ∀i

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
p

(cid:88)

i

di − d

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:54) (cid:15),

requires the average node degree to be within (cid:15) from a given target d. Similarly, quadratic constraints could

be added in order to match second moments. Finally, many real-world networks, including the network of

22

webpages or some gene regulatory networks, involve nodes which have a lot more edges than the others [59].
Our framework can account for such hubs by introducing additional binary variables yi, i = 1, . . . , p and
adding the following constraints

di (cid:54) dlow + (dhigh − dlow)yi, ∀i,

yi (cid:54) m,

(cid:88)

i

where dhigh (resp. dlow) is the maximum degree of a hub (resp. non-hub) node and m is an upper-bound
on the total number of hubs in the network.

Tree structure Finally, tree-structured graphical models have been extensively studied in the literature
i,j for all
ordered triples (i, j, k) of pairwise diﬀerent nodes, [46] provided an extended formulation for a spanning tree:

[14] for they are sparse and allow eﬃcient inference.

Introducing additional binary variables yk

(cid:107)Z(cid:107)0 = p − 1,

ij + yk
yk
ji = Zij,
(cid:88)
yk
ij = 1 − Zik

j:j /∈{i,k}

∀i, j = 1, . . . , p, i < p, ∀k = 1, . . . , p,

∀i, k = 1, . . . , p, i < k,

ij = 1 if and only if the edge (i, j) is contained in the tree and k is in the component of j when

where yk
removing (i, j) from the tree.

7 Summary

In this work, we use a variety of modern optimization methods to provide the ﬁrst provably exact algorithm

for solving the cardinality-constrained negative log-likelihood Problem (3). Through the unifying lens of

regularization, we show that the well known big-M constraints are not only a formulation technique but more

importantly a smoothing procedure. On that matter, ridge regularization can be considered as a fruitful

alternative. Our cutting-plane approach has the additional beneﬁt of treating separately the combinatorial

aspect of the problem from the SDP component of it. The method provides provably optimal solutions,

and delivers near optimal solutions in minutes for p in the 1, 000s and sparsity level of the order of 1%.

Computational experiments on both synthetic and real data show that such discrete formulations deliver

solutions with increased out-of-sample predictive power and lower false detection rate than existing methods,
while being as accurate.

A Proofs of Theorem 2 and corollaries

In this section, we detail the proof of Theorem 2. We ﬁrst specify the assumptions required on the regularizer

Ω, prove Theorem 2 and ﬁnally investigate some special cases of interest.

23

A.1 Assumptions

We ﬁrst assume that the function Ω is decomposable, i.e., there exist scalar functions Ωij such that

∀ Φ, Ω(Φ) =

(cid:88)

i,j

Ωij(Φij).

In addition, we assume that for all (i, j), Ωij is convex and tends to regularize towards zero. Formally,

∀ (i, j), min

x

Ωij(x) = Ωij(0).

(A1)

(A2)

Those ﬁrst two assumptions are not highly restrictive and are satisﬁed by (cid:96)∞-norm constraint (big-M ),
(cid:96)1-norm regularization (LASSO) or (cid:107) · (cid:107)2

2-regularization, among others.
For any function f , we denote with a superscript (cid:63) its Fenchel conjugate [?, see]chap. 3.3]boyd2004convex

deﬁned as

In particular, the Fenchel conjugate of any function f is convex. Given Assumption (A1),

f (cid:63)(y) := sup
x

(cid:104)x, y(cid:105) − f (x).

(cid:104)Φ, R(cid:105) − Ω(Φ),

Ω(cid:63)(R) = sup
Φ
(cid:88)

=

ΦijRij − Ωij(Φij),

sup
Φij

Ω(cid:63)

ij(Rij).

=

i,j
(cid:88)

i,j

As a result, it is easy to see that if Ω satisﬁes (A1) and (A2), so does its Fenchel conjugate.

Let us denote A ◦ B the Hadamard or component-wise product between matrices A and B. Consider a
matrix R and a support matrix Z ∈ {0, 1}p×p. The function Z (cid:55)→ Ω(cid:63)(Z ◦ R) is convex in Z, by convexity of
Ω(cid:63). We now assume that it is linear in Z, that is, there exists a function Ω(cid:63) : Rp×p → Rp×p satisfying:

∀ Z ∈ {0, 1}p×p, ∀ R ∈ Rp×p, Ω(cid:63)(Z ◦ R) = (cid:104)Z, Ω(cid:63)(R)(cid:105).

(A3)

A.2 Proof of Theorem 2

Given Z ∈ {0, 1}p×p such that Zii = 1 for all i = 1, . . . , p, we ﬁrst prove that under assumptions (A1) and
(A2):

˜h(Z) := min
Θ(cid:31)0

(cid:104)Σ, Θ(cid:105) − log det Θ + Ω(Θ)

s.t. Θij = 0 if Zij = 0 ∀(i, j),

=

max
R:Σ+R(cid:31)0

p + log det(Σ + R) − Ω(cid:63)(Z ◦ R).

Then, Assumption (A3) will conclude the proof.

24

Proof. We decompose the minimization problem `a la Fenchel.

˜h(Z) = min
Θ(cid:31)0

(cid:104)Σ, Θ(cid:105) − log det Θ + Ω(Θ) s.t. Θij = 0 if Zij = 0,

= min

Θ(cid:31)0,Φ

= min

Θ(cid:23)0,Φ

(cid:104)Σ, Θ(cid:105) − log det Θ + Ω(Z ◦ Φ) s.t. Θij = ZijΦij,

(cid:104)Σ, Θ(cid:105) − log det Θ + Ω(Z ◦ Φ) s.t. Θ = Z ◦ Φ.

In the last equality, we omitted the constraint Θ (cid:31) 0, which is implied by the domain of log det. Assuming

(A1) and (A2) hold, the regularization term Ω(Z ◦ Φ) can be replaced by Ω(Φ) and

˜h(Z) = min
Θ(cid:23)0,Φ

(cid:104)Σ, Θ(cid:105) − log det Θ + Ω(Φ) s.t. Θ = Z ◦ Φ.

The above objective function is convex in (Θ, Φ), the feasible set is a non-empty - Θ = Φ = Ip is feasible -
convex set, and Slater’s conditions are satisﬁed. Hence, strong duality holds.

˜h(Z) = min
Θ(cid:23)0,Φ

(cid:104)Σ, Θ(cid:105) − log det Θ + Ω(Φ) s.t. Θ = Z ◦ Φ,

= min

Θ(cid:23)0,Φ

max
R

(cid:104)Σ, Θ(cid:105) − log det Θ + Ω(Φ) + (cid:104)Θ − Z ◦ Φ, R(cid:105),

= max

R

min
Θ(cid:23)0

(cid:2)(cid:104)Σ + R, Θ(cid:105) − log det Θ(cid:3) + min

Φ

[Ω(Φ) − (cid:104)Z ◦ Φ, R(cid:105)] .

For the ﬁrst inner-minimization problem, ﬁrst-order conditions Σ + R − Θ−1 = 0 lead to the constraint
Σ + R (cid:31) 0 and the objective value is p + log det(Σ + R). The second inner-minimization problem is almost

the deﬁnition of the Fenchel conjugate:

min
Φ

Hence,

Ω(Φ) − (cid:104)Z ◦ Φ, R(cid:105) = − max

(cid:104)Φ, Z ◦ R(cid:105) − Ω(Φ),

Φ
= −Ω(cid:63)(Z ◦ R)

h(Z) = max

p + log det(Σ + R) − Ω(cid:63)(Z ◦ R).

R:Σ+R(cid:31)0

Remark: Notice that we proved that ˜h(Z) could be written as point-wise maximum of concave functions
of Z. Assumption (A3) is needed to ensure that the function in the maximization is convex in Z at the same

time.

25

A.3 Special Cases and Corollaries

A.3.1 No regularization

We ﬁrst consider the unregularized case of (6) where ∀ Φ, Ω(Φ) = 0. Assumptions (A1) and (A2) are

obviously satisﬁed. Moreover, for any R,

Ω(cid:63)(R) = sup
Φ

(cid:104)Φ, R(cid:105) =


0


if R = 0,



+∞ otherwise.

With the convention that 0 × ∞ = 0, Assumption (A3) is satisﬁed and Theorem 2 holds:

h(Z) =

max
R:Σ+R(cid:31)0

p + log det(Σ + R) − (cid:104)Z, Ω(cid:63)(R)(cid:105),

= max

p + log det(Σ + R)

R:Σ+R(cid:31)0

s.t. ZijRij = 0, ∀(i, j).

In particular, this reformulation proves that h(Z) is convex3, but that the coordinates of its sub-gradient
−Ω(cid:63)(R(cid:63)(Z)) are either 0 or −∞, hence uninformative. Note that the same conclusion is true for (cid:96)1-
regularization.

From the proof of Theorem 2, one can derive a lower bound on (cid:107)Θ(cid:63)(cid:107)∞ which will be useful for big-M

regularization.

Theorem 3. The solution of (8) satisﬁes (cid:107)Θ(cid:63)(cid:107)∞ (cid:62) p

(cid:107)Σ(cid:107)1

Proof. For a feasible support Z, denote the optimal primal and dual variables Θ(cid:63)(Z) and R(cid:63)(Z) respectively.
There is no duality gap and KKT condition Θ(cid:63)(Z)−1 = Σ + R(cid:63)(Z) holds, so that (cid:104)Σ, Θ(cid:63)(Z)(cid:105) = p. From
H¨older’s inequality, we obtain the desired lower bound.

A.3.2 Big-M regularization

For the big-M regularization,


0


Ω(Θ) =

if |Θij| (cid:54) Mij,

,



+∞ otherwise

is decomposable with Ωi,j(Θij) = 0 if |Θij| (cid:54) Mij, +∞ otherwise. Assumptions (A1) and (A2) are satisﬁed.
Moreover, for any R,

Ω(cid:63)(R) =

sup
Φ : (cid:107)Φ(cid:107)∞(cid:54)M

(cid:104)Φ, R(cid:105) = (cid:107)M ◦ R(cid:107)1.

In particular, for any binary matrix Z,

Ω(cid:63)(Z ◦ R) =

(cid:88)

i,j

|MijZijRij| =

(cid:88)

i,j

MijZij|Rij|,

so that Assumption (A3) is satisﬁed with Ω(cid:63)(R) = (Mij|Rij|)ij.

3Convexity of h(Z) can also be proved from the primal formulation (6) directly. Take two matrices Z1 and Z2, λ ∈ (0, 1),

Z := λZ1 + (1 − λ)Z2, then it follows from the deﬁnition (6) that h(Z) (cid:54) λh(Z1) + (1 − λ)h(Z2).

26

A.3.3 Ridge regularization

For the (cid:96)2

2-regularization,

Ω(Θ) =

1
2γ

(cid:107)Θ(cid:107)2
2,

is decomposable with Ωi,j(Θij) = 1

2γ Θ2

ij. Assumptions (A1) and (A2) are satisﬁed. Moreover, for any R,

Ω(cid:63)(R) = sup
Φ

(cid:104)Φ, R(cid:105) −

1
2γ

(cid:107)Φ(cid:107)2

2 =

γ
2

(cid:107)R(cid:107)2
2

In particular, for any binary matrix Z,

Ω(cid:63)(Z ◦ R) =

γ
2

(cid:88)

(ZijRij)2 =

i,j

γ
2

(cid:88)

i,j

ZijR2
ij,

since Z 2

ij = Zij, so that Assumption (A3) is satisﬁed with Ω(cid:63)(R) = (cid:0) γ

2 R2

ij

(cid:1)

.

ij

Moreover, from the proof of Theorem 2, one can connect the norm of Θ(cid:63)(Z) and γ.

Theorem 4. For any support Z, the norm of the optimal precision matrix Θ(cid:63)(Z) is bounded by

(cid:32)(cid:115)

(cid:107)Σ(cid:107)2

1 +

γ
2

(cid:33)

− 1

4p
γ(cid:107)Σ(cid:107)2
2

(cid:54) (cid:107)Θ(cid:63)(Z)(cid:107)2 (cid:54) √

pγ.

Proof. There is no duality gap:

(cid:104)Σ, Θ(cid:63)(Z)(cid:105) − log det Θ(cid:63)(Z) +

1
2γ

(cid:107)Φ(cid:63)(Z)(cid:107)2

2 = p + log det(Σ + R(cid:63)(Z)) +

γ
2

(cid:107)Z ◦ R(cid:63)(Z)(cid:107)2
2.

In addition, the following KKT conditions hold

Θ(cid:63)(Z)−1 = Σ + R(cid:63)(Z),

Φ(cid:63)(Z) = γZ ◦ R(cid:63)(Z),

where the second condition follows from the inner minimization problem deﬁning Ω(cid:63). All in all, we have

(cid:104)Σ, Θ(cid:63)(Z)(cid:105) +

1
γ

(cid:107)Φ(cid:63)(Z)(cid:107)2

2 = p.

Since Σ and Θ(cid:63)(Z) are semi-deﬁnite positive matrices, (cid:104)Σ, Θ(cid:63)(Z)(cid:105) (cid:62) 0. Hence,

(cid:107)Φ(cid:63)(Z)(cid:107)2 (cid:54) √

pγ.

To obtain the lower bound, we apply Cauchy-Schwartz inequality (cid:104)Σ, Θ(cid:63)(Z)(cid:105) (cid:54) (cid:107)Σ(cid:107)2(cid:107)Θ(cid:63)(Z)(cid:107)2 and solve
the quadratic equation

1
γ

(cid:107)Φ(cid:63)(Z)(cid:107)2

2 + (cid:107)Σ(cid:107)2(cid:107)Θ(cid:63)(Z)(cid:107)2 − p (cid:62) 0.

In particular, the lower bound in Theorem 4 is controlled by the factor

4p
γ(cid:107)Σ(cid:107)2
2

, suggesting an appropriate

27

scaling of γ to start a grid search with.

B An optimization approach for ﬁnding big-M values

In this section, we present a method for obtaining suitable constants M. The approach involves solving two

optimization problems for each oﬀ-diagonal entry of the matrix being estimated. The problems provide lower

and upper bounds for each entry of the optimal solution. First we present the problems, then we discuss

how they are solved.

B.1 Bound Optimization Problems

Let ˆΘ be a feasible solution for (3) and deﬁne,

A simple way to obtain lower bounds for the ijth entry of the optimal solution is to solve

u := (cid:104) ˆΘ, Σ(cid:105) − log det ˆΘ.

min
Θ(cid:31)0

s.t.

Θij

(cid:104)Σ, Θ(cid:105) − log det Θ (cid:54) u.

Likewise, to obtain upper bounds we solve

max
Θ(cid:31)0

s.t.

Θij

(cid:104)Σ, Θ(cid:105) − log det Θ (cid:54) u.

(11)

(12)

Note that it is suﬃcient to ﬁnd a feasible solution ˆΘ to formulate (11) and (12), and a feasible solution

with a smaller value leads to better bounds.

B.2 Solution Approach

We describe the approach for the lower bound Problem (11) only, the upper bound Problem (12) being

similar.

First, we make the additional assumption that Σ is invertible. We know this assumption cannot hold in

the high dimensional setting where p > n. Numerically, one can always argue that the lowest eigenvalues

of Σ are never exactly equal to zero but should be strictly positive. In this case however, these eigenvalues

should be small and close to machine precision, making matrix inversion very unstable. Note that this extra

assumption is required for problems (11) and (12) to be bounded.

Problem (11) is a semideﬁnite optimization problem and there are p(p + 1)/2 entries to bound so it is

necessary to eﬃciently solve (11) and avoid solving so many SDPs. Instead, one can solve the dual of (11)

very eﬃciently. Note an advantage for considering the dual is we do not need to solve the problem to

optimality to obtain a valid bound. Using basic arguments from convex duality theory similar to the ones

28

invoked in Section A.2, the dual problem for (11) writes

(cid:26)

(cid:18)

max
λ>0

λ

p − u + log det

(cid:18) 1
2λ

(eieT

j + ejeT

i ) + Σ

(cid:19)(cid:19)(cid:27)

(13)

Computationally, problem (13) is easier to solve because it is a convex optimization problem with a scalar

decision variable λ.

Denote g(λ) the objective function in the dual Problem (13). Algebraic manipulations yield

(cid:20)

g(λ) := λ

p − u + log det

(cid:18) 1
2λ

(cid:34)

(eieT

j + ejeT
(cid:32)

(cid:19)(cid:21)

,

i ) + Σ

= λ

p − u + log det(Σ) + log

1 +

+

Θ2

ij − ΘiiΘjj
4λ2

(cid:33)(cid:35)

,

Θij
λ

where Θ = Σ

−1

. We can then easily derive the ﬁrst and second derivatives of g and apply Newton’s method

to solve Problem (13).

C Additional material on computational performance of the cutting-

plane algorithm

In this section, we consider the runtime of the cutting-plane algorithm on synthetic problems as in Section

5.1. In Section 5.1.1, we illustrated how the regularization parameter M or γ can impact the convergence of

the cutting-plane algorithm, so we focus in this section on the impact of the problem sizes n, p and k.

In particular, we study the time needed by the algorithm to ﬁnd the optimal solution (opt-time) and to

verify the solution’s optimality (ver-time), as well as the number of cuts required (laz-cons). We carry out
all experiments by generating 10 instances of synthetic data4 for (p, ktrue) ∈ {30, 50, 80, 120, 200} × {5, 10}
and diﬀerent values of n. We solve each instance of (8) with big-M regularization for k = ktrue, M = 0.5
and report average performance in Table 5. These computations are performed on 4 Intel E5-2690 v4 2.6

GHz CPUs (14 cores per CPU, no hyper threading) with 16GB of RAM in total. We chose to ﬁx the value

of M = 0.5 in order to isolate the impact of p, k and n on computational time, the speciﬁc value 0.5 being

informed by the knowledge of the ground truth.

In general the algorithm provides an optimal solution in a matter of seconds, and a certiﬁcate of optimality

in seconds or minutes even for p in the 100s. Optimal veriﬁcation occurs signiﬁcantly quicker when the sample

size n is larger because the sparsity pattern of the underlying matrix is easier to recover. However, we note

that ﬁnding the optimal solution is not as aﬀected by the sample size n. As p or k increase, optimal detection

also does not signiﬁcantly change, but optimal veriﬁcation generally becomes signiﬁcantly harder. Similar

observations have been made for mixed-integer formulations of the best subset selection problem in linear

and logistic regression [7]. We also observe that changes in k have a more substantial impact on the runtime

than changes in n or p, especially when p is large. Finally, Meinshausen and B¨uhlmann’s approximation is

used as a warm-start and we observe that is often optimal, especially when n/p is large.

Thus, the cutting-plane algorithm in general provides an optimal or near-optimal solution fast, but

optimal veriﬁcation strongly depends on p, k, and n. Nonetheless, we observe that optimality of solutions

4For each instance, we generate a sparse precision matrix Θ0 as in Section 5.1 and n samples from the corresponding

multivariate normal distribution

29

can be veriﬁed for p in the 100s and k in the 10s in a matter of minutes.

p

ktrue

n

ver-time

opt-time

cut-time

laz-cons

30

5

30

10

50

5

50

10

80

5

80

10

120

5

120

10

200

5

200

10

200
150
100
300
250
200

200
150
100
300
250
200

200
150
100
300
250
200

200
150
100
300
250
200

200
150
100
300
250
200

2.37 (2.13)
6.33 (7.34)
30.7 (47.96)
31.11 (23.31)
35.13 (28.89)
33.7 (24.23)

9.59 (9.06)
29.43 (20.28)
183.7 (243.73)
24.19 (20.29)
31.37 (18.48)
40.38 (29.27)

70.12 (106.16)
179.76 (175.22)
988.9 (763.05)
37.83 (9.17)
71.4 (24.51)
161.8 (74.35)

152.54 (113.42)
713.45 (712.74)
1793.67 (445.58)
238.7 (150.61)
704.43 (568.93)
1379.58 (666.52)

858.4 (770.03)
1453.51 (614.68)
2000.28 (0.42)
934.55 (428.66)
1792.1 (353.35)
2000.47 (0.9)

0.0 (0.0)
0.0 (0.0)
0.0 (0.0)
5.05 (10.69)
11.2 (13.13)
7.75 (12.34)

0.0 (0.0)
0.0 (0.0)
0.0 (0.0)
0.0 (0.0)
0.0 (0.0)
0.55 (1.73)

0.0 (0.0)
0.0 (0.0)
0.0 (0.0)
0.0 (0.0)
0.0 (0.0)
9.87 (31.2)

34.89 (110.34)
251.25 (543.17)
646.84 (827.53)
0.0 (0.0)
0.0 (0.0)
0.0 (0.0)

418.1 (496.15)
515.58 (548.82)
917.42 (596.49)
337.16 (442.36)
354.84 (362.0)
571.71 (571.04)

1.95 (1.74)
2.71 (3.14)
14.46 (28.55)
14.32 (9.91)
19.93 (14.91)
15.35 (11.15)

5.23 (3.66)
18.49 (12.98)
99.36 (118.0)
12.57 (10.37)
15.2 (9.46)
26.14 (19.14)

51.56 (80.18)
127.19 (110.85)
482.83 (277.33)
30.33 (10.11)
47.06 (13.24)
105.48 (41.14)

119.24 (99.43)
480.18 (407.96)
1135.33 (320.83)
172.75 (99.92)
396.44 (238.16)
675.81 (248.96)

662.22 (567.77)
1023.24 (380.82)
1427.69 (139.69)
646.12 (255.69)
1062.81 (205.64)
1198.26 (109.66)

28 (17.9)
55 (55.8)
258 (472.6)
265 (176.6)
296 (204.8)
290 (196.5)

42 (25.2)
153 (107.0)
788 (937.8)
98 (80.8)
122 (77.8)
210 (149.1)

154 (212.2)
404 (348.3)
1581 (990.9)
85 (25.2)
139 (36.3)
309 (121.6)

170 (108.9)
740 (648.4)
1671 (412.7)
224 (116.4)
560 (348.5)
909 (393.1)

398 (335.0)
723 (271.4)
1024 (90.6)
368 (141.1)
657 (167.6)
763 (104.5)

Table 5: Average performance on instances of synthetic data with k = ktrue. All problems are solved to
a tolerance gap of 10−4, where the tolerance gap is the percentage diﬀerence between the ﬁnal lower and
upper bounds. Title ver-time and opt-time refer to the time (in seconds) it takes to verify optimality and
to ﬁnd the optimal solution respectively, cut-time refers to the amount of time spent solving the separation
problems, and laz-cons refers to the number of lazy constraints generated. We report average time over 10
random instances (and standard deviation).

D Additional comparisons on statistical performance

We report here additional results from the experiments conducted in Section 5.1.

30

D.1 Comparisons for varying sample sizes n/p

(a) BIC1/2 as a CV criterion.

(b) −LL as a CV criterion.

Figure 6: Impact of the number of samples n/p on out-of-sample negative log-likelihood. Results are averaged
over 10 instances with p = 200, t = 1%.

(a) Accuracy A vs. n/p.

(b) False detection rate F DR vs. n/p.

Figure 7: Impact of the number of samples n/p on support recovery. Results are averaged over 10 instances
with p = 200, t = 1%. Hyper-parameters are tuned using the BIC1/2 criterion.

D.2 Comparisons for varying sparsity levels t

(a) BIC1/2 as a CV criterion.

(b) −LL as a CV criterion.

Figure 8: Impact of the sparsity level t on out-of-sample negative log-likelihood. Results are averaged over
10 instances with p = 200, n = p.

31

0.51.01.52.02.53.0 n/p140160180200220Out-of-sample LLMBGlassoBig MRidge0.51.01.52.02.53.0 n/p140150160170180190200Out-of-sample LLMBGlassoBig MRidge0.51.01.52.02.53.0 n/p0.20.40.60.81.0Accuracy AMBGlassoBig MRidge0.51.01.52.02.53.0 n/p0.00.10.20.30.40.5False detection rate FDRMBGlassoBig MRidge103.0102.5102.0101.5 t (log scale)140160180200Out-of-sample LLMBGlassoBig MRidge103.0102.5102.0101.5 t (log scale)140160180200Out-of-sample LLMBGlassoBig MRidge(a) Accuracy A vs. t.

(b) False detection rate F DR vs. t.

Figure 9: Impact of the sparsity level t on support recovery. Results are averaged over 10 instances with
p = 200, n = p. Hyper-parameters are tuned using the BIC1/2 criterion.

D.3 Comparisons for varying dimensions p

(a) Accuracy A vs. p.

(b) False detection rate F DR vs. p.

Figure 10: Impact of the dimension p on support recovery. Results are averaged over 10 instances with
n = p, t = 1%. Hyper-parameters are tuned using −LL.

(a) Accuracy A vs. p.

(b) False detection rate F DR vs. p.

Figure 11: Impact of the dimension p on support recovery. Results are averaged over 10 instances with
n = p, t = 1%. Hyper-parameters are tuned using the BIC1/2 criterion.

32

103.0102.5102.0101.5 t (log scale)0.40.60.81.0Accuracy AMBGlassoBig MRidge103.0102.5102.0101.5 t (log scale)0.00.10.20.30.40.50.6False detection rate FDRMBGlassoBig MRidge25050075010001250 p (log scale)0.800.850.900.951.00Accuracy AMBGlassoBig MRidge25050075010001250 p (log scale)0.10.20.30.40.50.6False detection rate FDRMBGlassoBig MRidge25050075010001250 p (log scale)0.800.850.900.951.00Accuracy AMBGlassoBig MRidge25050075010001250 p (log scale)0.10.20.30.40.50.6False detection rate FDRMBGlassoBig MRidge(a) BIC1/2 as a CV criterion.

(b) −LL as a CV criterion.

Figure 12: Impact of the dimension p on out-of-sample negative log-likelihood. Results are averaged over 10
instances with n = p, t = 1%.

(a) BIC1/2 as a CV criterion.

(b) −LL as a CV criterion.

Figure 13: Impact of the dimension p on computational time. Results are averaged over 10 instances with
n = p, t = 1%. Recall that discrete formulations big-M and ridge are stopped after 5 minutes.

References

[1] Alper Atamt¨urk and Vishnu Narayanan. Conic mixed-integer rounding cuts. Mathematical Program-

ming, 122(1):1–20, 2010.

[2] Yves F Atchad´e, Rahul Mazumder, and Jie Chen. Scalable computation of regularized precision matrices

via stochastic optimization. arXiv preprint arXiv:1509.00426, 2015.

[3] Onureena Banerjee, Laurent El Ghaoui, and Alexandre dAspremont. Model selection through sparse

maximum likelihood estimation for multivariate gaussian or binary data. Journal of Machine learning

research, 9(Mar):485–516, 2008.

[4] Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust optimization. Princeton University

Press, 2009.

[5] Dimitris Bertsimas, David B Brown, and Constantine Caramanis. Theory and applications of robust

optimization. SIAM review, 53(3):464–501, 2011.

33

25050075010001250 p (log scale)2004006008001000Out-of-sample LLMBGlassoBig MRidge25050075010001250 p (log scale)2004006008001000Out-of-sample LLMBGlassoBig MRidge25050075010001250 p (log scale)100101102103Time(inseconds)MBGlassoBig MRidge25050075010001250 p (log scale)100101102103Time(inseconds)MBGlassoBig MRidge[6] Dimitris Bertsimas and Martin S Copenhaver. Characterization of the equivalence of robustiﬁcation and

regularization in linear and matrix regression. European Journal of Operational Research, 270:931942,

2018.

[7] Dimitris Bertsimas, Angela King, and Rahul Mazumder. Best subset selection via a modern optimization

lens. The Annals of Statistics, 44(2):813–852, 2016.

[8] Dimitris Bertsimas and Rahul Mazumder. Least quantile regression via modern optimization. The

Annals of Statistics, pages 2494–2525, 2014.

[9] Dimitris Bertsimas and Bart Van Parys. Sparse high-dimensional regression: Exact scalable algorithms

and phase transitions. arXiv preprint arXiv:1709.10029, 2017.

[10] Peter J Bickel, Elizaveta Levina, et al. Covariance regularization by thresholding. The Annals of

Statistics, 36(6):2577–2604, 2008.

[11] Tony Cai, Weidong Liu, and Xi Luo. A constrained (cid:96)1 minimization approach to sparse precision matrix

estimation. Journal of the American Statistical Association, 106(494):594–607, 2011.

[12] Mehmet Tolga C¸ ezik and Garud Iyengar. Cuts for mixed 0-1 conic programming. Mathematical Pro-

gramming, 104(1):179–202, 2005.

[13] David Maxwell Chickering. Learning bayesian networks is np-complete. In Learning from data, pages

121–130. Springer, 1996.

[14] C Chow and Cong Liu. Approximating discrete probability distributions with dependence trees. IEEE

transactions on Information Theory, 14(3):462–467, 1968.

[15] Joachim Dahl, Lieven Vandenberghe, and Vwani Roychowdhury. Covariance selection for nonchordal

graphs via chordal embedding. Optimization Methods & Software, 23(4):501–520, 2008.

[16] Arthur P Dempster. Covariance selection. Biometrics, pages 157–175, 1972.

[17] Mathias Drton and Marloes H Maathuis. Structure learning in graphical modeling. Annual Review of

Statistics and Its Application, 4:365–393, 2017.

[18] Marco A Duran and Ignacio E Grossmann. An outer-approximation algorithm for a class of mixed-

integer nonlinear programs. Mathematical programming, 36(3):307–339, 1986.

[19] Noureddine El Karoui. High-dimensionality eﬀects in the markowitz problem and other quadratic

programs with linear constraints: Risk underestimation. The Annals of Statistics, 38(6):3487–3566,

2010.

[20] Jianqing Fan, Yingying Fan, and Jinchi Lv. High dimensional covariance matrix estimation using a

factor model. Journal of Econometrics, 147(1):186–197, 2008.

[21] Jianqing Fan, Yang Feng, and Yichao Wu. Network exploration via the adaptive lasso and scad penalties.

The annals of applied statistics, 3(2):521, 2009.

[22] Jianqing Fan, Fang Han, and Han Liu. Challenges of big data analysis. National science review,

1(2):293–314, 2014.

34

[23] Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its oracle

properties. Journal of the American statistical Association, 96(456):1348–1360, 2001.

[24] Jianqing Fan, Yuan Liao, and Han Liu. An overview of the estimation of large covariance and precision

matrices. The Econometrics Journal, 19(1), 2016.

[25] Jianqing Fan, Jingjin Zhang, and Ke Yu. Vast portfolio selection with gross-exposure constraints.

Journal of the American Statistical Association, 107(498):592–606, 2012.

[26] Salar Fattahi and Somayeh Sojoudi. Graphical lasso and thresholding: Equivalence and closed-form

solutions. arXiv preprint arXiv:1708.09479, 2017.

[27] Rina Foygel and Mathias Drton. Extended bayesian information criteria for gaussian graphical models.

In Advances in neural information processing systems, pages 604–612, 2010.

[28] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance estimation with the

graphical lasso. Biostatistics, 9(3):432–441, 2008.

[29] Tristan Gally, Marc E Pfetsch, and Stefan Ulbrich. A framework for solving mixed-integer semideﬁnite

programs. Optimization Methods and Software, 33(3):594–632, 2018.

[30] Arthur M Geoﬀrion. Generalized benders decomposition. Journal of optimization theory and applica-

tions, 10(4):237–260, 1972.

[31] Inc Gurobi Optimization. Gurobi optimizer reference manual. URL http://www. gurobi. com, 2015.

[32] Trevor Hastie, Robert Tibshirani, and Martin Wainwright. Statistical learning with sparsity: the lasso

and generalizations. CRC press, 2015.

[33] Christoph Helmberg and Franz Rendl. Solving quadratic (0, 1)-problems by semideﬁnite programs and

cutting planes. Mathematical programming, 82(3):291–315, 1998.

[34] Kenneth R Hess, Keith Anderson, W Fraser Symmans, Vicente Valero, Nuhad Ibrahim, Jaime A Mejia,

Daniel Booser, Richard L Theriault, Aman U Buzdar, Peter J Dempsey, et al. Pharmacogenomic

predictor of sensitivity to preoperative chemotherapy with paclitaxel and ﬂuorouracil, doxorubicin, and

cyclophosphamide in breast cancer. Journal of clinical oncology, 24(26):4236–4244, 2006.

[35] Cho-Jui Hsieh, Inderjit S Dhillon, Pradeep K Ravikumar, and M´aty´as A Sustik. Sparse inverse covari-

ance matrix estimation using quadratic approximation. In Advances in neural information processing

systems, pages 2330–2338, 2011.

[36] Cho-Jui Hsieh, M´aty´as A Sustik, Inderjit S Dhillon, Pradeep K Ravikumar, and Russell Poldrack. Big

& quic: Sparse inverse covariance estimation for a million variables. In Advances in neural information

processing systems, pages 3165–3173, 2013.

[37] IBM

ILOG.

Cplex

optimizer.

Available:

http://www-01.

ibm.

com/software/commerce/optimization/cplex-optimizer, 2012.

[38] Vijay Krishnamurthy, Selin Damla Ahipasaoglu, and Alexandre dAspremont. A pathwise algorithm for

covariance selection. Optimization for Machine Learning, page 479, 2011.

35

[39] Cliﬀord Lam and Jianqing Fan. Sparsistency and rates of convergence in large covariance matrix

estimation. Annals of statistics, 37(6B):4254, 2009.

[40] Steﬀen L Lauritzen. Graphical models, volume 17. Clarendon Press, 1996.

[41] Zhenqiu Liu, Shili Lin, Nan Deng, Dermot PB McGovern, and Steven Piantadosi. Sparse inverse covari-
ance estimation with (cid:96)0 penalty for network construction with omics data. Journal of Computational
Biology, 23(3):192–202, 2016.

[42] Miles Lubin and Iain Dunning. Computing in operations research using julia. INFORMS Journal on

Computing, 27(2):238–248, 2015.

[43] Miles Lubin, Emre Yamangil, Russell Bent, and Juan Pablo Vielma. Polyhedral approximation in

mixed-integer convex optimization. Mathematical Programming, 172(1-2):139–168, 2018.

[44] Jianzhu Ma, Feng Zhao, and Jinbo Xu. Structure learning constrained by node-speciﬁc degree distri-

bution. In UAI, pages 533–541, 2015.

[45] Goran Marjanovic and Alfred O Hero. (cid:96)0 sparse inverse covariance estimation. IEEE Transactions on

Signal Processing, 63(12):3218–3231, 2015.

[46] R Kipp Martin. Using separation algorithms to generate mixed integer model reformulations. Operations

Research Letters, 10(3):119–128, 1991.

[47] Rahul Mazumder and Trevor Hastie. Exact covariance thresholding into connected components for

large-scale graphical lasso. Journal of Machine Learning Research, 13(Mar):781–794, 2012.

[48] Rahul Mazumder and Trevor Hastie. The graphical lasso: New insights and alternatives. Electronic

journal of statistics, 6:2125, 2012.

[49] Nicolai Meinshausen, Peter B¨uhlmann, et al. High-dimensional graphs and variable selection with the

lasso. The annals of statistics, 34(3):1436–1462, 2006.

[50] Figen Oztoprak, Jorge Nocedal, Steven Rennie, and Peder A Olsen. Newton-like methods for sparse

inverse covariance estimation. In Advances in neural information processing systems, pages 755–763,

2012.

[51] Franz Rendl, Giovanni Rinaldi, and Angelika Wiegele. Solving max-cut to optimality by intersecting

semideﬁnite and polyhedral relaxations. Mathematical Programming, 121(2):307, 2010.

[52] Philippe Rigollet and Alexandre Tsybakov. Estimation of covariance matrices under sparsity constraints.

arXiv preprint arXiv:1205.1210, 2012.

[53] Adam J Rothman, Peter J Bickel, Elizaveta Levina, Ji Zhu, et al. Sparse permutation invariant covari-

ance estimation. Electronic Journal of Statistics, 2:494–515, 2008.

[54] Narayana P Santhanam and Martin J Wainwright.

Information-theoretic limits of selecting binary

graphical models in high dimensions. IEEE Trans. Information Theory, 58(7):4117–4134, 2012.

[55] Katya Scheinberg, Shiqian Ma, and Donald Goldfarb. Sparse inverse covariance selection via alternating

linearization methods. In Advances in neural information processing systems, pages 2101–2109, 2010.

36

[56] Katya Scheinberg and Irina Rish. Sinco-a greedy coordinate ascent method for sparse inverse covariance

selection problem. preprint, 2009.

[57] Jun Shao. Linear model selection by cross-validation. Journal of the American statistical Association,

88(422):486–494, 1993.

[58] Renata Sotirov. Sdp relaxations for some combinatorial optimization problems. In Handbook on Semidef-

inite, Conic and Polynomial Optimization, pages 795–819. Springer, 2012.

[59] Kean Ming Tan, Palma London, Karthik Mohan, Su-In Lee, Maryam Fazel, and Daniela Witten. Learn-

ing graphical models with hubs. The Journal of Machine Learning Research, 15(1):3297–3331, 2014.

[60] Tomoki Tokuda, Ben Goodrich, I Van Mechelen, Andrew Gelman, and F Tuerlinckx. Visualizing

distributions of covariance matrices. Columbia Univ., New York, USA, Tech. Rep, pages 18–18, 2011.

[61] Lieven Vandenberghe, Martin S Andersen, et al. Chordal graphs and semideﬁnite optimization. Foun-

dations and Trends® in Optimization, 1(4):241–433, 2015.

[62] Huan Xu, Constantine Caramanis, and Shie Mannor. Robust regression and lasso.

In Advances in

Neural Information Processing Systems, pages 1801–1808, 2009.

[63] Kazuo Yonekura and Yoshihiro Kanno. Global optimization of robust truss topology via mixed integer

semideﬁnite programming. Optimization and Engineering, 11(3):355–379, 2010.

[64] Ming Yuan. High dimensional inverse covariance matrix estimation via linear programming. Journal of

Machine Learning Research, 11(Aug):2261–2286, 2010.

[65] Ming Yuan and Yi Lin. Model selection and estimation in the gaussian graphical model. Biometrika,

94(1):19–35, 2007.

[66] Cun-Hui Zhang et al. Nearly unbiased variable selection under minimax concave penalty. The Annals

of statistics, 38(2):894–942, 2010.

[67] Fuzhen Zhang. The Schur complement and its applications, volume 4. Springer Science & Business

Media, 2006.

37

