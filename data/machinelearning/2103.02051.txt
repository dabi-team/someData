Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

Yasaman Esfandiari 1 Sin Yong Tan 1 Zhanhong Jiang 2 Aditya Balu 1 Ethan Herron 1 Chinmay Hegde 3
Soumik Sarkar 1

Abstract

Decentralized learning enables a group of collab-
orative agents to learn models using a distributed
dataset without the need for a central parame-
ter server. Recently, decentralized learning algo-
rithms have demonstrated state-of-the-art results
on benchmark data sets, comparable with central-
ized algorithms. However, the key assumption to
achieve competitive performance is that the data
is independently and identically distributed (IID)
among the agents which, in real-life applications,
is often not applicable. Inspired by ideas from
continual learning, we propose Cross-Gradient
Aggregation (CGA), a novel decentralized learn-
ing algorithm where (i) each agent aggregates
cross-gradient information, i.e., derivatives of its
model with respect to its neighbors’ datasets, and
(ii) updates its model using a projected gradient
based on quadratic programming (QP). We theo-
retically analyze the convergence characteristics
of CGA and demonstrate its efﬁciency on non-IID
data distributions sampled from the MNIST and
CIFAR-10 datasets. Our empirical comparisons
show superior learning performance of CGA over
existing state-of-the-art decentralized learning al-
gorithms, as well as maintaining the improved
performance under information compression to re-
duce peer-to-peer communication overhead. The
code is available here on GitHub.

1
2
0
2

n
u
J

8
2

]

G
L
.
s
c
[

2
v
1
5
0
2
0
.
3
0
1
2
:
v
i
X
r
a

1. Introduction

Distributed machine learning refers to a class of algorithms
that are focused on learning from data distributed among
multiple agents. Approaches to design distributed deep
learning algorithms include: centralized learning (McMa-

1Department of Mechanical Engineering, Iowa State University,
Ames, Iowa, USA 2Johnson Controls, Milwaukee, Wisconsin,
USA 3Computer Science and Engineering Department, New York
University, New York City, New York, USA. Correspondence to:
Soumik Sarkar <soumiks@iastate.edu>.

Proceedings of the 38 th International Conference on Machine
Learning, PMLR 139, 2021. Copyright 2021 by the author(s).

Figure 1. Algorithm overview. In the proposed CGA algorithm
(1) each agent computes gradients of model parameters on its own
data set; (2) each agent sends its model parameters to its neighbors;
(3) each agent computes the gradients of its neighbors’ models on
its own data set and sends the cross gradients back to the respective
neighbors; (4) cross gradients and local gradients are projected into
an aggregated gradient (using Quadratic Programming); which is
then used to (5) update the model parameter.

han et al., 2017; Kairouz et al., 2019), decentralized learn-
ing (Lian et al., 2017; Nedi´c et al., 2018), gradient compres-
sion (Seide et al., 2014; Alistarh et al., 2018) and coordinate
updates (Richtárik and Takáˇc, 2016; Nesterov, 2012). In
centralized learning, a central parameter server collects,
processes, and sends processed information back to the
agents (Koneˇcn`y et al., 2016). As a popular approach for
centralized learning, Federated Learning (FL) leverages a
central parameter server and learns from dispersed datasets
that are private to the agents. Another approach is Feder-
ated Averaging (McMahan et al., 2017) where agents avoid
communicating with the server at each learning iteration
and signiﬁcantly decrease the communication cost.

Decentralized learning: While having a central parameter
server is acceptable for data center applications, in certain
use cases (such as learning over a wide-area distributed sen-
sor network), continuous communication with a central pa-
rameter server is often not feasible (Haghighat et al., 2020).
To address this concern, several decentralized learning algo-
rithms have been proposed, where agents only interact with
their neighbors without a central parameter server.

Recent advances in decentralized learning involve gossip

 
 
 
 
 
 
Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

Table 1. Comparison between different decentralized learning approaches. Rate: convergence rate for the optimization algorithm, Comm.:
Communication overhead per mini-batch, Bo. Gr. Var.: Bounded gradient variances and variations as an assumption, Bo. Sec. Mom.:
Bounded second moment of the gradient as an assumption, ms: model size for the local agent, Nb: total number of non-zero elements
in Π (total number of communications per mini-batch), γ: auxiliary costs due to forward and backward pass of the neural network, b:
ﬂoating point precision of arithmetic computations (e.g. 64)

Method
DPSGD
SGP
SwarmSGD
CGA (ours) O( 1

O( 1

N K

Rate
O( 1
K + 1√
)
K1.5 + 1√
K + 1
O( 1√
)
K
K1.5 + 1√

N K

K + 1

)

Comm.
O(msNb + γ)
O(msNb + γ)
Nb
O(ms
2 + γ)
K2 ) O(2msNb + γ)

N K

+ 1

Bo. Gr. Var. Bo. Sec. Mom.

Yes
Yes
No
Yes

No
No
Yes
No

∗ The communication overhead per mini-batch for CompCGA method is O( 2msNb

b + γ)

averaging algorithms (Boyd et al., 2006; Xiao and Boyd,
2004; Kempe et al., 2003). Combining SGD with gossip
averaging, Lian et al. (2017) shows analytically that decen-
tralized parallel SGD (DPSGD) has far less communication
overhead than its central counterpart (Dekel et al., 2012).
Along the same line of work, Scaman et al. (2018) intro-
duced a multi-step primal-dual algorithm while Yu et al.
(2019) and Balu et al. (2021) introduced the momentum ver-
sion of DPSGD. Tang et al. (2019) proposed DeepSqueeze,
error-compensated compression is used in decentralized
learning to achieve the same convergence rate as the one
of centralized algorithms. Koloskova et al. (2019) utilized
compression strategies to propose CHOCO-SGD algorithm
which learns from agents connected in varying topologies.
Similarly with the aid of compression, Lu and De Sa (2020)
and Vogels et al. (2020) introduced compression-based al-
gorithms that improves the memory usage and running time
of existing decentralized learning approaches. Assran et al.
(2019) proposed the SGP algorithm which converges at the
same sub-linear rate as SGD and achieves high accuracy on
benchmark datasets. Additionally, Koloskova et al. (2020)
presented a unifying framework for decentralized SGD anal-
ysis and provided the best convergence guarantees. More re-
cently, SwarmSGD was proposed by Nadiradze et al. (2019)
which leverages random interactions between participating
agents in a graph to achieve consensus. In a recent work, Ar-
jevani et al. (2020) proposes using AGD to achieve optimal
convergence rate both in theory and practice. Jiang et al.
(2018) propose multiple consensus and optimality rounds
and the tradeoff between the consensus and optimality in
decentralized learning.

Handling non-IID data: It is well known that decentral-
ized learning algorithms can achieve comparable perfor-
mance with its centralized counterpart under the so-called
IID (independently and identically distributed) assumption.
This refers to the situation where the training data is dis-
tributed in a uniformly random manner across all the agents.
However, in real life applications, such an assumption is dif-
ﬁcult to satisfy. Considering centralized learning literature,

Li et al. (2018) proposed a variant of FL by adding a penalty
term in the local objective function in FedProx algorithm.
They further showed that their algorithm achieves higher
accuracy when learning from non-IID data compared to Fe-
dAvg. Motivated by life-long learning (Shoham et al., 2019),
FedCurv was proposed by adding a penalty term to the local
loss function, with respect to Fisher information matrix. In
another research study, FedAvg-EMD (Zhao et al., 2018)
utilized the earth mover’s distance (EMD) as a metric to
quantify the distance between the data distribution on each
client and the population distribution, which was perceived
as the root cause of problems arising in the non-IID scenario.
Li et al. (2019b) showed the limitations with FedAvg on non-
IID data analytically. Also, FedNova was proposed in which
they use a normalized gradient in the update law of FedAvg
after they show that the standard averaging of client models
after heterogeneous local updates results in convergence to
a stationary point (Wang et al., 2020). Similar to the case
of decentralized learning, compression techniques (Sattler
et al., 2019; Rothchild et al., 2020), momentum variant of
algorithms (Wang et al., 2019; Li et al., 2019a), the use of
adaptive gradients (Tong et al., 2020), and use of controllers
in agent’s and server’s models (Karimireddy et al., 2019a)
are also used in centralized learning for coping with non-IID
data. Hsieh et al. (2019) proposes a solution for learning
from non-IID data by Estimating the degree of deviation
from IID by moving the model from one data partition to
another. They then Evaluate the accuracy on the other data
set and calculate the accuracy loss, and based on this mea-
sure, SkewScout controls the communication tightness by
automatically tuning the hyper-parameters of the decentral-
ized learning algorithm. In their experimental results, they
consider until 80% non-IID data whereas in our approach
our dataset is partitioned in a fully non-IID was based on
the classes.

Although the above centralized approaches can handle de-
parture from IID assumption, there still exists a gap in de-
centralized learning and several approaches fail under signif-
icant non-IID distribution of data among the agents (Hsieh

Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

et al., 2019; Jiang et al., 2017).

2. Cross-Gradient Aggregation

Contributions: To overcome the issue of handling non-
IID data distributions in a decentralized learning setting,
we propose the Cross-Gradient Aggregation (CGA) algo-
rithm in this paper. We show its effectiveness in learning
(deep) models in a decentralized manner from both IID and
non-IID data distributions. Inspired by continual learning
literature (Lopez-Paz and Ranzato, 2017), we devise an
algorithm which in each step of training, collects the gradi-
ent information of each agent’s model on all its neighbors’
datasets and projects them into a single gradient which is
then used to update the model. We use quadratic program-
ming (QP) to obtain such a projected gradient. We provide
an illustration of this algorithm in Figure 1.

The communication cost for our proposed algorithm is
higher than the other state-of-the-art algorithms due to addi-
tional cost for two-way communication of the model param-
eters to, and the gradient information from, the neighbors.
A comparison of the communication costs is provided in
Table 1. Therefore, we also propose a compressed variant
(CompCGA) to reduce the communication cost. Finally
we validate the performance of our algorithms on MNIST
and CIFAR-10 with different graph typologies. Our code
is publicly available on GitHub1. We then compare the ef-
fectiveness of our algorithm with SwarmSGD (Nadiradze
et al., 2019), SGP (Assran et al., 2019), and DPSGD (Lian
et al., 2017) and show that we can achieve higher accuracy
in learning from non-IID data compared to the state-of-the-
art decentralized learning approaches. Note that the goal
here is to provide comparison between different decentral-
ized learning algorithms; therefore, studies proposing novel
compression schemes (Tang et al., 2019; Koloskova et al.,
2019; Lu and De Sa, 2020; Vogels et al., 2020) are excluded
from our comparison.

In summary, (i) we introduce the concept of cross gradi-
ents to develop a novel decentralized learning algorithm
(CGA) that enables learning from both IID and non-IID
data distributions, (ii) to reduce the higher communication
costs of CGA, we propose a compressed variant, CompCGA
that maintains a reasonably good performance in both IID
and non-IID settings, (iii) we provide a detail convergence
analysis of our proposed algorithm and show that we have
similar convergence rates to the state-of-the-art decentral-
ized learning approaches as summarized in Table 1, (iv)
we demonstrate the efﬁcacy of our proposed algorithms on
benchmark datasets and compare performance with state-of-
the-art decentralized learning approaches.

1https://github.com/yasesf93/CrossGradientAggregation

Let us ﬁrst present a general problem formulation for de-
centralization deep learning, and then use it to motivate the
Cross-Gradient Aggregation (CGA) algorithmic framework.

2.1. Problem Formulation

Very broadly, decentralized learning involves N agents col-
laboratively solving the empirical risk minimization prob-
lem:

minx∈Rd F(x) :=

1
N

N
(cid:88)

i=1

fi(x),

(1)

where fi(x) := Eζi∼Di[Fi(x; ζi)] denotes a loss function
deﬁned in terms of dataset Di that is private to agent i ∈ [N ].
The agents are assumed to be communication-constrained
and can only exchange information with their neighbors
(where neighborliness is deﬁned according to a weighted
undirected graph with edge set C and adjacency matrix Π).
Note that the adjacency matrix Π is a doubly stochastic
matrix constructed using the edge set of the graph, C. For
(i, j) /∈ C, we assign zero link weights (i.e., πij = 0), and if
(i, j) ∈ C, the link weights are assigned such that the Π is
stochastic and symmetric, e.g., for a ring topology, πij = 1
3
if j ∈ {i − 1, i, i + 1}. The goal is for the agents to come
up with a consensus set of model parameters x (although
during training each agent operates on its own copy of x.)

Usual approaches in decentralized learning involve each
agent alternating between updating the local copies of their
parameters using gradient information from their private
datasets, and exchanging parameters with its neighbors. We
depart from this usual path by ﬁrst introducing two key
concepts.

Deﬁnition 1. For agent j, consider the dataset Dj, the dif-
ferentiable objective function fj, and the model parameter
copy xj. The self-gradient is deﬁned as:

gjj := ∇xfj(Dj; xj) .

(2)

Deﬁnition 2. For a pair of agents j, l, consider the dataset
Dl, the differentiable objective function fl, and the model
parameter copy xj. The cross-gradient is deﬁned as:

gjl := ∇xfl(Dl; xj) .

(3)

In words, the cross-gradient is calculated by evaluating the
gradient of the loss function private to agent l at the param-
eters of agent j. Both the self-gradient gjj and the cross-
gradient gjl immediately lend themselves to their stochastic
counterparts (implemented by simply mini-batching the pri-
vate datasets); in the rest of the paper, we will operate under
this setting.

Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

Algorithm 1 Cross-Gradient Aggregation (CGA)
Initialize: Dj, xj
for k = 1 : K do

0, vj

0, (j = 1, 2, . . . , N ), α, β, K, a QP solver

for j = 1 : N do

Randomly shufﬂe the data subset Dj
Compute gjj
k
Gj = {}
for each agent l s.t. (j, l) ∈ C do

Compute gjl
k
k ← Gj
Gj

k ∪ gjl

k

l πjlxl

end
k = (cid:80)
wj
k ← QP(gjj
˜gj
vj
k = βvj
k = wj
xj

k−1
k , Gj
k)
k−1 − α˜gj
k + vj

k

k

end

end

2.2. The CGA Algorithm

We now propose the CGA algorithm for decentralized deep
learning. Figure 1 provides a visual overview of the method.
Recall that xj is the model parameter copy for each agent j
which is initialized by training with Dj. Pick the number of
iterations K, step-size α, and the momentum coefﬁcient β
as user-deﬁned inputs.

In the kth iteration of CGA, each agent j ∈ [N ] calculates
its self-gradient gjj
k . Then, agent j’s model parameters are
transmitted to all other agents (l) in its neighborhood, and
the respective cross-gradients are calculated and transmit-
ted back to agent j and stacked up in a matrix Gj
k. Then
Gj
k are used to perform a quadratic programming
(QP) projection step, which we discuss in detail below. To
accelerate convergence, a momentum-like adjustment term
is also incorporated to obtain the ﬁnal update law.

k and gjj

The form of the algorithm is similar to momentum-
accelerated consensus SGD (Jiang et al., 2017). The key dif-
ference in Algorithm 1 when compared to existing gradient-
based learning methods is the QP projection step. We ob-
serve that the local gradient ˜gj is obtained via a nonlinear
projection, instead of just the self-gradient gjj (as is done
in standard momentum-SGD), or a linear averaging of self-
gradients in the neighborhood C (as is done in standard
decentralized learning methods).

The motivation for this difference stems from the nature
of the cross-gradients gjl
k . In the IID case, these should
statistically resemble the self-gradient gjj
k , and hence stan-
dard momentum averaging would succeed. However, with
non-IID data partitioning, the differences between the cross-
gradients in different agents becomes so signiﬁcant and

consensus may be difﬁcult to achieve, leading to overall
poor convergence properties. Therefore, in the non-IID case
we need an alternative approach.

We leverage the following intuition, borrowed from (Lopez-
Paz and Ranzato, 2017). We seek a descent direction that
is close to gll
k and simultaneously is positively correlated
with all the cross-gradients. This can be modeled via a QP
projection, posed in primal form as follows:

z(cid:62)z − g(cid:62)z +

minz

1
2
s.t. Gz ≥ 0

1
2

g(cid:62)g

(4a)

where g := gjj
formulation of the above QP can be posed as:

k and G := (gjl) ∀(j, l) ∈ C. The dual

minu

1
2

u(cid:62)GG(cid:62)u + g(cid:62)G(cid:62)u

(5a)

s.t. u ≥ 0

which is more efﬁcient from a computational standpoint.
Once we solve for the optimal dual variable u∗, we can
recover the optimal projection direction g∗ using the relation
g∗ = G(cid:62)u∗ + g.

2.3. The Compressed CGA Algorithm

The CGA algorithm requires multiple exchanges of model
parameters and gradients between neighbor agents in
each iteration, which can be a burden particularly in
communication-constrained environments. To reduce the
communication bandwidth, we propose adding a compres-
sion layer on top of the CGA framework. For that purpose,
we use Error Feedback SGD (EF-SGD) (Karimireddy et al.,
2019b) to compress gradients. The resulting algorithm is
same as Algorithm 1; except that instead of regular self- and
cross-gradients, a scaled signed gradient is calculated, the
error between the compressed and non-compressed gradi-
ents will be computed (eij
k in the algorithm), and this error
will be added as a penalty term to the gradients in the next
step. The resulting algorithm is shown in Algorithm 2. In
the pseudo code provided there, the quantity d corresponds
to the dimension of the computed gradients for each agent.

3. Convergence Analysis for CGA

We now present a theoretical analysis of our proposed CGA
approach. It should be noted that the communication among
the agents is assumed to be synchronous in the following
analysis. Let us begin with a deﬁnition of smoothness.

Deﬁnition 3. A function F(·) is L-smooth if ∀x, y:

F(x) ≤ F(y) + ∇F(y)(cid:62)(x − y) +

L
2

(cid:107)x − y(cid:107)2.

(6)

Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

Algorithm 2 Compressed Cross-Gradient Aggregation
(CompCGA)
Initialize: Dj, ej
for k = 1 : K do

0, (j = 1, . . . , N ), α, β, K, a QP solver

0, vj

0, xj

for j = 1 : N do

Randomly shufﬂe the data subset Dj
Compute gjj
k
pjj
k = gjj
k = ((cid:107)pjj
δjj
Gj = {}
for each agent l, s.t. (j, l) ∈ C do

k (cid:107)1/d)sgn(pjj
k )

k + ejj

k

k

k + ejl

Compute gjl
k
k = gjl
pjl
δjl
k = ((cid:107)pjl
ejl
k − δjl
k = pjl
k
Gj ← Gj ∪ δjl
k

k (cid:107)1/d)sgn(pjl
k )

l πjlxl

end
k = (cid:80)
wj
˜gj ← QP(δjj
k = βvj
vj
k = wj
xj
k = pjj
ejj

k−1
k , Gj)
k−1 − α˜gj
k + vj
k − δjj

k

k

end

end

In order to analyze the convergence of decentralized learn-
ing algorithms, the following assumptions are standard.
Assumption 1. Each function fi(x) is L-smooth.
Assumption 2. There exist σ > 0 and δ > 0 such that

Eζ∼Di[(cid:107)∇Fi(x; ζ) − ∇fi(x)(cid:107)] ≤ σ2,

(7)

and that

1
N

N
(cid:88)

i=1

(cid:107)∇fi(x) − ∇F(x)(cid:107)2 ≤ δ2.

(8)

the value of (cid:15) is governed by the difference between the data
distributions possessed by each agent. Note that thus far,
Eq. 8 has been used to study the effect of non-IID data in
most analyses of decentralized learning; previous methods
operate upon gi. In our work, we combine both Eq. 8 and
Eq. 9 to mathematically show convergence.

We next impose another assumption on the graph that serves
to characterize consensus.
Assumption 4. The mixing matrix Π ∈ RN ×N is a doubly
stochastic matrix with λ1(Π) = 1 and

max{|λ2(Π)|, |λN (Π)|} ≤

√

ρ < 1,

(10)

where λi(Π) is the ith-largest eigenvalue of Π and ρ is a
constant.

3.1. Theoretical Results

We now present our theoretical characterization of CGA. We
focus only on the case of non-convex objective functions.
All detailed proofs are presented in the Appendix, and follow
from basic algebra and sequence convergence theory. Below,
i indicates the agent index; the average of all agent model
copies is represented by ¯x; throughout the analysis, we
assume that the objective function value is bounded below
by F ∗. We also denote an = O(bn) if an ≤ c bn for some
constant c > 0.

We ﬁrst present a lemma showing that CGA achieves con-
sensus among the different agents, and then prove our main
theorem indicating convergence of the algorithm.
Lemma 1. Let Assumptions 1-4 hold. Deﬁne {¯xk}, ∀k ≥ 0
as the agent average sequence obtained by the iterations of
CGA. If β ∈ [0, 1) is the momentum coefﬁcient, then for all
K ≥ 1, we have:

N
(cid:88)

E

(cid:20)(cid:13)
(cid:13)
¯xk − xi
(cid:13)
k
(cid:13)

2(cid:21)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

K−1
(cid:88)

k=0

1
N

2α2
(1 − β)2

i=1
(cid:18) (cid:15)2

1 − ρ

+

3σ2
√

(1 −

ρ)2 +

3δ2
√

(1 −

ρ)2

(cid:19)

K+

Assumption 3. Deﬁne gi = ∇Fi(x; ζ). Then, there exists
(cid:15) > 0 such that

Eζ∼Di[(cid:107)˜gi − gi(cid:107)2] ≤ (cid:15)2.

(9)

6α2

(1 − β)2(1 −

√

ρ)

K−1
(cid:88)

k=0

E[(cid:107)

1
N

N
(cid:88)

i=1

∇fi(xi

k)(cid:107)2].

(11)

Assumption 1 implies that F(x) is L-smooth. Assumption 2
assumes bounded variances due to non-IID-ness. Equa-
tion 7 bounds the variance within the same agent ("intra-
variance") while Equation 8 bounds the variance among
different agents ("inter-variance").

Assumption 3 is necessitated by our adoption of the QP pro-
jection step. Intuitively, if the local optimization problem is
meaningful, then this assumption holds. In this assumption,

A complete proof can be found in the Supplementary Sec-
tion A.1. From Lemma 1, we can observe that the evolution
of the deviation of the model copies from their average
can be attributed to two terms. The ﬁrst is the following
constant:

2α2
(1 − β)2

(cid:18) (cid:15)2

1 − ρ
(cid:124) (cid:123)(cid:122) (cid:125)
I

+

3σ2
√

(1 −
(cid:124)

(cid:123)(cid:122)
II

+

ρ)2
(cid:125)

3δ2
√

(1 −
(cid:124)

(cid:123)(cid:122)
III

(cid:19)

K,

ρ)2
(cid:125)

Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

where (I) is controlled by Assumption 3 (which also implies
how well the local QP is solved), (II) is related to sampling
variance, and (III) indicates the gradient variations (deter-
mined by the data distributions). Additionally, the step size
and momentum coefﬁcient can be tuned to reduce the nega-
tive impact of these variance coefﬁcients. The second is the
following term:

6α2

(1 − β)2(1 −

√

ρ)

E[(cid:107)

1
N

K−1
(cid:88)

k=0
(cid:124)

N
(cid:88)

i=1
(cid:123)(cid:122)
IV

∇fi(xi

k)(cid:107)2]
,

(cid:125)

where (IV) is the summation of the squared norms of av-
erage gradients, the effect of which can be controlled by
leveraging the step size and momentum coefﬁcient.

Lemma 1 also aligns with the well-known result phe-
nomenon in decentralized learning that the consensus error
is inversely proportional to the spectral gap of the graph mix-
ing matrix. Using the above lemma, we obtain the following
main result.

Theorem 1. Let Assumptions 1-4 hold. Suppose that the
step size α satisﬁes the following relationships:

(cid:40)

0 < α ≤ βL
6α2L2
1 −
(1−β)(1−

(1−β)2

√

ρ)2 − 4Lα

(1−β)2 ≥ 0.

(12)

For all K ≥ 1, we have

1
K

K−1
(cid:88)

k=0

E

(cid:107)∇F (¯xk)(cid:107)2(cid:105)
(cid:104)

≤

(F (¯x0) − F ∗) +

(cid:18)

2C2 + C3

1
C1K

C5

C5

2α2
(1 − β)2(1 − ρ)

(cid:19)

(cid:15)2 +

(cid:18) 2
N

6α2

(1 − β)2(1 −

(cid:19)

σ2 + C5

√

p)2

α2β

(1 − β)4 + C4+
α2β
(1 − β)4 )+
6α2

(C2 + C3

(1 − β)2(1 −

√

ρ)2 δ2,
(13)

where C1
=
(cid:16) βLα2
2(1−β)3 + α2L

(1−β)2

(cid:17)

α

2(1−β) − (1−β)α2
2βL , C2
/C1,

/C1, C3 = (1−β)L

2β

=

C4 = βL

2(1−β)3 /C1, C5 = αL2

2(1−β) /C1.

A complete proof of Theorem 1 is discussed in the Supple-
mentary Section A.2.

Theorem 1 shows that the average gradient magnitude
achieved by the consensus estimates is upper-bounded by
the difference between initial objective function value and
the optimal value, as well as how well the local QP is solved,
the sampling variance, and the non-IID-ness. The coefﬁ-
cients before these constants are determined by α, β, and L;

judicious selection of α and β can be performed to reduce
the error bound. Additionally, the step size is required to sat-
isfy two conditions as listed in the above theorem statement.
The second condition can be solved to get another upper
bound, denoted by α∗ (which will be shown in the Appendix
section). Hence, if we choose 0 < α ≤ min{ βL
(1−β)2 , α∗},
the last inequality naturally holds. We next present a corol-
lary to explicitly show the convergence rate of CGA.
Corollary 1. Suppose that the step size satisﬁes α =
(cid:15) = O( 1√
For a sufﬁ-
).
O(
K
ciently large K ≥ max{ 144N L2
, N
β2L2 }, r = (1 −
√
ρ)2, we have,

ρ)2 + 24(1 − β)3 − 4(1 −

ρ)(cid:112)16(1 −

) and that

√
N√
K

√

√

r2

for some constant C > 0,

1
K

K−1
(cid:88)

k=0

E

(cid:107)∇F (¯xk)(cid:107)2(cid:105)
(cid:104)

(cid:32)

≤ C

√

1
N K

+

1
K

+

1
K 1.5 +

1
K 2

(cid:33)

.

(14)

N K

1√

An immediate observation is that when K is sufﬁciently
) will dominate the convergence rate
large, the term O(
such that the linear speed up can be achieved, if increasing
the number of agents N . This convergence rate matches the
well-known best result in decentralized SGD algorithms in
literature.

Analysis of CompCGA. We now provide some qualitative
arguments to facilitate the understanding of CompCGA.
Though we have not directly established its convergence
rates, we can presumably extend the analysis of CGA to this
setting. Observe that the core update laws for x are the same
for CompCGA as in CGA, but equipped with gradient com-
pression. Moreover, for our theoretical analysis presented
for CGA, the speciﬁc way in which ˜g is calculated does
not play a role, and additional compression can perhaps be
modeled by changing the variation constants. Therefore, we
hypothesize that CompCGA also exhibits a convergence rate
of O(
). This is also evidently seen from our empirical
studies, which we present next.

1√

N K

4. Experimental Results

In this section, we analyze the performance of CGA algo-
rithm empirically. We compare the effectiveness of our al-
gorithms with other baseline decentralized algorithms such
as SwarmSGD (Nadiradze et al., 2019), SGP (Assran et al.,
2019), and the momentum variant of DPSGD (Lian et al.,
2017) (DPMSGD).

Setup. We present the empirical studies on CIFAR-10 and
MNIST datasets (MNIST results can be found in the Supple-
mentary Section A.6). To explore the algorithm performance
under different situations, the experiments are performed

Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

with 5, 10 , and 40 agents. Here, we consider an extreme
form of non-IID-ness by assigning different classes of data
to each agent. For example, when there are 5 agents, each
agent has the data for 2 distinct classes, and similarly when
there are 10 agents, each agent has the data for 1 distinct
class. When the number of agents are more than the number
of classes, each class is divided into a sufﬁcient number of
subsets of samples and agents are randomly assigned dis-
tinct subsets. We use a deep convolutional neural network
(CNN) model (with 2 convolutional layers with 32 ﬁlters
each followed by a max pooling layer, then 2 more con-
volutional layers with 64 ﬁlters each followed by another
max pooling layer and a dense layer with 512 units, ReLU
activation is used in convolutional layers) for our validation
experiments. Additionally, We use a VGG11 (Simonyan and
Zisserman, 2014) model for CIFAR-10 (Detailed CIFAR-10
results can be found in the Supplementary Section A.5). A
mini-batch size of 128 is used, the initial step-size is set to
0.01 for CIFAR-10, and step size is decayed with constant
0.981. The stopping criterion is a ﬁxed number of epochs
and the momentum parameter (β) is set to be 0.98. The
consensus model is then used to be evaluated on the local
test sets and the average accuracy is reported.

The experiments are performed on a large high-performance
computing cluster with a total of 192 GPUs distributed
over 24 nodes. Each node in the cluster is made of 2 Intel
Xeon Gold 6248 CPUs with each 20 cores and 8 Tesla
V100 32GB SXM2 GPUs. An experiment with 40 agents
on a VGG11 model for CIFAR10 dataset takes about 55
seconds per epoch for execution. The code for performing
the experiments is publicly available2.

4.1. CGA convergence characteristics

We start by analyzing the performance of CGA algorithm on
CIFAR-10. Figure 2 shows the convergence characteristics
of our proposed algorithm via training loss versus epochs.
Figure 2(a) shows the convergence characteristics of CGA
for IID data distributions for different communication graph
topologies. While the fully connected graph represents
a dense topology, the ring and bipartite graphs represent
relatively much sparser topologies.

We observe that the convergence behavior induced by the
training loss remain similar across the different graph topolo-
gies, though at the ﬁnal stage of training, the ring and bi-
partite networks moderately outperform the fully connected
one. This can be attributed to more communication oc-
curring for the fully connected case. The phenomenon of
faster convergence with sparser graph topology is an ob-
servation that have been made by earlier research works in
Federated Learning (McMahan et al., 2017) by reducing
the client fraction which makes the mixing matrix sparser

2https://github.com/yasesf93/CrossGradientAggregation

Table 2. Testing accuracy comparison for CIFAR10 with IID data
distribution using CNN model architecture
Fully-connected Ring

Bipartite

Model

DPMSGD

SGP

SwarmSGD

CGA (ours)

CompCGA (ours)

68.8% (5)
68.1% (10)
67.6% (40)
66.6% (5)
59.3% (10)
46.3% (40)
70.6% (5)
68.3% (10)
31.5% (40)
68.5 % (5)
68.5% (10)
64.6% (40)
68.4% (5)
62.2% (10)
63.3% (40)

67.7% (5)
67.7% (10)
66.8% (40)
66.3% (5)
59.2% (10)
46.2% (40)
70.7% (5)
65.4% (10)
31.4% (40)
67.9 % (5)
67.8% (10)
63.7% (40)
68.3% (5)
62.9% (10)
53.4% (40)

67.7% (5)
67.3% (10)
57.1% (40)
66.3% (5)
58.4% (10)
46.3% (40)
70.7% (5)
60.3% (10)
33.4% (40)
68.2 % (5)
68.2% (10)
58.4% (40)
68.4% (5)
64.6% (10)
56.6% (40)

and decentralized learning (Jiang et al., 2017). However,
as Figure 6(a) in the Supplementary Section A.5 shows, we
observe that by training for more number of epochs, train-
ing losses associated with all graph topologies converge to
similar values.

Figure 2(b) shows similar curves but for the non-IID case. In
this case, we do observe a slight difference in convergence
with faster rates for sparser topologies compared to their
dense (fully connected) counterpart. Another phenomenon
observed here is that for sparser topologies, the training
process has more gradient variances and variations, which
has been caused by the non-IID data distributions. This well
matches the theoretical analysis we have obtained.

Finally, Figure 2(c) shows the comparison of convergence
characteristics with other state-of-the-art decentralized algo-
rithms with non-IID data distributions. CGA training is seen
to be smoother compared to SwarmSGD, and to converge
signiﬁcantly faster compared to both SwarmSGD and SGP.
From the theoretical analysis, we have shown that CGA
enables to converge faster at the beginning although after a
sufﬁciently large number of epochs, all methods listed here
achieve the same rate O(

1√

).

N K

Additionally, the SwarmSGD requires a geometrically dis-
tributed random variable to determine the number of local
stochastic gradient steps performed by each agent upon in-
teraction. That causes the largest variance shown in the loss
curve in Figure 2(c). Note that since DPMSGD diverges for
most of the non-IID experiments (see Table 3), we do not
provide its loss plots here.

4.2. Comparative evaluation

We compare our proposed algorithm, CGA and its com-
pressed version, compCGA with other state-of-the-art decen-
tralized methods - DPMSGD, SGP and SwarmSGD. Note
that in order to provide a fair comparison between the al-

Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

(a)

(b)

(c)

Figure 2. Average training loss (log scale) for (a) CGA method on IID (b) CGA method on non-IID data distributions (c) different methods
on non-IID data distributions for training 5 agents using CNN model architecture

(a)

(b)

(c)

(d)

Figure 3. Average testing accuracy for different methods learning from (a) IID data distributions w.r.t graph topology (b) non-IID data
distributions w.r.t graph topology (c) IID data distributions w.r.t the number of learning agents (d) non-IID data distributions w.r.t the
number of learning agents

Table 3. Testing accuracy comparison for CIFAR10 with non-IID
data distribution using CNN model architecture

Model

Fully-connected Ring

Bipartite

DPMSGD

SGP

SwarmSGD

CGA (ours)

CompCGA (ours)

Diverges (5)
10.1% (10)
10.5% (40)
66.4% (5)
59.3% (10)
46.2% (40)
67.3% (5)
47.3% (10)
30.6% (40)
68.4% (5)
68.2% (10)
62.1% (40)
61.7% (5)
60.2% (10)
59.8% (40)

Diverges (5)
Diverges (10)
10.0% (40)
46.3% (5)
25.8% (10)
31.4% (40)
62.5% (5)
38.5% (10)
25.8% (40)
66.5% (5)
48.8% (10)
40.9% (40)
50.3% (5)
39.5% (10)
32.7% (40)

Diverges (5)
10.0% (10)
10.7% (40)
49.8% (5)
24.9% (10)
11.8% (40)
63.9% (5)
33.8% (10)
23.5% (40)
67.2% (5)
38.9% (10)
25.7% (40)
40.4% (5)
36.7% (10)
23.6% (40)

gorithms, there are some minor adjustments that we made
during the experiments. For SGP optimizer, we considered
the graph to be undirected where the connected agents could
both send and receive information to and from each other.
On top of that, the adjacency matrix Π in our experiments
(a.k.a mixing matrix P (k) in Assran et al. (2019)) is ﬁxed
throughout the training process. In the implementation of
SwarmSGD, we deﬁned the number of local SGD steps,
H = 1, where the selected pair of agents perform only a

single local SGD update before averaging their model pa-
rameters. In term of graph topologies, SwarmSGD was run
not only on r-regular graphs (fully connected and ring) as
described in Nadiradze et al. (2019), we also performed
experiments using bipartite graph topology which is not
r-regular.

As a baseline, we ﬁrst provide a comparative evaluation for
IID data distributions in Table 2. Results show that CGA
performance is comparable with or slightly better than other
methods in most cases with smaller number of agents, i.e.,
5 and 10. However, we do observe a noticeable reduction
in testing accuracy for SGP and SwarmSGD with 40 agents
communicating over Ring or Bipartite graphs (which is an
expected trend as reported in Assran et al. (2019) and Sattler
et al. (2019)). While the testing accuracy of CGA also de-
creases in these scenarios, the performance reduction is not
as drastic in comparison. The performance of compCGA de-
teriorates slightly compared to CGA, while still maintaining
better accuracy than other methods in most scenarios.

The advantage of CGA is much more pronounced under
non-IID data distributions as seen in Table 3. With extreme
non-IID data distributions, CGA achieves the highest ac-
curacy for all scenarios with different number of learning
agents and communication graph topologies. In contrast,
the baseline method DPMSGD struggles signiﬁcantly in

Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

all scenarios with non-IID data. Other methods (SGP and
SwarmSGD) while having similar performance as CGA for
5 agents and fully connected topology, their performances
drop signiﬁcantly more than that of CGA with higher num-
ber of agents and sparser communication graphs. CompCGA
performs slightly worse than CGA, while still maintaining
better accuracy than other methods in most scenarios.

Finally, we graphically summarize the overall trends that we
observed in Figure 3. From Figure 3 (a) and (b), it is clear
that while there is no appreciable impact of graph topology
on testing accuracy under IID data distributions, the impact
is quite signiﬁcant under non-IID data distributions. The
results shown here are with 5 agents (see Supplementary
Section A.5 for 10 and 40 agents). In this case, testing accu-
racy decreases for sparse graph topologies which conforms
with observations made in Sattler et al. (2019). Figure 3 (c)
and (d) show accuracy trends with respect to the number
of agents. All results shown here are with fully connected
topology (see Supplementary Section A.5 for other topolo-
gies). In this regard, Assran et al. (2019) shows a slight
reduction in accuracy when the number of nodes/agents
increase for both SGP and DPSGD methods. We see a
similar trend here for both IID and non-IID data distribu-
tions. Clearly, the impact is more pronounced for non-IID
data. However, the performance decrease with increase in
number agents remain small for both CGA and CompCGA
under non-IID data distributions. Also, as discussed in Ta-
ble 1, we notice that the communication rounds for CGA is
twice as that of SGP and 4 times the communication round
of SwarmSGD. Therefore, we looked at their convergence
properties w.r.t communication rounds. As Figure 4 shows,
CGA converges to a lower loss value after 200 communica-
tion rounds.

Figure 4. Average training loss (log scale) for different algorithms
w.r.t communication rounds on non-IID data distributions (for 5
agents using CNN model architecture)

5. Conclusions

In this paper, we propose the Cross-Gradient Aggregation
(CGA) algorithm to effectively learn from non-IID data
distributions in a decentralized manner. We present conver-
gence analysis for our proposed algorithm and show that we
match the best known convergence rate for decentralized
algorithms using CGA. To reduce the communication over-
head associated with CGA, we propose a compressed variant
of our algorithm (CompCGA) and show its efﬁcacy. Finally,
we compare the performance of both CGA and CompCGA
with state-of-the-art decentralized learning algorithms and
show superior performance of our algorithms especially for
the non-IID data distributions. Future research will focus
on addressing performance reduction in scenarios with a
large number of agents communicating over sparse graph
topologies.

6. Acknowledgements

This work was partly supported by the National Science
Foundation under grants CAREER-1845969 and CAREER
CCF-2005804. We would also like to thank NVIDIA® for
providing GPUs used for testing the algorithms developed
during this research. This work also used the Extreme Sci-
ence and Engineering Discovery Environment (XSEDE),
which is supported by NSF grant ACI-1548562 and the
Bridges system supported by NSF grant ACI-1445606, at
the Pittsburgh Supercomputing Center (PSC).

References

Dan Alistarh, Torsten Hoeﬂer, Mikael Johansson, Nikola
Konstantinov, Sarit Khirirat, and Cédric Renggli. The
convergence of sparsiﬁed gradient methods. In Advances
in Neural Information Processing Systems, pages 5973–
5983, 2018.

Yossi Arjevani, Joan Bruna, Bugra Can, Mert Gürbüzbala-
ban, Stefanie Jegelka, and Hongzhou Lin. Ideal: Inexact
decentralized accelerated augmented lagrangian method.
arXiv preprint arXiv:2006.06733, 2020.

Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and
Mike Rabbat. Stochastic gradient push for distributed
deep learning. In International Conference on Machine
Learning, pages 344–353. PMLR, 2019.

Aditya Balu, Zhanhong Jiang, Sin Yong Tan, Chinmay
Hedge, Young M Lee, and Soumik Sarkar. Decentralized
deep learning using momentum-accelerated consensus.
In ICASSP 2021-2021 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP),
pages 3675–3679. IEEE, 2021.

Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and De-

Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

vavrat Shah. Randomized gossip algorithms. IEEE trans-
actions on information theory, 52(6):2508–2530, 2006.

Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin
Xiao. Optimal distributed online prediction using mini-
batches. The Journal of Machine Learning Research, 13:
165–202, 2012.

Arya Ketabchi Haghighat, Varsha Ravichandra-Mouli,
Pranamesh Chakraborty, Yasaman Esfandiari, Saeed
Arabi, and Anuj Sharma. Applications of deep learn-
ing in intelligent transportation systems. Journal of Big
Data Analytics in Transportation, 2(2):115–145, 2020.

Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip B
Gibbons. The non-iid data quagmire of decentralized ma-
chine learning. arXiv preprint arXiv:1910.00189, 2019.

Zhanhong Jiang, Aditya Balu, Chinmay Hegde, and Soumik
Sarkar. Collaborative deep learning in ﬁxed topology
networks. Advances in Neural Information Processing
Systems, 2017:5905–5915, 2017.

Zhanhong Jiang, Aditya Balu, Chinmay Hegde, and Soumik
Sarkar. On consensus-optimality trade-offs in collabo-
rative deep learning. arXiv preprint arXiv:1805.12120,
2018.

Peter Kairouz, H Brendan McMahan, Brendan Avent, Au-
rélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Keith
Bonawitz, Zachary Charles, Graham Cormode, Rachel
Cummings, et al. Advances and open problems in feder-
ated learning. arXiv preprint arXiv:1912.04977, 2019.

Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri,
Sashank J Reddi, Sebastian U Stich, and Ananda Theertha
Scaffold: Stochastic controlled averaging
Suresh.
arXiv preprint
for on-device federated learning.
arXiv:1910.06378, 2019a.

Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian U
Stich, and Martin Jaggi. Error feedback ﬁxes signsgd
and other gradient compression schemes. arXiv preprint
arXiv:1901.09847, 2019b.

David Kempe, Alin Dobra, and Johannes Gehrke. Gossip-
In 44th
based computation of aggregate information.
Annual IEEE Symposium on Foundations of Computer
Science, 2003. Proceedings., pages 482–491. IEEE, 2003.

Anastasia Koloskova, Tao Lin, Sebastian U Stich, and
Martin Jaggi. Decentralized deep learning with ar-
arXiv preprint
bitrary communication compression.
arXiv:1907.09356, 2019.

Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin
Jaggi, and Sebastian U Stich. A uniﬁed theory of decen-
tralized sgd with changing topology and local updates.
arXiv preprint arXiv:2003.10422, 2020.

Jakub Koneˇcn`y, H Brendan McMahan, Daniel Ramage, and
Peter Richtárik. Federated optimization: Distributed ma-
chine learning for on-device intelligence. arXiv preprint
arXiv:1610.02527, 2016.

Chengjie Li, Ruixuan Li, Haozhao Wang, Yuhua Li, Pan
Zhou, Song Guo, and Keqin Li. Gradient scheduling
with global momentum for non-iid data distributed asyn-
chronous training. arXiv preprint arXiv:1902.07848,
2019a.

Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar San-
jabi, Ameet Talwalkar, and Virginia Smith. Federated
optimization in heterogeneous networks. arXiv preprint
arXiv:1812.06127, 2018.

Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang,
and Zhihua Zhang. On the convergence of fedavg on
non-iid data. arXiv preprint arXiv:1907.02189, 2019b.

Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei
Zhang, and Ji Liu. Can decentralized algorithms outper-
form centralized algorithms? a case study for decentral-
ized parallel stochastic gradient descent. In Advances
in Neural Information Processing Systems, pages 5330–
5340, 2017.

David Lopez-Paz and Marc’Aurelio Ranzato. Gradient
episodic memory for continual learning. In Advances
in Neural Information Processing Systems, pages 6467–
6476, 2017.

Yucheng Lu and Christopher De Sa. Moniqua: Modulo
quantized communication in decentralized sgd. arXiv
preprint arXiv:2002.11787, 2020.

Brendan McMahan, Eider Moore, Daniel Ramage, Seth
Hampson, and Blaise Aguera y Arcas. Communication-
efﬁcient learning of deep networks from decentralized
data. In Artiﬁcial Intelligence and Statistics, pages 1273–
1282. PMLR, 2017.

Giorgi Nadiradze, Amirmojtaba Sabour, Dan Alistarh,
Aditya Sharma,
Ilia Markov, and Vitaly Aksenov.
SwarmSGD: Scalable decentralized SGD with local up-
dates. arXiv preprint arXiv:1910.12308, 2019.

Angelia Nedi´c, Alex Olshevsky, and Michael G Rabbat. Net-
work topology and communication-computation tradeoffs
in decentralized optimization. Proceedings of the IEEE,
106(5):953–976, 2018.

Yu Nesterov. Efﬁciency of coordinate descent methods
on huge-scale optimization problems. SIAM Journal on
Optimization, 22(2):341–362, 2012.

Peter Richtárik and Martin Takáˇc. Distributed coordinate
descent method for learning with big data. The Journal
of Machine Learning Research, 17(1):2657–2681, 2016.

Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

Lin Xiao and Stephen Boyd. Fast linear iterations for dis-
tributed averaging. Systems & Control Letters, 53(1):
65–78, 2004.

Hao Yu, Rong Jin, and Sen Yang. On the linear speedup
analysis of communication efﬁcient momentum sgd for
arXiv preprint
distributed non-convex optimization.
arXiv:1905.03817, 2019.

Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon
Civin, and Vikas Chandra. Federated learning with non-
iid data. arXiv preprint arXiv:1806.00582, 2018.

Daniel Rothchild, Ashwinee Panda, Enayat Ullah, Nikita
Ivkin, Ion Stoica, Vladimir Braverman, Joseph Gon-
zalez, and Raman Arora. Fetchsgd: Communication-
efﬁcient federated learning with sketching. arXiv preprint
arXiv:2007.07682, 2020.

Felix Sattler, Simon Wiedemann, Klaus-Robert Müller, and
Wojciech Samek. Robust and communication-efﬁcient
federated learning from non-iid data. IEEE transactions
on neural networks and learning systems, 2019.

Kevin Scaman, Francis Bach, Sébastien Bubeck, Laurent
Massoulié, and Yin Tat Lee. Optimal algorithms for non-
smooth distributed optimization in networks. In Advances
in Neural Information Processing Systems, pages 2740–
2749, 2018.

Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong
Yu. 1-bit stochastic gradient descent and its application
to data-parallel distributed training of speech dnns. In
Fifteenth Annual Conference of the International Speech
Communication Association, 2014.

Neta Shoham, Tomer Avidor, Aviv Keren, Nadav Israel,
Daniel Benditkis, Liron Mor-Yosef, and Itai Zeitak. Over-
coming forgetting in federated learning on non-iid data.
arXiv preprint arXiv:1910.07796, 2019.

Karen Simonyan and Andrew Zisserman. Very deep con-
volutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014.

Hanlin Tang, Xiangru Lian, Shuang Qiu, Lei Yuan,
Ce Zhang, Tong Zhang, and Ji Liu. Deepsqueeze: Par-
allel stochastic gradient descent with double-pass error-
compensated compression. CoRR, abs/1907.07346, 2019.
URL http://arxiv.org/abs/1907.07346.

Qianqian Tong, Guannan Liang, and Jinbo Bi. Effective
federated adaptive gradient methods with non-iid decen-
tralized data. arXiv preprint arXiv:2009.06557, 2020.

Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi.
Practical low-rank communication compression in decen-
tralized deep learning. Advances in Neural Information
Processing Systems, 33, 2020.

Jianyu Wang, Vinayak Tantia, Nicolas Ballas, and Michael
Rabbat. Slowmo: Improving communication-efﬁcient
distributed sgd with slow momentum. arXiv preprint
arXiv:1910.00643, 2019.

Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and
H Vincent Poor. Tackling the objective inconsistency
problem in heterogeneous federated optimization. arXiv
preprint arXiv:2007.07481, 2020.

Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

A. Appendix

A.1. Proof of Lemma 1

This section presents the detailed proof for Lemma 1. To begin with, we provide some technical auxiliary lemmas and the
associated proof. We start with bounding the ensemble average of local optimal gradients.

The core update law for CGA is:
Lemma 2. Let all assumptions hold. Let gi be the unbiased estimate of ∇fi(xi) at the point xi such that E[gi] = ∇fi(xi),
for all i ∈ [N ] := {1, 2, ..., N }. Thus the following relationship holds

E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

N
(cid:88)

i=1

˜gi

2(cid:21)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

2σ2
N

+ 2E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

N
(cid:88)

i=1

2(cid:21)

(cid:13)
(cid:13)
∇fi(xi)
(cid:13)
(cid:13)

+ 2(cid:15)2.

(15)

Proof.

E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

N
(cid:88)

i=1

˜gi

2(cid:21)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

= E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

a
≤ 2E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

N
(cid:88)

(˜gi − gi)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

≤

2
N

(cid:20) N
(cid:88)

E

i=1

i=1
(cid:13)
(cid:13)
˜gi − gi
(cid:13)
(cid:13)

N
(cid:88)

i=1
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

(cid:13)
(cid:13)
(˜gi − gi + gi)
(cid:13)
(cid:13)

2(cid:21)

= E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

1
N

N
(cid:88)

i=1

(cid:13)
(cid:13)
gi(cid:107)
(cid:13)
(cid:13)

2(cid:21) b

≤ 2

(cid:20)

E

N

1
N 2

N
(cid:88)

i=1
(cid:13)
(cid:13)
˜gi − gi
(cid:13)
(cid:13)

i=1

N
(cid:88)

(˜gi − gi) +

1
N

N
(cid:88)

i=1

gi

2(cid:21)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

2(cid:21)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ 2(

σ2
N

+ E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

N
(cid:88)

i=1

(cid:13)
(cid:13)
∇fi(xi)
(cid:13)
(cid:13)

2(cid:21)
)

2(cid:21)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ 2

σ2
N

+ 2E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

N
(cid:88)

i=1

2(cid:21)

(cid:13)
(cid:13)
∇fi(xi)
(cid:13)
(cid:13)

=

2
N

N
(cid:88)

i=1

E

(cid:20)(cid:13)
(cid:13)
˜gi − gi
(cid:13)
(cid:13)

2(cid:21)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ 2

σ2
N

+ 2E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

N
(cid:88)

i=1

∇fi(xi)

2(cid:21)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

c
≤ 2(cid:15)2 +

2σ2
N

+ 2E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

N
(cid:88)

i=1

2(cid:21)

(cid:13)
(cid:13)
∇fi(xi)
(cid:13)
(cid:13)

(16)

(a) refers to the fact that the inequality (cid:107)a + b(cid:107)2 ≤ 2(cid:107)a(cid:107)2 + 2(cid:107)b(cid:107)2. (b) holds as (cid:107) (cid:80)N
second term in the second inequality is the conclusion of Lemma 1 in (Yu et al., 2019) (c) follows from Assumption 3.

i=1 ai(cid:107)2 ≤ N (cid:80)N

i=1 (cid:107)ai(cid:107)2. The

Multiplying the update law by 1

N 11(cid:62), where 1 is the column vector with entries being 1, we obtain:

We deﬁne an auxiliary sequence such that

1
N

N
(cid:88)

i=1

˜gi
k−1

¯vk = β ¯vk−1 − α

¯xk = ¯xk−1 + ¯vk

¯zk :=

1
1 − β

¯xk −

β
1 − β

¯xk−1

Where k > 0. If k = 0 then ¯zk = ¯xk. For the rest of the analysis, the initial value will be directly set to 0.

Lemma 3. Deﬁne the sequence {¯zk}k≥0 as in Eq. 18. Based on CGA, we have the following relationship

¯zk+1 − ¯zk = −

α
1 − β

1
N

N
(cid:88)

i=1

˜gi
k.

(17)

(18)

(19)

Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

Proof. Using mathematical induction we have:

k = 0 :

¯zk+1 − ¯zk = ¯z1 − ¯z0 =

1
1 − β

¯x1 −

β
1 − β

¯x0 − ¯x0 =

1
1 − β

(¯x1 − ¯x0) =

1
1 − β

(¯v1) =

−α
N (1 − β)

N
(cid:88)

i=1

˜gi
0

k ≥ 1 :

¯zk+1 − ¯zk =

1
1 − β

¯xk+1 −

β
1 − β

¯xk −

1
1 − β

¯xk +

β
1 − β

¯xk−1 =

(20)

1
1 − β

((¯xk+1 − ¯xk) − (β(¯xk − ¯xk−1))) =

1
1 − β

(¯vk+1 − β(¯vk))
(cid:123)(cid:122)
(cid:125)
(cid:124)
(cid:80)N

i=1 ˜gi
k

−α 1
N

=

−α
N (1 − β)

N
(cid:88)

i=1

˜gi
k

Lemma 4. Deﬁne respectively the sequence {¯xk}k≥0 as in Eq. 17 and the sequence {¯zk}k≥0 as in Eq. 18. For all K ≥ 1,
CGA ensures the following relationship

K−1
(cid:88)

k=0

(cid:107)¯zk − ¯xk(cid:107)2 ≤

α2β2
(1 − β)4

K−1
(cid:88)

k=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

N
(cid:88)

i=1

˜gi
k

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

.

Proof. As ¯v0 = 0, we can apply 17 recursively to achieve an update rule for ¯vk. Therefor, we have :

Also, based on Eq. 18 we have:

¯vk = −α

k−1
(cid:88)

τ =0

βk−1−τ

(cid:35)

(cid:34)

1
N

N
(cid:88)

i=1

˜gi
τ

∀k ≥ 1

¯zk − ¯xk =

β
1 − β

[¯xk − ¯xk−1] =

β
1 − β

¯vk

Based on Equations 22 and 23 we have:

¯zk − ¯xk =

−αβ
1 − β

k−1
(cid:88)

τ =0

βk−1−τ

(cid:35)

(cid:34)

1
N

N
(cid:88)

i=1

˜gi
τ

∀k ≥ 1

We deﬁne sk = (cid:80)k−1

τ =0 βk−1−τ = 1−βk

1−β

∀k ≥ 1. We have:

||¯zk − ¯xk||2 =

α2β2
(1 − β)2 s2

k

k−1
(cid:88)

τ =0

k−1
(cid:88)

τ =0

k

(cid:13)
α2β2
(cid:13)
(1 − β)2 s2
(cid:13)
(cid:13)
(cid:34)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

βk−1−τ
sk

1
N

βk−1−τ
sk
(cid:35)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

˜gi
τ

N
(cid:88)

i=1

(cid:34)

1
N

N
(cid:88)

i=1

˜gi
τ

2 JensenInequality
≤

(cid:35)(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

α2β2(1 − βk)
(1 − β)3

k−1
(cid:88)

τ =0

βk−1−τ

(cid:34)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

(cid:35)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

˜gi
τ

N
(cid:88)

i=1

≤

α2β2
(1 − β)3

k−1
(cid:88)

τ =0

βk−1−τ

(cid:34)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

(cid:35)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

˜gi
τ

N
(cid:88)

i=1

(21)

(22)

(23)

(24)

(25)

Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

Setting K ≥ 1, As ¯z0 − ¯x0 = 0, by summing Eq. 25 over k ∈ {1, 2, . . . , K − 1}:

K−1
(cid:88)

k=0

||¯zk − ¯xk||2 ≤

α2β2
(1 − β)3

K−1
(cid:88)

k−1
(cid:88)

βk−1−τ

(cid:34)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

(cid:35)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

˜gi
τ

N
(cid:88)

i=1

τ =0

2 K−1
(cid:88)

k=1
(cid:35)(cid:13)
(cid:13)
(cid:13)
(cid:13)

˜gi
τ

l=τ +1

=

α2β2
(1 − β)3

α2β2
(1 − β)4

K−2
(cid:88)

(cid:34)

(cid:18)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

N
(cid:88)

τ =0
(cid:34)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K−2
(cid:88)

τ =0

1
N

N
(cid:88)

i=1

i=1
(cid:35)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

˜gi
τ

≤

α2β2
(1 − β)4

(cid:34)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

K−1
(cid:88)

τ =0

N
(cid:88)

i=1

(cid:35)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

˜gi
τ

βl−1−τ

(cid:19) a
≤

(26)

Here (a) refers to (cid:80)K−1

l=τ +1 βl−1−τ = 1−βK−1−τ

1−β

≤ 1

1−β .

Before proceeding to prove Lemma 1, we introduce some key notations and facts that serve to characterize the lemma.

We deﬁne the following notations:

˜Gk (cid:44) [˜g1
k, ˜g2
Vk (cid:44) [v1
k, v2
Xk (cid:44) [x1
k, x2
Gk (cid:44) [g1
k, g2
Hk (cid:44) [∇f1(x1

k, ..., ˜gN
k ]
k, ..., vN
k ]
k, ..., xN
k ]
k, ..., gN
k ]
k), ∇f2(x2

k), ..., ∇fN (xN

k )]

(27)

We can observe that the above matrices are all with dimension d × N such that any matrix A satisﬁes (cid:107)A(cid:107)2
where ai is the i-th column of the matrix A. Thus, we can obtain that:

F = (cid:80)N

i=1 (cid:107)ai(cid:107)2,

(cid:107)Xk(I − Q)(cid:107)2

F =

N
(cid:88)

i=1

(cid:107)xi

k − ¯xk(cid:107)2.

Fact 1. Deﬁne Q = 1

N 11(cid:62). For each doubly stochastic matrix Π, the following properties can be obtained

• QΠ = ΠQ;

• (I − Q)Π = Π(I − Q);

• For any integer k ≥ 1, (cid:107)(I − Q)Π(cid:107)S ≤ (

√

ρ)k, where (cid:107) · (cid:107)S is the spectrum norm of a matrix.

Fact 2. Let Ai, i ∈ {1, 2, ..., N } be N arbitrary real square matrices. It follows that

(cid:107)

N
(cid:88)

i=1

Ai(cid:107)2

F ≤

N
(cid:88)

N
(cid:88)

i=1

j=1

(cid:107)Ai(cid:107)F(cid:107)Aj(cid:107)F.

(28)

(29)

The properties shown in Facts 1 and 2 have been well established and in this context, we skip the proof here. We are now
ready to prove Lemma 1.

Proof. Since Xk = Xk−1Π + Vk we have:

Xk(I − Q) = Xk−1(I − Q)Π + Vk(I − Q)

(30)

Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

Applying the above equation k times we have:

Xk(I − Q) = X0(I − Q)Πk +

k
(cid:88)

τ =1

Vτ (I − Q)Πk−τ X0=0=

k
(cid:88)

τ =1

Vτ (I − Q)Πk−τ

(31)

As ¯Vk = β ¯Vk−1 − α 1
N

(cid:80)N

i=1

˜Gi

k−1

V0=0= −α 1
N

(cid:80)N

i=1

˜Gi

k−1, we can get:

Xk(I − Q) = −α

k
(cid:88)

τ −1
(cid:88)

τ =1

l=0

˜Glβτ −1−l(I − Q)Πk−τ = −α

k
(cid:88)

τ −1
(cid:88)

τ =1

l=0

˜Glβτ −1−lΠk−τ −l(I − Q)

− α

k−1
(cid:88)

n=1

k
(cid:88)

˜Gn[

l=n+1

βl−1−nΠk−1−n(I − Q) = −α

k−1
(cid:88)

τ =0

1 − βk−τ
1 − β

˜Gτ (I − Q)Πk−1−τ .

(32)

Therefore, for k ≥ 1, we have:

E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

Xk(I − Q)

(cid:21)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

= α2E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

k−1
(cid:88)

τ =0

1 − βk−τ
1 − β

˜Gτ (I − Q)Πk−1−τ

(cid:21)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

a
≤ 2α2E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:124)

k−1
(cid:88)

τ =0

1 − βk−τ
1 − β

( ˜Gτ − Gτ )(I − Q)Πk−1−τ

(cid:123)(cid:122)
I

(cid:21)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

+ 2α2E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:125)

(cid:124)

k−1
(cid:88)

τ =0

1 − βk−τ
1 − β

Gτ (I − Q)Πk−1−τ

(cid:123)(cid:122)
II

(33)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

(cid:21)

(cid:125)

(a) follows from the inequality (cid:107)A + B(cid:107)2

F ≤ 2(cid:107)A(cid:107)2

F + 2(cid:107)B(cid:107)2
F.

We develop upper bounds of term I:

E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

k−1
(cid:88)

τ =0

1 − βk−τ
1 − β

( ˜Gτ − Gτ )(I − Q)Πk−1−τ

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

(cid:21) a
≤

k−1
(cid:88)

τ =0

E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1 − βk−τ
1 − β

( ˜Gτ − Gτ )(I − Q)Πk−1−τ

b
≤

1
(1 − β)2

k−1
(cid:88)

τ =0

ρk−1−τ E

(cid:20)(cid:13)
(cid:13)
˜Gτ − Gτ
(cid:13)
(cid:13)

(cid:21) c
≤

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

1
(1 − β)2

k−1
(cid:88)

τ =0

ρk−1−τ N (cid:15)2

d
≤

N (cid:15)2
(1 − β)2(1 − ρ)

(cid:21)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

(34)

(a) follows from Jensen inequality. (b) follows from the inequality | 1−βk−τ
Frobenius norm. (d) follows from Assumption 4.

1−β | ≤ 1

1−β . (c) follows from Assumption 3 and

We then proceed to ﬁnd the upper bound for term II.

E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

k−1
(cid:88)

τ =0

1 − βk−τ
1 − β

Gτ (I − Q)Πk−1−τ

(cid:21) a
≤

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

k−1
(cid:88)

k−1
(cid:88)

τ =0

τ (cid:48)=0

E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

1 − βk−τ
1 − β

Gτ (I − Q)Πk−1−τ

(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:21) b
≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1 − βk−τ
1 − β

1
(1 − β)2

Gτ (cid:48)(I − Q)Πk−1−τ (cid:48)(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:18) 1
2

ρ(k−1− τ +τ (cid:48)

k−1
(cid:88)

)

2

k−1
(cid:88)

τ =0

τ (cid:48)=0

(cid:21)

≤

1
(1 − β)2

k−1
(cid:88)

k−1
(cid:88)

ρ(k−1− τ +τ (cid:48)

2

(cid:20)
(cid:107)Gτ (cid:107)F(cid:107)Gτ (cid:48)(cid:107)F

)E

τ =0

τ (cid:48)=0

E[(cid:107)Gτ (cid:107)2

F] +

(cid:19)

E[(cid:107)Gτ (cid:48)(cid:107)2
F]

1
2

=

1
(1 − β)2

k−1
(cid:88)

k−1
(cid:88)

τ =0

τ (cid:48)=0

ρ(k−1− τ +τ (cid:48)

2

)E[(cid:107)Gτ (cid:107)2
F]

(35)

c
≤

1

(1 − β)2(1 −

√

ρ)

k−1
(cid:88)

τ =0

ρ( k−1−τ

2

)E[(cid:107)Gτ (cid:107)2
F]

Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

(a) follows from Fact 2. (b) follows from the inequality xy ≤ 1
(cid:80)k−1

τ1=0 ρk−1− τ1+τ

2 ≤ ρ

k−1−τ
2
√

1−

ρ .

2 (x2 + y2) for any two real numbers x, y. (c) is derived using

We then proceed with ﬁnding the bounds for E[(cid:107)Gτ (cid:107)2

F]:

E[(cid:107)Gτ (cid:107)2

F] = E[(cid:107)Gτ − Hτ + Hτ − Hτ Q + Hτ Q(cid:107)2
F]

≤ 3E[(cid:107)Gτ − Hτ (cid:107)2

F] + 3E[(cid:107)Hτ (I − Q)(cid:107)2F] + 3E[(cid:107)Hτ Q(cid:107)2
F]

a
≤ 3N σ2 + 3N δ2 + 3E[(cid:107)

1
N

N
(cid:88)

i=1

∇fi(xi

τ )(cid:107)2]

(36)

(a) holds because E[(cid:107)Hτ Q(cid:107)2

F] ≤ E[(cid:107) 1

N

(cid:80)N

i=1 ∇fi(xi

τ )(cid:107)2]

Substituting (36) in (35):

E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

k−1
(cid:88)

τ =0

1 − βk−τ
1 − β

Gτ (I − Q)Πk−1−τ

(cid:21)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

≤

1

(1 − β)2(1 −

√

ρ)

k−1
(cid:88)

τ =0

(cid:20)

ρ( k−1−τ

2

)

3N σ2 + 3N δ2 + 3E[(cid:107)

1
N

N
(cid:88)

i=1

∇fi(xi

(cid:21)
τ )(cid:107)2]

≤

3N (σ2 + δ2)
√

(1 − β)2(1 −

ρ)2 +

3N

(1 − β)2(1 −

√

ρ)

k−1
(cid:88)

τ =0

ρ( k−1−τ

2

)E[(cid:107)

1
N

N
(cid:88)

i=1

∇fi(xi

τ )(cid:107)2]

substituting (37) and (34) into the main inequality (33):

(cid:21)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

≤

1
N

E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

Xk(I − Q)

2α2N (cid:15)2
(1 − β)2(1 − ρ)

+

2α2

(1 − β)2(1 −

3N

k−1
(cid:88)

τ =0

ρ( k−1−τ

2

)E[(cid:107)

N
(cid:88)

i=1

∇fi(xi

τ )(cid:107)2]

(cid:19)

=

2α2
(1 − β)2

6N α2
(1 − β)2(1 −

√

ρ)

k−1
(cid:88)

τ =0

ρ( k−1−τ

2

)E[(cid:107)

1
N

N
(cid:88)

i=1

∇fi(xi

τ )(cid:107)2]

√

ρ)
(cid:18) N (cid:15)2
1 − ρ

(37)

(cid:19)

+

(38)

(cid:18) 3N (σ2)
√
ρ
1 −

+

3N (δ2)
√
ρ
1 −

+

+

3N σ2
√

(1 −

ρ)2 +

3N δ2
√

(1 −

ρ)2

Summing over k ∈ {1, . . . , K − 1} and noting that E

X0(I − Q)

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:21)

= 0:

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

K−1
(cid:88)

k=1

E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

Xk(I − Q)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

(cid:21)

≤ CK +

6N α2
(1 − β)2(1 −

√

ρ)

K−1
(cid:88)

k−1
(cid:88)

k=1

τ =0

ρ( k−1−τ

2

)E[(cid:107)

1
N

N
(cid:88)

i=1

∇fi(xi

τ )(cid:107)2] ≤

CK +

6N α2
(1 − β)2(1 −

√

ρ)

CK +

6N α2
(1 − β)2(1 −

√

ρ)

K−1
(cid:88)

k=0

K−1
(cid:88)

k=0

1 − ρ( K−1−k
1 −

2
√

ρ

)

E[(cid:107)

1
N

N
(cid:88)

i=1

∇fi(xi

k)(cid:107)2] ≤

(39)

E[(cid:107)

1
N

N
(cid:88)

i=1

∇fi(xi

k)(cid:107)2]

Where C = 2α2
(1−β)2

(cid:18)

1−ρ + 3N σ2
N (cid:15)2

(1−

ρ)2 + 3N δ2

(1−

√

√

ρ)2

(cid:19)
.

Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

Dividing both sides by N :

K−1
(cid:88)

k=1

1
N

E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

Xk(I − Q)

(cid:21)

≤

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

2α2
(1 − β)2

(cid:18) (cid:15)2

1 − ρ

+

3σ2
√

(1 −

ρ)2 +

3δ2
√

(1 −

ρ)2

(cid:19)

K +

6α2

(1 − β)2(1 −

√

ρ)

K−1
(cid:88)

k=0

E[(cid:107)

1
N

N
(cid:88)

i=1

∇fi(xi

k)(cid:107)2]

We immediately have:

N
(cid:88)

E

(cid:20)(cid:13)
(cid:13)
¯xk − xi
(cid:13)
k
(cid:13)

2(cid:21)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

K−1
(cid:88)

k=0

1
N

2α2
(1 − β)2

i=1
(cid:18) (cid:15)2

1 − ρ

+

3σ2
√

(1 −

ρ)2 +

3δ2
√

(1 −

ρ)2

(cid:19)

K +

6α2

(1 − β)2(1 −

√

ρ)

K−1
(cid:88)

k=0

E[(cid:107)

1
N

N
(cid:88)

i=1

∇fi(xi

k)(cid:107)2]

A.2. Proof for Theorem 1

Proof. Using the smoothness properties for F we have:

E[F(¯zk+1)] ≤ E[F(¯zk)] + E[(cid:104)∇F(¯zk), ¯zk+1 − ¯zk(cid:105)] +

L
2

E[(cid:107)¯zk+1 − ¯zk(cid:107)2]

Using Lemma 3 we have:

E[(cid:104)∇F(¯zk), ¯zk+1 − ¯zk(cid:105)] =

−α
1 − β

E[(cid:104)∇F(¯zk),

1
N

N
(cid:88)

i=1

˜gi
k(cid:105)] =

−α
1 − β
(cid:124)

E[(cid:104)∇F(¯zk) − ∇F(¯xk),

(cid:123)(cid:122)
I

1
N

N
(cid:88)

i=1

(˜gi

k(cid:105)]

−

(cid:125)

α
1 − β
(cid:124)

E[(cid:104)∇F(¯xk),

1
N

(cid:123)(cid:122)
II

N
(cid:88)

(˜gi

k(cid:105)]

i=1

(cid:125)

We proceed by analysing (I):

−α
1 − β

E[(cid:104)∇F(¯zk) − ∇F(¯xk),

1
N

N
(cid:88)

(˜gi

k(cid:105)] ≤

i=1

(1 − β)
2βL

E[(cid:107)∇F(¯zk) − ∇F(¯xk)(cid:107)2] +

βLα2
2(1−β)3

E[(cid:107)

1
N

N
(cid:88)

i=1

k(cid:107)2] ≤
˜gi

(1 − β)L
2β

E[(cid:107)¯zk − ¯xk(cid:107)2] +

βLα2
2(1 − β)3

E[(cid:107)

1
N

N
(cid:88)

i=1

k(cid:107)2]
˜gi

(40)

(41)

(42)

(43)

(44)

Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

For term (II) we have:

(cid:104)∇F (¯xk) ,

(cid:104)∇F (¯xk) ,

(cid:124)

1
N

1
N

N
(cid:88)

i=1

N
(cid:88)

i=1
(cid:123)(cid:122)
(cid:63)

˜gi
k(cid:105) = (cid:104)∇F (¯xk) ,

1
N

N
(cid:88)

i=1

(cid:0)˜gi

k − gi

k + gi
k

(cid:1)(cid:105) =

(cid:0)˜gi

k − gi
k

(cid:1)(cid:105)

+ (cid:104)∇F (¯xk) ,

1
N

(cid:125)

(cid:124)

(cid:123)(cid:122)
(cid:63)(cid:63)

N
(cid:88)

i=1

˜gi
k(cid:105)

(cid:125)

We ﬁrst analyse ((cid:63)):

−α
(1 − β)

E[(cid:104)∇F (¯xk) ,

1
N

N
(cid:88)

i=1

(cid:0)˜gi

k − gi
k

(cid:1)(cid:105)] ≤

(1 − β)α2
2βL

E[(cid:107)∇F(¯xk)(cid:107)2] +

βL
2(1 − β)3

E[(cid:107)

1
N

N
(cid:88)

(˜gi

k − gi

k)(cid:107)2]

i=1

This holds as (cid:104)a, b(cid:105) ≤ 1

2 (cid:107)a(cid:107)2 + 1

2 (cid:107)b(cid:107)2 where a = −α

1−β

√
βL ∇F(¯xk) and b = −

√

βL

(1−β)

3
2

1
N

(cid:80)N

i=1(˜gi

k − gi

k).

Analysing ((cid:63)(cid:63)):

(cid:20)
(cid:104)∇F (¯xk) ,

E

1
N

N
(cid:88)

(cid:21)
˜gi
k(cid:105)

(cid:20)
= E
(cid:104)∇F(¯xk),

i=1

1
N

N
(cid:88)

i=1

(cid:21)

∇fi(xi

k)(cid:105)

(45)

(46)

(47)

The above equality holds because ¯xk and xi
E[gi

k|ζk−1] = E[gi

k] = ∇fi(xi

k are determined by ζk−1 = [ζ0, . . . , ζk−1] which is independent of ζk, and

k). With the aid of the equity (cid:104)a, b(cid:105) = 1

2 [(cid:107)a(cid:107)2 + (cid:107)b(cid:107)2 − (cid:107)a − b(cid:107)2], we have :

(cid:104)∇F (¯xk) ,

1
N

N
(cid:88)

i=1

∇fi

(cid:0)xi

k

(cid:1)(cid:105) =

1
2

(cid:32)

(cid:107)∇F (¯xk) (cid:107)2 + (cid:107)

1
N

N
(cid:88)

i=1

∇fi(xi

k)(cid:107)2 − (cid:107)∇F(¯xk) −

(cid:32)

(cid:107)∇F(¯xk)(cid:107)2 + (cid:107)

1
2

1
N

N
(cid:88)

i=1

∇fi(xi

k)(cid:107)2 − L2 1
N

N
(cid:88)

i=1

(cid:33)

(cid:107)¯xk − xi

k(cid:107)2

∇fi(xi

k)(cid:107)2

(cid:33)

a
≥

1
N

N
(cid:88)

i=1

(48)

(a) follows because (cid:107)∇F(¯xk)− 1
N
∇fi(xi

(cid:80)N
k(cid:107)2.
Substituting (48) into (47) and (46), (47) into (45) and (44), (45) into (43):

i=1 L2(cid:107)¯xk − xi

k)(cid:107)2 = (cid:107) 1
N

i=1 ∇fi(xi

k)(cid:107)2 ≤ 1
N

(cid:80)N

(cid:80)N

i=1 ∇fi(¯xk)− 1
N

(cid:80)N

i=1 ∇fi(xi

k)(cid:107)2 ≤ 1
N

(cid:80)N

i=1 (cid:107)∇fi(¯xk)−

E[(cid:104)∇F(¯zk), ¯zk+1 − ¯zk(cid:105)] ≤

(1 − β)L
2β

E[(cid:107)¯zk − ¯xk(cid:107)2] +

βLα2
2(1 − β)3

E[(cid:107)

1
N

N
(cid:88)

i=1

(˜gi

k)(cid:107)2] +

(cid:18) (1 − β)α2
2βL

−

α
2(1 − β)

(cid:19)

E[(cid:107)∇F(¯xk)(cid:107)2] −

α
2(1 − β)

E[(cid:107)

1
N

N
(cid:88)

i=1

∇fi(xi

k)(cid:107)2] +

βL
2(1 − β)3

E[(cid:107)

1
N

N
(cid:88)

(˜gi

k − gi

k)(cid:107)2] +

i=1

Lemma 3 states that:

E[(cid:107)¯zk+1 − ¯zk(cid:107)2] =

α2
(1 − β)2

E[(cid:107)

1
N

N
(cid:88)

i=1

k(cid:107)2].
˜gi

αL2
2(1 − β)

1
N

N
(cid:88)

i=1

E[(cid:107)¯xk − xi

k(cid:107)2]

(49)

(50)

Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

Substituting (49),(50) in (42):

E[F(¯zk+1)] ≤ E[F(¯zk)] +

(1 − β)L
2β

E[(cid:107)¯zk − ¯xk(cid:107)2] +

βLα2
2(1 − β)3

E[(cid:107)

1
N

N
(cid:88)

i=1

(˜gi

k)(cid:107)2] +

(cid:18) (1 − β)α2
2βL

−

α
2(1 − β)

(cid:19)

E[(cid:107)∇F(¯xk)(cid:107)2] −

α
2(1 − β)

E[(cid:107)

1
N

N
(cid:88)

i=1

∇fi(xi

k)(cid:107)2] +

βL
2(1 − β)3

E[(cid:107)

1
N

N
(cid:88)

i=1

αL2
2(1 − β)

1
N

N
(cid:88)

i=1

E[(cid:107)¯xk − xi

k(cid:107)2] +

α2
(1 − β)2

E[(cid:107)

1
N

N
(cid:88)

i=1

k(cid:107)2].
˜gi

(˜gi

k − gi

k)(cid:107)2]+

Rearranging the terms and dividing by C1 = α

2(1−β) − (1−β)α2

2βL to ﬁnd the bound for E[(cid:107)∇F(¯xk)(cid:107)2]:

E[(cid:107)∇F(¯xk)(cid:107)2] ≤

1
C1

(cid:18)

E[F(¯zk)] − E[F(¯zk+1)]

(cid:19)

+ C2 E[(cid:107)

1
N

N
(cid:88)

(˜gi

k)(cid:107)2] + C3 E[(cid:107)¯zk − ¯xk(cid:107)2]

i=1

− C6 E[(cid:107)

1
N

N
(cid:88)

i=1

∇fi(xi

k)(cid:107)2] + C4 E[(cid:107)

1
N

N
(cid:88)

i=1

(˜gi

k − gi

k)(cid:107)2] + C5

N
(cid:88)

i=1

E[(cid:107)¯xk − xi

k(cid:107)2]

Where C2 =

(cid:16) βLα2
2(1−β)3 + α2L

(1−β)2

(cid:17)

/C1, C3 = (1−β)L

2β

Summing over k ∈ {0, 1, . . . , K − 1}:

/C1, C4 = βL

2(1−β)3 /C1, C5 = αL2

2(1−β) /C1, C6 = α

2(1−β) /C1.

(51)

(52)

K−1
(cid:88)

k=0

E

(cid:104)

(cid:107)∇F (¯xk)(cid:107)2(cid:105)

≤

(cid:18)

1
C1

E [F (¯z0)] − E [F (¯zk)]

(cid:19)

− C6

k−1
(cid:88)

E

k=0





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

N
(cid:88)

i=1

∇fi

(cid:1)

(cid:0)xi

k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

 + C2

k−1
(cid:88)

E

k=0





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

N
(cid:88)

i=1

˜gi
k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2



+ C3

k−1
(cid:88)

k=0

E

(cid:104)

(cid:107)¯zk − ¯xk(cid:107)2(cid:105)

+ C4

k−1
(cid:88)

E

k=0





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

N
(cid:88)

i=1

(cid:0)˜gi

k − gi
k

(cid:1)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

 + C5

k−1
(cid:88)

k=0

1
N

N
(cid:88)

l=1

E

(cid:104)(cid:13)
(cid:13)¯xk − xi
k

2(cid:105)

(cid:13)
(cid:13)

(53)

Substituting Lemma 1, Lemma 2, and Lemma 4 and Assumption 3 into the above equation we have:

K−1
(cid:88)

E

(cid:104)

(cid:107)∇F (¯xk)(cid:107)2(cid:105)

≤

k=0

k−1
(cid:88)

k=0

E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

N
(cid:88)

i=1

∇fi

(cid:0)xi

k

3σ2
√

(1 −

ρ)2 +

3δ2
√

(1 −

ρ)2

Dividing both sides by K:

1
C1

(cid:18)

E [F (¯z0)] − E [F (¯zk)]

(cid:19)

(cid:18)

−

C6 − C5

6α2

(1 − β)(1 −

√

ρ)

− 2C2 − 2C3

(cid:19)

α2β2
(1 − β)4

2

(cid:18)

 +

C2 + C3

(cid:1)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:19)

K

α2β
(1 − β)4

(cid:19)(cid:18) 2σ2
N

(cid:19)

+ 2(cid:15)2

K + C4(cid:15)2K + C5

2α2
(1 − β)2

(cid:18) (cid:15)2

1 − ρ

+

(54)

1
K

K−1
(cid:88)

k=0

E

(cid:104)

(cid:107)∇F (¯xk)(cid:107)2(cid:105)

≤

1
C1K

(cid:18)

F (¯x0) − F (cid:63)

(cid:19)

(cid:18)

+

C2 + C3

α2β
(1 − β)4

(cid:19)(cid:18) 2σ2
N

(cid:19)

+ 2(cid:15)2

+ C4(cid:15)2+

C5

2α2
(1 − β)2

(cid:18) (cid:15)2

1 − ρ

+

3σ2
√

(1 −

ρ)2 +

3δ2
√

(1 −

ρ)2

(cid:19)

K

(55)

Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

The above follows from the fact that ¯z0 = ¯x0 and

(cid:18)

C6 − C5

6α2

(1−β)(1−

√

ρ) − 2C2 − 2C3

α2β2
(1−β)4

(cid:19)

≥ 0.

Therefor we have :
K−1
(cid:88)

(cid:104)

E

(cid:107)∇F (¯xk)(cid:107)2(cid:105)

1
K
(cid:18) 2
N

k=0
(cid:18)

1
C1K

≤

(cid:19)

+ C5

C2 + C3

α2β
(1 − β)4

6α2

(cid:19)

√

p)2

σ2 + C5

(1 − β)2(1 −

(F (¯x0) − F ∗) +

(cid:18)

2C2 + C3

α2β

(1 − β)4 + C4 + C5
6α2

(1 − β)2(1 −

√

ρ)2 δ2

2α2
(1 − β)2(1 − ρ)

(cid:19)

(cid:15)2+

(56)

A.3. Discussion on the Step Size

Recalling the conditions for the step size α in Theorem 1,
6α2L2
(1 − β)(1 −

1 −

√

ρ)2 −

4Lα

(1 − β)2 ≥ 0.

Solving the last inequality, combining the fact that α > 0, we have then the speciﬁc form of α∗

(1 −

√

ρ)(cid:112)16(1 −

α∗ =

√

ρ)2 + 24(1 − β)3 − 4(1 −
12L(1 − β)

√

ρ)2

.

Therefore, if the step size α is deﬁned as

α ≤ min

(cid:40)

βL
(1 − β)2 ,

(1 −

√

ρ)(cid:112)16(1 −

√

ρ)2 + 24(1 − β)3 − 4(1 −
12L(1 − β)

√

ρ)2

(cid:41)
,

Eq. 56 naturally holds true.

A.4. Proof for Corollary 1

Proof. According to Eq. 56, on the right hand side, there are four terms with different coefﬁcients with respect to the step
size α. We separately investigate each term in the following. As C1 = O(

). Therefore,

√
N√
K

F(¯x0) − F ∗
C1K

= O(

√

1
N K

).

While for the second term, we have

C2 = O(

√
√

N
K

), C3 = O(

√
√

K
N

), C4 = O(

√
√

K
N

), C5 = O(1),

such that

2C2(cid:15)2 = O(

√
N
K 1.5 ), C3

α2β
(1 − β)4 (cid:15)2 = O(

N

√
K 1.5 ), C4(cid:15)2 = O(

√

1
N K

), C5

2α2
(1 − β)2(1 − ρ)

(cid:15)2 = O(

N
K 2 ).

Similarly, we can obtain for the third term and the last term,

(cid:32)

C2 + C3

2
N

α2β
(1 − β)4

(cid:33)

σ2 = O(

√

1
N K

), C5

6α2

(1 − β)2(1 −

√

ρ)2 σ2 = O(

N
K

),

and

C5

6α2

(1 − β)2(1 −

√

ρ)2 δ2 = O(

N
K

).

Hence, By omitting the constant N in this context, there exists a constant C > 0 such that the overall convergence rate is as
follows:

1
K

K−1
(cid:88)

k=0

E

(cid:107)∇F (¯xk)(cid:107)2(cid:105)
(cid:104)

≤ C

√

(cid:32)

1
N K

+

1
K

+

1
K 1.5 +

1
K 2

(cid:33)
,

(57)

which suggests when N is ﬁxed and K is sufﬁciently large, CGA enables the convergence rate of O(

1√

N K

).

Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

A.5. Additional CIFAR-10 Results

In this section, we provide more experimental results for CIFAR10 dataset trained using a CNN architecture and more
complex VGG11 model architecture:

Additional CIFAR10 results trained using CNN :

We start by providing the corresponding accuracy plots for Figure 2 in the main paper:

(a)

(b)

(c)

Figure 5. Average training and validation accuracy for (a) CGA method on IID (b) CGA method on non-IID data distributions (c) different
methods on non-IID data distributions for training 5 agents using CNN model architecture

(a)

(b)

Figure 6. Average training loss for (a) different topologies trained using CGA algorithm (b) individual agents along with the average
model during training using CGA algorithm (log scale)

Based on Figure 5(a), (b) CGA achieves a high accuracy for different graph topologies when learning from both IID and
non-IID data distributions. However other methods i.e. DPMSGD suffer from maintaining the high accuracy when learning
from non-IID data distributions. The adverse effect of non-IIDness in the data can be more elaborated upon by looking at
Figure 7. Comparing (a) with (b) and (c) with (d) we can see that although the migration from IID to non-IID affects all
the methods, CGA suffers less than other methods for different combinations of graph topology and graph type. The same
observation can be made by looking at Figure 8 which shows the accuracy obtained for different methods w.r.t the graph
type.

While Figure 2(a) harps on the phenomenon of faster convergence with sparser graph topology which is an observation that
have been made by earlier research works in Federated Learning (McMahan et al., 2017) by reducing the client fraction
which makes the mixing matrix sparser and decentralized learning (Jiang et al., 2017). However, as Figure 6(a) shows,
by training for more epochs, all converge to similar loss values. Figure 6 shows that the loss value associated with the

020406080100120140Number of Epochs5060708090100Accuracy(%)Fully-ConnectedRingBipartite0255075100125150175Number of Epochs30405060708090100Testing Accuracy(%)Fully-ConnectedRingBipartite0255075100125150175Number of epochs20406080100Accuracy(%)CGADPMSGDSGPSwarmSGDCompCGA050100150200250Number of epochs0.00.20.40.60.81.01.21.4LossFully-ConnectedRingBipartite0255075100125150175Number of Epochs101100Loss ValueAverage Modelagent 1agent 2agent 3agent 4agent 5Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

consensus model is very close to the loss values corresponding to all other agents which means the projected gradient using
QP is capturing the correct direction.

(a)

(b)

(c)

(d)

Figure 7. Average testing accuracy for different methods w.r.t the number of learning agents learning from (a) IID data distributions for
Ring graph topology (b) non-IID data distributions for Ring graph topology (c) IID data distributions for Bipartite graph topology (d)
non-IID data distributions for Bipartite graph topology

(a)

(b)

(c)

(d)

Figure 8. Average testing accuracy for different methods w.r.t the graph topology learning from (a) IID data distributions learning from
10 agents (b) non-IID data distributions learning from 10 agents(c) IID data distributions learning from 40 agents (d) non-IID data
distributions learning from 40 agents

CIFAR10 with VGG11:

We now extend our experimental analysis by using a more complex model architecture (e.g. VGG11) for CIFAR10 dataset.
Tables 4 and 5 summarize the performance of CGA compared to other methods. Similar to CNN model architecture, CGA
can maintain the performance when migrating from IID to non-IID data distributions. However, we observe that as VGG11
model is much more complex than CNN, all the methods suffer from an increase in the number of learning agents and
complexity of graph topology.

A.6. MNIST Results

Same as what we did for CIFAR-10, we are comparing different methods performance on MNIST dataset. The results are
summarized in Tables 6 and 7. Although the accuracies are generally high when learning from MNIST dataset, and most of
the methods work in most of the settings, we can see that although CGA can maintain the model performance while learning
from non-IID data, DPMSGD, SGP and SwarmSGD suffer from non-IIDness in the data specially when the number of
agents and the graph topology combinations become more complex.

Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

Table 4. Model Accuracy Comparison for training CIFAR10 using VGG11 with IID data distribution

Model

Fully-connected Ring

Bipartite

DPMSGD

SGP

SwarmSGD

CGA (ours)

67.8% (5)
60.8% (10)
59.8% (40)
72.5% (5)
70.3% (10)
41.1% (40)
75.8% (5)
71.5% (10)
21.8% (40)
81.1% (5)
68.8% (10)
21.9% (40)

61.9% (5)
60.5% (10)
60.1% (40)
72.0% (5)
42.8% (10)
41.6% (40)
73.1% (5)
71.4% (10)
20.6% (40)
81.8% (5)
68.3% (10)
18.5% (40)

61.0% (5)
60.7% (10)
60.1% (40)
71.1% (5)
70.2% (10)
41.5% (40)
78.3% (5)
70.1% (10)
20.3% (40)
81.5% (5)
68.2% (10)
20.3% (40)

Table 5. Model Accuracy Comparison for training CIFAR10 with non-IID data distribution using VGG11

Model

Fully-connected Ring

Bipartite

DPMSGD

SGP

SwarmSGD

CGA (ours)

Diverges (5)
Diverges (10)
12% (40)
20.4% (5)
10.1% (10)
Diverges (40)
19.4% (5)
10.0% (10)
9.9% (40)
74.6% (5)
69.8% (10)
12.8% (40)

Diverges (5)
Diverges (10)
Diverges (40)
20.8% (5)
10.0% (10)
10.0% (40)
19.9% (5)
Diverges (10)
10.2% (40)
75.8% (5)
38.9% (10)
20.5% (40)

Diverges (5)
10% (10)
10.7% (40)
20.3% (5)
Diverges (10)
10.1% (40)
20.2% (5)
Diverges (10)
10% (40)
77.5% (5)
18.7% (10)
23.6% (40)

Table 6. Model Accuracy Comparison for training MNIST using CNN with IID data distribution

Model

Fully-connected Ring

Bipartite

DPSGD

SGP

SwarmSGD

CGA (ours)

98.8% (5)
98.6% (10)
96.9% (40)
96.2% (5)
93.2% (10)
71.4% (40)
98.4% (5)
96.1% (10)
38.3% (40)
98.6 % (5)
98.2% (10)
94.7% (40)

98.8% (5)
98.5% (10)
96.8% (40)
96.3% (5)
93.2% (10)
71.4% (40)
98.4% (5)
96.1% (10)
38.3% (40)
98.7% (5)
98.3% (10)
95.5% (40)

98.8% (5)
98.5% (10)
96.8% (40)
96.2% (5)
93.2% (10)
71.4% (40)
98.5% (5)
96.0% (10)
39.7% (40)
98.7% (5)
98.6% (10)
96.8% (40)

Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data

Table 7. Model Accuracy Comparison for training MNIST with non-IID data distribution using CNN

Model

Fully-connected Ring

Bipartite

DPSGD

SGP

SwarmSGD

CGA (ours)

98.3% (5)
87.1% (10)
85.3% (40)
95.9% (5)
92.7% (10)
71.2% (40)
98.2% (5)
93.2% (10)
24.8% (40)
98.6% (5)
98.2% (10)
94.1% (40)

98.2% (5)
74.5% (10)
72.5% (40)
96.0% (5)
91.3% (10)
74.6% (40)
98.1% (5)
90.9% (10)
33.5% (40)
98.5% (5)
96.2% (10)
91.6% (40)

98.2% (5)
70.9% (10)
34.3% (40)
95.9% (5)
90.2% (10)
62.2% (40)
98.2% (5)
91.4% (10)
18.3% (40)
98.5% (5)
96.2% (10)
91.8% (40)

