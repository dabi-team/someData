9
1
0
2

c
e
D
0
1

]

G
L
.
s
c
[

2
v
0
0
2
5
0
.
6
0
9
1
:
v
i
X
r
a

1

Macro-action Multi-time scale
Dynamic Programming for Energy Management
in Buildings with Phase Change Materials
Zahra Rahimpour∗, Gregor Verbiˇc∗, Archie Chapman†
∗School of Electrical and Information Engineering, University of Sydney, NSW, Australia
†School of Information Technology and Electrical Engineering, University of Queensland, QLD, Australia
Email:{zahra.rahimpour, gregor.verbic}@sydney.edu.au, archie.chapman@uq.edu.au

Abstract—This paper focuses on energy management in build-
ings with phase change material (PCM), which is primarily
used to improve thermal performance, but can also serve as
an energy storage system. In this setting, optimal scheduling
of an HVAC system is challenging because of the nonlinear
and non-convex characteristics of the PCM, which makes solv-
ing the corresponding optimization problem using conventional
optimization techniques impractical. Instead, we use dynamic
programming (DP) to deal with the nonlinear nature of the PCM.
To overcome DP’s curse of dimensionality, this paper proposes
a novel methodology to reduce the computational burden, while
maintaining the quality of the solution. Speciﬁcally, the method
incorporates approaches from sequential decision making in
artiﬁcial intelligence, including macro actions and multi-timescale
Markov decision processes, coupled with an underlying state-
space approximation to reduce the state-space and action-space
size. The performance of the method is demonstrated on an
energy management problem for a typical residential building
located in Sydney, Australia. The results demonstrate that the
proposed method performs well with a computational speed-up
of up to 12,900 times compared to the direct application of DP.

Index Terms—Demand response, dynamic programming, home
energy management, macro actions, multi-timescale Markov
decision processes, thermal inertia, phase change materials.

I. INTRODUCTION

The signiﬁcant contribution of heating, ventilation, and air
conditioning (HVAC) in building energy use (up to 50 %
in certain countries) and the total contribution of buildings
to the overall energy consumption (20 %-40 %), make space
heating and cooling of growing importance in the area of
demand response (DR)1 [1]. A potential DR resource available
to householders is to use their building’s thermal inertia as
an energy storage system. However, lightweight buildings,
which dominate the residential building stock in Australia and
which are the focus of our paper, have low thermal inertia.
A promising solution to increase their thermal inertia is to
use materials with a high heat capacity, such as phase change
materials (PCM). Storing or releasing the latent heat during
the phase-change (from solid to liquid or vice versa) provides
the building with sufﬁcient thermal mass to smooth indoor
temperature ﬂuctuations.

1Demand response refers to methods for inﬂuencing end-users to use
available ﬂexible resources to support network and system services such as
load balancing, peak load shaving, and peak load shifting.

However, to exploit the energy storage capacity of PCM
cost-effectively, it needs to be either precooled or preheated
(depending on the season) by the HVAC system during shoul-
der or off-peak hours. This task can be cast as an optimal
HVAC scheduling problem, with an objective of minimiz-
ing electricity cost while maintaining the indoor temperature
within the desired comfort range. In the existing literature,
this type of optimization problem is classiﬁed as a home
energy management (HEM) problem [2]–[4]. In spite of the
ample literature on the use of PCM for improving thermal
performance of buildings [5]–[11], there is a palpable lack of
understanding on how to integrate PCM into HEM, where the
non-linear nature of its energy storage can be be exploited
using suitable scheduling methods.

To bridge this gap, and in contrast to much of the literature
on HEM [2]–[4], we consider HEM that consists of an HVAC
system as a controllable device and a PCM layer as an energy
storage system. To date, most HEM optimization problems
are solved using linear programming (LP) and mixed integer
linear programming (MILP). However, these methods cannot
be used to solve nonlinear optimization problems, which
phase-change characteristics impart. Other methods that are
widely used to solve the HEM problems are heuristic meth-
ods, such as particle swarm optimization (PSO) and genetic
algorithms (GA). The downside of using these methods is that
the solution may end up in a local optimum instead of the
global optimum, which means the quality of the solution is
uncertain [12]. More importantly, PSO and GA are black-box
optimization routines, and in our speciﬁc problem they rely
on the huge computational task of solving the initial value
problems associated with the ordinary differential equations
that govern the building’s thermal behavior. In this sense, they
provide no beneﬁt over using principled optimization methods
like dynamic programming (DP) [3], [13].

In this paper, DP is used as the state-of-the-art algorithm
for dealing with the nonlinear features of PCM. To solve
our problem using DP, we ﬁrst formulate it as a Markov
decision process (MDP), where the objective is to minimize
the accumulated instantaneous cost over a scheduling horizon.
The main operator in DP is the value function, which is formed
by summing the expected future costs of following a policy (in
this problem speciﬁc on/off sequence of the HVAC system),
given the state transition probabilities. Importantly, the objec-

 
 
 
 
 
 
tive is equivalent to computing the minimum value function
of the problem. To do so, the value iteration (VI) algorithm
is typically employed, which computes the minimum value
function in a backward fashion using the Bellman optimality
condition2. However, VI becomes intractable when the time-
horizon of the problem, the number of state variables or the
number of controllable devices grow. In the DP literature, this
is known as the curse of dimensionality [13].

As such, in the HVAC-PCM HEM problem, as in many se-
quential decision problems in artiﬁcial intelligence (AI), large
state-spaces and long time-horizons contribute to a consider-
able computational challenge. In response, the AI literature
contains many methods and frameworks for dealing with such
large or complex problems. Given this, in the next section,
we present a brief review of three existing methods from AI
that we use to build our computational methodology, namely
state-space approximation, multi-timescale Markov decision
processes and macro actions. It
the
cornerstone of all these methods is the concept of abstraction.
In general, an abstraction is a compact representation of the
original problem that is easier to work with than the ground
representation. Each abstraction method we use works in a
different way to reduce the complexity of the HVAC-PCM
scheduling problem.

is worth noting that

A. Review of three abstraction approaches in AI

We now review the three abstraction methods: state-space

approximations, multi-timescale MDPs, and macro actions.

State-space approximations — The foundation of our
method is to discretize the continuous state-space, which is a
standard approach to reducing its complexity [14]. However,
we still left with a large state-space, so we build on this using
the following two methods.

Multi-timescale Markov decision processes — The sec-
ond abstraction approach to reducing the state-space size is
to use a multi-timescale MDP, in which decisions are made
at different discrete timescales [15]. Speciﬁcally, rather than
solving the original MDP as one monolithic problem, we
solve several smaller MDPs that are connected successively
together to form the original MDP. The computation time
of the resulting algorithm depends on the choice of each
MDP’s length, and can be tuned for good performance. In our
problem, using a multi-timescales model reduces computation
time by up to 5,300 times.

Macro actions — The third approach is to use macro
actions to reduce the action-space size [16]. This approach
ﬁnds commonalities in the solutions across regions of the
state-space to create macro actions, which result in signiﬁcant
computational savings over using the primitive action-space.
In our methodology, we build on the multi-timescale MDP
and treat nearly-identical policies as equivalent. This itself
reduces the number of policies to consider by one third and
speeds up the performance of the algorithm further by 2.4
times. However, the major limitation of applying macro actions

2An optimal policy has the property that whatever the initial state and
initial decision are, the remaining decisions must constitute an optimal policy
with regard to the state resulting from the ﬁrst [13].

2

is the quality of the solution. Certain policies cannot be
captured since, in the macro-action MDP, the policies contain
only macro actions. Therefore, the resulting policy may be
suboptimal [17]; however, this results in poor performance
only if the macro actions themselves are poorly selected [18].

B. Contributions of the paper

The main technical contribution of this paper is to develop
a computationally efﬁcient algorithm for scheduling a control-
lable HVAC system in buildings with PCM. To our knowledge,
this work is the ﬁrst attempt to solve an optimization problem
in buildings with PCM. Beyond this, the paper advances the
state of the art in the following ways:

1) We derive a novel and computationally-efﬁcient opti-
mization method for online non-linear scheduling, which
exploits several techniques from AI in one framework.
2) We demonstrate the method on a HVAC scheduling
problem in a typical PCM-building, incorporating a non-
linear RC lumped thermal model.

3) We evaluate the method on four different seasonal
weather conditions, with results showing that the method
has good accuracy over DP, and provides a considerable
electricity cost saving over a deadband controller.
4) The proposed method gives substantial computational
speed-ups, as much as 12,900 times faster than DP.
5) The method described in this paper can be implemented
on current smart meters and IoT gateway devices, such
as those built on Raspberry Pi boards.

C. Outline of the paper

This paper progresses as follows: in Section II, a thermal
model of a PCM-building is built in MATLAB, and validated
by benchmarking it against an identical model in EnergyPlus3.
In Section III, the optimization problem of the PCM-building
is formulated as an MDP, and the value iteration algorithm
is used to solve it. Section IV contains the main technical
contributions of this paper, where the proposed methodology
of macro-action multi-timescale dynamic programming is de-
rived. In Section V, the method is implemented on a typical
PCM-building in Sydney, and its performance is evaluated over
four seasonal weather conditions. Section VI concludes and
outlines future directions of this work.

II. THERMAL RC LUMPED MODEL OF PCM-BUILDINGS

To formulate the HVAC-PCM optimization problem, we
need a thermal model of the building that strikes the right bal-
ance between accuracy and computational efﬁciency. There-
fore, we ﬁrst build a thermal model of a PCM-building in
MATLAB, which we then evaluate by benchmarking it against
an identical model in the EnergyPlus software.

A. Thermal model of PCM-building

A simple way of modeling the thermal performance of
a building is using an RC representation of each building

3EnergyPlus is a software tool that is widely used for simulating the

thermal behavior of buildings.

Rdw

QHVAC + Qinf

Rout

Te

Rin

Tout

Ce+CPCM

Tin

maca

Fig. 1. Thermal lumped parameter model of a PCM-building.

element, which is known as an RC lumped model [19], [20].
For simplicity, we use a 2RC model, where all elements of
the wall, roof, and ﬂoor are lumped together into two lumped
resistances and one lumped capacitance, as shown in Fig. 1.
The circuit is thermally excited by the internal heat ﬂow,
including the HVAC system QHVAC, and natural air inﬁltration
Qinf , and the outside temperature Tout. The buildings interior
is simulated as a single capacitor Ca, which captures the
heat capacity of air. Elements like doors and windows have
negligible thermal inertia, so they are modeled using resistance
Rdw in parallel with the lumped model of the wall.

The dominant capacitance in the model is Ce+CPCM, which
captures the combined the thermal inertia of the envelope and
the PCM layer. The building’s envelope is made up of three
layers: rendered ﬁbro-cement, a timber stud wall containing
insulation batts, and plasterboard on the inside4. To improve
the thermal inertia, PCM is added as a 0.03 m layer underneath
the plasterboard. Therefore, two layers with signiﬁcant amount
of thermal mass (PCM layer and timber wall) are placed
adjacent and can be lumped together as a single capacitance.
The parameters are chosen so that the model represents a
single-zone 8 m × 6 m × 2.7 m cuboid with a total ﬂoor area
of 48 m2. The detailed calculations of the resistances and
capacitances of the model can be found in [22].

The speciﬁc heat capacity characteristic of the PCM used
in this paper is shown Fig. 2. Observe that the phase change
occurs over the range between 20 ◦C and 26 ◦C. The melting
point is at 25.1 ◦C, where the speciﬁc heat capacity is the
largest. The PCM melting point is an important design param-
eter, chosen to reﬂect the occupants’ comfortable temperature
range. The PCM speciﬁc heat capacity is given by:

cpcm = 1200 + 18800e−
cpcm = 1300 + 18700e−4(Tp−T )2

(cid:16) Tp−T
1.5

(cid:17)

if T < Tp,

if T ≥ Tp,

(1a)

(1b)

where Tp is the melting point of the PCM. Finally,
the
thermal model of a PCM building consists of two ﬁrst-order
differential equations capturing the energy balance at the two
nodes deﬁned by, respectively, the indoor temperature Tin and
the surface temperature of the PCM layer Te:

˙Te =

˙Tin =

1
maca

1
Ce + CPCM
(cid:18) Tout −Tin
Rdw

(cid:18) Tin − Te
Rin

+

Tout − Te
Rout

(cid:19)

,

+

Te −Tin
Rin

+ ˙QHVAC + ˙Qinf

(2)

(3)

(cid:19)
.

4The materials and conﬁguration are chosen based on a common practice

in lightweight building in Australia [21].

3

Fig. 2. Speciﬁc heat capacity characteristic of a typical PCM.

Fig. 3.
Indoor temperature of a building without PCM using RC lumped
model (RC lumped) benchmarked against EnergyPlus results (EP) and com-
pared to the ambient temperature (Amb).

Note that due to the discontinuity in (1a) and (1b), the PCM
heat capacity curve needs to be ﬁtted with a polynomial
function before it can be used to numerically solve (2).

B. Benchmarking thermal model against EnergyPlus

We now compare the performance of the RC lumped
model implemented in MATLAB against an identical model
in EnegyPlus, using the indoor temperature of the building for
comparison. Simulations are performed for a typical summer
month (1–28 February) in Sydney. We use the root-mean-
square error (RMSE) as comparison metric. Observe in Figs. 3
and 4 that the models match well in both scenarios, with and
without PCM, with the maximum RMSE value not exceeding
0.8 ◦C; this is acceptable with respect to the model uncertainty
and human temperature sensitivity.

III. HOME ENERGY MANAGEMENT IN PCM BUILDINGS

In this section, we present an MDP formulation of the
HVAC-PCM optimization problem using differential equations
(2) and (3) of the validated model as transition functions. Then
we show how to use DP to solve the optimization problem.

An MDP comprises a state-space, (s ∈ S), a decision-
space, (x ∈ X ), transition functions and contribution func-
tions. Let k = {1, . . . , K} denote a time-step of one hour. A
state variable, sk ∈ S, contains the information that is neces-
sary and sufﬁcient to make the decisions and compute costs,
rewards and transitions. The decision variable, xk ∈ X , is an
action that results in a transition from one state to another, in
a sequence over the decision horizon. Finally, random effects
are in general used to represent chance exogenous information,
such as weather conditions or inhabitants’ behavioural patterns

101520253035Temperature (°C)01020C (kJ.kg-1.K-1)0200400600Time (hours)1520253035Indoor Temperature (°C)RC lumped vs EP-NOPCMAmbRC lumpedEP0200400600Time (hours)01Error (°C)RC lumped vs EP-NOPCM4

future discounted cost of following policy π starting in state
sk. It is given by:
(cid:88)

Pr(s(cid:48)|sk, xk) [C(sk, xk, s(cid:48)) + V π(s(cid:48))],

V π(sk) =

(6)

s(cid:48)∈S

where Pr(s(cid:48)|sk, xk) is the transition probability of landing on
state s(cid:48) from sk if we take action xk [2]. However, because
the system model, sM is a deterministic function, we have

Pr(s(cid:48)|sk, xk) =

(cid:40)
1
0

if sM (sk, xk) = s(cid:48),
otherwise,

(7)

making the calculation simpler than in the stochastic case.

The expression in (6) is a recursive reformulation of the
in general, Bellman’s optimality

objective function. Thus,
condition states that the optimal value function is given by

V π∗
k (sk) = min
xk∈Xk

(cid:16)
Ck(sk, xk) + E

(cid:110)

V π∗
k+1(s(cid:48))|sk

(cid:111)(cid:17)

,

(8)

where π∗ is an optimal policy. To ﬁnd π∗, we need to solve
(8) for each state.

Value iteration (VI) is the process of computing (8) for each
state by backward induction; that is, starting at the end points
of the MDP. The optimal policy is extracted from the optimal
value function by selecting the minimum value action for each
state. To describe this in a simple way, in VI, the desired state
in step k + 1 is set to the lower value while the undesired
states and states that are out of comfort bounds are penalized
by assigning higher values. Then, for all possible states at time
k, the VI algorithm moves backward in time and, in each time
step, by solving the subproblem in (8), the minimum value
function is computed for different states of each time step.
In the ﬁnal step of backward induction, corresponding to the
initial starting point, all value function calculations converge to
the optimal value function. Then, by tracing a minimum value-
function path forward for a given time horizon, the optimal
policy is found [2].

However, despite advancements in computation power, di-
rectly applying VI (or other exact DP algorithms) has an
excessively high computational burden. Although we consider
only one state-variable representing the indoor temperature
of the building, the running time of the VI algorithm for a
decision horizon of 24 hours with slot length of one hour
is very long; in this problem, the running time is almost nine
days on a high-performance computer cluster. The main reason
behind this is that at each time step, the algorithm solves
differential equations (2) and (3) for each action to update the
MDP state, and this is repeated until the initial starting state
is reached. Given this shortcoming, we now propose a method
to overcome the computational burden of the DP algorithm.

IV. METHODOLOGY
In this section, we describe our methodology in three steps,
namely: state-space approximation, multi-time scale MDP, and
macro-action abstraction.

A. State-space approximation

We now describe the state-space approximation used as
a ﬁrst step to deal with the computational burden of DP.

Fig. 4.
Indoor temperature of a PCM-building using RC lumped model (RC
lumped) benchmarked against EnergyPlus results (EP) and compared to the
ambient temperature (Amb).

[2]. However, for simplicity and because we focus on the
non-linear characteristics of PCM, in this work the problem
is treated as deterministic, and as such, random effects are
omitted and left for future work. Thus, the form of the HVAC-
PCM MDP is given by

E

min
π

(cid:40) K
(cid:88)

k=0

Ck(sk, xk = π(sk))

(cid:41)

s.t.

thermal comfort constraints, and

thermal energy balance constraints,

(4)

where π : S → X is a policy, i.e. a sequence of actions taken
to move from each state to the next state over the whole time
horizon. In this work, a policy is a sequence of on/off status
of the HVAC system over a deﬁned time horizon.

The function Ck(sk, xk) is the contribution function, which
is the cost incurred at a given time-step k that accumulates
over time [2]. For our speciﬁc optimization problem, the cost
consists of the electricity cost and the discomfort cost:

Ck(sk, xk) = λcg,kPk + (1 − λ) (|Troom,k − Ts|) .

(5)

To balance the two cost components, the contribution func-
tion includes a weighting factor λ, applied to the electricity
cost, with (1 − λ) applied to the penalty for deviating from the
desired HVAC set point Ts. We assume a reverse-cycle HVAC
system able to operate both in a heating and cooling mode.
The setpoint Ts for the two modes is assumed 20 ◦C and 23 ◦C,
respectively. The electricity cost of the HVAC system is the
electricity time-of-use tariff, (cg,k), multiplied by the energy
used to run the HVAC system, Pk.

Referring back to (4), let sk+1 = sM (sk, xk) describe the
state evolution from time step k to the next time step, k + 1,
where sM is the underlying mathematical model of the studied
system [2]. In this problem, the system model is the thermal
model of the building, so the MDP transition functions are
given by (2) and (3).

Cost function (5), only considers the instantaneous cost
that results from the decision that is taken at each time step.
Building on this, DP solves the optimization problem by
computing a value function V π(sk), which is the expected

0200400600Time (hours)1520253035Indoor Temperature (°C)RC lumped vs EP-PCMAmbRC lumpedEP0200400600Time (hours)01Error (°C)RC lumped vs EP-PCMHowever, before explaining the methodology, a few terms need
to be deﬁned. We call the MDP that uses equations (2) and (3)
without any change, the exact model. The output of the exact
model in each time-step is a state (indoor temperature), which
we call the exact state. If we consider all the possible states
over the decision horizon, we call that the exact state-space.
The corresponding terms in the approximated methodology
are called, respectively, the approximate model, approximated
state and approximated state-space.

The proposed approximation involves rounding the output
of differential equations (2) and (3) in each time step to the
nearest multiple of 0.1. Given this, depending on the state
trajectory of the exact model, an approximate state may cover
more than one exact state, which improves the computational
performance of the VI algorithm by reusing the computed
state transitions and sub-problem solutions. In more detail,
for the desired comfort range between 20 ◦C and 26 ◦C and
assuming a penalty for out-of-bound temperatures, any state
in the state-space is a value between 20 ◦C and 26 ◦C with
a 0.1 ◦C discretization. In other words, we can group the
whole desired state-space into 61 groups. This approximation
is acceptable as long as it does not affect the quality of the
optimization solution. This is demonstrated in the ﬁrst part
of Section V, by deﬁning metrics to measure the quality of
the solution resulting from this state-space approximation. The
approximated MDPs that are developed in this section, will
be used as a ground MDP for the multi-time scale MDP and
macro-action abstraction.

B. Multi-timescale Markov decision processes

Applying a multi-timescale abstraction signiﬁcantly reduces
the computational burden of energy management optimization
problem. Building on the approximated state-space introduced
above, we divide the time-horizon of the problem into blocks,
each consisting of four time-steps (hours). We note that the
performance of our methodology highly depends on the length
of each block. Through the process of trial and error, we
found that choosing four time-steps for each block strikes the
right balance between the number of blocks and the length of
each block, and results in the highest speed-up. For reference,
we denote the multi-timescale method Algorithm 1 (ALG 1).
We can formulate each block as a separate MDP; therefore,
we solve a few successive block-MDPs using VI to ﬁnd the
optimal policy over the whole time-horizon.

As described in the pseudocode of ALG 1, ﬁrst we apply VI
algorithm on the last block-MDP (Lines 1-11). In more detail,
we set the corresponding value functions in the last time step to
zero for the states that have a value within the desired comfort
range, and assign a high value for the states with the values out
of the comfort range (Lines 5-9). To exploit the advantage of
the approximated state-space, we run the VI algorithm for 61
(20:0.1:26) initial points (Lines 1 and 13) for each block-MDP
except the ﬁrst one. We save all the optimal value functions
that correspond to each of the 61 initial points in look-up
tables (Lines 10 and 23). Before running VI on the remaining
block-MDPs, we update the initial value functions by replacing
the corresponding value function of the current state. In more
detail, we ﬁnd the initial states that have the same value as

5

Algorithm 1 : Multi-time scale algorithm (ALG 1)

L: length of each block
T1: lower bound of desired temperature range
T2: upper bound of desired temperature range
d: discretization step
T0: ﬁx initial temperature

(cid:46) Value iteration (VI) of the last block (BlkM )

for all 2L combinations of the action-space do
calculate states using equations (2) and (3)

end for
if T1 ≤ sBlkM,colL ≤ T2 then

1: for T1, ..., T2 with d step discretization do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11: end for

Initialize vBlkM,colL to inﬁnity vector.

Initialize vBlkM,colL to zero vector

else

end if
Execute the VI and store ﬁnal value function in vﬁnal,BlkM .

(cid:46) Value iteration of Blocks 2 to M − 1 (Blk2 to BlkM −1).

12: for M − 1 : ... : 2 do
13:
14:
15:
16:
17:
18:
19:

vﬁnal,BlkM .
else

for T1, ..., T2 with d step discretization do

for all 2L combinations of the action-space do
calculate states using equations (2) and (3)

end for
if T1 ≤ sBlkM−1,colL ≤ T2 then

for any state for which sBlkM−1,colL = sBlkM,col1 ,
Initialize

vBlkM−1,colL with

corresponding

the

20:
21:
22:
23:

Initialize vBlkM−1,colL with inﬁnity vector

end if
Execute the VI and store ﬁnal value function in

vﬁnal,BlkM−1 .
end for

24:
25: end for

(cid:46) Value iteration of the ﬁrst block (Blk1).

26: Set T0.
27: for all 2L combinations of the action-space do
calculate states using equations (2) and (3)
28:
29: end for
30: if T1 ≤ sBlk1,colL ≤ T2 then
31:
32:
33: else
34:
35: end if
36: Execute the VI and store ﬁnal value function in vﬁnal,total.

for any state for which sBlk1,colL = sBlk2,col1 ,
Initialize vBlk1,colL with the corresponding vﬁnal,Blk2 .

Initialize vBlk1,colL with inﬁnity vector

the current state and replace the corresponding value functions
as the initial value functions of the current block-MDP (Lines
18-19 and Lines 31-32). This process repeats until the ﬁrst
block-MDP. To ﬁnd a solution for the optimization problem
over a deﬁned time-horizon, we need to ﬁx either the initial
or the ﬁnal temperature. In this work, we provide the ALG 1
with a ﬁxed initial temperature (Line 26). Therefore, we have
only one VI to run for the ﬁrst MDP.

Comparing the results of ALG 1 with a one-block MDP
model shows that both methods converge to the exactly same
solution. This corroborates with Sutton’s result that n-block

φ = 0

φ = 0.5

φ = 1

0
0
0
0

0
0
0
1

0
0
1
0

0
1
0
0

1
0
0
0

0
0
1
1

0
1
0
1

0
1
1
0

1
0
0
1

1
0
1
0

1
1
0
0

0
1
1
1

1
0
1
1

1
1
0
1

1
1
1
0

1
1
1
1

φ = 0.25

φ = 0.75

Fig. 5. The combinatorial structure of each optimal policy.

MDPs act exactly same as the corresponding one-block MDP
[15]. The mathematical proof of this claim, tailored to our
setting, is given in the online Appendix. Our simulations show
that using ALG 1 reduces the computational burden of ﬁnding
the optimal policy by a factor of 5,300.

C. Macro actions

Building on ALG 1, we now describe the macro-action
abstraction. This is a widely-used method for reducing the
size of a state-space. After running VI on each block-MDP
for different initial points and observing the corresponding
state-spaces, we deﬁne the macro actions. Speciﬁcally, the
contribution function of the ground MDP (5) is modiﬁed to:

¯C π

k (¯sk, ¯xk) = λ cg,kPkφ + (1 − λ) (|Troom,k − Ts|) ,
(9)
where φ : S → ¯S, φ ∈ { 0, 0.25, 0.5, 0.75, 1 } is a percentage
of the HVAC system rated power used for the abstraction, and
¯S is the abstract state-space. To be clear, each ground MDP
over four-time steps has 24 possible policies consisting of
primitive actions. These 16 policies are all the possible on/off
permutations of the HVAC system over four-time steps. Under
our deﬁned macro actions, all 16 combinatorial arrangement
of zero, one, two, three and four hours of operation of the
HVAC system have the corresponding compact representations
of no operation, or four hours operation of HVAC system at
25 %, 50 %, 75 % and 100 % rated power, respectively. This
is illustrated in Fig. 5.

To this end, we implement our methodology in two phases,
and call it Algorithm 2 (ALG 2). In the ﬁrst phase, we im-
plement ALG 1 on MDPs with macro actions. These solutions
differ from those of ALG 1, which is applied to the MDP with
primitive actions (Section IV-B). Then, in the second phase,
we expand the optimal macro-policies for the next-executed
block-MDP to their related primitive policies, as shown in
Fig. 5. Sequentially expanding the optimal policy of these
new MDPs gives the overall policy computed by ALG 2. Our
simulation results in Section V show that policies produced
by ALG 2 are close to those found with ALG 1, so they can
be assumed near-optimal.

V. EVALUATION AND DISCUSSION

6

(MAE) of the approximated state-space against the exact state-
space (i.e. the temperature error), (ii) MAE of the ﬁnal value
function resulting from applying VI on the approximated state-
space versus the ﬁnal value function resulting from applying
VI on the exact state-space without temperature discretization,
and (iii) normalized calibration error between the optimal
policy using DP on the approximated state-space compared
to the optimal policy using DP on the exact state-space5. In
Section V-B, similar measures are used to compare the results
of ALG 1 and ALG 2.

The simulations of the Section V-B were run in MATLAB
using a computing platform with an Intel 2.7 GHz i7-7500U
CPU, 64-bit operating system and 16 GB RAM, while for
Section V-A, we used a high-performance computer cluster
due to the excessive computational burden.

A. Beneﬁts of the state-space approximation

As mentioned in the previous section, the runtime of the
exact VI for a time horizon of 24 hours is nine days. Thus, we
consider the beneﬁts of using state-space approximation, and
calculate the performance metrics, for only one typical summer
day in Sydney. To have a reasonable population to assess the
approximation by the three criteria above, we generated both
the approximated and the exact state-space for the 61 initial
points between 20 ◦C and 26 ◦C with a 0.1 ◦C discretization.
First, we calculate the MAE of the approximated state-
space, which represents the indoor temperature versus the
actual state-space over a time horizon of 24 hours. The
maximum error is approximately 0.06 ◦C, which is small and
acceptable.

Second, as mentioned in Section III, the ﬁnal value function
(5) consists of two parts: the electricity cost and the discomfort
cost. For each, the MAE is calculated separately. The results
show that the MAE of the electricity cost for λ = 0.95 is
0.74 cents, relative to an average value of 8.08 cents. For
the discomfort cost, the MAE is about 0.07 ◦C relative to an
average of 1.6 ◦C. These results indicate that the error of the
ﬁnal value function is acceptable.

Third, the approximation is also veriﬁed further by ex-
amining the calibration error between the optimal policy of
the exact and approximated models over a 24-hour horizon.
To calculate the calibration error for each starting point, the
difference in the number of on-cycles in the equivalent optimal
policy is divided by a total number of time steps (24), and the
results are averaged over the 61 starting points. Comparing the
two cases shows that the difference in the number of on-cycles
in the optimal policies is zero in 60 out of the total 61 studied
cases. In the last case, the difference is one on-cycle, giving
a calibration error of 0.07 %, which is acceptable.

Overall, these results demonstrate the efﬁcacy of the state-
space approximation, particularly in terms of model accuracy,
which permits us to move on to ALG 1 and ALG 2.

We begin by evaluating the loss in the solution quality
resulting from the state-space approximation. We then examine
the quality of the policies computed using, respectively, ALG 1
and ALG 2, and compare their computational performance.

We deﬁne three measures of the solution quality, which
we use in Section V-A. These are: (i) mean-absolute error

B. Evaluation of Algorithm 1 and Algorithm 2

We now evaluate the proposed methodology, ALG 2, and
the approach using only the multiple timescales abstraction,

5Here we use the term calibration in the statistical sense, to measure the

ﬁt of the approximate DP method to exact DP.

7

TABLE I
EVALUATION OF THE PROPOSED METHOD.

HVAC system with
deadband relay

Average number of HVAC operating hours

Season

Average cumulative cost

Electricity cost ($)
Discomfort (◦C)

Spring
26
14.62
325.24

Summer
12
11.14
301.47

Autumn
14
7.69
251.74

Winter
93
57.24
200.71

Average number of HVAC operating hours

ALG 1

Average cumulative cost

Electricity cost ($)
Discomfort (◦C)

ALG 2 against ALG 1

MAE of the cumulative cost

Elect. cost error (%)
Discomfort error (%)

Max. MAE of indoor temp. of optimal policy (◦C)

Normalised calibration error (%)

Spring
23
11.53
202.14

Spring
1.5
7.89
7.14
1.39

λ = 0.95

Summer
19
8.20
112.08

Autumn Winter

11
4.48
258.95

74
40.89
155.44

λ = 0.95

Summer
1.6
6.71
13.94
0.49

Autumn Winter

1.2
14.95
5.48
0.66

2.0
6.11
10.63
0.95

Spring
34
21.64
156.36

Spring
1.3
2.86
6.38
0.61

λ = 0.05

Summer
28
22.86
37.79

Autumn Winter

38
19.20
218.37

71
46.21
110.41

λ = 0.05

Summer
1.1
1.88
15.85
0.54

Autumn Winter

2.1
20.83
2.93
3.59

1.3
0.84
19.95
0.96

ALG 1,
in order to ascertain the beneﬁts of the multi-
timescale and macro-action abstractions. Moreover, to get a
better sense of the beneﬁts of using optimal HVAC scheduling
in PCM-buildings, the algorithms are compared to a simple
deadband relay for controlling the HVAC system. Furthermore,
to demonstrate the ability of the two algorithms to capture the
customer preferences in terms of electricity cost and comfort,
the algorithms are run for two different weighting factors:
λ = 0.95 (more weight on the electricity cost) and λ = 0.05
(more weight on the thermal discomfort). The results for four
typical weeks, one for each season, are summarized in Table I.
The temperature proﬁles of the four weeks are shown in Fig. 6.
1) Deadband policy vs. ALG 1: To begin, we consider
a conventional HVAC system operating with a deadband
control. We simulate an identical home with an identical
HVAC system. The only difference is that the HVAC system is
controlled using a deadband controller as opposed to optimal
scheduling used in ALG 1 and ALG 2. The deadband range is
set between 20 ◦C and 22 ◦C for heating, and between 22 ◦C
and 26 ◦C for cooling. The simulations are run for 61 initial
points. We record the average number of HVAC operating
hours and the average cumulative electricity cost, and compare
the performance of the HVAC system with a deadband relay
against ALG 1.

The results show considerable beneﬁts from using opti-
mization over simple deadband control. Speciﬁcally, using
ALG 1 for a typical winter week with a weighting factor
of λ = 0.05 reduces the number of HVAC operating hours
by 23.7 %. Importantly, both cost function components see
improvements: the electricity cost decreases by 19.3 %, while
the number of discomfort hours also decreases by 45 %. In
contrast, in summer, applying ALG 1 increases the number of
HVAC operating hours by a factor of 2.3, which increases the
electricity cost by a factor of two. The number of discomfort
hours, on the other hand, reduces by 87.5 %, which is to be
expected given that a much higher weight (λ = 0.05) is given
to the comfort of the occupants. Putting more weight on the
electricity cost (λ = 0.95) reduces the electricity cost by 26 %
at the expense of a slightly lower reduction in the thermal
discomfort (now 63 %), which goes to show that the weighting
factor λ has to be carefully tuned for optimal performance with
respect to customers’ preferences.

The only season when λ = 0.95 doesn’t result in the
reduction in both cost components is Autumn. However, the
increase in discomfort is only 3 % while the reduction in
electricity cost is signiﬁcant (about 40 %), which conﬁrms the
superiority of ALG 1 over the deadband control.

2) ALG 1 vs. ALG 2: Finally, we evaluate ALG 2 with
respect to ALG 1, using as a comparison metrics: (i) maximum
MAE of the indoor temperature; (ii) MAE of the cumulative
cost expressed as a percentage deviation from ALG 1; and, (iii)
the normalized calibration error. We compute the ﬁrst measure
only for the optimal policy rather than for the whole state-
space, because using macro actions in ALG 2, results in a
different size of the state-space compared to ALG 1.

The results for the ﬁrst metric show that the maximum MAE
of the indoor temperature over four typical weeks for λ = 0.95
and λ = 0.05, are 2 ◦C and 2.1 ◦C, respectively, which is about
0.01 ◦C on average per hour. For the second metric, using
macro-action abstraction in ALG 2 reduces the solution quality
compared to ALG 1 by up to 20 % at most (electricity cost in
Autumn and discomfort in Winter, both for λ = 0.05), but is
otherwise mostly below 10 %. The third metric, the maximum
calibration error, over four case studies is 1.39 % and 3.59 %
for λ = 0.95 and λ = 0.05, respectively. This implies that the
optimal policies from ALG 2 and ALG 1 have a very similar
number of on-cycles.

Overall, the results of these three metrics show that the
performance of ALG 2 is comparable compared to ALG 1.
Note that there is a trade-off between the superior runtime
of ALG 2 and a better accuracy of ALG 1 in tracking the
optimal policy. These errors may be mitigated by adding extra
macro actions, but this will in turn reduce the computational
efﬁciency of ALG 2.

To visually compare the performance of the two algorithms,
Fig. 7 shows the indoor temperature and the HVAC schedule
for the two algorithms for a typical summer week. Observe that
the both temperature proﬁles are within the desired comfort
range, with only a slight difference in the second day when
the the on/off schedules of the HVAC system don’t match. It is
worth mentioning that the indoor temperature for some hours
such as 0-10 and 90-130, is constant, and this where phase
change occurs. During this phase change, the PCM absorbs
the heat from the building’s interior and keeps the indoor

8

In future work, we will consider stochastic exogenous
inputs, such as weather conditions and occupants’ behavior,
which in this paper were considered deterministic.

REFERENCES

[1] L. P´erez-Lombard, J. Ortiz, and C. Pout, “A review on buildings energy
consumption information,” Energy and Buildings, vol. 40, no. 3, pp.
394–398, 2008.

[2] C. Keerthisinghe, G. Verbiˇc, and A. C. Chapman, “A fast technique
for smart home management: ADP with temporal difference learning,”
IEEE Transactions on Smart Grid, vol. 9, no. 4, pp. 3291–3303, 2018.
[3] H. Tischer and G. Verbiˇc, “Towards a smart home energy management
system - A dynamic programming approach,” in 2011 IEEE Innovative
Smart Grid Technologies - Asia (ISGT-Asia).

IEEE, 2011.

[4] M. Pipattanasomporn, M. Kuzlu, and S. Rahman, “An algorithm for
intelligent home energy management and demand response analysis,”
IEEE Transactions on Smart Grid, vol. 3, no. 4, pp. 2166–2173, 2012.
[5] C. Castell´on, A. Castell, M. Medrano, I. Martorell, and L. Cabeza,
“Experimental study of PCM inclusion in different building envelopes,”
Journal of Solar Energy Engineering, vol. 131, no. 4, 2009.

[6] G. Evola, N. Papa, F. Sicurella, and E. Wurtz, “Simulation of the
behaviour of phase change materials for the improvement of thermal
comfort in lightweight buildings,” in 12th Conference of International
Building Performance Simulation Association, 2011.

[7] Y. Konuklu, M. Ostry, H. O. Paksoy, and P. Charvat, “Review on
using microencapsulated phase change materials (PCM) in building
applications,” Energy and Buildings, vol. 106, pp. 134–155, 2015.
[8] Z. Rahimpour, A. Faccani, D. Azuatalam, A. C. Chapman, and
G. Verbiˇc, “Using thermal
inertia of buildings with phase change
material for demand response,” Energy Procedia, vol. 121, pp. 102–109,
2017.

[9] M. Alam, H. Jamil, J. Sanjayan, and J. Wilson, “Energy saving potential
of phase change materials in major australian cities,” Energy and
Buildings, vol. 78, pp. 192–201, 2014.

[10] F. Ascione, “Energy refurbishment of existing buildings through the use
of phase change materials: Energy savings and indoor comfort in the
cooling season,” Applied Energy, vol. 113, pp. 990–1007, 2014.
[11] A. Khudhair and M. Farid, “Use of phase change materials for thermal
comfort and electrical energy peak load shifting: experimental investi-
gations,” in Proceedings of ISES World Congress 2007 (Vol. I–Vol. V).
Springer, 2008, pp. 283–288.

[12] A. I. Orhean, F. Pop, and I. Raicu, “New scheduling approach using
reinforcement learning for heterogeneous distributed systems,” Journal
of Parallel and Distributed Computing, vol. 117, pp. 292–302, 2018.

[13] R. E. Bellman and S. E. Dreyfus, Applied dynamic programming.

Princeton University Press, 1962.

[14] Z. Rahimpour, G. Verbiˇc, and A. C. Chapman, “Energy management of
buildings with phase change materials based on dynamic programming,”
in 13th PowerTech 2019.

IEEE, 2019.
[15] R. S. Sutton, “TD models: Modeling the world at a mixture of time

scales,” in Machine Learning Proceedings 1995, pp. 531–539.

[16] M. Pickett and A. G. Barto, “Policyblocks: An algorithm for creating
useful macro-actions in reinforcement learning,” in Proc. Int. Conf. Ma-
chine Learning (ICML ‘02), vol. 19, 2002, pp. 506–513.

[17] M. Hauskrecht, N. Meuleau, L. P. Kaelbling, T. Dean, and C. Boutilier,
“Hierarchical solution of Markov decision processes using macro-
actions,” in Proceedings of the Fourteenth conference on Uncertainty
in Artiﬁcial Intelligence (UAI ‘98), 1998, pp. 220–229.

[18] D. Precup, R. S. Sutton, and S. Singh, “Theoretical results on reinforce-
ment learning with temporally abstract options,” in European conference
on Machine Learning (ECML ‘98). Springer, 1998, pp. 382–393.
[19] C. Underwood and F. Yik, Modelling methods for energy in buildings.

John Wiley & Sons, 2004.

[20] M. Gouda, S. Danaher, and C. Underwood, “Low-order model for the
simulation of a building and its heating system,” Building Services
Engineering Research and Technology, vol. 21, no. 3, pp. 199–208,
2000.

[21] K. Gregory, B. Moghtaderi, H. Sugo, and A. Page, “Effect of thermal
mass on the thermal performance of various Australian residential
constructions systems,” Energy and Buildings, vol. 40, no. 4, pp. 459–
465, 2008.

[22] Z. Rahimpour, G. Verbiˇc, and A. C. Chapman, “Using thermal inertia
of buildings with phase change material for demand response,” in Asia-
Paciﬁc Solar Research Conference 2018. Australian PV Institute, 2018.

Fig. 6. Outdoor temperature of four different seasonal weather conditions
in Sydney: spring (16/10-22/10), summer (16/01-22/01), autumn (1/04-7/04)
and winter (25/07-31/07).

Fig. 7. From top to bottom: outdoor temperature over a typical summer week
in Sydney; indoor temperature corresponds to an optimal scheduling of HVAC
system that results from applying the ALG 1 versus the ALG 2 and ﬁnally
optimal operation of HVAC system that results from applying the ALG 1
versus the ALG 2. The dash line represents customer’s desired temperature in
a summer week.

temperature of the building within the desired range.

We have also compared the runtime of both algorithms to be
able to quantify the computational cost saving resulting from
the use of macro-action and multi-timescale abstractions. We
can observe a speedup of up to 12,900 times compared to the
direct application of DP. The use of macro actions in ALG 2
results in a speedup of up to 2.4 times compared to ALG 1.

VI. CONCLUSION

In this work, we have addressed the computational challenge
of solving an optimization problem in buildings with PCM.
We have developed a computationally efﬁcient macro-action
multi-timescale algorithm to deal with the computational
burden of a large non-linear, non-convex HVAC scheduling
problem. We demonstrated the efﬁcacy of the proposed ap-
proach on a typical PCM-building over four typical weeks
that are representative of four seasons in Sydney. The results
demonstrate the superior computational performance of the
proposed scheduling algorithm compared to a direct applica-
tion of DP while maintaining an acceptable solution quality.
The results also show that the weighting of the electricity
and the discomfort costs in the objective function have to
be selected carefully to ensure an optimal trade-off between
electricity expenditure and thermal discomfort.

20406080100120140160A typical week (hours)010203040Outdoor temperature (°C)SpringSummerAutumnWinter20406080100120140160Summer week (hours)01Cooling status (on/off)Algorithm 1Algorithm 220406080100120140160203040Outdoor temp.(°C)20406080100120140160222426Indoor temp. (°C)Algorithm 1Algorithm 2Desired temp.9

APPENDIX A

Here we brieﬂy explain the proof of the multi-timescale
approach presented Section IV-B. The proof is based on a
generalized Bellman equation [15]

V = C + PT V.

(10)

The model is valid if it satisﬁes (10) for any P and C, with
limi→∞ Pi = 0, where i is the number of the MDPs in the
model. For any valid model, we can update the value function
through lookahead or backup operation as follows:

Vk+1 = C + PT Vk.

(11)

As long as the model is valid it converges to the same value
function regardless of the number of steps:

V∞ =

∞
(cid:88)

i=0

PT i

C = V

(12)

To prove that the solution of a multi-timescale MDP is the
same as the solution of an one-step MDP, we need to prove that
an i-step model formulation satisﬁes the generalized Bellman
equation (10).

Theorem VI.1: A multi timescale or an n-step model that has
a general form (13) satisﬁes the generalized Bellman equation
(10).

C (n) =

n−1
(cid:88)

i=0

(cid:0)PT (cid:1)i

C,

C (n)T

sk = E(c(n)

k |sk),

(13a)

(13b)

k = (cid:80)n

i=1 ck+i is the n-step truncated return starting

where c(n)
from state sk.
Proof: We combine P and C and the initial value s0 into a
matrix M :
(cid:18)s0 C T
P

M =

(cid:19)

0

If the vector V is also augmented by adding an initial
component whose value is always 1, then the generalized
Bellman equation (10), can be written as

V = M T V.

(14)

Same as before, we consider model M to be valid if and only
if it satisﬁes (14). For any valid model Mi, the composed
model

Mi is also valid because

n
(cid:81)
i=1

n
(cid:89)

i=1

(Mi)T V =

n
(cid:89)

i=1

M T

i V = V

(15)

Note that M has been constructed such that it is valid only if
the corresponding P and C are valid. Therefore, (15) proves
(cid:4)
the validity of the n-step model (13).

