1
2
0
2

t
c
O
6
2

]

G
L
.
s
c
[

2
v
7
3
9
4
1
.
5
0
1
2
:
v
i
X
r
a

Safe Pontryagin Differentiable Programming

Wanxin Jin
University of Pennsylvania
wanxinjin@gmail.com

Shaoshuai Mou
Purdue University
mous@purdue.edu

George J. Pappas
University of Pennsylvania
pappasg@seas.upenn.edu

Abstract

We propose a Safe Pontryagin Differentiable Programming (Safe PDP) methodol-
ogy, which establishes a theoretical and algorithmic framework to solve a broad
class of safety-critical learning and control tasks—problems that require the guaran-
tee of safety constraint satisfaction at any stage of the learning and control progress.
In the spirit of interior-point methods, Safe PDP handles different types of system
constraints on states and inputs by incorporating them into the cost or loss through
barrier functions. We prove three fundamentals of the proposed Safe PDP: ﬁrst,
both the solution and its gradient in the backward pass can be approximated by
solving their more efﬁcient unconstrained counterparts; second, the approximation
for both the solution and its gradient can be controlled for arbitrary accuracy by a
barrier parameter; and third, importantly, all intermediate results throughout the
approximation and optimization strictly respect the constraints, thus guaranteeing
safety throughout the entire learning and control process. We demonstrate the ca-
pabilities of Safe PDP in solving various safety-critical tasks, including safe policy
optimization, safe motion planning, and learning MPCs from demonstrations, on
different challenging systems such as 6-DoF maneuvering quadrotor and 6-DoF
rocket powered landing.

1

Introduction

Safety is usually a priority in the deployment of a learning or control algorithm to real-world systems.
For a physical system (agent), safety is normally given in various constraints on system states and
inputs, which must not be violated by the algorithm at any stage of the learning and control process,
otherwise will cause irrevocable or unacceptable failure/damage. Those systems are referred to as
safety-critical. The constraints in a safety-critical system can include the immediate ones, which are
directly imposed on the system state and input at certain or all-time instances, and the long-term ones,
which are deﬁned on the trajectory of system states and inputs over a long period.

Compared to the abundant results that focus on system optimality [1–3], systematic and principled
treatments for safety-critical learning and control problems seem largely insufﬁcient, particularly in
the following gaps (detailed in Section 1.1). First, existing safety strategies are either too conservative,
which may restrict the task performance, or violation-tolerable, which only pursues the near-constraint
guarantee and thus are not strictly constraint-respecting. Second, a systematic safety paradigm capable
of handling different types of constraints, including system state and input (or mixed), immediate,
or/and long-term constraints, is still lacked. Third, some existing safety strategies suffer from huge
computational- and data- complexity, difﬁcult to be integrated into any differentiable programming
frameworks to solve large-scale learning and continuous control tasks.

To address the above research gaps, this paper aims to develop a safe differentiable programming
framework with the following key capabilities. First, the framework provides a systematic treatment
for different types of constraints in a safety-critical problem; second, it attains provable safety- and
accuracy- guarantees throughout the learning and control process; third, it is ﬂexible to perform
safe learning of any unknown aspects of a constrained decision-making system, including policy,
dynamics, state and input constraints, and control cost; ﬁnally, it can be integrated to any differentiable
programming framework to efﬁciently solve large-scale safe learning and control tasks.

Preprint. Under review.

 
 
 
 
 
 
1.1 Related Work

In machine learning and control ﬁelds, safety has been deﬁned by different criteria, such as worst-
case [4, 5], risk-sensitive [6], ergodicity [7], robust [8, 9], etc., most of which are formulated by
directly altering an objective function [10]. In this paper, we focus only on constrained learning and
control problems, where constraints are explicitly formulated and must be satisﬁed. We categorize
existing techniques into at-convergence safety methods, which only concern constraint satisfaction at
convergence, or in-progress safety methods, which attempt to ensure constraint satisfaction during
the entire optimization process.

At-convergence safety methods.
In reinforcement learning (RL), a constrained agent is typically
formulated as a Constrained Markov Decision Process (CMDP) [11], seeking a policy that not only
optimizes a reward but also satisﬁes an upper bound for a cumulative cost. A common strategy [12–
17] to solve CMDPs is to use the primal-dual method, by establishing the unconstrained Lagrangian
and performing saddle-point optimization. In deep learning, the primal-dual method has been recently
used [18] to train deep neural networks with constraints. In control, the primal-dual method has been
used to solve constrained optimal control (constrained trajectory) problems [19–21]. While proved to
satisfy constraints at convergence [22, 23], the primal-dual type methods cannot guarantee constraint
satisfaction during optimization, as shown in [13, 24], thus are not suitable for safety-critical tasks.

In-progress safety methods. To enforce safety during training, [25] and [26] solve CMDPs by
introducing additional constraints into the Trust Region Policy Optimization (TRPO) [27]. Since
these methods only obtain the ‘near constraint’ guarantee, constraint violation is not fully eliminated.
Another line of constrained RL [24, 28–30] leverages the Lyapunov theory [31] to bound behavior of
an agent. But how to choose a valid Lyapunov function for general tasks is still an open problem
to date [32], particularly for constrained RL, since it requires a Lyapunov function to be consistent
with the constraints and to permit optimal policies [28]. Some other work also attempts to handle
immediate constraints — the constraints imposed on agent state and input at any time. In [33], a safe
exploration scheme is proposed to produce a safe reachable region; and it only considers ﬁnite state
space. [34] develops a method that learns safety constraints and then optimizes a reward within the
certiﬁed safe region; the method deﬁnes constraints purely on agent state and thus may not be readily
applicable to mixed state-input constraints.

In control, in-progress safety can be achieved via two model-based frameworks: reachability theory
[35] and control barrier functions [36, 37]. Safe control based on reachability theory [35, 38–40]
explicitly considers adversarial factors and seeks a strategy that maintains the constraints despite the
adversarial factors. This process typically requires solving the Hamilton-Jacobi-Isaacs equations [41],
which become computationally difﬁcult for high-dimensional systems [35]. Control barrier functions
[36, 37] constrain a system only on safety boundaries, making it a less-conservative strategy for
safety-critical tasks [42–44]. Most of the methods consider afﬁne dynamics and directly use the given
constraint function as a control barrier function. Such a choice could be problematic when a system is
uncontrollable at the boundary of the sublevel set. Thus, how to ﬁnd a valid control barrier function is
still an ongoing research topic [45–47]. The above two control safety frameworks favorably focus on
pure state constraints and cannot be readily extended to other constraints, such as mixed state-input
constraints or the cumulative constraints deﬁned on the system trajectory.

Interior-point methods and control.
Interior-point methods (IPMs) [48–50] solve constrained
optimization by sequentially ﬁnding solutions to unconstrained problems with the objective combining
the original objective and a barrier that prevents from leaving the feasible regions. IPMs have been
used for constrained linear quadratic regular (LQR) control in [51–57]. While IPMs for nonlinear
constrained optimal control are studied in [58–62], they mostly focus on developing algorithms to
solve the unconstrained approximation (from the perturbed KKT conditions) and lack of performance
analysis. Most recently, [63] uses the IPM to develop a zero-th order non-convex optimization method;
and [64] uses IPMs to solve reinforcement learning with only cumulative constraints. Despite the
promise of the trend, the theoretical results and systematic algorithms regarding the differentiability
of general constrained control systems based on IPMs have not been studied and established.

Differentiable projection layer.
In machine learning, a recent line of work considers embedding a
differentiable projection layer [65–67] into a general training process to ensure safety. Particularly,
[67] and [66] enforce safety by constructing a dedicated projection layer, which projects the unsafe
actions outputted from a neural policy into a safe region (satisfying safety constraints). This projection
layer is a differentiable convex layer [68, 69], which can be trained end-to-end. In [65], safety is

2

deﬁned as robustness in the case of the worst adversarial disturbance, and the set of robust policies is
solved by classic robust control (solving LMIs). An RL neural policy with a differentiable projection
layer is learned such that the action from the neural policy lies in the robust policy set. Different
from the above work, Safe PDP does not enforce safety by projection; instead, Safe PDP uses barrier
functions to guarantee safety constraint satisfaction. More importantly, we have shown, in both theory
and experiments, that with barrier functions, differentiability can also be attained.

Sensitivity analysis and differentiable MPCs. Other work related to Safe PDP includes the recent
results for sensitivity analysis [70], which focuses on differentiation of a solution to a general nonlinear
program, and differentiable MPCs [68], which is based on differentiable quadratic programming. In
long-horizon control settings, directly applying [70] and [68] can be inefﬁcient: the complexity of
[68, 70] for differentiating a solution to a general optimal control system is at least O(T 2) (T is the
time horizon) due to computing the inverse of Jacobian of the KKT conditions. Since an optimal
control system has more sparse structures than general nonlinear or quadratic programs, by exploiting
those structures and proposing the Auxiliary Control System, Safe PDP enjoys the complexity of
O(T ) for differentiating a solution to a general control system. Such advantages have been discussed
and shown in the foundational PDP work [71] and will also be shown later (in Section 8) in this paper.

1.2 Paper Contributions
We propose a safe differentiable programming methodology named as Safe Pontryagin Differentiable
Programming (Safe PDP). Safe PDP provides a systematic treatment of different types of system
constraints, including state and inputs (or mixed), immediate, and long-term constraints, with provable
safety- and performance-guarantee. Safe PDP is also a uniﬁed differentiable programming framework,
which can be used to efﬁciently solve a broad class of safety-critical learning and control tasks.

In the spirit of interior-point methods, Safe PDP incorporates different types of system constraints into
control cost and loss through barrier functions, approximating a constrained control system and task
using their unconstrained counterparts. Contributions of Safe PDP are theoretical and algorithmic.
Theoretically, we prove in Theorem 2 and Theorem 3 that (I) not only a solution but also the gradient
of the solution can be safely approximated by solving a more efﬁcient unconstrained counterpart; (II)
any intermediate results throughout the approximation and optimization are strictly safe, that is, never
violating the original system/task constraints; and (III) the approximations for both solution and its
gradient can be controlled for arbitrary accuracy by barrier parameters. Arithmetically, (IV) we prove
in Theorem 1 that if a constrained control system is differentiable, the gradient of its trajectory is a
globally unique solution to an Auxiliary Control System [71], which can be solved efﬁciently with the
complexity of only O(T ), T is control horizon; (V) in Section 7, we experimentally demonstrate the
capability of Safe PDP for efﬁciently solving various safety-critical learning and control problems,
including safe neural policy optimization, safe motion planning, learning MPCs from demonstrations.

2 Safe PDP Problem Formulation

Consider a class of constrained optimal control systems (models) Σ(θ), which are parameterized by
a tunable parameter θ ∈ Rr in its control cost function, dynamics, initial condition, and constraints:

control cost: J(θ) =

(cid:88)T −1
t=0

ct(xt, ut, θ) + cT (xT , θ)

Σ(θ) :

subject to
dynamics: xt+1 = f (xt, ut, θ) with x0 = x0(θ),

∀t,

(1)

terminal constraints: gT (xT , θ) ≤ 0, hT (xT , θ) = 0,

path constraints: gt(xt, ut, θ) ≤ 0, ht(xt, ut, θ) = 0,

∀t.

Here, xt ∈ Rn is the system state; ut ∈ Rm is the control input; ct : Rn × Rm × Rr → R and
cT : Rn × Rr → R are the stage and ﬁnal costs, respectively; f : Rn × Rm × Rr → Rn is the
dynamics with initial state x0 = x0(θ) ∈ Rn; t = 0, 1, ..., T is the time step with T the time horizon;
gT : Rn × Rr → RqT and hT : Rn × Rr → RsT are the ﬁnal inequality and equality constraints,
respectively; gt : Rn ×Rm ×Rr → Rqt and ht : Rn ×Rm ×Rr → Rst are the immediate inequality
and equality constraints at time t, respectively. All inequalities (here and below) are entry-wise. We
consider that all functions in Σ(θ) are three-times continuously differentiable (i.e., C 3) with respect
to its arguments. Although we here have parameterized all aspects of Σ(θ), for a speciﬁc application
(see Section 7), one only needs to parameterize the unknown aspects in Σ(θ) and keep others given.

3

Any unknown aspects in Σ(θ) can be implemented by differentiable neural networks. For a given θ,
Σ(θ) produces a trajectory ξθ = {xθ

0:T −1} by solving the following Problem B(θ):

0:T , uθ

ξθ = {xθ

0:T , uθ

0:T −1} ∈ arg min{x0:T ,u0:T −1}

J(θ)

subject to xt+1 = f (xt, ut, θ) with x0 = x0(θ),

∀t,

gT (xT , θ) ≤ 0 and hT (xT , θ) = 0,
gt(xt, ut, θ) ≤ 0 and ht(xt, ut, θ) = 0,

∀t.

B(θ)

Here, we use ∈ since ξθ to Problem B(θ) may not be unique in general, thus constituting a solution
set {ξθ}. We will discuss the existence and uniqueness of {ξθ} in Section 4.
For a speciﬁc task, we aim to ﬁnd a speciﬁc model Σ(θ∗), i.e, searching for a speciﬁc θ∗, such that its
trajectory ξθ∗ from B(θ∗) meets the following two given requirements. First, ξθ∗ minimizes a given
task loss (cid:96)(ξθ, θ); and second, ξθ∗ satisﬁes the given task constraints Ri(ξθ, θ) ≤ 0, i = 1, 2, ..., l.
Note that, we need to distinguish between the two types of objectives: task loss (cid:96)(ξθ, θ) and control
cost J(θ), and also the two types of constraints: task constraints Ri(ξθ, θ) and model constraints
gt(θ). In fact, J(θ) and gt(θ) in Σ(θ) are unknown and parameterized by θ and can represent the
unknown inherent aspects of a physical agent, while (cid:96)(ξθ, θ) and Ri(ξθ, θ) are given and known
depending on the speciﬁc task (they also explicitly depend on θ since θ needs to be regularized in
some learning cases). Assume (cid:96)(ξθ, θ) and Ri(ξθ, θ) are both twice-continuously differentiable.
The problem of searching for θ∗ can be formally written as:

θ∗ = arg min

θ

(cid:96)(ξθ, θ)

subject to Ri(ξθ, θ) ≤ 0,

i = 1, 2, ..., l,

ξθ solves Problem B(θ) .

P

For a speciﬁc learning and control task, one only needs to specify the details of Σ(θ) and give a task
loss (cid:96)(ξθ, θ) and constraints Ri(ξθ, θ) ≤ 0. Section 7 will give representative examples.

3 Challenges to Solve Problem P

Problem P belongs to bi-level optimization [72]—each time θ is updated in the outer-level (including
task loss (cid:96) and task constraint Ri) of Problem P, the corresponding trajectory ξθ needs to be solved
from the inner-level Problem B(θ). Similar to PDP [71], one could approach Problem P using
gradient-based methods by ignoring the process of solving inner-level Problem B(θ) and just viewing
ξθ as an explicit differentiable function of θ. Then, based on interior-point methods [48], one can
introduce a logarithmic barrier function for each task constraint, − ln (cid:0)−Ri
(cid:1), and a barrier parameter
(cid:15) > 0. This leads to solving the following unconstrained Problem SP((cid:15)) sequentially

θ∗((cid:15)) = arg min

θ

(cid:96)(ξθ, θ) − (cid:15)

(cid:88)l

i=1

ln (cid:0)−Ri(ξθ, θ)(cid:1)

SP((cid:15))

for a ﬁxed (cid:15). By controlling (cid:15) → 0, θ∗((cid:15)) is expected to converge to the solution θ∗ to Problem P.
Although plausible, the above process has the following technical challenges to be addressed:
(1) As ξθ is a solution to the constrained optimal control Problem B(θ), is ξθ differentiable? Does

the auxiliary control system [71] exist for solving ∂ξθ
∂θ ?

(2) Since we want to obtain ξθ and ∂ξθ

Problem B(θ), can we use an unconstrained system to approximate both ξθ and ∂ξθ
can the accuracy of the approximations for ξθ and ∂ξθ

∂θ at as low cost as possible, instead of solving the constrained
∂θ ? Importantly,

∂θ be arbitrarily and safely controlled?

(3) Can we guarantee that the approximation for ξθ is safe in a sense that the approximation always

respects the system original inequality constraints gt ≤ 0 and gT ≤ 0?

(4) With the safe approximations for both ξθ and ∂ξθ

∂θ , can accuracy of the solution to the outer-level

unconstrained optimization SP((cid:15)) be arbitrarily controlled towards θ∗?

(5) With the safe approximations for both ξθ and ∂ξθ

∂θ , can we guarantee the safety of the outer-level

inequality constraints Ri ≤ 0 during the optimization for the outer-level SP((cid:15))?

The following paper will address the above challenges. For reference, we give a quick overview:
Challenge (1) will be addressed in Section 4 and the result is in Theorem 1; Challenges (2) and (3)
will be addressed in Section 5 and the result is in Theorem 2; Challenges (4) and (5) will be addressed
in Section 6 and the result is in Theorem 3; and Section 7 gives some representative applications.

4

4 Differentiability for Σ(θ) and its Auxiliary Control System

4.1 Differentiability of ξθ
For the constrained optimal control system Σ(θ) in (1), we deﬁne the following Hamiltonian Lt for
t = 0, 1, ..., T −1 and LT , respectively,

Lt = ct(xt, ut, θ) + λ(cid:48)
LT = cT (xT , θ) + v(cid:48)

t+1f (xt, ut, θ) + v(cid:48)

tgt(xt, ut, θ) + w(cid:48)

tht(xt, ut, θ),

(2a)

T gT (xT , θ) + w(cid:48)

T hT (xT , θ),

(2b)
where λt ∈ Rn is the costate, vt ∈ Rqt and wt ∈ Rst are multipliers for the inequality and equality
constraints, respectively. The well-known second-order condition for ξθ to be a local isolated (locally
unique) minimizing trajectory to Σ(θ) in Problem B(θ) has been well-established in [73]. For
completeness, we present it in Lemma A.2 in Appendix A. Lemma A.2 states that there exist costate
sequence λθ
0:T and wθ
0:T ) satisﬁes
the well-known Constrained Pontryagin Minimum Principle (C-PMP) given in (S.5) in Lemma A.2.
Based on the above, one can have the following result for the differentiability of ξθ.
Lemma 1 (Differentiability of ξθ). Given a ﬁxed ¯θ, assume the following conditions hold for Σ( ¯θ):

1:T , and multiplier sequences vθ

0:T , such that (ξθ, λθ

0:T , wθ

1:T , vθ

(i) the second-order condition (Lemma A.2) is satisﬁed for Σ( ¯θ);
(ii) the gradients of all binding constraints at ξ ¯θ are linearly independent (binding constraints

include all equality constraints and all active inequality constraints);

1:T ,vθ

(iii) strict complementarity holds at ξ ¯θ, i.e., active inequality constraint has positive multiplier.
Then, for all θ in a neighborhood of ¯θ, there exists a unique once-continuously differentiable function
(ξθ,λθ
0:T ) that satisﬁes the second-order condition (Lemma A.2) for the constrained opti-
(cid:1) at θ= ¯θ. Hence, ξθ
(cid:1) = (cid:0)ξ ¯θ, λ
mal control system Σ(θ) with (cid:0)ξθ, λθ
0:T , wθ
0:T
is a local isolated minimizing trajectory to Σ(θ). Further, for all θ near ¯θ, the strict complementarity
is preserved, and the linear independence of the gradients of all binding constraints at ξθ hold.

0:T , w¯θ

¯θ
1:T , v ¯θ

0:T ,wθ

1:T , vθ

0:T

The proof of Lemma 1 can directly follow the well-known ﬁrst-order sensitivity result in Theorem 2.1
in [74]. Here, conditions (i)-(iii) are the sufﬁcient conditions to guarantee the applicability of the
well-known implicit function theorem [75] to the C-PMP. Condition (ii) is well-known and serves as
a sufﬁcient condition for the constraint qualiﬁcation to establish the C-PMP (see Corollary 3, pp. 22,
[48]). Condition (iii) is necessary to ensure that the Jacobian matrix in the implicit function theorem
is invertible, and it also leads to the persistence of strict complementarity, saying that the inactive
inequalities remain inactive and active ones remain active and there is no ‘switching’ between them
near ¯θ. Both our practice and previous works [68, 69, 71, 74, 76] show that the conditions (i)-(iii) are
very mild and the differentiability of ξθ can be attained almost everywhere in the space of θ.

4.2 Auxiliary Control System to Solve ∂ξθ
∂θ
If the conditions (i)-(iii) in Lemma 1 for differentiability of ξθ hold, we next show that ∂ξθ
∂θ can also
be efﬁciently solved by an auxiliary control system, which is originally proposed in the foundational
work [71]. First, we deﬁne the new state and input (matrix) variables Xt ∈ Rn×r and Ut ∈ Rm×r,
respectively. Then, we introduce the following auxiliary control system,

control cost:

¯J = Tr

(cid:32)

T −1
(cid:88)

t=0

(cid:20)Xt
Ut

(cid:21)(cid:48) (cid:20)Lxx
t
Lux
t

1
2

(cid:21)

Lxu
t
Luu
t

(cid:21) (cid:20)Xt
Ut

+

(cid:20)Lxθ
t
Luθ
t

(cid:21)(cid:48) (cid:20)Xt
Ut

(cid:21)(cid:33)

Σ(ξθ) :

+ Tr

(cid:18) 1
2

X (cid:48)

T Lxx

T XT + (Lxθ

T )(cid:48)XT

(cid:19)

(3)

subject to
dynamics: Xt+1 = F x
T XT + ¯Gθ
t Xt + ¯Gu

¯Gx
¯Gx

t Xt + F u
T = 0,
t Ut + ¯Gθ

t Ut + F θ
H x

t = 0,

terminal constraint:

path constraint:

t with X0 = X θ
0

T XT + H θ
H x

T = 0,
t Xt + H u

t Ut + H θ

t = 0.

t and Lxx

Here, Lx
Lt in (2) with respect to x; F x

t denote the ﬁrst- and second- order derivatives, respectively, of the Hamiltonian
t , ¯Gt denote the ﬁrst-order derivatives of f t, ht, ¯gt with respect

t , H x

5

1:T , vθ

1:T , vθ

0:T , wθ
0:T

0:T , and wθ

(cid:1), where λθ

to x, respectively, where ¯gt is the vector of stacking all active inequality constraints in gt; and the
similar convention applies to the other notations. All derivative matrices deﬁning Σ(ξθ) are evaluated
at (cid:0)ξθ, λθ
0:T are usually the byproducts of a constrained
optimal control solver [77] or can be easily solved from the C-PMP given ξθ, as done in [71]. We
note that Σ(ξθ) is a Equality-constrained Linear Quadratic Regulator (LQR) system, as its control
cost function is quadratic and dynamics and constraints are linear. For the above Σ(ξθ), we have the
following important result without additional assumptions.
Theorem 1 ( ∂ξθ
∂θ is a globally unique minmizing trajectory to Σ(ξθ)). Let the conditions (i)-(iii) in
Lemma 1 for differentiability of ξθ hold. Then, the auxiliary control system Σ(ξθ) in (3) has a
globally unique minimizing trajectory, denoted as (cid:8)X θ

(cid:9), which is exactly ∂ξθ

∂θ , i.e.,

(cid:110)

X θ

0:T , U θ

0:T −1

(cid:111)

=

∂ξθ
∂θ

with X θ

t =

and U θ

t =

∂uθ
t
∂θ

.

(4)

0:T −1

0:T , U θ
∂xθ
t
∂θ

The proof of the above theorem is in Appendix B. Theorem 1 states that as long as the conditions (i)-
(iii) in Lemma 1 for differentiability of ξθ are satisﬁed, without additional assumptions, the auxiliary
control system Σ(ξθ) always has a globally unique minimizing trajectory, which is exactly ∂ξθ
∂θ .
Thus, obtaining ∂ξθ
∂θ is equivalent to solving Σ(ξθ), which be efﬁciently done thanks to the recent
development of the equality-constrained LQR algorithms [78–80], all of which have a complexity of
O(T ). The algorithm that implements Theorem 1 is given in Algorithm 1 in Appendix E.1.

∂θ

5 Safe Unconstrained Approximations for ξθ and ∂ξθ
From Section 4, we know that one can solve the constrained system Σ(θ) to obtain ξθ and solve its
auxiliary control system Σ(ξθ) to obtain ∂ξθ
∂θ . Although theoretically appealing, there are several
difﬁculties in implementation. First, solving a constrained optimal control Problem B(θ) is not as
easy as solving an unconstrained optimal control, for which many trajectory optimization algorithms,
e.g., iLQR [81] and DDP [82], are available. Second, establishing Σ(ξθ) requires the values of
the multipliers vθ
0:T . And third, to construct Σ(ξθ), one also needs to identify all active
inequality constraints ¯gt, which can be numerically difﬁcult due to numerical error (we will show
this in later experiments). All those difﬁculties motivate us to develop a more efﬁcient paradigm to
obtain both ξθ and ∂ξθ
To proceed, we ﬁrst convert the constrained system Σ(θ) to an unconstrained system Σ(θ, γ) by
adding all constraints to its control cost via barrier functions. Here, we use quadratic barrier function
for each equality constraint and logarithm barrier functions for each inequality constraint; and all
barrier functions are associated with the same barrier parameter γ > 0. This leads to Σ(θ, γ) to be

∂θ , which is the goal of this section.

0:T and wθ

control cost: J(θ, γ) =

T −1
(cid:88)

(cid:16)

t=0

ct(xt, ut, θ)−γ

qt(cid:88)

i=1

ln (cid:0)−gt,i(xt, ut, θ)(cid:1)+

1
2γ

st(cid:88)

i=1

γ

qT(cid:88)

i=1

(cid:0)ht,i(xt, ut, θ)(cid:1)2(cid:17)

+ cT (xT , θ)−

ln (cid:0)−gT,i(xT , θ)(cid:1)+

1
2γ

sT(cid:88)

i=1

(cid:0)hT,i(xT , θ)(cid:1)2,

(5)

Σ(θ, γ) :

subject to
dynamics: xt+1 = f (xt, ut, θ) with x0 = x0(θ),

∀t.

The trajectory ξ(θ,γ) =

(cid:110)

0:T , u(θ,γ)
x(θ,γ)

0:T −1

(cid:111)

produced by the above unconstrained system Σ(θ, γ) is

ξ(θ,γ) =

(cid:110)

0:T , u(θ,γ)
x(θ,γ)

0:T −1

(cid:111)

∈ arg min{x0:T ,u0:T −1}

J(θ, γ)

s.t. xt+1 = f (xt, ut, θ) with x0 = x0(θ),

SB(θ, γ)

that is, ξ(θ,γ) is minimizing the new control cost J(θ, γ) subject to only dynamics. Then we have the
following important result about the safe unconstrained approximation for ξθ and ∂ξθ
∂θ using Σ(θ, γ).

6

Theorem 2. Let conditions (i)-(iii) in Lemma 1 for differentiability of ξθ hold. For any small γ > 0,

(a) there exists a local isolated minimizing trajectory ξ(θ,γ) that solves Problem SB(θ, γ), and
(cid:0)x(θ,γ)

, θ(cid:1)<0 and gT
(b) ξ(θ,γ) is once-continuously differentiable with respect to (θ, γ), and

Σ(θ, γ) is well-deﬁned at ξ(θ,γ), i.e., gt

, θ(cid:1)<0;

, u(θ,γ)
t

(cid:0)x(θ,γ)

T

t

ξ(θ,γ) → ξθ

and

∂ξ(θ,γ)
∂θ

→

∂ξθ
∂θ

as γ → 0;

(6)

(c) the trajectory derivative
control system Σ(cid:0)ξ(θ,γ)

∂ξ(θ,γ)
∂θ
(cid:1) corresponding to Σ(θ, γ).

is a globally unique minimizing trajectory to the auxiliary

∂θ

The proof of the above theorem is given in Appendix C. It is worth noting that the above assertions
require no additional assumption except the same conditions (i)-(iii) for differentiability of ξθ. We
make the following comments on the above results, using an illustrative cartpole example in Fig. 1.
First, assertion (b) states that by choosing a small γ > 0,
one can simply use ξ(θ,γ) and ∂ξ(θ,γ)
of the unconstrained
optimal control system Σ(θ, γ) to approximate ξθ and
∂ξθ
∂θ of the original constrained system Σ(θ), respectively.
Second, notably, assertion (b) also states that the above
approximations can be controlled for arbitrary accuracy
by simply letting γ → 0, as illustrated in the upper panels
in Fig. 1. Third, more importantly, assertion (a) states
that the above approximations are always safe in a sense
that the approximation ξ(θ,γ) with any small γ > 0 is
guaranteed to satisfy all inequality constraints in the orig-
inal Σ(θ), as illustrated in the bottom panels in Fig. 1.
Finally, similar to Theorem 1, assertion (c) states that the
derivative ∂ξ(θ,γ)
for Σ(θ, γ) is a globally unique mini-
mizing trajectory to its corresponding auxiliary control
system Σ(cid:0)ξ(θ,γ)
In addition to the theoretical importance of Theorem 2, we also summarize its algorithmic advantage
compared to directly handling the original constrained system Σ(θ) and its auxiliary control system
Σ(ξθ). First, solving the unconstrained Σ(θ, γ) is easier than solving the constrained Σ(θ) as more
off-the-shelf algorithms are available for unconstrained trajectory optimization than for constrained
(cid:1), there is no need to identify the inactive and
one. Second, when solving ∂ξ(θ,γ)
active inequality constraints, as opposed to solving ∂ξθ
∂θ using Σ(ξθ); thus it is easier to implement
and more numerically stable (we will show this later in experiments). Third, in contrast to Theorem 1,
(cid:1) avoid dealing with the multipliers v0:T and w0:T . Finally,
the unconstrained Σ(θ, γ) and Σ(cid:0)ξ(θ,γ)
by absorbing hard inequality constraints into the control cost through barrier functions, Σ(θ, γ) intro-
duces the ‘softness’ of constraints and mitigates the discontinuous ‘switching’ between inactive/active
inequalities over a large range of θ. This leads to a more numerically stable algorithm, as we will
show in later experiments. Implementation of Theorem 2 is given in Algorithm 2 in Appendix E.2.

Figure 1: ξ(θ,γ) and ∂ξ(θ,γ)
ξθ and ∂ξθ

(cid:1), thus PDP [71] directly applies here.

∂θ under different γ > 0.

using Σ(cid:0)ξ(θ,γ)

approximate

∂θ

∂θ

∂θ

6 Safe PDP to Solve Problem P

According to Theorem 2, we use the safe unconstrained approximation system Σ(θ, γ) in (5) to
replace the original inner-level constrained system Σ(θ) in (1). Then, we give the following important
result for solving Problem P, which addresses the Challenges (4) and (5) in Section 3.
Theorem 3. Consider all functions deﬁning the constrained optimal control system Σ(θ) are at least
three-times continuously differentiable, and let the conditions (i)-(iii) in Lemma 1 for differentiability
of ξθ hold in a neighborhood of θ∗. Suppose that the second-order condition for a local isolated
minimizor θ∗ to Problem P is satisﬁed, that the gradients ∇θRi(ξθ∗ , θ∗) of all binding constraints
Ri(ξθ, θ) = 0 are linearly independent at θ∗, and that the strict complementary holds at θ∗. Then,
for any small (cid:15) > 0 and any small γ > 0, the following outer-level unconstrained approximation

θ∗((cid:15), γ) = arg min

θ

(cid:96)(cid:0)ξ(θ,γ), θ(cid:1) − (cid:15)

(cid:88)l

i=1

(cid:16)

ln

−Ri

(cid:0)ξ(θ,γ), θ(cid:1)(cid:17)

,

SP((cid:15), γ)

7

1041031021010.00.51.01.5||(,)||21041031021010246||(,)||2×103=0.5true sol.=104constraint0510152025Time505Cart force input=0.5true sol.=104constraint0510152025Time1012Cart positionwith ξ(θ,γ) being the optimal trajectory to the inner-level safe unconstrained approximation system
Σ(θ, γ) in (5), has the following assertions:

(a) there exists a local isolated minimizor θ∗((cid:15), γ) to the above SP((cid:15), γ), and the corresponding
trajectory ξ(θ∗((cid:15),γ),γ) from the inner-level approximation system Σ(cid:0)θ∗((cid:15), γ), γ(cid:1) is safe with
(cid:0)ξ(θ∗((cid:15),γ),γ), θ∗((cid:15), γ)(cid:1) < 0, i = 1, 2, ..., l;
respect the original outer-level constraints, i.e., Ri

(b) θ∗((cid:15), γ) is once-continuously differentiable with respect to both (cid:15) and γ, and

θ∗((cid:15), γ) → θ∗

as

((cid:15), γ) → (0, 0);

(7)

(c) for any θ near θ∗((cid:15), γ), ξ(θ,γ) from the inner-level approximation system Σ(θ, γ) is safe
(cid:0)ξ(θ,γ), θ(cid:1) < 0, i = 1, 2, ..., l.

with respect to the original outer-level constraints, i.e., Ri

The proof of the above theorem is given in Appendix D. The above result says that instead of solving
the original constrained Problem P with the inner-level constrained system Σ(θ) in (1), one can solve
an unconstrained approximation Problem SP((cid:15), γ) with the inner-level safe unconstrained approxima-
tion system Σ(θ, γ) in (5). Particularly, we make the following comments on the importance of the
above theorem. First, claim (a) afﬁrms that although the inner-level trajectory ξ(θ,γ) is an approxima-
tion (recall Theorem 2), the outer-level unconstrained Problem SP((cid:15), γ) always has a locally unique
solution θ∗((cid:15), γ); furthermore, at θ∗((cid:15), γ), the corresponding inner-level trajectory ξ(θ∗((cid:15),γ),γ) is safe
(cid:0)ξ(θ∗((cid:15),γ),γ), θ∗((cid:15), γ)(cid:1) < 0, i = 1, 2, ..., l.
with respect to the original outer-level constraints, i.e., Ri
Second, claim (b) asserts that the accuracy of the solution θ∗((cid:15), γ) to the outer-level approximation
Problem SP((cid:15), γ) is controlled jointly by the inner-level barrier parameter γ and outer-level barrier
parameter (cid:15): as both barrier parameters approach zero, θ∗((cid:15), γ) is converging to the true solution θ∗
to the original Problem P. Third, claim (c) says that during the local search of the outer-level solution
θ∗((cid:15), γ), the corresponding inner-level trajectory ξ(θ,γ) is always safe with respect to the original
(cid:0)ξ(θ,γ), θ(cid:1) < 0, i = 1, 2, ..., l. The above Theorem 3, together with
outer-level constraints, i.e., Ri
Theorem 2 provide the safety- and accuracy- guarantees for the whole Safe PDP framework. Then
entire Safe PDP algorithm is given in Algorithm 3 in Appendix E.3.

7 Applications to Different Safety-Critical Tasks

We apply Safe PDP to solve some representative safety-critical learning/control tasks. For a speciﬁc
task, one only needs to specify the parameterization detail of Σ(θ), a task loss (cid:96)(ξθ, θ), and task
constraints Ri(ξθ, θ) in Problem P. The experiments are performed on the systems of different
complexities in Table 1. All codes are available at https://github.com/wanxinjin/Safe-PDP.

System Σ(θ)

Dynamics f (θdyn)

Control cost J(θobj) Constraints g(θcstr)

Table 1: Experimental environments [71]

Cartpole
Two-link Robot arm
6-DoF quadrotor
6-DoF rocket landing

cart & pole masses and length
length and mass of links
mass, wing length, inertia
rocket mass, length, inertia

2+

ct=(cid:107)u(cid:107)2
obj(x−xgoal)(cid:107)2
2,
cT =
obj(x−xgoal)(cid:107)2
2

(cid:107)θ(cid:48)

(cid:107)θ(cid:48)

gx(x)≤Xmax,
(cid:107)u(cid:107)2/∞ ≤ Umax,

θcstr={Xmax, Umax}

Note that for each system, g(θcstr) includes the immediate constraints on system input u and state x at any time
instance; gx is known; (cid:107)·(cid:107)2/∞ is the 2 or ∞ norm; and time horizon T is around 50 for all systems.

Problem I: Safe Policy Optimization aims to ﬁnd a policy that minimizes a control cost J subject
to constraints g while guaranteeing that any intermediate policy during optimization should never
violate the constraints. To apply Safe PDP to solve such a problem for the systems in Table 1, we set:

Σ(θ) :

dynamics: xt+1 = f (xt, ut) with x0,

policy: ut = πt(xt, θ),

(8)

where dynamics f is learned from demonstrations in Problem III, and π(θ) is represented by a (deep)
feedforward neural network (NN) with θ the NN parameter. In Problem P, the task loss (cid:96)(ξθ, θ) is
set as J(θobj), and task constraints Ri(ξθ, θ) as g(θcstr), with both θobj and θcstr known. Then, safe
policy optimization is to solve Problem P using Safe PDP. The results for the robot arm and 6-DoF
maneuvering quadrotor are in Fig. 2, and the other results and details are in Appendix F.1.

8

(a) Robot-arm loss (b) Constraint violation during opt. (c) Quadrotor loss (d) Constraint violation during opt.

Figure 2: Safe neural policy optimization for robot-arm (a)-(b) and 6-DoF quadrotor (c)-(d).

Fig. 2a and 2c plot loss (control cost) versus gradient-descent iteration under different (cid:15), showing that
the NN policy achieves a good convergence when (cid:15) ≤ 10−2 (as asserted by Theorem 3). Fig. 2b and
2d show all indeterminate control trajectories generated from the NN policy during entire iterations;
we also mark the constraints Umax and compare with the unconstrained policy optimization under
the same settings. The results conﬁrm that Safe PDP enables to achieve an optimal policy while
guaranteeing that any intermediate policy throughout optimization is safe.

Problem II: Safe Motion Planning searches for a dynamics-feasible trajectory that optimizes a
criterion and avoids unsafe regions (obstacles), meanwhile guaranteeing that any intermediate motion
trajectory during search must avoid the unsafe regions. To apply Safe PDP to solve such problem, we
specialize Σ(θ) as (8) except that policy here is ut = u(t, θ), which is represented by Lagrangian
polynomial [83] with θ the parameters (pivots). In Problem P, task loss is set as J(θobj), and task
constraints as g(θcstr), with θobj and θcstr known in Table 1. The safe planning results using Safe PDP
for cartpole and 6-DoF rocket landing are in Fig. 2, in comparison with ALTRO, a state-of-the-art
constrained trajectory optimization method [21]. Other results and more details are in Appendix F.2.

(a) Cartpole loss

(b) Constraint violation during opt.

(c) Rocket loss

(d) Constraint violation during opt.

Figure 3: Safe motion planning for cartpole (a)-(b) and 6-DoF rocket powered landing (c)-(d).

Fig. 3a and 3c plot the task loss versus gradient-descent iteration, showing that the trajectory achieves
a good convergence with (cid:15) ≤ 10−2. Fig. 3b and 3d show all intermediate motion trajectories during
entire optimization, with constraints marked. The results conﬁrm that Safe PDP can ﬁnd an optimal
trajectory while always respecting constraints throughout planning process.

0:T , udemo

Problem III: Learning MPC from Demonstrations. Suppose for all systems in Table 1, the control
cost J(θcost), dynamics f (θdyn) and constraints gt(θcstr) are all unknown and parameterized as in
Table 1. We aim to jointly learn θ = {θcost, θdyn, θcstr} from demonstrations ξdemo = {xdemo
0:T −1}
of a true expert system. In Problem P, set Σ(θ) as (1), consisting of J(θcost), f (θdyn), and gt(θcstr)
parameterized; set task loss (cid:96)(ξθ, θ) = (cid:107)ξdemo−ξθ(cid:107)2, which quantiﬁes the reproducing loss between
ξdemo and ξθ; and there is no task constraints. By solving Problem P, we can learn Σ(θ) such that its
reproduced ξθ is closest to given ξdemo. The demonstrations ξdemo here are generated with θ known
(two episode trajectories for each system with time horizon T =50). The plots of the loss versus
gradient-descent iteration are in Fig. 4, and more details and results are in Appendix F.3.
In Fig. 4a-4d, for each system, we use three strategies to obtain ξθ and ∂ξθ
∂θ for Σ(θ): (A) use
a solver [77] to obtain ξθ and use Theorem 1 to obtain ∂ξθ
∂θ ; (B) use Theorem 2 to approximate
both ξθ and ∂ξθ
, respectively, γ=10−2; and (C) use a solver to obtain ξθ and
Theorem 2 only for ∂ξθ
∂θ . Fig. 4a-4d show that for Strategies (B) and (C), the reproducing loss quickly
converges to zeros, indicating that the dynamics, constraints, and control cost are successfully learned
to reproduce the demonstrations. Fig. 4a-4d also show numerical instability for strategy (A); this is
due to the discontinuous ‘switching’ of active inequalities between iterations, and also the error in
correctly identifying active inequalities (we identify them by checking gt,i > −δ with δ > 0 a small

∂θ by ξ(θ,γ) and ∂ξ(θ,γ)

∂θ

9

=101=102=103=104050010001500Iteration14.515.015.516.016.5Loss (control cost)Iter. #0Iter. #1500ut,1ut,20510152025Time t210123Generated utSafe PDP, =104Iter. #0Iter. #15000510152025Time tumaxuminUnconstrained=1=101=1020500100015002000Iteration20002500300035004000Loss (control cost)Iter. #0Iter. #20000510152025Time t05101520Generated ||ut||Safe PDP, =102Iter. #0Iter. #20000510152025Time tumaxUnconstrained=1=101=1020100020003000Iteration160180200220240260Loss (planning loss)Iter. #0Iter. #3000505ControlSafe PDP, =102Iter. #0Iter. #3000ALTROIter. #0Iter. #30000510152025Time t202Cart pos.Iter. #0Iter. #30000510152025Time tumaxuminxmaxxmin=1=101=102050010001500Iteration1234Loss (planning loss)×105Iter. #0Iter. #15000102030Thrust ||u||2Safe PDP, =102Iter. #0Iter. #1500ALTROIter. #0Iter. #1500010203040Time t0.00.20.4Tilt angleIter. #0Iter. #1500010203040Time t||u||maxtiltmaxthreshold), as analyzed in Section 5. More analysis is given in Appendix F.3. Note that we are not
aware of any existing methods that can handle jointly learning of cost, dynamics, and constraints
here, and thus we have not given benchmark comparison. Fig. 4e gives timing results of Safe PDP.

(a) Cartpole (log-y)

(b) Robot-arm

(c) Quadrotor (log-y)

(d) Rocket

(e) Timing

Figure 4: Jointly learning dynamics, constraints, and control cost from demonstrations.

8 Discussion

Comparisons with other differentiable frameworks. Fig. 5 compares
Safe PDP, CasADi [70], and Differentiable MPC [68] for the computational
efﬁciency of differentiating an optimal trajectory of a constrained optimal
control system with different control horizons T . The results show a signiﬁ-
cantly computational advantage of Safe PDP over CasADi and Differentiable
MPC. Speciﬁcally, Safe PDP has a complexity of O(T ), while CasADi and
Differentiable MPC have at least O(T 2). This is because both CasADi and
differentiable MPC are based on the implicit function theorem [75] and need
to compute the inverse of a Hessian matrix of the size proportional to T × T .
In contrast, Safe PDP solves the gradient of a trajectory by constructing an
Auxiliary Control System, which can be solved using the Riccati equation.

Figure 5: Time for dif-
ferentiating optimal tra-
jectory with different T .

Limitation of Safe PDP. Safe PDP requires a safe (feasible) initialization such that the log-barrier
control cost or loss is well-deﬁned. While restrictive, safe initialization is common in safe learning
[63, 84]. We have the following empiricism on how to provide safe initializations for different types of
problems, as adopted in our experiments in Section 7. In safe policy optimization, one could ﬁrst use
supervised learning to learn a safe policy from some safe trajectories/demonstrations (not necessarily
be optimal) and then use the learned safe policy to initialize Safe PDP. In safe motion planning, one
could arbitrarily provide a safe trajectory (not necessarily optimal) to initialize Safe PDP. In learning
MPCs, the goal includes learning of constraint itself, and there is no such requirement.

Strategies to accelerate forward pass of Safe PDP. There are many strategies to accelerate a
long-horizon trajectory optimization (optimal control) in the forward pass of Safe PDP. (I) One
effective way is to scale the (continuous) long-horizon problem into a smaller one (e.g., a unit) by
applying a time-warping function to the dynamics and cost function [85]. After solving the scaled
short-horizon problem, re-scale the trajectory back. (II) There are also ‘warm-up’ tricks, e.g., one can
initialize the trajectory at the next iteration using the result of the previous iteration. (III) One can
also use a hierarchical strategy to solve trajectory optimization from coarse to ﬁne resolutions. We
have tested and provided the comparison for the above three acceleration strategies in Appendix G.2.

Please refer to Appendix G for more discussion, which includes G.1: comparison between Safe-PDP
and non-safe PDP; G.2: comparison of different strategies for accelerating long-horizon trajectory
optimization; G.3: trade-offs between accuracy and computational efﬁciency using barrier penalties;
G.4: learning MPCs from non-optimal data; and G.5: detailed discussion on limitation of Safe PDP.

9 Conclusions

This paper proposes a Safe Pontryagin Differentiable Programming methodology, which establishes a
provable and systematic safe differentiable framework to solve a broad class of safety-critical control
and learning tasks with different types of safety constraints. For a constrained system and task, Safe
PDP approximates both the solution and its gradient in backward pass by solving their more efﬁcient
unconstrained counterparts. Safe PDP has established two results: one is the controlled accuracy
guarantee for approximations of the solution and its gradient, and the other is the safety guarantee for
constraint satisfaction throughout the control and learning process. We envision the potential of Safe
PDP for addressing various safety-critical problems in machine learning, control, and robotics ﬁelds.

10

Thm 1Thm 2 for  and  Thm 2 only for 0204060Iteration101100101102Reproducing lossThm 1Thm 2 for  and  Thm 2 only for 0204060Iteration0102030405060Reproducing lossThm 1Thm 2 for  and  Thm 2 only for 050100Iteration101100101102Reproducing lossThm 1Thm 2 for  and  Thm 2 only for 03060Iteration0100200300400Reproducing loss20406080100Time horizon T0.000.250.500.751.001.25Time per iteration [s]Theorem 1 forwardTheorem 1 backwardTheorem 2 forwardTheorem 2 backward100200300Time horizon T012345Time for differentiation [s]Safe PDPCasADidiff MPCAcknowledgments and Disclosure of Funding

This work is supported by the NASA University Leadership Initiative (ULI) under grant number
80NSSC20M0161. The research of Prof. George J. Pappas is supported by the AFOSR Assured
Autonomy in Congested Environments under grant number FA9550-19-1-0265. This work has been
done primarily in the last semester of Wanxin Jin’s Ph.D. study at Purdue University. Wanxin Jin
thanks Prof. Zhaoran Wang for some discussion about this work.

References

[1] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.

[2] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas-
tering the game of go with deep neural networks and tree search. Nature, 529(7587):484–489,
2016.

[3] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In
International Conference on Learning Representations, 2016.

[4] Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with uncertain

transition matrices. Operations Research, 53(5):780–798, 2005.

[5] Matthias Heger. Consideration of risk in reinforcement learning. In International Conference

on Machine Learning, pages 105–111, 1994.

[6] Ronald A Howard and James E Matheson. Risk-sensitive markov decision processes. Manage-

ment science, 18(7):356–369, 1972.

[7] Teodor Mihai Moldovan and Pieter Abbeel. Safe exploration in markov decision processes.

International Conference on Machine Learning, 2012.

[8] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.

[9] Kemin Zhou and John Comstock Doyle. Essentials of robust control, volume 104. Prentice hall

Upper Saddle River, NJ, 1998.

[10] Javier Garcıa and Fernando Fernández. A comprehensive survey on safe reinforcement learning.

Journal of Machine Learning Research, 16(1):1437–1480, 2015.

[11] Eitan Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.

[12] Eitan Altman. Constrained markov decision processes with total cost criteria: Lagrangian
approach and dual linear program. Mathematical methods of operations research, 48(3):387–
417, 1998.

[13] Ming Yu, Zhuoran Yang, Mladen Kolar, and Zhaoran Wang. Convergent policy optimization

for safe reinforcement learning. Advances in Neural Information Processing Systems, 2019.

[14] Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained
reinforcement learning with percentile risk criteria. International Conference on Machine
Learning, 18(1):6070–6120, 2017.

[15] Shalabh Bhatnagar and K Lakshmanan. An online actor–critic algorithm with function ap-
proximation for constrained markov decision processes. Journal of Optimization Theory and
Applications, 153(3):688–708, 2012.

11

[16] Dongsheng Ding, Kaiqing Zhang, Tamer Basar, and Mihailo Jovanovic. Natural policy gradient
primal-dual method for constrained markov decision processes. Advances in Neural Information
Processing Systems, 33, 2020.

[17] Miguel Calvo-Fullana, Santiago Paternain, Luiz FO Chamon, and Alejandro Ribeiro. State
augmented constrained reinforcement learning: Overcoming the limitations of learning with
rewards. arXiv preprint arXiv:2102.11941, 2021.

[18] Yatin Nandwani, Abhishek Pathak, Parag Singla, et al. A primal dual formulation for deep

learning with constraints. Advances in Neural Information Processing Systems, 2019.

[19] Maïtine Bergounioux, Kazufumi Ito, and Karl Kunisch. Primal-dual strategy for constrained
optimal control problems. SIAM Journal on Control and Optimization, 37(4):1176–1194, 1999.

[20] Matthew R Kirchner, Gary Hewer, Jérôme Darbon, and Stanley Osher. A primal-dual method
for optimal control and trajectory generation in high-dimensional systems. In Conference on
Control Technology and Applications, pages 1583–1590, 2018.

[21] Taylor A Howell, Brian E Jackson, and Zachary Manchester. Altro: A fast solver for constrained
In IEEE/RSJ International Conference on Intelligent Robots and

trajectory optimization.
Systems, pages 7674–7679, 2019.

[22] Simon S Du and Wei Hu. Linear convergence of the primal-dual gradient method for convex-
In International Conference on

concave saddle point problems without strong convexity.
Artiﬁcial Intelligence and Statistics, pages 196–205. PMLR, 2019.

[23] Chi Jin, Praneeth Netrapalli, and Michael Jordan. What is local optimality in nonconvex-
nonconcave minimax optimization? In International Conference on Machine Learning, pages
4880–4889. PMLR, 2020.

[24] Yinlam Chow, Oﬁr Nachum, Aleksandra Faust, Edgar Duenez-Guzman, and Mohammad
Ghavamzadeh. Lyapunov-based safe policy optimization for continuous control. arXiv preprint
arXiv:1901.10031, 2019.

[25] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization.

In International Conference on Machine Learning, pages 22–31. PMLR, 2017.

[26] Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J Ramadge. Projection-based
constrained policy optimization. International Conference on Learning Representations, 2020.

[27] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pages 1889–1897.
PMLR, 2015.

[28] Yinlam Chow, Oﬁr Nachum, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh. A
lyapunov-based approach to safe reinforcement learning. Advances in Neural Information
Processing Systems, 2018.

[29] Theodore J Perkins and Andrew G Barto. Lyapunov design for safe reinforcement learning.

Journal of Machine Learning Research, 3(Dec):803–832, 2002.

[30] Felix Berkenkamp, Matteo Turchetta, Angela P Schoellig, and Andreas Krause. Safe model-
based reinforcement learning with stability guarantees. Advances in Neural Information Pro-
cessing Systems, 2017.

[31] Aleksandr Mikhailovich Lyapunov. The general problem of the stability of motion. International

journal of control, 55(3):531–534, 1992.

[32] Peter Giesl and Sigurdur Hafstein. Review on computational methods for lyapunov functions.

Discrete & Continuous Dynamical Systems-B, 20(8):2291, 2015.

[33] Matteo Turchetta, Felix Berkenkamp, and Andreas Krause. Safe exploration in ﬁnite markov
decision processes with gaussian processes. Advances in Neural Information Processing Systems,
29:4312–4320, 2016.

12

[34] Akifumi Wachi and Yanan Sui. Safe reinforcement learning in constrained markov decision
processes. In International Conference on Machine Learning, pages 9797–9806. PMLR, 2020.

[35] Somil Bansal, Mo Chen, Sylvia Herbert, and Claire J Tomlin. Hamilton-jacobi reachability:
A brief overview and recent advances. In IEEE Conference on Decision and Control, pages
2242–2253, 2017.

[36] Peter Wieland and Frank Allgöwer. Constructive safety using control barrier functions. IFAC

Proceedings Volumes, 40(12):462–467, 2007.

[37] Aaron D Ames, Xiangru Xu, Jessy W Grizzle, and Paulo Tabuada. Control barrier function
based quadratic programs for safety critical systems. IEEE Transactions on Automatic Control,
62(8):3861–3876, 2016.

[38] Jaime F Fisac, Anayo K Akametalu, Melanie N Zeilinger, Shahab Kaynama, Jeremy Gillula,
and Claire J Tomlin. A general safety framework for learning-based control in uncertain robotic
systems. IEEE Transactions on Automatic Control, 64(7):2737–2752, 2018.

[39] Sylvia L Herbert, Mo Chen, SooJean Han, Somil Bansal, Jaime F Fisac, and Claire J Tomlin.
In IEEE

Fastrack: A modular framework for fast and guaranteed safe motion planning.
Conference on Decision and Control, pages 1517–1522, 2017.

[40] Mo Chen, Jaime F Fisac, Shankar Sastry, and Claire J Tomlin. Safe sequential path planning
of multi-vehicle systems via double-obstacle hamilton-jacobi-isaacs variational inequality. In
European Control Conference, pages 3304–3309, 2015.

[41] Lawrence C Evans and Panagiotis E Souganidis. Differential games and representation formulas
for solutions of hamilton-jacobi-isaacs equations. Indiana University mathematics journal,
33(5):773–797, 1984.

[42] Aaron D Ames, Samuel Coogan, Magnus Egerstedt, Gennaro Notomista, Koushil Sreenath,
and Paulo Tabuada. Control barrier functions: Theory and applications. In European Control
Conference, pages 3420–3431, 2019.

[43] Jason Choi, Fernando Castaneda, Claire J Tomlin, and Koushil Sreenath. Reinforcement
learning for safety-critical control under model uncertainty, using control lyapunov functions
and control barrier functions. arXiv preprint arXiv:2004.07584, 2020.

[44] Richard Cheng, Gábor Orosz, Richard M Murray, and Joel W Burdick. End-to-end safe
reinforcement learning through barrier functions for safety-critical continuous control tasks. In
AAAI Conference on Artiﬁcial Intelligence, pages 3387–3395, 2019.

[45] Alexander Robey, Haimin Hu, Lars Lindemann, Hanwen Zhang, Dimos V Dimarogonas,
Stephen Tu, and Nikolai Matni. Learning control barrier functions from expert demonstrations.
In IEEE Conference on Decision and Control, pages 3717–3724, 2020.

[46] Alexander Robey, Lars Lindemann, Stephen Tu, and Nikolai Matni. Learning robust hybrid
control barrier functions for uncertain systems. arXiv preprint arXiv:2101.06492, 2021.

[47] Wanxin Jin, Zhaoran Wang, Zhuoran Yang, and Shaoshuai Mou. Neural certiﬁcates for safe

control policies. arXiv preprint arXiv:2006.08465, 2020.

[48] Anthony V Fiacco and Garth P McCormick. Nonlinear programming: sequential unconstrained

minimization techniques. SIAM, 1990.

[49] Yurii Nesterov and Arkadii Nemirovskii.

Interior-point polynomial algorithms in convex

programming. SIAM, 1994.

[50] Anders Forsgren, Philip E Gill, and Margaret H Wright.

Interior methods for nonlinear

optimization. SIAM review, 44(4):525–597, 2002.

[51] AEB Lim, JB Moore, and L Faybusovich. Linearly constrained lq and lqg optimal control.

IFAC Proceedings Volumes, 29(1):1110–1115, 1996.

13

[52] SJ Wright. Structured interior point methods for optimal control. In IEEE Conference on

Decision and Control, pages 1711–1716, 1991.

[53] Stephen J Wright. Interior point methods for optimal control of discrete time systems. Journal

of Optimization Theory and Applications, 77(1):161–187, 1993.

[54] Christopher V Rao, Stephen J Wright, and James B Rawlings. Application of interior-point
methods to model predictive control. Journal of optimization theory and applications, 99(3):723–
757, 1998.

[55] Anders Hansson and S Boydt. Robust optimal control of linear discrete-time systems using
primal-dual interior-point methods. In American Control Conference, volume 1, pages 183–187,
1998.

[56] Anders Hansson. A primal-dual interior-point method for robust optimal control of linear
discrete-time systems. IEEE Transactions on Automatic Control, 45(9):1639–1655, 2000.

[57] Christian Feller and Christian Ebenbauer. Relaxed logarithmic barrier function based model
predictive control of linear systems. IEEE Transactions on Automatic Control, 62(3):1223–1238,
2016.

[58] Julien Laurent-Varin, J Frederic Bonnans, Nicolas Bérend, Mounir Haddou, and Christophe
Talbot. Interior-point approach to trajectory optimization. Journal of Guidance, Control, and
Dynamics, 30(5):1228–1238, 2007.

[59] John Hauser and Alessandro Saccon. A barrier function method for the optimization of trajectory
functionals with constraints. In IEEE Conference on Decision and Control, pages 864–869.
IEEE, 2006.

[60] Paul Malisani, François Chaplais, and Nicolas Petit. An interior penalty method for optimal
control problems with state and input constraints of nonlinear systems. Optimal Control
Applications and Methods, 37(1):3–33, 2016.

[61] Alexander Domahidi, Aldo U Zgraggen, Melanie N Zeilinger, Manfred Morari, and Colin N
Jones. Efﬁcient interior point methods for multistage problems arising in receding horizon
control. In IEEE conference on decision and control, pages 668–674, 2012.

[62] Andrei Pavlov, Iman Shames, and Chris Manzie. Interior point differential dynamic program-

ming. IEEE Transactions on Control Systems Technology, 2021.

[63] Ilnura Usmanova, Andreas Krause, and Maryam Kamgarpour. Safe non-smooth black-box
optimization with application to policy search. In Learning for Dynamics and Control, pages
980–989. PMLR, 2020.

[64] Yongshuai Liu, Jiaxin Ding, and Xin Liu.

Ipo: Interior-point policy optimization under

constraints. In AAAI Conference on Artiﬁcial Intelligence, pages 4940–4947, 2020.

[65] Priya L Donti, Melrose Roderick, Mahyar Fazlyab, and J Zico Kolter. Enforcing robust
control guarantees within neural network policies. In International Conference on Learning
Representations, 2020.

[66] Bingqing Chen, Priya L. Donti, Kyri Baker, J. Zico Kolter, and Mario Bergés. Enforcing
policy feasibility constraints through differentiable projection for energy optimization. In ACM
International Conference on Future Energy Systems, page 199–210, New York, NY, USA, 2021.
Association for Computing Machinery.

[67] Tu-Hoa Pham, Giovanni De Magistris, and Ryuki Tachibana. Optlayer-practical constrained
optimization for deep reinforcement learning in the real world. In International Conference on
Robotics and Automation, pages 6236–6243. IEEE, 2018.

[68] Brandon Amos, Ivan Dario Jimenez Rodriguez, Jacob Sacks, Byron Boots, and J Zico Kolter.
Differentiable mpc for end-to-end planning and control. In Advances in Neural Information
Processing Systems, 2018.

14

[69] Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural

networks. In International Conference on Machine Learning, pages 136–145. PMLR, 2017.

[70] Joel AE Andersson and James B Rawlings. Sensitivity analysis for nonlinear programming in

casadi. IFAC-PapersOnLine, 51(20):331–336, 2018.

[71] Wanxin Jin, Zhaoran Wang, Zhuoran Yang, and Shaoshuai Mou. Pontryagin differentiable pro-
gramming: An end-to-end learning and control framework. In Advances in Neural Information
Processing Systems, 2020.

[72] Ankur Sinha, Pekka Malo, and Kalyanmoy Deb. A review on bilevel optimization: from
classical to evolutionary approaches and applications. IEEE Transactions on Evolutionary
Computation, 22(2):276–295, 2017.

[73] J Pearson and R Sridhar. A discrete optimal control problem. IEEE Transactions on automatic

control, 11(2):171–174, 1966.

[74] Anthony V Fiacco. Sensitivity analysis for nonlinear programming using penalty methods.

Mathematical programming, 10(1):287–311, 1976.

[75] Walter Rudin et al. Principles of mathematical analysis, volume 3. McGraw-hill New York,

1976.

[76] Charles D Kolstad and Leon S Lasdon. Derivative evaluation and computational experience
with large bilevel mathematical programs. Journal of optimization theory and applications,
65(3):485–499, 1990.

[77] Joel AE Andersson, Joris Gillis, Greg Horn, James B Rawlings, and Moritz Diehl. Casadi: a
software framework for nonlinear optimization and optimal control. Mathematical Programming
Computation, 11(1):1–36, 2019.

[78] Athanasios Sideris and Luis A Rodriguez. A riccati approach to equality constrained linear

quadratic optimal control. In American Control Conference, pages 5167–5172, 2010.

[79] Shuo Yang, Gerry Chen, Yetong Zhang, Frank Dellaert, and Howie Choset. Equality constrained

linear optimal control with factor graphs. arXiv preprint arXiv:2011.01360, 2020.

[80] Forrest Laine and Claire Tomlin. Efﬁcient computation of feedback control for equality-
constrained lqr. In International Conference on Robotics and Automation, pages 6748–6754.
IEEE, 2019.

[81] Weiwei Li and Emanuel Todorov. Iterative linear quadratic regulator design for nonlinear bio-
logical movement systems. In International Conference on Informatics in Control, Automation
and Robotics, pages 222–229, 2004.

[82] David H Jacobson and David Q Mayne. Differential dynamic programming. Number 24.

Elsevier Publishing Company, 1970.

[83] Milton Abramowitz and Irene A Stegun. Handbook of mathematical functions with formulas,

graphs, and mathematical tables, volume 55. US Government printing ofﬁce, 1964.

[84] Felix Berkenkamp, Andreas Krause, and Angela P Schoellig. Bayesian optimization with safety
constraints: safe and automatic parameter tuning in robotics. Machine Learning, pages 1–35,
2021.

[85] Wanxin Jin, Todd D Murphey, Dana Kuli´c, Neta Ezer, and Shaoshuai Mou. Learning from

sparse demonstrations. arXiv preprint arXiv:2008.02159, 2020.

[86] Michael Athans. The matrix minimum principle. Information and control, 11(5-6):592–606,

1967.

[87] Jean Dieudonné. Foundations of modern analysis. New York: Academic Press. Volume 1 of

Treatise on Analysis, 2011.

[88] Rolf Johansson. System modeling and identiﬁcation. Prentice-hall, 1993.

15

[89] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary

differential equations. Advances in Neural Information Processing Systems, 2018.

[90] Wanxin Jin, Dana Kuli´c, Shaoshuai Mou, and Sandra Hirche. Inverse optimal control from
incomplete trajectory observations. The International Journal of Robotics Research, 40(6-
7):848–865, 2021.

[91] Wanxin Jin and Shaoshuai Mou. Distributed inverse optimal control. Automatica, 129, 2021.

[92] Wanxin Jin, Dana Kuli´c, Jonathan Feng-Shun Lin, Shaoshuai Mou, and Sandra Hirche. Inverse
optimal control for multiphase cost functions. IEEE Transactions on Robotics, 35(6):1387–1398,
2019.

[93] Wanxin Jin, Todd D Murphey, and Shaoshuai Mou. Learning from incremental directional

corrections. arXiv preprint arXiv:2011.15014, 2020.

[94] Michael A Patterson and Anil V Rao. Gpops-ii: A matlab software for solving multiple-phase
optimal control problems using hp-adaptive gaussian quadrature collocation methods and sparse
nonlinear programming. ACM Transactions on Mathematical Software (TOMS), 41(1):1–37,
2014.

Checklist

1. For all authors...

(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s

contributions and scope? [Yes]

(b) Did you describe the limitations of your work? [Yes] Please see Sections 8 and

Appendix G.5 in the paper.

(c) Did you discuss any potential negative societal impacts of your work? [N/A]
(d) Have you read the ethics review guidelines and ensured that your paper conforms to

them? [Yes]

2. If you are including theoretical results...

(a) Did you state the full set of assumptions of all theoretical results? [Yes] Pleae ﬁnd

them in all theorems in the paper.

(b) Did you include complete proofs of all theoretical results? [Yes] Please ﬁnd complete

proofs for all theoretical results in the Appendix in the supplementary ﬁle.

3. If you ran experiments...

(a) Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [Yes] Please refer to
the code at https://github.com/wanxinjin/Safe-PDP.

(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they

were chosen)? [Yes] Please see the Appendix F in the supplementary ﬁle.

(c) Did you report error bars (e.g., with respect to the random seed after running experi-

ments multiple times)? [N/A]

(d) Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [Yes] Please see the Appendix F in the
supplementary ﬁle.

4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...

(a) If your work uses existing assets, did you cite the creators? [Yes] Please see the citation

of the experimental environments in Table 1 in the paper.

(b) Did you mention the license of the assets? [N/A]
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]

Some video demo links are included in Appendix F in the supplementary ﬁle.

(d) Did you discuss whether and how consent was obtained from people whose data you’re

using/curating? [N/A]

16

(e) Did you discuss whether the data you are using/curating contains personally identiﬁable

information or offensive content? [N/A]

5. If you used crowdsourcing or conducted research with human subjects...

(a) Did you include the full text of instructions given to participants and screenshots, if

applicable? [N/A]

(b) Did you describe any potential participant risks, with links to Institutional Review

Board (IRB) approvals, if applicable? [N/A]

(c) Did you include the estimated hourly wage paid to participants and the total amount

spent on participant compensation? [N/A]

17

Appendix to the Safe Pontryagin Differentiable Programming paper

A Second-order Sufﬁcient Condition

Before presenting the second-order condition for the optimal control Problem B(θ), we present the
second-order condition for a general constrained nonlinear programming. The interested reader can
ﬁnd the details in Theorem 4 in [48].
Lemma A.1 (Second-order sufﬁcient condition [48]). If all functions deﬁning a constrained opti-
mization

min
x

f (x)

subject to gi(x) ≤ 0

i = 1, 2, · · · , m,
hj(x) = 0 j = 1, 2, · · · , p,

(S.1)

are twice-continuous differentiable, the second-order sufﬁcient condition for x∗ to be a local isolated
minimizing solution to (S.1) is that there exist vectors v∗ and w∗ such that (x∗, v∗, w∗) satisﬁes

gi(x∗) ≤ 0,
hi(x∗) = 0,
vigi(x∗) = 0,
ui ≥ 0,
∇L(x∗, v∗, w∗) = 0,

i = 1, 2, · · · , m,
j = 1, 2, · · · , p,
i = 1, 2, · · · , m,
i = 1, 2, · · · , m,

with

L(x, v, w) = f (x) +

m
(cid:88)

vigi(x) +

p
(cid:88)

wihi(x),

(S.2)

(S.3)

i=1
and ∇L being the derivative of L with respect to x; and further for any nonzero y (cid:54)= 0 satisfying
y(cid:48)∇gi(x∗) = 0 for all i with v∗
i ≥ 0, and y(cid:48)∇hj(x∗) = 0 for
i > 0, y(cid:48)∇gi(x∗) ≤ 0 for all i with v∗
all j = 1, 2, · · · , p, it follows that

i=1

y(cid:48)∇2L(x∗, v∗, w∗)y > 0.

(S.4)

The above second-order sufﬁcient condition for nonlinear programming is well-known. The proof
for Lemma A.1 can be found in Theorem 4 in [48]. Similarly, we can establish the second-order
sufﬁcient condition for a general constrained optimal control system Σ(θ) in (1), as below.

Lemma A.2 (Second-order sufﬁcient condition for Σ(θ) to have a local isolated minimizing trajectory
ξθ [73]). Given θ, if all functions deﬁning the constrained optimal control system Σ(θ) are twice
continuously differentiable in a neighborhood (tube) of ξθ = {xθ
0:T −1}, ξθ is a local isolated
minimizing trajectory to Problem B(θ) if there exist sequences λθ
0:T , and wθ
0:T such that the
following Constrained Pontryagin Maximum/Minimum Principle (C-PMP) conditions hold,

0:T , uθ
1:T , vθ

T = Lx

T (xθ

t , vθ

t , wθ

t , θ),

t , θ) and xθ
t , uθ
0 = x0(θ),
t , λθ
t , θ) and λθ
t , uθ
t+1, vθ
t , wθ
t , λθ
t , uθ
t , wθ
t+1, vθ
t , θ),
j = 1, 2, · · · , st,
t , θ) = 0,

xθ
t+1 = f (xθ
λθ
t (xθ
t = Lx
t (xθ
0 = Lu
ht,j(xθ
t , uθ
hT,j(xθ
T , θ) = 0,
t , uθ
gt,i(xθ
gT,i(xθ
T , θ) ≤ 0,

t , θ) ≤ 0,

j = 1, 2, · · · , sT ,
t , uθ
vt,igt,i(xθ
T , θ) = 0,

vT,igT,i(xθ

t , θ) = 0,

vt,i ≥ 0,

i = 1, 2, · · · , qt,

vT,i ≥ 0,

i = 1, 2, · · · , qT ,

and further if

T −1
(cid:88)

t=0

(cid:20)xt
ut

(cid:21)(cid:48) (cid:20)Lxx
t
Lux
t

(cid:21)

Lxu
t
Luu
t

(cid:21) (cid:20)xt
ut

18

+ x(cid:48)

T Lxx

T xT > 0

(S.5)

(S.6)

for any non-zero trajectory {x0:T , u0:T −1} (cid:54)= 0 satisfying

x0 = 0,

xt+1 = F x
t xt + H u
H x
t xt + ˇGu
ˇGx
t xt + ¯Gu
¯Gx

t xt + F u
t ut
t ut = 0 and H x
ˇGx
t ut = 0 and
¯Gx
t ut ≤ 0 and

T xT = 0,
T xT = 0,
T xT ≤ 0.
t is the ﬁrst-order derivative of the Hamiltonian Lt in (2) with respect to
is the second-derivative of Lt with respect to x, and similar notation convention applies
t = (Lux
t is the ﬁrst-order derivative of ht with respect to x and the
t and ¯Gu
t and F u
t

t )(cid:48), and Luu
t
T , H u

T for ˇgT , ¯Gx

t for ˇgt, ˇGx

t for f , ˇGx

; H x
t , F x

t and ˇGu

(S.7)

Here, t = 0, 1, ..., T − 1; Lx
x, and Lxx
t
t , Lxu
T , Lu
to Lx
similar convention applies to H x
for ¯gt, ¯Gx
T for ¯gT , where

ˇgt(xt, ut, θ) = col{gt,i(xt, ut, θ) | vθ
ˇgT (xT , θ) = col{gT,i(xT , θ) | vθ

¯gt(xt, ut, θ) = col{gt,i(xt, ut, θ) | gt,i(xθ
¯gT (xT , θ) = col{gT,i(xT , θ) | gT,i(xθ

t,i > 0, i = 1, ..., qT },
t , uθ

t , θ) = 0, i = 1, ..., qt} ∈ R¯qt,

T , θ) = 0, i = 1, ..., qT } ∈ R¯qT ,

t,i > 0, i = 1, ..., qt},

(S.8)

i.e., ¯gt and ¯gT are the vector functions formed by stacking all active inequality constraints at ξθ. All
the above ﬁrst- and second-order derivatives are evaluated at (ξθ, λθ

1:T , vθ

0:T , wθ

0:T ).

The above second-order sufﬁcient condition for the constrained optimal control system Σ(θ) is well-
known and has been well-established since [73]. The conditions in (S.5) is referred to as discrete-time
Constrained Pontryagin Maximum/Minimum Principle (C-PMP) [73]. Note that in the case of strict
complementarity, one has ˇgt(xt, ut, θ) = ¯gt(xt, ut, θ) and ˇgT (xT , θ) = ¯gT (xT , θ) in (S.8).

B Proof of Theorem 1

To prove Theorem 1, in the ﬁrst part, we need to derive the Differential Constrained Pontryagin
(cid:9) must satisfy. Then,
Maximum/Minimum Principle (Differential C-PMP), which ∂ξθ
in the second part, we formally present the proof for Theorem 1.

∂θ = (cid:8)X θ

0:T , U θ
0:T

B.1 Differential Constrained Pontryagin Maximum/Minimum Principle

From Lemma 1, for the constrained optimal control system Σ(θ) with any θ in a neighborhood of ¯θ,
(ξθ, λθ
0:T ) satisﬁes the C-PMP conditions in (S.5). Since (ξθ, λθ
0:T ) is also
once-continuously differentiable with respect to θ from Lemma 1, one can differentiate the C-PMP
conditions in (S.5) on both sides with respect to θ, as below.

0:T , wθ

0:T , wθ

1:T , vθ

1:T , vθ

Differentiating the ﬁrst ﬁve lines in (S.5) is straightforward, yielding

t+1

= F x
t

+ F u
t

+ F θ
t

and X θ

0 =

∂xθ
∂θ
∂λθ
t
∂θ
∂λθ
T
∂θ

∂xθ
t
∂θ
∂xθ
t
∂θ
∂xθ
T
∂θ

∂uθ
t
∂θ
∂uθ
t
∂θ
T )(cid:48) ∂vθ
T
∂θ

+ Lxu
t

+ (Gx

= Lxx
t

= Lxx
T

∂xθ
t
∂θ

+ Luu
t

∂uθ
t
∂θ

+ (F x

t )(cid:48) ∂λθ

t+1

+ (Gx

∂θ
T )(cid:48) ∂wθ
T
∂θ

+ (H x
t )(cid:48) ∂λθ

∂θ

+ (F u

t+1

+ (Gu

+ Lxθ
T ,
t )(cid:48) ∂vθ
t
∂θ

+ H u
t

∂uθ
t
∂θ

+ H θ

t = 0 and H x
T

∂xθ
t
∂θ

+ H θ

T = 0.

0 = Lux

t

H x
t

∂xθ
t
∂θ

∂xθ
0
=
∂θ
t )(cid:48) ∂vθ
t
∂θ

∂x0(θ)
∂θ

+ (H x

,
t )(cid:48) ∂wθ
t
∂θ

+ Lxθ
t

and

(S.9)

+ (H u

t )(cid:48) ∂wθ
t
∂θ

+ Luθ
t

,

We now consider to differentiate the two last equations (i.e., complementarity conditions) in the last
two lines in (S.5). We start with

t,i gt,i(xθ
vθ

t , uθ

t , θ) = 0,

i = 1, 2, · · · , qt.

(S.10)

19

Differentiating the above (S.10) on both sides with respect to θ yields

∂vθ
t,i
∂θ

gt,i(xθ

t , uθ

t , θ) + µθ
t,i

∂gt,i(xθ
t , uθ
∂θ

t , θ)

= 0,

i = 1, 2, · · · , qt.

(S.11)

In the above, we consider two following cases. If gt,i(xθ
constraint, then, µθ
(S.11), one thus has

t , uθ
t , θ) = 0, i.e., gt,i is an active inequality
t,i > 0 according to strict complementarity (condition (iii) in Lemma 1). From

∂gt,i(xθ
t , uθ
∂θ

t , θ)

= 0.

t , uθ

If gt,i(xθ

t , θ) < 0, i.e., gt,i is an inactive constraint, then vθ
∂vθ
t,i
∂θ
Stacking (S.12) for all active inequality constraints deﬁned in (S.8) will lead to

vθ
t,i = 0.

= 0 for

t,i = 0 and one has

(S.12)

(S.13)

Similarly, we can show that differentiating vT,igT,i(xθ

0 = ¯Gx
t

∂xθ
t
∂θ

+ ¯Gu
t

+ ¯Gθ
t .

∂uθ
t
∂θ
T , θ) = 0, i = 1, 2, · · · , qT , will lead to

(S.14)

¯Gx
T

∂xθ
T
∂θ

+ ¯Gθ

T = 0.

If we further deﬁne

t = col{vθ
¯vθ

t,i | vθ

t,i > 0, i = 1, ..., qt} ∈ R¯qt,

(S.15)

(S.16)

then, due to (S.13), the following terms in the second, third, and fourth lines in (S.9) can be written in
an equivalent way:

(Gx

t )(cid:48) ∂vθ
t
∂θ

= (cid:0) ¯Gx

t

(cid:1)(cid:48) ∂ ¯vθ
t
∂θ

and

(Gu

t )(cid:48) ∂vθ
t
∂θ

= (cid:0) ¯Gu

t

(cid:1)(cid:48) ∂ ¯vθ
t
∂θ

.

(S.17)

In sum, combining (S.9), (S.14), (S.15), and (S.17), one can ﬁnally write the Differential C-PMP:

t+1

= F x
t

+ F u
t

+ F θ
t

and X θ

0 =

∂xθ
∂θ
∂λθ
t
∂θ
∂λθ
T
∂θ

∂xθ
t
∂θ
∂xθ
t
∂θ
∂xθ
T
∂θ

∂uθ
t
∂θ
∂uθ
t
∂θ
T )(cid:48) ∂ ¯vθ
T
∂θ

+ Lxu
t

+ ( ¯Gx

= Lxx
t

= Lxx
T

0 = Lux

t

∂xθ
t
∂θ

+ Luu
t

∂uθ
t
∂θ

+ (F x

t )(cid:48) ∂λθ

t+1

+ ( ¯Gx

∂θ
T )(cid:48) ∂wθ
T
∂θ

+ (H x
t )(cid:48) ∂λθ

∂θ

+ (F u

t+1

+ ( ¯Gu

+ Lxθ
T ,
t )(cid:48) ∂ ¯vθ
t
∂θ

∂xθ
0
,
∂θ
t )(cid:48) ∂ ¯vθ
t
∂θ

+ (H x

t )(cid:48) ∂wθ
t
∂θ

+ Lxθ
t

and

+ (H u

t )(cid:48) ∂wθ
t
∂θ

+ Luθ
t

,

(S.18)

H x
t

¯Gx
t

∂xθ
t
∂θ
∂xθ
t
∂θ

∂uθ
t
∂θ
∂uθ
t
∂θ

+ H u
t

+ H θ

t = 0 and H x
T

+ ¯Gu
t

+ ¯Gθ

t = 0 and

¯Gx
T

∂xθ
t
∂θ
∂xθ
T
∂θ

+ H θ

T = 0,

+ ¯Gθ

T = 0.

With the above Differential C-PMP, we next prove the claims in Theorem 1.

B.2 Proof of Theorem 1

We prove Theorem 1 by two steps. We ﬁrst prove that the trajectory in (4), rewritten below,

(cid:8)X θ

0:T , U θ

0:T −1

(cid:9) with X θ

t =

∂xθ
t
∂θ

and U θ

t =

∂uθ
t
∂θ

,

is the local isolated minimizing trajectory to the auxiliary control system Σ(ξθ) in (3); and second,
we prove that such a local minimizing trajectory is also a global minimizing trajectory.

20

0:T , U θ

First, we prove that (cid:8)X θ
To show that (cid:8)X θ
(cid:9) is a local isolated minimizing trajectory to Σ(ξθ), we only need to
check whether it satisﬁes the second-order sufﬁcient condition for the constrained optimal control
system Σ(ξθ), as stated in Lemma A.2. To that end, we deﬁne the following Hamiltonian for Σ(ξθ):

(cid:9) is a local isolated minimizing trajectory to Σ(ξθ).

0:T , U θ

0:T −1

0:T −1

(cid:32)

¯Lt = Tr

+ Tr

(cid:20)Xt
1
2
Ut
(cid:16) ¯V (cid:48)

(cid:21)(cid:48) (cid:20)Lxx
t
Lux
t

(cid:21)

Lxu
t
Luu
t

(cid:21) (cid:20)Xt
Ut

+

(cid:20)Lxθ
t
Lue
t

(cid:21)(cid:48) (cid:20)Xt
Ut

(cid:21) (cid:33)

+ Tr

(cid:16)
Λ(cid:48)

t+1(F x

t Xt + F u

(cid:17)
t Ut + F θ
t )

t ( ¯Gx

t Xt + ¯Gu

t Ut + ¯Gθ
t )

(cid:17)

(cid:16)

+ Tr

W (cid:48)

t (H x

t Xt + H u

t Ut + H θ
t )

(cid:17)

,

t = 0, .., T −1,

¯LT = Tr

X (cid:48)

T Lxx

T XT + (Lxθ

+ Tr

N (cid:48)

T (H x

T XT + H θ
T )

,

T )(cid:48)XT
(cid:17)

(cid:16) ¯M (cid:48)

T ( ¯Gx

T XT + ¯Gθ
T )

(cid:17)

(cid:19)

+ Tr

t = T.

(cid:18) 1
2
(cid:16)

(S.19)
Here, Λt ∈ Rn×r, t = 1, 2, ..., T , denotes the costate (matrix) variables for Σ(ξθ); ¯Vt ∈ R¯qt×r and
Wt ∈ Rst×r, t = 0, 1, ..., T , are the multipliers for the constraints in Σ(ξθ). Further deﬁne

Λθ

t =

∂λθ
t
∂θ

, W θ

t =

∂wθ
t
∂θ

,

¯V θ
t =

∂ ¯vθ
t
∂θ

,

(S.20)

t in (S.16). Then, the Differential C-PMP in (S.18) is exactly the Constrained Pontryagin

with ¯vθ
Minimal Principle (C-PMP) for the auxiliary control system Σ(ξθ) because

X θ

t+1 =

∂ ¯Lt
∂Λθ

t+1

= F x

t X θ

t + F u

t U θ

t + F θ
t

and X0 = X θ
0 ,

= Lxx

t X θ

t + Lxu

t U θ

t + Lxθ

t + (F x

t )(cid:48)Λθ

t+1 + ( ¯Gx

t )(cid:48) ¯V θ

t + (H x

t )(cid:48)W θ
t

Λθ

t =

Λθ

T =

∂ ¯Lt
∂X θ
t
∂ ¯Lt
∂X θ
T
∂ ¯Lt
∂U θ
t
t + H u

= H xx

T X θ

T + H xe

T + ( ¯Gx

T )(cid:48) ¯V θ

T + (H x

T )(cid:48)W θ
T ,

0 =

= Luu

t U θ

t + Lux

t X θ

t + Luθ

t + (F u

t )(cid:48)Λθ

t+1 + ( ¯Gu

t )(cid:48) ¯V θ

t + (H u

t )(cid:48)W θ
t ,

H x

t X θ

t U θ

t + H θ

t = 0 and H x

T X θ

T + H θ

T = 0,

and

(S.21)

¯Gx

t + ¯Gu

t X θ

T X θ
Note that in (S.21), we have used the following matrix calculus [86] and trace properties:

t = 0 and

T = 0.

t U θ

T + ¯Gθ

t + ¯Gθ

¯Gx

∂ Tr(AB)
∂A

= B(cid:48),

∂f (A)
∂A(cid:48) =

(cid:20) ∂f (A)
∂A

(cid:21)(cid:48)

,

∂ Tr(X (cid:48)HX)
∂X

= HX + H (cid:48)X,

Tr(A) = Tr(A(cid:48)), Tr(ABC) = Tr(BCA) = Tr(CAB), Tr(A + B) = Tr(A) + Tr(B).

Next, we need to show that the second-order condition



(cid:34)∆Xt
∆Ut















(cid:35)(cid:48) 


(cid:124)

T −1
(cid:88)

t=0

Tr

∂X θ

∂U θ

∂ ¯L2
t
t ∂X θ
t
∂ ¯L2
t
t ∂X θ
t
Lxx
t

Lux
t



∂ ¯L2
t
t ∂U θ
t
∂ ¯L2
t
t ∂U θ
t

∂X θ

∂U θ





Lxu
t
Luu
t

(cid:123)(cid:122)

















(cid:35)

(cid:34)∆Xt
∆Ut





(cid:125)

21







∆X (cid:48)
T

+ Tr







> 0,

(cid:104)

(cid:124)

∂X θ

∂ ¯L2
t
T ∂X θ
T
(cid:123)(cid:122)
Lxx
T

(cid:105)

(cid:125)

∆XT

(S.22)

hold for any trajectory {∆X0:T , ∆U0:T −1} (cid:54)= 0 satisfying

∆Xt+1 = F x
¯Gx
t ∆Xt + ¯Gu
t ∆Xt + H u
H x

t ∆X t + F u
and ∆X0 = 0,
t ∆U t
¯Gx
T ∆XT = 0,
t ∆Ut = 0 and
t ∆Ut = 0 and H x
T ∆XT = 0,

(S.23)

In fact, this is true directly due to (S.6) and (S.7) in Lemma A.2 and the strict complementarity in
condition (iii) in Lemma 1 (note that ˇgt(xt, ut, θ) = ¯gt(xt, ut, θ) and ˇgT (xT , θ) = ¯gT (xT , θ)
because of the strict complementarity). Therefore, with the C-PMP (S.21) and (S.22)-(S.23) holding
for (cid:8)X θ
(cid:9) is a local unique minimizing trajectory
(cid:9), we can conclude that (cid:8)X θ
0:T , U θ
to the auxiliary control system ¯Σ(ξθ) according to Lemma A.2.

0:T , U θ

0:T −1

0:T −1

Second, we prove that the local unique minimizing trajectory (cid:8)X θ

0:T , U θ

0:T −1

(cid:9) is also a global one.

We note that any feasible trajectory {X0:T , U0:T −1} that satisﬁes all constraints (dynamics, path and
ﬁnal constraints) in the auxiliary control system Σ(ξθ) can be written as

{X0:T , U0:T −1} = (cid:8)X θ

0:T , U θ

0:T −1

(cid:9) + {∆X0:T , ∆U0:T −1} ,

(S.24)

with {∆X0:T , ∆U0:T −1} satisfying the conditions in (S.23). Let

¯J(X0:T , U0:T −1) − ¯J(X θ
(cid:21)(cid:48) (cid:20)Lxx
t
Lux
t

(cid:20)∆Xt
∆Ut

T −1
(cid:88)

(cid:32)

1
2

= Tr

0:T , U θ

0:T −1)
(cid:21) (cid:20)∆Xt
∆Ut

Lxu
t
Luu
t

t=0

(cid:21)

+

(cid:20)X θ
t
U θ
t

(cid:21)(cid:48) (cid:20)Lxx
t
Lux
t

Lxu
t
Luu
t

(cid:21) (cid:20)∆Xt
∆Ut

(cid:21)

+

(cid:20)Lxθ
t
Luθ
t

(cid:21)(cid:48) (cid:20)∆Xt
∆Ut

(cid:21)(cid:33)

+ Tr

(cid:18) 1
2

∆X (cid:48)

T Lxx

T ∆XT + (X θ

T )(cid:48)Lxx

T ∆XT + (Lxθ

T )(cid:48)∆XT

(cid:19)

.

Based on (S.21), the following term in (S.25) can be simpliﬁed to

(cid:21)(cid:48) (cid:18)(cid:20)Lxx
t
Lux
t
(cid:21)(cid:48) (cid:20)−(F x

(cid:20)∆Xt
∆Ut
(cid:20)∆Xt
∆Ut

=

t+1)(cid:48)F x
t+1)(cid:48)F u
t+1)(cid:48)F x

= − (Λθ

− (Λθ
= −(Λθ
(cid:124)

(cid:21)

+

(cid:21)(cid:19)

t )(cid:48)Λθ

t )(cid:48)W θ

(cid:21) (cid:20)X θ
(cid:20)Lxθ
Lxu
t
t
t
Luu
U θ
Luθ
t
t
t
t )(cid:48) ¯V θ
t+1 − ( ¯Gx
t − (H x
t )(cid:48)Λθ
t )(cid:48) ¯V θ
t+1 − ( ¯Gu
t − (H u
−(F u
t )(cid:48)W θ
t
t ∆Xt − (cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)
t ∆Xt − (cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)
t )(cid:48) ¯Gx
( ¯V θ
t ∆Xt + (Λθ
(W θ
t )(cid:48)H x
t ∆Ut − (cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)
t ∆Ut − (cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)
t )(cid:48) ¯Gu
( ¯V θ
t )(cid:48)H u
(W θ
t ∆Ut
t ∆Xt − (Λθ
t+1)(cid:48)F u
t )(cid:48)∆Xt = −(Λθ
+(Λθ
t ∆Ut
(cid:125)
(cid:123)(cid:122)

t + Λθ
t

(cid:21)

−(Λθ

t+1)(cid:48)∆Xt+1

t )(cid:48)∆Xt

t+1)(cid:48)∆Xt+1 + (Λθ

t )(cid:48)∆Xt

(S.25)

(S.26)

where the cancellations in the last three lines are due to (S.23). Also based on (S.21), the following
term in (S.25) can be simpliﬁed to

T )(cid:48)(cid:1) ∆XT
(cid:0)(X θ
T )(cid:48)Lxx
T + (Lxθ
T )(cid:48) ¯GT ∆XT − (cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)
= − (cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)
( ¯V θ
(W θ
T )(cid:48)∆XT
=(Λθ

T )(cid:48)H x

T ∆XT + (Λθ

T )(cid:48)∆XT

(S.27)

where the cancellation here is due to (S.23).

22

(cid:18) 1
2
(cid:32)

(cid:18) 1
2
(cid:32)

Then, based on (S.26) and (S.27), (S.25) is simpliﬁed to

¯J(X0:T , U0:T −1) − ¯J(X θ
(cid:21)(cid:48) (cid:20)Lxx
t
Lux
t

(cid:20)∆Xt
∆Ut

T −1
(cid:88)

(cid:32)

1
2

= Tr

0:T , U θ

0:T −1)
(cid:21) (cid:20)∆Xt
∆Ut

Lxu
t
Luu
t

t=0

(cid:21)

+

(cid:20)X θ
t
U θ
t

(cid:21)(cid:48) (cid:20)Lxx
t
Lux
t

Lxu
t
Luu
t

(cid:21) (cid:20)∆Xt
∆Ut

(cid:21)

+

(cid:20)Lxθ
t
Luθ
t

(cid:21)(cid:48) (cid:20)∆Xt
∆Ut

(cid:21)(cid:33)

+ Tr

∆X (cid:48)

T Lxx

T ∆XT + (X θ

T )(cid:48)Lxx

T ∆XT + (Lxθ

T )(cid:48)∆XT

(cid:19)

(cid:33)

− (Λθ

t+1)(cid:48)∆Xt+1 + (Λθ

t )(cid:48)∆Xt

= Tr

T −1
(cid:88)

t=0

(cid:20)∆Xt
∆Ut

(cid:21)(cid:48) (cid:20)Lxx
t
Lux
t

1
2

Lxu
t
Luu
t

(cid:21)

(cid:21) (cid:20)∆Xt
∆Ut
(cid:19)

+ Tr

∆X (cid:48)

T Lxx

T ∆XT + (Λθ

T )(cid:48)∆XT

= Tr

T −1
(cid:88)

t=0

(cid:20)∆Xt
∆Ut

(cid:21)(cid:48) (cid:20)Lxx
t
Lux
t

1
2

Lxu
t
Luu
t

(cid:21) (cid:20)∆Xt
∆Ut

(cid:21)(cid:33)

+ Tr

(cid:18) 1
2

∆X (cid:48)

T Lxx

T ∆XT

(cid:19)

,

(S.28)

where the last line is because (note ∆X0 = 0 in (S.23))

(cid:0)−(Λθ

t+1)(cid:48)∆Xt+1 + (Λθ

t )(cid:48)∆Xt

(cid:1) + Tr (cid:0)(Λθ

T )(cid:48)∆XT

(cid:1) = Tr (cid:0)(Λθ

0 )(cid:48)∆X0

(cid:1) = 0.

Tr

T −1
(cid:88)

t=0

Since

(cid:32)

T −1
(cid:88)

Tr

t=0

(cid:20)∆Xt
∆Ut

(cid:21)(cid:48) (cid:20)Lxx
t
Lux
t

1
2

Lxu
t
Luu
t

(cid:21) (cid:20)∆Xt
∆Ut

(cid:21)(cid:33)

+ Tr

(cid:18) 1
2

∆X (cid:48)

T Lxx

T ∆XT

due to (S.22) for all {∆X0:T , ∆U0:T −1} satisfying (S.23), therefore

¯J(X0:T , U0:T −1) − ¯J(X θ

0:T , U θ

0:T −1) ≥ 0.

(cid:19)

≥ 0

(S.29)

(S.30)

for any feasible trajectory {X0:T , U0:T −1} in (S.24). This concludes that the local unique minimizing
trajectory (cid:8)X θ
In sum of the two proof steps, the assertion that the trajectory in (4), i.e.,

(cid:9) is also a global one.

0:T , U θ
0:T

∂ξθ
∂θ

= (cid:8)X θ

0:T , U θ

0:T −1

(cid:9) ,

is a globally unique minimizing trajectory to the auxiliary control system Σ(ξθ) in (3) follows. This
completes the proof of Theorem 1.

C Proof of Theorem 2

For the unconstrained optimal control system Σ(θ, γ) in (5), we deﬁne its Hamiltonian below:

ˆLt = ct(xt, ut, θ) + λ(cid:48)

t+1f (xt, ut, θ) − γ

qt
(cid:88)

i=1

ln (cid:0)−gt,i(xt, ut, θ)(cid:1)+

1
2γ

st(cid:88)

i=1

(cid:0)ht,i(xt, ut, θ)(cid:1)2

ˆLT = cT (xT , θ) − γ

qT(cid:88)

i=1

ln (cid:0)−gT,i(xT , θ)(cid:1)+

1
2γ

sT(cid:88)

i=1

(cid:0)hT,i(xT , θ)(cid:1)2

.

with t = 0, 1, · · · , T − 1.

(S.31)

23

C.1 Proof of Claim (a)

We ﬁrst modify the C-PMP condition (S.5) for the constrained optimal control system Σ(θ) into the
following set of equations:

and x0 = x0(θ),

xt+1 = f (xt, ut, θ)
λt = Lx
0 = Lu

t (xt, ut, λt+1, vt, wt, θ)
t (xt, ut, λt+1, vt, wt, θ),

and λT = Lx

T (xt, vt, wt, θ),

ht,i(xt, ut, θ) = wt,iγ,
hT,i(xT , θ) = wT,iγ,

i = 1, 2, · · · , st,
i = 1, 2, · · · , sT ,

vt,i gt,i(xt, ut, θ) = −γ,

i = 1, 2, · · · , qt,

vT,i gT,i(xT , θ) = −γ,

i = 1, 2, · · · , qT ,

(S.32)

where the ﬁrst three equations are the same with the those in (S.5) and only the last two lines of
equations are modiﬁed by adding some perturbation terms related to γ.

Now, one can view that the parameters (θ, γ) jointly determine ξ = {x0:T , u0:T −1}, λ1:T , v0:T , and
w0:T through the implicit equations in (S.32). Also, one can note that by letting γ = 0 and θ = ¯θ,
the above equations in (S.32) coincide with the C-PMP condition (S.5) for Σ( ¯θ). Thus, given that the
conditions (i)-(iii) in Lemma 1 hold for Σ( ¯θ), one can readily apply the implicit function theorem
[75] to (S.32) in a neighborhood of ( ¯θ, 0) and make the following assertion (its proof can directly
follow the proof for Lemma 1 (i.e., the ﬁrst-order sensitivity result) with little change):
For any (θ, γ) within a neighborhood of ( ¯θ, 0), there exists a unique once-continuously differentiable
function

, which satisﬁes (S.32) and

ξ(θ,γ), λ(θ,γ)

0:T , v(θ,γ)

0:T , w(θ,γ)

0:T

(cid:17)

(cid:16)

(cid:16)

ξ(θ,γ), λ(θ,γ)

0:T , v(θ,γ)

0:T , w(θ,γ)

0:T

(cid:1) = (cid:0)ξ ¯θ, λ

¯θ
1:T , v ¯θ

0:T , w ¯θ

0:T

(cid:17)

when

(θ, γ) = ( ¯θ, 0).

(S.33)

With the above claim, in what follows, we will prove that for any (θ, γ) near ( ¯θ, 0) additionally with
γ > 0, ξ(θ,γ) is a local isolated minimizing trajectory to the unconstrained optimal control system
Σ(θ, γ) in (5). First, we need to show that such ξ(θ,γ) will make Σ(θ, γ) well-deﬁned, which is the
second part of Claim (a), rewritten below

(cid:16)

x(θ,γ)
t
(cid:16)

x(θ,γ)
T

gt,i

gT,i

, u(θ,γ)
t
(cid:17)

, θ

< 0,

(cid:17)

, θ

< 0,

i = 1, 2, · · · , qt,

and

i = 1, 2, · · · , qT .

(S.34)

In fact, such an assertion always holds because the strict complementary for Σ( ¯θ) from Lemma 1.
t , u ¯θ
Speciﬁcally, for any i = 1, 2, · · · , qt, if gt,i(x ¯θ

t , ¯θ) < 0, from continuity of gt,i and ξ(θ,γ)

gt,i(x(θ,γ)
t

, u(θ,γ)
t

, θ) → gt,i(x ¯θ

t , u ¯θ

t , ¯θ) < 0 as

(θ, γ) → ( ¯θ, 0),

thus gt,i(x(θ,γ)
v ¯θ
t,i > 0 (due to strict complementarity), from continuity of v(θ,γ)

, θ) < 0 for any (θ, γ) near ( ¯θ, 0) with γ > 0; if gt,i(x ¯θ

, u(θ,γ)
t

,

t

t

t , u ¯θ

t , ¯θ) = 0 and

t,i → v ¯θ
v(θ,γ)

t,i > 0 as

(θ, γ) → ( ¯θ, 0),

(S.35)

thus v(θ,γ)
− γ

t,i > 0 for (θ, γ) near ( ¯θ, 0) with γ > 0, and also due to (S.32), gt,i(x(θ,γ)

, θ) =
< 0 for (θ, γ) near ( ¯θ, 0) with γ > 0. So, for either case, the ﬁrst inequality in (S.34) always

, u(θ,γ)
t

t

v(θ,γ)
t,i

holds. Similar proof procedure also applies to prove the second inequality in (S.34). In sum, we
conclude that ξ(θ,γ) satisﬁes (S.34) and thus makes the Σ(θ, γ) well-deﬁned for any (θ, γ) near
( ¯θ, 0) with γ > 0. This completes the second part of Claim (a).

24

From now on, we prove that for any (θ, γ) near ( ¯θ, 0) with γ > 0, ξ(θ,γ) is a local isolated minimizing
trajectory to the unconstrained optimal control system Σ(θ, γ) in (5). From the last four equations in
(S.32), we solve

w(θ,γ)

t,i =

ht,i(x(θ,γ)
, u(θ,γ)
t
t
γ

, θ)

,

w(θ,γ)

T,i =

hT,i(x(θ,γ)

T
γ

, θ)

,

v(θ,γ)
t,i = −

γ
, u(θ,γ)
gt,i(x(θ,γ)
t
t

,

, θ)

v(θ,γ)
T,i = −

γ

gT,i(x(θ,γ)
T

, θ)

,

(S.36)

and plug them into the ﬁrst three equations in (S.32), then one will ﬁnd that the obtained equations are
exactly the Pontryagin Maximum/Minimum Principle (PMP) for the unconstrained optimal control
system Σ(θ, γ) with its Hamiltonian already deﬁned in (S.31), that is to say,

and x(θ,γ)

0

= x0(θ),

, λ(θ,γ)

t+1 , (θ, γ)),

(S.37)

, θ)

, u(θ,γ)
t
, u(θ,γ)
t

t

t

x(θ,γ)
t+1 = f (x(θ,γ)
t (x(θ,γ)
λ(θ,γ)
= ˆLx
t
T (x(θ,γ)
λ(θ,γ)
T
t (x(θ,γ)
0 = ˆLu
t
(cid:110)
0:T , u(θ,γ)
x(θ,γ)

= ˆLx

0:T −1

, (θ, γ)),

t
, λ(θ,γ)
, u(θ,γ)
t
(cid:111)

t+1 , (θ, γ)),

indicating that ξ(θ,γ) =

already satisﬁes the PMP condition for unconstrained

(cid:110)

(cid:111)

0:T , u(θ,γ)
x(θ,γ)

optimal control system Σ(θ, γ). To show ξ(θ,γ) =
is a local isolated minimizing
trajectory to Σ(θ, γ) for any (θ, γ) near ( ¯θ, 0) with γ > 0, we only need to verify its second-order
condition as stated in (S.6)-(S.7) in Lemma A.2, which is presented next. In the remainder of proof,
for convenience of notation, all derivatives are evaluated at (θ, γ) (or ξ(θ,γ)) unless otherwise stated.
Before proceeding, we show two facts (easy to prove) about the second-order derivatives of Hamilto-
nian ˆLt and ˆLT in (S.31). First,
(cid:21)

0:T −1

(cid:21)

(cid:20) ˆLxx
t
ˆLux
t

ˆLxu
t
ˆLuu
t


qt(cid:80)

=

(cid:20)Lxx
t
Lux
t

Lxu
t
Luu
t

γ
g2

t,i

γ
g2

t,i

∂g(cid:48)
t,i
∂xt

∂g(cid:48)
t,i
∂ut

∂gt,i
∂xt

∂gt,i
∂xt





i=1

qt(cid:80)

i=1

and

+

+

+

st(cid:80)
i=1
st(cid:80)
i=1

∂h(cid:48)
t,i
∂xt

∂h(cid:48)
t,i
∂ut

1
γ

1
γ

∂ht,i
∂xt

∂ht,i
∂xt

qt(cid:80)

i=1

qt(cid:80)

i=1

γ
g2

t,i

γ
g2

t,i

∂g(cid:48)
t,i
∂xt

∂g(cid:48)
t,i
∂ut

∂gt,i
∂ut

∂gt,i
∂ut

+

+

st(cid:80)
i=1
st(cid:80)
i=1

∂h(cid:48)
t,i
∂xt

∂h(cid:48)
t,i
∂ut

1
γ

1
γ

∂ht,i
∂ut

∂ht,i
∂ut







,

(S.38)

,

(S.39)

ˆLxx

T = Lxx

T +

qT(cid:88)

γ
g2
t,i

∂g(cid:48)
T,i
∂xT

∂gT,i
∂xT

sT(cid:88)

+

1
γ

∂h(cid:48)
T,i
∂xT

∂hT,i
∂xT

(cid:21)

i=1

=

(cid:20)x
u

ˆLxu
t
ˆLuu
t

(cid:21) (cid:20)x
u

(cid:21)(cid:48) (cid:20) ˆLxx
t
ˆLux
t


i=1
respectively. Second, given any x and u with appropriate dimensions, one has
(cid:21)(cid:48) (cid:20)Lxx
(cid:20)x
t
Lux
u
t
st(cid:80)
i=1
st(cid:80)
i=1
qt(cid:88)

+ (cid:80)st
i=1
st(cid:80)
i=1

(cid:21) (cid:20)x
u
qt(cid:80)

γ
g2
qt(cid:80)

Lxu
t
Luu
t

∂h(cid:48)
t,i
∂xt

∂h(cid:48)
t,i
∂ut

(cid:20)x
u

∂g(cid:48)
t,i
∂ut

∂g(cid:48)
t,i
∂xt

∂g(cid:48)
t,i
∂xt

∂g(cid:48)
t,i
∂ut

∂ht,i
∂xt

∂ht,i
∂xt

∂gt,i
∂ut

∂gt,i
∂ut

∂gt,i
∂xt

∂gt,i
∂xt

st(cid:88)





qt(cid:80)

qt(cid:80)

γ
g2

γ
g2

γ
g2

i=1

i=1

i=1

(cid:17)2

+

+

+

+

(cid:21)(cid:48)

1
γ

1
γ

1
γ

(cid:21)

(cid:21)

t,i

t,i

t,i

t,i

i=1
(cid:21)(cid:48) (cid:20)Lxx
t
Lux
t

(cid:20)x
u

=

Lxu
t
Luu
t

(cid:21) (cid:20)x
u

+

i=1

γ
g2
t,i

(cid:16) ∂gt,i
∂xt

x +

u

+

∂gt,i
∂u

i=1

1
γ

(cid:16) ∂ht,i
∂xt

x +

∂ht,i
∂u

(cid:17)2

u

,

(S.40)

and

x(cid:48) ˆLxx

T x = x(cid:48)Lxx

T x + x(cid:48)(cid:16)

= x(cid:48)Lxx

T x +

qT(cid:88)

i=1

qT(cid:88)

i=1

γ
g2
T,i

γ
g2
t,i

∂g(cid:48)
T,i
∂xT

∂gT,i
∂xT

+

sT(cid:88)

i=1

1
γ

∂h(cid:48)
T,i
∂xT

∂hT,i
∂xT

(cid:17)

x

(cid:16) ∂gT,i
∂xT

(cid:17)2

x

+

sT(cid:88)

i=1

1
γ

(cid:16) ∂hT,i
∂xT

(cid:17)2

.

x

(S.41)

25

∂h(cid:48)
t,i
∂xt

∂ht,i
∂ut

1
γ

∂h(cid:48)
t,i
∂ut

∂ht,i
∂ut







(cid:21)

(cid:20)x
u

For the second-order condition of the unconstrained optimal control system Σ(θ, γ) with any (θ, γ)
near ( ¯θ, 0) with γ > 0, we need to prove that
(cid:20)xt
ut

T (θ, γ)xT > 0,

(cid:21) (cid:20)xt
ut

(cid:21)(cid:48) (cid:20) ˆLxx
ˆLux

ˆLxu
ˆLuu

+ x(cid:48)
T

(S.42)

T −1
(cid:88)

ˆLxx

t (θ, γ)
t (θ, γ)

t (θ, γ)
t (θ, γ)

(cid:21)

t=0

for any {x0:T , u0:T −1} (cid:54)= 0 satisfying

xt+1 = F x

t (θ, γ)xt + F u

t (θ, γ)ut

and x0 = 0.

(S.43)

t (θ, γ),
T (θ, γ) means that these ﬁrst- and second-order derivatives are evaluated at trajectory ξ(θ,γ)

Here, for convenience, the dependence in F x
and H xx
(the same notation convention applies below).

t (θ, γ), H uu

t (θ, γ), H xu

t (θ, γ), H xx

t (θ, γ), F u

Proof by contradiction: suppose that the above second-order condition in (S.42)-(S.43) is false.
Then, there must exist a sequence of parameters (θk, γk) with γk > 0 and a sequence of trajectories
{xk
t with
xk

0:T −1} (cid:54)= 0 such that (θk, γk) → ( ¯θ, 0), xk

t (θk, γk)uk

t (θk, γk)xk

t+1 = F x

t + F u

(cid:20)xk
t
uk
t

(cid:21)(cid:48) (cid:20) ˆLxx
t (θk, γk)
ˆLux
t (θk, γk)

ˆLxu
t (θk, γk)
ˆLuu
t (θk, γk)

(cid:21) (cid:20)xk
t
uk
t

(cid:21)

+ xk
T

(cid:48) ˆLxx

T (θk, γk)xk

T ≤ 0,

(S.44)

0:T , uk
0 = 0, and
T −1
(cid:88)

t=0

0:T , uk

for k = 1, 2, 3, · · · . Here, the dependence (θk, γk) means that these ﬁrst- and second-order deriva-
tives are evaluated at trajectory ξ(θk,γk) for notation convenience. Without loss of generality, assume
(cid:107)col{xk
0:T −1}, relabel the se-
quence {xk
0:T −1} →
0:T , u∗
{x∗
t with
0 = 0. Then, the limit {x∗
x∗

0:T , uk
0:T −1} for convenience, and call its limit {x∗
0:T −1} and (θk, γk) → ( ¯θ, 0) as k → +∞ and x∗

0:T , uk
0:T −1}, that is, {xk
t ( ¯θ, 0)x∗
t + F u

0:T −1}(cid:107) = 1 for all k. Select a convergent sub-sequence {xk

0:T −1} must fall into either of two cases discussed below.

0:T , u∗
t+1 = F x

0:T , uk
t ( ¯θ, 0)u∗

0:T , u∗

Case 1: (cid:107)col {x∗
¯Gx
t ( ¯θ, 0)x∗

0:T , u∗
t + ¯Gu
or

0:T −1}(cid:107) = 1 and at least one of the following holds:
t ( ¯θ, 0)u∗
¯Gx

t (cid:54)= 0 ∃t or H x
T (cid:54)= 0 or H x
0:T , u∗

0:T −1} → {x∗

t (cid:54)= 0 ∃t

t ( ¯θ, 0)u∗

t + H u
T (cid:54)= 0.

t ( ¯θ, 0)x∗
T ( ¯θ, 0)x∗
0:T −1}, (θk, γk) → ( ¯θ, 0), we will have

T ( ¯θ, 0)x∗

0:T , uk

(S.45)

In this case, as k → 0, {xk

T −1
(cid:88)

(cid:32) qt
(cid:88)

t=0

i=1

γk
(gk
t,i)2

(cid:16) ∂gk
t,i
∂xt

xk

t +

∂gk
t,i
∂uk
t
qT(cid:88)

i=1

(cid:17)2

uk
t

+

st(cid:88)

i=1
(cid:16) ∂gk
T,i
∂xT

γk
(gk
T,i)2

1
γk

(cid:16) ∂hk
t,i
∂xk
t

xk

t +

(cid:17)2

+

xk
T

sT(cid:88)

i=1

1
γk

uk
t

∂hk
t,i
∂uk
t
(cid:16) ∂hk
T,i
∂xT

+

(cid:17)2(cid:33)

(cid:17)2

xk
T

→ +∞,

(S.46)

∂gk
t,i
∂xt

∂gk
T ,i
∂xT

,

are with superscript k to denote their values are evaluated
where
at ξ(θk,γk) for notation convenience. (S.46) is because at least one of the terms in the summation is
+∞. Here, we have used the following facts from the last two equations in (S.32):

T,i,

,

, gk

t,i, gk

∂hk
t,i
∂xt

∂hk
T ,i
∂xT

γ
, u(θ,γ)
t

v(θ,γ)
t,i
, u(θ,γ)
gt,i(x(θ,γ)
t
t

, θ)(cid:1)2 = −

t

(cid:0)gt,i(x(θ,γ)
where → 0 corresponds to the inactive inequalities gt,i(x ¯θ
t , u ¯θ
the active inequalities gt,i(x ¯θ

, θ)

γ

(cid:0)gT,i(x(θ,γ)

T

, θ)(cid:1)2 = −

t , ¯θ) = 0 (vθ
v(θ,γ)
T,i
gT,i(x(θ,γ)
T

, θ)

→ 0

or → +∞ as

(θ, γ) → ( ¯θ, 0),

t , u ¯θ

t , ¯θ) < 0 and → +∞ corresponds to

t,i > 0 due to strict complementarity); and also

→ 0

or → +∞ as

(θ, γ) → ( ¯θ, 0).

where → 0 corresponds to the inactive inequalities gT,i(x ¯θ
active inequalities gT,i(x ¯θ

T , ¯θ) = 0 (vθ

T,i > 0 due to strict complementarity).

T , ¯θ) < 0 and → +∞ corresponds to the

26

By extending the left side of (S.44) based on the facts (S.40) and (S.41), (S.46) immediately leads to

lim
k→+∞

(cid:32) T −1
(cid:88)

t=0

(cid:20)xk
t
uk
t

(cid:21)(cid:48) (cid:20) ˆLxx
t (θk, γk)
ˆLux
t (θk, γk)

ˆLxu
t (θk, γk)
ˆLuu
t (θk, γk)

(cid:21) (cid:20)xk
t
uk
t

(cid:21)

+ xk
T

(cid:48) ˆLxx

T (θk, γk)xk
T

(cid:33)

→ +∞, (S.47)

which obviously contradicts (S.44).

Case 2: (cid:107)col {x∗

¯Gx

t ( ¯θ, 0)x∗

0:T , u∗
t + ¯Gu
and

0:T −1}(cid:107) = 1 and all of the following holds:
t ( ¯θ, 0)u∗
¯Gx

T = 0 and H x

T ( ¯θ, 0)x∗

T ( ¯θ, 0)x∗

t ( ¯θ, 0)x∗

t = 0 ∀t

and H x

t + H u
T = 0.

t ( ¯θ, 0)u∗

t = 0 ∀t

In this case, we have

lim
k→+∞

(cid:32) T −1
(cid:88)

t=0

≥ lim

k→+∞

(cid:20)xk
t
uk
t
(cid:32) T −1
(cid:88)

ˆLxu
t (θk, γk)
ˆLuu
t (θk, γk)

(cid:21)(cid:48) (cid:20) ˆLxx
t (θk, γk)
ˆLux
t (θk, γk)
(cid:21)(cid:48) (cid:20)Lxx
t (θk, γk)
t (θk, γk) Lxu
ˆLuu
t (θk, γk)
t (θk, γk)
Lux

(cid:21) (cid:20)xk
t
uk
t

(cid:20)xk
t
uk
t

(cid:21)

t=0

+ xk
T

(cid:48) ˆLxx

T (θk, γk)xk
T

(cid:33)

(cid:21)

(cid:21) (cid:20)xk
t
uk
t

(cid:48)

+ xk
T

T (θk, γk)xk
Lxx
T

(cid:33)

=

T −1
(cid:88)

t=0

(cid:20)x∗
t
u∗
t

(cid:21)(cid:48) (cid:20)Lxx
t ( ¯θ, 0)
t ( ¯θ, 0) Lxu
ˆLuu
t ( ¯θ, 0)
t ( ¯θ, 0)
Lux

(cid:21) (cid:20)x∗
t
u∗
t

(cid:21)

+ x∗
T

(cid:48)Lxx

T ( ¯θ, 0)x∗

T > 0.

(S.48)

(S.49)

Here, the ﬁrst inequality is based on the fact that the residual term is always non-negative, i.e.,

T −1
(cid:88)

(cid:32) qt
(cid:88)

t=0

i=1

γk
(gk
t,i)2

(cid:16) ∂gk
t,i
∂xt

xk

t +

∂gk
t,i
∂uk
t
qT(cid:88)

+

i=1

st(cid:88)

(cid:17)2

+

uk
t

1
γk

(cid:16) ∂hk
t,i
∂xk
t

xk

t +

i=1
(cid:16) ∂gk
T,i
∂xT

γk
(gk
T,i)2

(cid:17)2

+

xk
T

sT(cid:88)

i=1

(cid:17)2(cid:33)

uk
t

∂hk
t,i
∂uk
t
(cid:16) ∂hk
T,i
∂xT

1
γk

(cid:17)2

xk
T

≥ 0,

(S.50)

the last inequality is directly from the second-order condition in (S.6)-(S.7) in Lemma A.2. Obviously,
(S.49) also contracts (S.44).

Combining the above two cases, we can conclude that for any (θ, γ) near ( ¯θ, 0) with γ > 0, the
trajectory ξ(θ,γ) to the unconstrained optimal control system Σ(θ, γ) satisﬁes both its PMP condition
in (S.31) and the second-order condition in (S.42)-(S.43). Thus, one can assert that ξ(θ,γ) is a local
isolated minimizing trajectory to Σ(θ, γ). This completes the proof of Claim (a) in Theorem 2.

C.2 Proof of Claim (b)

Given that the conditions (i)-(iii) in Lemma 1 hold for Σ( ¯θ), we have the following conclusions:
(1) From Claim (a) and its proof, we know that for any (θ, γ) in the neighborhood of ( ¯θ, 0), there
, which
exists a unique once-continuously differentiable function
satisﬁes (S.32). Additionally provided γ > 0, such ξ(θ,γ) is also a local isolated minimizing
trajectory for the well-deﬁned unconstrained optimal control system Σ(θ, γ).

ξ(θ,γ), λ(θ,γ)

0:T , w(θ,γ)

0:T , v(θ,γ)

0:T

(cid:16)

(cid:17)

(2) Additionally let γ = 0 in (S.32), and (S.32) becomes the C-PMP condition for the constrained
optimal control system Σ(θ). From Lemma 1, for any θ near ¯θ, ξθ = ξ(θ,γ=0) is a differentiable local
isolated minimizing trajectory for Σ(θ), associated with the unique once-continuously differentiable
function

(cid:16)

(cid:17)

.

λ(θ,γ=0)
1:T

, v(θ,γ=0)
0:T

, w(θ,γ=0)
0:T

Therefore, due to the uniqueness and once-continuous differentiability of ξ(θ,γ) with respect to (θ, γ)
near ( ¯θ, 0), one can obtain

ξ(θ,γ) → ξ(θ,0) = ξθ

as γ → 0,

(S.51)

27

and

∂ξ(θ,γ)
∂θ

→

∂ξ(θ,0)
∂θ

=

ξθ
∂θ

as γ → 0.

(S.52)

Here (S.51) is due to that ξ(θ,γ) is unique and continuous at (θ, γ = 0), and (S.51) is because ξ(θ,γ)
is unique and once-continuously differentiable at (θ, γ = 0). This completes the proof of Claim (b)
in Theorem 2.

C.3 Proof of Claim (c)

For the unconstrained optimal control system Σ(θ, γ) with any (θ, γ) near ( ¯θ, 0), γ > 0, in order to
show that its trajectory derivative
is a globally unique minimizing trajectory to its correspond-
ing auxiliary control system Σ(ξ(θ,γ)), similarly to the claim of Theorem 1, we need to verify if the
following three conditions hold for Σ(θ, γ) at (θ, γ).

∂ξ(θ,γ)
∂θ

(i) The second-order condition holds for ξ(θ,γ) to be a local isolated minimizing trajectory for

Σ(θ, γ). In fact, this has been proved in the proof of Claim (a).

(ii) The gradients of all binding constraints (i.e., all equality and active inequality constraints)
are linearly independent at ξ(θ,γ). Since we do not have inequality constraints in Σ(θ, γ),
we only need to show the gradients of the dynamics constraint are linearly independent at
ξ(θ,γ). Speciﬁcally, we need to show that the following linear equations are independent

xt+1 = F x

t (θ, γ)xt + F u

t (θ, γ)ut,

and x0 = 0,

t = 0, 1, · · · , T.

(S.53)

where the dependence (θ, γ) means that the derivative matrices are evaluated at trajectory
ξ(θ,γ), x0:T and u0:T −1 here are variables. In fact, the above linear equations in (S.53) can
be equivalently written as

F xx1:T + F uu0:T −1 = 0,







−F u
0 (θ, γ)
0
...
0

0
−F u
1 (θ, γ)
...
0

· · ·
· · ·
. . .
· · · −F x

0
0
...
T −1(θ, γ)







,

with

and

F u =

F x =









I
−F x
1 (θ, γ)
...
0
0

0
I
...
0
0

· · ·
· · ·
. . .
· · ·
· · · −F x

0
0
...
I

T −1(θ, γ)









0
0
...
0
I

.

(S.54)

(S.55)

(S.56)

Obviously, all rows in the concatenation matrix [F u, F x] are linear-independent because
[F u, F x] is already in its the reduced echelon form and has full row rank. Thus, one can
conclude that the linear equations in (S.53) are linearly independent.

(iii) Strict complementarity does not apply because there are no inequality constraints in Σ(θ, γ)

at (θ, γ).

With the above three conditions satisﬁed, by applying Theorem 1, we can conclude that
is a
globally unique minimizing trajectory to the auxiliary control system Σ(ξ(θ,γ)). This completes the
Claim (c) in Theorem 2.

∂ξ(θ,γ)
∂θ

With the Claims (a), (b), and (c) proved, we have completed the proof of Theorem 2.

28

D Proof of Theorem 3

We know from the proof of Claim (a) of Theorem 2 in Appendix C.1 that given the conditions in
Theorem 3,

• for any (θ, γ) in the neighborhood of (θ∗, 0), there exists a unique once-continuously

differentiable function

, which satisﬁes (S.32), and

(cid:16)

ξ(θ,γ), λ(θ,γ)

(cid:17)

0:T , w(θ,γ)

0:T

0:T , v(θ,γ)
(cid:16)
(cid:17)
ξθ∗, λθ∗

=

(cid:16)

ξ(θ,γ), λ(θ,γ)

0:T , v(θ,γ)

0:T , w(θ,γ)

0:T

1:T , vθ∗

0:T , wθ∗

0:T

(cid:17)

when (θ, γ) = (θ∗, 0);

• additionally, if all functions deﬁning Σ(θ) are three-times continuously differentiable, it
immediately follows that ξ(θ,γ) is then twice continuously differentiable near (θ∗, 0). This
is a direct result by applying the C k implicit function theorem [87], to the C-PMP condition
(S.32) in the neighborhood of (θ∗, 0).

• additionally provided γ > 0, such ξ(θ,γ) is also a local isolated minimizing trajectory for

the well-deﬁned unconstrained optimal control system Σ(θ, γ) in Problem SB(θ, γ).

Thus, in the following, we will ignore the computation process for obtaining ξ(θ,γ) and simply
view that ξ(θ,γ)is the twice continuously differentiable function of (θ, γ) near (θ∗, 0) and ξθ∗ =
ξ(θ=θ∗,γ=0). The following proof of Theorem 3 follows the procedure of the general interior-point
minimization methods, which are systematically studied in [48] (see Theorem 14, p. 80).

Recall the optimization in Problem SP((cid:15), γ), re-write it below for easy reference,

with

θ∗((cid:15), γ) = arg min

θ

W (cid:0)θ, (cid:15), γ(cid:1)

W (cid:0)θ, (cid:15), γ(cid:1) = (cid:96)(cid:0)ξ(θ,γ), θ(cid:1) − (cid:15)

(cid:16)

ln

−Ri

(cid:0)ξ(θ,γ), θ(cid:1)(cid:17)

l
(cid:88)

i=1

(S.57)

(S.58)

Given in Theorem 3 that θ∗ satisﬁes the second-order sufﬁcient condition for a local isolated
minimizer to Problem P (recall the general second-order sufﬁcient condition in Lemma A.1), one can
say that there exists a multiplier u∗ ∈ Rl such that

∇L(θ∗, u∗) = ∇(cid:96)(ξθ∗ , θ∗) +
i Ri(ξθ∗ , θ∗) = 0,
u∗
Ri(ξθ∗ , θ∗) ≤ 0,

i = 1, 2, ..., l,
u∗
i ≥ 0,

i = 1, 2, ..., l,

(cid:88)l

i=1

i ∇Ri(ξθ∗ , θ∗) = 0,
u∗

with the Lagrangian deﬁned as

L(θ, u) = (cid:96)(ξθ, θ) +

(cid:88)l

i=1

uiRi(ξθ, θ),

(S.59)

(S.60)

and further for any θ (cid:54)= 0 satisfying θ(cid:48)∇Ri(ξθ∗ , θ∗) = 0 with u∗
with u∗

i ≥ 0, it follows

i > 0 and θ(cid:48)∇Ri(ξθ∗ , θ∗) ≤ 0

θ(cid:48)∇2L(θ∗, u∗)θ > 0.
(S.61)
Here, ∇L and ∇2L denote the ﬁrst- and second-order derivatives of L with respect to θ, respectively;
and ξθ∗ = ξ(θ=θ∗,γ=0).

D.1 Proof of Claim (a)

We modify the ﬁrst two equations in (S.59) into

(cid:16)

∇(cid:96)

(cid:17)
ξ(θ∗((cid:15),γ),γ), θ∗((cid:15), γ)

+

(cid:88)l

u∗
i ((cid:15), γ)Ri

i=1
(cid:17)
(cid:16)
ξ(θ∗((cid:15),γ),γ), θ∗((cid:15), γ)

= −(cid:15),

u∗
i ((cid:15), γ)∇Ri

(cid:16)

(cid:17)
ξ(θ∗((cid:15),γ),γ), θ∗((cid:15), γ)

= 0,

(S.62)

i = 1, 2, ..., l,

29

respectively, and consider both θ∗((cid:15), γ) and u∗((cid:15), γ) are implicitly determined by (cid:15) and γ through
the above equations.

Look at (S.62) and note that when (cid:15) = 0 and γ = 0, (S.62) is identical to the ﬁrst two equations
in (S.59). Given in Theorem 3 that all binding constraint gradients ∇Ri(ξθ∗ , θ∗) are linearly
independent at θ∗ and the strict complementary holds at θ∗, similar to the proof of Theorem 2, one can
apply the well-known implicit function theorem [75] to (S.62) in a neighborhood of ((cid:15), γ) = (0, 0),
leading to the following claim (i.e., the ﬁrst-order sensitivity result in Theorem 14 in [48]):

In a neighborhood of ((cid:15), γ) = (0, 0), there exists a unique once continuously differentiable
function (cid:0)θ∗((cid:15), γ), u∗((cid:15), γ)(cid:1), which satisﬁes (S.62) and (cid:0)θ∗((cid:15), γ), u∗((cid:15), γ)(cid:1) = (θ∗, u∗) when
((cid:15), γ) = (0, 0).

Next, we show that the above θ∗((cid:15), γ) always respects the constraints Ri
ξ(θ∗((cid:15),γ),γ), θ∗((cid:15), γ)
i = 1, 2, ..., l, for any small (cid:15) > 0 and any small γ > 0, which is the second-part of Claim (a).

(cid:16)

(cid:17)

< 0,

In fact, for any inactive constraint, Ri(ξθ∗ , θ∗) < 0, due to the continuity of
one has

(cid:17)
(cid:16)
ξ(θ∗((cid:15),γ),γ), θ∗((cid:15), γ)

Ri

→ Ri(ξθ∗ , θ∗) < 0

as

((cid:15), γ) → (0, 0),

(S.63)

(cid:16)

ξ(θ∗((cid:15),γ),γ), θ∗((cid:15), γ)

(cid:17)
,

(cid:16)

(cid:17)

ξ(θ∗((cid:15),γ),γ), θ∗((cid:15), γ)
and thus Ri
Ri(ξθ∗ , θ∗) = 0, and since the corresponding u∗
Theorem 3) and the continuity of u∗((cid:15), γ), one has

< 0 for any small (cid:15) > 0 and γ > 0. For any active constraint,
i > 0 (due to the strict complementarity given in

and thus u∗

i ((cid:15), γ) > 0 for small (cid:15) > 0 and consequently

i ((cid:15), γ) → u∗
u∗

i > 0

as

((cid:15), γ) → (0, 0),

(cid:16)

Ri

ξ(θ∗((cid:15),γ),γ), θ∗((cid:15), γ)

(cid:17)

= −

(cid:15)
u∗
i ((cid:15), γ)

< 0

(S.64)

(S.65)

because of (S.62). Therefore, we have proved that for any small (cid:15) > 0 and γ > 0, θ∗((cid:15), γ) always
(cid:17)
ξ(θ∗((cid:15),γ),γ), θ∗((cid:15), γ)
respect the constraints Ri
< 0, i = 1, 2, ..., l. This prove the second part of
Claim (a).

(cid:16)

From now on, we show that the above θ∗((cid:15), γ) with any small (cid:15) > 0 and γ > 0 also is a local isolated
minimizer to the unconstrained optimization (S.57). From the last equation in (S.62), we solve
(cid:15)
ξ(θ∗((cid:15),γ),γ), θ∗((cid:15), γ)

u∗
i ((cid:15), γ) = −

i = 1, 2, ..., l,

(S.66)

(cid:17) ,

Ri

(cid:16)

and substitute it to the ﬁrst equation, yielding

(cid:16)

∇(cid:96)

(cid:17)
ξ(θ∗((cid:15),γ),γ), θ∗((cid:15), γ)

−

l
(cid:88)

i=1

Ri

(cid:16)

(cid:15)
ξ(θ∗((cid:15),γ),γ), θ∗((cid:15), γ)

(cid:17) ∇Ri

(cid:16)

(cid:17)
ξ(θ∗((cid:15),γ),γ), θ∗((cid:15), γ)

= 0.

(S.67)
One can ﬁnd that the obtained equation in (S.67) is exactly the ﬁrst-order optimality condition (KKT
condition) for the unconstrained optimization in Problem SP((cid:15), γ) in (S.57), and this indicates that
θ∗((cid:15), γ) satisﬁes the KKT condition for Problem SP((cid:15), γ). To further show that θ∗((cid:15), γ) is a local
isolated minimizing solution to Problem SP((cid:15), γ) in (S.57), we only need to verify the second-order
condition, that is, for any nonzero θ (cid:54)= 0,

θ(cid:48)(cid:16)

∇2W (cid:0)θ∗((cid:15), γ), (cid:15), γ(cid:1)(cid:17)

θ > 0,

(S.68)

for any small (cid:15) > 0 and γ > 0, which will be proved next.

Proof by contradiction: suppose that the second-order condition (S.68) is false. Then, there must exist
a sequence of ((cid:15)k, γk) > 0 and a sequence of θk for k = 1, 2, ... such that ((cid:15)k, γk) → (0, 0) and

θ(cid:48)
k

(cid:0)∇2W (cid:0)θ∗((cid:15)k, γk), (cid:15)k, γk

(cid:1)(cid:17)

θk ≤ 0.

(S.69)

30

as k → +∞. Without loss of generality, assume (cid:107)θk(cid:107) = 1 for all k. Select a convergent sub-sequence
of θk, relabel the sequence θk for convenience, and call the limit ¯θ, that is, θk → ¯θ and ((cid:15)k, γk) → 0
as k → +∞. Then,

lim
k→+∞

θ(cid:48)
k

(cid:32)

= lim

k→+∞

θ(cid:48)
k

(cid:32)

θ(cid:48)
k

= lim

k→+∞

(cid:0)∇2W (cid:0)θ∗((cid:15)k, γk), (cid:15)k, γk

(cid:1)(cid:17)

θk

(cid:16)

∇2L(θ∗((cid:15)k, γk), u∗((cid:15)k, γk)) +

(cid:15)k

(Ri((cid:15)k, γk))2 (∇Ri((cid:15)k, γk)∇Ri((cid:15)k, γk)(cid:48))

(cid:17)

(cid:33)

θk

l
(cid:88)

i=1
(cid:33)

(cid:16)

(cid:17)
∇2L(θ∗((cid:15)k, γk), u∗((cid:15)k, γk))

θk

+ lim

k→+∞

(cid:32) l

(cid:88)

i=1

(cid:15)k

(Ri((cid:15)k, γk))2 (∇Ri((cid:15)k, γk)(cid:48)θk)2

(cid:33)

(cid:48)(cid:16)

= ¯θ

∇2L(θ∗, u∗)

(cid:17) ¯θ + lim

k→+∞

(cid:32) l

(cid:88)

i=1

(cid:15)k

(Ri((cid:15)k, γk))2 (∇Ri((cid:15)k, γk)(cid:48)θk)2

(cid:33)

,

=

Ri(ξ(θ∗((cid:15)k,γk),γk), θ∗((cid:15)k, γk))

(S.70)
=
where we write Ri((cid:15)k, γk)
∇Ri(ξ(θ∗((cid:15)k,γk),γk), θ∗((cid:15)k, γk))
line is because
for notation convenience,
L(θ, u) in (S.60) is twice-continuously differentiable with respect to (θ, u) near (θ∗, u∗),
and (cid:0)θ∗((cid:15), γ), u∗((cid:15), γ)(cid:1) is once-continuously differentiable with respect to ((cid:15), γ) near (0, 0). In
(S.70), we consider two cases for ¯θ:
Case 1: (cid:107) ¯θ(cid:107) = 1 and there exists at least an active inequality constraint Ri(ξθ∗ , θ∗) = 0, such that
(cid:48)
¯θ

∇Ri(ξθ∗ , θ∗) (cid:54)= 0. Then,

and ∇Ri((cid:15)k)

and the last

lim
k→+∞

= lim

k→+∞

(cid:32) l

(cid:88)

i=1

(cid:32) l

(cid:88)

i=1

(cid:15)k

(Ri((cid:15)k, γk))2 (∇Ri((cid:15)k, γk)(cid:48)θk)2

(cid:33)

−ui((cid:15)k, γk)
Ri((cid:15)k, γk)

(∇Ri((cid:15)k, γk)(cid:48)θk)2

= +∞.

(cid:33)

This is because the following term corresponding to such active constraint has

lim
k→+∞

−ui((cid:15)k, γk)
Ri(ξ(θ∗((cid:15)k,γk),γk), θ∗((cid:15)k, γk))

= +∞.

due to the strict complementarity given in Theorem 3. Therefore, (S.70) will have

(cid:16)

θ(cid:48)
k

lim
k→+∞

∇2W (cid:0)θ∗((cid:15)k, γk), (cid:15)k, γk

(cid:1)(cid:17)

θk = +∞,

which contradicts the assumption in (S.69) in that

(cid:16)

θ(cid:48)
k

lim
k→+∞

∇2W (cid:0)θ∗((cid:15)k, γk), (cid:15)k, γk

(cid:1)(cid:17)

θk ≤ 0.

(S.71)

(S.72)

(S.73)

(S.74)

Case 2: (cid:107) ¯θ(cid:107) = 1 and for any active constraint Ri(ξθ∗ , θ∗) = 0 (and u∗
(cid:48)
mentarity given in Theorem 3), ¯θ
∇Ri(ξθ∗ , θ∗) = 0. Then, from (S.70),

i > 0 due to strict comple-

lim
k→+∞

θ(cid:48)
k

(cid:0)∇2W (cid:0)θ∗((cid:15)k, γk), (cid:15)k, γk

(cid:1)(cid:17)

(cid:48)(cid:16)

θk ≥ ¯θ

∇2L(θ∗, u∗)

(cid:17) ¯θ > 0,

(S.75)

where the last inequality is because of the second-order condition in (S.61) satisﬁed for θ∗ given in
Theorem 3. The obtained (S.75) also contradicts the assumption in (S.69).
Combining the above two cases, we can conclude that given any small (cid:15) > 0 and γ > 0, θ∗((cid:15), γ)
satisﬁes both the KKT condition (S.67) and the second-order condition (S.68) for W (θ, (cid:15), γ). Thus,
one can assert that θ∗((cid:15), γ) is a local isolated minimizer to the unconstrained optimization W (θ, (cid:15), γ)
in (S.57), i.e., Problem SP((cid:15), γ). This completes the proof of the Claim (a) in Theorem 3.

31

D.2 Proof of Claim (b)

From the previous proof for Claim (a), we have the following conclusions: ﬁrst, for any ((cid:15), γ)
in a neighborhood of (0, 0), there exists a unique once-continuously differentiable function
(cid:0)θ∗((cid:15), γ), u∗((cid:15), γ)(cid:1), satisfying (S.62); second, additionally provided small (cid:15) > 0 and γ > 0,
such θ∗((cid:15), γ) is also a local isolated minimizer to the well-deﬁned unconstrained minimization
W (θ, (cid:15), γ) in (S.57); and third, when ((cid:15), γ) = (0, 0), (S.62) becomes the KKT condition for Problem
P, whose solution (θ∗, u∗) must satisfy. Therefore, due to the uniqueness and continuity of the
function (θ∗((cid:15), γ), u∗((cid:15), γ)) near ((cid:15), γ) = (0, 0), one can obtain

θ∗((cid:15), γ) → θ∗(0, 0) = θ∗,

as

((cid:15), γ) → (0, 0).

(S.76)

This completes the proof of Claim (b) in Theorem 3.

D.3 Proof of Claim (c)

To prove Claim (c) in Theorem 3, we use the following facts: ﬁrst, as proved in Claim (a), for
any small (cid:15) > 0 and γ > 0, θ∗((cid:15), γ) always respects the constraints Ri(ξ(θ∗((cid:15),γ),γ), θ∗((cid:15), γ)) < 0,
i = 1, 2, ..., l; second, as also proved in Claim (a), θ∗((cid:15), γ) is differentiable with respect to ((cid:15), γ)
near (0, 0) and θ∗((cid:15), γ) → θ∗ as ((cid:15), γ) → (0, 0); and third, as proved in Theorem 2, ξ(θ,γ) is a
differentiable function of (θ, γ) near (θ∗, 0). All these facts lead to that for small γ > 0, Ri(ξ(θ,γ), θ),
i = 1, 2, ..., l, is also a continuous function of θ near θ∗((cid:15), γ), and

Ri(ξ(θ,γ), θ) → Ri

(cid:0)ξ(θ∗((cid:15),γ),γ), θ∗((cid:15), γ)(cid:1) < 0,

as θ → θ∗((cid:15), γ),

∀i = 1, 2, ..., l.

(S.77)

Thus Ri(ξ(θ,γ), θ) < 0, i = 1, 2, ..., l, holds for any θ in a small neighborhood of θ∗((cid:15), γ) with
small (cid:15) > 0 and small γ > 0. This completes the proof of Claim (c) in Theorem 3.

With the above proofs for Claims (a), (b), and (c), we have completed the proof of Theorem 3.

32

E Algorithms for Safe PDP

We have implemented Safe PDP in Python and made it as a stand-alone package with friendly
interfaces. Please download at https://github.com/wanxinjin/Safe-PDP.

E.1 Algorithm for Theorem 1

Algorithm 1: Solving ∂ξθ

∂θ by establishing auxiliary control system Σ(ξθ)

Input: ξθ, with the costates λθ

1:T and multiplies vθ

0:T and wθ

0:T from solving Problem B(θ)

def Identify_Active_Inequality_Constraints (a small threshold δ > 0):

¯gt(xt, ut, θ) = col{gt,i | gt,i(xθ
¯gT (xT , θ) = col{gT,i | gT,i(xθ
Return: ¯gt(xt, ut, θ) and ¯gT (xT , θ)

t , θ)≥−δ, i = 1, 2, ..., qt}, t = 0, 1, ..., T −1;

t , uθ
T , θ)≥−δ, i = 1, 2, ..., qT };

Compute the derivative matrices Lxx
t , ¯Gx
H u

t , ¯Gu

t , ¯Gθ

t , H x

t , H θ

T , ¯Gx

T , H θ

, Luu
t

, Lxu
t , Luθ
T , Lxθ
t
t
T , ¯Gθ
T to establish Σ(ξθ) in (3);

, Lxx

, Lxθ

t

T , F x

t , F u

t , F θ

t , H x
t ,

def Equality_Constrained_LQR_ Solver ( Σ(ξθ) ):

Implementation of the equality constrained LQR algorithm in [80];

Return: {X θ

0:T , U θ

0:T −1}

Return: ∂ξθ

∂θ = {X θ

0:T , U θ

0:T −1}

1:T , vθ

0:T , and wθ

Note that λθ
0:T are normally the by-product outputs of an optimal control solver [77],
and can also be obtained by solving a linear equation of C-PMP (S.5) given ξθ, as done in [71]. Also
note that the threshold δ to determine the active inequality constraints can be set according to the
accuracy of the solver; in our experiments, we use δ = 10−3.

E.2 Algorithm for Theorem 2
Algorithm 2: Safe unconstrained approximations for ξθ and ∂ξθ
Input: The constrained optimal control system Σ(θ) and a choice of small γ > 0

∂θ

Convert Σ(θ) to an unconstrained optimal control system Σ(θ, γ) in (5) by adding all
constraints in Σ(θ) to its control cost through barrier functions with barrier parameter γ;

/* Below is an implmentation of uncsontrained PDP [71]
def Optimal_Control_Solver ( Σ(θ, γ) ):

*/

Implementation of any trajectory optimization algorithms, such as iLQR [81] and

DDP [82], or use any optimal control solver [77];

Return: ξ(θ,γ)

Use ξ(θ,γ) to compute the derivative matrices ˆLxu
F θ

t , ˆLuθ
t
t to establish the auxiliary control system Σ(ξ(θ,γ)) in (S.78) ;

, ˆLuu
t

, ˆLxθ

t

, ˆLxx

T , ˆLxθ

T , F x

t , F u
t ,

def LQR_ Solver ( Σ(ξ(θ,γ)) ):

Implementation of any LQR algorithm such as Lemma 2 in [71];

Return:

(cid:110)

X (θ,γ)

0:T , U (θ,γ)

0:T −1

Return: ξ(θ,γ) and

∂ξ(θ,γ)
∂θ

(cid:111)

=

∂ξ(θ,γ)
∂θ

33

Note that the auxiliary control system Σ(ξ(θ,γ)) corresponding to Σ(θ, γ) is

control cost

¯W = Tr

(cid:32)

T −1
(cid:88)

(cid:20)Xt
Ut

(cid:21)(cid:48) (cid:20) ˆLxx
t
ˆLux
t

1
2

ˆLxu
t
ˆLuu
t

(cid:21)

(cid:21) (cid:20)Xt
Ut

+

(cid:20) ˆLxθ
t
ˆLuθ
t

(cid:21)(cid:48) (cid:20)Xt
Ut

(cid:21)(cid:33)

Σ(ξ(θ,γ)) :

t=0
(cid:18) 1
2

+ Tr

X (cid:48)
T

T XT + ( ˆLxθ
ˆLxx

T )(cid:48)XT

(cid:19)

(S.78)

subject to
dynamics Xt+1 = F x

t Xt + F u

t Ut + F θ

t with X0 = X θ
0 .

Here, ˆLt, t = 0, 1, ..., T −1, and ˆLT are the Hamiltonian, deﬁned in (S.31), for the unconstrained
optimal control system Σ(θ, γ). The derivative (coefﬁcient) matrices ˆLxu
T , ˆLxθ
T ,
t
t , F u
F x

, ˆLuu
t
t in (S.78) are deﬁned in the similar notation convention as in (3).

t , ˆLuθ

t , F θ

, ˆLxx

, ˆLxθ

t

E.3 Algorithm for Theorem 3

Algorithm 3: Safe PDP to solve Problem P

Input: Small barrier parameter (cid:15) > 0 for outer-level and γ > 0 for inner-level, initialization θ0

/* Convert inner-level into its safe unconstrained approximation */

Convert the inner-level constrained control system Σ(θ) in (1) into an unconstrained
system Σ(θ, γ) in (5) by adding all constraints to its control cost through barrier
functions with the inner-level barrier parameter γ > 0;

/* Convert outer-level into its safe unconstrained approximation */

Convert the constrained Problem P to an unconstrained Problem SP((cid:15), γ) by adding all
task constraints Ri to the task loss through barrier functions with the outer-level barrier
parameter (cid:15) > 0;

/* Gradient-based update for θ

for k = 0, 1, 2, · · · do

*/

Apply Algorithm 2 to the inner-level safe unconstrained approximation system
Σ(θk, γ) to compute ξ(θk,γ) and

∂ξ(θ,γ)
∂θ

|θk ;

For the outer-level unconstrained Problem SP((cid:15), γ) with objective function
W (cid:0)θ, (cid:15), γ(cid:1) = (cid:96)(cid:0)ξ(θ,γ), θ(cid:1) − (cid:15) (cid:80)l
, compute the partial
gradients ∂W

(cid:0)ξ(θ,γ), θ(cid:1)(cid:17)

−Ri

(cid:16)

∂θ |θk and ∂W
∂ξ(θ,γ)

i=1 ln
|ξ(θk ,γ);

Apply the chain rule to obtain the gradient of the outer-level unconstrained objective
W (cid:0)θ, (cid:15), γ(cid:1) with respect to θ, i.e., dW
dθ |θk = ∂W

∂θ |θk + ∂W
∂ξ(θ,γ)

∂ξ(θ,γ)
∂θ

|ξ(θk ,γ)

|θk ;

Gradient-based update: θk+1 = θk − η dW

dθ |θk with η being the learning rate;

end

Return: θ∗((cid:15), γ) for the given barrier parameters (cid:15) > 0 and γ > 0

Note that after obtaining θ∗((cid:15), γ) from Algorithm 3, one can sequentially reﬁne θ∗((cid:15), γ) by choosing
a sequence of {((cid:15), γ)} such that ((cid:15), γ) → (0, 0).

Also note that in the case where the original inner-level control system Σ(θ) is already unconstrained,
such as in the applications of safe policy optimization and safe motion planning in Section 7, please
modify lines 1 and 3 in Algorithm 3 and just compute exact ξθk and ∂ξθ
∂θ |θk by following PDP [71].

34

1

2

3

4

5

6

F Experiment Details

The proposed Safe PDP has been evaluated in different simulated systems in Table 1, where each
system has the immediate constraints g(θcstr) on both its state and input during the entire time horizon
(around T = 50). For the detailed description and physical models of each system in Table 1, we refer
the reader to [71] and its accompanying codes. We have developed the Python code of Safe PDP as a
stand-alone package, which can be accessed at https://github.com/wanxinjin/Safe-PDP.

F.1 Safe Policy Optimization

In this experiment, we apply Safe PDP to perform safe policy optimization for the systems in Table 1.
In Problem P, we set the details of Σ(θ) as (8), where the dynamics f is learned from demonstrations
in Section F.3, and the policy ut = π(xt, θ) is represented using a neural network (NN) with θ the
NN parameter. In our experiment, we have used a fully-connected feedforward NN to represent the
policy; the number of nodes in the NN is n−n−m (meaning that the input layer has n nodes, hidden
layer n nodes, and output layer m nodes, with n and m the dimensions of the system state and input,
respectively); and the activation function of the NN is tanh. In Problem P, set the task loss (cid:96)(ξθ, θ)
as the control cost J(θobj), and set the task constraints Ri(ξθ, θ) as the system constraints g(θcstr),
both in Table 1, with both θobj and θcstr known.
Note that since the parameterized Σ(θ) in (8) does not include the control cost J anymore, solving
Problem B(θ) for ξθ becomes a simple integration of (8) from t = 0 to T −1, and the auxiliary
control system Σ(ξθ) in (3) to compute ∂ξθ
∂θ is simpliﬁed to a feedback control system [71] below:

Σ(ξθ) :

dynamics: X θ

control policy:

t+1 = F x
U θ
t = U x

t X θ
t X θ

t + F u
t U θ
t + U e
t .

t with X0 = 0,

(S.79)

and U e

t = ∂πt

Here, U x

t = ∂πt
∂xθ
t

∂θ . Integrating (S.79) from t = 0 to T −1 leads to {X θ

0:T −1}= ∂ξθ
∂θ .
In our experiments, in order to make sure the initial NN policy is feasible (safe), we initialize the NN
policy using supervised learning from a random demonstration trajectory (note that this demonstration
trajectory does not have to be optimal but only to be feasible/safe). For each system in Table 1,
we apply Safe PDP Algorithm 3 to optimize the NN policy, and the complete experiment results
are shown in Fig. S1-S3. More discussions about how to give a safe initialization are presented in
Appendix G.5

0:T , U θ

For each system, at a ﬁxed barrier parameter (cid:15), we have applied the vanilla gradient descent to solve
Problem SP((cid:15)) with the step size (learning rate η in Algorithm 3) set around 10−3. We plot the task
loss (i.e., control cost) (cid:96)(ξθ, θ) versus iteration of the gradient descent in the ﬁrst panel in Fig. S1-S3,
where we only show the results for outer-level barrier parameter (cid:15) taking from 100 to 10−4 because
the NN policy has already achieved a good convergence when (cid:15) ≤ 10−2. As shown in the ﬁrst panel
in Fig. S1-S3, for each system, the policy achieves a good convergence after a small number of
iterations for each (cid:15), and obtains a good convergence after (cid:15) ≤ 10−2.

Figure S1: Safe neural policy optimization for cartpole. The ﬁrst panel plots the loss (control cost)
versus gradient-descent iteration under different outer-level barrier parameter (cid:15); the second panel plots
all intermediate control trajectories generated by the NN policy during the entire gradient-descent
iteration ((cid:15) = 10−4); and the third panel plots all intermediate control trajectories generated by the
NN policy for the unconstrained policy optimization under the same experimental conditions. The
system constraints are also marked using black dashed lines in the second and third panels.

35

=101=102=103=1040100020003000Iteration125150175200225Loss (control cost)05101520Time t6420246Generated utSafe PDP, =104Iter #0Iter #300005101520Time tumaxuminUnconstrainedIter #0Iter #3000Figure S2: Safe neural policy optimization for robot arm. The ﬁrst panel plots the loss (control cost)
versus gradient-descent iteration under different outer-level barrier parameter (cid:15); the second panel plots
all intermediate control trajectories generated by the NN policy during the entire gradient-descent
iteration ((cid:15) = 10−4); and the third panel plots all intermediate control trajectories generated by the
NN policy for the unconstrained policy optimization under the same experimental conditions. The
system constraints are also marked using black dashed lines in the second and third panels.

Figure S3: Safe neural policy optimization for 6-DoF maneuvering quadrotor. The ﬁrst panel plots
the loss (control cost) versus gradient-descent iteration under different outer-level barrier parameter
(cid:15); the second panel plots all intermediate control trajectories generated by the NN policy during
the entire gradient-descent iteration ((cid:15) = 10−2); and the third panel plots all intermediate control
trajectories generated by the NN policy for the unconstrained policy optimization under the same
experimental conditions. The system constraints are also marked using black dashed lines in the
second and third panels.

In order to show the constraint satisfaction of Safe PDP throughout the entire policy optimization
process, in the second panel in Fig. S1-S3, respectively, we plot all intermediate control trajectories
generated from the NN policy throughout the entire gradient-descent iteration of Safe PDP, as shown
from the light to dark blue. From the second panel in Fig. S1-S3, we note that throughout the
optimization process, the NN policy is guaranteed safe, meaning that the generated trajectory will
never violate the constraints. Under the same experimental conditions (NN conﬁguration, policy
initialization, learning rate), we also compare with the unconstrained policy optimization and plot its
results in the third panel in Fig. S1-S3, respectively. By comparing the results between Safe PDP
and unconstrained policy optimization, we can conﬁrm that Safe PDP enables to achieve an optimal
policy while guaranteeing that any intermediate policy throughout optimization is safe, as asserted in
Theorem 3.

We have provided the video demonstrations for the above safe policy optimization using Safe
PDP; please visit https://youtu.be/sC81qc2ip8U. The codes for all experiments here can be
downloaded at https://github.com/wanxinjin/Safe-PDP.

F.2 Safe Motion Planning

In this experiment, we apply Safe PDP to solve the safe motion planning problem for the systems in
Table 1. In Problem P, we set the details of Σ(θ) as follows,

Σ(θ) :

dynamics: xt+1 = f (xt, ut) with x0,

control input: ut = u(t, θ),

(S.80)

where the dynamics f is learned from demonstrations in Section F.3, and we parameterize the control

36

=101=102=103=104050010001500Iteration14.515.015.516.016.5Loss (control cost)Iter. #0Iter. #1500ut,1ut,20510152025Time t210123Generated utSafe PDP, =104Iter. #0Iter. #15000510152025Time tumaxuminUnconstrained=1=101=1020500100015002000Iteration20002500300035004000Loss (control cost)Iter. #0Iter. #20000510152025Time t05101520Generated ||ut||Safe PDP, =102Iter. #0Iter. #20000510152025Time tumaxUnconstrainedinput function ut = u(t, θ) using the Lagrangian polynomial [83] as follows,

u(t, θ) =

N
(cid:88)

i=0

uibi(t)

with

bi(t) =

(cid:89)

0≤j≤N,j(cid:54)=i

t − tj
ti − tj

.

Here, bi(t) is called Lagrange basis, and the policy parameter θ is deﬁned as

θ = [u0, · · · , uN ](cid:48) ∈ Rm(N +1),

(S.81)

(S.82)

which is the vector of the pivots of the Lagrange polynomial. The beneﬁt of the above parameterization
is that the trajectory of system states, which results from integrating (S.80) given the input polynomial
trajectory ut = u(t, θ), is inherently smooth and dynamics-feasible. In our experiments, the degree
N of the Lagrange polynomial is set as N = 10. Also in Problem P, we set the task/planning loss
(cid:96)(ξθ, θ) as the control cost J(θobj), and set the task constraints Ri(ξθ, θ) as the system constraints
g(θcstr), both given in Table 1 with θobj and θcstr known.
Since the system Σ(θ) in (S.80) now does not include the control cost J anymore, solving Problem
B(θ) for ξθ becomes a simple integration of (S.80) from t = 0 to T −1, and the auxiliary control
system Σ(ξθ) in (3) to compute ∂ξθ

∂θ is simpliﬁed to a feedback control system [71] below:

Σ(ξθ) :

control input:

t+1 = F x
t X θ
t = U e
U θ
t ,

dynamics: X θ

t + F u

t U θ

t with X0 = 0,

(S.83)

where U e

t = ∂πt

∂θ . Integrating (S.83) from t = 0 to T − 1 leads to {X θ

0:T , U θ

0:T −1} = ∂ξθ
∂θ .

For each system in Table 1, we apply Safe PDP Algorithm 3 to perform safe motion planning, and
the complete experiment results are shown in Fig. S4-S6. For each system, at a ﬁxed outer-level
barrier parameter (cid:15), we have applied the vanilla gradient descent to solve Problem SP((cid:15)) with the step
size (learning rate η in Algorithm 3) set to 10−2 or 10−1. We plot the planning loss (cid:96)(ξθ, θ) versus
gradient descent iteration in Fig. S4a-S6a, respectively; here we only show the results for (cid:15) taking
from 100 to 10−2 because the trajectory has already achieved a good convergence when (cid:15) ≤ 10−2.
As shown in Fig. S4a-S6a, for each system, the trajectory achieves a good convergence after a small
number of iterations given a ﬁxed (cid:15), and obtains a good convergence after (cid:15) ≤ 10−2.

To demonstrate that Safe PDP can guarantee safety throughout the optimization process, we plot all
intermediate trajectories during the entire iteration of Safe PDP in S4b-S6b. At the same time, we
also show the results of the ALTRO method [21], which is a state-of-the-art method for constrained
trajectory optimization. By comparing the results in Fig. S4b-S6b, we can observe that Safe PDP
enables to ﬁnd the optimal trajectory while guaranteeing strict constraint satisfaction throughout the
entire optimization process; while for ALTRO, although the trajectory satisﬁes the constraints at
convergence, the intermediate trajectories during optimization may violate the constraints, making it
not suitable to handle safety-critical motion planning tasks.

(a) Loss versus iteration

(b) Constraint violation during optimization

Figure S4: Safe motion planning for cartpole. (a) plots the loss (i.e., planning loss) versus gradient-
descent iteration under different outer-level barrier parameter (cid:15). The left ﬁgure in (b) shows all
intermediate trajectories during the entire iteration of Safe PDP ((cid:15) = 10−2), and the right ﬁgure in
(b) shows all intermediate trajectories during the entire iteration of the ALTRO algorithm [21]. The
state and control constraints are also marked in (b).

37

=1=101=1020100020003000Iteration160180200220240260Loss (planning loss)Iter. #0Iter. #3000505ControlSafe PDP, =102Iter. #0Iter. #3000ALTROIter. #0Iter. #30000510152025Time t202Cart pos.Iter. #0Iter. #30000510152025Time tumaxuminxmaxxmin(a) Loss versus iteration

(b) Constraint violation during optimization

Figure S5: Safe motion planning for robot arm. (a) plots the loss (i.e., planning loss) versus gradient-
descent iteration under different outer-level barrier parameter (cid:15). The left ﬁgure in (b) shows all
intermediate trajectories during the entire iteration of Safe PDP ((cid:15) = 10−3), and the right ﬁgure in
(b) shows all intermediate trajectories during the entire iteration of the ALTRO algorithm [21]. The
control constraints are also marked in (b).

(a) Loss versus iteration

(b) Constraint violation during optimization

Figure S6: Safe motion planning for 6-DoF rocket powered landing. (a) plots the loss (i.e., planning
loss) versus gradient-descent iteration under different outer-level barrier parameter (cid:15). The left ﬁgure
in (b) shows all intermediate trajectories during the entire iteration of Safe PDP ((cid:15) = 10−2), and
the right ﬁgure in (b) shows all intermediate trajectories during the entire iteration of the ALTRO
algorithm [21]. The state and control constraints are also marked in (b).

We have provided the videos for the above safe motion planning using Safe PDP. Please visit the
link https://youtu.be/vZVxgo30mDs. The codes for all experiments here can be downloaded at
https://github.com/wanxinjin/Safe-PDP.

F.3 Learning MPCs from Demonstrations

In this experiment, we apply Safe PDP to learn dynamics f , constraints g, or/and control cost J for
the systems in Table 1 from demonstration data. This type of problems has been extensively studied
in system identiﬁcation [88] (neural ODEs [89]), inverse optimal control (inverse reinforcement
learning) [90–92], and learning from demonstrations [85, 93]. However, existing methods have the
following two technical gaps; ﬁrst, existing methods are typically developed without considering
constraints; second, there are rarely the methods that are capable to jointly learn dynamics, state-input
constraints, and control cost for continuous control systems. In this part, we will show that the above
technical gaps can be addressed by Safe PDP. Throughout this part, we deﬁne the task loss in Problem
P as the reproducing loss as below

(cid:96)(ξθ, θ) = (cid:107)ξdemo − ξθ(cid:107)2
2,
which is to penalize the distance between the reproduced trajectory ξθ from the learnable model
Σ(θ) and the given demonstrations ξdemo, and there is no task constraint. For Σ(θ) in Problem P,
only the unknown parts (dynamics, control cost, or/and constraints) are parameterized by θ. Thus, by
solving Problem P, we are able to learn Σ(θ) such that its trajectory ξθ has closest distance to the
given demonstrations ξdemo.
In our experiment, when dealing with ξθ and ∂ξθ

∂θ for Σ(θ), we use the following three strategies.

(S.84)

38

=101=102=10302004006008001000Iteration152025303540Loss (planning loss)Iter. #0Iter. #100002Control ut,1Safe PDP, =103Iter. #0Iter. #1000ALTROIter. #0Iter. #10000510152025Time t02Control ut,2Iter. #0Iter. #10000510152025Time tUmaxUminUmaxUmin=1=101=102050010001500Iteration1234Loss (planning loss)×105Iter. #0Iter. #15000102030Thrust ||u||2Safe PDP, =102Iter. #0Iter. #1500ALTROIter. #0Iter. #1500010203040Time t0.00.20.4Tilt angleIter. #0Iter. #1500010203040Time t||u||maxtiltmax• Strategy (A): use an optimal control solver [77] to solve the constrained optimal control
Σ(θ) in Problem B(θ) to obtain ξθ, and use Theorem 1 (i.e., Algorithm 1) to obtain the
trajectory derivative ∂ξθ

∂θ by solving Σ(ξθ) in (3).

• Strategy (B): by applying Theorem 2 (i.e., Algorithm 2), approximate ξθ and ∂ξθ

∂θ using

ξ(θ,γ) and

∂ξ(θ,γ)
∂θ

, respectively, with a choice of small barrier parameter γ > 0.

• Strategy (C): obtain ξθ by solving Σ(θ) in Problem B(θ) via a solver [77], and apply

Theorem 2 (i.e., Algorithm 2) only to approximate ∂ξθ

∂θ using

∂ξ(θ,γ)
∂θ

.

In the following experiments, when using Algorithm 2, we choose γ = 10−2 because the corre-
sponding inner-level approximations ξ(θ,γ) and
already achieve a good accuracy, as shown
in previous experiments. In practice, the choice of γ > 0 is very ﬂexible depending on the desired
accuracy (a smaller γ never hurts but would decrease the computational efﬁciency, as discussed in
Appendix G.3).

∂ξ(θ,γ)
∂θ

F.3.1 Learning Constrained ODEs from Demonstrations

In the ﬁrst experiment, consider that in Σ(θ) the control cost J is known while the dynamics
(Ordinary Difference Equation) f (θdyn) and constraints gt(θcstr) are unknown and parameterized, as
in Table 1, θ={θdyn, θcstr}. We aim to learn θ from given demonstrations ξdemo by solving Problem P.
Here, the demonstrations are generated by simulating the true system (i.e., expert) with θ known; the
demonstrations for each system contain two episode trajectories with time horizon around T = 50.

To solve Problem P, since there are no task constraints, we use the vanilla gradient descent to
minimize the reproducing loss (S.84) while using the three strategies as mentioned above to handle
the lower-level Problem B(θ). The initial condition for the gradient descent is given randomly, and
the learning rate for the gradient descent is set as 10−5. The complete results for all systems in Table
1 are given in Fig. S7.

(a) Cartpole

(b) Robot arm

(c) Quadrotor

(d) Rocket

Figure S7: Learning both dynamics and constraints from demonstrations.

Fig. S7a-S7d plot the reproducing loss (S.84) versus gradient-descent iteration. The results show
that for Strategies (B) and (C) (in blue and red, respectively), the reproducing loss (S.84) is quickly
covering to zeros, indicating that the dynamics and constraints are successfully learned to reproduce
the demonstrations. However, we also note that Strategy (A) (in green) suffers from some numerical
instability, and this will be discussed later.

F.3.2 Jointly Learning Dynamics, Constraints, and Control Cost from Demonstrations

In the second experiment, suppose in all systems in Table 1, the control cost J(θcost), dynamics
f (θdyn), and state and input constraints gt(θcstr) are all unknown and parameterized as in Table 1.
We aim to jointly learn θ = {θcost, θdyn, θcstr} from given demonstrations ξdemo by solving Problem
P. Here, the demonstrations are generated by simulating the system (i.e., expert) with θ known, the
demonstrations for each system contain two episode trajectories for each system with time horizon
around T = 50.

To solve Problem P, since there is no task constraints, we use the vanilla gradient descent to minimize
the reproducing loss (S.84) while using the three strategies as mentioned above to handle the lower-
level Problem B(θ). The initial condition for the gradient descent is given randomly, and the learning

39

Thm 1Thm 2 for  and  Thm 2 only for 0204060Iteration0100200300400500Reproducing lossThm 1Thm 2 for  and  Thm 2 only for 0204060Iteration0.02.55.07.510.012.5Reproducing lossThm 1Thm 2 for  and  Thm 2 only for 0204060Iteration020406080Reproducing lossThm 1Thm 2 for  and  Thm 2 only for 0204060Iteration0100200300400500Reproducing lossrate for the gradient-descent is set as 10−5. The complete results for all systems in Table 1 are given
in Fig. S8 (also see Fig. 4a-4d in the primary text of the paper).

(a) Cartpole (log-y)

(b) Robot arm

(c) Quadrotor (log-y)

(d) Rocket

Figure S8: Jointly learning dynamics, constraints, and control cost from demonstrations.

Fig. S8a - Fig. S8d plot the reproducing loss (S.84) versus gradient-descent iteration. The results
show that for Strategies (B) and (C) (in blue and red, respectively), the reproducing loss (S.84) is
quickly covering to zeros, indicating that the dynamics, constraints, and control cost function are
successfully learned to reproduce the demonstrations. However, we also note that Strategy (A) (in
green) suffers from some numerical instability, which will be discussed below.

We have provided some videos for the above learning MPCs from demonstrations using Safe PDP.
Please visit the link https://youtu.be/OBiLYYlWi98. The codes for all experiments here can be
downloaded at https://github.com/wanxinjin/Safe-PDP. 1

Why implementation of Theorem 1 is not numerically stable?
In both Fig. S7 and S8, we
have noted that Strategy (A) suffers from some numerical instability, and this is due to the following
reasons. First, Theorem 1 requires to accurately detect the inactive/active inequalities (i.e., whether
an inequality constraint is zero or not), which is always difﬁcult accurately due to computational error
(in our experiments, we detect the active constraints by applying a brutal threshold, as described in
Algorithm 1). Second, although the differentiability of ξθ holds at the local neighborhood of θ, ξθ
might be extremely discontinuous due to the ‘jumping switch’ of the active and inactive inequality
constraints for the large range of θ; thus, such non-smoothness will deteriorate the decrease of loss
between iterations.

Why implementation of Theorem 2 is more numerically stable?
Theorem 2 has perfectly
addressed the above numerical issues of Theorem 1. Speciﬁcally, ﬁrst, there is no need to distinguish
the active and inactive inequality constraints in Theorem 2; and second, in Theorem 2, by adding
all constraints to the control cost function, it introduces the ‘softness’ of the hard constraints and
potentially eliminates the discontinuous ‘jumping switch’ between inactive and active inequalities
over a large range of θ, enabling a stable decrease of loss when applying gradient descent.

1All experiments in this paper have been performed on a personal computer with 3.5 GHz Dual-Core Intel

Core i7 and macOS Big Sur system.

40

Thm 1Thm 2 for  and  Thm 2 only for 0204060Iteration101100101102Reproducing lossThm 1Thm 2 for  and  Thm 2 only for 0204060Iteration0102030405060Reproducing lossThm 1Thm 2 for  and  Thm 2 only for 050100Iteration101100101102Reproducing lossThm 1Thm 2 for  and  Thm 2 only for 03060Iteration0100200300400Reproducing lossG Further Discussion

In this section, we will provide further experiments and discussion on the performance of Safe PDP.

G.1 Comparison Between Safe PDP and PDP

In this part, we compare Safe PDP and non-safe PDP [71] to show the performance trade-offs between
the constraint enforcement of Safe PDP and its resulting computational expense. We use the example
of learning MPCs from expert demonstrations for the cartpole system (in Table 1) to show this, and
the experiment settings are the same with Appendix F.3. The comparison results between Safe PDP
and PDP are given in the following Table S1.

Table S1: Performance comparison between Safe PDP and PDP

Methods

Loss at convergence

Timing for
Forward Pass

Timing for
Backward Pass

Learning
constraints?

Constraint
Guaranteed?

PDP
Safe PDP

524.02
7.42

0.10s
0.21s

0.046s
0.042s

No
Yes

No
Yes

Based on the results in Table S1, we have the following comments and analysis.

(1) We note that Safe PDP achieves lower training loss. This is because compared to PDP, Safe PDP
has introduced the inductive bias of constraints within its model architecture, making it more suited
to learn from demonstrations which are the results of a constrained control system (expert). In this
sense, Safe PDP architecture (with an inductive bias of constraints) can be thought of as having more
expressive power than PDP architecture for the above experiments.

(2) For Safe PDP, its ability to learn and guarantee constraints comes at the cost of lower computational
efﬁciency in the forward pass, as shown in the second column in Table S1. Even though Safe PDP
handles constraint enforcement by adding them to the control cost using barrier functions, solving
the resulting unconstrained approximation still needs more time than solving the unconstrained PDP.
This could be because the added log barrier terms can increase the complex/stiff curvature of the
cost/loss landscape, thus taking longer to ﬁnd the minimizer. Further discussion about how barrier
parameter inﬂuences the computational efﬁciency of the forward pass will be given in Appendix G.3.

(3) The running time for the backward pass is almost the same for both PDP and Safe PDP because
both methods are solving an unconstrained LQR problem (auxiliary control system) of the same size
(see Theorem 2), which can be very efﬁcient based on Riccati equation.

G.2 Strategies to Accelerate Forward Pass of Safe PDP

In the previous experiments in Appendix F, we have used an NLP solver to solve the trajectory
optimization (optimal control) in the forward pass. Since the solver blindly treats an optimal control
problem as a general non-linear program without leveraging the (sparse) structures in an optimal
control problem. Thus, solving the long-horizon trajectory optimization is not very efﬁcient. To
accelerate long-horizon trajectory optimization, one can use plenty of strategies, as described below.

• To solve a long-horizon optimal control problem, one effective method is to scale a (continu-
ous) long time horizon into a smaller one (like a unit) by applying a time-warping function to
the system dynamics and control cost [85]. After discretizing and solving this short-horizon
optimal control problem, re-scale the obtained optimal trajectory back. This time-scaling
strategy is common in many commercial optimal control solvers, such as GPOPS [94].

• There are also the ‘warm-up’ tricks to accelerate the trajectory optimization in the forward
pass of Safe PDP. For example, one can initialize the trajectory at the next iteration using
the result of the previous iteration.

• One also can use a coarse-to-ﬁne hierarchical strategy to solve long-horizon trajectory
optimization. For example, given a long-time horizon optimal control system, ﬁrst, discretize
the trajectory with larger granularity and solve for a coarse-resolution optimal trajectory;

41

then use the coarse trajectory as initial conditions to solve the trajectory optimization with
ﬁne granular discretization.

As an additional experiment based on cartpole system (in Table 1), we tested and compared the above
three strategies for accelerating the forward pass of Safe PDP. The timing for each strategy is given
in the following Table S2. Here, tf is the continuous-time horizon of the cartpole system, ∆ is the
discretization interval, and the discrete-time horizon is T = tf /∆.

Table S2: Running time for different strategies in accelerating the forward pass of Safe PDP

Strategies

tf = 2s, ∆ = 0.1s,
T = tf /∆ = 20

tf = 6s, ∆ = 0.1s,
T = tf /∆ = 60

tf = 10s, ∆ = 0.1s,
T = tf /∆ = 100

tf = 20s, ∆ = 0.1s,
T = tf /∆ = 200

Plain NPL solver
Time scaling
Warm start
Hierarchical

0.082s
0.014s
0.055s
0.021s

0.202s
0.033s
0.095s
0.055s

0.491s
0.055s
0.108s
0.074s

1.743s
0.083s
0.224s
0.133s

From the results in Table S2, one can see that time-scaling is the most effective way among others
to accelerate long-horizon trajectory optimization. Of course, one can combine some of the above
strategies to further improve the running performance of the forward pass of Safe PDP.

Additionally, one can also use iLQR [81] and DDP [82] to solve optimal control problems. iLQR can
be viewed as the one-and-half-order method—linearizing dynamics and quadratizing cost function.
DDP is a second-order method — quadractizing both dynamics and cost function. Both methods
solve a local bellman equation to generate the update of the control sequence. But without coding
optimization, both methods are slower than the commercial NPL solver (e.g., CasADi [77]). Some
ongoing works are trying to take advantage of GPUs for accelerating trajectory optimization, which
is also our future research.

G.3 Trade-off Between Accuracy and Efﬁciency using Barrier Penalties

In the paper, we have provided both theoretical guarantees (see Theorem 2 and Theorem 3) and
empirical experiments (see Fig. 1, Fig. 2a and 2c, Fig. 3a and 3c, and Fig. 4) for the relationship
between the accuracy of a solution to an unconstrained approximation and the choice of the barrier
parameter. This subsection further investigates the trade-off between accuracy and computational
efﬁciency under different choices of the barrier parameter.

In the experiment below (based on the cartpole system in Table 1), by choosing different barrier
parameters γ in the forward pass of Safe PDP, we show the accuracy of the trajectory ξ(γ) solved
from an unconstrained approximation system Σ(γ) and the corresponding computation time. The
results are presented in Table S3.

Table S3: Accuracy of the trajectory ξ(γ) from the unconstrained approximation system Σ(γ) and
its computation time with different choices of barrier parameter γ

1

10−1

choice of γ
10−3

10−2

10−4

10−5

51.9% 12.2% 1.6% 0.18% 0.018% 0.0002%

Accuracy of ξ(γ) in percentage:

(cid:107)ξ(γ)−ξ∗(cid:107)2
(cid:107)ξ∗(cid:107)2

× 100% 1

Timing for computing ξ(γ)

0.033s
1 Note that in the above table, ξ∗ is the ground-truth solution obtained from solving the original constrained
trajectory optimization, and the computation time for such a constrained trajectory optimization is 0.062s.

0.035s

0.040s

0.023s

0.038s

0.047s

We have the following comments on the above results in Table S3.

• First, the results in the ﬁrst row of Table S3 show that a smaller barrier parameter leads to
higher accuracy of the approximation solution. This again conﬁrms the theoretical guarantee

42

in Theorem 2 (Claim (b)). The results here are also consistent with the ones in Fig. 1 in the
paper.

• Second, the second row of Table S3 shows that a smaller barrier parameter, however,
increases the computation time for solving the unconstrained approximation optimization.
This could be because using a small barrier parameter, the added barrier terms can increase
the complex/stiff curvature of the cost/loss landscape, thus taking Safe PDP longer to ﬁnd
the minimizer. Despite this, the time needed for ﬁnding a minimizer is still lower than
directly solving a constrained trajectory optimization in the above experiment.

• Third, if one still wants to further increase the computation efﬁciency of Safe PDP, we have
provided some strategies to achieve so, including "time scaling," "warm start," and "coarse-
to-ﬁne." Please check the Appendix G.2 for more detailed descriptions and corresponding
experiment results.

In summary, we have shown that higher accuracy of the unconstrained approximation solution can
be achieved using a smaller barrier parameter, while a smaller barrier parameter would increase the
computation time for ﬁnding the approximation solution. In practice, one would likely choose an
appropriate barrier parameter to balance the trade-off between accuracy and computational efﬁciency.
Also, there are multiple strategies available to increase the computational efﬁciency of Safe PDP, as
discussed in the Appendix G.2.

G.4 Learning MPCs from Non-Optimal Demonstrations

In the application of learning MPCs (including objective, dynamics, constraints), given non-optimal
demonstrations, Safe PDP can still learn an MPC such that the trajectory reproduced by the learned
MPC has the closest discrepancy to the given non-optimal demonstrations (e.g., when the task loss is
deﬁned as l2 norm between the reproduced trajectory and demonstrations). As an illustrative example,
the following Table S4 shows learning an MPC from a sub-optimal demonstration for the cartpole
system.

Table S4: Safe PDP for learning MPCs from non-optimal demonstrations

0

10

20

50

100

150

200

Number of iterations

loss with optimal demo

779.986

2.206

1.481

0.832

0.641

0.620

0.611

1000

0.232

loss with non-optimal demo

1126.820

18.975

17.771

15.602

13.690

12.469

11.620

10.923

As shown in S4, the only difference between learning from optimal and non-optimal demonstrations
is that the converged loss for the non-optimal demonstrations is relatively higher than for the optimal
ones. This is because, for non-optimal demonstrations, there might not necessarily exist an exact MPC
model in the parameterized model space which perfectly corresponds to the given demonstration.
In such a case, however, Safe PDP can still ﬁnd the best model in the parametrized model space
such that its reproduced trajectory has a minimal distance to the given non-optimal demonstrations.
For the extended research of the generalization ability of the learned MPCs from the non-optimal
demonstrations, please refer to [85].

G.5 Limitation of Safe PDP

Safe PDP requires a safe (feasible) initialization such that the log-barrier-based objectives (cost or
task) are well-deﬁned. While this requirement can be restrictive in some cases, we have the following
empirical experiences on how to provide safe initialization for different types of problems.

• In safe policy optimization, one could ﬁrst use supervised learning to learn a safe policy
from some safe trajectories/demonstrations (which could not necessarily be optimal). Then,
use the learned safe policy to initialize Safe PDP. We have used this strategy in our previous
experiments in Appendix F.1.

• In safe motion planning, one could arbitrarily provide a safe trajectory (not necessarily
optimal) to initialize Safe PDP. We have used this strategy in the previous experiments in
Appendix F.2.

43

• In learning MPCs from demonstrations (Appendix F.3), the goal includes learning constraint

models, and there is no such requirement.

Also, Safe PDP cannot apply to robust learning and control tasks. The goal of robust learning and
control concerns achieving or maintaining good performance (such as stability or optimality) in the
case of the worst disturbance or attacks to a system. Methods for handling those types of problems,
such as robust control and differential game, have been well-developed in both control and machine
learning communities. On the other hand, Safe PDP only focuses on guaranteeing the satisfaction of
inequality constraints throughout a learning or control process, and such constraints are deﬁned on
the system states and inputs.

44

