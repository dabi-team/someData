1
2
0
2

n
a
J

8

]
P
C
.
n
i
f
-
q
[

2
v
7
4
2
1
1
.
1
0
0
2
:
v
i
X
r
a

Deep combinatorial optimisation for optimal stopping time problems :
application to swing options pricing.

Thomas DESCHATRE ∗†

Joseph MIKAEL ‡§

January 11, 2021

Abstract

A new method for stochastic control based on neural networks and using randomisation of discrete random
variables is proposed and applied to optimal stopping time problems. The method models directly the policy
and does not need the derivation of a dynamic programming principle nor a backward stochastic diﬀerential
equation. Unlike continuous optimization where automatic diﬀerentiation is used directly, we propose a
likelihood ratio method for gradient computation. Numerical tests are done on the pricing of American and
swing options. The proposed algorithm succeeds in pricing high dimensional American and swing options in
a reasonable computation time, which is not possible with classical algorithms.

Mathematics Subject Classiﬁcation (2010). 91G60, 60G40, 90C27, 97R40.

Keywords. Optimal stopping, American option, Swing option, Combinatorial optimisation, Neural

network, Artiﬁcial intelligence.

1

Introduction

Motivation Optimal stopping problems are particularly important for risk management as they are
involved in the pricing of American options. American-style options are used not only by traditional asset
managers but also by energy companies to hedge “optimised assets” by ﬁnding optimal decisions to optimise
their P&L and ﬁnd their value. A common modelling of a power plant unit P&L is done using swing options
which are options allowing to exercise at most l ≥ 1 times the option with possibly a constraint on the delay
between two exercise dates (see Carmona and Touzi (2008) or Warin (2012)).

Formally, for T > 0, we are given a stochastic processes (Xt)t≥0 deﬁned on a probability space

(Ω, F, F = (Ft)t≥0, P) and we search for an increasing sequence of F stopping times τ = (τ1, τ2, . . . , τl)
maximizing the expectation of the objective function

  l

X

EP

i=1

!

f (τi, Xτi

)1τi≤T

.

Numerical methods to solve the optimal stopping problem when l = 1, f (x, t) = e−rtg(x) and X is

Markovian include:

• Dynamic programming equation: the option price P0 is computed using the following backward discrete

scheme over a grid t0 = 0 < t1 < . . . < tN = T :

PtN

Pti

= g(XT ),
= max(g(Xti

), e−r(ti+1−ti)EP(Pti+1 |Fti

)), i = 0, . . . , N − 1.

(1)

One then needs to perform regression to compute the conditional expectations, see Longstaﬀ and
Schwartz (2001) or Bouchard and Warin (2012).

∗EDF R&D & FiME, Laboratoire de Finance des Marchés de l’Energie
†thomas-t.deschatre@edf.fr
‡EDF R&D
§joseph.mikael@edf.fr

1

 
 
 
 
 
 
• Partial diﬀerential equation (PDE): a variational inequality derived from the Hamilton Jacobi Bellman

equation is given by

min(−(∂t + L)v + rv, v − g) = 0, v(x, T ) = g(x)
where L is the inﬁnitesimal generator of X (Shreve, 2004, Chapter 8, Section 3.3). A numerical scheme
can be applied to solve this PDE and ﬁnd the option value.

• Reﬂected Backward Stochastic Diﬀerential Equation (BSDE): the value of the American option is the

solution of the reﬂected BSDE El Karoui et al. (1997):

Yt = g(XT ) − r

Z T

t

Ysds −

Z T

t

ZsdWs + KT − Kt,

Yt ≥ g(Xt), 0 ≤ t ≤ T,
Z T

(Yt − g(Xt))dKt = 0.

0

Bouchard and Chassagneux (2008) provides a numerical scheme to solve these equations.

• Policy search: the decision rule or exercise region is parametrized by a vector and the parameters are
usually optimised by Monte Carlo methods as in reinforcement learning (Glasserman, 2013, Chapter 8,
Section 2); Andersen (1999); Garcıa (2003). The algorithm proposed in this paper is strongly related
to this class of method.

i=1 e−rτi g(Xτi

These approaches generalise well for l ≥ 1, see Carmona and Touzi (2008) for dynamic programming
principle or Bernhart et al. (2012) for the BSDE method. The non linear case where f is of the form
φ(Pl
)1τi≤T ) is studied by Trabelsi (2013). We refer to (Glasserman, 2013, Chapter 8) for
more exhaustive details on numerical methods for American option pricing. All these algorithms suﬀer from
the curse of dimensionality: the number of underlying is hardly above 5. However energy companies portfolio
may trade derivatives involving more that 4 commodities at one time (e.g. swing options indexed on C02,
natural gas, electricity, volume, fuel) and traditional numerical methods hardly provide good solutions in a
reasonable computing time.

Recently, neural network-based approaches have shown good results regarding stochastic control problems
and PDE numerical resolution in high dimension, see Han et al. (2017b); Sirignano and Spiliopoulos (2018);
Chan-Wai-Nam et al. (2019). In the following, one describes literature related to optimal stopping time
problems using neural networks. Kohler et al. (2010); Becker et al. (2020) use neural networks for regression
in the dynamic programming equation (1). Huré et al. (2018); Bachouch et al. (2018); Becker et al. (2019a)
also use the dynamic programming equation (1) but neural networks are used to parameterize the optimal
policy. Weights and bias of the neural network(s) minimise at each time step the right hand side of the
dynamic programming equation (1), going backward. The optimal decision consists in a continuous variable
(instead of a discrete one) taking value in (0, 1) modeled by a neural network. Han et al. (2017a); E et al.
(2017); Huré et al. (2019) use neural networks to solve BSDE’s. In Huré et al. (2019), the neural networks
parameterizing the solution and eventually its gradient minimise the L2 loss between the left hand-side
and the right hand side of the Euler discretisation of the BSDE, going backward from the terminal value.
Bachouch et al. (2018) and Huré et al. (2019) need to maximise one criteria by time step. The approaches
of Han et al. (2017a) and E et al. (2017) are quite diﬀerent: the neural network allows the parameterisation
of the initial value of the BSDE and the gradient at each time step, and it minimises the distance between
the terminal value obtained by the neural network and the terminal value of the BSDE, going forward.
American put options prices are computed in Huré et al. (2019) up to dimension 40 with 160 time steps.
Neural networks approaches have also been used in the context of swing options pricing in gas market in
Barrera-Esteve et al. (2006). The deﬁnition of swing options slightly diﬀers from ours as it considers a
continuous control: the option owner buys a certain amount of gas between a minimum and a maximum
quantity. It is however related to our problem as in continuous time, this option is bang-bang: it is optimal to
exercise at the minimum or the maximum level at each date, that is choosing between two actions. Barrera-
Esteve et al. (2006) directly models the policy by a neural network and optimises the objective function as
in Fécamp et al. (2020); Buehler et al. (2019).

Contrarily to Kohler et al. (2010); Huré et al. (2018); Bachouch et al. (2018); Han et al. (2017a); E
et al. (2017); Huré et al. (2019); Becker et al. (2020), the goal of this paper is to propose a reinforcement
learning algorithm to solve optimal multi-exercise (rather than one single) stopping time problems with
constraints on exercise times that does not need to derive a dynamic programming equation nor to ﬁnd an
equivalent BSDE of the problem. The only information needed is the dynamic of the state process X and
the objective function. This kind of algorithm is called policy gradient and is well known in the area of
reinforcement learning, see Sutton et al. (2000) for instance. Although continuous control approximation
with reinforcement learning shows good results, see Fécamp et al. (2020); Buehler et al. (2019) for European-
style option hedging, the case of optimal stopping times is more diﬃcult as it involves controls taking values
in a discrete set of actions. The problem is similar to a combinatorial optimisation one: at each time step, an
action belonging to a ﬁnite set needs to be taken. One way to solve this problem is to perform a relaxation
assuming that the control belongs to a continuous space. For instance, if one needs to price an American

2

option, a decision represented by a value in {0, 1} and consisting in exercising or not must be taken. Relaxing
the problem consists in searching for solutions in [0, 1]: this relaxation has successfully been applied to a
Bermudan option pricing in a high dimensional setting (up to 1000) in Becker et al. (2019b). These methods
apply well for American-style option pricing but seem to be not ﬂexible enough to be extended to swing
options pricing.

Main results Our approach follows the spirit of Fécamp et al. (2020) and Becker et al. (2019b): one
directly parameterises the optimal policy by a neural network and maximises the objective function moving
forward. We propose an algorithm using reinforcement learning in order to solve optimal stopping times
problem seen as an combinatorial optimisation problem. Note that solving combinatorial optimisation
problems with neural networks have been considered in Bello et al. (2016) in a deterministic framework
without dynamics on the state process. The stochastic optimization framework considered in this paper is
described in Section 2.

Neural network hardly handles integer outputs which is the main diﬃculty of the problem addressed
in this paper. To encompass this problem, the ﬁrst step of the algorithm consists in randomizing the
optimization variables (that is executing the option or not) and modelling their law by a neural network.
The second step consists in computing the gradient of the objective function. It can not be computed as
usual by automatic diﬀerentiation as the neural network does not output the optimization variables but their
law. The use of likelihood ratio method allows to rewrite the gradient as a function of the neural network
output gradient that can be computed with automatic diﬀerentiation. The algorithm is given in Section 3.
Compared to the papers referenced above our approach allows to solve stopping time problems without any
knowledge of the dynamic programming equation or of an equivalent BSDE. Furthermore, it presents many
advantages as it

• can solve multiple optimal stopping time problems;

• allows to add in a ﬂexible way any constraint on the stopping times;

• can then be associated with the one of Fécamp et al. (2020) considering continuous actions in order to
solve stochastic impulse control problems, combining discrete and continuous controls (see (Øksendal
and Sulem, 2005, Chapter 6) for more information on impulse control problems).

Let us also notice that our method does not take advantage of the linearity (possible inversion between the
sum and the expectation) of the considered optimal stopping problems contrarily to Becker et al. (2019b)
and can be applied to non linear problem, making it suitable for impulse control problems. The theoretical
convergence study of our algorithm is out of the scope of this paper.

Numerical tests covering Bermudan and swing options are proposed in Section 4 and show good results
in the pricing of 10 underlyings Bermudan option and also on 5 underlyings swing options having up to l = 6
exercise dates. However, our algorithm gives suboptimal results on one of the considered case.

2 Optimal stopping

2.1 Continuous time modelling

We are given a ﬁnancial market operating in continuous time. Let (Ω, F = (Ft)t≥0, F, P) a ﬁltered probability
space and W a d-dimensional F-Brownian motion. One assumes that F satisﬁes the usual conditions of right
continuity and completeness. Let T > 0 a ﬁnite horizon time and X = (X1, X2, . . . , Xd) be the unique
strong solution of the Stochastic Diﬀerential Equation (SDE):

Xt = X0 +

Z t

0

µ(s, Xs)ds +

Z t

0

σ(s, Xs)dWs, t ∈ [0, T ] ,

(2)

with µ : [0, T ] × Rd 7→ Rd and σ : [0, T ] × Rd 7→ Rd×d two measurable functions verifying |µ(t, x) − µ(t, y)| +
kσ(t, x) − σ(t, y)k ≤ K1|x − y| and |µ(t, x)| + kσ(t, x)k ≤ K2(1 + |x|) for x, y ∈ Rd and t ∈ [0, T ] (| · | denotes
the Euclidian distance in Rd and for a matrix A ∈ Rd×d, kAk = ptr(AA>)) and K1, K2 ∈ R. Using the
notations of Carmona and Touzi (2008) and with X as deﬁned in (2) for t ∈ [0, T ] and Xt = XT for t ≥ T ,
an optimal stopping time problem consists in solving the problem

EP

sup
τ ∈Sl

  l

X

i=1

!

f (τi, Xτi

)1τi≤T

(3)

where S l is the collection of all vectors of increasing stopping times τ = (τ1, . . . , τl) such that for all
i = 2, . . . , l, τi − τi−1 ≥ γ a.s. on the set of events {τi−1 ≤ T } and where f : [0, T ] × Rd 7→ R is a measurable
function. l ∈ N∗ = N \ {0} corresponds to the number of possible exercises and γ ≥ 0 to the minimum delay
between two exercise dates. One wants to ﬁnd the optimal value (3) but also the optimal policy

!

f (τi, Xτi

)1τi≤T

.

τ ∗ ∈ argmax

EP

τ ∈Sl

  l

X

i=1

3

(4)

2.2 Discrete time modelling

In practice, one only considers optimal stopping on a discrete time grid (for instance, the valuation of a
Bermudan option is used as a proxy of the American option). Let us consider N + 1 exercise dates belonging
to a discrete set DN = {t0 = 0 < t1 < . . . < tN = T }, N ∈ N∗. The problem consists in ﬁnding

  l

X

i=1

EP

sup
τ ∈Sl
N

!

f (τi, Xτi

)1τi≤T

(5)

where Sl
is the set of stopping times belonging to Sl such that τi ∈ DN on {τi ≤ T }, for i = 1, . . . , l. This
N
discretisation is needed for our algorithm as it is needed in classical methods such as Longstaﬀ and Schwartz
(2001). Problem (5) is equivalent to the following:

EP

sup
Y

  N
X

i=0

!

Yif (ti, Xti

)

(6)

where (Yi)i=0,...,N is a sequence of (Fti
that

)i=0,...,N -measurable random variables taking values in {0, 1} such

and

with

N
X

i=0

Yi ≤ l

Dj ≥ γ, j = 0, . . . , N,

Dj = γ + tj −

j−1
X

i=0

YiDi

(7)

(8)

(9)

the delay at tj from the last exercise assuming that we can exercise at time 0 (hence the γ term in (9) allowing
to start at time 0 with a delay γ) and with the convention P−1
i )i=0,...,N of
Problem (6), a proxy for the optimal control (4) is given by

i=0 · = 0. Given a solution (Y ∗

τ ∗
k = tm(k)1k≤m + ∞1k>m, k ∈ {1, . . . , l},

on the event {PN

i=0 Yi = m} with m ≤ l and m(k) = min{j ∈ {0, . . . , N }| Pj

i=0 Y ∗

i ≥ k}.

3 Algorithm description

3.1 Neural network parametrization

As the Yi’s are discrete we cannot assume that they are the output of a neural network which weights are
optimised by applying a stochastic gradient descent (SGD). To overcome this diﬃculty, one can randomize
Y and consider that at each time step tj, j ∈ {1, . . . , N }, the discrete variable Yj is a Bernoulli distributed
if j = 0). The Bernoulli distribution
random variable conditionally on Ftj ∪ σ(Yi, i ≤ j − 1) (and Ft0
parameter depends on the state variable of our control problem. This state variable, denoted Stj

, is

• Xtj
• (X 1

in the case of a Bermudan option, that is with only one execution date,

tj , . . . , X d

tj , Pj−1

i=0 Yi, Dj) when there are constraints (7) and (8).

i=0 Yi)).

tj , . . . , X d

tj , Pj−1

Of course, one can adapt the state depending on the constraints (for instance if there is no delay constraint
In a non Markovian framework, one could consider that the
(8), the state is (X 1
probability for Yj to be equal to 1 is a function of all the values of Xti
for i ≤ j and Yi for i ≤ j − 1. In
this case, one could use a Recurrent Neural Network to parameterise this function but we do not consider
) is parameterised by a neural
this case here. The Bernoulli distribution parameter, which is P(Yj = 1|Stj
network NN deﬁned on [0, T ] × S × Θ and taking values in R where S is the state space and Θ represents the
sets in which the biases and weights of the neural network lie. The neural network architecture is described
in Section 3.3. The parametrization is then the following:

P(Yj = 1|Stj

) = expit (cid:0)C tanh (cid:0)NN(tj, Stj , θ)(cid:1)(cid:1) c(Stj

), j = 0, . . . , N,

(10)

with expit : R 7→ (0, 1) and expit(x) =
) is equal to 0 if the constraints are
saturated and 1 otherwise. Typically, if there are no constraints, c is always equal to 1, and if there are
constraints (7) and (8),

for x ∈ R and c(Stj

1
1+e−x

c(Stj

) = 1Pj−1

i=0

1Dj ≥γ.

Yi<l

Note that the methodology can be extended to any constraint on the policy. C tanh(NN(x, θ)) outputs the
logit (the inverse function of expit) of P(Yj = 1|Stj
) (when constraints are not saturated). The function
tanh is not necessary and one could only consider NN to parameterise the logit of the probability. To

4

reduce the values taken by the logit, we bound the output of the neural using tanh and choose C such that
expit(−C) ≈ 0 and expit(C) ≈ 1 ; C is given in Section 3.4.

From now on, P is replaced by Pθ to indicate the dependence of the law of Y with θ. At this step, we still
cannot train our neural networks by applying a stochastic gradient descent because of the Y ’s randomization.

3.2 Optimization

To approximate a solution to (6) we search for

θ∗ ∈ arg max

θ∈Θ

EPθ

!

Yif (ti, Xti

)

.

  N
X

i=0

Classical neural network parameter optimization consists ﬁrst in evaluating the objective function

using Monte Carlo method and replacing it by

EPθ

  N
X

i=0

!
)

Yif (ti, Xti

1
Nbatch

NbatchX

N
X

i f (ti, X m
Y m
ti

)

i=0
with Nbatch ∈ N∗ and where X m and Y m correspond to one realization of (X, Y ) simulated according to (2)
for X and to Pθ for Y . Secondly, a gradient descent is done to update the parameter θ using gradient of
the objective function which is computed using backpropagation. However in our case, it is not possible to
directly use backpropagation: Y is not a function of θ, but a discrete variable with law depending on θ.

m=1

To encompass this problem, we use a likelihood ratio method. Let us consider a random variable Z :
Ω 7→ E with probability measure Pa, a ∈ Rd, absolutely continuous with respect to a measure P. Let
la(x) = dPa
(Z) =
(x) be the likelihood function. We have, under some integrability conditions, ∇aEPa
dP
(Z∇a log(la(Z))). Using this method and iterative conditioning for probability computation, we ﬁnd
EPa
that the gradient of EPθ

is given by:

j=0 Yjf (tj, Xtj

(cid:16)PN

(cid:17)
)

∇θEPθ

  N
X

j=0

!

Yjf (tj, Xtj

)

=

EPθ

  N
X

j=0

Yjf (tj, Stj

)

N
X

i=0

Yi∇θ log (Pθ (Yi = 1|Sti

)) + (1 − Yi)∇θ log (1 − Pθ (Yi = 1|Sti

))

.

!

(11)

Pθ(Yi = 1|Sti
) which is deﬁned in Equation (10) is a continuous function of the neural network: the gradients
appearing in Equation (11) can be easily computed using backpropagation. Let us notice that the method
does not take any advantage of the fact that we can exchange the sum and the expectation in (6). It is then
suitable for optimization problems of the form

EP (g(Y1, t1, Xt1 , . . . , YN , tN , XtN

))

sup
Y

and could be combined with forward neural network continuous optimization algorithms such as those of
Fécamp et al. (2020); Buehler et al. (2019) to solve impulse control problems.

Every ∆test steps, the objective value is computed over the testing set. The parameters kept at the end
are the ones minimising those evaluations. The objective function is ﬁnally evaluated on a validation set.
While on the training phase actions are sampled from the outputted probability on the training set, they
are chosen equal to 1 if the probability is greater than 0.5 and 0 otherwise on the test and validation sets.
The algorithm is described in 1 with hyperparameters given in Section 3.4.

3.3 Neural network architecture

The neural network architecture is inspired by Chan-Wai-Nam et al. (2019) and consists in one single feed
forward neural network which features are the time step ti and the current state realisation Sti
. Let t ∈ [0, T ]
and x = (x1, . . . , xr)> ∈ S that we assume to be included in Rr. The neural network is deﬁned as follow

NN(t, x, θ) = AL+1 ◦ ρ ◦ AL ◦ ρ . . . ◦ A1((t, x1, . . . , xr)>)

where Al(y) = Wly + bl for l = 1, . . . , L + 1, W1 ∈ R(r+1)×m, Wl ∈ Rm×m for l = 2, . . . , L, WL+1 ∈ Rm×1,
bl ∈ Rm for l = 1, . . . , L, bl ∈ Rm for l = 1, . . . , L and bL+1 ∈ R. L corresponds to the number of layers and
m to the number of neurons per layer (that we assume to be the same for every layer). The (Wl)l=1,...,L+1
correspond to the weights and (bl)l=1,...,L+1 to the biases. The function ρ is the activation function and is
chosen as the ReLu function, that is ρ(x) = max(0, x). θ is then equal to (W1, . . . , WL+1, b1, . . . , bL+1).

5

Algorithm 1 Algorithm for optimal stopping. The lines in blue are the main diﬀerence with classical back-
propagation.

1: α : Learning rate
2: β1, β2 ∈ [0, 1] : Exponential decay rates for the moment estimates,
3: Niter : number of iterations
4: Nbatch : number of simulations at each gradient descent iteration (batch size)
5: θ0 randomly chosen
6: m0 ← 0
7: v0 ← 0
8: for iiter = 1 . . . NIter do
9:

for u = 0 . . . N do

10:
11:
12:
13:

14:

15:
16:

17:

18:

19:

Xu ← Nbatch samples simulations of Xtu
Su ← state value
pθiiter −1,u ← expit (C tanh (NN(tu, Su, θiiter−1))) c(Su)
Yu ← Nbatch of a Bernouilli r.v. with parameter pθiiter −1,u

(cid:16)PN

(cid:17) (cid:16)PN

Nbatch

u=0 Y n

PNbatch
n=1

u ∇θ log (cid:0)pθiiter −1,u
giiter ← 1
u f (tu, X n
u )
miiter ← miiter−1 + (1 − β1)giiter (update biased ﬁrst moment estimate)
viiter ← β2viiter−1 + (1 − β2)g2
ˆmiiter ← miiter
ˆviiter ← viiter
θiiter ← θiiter−1 − α ˆmiiter /(pˆviiter + (cid:15)) (update parameters)

1−(β1)iiter (computes bias-corrected ﬁrst moment estimate )
1−(β2)iiter (computes bias-corrected second raw moment estimate)

u=0 Y n

(update biased second raw moment estimate)

iiter

(cid:1) + (1 − Y n

u ) log (cid:0)1 − pθiiter −1,u

(cid:1)(cid:17)

3.4 Hyper parameters

• The batch size Nbatch as the number of iterations Niter depend on the use case and are speciﬁed
at a later stage. The training set size is then equal to Niter × Nbatch. As the likelihood ratio estimator
of the gradient has high variance, choosing a large batch size (>1000) allows for a better estimation.
The drawback is that it tends to slow down the algorithm. To reduce the variance, one could also use
a baseline function as in Zhao et al. (2011).

• The test set size is chosen equal to 500,000 and the validation set size to 4,096,000 (500,000
and 4,096,000 are chosen high to have very accurate optimisation). The test set is evaluated every
∆test = 100 steps.

• The number of layers is chosen equal to 3. The number of neurons per layer is constant (but can

vary from a case to another).

• The learning rate (α in Algorithm 1) is chosen equal to 0.001.
• As in Chan-Wai-Nam et al. (2019), in our case, regularisation which is classically used to avoid
overﬁtting is not relevant and we won’t use it as our data is not redundant and thus the network does
not experience overﬁtting.

• Since we use the same network at each time step, we use a mean-variance normalisation over all
the time steps to center all the inputs (ti, Xti
) for all ti’s with the same coeﬃcients. The scaling and
recentering coeﬃcients are estimated on 100,000 pre-simulated data that is just used to this end. The
mean and the standard deviation are ﬁrst computed for every time step over the simulations then
averaged over the time steps.

• We use Xavier initialisation Glorot and Bengio (2010) for the weights and a zero initialisation for the

biases.

• The parameter C that bounds the input of the expit function is chosen such that expit(−C) ≈ 0 and

expit(C) ≈ 1. We choose C = 10.

• The library used is Abadi et al. (2015) and the algorithm runs on a laptop with 8 cores of 2,50 GHz,

a RAM memory of 15,6 Go and without GPU acceleration.

4 Numerical results

In this section Algorithm 1 is applied to the valuation of Bermudan and swing options. The function f (s, x)
is of the form e−rsg(x) where g is the payoﬀ of the option and r ≥ 0 is the risk free rate. We place ourselves
in the Black-Scholes framework: µ(s, x) = (r − δ)x, with δ ≥ 0 corresponding to the dividend rate and
Σ> with Σ a positive deﬁnite matrix. We choose to work with a regular time grid ti = i
σ(s, x) = diag(x)
N
for i = 0, . . . , N . The probability measure corresponds to the risk neutral probability and ﬁnding the value
of the option consists in solving Problem (6).

√

6

4.1 Bermudan options

In this section, we assume that l = 1 (only one exercise) and we consider diﬀerent options to price.

Put option with d = 1, payoﬀ g(x) = (K − x)+, K = 1, S0 = 1, r = 0.05, δ = 0,
Σ = 0.2, N = 10,
T = 1. We consider a batch size equal to Nbatch = 5, 000, a neural network with a depth of 3 layers having
10 neurons each and Niter = 5, 000 iterations.

√

Max-call option with d ∈ {2, 10}, payoﬀ g(x) = (max((xi)i=1,...,d) − K)+, K = 100, Si
0 = 100,
i = 1, . . . , d, r = 0.05, δ = 0.1,
Σ = 0.2Id (Id is the identity matrix with size d × d), N = 9, T = 3. We
consider a batch size equal to Nbatch = 5, 000 for d = 2 and Nbatch = 12, 000 for d = 10, a neural network
with 3 layers of size 30 for d = 2 and 70 for d = 10 and Niter = 10, 000 iterations.

√

Strangle spread option with d = 5, payoﬀ g(x) = −(K1 − 1
5
( 1
5
r = 0.05, δ = 0,

i=1 xi − K4)+, K1 = 75, K2 = 90, K3 = 110, K4 = 125, Si

i=1 xi − K3)+ − ( 1

P5

P5

5

P5

i=1 xi)+ + (K2 − 1

P5
i=1 xi) +
0 = 100, i = 1, . . . , 5,

5

√

Σ =








0.3024
0.1354
0.0722
0.1367
0.1641

0.1354
0.2270
0.0613
0.1264
0.1610

0.0722
0.0613
0.0717
0.0884
0.0699

0.1367
0.1264
0.0884
0.2937
0.1394








0.1641
0.1610
0.0699
0.1394
0.2535

,

N = 48, T = 1. We consider a batch size equal to Nbatch = 8, 000, a neural networks with 3 layers size 60
and Niter = 10, 000 iterations.

Losses and times obtained with Algorithm 1 are given in Table 1 for each case and losses are compared
to a reference value (Bouchard and Chassagneux (2008) for the put option and Becker et al. (2019b) for the
other options). The algorithm succeeds in pricing Bermudan options with a high precision (relative error
<1%) in dimension up to 10 and number of time steps up to 50. The computing time is more sensitive to
the number of time steps than to the dimension: the number of neural network estimation is equal to the
number of time steps. The increase of computing time when dimension increases is mostly caused by a need
to increase the batch size and a more important simulation time. Algorithm 1 succeeds in pricing Bermudan
options and solves problems that are usually hard to solve and very expensive in terms of computation time
as they suﬀer from the curse of dimensionality. The training and testing learning curves are given in Figure
1. The testing errors are relatively stable for the put and the max-call options but less stable for the strangle
spread. For the put and the 2 dimensional max-call, the testing error converges quickly to the optimal value.
The testing error of the 10 dimensional max-call decreases more slowly. The diﬀerent training errors are all
noisy as they are of smaller size.

Once trained, the neural network allows to compute the probability to exercise according to the price
and time to maturity and the exercise region in a few seconds, see Figure 2 for the Bermudan put option.
The probability to exercise has a S-shape with limit values 0 and 1 (do not exercise and exercise) and a small
transition region between those two values. As expected, the ﬁrst value such that the probability becomes
0 increases when time to maturity decreases. This is conﬁrmed in the exercise region in Figure 1 with the
frontier decreasing with time to maturity. The exercise region is the one below the curve, which is computed
using the ﬁrst value such that the probability of exercise is below 0.5. The frontier is extrapolated from the
neural network trained only on a discrete time grid but that allows to use diﬀerent time to maturity values
(even ones not used in the training), which is not possible with classical backward optimisation.

Use case / Method Algorithm 1 Reference Diﬀerence Time (s)

Bermudan put
Max-call, d = 2
Max-call, d = 10
Strangle spread

0.0603
13.8787
38.0347
11.7681

0.0603
13.8990
38.2780
11.7940

0.06%
0.15%
0.64%
0.22%

393.9
1377.8
5968.7
12991.3

Table 1: Results obtained on diﬀerent Bermudan options pricing with Algorithm 1 with the relative diﬀerence
between Algorithm 1 and a reference value (given by Bouchard and Warin (2012) for the put option and by
Becker et al. (2019b) for the other options). The time in seconds corresponds to the time of training and
predicting.

7

(a) Put.

(b) Max-call, d = 2.

(c) Max-call, d = 10.

(d) Strangle spread.

Figure 1: Learning curves for the diﬀerent Bermudan options.

4.2 Swing options without delay

In this section, we consider a swing option without delay constraint. We compare in Table 2 the results
obtained by Algorithm 1 with the results of Ibáñez (2004) in the case of a put option with d = 1, g(x) =
(K − x)+, K = 40, S0 ∈ {35, 40, 45}, r = 0.0488, δ = 0, Σ = 0.25, N = 12, T = 0.25, l ∈ {1, 2, 3, 4, 5, 6}
and no delay. We consider a batch size equal to Nbatch = 2, 000, a neural networks with 3 layers size 10 and
Niter = 5, 000 iterations. Every case takes around 4 minutes to converge, see Table 3. The algorithm gives
very accurate results in a short period of time for the valuation of the swing options. As for the Bermudan
put option, it is possible to compute the probability of exercise and the exercise region for this swing option,
see Figure 3. Those quantities depend now on the remaining number of exercises. As expected, the ﬁrst
value such that the probability to exercise is 0 increases with the remaining number of exercises. The exercise
frontier should increase with the remaining number of exercises which is the case most of the time : the curve
for a remaining number of exercises equal to 6 goes below the ones with a remaining number of exercises
equal to 5 (resp. 4) when time to maturity is greater than 0.1 (resp. 0.15).

l / S0

35

40

45

1
2
3
4
5
6

(5.104, 5.114, 0.19%)
(10.165, 10.195, 0.29%)
(15.194, 15.23, 0.24%)
(20.188, 20.23, 0.21%)
(25.19, 25.2, 0.04%)
(30.156, 30.121, 0.12%)

(1.776, 1.774, 0.12%)
(3.492, 3.48, 0.34%)
(5.115, 5.111, 0.07%)
(6.658, 6.661, 0.05%)
(8.148, 8.124, 0.3%)
(9.494, 9.502, 0.09%)

(0.409, 0.411, 0.42%)
(0.772, 0.772, 0.06%)
(1.09, 1.089, 0.09%)
(1.358, 1.358, 0.0%)
(1.58, 1.582, 0.12%)
(1.764, 1.756, 0.44%)

Table 2: Comparison of results obtained by Algorithm 1 with the ones of Ibáñez (2004) for diﬀerent initial values
S0 and diﬀerent number of executions l. The ﬁrst value corresponds to the swing option value obtained with
Algorithm 1, the second value to the one in Ibáñez (2004) and the third value is the relative diﬀerence in %.

To assess the performance of Algorithm 1 in high dimension, let us consider the pricing of the geometrical
put option having payoﬀ g(x) = (K − Qd
,
0 = 401/5, r = 0.0488, δ = 4r
i=1 xi)+. Let d = 5, K = 40, Si
5
I5, N = 12, T = 0.25 and l ∈ {1, 2, 3, 4, 5, 6}. Prices dynamic parameters are chosen in order to
Σ = 0.25√
5
have an option value equal to the one dimensional case put option value: the product of the components
of X follows a Black-Scholes dynamic with drift parameter equal to 0.0488 and volatility equal to 0.25. It
allows to have a reference value (from Ibáñez (2004)) while considering a high dimensional case. We consider
a batch size equal to Nbatch = 8, 000, a neural networks with 3 layers of size 30 and Niter = 5, 000 iterations.

8

(a) Probability of exercise.

Figure 2: Probability of exercise and exercise region (region below the curve) for the Bermudan put option
computed from the trained neural network.

(b) Exercise region.

Results are given in Table 4. The algorithm succeeds in pricing this option with 5 underlyings in a reasonable
time (less than 30 minutes). By using a less costly hyperparameterization (20 neurons density instead of 30,
2, 000 iterations instead of 5, 000, Nbatch = 3, 000 instead of 8, 000 we are able to obtain results in less than
4 minutes with a 2% accuracy as shown in Table 5

9

l / S0

35

40

45

1
2
3
4
5
6

270.1
241.6
240.6
239.6
271.3
243.8

278.6
246.6
237.7
242.4
239.9
240.9

249.9
236.8
243.4
240.0
235.3
241.7

Table 3: Time in seconds for training and predicting with Algorithm 1 to price the swing put option for diﬀerent
initial values S0 and diﬀerent number of executions l.

Use case / Method Algorithm 1 Reference Diﬀerence Time (s)

l = 1
l = 2
l = 3
l = 4
l = 5
l = 6

1.767
3.478
5.100
6.639
8.117
9.478

1.774
3.480
5.111
6.661
8.124
9.502

0.39%
0.04%
0.22%
0.32%
0.09%
0.25%

1814.3
1686.6
1701.5
1661.0
1689.2
1675.7

Table 4: Comparison of results obtained by Algorithm 1 for the pricing of a 5 dimensional swing put option
with the reference values reported in Ibáñez (2004). The time in seconds corresponds to the time of training and
predicting.

Use case / Method Algorithm 1 Reference Diﬀerence Time (s)

l = 1
l = 2
l = 3
l = 4
l = 5
l = 6

1.733
3.435
5.074
6.591
8.033
9.392

1.774
3.480
5.111
6.661
8.124
9.502

2.33%
1.29%
0.71%
1.05%
1.13%
1.16%

221.9
222.0
212.2
199.4
197.0
203.0

Table 5: Comparison of results obtained by Algorithm 1 with suboptimal hyperparameters for the pricing of
a 5 dimensional swing put option with the reference values reported in Ibáñez (2004). The time in seconds
corresponds to the time of training and predicting.

4.3 Swing options with delay

Let us now consider the case of a put option with d = 1, g(x) = (K − x)+, K = 100, S0 = 100, r = 0.05,
δ = 0, Σ = 0.3, N = 50, T = 1, γ = 5 T
and l ∈ {1, 2, 3, 4, 5}. Delay constraint is now present and a higher
N
number of dates is considered. We consider a batch size equal to Nbatch = 5, 000, a neural networks with 3
layers of size 10 and Niter = 10, 000 iterations. We compare in Table 6 the results obtained with Algorithm
1 to the ones obtained with Carmona and Touzi (2008). The algorithm gives satisfying results but in this
situation we can see that the relative error increases with the number of exercises.

Use case / Method Algorithm 1 Reference Diﬀerence Time (s)

l = 1
l = 2
l = 3
l = 4
l = 5

9.844
19.093
27.827
36.058
43.638

9.85
19.26
28.80
38.48
48.32

0.06%
0.87%
3.38%
6.29%
9.69%

5430.5
6658.0
5518.4
5566.1
5472.1

Table 6: Comparison of results obtained by Algorithm 1 with the reference values reported in Carmona and
Touzi (2008). The time in seconds corresponds to the time of training and predicting.

10

(a) Probability of exercise at time t = 0.125.

Figure 3: Probability of exercise and exercise region (region below the curve) for the swing put option with strike
40 and maximum number of exercises 6.

(b) Exercise region.

5 Conclusion and perspectives

A stochastic control algorithm able to deal with (discrete) optimal stopping variables is presented. The
diﬀerent use cases show that the proposed algorithm is able to solve optimal stopping time problems in
a reasonable time, even when the dimension is high and also for multi-exercise. The algorithm is simple

11

and allows us to ﬁnd an optimal policy without any knowledge on the dynamic programming equation.
The method presented in this paper avoids a costly backward pass and only need a forward pass. While the
computation time increases a little with dimension, it increases a lot more with the number of time steps and
the algorithm can have troubles to converge. To conﬁrm all those results, one should study the theoretical
convergence of the algorithm. This algorithm could easily be extended to impulse control if combined with
Fécamp et al. (2020) in order to solve problems involving both continuous and discrete controls such as
hedging with ﬁxed transaction costs.

Acknowledgements. This research is supported by the department OSIRIS (Optimization, SImulation,

RIsk and Statistics for energy markets) of EDF Lab which is gratefully acknowledged.

References

M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,
M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser,
M. Kudlur, J. Levenberg, D. Mane, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens,
B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viégas, O. Vinyals,
P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-scale machine learning
on heterogeneous systems, 2015. Software available from tensorﬂow.org.

L. B. Andersen. A simple approach to the pricing of bermudan swaptions in the multi-factor libor market

model. Available at SSRN 155208, 1999.

A. Bachouch, C. Huré, N. Langrené, and H. Pham. Deep neural networks algorithms for stochastic control

problems on ﬁnite horizon, part 2: numerical applications. arXiv preprint arXiv:1812.05916, 2018.

C. Barrera-Esteve, F. Bergeret, C. Dossal, E. Gobet, A. Meziou, R. Munos, and D. Reboul-Salze. Numerical
methods for the pricing of swing options: a stochastic control approach. Methodology and computing in
applied probability, 8(4):517–540, 2006.

S. Becker, P. Cheridito, and A. Jentzen. Deep optimal stopping. Journal of Machine Learning Research, 20

(74):1–25, 2019a.

S. Becker, P. Cheridito, A. Jentzen, and T. Welti. Solving high-dimensional optimal stopping problems using

deep learning. arXiv preprint arXiv:1908.01602, 2019b.

S. Becker, P. Cheridito, and A. Jentzen. Pricing and hedging american-style options with deep learning.

Journal of Risk and Financial Management, 13(7):158, 2020.

I. Bello, H. Pham, Q. V. Le, M. Norouzi, and S. Bengio. Neural combinatorial optimization with reinforce-

ment learning. arXiv preprint arXiv:1611.09940, 2016.

M. Bernhart, H. Pham, P. Tankov, and X. Warin. Swing options valuation: A bsde with constrained jumps

approach. In Numerical methods in ﬁnance, pages 379–400. Springer, 2012.

B. Bouchard and J.-F. Chassagneux. Discrete-time approximation for continuously and discretely reﬂected

bsdes. Stochastic Processes and their Applications, 118(12):2269–2293, 2008.

B. Bouchard and X. Warin. Monte-carlo valuation of american options: facts and new algorithms to improve

existing methods. In Numerical methods in ﬁnance, pages 215–255. Springer, 2012.

H. Buehler, L. Gonon, J. Teichmann, and B. Wood. Deep hedging. Quantitative Finance, 19(8):1271–1291,

2019.

R. Carmona and N. Touzi. Optimal multiple stopping and valuation of swing options. Mathematical Finance:

An International Journal of Mathematics, Statistics and Financial Economics, 18(2):239–268, 2008.

Q. Chan-Wai-Nam, J. Mikael, and X. Warin. Machine learning for semi linear pdes. Journal of Scientiﬁc

Computing, Feb 2019.

W. E, J. Han, and A. Jentzen. Deep learning-based numerical methods for high-dimensional parabolic partial
diﬀerential equations and backward stochastic diﬀerential equations. Communications in Mathematics and
Statistics, 5(4):349–380, 2017.

N. El Karoui, C. Kapoudjian, É. Pardoux, S. Peng, M.-C. Quenez, et al. Reﬂected solutions of backward

sde’s, and related obstacle problems for pde’s. the Annals of Probability, 25(2):702–737, 1997.

S. Fécamp, J. Mikael, and X. Warin. Deep learning for discrete-time hedging in incomplete markets. Journal

of computational Finance, 2020.

12

D. Garcıa. Convergence and biases of monte carlo estimates of american option prices using a parametric

exercise rule. Journal of Economic Dynamics and Control, 27(10):1855–1879, 2003.

P. Glasserman. Monte Carlo methods in ﬁnancial engineering, volume 53. Springer Science & Business

Media, 2013.

X. Glorot and Y. Bengio. Understanding the diﬃculty of training deep feedforward neural networks. In
Proceedings of the thirteenth international conference on artiﬁcial intelligence and statistics, pages 249–
256, 2010.

J. Han, A. Jentzen, and E. Weinan. Overcoming the curse of dimensionality: Solving high-dimensional
partial diﬀerential equations using deep learning. arXiv preprint arXiv:1707.02568, pages 1–13, 2017a.

J. Han, A. Jentzen, and E. Weinan. Overcoming the curse of dimensionality: Solving high-dimensional

partial diﬀerential equations using deep learning. arXiv:1707.02568, 2017b.

C. Huré, H. Pham, A. Bachouch, and N. Langrené. Deep neural networks algorithms for stochastic control

problems on ﬁnite horizon, part i: convergence analysis. arXiv preprint arXiv:1812.04300, 2018.

C. Huré, H. Pham, and X. Warin. Some machine learning schemes for high-dimensional nonlinear pdes.

arXiv preprint arXiv:1902.01599, 2019.

A. Ibáñez. Valuation by simulation of contingent claims with multiple early exercise opportunities.
Mathematical Finance: An International Journal of Mathematics, Statistics and Financial Economics,
14(2):223–248, 2004.

M. Kohler, A. Krzyżak, and N. Todorovic. Pricing of high-dimensional american options by neural networks.
Mathematical Finance: An International Journal of Mathematics, Statistics and Financial Economics, 20
(3):383–410, 2010.

F. A. Longstaﬀ and E. S. Schwartz. Valuing american options by simulation: a simple least-squares approach.

The review of ﬁnancial studies, 14(1):113–147, 2001.

B. K. Øksendal and A. Sulem. Applied stochastic control of jump diﬀusions, volume 498. Springer, 2005.

S. E. Shreve. Stochastic calculus for ﬁnance II: Continuous-time models, volume 11. Springer Science &

Business Media, 2004.

J. Sirignano and K. Spiliopoulos. Dgm: A deep learning algorithm for solving partial diﬀerential equations.

Journal of computational physics, 375:1339–1364, 2018.

R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour. Policy gradient methods for reinforcement
learning with function approximation. In Advances in neural information processing systems, pages 1057–
1063, 2000.

F. Trabelsi. Study of undiscounted non-linear optimal multiple stopping problems on unbounded intervals.

International Journal of Mathematics in Operational Research, 5(2):225–254, 2013.

X. Warin. Gas storage hedging. In Numerical methods in ﬁnance, pages 421–445. Springer, 2012.

T. Zhao, H. Hachiya, G. Niu, and M. Sugiyama. Analysis and improvement of policy gradient estimation.

In Advances in Neural Information Processing Systems, pages 262–270, 2011.

13

