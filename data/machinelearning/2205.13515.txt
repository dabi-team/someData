2
2
0
2

t
c
O
4
1

]

V
C
.
s
c
[

2
v
5
1
5
3
1
.
5
0
2
2
:
v
i
X
r
a

Green Hierarchical Vision Transformer
for Masked Image Modeling

Lang Huang1, Shan You2∗, Mingkai Zheng3, Fei Wang2, Chen Qian2, Toshihiko Yamasaki1
1The University of Tokyo; 2SenseTime Research; 3The University of Sydney
{langhuang,yamasaki}@cvm.t.u-tokyo.ac.jp
{youshan,wangfei,qianchen}@sensetime.com

Abstract

We present an efﬁcient approach for Masked Image Modeling (MIM) with hi-
erarchical Vision Transformers (ViTs), allowing the hierarchical ViTs to discard
masked patches and operate only on the visible ones. Our approach consists of three
key designs. First, for window attention, we propose a Group Window Attention
scheme following the Divide-and-Conquer strategy. To mitigate the quadratic com-
plexity of the self-attention w.r.t. the number of patches, group attention encourages
a uniform partition that visible patches within each local window of arbitrary size
can be grouped with equal size, where masked self-attention is then performed
within each group. Second, we further improve the grouping strategy via the
Dynamic Programming algorithm to minimize the overall computation cost of the
attention on the grouped patches. Third, as for the convolution layers, we convert
them to the Sparse Convolution [25, 13] that works seamlessly with the sparse data,
i.e., the visible patches in MIM. As a result, MIM can now work on most, if not all,
hierarchical ViTs in a green and efﬁcient way. For example, we can train the hierar-
chical ViTs, e.g., Swin Transformer [49] and Twins Transformer [14], about 2.7×
faster and reduce the GPU memory usage by 70%, while still enjoying competitive
performance on ImageNet classiﬁcation and the superiority on downstream COCO
object detection benchmarks.†

1

Introduction

Driven by the great success of Masked Language Modeling (MLM) [56, 57, 15, 6] in natural language
processing (NLP) and the advancement of Vision Transformers (ViTs) [17, 49, 66, 77], Masked
Image Modeling (MIM) emerged as a promising self-supervised pre-training paradigm for computer
vision (CV). MIM learns representations from unlabelled data by masked prediction, e.g., predicting
the discrete tokens [3], the latent features [81, 68, 2], or the raw pixels [28, 73] of the randomly
masked input image patches. Among them, the representative work Masked Autoencoder (MAE) [28]
exhibited competitive performance as well as impressive efﬁciency. In essence, MAE proposed
an asymmetric encoder-decoder architecture for MIM, where the encoder (e.g., a standard ViT
model [17]) operates only on visible patches, and the light-weight decoder recovers all patches for
mask prediction.

On the one hand, the asymmetric encoder-decoder architecture signiﬁcantly reduces the computation
burden of pre-training. On the other hand, MAE only supports the isotropic ViT [17] architecture as
the encoder, while most of the modern vision models adopt hierarchical structure [43, 31, 49], in part
due to the need of handling the scale-variations of visual elements. In fact, the hierarchical structure
and local inductive bias are crucial in various CV tasks that require representations of different levels

∗Corresponding author.

†Code and pre-trained models: https://github.com/LayneH/GreenMIM.

36th Conference on Neural Information Processing Systems (NeurIPS 2022).

 
 
 
 
 
 
Figure 1: Comparison with SimMIM in terms of efﬁciency. All methods use a Swin-B/Swin-L
backbone and batch size of 2,048. The experiments of our method are conducted on a single machine
with eight V100 GPUs, CUDA 10, and PyTorch 1.8, while those of SimMIM require 2 or 4 machines.

or scales to make predictions, including image classiﬁcation [31], and object detection [22]. Yet
it is still not straightforward how the hierarchical vision transformers, e.g.Swin Transformer [49],
can be integrated into the MAE framework. Moreover, though the work SimMIM [73] has explored
Swin Transformer for MIM, it operates on both visible and masked patches and suffers from heavy
computation costs compared with MAE. As a concrete example, we ﬁnd that even the base size model
of SimMIM cannot be trained on a single machine with eight 32GB GPUs, let alone the larger ones.
The computation burden makes it difﬁcult for a wider range of researchers to dive into this ﬁeld of
research, not even to mention the amount of carbon emission during the model development process.

To this end, we strive to devise a new and green approach for MIM with hierarchical models, in
the spirit of Green AI [61, 74]. Our work focuses on extending the asymmetric encoder-decoder
architecture of MAE to hierarchical vision transformers, particularly the representative model Swin
Transformer [49], for the sake of efﬁcient pre-training on visible patches only. We identify that
the major obstacle is the inductive bias of hierarchical ViTs, i.e., the locality induced by i) window
attention [36, 49, 77] with non-overlapped window partition, and ii) convolution/pooling [66, 19, 14,
27] with overlapped window partition. These operators are incompatible with random masking as it
creates various-sized local windows that are infeasible for computing in parallel.

This paper provides the ﬁrst attempt to address this drawback. We present a Green Hierarchical Vision
Transformer for Masked Image Modeling that advocates a more practical method with drastically
improved efﬁciency. Our methodology is conceptually simple and consists of three key designs.

1. Guided by the Divide-and-Conquer principle, we present a Group Window Attention scheme
ﬁrst partitioning the local windows with uneven numbers of visible patches into several
equal-sized groups and then applying the masked attention within each group.

2. We formulate the aforementioned group partition as a constrained optimization problem,
where the objective is to ﬁnd a group partition that minimizes the computation cost of the
attention on the grouped tokens. Inspired by the concept of Dynamic Programming [5] and
the greedy principle, we propose an Optimal Grouping algorithm that adaptively selects the
optimal group size and partitions the local windows into a minimum number of groups.

3. We convert the convolution layers in the hierarchical ViTs to the Sparse Convolution [25, 13,
1], which was originally designed for handling sparse point cloud data and works seamlessly
with the masked inputs in MIM.

Our methodology is generic and does not make any modiﬁcation to the architecture of the backbone
models, such that we can make apple-to-apple comparisons with the baseline operating on both
visible and masked patches. In our experimental evaluations, we observed that our method requires
substantially less training time and consumes much less GPU memory while performing on par with
the baseline on both ImageNet1K [60] classiﬁcation and MS-COCO [47] object detection. Concretely,
using Twins-L [14]/Swin-B [49]/Swin-L [49], our method achieves up to 2.7× speedup and consumes
as few as 30% of GPU memory compared with the baseline SimMIM at the pre-training stage, while
achieving 83.9%/83.8%/85.1% top-1 ﬁne-tuning accuracy on the ImageNet-1K that is on par with
those of SimMIM.

2

121.6233.6284.8727.0398.31123020040060080010001200Swin-BSwin-LGPU MEMORY (GB)Ours (224)SimMIM (192)SimMIM (224)1.11.32.03.52.85.60123456Swin-BSwin-LGPU HOURS / EPOCHOurs (224)SimMIM (192)SimMIM (224)83.885.184.085.484.180818283848586Swin-BSwin-LIN-1K ACCURACY (%)Ours (224)SimMIM (192)SimMIM (224)2 Related Works

Self-Supervised Learning. Representation learning is a long-standing and fundamental question
in CV. For a long time, representation learning had been dominated by supervised learning. Until
recent three years, self-supervised learning (SSL) exhibited impressive performance and attained
signiﬁcant attention. Generally, SSL solves a proxy task without the actual interest to learn good
representations. According to the proxy tasks, SSL methods can be categorized into generative
approaches and discriminative approaches. Generative approaches predict the original data based
on the partially observed inputs [65, 55, 44], predict the transformation applied to the input [52, 21],
or model pixels in the input space [42, 23, 39, 33]. Masked Image Modeling also falls into this
category. Discriminative approaches received more interest during the past few years, especially
the contrastive learning methods. Contrastive learning creates multiple views of images with a
set of random data distortions and encourages the representations to be invariant to the distortions.
A large number of contrastive learning approaches [70, 53, 29, 10, 79, 80] drive the training by
maximizing the similarities between positive samples (i.e., views from the same image) while
minimizing those between the negative samples, and some works simply get rid of the negative
pairs [26, 7, 11, 37, 78, 8]. Beyond contrastive learning on global features, several methods proposed
to maintain the spatial information of representations and use region/mask/pixel-level contrastive
learning [67, 72, 71, 32, 35].

Masked Language/Image Modeling. Self-supervised pre-training has revolutionized the NLP.
Among them, the Masked Language Modeling (MLM) proposed in BERT [15] and its variants [6] are
the most dominant methods, which learn representations by predicting the tokens that are randomly
masked in the input sentence. Masked Image Modeling has a similar idea of predicting corrupted
images, and some of these methods [65, 55] were proposed Even preceded to BERT. These methods
were, however, unable to perform on par with other pre-training paradigms at that time. Until recently,
aided by the signiﬁcant advancement of Vision Transformers [17], several MIM methods presented
promising results [17, 3, 28, 73, 68] and became the state-of-the-art of self-supervised learning
in CV. These methods can be roughly differentiated in terms of the prediction target, e.g., color
bins [17], discrete tokens [3, 16] from pre-trained VAEs [63, 58], raw pixels [28, 73], and handcrafted
features [68]. Among these approaches, MAE [28] exhibited competitive performance as well as
impressive efﬁciency as it discards the masked tokens and operates only on the visible ones.

Isotropic and Hierarchical Vision Transformers. The seminal work Vision Transformer
(ViT) [17] revolutionized the conventional view of images. ViT, and its variant [62], treat an image as
a sequence of patches and adopt a pure Transformer [64] backbone to model the patches for image
classiﬁcation, achieving an impressive performance even compared with the Convolutional Neural
Networks. Nevertheless, while the results of ViT are promising in classiﬁcation, its performance
on dense prediction tasks is less favorable, which is largely due to its low-resolution feature maps
inherited from its isotropic structure and the quadratic complexity of self-attention [64]. To this end,
a strand of works proposed hierarchical structure [66, 49, 19, 77], efﬁcient attention [36, 49, 14, 38],
and locality bias [18, 69, 27, 76] for ViTs, unleashing the potential of ViTs as general-purpose
vision backbones. Our work performs studies upon the representative hierarchical ViTs, Swin Trans-
former [49] and Twins Transformer [14] but generalizes to any other ViTs with window local attention
or convolution/pooling.

Green AI. Witnessing the exponential growth of computations of big AI models [15, 6, 48], the
concept of Green AI attains mounting attention in recent years [61, 74]. Rather than being merely
obsessed with accuracy, Green AI advocates making efﬁciency an important measure of AI models,
championing the greener approaches that are more inclusive to the research community. This work
follows the path of Green AI and presents a greener approach for MIM with hierarchical ViTs.

3 Approach

3.1 Preliminary

Notations. Let X ∈ RC×H×W denote the input feature where C, H, and W are the numbers of
channels, height, and width of X; M ∈ {0, 1}H×W denotes the (spatial) mask generated randomly
during training where 0 indicates a patch is invisible for the encoder, and vice versa.

3

Figure 2: Illustration of the Group Window Attention scheme. In Masked Image Modeling
(MIM), the input X, where different colors indicate the tokens belong to different local windows, is
randomly masked, producing (cid:98)X of which most tokens are invisible. Our Group Window Attention
ﬁrst performs an optimal grouping to group the visible tokens of the local windows into several
equal-sized groups, forming X. Finally, we perform the Masked Attention within each group to
ensure no inter-window information leakage.

Masked Image Modeling. MIM learns representations by predicting the masked portion of a input
X from its partial observation (cid:98)X ← Mask(X, M). Existing MIM methods fall into two categories
regarding the Mask(·, ·) operation. Most methods [3, 73, 68] use the Hadamard product for masking
and retain the masked patches, i.e., (cid:98)X ← Mask(X, M) = X (cid:12) M with M broadcasted for C times
along the channel dimension. In sharp contrast to these methods, Masked Autoencoders (MAE) [28]
proposes throwing the masked patches at the masking stage, i.e.,

(cid:98)X ← Mask(X, M) = {Xi,j : Mi,j = 1}.
MAE designs an asymmetric and isotropic encoder-decoder architecture to take advantage of partial
the encoder operates only on the visible patches (cid:98)X without mask tokens; the decoder
inputs:
reconstructs the original images from representations of visible patches and masked tokens. This
design allows MAE to achieve competitive performance as well as impressive efﬁciency, e.g.,
3× training speedup compared with the ones operating on all patches. Nevertheless, the MAE
works only with the isotropic ViTs and it is unclear how to translate the efﬁciency of MAE to
hierarchical ViTs, which exhibited nearly unanimous superiority over the isotropic ones on most
vision tasks [66, 49, 19, 14, 77]. In this paper, we attempt to answer this question and propose a
much greener approach for MIM with Hierarchical ViTs.

(1)

3.2 Green Hierarchical Vision Transformer for Masked Image Modeling

Adapting existing hierarchical ViTs for MIM. A typical hierarchical ViT mainly consists of
Feed-Forward Networks (FFNs) and efﬁcient attentions, which fail to operate only on the visible
patches. We identify that the major obstacle is the inductive bias of hierarchical ViTs, i.e., the
locality induced by i) window attention [36, 49, 77] with non-overlapped window partition, and ii)
convolution/pooling [66, 19, 14, 27] in FFNs or attentions with overlapped window partition. These
operators are incompatible with random masking as it creates various-sized local windows that are
infeasible for computing in parallel. To this end, we present two key insights to make most, if not all,
hierarchical ViTs be able to operate only on visible patches in a green manner.

Group Window Attention. Regarding the non-overlapped window partition, we propose a Group
Window Attention scheme that signiﬁcantly improves the computation efﬁciency of window attention
on masked features. Given the masked feature (cid:98)X = Mask(X, M) following Equation (1), we collect
a set of uneven local windows (cid:98)X → { (cid:98)Xi}nw
i=0 of which each element contains the visible tokens only,
with sizes {wi}nw
i=0 accordingly. As shown in Figure 2, our Group Window Attention ﬁrst uses an
Optimal Grouping algorithm to partition the uneven windows into several equal-sized groups and
then performs Masked Attention within each group to avoid information leakage. In the next two
subsections, we will elaborate on these two components, respectively.

Incorporating with Sparse Convolution. Our intuition is the features of mask inputs can be
viewed as sparse tensors where only features of (a small number of) visible patches are retained while

4

MaskOptimal GroupingMasked Attention𝐗"𝐗#𝐗i=1; partition Π ← ∅; the number of group ng ← 0

i=1 wi do

i=1 to (cid:80)nw

Remaining windows Φ ← {wi}nw
repeat

Algorithm 1 Optimal Grouping
Require: The number of visible patches within each local window {wi}nw
i=0,
1: Minimum computational cost c∗ ← +∞
2: for gs = maxi{wi}nw
3:
4:
5:
6:
7:
8:
9:
10:
11:
end if
12:
13: end for
14: return Optimal group partition Π∗

πng ← Knapsack(gs, Φ), as in Equation (7)
Π ← Π ∪ πng ; Φ ← Φ \ πng
ng ← ng + 1

until Φ = ∅
c ← C(gs, Π), as in Equation (8)
if c < c∗ then

c∗ ← c; Π∗ ← Π

the others are omitted. From this perspective, we propose to directly replace all the convolution/pool-
ing operations in hierarchical ViTs with the highly optimized Sparse Convolution [24, 25, 13, 1],
originally designed for the sparse 3D point cloud data.

3.3 Optimal Grouping with Dynamic Programming

General formulation. The ﬁrst step of the optimal grouping is to ﬁnd an indexes partition Π with
respect to the group size gs:

Π = {πj}ng

j=1,

s.t. ∪j πj = {1, · · · ,

(cid:88)

wi},

i

(cid:88)

j

|πj| =

(cid:88)

i

wi, and ∀j

(cid:88)

k∈πj

wk ≤ gs.

(2)

(3)

where ng is the number of resulting groups. The conditions in Equation (3) constrain the partition to
contain all local windows with no duplicate and enforce the actual size of each group to be smaller
than gs. Based on the partition Π, we obtain a set of grouped tokens { ¯Xj}ng

j=1 that

Xj = Concat({ (cid:98)Xk : ∀k ∈ πj}), Xj ∈ RC×gs , 2
upon which the Masked Attention is performed. Finally, we apply the inverse operation of the
partition Π to recover the positions of output tokens.

(4)

With the formulation above, there remain two questions unresolved: 1) how to choose the optimal
group size g∗
s . To this end, we formulate our
objectives as the below min-min optimization problem,

s , and 2) how to obtain the optimal partition Π∗ given g∗

g∗
s = argmings C(gs, Partition(gs, {wi}i)),
Partition(gs, {wi}nw ) = argminΠ |Π|, s.t. Equation (3),
where C(·) is a cost function measuring the computation cost of the attention with the grouped tokens.
Intuitively, Equation (5) aims to ﬁnd the optimal group size g∗
s that the computation cost of the
optimal partition w.r.t. g∗
s is the minimum. Equation (6) searches for the optimal partition, with
the constraints of Equation (3). Having the optimal group size, we can directly obtain the optimal
partition Π∗ = Partition(g∗
s , {wi}nw ). Next, we will present in detail how we solve the above
optimization problem.

(6)

(5)

Group partition with Dynamic Programming. We ﬁnd that the optimization problem in Equa-
tions (6) and (3) is a special case of the multiple subset sum problem with identical capacities
(MSSP-I), a variant of the well-known 0-1 multiple knapsack problem with identical capacities
(MKP-I) [40, Chapter 10]. In our case, the group size is analogous to the capacity of knapsacks, the

2Here we assume the number of tokens (cid:80)

πj when |πj| is smaller than gs.

i wi is divisible by gs for simplicity. In practice, we pad the group

5

Figure 3: Illustration of the Masked Attention scheme. Given a group of tokens, we ﬁrst compute
their pairwise attention weights and then set the attention weights between tokens from different local
windows to −∞ (indicated by the gray cells). The ﬁnal attention output is then computed with the
masked attention map.

numbers of visible tokens {wi}nw are analogous to the values of goods, the weights of goods are
the same as their values, and the number of knapsacks is unbounded. Although MSSP-I is strictly
NP-complete, there exist multiple polynomial time approximation schemes for it, e.g., using the
dynamic programming (DP) algorithms [5]. Speciﬁcally, we make use of the DP algorithm for the
single knapsack problem (or the subset sum problem):

π ← Knapsack(gs, Φ),

(7)

which selects a subset π that (cid:80)
u∈π |u| ≤ gs out of the full set Φ (the pseudo-code of this algorithm
is given in Appendix). We alternatively apply this algorithm to the remaining full set Φ and exclude
the selected subset π from Φ, until Φ is empty. In practice, we found that our algorithm is very fast
and the time cost is negligible because the number of local windows is often small, e.g., less than 100
in our pre-training stage.

Cost function. Because we mainly care about the efﬁciency, we use the FLOPs to measure the
computation complexity of the multi-head attention on grouped tokens, i.e.,
s C) = ng × (4gsC 2 + 2g2

C(gs, Π) = |Π| × (4gsC 2 + 2g2

s C),

(8)

where C is the number of channels. Although the complexity is quadratic w.r.t. the group size
gs, using smaller gs might produce more groups (and more padding) and suffers from suboptimal
efﬁciency. Therefore the optimal group size is determined adaptively during training.

Putting everything together. We sweep over the possible values of group size, from maxi wi to
(cid:80)
i wi, to ﬁnd the optimal group size. For each selected group size, we ﬁrstly use the DP algorithm
in Equation (7) to partition the windows and then calculate the computation cost of the attention this
partition. The one with minimum cost is chosen as the optimal group size. The pseudo-code of the
optimal grouping is summarized in Algorithm 1.

3.4 Masked Attention

Because non-adjacent local windows are partitioned into the same groups, masking the attention
weights is needed to avoid the information exchange between these local windows. As illustrated in
Figure 3, having computed the attention map, we retain only the intra-window attention weights (i.e.,
the block-diagonal elements) and discard the inter-window ones. A similar masking scheme is also
applied to the retrieving of relative position bias [49], where we store the original absolute position of
each token and compute the relative positions on-the-ﬂy to retrieve the corresponding biases.

3.5 Batch-wise Random Masking

We observed that the per-sample random masking strategy would deteriorate the efﬁciency of our
method: 1) it might produce different numbers of groups of local windows for each sample, which
is intractable for the parallel computation of the Masked Attention; 2) when the mask patch size is
smaller than the largest patch size of the hierarchical models, some patches might contain both visible

6

QueryKeyValueMaskFigure 4: The optimal group size gs at each stage. The ﬁgure of the fourth stage is omitted here
because there is only one local window in this stage, so the grouping is not necessary. The simulation
is repeated 100 times, of which the mean and standard deviation (the shaded regions) are reported.

and masked inputs. In this case, we can not discard such patches during training and fail to fully
take advantage of the sparsiﬁcation. Therefore, we propose to set the mask patch size to the same
value as the largest patch size of the encoder (e.g., 32 for most hierarchical models, which is also the
default choice of [73]) and use the same random mask for all samples in the same GPU device (a.k.a.,
a micro-batch).

4 Experiments

4.1

Implementation Details and Experimental Setups

We conduct experiments on the ImageNet-1K [60] (BSD 3-Clause License) image classiﬁcation
dataset and MS-COCO [47] (CC BY 4.0 License) object detection/instance segmentation dataset. The
Swin-Base, Swin-Large [49] and Twins-Large [14] models, which consist of four stages with features
of stride 4/8/16/32, are used as the encoder throughout this paper, allowing for direct comparisons
with the baseline SimMIM [73] with Swin Transformer. The models are ﬁrst pre-trained on the
ImageNet-1K dataset without label and then ﬁne-tuned on downstream tasks. All the experiments
of our method are performed on a single machine with eight 32G Tesla V100 GPUs, CUDA 10.1,
PyTorch [54] 1.8, and automatic mixed-precision training [51]. For the experiments involving
convolution, we replace all the standard convolutions with the sparse convolution implemented in [1]
as discussed in Sec. 3.2.

Pre-training setup. We patchify images of size 224×224 with a patch size of 4×4 and randomly
mask the patches with a ratio r (r = 0.75 by default) following the scheme in Section 3.5. The input
images are ﬁrst transformed by a set of simple data augmentations, including random cropping and
horizontal ﬂip, and standardization. Following the prior work MAE [28], we adopt a lightweight
decoder that consists of nd (by default nd = 1) standard transformer blocks with an embedding
dimension of 512. The decoder takes the representations of visible patches and the mask token as
input and is appended after the ﬁnal stage of the encoder to learn representations for the masked
patches. It is followed by a linear layer to predict the normalized pixel values of the masked patches.
The models are trained for 100/200/400/800 epochs with a total batch size of 2,048. We use the
AdamW optimizer [41] with the cosine annealing schedule [50]. We set the base learning rate to
1.5e−4, the weight decay to 0.05, the hyper-parameters of Adam β1 = 0.9, β2 = 0.999, the number
of warmup epochs to 40 with an initial base learning rate 1.5e−7. The effective learning rate is scaled
linearly by batch_size/256.

Fine-tuning on the ImageNet-1K dataset. For ﬁne-tuning, we drop the decoder and directly
append a 1,000-way fully-connected layer to the average-pooled output of the encoder as the classiﬁer.
The models are also optimized by the AdamW optimizer [41] with 100 training epochs in total, 20
warmup epochs, a base/warmup learning rate of 1.25e−4/2.5e−7, the cosine annealing schedule [50],
a weight decay of 0.05. A layerwise learning rate decay [3] of 0.9/0.8/0.9, and a stochastic depth [34]
ratio of 0.2/0.3/0.2 are used for Swin-B/Swin-L/Twins-L respectively. The data augmentations are
the same as [3, 73].

Fine-tuning on the MS-COCO dataset. We adopt the Mask R-CNN [30] architecture with the
FPN [46] as the detector. All models are ﬁne-tuned on the MS-COCO [47] 2017 train split (∼118k
images) and ﬁnally evaluated on the val split (∼5k images). We use a batch size of 16, AdamW

7

50100150200250Group size gs0.60.81.01.2FLOPS1e8Stage150100150200Group size gs0.60.81.01.21.4FLOPS1e8Stage2203040Group size gs0.50.60.70.80.91.0FLOPS1e8Stage3Figure 5: Ablation studies of (a) the choice of the mask ratio r, (b) the number of transformer blocks
nd in the decoder, and (c) the number of pre-training epochs.

optimizer [41] with a learning rate of 1e−4, a weight decay of 0.05. The 1×/3× schedule in the
mmdetection [9] is adopted, which uses 12/36 training epochs in total and decays the learning rate at
the 3
12 of the total epochs by a factor of 10. The standard COCO metrics, including AP, AP50,
and AP75 for both object detection and instance segmentation are used for evaluation.

4 and 11

4.2 Ablations studies

Efﬁciency comparison with SimMIM. We compare the efﬁciency of our method to the baseline
SimMIM in Figure 1. The evaluations are performed on a single machine with eight 32GB V100
GPUs for our method and on 2 or 4 machines for the SimMIM because it fails to ﬁt into a single
machine with the default batch size of the original paper [73] (i.e., 2,048). As we can see from the
ﬁgures, the training of SimMIM with images of size 2242 is very slow and memory-hungry. Although
training with smaller images considerably reduces the training time and memory consumption, it
still far lags behind our method with images of size 2242. Concretely, with the same amount of
training epochs, our method performs on par with baseline with ∼2× speedup and ∼60% of memory
reduction using Swin-B. We also observe that the efﬁciency improvements become larger with the
larger Swin-L, e.g., 2.7× speedup compared with SimMIM192, highlighting the efﬁciency of our
method with larger models. In addition, the efﬁciency comparison using Twins Transformer is
presented in Appendix A.1 for completeness, which exhibits similar superiority of our approach.
The optimal group size gs on each stage. Because hierarchical models have multiple stages with
different scales of features, the optimal group size of each stage might also be different. Regarding
this, we design a simulation experiment to analyze the optimal gs at different stages. In the simulation,
we randomly generate 100 masks following Section 3.5, compute the costs w.r.t. different choices
of gs, and report the mean/standard deviation of the costs in Figure 4. Note that the analysis of the
4th stage is omitted here because it only has one local window. In general, we observed that the cost
increases quadratically w.r.t. the group size, except for some cases where the group size is exactly
equal to the sum of a subset of local windows. Another intriguing observation is that the cost at each
stage seems to be the minimum around the point that gs = 49, which is equal to the window size of
the window attention. This observation indicates that we may not need to sweep over all possible
group sizes but simply set gs = p × p in practice. Moreover, we analyze the inﬂuence of the optimal
grouping and the cost of each part of it in Appendix A.2, showing that optimal grouping is indeed
crucial for the efﬁciency of our method.

The inﬂuence of the mask ratio, the number of decoder blocks, and the pre-training epochs.
From Figure 5(a), we can see that the performance of our method is quite stable with the mask ratio
r varying from 0.5 to 0.85, which conforms with the observation of [28]. In Figure 5(b), we also
study the inﬂuence of the depth of the decoder. Intriguingly, the results suggest that fewer decoder
blocks produce better results. This study favors the simple prediction head design of SimMIM [73]
with hierarchical models and is in contrast to the observation of MAE [28] with isotropic ones. For
simplicity and efﬁciency, we ﬁx r = 0.75 and the number of decoder blocks to 1 throughout the paper.
Furthermore, we study the impact of pre-training budgets on our method. As shown in Figure 5(c),
the ﬁne-tuning accuracy increases steadily w.r.t. the number of training epochs and does not seem to
stagnate, suggesting its potential for a further performance boost.

Pre-training with larger window size. The work of [48] puts forth that using a larger window
size is beneﬁcial for ﬁne-tuning. In practice, however, it might be less practical because of the

8

0.50.60.750.85(a) Mask Ratio r83.083.183.2ImageNet Top1 Acc. (%)1238(b) The Number of Deocoder Blocks83.083.183.2ImageNet Top1 Acc. (%)100200400800(c) The Number of Pre-Training Epochs83.283.383.483.583.683.7ImageNet Top1 Acc. (%)Table 2: Top1 accuracy on the ImageNet-1K validation set with the Swin-B or ViT-B models. All
methods are trained with images of size 224 × 224 in both the pre-training and ﬁne-tuning except for
SimMIM192 using 192 × 192 in the pre-training.

Method

Model

#Params PT Ep. Ep. Hours Total Hours FT Ep. Acc. (%)

Training from scratch
Scratch, DeiT [62]
Scratch, MAE [28]
Scratch, Swin [49]
Scratch, Twins [14]

ViT-B
ViT-B
Swin-B
Twins-L

86M
86M
88M
99M

Supervised Pre-training
Supervised, SimMIM [73]
Supervised, SimMIM [73]

Swin-B
Swin-L

88M
197M

Pre-training with Contrastive Learning
MoCov3 [12]
DINO [8]

ViT-B
ViT-B

86M
86M

Pre-training with Masked Image Modeling
BEiT [3]
MaskFeat [68]
MAE [28]
SimMIM224 [73]
SimMIM192 [73]
SimMIM192 [73]
Ours
Ours
Ours

ViT-B
ViT-B
ViT-B
ViT-B
Swin-B
Swin-L
Swin-B
Twins-L
Swin-L

86M
86M
86M
86M
88M
197M
88M
99M
197M

0
0
0
0

300
300

800
800

800
800
1600
800
800
800
800
800
800

-
-
-
-

-
-

-
-

-
-
1.3
4.1
2.0
3.5
1.1
0.8
1.3

-
-
-
-

-
-

-
-

-
-
2069
3307
1609
2821
887
676
1067

300
300
300
300

100
100

100
100

100
100
100
100
100
100
100
100
100

81.8
82.3
83.5
83.7

83.3
83.5

83.2
82.8

83.2
84.0
83.6
83.8
84.0
85.4
83.8
83.9
85.1

quadratic complexity of self-attention w.r.t. the window size.
Fortunately, operating only on the visible patches permits the
training with a larger window size with little extra cost. As
displayed in Table 1, pre-training with doubled window size
only marginally increases the training time/GPU memory
by less than 10%/20% yet brings a moderate performance
improvement despite p = 7 in the ﬁne-tuning stage.

Table 1: A larger window size p × p.

p × p

Time Mem.

Acc.

7 × 7
14 × 14

1.1h
1.2h

121.6G 83.2%
148.9G 83.4%

4.3

ImageNet-1K Classiﬁcation

We ﬁne-tune the pre-trained models on the ImageNet-1K dataset and report the results on the
validation set in Table 2. Here, we make direct comparisons with the models 1) trained from scratch
with a longer training schedule, 2) trained with contrastive learning, and 3) trained with other MIM
approaches. Our approach achieves 83.8%/83.9%/85.1% top-1 ﬁne-tuning accuracy with the Swin-
B/Twins-L/Swin-L backbone, which is clearly superior to the supervised learning/contrastive learning
methods and on par with other MIM methods using backbones with similar capacities. The results
demonstrate the effectiveness of our method, in addition to the substantial efﬁciency improvements
over both MAE and SimMIM.

4.4 MS-COCO Object Detection and Instance Segmentation

Finally, we evaluate the transfer learning performance of our pre-trained models to the MS-COCO
object detection and instance segmentation dataset. Here, we directly use the code base of the super-
vised Swin Transformer without any modiﬁcation to the ﬁne-tuning strategy. For direct comparisons,
we reran the experiments for the supervised Swin-B and SimMIM using their public checkpoints.
The experiment results are summarized in Table 3. Compared with the supervised pre-trained Swin-B,
our approach performs prominently better in terms of all metrics, e.g., 1.5% absolute improvement in
APb. In addition, we also observed that our approach still performs comparably to the SimMIM on
dense prediction tasks. More signiﬁcantly, our approach outperforms most of the baselines from [45]
using 3× or 10× more ﬁne-tuning epochs and advanced data augmentations [20]. These experiments,

9

Table 3: MS-COCO object detection and instance segmentation. All methods are based on the
Mask R-CNN [30] architecture with the FPN [46] neck. The methods in gray are cited from [45].
Most of them use much longer training schedules and advanced data augmentations.

Method

Backbone PT Ep. PT Hours FT Ep. APb APb

50 APb

75 APm APm

50 APm
75

Training from scratch
Benchmarking [45]

ViT-B

0

0

400

48.9

-

-

43.6

-

-

Supervised Pretraining
Benchmarking [45]
PVT [66]
Swin [49]

ViT-B
PVT-L
Swin-B

Self-Supervised Pre-training
ViT-B
MoCov3 [12]
ViT-B
BEiT [3]
ViT-B
MAE [28]
ViT-B
MAE [28]
Swin-B
SimMIM [73]
Swin-B
Ours

300
300
300

800
800
1600
1600
800
800

992
-
840

-
-
2069
2069
1609
887

100
36
36

100
100
25
100
36
36

47.9
44.5
48.5

47.9
49.8
48.1
50.3
50.4
50.0

-
66.0
69.8

-
-
-
-
70.9
70.7

-
48.3
53.2

-
-
-
-
55.5
55.4

42.9
40.7
43.2

42.7
44.4
-
44.9
44.4
44.1

-
63.4
66.9

-
-
-
-
68.2
67.9

-
43.7
46.7

-
-
-
-
47.9
47.5

combined with the results in Table 2, verify that our approach can achieve outstanding performance
with impressive pre-training efﬁciency.

5 Conclusion

In this paper, we present a green approach for Masked Image Modeling (MIM) with hierarchical
Vision Transformers, e.g., Swin Transformer [49] and Twins Transformer [14], allowing the hi-
erarchical models to discard masked patches and operate only on the visible ones. Coupling the
efﬁcient Group Window Attention scheme, the DP-algorithm-based Optimal Grouping strategy, and
the Sparse Convolution, our approach can train the hierarchical models ∼2.7× faster and reduce
the GPU memory consumption by up to 70%, while still enjoying a competitive performance on
ImageNet classiﬁcation and the superiority of downstream MS-COCO object detection benchmarks.
We hope that this work will facilitate future self-supervised learning methods targeting effectiveness
as well as efﬁciency.

Limitations. One of the limitations of our algorithm is that it requires the batch-wise masking
scheme (as in Section 3.5) to achieve the best efﬁciency. Although this limitation only has little
impact on the MIM pre-training, it restrains the application of our method on a broader range of
settings, e.g., training ViTs with token spariﬁcation [59, 75] that requires instance-wise sparsiﬁcation.
These applications are beyond the scope of this work and we will leave them for future study.

Broader Impact. This work proposed a green approach for MIM with hierarchical ViTs, promi-
nently alleviating the heavy computation burden of MIM. On the one hand, this work provokes the
efﬁciency as well as the effectiveness of MIM, which may inspire new algorithms and investigations
in this direction. On the other hand, since the pre-training datasets might contain biases, our approach,
in the same way as other unsupervised/self-supervised learning methods, might also be susceptible to
replicating these biases. This concern can be mitigated by combining the FairML methods [4].

References

[1] spconv. https://github.com/traveller59/spconv.

[2] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli. Data2vec: A general framework for

self-supervised learning in speech, vision and language. arXiv preprint arXiv:2202.03555, 2022.

[3] H. Bao, L. Dong, and F. Wei. Beit: Bert pre-training of image transformers.

arXiv preprint

arXiv:2106.08254, 2021.

[4] S. Barocas, M. Hardt, and A. Narayanan. Fairness in machine learning. Nips tutorial, 1:2, 2017.

[5] R. Bellman. Dynamic programming. Science, 153(3731):34–37, 1966.

10

[6] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell, et al. Language models are few-shot learners. In Advances in Neural Information Processing
Systems, volume 33, 2020.

[7] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin. Unsupervised learning of visual
features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33, 2020.

[8] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in

self-supervised vision transformers. arXiv preprint arXiv:2104.14294, 2021.

[9] K. Chen, J. Wang, J. Pang, Y. Cao, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J. Xu, et al. Mmdetection:

Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155, 2019.

[10] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual

representations. In International Conference on Machine Learning, 2020.

[11] X. Chen and K. He. Exploring simple Siamese representation learning. arXiv preprint arXiv:2011.10566,

2020.

[12] X. Chen, S. Xie, and K. He. An empirical study of training self-supervised vision transformers. In

Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9640–9649, 2021.

[13] C. Choy, J. Gwak, and S. Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural
networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 3075–3084, 2019.

[14] X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C. Shen. Twins: Revisiting the design of

spatial attention in vision transformers. Advances in Neural Information Processing Systems, 34, 2021.

[15] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers
for language understanding. In North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, pages 4171–4186, 2019.

[16] X. Dong, J. Bao, T. Zhang, D. Chen, W. Zhang, L. Yuan, D. Chen, F. Wen, and N. Yu. Peco: Perceptual

codebook for bert pre-training of vision transformers. arXiv preprint arXiv:2111.12710, 2021.

[17] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-
derer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at
scale. arXiv preprint arXiv:2010.11929, 2020.

[18] S. d’Ascoli, H. Touvron, M. L. Leavitt, A. S. Morcos, G. Biroli, and L. Sagun. Convit: Improving vision
transformers with soft convolutional inductive biases. In International Conference on Machine Learning,
pages 2286–2296. PMLR, 2021.

[19] H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, and C. Feichtenhofer. Multiscale vision
transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
6824–6835, 2021.

[20] G. Ghiasi, Y. Cui, A. Srinivas, R. Qian, T.-Y. Lin, E. D. Cubuk, Q. V. Le, and B. Zoph. Simple copy-paste is
a strong data augmentation method for instance segmentation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 2918–2928, 2021.

[21] S. Gidaris, P. Singh, and N. Komodakis. Unsupervised representation learning by predicting image

rotations. In International Conference on Learning Representation, 2018.

[22] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and
semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 580–587,
2014.

[23] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.
Generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2672–2680,
2014.

[24] B. Graham. Sparse 3d convolutional neural networks. arXiv preprint arXiv:1505.02890, 2015.

[25] B. Graham, M. Engelcke, and L. Van Der Maaten. 3d semantic segmentation with submanifold sparse
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 9224–9232, 2018.

[26] J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires,
Z. Guo, M. Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning.
Advances in Neural Information Processing Systems, 33, 2020.

[27] J. Guo, K. Han, H. Wu, Y. Tang, X. Chen, Y. Wang, and C. Xu. Cmt: Convolutional neural networks
meet vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 12175–12185, 2022.

[28] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick. Masked autoencoders are scalable vision learners.

arXiv preprint arXiv:2111.06377, 2021.

11

[29] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation
learning. In IEEE Conference on Computer Vision and Pattern Recognition, pages 9729–9738, 2020.

[30] K. He, G. Gkioxari, P. Dollár, and R. Girshick. Mask r-cnn. In Proceedings of the IEEE international

conference on computer vision, pages 2961–2969, 2017.

[31] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE Conference

on Computer Vision and Pattern Recognition, pages 770–778, 2016.

[32] O. J. Hénaff, S. Koppula, J.-B. Alayrac, A. v. d. Oord, O. Vinyals, and J. Carreira. Efﬁcient visual

pretraining with contrastive detection. arXiv preprint arXiv:2103.10957, 2021.

[33] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information

Processing Systems, 33:6840–6851, 2020.

[34] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger. Deep networks with stochastic depth. In

European Conference on Computer Vision, pages 646–661. Springer, 2016.

[35] L. Huang, S. You, M. Zheng, F. Wang, C. Qian, and T. Yamasaki. Learning where to learn in cross-view
self-supervised learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 14451–14460, 2022.

[36] L. Huang, Y. Yuan, J. Guo, C. Zhang, X. Chen, and J. Wang. Interlaced sparse self-attention for semantic

segmentation. arXiv preprint arXiv:1907.12273, 2019.

[37] L. Huang, C. Zhang, and H. Zhang. Self-adaptive training: Bridging supervised and self-supervised

learning. arXiv preprint arXiv:2101.08732, 2021.

[38] Z. Huang, Y. Ben, G. Luo, P. Cheng, G. Yu, and B. Fu. Shufﬂe transformer: Rethinking spatial shufﬂe for

vision transformer. arXiv preprint arXiv:2106.03650, 2021.

[39] T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial networks.
In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401–4410,
2019.

[40] H. Kellerer, U. Pferschy, and D. Pisinger. Knapsack problems. Springer, 2004.

[41] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on

Learning Representations, 2015.

[42] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In International Conference on Learning

Representations, 2014.

[43] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural

networks. In Advances in Neural Information Processing Systems, volume 25, pages 1097–1105, 2012.

[44] G. Larsson, M. Maire, and G. Shakhnarovich. Colorization as a proxy task for visual understanding. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6874–6883, 2017.

[45] Y. Li, S. Xie, X. Chen, P. Dollar, K. He, and R. Girshick. Benchmarking detection transfer learning with

vision transformers. arXiv preprint arXiv:2111.11429, 2021.

[46] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie. Feature pyramid networks for
object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
2117–2125, 2017.

[47] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft
coco: Common objects in context. In European conference on computer vision, pages 740–755. Springer,
2014.

[48] Z. Liu, H. Hu, Y. Lin, Z. Yao, Z. Xie, Y. Wei, J. Ning, Y. Cao, Z. Zhang, L. Dong, et al. Swin transformer

v2: Scaling up capacity and resolution. arXiv preprint arXiv:2111.09883, 2021.

[49] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical
vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 10012–10022, 2021.

[50] I. Loshchilov and F. Hutter. SGDR: Stochastic gradient descent with warm restarts. In International

Conference on Learning Representations, 2017.

[51] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia, B. Ginsburg, M. Houston,
O. Kuchaiev, G. Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017.

[52] M. Noroozi and P. Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In

European Conference on Computer Vision, pages 69–84. Springer, 2016.

[53] A. v. d. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding. arXiv

preprint arXiv:1807.03748, 2018.

12

[54] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in
Neural Information Processing Systems, pages 8024–8035, 2019.

[55] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros. Context encoders: Feature learning by

inpainting. In IEEE Conference on Computer Vision and Pattern Recognition, pages 2536–2544, 2016.

[56] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding by generative

pre-training, 2018.

[57] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised

multitask learners. OpenAI blog, 1(8):9, 2019.

[58] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever. Zero-shot
text-to-image generation. In International Conference on Machine Learning, pages 8821–8831. PMLR,
2021.

[59] Y. Rao, W. Zhao, B. Liu, J. Lu, J. Zhou, and C.-J. Hsieh. Dynamicvit: Efﬁcient vision transformers with

dynamic token sparsiﬁcation. Advances in neural information processing systems, 34, 2021.

[60] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,
M. Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer
Vision, 115(3):211–252, 2015.

[61] R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni. Green ai. Communications of the ACM, 63(12):54–63,

2020.

[62] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou. Training data-efﬁcient image
transformers & distillation through attention. In International Conference on Machine Learning, pages
10347–10357. PMLR, 2021.

[63] A. Van Den Oord, O. Vinyals, et al. Neural discrete representation learning. Advances in neural information

processing systems, 30, 2017.

[64] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.

Attention is all you need. Advances in neural information processing systems, 30, 2017.

[65] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with
denoising autoencoders. In International Conference on Machine Learning, pages 1096–1103, 2008.

[66] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao. Pyramid vision
transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the
IEEE/CVF International Conference on Computer Vision, pages 568–578, 2021.

[67] X. Wang, R. Zhang, C. Shen, T. Kong, and L. Li. Dense contrastive learning for self-supervised visual
pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 3024–3033, 2021.

[68] C. Wei, H. Fan, S. Xie, C.-Y. Wu, A. Yuille, and C. Feichtenhofer. Masked feature prediction for

self-supervised visual pre-training. arXiv preprint arXiv:2112.09133, 2021.

[69] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang. Cvt: Introducing convolutions to
vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
22–31, 2021.

[70] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin. Unsupervised feature learning via non-parametric instance
discrimination. In IEEE Conference on Computer Vision and Pattern Recognition, pages 3733–3742, 2018.

[71] T. Xiao, C. J. Reed, X. Wang, K. Keutzer, and T. Darrell. Region similarity representation learning. arXiv

preprint arXiv:2103.12902, 2021.

[72] Z. Xie, Y. Lin, Z. Zhang, Y. Cao, S. Lin, and H. Hu. Propagate yourself: Exploring pixel-level consistency
for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 16684–16693, 2021.

[73] Z. Xie, Z. Zhang, Y. Cao, Y. Lin, J. Bao, Z. Yao, Q. Dai, and H. Hu. Simmim: A simple framework for

masked image modeling. arXiv preprint arXiv:2111.09886, 2021.

[74] J. Xu, W. Zhou, Z. Fu, H. Zhou, and L. Li. A survey on green deep learning. arXiv preprint

arXiv:2111.05193, 2021.

[75] H. Yin, A. Vahdat, J. Alvarez, A. Mallya, J. Kautz, and P. Molchanov. Adavit: Adaptive tokens for efﬁcient

vision transformer. arXiv preprint arXiv:2112.07658, 2021.

[76] W. Yu, M. Luo, P. Zhou, C. Si, Y. Zhou, X. Wang, J. Feng, and S. Yan. Metaformer is actually what you
need for vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 10819–10829, 2022.

13

[77] Y. Yuan, R. Fu, L. Huang, W. Lin, C. Zhang, X. Chen, and J. Wang. Hrformer: High-resolution vision

transformer for dense predict. Advances in Neural Information Processing Systems, 34, 2021.

[78] J. Zbontar, L. Jing, I. Misra, Y. LeCun, and S. Deny. Barlow twins: Self-supervised learning via redundancy

reduction. arXiv preprint arXiv:2103.03230, 2021.

[79] M. Zheng, F. Wang, S. You, C. Qian, C. Zhang, X. Wang, and C. Xu. Weakly supervised contrastive
learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages
10042–10051, October 2021.

[80] M. Zheng, S. You, F. Wang, C. Qian, C. Zhang, X. Wang, and C. Xu. Ressl: Relational self-supervised
learning with weak augmentation. Advances in Neural Information Processing Systems, 34, 2021.

[81] J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, A. Yuille, and T. Kong. ibot: Image bert pre-training with

online tokenizer. arXiv preprint arXiv:2111.07832, 2021.

Checklist

1. For all authors...

(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s

contributions and scope? [Yes]

(b) Did you describe the limitations of your work? [Yes] In the supplemental materials.
(c) Did you discuss any potential negative societal impacts of your work? [Yes] In the

supplemental materials.

(d) Have you read the ethics review guidelines and ensured that your paper conforms to

them? [Yes]

2. If you are including theoretical results...

(a) Did you state the full set of assumptions of all theoretical results? [N/A]
(b) Did you include complete proofs of all theoretical results? [N/A]

3. If you ran experiments...

(a) Did you include the code, data, and instructions needed to reproduce the main experi-

mental results (either in the supplemental material or as a URL)? [Yes]

(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they

were chosen)? [Yes]

(c) Did you report error bars (e.g., with respect to the random seed after running experi-

ments multiple times)? [No]

(d) Did you include the total amount of compute and the type of resources used (e.g., type

of GPUs, internal cluster, or cloud provider)? [Yes]

4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...

(a) If your work uses existing assets, did you cite the creators? [Yes]
(b) Did you mention the license of the assets? [Yes]
(c) Did you include any new assets either in the supplemental material or as a URL? [No]
(d) Did you discuss whether and how consent was obtained from people whose data you’re

using/curating? [N/A]

(e) Did you discuss whether the data you are using/curating contains personally identiﬁable

information or offensive content? [N/A]

5. If you used crowdsourcing or conducted research with human subjects...

(a) Did you include the full text of instructions given to participants and screenshots, if

applicable? [N/A]

(b) Did you describe any potential participant risks, with links to Institutional Review

Board (IRB) approvals, if applicable? [N/A]

(c) Did you include the estimated hourly wage paid to participants and the total amount

spent on participant compensation? [N/A]

14

Table 4: Inﬂuence of the optimal grouping on Group Window Attention.

Group size gs

Greedy (Algorithm 1)
p × p (i.e., 49)
maxi wi
maxi wi
(cid:80)
i wi

Dynamic Programming
(cid:88)
(cid:88)
(cid:88)
×
×

FLOPS@Stage1

FLOPS@Stage1

FLOPS@Stage1

62.6M
65.0M
64.5M
137.5M
201.3M

55.4M
58.5M
59.7M
113.3M
69.2M

52.3M
52.7M
62.5M
75.7M
52.7M

Table 5: Time cost of each component in the Group Window Attention.

Time Cost (ms) DP Masking

Pre-proc Grouping Ungrouping Attention Fwd & Bwd

Stage1
Stage2
Stage3

4.9
0.6
0.1

3.8
0.1
0.1

14.8
2.1
1.0

0.4
0.2
0.1

0.4
0.2
0.1

61.9
20.8
15.4

A More Ablation Studies

A.1 Efﬁciency comparison using Twins-L

Here we compare our method with a variant of MAE
that operates on both visible and masked patches, which
is almost identical to SimMIM except for the choice
of the loss function. As shown in Table 6, our method
performs on par with the baseline MAE operating on all
patches while enjoying 2.6x pre-training speedup in a
greener way.

A.2 Analysis on the Optimal Grouping

Table 6: Comparison using Twins-L.

Method

Time Mem.

Acc.

MAE w/ all patches
Ours

2.6h
0.8h

408.3G 83.5%
102.3G 83.3%

Following the setting in Figure 4, we compare the complexity of a single group attention module, w/
or w/o dynamic programming, at each stage as below. As displayed in Table 4, when gs = maxi wi,
the complexity is doubled without the DP solver. Simply setting gs = (cid:80)
i wi (such that there is
only 1 group) also suffers from heavy cost, encountering an out-of-memory error in practice even
with a much smaller batch size (e.g., 64 per GPU). In contrast, with the DP solver, the complexity
is signiﬁcantly reduced even when we simply ﬁx the group size to the same as the window size
p × p (note that maxi wi = p × p with a high probability in practice) as discussed in Sec. 4.2. This
experiment demonstrates the efﬁcacy of our optimal grouping scheme.

In addition, We benchmark the time cost (ms) of each component in a single Group Attention module
and summarize the results in Table 5. Here we use a tensor of shape [256, Hi × Wi, Ci] as input,
where Hi, Wi, Ci denote the height, width, and the number of channels of the features in the ith stage.
Note that the DP, masking, and other pre-processing operations are only executed twice for each
stage, i.e., for the shifted/unshifted window partition. We can see that the extra cost of our method is
indeed moderate compared with the attention computation.

B More Details of Our Method

B.1 Method Overview.

We provide a diagram in Figure 6 for an intuitive illustration of our method. The input image is ﬁrst
randomly masked according to the masking scheme in Section 3.5, then fed into our green hierarchical
ViT to obtain representations for each visible patch. Finally, we concatenate the representations
of visible patches with the mask tokens and feed them into a transformer decoder, yielding the
representations for the masked patches. Finally, we predict the raw pixel values of masked patches

15

Figure 6: Overview of our method. The input image is randomly masked and then fed into a 4-stage
hierarchical ViTs. Finally, a lightweight decoder takes the representations of visible patches and
mask tokens to reconstruct the missing patches.

Figure 7: Illustration of the Group Window Attention scheme with shifted windows. It shows
that our approach is agnostic to the window partition.

from the corresponding representations and implement the training by minimizing the Mean Square
Error (MSE) between the predictions and ground truth.

B.2 Group Window Attention scheme with shifted windows.

In addition to Figure 2, we also illustrate how our method works with the irregular window partition
in Figure 7. We can observe that owing to the optimal grouping scheme, our method dynamically
ﬁnds out the best group partition despite the number of visible patches within each local window
being highly uneven. This ﬁgure further demonstrates that our approach is agnostic to the window
partition and works impressively well.

B.3 A Python Implementation of the Optimal Grouping algorithm

We provide a Python implementation of the Dynamic-Programming-based Optimal Grouping al-
gorithm in Algorithm 2. As we can see, the two components of the Optimal Grouping algorithm
are both easy to implement. For the DP algorithm for the single Knapsack problem, its time/space
complexity is O(gsnw) where gs is the group size and nw is the number of windows. In practice,
because gs and nw are generally small (i.e., smaller than 100) the running time of Algorithm 2 is
negligible (i.e., <1ms).

B.4 A PyTorch Implementation of the Group Attention scheme

With the group partition on the indexes, we can then permute visible patches according to the partition
and obtain several groups of patches with an equal size gs, upon which the Masked Attention is

16

MaskStage1Stage2Stage3Stage4Decoder:Isotropic ViTMSEMSEMSEMSEMSEMSEEncoder: Green Hierarchical ViT with Group Window AttentionMaskOptimal GroupingMasked Attention𝐗"𝐗#𝐗Algorithm 2 Dynamic Programming-based algorithm for the Optimal Grouping using Python.

1 def Knapsack(g_s, Phi):
2
3
4
5
6

algorithm

# g_s (int): Group size
# Phi (list[int]): The numbers of visible patches within each local window

n_w = len(Phi) # the number of windows
K = [[0 for w in range(g_s + 1)] for i in range(n_w + 1)] # a buffer for the DP

# Build table K[][] in a bottom up manner
for i in range(n_w + 1):

K[i][w] = max(Phi[i − 1] + K[i − 1][w − Phi[i − 1]], K[i − 1][w])

K[i][w] = K[i − 1][w]

# Store the result of Knapsack
res = K[n_w][g_s]

for w in range(g_s + 1):
if i == 0 or w == 0:
K[i][w] = 0

elif Phi[i − 1] <= w:

# Store the selected indexes
w = g_s
Pi = []

for i in range(n_w, 0, −1):

else:

continue

if res <= 0:
break

7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40 def GroupPartition(g_s, Phi):
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58

while len(win_szs) > 0:

return win_idxs

if res == K[i − 1][w]: # This window is not included.

else: # This window is included.

Pi.append(i − 1)
# Since this window is included, its value is deducted
res = res − Phi[i − 1]
w = w − Phi[i − 1]

return Pi[::−1] # Optional: make Pi in an increasing order

# g_s (int): Group size
# Phi (list[int]): The numbers of visible patches within each local window

win_szs = Phi.copy()
ori_win_idxs = list(range(len(win_szs)))
win_idxs = []

idx = knapsack(group_size, win_szs)

# Append the selected idx
win_idxs.append([ori_win_idxs[i] for i in idx])

# The remaining windows and indexes
win_szs = [win_szs[i] for i in range(len(ori_win_idxs)) if i not in idx]
ori_win_idxs = [ori_win_idxs[i] for i in range(len(ori_win_idxs)) if i not in idx]

performed. We also provide a PyTorch implementation of the Group Attention scheme in Algorithm 3
to facilitate future research. Note the padding operations are omitted here for simplicity.

17

Algorithm 3 Group Attention using the PyTorch framework.

# B is the batch size, L is the number of visible patches, C is the number of channels
B, L, C = x.shape

# x (3−d tensor): Features the visible patches
# g_s (int): Group size
# Phi (list[int]): The numbers of visible patches within each local window

1 def GroupAttention(x, g_s, Phi):
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26

return x

# Prepare for the group attention
win_idxs = GroupPartition(g_s, Phi)
patch_idxs = torch.arrange(sum(Phi))
patch_idxs = torch.split(patch_idxs, Phi)
shuffle_idxs = torch.cat([patch_idxs[wi] for wi in win_idxs])
unshuffle_idxs = torch.argsort(shuffle_idxs)

# Group partition. For simplicity, assume that the partition is even
x = torch.index_select(x, 1, shuffle_idxs) # (B, n_g ∗ g_s, C)
x = x.reshape(−1, g_s, C) # (B ∗ n_g, g_s, C)

# Attention with relative position bias as in Figure 3
x = MaskedAttention(x)

# Reverse the group partition
x = x.reshape(B, L, C).index_select(x, 1, unshuffle_idxs)

18

