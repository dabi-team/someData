1
2
0
2

n
u
J

9

]
L
P
.
s
c
[

2
v
2
5
4
2
0
.
6
0
1
2
:
v
i
X
r
a

Proving Equivalence Between Complex Expressions
Using Graph-to-Sequence Neural Models

Steve Kommrusch, ThÃ©o Barollet and Louis-NoÃ«l Pouchet

Abstract
We target the problem of provably computing the equiva-
lence between two complex expression trees. To this end, we
formalize the problem of equivalence between two such pro-
grams as finding a set of semantics-preserving rewrite rules
from one into the other, such that after the rewrite the two
programs are structurally identical, and therefore trivially
equivalent. We then develop a graph-to-sequence neural net-
work system for program equivalence, trained to produce
such rewrite sequences from a carefully crafted automatic
example generation algorithm. We extensively evaluate our
system on a rich multi-type linear algebra expression lan-
guage, using arbitrary combinations of 100+ graph-rewriting
axioms of equivalence. Our machine learning system guar-
antees correctness for all true negatives, and ensures 0 false
positive by design. It outputs via inference a valid proof of
equivalence for 93% of the 10,000 equivalent expression pairs
isolated for testing, using up to 50-term expressions. In all
cases, the validity of the sequence produced and therefore
the provable assertion of program equivalence is always
computable, in negligible time.

1 Introduction
Deep neural network systems have excelled at a variety of
classification and reinforcement learning tasks [26]. How-
ever, their stochastic nature tends to hinder their deployment
for automated program analysis: ensuring the correctness
of the solution produced is often required, e.g., when deter-
mining the semantics equivalence between two programs
(or symbolic expressions).

In this work we target the problem of automatically com-
puting whether two input symbolic expressions are semanti-
cally equivalent [31], under a well-defined axiomatic system
for equivalence using semantics-preserving rewrite rules
[21]. Program equivalence is summarized as determining
whether two programs would always produce the same out-
puts for all possible inputs, and is a central problem in com-
puting [23, 31, 55]. The problem ranges from undecidable, e.g.
[25], to trivial in cases of testing the equivalence of a program
with itself. Our work directly studies the subset of programs
represented by symbolic linear algebra expressions which
include scalar, vector, and matrix types for both constants
and variables, and 16 different operators with 147 distinct
axioms of equivalence. For example, the expression using

pe-graph2axiom, June, 2021, ArXiV
2021.

1

matrices, scalars, and a vector: (ğ´ +ğµ)ğ¼ ((ğ‘ + (ğ‘ âˆ’ğ‘))/ğ‘)(cid:174)ğ‘£ âˆ’ğ´(cid:174)ğ‘£
can be proven equivalent to ğµ(cid:174)ğ‘£ by applying 10 axioms in
sequence; our work generates the proof steps between these
expressions.

While prior work has shown promises for deep networks
to compute some forms of program equivalence [5, 62], the
system typically outputs only a probability of equivalence,
without any reasoning or insight that can be verified easily:
false positive can be produced. Programs can be represented
as a tree (or graph) of symbols, and deep networks for sym-
bolic reasoning have been studied, e.g. to compute the deriv-
ative of a symbolic expression [38]. In this work, we take a
significantly different approach to the problem of symbolic
program reasoning with deep networks: we make the sys-
tem produce the sequence of steps that lead to rewriting one
program into another, that is the reasoning for (or proof of)
equivalence between the two programs, instead of produc-
ing directly the result of this reasoning (e.g., a probability of
equivalence, without explanation about the reasoning). In a
nutshell, we approach expression equivalence as a theorem
proving problem, in which all the axioms as well as tactics
to compute a proof are all learned by example in a deep
learning system, without any human insight.

We propose a method for generating training samples
using probabilistic applications of production rules within
a formal grammar, and then develop a graph-to-sequence
[13, 40] neural network system for program equivalence,
trained to learn and combine rewrite rules to rewrite one
program into another. It can deterministically prove equiva-
lence, entirely avoids false positives, and quickly invalidates
incorrect answers produced by the network (no determin-
istic answer is provided in this case, only a probability of
non-equivalence). In a nutshell, we develop the first graph-
to-sequence neural network system to accelerate the search
in the space of possible combinations of transformation rules
(i.e., axioms of equivalence in the input language) to make
two graphs representing symbolic expressions structurally
identical without violating their original semantics. We pro-
pose a machine learning system for program equivalence
which ensures correctness for all non-equivalent programs
input (specificity = 100%) , and a deterministically checkable
output for equivalent programs (no false positives). We make
the following contributions:

1. We design, implement and evaluate two competing
approaches using graph-to-sequence neural network
systems to generate proofs of equivalence. We provide

 
 
 
 
 
 
pe-graph2axiom, June, 2021, ArXiV

Steve Kommrusch, ThÃ©o Barollet and Louis-NoÃ«l Pouchet

ğ‘

âˆ—

âˆ—

+

âˆ—

1

ğ‘

1

ğ‘

ğ‘

âˆ—

ğ‘

+

ğ‘

(a) ğ‘ âˆ— (1 âˆ— ğ‘ + 1 âˆ— ğ‘)

(b) ğ‘ âˆ— (ğ‘ + ğ‘)

+

âˆ—

âˆ—

ğ‘

ğ‘

ğ‘

ğ‘

(c) ğ‘ âˆ— ğ‘ + ğ‘ âˆ— ğ‘

+

âˆ—

âˆ—

ğ‘

ğ‘

ğ‘

ğ‘

(d) ğ‘ âˆ— ğ‘ + ğ‘ âˆ— ğ‘

Figure 1. Examples of Computations

the first implementation of such graph-to-sequence
systems in the popular OpenNMT-py framework [34].
2. We present a complete implementation of our system
operating on a rich language for multi-type linear alge-
bra expressions. Our system provides a correct rewrite
rule sequence between two equivalent programs for
93% of the 10,000 test cases. The correctness of the
rewrite rule is deterministically checkable in all cases
in negligible time.

The rest of the paper is organized as follows. Sec. 2 out-
lines the program equivalence problem we address, and mo-
tivates our proposed approach. Sec. 3 formalizes the equiv-
alence problem addressed. Automatic sample generation is
discussed in Sec. 4 before Sec. 5 which introduces our DNN
system, its overall design principles and key components. A
complete experimental evaluation of our system is detailed in
Sec. 6. We present related work in Sec. 7 before concluding.

2 Motivation and Overview
In this work we
Rewrite rules as axioms of equivalence
represent programs with symbolic expressions made of vari-
ables (e.g., ğ‘, ğ‘, ğ‘), operators (e.g., +, *) and neutral/absorbing
elements (e.g., 1). We consider a rich linear algebra expression
language, supporting three variable types (scalars as shown
in P1-P4, vectors, and matrices) and 5 different variables
per type; 16 operators including operators mixing different
variable types such as vector-matrix product. We represent
these programs as dataflow graphs [16] with a single root
node, that is to compute a single value.

P1 is equivalent to P2 if we consider the axiom ğ´1 : 1Nâˆ—ğ‘¥ =
ğ‘¥, âˆ€ğ‘¥ âˆˆ N. This axiom is also a clear rewrite rule: the LHS
expression 1N âˆ— ğ‘¥ (with ğ‘¥ âˆˆ N) can be matched and replaced
by the RHS expression ğ‘¥ anywhere in the program with-
out altering its semantics. An axiom, or equivalently here a
graph rewrite rule, may be applied repeatedly to different
subtrees. When applying ğ´1 on a specific location, the node
ğ‘ of ğ‘ƒ1, we obtain an equivalent and yet syntactically differ-
ent program, we note ğ‘ƒ1 â‰¡ ğ´1(ğ‘, ğ‘ƒ1). These equivalences
can be composed, incrementally, to form a complex transfor-
mation: we have ğ‘ƒ1 â‰¡ ğ´1(ğ‘, ğ´1(ğ‘, ğ‘ƒ1)). The result of these
semantics-preserving transformations can be computed in
sequence: first implement ğ´1(ğ‘, ğ‘ƒ1) to obtain a new program
ğ‘ƒ â€², then ğ´1(ğ‘, ğ‘ƒ â€²) to obtain ğ‘ƒ â€²â€². To prove ğ‘ƒ1 â‰¡ ğ‘ƒ2, we simply
check ğ‘ƒ â€²â€² is structurally identical to ğ‘ƒ2, a linear time process.

2

To assess the validity of a transformation sequence ğ‘†
where ğ‘ƒ2 = ğ‘† (ğ‘ƒ1), one simply needs to check for ğ‘†, in se-
quence, that each axiom is applicable at that program point,
apply it to obtain a new temporary program, and repeat
the process for each axiom in the complete sequence. If the
sequence is verified to be valid, and ğ‘† (ğ‘ƒ1) is structurally
equivalent to ğ‘ƒ2, then we have proved ğ‘ƒ1 â‰¡ ğ‘ƒ2, and ğ‘† forms
the complete proof of equivalence between the two pro-
grams. Using ğ´2 : ğ‘¥ âˆ— (ğ‘¦ + ğ‘§) = ğ‘¥ âˆ— ğ‘¦ + ğ‘¥ âˆ— ğ‘§, âˆ€ğ‘¥, ğ‘¦, ğ‘§ âˆˆ N
and ğ´3 : ğ‘¥ + ğ‘¦ = ğ‘¦ + ğ‘¥, âˆ€ğ‘¥, ğ‘¦ âˆˆ N, we have ğ‘ƒ1 â‰¡ ğ‘ƒ4 â‰¡
ğ´3(+, ğ´2(âˆ—, ğ´1(ğ‘, ğ´1(ğ‘, ğ‘ƒ1)))), a verifiable proof of equiva-
lence under our axioms between the programs ğ‘(1ğ‘ +1ğ‘) and
ğ‘ğ‘ + ğ‘ğ‘, which involved structural changes including node
deletion, creation and edge modification. Note the bidirec-
tional nature of the process: one can rewrite from ğ‘(1ğ‘ + 1ğ‘)
to ğ‘ğ‘ + ğ‘ğ‘, or the converse using the same (but reverted) se-
quence. Note also the non-unicity of a sequence: by possibly
many ways a program can be rewritten into another one,
for example the sequence ğ‘ƒ4 â‰¡ ğ´3(+, ğ´1(ğ‘, ğ´1(ğ‘, ğ´2(âˆ—, ğ‘ƒ1))
also correctly rewrites ğ‘ƒ1 into ğ‘ƒ4. Conversely, a sequence
may not exist: for example no sequence of the 3 above axioms
allow to rewrite ğ‘ +ğ‘ into ğ‘ âˆ—ğ‘. We call these non-equivalent
in our system, that is precisely if there is no sequence of
axioms that can be applied to rewrite one program into the
other. Our approach aims to compute some ğ‘† for a pair of
programs ğ‘ƒ1, ğ‘ƒ2, so that ğ‘† is verified correct when ğ‘ƒ1 â‰¡ ğ‘ƒ2.
Consequently, if ğ‘ƒ1 (cid:46) ğ‘ƒ2, no sequence ğ‘† produced can be
verified correct: true negatives are trivially detected.

Intuitively, we
Pathfinding program equivalence proofs
can view the solution space as a graph, where every possi-
ble syntactically different program in the language is rep-
resented by its own vertex ğ‘£ğ‘– . And âˆƒ ğ‘’ (ğ´ğ‘˜ ,ğ‘¥)
: ğ‘£ğ‘– â†’ ğ‘£ ğ‘— iff
âˆƒğ´ğ‘˜ an axiom and ğ‘¥ a node in ğ‘£ğ‘– such that ğ‘£ ğ‘— = ğ´ğ‘˜ (ğ‘¥, ğ‘£ğ‘– ).
Any two programs connected by a path in this graph are
therefore semantically equivalent. Building ğ‘† for ğ‘ƒ1 â‰¡ ğ‘† (ğ‘ƒ2)
amounts to exposing one path between ğ‘ƒ1 and ğ‘ƒ2 in this
graph when it exists, the path forming the proof of equiva-
lence. We build a deep learning graph-to-sequence system to
learn a stochastic approximation of an iterative algorithm to
construct such feasible path when possible, trained only by
randomly sampling pairs of programs and one carefully la-
beled path between them. This avoids the need to craft smart

Proving Equivalence Between Complex Expressions

pe-graph2axiom, June, 2021, ArXiV

Figure 2. pe-graph2axiom System Overview

exploration heuristics to make this path-finding problem
practical.

Graph-to-sequence network for pathfinding This is in-
stead what we let the neural network learn automatically;
and specifically why we implemented graph neural networks
to solve this problem [51, 62]. We rely on the network to
suggest a transformation path by inference, and then verify
its validity in linear time. To implement our approach, we
enumerate randomly valid sentences in a language, and a set
of axioms of equivalence expressible as semantics-preserving
rewrite rules from one to the other. The system in Fig. 2 takes
as input two programs represented as symbolic trees, and
produces a sequence of axioms along with their position of
application (or node) that can be used to rewrite sequentially
one input program into the other input program. To train
the system, we generate pairs of equivalent programs by it-
erating the axioms with random probability on one program,
thereby generating both a path to equivalence and the target
program. Random programs are generated so as to respect
the grammar defined. The training set is then appropriately
selected from these random samples, as detailed in Sec. 6.
Node initialization initializes the graph neural network, con-
verting the input programs text (e.g., (ğ‘ + (ğ‘ + ğ‘)) into nodes
and edges in the Graph Neural Network [51, 62]. The details
of the network are covered in Sec. 5. In a nutshell, the key
principle is to combine a memory-based neural network ap-
proach, e.g., using Long-Short Term Memory (LSTM) [28]
neurons and a graph neural network design (which uses
Gated Recurrent Units (GRUs) internally) [13] that matches
our program graph representation. Token embedding is a
neural network layer in which tokens are assigned a learn-
able multidimensional embedding vector [44]. Each layer
in LSTM 2 layers has 256 neurons, which support sequence
generation. Token generator is the final output portion of the
network. It learns to output the tokens based on the current
LSTM hidden states and the Global Attention from the graph
neural network. As each token is output, it feeds back into
the LSTM layer through the embedding layer to affect its
next state. We use a sequence generation principle, using a
global attention mechanism [42] to allow observation of pro-
gram graph node information while generating the axiom

3

and location on which it is applied. As developed below, we
specifically study the robustness of our approach to generate
proofs of increasingly complex length, contrasting models
to output the entire path at once with pe-graph2axiom
which incrementally builds the sequence one step at a time,
as shown in Sec. 6.

3 Framework for Program Equivalence
We now present the formalism we use in this work to repre-
sent symbolic expressions and their equivalences. We care-
fully co-designed this problem representation and the (graph)
neural network approach to make the best use of machine
learning via deep networks, as discussed in Sec. 5.

3.1 Input Representation

A key design aspect is to match the capability of the neu-
ral network to model the input as a walkable graph with
the actual input program representation to be handled. We
therefore model â€œprogramsâ€ in a dataflow-like representa-
tion (i.e., a directed graph), using a single root/output node.
Symbolic expressions computing a single result typically fit
this representation. The following definitions are applica-
ble to programs represented as dataflow graphs, albeit we
specialize them to symbolic expressions.

Definition 3.1 (Expression graph node). A node ğ‘› âˆˆ ğ‘
in the expression graph models n-ary operations and input
operands. A node produces a value which can be consumed
by any of its immediate successors in the graph. When a node
has no predecessor, it models an input value. The output
value for the computation is produced by the unique root
node ğ‘›ğ‘Ÿğ‘œğ‘œğ‘¡ of the graph, the only node without successor.

Definition 3.2 (Expression graph directed edge). A directed
edge ğ‘’ğ‘›1,ğ‘›2 : ğ‘›1 â†’ ğ‘›2 with ğ‘›1, ğ‘›2 âˆˆ ğ‘ in the expression graph
connects the producer of a value (ğ‘›1) to a node consuming
this value in the computation.

Definition 3.3 (Expression graph). A expression graph ğº is
a directed dataflow graph modeling the computation, made
of nodes ğ‘›ğ‘– âˆˆ ğ‘ and edges ğ‘’ğ‘›ğ‘–,ğ‘› ğ‘— âˆˆ ğ¸ as defined in Def. 3.1
and Def. 3.2. That is, ğº = âŸ¨ğ‘›ğ‘Ÿğ‘œğ‘œğ‘¡, ğ‘ , ğ¸âŸ©. There is no dandling
edge nor unconnected node in ğº.

pe-graph2axiom, June, 2021, ArXiV

Steve Kommrusch, ThÃ©o Barollet and Louis-NoÃ«l Pouchet

Language of linear algebra expressions We developed a
complex-enough language to evaluate carefully our work,
that captures rich linear algebra expressions. Specifically, we
support 3 types of data/variables in the expression: scalars,
vectors and matrices. We use the standard notation ğ‘, (cid:174)ğ‘, ğ´
for scalars, vectors and matrices. We evaluate using different
variable names for each of the 3 types above, along with
their identity and absorbing elements.

We also model a rich set of operators, mixing different
unary and binary operations for each type. Specifically, we
support âˆ—ğ‘ , +ğ‘ , âˆ’ğ‘ , /ğ‘  between scalar operands, and +ğ‘£, âˆ’ğ‘£, âˆ—ğ‘£
between vectors and +ğ‘š, âˆ’ğ‘š, âˆ—ğ‘š for matrices. For âˆ’, / we
also support their unary version for all types, e.g. âˆ’1ğ‘  for
unary scalar inversion and âˆ’ğ‘¢ğ‘š for unary matrix negation.
For example ğ‘âˆ’1ğ‘  computes to 1/ğ‘. We also support multi-
type operations, such as vector and matrix scaling by a scalar
âˆ—ğ‘ ğ‘£, âˆ—ğ‘ ğ‘š. We support two specific unary matrix operations,
transpose ğ‘¡ğ‘š and matrix inversion as âˆ’1ğ‘š . Note every opera-
tor has a unique name in our language, driven by the type of
its operand. This will facilitate the learning of the expression
embedding, avoiding the need to learn type propagation.

Examples Expressions of the form ğ´(ğµğ¶ğ‘¡ ğ·)ğ¸âˆ’1, (cid:174)ğ‘ +ğ‘(cid:174)ğ‘âˆ’1 âˆ’
0(cid:174)ğ‘’, (ğ‘ +ğ‘) + (ğ‘ (ğ‘‘/ğ‘’)), (ğ‘ğ´ +ğ‘ğµ)ğ¶ğ‘¡ etc. can be parsed trivially
to our representation, one simply needs to be able to provide
a unique name for each operand and operator type (possibly
via some analysis, or simple language design principles),
that is avoiding to overload the semantics of operators and
operands. Note the semantics is never explicitly provided to
our DNN approach, it is learned by examples. There will be
no example of the form e.g. ğ‘ + ğ´, an invalid expression in
our language.

We believe a sensible approach is to develop a clean, reg-
ular grammar for the language to be handled, as implicitly
these are concepts the DNN will need to learn. We did so,
using a classical LL(1) grammar description of our linear al-
gebra language. This is not a requirement of our approach, as
one can arrive to the desired input expression graph by any
means necessary, but we believe making the reasoning on
the language structure â€œeasyâ€ is an important design aspect.

3.2 Axioms of Equivalence

A central aspect of our approach is to view the problem of ex-
pression equivalence as finding a sequence of locally-correct
rewrite rules that each preserve the semantics, thereby mak-
ing incremental reasoning possible. We explicitly do not con-
sider non-semantics-preserving axioms. A rich structure of
alternate but equivalent ways to rewrite one expression to an-
other makes the problem easier to sample and more amenable
to machine learning. Semantics-preserving axioms enable
incremental per-axiom reasoning, and enforce semantics
preservation without overly complicated semantics analysis;
while still manipulating a very rich space of transformations.
To illustrate this we specifically design axioms that perform

4

complex graph modifications, such as node deletion or cre-
ation, subtree manipulation, multi-node graph changes, etc.
A graph pattern can be viewed as a pattern-matching
rule on graphs and its precise applicability criteria. It can
also be viewed as a sentential form of the language grammar,
e.g. ScalarVal PlusOp ScalarVal is a pattern, if the
grammar is well formed.
Definition 3.4 (Graph pattern). A graph pattern ğ‘ƒ is an un-
ambiguous structural description of a (sub-)graph ğºğ‘ƒ , which
can be deterministically matched in any expression graph ğº.
We have ğ‘ƒ = âŸ¨ğºğ‘ƒ, ğ‘€ğ‘›, ğ‘€ğ‘’ âŸ© where for each node ğ‘›ğ‘– âˆˆ ğ‘ ğºğ‘ƒ ,
{ğ‘›ğ‘šğ‘ğ‘¡ğ‘â„ } = ğ‘€ğ‘› (ğ‘›ğ‘– ) returns the set of node values ğ‘›ğ‘šğ‘ğ‘¡ğ‘â„
accepted to match ğ‘›ğ‘– on a graph ğº. For ğ‘›ğ‘–, ğ‘› ğ‘— âˆˆ ğ‘ ğºğ‘ƒ , ğ‘’ğ‘– =
ğ‘€ğ‘’ (ğ‘›ğ‘–, ğ‘› ğ‘— ) returns the set of edges between ğ‘€ (ğ‘›ğ‘– ) and ğ‘€ (ğ‘› ğ‘— )
to be matched in ğº. A pattern ğºğ‘ƒ is matched in ğº if (a) âˆ€ğ‘›ğ‘– âˆˆ
ğºğ‘, âˆƒ ğ‘›ğ‘š = ğ‘€ (ğ‘›ğ‘– ) âˆˆ ğ‘ ğº ; (b) âˆ€ğ‘’ğ‘– âˆˆ ğ¸ğºğ‘ƒ , âˆƒ ğ‘’ğ‘€ğ‘› (ğ‘›ğ‘– ),ğ‘€ğ‘› (ğ‘› ğ‘— ) =
ğ‘€ğ‘’ (ğ‘›ğ‘–, ğ‘› ğ‘— ) âˆˆ ğ¸ğº ; and (c) (cid:154)ğ‘’ğ‘€ğ‘› (ğ‘›ğ‘– ),ğ‘€ğ‘› (ğ‘› ğ‘— ) âˆˆ ğ¸ğº â‰  ğ‘€ğ‘’ (ğ‘›ğ‘–, ğ‘› ğ‘— ).
Note when a graph pattern models a rewrite, ğ‘€ğ‘› and ğ‘€ğ‘’
are adjusted accordingly to output the rewrite of a node
ğ‘› âˆˆ ğ‘ ğº into its desired value, instead of the set of acceptable
nodes from ğ‘› âˆˆ ğ‘ ğºğ‘ƒ .
Definition 3.5 (Axiom of equivalence). An axiom ğ´ is a
semantics-preserving rewrite rule ğº â€² = ğ´(ğ‘›, ğº) that can ar-
bitrarily modify a expression graph ğº, and produces another
expression graph ğº â€² respecting Def. 3.3 with identical se-
mantics to ğº. We note ğ´ : âŸ¨ğ‘ƒğ‘šğ‘ğ‘¡ğ‘â„, ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘™ğ‘ğ‘ğ‘’ âŸ© an axiom, where
ğ‘ƒğ‘šğ‘ğ‘¡ğ‘â„, ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘™ğ‘ğ‘ğ‘’ are graph patterns as per Def. 3.4. The appli-
cation of axiom ğ´ to node ğ‘› in ğº is written ğ´(ğ‘›, ğº).

We can compose axioms to form a complex rewrite se-

quence.

Definition 3.6 (Semantics-preserving axiom composition).
Given a sequence ğ‘† : ğ´1(ğ‘›1, ğ´2(ğ‘›2, ..., ğ´ğ‘š (ğ‘›ğ‘š, ğº))) of ğ‘š ax-
ioms applications. It is a semantics-preserving composition
if for each ğº ğ‘— = ğ´ğ‘– (ğ‘›ğ‘–, ğºğ‘– ) âˆˆ ğ‘†, ğ‘ƒğ´ğ‘–
ğ‘šğ‘ğ‘¡ğ‘â„ succeeds on the sub-
graph with root ğ‘›ğ‘– in ğºğ‘– , and ğº ğ‘— is obtained by applying
ğ‘ƒğ´ğ‘–
ğ‘Ÿğ‘’ğ‘ğ‘™ğ‘ğ‘ğ‘’ to ğ‘›ğ‘– .

Theorem 3.7 (Expression graph equivalence). Given a ex-
pression ğº. If ğº â€² = ğ‘† (ğº) such that ğ‘† is a semantics-preserving
sequence as per Def. 3.6, then ğº â‰¡ ğº â€², they are equivalent
under the axiom system used in ğ‘†.

This is a direct consequence of using only semantics-
preserving axioms, each rewrite cannot individually alter
the semantics, so such incremental composition does not. It
leads to the formal problem we are addressing:

Corollary 3.8 (Expression graphs equivalence matching).
Given two expressions ğº, ğº â€². If there exist a semantics-preserving
sequence ğ‘† such that ğº â€² = ğ‘† (ğº), then ğº â‰¡ ğº â€².

Note here = means complete structural equivalence be-
tween the two graphs: they are identical in structure and

Proving Equivalence Between Complex Expressions

pe-graph2axiom, June, 2021, ArXiV

label/node values. Determining ğº = ğº â€² amounts to visiting
both graphs simultaneously e.g. in depth-first search from
the root to ensure structural equivalence, and also verifying
the same node labels appear in both at the same time. This
is trivally implemented in linear time in the graph size.

Language of linear algebra expressions We have imple-
memented a total of 102 different axioms for our language,
made of the multi-type versions of the 13 core restructur-
ing axioms described later in Table 1. They all follow estab-
lished linear algebra properties. Note different data types
have different axioms following typical linear algebra rules,
e.g., matrix-multiplication does not commute, but scalar
and vector multiplications do. Examples of axioms include
ğ‘¥ (ğ‘¦ğ‘§) â†’ (ğ‘¥ğ‘¦)ğ‘§, ğ‘‹ âˆ’ ğ‘‹ â†’ ğ‘‚, âˆ’( (cid:174)ğ‘¥ âˆ’ (cid:174)ğ‘¦) â†’ (cid:174)ğ‘¦ âˆ’ (cid:174)ğ‘¥, or ğ‘‹ ğ‘¡ğ‘¡
â†’ ğ‘‹ ,
an exhaustive list is displayed in the Supplementary Material.
In our experiments, we presume matrix and vector dimen-
sions are appropriate for the given operation. Such dimen-
sion compatibility checks are simple to implement by e.g.
introducing additional nodes in the prgram representation,
but are not considered in our test language.

Examples We illustrate axiom-based rewrites using ax-
ioms presented in later Table 1. Note axiom names follow
the structural changes applied. For example, we have ğ‘ + ğ‘ â‰¡
ğ‘ + ğ‘ : {ğ‘ + ğ‘} = ğ¶ğ‘œğ‘šğ‘šğ‘¢ğ‘¡ğ‘’ ({+}, {ğ‘ + ğ‘}). ğ‘ + ğ‘ + ğ‘ â‰¡ ğ‘ + ğ‘ +
ğ‘ : {ğ‘ +ğ‘ +ğ‘} = ğ¶ğ‘œğ‘šğ‘šğ‘¢ğ‘¡ğ‘’ ({+1}, ğ¶ğ‘œğ‘šğ‘šğ‘¢ğ‘¡ğ‘’ ({+2}, {ğ‘ +ğ‘ +ğ‘}).
Note we refer to different nodes with the same symbol (e.g.,
+2) subscripting them by their order in a DFS traversal of the
expression graph, starting from the unique root. We have
0 â‰¡ ğ‘ âˆ’ ğ‘ : {0} = ğ¶ğ‘ğ‘›ğ‘ğ‘’ğ‘™ ({âˆ’}, {ğ‘ âˆ’ ğ‘}). These can be com-
bined in complex paths, e.g., ğ‘ +ğ‘ â‰¡ ğ‘ +ğ‘ + (ğ‘ âˆ’ğ‘) : {ğ‘ +ğ‘} =
ğ¶ğ‘œğ‘šğ‘šğ‘¢ğ‘¡ğ‘’ ({+}, ğ‘ğ‘œğ‘œğ‘ ({+}, ğ¶ğ‘ğ‘›ğ‘ğ‘’ğ‘™ ({âˆ’}, {ğ‘ + ğ‘ + (ğ‘ âˆ’ ğ‘)}))).
Such axioms are developed for scalars, matrices and vectors,
and include complex rewrites such as distributivity rules and
transpositions. A total of 102 axioms are used in our system.

3.3 Space of Equivalences

We now define the search space being explored in this work,
i.e., the exact space of solutions on which the DNN system
formally operates, and that we sample for training.

Definition 3.9 (Graph of the space of equivalences). Given
a language L. The directed graph of equivalences between
expressions is ğºğ‘’ğ‘ğ‘¢ğ‘–ğ‘£ = âŸ¨ğ‘ ğ‘’ğ‘ğ‘¢ğ‘–ğ‘£, ğ¸ğ‘’ğ‘ğ‘¢ğ‘–ğ‘£âŸ© such that âˆ€ğ‘™ âˆˆ L, ğ‘›ğ‘™ âˆˆ
ğ‘ ğ‘’ğ‘ğ‘¢ğ‘–ğ‘£, and ğ‘’ğ´ğ‘–,ğ‘¥
: ğ‘›ğ‘– â†’ ğ‘› ğ‘— âˆˆ ğ¸ğ‘’ğ‘ğ‘¢ğ‘–ğ‘£ iff ğ‘› ğ‘— â‰¡ ğ´ğ‘– (ğ‘¥, ğ‘›ğ‘– ), âˆ€ğ´ğ‘–
ğ‘›ğ‘–,ğ‘› ğ‘—
in the axiom system and ğ‘¥ a position in ğ‘›ğ‘– where ğ´ğ‘– is appli-
cable.

In other words, the graph has one node per possible expres-
sion in the language L, and a single axiom application leads
to connecting two nodes. We immediately note that ğºğ‘’ğ‘ğ‘¢ğ‘–ğ‘£
is a (possibly infinite) multigraph, and contains circuits.

Theorem 3.10 (Expression equivalence with pathfinding).
Given two expressions ğ‘›ğ‘–, ğ‘› ğ‘— âˆˆ ğ‘ ğ‘’ğ‘ğ‘¢ğ‘–ğ‘£. If there is any path from
ğ‘›ğ‘– to ğ‘› ğ‘— in ğºğ‘’ğ‘ğ‘¢ğ‘–ğ‘£, then ğ‘›ğ‘– â‰¡ ğ‘› ğ‘— .

5

The proof is a direct consequence of Def. 3.9. In this work,
we randomly sample this exact graph to learn how to build
paths between arbitrary expressions. As it is a multigraph,
there will be possibly many different sequences modeled to
prove the equivalence between two expressions. It is suffi-
cient to expose one to prove equivalence.

Corollary 3.11 (Semantics-preserving rewrite sequence).
Any directed path in ğºğ‘’ğ‘ğ‘¢ğ‘–ğ‘£ is a semantics-preserving rewrite
sequence between the expressions, described by the sequence of
axioms and expression position labeling the edges in this path.
This sequence forms the proof of equivalence.

We believe that ensuring there are possibly (usually) many
ways to compute a proof of equivalence in our specific frame-
work is key to enable the DNN approach to learn automat-
ically the pathfinding algorithm for building such proofs.
Other more compact representations of this space of equiva-
lences are clearly possible, including by folding nodes in the
equivalence graph for structurally-similar expressions and
folding equivalent paths between nodes. When building e.g.
a deterministic algorithm for pathfinding, such space size
reduction would bring complexity benefits [11, 31]. We be-
lieve that for the efficient deployment of graph-to-sequence
systems, exposing significant redundancy in the space fa-
cilitates the learning process. We also alleviate the need to
reason on the properties of this space to find an efficient
traversal heuristic.

4 Samples Generation
The careful creation of our training dataset is key: as we let
the DNN learn by example only what the axioms are and
when they are applicable in the structure of a program, we
must carefully sample the space of equivalences to ensure
appropriate distributions of the examples. We produce a final
dataset of tuples (ğ‘ƒ1, ğ‘ƒ2, ğ‘†), a pair of input programs and a
possible rewrite rule sequence that proves the pair equiva-
lent. Duplicates are removed such that all samples have a
unique ğ‘ƒ1. From this dataset, we create 1,000,000 training
samples, 10,000 validation samples, and 10,000 test samples.
We outline below its generation principles; extensive details
and the algorithms used are presented in section B.1.

Random sample generation Deep learning typically re-
quires large training sets to be effectively deployed, hence
we developed a process to automate sample generation. We
specifically use randomized program generation algorithms
that are inspired by a given language grammar. By randomly
choosing between production rulse, one can build random
parse trees by simply iterating the grammar. The leaves ob-
tained will form a sentence accepted by the language, i.e.,
a program [15]. We limit to programs of 50 nodes in the
program tree (or AST), with a maximal tree depth of 7. We

pe-graph2axiom, June, 2021, ArXiV

Steve Kommrusch, ThÃ©o Barollet and Louis-NoÃ«l Pouchet

assert that our random production rule procedure has a non-
zero probability of producing any program allowed by the
grammar for our datasets.

We produce equivalent program samples by pseudo-randomly

applying axioms on one randomly generated program to
produce a rewrite sequence and the associated equivalent
program. Given a randomly selected node in the program
graph, our process checks which axiom(s) can be applied.
E.g., the +ğ‘š operator may have the Commute axiom cate-
gory applied, or it may have the Transpose axiom category
applied, which affects the operatorâ€™s children.

Final experimental dataset: AxiomStep10 To train our
network to produce one axiom step at a time, as described
in Sec. 2, AxiomStep10 has a single axiom in each output se-
quence ğ‘†. For a complete proof ğ‘† : ğ´1(ğ´2(...) in a (ğ‘ƒ1, ğ‘ƒ2, ğ‘†)
we generated made of N axioms, we then create N training
examples for the network: (ğ‘ƒ1, ğ‘ƒ2, ğ´ğ‘ ) the first intermediate
step by applying the first axiom, then (ğ´ğ‘ (ğ‘ƒ1), ğ‘ƒ2, ğ´ğ‘ âˆ’1),
etc. We limit proof length to 10 axioms in our experiments
(hence AxiomStep10). Test samples only have the original
and target program and the network proposes axioms which
create intermediate programs towards the proof, fed back to
the system.

Table 1. Distribution for the 14 axiom categories in Axiom-
Step10 test set. Considering scalars (a, b, ...), vectors ((cid:174)ğ‘£, (cid:174)ğ‘¤,
...) and matrices (A, B, ...) types combinations, 147 distinct
axioms are represented.

Axiom Category Example axiom(s)

Samples with

(A-A)â†’O,(b/b)â†’1
Cancel
((cid:174)ğ‘£ - (cid:174)ğ‘œ) â†’ (cid:174)ğ‘£
NeutralOp
ğ´ğ‘¡ğ‘¡
â†’ ğ´, 1/1/xâ†’x
DoubleOp
(A*O)â†’O, (b*0)â†’0
AbsorbOp
(a + b) â†’ (b + a)
Commute
(a + b)c â†’ ac + bc
DistributeLeft
a(b + c) â†’ ab + ac
DistributeRight
ab + ac â†’ a(b+c)
FactorLeft
ac + bc â†’ (a+b)c
FactorRight
a(bc) â†’ (ab)c
AssociativeLeft
AssociativeRight (ab)c â†’ a(bc)
FlipLeft
FlipRight
Transpose

-((cid:174)ğ‘£ - (cid:174)ğ‘¤) â†’ (cid:174)ğ‘¤ âˆ’ (cid:174)ğ‘£
a/(b/c) â†’ a(c/b)
(ğ´ğµ)ğ‘¡ â†’ ğµğ‘¡ ğ´ğ‘¡ ,

13.8%
40.0%
7.3%
30.3%
48.6%
36.3%
27.8%
6.1%
9.0%
46.3%
43.1%
8.4%
26.1%
11.1%

In
Datasets to study generalizability and robustness
order to study our modelâ€™s ability to generalize, we have
created alternate datasets on which to train and test models
which are summarized in table 2. WholeProof10 will help us
contrast learning approaches. This dataset has the complete
proof sequence ğ‘† made of ğ‘ â‰¥ 1 axioms as reference output
for a program pair, while for AxiomStep10, ğ‘ = 1. Models
trained on WholeProofX must maintain internal state rep-
resenting the graph transformations that the axioms create.

6

They are not "iterative": a single inference is expected to
produce the complete proof; in contrast to AxiomStep10 for
which a single axiom of the sequence is produced at each
inference step. Training long output sequences can benefit
from complex training approaches such as Professor forcing
[37], but we will show that our AxiomStep10 model general-
izes well with our sequence model training approach.

Table 2. Datasets used for studies in experiments.

Dataset

AST depth AST #nodes

Proof length

Iterative

AxiomStep10
AxiomStep5
WholeProof10
WholeProof5

2-7
2-6
2-7
2-6

2-50
2-25
2-50
2-25

1-10
1-5
1-10
1-5

Yes
Yes
No
No

Complexity of equivalence space Figure 3 provides a view
of the complexity of the equivalence problem we tackle. The
distribution of the dataset per proof length is displayed in
the right chart; the left chart shows by size of bubble the
number of test samples with a given number of semantics-
preserving axioms that may be implemented as the first step
of the proof and the proof length needed.

Figure 3. Distribution of axiom possibilities and proof com-
plexity for test datasets.

There is a large number of proofs possible in our system,
as detailed in Appendix B.3. For example, for proofs of length
5, about 340,000 proofs made only of legal applications of ax-
ioms can be performed on the average sample in our dataset.
Since many programs have multiple possible proofs, about
10,000 different programs can be produced, only one of which
is the target to prove, i.e., randomly drawing a valid 5 axiom
proof on a program known to be 5 axiom steps from the
target has roughly a 1 in 10,000 chance of being a correct
proof of equivalence between the two programs.

Proving Equivalence Between Complex Expressions

pe-graph2axiom, June, 2021, ArXiV

5 Deep Neural Networks for Program

Equivalence

Fig. 2 overviews the entire system architecture including
sample generation, the pe-graph2axiom network, and
the rewrite checker. Key design decisions are presented be-
low.

Graph neural network The sample generation discussed
in section 4 provides input to the Node Initialization module
in Fig. 2 to create the initial state of our graph neural network
[13]. For each node in the program graph, a node will be
initialized in our graph neural network with a value that
encodes the AST level and language token of the program
node. To interconnect the edges we support 9 edge types and
their reverse edges which allows information to move in any
direction necessary: 1) left child of binary op, 2) right child
of binary op, 3) child of unary op, 4) root node to program
1, 5) root node to program 2, 6-9) there are 4 edge types for
the node grandchildren (LL, LR, RL, RR). The node states
and edge adjacency matrix represent the initial graph neural
network state.

After initialization, the graph neural network iterates 10
times in order to convert the initial node state into the
embeddings needed for rewrite rule generation. Given an
initial hidden state for node ğ‘› of ğ‘¥ğ‘› (0), ğ‘¥ğ‘› (ğ‘¡ + 1) is com-
puted with a learnable function ğ‘“ which combines the cur-
rent hidden state ğ‘¥ğ‘› (0), the edge types ğ‘™ğ‘–ğ‘› [ğ‘›] of edges en-
tering node ğ‘›, the edge types ğ‘™ğ‘œğ‘¢ğ‘¡ [ğ‘›] of edges exiting node
ğ‘›, and the hidden states ğ‘¥ğ‘›ğ‘’ [ğ‘›] of the neighbors of node ğ‘›:
ğ‘¥ğ‘› (ğ‘¡ + 1) = ğ‘“ (ğ‘¥ğ‘› (ğ‘¡), ğ‘™ğ‘–ğ‘› [ğ‘›], ğ‘¥ğ‘›ğ‘’ [ğ‘›] (ğ‘¡), ğ‘™ğ‘œğ‘¢ğ‘¡ [ğ‘›]).

Each edge type has a different weight matrix for learning,
allowing aggregation of information into a given node re-
lated to its function in the full graph of the program. The root
nodeâ€™s initial state along with the edge types connecting it
to the program graph trees allow it to aggregate and transfer
specific information regarding rewrite rules as demonstrated
by our experimental results. This is a novel feature of our
network not used in prior work with GNNs on program
analysis [4, 62].

Graph neural network output to decoder After stepping
the GGNN, the final node values are used by the decoder in
two ways to create rewrite rules. First, the final root node
value ğ‘¥ğ‘Ÿğ‘œğ‘œğ‘¡ (10) is fed through a learnable bridge function to
initialize the LSTMs of the decoder. In this way, the aggre-
gated information of the 2 programs seeds the generation
of rewrite rules. The LSTMs update as each output token ğ‘¦ ğ‘—
is generated with a learnable function based on the current
decoder hidden state â„ğ‘‘
ğ‘— at decoder step ğ‘— and the previous
output token ğ‘¦ ğ‘—âˆ’1 [18]. Second, all nodes in the graph can be
used by an attention layer [8]. The attention layer creates a
context vector ğ‘ ğ‘— which can be used by a learnable function
ğ‘” when computing the probability for generating the ğ‘—th out-
ğ‘— , ğ‘¦ ğ‘—âˆ’1, ğ‘ ğ‘— ).
put token ğ‘ƒ (ğ‘¦ ğ‘— ): ğ‘ƒ (ğ‘¦ ğ‘—

| ğ‘¦ ğ‘—âˆ’1, ğ‘¦ ğ‘—âˆ’2, ..., ğ‘¦0, ğ‘ ğ‘— ) = ğ‘”(â„ğ‘‘

7

Because pe-graph2axiom has a robust output verifica-
tion, we make use of beam search to track up to 10 likely
candidates for proofs of equivalence.

By using the root node only for seeding the initial hid-
den state â„ğ‘‘
0 of the decoder, the weights associated with its
connections to the program graphs for ğ‘ƒ1 and ğ‘ƒ2 learn to
represent the information necessary for the rewrite rule se-
quence. In parallel, after the graph neural network iterations
complete, the final embedding for all the nodes in the graphs
for ğ‘ƒ1 and ğ‘ƒ2 are only used by the attention network, so
their final embedding represents information useful during
rewrite rule generation.

Intermediate program generation pe-graph2axiom
applies the axiom and program node chosen by the neural
network token generator to the input program to create an
intermediate program ğ‘ƒ â€² on the path from ğ‘ƒ1 to ğ‘ƒ2. If this
program is equal to ğ‘ƒ2, then our axiom path is complete,
otherwise the new pair ğ‘ƒ â€², ğ‘ƒ2 is inferred to determine the
next axiom step.

Incremental versus non-incremental sequence produc-
tion The models we train on the AxiomStep5, WholeProof10,
and WholeProof5 datasets have the same neural network
hyperparemeters as the AxiomStep10 data model. However,
the models for WholeProof10 and WholeProof5 are trained
to output the entire sequence of axioms needed to prove the
2 programs identical, hence these models do not make use
of the intermediate program generation and instead have a
component which checks whether the full sequence of ax-
ioms legally transforms ğ‘ƒ1 into ğ‘ƒ2. We encode the path to the
AST node an which to apply an axiom using â€™leftâ€™ and â€™rightâ€™
tokens which specify the path from the current program
root node. This encoding is sufficient for the iterative model
and necessary to allow the non-iterative model to identify
nodes which may not have been in the initial AST for ğ‘ƒ1.
The non-iterative models must learn a representation in the
LSTM network to allow them to track AST transformations
as they are generated.

6 Experimental Results
We now present extensive experimental results, and compare
the quality of several neural network approaches to address
the problem of program equivalence. We have proceeded
incrementally for fine-tuning the final system design, and
report on several of these design points below.

We focus our experiments below on 4 key questions: 1) Is
performance related to input program size? 2) Is performance
related to proof length? 3) Is the incremental, per-axiom ap-
proach more generalizable than producing the full sequence
in a single inference step? And 4) Is performance consis-
tent across a range of datasets, including human-written
examples?

pe-graph2axiom, June, 2021, ArXiV

Steve Kommrusch, ThÃ©o Barollet and Louis-NoÃ«l Pouchet

Implementation setup We developed the neural network
system presented in the OpenNMT-py system [34], adding
on a new encoder based on a prior implementation of gated
graph neural networks [40]. For our training and evalua-
tion experiments, we use systems with Intel Xeon 3.6GHz
CPUs and 6GB GeForce GTX 1060 GPUs. During training,
we save a model snapshot every 50,000 iterations and score
the accuracy the model achieved on the validation dataset.
Graphs showing that validation accuracy plateaus at 200,000
to 300,000 iterations are provided in section E. We run each
model twice and evaluate the test set using the saved model
which achieved the highest validation score.

Evaluation procedure and neural network alternatives
The benefits of key components of our neural network model
are studied in table 3. The bidirectional RNN model is similar
to state-of-the-art sequence-to-sequence models used for
program repair [18]. The results for the graph-to-sequence
model without attention show the benefit of providing the
node information during the axiom generation process.
Table 3. pe-graph2axiom mini ablation study.

Model description

Beam width
1

5

2

Bidirectional RNN seq-to-seq with attention
Graph-to-sequence w/o attention
pe-graph2axiom model

48
73
76

62
81
84

71
87
90

10

75
90
93

Our final design was influenced by explorations we per-
formed on varied models, datasets, and hyperparameters
such as LSTM layers and graph neural network parameters.
In relation to the modelâ€™s ability to learn a representation of
the proof sequence, we note that our GGNN initialization us-
ing the root node connection to the decoder outperforms the
embedding learned by a bidirectional RNN model. Also, we
found that averaging the embedding of all graph nodes had
about 10% lower accuracy than using the more specific root
node information. Numerous additional results are reported
in Suppl. material E.

Generalizing across different datasets We specifically
look at the generalization potential for our models by study-
ing their success rate as a function of the input program
complexity, represented as the AST depth, in Table 4, and as
a function of the output complexity, represented by the proof
length in Table 5, all using a beam size of 10. We designed
our datasets in Sec. 4 to study how well pe-graph2axiom
generalizes and to assess we are not overfitting on training
data. Extensive in-depth additional experimental results are
presented in Suppl. Material E, we summarize key results
only below.

Table 4 illustrates the ability of a model trained on Ax-
iomStep5 (i.e., limited to proofs of length 5) to perform well
when evaluated on the more complex AxiomStep10, which
includes proofs of unseen length of up to 10. The robustness

8

Table 4. Performance vs. AST size: counts and percentage
pass rates.

Testset
Sample Count

Model trained
on AxiomStep5

Model trained
on AxiomStep10

AST depth

AS5

AS10

2-6
7
All

10000
0
10000

6865
3135
10000

AS5

99
n/a
99

AS10

93
86
90

AS5

99
n/a
99

AS10

94
92
93

to the input program complexity is illustrated with the 86%
pass rate on AST depth 7, for the model trained on Axiom-
Step5 which never saw programs of depth 7 during training.
Table 5 compares the results of our 4 models, each trained
on one of our 4 datasets, and evaluated with the test set of all
4 datasets. The models all have identical hypermeter settings.
We observe the inability of models trained to output the
whole proof to generalize to proofs of higher length (WP5
model on AS10/WP10), with near zero success rate. However,
per-axiom models (AS5 and AS10) show potential for gen-
eralization to proof length: AS5 model performs well when
evaluated on AS10, showing the ability to produce proofs of
length/complexity unseen in training. Overall, the success
rate degrades gracefully with proof length, bottoming at 66%
for AS10 for proofs of length 10.

6.1 WholeProof Models: Language Complexity and

Performance

Table 6 shows the result of 12 different experiments and
designs specifically for the WholeProof5 models. In particu-
lar, we incrementally increase the problem complexity from
rows 1 to 10, increasing the number of Operators that can be
used in any input program, of Axioms used in the rewrite se-
quence, of Operands in any input program, of the maximal
number of nodes in an input program graph (the Program
length, directly influencing the size of the graph network),
and the Rewrite rule length, which contains the description
of paths from the root node to reach the position of appli-
cation of an axiom, this is directly related to the maximal
graph height, itself determined by the maximal program size.
Details on each row are provided in Supplementary Material.
We specifically compare against a sequence-to-sequence
(S2S) approach, to quantify the gains brought by employing
graph-to-sequence (G2S). When the space is small enough,
S2S still performs well, especially using aggressive beam
search. We recall that by design of our system testing the
correctness of one sequence is trivial and deterministic, so
one can easily use large beam sizes without any correctness
impact nor major performance penalty during inference. For
example, inference of beam 1 is about 15ms for our most
complex networks, but beam 10 only takes 16ms. Checking
correctness is << 1ms.

Contrasting rows 2 and 3 displays the merits of the G2S
approach for our problem: on this simple problem, in fact G2S
gets near-perfect accuracy already. Progressively increasing
the complexity of the search space, till row 9 and 10, displays

Proving Equivalence Between Complex Expressions

pe-graph2axiom, June, 2021, ArXiV

Table 5. Performance vs. proof length: percentage pass rates.

Axiom
Count in

Model trained on
WholeProof5 (WP5)

Model trained on
WholeProof10 (WP10)

Model trained on
AxiomStep5 (AS5)

Model trained on
AxiomStep10 (AS10)

Proof WP5 WP10 AS5 AS10 WP5 WP10 AS5 AS10 WP5 WP10 AS5 AS10 WP5 WP10 AS5 AS10

1-5
6
7
8
9
10
All

95

95

89
14
0
0
0
0
66

44

44

44
4
1
0
0
0
27

94

94

93
72
63
54
47
34
84

44

44

44
5
2
1
0
0
27

99

99

97
81
67
54
35
24
87

99

99

98
88
81
75
64
57
90

99

99

98
90
83
73
63
46
93

99

99

98
93
87
82
74
66
93

s
r
o
t
a
r
e
p
O
#

s

m
o
i
x
A
#

s
d
n
a
r
e
p
O
#

h
t
g
n
e
l

s
e
l
u
r

e
t
i
r

w
e
R
1-5

h
t
g
n
e
l

m
a
r
g
o
r
P
3-19

)
S
2
S
(
q
e
s
2
q
e
s

r
o

)
S
2
G

(
q
e
s
2
h
p
a
r
G
S2S

e
z
i
s

t
e
s

g
n
n

i

i
a
r
T
80,000

g
n

i
h
c
t
a
m

1
h
t
d
i
w
m
a
e
b
h
t
i

t
n
e
c
r
e
w
P
90.0%

ID
1

2

3
4
5
7
8
9
10
11
12

1

2

1

2

10

Description
Rewrite sequence is only single Commute, uses sequence-to-
sequence model
Rewrite sequence is exactly 2 Commutes, uses sequence-to-
sequence model
1
Rewrite sequence exactly 2 Commutes
1
Rewrite sequence exactly 3 Commutes
1
Rewrite sequence 1 to 3 Commutes
5
Commute, Noop, Cancel, Distribute Left, Distribute Right
5
Scalars, Vectors, and Matrixes
13
13 Axioms
13
Rewrite sequence or Not_equal
13
Test sequence-to-sequence
15
Add loop axioms
Table 6. Results for various language complexities studied, on non-incremental models (WholeProof).

80,000
80,000
180,000
180,000
250,000
400,000
500,000
400,000
400,000

98.9%
91.4%
97.1%
93.1%
88.3%
85.5%
79.8%
59.8%
83.8%

5-24
7-45
3-45
3-45
3-30
3-30
3-30
3-30
3-30

3-10
5-15
1-15
1-15
1-25
1-25
1-25
1-25
1-25

G2S
G2S
G2S
G2S
G2S
G2S
G2S
S2S
G2S

2
2
2
4
16
16
16
16
18

10
10
10
12
20
20
20
20
20

80,000

80.3%

5-24

3-10

S2S

10

g
n

i
h
c
t
a
m

0
1
h
t
d
i
w
m
a
e
b
h
t
i

t
n
e
c
r
e
w
P
96.2%

96.5%

99.8%
99.0%
99.2%
97.4%
95.6%
95.5%
93.8%
81.1%
94.7%

a slow but steady decrease in quality, while still maintaining
excellent scores near or above 95% with beam 10. To reassess
the limits of a sequence-to-sequence approach, row 9 and 11
can be constrasted: they operate on the same search space,
but S2S peaks at 81% accuracy, while G2S reaches 95%.

Row 10 displays the result when learning using also sam-
ples of non-equivalent programs, using the â€œempty pathâ€
symbol Not_equal. We evaluated this system to measure
the impact of training on only equivalent programs vs. also
sampling pairs of unconnected nodes in the equivalences
graph. We recall that by design, if no rewrite rule produced
is verified as correct, our system outputs the programs are
not equivalent. In other words, whichever the sequence(s)
produced by the network, if the two input programs are
non-equivalent, the system will always output they are not
equivalent: no equivalence sequence produced can be veri-
fied as correct. So training on only equivalent programs is
clearly sensible for such system; furthermore as shown in
row 10 vs. 9, even increasing the training set size, training us-
ing non-equivalent programs seem to lower the performance
slightly.

Human written test expressions from Khan academy
exercises Unfortunately there is a dearth of existing large
reference datasets for equivalence of linear algebra expres-
sions, which justified our careful dataset creation approach
in Sec. 4 and their upcoming public release. However nu-
merous math exercises involve exactly this problem, and can

9

provide small but human-written datasets. We solve all of
the matrix expression equivalence programs from 2 relevant
Khan academy modules designed to test studentâ€™s knowl-
edge of matrix algebra [33]. Our AxiomStep10 model is able
to correctly prove all 15 equivalent pairs from the modules
with beam width 1 and wider. With a beam width of 10, the
WholeProof10 model proved 12. An example problem solv-
able by AxiomStep10 but not WholeProof10 is: ğ‘ (1ğ´ + ğµ) =
ğ‘ğµ + ğ‘ğ´ which can be proven by applying the rewrite rules
NeutralOp, DistributeRight, and Commute to the proper
nodes. The WholeProof10 model mostly fails because it was
not trained on how to apply repeated transformations at
the same point in the AST. This suggests AxiomStep10 has
generalized well to these hand-written problems.

7 Related Work
Theorem provers The problem of equivalence as we for-
mulated may be solved by other (smart) brute-force ap-
proaches, where a problem is solved by pathfinding. This
ranges from theoreom proving systems like Coq [14] which
supports the formal framework for equivalence we describe
in this paper, to (Approximate Probabilistic) Model Check-
ing [17, 20, 27], where a program equivalence system can
also be built, e.g. [19, 46, 52, 58]. Our contribution is not in
the formal definition of program equivalence we presented,
semantics-preserving rewrite systems have been studied,

pe-graph2axiom, June, 2021, ArXiV

Steve Kommrusch, ThÃ©o Barollet and Louis-NoÃ«l Pouchet

e.g. [41, 50, 57]. But understanding why this particular for-
malism was well suited to deep learning graph-to-sequence
systems was key. The merits of stochastic search to accel-
erate such systems has been demonstrated, e.g. [24, 27, 45].
The novelty of our approach is to develop carefully crafted
graph-to-sequence neural networks to automatically learn
an efficient pathfinding heuristic for this problem. Our ap-
proach is potentially applicable in these areas too, however
training scalability can become a challenge if increasing the
input representation size excessively. Theorem provers us-
ing deep learning have recently started to be investigated,
Aygun et al. [7] developed a graph neural network system
for automatic proof generation. Wu et al. [60] explores the
ability of theorem provers using GNNs, TreeLSTMs, and
BagOfWords architectures to generalize and solve proofs
with lengths up to 7 axioms and found that GNNs performed
the best of the architectures studied when more complex
proofs were required. While our model works in a slightly
different problem space, we study the ability of our mod-
els to generalize on proofs with lengths up to 10, with 14
different rewrite rules acting on 147 distinct axioms. These
frameworks could also be used to prove equivalence between
symbolic expressions, as theorem provers.

Static program equivalence Algorithms for static pro-
gram equivalence have been developed, e.g. [2, 11, 29, 56].
These approaches typically restrict to demonstrating the
equivalence of different schedules of the operations, possibly
dynamically [10]. In this work we target graph-modifying
rewrites (and therefore which alter the operation count).
Barthou et al. [2, 11] have developed techniques to recog-
nize algorithm templates in programs. These approaches
are restricted to static/affine transformed programs. Karfa et
al. also designed a method that works for a subset of affine
programs using array data dependence graphs (ADDGs) to
represent input and transforming behaviors. Operator-level
equivalence checking provides the capability to normalize
expressions and establish matching relations under algebraic
transformations [32]. Mansky and Gunter used the TRANS
language [30] to represent transformations. The correctness
proof implemented in the verification framework [43] is ver-
ified by the Isabelle [48] proof assistant. Other works also
include translation validation [35, 47].

Program analysis with machine learning Numerous
prior work has employed (deep) machine learning for pro-
gram analysis, e.g. [3, 5, 12, 36, 49, 54]. code2vec [5] teaches
a method for creating a useful embedding vector that sum-
marizes the semantic meaning of a snippet of code. Program
repair approaches, e.g. [18, 54] are deployed to automatically
repair bugs in a program. Output accuracies of up to 20%
on the test set is reported, using sequence-to-sequence mod-
els. Wang et al. [59] learns to extract the rules for Tomita
grammars [53] with recurrent neural networks. The learned

10

network weights are processed to create a verifiable deter-
ministic finite automata (DFA) representation of the learned
grammar. This work demonstrates that deterministic gram-
mars can be learned with RNNs, which we rely on.

Graph Neural Networks Graph neural networks [51, 61]
use machine learning to analyze a set of nodes and edges
for patterns related to a target problem. Using a graph-to-
sequence network with attention has been analyzed for nat-
ural language processing [13]. Allamanis et al. use graph
neural networks to analyze code sequences and add edge
types representing LastUse, ComputedFrom, and LastWrite
to improve the systemâ€™s ability to reason about the code
[4]. Their work achieves 84% accuracy on correcting vari-
able misuse cases and provides insights to useful edge types.
Structure2vec [62] uses a graph neural network to detect
binary code similarity. Structure2vec uses a graph neural net-
work to learn an embedding from a annotated control flow
graph (ACFG) of a program. This learning process targets the
embedding so that equivalent programs will have equivalent
embeddings, reporting precision scores of 84% and 85% on
various test datasets for correctly predicting program equiv-
alence. It only outputs a probability of equivalence, and not
a verifiable proof, which is sufficient in their context.

The G2SKGE model [39] has a similar graph network struc-
ture which uses a node embedding (which they refer to as
an information fusion mechanism) in order to predict rela-
tionships between nodes. This technique of using a neural
network to understand and predict node interelationships is
common to our approach.

8 Conclusion
In this work, we presented pe-graph2axiom, the first
graph-to-sequence neural network system to generate verifi-
able axiomatic proofs (via rewrite rules) for equivalence for
a class of symbolic programs. Evaluated on a rich language
for linear algebra expressions, this system produces correct
proofs of up to 10 axioms in length in 93% of the 10,000 equiv-
alent cases evaluated. We believe the performance of our
approach comes in part from using graph neural networks
for what they aim to excel at: learning efficient heuristics
to quickly find paths in a graph; and the observation that
program equivalence can be cast as a path-based solution
that is efficiently found by such networks.

Acknowledgments
This work was supported in part by the U.S. National Science
Foundation award CCF-1750399.

References
[1] Umair Z Ahmed, Pawan Kumar, Amey Karkare, Purushottam Kar,
and Sumit Gulwani. 2018. Compilation error repair: for the student
programs, from the student programs. In Proceedings of the 40th In-
ternational Conference on Software Engineering: Software Engineering

Proving Equivalence Between Complex Expressions

pe-graph2axiom, June, 2021, ArXiV

Education and Training. ACM, 78â€“87.

[2] Christophe Alias and Denis Barthou. 2004. On the recognition of
algorithm templates. Electronic Notes in Theoretical Computer Science
82, 2 (2004), 395â€“409.

[3] Miltiadis Allamanis, Earl T. Barr, Premkumar Devanbu, and Charles
Sutton. 2018. A Survey of Machine Learning for Big Code and Nat-
uralness. ACM Comput. Surv. 51, 4, Article 81 (July 2018), 37 pages.
https://doi.org/10.1145/3212695

[4] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi.
2018. Learning to Represent Programs with Graphs. In 6th Inter-
national Conference on Learning Representations, ICLR 2018, Vancou-
ver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.
https://openreview.net/forum?id=BJOFETxR-

[5] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019.
Code2Vec: Learning Distributed Representations of Code. Proc. ACM
Program. Lang. 3, POPL, Article 40 (Jan. 2019), 29 pages.
https:
//doi.org/10.1145/3290353

[6] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel
Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel,
and Wojciech Zaremba. 2017. Hindsight Experience Replay. In Ad-
vances in Neural Information Processing Systems 30, I. Guyon, U. V.
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett (Eds.). Curran Associates, Inc., 5048â€“5058. http://papers.nips.cc/
paper/7090-hindsight-experience-replay.pdf

[7] Eser AygÃ¼n, Zafarali Ahmed, Ankit Anand, Vlad Firoiu, Xavier
Glorot, Laurent Orseau, Doina Precup, and Shibl Mourad. 2020.
Learning to Prove from Synthetic Theorems.
arXiv e-prints,
Article arXiv:2006.11259 (June 2020), arXiv:2006.11259 pages.
arXiv:2006.11259 [cs.LO]

[8] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural
machine translation by jointly learning to align and translate. arXiv
preprint arXiv:1409.0473 (2014).

[9] Kshitij Bansal, Sarah Loos, Markus Rabe, Christian Szegedy, and Stew-
art Wilcox. 2019. HOList: An Environment for Machine Learning
of Higher Order Logic Theorem Proving. In Proceedings of the 36th
International Conference on Machine Learning (Proceedings of Ma-
chine Learning Research, Vol. 97), Kamalika Chaudhuri and Ruslan
Salakhutdinov (Eds.). PMLR, Long Beach, California, USA, 454â€“463.
http://proceedings.mlr.press/v97/bansal19a.html

[10] Wenlei Bao, Sriram Krishnamoorthy, Louis-NoÃ«l Pouchet, Fabrice
Rastello, and Ponnuswamy Sadayappan. 2016. Polycheck: Dynamic
verification of iteration space transformations on affine programs. In
ACM SIGPLAN Notices, Vol. 51. ACM, 539â€“554.

[11] Denis Barthou, Paul Feautrier, and Xavier Redon. 2002. On the equiva-
lence of two systems of affine recurrence equations. In Euro-Par 2002
Parallel Processing.

[12] Rohan Bavishi, Michael Pradel, and Koushik Sen. 2017. Context2Name:
A Deep Learning-Based Approach to Infer Natural Variable Names
from Usage Contexts. http://tubiblio.ulb.tu-darmstadt.de/101419/

[13] Daniel Beck, Gholamreza Haffari, and Trevor Cohn. 2018. Graph-to-
Sequence Learning using Gated Graph Neural Networks. In Proceedings
of the 56th Annual Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) (Melbourne, Australia). Association
for Computational Linguistics, 273â€“283. http://aclweb.org/anthology/
P18-1026

[14] Yves Bertot and Pierre CastÃ©ran. 2013. Interactive theorem proving and
program development: Coqâ€™Art: the calculus of inductive constructions.
Springer Science & Business Media.

[15] Pavol Bielik, Veselin Raychev, and Martin Vechev. 2016. PHOG: Proba-
bilistic Model for Code. In Proceedings of The 33rd International Confer-
ence on Machine Learning (Proceedings of Machine Learning Research,
Vol. 48), Maria Florina Balcan and Kilian Q. Weinberger (Eds.). PMLR,
New York, New York, USA, 2933â€“2942. http://proceedings.mlr.press/
v48/bielik16.pdf

[16] Joseph Tobin Buck and Edward A Lee. 1993. Scheduling dynamic
dataflow graphs with bounded memory using the token flow model.
In 1993 IEEE international conference on acoustics, speech, and signal
processing, Vol. 1. IEEE, 429â€“432.

[17] Jerry R Burch, Edmund M Clarke, Kenneth L McMillan, David L Dill,
and Lain-Jinn Hwang. 1992. Symbolic model checking: 1020 states
and beyond. Information and computation 98, 2 (1992), 142â€“170.
[18] Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-NoÃ«l Pouchet,
Denys Poshyvanyk, and Martin Monperrus. 2019.
SequenceR:
Sequence-to-Sequence Learning for End-to-End Program Repair. IEEE
Transactions on Software Engineering (2019). https://doi.org/10.1109/
TSE.2019.2940179

[19] Edmund Clarke, Daniel Kroening, and Karen Yorav. 2003. Behavioral
consistency of C and Verilog programs using bounded model check-
ing. In Proceedings 2003. Design Automation Conference (IEEE Cat. No.
03CH37451). IEEE, 368â€“371.

[20] Edmund M Clarke, Orna Grumberg, and David E Long. 1994. Model
checking and abstraction. ACM transactions on Programming Lan-
guages and Systems (TOPLAS) 16, 5 (1994), 1512â€“1542.

[21] Nachum Dershowitz. 1985. Computing with rewrite systems. Infor-

mation and Control 65, 2-3 (1985), 122â€“157.

[22] Alhussein Fawzi, Mateusz Malinowski, Hamza Fawzi, and Omar Fawzi.
2019. Learning dynamic polynomial proofs.
In Advances in Neu-
ral Information Processing Systems 32, H. Wallach, H. Larochelle,
A. Beygelzimer, F. dâ€™AlchÃ© Buc, E. Fox, and R. Garnett (Eds.).
Curran Associates, Inc., 4179â€“4188.
http://papers.nips.cc/paper/
8671-learning-dynamic-polynomial-proofs.pdf

[23] Benny Godlin and Ofer Strichman. 2008. Inference rules for proving
the equivalence of recursive procedures. Acta Informatica 45, 6 (2008),
403â€“439.

[24] Vibhav Gogate and Pedro Domingos. 2012. Probabilistic theorem

proving. arXiv preprint arXiv:1202.3724 (2012).

[25] Robert Goldblatt and Marcel Jackson. 2012. Well-structured program
equivalence is highly undecidable. ACM Transactions on Computational
Logic (TOCL) 13, 3 (2012), 26.

[26] Ian J. Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep
http://www.

Learning. MIT Press, Cambridge, MA, USA.
deeplearningbook.org.

[27] Thomas HÃ©rault, Richard Lassaigne, FrÃ©dÃ©ric Magniette, and Sylvain
Peyronnet. 2004. Approximate probabilistic model checking. In In-
ternational Workshop on Verification, Model Checking, and Abstract
Interpretation. Springer, 73â€“84.

[28] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term

memory. Neural computation 9, 8 (1997), 1735â€“1780.

[29] Guillaume Iooss, Christophe Alias, and Sanjay Rajopadhye. 2014. On
program equivalence with reductions. In International Static Analysis
Symposium. Springer, 168â€“183.

[30] Sara Kalvala, Richard Warburton, and David Lacey. 2009. Program
transformations using temporal logic side conditions. ACM Trans. on
Programming Languages and Systems (TOPLAS) 31, 4 (2009), 14.
[31] Donald M Kaplan. 1969. Regular expressions and the equivalence of

programs. J. Comput. System Sci. 3, 4 (1969), 361â€“386.

[32] Chandan Karfa, Kunal Banerjee, Dipankar Sarkar, and Chittaranjan
Mandal. 2013. Verification of loop and arithmetic transformations of
array-intensive behaviors. IEEE Trans. on Computer-Aided Design of
Integrated Circuits and Systems 32, 11 (2013), 1787â€“1800.

[33] Sal Khan. 2020.

Properties of matrix multiplication.

Academy (accessed May 20, 2020)
//www.khanacademy.org/math/precalculus/x9e81a4f98389efdf:
matrices/x9e81a4f98389efdf:properties-of-matrix-multiplication/a/
properties-of-matrix-multiplication

(May 2020).

Khan
https:

[34] Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexan-
der M. Rush. 2017. OpenNMT: Open-Source Toolkit for Neural
Machine Translation. In Proc. ACL.
https://doi.org/10.18653/v1/
P17-4012

11

pe-graph2axiom, June, 2021, ArXiV

Steve Kommrusch, ThÃ©o Barollet and Louis-NoÃ«l Pouchet

[49] Veselin Raychev, Martin Vechev, and Andreas Krause. 2015. Predicting
Program Properties from "Big Code". In Proceedings of the 42Nd Annual
ACM SIGPLAN-SIGACT Symposium on Principles of Programming Lan-
guages (Mumbai, India) (POPL â€™15). ACM, New York, NY, USA, 111â€“124.
https://doi.org/10.1145/2676726.2677009

[50] Uday S Reddy. 1989. Rewriting techniques for program synthesis.
In International Conference on Rewriting Techniques and Applications.
Springer, 388â€“403.

[51] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner,
and Gabriele Monfardini. 2009. The Graph Neural Network Model.
IEEE Transactions on Neural Networks 20 (2009), 61â€“80.

[52] Bernhard Steffen. 1991. Data flow analysis as model checking. In
International Symposium on Theoretical Aspects of Computer Software.
Springer, 346â€“364.

[53] M. Tomita. 1982. Dynamic Construction of Finite Automata from
examples using Hill-climbing. In Proceedings of the Fourth Annual
Conference of the Cognitive Science Society. Ann Arbor, Michigan, 105â€“
108.

[54] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta,
Martin White, and Denys Poshyvanyk. 2019. An Empirical Study on
Learning Bug-Fixing Patches in the Wild via Neural Machine Transla-
tion. ACM Trans. Softw. Eng. Methodol. 28, 4, Article 19 (Sept. 2019),
29 pages. https://doi.org/10.1145/3340544

[55] Sven Verdoolaege, Gerda Janssens, and Maurice Bruynooghe. 2009.
Equivalence checking of static affine programs using widening to
handle recurrences. In Computer aided verification. Springer, 599â€“613.
[56] Sven Verdoolaege, Gerda Janssens, and Maurice Bruynooghe. 2012.
Equivalence checking of static affine programs using widening to han-
dle recurrences. ACM Trans. on Programming Languages and Systems
(TOPLAS) 34, 3 (2012), 11.

[57] Eelco Visser. 2004. Program transformation with Stratego/XT.

In

Domain-specific program generation. Springer, 216â€“238.

[58] Willem Visser, Klaus Havelund, Guillaume Brat, SeungJoon Park, and
Flavio Lerda. 2003. Model checking programs. Automated software
engineering 10, 2 (2003), 203â€“232.

[59] Qinglong Wang, Kaixuan Zhang, Alexander G. Ororbia, II, Xinyu Xing,
Xue Liu, and C. Lee Giles. 2018. An Empirical Evaluation of Rule
Extraction from Recurrent Neural Networks. Neural Comput. 30, 9
(Sept. 2018), 2568â€“2591. https://doi.org/10.1162/neco_a_01111
[60] Yuhuai Wu, Albert Jiang, Jimmy Ba, and Roger Grosse. 2020. INT:
An Inequality Benchmark for Evaluating Generalization in Theo-
rem Proving. arXiv e-prints, Article arXiv:2007.02924 (July 2020),
arXiv:2007.02924 pages. arXiv:2007.02924 [cs.AI]

[61] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi
Zhang, and Philip S. Yu. 2019. A Comprehensive Survey on Graph
Neural Networks. CoRR abs/1901.00596 (2019). arXiv:1901.00596
http://arxiv.org/abs/1901.00596

[62] Xiaojun Xu, Chang Liu, Qian Feng, Heng Yin, Le Song, and Dawn Song.
2017. Neural Network-based Graph Embedding for Cross-Platform
Binary Code Similarity Detection. In Proceedings of the 2017 ACM
SIGSAC Conference on Computer and Communications Security (Dallas,
Texas, USA) (CCS â€™17). ACM, New York, NY, USA, 363â€“376. https:
//doi.org/10.1145/3133956.3134018

[35] Sudipta Kundu, Zachary Tatlock, and Sorin Lerner. 2009. Proving
optimizations correct using parameterized program equivalence. ACM
SIGPLAN Notices 44, 6 (2009), 327â€“337.

[36] Jeremy Lacomis, Pengcheng Yin, Edward J. Schwartz, Miltiadis Alla-
manis, Claire Le Goues, Graham Neubig, and Bogdan Vasilescu. 2019.
DIRE: A Neural Approach to Decompiled Identifier Naming. In Inter-
national Conference on Automated Software Engineering (ASE â€™19).
[37] Alex M Lamb, Anirudh Goyal ALIAS PARTH GOYAL, Ying Zhang,
Saizheng Zhang, Aaron C Courville, and Yoshua Bengio. 2016.
Professor Forcing: A New Algorithm for Training Recurrent Networks.
In Advances in Neural Information Processing Systems 29, D. D.
Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (Eds.).
Curran Associates, Inc., 4601â€“4609.
http://papers.nips.cc/paper/
6099-professor-forcing-a-new-algorithm-for-training-recurrent-networks.
pdf

[38] Guillaume Lample and FranÃ§ois Charton. 2020. Deep Learning For
Symbolic Mathematics. In International Conference on Learning Repre-
sentations. https://openreview.net/forum?id=S1eZYeHFDS

[39] W. Li, X. Zhang, Y. Wang, Z. Yan, and R. Peng. 2019. Graph2Seq:
Fusion Embedding Learning for Knowledge Graph Completion. IEEE
Access 7 (2019), 157960â€“157971. https://doi.org/10.1109/ACCESS.
2019.2950230

[40] Yujia Li, Richard Zemel, Marc Brockschmidt, and Daniel Tarlow.
2016. Gated Graph Sequence Neural Networks. In Proceedings of
ICLRâ€™16 (proceedings of iclrâ€™16 ed.). https://www.microsoft.com/en-us/
research/publication/gated-graph-sequence-neural-networks/
[41] Dorel Lucanu and Vlad Rusu. 2015. Program equivalence by circular
reasoning. Formal Aspects of Computing 27, 4 (2015), 701â€“726.
[42] Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Ef-
fective Approaches to Attention-based Neural Machine Translation.
In Proceedings of the 2015 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computational Linguistics,
Lisbon, Portugal, 1412â€“1421. https://doi.org/10.18653/v1/D15-1166
[43] William Mansky and Elsa Gunter. 2010. A framework for formal
verification of compiler optimizations. In Interactive Theorem Proving.
Springer.

[44] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and
Distributed Representations of Words and
Jeff Dean. 2013.
Phrases and their Compositionality.
In Advances in Neural
Information Processing Systems 26, C. J. C. Burges, L. Bottou,
M. Welling, Z. Ghahramani, and K. Q. Weinberger (Eds.). Cur-
ran Associates, Inc., 3111â€“3119.
http://papers.nips.cc/paper/
5021-distributed-representations-of-words-and-phrases-and-their-compositionality.
pdf

[45] Andrzej S Murawski and JoÃ«l Ouaknine. 2005. On probabilistic pro-
gram equivalence and refinement. In International Conference on Con-
currency Theory. Springer, 156â€“170.

[46] Kedar S Namjoshi and Robert P Kurshan. 2000. Syntactic program
transformations for automatic abstraction. In International Conference
on Computer Aided Verification. Springer, 435â€“449.

[47] George C Necula. 2000. Translation validation for an optimizing

compiler. ACM SIGPLAN Notices 35, 5 (2000), 83â€“94.

[48] Lawrence C. Paulson. [n.d.]. Isabelle Page. https://www.cl.cam.ac.uk/

research/hvg/Isabelle.

12

Proving Equivalence Between Complex Expressions

pe-graph2axiom, June, 2021, ArXiV

A Appendix

Supplementary Materials:
Learning Axioms to Compute Verifiable
Symbolic Expression Equivalence Proofs
using Graph-to-Sequence Networks

Document Overview
This document supplements the submission Proving Equiva-
lence Between Complex Expressions Using Graph-to-Sequence
Neural Models. We have provided below numerous additional
information for completeness. We also provide access to
anonymized software artifacts to replicate our results. Our
supplementary materials are organized as follows:
â€¢ Appendix B of this document presents the dataset genera-
tion approach we developed.
â€¢ Appendix C of this document presents exhaustively the
language for complex linear algebra expressions we evalaute
on, including the list of all 147 axioms of equivalence we
learned.
â€¢ Appendix D of this document presents additional details
about the neural network architectures we developed.
â€¢ The anonymized url https://gofile.io/d/IvqAnp contains all
trained models evaluated in this paper, including scripts to
train them directly from our datasets, using OpenNMT. It
contains also our code for generating datasets and training
models, the testsets for our 4 key datasets, and key results
files from our testset evaluations.
â€¢ Appendix E of this document presents complementary ex-
perimental results and additional in-depth details on results
presented in the main paper body.

B Dataset generation
B.1 Generation of Examples

Machine learning benefits from large training sets, so in
order to produce this data, we created algorithms that would
generate programs meeting a given language grammar along
with target programs which could be reached by applying a
given axiom set. By creating this process, we could create as
large and varied a dataset as our machine learning approach
required.

Algorithm 1 provides an overview of the full program gen-
eration algorithm. For this generation process, we define a set
of operations and operands on scalars, matrices, and vectors.
For our process, we presume matrix and vector dimensions
are appropriate for the given operation as such dimension
checks are simple to implement and are not considered in our
procedure. Note the token syntax here is exactly the one used
by our system, and is strictly semantically equivalent to the
mathematical notations used to describe these operations,
e.g. 1N is 1.
â€¢ Scalar operations: +s -s *s /s is ns, where is the
unary reciprical and ns is the unary negation.

â€¢ Matrix operations: +m -m *m im nm tm, where im is
matrix inversion, nm negates the matrix, and tm is matrix
transpose.
â€¢ Vector operations: +v -v *s nv, where nv is the unary
negation.
â€¢ Scalars: a b c d e 0 1
â€¢ Matrices: A B C D E O I, where O is the empty matrix
and I is the identity matrix.
â€¢ Vectors: v w x y z o, where o is the empty vector.
â€¢ Summary: 16 operations, 20 terminal symbols

Initially, GenP1 is called with GenP1("+s -s *s /s
+s -s *s /s +s -s *s /s is ns +m -m *m +m
-m *m +m -m *m im nm tm +v -v *v +v -v *v
+v -v *v nv",0.94)" In this initial call binary opera-
tions are repeated so that they are more likely to be created
than unary operations, and the initial probability that a child
of the created graph node will itself be an operation (as op-
posed to a terminal symbol) is set to 94%. Since the algorithm
subtracts a 19% probability for children at each level of the
graph, trees are limited to 7 levels.

Algorithm 1 starts execution by randomly selecting an
operation from the set provided as input. When GenP1 is
called recursively, the operation set is limited such that the
operation produces the correct type as output (scalar, ma-
trix, or vector). Lines 3 through 15 of the algorithm show an
example case where the *s operation is processed. This oper-
ation requires scalar operands. If the probability of children
at this level is met, then GenP1 is called recursively with
only scalar operands available, otherwise a random scalar
operand is chosen.

The text for algorithm 1 does not show the process for all
operations. Certain operations, such as *v, have a variety
of operand types that can be chosen. The *v operand is a
multiplication which produces a vector. As such, ğ´ğ‘£ (matrix
times vector), ğ‘ğ‘£ (scalar times vector), or ğ‘£ğ‘ (vector times
scalar) are all valid options and will be chosen randomly.

After generating a program which follows the grammar
rules of our language, algorithm 2 will produce a new pro-
gram along with a set of rewrite rules which transform the
source program to the target program.

Algorithm 2 receives as input the source program (or sub-
program) along with the path to the current root node
of the source program. If the source program is a terminal
symbol, the algorithm returns with no action taken. Oth-
erwise, the program starts with an operation and the algo-
rithm proceeds to process options for transforming the given
operation. For our wholeproof10 and wholeproof5 datasets,
algorithm 2 is only called once, simplifying the possible node
order and proof complexity. for the axiomstep10 and axiom-
step5 datasets, algorithm 2 is called multiple times, allowing
for the possibility that after a path is chosen for one axiom
any node can be accessed for the next axiom (including the
same node).

As shown on line 10 of the algorithm, when the operation
and children meet the conditions necessary for a rewrite rule

13

pe-graph2axiom, June, 2021, ArXiV

Steve Kommrusch, ThÃ©o Barollet and Louis-NoÃ«l Pouchet

Algorithm 2: GenP2

Result: Second program and transform_sequence
Input
Output : P2

: P1, path

return P1

1 if terminal symbol then
2
3 end
4 op = find operator of P1
5 L = find left operand of P1
6 R = find right operand of P1
7 Lop,LL,LR = operator and operands of left child
8 Rop,RL,RR = operator and operands of right child
9 // Randomly apply transform if allowed
10 if random < 0.5 and ((op == "+v" and (L == "o" or R ==

"o")) or (op == "-v" and R == "o")) then

append path."NeutralOp " to transform_sequence
// Eliminate unnecessary operator and 0 vector
if L == "o" then

return GenP2(R,path)

else

return GenP2(L,path)

11

12

13

14

15

16

end

17
18 end

Algorithm 1: GenP1

Result: Prefix notation of computation with

parenthesis
: Ops, P
Input
Output : (op L R) or (op L)

1 op = select randomly from Ops
2 // Create subtree for chosen op
3 if op == "*s" then
4

if random < P then

L = GenP1("+s -s *s /s +s -s *s /s is ns",P-0.19)

else

L = select random scalar operand

end
if random < P then

R = GenP1("+s -s *s /s +s -s *s /s is ns",P-0.19)

else

R = select random scalar operand

5

6

7

8

9

10

11

12

13

end
return (op L R)

14
15 end
16 // Other ops may have more complex options for

children types.

17 // (For example, "*m" may have a matrix multiplied by

a scalar or matrix)

18 ...

14

(in this case NeutralOp), the rule is applied with some
probability (in this case 50%). Note that before processing a
node, the left and right operands are further analyzed to de-
termine their operators and operands as well (or âŠ¥ if the child
is a terminal). Processing the left and right operands allows
for complex axioms to be applied, such as distribution or fac-
torization. When a rule is applied, the rewrite rule is added to
the rewrite rule sequence and a new target program is gener-
ated for any remaining subtrees. When creating the rewrite
rules for subtrees, the path variable is updated as rewrites
are done. In the case of NeutralOp, the current node is
being updated, so the path is not changed. But in the case of
the Commute rule, the return would be generated with (op
GenP2(R,path."left ") GenP2(L,path."right
")) which creates rewrite rules for the prior right and left
operands of the op and updates the path used to the new
node positions. In order to analyze nearly equal programs,
illegal rewrites can be optionally enabled; for example, com-
muting a subtraction operation or mutating one operation
into another. In that case, the GenP2 process continues to
create a target program, but transform_sequence is
set to Not_equal.

After these generation algorithms are run, a final data
preparation process is done which prunes the data set for
the learning algorithm. The pruning used on our final data
set insures that the (ğ‘ƒ1, ğ‘ƒ2) program pair total to 100 tokens
or fewer (where a token is an operation or terminal), that
the graph is such that every node is reachable from the root
with a path of length 6 or less, and that there are 10 or fewer
rewrite rules applied. But within these restrictions, we assert
that our random production rule procedure has a non-zero
probability of producing any program allowed by the gram-
mar. Also, the pruning insures that there are no lexically
equivalent programs in the process and removes some of the
cases with fewer than 10 rewrite rules generated to bias the
dataset to longer rewrite sequences. Table 1 details the dis-
tribution of rewrite rules created by the full process. Section
C details all axioms when variable types and operators are
considered.

We produce equivalent program samples by pseudo-randomly

applying axioms on one randomly generated program to
produce a rewrite sequence and the associated equivalent
program. Given a randomly selected node in the program
graph, our process checks which axiom(s) can be applied.
E.g., the +ğ‘š operator can have the Commute axiom applied,
or depending on subtrees it may be allowed to have the Fac-
torleft axiom applied, as discussed in Sec. 6. Generally we
choose to apply or not an operator with 50% probability, so
that pe-graph2axiom is forced to rely on analysis of the
two programs to determine whether an operator is applied
instead of learning a bias due to the local node features.

Proving Equivalence Between Complex Expressions

pe-graph2axiom, June, 2021, ArXiV

Table 7. Counts for equivalence proof possibilities

Proof description

All Possible nodes and axioms
Sample Node + Any Axiom
Sample Node + Legal Axiom
Unique Programs from Sample

1

5933
226
11.2
9.2

Proof length in axioms

2

3

4

5

6

7

3.5E+07
46900
77.8
47.4

2.1E+11
1.5E+07
931
264

1.2E+15
8.8E+09
15812
1574

7.4E+18
5.0E+12
3.4E+05
10052

4.4E+22
3.3E+15
8.2E+06
65176

2.6E+26
2.7E+18
1.8E+08
4.6E+05

B.2 Intermediate program generation

The intermediate program generation algorithm is very sim-
ilar to algorithm 2. For program generation of the target
program, algorithm 2 will check that a node can legally ap-
ply a given rule, apply the rule with some probability, record
the action, and process the remaining program. For interme-
diate program generation, we begin with a ğ‘ƒ1 and a rewrite
rule. We follow the path provided to identify the node, check
that a node can legally accept a rule, apply the rule, and re-
turn the adjusted program. If a rule cannot legally be applied,
ğ‘ƒ1 is not successfully transformed. If a rule can be legally
applied to ğ‘ƒ1, the program is compared lexically to ğ‘ƒ2 and if
they match then equivalence has been proven.

B.3 Complexity of Proving Equivalence

Table 7 shows the complexity of the solution space for our
problem for proofs from our AxiomStep10 test dataset up to
length 7 (deterministically computing all possible programs
requires too many resources for longer proof lengths). The
â€™All possible nodes and axiomsâ€™ row includes the total num-
ber of proofs of a given length available to our problem space.
The entry 5933 for a single axiom represents that for an AST
depth of 7 we have 43 axioms which can be applied to all
63 possible operator nodes and 104 axioms which can be
applied to the 31 nodes which possibly have child operator
nodes themselves: 63*43+31*104=5933. Subsequent columns
can select repeatedly from the same set growing as 59332
to 59337. The â€™sample node + axiom groupâ€™ row is based on
our 10,000 sample test dataset and represents the possible
selection of any of the 14 axiom groups being applied to
any node in the program. The â€™sample node + legal axiomâ€™
row represents only legal node plus legal axiom group being
applied and effectively represents the total number of pro-
grams derivable from the start program in the test dataset.
The final row â€™Sample derivable unique programsâ€™ represents
the total number of programs derived from legal node and
axiom sequences which are lexically unique.

C Language and Axioms for Complex

Linear Algebra Expressions

We now provide the complete description of the input lan-
guage for multi-type linear algebra expressions we use to
evaluate our work, and the complete list of all axioms that
are used to compute equivalence between programs.

15

Variable types We model programs made of scalars, vec-
tors and matrices. We limit programs to contain no more
than 5 distinct variable names of each type in a program:
â€¢ Scalar variables are noted ğ‘, ğ‘, ..., ğ‘’.
â€¢ Vector variables are noted (cid:174)ğ‘£, (cid:174)ğ‘¤, ..., (cid:174)ğ‘§.
â€¢ Matrix variables are noted ğ´, ğµ, ..., ğ¸.

Note we also explicitly distinguish the neutral and ab-
sorbing elements for scalars and matrices, e.g. 1 = 1N. This
enables the creation of simplification of expressions as a
program equivalence problem, e.g. if ğ´ + ğµ âˆ’ (ğµ + ğ´) = 0KÃ—K

Unary operators We model 6 distinct unary operators, all
applicable to any variable of the appropriate type:
â€¢ is(a) = ğ‘âˆ’1 is the unary reciprocal for scalars, im(A)
= ğ´âˆ’1 is matrix inverse.
â€¢ ns(a) = âˆ’ğ‘ is unary negation for scalars, nv(v) = âˆ’(cid:174)ğ‘£
for vectors, nm(M) = âˆ’ğ‘€ for matrices.
â€¢ tm(M) = ğ‘€ğ‘¡ is matrix transposition.

Binary operators We model 10 distinct binary operators
that operate on two values. 7 operators require the same
type for both operands, while 3 enable multi-type operands
(e.g., scaling a matrix by a scalar). Note we do not consider
potential vector/matrix size compatibility criterion for these
operators, in fact we do not represent vector or matrix sizes
at all in our language, for simplicity.
â€¢ +s(a, b)= ğ‘ + ğ‘, the addition on scalars, along with
-s(a,b) = ğ‘ âˆ’ ğ‘, *s(a,b) = ğ‘ âˆ— ğ‘ and /s(a,b) = ğ‘/ğ‘.
â€¢ +v( v, w) = (cid:174)ğ‘£ + (cid:174)ğ‘¤, the addition on vectors, along with
-v( v , w) = (cid:174)ğ‘£ âˆ’ (cid:174)ğ‘¤, *v( v, w) = (cid:174)ğ‘£. (cid:174)ğ‘¤ the dot product
between two vectors, producing a scalar.
â€¢ +m(A, B) = ğ´ + ğµ, the addition on matrices, along with
-m(A, B) = ğ´ âˆ’ ğµ, and *m(A, B) = ğ´ğµ the product of
matrices.
â€¢ *m(a,A) = ğ‘ (cid:164)ğ´ and *m(A,a) = ğ´ (cid:164)ğ‘ are used to represent
scaling a matrix by a scalar.
â€¢ *m(v,A) = (cid:174)ğ‘£ğ´ represents a vector-matrix product.
â€¢ *v(a,v) = ğ‘(cid:174)ğ‘£ and *v(v,a) = (cid:174)ğ‘£ğ‘ represent scaling a
vector by a scalar.

List of axioms of equivalence Tables 8-9 show the full
147 axioms supported by our rewrite rules. Many rewrite
rules can be applied to all 3 variable types as well as multiple
operator types.

pe-graph2axiom, June, 2021, ArXiV

Steve Kommrusch, ThÃ©o Barollet and Louis-NoÃ«l Pouchet

Rewrite Rule
Cancel

NeutralOp

DoubleOp

DistributeRight

ID Example(s)
(a - a) â†’ 0
1
(b/b) â†’ 1
2
(A - A) â†’ O
3
(ğ´ âˆ— ğ´âˆ’1) â†’ I
4
(ğ´âˆ’1 âˆ— ğ´) â†’ I
5
(v - v) â†’ o
6
(a + 0) â†’ a
7
(0 + a) â†’ a
8
(a - 0) â†’ a
9
(a * 1) â†’ a
10
(1 * a) â†’ a
11
(a / 1) â†’ a
13
(A + O) â†’ A
14
(O + A) â†’ A
15
(A - O) â†’ A
16
(A * I) â†’ A
17
(I * A) â†’ A
18
(v + o) â†’ v
19
(o + v) â†’ v
20
(v - o) â†’ v
21
-(-a)) â†’ a
22
(ğ‘âˆ’1)âˆ’1 â†’ a
23
24 âˆ’(âˆ’ğ´) â†’ ğ´
(ğ´âˆ’1)âˆ’1 â†’ ğ´
25
(ğ´ğ‘¡ )ğ‘¡ â†’ ğ´
26
27 âˆ’(âˆ’ğ‘£)) â†’ ğ‘£
a(b + c) â†’ ab + ac
64
a(b - c) â†’ ab - ac
65
a(v + w) â†’ av + av
66
a(v - w) â†’ av - av
67
68 A(B + C) â†’ AB + AC
69 A(B - C) â†’ AB - AC
a(B + C) â†’ aB + aC
70
a(B - C) â†’ aB - aC
71

Rewrite Rule
AbsorbOp

Commute

DistributeLeft

ID Example(s)
(a * 0) â†’ 0
28
(0 * a) â†’ 0
29
(A * 0) â†’ O
30
(0 * A) â†’ O
31
(A * O) â†’ O
32
(O * A) â†’ O
33
(A * o) â†’ o
34
(a * o) â†’ o
35
(o * a) â†’ o
36
(0 * v) â†’ o
37
(v * 0) â†’ o
38
(O * v) â†’ o
39
(a + b) â†’ (b + a)
40
(a * b) â†’ (b * a)
41
(A + B) â†’ (B + A)
42
(A * a) â†’ (a * A)
43
(a * A) â†’ (A * A)
44
(A * O) â†’ (O * A)
45
(O * A) â†’ (A * O)
46
(A * I) â†’ (I * A)
47
(I * A) â†’ (A * I)
48
(v + w) â†’ (w + v)
49
(v * a) â†’ (a * v)
50
(a * v) â†’ (v * a)
51
(a + b)c â†’ ac + bc
52
(a - b)c â†’ ac - bc
53
(a + b)/c â†’ a/c + b/c
54
(a - b)/c â†’ a/c - b/c
55
(v + w)*a â†’ va + wa
56
(v - w)*a â†’ va - wa
57
(A + B)C â†’ AC + BC
58
(A - B)C â†’ AC - BC
59
(A + B)v â†’ Av + Bv
60
(A - B)v â†’ Av - Bv
61
(A + B)a â†’ Aa + Ba
62
(A - B)a â†’ Aa - Ba
63

Table 8. Full axiom count when all type options and other supported permutations are included (part 1 of 2)

16

Proving Equivalence Between Complex Expressions

pe-graph2axiom, June, 2021, ArXiV

Rewrite Rule
FactorLeft

FactorRight

AssociativeLeft

Example(s)
ab + ac â†’ a(b+c)
ab - ac â†’ a(b-c)
AB + AC â†’ A(B+C)
AB - AC â†’ A(B-C)
Av + Aw â†’ A(v+w)
Av - Aw â†’ A(v-w)
Aa + Ab â†’ A(a+b)
Aa - Ab â†’ A(a-b)
va + vb â†’ v(a+b)
va - vb â†’ v(a-b)
ac + bc â†’ (a+b)c
ac - bc â†’ (a-b)c
a/c + b/c â†’ (a+b)/c
a/c - b/c â†’ (a-b)/c
AC + BC â†’ (A+B)C
AC - BC â†’ (A-B)C
Av + Bv â†’ (A+B)v
Av - Bv â†’ (A-B)v
Aa + Ba â†’ (A+B)a
Aa - Ba â†’ (A-B)a
va + wa â†’ (v+w)a
va - wa â†’ (v-w)a
a+(b+c) â†’ (a+b)+c
a+(b-c) â†’ (a+b)-c
a(bc) â†’ (ab)c
a(b/c) â†’ (ab)/c
A+(B+C) â†’ (A+B)+C
A+(B-C) â†’ (A+B)-C

ID
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100 A(BC) â†’ (AB)C
101 A(Ba) â†’ (AB)a
102 A(aB) â†’ (Aa)B
a(AB) â†’ (aA)B
103
104 A(Bv) â†’ (AB)v
105 A(va) â†’ (Av)a
106 A(av) â†’ (Aa)v
a(Av) â†’ (aA)v
107
v+(w+x) â†’ (v+w)+x
108
v+(w-x) â†’ (v+w)-x
109
v(ab) â†’ (va)b
110
a(vb) â†’ (av)b
111
a(bv) â†’ (ab)v
112

Rewrite Rule
AssociativeRight

FlipLeft

FlipRight

Transpose

ID
Example(s)
(a+b)+c â†’ a+(b+c)
113
(a+b)-c â†’ a+(b-c)
114
(ab)c â†’ a(bc)
115
(A+B)+C â†’ A+(B+C)
116
(A+B)-C â†’ A+(B-C)
117
(AB)C â†’ A(BC)
118
(AB)a â†’ A(Ba)
119
(Aa)B â†’ A(aB)
120
(aA)B â†’ a(AB)
121
(Av)a â†’ A(va)
122
(Aa)v â†’ A(av)
123
(aA)v â†’ a(Av)
124
(va)b â†’ v(ab)
125
(av)b â†’ a(vb)
126
(ab)v â†’ a(bv)
127
(v+w)+x â†’ v+(w+x)
128
(v+w)-x â†’ v+(w-x)
129
-(a - b) â†’ b-a
130
(ğ‘/ğ‘)âˆ’1 â†’ b/a
131
132 âˆ’(ğ´ âˆ’ ğµ) â†’ (B - A)
133 âˆ’(ğ‘£ âˆ’ ğ‘¤) â†’ (w - v)
a/(b/c) â†’ a(c/b)
134
ğ‘/(ğ‘âˆ’1) â†’ ab
135
a-(b-c) â†’ a+(c-b)
136
a-(-b) â†’ a+b
137
138 A-(B-C) â†’ A+(C-B)
139 A-(-B) â†’ A+B
140
141
142
143
144
145
146
147

v-(w-x) â†’ v+(x-w)
v-(-w) â†’ v+w
(ğ´ğµ) â†’ (ğµğ‘¡ ğ´ğ‘¡ )ğ‘¡
(ğ´ + ğµ) â†’ (ğ´ğ‘¡ + ğµğ‘¡ )ğ‘¡
(ğ´ âˆ’ ğµ) â†’ (ğ´ğ‘¡ âˆ’ ğµğ‘¡ )ğ‘¡
(ğ´ğµ)ğ‘¡ â†’ ğµğ‘¡ ğ´ğ‘¡
(ğ´ + ğµ)ğ‘¡ â†’ ğ´ğ‘¡ + ğµğ‘¡
(ğ´ âˆ’ ğµ)ğ‘¡ â†’ ğ´ğ‘¡ âˆ’ ğµğ‘¡

Table 9. Full axiom count when all type options and other supported permutations are included (part 2 of 2)

17

pe-graph2axiom, June, 2021, ArXiV

Steve Kommrusch, ThÃ©o Barollet and Louis-NoÃ«l Pouchet

Figure 4. pe-graph2axiom System Overview

D Details on neural network model
Figure 4 overviews the entire pe-graph2axiom architec-
ture including sample generation, the graph-to-sequence
network, the intermediate program generation, and lexical
equivalence checker. In this section we will discuss the im-
plementation details of these components.

Graph neural network internal representation The sam-
ple generation discussed in section 4 provides input to the
Node Initialization module in Fig. 4 to create the initial state
of our graph neural network. For each node in the program
graph, a node will be initialized in our graph neural network.
Each node has a hidden state represented by a vector of 256
floating point values which are used to create an embedding
for the full meaning of the given node. Initially all 256 di-
mensions of the hidden states of the nodes are set to zero
except for 2. Given ğ‘ tokens in our input program language,
one of the dimensions from 1 through ğ‘ of a node will be
set based on the token at the program position that the node
represents. For example, if the scalar variable ğ‘ is assigned
to be token 3 in our language, then the ğ‘ nodes of Fig. 5
recalled below would have their 3rd dimension initialized to
1.0. This is a one-hot encoding similar to that used in neural
machine translation models which leverage Word2vec [44].
The second non-zero dimension in our node initialization
indicates the tree depth, with the root for the program being
at depth 1. We set the dimension ğ‘ +ğ‘‘ğ‘’ğ‘ğ‘¡â„ to 1.0; hence, the
ğ‘ nodes in Fig 5, which vary from level 2 or 3 in the graph,
would set dimension ğ‘ + 2 or ğ‘ + 3 to 1. In addition to nodes
correlating to all tokens in both input programs, we initial-
ize a root node for program comparison which has edges
connecting to the root nodes of both programs. The root
node does not represent a token from the language, but it is
initialized with a 1.0 in a hidden state dimension reserved
for its identification.

For a graph neural network, the edge connections between
nodes are a crucial part of the setup. In particular, to match
the formulation of our problem, we must ease the ability of
the network to walk the input program graphs. We therefore
designed a unified graph input, where both program graphs

18

are unified in a single graph using a single connecting root
node; and where additional edges are inserted to make the
graph fully walkable.

In our full model, we support 9 edge types and their re-
verse edges. The edge types are: 1) left child of binary op, 2)
right child of binary op, 3) child of unary op, 4) root node
to program 1, 5) root node to program 2, 6-9) there are 4
edge types for the four node grandchilden (LL, LR, RL, RR).
After the node hidden states and edge adjacency matrix are
initialized, the network is ready to begin processing. This
initial state is indicated in figure 6 by the solid circles in the
lower left of the diagram.

Beam search A typical approach when using sequence-
to-sequence systems is to enable beam search, the process
of asking for multiple answers to the same question to the
network. It is particularly relevant when creating outputs
which can be automatically checked [1, 18]. Beam search can
be viewed as proposing multiple possible axioms to apply.
Given the stochastic nature of generation model, a beam
width of ğ‘› can be thought of as creating the ğ‘› most likely
sequences given the training data the model as learned on.
Each proposal can be checked for validity, the first valid one
is outputted by the system, demonstrating equivalence. Our
system builds on the neural network beam search provided
by OpenNMT to create a â€™system beam searchâ€™ of variable
width. In particular, we set the OpenNMT network beam
search to 3, which constrains the token generator to produce
3 possible axiom/node proposals for a given pair of input
programs. Using these 3 proposals, when our system beam
width is 10, we build up to 10 intermediate programs that are
being processed in the search for a proof. To illustrate with
a system beam width of 5, after ğ‘ƒ1 and ğ‘ƒ2 are provided to
the neural network, 3 possible intermediate programs may
be created (so long as all axioms are legal and donâ€™t produce
duplicates). After those 3 intermediates are processed, 9 pos-
sible new intermediates are created, all of which are checked
for lexical equivalence with ğ‘ƒ2, but only 5 of which are fed
back into the neural network for further axiom generation.
This process is continued for up to 12 axioms at which point
the system concludes an equivalence proof cannot be found

Proving Equivalence Between Complex Expressions

pe-graph2axiom, June, 2021, ArXiV

ğ‘

âˆ—

âˆ—

+

âˆ—

1

ğ‘

1

ğ‘

ğ‘

âˆ—

ğ‘

+

ğ‘

(a) ğ‘ âˆ— (1 âˆ— ğ‘ + 1 âˆ— ğ‘)

(b) ğ‘ âˆ— (ğ‘ + ğ‘)

+

âˆ—

âˆ—

ğ‘

ğ‘

ğ‘

ğ‘

(c) ğ‘ âˆ— ğ‘ + ğ‘ âˆ— ğ‘

+

âˆ—

âˆ—

ğ‘

ğ‘

ğ‘

ğ‘

(d) ğ‘ âˆ— ğ‘ + ğ‘ âˆ— ğ‘

Figure 5. Examples of Computations

Figure 6. Graph-to-sequence neural network data flow details.

Table 10. Hyperparameter experiments. Summary of best
validation token accuracy result after 2 runs for up to 100,000
training iterations. The golden model has 256 graph nodes
and decoder dimensions, 2 decoder LSTM layers, starts train-
ing with a learning rate of 0.8, and uses 10 steps to stabilize
the GGNN encoder.

Parameter

Value

Validation
token accuracy

Golden model
Graph node+decoder LSTM dimension

Decoder LSTM layers
Initial learning rate

GGNN stability steps

192
320
1
0.75
0.85
12
8

83.89
83.89
83.58
83.53
83.76
83.57
83.19
83.61

and the programs are likely not equivalent. We evaluate
in Sec. 6 beam sizes ranging from 1 to 10, showing higher
success with larger beams.

E Details on Experimental Results
E.1 Complementary Results and Observations

Table 10 describes part of our neural network hyperparam-
eter tuning showing that our golden model has as high a
result as other variations explored. Note that the validation
token accuracy is not too high (itâ€™s not above 90%) despite
the ability to predict full correct proofs with over 93% accu-
racy. This is because the training dataset can have multiple

examples of axioms given similar input programs. For exam-
ple, proving "(a+b)(c+d) = (b+a)(d+c)" requires commuting
the left and right subexpressions. The training dataset could
have similar programs which are sometimes transformed
first with a right Commute and then a left or vice-versa.
Given this data, the network would learn to apply one or the
other (it would not get trained to use associativity for these
program pairs for example), hence the actual output given
may or may not match the validation target axiom. We will
discuss this further in section E.2.

Training convergence Since our model trains on axiomatic
proofs which may vary in order (allowing 2 or 3 options to
be correct and occur in the training set), we see our training
and token accuracies plateau below 90% during training for
AxiomStep10 as shown in Figure 7. Full testset proof accura-
cies for beam width 10 exceed 90%, but also plateau along
with the training and validation results. This result differs
from our WholeProof10 training, which achieves training
and validation accuracies above 96% because the expected
axiom sequence is more predictable, but as we have seen less
generalized.

As another observation on generalization and overfitting,
we note that figure 7 shows a slight separation between
the training and validation accuracies starting at around
iteration 180,000. While the training accuracy rises slowly,
validation accuracy plateaus, indicating slight overfitting on
the training data. Yet our model continues to slowly increase
in quality, with the model snapshot that scores best on both
validation and test accuracies occurring at iteration 300,000.
This is our golden model, with 93.1% of P1 to P2 proofs
accurately found using beam width 10.

19

pe-graph2axiom, June, 2021, ArXiV

Steve Kommrusch, ThÃ©o Barollet and Louis-NoÃ«l Pouchet

up to 10 rewrite rules, ğ‘ƒ1 and ğ‘ƒ2 can have up to 50 AST nodes
each, and an AST depth of up to 7. AxiomStep5 has samples
requiring up to 5 rewrite rules, ğ‘ƒ1 and ğ‘ƒ2 can have up to
25 AST nodes each, and an AST depth of up to 6. Tables 12
and 13 (repeated from main paper below) demonstrate the
ability of a model trained on AxiomStep5 to perform well
on the larger distribution of programs from AxiomStep10,
implying that the model has generalized well to our program
equivalence problem and that pe-graph2axiom does not
overfit its response to merely the training set distribution.

Table 13 illustrates the ability of a model trained on Ax-
iomStep5 (i.e., limited to proofs of length 5) to perform well
when evaluated on the more complex AxiomStep10, which
includes proofs of unseen length of up to 10. The robustness
to the input program complexity is illustrated with the 86%
pass rate on AST depth 7, for the model trained on Axiom-
Step5 which never saw programs of depth 7 during training.
As an indication of the breadth of equivalent programs
represented by AxiomStep10 relative to WholeProof10, table
14 shows the full detail of models trained on all 4 datasets
when tested on test data from all 4 datasets. AxiomStep10,
while training on our broadest dataset in which axioms can
be applied to nodes repeatedly and in variable order, achieves
a 93% average success rate. 72% of the proofs of length 6
from the WholeProof10 testset were solved by the model
trained on WholeProof10, but only 5% of such proofs from
AxiomStep10 were, suggesting the method of generating
AxiomStep pairs covers the problem space more thoroughly.
The complete result for the WholeProof10 model on the
WholeProof10 dataset was 8,388 out of 10,000 program pairs
had a correct proof found; of those, 8,350 were the exact proof
created during ğ‘ƒ1, ğ‘ƒ2 generation, implying that WholeProof10,
while performing well on its own testset distribution, is not
learning to generalize to alternative proof paths.

Manual verifications We conducted a series of manual
verifications of the system used to produce all the above
results. First, we are happy to confirm that most likely ğ´ğµ â‰ 
ğµğ´ given no verifiable equivalence sequence was produced,
but that provably ğ‘ğ‘ = ğ‘ğ‘ indeed. We also verified that
ğ´ğ‘¡ğ‘¡
(ğµ + ğ¶ âˆ’ ğ¶) = ğ´ğµ, and that ğ´ğµ(cid:174)ğ‘£ âˆ’ ğ´ğµ (cid:174)ğ‘¤ = ğ´ğµ((cid:174)ğ‘£ âˆ’ (cid:174)ğ‘¤)
which would be a much faster implementation. The system
correctly suggests that ğ´ğµ(cid:174)ğ‘£ âˆ’ ğµğ´ (cid:174)ğ‘¤ â‰  ğ´ğµ((cid:174)ğ‘£ âˆ’ (cid:174)ğ‘¤). We ensured
that ğ´ğ‘¡ (ğ´ğ´ğ‘¡ )âˆ’1ğ´ â‰  ğ´ğ‘¡ (ğ´ğ´âˆ’1)ğ‘¡ğ´, from a typo we once made
when typing the computation of an orthonormal sub-space.
We also verified that indeed ğ´ğµ + ğ´ğ¶ + ğ‘ğ· âˆ’ ğ‘ğ· = ğ´(ğµ + ğ¶).

Generalizing variable types We explored the ability of
the model to understand variable typing by training a model
with the AxiomStep10 distribution but with no samples that
included the scalar variable â€™eâ€™ and scalar multiplication
âˆ—ğ‘  . This removed about 50% of the training set, as longer
programs were often included both tokens. When tested
with the unaltered AxiomStep10 test set and beam width
10, test samples that included a scalar variable not â€™eâ€™ and

Figure 7. Model training percentage accuracy up to 300,000
iterations on AxiomStep10. Training and Validation accura-
cies are per-token on the target axioms in the samples. Test
accuracies are for full correct proofs of P1 to P2.

In addition to the sequence-to-
Testing simpler models
sequence and graph-to-sequence models, we explored a feed-
forward equal/not equal classifier on a simple version of our
language. That model uses an autoencoder on the program
to find an embedding of the program and then a classifier
based on the program embeddings found. It achieves a 73%
accuracy on identifying equivalent pairs in the test data,
which, as expected, is much lower than the full proof rate
of 93% achieved with a graph-to-sequence proof generator
on our full language. This simple experiment highlights the
importance of a system which prevents the false positives
which a classifier might have by creating a verifiable proof.
We explore initial language generation using a simple lan-
guage in order to assess feasibility of different approaches.
For fine tuning network parameters and architectural fea-
tures, we add more complexity to the language as shown in
table 11. Language IDs 1 through 3 are all based on a simple
grammar which only allows the "+" or "-" operators on scalar
variables labeled a through j. The only axiom is Commute,
which can be applied on up to 3 nodes in language IDs 2 and
3. Language ID 4 adds the scalar constants 0 and 1, scalar
operations * and /, and 4 more axioms. We perform a fair
amount of network development on this model in an effort
to maintain high accuracy rates. Language ID also 4 expands
the operands to 3 types and hence the number of operators
also increases. To speed up model evaluation, we reduced the
program length for IDs 5, 6, and 7, allowing us to train larger
data sets for more epochs. ID 7 is a forward looking-model
which makes a minor increment to the language to support
the analysis of loop rolling and unrolling, discussed further
in section E.3. ID 8 is the WholeProof5 model in relation to
these early experiments.

We designed our datasets in section 4 with the goal of
using the varied models to understand the generalizability of
pe-graph2axiom and to show that our model is not over-
fitting on training data. For these next experiments, all results
of for beam width 10, which provides for a neural-network
directed search of up to 10 axiomatic proofs of equivalence
for each program pair. Recall that our most complex dataset
is AxiomStep10 which includes (ğ‘ƒ1, ğ‘ƒ2, ğ‘†) samples requiring

20

Proving Equivalence Between Complex Expressions

pe-graph2axiom, June, 2021, ArXiV

s
r
o
t
a
r
e
p
O
#

2

2

s

m
o
i
x
A
#

1

1

s
d
n
a
r
e
p
O
#

10

10

h
t
g
n
e
l

m
a
r
g
o
r
P
3-19

5-24

h
t
g
n
e
l

s
e
l
u
r

e
t
i
r

w
e
R
1-5

)
S
2
S
(
q
e
s
2
q
e
s

r
o

)
S
2
G

(
q
e
s
2
h
p
a
r
G
S2S

e
z
i
s

t
e
s

g
n

i

n

i
a
r
T
80,000

g
n

i
h
c
t
a
m

1
h
t
d
i
w
m
a
e
b
h
t
i

t
n
e
c
r
e
w
P
90.0%

g
n

i
h
c
t
a
m

0
1
h
t
d
i
w
m
a
e
b
h
t
i

t
n
e
c
r
e
w
P
96.2%

3-10

S2S

80,000

80.3%

96.5%

ID
1

2

Description
Rewrite sequence is only single Commute, uses sequence-to-
sequence model
Rewrite sequence is exactly 2 Commutes, uses sequence-to-
sequence model
Rewrite sequence exactly 2 Commutes
Rewrite sequence exactly 3 Commutes
Rewrite sequence 1 to 3 Commutes
Commute, Noop, Cancel, Distribute Left, Distribute Right
Scalars, Vectors, and Matrixes
13 Axioms
Rewrite sequence or Not_equal
Test sequence-to-sequence
Add loop axioms

3
4
5
7
8
9
10
11
12

1
1
1
5
5
13
13
13
15
Table 11. Results for various language complexities studied, on non-incremental models (WholeProof).
Table 12. Generalizing to longer P1 inputs. Percentage pass rates for equivalence proofs with P1 having increasing program
graph nodes. The model trained with the AxiomStep5 dataset had no training examples more than 25 program graph nodes
yet it performs relatively well on these more complex problems. The furthest right column shows the pe-graph2axiom
model results on the most complex dataset.

80,000
80,000
180,000
180,000
250,000
400,000
500,000
400,000
400,000

99.8%
99.0%
99.2%
97.4%
95.6%
95.5%
93.8%
81.1%
94.7%

98.9%
91.4%
97.1%
93.1%
88.3%
85.5%
79.8%
59.8%
83.8%

5-24
7-45
3-45
3-45
3-30
3-30
3-30
3-30
3-30

3-10
5-15
1-15
1-15
1-25
1-25
1-25
1-25
1-25

G2S
G2S
G2S
G2S
G2S
G2S
G2S
S2S
G2S

10
10
10
12
20
20
20
20
20

2
2
2
4
16
16
16
16
18

Testset
Sample Count

Model trained
on AxiomStep5

Model trained
on AxiomStep10

P1 nodes

AS5 AS10

1-5
6-10
11-15
16-20
21-25
26-30
31-35
36-40
41-45
46-50
All

231
2147
3980
2583
1059
0
0
0
0
0
10000

109
1050
2175
2327
1989
1229
698
304
101
27
10000

AS5

100
100
99
98
97
N/A
N/A
N/A
N/A
N/A
99

AS10

100
99
96
92
89
83
78
74
68
67
90

AS5

100
99
99
98
98
N/A
N/A
N/A
N/A
N/A
99

AS10

100
99
96
93
92
90
88
87
84
85
93

Table 13. Performance vs. AST size: counts and percentage pass rates.

Testset
Sample Count

Model trained
on AxiomStep5

Model trained
on AxiomStep10

AST depth

AS5 AS10

AS5

AS10

AS5

AS10

2
3
4
5
6
7
All

5
306
1489
4744
3456
0
10000

3
133
577
1844
4308
3135
10000

âˆ—ğ‘  were proven equal 90% of the time; test samples that in-
cluded â€™eâ€™ and âˆ—ğ‘  were also proven equal 90% of the time.

100
100
99
94
90
86
90

100
100
99
98
98
n/a
99

100
100
99
95
93
92
93

For beam width 1 the proof success rates were 72% and 70%
for without and with â€™eâ€™, implying that the heavily biased

100
100
100
99
98
n/a
99

21

pe-graph2axiom, June, 2021, ArXiV

Steve Kommrusch, ThÃ©o Barollet and Louis-NoÃ«l Pouchet

Table 14. Generalizing to longer proofs. Percentage pass rates for equivalence proofs of increasing axiom counts when testing
each of 4 datasets on models trained using each of 4 datasets.

Axiom Model trained on

Model trained on

Count in WholeProof5 (WP5) WholeProof10 (WP10)

Model trained on
AxiomStep5 (AS5)

Model trained on
AxiomStep10 (AS10)

Proof WP5 WP10 AS5 AS10 WP5 WP10 AS5

AS10 WP5 WP10 AS5 AS10 WP5 WP10 AS5 AS10

1
2
3
4
5

6
7
8
9
10
All

100
99
98
93
84

100 100
66
98
34
94
16
84
8
70

14
0
0
0
0
66

44

95

99
64
33
15
7

4
1
0
0
0
27

100
99
97
90
84

100 100
65
99
33
95
16
88
8
82

72
63
54
47
34
84

44

94

100
63
33
15
7

5
2
1
0
0
27

100
100
100
98
96

100 100 100
99
99 100
98
99
98
97
98
95
95
96
91

100
100
100
99
97

100 100 100
100 100 100
99
99
99
98
98
98
96
96
95

81
67
54
35
24
87

88
81
75
64
57
90

99

99

90
83
73
63
46
93

99

93
87
82
74
66
93

99

training set did have a small effect on the system general-
ization. pe-graph2axiom was still able to generalize the
relation of â€™eâ€™ to the âˆ—ğ‘  operator given that â€™eâ€™ was used in
contexts similar to other scalar variables in the training sam-
ples that were provided, implying it was forming an internal
representation of a â€™scalarâ€™ type by learning from examples.

E.2 Learning that multiple axiom choices are

possible

Our AxiomStep10 model is trained on axioms which may
be applied in varying order in the training set. For example,
((ğ‘ + ğ‘) âˆ— (ğ‘ + ğ‘‘)) = ((ğ‘ + ğ‘) âˆ— (ğ‘‘ + ğ‘)) may have the training
data to Commute the left node ğ‘+ğ‘ first and then ğ‘ +ğ‘‘ second;
in the same dataset, ((ğ‘+ğ‘’) âˆ— (ğ‘ +ğ‘)) = ((ğ‘’ +ğ‘) âˆ— (ğ‘ +ğ‘)) might
occur and the training data has the right node Commuted
first. In this way, we expect the model to learn that either
commuting the left or right node is a proper first axiom
choice. Table 15 explores the ability of the model to produce
such axiom proposals. Given 5 scalar variables, there are
120 possible expressions where two 2-variable additions are
multiplied together such as ((ğ‘ + ğ‘) âˆ— (ğ‘ + ğ‘‘)). We consider
here all 120 program pairs in which the left and right ad-
ditions are commuted. The table shows which axioms and
positions are recommended by the graph-to-sequence neural
network model within the pe-graph2axiom system as
most probably moving the 2 programs closer to equivalence
by the beam width 3 on this problem. Note that the 2 correct
axioms are always within the top 3 choices and the other
2 axioms (Commute and DistributeLeft on the root), while
not necessary for this problem, are at least legal choices for
axioms within our expression language.

The results in table 15 relate to the value of our approach
in relation to reinforcement learning models for proof gen-
eration [22] [9]. To make an analogy with reinforcement
learning, in our training, the world â€™stateâ€™ is presented as a

ğ‘ƒ1, ğ‘ƒ2 pair and the system must learn to produce an axiom at
a location which performs an â€™actionâ€™ on the â€™stateâ€™ of ğ‘ƒ1 in
a predictable way. Unlike reinforcement learning, we do not
produce a reward function and our system cannot learn from
a poor reward produced by an incorrect axiom. However, we
have demonstrated that our system, as it is presented with
a wide distribution of (ğ‘ƒ1, ğ‘ƒ2, ğ‘†) tuples to train on, learns a
probability distribution of possibly correct axioms to produce
for a given program pair. There may be value in combining
our graph-neural-network within a reinforcement learning
framework that used a hindsight mechanism [6] to learn
from every attempted axiom, but it is not immediately ob-
vious that our approach of learning only from examples of
successful equivalence proofs would be improved.

Table 15. Learning multiple output options. When con-
sidering scalar expressions that can be proven equivalent
by commuting the left and right subexpressions, such as
(ğ‘ + ğ‘)(ğ‘ + ğ‘‘) = (ğ‘ + ğ‘)(ğ‘‘ + ğ‘), pe-graph2axiom learns
that either the left or right commute can occur first. The
columns show counts for axioms and locations proposed by
the token generator with beam width of 3 when given 120
different scalar expression pairs.

Beam
position

First
Second
Third
Any of top 3

Axiom

Commute
left child

Commute Commute DistributeLeft
root
root
right child

49
58
13
120

35
59
26
120

36
3
45
84

0
0
36
36

In order to design the
Exploration of alternate designs
system, we explored parts of the design space quickly and

22

Proving Equivalence Between Complex Expressions

pe-graph2axiom, June, 2021, ArXiV

performed several single training run comparisons between
2 options, as shown in Table 16.

In cases where 2 options were similar, we chose the model
which ran faster, or run the models a second time to get
a more precise evaluation, or use our experience based on
prior experiments to select an option.

Table 16. Example explorations as a single feature or param-
eter is changed. Each comparison is a distinct experiment,
as the entire network and language used was being varied.

Options compared

1 layer LSTM vs
2 layer LSTM vs
3 layer LSTM
No edges to grandchild nodes vs
Edges to grandchild nodes
Encoder->Decoder only root node vs
Encoder->Decoder avg all nodes

Match
beam 1

Match
beam 10

198
5020
4358
9244
9284
8616
7828

1380
9457
8728
9728
9774
9472
9292

Experiments such as these informed our final network ar-
chitecture. For example, in pe-graph2axiom, we include
4 edges with learnable weight matrices from a node to its
grandchildren because such edges were found to improve re-
sults on multiple runs. Li et al. [39] discusses the importance
of selecting the optimal process for aggregating the graph in-
formation hence we explore that issue for our network. Our
approach uses the root comparison node to create aggregate
the graph information for the decoder as it performs better
than a node averaging.

Including Not_equal option Table 17 analyzes the chal-
lenge related to a model which only predicts Equal or Not_equal
for program pairs along with various options which produce
rewrite rules which can be checked for correctness. In all 4
output cases shown, 2 programs are provided as input. These
programs use an earlier version of our language model with
16 operators, 13 core axioms, and 20 operands generated
with a distribution similar to WholeProof5.

Table 17. Table showing alternate options for handling not
equal programs

Network
output
Description

Eq or NotEq,
Beam width 1
Rules or NotEq,
Beam width 1
Rules only,
Beam width 1
Rules only,
Beam width 10

Predicted Rules
or Eq

Predicted Correct
Rewrite
Rules

Actual NotEq

Eq
NotEq
Eq
NotEq
Eq
NotEq
Eq
NotEq

5.4%
90.4%
6.6%
90.9%
N/A
N/A
N/A
N/A

94.6%
9.6%
93.4%
9.1%
100%
N/A
100%
N/A

N/A
N/A
70.7%
N/A
87.8%
N/A
96.2%
N/A

23

For the first output case, the output sequence to produce
is either Equal or Not_equal. Given a false positive rate
of 9.6%, these results demonstrate the importance of produc-
ing a verifiable proof of equivalence when using machine
learning for automated equivalence checking. For the sec-
ond output case, the model can produce either Not_equal
or a rewrite rule sequence which can be checked for cor-
rectness. The source programs for the first and second case
are identical: 250,000 equivalent program pairs and 250,000
non-equivalent program pairs. In the second case, the false
positive rate from the network is 9.1% (rules predicted for
Not_equal programs), but the model only produces correct
rewrite rules between actual equivalent programs in 70.7%
of the cases.

One challenge with a model that produce rules or Not_equal

is that beam widths beyond 1 are less usable. Consider that
with a beam width of 1, if the network predicts Not_equal
then the checker would conclude the programs are not equal
(which is correct for 90.9% of the actually not equal pro-
grams). With a beam width of 10, there would be more pro-
posed rewrite rules for equal programs to test with, but if
1 of the 10 proposals is Not_equal, should the checker
conclude they are not equal? Or should the the checker only
consider the most likely prediction (beam width 1) when
checking for non-equivalence? The third and fourth net-
work output cases provide an answer. For these 2 cases, the
training set is 400,000 equivalent program pairs - none are
non-equivalent. 250,000 of these pairs are identical to the
equivalent programs in the first 2 cases, and 150,000 are new
but were produced using the same random generation pro-
cess. Note that by requiring the network to focus only on
creating rewrite rules, beam width 1 is able to create cor-
rect rewrite rules for 87.8% of the equivalent programs. And
now, since weâ€™ve remove the confusion of the Not_equal
prediction option, beam width 10 can be used to produce
10 possible rewrite rule sequences and in 96.2% of the cases
these rules are correct. Hence, we propose the preferred use
model for pe-graph2axiom is to always use the model which
is trained for rule generation with beam width 10 and rely
on our rule checker to prevent false positives. From the 10
rewrite rule proposals, non-equivalent programs will never
have a correct rewrite rule sequence produced, hence we
guarantee there are no false positives.

E.3 An Example of Back-Edge in the Program

Graph

Figure 8 shows an example of DoX and DoHalf. The new
operators result in 2 new edges in our graph representation
(along with 2 new back-edges): there is a â€™loopbodyâ€™ edge
type from the loop operator node to the start of the subgraph,
and there is a â€™loopfeedbackâ€™ edge type from the variable
which is written to each loop iteration. These 2 edge types
are shown in the figure. The new ğ·ğ‘œâ„ğ‘ğ‘™ ğ‘“ axiom intuitively

pe-graph2axiom, June, 2021, ArXiV

Steve Kommrusch, ThÃ©o Barollet and Louis-NoÃ«l Pouchet

states that ğ·ğ‘œğ‘‹ (ğ‘”(ğ‘¦)) = ğ·ğ‘œğ»ğ‘ğ‘™ ğ‘“ (ğ‘”(ğ‘”(ğ‘¦))) (where ğ‘¦ is the
variable reused each iteration), and ğ·ğ‘œğ‘¥ states the reverse.

DoH

/

/

ğ‘

ğ‘

ğ‘

+

+

ğ‘

ğ‘

DoX

/

+

ğ‘

ğ‘

ğ‘

(a) DoX(ğ‘
(ğ‘ + ğ‘)/ğ‘)

=

(b) DoHalf(ğ‘ = (ğ‘ +
(ğ‘ + ğ‘)/ğ‘)/ğ‘)

Figure 8. Adding loop constructs creates cycles in the pro-
gram graph.

24

