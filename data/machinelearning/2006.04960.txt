0
2
0
2

n
u
J

8

]
L
M

.
t
a
t
s
[

1
v
0
6
9
4
0
.
6
0
0
2
:
v
i
X
r
a

A Notion of Individual Fairness for Clustering

Matthäus Kleindessner
University of Washington
mk1572@uw.edu

Pranjal Awasthi
Rutgers University & Google
pranjal.awasthi@rutgers.edu

Jamie Morgenstern
University of Washington & Google
jamiemmt@cs.washington.edu

Abstract

A common distinction in fair machine learning, in particular in fair classiﬁcation, is
between group fairness and individual fairness. In the context of clustering, group
fairness has been studied extensively in recent years; however, individual fairness
for clustering has hardly been explored. In this paper, we propose a natural notion of
individual fairness for clustering. Our notion asks that every data point, on average,
is closer to the points in its own cluster than to the points in any other cluster. We
study several questions related to our proposed notion of individual fairness. On
the negative side, we show that deciding whether a given data set allows for such
an individually fair clustering in general is NP-hard. On the positive side, for the
special case of a data set lying on the real line, we propose an efﬁcient dynamic
programming approach to ﬁnd an individually fair clustering. For general data sets,
we investigate heuristics aimed at minimizing the number of individual fairness
violations and compare them to standard clustering approaches on real data sets.

1

Introduction

Clustering is a classic unsupervised learning procedure and is used in a wide range of ﬁelds to
understand which data points are most similar to each other, which regions in space a data set inhabits
with high density (Ester et al., 1996), or to select representative elements of a data set (Hastie et al.,
2009). The problem of clustering can be formulated in numerous ways, including objective-based
formulations like k-median (Awasthi and Balcan, 2014), hierarchical partitionings (Dasgupta, 2002),
and spectral clustering (von Luxburg, 2007), which have also been considered subject to additional
constraints (Wagstaff et al., 2001). A recent surge in work has designed clustering algorithms to satisfy
various notions of proportional representation, including proportionality for different demographics
within clusters (Chierichetti et al., 2017) or within the set of cluster centers (Kleindessner et al.,
2019a), or requiring a notion of coherence on large subsets of a cluster (Chen et al., 2019).

All the latter proportionality constraints fall into the category of group fairness constraints (Friedler
et al., 2016), which require a model to have similar statistical behavior for different demographic
groups. Such statistical guarantees necessarily give no guarantee for any particular individual. For
example, while proﬁles of women might be equally represented in different clusters, such a clustering
might not be a good clustering for any particular woman. This weakness of proportionality constraints
raises a natural question: can one construct clusterings that provide fairness guarantees for each
individual, and what kind of fairness guarantees would an individual want to have after all?

We argue that if a clustering is used in a machine learning downstream task, then rather than caring
about fairness of the clustering, one should care about fairness at the end of the pipeline and tune the
clustering accordingly. This is analogous to using clustering as a preprocessing step for classiﬁcation

Preprint. Under review.

 
 
 
 
 
 
Figure 1: Two data sets in R2 with d equaling the Euclidean metric. Left: An individually fair
3-clustering with clusters C1, C2, C3. Right: Four points for which there is no individually fair
, then x1 would
x2, x3}
2-clustering. For example, if the two clusters were C1 =
be treated unfair because of d(x1, x4) = 0.71 > 0.68 = [d(x1, x2) + d(x1, x3)]/2.

x1, x4}
{

and C2 =

{

and caring about accuracy (von Luxburg et al., 2012). However, if a clustering is used by a human
decision maker, say for exploratory data analysis or resource allocation, an individual may strive for
being well represented, which means to be assigned to a cluster with similar data points. As a toy
example, think of a company that clusters its customers and distributes semi-personalized coupons,
where all customers in one cluster get the same coupons according to their (hypothesized) preferences.
A customer that ends up in a cluster with rather different other customers (and hence is not well
represented by its cluster) might get coupons that are less valuable to her than the coupons she would
have got if she had been assigned to the cluster that is best representing her.

Motivated by such an example, our notion of individual fairness asks that each data point is assigned to
the best representing cluster in the sense that the data point, on average, is closer to the points in its own
cluster than to the points in any other cluster. While our notion is related to a well-known concept of
clustering stability (cf. Section 2.1), many questions are open. For instance, in contrast to the existing
group fairness notions, an individually fair clustering may not exist (for a ﬁxed number of clusters).
We make the following contributions towards understanding individual fairness for clustering:

We propose a natural notion of individual fairness for clustering requiring that every data point, on

•
average, is closer to the points in its own cluster than to the points in any other cluster.

When the data lies on the real line, we show that an individually fair clustering always exists, and
•
we design an efﬁcient algorithm to ﬁnd one. We argue why this 1-dim case is interesting on its own.
We show that even for Euclidean data sets in R2, individually fair clusterings might not exist and
•
prove that the problem of deciding whether a given data set has an individually fair k-clustering is
NP-hard, even for k = 2 and when the underlying distance function is assumed to be a metric.

We perform experiments on real data sets and compare the performance of our polynomial time
•
algorithm for the 1-dim case with k-means clustering. In the case of higher dimensions, we investigate
several standard clustering algorithms with respect to our fairness notion.

2 Fairness Notion

together with a given dissimilarity function d
Our notion of individual fairness applies to a data set
that measures how close two data points are. We use the terms dissimilarity and distance synony-
mously. We assume d :
0 to be symmetric with d(x, x) = 0, but not necessarily to be a
R
≥
metric (i.e., to additionally satisfy the triangle inequality and d(x, y) = 0

D × D →

x = y).

D

Our fairness notion deﬁnes what it means that a data point is treated fair in a clustering of
; namely:
a data point is treated individually fair if the average distance to the points in its own cluster (the
point itself excluded) is not greater than the average distance to the points in any other cluster. Then a
clustering of

is said to be individually fair if it treats every data point of

individually fair.

D

⇔

D

D

For the rest of the paper we assume
can then be formally stated as follows (for l
Deﬁnition 1 (Individually fair clustering). Let
. . . ˙
Ck and Ci (cid:54)
D
∪
belongs to. We say that x

= C1 ˙
∪

for i

=

D

∈

N, we write [l] =

to be ﬁnite. Our deﬁnition of individual fairness for clustering
1, . . . , l
{
= (C1, . . . , Ck) be a k-clustering of

, that is
, we write C(x) for the cluster Ci that x

[k]. For x

D

∈

):

C

}

∈ D

∈ D

∅
is treated individually fair if either C(x) =
1
C(x)
|

1
Ci|
|

d(x, y)

d(x, y)

(cid:88)

(cid:88)

| −

C(x)

≤

Ci

1

y

y

∈

∈

or

x

}

{

(1)

2

C3C2C1x1x2x3x40.720.640.71Figure1:TwodatasetsinR2withdequalingtheEuclideanmetric.Left:Anindividuallyfair3-clusteringwithclustersC1,C2,C3.Right:Fourpointsforwhichthereisnoindividuallyfair2-clustering.Forexample,ifthetwoclusterswereC1={x1,x4}andC2={x2,x3},thenx1wouldbetreatedunfairbecauseofd(x1,x4)=0.71>0.68=[d(x1,x2)+d(x1,x3)]/2.whereallcustomersinoneclustergetthesamecouponsaccordingtotheir(hypothesized)preferences.39Acustomerthatendsupinaclusterwithratherdifferentothercustomers(andhenceisnotwell40representedbyitscluster)mightgetcouponsthatarelessvaluabletoherthanthecouponsshewould41havegotifshehadbeenassignedtotheclusterthatisbestrepresentingher.42Motivatedbysuchanexample,ournotionofindividualfairnessasksthateachdatapointisassigned43tothebestrepresentingclusterinthesensethatthedatapoint,onaverage,isclosertothepointsin44itsownclusterthantothepointsinanyothercluster.Whileournotionisrelatedtoawell-known45conceptofclusteringstability(Balcanetal.,2008),manyfundamentalquestionsremainunanswered.46Forinstance,unlikethegroupfairnessnotionsstudiedabove,whereafairsolutionalwaysexistsand47thegoalistoﬁndtheonewithminimumcost,anindividuallyfairclusteringmaynotnecessarily48exist(foraﬁxednumberofclusters).Inthiswork,wemakethefollowingcontributionstowards49understandingindividualfairnessforclustering:50•Weproposeanaturalnotionofindividualfairnessforclusteringrequiringthateverydatapoint,on51average,isclosertothepointsinitsownclusterthantothepointsinanyothercluster.52•Whenthedataliesontherealline,weshowthatanindividuallyfairclusteringalwaysexists,and53wedesignanefﬁcientalgorithmtoﬁndone.Wearguewhythis1-dimcaseandouralgorithmare54interestingonitsown.55•WeshowthatevenforEuclideandatasetsinR2,individuallyfairclusteringsmightnotexistand56provethattheproblemofdecidingwhetheragivendatasethasanindividuallyfairk-clusteringis57NP-hard,evenfork=2andwhentheunderlyingdistancefunctionisassumedtobeametric.58•Weperformexperimentsonrealdatasetsandcomparetheperformanceofourpolynomialtime59algorithmforthe1-dimcasewithk-meansclustering.Inthecaseofhigherdimensions,weinvestigate60severalstandardclusteringalgorithmswithrespecttoourfairnessnotion.612FairnessNotion62OurnotionofindividualfairnessappliestoadatasetDtogetherwithagivendissimilarityfunctiond63thatmeasureshowclosetwodatapointsare.Weusethetermsdissimilarityanddistancesynony-64mously.Weassumed:D⇥D!R 0tobesymmetricwithd(x,x)=0,butnotnecessarilytobea65metric(i.e.,toadditionallysatisfythetriangleinequalityandd(x,y)=0,x=y).66OurfairnessnotiondeﬁneswhatitmeansthatadatapointistreatedfairinaclusteringofD;namely:67adatapointistreatedindividuallyfairiftheaveragedistancetothepointsinitsowncluster(the68pointitselfexcluded)isnotgreaterthantheaveragedistancetothepointsinanyothercluster.Thena69clusteringofDissaidtobeindividuallyfairifittreatseverydatapointofDindividuallyfair.70FortherestofthepaperweassumeDtobeﬁnite.Ourdeﬁnitionofindividualfairnessforclustering71canthenbeformallystatedasfollows(forl2N,wewrite[l]={1,...,l}):72Deﬁnition1(Individuallyfairclustering).LetC=(C1,...,Ck)beak-clusteringofD,thatis73D=C1˙[...˙[CkandCi6=;fori2[k].Forx2D,wewriteC(x)fortheclusterCithatx742C3C2C1x1x2x3x40.720.640.71Figure1:TwodatasetsinR2withdequalingtheEuclideanmetric.Left:Anindividuallyfair3-clusteringwithclustersC1,C2,C3.Right:Fourpointsforwhichthereisnoindividuallyfair2-clustering.Forexample,ifthetwoclusterswereC1={x1,x4}andC2={x2,x3},thenx1wouldbetreatedunfairbecauseofd(x1,x4)=0.71>0.68=[d(x1,x2)+d(x1,x3)]/2.whereallcustomersinoneclustergetthesamecouponsaccordingtotheir(hypothesized)preferences.39Acustomerthatendsupinaclusterwithratherdifferentothercustomers(andhenceisnotwell40representedbyitscluster)mightgetcouponsthatarelessvaluabletoherthanthecouponsshewould41havegotifshehadbeenassignedtotheclusterthatisbestrepresentingher.42Motivatedbysuchanexample,ournotionofindividualfairnessasksthateachdatapointisassigned43tothebestrepresentingclusterinthesensethatthedatapoint,onaverage,isclosertothepointsin44itsownclusterthantothepointsinanyothercluster.Whileournotionisrelatedtoawell-known45conceptofclusteringstability(Balcanetal.,2008),manyfundamentalquestionsremainunanswered.46Forinstance,unlikethegroupfairnessnotionsstudiedabove,whereafairsolutionalwaysexistsand47thegoalistoﬁndtheonewithminimumcost,anindividuallyfairclusteringmaynotnecessarily48exist(foraﬁxednumberofclusters).Inthiswork,wemakethefollowingcontributionstowards49understandingindividualfairnessforclustering:50•Weproposeanaturalnotionofindividualfairnessforclusteringrequiringthateverydatapoint,on51average,isclosertothepointsinitsownclusterthantothepointsinanyothercluster.52•Whenthedataliesontherealline,weshowthatanindividuallyfairclusteringalwaysexists,and53wedesignanefﬁcientalgorithmtoﬁndone.Wearguewhythis1-dimcaseandouralgorithmare54interestingonitsown.55•WeshowthatevenforEuclideandatasetsinR2,individuallyfairclusteringsmightnotexistand56provethattheproblemofdecidingwhetheragivendatasethasanindividuallyfairk-clusteringis57NP-hard,evenfork=2andwhentheunderlyingdistancefunctionisassumedtobeametric.58•Weperformexperimentsonrealdatasetsandcomparetheperformanceofourpolynomialtime59algorithmforthe1-dimcasewithk-meansclustering.Inthecaseofhigherdimensions,weinvestigate60severalstandardclusteringalgorithmswithrespecttoourfairnessnotion.612FairnessNotion62OurnotionofindividualfairnessappliestoadatasetDtogetherwithagivendissimilarityfunctiond63thatmeasureshowclosetwodatapointsare.Weusethetermsdissimilarityanddistancesynony-64mously.Weassumed:D⇥D!R 0tobesymmetricwithd(x,x)=0,butnotnecessarilytobea65metric(i.e.,toadditionallysatisfythetriangleinequalityandd(x,y)=0,x=y).66OurfairnessnotiondeﬁneswhatitmeansthatadatapointistreatedfairinaclusteringofD;namely:67adatapointistreatedindividuallyfairiftheaveragedistancetothepointsinitsowncluster(the68pointitselfexcluded)isnotgreaterthantheaveragedistancetothepointsinanyothercluster.Thena69clusteringofDissaidtobeindividuallyfairifittreatseverydatapointofDindividuallyfair.70FortherestofthepaperweassumeDtobeﬁnite.Ourdeﬁnitionofindividualfairnessforclustering71canthenbeformallystatedasfollows(forl2N,wewrite[l]={1,...,l}):72Deﬁnition1(Individuallyfairclustering).LetC=(C1,...,Ck)beak-clusteringofD,thatis73D=C1˙[...˙[CkandCi6=;fori2[k].Forx2D,wewriteC(x)fortheclusterCithatx7428

1
3

1

8

Figure 2: An example of a data set on the real line with more than one individually fair clustering.
Left: The data set and the distances between the points. Right: The same data set with two fair
2-clusterings (one encoded by color: red vs blue / one encoded by frames: solid vs dotted boundary).

for all i
∈
individually fair.1

[k] with Ci (cid:54)

= C(x). The clustering

C

is individually fair if every x

is treated

∈ D

We discuss some important observations about individually fair clusterings as deﬁned in Deﬁnition 1:
if in a clustering all clusters are well-separated and sufﬁciently far apart, then this clustering is fair.
An example of such a scenario is provided in the left part of Figure 1. Hence, at least for such
simple clustering problems with an “obvious” solution, individual fairness does not conﬂict with the
clustering goal of partitioning the data set such that “data points in the same cluster are similar to each
other, and data points in different clusters are dissimilar” (Celebi and Aydin, 2016, p. 306). However,
there are also data sets for which no fair k-clustering exists (for a ﬁxed k and a given distance
function d).2 This can even happen for Euclidean data sets and k = 2, as the right part of Figure 1
shows. If a data set allows for an individually fair k-clustering, there might be more than one fair
k-clustering. An example of this is shown in Figure 2. This example also illustrates that individual
fairness does not necessarily work towards the aforementioned clustering goal. Indeed, in Figure 2
the two clusters of the clustering encoded by the frames, which is fair, are not even contiguous.

These observations raise a number of questions such as: when does a fair k-clustering exist? Can we
efﬁciently decide whether a fair k-clustering exists? If a fair k-clustering exists, can we efﬁciently
compute it? Can we minimize some (clustering) objective over the set of all fair clusterings? If no
fair k-clustering exists, can we ﬁnd a clustering that violates inequality (1) only for a few data points,
or a clustering that potentially violates (1) for every data point, but only to a minimal extent? How do
standard clustering algorithms such as Lloyd’s algorithm (aka k-means) or linkage clustering (e.g.,
Shalev-Shwartz and Ben-David, 2014, Section 22) perform in terms of fairness? Are there simple
modiﬁcations to these algorithms in order to improve their fairness? In this paper, we explore some
of these questions as outlined in Section 1.

2.1 Related Work and Concepts

We provide a detailed overview in Appendix A. Here we only present a brief summary.

Existing Notions of Individual Fairness Dwork et al. (2012) were the ﬁrst to provide a notion of
individual fairness by asking that similar data points (as measured by a given task-speciﬁc metric)
should be treated similarly by a randomized classiﬁer. Subsequently, individual fairness has been
studied in multi-armed bandit problems (Joseph et al., 2016, 2018; Gillen et al., 2018). The recent
work of Kearns et al. (2019b) introduces the notion of average individual fairness.

Fairness for Clustering The most established notion of fairness for clustering has been proposed
by Chierichetti et al. (2017). It asks that each cluster has proportional representation from different
demographic groups. Several follow-up works extend that work (Rösner and Schmidt, 2018; Schmidt
et al., 2018; Ahmadian et al., 2019; Anagnostopoulos et al., 2019; Backurs et al., 2019; Bera et al.,
2019; Bercea et al., 2019; Huang et al., 2019; Kleindessner et al., 2019b; Davidson and Ravi, 2020).

Alternative fairness notions for clustering are tied to centroid-based clustering such as k-means,
k-center and k-median (Kleindessner et al., 2019a; Chen et al., 2019; Jung et al., 2020). The recent
notion of Jung et al. (2020) is the only one that comes with a guarantee for every single data point. It
asks that every data point is somewhat close to a center, where “somewhat” depends on how close the
data point is to its k nearest neighbors and the motivation for this notion comes from facility location.

1For brevity, when it is clear from the context, instead of “individually fair” we may only say “fair”.
2 Of course, the trivial 1-clustering C = (D) or the trivial |D|-clustering that puts every data point in a

singleton are fair, and for a trivial distance function d ≡ 0, every clustering is fair.

3

Our notion of individual
Average Attraction Property and Game-theoretic Interpretation
fairness is closely related to the average attraction property studied by Balcan et al. (2008), and our
notion also has a game-theoretic interpretation.

3 NP-Hardness

In this section we present one of the main results of our paper, stating the NP-hardness of deciding
whether an individually fair k-clustering exists. For such a result, it is crucial to specify how an input
together with a distance function d is represented by
instance is encoded: we assume that a data set
the distance matrix (d(x, y))x,y
Theorem 1 (NP-hardness of individually fair clustering). Deciding whether a data set
together
with a distance function d has an individually fair k-clustering (for a given parameter k) is NP-hard.
This even holds if k = 2 is ﬁxed and d is required to be a metric.

. Under this assumption we can prove the following theorem:

∈D

D

D

The proof of Theorem 1 is provided in Appendix B. It shows NP-hardness of the individually fair
clustering decision problem via a reduction from a variant of 3-SAT. In this variant, we can assume a 3-
SAT instance to have the same number of clauses as number of variables and that each variable occurs
C2 ∧
. . .
Cn over variables x1, . . . , xn, we
in at most three clauses. Given such a formula Φ = C1 ∧
xn}
, C1, . . . , Cn, x1,
T rue, F alse, (cid:63),
construct a metric space (
{
¬
∞
D
D
such that Φ is satisﬁable if and only if
has an individually fair 2-clustering. The difﬁcult part is in
deﬁning an appropriate metric d to accomplish this.

x1, . . . , xn,

, d) with

=

D

∧

¬

Unless P = NP, Theorem 1 implies that for general data sets, even when being guaranteed that a
fair k-clustering exists, there cannot be any efﬁcient algorithm for computing such a fair clustering.
However, as with all NP-hard problems, there are two possible remedies: ﬁrst, we can restrict our
considerations to data sets with some special structure. This is what we do in Section 4, where we
show that for 1-dimensional Euclidean data sets fair clusterings always exist and can be computed
in polynomial time. We consider it to be an interesting question for follow-up work whether one
can identify other classes of data sets with such a property (cf. Section 6). Second, we can look
at approximate versions of individual fairness in which we allow inequality (1) to be violated for a
certain number of points or where we relax inequality (1) by introducing a multiplicative factor γ > 1
on its right side. We start exploring this direction in our experiments in Section 5.2.

4 1-dimensional Euclidean Case

D ⊆

≤ |D|

R and d being the Euclidean metric. We ﬁrst show that in this case, for any 1

≤
, a fair k-clustering always exists. In fact, we show that there exists a fair k-clustering with

One way to cope with the NP-hardness of the individually fair clustering problem is to restrict our
considerations to data sets with some special structure. As an important example, here we study the
special case of
k
. . .
with x1 ≤
x2 ≤
=
x1, . . . , xn}
contiguous clusters. By contiguous clusters we mean that if
{
≤
) for
xik−1+1, . . . , xn}
, . . . ,
xi1+1, . . . , xi2 }
xn, the clustering is of the form
{
{
some 1
1 < n. It might be surprising at a ﬁrst glance that there also exist fair
clusterings of 1-dimensional data sets with non-contiguous clusters, and indeed this seems to happen
rarely, but it can happen as the example provided in Figure 2 shows. Subsequently, we provide an
efﬁcient dynamic programming (DP) approach that ﬁnds a fair k-clustering solving

,
x1, . . . , xi1 }
{

i1 < i2 < . . . < ik

= (

≤

D

C

−

=(C1,...,Ck):

is a fair clus-
with contiguous clusters

C
tering of

min
C

D

[n] with (cid:80)k

(
(cid:107)

C1| −
|

t1, . . . ,

Ck| −
|

tk)

(cid:107)p,

(2)

where t1, . . . , tk ∈
denotes the p-norm.

i=1 ti = n are given target cluster sizes, p

R

∈

1 ∪ {∞}

≥

and

(cid:107) · (cid:107)p

We believe that the results of this section are interesting on its own. As an example consider the
scenario that a teacher wants to give grades based on the number of points that a student obtained by
setting some threshold values (e.g., a student gets a B if her number of points is in between 75 and 90).
This can be interpreted as a 1-dim clustering problem, where clusters have to be contiguous and
individual fairness seems to be a highly desirable goal. Furthermore, some teachers aim for a certain
grade distribution (aka grading on a curve), in which case the problem can be phrased in the form of
(2). Clearly, one can think of similar examples in the context of credit scores or recidivism risk scores.

4

{

x2 ≤
, where x1 ≤
. . .
xik−1+1, . . . , xn}
, . . . , Ck =
{

Let us now present our technical results (proofs in Appendix C). A key observation is that a clustering
with contiguous clusters is fair if and only if the boundary points of the clusters are treated fair:
Lemma 1 (Fair boundary points imply fair clustering). Let

=
x1, . . . , xn}
D
xi1+1, . . . , xi2 }
{
individually fair if and only if all points xil and xil+1, l
xil (xil+1, resp.)
xil+1}
(Cl+1 \ {
The next theorem states that an individually fair k-clustering with contiguous clusters always exists.
R and d be the Euclidean metric.
Theorem 2 (Existence of individually fair k-clustering). Let
For any k
with contiguous clusters.

∈
is treated fair if and only if its average distance to the points in Cl \ {
, resp.) is not greater than the average distance to the points in Cl+1 (Cl, resp.).

= (C1, . . . , Ck) be a k-clustering of
, C2 =
{
i1 < . . . < ik
is
C
−
1], are treated fair. Furthermore,
xil }

C
xn, with contiguous clusters C1 =
, for some 1

, there exists an individually fair k-clustering of

x1, . . . , xi1}
1 < n. Then

1, . . . ,

D ⊆

≤
[k

≤

−

∈ {

|D|}

D

The proof of Theorem 2 is constructive and provides an algorithm to compute a fair k-center clustering
1 boundary indices, corresponding
with contiguous clusters. This algorithm works by maintaining k
to a clustering with contiguous clusters, and repeatedly increasing these indices until a fair clustering
is found. We prove that at the latest when no index can be increased anymore, a fair clustering must
have been found. However, the running time of the algorithm scales exponentially with k.

−

To overcome this, in the following we propose an efﬁcient DP approach to ﬁnd a solution to (2). Let

=

x1, . . . , xn}

{

D

. . .

with x1 ≤
T (i, j, l) =

≤

xn. Our approach builds a table T
C1| −
(
|

∈
Cl| −

t1, . . . ,

i,j,l (cid:107)

min
(C1,...,Cl)

|

(N

∪ {∞}
p
tl)
p
(cid:107)

∈H

)n

n

×

×

k with

(3)

for i

[n], j

∈

Hi,j,l = (cid:8)

∈

C

[n], l

∈

[k], where

= (C1, . . . , Cl) :
contiguous clusters such that the right-most cluster Cl contains exactly j points(cid:9)

is a fair l-clustering of

x1, . . . , xi}
{

with l non-empty

C

and T (i, j, l) =
approach to the case p =

∞

if

. Here, we consider the case p
∅

are minimal and are described in Appendix D.

∞

=

. The modiﬁcations of our

Hi,j,l =
∞

[n] T (n, j, k)1/p. Below, we will describe how to use the
The optimal value of (2) is given by minj
table T to compute an individually fair k-clustering solving (2). First, we explain how to build T . We
have, for i, j

[n],

∈

∈

(cid:26)

i

p,

t1|

j = i,
= i
j

T (i, j, 1) =

|
∞
,
∞
and the recurrence relation, for l > 1 and j + l

T (i, j, l) =

1 > i,

j + l

−
,

−

,

(cid:40)

T (i, j, i) =

∞

1

−

≤

i,

T (i, j, l) =

j
|

−

p + min

tl|

T (i

−

j, s, l

−

1) : s

[i

j

−

∈

−

(l

−

2)],

(cid:40)(cid:80)i

1
s=1 |
,

p,

ts|

−

j = 1,
= 1
j

,

(4)

≤
(cid:41)

,

(5)

(cid:80)s

1
−
f =1 |

xi
j −
−
1
s
−
j+1 −
−
s

xi

j

f |
−

−

xi

−

j

f |
−

(cid:80)j

xi

f =1 |

xi

j+f |
−

,

(cid:80)j

f =2 |

j −
−
j

xi

j+1 −
−
1
j
−

xi

j+f |

−

≤

(cid:80)s

1
−
f =0 |

xi

where we use the convention that 0
explain the recurrence relation (5) and argue why it is correct in Appendix D.

0 = 0 for the fractions on the left sides of the inequalities. We

It is not hard to see that using (5), we can build the table T in time
can compute a solution (C ∗1 , . . . , C ∗k ) to (2) by specifying
, . . . ,
let v∗ = minj
[n] T (n, j, k). We set
l = k
(cid:80)k

(n3k). Once we have T , we
O
C ∗k |
(nk) as follows:
|
= j0 for an arbitrary j0 with v∗ = T (n, j0, k). For
, h0, l) +
= h0 for an arbitrary h0 with (i) T (n
1 many
points on its left side is not greater than the average distance to the points in C ∗l+1, and (iii) the average
+1 to the other points in C ∗l+1 is not greater than the average distance to
distance of xn

1, . . . , 2, we then set
tr|
C ∗r | −

C ∗r |
r=l+1 |
to the closest h0 −

p = v∗, (ii) the average distance of xn

−
r=l+1 ||

C ∗k |
|

in time

C ∗l |

C ∗1 |

r=l+1 |

C∗
r |

(cid:80)k

(cid:80)k

(cid:80)k

O

−

−

∈

|

|

−

r=l+1 |

C∗
r |

5

(cid:54)
(cid:54)
(cid:54)
Table 1: Experiment on German credit data set. Clustering 1000 people according to their credit
amount. Target cluster sizes ti = 1000

[k]. k-ME++= k-means++. Best values in bold.

k , i

∈

# UNF MVI OBJ

COSQ

CO

# UNF MVI OBJ

COSQ

CO

NAIVE
DP
k-MEANS
k-ME++

105
0
1
0.79

2.95
1.0
1.0
1.0

k = 5

0
172
170
279

k = 50

4.78
1.39
1.39
1.38

23.53
17.62
17.61
19.36

101
0
18
11.04

2.6
1.0
1.26
1.15

0
8
10
50

0.19
0.08
0.1
0.01

3.06
2.29
2.54
1.72

the closest h0 many points on its left side. Finally, it is
C ∗1 |
deﬁnition of the table T in (3) and Lemma 1 that for l = k
−
satisfying (i) to (iii) and that our approach yields an individually fair k-clustering (C ∗1 , . . . , C ∗k ) of
Hence we have shown the following theorem:
Theorem 3 (Efﬁcient DP approach solves (2)). By means of the dynamic programming approach (3)
to (5) we can compute an individually fair clustering solving (2) in running time

. It follows from the
1, . . . , 2 we can always ﬁnd some h0
.

(n3k).

C ∗r |

r=2 |

= n

−

D

|

(cid:80)k

O

5 Experiments

We ﬁrst study the case of 1-dim Euclidean data, where we can apply our DP approach of Section 4.
We then deal with general data sets. In this case, individually fair clusterings in the strict sense of
Deﬁnition 1, which are required to treat every data point fair, may not exist, and even if they do, there is
no efﬁcient way to compute them (cf. Section 3). Hence, we have to settle for approximate versions of
Deﬁnition 1 and fall back on approximation algorithms or heuristics. As a starting point for a study of
“approximate individual fairness” and a thorough search for approximation algorithms with guarantees
(cf. Section 6), we investigate the extent to which standard clustering algorithms violate individual
fairness and consider a heuristic approach for ﬁnding approximately fair clusterings. Our experiments
are intended to serve as a proof of concept. They do not focus on the running times of the algorithms
or their applicability to large data sets. Hence, we only use rather small data sets of sizes 500 to 1885.

D

Let us deﬁne some quantities: we measure the extent to which a k-clustering
dataset

is (un-)fair by # Unf (“number unfair”) and MVi (“maximum violation”) deﬁned as

= (C1, . . . , Ck) of a

C

x

|{

∈ D

# Unf =

: x is not treated fair

C(x) d(x, y)
∈
Ci d(x, y)
∈
where we use the convention that 0
is fair if and only if # Unf = 0 and
MVi
(with
respect to the goal of putting similar data points into the same cluster) by the k-means cost, referred to
by Co (“cost”), which is compatible
as CoSq (“cost squared”). In general, we measure the quality of
with Deﬁnition 1 in that it uses ordinary rather than squared distances as CoSq. It is

C
Rm and d is the Euclidean metric, we measure the quality of

, MVi = max
∈D

0 = 0. The clustering

1. Mainly if

max
=C(x)

1
(cid:80)
y

1
C(x)
|−
1
Ci
|

, (6)

D ⊆

≤

}|

Ci

C

C

x

|

|

(cid:80)
y

CoSq =

k
(cid:88)

i=1

1
Ci|

2
|

(cid:88)

x,y

Ci

∈

d(x, y)2,

Co =

k
(cid:88)

i=1

1
Ci|

2
|

(cid:88)

x,y

Ci

∈

d(x, y).

(7)

The reason for using CoSq as a measure of quality is to provide a fair evaluation of k-means clustering.

We performed all experiments in Python (code in the supplementary material). We used the
standard clustering algorithms from Scikit-learn or SciPy with all parameters set to their default values.

5.1

1-dimensional Euclidean Data Sets

We used the German Credit data set (Dua and Graff, 2019). It comprises 1000 records (corresponding
to human beings) and for each record one binary label (good vs. bad credit risk) and 20 features.

In our ﬁrst experiment, we clustered the 1000 people according to their credit amount, which is one
of the 20 features. A histogram of the data can be seen in Figure 5 in Appendix E. We were aiming
for k-clusterings with clusters of equal size (i.e., target cluster sizes ti = 1000
[k]) and compared

k , i

∈

6

(cid:54)
Figure 3: # Unf (left), MVi (middle) and CoSq (right) for the clusterings produced by the various
algorithms as a function of k.

∞

our DP approach of Section 4 with p =
to k-means clustering as well as a naive clustering that
simply puts the t1 smallest points in the ﬁrst cluster, the next t2 many points in the second cluster,
and so on. We considered two initialization strategies for k-means: we either used the medians of
the clusters of the naive clustering for initialization (thus, hopefully, biasing k-means towards the
target cluster sizes) or we ran k-means++ (Arthur and Vassilvitskii, 2007). For the latter we report
average results obtained from running the experiment for 100 times. In addition to the four quantities
# Unf, MVi, CoSq and Co deﬁned in (6) and (7), we report Obj (“objective”), which is the value of
the objective function of (2) for p =
. Note that k-means yields contiguous clusters and Obj is
meaningful for all four clustering methods that we consider.

∞

The results are provided in Table 1 (k = 5 and k = 50) and in Table 2 (k = 10 and k = 20) in
Appendix E. As expected, for the naive clustering we always have Obj = 0, for our DP approach
1, and k-means++ (k-ME++) performs best in terms of CoSq.
(DP) we have # Unf = 0 and MVi
Most interesting to see is that both versions of k-means yield almost perfectly fair clusterings when k
is small and moderately fair clusterings when k = 50 (with k-means++ outperforming k-means).

≤

In our second experiment (presented in Appendix E), we used the ﬁrst 500 records to train a multi-
layer perceptron (MLP) for predicting the label (good vs. bad credit risk). We then applied the MLP
to estimate the probabilities of having a good credit risk for the other 500 people. We used the same
clustering methods as in the ﬁrst experiment to cluster the 500 people according to their probability
estimate. We believe that such a clustering problem may arise frequently in practice (e.g., when a
bank determines its lending policy) and that individual fairness is highly desirable in this context.

5.2 General Data Sets

We performed the same set of experiments on the ﬁrst 1000 records of the Adult data set, the Drug
Consumption data set (1885 records), and the Indian Liver Patient data set (579 records) (Dua and
Graff, 2019). As distance function d we used the Euclidean, Manhattan or Chebyshev metric. Here we
only present the results for the Adult data set and the Euclidean metric, the other results are provided
in Appendix F. Our observations are largely consistent between the different data sets and metrics.

First Experiment — (Un-)Fairness of Standard Algorithms Working with the Adult data set,
we only used its six numerical features (e.g., age, hours worked per week), normalized to zero mean
and unit variance, for representing records. We applied several standard clustering algorithms as
well as the group-fair k-center algorithm of Kleindessner et al. (2019a) (referred to as k-center GF)
to the data set (k-means++; k-medoids; spectral clustering (SC)) or its distance matrix (k-center
using the greedy strategy of Gonzalez (1985); k-center GF; single / average / complete linkage
clustering).
In order to study the extent to which these methods produce (un-)fair clusterings,
for k = 2, 5, 10, 20, 30, . . . , 100, we computed # Unf and MVi as deﬁned in (6) for the resulting
k-clusterings. For measuring the quality of the clusterings we computed CoSq or Co as deﬁned in (7).

The results are provided in Figure 3. For k-means++, k-medoids, k-center, k-center GF and SC
we show average results obtained from running them for 25 times since their outcomes depend on
random initializations. We can see that, in particular for large values of k, k-center, k-center GF, SC,
and the linkage algorithms can be quite unfair with rather large values of # Unf and MVi. In contrast,
1.4 even when k is large.
k-means++ produces rather fair clusterings with # Unf
For a baseline comparison, for a random clustering in which every data point was assigned to one of k
clusters uniformly at random we observed # Unf = 990 and MVi = 4.0 on average (when k = 100).
The beneﬁcial behavior of k-means++ with respect to our notion of individual fairness raises the

90 and MVi

≤

≤

7

0204060801000200400600800k-means++k-meGoiGsk-Fenterk-Fenter GF6C6ingle link.Avg. link.ComSl. link.25102030405060708090100k01002003004005006007008009001000#8nfAdult data set (1000 points) --- (uclidean metUic25102030405060708090100k1.01.52.02.53.03.54.04.509iAdult data Vet (1000 pointV) --- (uclidean metric020406080100k100020003000400050006000Co6qAdult data set (1000 Soints) --- (uclidean metricFigure3:#Unf(left),MVi(middle)andCoSq(right)fortheclusteringsproducedbythevariousalgorithmsasafunctionofk.TheresultsareprovidedinTable1(k=5andk=50)andinTable2(k=10andk=20)in237AppendixE.Asexpected,forthenaiveclusteringwealwayshaveObj=0,forourDPapproach238(DP)wehave#Unf=0andMVi≤1,andk-means++(k-ME++)performsbestintermsofCoSq.239Mostinterestingtoseeisthatbothversionsofk-meansyieldalmostperfectlyfairclusteringswhenk240issmallandmoderatelyfairclusteringswhenk=50(withk-means++outperformingk-means).241Inoursecondexperiment(presentedinAppendixE),weusedtheﬁrst500recordstotrainamulti-242layerperceptron(MLP)forpredictingthelabel(goodvs.badcreditrisk).WethenappliedtheMLP243toestimatetheprobabilitiesofhavingagoodcreditriskfortheother500people.Weusedthesame244clusteringmethodsasintheﬁrstexperimenttoclusterthe500peopleaccordingtotheirprobability245estimate.Webelievethatsuchaclusteringproblemmayarisefrequentlyinpractice(e.g.,whena246bankdeterminesitslendingpolicy)andthatindividualfairnessishighlydesirableinthiscontext.2475.2GeneralDataSets248Weperformedthesamesetofexperimentsontheﬁrst1000recordsoftheAdultdataset,theDrug249Consumptiondataset(1885records),andtheIndianLiverPatientdataset(579records)(Duaand250Graff,2019).AsdistancefunctiondweusedtheEuclidean,ManhattanorChebyshevmetric.Herewe251onlypresenttheresultsfortheAdultdatasetandtheEuclideanmetric,theotherresultsareprovided252inAppendixF.Ourobservationsarelargelyconsistentbetweenthedifferentdatasetsandmetrics.253FirstExperiment—(Un-)FairnessofStandardAlgorithmsWorkingwiththeAdultdataset,254weonlyuseditssixnumericalfeatures(e.g.,age,hoursworkedperweek),normalizedtozeromean255andunitvariance,forrepresentingrecords.Weappliedseveralstandardclusteringalgorithmsas256wellasthegroup-fairk-centeralgorithmofKleindessneretal.(2019a)(referredtoask-centerGF)257tothedataset(k-means++;k-medoids;spectralclustering(SC))oritsdistancematrix(k-center258usingthegreedystrategyofGonzalez(1985);k-centerGF;single/average/completelinkage259clustering).Inordertostudytheextenttowhichthesemethodsproduce(un-)fairclusterings,260fork=2,5,10,20,30,...,100,wecomputed#UnfandMViasdeﬁnedin(6)fortheresulting261k-clusterings.FormeasuringthequalityoftheclusteringswecomputedCoSqorCoasdeﬁnedin(7).262TheresultsareprovidedinFigure3.Fork-means++,k-medoids,k-center,k-centerGFandSC263weshowaverageresultsobtainedfromrunningthemfor25timessincetheiroutcomesdependon264randominitializations.Wecanseethat,inparticularforlargevaluesofk,k-center,k-centerGF,SC,265andthelinkagealgorithmscanbequiteunfairwithratherlargevaluesof#UnfandMVi.Incontrast,266k-means++producesratherfairclusteringswith#Unf≤90andMVi≤1.4evenwhenkislarge.267Forabaselinecomparison,forarandomclusteringinwhicheverydatapointwasassignedtooneofk268clustersuniformlyatrandomweobserved#Unf=990andMVi=4.0onaverage(whenk=100).269Thebeneﬁcialbehaviorofk-means++withrespecttoournotionofindividualfairnessraisesthe270questionwhetheronecanproveguaranteesontheextenttowhichclusteringsproducedbyk-means++271arefair(cf.Section6).Thek-medoidsalgorithmperformsworsethank-means++,butbetterthan272theotheralgorithms.Theclusteringsproducedbyk-centerGF,whichweranwiththeconstraint273ofchoosing⌊k/2⌋femaleand⌈k/2⌉malecenters,areslightlymorefairthantheonesproduced274byk-center.However,notethatitreallydependsonthedatasetwhetheragroup-fairclusteringis275individuallyfairornot(exampleprovidedinAppendixF.1).Unsurprisingly,k-means++outperforms276theothermethodsintermsofCoSqsinceitisdesignedwiththegoalofminimizingthisquantity.277SecondExperiment—HeuristicstoImproveLinkageClusteringOnemightwonderwhether278therearemodiﬁcationstothestandardclusteringalgorithmsthatmakethemmorefair.Anaturalidea2797Figure 4: # Unf (left), MVi (middle) and Co (right) for the clusterings produced by average linkage
clustering and the two variants of our heuristic to improve it: the ﬁrst (#U in the legend) greedily
chooses splits as to minimize # Unf, the second (MV) as to minimize MVi.

question whether one can prove guarantees on the extent to which clusterings produced by k-means++
are fair (cf. Section 6). The k-medoids algorithm performs worse than k-means++, but better than
the other algorithms. The clusterings produced by k-center GF, which we ran with the constraint
of choosing
male centers, are slightly more fair than the ones produced
by k-center. However, note that it really depends on the data set whether a group-fair clustering is
individually fair or not (example provided in Appendix F.1). Unsurprisingly, k-means++ outperforms
the other methods in terms of CoSq since it is designed with the goal of minimizing this quantity.

female and

k/2

k/2

(cid:98)

(cid:99)

(cid:100)

(cid:101)

Second Experiment — Heuristics to Improve Linkage Clustering One might wonder whether
there are modiﬁcations to the standard clustering algorithms that make them more fair. A natural idea
to make any clustering more fair is to make local changes to it and iteratively pick a data point that is
not treated fair and assign it to the cluster that it is closest too. After picking and reassigning a data
point, this point is treated fair. However, in experiments we observed that usually we can only pick a
very small number of data points whose reassignment does not cause other points that are initially
treated fair to be treated unfair after the reassignment (example provided in Appendix F.2).

Another idea that we want to present here is speciﬁcally tied to linkage clustering. As our experiments
show this idea results in linkage clustering producing clusterings that are signiﬁcantly more fair
than the ones produced by ordinary linkage clustering. Linkage clustering builds a binary tree that
represents a hierarchical clustering with the root of the tree corresponding to the whole data set and
every node corresponding to a subset such that a parent is the union of its two children. The leaves of
the tree correspond to singletons comprising one data point (e.g., Shalev-Shwartz and Ben-David,
2014, Section 22.1). If one wants to obtain a k-clustering of the data set, the output of a linkage
clustering algorithm is a certain pruning of this tree. When individual fairness is a goal, we propose
to construct a k-clustering / a pruning of the tree as follows (pseudocode provided in Appendix F.3):
starting with the two children of the root, we maintain a set of nodes that corresponds to a clustering
2 rounds. In round i, we greedily split one of the i + 1 many nodes that we
and proceed in k
currently have into its two children such that the resulting (i + 2)-clustering minimizes, over the i + 1
many possible splits, # Unf as deﬁned in (6). Alternatively, we can split the node that gives rise to a
minimum value of MVi (also deﬁned in (6)).

−

In Figure 4, we show # Unf, MVi and Co for ordinary average linkage clustering and a modiﬁed
version using our heuristic approach in its both variants (#U denotes the variant based on # Unf and
MV the variant based on MVi). Analogous experiments with single or complete instead of average
linkage clustering are presented in Appendix F. We can see that our approach leads to a signiﬁcant
50 this holds for both variants, but in particular for the variant aiming
improvement in # Unf (for k
to minimize # Unf). The variant based on MVi leads to an improvement in MVi. However, these
improvements come at the price of an increase in Co as we can see from the right plot of Figure 4.

≤

6 Discussion

In this work we contributed to the study of individual fairness in the context of clustering, which
is only in its infancy. We proposed a notion of individual fairness that aims at data points being
well represented by their clusters. Formally, it asks that every data point, on average, is closer to
the points in its own cluster than to the points in any other cluster. This notion raises numerous
questions, some of which we addressed: we showed that for general data sets, it is NP-hard to decide
whether an individually fair k-clustering exists. In contrast, for one-dimensional Euclidean data
sets we can compute a fair clustering by means of an efﬁcient dynamic programming approach. We

8

25102030405060708090100k0100200300400#8nfAdult data Vet (1000 pointV) --- (uclidean metUicAvg. link.Avg. #8Avg. 0925102030405060708090100k1.01.52.02.53.03.54.009iAdult data Vet (1000 pointV) --- (uclidean metric25102030405060708090100k600800100012001400CoAdult data set (1000 points) --- (uclidean metricFigure4:#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbyaveragelinkageclusteringandthetwovariantsofourheuristictoimproveit:theﬁrst(#Uinthelegend)greedilychoosessplitsastominimize#Unf,thesecond(MV)astominimizeMVi.tomakeanyclusteringmorefairistomakelocalchangestoitanditerativelypickadatapointthatis280nottreatedfairandassignittotheclusterthatitisclosesttoo.Afterpickingandreassigningadata281point,thispointistreatedfair.However,inexperimentsweobservedthatusuallywecanonlypicka282verysmallnumberofdatapointswhosereassignmentdoesnotcauseotherpointsthatareinitially283treatedfairtobetreatedunfairafterthereassignment(exampleprovidedinAppendixF.2).284Anotherideathatwewanttopresenthereisspeciﬁcallytiedtolinkageclustering.Asourexperiments285showthisidearesultsinlinkageclusteringproducingclusteringsthataresigniﬁcantlymorefair286thantheonesproducedbyordinarylinkageclustering.Linkageclusteringbuildsabinarytreethat287representsahierarchicalclusteringwiththerootofthetreecorrespondingtothewholedatasetand288everynodecorrespondingtoasubsetsuchthataparentistheunionofitstwochildren.Theleavesof289thetreecorrespondtosingletonscomprisingonedatapoint(e.g.,Shalev-ShwartzandBen-David,2902014,Section22.1).Ifonewantstoobtainak-clusteringofthedataset,theoutputofalinkage291clusteringalgorithmisacertainpruningofthistree.Whenindividualfairnessisagoal,wepropose292toconstructak-clustering/apruningofthetreeasfollows(pseudocodeprovidedinAppendixF.3):293startingwiththetwochildrenoftheroot,wemaintainasetofnodesthatcorrespondstoaclustering294andproceedink−2rounds.Inroundi,wegreedilysplitoneofthei+1manynodesthatwe295currentlyhaveintoitstwochildrensuchthattheresulting(i+2)-clusteringminimizes,overthei+1296manypossiblesplits,#Unfasdeﬁnedin(6).Alternatively,wecansplitthenodethatgivesrisetoa297minimumvalueofMVi(alsodeﬁnedin(6)).298InFigure4,weshow#Unf,MViandCoforordinaryaveragelinkageclusteringandamodiﬁed299versionusingourheuristicapproachinitsbothvariants(#Udenotesthevariantbasedon#Unfand300MVthevariantbasedonMVi).Analogousexperimentswithsingleorcompleteinsteadofaverage301linkageclusteringarepresentedinAppendixF.Wecanseethatourapproachleadstoasigniﬁcant302improvementin#Unf(fork≤50thisholdsforbothvariants,butinparticularforthevariantaiming303tominimize#Unf).ThevariantbasedonMVileadstoanimprovementinMVi.However,these304improvementscomeatthepriceofanincreaseinCoaswecanseefromtherightplotofFigure4.3056Discussion306Inthisworkwecontributedtothestudyofindividualfairnessinthecontextofclustering,which307isonlyinitsinfancy.Weproposedanotionofindividualfairnessthataimsatdatapointsbeing308wellrepresentedbytheirclusters.Formally,itasksthateverydatapoint,onaverage,iscloserto309thepointsinitsownclusterthantothepointsinanyothercluster.Thisnotionraisesnumerous310questions,someofwhichweaddressed:weshowedthatforgeneraldatasets,itisNP-hardtodecide311whetheranindividuallyfairk-clusteringexists.Incontrast,forone-dimensionalEuclideandata312setswecancomputeafairclusteringbymeansofanefﬁcientdynamicprogrammingapproach.We313examinedstandardclusteringalgorithmsandsawthatk-means++oftenproducesclusteringsthatare314onlyslightlyunfair.Wealsostudiedasimpleheuristictomakelinkageclusteringmorefair.315Still,manyquestionsremainopen,andwehopetoinspirefollow-upworktoaddresssomeofthese:316usingourmeasures#UnforMVi(cf.Section5),orsomeothermeasure,todeﬁneanotionof317“approximateindividualfairness”,canwedesignalgorithmswithprovableguaranteesforﬁndingsuch318anapproximatelyfairclustering?Canwedosoforgeneraldatasets,orwhichassumptionsaboutthe319datasetdoweneedtomake?Arethereclassesofdatasetsotherthan1-dimensionalEuclideanones320thatallowfora(strictly)individuallyfairclustering?Finally,canweprovideguaranteesforEuclidean321datasetsandk-means++clustering,whichperformedsurprisinglywellinourexperiments?3228examined standard clustering algorithms and saw that k-means++ often produces clusterings that are
only slightly unfair. We also studied a simple heuristic to make linkage clustering more fair.

Still, many questions remain open, and we hope to inspire follow-up work to address some of these:
using our measures # Unf or MVi (cf. Section 5), or some other measure, to deﬁne a notion of
“approximate individual fairness”, can we design algorithms with provable guarantees for ﬁnding such
an approximately fair clustering? Can we do so for general data sets, or which assumptions about the
data set do we need to make? Are there classes of data sets other than 1-dimensional Euclidean ones
that allow for a (strictly) individually fair clustering? Finally, can we provide guarantees for Euclidean
data sets and k-means++ clustering, which performed surprisingly well in our experiments?

References

S. Ahmadian, A. Epasto, R. Kumar, and M. Mahdian. Clustering without over-representation. In

ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2019.

A. Anagnostopoulos, L. Becchetti, M. Böhm, A. Fazzone, S. Leonardi, C. Menghini, and
C. Schwiegelshohn. Principal fairness: Removing bias via projections. arXiv:1905.13651 [cs.DS],
2019.

D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding. In Symposium on

Discrete Algorithms (SODA), 2007.

P. Awasthi and M.-F. Balcan. Center based clustering: A foundational perspective. In Handbook of

Cluster Analysis. CRC Press, 2014.

A. Backurs, P. Indyk, K. Onak, B. Schieber, A. Vakilian, and T. Wagner. Scalable fair clustering. In

International Conference on Machine Learning (ICML), 2019.

M.-F. Balcan, A. Blum, and S. Vempala. A discriminative framework for clustering via similarity

functions. In ACM Symposium on Theory of Computing (STOC), 2008.

S. Bera, D. Chakrabarty, N. Flores, and M. Negahbani. Fair algorithms for clustering. In Neural

Information Processing Systems (NeurIPS), 2019.

I. O. Bercea, M. Groß, S. Khuller, A. Kumar, C. Rösner, D. R. Schmidt, and M. Schmidt. On the cost
of essentially fair clusterings. In Approximation, Randomization, and Combinatorial Optimization.
Algorithms and Techniques (APPROX/RANDOM), 2019.

M. E. Celebi and K. Aydin. Unsupervised Learning Algorithms. Springer, 2016.

X. Chen, B. Fain, L. Lyu, and K. Munagala. Proportionally fair clustering. In International Conference

on Machine Learning (ICML), 2019.

F. Chierichetti, R. Kumar, S. Lattanzi, and S. Vassilvitskii. Fair clustering through fairlets. In Neural

Information Processing Systems (NIPS), 2017.

S. Dasgupta. Performance guarantees for hierarchical clustering. In International Conference on

Computational Learning Theory (COLT), 2002.

I. Davidson and S. S. Ravi. Making existing clusterings fairer: Algorithms, complexity results and

insights. In AAAI Conference on Artiﬁcial Intelligence, 2020.

D. Dua and C. Graff. UCI machine learning repository, 2019. German Credit data set
available on https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+
Data). Adult data set available on https://archive.ics.uci.edu/ml/datasets/adult.
Drug Consumption data set available on https://archive.ics.uci.edu/ml/datasets/
Drug+consumption+(quantified). Indian Liver Patient data set available on https://
archive.ics.uci.edu/ml/datasets/ILPD+(Indian+Liver+Patient+Dataset).

C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel. Fairness through awareness.

In

Innovations in Theoretical Computer Science Conference (ITCS), 2012.

9

M. Ester, H.-P. Kriegel, J. Sander, and X. Xu. A density-based algorithm for discovering clusters in
large spatial databases with noise. In International Conference on Knowledge Discovery and Data
Mining (KDD), 1996.

M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian. Certifying and
removing disparate impact. In ACM International Conference on Knowledge Discovery and Data
Mining (KDD), 2015.

S. Friedler, C. Scheidegger, and S. Venkatasubramanian. On the (im)possibility of fairness. arXiv:

1609.07236 [cs.CY], 2016.

M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory of NP-

Completeness. W. H. Freeman and Company, 1979.

S. Gillen, C. Jung, M. Kearns, and A. Roth. Online learning with an unknown fairness metric. In

Neural Information Processing Systems (NeurIPS), 2018.

T. F. Gonzalez. Clustering to minimize the maximum intercluster distance. Theoretical Computer

Science, 38:293–306, 1985.

G. Gottlob, G. Greco, and F. Scarcello. Pure nash equilibria: Hard and easy games. Journal of

Artiﬁcial Intelligence Research, 24:357–406, 2005.

T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning — Data Mining,

Inference, and Prediction. Springer, 2nd edition, 2009.

Ú. Hébert-Johnson, M. P. Kim, O. Reingold, and G. N. Rothblum. Calibration for the
(computationally-identiﬁable) masses. In International Conference on Machine Learning (ICML),
2018.

L. Huang, S. H.-C. Jiang, and N. K. Vishnoi. Coresets for clustering with fairness constraints. In

Neural Information Processing Systems (NeurIPS), 2019.

M. Joseph, M. Kearns, J. Morgenstern, and A. Roth. Fairness in learning: Classic and contextual

bandits. In Neural Information Processing Systems (NIPS), 2016.

M. Joseph, M. Kearns, J. Morgenstern, S. Neel, and A. Roth. Meritocratic fairness for inﬁnite and
contextual bandits. In AAAI / ACM Conference on Artiﬁcial Intelligence, Ethics, and Society, 2018.

C. Jung, S. Kannan, and N. Lutz. A center in your neighborhood: Fairness in facility location. In

Symposium on Foundations of Responsible Computing (FORC), 2020.

M. Kearns, S. Neel, and Z. S. Roth, A. Wu. Preventing fairness gerrymandering: Auditing and
learning for subgroup fairness. In International Conference on Machine Learning (ICML), 2018.

M. Kearns, S. Neel, and Z. S. Roth, A. Wu. An empirical study of rich subgroup fairness for machine
learning. In Conference on Fairness, Accountability, and Transparency (ACM FAT*), 2019a.

M. Kearns, A. Roth, and S. Shariﬁ-Malvajerdi. Average individual fairness: Algorithms, generaliza-

tion and experiments. In Neural Information Processing Systems (NeurIPS), 2019b.

M. P. Kim, A. Ghorbani, and J. Zou. Multiaccuracy: Black-box post-processing for fairness in
classiﬁcation. In AAAI / ACM Conference on Artiﬁcial Intelligence, Ethics, and Society, 2019.

M. Kleindessner, P. Awasthi, and J. Morgenstern. Fair k-center clustering for data summarization.
In International Conference on Machine Learning (ICML), 2019a. Code available on https:
//github.com/matthklein/fair_k_center_clustering.

M. Kleindessner, S. Samadi, P. Awasthi, and J. Morgenstern. Guarantees for spectral clustering with

fairness constraints. In International Conference on Machine Learning (ICML), 2019b.

S. Mahabadi and A. Vakilian. (individual) fairness for k-clustering. arXiv:2002.06742 [cs.DS], 2020.

C. Rösner and M. Schmidt. Privacy preserving clustering with constraints. In International Collo-

quium on Automata, Languages, and Programming (ICALP), 2018.

10

M. Schmidt, C. Schwiegelshohn, and C. Sohler. Fair coresets and streaming algorithms for fair

k-means clustering. arXiv:1812.10854 [cs.DS], 2018.

S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to algorithms.

Cambridge University Press, 2014.

U. von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):395–416, 2007.

U. von Luxburg, R. Williamson, and I. Guyon. Clustering: Science or art?

In Workshop on

Unsupervised and Transfer Learning, 2012.

K. Wagstaff, C. Cardie, S. Rogers, and S. Schrödl. Constrained k-means clustering with background

knowledge. In International Conference on Machine Learning (ICML), 2001.

11

Appendix

A Related Work and Concepts

Existing Notions of Individual Fairness As discussed in Section 1, the existing notions of fairness
in ML, in particular in the context of classiﬁcation, can largely be categorized into group fairness
and individual fairness. There is also a recent line of work on the notion of rich subgroup fairness
(Hébert-Johnson et al., 2018; Kearns et al., 2018, 2019a; Kim et al., 2019), which falls between these
two categories in that it requires some statistic to be similar for a large (or even inﬁnite) number of
subgroups. Here we focus on the work strictly falling into the category of individual fairness.

Dwork et al. (2012) were the ﬁrst to provide a notion of individual fairness by asking that similar
data points (as measured by a given task-speciﬁc metric) should be treated similarly by a randomized
classiﬁer. Joseph et al. (2016) and Joseph et al. (2018) study fairness in multi-armed bandit problems.
Their fairness notion aims at guaranteeing fairness on the individual level by asking that in any round,
an arm with a higher expected reward (corresponding to a better qualiﬁed applicant, for example) is
more likely to be played than an arm with a lower expected reward. Speciﬁcally in the contextual
bandit setting, Gillen et al. (2018) apply the principle of Dwork et al. by requiring that in any round,
similar contexts are picked with approximately equal probability. The recent work of Kearns et al.
(2019b) studies the scenario that every individual is subject to a multitude of classiﬁcation tasks and
introduces the notion of average individual fairness. It asks that all individuals are classiﬁed with the
same accuracy on average over all classiﬁcation tasks.

Fairness for Clustering The most established notion of fairness for clustering has been proposed
by Chierichetti et al. (2017). It is based on the fairness notion of disparate impact (Feldman et al.,
2015), which says that the output of a ML algorithm should be independent of a sensitive attribute, and
asks that each cluster has proportional representation from different demographic groups. Chierichetti
et al. provide approximation algorithms that incorporate their notion into k-center and k-median
clustering, assuming that there are only two demographic groups. Several follow-up works extend
this line of work to other clustering objectives such as k-means or spectral clustering, multiple or
non-disjoint groups, some variations of the fairness notion or to address scalability issues (Rösner and
Schmidt, 2018; Schmidt et al., 2018; Ahmadian et al., 2019; Anagnostopoulos et al., 2019; Backurs
et al., 2019; Bera et al., 2019; Bercea et al., 2019; Huang et al., 2019; Kleindessner et al., 2019b).
The recent work of Davidson and Ravi (2020) shows that for two groups, when given any clustering,
one can efﬁciently compute the fair clustering (fair according to the notion of Chierichetti et al.) that
is most similar to the given clustering using linear programming. Davidson and Ravi also show that it
is NP-hard to decide whether a data set allows for a fair clustering that additionally satisﬁes some
given must-link constraints. They mention that such must-link constraints could be used for encoding
individual level fairness constraints of the form “similar data points must go to the same cluster”.
However, for such a notion of individual fairness it remains unclear which pairs of data points exactly
should be subject to a must-link constraint.

Three alternative fairness notions for clustering are tied to centroid-based clustering such as k-means,
k-center and k-median, where one chooses k centers and then forms clusters by assigning every data
point to its closest center. (i) Motivated by the application of data summarization, Kleindessner et al.
(2019a) propose that the various demographic groups should be proportionally represented among
the chosen centers. (ii) Chen et al. (2019) propose a notion of proportionality that requires that no
sufﬁciently large subset of data points could jointly reduce their distances from their closest centers
by choosing a new center. The latter notion is similar to our notion of individual fairness in that
it assumes that an individual data point strives to be well represented (in the notion of Chen et al.
by being close to a center). Like our notion and other than the fairness notions of Chierichetti et al.
(2017) and Kleindessner et al. (2019a), it does not rely on demographic group information. However,
while our notion aims at ensuring fairness for every single data point, the notion of Chen et al. only
looks at sufﬁcient large subsets. Furthermore, since our notion deﬁnes “being well represented”
in terms of the average distance of a data point to the other points in its cluster, our notion is not
restricted to centroid-based clustering. (iii) Only recently, Jung et al. (2020) proposed a notion of
individual fairness for centroid-based clustering that comes with a guarantee for every single data
point. It asks that every data point is somewhat close to a center, where “somewhat” depends on
how close the data point is to its k nearest neighbors. Building on the work of Jung et al., Mahabadi

12

and Vakilian (2020) proposed a local search based algorithm for this fairness notion that comes with
constant factor approximation guarantees.

Average Attraction Property Balcan et al. (2008) study which properties of a similarity function
are sufﬁcient in order to approximately recover (in either a list or a tree model) an unknown ground-
truth clustering. One of the weaker properties they consider is the average attraction property, which
is closely related to our notion of individual fairness and requires inequality (1) to hold for the
ground-truth clustering with an additive gap of γ > 0 between the left and the right side of (1).
Balcan et al. show that the average attraction property is sufﬁcient to successfully cluster in the list
model, but with the length of the list being exponential in 1/γ, and is not sufﬁcient to successfully
cluster in the tree model. The conceptual difference between the work of Balcan et al. and ours is
that the former assumes a ground-truth clustering and considers the average attraction property as
a helpful property to ﬁnd this ground-truth clustering, while we consider individual fairness as a
constraint we would like to impose on whatever clustering we compute.

Game-theoretic Interpretation Fixing the number of clusters k, our notion of an individually fair
clustering can be interpreted in terms of a strategic game: let each data point correspond to a player
that can play an action in [k] in order to determine which cluster it belongs to. If, upon the cluster
choice of each player, a data point is treated fair according to Deﬁnition 1, this data point gets a
utility value of +1; otherwise it gets a utility value of 0. Then a clustering is individually fair if and
only if it is a pure (strong / Pareto) Nash equilibrium of this particular game. It is well-known for
many games that deciding whether the game has a pure Nash equilibrium is NP-hard (Gottlob et al.,
2005). However, none of the existing NP-hardness results in game theory implies NP-hardness of
individually fair clustering.

B Proof of Theorem 1

We show NP-hardness of the individually fair clustering decision problem (with k = 2 and d required
to be a metric) via a reduction from a variant of 3-SAT. It is well known that deciding whether a
Boolean formula in conjunctive normal form, where each clause comprises at most three literals, is
satisﬁable is NP-hard. NP-hardness also holds for a restricted version of 3-SAT, where each variable
occurs in at most three clauses (Garey and Johnson, 1979, page 259). Furthermore, we can require the
formula to have the same number of clauses as number of variables as the following transformation
shows: let Φ be a formula with m clauses and n variables. If n > m, we introduce l =
new
−
m is odd, we add only
variables x1, . . . , xl and for each of them add three clauses (xi) to Φ (if n
two clauses (xl)). The resulting formula has the same number of clauses as number of variables and
2 + 1
is satisﬁable if and only if Φ is satisﬁable. Similarly, if n < m, we introduce l =
new
−
2 (cid:99)
·
x6), . . . , (xl
xl)
variables x1, . . . , xl and add to Φ the clauses (x1 ∨
xl
1 ∨
2 ∨
−
−
(if m
xl)). As before, the resulting
xl) instead of (xl
formula has the same number of clauses as number of variables and is satisﬁable if and only if Φ is
satisﬁable.
So let Φ = C1 ∧
such that each clause Ci comprises at most three literals xj or
most three clauses (as either xj or
such that
sufﬁciently large). We set

Cn be a formula in conjunctive normal form over variables x1, . . . , xn
xj and each variable occurs in at
, d) in time polynomial in n
has an individually fair 2-clustering with respect to d if and only if Φ is satisﬁable (for n

xj). We construct a metric space (

n is odd, the last clause is (xl

x3), (x4 ∨
xl
2 ∨
−

x5 ∨
1 ∨
−

C2 ∧

x2 ∨

1 ∨
−

m+1
2

. . .

−

−

D

D

¬

∧

¬

3

m

(cid:98)

(cid:98)

(cid:99)

n

n

=

T rue, F alse, (cid:63),
{

∞

D

, C1, . . . , Cn, x1,

x1, . . . , xn,

¬

xn}
¬

and

d(x, y) = [d(cid:48)(x, y) + 1
{

x

= y

] + 1
{
}

x

= y

} ·

max
x,y
∈D

[d(cid:48)(x, y) + 1] ,

x, y

,

∈ D

for some symmetric function d(cid:48) :
R
∈ D
D × D →
≥
next paragraph. It is straightforward to see that d is a metric. Importantly, note that for any x
inequality (1) holds with respect to d if and only if it holds with respect to d(cid:48).

, that we specify in the
,

0 with d(cid:48)(x, x) = 0, x

∈ D

13

(cid:54)
(cid:54)
We set d(cid:48)(x, y) = 0 for all x, y

except for the following:

∈ D

d(cid:48)(T rue, F alse) = A,

d(cid:48)(T rue, (cid:63)) = B,

d(cid:48)((cid:63), F alse) = C,
d(cid:48)(Ci, F alse) = D,
d(cid:48)(Ci, (cid:63)) = E,
, T rue) = F,

d(cid:48)(

d(cid:48)(

, F alse) = G,

∞

∞

, (cid:63)) = H,

d(cid:48)(
∞
d(cid:48)(Ci,
d(cid:48)(xi,
d(cid:48)(Ci,

) = J,
∞
xi) = S,
xj) = U,
d(cid:48)(Ci, xj) = U,

¬

¬

i = 1, . . . , n,

i = 1, . . . , n,

i = 1, . . . , n,

i = 1, . . . , n,

(i, j)

(i, j)

(i, j)

(i, j)

∈ {

∈ {

1, . . . , n

1, . . . , n

∈ {

∈ {

2 : xj appears in Ci}
,
}
2 :
,
xj appears in Ci}
¬
}

where we set

A = n,

F = n2,

B = 2F = 2n2,

E =

5
2
D = J + log2 n =

F =

5
2

n2,

n2 + log n + log2 n,

n2 + log n,

5
2
n2 + 3 log n,

J = E + log n =

U = 3J =

15
2
G = H + 2n2

n

−

−
45
2

2n log2 n =

n3 +

45
2
n3 +

9
4

S = (3n + 3)U =

C =

A + G + nD
2

=

5
2

5
2
5
n3 +
2

H = nD + E =

5
2

5
2

n2

n3 +

9
2
n2 + 9n log n + 9 log n,

−

n + n log n

n2 + n log n.

n2 + n log n + n log2 n,

n log2 n,

−

(8)

We show that for n
fair 2-clustering of

≥
.
D

160 there is a satisfying assignment for Φ if and only if there is an individually

“Satisfying assignment

⇒

•

individually fair 2-clustering”

Let us assume we are given a satisfying assignment of Φ. We may assume that if xi only appears
xi, then xi is true; similarly, if xi only appears as
as xi in Φ and not as
xi, then xi is false. We
¬
into two clusters V1 and V2 as follows:
construct a clustering of

¬

D

V1 =
V2 =

T rue,
{
∞
F alse, (cid:63)
{

, C1, . . . , Cn} ∪ {
} ∪ {

xi : xi is true in sat. ass.

xi : xi is false in satisfying assignment

} ∪ {¬

} ∪ {¬

xi :

¬
xi :

,
xi is true in sat. ass.
}
.
xi is false in sat. ass.
}

¬

= 2+n. We need show that every data point in
It is
D
fair. This is equivalent to verifying that the following inequalities are true:

= 2+2n and

V1|
|

V2|

|

is treated individually

14

Points in V1:

T rue :

:

∞

Ci :

xi :

xi :

¬

Points in V2:

F alse :

(cid:63) :

xi :

xi :

¬

1
1 + 2n

1
1 + 2n

1
1 + 2n

1
1 + 2n

1
1 + 2n

(cid:88)

v
V1
∈
(cid:88)

v
V1
∈
(cid:88)

v
V1
∈
(cid:88)

v
V1
∈
(cid:88)

V1

v

∈

1
1 + n

1
1 + n

1
1 + n

1
1 + n

(cid:88)

V2

v

∈

(cid:88)

v
V2
∈
(cid:88)

v
V2
∈
(cid:88)

V2

v

∈

d(cid:48)(T rue, v) =

F

1 + 2n ≤

A + B
2 + n

=

1
2 + n

(cid:88)

d(cid:48)(T rue, v)

(9)

d(cid:48)(

∞

, v) =

F + nJ
1 + 2n ≤

G + H
2 + n

=

1
2 + n

d(cid:48)(Ci, v)

J + 2U
1 + 2n ≤

U + D + E
2 + n

≤

≤

V2

v
∈
(cid:88)

V2

v
∈
1
2 + n

d(cid:48)(

, v)

∞

(10)

d(cid:48)(Ci, v)

(11)

(cid:88)

v

V2

∈

d(cid:48)(xi, v)

2U

S

≤

1 + 2n ≤

2 + n ≤

1
2 + n

(cid:88)

d(cid:48)(xi, v)

d(cid:48)(

xi, v)

¬

2U

S

≤

1 + 2n ≤

2 + n ≤

1
2 + n

v

V2
∈
(cid:88)

V2

v

∈

d(cid:48)(

xi, v)

¬

d(cid:48)(F alse, v) =

C

1 + n ≤

A + G + nD
2 + 2n

=

1
2 + 2n

(cid:88)

V1

v

∈

d(cid:48)(F alse, v)

d(cid:48)((cid:63), v) =

C

1 + n ≤

B + H + nE
2 + 2n

=

1
2 + 2n

d(cid:48)((cid:63), v)

(cid:88)

v

V1

∈

d(cid:48)(xi, v) = 0

S

≤

2 + 2n ≤

1
2 + 2n

(cid:88)

d(cid:48)(xi, v)

d(cid:48)(

xi, v) = 0

¬

S

≤

2 + 2n ≤

1
2 + 2n

v

V1
∈
(cid:88)

v

V1

∈

d(cid:48)(

xi, v)

¬

(12)

(13)

(14)

(15)

(16)

(17)

It is straightforward to check that for our choice of A, B, C, D, E, F, G, H, J, S, U as speciﬁed in
(8) all inequalities (9) to (17) are true.

•

satisfying assignment”

⇒
C1, . . . , Cn}
{
l.

“Individually fair 2-clustering
Let us assume that there is an individually fair clustering of
into two sets of size l and n
any partitioning of
Cl and (cid:101)
sets by
Cn
We ﬁrst show that xi and
if we assume that xi,
1
V2|
|

¬
xi ∈
¬
(cid:88)

3n + 2 ≤

d(cid:48)(xi, v)

1
V1| −

U <

≤

S

V2

−

v

|

D
−

∈

d(cid:48)(xi, v)

(cid:88)

1

v

V1

∈

xi cannot be contained in the same cluster (say in V1). This is because
V1, for our choice of S and U in (8) we have

with two clusters V1 and V2. For
n) we denote the two
l (0

l

≤

≤

in contradiction to xi being treated individually fair. As a consequence we have n
V2| ≤
2n + 4.
Next, we show that due to our choice of A, B, C, D, E, F, G, H, J in (8) none of the following
cases can be true:

V1|

≤ |

|

,

1.

2.

∞} ∪ Cl ⊂

V1 and (cid:101)
T rue,
Cn
{
In this case, F alse would not be treated fair since for all 0
l)D
−
l + 1 + n

A + G + lD
l + 2 + n

C + (n
n

d(cid:48)(F alse, v) =

F alse, (cid:63)

l ∪ {
−

} ⊂

(cid:88)

<

V2 for any 0

≤
=

1
V1|
|

v

V1

∈

−

T rue
l ∪ {
{
In this case, F alse would not be treated fair since for all 0

V1 and (cid:101)
Cn

} ∪ Cl ⊂

F alse, (cid:63),

∞} ⊂

V2 for any 0

−

l

1
V1|

|

(cid:88)

V1

v

∈

d(cid:48)(F alse, v) =

A + lD
l + 1 + n

<

l < n

≤
l < n,
1
V2| −
|

n

l

≤

≤

≤
n,
1
V2| −

|

d(cid:48)(F alse, v).

(cid:88)

V2

v

∈

1

d(cid:48)(F alse, v).

(cid:88)

V2

v

∈

1

C + G + (n

−

l + 2 + n

≤
l)D

=

n

−

15

3.

4.

5.

6.

7.

8.

F alse,
V1 and (cid:101)
Cn
{
In this case, T rue would not be treated fair since for all 0

∞} ∪ Cl ⊂

T rue, (cid:63)

l ∪ {

} ⊂

−

V2 for any 0

l

≤

≤

n

1
V1|

|

(cid:88)

V1

v

∈

d(cid:48)(T rue, v) =

A + F
l + 2 + n

<

B

l + 1 + n

n

−

≤
=

l

|

≤

n,
1
V2| −

(cid:88)

V2

v

∈

1

d(cid:48)(T rue, v).

F alse
l ∪ {
{
−
In this case, T rue would not be treated fair since for all 0

V1 and (cid:101)
Cn

} ∪ Cl ⊂

T rue, (cid:63),

∞} ⊂

V2 for any 0

l

≤

≤

n

1
V1|

|

(cid:88)

V1

v

∈

d(cid:48)(T rue, v) =

A
l + 1 + n

<

n

B + F
l + 2 + n

−

≤
=

l

|

≤

n,
1
V2| −

(cid:88)

V2

v

∈

1

d(cid:48)(T rue, v).

∞} ∪ Cl ⊂

(cid:63),
l ∪ {
{
−
In this case, (cid:63) would not be treated fair since for all 0

V1 and (cid:101)
Cn

F alse, T rue

} ⊂

V2 for any 0

l

≤

≤

n

1
V2|

|

(cid:88)

V2

v

∈

d(cid:48)((cid:63), v) =

B + C + (n

l)E

−

l + 2 + n

<

n

−

l

≤
≤
H + lE
l + 1 + n

n,

=

1
V1| −
|

1

(cid:88)

V1

v

∈

d(cid:48)((cid:63), v).

(cid:63)
} ∪ Cl ⊂
{
In this case,
1
V1|

v

|

∞
(cid:88)

V1

∈

V1 and (cid:101)
Cn

l ∪ {

−

F alse, T rue,

∞} ⊂

V2 for any 0

would not be treated fair since for all 0
F + G + (n

l
≤
l)J

≤

d(cid:48)(

, v) =

∞

H + lJ
l + 1 + n

<

−

l + 2 + n

=

n

l

≤

≤
n,

n

−

1
V2| −
|

1

(cid:88)

v

V2

∈

d(cid:48)(

∞

, v).

V1 and (cid:101)
Cn

Cl ⊆
In this case, T rue would not be treated fair since for all 0

l ∪ {
−

∞} ⊆

T rue, F alse, (cid:63),

V2 for any 0

1
V1|

|

(cid:88)

V1

v

∈

d(cid:48)(T rue, v) = 0 <

A + B + F

3 + (n

l) + n

−

=

|

l

≤

≤

≤

l
≤
1
V2| −

n

n,

(cid:88)

V2

v

∈

1

d(cid:48)(T rue, v).

l ∪ {
{∞} ∪ Cl ⊆
−
In this case, T rue would not be treated fair since for all 0

} ⊆

V1 and (cid:101)
Cn

T rue, F alse, (cid:63)

V2 for any 0

1
V1|
|

(cid:88)

V1

v

∈

d(cid:48)(T rue, v) =

F
1 + l + n

<

A + B

2 + (n

−

l) + n

l

≤
=

l

≤

≤

n

≤

n,
1
V2| −

|

d(cid:48)(T rue, v).

(cid:88)

V2

v

∈

1

∞

, C1, . . . , Cn ∈

Of course, in all these cases we can exchange the role of V1 and V2. Hence, T rue,
, C1, . . . , Cn
must be contained in one cluster and (cid:63), F alse must be contained in the other cluster. W.l.o.g., let us
= n + 2.
assume T rue,
V1|
|
lj(cid:48)(cid:48) ), with
lj(cid:48)) or Ci = (lj ∨
Finally, we show that for the clause Ci = (lj) or Ci = (lj ∨
lj(cid:48),
lj,
lj or Ci,
xj, it cannot be the case that Ci,
the literal lj equaling xj or
¬
¬
¬
lj(cid:48)(cid:48) are all contained in V1. This is because otherwise
U + J
2n + 1 ≤

∞
= 2n + 2 and
V2|
|
lj(cid:48)
∨
lj(cid:48) or Ci,

V1 and (cid:63), F alse

D + E
n + 2

V2 and hence

d(cid:48)(Ci, v) =

d(cid:48)(Ci, v)

(18)

(cid:88)

(cid:88)

lj,

<

∈

¬

¬

¬

¬

1

1
V1| −
|

v

V1

∈

1
V2|
|

v

V2

∈

for our choice of D, E, J, U in (8) and Ci would not be treated fair. Consequently, since xj and

xj are not in the same cluster, for each clause Ci at least one of its literals must be in V1.

¬
Hence, if we set every literal xi or
literal xi or
¬
makes Φ true.

xi that is contained in V1 to a true logical value and every
xi that is contained in V2 to a false logical value, we obtain a valid assignment that
(cid:3)

¬

16

C Proof of Lemma 1 and Theorem 2

. . .
x2 ≤
We assume that
xi −
d(xi, xj) between two points xi and xj in its usual way
|

x1, . . . , xn} ⊆
{

R with x1 ≤

=

D

xn and write the Euclidean metric
. We ﬁrst prove Lemma 1.

≤
xj|

Proof of Lemma 1:

is fair, then all points xil and xil+1, l

C

If
xil and xil+1, l
˜x
Cl =
treated fair, we have

[k
−
xil−1+1, . . . , xil }
{

∈

∈

[k

−
1], are treated fair. We need to show that all points in
2, . . . , k

1], are treated fair. Conversely, let us assume that
are treated fair. Let
. Since xil is
}

D
l + 1, . . . , k

for some l

and l(cid:48)

1
}

∈ {

∈ {

−

∈

1
Cl| −
|
and hence

1

(cid:88)

Cl

y

∈

(xil −

y) =

1
Cl| −
|

1

(cid:88)

y

Cl

∈

xil −
|

y

| ≤

1
Cl(cid:48)

|

(cid:88)

y

|

∈

Cl(cid:48)

xil −

|

y

|

=

1
Cl(cid:48)
|

(cid:88)

y

|

Cl(cid:48)

∈

(y

−

xil )

1
Cl| −
|

1

(cid:88)

y

Cl

∈

y

˜x
|

−

| ≤

1
Cl| −
|
= (xil −

(
|

˜x

xil |

−

+

xil −
|

y

)
|

(cid:88)

Cl

∈

˜x

\{

}

(xil −

y)

(y

−

xil )

(cid:88)

1

y

Cl

∈

˜x) +

˜x) +

\{

˜x
}
1
Cl| −
|
1
Cl(cid:48)
|

y

|

1

y
(cid:88)

Cl(cid:48)

∈

(y

˜x)

−

˜x
|

y

.

|

−

≤

=

=

(xil −
1
Cl(cid:48)
|

|

1
Cl(cid:48)
|

|

1

}

−

(cid:88)

y
Cl(cid:48)
∈
(cid:88)

Cl(cid:48)

y

∈
that

y

˜x
|

−

| ≤

Similarly, we can show for l(cid:48)

1, . . . , l

∈ {
1
Cl| −
|

(cid:88)

1

y

Cl

1
Cl(cid:48)
|

(cid:88)

y

|

Cl(cid:48)

˜x

|

y

,
|

−

∈
and hence ˜x is treated fair. Similarly, we can show that all points x1, . . . , xi1
are treated fair.

∈

1 and xik−1+2, . . . , xn

−

1, the average distance of xil to the points in Cs
For the second claim observe that for 1
−
k, the
cannot be smaller than the average distance to the points in Cl \ {
average distance of xil to the points in Cs cannot be smaller than the average distance to the points in
(cid:3)
Cl+1. A similar argument proves the claim for xil+1.

and for l + 2

xil }

≤

≤

≤

≤

s

s

l

C

D

= (

) is an individually fair k-clustering of

For k = 1,
D
orem 2 is vacuously true. In order to prove Theorem 2 for k
compute an individually fair k-clustering of
tains an array T of k
of the ﬁrst k
(
x2}
,
x1}
{
{
clustering has been found. We formally state our algorithm as Algorithm 1 below.

with contiguous clusters, and The-
2, we present an algorithm to
with contiguous clusters. Our algorithm main-
1 strictly increasing boundary indices that specify the right-most points
1), corresponding to the clustering
), it keeps incrementing the entries of T until a fair

1 clusters. Starting from T = (1, 2, . . . , k
xk, xk+1, . . . , xn}
{

−
, . . . ,

xk
{

1}

≥

−

−

D

−

,

In order to prove Theorem 2, we need to show that Algorithm 1 always terminates and outputs an
increasingly sorted array T = (T [1], . . . , T [k
1] < n that
deﬁnes an individually fair clustering (obviously, the output T deﬁnes a k-clustering with contiguous
clusters). For doing so, we show several claims to be true.

T [1] < T [2] < . . . < T [k

1]) with 1

≤

−

−

Claim 1: Throughout the execution of Algorithm 1 we have T [j] < T [j + 1] for all j
This is true at the beginning of the execution. Assume it is true before an update of T happens. If
2] is updated, we have
T [k

1] is updated, it is still true after the update. If T [j0] for some j0 ∈

2].

[k

[k

−

−

−

∈

17

AvgDistN ot(xT [j0]+1, C T

0
≤
C T
xT [j0]+1}
j0+1 (cid:41)
{
T [j0] the claim is true.

j0 ) < AvgDistIn(xT [j0]+1, C T

j0+1) before the update. But then it is
and T [j0 + 1] > T [j0] + 1 before the update. Hence, also after the update of

Claim 2: Throughout the execution of Algorithm 1 we have T [k
Assume that T [k
1 would be updated to T [k
update, C T
k =
AvgDistIn(xn,

AvgDistN ot(xn, C T
k
−

1] = n
and 0

) = 0.

−
≤

n

1.

≤
1) < AvgDistIn(xn, C T

1]
1] = n. But then, before the
k ). However,

−
−

−

−
xn}
{
xn}
{

From Claim 1 and Claim 2 it follows that Algorithm 1 terminates after at most (cid:0)n

(cid:1) updates.

1
1

k

−
−

∈

−

[k

1], after any update T [j] = T [j] + 1 until the next update of T [j], the point

Claim 3: For j
xT [j] (referring to the value of T [j] after the update) is treated individually fair.
Since xT [j]+1 (referring to the value of T [j] before the update; after the update this point becomes
xT [j]) is the left-most point in its cluster, the closest cluster for xT [j]+1 is either its own cluster or the
cluster left of its own cluster. If T [j] is updated to T [j] + 1, this just means that xT [j]+1 is closer to
the left cluster and is now assigned to this cluster. So immediately after the update, xT [j] (referring
to the value of T [j] after the update) is treated individually fair. As long as T [j] is not updated for
j and cannot be closer to any cluster C T
another time, xT [j] is the right-most point in its cluster C T
l ,
l
1], is updated. If T [l] for
l
l ) can get only larger, so that xT [j]
is still treated individually fair.

1], than to its own cluster, no matter how often T [l], l
gets updated, then AvgDistN ot(xT [j], C T

[j
j + 1, . . . , k

∈
∈ {

1
}

−

−

−

[j

∈

are treated individually fair.

Claim 4: After the last update of T in the execution of Algorithm 1 all points xT [j]+1, j
1
}
After the last update, Algorithm 1 checks for every point xT [j]+1, j
1], whether it is closer to
its own cluster or the cluster on its left side and conﬁrms that it is closer to its own cluster. Since
xT [j]+1 is the left-most point in its cluster, this implies that xT [j]+1 is treated individually fair.

1, . . . , k

∈ {

[k

−

−

∈

From Claim 3, Claim 4 and Lemma 1 it follows that the output of Algorithm 1 is an individually fair
clustering.

18

Algorithm 1 Algorithm for ﬁnding an individually fair clustering in the 1-dim Euclidean case
1: Input: increasingly sorted array (x1, . . . , xn) of n distinct points in R; number of clusters k ∈ {2, . . . , |D|}
2: Output: increasingly sorted array T = (T [1], . . . , T [k − 1]) of k − 1 distinct boundary indices T [i] ∈
{1, . . . , n − 1} deﬁning k clusters as follows: C1 = {x1, . . . , xT [1]}, C2 = {xT [1]+1, . . . , xT [2]}, . . . ,
Ck = {xT [k−1]+1, . . . , xn}

3: # Conventions:

• for an array of boundary indices T as in Line 2, (C T

1 , . . . , C T

k ) denotes the clustering with clusters C T
i

deﬁned as in Line 2

• for a cluster C T

i and a point y /∈ C T

i , we write

AvgDistN ot(y, C T

i ) =

1
|C T
i |

(cid:88)

z∈CT
i

|y − z|

• for a cluster C T

i and a point y ∈ C T

i , we write (using the convention that 0

0 = 0)

AvgDistIn(y, C T

i ) =

1
i | − 1

|C T

(cid:88)

z∈CT
i

|y − z|

4: Initialize T = (1, 2, . . . , k − 1)
5: Set DistLef t = AvgDistN ot(xT [k−1]+1, C T

IsF airOuter = 1{DistOwn ≤ DistLef t}

k−1), DistOwn = AvgDistIn(xT [k−1]+1, C T

k ) and

6: while IsF airOuter == F alse do
Update T [k − 1] = T [k − 1] + 1
7:
Set SomethingChanged = T rue
8:
while SomethingChanged == T rue do
Set SomethingChanged = F alse
for j = k − 2 to j = 1 by −1 do

9:
10:

11:
12:

13:
14:
15:
16:

17:

18:

19:

20:

Set DistLef t = AvgDistN ot(xT [j]+1, C T

IsF airInner = 1{DistOwn ≤ DistLef t}

j ), DistOwn = AvgDistIn(xT [j]+1, C T

j+1) and

while IsF airInner == F alse do

Update T [j] = T [j] + 1
Set SomethingChanged = T rue
Set DistLef t = AvgDistN ot(xT [j]+1, C T

IsF airInner = 1{DistOwn ≤ DistLef t}

j ), DistOwn = AvgDistIn(xT [j]+1, C T

j+1) and

end while

end for

end while
Set DistLef t = AvgDistN ot(xT [k−1]+1, C T
IsF airOuter = 1{DistOwn ≤ DistLef t}

k−1), DistOwn = AvgDistIn(xT [k−1]+1, C T

k ) and

21: end while
22: return T

19

D Explanation of the Recurrence Relation (5) and Modiﬁcations of the
Dynamic Programming Approach of Section 4 to the Case p =

∞
(x1, . . . , xl
(cid:107)

|

j

p +

T (i, j, l) =

p
(x1, . . . , xl)
p =
(cid:107)
= j, we have
t1, . . . ,

p and for every clustering (C1, . . . , Cl)

Let us ﬁrst explain the recurrence relation (5): because of
(cid:107)
Cl|
xl|
∈ Hi,j,l it is
|
|
C1| −
(
min
|
i,j,l (cid:107)
(C1,...,Cl)

tl|
−
It follows from Lemma 1 that a clustering (C1, . . . , Cl) of
Cl =
j+1, . . . , xi}
j to the points in Cl
the average distance of xi
the points in Cl and the average distance of xi
the average distance to the points in Cl
in (5) (when
1|
utilize the ﬁrst condition and rather than minimizing over
2)] and
s

p
p.
(cid:107)
|
x1, . . . , xi}
with contiguous clusters and
{
x1, . . . , xi
1) is a fair clustering of
and
−
is not greater than the average distance to
j}
1 \ {
−
j+1 to the points in Cl \ {
is not greater than
j+1}
1. The latter two conditions correspond to the two inequalities
= s, where s is a variable). By explicitly enforcing these two constraints, we can
Hi,j,l in (19), we can minimize over both
1)-clusterings of

is fair if and only if (C1, . . . , Cl
xi

1 (corresponding to minimizing over all fair (l

p
p +
1)
(cid:107)

−

xi
{

1| −

j}
−

(19)

Cl

Cl

j,s,l

xi

1)

∈H

tl

[i

{

−

−

−

−

−

−

−

−

−

j

−

−

−
with non-empty contiguous clusters). It is
p
p =
1)
(cid:107)

C1| −

t1, . . . ,

1| −

(
|

Cl

tl

(cid:107)

−

−

|

−

min
(l
j
−
−

[i

s

∈

2)]

−

T (i

j, s, l

1),

−

−

−
∈
−
x1, . . . , xi
{
−

|
(l
j}
min
s
2)]
(l
j
−
−
−
∈
(C1,...,Cl−1)
i−j,s,l−1
∈H

[i

Hi

and hence we end up with the recurrence relation (5).

Now we describe how to modify the dynamic programming approach of Section 4 to the case p =
in this case, we replace the deﬁnition of the table T in (3) by
tl)
Cl| −
|

C1| −
|
as before. The optimal value of (2) is now given by

T (i, j, l) =

t1, . . . ,

[n], j

[n], l

:

∞

[k],

(cid:107)∞

∈H

∈

∈

∈

i

,

and T (i, j, l) =
minj

min
(C1,...,Cl)

(
i,j,l (cid:107)
Hi,j,l =
[n] T (n, j, k). Instead of (4), we have, for i, j
,
t1|

T (i, j, 1) =

∞

(cid:26)

if

∅

,

∈

T (i, j, i) =

[n],

∈

(cid:26)maxs=1,...,i |

,

∞

1

,

ts|

−

j = 1,
= 1
j

j = i,
= i
j

i
|
∞

−
,

and

and the recurrence relation (5) now becomes, for l > 1 and j + l

T (i, j, l) =

,

j + l

1 > i,

∞

−

(cid:40)

(cid:26)

T (i, j, l) = max

j

|

−

, min

tl|

T (i

−

j, s, l

−

1) : s

[i

j

−

∈

−

−

i,

2)],

1

(l

≤

−

1

−

1

−

s

j

1

1

s
1
(cid:88)
−

f =1

j
(cid:88)

f =2

xi
|

j −
−

xi

−

j

f | ≤
−

1
j

j
(cid:88)

f =1

xi
|

j −

−

xi

,
j+f |
−

xi
|

j+1 −
−

xi

j+f | ≤
−

1
s

s
1
(cid:88)
−

f =0

(cid:27)(cid:41)

xi

−

|

j+1 −

xi

−

j

f |
−

.

(n3k). Computing a solution (C ∗1 , . . . , C ∗k ) to
Just like before, we can build the table T in time
(2) also works similarly as before. The only thing that we have to change is the condition (i) on h0
(when setting

O
1, . . . , 2): now h0 must satisfy

C ∗l |
|

= h0 for l = k
(cid:32)

(cid:40)

max

T

n

or equivalently

−
k
(cid:88)

−

r=l+1

(cid:33)

(cid:41)

, h0, l

C ∗r |
|

, max
r=l+1,...,k ||

C ∗r | −

tr|

= v∗

(cid:32)

T

n

k
(cid:88)

−

r=l+1

(cid:33)

, h0, l

C ∗r |
|

v∗.

≤

20

(cid:54)
(cid:54)
Figure 5: Histograms of the data sets used in the experiments of Section 5.1. Left: The credit amount
(one of the 20 features in the German credit data set; normalized to be in [0, 1]) for the 1000 records
in the German credit data set. Note that there are only 921 unique values. Right: The estimated
probability of having a good credit risk for the second 500 records in the German credit data set. The
estimates are obtained from a multi-layer perceptron trained on the ﬁrst 500 records in the German
credit data set.

Table 2: Experiment on German credit data set. Clustering 1000 people according to their credit
amount. Target cluster sizes ti = 1000
[k]. NAIVE=naive clustering that matches the target
cluster sizes, DP=dynamic programming approach of Section 4, k-MEANS= k-means initialized
with medians of the clusters of the naive clustering, k-ME++= k-means++. Results for k-ME++
averaged over 100 runs. Best values in bold.

k , i

∈

# UNF MVI

OBJ

COSQ

CO

# UNF MVI

OBJ

COSQ

CO

NAIVE
DP
k-MEANS
k-ME++

113
0
4
2.51

2.16
1.0
1.01
1.01

k = 10

0
131
136
159.9

2.18
0.37
0.37
0.34

13.88
9.29
8.91
9.59

92
0
5
6.73

3.17
1.0
1.01
1.05

k = 20

0
37
37
98.4

0.8
0.15
0.28
0.08

7.32
4.87
5.74
4.78

E Addendum to Section 5.1

Figure 5 shows the histograms of the two 1-dimensional data sets that we used in the experiments of
Section 5.1.

Table 2 shows the results for the ﬁrst experiment of Section 5.1 when k = 10 or k = 20.

Table 3 and Table 4 provide the results for the second experiment of Section 5.1. In Table 3, we
consider uniform target cluster sizes ti = 500
[k], while in Table 4 we consider various non-
∈
uniform target cluster sizes. The interpretation of the results is similar as for the ﬁrst experiment
of Section 5.1. Most notably, k-MEANS can be quite unfair with up to 33 data points being treated
unfair when k is large, whereas k-ME++ produces very fair clusterings with not more than three data
points being treated unfair. However, k-ME++ performs very poorly in terms of Obj, which can be
almost ten times as large as for k-MEANS and our dynamic programming approach DP (cf. Table 3,
k = 50).

k , i

The MLP that we used for predicting the label (good vs. bad credit risk) in the second experiment of
Section 5.1 has three hidden layers of size 100, 50 and 20, respectively, and a test accuracy of 0.724.

21

Table 3: Experiment on German credit data set. Clustering the second 500 people according to
their estimated probability of having a good credit risk. Target cluster sizes ti = 500
[k].
NAIVE=naive clustering that matches the target cluster sizes, DP=dynamic programming approach
of Section 4, k-MEANS= k-means initialized with medians of the clusters of the naive clustering,
k-ME++= k-means++. Results for k-ME++ averaged over 100 runs. Best values in bold.

k , i

∈

TARGET CLUSTER SIZES

# UNF MVI

OBJ

COSQ

CO

k = 5

t1 = . . . = t5 = 100

k = 10

t1 = . . . = t10 = 50

k = 20

t1 = . . . = t20 = 25

k = 50

t1 = . . . = t50 = 10

NAIVE
DP
k-MEANS
k-ME++

NAIVE
DP
k-MEANS
k-ME++

NAIVE
DP
k-MEANS
k-ME++

NAIVE
DP
k-MEANS
k-ME++

197
0
1
0.71

162
0
5
0.98

116
0
33
2.62

73
0
28
3.07

58.28
0.99
1.02
1.01

10.27
0.98
1.06
1.01

9.64
1.0
2.13
1.06

3.8
1.0
2.39
1.24

0
214
212
220.66

0
217
207
248.66

0
155
95
239.64

0
24
13
234.17

6.8
0.64
0.64
0.63

1.82
0.19
0.37
0.12

0.43
0.17
0.1
0.03

0.06
0.04
0.04
0.0

16.91
6.9
6.85
7.0

8.35
3.36
4.3
3.13

4.06
2.96
2.16
1.34

1.54
1.32
1.28
0.41

Table 4: Experiment on German credit data set. Clustering the second 500 people according to
their estimated probability of having a good credit risk. Various non-uniform target cluster sizes.
NAIVE=naive clustering that matches the target cluster sizes, DP=dynamic programming approach
of Section 4, k-MEANS= k-means initialized with medians of the clusters of the naive clustering,
k-ME++= k-means++. Results for k-ME++ averaged over 100 runs. Best values in bold.

TARGET CLUSTER SIZES

# UNF

MVI

OBJ

COSQ

k = 12

ti =

(cid:40)

50 for 3 ≤ i ≤ 10
25 else

k = 12

t1 = t12 = 10,
t2 = t11 = 15,
t3 = t10 = 25,
t4 = t9 = 50,
t5 = t8 = 50,
t6 = t7 = 100

k = 20

ti =

(cid:40)

10 for i = 1, 3, 5, . . .
40 for i = 2, 4, 6, . . .

k = 20

ti =

(cid:40)

115 for i = 10, 11
15

else

NAIVE
DP
k-MEANS
k-ME++

NAIVE
DP
k-MEANS
k-ME++

NAIVE
DP
k-MEANS
k-ME++

NAIVE
DP
k-MEANS
k-ME++

188
0
3
1.25

251
0
5
1.22

189
0
30
2.37

224
0
25
2.71

12.85
0.97
1.05
1.03

65.99
0.97
1.16
1.03

137.31
1.0
1.91
1.07

215.88
1.0
2.04
1.07

0
232
217
255.1

0
247
247
270.5

0
140
91
225.17

0
165
156
249.9

1.82
0.17
0.18
0.08

2.2
0.17
0.14
0.08

0.97
0.17
0.09
0.03

1.96
0.17
0.09
0.03

22

CO

8.34
3.06
3.18
2.36

10.28
3.06
2.64
2.37

5.7
2.96
2.13
1.35

9.11
2.96
1.92
1.34

F Addendum to Section 5.2

In Appendix F.1, we present a simple example that shows that it really depends on the data set whether
a group-fair clustering is individually fair or not.

In Appendix F.2, we provide an example illustrating why the local search idea outlined in Section 5.2
does not work.

In Appendix F.3, we provide the pseudocode of our proposed heuristic to greedily prune a hierarchical
clustering with the goal of minimizing # Unf or MVi.

In Appendix F.4, we present the missing plots of Section 5.2 for the Adult data set: Figure 7 is
analogous to Figure 3, but for the Manhattan and Chebyshev metric, and shows # Unf, MVi and
Co as a function of the number of clusters k for the various standard clustering algorithms. The
results are very similar to the case of d equaling the Euclidean metric (shown in Figure 3), and their
interpretation is the same. Figure 8 is analogous to Figure 4, but with single and complete linkage
clustering instead of average linkage clustering. Just as for average linkage clustering (shown in
Figure 4), we see that our heuristic approach can lead to a signiﬁcant improvement in # Unf (for
complete linkage clustering, this is only true for k
20, however) and also to some improvement in
MVi, but comes at the price of an increase in the clustering cost Co. In Figures 9 and 10 we study
average / single / complete linkage clustering when d equals the Manhattan or Chebyshev metric and
make similar observations.

≤

In Appendix F.5, we show the same set of experiments as in Figures 3 to 4 and Figures 7 to 10,
respectively, on the Drug Consumption data set. We used all 1885 records in the data set, and we used
all 12 features describing a record (e.g., age, gender, or education), but did not use the information
about the drug consumption of a record (this information is usually used as label when setting up a
classiﬁcation problem on the data set). We normalized the features to zero mean and unit variance.
When running the standard clustering algorithms on the data set, we refrained from running spectral
clustering since the Scikit-learn implementation occasionally was not able to do the eigenvector
computations and aborted with a LinAlgError. Other than that, all results are largely consistent with
the results for the Adult data set.

In Appendix F.6, we show the same set of experiments on the Indian Liver Patient data set. Removing
four records with missing values, we ended up with 579 records, for which we used all 11 available
features (e.g., age, gender, or total proteins). We normalized the features to zero mean and unit
variance. Again, all results are largely consistent with the results for the Adult data set.

F.1 Compatibility of Group Fairness and Individual Fairness

By means of a simple example we want to illustrate that it really depends on the data set whether
group fairness and individual fairness are compatible or at odds with each other. Here we consider the
prominent group fairness notion for clustering of Chierichetti et al. (2017), which asks that in each
cluster, every demographic group is approximately equally represented. Let us assume that the data
set consists of the four 1-dimensional points 0, 1, 7 and 8 and the distance function d is the ordinary
).
7, 8
,
= (
0, 1
Euclidean metric. It is easy to see that the only individually fair 2-clustering is
}
}
{
{
Now if there are two demographic groups G1 and G2 with G1 =
1, 8
and G2 =
, the
}
{
is perfectly fair according to the notion of Chierichetti et al.. But if G1 =
0, 1
clustering
and
}
{
C
7, 8
G2 =
, the clustering
}
{

is totally unfair according to the latter notion.

0, 7
}
{

C

C

F.2 Why Local Search Does not Work

Figure 6 presents an example illustrating why the local search idea outlined in Section 5.2 does not
work: assigning a data point that is not treated fair to its closest cluster (so that that data point is
treated fair) may cause other data points that are initially treated fair to be treated unfair after the
reassignment.

F.3 Pseudocode of our Proposed Heuristic Approach

Algorithm 2 provides the pseudocode of our proposed strategy to greedily prune a hierarchical
clustering with the goal of minimizing # Unf or MVi.

23

Figure 6: An example illustrating why the local search idea outlined in Section 5.2 does not work.
Top left: 12 points in R2. Top right: A k-means clustering of the 12 points (encoded by color)
with two points that are not treated individually fair (surrounded by a circle). Bottom row: After
assigning one of the two points that are not treated fair in the k-means clustering to its closest cluster,
that point is treated fair. However, now some points are treated unfair that were initially treated fair.

Algorithm 2 Algorithm to greedily prune a hierarchical clustering
1: Input: binary tree T representing a hierarchical clustering obtained from running a linkage
that

; measure meas

2, . . . ,

∈ {

|D|}

# Unf, MVi
}

∈ {

clustering algorithm; number of clusters k
one aims to optimize for
2: Output: a k-clustering

C

3: # Conventions:
for a node v
for a j-clustering
write
|Cl(cid:44)
→
two clusters A and B in

•
•

∈

C

C

(cid:48)

−

(cid:48)

C

T , we denote the left child of v by Lef t(v) and the right child by Right(v)

(cid:48) = (C1, C2, . . . , Cj), a cluster Cl and A, B

B = Cl we
clustering that we obtain by replacing the cluster Cl with

Cl with A ˙
∪

A,B for the (j + 1)

⊆

4: Let r be the root of T and initialize the clustering
5: for i = 1 to k
Set
6:

2 by 1 do

−

as

C

C

= (Lef t(r), Right(r))

v(cid:63) =

argmin

v:v is a cluster in

with

v
|

>1
|

C

meas(

C|v(cid:44)

→

Lef t(v),Right(v))

and

7: end for
8: return

C

=

C

C|v(cid:63)(cid:44)
→

Lef t(v(cid:63)),Right(v(cid:63))

24

10110121011012101210121011012F.4 Adult Data Set

Figure 7: Adult data set — similar plots as in Figure 3, but for the Manhattan (top row) and
Chebyshev metric (bottom row): # Unf (left), MVi (middle) and Co (right) for the clusterings
produced by the various standard algorithms as a function of the number of clusters k.

Figure 8: Adult data set with Euclidean metric — similar plots as in Figure 4, but for single (top
row) and complete linkage clustering (bottom row): # Unf (left), MVi (middle) and Co (right) for
the clusterings produced by single / complete linkage clustering and the two variants of our heuristic
approach to improve it: the ﬁrst (#U in the legend) greedily chooses splits as to minimize # Unf, the
second (MV in the legend) as to minimize MVi.

25

F.4AdultDataSet7130204060801000200400600800k-means++k-meGoiGsk-Fenterk-Fenter GF6C6ingle link.Avg. link.ComSl. link.25102030405060708090100k0100200300400500600700800#8nfAdult data set (1000 points) --- 0anhattan metUic25102030405060708090100k1.01.52.02.53.03.54.04.55.05.56.009iAdult data Vet (1000 pointV) --- 0anhattan metric020406080100k7501000125015001750200022502500CoAdult data set (1000 points) --- 0anhattan metric25102030405060708090100k0100200300400500600700800#8nfAdult data set (1000 points) --- Chebyshev metUic25102030405060708090100k1.01.52.02.53.03.54.04.55.05.56.009iAdult data Vet (1000 pointV) --- ChebyVhev metric020406080100k40060080010001200CoAdult data set (1000 points) --- Chebyshev metricFigure6:Adultdataset—similarplotsasinFigure3,butfortheManhattan(toprow)andChebyshevmetric(bottomrow):#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbythevariousstandardalgorithmsasafunctionofthenumberofclustersk.25102030405060708090100k01002003004005006007008009001000#8nfAdult data Vet (1000 SointV) --- (uclidean metUic6ingle link.6ingle #86ingle 0925102030405060708090100k1.01.52.02.53.03.54.04.509iAdult data Vet (1000 pointV) --- (uclidean metric25102030405060708090100k11001200130014001500CoAdult data set (1000 points) --- (uclidean metric25102030405060708090100k0100200300400500600#8nfAdult data Vet (1000 pointV) --- (uclidean metUicCompl. link.Compl. #8Compl. 0925102030405060708090100k1.01.52.02.53.03.54.04.509iAdult data Vet (1000 pointV) --- (uclidean metric25102030405060708090100k400600800100012001400CoAdult data set (1000 points) --- (uclidean metricFigure7:AdultdatasetwithEuclideanmetric—similarplotsasinFigure4,butforsingle(toprow)andcompletelinkageclustering(bottomrow):#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbysingle/completelinkageclusteringandthetwovariantsofourheuristicapproachtoimproveit:theﬁrst(#Uinthelegend)greedilychoosessplitsastominimize#Unf,thesecond(MVinthelegend)astominimizeMVi.24F.4AdultDataSet7130204060801000200400600800k-means++k-meGoiGsk-Fenterk-Fenter GF6C6ingle link.Avg. link.ComSl. link.25102030405060708090100k0100200300400500600700800#8nfAdult data set (1000 points) --- 0anhattan metUic25102030405060708090100k1.01.52.02.53.03.54.04.55.05.56.009iAdult data Vet (1000 pointV) --- 0anhattan metric020406080100k7501000125015001750200022502500CoAdult data set (1000 points) --- 0anhattan metric25102030405060708090100k0100200300400500600700800#8nfAdult data set (1000 points) --- Chebyshev metUic25102030405060708090100k1.01.52.02.53.03.54.04.55.05.56.009iAdult data Vet (1000 pointV) --- ChebyVhev metric020406080100k40060080010001200CoAdult data set (1000 points) --- Chebyshev metricFigure6:Adultdataset—similarplotsasinFigure3,butfortheManhattan(toprow)andChebyshevmetric(bottomrow):#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbythevariousstandardalgorithmsasafunctionofthenumberofclustersk.25102030405060708090100k01002003004005006007008009001000#8nfAdult data Vet (1000 SointV) --- (uclidean metUic6ingle link.6ingle #86ingle 0925102030405060708090100k1.01.52.02.53.03.54.04.509iAdult data Vet (1000 pointV) --- (uclidean metric25102030405060708090100k11001200130014001500CoAdult data set (1000 points) --- (uclidean metric25102030405060708090100k0100200300400500600#8nfAdult data Vet (1000 pointV) --- (uclidean metUicCompl. link.Compl. #8Compl. 0925102030405060708090100k1.01.52.02.53.03.54.04.509iAdult data Vet (1000 pointV) --- (uclidean metric25102030405060708090100k400600800100012001400CoAdult data set (1000 points) --- (uclidean metricFigure7:AdultdatasetwithEuclideanmetric—similarplotsasinFigure4,butforsingle(toprow)andcompletelinkageclustering(bottomrow):#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbysingle/completelinkageclusteringandthetwovariantsofourheuristicapproachtoimproveit:theﬁrst(#Uinthelegend)greedilychoosessplitsastominimize#Unf,thesecond(MVinthelegend)astominimizeMVi.24Figure 9: Adult data set with Manhattan metric (similar plots as in Figures 4 and 8, respectively):
# Unf (left), MVi (middle) and Co (right) for the clusterings produced by average (top row) / single
(middle row) / complete linkage clustering (bottom row) and the two variants of our heuristic
approach to improve it.

Figure 10: Adult data set with Chebyshev metric (similar plots as in Figures 4 and 8, respectively):
# Unf (left), MVi (middle) and Co (right) for the clusterings produced by average (top row) / single
(middle row) / complete linkage clustering (bottom row) and the two variants of our heuristic
approach to improve it.

26

25102030405060708090100k0100200300#8nfAdult data Vet (1000 pointV) --- 0anhattan metUicAvg. link.Avg. #8Avg. 0925102030405060708090100k1.01.52.02.509iAdult data Vet (1000 pointV) --- 0anhattan metric25102030405060708090100k1000125015001750200022502500CoAdult data set (1000 points) --- 0anhattan metric25102030405060708090100k0100200300400500600700800#8nfAdult data Vet (1000 SointV) --- 0anhattan metUic6ingle link.6ingle #86ingle 0925102030405060708090100k1.01.52.02.53.03.54.04.509iAdult data Vet (1000 pointV) --- 0anhattan metric25102030405060708090100k18001900200021002200230024002500CoAdult data set (1000 points) --- 0anhattan metric25102030405060708090100k0100200300400500600#8nfAdult data Vet (1000 pointV) --- 0anhattan metUicCompl. link.Compl. #8Compl. 0925102030405060708090100k1.01.52.02.53.009iAdult data Vet (1000 pointV) --- 0anhattan metric25102030405060708090100k7501000125015001750200022502500CoAdult data set (1000 points) --- 0anhattan metricFigure8:AdultdatasetwithManhattanmetric(similarplotsasinFigures4and7,respectively):#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbyaverage(toprow)/single(middlerow)/completelinkageclustering(bottomrow)andthetwovariantsofourheuristicapproachtoimproveit.25102030405060708090100k0100200300400#8nfAdult data Vet (1000 pointV) --- ChebyVhev metUicAvg. link.Avg. #8Avg. 0925102030405060708090100k1.01.52.02.53.03.509iAdult data Vet (1000 pointV) --- ChebyVhev metric25102030405060708090100k400500600700800900100011001200CoAdult data set (1000 points) --- Chebyshev metric25102030405060708090100k0100200300400500600700800#8nfAdult data Vet (1000 SointV) --- ChebyVhev metUic6ingle link.6ingle #86ingle 0925102030405060708090100k1.01.52.02.53.03.54.04.509iAdult data Vet (1000 pointV) --- ChebyVhev metric25102030405060708090100k85090095010001050110011501200CoAdult data set (1000 points) --- Chebyshev metric25102030405060708090100k0100200300400500600700#8nfAdult data Vet (1000 pointV) --- ChebyVhev metUicCompl. link.Compl. #8Compl. 0925102030405060708090100k1.01.52.02.53.03.509iAdult data Vet (1000 pointV) --- ChebyVhev metric25102030405060708090100k40060080010001200CoAdult data set (1000 points) --- Chebyshev metricFigure9:AdultdatasetwithChebyshevmetric(similarplotsasinFigures4and7,respectively):#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbyaverage(toprow)/single(middlerow)/completelinkageclustering(bottomrow)andthetwovariantsofourheuristicapproachtoimproveit.2525102030405060708090100k0100200300#8nfAdult data Vet (1000 pointV) --- 0anhattan metUicAvg. link.Avg. #8Avg. 0925102030405060708090100k1.01.52.02.509iAdult data Vet (1000 pointV) --- 0anhattan metric25102030405060708090100k1000125015001750200022502500CoAdult data set (1000 points) --- 0anhattan metric25102030405060708090100k0100200300400500600700800#8nfAdult data Vet (1000 SointV) --- 0anhattan metUic6ingle link.6ingle #86ingle 0925102030405060708090100k1.01.52.02.53.03.54.04.509iAdult data Vet (1000 pointV) --- 0anhattan metric25102030405060708090100k18001900200021002200230024002500CoAdult data set (1000 points) --- 0anhattan metric25102030405060708090100k0100200300400500600#8nfAdult data Vet (1000 pointV) --- 0anhattan metUicCompl. link.Compl. #8Compl. 0925102030405060708090100k1.01.52.02.53.009iAdult data Vet (1000 pointV) --- 0anhattan metric25102030405060708090100k7501000125015001750200022502500CoAdult data set (1000 points) --- 0anhattan metricFigure8:AdultdatasetwithManhattanmetric(similarplotsasinFigures4and7,respectively):#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbyaverage(toprow)/single(middlerow)/completelinkageclustering(bottomrow)andthetwovariantsofourheuristicapproachtoimproveit.25102030405060708090100k0100200300400#8nfAdult data Vet (1000 pointV) --- ChebyVhev metUicAvg. link.Avg. #8Avg. 0925102030405060708090100k1.01.52.02.53.03.509iAdult data Vet (1000 pointV) --- ChebyVhev metric25102030405060708090100k400500600700800900100011001200CoAdult data set (1000 points) --- Chebyshev metric25102030405060708090100k0100200300400500600700800#8nfAdult data Vet (1000 SointV) --- ChebyVhev metUic6ingle link.6ingle #86ingle 0925102030405060708090100k1.01.52.02.53.03.54.04.509iAdult data Vet (1000 pointV) --- ChebyVhev metric25102030405060708090100k85090095010001050110011501200CoAdult data set (1000 points) --- Chebyshev metric25102030405060708090100k0100200300400500600700#8nfAdult data Vet (1000 pointV) --- ChebyVhev metUicCompl. link.Compl. #8Compl. 0925102030405060708090100k1.01.52.02.53.03.509iAdult data Vet (1000 pointV) --- ChebyVhev metric25102030405060708090100k40060080010001200CoAdult data set (1000 points) --- Chebyshev metricFigure9:AdultdatasetwithChebyshevmetric(similarplotsasinFigures4and7,respectively):#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbyaverage(toprow)/single(middlerow)/completelinkageclustering(bottomrow)andthetwovariantsofourheuristicapproachtoimproveit.25F.5 Drug Consumption Data Set

Figure 11: Drug Consumption data set: # Unf (left), MVi (middle) and Co (right) for the clusterings
produced by the various standard algorithms as a function of k for the Euclidean (top row), Manhattan
(middle row) and Chebyshev metric (bottom row).

Figure 12: Drug Consumption data set with Euclidean metric: # Unf (left), MVi (middle) and Co
(right) for the clusterings produced by average (top row) / single (middle row) / complete linkage
clustering (bottom row) and the two variants of our heuristic approach.

27

F.5DrugConsumptionDataSet71402040608010002505007501000125015001750k-means++k-meGoiGsk-Fenterk-Fenter GF6ingle link.Avg. link.ComSl. link.25102030405060708090100k02505007501000125015001750#8nfDUug Consumption (1885 points) --- (uclideDn metUic25102030405060708090100k1.01.52.009iDrug ConVumption (1885 pointV) --- (uclideDn metric020406080100k250027503000325035003750400042504500CoDrug Consumption (1885 points) --- (uclideDn metric25102030405060708090100k02505007501000125015001750#8nfDUug Consumption (1885 points) --- 0DnhDttDn metUic25102030405060708090100k1.01.52.02.53.009iDrug ConVumption (1885 pointV) --- 0DnhDttDn metric020406080100k6000700080009000100001100012000CoDrug Consumption (1885 points) --- 0DnhDttDn metric25102030405060708090100k02505007501000125015001750#8nfDUug Consumption (1885 points) --- Chebyshev metUic25102030405060708090100k1.01.52.02.53.009iDrug ConVumption (1885 pointV) --- ChebyVhev metric020406080100k16001800200022002400CoDrug Consumption (1885 points) --- Chebyshev metricFigure10:DrugConsumptiondataset:#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbythevariousstandardalgorithmsasafunctionofkfortheEuclidean(toprow),Manhattan(middlerow)andChebyshevmetric(bottomrow).25102030405060708090100k020040060080010001200#8nfDUug ConVumption (1885 pointV) --- (uclideDn metUicAvg. link.Avg. #8Avg. 0925102030405060708090100k1.01.52.009iDrug ConVumption (1885 pointV) --- (uclideDn metric25102030405060708090100k280030003200340036003800400042004400CoDrug Consumption (1885 points) --- (uclideDn metric25102030405060708090100k02505007501000125015001750#8nfDUug ConVumStion (1885 SointV) --- (uclideDn metUic6ingle link.6ingle #86ingle 0925102030405060708090100k1.01.52.02.509iDrug ConVumption (1885 pointV) --- (uclideDn metric25102030405060708090100k40004100420043004400CoDrug Consumption (1885 points) --- (uclideDn metric25102030405060708090100k02004006008001000#8nfDUug ConVumption (1885 pointV) --- (uclideDn metUicCompl. link.Compl. #8Compl. 0925102030405060708090100k1.01.52.02.53.009iDrug ConVumption (1885 pointV) --- (uclideDn metric25102030405060708090100k27503000325035003750400042504500CoDrug Consumption (1885 points) --- (uclideDn metricFigure11:DrugConsumptiondatasetwithEuclideanmetric:#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbyaverage(toprow)/single(middlerow)/completelinkageclustering(bottomrow)andthetwovariantsofourheuristicapproach.26F.5DrugConsumptionDataSet71402040608010002505007501000125015001750k-means++k-meGoiGsk-Fenterk-Fenter GF6ingle link.Avg. link.ComSl. link.25102030405060708090100k02505007501000125015001750#8nfDUug Consumption (1885 points) --- (uclideDn metUic25102030405060708090100k1.01.52.009iDrug ConVumption (1885 pointV) --- (uclideDn metric020406080100k250027503000325035003750400042504500CoDrug Consumption (1885 points) --- (uclideDn metric25102030405060708090100k02505007501000125015001750#8nfDUug Consumption (1885 points) --- 0DnhDttDn metUic25102030405060708090100k1.01.52.02.53.009iDrug ConVumption (1885 pointV) --- 0DnhDttDn metric020406080100k6000700080009000100001100012000CoDrug Consumption (1885 points) --- 0DnhDttDn metric25102030405060708090100k02505007501000125015001750#8nfDUug Consumption (1885 points) --- Chebyshev metUic25102030405060708090100k1.01.52.02.53.009iDrug ConVumption (1885 pointV) --- ChebyVhev metric020406080100k16001800200022002400CoDrug Consumption (1885 points) --- Chebyshev metricFigure10:DrugConsumptiondataset:#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbythevariousstandardalgorithmsasafunctionofkfortheEuclidean(toprow),Manhattan(middlerow)andChebyshevmetric(bottomrow).25102030405060708090100k020040060080010001200#8nfDUug ConVumption (1885 pointV) --- (uclideDn metUicAvg. link.Avg. #8Avg. 0925102030405060708090100k1.01.52.009iDrug ConVumption (1885 pointV) --- (uclideDn metric25102030405060708090100k280030003200340036003800400042004400CoDrug Consumption (1885 points) --- (uclideDn metric25102030405060708090100k02505007501000125015001750#8nfDUug ConVumStion (1885 SointV) --- (uclideDn metUic6ingle link.6ingle #86ingle 0925102030405060708090100k1.01.52.02.509iDrug ConVumption (1885 pointV) --- (uclideDn metric25102030405060708090100k40004100420043004400CoDrug Consumption (1885 points) --- (uclideDn metric25102030405060708090100k02004006008001000#8nfDUug ConVumption (1885 pointV) --- (uclideDn metUicCompl. link.Compl. #8Compl. 0925102030405060708090100k1.01.52.02.53.009iDrug ConVumption (1885 pointV) --- (uclideDn metric25102030405060708090100k27503000325035003750400042504500CoDrug Consumption (1885 points) --- (uclideDn metricFigure11:DrugConsumptiondatasetwithEuclideanmetric:#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbyaverage(toprow)/single(middlerow)/completelinkageclustering(bottomrow)andthetwovariantsofourheuristicapproach.26Figure 13: Drug Consumption data set with Manhattan metric: # Unf (left), MVi (middle) and Co
(right) for the clusterings produced by average (top row) / single (middle row) / complete linkage
clustering (bottom row) and the two variants of our heuristic approach.

Figure 14: Drug Consumption data set with Chebyshev metric: # Unf (left), MVi (middle) and Co
(right) for the clusterings produced by average (top row) / single (middle row) / complete linkage
clustering (bottom row) and the two variants of our heuristic approach.

28

25102030405060708090100k0200400600800100012001400#8nfDUug ConVumption (1885 pointV) --- 0DnhDttDn metUicAvg. link.Avg. #8Avg. 0925102030405060708090100k1.01.52.02.53.009iDrug ConVumption (1885 pointV) --- 0DnhDttDn metric25102030405060708090100k700080009000100001100012000CoDrug Consumption (1885 points) --- 0DnhDttDn metric25102030405060708090100k02505007501000125015001750#8nfDUug ConVumStion (1885 SointV) --- 0DnhDttDn metUic6ingle link.6ingle #86ingle 0925102030405060708090100k1.01.52.02.53.009iDrug ConVumption (1885 pointV) --- 0DnhDttDn metric25102030405060708090100k108001100011200114001160011800CoDrug Consumption (1885 points) --- 0DnhDttDn metric25102030405060708090100k020040060080010001200#8nfDUug ConVumption (1885 pointV) --- 0DnhDttDn metUicCompl. link.Compl. #8Compl. 0925102030405060708090100k1.01.52.02.53.03.509iDrug ConVumption (1885 pointV) --- 0DnhDttDn metric25102030405060708090100k6000700080009000100001100012000CoDrug Consumption (1885 points) --- 0DnhDttDn metricFigure12:DrugConsumptiondatasetwithManhattanmetric:#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbyaverage(toprow)/single(middlerow)/completelinkageclustering(bottomrow)andthetwovariantsofourheuristicapproach.25102030405060708090100k02004006008001000120014001600#8nfDUug ConVumption (1885 pointV) --- ChebyVhev metUicAvg. link.Avg. #8Avg. 0925102030405060708090100k1.01.52.02.53.03.509iDrug ConVumption (1885 pointV) --- ChebyVhev metric25102030405060708090100k170018001900200021002200230024002500CoDrug Consumption (1885 points) --- Chebyshev metric25102030405060708090100k02505007501000125015001750#8nfDUug ConVumStion (1885 SointV) --- ChebyVhev metUic6ingle link.6ingle #86ingle 0925102030405060708090100k1.01.52.02.53.009iDrug ConVumption (1885 pointV) --- ChebyVhev metric25102030405060708090100k2200225023002350240024502500CoDrug Consumption (1885 points) --- Chebyshev metric25102030405060708090100k020040060080010001200#8nfDUug ConVumption (1885 pointV) --- ChebyVhev metUicCompl. link.Compl. #8Compl. 0925102030405060708090100k1.01.509iDrug ConVumption (1885 pointV) --- ChebyVhev metric25102030405060708090100k1800200022002400CoDrug Consumption (1885 points) --- Chebyshev metricFigure13:DrugConsumptiondatasetwithChebyshevmetric:#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbyaverage(toprow)/single(middlerow)/completelinkageclustering(bottomrow)andthetwovariantsofourheuristicapproach.2725102030405060708090100k0200400600800100012001400#8nfDUug ConVumption (1885 pointV) --- 0DnhDttDn metUicAvg. link.Avg. #8Avg. 0925102030405060708090100k1.01.52.02.53.009iDrug ConVumption (1885 pointV) --- 0DnhDttDn metric25102030405060708090100k700080009000100001100012000CoDrug Consumption (1885 points) --- 0DnhDttDn metric25102030405060708090100k02505007501000125015001750#8nfDUug ConVumStion (1885 SointV) --- 0DnhDttDn metUic6ingle link.6ingle #86ingle 0925102030405060708090100k1.01.52.02.53.009iDrug ConVumption (1885 pointV) --- 0DnhDttDn metric25102030405060708090100k108001100011200114001160011800CoDrug Consumption (1885 points) --- 0DnhDttDn metric25102030405060708090100k020040060080010001200#8nfDUug ConVumption (1885 pointV) --- 0DnhDttDn metUicCompl. link.Compl. #8Compl. 0925102030405060708090100k1.01.52.02.53.03.509iDrug ConVumption (1885 pointV) --- 0DnhDttDn metric25102030405060708090100k6000700080009000100001100012000CoDrug Consumption (1885 points) --- 0DnhDttDn metricFigure12:DrugConsumptiondatasetwithManhattanmetric:#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbyaverage(toprow)/single(middlerow)/completelinkageclustering(bottomrow)andthetwovariantsofourheuristicapproach.25102030405060708090100k02004006008001000120014001600#8nfDUug ConVumption (1885 pointV) --- ChebyVhev metUicAvg. link.Avg. #8Avg. 0925102030405060708090100k1.01.52.02.53.03.509iDrug ConVumption (1885 pointV) --- ChebyVhev metric25102030405060708090100k170018001900200021002200230024002500CoDrug Consumption (1885 points) --- Chebyshev metric25102030405060708090100k02505007501000125015001750#8nfDUug ConVumStion (1885 SointV) --- ChebyVhev metUic6ingle link.6ingle #86ingle 0925102030405060708090100k1.01.52.02.53.009iDrug ConVumption (1885 pointV) --- ChebyVhev metric25102030405060708090100k2200225023002350240024502500CoDrug Consumption (1885 points) --- Chebyshev metric25102030405060708090100k020040060080010001200#8nfDUug ConVumption (1885 pointV) --- ChebyVhev metUicCompl. link.Compl. #8Compl. 0925102030405060708090100k1.01.509iDrug ConVumption (1885 pointV) --- ChebyVhev metric25102030405060708090100k1800200022002400CoDrug Consumption (1885 points) --- Chebyshev metricFigure13:DrugConsumptiondatasetwithChebyshevmetric:#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbyaverage(toprow)/single(middlerow)/completelinkageclustering(bottomrow)andthetwovariantsofourheuristicapproach.27F.6 Indian Liver Patient Data Set

Figure 15: Indian Liver Patient data set: # Unf (left), MVi (middle) and Co (right) for the clusterings
produced by the various standard algorithms as a function of k for the Euclidean (top row), Manhattan
(middle row) and Chebyshev metric (bottom row).

Figure 16: Indian Liver Patient data set with Euclidean metric: # Unf (left), MVi (middle) and Co
(right) for the clusterings produced by average (top row) / single (middle row) / complete linkage
clustering (bottom row) and the two variants of our heuristic approach.

29

F.6IndianLiverPatientDataSet7150204060801000200400600800k-means++k-meGoiGsk-Fenterk-Fenter GF6C6ingle link.Avg. link.ComSl. link.25102030405060708090100k0100200300400#8nIIndLan LLveU 3atLent (579 poLnts) --- (uclLdean PetULc25102030405060708090100k1.01.52.02.53.03.54.04.55.05.509LIndLan LLver 3atLent (579 poLntV) --- (uclLdean PetrLc020406080100k40060080010001200CoIndLan LLver PatLent (579 poLnts) --- (uclLdean PetrLc25102030405060708090100k0100200300400500#8nIIndLan LLveU 3atLent (579 poLnts) --- 0anhattan PetULc25102030405060708090100k1.01.52.02.53.03.54.04.55.05.56.009LIndLan LLver 3atLent (579 poLntV) --- 0anhattan PetrLc020406080100k10001500200025003000CoIndLan LLver 3atLent (579 poLnts) --- 0anhattan PetrLc25102030405060708090100k0100200300400#8nIIndLan LLveU 3atLent (579 poLnts) --- Chebyshev PetULc25102030405060708090100k1.01.52.02.53.03.54.04.55.05.56.06.57.07.58.009LIndLan LLver 3atLent (579 poLntV) --- ChebyVhev PetrLc020406080100k200300400500600700800CoIndLan LLver 3atLent (579 poLnts) --- Chebyshev PetrLcFigure14:IndianLiverPatientdataset:#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbythevariousstandardalgorithmsasafunctionofkfortheEuclidean(toprow),Manhattan(middlerow)andChebyshevmetric(bottomrow).25102030405060708090100k0100200#8nIIndLan LLveU 3atLent (579 poLntV) --- (uclLdean PetULcAvg. lLnk.Avg. #8Avg. 0925102030405060708090100k1.01.52.02.53.03.54.009LIndLan LLver 3atLent (579 poLntV) --- (uclLdean PetrLc25102030405060708090100k40060080010001200CoIndLan LLver 3atLent (579 poLnts) --- (uclLdean PetrLc25102030405060708090100k0100200300400#8nIIndLan LLveU 3atLent (579 SoLntV) --- (uclLdean PetULc6Lngle lLnk.6Lngle #86Lngle 0925102030405060708090100k1.01.52.02.53.03.509LIndLan LLver 3atLent (579 poLntV) --- (uclLdean PetrLc25102030405060708090100k600700800900100011001200CoIndLan LLver 3atLent (579 poLnts) --- (uclLdean PetrLc25102030405060708090100k0100#8nIIndLan LLveU 3atLent (579 poLntV) --- (uclLdean PetULcCoPpl. lLnk.CoPpl. #8CoPpl. 0925102030405060708090100k1.01.52.02.53.03.54.04.509LIndLan LLver 3atLent (579 poLntV) --- (uclLdean PetrLc25102030405060708090100k40060080010001200CoIndLan LLver 3atLent (579 poLnts) --- (uclLdean PetrLcFigure15:IndianLiverPatientdatasetwithEuclideanmetric:#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbyaverage(toprow)/single(middlerow)/completelinkageclustering(bottomrow)andthetwovariantsofourheuristicapproach.28F.6IndianLiverPatientDataSet7150204060801000200400600800k-means++k-meGoiGsk-Fenterk-Fenter GF6C6ingle link.Avg. link.ComSl. link.25102030405060708090100k0100200300400#8nIIndLan LLveU 3atLent (579 poLnts) --- (uclLdean PetULc25102030405060708090100k1.01.52.02.53.03.54.04.55.05.509LIndLan LLver 3atLent (579 poLntV) --- (uclLdean PetrLc020406080100k40060080010001200CoIndLan LLver PatLent (579 poLnts) --- (uclLdean PetrLc25102030405060708090100k0100200300400500#8nIIndLan LLveU 3atLent (579 poLnts) --- 0anhattan PetULc25102030405060708090100k1.01.52.02.53.03.54.04.55.05.56.009LIndLan LLver 3atLent (579 poLntV) --- 0anhattan PetrLc020406080100k10001500200025003000CoIndLan LLver 3atLent (579 poLnts) --- 0anhattan PetrLc25102030405060708090100k0100200300400#8nIIndLan LLveU 3atLent (579 poLnts) --- Chebyshev PetULc25102030405060708090100k1.01.52.02.53.03.54.04.55.05.56.06.57.07.58.009LIndLan LLver 3atLent (579 poLntV) --- ChebyVhev PetrLc020406080100k200300400500600700800CoIndLan LLver 3atLent (579 poLnts) --- Chebyshev PetrLcFigure14:IndianLiverPatientdataset:#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbythevariousstandardalgorithmsasafunctionofkfortheEuclidean(toprow),Manhattan(middlerow)andChebyshevmetric(bottomrow).25102030405060708090100k0100200#8nIIndLan LLveU 3atLent (579 poLntV) --- (uclLdean PetULcAvg. lLnk.Avg. #8Avg. 0925102030405060708090100k1.01.52.02.53.03.54.009LIndLan LLver 3atLent (579 poLntV) --- (uclLdean PetrLc25102030405060708090100k40060080010001200CoIndLan LLver 3atLent (579 poLnts) --- (uclLdean PetrLc25102030405060708090100k0100200300400#8nIIndLan LLveU 3atLent (579 SoLntV) --- (uclLdean PetULc6Lngle lLnk.6Lngle #86Lngle 0925102030405060708090100k1.01.52.02.53.03.509LIndLan LLver 3atLent (579 poLntV) --- (uclLdean PetrLc25102030405060708090100k600700800900100011001200CoIndLan LLver 3atLent (579 poLnts) --- (uclLdean PetrLc25102030405060708090100k0100#8nIIndLan LLveU 3atLent (579 poLntV) --- (uclLdean PetULcCoPpl. lLnk.CoPpl. #8CoPpl. 0925102030405060708090100k1.01.52.02.53.03.54.04.509LIndLan LLver 3atLent (579 poLntV) --- (uclLdean PetrLc25102030405060708090100k40060080010001200CoIndLan LLver 3atLent (579 poLnts) --- (uclLdean PetrLcFigure15:IndianLiverPatientdatasetwithEuclideanmetric:#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbyaverage(toprow)/single(middlerow)/completelinkageclustering(bottomrow)andthetwovariantsofourheuristicapproach.28Figure 17: Indian Liver Patient data set with Manhattan metric: # Unf (left), MVi (middle) and Co
(right) for the clusterings produced by average (top row) / single (middle row) / complete linkage
clustering (bottom row) and the two variants of our heuristic approach.

Figure 18: Indian Liver Patient data set with Chebyshev metric: # Unf (left), MVi (middle) and Co
(right) for the clusterings produced by average (top row) / single (middle row) / complete linkage
clustering (bottom row) and the two variants of our heuristic approach.

30

25102030405060708090100k0100200300#8nIIndLan LLveU 3atLent (579 poLntV) --- 0anhattan PetULcAvg. lLnk.Avg. #8Avg. 0925102030405060708090100k1.01.52.02.53.03.54.009LIndLan LLver 3atLent (579 poLntV) --- 0anhattan PetrLc25102030405060708090100k10001500200025003000CoIndLan LLver 3atLent (579 poLnts) --- 0anhattan PetrLc25102030405060708090100k0100200300400500#8nIIndLan LLveU 3atLent (579 SoLntV) --- 0anhattan PetULc6Lngle lLnk.6Lngle #86Lngle 0925102030405060708090100k1.01.52.02.53.03.54.009LIndLan LLver 3atLent (579 poLntV) --- 0anhattan PetrLc25102030405060708090100k16001800200022002400260028003000CoIndLan LLver 3atLent (579 poLnts) --- 0anhattan PetrLc25102030405060708090100k0100200300#8nIIndLan LLveU 3atLent (579 poLntV) --- 0anhattan PetULcCoPpl. lLnk.CoPpl. #8CoPpl. 0925102030405060708090100k1.01.52.02.53.009LIndLan LLver 3atLent (579 poLntV) --- 0anhattan PetrLc25102030405060708090100k1000150020002500CoIndLan LLver 3atLent (579 poLnts) --- 0anhattan PetrLcFigure16:IndianLiverPatientdatasetwithManhattanmetric:#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbyaverage(toprow)/single(middlerow)/completelinkageclustering(bottomrow)andthetwovariantsofourheuristicapproach.25102030405060708090100k0100200#8nIIndLan LLveU 3atLent (579 poLntV) --- ChebyVhev PetULcAvg. lLnk.Avg. #8Avg. 0925102030405060708090100k1.01.52.02.509LIndLan LLver 3atLent (579 poLntV) --- ChebyVhev PetrLc25102030405060708090100k300400500600700800CoIndLan LLver 3atLent (579 poLnts) --- Chebyshev PetrLc25102030405060708090100k0100200300400#8nIIndLan LLveU 3atLent (579 SoLntV) --- ChebyVhev PetULc6Lngle lLnk.6Lngle #86Lngle 0925102030405060708090100k1.01.52.02.53.03.509LIndLan LLver 3atLent (579 poLntV) --- ChebyVhev PetrLc25102030405060708090100k450500550600650700750800CoIndLan LLver 3atLent (579 poLnts) --- Chebyshev PetrLc25102030405060708090100k0100200300#8nIIndLan LLveU 3atLent (579 poLntV) --- ChebyVhev PetULcCoPpl. lLnk.CoPpl. #8CoPpl. 0925102030405060708090100k1.01.52.02.53.03.54.04.55.009LIndLan LLver 3atLent (579 poLntV) --- ChebyVhev PetrLc25102030405060708090100k200300400500600700800CoIndLan LLver 3atLent (579 poLnts) --- Chebyshev PetrLcFigure17:IndianLiverPatientdatasetwithChebyshevmetric:#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbyaverage(toprow)/single(middlerow)/completelinkageclustering(bottomrow)andthetwovariantsofourheuristicapproach.2925102030405060708090100k0100200300#8nIIndLan LLveU 3atLent (579 poLntV) --- 0anhattan PetULcAvg. lLnk.Avg. #8Avg. 0925102030405060708090100k1.01.52.02.53.03.54.009LIndLan LLver 3atLent (579 poLntV) --- 0anhattan PetrLc25102030405060708090100k10001500200025003000CoIndLan LLver 3atLent (579 poLnts) --- 0anhattan PetrLc25102030405060708090100k0100200300400500#8nIIndLan LLveU 3atLent (579 SoLntV) --- 0anhattan PetULc6Lngle lLnk.6Lngle #86Lngle 0925102030405060708090100k1.01.52.02.53.03.54.009LIndLan LLver 3atLent (579 poLntV) --- 0anhattan PetrLc25102030405060708090100k16001800200022002400260028003000CoIndLan LLver 3atLent (579 poLnts) --- 0anhattan PetrLc25102030405060708090100k0100200300#8nIIndLan LLveU 3atLent (579 poLntV) --- 0anhattan PetULcCoPpl. lLnk.CoPpl. #8CoPpl. 0925102030405060708090100k1.01.52.02.53.009LIndLan LLver 3atLent (579 poLntV) --- 0anhattan PetrLc25102030405060708090100k1000150020002500CoIndLan LLver 3atLent (579 poLnts) --- 0anhattan PetrLcFigure16:IndianLiverPatientdatasetwithManhattanmetric:#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbyaverage(toprow)/single(middlerow)/completelinkageclustering(bottomrow)andthetwovariantsofourheuristicapproach.25102030405060708090100k0100200#8nIIndLan LLveU 3atLent (579 poLntV) --- ChebyVhev PetULcAvg. lLnk.Avg. #8Avg. 0925102030405060708090100k1.01.52.02.509LIndLan LLver 3atLent (579 poLntV) --- ChebyVhev PetrLc25102030405060708090100k300400500600700800CoIndLan LLver 3atLent (579 poLnts) --- Chebyshev PetrLc25102030405060708090100k0100200300400#8nIIndLan LLveU 3atLent (579 SoLntV) --- ChebyVhev PetULc6Lngle lLnk.6Lngle #86Lngle 0925102030405060708090100k1.01.52.02.53.03.509LIndLan LLver 3atLent (579 poLntV) --- ChebyVhev PetrLc25102030405060708090100k450500550600650700750800CoIndLan LLver 3atLent (579 poLnts) --- Chebyshev PetrLc25102030405060708090100k0100200300#8nIIndLan LLveU 3atLent (579 poLntV) --- ChebyVhev PetULcCoPpl. lLnk.CoPpl. #8CoPpl. 0925102030405060708090100k1.01.52.02.53.03.54.04.55.009LIndLan LLver 3atLent (579 poLntV) --- ChebyVhev PetrLc25102030405060708090100k200300400500600700800CoIndLan LLver 3atLent (579 poLnts) --- Chebyshev PetrLcFigure17:IndianLiverPatientdatasetwithChebyshevmetric:#Unf(left),MVi(middle)andCo(right)fortheclusteringsproducedbyaverage(toprow)/single(middlerow)/completelinkageclustering(bottomrow)andthetwovariantsofourheuristicapproach.29