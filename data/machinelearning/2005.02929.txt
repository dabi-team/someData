Training robust neural networks using Lipschitz bounds

Patricia Pauli, Anne Koch, Julian Berberich, Paul Kohler and Frank Allg¨ower

0
2
0
2

p
e
S
5
1

]

G
L
.
s
c
[

2
v
9
2
9
2
0
.
5
0
0
2
:
v
i
X
r
a

Abstract— Due to their susceptibility to adversarial perturba-
tions, neural networks (NNs) are hardly used in safety-critical
applications. One measure of robustness to such perturbations
in the input is the Lipschitz constant of the input-output map
deﬁned by an NN. In this work, we propose a framework
to train multi-layer NNs while at the same time encouraging
robustness by keeping their Lipschitz constant small, thus
addressing the robustness issue. More speciﬁcally, we design
an optimization scheme based on the Alternating Direction
Method of Multipliers that minimizes not only the training
loss of an NN but also its Lipschitz constant resulting in
a semideﬁnite programming based training procedure that
promotes robustness. We design two versions of this training
procedure. The ﬁrst one includes a regularizer that penalizes
an accurate upper bound on the Lipschitz constant. The second
one allows to enforce a desired Lipschitz bound on the NN at
all times during training. Finally, we provide two examples to
show that the proposed framework successfully increases the
robustness of NNs.

I. INTRODUCTION

Neural networks (NNs) and deep learning have lately been
successful in many ﬁelds [1], [2], where they are mostly used
for classiﬁcation and segmentation problems, as well as in
reinforcement learning [3]. The main advantages of NNs are
that they can be trained straightforwardly using backprop-
agation and as universal function approximators they have
the capability to represent complex nonlinearities. While
NNs are powerful and broadly applicable they lack rigorous
guarantees which is why they are not yet applied to safety-
critical applications such as medical devices and autonomous
driving. Adversarial attacks can easily deceive an NN by
adding imperceptible perturbations to the input [4] which
is a problem that has recently been tackled increasingly in a
number of different ways such as adversarial training [5] and
defensive distillation [6]. Another promising approach is to
show that an NN is provably robust against norm-bounded
adversarial perturbations [7], [8], e.g. by maximizing margins
[9], while yet another one is to use Lipschitz constants
as a robustness measure that indicate the sensitivity of the
output to perturbations in the input [10]. There are a number
of other regression methods that provide guaranteed and
optimized upper bounds on the Lipschitz constant such as
nonlinear set membership predictions, kinky inference [11]

This work was funded by Deutsche Forschungsgemeinschaft

(DFG,
German Research Foundation) under Germanys Excellence Strategy - EXC
2075 - 390740016. The authors thank the International Max Planck Research
School for Intelligent Systems (IMPRS-IS) for supporting Patricia Pauli,
Anne Koch, and Julian Berberich.

Patricia

Pauli, Anne Koch,

Julian Berberich,

and Frank Allg¨ower
and
Automatic
patricia.pauli@ist.uni-stuttgart.de

Institute
of

are with the

University

Control,

Paul Kohler
for Systems Theory
Germany
Stuttgart,

and Lipschitz interpolation [12], [13]. Based on this notion
of Lipschitz continuity, we propose a framework for training
of robust NNs that encourages a small Lipschitz constant by
including a regularizer or respectively, a constraint on the
NN’s Lipschitz constant.

Trivial Lipschitz bounds of NNs can be determined by
the product of the spectral norms of the weights [4] which is
used during training in [14], [15]. Similar to [16], we use the
Lipschitz constant as a regularization functional. However,
they use local Lipschitz constants whereas we penalize the
global one. In [17], Fazlyab et al. propose an interesting
new estimation scheme for more accurate upper bounds
on the Lipschitz constant than the weights’ spectral norms
exploiting the structure of the nonlinear activation functions.
Activation functions are gradients of convex potential func-
tions, and hence monotonically increasing functions with
bounded slopes, which is used in [17] to state the property of
slope-restriction as an incremental quadratic constraint and
then formulate a semideﬁnite program (SDP) that determines
an upper bound on the Lipschitz constant. In [17], three
variants of the Lipschitz constant estimation framework are
proposed trading off accuracy and computational tractability.
In this paper, we disprove by counterexample the most
accurate approach presented in [17], and we employ the other
approaches for training of robust NNs. More speciﬁcally, we
include the SDP-based Lipschitz bound characterization of
[17] in the training procedure via an Alternating Direction
Method of Multipliers (ADMM) scheme. We present two
versions of the training method, (i) a regularizer rendering the
Lipschitz constant small and (ii) enforces guaranteed upper
bounds on the Lipschitz constant during training.

The main contributions of this manuscript are the two
training procedures for robust NNs based on the notion
of Lipschitz continuity, using a tight upper bound on the
Lipschitz constant. In addition, we show that the method
for Lipschitz constant estimation for NNs that was recently
proposed in [17] requires a modiﬁcation for the least con-
servative choice of decision variables. This manuscript is
organized as follows. In Section II, we introduce Lipschitz
constant estimation for NNs based on [17] but disprove
their most accurate Lipschitz estimator. In Section III, we
present a training procedure with Lipschitz regularization
and outline the setup of the optimization problem that is
solved using ADMM. Subsequently, we introduce a variation
of the proposed procedure that allows to enforce Lipschitz
bounds on the NN and ﬁnally, we discuss the convergence
and the computational tractability of the ADMM scheme.
In Section IV, we provide two examples on which we
successfully apply the proposed training procedures.

 
 
 
 
 
 
II. LIPSCHITZ CONSTANT ESTIMATION

In this section, we brieﬂy introduce robustness in the
context of neural networks. We then state a method to
estimate the Lipschitz constant of an NN based on [17] and
ﬁnally argue why one of the methods for Lipschitz constant
estimation proposed in [17] is incorrect.

A. Robustness of NNs

A robust NN should not change its prediction if the input
is perturbed imperceptibly. To quantify robustness, a suitable
robustness measure has to be deﬁned. One deﬁnition is that
perturbations from a norm-bounded uncertainty set may not
alter the prediction. Alternatively, using probabilistic ap-
proaches, random perturbations do not change the prediction
with a certain probability. A third alternative is the Lipschitz
constant, a sensitivity measure. A function f : Rn → Rm is
globally Lipschitz continuous if there exists an L ≥ 0 such
that

kf (x) − f (y)k ≤ L kx − yk

∀x, y ∈ Rn.

(1)

The smallest L for which (1) holds is the Lipschitz constant
L∗. If the input changes from x to y, the Lipschitz constant
gives an upper bound on how much the output f changes.
Hence, a low Lipschitz constant indicates low sensitivity
which is equivalent to high robustness. In this work, we aim
to minimize the Lipschitz constant or respectively bound the
Lipschitz constant from above during training to increase the
robustness of the resulting NN.

Regularization, i.e., adding a penalty term to the objective
function of the NN, is a prevalent measure in NN training
in order to prevent overﬁtting. L2 regularization penalizes
the squared norm of the weights and L1 regularization the
weights’ L1 norm. Bounding the weights counteracts the ﬁt
of sudden peaks and outliers, promotes better generalization,
the
and smoothens the resulting NN [18]. Furthermore,
product of the spectral norms of the weights provides a trivial
bound on an NN’s Lipschitz constant and consequently, L1
and L2 regularization improve the robustness of an NN in
the sense of Lipschitz continuity. In this paper, we penalize
a more accurate estimate of the Lipschitz constant, leading
to a more direct and potentially more effective approach.

B. Lipschitz constant estimation

In the following, we outline a method to estimate bounds
on the Lipschitz constant of multi-layer NNs exploiting the
slope-restricted structure of the nonlinear activation func-
tions, as it was shown in [17]. This method named LipSDP
yields more accurate bounds than trivial bounds, i.e., the
product of the spectral norms of the weights.

Continuous nonlinear activation functions ϕ : R → R
can be interpreted as gradients of continuously differentiable,
convex potential functions and consequently, they are slope-
restricted, i.e., their slope is at least α and at most β,

α ≤

ϕ(x) − ϕ(y)
x − y

≤ β

∀x, y ∈ R,

(2)

where 0 ≤ α < β < ∞. Consider the vector of activation
: Rni → Rni, φi(xi) = [ϕ(xi
ni )]⊤
functions φi
and a fully-connected feed-forward NN with l hidden layers
f : Rn0 → Rnl+1 described by the following equations

1) · · · ϕ(xi

x0 = x,

xi+1 = φi+1(W ixi + bi),
f (x) = W lxl + bl,

i = 0, . . . , l − 1,

(3)

where W i ∈ Rni+1×ni are the weight matrices and bi ∈
Rni+1 are the biases of the i-th layers, n0, · · · , nl+1 being
the dimension of the input, the neurons in the hidden layers,
and the output. For every neuron, the slope-restriction prop-
erty (2) can be written as an incremental quadratic constraint.
Weighting these quadratic constraints with λii ≥ 0, or
respectively, all activations with a diagonal weighting matrix

T ∈ Dn := {T ∈ Rn×n | T =

λiieie⊤

i , λii ≥ 0},

n

results in an incremental quadratic constraint for the stacked
activations:

i=1
X

˜x − ˜y
φ(˜x) − φ(˜y)

(cid:21)

(cid:20)

⊤

(cid:20)

−2αβT
(α + β)T

(α + β)T
−2T

˜x − ˜y
φ(˜x) − φ(˜y)

≥ 0,

(cid:21)

(cid:21)

(cid:20)

:=F (T )

|

{z

(4)
}
l
for all T ∈ Dn, ˜x, ˜y ∈ Rn with n =
i=1 ni. Herein,
the inputs to all neurons are stacked up in one vector
˜x, ˜y, respectively, and the activation function φ : Rn →
Rn, φ(˜x) = [φ1(x1)⊤ · · · φl(xl)⊤]⊤ is then applied to the
concatenated vector. We now formulate an SDP based on
[17] that exploits (4) for the estimation of an upper bound
on the Lipschitz constant of the map characterized by the
underlying NN. We therefore deﬁne

P

A = 

W 0
...
0




0
...

. . .
. . .
. . . W l−1

0
...
0

, B =

0 In

.



(cid:2)

(cid:3)




Theorem 1. Suppose there exist L2 > 0, T ∈ Dn such that
Pl(L2, T ) (cid:22) 0,

(5)

where

A
B

⊤

A
B

−L2I
0
0

0
0
0
0
0 W l⊤

Pl(L2, T ) :=

F (T )

.
W l

Then, (3) is globally Lipschitz continuous with Lipschitz

+





(cid:21)

(cid:21)

(cid:20)

(cid:20)

bound L ≥ L∗.

The proof directly follows from A.4 in [19], the extended
version of [17]. Note that this result differs from Theorem 2
in [17] as T in our case is a diagonal matrix. As we show
in Section II-C, the result does not hold in general for the
larger parametrization of T provided in [17]. The smallest
value for the Lipschitz upper bound is determined by solving
the SDP

L2 s. t. Pl(L2, T ) (cid:22) 0, T ∈ Dn,

(6)

min
L2,T

where T and L2 serve as decision variables.

which disproves Lemma 1 of [17].

Remark 2. In the case of one hidden layer the matrix P1
reduces to

−2αβW 0⊤
P1(L2, T )=
"

(α + β)T W 0

T W 0 − L2I (α + β)W 0⊤
−2T + W 1⊤

T
.
W 1#

C. Counterexample for LipSDP with coupling

In [17], three versions of the method LipSDP for Lipschitz
constant estimation are stated that reconcile accuracy of the
Lipschitz bound and computational complexity of the method
by adjusting the number of the decision variables in the SDP.
In this section, we give an illustrative counterexample to
show that Theorems 1 and 2 in [17] are incorrect for the
most accurate variant of LipSDP.

Theorem 2 in [17] resembles Theorem 1 of this manuscript
with the difference that in [17] a set of symmetric coupling
matrices

Tn ={T ∈ Rn×n | T =

λiieie⊤
i

n

+

i=1
X

λij (ei − ej)(ei − ej)⊤, λij ≥ 0},

X1≤i<j≤n

is introduced whereas we state Theorem 1 for a set of
diagonal matrices Dn. In the following, we give a minimal
counterexample to show that, as suggested in this manuscript,
a further restriction of the class of T is required. For that
purpose, consider an NN with one hidden layer of size
n1 = 2, input and output size n0 = n2 = 1, activation
function tanh and weights and biases

W 0 =

, b0 =

, W 1 =

−1 1

, b1 = −0.5.

−1
−1

(cid:20)

(cid:21)

−1
1

(cid:20)

(cid:21)

(cid:3)

2 , π

(cid:2)
The resulting NN provides a good ﬁt for the cosine function
on x ∈ [− π
2 ] with a maximum deviation in the output
of 0.0843. Therefore, the maximum slope of the cosine
gives a good approximation of the Lipschitz constant of
this NN, which is ±1 at x = ± π
2 , such that L∗ ≈ 1.
However, the linear matrix inequality (LMI) (5) is feasible
for arbitrarily small L2 and T = (e1 − e2)(e1 − e2)⊤ ∈
Tn. An arbitrarily small L is obviously no upper bound
on the Lipschitz constant of a cosine like function, thus
contradicting Theorem 1 in [17].

To understand why in this case the LipSDP method fails
to provide an upper bound on the Lipschitz constant, we look
into the coupling of the neurons accounted for by T ∈ Tn.
Theorems 1 and 2 in [17] build on the assumption that (4)
holds for all T ∈ Tn (Lemma 1 in [17]). In the following, we
show by counterexample that Lemma 1 in [17] is incorrect,
i.e., that for a given slope-restricted function ϕ there are
˜x, ˜y ∈ Rn and T ∈ Tn that violate (4). We choose n1 = 2,
ϕ as the ReLU function that is slope-restricted in the sector
⊤
[0, 1], and T = (e1 − e2)(e1 − e2)⊤ ∈ Tn. Let x =
and y =

. Then evaluating (4) yields

−1.5 0

1

0

⊤

(cid:2)

(cid:3)

(cid:2)
˜x − ˜y
φ(˜x) − φ(˜y)

(cid:21)

(cid:20)

⊤
(cid:3)

(cid:20)

T

0
T −2T

(cid:21) (cid:20)

˜x − ˜y
φ(˜x) − φ(˜y)

(cid:21)

= −2 < 0,

III. TRAINING ROBUST NNS

In Section II-B, we stated a method that provides cer-
tiﬁcates on an NN’s Lipschitz constant. In this section, we
employ these certiﬁcates to design a training procedure for
robust NNs. The proposed approach allows us to directly
regularize the Lipschitz constant during training, which is
only possible indirectly in regularization methods such as
L2 regularization. We present two versions of it, the ﬁrst one
allows for minimization of the upper bound on the Lipschitz
constant and the second one allows to enforce a desired
bound on the Lipschitz constant.

A. Weights as decision variables

Eq. (6) can be used to assess an NN’s robustness after
training, whereas in this manuscript, to promote robustness
during training, we use Eq. (6) to update the weights while
minimizing the bound on the Lipschitz constant. Applying
the Schur complement to (5) for α = 0, the LMI can be
rearranged, yielding an equivalent LMI that is linear in L2
and W = (W 0, · · · , W l), for ﬁxed T ∈ Dn:

Ml(L2, W ) :=

A
B

(cid:20)

⊤

A
B

βT
0
βT −2T
(cid:21) (cid:20)
0
0
0
0
0 W l⊤
0
0 W l −I

(cid:21)
0
0

(cid:21)
(cid:20)
−L2I
0
0
0

+ 





(7)



(cid:22) 0





Remark 3. For α > 0, the underlying constraint is not
convex in W . Consequently, we cannot state LMI constraints
for α > 0 and instead set α = 0. This is a conservative
choice for some activation functions, yet the tight lower
bound for the most common ones. E.g. for ReLU and tanh
the tight bounds are α = 0 and β = 1 and the sigmoid
function is slope-restricted with α = 0 and β = 1
4 .

Remark 4. For the single-layer case, M1 conveniently
reduces to

M1(L2, W ) = 



−L2I
βT W 0
0

T

βW 0⊤
−2T
W 1

0
W 1⊤
−I

.






While the Lipschitz constant estimation scheme in [17]
optimizes over T , throughout the manuscript, we choose
T to be a ﬁxed matrix. This introduces conservatism into
the framework and necessitates a suitable choice for T in
order to keep the introduced conservatism to a minimum. For
instance, the matrix T may be determined from the Lipschitz
constant estimation outlined in Section II-B on the vanilla
NN or the L2 regularized NN trained on the same problem.

B. Lipschitz regularization

In general, NNs are trained on input-output data with the
objective of minimizing a predeﬁned loss, e.g. the mean
squared error, cross-entropy, or hinge loss. We propose to
not only minimize the NN’s loss but also its Lipschitz

constant. This yields an optimization problem with two
separate objectives that can be solved conveniently using
ADMM.

ADMM is an algorithm that solves optimization problems
by splitting them into smaller subproblems that are easier
to handle individually [20]. In order to apply the ADMM
algorithm, the objective must be separable. The resulting
subobjectives are then deﬁned on uncoupled convex sets and
are subject to linear equality constraints. The ADMM scheme
solves the resulting optimization problem through indepen-
dent minimization steps on the augmented Lagrangian of the
optimization problem and a dual update step. The objectives
at hand, i.e., the NN’s loss and the Lipschitz bound, are
indeed separable and deﬁned on uncoupled convex sets.
However, the problems are not completely independent and
need to be connected through a linear constraint that requires
the introduction of additional variables ¯W = ( ¯W 0, . . . , ¯W l)
of equal size as W . The loss of the NN L(W ) is an
explicit function of the weights W and the Lipschitz bound
L depends on ¯W through the LMI (7), yielding the following
optimization problem:

min
W, ¯W ,L2
s. t.

L(W ) + µL2 + 1Ml (L2, ¯W )

W = ¯W

(8)

where µ > 0 is a weighting parameter adjusting the trade-off
between accuracy and robustness and

1M (L2, ¯W ) =

if Ml(L2, ¯W ) (cid:22) 0
0
∞ if Ml(L2, ¯W ) ≻ 0

.

(

is the indicator function. Applying the ADMM scheme to
problem (8), results in the augmented Lagrangian function
Lρ(W, ¯W , L2, Y ) := L(W ) + µL2 + 1Ml (L2, ¯W )
2
+tr(Y (W − ¯W )) +

W − ¯W

ρ
2

with Lagrange multipliers Y i ∈ Rni×ni+1, Y = (Y 0, Y 1)
and the penalty parameter ρ > 0. The optimum for (8) is
then determined via the following iterative ADMM update
steps:

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Wk+1 = arg min

W

Lρ(W, ¯Wk, L2

k, Yk)

(L2

k+1, ¯Wk+1) = arg min
L2, ¯W

Lρ(Wk+1, ¯W , L2, Yk)

Yk+1 =Yk + ρ(Wk+1 − ¯Wk+1).

(9a)

(9b)

(9c)

For training of robust NNs, we carry out the corresponding
updates consecutively until convergence. The loss function
is optimized analytically using backpropagation (Eq. (9a))
whereas the Lipschitz update step (9b) is an SDP, as im-
plementation of the indicator function corresponds to an
LMI constraint. Hence, the Lipschitz update step requires to
solve an SDP in every iteration and thereby adds additional
computations compared to the training of a vanilla NN.

Remark 5. It
is possible to extend the framework and
optimize over L2, T , and W at the same time which requires
a second LMI constraint in (8) and an additional update step

in (9), resulting in a multi-block ADMM scheme. This reduces
conservatism but increases computation time.

C. Enforcing Lipschitz bounds

In Section III-B, we suggested to minimize an upper bound
on the Lipschitz constant of an NN. Using the proposed
ADMM framework, it is also possible to enforce a desired
upper bound on the Lipschitz constant during training of an
NN. In that case, the Lipschitz constant is not minimized but
instead set to a desired value Ldes. Judiciously, L2 does not
appear in the optimization objective of this setup

min
W, ¯W
s. t.

L(W ) + 1Ml(L2

des, ¯W )

W = ¯W

(10)

where W and ¯W serve as decision variables. For enforce-
ment of Lipschitz bounds, we apply the ADMM algorithm
as in (9) to (10) instead of (8), i.e., we set L = Ldes instead
of optimizing over L2.

Theorem 6. When training a fully-connected NN with l
hidden layers (3) by executing the ADMM scheme (9) for
(10), Ldes is an upper bound on the Lipschitz constant for
the NN with weights ¯W at all times during training.

Proof. Theorem 6 follows from the fact that, by design,
the bound Ldes on the Lipschitz constant is enforced in
every iteration of the ADMM scheme, more speciﬁcally,
in every Lipschitz update step (9b) by the LMI constraint
Ml(L2

des, ¯W ) (cid:22) 0 on the weights ¯W .

The training procedure based on (10) allows to choose the
value of the Lipschitz bound and to train NNs with Lipschitz
guarantees. This way, a desired degree of robustness can be
directly enforced. However, the choice of such a constraint on
L is always connected to the trade-off between accuracy and
robustness, as the ﬁt generally deteriorates when decreasing
the Lipschitz constant constraint. In addition, it is helpful to
initialize the weight parameters appropriately which does not
only accelerate training but may also facilitate a better ﬁt.

D. Convergence

ADMM was ﬁrst introduced for optimization problems
with convex subobjectives [20] and later, analyses of the
ADMM scheme for non-convex objectives including further
structural assumptions on the objective were formulated
[21]. In the proposed framework, the loss L(W ) clearly is
not convex and has no obvious structural properties which
renders a thorough convergence analysis complicated and
beyond the scope of this work. However, looking at the
subproblems (9a) and (9b) separately, we point out that
for (9a) gradient descent almost surely converges to local
minima even for non-convex problems [22], and that (9b) is
a semideﬁnite program with a unique minimizer. Thus adding
the convex regularization term and the indicator function of
a convex set to the optimization problem of NN training, that
converges reliably, does not add complexity in the form of
non-convexity to the optimization problem.

1

0.8

0.6

0.4

0.2

0

0

1

0.8

0.6

0.4

0.2

0

0

0.5

1

0.5

1

(a) 2D data

(b) Logits at x2 = 0.5

Fig. 1: 2D classiﬁcation example.

E. Computational tractability

Computational tractability and scalability of the proposed
framework depend on the number of decision variables of
the SDP. Generally, the complexity of SDP solvers scales
cubically with the number of decision variables. Hence, for
high-dimensional decision variables, solving an SDP is com-
putationally more expensive than solving an unconstrained
optimization problem using gradient descent. Therefore, the
Lipschitz update step (9b) becomes the bottleneck of the
proposed method as the number of neurons per hidden layers
and the number of hidden layers, that together determine
the size of the weights, increases. For example for picture
inputs as commonly used in classiﬁcation problems, the input
dimension is usually high, potentially leading to high com-
putation times or computational intractability. Nevertheless,
downscaling the input using convolutional or pooling layers
provides an option to improve computation time on larger
scale problems and makes the proposed method indeed a
worthwhile one to infer robustness to a neural network and
obtain guarantees on the Lipschitz bound, also on large-
scale problems. In Section IV, we show in an example that
our method is applicable to MNIST, a typical benchmark
classiﬁcation problem.

In order to keep the number of Lipschitz update steps to
a minimum, we advise to ﬁrst fully train a neural network
without regularizers or with standard regularizers, such as
the L2 regularizer, which serves as an initialization of
the matrix T and the weights W , ¯W . Loosely speaking,
our method provides a reﬁnement of the pretrained neural
network and allows to subsequently optimize robustness or
impose robustness guarantees on the network by minimizing
or respectively, by enforcing an upper bound on the Lipschitz
constant.

IV. SIMULATION RESULTS

In this section, we illustrate the beneﬁts of the presented
framework for training of robust NNs on a 2D toy example
and on MNIST. Our ﬁrst illustrative example is a classiﬁca-
tion problem of 2D data with three classes, shown in Fig. 1a.
We design a feed-forward NN with two hidden layers of
n1 = n2 = 10 neurons each, activation function tanh that

is slope-restricted with α = 0, β = 1, and the cross entropy
loss (CEL) as the loss function. For comparison, we train
three NNs, a vanilla NN, an NN with L2 regularization (L2-
NN) for benchmarking and ﬁnally the Lipschitz regularized
NN (Lipschitz-NN) according to Section II-B, wherein the
NN loss update step (9a) is solved using stochastic gradient
descent and the SDP (9b) is solved using numerical SDP
solvers [23], [24]. Before training, we initialize the Lipschitz-
NN with the L2 regularized NN with penalty parameter λ =
4 × 10−3. The hyperparameters ρ = 0.25 and µ = 1 × 10−5
are chosen such that for comparability the Lipschitz constants
of the L2-NN and the Lipschitz-NN are roughly the same.
The resulting cross-entropy losses, accuracies on test data
and bounds on the Lipschitz constant, that are summarized
in Table I, show that the nominal NN achieves a small
CEL, yet a high Lipschitz bound of 242. Comparing the two
regularizers, we see that Lipschitz regularization here leads
to both a lower Lipschitz bound L, hence higher robustness,
and a lower CEL than L2 regularization. Even though, due
to the trade-off between accuracy and robustness, the CEL
of the Lipschitz-NN is higher than the CEL of the nominal
NN, the accuracy of the Lipschitz-NN is not compromised.
On the contrary, the Lipschitz-NN even provides the highest
accuracy, as the nominal NN tends to overﬁt the data whereas
the L2 regularized NN fails to provide a good ﬁt in this
example. Fig. 1b shows a projection at x2 = 0.5 of the
logits resulting from the three NNs for all three classes (blue,
green, and orange) onto the x1 dimension. We clearly see the
effect of a lower Lipschitz constant in the less steep slopes
of the curves while the decision boundaries of the Lipschitz-
NN remain accurate. The price to pay for the improved
robustness is an increase in computation time (compare
Table I), since training of the Lipschitz-NN requires several
computationally involved ADMM iterations. Note that for
low-dimensional problems the Lipschitz update step (9b)
it becomes
is faster than the loss update step (9a), yet
computationally expensive for high-dimensional problems
(cf. Section III-E).

TABLE I: CEL, accuracy, Lipschitz bound, training times
¯ttr (loss/Lip step)
12.6s
16.9s
531s (41.3s/0.63s)
56.4s
57.4s
2566s (365s/111s)

Accuracy
88.66%
86.10%
90.56%
96.65%
90.58%
96.45%

Nom-NN
L2-NN
Lip-NN
Nom-NN
L2-NN
Lip-NN

CEL
0.07
0.27
0.22
0.09
0.26
0.20

L
242
69.5
67.2
96.6
9.49
8.74

MNIST

2D ex.

In our second example, we apply the Lipschitz regulariza-
tion framework to the high-dimensional benchmark data set
MNIST [25]. We train a feed-forward NN with one pooling
layer, one hidden layer with n1 = 50 neurons, the activation
function tanh, the cross entropy loss (CEL) and again, we
train three NNs, a vanilla NN, an L2-NN, and a Lipschitz-NN
with ρ = 0.25 and µ = 0.01, initializing the Lipschitz-NN
from the L2-NN with penalty parameter λ = 3 × 10−3. The
input dimension of the data is 28 × 28 that is downscaled
by the pooling layer to an input size of 14 × 14. Note

100

90

80

70

0

100

90

80

70

0.1

0.2

0.3

0.4

0

0.2

0.4

0.6

0.8

(a) Gaussian noise

(b) Uniform noise

Fig. 2: Accuracy on noise corrupted MNIST test data.

that the method also works on the 28 × 28 data without
a pooling layer, yet the Lipschitz step becomes signiﬁcantly
more time-consuming. From the resulting accuracies on test
data and bounds on the Lipschitz constant shown in Table I,
we conclude that on MNIST our framework also ﬁnds an
NN with a low Lipschitz constant but comparable accuracy
to the nominal NN, while the accuracy of an L2-NN with a
comparably low Lipschitz constant is signiﬁcantly compro-
mised. Fig. 2 shows the evaluation of the three NNs on noise
corrupted data, that was created by adding Gaussian noise
N (0, σ2) and respectively, uniform noise U(−b, b) to the
(0, 1)-normalized MNIST data. Advantages of the Lipschitz-
NN become apparent for Gaussian noise with low standard
deviation and noise from a narrow uniform distribution.

Altogether, the results show that Lipschitz regularization
can be used to effectively train robust NNs while trading
off robustness and accuracy. Code to reproduce the examples
can be found at https://github.com/st157640/Training-robust-
neural-networks-using-Lipschitz-bounds.git.

V. CONCLUSION

We proposed a framework for training of multi-layer NNs
that encourages robustness, by both considering Lipschitz
regularization and by enforcing Lipschitz bounds during
training. The underlying SDP [17] estimates the upper bound
on the Lipschitz constant more accurately than traditional
methods as it exploits the fact that activation functions are
slope-restricted. We designed an optimization scheme based
on this SDP that trains an NN to ﬁt input-output data and
at the same time increases its robustness in terms of Lip-
schitz continuity. We used ADMM to solve the underlying
optimization problem and to therein conveniently incorporate
the trade-off between accuracy and robustness. In addition,
we presented a variation of the framework that allows for
bounding the Lipschitz constant by a desired value, i.e.,
training NNs with robustness guarantees. We successfully
tested our method on two examples where we benchmarked
it with L2 regularization.

Next steps include the application of our method to
control problems by using LMI constraints to verify and
enforce properties, such as closed-loop stability, on feed-
back interconnections that include NN controllers. Also, we
plan to explore alternatives for the ADMM algorithm that
solve the underlying optimization problem in an accelerated
manner. In addition, for benchmarking purposes, we plan to
compare the proposed methods to other training procedures
that improve robustness.

REFERENCES

[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in Neural
Information Processing Systems, 2012, pp. 1097–1105.

[2] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu,
and P. Kuksa, “Natural language processing (almost) from scratch,”
Journal of machine learning research, vol. 12, no. Aug, pp. 2493–
2537, 2011.

[3] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol.

521, no. 7553, pp. 436–444, 2015.

[4] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfel-
low, and R. Fergus, “Intriguing properties of neural networks,” arXiv
preprint arXiv:1312.6199, 2013.

[5] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harness-
ing adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.
[6] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation
as a defense to adversarial perturbations against deep neural networks,”
in 2016 IEEE Symposium on Security and Privacy (SP).
IEEE, 2016,
pp. 582–597.

[7] E. Wong and Z. Kolter, “Provable defenses against adversarial ex-
amples via the convex outer adversarial polytope,” in International
Conference on Machine Learning, 2018, pp. 5283–5292.

[8] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu,
“Towards deep learning models resistant to adversarial attacks,” arXiv
preprint arXiv:1706.06083, 2017.

[9] Y. Tsuzuku, I. Sato, and M. Sugiyama, “Lipschitz-margin training:
Scalable certiﬁcation of perturbation invariance for deep neural net-
works,” in Advances in neural information processing systems, 2018,
pp. 6541–6550.

[10] P. Combettes and J.-C. Pesquet, “Lipschitz certiﬁcates for layered

network structures driven by averaged activation operators,” 2020.

[11] J.-P. Calliess, “Lazily adapted constant kinky inference for non-
parametric regression and model-reference adaptive control,” arXiv
preprint arXiv:1701.00178, 2016.

American Control Conference (ACC).

[12] ——, “Lipschitz optimisation for Lipschitz interpolation,” in 2017
IEEE, 2017, pp. 3141–3146.
[13] E. Maddalena and C. Jones, “Learning non-parametric models with
guarantees: A smooth Lipschitz interpolation approach,” Tech. Rep.,
2019.

[14] M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier,
“Parseval networks: Improving robustness to adversarial examples,” in
International Conference on Machine Learning, 2017, pp. 854–863.
[15] H. Gouk, E. Frank, B. Pfahringer, and M. Cree, “Regularisation of
neural networks by enforcing Lipschitz continuity,” arXiv preprint
arXiv:1804.04368, 2018.

[16] M. Hein and M. Andriushchenko, “Formal guarantees on the robust-
ness of a classiﬁer against adversarial manipulation,” in Advances in
Neural Information Processing Systems, 2017, pp. 2266–2276.
[17] M. Fazlyab, A. Robey, H. Hassani, M. Morari, and G. Pappas, “Ef-
ﬁcient and accurate estimation of Lipschitz constants for deep neural
networks,” in Advances in Neural Information Processing Systems,
2019, pp. 11 423–11 434.

[18] A. Krogh and J. A. Hertz, “A simple weight decay can improve gen-
eralization,” in Advances in Neural Information Processing Systems,
1992, pp. 950–957.

[19] M. Fazlyab, A. Robey, H. Hassani, M. Morari, and G. J. Pappas,
“Efﬁcient and accurate estimation of Lipschitz constants for deep
neural networks,” arXiv preprint arXiv:1906.04893, 2019.

[20] S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein et al., “Dis-
tributed optimization and statistical learning via the alternating di-
rection method of multipliers,” Foundations and Trends R(cid:13) in Machine
learning, vol. 3, no. 1, pp. 1–122, 2011.

[21] Y. Wang, W. Yin, and J. Zeng, “Global convergence of admm in
nonconvex nonsmooth optimization,” Journal of Scientiﬁc Computing,
vol. 78, no. 1, pp. 29–63, 2019.

[22] J. D. Lee, M. Simchowitz, M. I. Jordan, and B. Recht, “Gradient
descent only converges to minimizers,” in Conference on learning
theory, 2016, pp. 1246–1257.

[24] MOSEK

[23] J. L¨ofberg, “Yalmip : A toolbox for modeling and optimization in
matlab,” in In Proc. of the CACSD Conference, Taipei, Taiwan, 2004.
for
toolbox
[Online]. Available:

optimization
ApS,
MATLAB manual. Version
2019.
http://docs.mosek.com/9.0/toolbox/index.html

The MOSEK
9.0.,

[25] Y. LeCun, “The MNIST database of handwritten digits,” http://yann.

lecun. com/exdb/mnist/.

