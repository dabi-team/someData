9
1
0
2

n
u
J

1
2

]
L
M

.
t
a
t
s
[

1
v
4
3
2
9
0
.
6
0
9
1
:
v
i
X
r
a

Trade-oﬀs in Large-Scale Distributed Tuplewise
Estimation and Learning

Robin Vogel ((cid:0))1,2, Aur´elien Bellet3, Stephan Cl´emen¸con1, Ons
Jelassi1, and Guillaume Papa1

1Telecom ParisTech, LTCI, Universit´e Paris Saclay, France,
ﬁrst.last@telecom-paristech.fr
2IDEMIA, France, ﬁrst.last@idemia.fr
3INRIA, France, ﬁrst.last@inria.fr

Abstract

The development of cluster computing frameworks has allowed prac-
titioners to scale out various statistical estimation and machine learning
algorithms with minimal programming eﬀort. This is especially true for
machine learning problems whose objective function is nicely separable
across individual data points, such as classiﬁcation and regression.
In
contrast, statistical learning tasks involving pairs (or more generally tu-
ples) of data points — such as metric learning, clustering or ranking —
do not lend themselves as easily to data-parallelism and in-memory com-
puting. In this paper, we investigate how to balance between statistical
performance and computational eﬃciency in such distributed tuplewise
statistical problems. We ﬁrst propose a simple strategy based on occa-
sionally repartitioning data across workers between parallel computation
stages, where the number of repartitioning steps rules the trade-oﬀ be-
tween accuracy and runtime. We then present some theoretical results
highlighting the beneﬁts brought by the proposed method in terms of
variance reduction, and extend our results to design distributed stochas-
tic gradient descent algorithms for tuplewise empirical risk minimization.
Our results are supported by numerical experiments in pairwise statistical
estimation and learning on synthetic and real-world datasets.

Keywords: Distributed Machine Learning · Distributed Data Processing
· U -Statistics · Stochastic Gradient Descent · AUC Optimization

1

Introduction

Statistical machine learning has seen dramatic development over the last decades.
The availability of massive datasets combined with the increasing need to per-
form predictive/inference/optimization tasks in a wide variety of domains has
In
given a considerable boost to the ﬁeld and led to successful applications.
parallel, there has been an ongoing technological progress in the architecture of
data repositories and distributed systems, allowing to process ever larger (and
possibly complex, high-dimensional) data sets gathered on distributed storage

1

 
 
 
 
 
 
platforms. This trend is illustrated by the development of many easy-to-use
cluster computing frameworks for large-scale distributed data processing. These
frameworks implement the data-parallel setting, in which data points are par-
titioned across diﬀerent machines which operate on their partition in parallel.
Some striking examples are Apache Spark [26] and Petuum [25], the latter being
fully targeted to machine learning. The goal of such frameworks is to abstract
away the network and communication aspects in order to ease the deployment
of distributed algorithms on large computing clusters and on the cloud, at the
cost of some restrictions in the types of operations and parallelism that can
be eﬃciently achieved. However, these limitations as well as those arising from
network latencies or the nature of certain memory-intensive operations are often
ignored or incorporated in a stylized manner in the mathematical description
and analysis of statistical learning algorithms (see e.g., [2, 15, 4, 1]). The im-
plementation of statistical methods proved to be theoretically sound may thus
be hardly feasible in a practical distributed system, and seemingly minor ad-
justments to scale-up these procedures can turn out to be disastrous in terms
of statistical performance, see e.g. the discussion in [19]. This greatly restricts
their practical interest in some applications and urges the statistics and machine
learning communities to get involved with distributed computation more deeply
[3].

In this paper, we propose to study these issues in the context of tuplewise
estimation and learning problems, where the statistical quantities of interest
are not basic sample means but come in the form of averages over all pairs (or
more generally, d-tuples) of data points. Such data functionals are known as
U -statistics [20, 16], and many empirical quantities describing global properties
of a probability distribution fall in this category (e.g., the sample variance, the
Gini mean diﬀerence, Kendall’s tau coeﬃcient). U -statistics are also natural
empirical risk measures in several learning problems such as ranking [13], metric
learning [24], cluster analysis [11] and risk assessment [5]. The behavior of these
statistics is well-understood and a sound theory for empirical risk minimization
based on U -statistics is now documented in the machine learning literature [13],
but the computation of a U -statistic poses a serious scalability challenge as
it involves a summation over an exploding number of pairs (or d-tuples) as
the dataset grows in size. In the centralized (single machine) setting, this can
be addressed by appropriate subsampling methods, which have been shown to
achieve a nearly optimal balance between computational cost and statistical
accuracy [12]. Unfortunately, naive implementations in the case of a massive
distributed dataset either greatly damage the accuracy or are ineﬃcient due to a
lot of network communication (or disk I/O). This is due to the fact that, unlike
basic sample means, a U -statistic is not separable across the data partitions.

Our main contribution is to design and analyze distributed methods for sta-
tistical estimation and learning with U -statistics that guarantee a good trade-oﬀ
between accuracy and scalability. Our approach incorporates an occasional data
repartitioning step between parallel computing stages in order to circumvent the
limitations induced by data partitioning over the cluster nodes. The number
of repartitioning steps allows to trade-oﬀ between statistical accuracy and com-
putational eﬃciency. To shed light on this phenomenon, we ﬁrst study the
setting of statistical estimation, precisely quantifying the variance of estimates
corresponding to several strategies. Thanks to the use of Hoeﬀding’s decom-
position [18], our analysis reveals the role played by each component of the

2

variance in the eﬀect of repartitioning. We then discuss the extension of these
results to statistical learning and design eﬃcient and scalable stochastic gradi-
ent descent algorithms for distributed empirical risk minimization. Finally, we
carry out some numerical experiments on pairwise estimation and learning tasks
on synthetic and real-world datasets to support our results from an empirical
perspective.

The paper is structured as follows. Section 2 reviews background on U -
statistics and their use in statistical estimation and learning, and discuss the
common practices in distributed data processing. Section 3 deals with statisti-
cal tuplewise estimation: we introduce our general approach for the distributed
setting and derive (non-)asymptotic results describing its accuracy. Section 4
extends our approach to statistical tuplewise learning. We provide experiments
supporting our results in Section 5, and we conclude in Section 6. Proofs, tech-
nical details and additional results can be found in the supplementary material.

2 Background

In this section, we ﬁrst review the deﬁnition and properties of U -statistics, and
discuss some popular applications in statistical estimation and learning. We
then discuss the recent randomized methods designed to scale up tuplewise
statistical inference to large datasets stored on a single machine. Finally, we
describe the main features of cluster computing frameworks.

2.1 U -Statistics: Deﬁnition and Applications

U -statistics are the natural generalization of i.i.d. sample means to tuples of
points. We state the deﬁnition of U -statistics in their generalized form, where
points can come from K
1 independent samples. Note that we recover classic
≥
sample mean statistics in the case where K = d1 = 1.

∈ {

1 and (d1,
. . . , dK)
∈
≥
1 , . . . , X (k)
, let X{1, ..., nk} = (X (k)
nk ) be an inde-
}
dk composed of i.i.d. random variables with values

Deﬁnition 1 (Generalized U -statistic) Let K
N∗K. For each k
1, . . . , K
pendent sample of size nk
dK
in some measurable space
K →
R be a measurable function, square integrable with respect to the probability dis-
F ⊗dK
tribution µ = F ⊗d1
K . Assume w.l.o.g. that h(x(1), . . . , x(K)) is
dk
symmetric within each block of arguments x(k) (valued in
k ). The generalized
(or K-sample) U -statistic of degrees (d1, . . . , dK) with kernel H is deﬁned as

≥
k with distribution Fk(dx). Let h :
X

d1
1 ×· · ·×X

1 ⊗ · · · ⊗

X

X

Un(h) =

1

(cid:81)K

k=1

(cid:1)

(cid:0)nk
dk

(cid:88)

. . .

(cid:88)

I1

IK

h(X(1)

I1 , X(2)

I2 , . . . , X(K)
IK ),

(1)

where (cid:80)
related to a set Ik of dk indexes 1

denotes the sum over all (cid:0)nk
dk

Ik

(cid:1) subsets X(k)

. . . , X (k)
Ik = (X (k)
)
i1 ,
idk
nk and n = (n1, . . . , nK).

i1 < . . . < idk ≤

≤

The U -statistic Un(h) is known to have minimum variance among all unbiased
estimators of the parameter µ(h) = E(cid:2)h(X (1)
, . . . , X (K)
The price to pay for this low variance is a complex dependence structure ex-
hibited by the terms involved in the average (1), as each data point appears in

d1 , . . . , X (K)

1 , . . . , X (1)

1

dK )(cid:3).

3

multiple tuples. The (non)asymptotic behavior of U -statistics and U -processes
(i.e., collections of U -statistics indexed by classes of kernels) can be investigated
by means of linearization techniques [18] combined with decoupling methods
[16], reducing somehow their analysis to that of basic i.i.d. averages or empiri-
cal processes. One may refer to [20] for an account of the asymptotic theory of
U -statistics, and to [23] (Chapter 12 therein) and [16] for nonasymptotic results.
U -statistics are commonly used as point estimators for inferring certain
global properties of a probability distribution as well as in statistical hypothesis
testing. Popular examples include the (debiased) sample variance, obtained by
x2)2, the Gini mean diﬀerence,
setting K = 1, d1 = 2 and h(x1, x2) = (x1
, and Kendall’s tau rank correla-
where K = 1, d1 = 2 and h(x1, x2) =
tion, where K = 2, d1 = d2 = 1 and h((x1, y1), (x2, y1)) = I
y2) >
0
}

U -statistics also correspond to empirical risk measures in statistical learning
problems such as clustering [11], metric learning [24] and multipartite rank-
ing [14]. The generalization ability of minimizers of such criteria over a class
of kernels can be derived from probabilistic upper bounds for the maximal
H
deviation of collections of centered U -statistics under appropriate complexity
(e.g., ﬁnite VC dimension) [13, 12]. Below, we describe the
conditions on
example of multipartite ranking used in our numerical experiments (Section 5).
We refer to [12] for details on more learning problems involving U -statistics.

(x1
{

−
x2
|

x2)

(y1

x1

H

−

−

−

.

|

·

≥

∈ X

Example 1 (Multipartite Ranking) Consider items described by a random
vector of features X
, where
with associated ordinal labels Y
}
2. The goal of multipartite ranking is to learn to rank items in the same
K
preorder as that deﬁned by the labels, based on a training set of labeled examples.
R transport-
Rankings are generally deﬁned through a scoring function s :
ing the natural order on the real line onto
. Given K independent samples,
the empirical ranking performance of s(x) is evaluated by means of the empirical
VUS (Volume Under the ROC Surface) criterion [14]:

1, . . . , K

X →

∈ {

X

(cid:91)V U S(s) =

1
(cid:81)K
k=1 nk

n1(cid:88)

. . .

nK(cid:88)

i1=1

iK =1

I
{

s(X (1)

i1 ) < . . . < s(X (K)
,
iK )
}

(2)

which is a K-sample U -statistic of degree (1, . . . , 1) with kernel hs(x1, . . . , xK) =
I
{

.
s(x1) < . . . < s(xK)
}

2.2 Large-Scale Tuplewise Inference with Incomplete U -

statistics

(cid:1)

The cost related to the computation of the U -statistic (1) rapidly explodes as
the sizes of the samples increase. Precisely, the number of terms involved in
the summation is (cid:0)n1
(cid:1), which is of order O(nd1+...+dK ) when the
d1
nk’s are all asymptotically equivalent. Whereas computing U -statistics based
on subsamples of smaller size would severely increase the variance of the estima-
tion, the notion of incomplete generalized U -statistic [6] enables to signiﬁcantly
mitigate this computational problem while maintaining a good level of accuracy.

× · · · ×

(cid:0)nK
dK

4

Deﬁnition 2 (Incomplete generalized U -statistic) Let B
complete version of the U -statistic (1) based on B terms is deﬁned by:

≥

1. The in-

(cid:101)UB(H) =

1
B

(cid:88)

h(X(1)

I1 , . . . , X(K)
IK )

(3)

I=(I1, ..., IK )∈DB

D

B is a set of cardinality B built by sampling uniformly with replacement
dK )), where

where
in the set Λ of vectors of tuples ((i(1)
1

1 , . . . , i(1)
K.
k

d1 ), . . . , (i(K)

, . . . , i(K)

1 < . . . < i(k)
i(k)

nk and 1

1

≤

dk ≤

≤

≤

Note incidentally that the subsets of indices can be selected by means of other
sampling schemes [12], but sampling with replacement is often preferred due
to its simplicity. In practice, the parameter B should be picked much smaller
than the total number of tuples to reduce the computational cost. Like (1),
the quantity (3) is an unbiased estimator of µ(H) but its variance is naturally
larger:

Var( (cid:101)UB(h)) =

(cid:16)
1

(cid:17)

1
B

−

Var(Un(h)) +

1
B

Var(h(X (1)

1 , . . . , X (K)

dK )).

(4)

H

(cid:112)

The recent work in [12] has shown that the maximal deviations between (1)
of controlled complexity decrease at a rate
and (3) over a class of kernels
of order O(1/√B) as B increases. An important consequence of this result
is that sampling B = O(n) terms is suﬃcient to preserve the learning rate of
order OP(
log n/n) of the minimizer of the complete risk (1), whose computa-
tion requires to average O(nd1+...+dK ) terms. In contrast, the distribution of a
complete U -statistic built from subsamples of reduced sizes n(cid:48)
k drawn uniformly
at random is quite diﬀerent from that of an incomplete U -statistic based on
(cid:1) terms sampled with replacement in Λ, although they involve
B = (cid:81)K
the summation of the same number of terms. Empirical minimizers of such a
complete U -statistic based on subsamples achieve a much slower learning rate of
log(n)/n1/(d1+...+dK )). We refer to [12] for details and additional results.
OP(
We have seen that approximating complete U -statistics by incomplete ones
is a theoretically and practically sound approach to tackle large-scale tuplewise
estimation and learning problems. However, as we shall see later, the implemen-
tation is far from straightforward when data is stored and processed in standard
distributed computing frameworks, whose key features are recalled below.

(cid:0)n(cid:48)
k
dk

k=1

(cid:112)

2.3 Practices in Distributed Data Processing

Data-parallelism, i.e. partitioning the data across diﬀerent machines which op-
erate in parallel, is a natural approach to store and eﬃciently process massive
datasets. This strategy is especially appealing when the key stages of the com-
putation to be executed can be run in parallel on each partition of the data.
As a matter of fact, many estimation and learning problems can be reduced
to (a sequence of) local computations on each machine followed by a simple
aggregation step. This is the case of gradient descent-based algorithms applied
to standard empirical risk minimization problems, as the objective function is
nicely separable across individual data points. Optimization algorithms oper-
ating in the data-parallel setting have indeed been largely investigated in the

5

machine learning community, see [3, 8, 1, 22] and references therein for some
recent work.

Because of the prevalence of data-parallel applications in large-scale ma-
chine learning, data analytics and other ﬁelds, the past few years have seen a
sustained development of distributed data processing frameworks designed to
facilitate the implementation and the deployment on computing clusters. Be-
sides the seminal MapReduce framework [17], which is not suitable for iterative
computations on the same data, one can mention Apache Spark [26], Apache
Flink [10] and the machine learning-oriented Petuum [25]. In these frameworks,
the data is typically ﬁrst read from a distributed ﬁle system (such as HDFS,
Hadoop Distributed File System) and partitioned across the memory of each
machine in the form of an appropriate distributed data structure. The user can
then easily specify a sequence of distributed computations to be performed on
this data structure (map, ﬁlter, reduce, etc.) through a simple API which hides
low-level distributed primitives (such as message passing between machines).
Importantly, these frameworks natively implement fault-tolerance (allowing ef-
ﬁcient recovery from node failures) in a way that is also completely transparent
to the user.

While such distributed data processing frameworks come with a lot of ben-
eﬁts for the user, they also restrict the type of computations that can be per-
formed eﬃciently on the data. In the rest of this paper, we investigate these
limitations in the context of tuplewise estimation and learning problems, and
propose solutions to achieve a good trade-oﬀ between accuracy and scalability.

3 Distributed Tuplewise Statistical Estimation

In this section, we focus on the problem of tuplewise statistical estimation in the
distributed setting (an extension to statistical learning is presented in Section 4).
1 workers in a complete network graph (i.e., any pair
We consider a set of N
of workers can exchange messages). For convenience, we assume the presence of
a master node, which can be one of the workers and whose role is to aggregate
estimates computed by all workers.

≥

Q

∈ {

1, . . . , N

Z1, . . . , Zm
{

For ease of presentation, we restrict our attention to the case of two sample
U -statistics of degree (1, 1) (K = 2 and d1 = d2 = 1), see Remark 1 in Sec-
n =
tion 3.3 for extensions to the general case. We denote by
}
the ﬁrst sample and by
the second sample (of sizes n and
m =
m respectively). These samples are distributed across the N workers. For
i the subset of data points held by worker i
i
and, unless otherwise noted, we assume for simplicity that all subsets are of
Z
i respectively denote the
equal size
N ∈
Z
X
i.
m, with
subset of data points held by worker i from
i =
i ∪ R
R
R
Z
X
.
and mi =
We denote their (possibly random) cardinality by ni =
i |
i |
Given a kernel h, the goal is to compute a good estimate of the parameter
U (h) = E[h(X1, Z1)] while meeting some computational and communication
constraints.

, we denote by
}
= n+m

R
N. The notations

X
i and
n and

X1, . . . , Xn

R
Q

i
|R

|R

|R

R

D

D

}

{

|

6

3.1 Naive Strategies

Before presenting our approach, we start by introducing two simple (but inef-
fective) strategies to compute an estimate of U (h). The ﬁrst one is to compute
n and
the complete two-sample U -statistic associated with the full samples

D

m:

Q

Un(h) =

1
nm

n
(cid:88)

m
(cid:88)

k=1

l=1

h(Xk, Zl),

(5)

with n = (n, m). While Un(h) has the lowest variance among all unbiased
estimates that can be computed from (
m), computing it is a highly unde-
D
sirable solution in the distributed setting where each worker only has access to
a subset of the dataset. Indeed, ensuring that each possible pair is seen by at
least one worker would require massive data communication over the network.
Note that a similar limitation holds for incomplete versions of (5) as deﬁned in
Deﬁnition 2.

n,

Q

A feasible strategy to go around this problem is for each worker to compute
the complete U -statistic associated with its local subsample
i, and to send it
to the master node who averages all contributions. This leads to the estimate

R

Un,N (h) =

1
N

N
(cid:88)

i=1

URi(h) where URi(h) =

1
nimi

(cid:88)

(cid:88)

k∈RX
i

l∈RZ
i

h(Xk, Zl).

(6)

Note that if min(ni, mi) = 0, we simply set URi(h) = 0.

Alternatively, as the

i’s may be large, each worker can compute an incom-
plete U -statistic (cid:101)UB,Ri(h) with B terms instead of URi, leading to the estimate

R

(cid:101)Un,N,B(h) =

1
N

N
(cid:88)

i=1

(cid:101)UB,Ri(h) where (cid:101)UB,Ri(h) =

1
B

(cid:88)

(k,l)∈Ri,B

h(Xk, Zl),

(7)

i,B a set of B pairs built by sampling uniformly with replacement from

with
the local subsample

R

X
i × R

Z
i .

R

As shown in Section 3.3, strategies (6) and (7) have the undesirable prop-
erty that their accuracy decreases as the number of workers N increases. This
motivates our proposed approach, introduced in the following section.

3.2 Proposed Approach

The naive strategies presented above are either accurate but very expensive
(requiring a lot of communication across the network), or scalable but poten-
tially inaccurate. The approach we promote here is of disarming simplicity and
aims at ﬁnding a sweet spot between these two extremes. The idea is based on
repartitioning the dataset a few times across workers (we keep the repartition-
ing scheme abstract for now and postpone the discussion of concrete choices to
subsequent sections). By alternating between parallel computation and repar-
titioning steps, one considers several estimates based on the same data points.
This allows to observe a greater diversity of pairs and thereby reﬁne the quality
of our ﬁnal estimate, at the cost of some additional communication.

7

Figure 1: Graphical summary of the statistics that we compare: with/without
T
repartition and with/without subsampling. Note that
t=1 denotes a
set of T independent couples of random permutations in Sn

(σt, πt)
}
Sm.

{

×

Formally, let T be the number of repartitioning steps. We denote by
the subsample of worker i after the t-th repartitioning step, and by URt
the complete U -statistic associated with
worker i computes URt
master node has access to the following estimate:

t
i
R
(h)
i
t
, each
i. At each step t
}
R
(h) and sends it to the master node. After T steps, the

1, . . . , T

∈ {

i

(cid:98)Un,N,T (h) =

1
T

T
(cid:88)

t=1

U t

n,N (h),

(8)

(cid:80)N

n,N (h) = 1
N

where U t
tively compute incomplete U -statistics (cid:101)UB,Rt
then:

i=1 URt

i

i

(h). Similarly as before, workers may alterna-

(h) with B terms. The estimate is

(cid:101)Un,N,B,T (h) =

1
T

T
(cid:88)

t=1

(cid:101)U t

n,N,B(h),

(9)

where (cid:101)U t
Section 3.1 which do not rely on repartition, are summarized in Figure 1.

(h). These statistics, and those introduced in

n,N,B(h) = 1
N

i=1 (cid:101)UB,Rt

i

(cid:80)N

Of course, the repartitioning operation is rather costly in terms of runtime
so T should be kept to a reasonably small value. We illustrate this trade-oﬀ by
the analysis presented in the next section.

3.3 Analysis

In this section, we analyze the statistical properties of the various estimators
introduced above. We focus here on repartitioning by proportional sampling
without replacement (prop-SWOR). Prop-SWOR creates partitions that contain
the same proportion of elements of each sample: speciﬁcally, it ensures that at
= m
= n+m
N .
any step t and for any worker i,
We discuss the practical implementation of this repartitioning scheme as well
as some alternative choices in Section 3.4.

t,X
i
|R

N with

t,Z
i
|R

N and

= n

t
i|

|R

|

|

All estimators are unbiased when repartitioning is done with prop-SWOR.
We will thus compare their variance. Our main technical tool is a linearization
technique for U -statistics known as Hoeﬀding’s Decomposition (see [18, 13, 12]).

Deﬁnition 3 (Hoeffding’s decomposition) Let h1(x) = E[h(x, Z1)], h2(z) =
E[h(X1, z)] and h0(x, z) = h(x, z)
U (h) can be
h1(x)

h2(z) + U (h). Un(h)

−

−

−

8

written as a sum of three orthogonal terms:

Un(h)

−

U (h) = Tn(h) + Tm(h) + Wn(h),

(cid:80)n

k=1 h1(Xk)

where Tn(h) = 1
n
are sums of independent r.v, while Wn(h) = 1
nm
degenerate U -statistic (i.e., E[h(X1, Z1)
|
U (h)).

−
k=1
X1] = U (h) and E[h(X1, Z1)

U (h)
l=1 h2(Zl)
l=1 h0(Xk, Zl) is a
Z1] =
|

U (h) and Tm(h) = 1
m

(cid:80)n

−

(cid:80)n
(cid:80)m

This decomposition is very convenient as the two terms Tn(h) and Tm(h)
are decorrelated and the analysis of Wn(h) (a degenerate U -statistic) is well
It will allow us to decompose the variance of the
documented [18, 13, 12].
estimators of interest into single-sample components σ2
2 =
Var(h2(Z)) on the one hand, and a pairwise component σ2
0 = Var(h0(X1, Z1))
1 + σ2
0 + σ2
on the other hand. Denoting σ2 = Var(h(X1, Z1)), we have σ2 = σ2
2.
It is well-known that the variance of the complete U -statistic Un(h) can be
written as Var(Un(h)) = σ2
nm (see supplementary material for details).
Our ﬁrst result gives the variance of the estimators which do not rely on a
repartitioning of the data with respect to the variance of Un(h).

1 = Var(h1(X)) and σ2

m + σ2

n + σ2

1

0

2

Theorem 1 If the data is distributed over workers using prop-SWOR, we have:

Var(Un,N (h)) = Var(Un(h)) + (N
(cid:18)
1

Var( (cid:101)Un,N,B(h)) =

(cid:19)

1
B

−

Var(Un,N (h)) +

σ2
0
nm

,

1)

−

σ2
N B

.

Theorem 1 precisely quantiﬁes the excess variance due to the distributed
setting if one does not use repartitioning. Two important observations are in
order. First, the variance increase is proportional to the number of workers N ,
which clearly defeats the purpose of distributed processing. Second, this increase
only depends on the pairwise component σ2
0 of the variance. In other words, the
average of U -statistics computed independently over the local partitions contains
all the information useful to estimate the single-sample contributions, but fails
to accurately estimate the pairwise contributions. The resulting estimates thus
lead to signiﬁcantly larger variance when the choice of kernel and the data
distributions imply that σ2
2 and/or σ2
0 is large compared to σ1
1. The extreme
case happens when Un(h) is a degenerate U -statistic, i.e. σ2
1 = σ2
2 = 0 and
σ2
z and X, Z are both
0 > 0, which is veriﬁed for example when h(x, z) = x
centered random variables.

·

We now characterize the variance of the estimators that leverage data repar-

titioning steps.

Theorem 2 If the data is distributed and repartitioned between workers using
prop-SWOR, we have:

Var( (cid:98)Un,N,T (h)) = Var(Un(h)) + (N

Var( (cid:101)Un,N,B,T (h)) = Var( (cid:98)Un,N,T (h))

−

σ2
0
nmT

,

Var(Un,N (h)) +

σ2
N T B

.

1)

−

1
T B

9

Figure 2: Theoretical variance as a function of the number of evaluated pairs
for diﬀerent estimators under prop-SWOR, with n = 100, 000, m = 200 and
N = 100.

T ).

Theorem 2 shows that the value of repartitioning arises from the fact that
the term accounting for the pairwise variance in (cid:98)Un,N,T (h) is T times lower
than that of Un,N (h). This validates the fact that repartitioning is beneﬁcial
when the pairwise variance term is signiﬁcant in front of the other terms. In-
terestingly, Theorem 2 also implies that for a ﬁxed budget of evaluated pairs,
using all pairs on each worker is always a dominant strategy over using in-
complete approximations. Speciﬁcally, we can show that under the constraint
N BT = nmT0/N , Var( (cid:98)Un,N,T0 (h)) is always smaller than Var( (cid:101)Un,N,B,T (h)), see
supplementary material for details. Note that computing complete U -statistics
also require fewer repartitioning steps to evaluate the same number of pairs (i.e.,
T0

≤
We conclude the analysis with a visual illustration of the variance of various
estimators with respect to the number of pairs they evaluate. We consider the
imbalanced setting where n
m, which is commonly encountered in applica-
(cid:29)
tions such as imbalanced classiﬁcation, bipartite ranking and anomaly detection.
In this case, it suﬃces that σ2
2 be small for the inﬂuence of the pairwise compo-
nent of the variance to be signiﬁcant, see Fig. 2 (left). The ﬁgure also conﬁrms
that complete estimators dominate their incomplete counterparts. On the other
hand, when σ2
2 is not small, the variance of Un mostly originates from the rarity
of the minority sample, hence repartitioning does not provide estimates that
are signiﬁcantly more accurate (see Fig. 2, right). We refer to Section 5 for
experiments on concrete tasks with synthetic and real data.

Remark 1 (Extension to high-order U -statistics) The extension of our anal-
ysis to general U -statistics is straightforward and left to the reader (see [12] for
a review of the relevant technical tools). We stress the fact that the beneﬁts
of repartitioning are even stronger for higher-order U -statistics (K > 2 and/or
larger degrees) because higher-order components of the variance are also aﬀected.

3.4 Practical Considerations and Other Repartitioning Schemes

The analysis above assumes that repartitioning is done using prop-SWOR, which
has the advantage of exactly preserving the proportion of points from the two
m even in the event of signiﬁcant imbalance in their size.
samples
However, a naive implementation of prop-SWOR requires some coordination
between workers at each repartitioning step. To avoid exchanging many mes-
sages, we propose that the workers agree at the beginning of the protocol on

n and

Q

D

10

a numbering of the workers, a numbering of the points in each sample, and
a random seed to use in a pseudorandom number generator. This allows the
workers to implement prop-SWOR without any further coordination: at each
repartitioning step, they independently draw the same two random permuta-
using the common random seed and use
tions over
these permutations to assign each point to a single worker.

1, . . . , m
{

1, . . . , n
{

and

}

}

Of course, other repartitioning schemes can be used instead of prop-SWOR.
A natural choice is sampling without replacement (SWOR), which does not re-
quire any coordination between workers. However, the partition sizes generated
by SWOR are random. This is a concern in the case of imbalanced samples,
where the probability that a worker i does not get any point from the minor-
ity sample (and thus no pair to compute a local estimate) is non-negligible.
For these reasons, it is diﬃcult to obtain exact and concise theoretical vari-
ances for the SWOR case, but we show in the supplementary material that
the results with SWOR should not deviate too much from those obtained with
prop-SWOR. For completeness, in the supplementary material we also analyze
the case of proportional sampling with replacement (prop-SWR): results are
quantitatively similar, aside from the fact that redistribution also corrects for
the loss of information that occurs because of sampling with replacement.

Finally, we note that deterministic repartitioning schemes may be used in
practice for simplicity. For instance, the repartition method in Apache Spark
relies on a deterministic shuﬄe which preserves the size of the partitions.

4 Extensions to Stochastic Gradient Descent for

ERM

The results of Section 3 can be extended to statistical learning in the empir-
In such problems, given a class of kernels
ical risk minimization framework.
, one seeks the minimizer of (6) or (8) depending on whether repartition is
H
(e.g., of ﬁnite VC di-
used.1 Under appropriate complexity assumptions on
mension), excess risk bounds for such minimizers can be obtained by combining
our variance analysis of Section 3 with the control of maximal deviations based
on Bernstein-type concentration inequalities as done in [13, 12]. Due to the lack
of space, we leave the details of such analysis to the readers and focus on the
more practical scenario where the ERM problem is solved by gradient-based
optimization algorithms.

H

4.1 Gradient-based Empirical Minimization of U -statistics

In the setting of interest, the class of kernels to optimize over is indexed by a
Rq representing the model. Adapting the notations
real-valued parameter θ
∈
R then measures the performance
of Section 3, the kernel h :
Rq on a given pair, and is assumed to be convex and smooth in
of a model θ

2
× X

1
X

Rq

→

×

∈

1Alternatively, for scalability purposes, one may instead work with their incomplete coun-

terparts, namely (7) and (9) respectively.

11

θ. Empirical Risk Minimization (ERM) aims at ﬁnding θ

Rq minimizing

∈

Un(θ) =

1
nm

n
(cid:88)

m
(cid:88)

k=1

l=1

h(Xk, Zl; θ).

(10)

The minimizer can be found by means of Gradient Descent (GD) techniques.2
Rq and given a learning
Starting at iteration s = 1 from an initial model θ1
rate γ > 0, GD consists in iterating over the following update:

∈

θs+1 = θs

γ

∇

−

θUn(θs).

(11)

∇

θUn(θ) is itself a U -statistic with kernel given by

Note that the gradient
θH,
and its computation is very expensive in the large-scale setting. In this regime,
Stochastic Gradient Descent (SGD) is a natural alternative to GD which is
known to provide a better trade-oﬀ between the amount of computation and the
performance of the resulting model [7]. Following the discussion of Section 2.2,
a natural idea to implement SGD is to replace the gradient
θUn(θ) in (11)
by an unbiased estimate given by an incomplete U -statistic. The work of [21]
shows that SGD converges much faster than if the gradient is estimated using
a complete U -statistic based on subsamples with the same number of terms.

∇

∇

However, as in the case of estimation, the use of standard complete or incom-
plete U -statistics turns out to be impractical in the distributed setting. Building
upon the arguments of Section 3, we propose a more suitable strategy.

4.2 Repartitioning for Stochastic Gradient Descent

The approach we propose is to alternate between SGD steps using within-
partition pairs and repartitioning the data across workers. We introduce a pa-
Z+ corresponding to the number of iterations of SGD between each
rameter nr
redistribution of the data. For notational convenience, we let r(s) :=
s/nr
(cid:101)
1 of
denotes its data partition at iteration s
so that for any worker i,
SGD.

r(s)
i
R

(cid:100)
≥

∈

Given a local batch size B, at each iteration s of SGD, we propose to adapt
the strategy (9) by having each worker i compute a local gradient estimate using
s
i,B of B randomly sampled pairs in its current local partition
a set
R

r(s)
i

R

:

θ (cid:101)UB,Rr(s)

i

(θs) =

∇

1
B

(cid:88)

(k,l)∈Rs

i,B

∇

θh(Xk, Zl; θs).

This local estimate is then sent to the master node who averages all contribu-
tions, leading to the following global gradient estimate:

θ (cid:101)Un,N,B(θs) =

∇

1
N

N
(cid:88)

∇

i=1

θ (cid:101)UB,Rr(s)

i

(θs).

(12)

The master node then takes a gradient descent step as in (11) and broadcasts
the updated model θs+1 to the workers.

Following our analysis in Section 3, repartitioning the data allows to reduce
the variance of the gradient estimates, which is known to greatly impact the

2When H is nonsmooth in θ, a subgradient may be used instead of the gradient.

12

Figure 3: Relative variance estimated over 5000 runs, n = 5000, m = 50, N = 10
and T = 4. Results are divided by the true variance of Un deduced from (13)
and Theorem 1.

Figure 4: Learning dynamics for diﬀerent repartition frequencies computed over
100 runs.

convergence rate of SGD (see e.g. [9], Theorem 6.3 therein). When nr = +
,
∞
data is never repartitioned and the algorithm minimizes an average of local
U -statistics, leading to suboptimal performance. On the other hand, nr = 1
corresponds to repartitioning at each iteration of SGD, which minimizes the
variance but is very costly and makes SGD pointless. We expect the sweet
spot to lie between these two extremes: the dominance of (cid:98)Un,N,T over (cid:101)Un,N,B,T
established in Section 3.3, combined with the common use of small batch size
B in SGD, suggests that occasional redistributions are suﬃcient to correct for
the loss of information incurred by the partitioning of data. We illustrate these
trade-oﬀs experimentally in the next section.

5 Numerical Results

In this section, we illustrate the importance of repartitioning for estimating and
optimizing the Area Under the ROC Curve (AUC) through a series of numerical
experiments. The corresponding U -statistic is the two-sample version of the
multipartite ranking VUS introduced in Example 1 (Section 2.1). The ﬁrst
experiment focuses on the estimation setting considered in Section 3. The second
experiment shows that redistributing the data across workers, as proposed in
Section 4, allows for more eﬃcient mini-batch SGD. All experiments use prop-
SWOR and are conducted in a simulated environment.

13

10−310−210−1(cid:15)123Rel.var.UnUn,NUn,N,TEstimation experiment. We seek to illustrate the importance of redistri-
bution for estimating two-sample U -statistics with the concrete example of the
AUC. The AUC is obtained by choosing the kernel h(x, z) = I
, and is
z < x
}
{
widely used as a performance measure in bipartite ranking and binary classiﬁca-
tion with class imbalance. Recall that our results of Section 3.3 highlighted the
key role of the pairwise component of the variance σ2
0 being large compared to
the single-sample components. In the case of the AUC, this happens when the
data distributions are such that the expected outcome using single-sample in-
formation is far from the truth, e.g. in the presence of hard pairs. We illustrate
this on simple discrete distributions for which we can compute σ2
2 in
closed form. Consider positive points X
1, +1
}
and P (X = 2) = q, P (Z = +1) = p. It follows that:

, negative points Z
0, 2
}

1 and σ2

0, σ2

∈ {−

∈ {

1 = p2q(1
σ2

q),

−

σ2
2 = (1

−

q)2p(1

−

p), and σ2 = p(1

p + pq)(1

q). (13)

−

−

σ2
0
1 +σ2
σ2
2

Assume that the scoring function has a small probability (cid:15) to assign a low score
to a positive instance or a large score to a negative instance.
In our formal
q = (cid:15) for a small (cid:15) > 0, which implies
setting, this translates into letting p = 1
(cid:15)2
that
gets closer to 1, repartitioning the dataset becomes more critical to achieve good
relative precision. This is conﬁrmed numerically, as shown in Fig. 3. Note that
in practice, settings where the AUC is very close to 1 are very common as they
correspond to well-functioning systems, such as face recognition systems.

. We thus expect that as the true AUC U (h) = 1

2(cid:15) →(cid:15)→0 ∞

= 1−(cid:15)

−

−

Learning experiment. We now turn to AUC optimization, which is the
R that optimizes the VUS criterion
task of learning a scoring function s :
X →
(2) with K = 2 in order to discriminate between a negative and a positive
class. We learn a linear scoring function sw,b(x) = w(cid:62)x + b, and optimize a
continuous and convex surrogate of (2) based on the hinge loss. The resulting
loss function to minimize is a two-sample U-statistic with kernel gw,b(x, z) =
max(0, 1 + sw,b(x)
sw,b(z)) indexed by the parameters (w, b) of the scoring
function, to which we add a small L2 regularization term of 0.05

−

We use the shuttle dataset, a classic dataset for anomaly detection.3
It
contains roughly 49,000 points in dimension 9, among which only 7% (approx.
3,500) are anomalies. A high accuracy is expected for this dataset. To monitor
the generalization performance, we keep 20% of the data as our test set, cor-
responding to 700 points of the minority class and approx. 9,000 points of the
majority class. The test performance is measured with complete statistics over
the 6.3 million pairs. The training set consists of the remaining data points,
which we distribute over N = 100 workers. This leads to approx. 10, 200 pairs
per worker. The gradient estimates are calculated following (12) with batch
size B = 100. We use an initial learning rate of 0.01 with a momentum of 0.9.
As there are more than 100 million possible pairs in the training dataset, we
105 randomly
monitor the training loss and accuracy on a ﬁxed subset of 4.5
sampled pairs.

×

w
(cid:107)

2
2.
(cid:107)

Fig. 4 shows the evolution of the continuous loss and the true AUC on the
training and test sets along the iteration for diﬀerent values of nr, from nr = 1
(no repartition). The lines are
(repartition at each iteration) to nr = +

∞

3http://odds.cs.stonybrook.edu/shuttle-dataset/

14

the median at each iteration over 100 runs, and the shaded area correspond
to conﬁdence intervals for the AUC and loss value of the testing dataset. We
can clearly see the beneﬁts of repartition: without it, the median performance
is signiﬁcantly lower and the variance across runs is very large. The results
also show that occasional repartitions (e.g., every 25 iterations) are suﬃcient to
mitigate these issues signiﬁcantly.

6 Future Work

We envision several further research questions on the topic of distributed tuple-
wise learning. We would like to provide a rigorous convergence rate analysis of
the general distributed SGD algorithm introduced in Section 4. This is a chal-
lenging task because each series of iterations executed between two repartition
steps can be seen as optimizing a slightly diﬀerent objective function. It would
also be interesting to investigate settings where the workers hold sensitive data
that they do not want to share in the clear due to privacy concerns.

References

[1] Y. Arjevani and O. Shamir. Communication complexity of distributed

convex learning and optimization. In NIPS, 2015.

[2] M.-F. Balcan, A. Blum, S. Fine, and Y. Mansour. Distributed Learning,

Communication Complexity and Privacy. In COLT, 2012.

[3] R. Bekkerman, M. Bilenko, and J. Langford. Scaling Up Machine Learning:
Parallel and Distributed Approaches. Cambridge University Press, 2011.

[4] A. Bellet, Y. Liang, A. B. Garakani, M.-F. Balcan, and F. Sha. A
Distributed Frank-Wolfe Algorithm for Communication-Eﬃcient Sparse
Learning. In SDM, 2015.

[5] P. Bertail and J. Tressou. Incomplete generalized U -statistics for food risk

assessment. Biometrics, 62(1):66–74, 2006.

[6] G. Blom.

Some properties of

incomplete U -statistics.

Biometrika,

63(3):573–580, 1976.

[7] L. Bottou and O. Bousquet. The Tradeoﬀs of Large Scale Learning. In

NIPS, 2007.

[8] S. P. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed Op-
timization and Statistical Learning via the Alternating Direction Method
of Multipliers. Foundations and Trends in Machine Learning, 3(1):1–122,
2011.

[9] S. Bubeck. Convex Optimization: Algorithms and Complexity. Founda-

tions and Trends in Machine Learning, 8(3–4):231–357, 2015.

[10] P. Carbone, A. Katsifodimos, S. Ewen, V. Markl, S. Haridi, and
K. Tzoumas. Apache FlinkTM: Stream and Batch Processing in a Sin-
gle Engine. IEEE Data Engineering Bulletin, 38(4):28–38, 2015.

15

[11] S. Cl´emen¸con. A statistical view of clustering performance through the
theory of U-processes. Journal of Multivariate Analysis, 124:42–56, 2014.

[12] S. Cl´emen¸con, A. Bellet, and I. Colin. Scaling-up Empirical Risk Min-
imization: Optimization of Incomplete U-statistics. Journal of Machine
Learning Research, 13:165–202, 2016.

[13] S. Cl´emen¸con, G. Lugosi, and N. Vayatis. Ranking and empirical risk
minimization of U -statistics. The Annals of Statistics, 36(2):844–874, 2008.

[14] S. Cl´emen¸con and S. Robbiano. Building conﬁdence regions for the ROC

surface. Pattern Recognition Letters, 46:67–74, 2014.

[15] H. Daum´e III, J. M. Phillips, A. Saha, and S. Venkatasubramanian. Pro-
tocols for Learning Classiﬁers on Distributed Data. In AISTATS, 2012.

[16] V. de la Pena and E. Gin´e. Decoupling: from Dependence to Independence.

Springer, 1999.

[17] J. Dean and S. Ghemawat. Mapreduce: simpliﬁed data processing on large

clusters. Communications of the ACM, 51(1):107–113, 2008.

[18] W. Hoeﬀding. A class of statistics with asymptotically normal distribution.

Annals of Mathematics and Statistics, 19:293–325, 1948.

[19] M. Jordan. On statistics, computation and scalability.

Bernoulli,

19(4):1378–1390, 2013.

[20] A. Lee. U -statistics: Theory and practice. Marcel Dekker, Inc., New York,

1990.

[21] G. Papa, A. Bellet, and S. Cl´emen¸con. SGD Algorithms based on Incom-
plete U-statistics: Large-Scale Minimization of Empirical Risk. In NIPS,
2015.

[22] V. Smith, S. Forte, C. Ma, M. Tak´ac, M. I. Jordan, and M. Jaggi. CoCoA:
A General Framework for Communication-Eﬃcient Distributed Optimiza-
tion. Journal of Machine Learning Research, 18(230):1–49, 2018.

[23] A. Van Der Vaart. Asymptotic Statistics. Cambridge University Press,

2000.

[24] R. Vogel, A. Bellet, and S. Cl´emen¸con. A Probabilistic Theory of Su-
pervised Similarity Learning for Pointwise ROC Curve Optimization. In
ICML, 2018.

[25] E. P. Xing, Q. Ho, W. Dai, J. K. Kim, J. Wei, S. Lee, X. Zheng, P. Xie,
A. Kumar, and Y. Yu. Petuum: A New Platform for Distributed Machine
Learning on Big Data. IEEE Transactions on Big Data, 1(2):49–67, 2015.

[26] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and I. Stoica. Spark

: Cluster Computing with Working Sets. In HotCloud, 2012.

16

SUPPLEMENTARY MATERIAL

The code of the experiments can be found on the authors’ repository.4

A Acknowledgments

This work was supported by IDEMIA. We would like to thank Anne Sabourin
for her feedback that helped improve this work, as well as the ECML PKDD
reviewers for their constructive input.

B Proof of Theorem 1

First, consider Var(Un,N ). Hoeﬀding’s decomposition implies that:

Un,N (h)

−

U (h) = Tn(h) + Tm(h) +

1
N

N
(cid:88)

k=1

1
n0m0

(cid:88)

(cid:88)

i∈RX
k

j∈RZ
k

h0(Xi, Zj),

as well as the following properties,

k, l

1, . . . , n

} × {

1, . . . , m

,
}

∀

∈ {
Cov(h1(Xk), h2(Zl)) = 0,
Cov(h1(Xj), h0(Xk, Zl)) = 0,
Cov(h2(Zj), h0(Xk, Zl)) = 0,

,

(14)

j
∀
j
∀

∈ {

∈ {

1, . . . , n

}
1, . . . , m

,
}

which imply the result. The variance of the complete U-statistic Un is just the
special case N = 1 of the variance Un,N . Explicitely,

Var(Un,N (h)) =

σ2
1
n

+

σ2
2
m

+

N σ2
0
nm

.

Now for (cid:101)Un,N,B(h), since (cid:101)Un,N,B conditioned upon the data has expectation
Un,N (h), i.e.

E

(cid:104)
(cid:101)Un,N,B(h)

n,

m, (

= Un,N (h),

(cid:105)

k)N

k=1

|D
the law of total variance implies,
Var( (cid:101)Un,N,B(h)) = Var(Un,N (h)) + E[Var( (cid:101)Un,N,B(h)

R

Q

= Var(Un,N (h)) +

1
N

|D
E[Var( (cid:101)UR1,B(h)

n,

m, (

Q
n,

R
m, (

|D

Q

R

k)N

k=1)],

k)N

k=1)],

(Since the draws of B pairs on diﬀerent workers are independent)

= Var(Un,N (h)) +

1
N

(cid:20)

1
B

−

Var(UR1 ) +

(cid:21)

Var(h(X, Z))

,

1
B

(See [12])
(cid:18)

=

1

−

(cid:19)

Var(Un,N (h)) +

1
N B

Var(h(X, Z)),

1
B

which concludes our proof. Explicitly,
(cid:19) (cid:18) σ2
1
n

Var( (cid:101)Un,N,B(h)) =

1
B

(cid:18)

1

−

(cid:19)

+

σ2
2
m

+

N σ2
0
nm

+

1
N B

Var (h(X, Z)) .

4 https://github.com/RobinVogel/Trade-offs-in-Large-Scale-Distributed-Tuplewise-Estimation-and-Learning

17

C Proof of Theorem 2

We ﬁrst detail the derivation of Var( (cid:98)Un,N,T (h)). Deﬁne the Bernouilli r.v. (cid:15)t
as equal to one if Xk is in partition i at time t, and similarly γt
to one if Zl is in partition i at time t. Note that for t
are independent, as well as γt
independent for any t, t0

i (l) and γt1
i1 (l1). Additionally, (cid:15)t
2.
1, . . . , T
}
Hoeﬀding’s decomposition implies:

i(k)
i (l) is equal
i1 (k1)
i1 (l) are

i(k) and γt1

i(k) and (cid:15)t1

= t1, (cid:15)t

∈ {

U t

n,N (h)

−

Un(h) =

1
N

N
(cid:88)

i=1

1
nm

n
(cid:88)

m
(cid:88)

k=1

l=1

(N 2(cid:15)t

i(k)γt

i (l)

1)h0(Xk, Zl).

−

The law of total variance, the fact that conditioned upon the data (cid:98)Un,N,T (h) is
an average of T independent experiments and the properties of Eq. (14) imply:

(cid:16)

Var

(cid:98)Un,N,T (h)

(cid:17)

= Var (Un(h)) + E

(cid:16)

(cid:104)

Var

= Var (Un(h)) +

= Var (Un(h)) +

On the other hand, observe that:

1
T
N 2σ2
0
nmT

(cid:98)Un,N,T (h)

|D

n,

m

Q

E (cid:2)Var (cid:0)U t

n,N (h)

n,

|D

m

Q

(cid:17)(cid:105)

,

(cid:1)(cid:3) ,

N
(cid:88)

i1,i2=1

Cov (cid:0)(cid:15)t

i1 (1)γt

i1(1), (cid:15)t

i2 (1)γt

i2(1)(cid:1) .

(15)

(16)

= i2,
if i1
if i1 = i2.

Cov (cid:0)(cid:15)t

i1(1)γt

i1(1), (cid:15)t

i2(1)γt

i2(1)(cid:1) =

(cid:40)

N −4

−
N −2

N −4

−

The result is obtained by plugging Eq. (16) in Eq. (15). Explicitly,

(cid:16)

Var

(cid:98)Un,N,T (h)

(cid:17)

= Var (Un(h)) +

N
1
−
nmT

σ2
0.

Using that E[ (cid:101)Un,N,B,T (h)

n,

m, (cid:15), γ] = (cid:98)Un,N,T (h), we now compute Var( (cid:101)Un,N,B,T (h))

|D

Q

by decomposing it as the variance of its conditional expectation plus the expec-
tation of its conditional variance. It writes:
(cid:104)

(cid:17)(cid:105)

(cid:16)

(cid:17)

(cid:16)

Var( (cid:101)Un,N,B,T (h)) = Var

(cid:98)Un,N,T (h)

+ E

Var

(cid:16)

= Var

(cid:17)

(cid:98)Un,N,T (h)

+

1
N T

E

(cid:104)
Var

(cid:101)Un,N,B,T
(cid:16)

n,

m, (cid:15), γ

|D

Q

(cid:101)UB,Rt

n,

m, (cid:15), γ

Q

i |D

(cid:17)(cid:105)

(Since the draws of B pairs on diﬀerent workers are independent)

(cid:16)

= Var

(cid:17)

(cid:98)Un,N,T (h)

+

1
N T

(cid:20)

−

1
B

Var

(cid:16)

URt

i

(cid:17)

+

1
B

(cid:21)

Var(h(X, Z))

(See [12].)

=

Var(h(X, Z))
N T B

(cid:18)
1

+

1
T B

(cid:19) (cid:18) σ2
1
n

(cid:19)

σ2
2
m

+

σ2
0
nm

(cid:20)

1 +

+

−

N

1
−
T −

N
T B

(cid:21)

,

which gives the desired result after reorganizing the terms.

18

(cid:54)
(cid:54)
Figure 5: Empirical variances as a function of the number of evaluated pairs for
SWOR, with n = 100, 000, m = 200 and N = 100, evaluated over 500 runs.

D Why (cid:98)Un,N,T Dominates (cid:101)Un,N,B,T for prop-SWOR

To establish a fair comparison between both estimators, we calculate the diﬀer-
ence ∆ between the variance of (cid:98)Un,N,T0 and (cid:101)Un,N,B,T for the same number of
pairs, i.e. when N BT = T0nm/N . Note that (cid:101)Un,N,B,T involves more reparti-
tioning of the data in all sensible cases, i.e. as soon as B < nm/N 2.

The expressions of Theorem 1 and Theorem 2 imply:

(cid:16)

∆ :=Var

(cid:101)Un,N,B,T (h)
(cid:18) 1

1

(cid:20) N

Var
(cid:19)

(cid:17)

−
1
T0

(cid:16)

(cid:98)Un,N,T0

(cid:17)

,

N
nmT B

+

1
N T B

(cid:21)

+

σ2
1
T B

(cid:21)

(cid:20) 1
N −

1
n

+

σ2
2
T B

(cid:20) 1
N −

1
m

(cid:21)

.

T −

−

=σ2
0

−
nm

Pluging in the constraint on the pairs gives:

∆ =σ2
0

(cid:18)

(cid:20) N

1
−
nmT

1
B

1

−

(cid:19)

+

1
T B

(cid:18) 1

N 2 −

1
nm

(cid:19)(cid:21)

+

σ2
1
T B

(cid:21)

(cid:20) 1
N −

1
n

+

σ2
2
T B

(cid:20) 1
N −

1
m

(cid:21)

,

which implies that ∆ > 0.

E Empirical Results for Sampling Without Re-

placement (SWOR)

In this section, we numerically show that in practice, the results for SWOR
do not deviate much from the theoretical ones obtained for prop-SWOR in
Theorem 1 and Theorem 2. To illustrate this, we use the kernel h(x, z) = x
z
and random variables in R that follow a normal law X
(µX , σX ) and
X σ2
Zσ2
Z and
Z
∼ N
0 = σ2
σ2
Z, which means that by tweaking the parameters µX , µZ, σX , σZ, one
can obtain any possible value of σ1, σ2, σ0.

In that setting, note that σ2

(µZ, σZ).
X σ2

∼ N
X , σ2

2 = µ2

1 = µ2

·

19

104105#pairs10−4Varianceσ20=1.00,σ21=0.25,σ22=0.00104105#pairs10−29×10−3σ20=1.00,σ21=0.00,σ22=0.25UnUn,NUn,N,BUn,N,TUn,N,B,TB=250B=1,000B=4,000T=2T=8The results, shown in Fig. 5 are very similar to those obtained for prop-
SWOR in Fig. 2. The fact that SWOR has slightly lower variance is expected,
since when no pairs are available the default value is always 0. This makes the
estimator give a stable prediction, but also makes it biased.

F Analysis of Proportional Sampling with Re-

placement (prop-SWR)

While the use of prop-SWR is not very natural in a standard distributed setting,
it is relevant in cases where workers have access to joint database that they can
eﬃciently subsample. We have the following results for the variance of estimates
based on prop-SWR (see Appendix F.1 and Appendix F.2 for the proofs).

(cid:19)

(cid:18)
2

Var(Un,1(h)) =

Theorem 3 If the data is distributed between workers with prop-SWR, and
denoting n0 = (n/N, m/N ), we have:
σ2
σ2
2
1
m
n
σ2
0
nm
1
N B

σ2
0
nm
1
n
−
Var (Un0,1(h))(cid:3) .

Var( (cid:101)Un,N,B(h)) = Var (Un,N (h)) +

Var(Un,N (h)) = Var(Un,1(h)) +

(cid:18) 1
n
1
m

−
(cid:19) (cid:18)
1

−
(cid:2)σ2

1
nm

(cid:18)
1

(cid:20)
4

1
m

1
m

1
n

(N

1)

−

+

−

+

+

+

−

(cid:18)

(cid:19)

(cid:19)

(cid:19)

2

2

,

−

(cid:21)

,

Theorem 4 If the data is distributed and repartitioned between workers with
prop-SWR, we have:

Var( (cid:98)Un,N,T (h)) = Var(Un(h)) +

[Var(Un,N (h))

Var( (cid:101)Un,N,B,T (h)) = Var

(cid:16)

(cid:98)Un,N,T (h)

+

1
N BT

(cid:2)σ2

−

Var(Un(h))] ,

−
Var(Un0,1(h))(cid:3) .

1
T
(cid:17)

Fig. 6 gives a visual illustration of these results. First note that they are
similar to those obtained for prop-SWOR in Fig. 2. Yet, the right-hand side
ﬁgure shows that (cid:101)Un,N,B,T can have a signiﬁcantly lower variance than (cid:98)Un,N,T ,
for the same number of evaluated pairs. This comes from the fact that (cid:101)Un,N,B,T
works on more bootstrap re-samples of the data than (cid:98)Un,N,T , hence better
correcting for the loss of information due to sampling with replacement (at the
cost of more communication or disk reads). To stress this, we also represented
Un,1, i.e. the point that gives the variance of a complete estimator based on one
bootstrap re-sample of the data.

F.1 Proof of Theorem 3
First we derive the variance of Un,N (h). Since E[Un,N (h)
law of total variance implies:

n,

m] = Un(h), the

Q

|D

Var(Un,N (h)) = Var(Un(h)) + E [Var (Un,N (h)

n,

m)] ,

|D
E [Var (UR1 (h)

Q
n,

m)] .

|D

Q

= Var(Un(h)) +

1
N

20

Figure 6: Theoretical variances as a function of the number of evaluated pairs
for diﬀerent estimators under prop-SWR, with n = 100, 000, m = 200 and
N = 100.

Introduce (cid:15)(k) (resp. γ(l)) as the random variable that is equal to the number
of times that k has been sampled in cluster 1 for the
n elements (resp. that l
has been sampled in cluster 1 for the
m elements). The random variable (cid:15)(k)
Q
(resp. γ(l)) follows a binomial distribution with parameters (n/N, 1/n) (resp.
(m/N, 1/m)). Note that the (cid:15) and γ are independent and that (cid:80)n
k=1 (cid:15)(k) = n/N
and (cid:80)m

l=1 γ(l) = m/N . It follows that:

D

UR1 (h)

−

Un(h) = U (h) +

+

+

n
(cid:88)

(N (cid:15)(k)

k=1
m
(cid:88)

(N γ(l)

1
n

1
m

l=1
n
(cid:88)

1
nm

k=1

l=1

1) (h1(Xk)

1) (h2(Zl)

U (h))

U (h))

−

−

−

−

m
(cid:88)

(cid:0)N 2(cid:15)(k)γ(l)

1(cid:1) h0(Xk, Zl),

−

which implies, using the results of Eq. (14),

E [Var (UR1 (h)

n,

|D

m)] =

Q

N 2σ2
1
n

Var((cid:15)(1)) +

N 2σ2
2
m

Var(γ(1)) +

N 4σ2
0
nm

Var((cid:15)(1)γ(1)).

(17)

The mean and variance of a binomial distribution is known. Since (cid:15)(1) and γ(1)
are independent,

Var((cid:15)(1)γ(1)) =

1
N 2

Var((cid:15)(1)) =

1
N

(cid:18)

1
n

1

−

(cid:20)(cid:18)

1

(cid:19)

(cid:19)(cid:21)

,

1
m

(18)

(cid:18)

1
N
(cid:18)

2

1

−

−
1
m

1
n −
(cid:19)

.

+

1
N

(cid:19) (cid:18)
1

1
n

−

1
m

−

(cid:19)

, Var(γ(1)) =

21

104105#pairs10−4Varianceσ20=1.00,σ21=0.25,σ22=0.00104105#pairs10−22×10−23×10−2σ20=1.00,σ21=0.00,σ22=0.25UnUn,NUn,1Un,N,BUn,N,TUn,N,B,TB=250B=1,000B=4,000T=2T=8Un,N,TvaryingTUn,N,BvaryingBUn,N,1,TvaryingTPlugging Eq. (18) into Eq. (17) gives the result. Explicitly,

Var (Un,N (h)) =

σ2
1
n

+

(cid:18)

2

σ2
0
nm

1
n
−
(cid:20)(cid:18)

(cid:19)

+

σ2
2
m

(cid:18)
2

1
n −

1
m

3

−

(cid:19)

1
m

(cid:18)

−
(cid:19)

+ N

1

1
n

−

(cid:19) (cid:18)
1

(cid:19)(cid:21)

.

1
m

−

Now we derive the variance of (cid:101)Un,N,B(h). Note that E[ (cid:101)Un,N,B

Un,N (h), hence:

n,

|D

m, (cid:15), γ] =

Q

Var( (cid:101)Un,N,B) = Var(Un,N (h)) + E
(cid:16)

= Var(Un,N (h)) +

E

(cid:104)
Var

1
N

(cid:104)

(cid:16)

(cid:101)Un,N,B

Var

n,

|D

m, (cid:15), γ

(cid:17)(cid:105)

,

Q
(cid:17)(cid:105)

(cid:101)UB,R1|D

n,

Q

m, (cid:15), γ

.

(19)

n,

m, (cid:15), γ, the statistic (cid:101)UB,R1 is an average of B inde-
Conditioned upon
Q
D
pendent experiments.
Introducing δk,l as equal to 1 if the pair (k, l) is se-
lected in worker 1 as the 1th pair of (cid:101)UB,R1 , and ∆k,l its expected value, i.e.
∆k,l := E[δk,l] = N 2(cid:15)(k)γ(l)/nm, it implies
(cid:32) n
(cid:88)

m
(cid:88)

(cid:33)

(cid:17)

(cid:16)

m, (cid:15), γ

=

Var

δk,lh(Xk, Zl)

n,

m, (cid:15), γ

.

Var

n,
(cid:101)UB,R1 | D

Q

| D

Q

1
B

k=1

l=1

(20)

From the deﬁnition of δk,l we have δk,lδk1,l1 = 0 as soon as k
= l1,
writing the right-hand-side of Eq. (20) as the second order moment minus the
squared means gives:

= k1 or l

Var

(cid:16)
n,
(cid:101)UB,R1 | D

m, (cid:15), γ

Q

(cid:17)

=

1
B

m
(cid:88)

n
(cid:88)

k=1

l=1

∆k,jh2(Xk, Zl)

1
B

−

(cid:32) n
(cid:88)

m
(cid:88)

k=1

l=1

(cid:33)2

∆k,lh(Xk, Zl)

.

(21)

Taking the expectation of Eq. (21) gives:

(cid:104)
E

Var

(cid:16)

(cid:101)UB,R1 |D
n,

m, (cid:15), γ

Q

(cid:17) (cid:105)

=

=

1
B
1
B

(cid:2)E[h2(X, Z)]

E[U 2

R1](cid:3) ,

−

[Var(h(X, Z))

−

Var(UR1 )] .

(22)

Pluging Eq. (22) into Eq. (19) gives

Var( (cid:101)Un,N,B) =

+ Var(Un,N (h))

−
and we can conclude from preceding results, since URi
n0 = (n/N, m/N ).

Var(h(X, Z))
BN

Var(URi)
BN

,

is simply Un0,1 with

F.2 Proof of Theorem 4
Since E

(cid:104)
(cid:98)Un,N,T (h)

n,

m

(cid:105)

|D

Q

= Un(h) the law of total covariances followed by

the fact that, conditioned upon
T independent random variables, implies:

n,

Q

D

, the statistic (cid:98)Un,N,T (h) is an average of

(cid:16)

Var

(cid:17)

(cid:98)Un,N,T (h)

= Var(Un(h)) +

1
T

E [Var (Un,N (h)

|D

m)] .

n,

Q

22

(cid:54)
(cid:54)
The calculations of Appendix F.2 give the result. Explicitly,

(cid:16)

Var

(cid:98)Un,N,T (h)

(cid:17)

= Var(Un(h)) +

1
T

[Var(Un,N (h))

Var(Un(h))] .

−

We now derive the variance of (cid:101)Un,N,B,T . Since

(cid:104)

E

(cid:101)Un,N,B,T (h)

n,

|D

m, (cid:15), γ

Q

(cid:105)

= (cid:98)Un,N,T (h),

the law of total covariance followed by the calculations of Appendix F.2 imply
the result:
(cid:16)

(cid:17)(cid:105)

(cid:16)

(cid:17)

(cid:16)

(cid:17)

(cid:104)

Var

(cid:101)Un,N,B,T (h)

= Var

(cid:98)Un,N,T (h)

+ E

Var

(cid:16)

(cid:16)

= Var

= Var

(cid:98)Un,N,T (h)

(cid:98)Un,N,T (h)

(cid:17)

(cid:17)

+

+

(cid:101)Un,N,B,T (h)
(cid:16)

(cid:101)Un,N,B(h)

n,

m, (cid:15), γ

,

|D

Q

n,

|D

m, (cid:15), γ

Q

(cid:17)(cid:105)

,

(cid:104)

E

1
T

Var

1
N BT

[Var(h(X, Z))

Var(URi)] .

−

G Details on the Estimation Experiment of Sec-

tion 5

Here we give some details on the derivations leading to Eq. (13). We have:

−
p + p

U (h) = P (X > Z) = q + (1
h1(x) = P (x > Z) = 1
h2(z) = P (X > z) = q + (1
+ I
h0(x, z) = I
x = 0
x = 2
}
{
{
q) (I
(I
X = 2
{

} −

−

=

−

−

} ·

{

p),

q)(1
−
I
,
x = 2
}
{
·
q)I
,
1
z =
{
}
I
z =
{
Z =

−
1
} −
(1

−
1

−

} −

−

h1(x)

h2(z) + U (h),

−
p)) .

It follows that:

1 = p2q(1
σ2
σ2
2 = (1
−
σ2
0 = pq(1

q),
−
q)2p(1

p)(1

−

p),

q).

−

−

23

