2
2
0
2

g
u
A
3
2

]
I
S
.
s
c
[

1
v
7
2
2
2
1
.
8
0
2
2
:
v
i
X
r
a

Community Detection in the Hypergraph SBM:
Optimal Recovery Given the Similarity Matrix

Julia Gaudio∗

Nirmit Joshi†

Abstract

Community detection is a fundamental problem in network science. In this paper, we consider
community detection in hypergraphs drawn from the hypergraph stochastic block model (HSBM),
with a focus on exact community recovery. We study the performance of polynomial-time
algorithms for community detection in a case where the full hypergraph is unknown. Instead,
we are provided a similarity matrix W , where Wij reports the number of hyperedges containing
both i and j. Under this information model, Kim, Bandeira, and Goemans [KBG18] determined
the information-theoretic threshold for exact recovery, and proposed a semideﬁnite programming
relaxation which they conjectured to be optimal. In this paper, we conﬁrm this conjecture. We
also show that a simple, highly eﬃcient spectral algorithm is optimal, establishing the spectral
algorithm as the method of choice. Our analysis of the spectral algorithm crucially relies on
strong entrywise bounds on the eigenvectors of W . Our bounds are inspired by the work of
Abbe, Fan, Wang, and Zhong [AFWZ20], who developed entrywise bounds for eigenvectors of
symmetric matrices with independent entries. Despite the complex dependency structure in
similarity matrices, we prove similar entrywise guarantees.

1

Introduction

Community detection is the problem of partitioning a network into densely connected clusters.
As a fundamental network science problem, community detection arises in numerous applications:
sociology [GZFA10, NWS02], protein interactions [CY06, MPN+99], image applications [SM00],
natural language processing [BKN11], webpage sorting [KRRT99] and many more.

In 1983, Holland et al. [HLL83] introduced the stochastic block model (SBM), a probabilis-
tic generative model for networks with community structure. Since then, community detection
in the SBM has been intensely studied in the probability, statistics, and theoretical computer
science communities; see [Abb17] for a survey. The (general) SBM is speciﬁed by two types of
parameters: community sizes and edge formation probabilities. The probability that two ver-
tices i, j are connected by an edge depends only on the community memberships of those ver-
tices. Research on community detection in the SBM has focused on questions of exact, par-
tial, and weak recovery of the communities. These questions have been approached from both
the statistical and algorithmic perspectives, with many exciting developments over the past few
decades [DF89, McS01, DKMZ11, MNS13, Mas14, AS15a, ABH16, AS15b].

In this paper, we consider an extension of the SBM to hypergraphs. A hypergraph is a gener-
alization of a graph that is able to capture higher-order interactions. For example, an academic

∗(julia.gaudio@northwestern.edu) Department of Industrial Engineering and Management Sciences, Northwestern

University.

†(nirmit@u.northwestern.edu) Department of Computer Science, Northwestern University.

1

 
 
 
 
 
 
co-authorship network may be modeled as a hypergraph, where each hyperedge represents the
author list of a paper. Formally, a hypergraph is speciﬁed by a set of vertices V and a set of hyper-
E is a subset of V . We specialize to uniform hypergraphs, where
edges E. Each hyperedge e
each hyperedge contains the same number of vertices. A d-uniform hypergraph satisﬁes
= d for
E (in particular, a graph is 2-uniform hypergraph). We then say that the hypergraph has
all e
order d.

e
|
|

∈

∈

We now describe the Hypergraph Stochastic Block Model (HSBM), which was ﬁrst proposed by
Ghoshdastidar and Dukkipati [GD14]. We consider the version with two balanced communities, and
equal inter-community edge probabilities. The model is speciﬁed by its order d and two parameters
n is sampled uniformly at random
1
α > β > 0. First, a community assignment vector σ∗ ∈ {±
}
1n, σ
from the set
h

Rn is the vector of all ones.

, where 1n ∈
= 0
}

n :
1
}

∈ {±

σ
{

i

Conditioned on σ∗, we sample a hypergraph G = ([n], E) as follows. Each e =

i1, i2, . . . , id} ∈
{

appears as a hyperedge independently with probability

[n]
d

(cid:0)

(cid:1)

P (e

∈

E) =

(

α log n/(n
−
d
−
β log n/(n
−
d
−

1

1) σ∗(i1) = σ∗(i1) =
1)

otherwise.

1

= σ∗(id)

· · ·

HSBM(d, n, α, β).

We then write G

∼

We say that an estimator ˆσn achieves exact recovery if

For clarity of presentation, we typically drop the dependence on n.

lim
n
→∞

P (ˆσn ∈ {±

) = 1.

σ∗n}

1

−

√β

√α

1
2d

mined by Kim, Bandeira, and Goemans [KBG18]: if

The information-theoretic threshold for exact recovery in the HSBM(d, n, α, β) model was deter-
2 < 1, then exact recovery is
√β
2 > 1, then exact recovery is possible. The maximum-likelihood
impossible, while if
estimator, which achieves the threshold, is equivalent to solving a min-bisection problem [KBG18].
Since min-bisection is NP-hard, this motivated the search for polynomial-time algorithms for the
exact recovery problem [GD15a, GD15b, GD17, ALS18, CLW19], culminating in the work of Zhang
and Tan [ZT21], whose algorithm applies to a general class of HSBMs. Their results essentially show
that there is no statistical-computational gap for the exact recovery problem (see [ZT21, Theorem
2] for a precise statement).

1
2d

√α

−

−

(cid:1)

(cid:0)

(cid:1)

(cid:0)

−

1

While the work of Zhang and Tan [ZT21] resolves the question of eﬃcient exact recovery even for
general HSBMs, the result assumes that the hypergraph is fully observed. Motivated by [KBG18,
LKC20], we instead consider exact community recovery given only the similarity matrix of the
hypergraph.

Deﬁnition 1.1 (Similarity Matrix). Let G = ([n], E) be a hypergraph on n vertices. The
similarity matrix of G is the zero-diagonal matrix W whose entries are

i, j
{
= j. In other words, Wij counts the number of edges which contain both i and j. We also

Wij =

e
}|

} ⊂

E :

|{

∈

e

for i
write W =

(G) to deﬁne the similarity matrix transformation.

S

Suppose G

(G). Given only the restricted informa-
tion present in W , when is it possible to recover the communities exactly? Kim et al. [KBG18, The-
(G). Letting
orem 3] determined the information-theoretic threshold for exact recovery given W =

HSBM(d, n, α, β), and we observe W =

∼

S

I(d, α, β) = max
0

t
≥

1
2d

−

1

"

α

1

(cid:16)

−

1

d

−

d

Xr=1 (cid:18)

−
r

1

(cid:19) (cid:16)

e−

(d

−

1)t

+ β

(cid:17)

2

S

e−

(d

−

1

−

2r)t

1

−

,

#

(cid:17)

(1)

6
300

280

260

240

220

200

180

160

140

120

10

20

30

40

50

60

70

Figure 1: Visualization of I(d, α, β) = 1 and ISDP(d, α, β) = 1 when d = 6.

exact recovery given W is not possible if I(d, α, β) < 1. Again, the optimal estimator corresponds
to solving a min-bisection problem [KBG18]. Therefore, Kim et. al. [KBG18] additionally pro-
posed a semideﬁnite programming (SDP) relaxation, showing that it succeeds in exact recovery if
ISDP(d, α, β) > 1, where

ISDP(d, α, β) =

1)

(d

−
22d

·

(α
−
2d + β

α d

β)2
1

−

d
2d

.

(cid:0)
3; see Figure 1 for an illustration. The authors
Note that ISDP(d, α, β) > I(d, α, β) for d
it succeeds whenever I(d, α, β) > 1 [KBG18,
conjectured that the SDP algorithm is optimal; i.e.
Conjecture 1.2]. Our ﬁrst main contribution is to conﬁrm this conjecture. We note that there has
been some follow-up work on community recovery from the similarity matrix [LKC20, CZ20], but
to our knowledge we are the ﬁrst to resolve [KBG18, Conjecture 1.2].

(cid:1)(cid:1)

≥

(cid:0)

A natural question emerges:

if the SDP algorithm is optimal, are other eﬃcient algorithms
optimal as well? Given that spectral algorithms are more eﬃcient than SDPs, and motivated
by the success of simple spectral algorithms for other community detection problems [AFWZ20,
SWZ19, DLS21, DGMS22a, DGMS22b, DGMS22c], we ask:

Do spectral algorithms achieve the information-theoretic threshold for exact recovery from a
similarity matrix?

In this direction, our second main contribution is a spectral algorithm which achieves the information-
theoretic threshold. Moreover, our algorithm does not require any clean-up phase. In order to ana-
lyze our spectral algorithm, inspired by the work of [AFWZ20], we develop ℓ
(entrywise) bounds
for the eigenvectors of similarity matrices of a large class of random hypergraph models, which
may be of independent interest. Roughly speaking, we show that an eigenvector uk of a random
similarity matrix W is close to its ﬁrst order approximation W u∗k/λ∗k in the ℓ
norm under mild
conditions, where (λ∗k, u∗k) is the corresponding eigenpair of E [W ].
Notation. For any real numbers a, b
Let sgn : R
also extend the deﬁnition to vectors; let sgn : Rn
sign function componentwise.

.
a, b
b = min
}
{
1 if x < 0. We
n be the map deﬁned by applying the
1
}

be the function deﬁned by sgn(x) = 1 if x

and a
}
0 and sgn(x) =

R, we denote a

b = max

a, b
{

→ {±

→ {±

∧
−

1
}

≥

∨

∈

∞

∞

3

We use the notation R+ = (0,

.We use the Bach-
}
mann–Landau notation o(.), O(.), ω(.), Ω(.), Θ(.) etc. throughout the paper. For nonnegative
Cbn for some constant C > 0. The
sequences (an)n
notation
bn
as a shorthand for limn

is similar, hiding two constants in upper and lower bounds. Moreover, we write an ≈

1, 2, . . . , n
{

N, we write [n] =

1 and (bn)n

). For n

∞

≍

≥

≥

an
bn
For any two vectors x, y
n
i=1 x2
k2 = (

i )1/2,

∈
k1 =

→∞

·

P

x
k

k∞
refers to its i-th row, which is a row vector, and M
·
k2 = sup
x

x
k
Mi
column vector. The matrix spectral norm is
is
Mi
(

P
= supi k

= sup
x
k
ij)1/2.

→∞
n
j=1 M 2

M
k
n
i=1

k2=1 k

M x

k∞

k2

k

= maxi |

represents a standard inner product in Rn; we deﬁne
n,
i refers to its i-th column, which is a
norm
k2, the matrix 2
k2=1 k
kF =

M
M x
k
·k2, and the the matrix Frobenius norm is

. For any matrix M

→ ∞
M
k

xi|

Rn

∈

×

k

∈
1, we write an . bn to mean an ≤
= 1.
Rn,
x, y
h
n
xi|
i=1 |

i
, and

x

P

Organization. Section 2 contains our main results along with their proof outlines, as well as a
P
discussion of future work. We show the optimality of the SDP relaxation in Section 3. Finally,
in Section 4, we present the analysis of the spectral algorithm and the entrywise bounds on the
eigenvectors of similarity matrices.

2 Models and results

2.1 Models

Recall the deﬁnition of HSBM(d, n, α, β). We additionally deﬁne a general model of random hy-
pergraphs with independent edges.

d ).
Deﬁnition 2.1 (General random hypergraph). Let d
Deﬁne H(d, n, p) to be the distribution on d-uniform hypergraphs with n vertices, where each edge
e

appears in the hypergraph with probability pe, independently.

N, and p

, n
}

2, 3, . . .

∈ {

[0, 1]([n]

∈

∈

[n]
d

∈

(cid:1)
2.2 Results

(cid:0)

Recall that σ∗n ∈ {±
on n vertices, and let W =
proposed the following SDP recovery algorithm.

S

n denotes the true community assignment vector. Let G be a hypergraph
1
}
(G) be its similarity matrix. Kim, Bandeira, and Goemans [KBG18]

Algorithm 1 SDP recovery algorithm [KBG18]
Input: An n

n similarity matrix W

×

Output: An estimate of community assignments

1: Solve the following SDP, where X

Rn

n.

×

∈

max

W, X
h
subject to Xii = 1 for all i

i

= 0

X, 11⊤
h
0.
X

(cid:23)

i

[n]

∈

(2)

2: Let X ∗ be the optimal solution, and let X ∗ =
λ2 ≥ · · · ≥

X ∗, where λ1 ≥

λn.

3: Return ˆσSDP = sgn(v1).

n
i=1 λiviv⊤i denote the eigendecomposition of

P

4

Our ﬁrst main result states that the SDP relaxation achieves exact recovery down to the information-
theoretic threshold.

Theorem 1. Fix d
HSBM(d, n, α, β), and let W =
X ∗ = σ∗σ∗⊤ with probability 1
achieves exact recovery.

∈ {

2, 3, . . .

S
−

and α > β > 0 such that I(d, α, β) > 1. Suppose G
}
∼
(G). Let X ∗ be the optimal solution to (2) with input W . Then
o(1). It follows that the estimator ˆσSDP returned by Algorithm 1

We also establish that the SDP continues to achieve exact recovery even under a monotone adversary
model (Lemma 3.3).

Having shown optimality of the SDP relaxation, we propose a spectral algorithm and establish

its optimality.

Algorithm 2 Spectral recovery algorithm
Input: An n

n similarity matrix W

×

Output: An estimate of community assignments
1: Compute the eigenpairs of W , denoted by (λi, ui)n
2: Return ˆσspec = sgn(u2).

i=1, where λ1 ≥

λ2 ≥ · · · ≥

λn.

Theorem 2. Fix d
HSBM(d, n, α, β), and let W =
1

o(1), there exist s

∈ {

2, 3, . . .

∈ {±

1
}

−

and α > β > 0 such that I(d, α, β) > 1. Suppose G
}
∼
(G). Let u2 be the second eigenvector of W . Then with probability

S
and η = η(d, α, β) > 0 such that

As a result, the estimator ˆσspec produced by Algorithm 2 on input W achieves exact recovery.

√n min
[n]
i
∈

sσ∗(i)u2,i > η.

Our proof of Theorem 2 crucially relies on entrywise bounds on the second eigenvector of the
similarity matrix. To this end, we develop entrywise bounds on the eigenvectors of similarity matri-
ces of general random hypergraphs (Deﬁnition 2.1), in the logarithmic degree regime. Speciﬁcally,
H(d, n, p). Let (λi, ui)n
we consider W =
i=1 denote the eigenpairs of W , where
λ1 ≥
λ2 ≥ · · · ≥
i=1. We use the convention
λ0 = λ∗0 =
∞

S
λn. Let W ∗ = E[W ], with ordered eigenpairs (λ∗i , u∗i )n

. We then deﬁne the following eigengap quantity:

and λn+1 = λ∗n+1 =

(G), where G

∼

1 −
Our entrywise guarantee requires a spectral separation assumption. The assumption easily holds
for similarity matrices of (general) HSBMs, in the logarithmic degree regime.

−

λ∗k
{

λ∗k, λ∗k −

.
λ∗k+1}

−∞
∆∗k = min

A 1 (Spectral separation). There is a constant c1 > 0 such that 1
c1

log n

,
λ∗k|

≤ |

∆∗k| ≤
|

c1 log n.

Under this assumption, we state our entrywise guarantee.

N and d

Theorem 3. Let k

Let G
probability 1

∼

∈
H(d, n, p), and W =
3),

O(n−

∈ {

S

−

2, 3, . . .

be constants. Let p

}

[0, 1]([n]

d ), where maxe pe ≤

∈

c0 log n
(n
1)
−
d
−

1

.

(G). Suppose that A 1 holds with constant c1 > 0. Then with

where c > 0 is some constant depending only on d, c0, and c1.

s∗

min
uk −
1
s∗∈{±
} (cid:13)
(cid:13)
(cid:13)
(cid:13)

W u∗k
λ∗k (cid:13)
(cid:13)
(cid:13)
(cid:13)

∞

u∗kk∞
c
log log n

k

,

≤

5

2.3 Proof ideas

Showing optimality of the SDP. We use a dual certiﬁcate strategy as in [KBG18]. The dual
of (2) is given by

min trace(D)

subject to D is n

n diagonal matrix, ν

×

D + ν11⊤

W

0.

R,

∈

(3)

−
The form of the dual motivates the following suﬃcient condition, whose proof we include for
completeness (see Appendix A).

(cid:23)

Lemma 2.2. Suppose there is a diagonal matrix D
Letting S , D + ν11⊤ −
λn

R such that the following holds.
n and ν
W , the matrix S is positive semideﬁnite, its second-smallest eigenvalue
1(S) is strictly positive, and Sσ∗ = 0. Then X ∗ = σ∗σ∗⊤ is the unique optimal solution to (2).

Rn

∈

∈

×

−

To apply Lemma 2.2, we let D be the diagonal matrix whose diagonal entries are speciﬁed by

Dii =

Wijσ∗(i)σ∗(j).

(4)

[n]
Xj
∈

Setting ν = 1, write S = D + 11⊤ −

W . By construction, we have Sσ∗ = 0. It remains to show

P

inf
x
k

σ∗:

2=1
k

x

⊥

(cid:18)

x⊤Sx > 0

= 1

(cid:19)

o(1).

−

(5)

This is where our proof diverges from [KBG18]; rather than showing (5), [KBG18] proceeds through
a diﬀerent suﬃcient condition. Using steps similar to the proof of [HWX16a, Theorem 2], we show
that for all x

σ∗ such that

⊥

x
k

k2 = 1,
x⊤Sx

≥

min
[n]
i
∈

Dii − k

W

W ∗

k2,

−

(6)

where W ∗ is the expected value of W , conditioned on σ∗. It remains to (1) lower-bound Dii for
each i

[n] and (2) upper-bound

∈

W
k

W ∗k2.

−

To lower-bound Dii, we condition on σ∗ and establish concentration of Dii around its mean. To

see why Dii should be positive and bounded away from zero, it helps to rewrite (4) as follows:

Dii =

Wij −

[n]:σ∗(i)=σ∗(j)

Xj
∈

[n]:σ∗(i)
Xj
∈

=σ∗(j)

Wij.

Wij}
{

o(n−
To bound

n
j=1 are dependent, they are functions of independent random variables
While the values
(namely, the hyperedge random variables). After re-expressing Dii in terms of the underlying hy-
peredge random variables, the proof proceeds by a Chernoﬀ-style argument (Lemma 3.2). Whenever
ǫ log n with probability
I(d, α, β) > 1, we establish the existence of ǫ > 0 such that for each i, Dii ≥
1
−

1). A union bound then implies mini

ǫ log n with high probability.

W ∗k2, we directly apply the sharp result of Lee, Kim, and Chung [LKC20, Theo-
−
√log n
rem 4, Corollary 2], which shows that
W ∗k2 = O
with high probability. Interestingly,
we found a simple bound on the expected spectral norm, showing that E [
√log n
.
W
k
Markov’s inequality then implies a looser version of [LKC20, Theorem 4, Corollary 2] (see Theorem
(cid:1)
4 in Appendix B). Returning to (6), we see that x⊤Sx > 0 simultaneously for all x satisfying
x

W ∗k2] = O

[n] Dii ≥

W
k

W
k

−

−

(cid:1)

(cid:0)

(cid:0)

∈

σ∗,

x
k

k2 = 1, with high probability.

⊥

6

6
1.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

1

2

3

Figure 2: Consider HSBM(d, n, α, β) for d = 4, n = 1000, α = 50, β = 10. The boxplots show three diﬀerent
errors over 100 realizations: (1)√n

∞, and (3) √n

W u∗

W u∗

2/λ∗
2

2/λ∗
2

u∗
2

u∗
2

∞.

u2

u2

k

−

∞, (2)√n
k

k

−

k

k

−

k

Analyzing the spectral algorithm. Recall that u2 denotes the second eigenvector of W =
(G). Since our algorithm determines communities based on the signs of u2, we need precise
S
bounds on each entry of u2. A natural strategy would be to compare u2 to u∗2, since u∗2 = 1
√n σ∗.
is too large for our purposes, but still u2 recovers communities by sign.
u∗2k∞
Unfortunately,
To gain some intuition for this behavior, write

u2 −
k

u2 −

u∗2 =

W u∗2
λ∗2 −

(cid:18)

u∗2

+

(cid:19)

(cid:18)

u2 −

W u∗2
λ∗2 (cid:19)

.

We can think of the ﬁrst term on the right hand side as the main term, while the second represents
a smaller-order term (see Figure 2). Such behavior was also observed in the SBM setting [AFWZ20].

Our ﬁrst step is to apply Theorem 3, showing that

min
u2 −
s∗∈{±
1
} (cid:13)
(cid:13)
(cid:13)
(see Corollary 4.1). Therefore, if we can show that the vector W u∗2
(cid:13)
has the same signs as σ∗ (up to
λ∗2
a global sign ﬂip), then the same is true for u2. Our goal is to show that σ∗ and W u∗2/λ∗2 have the
same signs, i.e.

= o(1/√n)

(7)

s∗

∞

W u∗2
λ∗2 (cid:13)
(cid:13)
(cid:13)
(cid:13)

Fixing the orientation u∗2 = 1

σ∗(i)

W u∗2
λ∗2 (cid:19)i
√n σ∗, we obtain that for i

min
[n]
i
∈

(cid:18)

> 0.

[n],

∈

σ∗(i)

W u∗2
λ∗2 (cid:19)i

(cid:18)

=

σ∗(i)

P

7

[n] Wijσ∗(j)
j
∈
λ∗2√n

=

Dii
λ∗2√n

,

where Dii is deﬁned in (4). As in the SDP analysis, we note that for ǫ > 0 suﬃciently small,
has
mini
the same signs as σ∗. Moreover, the entries are of order 1/√n; in turn, (7) implies that u2 also has
the same signs as σ∗, up to a global sign ﬂip.

ǫ log n with high probability, whenever I(d, α, β) > 1. Therefore, the vector W u∗2
λ∗2

[n] Dii ≥

∈

Entrwise eigenvector analysis. Abbe, Fan, Wang, and Zhong [AFWZ20] introduced a pow-
erful entrywise eigenvector bound, which has been used to show optimality of spectral algorithms
without the need of a clean-up stage [AFWZ20, DGMS22a, DGMS22b, DGMS22c]. Unfortunately,
the entrywise bound [AFWZ20, Theorem 2.1] does not apply to W =
(G), since W violates
a certain independence assumption. The independence assumption is critically used in a leave-
one-out argument; we therefore carefully adapt this step. It also remains to verify a certain row
concentration property of the matrix W .

S

For simplicity, let λ = λk, λ∗ = λ∗k and ∆∗k = ∆∗. For clarity of presentation, we assume the
0, and make similar simplifying assumptions throughout the outline. Our goal

orientation
is then to show

u, u∗i ≥
h

W u∗
λ∗ (cid:13)
∞
(cid:13)
We ﬁrst relate λ to λ∗. By Weyl’s inequality,
(cid:13)
(cid:13)
implies
1
λ−
|

u∗k∞
log log n
W ∗k2. [LKC20, Theorem 4]
λ∗| ≤ k
γλ∗ with high probability, for certain γ = γn = o(1). It then follows that
1. Using this observation along with the triangle inequality, we can show

W ∗k2 ≤
−
. γ
λ∗|−
|
|

W
k
1
λ∗−

. k

(cid:13)
(cid:13)
(cid:13)
(cid:13)

λ
|

W

−

−

−

−

u

.

u

W u∗
λ∗ (cid:13)
(cid:13)
(cid:13)
(cid:13)

W u

=

W u∗

W u∗
λ

+

∞

−

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
1
(cid:13)
(cid:13)
λ −
(cid:12)
(cid:12)
1
(cid:12)
(cid:12)
λ∗|
|
Note that u∗ is a deterministic vector. Therefore, in order to bound the term
a row concentration result (Lemma 4.4). For a ﬁxed vector v
controls

λ −
1
λ∗ (cid:12)
(cid:12)
(cid:12)
W u∗
(γ
(cid:12)

λ −
1
λ
|
|
W (u
k

in terms of both

W u∗
k

and

u∗)

u∗)

k∞

k∞

k∞

k∞

) .

−

+

+

−

≤

.

∈

∞

k

v
k

k∞

v
k

k2.

W v
k

k∞

W u∗
λ∗ (cid:13)
(cid:13)
(cid:13)
W (u
(cid:13)
k

(8)

, we derive
W u∗k∞
k
Rn, our row concentration result

Since u depends on W , the second term in (8) requires a diﬀerent strategy than the ﬁrst.
We therefore apply the leave-one-out technique, motivated by other works using a similar strategy
u∗)]m|
[BBKY13,JMRT16,ZB18,AFWZ20]. Bounding
[n] and deﬁne a random matrix W (m) which is independent
for each m
of the m-th rows and columns of W . Let G(m) be the hypergraph formed from G by deleting all
hyperedges containing m, and let W (m) =
(G(m)) be its similarity matrix. Let u(m) be the k-th
S
eigenvector of W (m). Applying the triangle and Cauchy–Schwarz inequalities, we obtain

[n]. To this end, we ﬁx m

reduces to bounding

[W (u
|

W (u
k

u∗)

k∞

−

−

∈

∈

[W (u
|

−

u∗)]m| ≤

Wm
·

(u

−

(cid:12)
(cid:12)
≤ k
(cid:12)

W

k2

→∞ k

+

u(m))
Wm
·
(cid:12)
(cid:12)
u(m)
k2 +
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

u

(u(m)

Wm
|
·

u∗)

−
(u(m)

(cid:12)
(cid:12)
−
(cid:12)

.
u∗)
|

(9)

Observe that Wm
·
can bound the second term in (9) using our row concentration result. In order to bound
we apply a version of the Davis–Kahan sin(θ) theorem [DK70], which yields

u∗ are independent by the leave-one-out construction. Therefore, we
k2,

u(m)

u
k

−

−

and u(m)

u(m)

u
k

−

(W

k2 . k

−

W (m))u
∆∗

k2

.

8

Our analysis so far is almost identical to that of [AFWZ20]; the main diﬀerence arises here.
W (m) are the same as those of W . Since a hyperedge
Note that the m-th row and column W
containing the vertex m contributes to other entries of W , there will be additional non-zero entries
outside of the m-th row and column. This complication arises whenever d
3, and is absent from
the analysis of [AFWZ20], due to their independence assumption. Still, we are able to prove a
similar probabilistic bound on
2, using structural properties of similarity matrices.

W (m))u

(W

≥

−

−

2.4 Discussion and future work

(cid:13)
(cid:13)

(cid:13)
(cid:13)

This paper contributes to a line of work which studies random graph inference problems under
restricted information. This paper considers aggregated information through the similarity matrix
transformation. Other recent works consider noisy or censored [ABBS14, HWX16b, DGMS22a,
DGMS22b, DGMS22c] information models.

Our work shows that the spectral algorithm recovers communities from the similarity matrix
whenever the MLE does. Abbe et. al. [AFWZ20], who observed the same phenomenon both in
community detection and in the Z2-synchronization problem, noted that such a phenomenon may
be more general. Indeed, our result adds to a growing list of examples where a spectral algorithm
matches the MLE [AFWZ20,DLS21,DGMS22a,DGMS22b,DGMS22c]. All of these results crucially
rely on entrywise eigenvector bounds. Finally, we note that a sharp phase transition occurs at
I(d, α, β) = 1, reiterating a common theme in community detection problems. In particular, the
spectral algorithm succeeds with high probability if I(d, α, β) > 1, while any algorithm fails with
high probability if I(d, α, β) < 1.

Future directions include:

1. Another widely studied problem in community detection is the Planted Dense Subgraph
(PDS) model [HWX16a, DGMS22b]. Consider its natural hypergraph extension, where there
(0, 1). Intra-community
ρn
is one dense community to be recovered of size
⌋
1
and other edges exist with prob-
edges exist independently with probability α log n/
−
1
−
ability β log n/
It can be veriﬁed that A1 holds for
(cid:1)
[2], hence the the entrywise guarantees of eigenvectors (Theorem 3) apply. Due to the
k
non-symmetric nature of the problem, a single eigenvector may not be an optimal estima-
tor. However, we believe that some linear combination of the two leading eigenvectors of the
similarity matrix would lead to an optimal spectral algorithm.

for a ﬁxed ρ
n
d
for ﬁxed constants α > β > 0.
(cid:0)

1
−
1
−

n
d

∈

∈

(cid:0)

(cid:1)

⌊

2. We show that the SDP is robust to a monotone adversary who operates on the similarity
matrix. What can we say about an adversary who instead operates on the hypergraph
directly, by adding intra-community edges and removing inter-community edges?

3.

W ∗k2. Their proof is similar in spirit
[DLS21, Theorem 4] provides a sharp bound on
to the combinatorial proof of Feige and Ofek [FO05], who consider a symmetric matrix with
independent entries. On the other hand, a simpler proof is known in this setting [HWX16a],
and is based on a symmetrization trick. Using the symmetrization strategy, we were able
to show that E [
W ∗k2 through
W
k
concentration?

. Can we then argue about

W ∗k2] = O

√log n

W
k

W
k

−

−

−

(cid:0)

(cid:1)

3 Optimality of the SDP relaxation

For completeness, we include the proof of Lemma 2.2, which establishes the uniqueness of the
optimal solution X ∗ = σ∗σ∗⊤ by a dual certiﬁcate strategy (see Appendix A). Here we focus on

9

showing that the choice of D mentioned in (4) and ν = 1 satisfy the conditions in Lemma 2.2 with
high probability whenever I(d, α, β) > 1 to show the theorem. We begin by noting a spectral norm
concentration result of Lee, Kim, and Chung for general random hypergraphs [LKC20].

Lemma 3.1 (Theorem 4, [LKC20]). Fix d

c0 log n
.
(n
1)
−
d
−
(G). Let W ∗ , E [W ]. Then there exists a constant C = C(d, c0) >

d ), where maxe pe ≤

. Let p
}

2, 3, . . .

[0, 1]([n]

∈ {

∈

1

Let G
0 such that

∼

H(d, n, p), and W =

S

P

k
(cid:16)

W

−

W ∗

k2 ≤

C

log n

1

−

≥

O(n−

11).

(cid:17)

p

∈
2, 3, . . .

Since hyperedges in the HSBM model appear with probability at most α log n/

, Lemma
HSBM(d, n, α, β). We now prove a lemma that plays an
3.1 applies to W =
important role in proving the theorem. Roughly speaking, it provides a probabilistic lower bound
on Dii deﬁned in (4) for any i

(G), where G

[n].

∼

S

(cid:0)

(cid:1)

n
d

1
−
1
−

Lemma 3.2. Let d
constant ǫ := ǫ(d, α, β) > 0 such that for any ﬁxed i

, and α > β > 0, such that I(d, α, β) > 1. Then there exists a
}

[n], with probability at least 1

∈ {

1),

o(n−

∈

−

Wijσ∗(i)σ∗(j)

ǫ log n.

≥

[n]
Xj
∈

Proof. Fix i
For each e
the set of potential edges incident on i.

[n], and let X ,
, let Ae be the indicator that edge e is present. Let
(cid:0)

[n] Wijσ∗(i)σ∗(j). Let
∈

∈
∈ E

P

:=

E

j

[n]
d

be the set of possible edges.
represent

: i

(i) :=
(cid:1)
E

e
{

∈ E

e
}

∈

For any edge e

(i), let ni(e) :=

∈ E

j
|{

∈

[n]

i
}
\ {

e that belong to the opposite community as i. We can rewrite X as follows

}|

: σ∗(i) = σ∗(j)

be the number of vertices in

X =

σ∗(i)σ∗(j)Ae

=

=

(i)

Xe
∈E

(i)

Xe
∈E

(i)

Xe
∈E

Xj
=i
e,j
∈
((d

1

−

ni(e))

−

−

ni(e)) Ae

(d

1

−

−

2ni(e)) Ae

(i) : ni(e) = r

has cardinality

}

Next, observe that for r

−

∈ {

∈ E

Nr :=

0, 1, . . . , d

, the set
1
}

e
{
n/2
−
1
d
−
r=0 be independent random variables, where Yr ∼
if r = 0;
r
if 1

n/2
r

qr =

(cid:19)(cid:18)

−

(cid:18)

−

d

1

1

1

α log n/(n
−
d
−
β log n/(n
−
d
−

1),
1),

(

≤

1
r

.

(cid:19)

Bin(Nr, qr), with

1.

d

−

≤

Let

Yr}
{

We then further rewrite X as follows:

1

d

−

X =

Xr=0 Xe
∈E

(i)

ni(e) = r

1{

(d

1

−

−

}

2ni(e)) Ae,

10

6
0. Exponentiating and applying the Markov inequality,

≥

so that X is equal to

Let hr = d

1

−

−

d
1
r=0(d
−
2r. Fix ǫ
P
P (X

≤

−

1
−
R and t

∈
ǫ log n)

2r)Yr in distribution.

tX

≥

tǫ log n

e−

(cid:17)

≤

P

e−
(cid:16)
E [e−
e−

tX ]
tǫ log n
≤
= etǫ log n E [e−

t Pd

1

−

r=0 hrYr ]

1

d

−

= etǫ log n

E [e−

thrYr ]

= etǫ log n

Yr=0
1
d
−

1
Yr=0 (cid:16)

qr(1

e−

−

−

Nr

thr )
(cid:17)

exp

tǫ log n

−

≤

Nrqr(1

e−

−

hrt)

!

.

(10)

1

d

−

Xr=0

Here, the second equality is due to independence of the Yr random variables, and the ﬁnal step
uses 1

x. Next,

x

e−

−

≤

Nr =

n/2
r

(cid:19)(cid:18)

(cid:18)

= (1 + o(1))

= (1 + o(1))

n/2
r

n/2
−
1
d
−

1
r

=

1
r

n/2
−
1
d
−
−
(n/2)r
(r) !
1
2d

1

−

(cid:18)

(cid:0)
(cid:19)
(n/2)d
(d
1
−
1

·
d

−
r

(cid:19)(cid:18)

−
1
n
−
(cid:1)(cid:0)
1
d
−
r
1
(cid:1)
(cid:0)
−
−
r) ! ·
1
1

−
n
d

−
−

(cid:19)

.

n
d

−
−
1)!
1

1
1

(cid:19)
n
d

(cid:18)

·

1
1

−
−

(cid:19)

(cid:18)

(cid:1)
(d

−
nd
−

Substituting into (10), we obtain

P (X

≤

ǫ log n)

≤

exp

tǫ log n

−

Let t = t∗(d, α, β), where

(1 + o(1))
2d

1

−

e−

(d

−

1)t) +

α(1

−

(cid:18)

1

d

−

β

r=1
X

(cid:18)

d

1

−
r

(1

(cid:19)

−

e−

(d

−

1

−

2r)t)

(cid:19)

log n

.

!

t∗(d, α, β) = arg max
0

t
≥

We then obtain

1
2d

−

1

α(1

−

e−

(d

−

1)t) +

1

d

−

β

r=1
X

(cid:18)

d

1

−
r

(1

−

(cid:19)

e−

(d

−

1

−

2r)t)

!

:= arg max
0

t
≥

ψ(t).

P (X

≤

ǫ log n)

≤

≤

exp

t∗ǫ log n

I(d, α, β) log n

−

n−

I(d,α,β)+t∗ǫ+o(1).
(cid:0)

o(log n)

−

(cid:1)

Note that t∗ 6
= (α
β)(d
−
−
suﬃciently small such that

= 0 as limt
1)/2d

→

−

0+ ψ′(t) = 1
2d

d
1
−
r=1
1 > 0. Furthermore, since I(d, α, β) > 1, one can choose ǫ = ǫ(d, α, β) > 0

1) + β

α(d

2r)

(d

−

−

−

−
r

(cid:16)

(cid:17)

1

d

−

1

1

P

(cid:0)

(cid:1)

P





[n]
Xj
∈

Wijσ∗(i)σ∗(j)

ǫ log n

≤

= o(n−

1).





11

 
 
 
Finally, we make some important observations about the structure of W ∗. Observe that W ∗
has a block structure (up to the diagonal entries). In particular, W ∗ is a zero diagonal symmetric
matrix whose non-diagonal entries are given by

W ∗ij =

p′ ,

q′ ,




1

α log n
(n
1)
−
d
−
β log n
(cid:1)
,
(n
1)
−
d
−

1

2

n/2
d

−
2

n
(cid:0)
d

−
2
−
2
−

(cid:0)

(cid:1)

Observe that W ∗ can be decomposed as


+

n
d

2
−
2
−

−

(cid:1)

(cid:0)

(cid:16)(cid:0)

n/2
d

−
2

−

,

β log n
(n
1)
−
d
−

1

2

(cid:1)(cid:17)

if σ∗(i) = σ∗(j);

if σ∗(i)

= σ∗(j).

W ∗ =

p′ + q′
2

(cid:18)

11⊤ +

(cid:19)

(cid:18)

q′

p′ −
2

(cid:19)

σ∗σ∗⊤

p′I.

−

(11)

(12)

Proof of Theorem 1. The proof uses ideas from the proof of [HWX16a, Theorem 2]. Let ν = 1 and
S , D + ν11⊤ −
W . The goal is to show that S satisﬁes the conditions mentioned
in Lemma 2.2 with high probability whenever I(d, α, β) > 1. Observe that, by deﬁnition of D in
[n] Wijσ∗(j); i.e. Dσ∗ = W σ∗. Therefore, using the
(4), for any i
∈
fact that

W = D + 11⊤ −
[n], we have Diiσ∗(i) =
= 0, we get

j

∈
1, σ∗i
h

P
Sσ∗ = Dσ∗ + 11⊤σ∗

W σ∗ = 0.

−

Therefore, it remains to show that

For any x

⊥

σ∗ such that

P

⊥

x
(cid:18)(cid:26)
k2 = 1,

x
k

inf
x
k

σ∗:

2=1
k

x⊤Sx > 0

(cid:27)(cid:19)

1

−

≥

o(1).

x⊤Sx = x⊤Dx + x⊤11⊤x

= x⊤Dx + (1⊤x)2

x⊤(W

−
x⊤(W

W ∗)x

−
W ∗)x

−

−

−

x⊤W ∗x
q′
p′ −
2

−

(cid:18)

(cid:19)

= x⊤Dx +

1

(p′ + q′)/2

(1⊤x)2

−

x⊤(W

−

−

≥

≥

x⊤Dx + p′
(cid:0)
−
W
Dii − k
min
[n]
i
∈

x⊤(W
W ∗

−

−
k2 .

W ∗)x
(cid:1)

(by the deﬁnition of

(x⊤σ∗)2

p′ + q′
2

(1⊤x)2 + p′

−
(using (12) to substitute W ∗)

(cid:18)

(cid:19)

W ∗)x + p′

(since x

σ∗)

⊥

(since p′, q′ = Θ(log n/n) are vanishing)
0)
k2 for matrices and the fact p′ ≥
.
k

o(1). Moreover, applying Lemma 3.1,

11). Therefore, one can conclude that x⊤Sx
σ∗, completing the proof.

We now use Lemma 3.2 and take a union bound over i to obtain mini
probability 1
−
O(n−
1
−
k2 = 1 and x
x
k
We additionally show that the SDP is robust to a monotone adversary. Here, we consider an
adversary who can increase the value of Wij for any (i, j) in the same community, and decrease the
value of Wij for any (i, j) in opposite communities. The robustness of SDPs to monotone adversaries
is well-known (see e.g. [FK00, FK01]). While the monotone adversary appears to provide helpful
information, spectral algorithms generally fail under such a semirandom model.

ǫ log n with
[n] Dii ≥
C√log n with probability
C√log n > 0 for any x such that

W ∗k2 ≤
−

W
k
−
ǫ log n

≥

⊥

∈

Lemma 3.3. Consider the modiﬁed SDP based on (2), with W replaced by
if σ∗(i) = σ∗(j), and
optimal solution to the modiﬁed SDP.

Wij
= σ∗(j). If I(d, α, β) > 1, then X ∗ = σ∗σ∗⊤ is the unique

Wij ≥
f

Wij if σ∗(i)

W such that

f

Wij ≤
f

12

6
6
Proof. Let X be a feasible solution to the modiﬁed SDP, and let X ∗ be the unique optimal solution
to (2), which is guaranteed by Theorem 1. Due to uniqueness of X ∗, we have
.
W, X ∗i
h
Since X

0, we can write

W, X
h

<

i

(cid:23)

n

X =

λlvlv⊤l

Xl=1

as its eigendecomposition, where λl ≥

0 for all l. Then by the Cauchy–Schwarz inequality,

X 2

ij =

λlvl,ivl,j

n

Xl=1
n

2

!

n

λlv2
l,i

λlv2
l,j

!

!  

Xl=1

≤  
Xl=1
= Xii ·
= 1.

Xjj

Therefore,
Consequently,

Xij| ≤
|

1 for all i, j, which implies

W
h

−

W, X

i ≤

W , X
h

=

W, X
h

+

W
h

i
establishing the unique optimality of X ∗.

−

i

f

f

W, X

f
<

W, X ∗
h

i

P

+

W
h

i

−

Wij|

=

W
h

.
W, X ∗i

−

i,j

[n] |
∈

Wij −
f
W, X ∗

=

W , X ∗
h

i

f

,
i

f

f

One could also consider another natural monotone adversary that operates directly on the
underlying hypergraph instead of on its similarity matrix. More speciﬁcally, the adversary is
allowed to add intra-community edges and delete cross-community edges. It is not immediately
clear whether an SDP algorithm would be robust to such an adversary.

4 Optimal spectral recovery

In Section 4.1, we prove the optimality of Algorithm 1 (Theorem 2). In Section 4.2, we present the
proof of our general entrywise eigenvector bound (Theorem 3).

4.1 Analysis of spectral algorithm

We ﬁrst derive a corollary of Theorem 3, speciﬁc to HSBMs.

Corollary 4.1. Fix d
Then with probability at least 1

∈ {

2, 3, . . .

}
O(n−

3),

−

and α > β > 0. Let G

HSBM(d, n, α, β) and W =

(G).

S

∼

where c := c(d, α, β) is some positive constant that only depends on d, α, and β.

s∗

min
u2 −
1
s∗∈{±
} (cid:13)
(cid:13)
(cid:13)
(cid:13)

W u∗2
λ∗2 (cid:13)
(cid:13)
(cid:13)
(cid:13)

∞

c
√n log log n

,

≤

Proof. Since each hyperedge exists with probability at most α log n/
, we can set c0 in Theorem
3 to α. We now verify A1. Let W ∗ be the expectation of W conditioned on σ∗, whose entries are
(cid:0)
p′ and q′ as given by (11). By (12), W ∗ + p′I is a rank-2 matrix.
Its non-zero eigenvalues are

(cid:1)

n
d

1
−
1
−

13

 
(p′ + q′)n/2 and (p′ −
given by

q′)n/2. Accounting for the diagonal matrix p′I, the eigenvalues of W ∗ are

λ∗1 = (1 + o(1))(p′ + q′)n/2 and λ∗2 = (1 + o(1))(p′

q′)n/2.

−

Since p′ ≍

q′ ≍

log n/n, we have λ∗1, λ∗2 ≍

log(n). Furthermore,

∆∗2 = min

λ∗1 −
{

λ∗2, λ∗2 −

0
}

= (1 + o(1)) min

q′n,

(cid:26)

q′)n

(p′ −
2

log n,

≍

(cid:27)

hiding constants in α, β and d. Therefore, Theorem 3 applies.
u∗2k∞
k

It remains to verify that

W ∗ + p′I is

1
√n σ∗ by (12). Hence

= O(1/√n). We already know that the second eigenvector of

u∗2 =

1 + o(1)
√n

σ∗.

(13)

Proof of Theorem 2. Let us ﬁx s = s∗ for which Corollary 4.1 holds. Using the corollary, with
probability 1

o(1),

−

√n min
[n]
i
∈

sσ∗(i)u2,i ≥

√n min
[n]
i
∈

s2σ∗(i)(W u∗2)i/λ∗2 −

c(log log n)−

1,

(14)

where c is deﬁned in Corollary 4.1. Note that s2 = 1. Also, using (13),

√nσ∗(i)(W u∗2)i = (1 + o(1))

Wijσ∗(i)σ∗(j).

[n]
Xj
∈

By Lemma 3.2, if I(d, α, β) > 1, then there exists a positive constant ǫ(d, α, β) > 0 such that for a
1). Therefore, a union bound
ǫ log n with probability 1
ﬁxed i
j
implies that with probability 1
P

[n] Wijσ∗(i)σ∗(j)
∈

≥
o(1),

o(n−

[n],

−

−

∈

√n min
[n]
i
∈

s2σ∗(i)

(W u∗2)i

λ∗2 ≥

(1 + o(1))ǫ log n.

Since λ∗2 ≍

log(n), (14) implies that there exists η > 0 such that

√n min
[n]
i
∈

s∗σ∗(i)(u2)i ≥

(1 + o(1))ǫ log n/λ∗2 −

c(log log n)−

1 > η,

with probability 1

o(1).

−

4.2 Entrywise analysis

We begin by recalling the setup of Theorem 3. For simplicity, let λ = λk, λ∗ = λ∗k and ∆∗ = ∆∗k,
[n],
u, u∗i
dropping the subscript k. Let s = sgn(
su, u∗i ≥
∈
h
h
(G(m)).
let G(m) denote the hypergraph after deleting all the edges incident on m and W (m) =
S
Let (λ(m), u(m)) be its k-th eigenpair. Let s(m) = sgn
s(m)u(m), u∗i ≥
u(m), u∗i
0. The
h
h
notation . and
hide constants in d, c0, and c1 (deﬁned as in Theorem 3) throughout this section.
Before proving the theorem, we require some additional observations and results.

0. Also, for any ﬁxed m

), so that

, so that

≍

Observation 4.2.

W ∗k2

→∞

k

. log n
√n .

14

Proof. Recall that maxe pe ≤
variables, E [Wij] = W ∗ij . log n/n. Therefore,

c0 log n
(n
1)
−
d
−

1

. Since each entry Wij is a sum of

Bernoulli random

n
d

2
−
2
−

(cid:0)

(cid:1)

W ∗
k

k2

→∞

= max

W ∗i

·k2 . √n max

i k

W ∗ij|
|

.

log n
√n

.

Deﬁne the function ϕ : R+ →

R+ such that ϕ(x) = (4c0+16)d

(1

log(1/x)) , where c0 is as in the statement
∨

of Theorem 3. We note the following properties of ϕ(
).
·
Observation 4.3. ϕ(x) is non-decreasing and ϕ(x)/x is non-increasing on R+.

The next result provides a probabilistic upper bound on the inner product of a row of W

W ∗
and a ﬁxed vector v, in terms of the function ϕ(
). Intuitively, one can think that the rate of growth
·
) essentially controls the strength of concentration bound. Formally,
of ϕ(
·

−

Lemma 4.4 (Row concentration). For any m

[n] and any ﬁxed non-zero v

P

(cid:18)

(W
|

W ∗)m
·

v

v
| ≤ k

−

k∞

ϕ

(cid:18)

∈
v
k
√n

k2
v
k

k∞ (cid:19)

log n

(cid:19)

O

1

−

≥

1
n4

(cid:18)

(cid:19)

Rn,

∈

.

Proof. Recall the deﬁnition of
X , (W
W ∗)m
·

−

E

v. It is convenient to rewrite X as a sum of random variables

(m) =

e
{

∈ E

: m

. Let v
e
}

∈

∈

Rn be a ﬁxed vector. Let
Ae}e
{

(m):

∈E


Without loss of generality, assume that

inequality, for any δ, t > 0,

v
k

k∞

X =



(m)

Xe
∈E

m
e
Xj
\{
∈

E[Ae]).

(Ae −

vj
}

= 1
d (otherwise, v may be scaled). By Markov’s

P (X

δ)

≥

≤

P

etX
(cid:16)

etδ

≥

≤

(cid:17)

tδ

e−

E

et(Pj
(cid:16)

(m)

Ye
∈E

e

\{

m

}

∈

vj )(Ae

−

E[Ae])

.

(cid:17)

(15)

(m), we can bound the logarithm of the moment generating function as follows:

log

E

et(Pj

e

\{

m

}

∈

vj )(Ae

−

E[Ae])

For e

∈ E

(cid:16)
= log

h
E

(cid:16)

h

et(Pj

e

\{

m

}

∈

vj )Ae

i(cid:17)

i(cid:17)
t

−



= log

1

pe + peet Pj

∈

e

\{

m

}

vj

−

(cid:16)
et Pj

pe

≤

(cid:16)

vj

e

\{

m

}

∈

tpe

−

−

1

(cid:17)

m
e
Xj
\{
∈

}

m
e
Xj
\{
∈

}

E[Ae]

vj


tpe 

vj,

m
e
Xj
\{
∈

}

vj


(16)



−

(cid:17)

where we have used the fact that log(1 + x)
that ex
2 er for

1 + x + x2

≤

x
|

| ≤

≤

r to further upper-bound (16) by

x for x > 1 in the last step. Next, we use the fact

1 + t

pe 


m
e
Xj
\{
∈

}

vj +

k∞

v

etd
k
2

t2

·



m
e
Xj
\{
∈

}


15

2

vj


1


−

−

tpe



vj

m
e
Xj
\{
∈

}

= pe

et
2 ·

t2

2

vj






m
e
Xj
\{
∈

}

etpmaxt2
2

d

·

≤

2,

vj

m
e
Xj
\{
∈

}

where maxe pe , pmax.

Substituting our bounds on the log of moment generating functions into (15), we obtain

log(P (X

δ))

≥

≤ −

tδ +

etpmaxd
2

tδ +

≤ −

etpmaxd
2

·

t2

2

vj

(m)

Xe
∈E
n
t2
d

(cid:18)

}

Xj
m
e
\{
∈
2
2

v
k

(cid:19)

−
−

2
2 ,

k

where the last step follows from the fact that each j
√n
v

. Using the fact that (1

(m). Let t = 1

log

d

E

= m appears in

log x)2

∨

≤

2
n
−
2
d
−
x for x

(cid:0)

(cid:1)
≥

potential hyperedges in

1,

W

k

k2

(cid:17)

(cid:16)
log(P (X

δ))

≥

≤ −

tδ +

etpmaxd
2

d

√n
v

n
d

2
2

−
−

(cid:19)

2
2 .

v
k

k

k

k2 (cid:18)

Observe that
e1+log( √n

k2

)

d

k

v

v
k

k2 ≤
e√n
v
d
2
k

k

√n

v

k
. Hence

≤

= √n/d, so that log

k∞

(cid:16)

√n
v

k

2
k

d

0. Therefore, et = e1
∨

log( √n
v
d
k2

k

)

≤

≥

(cid:17)

log(P (X

δ))

≥

≤ −

tδ +

Let a = 8/c0, and set δ = t−

1d−

epmaxd
2

√n
v

·

d

k2
1(2 + a)pmaxn

k

√n
v

d

k
2
−
2
−

n
d

n
d

2
2

−
−

2
2 =

v

k

k

−

tδ +

(cid:19)

k2 (cid:18)
. We then obtain the bound

etpmaxn
2d

n
d

2
−
2
−

.

(cid:0)

(cid:1)

log(P (X

By replacing v with

≥

−

δ))

≤ −

(2 + a)pmaxn
d

(cid:0)
n
d

(cid:1)
2
2

(cid:19)

−
−

(cid:18)

+

epmaxn
2d
(cid:0)

n
d

2
−
2
−

apmaxn
d

n
d

2
−
2
−

.

(cid:0)

(cid:1)

≤ −

(cid:1)

v, we obtain a similar bound for the lower tail. The union bound gives

P (

X
|

| ≥

δ)

≤

2 exp

apmaxn
d

n
d

2
−
2
−

.

(cid:0)

(cid:1)

!

 −

Substituting in the value of t and using pmax ≤

c0 log n
(n
1)
−
d
−

1

,

P

(W
|



−

W ∗)m
·

v

| ≥

Finally, substituting the value of a,



(2 + a)2c0 log n

1

log

W

(cid:16)

√n
v

k

k2

d



≤



(cid:17)

2e−

ac0 log n
2

.

P

(W
|



−

W ∗)m
·

v





≥

1

−

2n−

4.

4(c0 + 4) log n

√n
v

k

k2

d

| ≤

1

log

W

(cid:16)

16



(cid:17)

6
Recalling that

v

k

k∞

= 1/d yields:

Since ϕ

k
√n

(cid:16)

Lemma 4.5.

v

k

k2
v
k∞ (cid:17)
W

P

(W
|



−

W ∗)m
·

v

| ≤



= (4c0 + 16)d/(1

log

∨

W (m)

−

W
k

k2

→∞

1

log

W
√n
v
k
k∞
v
k2

k

(cid:16)
.

(cid:17)

2 .
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(4c0 + 16)d

log n

v
k
√n

k∞
v
k
k∞
v
k2

k



≥

1

−

2n−

4.

(cid:16)
(cid:17)
, the lemma follows.



Proof. Note that W
(m). Consider any i

W (m) is the similarity matrix of the graph with edges present only from
e, the label of e is added to exactly

−
= m. Since for any e

E
(d

1) entries of the i-th row of W

−

−

(m) with
W (m), we have

∈ E

i, m
{

} ⊆

Therefore

(W

−

(cid:13)
(cid:13)

W (m))i

·

2 ≤
(cid:13)
W
(cid:13)
k

−

(W
k

(W

−

W (m))i

−
W (m))i

(cid:13)
W (m)
(cid:13)

W

k2 ≤ k
=

1 ≤
W (m)

·

(cid:13)
(cid:13)
−

−

kF

·k1 = (d
(d

1)Wim.

−
1)Wim. Hence we get

(17)

(W

W (m))i

2
2

·

−

sXi
[n]
(cid:13)
∈
(cid:13)
Wm

k

s

2
2 +
·k

Wm
k

·k

q

≤

.

(cid:13)
(cid:13)
(d

1)2W 2
im

−

.

m

}

[n]
Xi
\{
∈
W

2
2 ≤ k

k2

→∞

Let γ = γn := C/√log n, where C = C(d, c0) > 0 is the constant from Lemma 3.1. We deﬁne

the following event:

F0 :=

By Lemma 3.1, P (F0)

1

−

≥

O(n−

W

W ∗

{k
−
11). Also,

k2 ≤

γ log n = C

log n

.
}

p

γ = o(1) and ϕ(γ) .

1

1

(cid:18)

∨

log √log n

(cid:19)

.

1
log log n

= o(1).

(18)

We now derive some bounds on important quantities conditioned on the above event.

Lemma 4.6. Conditioned on F0, we have

λ
λ∗| ≍ |
|

| ≍

log n.

Proof. Conditioned on the event F0, Weyl’s inequality implies
Therefore, λ
log n.

γ log n, λ∗ + γ log n]. A1 (i.e.

λ∗| ≍
|

[λ∗ −

∈

λ∗| ≤ k
log n) then ensures that

λ
|

W

−

−

γ log n.
W ∗k2 ≤
λ
λ∗| ≍ |
|

| ≍

Lemma 4.7. Conditioned on F0,

W

k

k2

→∞

. γ log n and for all m

∈

[n],

W (m)

(cid:13)
(cid:13)
(cid:13)

W ∗

−

2
(cid:13)
(cid:13)
(cid:13)

. γ log n.

17

6
Proof. By the triangle inequality,

W

k

k2

→∞ ≤ k

W

W ∗

k2 +

k

W ∗

−

k2

→∞ ≤

γ log n +

W ∗
k

k2

→∞

. γ log n,

where we use Observation 4.2 to get the last inequality. Similarly, using the triangle inequality and
Lemma 4.5,

W (m)
k

−

W ∗

k2 ≤ k

W

W ∗

k2 +

W
k

−

−

W (m)

k2 .

W
k

W ∗

k2 +

W
k

−

k2

→∞

. γ log n.

Having derived the above bounds, we now bound the ℓ2 and ℓ

u∗) and
s(m)u(m)) using two variants of the Davis and Kahan sin(θ) theorem. For completeness, we

(su
include the less well-known variant here, which is a special case of [DLS21, Theorem 3].

norms of (s(m)u(m)

−

−

∞

n be
Proposition 4.8 (Generalized Davis and Kahan sin(θ) theorem [DLS21]). Let M
a symmetric matrix and let X be the matrix that has the eigenvectors of M as columns. Then M can

∈

×

Rn

be decomposed as M = XΛX ⊤ = X1Λ1X1⊤ + X2Λ2X2⊤, where X = [X1 X2] and Λ =

Λ1
0
0 Λ2(cid:21)
is the absolute separation of some ˆλ from Λ2, then for any vector ˆu

(cid:20)

.

Suppose δ = mini |
we have

(Λ2)ii −

ˆλ

|

sin(θ)

(M
k

ˆλI) ˆu
−
δ

k2

,

≤

where θ is the canonical angle between the span of X1 and ˆu.

Lemma 4.9. Conditioned on F0,

s(m)u(m)

max
m
∈

[n] k

u∗

k2 . γ,

−

su

max
m
∈

[n] k

s(m)u(m)

−

s(m)u(m)

max
m
∈

[n] k

u∗

−

k∞

k2 . (γ
. (
k

u

),

u

k∞

∧ k

+

u∗
k

k∞

).

k∞

(19)

(20)

(21)

Proof. For any m
3] to get

∈

[n], we apply a variant of the Davis–Kahan sin(θ) theorem [YWS14, Corollary

s(m)u(m)
k

−

u∗

k2 ≤

23/2

W (m)
∆∗

(cid:13)
(cid:13)

W ∗

−

2

.

γ log n
log n

= γ,

(cid:13)
(cid:13)

where the second inequality follows from Lemma 4.7 and A1, concluding the proof of (19). Similarly,
we also get

su
k

−

s(m)u(m)

k2 ≤ k

su

u∗

k2 +

−

s(m)u(m)
k

u∗

k2 . k

−

W

W ∗k2
−
∆∗

+ γ . γ.

(22)

Therefore,

su, s(m)u(m)
h

i ≥

0. Let θ denote the angle between su and s(m)u(m). Then

su
k

−

s(m)u(m)

k2 ≤

2
2 +
u
k
k
q

u(m)
k

2
2 −
k

u
2
k2k
k

u(m)

k2 cos θ

≤

2

−

2 cos2θ = √2 sin θ.

We then apply Proposition 4.8 with M = W (m), X1 = [u(m)] and (ˆλ, ˆu) = (λ, u):

p

s(m)u(m)

su
k

−

k2 ≤

√2 sin θ

≤

√2
λ(m)
(cid:13)
k+1 −
|
(cid:13)

λ

(W (m)

λ)u

−
λ

2
λ(m)
(cid:13)
1|
k
(cid:13)
−

| ∧ |

−

18

∆∗ −
Using A1, the deﬁnition of F0, and Lemmas 4.5, 4.7, we can lower-bound the denominator by

W
2
(cid:13)
k
(cid:13)

k2

−

−

W (m)

≤

(λk+1 −

λk)

.

√2

∧
(W

−

(W (m)
λk
(λk −
(cid:13)
−
(cid:13)
W (m))u
−
W
W ∗k2 − k
(cid:13)
(cid:13)

W )u
1)

2
W
(cid:13)
− k
(cid:13)
2

W (m)

k2

−

.

∆∗

W
2
k

−

−

W ∗

W

k2 − k

−

W (m)

k2 & (1

−

γ) log n.

Since γ = o(1) by (18), we obtain

su
k

−

s(m)u(m)

(W

k2 . k

W (m))u
log n

−

k2

.

W (m))u. We have already seen that the m-th row of W

W (m) is the same as W ,
Let v = (W
so that vm = λum. Therefore, we bound the m-th entry of v and the rest of its entries separately.
Formally,

−

−

vm| ≤ |
|
vi| ≤ |
|

[W u]m|
[(W
−

λ
um| ≤ |
=
λ
||
|
W (m))u]i| ≤ k

| k
(W

u

,
k∞
W (m))i

−

u
·k1 k

k∞

. Wim k

u

k∞

, for i

= m,

where the last inequality follows from (17). Therefore,

v
k

k2 .

λ2 +

k

u
k∞ s

=m
Xi

W 2

im .

u

k∞

k

2 +

λ∗|
|
q

W
k

2
2
k

→∞

,

where the last step requires Lemma 4.6. Substituting the bound (23):

s(m)u(m)

su
k

−

k2 . k

v
k2
log n

u
. k

k∞

2 +
λ∗|
|
log n

q

W

k

2
2
k

→∞

.

u
k

k∞

,

(23)

(24)

where the last inequality follows from Lemma 4.7 and A1. Observe that (22) and (24) together
imply (20). Finally, to prove (21), we apply the triangle inequality again and use (24).

s(m)u(m)
k

−

u∗

su

k∞ ≤ k

−

s(m)u(m)

k2 +

k

u

k∞

+

u∗
k

k∞

.

k

u

k∞

+

u∗

k

k∞

,

concluding the proof.

Lemma 4.10. With probability at least 1

O(n−

3),

−

(W

k

−

W ∗)u∗

.

u∗
k

k∞

k∞

log n and

W u∗

.

u∗
k

k∞

k∞

k

log n.

−

u∗| ≤

W ∗)m
·

Proof. To prove the ﬁrst inequality, we apply Lemma 4.4. For each m
u∗k2
u∗k∞
)
(W
k
√n
u∗k∞
k
|
k
√n
(Observation 4.3) and
u
u∗k2 ≤
k
k
ϕ(1) = O(1). Taking a union bound over all m
with probability at least 1

−
W ∗)m
·
(W
k

(W
|
[n], we have

log n with probability 1

u∗| ≤
−

, we obtain

O(n−

k∞

3).

ϕ(

−

∈

O(n−

[n], we have

∈

4). Using the monotonicity of ϕ
log n. Note that
log n,
u∗k∞
ϕ(1)

u∗k∞
ϕ(1)
k
W ∗)u∗k∞ ≤

k

To get the second inequality, we use the triangle inequality as follows.

−

W u∗

W ∗u∗

k

k∞ ≤ |
k∞
The last inequality in the above follows from A1.

k∞ ≤ k

−

+

(W
k

W ∗)u∗

λ∗

u∗

| k

k∞

+ ϕ(1)

u∗

k

k∞

log n .

u∗

k

k∞

log n.

19

6
6
Lemma 4.11. With probability at least 1

O(n−

3),

−

max
m
∈

[n] |

Wm
·

v(m)

. (γ + ϕ(γ))(
k

|

u

k∞

+

u∗
k

k∞

) log n.

Proof. We know that P (F0)

1

O(n−

11) by Lemma 3.2. Conditioned on F0, for all m

[n]

∈

v(m)

Wm
|
·

| ≤ |

−
v(m)

≥
W ∗m
·
W ∗
k2 k
γ log n
+
√n

≤ k
.

+
|
v(m)

(W
|
k2 +

(W
|

−

v(m)

W ∗)m
·
W ∗)m
·

−
v(m)

−
(W
|
W ∗)m
·

|

|
v(m)

(by the triangle inequality)
k2)
.
(by Observation 4.2 and (19))

(by deﬁnition of

k

|

−
We now focus on bounding the second term. We know that v(m) is independent of randomness in
the m-th row of W . Therefore, by the row concentration (Lemma 4.4), for a ﬁxed m

k∞

[n]

≤

γ

u∗
k

log n +

(W
|

W ∗)m
·

v(m)

.
|

(25)

∈

(W
|

−

W ∗)m
·

v(m)

v(m)

| ≤ k

ϕ

k∞

v(m)
k2
k
v(m)
√n
k∞ !
k

log n,

(26)

∈

[n]. By a union bound, P (F )

holds with probability at least 1
all m
two diﬀerent cases.
Case 1 : Suppose
k∞ ≤
(Observation 4.3) in (26) to get:

v(m)
2
k
k
v(m)
√n
k

−

O(n−

4). Let F be the event that (26) holds simultaneously for
3). Conditioned on the event F , let us consider
1

O(n−

−

≥

γ. Under this case, we use the fact that ϕ(x) is non-decreasing

Case 2 : Suppose

v(m)
2
k
k
v(m)
√n
k

k∞

(W
|

−

W ∗)m
·

v(m)

v(m)
ϕ(γ)
k

k∞

| ≤

log n.

> γ. In this case, multiplying and dividing (26) by 1,

(W
|

−

W ∗)m
·

v(m)

v(m)

| ≤ k

log n

ϕ

k∞

√n

v(m)
k2
k
v(m)
√n
k∞ !
k
v(m)
k
v(m)
k
v(m)
k2
k
√n

k∞
k2

log n.

k2

ϕ

= k

v(m)
√n

ϕ(γ)
γ

≤

v(m)
k2
k
v(m)
√n
k∞ !
k

log n

(by Observation 4.3; ϕ(x)/x is non-increasing.)

Combining both the cases, for all m

[n]:

∈

(W
|

−

W ∗)m
·

v(m)

| ≤

ϕ(γ) log n

v(m)

 k

k∞ ∨

v(m)
k2
k
γ√n !

.

Substituting bounds from (19) and (21) in the above, and then using the fact that
u∗k2 /√n = 1/√n, we obtain
k

u∗k∞ ≥
k

(W
|

−

W ∗)m
·

v(m)

|

. ϕ(γ) log n

(
k

u

k∞

+

u∗

k

k∞

)

∨

(cid:18)

γ
γ√n

(cid:19)

. ϕ(γ) (

u
k

k∞

+

u∗
k

k∞

) log n.

20

 
 
 
Finally, we substitute into (25):

max
m
∈

[n] |

Wm
·

v(m)

|

. γ

u∗

k

k∞
(γ + ϕ(γ))(
k

log n + ϕ(γ)(
k

u

k∞

+

u∗

k

k∞

) log n

u

k∞

+

u∗

k

k∞

) log n,

≤

concluding the proof.

We ﬁnally prove the theorem.

Proof of Theorem 3. Recall the deﬁnition of the event F0 and P (F0)
on F0, we have

1
by Lemma 4.6; we use this throughout the proof.

≥

−

O(n11). Conditioned

u∗k∞

k

as we need our ﬁnal bounds only in terms of the latter.

We ﬁrst bound

in terms of

By the triangle inequality,

λ
|

λ∗|
k∞

| ≍ |
u
k

u
k

k∞

=

sW u
λ

W u∗
λ

+

W (su
−
λ

u∗)

.

≤

W u∗
k

k∞

+ max
m
∈

[n] |

Wm
·

(su

−

u∗)
|
(cid:19)

≤

[n] |

(su

(cid:13)
(cid:13)
(cid:13)
(cid:13)
k∞

∞
W u∗

(cid:13)
(cid:13)
(cid:13)
Wm
(cid:13)
·

(cid:13)
∞
(cid:13)
(cid:13)
+ max
(cid:13)
m
∈

(cid:13)
∞
(cid:13)
(cid:13)
s(m)u(m))
(cid:13)
|

(cid:13)
(cid:13)
(cid:13)
(cid:13)
1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
λ∗| (cid:18)
k
|
1
max
λ∗| (cid:18)
m
|
∈
In light of Lemma 3.1, P (F0) = 1
11). We substitute the derived bounds from Lemma 4.10
in the ﬁrst term, Lemmas 4.7 and 4.9 in the second term, and Lemma 4.11 in the third term. In
particular, we obtain that with probability at least 1

k2 + max
[n] |

u∗)
|
(cid:19)

u∗)
|
(cid:19)

(s(m)u(m)

(s(m)u(m)

s(m)u(m)

Wm
·

Wm
·

O(n−

W
k

W u∗

[n] k

[n] |

k∞

3),

→∞

k2

su

−

−

−

−

≤

−

+

m

k

∈

.

O(n−

1
λ∗| (cid:18)
|
+ max
m
∈

−

u
k

k∞

.

.

u∗

1
λ∗| (cid:16)
|
(1 + γ + ϕ(γ))

k∞

k

(cid:16)

u∗

k

k∞

+ (γ + ϕ(γ)

u

k∞

k

.

(cid:17)

log n + (γ log n)

u
k

k∞

+ (γ + ϕ(γ))(
k

u

k∞

+

u∗

k

k∞

) log n

Using the fact that γ, ϕ(γ) = o(1) from (18), we have that for some constant c3 > 0,

(cid:17)
(using A1)

o(1))

(1

−

k

k

k

u

u

k∞ ≤
k∞ ≤

u

k∞ ≤

c3 k
c3 k
1

k∞
k∞

u∗
u∗
c3
o(1) k

−

+ o(1)

u

k∞

k

u∗

.

u∗
k

k∞

.

k∞

(27)

Similarly, conditioned on F0, we bound the quantity of interest. Using the triangle inequality,

−

su

(cid:13)
(cid:13)
(cid:13)
(cid:13)

W u∗
λ∗ (cid:13)
(cid:13)
(cid:13)
(cid:13)

∞

=

≤

W u∗

W u∗
λ

+

W u∗
k

k∞

W u∗
λ∗ (cid:13)
(cid:13)
(cid:13)
W (su
(cid:13)
−
k

∞

λ −
1
λ
|

|

+

u∗)

k∞

.

sW u

λ −
1
λ∗ (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:13)
(cid:13)
1
(cid:13)
(cid:13)
λ −
(cid:12)
(cid:12)
(cid:12)
(cid:12)

W u∗

+

W (su
k

−

k∞

k

u∗)

k∞

)

Recalling that

λ
|

su

−

(cid:13)
(cid:13)
(cid:13)
(cid:13)

W u∗
λ∗ (cid:13)
(cid:13)
(cid:13)
(cid:13)

∞

,
λ∗|

(γ

| ≍ |
1
λ∗|
|
1
λ∗| (cid:18)
|

.

.

γ

W u∗
k

k∞

+

W
k

k2

→∞

su

max
m
∈

[n] k

s(m)u(m)

−

21

k2 + max
[n] |

m

Wm
·

∈

(s(m)u(m)

−

.

u∗)
|
(cid:19)

We again substitute the bounds from Lemma 4.10 in the ﬁrst term, Lemma 4.7 and 4.9 in the
second term, and Lemma 4.11 in the third term. In particular, we obtain that with probability at
least 1

3),

O(n−

−

−

su

(cid:13)
(cid:13)
(cid:13)
(cid:13)

W u∗
λ∗ (cid:13)
(cid:13)
(cid:13)
(cid:13)

∞

.

.

γ

u∗
k

1
λ∗| (cid:16)
|
(γ + ϕ(γ))

k
u∗

k∞

(cid:16)

. (γ + ϕ(γ))
u∗k∞
log log n

. k

.

k

log n + (γ log n)

u
k

k∞

+ (γ + ϕ(γ))(
k

u

k∞

+

u∗
k

k∞

k∞

u∗

+ (γ + ϕ(γ)

k

k∞

u

k∞

) log n

(cid:17)
(using A1)

(cid:17)
(using

u

k

.

u∗k∞

as shown in (27))

k

k∞
(γ = O(ϕ(γ)) and using (18))

This is precisely the bound we need, concluding the proof.

References

[Abb17]

Emmanuel Abbe. Community detection and stochastic block models: recent develop-
ments. The Journal of Machine Learning Research, 18(1):6446–6531, 2017. 1

[ABBS14]

Emmanuel Abbe, Afonso S Bandeira, Annina Bracher, and Amit Singer. Decoding
binary node labels from censored edge measurements: Phase transition and eﬃcient
recovery. IEEE Transactions on Network Science and Engineering, 1(1):10–22, 2014.
9

[ABH16]

Emmanuel Abbe, Afonso S. Bandeira, and G. Hall. Exact recovery in the stochastic
block model. IEEE Transactions on Information Theory, 62:471–487, 2016. 1

[AFWZ20] Emmanuel Abbe, Jianqing Fan, Kaizheng Wang, and Yiqiao Zhong. Entrywise eigen-
vector analysis of random matrices with low expected rank. Annals of statistics, 48
3:1452–1474, 2020. 1, 3, 7, 8, 9

[ALS18]

[AS15a]

[AS15b]

Kwangjun Ahn, Kangwook Lee, and Changho Suh. Hypergraph spectral clustering
in the weighted stochastic block model. IEEE Journal of Selected Topics in Signal
Processing, 12(5):959–974, 2018. 2

Emmanuel Abbe and Colin Sandon. Community detection in general stochastic block
models: Fundamental limits and eﬃcient algorithms for recovery. In 2015 IEEE 56th
Annual Symposium on Foundations of Computer Science, pages 670–688, 2015. 1

Emmanuel Abbe and Colin Sandon. Recovering communities in the general stochastic
block model without knowing the parameters.
In Advances in Neural Information
Processing Systems, volume 28, 2015. 1

[BBKY13] Derek Bean, Peter J. Bickel, Noureddine El Karoui, and Bin Yu. Optimal m-estimation
in high-dimensional regression. Proceedings of the National Academy of Sciences,
110(36):14563–14568, 2013. 8

[BKN11]

Brian Ball, Brian Karrer, and M. E. J. Newman. Eﬃcient and principled method for
detecting communities in networks. Phys. Rev. E, 84:036103, Sep 2011. 1

22

[CLW19]

I Eli Chien, Chung-Yi Lin, and I-Hsiang Wang. On the minimax misclassiﬁcation
ratio of hypergraph community detection. IEEE Transactions on Information Theory,
65(12):8095–8118, 2019. 2

[CY06]

[CZ20]

[DF89]

Jingchun Chen and Bo Yuan. Detecting functional modules in the yeast pro-
tein–protein interaction network. Bioinformatics, 22(18):2283–2290, 07 2006. 1

Sam Cole and Yizhe Zhu. Exact recovery in the hypergraph stochastic block model:
A spectral algorithm. Linear Algebra and its Applications, 593:45–73, 2020. 3

Martin E. Dyer and Alan M. Frieze. The solution of some random NP-hard problems
in polynomial expected time. Journal of Algorithms, 10(4):451–489, 1989. 1

[DGMS22a] Souvik Dhara, Julia Gaudio, Elchanan Mossel, and Colin Sandon. The power of two

matrices in spectral algorithms. 2022. Forthcoming. 3, 8, 9, 26

[DGMS22b] Souvik Dhara, Julia Gaudio, Elchanan Mossel, and Colin Sandon.

gorithms optimally recover (censored) planted dense subgraphs.
arXiv:2203.11847, 2022. 3, 8, 9

Spectral al-
arXiv preprint

[DGMS22c] Souvik Dhara, Julia Gaudio, Elchanan Mossel, and Colin Sandon. Spectral recovery
In Proceedings of the 2022 Annual ACM-SIAM

of binary censored block models.
Symposium on Discrete Algorithms (SODA), pages 3389–3416. SIAM, 2022. 3, 8, 9

[DK70]

Chandler Davis and William Kahan. The rotation of eigenvectors by a perturbation.
iii. SIAM Journal on Numerical Analysis, 7:1–46, 1970. 8

[DKMZ11] Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborov´a. Asymp-
totic analysis of the stochastic block model for modular networks and its algorithmic
applications. Physical Review E, 84(6):066106, 2011. 1

[DLS21]

Shaofeng Deng, Shuyang Ling, and Thomas Strohmer. Strong consistency, graph
Laplacians, and the stochastic block model. J. Mach. Learn. Res., 22(117):1–44, 2021.
3, 9, 18

[FK00]

[FK01]

[FO05]

[GD14]

Uriel Feige and Robert Krauthgamer. Finding and certifying a large hidden clique in
a semirandom graph. Random Structures & Algorithms, 16(2):195–208, 2000. 12

Uriel Feige and Joe Kilian. Heuristics for semirandom graph problems. Journal of
Computer and System Sciences, 63(4):639–671, 2001. 12

Uriel Feige and Eran Ofek. Spectral techniques applied to sparse random graphs.
Random Structures & Algorithms, 27(2):251–275, 2005. 9

Debarghya Ghoshdastidar and Ambedkar Dukkipati. Consistency of spectral parti-
tioning of uniform hypergraphs under planted partition model. Advances in Neural
Information Processing Systems, 27, 2014. 2

[GD15a]

Debarghya Ghoshdastidar and Ambedkar Dukkipati. A provable generalized tensor
spectral method for uniform hypergraph partitioning. In International Conference on
Machine Learning, pages 400–409. PMLR, 2015. 2

23

[GD15b]

[GD17]

Debarghya Ghoshdastidar and Ambedkar Dukkipati. Spectral clustering using multi-
linear SVD: Analysis, approximations and applications. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, volume 29, 2015. 2

Debarghya Ghoshdastidar and Ambedkar Dukkipati. Consistency of spectral hyper-
graph partitioning under planted partition model. The Annals of Statistics, 45(1):289–
315, 2017. 2

[GZFA10] Anna Goldenberg, Alice X. Zheng, Stephen E. Fienberg, and Edoardo M. Airoldi. A
survey of statistical network models. Found. Trends Mach. Learn., 2(2):129–233, feb
2010. 1

[HLL83]

Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic
blockmodels: First steps. Social networks, 5(2):109–137, 1983. 1

[HWX16a] Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery threshold
via semideﬁnite programming. IEEE Transactions on Information Theory, 62(5):2788–
2797, 2016. 6, 9, 12, 27

[HWX16b] Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery thresh-
old via semideﬁnite programming: Extensions. IEEE Transactions on Information
Theory, 62(10):5918–5937, 2016. 9

[JMRT16] Adel Javanmard, Andrea Montanari, and Federico Ricci-Tersenghi. Phase transi-
tions in semideﬁnite relaxations. Proceedings of the National Academy of Sciences,
113(16):E2218–E2223, 2016. 8

[KBG18]

Chiheon Kim, Afonso S Bandeira, and Michel X Goemans. Stochastic block model
for hypergraphs: Statistical limits and a semideﬁnite programming approach. arXiv
preprint arXiv:1807.02884, 2018. 1, 2, 3, 4, 6

[KRRT99] Ravi Kumar, Prabhakar Raghavan, Sridhar Rajagopalan, and Andrew Tomkins.
Trawling the web for emerging cyber-communities. Computer Networks, 31(11):1481–
1493, 1999. 1

[LKC20]

[Mas14]

Jeonghwan Lee, Daesung Kim, and Hye Won Chung. Robust hypergraph clustering via
convex relaxation of truncated mle. IEEE Journal on Selected Areas in Information
Theory, 1(3):613–631, 2020. 2, 3, 6, 8, 10

Laurent Massouli´e. Community detection thresholds and the weak Ramanujan prop-
erty. In Symposium on Theory of Computing, STOC 2014, page 694–703, New York,
NY, USA, 2014. Association for Computing Machinery. 1

[McS01]

Frank McSherry. Spectral partitioning of random graphs. In Proceedings 42nd IEEE
Symposium on Foundations of Computer Science, pages 529–537. IEEE, 2001. 1

[MNS13]

Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold
conjecture, 2013. 1

[MPN+99] Edward M. Marcotte, Matteo Pellegrini, Ho-Leung Ng, Danny W. Rice, Todd O.
Yeates, and David Eisenberg. Detecting protein function and protein-protein interac-
tions from genome sequences. Science, 285(5428):751–753, 1999. 1

24

[MU17]

Michael Mitzenmacher and Eli Upfal. Probability and computing: Randomization and
probabilistic techniques in algorithms and data analysis. Cambridge university press,
2017. 29

[NWS02] M. E. J. Newman, D. J. Watts, and S. H. Strogatz. Random graph models of social

networks. Proc. Natl. Acad. Sci. USA, 99:2566–2572, 2002. 1

[SM00]

[SWZ19]

Jianbo Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 22(8):888–905, 2000. 1

Liangjun Su, Wuyi Wang, and Yichong Zhang. Strong consistency of spectral cluster-
ing for stochastic block models. IEEE Transactions on Information Theory, 66(1):324–
338, 2019. 3

[YWS14]

Yi Yu, Tengyao Wang, and Richard J. Samworth. A useful variant of the Davis–Kahan
theorem for statisticians. Biometrika, 102:315–323, 2014. 18

[ZB18]

[ZT21]

Yiqiao Zhong and Nicolas Boumal. Near-optimal bounds for phase synchronization.
SIAM Journal on Optimization, 28(2):989–1016, 2018. 8

Qiaosheng Zhang and Vincent YF Tan. Exact recovery in the general hypergraph
stochastic block model. arXiv preprint arXiv:2105.04770, 2021. 2

25

A Dual certiﬁcate

Proof of Lemma 2.2. We ﬁrst show that X ∗ = σ∗σ∗⊤ is an optimal solution. For any X satisfying
the constraints in (2),

i

+

+

S, X
h
i
D + ν11⊤
h

W, X

=

D, X
h

i

i

−

W, X
h

W, X

i ≤ h
=

W, X
h
i
D, X ∗
i
h
W + S
h

=

=

ν11⊤, X ∗

=

W, X ∗
h

i

+

S, X ∗
h

i

=

W, X ∗
h

i

−

(since S, X

0)

(cid:23)

(since X is primal feasible)

(as X ∗ii = Xii = 1 for all i
S, X ∗i
h

. (since
i

= (σ∗)⊤Sσ∗ = 0)

[n])

∈

Therefore it only remains to establish the uniqueness of X ∗. Consider an optimal solution ˜X. Then

S, ˜X
h

i

−

=

=

D + ν11⊤
h
D
h
−
S, X ∗
h
0 and λn

W, X ∗
i
= 0.

=

W, ˜X

=

D
h

−

i

W, ˜X

(as

i

W, X ∗i
h

=

W, ˜X
h

11⊤, ˜X
= 0)
(since
h
and X ∗ii = ˜Xii = 1)
(using Sσ∗ = 0)

i

i

Since ˜X
must be a multiple of σ∗σ∗⊤. Moreover, as ˜Xii = 1 for all i

i
1(S) > 0, we obtain that ˜X is also a rank-1 matrix and hence it
[n], it must be that ˜X = X ∗ = σ∗σ∗⊤.

0 and S

(cid:23)

(cid:23)

−

∈

B Spectral norm concentration for similarity matrices

Theorem 4 (Spectral norm expectation). Let d
1). Let G
maxe pe ≤
exists a constant c := c(d, c0) > 0 such that

H(d, n, p), and W =

c0 log n/(n
−
d
−

∼

1

∈ {
S

d ), where
2, 3, . . .
(G) whose expectation is W ∗. Then there

be ﬁxed. Let p

[0, 1]([n]

∈

}

k2]
Remark B.1. By applying Markov’s inequality, we can show that for any function f (n) = ω(1),

log n.

W ∗

p

−

≤

c

E [
W
k

In order to prove Theorem 4, we require the following result.

P

(cid:16)

W
k

−

W ∗

k2 ≤

f (n)

log(n)

= 1

o(1).

−

p

(cid:17)

≤

q for all i, j, where c2 log n

Lemma B.2. Let A be a random matrix with independent entries, where Aij ∈
constants a < b. Suppose E [
]
Aij|
−
|
c2, c3 > 0. Then, there exists a constant c′ := c′(c2, c3, a, b) > 0 such that
E [
k

k2]
Proof. We use ideas from [DGMS22a, Lemma 4.5], who showed a similar result for zero diagonal,
symmetric matrices with independent entries. We ﬁrst construct a symmetric matrix B using A to
reduce it to the symmetric case. Let

[a, b] for two
c3 for arbitrary constants

n ≤

c′√nq.

E [A]

≤

−

≤

A

1

q

Fix a vector x

Observe that

k

Rn such that
x
∈
k
k2 = 1. Moreover,
y

B =

0 A
A⊤ 0

(cid:21)

(cid:20)

.

k2 = 1, and consider the vector y

∈

R2n such that y =

0n
x

.

(cid:21)

(cid:20)

(B

−

E[B])y =

(A

(cid:20)

26

E[A])x
0n

−

.

(cid:21)

Therefore,

so that

(B

k

−

E[B])y

k2 =

(A
k

−

E[A])x

k2 ,

Since x was arbitrary, we have shown

(A
k

−

E[A])x

(B
−
k
E [A]

k2 =
A
−
k
E [A]

k2]

E[B])y

k2 ≤ k
E[
B
k

E[
k

A

k2 ≤ k
B

−
E [B]

E[B]

B
E [B]

k2.
−
k2, and therefore
k2].
, where the maximum is taken
B, 0
}
{
B−. By the triangle

(28)

Thus, it suﬃces to bound E[
B
k
entrywise. Similarly, let B− =
inequality,

E [
B
k

−

E[B]

−

−
≤
E [B]
k2]. Let B+ = max
−
. Then we can write B = B+
B, 0
min
}
−
{
B+
k2]
k
(cid:2)

E[B+]

B−
k

+ E

k2

−

−

≤

E

(cid:3)

(cid:2)

−
E[B−]

k2

Observe that B+ and B− are nonnegative, zero-diagonal, symmetric matrices with independent
entries. Also, for all i, j,

(cid:3)

.

(29)

If b
0, then
−
exists c+ > 0 such that

B+
k

≤

q.

≤

} ≤

max

ij ], E[B−ij ]

E[
]
Bij|
|

E[B+
max E[
]
Aij|
{
|
E[B+]
k2 = 0. Otherwise it follows from [HWX16a, Theorem 5] that there
nq
b

1
B+
(
b
k
(cid:20)
E[B−]
k2 = 0. Otherwise, we again use [HWX16a, Theorem 5] to

k2)
(cid:21)

E[B+]

c+

r

−

≤

≤

E

.

B− −
Similarly, if a
k
conclude that there exists a constant c− > 0 such that

0, then

≥

E

1
a
|

|

(cid:20)

B−
(
k

−

E[B−]

k2)
(cid:21)

c−

≤

.

nq
a
|

|

r

Combining these with (28) and (29) we get

where c′ = c+

max

b, 0
}
{

+ c−

min

a, 0
{

}|

.

|

E[
k

A

−

E [A]

k2]

≤

c′√nq,

p
Proof of Theorem 4. The symbols . and
is to bound E [
(G)]
kS
matrix X, the function f (Y ) =

E [
S

(G)

−

Y

E [

(G)

E[
S

(G)]

−

kS
We can extend the deﬁnition of
S
.
1
with edges labeled by
}

1, 0,
{

−

≍
k2 is convex. By Jensen’s inequality,

p
hide constants in d, c0 throughout the proof. Our goal
k2]. Let G′ be an independent copy of G. Observe that for a ﬁxed
X
k
−
k2] = E
(cid:2)
so that

kS
(G′); i.e. G

E[
S
−
G′) =

G′ is a “hypergraph”

k2
(G)

(G′)]

(G′)

− S

(G)

(G)

k2

(G

kS

≤

E

(cid:3)

(cid:2)

.

S

−

S

−

(cid:3)
− S

Let R be a symmetric tensor of order d and dimension n with independent Rademacher entries;
denote the
id}

i2 ≤ · · · ≤

are mutually independent. Let
G′)

◦
R, we obtain

i.e. the entries
edge-wise product. Since G

G′ has the same distribution as (G

E [

E[
S

−

kS

(G)

(G)]

k2] ,
where the last inequality follows from the triangle inequality. Let pmax , c0 log n/
for simplic-
ity. Also, we now consider the hypergraph G+ that is coupled to G as follows. Each edge e that is

◦
1
−
1
−

k2]

(30)

((G

G′)

G′)

k2

k2

(G

(G

R)

kS

kS

kS

−

≤

≤

−

n
d

◦

(cid:2)

(cid:3)

(cid:3)

(cid:2)

= E

(cid:0)

(cid:1)

R(i1, i2, . . . , id) : i1 ≤
{
−
E

−
R)

◦

2E [

27

not present in G is present in G+ with probability pmax
1
−
G(1)
HSBM(d, n, pmax, pmax), we see that (G + G+)
◦
Also, E [
S

∼

◦
G] = 0. Using these observations along with Jensen’s inequality, we obtain

pe

−
pe
R has the same distribution as G(1)

(independently across edges). Letting
R.

(G+ ◦
(G

◦

kS

E [

R)
|
k2] = E [
= E [

R)

Substituting this into (30), we have

(G

◦

kS

R) + E [
(G+ ◦
R)
S
k2] = E
R)

◦

|

G]

k2]
(G(1)

E [

R)

≤

◦

((G + G+)

kS

kS
h

kS
k2

i

(G

◦

R) +

(G+ ◦

S

R)

k2]

.

E [

kS

(G)

E [
S

−

(G)]

k2]

≤

2E

S(G(1)
k

◦

R)

k2

h

i

.

(31)

Let K = d2
respectively for m
matrix, and then using Jensen’s inequality we get

[K]. Note that E [
S

d for ease of notation. Let G(m) and R(m) be independent copies of G(1) and R
R(m))] is the zero matrix. Thus, adding the zero

(G(m)

−

∈

◦

K

K

◦

S

kS

R)

E [

(G(1)

(G(1)

R(1)) +

k2] = E

R(m))
R(m))]
(cid:13)
(cid:13)
2#
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(32)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
R(m)) is the sum of independent copies of random matrices with
dependent entries. The goal is to re-express this same quantity as the sum of dependent matrices
with fully independent entries. To this end, let us consider the following construction.

"(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
K
(G(m)
m=1 S

Observe that

"(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

E [
S

2# ≤

(G(m)

(G(m)

m=2
X

m=1
X

P

E

S

◦

◦

◦

◦

.

Observe that the random variable (label) associated with any ﬁxed edge e is added to exactly
). Let L(e) be a ﬁxed ordered list of
(
·
) is an ordered list of all ordered pairs of e;

K entries of the similarity matrix when we apply the map
such locations. Formally, L(e) := (op(1)
i.e. (i, j) : i, j

e , . . . , op(
|
e

L(e)
|

e.

S

)

For e

∈ E

∈
and m, ℓ

∈

[K] let X (e,m,ℓ) be the n

n matrix which has only one non-zero entry:

X (e,m,ℓ)
ij

=

A(m)
e
0
(

◦

(i, j) = op(ℓ)
e
otherwise,

×
R(m)
e

where A(m)
e
struction,

denotes the indicator random variable associated with the edge e in G(m). By con-

K

K

K

(G(m)

S

◦

R(m)) =

Xm=1

Xm=1 Xe
∈E

Xℓ=1

X (e,m,ℓ)

(33)

We will now re-express this summation as the sum of dependent matrices with independent entries.

Rad are independent. Let D be a diagonal
Let Y =
matrix whose diagonal entries are i.i.d. copies of Y . Let us consider matrices C (1), . . . , C (K), where

Bern(pmax) and Zi ∼

2

2)

(n
−
d
i=1 XiZi, where Xi ∼
−

P

K

C (1) =

X (e,ℓ,ℓ) + D

Xe
∈E

Xℓ=1

K

C (k) =

X (e,(ℓ+k

−

1)%K,ℓ) + (

−

1)k+1D

Xe
∈E

Xℓ=1

Here % represents a standard modulo operation, except K%K is K instead of 0. Intuitively, C (1)
is the matrix for which the K locations corresponding to a given edge “consult” K independent

28

(34)

(35)

copies of G. Observe that all the entries of C (1) are independent copies of Y . To ensure that we
add the random variable associated with an edge of any given copy of G at all K locations in the
similarity matrix, we iterate over the list in a cyclic manner when constructing C (2), . . . , C (K). Note
D. Therefore, from the symmetry all C (k)s have the same
that D has the same distribution as
distribution. Moreover,

−

K

K

K

C (k) =

Xk=1

Xk=1 Xe
∈E
K

Xℓ=1
K

X (e,(ℓ+k

−

1)%K,ℓ) + (

−

1)k+1D

X (e,(ℓ+k

−

1)%K,ℓ) + (

−

1)k+1D

=

=

=

Xe
∈E

Xℓ=1
K

Xk=1
K

X (e,m,ℓ)

Xe
∈E
K

Xℓ=1

m=1
X

(G(m)

◦

R(m)),

S

m=1
X

where the last step follows from (33). Therefore, combining (31), (32), and (34)

E [
k

W

W ∗

k2]

≤

−

2 E

K

C (k)

2K E [
k

C

k2],

2# ≤

Xk=1
where C has the same distribution as C (1). Each entry of C is an independent copy of Y . Let F
be the event that all the entries of C are in the range [

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
4d, 4d]. By a union bound,

"(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

P (F c)

n2

P (
Y
|

·

≤

| ≥

4d)

n2

P

·

≤

−

(n
−
d
−

2

2)






Xi=1

Xi ≥

4d

.




n
d

2
−
2
−

Since

2

(n
2)
−
d
i=1 Xi ∼
−

Bin

P

(cid:16)(cid:0)

Therefore,

, pmax

, by [MU17, Theorem 4.4, Equation 4.1]

(cid:1)
P (F c)

(cid:17)

≤

n2

e4d

Θ(n/ log n)4d = O

1
n4d

−

3

.

(cid:19)

(cid:18)

E [
k

C

k2] = P (F ) E [
k
E [
C
k

k2 |

≤

E [
C
≤
k
= E [
C
k
E [
C
k

≤

k2 |

k2 |

k2 |

F c]

F c]

C

3

(cid:18)

k2 |
F ] + O

F ] + P (F c) E [
C
k
1
E[
C
n4d
k
−
n
1
n4d
d
(cid:18)
n3
−
(cid:16)
F ] + o(1),

(cid:19)
4d+1+d

F ] + O

F ] + O

(cid:18)
2
−

(cid:19)

n

(cid:17)

−

3

k2 |

kF |
2
2

−
−

(cid:19)

(36)

where the second inequality uses the fact that each entry of C is at most
left to show E [
k

. Thus, it is only
F ] = O(√log n). Note that the entries of C are independent even after

k2 |

C

(cid:1)

(cid:0)

n
d

2
−
2
−

29

conditioning on F . Moreover, the entries are bounded in [

−

4d, 4d]. Thus

E [
Cij| |
|

F ] =

.

4d

Xk=1
4d

Xk=1

k

P (
Cij|
|

·

= k

|

F )

4d

k

P (F

·

Cij|
∩ |
P (F )

4d

= k)

=

≤

Xk=1
E [
]
Cij|
|

k

P (
Cij|
|

·

= k)

≤

n
d

2
2

−
−

(cid:19)

≤

(cid:18)

pmax =

(cid:0)

k

·

= k)

P (
Cij|
|
1
−

o(1)

Xk=1
2
−
2
−

n
d

c0 log n
n
(cid:1)
d

1
−
1
−

.

log n
n

.

Therefore, by Lemma B.2 and the fact that E [C

|

F ] is the zero matrix, we obtain

(cid:0)

(cid:1)

E [
C
k

k2 |

F ] = E[
C
k

−

E [C]

k2 |

Substituting this back in (35) and using (36),

F ]

≤

c′

n

r

log n

n ≤

c′

log n.

p

E [
k

W

W ∗

k2]

≤

−

2K E[
C
k

k2]

≤

2K E[
k

C

k2 |

F ] + o(1)

≤

2Kc′

log n + o(1)

for some c that depends on d and c0, concluding the proof.

p

c

log n,

≤

p

30

