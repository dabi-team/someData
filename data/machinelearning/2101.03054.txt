1
2
0
2

n
a
J

7

]

R

I
.
s
c
[

1
v
4
5
0
3
0
.
1
0
1
2
:
v
i
X
r
a

Application of Knowledge Graphs to Provide Side
Information for Improved Recommendation Accuracy

Yuhao Mao, Serguei A. Mokhov, Sudhir Mudur

aComputer Science and Software Engineerin, Concordia University, 1515, Ste Catherine
West, Montreal, H3G 2W1, Quebec, Canada

Abstract

Personalized recommendations are popular in these days of Internet driven
activities, speciﬁcally shopping. Recommendation methods can be grouped
into three major categories, content based ﬁltering, collaborative ﬁltering
and machine learning enhanced. Information about products and preferences
of diﬀerent users are primarily used to infer preferences for a speciﬁc user.
Inadequate information can obviously cause these methods to fail or perform
poorly. The more information we provide to these methods, the more likely it
is that the methods perform better. Knowledge graphs represent the current
trend in recording information in the form of relations between entities, and
can provide additional (side) information about products and users. Such
information can be used to improve nearest neighbour search, clustering users
and products, or train the neural network, when one is used. In this work, we
present a new generic recommendation systems framework, that integrates
knowledge graphs into the recommendation pipeline. We describe its software
design and implementation, and then show through experiments, how such a
framework can be specialized for a domain, say movie recommendations, and
the improvements in recommendation results possible due to side information
obtained from knowledge graphs representation of such information. Our
framework supports diﬀerent knowledge graph representation formats, and
facilitates format conversion, merging and information extraction needed for
training recommendation methods.

Keywords: knowledge graphs, recommendation systems, side information,
software frameworks

Preprint submitted to Science of Computer Programming

January 11, 2021

 
 
 
 
 
 
1. Introduction

Recommendation systems are the product of the rapid development of
the Internet. The use of the Internet to support diﬀerent human activities
has been on a constant rise, but propelled by the current pandemic, we start
to see a very steep rise. There are more and more product oﬀerings and
providers on the Internet and user acceptance has jumped. However, the
users are over loaded with information. In such situations, the recommenda-
tion system comes into play [1]. The recommendation system is essentially
a technical means for users to narrow the information they are interested in
from the massive amount of information available on the Internet, when the
user desired product is not speciﬁc to a single item [2].

Applications of recommendation systems are very wide. According to
reports, the recommendation system has brought 35% of sales revenue to
Amazon [3] and up to 75% of consumption to Netﬂix [4], and 60% of the
browsing on the Youtube homepage comes from recommendation services
[5].
It is also widely used by various Internet companies. As long as the
company has a large number of products to oﬀer to the clients, the rec-
ommendation system will be useful [6, 7, 8]. The current application ﬁelds
of the recommendation system have transcended beyond e-commerce, into
news, video, music, dating, health, education, etc.

A recommendation system can be regarded as an information ﬁltering
system, which can learn the user’s interests and preferences based on the
user’s ﬁles or historical behaviour, and predict the user’s rating or prefer-
ence for a given item, based on information about the item, and the user.
Clearly, the more information, the recommendation method has about users
and products, the better is its ability to predict user preferences.

In this paper, we present a new generic software framework which en-
ables easy integration of any available additional information, called side
information, by including it into a knowledge graph to be used for training
the recommendation method.

In most recommendation scenarios, items may have rich associated knowl-
edge in the form of interlinked information, and the network structure that
depicts this knowledge is called a knowledge graph. Information is encoded in
a data structure called triples made of subject-predicate-object statements.
A knowledge graph on the item side greatly increases information about the
item, strengthens the connection between items, provides a rich reference
value for the recommendation, and can bring additional diversity and inter-

2

pretability to the recommendation result. recommendation systems.

There is a clear need for a general framework, which (i) integrates search
and update of information, (ii) includes crawling of websites for additional
information, (iii) supports storing of the information in a structured, easily
accessible manner, (iv) enables easy retrieval of the information about items
and users as input for the training of a recommendation system. Adding
a knowledge graph into the recommendation framework can help us better
manage knowledge data, process data, and query the information we need
faster.

Firstly, knowledge graphs as a form of structured human knowledge
have drawn great research attention from both the academia and the indus-
try [9, 10, 11]. A knowledge graph is a structured representation of facts,
consisting of entities, relationships, and semantic descriptions. The knowl-
edge graph can be used wherever there is a relationship. It has successfully
captured a large number of customers, including Walmart, Google, LinkedIn,
Adidas, HP, FT Financial Times, etc. wellknown companies and institutions.
Applications are still growing.

Compared with traditional data bases and information retrieval methods,

the advantages of knowledge graph are the following:

• Strong ability to express relationships: Based on graph theory and
probability graph models, it can handle complex and diverse association
analyses.

• Knowledge learning:

it can support learning functions based on in-
teractive actions such as reasoning, error correction, and annotation,
and continuously accumulates knowledge logic and models, improves
system intelligence.

• High-speed feedback: Schematic data storage method enables fast data

retrieval speeds.

Knowledge graphs usually have two main types of storage formats [12, 13]:
one is RDF (Resource Description Framework) based storage, and the other
is graph database (e.e., neo4j), with their advantages and disadvantages.

Secondly, the recommendation system has become a relatively indepen-
dent research direction. It is generally considered to have started with the
GroupLens system launched by the GroupLens research group of the Uni-
versity of Minnesota in 1994 [14]. As a highly readable external knowledge

3

carrier, knowledge graphs provide a great possibility to improve algorithm
interpretation capabilities [15]. Therefore, combining knowledge graph with
recommendation systems is one of the hottest topics in the current recom-
mendation system research.

Our main contribution is the following:

• We present an overall architecture that allows users to build knowledge
graphs, display knowledge graphs, and enable recommender algorithms
to be trained with knowledge when predicting user ratings for items.
We demonstrate our framework for the movie recommendation domain.

Other contributions include:

• A pipeline architecture that allows users to crawl data, build and merge
knowledge graphs in diﬀerent formats, to display knowledge graphs and
extract information, without knowing the underlying format details.

• We oﬀer a way for the recommendation systems researchers to enable
recommendation system experiments on top of TensorFlow and Keras.

The rest of the paper is organized as follows.

In the next section we
provide a brief review of existing software frameworks for recommendations
reported in the literature. Next, we describe the design and implementation
of our framework in a top-down manner, along with examples of instantation
and applications. Lastly, we present our conclusions and some extensions.

2. Literature Review

While we can ﬁnd a tremendous lot on recommendation algorithms, a
literature scan reveals only a few attempts at development of frameworks for
recommendation systems. Below is a brief review of these.

2.1. A Gradient based Adaptive Learning Framework for Eﬃcient Personal

Recommendation

Yue et al. [16] use gradient descent to learn the user’s model for the rec-
ommendation. Three machine learning algorithms (including logistic regres-
sion, gradient boosting decision tree and matrix decomposition) are used.
Although gradient boosting decision tree can prevent overﬁtting and has
strong interpretability, it is not suitable for high-dimensional sparse features,
usually the case with items and users. If there are many features, each re-
gression tree will consume a lot of time.

4

2.2. Raccoon Recommendation Engine

Raccoon [17] is a recommendation system framework based on collabora-
tive ﬁltering. The system uses k-nearest-neighbours to classify data. Raccoon
needs to calculate the similarity of users or items. The original implemen-
tation of Raccoon uses Pearson which was good for measuring similarity of
discrete values in a small range. But to make the calculation faster, one can
also use Jaccard, which is a calculation method for measuring binary rating
data (ie like/dislike). But the collaborative ﬁltering algorithm does not care
about the inner connection of characters or objects. It only uses the user ID
and product ID to make recommendations.

2.3. Good Enough Recommendations (GER)

GER (Good Enough Recommendation) [18] is a scalable, easy-to-use and
easy-to-integrate recommendation engine. GER is an open source NPM mod-
ule.
Its core is the same as the knowledge graph triplet (people, actions,
things). GER recommends in two ways. One is by comparing two people,
looking at their history, and another one is from a person’s history. Its core
logic is implemented in an abstraction called the Event Storage Manager
(ESM). Data can be stored in memory ESM or PostgreSQL ESM. It also
provides corresponding interfaces to the framework developer, including the
Initialization API for operating namespace, the Events API for operating on
triples, the Thing Recommendations API for computing things, the Person
Recommendations API for recommending users, and Compacting API for
compressing items.

2.4. LensKit

LensKit [19] is an open-source recommendation system based on java,
produced by the GroupLens Research team of the University of Minnesota.
But the java version of LensKit has been deprecated, and the latest version
uses python. The python version of Lenskit is a set of tools for experimenting
and researching recommendation systems. It provides support for training,
running and evaluating recommendation systems. The recommendation al-
gorithms in LensKit include SVD, Hierarchical Poisson Factorization, and
KNN. LensKit can work with any data in pandas.DataFrame with the ex-
pected (ﬁxed set) columns. Lenskit loads data through the dataLoader
function. Each data set class or function takes a path parameter specifying
the location of the data set. These data ﬁles have normalized column names
to ﬁt with LensKit’s general conventions.

5

Name
Yue Ning et. al.
Racoon
GER
LensKit
DKN
MKR
PredictionIO
Surprise

Type
Decision Tree
Collaborative Filtering
Collaborative Filtering
Machine Learning
knowledge graph enhanced
knowledge graph enhanced
machine learning
Collaborative Filtering

Storage types
-
Redis
PostgreSQL
LocalFile
LocalFile
LocalFile
Hadoop,HBase
LocalFile

Domain
content recommendation
cross-domain
movie
cross-domain
News
cross-domain
-
movie,joke

Example uses
content recommendation
https : //github.com/guymorita/benchmarkraccoonmovielens
https : //github.com/grahamjenson/ger/tree/master/examples
https : //github.com/lenskit/lkpy/tree/master/examples
https : //github.com/hwwang55/DKN
https : //github.com/hwwang55/MKR
https : //github.com/apache/predictionio
https : //github.com/NicolasHug/Surprise/tree/master/examples

Language
-
Java
Java
Python
Python
Python
Scala
Python

ML Models
Boosting
KNN
-
SVD, hpf
tensorﬂow
tensorﬂow
Apache Spark MLlib

Maintained
-
Jan 10, 2017
Jul 9, 2015
Nov 10, 2020
Nov 22, 2019
Nov 22, 2019
Mar 11, 2019
matrix factorization, KNN Aug 6, 2020

Table 1: Characteristics of Existing Frameworks

2.5. Deep Knowledge-Aware Network for News Recommendation (DKN)

DKN [20] proposes a model that integrates the embedded representation
of knowledge graph entities with neural networks for news recommendation.
News is characterized by highly condensed representation and contains many
knowledge entities, snd it is time sensitive. A good news recommendation
algorithm should be able to make corresponding changes as users’ interests
change. To solve the above problems, the DKN model is proposed. First, a
knowledge-aware convolutional neural network (KCNN) is used to integrate
the semantic representation of news with the knowledge representation to
form a new embedding, and then the attention from the user’s news click
history to the candidate news is established. The news with higher scores is
recommended to users.

2.6. Multi-task Feature Learning for Knowledge Graph enhanced Recommen-

dation (MKR)

MKR [21] is a model that uses knowledge graph embedding tasks to assist
recommendation tasks. These two tasks are linked by a cross-compression
unit, which automatically shares potential features, and learns the high-level
interaction between the recommendation system and entities in the knowl-
edge graph. It is shown that the cross-compression unit has suﬃcient poly-
nomial approximation ability, and MKR is a model of many typical recom-
mendation systems and multi-task learning methods.

2.7. Summary of Recommendation System Literature Review

There are three types of recommendation system models (collaborative
ﬁltering, content-based and machine learning) in the above frameworks. Ta-
ble 1 is a summary characterizing the above frameworks.

As we can see there are no recommendation system software frameworks
yet that are generic to support web crawling, information update, visualiza-
tion and input to recommendation methods independent of storage formats

6

and algorithms. We will brieﬂy discuss the limitations of presently available
frameworks. Yue et al. mainly uses the boosting model, which makes the
training time high. Because Raccoon uses the K-nearest-neighbours model, it
cannot handle new users or new items, well known as the cold start problem.
Further, it also has data sparseness and scalability issues. The advantage of
GER is that it contains no business rules, with limited conﬁguration, and
almost no setup required. But this is at the expense of the scalability of the
engine. Other limitations are that it does not generate recommendations for a
person with less history, and has the data set compression limit problem, i.e.,
certain items will never be used. For example, if items are older or belong
to users with a long history, these items will not be used in any calcula-
tions but will occupy space. The advantage of LensKit is that its framework
contains many algorithms, such as funksvd and KNN, and users can call
diﬀerent algorithms according to their needs. In LensKit the data is called
through the path parameter, and the data has a speciﬁc format. This means
that the LensKit framework can only process user, item and rating data. If
the user has new data, such as user information or item classiﬁcation, the
LensKit framework cannot handle it. Although DKN uses knowledge graph
embedding as an aid, it can only be used for news recommendations. The
main disadvantage of MKR is that it is not a generic framework, and it in-
puts text documents as knowledge graphs, but not in their graph structured
form, making it cumbersome to update knowledge.

3. Framework Design

Figure 1 shows the overall design of our framework. It is designed as a
pipeline of tasks from end user input to ﬁnal recommendations to the end
user. Further, each stage in the pipeline is designed as a sub-framework, some
stages are nested, enabling specialization and expansion at a more granular
level. We denote a generic component as a frozen spot and its specialized
component as a hot spot.

The four major stages in the pipeline have the following functionality:

• InfoExtractor framework: abstracts a web extractor. It takes a URL
address (such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx) and
extraction rules as input then formats the output that is extracted.
This is further nested, to enable domain level (movies, news, etc.) spe-
cialization.

7

Figure 1: Core components of the framework, each box in blue means the framework’s
frozen spot and each box in red represents a set of the hot spots for each specialized
framework.

• StorageManager framework: abstracts diﬀerent knowledge graph stor-
age formats. It takes string data stream as input then generates the
output ﬁle according to the required format.

• Knowledge graph viewer framework: abstracts knowledge graph visual-
ization. It takes triples stream as input then creates the visualization.

• Recommendation method: abstracts the recommendation method and
knowledge input. The recommendation method can take these knowl-
edge graph triples as input for training the recommendation prediction
model.

3.1. InfoExtractor Framework Design

It is necessary to abstract the common methods of information extrac-
tors. We will take the example of extracting movie information. We have
designed a small module called MovieExtractor nested in InfoExtractor.
MovieExtractor serves as a frozen spot to provide users with functions such
as capturing movie information, as shown in Figure 2.

MovieExtractor is the abstract class of the extractor, it has three basic
methods, extractDirectorInfo,extractWriterInfo and extractActorInfo.
It is used to extract director information, author information and star infor-
mation respectively. extractPosterInfo is the abstract class of the poster
downloader. It downloads the movie poster (an example of side information)
and then converts the image into a string. to facilitate storage and coding.

8

Figure 2: Design of the InfoExtractor Framework.

Figure 3: Design of the knowledge graph StorageManager framework.

There are many websites on the Internet that store information about
movies, such as IMDB, Wikipedia, Netﬂix and Douban. Extractors dedicated
to these websites can be used as hotspots to access our MovieExtractor API.

3.2. StorageManager Framework Design

StorageManager Framework serves as a frozen spot, as shown in Figure 3.
Also, the design allows us to add support for more kinds of storage modes
easily without changing the frozen spots themselves.

For the SideInformationLoader, We again design it as a frozen spot, and
its responsibility is to add triples to the knowledge graph through the method
in StorageManager to increase the richness of the knowledge provided to the
recommender method. SideInfomationLoader contains three methods, they
are loadFile, loadFiles and setConfiguration.

9

Figure 4: Design of the knowledge graph viewer framework.

3.3. Knowledge Graph Viewer Framework Design

This frozen spot’s, responsibility is to display triples for knowledge visu-
alization. As shown in Figure 4, KnowledgeGraphViewer is an abstract class,
which contains a show method.

3.4. Recommendation Method Framework Design

We designed a dedicated framework for the recommendation method as
shown in Figure 5. It consists of four frozen spots. Including DataLoader
module, DataPreprocessor module, MLModelBuilder module and Predictor
module.

DataPreprocessor module includes three methods, which are preprocessKG,

preprocessUserInfo and preprocessRating. preprocessKG is used to
process knowledge graph triples. It returns three dictionaries to store the
id corresponding to the product, the id corresponding to the relationship
and the id corresponding to the character. preprocessUserInfo is used to
encode user information, including the user’s gender, age, and occupation.
preprocessRating uses the three dictionaries stored in the previous step to
convert the product and user ID in the rating information.

The function of the DataLoader module is to read all the triples from the
ﬁle generated in the previous step. And return three lists, used to store the
number of users, the number of items, the number of relations. loadKG is used
to load the KG ﬁle processed in the previous step, calculate the number of
entities, the number of relations, and return these values. These parameters
are used to create the matrix of entities and relations when building the

10

Figure 5: Design of the recommendation method framework.

prediction model, for example neural network, in the next step. loadUsers
loads the user information ﬁle processed in the previous step, calculate the
number of users, genders, ages and jobs. These parameters are used to
create a user information matrix when building the neural network in the
next step. loadRatings is used to load the rating ﬁle, then calculate the
number of items, and then divide the data into the training data, eval data
and test data according to the ratio of 6:2:2. The model is built through
MLModelBuilder.

After the user trains the model, s/he will get a pre-trained model. This
model will be used to predict the user’s score in the later stage. Predictor
It includes getUserInfo and
framework is to facilitate this prediction.
predictScore two methods.
If it is an old user, the user only needs to
provide the user ID to query the user’s personal information. Three lists are
returned, user’s gender, age, and job information. predictScore returns a
ﬂoat value, which represents the user’s rating of the product.

11

Figure 6: Instantiation of the StorageManager in framework.

4. Framework Instantiation

4.1. IMDBExtractor Instantiation

In Section 3.1, we described the design of the InfoExtractor module
within the framework. If instantiated for movie recommendation, we extract
director, writer, stars information, movie genre, movie poster and other avail-
able movie data as side information.

In IMDBExtractor module, we create a list for each kind of information to
be extracted. Because there may be many directors, actors, and stars of the
movie, the information for each category is returned as a list. If the relevant
information cannot be found in IMDB, an empty list will be returned. Each
triplet will be stored in the form of head, relation, tail.

4.2. StorageManager Instantiation

In Section 3.2, we proposed the design of frozen spot for a knowledge
graph StorageManager framework in general. We created two storage mod-
ules as hot spots for knowledge graphs. They are Neo4jManager and RDFManager.
Figure 6 shows the structure of this module.

4.2.1. Neo4jManager

To facilitate users to use the Neo4jManager framework, we provide a
framework API for Neo4j operations. Figure 7 illustrates the structure of

12

Figure 7: Structure of the Neo4j storage module in framework.

Figure 8: Structure of the RDF storage module in framework.

Neo4j storage.

4.2.2. RDFManager

To facilitate users to use the RDFManager framework, we provide our
framework’s API for all operations of owl, including: RDFSave, RDFGetOntology,
RDFAddClass, RDFAddIndividual, RDFAddDataproperty, RDFAddDatapropertyValue
and RDFAddObjectproperty. Figure 8 illustrates the structure of RDF stor-
age. When the user chooses to use RDF storage, it will call the RDFManager
in the framework, use the API in our framework to operate on the triples,
and then save it as an RDF ﬁle.

13

Figure 9: TextInformationLoader instantiation.

4.2.3. TextInfomationLoader

In Section 3.2, we described the design of the SideInformationLoader.
To add side information to meet our requirements, based on the frozen spot
of SideInformationLoader, we created a TextInformationLoader hot spot
based on text documents. The overall idea is shown in Figure 9. In our imple-
mentation, we need to read the ﬁle in the parameter, parse the ﬁle according
to the format, and extract the head, relation and tail of the triples. Then add
new triples to the knowledge graph through addTriple in StorageManager.
Since there is already a method for adding a single ﬁle, we only need to make
some adjustments about loadFiles. When reading ﬁles, we just need to call
loadFile for each ﬁle.

4.3. networkxViewer Instantiation

In Section 3.3, we described the design of the viewer module. The work-
ﬂow of the viewer module is shown in Figure 10. Our process can be described
by the following steps:

1. Read all the individual names and store all the individuals in viewerIndividuals.
2. Take the individual list from the previous step and get all the informa-

tion connected to this individual.

3. According to the content in the individual, create nodes or links re-

spectively.

4.4. Recommendation Method Instantiation

The recommendation method is the most important part of our frame-
work. In Section 3.4, we proposed the design of a generic recommendation

14

Figure 10: Instantiation of the networkxViewer framework.

Figure 11: Workﬂow of recommendation system framework

method. Here we present an instance which adopts the deep learning method.
We decided to implement the entire method ourselves, as we wanted to in-
corporate the beneﬁts of side information obtained from using knowledge
graphs. It is based on the work of [22, 23]. Our recommendation method is
written in Python and built on top of TensorFlow. Figure 11 illustrates the
workﬂow of the recommendation method module.

We implemented and modiﬁed the network architecture shown in Sec-
tion 2.6. For the recommendation method model, the structure is shown

15

Figure 12: Structure of the recommendation module in RS framework.

below in Figure 12. In our implementation, we made some changes to the
structure of the model. The original work does not contain user gender,
user age and user job embeddings. Here we decided to add this as side infor-
mation to improve the accuracy. To unify the latitude (in the deep learning
network), we also choose arg.dim as the dimension of user age, user job and
user gender.

For the knowledge graph embedding model, the structure is shown below
in Figure 13. Let’s take our data as an example. There are 3883 heads and
four relations in the data. Therefore, the item matrix is the 3883 × arg.dim
matrix and the relation matrix is the 4 × arg.dim matrix. Each time we
take out the vector corresponding to the head from the head embedding
according to the head index, and then process the crossover and compression
unit to obtain an Hlb (head) vector. The Rl (relation) vector is obtained by
looking up the vector of the relation index in the relation matrix and then
passing through a fully connected layer. Then we merge the [batch size, dim]
dimension Hl and Rl into a [batch size, 2 × arg.dim] vector, and this vector
is passed through a fully connected layer to get a [batch size,arg.dim] vector.
This vector includes the predicted tail.

For the cross and compress unit, item and head are each a vector of dimen-
sion [batch size,arg.dim]. To facilitate calculation, we ﬁrst expand them by
one dimension so that they become [batch size,arg.dim,1] and [batch size,1,arg.dim]
respectively, and then multiply them, to get the cross matrix c matrix, a ma-
trix of [batch size,dim,dim], and a transpose matrix c matrix transpose, the
two matrices are multiplied by diﬀerent weights, then reshaped to obtain the
ﬁnal vectors.

16

Figure 13: Structure of the knowledge graph embedding module in recommendation system
framework.

During the training, our model is guided by three loss functions: LRS loss

and LKGE loss and LREG loss.

L = LRS + LKG + LREG

(1)

The complete loss function is as follows:

• The ﬁrst item is the loss in the recommendation module.

• The second item is the loss of the KGE module, which aims to increase
the score of the correct triplet and reduce the score of the wrong triplet.

• The third item is L2 regularization to prevent overﬁtting.

4.4.1. Predictor

Once the model is trained, we need to make predictions. We use our
predictor module. There are two methods in this module, getUserInfo and
predictScore. getUserInfo is a method used to extract user’s information.
The model trained in the previous step is loaded into predictScore, and
then diﬀerent matrices are read by name. If the id entered by the user is
greater than the dimension of the model. That means the id is a new user.
Our recommendation will focus on the user’s age and job. Finally, a predicted
ﬂoat value is returned.

The steps we describe here can be represented by Algorithm 1.

17

Algorithm 1: prediction application procedure
1 parser ← init a argument parser
2 dataset ← init default dataset
3 userid ← init default userid
4 movieid ← init default movieid
5
6 if trained model path exist then
7

load trained model
predict user’s rating to movie

8

9 else
10

Error

4.5. Deployment

Figure 14 shows the diagram of all the required libraries. In InfoExtractor,

we need request, bs4, IMDB and base64 libraries. The requests library is
used to issue standard HTTP requests in Python. It abstracts the complex-
ity behind the request into an API so that users can focus on interacting
with the service and using data in the application. HTML is composed of a
“tag tree”, and the bs4 library is a functional library responsible for parsing,
traversing, and maintaining the “tag tree”. IMDB is an online database of
movie information. Base64 is a library that uses 64 characters to represent
arbitrary binary data.

In StorageManager, we need py2neo, owlready2 and rdflib libraries.
py2neo can use Neo4j from within the Python application and from the
command line. owlready2 is a module for ontology-oriented programming
in Python. rdflib is used to parse and serialize ﬁles in RDF, owl, JSON
and other formats.

In KGViewer, we need networkx and matplotlib libraries. networkx is
a graph theory and complex network modelling tool developed in Python
language, which can facilitate complex network data analysis and simulation
modelling. The matplotlib library is an essential data visualization tool.

In RecommendationSystem, we need random, numpy, sklearn linecache
and tensorflow libraries. random library is used to generate random num-
bers. NumPy is a math library mainly used for array calculations. sklearn is
an open-source Python machine learning library that provides a large number
of tools for data mining and analysis. linecache is used to read arbitrary

18

lines from a ﬁle. TensorFlow is a powerful open-source software library de-
veloped by the Google Brain team for deep neural networks.

Figure 14: UML deployment diagram.

5. Framework Application

5.1. Integrated Lenskit Application

This Lenskit application is a comparison of NDCG (Normalized Dis-
counted Cumulative Gain) values for Lenskit recommendation algorithms.
Figure 15 shows the result of the evaluation.

5.2. Prediction of User’s Rating for a Movie

So far, what we have described in the previous sectiuons are the design,
implementation and instantiation of our framework. Our prediction service
predicts users’ ratings of products (as an example product we have chosen
movies). The workﬂow can be described as follows:

1. Read all the triple data through dataLoader to get the corresponding

information.

2. Use these triples and the user’s rating of the movie as input, and train

through the RS module.

3. Load the trained model, and predict the user’s rating of the movie.

For the prediction task, we use the pipeline shown in Figure 16.
Now, with this pipeline instance, application developers no longer need

to worry about details. All the user needs to do is provide the right input.

19

Figure 15: LensKit evaluation result.

Figure 16: Prediction pipeline.

5.3. Predict User’s Rating for a Book

The data we use is called Book-Crossing [24]. Book-Crossing dataset is

Collected by Cai Nicolas Ziegler from the Book-Crossing community.

The Book-Crossing dataset comprises 3 tables. They are users, Books
and Ratings. Users contain the user’s id, the user’s location, and the user’s
age. Books include book title, book author, year of publication, publisher
and other information. Ratings include user reviews of the book. Because
the process used is the same as Section 5.2, we won’t repeat it here.

5.4. Knowledge Graph Fusion Application

Knowledge fusion is an eﬀective way to avoid node duplication in knowl-
edge graphs. Users may have knowledge graph ﬁles in diﬀerent formats which
may result in duplicate nodes with the same information. We use knowledge
graph fusion to solve the problem of duplicate nodes.

In our implementation, we provide an API where users can convert Neo4j
triples to RDF or convert them to Neo4j based on RDF triples according to
their needs.

20

The procedure is described in Algorithm 2.

Algorithm 2: Knowledge graph fusion application procedure
1 parser ← init a argument parser
2 mode ← init default storage mode
3 path ← init default path
4
5 if args.mode == ”neo4j2RDF” then
6

call Neo4j to RDF API

7 else
8

call RDF to Neo4j API

9
10 Neo4j triples list ← init a list for all the triples in Neo4j
11 RDF triples list ← init a list for all the triples in RDF
12 while read ﬁle do
13

add triples to Neo4j triples list
add triples to RDF triples list
while read Neoj triples list or RDF triples list do

if node exist in RDF triples list or Neo4j triples list then

ﬁnd the corresponding node
else

create new node

connect nodes and relations

14

15

16

17

18

19

20

6. Framework Evaluation and Conclusions

6.1. Evaluation Testbed Speciﬁcations

Before starting the discussion about the evaluation, we ﬁrst describe the
environment, including the operating system used, processor power, memory,
hardware, etc. The detailed speciﬁcations can be seen in Table 2.

Table 3 lists the various libraries used in this research.
Due to diﬀerent operating systems, the software is slightly diﬀerent, so
we also need to give the software details of the environment. For these
two environments: one is a laptop and the other is a server, we call them

21

Setting

Laptop

Server (colab)

Name
Memory
Processor
Graphic
OS
Memory
Processor
Graphic
OS

Device
8 GB
2.3 GHz Intel Core i5
Intel Iris Plus Graphics 640 1536 MB
Mac OS Mojave 10.14.6
12 GB
Intel Core i7-920 CPU 2.67GHz
GeForce GTX1080 Ti (12 GB)
Ubuntu 18.04.5 LTS 64-bit

Table 2: Environment hardware speciﬁcations.

Type

Libraries

Name
py2neo
Tensorflow
bs4
csv
networkx
matplotlib
rdflib
numpy
sklearn
pandas

Version
4.3.0
1.14.0
4.8.0
1.0
2.4
3.1.2
4.2.2
1.17.0
0.21.3
0.25.0

Table 3: Python libraries used.

Setting 1 and Setting 2, respectively. There is a slight diﬀerence between
the installed software versions. For these two environments, we give more
detailed information in Table 4.

Software
Python
Neo4j Desktop
Protege

Setting 1 Setting 2

3.7.5
1.2.1
5.5.0

3.7.5
1.2.9
5.5.0

Table 4: Software packages and IDE tools used.

22

MovieLens 1M
baseline
baseline+movie
baseline+user
baseline+user+movie
baseline+poster
baseline+movie+user+poster

train AUC train ACC evaluate AUC evaluate ACC test AUC test ACC

0.9189
0.9227
0.9238
0.9292
0.9173
0.9273

0.8392
0.8439
0.8455
0.8516
0.8279
0.8497

0.9046
0.9081
0.9096
0.9142
0.9041
0.9113

0.8277
0.8295
0.8321
0.8375
0.8153
0.8351

0.9042
0.9061
0.9091
0.9136
0.9029
0.9111

0.8261
0.8297
0.8331
0.8359
0.8214
0.8349

Table 5: Results of the same models on diﬀerent datasets.

6.2. Real-time Response

According to [25], we set the real-time response baseline to be 2000 ms. To
evaluate the whole system’s processing ability, we performed the experiment
described below:

1. Load the trained model to get the pre-trained graph structure and

weights.

2. Prepare feed dict, new training data or test data. In this way, the same

model can be used to train or test diﬀerent data..

3. Measure the diﬀerence between the timestamp tsbefore the pipeline

start and the timestamp td after system processing.

4. Repeat the previous operation 100 times, and then calculate the average

processing time through the formula Equation 2.

Result =

(cid:80)100

i=1 te − ts
100

.

(2)

The result shows that the speed of our solution on a local laptop machine is
634.33ms. It is faster than the real-time baseline of 2000ms.

6.3. Experimental Results

We trained the MKR model using the MovieLens-1m dataset, and then
used the validation set to verify the model. We split all data according to
6:2:2, i.e., 60% is the training set, 20% is the validation set, and 20% is the
test set. The data of the validation set and test set were not be used for
training.

As Section 4.2 shows, our side information has many types, including
movie information, user information, and movie posters. We train with dif-
ferent side information through our model and obtain the results through 20
epoch training, as shown in Table 5.

From the results, we can see that the accuracy of data with user informa-
tion and movie information is the highest, about 1% higher than the baseline.

23

Because users may watch a movie because of a director or an actor, the other
movie information can help improve accuracy. The age, job and other infor-
mation in the user information also help to improve the accuracy, because
the user may choose some related movies to watch based on age and occu-
pation. But because the poster of each movie is diﬀerent, in the knowledge
graph, each poster is connected to only one movie node, so the poster data is
sparse data for the knowledge graph. Therefore, the poster information does
not have a good eﬀect for us at present, but if we can extract some useful
information from the poster through technologies such as computer vision,
it may be helpful in improving the accuracy of the recommendation.

6.4. Concluding Remarks and Extensions

We proposed, designed and implemented a generic software framework
that integrates knowledge graph representations for providing training data
to recommendation methods. At the core of this framework are knowledge
graphs for storing and managing information of use to recommendation meth-
ods. To the best of our knowledge, a similar framework is not available
elsewhere.

The ultimate goal of our work is to make it as a research platform for
more developers in the recommendation systems ﬁeld. With that goal it
needs to be extended as follows:

Java API wrapper: Our framework was written in Python, but the
movie recommendation system is mostly used on web pages. So it is better
for users, if can we provide a Java wrapper for our API.

Support diﬀerent machine learning backend: Currently, our rec-
ommended module only supports TensorFlow. But there are many diﬀerent
deep learning frameworks, such as PyTorch, Caﬀe or Scikit-learn. Diﬀerent
frameworks have their advantages. We plan to add various machine learning
frameworks to our framework in the future.

Support more storage methods and more input formats: Cur-
rently, we only support four storage formats, namely RDF, RDFS, OWL
and Neo4j. For the input format, because we use CSV for storage, some
users may choose JSON format or TTL format, so we also need to update
the program to support these formats.

24

References

[1] G. Adomavicius, A. Tuzhilin, Towards the Next Generation of Recom-
mender Systems: A Survey of the State-of-the-Art and Possible Exten-
sions, IEEE Transactions on Knowledge and Data Engineering 17 (6)
(2005) 734–749. doi:http://doi.acm.org/10.1145/223904.223929.

[2] Recommender system — Wikipedia, the free encyclopedia, https://
en.wikipedia.org/wiki/Recommender_system, accessed: 2020-07-27.

amazon
more

sell-
[3] The
ing
http://rejoiner.com/resources/
amazon-recommendations-secret-selling-online, accessed: 2018-
03-27.

recommendations

online,

secret

to

dive

[4] Deep
tem,
deep-dive-into-netflixs-recommender-system-341806ae3b48,
accessed: 2016-04-30.

sys-
netﬂix
https://towardsdatascience.com/

recommender

into

[5] Youtube

recommendations

what

of
watch,
youtubes-recommendations-drive-70-of-what-we-watch/,
cessed: 2018-01-13.

we

drive

70

percent
https://qz.com/1178125/
ac-

[6] N. Heist, S. Hertling, D. Ringler, H. Paulheim, Knowledge graphs on

the web – an overview (2020). arXiv:2003.00719.

[7] N. Chah, Ok google, what is your ontology? or: Exploring freebase
classiﬁcation to understand google’s knowledge graph (2018). arXiv:
1805.03885.

[8] T. Hanika, M. Marx, G. Stumme, Discovering implicational knowledge

in wikidata (2019). arXiv:1902.00916.

[9] X. Dong, E. Gabrilovich, G. Heitz, W. Horn, N. Lao, K. Murphy,
T. Strohmann, S. Sun, W. Zhang, Knowledge vault: A web-scale ap-
proach to probabilistic knowledge fusion, in: Proceedings of the 20th
ACM SIGKDD international conference on Knowledge discovery and
data mining, 2014, pp. 601–610.

25

[10] M. Nickel, K. Murphy, V. Tresp, E. Gabrilovich, A review of relational
machine learning for knowledge graphs, Proceedings of the IEEE 104 (1)
(2015) 11–33.

[11] Q. Wang, Z. Mao, B. Wang, L. Guo, Knowledge graph embedding: A
survey of approaches and applications, IEEE Transactions on Knowledge
and Data Engineering 29 (12) (2017) 2724–2743.

[12] Z. Zhao, S.-K. Han, I.-M. So, Architecture of knowledge graph construc-
tion techniques, International Journal of Pure and Applied Mathematics
118 (19) (2018) 1869–1883.

[13] S. Auer, V. Kovtun, M. Prinz, A. Kasprzik, M. Stocker, M. E. Vidal,
Towards a knowledge graph for science, in: Proceedings of the 8th Inter-
national Conference on Web Intelligence, Mining and Semantics, 2018,
pp. 1–6.

[14] M. D. Ekstrand, J. T. Riedl, J. A. Konstan, Collaborative ﬁltering rec-

ommender systems, Now Publishers Inc, 2011.

[15] Q. Guo, F. Zhuang, C. Qin, H. Zhu, X. Xie, H. Xiong, Q. He, A survey
on knowledge graph-based recommender systems (2020). arXiv:2003.
00911.

[16] Y. Ning, Y. Shi, L. Hong, H. Rangwala, N. Ramakrishnan, A gradient-
based adaptive learning framework for eﬃcient personal recommenda-
tion, in: Proceedings of the Eleventh ACM Conference on Recommender
Systems, 2017, pp. 23–31.

[17] recommendation-raccoon,

[online],

wikipedia,

https:

//github.com/guymorita/recommendationRaccoon#
recommendationraccoon-raccoon (2013).

[18] recommendation-ger,

[online], wikipedia,

https://github.com/

grahamjenson/ger (2013).

[19] Lenskit recommendation framework,

[online], wikipedia, https://

github.com/lenskit/lkpy (2006–2011).

[20] H. Wang, F. Zhang, X. Xie, M. Guo, Dkn: Deep knowledge-aware net-

work for news recommendation (2018). arXiv:1801.08284.

26

[21] H. Wang, F. Zhang, M. Zhao, W. Li, X. Xie, M. Guo, Multi-task feature
learning for knowledge graph enhanced recommendation (2019). arXiv:
1901.08907.

[22] P. Covington, J. Adams, E. Sargin, Deep neural networks for youtube
recommendations, in: Proceedings of the 10th ACM Conference on Rec-
ommender Systems, New York, NY, USA, 2016.

[23] R. v. d. Berg, T. N. Kipf, M. Welling, Graph convolutional matrix com-

pletion, arXiv preprint arXiv:1706.02263 (2017).

[24] Book-crossing dataset, [online], wikipedia, http://www2.informatik.

uni-freiburg.de/~cziegler/BX/ (2004).

[25] J. Mizgajski, M. Morzy, Aﬀective recommender systems in online news
industry: How emotions inﬂuence reading choices, User Modeling and
User-Adapted Interaction (Apr. 2019).
URL https://doi.org/10.1007/s11257-018-9213-x

27

