0
2
0
2

n
a
J

9
1

]

B
D
.
s
c
[

1
v
6
4
8
6
0
.
1
0
0
2
:
v
i
X
r
a

SQLFlow: A Bridge between SQL and Machine Learning

Yi Wang
Yang Yang
Wei Yan
Mingjie Tang
Yuan Tang
yi.w,yang.y,wei.yan,
m.tang,yuan.tang@antfin.com
Ant Financial USA
Sunnyvale, California

Weiguo Zhu
Yi Wu
Xu Yan
Yongfeng Liu
Yu Wang
weiguo.zhuwg,xiongmu.wy,yancey.xy,
antonov.lyf,wy232418@antfin.com
Ant Financial
Hangzhou, China

Liang Xie
Ziyao Gao
Wenjing Zhu
Xiang Chen
xieliang,gaoziyao,zhuwenjing,
alfredchenxiang@didiglobal.com
DiDi
Beijing, China

ABSTRACT
Industrial AI systems are mostly end-to-end machine learning (ML)
workflows. A typical recommendation or business intelligence sys-
tem includes many online micro-services and offline jobs. We de-
scribe SQLFlow for developing such workflows efficiently in SQL.
SQL enables developers to write short programs focusing on the
purpose (what) and ignoring the procedure (how). Previous data-
base systems extended their SQL dialect to support ML. SQLFlow1
takes another strategy to work as a bridge over various database sys-
tems, including MySQL, Apache Hive, and Alibaba MaxCompute,
and ML engines like TensorFlow, XGBoost, and scikit-learn. We
extended SQL syntax carefully to make the extension working with
various SQL dialects. We implement the extension by inventing a
collaborative parsing algorithm. SQLFlow is efficient and expressive
to a wide variety of ML techniques – supervised and unsupervised
learning; deep networks and tree models; visual model explanation
in addition to training and prediction; data processing and feature
extraction in addition to ML. SQLFlow compiles a SQL program
into a Kubernetes-native workflow for fault-tolerable execution
and on-cloud deployment. Current industrial users include Ant
Financial, DiDi, and Alibaba Group.

ACM Reference Format:
Yi Wang, Yang Yang, Wei Yan, Mingjie Tang, Yuan Tang, Weiguo Zhu,
Yi Wu, Xu Yan, Yongfeng Liu, Yu Wang, Liang Xie, Ziyao Gao, Wenjing
Zhu, and Xiang Chen. 2020. SQLFlow: A Bridge between SQL and Machine
Learning. In Proceedings of ACM Conference (Conference’17). ACM, New
York, NY, USA, 4 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
Most industrial AI systems are end-to-end machine learning work-
flows of multiple steps. Typical workflows include search engines,
recommendation systems for e-commerce, and business analysis

1SQLFlow is an open-source project at https://sqlflow.org/sqlflow

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference’17, July 2017, Washington, DC, USA
© 2020 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

platforms like Salesforce. A workflow usually starts with a Web
server that creates the UI, followed by log collectors like fluientd,
log streams aggregators like Kafka, database systems like Apache

Figure 1: SQLFlow with Jupyter Notebook as the GUI.

 
 
 
 
 
 
Conference’17, July 2017, Washington, DC, USA

Yi Wang, Yang Yang, Wei Yan, Mingjie Tang, Yuan Tang, Weiguo Zhu, Yi Wu, Xu Yan, Yongfeng Liu, Yu Wang, Liang Xie, Ziyao Gao, Wenjing Zhu, and Xiang Chen

Hive, and ML systems built on top of TensorFlow [4], PyTorch [11],
XGBoost [5], or scikit-learn [12]. Most workflows are cyclic be-
cause the trained models help the Web server to answer more user
questions via an online serving system or offline scoring system.
Each of such a workflow might need tens or hundreds of engineers
to build and continuously improve.

A recent trend to simplify the development of end-to-end ML
workflows, with the commonly used components getting modulized,
is to program and deploy the workflow as a whole using expressive
languages like Python. Examples include Amazon SageMaker [7],
Netflix Metaflow [2], and BlazingSQL [10].

We propose SQLFlow to program in SQL other than Python. SQL
is unique as it allows programmers to focus on their purposes while
ignoring the procedure of achieving that purpose, thus improves
the development efficiency significantly. Figure 1 shows how to
train an XGBoost model, make a prediction, and visually explain
the model by calling SHAP [8]. The SQLFlow syntax extension used
in this SQL program takes care of data pre-processing and extracts
features automatically.

Some database systems extend their SQL dialects to support ML.
For example, Google BigQuery [15], a database system running on
Google Cloud, adds the CREATE MODEL statement for training
ML models. Other examples include Microsoft SQL Server Machine
Learning Services [3], Azure SQL Database [9], Amazon Athena [1],
IBM DB2 [14], Apache Madlib [6], and MemSQL [13]. However,
these approaches have some drawbacks.

• They lock end-to-end ML users to specific database systems.
Switching to another database system requires rewriting of
all ML solutions.

• Most of the above systems and their extensions are not open-
source, thus prohibits community contributions of ML fea-
tures.

• These solutions are based on existing database computing
infrastructures, and cannot use the fault-tolerance and ex-
tensibility of general cluster computing technologies like
Kubernetes.

SQLFlow takes a different strategy. It is open-sourced and works
like a bridge connecting various database systems and ML engines.
Since open-sourced in May 2019, the community added support to
MySQL, Apache Hive, Alibaba MaxCompute, TensorFlow, XGBoost,
scikit-learn, and SHAP. The list is growing. To make the ML syntax
extension to work with various SQL dialects, we carefully designed
the extension and invented a collaborative parsing algorithm, which
calls parsers of SQL dialects and that of the syntax extension.

Instead of running ML workloads on database infrastructure,
SQLFlow compiles SQL programs into workflows for deployment
and execution on Kubernetes, which is overwhelming public clouds
and on-premise clusters. Workflow execution engines, like Argo2
and Tekton3, run on Kubernetes and provide fault-tolerance and
scalability. SQLFlow-generated workflows access the database sys-
tems and ML engines, which also run on Kubernetes. The compi-
lation algorithm automatically generates workflow steps like data
cleanup and feature extraction to fill the gap between raw data and
ML.

In the ecosystem of end-to-end ML, there are three roles:

• End-to-end ML users call ML models from SQL.
• ML researchers define models in Python and abstraction

APIs like Keras.

• Tool developers create ML engines and database systems in

mostly C++ and Go.

ML researchers need to write workflows to verify and test model
definitions. When doing so, they prefer using their well-used tool
Python. However, Argo and Tekton accept YAML files, which are
lengthy and error-prone to write. To flourishing the community,
SQLFlow provides Couler, a Python library for describing work-
flows. Tool developers can contribute Couler functions to encapsu-
late their tools, for example, couler.train_xgboost_model and
couler.run_mysql_query. ML researchers can call these functions
to build up their workflows. SQLFlow translates SQL programs
in Couler programs to ease of debugging. Couler then translates
Python programs into YAML files.

The open architecture of SQLFlow poses some restrictions. For
example, we don’t yet have a plan to support transactions shortly.
Users can use the transaction mechanism provided by the database
systems in their SQL programs, as the generated workflow submits
"normal SQL statements" to the database system for execution.
However, SQL statements with SQLFlow syntax extensions can not
be in part of the transactions.

2 SYNTAX EXTENSION
Some prior work adds user-defined functions (UDFs) to a database
system to support ML. For SQLFlow, however, it is intractable to
implement the UDFs for all combinations of the supported database
and ML systems. Some other works introduce new statements. For
example, Google BigQuery has the CREATE MODEL statement.
We noticed that many data analysts have SELECT statements for
data cleaning, data transformation, and feature extraction. The
next thing in their mind is naturally ML. It is intuitive to append a
clause for the purpose. SQLFlow introduces the following clause
extensions.

SELECT ... TO TRAIN model_definition

WITH parameters
COLUMN features LABEL target
INTO model_name;

SELECT ... TO PREDICT field USING model_name;
SELECT ... TO EXPLAIN model_name WITH parameters;
The TRAIN clause trains a model using the result from the SE-
LECT statement, where model_definition names a Python class
that defines a model. For example, DNNRegressor, or prefixed with
the package name, dnn.Regressor, and even Docker image name,
sqlflow/models.dnn.Regressor. The identifier model_name names
an output folder for saving the trained model parameters. It can
be a URL pointing to external storage like Amazon S3. Users can
specify ML parameters using the WITH clause. For example, WITH
learning_rate=0.01, hidden_units=[10, 100, 15]. We are working
on automatic parameters tuning using Katib4. The COLUMN and
LABEL clauses specify the input and output of models. LABEL is
not required when training unsupervised models.

2https://argoproj.github.io
3https://github.com/tektoncd/pipeline

4https://github.com/kubeflow/katib

SQLFlow: A Bridge between SQL and Machine Learning

Conference’17, July 2017, Washington, DC, USA

The PREDICT clause makes a prediction using a trained model.

The EXPLAIN clause visually explains a model.

The word TO before TRAIN, PREDICT, and EXPLAIN is neces-
sary for disambiguation. In some SQL dialects, for example, MySQL,
the SELECT statement could have aliases. Users can write SELECT
* FROM t1, t2, t3 TRAIN ... where TRAIN is an alias of table t3.
As a reserved keyword in the SQL language specification, various
dialects don’t accept TO as table or alias name.

3 COLLABORATIVE PARSING
The collaborative parsing algorithm allows SQLFlow to understand
“normal statements” of SQL dialects, and those SELECT statements
followed by SQLFlow extension clauses. This parsing algorithm
can use any open-source parser, like the MySQL/TiDB, HiveQL,
and Calcite parser, and proprietary parser under grants when we
integrate with non-open-source systems. This parsing algorithm
also calls the SQLFlow extension parser written in goyacc and un-
derstanding only the TO TRAIN, PREDICT, and EXPLAIN clauses.
As shown below, the collaborative parser repeatedly calls the di-
alect parser via parseDialect and the SQLFlow extension parser via
parseExtension.

Input: A SQL program with optional SQLFlow extensions
Output: Parsed statement list and optionally an error
lst ← []
while True do

stmts, stop_at, err ← parseDialect(program)
if err (cid:44) nil then
return [], err
lst ← append(lst, stmts)
if stop_at the end of program then

return lst, nil

program ← program[stop_at:]

ext, stop_at, err ← parseExtension(program)
if err (cid:44) nil then
return [], err

lst ← appendExtension(lst, ext)
if stop_at the end of program then

return lst, nil

program ← program[stop_at:]

end

The function parseDialect calls the dialect parser up to twice.
The first call would fail if there is an SQLFlow extension clause in
the program and stop at the beginning of the clause. In this case,
parseDialect makes a second call to parse the part of the SQL pro-
gram before the error position, which is before the extension clause.
If there are no syntax errors in that part, the second call succeeds.
This algorithm relies on the convention that the dialect parsers
report error position, which is true because parsers are supposed to
report errors position. They do this in different ways; for example,
the MySQL/TiDB parser returns strings, but HiveQL and Calcite
parser throw exceptions. We have a thin layer of abstraction to
shield these minor differences.

4 COMPILE SQL TO WORKFLOW
SQLFlow can run as a gRPC server or a command-line tool. In ei-
ther way, it compiles a SQL program into a workflow. We used to
represent a workflow by a Python program, which, however, is not
fault-tolerable. In industrial deployments, like on Kubernetes clus-
ters, the SQLFlow service runs as a group of replicated processes,
and Kubernetes might preempt some processes when scheduling. If
the workflow runs as a Python program, they are subject to preemp-
tion. As a solution, we changed SQLFlow to generate workflows in
the form of YAML files for the execution by fault-tolerable engines
like Argo and Tekton.

4.1 Python API of Workflow
The Argo/Tekton YAML files are hard to read and debug. So we
wrote a Python library Couler to represent workflows as Python
source code. The following 6-line Python program runs and prints
a 46-line YAML file representing an equivalent workflow task of
two steps. The YAML file are for execution by Argo/Tekton. In prac-
tice, we have Couler functions corresponding to ML and database
systems like train_xgboost_model and run_mysql_query.

@couler.task
def echo_hello_world(hello, world="El mundo"):
couler.step(cmd=["echo"], args=[hello])
couler.step(cmd=["echo"], args=[world])

echo_hello_world("Aloha")

4.2 Two-Tier Compilation
SQLFlow features a two-tier compilation architecture. The top one
translates a SQL program into a YAML, where each SQL statement
is a step. Argo and Tekton run each step as a container, inside which,
SQLFlow translates the SQL statement into a Python program and
runs it. If the statement is a normal one, the generated Python
program submits it literally to the database system for execution;
or, if it is a SELECT statement with SQLFlow extension, the Python
program retrieves data, extracts features, and launches the ML task.
The two-tier compilation is necessary because SQLFlow needs
to do feature extraction, an essential part of end-to-end machine
learning, at run-time, after the execution of previous SQL state-
ments and the schema of the input data table is known. The second
tier compilation happens at the run-time of the first tier.

5 DEMO OVERVIEW
The objectives of this demo are to show SIGMOD attendees: (1) the
superior efficiency of end-to-end machine learning experience pro-
vided by an open architecture that connects various database and
ML systems, (2) the expressiveness of SQLFlow syntax extension on
both supervised and unsupervised learning, and (3) the versatility
to integrate with GUI systems and to work alone as command-line
compiler toolchain.

We plan to run SQLFlow as a local service so that users can
interact with it via Jupyter Notebook or from the command-line.
We plan to preload datasets to MySQL and Hive running locally
and to MaxCompute running on the public cloud; users can access
them from SQLFlow. We provide a supplementary video with this
paper showing four episodes; each is a typical use case. As shown in
Table 1, these examples cover four datasets, various combinations

Conference’17, July 2017, Washington, DC, USA

Yi Wang, Yang Yang, Wei Yan, Mingjie Tang, Yuan Tang, Weiguo Zhu, Yi Wu, Xu Yan, Yongfeng Liu, Yu Wang, Liang Xie, Ziyao Gao, Wenjing Zhu, and Xiang Chen

Episode Database system

Database deployment ML system Model explanation

Learning paradigm UI

Table 1: Episodes in the Demo

One.
Two.
Three.
Four.

MySQL
Apache Hive
Alibaba MaxCompute Cloud
MySQL

local
Docker

Kubernetes

SHAP
XGBoost
TensorFlow SHAP
scikit-learn metaplot
XGBoost

a text-backend for SHAP

Supervised
Supervised
Unsupervised
Supervised

Jupyter
Jupyter
Jupyter
Terminal

of database and ML systems, tree models and deep learning models,
supervised and unsupervised learning. They also cover visual model
explanations in the GUI and a text-based terminal. The audience
can freely change example SQL programs used in the episodes and
invent their own. They are free to invent new combinations out
of Table 1, like unsupervised learning with MySQL and XGBoost
models with Hive.

Episode One. This episode shows SQLFlow works with MySQL
and XGBoost on supervised learning of tree models. It also shows
the visual model explanation in Jupyter Notebook by calling SHAP.
The dataset is Boston Housing5. This episode includes the following
steps. (1) Dataset examination by SELECT few rows to display in
the Jupyter Notebook. Demo audience can change and add new SQL
statements freely to, say, select a subset of the dataset, or split the
dataset into training and test data. (2) We train an XGBoost model
using the SELECT ... TO TRAIN syntax extension. The audience
can train any other models in the model zoo. (3) Using the SELECT
... TO PREDICT syntax extension, we make a prediction using
the trained model. And (4) Using the SELECT ... TO EXPLAIN
extension, we explain the model visually as an image showing in
Jupyter Notebook. The audience can change visual explainer and
get other types of visualizations.

Episode Two. This episode shows SQLFlow works with Apache
Hive and TensorFlow using the Iris Dataset6. This episode follows
the same steps as Episode One but focusing on deep learning models
other than tree models.

Episode Three. Differing from the previous two, this episode is
about unsupervised learning. It uses Alibaba MaxCompute and
scikit-learn. The dataset is the Electric Power Consumption7. SHAP
doesn’t support explaining an unsupervised model, so we created
our own tool with metaplot. Episode Three consists of the same
steps as the previous episodes.

Episode Four. The episode shows how to use SQLFlow as a com-
mand line tool that compiles a SQL program to workflow, as ex-
plained in Section 4. This episode includes the following steps. (1)
In the terminal, we type a command to convert the SQL program
used in episode one into a Couler program. (2) Using the command-
line tool couler, we convert the Couler program into a YAML file.
(3) Using the workflow engine Argo’s client tool, we submit the
YAML for execution on a Kubernetes cluster. The audience is free
to compare the SQL, Couler, and YAML files; they would notice
that the SQL program has a few lines of code, the Couler program

5https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html
6https://en.wikipedia.org/wiki/Iris_flower_data_set
7https://www.kaggle.com/uciml/electric-power-consumption-data-set

is longer than the SQL program, and the YAML file is very lengthy.
In the dashboard of Argo in a Web browser, the audience is free
to explore the visualized steps and their running progress. They
are can also type Argo commands to monitor the progress and
logs using command-line tools in the terminal. In the terminal,
the audience can see the visual model explanation in the form of
ASCII-art generated by a text-based backend we invented to work
with SHAP. From these steps, the audience might feel that sqlflow
works like GCC as a compiler and couler works like LD as a linker
as it generates an executable (YAML) file.

REFERENCES
[1] 2020. Amazon Athena adds support for invoking machine learning models in
SQL queries. https://aws.amazon.com/about-aws/whats-new/2019/11/amazon-
athena-adds-support-for-invoking-machine-learning-models-in-sql-queries

[2] 2020. A framework for real-life data science. https://metaflow.org
[3] 2020. SQL Server Machine Learning Services (Python and R) documentation. https:
//docs.microsoft.com/en-us/sql/advanced-analytics/?view=sql-server-2017
[4] Martín Abadi and et al. 2016. TensorFlow: A System for Large-Scale Machine
Learning. In 12th USENIX Symposium on Operating Systems Design and Imple-
mentation (OSDI 16). Savannah, GA, 265–283.

[5] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system.
In Proceedings of the 22nd acm sigkdd international conference on knowledge
discovery and data mining. ACM, 785–794.

[6] Xixuan Feng, Arun Kumar, Benjamin Recht, and Christopher Ré. 2012. Towards
a unified architecture for in-RDBMS analytics. In Proceedings of the 2012 ACM
SIGMOD International Conference on Management of Data. ACM, 325–336.
[7] Ameet V. Joshi. 2020. Amazon’s Machine Learning Toolkit: Sagemaker. Springer
International Publishing, Cham, 233–243. https://doi.org/10.1007/978-3-030-
26622-6_24

[8] Scott M Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model
Predictions. In Advances in Neural Information Processing Systems 30, I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(Eds.). Curran Associates, Inc., 4765–4774.

[9] Anthony Puca Mike Manning David Gollob Marshall Copeland, Julian Soh. 2015.
Microsoft Azure: Planning, Deploying, and Managing Your Data Center in the Cloud.
Apress.

[10] Alexander Ocsa. 2019. SQL for GPU Data Frames in RAPIDS Accelerating end-
to-end data science workflows using GPUs. LatinX in AI Research at ICML 2019.
Poster.

[11] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,
Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
2017. Automatic differentiation in PyTorch. (2017).

[12] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.
Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-
napeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine
Learning in Python . Journal of Machine Learning Research 12 (2011), 2825–2830.
[13] Nikita Shamgunov. 2020. MemSQL as a Data Backbone for Machine Learning and
AI. https://www.memsql.com/blog/memsql-data-backbone-machine-learning-
and-ai/

[14] Yuanyuan Tian, Wen Sun, Sui Jun Tong, En Liang Xu, Mir Hamid Pirahesh, and
Wei Zhao. 2019. Synergistic graph and SQL analytics inside IBM Db2. Proceedings
of the VLDB Endowment 12, 12 (2019), 1782–1785.

[15] Jordan Tigani and Siddartha Naidu. 2014. Google BigQuery Analytics. John Wiley

& Sons.

