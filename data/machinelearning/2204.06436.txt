2
2
0
2

r
p
A
3
1

]

G
L
.
s
c
[

1
v
6
3
4
6
0
.
4
0
2
2
:
v
i
X
r
a

LABEL AUGMENTATION WITH
REINFORCED LABELING FOR WEAK SUPERVISION

GÃ¼rkan Solmaz, Flavio Cirillo, Fabio Maresca, Anagha Gode Anil Kumar
NEC Laboratories Europe
Heidelberg, Germany
{gurkan.solmaz;flavio.cirillo;fabio.maresca;anagha.gode}@neclab.eu

ABSTRACT

Weak supervision (WS) is an alternative to the traditional supervised learning to
address the need for ground truth. Data programming is a practical WS approach
that allows programmatic labeling data samples using labeling functions (LFs)
instead of hand-labeling each data point. However, the existing approach fails
to fully exploit the domain knowledge encoded into LFs, especially when the
LFsâ€™ coverage is low. This is due to the common data programming pipeline that
neglects to utilize data features during the generative process. This paper proposes
a new approach called reinforced labeling (RL). Given an unlabeled dataset and
a set of LFs, RL augments the LFsâ€™ outputs to cases not covered by LFs based
on similarities among samples. Thus, RL can lead to higher labeling coverage for
training an end classiï¬er. The experiments on several domains (classiï¬cation of
YouTube comments, wine quality, and weather prediction) result in considerable
gains. The new approach produces signiï¬cant performance improvement, leading
up to +21 points in accuracy and +61 points in F1 scores compared to the state-of-
the-art data programming approach.

1

INTRODUCTION

Supervised machine learning has proven to be very powerful and effective for solving various classiï¬-
cation problems. However, training fully-supervised models can be costly since many applications
require large amounts of labeled data. Manually annotating each data point of a large dataset may
take up to weeks or even months. Furthermore, only domain experts can label the data in highly
specialized scenarios such as healthcare and industrial production. Thus, the costs of data labeling
might become very high.

In the past few years, a new weak supervision (WS) approach, namely data programming [Ratner
et al. (2016; 2017)], has been proposed to signiï¬cantly reduce the time for dataset preparation. In
this approach, a domain expert writes heuristic functions named labeling functions (LFs) instead of
labeling each data point. Each function annotates a subset of the dataset with an accuracy expected
to be better than a random prediction. Data programming has been successfully applied to various
classiï¬cation tasks. However, writing LFs might not always be trivial, for instance, when data points
are huge vectors of numbers or when they are not intuitively understandable. Developers can quickly
code a few simple functions, but having heuristics to cover many corner cases is still a burden. Further,
simple heuristics might cover only a tiny portion of the unlabeled dataset (small coverage problem).

The existing data programming framework Snorkel [Ratner et al. (2016; 2017)] implements a machine
learning pipeline as follows. The LFs are applied to the unlabeled data points and the outcomes of
LFs produce a labeling matrix where each data point might be annotated by multiple, even conï¬‚icting,
labels. A generative model processes the labeling matrix to make single label predictions for a subset
of data points, based on the agreements and disagreements on the LF outputs for a given data point
x(i) using techniques such as majority voting (MV) or minimizing the marginalized log-likelihood
estimates. Later, the label predictions are used to train a supervised model that serves as the end
classiï¬er (discriminative model). This approach has two major limitations: 1) Coarse information
about the dataset (only LF outputs) fed to the generative model, 2) Lack of generalization due to
the sparsity of labeling matrix and relying only on end classiï¬er to generalize. The current data

1

 
 
 
 
 
 
Figure 1: The label augmentation pipeline that brings data features f1, f2, . . . , fn and weakly-
supervised labels LF1, LF2, . . . , LFm together in the generative process.

programming does not take the data pointsâ€™ data features into account during the generative process,
even though they are available throughout the pipeline. It utilizes the data features of only the data
points with label predictions to train the end classiï¬er. Various additional techniques are considered
to complement the existing approach [Varma & RÃ© (2018); Chatterjee et al. (2020); Varma et al.
(2019); Nashaat et al. (2018); Varma et al. (2016)] and improve the learning process. However, these
approaches do not address this major problem in the design of existing data programming.

This article proposes a label augmentation approach for weak supervision that takes the data features
and the LF outputs into account earlier in the generative process. The proposed approach utilizes the
features for augmenting labels. The augmentation algorithm, namely reinforcement labeling (RL),
checks similarities between data features to project existing LF outcomes to non-existing labels in the
matrix (i.e., to unknown cases or abstains). Moreover, it uses a heuristic that considers unknown cases
â€œgravitateâ€ towards the known cases with LF output labels. In such a way, RL enables generalization
early on and creates a â€œreinforcedâ€ label set to train an end classiï¬er.

Label augmentation extends the data programming to new scenarios, such as when LFs have low
coverage, domain experts can implement only a limited number of LFs, or LFs outcome result in
a sparse labeling matrix. Label augmentation can provide satisfactory performances in these cases,
although data programming was previously non-applicable. The proposed approach can reduce the
time spent by the domain experts to train a classiï¬er as they need to implement fewer LFs. One
advantage compared to the existing complementary approaches is that RL does not require any
additional effort for labeling data, annotating data, or implementing additional LFs. In other words,
the label augmentation enhances classiï¬cation without any further development burden or assumption
of available labeled datasets (e.g., the so-called â€œgold dataâ€). Furthermore, one can easily combine
this approach with the existing solutions.

The RL method is implemented and tested compared to Snorkel (Sn) using different fully-supervised
models as end classiï¬ers. The experimental results span classiï¬cation tasks from several domains
(YouTube comments, white/red wine datasets, weather prediction). The new approach outperforms
the existing model in terms of accuracy and F1 scores, having closer outcomes to the fully-supervised
learning, thanks to the improved coverage that enables end classiï¬er convergence.

2 METHOD DESCRIPTION

2.1 BACKGROUND ON DATA PROGRAMMING

In data programming [Bach et al. (2019); Ratner et al. (2017)], a set of LFs annotate a portion (subset)
of the original unlabeled dataset X = {x(1), x(2), . . . , x(k)} with a total labeling coverage of Î³ âˆ— |X|,
where Î³ âˆˆ [0, 1]. Given a data point x(i), an LFj takes x(i) as input and annotates the input with a
label. LFs are considered weak supervisors implemented by application developers, and they can
programmatically annotate many data points at once, as opposed to hand-labeling data points one by
one. On the other hand, LFs may have lower accuracies than ground truth for the data points.

2

ğ¿ğ¹1ğ¿ğ¹2â€¦ğ¿ğ¹ğ‘šğ’‡ğŸğ’‡ğŸâ€¦ğ’‡ğ’10â€¦00.230.59â€¦0.8201â€¦00.010.98â€¦0.12-1-1â€¦-10.320.16â€¦0.23â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦1-1â€¦-10.150.92â€¦0.01ğ¿ğ¹outputsData featuresğ’‡ğŸğ’‡ğŸâ€¦ğ’‡ğ’0.230.59â€¦0.820.010.98â€¦0.120.320.16â€¦0.23â€¦â€¦â€¦â€¦0.150.92â€¦0.01Data featuresReinforced Labeling0.820.120.43â€¦0.65EndClassifierEnd predictionsGenerative processif (condition A):  return 1;elif(condition B): return 0;else: return -1;if (condition C): return 1;else: return -1;if (condition D): return 0;else: return -1;ğ¿ğ¹1ğ¿ğ¹2ğ¿ğ¹ğ‘šâ€¦LF label augmentationFurther generalizationWS sourceUnlabeled datasetpreprocessingGenerative modelFigure 2: Illustration of the reinforced labeling method (see Alg. 1 in Supp. material A.3).

For a binary classiï¬cation task, an LF may return two classes or abstain from making a prediction.
For simplicity, let us consider LFj returning 1 or 0 as the class labels and -1 (abstain) when it refrains
from class prediction. LFsâ€™ outputs form a labeling matrix where rows represent the data point indices
x(1), x(2), . . . , x(k), and columns represent the LF indices LF1, LF2, . . . , LFm. A generative model
takes the labeling matrix as input, ï¬lters out the data points with no label (all LFs voted for abstains),
and tries to predict a label for the remaining data points. An example of a generative model might be
a majority voter (MV) based on LF outputs or by minimizing the negative log marginal likelihood
[Ratner et al. (2017)] (likelihood over latent variables, i.e., LF outputs). Dependency structures
between the LFs are learned as shown in Alg. 1 in [Bach et al. (2017)]. In both MV and marginal
likelihood approaches, the generative model takes the labeling matrix as input and tries to make a
label decision based on the agreements or disagreements of LFs. The generative model may fail to
make a decision in certain cases, such as when equal numbers of LFs disagree on a data point. Lastly,
the features of the weakly-labeled data points within X and the labels from the generative model
are used to supervise an end classiï¬er (discriminative) model. The design of the data programming
model is agnostic to the end classiï¬er (discriminative) model, so various supervised machine learning
models can be candidates as the end model.

2.2 LABEL AUGMENTATION AND REINFORCED LABELS

In the described design of data programming, the two limitations mentioned above (in Sec. 1), namely
the coarse information in the labeling matrix and sparsity, lead to failure for generalizing to new and
unseen data points. Therefore, these limitations may lead to reduced performances. In such scenarios,
the outputs of the existing generative model may not be satisfactory to train the end classiï¬er.
Therefore, the end classiï¬er model may not converge or generalize well enough to cover different
cases. Implementing many LFs that cover different cases is costly and not very straightforward in
most scenarios. Although various additional techniques focus on the weak supervision problem
[Varma & RÃ© (2018); Chatterjee et al. (2020); Varma et al. (2019); Nashaat et al. (2018); Varma et al.
(2016)], they rather extend the existing pipeline with additional features. On the other hand, label
augmentation targets these major limitations by eliminating the sparsity using data features. Thus, it
can lead to higher accuracy.

1 , x(i)

Fig. 1 illustrates the new pipeline for label augmentation. The new generative process brings
together the outputs of (cid:104)LF1, LF2, . . . , LFm(cid:105), and the data features f1, f2 . . . , fn of data points
x(i) = {x(i)
n } in the unlabeled dataset early on. Different methods can utilize data
features in the generative process for augmenting the labels in the labeling matrix. This label
augmentation approach differs from the existing â€œdata augmentationâ€ approaches [Wang et al. (2019);
Tran et al. (2017); Cubuk et al. (2019)] that create new (synthetic) data points, as the new goal is to
project LF outcomes to the existing data points as opposed to creating new data points.

2 , . . . , x(i)

The outcome of the new generative process can be more representative of the data and weak supervi-
sors than the outputs of the previously existing generative process due to additional coverage and
accuracy gains without any additional LF implementation or data annotation. For instance, abstain
values (-1s) from the LFsâ€™ outputs representing the unknown cases (left side of the matrix in Fig. 1)
can be predicted by the new generative process (as outlined in Sec. 2.3). The abstain values in the
labeling matrix can be augmented with classiï¬cation prediction values.

3

ğ¿ğ¹1ğ¿ğ¹2â€¦ğ¿ğ¹ğ‘šğ‘“1ğ‘“2â€¦ğ‘“ğ‘›ğ‘¥(1)10â€¦00.230.59â€¦0.82ğ‘¥(2)01â€¦00.010.98â€¦0.12ğ‘¥(3)-1-1â€¦-10.320.16â€¦0.23â€¦         â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦ğ‘¥(ğ‘˜)1-1â€¦-10.150.92â€¦0.01ğ¿ğ¹outputsData featuresData pointsà·ğ‘–=1ğ‘˜ğ¸ğ‘“ğ‘“ğ‘¥(ğ‘–),ğ¿ğ¹2ğ‘¥(ğ‘–),ğ‘¥(3)â‡’ğ¿ğ¹2ğ‘¥(3)=âˆ’1ïƒ Abstainğ¸ğ‘“ğ‘“ğ‘¥(1),ğ¿ğ¹2ğ‘¥(1),ğ‘¥(3)<0ğ¸ğ‘“ğ‘“ğ‘¥(2),ğ¿ğ¹2ğ‘¥(2),ğ‘¥(3)>0â€¦ğ¸ğ‘“ğ‘“ğ‘¥(ğ‘˜),ğ¿ğ¹2ğ‘¥(ğ‘˜),ğ‘¥(3)=0RL method iteration at   ğ¿ğ¹2for ğ‘¥(3)Iteration over LFsIteration over dataFigure 3: Illustration of the gravitation-based RL method Figure 4: The IQR factor adjustment to

dynamically select Îµ for gravitation.

Data programming applies statistics only to data points that are already covered by LFs, resulting in a
single predicted label for a subset of the dataset. Similarly, by using likelihood estimation over the
augmented matrix, the generative process predicts â€œreinforced labelsâ€. The details of the generative
model implementation can be seen in [Bach et al. (2017)]. After the generative process, the reinforced
labels are used to train the end classiï¬er. The end classiï¬er can still be the same supervised machine
learning model. As the augmented matrix has more density compared to the labeling matrix, it may
lead to a larger training set. As a simple example, a disagreement between two LFs for a data point
can be eliminated by a new prediction for the abstain case of a third LF. As a result, more data
points can be used to train the end model. The beneï¬ts of using the end classiï¬er include further
generalization for the data points that are still not labeled. Furthermore, the reinforced labels can
ï¬ne-tune pretrained machine learning models.

2.3

IMPLEMENTING REINFORCED LABELING

Let us describe the RL method for label augmentation in the generative process. The intuition behind
this method is that a data point x(j), that a LF does not label, might have very similar features
compared to another data point, such as a data point x(i) labeled by the same LF. x(j) might not
be labeled due to the conditions or boundaries in the LF or missing a subset of the data features
in its heuristic implementation. Reinforcing the labels means predicting a label for the previously
unlabeled data points by certain LFs and therefore creating a denser version of the labeling matrix
during the generative process. Furthermore, RL could lead to a higher coverage Î³
âˆ— |X| â‰¥ Î³ âˆ— |X|
for training the end classiï¬er.

(cid:48)

Fig. 2 illustrates the RL method. The method iterates over (cid:104)LF1, LF2, . . . , LFm(cid:105) and for each LFl
on the left part of the matrix, it ï¬nds the data points x(j)s that LFl abstains from (-1s on the matrix).
The abstain points are compared with the points that are labeled by LFl for their distances based
on data features. The similarity between a point x(i) labeled by LFl point and the unlabeled x(j) is
represented by the â€œeffect functionâ€ Ef f (x(i), LFl(x(i)), x(j)). As inputs, the effect function has the
2 , . . . , x(i)
1 , x(i)
data features of the labeled x(i) = {x(i)
n }, output label LFl(x(i)) and the features of
2 , . . . , x(j)
the unlabeled x(j) = {x(j)
n }. Based on the label, the effect may take a positive value
(if LFl(x(i)) = 1) or a negative value (if LFl(x(i)) = 0). For any other data point that the LFl also
abstains, it takes the value zero. These values can be aggregated as (cid:80)k
i=1 Ef f (x(i), LFl(x(i)), x(j))
for any LFl and where k = |X|.

1 , x(j)

We propose a heuristic algorithm to implement RL based on gravitation. Fig. 3 illustrates the heuristic
method considering a simple case of having only two data features x1, x2. In the gravitation-based
RL method, for the abstain point x(j) that is not labeled by LFl, each other point that LFl labels
(shown as colored particles in Fig. 3) is considered as a particle that attracts x(j) towards positive
(LFl(x(i)) = 1) or negative (LFl(x(i)) = 0) aggregated effect. Fig. 3 shows positive or negative
attractions by different-colored lines between the data points. The attraction is inversely proportional

4

ğ‘¥2ğ‘¥1à·ğ‘–=1ğ‘˜ğ¸ğ‘“ğ‘“ğ‘¥(ğ‘–),ğ¿ğ¹1ğ‘¥(ğ‘–),ğ‘¥(1)>Æâ‡’ğ¿ğ¹1ğ‘¥(1)=0ğ¿ğ¹1ğ‘¥(ğ‘–)=âˆ’1ğ¿ğ¹1ğ‘¥(ğ‘–)=0ğ¿ğ¹1ğ‘¥(ğ‘–)=1Distance adding negative effectDistance adding positive effectDistance exceeding Æğ‘‘Aggregated effect thresholdÆğ‘¥(1)=ğ‘¥1(1),ğ‘¥2(1)Æğ‘‘ğ‘˜data points w/ features ğ‘¥1,ğ‘¥2Q1median       IQR Q3outliersoutliersAggregated effect boundaries adjustment based on IQR factorğ’ƒğ’‘ğ’ğ’”=ğ‘¸ğŸ‘+ğ‘°ğ‘¸ğ‘¹âˆ—ğ’‰ğ‘°ğ‘¸ğ‘¹ğ’ƒğ’ğ’†ğ’ˆ=ğ‘¸ğŸâˆ’ğ‘°ğ‘¸ğ‘¹âˆ—ğ’‰ğ‘°ğ‘¸ğ‘¹to the pairwise distance of every labeled point x(i) to x(j). For optimizing the calculations, we
consider a maximum distance threshold Îµd above which attraction does not take place. Hence, the
effect function is deï¬ned as follows:

Ef f

(cid:16)

x(i), LFl(x(i)), x(j)(cid:17)

=

(cid:40)

Î²
Distance(x(i),x(j))Î±
0

if Distance(x(i), x(j)) < Îµd

otherwise,

(1)

where Î± and Î² : are constants for optional adjustment of the effect and distance relationship. As a
distance function Distance(x(i), x(j)) RL can use different metrics based on the data types (e.g.,
sensor readings, text, image) such as Euclidean, Levenshtein or Mahalanobis distances. The effects
of all labeled points are aggregated based on their positive or negative attractions. The decision of
updating an abstain value depends upon a parameter called the aggregated effect threshold Îµ. x(j) is
labeled with that class for the given LFl if the following condition holds:

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|X|
(cid:88)

i=1

Ef f

x(i), LFl(x(i)), x(j)(cid:17)
(cid:16)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

> Îµ.

(2)

Îµ adjusts the degree of reinforcement and the density of the resulting augmented matrix. In the evalu-
ation section, we consider various empirical parameters and a heuristic for automatic Îµ conï¬guration.

3 EXPERIMENTAL EVALUATION

3.1 BENCHMARK FRAMEWORK

The experimental study includes four metrics: Classiï¬cation accuracy (# correct/# tested), precision,
recall, and F1 score (F1). Particularly, higher accuracy and F1 score are important to understand the
performance for the given classiï¬cation task for non-biased and biased tasks, respectively. Other
than these performance metrics, the experimental evaluation includes insights on the labeling itself
(labeling metrics) such as: Number of LFs: The number of LFs used as weak supervisors; Labeled
samples: The number of samples labeled for training the end models; LF coverage: The ratio
of the number of data points labeled by LFs to the number of samples; LF overlap: The ratio of
LFsâ€™ overlapping outputs with each other to the number of samples; LF conï¬‚icts: The ratio of LFsâ€™
conï¬‚icting outputs with other each other to the number of samples. The last three metrics represent
the mean values, averaged based on the number of LFs.

We evaluate the RL approach with four datasets from different domains: YouTube comments [Alberto
et al. (2015)], red and white wine quality [Cortez et al. (2009)], and Australia Weather (rain) datasets1.
YouTube comments dataset consists of texts that may be legit user comments or spam. This text-based
dataset is used for benchmarking various data programming approaches [Chen et al. (2020); Evensen
et al. (2020); Ren et al. (2020); Karamanolakis et al. (2021); Sedova et al. (2021); Awasthi et al.
(2020)] and also as Snorkelâ€™s tutorial for LFs. The YouTube dataset is largely unlabeled except for a
small testing dataset. Only the text comment is used as a feature for the YouTube dataset, whereas
we removed the others (such as user ID and timestamp). On the other hand, the end models get a
sparse matrix of token counts computed with Scikit-learn CountVectorizer2. To classify the wine
quality, there exist 12 real number features (e.g., acidity, residual sugar) for the two wine datasets. For
training and testing, a wine is considered good (labeled with 1) when the wine quality feature is more
than 5 out of 10; otherwise considered a bad wine (0). The Australia Weather dataset is widely used
for the next-day rain predictions. It also consists of 62 features based on daily weather observations
spanning ten years period. The wine and Australia Weather datasets are fully labeled with ground
truth. For all datasets Euclidean distance metric computes distances between data point pairs. For the
YouTube dataset, Euclidean distance is applied to the one-hot encoding of the tokenized texts3.

Snorkel (Sn) and fully-supervised learning (Sup) are the two main approaches for comparison. In
addition to those, the majority voting (MV) approach is tested as the simple generative model. For

1https://www.kaggle.com/jsphyg/weather-dataset-rattle-package
2https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
3https://www.nltk.org/api/nltk.tokenize.html

5

Dataset

Acc Prec Rec

Reinforced labeling

Snorkel
F1 F1-Gain Acc Prec Rec

Supervised learning
F1

F1 Acc Prec Rec

0.75 0.98 0.47 0.64
YouTube
Red Wine
0.71 0.80 0.66 0.72
White Wine 0.63 0.64 0.98 0.78
0.59 0.29 0.78 0.42
Weather

+61
+7
+34
+34

0.54 1.00 0.02 0.03 0.91 0.96 0.84 0.90
0.61 0.66 0.77 0.65 0.75 0.81 0.74 0.76
0.50 0.82 0.32 0.44 0.54 0.71 0.48 0.57
0.54 0.06 0.10 0.08 0.90 0.86 0.59 0.70

Table 1: RL, Snorkel, and (fully-)supervised learning results: Accuracy, precision, recall, and
F1. F1-Gain shows the F1 score advantage of RL compared to Snorkel.

Dataset

RL labels LF cov. LF ov. LF con.

Îµ

Îµd Sn labels LF cov. LF ov. LF con. End model LFs

Reinforced labeling

Snorkel

All

YouTube
Red Wine
White Wine
Weather

1273
375
3269
3415

0.22
0.12
0.39
0.58

0.11
0.02
0.15
0.56

0.04
0.02
0.07
0.49

75 5.0
125 0.5
350 0.5
200 5.0

916
247
1995
2384

0.16
0.08
0.21
0.19

0.08
0.02
0.04
0.13

0.03
0.01
0.01
0.10

svm
RF
NaivB
RF

5
3
3
6

Table 2: Labeling metrics for the approaches and dataset statistics.

each data point, MV labels the point based on the majority of the LF outputs (i.e., 0 or 1) from the
LFs that do not abstain. We also experiment with MV combined with RL (MVRL). For all results, the
existing data programming approaches Sn and MV use the same set of LFs as RL. The LFs are listed
in Supp. material A.5. We implement Sn and MV as well as the RL approach using the Snorkel library
(version 0.9.5) with the existing features in the tutorial. The framework uses absolute latent labels
for training end model (0,1), RL adapts the same scheme for the generative model and training end
classiï¬ers.Supervised learning leverages ground truth data (30% of the dataset) for the training. For
the experimental scenario, supervised learning is considered the optimal result of machine learning,
whereas the other two models are based on only programmatic labels. For supervised learning, the
small testing dataset of YouTube comments is used for both training and testing due to lacking ground
truth in the training set.

For the end classiï¬er, different machine learning models are used for testing purposes. The models
include two logistic regression models: the ï¬rst one, namely â€œlogitâ€, is the model that Snorkel
uses by default (inverse of regularization strength C := 1000 and liblinear solver); the second
one, â€œLogRâ€, uses lbfgs solver for optimization. In addition, we test the random forest (RF), naive
Bayes (NaivB), decision tree (DT), k-nearest neighbor (knn), support vector machine (svm), and
multi-layer perceptron (mlp) end models. In each experiment, all approaches use the same end
classiï¬er. Each experiment consists of 5 runs, and the results are averaged. We do not observe any
notable difference between experiment runs. Our assumption is that the tested end models learning
behavior is rather deterministic as they use the same the data points for their trainings. There are no
additional hyperparameters used other than the stated in this section.

3.2 EXPERIMENTAL RESULTS

Beneï¬ts of RL The ï¬rst set of results shows the advantage of using RL compared to the existing
data programming approach Sn in terms of accuracy and F1 score gains. Table 1 includes results from
the four datasets in terms of the four performance metrics. In all datasets, we observe substantial
gains in accuracy and F1 scores compared to the benchmark Sn approach. Moreover, RL performance
is closer to fully-supervised learning, although it does not use any ground truth labels, even when
LFs are relatively few. For the YouTube dataset, we experience that although the Snorkel and RL
approaches have the same set of LFs, RL provides up to 64% F1, whereas Sn provides less than 3%
F1. In RL, the end model can converge thanks to the additional coverage by the reinforced labels.
Table 2 reports the number of computed labels after the application of the generative model with
reinforcement (RL) and without it (Snorkel). The gains for accuracy and F1 require no additional
human effort or involvement. Only two parameters Îµ and Îµd conï¬gure the RL (see Table 2 for Table 1
experiments settings). Later in this section, we present how these parameters could be conï¬gured
automatically for any given scenario based on labeling metrics (i.e., LF coverage, LF overlaps, and
LF conï¬‚icts). Furthermore, our approach consistently outperforms Sn for training any of the tested
end models. An extended table in Supp. material A.1 reports the experimental results varying end

6

models for each dataset. Lastly, we observe that Îµd affects the performance as it adjusts the trade-off
between augmenting similar labels and the bias (noise) of the augmentation.

Auto-adjustment of the RL method We observe that Îµ is dependent on the dataset and the set
of LFs. A simple heuristic method to choose Îµ is as follows: First calculate the distribution of
aggregated effects of unlabeled data point for each LF (through a boxplot as in Fig. 4), then set Îµ
to have boundaries bneg and bpos far enough from the quartiles Q1 and Q3. When using 2 such
boundaries are symmetric to aggregate effect equal to 0 and they are chosen by the data scientist
as we did for the results shown in Tables 1. Another way to calculate such boundaries is to have
them symmetrical to the aggregated effect distribution. We can achieve this with the formula
bneg = Q1 âˆ’ IQR âˆ— hIQR (and similarly for bpos) where InterQuartileRange(IQR) = Q3 âˆ’ Q1
and hIQR is a parameter, namely the IQR factor. When hIQR = 1.5, the gravitation method labels
only outliers of the aggregated effect distribution. At this point, there is no need to set the Îµ parameter
and the boundaries automatically adapts to the aggregated effects that depends on the features range,
distance metrics and the sparsity of the initial labeling matrix. However, varying hIQR (as tested for
the range [0, 2]) affects the performances. Thus, the problem of ï¬nding an optimal hIQR still persists.

Fig. 5a-top row shows the comparison with Snorkel generative model, while Fig. 5a-middle row
shows the comparison with MV. As in Fig. 5a-top, hIQR â† 0.5 cause a substantial F1 gain for fewer
LFs (e.g., 5 to 8 LFs), whereas it may cause detrimental bias for the case of a higher number of LFs
as the sum of LFsâ€™ coverage increases. In this case, the reinforcement may not need to be applied as
extensively as when the sum of LFsâ€™ coverage is low. In the latter scenario, using hIQR â† 1.0 would
be the most conservative approach (no reinforcement) that makes sure to add no additional noise.
Similar behavior is observed with majority voter as label aggregation algorithm (Fig. 5a-middle row).

Fig. 5a-bottom row shows the effect of RL with different hIQR in terms of labeled samples, LFsâ€™
coverage, mean overlaps, and mean conï¬‚icts. The smaller is IQR factor, the higher the values for
those metrics are because the boundaries are closer to the IQR. One can infer that for a higher number
of LFs, a smaller IQR factor results in excessive noise confusing the generative model (Sn or MV)
and degrading the end model. Thus, we deï¬ne a simple heuristic to automatically conï¬gure the
gravitation method (shown in Alg. 2 in Supp. material A.3) through calculating hIQR by linking it
to the LF statistics for the given dataset. The below formula uses these statistics and an empirical
constant Î¾.

hIQR = Î¾ âˆ—

coverage âˆ—

(cid:88)

LFl

overlaps âˆ—

(cid:88)

LFl

(cid:88)

LFl

conf licts

(3)

We set Î¾ = 0.35 through empirical experiments. By this approach, the higher density the labeling
matrix has (e.g., more LFs, higher LF coverage), the fewer the abstain labels updated by RL are.

We test the auto-adjustment approach and show the results in Fig. 5b for both Sn (top row) and
MV (middle row). We have also set the second parameter Îµd virtually to inï¬nite, relying on no
hyper-parameter. When fewer LFs exist (e.g., 5, 6, or 7), RL provides signiï¬cant performance gains,
especially for F1 scores. On the other hand, when more LFs exist, RL adjusts itself by becoming
more conservative on the reinforcement. Therefore, auto-adjustment preserves the good performances
when having sufï¬cient LFs, leading to a larger area under the curve for both Sn and MV. Our
proposed RL approach, together with this heuristic, does not require any additional parameter setting
or human effort by automatically adjusting its IQR factor. However, additional studies are due for
more enhanced methods of auto-adjustment.

Fig. 5b shows the effect of the automatic IQR adaptation in terms of labeled samples and LF metrics.
As expected, when fewer LFs exist, RL provides gains for the number of labeled data points and
mean LF coverage, overlaps, and conï¬‚icts. Furthermore, it adapts itself and gradually provides lower
numbers of additional labels for higher numbers of LFs, thus reducing the noise.

Varying distance metrics Our gravitation method highly rely on similarities between data points.
In the experiments described above, we always use the Euclidean distance for the real number features.
Therefore, we investigate how different distance metrics affect the performances. Table 3 shows the
results on the white wine dataset with RF as the end classiï¬er and the previous heuristics to calculate
the aggregated effect boundaries with hIQR â† 1.4. RL consistently provides improved performance
even when the distance metric changes.

7

(a) Top: YouTube comments classiï¬cation results by increasing LFs
(from 5 to 12) with different IQR factors.
Bottom: RL effects on the total number of labeled samples, and mean
LF coverage, overlaps, and conï¬‚icts.

(b) Top: YouTube comments classiï¬cation results by increasing LFs
(from 5 to 12) with auto-adjusted IQR factor.
Bottom: effects of RL with auto-adjusted IQR factor on the total
number of labeled samples and mean LFs coverage, overlaps, and
conï¬‚icts.

Figure 5: (a) Effects of the hIQR by the number of LFs. (b) RL with the auto-adjusted hIQR.

Distance metric Acc Prec Rec

F1

F1-Gain Acc Prec Rec

F1

Reinforced LFs

Snorkel

Chebyshev
Cosine
Euclidean
Hamming
Jaccard
Mahalanobis
Minkowski

0.63
0.66
0.66
0.62
0.60
0.64
0.57

0.65
0.67
0.68
0.66
0.67
0.66
0.68

0.94 0.77
0.96 0.79
0.92 0.78
0.88 0.75
0.79 0.71
0.94 0.78
0.67 0.66

+20
+25
+23
+16
+14
+24
+9

0.53
0.51
0.51
0.52
0.51
0.51
0.52

0.71
0.70
0.70
0.68
0.69
0.70
0.69

0.48 0.57
0.44 0.54
0.46 0.55
0.52 0.59
0.49 0.57
0.44 0.54
0.49 0.57

Table 3: Testing RL with various distance metrics for WhiteWine: RF as end model, hIQR = 1.4.

Limitations As label augmentation relies on the data features, using a poor embedding for distance
computation might lead to detrimental noise in the augmented labeling matrix. In addition, a more
advanced augmentation adjustment would avoid performance degradation due to the tradeoff between
the coverage and noise.

4 RELATED WORK

Data programming [Ratner et al. (2016)] enables programmatically labeling data and training using
WS sources. In particular, the Snorkel framework [Ratner et al. (2017); Bach et al. (2019)] provides
an interface for data programming where users can write LFs and apply them to generate a training
dataset for their end models. The generation of the training dataset relies on the generative model,
and several studies focus on this aspect [Ratner et al. (2017); Bach et al. (2017); Varma et al. (2017;
2019)].

Various recent works focus on extending the existing data programming approach. The extensions
include multi-task classiï¬cation [Ratner et al. (2019)], using small labeled gold data for augmenting
WS [Varma & RÃ© (2018)], learning tasks (or sub-tasks) in slices of dataset [Chen et al. (2019)],
user guidance for LF precision [Chatterjee et al. (2020)], making the training process faster [Fu
et al. (2020)], generative adversarial data programming [Pal & Balasubramanian (2020; 2018)], user
supervision for LF error estimates [Arachie & Huang (2019)], learning LF dependency structures
[Varma et al. (2019)], user annotation of LFs [Boecking et al. (2021)], language description of LFs
[Hancock et al. (2018)], active learning Nashaat et al. (2018), and so on. Varma et al. (2016) aims to
learn common features by using a â€œdifference modelâ€ and feeding these features back to generative
model. Mallinar et al. (2019) takes advantage of the natural language processing query engine to
expand gold labels and generate a label matrix as input for the generative model. Zhou et al. (2020)
adopts a soft LFs matcher approach based on the distances between LFsâ€™ conditions and data points.

8

0.000.250.500.751.00accuracyprecisionrecallf1SnRL_iqr0.0RL_iqr0.5RL_iqr1.0510number_lfs0.000.250.500.751.00510number_lfs510number_lfs510number_lfsMVMVRL_iqr0.0MVRL_iqr0.5MVRL_iqr1.0510number_lfs100012001400labelled_samples510number_lfs0.20.30.40.50.6lfscoverage510number_lfs0.20.4lfsoverlaps510number_lfs0.20.4lfsconflictsSnRL_iqr0.0RL_iqr0.5RL_iqr1.00.000.250.500.751.00accuracyprecisionrecallf1SnRL510number_lfs0.000.250.500.751.00510number_lfs510number_lfs510number_lfsMVMVRL510number_lfs100012001400labelled_samples510number_lfs0.20.30.4lfscoverage510number_lfs0.10.20.30.4lfsoverlaps510number_lfs0.10.20.3lfsconflictsSnRLChen et al. (2020) uses pre-trained machine learning models to estimate distances for natural language
processing. The last two studies focus on the semantic similarity of texts to improve the labeling.

Although the studies mentioned above may improve the existing generative model (e.g., through
additional human supervision), they do not focus on the problem of LF abstraction with coarse
information. Solving this problem would improve the validity of data programming in various
scenarios especially since the human supervision is limited in its nature. In this paper, we identify
this problem and propose the reinforced labeling that takes the data features into account early on
in the generative process. Using this approach, one can leverage the data features and augment the
matrix for further generalization and producing satisfactory performance without additional human
supervision.

Weak supervision approaches outside the context of data programming consider learning from a
set of rules, experts, or workers as in crowdsourcing. Platanios et al. (2017) infer accuracy from
multiple classiï¬er outputs based on the conï¬dences. Safranchik et al. (2020) study the usage of
Hidden Markov Models for tagging data sequences. Dehghani et al. (2018) train deep NNs using
weakly-labeled data. Their approach is semi-supervised, where a teacher network based on rules
adjusts the predictions of a student network trained iteratively by given data samples. In another study,
Dehghani et al. (2017) propose WS to train neural ranking models in natural language processing and
computer vision tasks.

Takeoka et al. (2020) consider leveraging unsure responses of human annotators as WS sources to
improve the traditional supervised approach. Kulkarni et al. (2018) study labeling based on consensus
and interactive learning based on active labeling for multi-label image classiï¬cation tasks. Khetan
et al. (2018) propose an expectation-maximization algorithm for learning workersâ€™ quality, where
each worker represents a WS source for image classiï¬cation tasks. Qian et al. (2020) propose WS
with active learning for learning structured representations of entity names. Guan et al. (2018) learn
individualâ€™s weights for predicting a weighted sum of noisy labelers (experts). Das et al. (2020)
propose a domain-agnostic approach to replace the needs of LFs that apply afï¬nity functions to relate
samples with each other. This approach uses a small gold dataset with probabilistic distributions to
infer probabilistic labels.

Similar approaches other than WS include domain-speciï¬c machine learning applications such as
ontology matching. Doan et al. (2004) use a relaxation method to label a node into a graph dataset
by analyzing features of the nodeâ€™s neighborhood in the graph. The relaxation process is based
on constraint and knowledge that leads to the ï¬nal labeling. In a similar approach, Li & Srikumar
(2019) describe a methodology framework to augment labels guided by external knowledge. In both
approaches, label augmentation is in the ï¬nal phase. These approaches do not involve any generative
process. Lastly, literature related to semi-supervised learning (e.g., [Zhu et al. (2003)]) or other hybrid
approaches (e.g., [Awasthi et al. (2020)]) consider using a mix of clean and noisy labels, whereas this
paper focuses on using only the labels from LFs and improve the validity of the existing approach.

5 CONCLUSION

This paper proposes a novel method for label augmentation in weak supervision. In the new machine
learning pipeline, the proposed RL method in the generative process leverages existing LF outputs
and data features to augment the weakly-supervised labels. The experimental evaluation shows the
beneï¬ts of RL for four classiï¬cation tasks compared to the existing data programming approach in
terms of substantial accuracy and F1 gains. Furthermore, the new method enables the convergence of
the end classiï¬er even when there exist few LFs. We consider applying RL for matching problems
(e.g., entity matching) and active learning as future work. We consider RL as an initial approach
for the identiï¬ed limitation of the generative process, whereas the pipeline opens up the possibility
for more advanced (e.g., machine learning) models to leverage data features during the generative
process.

9

REFERENCES

TÃºlio C. Alberto, Johannes V. Lochter, and Tiago A. Almeida. Tubespam: Comment spam ï¬ltering
on youtube. In 2015 IEEE 14th International Conference on Machine Learning and Applications
(ICMLA), pp. 138â€“143, 2015.

Chidubem Arachie and Bert Huang. Adversarial label learning.

In Proceedings of the AAAI

Conference on Artiï¬cial Intelligence, volume 33, pp. 3183â€“3190, 2019.

Abhijeet Awasthi, Sabyasachi Ghosh, Rasna Goyal, and Sunita Sarawagi. Learning from rules

generalizing labeled exemplars. arXiv preprint arXiv:2004.06025, 2020.

Stephen H Bach, Bryan He, Alexander Ratner, and Christopher RÃ©. Learning the structure of
generative models without labeled data. In International Conference on Machine Learning, pp.
273â€“282. PMLR, 2017.

Stephen H Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cassandra Xia, Souvik
Sen, Alex Ratner, Braden Hancock, Houman Alborzi, et al. Snorkel drybell: A case study in
deploying weak supervision at industrial scale. In Proceedings of the 2019 International Conference
on Management of Data, pp. 362â€“375, 2019.

Benedikt Boecking, Willie Neiswanger, Eric P. Xing, and Artur Dubrawski.

Interactive weak
supervision: Learning useful heuristics for data labeling. In Proceedings of (ICLR) International
Conference on Learning Representations, May 2021.

Oishik Chatterjee, Ganesh Ramakrishnan, and Sunita Sarawagi. Robust data programming with
precision-guided labeling functions. In Proceedings of the AAAI Conference on Artiï¬cial Intelli-
gence, volume 34, pp. 3397â€“3404, 2020.

Mayee F Chen, Daniel Y Fu, Frederic Sala, Sen Wu, Ravi Teja Mullapudi, Fait Poms, Kayvon
Fatahalian, and Christopher RÃ©. Train and youâ€™ll miss it: Interactive model iteration with weak
supervision and pre-trained embeddings. arXiv preprint arXiv:2006.15168, 2020.

Vincent S Chen, Sen Wu, Zhenzhen Weng, Alexander Ratner, and Christopher RÃ©. Slice-based
learning: A programming model for residual learning in critical data slices. Advances in neural
information processing systems, 32:9392, 2019.

Paulo Cortez, AntÃ³nio Cerdeira, Fernando Almeida, Telmo Matos, and JosÃ© Reis. Modeling wine
preferences by data mining from physicochemical properties. Decision support systems, 47(4):
547â€“553, 2009.

Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 113â€“123, 2019.

Nilaksh Das, Sanya Chaba, Renzhi Wu, Sakshi Gandhi, Duen Horng Chau, and Xu Chu. Goggles:
In Proceedings of the 2020 ACM SIGMOD

Automatic image labeling with afï¬nity coding.
International Conference on Management of Data, SIGMOD â€™20, pp. 1717â€“1732, 2020.

M. Dehghani, A. Mehrjou, S. Gouws, J. Kamps, and B. SchÃ¶lkopf. Fidelity-weighted learning. In

6th International Conference on Learning Representations (ICLR), May 2018.

Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W Bruce Croft. Neural
ranking models with weak supervision. In Proceedings of the 40th International ACM SIGIR
Conference on Research and Development in Information Retrieval, pp. 65â€“74, 2017.

AnHai Doan, Jayant Madhavan, Pedro Domingos, and Alon Halevy. Ontology Matching: A Machine

Learning Approach, pp. 385â€“403. Springer Berlin Heidelberg, 2004.

Sara Evensen, Chang Ge, and Cagatay Demiralp. Ruler: Data programming by demonstration for
document labeling. In Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing: Findings, pp. 1996â€“2005, 2020.

10

Daniel Fu, Mayee Chen, Frederic Sala, Sarah Hooper, Kayvon Fatahalian, and Christopher RÃ©. Fast
and three-rious: Speeding up weak supervision with triplet methods. In International Conference
on Machine Learning, pp. 3280â€“3291, 2020.

Melody Guan, Varun Gulshan, Andrew Dai, and Geoffrey Hinton. Who said what: Modeling
individual labelers improves classiï¬cation. In Proceedings of the AAAI Conference on Artiï¬cial
Intelligence, volume 32, 2018.

Braden Hancock, Martin Bringmann, Paroma Varma, Percy Liang, Stephanie Wang, and Christopher
RÃ©. Training classiï¬ers with natural language explanations. In Proceedings of the conference.
Association for Computational Linguistics. Meeting, volume 2018, pp. 1884. NIH Public Access,
2018.

Giannis Karamanolakis, Subhabrata Mukherjee, Guoqing Zheng, and Ahmed Hassan Awadallah.

Self-training with weak supervision. arXiv preprint arXiv:2104.05514, 2021.

Ashish Khetan, Zachary C Lipton, and Animashree Anandkumar. Learning from noisy singly-labeled

data. In International Conference on Learning Representations, 2018.

Ashish Kulkarni, Narasimha Raju Uppalapati, Pankaj Singh, and Ganesh Ramakrishnan. An interac-
tive multi-label consensus labeling model for multiple labeler judgments. In Proceedings of the
AAAI Conference on Artiï¬cial Intelligence, volume 32, 2018.

Tao Li and Vivek Srikumar. Augmenting neural networks with ï¬rst-order logic. In Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics, pp. 292â€“302. Association
for Computational Linguistics, July 2019.

Neil Mallinar, Abhishek Shah, Rajendra Ugrani, Ayush Gupta, Manikandan Gurusankar, Tin Kam
Ho, Q. Vera Liao, Yunfeng Zhang, Rachel K.E. Bellamy, Robert Yates, Chris Desmarais, and
Blake McGregor. Bootstrapping conversational agents with weak supervision. Proceedings of the
AAAI Conference on Artiï¬cial Intelligence, 33(01):9528â€“9533, Jul. 2019.

Mona Nashaat, Aindrila Ghosh, James Miller, Shaikh Quader, Chad Marston, and Jean-Francois
Puget. Hybridization of active learning and data programming for labeling large industrial datasets.
In 2018 IEEE International Conference on Big Data (Big Data), pp. 46â€“55. IEEE, 2018.

Arghya Pal and Vineeth N Balasubramanian. Adversarial data programming: Using gans to relax the
bottleneck of curated labeled data. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 1556â€“1565, 2018.

Arghya Pal and Vineeth N Balasubramanian. Generative adversarial data programming. arXiv

preprint arXiv:2005.00364, 2020.

Emmanouil A Platanios, Hoifung Poon, Tom M Mitchell, and Eric Horvitz. Estimating accuracy
from unlabeled data: a probabilistic logic approach. In Proceedings of the 31st International
Conference on Neural Information Processing Systems, pp. 4364â€“4373, 2017.

Kun Qian, Poornima Chozhiyath Raman, Yunyao Li, and Lucian Popa. Learning structured represen-
tations of entity names using active learning and weak supervision. In Proceedings of the 2020
Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 6376â€“6383,
2020.

Alexander Ratner, Christopher De Sa, Sen Wu, Daniel Selsam, and Christopher RÃ©. Data program-
ming: Creating large training sets, quickly. Advances in neural information processing systems,
29:3567â€“3575, 2016.

Alexander Ratner, Stephen H. Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher RÃ©.
Snorkel: Rapid training data creation with weak supervision. volume 11, pp. 269â€“282, November
2017.

Alexander Ratner, Braden Hancock, Jared Dunnmon, Frederic Sala, Shreyash Pandey, and Christopher
RÃ©. Training complex models with multi-task weak supervision. In Proceedings of the AAAI
Conference on Artiï¬cial Intelligence, volume 33, pp. 4763â€“4771, 2019.

11

Wendi Ren, Yinghao Li, Hanting Su, David Kartchner, Cassie Mitchell, and Chao Zhang. Denoising
multi-source weak supervision for neural text classiï¬cation. arXiv preprint arXiv:2010.04582,
2020.

Esteban Safranchik, Shiying Luo, and Stephen Bach. Weakly supervised sequence tagging from
noisy rules. In Proceedings of the AAAI Conference on Artiï¬cial Intelligence, volume 34, pp.
5570â€“5578, 2020.

Anastasiia Sedova, Andreas Stephan, Marina Speranskaya, and Benjamin Roth. Knodle: Modular

weakly supervised learning with pytorch. arXiv preprint arXiv:2104.11557, 2021.

Snorkel. Intro to labeling functions, 2021. URL https://www.snorkel.org/use-cases/

01-spam-tutorial.

Kunihiro Takeoka, Yuyang Dong, and Masafumi Oyamada. Learning with unsure responses. In
Proceedings of the AAAI Conference on Artiï¬cial Intelligence, volume 34, pp. 230â€“237, 2020.

Toan Tran, Trung Pham, Gustavo Carneiro, Lyle Palmer, and Ian Reid. A bayesian data augmentation
approach for learning deep models. In Proceedings of the 31st International Conference on Neural
Information Processing Systems, pp. 2794â€“2803, 2017.

Paroma Varma and Christopher RÃ©. Snuba: automating weak supervision to label training data.
In Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases,
volume 12, pp. 223. NIH Public Access, 2018.

Paroma Varma, Bryan He, Dan Iter, Peng Xu, Rose Yu, Christopher De Sa, and Christopher RÃ©.
Socratic learning: Augmenting generative models to incorporate latent subsets in training data.
arXiv preprint arXiv:1610.08123, 2016.

Paroma Varma, Bryan He, Payal Bajaj, Imon Banerjee, Nishith Khandwala, Daniel L Rubin, and
Christopher RÃ©. Inferring generative model structure with static analysis. Advances in neural
information processing systems, 30:239, 2017.

Paroma Varma, Frederic Sala, Ann He, Alexander Ratner, and Christopher RÃ©. Learning dependency
structures for weak supervision models. In International Conference on Machine Learning, pp.
6418â€“6427, 2019.

Yulin Wang, Xuran Pan, Shiji Song, Hong Zhang, Gao Huang, and Cheng Wu. Implicit semantic
data augmentation for deep networks. Advances in Neural Information Processing Systems, 32:
12635â€“12644, 2019.

Wenxuan Zhou, Hongtao Lin, Bill Yuchen Lin, Ziqi Wang, Junyi Du, Leonardo Neves, and Xiang Ren.
Nero: A neural rule grounding framework for label-efï¬cient relation extraction. In Proceedings of
The Web Conference 2020, WWWâ€™20, pp. 2166â€“2176, 2020.

Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian
ï¬elds and harmonic functions. In Proceedings of the 20th International conference on Machine
learning (ICML-03), pp. 912â€“919, 2003.

12

A APPENDIX

A.1 REINFORCED LABELING RESULTS ON DIFFERENT DATASETS VARYING END MODELS

Table 4 extends Table 1 by reporting the results of 8 end models and 4 datasets. The hyperparameter
conï¬gurations of Îµ and Îµd are listed in Table 2. For all experiments and end models RL outperforms
Snorkel and maintains closer performance to the fully-supervised machine learning.

Dataset

YouTube
YouTube
YouTube
YouTube
YouTube
YouTube

Red Wine
Red Wine
Red Wine
Red Wine
Red Wine
Red Wine
Red Wine
Red Wine

White Wine
White Wine
White Wine
White Wine
White Wine
White Wine
White Wine
White Wine

Australia Rain
Australia Rain
Australia Rain
Australia Rain
Australia Rain
Australia Rain
Australia Rain
Australia Rain

Reinforced LFs

Snorkel

Supervised learning

Acc Prec Rec

F1 F1-Gain Acc Prec Rec

F1 Acc Prec Rec

F1 End model

0.75
0.72
0.68
0.72
0.67
0.69

0.71
0.68
0.69
0.71
0.71
0.69
0.71
0.70

0.65
0.65
0.63
0.65
0.64
0.63
0.64
0.66

0.58
0.58
0.55
0.59
0.49
0.54
0.59
0.57

0.98
1.00
0.98
1.00
0.93
1.00

0.82
0.85
0.78
0.79
0.82
0.69
0.80
0.83

0.65
0.65
0.64
0.66
0.65
0.64
0.65
0.66

0.28
0.28
0.28
0.29
0.25
0.26
0.29
0.27

0.47
0.40
0.34
0.41
0.33
0.34

0.62
0.53
0.63
0.68
0.61
0.81
0.66
0.60

1.00
1.00
0.97
0.99
0.99
0.98
0.99
1.00

0.77
0.76
0.83
0.76
0.81
0.75
0.78
0.82

0.64
0.57
0.50
0.58
0.49
0.51

0.70
0.64
0.69
0.72
0.70
0.74
0.72
0.69

0.79
0.79
0.77
0.79
0.78
0.78
0.78
0.79

0.41
0.41
0.42
0.42
0.38
0.39
0.42
0.41

+61
+55
+50
+56
+44
+51

+1
+3
+4
+2
+5
0
+7
+1

+23
+22
+26
+23
+8
+34
+27
+18

+31
+32
+30
+32
+29
+27
+34
+33

0.54
0.53
0.53
0.53
0.53
0.53

0.70
0.68
0.60
0.68
0.68
0.66
0.61
0.70

0.51
0.52
0.50
0.50
0.59
0.50
0.50
0.54

0.46
0.49
0.37
0.44
0.53
0.50
0.54
0.48

1.00
1.00
0.00
1.00
0.50
0.00

0.80
0.85
0.65
0.73
0.80
0.63
0.66
0.83

0.67
0.68
0.69
0.67
0.67
0.82
0.69
0.68

0.07
0.07
0.08
0.07
0.07
0.09
0.06
0.06

0.02
0.01
0.00
0.01
0.03
0.00

0.64
0.55
0.78
0.74
0.59
0.89
0.77
0.61

0.48
0.51
0.41
0.49
0.73
0.32
0.41
0.56

0.15
0.13
0.21
0.16
0.12
0.18
0.10
0.12

0.03
0.02
0.00
0.02
0.05
0.00

0.69
0.61
0.65
0.70
0.65
0.74
0.65
0.68

0.56
0.57
0.51
0.56
0.70
0.44
0.51
0.61

0.10
0.09
0.12
0.10
0.09
0.12
0.08
0.08

0.91
0.81
0.92
0.91
0.76
0.86

0.74
0.73
0.65
0.73
0.75
0.71
0.75
0.74

0.72
0.72
0.56
0.67
0.65
0.54
0.71
0.71

0.86
0.86
0.85
0.87
0.82
0.69
0.90
0.87

0.96
1.00
0.92
1.00
1.00
1.00

0.82
0.80
0.70
0.82
0.80
0.77
0.81
0.82

0.73
0.74
0.70
0.86
0.75
0.71
0.81
0.82

0.75
0.73
0.61
0.74
0.54
0.34
0.86
0.66

0.84
0.61
0.90
0.82
0.53
0.70

0.70
0.71
0.65
0.69
0.75
0.67
0.74
0.71

0.91
0.90
0.57
0.62
0.70
0.48
0.73
0.71

0.40
0.40
0.69
0.53
0.55
0.65
0.59
0.55

0.90
0.76
0.91
0.90
0.69
0.82

0.74
0.74
0.67
0.73
0.77
0.71
0.76
0.74

0.81
0.81
0.63
0.69
0.72
0.57
0.76
0.75

0.52
0.52
0.65
0.61
0.55
0.45
0.70
0.60

svm
LogR
DT
logit
knn
RF

svm
LogR
DT
logit
knn
NaivB
RF
mlp

svm
LogR
DT
logit
knn
NaivB
RF
mlp

svm
LogR
DT
logit
knn
NaivB
RF
mlp

Table 4: RL, Snorkel, and (fully-)supervised model results: Accuracy, recall, precision and F1 scores.
F1-Gain shows the F1 score advantage of RL compared to Snorkel.

13

A.2 SYMBOLS AND NOTATIONS IN THE PAPER

Table 5 lists and describes the frequently used symbols throughout the paper. Some listed parameters
are not normalized (e.g., aggregated effects) or adjusted for simplicity, whereas they can be easily
normalized to the range [0, 1], and so Îµ âˆˆ [0, 1] based on the observed values, without causing any
change in the outcomes.

1 , x(i)

Description

2 , . . . , x(i)
n }

Data point composed of n features
Data feature
Dataset
Dataset coverage
Number of data points
Labeling function with index l
Output of LFj on data point x(i). Possible outcomes
are classes 0 or 1, or abstain -1

Symbol
x(i) = {x(i)
fn
X = {x(1), x(2), . . . , x(k)}
Î³ âˆˆ [0, 1]
k = |X|
LFl
LFl(x(i)) âˆˆ {0, 1, âˆ’1}
(cid:104)LFl(cid:105) = {LFl(x(1)), ..., LFl(x(k))} Output of labeling function LFl applied on the whole dataset
Ef f (x(i), LFl(x(i)), x(j))
(cid:80)k
Distance(x(i), x(j))
Îµd
Î± = 1, Î² = 1, Î¾ = 0.35

Effect function of data points x(i) and x(j), and
output of LFl on data point x(i)
Aggregated effect on point x(j) for LFl
Distance (e.g., Euclidean) between two data points
Cut off distance for an effect to be considered
Constants
Thresholds on the aggregated effected to augment a label
as negative or positive
Symmetric aggregated effect threshold
Quartiles. 25th, 50th and 75th percentile respectively.
InterQuartile Range
IQR factor. hIQR = 1.5 is used to calculate the outliers range
Statistics of the LFl also in relations with all the other LFj

bneg, bpos
Îµ = bpos = âˆ’bneg
Q1, Q2, Q3
IQR = Q3 âˆ’ Q1
hIQR
coveragel, overlapsl, conf lictsl

i=1 Ef f (x(i), LFl(x(i)), x(j))

Table 5: Frequently used symbols

14

A.3 REINFORCED LABELING PSEUDOCODE FOR LABEL AUGMENTATION

Algorithm 1: REINFORCED LABELING ALGORITHM
Input: LFs (cid:104)LF1, LF2, . . . LFm(cid:105) and unlabeled data points X = {x(1), x(2), . . . , x(k)}, where

x(i) has features (cid:104)x(i)
1 , x(i)
IQR adjusting parameter Î¾

2 , . . . x(i)

n (cid:105). Gravity parameters Î±, Î². Distance threshold Îµd.

Output: Label for a subset Î³(cid:48) âˆ— |X| of the data points (augmented labels)
Y = {y(1), y(i), . . . , y(k)}, where y(i) âˆˆ [0, 1] âˆª {âˆ’1}

1 Distances â† âˆ…
2 Ef f ects â† âˆ…
3 Labels â† âˆ…
4 for i = 1, 2, . . . , |X| do
5

for l = 1, 2, . . . , m do

6

7

Labels[i][l] â† LFl(x(i))
Ef f ects[i][l] â† 0

end

8
9 end
10 LF s_stats â† CalculateLF sStats(X, Labels)
11 for l = 1, 2, . . . , m do
12

for i = 1, 2, . . . , |X| do

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

if Labels[i][l] == âˆ’1 then
for j = 1, 2, . . . |X| do

if i (cid:54)= j & Labels[j][l] (cid:54)= âˆ’1 then

Distance â† âˆ
if Distances[i][j] is undeï¬ned then

Distance â† F indDistanceBetweenDataP oints(x(i), x(j))
Distances[i][j] â† Distance
Distances[j][i] â† Distance

if Distances[i][j] â‰¤ Îµd then
if Labels[j][l] = 1 then

Ef f ects[i][l] â† Ef f ects[i][l] +

Î²
(Distances[i][j])Î±

else

Ef f ects[i][l] â† Ef f ects[i][l] âˆ’

Î²
(Distances[i][j])Î±

end

end

end

28
29 end
30 boundary_min, boundary_max â† CalculateIQRBoundaries(LF _stats, Ef f ects, Î¾)
31 for l = 1, 2, . . . , m do
32

for i = 1, 2, . . . , |X| do

33

34

35

36

37

if Labels[i][l] == âˆ’1 then

if Ef f ects[i][l] < boundary_min then

Labels[i][l] â† 0

if Ef f ects[i][l] > boundary_max then

Labels[i][l] â† 1

end

38
39 end
40 return Labels

Alg. 1 shows the pseudocode of a similarity-based heuristic algorithm for reinforced labeling. Given
m LFs and the unlabeled dataset X of size |X|, the algorithm outputs the augmented labeling
matrix. The listed gravity parameters Î± := 1, Î² := 1 are constants used for all datasets in the
experimental study. The distance threshold Îµd is an optional parameter to optimize the computation
or to remove outliers, whereas it is not always used in the experimental study (Îµd is set to a high
number). Labels, Ef f ects are 2D arrays, where rows represent the index i of the data points (x(i))

15

Algorithm 2: CalculateIQRBoundaries(LF _stats, Ef f ects, Î¾)
Input: LF s_stats = (cid:104)LF s_coverage, LF s_overlaps, LF s_conf licts(cid:105) where

LF s_coverage = (cid:104)LF1_coverage, LF2_coverage, . . . , LFm_coverage(cid:105) and
similarly LF s_overlaps and LF s_overlaps; Ef f ects: Array of aggregated effects,
Î¾ â† 0.35

Output: boundary_min, boundary_max

1 sumcoverage â† 0
2 sumoverlaps â† 0
3 sumconf licts â† 0
4 for l = 1, 2, . . . , m do
5

6

sumcoverage â† sumcoverage + LFl_coverage
sumoverlaps â† sumoverlaps + LFl_overlaps
sumconf licts â† sumconf licts + LFl_conf licts

7
8 end
9 hiqr â† Î¾ âˆ— sumcoverage âˆ— sumoverlaps âˆ— sumconf licts
10 q1 â† calculateP ercentile(Ef f ects, 25)
11 q3 â† calculateP ercentile(Ef f ects, 75)
12 iqr â† q3 âˆ’ q1
13 boundary_min â† q1 âˆ’ iqr âˆ— hiqr
14 boundary_max â† q3 + iqr âˆ— hiqr
15 return boundary_min, boundary_max

and columns represent the index l of the LFs (LFl). Distances is a 2D array representing the
distance between any two data points. For instance, Distances[i][j] represents the distance between
x(i) and x(j).

The âˆ’1 values that represent abstains of LFs are updated based on their similarity given by the
F indDistanceBetweeDataP oints function. This function can be implemented using various
distance metrics such as Euclidean, Cosine, or Mahalanobis distances. Lastly, the updated Labels
array represents the augmented labeling matrix that can be given to a generative process such as
Snorkelâ€™s generative model.

Alg. 2 shows the pseudocode of the CalculateIQRBoundaries used for the RL implementation.
The heuristic in Alg. 2 leverages three LF statistics, that are LF coverage, overlaps, and conï¬‚icts, to
calculate the boundaries that are effectively the aggregated effect threshold of RL.

16

A.4 ADDITIONAL EXPERIMENTAL INSIGHTS

In addition to the experimental study, we implement and test a hybrid approach of leveraging labels of
LFs as well as strong supervision through a gold dataset. We call this approach the generative neural
network for data programming (GNN). In GNN, the label outputs of LFs and data features are fed to a
simple neural network (NN) along with the labels. The NN model contains two hidden layers (# nodes
12, 8) with ReLU activation function, and it uses Adam optimizer. The output layer has the sigmoid
activation function. The NN model is used in different stages of the pipeline. First, GNN replaces the
generative part using labels of Snorkel (Sn+GNN+<endmodel>) or RL (RL+GNN+<endmodel>).
Then, the outputs of GNN are fed to an end classiï¬er as usual. Second, the GNN itself serves as the
end classiï¬er as well as the generative model (Sn+GNN or RL+GNN). We applied the GNN model
to both red wine and white wine datasets as these datasets have the available ground truth data to
create gold datasets.

Figure 6: Experimental results of the red wine and white wine datasets for different approaches: Sn,
RL, Sup., Sn+GNN, RL+GNN, Sn+GNN+<end_model> and RL+GNN+<end_model>.

Fig. 6 shows results of the red and white wine datasets in more detail, including the hybrid GNN
approach using RF and NaivB end models, respectively. The results of the following 7 approaches
are listed in order: Sn+<endmodel>, RL+<endmodel>, Sup+<endmodel>, Sn+GNN+<endmodel>,
RL+GNN+<endmodel, Sn+GNN, RL+GNN>. We observe that RL+RF outperforms the Sn+RF
benchmark for the white wine dataset by +13 points accuracy and +34 points F1 and even provides
better results than Sup (RF). For the red wine dataset, RL+NaivB outperforms Sn+NaivB by +10
points in accuracy and +7 points in F1 score. Moreover, although approaches such as Sup or GNN
leverage ground truth labels in their training, outcomes of RL are competitive for the red wine dataset,
whereas RL outperforms Sup (NaivB), Sn+GNN+NaivB, and RL+GNN+NaivB for the white wine
dataset (see Fig. 6-right).

Figure 7: Experimental results for the YouTube comments and Australia rain datasets testing different
approaches: Snorkel (Sn), Reinforced Labeling (RL), Supervised learning (Sup.).

Fig. 7 shows the bar graph for the results of the YouTube comments and Australia Rain datasets (also
listed in Table 1) in terms of the four metrics: Accuracy, precision, recall, and F1. The results of the
following approaches are listed in order: Sn+<endmodel>, RL+<endmodel>, Sup+<endmodel>.
We observe that RL+svm outperforms the Sn+svm benchmark for the YouTube dataset by +21 points

17

accuracyprecisionrecallf10.00.20.40.60.81.0Red WineApproachSn+RFRL+RFSup+RFSn+GNN+RFRL+GNN+RFSn+GNNRL+GNNaccuracyprecisionrecallf10.00.20.40.60.81.0White WineApproachSn+NaivBRL+NaivBSup+NaivBSn+GNN+NaivBRL+GNN+NaivBSn+GNNRL+GNNaccuracyprecisionrecallf10.00.20.40.60.81.0You TubeApproachSn+svmRL+svmSup+svmaccuracyprecisionrecallf10.00.20.40.60.81.0Australia RainApproachSn+RFRL+RFSup+RFaccuracy and +61 points F1. For the Australia Rain dataset, RL+RF outperforms Sn+RF by +5 points
in accuracy and +34 points in F1 score.

18

A.5 LABELING FUNCTIONS

We use the Snorkel library to implement LFs and encode domain knowledge programmatically. The
LFs are implemented using the interactive and user-friendly features of the Snorkel framework, such
as providing LF statistics and allowing high-level deï¬nitions of LFs.

As in almost all the previous data programming studies, we do not follow a certain scheme or strict
guidance on implementing LFs but rather rely on the best effort based on some understanding of
the datasets by visualizing the features and interactively checking the LF coverages, overlaps, and
conï¬‚icts. In general, the assumption is that the developers would make the best effort to write LFs.

The Australia Weather dataset is used to build a model that predicts for any given day if it will rain
the day after4. LFs in Listing 1 use weather data features to label data points as GOOD (1) or BAD
(0) based on the features such as Humidity, Rain Today, Temperature at 9am, Humidity at 9am, and
pressure at 3pm.

1 from snorkel.labeling import labeling_function
2 ABSTAIN = -1
3 NO_RAIN = 0
4 RAIN = 1
5
6 @labeling_function()
7 def check_humidity3pm(x):
8

if x.Humidity3pm is None:

9

10

11

12

13

14

15

return ABSTAIN

elif x.Humidity3pm>0.75:

return RAIN

elif x.Humidity3pm<0.15:

return NO_RAIN

else:

return ABSTAIN

16
17 @labeling_function()
18 def check_rain_today(x):
19

if x.RainToday is None:
return ABSTAIN

elif x.RainToday==1:
return RAIN

else:

return ABSTAIN

25
26 @labeling_function()
27 def check_temp9am(x):
28

if x.Temp9am is None:
return ABSTAIN

elif x.Temp9am>0.60:
return RAIN

else:

return ABSTAIN

34
35 @labeling_function()
36 def check_rainfall(x):
37

if x.Rainfall is None:
return ABSTAIN
elif x.Rainfall>0.60:

return RAIN

else:

return ABSTAIN

20

21

22

23

24

29

30

31

32

33

38

39

40

41

42

43
44 @labeling_function()
45 def check_humidity9am(x):
46

if x.Humidity9am is None:

47

return ABSTAIN

4https://www.kaggle.com/jsphyg/weather-dataset-rattle-package

19

48

49

50

51

52

53

elif x.Humidity9am>0.90:

return NO_RAIN

elif x.Humidity9am<0.20:

return RAIN

else:

return ABSTAIN

54
55 @labeling_function()
56 def check_pressure3pm(x):
57

if x.Pressure3pm is None:

58

59

60

61

62

63

64

return ABSTAIN

elif x.Pressure3pm<0.05:

return RAIN

elif x.Pressure3pm>0.70:

return NO_RAIN

else:

return ABSTAIN

Listing 1: Australia rain labeling functions

The wine datasets have various data features such as alcohol, sulfates, citric acid levels, etc. Listing 2
and 3 include the LFs implemented for wine quality classiï¬cation for the red wine and white wine
datasets, respectively. LFs labels data points as GOOD (1) or BAD (0) quality wine.

1 from snorkel.labeling import labeling_function
2 ABSTAIN = -1
3 BAD = 0
4 GOOD = 1
5
6 @labeling_function()
7 def check_alcohol(x):
8

if x.alcohol is None:
return ABSTAIN

9

10

11

12

13

14

15

20

21

22

23

24

elif x.alcohol>0.75:
return GOOD
elif x.alcohol<0.15:

return BAD

else:

return ABSTAIN

16
17 @labeling_function()
18 def check_sulphate(x):
19

if x.sulphates is None:
return ABSTAIN
elif x.sulphates>0.3:

return GOOD

else:

return ABSTAIN

25
26 @labeling_function()
27 def check_citric(x):
28

if x.acidity_citric is None:

29

30

31

32

33

return ABSTAIN

elif x.acidity_citric>0.7:

return GOOD

else:

return ABSTAIN

Listing 2: Red wine labeling functions

1 from snorkel.labeling import labeling_function
2 ABSTAIN = -1
3 BAD = 0
4 GOOD = 1
5

20

6 @labeling_function()
7 def check_alcohol(x):
8

if x.alcohol is None:
return ABSTAIN

elif x.alcohol>0.75:
return GOOD
elif x.alcohol<0.15:

return BAD

else:

return ABSTAIN

16
17 @labeling_function()
18 def check_sulphate(x):
19

if x.sulphates is None:
return ABSTAIN
elif x.sulphates>0.3:

return GOOD

else:

return ABSTAIN

9

10

11

12

13

14

15

20

21

22

23

24

25
26 @labeling_function()
27 def check_citric(x):
28

if x.acidity_citric is None:

29

30

31

32

33

return ABSTAIN

elif x.acidity_citric>0.7:

return GOOD

else:

return ABSTAIN

Listing 3: White wine labeling functions

For the YouTube dataset, we implemented two LFs that search for the exact string "check out" or
"check" in the text. Other than these two additional LFs, the LFs in the Snorkel tutorial [Snorkel
(2021)] named â€œtextblob_subjectivityâ€, â€œkeyword_subscribeâ€, â€œhas_person_nlpâ€ are used in the
experiments of Table 1. The rest of LFs available from the Snorkel tutorial are used for the experiments
in Fig. 5.

1 from snorkel.labeling import labeling_function
2 ABSTAIN = -1
3 HAM = 0
4 SPAM = 1
5
6 @labeling_function()
7 def check(x):
8

return SPAM if "check" in x.text.lower() else ABSTAIN

9
10 @labeling_function()
11 def check_out(x):
12

return SPAM if "check out" in x.text.lower() else ABSTAIN

Listing 4: YouTube labeling functions

21

