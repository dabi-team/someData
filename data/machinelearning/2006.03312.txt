0
2
0
2

n
u
J

5

]

G
L
.
s
c
[

1
v
2
1
3
3
0
.
6
0
0
2
:
v
i
X
r
a

PLANS: Robust Program Learning from Neurally
Inferred Speciﬁcations

Raphaël Dang-Nhu
ETH Zürich
dangnhur@ethz.ch

Abstract

Recent years have seen the rise of statistical program learning based on neural mod-
els as an alternative to traditional rule-based systems for programming by example.
Rule-based approaches offer correctness guarantees in an unsupervised way as they
inherently capture logical rules, while neural models are more realistically scalable
to raw, high-dimensional input, and provide resistance to noisy I/O speciﬁcations.
We introduce PLANS (Program LeArning from Neurally inferred Speciﬁcations),
a hybrid model for program synthesis from visual observations that gets the best of
both worlds, relying on (i) a neural architecture trained to extract abstract, high-
level information from each raw individual input (ii) a rule-based system using the
extracted information as I/O speciﬁcations to synthesize a program capturing the
different observations. In order to address the key challenge of making PLANS
resistant to noise in the network’s output, we introduce a ﬁltering heuristic for I/O
speciﬁcations based on selective classiﬁcation techniques. We obtain state-of-the-
art performance at program synthesis from diverse demonstration videos in the
Karel and ViZDoom environments, while requiring no ground-truth program for
training. We make our implementation available at github.com/rdang-nhu/PLANS.

1

Introduction

The problem of automatically generating a program satisfying given speciﬁcations is a long-standing
challenge of artiﬁcial intelligence (Waldinger, Lee, 1969). The sub-area of inductive program
synthesis, also known as programming by example (Gulwani, 2011), focuses on speciﬁcations formed
of examples of desired I/O program behavior. In existing systems, I/O examples typically are instances
of simple programming data types such as booleans, integers, ﬂoats or strings, or elementary data
structures such as lists. Extending programming by example to more complex input domains is
a very useful task, as it opens the range of possible real-world applications. Sun et al. (2018)
made an important step in this direction by introducing the task of program synthesis from diverse
demonstrations videos. This is a supervised learning problem in which the input is a set of videos of
an agent’s behavior provided as raw visual input, and the desired output is a program summarizing
the decision making logic (or policy) of this agent.

Inductive synthesis techniques can be divided in two categories. The traditional symbolic or rule-
based approaches (Jha et al., 2010; Alur et al., 2013) typically rely on efﬁcient search through
a user-deﬁned domain speciﬁc language, leveraging constraint solvers based on SAT and SMT
solving for search-space pruning. Alternatively, recent efforts aim at leveraging statistical learning
methods, by phrasing inductive synthesis as a differentiable problem (Gaunt et al., 2016) or a
supervised learning task (Parisotto et al., 2017). Rule-based approaches have several advantages:
they offer guarantees that the generated program is correct (i.e., satisﬁes the provided speciﬁcations).
Additionally, they demonstrate better generalization to unseen inputs. The latter was experimentally
shown by Gaunt et al. (2016) in the Terpret framework, and conﬁrmed by Devlin et al. (2017) on the

Preprint

 
 
 
 
 
 
FlashFill benchmark (Gulwani et al., 2012). However, they are computationally expensive and suffer
from a serious scalability issue (Balog et al., 2017). As a result, they are inherently unable to handle
raw visual input. Besides, they struggle to deal with incorrect input as they systematically attempt
to satisfy all examples. Statistical approaches (see Allamanis et al. (2018) for a survey) appear as a
way to mitigate these problems, by handling images (Ellis et al., 2018), scenes (Liu et al., 2019) or
videos (Sun et al., 2018) as inputs, and learning to be robust against noise (Devlin et al., 2017). But
this comes at the cost of generating large datasets labeled with ground-truth programs for training.

We introduce PLANS, a hybrid model for program synthesis from demonstration videos that combines
neural and rule-based techniques in order to get the best of both worlds. It has two components:
First, a neural architecture is trained to infer a high-level description of each individual behavior from
raw visual observation. Second, a rule-based solver uses the information inferred by the network as
I/O speciﬁcations to synthesize a program capturing the different behaviors. The neural component
allows to handle raw visual input by transforming it into abstract speciﬁcations, while the rule-based
system removes the need for ground-truth programs by inherently capturing logical rules. Therefore,
our model uses strictly less supervision than prior work. We address the key challenge of making
PLANS robust to the noise introduced by the imperfect inference of speciﬁcations, which has a
dramatic impact on the solver’s performance if not properly handled. Speciﬁcally, we introduce
an adaptive ﬁltering heuristic for I/O speciﬁcations based on selective classiﬁcation techniques, in
which only high-conﬁdence predictions are passed to the solver. Using this heuristic, PLANS clearly
outperforms previous methods (Sun et al., 2018; Duan et al., 2019). Finally, our model maintains a
reasonable temporal complexity, as it relies on several independent solver calls that can be performed
in parallel. To summarize, we make the following contributions:

• We develop a neural architecture for inferring I/O speciﬁcations from videos and an encoding
of the synthesis task into an off-the-shelf rule-based solver (Rosette (Torlak, Bodik, 2013)).

• We address the key problem of making the rule-based solver robust to noise in the network’s

output by developing an adaptive ﬁltering heuristic for speciﬁcations.

• We evaluate PLANS on the Karel and ViZDoom benchmarks and show signiﬁcant perfor-
mance improvements compared to state-of-the-art end-to-end neural architectures, despite
using strictly less supervision signals.

2 Background

2.1 Motivation

Understanding an agent’s decision making logic is a long standing task in artiﬁcial intelligence,
originally motivated by the performance beneﬁts of imitation learning techniques (Schaal, 1997; Ng,
Russell, 2000; Ross et al., 2011). In addition, it is related to making algorithms more interpretable to
humans. In particular, recent work has focused on interpreting the neural policy representation in
modern deep reinforcement learning algorithms (Lipton, 2018; Montavon et al., 2018). In this context,
program representation of policies has several advantages. First, it gives a concise representation that
is suitable for intuitive human understanding. Second, it permits to leverage symbolic veriﬁcation
techniques to ensure safety guarantees in the learning process (Verma et al., 2018; Zhu et al.,
2019). The core aspect of these programmatic approaches is the ability to synthesize a summarized
representation of the different observed behaviors of the agent.

The setting considered here is speciﬁc in two ways. First, most existing techniques assume that a
high-level description of actions performed by the agent is always available. Second, the control-ﬂow
interpretability of the synthesized program is limited by the complexity of the underlying state space
representation. This can lead to degenerate situations where the execution of the program depends
on non-interpretable boolean conditions. For instance, in the case of visual input, this might be a
threshold on the value of a single pixel in a 80 × 80 image. On the contrary, we assume that ground
truth action sequences are not available at test time, and that the high dimension of the state-space
requires restricting possible boolean conditions to high-level description of the state. In the context
of playing video games, an example of meaningful condition is a boolean that indicates whether
an enemy is present in the environment. These assumptions create a black-box setting where the
speciﬁcations for synthesis of a program policy are not immediately available and need to be inferred.

2

2.2 Task description

Here, we describe the task of program synthesis from diverse demonstration videos (Sun et al., 2018).

Agent-environment interaction We consider an environment with observable state s ∈ S, and an
agent interacting with this environment in discrete timesteps. We assume that the agent has a ﬁxed
set of possible actions A, and that the inﬂuence of each action on the state of the environment is
determined by a deterministic transition function T : S × A → S. Additionally, we suppose that
the agent behaves according to an unknown, deterministic policy η(s) ∈ A that determines the next
action depending on the sequence of previously observed states s = (s1, . . . , si). Finally, we assume
that the agent has control over the end of the interaction by means of a speciﬁc action end ∈ A.

Demonstrations A demonstration τ is formed of a sequence of states s = (s1, . . . , sT ) together
with a sequence of actions of same length a = (a1, . . . , aT ), satisfying the following conditions
(i) The actions of the agent follow the policy η. That is, for all i in {1 . . . T }, we have ai =
η((s1, . . . , si)). (ii) The environment transitions are determined by T , i.e. for all i in {1 . . . T − 1},
we have si+1 = T (si, ai). (iii) The sequence is complete, i.e. the last action aT is end. Since the
transition function and policy are deterministic, the demonstration is uniquely determined by the
environment’s initial state s1. Therefore, we can generate demonstrations for a policy by sampling
random initial states and applying the policy and transition iteratively. In the case of visual state
observations, the state sequence s can be seen as video composed of successive frames.

In order to address the interpretability issue arising from the high dimensionality of S,
Perceptions
we assume the existence of d perception primitives γ1, . . . , γd : S → {true, false}. Intuitively, the
perceptions (γ1(s), . . . , γd(s)) give a high-level, abstract representation of the state. For instance, in
the context of video games, γ1(s) could be true if and only if s the agent is facing an enemy. The
underlying idea is that the agent’s behavior η should only depend on the compact state representation
provided by the perception primitives. Given a demonstration τ , we deﬁne d associated perception
sequences p1, . . . , pd as pj = (pj

T ) = (γj(s1), . . . , γj(sT )).

1, . . . , pj

Program synthesis from demonstration videos Sun et al. (2018) proposed the following super-
vised learning task, where the input is a set of demonstrations generated with an unknown policy η
and different initial states, and the desired output is a representation of η. The benchmark datasets are
generated such that η can always be represented by a program in a given domain speciﬁc language
(DSL). The execution of the program speciﬁes which sequence of actions is executed, with different
control-ﬂow constructs, such as conditional branchings and loops. The boolean conditions used for
these constructs are only allowed to depend on the high-level representation γ(s) = (γ1(s), . . . , γl(s))
of the state obtained via the perception primitives. The DSL is formally deﬁned in Figure 1.

Supervision signals Sun et al. (2018) make the assumption that neither the action nor the perception
sequences are available at test time. They can only be used as a supervision signal in the training
phase. PLANS conforms to this assumption, explaining the need for a neural model that learns to
infer the I/O speciﬁcations. Compared to previous work, the main speciﬁcity of PLANS is that it does
not require program supervision. That is, the ground-truth program is never accessible to the model.

2.3 Summary of previous approaches

demo2program Sun et al. (2018) designed an end-to-end neural model for the task of program
synthesis from demonstration videos. It is based on the following components (i) a convolutional
neural network to encode each of the video frames (ii) an encoder LSTM layer that takes as input
each video as a sequence of frame encodings (iii) a summarizer LSTM layer that re-encodes the video
with aggregated video encodings as initial state (iv) a relational network module that summarizes the
different video encodings (v) a ﬁnal decoder LSTM layer that outputs the program in the form of a
sequence of code tokens. They demonstrate that the summarizer LSTM layer as well as the relational
module are essential to obtain reliable identiﬁcation of the correct program control-ﬂow.

watch-reason-code Duan et al. (2019) proposed to reduce the memory footprint of the architecture
by introducing a deviation-pooling strategy replacing the relational module, and to use multiple
decoding layers to reﬁne the generated program, obtaining slight performance improvements.

3

3 The PLANS model

Figure 2 gives a high-level overview of PLANS. We describe the neural component (Section 3.1), the
rule-based solver (Section 3.2) and our adaptive ﬁltering heuristic (Section 3.3).

3.1 Neural inference of speciﬁcations

The ﬁrst component of PLANS is a neural architecture that learns to infer the action and perception
sequences from raw visual input. This formalizes as a sequence-to-sequence learning task where the
input is a video demonstration τ = (s1, . . . , sT ), and the desired outputs are (i) the ground-truth action
sequence (a1, . . . , aT ) (ii) the ground-truth perception sequences (p1
T ). To
perform this task, we used a vanilla seq2seq model enhanced with an attention mechanism. In order
to interpret visual input, we use a multi-layer convolutional network as a state embedding, i.e the
state si is encoded by a convolutional network to a state vector vi before it is fed to the encoder layer.
We use two different decoder layers for actions and perceptions, while the embedding and encoder
layer are common for both action and perceptions. To ensure reproducibility of our results, extensive
description of hyperparameters and training process can be found in the supplementary material.

T ) . . . (pl

1, . . . , p1

1, . . . , pl

3.2 Encoding into an inductive synthesis problem

Our rule-based synthesizer is implemented in
Rosette (Torlak, Bodik, 2013), which itself
builds on the Z3 solver (De Moura, Bjørner,
2008). Rosette allows to specify a program
with holes (or sketch), and a desired behav-
ior. Then, it automatically ﬁlls the holes by
encoding the resulting problem into a set of
logical constraints that have to be satisﬁed.
A very convenient functionality of Rosette is
the possibility to specify complex holes that
can be ﬁlled with expressions from a prede-
ﬁned grammar, allowing to encode the DSL
of Figure 1, and restricting the search to valid
programs. In the supplementary material, we
show how we encoded the different control-
ﬂow constructs of the DSL in Rosette.

Program m → s ; end
Statement s → s1 ; s2

| while(b) : s
|
|
|
|

repeat(r) : s
if(b) : s else : s2
if(b) : s
Action a

r ∈ N

a ∈ A

Condition b → not b

|

Perception γi

i ∈ {1 . . . l}

Figure 1: DSL for representing policies.

The task of program synthesis from diverse demonstration videos naturally phrases as a programming
by example problem. The I/O speciﬁcations contain the perception sequences, which serve as
program input, and the corresponding action sequence, which is the desired output. In order to
achieve good generalization of the synthesized program to unseen demonstrations, it is crucial to
privilege simpler solutions among several programs satisfying the I/O speciﬁcations. That is, we aim
at ﬁnding the solution with the least cost, deﬁned as the number of control-ﬂow branchings (if or
while statements). Since Rosette has no native support for cost functions, we propose to bound the
number of control-ﬂow statements in the Rosette encoding of the DSL, and make repeated calls to
the solver while increasing this bound, until the resulting set of constraints is satisﬁable. We give
additional details about this procedure in the supplementary material.

Rule-based synthesizers have the advantage that they are both sound and complete: any program
returned by the solver is guaranteed to satisfy all the provided speciﬁcations, and if there exists a
satisfying way to ﬁll the holes, the solver will ﬁnd a solution in ﬁnite time (in the case of bounded
number of control-ﬂow statements). These guarantees are usually associated with a high compu-
tational cost. In the case at hand, thanks to the relative simplicity of the DSL which contains no
variable assignments, we did not observe time to be a problem. All solver queries terminate in a
few seconds, which in orders of magnitude is similar to the time needed by the neural architecture to
infer the different speciﬁcations. Besides, the different calls to the solver are independent and can be
made in parallel. We give precise time measurements in the supplementary material.

4

Erroneous prediction

Filtering unreliable predictions

Demo 1

Demo
encoder

Action decoder

(move, move, move, end)

High prediction
conﬁdence

Perception decoder

(True, True, True, False)

Demo 2

Demo
encoder

Action decoder

(move, move, end)

Perception decoder

(False, True, False)

Demo k

Demo
encoder

Action decoder

(move, end)

Perception decoder

(True, False)

Low prediction
conﬁdence

High prediction
conﬁdence

Rule-based
program
synthesizer

while(frontIsClear())
| move
end

Figure 2: Overview of PLANS. The video demonstrations are individually fed at pixel-level into the
neural encoder-decoder architecture, which infers the high-level action and perception sequences.
For the sake of clarity, we only show one perception sequence corresponding to the frontIsClear()
perception primitive. The different sequences are then given to a rule-based solver, that generates a
program summarizing the different demonstrations. The demonstration with inconsistent perception
sequence is identiﬁed thanks to its low prediction conﬁdence, and is not provided to the solver.

3.3 Synthesis from noisy speciﬁcations

The ability to generate correct perception-action speciﬁcations for the synthesis task is crucial to
the performance of PLANS. Indeed, existing rule-based solvers are very sensitive to even small
amounts of input noise (Devlin et al., 2017). Consequently, our model can only achieve satisfying
performance if all the speciﬁcations given to the solver are reliable. While this assumption is met
on the Karel benchmark, we observe in the ViZDoom environment that the quasi totality (above
99%) of test programs have at least one erroneous predicted action or perception token among the
25 observed demonstrations. Existing work on rule-based synthesis from noisy data (Raychev et al.,
2016) propose dataset sub-sampling as a way to avoid overﬁtting to incorrect speciﬁcations. This is
not directly applicable to our setting. Indeed, it only guarantees to ﬁnd a program that is close to the
correct solution, in the sense that it satisﬁes as many I/O speciﬁcations as possible, with a focus on
datasets containing a large number of examples. In contrast, we deal with a ﬁxed and limited amount
of demonstrations, and aim at identifying exactly the original program.

Instead, we designed a simple yet effective ﬁltering heuristic for demonstrations, based on the
principle of selective classiﬁcation. This heuristic makes use of the neural network’s prediction
conﬁdence. For a given sequence token, the prediction conﬁdence is deﬁned as the probability of
the predicted class in the softmax output layer. The ﬁltering works in two steps: First, we assign to
each demonstration τ an action conﬁdence level actionconf(τ ) ∈ [0, 1] and a perception conﬁdence
level perconf(τ ) ∈ [0, 1] to characterize the global conﬁdence of the whole sequence of predictions.
Then, if either one of the two conﬁdence levels is too low, we discard the whole demonstration. We
explore two variants of this method: a static ﬁltering heuristic that uses ﬁxed thresholds for the action
and prediction conﬁdence, and a dynamic heuristic that incrementally increases the thresholds while
making repeated calls to the solver, until a valid solution is found. Before we describe these heuristics
in detail, let us formally deﬁne the action and prediction conﬁdence levels.

Deﬁnition. Let τ be a demonstration, for which the neural architecture predicts action sequence
a = (a1, . . . , aT ) and perception sequences p1 = (p1
T ). Let us
denote conf(ai) (resp conf(pj
i )) for the prediction conﬁdence of action token ai (resp perception
token pj
i ) in the softmax output layer. We deﬁne the
action conﬁdence actionconf(τ ) of the demonstration as the minimum of the action tokens conﬁdence.
That is,

i ), which is deﬁned as the probability of ai (resp. pj

T ), . . . , pl = (pl

1, . . . , p1

1, . . . , pl

actionconf(τ ) = min

i

conf(ai).

Similarly, we deﬁne the perception conﬁdence perconf(a) of τ to be the minimum of the perception
tokens conﬁdence, formally

perconf(τ ) = min
i,j

conf(pj

i ).

Note that for the perception conﬁdence level, the min is taken both across timesteps (indexed by i)
and perception primitives (indexed by j).

5

Static ﬁltering In the static ﬁltering strategy, we ﬁx two thresholds (cid:15)a and (cid:15)p for the action and
perception conﬁdence respectively. Then, we ﬁlter all the I/O examples for which the underlying
demonstration τ veriﬁes actionconf(τ ) < (cid:15)a or perconf(τ ) < (cid:15)p. (cid:15)a are (cid:15)p are treated as hyper-
parameters and optimized on the validation dataset. Tuning the thresholds creates a trade-off between
the number of remaining demonstrations and their reliability.

Dynamic ﬁltering In practice, we observe the static ﬁltering strategy to be efﬁcient at ﬁltering out
incorrect action sequences. However, it is sub-optimal for ﬁltering perception sequences, because the
best threshold (cid:15)p varies signiﬁcantly across programs. To mitigate this problem, we employ a dynamic
ﬁltering strategy for perception sequences. We sort the demonstrations by increasing perception
conﬁdence, i.e. we have τ1, . . . , τk with perconf(τ1) < . . . . perconf(τk). Then, we incrementally
ﬁlter out the demonstrations by increasing order of conﬁdence, while making repeated calls to the
program synthesizer, until the resulting set of constraints is satisﬁable. This progressively reduces the
number of I/O examples, with the hope that incorrect predictions will have low conﬁdence and will
be ﬁltered ﬁrst. This dynamic ﬁltering strategy is adaptive in the sense that it removes the need for
tuning the threshold (cid:15)p. It is coupled with the search for optimal program cost in the following way:
in the outer loop, we increment the perception threshold (cid:15)p, and in the inner loop, we increment the
number of control-ﬂow statements. In the supplementary material, we provide a detailed description
of this algorithm.

4 Experimental results

In this section, we report experimental observations concerning the performance of PLANS.

Benchmarks Karel (Pattis, 1981) is an educational programming language that controls a robot
navigating through a grid world with walls and markers. ViZDoom (Kempka et al., 2016) is an
open-source platform for Doom, a classical ﬁrst-person shooter game. It allows training bots via
reinforcement learning from visual observations. We use the three evaluation metrics designed
by Sun et al. (2018). To measure execution accuracy, we compare if the predicted and ground-truth
programs behave similarly on a ﬁxed number of not previously observed initial states. Sequence
accuracy measures if the predicted and ground-truth programs match exactly. Program accuracy
is similar to sequence accuracy, with identiﬁcation of some semantically equivalent programs: e.g.,
repeat(3) : move and move : move : move will be considered as equivalent by program accuracy.

Experimental setup We performed all experiments on a machine with 2.00GHz Intel Xeon E5-
2650 CPUs and using a single GeForce RTX 2080 Ti GPU. We make our implementation public and
provide additional details about the experiments duration in the supplementary material.

4.1 Comparison with demo2program and watch-reason-code

Results on the main Karel and ViZDoom benchmarks are presented in Table 1. Values for both
baselines are directly reported from prior work, and show best obtained performance. For PLANS,
we report mean and standard deviation over three independent runs. PLANS strongly improves
execution accuracy compared to prior work: approximately +15% absolute improvement for Karel,
and +10% for ViZDoom. This means that PLANS is signiﬁcantly better at capturing the different
possible behaviors. We also observe improvements of program accuracy, though less signiﬁcant. We
surmise that this is due to imperfections of the program accuracy metric, which captures some but not
all semantically equivalent programs. For instance, we observe that if(frontIsClear()) : move else :
turn and if(not frontIsClear()) : turn else : move are not recognized as equivalent. An important
direction for future work is to improve this metric in the original benchmark released by Sun et al.
(2018). Finally, we observe no sequence accuracy improvement in the Karel benchmark. However,
we believe that the obtained performance is still very reasonable. Indeed, as opposed to the two
baselines, PLANS does not access any ground-truth program during training. Therefore, it can not be
expected to distinguish between semantically equivalent programs.

In the ViZDoom environement, our results conﬁrm that the performance of PLANS heavily relies
on the ﬁltering heuristics. Without ﬁltering, the model achieves very poor performance. With static
ﬁltering only, the model achieves reasonable accuracy but fails to outperform both baselines. Dynamic
ﬁltering yields the best results.

6

Table 1: Accuracy comparison on the main Karel and ViZDoom benchmarks. Values are in %.

Execution
72.1
74.7

Karel
Program
48.9
51.2

91.6 ± 1.3 53.9 ± 1.0

Sequence
41.0
43.3
34.2 ± 0.5

%
Model
demo2program
watch-reason-code
PLANS (none)
PLANS (static)
PLANS (dynamic)

ViZDoom
Sequence
Program
Execution
53.2
62.5
78.4
55.8
63.4
68.1
19.3 ± 0.6
21.7 ± 0.6
25.8 ± 0.9
51.2 ± 0.8
57.9 ± 0.9
77.5 ± 1.5
88.0 ± 0.6 65.5 ± 0.6 58.8 ± 0.6

Table 2: Comparison on the ViZDoom if-else benchmark.

%
Execution
Model
89.4
demo2program
82.1
watch-reason-code
16.3 ± 1.2
PLANS (none)
77.0 ± 0.6
PLANS (static)
PLANS (dynamic) 91.4 ± 0.7

ViZDoom if-else
Program
69.1
67.8
13.8 ± 1.0
62.4 ± 0.7
74.6 ± 0.7

Sequence
58.8
57.7
10.2 ± 0.6
50.2 ± 0.7
62.1 ± 0.7

Figure 3: Inﬂuence of number of ob-
served demonstrations on exec. accu-
racy (ViZDoom, main benchmark).

4.2 Additional experiments

Number of observed demonstrations
In Figure 3, we show the inﬂuence of the number of ob-
served demonstrations on ViZDoom execution accuracy. We observe consistent improvement over
demo2program when the number of demonstrations increases. We do not report values for watch-
reason-code as it is not given by prior work and no implementation is provided. Watch-reason-code
is anyway signiﬁcantly less accurate than demo2program for this metric. This experiment conﬁrms
the claim that our model reliably yields superior performance on the ViZDoom benchmark, and
generalizes better to unseen initial states.

If-else dataset
In order to speciﬁcally assess the ability of PLANS to identify control-ﬂow diver-
gences among demonstrations, we consider a speciﬁc dataset with programs composed of only one
if-else branching statement. Results are shown in Table 2. Contrary to the main experiment, we
observe similar improvements on all three metrics. We attribute this observation to the fact that
programs in this dataset have less semantically equivalent variations, because of their simple form.

5 Related work

Learning graphics programs from images and scenes A related line of work is focused on
inference of graphical programs from visual input (Ellis et al., 2015; Ganin et al., 2018; Liu et al.,
2019; Tian et al., 2019). Most notably, Ellis et al. (2018) also use a hybrid architecture composed
of a neural network and a rule-based solver. However, it can not be considered a programming
by example task, as the speciﬁcation only relates to a single input image, for which a compact,
synthetic representation is desired. In contrast, we address the challenging task of summarizing the
decision making in several videos, which requires to identify the control-ﬂow divergences between the
different observed behaviors. Besides, they ensure robustness against noise by incrementally adding
speciﬁcations while rendering the resulting image and measuring its similarity with the input. This
approach to identifying wrong speciﬁcations is task-speciﬁc as it requires the ability to incrementally
render the generated speciﬁcations into an image, and does not apply to our setting.

7

1510152025303540Observed demonstrations2030405060708090Execution accuracy (%)PLANSdemo2programSynthesis of programs from noisy examples Our work conﬁrms the idea that naive rule-based
solvers are very vulnerable to noise in speciﬁcations (RobustFill, (Devlin et al., 2017)). We propose a
general approach to mitigate this problem when the I/O speciﬁcations are inferred by a statistical
learning system, and the noise originates from inaccuracies of this system. We believe that such
settings will become increasingly common with the rise of hybrid neuro-symbolic models for machine
reasoning. According to the taxonomy of Raychev et al. (2016), our ﬁltering heuristics fall into the
category of dataset cleaning methods, aiming at removing incorrect speciﬁcations before they are fed
to the solver. Other categories include (i) probabilistic program synthesis (Nori et al., 2015), which
relaxes the speciﬁcations into stochastic constraints (ii) genetic programming as a way of exploring
large solution spaces while maximizing an objective function (Cramer, 1985; Baker, 1987).

Neurally enhanced program synthesis An important line of work aims at using neural architec-
tures to speed-up rule-based program synthesis by guiding the search process. DeepCoder (Balog
et al., 2017) predicts an order on the program space in order to guide the rule-based solver towards
potential solutions. Lee et al. (2018) propose to learn a probabilistic model attributing a likelihood to
each program, with the aim of ﬁrst exploring likely solutions. Ellis et al. (2018) learn a bias optimal
strategy based on Levin search (Levin, 1973), with the goal of efﬁciently allocating compute time to
different parts of the program search space. We did not need to use such approaches as synthesis was
reasonably fast in the Karel and ViZDoom benchmarks.

Programmatic reinforcement learning The idea of representing reinforcement learning policies
as programs is investigated in a growing body of work. However, the search space of programs is
highly non-smooth and makes the resulting problem intractable. Different works aim at mitigating
this problem by ﬁrst training a good neural policy via deep reinforcement learning. The policy
is then used as an oracle providing I/O examples to generate a programmatic policy with similar
behavior (Bastani et al., 2018; Verma et al., 2018, 2019). Despite some high-level similarities, the
setting of program synthesis from diverse demonstration videos has some fundamental particularities
(i) we aim at ﬁnding at program matching exactly the different examples, rather than just exhibiting
similar behavior (ii) we consider that high-level description of each video is only available at training
time (iii) we only dispose of a ﬁxed, limited number of demonstrations, while the neural policy can
be used to generate an arbitrary number of examples.

Black-box imitation learning There exists prior work in the ﬁeld of imitation learning which
does not assume access to agent’s actions. Torabi et al. (2018) develop a method for behavioral
cloning from state observations. In (Nair et al., 2017), a robot learns pixel-level inverse dynamics
by examining the inﬂuence of its own actions, in order to be able to infer the actions of the expert.
However, this line of work does not consider program representations of policies.

Selective prediction Our ﬁltering strategies relates to the concept of selective prediction in sta-
tistical learning, also known as reject option. Since the seminal work of Chow (1957), the idea of
rejecting certain predictions because of their low conﬁdence has been extensively studied in various
settings (Fumera, Roli, 2002; Geifman, El-Yaniv, 2017). Our action and prediction conﬁdences can be
seen as extensions of the softmax response (Geifman, El-Yaniv, 2019) for sequence prediction tasks.
Other works have proposed to integrate the reject option in the learning process Cortes et al. (2016):
a potential direction for future work is to integrate these mechanisms in our neural architecture. In the
speciﬁc ﬁeld of statistical program learning, very recent work has proposed to use selective prediction
in order to ensure adversarial robustness for code (Bielik, Vechev, 2020).

6 Conclusion

A crucial challenge for intelligent systems is the ability to perform abstract reasoning from raw,
unstructured data. To achieve this goal, prior work (Ellis et al., 2018) has evidenced the power of
combining formal rule-based techniques with neural architectures. PLANS is the ﬁrst application
of such hybrid systems to the challenging task of identifying an agent’s decision making logic from
multiple videos. It sets up a new state-of-the-art for this task, with strictly less supervision signals. To
efﬁciently combine neural and rule-based components, we developed an adaptive ﬁltering heuristic for
neurally inferred speciﬁcations. We believe that the heuristic is quite general and will be applicable
to similar hybrid neuro-symbolic systems in the future.

8

Broader Impact

The ability to understand agents’ decision making logic from visual input can be applied to real-world
observations such as videos of people driving. In this context, a simple example of logic that can be
represented by a program is: "if the trafﬁc light is green, move, else stop". Since PLANS requires no
program supervision, it extends the applicability of prior work to new settings where no ground-truth
programs are available. We believe that PLANS can eventually be applied in all contexts involving
interaction between agents (human, animal or robotic) and their environment.

In the case of human agents, it is essential to foresee the impact on privacy of the observed subjects.
In order to avoid privacy violations, it must be carefully assessed in each application of PLANS which
components of a person’s decision making logic can be interpreted without being intrusive. While the
concern of data privacy is not speciﬁc to PLANS, it opens up new ethical and legal questions about
the exact meaning and status of describing an agent’s behavior.

It is also crucial to protect individuals from misinterpretation of their behavior. Prior systems for
program synthesis from visual observations essentially have two failures modes: The ﬁrst one
corresponds to the case where the behavior in one individual video is not understood correctly, which
will lead the synthesized program to capture behaviors that have not been actually observed. The
second one occurs when the different videos are individually correctly interpreted, but the branching
between the different behaviors (represented by the program control-ﬂow) is not correctly inferred.
PLANS addresses the ﬁrst problem by leveraging the prediction conﬁdence of its neural component,
and the second by using a rule-based system that offers correctness guarantees. These aspects make
PLANS more robust than prior work, which implies more reliability for end-users.

References

Allamanis Miltiadis, Barr Earl T, Devanbu Premkumar, Sutton Charles. A survey of machine learning

for big code and naturalness // ACM Computing Surveys (CSUR). 2018. 51, 4. 1–37.

Alur Rajeev, Bodik Rastislav, Juniwal Garvit, Martin Milo MK, Raghothaman Mukund, Seshia
Sanjit A, Singh Rishabh, Solar-Lezama Armando, Torlak Emina, Udupa Abhishek. Syntax-guided
synthesis. 2013.

Baker James E. Reducing bias and inefﬁciency in the selection algorithm // Proceedings of the

second international conference on genetic algorithms. 1987.

Balog Matej, Gaunt Alexander L, Brockschmidt Marc, Nowozin Sebastian, Tarlow Daniel. Deepcoder:

Learning to write programs // International Conference on Learning Representations. 2017.

Bastani Osbert, Pu Yewen, Solar-Lezama Armando. Veriﬁable reinforcement learning via policy

extraction // Advances in Neural Information Processing Systems. 2018.

Bielik Pavol, Vechev Martin. Adversarial Robustness for Code // International Conference on Machine

Learning. 2020.

Chow Chi-Keung. An optimum character recognition system using decision functions // IRE

Transactions on Electronic Computers. 1957. 4. 247–254.

Cortes Corinna, DeSalvo Giulia, Mohri Mehryar. Learning with rejection // International Conference

on Algorithmic Learning Theory. 2016.

Cramer Nichael Lynn. A representation for the adaptive generation of simple sequential programs //

Proceedings of the ﬁrst international conference on genetic algorithms. 1985. 183–187.

De Moura Leonardo, Bjørner Nikolaj. Z3: An efﬁcient SMT solver // International conference on

Tools and Algorithms for the Construction and Analysis of Systems. 2008.

Devlin Jacob, Uesato Jonathan, Bhupatiraju Surya, Singh Rishabh, Mohamed Abdel-rahman, Kohli
Pushmeet. Robustﬁll: Neural program learning under noisy i/o // International Conference on
Machine Learning. 2017.

9

Duan Xuguang, Wu Qi, Gan Chuang, Zhang Yiwei, Huang Wenbing, Hengel Anton van den, Zhu
Wenwu. Watch, Reason and Code: Learning to Represent Videos Using Program // Proceedings of
the 27th ACM International Conference on Multimedia. 2019.

Ellis Kevin, Ritchie Daniel, Solar-Lezama Armando, Tenenbaum Josh. Learning to infer graphics
programs from hand-drawn images // Advances in Neural Information Processing Systems. 2018.

Ellis Kevin, Solar-Lezama Armando, Tenenbaum Josh. Unsupervised learning by program synthesis

// Advances in Neural Information Processing Systems. 2015.

Fumera Giorgio, Roli Fabio. Support vector machines with embedded reject option // International

Workshop on Support Vector Machines. 2002.

Ganin Yaroslav, Kulkarni Tejas, Babuschkin Igor, Eslami SM, Vinyals Oriol. Synthesizing programs

for images using reinforced adversarial learning // arXiv preprint arXiv:1804.01118. 2018.

Gaunt Alexander L, Brockschmidt Marc, Singh Rishabh, Kushman Nate, Kohli Pushmeet, Taylor
Jonathan, Tarlow Daniel. Terpret: A probabilistic programming language for program induction //
arXiv preprint arXiv:1608.04428. 2016.

Geifman Yonatan, El-Yaniv Ran. Selective classiﬁcation for deep neural networks // Advances in

Neural Information Processing Systems. 2017. 4878–4887.

Geifman Yonatan, El-Yaniv Ran. Selectivenet: A deep neural network with an integrated reject option

// arXiv preprint arXiv:1901.09192. 2019.

Gulwani Sumit. Automating string processing in spreadsheets using input-output examples // ACM

Sigplan Notices. 2011. 46, 1. 317–330.

Gulwani Sumit, Harris William R, Singh Rishabh. Spreadsheet data manipulation using examples //

Communications of the ACM. 2012. 55, 8. 97–105.

Jha Susmit, Gulwani Sumit, Seshia Sanjit A, Tiwari Ashish. Oracle-guided component-based program
synthesis // 2010 ACM/IEEE 32nd International Conference on Software Engineering. 2010.

Kempka Michał, Wydmuch Marek, Runc Grzegorz, Toczek Jakub, Ja´skowski Wojciech. Vizdoom: A
doom-based ai research platform for visual reinforcement learning // 2016 IEEE Conference on
Computational Intelligence and Games (CIG). 2016.

Lee Woosuk, Heo Kihong, Alur Rajeev, Naik Mayur. Accelerating search-based program synthesis

using learned probabilistic models // ACM SIGPLAN Notices. 2018. 53, 4. 436–449.

Levin Leonid Anatolevich. Universal sequential search problems // Problemy peredachi informatsii.

1973. 9, 3. 115–116.

Lipton Zachary C. The mythos of model interpretability // Queue. 2018. 16, 3. 31–57.

Liu Yunchao, Wu Zheng, Ritchie David A., Freeman William T., Tenenbaum Joshua B., Wu Jiajun.
Learning to Describe Scenes with Programs // International Conference on Learning Representa-
tions. 2019.

Luong Minh-Thang, Pham Hieu, Manning Christopher D. Effective approaches to attention-based

neural machine translation // arXiv preprint arXiv:1508.04025. 2015.

Maas Andrew L, Hannun Awni Y, Ng Andrew Y. Rectiﬁer nonlinearities improve neural network

acoustic models // International Conference on Machine Learning. 2013.

Montavon Grégoire, Samek Wojciech, Müller Klaus-Robert. Methods for interpreting and under-

standing deep neural networks // Digital Signal Processing. 2018. 73. 1–15.

Nair Ashvin, Chen Dian, Agrawal Pulkit, Isola Phillip, Abbeel Pieter, Malik Jitendra, Levine Sergey.
Combining self-supervised learning and imitation for vision-based rope manipulation // 2017 IEEE
International Conference on Robotics and Automation (ICRA). 2017.

10

Ng Andrew Y., Russell Stuart J. Algorithms for Inverse Reinforcement Learning // International

Conference on Machine Learning. 2000.

Nori Aditya V, Ozair Sherjil, Rajamani Sriram K, Vijaykeerthy Deepak. Efﬁcient synthesis of

probabilistic programs // ACM SIGPLAN Notices. 2015. 50, 6. 208–217.

Parisotto Emilio, Mohamed Abdel-rahman, Singh Rishabh, Li Lihong, Zhou Dengyong, Kohli Push-
meet. Neuro-symbolic program synthesis // International Conference on Learning Representations.
2017.

Pattis Richard E. Karel the robot: a gentle introduction to the art of programming. 1981.

Raychev Veselin, Bielik Pavol, Vechev Martin, Krause Andreas. Learning programs from noisy data //

ACM SIGPLAN Notices. 2016. 51, 1. 761–774.

Ross Stéphane, Gordon Geoffrey, Bagnell Drew. A reduction of imitation learning and structured
prediction to no-regret online learning // Proceedings of the fourteenth international conference on
artiﬁcial intelligence and statistics. 2011.

Schaal Stefan. Learning from demonstration // Advances in Neural Information Processing Systems.

1997.

Sun Shao-Hua, Noh Hyeonwoo, Somasundaram Sriram, Lim Joseph. Neural program synthesis from

diverse demonstration videos // International Conference on Machine Learning. 2018.

Tian Yonglong, Luo Andrew, Sun Xingyuan, Ellis Kevin, Freeman William T., Tenenbaum Joshua B.,
Wu Jiajun. Learning to Infer and Execute 3D Shape Programs // International Conference on
Learning Representations. 2019.

Torabi Faraz, Warnell Garrett, Stone Peter. Behavioral cloning from observation // Proceedings of

the Twenty-Seventh International Joint Conference on Artiﬁcial Intelligence. 2018.

Torlak Emina, Bodik Rastislav. Growing solver-aided languages with rosette // Proceedings of the
2013 ACM international symposium on New ideas, new paradigms, and reﬂections on programming
& software. 2013.

Verma Abhinav, Le Hoang, Yue Yisong, Chaudhuri Swarat.

Imitation-Projected Programmatic

Reinforcement Learning // Advances in Neural Information Processing Systems. 2019.

Verma Abhinav, Murali Vijayaraghavan, Singh Rishabh, Kohli Pushmeet, Chaudhuri Swarat. Pro-
grammatically interpretable reinforcement learning // International Conference on Machine
Learning. 2018.

Waldinger Richard J, Lee Richard CT. PROW: A step toward automatic program writing // Proceed-

ings of the 1st international joint conference on Artiﬁcial intelligence. 1969.

Zhu He, Xiong Zikang, Magill Stephen, Jagannathan Suresh. An inductive synthesis framework
for veriﬁable reinforcement learning // Proceedings of the 40th ACM SIGPLAN Conference on
Programming Language Design and Implementation. 2019.

11

We provide the following appendices.

• In A, we give additional information about the datasets (Karel and ViZDoom).
• In B, we describe precisely the neural component of PLANS and its training process.
• In C, we present the implementation of the rule-based solver.
• In D, we analyse the temporal complexity of PLANS.

A Datasets

Table 3 contains high-level information about both datasets. We point to the following three differ-
ences that are relevant to our experimental results:

• Contrary to Karel, the ViZDoom demonstrations have a ﬁrst-person point of view. Because
of this subjective view, the environment is sometimes only partially observable, for instance
when the agent’s ﬁeld of vision is occluded by a monster. In this situation, there might
be a doubt about whether a second monster is hidden behind the ﬁrst one. This accounts
for the high number of uncertain predictions of actions and perceptions in the ViZDoom
environment.

• The resolution of observations is signiﬁcantly larger in the ViZDoom environment, and
there are more possible actions. This explains the need for a longer training of the neural
component in the ViZDoom environment.

• In the ViZDoom environment, PLANS has access to more demonstrations to infer the
underlying program. This is why we achieve similar accuracy on Karel and ViZDoom,
despite the difﬁculties mentioned above.

For additional information about the datasets, we refer to (Sun et al., 2018). Figures 4 and 5 give
examples of programs and demonstrations for the Karel and ViZdoom benchmarks respectively.

B Architecture and training

All video frames are ﬁrst encoded with a convolutional neural network. All convolutional layers have
kernel size 3 and stride 2. For the Karel dataset, we use 3 layers with respectively 16, 32 and 48
channels. For The ViZDoom dataset, we use 5 layers with respectively 16,32,48,48 and 48 channels.
All the layers have LeakyRELU (Maas et al., 2013) activation and batch normalization. The resulting
frame encodings are fed to a LSTM layer with 512 hidden units. We use two different LSTM layers
for decoding: one predicts the action sequence, the other the perception sequences. Both decoding
layers have 512 hidden units. For both action and perceptions, we use a softmax output layer for
predicting the probability of all classes. This encoder-decoder model is enhanced with the attention
mechanism from Luong et al. (2015).

12

Table 3: Dataset properties

Model

Karel

ViZDoom

Point of view
Frame resolution
# Actions
# Perceptions
# Observed demonstrations
# Unseen demonstrations
# Training samples
# Test samples

Third person
8x8
5
5
10
5
30000
5000

First person
80x80
11
6
25
5
80000
8000

Figure 4: Example program and demonstrations for the Karel dataset. Figure from Sun et al. (2018).

Figure 5: Example program and demonstrations for ViZDoom. Figure from Sun et al. (2018).

Table 4: Number of training steps

Model

Karel

ViZDoom

Phase 1
Steps Batch size Steps Batch size Steps Batch size

Phase 2

demo2program ?

ours

10000

128
32

50000
30000

32
8

50000

8

13

All models are trained with the Adam optimizer, using default parameters and learning rate (0.001).
In Table 4, we give batch size and number of training steps for both our model and the demo2program
baseline. To the best of our knowledge, the number of training steps for demo2program on the Karel
environment has not been provided by Sun et al. (2018).

C Details about solver implementation

In C.1, we present two algorithms that describe the exact order of solver calls with static and dynamic
ﬁltering respectively. In C.2, we detail which heuristics were used to improve the program and
sequence accuracy metric. In C.3, we show how the different control-ﬂow constructs were encoded
in the Rosette solver.

C.1 Algorithms

Algorithm 1: Calls to the solver, with static ﬁltering

Input: Set of demonstrations T with neurally inferred action and perception sequences
Output: Program summarizing the different demonstrations, or unsat

G ← {τ ∈ T | actionconf(τ ) ≥ 0.98 ∧ perconf(τ ) ≥ 0.9}
for n ∈ range(max_n) do

(cid:46) Static ﬁltering
(cid:46) Progressively increase number of control-ﬂow statements
(cid:46) Call to Rosette solver

solution ← solver(G, n)
if solution is not unsat then return solution

return unsat

Algorithm 2: Calls to the solver, with dynamic ﬁltering

Input: Set of demonstrations T with neurally inferred action and perception sequences
Output: Program summarizing the different demonstrations, or unsat

F ← {τ ∈ T | actionconf(τ ) ≥ 0.98}
τ1, . . . , τk ← elements of F sorted by decreasing perception conﬁdence
for prop ∈ [1, 0.95, 0.9, 0.8, . . . , 0.2, 0.1] do

(cid:46) Static ﬁltering by action conﬁdence

u ← (cid:100)prop · k(cid:101)
G ← {τ1, . . . , τu}
for n ∈ range(max_n) do

solution ← solver(G, n)
if solution is not unsat then return solution

(cid:46) Dynamic ﬁltering by perception conﬁdence
(cid:46) Determines number of demonstrations
(cid:46) Demos with highest perception conﬁdence
(cid:46) Progressively increase number of control-ﬂow statements
(cid:46) Call to Rosette solver

return unsat

C.2 Solver heuristics

In order to guarantee fair comparison with fully neural approaches, we describe in detail the heuristics
used in designing the solver

1. On the Karel dataset, some synthesis problem can be solved indifferently with an if or a while
statement. We observed that choosing the solution using while yields better perfomance
on the program and sequence accuracy metrics. On the ViZDoom dataset, we privilege
solutions with if.

2. On the Karel dataset, some synthesis problem involving loops have several satisfying
solutions, in which the size of the block before the loop differs. We observed that choosing
the solution with the smallest number of actions outside the loop body improved program
and sequence accuracy.

14

Table 5: Time measurements for PLANS. We report training and inference time. Training time
corresponds to the whole training process. Inference time is measured for each program individually
and averaged on the whole test set. For inference, we measure separately time spent inferring
speciﬁcations with the neural component, and time of the longest solver call.

Karel

ViZDoom

Training

∼ 10 hours ∼ 2 days

Inference of speciﬁcations
Longest solver call

1.39s
12.43s

3.68s
2.28s

C.3 DSL encoding in Rosette

In Figure 6, we describe the encoding of the different control-ﬂow constructs in the DSL. This
encoding is for the Karel environment that has ﬁve different actions and ﬁve perception primitives.
The ViZDoom encoding is exactly similar except that it has more actions and perception primitives.
This encoding does not consider nested control-ﬂow constructs: Indeed, we observed experimentally
that these are not necessary to obtain satisfying accuracy. Besides, this allows for faster solver calls
as this reduces the size of the search space. However, if this comes to be needed in other application
domains, this assumption can easily be lifted by slightly modifying the encoding.

D Analysis of temporal complexity

In the section, we analyze the complexity of our algorithms. The static ﬁltering algorithm makes
O(max_n) calls to the solver, where max_n is the maximum number of control-ﬂow statements
allowed in the generated program. The dynamic ﬁltering algorithm makes O(max_n · n_prop)
calls to the solver, where n_prop is the number of iterations of the outer loop that increments the
perception ﬁltering threshold. In both cases, all solver calls are independent and can be performed in
parallel. Therefore, the bottleneck of our algorithms is the duration of the longest solver call.

In both environments, we measured the duration of the longest solver call for each test program,
and we averaged the measurements over all instances. For comparison purposes, we also report the
average time taken by the neural component of PLANS to infer the speciﬁcations for one program.
We performed all experiments on a machine running Ubuntu 18.04 with 2.00GHz Intel Xeon E5-2650
CPU and using a single GeForce RTX 2080 Ti GPU. The resulting values are reported in Table 5. We
also report training time of the neural component.

In the ViZDoom environement, we observe that for a given program, the time spent by the neural
component to infer the speciﬁcations and the longest solver call have same order of magnitude. This
means that our model has no signiﬁcant computational overhead with respect to the fully neural
baselines. In the Karel environment, we observe that the duration of the longest solver call is one
order of magnitude higher. In our experiments, we observe that the longest solver call is always the
last one, with n = 2. If max _n is decreased, the average duration falls below 3s, but at the cost of
an execution accuracy decrease of a few %.

15

Figure 6: Rosette encoding of the different DSL constructs for the Karel dataset.

16

