0
2
0
2

t
c
O
9
2

]

G
L
.
s
c
[

2
v
0
8
0
2
0
.
6
0
0
2
:
v
i
X
r
a

A mathematical model for automatic differentiation
in machine learning

J´erˆome Bolte˚
Toulouse School of Economics
Univ. Toulouse
Toulouse, France

Edouard Pauwels
IRIT, CNRS
Univ. Toulouse
Toulouse, France

Abstract

Automatic differentiation, as implemented today, does not have a simple mathe-
matical model adapted to the needs of modern machine learning. In this work we
articulate the relationships between differentiation of programs as implemented
in practice and differentiation of nonsmooth functions. To this end we provide
a simple class of functions, a nonsmooth calculus, and show how they apply to
stochastic approximation methods. We also evidence the issue of artiﬁcial critical
points created by algorithmic differentiation and show how usual methods avoid
these points with probability one.

1

Introduction

Optimization algorithms based on backpropagation oracles, and more generally on automatic or
algorithmic differentiation (AD) [41, 39], are one of the most widely used training tools for modern
learning architectures [14, 32, 15, 18, 20, 3, 16]. They often rely on popular numerical implementa-
tions as TensorFlow or PyTorch [1, 36]. However, for nonsmooth, nonconvex losses, AD does not
have a stable theory [23, 25, 26, 2, 30, 28, 29, 12], matching the actual practice. We wish to present a
simple mathematical framework addressing this issue. Let us progressively explain our approach.

1.1 What is backpropagation?

Algorithmic differentiation acts on programs not on functions: To convey this fact we carry
out a small experiment in TensorFlow [1] with the function relu : t ÞÑ maxt0, tu, see Appendix A.2
for implementation details. Algorithmic differentiation is displayed in Figure 1, in particular, we
have relu1p0q “ 0. Consider the two functions

relu2 : t ÞÑ relup´tq ` t,

relu3 : t ÞÑ

1
2

preluptq ` relu2ptqq.

As mathematical functions on R these are equal to relu. However TensorFlow returns relu1
2p0q “ 1
and relu1
3p0q “ 1{2 (Figure 1). Indeed, AD does not act on functions, but on their representations,
i.e., on programs. Different programs implementing the same function may provide different results,
beyond numerical precision; we refer to this as the spurious behaviour of AD for nonsmooth
functions2. Let us explore this phenomenon further. The function zero : t ÞÑ relu2ptq ´ reluptq,
outputs constantly 0 but AD gives zero1p0q “ 1. More generally, one can modify the value of the
derivative of a given function at prescribed arguments (Figure 1). This may generate artiﬁcial critical
points; for instance x Ñ x ´ zero is the identity but its derivative at 0 according to AD is 0.

˚Authors in alphabetical order.
2The validity domain of AD is restricted in theory to smooth functions [23], yet it is common practice to use

it for nonsmooth functions.

34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

 
 
 
 
 
 
Figure 1: Top: AD applied to relu and two different implementations of the same function. Bottom:
Algorithmic differentiation of a constant function, creation of artiﬁcial critical point or arbitrary
derivatives at prescribed arguments for the sine function.

This discussion was limited to univariate functions, but these pathologies grow in size and in
complexity when occurring in higher dimensions. Besides, as the “compositional depth” of functions
increases the phenomenon gets more complex, making the geometry of artiﬁcial point difﬁcult to
grasp.

Canonical surjection between programs functions: Numerical programs combine basic mathe-
matical functions within an algorithm and return an output. This can be understood in two ways:

• Computer science: it is a sequence of instructions with numerical inputs-outputs,
• Mathematics: the program is a function3 of its arguments.

It is tempting to identify both, but functions can be represented by different programs. This deﬁnes a
surjection F mapping a program to a function (in the class of functions “accessible through coding”).

Algorithmic differentiation: As presented above, AD is an operation on programs, A which takes
as argument a program and returns a program with the same input variables. This operation can
be “pushed” to the space of functions using the canonical surjection F. Remarkably, if we restrict
ourselves to programs P which only smoothly combine smooth functions, then we have the following
fundamental relation, depicted in Figure 2:

In other words, algorithmic differentiation of a program which smoothly combines smooth functions,
is equivalent, through the canonical surjection, to derivation.

FpApPqq “ ∇FpPq.

(1)

Figure 2: Left: Algorithmic differentiation applied to programs combining smooth functions in
a smooth way, the diagram commutes. Right: Algorithmic differentiation in nonsmooth settings,
connection with known notion of generalized derivative is much less clear.

3In the usual mathematical sense.

2

21012x0.00.51.01.52.0relu'relu21012x0.00.51.01.52.0relu2'relu221012x0.00.51.01.52.0relu3'relu321012x0.000.250.500.751.00zero'zero21012x1.00.50.00.51.0mysin2'mysin221012x1012mysin3'mysin3J(function)P(program)∇J(function)D⊂P∇:diﬀF:surjF:surjA:autodiﬀJ(function)P(program)∂J(setfunction)D⊂P∂:sub-diﬀF:surjF:surj(cid:54)=A:autodiﬀHowever practitioners use AD and backpropagation beyond smooth programs with nonsmooth ele-
mentary functions or program branching for instance. Can we ﬁnd a proper operational interpretation
of this widespread practice?

Algorithmic differentiation cannot be represented through a variational operator At ﬁrst, it
is tempting to simply use AD to induce a differential operator on functions generalizing classical
differentiation. This operator, say BA, should:

(a) encompass the outputs of algorithmic differentation for all functions

(b) be such that 0 is an element of BApreluq at 0.

Unfortunately such an operator does not exist:

Theorem 1 (Algorithmic differentiation does not induce an operator on functions) There is no
nontrivial operator on functions satisfying paq and pbq.

1.2 Contribution and related work

We address this impossibility result and provide a class of functions together with an operational
nonsmooth differential calculus which is able to cope with spurious behaviours.

Elementary selections and selection derivatives: We introduce a new class of nonsmooth non-
convex functions, encompassing most objective functions met in machine learning, having appealing
stability properties. This allows us to deﬁne simple differential objects called selection derivatives.
Selection derivatives turn out to have an operational calculus adapted to the analysis of many learning
methods, as backpropagation or stochastic ﬁrst order methods. They thus provide an operational
model to capture nonsmooth AD as implemented in current numerical software.

Algorithmic differentiation, algorithms This framework allows to formalize properly the rela-
tionships between, functions, algorithmic differentiation and capture the corresponding notion of
critical points as met in practice. These characterize the set of attractors (limit points) for stochastic
approximation algorithms based on nonsmooth backpropagation [37, 4, 31, 5, 13]. It is important to
stress that these attractors, which models sharply the whole scope of AD-induced stationnarity, are
different from the traditional notions as Clarke criticality [17, 38, 20]. This is described in Theorems 3
and 4.

Avoidance of traps: As sketched above and in the introduction AD produces artiﬁcial critical
points, i.e. stationary points which are not Clarke critical. These points have a parasitic nature
which could be detrimental to training purposes, were they met in practice. We show that randomly
initialized mini-batch stochastic gradient method do not lead to artiﬁcial critical points (Theorem 4).
This result applies to modern machine learning software libraries based on AD [1, 36], seen as
performing operation over the reals, without any modiﬁcation. Although AD may have unpredictable
behavior in nonsmooth contexts, both theoretically and numerically, this result justiﬁes theoretically
that the practical impact is somewhat negligible in the context of common machine learning usage.

Related work: Spurious behaviour of AD in nonsmooth context has been investigated in [23, 25,
26, 27, 2, 30, 12]. In particular, [27, 30] considers qualiﬁcation conditions allowing to construct
AD algorithms which compute proper Clarke subgradients [17, 38, 20]. However qualiﬁcation is
extremely hard to check and almost impossible to enforce in practice. Let us also mention [2] which
uses the notion of lexicographic derivatives, but, at this day, algorithmic computations are limited to
forward mode for the moment which is of little use in machine learning.

[23, 25, 27, 26, 28, 29] use settings closer to ours. Piecewise smooth functions, selection derivatives
and their variational properties are extensively described in [40]. Our approach differs because we
adopt more stringent deﬁnitions and rigidity assumptions, which allows in turn for much stronger
properties. For instance, we fully treat backward algorithmic differentiation which is the most useful
tool in machine learning.

3

Altogether, our contribution is an accessible and elementary framework for the conservative ﬁelds
recently introduced in [12], without explicitly requiring the introduction of semialgebraic geometry
and o-minimal structures [21, 19].

Stochastic approximation algorithms [37, 4, 31, 5, 13] are widely used in machine learning contexts
[39, 14, 35, 32, 15, 18, 16]. For example [20] describes asymptotics of stochastic subgradient
algorithms in nonsmooth, nonconvex settings. In contrast, we do not assume access to subgradients
and instead explicitly model the behaviour of AD in optimization contexts. Our convergence results
are based on [12], complemented by a new result on “the avoidance of critical traps” in the line of [7]
in the context of long run convergence.
Notations The ambient space is Euclidean Rp. For each k, ek is the k-th vector of the canonical
basis. We use D : Rm Ñ Rq for set valued functions, i.e functions from Rm to the subsets of Rq.
The convex hull of A Ă Rp is denoted by convpAq. All proofs are postponed to the Appendix.

2 Basic piecewise differentiable functions and selection gradient

We introduce a simple but vast class of functions that model rigorously the machine learning models
and losses for applications such as deep learning.

Deﬁnition 1 (Elementary (log-exp) functions) Elementary (log-exp) functions are functions on Rp
described by a ﬁnite compositional expression involving basic operations, `, ´, ˆ, { as well as afﬁne
mappings, exponential and logarithms, inside their domain of deﬁnition. We denote by E the set of
elementary functions in any dimension p.

Examples include polynomials, logistic loss, boosting loss, Gaussian likelihood. Observe that the
corresponding functions are C 8 smooth on their open domains. Note also that if log and exp are not
present we obtain the ﬁeld of rational functions. See Remark 1 in Appendix A.3.

Deﬁnition 2 (Elementary index) s : Rp ÞÑ t1, . . . , mu is an elementary (log-exp) index if the set
tx P Rp, spxq “ iu is the solution set of a ﬁnite number of inequalities and equalities involving
elementary functions on Rp. The set of such functions is denoted by I (for any input dimensions p).

Examples: The Heaviside function, the index of the largest or k-th largest element in a vector, the
sign pattern of a vector in Rp which is indexed by integers from 1 to 2p.

Deﬁnition 3 (Elementary selection) Let f : Rp ÞÑ R be continuous. We say that f has an elemen-
tary (log-exp) selection ps, f1, . . . , fmq if s : Rp ÞÑ p1, . . . , mq is an elementary index in I and for
i “ 1 . . . , m, fi : Rp ÞÑ R are elementary functions in E, such that for all x P Rp,

f pxq “ fspxqpxq.

(2)

The m ` 1-uplet ps, f1, . . . , fmq is a representation of f , and f admits an elementary (log-exp)
selection. The class of such functions is denoted by Slog exp or simply here S. This extends to
functions from Rp to Rm by applying a coordinatewise deﬁnition with a common elementary index.

Observe that the representation is never unique, both in s and in the sequence f1, . . . , fm. The ReLU,
hinge loss, maximal entry, k-th largest entry functions are elementary selections. Note also that
continuity is part of the deﬁnition.

Proposition 1 (Stability of S by ˝, `, ˆ) The class S of elementary selections is stable by compo-
sition, sum and product.

The class S is close to the one of piecewise C k functions, see e.g [40], but it is also much more
disciplined since indices and functions are required to satisfy strong “log-exp” rigidity assumptions.

2.1 Selection derivative

Functions in S can be associated with a ﬂexible notion of generalized derivative based on the selection
structure of the underlying function.

4

Deﬁnition 4 (Selection gradient) (i) Let f : Rp ÞÑ R, in S with selection ps, f1, . . . , fmq. We set
the selection derivative of f with respect to s to be

p∇sf : x ÞÑ ∇fspxqpxq.
This extends to multivariate outputs by applying the deﬁnition coordinatewise, which leads to a
notion of a selection Jacobian denoted by pJ s.
(ii) Given a function f P S, a selection derivative is a derivative of the form (3) for a given
representation. In that case a selection derivative of f is merely denoted by p∇f .

(3)

Example: Set for all x P R, f1pxq “ 0, f2pxq “ x and spxq “ 1 for x ď 0 and spxq “ 2 for x ą 0.
This this deﬁnes the relu function and its selection derivative at 0 is 0. See more in Appendix A.2.
Remark: (a) p∇f is different from any known notion of subgradient. Set for all x P R, f1pxq “ 0,
f2pxq “ x and spxq “ 1 for x ‰ 0 and sp0q “ 2. This deﬁnes a elementary selection for the null
function however, p∇sf p0q “ 1. This is the zero function of the introduction.
(b) This formalizes what one would obtained by differentiating a code with all decision branches
frozen and hence represents the numerical output of AD (see 4). Note that one only needs one branch
and do not need to explore all possible outcomes, avoiding combinatorial explosion.

The properties of selection derivatives might seem too liberal at ﬁrst sight and too disconnected from
the original function, but this is not the case as shown below.

Proposition 2 (Integration along segments) Let f : Rp ÞÑ R be in S, with elementary selection
ps, f1, . . . , fmq. Then f is locally Lipschitz and for all y, x in Rp.:

f pyq ´ f pxq “

A
y ´ x, p∇sf px ` tpy ´ xqq

E

dt

ż

1

0

Proposition 3 (Gradient almost everywhere) Let f : Rp ÞÑ R be in S, with elementary selection
i“1 clpUiq “ Rp
ps, f1, . . . , fmq. There exists sets U1, . . . , UN with nonempty interior such that
and for each i “ 1, and for all x in the interior of Ui, p∇sf pxq “ ∇f pxq. Furthermore, the Ui are
solution sets of equations and inequalities involving functions in E.

Ť

N

Remark: Although less transparent, Proposition 2 is not a consequence of Proposition 3. Both results
crucially rely on the rigidity of elementary functions in E (Deﬁnition 3), not only on their piecewise
smoothness. This a central novelty of our approach.

2.2 A calculus for selection derivatives

One has an unusual differential calculus: although it does not involve the linearity of some
(sub)differential operator, the selection derivative of a sum gives a sum of selection derivatives
provided that the selection is reﬁned.

Proposition 4 (Chain rule) Let F : Rp1 ÞÑ Rp2 such that each of its coordinate fi, i “ 1 . . . p2, is
in S and g : Rp2 ÞÑ R, g P S. Consider a selection Jacobian for F , pJF : Rp1 ÞÑ Rp2ˆp1

˛

¨

˚
˝

p∇f1pxqT
...
p∇fqpxqT

x ÞÑ

‹
‚

(4)

Then g ˝ F P S and the function x ÞÑ

pJF pxqT p∇gpF pxqq is a selection derivative for g ˝ F .

Proposition 4 extends readily to the case when the outer function g is multivariate. For example, we
have a sum rule p∇pf ` gq “
p∇g for full-domain functions f, g in S. Indeed, if F1 and F2 are
elementary selections then F1 ˝ F2 P S and

p∇f `

pJF1˝F2 “ p

pJF1 ˝ F2q ˆ

pJF2 .

(5)

5

3 Programs and elementary selections

Numerical programs encode numerical functions by combining elementary functions using a prede-
cessor relation which models program execution. In what follows, m can be seen as an estimate of
the memory footprint of a program4, while p and q the number of inputs and outputs respectively.

Given positive integers m ě p ` q, a predecessor relation is a set valued map pr : t1, . . . , mu Ñ
t1, . . . , mu such that

• For i P t1, . . . , mu and j P prpiq, j ă i.

‚ For i P tp ` 1, . . . , mu, prpiq is nonempty.

Algorithm 1: Program evaluation
Program data: p, q ě 1,
m ě p ` q, pr a predecessor
relation, G “ pgiqm
function sequence.

A predecessor relation induces a partial order on the set
of integers from 1 to m and hence can be represented
by a directed acyclic graph [34, Theorem 9.4.9]. Given
pp, q, mq and a predecessor relation pr, a elementary
function sequence G “ pgiqm
i“p`1 is a set of functions
ÞÑ R, and gi P S, for all i “
such that gi : R|prpiq|
p ` 1, . . . , m. A program P is then given by the data
P “ pp, q, m, pr, Gq , while its evaluation is described
in Algorithm 1. We denote by P the set of programs,
and Pp,q when input-output dimensions have to be made
explicit.
By deﬁnition a program encodes a function, but the
representation is not unique. We express this fact below
through the canonical surjection F of the introduction.
The following proposition illustrates the fact that practitioners implicitly implement selection
functions when writing programs.

Input: x “ px1, . . . xpq
1: for k “ p ` 1, p ` 2, . . . m do
xk “ gkpxprpkqq where
2:
xprpkq “ pxiqiPprpkq.

3: end for
Return: y :“ pxjqm

i“p`1 an adapted

j“m´q`1.

Proposition 5 (Programs represents elementary selections) Through its input-output correspon-
dence each program P of the form (3) induces a function which is an elementary selection. In other
words FpP q P S.

4 Algorithmic differentiation and a variational model

Algorithmic differentiation is based on the idea of propagating inﬁnitesimal variations in a program
P through the chain rule, either forward or backward.

Algorithm 2: Algorithmic differentiation computes selection gradients
Program data: p ě 1, m ě p ` 1, pr a predecessor relation, G “ pgiqm
function sequence.
Input: variables px1, . . . xmq computed by Algorithm 1, di “ pdirjsq|prpiq|
i “ p ` 1 . . . m

i“p`1 an adapted

j“1 “

p∇gipxprpiqq,

1: Forward mode:
2: Initialize: Bxk
k “ 1, . . . , p.

Bx “ ek,

3: for k “ p ` 1, . . . m do
4:

ÿ

Bxk
Bx

“

jPprpkq

Bxj
Bx

dkrjs

where x “ px1, . . . , xpq.

5: end for
Return: Bxm
Bx .

1: Backward mode:
2: Initialize: v “ em
3: for t “ m, . . . p ` 1 do
for j P prptq do
4:
5:

Update coordinate j of v:

vrjs :“ vrjs ` vrtsdtrjs

end for

6:
7: end for
Return: pvr1s, vr2s, . . . , vrpsq.

4We consider programs which do not overwrite values in memory

6

Consider Algorithm 1, and assume for simplicity that q “ 1. The program can be seen as the
implementation of m ´ p successive transformations on Rm, of the form

Gk : Rm ÞÑ Rm

x ÞÑ x ` ekpgkpxprpkqq ´ xkq,

for k “ p ` 1, . . . , m which belong to S. Algorithm 2 combines gradients dynamically along two
modes: forward or backward. Let us describes these two forms.
Fix x P Rm. After applying Algorithm 1, for each k, let dk P Rm be the selection gradient
p∇gkpxprpkqq, appending 0 to non dependant coordinates. A selection Jacobian of Gk (at x) is given
by

pJGk “ I ´ ekeT

k ` ekdT
k

Denote by Jp P Rmˆp, the matrix whose entries are 0, except for diagonal entries which are 1. In
Algorithm 2, the forward mode computes
pJGm . . . pJGp`1Jp “ eT

m ` emdT
m
which is a selection Jacobian thanks to the chain rule in (5). On the other hand the backward mode
computes

p`1 ` ep`1dT

I ´ ep`1eT

I ´ emeT

eT
m

. . .

Jp

p`1

m

`

˘

˘

`

`

˘

`

˘

p`1
This quantity turns out to be the same as the one computed by the forward mode thanks to:

J T
p

I ` dp`1eT

. . .

I ` dmeT
m

em.

I ´ ep`1eT

Lemma 1 Let p, m P N, 0 ă p ă m. Assume that for i “ p ` 1, . . . , m we have di P Rm. Then we
have
˘
`
Pp

I ` dmeT
m
(6)
where I P Rmˆm is the identity matrix and Pp P Rmˆm denotes the projection on the ﬁrst p
coordinates.

p`1 ` dp`1eT

I ` dp`1eT

m ` dmeT
m

I ´ emeT

“ Pp

. . .

. . .

p`1

p`1

˘

`

˘

`

˘

`

Denote by A : Pp,1 Ñ Pp,p the algorithmic-differentiation operator. This establishes the following
fundamental fact which is at the root of this work. This result asserts that practitioners implicitly
implement selection derivatives when writing numerical programs and calling forward or backward
AD on these programs.

Theorem 2 (Algorithmic differentiation outputs a selection gradient) Algorithmic differentia-
tion of a given program, i.e., ApP q, outputs a selection derivative of the underlying numerical
function. In other words there exists a representation of the numerical function FpP q with elementary
index s such that:

FpApP qq “

p∇sFpP q.

5 Algorithmic differentiation at work

5.1 Selection derivatives, conservative ﬁelds and Clarke subgradient

The asymptotic study of ﬁrst-order optimization methods implies limiting processes and necessitates
thus the introduction of graph closed operators. Given a representation for f , we may construct such
a convex-valued mapping pointwise as follows5.

S with elementary selection
Deﬁnition 5 (Representation minimal operator) Let f
ps, f1, . . . , fmq. For any x P Rp, set Ipxq “ ti P t1, . . . , mu , f pxq “ fipxqu. The index closure of
p∇sf is given by the set valued map

P

Ds

f : Rp Ñ Rp

x Ñ conv pt∇fipxq, i P Ipxquq .

where the double arrows express that the map has values in subsets of Rp, much like subgradients,
and conv denotes the convex hull.

5Minimality relates to the representation of the function, not the function itself. This is the minimal

convex-valued operator, constructed pointwise and guaranteed to be graph-closed.

7

The role of Ds
f is to capture all possible outputs of AD including all possible program branches. Of
course, due to combinatorial explosion, this quantity is intractable in practice. Its introduction here
is only instrumental, we do not use it in algorithms, we just need to access one of its element, for
example using a selection derivatives, obtained from AD. A point x satisfying 0 P Ds
f pxq is called a
selection critical point. We will often drop the index s and write Df “ Ds
f .

The two following results highlight crucial properties of Df in terms of optimization, they again rely
on the rigidity constraint of elementary functions.

Theorem 3 Let f P S with elementary selection ps, f1, . . . , fmq and Df be as in Deﬁnition 5. Then
Df is conservative for f , that is for all absolutely continuous curves γ : r0, 1s ÞÑ Rp, for almost all
t P r0, 1s, f ˝ γ is differentiable and

d
dt

f pγptqq “ xv, 9γptqy ,

@v P Df pγptqq.

The previous result generalizes Proposition 2 by allowing to integrate arbitrary selections along
absolutely continuous curves. This connects our work to the general setting of [12], note that Df has
a closed graph thanks to Proposition 6 in Appendix A.3.
In [40], the author considers the essential index set, for each x P Rp,

SEpxq “ ti P t1, . . . , mu , x P clpintpty, f pyq “ fipyquqqu Ă Spxq.

Considering Deﬁnition 5 with SEpxq instead of Ipxq leads to the Clarke subgradient, which can also
be deﬁned as

Bcf pxq “ convtd P Rp : Dxk P ∆f , xk Ñ x, ∇f pxkq Ñ du
where ∆f is the dense set of differentiability points of f . While Ipxq can be computed pointwise
(check ﬁnitely many equalities), it might be very hard to check membership in SEpxq without
restrictive qualiﬁcation conditions on programs [30].

(a) Set for all x P R, f1pxq “ 0, f2pxq “ x, spxq “ 1 for

Illustration with ReLU and sorting:
x ď 0 and spxq “ 2 for x ą 0. This is relu. In this case Df “ Brelu, the convex subgradient.
(b) Let F : Rp ÞÑ Rp to be the sorting function which associates to x a vector P x where P is
any permutation such that P x belongs to the set of vectors which values are sorted in descending
order coordinatewise. F obviously has an elementary selection and the construction which we have
proposed leads to

DF : x ÞÑ conv tP P ∆, P x “ F pxqu ,

where ∆ denotes the set of permutation matrices of size p ˆ p. Then D is a conservative mapping for
F and it actually corresponds to the Clarke Jacobian.

5.2 Convergence of gradient type algorithm and criticality of limit points

Optimization processes in learning are supposed to provide at least a critical point x of the loss, i.e. a
point satisfying 0 P Bcf pxq. When using AD one enlarges the deﬁnition of criticality into 0 P Df pxq
and artiﬁcial critical points appear, they satisfy 0 R Bcf pxq and 0 P Df pxq. Artiﬁcial critical points
could possibly trap the optimization process in strongly non-optimal situations, we thus have to
determine if they have an impact on learning phases.

We consider the problem

min
xPRp

Jpxq “

1
n

nÿ

i“1

fipxq

(7)

where fi : Rp ÞÑ R, fi P S, i “ 1, . . . , n. We consider the following algorithm, given x0 P Rp, a
sequence of positive step sizes pγkqkPN and a sequence of iid indices pIkqkPN taken uniformly in the
nonempty subsets of t0, . . . , nu,

xk`1 “ xk ´ γk

p∇fIk pxkq where fI “

1
|I|

ÿ

iPI

fi, I Ă t1, . . . , nu.

(8)

8

Note that as discussed in Section 4 selection derivatives can be computed by AD if fi are given by the
data of numerical programs as in (3), and could be far from usual notions of subgradients. Hence this
algorithm models explicitly the training of a nonsmooth deep network using existing backpropagation
ř
p∇fi is a selection gradient for J as stated in
n
implementations. Note that J P S and that 1{n
i“1
Proposition 4, denote by p∇J this quantity and DJ the corresponding set valued ﬁeld (Deﬁnition 5).
The following result illustrates that selection critical points are the only attractors for the recursion
and that generically such attractors are actually Clarke critical. The ﬁrst result stands on the theory
developed in [5]. The second parallels developments in [7] in the context of long run convergence.
The spurious behaviour illustrated in Figure 1 does not affect asymptotics, for typical initialization.

Theorem 4 (Convergence and insigniﬁcance of artefacts) Let for all k, γk “ cαk where c P
p0, 1s and αk “ op1{ log kq and K Ă Rp be open. Assume that for all c P p0, 1s and all x0 P K the
sequence in (8) is bounded almost surely.

• For all x0 P K, almost surely, Jpxkq converges as k tends to inﬁnity and all accumulation

points, ¯x, of pxkqkPN are selection critical points: 0 P DJ p¯xq.

• For almost all c P p0, 1s, almost all x0 P K, and almost surely, any accumulation point, ¯x,

of pxkqkPN is Clarke critical: 0 P BcJp¯xq.

6 Conclusion

The current work departs from existing approaches to nonsmooth algorithmic differentiation in a
fundamental way. We propose to study the backward mode of AD, as implemented in machine
learning, without any modiﬁcation. Our theoretical results model thus AD “as is”, and our focus
is precisely on its unpredictable behavior in a nonsmooth context, addressing an issue which is
ubiquitous in machine learning. Our main contribution was to prove that, in a stochastic optimization
context, this spurious behavior is essentially harmless from a theoretical point of view, providing
justiﬁcations for the use of AD outside of its original domain of validity in machine learning.

We achieve our goal by modeling sharply common machine learning functions and their differentiation
using selection derivatives, a known concept, which models the way AD differentiates nonsmooth
programs. We restrict it to certain classes of elementary functions, opening the possibility to use
powerful geometric techniques.

Further questions include convergence rates and complexity issues, hardly tackled at this day, let us
mention the attempt of [43]. Our theory is limited to continuous functions and an interesting venue
is to extend it to discontinuous functions, in view of treating ranking operations [10] ubiquitous in
recommendation systems, or more generally differentiating through an argmax [6].

Broader impact

One of the goals of the paper is to raise awareness about an important issue of in the training of ML
methods: the spuriousness of AD. To address adequately this issue, we think it is necessary to include
algorithmic differentiation explicitly in the study of optimization algorithms, a point of view which is
largely ignored by today’s machine learning community.

Acknowledgments and Disclosure of Funding

The authors acknowledge the support of ANR-3IA Artiﬁcial and Natural Intelligence Toulouse
Institute, Air Force Ofﬁce of Scientiﬁc Research, Air Force Material Command, USAF, under grant
numbers FA9550-19-1-7026, FA9550-18-1-0226, and ANR MasDol. J. Bolte acknowledges the
support of ANR Chess, grant ANR-17-EURE-0010 and ANR OMS. The authors would like to thank
anonymous referees for careful reading of this work and useful suggestions. The authors would like
to thank N. Asher and S. Gerchinovitz for useful discussions. We also warmly thank J. Malick who
triggered this research.

9

References

[1] Abadi M., Barham P., Chen J., Chen Z., Davis A., Dean J., Devin M., Ghemawat S.,
Irving G., Isard M., Kudlur M., Levenberg J., Monga R., Moore S., Murray D., Steiner B.,
Tucker P., Vasudevan V., Warden P., Wicke M., Yu Y. and Zheng X. (2016). Tensorﬂow: A
system for large-scale machine learning. In Symposium on Operating Systems Design and
Implementation.

[2] Barton, P. I., Khan, K. A., Stechlinski, P., Watson, H. A. (2018). Computationally relevant
generalized derivatives: theory, evaluation and applications. Optimization Methods and
Software, 33(4-6), 1030-1072.

[3] Baydin A., Pearlmutter B., Radul A. and Siskind J. (2018). Automatic differentiation in

machine learning: a survey. Journal of machine learning research, 18(153).

[4] Bena¨ım, M. (1999). Dynamics of stochastic approximation algorithms. In S´eminaire de

probabilit´es XXXIII (pp. 1-68). Springer, Berlin, Heidelberg.

[5] Bena¨ım, M., Hofbauer, J., Sorin, S. (2005). Stochastic approximations and differential

inclusions. SIAM Journal on Control and Optimization, 44(1), 328-348.

[6] Berthet, Q., Blondel, M., Teboul, O., Cuturi, M., Vert, J. P., Bach, F. (2020). Learning with

differentiable perturbed optimizers. arXiv preprint arXiv:2002.08676.

[7] Bianchi, P., Hachem, W., and Schechtman, S. (2020). Convergence of constant
step stochastic gradient descent for non-smooth non-convex functions. arXiv preprint
arXiv:2005.08513.

[8] Bischof, C., Carle, A., Corliss, G., Griewank, A., Hovland, P. (1992). ADIFOR-generating

derivative codes from Fortran programs. Scientiﬁc Programming, 1(1), 11-29.

[9] Bischof, C., Khademi, P., Mauer, A., Carle, A. (1996). ADIFOR 2.0: Automatic differ-
entiation of Fortran 77 programs. IEEE Computational Science and Engineering, 3(3),
18-32.

[10] Blondel, M., Teboul, O., Berthet, Q., Djolonga, J. (2020). Fast Differentiable Sorting and

Ranking. arXiv preprint arXiv:2002.08871.

[11] Bolte, J., Daniilidis, A., Lewis, A., Shiota, M. (2007). Clarke subgradients of stratiﬁable

functions. SIAM Journal on Optimization, 18(2), 556-572.

[12] Bolte, J. and Pauwels, E. (2020). Conservative set valued ﬁelds, automatic differentiation,

stochastic gradient methods and deep learning. Mathematical Programming.

[13] Borkar, V. (2009). Stochastic approximation: a dynamical systems viewpoint (Vol. 48).

Springer.

[14] Bottou L. and Bousquet O. (2008). The tradeoffs of large scale learning. In Advances in

neural information processing systems (pp. 161-168).

[15] Bottou L., Curtis F. E. and Nocedal J. (2018). Optimization methods for large-scale

machine learning. Siam Review, 60(2), 223-311.

[16] Castera C., Bolte J., F´evotte C., Pauwels E. (2019). An Inertial Newton Algorithm for

Deep Learning. arXiv preprint arXiv:1905.12278.

[17] Clarke F. H. (1983). Optimization and nonsmooth analysis. Siam.
[18] Chizat, L., and Bach, F. (2018). On the global convergence of gradient descent for over-
parameterized models using optimal transport. In Advances in neural information process-
ing systems, 3036-3046.

[19] Coste M., An introduction to o-minimal geometry. RAAG notes, Institut de Recherche

Math´ematique de Rennes, 81 pages, November 1999.

[20] Davis, D., Drusvyatskiy, D., Kakade, S., Lee, J. D. (2018). Stochastic subgradient method

converges on tame functions. Foundations of Computational Mathematics.

[21] van den Dries L. and Miller C. (1996). Geometric categories and o-minimal structures.

Duke Math. J, 84(2), 497-540.

[22] Griewank, A., Juedes, D., Utke, J. (1996). Algorithm 755: ADOL-C: a package for
the automatic differentiation of algorithms written in C/C++. ACM Transactions on
Mathematical Software (TOMS), 22(2), 131-167.

10

[23] Griewank, A., Walther, A. (2008). Evaluating derivatives: principles and techniques of

algorithmic differentiation (Vol. 105). SIAM.

[24] Griewank, A. (2012). Who invented the reverse mode of differentiation. Documenta

Mathematica, Extra Volume ISMP, 389-400.

[25] Griewank A. (2013). On stable piecewise linearization and generalized algorithmic differ-

entiation. Optimization Methods and Software, 28(6), 1139-1178.

[26] Griewank A., Walther A., Fiege S. and Bosse T. (2016). On Lipschitz optimization based
on gray-box piecewise linearization. Mathematical Programming, 158(1-2), 383-415.
[27] Griewank, A., Walther, A. (2016). First-and second-order optimality conditions for piece-

wise smooth objective functions. Optimization Methods and Software, 31(5), 904-930.

[28] Griewank, A., Rojas, A. (2019, September). Treating artiﬁcial neural net training as a
nonsmooth global optimization problem. In International Conference on Machine Learning,
Optimization, and Data Science (pp. 759-770). Springer, Cham.

[29] Griewank, A., Walther, A. (2020). Beyond the Oracle: Opportunities of Piecewise Differ-

entiation. In Numerical Nonsmooth Optimization (pp. 331-361). Springer, Cham.
[30] Kakade, S. M. and Lee, J. D. (2018). Provably correct automatic sub-differentiation for
qualiﬁed programs. In Advances in Neural Information Processing Systems (pp. 7125-
7135).

[31] Kushner H. and Yin, G. G. (2003). Stochastic approximation and recursive algorithms and

applications (Vol. 35). Springer Science & Business Media.

[32] LeCun Y., Bengio Y., Hinton, G. (2015). Deep learning. Nature, 521(7553).
[33] Lee, W., Yu, H., Rival, X., Yang, H. (2020). On Correctness of Automatic Differentiation

for Non-Differentiable Functions. arXiv preprint arXiv:2006.06903.

[34] Lehman, E., Leighton, T., and Meyer, A. R. (2010). Mathematics for computer science.

Technical report, 2006. Lecture notes.

[35] Moulines E. and Bach, F. (2011). Non-asymptotic analysis of stochastic approximation
algorithms for machine learning. In Advances in Neural Information Processing Systems
(pp. 451-459).

[36] Paszke A., Gross S., Chintala S., Chanan G., Yang E., DeVito Z., Lin Z., Desmaison A.,
Antiga L. and Lerer A. (2017). Automatic differentiation in pytorch. In NIPS workshops.
[37] Robbins H. and Monro, S. (1951). A stochastic approximation method. The annals of

mathematical statistics, 400-407.

[38] Rockafellar, R. T., Wets, R. J. B. (1998). Variational analysis. Springer.
[39] Rumelhart E., Hinton E., Williams J. (1986). Learning representations by back-propagating

errors. Nature 323:533-536.

[40] Scholtes, S. (2012). Introduction to piecewise differentiable equations. Springer Science &

Business Media.

[41] Speelpenning, B. (1980). Compiling fast partial derivatives of functions given by algo-
rithms (No. COO-2383-0063; UILU-ENG-80-1702; UIUCDCS-R-80-1002). Illinois Univ.,
Urbana (USA). Dept. of Computer Science.

[42] Wilkie, A. J. (1999). A theorem of the complement and some new o-minimal structures.

Selecta Mathematica, 5(4), 397-421.

[43] Zhang, J., Lin, H., Sra, S., Jadbabaie, A. (2020). On Complexity of Finding Stationary

Points of Nonsmooth Nonconvex Functions. arXiv preprint arXiv:2002.04130.

11

This is the appendix for “A mathematical model for automatic differentiation in machine learning”.

A A more comprehensive discussion and auxiliary results

A.1 Related work and contribution

The use of backward mode of algorithmic differentiation (AD) for neural network training expanded in
the 80’s, the most cited reference being [39]. However the theory applies to much more optimization
problems, see for example [24]. Indeed, numerical libraries implementing the backward mode of AD
were already available in the 90’s for FORTRAN code [8, 9] or C/C++ code [22], 30 years before the
emergence of python libraries. These early implementation could differentiate virtually any code,
but their domain of validity, i.e., the setting for which one could predict what the output would be,
was restricted to differentiable functions evaluated on their (open) domain of differentiability.

This was well known to the AD community, see for example [23], and exploring further the domain
of validity of AD, beyond mere differentiability, was already a vivid problem.

Let us mention [23] who used notions such as ﬁnite selection, “isolated criticalities”, stable domain
or regular arcs, and argued that “functions given by evaluation procedures are almost everywhere real
analytic or stably undeﬁned” where “undeﬁned” meant that a nonsmooth elementary function is used
in the evaluation process. For piecewise smooth functions which nonsmoothness can be described
using the absolute value function (abs-normal form), [25] developped a piecewis linearisation
formalism and local approximation related to AD, [26] proposed an AD based bundle type method.
These developments are based on the notion of piecewise smooth functions [40] which we use in
this work. More recently, [28] applied these techniques to single layer neural network training and
[29] proposed to avoid the usage of subgradient “oracles” in nonsmooth analysis as they are not
available in practice. In a similar vein, let us mention [2] study lexicographic derivatives, a notion
of directional derivatives which satisfy a chain rule making them compatible by forward mode AD,
and [43] who use directional derivatives in the context of local sampling stochastic approximation
algorithms for machine learning.

Constraint qualiﬁcation is known in nonsmooth analysis to ensure favorable behavior of chain
rules of differential calculus for nonsmooth objects (see [38]). These already appeared in the
context of piecewise smooth functions of Scholtes with the notion of “essential selections”. Such an
approach was used in [30] to propose an AD algorithm for subgradient computation under constrant
qualiﬁcation. Similarly [27] study ﬁrst and second order optimality, in relation to AD using constraint
qualiﬁcation.

The current work departs from all these approaches in a fundamental way. We propose to study back-
ward mode of AD, as implemented for nonsmooth functions by standard software (e.g. TensorFlow,
PyTorch), without any modiﬁcation, addition of operations or hypotheses. Our theoretical results
model AD as implemented in current machine learning libraries. Contrary to previous works, our
focus is precisely on the unpredictable behavior of AD in nonsmooth context. Our main contribution
is to show that in a stochastic optimization context, this spurious behavior is essentially harmless
from a theoretical point of view, providing justiﬁcations for the use of AD outside of its original
domain of validity in machine learning.

At the time this paper was accepted, we learnt about a paper proposing an analysis close to ours
[33]. The authors show that AD applied to programs involving piecewise analytic continuous
functions, under analytic partitions, compute gradients almost everywhere. This is the counterpart of
Proposition 3, replacing log-exp elementary function in Deﬁnitions 1 and 2, by analytic functions.

A.2

Implementation of relu

The implementation of the relu function used in Figure 1 is given by the function tf.nn.relu in
Tensorﬂow software library [1]. This implementation corresponds to the selection function described
in Section 2 and the same result may be obtained by an explicit implementation of this branching
selection as illustrated in the following ﬁgure

12

One can imagine an equivalent implementation of relu with a slightly different branching involving
a strict inequality, that would correspond to an equivalent implementation of the same function, but
the computed derivative at 0 is different due to the implementation

A.3 Auxiliary results and remarks

Remark 1 (Elementary piecewise differentiable functions)
(a) The building blocks in the construction of S in Deﬁnition 3 could be modiﬁed and adapted to
other needs. Besides, the results we present in this article would remain true if we added real analytic
functions restricted to compact sets.
(b) Note also that in Deﬁnition 3, functions are actually real analytic on their (open) domain of
deﬁnition. Yet their extension might not be analytic, as for instance the function f : x ‰ 0 Ñ
expp´1{x2q extended by f p0q “ 0.
(c) The construction of elementary piecewise functions in Deﬁnition 3, does not coincide in general
with some natural minimal o-minimal, but are contained in a larger such structure. For instance, when
the basic bricks are polynomial functions, we obtain the ﬁeld of rational functions which differs from
the set of semi-algebraic functions.

Proposition 6 (Df has a closed graph) As k Ñ 8, assume that xk Ñ ¯x P Rp and vk P Df pxkq,
vk Ñ ¯v. Then ¯v P Dp¯xq.

B Proofs

Proof of Theorem 1: Recall the operator is denoted by BA. Fix a function f , by point (a), the
operator BAf should contain
"

Rp Ñ Rp
x Ñ tApP qpxq : FpP q “ f, P P Pu
Let us show that the graph of the above is Rp ˆ Rp. Assume p “ 1 for simplicity. For real numbers
r, s, consider the functions fr,s “ f ` r zerop¨ ´ sq which coincide with f but whose form induces
programs Pr,s of f . These satisfy FpPr,sq “ f and ApPr,sqpsq Q Apf qpsq ` r. Since r is arbitrary,
BAf psq “ Rp and since s is arbitrary, we actually have

graph BAf “ Rp ˆ Rp.

Since f is arbitrary, we have shown that BA is trivial.

l

Proof of Proposition 2: The proposition is a consequence of Theorem 3 and (11) but it admits a
more elementary proof which we detail here. Fix x, y P Rp. Let us admit the following claim –whose
independent proof is given in Section C.

Claim 1 There exists a ﬁnite set of numbers 0 “ a0 ă a1 ă . . . ă aN “ 1, such that for all
i P 0, . . . N ´ 1, the function t ÞÑ spx ` tpy ´ xqq is constant.

13

Fix i P 0 . . . , N ´ 1, and j P 1 . . . m such that f “ fj on px ` aipy ´ xq, x ` ai`1py ´ xqq. Since
fj P Ep, it is C 1 and we have by the fundamental theorem of integral calculus

ż

ai`1

f px ` ai`1py ´ xqq ´ f px ` aipy ´ xqq “

ai
ż
ai`1

“

ai

x∇fjpx ` tpy ´ xqq, y ´ xy dt
A
p∇f px ` tpy ´ xqq, y ´ x

E

dt.

The conclusion follows because

N ´1ÿ

f pyq ´ f pxq “

f px ` ai`1py ´ xqq ´ f px ` aipy ´ xqq

i“0
N ´1ÿ

ż

ai`1

A
p∇f px ` tpy ´ xqq, y ´ x

E

dt

ai

i“0
ż
1

A
p∇f px ` tpy ´ xqq, y ´ x

E

dt.

“

“

0

l
Proof of Proposition 3: Constructs the sets Ui by considering sets Vj “ tx P Rp, spxq “ ju,
j “ 1 . . . m, the proof of the following claim is postponed to Section C.

Claim 2 The boundary of each Vj has zero measure and cl

`

Ym

i“jintpVjq

˘

“ Rp.

Hence, we may deﬁne U1, . . . , UN by keeping only those sets with nonempty interior and take their
closure. On each set Ui, f is identical to fk for some k and the result follows.
l

Lemma 2 Let t P I be an elementary index on Rp2 and F : Rp1 ÞÑ Rp2 with each coordinate in E,
then t ˝ F is an elementary index on Rp1.

Proof : Fix an arbitrary integer i in the image of t, by Deﬁnition 2, there exists elementary functions
h1, . . . , hJ , J P N on Rp2 such that tpyq “ i if and only if y P Ki :“ tz P Rp2, hjpzq ˛j 0, j “
1, . . . Ju where ˛j is an equality or inequality sign depending on j. Then tpF pxqq “ i if and only if
F pxq P Ki which is equivalent to say that x P ˜Ki :“ tx P Rp1, hjpF pxqq ˛j 0, j “ 1, . . . Ju. By
Deﬁnition 1, hj ˝ F is an elementary function for j “ 1, . . . , J and i was an arbitrary integer, this
shows that we have an elementary index.
l
Proof of Proposition 1: Let F : Rp1 ÞÑ Rp2 such that each of its coordinate fi, i “ 1 . . . p2, is in
S and g : Rp2 ÞÑ R, g P S. We establish that g ˝ F is an elementary selection, the other cases are
similar. We may consider all possible intersections of constant index domains across all coordinates
of F in t1, . . . , p2u. We obtain ps, F1, . . . , Fmq, an elementary selection for F (each Fi : Rp1 ÞÑ Rp2
has coordinates in E) . Consider g P S with elementary selection pt, g1, . . . , glq. The composition
g ˝ F may be written as

gpF pxqq “ gtpF pxqqpF pxqq “ gtpFspxqpxqqpFspxqpxqq.

For each i “ 1 . . . , m and j “ 1, . . . , l, consider the set

Uij “ tx P Rp, spxq “ i, tpFipxqq “ ju .
Fix pi, jq in t1, . . . , mu ˆ t1, . . . , lu, by Lemma 2, t ˝ Fi is an elementary index on Rp1. Hence Uij
is the solution set of ﬁnitely many equalities and inequalities involving functions in E. We associate
to the bi-index pi, jq the corresponding set Uij and the function gjpFipxqq P E. Note that we assumed
that the composition is well deﬁned. Identifying each pair pi, jq with a number in t1, . . . , nmu, we
obtain an elementary selection for g ˝ F and hence g ˝ F P S.
l

Proof of Proposition 4: The derivation formula follows from the proof argument of Proposition 1,
for each pair pi, jq, the function gj ˝ Fi is the composition of two C 1 functions and its gradient is
given by JFi ˆ ∇gj ˝ Fi on Uij. By construction of Uij and deﬁnition of the selection derivative,
this corresponds to (5) on Uij and the result follows.
l

14

Proof of Lemma 1: We actually prove a slightly stronger result, namely for each i P tp`1, . . . , m´
1u

˘

`

˘

`

˘

`

Pi

I ´ ei`1eT

i`1 ` di`1eT

i`1

. . .

I ´ emeT

m ` dmeT
m

“ Pi

I ` di`1eT

i`1

. . .

I ` dmeT
m

`

˘

(9)

We argue by exhaustion from i “ m ´ 1 downward to i “ p, which is the result of interest. If
i “ m ´ 1, we indeed have

`

˘

`

˘

Pm´1

I ´ emeT

m ` dmeT
m

“ Pm´1

I ` dmeT
m

since Pm´1emeT
m “ 0. Now assume that (9) holds true for an index i within tp ` 1, . . . , m ´ 1u,
then we have
`

˘

`

˘

Pi´1
“ Pi´1
“ Pi´1
“ Pi´1
“ Pi´1

`

`

`

`

˘ `

I ´ emeT

m ` dmeT
m
i`1 ` di`1eT

I ´ eieT
I ´ eieT
I ´ eieT
I ` dieT
i
I ` dieT
i

i ` dieT
. . .
i
I ´ ei`1eT
i ` dieT
i
i ` dieT
Pi
i
I ` di`1eT
Pi
i`1
˘
˘ `
I ` di`1eT

i`1
i`1 ` di`1eT
I ´ ei`1eT
i`1
˘
`
˘
I ` dmeT
. . .
m
˘
`
,

I ` dmeT
m

. . .

`

`

˘

˘

˘

`

. . .
˘

`

I ´ emeT

m ` dmeT
m

. . .

I ´ emeT

m ` dmeT
m

˘

˘

i`1
where step 1 is expanding the product, step 2 is because Pi´1Pi “ Pi´1 and eT
i , step 3
combines the fact that Pi´1ei “ 0 and (9) which we assumed to be true, the last step uses again the
fact that Pi´1Pi “ Pi´1 and eT
l
Proof of Proposition 6: Consider the sequence sk “ Spxkq, by taking a subsequence we may as-
sume that sk is constant, say equal to t1, . . . , ru. Hence for all k, vk P conv pt∇fipxkq, i “ 1, . . . ruq
and f pxkq “ fipxkq, i “ 1, . . . , r. Passing to the limit, we have f p¯xq “ fip¯xq, i “ 1, . . . , r and
hence t1, . . . , ru P Spxq. Furthermore, ¯v P conv pt∇fip¯xq, i “ 1, . . . ruq Ă Df p¯xq.
l

i . Hence the result holds by exhaustion.

i Pi “ eT

i Pi “ eT

C o-minimal structures, deﬁnability and conservative ﬁelds

C.1 pR, expq-deﬁnability

We recall here the results of geometry that we use in the present work. Some references on this topic
are [19, 21].
An o-minimal structure on pR, `, ¨q is a collection of sets O “ pOpqpPN where each Op is itself a
family of subsets of Rp, such that for each p P N:

(i) Op is stable by complementation, ﬁnite union, ﬁnite intersection and contains Rp.
(ii) if A belongs to Op, then both A ˆ R and R ˆ A belong to Op`1;
(iii) if π : Rp`1 Ñ Rp is the canonical projection onto Rp then, for any A P Op`1, the set πpAq

belongs to Op;

(iv) Op contains the family of real algebraic subsets of Rp, that is, every set of the form

where g : Rp Ñ R is a polynomial function;

(v) the elements of O1 are exactly the ﬁnite unions of intervals.

tx P Rp | gpxq “ 0u

A subset of Rp which belongs to an o-minimal structure O is said to be deﬁnable in O. A function is
deﬁnable in O whenever its graph is deﬁnable in O). A set valued mapping (or a function) is said to
be deﬁnable in O whenever its graph is deﬁnable in O. The terminology tame refers to deﬁnability in
an o-minimal structure without specifying which structure.

The simplest o-minimal structure is given by the class of real semialgebraic objects. Recall that a set
A Ă Rp is called semialgebraic if it is a ﬁnite union of sets of the form

kč

i“1

tx P Rp | gipxq ă 0, hipxq “ 0u

15

where the functions gi, hi : Rp Ñ R are real polynomial functions and k ě 1. The key tool to show
that these sets form an o-minimal structure is Tarski-Seidenberg principle which ensures that (iii)
holds true.

According to [42], there is an o-minimal structure which contains all semialgebraic sets and the
graph of the exponential function, we ﬁx this o-minimal structure and call it O. As a consequence,
all functions which can be described by a ﬁnite compositional expression involving polynomials,
quotients, exponential and logarithms are deﬁnable in O. In particular any function f P S is deﬁnable
in O, which opens the use of powerful geometric tools [19, 21] for functions in S. From now on, we
call an object deﬁnable if it is deﬁnable in O.

As detailed in [19] the following holds true

Proposition 7 (Quantiﬁer elimination) Any ﬁrst order formula (quantiﬁcation on variables only)
involving deﬁnable functions and deﬁnable sets describes a deﬁnable set.

This allows to prove Claim 1

Proof of Claim 1: The function t ÞÑ spx ` tpy ´ xqq is deﬁnable and has values in t1, . . . , mu. For
each j P t1, . . . , mu, the set Sj “ tt P r0, 1s, spx ` tpy ´ xqq “ ju is deﬁnable, and by (v), it is a
ﬁnite union of intervals. For each j consider only the endpoints of those intervals with nonempty
interior, this provides the desired partition.
l

C.2 Properties of deﬁnable sets

The tangent space at a point x of a manifold M is denoted by TxM . Given a submanifold6 M of
a ﬁnite dimensional Riemannian manifold, it is endowed by the Riemanninan structure inherited
from the ambient space. Given f : Rp Ñ R and M Ă Rp a differentiable submanifold on which f is
differentiable, we denote by gradM f its Riemannian gradient or even, when no confusion is possible,
grad f .
A C r stratiﬁcation of a (sub)manifold M (of Rp) is a partition S “ pM1, . . . , Mmq of M into C r
manifolds having the property that cl Mi X Mj ‰ H implies that Mj is entirely contained in the
boundary of Mi whenever i ‰ j. Assume that a function f : M Ñ R is given and that M is stratiﬁed
into manifolds on which f is differentiable. For x in M , we denote by Mx the strata containing x
and we simply write grad f pxq for the gradient of f with respect to Mx.

Stratiﬁcations can have many properties, we refer to [21] and references therein for an account on this
question and in particular for more on the idea of a Whitney stratiﬁcation that we will use repeatedly.
We pertain here to one basic deﬁnition: a C r-stratiﬁcation S “ pMiqiPI of a manifold M has the
Whitney-(a) property, if for each x P cl Mi X Mj (with i ‰ j) and for each sequence pxkqkPN Ă Mi
we have:

xk “ x

,
/.

Txk Mi “ T

/-

ùñ TxMj Ă T

lim
kÑ8

lim
kÑ8

where the second limit is to be understood in the Grassmanian, i.e., “directional”, sense. In the
sequel we shall use the term Whitney stratiﬁcation to refer to a C 1-stratiﬁcation with the Whitney-(a)
property. The following can be found for example in [21].

Theorem 5 (Whitney stratiﬁcation) Let A1, . . . , Ak be deﬁnable subsets of Rp, then there exists a
deﬁnable Whitney stratiﬁcation pMiqiPI compatible with A1, . . . , Ak, i.e. such that for each i P I,
there is t P t1, . . . ku, such that Mi Ă At.

This allows for example to prove Claim 2
Proof of Claim 2: The sets V1, . . . , Vm form a deﬁnable partition of Rp. Consider a Whitney
stratiﬁcation of Rp, pMiqiPI compatible with the closure of V1, . . . , Vm. The boundary of each Vi is a
ﬁnite union of strata of dimension strictly smaller than p and hence has measure zero. The remaining
strata (open of maximal dimension) have to be dense in Rp since we started with a partition.
l

6We only consider embedded submanifolds

16

C.3 Variational stratiﬁcation and projection formulas

Deﬁnition 6 (Variational stratiﬁcation) Let f : Rp Ñ R, be locally Lipschitz continuous, let
D : Rp Ñ Rp be a set valued map and let r ě 1. We say that the couple pf, Dq has a C r variational
stratiﬁcation if there exists a C r Whitney stratiﬁcation S “ pMiqiPI of Rp, such that f is C r on each
stratum and for all x P Rp,

ProjTMx pxqDpxq “ tgrad f pxqu ,

(10)

where grad f pxq is the gradient of f restricted to the active strata Mx containing x.

The equations (10) are called projection formulas and are motivated by Corollary 9 in [11] which
states that Clarke subgradients of deﬁnable functions have projection formulas.

Let us recall the deﬁnition of conservative set-valued mappings from [12] and one of its characteriza-
tion.

Deﬁnition 7 (Conservative set-valued mappings) Let f be a Lipschitz continuous function. A set
valued vector ﬁeld D is called conservative if for any absolutely continuous path γ : r0, 1s ÞÑ R, we
have

ż

1

ż

1

f pγp1qq ´ f pγp0qq “

min
vPDpγptqq

0

xv, 9γptqy dt “

max
vPDpγptqq

0

xv, 9γptqy dt.

(11)

Equivalently D is conservative for f , if for all absolutely continuous curves γ : r0, 1s ÞÑ Rp, for
almost all t P r0, 1s, f ˝ γ is differentiable and

d
dt

f pγptqq “ xv, 9γptqy ,

@v P Dpγptqq.

The following combines other results from [12], where one implication is essentially due to [20]
based on [11].

Theorem 6 (Characterization of conservativity) Let D : Rp Ñ Rp be a deﬁnable, nonempty com-
pact valued, graph closed set valued ﬁeld and f : Rp ÞÑ R be a deﬁnable locally Lipschitz function.
Then the following are equivalent

• D is conservative for f .

• For any r ě 1, pf, Dq admit a C r variational stratiﬁcation.

This result allows to prove the following
Proof of Theorem 3: We prove that there is a C 1 projection formula (see Theorem 6). For each
I Ă t1, . . . , mu, set VI “ tx P Rp, Spxq “ Iu. On each set VI , f pxq “ fipxq for all i P I. These
sets are deﬁnable, hence, there is a deﬁnable Whitney stratiﬁcation of Rp which is compatible with
them (Theorem 5). For any C 1 manifold M in the stratiﬁcation there is an index set I Ă t1, . . . , mu
such that for all i P I and all x P M , f pxq “ fipxq and Spxq “ I. Since each fi, i P I is C 1 and
they agree on M , they represent the same function when restricted to M . Hence they have the same
differential on M and since they are all globally C 1 this agrees with the projection of their gradient
on the tangent space of M . Hence the projection of Df pxq to the tangent space to M at x is single
valued and corresponds to the derivative of f restricted to M . This is sufﬁcient to conclude as this is
precisely the variational stratiﬁcation required by Theorem 6.
l

D Convergence to selection critical points

Proof of Theorem 4, ﬁrst part: We use here the results on conservative ﬁelds developed in [12].
To prove the theorem it sufﬁces to establish that:

• DJ is a conservative ﬁeld for J
• the number of DJ critical values are ﬁnite.

17

The ﬁrst point is Theorem 6 while the second one is the consequence of the latter and the deﬁnability
of the couple f, Df , see Proposition 8 (ii). To conclude it sufﬁces to apply the convergence results in
[12, Theorem 9].
l

Proof of Theorem 4, second part: This result is a consequence of the more general Theorem 7
established in Section E. Let F be the ﬁnite set given in Theorem 7, the set

tc P p0, 1s, Dk P N, cγk P F u,
is countable, and hence has zero measure. So for almost all c P p0, 1s, tcγkukPN does not intersect
F . Using Theorem 7, there is a zero measure set N such that any initialization outside N provides
almost surely a subgradient sequence. By hypothesis, for almost every x0 P KzN , the sequence is
bounded almost surely and the result follows from Theorem 7.
l

E Artiﬁcial critical points

Being given a Lipschitz continuous function on Rp and a conservative ﬁeld D, one has two types of
D-critical points:

• Clarke critical points: Bcf pxq Q 0, we denote the set of these points by critcf
• Artiﬁcial critical points Bcf pxq S 0 and Dpxq Q 0, we denote this set by critaf

Critical values are deﬁned accordingly as images of critical points.

Proposition 8 (Artiﬁcial critical points) Assume f : Rp Ñ R and D : Rp Ñ Rp are deﬁnable in
a common o-minimal structure. The connected components Ci of critaf , which are in ﬁnite number,
satisfy

(i) dim Ci ă p

(ii) f pCiq is a singleton, and as a consequence the D critical values of f are in ﬁnite number,
(iii) critaf does not contain local minimum (nor local maximum)

Proof : By deﬁnability of critaf , the number of connected components is ﬁnite.

If Ci had full dimension it would contain a non trivial ball on which f should be constant by the
integral property. This would in turn imply that the points in the ball would also be local minimum
and thus Clarke critical, which is impossible.

To see that the critical values are in ﬁnite number it sufﬁces to evoke the fact that Clarke critical
values are ﬁnite [11] and use that artiﬁcial critical values are in ﬁnite number.
By deﬁnability the connected components are arcwise-connected with piecewise C 1 paths. Using the
integral property this shows f is constant on Ci.

(iii) is obvious since local minimum or maximum are Clarke critical.

l

As explained in the introduction, artiﬁcial critical points are “computing artefacts”, whence their
names. For algorithmic differentiation the “gradient” provided by a program is zero while the point
might even be a smooth non critical point. We consider the setting of the mini-batch algorithm of the
last section.

Theorem 7 Assume that each f1, . . . , fn belongs to S. There exists a ﬁnite subset of steps F Ă
p0, `8q and a zero measure meager subset N of Rp, such that for any positive sequence γk “
op1{ log kq avoiding values in F , and any almost surely bounded sequence with initial condition in
RpzN , we have

• Jpxkq converges towards a Clarke critical value almost surely,

• the cluster points of xk are Clarke critical point almost surely.

Proof : The proof is twofold. We ﬁrst prove that the set of initial conditions leading to an artiﬁcial
critical point or more generally to a non differentiability point within a ﬁnite time is “small”. We then
use this fact to conclude.

18

Claim 3 Let g : Rp Ñ R be a deﬁnable differentiable function. Set, for λ ą 0,

Φλ “ λId ´ ∇g,

where Id denotes the identity. There exists a ﬁnite set F in p0, `8q such that,

@λ P p0, `8qzF, @Z Ă Rp deﬁnable , dim Z ă p ñ dim Φ´1

λ pZq ă p.

(12)

Proof of the claim. Denote by L the set of points where g is twice differentiable so that L is dense
and deﬁnable. Denote by λ1, . . . , λp : L Ñ R a representation of the eigenvalues of ∇2g. Reﬁne L
to be contained in the common domain of differentiability for each λi, L remains open and dense. By
the deﬁnable Sard’s theorem the critical values of each function λi is ﬁnite, so that the set of all these
values which we denote by F is itself ﬁnite.

Take a positive real λ R F and consider the set

Kλ :“ tx P L : Φ1

λpxq “ λId ´ ∇2gpxq is not invertibleu.

dź

By diagonalization, we see that the determinant of Φ1

λpxq is

pλ ´ λipxqq for any x, thence

i“1

mď

Kλ Ă

tx P L, λipxq “ λu.

i“1

Since λ is a regular value for each λi the previous set is a ﬁnite union of manifolds of dimension p ´ 1,
see e.g., [19]. This implies that the set RpzKλ “ tx P L : Φ1
λpxq is invertible u is dense. Using the
above, we deduce that there exists ﬁnitely many open connected subsets U1, . . . , Ur Ă L of RpzKλ
such that U1 Y . . . Y Ur is dense in L and thus in Rp. Take now Z Ă Rp deﬁnable with dim Z ă p.
Assume towards a contradiction that there exists a nonempty open ball B in Φ´1
λ pZq. In that case B
must have a nonempty intersection with some Ui0. The set ΦλpB X Ui0 q is open because Φλ is a
diffeomorphism on Ui on its image. Since on the other hand we have ΦλpB X Ui0q Ă Z, we have a
contradiction and the claim is proved.
l

For each I Ă t1, . . . , nu, we denote by fI,1, . . . , fI,mI the bricks attached to fI where mI ě 1.
Denote by Sing the set of points on which at least one fI is non differentiable and C the set of points
for which p∇fI ‰ ∇fI for at least one I. By Proposition 3 and deﬁnability, Sing and C are ﬁnite
unions of manifolds of dimension at most p ´ 1.
Set Φk
I,j “ Id ´ γk∇fI,j, with I Ă t1, . . . , mu, j P t1, . . . , mI u and Id denotes the identity.
Applying Claim 3, we can ﬁnd a ﬁnite set F for which γk R F implies that each Φk
I,j has the
property (12). Indeed, for each I Ă t1, . . . , mu, j P t1, . . . , mI u, there is FI,j Ă R ﬁnite such
that fI,j has property (12). Since the subsets I are in ﬁnite number and each mI is ﬁnite, the set
jPt1,...,mI u FI,j, is also ﬁnite. For each k P N, I Ă t1, . . . , mu, j P t1, . . . , mI u.
F “
Remark that if γk R F then Φk

I,j has property (12).
For k ď k0 ﬁxed, let us consider the ﬁnite set of deﬁnable mappings deﬁned by

IĂt1,...,mu

Ť

Ť

+

Ψk0 :“

: k ď k0, Ij Ă t1, . . . , nu, ij P t1, . . . , mIj u

.

#

kź

j“1

Φj

Ij ,ij

We now assume that γk R F, @k ě 0, so that each mapping in Ψk0 has the property (12) and

Nk0 :“ tx P Rp : Dk ď k0, DΦ P Ψkpxq P C Y Singu
These are initial conditions in U leading to an artiﬁcial critical or a non-differentiability point within
U before time k0.

We can also write

Nk0 Ă

ď

ΦPΨk0

Φ´1 pC Y Singq .

From stratiﬁcation arguments we know that Sing has a dimension lower than p ´ 1. On the
other hand, C has dimension strictly lower than p by Proposition 3. Claim 3 applies and yields

19

dim Φ´1 pC Y Singq ă p for all Φ P Φk0. As a consequence Nk0 is closed with nonempty interior
and so does N :“ YkPNNk by Baire’s theorem. Similarly N has zero measure as a countable union
of zero measure sets.

This proves that any sequence with initial condition out of N must remain in the zone of differentia-
bility of J as well as all fI . In particular if I is taken uniformly at random among possible subsets,
p∇Jpxq “ ∇Jpxq “ BcJpxq, so that these speciﬁc sequences
for all x R N , we have EI r
can also be seen as stochastic subgradient sequences for J. To be more speciﬁc, the sequence xk can
be seen as one of the sequence generated by the algorithm

p∇fI pxqs “

yk`1 P yk ´ γkBcJpykq ` (cid:15)k

where (cid:15)k is a random noise with zero mean. Using general results [20, 5], we know that yk sequences,
when bounded almost surely, have limit points which are Clarke critical.
l

20

