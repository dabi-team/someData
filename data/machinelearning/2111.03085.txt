2
2
0
2

y
a
M
3
2

]

G
L
.
s
c
[

2
v
5
8
0
3
0
.
1
1
1
2
:
v
i
X
r
a

Application of Machine Learning to Sleep Stage
Classiﬁcation

1st Andrew Smith
Department of Computer Science and Engineering
(University of South Carolina)
Columbia, SC 29208 USA
aks3@email.sc.edu

2nd Hardik Anand
Department of Computer Science and Engineering
(University of South Carolina)
Columbia, SC 29208 USA
hanand@email.sc.edu

3rd Snezana Milosavljevic
Department of Pharmacology, Physiology, and Neuroscience
University of South Carolina School of Medicine
Columbia, SC 29208 USA
snezana.milosavljevic@uscmed.sc.edu

4th Katherine M. Rentschler
Department of Pharmacology, Physiology, and Neuroscience
University of South Carolina School of Medicine
Columbia, SC 29208 USA
katherine.rentschler@uscmed.sc.edu

5th Ana Pocivavsek
Department of Pharmacology, Physiology, and Neuroscience
University of South Carolina School of Medicine
Columbia, SC 29208 USA
ana.pocivavsek@uscmed.sc.edu

6th Homayoun Valafar
Department of Computer Science and Engineering
(University of South Carolina)
Columbia, SC 29208 USA
homayoun@cse.sc.edu

Abstract—Sleep studies are imperative to recapitulate phe-
notypes associated with sleep loss and uncover mechanisms
contributing to psychopathology. Most often, investigators man-
ually classify the polysomnography into vigilance states, which
is time-consuming, requires extensive training, and is prone
to inter-scorer variability. While many works have successfully
developed automated vigilance state classiﬁers based on multiple
EEG channels, we aim to produce an automated and open-
access classiﬁer that can reliably predict vigilance state based
on a single cortical electroencephalogram (EEG) from rodents
to minimize the disadvantages that accompany tethering small
animals via wires to computer programs. Approximately 427
hours of continuously monitored EEG, electromyogram (EMG),
and activity were labeled by a domain expert out of 571 hours
of total data. Here we evaluate the performance of various
machine learning techniques on classifying 10-second epochs into
one of three discrete classes: paradoxical, slow-wave, or wake.
Our investigations include Decision Trees, Random Forests, Naive
Bayes Classiﬁers, Logistic Regression Classiﬁers, and Artiﬁcial
Neural Networks. These methodologies have achieved accuracies
ranging from approximately 74% to approximately 96%. Most
notably, the Random Forest and the ANN achieved remarkable
accuracies of 95.78% and 93.31%, respectively. Here we have
shown the potential of various machine learning classiﬁers to
automatically, accurately, and reliably classify vigilance states
based on a single EEG reading and a single EMG reading.

Index Terms—sleep-scoring, machine learning, artiﬁcial intel-

ligence, neuroscience, electrophysiology

I. INTRODUCTION

Nearly 70 million Americans are afﬂicted by chronic sleep
disorders or intermittent sleep disturbances that negatively
impact health and substantially burden our health system.

Sleep is essential for optimal health. Sleep is one of the
most critical and ubiquitous biological processes, next
to
eating and drinking. It has been shown that there is no clear
evidence of the existence of an animal species that does not
sleep [1]. Sleep constitutes about 30% of the human lifespan.
Assessment of sleep quality is multifactorial and is composed
of adequate duration, good quality, appropriate timing and
regularity, and the absence of sleep disturbances or disorders.
Sleep duration is used as a metric to describe the standard
of healthy sleep. The American Academy of Sleep Medicine
(AASM) and Sleep Research Society (SRS) issued a consensus
statement recommending “adults should sleep 7 or more hours
per night on a regular basis to promote optimal health”
[2]. However, sufﬁcient sleep is severely undervalued as a
necessary biological process for maintaining proper mental
health. A recent survey from the Center for Disease Control
and Prevention (CDC) found that only 65% of adults reported
a healthy duration of sleep [3].

From a translational perspective, animal studies are im-
perative to recapitulate phenotypes associated with sleep
loss (hyperarousal, cognitive impairment, slowed psychomo-
tor vigilance, behavioral despair) and uncover mechanisms
contributing to psychopathology, with the added beneﬁt of
homogeneity within rodent subjects [4], [5]. Sleep studies are
readily conducted in small animals by implanting electrodes
to obtain electroencephalogram (EEG) and electromyogram
(EMG). Sleep is categorized into two major classes, non-rapid
eye movement (NREM) and rapid eye movement (REM) sleep,
and arousal is classiﬁed as wake. Most often, investigators

 
 
 
 
 
 
manually classify the polysomnography into vigilance states,
and this practice is time-consuming and also greatly limits
the size of a study’s data set. To accurately classify vigilance
states, investigators undergo extensive training, yet the subjec-
tive nature of classifying limits inter-scorer reliability.

Several automated vigilance state classiﬁers have been es-
tablished, and nearly all of these algorithms rely on multi-
channel EEG data and local ﬁeld potential (LFP) signaling
oscillations within the brain [6]–[12]. The advantages of
multi-channel systems are outweighed by the disadvantage of
tethering small animals to transmit signals via wired connec-
tions to computer programs. Tethered animals are combating
confounds including limited mobility within recording cages
and potential impacts on natural sleep states [13]–[15]. For
these reasons, it is advantageous to automate vigilance state
classiﬁcation with telemetric battery devices that are surgically
implanted to open a single EEG and EMG from each small
animal. Consistent with the principles of Information Theory,
data collected from a single EEG channel signiﬁcantly in-
creases the complexity of sleep-state identiﬁcation for humans
and automated approaches. Therefore, the goal of this research
has been to produce an open access and automated classiﬁer
that can reliably predict vigilance state based on a single
cortical EEG from rodents. To that end, we have evaluated
several of the commonly used Machine Learning techniques
for suitability and success in this task. Our investigations
include Decision Trees, Naive Bayes Classiﬁers, Random
Forests, and Artiﬁcial Neural Networks. An Artiﬁcial Neural
Network (ANN) has been developed to examine and ascertain
the sleep state of each animal.

II. METHODOLOGY

A. Sleep EEG/EMG Data Collection

Adult Wistar rats (n=8) were used in experiments in a
facility fully accredited by the American Association for the
Accreditation of Laboratory Animal Care. Animals were kept
on a 12/12 h light-dark cycle. All protocols were approved
by the Institutional Animal Care and Use Committee at the
University of South Carolina and were in accordance with the
National Institutes of Health Guide for the Care and Use of
Laboratory Animals.

Rats were implanted with EEG/EMG telemetry devices
(PhysioTel HD-S02, Data Science International, St. Paul, MN),
as previously described [15]–[18]. Brieﬂy, under isoﬂurane
anesthesia, animals were placed in a stereotaxic frame. The
transmitter device was intraperitoneally implanted through a
dorsal incision of the abdominal region. After an incision at
the midline of the head was made, EEG leads were secured
to two surgical screws inserted into 0.5-mm burr holes at 2.0
mm anterior/1.5 mm lateral and 7.0 mm posterior/-1.5 mm
lateral to bregma. Two EMG leads were inserted into the
dorsal cervical neck muscle about 1.0 mm apart and sutured
into place. The skin was sutured, and animals were recovered
for a minimum of 7 days prior to experimentation. Sleep data
were acquired in a quiet, designated room where rats remained
undisturbed for the duration of recording using Ponemah

6.10 software (DSI). Digitized signal data were imported into
NeuroScore 3.0 (DSI) and powerband frequencies (0 to 20
Hz) in 0.5 Hz increments were exported to CSV formatted
ﬂat ﬁles.

B. Data Processing and Annotation

Approximately 571 hours of continuously monitored elec-
troencephalogram (EEG), electromyogram (EMG), and activ-
ity were recorded across the eight laboratory rodents. The
collection of data was then partitioned into 10-second epochs,
or segments, and labeled by a domain expert. Approximately
427 hours of the recorded 571 hours have been manually
labeled by a domain expert. In this study, our focus has been
based on the manually annotated 427 hours of the data. The
remaining 144 hours of unlabelled data will be used in the
future for more rigorous testing of our automated classiﬁcation
method. Based on the polysomnogram (PSG), an expert labels
each 10-second epoch as one of three discrete classes: Para-
doxical, Slow-wave, or Wake. Paradoxical sleep is also known
as rapid-eye-movement (REM) sleep, because of the paradox
of the high-frequency brain waves mimicking wakefulness
despite being asleep. Slow-Wave sleep, characterized by low
frequency, high amplitude brain waves, is also known as non-
rapid eye movement sleep (NREM), and it constitutes all sleep
that is not REM sleep. Finally, the “wake” classiﬁcation is
given to a PSG that elicits characteristics of wakefulness.
These sleep stages constitute all three classes and will be
referred to as P, S, and W, which correspond to Paradoxical,
Slow-wave, and Wake states, respectively.

42 discrete features were extracted from each 10-second
epoch of continuous EEG, EMG, and activity signals. To
obtain the ﬁrst 40 features, the 10-second epoch is transformed
from the time domain into the frequency domain using Dis-
crete Fourier Transformation, then partitioned into 40 channels
of equal width from 0 to 20 Hz. Thus, the ﬁrst channel is
the EEG from 0 to 0.5 Hz, the second channel is the EEG
from 0.5 to 1 Hz, and so on. The 41st feature is that of the
EMG, which is averaged over the 10-second epoch. The 42nd
and ﬁnal feature is the “Activity” feature, which is a derived
parameter from Ponemah (indicating the level of an animal’s
activity) which depends on the transmitter model, the speed
with which the transmitter moves, outside radio interference,
and variations from sensor to sensor.

C. Input Formalization

Many tactics in modern machine learning and artiﬁcial
intelligence have been presented that aim to formalize the
use of temporal data in the tasks of prediction and classiﬁ-
cation. Although the proper formulation of input can have a
substantial impact on the trainability and the outcome of ML
developments, in this investigation, we have implemented the
most prevalent and natural approach. More speciﬁcally, the
input to our ML activities consisted of a concatenation of ﬁve
consecutive processed data (42 channels) that summarize a 10-
second epoch from the raw continuous data. We postulate that
a single epoch (summary of 10 seconds of data) is not the

optimal temporal representation of time-series data for use in
ML applications. Therefore, we choose to reformat the data
such that each sample spans a longer period, theoretically
providing each classiﬁer with more temporal information and
conﬁdence. Five consecutive epochs encapsulate 50 seconds of
the temporal signal, to which we will refer as the network input
in the remainder of this report. Before this windowing, the data
consisted of 154043 rows and 43 columns. Each row, which
itself constitutes a 10-second epoch, consists of the 42 features
and 1 output label describing the three stages of vigilance.

This mechanism of input data creation results in the follow-

ing class distribution:

T o t a l : 154039
P : 10028 (6.50% o f
S : 64539 (41.77% o f
W: 79472 (51.73% o f

t o t a l )

t o t a l )
t o t a l ) .

This data set was created by a simple moving window,
where the ﬁrst windowed sample will consist of samples 0-4
in the original data. The second windowed sample consisted
of samples 1-5 in the original data, and the ﬁnal windowed
sample consisted of samples (n-5)-(n-1) in the original data.
Since each input spans ﬁve individual vigilance states, various
methods of arriving at a single output (given from ﬁve outputs)
can be envisioned. In this work, we choose to label each
windowed sample with the most frequent class in the window.
If there is a tie, we label the sample with the label of the
10-second epoch in the center. The total number of samples
after the simple moving windowing has 4 samples less than
the original data, which is well known to be the case when
windowing data. Thus, each row in the input data consists of
210 features (5 sets of 42 features), and 1 label.

D. Data Balancing

The class distribution in the raw windowed data is unequal.
Ideally, there would be an equal representation of each class,
where each class constitutes 33% of the data, in this instance.
However, class P is severely underrepresented at 6.51% of
the total data, or 10028 samples. Classes S and W are over-
represented at 41.90% and 51.59%, or 64539 samples and
79472 samples, respectively. In balancing the representation
between classes, we aim to replicate copies of classes to the
data while minimizing total samples. Minimizing total samples
is important
to improve training speed. By concatenating
complete copies of any given class to itself, we aim to
ensure that the model adequately generalizes to the entire class
sample. Therefore, we concatenate 7 additional copies of the
entire P class to the data, which produced the following and
improved class distribution:

T o t a l : 224235
P : 80224 (35.78% o f
S : 64539 (28.78% o f
W: 79472 (35.44% o f

t o t a l )
t o t a l )
t o t a l ) .

E. Training, Validation, and Testing Split and Shufﬂe

To ﬁnalize data preparation before training a classiﬁer, the
data must be shufﬂed and partitioned into training, validation,

and testing sets. To perform the shufﬂing and partitioning,
we use a function from the popular machine learning module
in python scikit-learn. This function randomly shufﬂes the
data and partitions it into training and testing data. We split
the data into 80% training and 20% testing data. The only
machine learning classiﬁer which uses a validation set is the
Artiﬁcial Neural Network. For the Artiﬁcial Neural Network,
we further split
the training data into 80% training, 20%
validation, which constitutes 64% and 16% of the entire data.
This process was performed only once to keep the training,
testing, and validation (when needed) sets constant across each
ML technique. A consistent training/testing set will help to
establish a more consistent comparison of performances across
multiple techniques while also reducing the data preparation
time.

F. Evaluation of Machine Learning Classiﬁers

We propose to evaluate various machine learning techniques
to classify vigilance states. Following are brief descriptions of
Decision Trees, Random Forests, Naive Bayes Classiﬁers, Lo-
gistic Regression Classiﬁers, and Artiﬁcial Neural Networks.
We aim to ﬁnd the simplest model that achieves the highest
accuracy.

G. Decision Tree

A Decision Tree Classiﬁer is a supervised machine learning
algorithm that performs classiﬁcation tasks. The algorithm
behind a Decision Tree makes a sequence of decisions based
on input features, one decision at each node along a path
from the root
to a leaf of the tree. The leaf node that
the algorithm ends on for any given input determines the
output class. Decision trees are self-interpretable, meaning
the tree itself describes the underlying rules for classiﬁcation.
The advantages of the Decision Tree Classiﬁer include its
simplicity,
interpretability, ability to model nonlinear data,
ability to model high dimensional data, ability to work with
large datasets to produce accurate results, and ability to handle
outliers during training.

H. Random Forest

A Random Forest Classiﬁer is a supervised predictive
machine learning algorithm commonly used for classiﬁcation
tasks. A Random Forest consists of an ensemble of decision
trees, each of which provides a “vote”, or a classiﬁcation,
predicting class based on a majority of votes from the decision
trees. Random forests generally outperform decision trees in
terms of accuracy; however, the random forest is a blackbox, a
model unable to describe its underlying rules for classiﬁcation,
sacriﬁcing the interpretability of the decision tree.

I. Naive Bayes

A Naive Bayes Classiﬁer is a supervised probabilistic
machine learning algorithm commonly used for classiﬁcation
tasks. It relies on Bayes’ Theorem, which is a theorem of
conditional probabilities. Naive Bayes assumes strong inde-
pendence between the input features. We use the Gaussian

Naive Bayes Classiﬁer based on the assumption that each input
feature is normally distributed. The Naive Bayes Classiﬁer is
simple (and, therefore, computationally fast), scalable (requir-
ing parameters linear in the number of features), and works
well with high-dimensional data.

J. Logistic Regression

A Logistic Regression Classiﬁer is a supervised predictive
machine learning algorithm used for classiﬁcation tasks. It
is a type of generalized Linear Regression algorithm with
a complex cost function. The cost function used here is the
Sigmoid function, which continuously maps real-valued num-
bers between 0 and 1. We partition these continuously-mapped
values from the cost function by various threshold values
to determine output class. We include Logistic Regression
in this work based on its speed, non-assumption of feature
independence, and ubiquity in multi-class classiﬁcation.

K. Artiﬁcial Neural Network

An artiﬁcial neural network is a supervised predictive ma-
chine learning technique that is successful in classiﬁcation
tasks in various applications [19]–[21]. A feed-forward neural
network is a subset of neural networks in general where there
are no cycles formed between neurons. We choose to use a
speciﬁc type of feed-forward neural network, the multi-layer
perceptron (MLP). An MLP consists of at least one hidden
layer of neurons, as opposed to a single-layer perceptron,
which does not have a hidden layer of neurons separate from
the input and output layers. We propose to use a shallow neural
network, as opposed to a deep neural network, which is a
subset of the multi-layer perceptron. For a neural network to
be shallow, it means that the network has exactly one hidden
layer with any number of neurons.

1) Architecture: For this investigation, we propose a fully
connected artiﬁcial neural network with one hidden layer. The
network has a single input layer where the number of neurons
equals the number of input features and a single output layer
where the number of neurons equals the number of classes.
Thus, we use a neural network architecture similar to Figure.1
with 210 input neurons, 256 hidden layer neurons, and three
output neurons. Non-linear activation functions allow neural
networks to learn complex patterns. We use the rectiﬁed linear
unit activation function after the hidden layer to introduce this
non-linearity into the model. We use the softmax activation
function after the output layer. Softmax is commonly used in
multi-class classiﬁcation problems, more general than sigmoid,
and maps output into the range between 0 and 1, making it a
good function for determining class. We use the backpropa-
gation learning technique and the adam optimizer [22].

L. Evaluation of Model Performance

We evaluate the performance of each of the classiﬁers by
comparing the predictions of each model to the manual scoring
of domain experts. Testing accuracy is a ratio of the number of
correctly classiﬁed samples by the model to the total number

Input
layer

Hidden
layer

Output
layer

Fig. 1. Fully-connected artiﬁcial neural network with shape (210,256,3),
meaning the input layer has 210 neurons, the single hidden layer has 256
neurons, and the output layer has 3 neurons.

of samples. The primary measure of performance used in this
investigation will be testing accuracy.

However, testing accuracy does not always fully describe
the performance of a machine learning model. Oftentimes
it is important to maximize the true positive classiﬁcations
or to minimize the false positive classiﬁcations. In order
to measure these situations, we have the proportions called
precision and recall. Precision is the proportion of model
classiﬁcations that correspond to the sample’s true class. Recall
is the proportion of samples that are actually classiﬁed by the
model correctly. In addition to these metrics, we also provide
F1 Score. The F1 Score is the harmonic mean of precision
and recall. Finally, AUC, or Area Under the receiver oper-
ating characteristic Curve, represents the probability that the
model ranks a random positive example higher than a random
negative example. We choose to display testing classiﬁcations
for each machine learning model in the form of a Confusion
Matrix. In each confusion matrix, class 0 corresponds to class
P, class 1 corresponds to class S, and class 2 corresponds to
class W. Accuracy, precision, recall, F1 score, and AUC can
all be derived from a confusion matrix.

III. RESULTS

We evaluate the performance of the aforementioned method-
ologies primarily through testing accuracy. The testing accu-
racies for each methodology are described in Table. I. F1
Score and AUC Score are provided for each machine learning
technique; however, it will sufﬁce to focus solely on accuracy.
Notably, the highest performing classiﬁer is the Random
Forest model with a testing accuracy of 95.78%. The confusion
matrix for the testing classiﬁcation of this model is shown
in Fig. 2. Other metrics such as precision and recall may be
derived from the confusion matrix. The model performs excel-
lently on class 0, which corresponds to paradoxical sleep. The
most frequently misclassiﬁed prediction-label pair occurred
between slow-wave sleep and wakefulness. There were 1017
instances of predicting wakefulness where the true label was

TABLE I
PERFORMANCE OF VARIOUS MACHINE LEARNING MODELS IN SLEEP
STAGE CLASSIFICATION.

Classiﬁer

Accuracy

F1 Score

AUC Score

Random Forest
ANN
Decision Tree Classiﬁer
Logistic Regression
Naive Bayes

95.78%
93.31%
92.77%
77.33%
74.37%

96%
93.9%
93%
77%
74%

.9954
.9867
.9427
.9242
.9011

slow-wave and 606 samples where the model predicted slow-
wave sleep where the true label was wake.

Not only did this model achieve a high degree of categorical

0

15166

494

550

582

11911

l
e
b
a
l

1

l
a
u
t
c
A

2

180

405

0

990

14569

1
Predicted label

2

Fig. 2. Confusion matrix for the Ran-
dom Forest model with overall accu-
racy of 95.78%.

Fig. 3. Confusion matrix for the Deci-
sion Tree model with overall accuracy
of 92.77%.

Fig. 4. Confusion matrix for the Lo-
gistic Regression model with overall
accuracy of 77.33%.

Fig. 5. Confusion matrix for the Naive
Bayes model with overall accuracy of
74.37%.

The second-highest performing classiﬁer is the Artiﬁcial
Neural Network with a testing accuracy of 93.31%. Across
the entire testing partition,
the model achieves over 93%
categorical accuracy, which is comparable or higher than
many other methodologies in the literature [8], [23], [24]. The
confusion matrix for the testing classiﬁcation of this model
is shown in Fig. 6. The prediction-label pair that was most
frequently misclassiﬁed by the ANN occurred when the model
predicted slow-wave and the true label was wake. Interestingly,
this is the same confusion pair that occurred most frequently in
the Random Forest model. Theoretically, there is a dimension
that would reliably distinguish between slow-wave and wake.
Future research will investigate this relationship and attempt
to distinguish more reliably and accurately between these two
classes. The ANN achieved a high F1 Score and AUC Score
of 93.9% and .9867, respectively. After training for only 50
epochs, the model achieves a testing categorical accuracy of
94.01%, testing precision of 94.54%, testing recall of 93.43%,
and a testing AUC of 99%. The value of these metrics over
the period of training are shown in Fig. 7, Fig. 9, and Fig. 10.

Fig. 6. Confusion matrix for the Artiﬁcial Neural Network classiﬁer with
overall accuracy of 93.31%.

0.4

0.3

s
s
o
L

0.2

0.1

0.0

Train
Val

0

10

20

30

Epoch

40

50

y
c
a
r
u
c
c
a

l
a
c
i
r
o
g
e
t
a
C

1.0

0.8

0.6

0.4

0.2

0.0

Train
Val

0

10

20

30

40

50

Epoch

Fig. 7. Loss curve for training and
validation data over 50 epochs.

Fig. 8. Accuracy curve for training
and validation data over 50 epochs.

n
o
i
s
i
c
e
r
P

1.0

0.8

0.6

0.4

0.2

0.0

Train
Val

0

10

20

30

40

50

Epoch

l
l
a
c
e
R

1.0

0.8

0.6

0.4

0.2

0.0

Train
Val

0

10

20

30

Epoch

40

50

Fig. 9. Precision curve for training
and validation data over 50 epochs.

Fig. 10. Recall curve for training and
validation data over 50 epochs.

accuracy, precision, and recall, but it did this with very little
training and computation. All training instances consisted of
only 50 epochs, each of which took approximately 220.66 sec-
onds when run on a 2.4 GHz 7th Generation Intel(R) Core(T
M) i7-7700HQ Quad-Core Processor with 16GB (2x8GB)
DDR4 at 2400Mhz.

The Decision Tree model performed well with an accuracy
of 92.77%; however, we do not discuss this model any further
because it is a special case of the Random Forest. Logistic
Regression and Naive Bayes had testing accuracies of 77.33%
and 74.37%, respectively. These low accuracies suggest that
this vigilance state classiﬁcation problem is not trivial and that
the accuracies reached by the Random Forest and the Artiﬁcal
Neural Network are remarkable achievements.

IV. CONCLUSION
Many works which rely on multiple EEG channels have
successfully produced automated vigilance state classiﬁers;

however, the hardware required to obtain such signals produces
undesirable disadvantages. Here we evaluate various machine
learning techniques in vigilance state classiﬁcation based on
a single EEG channel. Random Forests and Artiﬁcial Neural
Networks produced remarkable accuracies of approximately
96% and 93%. In evaluation of these classiﬁcation techniques
against human scoring as ground truth, we note that humans
may make misclassiﬁcations. In fact, due to the rigidly pat-
terned nature of the data and the power of these statistical
models, it would be appropriate to reevaluate human scores
based on model classiﬁcations. Future research will have
a domain expert label polysomnograms from the additional
147 hours of unscored data aided by the model from this
work. Additionally, future research will have a domain expert
reevaluate classiﬁcation pairs that confused the models in this
work. Here we achieved two accurate and reliable models
that can be used immediately in an automated vigilance state
classiﬁer and may be reinforced in future work.

REFERENCES

[1] C. Cirelli and G. Tononi, “Is

Here, vol. 6, no. 8, p. e216, Aug. 2008.
https://doi.org/10.1371/journal.pbio.0060216

sleep essential?” Journal Name
[Online]. Available:

[2] N. F. Watson, M. S. Badr, G. Belenky, D. L. Bliwise, O. M. Buxton,
D. Buysse, D. F. Dinges, J. Gangwisch, M. A. Grandner, C. Kushida,
R. K. Malhotra, J. L. Martin, S. R. Patel, S. Quan, and E. Tasali,
“Recommended amount of sleep for a healthy adult: A joint consensus
statement of
the american academy of sleep medicine and sleep
research society,” Journal Name Here, Jun. 2015. [Online]. Available:
https://doi.org/10.5665/sleep.4716

[3] and Nathaniel F. Watson, M. S. Badr, G. Belenky, D. L. Bliwise, O. M.
Buxton, D. Buysse, D. F. Dinges, J. Gangwisch, M. A. Grandner,
C. Kushida, R. K. Malhotra, J. L. Martin, S. R. Patel, S. F. Quan,
E. Tasali, M. Twery, J. B. Croft, E. Maher, J. A. Barrett, S. M. Thomas,
and J. L. Heald, “Joint consensus statement of the american academy of
sleep medicine and sleep research society on the recommended amount
of sleep for a healthy adult: Methodology and discussion,” Journal
Name Here, vol. 38, no. 8, pp. 1161–1183, Aug. 2015. [Online].
Available: https://doi.org/10.5665/sleep.4886

[4] G. I. Elmer, P. L. Brown, and P. D. Shepard, “Engaging research domain
criteria (RDoC): Neurocircuitry in search of meaning,” Schizophrenia
Bulletin, vol. 42, no. 5, pp. 1090–1095, Jul. 2016. [Online]. Available:
https://doi.org/10.1093/schbul/sbw096

[5] G. Missig, E. L. Mokler, J. O. Robbins, A. J. Alexander, C. J.
McDougle, and W. A. Carlezon, “Perinatal immune activation produces
persistent sleep alterations and epileptiform activity in male mice,”
Neuropsychopharmacology, vol. 43, no. 3, pp. 482–491, Oct. 2017.
[Online]. Available: https://doi.org/10.1038/npp.2017.243

[6] V. Gao, F. Turek, and M. Vitaterna, “Multiple classiﬁer systems for
automatic sleep scoring in mice,” J Neurosci Methods, vol. 264,
pp. 33–39, 2016. [Online]. Available: https://www.ncbi.nlm.nih.gov/
pubmed/26928255

[7] B. A. Gross, C. M. Walsh, A. A. Turakhia, V. Booth, G. A. Mashour,
and G. R. Poe, “Open-source logic-based automated sleep scoring
software using electrophysiological recordings in rats,” J Neurosci
Methods, vol. 184, no. 1, pp. 10–8, 2009.
[Online]. Available:
https://www.ncbi.nlm.nih.gov/pubmed/19615408

[8] D. Miladinovic, C. Muheim, S. Bauer, A. Spinnler, D. Noain,
M. Bandarabadi, B. Gallusser, G. Krummenacher, C. Baumann,
A. Adamantidis, S. A. Brown, and J. M. Buhmann, “Spindle:
End-to-end learning from eeg/emg to extrapolate animal
sleep
scoring across experimental settings, labs and species,” PLoS Comput
Biol, vol. 15, no. 4, p. e1006968, 2019.
[Online]. Available:
https://www.ncbi.nlm.nih.gov/pubmed/30998681

[9] R. Stephenson, A. M. Caron, D. B. Cassel, and J. C. Kostela,
J Neurosci
“Automated analysis of
sleep-wake
Methods, vol. 184, no. 2, pp. 263–74, 2009. [Online]. Available:
https://www.ncbi.nlm.nih.gov/pubmed/19703489

in rats,”

state

[10] J. M. Tredger, J. Grevel, N. Naoumov, C. M. Steward, A. A. Niven,
B. Whiting, and R. Williams, “Cyclosporine pharmacokinetics in
liver transplant recipients: evaluation of results using both polyclonal
radioimmunoassay and liquid chromatographic analysis,” Eur J Clin
Pharmacol, vol. 40, no. 5, pp. 513–9, 1991. [Online]. Available:
https://www.ncbi.nlm.nih.gov/pubmed/1884727

[11] R. P. Louis, J. Lee, and R. Stephenson, “Design and validation
of a computer-based sleep-scoring algorithm,” J Neurosci Methods,
vol. 133, no. 1-2, pp. 71–80, 2004.
[Online]. Available: https:
//www.ncbi.nlm.nih.gov/pubmed/14757347

[12] J. G. Ellen and M. B. Dash, “An artiﬁcial neural network for automated
behavioral state classiﬁcation in rats,” PeerJ, vol. 9, p. e12127, 2021.
[Online]. Available: https://www.ncbi.nlm.nih.gov/pubmed/34589305

[13] K. Kramer and L. B. Kinter, “Evaluation and applications of
laboratory animals,” Physiol Genomics,
[Online]. Available: https:

radiotelemetry in small
vol. 13, no. 3, pp. 197–205, 2003.
//www.ncbi.nlm.nih.gov/pubmed/12746464

[14] A. Papazoglou, A. Lundt, C. Wormuth, D. Ehninger, C. Henseler,
J. Soos, K. Broich, and M. Weiergraber, “Non-restraining eeg
radiotelemetry: Epidural
eeg
electrode placement,” J Vis Exp, no. 112, 2016. [Online]. Available:
https://www.ncbi.nlm.nih.gov/pubmed/27404845

and deep intracerebral

stereotaxic

[15] K. M. Rentschler, A. M. Baratta, A. L. Ditty, N. T. J. Wagner, C. J.
Wright, S. Milosavljevic, J. A. Mong, and A. Pocivavsek, “Prenatal
kynurenine elevation elicits sex-dependent changes in sleep and arousal
Implications for psychotic disorders,” Schizophr
during adulthood:
Bull, vol. 47, no. 5, pp. 1320–1330, 2021.
[Online]. Available:
https://www.ncbi.nlm.nih.gov/pubmed/33823027

[16] A. M. Baratta, S. A. Buck, A. D. Buchla, C. B. Fabian, S. Chen, J. A.
Mong, and A. Pocivavsek, “Sex differences in hippocampal memory
and kynurenic acid formation following acute sleep deprivation in
rats,” Sci Rep, vol. 8, no. 1, p. 6963, 2018. [Online]. Available:
https://www.ncbi.nlm.nih.gov/pubmed/29725029

[17] A. Pocivavsek, A. M. Baratta, J. A. Mong, and S. S. Viechweg, “Acute
kynurenine challenge disrupts sleep-wake architecture and impairs
contextual memory in adult rats,” Sleep, vol. 40, no. 11, 2017. [Online].
Available: https://www.ncbi.nlm.nih.gov/pubmed/29029302

[18] A. M. Baratta, N. R. Kanyuch, C. A. Cole, H. Valafar, J. Deslauriers,
and A. Pocivavsek, “Acute sleep deprivation during pregnancy in rats:
Rapid elevation of placental and fetal
inﬂammation and kynurenic
acid,” Neurobiol Stress, vol. 12, p. 100204, 2020. [Online]. Available:
https://www.ncbi.nlm.nih.gov/pubmed/32258253

[19] C. A. Cole, D. Anshari, V. Lambert, J. F. Thrasher, and H. Valafar,
“Detecting smoking events using accelerometer data collected via
smartwatch technology: Validation study,” JMIR Mhealth Uhealth,
vol. 5, no. 12, p. e189, Dec. 2017.
[Online]. Available: https:
//doi.org/10.2196/mhealth.9035

[20] T. M. Fawcett, S. J.

Irausquin, M. Simin, and H. Valafar, “An
artiﬁcial neural network approach to improving the correlation
between protein energetics and the backbone structure,” Proteomics,
vol. 13, no. 2, pp. 230–238, Dec. 2012.
[Online]. Available:
https://doi.org/10.1002/pmic.201200330

[21] C. O. Odhiambo, C. A. Cole, A. Torkjazi, and H. Valafar, “State
transition modeling of the smoking behavior using lstm recurrent neural
networks,” 2020.

[22] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

2017.

[23] I. Exarchos, A. A. Rogers, L. M. Aiani, R. E. Gross, G. D. Clifford,
N. P. Pedersen, and J. T. Willie, “Supervised and unsupervised machine
learning for automated scoring of sleep–wake and cataplexy in a mouse
model of narcolepsy,” Sleep, vol. 43, no. 5, Nov. 2019. [Online].
Available: https://doi.org/10.1093/sleep/zsz272

[24] M. Yamabe, K. Horie, H. Shiokawa, H. Funato, M. Yanagisawa, and
H. Kitagawa, “MC-SleepNet: Large-scale sleep stage scoring in mice
by deep neural networks,” Scientiﬁc Reports, vol. 9, no. 1, Oct. 2019.
[Online]. Available: https://doi.org/10.1038/s41598-019-51269-8

