2
2
0
2

l
u
J

5
1

]

G
L
.
s
c
[

2
v
4
0
1
4
0
.
2
0
2
2
:
v
i
X
r
a

Teaching Networks to Solve Optimization Problems

Xinran Liu1 Yuzhe Lu1 Ali Abbasi1 Meiyi Li2

Javad Mohammadi2 Soheil Kolouri1

1Vanderbilt University

2University of Texas at Austin

{xinran.liu, yuzhe.lu, ali.abbasi, soheil.kolouri}@vanderbilt.edu
{meiyil, javadm}@utexas.edu

Abstract

Leveraging machine learning to facilitate the optimization process is an emerging
ﬁeld that holds the promise to bypass the fundamental computational bottleneck
caused by classic iterative solvers in critical applications requiring near-real-time
optimization. The majority of existing approaches focus on learning data-driven
optimizers that lead to fewer iterations in solving an optimization. In this paper, we
take a different approach and propose to replace the iterative solvers altogether with
a trainable parametric set function, that outputs the optimal arguments/parameters
of an optimization problem in a single feed forward. We denote our method as
Learning to Optimize the Optimization Process (LOOP). We show the feasibility
of learning such parametric (set) functions to solve various classic optimization
problems including linear/nonlinear regression, principal component analysis,
transport-based coreset, and quadratic programming in supply management ap-
plications. In addition, we propose two alternative approaches for learning such
parametric functions, with and without a solver in the LOOP. Finally, through
various numerical experiments, we show that the trained solvers could be orders
of magnitude faster than the classic iterative solvers while providing near optimal
solutions.

1

Introduction

Optimization problems are ubiquitous in computational sciences and engineering. Classic solutions
to optimization problems involve iterative algorithms often relying on predetermined ﬁrst and second
order methods like (sub)gradient ascent/descent, conjugate gradients, simplex basis update, among
others. These methods often come with desirable theoretical convergence guarantees, but their
iterative nature could be limiting in applications requiring near-real time inference. Moreover, these
algorithms’ performance remains the same regardless of the number of times a similar optimization
problem is visited. Recently, there has been an emerging interest in leveraging machine learning
to enhance the efﬁciency of optimization processes and address some of these shortcomings. The
learning based solutions are often referred to as Learning to Optimize (L2O) methods in the literature.

While L2O methods do not come with theoretical guarantees, they hold the promise of: 1) reducing
the number of iterations needed to arrive at a solution, and 2) improving over time as more optimiza-
tion problems are visited. L2O allows for transferring recent advances in machine learning, e.g.,
self-supervised learning, meta-learning, and continual learning, to learn data-driven optimization
algorithms that could improve over time. Most existing L2O methods aim to learn a function that
receives the current loss or its gradient, and based on the memory of previous loss values (or gradients)
provide an update for the optimization parameters. Hence, these methods do not eliminate the iterative
nature of the solution but aim at improving the iterative solution to: 1) reduce the number of total
iterations, and 2) leading to better solutions for non-convex problems.

In this paper, we consider an inherently different use-case of machine learning in solving optimization
problems. We propose to replace the classic iterative solutions of an optimization problem with a

Preprint. Under review.

 
 
 
 
 
 
trainable parametric (set) function that directly maps the input of the optimization problem to the
optimal parameters in a single feed forward. This process, which we denote as Learning to Optimize
the Optimization Process (LOOP), is inspired by biological systems that are capable of solving
complex optimization problems upon encountering the problem multiple times. By omitting the
classic iterative solutions, LOOP overcomes one of the major optimization bottlenecks enabling
near-real-time optimization in a wide range of critical applications.

LOOP is particularly suitable when one needs to perform a certain type of optimization (e.g.,
linear/quadratic programming) over a speciﬁc distribution of input data (e.g., sensors data collection)
repeatedly. These problems abound in practice, with examples being cyber-physical infrastructures,
autonomous vehicle networks, sensor networks monitoring a physical ﬁeld, ﬁnancial markets, and
supply chains. For example, the resiliency and cost-effectiveness of our cyber-physical energy system
relies on ﬁnding optimal energy dispatch decisions in near-real-time. This is a prime example of an
optimization required to be repeatedly solved over the distribution of electricity demands on the power
grid. Another example is trafﬁc ﬂow management in transportation networks, where trafﬁc control
systems need to determine trafﬁc lights’ status based on the trafﬁc measurements continuously.

At a ﬁrst glance, the use of neural networks for solving frequently solved optimization problems may
seem inefﬁcient. However, such paradigm shift would allow us to leverage recent advances in deep
learning, in particular, deep learning on edge-devices, continual learning, and transfer learning to
improve the performance of an optimizer over time, even for a ﬁxed given computational budget.
Below we enumerate our speciﬁc contributions.

1. Providing a generic framework, LOOP, for replacing the classic iterative optimiza-
tion algorithms with a trainable parametric (set) function that outputs the optimal argu-
ments/parameters in a single feed forward.

2. Proposing two generic approaches for training parametric (set) functions to solve a certain

type of optimization problem over a distribution of input data.

3. Demonstrating the success of our LOOP framework in solving various types of opti-
mization problems including linear/nonlinear regression, principal component analysis, the
optimal transport-based coreset, and the quadratic programming in supply management
application.

2 Prior Work

One of the classic applications of machine learning in optimization has been in predicting proper
hyper-parameters to solve an optimization problem. Such hyper-parameters could include learning
rate, momentum decay, and regularization coefﬁcients, etc. The existing literature on learning to
predict hyper-parameters include approaches based on sequential model-based Bayesian optimization
(SMBO)[16, 6, 28], and gradient-based methods [5, 24, 32]. At their core, these methods instantiate
different variations of the same optimization algorithm, e.g., stochastic gradient descent (SGD), by
selecting different hyper-parameters.

More recently, a large body of work has focused on leveraging machine learning to improve the
optimization process by replacing the engineered optimizers with learnable ones. These methods,
referred to as Learning to Optimize (L2O) approaches, are based on learning a parametric function,
often in the form of a recurrent neural network, that receives the current loss (or its gradient) as
input and outputs the parameter updates [15, 22, 3, 33, 8]. Such methods are effective in optimizing
a wide range of optimization problems by reducing the number of iterations and often achieve
better solutions for non-convex optimization problems. Chen et al. [7] provide a comprehensive
review of these approaches and their numerous applications. Unlike the hyper-parameter search
methods that instantiate different variations of the same optimization algorithm (e.g., SGD), L2O
approaches effectively search over an expansive space of optimization algorithms to ﬁnd an optimal
algorithm. The optimal algorithm (i.e., the learned optimizer) ﬁts input data distribution for a speciﬁc
optimization problem (e.g., linear/quadratic programming); hence, it can lead to better performance
than generic algorithms.

In this paper, our focus is entirely different from both hyper-parameter optimization approaches,
and L2O approaches discussed above. Instead of searching in the space of possible optimizers, our
goal is to replace the optimization algorithm with a parametric (set) function that directly maps the

2

Figure 1: Our two proposed approaches for training LOOP: 1) with solver in the loop (left), and 2)
without solver in the loop and by directly minimizing the objective function (right).

optimization’s input data to the optimal arguments/parameters. The motivation behind such transition
is to: 1) discard iterations altogether, 2) have an optimizer that improves over time and encounters
more optimization problems of a speciﬁc type. More importantly, the proposed framework allows
one to leverage some of the core machine learning concepts, including continual/lifelong learning,
transfer learning, domain adaptation, few/one/zero-shot learning, model compression (through sparse
training and/or training), and many others into the improving the optimization process.

Several recent papers in the literature leverage deep neural networks to approximate the output of
an optimization algorithm, which is in essence similar to our proposed framework, LOOP. In
VoxelMorph, for instance, Balakrishnan et al. [4] trained a convolutional neural network to register
medical images; image registration is a non-convex optimization problem often solved through
time-consuming iterative and multi-scale solvers. In an entirely different application, Pan et al.
[26] trained a neural network to predict the set of independent operating variables (e.g., energy
dispatch decisions) for optimal power ﬂow (OPF) optimization problems, denoted as DeepOPF. They
showed that DeepOPF requires a fraction of the time used by conventional solvers while resulting
in competitive performance. More recently, Knyazev et al. [17] trained a neural network to directly
predict the parameters of an input network (with unseen architecture) to solve the CIFAR-10 and
ImageNet datasets. LOOP is the common theme behind these seemingly unrelated works. In this
paper, we establish LOOP as a generic alternative framework to the classic optimization algorithms,
as well as, the L2O approaches, and show that many optimization problems can be directly solved
through training neural networks.

3 Method

We start by considering unconstrained optimization problems of the following type:

u∗ = arg min

f (X , u)

u

(1)

where X = {xn ∈ Rd}N
n=1 is the set of inputs to the distribution, u ∈ Rl is the optimization
parameters, and f (X , u) is the objective function with respect to parameters u and inputs X . To
replace this optimization with a set function approximator, we propose two approaches as in Figure 1.

Solver in the LOOP– In our ﬁrst formulation, during the training, we use the classic solvers to
obtain u∗ and use it as the ground truth. Then we pose the problem as a supervised learning problem.
Our training objective is shown below:

arg min
θ

EX ∼PX [d(φθ(X ), u∗)]

s.t. u∗ = arg min

f (X , u)

u

where d(·, ·) : Rl × Rl → R+ is a discrepancy/distance deﬁned in Rl, and φθ denotes our set neural
network, and PX is a set distribution.

Without Solver– The use of a solver in our ﬁrst formulation could be limiting, as such solvers
are often computationally expensive turning the training excruciatingly slow. More importantly, in
non-convex problems the calculated u∗ for input X is not unique (e.g., due to different initialization),

3

which leads to solving a regression problem with changing targets. To avoid these problems, in our
second formulation, we directly optimize the objective function and with a slight abuse of the term
call it a “self-supervised” formulation:

arg min
θ

EX ∼PX [f (X , φθ(X ))]

(2)

where the expected objective value over the distribution of the input sets is minimized. Note, for
constrained problems (depending on the use case) we leverage different optimization techniques. For
instance, we can enforce simple constraints (e.g., u ≥ 0) into our model (i.e., the set function) using
Rectiﬁed Linear Unit (ReLU) activations in the output layer of our network. Also, we can use the
Lagrange dual function and absorb the constraints into our objective function as penalty terms. Next
we describe the different optimization problems we consider in this paper.

3.1 Problem 1: Linear/Nonlinear Regression

n ∈ R)}Ni
We start by the simple and yet routine problem of regression. Let Xi = {(xi
n=1
where the goal is to learn a parametric function ρu : Rd → R. Here, index i refers to the i’th
regression problem of interest. In linear regression, ρu(x) = uT x (we absorbed the bias into x for
simplicity of notation). For nonlinear regression ρu(x) = uT ψ(x), ψ : Rd → Rl is a nonlinear
mapping to a feature space (i.e., the kernel space). The optimization problem is then as follows:

n ∈ Rd, yi

u∗ = arg min

u

1
2

N
(cid:88)

n=1

(cid:107)ρu(xn) − yn(cid:107)2

2 + λΩ(u)

(3)

where Ω(u) is the regularization term (e.g., (cid:96)2 or (cid:96)1 norm), and λ is the regularization coefﬁcient.
Our goal is then to learn a network that can solve the regression problem for unseen input data.

3.2 Problem 2: Principal Component Analysis

Next, we consider the principle components analysis (PCA) problem, a common technique to project
high-dimensional samples into a lower dimensional space while maximizing the variation of the data.
Let Xi = {xi

n=1, then PCA seeks an orthornormal set of k vectors, {wl}k

n ∈ Rd}Ni

l=1 such that:

wT Siw

s.t. wT

j wl =

(cid:26) 1
0

j = l
j < l

wl = arg max

w
n−¯xi)(xi

(cid:80)Ni

n=1(xi

n−¯xi)T is the covariance matrix of the data, and ¯xi = 1
Ni

where Si = 1
n=1 xi
n
Ni
is the mean. Deriving the closed-form-solution for this problem involves calculation of the eigen-
l = λlw∗
vectors of the covariance matrix, i.e., Siw∗
l are the l’th eigenvalue and
eigenvector, respectively. This optimization problem can be presented as a set-function that receives a
set of d-dimensional points, X i with cardinality |Xi| = Ni, and returns U ∗ = [w∗
k]. Using
this representation, LOOP approximates the discussed set-function and outputs the top k principle
components for the input set. Hence, we aim to ﬁnd a φθ, such that φθ(X ) ≈ U ∗ for X ∼ PX .

l . Here λl and w∗

2, ..., w∗

1, w∗

(cid:80)Ni

3.3 Problem 3: Optimal transport-based Coreset

For our third problem, we consider the optimal transport-based coreset problem. The notion of
coreset originates from computational geometry [2] and has been widely used in machine learning
tasks. Constructing a coreset from a large dataset is an optimization problem of ﬁnding a smaller
set to best approximate the original dataset on a certain measure. Claici et al. [9] leveraged optimal
transport theory and introduced Wasserstein measure to calculate the coreset. Their work aims to
minimize the Wasserstein distance of the coreset from a given input data distribution. In this paper
we consider this transport-based coreset problem with respect to a ﬁxed size output.
Let X = {xn ∈ Rd}N
n=1 be an input set. We assume that elements of each set are i.i.d. samples
from an underlying distribution. Our sets are represented as empirical distributions, i.e., p(x) =
1
m=1 with the
N
empirical distribution qU (x) = 1
M

n=1 δ(x − xn). Given a size M (M (cid:28) N ), we seek a set U ∗ = {µm ∈ Rd}M

(cid:80)M

(cid:80)N

m=1 δ(x − µm), such that
U ∗ = arg min
W2(p, qU )

(4)

U

4

Figure 2: For an input set X = {xn ∈ Rd}N
minimizes the Wasserstein distance between the empirical distributions p(x) = 1
N
and qU (x) = 1
M

n=1, LOOP returns a coreset U = {µm ∈ Rd}M

m=1 δ(x − µm)

m=1 that
n=1 δ(x − xn)

(cid:80)M

(cid:80)N

where W2(·, ·) denotes the 2-Wasserstein distance. Existing approaches to this optimization problem
rely on iterative linear programming to compute optimal transports in each iteration. We replace
this costly process with a parametric set function φθ such that φθ(X ) ≈ U ∗ for X ∼ PX (Figure 2).
The optimal transport-based coreset problem is equivalent to the free-support Wasserstein barycenter
problem [10] when there is only one input distribution.

3.4 Problem 4: Supply management in Cyber-Physical Systems

Lastly, we utilize LOOP to solve the fundamental problem of supply management in Cyber-Physical
Systems (CPS). The electric power grid is an example of a CPS that is increasingly facing supply-
demand issues. Power networks are large-scale systems spanning multiple cities, states, countries,
and even continents and are characterized as a complex interconnect of multiple entities with diverse
functionalities. The grid of future will differ from the current system by the increased integration of
decentralized generation, distributed storage, and communications and sensing technologies. These
advancements, combined with climate change concerns, resiliency needs, and electriﬁcation trends,
are resulting in a more distributed and interconnected grid, requiring decisions to be made at scale
and in a limited time window. At its basic form, energy supply-demand problem seeks to ﬁnd the
most cost effective power production for meeting the end-users’ needs and can be formulated as,

arg min
u

N
(cid:88)

n=1

Cn(un)

s.t.

N
(cid:88)

n=1

un =

M
(cid:88)

m=1

xm, un ≤ un ≤ un

(5)

where un is the produced electric power from source n and Cn is its corresponding cost, which is a
quadratic function. Given that un represents the power output, it is bounded by physical limitation
of the resource n, i.e., un and un. In this setup, xm refers to the hourly electric demand in node m
(where the term ‘node’ identiﬁes an end-user/consumer). Note, the values of xm are positive. The
equality constraint ensures the supply-demand balance. In practice, this problem is solved on an
hourly basis to serve the predicted electric demand for the next hour. We aim to approximate this
process with a parametric set function, such that φθ(X ) ≈ U ∗ for X ∼ PX .

4 Experiments

In this section, we demonstrate the application of LOOP on problems enumerated in Section 3 and
compare it to classic solvers. Throughout this section, GT refers to the Ground Truth and Solver refers
to the results obtained from using commercial solvers to solve optimization problems of interest. For
each problem and for each model architecture, we repeat the training of our LOOP models ﬁve
times, and we test the performance on a set of 100 problems per model. We then report the mean and
standard deviations of all experiments over the ﬁve models and the 100 test sets. We start by laying
out the speciﬁcs of our models and then discuss the implementation details for each problem.

4.1 Models

Given that the inputs to our optimization problems are all sets, we pose these problems as learning
permutation invariant deep neural networks on set-structured data. To that end, we use Deep Sets
[35] with different pooling mechanisms and the Set Transformer [21].

5

𝑥!𝜇"A Parametric (Set) Function 𝜙!𝒳=𝑥!!"#$𝒰=𝜇%%"#&Figure 3: Performance comparison between LOOP and the solver for three different model archi-
tectures (left) and for the two proposed learning settings (with or without the solver in the LOOP)
for the linear regression (a) and nonlinear regression (b) problems. The plots on the right shows the
performance of the Set Transformer network and the solver as a function of training samples.

Deep Sets are permutation invariant neural architectures (i.e., the output remains unchanged under
any permutation of input set’s elements), which consist of: 1) a multi-layer perceptron (MLP)
encoder, 2) a global pooling mechanism (e.g., average pooling), and 3) a MLP decoder that projects
the pooling representation to the output; φ(X ) = ψ(pool({η(x1), · · · , η(xn)})). Here, η is the
encoder that extracts features from each element of X independently, resulting in a permutation
equivariant function on the input set, and ψ is the decoder that generates ﬁnal output after a pooling
layer (pool). To achieve a permutation invariance set function, the pooling mechanisms must be a
permutation invariance operator (e.g., average pooling, or more advanced methods like Pooling by
sliced-wasserstein embedding (PSWE) [25]). Speciﬁcally, we use global average pooling (GAP) and
Sliced-Wasserstein Embedding (SWE) [25, 23] respectively as the pooling layer.

Set Transformer follows a similar blueprint of permutation equivariant encoder, permutation in-
variant pooling, and permutation equivariant decoder as Deep Sets. However, while the encoder in
the Deep Sets model acts on each set element independently, Set Transformers use attention to pass
information between elements in the encoder. This allows the encoder to model relations between
elements, which can be crucial to approximate a parametric (set) function in some learning tasks.

More precisely, the encoder is a stack of multiple trainable (Induced) Set Attention Blocks (SAB and
ISAB) [21] that perform self-attention operations on a set and produce output containing information
about pairwise relations between elements. Note that these blocks are permutation equivariant, that
is, for any permutation π of elements in X = {xi}n
i=1, block(πX ) = πblock(X ). As a composition
of permutation equivariant blocks, the encoder is also permutation equivariant and captures higher-
order interaction features. The decoder aggregates features by a learnable pooling layer, Pooling
by Multihead Attention (PMA) [21], and send them through a SAB to get output. Since PMA is
a permutation invariant operator, and the rest of the operators (SAB or ISAB) are all permutation
equivariant, Set Transformer becomes a permutation invariant architecture.

4.2 Problem 1: Linear/Nonlinear Regression

Dataset: We follow a generative model y = wT φ(x)+(cid:15), where φ(·) is the feature map, w contains the
ground truth parameters of our regression problem, and (cid:15) denotes noise. For feature maps, in the linear
case we have φ(x) = [1, x]T and in the nonlinear case, we select φ(x) = [ρ(x − µ1), ..., ρ(x − µM )]
with ρ(x) being a radial basis function and {µm}M
m=1 form a grid in a predeﬁned interval. To generate
each dataset Xi, we ﬁrst sample the set cardinality Ni uniformly from a predeﬁned interval. Then,
we sample w, {(cid:15)n}Ni
For each model architecture and each learning setting (i.e., with and without solver in the LOOP)
we train our LOOP model 5 times and report the test MSE of our model, the solver, and the ground

n=1, and generate our (xi

n) pairs (train and test).

n=1, and {xn}Ni

n, yi

6

Figure 4: Sample qualitative results for our nonlinear regression problem compared with the Ground
Truth (GT) and the solver’s result. Note that the input set cardinality is a random variable and it varies
among these plots.

Figure 5: LOOP’s performance in predicting the principle components as measured by the cosine
similarity between the solver’s and our model’s outputs. We also provide the performance of the
network as a function of the input data cardinality. On the bottom is the visualization of the ﬁrst
eigenvector calculated by our LOOP model on four different problems with two random pairs of
digits. We can see that the network’s output is quantitatively and qualitatively aligned with the ﬁrst
principle component.

truth. Figure 4 shows sample qualitative results of our nonlinear regression experiments with ground
truth, solver, and LOOP results overlayed on the observed noisy data. In addition, for the Set
Transformer architecture, we report the test performance of our trained LOOP model and the solver
as a function of the number of training samples (Figure 3). We see that while for all architectures
LOOP is able to perform comparable with the solver, for the Set Transformer architecture the gap
between LOOP and the solver is the smallest.

4.3 Problem 2: Principal Component Analysis

Dataset: We used the MNIST [20] dataset to sample train and test sets. MNIST contains 60,000 train
and 10,000 test images of handwritten digits. The size of a single image is 28 × 28. During training,
we ﬁrst select pairs of random digits to sample images from. Then a random number of data ranging
from 500 to 1000 is uniformly sampled from the two digits to form the input set.

Given an input set Xi, our network aims to predict the top K = 5 eigenvectors of the input data. In
“solver in the LOOP" approach, the top K = 5 eigenvectors are calculated by the solver. Then our set
transformer [21] is trained to maximize the cosine similarities between the ground-truth eigenvectors
and the predicted ones. In our “no solver" approach, the set transformer maximizes the area under the
curve of the captured variances along the predicted eigenvectors. We train 5 different models for this
experiment and evaluated each model on 100 different test problems. For our metric, we calculate the
cosine similarities between the predicted vectors and the principle components obtained from the

7

Figure 6: Performance comparison between LOOP and the solver for our three different models
(left) under the two proposed learning settings. The y-axis represents the average Wasserstein
distance between the input distribution p and the coreset distribution qU , when the coreset are: 1)
random samples from the uniform distribution, 2) the output of the solver, and 3) the output of our
LOOP model. The plots on the right of each column show the performance of the Set Transformer
and the solver as a function of the number of training samples.

solver. The mean and standard deviation of the cosine similarities for each eigenvector is depicted
in Figure 5 (left). We also show the performance of the trained model as a function of different
number of training samples from 128 to 2048 (on the right). Results of “solver in the LOOP" and
“no solver" training are shown for the Set Transformer model in 5. Our network is able to effectively
predict the top principle components in all experiments, while having a higher ﬁdelity for the ones
with larger eigenvalues.

4.4 Problem 3: Transport-based coreset

Dataset: We generate datasets by drawing samples from random 2D Gaussian Mixture Models
(GMMs). We start by initializing a random number of Gaussians with random means but a ﬁxed
covariance matrix. Then, we draw random number of samples from each of these randomly initialized
Gaussians to generate our input sets X i.

Results: Results of the two training approaches “solver in the LOOP" and “no solver" using three
different models are shown in 6. Since the problem is equivalent to the free-support Wasserstein
barycenter problem, we used the solver from the Python Optimal Transport package [14] as the solver.
To compare the output of our LOOP model with the solver, we use W2(p, qU ) as our metric (the
lower the better). Also, to provide a reference for comparison, we also consider the Wasserstein
distance between the input distribution, p, and a uniform distribution in the input domain, ¯q, which
we refer to as Rand (equivalent to chance). We used the Sliced-Wasserstein distance (SWD) [18] as
the objective function in the “no solver" training, as SWD is signiﬁcantly faster to compute than the
Wasserstein distance. Finally, we compare the performance of LOOP and the solver as a function
of number of training samples in Figure 6.

4.5 Problem 4: Supply management in Cyber-Physical Systems

Dataset: We use the publicly available IEEE 2000-bus system data set [34] as the seed infromation to
generate hourly energy data for one week. We use different load proﬁles for weekdays and weekends
and randomly scale the original data. The scaling coefﬁcient lies between 0.95 and 1.05. This process
results in 24 × 7 data points. We use the data of odd hours for training and that of even hours for
testing. The IEEE 2000-bus system is a 2,000 nodes graph representing a realistic large scale electric
grid. This network consists of 1,125 demand nodes and 544 supply nodes.

n=1(un−u∗
n

Results: For solver in the LOOP, we use the mean squared error as the loss function, i.e., L =
(cid:80)N
n=1 are the solver’s output. We use the quadratic programming (QP)
solver of CVXPY library [12] as our solver. In our second learning setting, i.e., with no solver in the
LOOP, we include the optimization constraints in our objective as penalty terms. Therefore, the

. Here {u∗

n}N

n)2

8

Figure 7: LOOP ’s performance measured by the distance of its output from the optimal solution and
the feasible set We deﬁne the optimality distance as (cid:80)N
n=1 |un − u∗
n and un
refer to the solver’s and LOOP’s outputs. The feasibility distance (cid:80)N
n=1 uproj
n
where uproj
denotes projection of un onto the feasible set.
Table 1: Average computing time of using different solvers and LOOP approaches over ten runs.

n| / (cid:80)N
n=1 u∗
(cid:12)
(cid:12)un − uproj
n

n , where u∗
(cid:12)
(cid:12) / (cid:80)N

n=1

n

Method
ECOS Solver (CPU)
CVXOPT Solver(CPU)
OSQP Solver(CPU)
ECOS Solver(GPU)
CVXOPT Solver(GPU)
OSQP Solver(GPU)
Matpower 7.1 Solver

Time(s)
0.1237
1.8277
0.1421
0.1005
1.6288
0.1218
0.9609

s
r
e
v
l
o
S

Method
SetTransformer(CPU)
DeepSet-SWE(CPU)
DeepSet-GAP(CPU)
SetTransformer(GPU)
DeepSet-SWE(GPU)
DeepSet-GAP(GPU)

Time(s)
0.1284
0.0600
0.0538
0.0057
0.0070
0.0022

P
O
O
L

loss function will consist of three terms,

L =

N
(cid:88)

n=1

Cn(un) + λ1

(cid:32) N
(cid:88)

(cid:33)2

M
(cid:88)

xm

un −

n=1

m=1

+ λ2

(cid:2)(ReLU (un − un))2 + (ReLU (un − un))2(cid:3)

(6)

where λis are the penalty coefﬁcients. We use λ1 = 0.001, λ2 = 10 in our experiments.

In 6, the ﬁrst term represents the cost of electricity production. The second term ensures the equality
of supply and demand, and the third term enforces the the supply to be bounded. We bound the output
according to inequality constraints for testing. To quantify the performance of our LOOP model,
we report two metrics: 1) optimality, which measures how far we are from the solver’s output, and
2) feasibility, which measures the distance of LOOP’s output from the feasible set. We measure
feasibility distance by projecting the network’s output onto the feasible set and measuring the distance
between the original and project solutions.

Figure 7 shows the results for two LOOP approaches (solver in the LOOPand no solver) using
different models. The gap between our two learning settings for this problem is more signiﬁcant than
previous unconstrained ones.

This is because the LOOP model with no solver minimizes a combination of the objective function
and the penalty terms. Unlike the solver in the LOOP model (which could leverage optimality
information), the LOOP model with no solver does not establish any relation between feasibility
and optimality. We also present results of different penalty parameters for the LOOP model with
no solver in the supplemental materials, where the gap between the two LOOP approaches is
reduced by more careful tuning of penalty terms (λ1 and λ2). Moreover, Table 1 presents the
average computing time of different solvers (ECOS [13], CVXOPT [31], OSQP [29], Matpower 7.1
[36]), and LOOP over ten runs. Using GPU, three LOOP approaches outperform all solvers. On
CPU, the LOOP model of Set Transformer performs on par with the today’s solvers while other
LOOP models are noticeably faster.

9

4.6 Continual learning on LOOP

A promising capability of LOOP is to continually train the network to adjust to the change in the
input distribution. Note, LOOP (speciﬁcally in the no solver in the LOOP setting) is ripped for
continual learning. In short, the LOOP agent can evaluate the quality of its prediction (i.e., by
measuring the objective value or by checking the feasibility) and perform continual learning if the
prediction quality is degraded. We perform a continual learning experiment on LOOP , while the
agent is tasked to solve nonlinear regression problems where the frequency spectrum of the input
data drifts from Task 1 to Task 2. To overcome catastrophic forgetting, we use memory replay [30]
as one of the core bio-inspired mechanisms for overcoming catastrophic forgetting [19]. Table 2
demonstrates the application of LOOP in continual learning of non-linear regression under domain
shift. We see that memory replay enables LOOP to learn the new task (Task 2) while achieving
positive backward transfer on Task 1. More details are provided in the supplementary materials.
Lastly, the application of other continual learning mechanisms like regularization-based approaches
[11] and gradient projection approaches [27, 1], opens up an exciting research direction for future
work.

Table 2: Test MSE for both tasks after each training phase using LOOP (left) and LOOP with
memory replay (right). The models are trained on task 1 in phase 1 and on task 2 in phase 2.

test MSE after phase 1
test MSE after phase 2

Task 1 Task 2
4.805
0.091
0.130
0.287

test MSE after phase 1
test MSE after phase 2

Task 1 Task 2
4.805
0.091
0.136
0.088

5 Conclusion

This paper presents a novel alternative for existing iterative methods to solve optimization problems.
Speciﬁcally, this paper introduces LOOP (Learning to Optimize Optimization Process) framework,
which approximates the optimization process with a trainable parametric (set) function. Such a
function maps optimization inputs to the optimal parameters in a single feed forward. We proposed
two approaches for training LOOP; using a classic solver for providing ground truth (supervised
learning) and without a solver in the LOOP (self-supervised learning). The performance of the
proposed methods is showcased in the contexts of diverse optimization problems; (i) linear and
non-linear regression, (ii) principal component analysis, (iii) transport-based coreset, and (iv) supply
management in cyber-physical setups. We used three separate models in our experiments, namely deep
sets with global average pooling,deep sets with sliced-Wasserstein Embedding, and Set Transformers.
Our results supports that replacing optimization problem with a single forward mapping yields outputs
within a reasonable distance from commercial solvers’ solutions. LOOP holds the promise for the
next generation of optimization algorithms that improve by solving more optimization problems.

References

[1] Ali Abbasi, Parsa Nooralinejad, Vladimir Braverman, Hamed Pirsiavash, and Soheil Kolouri.
Sparsity and heterogeneous dropout for continual learning in the null space of neural activations.
arXiv preprint arXiv:2203.06514, 2022.

[2] Pankaj K Agarwal, Sariel Har-Peled, Kasturi R Varadarajan, et al. Geometric approximation

via coresets. Combinatorial and computational geometry, 52(1-30):3, 2005.

[3] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom
Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent
by gradient descent. In Advances in neural information processing systems, pages 3981–3989,
2016.

[4] Guha Balakrishnan, Amy Zhao, Mert R Sabuncu, John Guttag, and Adrian V Dalca. Voxelmorph:
a learning framework for deformable medical image registration. IEEE transactions on medical
imaging, 38(8):1788–1800, 2019.

[5] Yoshua Bengio. Gradient-based optimization of hyperparameters. Neural computation,

12(8):1889–1900, 2000.

[6] James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper-

parameter optimization. Advances in neural information processing systems, 24, 2011.

10

[7] Tianlong Chen, Xiaohan Chen, Wuyang Chen, Howard Heaton, Jialin Liu, Zhangyang
Wang, and Wotao Yin. Learning to optimize: A primer and a benchmark. arXiv preprint
arXiv:2103.12828, 2021.

[8] Yutian Chen, Matthew W Hoffman, Sergio Gómez Colmenarejo, Misha Denil, Timothy P
Lillicrap, Matt Botvinick, and Nando Freitas. Learning to learn without gradient descent by
gradient descent. In International Conference on Machine Learning, pages 748–756. PMLR,
2017.

[9] Sebastian Claici, Aude Genevay, and Justin Solomon. Wasserstein measure coresets. arXiv

preprint arXiv:1805.07412, 2018.

[10] Marco Cuturi and Arnaud Doucet. Fast computation of wasserstein barycenters. In International

conference on machine learning, pages 685–693. PMLR, 2014.

[11] Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Greg
Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classiﬁcation
tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.

[12] Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for

convex optimization. Journal of Machine Learning Research, 17(83):1–5, 2016.

[13] Alexander Domahidi, Eric Chu, and Stephen Boyd. Ecos: An socp solver for embedded systems.

In 2013 European Control Conference (ECC), pages 3071–3076, 2013.

[14] Rémi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurélie Boisbunon,
Stanislas Chambon, Laetitia Chapel, Adrien Corenﬂos, Kilian Fatras, Nemo Fournier, Léo
Gautheron, Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko,
Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander
Tong, and Titouan Vayer. Pot: Python optimal transport. Journal of Machine Learning Research,
22(78):1–8, 2021.

[15] Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings
of the 27th international conference on international conference on machine learning, pages
399–406, 2010.

[16] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization
for general algorithm conﬁguration. In International conference on learning and intelligent
optimization, pages 507–523. Springer, 2011.

[17] Boris Knyazev, Michal Drozdzal, Graham W Taylor, and Adriana Romero Soriano. Parameter
prediction for unseen deep architectures. Advances in Neural Information Processing Systems,
34, 2021.

[18] Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, and Gustavo K Rohde. Gener-

alized sliced wasserstein distances. arXiv preprint arXiv:1902.00434, 2019.

[19] Dhireesha Kudithipudi, Mario Aguilar-Simon, Jonathan Babb, Maxim Bazhenov, Douglas
Blackiston, Josh Bongard, Andrew P Brna, Suraj Chakravarthi Raja, Nick Cheney, Jeff Clune,
et al. Biological underpinnings for lifelong learning machines. Nature Machine Intelligence,
4(3):196–210, 2022.

[20] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/,

1998.

[21] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh.
Set transformer: A framework for attention-based permutation-invariant neural networks. In
International Conference on Machine Learning, pages 3744–3753. PMLR, 2019.
[22] Ke Li and Jitendra Malik. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016.
[23] Yuzhe Lu, Xinran Liu, Andrea Soltoggio, and Soheil Kolouri. Slosh: Set locality sensitive

hashing via sliced-wasserstein embeddings. arXiv preprint arXiv:2112.05872, 2021.

[24] Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter opti-
mization through reversible learning. In International conference on machine learning, pages
2113–2122. PMLR, 2015.

[25] Navid Naderializadeh, Joseph F. Comer, Reed W Andrews, Heiko Hoffmann, and Soheil
Kolouri. Pooling by sliced-wasserstein embedding. In A. Beygelzimer, Y. Dauphin, P. Liang,
and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021.

11

[26] Xiang Pan, Minghua Chen, Tianyu Zhao, and Steven H Low. Deepopf: A feasibility-
optimized deep neural network approach for ac optimal power ﬂow problems. arXiv preprint
arXiv:2007.01002, 2020.

[27] Gobinda Saha, Isha Garg, and Kaushik Roy. Gradient projection memory for continual learning.

In International Conference on Learning Representations, 2020.

[28] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine

learning algorithms. Advances in neural information processing systems, 25, 2012.

[29] Bartolomeo Stellato, Goran Banjac, Paul Goulart, Alberto Bemporad, and Stephen Boyd. Osqp:
An operator splitting solver for quadratic programs. Mathematical Programming Computation,
12(4):637–672, 2020.

[30] Gido M van de Ven, Hava T Siegelmann, and Andreas S Tolias. Brain-inspired replay for
continual learning with artiﬁcial neural networks. Nature communications, 11(1):1–14, 2020.
[31] Lieven Vandenberghe. The cvxopt linear and quadratic cone program solvers. Online:

http://cvxopt. org/documentation/coneprog. pdf, 2010.

[32] Ying Wei, Peilin Zhao, and Junzhou Huang. Meta-learning hyperparameter performance
prediction with neural processes. In International Conference on Machine Learning, pages
11058–11067. PMLR, 2021.

[33] Olga Wichrowska, Niru Maheswaranathan, Matthew W Hoffman, Sergio Gomez Colmenarejo,
Misha Denil, Nando Freitas, and Jascha Sohl-Dickstein. Learned optimizers that scale and
generalize. In International Conference on Machine Learning, pages 3751–3760. PMLR, 2017.
[34] Ti Xu, Adam B Birchﬁeld, Kathleen M Gegner, Komal S Shetye, and Thomas J Overbye.
Application of large-scale synthetic power system models for energy economic studies. In
Proceedings of the 50th Hawaii International Conference on System Sciences, 2017.

[35] Manzil Zaheer, Satwik Kottur, Siamak Ravanbhakhsh, Barnabás Póczos, Ruslan Salakhutdinov,
and Alexander J Smola. Deep sets. In Proceedings of the 31st International Conference on
Neural Information Processing Systems, pages 3394–3404, 2017.

[36] Ray Daniel Zimmerman, Carlos Edmundo Murillo-Sánchez, and Robert John Thomas. Mat-
power: Steady-state operations, planning, and analysis tools for power systems research and
education. IEEE Transactions on power systems, 26(1):12–19, 2010.

12

