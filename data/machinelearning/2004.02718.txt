1
2
0
2

n
a
J

0
1

]
T
S
.
h
t
a
m

[

2
v
8
1
7
2
0
.
4
0
0
2
:
v
i
X
r
a

Low-Rank Matrix Estimation From Rank-One Projections by
Unlifted Convex Optimization

Sohail Bahmani∗
sohail.bahmani@ece.gatech.edu

Kiryung Lee†
lee.8763@osu.edu

January 12, 2021

Abstract

We study an estimator with a convex formulation for recovery of low-rank matrices from
rank-one projections. Using initial estimates of the factors of the target d1 × d2 matrix of
rank-r, the estimator admits a practical subgradient method operating in a space of dimension
r(d1 + d2). This property makes the estimator signiﬁcantly more scalable than the convex esti-
mators based on lifting and semideﬁnite programming. Furthermore, we present a streamlined
analysis for exact recovery under the real Gaussian measurement model, as well as the partially
derandomized measurement model by using the spherical t-design. We show that under both
models the estimator succeeds, with high probability, if the number of measurements exceeds
r2(d1 + d2) up to some logarithmic factors. This sample complexity improves on the existing
results for nonconvex iterative algorithms.

1 Introduction

We consider the problem of estimating a matrix M0 ∈ (cid:67)d1×d2 of known rank r (cid:28) min{d1, d2} from
rank-one “sketches” of the form

mi = a∗

i M0bi ,

i = 1, . . . , n ,

(1)

for random vectors ai ∈ (cid:67)d1 and bi ∈ (cid:67)d2 drawn from certain distributions. More speciﬁcally, given
the observations {(ai, bi, mi)}n
i=1, the goal is to estimate factors X0 ∈ (cid:67)d1×r and Y0 ∈ (cid:67)d2×r of
M0 (i.e., M0 = X0Y ∗

0 ).

Depending on the distribution of (ai, bi)i, the observation model (1) can describe various low-
rank matrix recovery problems including matrix completion [11, 25, 30, 28, 46], phase retrieval
[12, 13], blind deconvolution and calibration [1, 10], and sketching [27, 9, 7, 23] to name a few. There
are various algorithms proposed in the literature for these problems that can be broadly categorized
as follows: the algorithms based on semideﬁnite relaxation [11, 25], the iterative methods based on
variants of nonconvex gradient descent [30, 46, 37], alternating minimization [28], or approximate
message passing [41]. We refer the interested reader to the survey papers [16] and [15] for a broader
view of the low-rank matrix recovery literature.

Semideﬁnite relaxations of the low-rank matrix recovery provide the state of the art sample
complexity with linear scaling in the rank and no dependence on the condition number. From
the algorithmic perspective, however, these methods suﬀer from a high computational cost, and

∗School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA 30332
†Department of Electrical and Computer Engineering, The Ohio State University, Columbus, OH 43210

1

 
 
 
 
 
 
more importantly memory usage. This drawback motivated a suite of nonconvex approaches with
comparable but weaker sample complexity guarantees [15]. Another line of research, originally
studied for phase retrieval, proposed recovery by a convex program that avoids lifting and the
semideﬁnite constraint altogether [3, 24, 4, 2]. This new convex approach admits inherent robustness
due to convex geometry of the optimization formulation together with ﬂexibility in adopting a
number of oﬀ-the-shelf numerical convex optimization algorithms. We apply this framework to the
low-rank desketching problem in this paper. We extend and streamline the analysis of the special
case of rank-1 recovery with Gaussian factors provided in [2], to a more general low-rank recovery
problem.

1.1 Anchored regression

Let fX0 and fY0 be a pair of matrices for which fX0fY ∗
With h·, ·i being used throughout the paper to denote the real-valued inner product deﬁned as

0 approximates the ground truth matrix M0.

hU , V i def= Re (tr(U ∗V )) ,

our proposed estimator is formulated as

( cX, cY ) ∈

argmax
X∈(cid:67)d1×r,Y ∈(cid:67)d2×r

h fX0, Xi + hfY0, Y i −

1
n

n
X

i=1

‘i(X, Y ) ,

(2)

where

‘i(X, Y ) def=

1
2

kX ∗aik2 +

1
2

kY ∗bik2 + |a∗

i XY ∗bi − mi| .

The optimization in (2) is eﬀectively a convex program and can be solved eﬃciently. To clarify

this fact, observe that the functions ‘i(X, Y ) can be written equivalently as

‘i(X, Y ) = sup

φ : |φ|=1

= sup

φ : |φ|=1

1
2
1
2

kX ∗aik2 +

1
2

kY ∗bik2 + Re(φ (a∗

i XY ∗bi − mi))

(3)

kX ∗ai + φ Y ∗bik2 − Re(φ mi)

For any ﬁxed φ, the argument of the supremum is clearly convex in [X; Y ] ∈ (cid:67)(d1+d2)×r. Therefore,
‘i(X, Y ) is also a convex function of [X; Y ], meaning that (2) is a convex program.

Because of the speciﬁc form of the loss functions ‘i(X, Y ), the estimator in (2) can be viewed
as a “convexiﬁcation” of the (nonconvex) least absolute deviation (LAD) estimator by quadratic
regularization. Previously, [2] has studied similar estimators for observations in the form of diﬀer-
ence of convex functions, with the bilinear observations for rank-1 matrices as a special case. In
this paper we provide a streamlined analysis of the estimator tailored to desketching of a low-rank
matrix from its rank-one measurements in (1). The following provides the high-level description
of the sample complexity we have established for the anchored regression estimator. The precise
statements are provided in Theorems 1 and 2.

≈ M0, with
Theorem. Given a “good” approximation of the unknown rank-r matrix M0 as fX0 fY0
high probability, the proposed anchored regression recovers M0 exactly, from O(r(d1+d2) polylog(d1+
d2)) random rank-one measurements. The hidden constant in the sample complexity depends also
on the “low-rank condition number” of M0.

∗

The suﬃcient number of samples for the exact recovery provided by the theorem is near optimal
compared to the degrees of freedom of the rank-r matrix model. The random sketching models and
the size of the neighborhood will be speciﬁed in the following section.

2

1.2 Sketching Models

We study the sample complexity of our proposed estimator under two diﬀerent random measure-
ments models. The ﬁrst model, which we refer to as the real Gaussian sketching, simply uses the
outer product of two independent Gaussian vectors as the rank-one sketching matrix. The second
model, called the partially derandomized sketching, mimics the behavior of the ﬁrs model but the
random rank-one sketching matrix takes realizations from a ﬁnite set. More precisely, the ﬁrst few
moments of the sketching matrix of the second model are designed to coincide with those of the
real Gaussian model.

1.2.1 Real Gaussian sketching

This model simply considers the random vectors [ai; bi] ∈ (cid:82)d1+d2 to be independent copies of a
random vector [a; b] ∈ (cid:82)d1+d2 satisfying

[a; b] ∼ N (0, Id1+d2) .

(4)

1.2.2 Partially derandomized sketching

Next we consider a measurement distribution supported on a special ﬁnite set whose ﬁrst 2t mo-
ments coincide with those of the real Gaussian model. A conﬁguration of such a set is called a
complex projective t-design and designs were ﬁrst introduced by Delsarte et al. [19]. A variant of
this model has been previously employed for the phase retrieval [5, 26, 32]. The concept of t-design
has been utilized in coding theory and quantum information theory, particularly in the analysis of
randomized algorithms. One drawback is the size of the set generally grows exponentially in the
dimension while the exponent is proportional to the parameter t. Concrete constructions are widely
available for the special case with degree 2 and numerical algorithms for an approximate design for
higher order are also available in the literature. We refer to [26] for more details on the t-design
model and the related references. Below we describe the version of the model that is relevant for
our purposes.

Let PSymt denote the totally symmetric subspace of ((cid:67)d)⊗t such that all elements are invariant
under every possible permutation of t factors (see, e.g., [33]). Then a weighted t-design is deﬁned
as follows [26].

Deﬁnition 1. Let t ∈ (cid:78) and w1, . . . , wN ∈ (cid:67)d be unit vectors. The set {wi}N
i=1 such that pi ≥ 0 for all i = 1, . . . , N , and PN
weights {pi}N
projective t-design of dimension n and cardinality N , if

i=1 with corresponding
i=1 pi = 1 is a weighted complex

N
X

i=1

pi (wiw∗

i )⊗t =

!−1

d + t − 1
t

PSymt ,

where PSymt denotes the projector onto the totally symmetric subspace Symt of ((cid:67)d)⊗t.

Our second sketching model is given by the concatenation of two independent random vectors
i=1 in (cid:67)d1 with t ≥ 2,

a and b in the following construction: Given a weighted t-design {(wi, pi)}N1
let a be a random vector given by
n
a = pd1wi

i = 1, . . . , N1 .

= pi,

(cid:80)

o

Then a satisﬁes

(cid:69)(aa∗)⊗t = dt
1

!−1

d1 + t − 1
t

PSymt .

3

(5)

(6)

 
 
(7)

(8)

Similarly, given a weighted t-design {(w0
by

i, p0

i)}N2

i=1 in (cid:67)d2 with t ≥ 2, let b be a random vector given

n
b = pd2w0

o

= p0
i,

i = 1, . . . , N2 .

i

(cid:80)

Then b satisﬁes

(cid:69)(bb∗)⊗t = dt
2

!−1

d2 + t − 1
t

PSymt .

By construction, a weighted t-design is automatically a weighted t0-design for all t0 ≤ t. Given
accurate anchor matrices, a performance guarantee for (2) can be derived only requiring the moment
conditions of up to the 4th order. However, obtaining such accurate anchor matrices require a more
stringent condition on higher moments of up to order 2t = Ω(log(d1 + d2)). This is why we assume
that t ≥ 2 in constructing partially derandomized measurement vectors with t-designs.

1.3 Spectral initialization

Our main results rely on the availability of fX0 and fY ∗
0 is close to the ground truth
matrix M0. To provide a stand-alone theory that does not require any oracle information, we also
analyze a speciﬁc method to obtain such matrices fX0 and fY ∗

0 such that fX0fY ∗

0 described below.

Let A : (cid:67)d1×d2 → (cid:67)n denote the linear operator representing the rank-one measurements in

(1), i.e., it is deﬁned by

(cid:18) 1
√
n
Then its adjoint operator, denoted by A∗, is given by

M 7→ A(M ) =

a∗

i M bi

(cid:19)n

i=1

.

y 7→ A∗(y) =

1
n

n
X

i=1

yiaib∗
i .

The spectral method computes an estimate fM0 of the unknown matrix M0 as the best rank-r
approximation of A∗A(M0) with respect to the Frobenius norm. Under the two random sketching
models, we obtain suitable upper bounds on the approximation error through matrix concentration
inequalities.

Next we factorize the estimated matrix into fM0 = fX0fY ∗

0 through the singular value decompo-
0 be the compact singular value decomposition of fM0. Then we choose

sition. Let fM0 = fU0 fΣ0fV ∗
fX0 and fY0 by

fX0 = fU0 fΣ1/2

0

and fY0 = fV0 fΣ1/2

0

,

so that they have the same singular values. This particular decomposition provides a set of useful
properties, such as the identity

fΣ0 = fX ∗

0 fX0 = fY ∗

0 fY0 ,

that are utilized in the proof of our main results.

The precise statement of the requirements for the spectral initialization, and the corresponding
sample complexity under the two considered measurement models are provided in Section 2. The
pertaining derivations are provided in Section 6.

4

 
1.4 Discussion and Related work

Under the real Gaussian sketching model and given an initial estimate satisfying (9), we demonstrate
that, with high probability, the estimator in (2) recovers M0 exactly, provided the number of
measurements scales as n ≥ Cdr, where d = max(d1, d2). This sample complexity coincides with
the sample complexity achieved by the estimators based on lifting and semideﬁnite relaxation [14, 9].
On the other hand, our estimator is formulated through an explicit factorization only with r(d1+d2)
variables while the lifted convex estimator over d1d2 variables [14, 9]. Furthermore, because the
methods based on semideﬁnite relaxation do not operate in the factorized domain, they often need
singular value calculations which further complicates their scalability.

However, computationally inexpensive methods used to ﬁnd the initial estimates obeying (9),
often lead to a suboptimal overall sample complexity. In fact, we show that, with high probability,
the spectral initialization succeeds if n ≥ Cdr2 which dominates the sample complexity n ≥ Cdr
for the “oracle-assisted” estimator mentioned above.

The iterative hard thresholding algorithm is studied under variants of the restricted isometry
property for low-rank matrices in [28] and [23]. This algorithm is computationally less expensive
than the generic convex optimization algorithms that solve the semideﬁnite relaxation, because it
only requires to perform low-rank SVDs in its iterations rather than the full SVDs. However, the
fact that the iterative hard thresholding method operates in the lifted domain, is an obstruction
to its scalability. Several other iterative algorithms have been proposed and analyzed under the
real Gaussian sketching model. Earlier methods used resampling to draw fresh measurements per
iteration. Therefore, these methods need to terminate after ﬁnitely many iterations, which only
allows for approximate recovery up to a prescribed accuracy (cid:15). Prior work on this approach achieve
the sample complexities O(dr4 log2 d log(1/(cid:15))) [51], O(dr3 log(1/(cid:15))) [36], and O(dr2 log4 d log(1/(cid:15)))
[44]. In more recent work, [42] and [35] studied performance of the nonconvex gradient descent
and established the sample complexities O(dr6 log2 d) and O(dr4 log d), respectively. Our estimator
outperforms these results for nonconvex approaches. In fact, our estimator would have achieved
the ideal sample complexity should there be an initialization with the sample complexity O(dr).
The hidden constant in this sample complexity, similar to the sample complexity of the nonconvex
methods, depends on the “low-rank condition number” of the ground truth matrix, deﬁned precisely
It is also worthwhile to mention that the existing results in the literature
below in Section 2.
often focus on the case where the rank-one measurement matrices or the ground truth matrix are
symmetric. The model (1) considered in this paper allows for a general choice of the measurement
factors ai and bi. For simplicity, we only consider independent factors ai and bi, but the provided
framework can be adapted to the case of dependent factors by modifying some of the relevant
calculations.

2 Main results

Our main results demonstrate how many observations suﬃce for the estimator (2) to reconstruct
the unknown matrix X0Y ∗
0 . Our ﬁrst theorem provides a sample complexity that guarantees
accuracy of the estimator (2) under the real Gaussian sketching model. Throughout we use κ ≥ 1
to denote the (low-rank) condition number of M0, which refers to the ratio of the largest and
smallest non-zero singular values of M0, i.e.,

κ =

σ1(M0)
σr(M0)

.

5

Theorem 1 (Real Gaussian desketching). Let ([ai; bi])n
N (0, Id1+d2). Let fX0 ∈ (cid:67)d1×r and fY0 ∈ (cid:67)d2×r be matrices that satisfy fX ∗

i=1 be independent copies of [a; b] ∼
0 fX0 = fY ∗

0 fY0 and

(cid:13)
(cid:13) fX0fY ∗
(cid:13)

0 − M0

(cid:13)
(cid:13)
(cid:13)

(cid:46) r−1/2κ−2kM0k .

If the number of measurements n obeys

n (cid:38) max{κr(d1 + d2), log(1/δ)} ,

(9)

(10)

then with probability at least 1 − δ the estimates cX and cY obtained by the anchored regression
satisfy cXcY ∗ = M0.

The result by Theorem 1 is comparable to the analogous result for the lifted convex optimization
by nuclear norm minimization [14, 9]. However, the dependence on the condition number, which
does not appear in the lifted case, is the cost we need to pay to save the computation through
explicit factorization.

We also present the sample complexity for the success of the spectral initialization under the

same model.

Proposition 1. Let ([ai; bi])n
( fX0, fY0) by the spectral initialization satisﬁes (9) with probability at least 1 − (d1 + d2)−α, if

i=1 be independent copies of [a; b] ∼ N (0, Id1+d2). Then the estimate

n (cid:38) α3κ4r2(d1 + d2) log3(d1 + d2) .

As shown in Theorem 1 and Proposition 1, the number of samples enough for the success of
the spectral initialization dominates that for the estimator. Although the spectral initialization is
just one approach to obtain an initial estimate satisfying (9), it has not been shown any alternative
practical method providing the same accuracy from fewer measurements.

Next we present the corresponding results for partially derandomized sketching below.

Theorem 2 (Partially derandomized desketching). Let a and b are independent random vectors
uniformly distributed over the corresponding t-design sets according to (5) and (7) with t ≥ 2. Let
([ai; bi])n
i=1 be independent copies of [a; b]. Let fM0, fX0, and fY0 as in Theorem 1 satisfying (9).
If the number of measurements n satisﬁes

n (cid:38) κr(d1 + d2) max{log(d1 + d2), log(1/δ)} ,

(11)

then the anchored regression exactly recovers M0 with probability at least 1 − δ.

Proposition 2. Under the sketching model in Theorem 2, the spectral initialization provides
( fX0, fY0) satisfying (9) with probability at least 1 − (d1 + d2)−α provided

n (cid:38) tκ4r2(d1 + d2)1+1/2t+α/t ∨

(cid:16)

t2κ2r3/2(d1 + d2)1+α/2t(cid:17)2t/(2t−1)

.

(12)

Particularly, if t ≥ (α + 1/2) ln(d1 + d2), then the condition in (12) simpliﬁes to

n (cid:38) α2κ4r2(d1 + d2) ln4(d1 + d2) .

Compared to the real Gaussian sketching model, the derandomized case is guaranteed by slightly

more measurements (larger by a logarithmic factor).

6

3 Numerical Results

A set of Monte Carlo numerical results are provided to illustrate that the empirical behavior of
the estimator is consistent with the main theoretical results. We ﬁrst discuss how the convex
program in (2) can be solved by a practical numerical algorithm. Recall that the estimator in (2)
is equivalently written as

( cX, cY ) ∈

argmin
X∈(cid:67)d1×r,Y ∈(cid:67)d2×r

f (X, Y ) ,

(13)

where the convex objective function is given by

f (X, Y ) = −h fX0, Xi − hfY0, Y i −

1
n

n
X

i=1

(cid:18) 1
2

kX ∗aik2 +

1
2

kY ∗bik2 + |a∗

(cid:19)
i XY ∗bi − mi|

.

A simple subgradient method can be used to ﬁnd a minimizer to (13). The estimator is reﬁned

by

"

Xt+1
Yt+1

#

=

#

"
Xt
Yt

− ηtGt ,

where ηt denotes the step size at the tth iteration and Gt ∈ (cid:67)(d1+d2)×r is a subgradient of f at
[Xt Yt] speciﬁcally given by

Gt = −

#

"

fX0
fY0

+

1
n

"

n
X

i=1

aia∗
i
φibia∗
i

φ∗
i aib∗
i
bib∗
i

# "

Xt
Yt

#

,

where

φi = exp (ı Arg(a∗

i XtY ∗

t bi − mi)) ,

√

−1 denoting imaginary unit, and Arg (z) denoting the principal argument of z ∈ (cid:67).
with ı =
The per-iteration-cost of this subgradient method is comparable to that of the nonconvex gradient
descent. We use the diminishing step size rule for (ηt)t. We chose the simplest algorithm to solve
the convex program in (13). However, we believe that more sophisticated optimization algorithms
are also applicable to our problem. For example, in expression of ‘i(·, ·) as (3) the constraint |φ| = 1
can be relaxed to |φ| ≤ 1 without aﬀecting the function value. Then, we can show that the proposed
estimator can be equivalently formulated as a convex-concave saddle-point problem, which can be
solved using algorithms based on mirror descent and mirror prox [45, Chs. 5 and 6], [8, Ch. 5].

The ﬁrst set of simulation provides the empirical phase transition as a function of the rank while
the other parameters are ﬁxed (d1 = d2 = 128 and n = d1d2/4). The measurements are generated
is close enough to
by the standard complex normal distribution. Theorem 1 shows that, if gX0 fY0
the ground-truth matrix M0, the maximum rank that leads to the exact recovery is determined
by (10). In order to consider the eﬀect of the accuracy of the anchor, we introduce a parameter
α ∈ [0, 1] to linearly interpolate between the spectral initialization, corresponding to α = 1, and
the ground truth, corresponding to α = 0. Figure 1 illustrates the empirical phase transitions,
with the left and right panels respectively showing the median and 90th percentile of the relative
error over 100 trials. Exact recovery is achieved in most cases for r ≤ 2. The top rows in Figure 1
shows the empirical recovery phase transition when (gX0, fY0) is given by the spectral initialization,
whereas the bottom rows correspond to the ground truth chosen as the anchor. Figure 1 suggests
that at larger r the phase transition occurs at smaller α, thereby requiring a more accurate initial
estimate gX0 fY0

, which is consistent with the requirement in (9).

∗

∗

7

(a) Median

(b) 90th percentile

Figure 1: Empirical phase transition in the noiseless case. The logarithm base 10 of percentiles
of the normalized estimation error k cXcY ∗ − M0kF/kM0kF is plotted. The size of matrix and the
number of measurements are set to d1 = d2 = 128 and n = d1d2/4, respectively. The anchor matrix
is computed from a convex combination of the rank-r matrix in the spectral initialization and the
ground truth respectively with weights α and 1 − α. Thus α = 1 denotes the case with the spectral
initialization.

The second simulation illustrates how the additive noise to measurements propagates to the
estimation error. We consider the regime of parameters where the convex estimator provides exact
recovery in the noiseless case. We set d1 = d2 = 128, r = 2, and n = d1d2/4. The 90th percentile
of 100 realizations was observed while the signal-to-noise ratio (SNR) varies over 5 to 50 dB.
Figure 2 shows that the estimation error decreases gradually as SNR increases. In other words, the
estimation error in the presence of measurement noise scales smoothly as a function of SNR. This
stable performance of the estimator is due to the nice geometry of the convex program in (13).

Figure 2: Estimation error for varying SNR. The observation is corrupted with additive Gaussian
noise so that mi = a∗
i M0bi + ζi for i = 1, . . . , n with ζ1, . . . , ζn being i.i.d. N (0, σ2). SNR is deﬁned
as 10 log10(Pn
i /σ2). The 90th percentile of the normalized estimation error is plotted. The
i=1 m2
size of matrix, rank, and number of measurements are set to d1 = d2 = 128, r = 2, and n = d1d2/4,
respectively. The anchor matrix is computed by the spectral initialization.

8

4 Proof of the Main Theorems

We prove Theorems 1 and 2 in two steps. First, we derive a set of deterministic conditions to
guarantee exact recovery for the proposed estimator. Then, we show that these conditions hold,
with high probability, for the sketching models introduced in Section 1.2.

4.1 A deterministic suﬃcient condition for exact recovery

Recall that M0 = U0Σ0V ∗
0 is the compact singular decomposition of the ground truth matrix M0,
where both U0 ∈ (cid:67)d1×r and V0 ∈ (cid:67)d2×r have orthonormal columns, and Σ0 ∈ (cid:82)r×r is the diagonal
matrix of the singular values. The support space of M0, denoted by T , is deﬁned as

T def=

n
∆1V ∗

0 + U0∆∗

o
2 : ∆i ∈ (cid:67)di×r, i = 1, 2

.

With these notations, the following proposition provides a set of deterministic suﬃcient conditions
for the estimator (2) to exactly recover M0. The proof is deferred to Section 5.

Proposition 3. Let r and κ respectively denote the rank and the condition number of M0, and
( fX0, fY0) be a given pair of matrices that satisfy fX ∗
0 fY0. Furthermore, suppose that there
exist ρ ∈ (0, 1], and absolute constants C1, C2 ≥ 1 such that

0 fX0 = fY ∗

1
n

n
X

i=1

|haib∗

i , Hi| ≥ ρkHkF,

for all H ∈ T ,

(cid:13)
(cid:13)
(cid:13)
Id1 −
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Id2 −
(cid:13)
(cid:13)

1
n

1
n

n
X

aka∗
k

k=1
n
X

k=1

bkb∗
k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

≤

ρ
√

ρ
√

rκ

rκ

,

,

C1

C1

(cid:13)
(cid:13) fX0fY ∗
(cid:13)

0 − M0

(cid:13)
(cid:13)
(cid:13) ≤

ρkM0k
√
rκ2 .
C2

(14)

(15)

(16)

(17)

and

Then, the maximizer ( cX, cY ) to (2) is unique and satisﬁes cXcY ∗ = M0.

4.2 Verifying the suﬃcient condition under the random models

We demonstrate that under the random measurement models introduced in Section 2 the assump-
tions made in Proposition 3 hold with high probability.

4.2.1 Small-ball method

Our analysis is based on the small-ball method [31, 39]. A simple exposition and some applications
of this method can be found in [48] and [21]. We provide the proofs for the manuscript to be
self-contained as well as addressing some subtle but important diﬀerences due to operation in the
complex domain.

We ﬁrst show in the following proposition that (14) is satisﬁed with high probability.

9

Proposition 4 (Lower-tail via the small-ball method). Let T be a subset of (cid:67)d1×d2 that is invari-
ant under multiplication by unit-modulus scalars. For i.i.d. and isotropic random vectors [a; b],
[a1; b1], . . . , [an; bn] ∈ (cid:67)d1+d2 deﬁne

pτ (T ) def= inf

H∈T \{0}

(cid:80) {|a∗Hb| ≥ τ kHkF} ,

Cn(T ) def= (cid:69) sup

H∈T \{0}

1
√
n

n
X

εihaib∗

i , Hi

i=1

kHkF

,

where ε1, . . . , εn are i.i.d. Rademacher random variables independent of everything else. Then, for
any τ > 0 and δ ∈ (0, 1), with probability at least 1 − δ, we have

inf
H∈T \{0}

1
n

n
X

i=1

|a∗
i Hbi|
kHkF

≥ τ pτ (T ) −

π Cn(T )
√
n

− τ

s

log(1/δ)
n

.

Proof. Using [z]≤t

def= min{z, t} to denote the “saturation” at t, for any τ > 0, we have

1
n

n
X

i=1

|a∗

i Hbi| ≥

1
n

n
X

i=1

[|a∗

i Hbi|]≤τ kHkF

,

(18)

(19)

for every H ∈ (cid:82)d1×d2. By normalizing by kHkF, it suﬃces to ﬁnd a lower bound for the right-hand
side of (19) for all H ∈ T T (cid:83), where (cid:83) denotes the unit sphere of the Frobenius norm in (cid:67)d1×d2.

Adding and subtracting (cid:69)

, and using the fact that (cid:69)

(cid:16)

|a∗

i Hbi|≤τ

(cid:17)

(cid:16)

[|a∗

i Hbi|]≤τ

(cid:17)

we can write

1
n

n
X

i=1

[|a∗

i Hbi|]≤τ ≥

1
n

−

n
X

τ (cid:80) (|a∗

i Hbi| ≥ τ )

i=1
n
1
X
n

i=1

(cid:16)

(cid:69)

[|a∗

i Hbi|]≤τ

(cid:17)

− [|a∗

i Hbi|]≤τ .

≥ τ (cid:80) (|a∗

i Hbi| ≥ τ ),

(20)

The function F : ((cid:67)d)n → (cid:82)≥0 deﬁned as

F ([a1; b1], . . . , [an; bn]) def= sup

H∈T T (cid:83)

1
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
X

i=1

(cid:16)

(cid:69)

[|a∗

i Hbi|]≤τ

(cid:17)

− [|a∗

i Hbi|]≤τ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

has the bounded diﬀerence property. Therefore, invoking the bounded diﬀerence inequality [38],
with probability at least 1 − δ, we have

F ([a1; b1], . . . , [an; bn]) ≤ (cid:69)F ([a1; b1], . . . , [an; bn]) + τ

s

log(1/δ)
2n

.

(21)

Using the standard Giné-Zinn symmetrization argument (e.g., see [49, Lemma 2.3.1]), the expec-
tation on the right-hand side of (21) can be bounded as

(cid:69)F ([a1; b1], . . . , [an; bn]) ≤

2
√
n

(cid:69)


 sup

H∈T T (cid:83)

1
√
n

n
X

i=1

εi[|a∗

i Hbi|]≤τ


 ,

10

where the expectation on the right-hand side is with respect to [a1; b1], . . . , [an; bn] as well as
the i.i.d. Rademacher random variables ε1, . . . , εn. Since the function z 7→ [|z|]≤τ is 1-Lipschitz,
invoking the Rademacher contraction principle [34, Theorem 4.12] yields


 sup

H∈T T (cid:83)

(cid:69)

1
√
n

n
X

i=1

εi[|a∗

i Hbi|]≤τ


 ≤ (cid:69)


 sup

H∈T T (cid:83)

1
√
n

n
X

i=1

εi |a∗

i Hbi|


 .

(22)

Let φ be a unit-modulus scalar in (cid:67) that is selected uniformly at random, and (cid:69)φ denote the
expectation with respect to φ conditioned on everything else. Straightforward calculus shows that
for any z ∈ (cid:67) we have |z| = (π/2)(cid:69)φ (|Re(φ∗z)|). Applying this identity in (22) then yields


 sup

H∈T T (cid:83)

(cid:69)

1
√
n

n
X

i=1

εi[|a∗

i Hbi|]≤τ


 ≤

≤

≤

≤

π
2

π
2

π
2

π
2


 sup

(cid:69)

H∈T T (cid:83)

 sup

H∈T T (cid:83)

(cid:69)(cid:69)φ



εi(cid:69)φ |haib∗

i , φHi|



1
√
n

n
X

i=1



εi |haib∗

i , φHi|



1
√
n

n
X

i=1


 sup

H∈T T (cid:83)

(cid:69)

1
√
n

n
X

i=1

Cn(T ) ,



εi |haib∗

i , Hi|



where the second, third, and fourth lines follow respectively from the Jensen’s inequality, the
assumption that T is invariant under multiplication by unit-modulus scalars, and applying the
Rademacher contraction principle once more.

Furthermore, since [a1; b1], . . . , [an; bn] ∈ (cid:67)d1+d2 are identically distributed, we have

1
n

n
X

i=1

(cid:80) (|a∗

i Hbi| ≥ τ ) = (cid:80) (|a∗Hb| ≥ τ ) .

(23)

Therefore, in view of (20), (21), and (23), with probability at least 1 − δ, for all H ∈ T ∩ (cid:83) we

have

1
n

n
X

i=1

[|a∗

i Hbi|]≤τ ≥ τ (cid:80) (|a∗Hb| ≥ τ ) −

π Cn(T )
√
n

− τ

s

log(1/δ)
2n

.

Recalling the deﬁnition of pτ (T ) is enough to complete the proof.

We apply Proposition 4 under the assumptions in either of Theorems 1 and 2. Then (14) is
satisﬁed with high probability provided that the right-hand side of (18) is lower bounded by a
nonnegative scalar ρ. The following lemmas provides a lower (resp. upper) bound on pτ (T ) (resp.
Cn(T )). The proofs are provided in Appendix sections B.1 and B.2.

Lemma 1 (Lower bound on probability). Let [a; b] to be a random vector drawn either according
to (4), or the pair (5) and (7). Then

for an absolute constant c > 0.

pτ (T ) ≥ c(1 − τ 2)2

11

Lemma 2 (Upper bound on Rademacher complexity). Let [a; b] satisfy that i) a and b are
independent; ii) each of a and b is isotropic. Then

q

Cn(T ) ≤

(d1 + d2)r ,

By plugging in the results of the above lemmas, a suﬃcient condition for satisfying (14) with

probability 1 − δ is given by

c(1 − τ 2)2 −

4p(d1 + d2)r
√
n

− τ

s

log(1/δ)
2n

≥ ρ .

(24)

Given ρ, by choosing C in the assumption n ≥ C max{r(d1 + d2), log(1/δ)} large enough and

by choosing τ small we obtain that (24) holds with probability 1 − δ.

4.2.2 Approximate Isotropy

Next we show that (15) and (16) are satisﬁed with high probability for both the Gaussian and
t-design cases. To simplify the notation, let η denote the right-hand side of (15), which coincides
with that of (16), i.e., η = ρ/C1

rκ. We are interested in the regime where 0 < η < 1.

√

In the Gaussian case, the concentration of extreme singular values of a Wishart matrix has been
well studied in the literature (e.g., see [17, Theorem II.13], which is summarized as Theorem 3 in
Appendix). If [a; b] is a standard Gaussian random vector, then we have

and






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
X

i=1

(cid:80)






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
X

i=1

(cid:80)

aia∗

i − Id1



s

> 3 max



(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

4d1
n

,

4d1
n










bib∗

i − Id2



s

> 3 max



(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

4d2
n

,

4d2
n










≤ 2 exp(−d1/2)

≤ 2 exp(−d2/2) .

Therefore, (15) and (16) are satisﬁed with probability 1 − δ if

n ≥ max{36η−2(d1 + d2), 2 log(4/δ)} = max{C2

1 ρ−2κr(d1 + d2), 2 log(4/δ)} ,

which is implied by (10) in Theorem 1.

In the t-design case with t ≥ 2, we obtain a similar result via the matrix Bernstein inequality
[47, Theorem 1.6], summarized as Theorem 4 in the appendix. If [a; b] satisfy (5) and (7), then
we have

(cid:80)

((cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
X

i=1

aia∗

i − Id1

> η

≤ 2d1 exp

)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

!

−η2n
4d1

and

(cid:80)

((cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
X

i=1

bib∗

i − Id2

)

> η

≤ 2d2 exp

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

!

.

−η2n
4d2

Therefore, (15) and (16) are satisﬁed with probability 1 − δ if

n ≥

4(d1 + d2)
η2

· log

(cid:18) 4(d1 + d2)
δ

(cid:19)

=

4C2

1 κr(d1 + d2)

ρ2

· log

(cid:18) 4(d1 + d2)
δ

(cid:19)

,

which is implied by (11) in Theorem 2.

12

 
 
5 Proof of Proposition 3

For conciseness, we introduce the following shorthand notations. Let

h

A =

a1 a2

. . . an

i

,

h

B =

b1 b2

. . .

i

,

bn

and deﬁne

Then, (15) and (16) are equivalent to

η def=

ρ
√

rκ

.

C1

(cid:13)
(cid:13)
Id1 −
(cid:13)
(cid:13)

1
n

AA∗

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ η

and

(cid:13)
(cid:13)
Id2 −
(cid:13)
(cid:13)

1
n

BB∗

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ η .

(25)

(26)

First, through the following lemma we establish a suﬃcient optimality condition needed to prove

Proposition 3.

Lemma 3. Let X0 ∈ (cid:67)n1×r and Y0 ∈ (cid:67)n2×r satisfy X0Y ∗
maximizer of (2), if for every ∆1 ∈ (cid:67)d1×r and ∆2 ∈ (cid:67)d2×r we have

0 = M0. Then [X0; Y0] is the unique

h fX0 −

1
n

AA∗X0, ∆1i + hfY0 −

1
n

BB∗Y0, ∆2i ≤

1
n

n
X

i=1

with equality occurring only when both ∆1 and ∆2 are zero.

|haib∗

i , X0∆∗

2 + ∆1Y ∗

0 i|

(27)

Proof. Let X0 ∈ (cid:67)d1×r and Y0 ∈ (cid:67)d2×r satisfy X0Y ∗
unique maximizer of (2) if for any ∆1 ∈ (cid:82)d1×r and ∆2 ∈ (cid:82)d2×r

0 = M0. Note that (X0, Y0) would be the

h fX0, ∆1i + hfY0, ∆2i ≤

1
n

n
X

i=1

‘i(X0 + ∆1, Y0 + ∆2) − ‘i(X0, Y0) ,

(28)

with equality holding only for ∆1 = 0 and ∆2 = 0.

Since a∗

i X0Y ∗

0 bi = mi, for each i, we obtain

=

k∆∗

k∆∗

0 ai, ∆∗

1
2
+ |a∗
≥ haia∗

‘i(X0 + ∆1, Y0 + ∆2) − ‘i(X0, Y0)
1
1aii +
2
0 + ∆1∆∗
2)bi|
i Y0, ∆2i + |a∗
1aik2 +
k∆∗
i Y0, ∆2i + |a∗

1aik2 + hX ∗
i (X0∆∗
2 + ∆1Y ∗
i X0, ∆1i + hbib∗
i ∆1∆∗
k∆∗
2bi| +
i X0, ∆1i + hbib∗

≥ haia∗

− |a∗

1
2

1
2

i (X0∆∗
2bik2
i (X0∆∗

2bik2 + hY ∗

0 bi, ∆∗

2bii

2 + ∆1Y ∗

0 )bi|

2 + ∆1Y ∗

0 )bi| ,

(29)

where the ﬁrst lower bound is obtained by the triangle inequality and the next lower bound follows
from the Cauchy-Schwarz inequality.

Using (29), for i = 1, . . . , n, the right-hand side of (27) can be bounded from above. This bound
shows that if (27) holds with equality occurring only at [∆1; ∆2] = 0, then (28) holds and the
claim is proved.

13

For any X0 ∈ (cid:67)d1×r and Y0 ∈ (cid:67)d2×r that satisfy X0Y ∗

0 = M0, we have X0∆∗

2 + ∆1Y ∗

0 ∈ T .

Therefore, by Lemma 3 and (14), it suﬃces to show that

h fX0 −

1
n

AA∗X0, ∆1i + hfY 0 −

1
n

BBTY 0, ∆2i ≤ ρkX0∆∗

2 + ∆1Y ∗

0 kF

(30)

for all ∆1 ∈ (cid:67)d1×r and ∆2 ∈ (cid:67)d2×r with the equality only when [∆1; ∆2] = 0.

Deﬁne the linear operator L : (cid:67)(d1+d2)×r → (cid:67)d1×d2 by

(cid:16)h

∆1; ∆2

L

i(cid:17)

= X0∆∗

2 + ∆1Y ∗
0 ,

for all ∆1 ∈ (cid:67)d1×r , ∆2 ∈ (cid:67)d2×r ,

whose adjoint operator is

L∗(Z) =

h

ZY0; Z TX0

i

,

for all Z ∈ (cid:67)d1×d2 .

h

∆ =

∆1; ∆2

i

,

With

and

E =

(cid:20)
fX0 −

1
n

AA∗X0; fY 0 −

BBTY0

(cid:21)

,

1
n

(31)

we can rewrite (30) as

hE, ∆i ≤ ρkL (∆)kF .
Note that L generally has a nontrivial nullspace, particularly, if (d1 + d2)r < d1d2. Therefore,
in view of the inequality above, it is necessary to have hE, ∆i = 0 for all ∆ in the nullspace of
L. Fortunately, for a certain choice of (X0, Y0) the corresponding matrix E satisﬁes the required
condition, as shown by the following lemma, which is proved in Appendix B.3.

Lemma 4. Let (X0, Y0) be the solution to

max
X∈(cid:67)d1×r,Y ∈(cid:67)d2×r

h fX0, Xi + hfY0, Y i −

subject to XY ∗ = M0 .

1
2n

kX ∗Ak2

F −

1
2n

kY ∗Bk2
F

(32)

For the operator L and the matrix E, deﬁned by (31) in terms of the particular solution (X0, Y0)
above, we have

E ∈ (cid:86) def= range(L∗) .

Furthermore, if (26) holds, then

#

"

−

"

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X0
Y0

fX0
fY0

#(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F

≤

1 + η
1 − η

·

#

"

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X
Y

−

"
fX0
fY0

#(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F

+

2η
1 − η

·

#

"

−

"

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X0
Y0

X
Y

#(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

"

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

fX0
fY0

#(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

(33)

for all X ∈ (cid:67)d1×r and Y ∈ (cid:67)d2×r satisfying XY ∗ = M0.

Hereafter, the pair (X0, Y0) is chosen as in Lemma 4. The subspace (cid:86) can be described explicitly

as

(cid:86) = range(L∗) =

nh

ZY0; Z TX0

i

: Z ∈ (cid:67)d1×d2

o

.

(34)

14

By the fundamental theorem of linear algebra, we also have (cid:86) = null(L)⊥. Thus, with P(cid:86) denoting
the orthogonal projection onto the subspace (cid:86) , Lemma 4 implies that E = P(cid:86) E. Consequently, to
guarantee (30), it suﬃces to have

kEkF kP(cid:86) ∆kF ≤ ρkL(P(cid:86) ∆)kF ,

(35)

because by the Cauchy-Schwarz inequality

hE, ∆i = hP(cid:86) E, ∆i = hE, P(cid:86) ∆i ≤ kEkF kP(cid:86) ∆kF .

Furthermore, the following technical lemma provides a lower bound for kL (P(cid:86) ∆)kF/kP(cid:86) ∆kF.

Lemma 5. The linear operator L satisﬁes

kL(P(cid:86) ∆)kF ≥ min{σmin(X0), σmin(Y0)}kP(cid:86) ∆kF ,

∀∆ ∈ (cid:67)(d1+d2)×r .

(36)

Proof. Let [∆1; ∆2] belong to (cid:86) = range(L∗). Then there exists Z ∈ (cid:67)d1×d2 such that ∆1 = ZY0
and ∆2 = Z∗X0. Thus

(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)L([∆1; ∆2])
(cid:13)

F

= kX0∆∗
= kX0X ∗
= kX0X ∗
= kX0X ∗
≥ σ2

0 k2
2 + ∆1Y ∗
F
0 k2
0 Z + ZY0Y ∗
F
0 k2
0 Zk2
F + 2hX0X ∗
F + kZY0Y ∗
0 k2
0 Zk2
F + 2kX ∗
F + kZY0Y ∗
min(Y0)kZY0k2
F + σ2
min(X0)kX ∗
F
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)[∆1; ∆2]
(cid:13)

0 Zk2
min(X0), σ2

≥ min{σ2

min(Y0)}

.

F

0 Z, ZY0Y ∗
0 i

0 ZY0k2
F

Note that

max
X∈(cid:67)d1×r,Y ∈(cid:67)d2×r

min{σmin(X), σmin(Y )} =

q

σr(M0)

subject to XY ∗ = M0 .

Indeed, the assumptions of the proposition implies that min{σmin(X0), σmin(Y0)} is larger than
pσr(M0) divided by a numerical constant.
In order to show this, we introduce another pair
(X1, Y1) with X1Y ∗
1 = M0 so that [X1; Y1] approximates [ fX0; fY0]. The following lemma provides
an upper bound on the approximation error; the proof is provided in Appendix C.1.

Lemma 6. Suppose that the rank-r matrices M0 and fM0, whose compact SVDs are respectively
U0Σ0V ∗

0 , satisfy k fM0 − M0k < σr(M0). Then

0 and fU0 fΣ0fV ∗
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

min
Q∈(cid:67)r×r : Q−1=Q∗

"
U0
V0

#

Σ1/2

0 Q −

#

"

fU0
fV0

fΣ1/2
0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

≤

√
8

2 σ1(Σ0)
pσr(Σ0)

·

k fM0 − M0kF
σr(Σ0) − k fM0 − M0k

.

Let fU0, fΣ0 and fV0 be as in Lemma 6. Let Q be the minimizer in Lemma 6. Let

X1 = U0Σ1/2

0 Q and Y1 = V0Σ1/2

0 Q .

15

Then (17) implies

Choosing C2 ≥ 8

√

2 + 1 yields

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)[X1; Y1] − [ fX0; fY0]
(cid:13)F
pσr(M0)

√

2ρ
8
√
C2 − ρ/

≤

≤

rκ

√

8
2ρ
C2 − 1

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)[X1; Y1] − [ fX0; fY0]
(cid:13)F

q

≤ ρ

σr(M0) .

(37)

(38)

It follows from (38) via the triangle inequality that
"
fX0
fY0

"
X1
Y1

fX0
fY0

X1
Y1

≤ ρ

−

+

≤

#

"

"

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

#(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

#(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

#(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

q

q

σr(M0) +

rσ1(M0) ≤ 2

q

rσ1(M0) .

(39)

Plugging in (39) to (33) with X = X1 and Y = Y1 gives

#

"

−

"

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X0
Y0

fX0
fY0

#(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F

≤

+

#

·

(cid:13)
"
1 + η
(cid:13)
X1
(cid:13)
(cid:13)
Y1
1 − η
(cid:13)
4ηprσ1(M0)
1 − η

−

"
fX0
fY0

 (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

"
X0
Y0

#(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
#

F

−

"
fX0
fY0

#(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

+

"
X1
Y1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

#

"

−

!

fX0
fY0

#(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

.

After some simpliﬁcation, the above inequality and (37) imply
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)[X0; Y0] − [ fX0; fY0]
(cid:13)F
ρpσr(M0)

128(C1 + 1)
(C1 − 1)(C2 − 1)2 +

1
C1 − 1

2 +

s

≤

√

32

2

(C1 − 1)(C2 − 1)

+

4
(C1 − 1)2

!

.

By choosing both C1 and C2 large enough, we obtain

#

"

−

"

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X1
Y1

fX0
fY0

#(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

≤

ρpσr(M0)
10

and

#

"

−

"

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X0
Y0

fX0
fY0

#(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

≤

ρpσr(M0)
10

.

Then by the triangle inequality we have
#

"

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X0
Y0

−

"
X1
Y1

#(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

≤

ρpσr(M0)
5

.

By the singular value perturbation theory by Weyl [6] and because ρ ∈ (0, 1), it follows that

and

σr(X0) ≥ σr(X1) − kX0 − X1k ≥

4pσr(M0)
5

σr(Y0) ≥ σr(Y1) − kY0 − Y1k ≥

4pσr(M0)
5

.

Then (36) is implied by

kL(P(cid:86) ∆)k2 ≥

pσr(M0)
2

· kP(cid:86) ∆kF .

Therefore, by combinig the above estimates, we obtain a suﬃcient condition for (35) given by

kEkF ≤

4ρpσr(M0)
5

.

(40)

The following lemma, which is proved in Appendix C.2, provides an upper bound for kEkF that

can be used to ensure the suﬃcient condition (40).

16

 
Lemma 7. Let (X0, Y0) and E be as in Lemma 4. Suppose that (26) holds. Then for all (X, Y )
satisfying XY ∗ = M0, we have

kEkF ≤

3 + η
1 − η

·

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)[ fX0 − X; fY0 − Y ]
(cid:13)F

!

+ ηk[X; Y ]kF

.

By applying Lemma 7 with X = X1 and Y = Y1, and by (25), we obtain

kEkF ≤

3C1 + 1
C1 − 1

(cid:18) 1
10

+

(cid:19)

1
C1

Finally we choose C1 large enough so that

q

ρ

σr(M0) .

3C1 + 1
C1 − 1

(cid:18) 1
10

+

(cid:19)

1
C1

≤

4
5

.

This completes the proof.

6 Analysis of Spectral Initialization

Note that

A∗A(M0) =

1
n

n
X

i=1

aia∗

i M0bib∗
i .

Then by the independence between a and b together with the isotropy of each of them implies

Since the spectral initialization computes fM0 as the best rank-r approximation of A∗A(M0),

(cid:69)A∗A(M0) = M0 .

by the optimality, we have

Our goal is to show that

k fM0 − M0k ≤ 2k(A∗A − Id)M0k .

k(A∗A − Id)M0k ≤

CkM0k
√
rκ2

for an absolute constant C. Then it will imply (9).

Let

Then we will show that

Zi =

1
n

aia∗

i M0bib∗
i ,

i = 1, . . . , n .

(cid:13)
(cid:13)
(cid:13)

n
X

i=1

Zi − (cid:69)Zi

(cid:13)
(cid:13)
(cid:13) ≤

CkM0k
√
rκ2

holds with high probability respectively in the cases of Gaussian and t-design measurements.

In the following derivations we use the shorthand

ξ def=

for compact notation.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
X

i=1

Zi − (cid:69)Zi

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Sp

17

(41)

 
6.1 Proof of Proposition 1

By the triangle inequality we have

nkZikSp

=

r
X

σkaia∗

i ukv∗

kbib∗
i

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Sp

≤

r
X

k=1

σkkaik2|a∗

i uk||v∗

kbi|kb∗

i k2 .

It follows that

(cid:16)

(cid:69)kZikp
Sp

(cid:17)1/p

≤

(a)
≤

(b)
≤

1
n

1
n

1
n

r
X

k=1
r
X

σk ((cid:69)kaikp

2|a∗

i uk|p|v∗

kbi|pkb∗

i kp

2)1/p

σk ((cid:69)kaikp

2|a∗

i uk|p)1/p ((cid:69)|v∗

kbi|pkb∗

i kp

2)1/p

k=1
r
X

(cid:16)

σk

(cid:69)kaik2p
2

(cid:17)1/2p (cid:16)

(cid:69)|a∗

i uk|2p(cid:17)1/2p (cid:16)

(cid:69)|v∗

kbi|2p(cid:17)1/2p (cid:16)

(cid:69)kb∗

i k2p
2

(cid:17)1/2p

k=1

(c)

(cid:46) p2√

d1d2
n

r
X

k=1

σk

(cid:46) kM0kp2r(d1 + d2)
n

,

where (a) holds by the independence between ai and bi; (b) follows from Cauchy-Schwarz inequality;
and (c) holds by [50, Eq. (2.15) and Lemma 2.7.6] since kaik2
2 are sub-
exponential random variables. Then we deduce that

kbi|2, and kbik2

i uk|2, |v∗

2, |a∗

(cid:16)

(cid:69)kZi − (cid:69)Zikp
Sp

(cid:17)1/p

(cid:16)

≤ 2

(cid:69)kZikp
Sp

(cid:17)1/p

(cid:46) kM0kp2r(d1 + d2)
n

.

Furthermore, the second-order moments are computed as

(cid:69)(Zi − (cid:69)Zi)(Zi − (cid:69)Zi)∗ =

d2 + 2
n2 kM0k2

FId1 +

(cid:69)(Zi − (cid:69)Zi)∗(Zi − (cid:69)Zi) =

d1 + 2
n2 kM0k2

FId2 +

and

Then we obtain

2d2 + 3

n2 M0M ∗

0

2d1 + 3

n2 M ∗

0 M0 .





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
X

i=1

(cid:69)(Zi − (cid:69)Zi)(Zi − (cid:69)Zi)∗

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Sp

∨

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
X

i=1

(cid:13)
(cid:13)
(cid:69)(Zi − (cid:69)Zi)∗(Zi − (cid:69)Zi)
(cid:13)
(cid:13)
(cid:13)Sp



 (cid:46) kM0k2r(d1 + d2)1+1/p

.

n

Now we are ready to apply Theorem 5. Recalling (41), Theorem 5 provides

((cid:69)ξp)1/p
kM0k

(cid:46)

pr(d1 + d2)1+1/p
n

!1/2

∨

p3n1/pr(d1 + d2)
n

.

By Markov’s inequality, it implies

(cid:80){ξ ≥ ((cid:69)ξp)1/pη−1/p} ≤ η.

(42)

18

 
We choose η = (d1 + d2)−α. Then there exists an absolute constant C1 such that if

n ≥ C1

(cid:20)
(cid:15)−2prd1+1/p+2α/p ∨

(cid:16)

(cid:15)−1p3rd1+α/p(cid:17)p/(p−1)(cid:21)

,

then

(cid:80){ξ ≥ (cid:15)kM0k} ≤ (d1 + d2)−α .

Since the Shatten-p norm is larger than the spectral norm, it follows that (44) implies

(cid:80) {kA∗AM0 − (cid:69)A∗AM0k ≥ (cid:15)kM0k} ≤ (d1 + d2)−α .

(43)

(44)

We choose p = (2α + 1) ln(d1 + d2) so that the condition in (43) simpliﬁes to
(cid:17)
(cid:16)

n ≥ C2r(d1 + d2)

(cid:15)−2α ln2(d1 + d2) ∨ (cid:15)−1α3 ln3(d1 + d2)

for another an absolute constant C2. We obtain a suﬃcient condition by choosing (cid:15) = C/κ2√
This compltes the proof.

r.

6.2 Proof of Proposition 2
√

√

Since kaik2 =

d1 and kbik2 =

d2, we have

kZikS2t

=

√

d1d2
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

r
X

k=1

σka∗

i ukv∗

kbi

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

Note that we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

r
X

k=1

σka∗

i ukv∗

kbi

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

r
X

k=1

σk |a∗

i ukv∗

kbi| .

Thus, by the triangle inequality in L2t together with the homogeneity of the L2p norm, we obtain

(cid:16)

(cid:69)kZik2t
S2t

(cid:17)1/2t

≤

≤

√

d1d2
n

k=1
r
X

σk

k=1

d1d2
n

r
X

(cid:16)

σk

(cid:69) |a∗

i ukv∗

kbi|2t(cid:17)1/2t

(cid:18)

(cid:12)
(cid:12)d−1/2
(cid:12)

1

(cid:69)

a∗

i uk

(cid:12)
(cid:12)
(cid:12)

2t(cid:19)1/2t (cid:18)

(cid:12)
(cid:12)d−1/2
(cid:12)

2

(cid:69)

b∗
i vk

(cid:12)
(cid:12)
(cid:12)

2t(cid:19)1/2t

,

(45)

where the second inequality holds since ai and bi are independent.
Recall that, by the construction of a t-design set, ai satisﬁes

(cid:16)

(cid:69)

d−1
1 aia∗
i

(cid:17)⊗t

=

!−1

d + t − 1
t

PSymt .

Therefore we have

(cid:12)
(cid:12)d−1/2
(cid:12)

1

(cid:69)

a∗

i uk

2t

(cid:12)
(cid:12)
(cid:12)

(cid:18)

(cid:69)

= tr

=

=

(cid:16)
d−1
1 aia∗
i
!−1

d1 + t − 1
t

(cid:17)⊗t

(cid:19)

(uku∗

k)⊗t

(cid:16)

tr

PSymt(uku∗

k)⊗t(cid:17)

!−1

d1 + t − 1
t

(cid:16)

(uku∗

k)⊗t(cid:17)

tr

≤ d−t

1 t! ,

19

 
 
 
where the third identity follows since (uku∗
Then we obtain

k)⊗t is invariant under the factor-wise permutation.

(cid:18)

(cid:12)
(cid:12)d−1/2
(cid:12)

1

(cid:69)

a∗

i uk

(cid:12)
(cid:12)
(cid:12)

2t(cid:19)1/2t

(cid:17)1/2t

(cid:16)
d−t
1 t!

≤

≤

Similarly we also have

(cid:18)

(cid:12)
(cid:12)d−1/2
(cid:12)

2

(cid:69)

b∗
i vk

(cid:12)
(cid:12)
(cid:12)

2t(cid:19)1/2t

(cid:16)

≤

(cid:17)1/2t

d−t
2 t!

≤

s

t
d1

.

s

t
d2

.

(46)

(47)

Then by plugging in the upper bounds in (46) and (47) into (45), we obtain

(cid:16)

(cid:69)kZik2t(cid:17)1/2t

≤

tσ1r

√

n

d1d2

≤

tσ1r(d1 + d2)
2n

.

Furthermore, the triangle inequality and Jensen’s inequality yield

(cid:16)

(cid:69)kZi − (cid:69)Zik2t
S2t

(cid:17)1/2t

(cid:16)

(cid:16)

≤

=

≤ 2

(cid:69)kZik2t
S2t

(cid:17)1/2t

(cid:16)

+

(cid:69)k(cid:69)Zik2t
S2t

(cid:17)1/2t

(cid:17)1/2t

+ k(cid:69)ZikS2t

(cid:69)kZik2t
S2t
(cid:16)

(cid:69)kZik2t
S2t

(cid:17)1/2t

.

Putting these bounds together we obtain

(cid:16)

(cid:69)kZik2t(cid:17)1/2t

≤

2tσ1r(d1 + d2)
2n

.

Furthermore, the second moments are computed as

(cid:69)(Zi − (cid:69)Zi)(Zi − (cid:69)Zi)∗ =

1
n2

d1d2kM0k2
d1 + 1

FId1

+

(cid:18) d1d2
d1 + 1

(cid:19)

− 1

!

M0M ∗
0

and

(cid:69)(Zi − (cid:69)Zi)∗(Zi − (cid:69)Zi) =

1
n2

d1d2kM0k2
d2 + 1

FId2

+

(cid:18) d1d2
d2 + 1

(cid:19)

− 1

!

M ∗

0 M0

.

Then we have





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
X

i=1

(cid:69)(Zi − (cid:69)Zi)(Zi − (cid:69)Zi)∗

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)S2t

∨

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
X

i=1

(cid:69)(Zi − (cid:69)Zi)∗(Zi − (cid:69)Zi)


 ≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)S2t

4σ2

1r(d1 + d2)1+1/2t
n

.

With ξ deﬁned in (41), Theorem 5 provides

(cid:16)

(cid:69)ξ2t(cid:17)1/2t

(cid:46)

tσ2

1r(d1 + d2)1+1/2t
n

!1/2

∨

t2n1/2tσ1r(d1 + d2)
n

.

Then invoking (42) with η = (d1 + d2)−α, we can show that there exists an absolute constant C1
such that if

n ≥ C1

(cid:20)
(cid:15)−2tr(d1 + d2)1+1/2t+α/t ∨

(cid:16)

(cid:15)−1t2r(d1 + d2)1+α/2t(cid:17)2t/(2t−1)(cid:21)

,

then

The desired result follows by choosing (cid:15) = C/κ2√

r.

(cid:80) {kA∗AM0 − (cid:69)A∗AM0k ≥ (cid:15)kM0k} ≤ d−α .

20

 
 
 
A Tools from Random Matrix Theory

Theorem 3 ([17, Theorem II.13]). Let G ∈ (cid:82)m×n be a random matrix whose entries are indepen-
dent copies of g ∼ N (0, 1). Then

((cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

(cid:80)

GTG − In

(cid:13)
(cid:13)
(cid:13)
(cid:13)

> 3 max

!)

 r 4n
m

,

4n
m

≤ 2 exp(−n/2) .

Theorem 4 (Matrix Bernstein inequality [47, Theorem 1.6]). Let (Yk) ⊂ (cid:67)m×n be a ﬁnite sequence
of independent zero-mean random matrices such that kYkk ≤ R almost surely for all k. Let

σ2 = max

((cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X

k

(cid:69)YkY ∗
k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X

k

(cid:69)Y ∗

k Yk

)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

.

Then for all t > 0

(cid:80)

((cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X

k

Yk

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

)

≥ t

≤ (m + n) · exp

−t2/2
σ2 + Rt/3

!

.

Theorem 5 (Noncommutative Rosenthal inequality [29, Theorem 0.4], [20, Theorem 3.8]). Let
(Yk) ⊂ (cid:67)d1×d2 be a ﬁnite sequence of independent zero-mean random matrices. Then there exists
an absolute constant C > 0 such that for all 2 ≤ p < ∞

!1/p

(cid:13)
(cid:13)
(cid:13)

(cid:69)

X

k

Yk

p
(cid:13)
(cid:13)
(cid:13)

Sp

≤ C

"

√

p

 (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X

k

(cid:69)YkY ∗
k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Sp

∨

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X

k

(cid:69)Y ∗

k Yk

!1/2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Sp

∨ p

(cid:69)

kYkkp
Sp

X

k

!1/p#

.

B Proofs of the main lemmas

B.1 Proof of Lemma 1

Let τ > 0 be ﬁxed. Since a and b are independent and isotropic, they satisfy

which implies

(cid:69)aa∗ ⊗ bb∗ = Id1d2 ,

(cid:69) |a∗Hb|2 = kHk2
F,

∀H ∈ (cid:82)d1×d2 .

Therefore, the Paley-Zygmund inequality [40] (see also [18, Corollary 3.3.2]), we have

(cid:80) (|a∗Hb| ≥ τ kHkF) = (cid:80)

(cid:16)

|a∗Hb|2 ≥ τ 2(cid:69) |a∗Hb|2(cid:17)
(cid:69) |a∗Hb|2(cid:17)2

(1 − τ 2)2 (cid:16)

≥

(cid:69)|a∗Hb|4

.

(48)

(49)

(50)

Then it suﬃces to show that the fourth order moment (cid:69)|a∗Hb|4 is upper-bounded by
F within a constant factor. We ﬁrst show this for the real Gaussian case. Since [a; b] ∼

kHk4

(cid:69) |a∗Hb|2(cid:17)2
(cid:16)

=

21

 
 
 
N (0, Id1+d2), we have

(cid:69)|a∗Hb|4 = (cid:69)b(b∗H ∗ ⊗ b∗H ∗)(cid:69)a(aa∗ ⊗ aa∗)(Hb ⊗ Hb)
F + 2 kb∗H ∗k4
S4

= (cid:69)bkb∗H ∗k4
= 3 (cid:69)bkb∗H ∗k4
2
= 3 tr [(H ⊗ H)(cid:69)b(bb∗ ⊗ bb∗)(H ∗ ⊗ H ∗)]
= 3kHk4
F + 6 kHk4
S4
≤ 9kHk4
F ,

(51)

where k·kS4

denotes the Schatten-4 norm.
By plugging in (49) and (51) to (50), we obtain

(cid:80) (|a∗Hb| ≥ τ kHkF) ≥

(1 − τ 2)2
18

.

We obtain an analogous upper bound in the t-design case. With the isotropic normalization, a

and b satisfy (6) and (8). Therefore,

(cid:69)|a∗Hb|4 = (cid:69)b(b∗H ∗ ⊗ b∗H ∗)(cid:69)a(aa∗ ⊗ aa∗)(Hb ⊗ Hb)

=

=

=

=

≤

d1
d1 + 1
2d1
d1 + 1
2d1
d1 + 1

(cid:16)

(cid:69)bkb∗H ∗k4

F + kb∗H ∗k4
S4

(cid:17)

· (cid:69)bkb∗H ∗k4
2

· tr [(H ⊗ H)(cid:69)b(bb∗ ⊗ bb∗)(H ∗ ⊗ H ∗)]

2d1d2
(d1 + 1)(d2 + 1)
4d1d2
(d1 + 1)(d2 + 1)

(cid:16)

·

kHk4

F + kHk4
S4

(cid:17)

· kHk4
F .

(52)

By plugging in (49) and (52) to (50), we obtain

(cid:80) (|a∗Hb| ≥ τ kHkF) ≥

(1 − τ 2)2
8

.

B.2 Proof of Lemma 2

The independence and isotropy assumptions imply (48). Note that any H ∈ T can be written as
H = ∆1V ∗
2. Furthermore, without loss of generality, we may assume

0 + U0∆∗

h∆1, U0i = 0 ,

(53)

which implies

Since

kHk2

F = k∆1k2

F + k∆2k2
F .

i Hbi = tr ((aib∗
a∗

i V0)∗ ∆1) + tr ((U ∗

0 aib∗

i )∗ ∆∗

2) ,

22

by (53) we have

Cn(T ) ≤ (cid:69) sup

H∈T ∩(cid:83)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
√
n

n
X

i=1

εia∗

i Hbi

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

k∆1kF ·

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
√
n

n
X

i=1

εiaib∗

i V0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

+ k∆2kF ·

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
√
n

n
X

i=1

εiU ∗

0 aib∗
i

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

≤ (cid:69)

sup
k[∆1; ∆2]kF=1


= (cid:69)



(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
√
n


(cid:69)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
√
n

q

(d1 + d2)r ,

≤

=

n
X

i=1

n
X

i=1

εiaib∗

i V0

εiaib∗

i V0

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
√
n

n
X

i=1

εiU ∗

0 aib∗
i



1/2



(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F

+ (cid:69)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
√
n

n
X

i=1

εiU ∗

0 aib∗
i



1/2



(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F

where the second step follows by the Cauchy-Schwarz inequality, the fourth step is obtained by
Jensen’s inequality, and the last step holds since (εi)n
i=1 is a Rademacher sequence and (a, b)
satisﬁes (48).

B.3 Proof of Lemma 4

This lemma basically follows from requiring stationarity at Q = I which is away from singu-
larities of the objective. However, to avoid complications arising from derivatives with respect
to complex-valued variables, we provide a “lower level” proof. To show this claim, ﬁrst observe
that any (X, Y ) satisfying XY ∗ = M0 can be parameterized by an invertible r × r matrix Q
as (X, Y ) = (X0Q, Y0Q−∗). Let (X0, Y0) be the maximizer considered in the statement of the
lemma. Therefore, for any invertible Q in GLr((cid:67)), the set of invertible r × r complex matrices, we
should necessarily have

h fX0, X0Qi + hfY0, Y0Q−∗i −
1
2n

≤ h fX0, X0i + hfY0, Y0i −

which is equivalent to

1
2n
kX ∗

kQ∗X ∗

F −

0 Ak2
1
2n

1
2n
0 Bk2
F ,

0 Ak2

F −

kY ∗

(cid:13)
(cid:13)Q−1Y ∗
(cid:13)

0 B

(cid:13)
2
(cid:13)
(cid:13)

F

h fX0, X0Q − X0i + hfY0, Y0Q−∗ − Y0i −

h(Q − Ir)∗ X ∗

−

1
n

h(Q−1 − Ir)Y ∗

0 B, Y ∗

0 Bi −

1
2n

1
n
k(Q − Ir)∗ X ∗

0 Ai

0 A, X ∗
1
(cid:13)
(cid:13)(Q−1 − Ir)Y ∗
(cid:13)
2n

0 B

(cid:13)
2
(cid:13)
(cid:13)

F

≤ 0 .

0 Ak2

F −

To simplify the notation let us deﬁne the short-hands

and

ΘX = X ∗
0

(cid:18)

fX0 −

1
n

AA∗X0

(cid:19)

,

ΘY = Y ∗
0

(cid:18)

fY0 −

1
n

BB∗Y0

(cid:19)

.

The necessary inequality can be expressed as

hΘX , Q − Iri + hΘY , (Q−1 − Ir)∗i

≤

1
2n

k(Q − Ir)∗X ∗

0 Ak2

F +

1
2n

(cid:13)
(cid:13)(Q−1 − Ir)Y ∗
(cid:13)

0 B

(cid:13)
2
(cid:13)
(cid:13)

F

.

(54)

23

Choosing Q within an arbitrarily small neighborhood of Ir allows us to use the identity

Q−1 − Ir =

∞
X

(−1)k(Q − Ir)k.

k=1

Applying this identity in (54) yields

hΘX − Θ∗

Y , Q − Iri +

∞
X

(−1)khΘ∗

Y , (Q − Ir)ki

k(Q − Ir)∗X ∗

k=2
0 Ak2

F +

kQ − Irk2kX ∗

0 Ak2

F +

≤

≤

1
2n
1
2n

1
2n
1
2n

(cid:13)
(cid:13)(Q−1 − Ir)Y ∗
(cid:13)
0 B
(cid:13)Q−1(cid:13)
kQ − Irk2(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)

F
kY ∗

0 Bk2
F .

As Q → Ir, the terms linear in Q − Ir dominate and the terms with superlinear dependence on
kQ − Irk vanish faster. Therefore, the inequality above implies

ΘX = Θ∗

Y .

(55)

Let us express (31) as E = [EX ; EY ] with EX = fX0 − 1

n AA∗X0 and EY = fY0 − 1

n BBTY0.

Then (55) implies that [EX ; EY ] belongs to the subspace

(cid:87) def=

nh

E1; E2

i

: E1 ∈ (cid:67)d1×r, E2 ∈ (cid:67)d2×r, X ∗

0 E1 = ET

2 Y0

o

.

It only remains to show that (cid:87) coincides with (cid:86) = null(L)⊥ deﬁned in (34).

We ﬁrst verify that (cid:86) ⊆ (cid:87). By the deﬁnition of T , we can express (cid:86) equivalently as

(cid:86) =

nh

ZY0; Z TX0

i

: Z ∈ T

o

.

Suppose that [E1; E2] belongs to (cid:86) . Then there exists Z ∈ T such that E1 = ZY0 and E2 = Z TX0.
Therefore,

X ∗

0 E1 = X ∗

0 ZY0 = (Z TX0)TY0 = ET

2 Y0 ,

h

which implies
shown (cid:86) ⊆ (cid:87).

i

E1; E2

is contained in (cid:87). Since [E1; E2] can be chosen arbitrarily in (cid:86) , we have

Furthermore, the orthogonal complement of (cid:87) within (cid:67)(d1+d2)×r can be written as
X0S; −Y0STi

: S ∈ (cid:67)r×ro

(cid:87) ⊥ =

nh

.

Since both X0 and Y0 are full column rank (otherwise, the rank of M0 = X0Y ∗
0 would be smaller
than r), we deduce that the dimension of (cid:87) ⊥ is r2, thereby the dimension of (cid:87) is r(d1 + d2 − r).
In view of the inclusion (cid:86) ⊆ (cid:87), it only remains to show that the dimension of (cid:86) is r(d1 + d2 − r).
We do so by arguing that the linear function Z 7→ [ZY0; Z TX0] is a bijection from T to (cid:86) , or
equivalently if Z ∈ T is mapped to 0 ∈ (cid:86) , then Z = 0. If for some Z = ∆1Y ∗
2 ∈ T we
0 Z = 0 and ZY0 = 0, then we should have kZk2
have X ∗
F = tr(Z∗Z) = 0, and consequently Z = 0.
Therefore, we have shown that (cid:86) = (cid:87), and particularly

0 + X0∆∗

E = [EX ; EY ] ∈ (cid:87) = (cid:86) .

24

Next we prove the second part of the lemma. By (26), we have

(1 − η)

+ (1 − η)

F

(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)X0 − fX0
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)A∗(X0 − fX0)
(cid:13)
(cid:13)
(cid:13)
(cid:28)
kA∗X0k2

F − 2

F

+

X0,

(cid:28)

F − 2

Y0,

kB∗Y0k2

1
n
kA∗X0k2

F − 2hX0, fX0i + 2

1
n

kB∗Y0k2

F − 2hY0, fY0i + 2

F

F

+

1
n
1
AA∗ fX0
n
1
n

(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)Y0 − fY0
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)B∗(Y0 − fY0)
(cid:13)
(cid:13)
(cid:13)
(cid:29)
1
(cid:13)
(cid:13)A∗ fX0
(cid:13)
n
1
(cid:13)
(cid:13)B∗fY0
(cid:13)
n
1
n
1
n

BB∗fY0

Id2 −

Id1 −

X0,

Y0,

+

(cid:28)

(cid:29)

(cid:28)

(cid:18)

(cid:18)

(cid:13)
2
(cid:13)
(cid:13)

F

(cid:13)
2
(cid:13)
(cid:13)

F

≤

=

=

1
n
1
n

+

1
n

+

AA∗

(cid:19)

(cid:29)

fX0

+

BB∗

(cid:19)

(cid:29)

fY0

+

(cid:13)
2
(cid:13)
(cid:13)

F

1
(cid:13)
(cid:13)A∗ fX0
(cid:13)
n
1
(cid:13)
(cid:13)
2
(cid:13)B∗fY0
(cid:13)
(cid:13)
(cid:13)
n

F

.

Since (X0, Y0) is the maximizer, for any X ∈ (cid:67)d1×r and Y ∈ (cid:67)d2×r satisfying XY ∗ = M0, we can
continue by

≤

=

1
n

+

1
n

+

(1 − η)

(cid:13)
(cid:13)
(cid:13)X0 − fX0

(cid:13)
2
(cid:13)
(cid:13)

F

+ (1 − η)

(cid:13)
(cid:13)
(cid:13)Y0 − fY0
(cid:18)
(cid:28)

(cid:13)
2
(cid:13)
(cid:13)

F

kA∗Xk2

F − 2hX, fX0i + 2

X0,

F − 2hY , fY0i + 2

(cid:28)

Y0,

(cid:28)

X0 − X,

+ 2

(cid:28)

+ 2

Y0 − Y ,

(cid:13)
2
(cid:13)
(cid:13)

kB∗Y k2

1
n
(cid:13)
(cid:13)A∗(X − fX0)
(cid:13)
1
n

F
(cid:13)
(cid:13)
2
(cid:13)B∗(Y − fY0)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)X − fX0

F

≤ (1 + η)

+ (1 + η)

(cid:13)
(cid:13)
(cid:13)Y − fY0

+ 2η

+ 2η

F
(cid:13)
2
(cid:13)
(cid:13)

F

(cid:13)
(cid:13)
(cid:13)X − fX0
(cid:13)
(cid:13)
(cid:13)Y − fY0

(cid:13)
(cid:13)
(cid:13)F
(cid:13)
(cid:13)
(cid:13)F

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) fX0
(cid:13)F
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)fY0
(cid:13)F

,

Id1 −
(cid:18)

Id2 −
(cid:18)

Id1 −
(cid:18)

Id2 −

1
n
1
n
1
n
1
n

AA∗

(cid:19)

fX0

(cid:19)

(cid:19)

BB∗

fY0

(cid:29)

(cid:29)

AA∗

fX0

BB∗

(cid:19)

fY0

(cid:29)

(cid:29)

+

+

1
n
1
n

(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
(cid:13)A∗ fX0
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)B∗fY0
(cid:13)
(cid:13)
(cid:13)

F

F

where the last step follows from (26). Finally note that

(cid:13)
(cid:13)
(cid:13)F

(cid:13)
(cid:13)
(cid:13)X − fX0
(cid:18)(cid:13)
(cid:13)
(cid:13)X − fX0

(cid:13)
(cid:13)
(cid:13) fX0
(cid:13)
2
(cid:13)
(cid:13)

≤

(cid:13)
(cid:13)
(cid:13)F

+

(cid:13)
(cid:13)
(cid:13)Y − fY0
(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)Y − fY0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)fY0
(cid:13)F
(cid:13)F
(cid:19)1/2 (cid:18)(cid:13)
(cid:13)
(cid:13) fX0

F

+

F

(cid:13)
2
(cid:13)
(cid:13)

F

+

(cid:13)
(cid:13)
(cid:13)fY0

(cid:13)
2
(cid:13)
(cid:13)

F

(cid:19)1/2

.

This completes the proof.

C Proofs of the supporting lemmas

C.1 Proof of Lemma 6

It suﬃces to show the upper bound for some Q in the orthogonal group Or((cid:67)). Let Q be given by

Q ∈ argmin
R∈(cid:82)r×r

#

"

((cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

U0
V0

R −

"
fU0
fV0

#(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

)

: R∗R = Ir

.

25

Then it follows that

#

fΣ1/2

0 −

 "

#

fU0
fV0

−

"

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

≤

fU0
fV0
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
q

σ1( fΣ0)

#

"
U0
V0
#

"

U0
V0
"
fU0
fV0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Σ1/2

0 Q

!

Q

fΣ1/2
0

+

#

"
fU0
fV0
"
U0
V0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

#

=

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

Q

#

#

−

"
U0
V0

fΣ1/2

0 −

#

"
U0
V0

QQ∗Σ1/2

(cid:13)
(cid:13)
(cid:13)
0 Q
(cid:13)
(cid:13)F

Q( fΣ1/2

0 − Q∗Σ1/2

(cid:13)
(cid:13)
(cid:13)
0 Q)
(cid:13)
(cid:13)F

+

(cid:13)
(cid:13) fΣ1/2
(cid:13)

0 − Q∗Σ1/2

0 Q

(cid:13)
(cid:13)
(cid:13)F

.

Since fΣ1/2

0

(resp. Q∗Σ1/2

0 Q) is the matrix square root of fΣ0 (resp. Q∗Σ0Q), by the pertur-

bation bound by Schmitt [43, equation (1.3)], we obtain

(cid:13)
(cid:13) fΣ1/2
(cid:13)

0 − Q∗Σ1/2

(cid:13)
(cid:13)
0 Q
(cid:13)F

≤

(cid:13)
(cid:13)
(cid:13) fΣ0 − Q∗Σ0Q
(cid:13)
(cid:13)
(cid:13)F
σr( fΣ0) + pσr(Σ0)

q

(cid:13)
(cid:13) fΣ0 − Q∗Σ0Q
(cid:13)
pσr(Σ0)

(cid:13)
(cid:13)
(cid:13)F

.

≤

On the other hand, by the triangle inequality, we have

k fM0 − M0kF = kfU0 fΣ0fV ∗

0 − U0QQ∗Σ0QQ∗V ∗

0 kF

≥ kU0Q( fΣ0 − Q∗Σ0Q)fV ∗
− kU0QQ∗Σ0Q(fV0 − V0Q)∗kF
≥ k fΣ0 − Q∗Σ0QkF − σ1( fΣ0) kfU0 − U0QkF − σ1(Σ0) kfV0 − V0QkF.

0 kF − k(fU0 − U0Q) fΣ0fV ∗

0 kF

By rearranging the above inequality, we obtain

k fΣ0 − Q∗Σ0QkF ≤ k fM0 − M0kF + σ1( fΣ0) kfU0 − U0QkF + σ1(Σ0) kfV0 − V0QkF.

By combining the results with Weyl’s inequality, we obtain

#

"

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

U0
V0

Σ1/2

0 Q −

#

"
fU0
fV0

fΣ1/2
0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

q

≤

σ1( fΣ0)

#

"

−

"

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

fU0
fV0

#

U0
V0

Q

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

+

k fM0 − M0kF + σ1( fΣ0) kfU0 − U0QkF + σ1(Σ0) kfV0 − V0QkF
pσr(Σ0)
s

!

"

#

#

≤

k fM0 − M0kF
pσr(Σ0)

q

+

2σ1(Σ0)

1 + 2

σ1(Σ0)
σr(Σ0)

·

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
|

fU0
fV0

"
U0
V0

−

{z
(])

Q

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
}

Finally, (]) is bounded by a variant of the Davis-Kahan theorem by Dopico [22, Theorem 2.1]

as follows:

#

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

"
fU0
fV0

−

#

"
U0
V0

Q

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

≤

.

q

2(k( fM0 − M0)fV0k2

F + kfU ∗

0 ( fM0 − M0)k2
F)

σr(Σ0) − k fM0 − M0k

26

 
C.2 Proof of Lemma 7

Since (X0, Y0) is a maximizer to (32), we have

h fX0, X0 − Xi + hfY , Y0 − Y i
F − kA∗Xk2

kA∗X0k2

≥

(cid:16)

F + kB∗Y0k2

(cid:17)

F

F − kB∗Y k2
1
n

F +

1 − η
2

1
2n
1
n

≥

hAA∗X, X0 −Xi +

kX0 −Xk2

hBB∗Y , Y0 −Y i +

(56)

1 − η
2

kY0 −Y k2
F ,

where the second inequality follows from the strong convexity of kA∗X0k2
are quadratic functions of X0 and Y0, respectively. Then (56) is rearranged to

F and kB∗Y0k2

F, which

≤ h fX −

1 − η
2

kX0 − Xk2

1
n

F +

kY0 − Y k2
F

1 − η
2
AA∗X, X0 − Xi + hfY0 −
s(cid:13)
(cid:13)
fX0 −
(cid:13)
(cid:13)

F + kY0 − Y k2
F ·

1
n

q

≤

kX0 − Xk2

BB∗Y , Y0 − Y i

1
n

AA∗X

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

+

(cid:13)
(cid:13)
fY0 −
(cid:13)
(cid:13)

1
n

BB∗Y

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

F

,

where the last step follows from the Cauchy-Schwarz inequality.

Therefore it follows that
(cid:13)
(cid:20)
(cid:13)
fX0 −
(cid:13)
(cid:13)

1
n

AA∗X; fY0 −

1
n

BB∗Y

(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)F

≥

1 − η
2

k[X0 − X; Y0 − Y ]kF .

Finally by the triangle inequality we obtain

(cid:20)
fX0 −

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

AA∗X0; fY0 −

1
n

BB∗Y0

1
n
1
n

AA∗X; fY0 −

AA∗X; fY0 −

1
n
1
n

BB∗Y

BB∗Y

1
n

AA∗X; fY0 −

1
n

BB∗Y

(cid:20)
fX0 −

(cid:20)
fX0 −

(cid:20)
fX −

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:18)

≤

≤

≤

≤

1 +

2(1 + η)
1 − η

(cid:19) (cid:26)(cid:13)
h
(cid:13)
(cid:13)

fX0 − X; fY0 − Y

+

(cid:13)
(cid:20) 1
(cid:13)
(cid:13)
n
(cid:13)

AA∗(X0 − X);

1
n

BB∗(Y0 − Y )

(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)F

+ (1 + η) k[X0 − X; Y0 − Y ]kF

2(1+η)
1−η
(cid:13)
(cid:13)
(cid:13)
(cid:13)

i(cid:13)
(cid:13)
(cid:13)F

+

(cid:20)
fX0 −

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

AA∗X; fY −

1
n

BB∗Y

(cid:20)
X −

1
n

AA∗X; Y −

1
n

BB∗Y

(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:27)

(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)F

.

(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)F

(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)F

+

Finally note that

(cid:20)
X −

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

AA∗X; Y −

1
n

BB∗Y

(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)F

≤ η k[X; Y ]kF ,

which completes the proof.

References

[1] A. Ahmed, B. Recht, and J. Romberg. Blind deconvolution using convex programming. IEEE

Transactions on Information Theory, 60(3):1711–1732, 2013.

[2] S. Bahmani. Estimation from nonlinear observations via convex programming with application

to bilinear regression. Electronic Journal of Statistics, 13(1):1978–2011, 2019.

27

[3] S. Bahmani and J. Romberg. Phase retrieval meets statistical learning theory: A ﬂexible

convex relaxation. In Artiﬁcial Intelligence and Statistics, pages 252–260, 2017.

[4] S. Bahmani and J. Romberg. Solving equations of random convex functions via anchored

regression. Foundations of Computational Mathematics, 19(4):813–841, 2019.

[5] R. Balan, B. G. Bodmann, P. G. Casazza, and D. Edidin. Painless reconstruction from mag-
nitudes of frame coeﬃcients. Journal of Fourier Analysis and Applications, 15(4):488–501,
2009.

[6] R. Bhatia. Matrix analysis, volume 169. Springer Science & Business Media, 2013.

[7] J. Bourgain, S. Dirksen, and J. Nelson. Toward a uniﬁed theory of sparse dimensionality
reduction in Euclidean space. Geometric and Functional Analysis, 25(4):1009–1088, 2015.

[8] S. Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in

Machine Learning, 8(3-4):231–357, 2015. ISSN 1935-8237. doi: 10.1561/2200000050.

[9] T. T. Cai and A. Zhang. ROP: Matrix recovery via rank-one projections. The Annals of

Statistics, 43(1):102–138, 2015.

[10] V. Cambareri and L. Jacques. Through the haze: a non-convex approach to blind gain cali-
bration for linear random sensing models. Information and Inference: A Journal of the IMA,
8(2):205–271, 04 2018. ISSN 2049-8772. doi: 10.1093/imaiai/iay004.

[11] E. J. Candès and T. Tao. The power of convex relaxation: Near-optimal matrix completion.

IEEE Transactions on Information Theory, 56(5):2053, 2010.

[12] E. J. Candes, Y. C. Eldar, T. Strohmer, and V. Voroninski. Phase retrieval via matrix com-

pletion. SIAM Review, 57(2):225–251, 2015.

[13] E. J. Candes, X. Li, and M. Soltanolkotabi. Phase retrieval via wirtinger ﬂow: Theory and

algorithms. IEEE Transactions on Information Theory, 61(4):1985–2007, 2015.

[14] Y. Chen, Y. Chi, and A. J. Goldsmith. Exact and stable covariance estimation from quadratic
sampling via convex programming. IEEE Transactions on Information Theory, 61(7):4034–
4059, 2015.

[15] Y. Chi, Y. M. Lu, and Y. Chen. Nonconvex optimization meets low-rank matrix factorization:

An overview. IEEE Transactions on Signal Processing, 67(20):5239–5269, 2019.

[16] M. A. Davenport and J. Romberg. An overview of low-rank matrix recovery from incomplete
observations. IEEE Journal of Selected Topics in Signal Processing, 10(4):608–622, 2016.

[17] K. R. Davidson and S. J. Szarek. Local operator theory, random matrices and banach spaces.

Handbook of the geometry of Banach spaces, 1(317-366):131, 2001.

[18] V. de la ˜Pena and E. Giné. Decoupling: from dependence to independence. Springer Science

& Business Media, 2012.

[19] P. Delsarte, J.-M. Goethals, and J. J. Seidel. Spherical codes and designs. In Geometry and

Combinatorics, pages 68–93. Elsevier, 1991.

28

[20] S. Dirksen. Noncommutative and vector-valued Rosenthal inequalities. PhD thesis, Delft Uni-

versity of Technology, 2011.

[21] S. Dirksen, G. Lecué, and H. Rauhut. On the gap between restricted isometry properties and
sparse recovery conditions. IEEE Transactions on Information Theory, 64(8):5478–5487, 2016.

[22] F. M. Dopico. A note on sin θ theorems for singular subspace variations. BIT Numerical

Mathematics, 40(2):395–403, 2000.

[23] S. Foucart and S. Subramanian. Iterative hard thresholding for low-rank recovery from rank-
one projections. Linear Algebra and its Applications, 572:117–134, 2019. ISSN 0024-3795.

[24] T. Goldstein and C. Studer. Phasemax: Convex phase retrieval via basis pursuit.

IEEE

Transactions on Information Theory, 64(4):2675–2689, 2018.

[25] D. Gross. Recovering low-rank matrices from few coeﬃcients in any basis. IEEE Transactions

on Information Theory, 57(3):1548–1566, 2011.

[26] D. Gross, F. Krahmer, and R. Kueng. A partial derandomization of phaselift using spherical

designs. Journal of Fourier Analysis and Applications, 21(2):229–266, 2015.

[27] N. Halko, P. G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic
algorithms for constructing approximate matrix decompositions. SIAM Review, 53(2):217–288,
2011. doi: 10.1137/090771806.

[28] P. Jain, P. Netrapalli, and S. Sanghavi. Low-rank matrix completion using alternating mini-
mization. In Proceedings of the forty-ﬁfth annual ACM symposium on Theory of computing,
pages 665–674. ACM, 2013.

[29] M. Junge and Q. Zeng. Noncommutative Bennett and Rosenthal inequalities. The Annals of

Probability, 41(6):4287–4316, 2013.

[30] R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries.

IEEE

Transactions on Information Theory, 56(6):2980–2998, 2010.

[31] V. Koltchinskii and S. Mendelson. Bounding the smallest singular value of a random matrix
without concentration. International Mathematics Research Notices, 2015(23):12991–13008,
2015.

[32] R. Kueng, H. Rauhut, and U. Terstiege. Low rank matrix recovery from rank one measure-

ments. Applied and Computational Harmonic Analysis, 42(1):88–116, 2017.

[33] J. M. Landsberg. Tensors: Geometry and applications. Representation theory, 381(402):3,

2012.

[34] M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes,

volume 23. Springer Science & Business Media, 1991.

[35] Y. Li, C. Ma, Y. Chen, and Y. Chi. Nonconvex matrix factorization from rank-one measure-

ments. arXiv preprint arXiv:1802.06286, 2018.

[36] M. Lin and J. Ye. A non-convex one-pass framework for generalized factorization machine
and rank-one matrix sensing. In Advances in Neural Information Processing Systems, pages
1633–1641, 2016.

29

[37] C. Ma, K. Wang, Y. Chi, and Y. Chen. Implicit regularization in nonconvex statistical es-
timation: Gradient descent converges linearly for phase retrieval and matrix completion. In
International Conference on Machine Learning, pages 3345–3354. PMLR, 2018.

[38] C. McDiarmid. On the method of bounded diﬀerences. Surveys in combinatorics, 141(1):

148–188, 1989.

[39] S. Mendelson. Learning without concentration.

In Conference on Learning Theory, pages

25–39, 2014.

[40] R. E. A. C. Paley and A. Zygmund. A note on analytic functions in the unit circle.

In
Mathematical Proceedings of the Cambridge Philosophical Society, volume 28, pages 266–272.
Cambridge University Press, 1932.

[41] J. T. Parker, P. Schniter, and V. Cevher. Bilinear generalized approximate message passing–

Part I: Derivation. IEEE Transactions on Signal Processing, 62(22):5839–5853, 2014.

[42] S. Sanghavi, R. Ward, and C. D. White. The local convexity of solving systems of quadratic

equations. Results in Mathematics, 71(3-4):569–608, 2017.

[43] B. A. Schmitt. Perturbation bounds for matrix square roots and Pythagorean sums. Linear

algebra and its applications, 174:215–227, 1992.

[44] M. Soltani and C. Hegde. Improved algorithms for matrix recovery from rank-one projections.

arXiv preprint arXiv:1705.07469, 2017.

[45] S. Sra, S. Nowozin, and S. J. Wright. Optimization for Machine Learning. Neural Information

Processing Series. MIT Press, 2011. ISBN 9780262537766.

[46] R. Sun and Z.-Q. Luo. Guaranteed matrix completion via non-convex factorization. IEEE

Transactions on Information Theory, 62(11):6535–6579, 2016.

[47] J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computa-

tional mathematics, 12(4):389–434, 2012.

[48] J. A. Tropp. Convex recovery of a structured signal from independent random linear measure-

ments. In Sampling Theory, a Renaissance, pages 67–101. Springer, 2015.

[49] A. W. van Der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes. Springer

Series in Statistics. Springer, 1996.

[50] R. Vershynin. High-dimensional probability: An introduction with applications in data science,

volume 47. Cambridge university press, 2018.

[51] K. Zhong, P. Jain, and I. S. Dhillon. Eﬃcient matrix sensing using rank-1 gaussian mea-
surements. In International conference on algorithmic learning theory, pages 3–18. Springer,
2015.

30

