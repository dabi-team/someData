9
1
0
2

p
e
S
9

]

C
O
.
h
t
a
m

[

1
v
5
0
7
3
0
.
9
0
9
1
:
v
i
X
r
a

Sparse linear regression with compressed and
low-precision data via concave quadratic
programming

V. Cerone, S. M. Fosson∗, D. Regruto

September 10, 2019

Abstract

We consider the problem of the recovery of a k-sparse vector from
compressed linear measurements when data are corrupted by a quantiza-
tion noise. When the number of measurements is not suﬃciently large,
diﬀerent k-sparse solutions may be present in the feasible set, and the
classical (cid:96)1 approach may be unsuccessful. For this motivation, we pro-
pose a non-convex quadratic programming method, which exploits prior
information on the magnitude of the non-zero parameters. This results
in a more eﬃcient support recovery. We provide suﬃcient conditions for
successful recovery and numerical simulations to illustrate the practical
feasibility of the proposed method.

1

Introduction

Sparse linear regression is the recovery of a sparse vector x ∈ Rn from linear
measurements y = Ax ∈ Rm, with A ∈ Rm,n. A vector is sparse if it has few
non-zero components; more precisely, we call k-sparse a vector with k (cid:28) n non-
zero components. The interest for sparse solutions has diﬀerent motivations.
In machine learning and system identiﬁcation, a purpose is to build models as
Indeed, we know that in many cases
simple as possible from large datasets.
the true number of parameters of a system is much smaller than the global
dimensionality of the problem, and sparsity supports the removal of redundant
parameters. In the recent literature, the identiﬁcation of linear systems under
sparsity constraints is considered in [21, 25, 24, 9]. Furthermore, in the last
decade, sparsity has been attracting a lot of attention due to the theory of com-
pressed sensing (CS, [6, 12]), which states that a sparse vector can be recovered
from compressed linear measurements, that is, when m < n, under suitable
conditions on the structure of A.

In real applications, observations may be aﬀected by diﬀerent sources of
noise. The most common one is measurement noise, which is usually modeled
as an unknown additive term δy ∈ Rm, i.e., y = Ax + δy. Linear regression
in the presence of measurement noise is usually solved via least squares; if x is
desired to be sparse, an (cid:96)1 regularizer is added, which promotes sparsity and
keeps the problem convex. More precisely, the problem can be formulated as
minx∈Rn (cid:107)x(cid:107)1 s. t. (cid:107)Ax − y(cid:107)p ≤ (cid:15), where (cid:107) · (cid:107)p is a suitable norm and (cid:15) > 0 is

1

 
 
 
 
 
 
a known bound; this is referred to as Basis Pursuit Denoising (BPDNp). In the
literature, BPDN2 is very popular, in particular in signal processing and CS, see,
e.g., [12]. The choice p = 2 bounds the mean energy of the error, hence BPDN2
is tolerant to possible outliers. The case p = ∞, instead, is considered to deal
with quantized or low-precision data in [7, 14, 26, 4]. When y is quantized, a
bound on each component is given, and no outliers occur. This makes the (cid:96)∞
description more suitable than the (cid:96)2 one.

In many applications, both the measurements and the matrix A are aﬀected
by noise. In particular, in linear systems identiﬁcation, this is analyzed by the
errors-in-variables (EIV) model, where both the input and the output are cor-
rupted by noise; we refer the reader to [22] for a complete overview on EIV
models. In [3, 5], the identiﬁcation of the feasible set for EIV models is tack-
led. Perturbations on A are more tricky to deal with than measurement noise,
as they make the problem non-convex. Furthermore, in CS, the recovery of
sparse vectors from compressed measurements is strongly compromised by per-
turbations on A, which are tolerated only if suﬃciently small or with particular
structures.
In [13], robustness to perturbation in CS is investigated, provid-
ing information on the amount of perturbation on A that can be tolerated by
BPDN2. In [27], the perturbation is assumed to be n-dimensional, instead of
nm-dimensional, and bounded in the (cid:96)∞ norm. Alternating minimization is
used to ﬁnd a minimum, which is global when the components of x are known
to be non-negative, see [27, Section IV.A].

In [4], the same non-negativity assumption is leveraged to formulate the
sparse linear regression problem, with bounded noise both on y and A, as a
linear programming problem. Speciﬁcally, the proposed problem in [4] is the
minimization of the (cid:96)1 norm within the feasible set, which, under the non-
negativity assumption, is a convex polytope. The approach is proved to be
robust, in the sense that the distance between the solution and the true vector
is proportional to the noise and to the sparsity level of the true signal.

The (cid:96)1 approach may fail if the feasible set does not delineate the sparsity
pattern. More precisely, many k-sparse solutions might belong to the feasible
set, though with diﬀerent support. When several k-sparse solutions are feasible,
the (cid:96)1 minimizer selects the one with less energy, which may not correspond
to the desired one. We highlight that, particularly in the case of compressed
measurements, the support recovery is the key point for the vector recovery. If
the support is known, and if we assume sparsity level k ≤ m, the problem is not
underdetermined, and the solution can be found by inversion, see, e.g., [11]. We
also remark that, in CS, conditions on A are provided that guarantee a unique
k-sparse solution for BPDN2, see, e.g., [12]; nevertheless, these conditions are
actually limited to speciﬁc classes of random matrices, and perturbations on A
are not envisaged.

In this work, we tackle the above described problem by proposing a novel
solution. Speciﬁcally, we propose a non-convex, polynomial approach which is
oriented to improve the recovery performance, particularly in terms of sparsity
pattern, by leveraging a prior information on the magnitude of the non-zero
components. Suﬃcient conditions that guarantee the recovery of the correct
support are proven, and numerical simulations illustrate the eﬀectiveness of the
approach in diﬀerent settings.

The paper is organized as follows. In Section 2, we present the background
and a motivating example, which highlights the ﬂaws of the (cid:96)1 approach. In

2

Section 3, we formally state the proposed problem, and discuss the considered
assumptions.
In Section 4, we explain how to solve the proposed problem.
In Section 5, we prove suﬃcient conditions that guarantee successful recovery,
while Section 6 is devoted to numerical experiments. Finally, some conclusions
are drawn in Section 7.

2 Background and motivating example

Let us consider an over-dimensioned vector of parameters (cid:101)x ∈ Rn, i.e., only k (cid:28)
n components are diﬀerent from zero, and let us assume that the system output
is given by y = A(cid:101)x, A ∈ Rm,n. The goal is to recover (cid:101)x under the following
assumptions: (a) m < n, i.e., compressed output observations are available;
(b) both y and A are not exactly known. More precisely, low-precision (or
quantized) versions of A and y are known, which are denoted by Q(A) ∈ Rm,n
and Q(y) ∈ Rm, respectively. The corresponding perturbations are denoted by
δA = Q(A) − A and δy = Q(y) − y. Moreover, we assume to know the maximum
perturbation magnitudes, denoted by ∆A and ∆y, respectively. For example, if
we consider data quantized over a ﬁxed number of bits, we assume to know the
number of bits.

Let us represent by D the feasible set. The natural approach to estimate
(cid:101)x, which is sparse, is by looking for the sparsest solution in D. This can be
performed by minimizing the (cid:96)1-norm over D ⊂ Rn, as illustrated in [4]. Specif-
ically, the problem is formulated as follows: given Q(y) ∈ Rm, Q(A) ∈ Rm,n,
∆A > 0, and ∆y > 0,

min
x∈Rn

(cid:107)x(cid:107)1 s. t. y = Ax

Q(y) = y + δy
Q(A) = A + δA
(cid:107)δy(cid:107)∞ ≤ ∆y
(cid:107)δA(cid:107)∞ ≤ ∆A.

(1)

In [4], the assumption xi ≥ 0, for any i = 1, . . . , n, is considered, which makes the
problem solvable via linear programming. Actually, the same result is obtained
if each component is known to be either non-positive or non-negative: non-
negativity is assumed for simplicity without loss of generality. However, we
notice that the non-negative setting has itself a number of applications, that
range from sparse localization problems, see [1], to sparse sensor selection, see
[2].

In [4], Problem (1) is shown to provide encouraging results if compared to
state-of-the-art algorithms for the recovery of sparse signals from compressed
and low-precision data. However, a drawback of the (cid:96)1 approach is that it
may fail the recovery of the right solution when several k-sparse vectors are
present in D, with diﬀerent supports. We illustrate this occurrence with a
simple motivating example. Let us consider A = [0.2131, 1.2414], (cid:101)x = [1, 0]T .
Then we have y = A(cid:101)x = 0.2131. Data are subject to quantization with step
0.1, thus we observe Q(A) = [0.2, 1.2] and Q(y) = 0.2.
If we assume non-
negativity, the feasible set D, which is shown in Fig. 1, is the intersection
between the ﬁrst orthant and the area between the two lines that represent the

3

Figure 1: A = [0.2131, 1.2414], (cid:101)x = [1, 0]T (blue point); Q(A) = [0.2, 1.2],
Q(y) = 0.2. The feasible set D is the light blue polytope. The solution of
Problem (1) is [0, 0.0769]T (red point). The support is inverted with respect to
(cid:101)x.

constraints. The solution of problem (1) is [0, 0.0769]T , depicted in Fig. 1 with
a red point. This vector is as sparse as (cid:101)x, while it has the opposite support.
Moreover, this solution has smaller non-zero value than the true one, which is
not a natural choice in practice. In fact, in real applications, the null entries
of (cid:101)x are not exactly zero, instead they take small values; from this point of
view, the support discriminates between signiﬁcant values and non-signiﬁcant
ones. In our example, 0.0769 is rather close to zero if compared to 1, therefore a
suitable algorithm should estimate it as a zero, and search for a more signiﬁcant
non-zero value to put in the support.

To tackle this occurrence, intuitively, one could assume that an interval
[α, β], α > 0, β ≥ α, of signiﬁcant magnitudes is known, thus the constraint
xi ∈ {0} ∪ [α, β], i = 1, . . . , n, might be added to Problem (1). Nevertheless,
this would compromise the convexity of D and make the problem deﬁnitely more
complex. For this motivation, in this work we propose a diﬀerent solution.

3 Problem Statement

In this section, we present a novel formulation of the sparse linear regression
problem with compressed measurements and low-precision data, under the as-
sumption that the signiﬁcant values are in [α, β]. We start by studying the case
of non-negative parameters, while we leave for future work more general sign
settings.

Assumption 1 For any i = 1, . . . , n, xi ∈ {0} ∪ [α, β].

Then, we deﬁne d := α+β

2 , and we formulate the problem as follows.

Problem 1

4

00.511.522.5300.050.10.150.20.250.3Given Q(y) ∈ Rm, Q(A) ∈ Rm,n, ∆A > 0, and ∆y > 0,

min
x∈[0,d]n

d(cid:107)x(cid:107)1 − (cid:107)x(cid:107)2

2 s. t. y = Ax

Q(y) = y + δy
Q(A) = A + δA
(cid:107)δy(cid:107)∞ ≤ ∆y
(cid:107)δA(cid:107)∞ ≤ ∆A.

(2)

Intuitively, Problem 1 is more accurate than (1) since the concave penalty
2 is closer to (cid:107)x(cid:107)0 than (cid:107)x(cid:107)1, see, e.g., [8] for more details. The rest

d(cid:107)x(cid:107)1 − (cid:107)x(cid:107)2
of the paper is devoted to prove this enhancement in the proposed setting.

First of all, we observe that if x ∈ [0, d]n, then the minimum of the objective
functional is zero, which is achieved for any x ∈ {0, d}n. Since the true non-
zero parameters are in [α, β], this approach does not exactly evaluate the non-
zero parameters. However, our main goal is to obtain the right support. As
mentioned before, once the right support is obtained, if k ≤ m, the problem of
evaluating the non-zero parameters is not underdetermined and can be obtained
by inversion. Nevertheless, in this work, we assume [α, β] small enough such that
d itself is a good approximation; the case of large intervals is left for future work.
In the next section, we illustrate how to practically tackle Problem 1 through

polynomial optimization.

4 Polynomial optimization approach

Thanks to the following result, we show that, under Assumption 1, Prob-
lem 1 can be solved through non-convex quadratic programming. In the fol-
lowing, given two vectors a, b ∈ Rn, we write a (cid:23) b to denote ai ≥ bi for
each i = 1, . . . , n. We denote by In ∈ Rn,n the identity matrix. Moreover,
1n := (1, 1, . . . , 1)T ∈ Rn.

Result 1 Under Assumption 1, Problem 1 can be equivalently written as the
following polynomial problem:

min
x∈[0,d]n

d(cid:107)x(cid:107)1 − (cid:107)x(cid:107)2

2 s. t. Cx (cid:22) g

where

C =

g =

(cid:18) Q(A) − ∆A1m1T
n
−Q(A) − ∆A1m1T
n
(cid:19)

(cid:18) Q(y) + ∆y1m
−Q(y) + ∆y1m

∈ R2m.

(cid:19)

∈ R2m,n

(3)

Result 1 is obtained by observing that Problem 1 can be rewritten as follows:

min
x∈[0,d]n

d(cid:107)x(cid:107)1 − (cid:107)x(cid:107)2

2 s. t. (cid:107)Q(y) − (Q(A) − δA)x(cid:107)∞ ≤ ∆y

(4)

(cid:107)δA(cid:107)∞ ≤ ∆A

and by applying the results on bounded EIV identiﬁcation proved in [3], under
Assumption 1.

5

We notice that Problem 1, as written in (3), is non-convex and semi-algebraic,
with linear constraints. In particular, when x is known to have only non-negative
entries, the (cid:96)1 term is a sum, and the problem is a concave quadratic program-
ming problem. To solve Problem 1, we can apply polynomial optimization re-
sults by [15], which state that the global minimum of a constrained polynomial
problem can be achieved by a hierarchy of relaxed semideﬁnite programming
(SDP) problems. As the order of relaxation increases, the solutions of these
SDP converge to the global optimal solution of the original problem. The hier-
archy has ﬁnite convergence generically, as illustrated in [19, 20].

The SDP relaxation technique is computationally intense in case of large
dimensional data. However, in [16] it is shown that if the polynomial problem
has a sparse structure, the dimension of the SDP relaxations can be reduced.
This polynomial sparsity arises, for example, when the variables are decoupled,
which is the case of the functional d(cid:107)x(cid:107)1 − (cid:107)x(cid:107)2
2. Moreover, since the functional
is quadratic, also the relaxation order can be kept low, as described in the
numerical experiments in Section 6.

We remark that the generalization to a formulation that envisages unknown
signs can be derived by exploiting results in [3]. This generalization produces a
feasible set which is not overall convex, though convex over each orthant. The
solution is then feasible, while computationally intensive for large dimensional
problem. The analysis of this problem is left for future work.

5 Analysis

In this section, we provide conditions under which Problem 1 is successful when
the non-zero parameters of the system are known to be in the range [α, β], α > 0,
β ≥ α. We start from the simpliﬁed case where α = β = d, i.e., (cid:101)x ∈ {0, d}n.

The following proposition analyses conditions under which (cid:101)x is the unique

feasible solution in {0, d}n.
Proposition 1 Let A(cid:101)x = y, with (cid:101)x ∈ {0, d}n. Let Ai be the i-th column of A.
If

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

γiAi

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

>

2∆y
d

, for any non-null γ ∈ {0, ±1}n,

(5)

then (cid:101)x is the unique solution of Problem 1.

Proof: Let us consider any z ∈ {0, d}n, z (cid:54)= (cid:101)x. Then, w := (cid:101)x − z ∈ {0, ±d}n.
If (cid:107) (cid:80)n

for any non-null γi ∈ {0, ±1}, then

i=1 γiAi(cid:107)∞ > 2∆y

d

This implies:

(cid:107)Aw(cid:107)∞ > 2∆y.

(cid:107)Aw ± δy(cid:107)∞ > 2∆y

⇒ (cid:107)Aw + δy(cid:107)∞ + (cid:107)δy(cid:107)∞ > 2∆y
⇒ (cid:107)Aw + δy(cid:107)∞ > 2∆y − (cid:107)δy(cid:107)∞ ≥ ∆y.

Since w = (cid:101)x − z and Q(y) = A(cid:101)x + δy, we obtain:
(cid:107)Q(y) − Az(cid:107)∞ > ∆y.

6

(6)

(7)

(8)

The last inequality follows from the fact that (cid:107)δy(cid:107)∞ ≤ ∆y. This proves that z
does not belong to the feasible set. Therefore, (cid:101)x is the unique feasible solution. (cid:3)
Concerning the condition (cid:107) (cid:80)n
i=1 γiAi(cid:107)∞ > 2∆y
for any non-null γ ∈ {0, ±1}n,
we highlight that, as discussed in [23], in many applications the columns of A
are in general position, i.e., (cid:80)n
i=1 γiAi (cid:54)= 0 for any non-null γ ∈ {0, ±1}n. This
deﬁnition is rather technical; however, it is proven to hold in many common
cases, for instance, when A has random entries generated from a continuous
distribution, see Lemmas 3-4 in [23]. Under the general position assumption,
there exists a σ > 0 such that (cid:107) (cid:80)n
i=1 γiAi(cid:107)∞ ≥ σ. Therefore, if d is suﬃciently
large with respect to ∆y, we can expect 2∆y
d < σ. On the other hand, σ can be
increased by increasing m.

d

Now, we extend Proposition 1 to the case of non-null (cid:101)xi belonging to [α, β].
Proposition 2 Let A(cid:101)x = y, with (cid:101)xi ∈ {0} ∪ [α, β] for any i = 1, . . . , n. Let Ai
be the i-th column of A. If

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

γiAi

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

> 2∆y for any non-null γ ∈ Qn,

(9)

where

and d = α+β
the same support of (cid:101)x.

Q := {−d} ∪

β − α
2
2 , then Problem 1 has a unique solution x(cid:63) ∈ {0, d}n, and x(cid:63) has

∪ [α, β]

,

(cid:20) α − β
2

(cid:21)

Proof: We denote by S(v) the support of any v ∈ Rn. Let us consider any
z ∈ {0, d}n, S(z) (cid:54)= S((cid:101)x). Let w := (cid:101)x − z. Then, w ∈ Qn.
i=1 γiAi(cid:107)∞ > 2∆y for any γ ∈ Qn, then

If (cid:107) (cid:80)n

(cid:107)Aw(cid:107)∞ > 2∆y.

(10)

By computations similar to (7) and (8), we can conclude that z is not in the
feasible set. Therefore, the solution of Problem 1 is the unique x(cid:63) ∈ {0, d}n ∩ D
(cid:3)
such that S(x(cid:63)) = S((cid:101)x).
We ﬁnally notice that in Proposition 1 and Proposition 2, the condition
(cid:107) (cid:80)n
i=1 γiAi(cid:107)∞ > σ for some σ > 0 is deﬁned on A, which is not known.
However, even if unknown, it makes sense to assume that some information on
the properties of A is available. In particular, the fact that the general position
of the columns holds can be assumed in many applications. Nevertheless, in the
case that σ has to be assessed more precisely, one can state the condition on
the observed Q(A), and the following suﬃcient condition can be proven.

Proposition 3 Let A(cid:101)x = y, with (cid:101)xi ∈ {0} ∪ [α, β] for any i = 1, . . . , n. Let
Q(A) = A + δA, and let Q(A)i be the i-th column of Q(A). If

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

γiQ(A)i

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

> 2∆y + ∆Aβn for any non-null γ ∈ Qn

(11)

(cid:104) α−β

(cid:105)

where Q = {−d} ∪
solution x(cid:63) ∈ {0, d}n, and x(cid:63) has the same support of (cid:101)x.

∪ [α, β], d = α+β

2 , β−α

2

2 , then, Problem 1 has a unique

7

Figure 2: Experiment 1: comparison between the proposed method, denoted by
CQP, and the (cid:96)1 method [4], under the assumption that (cid:101)x ∈ {0, d}n.

The proof is similar to that of Proposition 2, thus omitted for brevity. As
expected, the suﬃcient condition in Proposition 3 is more restrictive than that
of Proposition 2, while the two tend to coincide when the perturbation on A
tends to zero.

If we consider the motivating example illustrated in Section 2, condition (5)
is fulﬁlled if d = 1, since |Aγ| ≥ 0.2131 > 0.2 = 2∆y. Therefore, our approach
provides the right solution. Furthermore, condition (9) is fulﬁlled, for example,
with [α, β] = [0.9, 1.1] and ∆y = 0.03.

6 Numerical results

In this section, we present numerical results, which illustrate the eﬀectiveness
of the proposed approach with respect to the (cid:96)1 approach in [4]. We show two
experiments. In the ﬁrst one, we consider the ideal setting where the true vector
of parameters is (cid:101)x ∈ {0, d}n; thus, in this case, we expect to recover exactly both
the support and the non-zero values, under suitable conditions. We specify that,
despite its peculiarity, the binary setting is relevant in widespread applications,
such as localization and image processing, see [8, 10] for a complete overview.
In the second experiment, instead, the non-zero components are in [α, β], and
d = α+β
; in this case, we expect to obtain the right support and a biased
2
estimation of the non-zero values.

For both experiments, we propose the following setting. A system acquires
a sparse vector (cid:101)x ∈ Rn , with n = 10, through a regression matrix A ∈ Rm,n,
m = 4. The sparsity level of (cid:101)x is k = 2, i.e., only two parameters are non-null.
The knowledge of k is not required by the considered recovery algorithms. A
is generated according to a Gaussian distribution N (0, 1
m ); the support of (cid:101)x is
generated uniformly at random, while the non-zero entries are uniformly dis-
tributed in [α, β]. We assume that data are uniformly quantized: given a certain

8

 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7100 1000 2000 3000 4000 5000 6000Relative square l2 errorQuantization levelsCQPl1 8.5 9 9.5 10 10.5 11 11.5 12100 1000 2000 3000 4000 5000 6000Recovery time (s)Quantization levelsCQPl1 0 0.2 0.4 0.6 0.8 1100 1000 2000 3000 4000 5000 6000False positive rateQuantization levelsCQPl1 0 0.2 0.4 0.6 0.8 1100 1000 2000 3000 4000 5000 6000False negative rateQuantization levelsCQPl1Figure 3: Experiment 2: comparison between the proposed method, denoted by
CQP, and the (cid:96)1 method [4], under the assumption that the non-zero values of
(cid:101)x are in [0.8, 1.2].

number of equidistant quantization levels, each entry of A and y is approximated
with the closest point in the quantization codebook. The quantization range
is considered suﬃciently large to avoid saturation problems. The considered
quantization levels range from 100 to 6000, both for A and y.

2/(cid:107)(cid:101)x(cid:107)2

In the ﬁrst experiment, depicted in Fig. 2 we consider the case d = α =
β = 1; in the second experiment, illustrated in Fig. 3, [α, β] = [0.8, 1.2]. The
parameters are supposed to be non-negative. The proposed method, based on
concave quadratic programming (CQP in the ﬁgures) is compared to the (cid:96)1
approach of [4] in terms of diﬀerent performance metrics: the relative square
2, where (cid:98)x ∈ Rn is the estimate; the false
(cid:96)2 error, deﬁned as (cid:107)(cid:98)x − (cid:101)x(cid:107)2
positive rate, deﬁned as the number of events where (cid:98)xi (cid:54)= 0 while (cid:101)xi = 0, over
n − k; the false negative rate, deﬁned as the number of events where (cid:98)xi = 0
while (cid:101)xi (cid:54)= 0, over k; the run time (in seconds). Experiments are performed
on Matlab R2016b, on a 2.3GHz multicore processor. The results are averaged
over 20 random runs. The solution of CQP is obtained via SDP relaxation
according to [15, 16], using the software YALMIP, with MOSEK solver, see
[17, 18]. Concerning the SDP relaxation, we observe the relaxation order equal
to 2 is generally suﬃcient to achieve the minimum and the minimizer of the
problem.

In Fig. 2, we observe a signiﬁcant improvement obtained with the proposed
CQP with respect to the (cid:96)1 minimization. In particular, when the quantization
is suﬃciently ﬁne, i.e., more than 1000 quantization levels, the CQP recovery
is always exact, while the (cid:96)1 approach slightly improves with the decreasing of
the perturbation. CQP is more accurate, at the price of a slight increase of
run time: 9.5 seconds versus 9.2 seconds on average. We also notice that the
false positive rate is small for both methods, which is expected by methods that
encourage sparsity in the solution. Thus, it is more signiﬁcant to observe the
false negative rate, which shows whether the signiﬁcant parameters are selected.

9

 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6100 1000 2000 3000 4000 5000 6000Relative square l2 errorQuantization levelsCQPl1 8.5 9 9.5 10 10.5 11 11.5 12100 1000 2000 3000 4000 5000 6000Recovery time (s)Quantization levelsCQPl1 0 0.2 0.4 0.6 0.8 1100 1000 2000 3000 4000 5000 6000False positive rateQuantization levelsCQPl1 0 0.2 0.4 0.6 0.8 1100 1000 2000 3000 4000 5000 6000False negative rateQuantization levelsCQPl1In Fig. 2, we see that the (cid:96)1 approach never achieves a false negative rate below
0.375, which means that the erasure of signiﬁcant parameters is quite frequent.
From Fig. 3, we see that when (cid:101)x is not binary, the estimate provided by
CQP is still favorable with respect to the (cid:96)1 approach, in terms of relative square
(cid:96)2 error and false negative rate. In particular, we notice that the (cid:96)1 approach
does not reach a false negative rate below 1
2 , which is particularly critical. CQP
instead achieves values below 1
4 . On the other hand, the false positive rate is a
bit unfavorable for CQP, but generally below 1
10 . We also remark that the false
negative rate is more relevant for those applications where erasing signiﬁcant
entries is more damaging than preserving a few more parameters. In this second
experiment, the run time of CQP is slightly increased, passing from 9.5 seconds
to 10.5 seconds, which is however acceptable.

7 Conclusions

In this paper, we address the problem of sparse linear regression from few mea-
surements, when both the regression matrix and the measurements are known
in low-precision, or quantized. In the literature, the problem is tackled via (cid:96)1
minimization to promote sparsity on the solution; however, this approach is not
reliable when diﬀerent sparse solutions are present in the feasible set. In this
paper, we propose a concave quadratic programming approach which, under
suitable conditions, is eﬃcient in the support recovery as well as in the estima-
tion of the non-zero parameters. Suﬃcient conditions that guarantee the exact
recovery of the support are provided. Numerical results show enhancement with
respect to the (cid:96)1 approach, at the price of a slightly increased computational
complexity. Future work will envisage a deeper analysis of the performance and
the reﬁnement of the suﬃcient conditions.

References

[1] A. Bay, D. Carrera, S. M. Fosson, P. Fragneto, M. Grella, C. Ravazzi, and
E. Magli. Block-sparsity-based localization in wireless sensor networks.
EURASIP J. Wirel. Commun. Netw., 2015(182):1–15, 2015.

[2] M. Calvo-Fullana, J. Matamoros, C. Ant´on-Haro, and S. M. Fosson.
Sparsity-promoting sensor selection with energy harvesting constraints. In
IEEE ICASSP, pages 3766–3770, 2016.

[3] V. Cerone. Feasible parameter set for linear models with bounded errors

in all variables. Automatica, 29(6):1551 – 1555, 1993.

[4] V. Cerone, S. M. Fosson, and D. Regruto. A linear programming approach
to sparse linear regression with quantized data. In Proc. American Control
Conf. (ACC), pages 2990–2995, 2019.

[5] V. Cerone, D. Piga, and D. Regruto. Set-membership error-in-variables
identiﬁcation through convex relaxation techniques. IEEE Trans. Autom.
Control, 57(2):517–522, 2012.

[6] D. L. Donoho. Compressed sensing. IEEE Trans. Inf. Theory, 52(4):1289–

1306, 2006.

10

[7] David L. Donoho and Michael Elad. On the stability of the basis pursuit

in the presence of noise. Signal Processing, 86(3):511 – 532, 2006.

[8] S. M. Fosson. Non-convex approach to binary compressed sensing. In Proc.

Asilomar Conf. Signals Syst. Comput., pages 1959–1963, 2018.

[9] S. M. Fosson. Online optimization in dynamic environments: a regret
In Proc. IEEE Conf. Dec. Contr. (CDC),

analysis for sparse problems.
pages 7225–7230, 2018.

[10] S. M. Fosson and M. Abuabiah. Recovery of binary sparse signals from
compressed linear measurements via polynomial optimization. IEEE Signal
Process. Lett., 26(7):1070–1074, 2019.

[11] S. M. Fosson, J. Matamoros, C. Ant´on-Haro, and E. Magli. Distributed
In IEEE Int. Conf. Acoust.

support detection of jointly sparse signals.
Speech Signal Process. (ICASSP), pages 6434–6438, 2014.

[12] S. Foucart and H. Rauhut. A Mathematical Introduction to Compressive

Sensing. Springer, New York, 2013.

[13] M. A. Herman and T. Strohmer. General deviants: An analysis of perturba-
tions in compressed sensing. IEEE J. Sel. Top. Sign. Proces., 4(2):342–349,
2010.

[14] J. N. Laska, P. T. Boufounos, M. A. Davenport, and R. G. Baraniuk.
Democracy in action: Quantization, saturation, and compressive sensing.
Appl. Comput. Harmon. Anal., 31(3):429 – 443, 2011.

[15] J. Lasserre. Global optimization with polynomials and the problem of

moments. SIAM J. Optim., 11(3):796–817, 2001.

[16] J. B. Lasserre. Convergent SDP-relaxations in polynomial optimization

with sparsity. SIAM J. Optim., (17):822–843, 2006.

[17] J. L¨ofberg. YALMIP: A toolbox for modeling and optimization in MAT-

LAB. In Proc. CACSD Conference, volume 3, 2004.

[18] MOSEK ApS. The MOSEK optimization toolbox for MATLAB manual,

Version 8.1., 2017.

[19] J. Nie. Certifying convergence of lasserre’s hierarchy via ﬂat truncation.

Math. Program., 142(1):485–510, 2013.

[20] J. Nie. Optimality conditions and ﬁnite convergence of Lasserre’s hierarchy.

Math. Program., 146(1):97–121, 2014.

[21] B. M. Sanandaji, T. L. Vincent, M. B. Wakin, and R. T´oth. Compressive
system identiﬁcation of LTI and LTV ARX models. In Proc. IEEE Conf.
Dec. Contr. (CDC), pages 791–798, 2011.

[22] T. S¨oderstr¨om. Errors-in-Variables Methods in System Identiﬁcation.

Springer, 2018.

[23] Ryan J. Tibshirani. The Lasso problem and uniqueness. Electronic Journal

of Statistics, 7:1456–1490, 2013.

11

[24] R. Toth, H. Hjalmarsson, and C.R. Rojas. Sparse estimation of rational

dynamical models. In Proc. IFAC SYSID, pages 983–988, 2012.

[25] R. T´oth, B. M. Sanandaji, K. Poolla, and T. L. Vincent. Compressive
system identiﬁcation in the linear time-invariant framework. In Proc. IEEE
Conf. Dec. Contr. (CDC), pages 783–790, 2011.

[26] D. Valsesia, G. Coluccia, and E. Magli. Graded quantization for multiple
description coding of compressive measurements. IEEE Trans. Commun.,
63(5):1648–1660, 2015.

[27] Z. Yang, C. Zhang, and L. Xie. Robustly stable signal recovery in com-
pressed sensing with structured matrix perturbation. IEEE Trans. Signal
Process., 60(9):4658–4671, 2012.

12

