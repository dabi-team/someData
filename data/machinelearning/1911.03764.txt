Optimal Experimental Design for Staggered Rollouts

Ruoxuan Xiong*
Department of Quantitative Theory and Methods, Emory University, ruoxuan.xiong@emory.edu

Susan Athey
Graduate School of Business, Stanford University, athey@stanford.edu

Mohsen Bayati
Graduate School of Business, Stanford University, bayati@stanford.edu

Guido Imbens
Graduate School of Business, Stanford University, imbens@stanford.edu

In this paper, we study the problem of designing experiments that are conducted on a set of units such as

users or groups of users in an online marketplace, for multiple time periods such as weeks or months. These

experiments are particularly useful to study the treatments that have causal eﬀects on both current and future

outcomes (instantaneous and lagged eﬀects). The design problem involves selecting a treatment time for each

unit, before or during the experiment, in order to most precisely estimate the instantaneous and lagged eﬀects,

post experimentation. This optimization of the treatment decisions can directly minimize the opportunity

cost of the experiment by reducing its sample size requirement. The optimization is an NP-hard integer

program for which we provide a near-optimal solution, when the design decisions are performed all at the

beginning (ﬁxed-sample-size designs). Next, we study sequential experiments that allow adaptive decisions

during the experiments, and also potentially early stop the experiments, further reducing their cost. However,

the sequential nature of these experiments complicates both the design phase and the estimation phase. We

propose a new algorithm, PGAE, that addresses these challenges by adaptively making treatment decisions,

estimating the treatment eﬀects, and drawing valid post-experimentation inference. PGAE combines ideas

from Bayesian statistics, dynamic programming, and sample splitting. Using synthetic experiments on real

data sets from multiple domains, we demonstrate that our proposed solutions for ﬁxed-sample-size and

sequential experiments reduce the opportunity cost of the experiments by over 50% and 70%, respectively,

compared to benchmarks.

Key words : Sequential Experiments, Treatment Eﬀect Estimation, Carryover Eﬀects, Panel Data,

Dynamic Programming

2
2
0
2

n
a
J

0
1

]

M
E
.
n
o
c
e
[

3
v
4
6
7
3
0
.
1
1
9
1
:
v
i
X
r
a

1.

Introduction

Large technology companies run tens of thousands of experiments (also known as A/B tests) per

year to evaluate the impact of various decisions, new features, or products (Gupta et al. 2019). It

is common that these experiments are run on multiple units, e.g., groups of users or geographical

locations, for multiple hours, days, or weeks. The decision maker gets to observe the outcome

* Alphabetical order other than the ﬁrst author.

1

 
 
 
 
 
 
2

of each unit at every time period, and obtain a panel of experimental data. We refer to these

experiments as panel data experiments. To be more speciﬁc, let us provide two examples.

Example 1.1 (Driver Experience). Consider a ride-hailing platform that plans to test the

impact of a new app feature that improves their experience with the goal of increasing their partic-

ipation in the platform. A standard A/B test experiment, randomizing the drivers into treatment

and control groups, is not applicable due to interference between the drivers. Speciﬁcally, treated

drivers drive more which could reduce available rides for the control group, lowering their partici-

pation which could lead to overestimation of the app feature. One approach to address this issue

is by changing the units of experimentation to cities instead of the drivers, treating a whole city at

once or not. However, this substantially reduces the sample size, potentially going from millions of

drivers to hundreds of cities. To address this challenge, the platform can leverage time, choosing T

consecutive time periods of ﬁxed length (e.g., weeks) for each of the N units (cities) and observing

their outcomes (weekly driving hours) when treating diﬀerent units at diﬀerent time periods. Once

such panel data experiment concludes, the platform estimates the eﬀect of the treatment (app

feature), and decides whether to roll out the feature universally.

Example 1.2 (Public Health Intervention). Consider a country or state that aims to

measure the eﬀect of a new public health intervention (e.g. lock-down or social distancing policies)

on the spread of an infectious disease (Abaluck et al. 2021). Similar to Example 1.1, experimen-

tation should be performed at an aggregate level (e.g., geographic regions including neighboring

communities) to avoid interference between units. However, longer (e.g., monthly) time periods

may be more appropriate in this case.

In addition to increasing the sample size, panel data experiments are also appealing since they

allow us to study the eﬀect of treatments that last for multiple periods (Basse et al. 2019). For

example, the eﬀect of a new app feature could wear out over time and the resulting increase in

driver participation may fade away. But a lock-down policy, while in place, could have a persistent

eﬀect on the spread of the disease. In fact, estimating such dynamics of the treatment eﬀects

(i.e., instantaneous and lagged eﬀects) is important even in settings that units are individual users

and not necessarily are clusters or groups of users. Two such examples are, studying the impact

of Supplemental Nutrition Assistance Program on monthly at-home food expenditure for needy

families (Hastings and Shapiro 2018) or testing eﬀect of automatically enrolling bank customers

into receiving overdraft alerts (Caﬂisch et al. 2018). But, for the ease of reading, we only use

Examples 1.1-1.2 as running examples throughout the paper.

The primary objective of this paper is to design the panel data experiments such that the quality

of post-experimentation estimates of the treatment eﬀects (instantaneous and lagged eﬀects) are

maximized. We measure the estimation quality by its precision which is inversely proportional

3

to its variance. Increasing the precision, or equivalently increasing the statistical power, of the

experiment can eﬀectively reduce the required sample size (number of units or time periods) to

detect the treatment eﬀect at a desired level of signiﬁcance, which can reduce the opportunity cost

of the experiment.

In panel data experiments, we assume all units start in the control (no treatment) state at

the initial time period. For each unit, we need to choose a particular time period to allocate the

treatment. Then the unit remains exposed to this treatment in subsequent time periods; more

general settings that the treatment can be stopped are discussed in Section 2 and Appendix A.5.

The treatment allocation time is not necessarily the same for all units, and we refer to this treat-

ment adoption pattern as staggered treatment adoption. This type of treatment adoption pattern

sometimes occurs naturally in observational data, and statistical properties of the treatment eﬀect

estimation with such treatment pattern has been actively studied in recent years in causal inference

literature (Athey and Stern 2002, De Chaisemartin and d’Haultfoeuille 2020, Sun and Abraham

2020, Callaway and Sant’Anna 2020, Han 2020, Athey and Imbens 2021, Goodman-Bacon 2021).

However, in our experimental design setting, in addition to the estimation phase, the decision maker

also faces a design problem which involves selecting the treatment adoption times to maximize the

power.

1.1. Summary of Contributions

Modeling. In Section 2 of this paper, we consider a rich panel data generating model that cap-

tures heterogeneity in units and time periods. For example, in the driver experience application,

participation is inherently diﬀerent across units (e.g., San Jose, CA versus Boston) or across time

periods (rainy versus dry or busy versus holiday weeks), and controlling for these variations helps

increase the statistical power. The heterogeneity in units and times is captured by unit and time

ﬁxed eﬀects, as well as observed and latent covariates. We also allow time-dependent coeﬃcients

for the covariates, capturing the fact that during the winter holidays, there are more drivers in Las

Vegas than in a college town. More importantly, for the treatment eﬀect, we consider a general

and dynamic model of the instantaneous and lagged eﬀects involving scenarios for when the eﬀect

of the treatment accumulates or wears out over time.

Near-optimal solution for ﬁxed-sample-size designs. In Section 3, for the estimation

of treatment eﬀects, we consider the generalized least squares (GLS) estimator that is the best

linear unbiased estimator (BLUE) for instantaneous and lagged eﬀects while also accounting for

the observed and latent covariates. Finding the corresponding optimal design, selecting treatment

times that maximize the estimator precision, is NP-hard, even when all the treatment decisions as

well as choosing the duration of the experiment are made at the beginning. We provide a family

4

of solutions to this problem that approximate the optimal objective (best achievable precision)

within a multiplicative factor of 1 +

O

(1/N 2). The solutions have two prominent features. First,

the fraction of treated units per time period takes an S-shaped curve; the treatment rolls out to

units more slowly (over time) in the beginning and at the end, but rapidly in the middle, of the

experiment. Second, this optimal fraction is satisﬁed within each stratum (group of units with the

same observed and latent covariate value).

Sequential experiments. In Section 4, we propose a new algorithm, PGAE, that can adap-

tively terminate the experiment based on estimated precision of the treatment eﬀect from partially

observed results of the experiment, and employs dynamic programming (DP) for adaptive optimiza-

tion of the treatment design for subsequent time periods. The resulting sequential experiment can

therefore achieve the same precision as the ﬁxed-sample size experiments, but using a shorter dura-

tion, or equivalently incurring a lower cost. Achieving this involves several important and nontrivial

contributions. First, we prove that the estimates obtained by PGAE are consistent and asymptot-

ically converge to a normal distribution, despite the endogeneity in PGAE’s adaptive treatment

eﬀect estimation caused by dependence between the treatment decisions and the observed data

from prior periods. Building on this result, we use an empirical Bayes approach for solving the

DP. Second, we prove that the test statistic for the estimator is independent of the experiment

termination rule. This tackles the well-known “peeking” problem in sequential experiments (i.e.,

the estimation and hypothesis testing of treatment eﬀect are biased by the experiment termination

rule). We achieve these results by implementing a carefully designed sample splitting scheme (for

the units) that allows managing the bias due to the dependency between diﬀerent stages of making

treatment decisions, ending the experiment, and post-experiment treatment eﬀect estimation and

inference. An appealing property of this scheme is that, the ﬁnal treatment eﬀect estimation uses

all of the data, incurring no eﬃciency loss compared to an oracle who would have access to the

same design at the beginning of the experiment.

Empirical results. In Section 5, we illustrate the performance of our solutions for the ﬁxed-

sample-size and sequential experiments through synthetic experiments on four real data sets of

ﬂu occurrence rates, home medical visits, grocery expenditure, and Lending Club loans. For the

ﬁxed-sample-size experiments, our solution requires less than 50% of the sample size to achieve

the same precision as the benchmark designs. In addition, we show that stratiﬁcation based on the

latent covariates estimated from historical control data can further reduce the required sample size

by more than 20%. For sequential experiments we show that our adaptive design from PGAE can

improve the precision of treatment eﬀect estimation by more than 20%, on top of the improvements

obtained by our ﬁxed-sample-size design.

5

1.2. Related Literature

Our designs of panel data experiments are most closely related to the stepped wedge designs in

clinical trials (Brown and Lilford 2006). Stepped wedge designs sequentially roll out an intervention

to clusters or individuals over several periods. Prior work on optimal stepped wedge design (Hussey

and Hughes 2007, Hemming et al. 2015, Li et al. 2018) studies optimal treatment assignments for

the model with instantaneous eﬀect only and without covariates, whose optimal treated fraction

is linear in time. In contrast, the outcome model we study allows for carryover eﬀects, observed

and latent covariates. Our solutions are also related to the designs of experiments that focus on

synthetic control estimators and decide whom to treat (Doudchenko et al. 2021, Abadie and Zhao

2021). As a comparison, we not only focus on whom to treat but also on when to treat.

Our solution for the ﬁxed-sample-size experiments balances covariates between the control and

treated groups, that is aligned with prior work on covariate balancing in experimental design

(Pocock and Simon 1975, Cook et al. 2002, Hu et al. 2012, Bertsimas et al. 2015, 2019, Kallus

2018, Bhat et al. 2019, Krieger et al. 2019) and in causal inference (Nikolaev et al. 2013, Imai

and Ratkovic 2014, Sauppe and Jacobson 2017). Our solution is also conceptually aligned with

stratiﬁcation and clusterization in randomized experiments (Simon 1979, Mulvey 1983, Cheng and

Davenport 1989, Fox 2000, Athey and Imbens 2017, Hayes and Moulton 2017). We extend prior

work to panel data experiments. We show that stratiﬁcation and covariate balancing can increase

the statistical power of panel data experiments. Compared to prior work, we leverage the beneﬁt of

repeated observations on the same set of units and provide a solution to balance latent covariates

by using historical control data that contain information about latent covariates. Overall, our focus

on estimating both instantaneous and carryover treatment eﬀects over time is the main diﬀerence

between this work and prior covariate-balancing literature.

The PGAE algorithm for the sequential experiments is closely connected to the literature on

sequential testing (Johari et al. 2017, 2020), and the literature on adaptive designs in sequential

experiments (Efron 1971, Bhat et al. 2019), online learning and multi-armed bandits (e.g., Bubeck

and Cesa-Bianchi (2012), Lattimore and Szepesv´ari (2018)). Ju et al. (2019) is probably closest

to PGAE which also simultaneously considers sequential testing and adaptive allocation of data.

Compared to prior work on sequential experiments and adaptive designs (Efron 1971, Johari et al.

2017, Bhat et al. 2019, Ju et al. 2019, Johari et al. 2020), the key distinction is that we make

the treatment decisions on the same set of units over time. Given that the treatment cannot be

removed during the experiment, the treatment decisions in prior periods restrict the feasible set of

treatment decisions we could make for future periods, and PGAE is designed to take this restriction

into account. Regarding the sequential testing component in PGAE, the precision-based stopping

rule that is similar to the one in PGAE has been used Chow and Robbins (1965), Glynn and

6

Whitt (1992), Singham and Schruben (2012). The sample splitting component in PGAE is closely

connected to the forced sampling in multi-armed bandits (Goldenshluger and Zeevi 2013, Bastani

and Bayati 2020).

Post-experimentation inference based on panel data experiments is closely related to the growing

literature in causal inference to estimate the eﬀect of a policy or an intervention on observational

panel data. Panel data is particularly appealing as it has repeated observations on the same units

that can be used to deal with unobserved confounders (Angrist and Pischke 2008). Many public

policies and interventions have an irreversible treatment adoption pattern, such as the rise of

minimum wage (Card and Krueger 1994) and tobacco control program (Abadie et al. 2010). The

most widely used tool on panel data is the diﬀerence-in-diﬀerences (DID) estimator (Card and

Krueger 1994, Bertrand et al. 2004, Angrist and Pischke 2008, Abadie 2005). DID requires a parallel

trend assumption between the treatment and control groups that can be restrictive in empirical

studies. The synthetic control method relaxes this assumption, and constructs a weighted average

of control groups, to which the treated group is compared (Abadie and Gardeazabal 2003, Abadie

et al. 2010). Recently, an increasing number of studies develop treatment eﬀect estimation methods

tailored to staggered treatment adoption, where units adopt the new policy or treatment at diﬀerent

points in time (De Chaisemartin and d’Haultfoeuille 2020, Sun and Abraham 2020, Callaway and

Sant’Anna 2020, Han 2020, Athey and Imbens 2021, Goodman-Bacon 2021). As mentioned before,

our experimental design problem is quite diﬀerent given that, in addition to the estimation, we

also aim to select the treatment times to maximize the power.

2. Panel Data Experiments and Assumptions

In this section, we start by introducing panel data experiments and then describing the two types

of experiments that are considered in this paper. We also specify our assumptions on the data

model and treatment designs.

Consider Examples 1.1-1.2 from Section 1, and assume that we are planning to run an experiment

to estimate eﬀect of a treatment of interest (app feature or public health intervention). We assume

that the treatment has not been used before the experiment starts. The experimental designer

decides the treatment assignments for all the N units over T time periods. Let zit in

the decision variable representing treatment assignment for unit i at time t, for all i in
and t in T =

, where zit = +1 means unit i is treated at time t and zit =
}

1,
{

· · ·

, T

−

otherwise. Diﬀerent treatment decisions lead to diﬀerent panels of observed outcomes that can

aﬀect precision of the treatment eﬀect estimates. The estimation precision, to be deﬁned formally

in Section 3.2, directly impacts the cost of running the experiment since it speciﬁes the required

number of units and time periods. Therefore, optimizing the treatment decisions is an important

step in running panel data experiments.

{−

be

1, +1
}
, N

}
· · ·
1 means

1,
{

7

In this paper we consider two types of panel data experiments: ﬁxed-sample-size experiments and

sequential experiments. Fixed-sample-size experiments refer to the experiments with ﬁxed N and T .

Sequential experiments refer to the experiments with ﬁxed N and varying T . The ﬁxed-sample-size

experiments enjoy the beneﬁt of simplicity and involve straightforward construction of statistical

tests for the treatment eﬀects. However, the ﬁxed-sample-size experiments can be ineﬃcient in

time and cost, as the experiment may run longer than needed to attain a certain precision of the

treatment eﬀect estimates. In comparison, sequential experiments can early-stop the experiment if

needed, at the expense of making statistical inference for the treatment eﬀects more challenging.

We study the design of ﬁxed-sample-size experiments in Section 3. Building on our insights for the

ﬁxed-sample-size experiments, we propose an algorithm in Section 4 to run sequential experiments

that make treatment decisions adaptively and provide valid statistical inference of the treatment

eﬀects.

We focus on the treatment assignments that satisfy the irreversible treatment adoption; once a

unit adopts the treatment, it remains exposed to the treatment for all periods afterwards.

Assumption 2.1 (Irreversible Treatment Adoption). For all i and 1

t

≤

≤

T

−

1,

zi,t+1,

for all i and 1

zit

≤

t

≤

≤

T

−

1.

We mostly focus on the irreversible scenario for two reasons. First, often there are practical con-

straints restricting units from switching between control and treatment, such as the policies or

programs implemented at the group level (e.g., city or state level), or the features that are electron-

ically rolled to user interface (e.g., ride-hailing app feature). Second, when the treated units switch

back to the control, they may not return to the original control status. For example, drivers may

develop diﬀerent driving habits via the new feature. In fact, the irreversible pattern is common

in observational studies (Heckman et al. 1997, 1998, Blundell et al. 2004, Abadie 2005, Abadie

et al. 2010, 2015). In the experiment design literature, the irreversible pattern appears in the

stepped wedge designs of randomized trials in public health (Brown and Lilford 2006, Hussey and

Hughes 2007, Woertman et al. 2013, Hemming et al. 2015), and in the design of synthetic control

experiments (Doudchenko et al. 2021, Abadie and Zhao 2021).

Remark 2.1 (Generalization to Reversible Treatments). In Appendix A.5, we study

ﬁxed-sample-size experiment design problems where the treatment can be stopped. We show that

optimizing the treatment designs for this case follows from our results in Section 3.

When the treatment is irreversible, we can reduce the number of decision variables from N

N . Let Ai be the ﬁrst time that unit i adopts the treatment, where Ai belongs to set A = T

T to

×

=

∪ {∞}

8

1, 2,

{
· · ·
between

zit
{

}

, T,

and Ai =

means unit i was never treated.1 Then, there is a one-to-one mapping

∞}
T
t=1 and Ai, under Assumption 2.1. Speciﬁcally, given

∞

T
t=1,

zit
{

}

Ai =

1 + 0.5

(cid:40)

∞

T
t=1(1

zit)

−

(cid:80)

if ziT = 1
otherwise

.

On the other hand, given Ai,

zit = 2

1Ai≤t

1,

−

·

where 1Ai≤t is a binary indicator that equals one if Ai

t and zero otherwise.

≤
R be the observed outcome of unit i at time t. We impose the following assumption

Let Yit

∈

on the reduced-form outcome model, with a discussion on the potential outcome framework in

Remark 2.2.

Assumption 2.2 (Outcome Model Speciﬁcation). The observed outcome Yit for unit i at

time t can be speciﬁed as

Yit = αi + βt + X(cid:62)

i θt + u(cid:62)

i vt + τ0zit + τ1zi,t−1 +

+ τ(cid:96)zi,t−(cid:96) + εit,

· · ·

(2.1)

where τ0 is the instantaneous eﬀect, τj is the lagged j-th period eﬀect (j > 0), αi and βt are
Rdx

Rdx are observed covariates of dimension dx, θt

unobserved unit and time ﬁxed eﬀects, Xi

are their (non-random) unobserved time-varying coeﬃcients, ui

Rdu are latent (non-random)
∈
Rdu are (random) latent factors. Unobserved idiosyncratic

covariates of dimension du, and vt

∈

it∈[N ]×[T ] are i.i.d. in both i and t. In addition, we assume (cid:96), dx, du are ﬁxed.

∈

∈

errors

εit
{

}

Model (2.1) implies a unit’s treatment zit only aﬀects the outcomes of this unit (no cross-

unit interference). For the applications where cross-unit interference may occur (e.g., infectious

diseases or online marketplaces), no interference can hold when units are at the aggregate level

(e.g., geographic area or groups of substitute-able products). Model (2.1) also implies that unit

outcomes are not impacted by future treatment assignments (no anticipation). For example, drivers

do not increase their working hours in anticipation of the new app feature. No anticipation is also

commonly imposed in the literature (Robins 1997, Heckman and Vytlacil 2005, Basse et al. 2019,

Bojinov et al. 2020).

An important feature of model (2.1) is that it allows the treatment of a unit to aﬀect the outcomes

of the same unit for multiple periods (carryover eﬀects). The dynamics of carryover eﬀects can be

quite general, as the instantaneous eﬀect τ0 and lagged eﬀects τ1:(cid:96) =

· · ·
values. To demonstrate this generality, we describe a few scenarios below.

τ1,

(cid:0)

, τ(cid:96)

can take arbitrary

(cid:1)

1 We use “

” to preserve the ordering that a larger value for Ai implies the treatment is assigned at a later time.

∞

9

(a) Example of cumulative eﬀects

(b) Example of wearout eﬀects

Figure 1

Illustrative examples of cumulative and wearout eﬀects. Figure 1a shows that the eﬀect of a new public

health intervention on new infections accumulates over time (τ0, τ1, τ2 < 0). Figure 1b shows the eﬀect of a new app

feature on drivers’ working hours wears out over time (τ0, τ1 > 0 and τ2, τ3, τ4, τ5 < 0).

• Cumulative eﬀect. If τ0, . . . , τ(cid:96) all have the same sign, the eﬀect accumulates over (cid:96) consec-

utive time periods, as shown via an example in Figure 1a.

• Wear-out eﬀect. If τ0 has a diﬀerent sign than all of τ2, . . . , τ(cid:96), the total eﬀect wears out over

time. An example of this is shown in Figure 1b.

• An example of reversible treatment. As we discusse in Appendix A.5.1, when

(cid:96)
j=0 τj = 0,

there is no total eﬀect after (cid:96) periods which is equivalent to assuming the treatment is stopped

(cid:80)

after (cid:96) periods.

In this paper, we assume that the duration of the carryover eﬀects, (cid:96), is ﬁxed.

Instantaneous eﬀect τ0 and lagged eﬀects τ1:(cid:96) are the main objects of interest. We seek to opti-

mally choose the treatment adoption times

N
i=1 such that these quantities can be estimated as

Ai
{

}

accurately as possible.

Model (2.1) has two-way (i.e., unit and time) ﬁxed eﬀects αi and βt, observed covariates Xi with

time varying coeﬃcients, and interactive latent factor structure u(cid:62)

i vt. These components allow for

the heterogeneity in units and time periods. The two-way ﬁxed eﬀect model (and possibly with

observed covariates) has been widely used in observational studies (Bertrand et al. 2004, Angrist

and Pischke 2008).2 Observed covariates Xi in (2.1) are time-invariant and are not aﬀected by the

2 There are two reasons. First, αi and βt can be arbitrarily correlated with the observed explanatory variables
Xi, zit,
, so they can capture the unobserved additive unit-speciﬁc and time-speciﬁc confounders that
{
jointly aﬀect zit and Yit (Angrist and Pischke 2008). Second, αi captures the mean eﬀect of unobservables on unit i’s
outcome over time, and βt captures the mean eﬀect of unobservables on units’ outcomes at time t.

, zi,t−(cid:96)

· · ·

}

135791113monthnewinfectionsCumulativeEﬀecttreatedcontrol135791113weekworkinghoursWear-outEﬀecttreatedcontrol10

treatment, examples of such covariates are an individual’s gender and race or attributes of a city.3

In addition, the interactive latent factor structure u(cid:62)

i vt captures the multiplicative unobserved

eﬀects that addresses the restriction in the two-way ﬁxed eﬀect model with additive unobserved

eﬀects αi and βt only (Holtz-Eakin et al. 1988). We assume ui is nonrandom while vt is random,

following the literature on large dimensional factor modeling (Bai and Ng 2002, Bai 2003).

Our model is more general than (Lawrie et al. 2015, Girling and Hemming 2016, Li et al. 2018)

that study a similar optimal experimental design but their models does not capture carryover

eﬀects and lacks observed and latent covariates. As shown in Sections 3.3 and 5.2.2 below, the

estimation error of

ˆτj

(cid:96)
j=0 can be reduced in the estimation phase by accounting for carryover

{
eﬀects and covariates in the design phase.4

}

Remark 2.2 (Potential outcome framework). Under the potential outcome framework,

let Yit(zit, zi,t−1,

· · ·

, zi,t−(cid:96)) be the potential outcome for unit i at time t for treatment vector

(zit, zi,t−1, . . . , zi,t−(cid:96)). Under Assumption 2.1, without loss of generality, we can deﬁne the instanta-

neous and lagged eﬀects for unit i at time t as

τj,it :=

1
2

(cid:16)

Yit(+1,

· · ·

, +1, +1

,

zi,t−j

1,

−

· · ·

,

1)

−

−

Yit(+1,

· · ·

, +1,

,

1
−
zi,t−j

1,

−

· · ·

,

1)

−

(cid:17)

for

0

j

≤

≤

(cid:96).

Our estimands are the average instantaneous and lagged eﬀects deﬁned as

(cid:124)(cid:123)(cid:122)(cid:125)

(cid:124)(cid:123)(cid:122)(cid:125)

i,t
(cid:88)
and one can estimate the average eﬀects τj from the outcome model speciﬁcation (2.1).

τj =

1
N T

τj,it

for

0

j

(cid:96) ,

≤

≤

Remark 2.3 (Heterogeneous Treatment Effects). Note that τj,it can vary with i and t.

Assume the diﬀerences, τj,it

−

τj, are i.i.d. with mean 0 across both i and t, 1

i

≤

≤

N and 1

t

≤

≤

T .

Then we can incorporate each τj,it
τj into the error term εit and the OLS or GLS estimator
for the average eﬀects τj from model (2.1) will be unbiased.5 It is fairly common in the causal

−

inference literature to estimate a single parameter, the average treatment eﬀect, without making the

assumption that treatment eﬀects are homogeneous (Card and Krueger 1994, Imbens and Rubin

2015). The outcome model like (2.1) with a single or a few treatment eﬀect parameter(s) has been

used in empirical studies across various domains with an average treatment eﬀect interpretation for

3 The time-invariance assumption is commonly imposed in diﬀerence-in-diﬀerences estimators (Heckman et al. 1997,
1998, Blundell et al. 2004, Abadie 2005) and synthetic control estimators (Abadie et al. 2010, 2015) in observational
studies.
4 Lawrie et al. (2015), Girling and Hemming (2016), Li et al. (2018) assume random unit eﬀects as opposed to ﬁxed
unit eﬀects in model (2.1) and they treat unit eﬀects as an additional noise term. Their model does not have observed
covariates Xi, so they do not need face the endogeneity problem arising from E[Xi(αi + εit)]
5 In large samples, one can show the consistency of the OLS or GLS estimator for τj, under weaker assumptions on
τj,it

N and 1

τj, 1

= 0.

T .

t

i

−

≤

≤

≤

≤

(cid:54)
11

the parameter(s), such as in operations management (Cui et al. 2019, Cachon et al. 2019, Cui et al.

2020, Zhang et al. 2020), healthcare (Abaluck et al. 2021), and economics (Card and Krueger 1994).

We discuss an alternative way that accounts for heterogeneous treatment eﬀects using observable

sources of heterogeneity in Remark 3.7 below.

Additional Notations. For any vector x = (x1, . . . , xn), and integers a and b, 1

a < b

n, we

≤

≤

z, X, U, 1N , and IN that are deﬁned as follows: τ =

use notation xa:b to refer to the sub-vector (xa, . . . , xb). We also introduce new notations τ , yt, y,
RN ×1,
∈
RN ×dx,
RN T ×1, z =
RN ×du, 1N is vector of all ones of dimension N , and IN is the identity

, yt =
, τ0
· · ·
RN T ×1, X =
(cid:1)

1 , y(cid:62)
y(cid:62)
2 ,
u1 u2

τ(cid:96) ,
(cid:62)
(cid:0)

, y(cid:62)
T
uN

X1 X2
(cid:0)

1 , z(cid:62)
z(cid:62)
2 ,

Y1t, Y2t,

, YN t

, z(cid:62)
T

y =

XN

· · ·

· · ·

· · ·

· · ·

(cid:62)
(cid:1)

∈

∈

∈

(cid:62)

(cid:62)

U =
(cid:0)

(cid:1)

(cid:0)

(cid:0)

(cid:1)

(cid:62)
(cid:1)

∈

· · ·

matrix of dimension N

(cid:1)

(cid:0)

N .

×

3. Fixed-Sample-Size Experiments

In this section, we study the design of ﬁxed-sample-size experiments. We ﬁrst present an estimation

approach of the instantaneous and lagged eﬀects in Section 3.1. Next, in Section 3.2, we deﬁne

the precision of the proposed estimators and formulate an optimization problem to ﬁnd optimal

treatment times for each unit that maximizes the precision. Finally, we present our solution to the

optimization problem in Section 3.3.

3.1. Estimation of Instantaneous and Lagged Eﬀects

The decision maker needs to consider two objectives when choosing an estimator for the instan-

taneous and lagged eﬀects. First is the statistical quantities of this estimator, such as the bias,

variance (inverse of precision), and mean-squared error (MSE) of this estimator in the estima-

tion phase. Second is the feasibility of optimizing units’ treatment times based on the precision

of this estimator in the design phase. One can study the optimal treatment assignments based on

the statistical quantities of a simple estimator, but this estimator can have a large MSE in the

estimation phase. Alternatively, for a sophisticated estimator with a small MSE, the functional

form of its statistical quantities may be very complex or implicit, which can lead to an intractable

optimization problem in the design phase.

In this paper we study the optimization of treatment assignments based on the statistical quan-

tities of the generalized least squares (GLS) estimator. The corresponding optimization problem

is

T

min
τ ,α,β((cid:96)+1):T ,θ((cid:96)+1):T

W−1

e(cid:62)
t ·

et

·

(3.1)

t=(cid:96)+1
(cid:88)

s.t. et = yt

α

βt

−
αi = 0 for i > N

−

1N

·

−

Xθt

τ0

zt

·

−

− · · · −

zt−(cid:96)

τ(cid:96)

·

dx

1

−

−

12

where α =

α1,

· · ·

, αN

, β((cid:96)+1):T =

β(cid:96)+1,

· · ·

, βT

, θ((cid:96)+1):T =

θ(cid:96)+1

θT

, and W is a weighting

· · ·

matrix. The solution to (3.1) is denoted as
−1

(cid:1)
, i.e., the inverse covariance matrix of et, under Assumption 3.1 below. As

. The optimal W is proportional

(cid:0)
UΣvU(cid:62) + σ2

ˆτ , ˆα, ˆβ((cid:96)+1):T , ˆθ((cid:96)+1):T

to

(cid:0)

(cid:1)

(cid:1)

(cid:0)

ε IN

(cid:0)

(cid:1)

U, Σv, and σ2

(cid:0)

ε are unknown in practice, we can use feasible generalized least squares (FGLS).

(cid:1)

FGLS ﬁrst solves (3.1) using identity matrix as the weighting matrix, then estimates U, Σv, and

σ2

ε , and lastly uses

ˆU ˆΣv ˆU(cid:62) + ˆσ2

ε IN

−1

as the weighting matrix to solve (3.1) again.

Assumption 3.1 (Error Structure). vt

(cid:1)

(cid:0)

1t=s for all i, j, t, s, where ξit = (Xi, αi, βt, zit,

· · ·

Σv
ξit] = 0 and E[εitεjs

·

ξit, ξjs] = σ2
ε ·

|

∈

Rdu is i.i.d. with E[vt

ξit, ξjs] =
, zi−(cid:96),t). εit is i.i.d. in both i and t with E[εit

ξit] = 0 and E[vtv(cid:62)
s |

|

|

1i=j,t=s for all i, j, t, s. εit is independent of vs for all i, t, s.

Assumption 3.1 is analogous to, but slightly weaker than, the standard strict exogeneity assump-

tion (Wooldridge 2010). Under Assumption 3.1, the solution to (3.1) is unbiased from the Gaussian-

Markov Theorem (see Lemma A.1 in Appendix A.1 and Wooldridge (2010)). Note that Assumption

3.1 is stated conditional on αi and βt, and αi and βt are unobserved and unrestricted. Therefore,

Assumption 3.1 allows the ﬁrst and second moments of vt and εit to depend on Xi, zit,

, zi−(cid:96),t,

· · ·

which is slightly weaker than the standard strict exogeneity assumption.

GLS has two nice properties under Assumption 3.1 from the Gauss-Markov Theorem. First, GLS

is the best linear unbiased estimator (BLUE) of τ , meaning that this estimator has the smallest

variance among all the linear unbiased estimators. Second, there is an explicit formula for the

variance and precision of GLS in terms of zit, which makes it possible to provide a tractable solution

to the optimization problem.

Remark 3.1 (Machine learning heuristics for τ ). Instead of BLUE, one can look at

machine learning estimators. We provide one such estimator in Appendix C for τ that does not

rely on Assumption 3.1 about vt. This type of machine learning based estimators are typically

biased in the estimation of τ , but could have a smaller variance than the unbiased estimators. The

optimization of

Ai
{
not emphasize them in this paper.

}

N
i=1 based on these estimators is generally not tractable, and therefore we do

Remark 3.2 (Implication of Assumption 3.1). E[vt] = 0 holds without loss of generality
as we can always project their mean to αi.6 Note that et in (3.1) equals et = Uvt + εt, and then

under Assumption 3.1, et satisﬁes

E[eit] = 0,

E[e2

it] = u(cid:62)

i Σvui + σ2
ε ,

E[eitejt] = u(cid:62)

i Σvuj,

E[eitejs] = 0,

for t

= s,

implying that eit is correlated in the unit dimension, but it is uncorrelated in the time dimension.

6 If E[vt]

= 0, let α†

i = αi + u(cid:62)

i E[vt] and v†

t = vt

E[vt]. Then v†

t has mean zero.

−

(cid:54)
(cid:54)
13

3.2. Optimization Problem for Treatment Decisions

In this section, we introduce the optimization problem to solve the optimal

Ai
{

N
i=1 based on the
}

Ai

N
i=1 that

{

}

statistical quantities of GLS. From a high level perspective, we are interested in precisely estimating

τ . However, there are (cid:96) + 1 estimands in τ and it is generally infeasible to ﬁnd an

simultaneously maximizes the precision of each of

1. Instead, one needs to consider

(cid:96)
j=0 for (cid:96)

ˆτj
{

}

≥

an objective function that has a feasible solution. There are two categories of objective functions

that one may be interested in

• balance the precision/variance of each of
• maximize the precision of some linear combination of

(cid:96)
j=0

ˆτj

}

{

(cid:96)
j=0

ˆτj

{

}

Examples of the objective functions related to the ﬁrst category include (Atkinson et al. 2007)
• A(average)-optimal design: minimizes the trace of Var( ˆτ )
• D(determinant)-optimal design: minimizes the determinant of Var( ˆτ )
• T(trace)-optimal design: maximizes the trace of the inverse of Var( ˆτ )

An example of the objective functions related to the second category is
• Total treatment eﬀect (

(cid:96)
j=0 τj): minimizes Var

(cid:96)
j=0 ˆτj

When (cid:96) = 0, the four objective functions mentioned above are equivalent to each other. For

(cid:0) (cid:80)

(cid:1)

(cid:80)

general (cid:96), they are not equivalent, and the optimal treatment assignments for these four objectives

can be diﬀerent (though the diﬀerence can be small).

In this section, we focus on analytically solving the T(trace)-optimal design:

max
{Ai}N
i=1

tr

Prec( ˆτ )

,

(3.2)

(cid:0)
where Prec( ˆτ ) is deﬁned as the inverse of Var( ˆτ ). (3.2) is an integer program. In Section 3.3, we

(cid:1)

provide the explicit optimality conditions for (3.2) and we provide a rounding algorithm to obtain

(near-optimal) feasible solutions from the optimality conditions. The optimality conditions provide

insights into how the presence of diﬀerent components in model (2.1) (ﬁxed eﬀects, observed and

latent covariates) aﬀect the optimal treatment assignments. Admittedly, the other three objectives

mentioned above would be natural and of practical interest, especially the one that minimizes

Var

(cid:96)
j=0 ˆτj

. However, analytically solving the other three objectives is generally infeasible, as

explained in Remark 3.3 below. Instead, one could numerically solve the other three objectives

(cid:0) (cid:80)

(cid:1)

in practice. We visualize the numerical solutions for D-optimal design in Figure 8 in Appendix

A.4, which has a similar structure as our solutions to (3.2). We empirically show in Section 5 and

Appendix D that our solutions to (3.2) outperform the benchmark treatment designs measured by

the objective of A-optimal design and Var

(cid:96)
j=0 ˆτj

.

For the remaining of this section, we focus on the T-optimal design. T-optimal design was ﬁrst

(cid:0) (cid:80)

(cid:1)

introduced in Atkinson and Fedorov (1975a) to discriminate between two competing regression

≥

14

models (mapping to our setting, for example to determine whether (cid:96) = (cid:96)0 or (cid:96) = (cid:96)1 is true, for

distinct number of lags (cid:96)0 and (cid:96)1). Since then, the T-optimal design has been studied by Atkinson

and Fedorov (1975b), Uci´nski and Bogacka (2005), Wiens (2009), Dette et al. (2012, 2013, 2015)

and some others.

Example 3.1. When T = 1 and (cid:96) = 0, Ai takes value between 1 (treatment) and

(control).

∞

If all the covariates are observed (i.e., du = 0), the optimization problem (3.2) coincides with the

oﬄine optimization problem in Bhat et al. (2019), and is equivalent to the MAX-CUT problem

and is NP-hard (Hayes 2002, Mertens 2006). In this paper, we focus on the case of low-dimensional

covariates,7 where the feasible solution from our rounding algorithm is provably close to the optimal

integer solution to (3.2).

Remark 3.3 (Challenges in alternative objectives). Note that each entry in Var( ˆτ ) is

a ratio of two polynomial functions of zit, where the degrees of numerator and of demonimator are

2(cid:96) and 2((cid:96) + 1), respectively (for (cid:96)

1). The objective of A-optimal design and Var

(cid:96)
j=0 ˆτj

are

both sums of entries in Var( ˆτ ) (i.e., sums of ratios of two higher-order polynomials of zit). The

(cid:0) (cid:80)

(cid:1)

objective of D-optimal design is the inverse of a 2((cid:96) + 1)-th order polynomial function of zit.

3.3. Optimal Solutions

In this section, we seek to solve the T-optimal design. Note that there are many possibilities on how

diﬀerent components of model (2.1) can aﬀect the optimal treatment assignments (compared to

single-unit multi-period experiments and single-period multi-unit experiments). For example, the

eﬀects can happen in both the unit and time dimensions, and the eﬀects of diﬀerent components

may interact with each other. The large number of possibilities could complicate the optimal

solution, and therefore makes the optimization problem challenging to solve.

The ﬁrst critical result we show is that the eﬀects of diﬀerent components on the optimal treat-

ment assignments of (3.2) can in fact be separable, which greatly reduces the number of possibilities

and makes it possible to solve (3.2). This result is implied by Lemma 3.1 below that Prec( ˆτ ) can

be decomposed into three separable quadratic functions: the ﬁrst one does not depend on X and

U, the second one only depends on X, and the third one only depends on U. The solutions to each

of these three show the eﬀects of ﬁxed eﬀects, of observed covariates, and of latent covariates on

the optimal treatment assignments, respectively. If a solution simultaneously optimizes all three

quadratic functions, then this solution is an optimal solution to (3.2).

Lemma 3.1 (Separable Quadratic Representation). Suppose Assumptions 2.1-3.1 hold.

Also, assume that ˆτ is estimated from GLS with W = [wij]

UΣvU(cid:62) + σ2

ε IN

−1

, rows in X and

∝

7 This makes sense as we allow for latent covariates/factors, which can summarize the information and reduce the
dimensionality of (high-dimensional) observed covariates.

(cid:0)

(cid:1)

15

U are demeaned, i.e.,

N
i=1 Xi = 0dx and

N
i=1 ui = 0du,

Let T(cid:96) = T

(cid:96) and j(cid:96) = j + T(cid:96)

(cid:80)

−

−

1 for all j. Then tr

(cid:80)

Prec( ˆτ )
(cid:80)

N

i=1 Xiu(cid:62)
takes the form of

i = 0dx,du, and Σv = σ2
ε ·

tr

Prec( ˆτ )

=

(cid:0)

(cid:1)

N
σ2
ε

−

(cid:96)+1

j=1 (cid:18)
(cid:88)

(ωj:j(cid:96))(cid:62)P1T(cid:96)

ωj:j(cid:96) + 2b(cid:62)

(cid:0)
(cid:96) ωj:j(cid:96)

+

dx

(cid:1)
k=1(ωxk

fj,1(z)

(cid:80)

j:j(cid:96))(cid:62)P1T(cid:96)
fj,X(z)

ωxk
j:j(cid:96)

+ 1

N z(cid:62)
j:j(cid:96)

MUzj:j(cid:96)

fj,U(z)

Idu.

,

(cid:19)

where ωj:j(cid:96) =
, ωj(cid:96)
ωj,
· · ·
(cid:62)
z(cid:62)
, z(cid:62)
zj:j(cid:96) =
j ,
(cid:1)
j(cid:96)
∈ {−
T(cid:96)+2−2t
, and MU = P1T(cid:96) ⊗
(cid:0)
T(cid:96)
Example 3.2. When (cid:96) = 0, tr

(cid:0)
· · ·

(cid:1)

(cid:124)
(cid:62)

(cid:123)(cid:122)
with ωt = 1
N

N T(cid:96)×1, P1T(cid:96)
1, +1
}
U(Idu + U(cid:62)U)−1U(cid:62).8

(cid:80)

N

(cid:125)
(cid:124)
i=1 zit, ωxk
ωxk
j:j(cid:96) =
,
j
· · ·
1
1T(cid:96)1(cid:62)
= IT(cid:96) −
(cid:0)
T(cid:96)
T(cid:96)

(cid:62)

(cid:123)(cid:122)
, ωxk
j(cid:96)
, b(cid:96) = [b(cid:96),t]
(cid:1)

(cid:124)
(cid:125)
with ωxk
t = 1
N

(cid:123)(cid:122)

(3.3)
(cid:125)
N
i=1 Xikzit,
1, 1]T(cid:96) with b(cid:96),t =

(cid:80)

[
−

∈

Prec( ˆτ )

= Prec(ˆτ0) = 1/Var(ˆτ0) and Prec(ˆτ0) equals

Prec(ˆτ0) =

−

N
σ2
ε

(ω1:T )(cid:62)P1T ω1:T +2b(cid:62)

(cid:1)
0 ω1:T

(cid:0)

+

dx

k=1(ωxk

1:T )(cid:62)P1T ωxk

1:T

where b0 = [b0t]

1, 1]T with b0t = T +1−2t

T

(cid:1)

(cid:0) (cid:80)

.

(cid:16)(cid:0)
[
−
∈

1

N z(cid:62)

1:T MUz1:T

+

(cid:1)

(cid:0)

, (3.4)

(cid:1)(cid:17)

The decomposition of Prec( ˆτ ) relies on the assumptions on X, U and Σv in Lemma 3.1. However,

these assumptions are non-restrictive for the following three reasons. First, the assumption that

rows in X and U are demeaned can be satisﬁed by projecting the mean onto the unit ﬁxed eﬀects
α. Second, Σv = σ2
ε ·

Idu is an identiﬁcation assumption (Bai and Ng 2008), so that we can uniquely
identify ui and vt. Third, the fundamental structure of our problem does not change with these

assumptions because Prec( ˆτ ) is still a quadratic function of

We provide additional discussion on the assumptions in Lemma 3.1 (e.g.,

zt

{

T
t=1 even without these assumptions.
}
i=1 Xiu(cid:62)
i = 0dx,du) in

N

Appendix A.2.

(cid:80)

From Lemma 3.1, maximizing tr

Prec( ˆτ )

is equivalent to

min
{Ai}N
i=1

(cid:0)
(cid:96)+1
j=1 fj,1(z)

(cid:1)
+

(cid:96)+1
j=1 fj,X(z)

+

(cid:96)+1
j=1 fj,U(z)

.

(3.5)

If we can ﬁnd an

f1(z)

(cid:80)

(cid:80)
N
i=1 that simultaneously minimizes each of f1(z), fX(z), and fU(z), then
(cid:124)
}
N
i=1 minimizes (3.5). In the following, we separately analyze the three sub-problems of
}

(cid:80)

Ai

(cid:123)(cid:122)

(cid:123)(cid:122)

(cid:123)(cid:122)

fU(z)

fX(z)

{

(cid:125)

(cid:125)

(cid:125)

(cid:124)

(cid:124)

this

Ai

{

(3.5): min{Ai}N

f1(z), min{Ai}N
We ﬁrst consider the sub-problem min{Ai}N

fX(z), and min{Ai}N

i=1

i=1

i=1

i=1

fU(z).

f1(z). The solution to this problem characterizes

the eﬀect of the presence of two-way ﬁxed eﬀects, αi and βt on the optimal treatment assignments.

Note that f1(z) is a sum of quadratic and linear terms:

(ωj:j(cid:96))(cid:62)P1T(cid:96)

ωj:j(cid:96) + 2b(cid:62)

(cid:96) ωj:j(cid:96)

.

(3.6)

It is possible to provide the analytical solution to min{Ai}N
To build intuition, let us ﬁrst consider a simple case where (cid:96) = 0 (instantaneous eﬀect only).

f1(z) based on its ﬁrst order condition.

i=1

(cid:1)

f1(z) =

(cid:96)+1

j=1
(cid:88)

(cid:0)

8 “

” is the Kronecker product.

⊗

16

Example 3.3. When (cid:96) = 0, following Example 3.2,

f1(z) = (ω1:T )(cid:62)P1T ω1:T + 2b(cid:62)

0 ω1:T ,

(3.7)

. The ω1:T

where ω1:T has ωt = 1
N

N
i=1 zit, P1T = IT

1

T 1T 1(cid:62)

T , and b0 = [b0t] satisﬁes b0t = T +1−2t

T

that solves the ﬁrst order condition of f1(z) takes the form of

(cid:80)

−

N

i=1
(cid:88)

ω∗

t =

1
N

zit =

2t

−

1
T

T

.

−

Plugging zit = 2

1Ai≤t

·

−

1 into the above equation, ω∗

t deﬁnes A(cid:96)=0

A(cid:96)=0 =

Ai

N
i=1 :
}

(cid:40){

1
N

1Ai≤t =

1

2t

−
2T

,

t
∀
(cid:41)

N

i=1
(cid:88)

where any

i=1 1Ai≤t equals the treated fraction at time t. A(cid:96)=0
implies that the optimal treated fraction is linear in time, which is is analogous to the optimal

A(cid:96)=0 is optimal. 1

N
i=1 ∈
}

Ai

{

N

N

(cid:80)

stepped wedge designs in Hemming et al. (2015), Li et al. (2018) whose treated fraction is linear

in time (but with a diﬀerent slope and intercept term).

Analytically solving the optimal ωt for a general (cid:96) is much more challenging as the constraints

“ωt

[
−

∈

1, 1] and ωt is non-decreasing in t” (from Assumption 2.1 and the deﬁnition of ωt) are

binding for some t. The binding constraints aﬀect the optimal treatment fraction at diﬀerent time

periods diﬀerently, and result in a complex structure of the optimal solution. Theorem 1 below

provides the analytical solution to min{Ai}N
complex structure.

i=1

f1(z) for a general (cid:96) that fully characterizes this

Next we consider the other two sub-problems: min{Ai}N

i=1

fX(z) and min{Ai}N

i=1

fU(z). The solu-

tion to these two problems characterize the eﬀects of the presence of Xi and/or ui on the opti-

mal treatment assignments, respectively. Note that both fX(z) and fU(z) are sums of quadratic

{

zt

functions of
nite, following the deﬁnition of two matrices in fX(z) and fU(z), i.e., P1T(cid:96)
MU = P1T(cid:96) ⊗
shown in Theorem 3.1 below, the minimum value of fX(z) can be achieved if

T
t=1 without linear terms. The Hessian of fX(z) and fU(z) are both semideﬁ-
}
T(cid:96) and
U(Idu + U(cid:62)U)−1U(cid:62). Then the minimum possible value of fX(z) and fU(z) is 0. As
AX, and

= IT(cid:96) −

1T(cid:96)1(cid:62)

1
T(cid:96)

the minimum value of fU(z) can be achieved if

{
Theorem 3.1 (Optimality Conditions). Suppose the assumptions in Lemmas 3.1 hold, (cid:96)

Ai

N
i=1 ∈
}

AU.

Ai
{

N
i=1 ∈

}

0, and T > (cid:96)3+13(cid:96)2+7(cid:96)+3

8(cid:96)

.

Ai

{

}

N
i=1 is an optimal treatment design if

Ai

N
i=1 ∈
}

{

Aopt, where

≥

Aopt = A(cid:96)

AX

∩

∩

AU,

(3.8)

with

A(cid:96) =

N
i=1 :

}

1
N

Ai

(cid:40){

1Ai≤t =

1 + ω∗
(cid:96),t
2

t
∀

(cid:41)

N

i=1
(cid:88)

17

AX =

N
i=1 :

µX s.t.

Ai
{
(cid:26)
i=1
(cid:88)
(cid:96),t in A(cid:96) takes the form of
The ω∗

∃

}

N

1
N

Xi1Ai≤t = µX

t
∀
(cid:27)

AU =

{
(cid:26)

Ai

N
i=1 :
}

µU s.t.

∃

1
N

N

i=1
(cid:88)

ui1Ai≤t = µU

.
t
∀
(cid:27)

1
−
((M ((cid:96)))−1b((cid:96)))t−(cid:98)(cid:96)/2(cid:99)

ω∗

(cid:96),t =

T −(cid:96)

1 + 2t−((cid:96)+1)
((M ((cid:96)))−1b((cid:96)))T +1−(cid:98)(cid:96)/2(cid:99)−t T
T

−
−
1

t

(cid:96)/2
(cid:99)
≤ (cid:98)
< t
(cid:96)/2
(cid:98)
(cid:99)
T
(cid:96) < t
≤
(cid:96) < t
(cid:96)/2

(cid:96)
≤
(cid:96)
−
T
≤
< t
(cid:99)






(3.9)

(cid:96)/2

(cid:99)

− (cid:98)

−
− (cid:98)
R(cid:96)−(cid:98)(cid:96)/2(cid:99) provided in (A.1) and (A.2) in

with the deﬁnition of M ((cid:96))

R((cid:96)−(cid:98)(cid:96)/2(cid:99))×((cid:96)−(cid:98)(cid:96)/2(cid:99)) and b((cid:96))

∈

∈

Appendix A.3.

Moreover, if (Xi, ui) can only take G values for a ﬁnite G, denoted as (x1, u1),

, (xG, uG),

each with positive probability, then

g =

i : (Xi, ui) = (xg, ug)
{

}

O

and

N
i=1 is an optimal treatment design if

Ai
{

}

Ai
{

}

· · ·

N
i=1 ∈

Adisc

opt , where

Adisc

opt =

Ai

N
i=1 :
}

{
(cid:26)

1

g
|O

i∈Og
| (cid:88)

1Ai≤t =

1 + ω∗
(cid:96),t
2

t, g

∀

.
(cid:27)

(3.10)

1 + 2

2T −5 , ω∗

(cid:96),t =

−

1 + 2t−3
T −2

−

for t = 3,

, T

−

· · ·

Example 3.4. When (cid:96) = 1, ω∗
Example 3.5. When (cid:96) = 2, ω∗
2

(cid:96),T = 1.
We provide the expression of ω∗

2T −5 , ω∗

(cid:96),T −1 = 1

−

2, ω∗

1,t =

(cid:96),1 =

−

−

1 + 2(t−1)
T −1
1, ω∗

(cid:96),2 =

for all t.

(cid:96),t for (cid:96) = 3 in Example A.1 in Appendix A.3. Figure 2 shows the

optimal treated proportion for other (cid:96) in a T = 10 period problem.

As shown in Theorem 3.1, the presence of αi and βt aﬀects the optimal treatment assignments in

the time dimension only. Speciﬁcally, the presence of αi and βt uniquely deﬁnes the optimal treated

fraction at every time period, but does not restrict whom to treat. The optimal treated fraction

(for each time period) takes an S-shaped curve in time. If the eﬀect of the treatment carries over

to longer periods (i.e., larger (cid:96)), the optimal treated fraction is smaller at the beginning and is

larger towards the end as shown in Figure 2. There are ﬁve stages in the optimal solution: in the

ﬁrst stage, all units are under control; in the second stage, ω∗

t grows non-linearly in time; in the

third stage, ω∗

t grows linearly in time; in the fourth stage, ω∗

t grows non-linearly in time again; in

the ﬁfth stage, all units are under treatment. The optimal solution is symmetric with respect to

the origin.

The presence of Xi and/or ui aﬀects the optimal treatment assignments in the unit dimension

only. Speciﬁcally, the presence of Xi and/or ui imposes restrictions on which units should be

clustered together (into either the treated or control group), but does not impose (additional)

restrictions on when to treat. To be more concrete, without Xi and ui, any treatment design that

1
N 2

O

(cid:0)

(cid:1)

18

satisﬁes treated fraction

1+ω∗
(cid:96),t
2T

discrete, a treatment design is optimal only if the treated fraction

1+ω∗
(cid:96),t
2T

is satisﬁed within each

at time t is optimal; with Xi and ui, and suppose Xi and ui are

stratum, where stratum means the group of units with the same Xi and ui.

There are three practical considerations of Theorem 3.1. First is that there may not exist a

feasible solution in Adisc

opt , which happens for example when

stratum. We suggest an approach in Appendix A.6 that rounds

(cid:96),t)

|Og |(1+ω∗
2T
|Og |(1+ω∗
2T

is not an integer for each
(cid:96),t)

to the nearest integer

to obtain a feasible solution of

Ai
{

N
i=1. As shown in Proposition A.2 in Appendix A.6, the value
}

of tr

Prec( ˆτ )

evaluated at this feasible solution is within a factor of 1 +

of the optimal

value of tr
(cid:0)

Prec( ˆτ )

(cid:1)

.

Second is that we do not know ui in practice. If we have historical control data for the same set

(cid:0)

(cid:1)

of units, then the historical data has information about ui. One can estimate ui from the historical

data and select

N
i=1 based on the estimated ui.
Third is that there may be multiple optimal solutions in Aopt (or Adisc

Ai
{

}

opt ). The experimental designer

can randomly choose one from Aopt (or Adisc

opt ) with equal probability. This random selection can

ensure that

Ai
{

N
i=1 is not confounded with the covariates that are relevant in the outcome model,
}

but are omitted in the design of experiments (Hayes and Moulton 2017).
Remark 3.4 (Magnitude of treatment effects). Note that tr

Prec( ˆτ )

does not depend

on the scale of τ . Therefore, our optimal design that maximizes tr

Prec( ˆτ )
(cid:0)

does not depend on

(cid:1)

the scale of τ .

(cid:0)

(cid:1)

Remark 3.5 (Nonnegativity of tr(Prec( ˆτ ))). Even though there is a minus sign in (3.3),

tr

Prec( ˆτ )

is always nonnegativity. This is because Prec( ˆτ ) is the inverse of Var( ˆτ ), and Var( ˆτ )

is always positive deﬁnite. See (E.8) in Appendix E for more details.

(cid:0)
Remark 3.6 (Without Observed and/or Latent Covariates). The dimensions of Xi

(cid:1)

and ui (i.e., dx and du) can be zero. In this case, if dx = 0, then AX =
A(cid:96)

and
∈
AX. (cid:3)
Ai
(cid:9)
∩
}
{
Remark 3.7 (Conditional Average Treatment Effect (CATE)). An alternative way

AU. If du = 0, then AU =

N
i=1 : Ai
}
AU = A(cid:96)

Ai
{
AX
(cid:8)

AU = A(cid:96)

and A(cid:96)

N
i=1 : Ai

AX

A

A

∩

∩

∩

∩

∈

∩

(cid:8)

(cid:9)

to allow for heterogeneous treatment eﬀects is to use observable sources of heterogeneity. In this

case, we may be interested in CATE (Robinson 1988, Angrist and Pischke 2008, K¨unzel et al. 2019,
Wager and Athey 2018), i.e., E[τj,it] = τj(Xi) for j = 0,

, (cid:96). For discrete (Xi, ui), (3.10) continues

· · ·

to be an optimal design if we seek to maximize the precision of ˆτj(Xi) for all j and Xi, following

from the fact that the optimal treated fraction, ω∗

(cid:96),t, is satisﬁed within each stratum.

Remark 3.8 (Discussion on Adaptive Design). In this section, we study the treatment

assignments that are planned ahead before the experiment starts and do not change during the

experiment. We do not pursue adaptive designs (treatment assignments for subsequent periods can

19

Figure 2

T-optimal design: Optimal treated fraction at each period for a T -period treatment design in the presence

of carryover treatment eﬀects, where T = 10. Diﬀerent colors represent the number of periods (cid:96) to which the treatment

can carry over.

change during the experiment) for ﬁxed-sample-size experiments for the following reason. From

Theorem 1, the information that is useful but unknown before the experiment starts is ui. However,

making adaptive treatment decisions based on the estimated ui is unreliable since, in general, we

need a large T to estimate ui well (Bai 2003, 2009). The estimation of ui can be quite noisy early

in the experiment, and the mistakes we make in the early periods (due to very noisy estimates

of ui) can carry over to later periods. We have veriﬁed this in numerical simulations but did not

include them in the paper, given the paper is quite lengthy already.

Remark 3.9 (Assumption on T ). The assumption T > (cid:96)3+13(cid:96)2+7(cid:96)+3

8(cid:96)

ensures that the mini-

mum value of the third stage is larger than the maximum value of the second stage. This assump-

tion can be potentially relaxed. We veriﬁed using optimization software that (3.8) is optimal even

without this assumption.

4. Sequential Experiments and Adaptive Design

In this section, we study sequential experiments, where N is ﬁxed, but T can vary. In Section 4.1,

We introduce our main algorithm that sequentially selects treatment decisions, estimates treatment

eﬀect, terminates the experiment when suﬃcient estimation precision is reached, and draws post-

experimentation inference. Next, we provide theoretical results that support for each of these

components. For the ease of understanding, we present our algorithm and results based on the

two-way ﬁxed eﬀect model with instantaneous eﬀect only, and without observed or latent covariates,

Yit = αi + βt + τ zit + εit,

(4.1)

(0, σ2

ε ).9 We discuss how our algorithm and results can be generalized to the carryover

where εit

iid
∼

model with covariates in Section 4.3.

9 We use τ instead of τ0 in this section for notation simplicity.

246810t0.00.20.40.60.81.0%treated‘=0‘=1‘=2‘=3‘=4‘=520

Before explaining the algorithm, we introduce a new estimator and provide explain the key

challenges that the algorithm needs to address.

First, note that there exists an eﬃcient approach to estimate τ without explicitly estimating αi

and βt, on which our algorithm in this section is built. This approach is referred to as the within

estimator (Wallace and Hussain 1969) that produces the same estimate of τ as OLS estimator that

IN in (3.1)).10 The within
regresses Yit on zit and unit and time dummies (i.e., GLS with W
estimator regresses ˙Yit on ˙zit, where ˙Yit and ˙zit are the within transformed Yit and zit, respectively.

∝

The within transformation of any variables

xit
{

it∈[N ]×[T ] is deﬁned as
}

˙xit = xit

¯xi·

−

−

¯x·t + ¯x

(4.2)

where ¯xi·, ¯x·t and ¯x are averages of xit’s over time periods, units and both respectively.

In sequential experiments, we have the ﬂexibility to early stop the experiments if needed. A

natural experiment termination rule following the objective function (3.2) is based on precision,

i.e.,

Prec(ˆτ )

c

≥

(4.3)

for some constant c, where for a T -period experiment,

Prec(ˆτ ) =

N
σ2
ε

−

(ω(cid:62)P1T ω + 2b(cid:62)

0 ω)

from Lemma 3.1. The experiment termination rule (4.3) is equivalent to terminating the experiment

when Var(ˆτ ) falls below a threshold. Stopping rules based on precision have also been used in

(Chow and Robbins 1965, Glynn and Whitt 1992, Singham and Schruben 2012).

To run a sequential experiment using a precision-termination stopping rule, we face two key

questions.

1. Implementation of termination rule: Prec(ˆτ ) is unknown as σ2

ε is unknown. How can we

estimate σ2

ε and design a termination rule based on the estimated σ2

ε , such that we can perform

valid statistical inference of τ , post-experimentation?

2. Optimal treatment assignments: The optimal treatment assignments that maximize

Prec(ˆτ ) satisfy 1
N

N

i=1 zit = 2t−1−T

T

from Example 3.3. Since T is unknown, the analyst is gener-

ally not able to make the optimal treatment decisions before the experiment starts. How can we

(cid:80)

adaptively improve the treatment decisions as we gather more information (about T ) during the

experiment?

10 The within estimator is also called as Least-Squares Dummy Variable (LSDV) estimator (e.g., Arellano (2003),
Baltagi (2008), Hsiao (2014)). Unit and time ﬁxed eﬀects are swept out in the within transformed model, i.e.,
˙Yit = τ ˙zit + ˙eit.

21

The ﬁrst question is closely related to the problem considered in sequential testing, as the

estimation of treatment eﬀect can be biased by the sequential tests to terminate the experiment

(Johari et al. 2017, 2020). The second question is conceptually close to adaptive designs in sequential

experiments (Efron 1971, Bhat et al. 2019), online learning and multi-armed bandits (Bubeck and

Cesa-Bianchi 2012, Lattimore and Szepesv´ari 2018).

In this section, we propose an algorithm in Section 4.1 to simultaneously tackle both questions.

In Section 4.2, We show the consistency and asymptotic normality of τ and Var(ˆτ ) estimated from

our algorithm, justifying that the ﬁrst problem is addressed by our algorithm.

4.1. Precision-Guided Adaptive Experiment (PGAE) via Dynamic Programming

In this section, we provide the Precision-Guided Adaptive Experiment (PGAE) algorithm, as

shown in Algorithm 1 below. PGAE uses the ideas from Bayesian statistics, dynamic programming,

and sample splitting to simultaneously the two problems above. PGAE has ﬁve components, as

described below.

1. Partitioning units into static treatment units (STU) and adaptive treatment units (ATU). We

specify a static treatment ratio psta (psta is small), and partition units into the set of STU (

sta)

S

and the set of ATU (

ada), where these two sets are disjoint. We further partition ATU into two

S

disjoint, equally-sized sets

ada,1 and

S

ada,2.

S

We initialize the treatment designs for

sta,

S

ada,1,

S

ada,2 by the optimal designs for a Tmax-period

S

experiment before the experiment starts, where Tmax is the maximum duration of the sequential

experiment. The treatment design of

sta does not change during the experiment, but the treatment

S

ada,2 for subsequent periods are sequentially updated based on the

assignments of

ada,1 and

S

S

experimental data of

sta that have been observed so far. We will use

S
experiment termination rule, and we will use

S
ada,2 to estimate another σ2

ada,1 to estimate σ2

ε for the

ε for post-experimentation

S

inference. As we will see in Section 4.2, partitioning units into three sets is critical for valid statistical

inference from PGAE.

2. Updating our belief about T via empirical Bayes. At time t, we use the observed outcomes

S

of

sta up to time t to construct a belief about T . We ﬁrst construct a belief about σ2

ε and then

map it to the belief about T . To construct a belief about σ2
ε := E[(ε2
ξ2
formulas:

ε )2]. ˆτsta is the within estimator of τ . σ2
σ2

it −

ε and ξ2

ε , we ﬁrst need to estimate τ , σ2

ε , and

ε are estimated from the following

1
(t

ˆσ2
sta,t =

ˆξ2
sta,t =

sta

|S

sta

|S

| ·
t3
(t
|

t

s=1
(cid:88)
1
t

2

ˆτsta ˙zis

˙yis

−

1)

−

i∈Ssta
(cid:88)

(cid:0)
t

( ˙yis

(cid:1)
ˆτsta ˙zis)2

−

ˆσ2
sta,t

−

2

−

(cid:17)

6t
(t

5
1)2 ˆσ4
−
−

sta,t

1)2

−

i∈Ssta (cid:16)
(cid:88)

s=1
(cid:88)

(4.4)

(4.5)

22

1

2

3

4

5

ada,1, and

ada,2, that satisfy

S
ada,1

sta

= N psta, and

|

|S

=

ada,2

|S

|

= N (1

−

psta)/2

|

Function partition initialize(N, psta, Tmax):
sta,
Randomly partition units into three sets ,

S

S

ada,1

ada,2 =

1,
{
for 1

· · ·
s

, N

,
}
Tmax

|S

∪ S
2s−1−Tmax
Tmax

≤
treatment design for

≤
sta with 1(cid:62)Zsta,s =

sta

S
∪ S
ω∗
s,Tmax ←

Zsta

←

S

Zada,1 and Zada,2

treatment designs for

←

1(cid:62)Zada,1,s = 1(cid:62)Zada,2,s =

ada,1

|S

ω∗

s,Tmax

| ·

return Zsta, Zada,1, Zada,2

sta

|S

ω∗

s,Tmax

| ·
ada,2 with

ada,1 and

S

S

where ˙yis and ˙zis are the within transformations of yis and zis for i

sta and s

∈ S

≤

t, respectively.

We then use a normal distribution with parameters ˆσ2

sta,t and ˆξ2

sta,t to approximate the belief about

σ2
ε ,

σ2
ε ∼ N

(cid:16)

which is justiﬁed by Lemma 4.1 below.

(cid:0)

ˆσ2
sta,t,

ˆξ2
sta,t + ˆσ4

sta,t/(t

1)

/

−

sta

|S

| ·

(cid:1)

(cid:0)

t

,

(cid:1)(cid:17)

(4.6)

Next, we map our belief about σ2

ε to a belief about T , denoted by Pt(T ), through Monte Carlo
simulations. Speciﬁcally, we ﬁrst initialize Pt(T ) to zero for all T . Second, we draw a ˜σ2 from (4.6).
Third, we ﬁnd the minimum ˜T such that the estimated precision

f1(zsta,1:T ) is bigger than
) is deﬁned in Example 3.3. Fourth, we increase Pt( ˜T ) by 1/m, where
·

our target level c, where f1(

N
˜σ2 ·

−

m is the number of iterations. Finally, we repeat the second to fourth steps for m

1 times.

−

Function estimate belief T(Z, Y, N, n, Tmax, t, m, c):

1

2

3

4

5

6

7

8

9

within estimator from Y and Z
t
s=1

ˆτ ˙zis

˙yis

2

1
n(t−1)
t3
n(t−1)2

n
i=1
n
i=1

(cid:80)

1
(cid:80)
t
0 for T = 1,

(cid:2)
(cid:80)
, m do
· · ·
draw from

(cid:80)
· · ·

N

←

←

ˆτ

ˆσ2
ˆξ2

←
Pt(T )

←
for j = 1,
˜σ2
˜T
←
Pt( ˜T )

←

end

minimum T such that
(cid:0)
Pt( ˜T ) + 1/m

←

return Pt(

)
·

−
˙yis

t
(cid:0)
s=1

ˆτ ˙zis
(cid:1)

−

2

ˆσ2

2

−

−

6t−5

(t−1)2 ˆσ4

, Tmax
(cid:0)

(cid:1)

(cid:3)

ˆσ2, ( ˆξ2 + ˆσ4/(t

1))/(nt)

−
f1(z1:T )

c
(cid:1)

≥

N
˜σ2 ·

−

23

3. Treatment design decisions for ATU based on our belief about T via dynamic programming.

We use Pt(T ) to update the treatment assignments of ATU at time t + 1. The treated faction at

time t + 1, denoted by ωada,t+1, minimizes the following objective function given the treated fraction

from time 1 to time t, i.e.,

ωada,t+1 =

arg min
ωt+1:ωada,t≤ωt+1≤1
s.t. ωs=ωada,s for s≤t

ET ∼Pt

ω(cid:62)P1T ω + 2b(cid:62)

0 ω

,

(cid:104)

(cid:105)

which is equivalent to maximizing the precision Prec(ˆτ ).

Function update treatment design(Z1, Z2, ˜ω, n, t):
ω(cid:62)P1T ω + 2b(cid:62)

arg minωt+1: ˜ωt≤ωt+1≤1
ωs=˜ωs for s≤t
randomly assign treatment to 0.5n(ωt+1

←
Z1,t+1, Z2,t+1

ET ∼Pt

ωt+1

0 ω

(cid:105)

(cid:104)

←

ωt) control units at time t in

−

Z1, Z2

return Z1,t+1, Z2,t+1

1

2

3

4. Termination of the experiment based on

ada,1. At time t, we estimate the precision Prec(ˆτ )

based on the observed outcomes of

S
ada,1 up to time t,

S

Precada,1,t =

N t
ada,1,t ·

ˆσ2

−

f1,t(zada,1,1:t)

If Precada,1,t

≥

c, we terminate the experiment; otherwise, we keep running the experiment.

Function estimate var prec(Z, Y, N, n, t):

1

2

3

4

within estimator from Y and Z

ˆτ

←

ˆσ2

←
Prec

1
nt

n
i=1

t
s=1

˙yis

N
ˆσ2 f1(z1:t)
(cid:80)
(cid:80)
← −
return ˆσ2, Prec

(cid:0)

2

ˆτ ˙zis

−

(cid:1)

5. Estimation of τ and Var(ˆτ ) post-experimentation. We use the observations of all units (both

STU and ATU) to estimate τ , and we use the observed outcomes of

ada,2 to estimate Var(ˆτ ), i.e.,

S

ˆσ2
ada,2,t =

1

ada,2

(t

1)

t

˙yis

−

ˆτada,2 ˙zis

2

.

i∈Sada,2
(cid:88)
As shown in Algorithm 1, PGAE combines the ﬁve components above, takes advantage of data

s=1
(cid:88)

|S

−

| ·

(cid:0)

(cid:1)

gathered up to the current time and then jointly optimizes treatment assignments alongside the

choice of whether to continue the experiment.

24

Algorithm 1: Precision-Guided Adaptive Experiment (PGAE)
Algorithm page(N, Tmax, c, psta, m)

Zsta, Zada,1, Zada,2

←

partition initialize(N, psta, Tmax)

t

2; run the experiment for t time periods

1

2

3

4

5

6

7

8

9

10

11

12

estimate var prec(Zada,1,1:t, Yada,1,1:t, N, nada, t)

←
ˆσ2
ada,1,t, Precada,1,t

←
while Precada,1,t < c and t < Tmax do

Pt(

)
·

←

Zada,1,t+1, Zada,2,t+1

←

estimate belief T(Zsta,1:t, Ysta,1:t, N, nsta, Tmax, t, m, c)

update treatment design(Zada,1,1:t, Zada,2,1:t, ωada,1:t, nada, t)

Run the experiment for another time period

t

t + 1

←
ˆσ2
ada,1,t, Precada,1,t

end

estimate var prec(Zada,1,1:t, Yada,1,1:t, N, nada, t)

←

within estimator of τ from both STU and ATU

estimate var prec(Zada,2,1:t, Yada,2,1:t, N, nada, t)

ˆτall

←

ˆσ2

ada,2,t, Precada,2,t
return ˆτall and ˆσ2

←

ada,2,t

4.2. Analysis of the Algorithm

In this section, we show the asymptotic properties of the estimates of τ and σ2

ε obtained at various

stages in PGAE. The asymptotic results serve two purposes. First is to justify our approach to

construct a belief about σ2

ε . Second is to show that the outputs from PGAE, i.e, ˆτall and ˆσ2

ada,2,

can be used for valid statistical inference and hypothesis test.

We start by providing a useful lemma that serves the ﬁrst purpose and is a critical intermediate

step to show the results (Theorem 4.1) for the second purpose.

Lemma 4.1. Suppose we estimate τ from the within estimator and estimate σ2

ε from the
same formulas as (4.4) and (4.5), using the experimental data generated from a static treatment
design with N units and T time periods. Suppose εit is i.i.d. with E[εit] = 0, E[ε2
ε )2] = ξ2
σ2

ε . Then for a ﬁnite T , as N

ε and E[(ε2

ε and ξ2

it] = σ2

it −

,

√N T

ˆτ
ˆσ2

(cid:21)

(cid:18)(cid:20)

d
−→ N

,

0
0
(cid:21)

(cid:18)(cid:20)

Στ (ω)
0

(cid:20)

0
ε + 1
T −1 σ4
ξ2

ε (cid:21)(cid:19)

,

→ ∞
τ
σ2
ε (cid:21)(cid:19)
(cid:20)
T b(cid:62)

0 ω

2

−

−1

where Στ (ω) = σ2
ε

N

,

→ ∞

1

T ω(cid:62)P1T ω

−

−

(cid:0)

and ω

∈

1, 1]T with ωt = 1
N

[
−

i zit. Furthermore, as

(cid:1)
√N ( ˆξ2

ε −

ξ2
ε ) = OP (1).

(cid:80)

From Lemma 4.1, ˆσ2

sta,t is asymptotically normal with asymptotic variance ξ2

consistent. Then the normal approximation of the belief about σ2

ε + 1
ε can be justiﬁed from a Bayesian

ε , and ˆξ2

t−1 σ4

sta,t is

25

perspective. Speciﬁcally, as both σ2

ε and ξ2

ε + 1

t−1 σ4

ε are unknown, we can assume a normal-gamma

prior for σ2

ε and ξ†2
ε

(Murphy 2007). We provide a method based on Lemma 4.1 in Appendix B.1

to update the prior by ˆσ2

sta,t and ˆξ2

sta,t. The posterior distribution is a t-distribution that can be

approximated by the normal distribution (4.6), as shown in Appendix B.1.

Remark 4.1. Lemma 4.1 is derived under a ﬁnite T . In fact, if we assume T

, the expression

→ ∞

of the asymptotic variance of ˆσ is simpliﬁed, and we can provide a consistent estimator for ξ2

ε with

a much simpler expression. We study the ﬁnite-T regime as we want to apply Lemma 4.1 to the

estimates from STU early on in the experiment (i.e., T is small).

Next we show that the output of PGAE, ˆτall and ˆσ2

ada,2, can be used for valid statistical inference

and hypothesis test. To show this, there are three critical steps: (a) show the consistency and

asymptotic distribution of ˆτall; (b) show that ˆσ2

ada,2 is a consistent estimator of the asymptotic

variance of ˆτall; (c) show ˆτall and ˆσ2

ada,2 are not biased by the value of ˆσ2

ada,1 (i.e., the experiment

termination rule).

The following theorem provides the joint asymptotic distribution of ˆτall, ˆσ2

ada,1 and ˆσ2

ada,2, which

simultaneously covers the above three critical steps.

Theorem 4.1. Suppose we use PGAE to run the sequential experiment and the experiment is

run for T periods in total. For a ﬁnite T and psta < 1, as N

,

→ ∞

√N T

ˆτall
ˆσ2
ˆσ2

ada,1

ada,2





τ
σ2
ε
σ2
ε

− 







d
−→ N 



where Στ (ω) = σ2
ε






1
T ω(cid:62)P1T ω

−

−

,

0
0
0




2
T b(cid:62)
0 ω


−1

(cid:1)

Στ (ω)
0
0



0
ε + 1
ξ2
0

T −1 σ2

ε

2
1−psta

(cid:0)

2
1−psta

(cid:1)

0
0
ε + 1
ξ2

T −1 σ2

ε

and ω

∈

1, 1]T with ωt = 1
N

[
−

(cid:0)

i∈Ssta∪Sada,1∪Sada,2

zit.

, (4.7)









(cid:1)

There are ﬁve takeaways from Theorem 4.1. The ﬁrst takeaway is that ˆτall estimated on all the

(cid:80)

(cid:0)

sequential experimental data (STU and ATU) is consistent and asymptotically normal. This result

is not obvious, as the treatment design of ATU depends on the observed outcomes of STU, violating

the weak exogeneity assumption that is commonly imposes to show an estimator’s consistency. We

can see how the weak exogeneity assumption is violated from the expression of the within estimator,

ˆτall =

1
N T

(cid:18)

1
N T

˙z2
it

i,t
(cid:88)

(cid:19)(cid:18)

˙zit ˙yit

,

(cid:19)

i,t
(cid:88)

where ˙zit and ˙yit are the within transformation of zit and yit on both STU and ATU. Note that

zjs and yjs have nonzero weights in ˙zit and ˙yit for all j and s. As the the average of zit for

i

∈ S

ada,1

∪ S

ada,2 depends on ˆσ2

sta,t estimated from

sta, ˙zit is correlated with ˙εit, violating the weak

S

exogeneity assumption. A simple ﬁx is to ATU only to estimate τ , but this solution is not ideal as

it does not eﬃciently use all experimental data. We take a more complex path by using all data and

26

showing the asymptotic properties of ˆτall without the weak exogeneity assumption. See Appendix

B.2 for more discussion on the intuition of the proof.

The second takeaway is that the asymptotic distribution of ˆτall is the same as that in Lemma

4.1 given the same ω. Therefore, ˆτall achieves the optimal convergence rate. The loss of eﬃciency

in ˆτall is no more than a constant order that comes from a suboptimal ωsta and a suboptimal ωada,t

for a small t when we do not have much knowledge about the duration of the experiment.

The third takeaway is that the asymptotic variance of ˆτall can be consistently estimated by

multiplying ˆσ2

ada,2 and the constant

−
The fourth takeaway is that PGAE can “correctly” terminate the experiment with a high prob-

1

T ω(cid:62)P1T ω

2

T b(cid:62)

0 ω

, following the consistency of ˆσ2

ada,2.

−
(cid:0)

−1

(cid:1)

ability for a large N , following from the asymptotic properties of ˆσ2

ada,1. We refer to “correct” as

both the true precision and the estimated precision from ˆσ2

ada,1 exceed threshold c.

The last and most important takeaway is that PGAE does not suﬀer from the “peeking” chal-

lenge: ˆτ and ˆσ2

ada,2 are not biased by the experiment termination rule based on ˆσ2

ada,1. This is because

ˆτall, ˆσ2

ada,1 and ˆσ2
conditional on ˆσ2

ada,2 are mutually asymptotically independent as shown in Theorem 4.1. Therefore,
ada,1, the joint asymptotic distribution of ˆτall and ˆσ2

ada,2 stays the same. As a result,

the test statistic or conﬁdence interval as a function of ˆτall and ˆσ2

ada,2 is asymptotically independent

of the experiment termination rule. The mutual asymptotic independence is achieved by careful

sample splitting and careful analyses of diﬀerent sources of randomness in these estimators, where

we provide more discussion in Appendix B.2.

4.3. Generalization to Carryover Model with Covariates

observed covariates. We can continue to use the within estimator for

PGAE and Theorem 4.1 can be easily generalized to the model with carryover eﬀects and
j=0 by regressing ˙Yit on
(cid:96)
}
c

, ˙zi,t−(cid:96), ˙Xi. For the experiment termination rule, we can generalize 4.3 to tr

Prec( ˆτ )

τj
{

˙zit,

· · ·

or other criteria deﬁned based on the objectives discussed in Section 3.2. From Lemma 3.1, the

(cid:0)

only unknown parameter for the criteria deﬁned by either Prec( ˆτ ) or Var( ˆτ ) is σ2

ε . Then we can

continue to use ﬁve components in PGAE to construct a belief about T , make adaptive treatment

decisions, and sequentially decide whether to terminate the experiment. We can follow the same

proof to show that Lemma 4.1 and Theorem 4.1 continue to hold with ˆτ /ˆτall replaced by ˆτ / ˆτall

and with Στ (ω) replaced by a more general deﬁnition of the covariance matrix (see Lemma E.1 for

its deﬁnition).

In the model has latent covariates, the within estimator does not directly apply if we want to

account for ui in the estimation of

(cid:96)
j=0, but we can use GLS as (3.1). In PGAE, we need
}
ε alongside ui when making adaptive treatment decisions and deciding whether to

τj
{

to estimate σ2

terminate the experiment.

≥

(cid:1)

27

5. Empirical Applications

In this section, and in Appendix D, we run synthetic experiments on multiple real data sets to

compare our solutions from Sections 3-4 with benchmark designs. First, we introduce four data

sets from multiple domains in 5.1. Next, in 5.2.2, we show that for ﬁxed-sample-size experiments

our designs from Section 3.3 require less than 50% of the sample size to achieve the same treatment

eﬀect estimation error as the benchmark designs.Then for sequential experiments, in 5.3, we show

that our adaptive design from PGAE can improve the precision of treatment eﬀect estimation by

more than 20%, on top of the improvements obtained by our ﬁxed-sample-size design.

5.1. Data Descriptions

We introduce four diﬀerent data sets here. The ﬁrst one, MarketScan Research Databases, is used

for the empirical results of this section, and the same results are presented, as robustness checks,

on the remaining three data sets in Appendix D.

MarketScan research databases. These databases (formerly called Truven) contain inpatient and

outpatient claim records from the beginning of 2007 to mid-2017. We focus on both inpatient and

outpatient records, where the primary diagnosis of the patient is inﬂuenza, according to ICD-9-CM

diagnosis codes.11 Since these are claims data, unlike electronic medical records, we can see all

clinical visits of every patient for the duration of their enrollment.12 Empowered by this unbiased

coverage patient visits in our data, we notice that patients are not usually admitted to hospitals for

inﬂuenza, as there are 21,277 inpatient admissions and 9,678,572 outpatient records with primary

diagnosis inﬂuenza. We denote all of these as inﬂuenza visits. Our outcome variables are ﬂu visit

occurrence rates per Metropolitan Statistical Area (MSA) and month.13 Here, ﬂu visit occurrence

rate is deﬁned as the ratio of the number of inﬂuenza visits among all enrolled patients for the given

month in a given MSA. Moreover, our analysis focuses on the ﬂu peak seasons that are deﬁned

as October this year to April next year. We further focus on the period between October 2007 to

April 2015 as the databases have few observations outside this period. We then obtain a panel of

185 MSAs over 56 months.

Home medical visits. This data set has 40,079 records of home medical visits from Jan 2016 to

Dec 2018 in the metropolitan area of Barcelona Spain.14 This data set has been used to study

11 These databases only have claim records with ICD-9 diagnosis codes. In ICD-9-CM, the following diagnosis codes
indicate inﬂuenza: 488, 487.0, 487.1, 487.8, 488.0, 488.1, 488.01, 488.02, 488.09, 488.11, 488.12, 488.19, 488.81, 488.82,
and 488.89.
12 Electronic medical records typically contain visits to a few providers and do not capture clinical visits to other
providers.
13 According to US Census Beureau, MSA have at least one urbanized area with a minimum population of 50,000
https://www.census.gov/programs-surveys/metro-micro/about.html.
14 This data set is publicly available on kaggle and can be downloaded at https://www.kaggle.com/ckroxigor/
home-medical-visits-eda/.

28

how the environmental factors adversely aﬀect vulnerable people to environmental agents (climate,

pollution, etc). We aggregate this data at the city level, as many environmental policies are carried

out at an aggregate level. Given the high noise in the number of visits, we consider the 16-week

moving average of medical visits. Then we obtain a panel of 61 cities across 144 weeks.

Transactions from a large grocery store. This data set contains 17,880,248 transactions from a

large grocery store between May 2005 and May 2007.15 We aggregate the transactions by household

and week. Our analysis focuses on “frequent” households deﬁned as those who had expenditure in

at least half of the weeks in the data set. These households tend to pay more attention to changes

in the loyalty program. We then obtain a panel of 7, 130 frequent households over 97 weeks.

Lending Club loan data. This data set contains 2,260,668 loans issued from June 2007 to Decem-

ber 2018 on Lending Club.16 This data set contains information, such as the current loan status

(Current, Late, Fully Paid, etc.), latest payment information, ﬁrst three digits of zip codes and

issued month. We aggregate the number of loans issued by month and by the ﬁrst three digits of

zip codes. We get a panel of 956 units over 139 months.

The ﬁrst three panels have positive entries, but 38.1% of the last panel entries are equal to 0.

5.2. Fixed-Sample-Size Experiments

We ﬁrst, Section 5.2.1, discuss what choices were made in setting up the synthetic experiments

and evaluations and then present the results in Section 5.2.2.

5.2.1. Fixed-Sample-Size Experiments: Setup Here, we ﬁrst introduce the benchmark

designs as well as diﬀerent versions of our solution, depending on the modeling assumptions. Then

we explain how the synthetic experiment and treatment eﬀect are generated, and then discuss the

evaluation metrics that are used.

Treatment designs. We consider the following treatment designs. Illustrations of the designs

are provided in Figure 5 that can facilitate the reading.

1. Benchmark treatment designs:

(a) Zﬀ (ﬁfty-ﬁfty): Zﬀ has 50% control and 50% treated at every time period. More precisely,

Zﬀ is a rounded solution when starting with ωt = 0 for all t.

(b) Zba (before-after): Zba has all units in the control state before halftime and all units in

the treatment state after halftime. More precisely, Zba is a rounded solution when starting with

ωt =

1 for t < T +1

2 and ωt = 1 for t > T +1
2 .

−

15 This data set is available to researchers at Stanford and Berkeley by application. Papers that use this data set is
available at https://are.berkeley.edu/SGDC/publications.html.
16 This data set can be downloaded at https://www.kaggle.com/wordsforthewise/lending-club

29

(c) Zﬀba (ﬁfty-ﬁfty with before-after): Zﬀba has all units in the control state before halftime

and half units in the treatment state after halftime. More precisely, Zﬀba is a rounded solution

when starting with ωt =

2 and ωt = 0 for t
simultaneous treatment adoption pattern (Bertrand et al. 2004, Angrist and Pischke 2008).

T +1
2 . Zﬀba combines Zﬀ and Zba, and has

1 for t < T +1

−

≥

2. Variations of Zopt from Section 3: In order to assess the beneﬁt of various features of our

model (2.1), we consider three diﬀerent designs, each one is a variant of Zopt, but with diﬀerent

assumptions on the data and the treatment eﬀect.

(a) Zopt,linear: This design assumes treatment eﬀect has no carryover eﬀect ((cid:96)=0) and no

covariates. This design can in fact be considered as the “state-of-the-art” benchmark design since it

treated fraction is linear in time. To generate it, we sample

is analogous to the optimal stepped wedge designs in Hemming et al. (2015), Li et al. (2018) whose
i=1 from A0 (where A0 is deﬁned
N
}
{
N
i=1 uniquely deﬁnes the design. Zopt,linear has indeed a linear staggered
}

in Example 3.3) and

Ai

Ai

design, and is optimal when the outcome model does not have covariates and the treatment has

{

instantaneous only.

(b) Zopt: This design assumes that the treatment eﬀect carries over (cid:96) periods, with a known

value (cid:96), but does not assume there are covariates. To construct it, we ﬁrst sample
A(cid:96) (recall (3.8) for the deﬁnition of A(cid:96)). The sampled

Ai

Ai

N
i=1 from
}

{

N
i=1 uniquely deﬁnes Zopt. Zopt is a
}

{

nonlinear staggered design, and is optimal when the outcome model does not have covariates and

the treatment has (cid:96) periods of lagged eﬀects.

(c) Zopt,stratiﬁed: This design assumes all features of our model, both carryover eﬀects with

known (cid:96) and presence of discrete-valued latent covariates. The value of this design is only demon-

strated when historical control data is available, which is a realistic assumption in practice. We

can estimate the latent covariates via spectral clustering. Speciﬁcally, we perform singular value

decomposition on the historical data and cluster units into k groups (assuming the discrete-valued

let k vary in

latent covariate takes k distinct values) by applying k-means on the largest singular vector. We
. Let ˆU be a vector indicating which cluster each unit belongs to. We sample
}
A ˆU (recall (3.8) for the deﬁnition of A ˆU). The sampled

2, 3, 4

Ai

Ai

{

N
i=1 uniquely deﬁnes
}

{

{
Zopt,stratiﬁed which is a nonlinear staggered design with stratiﬁcation.

∩

i=1 from A(cid:96)
N
}

Synthetic treatments. Since we are not aware of any speciﬁc experiment that was performed

on the data, we assume it is all control data (i.e., original panel data entries are Yit(

1(cid:96)+1), for all

−

i and t), and then create a hypothetical treatment with instantaneous and lagged eﬀects. Given

a treatment design Z, the observed outcome (in a hypothetical experiment) for unit i at time t

would be (recall that zit

1, +1
}

)

∈ {−

Yit = Yit(

1(cid:96)+1) +

−

(cid:96)

(zi,t−j + 1)τj ,

j=0
(cid:88)

30

where zis =

1 for s

−

≤

0. For the results presented in Section 5.2.2, (cid:96) is chosen at 2. We consider

other values of (cid:96) in Appendix D. The precision of GLS does not depend on the true value of

instantaneous and lagged eﬀects as discussed in Remark 3.4. For illustration purposes, we set the
lagged eﬀects to decay linearly in lag, τ1 = 2
3 τ0, τ2 = 1
= 0.1
N T

3 τ0, τj = 0 for j > 2, and the total treatment
1(cid:96)+1). The latter selection means the total treatment eﬀect has

τ0 + τ1 + τ2

i,t Yit(

eﬀect

|

|

−

a magnitude that is 10% of the average outcome in the panel. We verify that our results are robust

(cid:80)

to other values of

(cid:96)
j=0 in Figure 11.

τj
{

}

Evaluation metrics. Instead of running a single simulation on the entire panel of control data,

we m random blocks of dimension N
×
(j > 0) on k-th block are denoted as ˆτ (k)
0

T . The estimated instantaneous and j-th lagged eﬀects
and ˆτ (k)

, respectively. Note that, while diﬀerent designs

j

make diﬀerent assumptions on the data model, some of them without a carryover eﬀect, we can

still estimate all eﬀects using GLS and model (2.1). As a robustness check, We will compare

diﬀerent estimation models as well. We report the mean and 95% conﬁdence band of total squared

estimation error

(cid:96)
j=0

ˆτ (k)
j −

τj

2

(related to the objective of A-optimal design, deﬁned in Section

3.2). As a robustness check, we also report the squared estimation error of total treatment eﬀect

(cid:1)

(cid:96)

j −

j=0 ˆτ (k)
We further evaluate various designs by the metrics related to hypothesis testing. When we

(cid:0) (cid:80)
conduct hypothesis testing, we are interested in true positives (TP), true negatives (TN), false

(cid:80)

(cid:1)

.

(cid:0)
2

(cid:80)
(cid:96)
j=0 τj

positives (FP), and false negatives (FN) in the confusion matrix. For each j, 0

class of τj is deﬁned as

τj

{|

, and the negative class is deﬁned as
= c
}

|

τj

{|

|

(cid:96), the positive

≤
. As mentioned

j

≤
= 0
}

above, the values of the treatment eﬀects do not impact the precision and therefore we select all

other

τj(cid:48)

|

|

for j(cid:48)

= j equal to c as well (in Appendix D, we will empirically conﬁrm that varying c

does not change the results). On each randomly selected block from the original control data, we

then run a pair of synthetic experiments such that τj is set to c (positive class) in one experiment,

or to 0 (negative class) in the other experiment. We then calculate the t-statistic of the estimated

instantaneous and lagged eﬀects for all of these 2m experiments: If the absolute t-statistic is above

some threshold ι, the estimated class is positive; otherwise, the estimated class is negative. We can

then compare the true class with the estimated class on all 2m experiments, and count TP, TN,

FP and FN. If we vary the threshold ι, in the same spirit as when receiver operating characteristic

(ROC) curve for a binary classiﬁcation problem is generated, then the estimated class may change,

and TP, TN, FP and FN may change as well. In fact FP and FN are type I and type II errors of

the test. Therefore, we can vary ι and study, for various treatment designs, how the TP rate, or

equivalently, power, varies with the FP rate, or equivalently, signiﬁcance level. Similarly, we can
study how the precision ( TP

TP+FP ) varies with the recall ( TP

TP+FN ). Overall, for each j, 0

(cid:96), we

j

≤

≤

obtain one ROC curve and one precision-recall curve.

(cid:54)
31

5.2.2. Fixed-Sample-Size Experiments: Results

Staggered treatment designs outperform benchmark designs. The left subplot in Figure 3 shows

the total estimation error

j(ˆτj

−

τj)2 of our nonlinear staggered design Zopt and benchmark

designs Zﬀ, Zba and Zﬀba. Both Zopt and Zﬀba consistently and signiﬁcantly outperform Zba and

(cid:80)

Zﬀ. The design Zﬀba, as a combination of Zﬀ and Zba, performs signiﬁcantly better, but is still

outperformed by Zopt. Speciﬁcally, by using only 50% of the sample size (N = 25 versus N = 50),

Zopt achieves lower estimation error than Zﬀba.

Figure 3

These ﬁgures compare the total squared estimation error (cid:80)

j(ˆτj

τj)2 from GLS using benchmark

−

treatment designs (Zﬀ , Zba and Zﬀba), our linear staggered design (Zopt,linear), our nonlinear staggered design (Zopt)

and our nonlinear staggered design with stratiﬁcation (Zopt,stratiﬁed) on the ﬂu data. These ﬁgures show the mean and
95% conﬁdence band of (cid:80)

τj)2 that are calculated from 2,000 randomly sampled blocks of dimension N

T

×

j(ˆτj
for (cid:96) = 2, T = 7, and varying N .

−

Nonlinear staggered design outperforms linear staggered design. The right subplot in Figure 3

compares our nonlinear staggered design Zopt with the linear staggered design Zopt,linear. In the

presence of carryover eﬀects, Zopt,linear requires 10% more samples than Zopt to achieve the same

estimation error. Note that the improvement is solely because of the carryover eﬀects. In fact, if

the treatment only has an instantaneous eﬀect, the treated fraction of Zopt,linear is optimal. We can

conﬁrm this empirically (in Figure 12 of Appendix D) by observing that Zopt,linear requires about

5% fewer samples than Zopt.

Stratiﬁcation further improves upon the staggered treatment design. The right subplot in Figure

3 additionally compares our nonlinear staggered designs without stratiﬁcation Zopt and with strat-

iﬁcation Zopt,stratiﬁed. The stratiﬁcation is based on spectral clustering of the units, using historical

data. Using Zopt,stratiﬁed can further reduce 20% samples to achieve the same total estimation error.

In Appendix D we show that the result only slightly ﬂuctuates by varying the number of clusters.

Overall, this result suggests the existence of latent covariates in the original data. Therefore, when

304050N012345Pj(ˆτj−τj)2designZﬀZbaZﬀbaZopt304050N0.0750.1000.1250.1500.1750.2000.225Pj(ˆτj−τj)2designZopt,linearZoptZopt,stratiﬁed32

there are latent covariates, we could use the historical data that contains information about latent

covariates to design a stratiﬁed experiment.

Robustness on other data sets. Figure 9 in Appendix D shows that the above three ﬁndings

continue to hold on the other three data sets, as N is varied. Figure 10 in Appendix D shows the

above three ﬁndings continue to hold on all four data sets, as T is varied.

Robustness to other evaluation metrics. The above three ﬁndings are robust to other evaluation

metrics. Figure 16 in Appendix D shows the squared estimation error of total treatment eﬀect

of various treatment designs on all four data sets. The ﬁndings from Figure 16 are aligned with

those from Figure 3. Figure 4 shows the ROC curve of various designs (i.e., power vs. signiﬁcance

level) on the ﬂu data for testing each of τ0, τ1, and τ2, for when signiﬁcance level is up to 10%.

Zopt,stratiﬁed has a consistently higher power than all other designs, at all signiﬁcance levels. Zopt

also outperforms or nearly ties with Zopt,linear. And the three of these designs dominate Zﬀ, Zba,

and Zﬀba. Their corresponding area under the curve (AUC) values (for the full ROC curves) are

shown in Table 1. The AUC of Zﬀ and Zba are consistently and signiﬁcantly lower than the AUC

of other treatment designs in the test of τ0, τ1 and τ2. The AUC of Zopt is consistently higher than

that of Zﬀba and the improvement is more noticeable for τ1. Consistent with the previous metric,

Zopt,stratiﬁed further improves upon Zopt.

Figure 4

ROC curve. The TP and FP rates are calculated from 2,000 pairs of synthetic experiments with

dimension 50

×

7 and (cid:96) = 2 on the ﬂu data. The true positive class of τj is deﬁned as

τj

{|

=

|

−

0.1
N T

(cid:80)

i,t Yit(

1(cid:96)+1)

.

}

−

Zﬀ

τ0 0.614
τ1 0.598
τ2 0.590

Zba

0.633
0.563
0.629

Zﬀba

0.744
0.702
0.738

Zopt

0.750
0.745
0.748

Zopt,linear Zopt,stratiﬁed

0.740
0.740
0.736

0.761
0.756
0.762

Table 1

The area under the curve (AUC) of various treatment designs and hypothesis tests in Figure 4.

0.000.050.10FPR(signiﬁcancelevel)0.00.20.40.6TPR(power)testofτ00.000.050.10FPR(signiﬁcancelevel)0.00.20.40.6TPR(power)testofτ10.000.050.10FPR(signiﬁcancelevel)0.00.20.40.6TPR(power)testofτ2ZﬀZbaZﬀbaZopt,linearZoptZopt,stratiﬁed33

The estimation model with two-way ﬁxed eﬀects and covariates is preferable and Zopt,stratiﬁed

performs best under various estimation model speciﬁcations. We compare the performance of var-

ious treatment designs, when the estimation model varies. The ﬁve model speciﬁcations and the

zit,

, zi,t−(cid:96) and the intercept term, which is the same as the diﬀerence-in-means estimator.

+ τ(cid:96)zi,t−(cid:96) + εit. We run a linear regression of Yit on

corresponding treatment eﬀect estimators are
• “no fe”: The model is Yit = c + τ0zit +

· · ·

· · ·
• “unit fe only”: The model is Yit = αi + τ0zit +

zit,

, zi,t−(cid:96) and unit dummy variables.

· · ·
• “time fe only”: The model is Yit = βt + τ0zit +

zit,

, zi,t−(cid:96) and time dummy variables.

· · ·
• “two-way fe”: The model is Yit = αi + βt + τ0zit +

+ τ(cid:96)zi,t−(cid:96) + εit. We run a regression of Yit on

+ τ(cid:96)zi,t−(cid:96) + εit. We run a regression of Yit on

· · ·

· · ·

+ τ(cid:96)zi,t−(cid:96) + εit. We use (3.1) with W

IN

∝

· · ·

· · ·

∝

(cid:0)

(equivalent to the within estimator and the regression of Yit on zit,

, zi,t−(cid:96), unit and time dummy

variables.

• “two-way fe+covar”: The model is (2.1). We use (3.1) with W

ˆU ˆΣv ˆU(cid:62) + ˆσ2

ε IN

−1

.

Figure 5 provides examples of T-optimal designs that maximize (3.2) under each of the above ﬁve

(cid:1)

model speciﬁcations. Figure 6 compares the total estimation error of diﬀerent treatment designs

under the above estimation model speciﬁcations. There are three ﬁndings from Figure 6:

1. Allowing for time ﬁxed eﬀects in the outcome model (i.e., “time fe only”, “two-way fe”, and

“two-way fe+covar”) can signiﬁcantly reduce the estimation error. This makes sense as the ﬂu

occurrence rate ﬂuctuates by month, and time ﬁxed eﬀects can capture this seasonality.

2. Allowing for covariates in the estimation model (i.e., “two-way fe+covar”) can further reduce

the estimation error.

3. Even though optimal designs vary with model speciﬁcations, as illustrated in Figure 5,

Zopt,stratiﬁed consistently performs best under various estimation model speciﬁcations, implying that

“no fe”, “unit fe only”, “time fe only”, and “two-way fe” are probably misspeciﬁed.

In summary, to maximally reduce the estimation error, both the treatment decisions (design) and

the estimation model speciﬁcation play major roles.

5.3. Sequential Experiments

In this section, we run synthetic adaptive experiments and evaluate performance of the adaptive

design from PGAE. We describe the experimental setup in Section 5.3.1 and then present the

results in Section 5.3.2.

5.3.1. Sequential Experiments: Setup

Suppose the sequential experiment can run for a maximum of Tmax periods in total and the hypo-
thetical intervention has instantaneous eﬀect only.17 The sequential experiment is terminated if the

17 The results are robust to the hypothetical intervention with carryover eﬀects, and are available upon request.

34

Figure 5

Optimal treatment designs under various model speciﬁcations. These ﬁgures provide examples

of optimal designs of 7-period experiments with (cid:96) = 2. Darker color denotes treatment, while lighter color denotes

control. An optimal design for “no fe” is Zﬀ ; an optimal design for “unit fe only” is Zba; an optimal design for

“time fe only” is Zﬀ ; an optimal design for “two-way only” is Zopt; an optimal design for “two-way only+covar” is

Zopt,stratiﬁed.

Figure 6

Estimation error under various model assumptions. Instantaneous and lagged eﬀects are estimated

under various model assumptions from 1,000 synthetic experiments of dimension 25

7 with (cid:96) = 2 on the ﬂu data.

×

The model assumptions are identical to those in Figure 5.

estimated precision is larger than some threshold. Let T ∗ be the number of periods for which the

sequential experiment is run. The number of units N is ﬁxed with psta fraction to be STU.

Treatment designs. Overall, we consider the following three designs.

1. Adaptive design: The design produced by PGAE.

2. Benchmark design: Optimal (ﬁxed-sample-size) design for a Tmax-period experiment. From

results of Section 3 we can fully characterize the design via 1
N

i zis = 2s−1−Tmax

Tmax

. This is identical

to Zopt,linear when T = Tmax.

(cid:80)

3. Oracle design: Optimal design for a T ∗-period experiment with 1
N

i zis = 2s−1−T ∗

T ∗

(identical

to Zopt,linear when T = T ∗).

(cid:80)

Synthetic sequential experimental data. Similar to synthetic ﬁxed-sample-size experiments,

the original data does not contain any speciﬁc treatment that we study. Given a treatment design

Z, the observed outcome for unit i at time t (t

T ∗) is

≤

Yit = Yit(

1) + τ0(zit + 1).

−

0123456nofe0123456unitfeonly0123456timefeonly0123456two-wayfe0123456two-wayfe+covarZﬀZbaZﬀbaZopt,linearZoptZopt,stratiﬁeddesign024681012Pj(ˆτj−τj)2TotalestimationerrorundervariousmodelassumptionsZopt,linearZoptZopt,stratiﬁeddesign0.00.10.20.30.4Pj(ˆτj−τj)2Zoomedinnofeunitfeonlytimefeonlytwo-wayfetwo-wayfe+covar35

Evaluation metrics. Similar to synthetic ﬁxed-sample-size experiments, we randomly select m

blocks, each with dimension N

Tmax from the original control data. We report the mean and 95%

conﬁdence band of instantaneous eﬀect estimation error

×

ˆτ (k)
0 −

τ0

2

, where ˆτ (k)

0

is the estimated

T ∗ based on the k-th

×

instantaneous eﬀect on the synthetic experimental data of dimension N

(cid:0)

(cid:1)

block of the original data (T ∗ can vary with k).

5.3.2. Sequential Experiments: Results

Figure 7 compares the estimation error of ˆτ0 on the experimental data generated from adaptive,

benchmark, and oracle designs. The adaptive design from PGAE can consistently reduce the esti-

mation error (i.e., improve the precision) by more than 20% compared to the benchmark design

(i.e., ﬁxed-sample-size design). The diﬀerence between adaptive and benchmark designs measures

the beneﬁt of making treatment decisions adaptively in sequential experiments.

Figure 7

This ﬁgure compares (ˆτ0

−

τ0)2 for adaptive, benchmark, and oracle designs, where the duration of the

experiment depends on the estimated variance of the treatment eﬀect. N is chosen at 50. In P-AGE, N psta = 50
10 units are STU. τ0 is chosen at −0.1
N T

i,t Yit(

(cid:80)

0.2 =

×

−

1) (i.e., 10% of the average monthly ﬂu occurrence rate). This
τ0)2 based on 500 randomly sampled blocks of dimension

ﬁgure shows the mean and 95% conﬁdence interval of (ˆτ0

−

Tmax.

N

×

On the other hand, the adaptive design consistently has a larger estimation error than the oracle

design. The diﬀerence between adaptive and oracle designs measures the loss in precision from not

knowing T before the experiment starts and from the eﬃciency loss due to sample splitting (the

STU partition). Note that there is still a gap between adaptive and oracle designs for the largest

Tmax in Figure 7. If we seek to reduce the gap, we can increase N and set a higher threshold for

the estimated precision to exceed in the termination rule.

6. Concluding Remarks

In this paper, we study the optimal design of panel data experiments. Panel data experiments are

particularly useful to study the treatments that have causal eﬀects on both current and future

203040Tmax0.00.20.40.60.81.01.2(ˆτ0−τ0)2benchmarkoracleadaptive36

outcomes. Our goal is to optimally make treatment decisions for every unit at every time period, in

anticipation of most precisely estimating the instantaneous and lagged eﬀects. This optimization

problem can reduce the sample size requirement and directly minimize the experiment’s opportu-

nity cost in practice. We ﬁrst study the ﬁxed-sample-size experiments, where the sample size is ﬁxed

and treatment decisions are made pre-experimentation, and we provide a near-optimal solution to

the optimization problem. We further study sequential experiments, where the experiments can

be stopped early if needed. We propose the Precision-Guided Adaptive Experimentation (PGAE)

algorithm for sequential experiments. PGAE makes treatment decisions adaptively and allows for

valid post-experimentation inference. Finally, synthetic experiments on healthcare, grocery, and

ﬁnancial data sets show that our proposed solutions for ﬁxed-sample-size and sequential experi-

ments reduce the opportunity cost of the experiments by over 50% and 70% respectively, compared

to benchmarks.

References

Abadie, Alberto. 2005. Semiparametric diﬀerence-in-diﬀerences estimators. The Review of Economic Studies

72(1) 1–19.

Abadie, Alberto, Alexis Diamond, Jens Hainmueller. 2010. Synthetic control methods for comparative case

studies: Estimating the eﬀect of california’s tobacco control program. Journal of the American statistical

Association 105(490) 493–505.

Abadie, Alberto, Alexis Diamond, Jens Hainmueller. 2015. Comparative politics and the synthetic control

method. American Journal of Political Science 59(2) 495–510.

Abadie, Alberto, Javier Gardeazabal. 2003. The economic costs of conﬂict: A case study of the basque

country. American economic review 93(1) 113–132.

Abadie, Alberto, Jinglong Zhao. 2021.

Synthetic controls for experimental design.

arXiv preprint

arXiv:2108.02196 .

Abaluck, Jason, Laura H. Kwong, Ashley Styczynski, Ashraful Haque, Md. Alamgir Kabir, Ellen Bates-

Jeﬀerys, Emily Crawford, Jade Benjamin-Chung, Shabib Raihan, Shadman Rahman, Salim Benhachmi,

Neeti Zaman Bintee, Peter J. Winch, Maqsud Hossain, Hasan Mahmud Reza, Abdullah All Jaber,

Shawkee Gulshan Momen, Aura Rahman, Faika Laz Banti, Tahrima Saiha Huq, Stephen P. Luby,

Ahmed Mushﬁq Mobarak. 2021. Impact of community masking on covid-19: A cluster-randomized trial

in bangladesh. Science 0(0) eabi9069. doi:10.1126/science.abi9069. URL https://www.science.org/

doi/abs/10.1126/science.abi9069.

Angrist, Joshua D, J¨orn-Steﬀen Pischke. 2008. Mostly harmless econometrics: An empiricist’s companion.

Princeton university press.

Arellano, Manuel. 2003. Panel data econometrics. Oxford university press.

37

Athey, S, S Stern. 2002. The impact of information technology on emergency health care outcomes. The

Rand Journal of Economics 33(3) 399.

Athey, Susan, Guido W Imbens. 2017. The econometrics of randomized experiments. Handbook of Economic

Field Experiments, vol. 1. Elsevier, 73–140.

Athey, Susan, Guido W Imbens. 2021. Design-based analysis in diﬀerence-in-diﬀerences settings with stag-

gered adoption. Tech. rep.

Atkinson, AC, VV Fedorov. 1975a. The design of experiments for discriminating between two rival models.

Biometrika 62(1) 57–70.

Atkinson, Anthony, Alexander Donev, Randall Tobias. 2007. Optimum experimental designs, with SAS ,

vol. 34. Oxford University Press.

Atkinson, Anthony C, Valerii Vadimovich Fedorov. 1975b. Optimal design: Experiments for discriminating

between several models. Biometrika 62(2) 289–303.

Bai, Jushan. 2003. Inferential theory for factor models of large dimensions. Econometrica 71(1) 135–171.

Bai, Jushan. 2009. Panel data models with interactive ﬁxed eﬀects. Econometrica 77(4) 1229–1279.

Bai, Jushan, Serena Ng. 2002. Determining the number of factors in approximate factor models. Econometrica

70(1) 191–221.

Bai, Jushan, Serena Ng. 2008. Large dimensional factor analysis. Now Publishers Inc.

Baltagi, Badi. 2008. Econometric analysis of panel data. John Wiley & Sons.

Basse, Guillaume, Yi Ding, Panos Toulis. 2019. Minimax designs for causal eﬀects in temporal experiments

with treatment habituation. arXiv preprint arXiv:1908.03531 .

Bastani, Hamsa, Mohsen Bayati. 2020. Online decision making with high-dimensional covariates. Operations

Research 68(1) 276–294.

Bertrand, Marianne, Esther Duﬂo, Sendhil Mullainathan. 2004. How much should we trust diﬀerences-in-

diﬀerences estimates? The Quarterly journal of economics 119(1) 249–275.

Bertsimas, Dimitris, Mac Johnson, Nathan Kallus. 2015. The power of optimization over randomization in

designing experiments involving small samples. Operations Research 63(4) 868–876.

Bertsimas, Dimitris, Nikita Korolko, Alexander M Weinstein. 2019. Covariate-adaptive optimization in online

clinical trials. Operations Research .

Bhat, Nikhil, Vivek F Farias, Ciamac C Moallemi, Deeksha Sinha. 2019. Near optimal ab testing. Manage-

ment Science .

Blundell, Richard, Monica Costa Dias, Costas Meghir, John Van Reenen. 2004. Evaluating the employment

impact of a mandatory job search program. Journal of the European economic association 2(4) 569–606.

Bojinov, Iavor, David Simchi-Levi, Jinglong Zhao. 2020. Design and analysis of switchback experiments.

Available at SSRN 3684168 .

38

Brown, Celia A, Richard J Lilford. 2006. The stepped wedge trial design: a systematic review. BMC medical

research methodology 6(1) 54.

Bubeck, S´ebastien, Nicolo Cesa-Bianchi. 2012. Regret analysis of stochastic and nonstochastic multi-armed

bandit problems. arXiv preprint arXiv:1204.5721 .

Cachon, G´erard P, Santiago Gallino, Marcelo Olivares. 2019. Does adding inventory increase sales? evidence

of a scarcity eﬀect in us automobile dealerships. Management Science 65(4) 1469–1485.

Caﬂisch, Andrea, Michael D Grubb, Darragh Kelly, Jeroen Nieboer, Matthew Osborne. 2018. Sending out

an sms: The impact of automatically enrolling consumers into overdraft alerts. Available at SSRN

3538527 .

Callaway, Brantly, Pedro HC Sant’Anna. 2020. Diﬀerence-in-diﬀerences with multiple time periods. Journal

of Econometrics .

Candes, Emmanuel, Benjamin Recht. 2009. Exact matrix completion via convex optimization. Communi-

cations of the ACM 55(6) 111–119.

Candes, Emmanuel J, Yaniv Plan. 2010. Matrix completion with noise. Proceedings of the IEEE 98(6)

925–936.

Card, David, Alan B Krueger. 1994. Minimum wages and employment: A case study of the fast-food industry

in new jersey and pennsylvania. American Economic Review 84(4) 772–93.

Cheng, Russell CH, Teresa Davenport. 1989. The problem of dimensionality in stratiﬁed sampling. Manage-

ment Science 35(11) 1278–1296.

Chow, Yuan S, Herbert Robbins. 1965. On the asymptotic theory of ﬁxed-width sequential conﬁdence

intervals for the mean. The Annals of Mathematical Statistics 36(2) 457–462.

Cook, Thomas D, Donald Thomas Campbell, William Shadish. 2002. Experimental and quasi-experimental

designs for generalized causal inference. Houghton Miﬄin Boston, MA.

Cui, Ruomeng, Zhikun Lu, Tianshu Sun, Joseph Golden. 2020. Sooner or later? promising delivery speed in

online retail. Promising Delivery Speed in Online Retail (March 29, 2020) .

Cui, Ruomeng, Dennis J Zhang, Achal Bassamboo. 2019. Learning from inventory availability information:

Evidence from ﬁeld experiments on amazon. Management Science 65(3) 1216–1235.

De Chaisemartin, Cl´ement, Xavier d’Haultfoeuille. 2020. Two-way ﬁxed eﬀects estimators with heterogeneous

treatment eﬀects. American Economic Review 110(9) 2964–96.

Dette, Holger, Viatcheslav B Melas, Roman Guchenko. 2015. Bayesian t-optimal discriminating designs.

Annals of statistics 43(5) 1959.

Dette, Holger, Viatcheslav B Melas, Petr Shpilev, et al. 2012. T-optimal designs for discrimination between

two polynomial models. The Annals of Statistics 40(1) 188–205.

39

Dette, Holger, Viatcheslav B Melas, Petr Shpilev, et al. 2013. Robust t-optimal discriminating designs.

Annals of Statistics 41(4) 1693–1715.

Doudchenko, Nick, David Gilinson, Sean Taylor, Nils Wernerfelt. 2021. Designing experiments with synthetic

controls .

Efron, Bradley. 1971. Forcing a sequential experiment to be balanced. Biometrika 58(3) 403–417.

Fox, Bennet L. 2000. Separability in optimal allocation. Operations Research 48(1) 173–176.

Girling, Alan J, Karla Hemming. 2016. Statistical eﬃciency and optimal design for stepped cluster studies

under linear mixed eﬀects models. Statistics in medicine 35(13) 2149–2166.

Glynn, Peter W, Ward Whitt. 1992. The asymptotic eﬃciency of simulation estimators. Operations research

40(3) 505–520.

Goldenshluger, Alexander, Assaf Zeevi. 2013. A linear response bandit problem. Stochastic Systems 3(1)

230–261.

Goodman-Bacon, Andrew. 2021. Diﬀerence-in-diﬀerences with variation in treatment timing. Journal of

Econometrics .

Gupta, Somit, Ronny Kohavi, Diane Tang, Ya Xu, Reid Andersen, Eytan Bakshy, Niall Cardin, Sumita Chan-

dran, Nanyu Chen, Dominic Coey, Mike Curtis, Alex Deng, Weitao Duan, Peter Forbes, Brian Frasca,

Tommy Guy, Guido W. Imbens, Guillaume Saint Jacques, Pranav Kantawala, Ilya Katsev, Moshe

Katzwer, Mikael Konutgan, Elena Kunakova, Minyong Lee, MJ Lee, Joseph Liu, James McQueen, Amir

Najmi, Brent Smith, Vivek Trehan, Lukas Vermeer, Toby Walker, Jeﬀrey Wong, Igor Yashkov. 2019.

Top challenges from the ﬁrst practical online controlled experiments summit. SIGKDD Explor. Newsl.

21(1) 20–35. doi:10.1145/3331651.3331655. URL https://doi.org/10.1145/3331651.3331655.

Han, Sukjin. 2020. Identiﬁcation in nonparametric models for dynamic treatment eﬀects. Journal of Econo-

metrics .

Hastie, Trevor, Rahul Mazumder, Jason D Lee, Reza Zadeh. 2015. Matrix completion and low-rank svd via

fast alternating least squares. The Journal of Machine Learning Research 16(1) 3367–3402.

Hastings, Justine, Jesse M Shapiro. 2018. How are snap beneﬁts spent? evidence from a retail panel. American

Economic Review 108(12) 3493–3540.

Hayes, Brian. 2002. Computing science: The easiest hard problem. American Scientist 90(2) 113–117.

Hayes, Richard J, Lawrence H Moulton. 2017. Cluster randomised trials. Chapman and Hall/CRC.

Heckman, James J, Hidehiko Ichimura, Petra Todd. 1998. Matching as an econometric evaluation estimator.

The review of economic studies 65(2) 261–294.

Heckman, James J, Hidehiko Ichimura, Petra E Todd. 1997. Matching as an econometric evaluation esti-

mator: Evidence from evaluating a job training programme. The review of economic studies 64(4)

605–654.

40

Heckman, James J, Edward Vytlacil. 2005. Structural equations, treatment eﬀects, and econometric policy

evaluation 1. Econometrica 73(3) 669–738.

Hemming, Karla, Terry P Haines, Peter J Chilton, Alan J Girling, Richard J Lilford. 2015. The stepped

wedge cluster randomised trial: rationale, design, analysis, and reporting. Bmj 350 h391.

Holtz-Eakin, Douglas, Whitney Newey, Harvey S Rosen. 1988. Estimating vector autoregressions with panel

data. Econometrica: Journal of the econometric society 1371–1395.

Hsiao, Cheng. 2014. Analysis of panel data. 54, Cambridge university press.

Hu, Yanqing, Feifang Hu, et al. 2012. Asymptotic properties of covariate-adaptive randomization. The

Annals of Statistics 40(3) 1794–1815.

Hussey, Michael A, James P Hughes. 2007. Design and analysis of stepped wedge cluster randomized trials.

Contemporary Clinical Trials 28(2) 182–191.

Imai, Kosuke, Marc Ratkovic. 2014. Covariate balancing propensity score. Journal of the Royal Statistical

Society: Series B (Statistical Methodology) 76(1) 243–263.

Imbens, Guido W, Donald B Rubin. 2015. Causal inference in statistics, social, and biomedical sciences.

Cambridge University Press.

Johari, Ramesh, Pete Koomen, Leonid Pekelis, David Walsh. 2017. Peeking at a/b tests: Why it matters, and

what to do about it. Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge

Discovery and Data Mining. 1517–1525.

Johari, Ramesh, Hannah Li, Inessa Liskovich, Gabriel Weintraub. 2020. Experimental design in two-sided

platforms: An analysis of bias. arXiv preprint arXiv:2002.05670 .

Ju, Nianqiao, Diane Hu, Adam Henderson, Liangjie Hong. 2019. A sequential test for selecting the better

variant: Online a/b testing, adaptive allocation, and continuous monitoring. Proceedings of the Twelfth

ACM International Conference on Web Search and Data Mining. 492–500.

Kallus, Nathan. 2018. Optimal a priori balance in the design of controlled experiments. Journal of the Royal

Statistical Society: Series B (Statistical Methodology) 80(1) 85–112.

Krieger, Abba M, David Azriel, Adam Kapelner. 2019. Nearly random designs with greatly improved balance.

Biometrika 106(3) 695–701.

K¨unzel, S¨oren R, Jasjeet S Sekhon, Peter J Bickel, Bin Yu. 2019. Metalearners for estimating heterogeneous

treatment eﬀects using machine learning. Proceedings of the national academy of sciences 116(10)

4156–4165.

Lattimore, Tor, Csaba Szepesv´ari. 2018. Bandit algorithms. preprint 28.

Lawrie, Jock, John B Carlin, Andrew B Forbes. 2015. Optimal stepped wedge designs. Statistics & Probability

Letters 99 210–214.

41

Li, Fan, Elizabeth L Turner, John S Preisser. 2018. Optimal allocation of clusters in cohort stepped wedge

designs. Statistics & Probability Letters 137 257–263.

Mertens, Stephan. 2006. The easiest hard problem: Number partitioning. Computational Complexity and

Statistical Physics 125(2) 125–139.

Mulvey, John M. 1983. Multivariate stratiﬁed sampling by optimization. Management Science 29(6) 715–

724.

Murphy, Kevin P. 2007. Conjugate bayesian analysis of the gaussian distribution. def 1(2σ2) 16.

Nikolaev, Alexander G, Sheldon H Jacobson, Wendy K Tam Cho, Jason J Sauppe, Edward C Sewell. 2013.

Balance optimization subset selection (boss): An alternative approach for causal inference with obser-

vational data. Operations Research 61(2) 398–412.

Pocock, Stuart J, Richard Simon. 1975. Sequential treatment assignment with balancing for prognostic

factors in the controlled clinical trial. Biometrics 103–115.

Robins, James M. 1997. Causal inference from complex longitudinal data. Latent variable modeling and

applications to causality. Springer, 69–117.

Robinson, Peter M. 1988. Root-n-consistent semiparametric regression. Econometrica: Journal of the Econo-

metric Society 931–954.

Sauppe, Jason J, Sheldon H Jacobson. 2017. The role of covariate balance in observational studies. Naval

Research Logistics (NRL) 64(4) 323–344.

Simon, Richard. 1979. Restricted randomization designs in clinical trials. Biometrics 503–512.

Singham, Dashi I, Lee W Schruben. 2012. Finite-sample performance of absolute precision stopping rules.

INFORMS Journal on Computing 24(4) 624–635.

Sun, Liyang, Sarah Abraham. 2020. Estimating dynamic treatment eﬀects in event studies with heterogeneous

treatment eﬀects. Journal of Econometrics .

Uci´nski, Dariusz, Barbara Bogacka. 2005. T-optimum designs for discrimination between two multiresponse

dynamic models. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67(1)

3–18.

Wager, Stefan, Susan Athey. 2018. Estimation and inference of heterogeneous treatment eﬀects using random

forests. Journal of the American Statistical Association 113(523) 1228–1242.

Wallace, T Dudley, Ashiq Hussain. 1969. The use of error components models in combining cross section

with time series data. Econometrica: Journal of the Econometric Society 55–72.

Wiens, Douglas P. 2009. Robust discrimination designs. Journal of the Royal Statistical Society: Series B

(Statistical Methodology) 71(4) 805–829.

Woertman, Willem, Esther de Hoop, Mirjam Moerbeek, Sytse U Zuidema, Debby L Gerritsen, Steven Teeren-

stra. 2013. Stepped wedge designs could reduce the required sample size in cluster randomized trials.

Journal of Clinical Epidemiology 66(7) 752–758.

42

Wooldridge, Jeﬀrey M. 2010. Econometric analysis of cross section and panel data.

Zhang, Dennis J, Hengchen Dai, Lingxiu Dong, Fangfang Qi, Nannan Zhang, Xiaofei Liu, Zhongyi Liu, Jiang

Yang. 2020. The long-term and spillover eﬀects of price promotions on retailing platforms: Evidence

from a large randomized experiment on alibaba. Management Science 66(6) 2589–2609.

43

Appendix A: More Details for Fixed-Sample-Size Experiments

A.1. More Details for Generalized Least Squares

Lemma A.1 (Gauss-Markov Theorem). Consider a linear model y = Xβ + noise with E[noise

X] = 0
|
X] = Ω. If Ω = σ2I, then the ordinary least squares estimator ˆβ = (X(cid:62)X)−1X(cid:62)y is the best

and Cov[noise

|

linear unbiased estimator (BLUE). Otherwise, when Ω is not a multiple of the identity matrix, the generalized

least squares estimator ˆβ = (X(cid:62)Ω−1X)−1X (cid:62)Ω−1y is BLUE. Here, “best” means the estimator has the lowest

variance among all unbiased linear estimators.

A.2. Discussion on the Assumptions in Lemma 3.1

The assumptions in Lemma 3.1 allow us to decompose Prec( ˆτ ) into three separable summations of quadratic

functions. We have discussed in Section 3.2 that the assumptions in Lemma 3.1 are nonrestrictive. Here we

provide additional discussion. First, for the assumption

N

i=1 Xiu(cid:62)

i = 0dx,du (X is orthogonal to U) to hold,
RN ×(dx+du). Let the ﬁrst dx

= QR

we can always use the Gram-Schmidt procedure18 to decompose
and last du columns in Q be ˜X and ˜U. Then ˜X is orthogonal to ˜U. Second, Σv = σ2
ε ·

X U

(cid:80)

∈

(cid:2)

(cid:3)

Idu is an identiﬁcation

assumption (Bai and Ng 2008) so that we can uniquely identify ui and vt. In other words, for arbitrary ui

and vt, we can right multiply u(cid:62)

i by σε ·

Σ−1/2
v

N

i=1 ui = 0dx and

N

i=1 Xiu(cid:62)

and left multiply vt by σε ·
i = 0dx,du stay valid after this manipulation).

Σ−1/2
v

so that vt has variance

σ2
ε ·

Idu (conditions

A.3. Deﬁnition of M ((cid:96)) and b((cid:96)) in Eq. (3.8)

(cid:80)

(cid:80)

In Theorem 3.1, M ((cid:96)) and b((cid:96)) in Eq. (3.8) are deﬁned as follows

+ 1

(cid:96)/2
(cid:99)

(cid:96)/2
(cid:99)

(cid:98)

+ 2

. . .

(cid:98)
M ((cid:96)) = 





b((cid:96)) =

−

+ 1

(cid:98)


(cid:96)/2
(cid:99)
...
−
(cid:96)

(cid:96)

1



+

1

(cid:96)

−

T





Example A.1. When (cid:96) = 3,






1

−

T

(cid:96)

−





+ 1)2




(cid:96)



(cid:96)/2
(
(cid:98)

(cid:99)
...
−
(cid:96)2

1)2

((cid:96)








(cid:96)

(cid:96)


−

(cid:96)/2

(cid:99)
(cid:96)/2
(cid:99)

− (cid:98)
1
− (cid:98)
...
1

(cid:96)
(cid:96)

1
1

−
−

− (cid:98)
− (cid:98)
...
1
(cid:96)−(cid:98)(cid:96)/2(cid:99)
l=1

(cid:96)/2
(cid:96)/2

2
2

(cid:96)
(cid:96)

−
−

(cid:99)
(cid:99)

(cid:96)/2
(cid:96)/2

− (cid:98)
− (cid:98)
...
1

1

(cid:96)

−

−

T










(cid:80)






2

(cid:98)

(
(cid:98)

(cid:96)/2
(cid:99)
...
(cid:96)/2

(cid:99) −
(cid:96)/2
(cid:99)
(cid:98)

+ 1

l)

−

1








ω∗

1 =

1, ω∗

2 =

1 +

−

ω∗

t =

1 +

−

2t
T

−
4
3
12(T

−
−

6
44T + 79

, ω∗

3 =

1 +

−

6T 2

−

12(T

4)
44T + 79

−

6T 2

−

for t = 4,

4)
44T + 79

−

, T

3,

−

· · ·

, ω∗

T −1 = 1

6
44T + 79

, ω∗

T = 1

−

6T 2

−

−

6T 2

−

ω∗

T −2 = 1

(cid:3)

18 Equivalent to QR decomposition

1
1

...

1




(A.1)

(A.2)

(cid:99) · · ·
(cid:99) · · ·
. . .

· · ·

,

44

A.4. D-Optimal Treatment Design

Since Var( ˆτ ) = Prec( ˆτ )−1 and det(Var( ˆτ )) = det(Prec( ˆτ )−1), minimizing the objective function det(Var( ˆτ ))

is equivalent to maximizing det(Prec( ˆτ )). Besides the objective (3.2), we could consider an alternative objec-

tive that is based on the determinant of the inverse of Prec( ˆτ ) (recall Var( ˆτ ) = Prec( ˆτ )−1):

min
{Ai}N

i=1 −

det

Prec( ˆτ )

.

(A.3)

In order to calculate det

Prec( ˆτ )

, we need to write every entry in Prec( ˆτ ) as a function of treatment

(cid:0)

(cid:1)

variables zit in order to optimize the treatment decisions. To build intuition, we provide the expression for

(cid:0)

(cid:1)

every entry in Prec( ˆτ ) for the carryover model without observed or latent covariates Yit = αi + βt + τ0zit +

τ1zi,t−1 +

· · ·

+ τ(cid:96)zi,t−(cid:96) + εit. If the model has covariates, then the role of covariates in the D-optimal design

is conceptually the same as the role of covariates in the T-optimal design (see Theorem 3.1).

N
σ2
ε

−

T −(cid:96)−1+j
t=j

ω2

1
T −(cid:96)

T −(cid:96)−1+j
t=j

2

ωt

+ T −(cid:96)
2

t −
(cid:16)(cid:80)
ωtωt+m−j −
υ(j,m)
(cid:16)(cid:80)
T −t )ωt −
T +1−t −

1
T −(cid:96)

(cid:17)
T −(cid:96)−1+j
t=j

ωt
t=j (ωt −

m−1

t=1(υ(j,m)

T

t=1(υ(j,j)

T +1−t −
ωt

T −(cid:96)−1+m
t=m

υ(j,j)
T −t )ωt

j = m

(cid:21)

(A.4)

(cid:80)

(cid:17) (cid:16)(cid:80)
ωT −(cid:96)+t)

(cid:17)

(cid:105)

j < m

j > m

Prec( ˆτ )jm =

where ωt = 1
N



(cid:20)

T

−

(cid:80)

T −(cid:96)−1+j
t=j

N
σ2
ε
(cid:104)(cid:80)
+ T −(cid:96)
2
Prec( ˆτ )mj
(cid:80)



i=1 zit and υ(j,m)

N

t

(cid:80)

is deﬁned as

(cid:80)

1

υ(j,m)

t

=

1 + 2(t−1−(cid:96)+m)

−

T −(cid:96)

−
1 + 2(t−1−(cid:96)+m)
(cid:16)
−
1 + 2(t−1−(cid:96)+j)

T −(cid:96)

T −(cid:96)

(cid:17)
−

(cid:17) (cid:16)

(cid:17)






(cid:16)

(cid:16)
1

−

1 + 2(t−1−(cid:96)+j)

T −(cid:96)

(cid:96) + 1

(cid:96) + 1

t

≤
(cid:96) + 1

m

−
m < t

−

−

(cid:96) + 1

j

−

≤

T + 1

m

−
T + 1

j

−

j < t

≤
m < t

j < t

≤

(cid:17)

T + 1

T + 1

−

−

Note that each entry in Prec( ˆτ ) is a quadratic function of ωt. As det

Prec( ˆτ )

is a sum of multiples of (cid:96)

entries in Prec( ˆτ ), det

Prec( ˆτ )

is the 2(cid:96)-th degree polynomial function of zit. It is generally infeasible to

(cid:0)

(cid:1)

analytically solve (A.3) for (cid:96) > 1. In practice, we can use the oﬀ-the-shelf software to ﬁnd the optimal ωt.

(cid:0)

(cid:1)

We show the optimal treated fraction of the solution to optimization problem (A.3) for T = 10 in Figure

8. Similar as all the previous results, the treated proportion is symmetric with respect to the coordinate
( T +1

2 , 0.5). Similar as the T-optimal design in Theorem 3.1, if the treatment aﬀects more periods ((cid:96) is larger),
the optimal treated proportion is in general smaller at the beginning, increases at a faster rate in the middle,

and is larger in the end.

A.5. Reversible Treatment Adoption Case

We start by noting that there are two ways to look at the reversible treatment adoption case. When each

unit is treated at most once versus when units can be treated more than once. Surprisingly, the ﬁrst case is

a special case of the irreversible treatment adoption case that we introduced in Section 3 as we will show

in Section A.5.1 below. For the second case of general reversible patterns we show that our results for the

irreversible pattern can be used to derive analogous results.

45

Figure 8

D-optimal treatment design: Optimal treated proportion at each period for a T -period treatment design

in the presence of carryover treatment eﬀects, where T = 10. Diﬀerent colors represent the number of lags (cid:96) in the

carryover eﬀects.

A.5.1. Equivalent Representation of One-time Treatment Model (2.1) can represent the one-time

treatment, such as the participation of a job-training or skill-improvement program, but the eﬀect of the

treatment carries over to future periods. To see this, let wit =

time t and the outcome model follows

0, 1
}
{

indicate whether unit i is treated at

Yit = ˜αi + ˜βt + X(cid:62)

i θt + u(cid:62)

i vt + ˜τ0wit + ˜τ1wi,t−1 +

+ ˜τ(cid:96)wi,t−(cid:96) + εit.

· · ·

(A.5)

˜τj can be interpreted as the treatment eﬀect given that a unit was treated j-period ago. In the case of

one-time treatment, at most one of wit,

treatment wears out over time.

, wi,t−(cid:96) is 1. If

<

˜τ(cid:96)|
|

<

˜τ1
|

|

<

˜τ0
|

, then the eﬀect of the one-time
|

· · ·

· · ·

If zit =

1, +1
}

{−

Model (2.1) with the change of variables:

indicates whether unit i has been treated up to time t, then Model (A.5) is equivalent to

A.5.2. General Reversible Treatment Pattern

j

˜τj = 2

τk

k=0
(cid:88)

˜αi = αi −

(cid:96)

τk

k=0
(cid:88)

˜βt = βt.

Proposition A.1. Suppose Assumption 2.2 holds, and the treatment is reversible. Let ωt = 1

N

and ζi = 1
T

T

i=1 zit.

N
i=1 zit

(cid:80)

1. Suppose the potential outcome can be modeled as Yit(zit) = βt + τ zit + εit. Any treatment design is

(cid:80)

optimal if it satisﬁes

ωt = 0.

2. Suppose the potential outcome can be modeled as Yit(zit) = αi + τ zit + εit. Any treatment design is

optimal if it satisﬁes

ζi = 0.

3. Suppose the potential outcome can be modeled as Yit(zit) = αi + βt + τ zit + εit. Any treatment design is

optimal if it satisﬁes

ωt = 0,

ζi = 0.

246810t0.00.20.40.60.81.0%treated‘=0‘=1‘=2‘=3‘=4‘=546

4. Suppose the potential outcome can be modeled as Yit(zit,

i vt + τ0zit +
0. Suppose the assumptions in Theorem 3.1 hold and let ωg,t =

, zi,t−(cid:96)) = αi + βt + X(cid:62)

i θt + u(cid:62)

· · ·

τ1zi,t−1 +

+ τ(cid:96)zi,t−(cid:96) + εit

for (cid:96)

· · ·

1
|Og |

i∈Og zit, where

Og =

{

≥
i : Xi = xg, ui = u0,g}

. Any treatment design is optimal if it satisﬁes

(cid:80)

ωg,t = 0,

for all t and g,

ζi = 0.

Proof of Proposition A.1

1. When the potential outcome is Yit(zit) = βt + τ zit + εit,

It is maximized at ωt = 0.

Prec(ˆτ ) = N T

N

−

ω2
t .

T

t=1
(cid:88)

2. When the potential outcome is Yit(zit) = αi + τ zit + εit,

It is maximized at ζi = 0.

Prec(ˆτ ) = N T

T

−

ζ 2
i .

N

i=1
(cid:88)

3. When the potential outcome is Yit(zit) = αi + βt + τ zit + εit, from the proof of Lemma 3.1, we have

Prec(ˆτ ) = N T

T

− 

N

ζ 2
i −

N
T (cid:32)

T

2

T

ωt

+ N

(cid:33)

ω2

t 

.

It is maximized at ζi = 0 and ωt = 0, given that

4. When the potential outcome is Yit(zit,



i=1
(cid:88)

T

t=1
(cid:88)
1
T
i θt + u(cid:62)
, zi,t−(cid:96)) = αi + βt + X(cid:62)
(cid:1)
(cid:80)

T
t=1 ωt

t=1 ω2

t −

(cid:0)(cid:80)

2

t=1
(cid:88)
is minimized at ωt = 0.



i vt + τ0zit + τ1zi,t−1 +

· · ·

+

· · ·

τ(cid:96)zi,t−(cid:96) + εit, the role of observed and latent covariates in the optimal design is the same as Theorem 3.1.

However, for the treated fraction, we can no longer write

treatment be reversed, the value of
N

part with ζi and ωt. Note that

i=1 ζ 2
(cid:80)
i
is positive semi-deﬁnite, this part is minimized at ωt = 0 (and ωg,t = 0).

(cid:80)

N

i=1 ζ 2

N

i as a function of ωt, because when the
is not unique given ωt. Then we can separately optimize the

i=1 ζ 2
is minimized at ζi = 0. Since the Hessian of the part with ωt (ωg,t)

(cid:80)

i

(cid:3)

A.6. A Rounding Approach for A Feasible Solution

When there does not exist a feasible solution in Adisc

opt , we suggest using the nearest integer rounding rule as

follows to obtain a feasible

Ai}
{

N
i=1.

Nearest integer rounding rule: We round

|Og |(1+ω∗
2T

(cid:96),t)

be a treatment design that is deﬁned by the rounded

to the nearest integer for all stratum g. Let
|Og |(1+ω∗
2T

(cid:96),t)

:

Arnd
N
i=1
i }
{

1Arnd

i ≤t =

1
|Og| (cid:88)
where frac

i∈Og

|Og |(1+ω∗
2T

(cid:96),t)

|Og |(1+ω∗
2T

|Og |(1+ω∗
2T
|Og |(1+ω∗
2T

(cid:96),t)

(cid:99)

(cid:96),t)

(cid:101)
(cid:96),t)

if frac

if frac

(cid:0)
|Og |(1+ω∗
(cid:0)
2T
Prec( ˆτ )

(cid:96),t)

− (cid:98)

|Og |(1+ω∗
2T
|Og |(1+ω∗
2T

(cid:96),t)

(cid:96),t)

< 1
> 1

2 , or frac
2 , or frac

(cid:0)

(cid:1)

|Og |(1+ω∗
2T
|Og |(1+ω∗
2T

(cid:96),t)

(cid:96),t)

= 1
= 1

2 with
2 with

(cid:1)

1+ω∗

1+ω∗

(cid:96),t

2T < 1
2
1
2 ,

2T ≥

(cid:96),t

(cid:1)

. Let zrnd be deﬁned by
(cid:99)

(cid:0)

(cid:1)
Arnd
N
i }

{

z be the objective function (3.2) (i.e. tr

i=1 and let zint∗ be the
) evaluated

Prec( ˆτ )

(cid:98)

(cid:100)

(cid:40)

=

(cid:1)

optimal integer solution to (3.2). Let tr

(cid:0)

at z. The following proposition bounds the diﬀerence between tr

(cid:0)

(cid:1)

Prec( ˆτ )

zrnd and tr

Prec( ˆτ )
(cid:0)

zint∗ .
(cid:1)

Proposition A.2. Suppose the assumptions in Theorem 3.1 hold, du = 0 and N is even, and xk,max =

(cid:0)

(cid:1)

(cid:0)

(cid:1)

maxg |

xgk|

is ﬁnite, where xgk is the k-th coordinate of xg for all strata g. We have

tr

Prec( ˆτ )

zrnd = tr

Prec( ˆτ )

(cid:0)

(cid:1)

(cid:0)

(cid:1)

1 +

1 +

zint∗ · (cid:32)

O (cid:32)

k,max

dx
k=1 x2
N 2

min

(cid:80)

.

(cid:33)(cid:33)

47

Since the probability of each realization of Xi is bounded away from 0 and xk,max is ﬁnite, we have

1+(cid:80)dx
k=1 x2
N 2

O
many values),

(cid:18)

min

k,max

=

O

1
N 2

as N

(cid:0)

(cid:1)

(cid:19)

. However, if G is large compared with N (or Xi takes inﬁnitely

→ ∞

tr

Prec( ˆτ )

zrnd

could be much larger than tr

zint∗ . In this case, we could instead partition units into only a few
(e.g., 2 or 3) groups based on their covariates’ values using K-means or some other methods. Within each

Prec( ˆτ )

(cid:1)

(cid:0)

(cid:1)

group, the treated fraction

is satisﬁed.

(cid:0)
1+ω∗
2

(cid:96),t

Appendix B: More Details for Sequential Experiments

B.1. Bayesian Update for σ2
ε

sta,t are used to construct a belief about σ2

sta,t and ˆξ2
ˆσ2
approach to construct such a belief is supported by the asymptotic distribution of ˆσ2
t−1 σ4
ε := ξ2

ε are unknown. We then follow Murphy (2007) and assume a
ε + 1

ε that is approximated by a normal distribution. Our

ε ) with parameters µ0, κ0, α0 and β0, i.e.,

perspective. Suppose both σ2

normal-gamma prior for σ2

sta,t with a Bayesian

ε + 1
(ξ†2

ε and ξ†2

ε and ξ2

ε

t−1 σ4

NG(σ2

ε , ξ†2
ε |

µ0, κ0, α0, β0) :=

(σ2
ε |

N

µ0, ξ†2

ε /κ0)Gamma(ξ†−2

ε

α0, β0).

|

sta,t and ˆξ2

sta,t to update the normal-gamma prior. Note that ˆσ2

We seek to use ˆσ2
as the sample mean and variance of i.i.d. random variables ϕis with ϕis ∼ N
can show that ˆσ2

ε ). More precisely, one
t = 1
t )2,
¯σ2
respectively. Murphy (2007) shows that the prior can be updated based on i.i.d. observations ϕis and in fact

sta,t are asymptotically the same as ¯σ2

sta,t and ˆξ2
ε , ξ†2
(σ2

sta,t can be viewed

i,s ϕis and ¯ξ2

sta,t and ˆξ2

i,s(ϕis −

t = 1

N t−1

N t

(cid:80)

the updating rule only uses ¯σ2

then plug ˆσ2

sta,t and ˆξ2

t and ¯ξ2

sta,t are asymptotically the same as ¯σ2
sta,t into the updating rule and obtain the posterior distribution of σ2

t . As ˆσ2

sta,t and ˆξ2

(cid:80)
t and ¯ξ2
ε , i.e.,

t , we can

p(σ2
ε |

sta,t, ˆξ†2
ˆσ2

sta) = T2αt(µ

µt, βt/(αtκt)),
|

where T2αt(
·

) is a T -distribution with 2αt degrees of freedom,

µt =

κtµ0 + N tˆσ2
κt + N t

sta,t

, κt = κ0 + N t, αt = α0 +

N t
2

, and βt = β0 +

N t
2

ˆξ†2
sta +

κ0N t(ˆσ2

sta,t −
2(κ0 + N t)

µ0)2

.

With a large 2αt, the above t-distribution can be approximated by

(µt, βt/(αtκt)). We assume an uninfor-

N
0. Suppose µ0, α0 and β0 are some small constants. Then one can show that as

mative prior of σ2

ε with κ0

→

N

→ ∞

, the posterior mean and variance are

µt →

ˆσ2
sta,t and

βt
αtκt →

ˆξ†2
sta
N t

,

and the posterior distribution of σ2

ε can be approximated by

that is identical to our approximation of the belief about σ2
ε .

p(σ2
ε |

sta,t, ˆξ†2
ˆσ2
sta)

≈ N

(ˆσ2

sta,t, ˆξ†2

sta/(N t))

48

B.2. More Discussion on Theorem 4.1

Intuition of the proof without the weak exogeneity assumption. The key idea is the violation of the

exogeneous assumption is suﬃciently “weak”, so that ˆτall can still be consistent and asymptotically normal.

The intuition for the “weak” violation is that the weight of zit in ωt and the weight of εju in ˆσ2

sta,s are both

small for adaptive unit i, static unit j, and u

s

t. Even though ωt and ˆσ2

sta,s is correlated, the correlation

≤
between zit and εju is weak, and we can then show correlation between ˙zit and ˙εit is weak.

≤

Intuition of mutual asymptotic independence. The diagonal asymptotic covariance matrix of ˆτall,
ada,1 and ˆσ2
ˆσ2

ada,2 comes from careful sample splitting and careful analyses of diﬀerent sources of randomness

in these estimators. The leading terms in ˆτall
zit}it∈[N]×[T ]. The leading terms in ˆσ2
{

−
ada,1 −

on

ε are ε2
σ2

it −

ˆτ are ζitεit for all i and t and for some ζit that depends

σ2 and εitεis for all i, t and s

= t. If we can

show the average of ζitεit is asymptotically independent of the average of ε2

σ2 and the average of ζitεit,

then the asymptotic independence between ˆτall and ˆσ2

independence between the average of ζitεit and the average of ε2
of (i, t) and (j, s) that E[ζitεit(ε2

it −

js −

it −

ada,1 follows. The critical step to show the asymptotic
σ2 is that there is only a negligible fraction

σ2)] = O(1) and for the remaining pairs, the expected value is either zero

or suﬃciently close to zero. The asymptotic independence between the average of ζitεit and the average of
εitεis follows from E[ζitεitεjsεju] = 0 for all pairs of (i, t) and (j, s, u) as s
we can use show the asymptotic independence between ˆτall and ˆσ2
ada,1 and ˆσ2
ˆσ2
i.i.d., the randomness of ˆσ2

ada,2 holds because we use diﬀerent sets of ATU (

ada,2. The asymptotic independence between

ada,1 is independent of the randomness of ˆσ2

= u. Following the same argument,

ada,2) to estimate them. As εit is

ada,1 that depends on εit for i

ada,1 and

ada,2

S

S

∈ S

that depends on εjs for j

ada,2.

∈ S

Appendix C: A Machine Learning Estimator for τ

One could directly estimate L with τ using the following objective function,

ˆτ , ˆα, ˆβ((cid:96)+1):T , ˆL

= arg min

τ ,α,β((cid:96)+1):T ,L

1
N (T

−
(cid:107)∗ ,
L
(cid:107)

(cid:96))

Y:,((cid:96)+1):T −
(cid:13)
(cid:13)

α1(cid:62)

T −(cid:96) −

1N β(cid:62)

((cid:96)+1):T −

L

τ0Z:,((cid:96)+1):T −

−

τ1Z:,(cid:96):(T −1)

− · · · −

τ(cid:96)Z:,1:(T −(cid:96))

2

F

+ µ

where we refer to this objective as low-rank matrix estimation with ﬁxed eﬀects (LRME). Here,

(cid:13)
(cid:13)
(C.6)
L
(cid:107)∗ is the
(cid:107)
(cid:107)·(cid:107)F refers
nuclear norm (or trace norm) of matrix L, which is equal to the sum of its singular values. Also,
to the Frobenius norm of a matrix. If the regularization parameter µ is larger, the rank of ˆL tends to be

smaller. It is known in the matrix estimation literature that such a bias in estimating L can lead to a lower
variance (Candes and Recht 2009, Candes and Plan 2010). If the root mean squared error (RMSE) of ˆL is

lower, then the RMSE of ˆτ0, ˆτ1,

, ˆτ(cid:96) tend to be smaller.

· · ·

The objective function (C.6) is convex in τ, α, β and L, which has N (T

(cid:96)) + N + T + 1 variables in

−

total. Finding the global optimal solution of convex program (C.6) can be slow with oﬀ-the-shelf software

for convex optimization problems such as cvxpy. Alternatively, we propose to use the iterative singular

value thresholding and ordinary least squares (iterative SVT and OLS) algorithm to eﬃciently solve convex

(cid:54)
(cid:54)
49

program (C.6). The details of this algorithm are described in Algorithm 2. We can justify SVT using Theorem

1 in Hastie et al. (2015) that shows the optimal solution of

ˆL = arg min
rank((cid:96))≤k0

1
2 (cid:107)

Y

2
F + µ
L
(cid:107)

(cid:107)∗ ,
L
(cid:107)

−

is ˆL = Uk0Sµ(Dk0 )V (cid:62)
with its diagonal entries to be (σ1

k0 , where the rank-k0 SVD of Y is Uk0 Dk0V (cid:62)

k0 and Sµ(Dk0 ) is a diagonal k0 by k0 matrix
µ)+. When we have historical control data, we can use

µ)+,

, (σk0 −

· · ·

−

cross-validation to ﬁnd the optimal µ by the grid search algorithm.

Algorithm 2: Iterative SVT and OLS
Inputs : Y, Z, k0, µN T , ∆τ , and tmax

arg minτ,α,β

1
2

Y:,((cid:96)+1):T

α1(cid:62)

T −(cid:96) −

1N β(cid:62)

((cid:96)+1):T −

τ0Z:,((cid:96)+1):T

−

τ1Z:,(cid:96):(T −1)

− · · · −

τ(cid:96)Z:,1:(T −(cid:96))

−

0 ;

ˆτ (−1)
At t = 0, ˆτ (0), ˆα(0), ˆβ(0)

←

((cid:96)+1):T ←

(cid:13)
(cid:13)
(cid:13)
Y:,((cid:96)+1):T

ˆY (0)
e ←
while maxj

T −(cid:96) −

ˆα(0)1(cid:62)
ˆτ (t−1)
j

|

−
ˆτ (t)
j −
|
The rank-k0 SVD of ˆY (t)
SµN T (D(t)
k0 )
ˆL(t+1) = U (t)
ˆτ (t+1), ˆα(t+1), ˆβ(t+1)

e
diag((d(t)
←
k0 SµN T (D(t)
((cid:96)+1):T =
1
2

arg minτ ,α,β((cid:96)+1):T

ˆY (t+1)
e

= Y:,((cid:96)+1):T

(cid:13)
(cid:13)
(cid:13)
ˆα(t+1)1(cid:62)

−

1N ( ˆβ(0)

((cid:96)+1):T )(cid:62)

ˆτ (0)
0 Z:,((cid:96)+1):T

−

ˆτ (0)
1 Z:,(cid:96):(T −1)

−

− · · · −

ˆτ (0)
(cid:96) Z:,1:(T −(cid:96)) ;

> ∆τ and t < tmax do
k0 D(t)

is U (t)

k0 )(cid:62), where D(t)
k0 (V (t)
, (d(t)

µN T )+) ;

· · ·

k0 −

1 −
k0 )(V (t)

µN T )+,
k0 )(cid:62) ;

k0 = diag(d(t)
1 ,

, d(t)

k0 ) ;

· · ·

Y:,((cid:96)+1):T

α1(cid:62)

T −(cid:96) −

−

1N β(cid:62)

((cid:96)+1):T −

τ0Z:,((cid:96)+1):T

− · · · −

τ(cid:96)Z:,1:(T −(cid:96))

ˆL(t+1)

−

2

F

;

(cid:13)
(cid:13)
(cid:13)

2

F

;

(cid:13)
(cid:13)
(cid:13)

1( ˆβ(t+1)

((cid:96)+1):T )(cid:62)

−

−

ˆτ (t+1)
0

Z:,((cid:96)+1):T

− · · · −

ˆτ (t+1)
(cid:96)

Z:,1:(T −(cid:96)) ;

←

t + 1 ;

t
end
Outputs: ˆτ (t−1), ˆα(t−1), ˆβ(t−1)

((cid:96)+1):T , ˆL(t−1)

Appendix D: Additional Empirical Results

Magnitude of treatment eﬀects. The total estimation error does not vary with the magnitude of treatment

eﬀects, as shown in Figure 11. However, TP, FP, TN and FN do vary with the magnitude of treatment eﬀects,

as shown in Figures 13-15. Given a certain recall (fraction of relevant instances that were retrieved), precision

(fraction of relevant instances among the retrieved instances) increases with the magnitude of treatment

eﬀects, as there is a clearer distinction between the positive and negative classes. Additionally, whether

Zopt,stratiﬁed or Zﬀba performs better in the large recall region depends on the magnitude of treatment eﬀects.

Zopt,stratiﬁed performs better when the treatment eﬀect is large. This is because diﬀerent treatment designs

have diﬀerent distributions of estimation error. For example, the kurtosis of the distribution of ˆτ2 using Zﬀba

50

(a) Home medical visit data

(b) Grocery data

Figure 9 Varying N (additional data sets). These ﬁgures compare the total estimation error (cid:80)

j(ˆτj

τj)2 from

−

(c) Loan data

GLS using benchmark treatment designs (Zﬀ , Zba and Zﬀba), our linear staggered design (Zopt,linear), our nonlinear

staggered design (Zopt) and our nonlinear staggered design with stratiﬁcation (Zopt,stratiﬁed). These ﬁgures show the
mean and 95% conﬁdence band of (cid:80)
τj)2 that are calculated from 2,000 randomly sampled blocks of dimension

j(ˆτj

−

T with (cid:96) = 2 and varying N . For the medical data, T is 10, and the total eﬀect is

N

×

10% of the average monthly

−

visit rate. For the grocery data, T is 20, and the total eﬀect is 10% of the average weekly expenditure. For the

loan data, T is 20, and the total eﬀect is 10% of the average monthly number of loans issued, but we note that the

estimation error does not vary with the value of τj for all j.

is 7.51 (heavier tail), while the kurtosis using Zopt,stratiﬁed is 2.05 (lighter tail). Therefore, in the large recall

region (larger value to reject the null), FP is smaller for Zﬀba because of its heavier tail.

304050N0.000.050.100.150.20Pj(ˆτj−τj)2designZﬀZbaZﬀbaZopt304050N0.0020.0040.0060.008Pj(ˆτj−τj)2designZopt,linearZoptZopt,stratiﬁed304050N0100020003000400050006000Pj(ˆτj−τj)2designZﬀZbaZﬀbaZopt304050N3040506070Pj(ˆτj−τj)2designZopt,linearZoptZopt,stratiﬁed3040506070N05101520Pj(ˆτj−τj)2designZﬀZbaZﬀbaZopt3040506070N0.10.20.30.4Pj(ˆτj−τj)2designZopt,linearZoptZopt,stratiﬁed51

(a) Flu data

(b) Home medical visit data

(c) Grocery data

Figure 10 Varying T . These ﬁgures compare the total estimation error (cid:80)

(d) Loan data

j(ˆτj

−

τj)2 from GLS using benchmark

treatment designs (Zﬀ , Zba and Zﬀba), our linear staggered design (Zopt,linear), our nonlinear staggered design (Zopt)

and our nonlinear staggered design with stratiﬁcation (Zopt,stratiﬁed). These ﬁgures show the mean and 95% conﬁdence
band of (cid:80)

τj)2 that are calculated from 1,000 randomly sampled blocks of dimension N

T with (cid:96) = 2, varying

j(ˆτj

−

×

T and N = 50. For the ﬂu data, the total eﬀect is

−

10% of the average monthly ﬂu occurrence rate. For the medical

data, the total eﬀect is

−

10% of the average monthly visit rate. For the grocery data, the total eﬀect is 10% of the

average weekly expenditure. For the loan data, the total eﬀect is 10% of the average monthly number of loans issued,

but we note that the estimation error does not vary with the value of τj for all j.

10152025T0.00.51.01.52.02.53.0Pj(ˆτj−τj)2ZﬀZbaZﬀbaZopt10152025T0.020.040.060.080.100.12Pj(ˆτj−τj)2Zopt,linearZoptZopt,stratiﬁed68101214T0.00.10.20.30.40.5Pj(ˆτj−τj)2ZﬀZbaZﬀbaZopt68101214T0.0020.0040.0060.008Pj(ˆτj−τj)2Zopt,linearZoptZopt,stratiﬁed7.510.012.515.0T0.00.10.20.3Pj(ˆτj−τj)2ZﬀZbaZﬀbaZopt7.510.012.515.0T0.0050.0100.0150.020Pj(ˆτj−τj)2Zopt,linearZoptZopt,stratiﬁed810121416T01020304050Pj(ˆτj−τj)2ZﬀZbaZﬀbaZopt810121416T0.20.40.60.81.01.2Pj(ˆτj−τj)2Zopt,linearZoptZopt,stratiﬁed52

Figure 11 Varying treatment eﬀect. These ﬁgures compare the total estimation error (cid:80)

j(ˆτj

τj)2 from GLS

−

using benchmark treatment designs (Zﬀ , Zba and Zﬀba), our linear staggered design (Zopt,linear), our nonlinear stag-

gered design (Zopt) and our nonlinear staggered design with stratiﬁcation (Zopt,stratiﬁed). These ﬁgures show the mean
and 95% conﬁdence band of (cid:80)

τj)2 that are calculated from 1,000 randomly sampled blocks of dimension N

T

j(ˆτj

−

×

with (cid:96) = 2, T = 7 and N = 50 on the ﬂu data. The total eﬀect varies from

20% to 20% of the average monthly ﬂu

−

occurrence rate. The total estimation error stays constant with varying total eﬀect.

Figure 12

eﬀect (ˆτ0

−

Instantaneous eﬀect only. These ﬁgures compare the estimation error of the estimated instantaneous
τ0)2 from GLS using benchmark treatment designs (Zﬀ , Zba and Zﬀba), our linear staggered design

(Zopt,linear), and our nonlinear staggered design (Zopt,nonlinear). Zopt,nonlinear is the same as Zopt in Figure 3, which
τ0)2 that are calculated from

is optimal when (cid:96) = 2. This ﬁgure shows the mean and 95% conﬁdence band of (ˆτ0

−

1,000 randomly sampled blocks of dimension N

×

T with (cid:96) = 0 and T = 14 on the ﬂu data. The instantaneous eﬀect

equals

−

10% of the average monthly ﬂu occurrence rate. Since the treatment only has instantaneous eﬀect, Zopt,linear

is optimal and Zopt,nonlinear is sub-optimal. As a contrast, in Figure 3, Zopt,nonlinear is optimal, but Zopt,linear is

sub-optimal. What is the same in this ﬁgure and Figure 3 is that benchmark designs are sub-optimal for various N .

Figure 13

Precision vs. recall. The precision and recall are calculated from 1,000 pairs of synthetic experiments

with dimension 50

×

7 and (cid:96) = 2. The true positive class of τj is deﬁned as

τj

{|

=

|

−

0.1
N T

(cid:80)

i,t Yit(

1(cid:96)+1)

.

}

−

−0.2−0.10.00.10.2%totaleﬀect0.00.51.01.52.02.53.0Pj(ˆτj−τj)2×10−6ZﬀZbaZﬀbaZopt−0.2−0.10.00.10.2%totaleﬀect0.70.80.91.01.11.21.3Pj(ˆτj−τj)2×10−7Zopt,linearZoptZopt,stratiﬁed304050N0.000.050.100.150.20Pj(ˆτj−τj)2ZﬀZbaZﬀbaZopt,linear304050N0.0040.0060.0080.0100.0120.014Pj(ˆτj−τj)2ZﬀbaZopt,linearZopt,nonlinearZopt,stratiﬁed0.000.250.500.751.00recall0.20.40.60.81.0precisiontestofτ00.000.250.500.751.00recall0.20.40.60.81.0precisiontestofτ10.000.250.500.751.00recall0.20.40.60.81.0precisiontestofτ2ZﬀZbaZﬀbaZopt,linearZoptZopt,stratiﬁed53

Figure 14

Precision vs. recall (small treatment eﬀect). The precision and recall are calculated from 1,000

pairs of synthetic experiments with dimension 50

(cid:80)

0.05
N T

i,t Yit(

−

1(cid:96)+1)

−

.
}

7 and (cid:96) = 2. The true positive class of τj is deﬁned as

τj

{|

|

=

×

Figure 15

Precision vs. recall (large treatment eﬀect). The precision and recall are calculated from 1,000

pairs of synthetic experiments with dimension 50

(cid:80)

0.2
N T

i,t Yit(

−

1(cid:96)+1)

−

.
}

7 and (cid:96) = 2. The true positive class of τj is deﬁned as

τj

{|

|

=

×

0.250.500.751.00recall0.20.40.60.81.0precisiontestofτ00.000.250.500.751.00recall0.20.40.60.81.0precisiontestofτ10.000.250.500.751.00recall0.20.40.60.81.0precisiontestofτ2ZﬀZbaZﬀbaZopt,linearZoptZopt,stratiﬁed0.000.250.500.751.00recall0.20.40.60.81.0precisiontestofτ00.000.250.500.751.00recall0.20.40.60.81.0precisiontestofτ10.000.250.500.751.00recall0.20.40.60.81.0precisiontestofτ2ZﬀZbaZﬀbaZopt,linearZoptZopt,stratiﬁed54

(a) Flu data

(b) Home medical visit data

(c) Grocery data

(d) Loan data
Figure 16 Varying N (estimation error of total treatment eﬀect). These ﬁgures compare the total estima-
tion error ((cid:80)
j τj)2 from GLS using benchmark treatment designs (Zﬀ , Zba and Zﬀba), our linear staggered
design (Zopt,linear), our nonlinear staggered design (Zopt) and our nonlinear staggered design with stratiﬁcation
(Zopt,stratiﬁed). These ﬁgures show the mean and 95% conﬁdence band of ((cid:80)
j τj)2 that are calculated from
−
T with (cid:96) = 2 and varying N . For the medical data, T is 10, and the

2,000 randomly sampled blocks of dimension N

j ˆτj

j ˆτj

(cid:80)

(cid:80)

−

×

total eﬀect is

−

10% of the average monthly visit rate. For the grocery data, T is 20, and the total eﬀect is 10% of the

average weekly expenditure. For the loan data, T is 20, and the total eﬀect is 10% of the average monthly number of

loans issued, but we note that the estimation error does not vary with the value of τj for all j.

304050N0123(Pjˆτj−Pjτj)2ZﬀZbaZﬀbaZopt304050N0.0750.1000.1250.1500.1750.2000.225(Pjˆτj−Pjτj)2Zopt,linearZoptZopt,stratiﬁed304050N0.00.10.20.3(Pjˆτj−Pjτj)2ZﬀZbaZﬀbaZopt304050N0.00000.00010.00020.00030.00040.0005(Pjˆτj−Pjτj)2Zopt,linearZoptZopt,stratiﬁed304050N0100200300(Pjˆτj−Pjτj)2ZﬀZbaZﬀbaZopt304050N681012(Pjˆτj−Pjτj)2Zopt,linearZoptZopt,stratiﬁed3040506070N01234(Pjˆτj−Pjτj)2ZﬀZbaZﬀbaZopt3040506070N0.0250.0500.0750.1000.1250.150(Pjˆτj−Pjτj)2Zopt,linearZoptZopt,stratiﬁed55

Appendix E: Proof of Results for Fixed-Sample-Size Experiments

We can combine βt with θt in the potential outcome model (2.1), that is,

Yit = αi +

1 X(cid:62)
i

(cid:2)

˜X(cid:62)
i

(cid:3)

βt
θt

(cid:20)

(cid:21)

+ τ0zit + τ1zi,t−1 +

· · ·

+ τ(cid:96)zi,t−(cid:96) + u(cid:62)

i vt + εit

.

eit

Denote p := dx + 1, and then ˜Xi ∈

(cid:124) (cid:123)(cid:122) (cid:125)

Rp. Denote ζi = 1

T

T

t=1 zit for all i and ˜ωt = 1

(cid:123)(cid:122)
N

(cid:124)

(cid:125)
N
i=1

˜Xizit ∈

Rp for all t.

We write the potential outcomes from time (cid:96) + 1 to T into a vectorized form and then we have

(cid:80)

(cid:80)

y((cid:96)+1):T =

z1:(T −(cid:96))

z(cid:96):(T −1) z((cid:96)+1):T Γ

· · ·

τ
α1:(N −p)
β((cid:96)+1):T
θ((cid:96)+1):T






+ e((cid:96)+1):T ,






(cid:3)

˜X

R(N(T −(cid:96)))×(N+(T −(cid:96)−1)p),

∈

1N X

. . .

1N X

˜IN −p ˜X
˜IN −p
= 
...

˜IN −p











. . .

˜X








where ˆτ =

τ(cid:96) ,

, τ0

,

· · ·

(cid:2)

(cid:0)

(cid:1)
˜IN −p 1N X
˜IN −p
Γ = 
...

˜IN −p



IN −p 0N −p,p

(cid:62)

∈

(cid:3)

˜IN −p =

Let

is a matrix of 0. Note that we restrict α(N −p+1):N = 0 such that all other αi and βt can be uniquely identiﬁed.

(cid:2)

RN ×(N −p), IN −p is an identity matix of dimension (N

p)

(N

−

×

−

p) and 0N −p,p

Then the precision of the estimated

(cid:0)

Z(cid:96) =

z((cid:96)+1):T z(cid:96):(T −1)

· · ·
(cid:2)
ˆτ , ˆα, ˆβ((cid:96)+1):T , ˆθ((cid:96)+1):T

z1:(T −(cid:96))

.

from (3.1)

(cid:3)

Var 







=

ˆτ
ˆα1:(N −p)
ˆβ((cid:96)+1):T
ˆθ((cid:96)+1):T

(cid:1)

Σ−1
e

·

·

Z (cid:62)
(cid:96)
Γ(cid:62)

(cid:18)(cid:20)

(cid:21)

−1

Z(cid:96) Γ

(cid:19)
(cid:3)







R(N(T −(cid:96)))×(N(T −(cid:96))) and Ψ = UΣvU(cid:62) + σ2
∈









(cid:2)

where Σe = diag(Ψ, Ψ,

, Ψ)

· · ·

ε IN from Assumption 3.1.

From block matrix inversion, we have

Prec ( ˆτ )−1 = Var ( ˆτ ) =

E.1. Proof of Lemma 3.1

(cid:0)

Z (cid:62)

(cid:96) Σ−1

e (Σe −

Γ(Γ(cid:62)Σ−1

e Γ)−1Γ(cid:62))Σ−1

e Z(cid:96)

−1

(cid:1)

To prove the separable quadratic representation of Prec ( ˆτ ), we ﬁrst state and prove a useful lemma.

Lemma E.1. Suppose the assumptions in Lemma 3.1 hold and σ2

ε = 1

1. The (j, m)-th entry in Z (cid:62)

(cid:96) ΣeZ(cid:96) equals

Z (cid:62)

(cid:96),jΣ−1

e Z(cid:96),m =

T −(cid:96)

t=1
(cid:88)

z(cid:62)

j−1+t

IN −

(cid:0)

U(Ik + U(cid:62)U)−1U(cid:62)

zm−1+t

(cid:1)

(E.7)

(E.8)

56

2. The (j, m)-th entry in Z (cid:62)

(cid:96) Σ−1

e Γ

(Γ(cid:62)Σ−1

e Γ)−1

·

·

Γ(cid:62)Σ−1

e Z(cid:96) equals

Z (cid:62)

(cid:96),jΣ−1

e Γ
T −(cid:96)+j−1

(Γ(cid:62)Σ−1

e Γ)−1

·

=N

t=j
(cid:88)

˜ω(cid:62)
t ˜ωt+m−j −

where ˜ωt = 1
N

N
i=1

˜Xizit ∈

Proof of Lemma E.1

(cid:80)

Γ(cid:62)Σ−1

e Z(cid:96),m
T −(cid:96)+j−1

·
N

T −(cid:96)+m−1

˜ω(cid:62)
t

(cid:33) (cid:32)

T

(cid:96) (cid:32)

−

t=j
(cid:88)
Rp and ζ (j) = 1

T −(cid:96)

t=m
(cid:88)
T −(cid:96)+j−1
t=j

1. Since Σe = diag(Ψ, Ψ,

(cid:80)

lowing the assumption that Σv = σ2
ε ·

Ik and σ2

· · ·
ε = 1, we have

+ (T

˜ωt

(cid:33)

−

(cid:96))(ζ (j))(cid:62)

IN −

(cid:0)

U(Ik + U(cid:62)U)−1U(cid:62)

ζ (m)

(cid:1)

zt ∈
, Ψ)

RN .

R(N(T −(cid:96)))×(N(T −(cid:96))) and Ψ = UΣvU(cid:62) + IN fol-

∈

Then

Ψ−1 =

1
σ2
ε

(IN + UU(cid:62))−1 =

U(Ik + U(cid:62)U)−1U(cid:62)

RN ×N .

∈

(cid:1)

IN −

(cid:0)

T −(cid:96)

Z (cid:62)

(cid:96),jΣeZ(cid:96),m =

z(cid:62)
j−1+tΨzm−1+t.

2. Next we show the (j, m)-th entry in Z (cid:62)

(cid:96) Σ−1

e Γ

(Γ(cid:62)Σ−1

e Γ)−1

t=1
(cid:88)

·

Γ(cid:62)Σ−1

e Z(cid:96). This consists of the following

·

three steps

(a) Z (cid:62)

(cid:96) Σ−1

e Γ has

Z (cid:62)

(cid:96),jΣ−1

e Γ = Z (cid:62)

(cid:96),j

where (φ(j))(cid:62) =








Since ζ (j) = 1
(cid:80)
T −(cid:96)

Ψ˜IN −p Ψ ˜X
Ψ˜IN −p
...
Ψ˜IN −p

Ψ ˜X

. . .

T −(cid:96)

t=1 z(cid:62)

j−1+tΨ˜IN −p = (
zt ∈

T −(cid:96)+j−1
t=j

RN , we have
(cid:80)



=

T −(cid:96)

t=1 z(cid:62)

j−1+tΨ˜IN −p, z(cid:62)

j Ψ ˜X,

Ψ ˜X

(cid:2)(cid:80)





t=1 zj−1+t)(cid:62)Ψ˜IN −p and (ι(j))(cid:62) =
T −(cid:96)

, z(cid:62)

T −(cid:96)+j−1Ψ ˜X

=

(φ(j))(cid:62) (ι(j))(cid:62)

,

· · ·

(cid:3)

(cid:2)

(cid:3)

j Ψ ˜X,
z(cid:62)

· · ·

, z(cid:62)

T −(cid:96)+j−1Ψ ˜X

.

(cid:2)

(cid:3)

(cid:80)

φ(j) = T (ζ (j))(cid:62)Ψ˜IN −p.

Note that U and ˜X are orthogonal (from the assumptions in Lemma 3.1), we have

Ψ ˜X = ˜X

and

˜X(cid:62)Ψ ˜X = N

Ip

·

(E.9)

Then

(ι(j))(cid:62) =

z(cid:62)
j

˜X,

· · ·

, z(cid:62)

T −(cid:96)+j−1

˜X

=

N ˜ω(cid:62)
j ,

, N ˜ω(cid:62)

T −(cid:96)+j−1

· · ·

= N ˜ω(cid:62)

j:j(cid:96) ∈

R(T −(cid:96))p,

where ˜ωt = 1
N

N
i=1

(cid:2)
˜Xizit ∈

In summary,

(cid:80)

Rp, and ˜ω(cid:62)

(cid:3)
j:j(cid:96) is deﬁned as

(cid:2)

˜ω(cid:62)
j ,

· · ·

(cid:2)

(cid:3)
T −(cid:96)+j−1

, ˜ω(cid:62)

.

(cid:3)

Z (cid:62)

(cid:96),jΣ−1

e Γ =

T (ζ (j))(cid:62)Ψ˜IN −p, N ˜ω(cid:62)
j:j(cid:96)

.

(cid:2)

(cid:3)

57

(b) Using block matrix inverse, we decompose (Γ(cid:62)Σ−1

e Γ)−1 as

(Γ(cid:62)ΣeΓ)−1 =

=

Ξ11 Ξ12
Ξ21 Ξ22
(cid:20)
(cid:21)
M ˜M, Ξ21 =

M ˜M

M
−
˜M(cid:62)M ¯M + ˜M(cid:62)M ˜M
(cid:21)

(cid:20)
−
˜M(cid:62)M, Ξ22 = ¯M + ˜M(cid:62)M ˜M with

∈

where Ξ11 = M, Ξ12 =

−

−

R(N+(T −(cid:96)−1)p))×(N+(T −(cid:96)−1)p),

1

(cid:96)

M =

N −pΨ˜IN −p −
˜I(cid:62)
T
−
N −pΨ ˜X( ˜X(cid:62)Ψ ˜X)−1,
˜I(cid:62)
¯M =diag(( ˜X(cid:62)Ψ ˜X)−1, ( ˜X(cid:62)Ψ ˜X)−1,

˜M =

, ˜I(cid:62)

· · ·

(cid:16)

(cid:2)

N −pΨ ˜X( ˜X(cid:62)Ψ ˜X)−1 ˜X(cid:62)Ψ˜IN −p
˜I(cid:62)

N −pΨ ˜X( ˜X(cid:62)Ψ ˜X)−1
, ( ˜X(cid:62)Ψ ˜X)−1) =
(cid:3)

· · ·

=
1
N

1
N −pΨ˜IN −p −
˜I(cid:62)
N
R(N −p)×((T −(cid:96))p)

˜X ˜X(cid:62)

−1

∈

(cid:19)

R(N −p)×(N −p)

1

−
1
N

−1

=

T

(cid:17)
1
N

˜X

· · ·
(cid:2)
I((T −(cid:96))p)

(cid:96)
(cid:18)
˜X

∈

R((T −(cid:96))p)×((T −(cid:96))p)

(cid:3)

∈

and we use (E.9) in the simpliﬁcation.

We can further simplify M using Woodbury matrix identity

1

−

T

(cid:96) (cid:34)

M =

where

˜I(cid:62)
N −pΨ˜IN −p

−1

+

N −pΨ˜IN −p
˜I(cid:62)

(cid:16)

(cid:17)

(cid:16)

−1

˜X

N

(cid:17)

(cid:18)

−

˜X(cid:62)

N −pΨ˜IN −p
˜I(cid:62)

−1

−1

˜X

˜X(cid:62)

N −pΨ˜IN −p
˜I(cid:62)

(cid:16)

(cid:17)

(cid:19)

(cid:16)

−1

(cid:35)

(cid:17)

N −pΨ˜IN −p
˜I(cid:62)

−1

U(1)(Ik + U(cid:62)U)−1U(cid:62)
(1)

−1

= IN −p + U(1)(Ik + U(cid:62)

(2)U(2))−1U(cid:62)

(1),

=

IN −p −
(cid:0)
R(N −p)×k and U(2) =
∈

(cid:16)
u1 u2

(cid:17)
uN −p

U(1) =

(cid:62)

(cid:1)
uN
· · ·
∈
e Γ)−1Γ(cid:62)Σ−1
e Γ(Γ(cid:62)Σ−1
(cid:96),jΣ−1
From step (a), it is equivalent to calculating each term in

(c) We combine steps (a) and (b) to calculate Z (cid:62)

uN −p+1

· · ·

(cid:3)

(cid:2)

(cid:2)

(cid:3)

(cid:62)

Rp×k (note that U =

e Z(cid:96),m for 1
(cid:2)

(cid:62)

).

U(cid:62)

(1) U(cid:62)
(2)
j, m

≤

(cid:96) + 1.
(cid:3)

≤

(φ(j))(cid:62) (ι(j))(cid:62)

(cid:2)
Each term has

(cid:3)

Ξ11 Ξ12
Ξ21 Ξ22
(cid:20)

φ(m)
ι(m)

(cid:21) (cid:20)

(cid:21)

= (φ(j))(cid:62)Ξ11φ(m) + (φ(j))(cid:62)Ξ12ι(m) + (ι(j))(cid:62)Ξ21φ(m) + (ι(j))(cid:62)Ξ22ι(m)

(φ(j))(cid:62)Ξ11φ(m) = (T

−

(cid:96))2(ζ (j))(cid:62)Ψ˜IN −pM˜I(cid:62)

N −pΨζ (m)

T −(cid:96)+m−1

(cid:96))(ζ (j))(cid:62)Ψ˜IN −pM ˜M ˜ωm:m(cid:96) =

(T

−

−

(cid:96))(ζ (j))Ψ˜IN −pM ˜X

(cid:32)

˜ωm−1+t

(cid:33)

(φ(j))(cid:62)Ξ12ι(m) =

(ι(j))(cid:62)Ξ21φ(m) =

N (T

N (T

−

−

−

−

(cid:96)) ˜ω(cid:62)
j:j(cid:96)

˜M(cid:62)MΨ˜IN −pζ (m) =

t=m
(cid:88)
N −pζ (m)

˜X(cid:62)MΨ˜I(cid:62)

T −(cid:96)+j−1

(T

−

−

(cid:96))

(cid:32)

˜ωt

t=j
(cid:88)
T −(cid:96)+m−1

˜X(cid:62)M ˜X

˜ω(cid:62)
t

(cid:33)

(cid:32)

t=m
(cid:88)

(cid:33)

˜ωt

(cid:33)

(ι(j))(cid:62)Ξ22ι(m) = N

˜ω(cid:62)

t ˜ωt+m−j +

T −(cid:96)+j−1

T −(cid:96)+j−1

t=j
(cid:88)

(cid:32)

t=j
(cid:88)
zt and ˜ωj:j(cid:96) =

where we use ζ (j) = 1
T −(cid:96)
Let ˜X(1) :=
˜X1 ˜X2
˜X (cid:62)
˜X (cid:62)
(2)
(1)

(cid:62)

(cid:2)

T −(cid:96)+j−1
t=j
˜XN −p

(cid:62)

(cid:3)

˜X =
). We can simplify (φ(j))(cid:62)Ξ11φ(m), (φ(j))(cid:62)Ξ12ι(m), (ι(j))(cid:62)Ξ21φ(m) and (ι(j))(cid:62)Ξ22ι(m) by calcu-

(note

Rp×p

˜XN

· · ·

· · ·

(cid:80)

∈

∈

(cid:3)

(cid:2)

(cid:62)

(cid:3)

˜ω(cid:62)
j
R(N −p)×p and ˜X(2) :=
(cid:2)

˜ω(cid:62)

· · ·

T −(cid:96)+j−1

.
˜XN −p+1

(cid:104)
lating the following terms

(cid:105)

˜IN −p(˜I(cid:62)

N −pΨ˜IN −p)−1˜I(cid:62)

N −p =

IN −p + U(1)(Ik + U(cid:62)

(2)U(2))−1U(cid:62)

(cid:20)

0(cid:62)

(1) 0
0

(cid:21)

RN ×N

∈

58

and

and

and

Ω :=Ψ˜IN −p(˜I(cid:62)

N −pΨ

N −pΨ˜IN −p)−1˜I(cid:62)
IN −p

0
(1) 0
U(1)(Ik + U(cid:62)U)−1U(cid:62)
(1)

(2)U(2))−1U(cid:62)

Ψ

(cid:21)

U(2)(Ik + U(cid:62)U)−1U(cid:62)

=

=

U(2)(Ik + U(cid:62)
−
(cid:20)
IN −p −
−
(cid:20)

−
(2)U(2))−1U(cid:62)
(1) U(2)(Ik + U(cid:62)

U(1)(Ik + U(cid:62)U)−1U(cid:62)
(2)
(1)U(1)(Ik + U(cid:62)U)−1U(cid:62)

RN ×N

∈

(2)(cid:21)

(˜I(cid:62)

N −pΨ˜IN −p)−1 ˜X(1) = ˜X(1)
−
˜X(1)

N −pΨ˜IN −p)−1 ˜X(1) =

˜IN −p(˜I(cid:62)

U(1)(Ik + U(cid:62)

(2)U(2))−1U(cid:62)

(2)
(2)U(2))−1U(cid:62)
U(1)(Ik + U(cid:62)
(2)
0

−

˜X(2)

∈
˜X(2)

R(N −p)×p

RN ×p

∈

(cid:21)

(cid:20)

δ := ˜X(cid:62)

(1)(˜I(cid:62)

γ := Ψ˜IN −p(˜I(cid:62)

N −pΨ˜IN −p)−1 ˜X(1) = N Ip −
N −pΨ˜IN −p)−1 ˜X(1) =

˜X(2)

( ˜X(cid:62)
(2)

˜X(cid:62)
−
˜X(1)
(2)U(2))−1U(cid:62)
(2)
i=1 ζ (j)
ζ (j)
N . More generally, we have (T

U(2)(Ik + U(cid:62)
(cid:20)

i = N

(cid:96))

−

N

and ωt, we have (T

(cid:80)

ωt −

(cid:96))
(cid:80)

−

From the deﬁnition of ζ (j)
i=1 ζ (j)
N −1

T −(cid:96)−1+j
t=j

i = N

i

T −(cid:96)
these properties,
(cid:80)

(2)U(2)(Ik + U(cid:62)

(2)U(2))−1U(cid:62)

(2)

˜X(2))

∈

Rp×p

RN ×p.

˜X(2)(cid:21)

∈

T −(cid:96)−1+j
t=j
N
i=1

˜Xiζ (j)

i = N

ωt for j = 1, 2,

, (cid:96) + 1 and

· · ·
T −(cid:96)−1+j
t=j

˜ωt. Using

(cid:80)

(cid:80)

(cid:80)

(φ(j))(cid:62)Ξ11φ(m) = (T

(cid:96))(ζ (j))(cid:62)

δ)−1γ (cid:62)

ζ (m)

(φ(j))(cid:62)Ξ12ι(m) =

(ι(j))(cid:62)Ξ21φ(m) =

−

−

−
(T

−

(cid:96))(ζ (j))(cid:62)
(cid:0)

(T

(cid:96))(ζ (j))(cid:62)

−
T −(cid:96)+j−1

(cid:16)

Ω + γ(N Ip −
γ(N Ip −
˜X(N Ip −

(cid:16)

(ι(j))(cid:62)Ξ22ι(m) = N

˜ω(cid:62)

t ˜ωt+m−j + (T

t=j
(cid:88)
We sum these four terms together and obtain

δ)−1 ˜X(cid:62)

δ)−1γ (cid:62)

ζ (m)
(cid:1)

(cid:17)

ζ (m)

(cid:17)
(cid:96))(ζ (j))(cid:62)

−

˜X(N Ip −

(cid:16)

δ)−1 ˜X(cid:62)

ζ (m)

(cid:17)

N

−

T

(cid:96) (cid:32)

−

t=j
(cid:88)

˜ω(cid:62)
t

(cid:33) (cid:32)

˜ωt

(cid:33)

t=m
(cid:88)

T −(cid:96)+j−1

T −(cid:96)+m−1

T −(cid:96)+j−1

N

t=j
(cid:88)

with

˜ω(cid:62)
t ˜ωt+m−j −

T

N

T −(cid:96)+j−1

T −(cid:96)+m−1

(cid:96) (cid:32)

−

t=j
(cid:88)

˜ω(cid:62)
t

(cid:33) (cid:32)

t=m
(cid:88)

+ (T

˜ωt

(cid:33)

−

(cid:96))(ζ (j))(cid:62)

Ω +

γ

(cid:18)

(cid:16)

˜X

−

(cid:17)

(N Ip −

δ)−1

γ

(cid:16)

˜X

−

(cid:62)

ζ (m).

(cid:19)

(cid:17)

Ω +

γ

˜X

−

(N Ip −

δ)−1

γ

˜X

−

(cid:16)
(cid:17)
(2)U(2))−1U(cid:62)
U(2)(Ik + U(cid:62)

0

(2)(cid:21)

= Ω +

(cid:16)
(cid:17)
0
0(cid:62) Ip −
(cid:20)
U(Ik + U(cid:62)U)−1U(cid:62),
= IN −
(2)U(2) and

following U(cid:62)U = U(cid:62)

(1)U(1) + U(cid:62)

U(2)(Ik + U(cid:62)

(2)U(2))−1U(cid:62)

= U(2)(Ik + U(cid:62)

(2)U(2))−1(U(cid:62)

U(2)(Ik + U(cid:62)U)−1U(cid:62)

(2).

=

−

(1)U(1)(Ik + U(cid:62)U)−1U(cid:62)
Ik −

(1)U(1)

−

(2) −

U(cid:62)U)(Ik + U(cid:62)U)−1U(cid:62)
(2)

U(2)(Ik + U(cid:62)

(2)U(2))−1U(cid:62)

(2)

59

In summary Z (cid:62)

(cid:96),jΣ−1

e Γ(Γ(cid:62)Σ−1

e Γ)−1Γ(cid:62)Σ−1

e Z(cid:96),m equals

T −(cid:96)+j−1

N

t=j
(cid:88)
(cid:3)

˜ω(cid:62)
t ˜ωt+m−j −

T

N

T −(cid:96)+j−1

T −(cid:96)+m−1

(cid:96) (cid:32)

−

t=j
(cid:88)

˜ω(cid:62)
t

(cid:33) (cid:32)

t=m
(cid:88)

+ (T

˜ωt

(cid:33)

−

(cid:96))(ζ (j))(cid:62)

IN −

(cid:0)

U(Ik + U(cid:62)U)−1U(cid:62)

ζ (m).

(cid:1)

Proof of Lemma 3.1 and Equation (A.4) From Lemma E.1, when σ2

ε = 1, the (j, m)-th entry in Prec( ˆτ )

is Z (cid:62)

(cid:96) Σ−1

Γ(Γ(cid:62)Σ−1

e Γ)−1Γ(cid:62))Σ−1

e Z(cid:96) and equals

e (Σe −
e Z(cid:96),m −
z(cid:62)

j−1+t

(cid:96),jΣ−1

Z (cid:62)
T −(cid:96)+j−1

t=j
(cid:88)

T −(cid:96)+j−1

N

−

t=j
(cid:88)

T −(cid:96)+j−1

=

=

Z (cid:62)

(cid:96),jΣ−1

e Γ(Γ(cid:62)Σ−1

e Γ)−1Γ(cid:62)Σ−1

e Z(cid:96),m

U(Ik + U(cid:62)U)−1U(cid:62)

zm−1+t

IN −

(cid:0)
˜ω(cid:62)

t ˜ωt+m−j +

N

−

T

T −(cid:96)+j−1

T −(cid:96)+m−1

(cid:1)
˜ω(cid:62)
t

(cid:96) (cid:32)

t=j
(cid:88)
(cid:96))(ζ (j))(cid:62)ζ (m)

(cid:33) (cid:32)

−

T

N

˜ωt

(cid:33) −
T −(cid:96)+j−1

t=m
(cid:88)

(cid:96) (cid:32)

−
:=a(j,m)

t=j
(cid:88)

z(cid:62)
j−1+tzm−1+t − (cid:34)

(T

−

t=j
(cid:88)

(cid:124)
+ N

dx

T −(cid:96)+j−1

·

q=1 (cid:34) −
(cid:88)

t=j
(cid:88)

t ωxq
ωxq

t+m−j +

(cid:123)(cid:122)
ωxq
t

T −(cid:96)+m−1

(cid:33) (cid:32)

t=m
(cid:88)

ωxq

t

(cid:33) (cid:35)

T −(cid:96)+j−1

1

(cid:96) (cid:32)

T

−
:=b(j,m)

t=j
(cid:88)

T −(cid:96)+j−1

−

t=j
(cid:88)

(cid:96))(ζ (j))(cid:62)

(T

−

IN −

U(Ik + U(cid:62)U)−1U(cid:62)

ζ (m)

T −(cid:96)+m−1

ωt

(cid:33) (cid:32)

t=m
(cid:88)

(cid:0)

ωt

+ N

(cid:33)

T −(cid:96)+j−1

(cid:1)

ωtωt+m−j

t=j
(cid:88)

(cid:35)

(cid:125)

(cid:124)
+ (T

−

(cid:123)(cid:122)
(cid:96))(ζ (j))(cid:62)U(Ik + U(cid:62)U)−1U(cid:62)ζ (m)

t U (Ik + U(cid:62)U)−1U (cid:62)zt+m−j
z(cid:62)

(cid:125)

,

where ωt = 1
(cid:124)
N

N

i=1 zit and ωxq

t = 1

N

:=c(j,m)

N
i=1 Xiqzit for q = 1,

(cid:123)(cid:122)

, dx.

· · ·

When there are no covariates, we only have the term a(j,m). We can write

(cid:80)

(cid:80)

(cid:125)
T −(cid:96)+j−1
t=j

z(cid:62)
j−1+tzm−1+t and

(ζ (j))(cid:62)ζ (m) in a(j,m) in terms of ω1,

, ωT .

First, for the term

T −(cid:96)+j−1
t=j

(cid:80)
· · ·
j−1+tzm−1+t and (ζ (j))(cid:62)ζ (m), if j = m, then
z(cid:62)

T −(cid:96)+j−1
t=j

z(cid:62)
j−1+tzj−1+t =

N (T

−

(cid:96)). if j

= m, suppose j < m, then we have

(cid:80)

T −(cid:96)+j−1

t=j
(cid:88)

z(cid:62)
j−1+tzm−1+t = N

(cid:96)) +

(T

(cid:34)

−

(cid:80)

.

m−1

t=j
(cid:88)

(ωt −

ωT −(cid:96)+t)

(cid:35)

Second, let us write (ζ (j))(cid:62)ζ (m) in terms of ω1,

i ζ (m)

i

can take, denoted as υ(j,m)

· · ·

, ωT . Recall the deﬁnition ζ (j)
, υ(j,m)

, υ(j,m)
1

,

T

0

i = 1

T −(cid:96)

when unit i starts to get the treatment at time period T + 1

are T + 1 diﬀerent values that ζ (j)
value of ζ (j)
value of ζ (j)

i ζ (m)
i ζ (m)

i

i

T −(cid:96)−1+j
t=j

z1t, there

· · ·

, where υ(j,m)
(cid:80)

t

denotes the

t (and υ(j,m)

0

represents the

−

when unit i stays in the control group for all time periods). Without loss of generality, we

assume j

≤

m and have

1

1 + 2(t−1−(cid:96)+m)

−

T −(cid:96)

−
1 + 2(t−1−(cid:96)+m)
(cid:16)
−
1 + 2(t−1−(cid:96)+j)

T −(cid:96)

T −(cid:96)

(cid:17)
−

(cid:17) (cid:16)

(cid:17)

υ(j,m)

t

=






(cid:16)

(cid:16)
1

−

1 + 2(t−1−(cid:96)+j)

T −(cid:96)

(cid:96) + 1

(cid:96) + 1

t

≤
(cid:96) + 1

m

−
m < t

−

−

(cid:96) + 1

−

≤

j < t

≤
m < t

T + 1

−
T + 1

≤

j < t

j

k

j

−

(cid:17)

T + 1

T + 1

−

−

(cid:54)
60

Given ωt, there are N(1+ω1)
having N(1+ω1)
2
and leaving N(1−ωT )

2
, N(ω2−ω1)
2

,

· · ·

2

,

, N(1+ω2)
2
· · ·
, N(ωT −ωT −1)

2

N(1+ωT )
2

treated units in time period 1, 2,

, T . It is equivalent to

· · ·

untreated units to start the treatment in time period 1, 2,

, T

· · ·

units in the control group in the end.

(ζ (j))(cid:62)ζ (m) =

N

i=1
(cid:88)

i ζ (m)
ζ (j)

i = N

(cid:20)

= N

1 +

(cid:34)

following υ(j,m)

0

= υ(j,m)

T

= 1.

1 + ω1
2

υ(j,m)

T

ω2

+

ω1

−
2

υ(j,m)
T −1 +

1

ωT
−
2

·

υ(j,m)
0

·
υ(j,m)

T

υ(j,m)
T −1

ω1 +

υ(j,m)
T −1 −
2

−
2

+

· · ·
υ(j,m)
T −2

(cid:21)
υ(j,m)
1

υ(j,m)
0

−
2

ωT

,

(cid:35)

ω2 +

+

· · ·

We plug the expression of

T −(cid:96)+j−1
t=j

j−1+tzm−1+t and (ζ (j))(cid:62)ζ (m) into a(j,m) and multiply a(j,m) by 1/σ2
z(cid:62)

ε

to account for σ2
ε (cid:54)

= 1, then we obtain Equation (A.4).

(cid:80)

To show Lemma 3.1, we plug the expression of

T −(cid:96)+j−1
t=j
ε to account for σ2
ε (cid:54)

(cid:80)

multiply a(j,j), b(j,j) and c(j,j) by 1/σ2

= 1 then we have

j−1+tzj−1+t and (ζ (j))(cid:62)ζ (j) into a(j,j), and
z(cid:62)

tr

=

1
(cid:0)
σ2
ε

=

−

j=1
(cid:88)
N
σ2
ε ·

(cid:96) Σ−1

Z (cid:62)
(cid:96)+1

Γ(Γ(cid:62)Σ−1

e (Σe −
a(j,j) + b(j,j) + c(j,j)

e Γ)−1Γ(cid:62))Σ−1

e Z(cid:96)

(cid:1)

T −(cid:96)−1+j

2

T −(cid:96)−1+j

(cid:1)
1

(cid:0)
(cid:96)+1

T −(cid:96)−1+j

j=1
(cid:88)





t=j
(cid:88)

ω2

t −

T

(cid:96) (cid:32)

−

t=j
(cid:88)

N
σ2
ε ·

−

(cid:96)+1

dx
(cid:124)

T −(cid:96)+j−1

j=1
(cid:88)

k=1 (cid:34)
(cid:88)

t=j (cid:32)
(cid:88)

1
N

N

i=1
(cid:88)

Xikzit

+

ωt

(cid:33)

2

fj,1(z)

(cid:123)(cid:122)
1

−

T

(cid:33)

(cid:96) (cid:32)

−

fj,X(z)

2(T

(cid:96)

−

2t)

−

−
T

1 + 2j
(cid:96)

−

t=j
(cid:88)

1
N

T −(cid:96)+j−1

N

t=j
(cid:88)

i=1
(cid:88)

Xikzit

2

(cid:33)

(cid:35)

ωt


(cid:125)

N
σ2
ε

−

1
(cid:124)
N

(cid:96)+1

j=1 (cid:34)
(cid:88)

T −(cid:96)+j−1

t=j
(cid:88)

t U(Ik + U(cid:62)U)−1U(cid:62)zt −
z(cid:62)

(cid:123)(cid:122)

1
N (T

−

T −(cid:96)+j−1

(cid:62)

zt

(cid:19)

(cid:96))

(cid:18)

fj,U(z)

t=j
(cid:88)

(cid:125)

U(Ik + U(cid:62)U)−1U(cid:62)

T −(cid:96)+j−1

zt

,

(cid:19)(cid:35)

(cid:18)

t=j
(cid:88)

(cid:124)

1

where fj,X(z) can be written as
N z(cid:62)
MUzj:j(cid:96) with MU = P1T −(cid:96) ⊗
j:j(cid:96)
(cid:3)

U(Idu + U(cid:62)U)−1U(cid:62).

(cid:80)

dx

k=1(ωxk

j:j(cid:96))(cid:62)P1T(cid:96)

ωxk

(cid:123)(cid:122)

j:j(cid:96), and fj,U(z) can be written as fj,U(z) =

(cid:125)

E.2. Proof of Theorem 3.1

Proof of Theorem 3.1 As described in Section 3.3, if we can ﬁnd a design that can separately minimize

fj,1(z), fj,X(z), fj,U(z), then this design can maximize the precision Prec( ˆτ ).

Let us ﬁrst consider the design that minimizes fj,1(z). We can write it out as

fj,1(z) =

(cid:96)+1

T −(cid:96)−1+j

j=1
(cid:88)





t=j
(cid:88)

ω2

t −

T

1

−

T −(cid:96)−1+j

2

T −(cid:96)−1+j

(cid:96) (cid:32)

t=j
(cid:88)

+

ωt

(cid:33)

t=1
(cid:88)

2(T

(cid:96)

−

2t)

−

−
T

1 + 2j
(cid:96)

−

(E.10)

.

ωt


61

The Lagrangian of fj,1(z) is

(cid:96)+1

T −(cid:96)−1+j

(ω, λ, κ, ι) =

L



j=1
(cid:88)

T

+

ω2

t −

T

1

−
T

t=j
(cid:88)

T −(cid:96)−1+j

2

T

(cid:96) (cid:32)

t=j
(cid:88)

+

ωt

(cid:33)

t=1
(cid:88)

2(T

(cid:96)

−

2t)

−

−
T

1 + 2j
(cid:96)

−

ωt


λt(

1

ωt) +

−

−

t=1
(cid:88)
(ω, λ, κ, ι) are

κt(ωt −

1) +

t=1
(cid:88)

T −1

t=1
(cid:88)

ιt(ωt −

ωt+1).

The KKT conditions of

∂
L
∂ωt

∂
L
∂ωt

∂
L
∂ωt

L
t
j=1 sj
(cid:96)
T

= tωt − (cid:80)
= ((cid:96) + 1)ωt − (cid:80)

−

= (T + 1

−

1

λt(

−

1

−

≤

−
ωt ≤

−

+

(cid:96)+1
j=1 sj
(cid:96)
T
T +1−t
j=1
t)ωt − (cid:80)
T
ωt) = 0, κt(ωt −

sj

−

(cid:96)

(cid:96)

(T

+

−
T

−
(cid:96)

t)t

−

λt + κt + ιt −
2t)

−
((cid:96) + 1)(T + 1
(cid:96)
T

−

ιt−1 = 0,

(cid:96)

t

≤

−

λt + κt + ιt −
t)

−

−
ωt+1) = 0

−

(T

+

1) = 0,

−

(cid:96)

1)(T + 1
−
T
(cid:96)
−
ιt(ωt −
0,
0, κt ≥

1, ωt ≤

ωt+1, λt ≥

0

ιt ≥

(E.11)

(E.12)

ιt−1 = 0,

(cid:96) < t

T

(cid:96)

−

≤

λt + κt + ιt −

ιt−1 = 0, t > T

(cid:96) (E.13)

−

where sj =

T −(cid:96)−1+j
t=j

ωt for j = 1,

, (cid:96) + 1 and ι0 = 0.

· · ·

The Hessian of f (ω) is positive semi-deﬁnite. Any solution that satisﬁes the KKT conditions is optimal.

(cid:80)

First we can show the optimal solution is symmetric with respect to the origin. The proof is as follows.

If ω‡ is the optimal solution that minimizes (E.10), then we can show ω† =

same value in the objective function as ω‡ because

(cid:96)+1
j=1

T −(cid:96)−1+j
t=1

ω‡
−
2(T −(cid:96)−1+2j−2t)
(cid:2)
T −(cid:96)

T −

ω‡
T −1 · · · −

ω‡
1

has the

ωt in f (ω) is symmetric

with respect to the origin and similarly for the other two terms in f (ω). Since (E.10) is convex, f

(cid:80)

(cid:80)

1
2

f (ω‡) + f (ω†)

= f (ω‡). Then if ω‡ is optimal, ω† = ω‡.

(cid:16)

(cid:3)
ω‡+ω†
2

≤

(cid:17)

Now we can focus on the ω that satisﬁes
(cid:2)

(cid:3)

ω1 ω2

ωT

=

−
s(cid:96)+1−j. If (cid:96) is even, s(cid:96)/2+1 = 0.
(cid:2)

· · ·

(cid:2)

(cid:3)

ωT −

ωT −1

ω1

· · · −

. From the deﬁnition of

(cid:3)

sj =

T −(cid:96)−1+j
t=j

ωt, we have sj =

−

Now we are going to verify ω∗ =

(cid:80)

feasible λ, κ, ι.

ω∗

1 ω∗

2 · · ·

deﬁned in Eq. (3.8) satisﬁes the KKT conditions with

ω∗
T

(cid:3)

(cid:2)
1 + 2t−((cid:96)+1)

T −(cid:96)

−

sj =

(cid:96)+1−j
t=j ωt
T −(cid:96)+j
t=T −j ωt

(cid:40)(cid:80)
(cid:80)

1. ω∗

t for (cid:96) < t

T

(cid:96): ω∗

t =

satisﬁes Eq. (E.12) with λt = κt = ιt = 0 and ι(cid:96) = 0.

2. ω∗

t for t

≤

≤

−
(cid:96): Given ωt =

−
ωT +1−t. We can simplify sj to

for j = 1,
for j =

,
· · ·
((cid:96) + 1)/2
(cid:99)

((cid:96) + 1)/2
(cid:99)
+ 1,
· · ·

(cid:98)

.

, (cid:96) + 1

(cid:98)
As an example, when (cid:96) = 2, we have s1 = ω1 + ω2, s2 = 0 and s3 = ωT −1 + ωT ; when (cid:96) = 3, we have s1 =

ω1 + ω2 + ω3, s2 = ω2, s3 = ωT −1 and s4 = ωT −2 + ωT −1 + ωT . Furthermore, sj + s(cid:96)+2−j = 0 for 1

j

≤

≤

(cid:96) + 1.

(cid:96), we have

t
j=1 sj =

(cid:96)+1−t
j=1

sj.

−

(cid:96)/2

, there exist some ωt for
(cid:99)

(cid:80)

(cid:80)

(cid:96)/2

(cid:98)

(cid:99)

< t

≤

(cid:96) and some feasible λt, κt, ιt

≤ (cid:98)

Using this property, for

(cid:98)
Next we show when ωt =

< t

(cid:96)/2
(cid:99)
≤
1 for t

that satisfy Eq. (E.11).

When ωt =

1 for t

(cid:96)/2

, then for

(cid:96)/2

< t

(cid:96),

−
t, (cid:96)

(cid:99)

≤ (cid:98)
)ω(cid:98)(cid:96)/2(cid:99)+1 +

(cid:96)/2

(cid:98)
+ min((cid:96) + 1

(cid:99)

−

· · ·
(cid:99)
1 + ω2; when (cid:96) = 3, s1 + s2 =

− (cid:98)

min((cid:96) + 1

s2 =

−

t
j=1 sj =

≤
t, 2)ω(cid:96)−1 + min((cid:96) + 1
(cid:80)

(cid:80)

(cid:96)+1−t
j=1

(cid:96)+1−t
j=1

sj =

(cid:96)/2
(cid:99)
(cid:98)
t, 1)ω(cid:96). As an example, when (cid:96) = 2,

+ 1

j)

−

+

(

(cid:2) (cid:80)

(cid:3)

−

−

1 + 2ω2 + ω3, s1 + s2 + s3 =

−

−

1 + ω2 + ω3. We can rewrite Eq. (E.11)

62

for

(cid:96)/2

< t

(cid:98)

(cid:99)
part of this proof)

≤

(cid:96) in a vectorized form as the following (we will consider Eq. (E.11) for t

(cid:96)/2
(cid:99)

≤ (cid:98)

in the later

∂L
∂ω(cid:98)(cid:96)/2(cid:99)+1

...



= A((cid:96))



∂L
∂ω(cid:96)

ω(cid:98)(cid:96)/2(cid:99)+1
...
ω(cid:96)



b((cid:96))



−

λ(cid:98)(cid:96)/2(cid:99)+1
...
λ(cid:96)

+ 



+ 



κ(cid:98)(cid:96)/2(cid:99)+1
...
κ(cid:96)



where A((cid:96)) and b((cid:96)) are deﬁned in Eq. (A.1) and Eq. (A.2). When






















ω(cid:98)(cid:96)/2(cid:99)+1

ι(cid:98)(cid:96)/2(cid:99)+1
...
ι(cid:96)

ι(cid:98)(cid:96)/2(cid:99)
...
ι(cid:96)−1









− 


(cid:62)

ω(cid:96)

− 



= 0,



(E.14)

= (A((cid:96)))−1b((cid:96)), Eq.

· · ·
, (cid:96) and ι(cid:98)(cid:96)/2(cid:99) = 0. The remaining step is to verify the

(cid:96)/2

1
−

1 + (cid:96)+1

constraints

(E.14) holds with λt = κt = ιt = 0 for t =
ωt ≤ −
T −(cid:96) and ωt ≤
A((cid:96))1
(a) The ﬁrst step is to show
−
are positive while the oﬀ-diagonal entries in A((cid:96)) are negative, then A((cid:96))
decreasing in ωs for t(cid:48) = 1,

(cid:98)
· · ·
(cid:99)
ωt+1 hold if
b((cid:96))

ω(cid:98)(cid:96)/2(cid:99)+1
1 + (cid:96)+1
(cid:2)

and s

+ 1,

· · ·

(cid:2)
(cid:62)

ω(cid:96)

, (cid:96)

≤

≤

≤

(cid:3)

(
−

T −(cid:96) )A((cid:96))1. Note that the diagonal entries in A((cid:96))
t(cid:48),:ω((cid:98)(cid:96)/2(cid:99)+1):L is increasing in ωt and
T −(cid:96) )A((cid:96))1
, (cid:96).
+ 1,
−
b((cid:96)), which is equivalent to showing every entry in A((cid:96))1 + b((cid:96)) is non-negative,

= t(cid:48) +
(cid:98)
(cid:99)
= (A((cid:96)))−1b((cid:96)) is between

≤
T −(cid:96) for t =

A((cid:96))1
≤
1 + (cid:96)+1

(cid:96)/2
(cid:99)
1 and

, t = t(cid:48) +
(cid:62)

(
−
(cid:96)/2

(cid:96)/2
(cid:99)
ω(cid:96)

1 + (cid:96)+1

b((cid:96))

(cid:96)/2

− (cid:98)

. If

· · ·

· · ·

−

−

(cid:98)

(cid:99)

(cid:98)

(cid:3)
= (A((cid:96)))−1b((cid:96)).

hold, then ωt deﬁned in

· · ·
ω(cid:98)(cid:96)/2(cid:99)+1

0. If (cid:96) is even,

≥

(cid:96)−t
l=1((cid:96)

(cid:96)/2
(cid:99)

− (cid:98)

+ 1

−

l) = t((cid:96)+1−t)

2

and

(cid:80)

First, let us show

that is, for t(cid:48) = 1,

−

A((cid:96))1
(cid:2)
, (cid:96)
· · ·
− (cid:98)
l) = t((cid:96)+1−t)

≤
(cid:96)/2
(cid:99)

2

(cid:96)−t
l=1(

(cid:80)

+ 1

(cid:96)/2
−
(cid:99)
(cid:98)
(A((cid:96))1)t(cid:48) + b((cid:96))

t(cid:48) = t

If (cid:96) is odd,

(cid:96)−t
j=1((cid:96)

(cid:96)/2

− (cid:98)

(A((cid:96))1)t(cid:48) + b((cid:96))

t(cid:48) = t

(cid:80)

(cid:3)
, (A((cid:96))1)t(cid:48) + b((cid:96))
t(cid:48)

. Let t = t(cid:48) +

(cid:96)/2
(cid:99)

(cid:98)

. We have

1

t)

−

t((cid:96) + 1
2
(cid:96)
−
−
j) = (t+1)((cid:96)+1−t)

2

−

T

+ 1

(cid:99)
1

−
(t + 1)((cid:96) + 1

t)

t +

T

and

t2

1

−

(cid:96) −
(cid:96)−t
j=1(

(cid:96)

T

−
(cid:96)/2
(cid:99)

(cid:98)

t2
(cid:80)

1

(t

t)

−

=

t((cid:96) + 1
−
2
−
j) = (t−1)((cid:96)+1−t)

t(2T
T

+ 1

(cid:96)
−
(cid:96)

1)

0.

≥

. We have

−
1)((cid:96) + 1
2

2

t)

A((cid:96))(1

−

(cid:16)

)1

t(cid:48)

+ b((cid:96))

t(cid:48) =

(cid:17)

following t(T

1) < 0 and 2

(cid:96) + 1
(cid:96)
T

−
(cid:96)

−

−
(cid:96) + 1
(cid:96)
T

A((cid:96))(1

−

)1

t(cid:48)

+ b((cid:96))

t(cid:48) =

−
(cid:16)
following t(T

(cid:17)

(cid:96)

−
t(2T
T

−
−
t+1
2t

−

T
Second, let us show b((cid:96))

2
1 + (cid:96)+1
b((cid:96)) is non-positive, that is, for t(cid:48) = 1,

−
(
−

≤

(cid:96)

t +

T

−

−

−
−
−
T −(cid:96) )A((cid:96))1, which is equivalent to showing every entry in (1
(cid:96)+1
T −(cid:96) )1
0. If (cid:96) is even

A((cid:96))(1

(cid:96) −

, (cid:96)

−

−

−

T

=

(cid:96)

−

t(2T
T

1)

(cid:96)
−
(cid:96)

0.

≥
T −(cid:96) )A((cid:96))1 +

(cid:96)+1

· · ·

t(2T
T

(cid:96)
−
(cid:96)

−
−

− (cid:98)
1)

,

(cid:96)/2
(cid:99)
(cid:96) + 1
(cid:0)
(cid:96)
T

−

(cid:16)
(cid:96)+1
T −(cid:96) > 0. If (cid:96) is odd,

1
2

−

−
t((cid:96) + 1
2(T

t

−

t(cid:48)

t(cid:48) + b((cid:96))
t)
(cid:1)
−
(cid:96))

=

≤
t(T

−

(cid:17)

−
T

(cid:96)

−

1)

−
(cid:96)

2

−

(cid:16)

1
2

(cid:96) + 1
(cid:96)
T

−

(cid:17)

< 0

(cid:96)
−
(cid:96)

1)

−

(cid:96) + 1
(cid:96)
T

−

(cid:16)

t

−

(t + 1)((cid:96) + 1
(cid:96))
2(T

−

t)

−

t(T

=

(cid:17)

−
T

(cid:96)

−

t + 1
2t

2

−

1)

−
(cid:96)

(cid:16)

(cid:96) + 1
(cid:96)
T

−

(cid:17)

< 0

−
(b) We can show

−

1) < 0 and 2
b((cid:96))
t(cid:48) /(A((cid:96))1)t(cid:48) is non-decreasing in t(cid:48) for t(cid:48) = 1,

(cid:96)+1
T −(cid:96) > 0.

−

(cid:96)/2
(cid:99)
entries in A((cid:96)) are positive while the oﬀ-diagonal entries in A((cid:96)) are negative, then A((cid:96))
t(cid:48),:ω((cid:98)(cid:96)/2(cid:99)+1):L is increasing
b((cid:96))
t(cid:48) /(A((cid:96))1)t(cid:48) is
in ωt and decreasing in ωs for t(cid:48) = 1,
(cid:96)/2
(cid:99)

. Note that the diagonal

, t = t(cid:48) +

(cid:96)/2
(cid:99)

= t(cid:48) +

and s

− (cid:98)

(cid:96)/2

. If

− (cid:98)

· · ·

· · ·

, (cid:96)

, (cid:96)

−

−

(cid:98)

(cid:98)

(cid:99)

non-decreasing in t(cid:48), then ωt is non-decreasing in t, where t = t(cid:48) +

(cid:96)/2

.

Let ct(cid:48) be the ct(cid:48) that satisﬁes (A((cid:96))(

t(2T
T

−
(cid:96)
−
(cid:96)

−
−
(cid:96)

−

2T

−

⇔

1 = ct(cid:48)

(cid:98)
t(cid:48) and let t = t(cid:48) +

(cid:99)

1 + ct(cid:48)

T −(cid:96) )1)t(cid:48) = b((cid:96))
1)
ct(cid:48)

=

T
(cid:96)
−
t + 2T
2(T

t
(cid:18)
−
−

−
3(cid:96)
(cid:96))

T

−

t((cid:96) + 1
2

(cid:96)

−

1

−
1
.

. If (cid:96) is even, we have

(cid:96)/2
(cid:99)
(cid:98)
t)

(cid:19)

Since ∂(2T −(cid:96)−1)

∂t

= 2,

∂ t+2T −3(cid:96)−1
2(T −(cid:96))
∂t

= 1

2(T −(cid:96)) , and 2 > 1

2(T −(cid:96)) , we have ct(cid:48)

increases in t and t(cid:48). This implies

b((cid:96))
t(cid:48) /(A((cid:96))1)t(cid:48) is non-decreasing in t(cid:48) for even (cid:96).

−

(cid:54)
(cid:54)
63

If (cid:96) is odd, we have

t(2T
T

1)

=

(cid:96)
−
(cid:96)

ct(cid:48)

2T

−

⇔

1 = ct(cid:48)

(cid:18)

1

(t + 1)((cid:96) + 1

t

T

−
1 +

(cid:96)
T
−
(cid:18)
(t + 1)(T
2t(T

(cid:96)

−
−
−

2

.

1)

(cid:96)
(cid:96))

−

(cid:19)

t)

−

(cid:19)

−
−
(cid:96)

−

(cid:96)+3
(cid:96)+1

1
T −(cid:96) , and 2 >

(cid:96)+3

(T −(cid:96))((cid:96)+1) , we have ct(cid:48) increases in t and t(cid:48). This again

Since ∂(2T −(cid:96)−1)

= 2,

∂t

∂ t+2T −3(cid:96)−1
2(T −(cid:96))
∂t

implies

≤
b((cid:96))
t(cid:48) /(A((cid:96))1)t(cid:48) is non-decreasing in t(cid:48) for odd (cid:96).
(cid:96), ωt deﬁned in
< t

−

We have veriﬁed that for

(cid:96)/2
(cid:99)

(cid:98)

≤

conditions. The remaining step is to verify for t

When ωt =

1, ωt ≤
need to verify that we can ﬁnd feasible λt, κt, ιt to satisfy Eq. (E.11). Since ωt =

1, constraints

ωt ≤

≤ (cid:98)

−

−

≤

1

(cid:96)/2
(cid:99)
≤ (cid:98)
ωt+1 for t

ω(cid:98)(cid:96)/2(cid:99)+1

(cid:62)

ω(cid:96)

· · ·

= (A((cid:96)))−1b((cid:96)) satisiﬁes the KKT

, ωt deﬁned as ωt =
(cid:2)
−
ω+(cid:98)(cid:96)/2(cid:99)+1 are satisﬁed. We only
and ω(cid:98)(cid:96)/2(cid:99) ≤

1 satisﬁes the KKT conditions.

(cid:96)/2
(cid:99)

(cid:3)

1, from complementary

−

slackness, κt = 0. Plug ωt =

1 into Eq. (E.11), we have

−

λ1

−

ι1 =

ιt + ιt−1 =

λt −

λt + ιt−1 =

1 + s1
T
(cid:96)
−
t
t2 +
j=1 sj
T
(cid:96)
(cid:80)
−
t
t2 +
j=1 sj
(cid:96)
T
(cid:80)
−

−

−

−

for t = 2,

(cid:96)/2

· · · (cid:98)

1

(cid:99) −

for t =

(cid:96)/2
(cid:99)

(cid:98)

We only need to verify

t2+(cid:80)t
T −(cid:96)

j=1 sj

−

Note that

take any value by properly choosing λt and ι.
j=1 sj = 1
we can show ω(cid:96)+1−(cid:98)(cid:96)/2(cid:99) + 1

1
T −(cid:96)

(cid:98)(cid:96)/2(cid:99)

T −(cid:96)

(cid:80)

≤

0 for t =

(cid:96)/2
(cid:99)

(cid:98)

≥

as for the other conditions, λ1

ι and λt −

−

ιt + ιt−1 can

sj = ((cid:96) + 1

L+1−(cid:98)(cid:96)/2(cid:99)
j=1
− (cid:98)
(cid:96)+1−(cid:98)(cid:96)/2(cid:99) for even (cid:96) and ω(cid:96)+1−(cid:98)(cid:96)/2(cid:99) + 1

(cid:96)/2
(cid:99)

)(ω(cid:96)+1−(cid:98)(cid:96)/2(cid:99) + 1)

1

−
(cid:96)+1
T −(cid:96)

(cid:96)+1
(cid:80)
T −(cid:96)

((cid:96)+1−(cid:98)(cid:96)/2(cid:99))2
T −(cid:96)

. Furthermore, if

2

(cid:96)+1−(cid:98)(cid:96)/2(cid:99) for odd (cid:96), then we

≤

have

=

and therefore

−
Next is to show “ω(cid:96)+1−(cid:98)(cid:96)/2(cid:99) + 1

≥

t2+(cid:80)t
T −(cid:96)

j=1 sj

0.

2
(cid:96)/2
(cid:99)
(cid:98)
(cid:96) −
T
−
−
2
((cid:96) + 1
(cid:98)
T

−

(cid:96)/2
(cid:99)
(cid:96)

−

((cid:96) + 1

(cid:96)/2
(cid:99)

− (cid:98)
)((cid:96) + 1)

)(ω(cid:96)+1−(cid:98)(cid:96)/2(cid:99) + 1) +

)2

((cid:96) + 1
− (cid:98)
T
−

(cid:96)/2
(cid:99)
(cid:96)

((cid:96) + 1

(cid:96)/2

− (cid:98)

)(ω(cid:96)+1−(cid:98)(cid:96)/2(cid:99) + 1)
(cid:99)

≥

0

−

(cid:96)+1
T −(cid:96)

1

cu
t :=

1 + (cid:96)+1
T −(cid:96)

≤
(cid:98)((cid:96)+1)/2(cid:99)+1 . If we can show ωt ≤ −
≤

−
“ω(cid:96)+1−(cid:98)(cid:96)/2(cid:99) + 1
entries in A((cid:96)) are positive while the oﬀ-diagonal entries in A((cid:96)) are negative, then A((cid:96))

(cid:96)+1−(cid:98)(cid:96)/2(cid:99) for even (cid:96) and ω(cid:96)+1−(cid:98)(cid:96)/2(cid:99) + 1

(cid:96)+1−(cid:98)(cid:96)/2(cid:99) for even (cid:96) and ω(cid:96)+1−(cid:98)(cid:96)/2(cid:99) + 1
(cid:98)((cid:96)+1)/2(cid:99)+1 := cu

1 + (cid:96)+1
T −(cid:96)

(cid:96)+1
T −(cid:96)

t for t = t(cid:48) +

≤

≤

t(cid:48)

t(cid:48)

1

(cid:96)+1
T −(cid:96)

2

(cid:96)+1−(cid:98)(cid:96)/2(cid:99) for odd (cid:96).” Denote
, then it implies

(cid:96)/2
(cid:99)

2

(cid:96)+1
T −(cid:96)

(cid:98)
(cid:96)+1−(cid:98)(cid:96)/2(cid:99) for odd (cid:96).” Note that the diagonal
t(cid:48),:ω((cid:98)(cid:96)/2(cid:99)+1):L is increasing
b((cid:96))
(cid:96)−(cid:98)(cid:96)/2(cid:99),

. We only need to show (A((cid:96))cu)(cid:96)−(cid:98)(cid:96)/2(cid:99) ≥

in ωt and decreasing in ωs for t = t(cid:48) +

where cu =

cu
(cid:98)(cid:96)/2(cid:99)+1 · · ·

cu
(cid:96)−(cid:98)(cid:96)/2(cid:99)

(cid:62)

and s

(cid:96)/2
(cid:99)
(cid:98)

= t(cid:48) +
. If (cid:96) is even, and when T > (cid:96)2+11(cid:96)+2

(cid:96)/2
(cid:99)
(cid:98)

,

(cid:2)

−

(A((cid:96))cu)(cid:96)−(cid:98)(cid:96)/2(cid:99) + b((cid:96))

−
−
If (cid:96) is odd, and when T > (cid:96)3+13(cid:96)2+7(cid:96)+3

(cid:3)
(cid:96)−(cid:98)(cid:96)/2(cid:99) =

(cid:96)((cid:96)
T

8(cid:96)

1)
(cid:96) −

(cid:96)((cid:96) + 1)
(cid:96)
T

−

(cid:18)

(cid:96)

4

(cid:96) + 2 −

T

(note that (cid:96)3+13(cid:96)2+7(cid:96)+3

8(cid:96)

=

−

T

(cid:19)
−
> (cid:96)2+11(cid:96)+2

8

(A((cid:96))cu)(cid:96)−(cid:98)(cid:96)/2(cid:99) + b((cid:96))

(cid:96)−(cid:98)(cid:96)/2(cid:99) =

−

(cid:96)((cid:96)
T

−
−

1)
(cid:96) −

(cid:96)((cid:96) + 1)
(cid:96)
T

−

(cid:96) + 1
(cid:96) + 3

+

((cid:96) + 1)2
4(T

(cid:96))2 =

−

(cid:96)

(cid:96)

(cid:18)

−

2

(cid:96) + 1

(cid:96) + 2 −

4(T

(cid:96))

(cid:19)

−

< 0.

),

1

2(cid:96)

−

T

(cid:96)

(cid:18)

−

(cid:96) + 3 −

(L + 2)2
(cid:96))
4(T

−

(cid:19)

< 0.

8

(cid:96)

(cid:54)
64

3. ω∗

t for t > T

this case.

−

(cid:96): this is a symmetric case of ω∗

t for t < (cid:96). The proof of ω∗

t for t > T

(cid:96) carries over to

−

We have veriﬁed that the ω∗ deﬁned in Eq. (3.8) satisﬁes the KKT conditions and the Hessian of (E.10) is

positive semi-deﬁnite, then ω∗ is an optimal solution that minimizes fj,1(z).

to ﬁnd a solution that minimizes fj,X(z). Note

that fj,X(z)

can be written as

is a positive semi-deﬁnite matrix with one eigenvalue to be 0 and the corre-

is
Next
dx
k=1(ωxk
j:j(cid:96) )(cid:62)P1T(cid:96)

ωxk

j:j(cid:96). P1T(cid:96)
sponding eigenvector to be 1. Therefore, (ωxk
(cid:80)
when X(cid:62)zt is the same for all t, or equivalently 1
N

j:j(cid:96))(cid:62)P1T(cid:96)

j:j(cid:96) ≥

ωxk
N
i=1 Xizit = µX for some µX ∈

0 for all z and the minimum value is attained
Rdx .

Finally is to ﬁnd a solution that minimizes fj,U(z). Note that fj,U(z) can be written as fj,U(z) =

(cid:80)

1

MUzj:j(cid:96) with MU = P1T −(cid:96) ⊗

N z(cid:62)
j:j(cid:96)
minimum value is attained when U(cid:62)zt is the same for all t, or equivalently, 1
N
µU ∈

U(Idu + U(cid:62)U)−1U(cid:62). Similar to fj,X(z), z(cid:62)MU z

Rdu.

(cid:80)

≥

0 for all z and the

N
i=1 uizit = µU for some

Combining the optimality conditions for fj,1(z) fj,X(z) and fj,U(z). A solution is optimal if it satisﬁes

N

zit = ω∗
t ,

1
N

i=1
(cid:88)

N

Xizit = µX,

1
N

i=1
(cid:88)

1
N

N

i=1
(cid:88)

uizit = µU ,

for all t.

(E.15)

We therefore ﬁnish the proof of Theorem 3.1.

(cid:3)

E.3. Proof of Proposition A.2

Suppose the assumptions in Theorem 3.1 hold and du = 0. Suppose (1 +
Nmin = ming |Og|

, xj,max = maxg |

, and xgj is the j-th coordinate of xg across all g. We have
j=1 x2

T(cid:96)((cid:96) + 1)

j,max

(cid:80)

r

xgj|
f (zrnd)

f (z∗)

≤
Proof of Proposition A.2 Let z∗ be the optimal solution to (3.2) with relaxed constraint zit ∈

−

N 2
(cid:80)
min

and therefore z∗ is feasible and satisﬁes all the conditions in Theorem 3.1. Note that

.

1, +1],

[
−

r

j=1 x2

j,max)/N 2

min < 1, where

We can provide a bound of tr

(cid:0)
Prec( ˆτ )

(cid:1)
Prec( ˆτ )

tr

Prec( ˆτ )

tr

Prec( ˆτ )

z∗ ≥

(cid:0)
tr

(cid:1)
zint∗ −
(cid:0)
Prec( ˆτ )

(cid:1)
tr

(cid:0)

tr

Prec( ˆτ )

zint∗ ≥
(cid:1)
(cid:0)
zrnd by bounding

zrnd .

tr

(cid:1)
Prec( ˆτ )

zrnd ,

z∗ −

(cid:1)
which is what we are going to do in the following.

(cid:0)

(cid:0)

(cid:1)

Note that when du = 0,

where

tr

Prec( ˆτ )

z =

(cid:0)

(cid:1)

N
σ2
ε

−

(f1(z) + fX(z)) ,

f1(z) =

(cid:96)+1

T −(cid:96)−1+j

j=1
(cid:88)





t=j
(cid:88)

ω2

t −

T

1

−

(cid:96) (cid:32)

t=j
(cid:88)

T −(cid:96)−1+j

2

T −(cid:96)−1+j

2(T

(cid:96)

−

2t)

−

−
T

1 + 2j
(cid:96)

−

t=j
(cid:88)

fX(z) =

(cid:96)+1

dx
(cid:124)

T −(cid:96)+j−1

j=1
(cid:88)

k=1 (cid:34)
(cid:88)

t=j (cid:32)
(cid:88)

1
N

N

i=1
(cid:88)

Xikzit

(cid:124)

−

T

(cid:33)

(cid:96) (cid:32)

−

fj,X(z)

(cid:123)(cid:122)

1
N

T −(cid:96)+j−1

N

2

Xikzit

.

(cid:33)

(cid:35)

t=j
(cid:88)

i=1
(cid:88)

(cid:125)

+

ωt

(cid:33)

2

fj,1(z)

(cid:123)(cid:122)
1

ωt


(cid:125)

65

We can bound tr

Prec( ˆτ )

z∗ −
Let us bound the gap between f1(zrnd) and f1(z∗).

(cid:0)

(cid:1)

(cid:1)

(cid:0)

tr

Prec( ˆτ )

zrnd by bounding f1(zrnd)

f1(z∗) and fX(zrnd)

fX(z∗).

−

−

Note that ωt is bounded between

−

0. Therefore, for all j,

1 and +1 and the eigenvalues of the Hessian of fj,1(z) are either 1 or

Next let us bound the diﬀerence between ωrnd

t

and ω∗

t . We introduce the notation ωg,t:

fj,1(z) = O(T

(cid:96)).

−

ωg,t =

1
|Og| (cid:88)

i∈Og

(2

1Ai≤t −

·

1)

zit

be the treated fraction of units in stratum g scaled between

(cid:124)

1 and +1. Let ωrnd
(cid:123)(cid:122)

(cid:125)

g,t be the value of ωg,t evaluated

−

at the rounded feasible solution

Arnd
N
i=1.
i }
Since we use the nearest rounding rule to get a feasible zrnd, we have

{

ωrnd
|

g,t −

ω∗

g,t| ≤

1
|Og | for all t and g,

and therefore,

wrnd
|

(cid:96),t −

w∗

(cid:96),t|

=

|

G

g=1
(cid:88)

pg(ωrnd

g,t −

ω∗

g,t)

| ≤

G

g=1
(cid:88)

pg|

ωrnd

g,t −

ω∗

g,t| ≤

G

g=1
(cid:88)

pg
Nmin

= O

1
Nmin (cid:19)

.

(cid:18)

Let δt = wrnd

t −

w∗

t . The diﬀerence between fj,1(zrnd) + f(cid:96)+1−j,1(zrnd) and fj,1(z∗) + f(cid:96)+1−j,1(z∗) equals

fj,1(zrnd) + f(cid:96)+1−j,1(zrnd)

T −(cid:96)−1+j

T −j

fj,1(z∗) + f(cid:96)+1−j,1(z∗)
T −(cid:96)−1+j

−

(cid:1)
ω∗

(cid:0)
(cid:96),tδt

2

−

T

(cid:33)

(cid:96) (cid:34)(cid:32)

−

t=j
(cid:88)

(cid:1)
ω∗
(cid:96),t

T −(cid:96)−1+j

T −j

T −j

(cid:33) (cid:32)

t=j
(cid:88)

+

δt

(cid:33)

(cid:32)

t=(cid:96)+1−j
(cid:88)

ω∗
(cid:96),t

(cid:33) (cid:32)

t=(cid:96)+1−j
(cid:88)

δt

(cid:33)(cid:35)

T −(cid:96)−1+j

2

T −j

2

2
(cid:123)(cid:122)

(cid:125)

(cid:0)
= 2

(cid:32)

t=j
(cid:88)

ω∗

(cid:96),tδt +

t=(cid:96)+1−j
(cid:88)

1

T −j

(cid:123)(cid:122)

δ2
t +

δ2
t

t=(cid:96)+1−j
(cid:88)

3

(cid:123)(cid:122)
b(cid:96),tδt +

T −j

t=(cid:96)+1−j
(cid:88)

5

(cid:124)
+

T −(cid:96)−1+j

(cid:32)

t=j
(cid:88)

T −(cid:96)−1+j

(cid:124)

+

(cid:32)

t=j
(cid:88)

(cid:125)
1

(cid:124)

−

T

(cid:33)

(cid:96) 

(cid:32)

−



(cid:125)
(cid:124)
b(cid:96),tδt

(cid:33)

+

δt

(cid:33)

δt

(cid:33)

(cid:32)

t=(cid:96)+1−j
(cid:88)

t=j
(cid:88)

4

(cid:123)(cid:122)





(cid:125)

Since ω∗

(cid:96),t =

(cid:124)

−

ω∗

(cid:96),T +1−t for all t, we have ω∗

(cid:96),tδt + ω∗

(cid:96),T +1−tδT +1−t = 0 and b(cid:96),tδt + b(cid:96),T +1−tδT +1−t = 0 following

(cid:123)(cid:122)

(cid:125)

the property of our rounding algorithm. Therefore, assume N is even, we have

1 = 0

2 = 0

5 = 0

and

fj,1(zrnd) + f(cid:96)+1−j,1(zrnd)

We sum j together and obtain

(cid:0)

fj,1(z∗) + f(cid:96)+1−j,1(z∗)

= 3 + 4 = O

(cid:1)

(cid:18)

−

(cid:1)

(cid:0)

f1(zrnd)

−

f1(z∗) = O

((cid:96) + 1)(T

N 2

min

(cid:96))

−

.

(cid:19)

(cid:18)

2(T
N 2

(cid:96))

.

−
min (cid:19)

66

Next let us bound the gap between

(cid:96)+1

j=1 fj,X(zrnd) and

(cid:96)+1
j=1 fj,X(z∗). Then for each covariate Xik,

wxk,rnd

(cid:96),t

|

wxk,∗

(cid:96),t

=

|

|

−

(cid:80)
pgxgk(ωrnd
g,t −

ω∗

g,t)

| ≤

G

g=1
(cid:88)

G

(cid:80)
pgxj,max

g=1
(cid:88)

ωrnd
|

g,t −

ω∗

g,t| ≤

We use a similar procedure as above and obtain that for covariate Xik,

fj,Xk (zrnd) + f(cid:96)+1−j,Xk (zrnd)

fj,Xk (z∗) + f(cid:96)+1−j,Xk (z∗)

=

−

G

g=1
(cid:88)

O

(cid:18)

pgxk,max
Nmin

=

xk,max
Nmin (cid:19)

.

O

(cid:18)

2x2

k,max(T
N 2

min

(cid:96))

−

.

(cid:19)

We sum over k (all covariates) and j (all treatment eﬀects) together and obtain

(cid:1)

(cid:0)

(cid:1)

(cid:0)

fX(zrnd)

−

fX(z∗) = O

(cid:32)

((cid:96) + 1)(T

(cid:96))
−
N 2

min
(cid:80)

dx
k=1 x2

k,max

.

(cid:33)

Since both fX(z∗) and fX(z∗) are at the order of O(((cid:96) + 1)(T

(cid:96))), we have

−

f1(zrnd) + fX(zrnd) =

f1(zrnd) + fX(zrnd)

and from the deﬁnition of tr

Prec( ˆτ )

(cid:0)
z we have

(cid:0)
Prec( ˆτ )

tr

(cid:1)
zrnd = tr

Prec( ˆτ )

Together with tr

(cid:0)
Prec( ˆτ )

z∗ ≥

(cid:1)
tr

(cid:0)
Prec( ˆτ )

(cid:1)

zint∗ , we have

1 +

1 +

· (cid:32)

O (cid:32)

(cid:1)

k,max

dx
k=1 x2
N 2

min

(cid:80)

(cid:33)(cid:33)

1 +

1 +

z∗ · (cid:32)

O (cid:32)

k,max

dx
k=1 x2
N 2

min

(cid:80)

.

(cid:33)(cid:33)

(cid:0)

tr

(cid:0)

(cid:1)

Prec( ˆτ )

(cid:0)
zrnd = tr

(cid:1)
Prec( ˆτ )

(cid:1)

(cid:0)

(cid:1)

1 +

1 +

zint∗ · (cid:32)

O (cid:32)

(cid:3)

k,max

dx
k=1 x2
N 2

min

(cid:80)

.

(cid:33)(cid:33)

67

Appendix F: Proof of Results for Sequential Experiments

Proof of Lemma 4.1 The the estimation error of ˆτ from the within estimator is

With some algebra, we can show that

−1

τ =

ˆτ

−

˙z2
it

(cid:33)

(cid:32)

i,t
(cid:88)

i,t
(cid:88)

˙zit ˙εit.

1
N T

i,t
(cid:88)
i,t aitbit, ¯ai,· = 1

T

˙ait ˙bit = ab

1
N

−

t ait, ¯a·,t = 1

N

where ab = 1
N T

¯ai,·¯bi,· −

1
T

i
(cid:88)

t
(cid:88)
i ait, and ¯a = 1

N T

deﬁned. Using this equality and from Lemma 3.1, we have

(cid:80)

(cid:80)

(cid:80)

¯a·,t¯b·,t + ¯a¯b,

(F.16)

i,t ait. ¯bi,·, ¯b·,t and ¯b are similarly

(cid:80)

1
N T

˙z2
it =

1
T

−

ω(cid:62)P1T ω

2
T

−

b(cid:62)

0 ω

i,t
(cid:88)
˙zit ˙εit

0,

p
−→

1
N T

i,t
(cid:88)

where the second line follows from 1
N T
p
p
E[εit] = 0 and ¯ε
E[¯zi,·]E[¯εi,·] = 0, ¯εi,·
−→
−→

i,t zitεit
E[εit] = 0.
(cid:80)

is consistent.

p
−→
1
N T

(cid:80)

E[zitεit] = E[zit]E[εit] = 0, 1

i ¯zi,· ¯εi,·

N

E[¯zi,· ¯εi,·] =

p
−→

i,t ˙z2

it is nonzero for any vector ω and therefore, ˆτ
(cid:80)

Next we show ˆτ is asymptotically normal. We can rewrite the estimation error ˆτ as

τ =

ˆτ

−

1
N T

ζitεit,

i,t
(cid:88)

where ζit = ( 1
N T

N T (zit −
we can verify the condition maxi,t
(cid:80)

it)−1 1

i,t ˙z2

(cid:80)

i ¯zi,· −
σ2ζ2
it
(cid:80)
i,t σ2ζ2
it →

(cid:80)

t ¯z·,t + ¯z) is a constant that depends on Z. Since εit

(0, σ2

ε ),

iid
∼

0 holds, given the expression of ζit and the boundedness of zit.

Therefore, from Lindeberg-Feller CLT, and with some algebra, we have

For the estimated variance σ2,

√N T (ˆτ

τ )

d
−→

−

N (0, Στ (ω)).

ˆσ2 =

=

From (F.16),

1
N (T

−
1
N (T

−

ˆτ ˙zit)2 =

( ˙yit −

1
N (T

−

˙ε2
it + Op

1
N

.

(cid:19)

(cid:18)

1)

1)

it
(cid:88)

i,t
(cid:88)

˙ε2
it −

1)

i,t
(cid:88)

2(ˆτ
N (T

τ )
1)

−
−

˙zit ˙εit +

i,t
(cid:88)

(ˆτ
−
N (T

τ )2
1)

−

˙z2
it

i,t
(cid:88)

1
N (T

−

1)

i,t
(cid:88)

˙ε2
it =

=

1
N (T

−
1
N (T

ε2
it −

ε2
it −

T
N (T

1)

i,t
(cid:88)

i,t
(cid:88)

1)

−
1
N T

=σ2

ε +

(ε2

it −

σ2
ε ) +

i,t
(cid:88)

T
N (T

−

1)

i
(cid:88)

¯ε2
i,· −

T

1

−

1

t
(cid:88)

¯ε2
·,t +

¯ε2

T

T

1

−

−

1)

i (cid:32)
(cid:88)
1
N T (T

ε2
it −

1
T 2

1
T 2

t
(cid:88)

t(cid:54)=s
(cid:88)

1)

−

εitεis + Op

(cid:18)

i,t(cid:54)=s
(cid:88)

εitεis

(cid:33)

+ Op

1
N

(cid:18)

(cid:19)

1
N

(cid:19)

68

Then from the classical CLT, we have

√N T (ˆσ2

σ2
ε )

−

Regarding the asymptotic covariance between ˆτ

1
N T

(cid:32)

i,t
(cid:88)

ζitεit

1
N T

(cid:33) (cid:32)

i,t
(cid:88)

(ε2

it −

σ2
ε )

(cid:33)

=

−
1
N 2T 2

ε +

0, ξ2

N

d
−→
(cid:18)
τ and ˆσ2

T
σ2
ε ,

−

1

−

σ4
ε

1

.

(cid:19)

ζitεit(ε2

it −

σ2
ε )

+

1
N 2T 2

i,t
(cid:88)

ζitεit(ε2

js −

σ2
ε )

.

(i,t)(cid:54)=(j,s)
(cid:88)

(a)

(b)

1
N

(cid:0)

(cid:1)

1
N T

(cid:32)

(a) = Op
independent of εjs and (b) is an average of O(N 2T 2) uncorrelated terms each with mean 0. In addition,

because (a) is sum of N T terms but the denominator is

N 2T 2 . (b) = Op
(cid:124)
(cid:123)(cid:122)

because εit is
(cid:125)

(cid:123)(cid:122)

(cid:125)

(cid:124)

1

1
N

(cid:0)

(cid:1)

ζitεit

(cid:33) (cid:32)

1
N T (T

εitεis

(cid:33)

=

1
N 2T 2(T

1)

−

i,t(cid:54)=s
(cid:88)

1)

−

i,j,t,u(cid:54)=s
(cid:88)

ζitεitεjuεjs = Op

1
N

(cid:18)

(cid:19)

i,t
(cid:88)
because the mean of each term is 0.

Therefore,

(ˆτ

τ )(ˆσ2

σ2
ε ) = Op

−

−

1
N

.

Since ˆτ and ˆσ2

(cid:18)
ε converge with rate √N T , ˆτ and ˆσ2 are asymptotically independent and they follow a joint

(cid:19)

CLT.

Next we show the consistency of ˆξ2

ε . For the term 1
T −1

i( ˙yit −

ˆτ ˙zit)2

ˆσ2,

−

(cid:80)

i (cid:16)
(cid:88)

i (cid:16)
(cid:88)

ˆτ ˙zit)2

( ˙yit −

−

t
(cid:88)

ˆσ2

2

=

(cid:17)

1
N

t
(cid:88)

(cid:0)

( ˙ε2

it −

σ2
ε ) + (τ

( ˙ε2

it −

σ2
ε ) + (τ

−

−

ˆτ ) ˙zit

(cid:1)(cid:17)

2

ˆτ ) ˙zit

+ Op

1
T
2(σ2

i (cid:16)
(cid:88)
2
+

t
(cid:88)
ε −
N T

1
N

( ˙ε2

it −

σ2
ε ) + (τ

ˆτ ) ˙zit

−

+ (σ2

ε −

(cid:0)
ˆσ2)

i,t
(cid:88)

(cid:0)

(cid:1)
σ2
ε ) + (τ

−

( ˙ε2

it −

ˆτ ) ˙zit

(cid:1)

2

ˆσ2)

(cid:17)
+ (σ2

ˆσ2)2

ε −

1
T

1
T

1
T

1
T

1
T

1
N

1
N

1
N

1
N

+

1
N

1
N

=

=

=

=

=

=

i (cid:16)
(cid:88)

t
(cid:88)

1
T
ˆτ )2

t
(cid:88)

i (cid:16)
(cid:88)
(τ
−
N

(cid:0)
( ˙ε2

it −

σ2
ε )

2

+

2(τ

−
N

(cid:1)(cid:17)

ˆτ )

(cid:17)
˙zit

2

+ Op

1
T

i (cid:16)
(cid:88)
( ˙ε2

t
(cid:88)
σ2
ε )

(cid:17)
2
+ Op

it −

(cid:17)

i (cid:16)
(cid:88)

(cid:19)

1
N

(cid:19)

(cid:18)
1
N

t
(cid:88)

t
(cid:88)

i (cid:16)
(cid:88)

i (cid:16)
(cid:88)
1
N T 2

(ε2

it −

σ2
ε )

−

¯ε2
i,·

2

σ2
ε )

(ε2

it −

(cid:17)

2
N T

−

(cid:18)

2

+ Op

1
N

(cid:18)
(ε2

it −

(cid:19)

i,t
(cid:88)
For ﬁrst term in the above equation, we have

i (cid:16)(cid:88)
(cid:88)

(cid:17)

t

(cid:19)
( ˙ε2

it −

(cid:18)

1
T

t
(cid:88)

σ2
ε )

(cid:17)(cid:16)

1
T

˙zit

t
(cid:88)

(cid:17)

ε )¯ε2
σ2

i,· +

1
N

¯ε4
i,· + Op

1
N

.

(cid:19)

(cid:18)

i
(cid:88)

1
N T 2

(ε2

it −

σ2
ε )

2

=

1
N T 2

(ε2

it −

ε )2 +
σ2

(ε2

it −

ε )(ε2
σ2

is −

σ2
ε )

=

i (cid:16)(cid:88)
(cid:88)

t

(cid:17)

i (cid:16)(cid:88)
(cid:88)

t

t(cid:54)=s
(cid:88)

following the independence between εit and εis. For the second term 2
N T

2
N T

i,t
(cid:88)

(ε2

it −

ε )¯ε2
σ2

i,· =

2
N T 3

i,t,s,u
(cid:88)

(ε2

it −

σ2
ε )εisεiu =

2
N T 3

(ε2

it −

i,t
(cid:88)

ε )ε2
σ2

(cid:80)
it + Op

1
T

ξ2
ε + Op

1
√N (cid:19)

.

(cid:18)

i,·, we have

(cid:17)

i,t(ε2
it −
1
√N (cid:19)

(cid:18)

σ2
ε )¯ε2
2
T 2 ξ2

=

ε + Op

1
√N (cid:19)

.

(cid:18)

69

For the third term 1
N

i ¯ε4

i,·, we have

¯ε4
i,· =

1
N

i
(cid:88)

(cid:80)

1
N T 4

i,t
(cid:88)

ε4
it +

6
N T 4

itε2
ε2

is + Op

1
√N (cid:19)

(cid:18)

=

i,t(cid:54)=s
(cid:88)

1
T 3 (ξ2

ε + σ4

ε ) +

1)

6(T

−
T 3

σ4
ε + Op

1
√N (cid:19)

.

(cid:18)

Summing three terms up, we have

1
N

1
T

t
(cid:88)

i (cid:16)
(cid:88)

ˆτ ˙zit)2

( ˙yit −

−

ˆσ2

(T

2

=

(cid:17)

1)2

−
T 3

ξ2
ε +

6T
5
T 3 σ4
−

ε + Op

1
√N (cid:19)

.

(cid:18)

Therefore,

ˆξ2
ε =

T 3

N (T

1)2

−

i (cid:16)
(cid:88)

1
T

t
(cid:88)

( ˙yit −

ˆτ ˙zit)2

ˆσ2

−

2

−

(cid:17)

that ﬁnishes the proof of the consistency of ˆξ2
ε .

6T
(T

5
1)2 ˆσ4
−
−

ε = ξ2

ε + Op

1
√N (cid:19)

(cid:18)

Lemma F.1. For i
sta, E[εit|
, ˆσ2
sta,1,
· · ·

(cid:54)∈ S
ˆσ2

For i
E[εitεjs|
E[εitεjs|

ˆσ2

sta,1,

, ˆσ2

· · ·

In addition, for i

E[ε2
it|
then t

Z] = σ2
ε . For i, j
= s), E[εitεjs|

1
N 3/2

(cid:1)

sta,T ] = Op
sta,T ] = 0.

∈ S

(cid:0)
sta, E[εit|
sta and t
∈ S
Z] = 0.

∈ S
ˆσ2
sta,1,

sta, E[εit|
, ˆσ2
· · ·

ˆσ2

, ˆσ2

sta,1,

and E[ε2
sta,1,
it|
· · ·
sta,T ] = 0 and E[ε2
sta,T ] = σ2
, ˆσ2
it|
∈ S
sta, and for all j, t and s (if i = j,
. For i

sta,T ] = Op
ˆσ2

1
N 2
sta,1,
(cid:0)

ε . For i, j

sta,T ] = σ2

(cid:1)
· · ·

, ˆσ2

ˆσ2

· · ·

ε + Op

1
N psta

.

sta and t

(cid:16)

(cid:17)
= s,

then t

= s),

(cid:54)∈ S

1
N 2

Z] = Op
= s, E[εitεjs|
(cid:0)
(cid:1)

and E[ε2
it|
Z] = Op

Z] = σ2

ε + Op

1
N 3/2

. For i

1
N psta

. For i

sta, E[εit|
sta, and for all j, t and s (if i = j,

Z] = 0 and

(cid:54)∈ S

(cid:17)

(cid:16)
(cid:54)∈ S

(cid:0)

(cid:1)

Proof of Lemma F.1 Note that ˆσ2

sta,t = 1

N tpsta

from εis for i

sta and 1

∈ S

s

≤

≤

t. If j

εit is i.i.d.

sta, E[εjt|
(cid:80)

(cid:54)∈ S

i∈Ssta,1≤s≤t ε2
, ˆσ2
ˆσ2
sta,1,

1
N

is + Op
sta,T ] = 0 and E[ε2
(cid:0)
jt|

(cid:1)

· · ·

, where the randomness comes

ˆσ2

sta,1,

· · ·

, ˆσ2

sta,T ] = σ2

ε as

If j

sta, note that εit is equally likely to be positive and negative given

1
N tpsta

i∈Ssta,1≤s≤t ε2

is. Condition-

ing on

∈ S
1
N tpsta

i∈Ssta,1≤s≤t ε2

is does not change the mean of εjt. Therefore, only the Op
(cid:80)
1
term is at the order of Op
N
, i.e., E[εit|
1
ˆσ2
sta,1,
(cid:0)
(cid:1)
N 2
t, the weight of ε2

1
N

1
N

term could possible

. Therefore conditioning
(cid:0)

(cid:1)

· · ·
js is

, ˆσ2

sta,T ] = Op
1
N tpsta

. Therefore,
(cid:0)
(cid:1)

1
N 2

.

(cid:80)
, ˆσ2

change the mean of εjt. The weight of εjt in this Op

on ˆσ2

(cid:0)

· · ·

sta,1,

conditioning on ˆσ2

sta,T , the mean can be changed at the order of Op

(cid:1)
js appears in ˆσ2
For the second moment, although ε2
sta,t for s
1
sta,t, the mean of ε2
js changes by
N tpsta
Let us consider the term E[εitεjs|
ˆσ2
, ˆσ2
sta,T ] = Op
sta,1,
sta,T ] = E[εit]E[εjs|
ˆσ2
· · ·
max(t, s). More speciﬁcally, εitεjs appears in
. Therefore, E[εitεjs|

= s), then E[εitεjs|
sta,u for u
≥

i,t(cid:54)=s εitεis is at the order of Op

appears in ˆσ2

1
N 2
sta,1,
(cid:0)

1
N u(u−1)

. For i

then t

sta,1,

(cid:1)
· · ·

1√
N

, ˆσ2

, ˆσ2

and

(cid:0)
≤

ˆσ2

· · ·

(cid:1)

.

(cid:54)∈ S

sta, and for all j, t and s (if i = j,

sta,T ] = 0. For i, j
1
N u(u−1)
ˆσ2
sta,1,

(cid:80)
· · ·

sta and t

= s, εitεjs

∈ S

i,t(cid:54)=s εitεis with weight
, ˆσ2

sta,T ] = Op

1
N 3/2

.

1
N 2

The treatment design of STU is chosen before the experiment starts. The cross-section average of treatment

(cid:0)

(cid:1)

(cid:80)

(cid:16)

(cid:17)

design of ATU depends on ˆσ2
design of both STU and ATU, and consequently E[εit|
Z] = 0.
for all j, t and s (if i = j, then t

sta,1,

, ˆσ2

· · ·

= s), E[εitεjs|

sta,T . Therefore, the error of ATU is independent of the treatment
Z] = σ2

ε . In addition, for i

sta, and

Z] = 0 and E[ε2
it|

(cid:54)∈ S

For the error of STU, since the treatment design of ATU is randomly chosen given the cross-section

average of treatment design, we can write Z as a function of ˆσ2

sta,1,

, ˆσ2

sta,T and some random term η that

· · ·

(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
70

determines which a unit is treated or control given the treated fraction. By deﬁnition, η is independent of

ˆσ2

sta,1,

· · ·

, ˆσ2

sta,T and εit. Therefore, E[εit|
Z] = E[E[εit|
E[εit|
, ˆσ2

ˆσ2
sta,1,

· · ·

Z] equals

sta,T , η]

Z] = E[E[εit|
|

ˆσ2
sta,1,

· · ·

, ˆσ2

sta,T ]

Z] = Op
|

1
N

.

(cid:19)

(cid:18)

Similarly,

E[ε2
it|

For i, j

sta and t

∈ S

ˆσ2
sta,1,

Z] = E[E[ε2
it|
= s, E[εitεjs|

ˆσ2

sta,1,

· · ·

, ˆσ2

sta,T , η]

Z] = E[E[ε2
it|
|

ˆσ2
sta,1,

, ˆσ2

Z] = Op
sta,T ]
|

· · ·

· · ·

1
N psta (cid:19)

.

(cid:18)

, ˆσ2

sta,T ] = Op

1
N 2

. (cid:3)

Proof of Theorem 4.1 Without loss of generality, assume the ﬁrst N psta units are STU, the subsequent
(cid:0)

(cid:1)

0.5N (1

−

psta) units are the ﬁrst set of ATU, and the remaining 0.5N (1

psta) units are the second set

−

1, 2,

, N psta

,
}

· · ·

ada,1 =

S

N psta + 1, N psta + 2,
{

· · ·

, 0.5N (1 + psta)

and

}

of ATU. In other words,

sta =

S
0.5N (1 + psta) + 1,

{
, N

S

ada,2 =
Note that the treatment design of ATU depends on the ˆσ2

.
}

· · ·

{

sta,t. We need to show that the asymptotic

distribution of ˆτ continues to hold even if zis depends on ˆσ2

sta,t for s

t. From Lemma 4.1,

≥

−1

ˆτall

−

τ =

˙z2
it

(cid:33)

(cid:32)

i,t
(cid:88)

i,t
(cid:88)

˙zit ˙εit.

Let us ﬁrst show the consistency of ˆτall. For the term,

From Lemma F.1, zε = 1
N T

1
N T

˙zit ˙εit = zε

i,t
(cid:88)
i,t zitεit = Op

1√
N

1
N

−

¯zi,· ¯εi,· −

1
T

¯z·,t ¯ε·,t + ¯z ¯ε

i
(cid:88)
because the treatment design of STU is chosen before

t
(cid:88)

the experiment starts and the treatment design of ATU only depends on the error of STU, and therefore
E[εit|
depend on Z and then ¯ε·,t = Op

zit] = 0 for all i and t. Following the same argument, 1
N
1√
N

. Therefore, combining these four terms, we have
(cid:0)

. Both ¯ε·,t and ¯ε do not

i ¯zi,· ¯εi,· = Op

and ¯ε = Op

1√
N

1√
N

(cid:80)

(cid:80)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

1
N T

(cid:0)

(cid:1)
˙zit ˙εit

i,t
(cid:88)

p
−→

0,

and ˆτ is consistent. Next we show the asymptotic distribution of ˆτ . The estimation error of ˆτ can be rewritten

as

where ζit = ( 1
N T

i,t ˙z2

it)−1 1

N T (zit −

(cid:80)
F.1, we can verify the condition maxi,t

(cid:80)

(cid:80)

0 holds, given the expression of ζit and the

boundedness of zit. Therefore, from Lindeberg-Feller CLT, we have

t ¯z·,t + ¯z) is a constant that depends on Z. Based on Lemma

ˆτall

−

τ =

1
N T

ζitεit,

i,t
(cid:88)

i ¯zi,· −

σ2+Op
(cid:80)
σ2+Op
(cid:0)

1
N psta
1
(cid:1)(cid:1)
N psta

(cid:0)
i,t

ζ2
it

it →
ζ2

(cid:0)

(cid:1)(cid:1)

(cid:0)
√N T (ˆτall

τ )

d
−→

−

N (0, Στ (ω)),

where the asymptotic variance equals Στ (ω) following that

1
N T

(cid:32)

i,t
(cid:88)

2

˙zit ˙εit

=

zε

(cid:33)

(cid:32)

1
N

−

i
(cid:88)

¯zi,· ¯εi,·

2

(cid:33)

=

1)2
(T
−
N 2T 4

itε2
z2

it + op

1
N T

(cid:18)

(cid:19)

i,t
(cid:88)

(cid:54)
71

and z2

it is always one, the each unit’s error is multiplied by this unit’s treatment variable in zε and 1
Then the weak dependence between εit and Z for STU does not aﬀect the asymptotic variance of ˆτall.

N

i ¯zi,· ¯εi,·.

(cid:80)

Next let us consider the asymptotic distribution of ˆσ2
1
(T

ˆσ2
ada,1 =

( ˙yit −

ˆτ ˙zit)2

ada,1

1)

ada,1.

|S

|

−

i∈Sada,1,t
(cid:88)

1
(T

|
1
(T

|

1

ada,1

|S

=

=

ada,1

|S

=σ2

ε +

1)

−

1)

−

i∈Sada,1,t
(cid:88)

i∈Sada,1,t
(cid:88)

˙ε2
it −

2(ˆτ

τ )

−
(T
|

1

ada,1

|S

1)

−

i∈Sada,1,t
(cid:88)

˙ε2
it + Op

ada,1

(cid:18)

|S

| (cid:19)
1
T (T
|

ada,1

|S

(ε2

it −

σ2
ε ) +

i∈Sada,1,t
(cid:88)
Then from Lemma 4.1, we have

|S

|

ada,1

T

˙zit ˙εit +

(ˆτ

τ )2
−
(T
|

−

ada,1

|S

˙z2
it

1)

i∈Sada,1,t
(cid:88)

1)

−

i∈Sada,1,t(cid:54)=s
(cid:88)

εitεis + Op

1
N

(cid:18)

(cid:19)

Similarly,

ada,1

(T
|

−

1)(ˆσ2

ada,1 −

σ2
ε )

N

d
−→

0, ξ2

ε +

(cid:18)

|S

(cid:113)

ada,2

|S

(T
|

−

1)(ˆσ2

ada,2 −

σ2
ε )

d
−→

N

0, ξ2

ε +

T

1

−

1

T

σ4
ε

1

(cid:19)

σ4
ε

1

.

.

Since ˆσ2

ada,1 and ˆσ2

(cid:18)
ada,2 are estimated from diﬀerent set of units, and εit is i.i.d., they are independent and

(cid:113)

−

(cid:19)

are jointly normal, together with

0.5N (1

psta)T

−

= 0.5N (1

|S

=

ada,1
|
ˆσ2
Sada,1
ˆσ2
Sada,2(cid:21)

|S

−

|

ada,2
σ2
ε
σ2
ε (cid:21)(cid:19)
(cid:20)

d
−→ N

psta) i.e.,
ε + 1
ξ2
T −1 σ4
0

,

ε

−
0
0
(cid:21)

(cid:18)(cid:20)
Regarding the asymptotic covariance between ˆτall and ˆσ2
Sada,1, we need to consider two terms. The ﬁrst

(cid:18)(cid:20)

(cid:112)

(cid:20)

ε + 1
ξ2

T −1 σ4

ε (cid:21)(cid:19)

.

(F.17)

term is

1
N T

(cid:32)

1

ζitεit

i,t
(cid:88)
1

T

|

ada,1

|S

(cid:33) 

ζitεit(ε2

it −

i∈Sada,1,t
(cid:88)
+

σ2
ε )

N

(ε2

it −

σ2
ε )

1





=

N

ada,1

|S

T 2
|

i,t
(cid:88)
(a)

(cid:123)(cid:122)

ada,1

|S

T 2

|

(i,t)(cid:54)=(j,s),j∈Sada,1
(cid:88)
(b)

ζitεit(ε2

js −

σ2
ε )

.

(a) = Op

1
N

because

(cid:1)
(cid:0)
(b)2 =

N 2

(cid:124)

because (a) is sum of

ada,1

|S

T terms but the denominator is
|

(cid:123)(cid:122)

(cid:125)

(cid:124)

1

N |Sada,1|T 2 . (b) = Op

(cid:125)

1
√N |Sada,1|

(cid:19)

(cid:18)

1

ada,1

|S

2T 4
|

(i,t)(cid:54)=(j,s),j∈Sada,1

(cid:26) (cid:88)

itε2
ζ 2

it(ε2

js −

ε )2
σ2

+

(i,t)(cid:54)=(j,s),(k,u)(cid:54)=(j,s),
(cid:88)
(i,t)(cid:54)=(k,u),j∈Sada,1

ζitεitζkuεku(ε2

js −

ε )2
σ2

O(N |Sada,1|T 2) terms

(cid:124)

(cid:123)(cid:122)

O(N 2|Sada,1|T 3) terms,

(cid:125)

mean of each term is Op(
(cid:123)(cid:122)
(cid:124)

N 3/2 ) from Lemma F.1
(cid:125)

1

=

(i,t),(k,u),(j,s),(l,v)
(cid:88)
diﬀers from each other,
j,l∈Sada,1

ζitεitζkuεku(ε2

js −

ε )(ε2
σ2

lv −

σ2
ε )

(cid:27)

(cid:124)
=Op

=Op

O(N 2|Sada,1|2T 4) terms, mean of each term is 0
N
N 2

N 2
N 2

(cid:123)(cid:122)
+ Op

ada,1

ada,1

ada,1

ada,1

T 2
|
2T 4
|

(cid:19)

|S
|S

(cid:18)

(cid:18)

T 2
|
2T 4
|

Op

1
(cid:125)
N 3/2

(cid:18)

(cid:19)

+ Op

1

N

(cid:18)

|S

ada,1

T 2

|

(cid:19)

(cid:19)

|S
|S
1

N

(cid:18)

|S

ada,1

| (cid:19)

72

Therefore, the sum of (a) and (b) is at the order of Op

The second term is

1
√N |Sada,1|

.

(cid:19)

(cid:18)

1
N T

(cid:32)

(cid:33) (cid:32)

|S

ada,1

1
T (T
|

1)

−

i,t(cid:54)=s
(cid:88)

εitεis

=

N

(cid:33)

1
T 2(T
|

ada,1

|S

1)

−

i,j,t,u(cid:54)=s
(cid:88)

ζitεitεjuεjs

ζitεit

i,t
(cid:88)

1

=Op

(cid:32)

N

ada,1

|S

(cid:33)

|

(cid:112)
because the mean of each term is 0.

Summing up two terms, the asymptotic covariance between ˆτall and ˆσ2

Sada,1

is at the order of

. Given the convergence rate of ˆτ and ˆσ2 are √N T and

, the sum of (a) and (b) is

ada,1

|

|S

Op

1
√N |Sada,1|

(cid:18)
negligible.

(cid:19)

(cid:112)
Sada,2 is at the order of Op

1
√N |Sada,2|

(cid:19)

(cid:18)

that is

Similarly, the asymptotic variance between ˆτall and ˆσ2

negligible.

We then ﬁnish the proof of Theorem 4.1.

