1
2
0
2

y
a
M
2

]

R
C
.
s
c
[

1
v
3
7
4
0
0
.
5
0
1
2
:
v
i
X
r
a

Analysis of Machine Learning Approaches to Packing Detection

Charles-Henry Bertrand Van Ouytsela,∗, Thomas Given-Wilsona,, Jeremy Minet, Julian Roussieau,
Axel Legaya,

aINGI, ICTEAM, Universite Catholique de Louvain, Place Sainte Barbe 2, LG05.02,01, 1348 Louvain-La-Neuve,
Belgium

Abstract

Packing is an obfuscation technique widely used by malware to hide the content and behavior of
a program. Much prior research has explored how to detect whether a program is packed. This
research includes a broad variety of approaches such as entropy analysis, syntactic signatures and
more recently machine learning classiﬁers using various features. However, no robust results have
indicated which algorithms perform best, or which features are most signiﬁcant. This is complicated
by considering how to evaluate the results since accuracy, cost, generalization capabilities, and
other measures are all reasonable. This work explores eleven diﬀerent machine learning approaches
using 119 features to understand: which features are most signiﬁcant for packing detection; which
algorithms oﬀer the best performance; and which algorithms are most economical.

Keywords: Malware, Machine Learning, Packing, Features analysis

1. Introduction

Malware detection represents a signiﬁcant and expensive problem for current computer security.
Since the vast majority of malware detection programs are based on signatures, this means that
they are reactive in nature and so the malware writers are typically one step ahead.Further, new
malware are constantly being created, the AV-Test Institute [4] registers an average 350,000 new
malware (malicious programs) every day.

Malware analysis techniques used to identify, understand, and detect malware are typically
divided into two approaches: static analysis and dynamic analysis. Static analysis operates by
examining the sample program without executing the program. This can range from very simple
analysis of strings or bytes or the code, to complex disassembly and reconstruction of key program
behaviours and features. Most malware detection programs use static analysis on simple features
such as strings. Dynamic analysis operates by executing the sample program in a protected envi-
ronment, e.g. a sandbox, and observing the program’s behavior such as recording API calls, network
activity, process creation, etc. While dynamic analysis allows more in-depth inspection, the cost
of starting and running a sandbox is signiﬁcant. Most detection engines cannot aﬀord to perform
dynamic analysis, and so dynamic analysis is usually only done for signature creation or by malware
analysis teams to understand new samples.

∗Corresponding author
Email address: charles-henry.bertrand@uclouvain.be (Charles-Henry Bertrand Van Ouytsel)

Preprint submitted to Elsevier

May 2, 2021

 
 
 
 
 
 
Packing is a widely used technique strategy that is able to evade or disrupt many malware
analysis techniques. Packing operates using various methods such as: compressing or encrypting
sections of the program; encoding the program in a virtual machine; or fragmenting program
behavior. All of these make both static and dynamic analysis much more diﬃcult, since they
obfuscate the static features and program behavior, and also make the dynamic execution behavior
more complex and harder to observe. Packing techniques are typically implemented with a small
stub section of the program that performs the initial decompression/decryption or runs the virtual
machine and thus allows the program to execute its behavior. While packing is used by benign
programs (typically for compression or to protect intellectual property) WildList [8] states that
92% of packed programs hide harmful behavior.

Figure 1: Life-cycle of a packed program

Figure 1 illustrates the life-cycle of a typical packed program. The original program is com-
pressed and the compressed program is stored in the “Packed section”. Another section is also
added containing the decompression stub routine. When the program executed, the decompression
stub decompresses the packed section and then executes the decompressed program. .

To create an eﬀective malware detection or classiﬁcation engine, it is critical to be able to detect
whether a program being analysed is packed. A packed program can be unpacked or treated with
greater suspicion to improve the analysis and response. This approach was developed by Perdisci
et al.
[32] to eﬃciently distinguish if a program is packed and then deliver the packed programs
to a universal unpacker (a program to reverse the packing). Combined with a fast and eﬃcient
static malware detector such as the one proposed by Baldangombo et al. [13], a complete malware
detection engine can oﬀer high-end performance malware detection and classiﬁcation.

Packing detection and classiﬁcation have been widely studied in the literature [27, 27, 22, 24, 34,
37, 39, 38, 32, 22, 24, 37, 38, 34, 41, 19, 10, 14, 18]. These works consider packing detection through a
wide variety of approaches to detection and classiﬁcation such as using rules, heuristics, or machine
learning. There is also a wide variety of potential static (and dynamic) features considered for
which may be most signiﬁcant for accurate (and eﬃcient) packer detection and classiﬁcation. Static
features are preferred as the basis for detection and classiﬁcation since they can be easily extracted
with low cost, and thus are widely used as the basis for popular antivirus such as ClamAV [1] or
tools such as PEiD [5].

This work addresses the challenges with the current state-of-the-art on packing detection that
various ML approaches on various features appear eﬀective, but there is a lack of clarity on which
ML techniques and features are the most eﬀective. To address this challenge, this work explores
11 ML algorithms over a variety of data sets with various forms of ground truth, samples, and

2

to measure diﬀerent kinds of performance.
In particular this works considers 11 diﬀerent ML
algorithms that have been used for packing detection. The ground truth generation for data sets is
performed using 5 diﬀerent labeling approaches and a majority vote among them [18]. The features
are also processed in diﬀerent forms since data processing is a signiﬁcant factor for the eﬀectiveness
of ML algorithms. The 11 ML algorithms are then each tuned according to their own (hyper)
parameters to gather information on which parameters are most signiﬁcant to each algorithm. The
11 tuned ML algorithms are then used to explore which features of 119 static features are the
most signiﬁcant for packer detection. The 11 ML algorithms are then applied to larger data sets
to explore their eﬀectiveness considering diﬀerent metrics. Their resilience against diﬀerent known
packers is also assessed by packing malware ourself. Finally, an economical analysis (how accuracy
degrades over time and on new samples) is considered to explore the eﬀectiveness of the 11 ML
algorithms over time and as an tool.

The challenges explored in this work can be focused into the following research questions:

RQ1 Which features are most signiﬁcant for packing detection? We explore relative rele-

vance of features present in literature into the classiﬁcation decision process.

RQ2 Which ML classiﬁers are eﬀective for packing detection? We study performances of
our classiﬁers over large dataset and against speciﬁc packers (present or not in the dataset)
to discover their strengths and weakness.

RQ3 Which ML algorithms perform well in long term for packing detection? We train
our classiﬁers on a huge dataset and evaluate them over diﬀerent samples recorded later.
Observing evolution of their eﬀectiveness reveals us how accuracies of classiﬁers change over
time and which classiﬁers are more economical in term of training.

1.1. Related Work

This section brieﬂy overviews popular approaches to packing detection.
Syntactic signatures are a popular approach to packer detection that can be extremely cheap
to compute and still achieve high accuracy for known packers. Signatures such as those used by
YARA [40] typically consider simple static features such as byte sequences, strings, and ﬁle features
such as size or hash. Although such signatures are eﬀective for exactly matching known samples,
they perform less eﬀectively against even minor permutations and can be easily evaded [19].

Entropy metrics have been shown eﬀective in detecting packing since packed ﬁles often have much
higher entropy than unpacked ﬁles. Such approaches were considered in [27, 22, 24, 34, 37, 39, 38].
Lyda et al. [27] were ﬁrst to address the problem of packing detection using entropy in an academic
publication. Their approach was focused on global entropy (computed over the whole program)
used as a proxy to detect packing. Their dataset was cleanware for non-packed samples while packed
samples where obtained by employing well-known packers on each cleanware sample. Although not
veriﬁed experimentally, their approach was calculated to target a 3.8% false positive rate and 0.5%
false negative rate. The concept was then improved by evaluating entropy for each section of the
sample in the paper of Han and Lee [23]. They use 200 cleanware for non-packed samples and
200 malware from a honeypot for packed samples, obtaining a detection rate of 97.5% with false
positive of 2.5%. However, methods focusing exclusively on entropy are vulnerable to adversaries.
For example, inserting selected set of bytes in an executable in order to keep the entropy of the ﬁle
low such as in the Zeus malware family [39].

3

Machine learning (ML) techniques using supervised learning on variety of features extracted
statically have been investigated in diﬀerent works [27, 22, 24, 37, 38, 34, 41, 10, 14, 18]. Perdisci
et al. investigated diﬀerent ML methods in packing detection : Naive Bayes Classiﬁer, decision
tree, ensemble of decision trees, k-nearest neighbors and Multi-layer perceptrons. They consider 9
features (also considered in this work) related to section names and properties, import access table
(IAT) entries and several entropy metrics in [32] while using a dataset of 5,498 PE ﬁles (benign
and malicious PE). Multi-layer perceptrons performed well in their work (98.91% test accuracy)
compared to other investigated methods. That is why, multi-layer perceptron algorithm is also
investigated in our work. We will go deeper by presenting more insights on features relevance,
tuning and performing an economical analysis. Our bigger data set will additionally allow more
meaningful measure of accuracies. In [33], they add N-grams features to their work and include the
packing detection process into a workﬂow to analyse and detect malwares.

Diverse new features were proposed by malware researchers, for instance in [12, 35, 38] like
structural PE attributes, heuristic values, entropy-based metrics or diﬀerent ratio (raw data per
virtual size ratio or the ratio of the sections with virtual size higher than raw data for instance).
Although more expensive to extract, some dynamic features have also been studied in the literature,
such as using evolution of the import address table (IAT) table during the unpacking process [20]
or basing analysis on ‘write-then-execute’ instructions typical in packed samples [21, 25].

Recent work [18] has surveyed the literature, explored the feasibility of using machine learning
techniques for packing detection and produced a list of 119 commonly used features (the same used
in this work). Most of this work focus on deﬁning the methodology and on developing eﬃcient
techniques to extract interesting features statically. In their paper, Biondi et al. divide them into
six categories: Byte entropy, Entry bytes, Import functions, Metadata, Resource and Sections.
Moreover, they focus on impact of each categories. Our work investigates this set of 119 features
(See Appendix A) by considering all of them as independent. Their paper also discuss few machine
learning algorithms based on those features to show the feasibility of the approach (i.e. Bayes clas-
siﬁer, Decision trees and Random Forest).They studied these algorithms over a dataset of 280,000
samples in terms of eﬀectiveness, robustness and eﬃciency (related to computational cost). They
conclude their study by showing that accepting a small decrease in eﬀectiveness can increase ef-
ﬁciency by many orders of magnitude. We expand their set of studied ML classiﬁers and apply
similar analysis on their performance.

The problem of packing detection has also been extended to packing classiﬁcation [36]. Diﬀerent
classiﬁer have been compared in this perspective using pattern recognition techniques on random-
ness proﬁles of packers. In [15, 17, 24], entropy patterns extracted dynamically (i.e., by executing
the binary) are considered as an eﬃcient way to classify packers. Knowing the ﬁle is packed, the
entropy of the whole ﬁle is recorded after each JMP instruction until entropy ceases to change.
In [16], the authors use similar techniques to determine if multiple packers have been used on the
same program. The problem of known packer classiﬁcation has been demonstrated as a trivial task
in [9]. Using samples from each packer (class), their classiﬁer achieved precision and recall greater
than 99.99% for each class. However, the authors have highlighted challenges related to problems
of packing detection.

Unsupervised learning has recently also been investigated [30] to build clusters related to packers
and identify variants of known packers or new packers. However, this is beyond the scope of this
work, as the focus here is on exploring supervised learning algorithms.

The structure of the paper is presented hereafter. Section 2 recalls background on the machine
learning algorithms used in this work. Section 3 presents the construction of datasets used in this

4

work, the methodology applied to answer our research questions and conﬁgurations used for our
ML algorithms (i.e.: pre-processing and hyper-parameter tuning). Section 4 explores the relevance
of diﬀerent feature in the decision process of classiﬁers, including feature selection methods and
principal component analysis. Section 5 evaluates the eﬀectiveness of the ML algorithms on a large
data set. Section 6 explores the economics of the trained packing detectors over a chronological
data set. Section 7 discusses the validity of the results and some observations on dynamic features.
Section 8 concludes.

2. Background on Machine Learning algorithm

This section recalls useful background information about eleven machine learning algorithm

used in this paper.

2.1. K-Nearest Neighbors ( KNN)

The K-Nearest Neighbors (KNN) ML algorithm considers each input data as a point in a feature
space with a particular label. When given a new data point to classify KNN will ﬁnd the K nearest
points in the feature space in terms of a deﬁned distance (e.g. Euclidean distance) and classify the
new data point as a plurality vote of its neighbors within the deﬁned distance. In the case of k = 1,
the input is simply assigned the label of its nearest neighbor.

Figure 2: Binary classiﬁcation involving two features (X1 and X2) and using a 5-Nearest Neighbors classiﬁer

An example is shown in Figure 2 with two labels (red and blue) where the new data point
(orange) will be assigned the color red according to the majority vote. This classiﬁcation algorithm
requires complete information about the whole training data set to classify each new data point
and thus requires signiﬁcant memory use in practice for larger data sets.

2.2. Naive Bayes Classiﬁers ( GNBC & BNBC)

Naive Bayes classiﬁers (NBC) are probabilistic models based on Bayes theorem:

Pr(Ck|xi) =

Pr(xi|Ck) Pr(Ck)
Pr(xi|Ck) Pr(Ck) + Pr(xi|¬Ck) Pr(¬Ck)

(1)

5

where Ck is a class in the set of classes k ∈ {1, .., K} and xi is a feature in the set of feature vectors
i ∈ {1, .., n} and Pr(a) is the probability of a (where a could be a class label or feature vector)
and Pr(a|b) is the probability of a given b (where a is a class label and b is a feature vector or
vice versa). These classiﬁcation use Equation 1 to evaluate probability to observe label Ck for a
sample according to value of feature vector xj. These estimated probabilities are then combined to
construct the classiﬁer as in Equation 2 where ˆy is the estimated label of the sample:

ˆy = arg max
k∈{1,..,K}

p(Ck)

n
(cid:89)

i=1

p(xi|Ck)

(2)

The naive adjective of the classiﬁer comes from the assumption that each data point’s feature is
independent from each other feature. Although this assumption does not usually hold in general
NBC is known to work well in practice.

Two variant of the NBC are investigated in this paper: Gaussian (GNBC) based on normal
distribution and Bernouilli (BNBC) where features are considered as independent booleans. Al-
though NBC are among the simplest ML algorithms, they oﬀer good accuracy in some areas and
have low performance costs.

2.2.1. Linear Models ( LR & LSVM)

In linear models (Lin) ML algorithms a model is built from training data as a linear combination

of features. The model has the form given in Equation 3 below:

y = x[1] · w[1] + x[2] · w[2] + ... + x[p] · w[p] + b

(3)

where y is the prediction calculated from x[i] is each feature indexed by i in the feature vector x
and w[i] is the learned weight for each i in the length of the feature vector and b is the oﬀset.

Since our focus is binary classiﬁcation, the predicted value is set to a threshold at zero. Thus, a
y value greater than zero corresponds to label 0 while a value less than zero corresponds to label 1.
Therefore, the decision boundary can be represented as a line or a plane that separates the data in
two classes. Since Lin are a very large family of algorithms this paper considers two forms of Lin:
logistic regression and linear support vector machines.

Figure 3: Logistic regression [3]

Figure 4: Linear support vector machine.

6

Logistic regression (LR) is based on the logistic function:

σ(z) =

1
1 + e−z .

(4)

that is also illustrated by the red curve in Figure 3. To obtain optimal weights w for LR, maximum
likelihood estimation is used. This involves ﬁnding weights w[i] that maximize the log-likelihood
function deﬁned as follows:

L(w) =

n
(cid:89)

i=1

σ(wT x(i))yi · [1 − σ(wT x(i))]1−yi .

A Linear support vector machine (LSVM) is based on parallel hyperplanes that separate the
classes of data, so that the distance (or margin) between them is maximised. Samples used to
construct the two planes are called support vectors. The equation used to deﬁne these is as follows:

wT x − b = 1

and wT x − b = −1 .

(5)

Figure 4 shows the two planes (here two lines), support vectors used to build the planes, and the
margin width that we try to maximize here.

2.3. Tree-based Models ( DT, GBDT, RF, & DL8.5)

Figure 5: Example of a simple decision tree

Decision trees (DT) are classiﬁcation models based on the sequential application of simple binary
rules. They are structured as a directed tree as illustrated in Figure 5. Starting at the root (in
green), the tree is traversed towards a leaf (containing a label) by selecting an edge at each node
according to a simple rule (e.g. in Figure 5, the rule at the root is “if X1 of the sample is smaller
than 1 go to right branch, else go to left branch”). In the building of a decision tree, all features are
considered. Diﬀerent combinations of splits are considered and the best one is selected according
to a cost function (such as gini impurity or entropy). The choice of eﬀective split being of ﬁrst
importance in term of tree size and cost, although features relevance and features selection is an
important concern which can assist with pruning or depth limitation.

Simple DT models can be combined to construct more complex classiﬁers solving variance and
bias that single DT can suﬀer from. The concept is to combine many weak learners to shape a
stronger one at the cost of more resource consumption. Two of these classiﬁers, known to have

7

Figure 6: Decision boundary found by Lin [28].

Figure 7: Decision boundary found by KSVM [28].

demonstrated good performance on a diversity of data sets are considered in this work: Random
Forests and Gradient Boosted Decision Trees.

Random Forests (RF) represent the combination of slightly diﬀerent DT. While a single DT
may be very eﬀective there is a risk be overﬁtting the training data. RF therefore address this risk
by considering outcomes from several DT, resulting in a reduced overﬁtting while maintaining the
predictive power of each tree. The random nature of this algorithm resides in the creation of the
DT, which are built upon a randomly generated set of samples (this process is called bagging). Each
tree is then trained individually and their outputs are aggregated with each other in a majority
vote.

Gradient Boosted Decision Trees (GBDT) is another approach to address overﬁtting that works
by using a technique called boosting (while RF uses bagging). In this process, each tree is con-
structed sequentially with each subsequent tree trying to correct the errors of the previous trees.
Typically GBDT start with extremely shallow trees, often called weak learners, which provide
good prediction only on some parts of the data. Increasingly trees are then grown iteratively to in-
crease the global performance by reducing the loss margin. In comparison to RF, GBDT produces
shallower trees. Hence, the model is more economical in terms of memory but harder to tune.

In addition to the traditional approach, a recent version of DT is also considered in this work,
namely DL8.5 [29], working only with binary data (implementation available at [6]). DL8.5 diﬀers
from classical approaches by using a cache of item sets combined with branch-and-bound search in
order to create less complex trees and avoid unnecessary computations.

2.4. Kernelized Support Vector Machines ( KSVM)

Kernelized Support Vector Machines (KSVM) are an extension of Lin which allows separation
of data not linearly separable by using hyper-planes. In KSVM low dimensional data are converted
to higher dimensional data using a kernal function (e.g. polynomial, Gaussian, Sigmoid, etc.) and
the planes used to separate the data are now higher dimensional hyperplanes. For example, on
Figure 6 is shown how Lin would try to ﬁnd planes to classify the red and blue data points. On
Figure 7 the data points are converted to a higher dimension and a higher dimensional hyperplane
is used to separate the points, leading to a much more eﬀective separation.

Although KSVM work well on a variety of datasets, they are known to suﬀer from performance

degradation when scaled to a large number of samples.

8

2.5. Neural Networks ( MLP)

Neural Networks are mainly used in deep learning and have been employed in thousands of diﬀer-
ent classiﬁers for speciﬁc purposes. This work uses the most popular variation of NN: Multi-Layer
Perceptrons (MLP) algorithm. MLP can be viewed as a generalization of Lin that is constituted
with various layers of neurons. Each neuron is a unit taking its input applying a weighted sum
of its features and passing it through a non-linear function before forwarding it to the subsequent
layer as illustrated in Figure 8.

Figure 8: Multi-Layer Perceptron with two hidden layers.

Each neuron of each layer with input features X will apply the following equation to calculate

its output y:

y = f (x[0] · w[0] + w[1] · x[1] + ... + w[p] · x[p] + b)

(6)

where f is a non-linear function (e.g. logistic function like Equation 4 or hyperbolic tangent) ap-
plied to a weighted sum on x features. With MLP, this process of weighted sums is repeated
multiple times. Data are supplied to the input layer, then there may be one or more hidden lay-
ers representing intermediate processing steps, and eventually predictions are made on the output
layer.

The main advantage of using NN is that they have demonstrated their eﬃciency to deal with
large data sets and come up with incredibly complex models at the cost of training time, wise
parameter tuning and accurate pre-processing.

3. Experimental Setup

This section details the experimental setup used in this work. This is divided into three areas:
construction of data sets; experimental methodologies; and pre-processing and hyper-parameter
tuning of the ML algorithms.

3.1. Data Sets and Labeling Tools

All data sets are built from a feed of malware provided by Cisco that oﬀers 1,000 new (assumed)
malware samples per day [7]. The construction of data sets begins by taking samples of malware
from the feed that are collected over a given time period. For each sample, 119 static features are

9

(attempted to be) extracted1. These features are a collection of all those used in other works and
identiﬁed in the work of Biondi et al. [18].

Once gathered, these samples need to be labeled as packed or not packed to obtain a ground
truth for training ML classiﬁers.
In this work a similar approach to the one of VirusTotal was
taken; to employ multiple detector engines for classiﬁcation and then build on these results. There
were ﬁve classiﬁcation engines used, described below.

• PEframe [11] This is an open source tool written in Python and freely available to perform
static analysis of portable executable (PE) ﬁles, object code, DLLs and others. Its utilization
will be restricted to packer detection but the tool can also be used to obtain other digital
forensics information such as macros, anti-forensics techniques, etc.

• Manalyze [26] This is a static analyzer mainly used by large enterprises and composed of
several plugins.
Its packer detection plugin is based on signatures and custom heuristics.
Signatures are generally related to names of PE sections (e.g. UPX renames some of the
section after packing as UPX0, UPX1, etc.) while heuristics are more related to anomalies
in the PE structure (unusual section names, sections both writable and executable, etc.) or
entropy-based features.

• PEiD [5] This is a widely known detector, freely available and signature based. Its signatures
only contains low-level byte patterns which can match either at the entry point or more
generally anywhere within a PE ﬁle.

• DIE Detect-it-Easy [2] is an open source architecture of signatures written in javascript

which allows more complex and ﬁne tuned signatures.

• Cisco This was an in-house engine that Cisco used on some samples. This is not publicly

available and was only used for some of experiments due to limited availability.

A sample is considered as packed if a majority of detectors label the sample as packed, in this
case three out of ﬁve detectors. When the Cisco detector was unavailable, a threshold of two out
of four was used.

A ﬁrst data set Tune of 43,092 samples is used for parameters tuning and ﬁnding optimal
representation of the dataset (by feature selection and Boolean conversion). This data set consists
of samples collected from 2019-06-15 to 2019-07-28 and according to the labeling has 10,009 packed
samples and 33,083 not packed samples. Labels for all ﬁve label engines were available for this data
set.

A second much larger data set Big of 95,876 samples is used for the broader experimental
results in this work. This data set consists of samples collected from 2019-10-01 to 2020-02-28
and according to the labeling has 23,894 packed samples and 71,982 not packed samples. Since
labels from the Cisco engine were exclusively available for the samples in Tune, this data set was
generated to carry out larger experiments on the machine learning and features, but had reduced
label information. An extended data set denoted Big+f is the data set Big with 500 samples taken

1Occasional errors in the extraction software caused a few samples to be discarded, although this is an insigniﬁcant

number and so ignored for the rest of this work.

10

at random from Tune and packed with the packer f added to Big. The label information is the
same for samples originally from Big, the extended samples have known packer.

Speciﬁc data sets SelfPack f of approximately 12,036 samples are samples collected from

2020-06-15 to 2020-06-30 and packed ourself with known packer f .

A third data set Chrono of 37,794 samples is used for the economical analysis results in this
work. This data set consists of samples collected from 2020-04-01 to 2020-05-26 and according
to the labeling has 8745 packed samples and 29,049 not packed samples. This data set has also
reduced label information as Big.

3.2. Methodology

To address the research questions eﬀectively it is necessary to properly pre-process the data sets
and tune the ML algorithms appropriately. This is detailed in Section 3.3, however an overview
is as follows. The pre-processing includes Boolean conversion of non-Boolean data, bucketing, and
normalization of feature values. Once the data sets have been pre-processed, the hyper-parameters
of all the ML algorithms are tuned using Tune to ﬁnd the best conﬁgurations for later use.

The methodology used to address the various research questions in this paper is split into several

parts based upon the research question being considered.

• RQ1 is addressed by exploring which features are the more signiﬁcant in diﬀerent ML al-
gorithms. Due to there being many diﬀerent ML algorithms several approaches are used for
reducing the number of features including: feature coeﬃcients, K-best features, and principle
components analysis. These approaches are applied to all 11 ML algorithms using Tune and
experimentally evaluated with 10-fold cross-validation and 90% of the data for training and
10% for testing. Details on the approach and results are presented in Section 4.

• RQ2 is addressed by taking all the ML algorithms along with all the lessons learned on
data processing, ML tuning, and feature selection to evaluate their overall eﬀectiveness on
the larger data set Big. The analysis is performed by using 10-fold cross-validation on Big
and taking 90% of the data for training and 10% for testing. Further, the adaptability of
the 11 ML algorithms is evaluated by comparing the accuracy when the data set is extended
with more samples of a speciﬁc packer. This evaluates the adaptability of the algorithms as
their training data changes, and to evolutions in packer families. Details of the results and
experiments are presented in Section 5.

• RQ3 is addressed by training the ML algorithms (developed and tuned as above) on Big
and then evaluating them on Chrono. Observe that Chrono is chronologically after Big
and thus indicates a representation of the evolution of malware over time (there is a time gap
of a month between last malware of Big and ﬁrst malware of Chrono to focus on packed
samples which would be release later than the training set). To further assess the economic
and chronological performance of the ML algorithms, the testing on Chrono is considered
over smaller chronological periods (four periods are considered, each one consisting of two
weeks). This evaluates the eﬀectiveness of the various ML algorithms over time and allows
to calculate the eﬀective cost of retraining to maintain a desired level of accuracy. Note that
since the experiments here consider using data sets from various times, the whole early data
set is used for training and not cross-validation is applied. Details of the approach and results
are presented in Section 6.

11

All the ML algorithm implementations used are from the scikit-learn library [31] version 0.23.2.
except the DL8.5 algorithm [29] where the implementation on github is used (that implement the
same functions as the scikit interface). All experiments here are performed on a desktop PC with
an Intel Core i7-8665U CPU (1.90GHz x 8) and 16GB RAM running Ubuntu 18.04.5.

3.3. Pre-processing and Hyper-Parameter Tuning

This section overviews the data pre-processing and ML algorithm hyper-parameter tuning used
in the later experiments. Pre-processing includes the consideration and application of Boolean
conversion, standardization and normalization of the data. For each algorithm, a grid search is
applied to ﬁnd the best combination of pre-processing and hyper parameters.

3.3.1. Data Pre-processing

• 42% have value 1024 (in blue).

• 34% have value 4096 (in orange).

• 16% have value 512 (in green).

• 7% have value 1536 (in red).

• 1% have other values (in yellow).

Figure 9: Distribution of values for feature 16 (total header size in bytes).

Of the 119 static features considered for each sample, only 16 of them are Boolean values. Some
of the ML algorithms such as DL8.5 requires Boolean features (for details see Section 2). Therefore
it is necessary to convert non-Boolean values to Boolean values for use in these algorithms.

This conversion begins by observing the value distribution of Tune features. For example,
Figure 9 shows the distribution of feature 16; namely header size in bytes. Bucketing is then
applied which gathers similar values into buckets and hence reduces the range of possible values.
For example: value 1 is mapped to 1024, 2 is mapped to 4096, 3 is mapped to 512, 4 is mapped
to 1536 and 0 is mapped to other possible values. Then one-hot encoding is used to convert these
bucket values into Boolean values. Therefore in the case of the feature 16, ﬁve Boolean features will
be used in the new dataset to represent header size. This method induces an important increase
in term of the number of features and it is therefore important to eﬃciently limit the number of
buckets for each feature. Details on the results of this pre-processing via bucketing and conversion
to Boolean values is given in Appendix B.

Initially Boolean conversion is used only in the BNBC and DL8.5 ML algorithms which requires
Boolean data. However, since other algorithms could beneﬁt from this conversion, Boolean features
conversion is also tested on all the ML algorithms to observe the impact. Final choice for each
algorithm is investigated in Section 3.3.2.

12

To work most eﬃciently, some ML algorithms require their data to be normalized (values ∈ [0; 1])
or standardized (considered as a Gaussian distribution ∈ [−1; 1]). For example, GNBC requires
normalization to produce an eﬃcient and coherent model. Like for the Boolean conversion, although
many ML algorithms do not require normalization or standardization, they may be able to beneﬁt
from this data pre-processing. This is investigated below.

3.3.2. Hyper-Parameters Tuning

Each of the 11 ML algorithms has their own hyper-parameters that can be tuned to improve
their performance on diﬀerent kinds of data sets and classiﬁcation problems. In this section a grid
search is applied to each ML algorithm to obtain the best combination of pre-processing and hyper-
parameters. The best combination is chosen by accuracy, with equal accuracy outcomes decided by
training time. Each combination has been tested with 10-fold cross-validation on Tune (90% for
training and 10% for testing).

Together with the three types of pre-processing (Boolean conversion, normalization, and stan-

dardization) the hyper parameters investigated for each ML algorithm are listed below.

• KNN: Number of neighbors [1; 30].

• LR and LSVM: Loss function (hinge loss or squared hinge loss).

• DT: Criterion to measure quality of split (entropy or Gini impurity), minimal number of

samples required for a leaf [2; 12], and the maximal depth of the tree [1; 12].

• DL8.5: Maximal depth of the tree [1; 10] (limited for important training time purpose).

• RF and GBDT: Number of estimators [2; 150], best parameters of DT are also used for

individual tree.

• MLP: Architecture of the network (i.e. [1; 3] hidden layers and 25, 50, 100 neurons), activation
functions (identity, hyperbolic tangent, logistic function, rectiﬁed) and solver (adam, sgd or
lbfgs).

• KSVM: Kernel used (linear, polynomial, rbf or sigmoid).

Note that NBC and BNBC do not have hyper-parameters of interest and are only tested for
appropriate data pre-processing.

Figure 10 shows a summary of diﬀerent trained classiﬁers with their best accuracy (according
to all the possible pre-processing and hyper-paramater tuning) on Tune. This provides a baseline
for later comparison as well as an overview of the potential accuracy achievable for each of the 11
ML trained classiﬁers. The best choices of data pre-processing and hyper-parameter selection for
each ML algorithm as listed below.

• KNN: Boolean conversion, 16 neighbors.

• BNBC: Boolean conversion.

• GNBC: Standardization.

• LR: Standardization and square hinge for loss function

• LSVM: Boolean conversion and square hinge for loss function

13

Figure 10: Accuracy and training time for all 11 ML trained classiﬁers with optimal pre-processing and hyper
parameters. 10-fold cross validation has been used with 90%-training and 10%-testing using the Tune data set.

• DT: No pre-processing, entropy as criterion to measure quality of split, 10 as minimal number

of samples required for a leaf and 6 as the maximal depth of the tree.

• DL8.5: Boolean conversion, maximal depth of 10.

• RF and GBDT: No pre-processing, 20 trees as number of estimators.

• MLP: Boolean conversion, 50 neurons in the ﬁrst layer and 100 in the second layer as archi-

tecture. Stochastic descent and logistic function as solver and activation function.

• KSVM: Standardization, Gaussian radial basis functions (rbf) as kernel.

The above pre-processing and hyper-parameter conﬁgurations are used for all later experiments in
this paper.

4. Features Analysis

This section addresses RQ1 (Which features are most signiﬁcant for packing detection?) by
considering the relevance and signiﬁcance of the 119 diﬀerent features explored in the literature. In
Biondi et al. [18] these features are grouped into six categories: Metadata, Section, Entropy, Entry
bytes, import functions and Resource features. See Appendix A for the full list of features and
their categories). The relevance and signiﬁcance of the features is considered using three diﬀerent
approaches: analyzing the weights and choices from the classiﬁers produced with the ML algorithms
(LR, LSVM, DT, GBDT, & RF); using iterative K-best feature selection; and principal component
analysis. The accuracy of the classiﬁer is used to measure the relevance and signiﬁcance of features,
and all analysis in this section is performed on Tune using 10-fold cross-validation with 90% of
Tune used for training and 10% for testing.

14

Figure 11: Coeﬃcient magnitudes for the LR classiﬁer.
10 most important features by order of relevance are:
39, 116, 19, 30, 12, 29, 35, 22, 60, & 36.

Figure 12: Coeﬃcient magnitudes for the LSVM classi-
ﬁer. 10 most important features by order of relevance
are: 39, 116, 19, 12, 29, 35, 30, 36, 60, & 56.

Figure 13: Feature importance for the DT classiﬁer. 10
most important features by order of relevance are: 35,
43, 60, 87, 116, 50, 28, 22, 32, 85, & 74.

Figure 14: Feature importance for the GBDT classiﬁer.
10 most important features by order of relevance are:
37, 116, 29, 60, 11, 115, 26, 40, 50, 39, & 119.

15

Figure 15: Feature importance for the RF classiﬁer. 10 most important features by order of relevance are: 116, 37,
48, 23, 29, 43, 40, 39, 27, 70, & 117.

4.1. Overview of Feature Relevance

One approach to understanding which features are most relevant and signiﬁcant is to examine
the classiﬁers produced by diﬀerent ML algorithms. For LR, and LSVM trained classiﬁers the
coeﬃcients magnitude is one way to examine which features are most signiﬁcant. For DT, RF, and
GBDT the position in the decision tree(s) also implies relevance and signiﬁcance of features and
provides another insight. Observe that since these two diﬀerent groups of ML algorithms (Lin-based
Lin, LR, & LSVM, and DT-based DT, RF, & GBDT) operate in diﬀerent ways, they provide
diﬀerent kinds of insights.

The following features appear relevant and signiﬁcant to ML classiﬁers based on this analysis
approach (presented in feature index order). Note that an overview of feature signiﬁcance for each
of the classiﬁers considered here is presented in Figures 11 to 15.

• Feature 19 corresponds to the size of the stack to reserve. Since unpacking implies in-memory
operations and execution, this feature appears to be intrinsically linked to packing behavior.
This feature is mainly signiﬁcant in Lin-based models.

• Feature 29 corresponds to the number of readable and writable sections the PE holds. Typ-
ically, a packer will unpack its code in speciﬁc sections which means it will re-write some of
its previously encrypted sections. Therefore it is expected that this feature is in the top ten
used features of LR, LSVM, RF and GBDT.

• Feature 35 corresponds to the entry point not being in a standard section. This can could
happen when the entry point which corresponds to the unpacking stub is in a dedicated
section. This feature is in the top ten features of LR, LSVM and DT.

• Feature 37 corresponds to the ratio between raw data and virtual size for the section of the
entry point. A huge ratio can be link to a section that will be modiﬁed during execution,

16

Classiﬁer
LR

Selection Best # features Old accuracy New Accuracy Old time New time Ratio
(-)446
Iterative
LSVM Iterative
(-)152
470
K best
961.68
Iterative
6283
K best

4.385
281.243
0.1797
0.6423
9.6522

3.462
128.804
0.0218
0.2843
2.6470

0.8480
0.8470
0.8556
0.8622
0.8656

0.8476
0.8440
0.8572
0.8627
0.8657

DT
RF
GBDT

103
84
16
24
28

Table 1: Best results for feature selection based on ratio value.

in the case of the entry point section this is quite suspicious. This feature is one of the top
features of RF and GBDT.

• Feature 39 corresponds to the number of sections having their virtual size greater than their
raw data size. A packed executable will typically create this change in section size due to
modifying data inside a section. This is a top ten features of LR, LSVM, RF and GBDT.

• Feature 43 corresponds to the byte entropy of code (.text) sections. Since packing implies
generally compression/encryption, entropies of code sections is generally high. This explains
why this feature is often used in literature [27, 22, 24, 34, 38].

• Feature 60 represent the 12th-byte value following the entry point. The signiﬁcance of this
byte speciﬁcally is interesting to observe and is in the decision process for LR, LSVM and
DT.

• Features 115-116 are related to the number of API calls imported by the binary and often
used in malicious software according to [41]. Feature 115 corresponds directly to the number
of API calls imported while Feature 116 is related to the ratio between malicious API calls
imported and the total API calls imported. These API calls are not intrinsically malicious
but allow easily alteration or obfuscation of control ﬂow of a running program. This can be by
allocating memory to store unpacked ﬁles, getting address of speciﬁc API function call after
unpacking, etc. Feature 116 is part of top ten used features of all explored methods while
Feature 115 is also utilized in GBDT.

4.2. Selecting Features

Another approach to better understand the relevance and signiﬁcance of features is to explore
how to reduce the features used by some of the ML algorithms. Here this is done in two ways: the
ﬁrst is by exploring the K-best features extracted from the initial models; and the second is by
iterative increase where (increasing iteratively) K features are chosen and the intersection of the
best features is kept over iterations.

To select the K-best features for models, the scikit implementation (namely SelectFromModel
function) was used. A threshold is used to decide if a feature is suﬃciently important to be kept,
here the threshold has been incremented progressively, and all feature combinations tested with
combinations that did not decrease accuracy of the classiﬁer by more than 5% kept. For the list of
combinations of features satisfying this requirement, an indicative time-to-accuracy ratio has been
computed which represents the ratio between the percentage of time saved and the percentage of
accuracy lost. Observe that this process of selection is only possible on estimators making direct
use of coeﬃcient or feature importance namely: LR, LSVM, DT, RF and GBDT.

17

Classiﬁer Best # components Old accuracy New accuracy Old time New time (s)

KNN
GNBC
LR
LSVM
DT
RF
GBDT
MLP
KSVM

71
14
99
101
58
11
18
57
101

0.8391
0.2458
0.8476
0.844
0.8572
0.8627
0.8657
0.8592
0.8682

0.8411
0.8086
0.8513
0.8458
0.8464
0.8589
0.8622
0.8628
0.8682

11.97
0.145
4.648
188.713
0.195
0.711
10.15
90.392
106.72

0.634462
0.0201
3.808
153.405
0.0829
0.6717
5.13846
85.882
92.990

Ratio
(-)397
(-)0.37
(-)41
(-)87.72
46
13
122.33
(-)12
1117

Table 2: Top result from applying PCA and keeping only iterations improving time and ﬁnally sorting them by
accuracy.

A summary of the feature relevance process outcomes can be seen in Table 1. These results
indicate that feature selection is in general beneﬁcial for every tested ML classiﬁer measured by the
high value of time-to-accuracy ratios For ML approaches such as LSVM which take a signiﬁcant
training time, feature selection not only reduces training time but also improves classiﬁer accu-
racy. A small improvement in performance is also observed for LR. Generally the results indicate
that Lin-based models tend to use more features than decision tree based models. This conﬁrms
observations in Section 4.1.

4.3. Principal Component Analysis

Principal component analysis (PCA) aims to reduce the dimension of a feature space by grouping
features together to constitute new components. Each new component is an independent novel linear
combination of some prior features. These are ranked by importance, the ﬁrst component trying to
capture as much information (i.e. variance of the data) as possible.

Using diﬀerent number of principal components, best sets of these new features in terms of
accuracy were selected. Table 2 summarizes best results from applying PCA over Tune before
training of our ML classiﬁers. Note that since PCA expects standardization as a part of the
process, DL8.5 and BNBC are ineligible since they require binary data.

An overview of the results of PCA can be seen in Table 2. Although computation times generally
improve in every case, three types of results could be distinguished. First, for KNN and GNBC
the number of features is substantially decreased and a signiﬁcant improvement can be observed
in accuracy. These methods being more based on distance and statistical distribution, it appears
intuitive creating principal components will beneﬁt to them.
Second, the impact of PCA on algorithms like LR, LSVM, MLP and KSVM seems less signiﬁ-
cant: the number of features didn’t decrease signiﬁcantly, and accuracy performance did not reveal
substantial improvement. These algorithms already attach importance to diﬀerent feature and
since PCA reduces dimensionality by employing linear combinations over data, these are somewhat
redundant in their eﬀect. Although this limited the improvements in accuracy, improvement in
training time can be observed for MLP and KSVM. Third, tree-based ML algorithms such as DT,
RF and GBDT dramatically decreases the number of features without impacting accuracy. Since
trees-based algorithms derive their cogency from the diversity of features to perform their split, ap-
plying PCA reduces dimensionality and creates more complex feature to split upon for tree-based
models.

18

Figure 16: Accuracy and training time for all ML classiﬁers after processing of features.

Ultimately, PCA will be kept as a step for later application of KNN, GNBC, MLP and KSVM

since improvements in accuracy or training time justiﬁes always using PCA.

4.4. Accuracy with Modiﬁed Features

Figure 16 illustrates the accuracy and training time of all ML classiﬁers after applying the best
choices from the above analysis. Feature selection is applied to LR, LSVM, DT, RF and GBDT,
and PCA is applied to KNN, GNBC, MLP and KSVM. Using feature selection/PCA reduces the
number of necessary features stored and used for training. Most ML algorithms can signiﬁcantly
reduce their number of features while maintaining their accuracy.

Regarding RQ1, we observe that a small number of features are clearly more inﬂuential than
others. These are: the stack reserve (Feature 19), number of readable and writable sections (Feature
29), non-standard entry point section (Feature 35), raw and virtual data size of sections (Features
37 & 39), byte entropy of .text section (Feature 43), 12th entry point byte (Feature 60), and
malicious API calls (Features 115-116). In Lin classiﬁers, all features have weighted contributions
to the classiﬁcation process even if some of them are signiﬁcantly lower weight than others. By
contrast, in tree-based models, classiﬁers take advantage of a small amount of features to classify
a sample. These properties of the classiﬁers and the diversity of the features and their provenance
correlates with prior results showing diﬀerent features and approaches can be eﬀective and play a
role in classiﬁcation even if many are not signiﬁcant independently.

5. Classiﬁer Eﬀectiveness

This section addresses RQ2 (Which ML classiﬁers are eﬀective for packing detection?) by
examining various metrics for classiﬁcation over a large data set. The adaptability of the classiﬁers
is also evaluated by testing the trained classiﬁers on speciﬁc packers.

19

Figure 17: Accuracy for all 11 ML trained classiﬁers after tuning tested on Big.

Classiﬁer
KNN
GNBC
BNBC
LR
LSVM
DT
RF
GBDT
DL8.5
MLP
KSVM

precisionp
0.9746
0.7882
0.8091
0.9415
0.9393
0.9119
0.9733
0.9844
0.9473
0.9729
0.9875

precisionnp
0.9955
0.9478
0.9728
0.9801
0.9809
0.9738
0.9847
0.9892
0.9451
0.9934
0.9947

recallp
0.9867
0.8487
0.9219
0.9407
0.9432
0.9225
0.9543
0.9676
0.8276
0.9805
0.9843

recallnp
0.9913
0.9234
0.9278
0.9803
0.9795
0.9700
0.9912
0.9948
0.9847
0.9908
0.9958

F -scorep
0.9806
0.8173
0.8619
0.9411
0.9412
0.9172
0.9637
0.9759
0.8834
0.9767
0.9859

F -scorenp
0.9934
0.9354
0.9498
0.9802
0.9802
0.9719
0.9880
0.9920
0.9645
0.9921
0.9952

precisionwa
0.9903
0.9077
0.9321
0.9704
0.9704
0.9586
0.9819
0.9880
0.9456
0.9882
0.9929

recallwa
0.9902
0.9046
0.9264
0.9703
0.9704
0.9581
0.9820
0.9880
0.9456
0.9882
0.9929

F -scorewa
0.9902
0.9057
0.9279
0.9703
0.9704
0.9582
0.9819
0.9880
0.9443
0.9882
0.9929

Table 3: Detailed metrics for all 11 ML trained classiﬁers after tuning tested on Big.

5.1. Evaluation of Classiﬁcation Metrics

To assess the performance of the 11 ML trained classiﬁers an analysis is performed over the
larger Big. For the results in this section 10-fold cross-validation is performed with 90% of Big
used for training and 10% for validation.

An overview of the average accuracy for each classiﬁer can be seen in Figure 17. Observe
that in all cases except for BNBC the ML classiﬁer accuracy is higher than when performing the
parameter tuning (see Section 4.4 and Figure 16 for details). These results indicate that the choice
of parameters appears reasonable and that they are not over-ﬁtted to the data set used for the
parameter tuning (Tune).

A more detailed exploration of the metrics for evaluation the diﬀerent classiﬁers is presented in

Table 3. For each classiﬁer the average of the following metrics is presented.

• The precision of a class A is deﬁned as the ratio of samples correctly labeled A over all samples

labeled A.

• The recall of a class A is deﬁned as the ratio of samples correctly classiﬁed as A over all

samples belonging to class A.

20

Classiﬁer

Big

KNN
GNBC
BNBC
LR

0.9873
0.9873
0.9924
0.9600
LSVM 0.9623
DT
0.9911
RF
0.9955
GBDT
0.9955
DL8.5
0.75
MLP
0.9848
KSVM 0.9671

UPX

kkrunchy

MPress

TElock

PEtite

Big+U P X
0.9756
0.9955
0.9822
0.9977
0.9977
0.9955
1.0
1.0
0.8824
1.0
1.0

Big

0.7682
0.5133
0.7433
0.792
0.816
0.01
0.34
0.002
0.03
0.5989
0.037

Big+kkrunchy
0.968
1.0
0.972
0.988
0.982
0.12
1.0
0.998
0.836
1.0
0.996

Big

0.9264
0.3676
0.875
0.5755
0.6337
0.9709
0.9709
0.9709
0.93
0.9166
0.897

Big+M P ress
0.9728
0.9689
0.9689
0.9844
0.982
0.9903
0.9903
1.0
0.9573
0.9883
1.0

Big

0.7606
0.3204
0.5922
0.1837
0.1739
0.426
0.1482
0.3913
0.6267
0.5415
0.6227

Big+T Elock
0.8359
0.3735
0.9486
0.4940
0.8675
0.4466
0.7035
0.91
0.1660
0.7509
0.8992

Big

0.016
0.0115
0.1598
0.0098
0.009
0.003
0.0
0.0
0.0268
0.0089
0.0089

Big+P Etite
0.9277
0.0246
0.7701
0.8998
0.8456
0.08
0.0476
1.0
0.267
0.9688
0.9852

Table 4: Accuracy for all 11 ML trained classiﬁers after tuning applied to a Big training set and Big+training set
with added samples.

• The F -score of a class A is deﬁned as 2·precisionA·recallA
precisionA+recallA

.

The subscript p denotes the packed class, the subscript np denotes non-packed class, and the
subscript wa denotes the weighted average.

Overall the precision, recall, and F -score tend to be higher for the non-packed class for almost
all ML classiﬁers. The only exception is small improvement for DL8.5 regarding precision. Globally
of the 11 ML trained classiﬁers KSVM scores the best in all metrics except for precisionnp and
recallp which are both highest with KNN.

Overall the Bayes-based classiﬁers (GNBC & BNBC) perform relatively poorly compared to
the other ML trained classiﬁers. The signiﬁcant weakness of these classiﬁers appears to be in their
ability to detect packed samples, with all metrics much lower than non-packed metrics. GNBC
performs worse than BNBC, which is in line with the prior tuning results.

The linear models (LR & LSVM) performed in the middle compared with the other algorithms,
and very similarly to each other. There is no easy way to distinguish LR from LSVM fro these
metrics, and so a decision between them may consider the training time where LR is much faster.
The decision tree based classiﬁers (DT, GBDT, RF, & DL8.5) mostly perform well, with
GBDT performing slightly better than RF while DT and DL8.5 are less eﬀective on all metrics.
MLP appears eﬀective and comparable with the best decision tree based classiﬁer (GBDT)

although worse than KNN and LSVM.

Finally, KSVM appears to oﬀer the best overall performance among the diﬀerent classiﬁers.

5.2. Adaptability

In the prior experiments the data sets consist of collected samples labeled with majority vote (as
detailed in Section 3.1). To explore issues related to poorly represented packers and adaptability of
the ML algorithms, the accuracy contrasting Big with Big+f is here evaluated with a variety of
packers f . This contrast is detailed for ﬁve packer families: UPX, kkrunchy, MPress, TElock and
PEtite. In each case all 11 ML trained classiﬁers are trained on both Big and Big+f for each f of
the ﬁve packers and evaluated on SelfPack f .

These ﬁve packers are chosen as they represent very diﬀerent frequencies within Big. The
frequency of UPX is 11978 in Big and 12478 in Big+U P X . The frequency of kkrunchy is 71 in Big
and 571 in Big+kkrunchy. The frequency of MPress is 344 in Big and 844 in Big+M P ress. The
frequency of TElock is 351 in Big and 851 in Big+T Elock. Finally, the frequency of PEtite is 23 in

21

Big and 523 in Big+P Etite. Note that the frequencies for all packers except f in Big+f remains
unchanged.

The experimental approach is an adaptation of the approach used in [9]. All the ML classiﬁers
are trained using either Big or Big+f and then evaluated on SelfPack f . The goal of this
experiment was to observe the adaptability of the ML trained classiﬁers to detect new samples
created with known packer f in Big+f and compare this with the ML classiﬁer that is trained on
Big without the additional packed samples of family f . As usual all experiments were performed
with 10-fold cross-validation, although here the training set and testing sets are distinct.

An overview of the results can be seen in Table 4. Observe that in almost all cases the extended
training set improves the accuracy for classiﬁcation on SelfPack f for all f (the only exception
is TElock using MLP). Observe that not all classiﬁers are equal against new packed samples and
many perform poorly on unknown packers. Since KNN keeps and uses all the data set for its
model, more new samples appear necessary to improve classiﬁcation to a high accuracy. GNBC
seems to perform great on samples suﬃciently present in the data set and show diﬃculties facing
other packers (even when extending the data set). Comparatively, BNBC performs better by
directly recognizing more packers and being able to integrate new information for its model. LR
and LSVM models both perform well and are able to integrate new samples to improve their model
(particularly LSVM). DT and DL8.5 performs well for some packers (UPX and MPress) without
additional samples but fail to integrate new packers eﬃciently even when extending the training
set. Contrarily, GBDT adapts extremely well and RF also improves, albeit less than GBDT.
DL8.5 shows improvement with new information, although like RF struggling to handle PEtite.
MLP shows excellent results by detecting many packers with high accuracy and integrating new
information eﬀectively. Finally, KSVM already performed extremely like MLP and integrated new
information even more eﬀectively.

These results address RQ2 by examining the eﬀectiveness of all 11 ML algorithms via various
metrics and experiments. There are 4 ML algorithms that appear to be most eﬀective as brieﬂy
highlighted below.

• KNN is memory expensive (all data points are part of the model) and requires many samples
to improve its model. However, KNN avoids false positives/negatives and performs well if
samples are suﬃciently present in the training data set.

• GBDT is light in training (compared to other methods). GBDT oﬀers good performance
and can easily extend its model in case of a new packers detected (if samples are provided).
(Note RF is slightly less eﬃcient but in many ways comparable to GBDT if a decision trees
based classiﬁer is desirable.)

• MLP is resource demanding for training but oﬀers good performance and adaptability.

• KSVM appears to be the most eﬀective and adaptable ML algorithm for packing detection.
KSVM has a low false positive/negative rate and all packers are detected in the adaptability
experiments.

6. Economical Analysis

This section addresses RQ3 (Which ML algorithms perform well in long term for packing

detection?) by exploring the eﬀectiveness of all 11 ML algorithms over chronological data.

22

Classiﬁer Baseline Period 1 Period 2 Period 3 Period 4
0.9696
0.8751
0.8545
0.9402
0.9396
0.9324
0.9685
0.9701
0.9016
0.9669
0.9788

KNN
GNBC
BNBC
LR
LSVM
DT
RF
GBDT
DL8.5
MLP
KSVM

0.9902
0.9046
0.9263
0.9703
0.9703
0.9580
0.9819
0.9880
0.9456
0.9882
0.9929

0.9899
0.9034
0.9170
0.9634
0.9627
0.9551
0.9776
0.9851
0.9383
0.9823
0.9924

0.9830
0.8940
0.9158
0.9506
0.9509
0.9438
0.9751
0.9806
0.9408
0.9781
0.9922

0.9823
0.8738
0.8890
0.9452
0.9466
0.9395
0.9768
0.9824
0.9037
0.9757
0.9913

Table 5: Economical analysis for all 11 ML trained classiﬁers after tuning trained on Big and tested on Big (baseline)
and Chrono split by chronological period.

Classiﬁer Train time [s] Uptime 0.92 [s]
8,986,447
2.1259
N/A
0.0368
1,661,053
0.1673
8,346,239
11.0572
9,961,820
316.172
6,874,338
0.0607
14,776,270
0.7492
10,303,293
2.7977
3,310,904
599.31
12,244,014
317.29
9,262,621
161.9987

KNN
GNBC
BNBC
LR
LSVM
DT
RF
GBDT
DL8.5
MLP
KSVM

Ratio 0.92 Uptime 0.95 [s] Ratio 0.95 Uptime 0.97 [s] Ratio 0.97
2,308,222
3,220,415
4,227,126
N/A
N/A
N/A
N/A
N/A
9,928,592
12,187
252,500
754,824
9,064
31,507
286
N/A
27,691,918
113,251,038
6,423,250
12,967,467
19,722,731
1,828,869
2,722,403
3,682,772
N/A
N/A
5,524
13,859
25,228
38,589
36,424
46,133
57,177

4,907,050
N/A
N/A
134,757
90,561
N/A
4,812,299
5,116,627
N/A
4,397,608
5,900,674

6,846,280
N/A
N/A
2,791,949
2,865,875
1,680,899
9,715,226
7,616,469
N/A
8,004,772
7,473,608

Table 6: Economical analysis for all 11 ML trained classiﬁers after tuning trained on Big including time to train,
uptime while maintaining F -scores, and uptime to train time ratio for F -scores 0.92, 0.95, and 0.97.

This section evaluates the eﬀectiveness of all 11 ML algorithms over time and the retraining
required to maintain a high level of accuracy. To perform this analysis all 11 ML algorithms are
trained on Big and their eﬀectiveness evaluated on Chrono. Big is used to train each classiﬁer,
thus training on samples collected from 2019-10-01 to 2020-02-28. The testing is then done on
Chrono which corresponds to malware samples obtained from 2020-04-01 to 2020-05-26. Chrono
has been divided into four subsets corresponding to separate periods of time as follows. Period
1 designates malware obtained from 2020-04-01 to 2020-04-14. Period 2 malware samples are
obtained from 2020-04-15 to 2020-04-28. Period 3 malware samples are obtained from 2020-04-29
to 2020-05-12. Finally, period 4 malware samples are obtained from 2020-05-13 to 2020-05-26

An overview of the accuracy and eﬃciency of all 11 ML trained classiﬁers on these four periods
from Chrono are shown in Table 5. As expected, the eﬀectiveness of all 11 ML trained classiﬁers
decreases with time. This corresponds to prior research [18] and can be explained by diﬀerent
trends in packers used depending on the period of interest. Some packers are employed frequently
by malware in the period 1 of Chrono and are slowly replaced by other packers during later
periods.

The training time and uptime can be used to predict the economics of an ML trained classiﬁer

23

by calculating the retraining required to maintain a given quality. Here the quality is picked as a
target F -score and then the uptime is the time after training during which the ML trained classiﬁer
has at least the chosen quality. An overview of the quality measures for three diﬀerent F -scores
(0.92, 0.95, and 0.97) are shown in Table 6. Using the period calculated F -scores for each period
a curve for more precise estimates is obtained by a quadratic least squares regression. This curve
can then be used to predict the time that a particular trained classiﬁer will drop below a given
F -score threshold. The ratio then indicates the economics of cost in training to uptime, with higher
numbers being better, Here these indicate that DT and RF have the best economics due to their
extremely quick training time and generally high accuracy and F -score. KNN and GBDT also
perform well, although the signiﬁcant increase in training time brings their economic ratio down
signiﬁcantly. LR, LSVM, MLP, and KSVM are all also capable of meeting the requirements, but
are economically hampered by their signiﬁcantly larger training times. However, observe that since
the uptime for KNN, RF, GBDT, MLP, and KSVM all have uptimes >∼ 80 days even the ∼ 5
minutes training time for the highest (MLP) is not too signiﬁcant.

In machine learning, the training data set used is critical to ensure good performance (as noted
also in Section 5.2). Although the performance of all classiﬁers decreases over time, some classiﬁers
are more robust than others and require less training time and frequency to maintain a given level
of eﬀectiveness. This allows us to answer RQ3: KNN, LR, DT, RF, GBDT, MLP and KSVM
are all able to maintain an F -score of 0.95. From the pure economics, DT and RF signiﬁcantly out
perform the others, although considering all training times as reasonable then KNN, RF, GBDT,
MLP, and KSVM are all able to remain eﬀective for long periods of time between retraining.

7. Challenges to Validity

This section brieﬂy discusses the main potential challenges to the validity of this work and a

brief examination of the signiﬁcance of dynamic features in packer detection.

One challenge to the validity of many works in this area is the selection of ground truth. The
approach used here has the advantage of a broad selection of samples that are believed to be
malware. Although this has the potential limitation of having few or no clean samples, this choice
was made since this work explores the role of packing detection within a malware detection setting.
Another challenge the ground truth used here is that this relies upon a multiple approaches that
in turn are subject to their own limitations. Broadly the consensus approach used here should
mitigate some of the limitations (see also [18]). However, this leaves those approaches and this
work relatively vulnerable to rare or unknown packers. To some extent the adaptability exploration
(see Section 5.2) addresses this, by showing the eﬀectiveness of the ML algorithms when presented
with new training data. The above aside, clearly better ground truth and training data should
improve the behaviour of all the ML algorithms considered here.

Another challenge is the use of only accuracy as a metric for the tuning and most analysis
performed in this work. This choice was to simplify the work by having a single metric (that in
turn accounts for both positive and negative outcomes) without giving too much bias or favour to
any particular outcome. The exploration of diﬀerent metrics (see Section 5.1) indicates that other
metrics do not vary signiﬁcantly from accuracy. Thus, the results here should be easily broadly
applicable to other metrics, and the methodology easily adaptable should another metric be more
important in another application.

A further challenge is the use of only static features for packing detection although some related
works have also considered dynamic features [15, 21, 25]. The problem of packing detection as part of

24

Classiﬁer
KNN
GNBC
BNBC
LR
LSVM
DT
RF
GBDT
DL8.5
MLP
KSVM

Static Features Only
0.9889
0.939
0.947
0.9836
0.9833
0.9864
0.9888
0.9941
0.9906
0.9902
0.9938

Static & Dynamic Features
0.9889
0.943
0.947
0.9836
0.9833
0.9864
0.9888
0.9941
0.9907
0.9915
0.9931

Table 7: Accuracy comparison for all 11 ML algorithms using Chrono static features and with additional dynamic
features.

a larger malware detection and defence system motivated the choice to focus only on static features
in this work, as well as this being proven highly eﬀective both here and in related works. However,
a small investigation was done to explore whether dynamic features may have been signiﬁcantly
useful. This is included below, although due to the cost of feature extraction for dynamic features
this work chose to focus on larger data sets and extensive results on cheap features to extract.

7.1. A Note on Dynamic Features

Since dynamic features have been used in the literature, and since packing could be more
obvious when a program is executed, this raises a potential further research questions. RQA Do
dynamic features improve packing detection? This section brieﬂy presents results exploring
this question.

Dynamic entropy features are often used in the literature to detect packing [15, 17, 24]. To
explore whether dynamic features were signiﬁcant, we considered four dynamic entropy features.
The maximum increase of entropy of a section (Feature D1). The maximum decrease of entropy of
a section (Feature D2). The biggest change of entropy of the whole ﬁle (Feature D3). The change of
entropy of the entrypoint section (Feature D4). Since initial entropy of code section, data section,
resource section, entire PE ﬁle, and entrypoint sections are already present in the static features,
adding these novel features constitute an approximation of the entropy pattern generally studied
in literature.

To extract these features each sample was executed once for each of their executable sections.
The execution continues until control ﬂow reached the ﬁrst jump instruction into the target ex-
ecutable section. This stop condition reached, entropy values of all distinct sections and global
entropy of the ﬁle are recorded and the process is reiterated for the subsequent executable section.
This way of proceeding is less powerful than monitoring all entropy changes and stopping accord-
ingly (e.g. a packer using multiple layers could still be engaged in the unpacking process). However,
it could be applied easily to non-packed executables required in the context of packing detection.
The dynamic features were extracted for a subset of 12,036 samples from Chrono and then all
11 ML algorithms were trained and tested using 10-fold cross-validation with 90% of the data set
used for training and 10% for validation (as in all the previous experiments).

25

The impact of these features on accuracy results can be observed in Table 7. These results
indicate that (our) dynamic features do not signiﬁcantly inﬂuence the decision process. At best
there is a negligible increase in accuracy (e.g. for GNBC, DL8.5, and MLP) and at worse a
degradation (KSVM), although most did not beneﬁt from the addition of dynamic features.

Due to the signiﬁcant extraction cost (in time, instrumentation, execution environment, etc.)
of dynamic features and the lack of impact, these results show that static features can be preferred
for eﬃcient packer classiﬁcation as they already provide high accuracy for packing detection. Note
that since these results indicated a negligible improvement on a restricted data set. Thus, further
exploration of these features was not considered here. This result agrees with what has been reported
in literature, in that dynamic features can contribute to accurate detection, but are not necessary.
Particularly when handling only detection, and when the accuracy is already very high. Generally,
when dynamic features are used in the literature, the objective is not just packing detection but also
classiﬁcation or direct unpacking by trying to ﬁnd the original entrypoint. Thus the high costs of
computing dynamic features need to be countered balance by stronger objectives than just malware
detection. However, since IAT table features show signiﬁcant inﬂuence on classiﬁcation, focusing on
these kinds of dynamic features could deliver more impact (similar to [21]). Even though overhead
would generally stay important. We leave this question for future works.

8. Conclusion

The challenge of detecting packed programs is a key part of contemporary cyber-security defense.
Many approaches have been taken to detect whether a sample is packed, applying various algorithms
to many diﬀerent features and measuring the eﬀectiveness in diﬀerent ways. This work addresses
three main research questions about the: signiﬁcant features, eﬀective ML algorithms, and long
term eﬀectiveness for packing detection.

The feature signiﬁcance shows that across 11 diﬀerent ML algorithms there are a small number of
static features that are most signiﬁcant. These are related to: the stack reserve (Feature 19), number
of readable and writable sections (Feature 29), non-standard entry point section (Feature 35), raw
and virtual data size of sections (Features 37 & 39), byte entropy of .text sections (Feature 43), 12th
entry point byte (Feature 60), and malicious API calls (Features 115-116). This diversity of features
correlates with prior results showing that many diﬀerent kinds of features can be eﬀective, and that
none is clearly dominant. Further, by gathering feature use data across many ML algorithms there
is evidence that many features can play a small role in improving accuracy, even if they are not
signiﬁcant alone.

The eﬀectiveness on 11 diﬀerent ML algorithms was considered with various metrics and also
considering permutations in the training data. Overall KNN, GBDT, MLP, and KSVM proved to
be the most eﬀective and robust. These 4 algorithms were all able to score well on: accuracy, preci-
sion, recall, and F -score taking into account packed unpacked, and averaging. There 4 algorithms
also performed well in adapting to new training data eﬀectively.

The long term eﬀectiveness of the 11 ML algorithms was also evaluated using chronological
samples. The results here showed that all classiﬁers had reduced accuracy over time (as expected).
The uptime between retraining for the diﬀerent algorithms varied substantially, with KNN, RF,
GBDT, MLP, and KSVM all being able to maintain a high eﬃciency for ∼ 80 days or more.
However, the extremely low training cost for DT and RF makes them also competitive if time to
retrain is weighted signiﬁcantly higher than time between retraining.

26

Overall the results here indicate that KNN, GBDT, MLP, and KSVM are all eﬀective and
economical. Of these 4 the choice for a particular application depends on which metrics are most
important, with KNN and GBDT scoring higher in economic ratio due to lower training time and
needing less features, but tending to be lower in eﬀectiveness. By contrast KSVM scores higher in
eﬀectiveness metrics at the cost of using more features and taking more training time. MLP does
not stand out signiﬁcantly in any one area, but has a diﬀerent balance that may be optimal for
some applications.

Acknowledgments. Charles-Henry Bertrand Van Ouytsel is FRIA grantee of the Belgian Fund
for Scientiﬁc Research (FNRS-F.R.S.). We would like to thanks Cisco for their malware feed and
the availability of their packing detector engine.

References

[1] ClamAV. https://www.clamav.net. November 2019.

[2] Detect-it-easy version 2.06. https://github.com/horsicq/Detect-It-Easy. November 2019.

[3] Generalized linear model (GLM) — h2o 3.30.0.4 documentation. https://docs.h2o.ai/h2o/

latest-stable/h2o-docs/data-science/glm.html.

[4] Malware statistics trends report: Av-test. https://portal.av-atlas.org/malware. 14 Jan-

uary 2021.

[5] PEiD, version 0.95. https://appnee.com/peid/. November 2019.

[6] pydl8.5. https://github.com/aia-uclouvain/pydl8.5. May 2020.

[7] The shadowserver foundation. https://www.shadowserver.org/. November 2019.

[8] The wildlist organization international. http://www.wildlist.org/.

[9] H. Aghakhani, F. Gritti, F. Mecca, M. Lindorfer, S. Ortolani, D. Balzarotti, G. Vigna, and
C. Kruegel. When malware is packin’heat; limits of machine learning classiﬁers based on static
In Network and Distributed Systems Security (NDSS) Symposium 2020,
analysis features.
2020.

[10] M. Ahmadi, D. Ulyanov, S. Semenov, M. Troﬁmov, and G. Giacinto. Novel feature extraction,
selection and fusion for eﬀective malware family classiﬁcation.
In Proceedings of the Sixth
ACM on Conference on Data and Application Security and Privacy, CODASPY 2016, New
Orleans, LA, USA, March 9-11, 2016, pages 183–194, 2016.

[11] G. Amato. peframe version 6.0.3. https://github.com/guelfoweb/peframe. November 2019.

[12] R. Arora, A. Singh, H. Pareek, and U. R. Edara. A heuristics-based static analysis approach for
detecting packed pe binaries. International Journal of Security and Its Applications, 7(5):257–
268, 2013.

[13] U. Baldangombo, N. Jambaljav, and S.-J. Horng. A static malware detection system using

data mining methods.

27

[14] R. Balestriero, H. Glotin, and R. G. Baraniuk. Semi-supervised learning enabled by multiscale

deep neural network inversion. CoRR, abs/1802.10172, 2018.

[15] M. Bat-Erdene, T. Kim, H. Li, and H. Lee. Dynamic classiﬁcation of packing algorithms
In 2013 8th International Conference on

for inspecting executables using entropy analysis.
Malicious and Unwanted Software:” The Americas”(MALWARE), pages 19–26. IEEE, 2013.

[16] M. Bat-Erdene, T. Kim, H. Park, and H. Lee. Packer detection for multi-layer executables

using entropy analysis. Entropy, 19(3):125, 2017.

[17] M. Bat-Erdene, H. Park, H. Li, H. Lee, and M.-S. Choi. Entropy analysis to classify un-
known packing algorithms for malware detection. International Journal of Information Security,
16(3):227–248, 2017.

[18] F. Biondi, M. A. Enescu, T. Given-Wilson, A. Legay, L. Noureddine, and V. Verma. Eﬀective,

eﬃcient, and robust packing detection and classiﬁcation. 85:436–451.

[19] F. Biondi, T. Given-Wilson, A. Legay, C. Puodzius, and J. Quilbeuf. Tutorial: An overview of
malware detection and evasion techniques. In T. Margaria and B. Steﬀen, editors, Leveraging
Applications of Formal Methods, Veriﬁcation and Validation. Modeling, volume 11244, pages
565–586. Springer International Publishing. Series Title: Lecture Notes in Computer Science.

[20] G. Bonfante, J. M. Fernandez, J. Marion, B. Rouxel, F. Sabatier, and A. Thierry. Codisasm:
Medium scale concatic disassembly of self-modifying binaries with overlapping instructions.
In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications
Security, Denver, CO, USA, October 12-16, 2015, pages 745–756. ACM, 2015.

[21] B. Cheng, J. Ming, J. Fu, G. Peng, T. Chen, X. Zhang, and J. Marion. Towards paving the way
for large-scale windows malware analysis: Generic binary unpacking with orders-of-magnitude
performance boost. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and
Communications Security, CCS 2018, Toronto, ON, Canada, October 15-19, 2018, pages 395–
411. ACM, 2018.

[22] Y.-s. Choi, I.-k. Kim, J.-t. Oh, and J.-c. Ryou. Pe ﬁle header analysis-based packed pe ﬁle detec-
tion technique (phad). In International Symposium on Computer Science and its Applications,
pages 28–31. IEEE, 2008.

[23] S. Han, K. Lee, and S. Lee. Packed pe ﬁle detection for malware forensics.

In 2009 2nd
International Conference on Computer Science and Its Applications, CSA 2009, page 5404211,
2009.

[24] G. Jeong, E. Choo, J. Lee, M. Bat-Erdene, and H. Lee. Generic unpacking using entropy
analysis. In 2010 5th International Conference on Malicious and Unwanted Software, pages
98–105. IEEE, 2010.

[25] M. G. Kang, P. Poosankam, and H. Yin. Renovo: A hidden code extractor for packed ex-
In Proceedings of the 2007 ACM workshop on Recurring malcode, pages 46–53,

ecutables.
2007.

[26] I. Kwiatkowski. Manalyze. https://github.com/JusticeRage/Manalyze. November 2019.

28

[27] R. Lyda and J. Hamrock. Using entropy analysis to ﬁnd encrypted and packed malware. IEEE

Security & Privacy, 5(2):40–45, 2007.

[28] A. C. M¨uller and S. Guido. Introduction to machine learning with Python: a guide for data

scientists. O’Reilly.

[29] S. Nijssen, P. Schaus, et al. Learning optimal decision trees using caching branch-and-bound

search. In Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, 2020.

[30] L. Noureddine, A. Heuser, C. Puodzius, and O. Zendra. Se-pac: A self-evolving packer classiﬁer
against rapid packers evolution. In CODASPY’21: Eleventh ACM Conference on Data and
Application Security and Privacy, 2021.

[31] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research, 12:2825–2830, 2011.

[32] R. Perdisci, A. Lanzi, and W. Lee. Classiﬁcation of packed executables for accurate computer

virus detection. 29(14):1941–1946.

[33] R. Perdisci, A. Lanzi, and W. Lee. Mcboost: Boosting scalability in malware collection and
In 2008 Annual Computer Security

analysis using statistical classiﬁcation of executables.
Applications Conference (ACSAC), pages 301–310. IEEE, 2008.

[34] J. Raphel and P. Vinod.

Information theoretic method for classiﬁcation of packed and en-
coded ﬁles. In Proceedings of the 8th International Conference on Security of Information and
Networks, pages 296–303, 2015.

[35] I. Santos, X. Ugarte-Pedrero, B. Sanz, C. Laorden, and P. G. Bringas. Collective classiﬁcation
for packed executable identiﬁcation. In Proceedings of the 8th Annual Collaboration, Electronic
messaging, Anti-Abuse and Spam Conference, pages 23–30, 2011.

[36] L. Sun, S. Versteeg, S. Bozta¸s, and T. Yann. Pattern recognition techniques for the classiﬁcation
of malware packers. In Australasian Conference on Information Security and Privacy, pages
370–390. Springer, 2010.

[37] X. Ugarte-Pedrero, I. Santos, and P. G. Bringas. Structural feature based anomaly detection
for packed executable identiﬁcation. In Computational intelligence in security for information
systems, pages 230–237. Springer, 2011.

[38] X. Ugarte-Pedrero, I. Santos, I. Garc´ıa-Ferreira, S. Huerta, B. Sanz, and P. G. Bringas. On
the adoption of anomaly detection for packed executable ﬁltering. Computers & Security,
43:126–144, 2014.

[39] X. Ugarte-Pedrero, I. Santos, B. Sanz, C. Laorden, and P. G. Bringas. Countering entropy
measure attacks on packed software detection. In 2012 IEEE Consumer Communications and
Networking Conference (CCNC), pages 164–168. IEEE, 2012.

[40] VirusTotal. Virustotal: Yara in a nutshell (2019).

[41] M. Zakeri, F. Faraji Daneshgar, and M. Abbaspour. A static heuristic approach to detecting

malware targets. Security and Communication Networks, 8(17):3015–3027, 2015.

29

Appendix A. Extracted Features

This section includes a full table (A.8) of all the 119 static features considered in this work and

their descriptions. They are also grouped according to their commonality as in Biondi et al. [18].

ID

Type

Description

1
2
3
4

5

6
7
8
9
10
11

12
13
14

15

16

17

18

19
20
21

22
23
24

25
26
27
28

Boolean
Boolean
Boolean
Boolean

Boolean

Boolean
Boolean
Boolean
Integer
Integer
Integer

Integer
Integer
Integer

Integer

Integer

Integer

Integer

Integer
Integer
Integer

Integer
Integer
Float

Integer
Integer
Integer
Integer

Metadata features

DLL can be relocated at load time. Extracted from DLLs characteristics
Code Integrity checks are enforced. Extracted from DLLs characteristics
Image is NX compatible. Extracted from DLLs characteristics
Isolation aware, but do not isolate the image. Extracted from DLLs character-
istics
Does not use structured exception (SE) handling. No SE handler may be called
in this image. Extracted from DLLs characteristics
Do not bind the image. Extracted from DLLs characteristics
A WDM driver. Extracted from DLLs characteristics
Terminal Server aware. Extracted from DLLs characteristics
The image ﬁle checksum.
The preferred address of the ﬁrst byte of image when loaded into memory
The address that is relative to the image base of the beginning-of-code section
when it is loaded into memory.
The major version number of the required operating system.
The minor version number of the required operating system.
The size (in bytes) of the image, including all headers, as the image is loaded
in memory.
The size of the code (.text) section, or the sum of all code sections if there are
multiple sections.
The combined size of an MS DOS stub, PE header, and section headers rounded
up to a multiple of FileAlignment.
The size of the initialized data section, or the sum of all such sections if there
are multiple data sections.
The size of the uninitialized data section (BSS), or the sum of all such sections
if there are multiple BSS sections
The size of the stack to reserve.
The size of the stack to commit.
The alignment (in bytes) of sections when they are loaded into memory.

Section features
The number of standard sections the PE holds
The number of non-standard sections the PE holds
The ratio between the number of standard sections found and the number of
all sections found in the PE under analysis
The number of Executable sections the PE holds
The number of Writable sections the PE holds
The number of Writable and Executable sections the PE holds
The number of readable and executable sections

30

29
30
31
32
33
34
35
36
37
38
39

40
41
42

43
44
45
46
47
48

Integer
Integer
Boolean
Boolean
Boolean
Boolean
Boolean
Boolean
Float
Integer
Integer

Float
Float
Boolean

The number of readable and writable sections
The number of Writable and Readable and Executable sections the PE holds
The code section is not executable
The executable section is not a code section
The code section is not present in the PE under analysis
The entry point is not in the code section
The entry point is not in a standard section
The entry point is not in an executable section
The ratio between raw data and virtual size for the section of the entry point
The number of section having their raw data size zero
The number of sections having their virtual size greater than their raw data
size.
The maximum ratio raw data to virtual size among all sections
the minimum ratio raw data to virtual size among all sections
The address pointing to raw data on disk is not conforming with the ﬁle align-
ment

Entropy features

Float ∈ [0; 8]
Float ∈ [0; 8]
Float ∈ [0; 8]
Float ∈ [0; 8]
Float ∈ [0; 8]
Float ∈ [0; 8]

The byte entropy of code (.text) sections
The byte entropy of data section
The byte entropy of resource section
The byte entropy of PE header
The byte entropy of the entire PE ﬁle
The byte entropy of the section holding the entry point of the PE under analysis

Entry byte features

49 - 112

Integer ∈ [0; 255] Values of 64 bytes following the entry point, each byte correspond to 1 feature

113
114
115

116

117

118
119

Integer
Integer
Integer

Float

Integer

Boolean
Integer

position

Import functions features

The number of DLLs imported
The number of functions imported found in the import table directory (IDT)
The number of malicious APIs imported (malicious as deﬁned in [41]). This
list includes: ”GetProcAddress”, ”LoadLibraryA”, ”LoadLibrary”, ”ExitPro-
cess”, ”GetModuleHandleA”, ”VirtualAlloc”, ”VirtualFree”, ”GetModuleFile-
NameA”, ”CreateFileA”, ”RegQueryValueExA”, ”MessageBoxA”, ”GetCom-
mandLineA”, ”VirtualProtect”, ”GetStartupInfoA”, ”GetStdHandle”, ”Re-
gOpenKeyExA”.
The ratio between the number of malicious APIs imported to the number of
all functions imported by the PE
the number of addresses (corresponds to functions) found in the import address
table (IAT)

Ressource features

The debug directory is present or not
The number of resources the PE holds

Table A.8: Features description.

31

Appendix B. Boolean Conversion Applied to Features

This section details in Table B.9 how each of the 119 features (detailed in Table A.8) is converted

into Booleans via bucketing.

ID
1-8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
23
24
25
26
27
28
29
30
31-36
37
38
39
40
41
42
43-45
46-117
118
119

Description
Already boolean
2 buckets : 0 or other values
2 buckets : 4194304 or other values
3 buckets : 4096, 8192 or other values
3 buckets : 4, 5 or other values
2 buckets : 0 or other values
2 buckets : < 250000 or >= 25000
2 buckets : < 50000 or >= 50000
5 buckets : 1024, 4096, 512, 1536 or other values
feature deleted due to high sparsity
2 buckets : 0 or other values
2 buckets : 1048576 or other values
5 buckets : 4096, 16384, 8192, 65536 or other values
3 buckets : 4096, 8192 or other values
10 buckets : 0, 1, 2, 3, 4, 5, 6, 7, 8 or >= 9
5 buckets : 0, 1, 2, 3 or >= 4
2 buckets : 0 or other values
2 buckets : < 3 or >= 3
2 buckets : 0 or other values
5 buckets : 0, 1, 2, 3 or >= 4
2 buckets : 0 or other values
2 buckets : < 3 or >= 3
6 buckets : 0, 1, 2, 3, 4 or >= 5
4 buckets : 0, 1, 2 or >= 3
4 buckets : 0, 1, 2 or >= 3
6 buckets : 0, 1, 2, 3, 4 or >= 5
4 buckets : 0, 1, 2 or >= 3
Already boolean
3 buckets : > 1, < 1 or 1
4 buckets : 0, 1, 2 or >= 3
5 buckets : 0, 1, 2, 3 or >= 4
3 buckets : > 1, < 1 or 1
2 buckets : 0 or other values
Already boolean
2 buckets : −1 or other values
2 buckets : 0 or other values
Already boolean
2 buckets : 0 or other values

Table B.9: Explanation of Boolean conversion with bucketing for all 119 features.

32

