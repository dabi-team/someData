2
2
0
2

p
e
S
0
1

]
I

A
.
s
c
[

1
v
2
8
5
4
0
.
9
0
2
2
:
v
i
X
r
a

Explaining Results of Multi-Criteria Decision Making

Martin Erwig, Prashant Kumar

{erwig,kumarpra}@oregonstate.edu

Abstract

We introduce a method for explaining the results of various linear and hierarchical multi-criteria

decision-making (MCDM) techniques such as WSM and AHP. The two key ideas are (A) to main-

tain a ﬁne-grained representation of the values manipulated by these techniques and (B) to derive

explanations from these representations through merging, ﬁltering, and aggregating operations. An

explanation in our model presents a high-level comparison of two alternatives in an MCDM problem,

presumably an optimal and a non-optimal one, illuminating why one alternative was preferred over the

other one. We show the usefulness of our techniques by generating explanations for two well-known

examples from the MCDM literature. Finally, we show their eﬃcacy by performing computational

experiments.

Key words : Multiple criteria analysis, MCDM, AHP, WSM, contrastive explanations

1 Introduction

The theory and methods of multi-criteria decision-making (MCDM) have been extensively applied in many areas,

ranging from engineering projects, economics, public administration, to management and military projects. For

example, in 1986 the Institute of Strategic Studies in Pretoria, a government-backed organization, used the Analytic

Hierarchy Process (AHP) (Saaty 1990, 1987) to analyze the conﬂict in South Africa and recommended actions

ranging from the release of Nelson Mandela to the removal of apartheid and the granting of full citizenship and

equal rights to the black majority (Saaty 2003). All the recommendations were implemented within a short time.

Another high-proﬁle example is the use of AHP in the 1995 US/China conﬂict over Chinese illegal copying of

music, video, and software (Saaty and Cho 2001). An AHP analysis involving four hierarchies for beneﬁts, costs,

1

 
 
 
 
 
 
opportunities, and risks showed, surprisingly, that it was much better for the United States not to sanction China.
The result of the study predicted what happened. Shortly after the study was complete, the United States awarded

China the most-favored nation status and didn’t sanction it. In the domain of business, the Xerox Corporation has

used AHP to allocate almost a billion dollars to its research projects (Saaty 2002), and IBM used AHP in 1991 in

designing its successful mid-range AS 400 computer (Tang and Collar 1992).

Given the wide-spread use and impact of MCDM methods, it is natural for decision makers to ask why a

certain alternative was suggested. Automation systems can earn their users’ trust by explaining results because
explanations give users conﬁdence in the correctness and reliability of computation processes (Faulhaber et al.

2021), in particular when the recommended actions are counter-intuitive, such as the recommendation not to

sanction China in the trade conﬂict. However, existing MCDM techniques do not provide explanations about their

recommended actions.

With the ever-increasing impact of algorithms and mathematical models in our daily lives, there has been

an increased demand for their explainability, so much so that countries have started to incorporate the right to
explanation (Selbst and Powles 2017) of an algorithm’s result impacting the life of its citizens as part of their legal
framework. The General Data Protection Regulation of the European Union (Goodman and Flaxman 2017) and

the Digital Republic Act of France (The French Government 2016) are examples of such regulations. This trend is

expected to continue, and therefore it stands to reason that sooner rather than later there will be an expectation, if

not an obligation, for modeling tools such as MCDM to explain their results.

When presented with a result by an algorithm, a natural question for users to ask is “Why is the result 𝑋 and

not 𝑌 ?”. Such a question calls for what the philosophy literature calls a contrastive explanation (Lipton 2004,
1990). A contrastive explanation compares two speciﬁc phenomena, the actual result (called fact or solution) and
a hypothetical alternative (called foil) and justiﬁes “Why this [fact] rather than that [foil]?” (Garﬁnkel 1981). A
nice property of contrastive explanations is that they can be tailored to diﬀerent users who may be wondering about

diﬀerent aspects of a solution. Diﬀerent (parts of) results may result in diﬀerent foils and consequently in diﬀerent

explanations.

In this paper, we present a method to automatically generate explanations for the various forms of MCDM,

using an explanation mechanism called minimal dominating sets (MDS) (Erwig and Kumar 2021a), which allows
explanations to focus on only the most relevant aspects of a decision, thereby allowing the generation of concise

explanations. The main contributions of this paper are the following.

• An MDS-based explanation method for the two linear MCDM techniques Weighted Sum Method

2

(WSM) (Fishburn 1967) and Weighted Product Method (WPM) (Tofallis 2014).

• Hierarchical value decomposition as a novel representation that facilitates the generalization of MDS

explanations to the hierarchical MCDM technique AHP.

• Experimental evidence for the eﬀectiveness of our approach.

The rest of this paper is structured as follows. After reviewing the various MCDM techniques in Section 2, we

present in Section 3 the MDS explanation technique for linear MCDM using linear value decomposition.

In

Section 4, we introduce hierarchical value decomposition as a generalization of the linear case. In Section 5, we

demonstrate how MDS explanations can be extended to work with this hierarchical structure. In Section 6, we

introduce a method for simplifying explanations to a more coarse-grained form, thereby making them easier to

understand. In Section 7 we apply our explanation techniques to two examples from the MCDM literature, and in

Section 8 we evaluate the eﬀectiveness of the MDS explanation mechanism. We discuss related work in Section 9

and present conclusions in Section 10.

2 Multi-Criteria Decision Making

MCDM is the process of making decisions in the presence of multiple, usually conﬂicting, criteria. In this context,

an alternative represents one of several choices available to the decision maker. The goal of MCDM is to identify
the best alternative. Each MCDM problem is associated with multiple attributes, which represent the decision
criteria. An attribute can be beneﬁcial or detrimental to an alternative. Each attribute has an associated weight that
signiﬁes the importance of that attribute. The weight of an attribute remains constant across all alternatives and

varies between 0 and 1. All the attribute weights should sum to 1.

When the number of attributes gets large, they can be arranged in a hierarchical manner so that attributes higher

up in the hierarchy, also called major attributes, aggregate the contributions of minor attributes that appear lower
in the hierarchy. In Section 2.1 we describe the two linear MCDM techniques WSM and WPM, followed by the

hierarchical technique AHP in Section 2.2.

2.1 Linear MCDM

A linear MCDM problem with 𝑚 alternatives and 𝑛 attributes can be expressed by an 𝑚 ×𝑛 matrix, called a decision
matrix. Row 𝐴 𝑗 represents the attribute values of the 𝑗 th alternative, and row 𝐶𝑖 represents the values of 𝑖th attribute

3

Price

Eﬃciency Safety Comfort

0.1 (−)

0.4 (+)

0.3 (+)

0.2 (+)

Toyota

$22,000

Honda

$25,000

BMW $27,000

32

38

35

8.5

7.5

9.6

6.7

7.9

9.2

Figure 1: WSM example: decision matrix for car selection

for diﬀerent alternatives, where 𝑤𝑖 is the weight of the 𝑖th attribute. Each value 𝑎 𝑗𝑖 is the value of the attribute 𝐶𝑖

for the alternative 𝐴 𝑗 .

C1

𝑤1

𝑎11

𝑎21










𝑎𝑚1



C2

𝑤2

𝑎12

𝑎22
...
𝑎𝑚2

. . .

. . .
. . .

. . .

Cn

𝑤𝑛

𝑎1𝑛

𝑎2𝑛
...
𝑎𝑚𝑛













A1

A2
...
Am

As an example, consider the task of deciding which car to buy. Some attributes to be considered are price, fuel
eﬃciency, safety rating, and comfort. A decision matrix for this MCDM problem is shown in Figure 1. Price is a
detrimental attribute, which is indicated by a minus sign after the weight, whereas all other attributes are beneﬁcial,

as indicated by the plus sign.

In general, the values for the diﬀerent attributes are represented in diﬀerent units and at diﬀerent scales.

For instance, cost is measured in dollars whereas fuel eﬃciency is measured in miles per gallon. To compare and

combine values of diﬀerent attribute values, all values have to be normalized as follows. For a beneﬁcial attribute 𝐶𝑘
and the alternative 𝐴𝑙 , the normalized attribute value ¯𝑎𝑙𝑘 corresponding to 𝑎𝑙𝑘 is deﬁned as ¯𝑎𝑙𝑘 = 𝑎𝑙𝑘 /max1≤ 𝑗 ≤𝑚 𝑎 𝑗𝑘 .
Similarly, in case the attribute 𝐶𝑘 is detrimental, the attribute value ¯𝑎𝑙𝑘 is deﬁned as ¯𝑎𝑙𝑘 = min1≤ 𝑗 ≤𝑚 𝑎 𝑗𝑘 /𝑎𝑙𝑘 .

With a normalized decision matrix we can compute the optimal solution for a problem as follows. The

contribution of each attribute is obtained by a linear binary function of its weight and value. The particulars of the

binary function depend on the speciﬁc MCDM method that is employed. The so-called score 𝜏 of an alternative is
the aggregation of the contributions of its attributes. For WSM, the contribution of attribute 𝐶𝑖 for alternative 𝐴 𝑗

4

Price Eﬃciency Safety Comfort Score

0.1 (−)

0.4 (+)

0.3 (+)

0.2 (+)

𝜏

Toyota

1

Honda

0.88

BMW 0.81

0.84

1

0.92

0.89

0.78

1

0.73

0.86

1

0.85

0.89

0.95

Figure 2: Normalized decision matrix for car selection with ﬁnal score

is given by 𝑤𝑖 ¯𝑎 𝑗𝑖 . If each alternative consists of 𝑛 attributes, the score for 𝐴 𝑗 in the WSM is given by:

For WPM the contribution of 𝐶𝑖 for alternative 𝐴 𝑗 is given by ¯𝑎𝑤𝑖

𝑗𝑖 , and the score of 𝐴 𝑗 is given by:

𝜏𝐴𝑗 =

∑︁

𝑤𝑖 ¯𝑎 𝑗𝑖

1≤𝑖 ≤𝑛

𝜏𝐴𝑗 =

(cid:214)

¯𝑎𝑤𝑖
𝑗𝑖

1≤𝑖 ≤𝑛

The alternative with the highest score is by deﬁnition the best alternative. In Figure 2 we present the normalized

decision matrix with the aggregate scores in the last column for the car selection problem. The numbers suggest to

by a BMW despite it being the most expensive car.

2.2 Hierarchical MCDM

The idea behind the Analytic Hierarchy Process (AHP) is the decomposition of a complex problem into a hierarchy.

The leaves at the bottom of the hierarchy present the diﬀerent alternatives, whereas internal nodes play a dual role:

On one hand, they are attributes of the alternatives for the level immediately below them, on the other hand, they

represent alternatives for the level above them. AHP consists of the following three steps (Harker and Vargas 1987,

Saaty and Vargas 2001).

Step 1. Decomposition The attributes identiﬁed for a problem are organized in a hierarchy. For example,
Figure 3 shows the decomposition step applied to the car selection problem, which takes into account the personal

preferences as well as expert evaluations. Each quantify their preference for a car by specifying its various attributes

which are price, fuel eﬃciency, and safety ratings. (To make the example more manageable, we ignore the comfort

attribute in the following and consider only the Honda and BMW alternatives.)

5

The bottom nodes represent the two alternatives from which one is to be selected. Each node on the level

above (such as Price) is an attribute for each of the alternatives, and its value is given in a corresponding matrix that

captures the relationship between the two levels of the hierarchies. The AHP model is essentially a tree, represented

as a DAG that shares common children.

Step 2. Comparative Judgment The second step generates a matrix of pair-wise comparisons of all attributes
in a level with respect to each related attribute in the level immediately above it. For a DAG of 𝑛 levels, 𝐵𝑖 for

(1 ≤ 𝑖 < 𝑛) is the comparison matrix for the elements at level 𝑖 and 𝑖 + 1. The comparison matrices for our example

𝐵1 = 𝐴, 𝐵2 = 𝐹 , and 𝐵3 = 𝐶 are shown in Figure 4. (Note that we have normalized the numbers to account for the

AHP constraint that the numbers in each column must sum up to 1. Moreover, we changed the numbers slightly to

make the example a bit more interesting; we also rounded them to make the subsequent arithmetic easier to follow.)

Consider the matrix 𝐵3. Since the level 4 of the DAG consists of 2 nodes, and level 3 consists of 3 nodes, the

dimension of matrix 𝐵3 is 2 × 3. This matrix relates each feature with each car. Speciﬁcally, the entry .6 for fe and

honda in 𝐶 says that a Honda’s fuel eﬃciency is considered to be 50% better than a BMW’s.

Step 3. Synthesis of priorities This step generates the global (or composite) priorities of the elements at the
lowest level of the hierarchy. Given the priority matrices 𝐵1, . . . , 𝐵𝑛, the priority vector 𝑊 corresponding to the

alternatives at the leaf nodes of the DAG is given by the matrix product 𝑊 = 𝐵𝑛𝐵𝑛−1 . . . 𝐵2𝐵1. In our example,
synthesis of priorities produces the following result.

𝑊 = 𝐵3𝐵2𝐵1 = 𝐶𝐹𝐴 =








0.516

0.484








Honda

BMW

The synthesis concludes that Honda is the best car to buy, since it has a slight edge over BMW.

Score

Personal

Expert

Price

Fuel Eﬃciency

Safety Ratings

Honda

BMW

Figure 3: AHP model for the car selection problem

6

𝐵1 = 𝐴

car

𝐵2 = 𝐹

personal

expert

𝐵3 = 𝐶

p

fe

sr

personal

expert

.6

.4

p

fe

sr

.5

.3

.2

.2

.4

.4

honda .6

bmw

.4

.6

.4

.3

.7

Figure 4: Car selection decision matrices. 𝐴: weight of advice, 𝐹 : feature advice, 𝐶: car features.

This is all well and good, but since the scores for the two alternatives are quite close, the decision maker

might be interested to know why Honda is better and which assumptions lead to this conclusion. An answer to
this question is not obvious due to the complicated hierarchical relationships between the various nodes in the

corresponding AHP diagram. In the following, we demonstrate how to generate explanations for the linear and

hierarchical MCDM techniques.

3 Explaining Decisions of Linear MCDM With Value Decompositions

Consider again the car selection example from Figure 2. We abbreviate the alternatives Toyota, Honda, and BMW

by 𝑇 , 𝐻 , and 𝐵, respectively. The normalized contributions for the categories price (p), fuel eﬃciency (fe), safety

rating (sr), and comfort (c) are combined into a total score for every car alternative. This view can be formalized

using the concepts of value decomposition and valuation. Given a set of categories C, a mapping 𝑣 : C → R is
called a value decomposition (with respect to C). The (total) value of a value decomposition is deﬁned as the sum
of its components, that is, ˆ𝑣 = (cid:205)
(𝑐,𝑥) ∈𝑣 𝑥. A valuation for a set 𝑆 (with respect to C) is a function 𝜑 that maps each
element of 𝑆 to a corresponding value decomposition, that is, 𝜑 : 𝑆 → (C → R) (or, 𝜑 : 𝑆 → RC). We write ˆ𝜑 (𝐴)

to denote the total value of 𝐴’s value decomposition.

The value decompositions for the alternatives in our example can be derived from Figure 2 by multiplying the

scores in each column by the weighting factor for that column, see Figure 5.

To explain why BMW is the best choice, we have to speciﬁcally explain why it was chosen over Honda, the

second-best alternative. We can also explain why BWM was chosen over Toyota, but that decision is not as close

and therefore not as much in need of an explanation. We therefore focus on comparing BMW with Honda.

Focusing on 𝐵 and 𝐻 with their respective value decompositions 𝑣𝐵 = {p ↦→ .08, fe ↦→ .37, sr ↦→ .30, c ↦→ .20}

and 𝑣𝐻 = {p ↦→ .09, fe ↦→ .40, sr ↦→ .23, c ↦→ .17} leads to the valuation 𝜑 = {𝐵 ↦→ 𝑣𝐵, 𝐻 ↦→ 𝑣𝐻 }. The elements

7

p

fe

sr

c

𝜏

Toyota

Honda

0.10

0.34

0.27

0.14

0.85

0.09

0.40

0.23

0.17

0.89

BMW 0.08

0.37

0.30

0.20

0.95

Figure 5: Value decompositions for the car selection example

of 𝑆 can be ordered based on the valuation totals in an obvious way:

∀𝑋, 𝑌 ∈ 𝑆. 𝑋 > 𝑌 ⇔ ˆ𝜑 (𝑋 ) > ˆ𝜑 (𝑌 )

When we ask why alternative 𝑋 was chosen over 𝑌 , the obvious explanation is to give the valuation totals, which

provide the justiﬁcation ˆ𝜑 (𝑋 ) > ˆ𝜑 (𝑌 ). However, such an answer might not be useful, since it ignores the categories

that link the raw numbers to the application domain and thus lacks a context to interpret the numbers.

In our

example, BMW is chosen, since ˆ𝜑 (𝐵) = 0.95 > ˆ𝜑 (𝐻 ) = 0.89, which might be surprising because Honda is clearly

cheaper as well as more fuel eﬃcient.

If the value decomposition is maintained during the computation, we can generate a more detailed explanation.

First, we can rewrite ˆ𝜑 (𝑋 ) > ˆ𝜑 (𝑌 ) as ˆ𝜑 (𝑋 ) − ˆ𝜑 (𝑌 ) > 0, which suggests the deﬁnition of the valuation diﬀerence
between two elements 𝑋 and 𝑌 as follows.

𝛿𝜑 (𝑋, 𝑌 ) = {(𝑐, 𝑥 − 𝑦) | (𝑐, 𝑥) ∈ 𝜑 (𝑋 ) ∧ (𝑐, 𝑦) ∈ 𝜑 (𝑌 )}

(In the following we will omit 𝜑 whenever it is clear from the context.) The total of the valuation diﬀerence
ˆ𝛿 (𝑋, 𝑌 ) is given by the sum of all components, just like the total of a value decomposition. In our example we
have 𝛿 (𝐵, 𝐻 ) = {p ↦→ −.01, fe ↦→ −.03, sr ↦→ .07, c ↦→ .03}. It is clear that the valuation diﬀerence generally

contains positive and negative entries and that for 𝛿 (𝑋, 𝑌 ) > 0 to hold, the sum of the positive entries must exceed

the absolute value of the sum of the negative entries. We call the negative components of a valuation diﬀerence its

barrier. It is deﬁned as follows.

𝛽𝜑 (𝑋, 𝑌 ) = {(𝑐, 𝑥) | (𝑐, 𝑥) ∈ 𝛿𝜑 (𝑋, 𝑌 ) ∧ 𝑥 < 0}

The total value ˆ𝛽 (𝑋, 𝑌 ) is again the sum of all the components.
In our example we have 𝛽 (𝐵, 𝐻 ) = {p ↦→
−0.01, fe ↦→ −.03} and ˆ𝛽 (𝐵, 𝐻 ) = −.04. The decision to select 𝑋 over 𝑌 needs as support some, but not necessarily
all, of the positive components of 𝛿 (𝑋, 𝑌 ), which are called the dominator candidates and which are deﬁned as

8

follows.

Γ𝜑 (𝑋, 𝑌 ) = {(𝑐, 𝑥) | (𝑐, 𝑥) ∈ 𝛿𝜑 (𝑋, 𝑌 ) ∧ 𝑥 > 0}

Any subset of Γ(𝑋, 𝑌 ) whose total is larger than | ˆ𝛽 (𝑋, 𝑌 )| will suﬃce as an explanation. We call such a subset a
dominator. The set of all dominators is deﬁned as follows.

Δ𝜑 (𝑋, 𝑌 ) = {𝐷 | 𝐷 ⊆ Γ𝜑 (𝑋, 𝑌 ) ∧ ˆ𝐷 > | ˆ𝛽 (𝑋, 𝑌 )|}

In our example we have two dominators, that is, Δ (𝐵, 𝐻 ) = {{sr ↦→ .07}, {sr ↦→ .07, c ↦→ .03}}. The smaller

a dominator, the better it is suited as an explanation, since it requires fewer details to explain how the barrier is

overcome. We therefore deﬁne a minimal dominating set (MDS) as any dominator with the fewest possible number
of dominator candidates.

Δ𝜑 (𝑋, 𝑌 ) = {𝐷 | 𝐷 ⊆ Δ𝜑 (𝑋, 𝑌 ) ∧ 𝐷 (cid:48) ⊂ 𝐷 ⇒ 𝐷 (cid:48) ∉ Δ (𝑋, 𝑌 )}

Note that Δ may contain multiple elements, which means that minimal dominators are in general not unique.

In other words, a decision may have diﬀerent minimally sized explanations. In our example, the only MDS is

Δ (𝐵, 𝐻 ) = {sr ↦→ .07}; it captures the explanation that BMW is to be preferred over Honda due to the signiﬁcant

diﬀerence in the safety ratings of the two cars alone; we don’t have to mention comfort at all to explain the decision.

We can apply the described technique to WPM examples by simply using multiplication for aggregation (using

the multiplicative identity 1) and division for computing valuation diﬀerences. The barrier set then consists of all

the components with values less than 1, and the dominator set consists of components with values greater than 1.

An MDS explanation in this case is the smallest subset of dominator components whose product of component

values will exceed that of the inverse of product of all the component values in the barrier set. Alternatively, we

could apply the log transform to the individual component values and then use the additive version of MDS.

4 Hierarchical Value Decomposition

The idea of value decomposition relies on the fact that each alternative consists of a ﬂat list of attributes. In contrast,

attributes in the AHP setting are recursively decomposed into sub-attributes forming a hierarchical structure, which

raises the question of whether the idea of MDS-based explanations can also work for hierarchical decision-making

methods. This would require extending the concepts of value decomposition and dominators meaningfully to the

hierarchical case.

9

To this end, we deﬁne the concept of a hierarchical value decomposition, which records the individual
contributions of the attributes at the various levels towards the overall priority of an alternative. The hierarchical

value decomposition for a priority value of an alternative in AHP results in a tree that maps attributes to values.

Since the synthesis of priorities in AHP consists of multiplication of decision matrices, we need a way to trace this

matrix multiplication to come up with the tree structure, which we describe in this section. This tree forms the

basis of explanations in the hierarchical case, which we describe in Section 4.2.

4.1 Tracing Matrix Multiplication with Value Decomposition Trees

Consider the decision matrix 𝐵3 = 𝐶 from our example. Multiplying 𝐶 with 𝐵2 = 𝐹 yields a 2 × 2 matrix, where

each element is the sum of 3 products.

.6

.4








.6

.4


.3



.7



.5

.3

.2












.2



.4



.4



.6 · .5 + .6 · .3 + .3 · .2

. . .

=









.6 · .2 + .6 · .4 + .3 · .4






. . .

Any such sum of products can be visually represented as a tree whose leaf nodes contain the products and whose

internal nodes contain the sum of the values of its children. Here are the trees for the elements of the ﬁrst row of

the resulting matrix.

.6 · .5 + .6 · .3 + .3 · .2

.6 · .2 + .6 · .4 + .3 · .4

.6 · .5

.6 · .3

.3 · .2

.6 · .2

.6 · .4

.3 · .4

In the next step we multiply the result of 𝐵3𝐵2 = 𝐶𝐹 with the ﬁrst decision matrix 𝐵1 = 𝐴, which means to multiply

each summand of each matrix element with another factor and creating new sums of the results.

𝐵3𝐵2𝐵1 = 𝐶𝐹𝐴 =

.6 · .5 + .6 · .3 + .3 · .2

. . .









.6 · .2 + .6 · .4 + .3 · .4






. . .









.6



.4



The ﬁrst entry of the resulting vector is given by the following value.

(.6 · .5 + .6 · .3 + .3 · .2) · .6

+ (.6 · .2 + .6 · .4 + .3 · .4) · .4

= (.6 · .5 · .6 + .6 · .3 · .6 + .3 · .2 · .6) + (.6 · .2 · .4 + .6 · .4 · .4 + .3 · .4 · .4)

Again, this sum of products can be represented by a tree, now with three levels.

10

honda : .516

personal: .324

expert: .192

p: .180

fe: .108

sr: .036

p: .048

fe: .096

sr: .048

(a) VD tree 𝜑honda showing the attribute contributions toward the overall priority of Honda

bmw : .484

personal: .276

expert: .208

p: .120

fe: .072

sr: .084

p: .032

fe: .064

sr: .112

(b) VD tree 𝜑bmw showing the attribute contributions toward the overall priority of BMW

Figure 6: Value decomposition trees for the hierarchical car example

.6 · .5 · .6 + .6 · .3 · .6 + .3 · .2 · .6 + .6 · .2 · .4 + .6 · .4 · .4 + .3 · .4 · .4

.6 · .5 · .6 + .6 · .3 · .6 + .3 · .2 · .6

.6 · .2 · .4 + .6 · .4 · .4 + .3 · .4 · .4

.6 · .5 · .6

.6 · .3 · .6

.3 · .2 · .6

.6 · .2 · .4

.6 · .3 · .4

.3 · .3 · .4

This tree represents the contribution of the various attributes toward the overall priority value of Honda (.516). For

example, the left subtree of the root represents the contribution of the personal opinion. Speciﬁcally, the root of

the left subtree contains the total of the personal opinion (.324), whereas the children contain the decomposition of

that value into the individual values for price (.180), fuel eﬃciency (.108), and safety ratings (.036) of the personal

opinion.

To assign meaning to the tree components, we can label them with the attribute names, which are already used

11

as row and column labels, linking the individual and aggregated value contributions to the attributes of the decision

problem. We can observe the following.

(A) The row labels of 𝐵𝑛 should label the roots of the trees for the resulting priority vector.

(B) The column labels of matrix 𝐵ℓ (for 1 < ℓ ≤ 𝑛), which are equal to the row labels of matrix 𝐵ℓ−1, should label

the nodes on level ℓ.

We call each such labeled tree a value decomposition tree, or VD tree for short. An example is shown in Figure 6a.
Now we describe a simple method to create VD trees from a sequence of matrices 𝐵1, . . . , 𝐵𝑛. Observations

(A) and (B) tell us that every path from root to a leaf in a VD tree is labeled by row labels taken from matrices

in the order 𝐵𝑛, 𝐵1, . . . , 𝐵𝑛−1. Let’s write [𝜔𝑛, 𝜔1, . . . , 𝜔𝑛−1] for the row indices corresponding to those labels.
For example, in Figure 6a the leftmost leaf is identiﬁed by the path of row labels (honda, personal, p), which

corresponds to the row indices (1, 1, 1), and the rightmost leaf is identiﬁed by the path (honda, expert, sr), which

corresponds to the row indices (1, 2, 3). The values in a VD tree are determined as follows.

1. Assign each leaf connected to the root a path of row labels with indices [𝜔𝑛, 𝜔1, . . . , 𝜔𝑛−1] the value

𝐵𝑛 [𝜔𝑛, 𝜔𝑛−1] × 𝐵𝑛−1 [𝜔𝑛−1, 𝜔𝑛−2] × . . . × 𝐵1 [𝜔1].

2. Assign each internal node the sum of the values of its children.

In the example from Figure 6a, the value of the leftmost leaf is therefore computed as 𝐵3 [1, 1] × 𝐵2 [1, 1] × 𝐵1 [1] =

.6× .5× .6 = .180. Similarly, the value of the rightmost leaf is computed as 𝐵3 [1, 3] ×𝐵2 [3, 2] ×𝐵1 [2] = .3× .4× .4 =

.048. The sums of the internal nodes and the root are computed in the obvious way.

4.2 Hierarchical Valuation Diﬀerences

To generate explanations from VD trees we have to generalize the concepts of valuation diﬀerence to the hierarchical

case.

The deﬁnition of valuation changes only slightly insofar as elements of the set 𝑆 (which are identical to the row

labels of 𝐵𝑛) are mapped to VD trees instead of plain value decompositions. For our car example the valuation is

𝜑 = {bmw ↦→ Vbmw, honda ↦→ Vhonda} (cf. Figure 6).

The concept of valuation diﬀerence then extends in a natural way to the hierarchical case. First, we write 𝑉 (ℓ)

for the value in the node that is identiﬁed by the path of labels ℓ from the root. Then the valuation diﬀerence 𝛿𝜑 (𝐴, 𝐵)

between two VD trees 𝑉𝐴 and 𝑉𝐵 is deﬁned as the VD tree V𝐴−𝐵 which has the same structure and labels as V𝐴 and

12

honda − bmw : .032

personal: .048

expert: -.016

p: .060

fe: .036

sr: -.048

p: .016

fe: .032

sr: -.064

Figure 7: Hierarchical valuation diﬀerence between Honda and BMW, represented as a VD tree Vhonda−bmw.

V𝐵 (except for the root label) such that for all root-path labels ℓ in V𝐴 except the root: V𝐴−𝐵 (ℓ) = V𝐴 (ℓ) − V𝐵 (ℓ).
The label of the root of V𝐴−𝐵 is 𝐴 − 𝐵, and the value of the root is V(𝐴) − V(𝐵). As an example, the VD tree
Vhonda−bmw is shown in Figure 7.

5 Hierarchical Explanations With Value Decomposition Trees

We saw in Section 4 that the children of any node in a VD tree represent a value decomposition for that node, which

means that the leaves of a VD tree represent the most granular value decomposition for the diﬀerence between

scores of the alternatives. The component value for such a value decomposition is the value at a given leaf node,

and the component label is the list of node labels on the path from the root to the leaf node. Since the root node label

is shared across all decompositions, we can usually remove it without losing any relevant information. Moreover,

by construction all non-leaf nodes in a VD trees represent redundant information, that is, the aggregation of the

values of their children. Therefore, each VD tree can be succinctly represented by its root-path-labeled leaves. For

our example, we have for Vhonda−bmw = 𝛿 (honda, bmw):

Vhonda−bmw (cid:27) {(personal, p) ↦→ .060, (personal, fe) ↦→ .036, (personal, sr) ↦→ −.048,

(expert, p) ↦→ .016, (expert, fe) ↦→ .032, (expert, sr) ↦→ −.064}

To this ﬂat mapping representation of hierarchical value decompositions we can now apply the concepts of

dominators and MDS as deﬁned in Section 3. For example, the barrier is given by the negative components

13

(personal, fe) and (expert, fe).

𝛽 (honda, bmw) = {(personal, sr) ↦→ −.048, (expert, sr) ↦→ −.064} and
ˆ𝛽 (honda, bmw) = −1.12.

That is, BMW has an advantage over Honda in the personal and expert opinion about safety ratings. We can justify

Honda as the preferred car with any dominating set, that is, any set of components whose sum exceeds the absolute

value of the barrier. Here we have two dominators.

Δ (honda, bmw) =

{{(personal, p) ↦→ .060, (personal, fe) ↦→ .036, (expert, fe) ↦→ .032},

{(personal, p) ↦→ .060, (personal, fe) ↦→ .036, (expert, p) ↦→ .016, (expert, fe) ↦→ .032}}

Δ (honda, bmw) = {(personal, p) ↦→ .060, (personal, fe) ↦→ .036, (expert, fe) ↦→ .032}

It is obvious that the ﬁrst dominator is the MDS in this case, since it is a proper subset of the second one. Interpreted

as an explanation, the MDS says that personal preference for Honda’s cheaper price as well as personal and expert

favorable opinion for its fuel eﬃciency more than compensates for BMW’s advantage in safety rating, making

Honda the preferred car overall.

6 Explanation Simpliﬁcation

Consider the MDS component (personal, p) ↦→ .060. Its attribute labels personal and p come from diﬀerent

levels of a VD tree. Comprehending such an explanation can be challenging, especially as the number of levels

increases. A simpliﬁed explanation that employs labels from just one level might be easier to understand and thus

may have more explanatory value, even if it is less speciﬁc.

The hierarchical decomposition of valuation diﬀerences makes it possible to provide explanations on diﬀerent

levels of the VD tree. In particular, the tree structure enables decision makers to inquire speciﬁc details about the

reasons for a decision at the various levels of the VD tree.

For example in the car selection decision an answer to the question “Why is Honda the preferred option with

respect to the decision makers?” is given by the children of Vhonda−bmw in Figure 7: The value -.016 for expert
represents a barrier, and the value .048 for personal is the MDS, which corresponds to the explanation that the

positive personal opinion of Honda outweighs the negative opinion of the experts. This explanation mentions only

two values and is simpler, albeit less speciﬁc, than the explanation given in the previous section.

14

Similarly, we could ask “Why is Honda the preferred option with respect to the features?”. The answer is given

by the children of Vpersonal and Vexpert in Vhonda−bmw: The overall value for sr (-0.064 + -0.048 = -0.112), obtained
by summing the sr values of Vpersonal and Vexpert, represents a barrier and the total values for p (0.060 + 0.016 =
0.076) and fe (0.036 + 0.032 = 0.068) obtained by summing the corresponding p and fe values of Vpersonal and
Vexpert is the MDS. This corresponds to the explanation that although the safety ratings are against Honda, those
are more than compensated by a better price and fuel eﬃciency. Note that an MDS for level ℓ + 1 is not a reﬁnement
of the MDS for level ℓ; rather they are independent explanations for the same outcome.

To formalize the focusing on diﬀerent levels in a VD tree, we need an operation for aggregating functions over

multiple domain values. Speciﬁcally, given 𝑓 : 𝐴 → R and 𝑔 : 𝐴 → 𝐵, the aggregation of 𝑓 with respect to 𝑔 is the
function 𝑓 /𝑔 : 𝐵 → R, deﬁned as follows.

𝑓 /𝑔 = {(𝑥 (cid:48), ∑︁

{𝑦 | (𝑥, 𝑦) ∈ 𝑓 , 𝑔(𝑥) = 𝑥 (cid:48)}) | 𝑥 (cid:48) ∈ 𝑔(𝐴)}

We can use this aggregation to create mappings that summarize the values of a VD tree on diﬀerent levels. Let 𝜋𝑛

be the function that selects (or projects onto) the 𝑛th element of a list or tuple. Then 𝑉 /𝜋𝑛 creates an aggregation

of the (root-path-labeled representation of the) VD tree that maps the labels on the 𝑛th level to their aggregated

values.

For example, the levels 2 and 3 of the VD tree Vhonda can be obtained as follows.

Vhonda/𝜋2 = {personal ↦→ .324, expert ↦→ .192}

Vhonda/𝜋3 = {p ↦→ .180 + .048, fe ↦→ .108 + .096, sr ↦→ .036 + .048}

= {p ↦→ .228, fe ↦→ .204, sr ↦→ .084}

Similarly, we can focus on diﬀerent levels of a VD tree that stores valuation diﬀerences, and we can also focus the

deﬁnitions of barrier, MDS, etc. by applying the corresponding function to the focused valuation diﬀerence.

𝛽ℓ
𝜑 (𝐴, 𝐵) = 𝛽𝛿𝜑 (𝐴,𝐵)/𝜋ℓ (𝐴, 𝐵)

Δℓ
𝜑 (𝐴, 𝐵) = Δ𝛿𝜑 (𝐴,𝐵)/𝜋ℓ (𝐴, 𝐵)

Γℓ
𝜑 (𝐴, 𝐵) = Γ𝛿𝜑 (𝐴,𝐵)/𝜋ℓ (𝐴, 𝐵)

Δℓ
𝜑 (𝐴, 𝐵) = Δ𝛿𝜑 (𝐴,𝐵)/𝜋ℓ

(𝐴, 𝐵)

Applying these deﬁnitions to level 2, we get the following valuation diﬀerence, barrier and, MDS explanation,

leading to the explanation we saw for level 2 at the beginning of this section.

𝛿 2(honda, bmw) = {personal ↦→ .048, expert ↦→ −.016}

𝛽 2 (honda, bmw) = {personal ↦→ .048}

Δ2(honda, bmw) = {expert ↦→ −.016}

15

Figure 8: AHP model for selecting materials to build bridges. Figure taken from Smith et al. (1997).

Similarly, we can compute these values for level 3 explanations.

𝛿 3 (honda, bmw) = {p ↦→ .076, fe ↦→ .068, sr ↦→ −.112}

𝛽 3(honda, bmw) = {p ↦→ .076, fe ↦→ .068}

Δ3 (honda, bmw) = {sr ↦→ −.112}

7 Case Studies of Applications of MDS to AHP

In this section we apply our explanation mechanism to two real-world AHP applications.

7.1 Selecting Materials to Build Bridges in Rural Winsconsin Counties

Figure 8 shows the AHP model for the problem of selecting the best material amongst prestressed concrete (p),

steel (s), timber (t), and reinforced concrete (r) to build bridges in the rural counties of Wisconsin (Smith et al.

16

𝐵1

importance

𝐵T
2

pp

ls mn rs

ic

lc

dot

consultants

officials

.4

.2

.4

dot

.28 .28 .17 .08 .10 .09

consultants

.08 .08 .35 .08 .32 .09

officials

.14 .12 .22 .31 .10 .11

𝐵3

pp

ls mn rs

ic

lc

p

s

t

r

.33 .27 .42 .32 .23 .28

.09 .16 .08 .09 .15 .09

.20 .23 .23 .26 .32 .31

.38 .34 .27 .33 .29 .31

Figure 9: Decision matrices for the bridge material selection problem.

1997). The decision to select the best material takes into account the various stakeholders in the process (the state

department of transport (dot), the private consultants (consultants), and the county highway oﬃcials (officials))

who base their preferences of the materials on their characteristics such as past performance (pp), lifespan (ls),

maintenance requirements (mn), resistance to natural deterioration (rs), initial cost (ic), and life cycle cost (lc).

The decision matrices 𝐵1, 𝐵2, and 𝐵3 for the problem are shown in Figure 9. Note that the transposed matrix of 𝐵2

is shown in the ﬁgure for easier presentation.

The synthesis of priorities produces the following result for the various building materials and concludes that

reinforced concrete is the best material to build the bridges, since it has a slightly higher priority value than the

prestressed concrete.

0.319372













Since the p and r scores are very close, one might wonder why that is the case and which assumptions lead to this

Prestressed Concrete (p)

Reinforced Concrete (r)

𝑊 = 𝐵3𝐵2𝐵1 =

Timber (t)

0.320409

0.109007

0.251212

Steel (s)














conclusion.

For lack of space, we show in Figure 10 only the hierarchical valuation diﬀerence (and not the individual VD

trees) for the two best alternatives. From this we can generate an explanation for why reinforced concrete was

17

Figure 10: Hierarchical valuation diﬀerence between r and p in the bridge selection example.

preferred. The barrier comprises the opinions of the DOT, the consultants, and county oﬃcials with regard to the

maintenance costs.

𝛽 (r, p) = {(dot, mn) ↦→ −.0102, (consultants, mn) ↦→ −.0105, (officials, mn) ↦→ −.0132}

The barrier against the reinforced concrete is overcome with the following MDS explanation.

Δ (r, p) = {(dot, pp) ↦→ .0056, (dot, ls) ↦→ .00784, (dot, ic) ↦→ .002424, (dot, lc) ↦→ .001091,

(consultants, pp) ↦→ .0008, (consultants, ls) ↦→ .00112, (consultants, ic) ↦→ .003879,

(officials, pp) ↦→ .0028, (officials, ls) ↦→ .00336, (officials, rs) ↦→ .00124,

(officials, ic) ↦→ .002424, (officials, lc) ↦→ .001333}

Since the priorities for reinforced and prestressed concrete are very close, the MDS contains a large number of

components, which might be diﬃcult to interpret. We can help by generating single-level explanations, either

in terms of the decision criteria or the decision makers. The valuation diﬀerence in terms of decision criteria is

obtained by focusing on level 3.

𝛿 3(r, p) = {pp ↦→ .0092, ls ↦→ .01232, mn ↦→ −.0339, rs ↦→ .00172, ic ↦→ .008727, lc ↦→ .00297}

In the valuation diﬀerence, maintenance is the only component acting as a barrier 𝛽 3 (r, p) = {mn ↦→ −.0339}. The

18

MDS explanation consists of the remaining components as shown below.

Δ3 (r, p) = {pp ↦→ .0092, ls ↦→ .01232, rs ↦→ .00172, ic ↦→ .008727, lc ↦→ .00297}

The explanation at the level of decision criteria can be read like this: Although reinforced concrete has a disadvantage

in terms of maintenance requirements, the cumulative advantage it has for the remaining criteria makes up for this

disadvantage.

Similarly, we can get an explanation regarding decision makers by focusing on level 2.

𝛿 2(r, p) = {dot ↦→ .007075, consultants ↦→ −.003996, officials ↦→ −0.002042}

The consultants and county highway oﬃcials act as the barrier, but the MDS Δ2 (r, p) = {dot ↦→ .007075} tells us
that the preference of the DOT compensates for this disadvantage.

7.2 Supreme Court Rulings on Abortion

Roe v. Wade (US Supreme Court 1973) was a landmark decision of the U.S. Supreme Court in which the Court

ruled that the Constitution of the United States generally protects a pregnant woman’s liberty to choose to have an

abortion. However, it was recently overturned by the Supreme Court, sparking oﬀ an intense public debate. It is

in this context that the current AHP example (Saaty and Vargas 2012), describing an AHP model to predict the

Supreme Court ruling on a related issue of Roe v. Wade in 1992, and our techniques explaining why those outcomes

were predicted gain special relevance.

In the summer of 1992 the Supreme Court of the United States was supposed to rule on a controversial

Pennsylvania statute restricting the rights of women in obtaining an abortion.

Included in this statute were

provisions requiring that doctors provide women with state-prescribed information about pregnancy and abortion,

that the procedure be delayed 24 hours after the recitation, and that husbands be notiﬁed prior to the procedure.

The lower court upheld the ﬁrst two provisions, but declared unconstitutional the husband notiﬁcation requirement.

The AHP model for the example is shown in Figure 11. It correctly predicted that the Supreme Court will uphold

at least parts of the Pennsylvania statue and will, as a result, weaken the rights of women who choose to have an

abortion in the state of Pennsylvania.

The model uses the nine Supreme Court justices as the criteria, giving each of them an equal weight. Beneath

each justice there are ﬁve sub-criteria that were determined to be the most important for the judges to adjudicate

on the matter.

19

Figure 11: AHP model for predicting the Supreme Court decision on Pennsylvania abortion issue. Figure taken

from Saaty and Vargas (2012).

• Women’s issues (w) These are issues deemed important by the pro-choice movement, such as the constitu-

tional right of each woman to make her own decisions regarding her body.

• Precedent (p) Cases that have gone before the Supreme Court since the early 1970s.

• Moral issues (m) Constitutional rights of the fetuses and the belief that abortion is murder.

• Political issues (o) To make the decision-making process easier, the political issues are deﬁned as con-
servatism. The original paper determined that conservatives are more pro-life than liberals. It also links

Republicans with conservatism and Democrats with liberalism.

• Biological issues (b) The medical concept of viability that speciﬁes a certain time when the fetus is capable

of independent survival outside the mother’s womb.

The AHP model envisaged three likely outcomes of the ruling: overturn, uphold, or weaken Roe v. Wade by

giving states more independent power to restrict abortions. The original paper used many experts’ opinions from

20

books and law journals to determine how each justice will weight each criterion and how each sub-criterion will

aﬀect the alternative selected.

The synthesis of priorities gives us priorities of 0.378, 0.394, and 0.228 for overturn, weaken and uphold,

respectively. The weaken rather than the overturn verdict is surprising, given the strong conservative leaning of the

court. We can try to explain this surprising decision using our techniques. As a ﬁrst step, we need to compute the

hierarchical valuation diﬀerence. However, the original paper doesn’t provide the matrix between levels 3 and 4 of

the AHP model in Figure 11. Due to lack of this matrix, we can’t create the VD trees for various alternatives in

the usual way. Interestingly, the paper provides a trace of the priority synthesis step as shown in Figure 12a, which

facilitates the computation of the VD trees for various alternatives, as shown for the overturn alternative in Figure

12b.

Once we have the VD trees for the two alternatives, we can compute the hierarchical valuation diﬀerence

between the two alternatives, as shown in Figure 13. The barrier for the decision shows that moral considerations

and political leaning of all the 9 justices are for overturning Roe v. Wade rather than weakening it.

𝛽 (weaken, overturn) =

{(1, m) ↦→ −.003, (2, m) ↦→ −.004, (3, m) ↦→ −.005, (4, m) ↦→ −.055, (5, m) ↦→ −.051, (6, m) ↦→ −.006,

(7, m) ↦→ −.004, (8, m) ↦→ −.051, (9, m) ↦→ −.048, (1, o) ↦→ −.004, (2, o) ↦→ −.013, (3, o) ↦→ −.003,

(4, o) ↦→ −.008, (5, o) ↦→ −.016, (6, o) ↦→ −.007, (7, o) ↦→ −.003, (8, o) ↦→ −.016, (9, o) ↦→ −.006}

However, the respect for precedent for all justices, ambiguity deﬁning viability for all but justice 5 (Scalia) along

with the consideration of the women issues by justices 1 and 7 (Blackmun and Stevens, respectively) lead to

overcoming the barrier. This is a minimal explanation for why a weaken rather than an overturn verdict was

reached.

Δ (weaken − overturn) =

{(1, p) ↦→ .022, (2, p) ↦→ .045, (3, p) ↦→ .031, (4, p) ↦→ .005, (5, p) ↦→ .006, (6, p) ↦→ .049,

(7, p) ↦→ .030, (8, p) ↦→ .006, (9, p) ↦→ .008, (1, b) ↦→ .008, (2, b) ↦→ .007, (3, b) ↦→ .027,

(4, b) ↦→ .006, (6, b) ↦→ .005, (7, b) ↦→ .016, (8, b) ↦→ .004, (9, b) ↦→ .017, (1, w) ↦→ .009, (7, w) ↦→ .005}

Since barrier as well as MDS contain a large number of components, focusing on speciﬁc levels of the hierarchy

can simplify the explanation. An explanations which focuses on the sub-criteria used by justices is based on level

21

(a) Trace of synthesis of priorities. Figure taken from Saaty and Vargas (2012).

(b) VD tree for the overturn alternative.

Figure 12: Synthesis of priorities in the Roe v. Wade example

22

Figure 13: Hierarchical valuation diﬀerence between overturn and weaken in the Roe v. Wade example.

3 of the value diﬀerence.

𝛿 3 (weaken − overturn) = {w ↦→ .022, p ↦→ .207, m ↦→ −.227, o ↦→ −.076, b ↦→ .090}

The corresponding barrier and MDS explanation are shown below.

𝛽 3(weaken − overturn) = {m ↦→ −.227, o ↦→ −.076}

Δ3 (weaken − overturn) = {w ↦→ .022, p ↦→ .207, b ↦→ .090}

We observe that although moral consideration and political aﬃliation of the judges supports overturn of Roe v.

Wade, their consideration for women’s issues, precedent, and diﬃculty around deﬁning viability outweigh this

support, resulting in the less extreme verdict of weaken. It is interesting to note that a 7-2 conservative-leaning

Supreme Court decided to just weaken Roe v. Wade in 1992 whereas a 6-3 conservative-leaning court overturned

it in 2022.

We can ﬁnd out which judges were responsible for the weaken verdict by focusing on level 2.

𝛿 2(weaken − overturn) = {1 ↦→ .032, 2 ↦→ .035, 3 ↦→ .052, 4 ↦→ −.051, 5 ↦→ −.061, 6 ↦→ .044,

7 ↦→ .049, 8 ↦→ −.056, 9 ↦→ −.028}

The barrier and MDS explanation tell us that justices 4, 5, 8, and 9 would most probably vote to overturn Roe

v. Wade, however, the majority will prefer to weaken but uphold it. The ﬁnal verdict (US Supreme Court 1992)

23

showed the same voting pattern as predicted by our explanation here.

𝛽 2(weaken, overturn) = {4 ↦→ −.051, 5 ↦→ −.061, 8 ↦→ −.056, 9 ↦→ −.028}

Δ2 (weaken, overturn) = {1 ↦→ .032, 2 ↦→ .035, 3 ↦→ .052, 6 ↦→ .044, 7 ↦→ .049}

8 Evaluation

To assess the eﬀectiveness of MDS explanations for AHP decisions, we have performed a number of experiments

to estimate the reduction in complexity that they can be expected to deliver. In the following we describe the setup

and results of these experiments.

First, we have to establish criteria to measure the eﬃcacy of explanations. Without any speciﬁc explanation,

a user has to inspect all 𝑛 components of a value decomposition generated by the AHP process. The explanatory

strength of an MDS comes from the fact that it can often reduce this number considerably to, say, 𝑚. The reduction

can then be captured by deﬁning the explanatory ratio of an MDS as 𝑚/𝑛. The smaller the ratio, the fewer
components users have to look at, relative to the original decision, thus making it easier to understand. We can

express the same idea more intuitively as a percentage size reduction achieved. We thus deﬁne the MDS reduction
as 𝑅 = (1 − 𝑚/𝑛) × 100, that is, an explanation ratio of 0.15 translates into a reduction by 85%.

8.1 Eﬃcacy of MDS Explanations

Since there are no AHP benchmark data sets available, we have generated data for evaluating the eﬃcacy of MDS

explanations. The examples reported in the literature indicate that AHP models rarely have more than 6 levels. Yet,

each dimension can be wide: For example, an AHP model with 51 attributes in one dimension can be found in (Liu

et al. 2008). In general, it is common for an AHP to have about 10 attributes in one of the dimensions (Pan 2008).

Based on these observations, we have randomly generated data for AHP examples having between 3 and 6

levels and computed the reduction for each case. A 3 level AHP is essentially a linear MCDM. The examples in

the literature suggest limiting the number of components to 30 for models with 3 levels. For problems with 4, 5,

or 6 levels, we limit the total number of components in the corresponding value decomposition of an AHP to 100,

with intermediate dimensions having between 2 to 10 components each. We have used 20,000 random inputs for

each scenario. For an AHP problem with a ﬁxed number of levels, the inputs vary in two regards: (a) the number

of attributes at each level, and (b) the values of the decision matrices.

Another aspect that should be reﬂected in the test data is whether an explanation is necessary at all. For

24

example, when the ﬁrst alternative from an AHP process is better than the runner-up in every regard, no explanation

is necessary. In contrast, an explanation is most helpful in cases when the two alternatives are really close, that is,

when the priority values of the alternatives are similar. To reﬂect this situation, we ﬁlter out those cases whose ﬁrst

two alternatives are not close. We call the relative diﬀerence between the priority values of two alternatives their

decision margin and consider scenarios in which the decision margin is bounded to 1%, 5%, 10%, 20%, and 30%.
Figure 14 shows how MDS reduction varies with the total number of components. We show graphs for AHPs

with diﬀerent number of levels containing plots for diﬀerent decision margins.

The plots reveal some interesting trends. First, on average an MDS can prune the number of components

by about 55-60% even for a decision margin as low as 1%. Second, the reduction decreases with smaller

decision margins, which makes intuitive sense, since a greater value distance between alternatives provides more

opportunities to explain the diﬀerence with fewer components. But unfortunately, this also means that the eﬃcacy

of MDS explanation shrinks when they might be needed most. Third, with an increasing number of levels, the

curves “move upward”, that is, for a given decision margin the reduction increases with the number of levels in the

AHP problems. In other words, MDS explanations scale well with the structural complexity of AHP problems.

8.2 Eﬃcacy of Single-Level Explanations

Consider the example from Section 7.2 where levels 2 and 3 consists of 9 and 5 components, respectively. Therefore,

the total number of components in the value decomposition is 45 (= 9×5), whereas a simpliﬁed explanation contains

no more than the sum of the number of components in the two levels, that is, 14 (= 9 + 5).

We can measure the improvement of a single-level explanation over an MDS as the size reduction given by

𝑅 = (1 − 𝑠/𝑚) × 100 where 𝑚 is the size of the MDS and 𝑠 is the size of the single-level explanation. With this

deﬁnition, single-level explanations promise an improvement in the Roe v. Wade example of at least 26%. The

actual improvement was (1 − 5+3

19 ) × 100 = 58%, which shows that the improvement can be signiﬁcantly better than

indicated by the worst-case estimate.

Figure 15 shows the percentage improvement for simpliﬁed explanations over MDS. The 𝑥 axis shows the

number of explanation components. For lack of space, we present the data only for AHP problems with 4 levels.

However, the same trends can be observed for AHP problems with 5 and 6 levels.

We can observe that for this worst-case consideration, savings can be obtained only for models with a signiﬁcant

number of components. But in general, the situation will be much better, since on average a single-level explanation

will contain only half the number of components (because the other half will be used for the barrier).

25

(a) Number of levels: 3

(b) Number of levels: 4

(c) Number of levels: 5

(d) Number of levels: 6

Figure 14: Average MDS reduction (𝑦 axis) dependent on the number of components (𝑥 axis) for AHP problems

with diﬀerent number of levels. Decision margins:

≤ 30%,

≤ 20%,

≤ 10%,

≤ 5%,

≤ 1%

26

Figure 15: Size reduction of single-level explanations over MDS (𝑦 axis) dependent on the number of components

(𝑥 axis) for AHP problems with 4 levels. Decision margins:

≤ 30%,

≤ 10%,

≤ 1% (solid lines: worst

case, dash-dot lines: average case).

9 Related Work

Sensitivity analysis (Triantaphyllou and Sánchez 1997) is the tool of choice employed by decision makers to

comprehend the results of various MCDM methods, including AHP. Sensitivity analysis is usually the only

explanation mechanism available to a decision maker. Despite being useful, a potential limitation of sensitivity

analysis is that it can only analyze the impact of one attribute at a time, keeping other attributes values constant.

Thus, sensitivity analysis produces a number of localized explanations. In comparison, our value-decomposition

explanation method is global, and an MDS explanation takes into consideration the combined impact of various

attributes in the decision, leading to generally more accurate and comprehensive explanations. On the other hand,

MDS explanations are larger than the variation of one attribute, but the size of MDS explanations can be eﬀectively

reduced by employing single-level explanations.

The topic of explanations in general has been explored in a number of diﬀerent areas. While the origins of

research into the nature of explanations can be traced back to philosophy (Hempel 1965, Achinstein 1983, Ruben

1990), the need for explaining computation has recently received a lot of attention, speciﬁcally in the area of AI

(Miller 2019, Adadi and Berrada 2018).

The notion of a value decomposition was introduced in (Erwig and Kumar 2021a) as a structure for explaining

the results of dynamic programming algorithms. Value decomposition is generated as a domain-speciﬁc structure

there, using the fact that dynamic programming algorithms can be viewed as instances of a mathematical semiring

27

structure. This is similar to the current work, where the value decomposition is a domain-speciﬁc structure

generated from the computations of the MCDM problems. Another point of similarity is that both generate

contrastive explanations and thus also require two program results. In (Erwig and Kumar 2021b) we describe a

domain-speciﬁc language, which is based on the theory developed in this paper and allows users to specify MCDM

problems, synthesize priorities for various alternatives, and generate MDS explanations. That work is primarily

concerned with questions of language design and how to represent MCDM problems and explanations in support

of computational transformations.

Similar to the current approach, another domain-speciﬁc structure created explicitly for explanations are

provenance traces (Acar et al. 2012). A provenance trace consists of meta-information about the origin, history, or

derivation of an object which is used in establishing trust and providing security in computer systems, particularly

on the web. Like value decompositions, provenance traces are a domain-speciﬁc explanation structure that works

only in certain situations.

10 Conclusions

We have demonstrated an eﬀective method for explaining the results of MCDM methods. Our approach of using

minimal dominating sets is general enough to work well for ﬂat and hierarchical models. Through the concept of

single-level explanations, users have the option to additionally get simpliﬁed explanations. As with explanations

for algorithmic systems in general, the ability for generating concise explanations can contribute to the acceptance

of results and adds transparency to computational systems.

Acknowledgement

This work is partially supported by the National Science Foundation under the grants CCF-1717300 and CCF-

2114642.

References

Acar UA, Ahmed A, Cheney J, Perera R (2012) A Core Calculus for Provenance. Int. Conf. on Principles of Security

and Trust, 410–429.

Achinstein P (1983) The Nature of Explanation (New York, NY: Oxford University Press).

28

Adadi A, Berrada M (2018) Peeking Inside the Black-Box: A Survey on Explainable Artiﬁcial Intelligence (XAI).

IEEE Access 6:52138–52160.

Erwig M, Kumar P (2021a) Explainable Dynamic Programming. Journal of Functional Programming 31(e10).

Erwig M, Kumar P (2021b) MADMAX: A DSL for Explanatory Decision Making. ACM SIGPLAN Conf. on

Generative Programming: Concepts & Experiences, 144–155.

Faulhaber AK, Ni I, Schmidt L (2021) The eﬀect of explanations on trust in an assistance system for public transport

users and the role of the propensity to trust. Mensch Und Computer 2021, 303–310, MuC ’21 (New York,
NY, USA: ACM).

Fishburn P (1967) Additive utilities with incomplete product sets: Application to priorities and assignments.

Operations Research 15(3):537–542.

Garﬁnkel P (1981) Forms of Explanation (New Haven, CT, USA: Yale University Press).

Goodman B, Flaxman S (2017) European union regulations on algorithmic decision-making and a “right to

explanation”. AI Magazine 38:50–57.

Harker P, Vargas L (1987) The theory of ratio scale estimation: Saaty’s analytic hierarchy process. Management

Science 33:1383–1403.

Hempel C (1965) Aspects of Scientiﬁc Explanation and Other Essays in the Philosophy of Science (New York, NY:

Free Press).

Lipton P (1990) Contrastive Explanation. Royal Institute of Philosophy Supplement 27:247–266.

Lipton P (2004) Inference to the Best Explanation (New York, NY, USA: Routledge).

Liu L, Berger P, Zeng AZ, Gerstenfeld A (2008) Applying the analytic hierarchy process to the oﬀshore outsourcing

location decision. Supply Chain Management 13:435–449.

Miller T (2019) Explanation in Artiﬁcial Intelligence: Insights from the Social Sciences. Artiﬁcial Intelligence

267:1–38.

Pan N (2008) Fuzzy AHP approach for selecting the suitable bridge construction method. Automation in Construc-

tion 17:958–965.

Ruben DH (1990) Explaining Explanation (London, UK: Routledge).

Saaty RW (1987) The Analytic Hierarchy Process—what it is and how it is used. Mathematical Modelling 9(3):161–

176.

29

Saaty TL (1990) How to make a decision: The analytic hierarchy process. European Journal of Operational

Research 48(1):9–26.

Saaty TL (2002) Decision making with the Analytic Hierarchy Process. International Journal of Services Sciences

1:83–98.

Saaty TL (2003) The negotiation and resolution of the conﬂict in South Africa: The AHP. ORiON 4, URL

http://dx.doi.org/10.5784/4-1-488.

Saaty TL, Cho Y (2001) The decision by the US congress on China’s trade status: a multicriteria analysis.

Socio-Economic Planning Sciences 35(4):243 – 252.

Saaty TL, Vargas L (2001) Models, Methods, Concepts & Applications of the Analytic Hierarchy Process.

Saaty TL, Vargas LG (2012) Abortion and the States: How will the Supreme Court Rule on the Upcoming

Pennsylvania Abortion Issue?, 281–289 (Boston, MA: Springer US).

Selbst AD, Powles J (2017) Meaningful information and the right to explanation. International Data Privacy Law

7:233–242.

Smith R, Bush R, Schmoldt D (1997) The selection of bridge materials utilizing the analytical hierarchy process .

Tang V, Collar E (1992) IBM AS/400 new product launch process ensures satisfaction. Long Range Plan-
ning 25(1):22 – 27, ISSN 0024-6301, URL http://www.sciencedirect.com/science/article/pii/
002463019290306M.

The French Government (2016) Digital Republic Act of France. https://www.republique-numerique.fr/

pages/digital-republic-bill-rationale.

Tofallis C (2014) Add or multiply? a tutorial on ranking and choosing with multiple criteria. INFORMS Transactions

on Education 14(3):109–119.

Triantaphyllou E, Sánchez A (1997) A sensitivity analysis approach for some deterministic multi-criteria decision-

making. Decision Sciences 28:151–194.

US Supreme Court (1973) Roe v. Wade, 410 U.S. 113 (1973). https://supreme.justia.com/cases/federal/

us/410/113/.

US Supreme Court (1992) Planned Parenthood v. Casey. https://www.law.cornell.edu/supremecourt/

text/505/833.

30

