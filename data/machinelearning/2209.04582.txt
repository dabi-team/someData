2
2
0
2

p
e
S
0
1

]
I

A
.
s
c
[

1
v
2
8
5
4
0
.
9
0
2
2
:
v
i
X
r
a

Explaining Results of Multi-Criteria Decision Making

Martin Erwig, Prashant Kumar

{erwig,kumarpra}@oregonstate.edu

Abstract

We introduce a method for explaining the results of various linear and hierarchical multi-criteria

decision-making (MCDM) techniques such as WSM and AHP. The two key ideas are (A) to main-

tain a ï¬ne-grained representation of the values manipulated by these techniques and (B) to derive

explanations from these representations through merging, ï¬ltering, and aggregating operations. An

explanation in our model presents a high-level comparison of two alternatives in an MCDM problem,

presumably an optimal and a non-optimal one, illuminating why one alternative was preferred over the

other one. We show the usefulness of our techniques by generating explanations for two well-known

examples from the MCDM literature. Finally, we show their eï¬ƒcacy by performing computational

experiments.

Key words : Multiple criteria analysis, MCDM, AHP, WSM, contrastive explanations

1 Introduction

The theory and methods of multi-criteria decision-making (MCDM) have been extensively applied in many areas,

ranging from engineering projects, economics, public administration, to management and military projects. For

example, in 1986 the Institute of Strategic Studies in Pretoria, a government-backed organization, used the Analytic

Hierarchy Process (AHP) (Saaty 1990, 1987) to analyze the conï¬‚ict in South Africa and recommended actions

ranging from the release of Nelson Mandela to the removal of apartheid and the granting of full citizenship and

equal rights to the black majority (Saaty 2003). All the recommendations were implemented within a short time.

Another high-proï¬le example is the use of AHP in the 1995 US/China conï¬‚ict over Chinese illegal copying of

music, video, and software (Saaty and Cho 2001). An AHP analysis involving four hierarchies for beneï¬ts, costs,

1

 
 
 
 
 
 
opportunities, and risks showed, surprisingly, that it was much better for the United States not to sanction China.
The result of the study predicted what happened. Shortly after the study was complete, the United States awarded

China the most-favored nation status and didnâ€™t sanction it. In the domain of business, the Xerox Corporation has

used AHP to allocate almost a billion dollars to its research projects (Saaty 2002), and IBM used AHP in 1991 in

designing its successful mid-range AS 400 computer (Tang and Collar 1992).

Given the wide-spread use and impact of MCDM methods, it is natural for decision makers to ask why a

certain alternative was suggested. Automation systems can earn their usersâ€™ trust by explaining results because
explanations give users conï¬dence in the correctness and reliability of computation processes (Faulhaber et al.

2021), in particular when the recommended actions are counter-intuitive, such as the recommendation not to

sanction China in the trade conï¬‚ict. However, existing MCDM techniques do not provide explanations about their

recommended actions.

With the ever-increasing impact of algorithms and mathematical models in our daily lives, there has been

an increased demand for their explainability, so much so that countries have started to incorporate the right to
explanation (Selbst and Powles 2017) of an algorithmâ€™s result impacting the life of its citizens as part of their legal
framework. The General Data Protection Regulation of the European Union (Goodman and Flaxman 2017) and

the Digital Republic Act of France (The French Government 2016) are examples of such regulations. This trend is

expected to continue, and therefore it stands to reason that sooner rather than later there will be an expectation, if

not an obligation, for modeling tools such as MCDM to explain their results.

When presented with a result by an algorithm, a natural question for users to ask is â€œWhy is the result ğ‘‹ and

not ğ‘Œ ?â€. Such a question calls for what the philosophy literature calls a contrastive explanation (Lipton 2004,
1990). A contrastive explanation compares two speciï¬c phenomena, the actual result (called fact or solution) and
a hypothetical alternative (called foil) and justiï¬es â€œWhy this [fact] rather than that [foil]?â€ (Garï¬nkel 1981). A
nice property of contrastive explanations is that they can be tailored to diï¬€erent users who may be wondering about

diï¬€erent aspects of a solution. Diï¬€erent (parts of) results may result in diï¬€erent foils and consequently in diï¬€erent

explanations.

In this paper, we present a method to automatically generate explanations for the various forms of MCDM,

using an explanation mechanism called minimal dominating sets (MDS) (Erwig and Kumar 2021a), which allows
explanations to focus on only the most relevant aspects of a decision, thereby allowing the generation of concise

explanations. The main contributions of this paper are the following.

â€¢ An MDS-based explanation method for the two linear MCDM techniques Weighted Sum Method

2

(WSM) (Fishburn 1967) and Weighted Product Method (WPM) (Tofallis 2014).

â€¢ Hierarchical value decomposition as a novel representation that facilitates the generalization of MDS

explanations to the hierarchical MCDM technique AHP.

â€¢ Experimental evidence for the eï¬€ectiveness of our approach.

The rest of this paper is structured as follows. After reviewing the various MCDM techniques in Section 2, we

present in Section 3 the MDS explanation technique for linear MCDM using linear value decomposition.

In

Section 4, we introduce hierarchical value decomposition as a generalization of the linear case. In Section 5, we

demonstrate how MDS explanations can be extended to work with this hierarchical structure. In Section 6, we

introduce a method for simplifying explanations to a more coarse-grained form, thereby making them easier to

understand. In Section 7 we apply our explanation techniques to two examples from the MCDM literature, and in

Section 8 we evaluate the eï¬€ectiveness of the MDS explanation mechanism. We discuss related work in Section 9

and present conclusions in Section 10.

2 Multi-Criteria Decision Making

MCDM is the process of making decisions in the presence of multiple, usually conï¬‚icting, criteria. In this context,

an alternative represents one of several choices available to the decision maker. The goal of MCDM is to identify
the best alternative. Each MCDM problem is associated with multiple attributes, which represent the decision
criteria. An attribute can be beneï¬cial or detrimental to an alternative. Each attribute has an associated weight that
signiï¬es the importance of that attribute. The weight of an attribute remains constant across all alternatives and

varies between 0 and 1. All the attribute weights should sum to 1.

When the number of attributes gets large, they can be arranged in a hierarchical manner so that attributes higher

up in the hierarchy, also called major attributes, aggregate the contributions of minor attributes that appear lower
in the hierarchy. In Section 2.1 we describe the two linear MCDM techniques WSM and WPM, followed by the

hierarchical technique AHP in Section 2.2.

2.1 Linear MCDM

A linear MCDM problem with ğ‘š alternatives and ğ‘› attributes can be expressed by an ğ‘š Ã—ğ‘› matrix, called a decision
matrix. Row ğ´ ğ‘— represents the attribute values of the ğ‘— th alternative, and row ğ¶ğ‘– represents the values of ğ‘–th attribute

3

Price

Eï¬ƒciency Safety Comfort

0.1 (âˆ’)

0.4 (+)

0.3 (+)

0.2 (+)

Toyota

$22,000

Honda

$25,000

BMW $27,000

32

38

35

8.5

7.5

9.6

6.7

7.9

9.2

Figure 1: WSM example: decision matrix for car selection

for diï¬€erent alternatives, where ğ‘¤ğ‘– is the weight of the ğ‘–th attribute. Each value ğ‘ ğ‘—ğ‘– is the value of the attribute ğ¶ğ‘–

for the alternative ğ´ ğ‘— .

C1

ğ‘¤1

ğ‘11

ğ‘21

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ğ‘ğ‘š1
ï£¯
ï£°

C2

ğ‘¤2

ğ‘12

ğ‘22
...
ğ‘ğ‘š2

. . .

. . .
. . .

. . .

Cn

ğ‘¤ğ‘›

ğ‘1ğ‘›

ğ‘2ğ‘›
...
ğ‘ğ‘šğ‘›

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

A1

A2
...
Am

As an example, consider the task of deciding which car to buy. Some attributes to be considered are price, fuel
eï¬ƒciency, safety rating, and comfort. A decision matrix for this MCDM problem is shown in Figure 1. Price is a
detrimental attribute, which is indicated by a minus sign after the weight, whereas all other attributes are beneï¬cial,

as indicated by the plus sign.

In general, the values for the diï¬€erent attributes are represented in diï¬€erent units and at diï¬€erent scales.

For instance, cost is measured in dollars whereas fuel eï¬ƒciency is measured in miles per gallon. To compare and

combine values of diï¬€erent attribute values, all values have to be normalized as follows. For a beneï¬cial attribute ğ¶ğ‘˜
and the alternative ğ´ğ‘™ , the normalized attribute value Â¯ğ‘ğ‘™ğ‘˜ corresponding to ğ‘ğ‘™ğ‘˜ is deï¬ned as Â¯ğ‘ğ‘™ğ‘˜ = ğ‘ğ‘™ğ‘˜ /max1â‰¤ ğ‘— â‰¤ğ‘š ğ‘ ğ‘—ğ‘˜ .
Similarly, in case the attribute ğ¶ğ‘˜ is detrimental, the attribute value Â¯ğ‘ğ‘™ğ‘˜ is deï¬ned as Â¯ğ‘ğ‘™ğ‘˜ = min1â‰¤ ğ‘— â‰¤ğ‘š ğ‘ ğ‘—ğ‘˜ /ğ‘ğ‘™ğ‘˜ .

With a normalized decision matrix we can compute the optimal solution for a problem as follows. The

contribution of each attribute is obtained by a linear binary function of its weight and value. The particulars of the

binary function depend on the speciï¬c MCDM method that is employed. The so-called score ğœ of an alternative is
the aggregation of the contributions of its attributes. For WSM, the contribution of attribute ğ¶ğ‘– for alternative ğ´ ğ‘—

4

Price Eï¬ƒciency Safety Comfort Score

0.1 (âˆ’)

0.4 (+)

0.3 (+)

0.2 (+)

ğœ

Toyota

1

Honda

0.88

BMW 0.81

0.84

1

0.92

0.89

0.78

1

0.73

0.86

1

0.85

0.89

0.95

Figure 2: Normalized decision matrix for car selection with ï¬nal score

is given by ğ‘¤ğ‘– Â¯ğ‘ ğ‘—ğ‘– . If each alternative consists of ğ‘› attributes, the score for ğ´ ğ‘— in the WSM is given by:

For WPM the contribution of ğ¶ğ‘– for alternative ğ´ ğ‘— is given by Â¯ğ‘ğ‘¤ğ‘–

ğ‘—ğ‘– , and the score of ğ´ ğ‘— is given by:

ğœğ´ğ‘— =

âˆ‘ï¸

ğ‘¤ğ‘– Â¯ğ‘ ğ‘—ğ‘–

1â‰¤ğ‘– â‰¤ğ‘›

ğœğ´ğ‘— =

(cid:214)

Â¯ğ‘ğ‘¤ğ‘–
ğ‘—ğ‘–

1â‰¤ğ‘– â‰¤ğ‘›

The alternative with the highest score is by deï¬nition the best alternative. In Figure 2 we present the normalized

decision matrix with the aggregate scores in the last column for the car selection problem. The numbers suggest to

by a BMW despite it being the most expensive car.

2.2 Hierarchical MCDM

The idea behind the Analytic Hierarchy Process (AHP) is the decomposition of a complex problem into a hierarchy.

The leaves at the bottom of the hierarchy present the diï¬€erent alternatives, whereas internal nodes play a dual role:

On one hand, they are attributes of the alternatives for the level immediately below them, on the other hand, they

represent alternatives for the level above them. AHP consists of the following three steps (Harker and Vargas 1987,

Saaty and Vargas 2001).

Step 1. Decomposition The attributes identiï¬ed for a problem are organized in a hierarchy. For example,
Figure 3 shows the decomposition step applied to the car selection problem, which takes into account the personal

preferences as well as expert evaluations. Each quantify their preference for a car by specifying its various attributes

which are price, fuel eï¬ƒciency, and safety ratings. (To make the example more manageable, we ignore the comfort

attribute in the following and consider only the Honda and BMW alternatives.)

5

The bottom nodes represent the two alternatives from which one is to be selected. Each node on the level

above (such as Price) is an attribute for each of the alternatives, and its value is given in a corresponding matrix that

captures the relationship between the two levels of the hierarchies. The AHP model is essentially a tree, represented

as a DAG that shares common children.

Step 2. Comparative Judgment The second step generates a matrix of pair-wise comparisons of all attributes
in a level with respect to each related attribute in the level immediately above it. For a DAG of ğ‘› levels, ğµğ‘– for

(1 â‰¤ ğ‘– < ğ‘›) is the comparison matrix for the elements at level ğ‘– and ğ‘– + 1. The comparison matrices for our example

ğµ1 = ğ´, ğµ2 = ğ¹ , and ğµ3 = ğ¶ are shown in Figure 4. (Note that we have normalized the numbers to account for the

AHP constraint that the numbers in each column must sum up to 1. Moreover, we changed the numbers slightly to

make the example a bit more interesting; we also rounded them to make the subsequent arithmetic easier to follow.)

Consider the matrix ğµ3. Since the level 4 of the DAG consists of 2 nodes, and level 3 consists of 3 nodes, the

dimension of matrix ğµ3 is 2 Ã— 3. This matrix relates each feature with each car. Speciï¬cally, the entry .6 for fe and

honda in ğ¶ says that a Hondaâ€™s fuel eï¬ƒciency is considered to be 50% better than a BMWâ€™s.

Step 3. Synthesis of priorities This step generates the global (or composite) priorities of the elements at the
lowest level of the hierarchy. Given the priority matrices ğµ1, . . . , ğµğ‘›, the priority vector ğ‘Š corresponding to the

alternatives at the leaf nodes of the DAG is given by the matrix product ğ‘Š = ğµğ‘›ğµğ‘›âˆ’1 . . . ğµ2ğµ1. In our example,
synthesis of priorities produces the following result.

ğ‘Š = ğµ3ğµ2ğµ1 = ğ¶ğ¹ğ´ =

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

0.516

0.484

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»

Honda

BMW

The synthesis concludes that Honda is the best car to buy, since it has a slight edge over BMW.

Score

Personal

Expert

Price

Fuel Eï¬ƒciency

Safety Ratings

Honda

BMW

Figure 3: AHP model for the car selection problem

6

ğµ1 = ğ´

car

ğµ2 = ğ¹

personal

expert

ğµ3 = ğ¶

p

fe

sr

personal

expert

.6

.4

p

fe

sr

.5

.3

.2

.2

.4

.4

honda .6

bmw

.4

.6

.4

.3

.7

Figure 4: Car selection decision matrices. ğ´: weight of advice, ğ¹ : feature advice, ğ¶: car features.

This is all well and good, but since the scores for the two alternatives are quite close, the decision maker

might be interested to know why Honda is better and which assumptions lead to this conclusion. An answer to
this question is not obvious due to the complicated hierarchical relationships between the various nodes in the

corresponding AHP diagram. In the following, we demonstrate how to generate explanations for the linear and

hierarchical MCDM techniques.

3 Explaining Decisions of Linear MCDM With Value Decompositions

Consider again the car selection example from Figure 2. We abbreviate the alternatives Toyota, Honda, and BMW

by ğ‘‡ , ğ» , and ğµ, respectively. The normalized contributions for the categories price (p), fuel eï¬ƒciency (fe), safety

rating (sr), and comfort (c) are combined into a total score for every car alternative. This view can be formalized

using the concepts of value decomposition and valuation. Given a set of categories C, a mapping ğ‘£ : C â†’ R is
called a value decomposition (with respect to C). The (total) value of a value decomposition is deï¬ned as the sum
of its components, that is, Ë†ğ‘£ = (cid:205)
(ğ‘,ğ‘¥) âˆˆğ‘£ ğ‘¥. A valuation for a set ğ‘† (with respect to C) is a function ğœ‘ that maps each
element of ğ‘† to a corresponding value decomposition, that is, ğœ‘ : ğ‘† â†’ (C â†’ R) (or, ğœ‘ : ğ‘† â†’ RC). We write Ë†ğœ‘ (ğ´)

to denote the total value of ğ´â€™s value decomposition.

The value decompositions for the alternatives in our example can be derived from Figure 2 by multiplying the

scores in each column by the weighting factor for that column, see Figure 5.

To explain why BMW is the best choice, we have to speciï¬cally explain why it was chosen over Honda, the

second-best alternative. We can also explain why BWM was chosen over Toyota, but that decision is not as close

and therefore not as much in need of an explanation. We therefore focus on comparing BMW with Honda.

Focusing on ğµ and ğ» with their respective value decompositions ğ‘£ğµ = {p â†¦â†’ .08, fe â†¦â†’ .37, sr â†¦â†’ .30, c â†¦â†’ .20}

and ğ‘£ğ» = {p â†¦â†’ .09, fe â†¦â†’ .40, sr â†¦â†’ .23, c â†¦â†’ .17} leads to the valuation ğœ‘ = {ğµ â†¦â†’ ğ‘£ğµ, ğ» â†¦â†’ ğ‘£ğ» }. The elements

7

p

fe

sr

c

ğœ

Toyota

Honda

0.10

0.34

0.27

0.14

0.85

0.09

0.40

0.23

0.17

0.89

BMW 0.08

0.37

0.30

0.20

0.95

Figure 5: Value decompositions for the car selection example

of ğ‘† can be ordered based on the valuation totals in an obvious way:

âˆ€ğ‘‹, ğ‘Œ âˆˆ ğ‘†. ğ‘‹ > ğ‘Œ â‡” Ë†ğœ‘ (ğ‘‹ ) > Ë†ğœ‘ (ğ‘Œ )

When we ask why alternative ğ‘‹ was chosen over ğ‘Œ , the obvious explanation is to give the valuation totals, which

provide the justiï¬cation Ë†ğœ‘ (ğ‘‹ ) > Ë†ğœ‘ (ğ‘Œ ). However, such an answer might not be useful, since it ignores the categories

that link the raw numbers to the application domain and thus lacks a context to interpret the numbers.

In our

example, BMW is chosen, since Ë†ğœ‘ (ğµ) = 0.95 > Ë†ğœ‘ (ğ» ) = 0.89, which might be surprising because Honda is clearly

cheaper as well as more fuel eï¬ƒcient.

If the value decomposition is maintained during the computation, we can generate a more detailed explanation.

First, we can rewrite Ë†ğœ‘ (ğ‘‹ ) > Ë†ğœ‘ (ğ‘Œ ) as Ë†ğœ‘ (ğ‘‹ ) âˆ’ Ë†ğœ‘ (ğ‘Œ ) > 0, which suggests the deï¬nition of the valuation diï¬€erence
between two elements ğ‘‹ and ğ‘Œ as follows.

ğ›¿ğœ‘ (ğ‘‹, ğ‘Œ ) = {(ğ‘, ğ‘¥ âˆ’ ğ‘¦) | (ğ‘, ğ‘¥) âˆˆ ğœ‘ (ğ‘‹ ) âˆ§ (ğ‘, ğ‘¦) âˆˆ ğœ‘ (ğ‘Œ )}

(In the following we will omit ğœ‘ whenever it is clear from the context.) The total of the valuation diï¬€erence
Ë†ğ›¿ (ğ‘‹, ğ‘Œ ) is given by the sum of all components, just like the total of a value decomposition. In our example we
have ğ›¿ (ğµ, ğ» ) = {p â†¦â†’ âˆ’.01, fe â†¦â†’ âˆ’.03, sr â†¦â†’ .07, c â†¦â†’ .03}. It is clear that the valuation diï¬€erence generally

contains positive and negative entries and that for ğ›¿ (ğ‘‹, ğ‘Œ ) > 0 to hold, the sum of the positive entries must exceed

the absolute value of the sum of the negative entries. We call the negative components of a valuation diï¬€erence its

barrier. It is deï¬ned as follows.

ğ›½ğœ‘ (ğ‘‹, ğ‘Œ ) = {(ğ‘, ğ‘¥) | (ğ‘, ğ‘¥) âˆˆ ğ›¿ğœ‘ (ğ‘‹, ğ‘Œ ) âˆ§ ğ‘¥ < 0}

The total value Ë†ğ›½ (ğ‘‹, ğ‘Œ ) is again the sum of all the components.
In our example we have ğ›½ (ğµ, ğ» ) = {p â†¦â†’
âˆ’0.01, fe â†¦â†’ âˆ’.03} and Ë†ğ›½ (ğµ, ğ» ) = âˆ’.04. The decision to select ğ‘‹ over ğ‘Œ needs as support some, but not necessarily
all, of the positive components of ğ›¿ (ğ‘‹, ğ‘Œ ), which are called the dominator candidates and which are deï¬ned as

8

follows.

Î“ğœ‘ (ğ‘‹, ğ‘Œ ) = {(ğ‘, ğ‘¥) | (ğ‘, ğ‘¥) âˆˆ ğ›¿ğœ‘ (ğ‘‹, ğ‘Œ ) âˆ§ ğ‘¥ > 0}

Any subset of Î“(ğ‘‹, ğ‘Œ ) whose total is larger than | Ë†ğ›½ (ğ‘‹, ğ‘Œ )| will suï¬ƒce as an explanation. We call such a subset a
dominator. The set of all dominators is deï¬ned as follows.

Î”ğœ‘ (ğ‘‹, ğ‘Œ ) = {ğ· | ğ· âŠ† Î“ğœ‘ (ğ‘‹, ğ‘Œ ) âˆ§ Ë†ğ· > | Ë†ğ›½ (ğ‘‹, ğ‘Œ )|}

In our example we have two dominators, that is, Î” (ğµ, ğ» ) = {{sr â†¦â†’ .07}, {sr â†¦â†’ .07, c â†¦â†’ .03}}. The smaller

a dominator, the better it is suited as an explanation, since it requires fewer details to explain how the barrier is

overcome. We therefore deï¬ne a minimal dominating set (MDS) as any dominator with the fewest possible number
of dominator candidates.

Î”ğœ‘ (ğ‘‹, ğ‘Œ ) = {ğ· | ğ· âŠ† Î”ğœ‘ (ğ‘‹, ğ‘Œ ) âˆ§ ğ· (cid:48) âŠ‚ ğ· â‡’ ğ· (cid:48) âˆ‰ Î” (ğ‘‹, ğ‘Œ )}

Note that Î” may contain multiple elements, which means that minimal dominators are in general not unique.

In other words, a decision may have diï¬€erent minimally sized explanations. In our example, the only MDS is

Î” (ğµ, ğ» ) = {sr â†¦â†’ .07}; it captures the explanation that BMW is to be preferred over Honda due to the signiï¬cant

diï¬€erence in the safety ratings of the two cars alone; we donâ€™t have to mention comfort at all to explain the decision.

We can apply the described technique to WPM examples by simply using multiplication for aggregation (using

the multiplicative identity 1) and division for computing valuation diï¬€erences. The barrier set then consists of all

the components with values less than 1, and the dominator set consists of components with values greater than 1.

An MDS explanation in this case is the smallest subset of dominator components whose product of component

values will exceed that of the inverse of product of all the component values in the barrier set. Alternatively, we

could apply the log transform to the individual component values and then use the additive version of MDS.

4 Hierarchical Value Decomposition

The idea of value decomposition relies on the fact that each alternative consists of a ï¬‚at list of attributes. In contrast,

attributes in the AHP setting are recursively decomposed into sub-attributes forming a hierarchical structure, which

raises the question of whether the idea of MDS-based explanations can also work for hierarchical decision-making

methods. This would require extending the concepts of value decomposition and dominators meaningfully to the

hierarchical case.

9

To this end, we deï¬ne the concept of a hierarchical value decomposition, which records the individual
contributions of the attributes at the various levels towards the overall priority of an alternative. The hierarchical

value decomposition for a priority value of an alternative in AHP results in a tree that maps attributes to values.

Since the synthesis of priorities in AHP consists of multiplication of decision matrices, we need a way to trace this

matrix multiplication to come up with the tree structure, which we describe in this section. This tree forms the

basis of explanations in the hierarchical case, which we describe in Section 4.2.

4.1 Tracing Matrix Multiplication with Value Decomposition Trees

Consider the decision matrix ğµ3 = ğ¶ from our example. Multiplying ğ¶ with ğµ2 = ğ¹ yields a 2 Ã— 2 matrix, where

each element is the sum of 3 products.

.6

.4

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

.6

.4

ï£¹
.3
ï£º
ï£º
ï£º
.7
ï£º
ï£»

.5

.3

.2

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

ï£¹
.2
ï£º
ï£º
ï£º
.4
ï£º
ï£º
ï£º
.4
ï£º
ï£»

.6 Â· .5 + .6 Â· .3 + .3 Â· .2

. . .

=

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

ï£¹
.6 Â· .2 + .6 Â· .4 + .3 Â· .4
ï£º
ï£º
ï£º
ï£º
ï£»

. . .

Any such sum of products can be visually represented as a tree whose leaf nodes contain the products and whose

internal nodes contain the sum of the values of its children. Here are the trees for the elements of the ï¬rst row of

the resulting matrix.

.6 Â· .5 + .6 Â· .3 + .3 Â· .2

.6 Â· .2 + .6 Â· .4 + .3 Â· .4

.6 Â· .5

.6 Â· .3

.3 Â· .2

.6 Â· .2

.6 Â· .4

.3 Â· .4

In the next step we multiply the result of ğµ3ğµ2 = ğ¶ğ¹ with the ï¬rst decision matrix ğµ1 = ğ´, which means to multiply

each summand of each matrix element with another factor and creating new sums of the results.

ğµ3ğµ2ğµ1 = ğ¶ğ¹ğ´ =

.6 Â· .5 + .6 Â· .3 + .3 Â· .2

. . .

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

ï£¹
.6 Â· .2 + .6 Â· .4 + .3 Â· .4
ï£º
ï£º
ï£º
ï£º
ï£»

. . .

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

ï£¹
.6
ï£º
ï£º
ï£º
.4
ï£º
ï£»

The ï¬rst entry of the resulting vector is given by the following value.

(.6 Â· .5 + .6 Â· .3 + .3 Â· .2) Â· .6

+ (.6 Â· .2 + .6 Â· .4 + .3 Â· .4) Â· .4

= (.6 Â· .5 Â· .6 + .6 Â· .3 Â· .6 + .3 Â· .2 Â· .6) + (.6 Â· .2 Â· .4 + .6 Â· .4 Â· .4 + .3 Â· .4 Â· .4)

Again, this sum of products can be represented by a tree, now with three levels.

10

honda : .516

personal: .324

expert: .192

p: .180

fe: .108

sr: .036

p: .048

fe: .096

sr: .048

(a) VD tree ğœ‘honda showing the attribute contributions toward the overall priority of Honda

bmw : .484

personal: .276

expert: .208

p: .120

fe: .072

sr: .084

p: .032

fe: .064

sr: .112

(b) VD tree ğœ‘bmw showing the attribute contributions toward the overall priority of BMW

Figure 6: Value decomposition trees for the hierarchical car example

.6 Â· .5 Â· .6 + .6 Â· .3 Â· .6 + .3 Â· .2 Â· .6 + .6 Â· .2 Â· .4 + .6 Â· .4 Â· .4 + .3 Â· .4 Â· .4

.6 Â· .5 Â· .6 + .6 Â· .3 Â· .6 + .3 Â· .2 Â· .6

.6 Â· .2 Â· .4 + .6 Â· .4 Â· .4 + .3 Â· .4 Â· .4

.6 Â· .5 Â· .6

.6 Â· .3 Â· .6

.3 Â· .2 Â· .6

.6 Â· .2 Â· .4

.6 Â· .3 Â· .4

.3 Â· .3 Â· .4

This tree represents the contribution of the various attributes toward the overall priority value of Honda (.516). For

example, the left subtree of the root represents the contribution of the personal opinion. Speciï¬cally, the root of

the left subtree contains the total of the personal opinion (.324), whereas the children contain the decomposition of

that value into the individual values for price (.180), fuel eï¬ƒciency (.108), and safety ratings (.036) of the personal

opinion.

To assign meaning to the tree components, we can label them with the attribute names, which are already used

11

as row and column labels, linking the individual and aggregated value contributions to the attributes of the decision

problem. We can observe the following.

(A) The row labels of ğµğ‘› should label the roots of the trees for the resulting priority vector.

(B) The column labels of matrix ğµâ„“ (for 1 < â„“ â‰¤ ğ‘›), which are equal to the row labels of matrix ğµâ„“âˆ’1, should label

the nodes on level â„“.

We call each such labeled tree a value decomposition tree, or VD tree for short. An example is shown in Figure 6a.
Now we describe a simple method to create VD trees from a sequence of matrices ğµ1, . . . , ğµğ‘›. Observations

(A) and (B) tell us that every path from root to a leaf in a VD tree is labeled by row labels taken from matrices

in the order ğµğ‘›, ğµ1, . . . , ğµğ‘›âˆ’1. Letâ€™s write [ğœ”ğ‘›, ğœ”1, . . . , ğœ”ğ‘›âˆ’1] for the row indices corresponding to those labels.
For example, in Figure 6a the leftmost leaf is identiï¬ed by the path of row labels (honda, personal, p), which

corresponds to the row indices (1, 1, 1), and the rightmost leaf is identiï¬ed by the path (honda, expert, sr), which

corresponds to the row indices (1, 2, 3). The values in a VD tree are determined as follows.

1. Assign each leaf connected to the root a path of row labels with indices [ğœ”ğ‘›, ğœ”1, . . . , ğœ”ğ‘›âˆ’1] the value

ğµğ‘› [ğœ”ğ‘›, ğœ”ğ‘›âˆ’1] Ã— ğµğ‘›âˆ’1 [ğœ”ğ‘›âˆ’1, ğœ”ğ‘›âˆ’2] Ã— . . . Ã— ğµ1 [ğœ”1].

2. Assign each internal node the sum of the values of its children.

In the example from Figure 6a, the value of the leftmost leaf is therefore computed as ğµ3 [1, 1] Ã— ğµ2 [1, 1] Ã— ğµ1 [1] =

.6Ã— .5Ã— .6 = .180. Similarly, the value of the rightmost leaf is computed as ğµ3 [1, 3] Ã—ğµ2 [3, 2] Ã—ğµ1 [2] = .3Ã— .4Ã— .4 =

.048. The sums of the internal nodes and the root are computed in the obvious way.

4.2 Hierarchical Valuation Diï¬€erences

To generate explanations from VD trees we have to generalize the concepts of valuation diï¬€erence to the hierarchical

case.

The deï¬nition of valuation changes only slightly insofar as elements of the set ğ‘† (which are identical to the row

labels of ğµğ‘›) are mapped to VD trees instead of plain value decompositions. For our car example the valuation is

ğœ‘ = {bmw â†¦â†’ Vbmw, honda â†¦â†’ Vhonda} (cf. Figure 6).

The concept of valuation diï¬€erence then extends in a natural way to the hierarchical case. First, we write ğ‘‰ (â„“)

for the value in the node that is identiï¬ed by the path of labels â„“ from the root. Then the valuation diï¬€erence ğ›¿ğœ‘ (ğ´, ğµ)

between two VD trees ğ‘‰ğ´ and ğ‘‰ğµ is deï¬ned as the VD tree Vğ´âˆ’ğµ which has the same structure and labels as Vğ´ and

12

honda âˆ’ bmw : .032

personal: .048

expert: -.016

p: .060

fe: .036

sr: -.048

p: .016

fe: .032

sr: -.064

Figure 7: Hierarchical valuation diï¬€erence between Honda and BMW, represented as a VD tree Vhondaâˆ’bmw.

Vğµ (except for the root label) such that for all root-path labels â„“ in Vğ´ except the root: Vğ´âˆ’ğµ (â„“) = Vğ´ (â„“) âˆ’ Vğµ (â„“).
The label of the root of Vğ´âˆ’ğµ is ğ´ âˆ’ ğµ, and the value of the root is V(ğ´) âˆ’ V(ğµ). As an example, the VD tree
Vhondaâˆ’bmw is shown in Figure 7.

5 Hierarchical Explanations With Value Decomposition Trees

We saw in Section 4 that the children of any node in a VD tree represent a value decomposition for that node, which

means that the leaves of a VD tree represent the most granular value decomposition for the diï¬€erence between

scores of the alternatives. The component value for such a value decomposition is the value at a given leaf node,

and the component label is the list of node labels on the path from the root to the leaf node. Since the root node label

is shared across all decompositions, we can usually remove it without losing any relevant information. Moreover,

by construction all non-leaf nodes in a VD trees represent redundant information, that is, the aggregation of the

values of their children. Therefore, each VD tree can be succinctly represented by its root-path-labeled leaves. For

our example, we have for Vhondaâˆ’bmw = ğ›¿ (honda, bmw):

Vhondaâˆ’bmw (cid:27) {(personal, p) â†¦â†’ .060, (personal, fe) â†¦â†’ .036, (personal, sr) â†¦â†’ âˆ’.048,

(expert, p) â†¦â†’ .016, (expert, fe) â†¦â†’ .032, (expert, sr) â†¦â†’ âˆ’.064}

To this ï¬‚at mapping representation of hierarchical value decompositions we can now apply the concepts of

dominators and MDS as deï¬ned in Section 3. For example, the barrier is given by the negative components

13

(personal, fe) and (expert, fe).

ğ›½ (honda, bmw) = {(personal, sr) â†¦â†’ âˆ’.048, (expert, sr) â†¦â†’ âˆ’.064} and
Ë†ğ›½ (honda, bmw) = âˆ’1.12.

That is, BMW has an advantage over Honda in the personal and expert opinion about safety ratings. We can justify

Honda as the preferred car with any dominating set, that is, any set of components whose sum exceeds the absolute

value of the barrier. Here we have two dominators.

Î” (honda, bmw) =

{{(personal, p) â†¦â†’ .060, (personal, fe) â†¦â†’ .036, (expert, fe) â†¦â†’ .032},

{(personal, p) â†¦â†’ .060, (personal, fe) â†¦â†’ .036, (expert, p) â†¦â†’ .016, (expert, fe) â†¦â†’ .032}}

Î” (honda, bmw) = {(personal, p) â†¦â†’ .060, (personal, fe) â†¦â†’ .036, (expert, fe) â†¦â†’ .032}

It is obvious that the ï¬rst dominator is the MDS in this case, since it is a proper subset of the second one. Interpreted

as an explanation, the MDS says that personal preference for Hondaâ€™s cheaper price as well as personal and expert

favorable opinion for its fuel eï¬ƒciency more than compensates for BMWâ€™s advantage in safety rating, making

Honda the preferred car overall.

6 Explanation Simpliï¬cation

Consider the MDS component (personal, p) â†¦â†’ .060. Its attribute labels personal and p come from diï¬€erent

levels of a VD tree. Comprehending such an explanation can be challenging, especially as the number of levels

increases. A simpliï¬ed explanation that employs labels from just one level might be easier to understand and thus

may have more explanatory value, even if it is less speciï¬c.

The hierarchical decomposition of valuation diï¬€erences makes it possible to provide explanations on diï¬€erent

levels of the VD tree. In particular, the tree structure enables decision makers to inquire speciï¬c details about the

reasons for a decision at the various levels of the VD tree.

For example in the car selection decision an answer to the question â€œWhy is Honda the preferred option with

respect to the decision makers?â€ is given by the children of Vhondaâˆ’bmw in Figure 7: The value -.016 for expert
represents a barrier, and the value .048 for personal is the MDS, which corresponds to the explanation that the

positive personal opinion of Honda outweighs the negative opinion of the experts. This explanation mentions only

two values and is simpler, albeit less speciï¬c, than the explanation given in the previous section.

14

Similarly, we could ask â€œWhy is Honda the preferred option with respect to the features?â€. The answer is given

by the children of Vpersonal and Vexpert in Vhondaâˆ’bmw: The overall value for sr (-0.064 + -0.048 = -0.112), obtained
by summing the sr values of Vpersonal and Vexpert, represents a barrier and the total values for p (0.060 + 0.016 =
0.076) and fe (0.036 + 0.032 = 0.068) obtained by summing the corresponding p and fe values of Vpersonal and
Vexpert is the MDS. This corresponds to the explanation that although the safety ratings are against Honda, those
are more than compensated by a better price and fuel eï¬ƒciency. Note that an MDS for level â„“ + 1 is not a reï¬nement
of the MDS for level â„“; rather they are independent explanations for the same outcome.

To formalize the focusing on diï¬€erent levels in a VD tree, we need an operation for aggregating functions over

multiple domain values. Speciï¬cally, given ğ‘“ : ğ´ â†’ R and ğ‘” : ğ´ â†’ ğµ, the aggregation of ğ‘“ with respect to ğ‘” is the
function ğ‘“ /ğ‘” : ğµ â†’ R, deï¬ned as follows.

ğ‘“ /ğ‘” = {(ğ‘¥ (cid:48), âˆ‘ï¸

{ğ‘¦ | (ğ‘¥, ğ‘¦) âˆˆ ğ‘“ , ğ‘”(ğ‘¥) = ğ‘¥ (cid:48)}) | ğ‘¥ (cid:48) âˆˆ ğ‘”(ğ´)}

We can use this aggregation to create mappings that summarize the values of a VD tree on diï¬€erent levels. Let ğœ‹ğ‘›

be the function that selects (or projects onto) the ğ‘›th element of a list or tuple. Then ğ‘‰ /ğœ‹ğ‘› creates an aggregation

of the (root-path-labeled representation of the) VD tree that maps the labels on the ğ‘›th level to their aggregated

values.

For example, the levels 2 and 3 of the VD tree Vhonda can be obtained as follows.

Vhonda/ğœ‹2 = {personal â†¦â†’ .324, expert â†¦â†’ .192}

Vhonda/ğœ‹3 = {p â†¦â†’ .180 + .048, fe â†¦â†’ .108 + .096, sr â†¦â†’ .036 + .048}

= {p â†¦â†’ .228, fe â†¦â†’ .204, sr â†¦â†’ .084}

Similarly, we can focus on diï¬€erent levels of a VD tree that stores valuation diï¬€erences, and we can also focus the

deï¬nitions of barrier, MDS, etc. by applying the corresponding function to the focused valuation diï¬€erence.

ğ›½â„“
ğœ‘ (ğ´, ğµ) = ğ›½ğ›¿ğœ‘ (ğ´,ğµ)/ğœ‹â„“ (ğ´, ğµ)

Î”â„“
ğœ‘ (ğ´, ğµ) = Î”ğ›¿ğœ‘ (ğ´,ğµ)/ğœ‹â„“ (ğ´, ğµ)

Î“â„“
ğœ‘ (ğ´, ğµ) = Î“ğ›¿ğœ‘ (ğ´,ğµ)/ğœ‹â„“ (ğ´, ğµ)

Î”â„“
ğœ‘ (ğ´, ğµ) = Î”ğ›¿ğœ‘ (ğ´,ğµ)/ğœ‹â„“

(ğ´, ğµ)

Applying these deï¬nitions to level 2, we get the following valuation diï¬€erence, barrier and, MDS explanation,

leading to the explanation we saw for level 2 at the beginning of this section.

ğ›¿ 2(honda, bmw) = {personal â†¦â†’ .048, expert â†¦â†’ âˆ’.016}

ğ›½ 2 (honda, bmw) = {personal â†¦â†’ .048}

Î”2(honda, bmw) = {expert â†¦â†’ âˆ’.016}

15

Figure 8: AHP model for selecting materials to build bridges. Figure taken from Smith et al. (1997).

Similarly, we can compute these values for level 3 explanations.

ğ›¿ 3 (honda, bmw) = {p â†¦â†’ .076, fe â†¦â†’ .068, sr â†¦â†’ âˆ’.112}

ğ›½ 3(honda, bmw) = {p â†¦â†’ .076, fe â†¦â†’ .068}

Î”3 (honda, bmw) = {sr â†¦â†’ âˆ’.112}

7 Case Studies of Applications of MDS to AHP

In this section we apply our explanation mechanism to two real-world AHP applications.

7.1 Selecting Materials to Build Bridges in Rural Winsconsin Counties

Figure 8 shows the AHP model for the problem of selecting the best material amongst prestressed concrete (p),

steel (s), timber (t), and reinforced concrete (r) to build bridges in the rural counties of Wisconsin (Smith et al.

16

ğµ1

importance

ğµT
2

pp

ls mn rs

ic

lc

dot

consultants

officials

.4

.2

.4

dot

.28 .28 .17 .08 .10 .09

consultants

.08 .08 .35 .08 .32 .09

officials

.14 .12 .22 .31 .10 .11

ğµ3

pp

ls mn rs

ic

lc

p

s

t

r

.33 .27 .42 .32 .23 .28

.09 .16 .08 .09 .15 .09

.20 .23 .23 .26 .32 .31

.38 .34 .27 .33 .29 .31

Figure 9: Decision matrices for the bridge material selection problem.

1997). The decision to select the best material takes into account the various stakeholders in the process (the state

department of transport (dot), the private consultants (consultants), and the county highway oï¬ƒcials (officials))

who base their preferences of the materials on their characteristics such as past performance (pp), lifespan (ls),

maintenance requirements (mn), resistance to natural deterioration (rs), initial cost (ic), and life cycle cost (lc).

The decision matrices ğµ1, ğµ2, and ğµ3 for the problem are shown in Figure 9. Note that the transposed matrix of ğµ2

is shown in the ï¬gure for easier presentation.

The synthesis of priorities produces the following result for the various building materials and concludes that

reinforced concrete is the best material to build the bridges, since it has a slightly higher priority value than the

prestressed concrete.

0.319372

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»
Since the p and r scores are very close, one might wonder why that is the case and which assumptions lead to this

Prestressed Concrete (p)

Reinforced Concrete (r)

ğ‘Š = ğµ3ğµ2ğµ1 =

Timber (t)

0.320409

0.109007

0.251212

Steel (s)

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

conclusion.

For lack of space, we show in Figure 10 only the hierarchical valuation diï¬€erence (and not the individual VD

trees) for the two best alternatives. From this we can generate an explanation for why reinforced concrete was

17

Figure 10: Hierarchical valuation diï¬€erence between r and p in the bridge selection example.

preferred. The barrier comprises the opinions of the DOT, the consultants, and county oï¬ƒcials with regard to the

maintenance costs.

ğ›½ (r, p) = {(dot, mn) â†¦â†’ âˆ’.0102, (consultants, mn) â†¦â†’ âˆ’.0105, (officials, mn) â†¦â†’ âˆ’.0132}

The barrier against the reinforced concrete is overcome with the following MDS explanation.

Î” (r, p) = {(dot, pp) â†¦â†’ .0056, (dot, ls) â†¦â†’ .00784, (dot, ic) â†¦â†’ .002424, (dot, lc) â†¦â†’ .001091,

(consultants, pp) â†¦â†’ .0008, (consultants, ls) â†¦â†’ .00112, (consultants, ic) â†¦â†’ .003879,

(officials, pp) â†¦â†’ .0028, (officials, ls) â†¦â†’ .00336, (officials, rs) â†¦â†’ .00124,

(officials, ic) â†¦â†’ .002424, (officials, lc) â†¦â†’ .001333}

Since the priorities for reinforced and prestressed concrete are very close, the MDS contains a large number of

components, which might be diï¬ƒcult to interpret. We can help by generating single-level explanations, either

in terms of the decision criteria or the decision makers. The valuation diï¬€erence in terms of decision criteria is

obtained by focusing on level 3.

ğ›¿ 3(r, p) = {pp â†¦â†’ .0092, ls â†¦â†’ .01232, mn â†¦â†’ âˆ’.0339, rs â†¦â†’ .00172, ic â†¦â†’ .008727, lc â†¦â†’ .00297}

In the valuation diï¬€erence, maintenance is the only component acting as a barrier ğ›½ 3 (r, p) = {mn â†¦â†’ âˆ’.0339}. The

18

MDS explanation consists of the remaining components as shown below.

Î”3 (r, p) = {pp â†¦â†’ .0092, ls â†¦â†’ .01232, rs â†¦â†’ .00172, ic â†¦â†’ .008727, lc â†¦â†’ .00297}

The explanation at the level of decision criteria can be read like this: Although reinforced concrete has a disadvantage

in terms of maintenance requirements, the cumulative advantage it has for the remaining criteria makes up for this

disadvantage.

Similarly, we can get an explanation regarding decision makers by focusing on level 2.

ğ›¿ 2(r, p) = {dot â†¦â†’ .007075, consultants â†¦â†’ âˆ’.003996, officials â†¦â†’ âˆ’0.002042}

The consultants and county highway oï¬ƒcials act as the barrier, but the MDS Î”2 (r, p) = {dot â†¦â†’ .007075} tells us
that the preference of the DOT compensates for this disadvantage.

7.2 Supreme Court Rulings on Abortion

Roe v. Wade (US Supreme Court 1973) was a landmark decision of the U.S. Supreme Court in which the Court

ruled that the Constitution of the United States generally protects a pregnant womanâ€™s liberty to choose to have an

abortion. However, it was recently overturned by the Supreme Court, sparking oï¬€ an intense public debate. It is

in this context that the current AHP example (Saaty and Vargas 2012), describing an AHP model to predict the

Supreme Court ruling on a related issue of Roe v. Wade in 1992, and our techniques explaining why those outcomes

were predicted gain special relevance.

In the summer of 1992 the Supreme Court of the United States was supposed to rule on a controversial

Pennsylvania statute restricting the rights of women in obtaining an abortion.

Included in this statute were

provisions requiring that doctors provide women with state-prescribed information about pregnancy and abortion,

that the procedure be delayed 24 hours after the recitation, and that husbands be notiï¬ed prior to the procedure.

The lower court upheld the ï¬rst two provisions, but declared unconstitutional the husband notiï¬cation requirement.

The AHP model for the example is shown in Figure 11. It correctly predicted that the Supreme Court will uphold

at least parts of the Pennsylvania statue and will, as a result, weaken the rights of women who choose to have an

abortion in the state of Pennsylvania.

The model uses the nine Supreme Court justices as the criteria, giving each of them an equal weight. Beneath

each justice there are ï¬ve sub-criteria that were determined to be the most important for the judges to adjudicate

on the matter.

19

Figure 11: AHP model for predicting the Supreme Court decision on Pennsylvania abortion issue. Figure taken

from Saaty and Vargas (2012).

â€¢ Womenâ€™s issues (w) These are issues deemed important by the pro-choice movement, such as the constitu-

tional right of each woman to make her own decisions regarding her body.

â€¢ Precedent (p) Cases that have gone before the Supreme Court since the early 1970s.

â€¢ Moral issues (m) Constitutional rights of the fetuses and the belief that abortion is murder.

â€¢ Political issues (o) To make the decision-making process easier, the political issues are deï¬ned as con-
servatism. The original paper determined that conservatives are more pro-life than liberals. It also links

Republicans with conservatism and Democrats with liberalism.

â€¢ Biological issues (b) The medical concept of viability that speciï¬es a certain time when the fetus is capable

of independent survival outside the motherâ€™s womb.

The AHP model envisaged three likely outcomes of the ruling: overturn, uphold, or weaken Roe v. Wade by

giving states more independent power to restrict abortions. The original paper used many expertsâ€™ opinions from

20

books and law journals to determine how each justice will weight each criterion and how each sub-criterion will

aï¬€ect the alternative selected.

The synthesis of priorities gives us priorities of 0.378, 0.394, and 0.228 for overturn, weaken and uphold,

respectively. The weaken rather than the overturn verdict is surprising, given the strong conservative leaning of the

court. We can try to explain this surprising decision using our techniques. As a ï¬rst step, we need to compute the

hierarchical valuation diï¬€erence. However, the original paper doesnâ€™t provide the matrix between levels 3 and 4 of

the AHP model in Figure 11. Due to lack of this matrix, we canâ€™t create the VD trees for various alternatives in

the usual way. Interestingly, the paper provides a trace of the priority synthesis step as shown in Figure 12a, which

facilitates the computation of the VD trees for various alternatives, as shown for the overturn alternative in Figure

12b.

Once we have the VD trees for the two alternatives, we can compute the hierarchical valuation diï¬€erence

between the two alternatives, as shown in Figure 13. The barrier for the decision shows that moral considerations

and political leaning of all the 9 justices are for overturning Roe v. Wade rather than weakening it.

ğ›½ (weaken, overturn) =

{(1, m) â†¦â†’ âˆ’.003, (2, m) â†¦â†’ âˆ’.004, (3, m) â†¦â†’ âˆ’.005, (4, m) â†¦â†’ âˆ’.055, (5, m) â†¦â†’ âˆ’.051, (6, m) â†¦â†’ âˆ’.006,

(7, m) â†¦â†’ âˆ’.004, (8, m) â†¦â†’ âˆ’.051, (9, m) â†¦â†’ âˆ’.048, (1, o) â†¦â†’ âˆ’.004, (2, o) â†¦â†’ âˆ’.013, (3, o) â†¦â†’ âˆ’.003,

(4, o) â†¦â†’ âˆ’.008, (5, o) â†¦â†’ âˆ’.016, (6, o) â†¦â†’ âˆ’.007, (7, o) â†¦â†’ âˆ’.003, (8, o) â†¦â†’ âˆ’.016, (9, o) â†¦â†’ âˆ’.006}

However, the respect for precedent for all justices, ambiguity deï¬ning viability for all but justice 5 (Scalia) along

with the consideration of the women issues by justices 1 and 7 (Blackmun and Stevens, respectively) lead to

overcoming the barrier. This is a minimal explanation for why a weaken rather than an overturn verdict was

reached.

Î” (weaken âˆ’ overturn) =

{(1, p) â†¦â†’ .022, (2, p) â†¦â†’ .045, (3, p) â†¦â†’ .031, (4, p) â†¦â†’ .005, (5, p) â†¦â†’ .006, (6, p) â†¦â†’ .049,

(7, p) â†¦â†’ .030, (8, p) â†¦â†’ .006, (9, p) â†¦â†’ .008, (1, b) â†¦â†’ .008, (2, b) â†¦â†’ .007, (3, b) â†¦â†’ .027,

(4, b) â†¦â†’ .006, (6, b) â†¦â†’ .005, (7, b) â†¦â†’ .016, (8, b) â†¦â†’ .004, (9, b) â†¦â†’ .017, (1, w) â†¦â†’ .009, (7, w) â†¦â†’ .005}

Since barrier as well as MDS contain a large number of components, focusing on speciï¬c levels of the hierarchy

can simplify the explanation. An explanations which focuses on the sub-criteria used by justices is based on level

21

(a) Trace of synthesis of priorities. Figure taken from Saaty and Vargas (2012).

(b) VD tree for the overturn alternative.

Figure 12: Synthesis of priorities in the Roe v. Wade example

22

Figure 13: Hierarchical valuation diï¬€erence between overturn and weaken in the Roe v. Wade example.

3 of the value diï¬€erence.

ğ›¿ 3 (weaken âˆ’ overturn) = {w â†¦â†’ .022, p â†¦â†’ .207, m â†¦â†’ âˆ’.227, o â†¦â†’ âˆ’.076, b â†¦â†’ .090}

The corresponding barrier and MDS explanation are shown below.

ğ›½ 3(weaken âˆ’ overturn) = {m â†¦â†’ âˆ’.227, o â†¦â†’ âˆ’.076}

Î”3 (weaken âˆ’ overturn) = {w â†¦â†’ .022, p â†¦â†’ .207, b â†¦â†’ .090}

We observe that although moral consideration and political aï¬ƒliation of the judges supports overturn of Roe v.

Wade, their consideration for womenâ€™s issues, precedent, and diï¬ƒculty around deï¬ning viability outweigh this

support, resulting in the less extreme verdict of weaken. It is interesting to note that a 7-2 conservative-leaning

Supreme Court decided to just weaken Roe v. Wade in 1992 whereas a 6-3 conservative-leaning court overturned

it in 2022.

We can ï¬nd out which judges were responsible for the weaken verdict by focusing on level 2.

ğ›¿ 2(weaken âˆ’ overturn) = {1 â†¦â†’ .032, 2 â†¦â†’ .035, 3 â†¦â†’ .052, 4 â†¦â†’ âˆ’.051, 5 â†¦â†’ âˆ’.061, 6 â†¦â†’ .044,

7 â†¦â†’ .049, 8 â†¦â†’ âˆ’.056, 9 â†¦â†’ âˆ’.028}

The barrier and MDS explanation tell us that justices 4, 5, 8, and 9 would most probably vote to overturn Roe

v. Wade, however, the majority will prefer to weaken but uphold it. The ï¬nal verdict (US Supreme Court 1992)

23

showed the same voting pattern as predicted by our explanation here.

ğ›½ 2(weaken, overturn) = {4 â†¦â†’ âˆ’.051, 5 â†¦â†’ âˆ’.061, 8 â†¦â†’ âˆ’.056, 9 â†¦â†’ âˆ’.028}

Î”2 (weaken, overturn) = {1 â†¦â†’ .032, 2 â†¦â†’ .035, 3 â†¦â†’ .052, 6 â†¦â†’ .044, 7 â†¦â†’ .049}

8 Evaluation

To assess the eï¬€ectiveness of MDS explanations for AHP decisions, we have performed a number of experiments

to estimate the reduction in complexity that they can be expected to deliver. In the following we describe the setup

and results of these experiments.

First, we have to establish criteria to measure the eï¬ƒcacy of explanations. Without any speciï¬c explanation,

a user has to inspect all ğ‘› components of a value decomposition generated by the AHP process. The explanatory

strength of an MDS comes from the fact that it can often reduce this number considerably to, say, ğ‘š. The reduction

can then be captured by deï¬ning the explanatory ratio of an MDS as ğ‘š/ğ‘›. The smaller the ratio, the fewer
components users have to look at, relative to the original decision, thus making it easier to understand. We can

express the same idea more intuitively as a percentage size reduction achieved. We thus deï¬ne the MDS reduction
as ğ‘… = (1 âˆ’ ğ‘š/ğ‘›) Ã— 100, that is, an explanation ratio of 0.15 translates into a reduction by 85%.

8.1 Eï¬ƒcacy of MDS Explanations

Since there are no AHP benchmark data sets available, we have generated data for evaluating the eï¬ƒcacy of MDS

explanations. The examples reported in the literature indicate that AHP models rarely have more than 6 levels. Yet,

each dimension can be wide: For example, an AHP model with 51 attributes in one dimension can be found in (Liu

et al. 2008). In general, it is common for an AHP to have about 10 attributes in one of the dimensions (Pan 2008).

Based on these observations, we have randomly generated data for AHP examples having between 3 and 6

levels and computed the reduction for each case. A 3 level AHP is essentially a linear MCDM. The examples in

the literature suggest limiting the number of components to 30 for models with 3 levels. For problems with 4, 5,

or 6 levels, we limit the total number of components in the corresponding value decomposition of an AHP to 100,

with intermediate dimensions having between 2 to 10 components each. We have used 20,000 random inputs for

each scenario. For an AHP problem with a ï¬xed number of levels, the inputs vary in two regards: (a) the number

of attributes at each level, and (b) the values of the decision matrices.

Another aspect that should be reï¬‚ected in the test data is whether an explanation is necessary at all. For

24

example, when the ï¬rst alternative from an AHP process is better than the runner-up in every regard, no explanation

is necessary. In contrast, an explanation is most helpful in cases when the two alternatives are really close, that is,

when the priority values of the alternatives are similar. To reï¬‚ect this situation, we ï¬lter out those cases whose ï¬rst

two alternatives are not close. We call the relative diï¬€erence between the priority values of two alternatives their

decision margin and consider scenarios in which the decision margin is bounded to 1%, 5%, 10%, 20%, and 30%.
Figure 14 shows how MDS reduction varies with the total number of components. We show graphs for AHPs

with diï¬€erent number of levels containing plots for diï¬€erent decision margins.

The plots reveal some interesting trends. First, on average an MDS can prune the number of components

by about 55-60% even for a decision margin as low as 1%. Second, the reduction decreases with smaller

decision margins, which makes intuitive sense, since a greater value distance between alternatives provides more

opportunities to explain the diï¬€erence with fewer components. But unfortunately, this also means that the eï¬ƒcacy

of MDS explanation shrinks when they might be needed most. Third, with an increasing number of levels, the

curves â€œmove upwardâ€, that is, for a given decision margin the reduction increases with the number of levels in the

AHP problems. In other words, MDS explanations scale well with the structural complexity of AHP problems.

8.2 Eï¬ƒcacy of Single-Level Explanations

Consider the example from Section 7.2 where levels 2 and 3 consists of 9 and 5 components, respectively. Therefore,

the total number of components in the value decomposition is 45 (= 9Ã—5), whereas a simpliï¬ed explanation contains

no more than the sum of the number of components in the two levels, that is, 14 (= 9 + 5).

We can measure the improvement of a single-level explanation over an MDS as the size reduction given by

ğ‘… = (1 âˆ’ ğ‘ /ğ‘š) Ã— 100 where ğ‘š is the size of the MDS and ğ‘  is the size of the single-level explanation. With this

deï¬nition, single-level explanations promise an improvement in the Roe v. Wade example of at least 26%. The

actual improvement was (1 âˆ’ 5+3

19 ) Ã— 100 = 58%, which shows that the improvement can be signiï¬cantly better than

indicated by the worst-case estimate.

Figure 15 shows the percentage improvement for simpliï¬ed explanations over MDS. The ğ‘¥ axis shows the

number of explanation components. For lack of space, we present the data only for AHP problems with 4 levels.

However, the same trends can be observed for AHP problems with 5 and 6 levels.

We can observe that for this worst-case consideration, savings can be obtained only for models with a signiï¬cant

number of components. But in general, the situation will be much better, since on average a single-level explanation

will contain only half the number of components (because the other half will be used for the barrier).

25

(a) Number of levels: 3

(b) Number of levels: 4

(c) Number of levels: 5

(d) Number of levels: 6

Figure 14: Average MDS reduction (ğ‘¦ axis) dependent on the number of components (ğ‘¥ axis) for AHP problems

with diï¬€erent number of levels. Decision margins:

â‰¤ 30%,

â‰¤ 20%,

â‰¤ 10%,

â‰¤ 5%,

â‰¤ 1%

26

Figure 15: Size reduction of single-level explanations over MDS (ğ‘¦ axis) dependent on the number of components

(ğ‘¥ axis) for AHP problems with 4 levels. Decision margins:

â‰¤ 30%,

â‰¤ 10%,

â‰¤ 1% (solid lines: worst

case, dash-dot lines: average case).

9 Related Work

Sensitivity analysis (Triantaphyllou and SÃ¡nchez 1997) is the tool of choice employed by decision makers to

comprehend the results of various MCDM methods, including AHP. Sensitivity analysis is usually the only

explanation mechanism available to a decision maker. Despite being useful, a potential limitation of sensitivity

analysis is that it can only analyze the impact of one attribute at a time, keeping other attributes values constant.

Thus, sensitivity analysis produces a number of localized explanations. In comparison, our value-decomposition

explanation method is global, and an MDS explanation takes into consideration the combined impact of various

attributes in the decision, leading to generally more accurate and comprehensive explanations. On the other hand,

MDS explanations are larger than the variation of one attribute, but the size of MDS explanations can be eï¬€ectively

reduced by employing single-level explanations.

The topic of explanations in general has been explored in a number of diï¬€erent areas. While the origins of

research into the nature of explanations can be traced back to philosophy (Hempel 1965, Achinstein 1983, Ruben

1990), the need for explaining computation has recently received a lot of attention, speciï¬cally in the area of AI

(Miller 2019, Adadi and Berrada 2018).

The notion of a value decomposition was introduced in (Erwig and Kumar 2021a) as a structure for explaining

the results of dynamic programming algorithms. Value decomposition is generated as a domain-speciï¬c structure

there, using the fact that dynamic programming algorithms can be viewed as instances of a mathematical semiring

27

structure. This is similar to the current work, where the value decomposition is a domain-speciï¬c structure

generated from the computations of the MCDM problems. Another point of similarity is that both generate

contrastive explanations and thus also require two program results. In (Erwig and Kumar 2021b) we describe a

domain-speciï¬c language, which is based on the theory developed in this paper and allows users to specify MCDM

problems, synthesize priorities for various alternatives, and generate MDS explanations. That work is primarily

concerned with questions of language design and how to represent MCDM problems and explanations in support

of computational transformations.

Similar to the current approach, another domain-speciï¬c structure created explicitly for explanations are

provenance traces (Acar et al. 2012). A provenance trace consists of meta-information about the origin, history, or

derivation of an object which is used in establishing trust and providing security in computer systems, particularly

on the web. Like value decompositions, provenance traces are a domain-speciï¬c explanation structure that works

only in certain situations.

10 Conclusions

We have demonstrated an eï¬€ective method for explaining the results of MCDM methods. Our approach of using

minimal dominating sets is general enough to work well for ï¬‚at and hierarchical models. Through the concept of

single-level explanations, users have the option to additionally get simpliï¬ed explanations. As with explanations

for algorithmic systems in general, the ability for generating concise explanations can contribute to the acceptance

of results and adds transparency to computational systems.

Acknowledgement

This work is partially supported by the National Science Foundation under the grants CCF-1717300 and CCF-

2114642.

References

Acar UA, Ahmed A, Cheney J, Perera R (2012) A Core Calculus for Provenance. Int. Conf. on Principles of Security

and Trust, 410â€“429.

Achinstein P (1983) The Nature of Explanation (New York, NY: Oxford University Press).

28

Adadi A, Berrada M (2018) Peeking Inside the Black-Box: A Survey on Explainable Artiï¬cial Intelligence (XAI).

IEEE Access 6:52138â€“52160.

Erwig M, Kumar P (2021a) Explainable Dynamic Programming. Journal of Functional Programming 31(e10).

Erwig M, Kumar P (2021b) MADMAX: A DSL for Explanatory Decision Making. ACM SIGPLAN Conf. on

Generative Programming: Concepts & Experiences, 144â€“155.

Faulhaber AK, Ni I, Schmidt L (2021) The eï¬€ect of explanations on trust in an assistance system for public transport

users and the role of the propensity to trust. Mensch Und Computer 2021, 303â€“310, MuC â€™21 (New York,
NY, USA: ACM).

Fishburn P (1967) Additive utilities with incomplete product sets: Application to priorities and assignments.

Operations Research 15(3):537â€“542.

Garï¬nkel P (1981) Forms of Explanation (New Haven, CT, USA: Yale University Press).

Goodman B, Flaxman S (2017) European union regulations on algorithmic decision-making and a â€œright to

explanationâ€. AI Magazine 38:50â€“57.

Harker P, Vargas L (1987) The theory of ratio scale estimation: Saatyâ€™s analytic hierarchy process. Management

Science 33:1383â€“1403.

Hempel C (1965) Aspects of Scientiï¬c Explanation and Other Essays in the Philosophy of Science (New York, NY:

Free Press).

Lipton P (1990) Contrastive Explanation. Royal Institute of Philosophy Supplement 27:247â€“266.

Lipton P (2004) Inference to the Best Explanation (New York, NY, USA: Routledge).

Liu L, Berger P, Zeng AZ, Gerstenfeld A (2008) Applying the analytic hierarchy process to the oï¬€shore outsourcing

location decision. Supply Chain Management 13:435â€“449.

Miller T (2019) Explanation in Artiï¬cial Intelligence: Insights from the Social Sciences. Artiï¬cial Intelligence

267:1â€“38.

Pan N (2008) Fuzzy AHP approach for selecting the suitable bridge construction method. Automation in Construc-

tion 17:958â€“965.

Ruben DH (1990) Explaining Explanation (London, UK: Routledge).

Saaty RW (1987) The Analytic Hierarchy Processâ€”what it is and how it is used. Mathematical Modelling 9(3):161â€“

176.

29

Saaty TL (1990) How to make a decision: The analytic hierarchy process. European Journal of Operational

Research 48(1):9â€“26.

Saaty TL (2002) Decision making with the Analytic Hierarchy Process. International Journal of Services Sciences

1:83â€“98.

Saaty TL (2003) The negotiation and resolution of the conï¬‚ict in South Africa: The AHP. ORiON 4, URL

http://dx.doi.org/10.5784/4-1-488.

Saaty TL, Cho Y (2001) The decision by the US congress on Chinaâ€™s trade status: a multicriteria analysis.

Socio-Economic Planning Sciences 35(4):243 â€“ 252.

Saaty TL, Vargas L (2001) Models, Methods, Concepts & Applications of the Analytic Hierarchy Process.

Saaty TL, Vargas LG (2012) Abortion and the States: How will the Supreme Court Rule on the Upcoming

Pennsylvania Abortion Issue?, 281â€“289 (Boston, MA: Springer US).

Selbst AD, Powles J (2017) Meaningful information and the right to explanation. International Data Privacy Law

7:233â€“242.

Smith R, Bush R, Schmoldt D (1997) The selection of bridge materials utilizing the analytical hierarchy process .

Tang V, Collar E (1992) IBM AS/400 new product launch process ensures satisfaction. Long Range Plan-
ning 25(1):22 â€“ 27, ISSN 0024-6301, URL http://www.sciencedirect.com/science/article/pii/
002463019290306M.

The French Government (2016) Digital Republic Act of France. https://www.republique-numerique.fr/

pages/digital-republic-bill-rationale.

Tofallis C (2014) Add or multiply? a tutorial on ranking and choosing with multiple criteria. INFORMS Transactions

on Education 14(3):109â€“119.

Triantaphyllou E, SÃ¡nchez A (1997) A sensitivity analysis approach for some deterministic multi-criteria decision-

making. Decision Sciences 28:151â€“194.

US Supreme Court (1973) Roe v. Wade, 410 U.S. 113 (1973). https://supreme.justia.com/cases/federal/

us/410/113/.

US Supreme Court (1992) Planned Parenthood v. Casey. https://www.law.cornell.edu/supremecourt/

text/505/833.

30

