1

0
2
0
2

r
p
A
7
1

]

G
L
.
s
c
[

1
v
1
0
1
8
0
.
4
0
0
2
:
v
i
X
r
a

A stochastic approach to handle knapsack
problems in the creation of ensembles

Andr ´as Hajdu∗, Senior Member, IEEE, Gy ¨orgy Terdik, Attila Tiba, and Henrietta Tom ´an

Abstract—Ensemble-based methods are highly popular approaches that increase the accuracy of a decision by
aggregating the opinions of individual voters. The common point is to maximize accuracy; however, a natural limitation
occurs if incremental costs are also assigned to the individual voters. Consequently, we investigate creating ensembles
under an additional constraint on the total cost of the members. This task can be formulated as a knapsack problem,
where the energy is the ensemble accuracy formed by some aggregation rules. However, the generally applied
aggregation rules lead to a nonseparable energy function, which takes the common solution tools – such as dynamic
programming – out of action. We introduce a novel stochastic approach that considers the energy as the joint probability
function of the member accuracies. This type of knowledge can be efﬁciently incorporated in a stochastic search
process as a stopping rule, since we have the information on the expected accuracy or, alternatively, the probability of
ﬁnding more accurate ensembles. Experimental analyses of the created ensembles of pattern classiﬁers and object
detectors conﬁrm the efﬁciency of our approach. Moreover, we propose a novel stochastic search strategy that better
ﬁts the energy, compared with general approaches such as simulated annealing.

Index Terms—Ensemble creation, majority voting, knapsack problems, stochastic selection.

(cid:70)

1 INTRODUCTION

E nsemble-based systems are rather popular in

several application ﬁelds and are employed
to increase the decision accuracy of
individual
approaches. We also encounter such approaches
for pattern recognition purposes [1], using models
based on, e.g., neural networks [2], [3], decision
trees [4] or other principles [5], [6], [7]. In the most
recent results, we can recognize this approach in
the design of state-of-the-art convolutional neural
networks (such as GoogLeNet, incorporating the
Inception module [8]) or the direct combination of
them [9]. In our practice, we also routinely consider
ensemble-based approaches to aggregate the out-
puts of pattern classiﬁers [10] or detector algorithms
[11], usually by some majority voting-based rule.
During these efforts, we have also faced perhaps the
most negative property of creating ensembles, that
is, the increasing demand on resources. This type of
cost may occur as the execution/training time and
the working hours needed to create the ensemble

• A. Hajdu∗, Gy. Terdik, A. Tiba, and H. Tom´an are with the
Faculty of Informatics, University of Debrecen, 4002 Debrecen,
POB 400, Hungary.
E-mail: {hajdu.andras, terdik.gyorgy, tiba.attila, toman.henrietta}
@inf.unideb.hu.

* Corresponding author.

components, etc., according to the characteristics of
the given problem. Thus, in addition to the primary
aim of the composition of the most accurate ensem-
ble, a natural constraint emerges as a cost limitation
for that endeavor.

More formally,

let us consider a pool D =
{D1, . . . , Dn} containing possible ensemble mem-
bers, where each Di (i = 1, . . . , n) is characterized
by a pair (pi, ti) describing its individual accuracy
pi ∈ [0, 1] and cost ti ∈ R>0. The individual accura-
cies are supposed to be known, e.g., by determining
them on some test data and by an appropriate per-
formance metric. In this work, we will focus on the
majority voting-based aggregation principle, where
the possible ensemble members Di (i = 1, . . . , n)
are classiﬁers (see [12]). In [13], we have dealt with
the classic case in which the individual classiﬁers
make true/false (binary) decisions. In this model,
a classiﬁer Di with accuracy pi is considered as a
Bernoulli distributed random variable ηi, that is,
P (ηi = 1) = pi, P (ηi = 0) = 1 − pi (i = 1, . . . , n),
where ηi = 1 means the correct classiﬁcation by
Di. In this case, we obtain that the accuracy of
an ensemble D(cid:48) = {Di1, . . . , Di(cid:96)} ⊆ D of |D(cid:48)| = (cid:96)
members can be calculated as

 
 
 
 
 
 
q(cid:96)(L) =








(cid:96)
(cid:88)

k=(cid:98) (cid:96)

2 (cid:99)+1



(cid:88)

(cid:89)

(cid:89)

pi

i∈I

j∈L\I

I⊆L
|I|=k

(1 − pj)






,

(1)

where L = {i1, . . . , i(cid:96)} ⊆ N = {1, . . . , n} is the index
set of D(cid:48). As an important practical issue, notice
that (1) is valid only for independent members to
calculate the ensemble accuracy. The dependency
of the members can be discovered further by, e.g.,
using different kinds of diversity measures [14].

Regarding ensemble-based systems, the stan-
dard task is to devise the most accurate ensemble
from D for the given energy function. In this paper,
we add a natural constraint of a bounded total cost
to this optimization problem. That is, we have to
maximize (1) under the cost constraint

(cid:88)

i∈L

ti ≤ T,

(2)

where the total allowed cost T ∈ R>0 is a pre-
deﬁned constant. Consequently, we must focus on
those subsets L ⊆ N with cardinalities |L| = (cid:96) ∈
{1, . . . , n} for which (2) is fulﬁlled. Let L0 denote
that index set of cardinality |L0| = (cid:96)0, where the
global maximum ensemble accuracy is reached.
The following lemma states that one can reach L0
calculating q(cid:96) (L) for odd values of (cid:96) only, which
results in more efﬁcient computation, since not all
the possible subsets should be checked.

Lemma 1.1. If

max (q(cid:96) (L) | L ⊆ N ) = q(cid:96)0 (L0) ,

(3)

then (cid:96)0 is odd.

Proof. See Appendix A for the proof.

The optimization task deﬁned by the energy
function (1) and the constraint (2) can be interpreted
as a typical knapsack problem [15]. Such problems
are known to be NP-hard; however, if the energy
function is linear and/or separable for the pi-s,
then a very efﬁcient algorithmic solution can be
given based on dynamic programming. However,
if the energy lacks these properties, the currently
available theoretical foundation is rather poor. As
some speciﬁc examples, we were able to locate in-
vestigations of an exponential-type energy function
[16], and a remarkably restricted family of nonlinear
and nonseparable ones [17]. In [16], an approach
based on calculus was made by representing the
energy function by its Taylor series. Unfortunately,

2

it has been revealed that dynamic programming can
be applied efﬁciently only to at most the quadratic
member of the series; thus, the remaining higher-
order members had to be omitted. This compulsion
suggests a large error term if this technique is
attempted to be considered generally. Thus, to the
best of our knowledge, there is a lack of theoreti-
cal instructions/general recommendations to solve
knapsack problems in the case of complex energy
functions. As our energy (1) is also nonlinear and
nonseparable, we were highly motivated to develop
a well-founded framework for efﬁcient ensemble
creation.

As a common empirical approach to ﬁnd the op-
timal ensemble, the usefulness pi/ti (i = 1, . . . , n) of
the possible members are calculated. Then, as deter-
ministic greedy methods, the ensemble is composed
of forward/backward selection strategies (see, e.g.,
[18]). Since the deterministic methods are less ef-
ﬁcient – e.g., the greedy one is proven to have 50%
accuracy for the simplest knapsack energy (cid:80)n
i=1 pi –
popular stochastic search algorithms are considered
instead, such as simulated annealing (SA). How-
ever, for the sake of completeness, we will start
our theoretical investigation regarding the accuracy
of the existing deterministic methods when a cost
limitation is also applied.

As our main contribution, in this paper, we pro-
pose a novel stochastic technique to solve knapsack
problems with complex energy functions. Though
the model is worked out in detail for (1) settled
on the majority voting rule, it can also be applied
to other energy functions. Our approach is based
on the stochastic properties of the energy q(cid:96) in (1),
providing that we have some preliminary knowl-
edge on where the distribution its parameters pi
(i = 1, . . . , n) is coming from, with a special focus
on beta distributions that ﬁt practical problems very
well. In other words, we estimate the distribution
of q(cid:96) in terms of its mean and variance. These
information can be efﬁciently incorporated as a
stopping rule in stochastic search algorithms, as we
demonstrate it for SA. The main idea here is to be
able to stop building ensembles when we can expect
that better ones can be found by low probability.

As a further contribution, we introduce a novel
stochastic search strategy, where the usefulness of
the components are deﬁned in a slightly more
complex way to better ﬁt the investigated energy;
the stopping rule can be successfully applied in
this approach, as well. Our empirical analyses also
show that including the stochastic estimation as a
stopping rule saves a large amount of search time

to build accurate ensembles. Moreover, our novel
search strategy is proven to be very competitive
with SA. Our stochastic approach was ﬁrst pro-
posed in our former work [19] with limited empiri-
cal evaluations; however, only heuristic results were
achieved there without being able to take advantage
of the theoretical model completed here.

The rest of the paper is organized as follows. In
section 2, we analyze the maximum accuracy of the
common deterministic ensemble creator strategies
in the case of limited total cost. Existing stochastic
approaches are described in section 3 with some
preliminary simulation results. Moreover, we
introduce a novel stochastic search algorithm that
determines the expected usefulness of possible
members in a way that adapts to the characteristics
of the energy function better than, e.g., SA. The
stochastic estimation of the ensemble energy from
the individual accuracies of the components is
presented in section 4; our code is available at
https://codeocean.com/capsule/3944336.
Our
experimental analysis are enclosed in section
5,
the possible
creation of ensembles from participating methods
of Kaggle challenges and binary classiﬁcation
problems in UCI databases; our data is available
https://ieee-dataport.org/documents/binary-
at
classiﬁers-outputs-ensemble-creation. We
also
present how the proposed model is expected to be
generalized to multiclass classiﬁcation tasks with a
demonstrative example on our original motivating
object detection problem. Finally, in section 6, we
discuss several issues regarding our approach that
can be tuned towards special application ﬁelds.

including the investigation of

2 DETERMINISTIC SELECTION STRATEGIES

In this section, we address deterministic selection
strategies to build an ensemble that has maximal
system accuracy q(cid:96)0(L0), applying the cost limita-
tion. However, since we have 2n different subsets
of elements of a pool of cardinality n, this selec-
tion task is known to be NP-hard. To overcome
this issue, several selection approaches have been
proposed. The common point of these strategies is
that in general, they do not assume any knowledge
on the proper determination of the classiﬁcation
performance q(cid:96)(L); rather, they require only the
ability to evaluate it. Moreover, to the best of our
knowledge, strategies that consider the capability
of individual feature accuracies to be modeled by
drawing them from a probability distribution, as in
our approach, have not yet been proposed.

3

Based on the above discussion, it seems to be
natural to ascertain how the widely applied selec-
tion strategies work in our setup. The main differ-
ence in our case, in contrast to the general recom-
mendations, is that now we can properly formulate
the performance evaluation using the exact func-
tional knowledge of q(cid:96). That is, we can characterize
the behavior of the strategy with a strict analysis
instead of the empirical tests generally applied.

We start our investigation with greedy selection
approaches by discussing them via the forward
selection strategy. Here, the most accurate item is se-
lected and put in a subset S ﬁrst. Then, from the re-
maining n − 1 items, the component that maximizes
the classiﬁcation accuracy of the extended ensemble
is moved to S . This procedure is then iteratively
repeated; however, if the performance cannot be
increased by adding a new component, then S is
not extended and the selection stops. The ﬁrst issue
we address is to determine the largest possible error
this strategy can lead to in our scenario.

Proposition 2.1. The simple greedy forward selection
strategy to build an ensemble that applies the majority
voting-based rule has a maximum error rate 1/2.

Proof. For the proof, see Appendix B.

As seen from the proof, the error rate of 1/2
holds for the forward strategy independent of the
time constraint. As a quantitative example, let p1 =
0.510 and p2 = p3 = p4 = p5 = 0.505. With this
setup, where Ik = {1, . . . , k}, we have q1(I1) = p1 =
0.5100, q3(I3) = 0.5100, and q5(I5) = 0.5112, which
shows that the greedy forward selection strategy is
stuck at the single element ensemble, though a more
accurate larger one could be found.

In addition to forward selection, its inverse vari-
ant, the backward selection strategy, is also popular.
It puts all the components into an ensemble ﬁrst,
and in every selection step, leaves the worst one out
to gain maximum ensemble accuracy. As a major
difference from the forward strategy, backward se-
lection is efﬁcient in our case if the time constraint is
irrelevant. Namely, either the removal of the worst
items will lead to an increase in q(cid:96) deﬁned in (1),
or the selection can be stopped without the risk of
missing a more accurate ensemble. However, if the
time constraint applies, the same maximum error
rate can be proved.

Proposition 2.2. The simple greedy backward selection
strategy considering the individual accuracy values to
build an ensemble that applies the majority voting-based
rule has a maximum error rate of 1/2.

Proof. Proposition 2.2. is proved in Appendix C.

3 STOCHASTIC SEARCH ALGORITHMS

4

Propositions 2.1. and 2.2. have shown the worst
case scenarios for the forward and backward se-
lection strategies. However, the greedy approach
was applied only regarding the accuracy values
of the members, and their execution times were
omitted. To consider both the accuracies and exe-
cution times of the algorithms in the ensemble pool
D = {D1 = (p1, t1), D2 = (p2, t2), . . . , Dn = (pn, tn)},
we consider their usefulness in the selection strate-
gies, deﬁned as

ui = pi/ti,

i = 1, . . . , n,

(4)

which is a generally used deﬁnition to show the
price-to-value ratio of an object. The composition
of ensembles based on similar usefulness measures
has also been efﬁcient, e.g., in sensor networks [18].
After the introduction of the usefulness (4), the
ﬁrst natural question to clarify is to investigate
whether the validity of the error rate of the de-
terministic greedy forward and backward selection
strategies operating with the usefulness measure
holds. Through the following two statements, we
will see that the 1/2 error rates remain valid for both
greedy selection approaches.

Corollary 2.1. Proposition 2.1 remains valid when the
forward feature selection strategy operates on the use-
let t1 =
fulness. Namely, as a worst case scenario,
t2 = · · · = tn = T /n be the execution times in the
example of the proof of Proposition 2.1 while keeping the
same p1, p2, . . . , pn values. Then, the selection strategy
operates completely in the same way on the ui = pi/ti
values (i = 1, . . . , n) as on the pi ones, since the ti values
are equal. That is, the error rate is 1/2 in the same way.

Proposition 2.3. The simple greedy backward selection
strategy considering the individual usefulness (4) to
build an ensemble that applies the majority voting-based
rule has a maximum error rate of 1/2.

Proof. The proof is provided in Appendix D.

The main problem with the above deterministic
procedures is that they leave no opportunity to ﬁnd
better performing ensembles. Thus, we move on
now to the more dynamic stochastic strategies. Keep
in mind that since in our model the distribution of
q will be estimated, in any of the selection strategies
we can exploit this knowledge as a stopping rule.
Namely, even for the deterministic approaches, we
can check whether the accuracy of the extracted
ensemble is already attractive or whether we should
continue and search for a better one.

As the deterministic selection strategies may have
poor performance, we investigate stochastic algo-
rithms to address our optimization problem. Such
randomized algorithms, where randomization only
affects the order of the internal executions, produce
the same result on a given input, which can cause
the same problem we have found for the determin-
istic ones. In case of Monte Carlo (MC) algorithms
[20], the result of the simulations might change,
but they produce the correct result with a certain
probability. The accuracy of the MC approach de-
pends on the number of simulations N ; the larger
N is, the more accurate the algorithm will be. It
is important to know how many simulations are
required to achieve the desired accuracy. The error
of the estimate of the probability failure is found
(cid:112)(1 − Pf )/N Pf , where u1−α/2 is the
to be u1−α/2
1−α/2 quantile of the standard normal distribution,
and Pf is the true value of the probability of failure.
Simulated annealing (SA) [21], as a variant of
the Metropolis algorithm, is composed of two main
stochastic processes: generation and acceptance of
solutions. SA is a general-purpose, serial search
algorithm, whose solutions are close to the global
extremum for an energy function within a polyno-
mial upper bound for the computational time and
are independent of the initial conditions.

To compare the MC method with SA for solving
a knapsack problem, we applied simulations for
that scenario in which the deterministic approaches
failed to ﬁnd the most accurate ensemble, that is,
when D1 = (1 − β, T ), and D2 = D3 = . . . = Dn =
(1/2 + ε, T /n) with 0 < β < 1/2, 0 < ε < 1/2. For
this setup, we obtained that the precision of the MC
method was only 11%, while SA found the most
accurate ensemble in 96% of the simulations.

Now, we introduce a novel search strategy that
takes better advantage of our stochastic approach
than, e.g., SA. This strategy builds ensembles using
a randomized search technique and introduces a
concept of usefulness for member selection, which
better adapts to the ensemble energy than the classic
one (4). Namely, in our proposed approach, the
selection of the items for the ensemble is based on
the efﬁciency of the members determined in the
following way: for the i-th item with accuracy pi
and execution time ti, the system accuracy q(pi, ti)
of the ensemble containing the maximal number of
i-th items

q(pi, ti) =

(cid:98)T /ti(cid:99)
(cid:88)

k=0

(cid:18)(cid:98)T /ti(cid:99)
k

(cid:19)

pi

k(1 − pi)(cid:98)T /ti(cid:99)−k

(5)

characterizes the efﬁciency (usefulness) of the i-th
item, instead of (4).

A greedy algorithm for an optimization problem
always chooses the item that seems to be the most
useful at that moment. In our selection method,
a discrete random variable depending on the efﬁ-
ciency values of the remaining items is applied in
each step to determine the probability of choosing
an item from the remaining set to add to the ensem-
ble. Namely, in the k-th selection step, if the items
i1, . . . , ik−1 are already in the ensemble, then the efﬁ-
ciency values q(k−1)(pi, ti) of the remaining items are
updated to the maximum time of Tk = T − (cid:80)k−1
j=1 tij ,
where q(0)(pi, ti) = q(pi, ti) and T0 = T .

The i-th item is selected as the next member of

the ensemble with the following probability:

(6)

(Pens)(k)

i =

q(k−1)(pi, ti)
q(k−1)(pj, tj)

,

(cid:80)
j
where i, j ∈ N \{i1, . . . , ik−1}. This discrete random
variable reﬂects that the more efﬁcient the item
is, the more probable it is to be selected for the
ensemble in the next step.

If ti > Tk for all i ∈ N \{i1, . . . , ik−1}, then
our stochastic process ends for the given search
step since there is not enough remaining time for
any items. Then, we restart the process to extract
another ensemble in the next search step. As a for-
mal description of our proposed stochastic search
method, see Algorithm 1; notice that we evaluate
the accuracy of ensembles with odd cardinalities
only as in Lemma 1.1. A very important issue
regarding both our approach and SA is the exact
deﬁnition of the number of search steps, that is,
a meaningful STOP parameter – and also an es-
caping MAXSTEP one – for Algorithm 1. In our
preliminary work [19], we have already tested the
efﬁciency of our approach; however, we tested it
empirically with an ad hoc stopping rule. Now, in the
forthcoming sections, we present how the proper
derivation of the stopping parameters (STOP and
MAXSTEP) can be derived.

4 STOCHASTIC ESTIMATION OF ENSEMBLE
ENERGY

We need to examine and characterize the behavior
of q(cid:96) in (1) to exploit these results to ﬁnd and apply
the proper stopping criteria in stochastic search
methods.

Let p ∈ [0, 1] be a random variable with mean µp
and variance σ2
p, where pi (i = 1, 2, . . . , n) are inde-
pendent and identically distributed according to p,

5

Algorithm 1 Proposed Stochastic searcH for Ensem-
bLe Creation (SHErLoCk).

Input: Pool D = {(pi, ti) , i = 1, . . . , n},
Total allowed time T ,
Stochastic stopping value STOP,
Maximum search steps MAXSTEP.

Output: An ensemble MAXENS ⊆ D to maximize
system accuracy (1) within time T as in (2).

1: STEP ← 0, MAXENS ← ∅, q(cid:96)0 ← 0
2: while STEP<MAXSTEP do
3: H ← D, ENS ← ∅, T (cid:48) ← T , SP ← ∅
4: while ∃(pj, tj) ∈ H : tj ≤ T − (cid:80)

tk do

(pk,tk)∈ENS
∀(pi, ti) ∈ H calculate q(pi, ti) by (5)
∀(pi, ti) ∈ H calculate Pensi by (6) and

SP ← SP ∪ {Pensi}

Select a (pj, tj) randomly from H by

5:
6:

7:

EN S ← EN S ∪ {(pj, tj)}
H ← H \ {(pj, tj)}
T (cid:48) ← T (cid:48) − tj
if mod(size(ENS), 2) = 1 then
Calculate q(cid:96)(ENS) by (1)

distribution SP

if tj < T (cid:48) then

end if
if q(cid:96)0 < q(cid:96) then

8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24: end while
25: return MAXENS

end if

end if
end while
STEP ← STEP + 1

end if
if q(cid:96)0 > ST OP then
return MAXENS

q(cid:96)0 ← q(cid:96), MAXENS ← ENS

i.e., a sample. Furthermore, let µq(cid:96) and σ2
q(cid:96) denote
the mean and variance of the ensemble accuracy q(cid:96),
respectively. In this case, it is seen that µp ≤ 1 and a
simple calculation shows that

(cid:96)
(cid:88)

µq(cid:96) =

k=(cid:98) (cid:96)

2 (cid:99)+1

(cid:19)

p (1 − µp)(cid:96)−k .
µk

(cid:18)(cid:96)
k

(7)

The following lemma shows the basic properties of
the mean and the variance of q(cid:96).

Lemma 4.1. Let p ∈ [0, 1] be a random variable with
mean µp and variance σ2
p. Consider the accuracy (1),
where pi, i = 1, 2, . . . , n are i.i.d. random variables
distributed as p. Then,

1)

lim
(cid:96)→∞

µq(cid:96) =






0,
1/2,
1,

if µp /∈ [1/2, 1),
if
if µp ∈ (1/2, 1).

µp = 1/2,

(8)

for odd (cid:96):

is
Moreover,
increasing, and if µp ∈ (0, 1/2), then µq(cid:96) is decreasing.
2) The variance of q(cid:96) is expressed by

if µp ∈ (1/2, 1), then µq(cid:96)

(cid:96)
(cid:88)

k
(cid:88)

(cid:96)−k
(cid:88)

σ2
q(cid:96)

=

m=1

h=k(cid:96)−m

δ ((cid:96), m, k)

(cid:19)

(cid:18)(cid:96)
k

(9)

T sk−m+h
sm
T F

s(cid:96)−k−h
F

− (µq(cid:96))2 ,

(cid:18) k
m

×

(cid:19)(cid:18)(cid:96) − k

k=k(cid:96)
(cid:19)

h

p + (1 − µp)2, sT F = µp (1 − µp) − σ2
(cid:5) + 1.

where δ ((cid:96), m, k) = δk(cid:96)−m≤(cid:96)−k, sT = σ2
sF = σ2
k(cid:96) = (cid:4) (cid:96)
3) If µp (cid:54)= 1/2, µp (1 − µp) − σ2
then

p + µ2
p,

2

p > 0, and sT (cid:54)= 1/2,

p, and

lim
(cid:96)→∞

σ2
q(cid:96)

= 0.

(10)

If sT = 1/2, then the limit (10) is 1.

Proof. See Appendix E for the proof.

Notice that the condition (cid:96) → ∞ naturally as-
sumes the same for the pool size with n → ∞ in
Lemma 4.1. As a demonstrative example for the
ﬁrst part of Lemma 4.1, see Figure 1 regarding the
three possible accuracy limits described in (8) with
respective Beta(αp, βp) distributions for p.

Fig. 1: Convergence of ensemble accuracies for
member accuracies coming from different
Beta(αp, βp) distributions.

Now, to devise a stochastic model, we start with
checking the possible distributions of the member

6

accuracy values pi to estimate the ensemble accu-
racy. Then, we extend our model regarding this
estimation by incorporating time information, as
well. Notice that the estimation of the ensemble
accuracy will be exploited to derive a stopping rule
for the ensemble selection process.

4.1 Estimation of the distribution of member
accuracies

Among the various possibilities, we have found
that the beta distribution is a very good choice to
analyze the distribution of member accuracies. The
main reason is that beta concentrates on the interval
[0, 1], that is, it can exactly capture the domain for
the smallest/largest accuracy. Moreover, the beta
distribution is able to provide density functions of
various shapes that often appear in practice. Thus,
to start the formal description, let the variate p be
distributed as Beta(αp, βp) with density

b (x; αp, βp) =

xαp−1 (1 − x)βp−1
B (αp, βp)

,

(11)

where B (αp, βp) = Γ (αp) Γ (βp) /Γ (αp + βp). In this
case,

µp = αp/ (αp + βp) ,

(12)

and µp ∈ (1/2, 1) if and only if αp > βp. If αp = βp,
then µp = 1/2. In the case of αp > βp, the mode is
also greater than 1/2. The mode is inﬁnite if βp < 1;
therefore, we exclude this situation and we assume
from now on that

1 < βp < αp.

The variance of p is

σ2
p =

αpβp
(αp + βp)2 (αp + βp + 1)

.

(13)

(14)

q(cid:96) depend on µp, and σ2

Since µq(cid:96), and σ2
p according
to (7) and (9) respectively, one can calculate both
of them explicitly. The convergence of µq(cid:96) to 1 is
fast if µp is close to 1, i.e., βp (cid:28) αp; for instance, if
αp = 17, βp = 5. Simulations show that the speed
of the convergence of σ2
q(cid:96) is exponential; hence, the
usual square-root law does not provide the Central
Limit Theorem for q(cid:96).

In practice, we perform a beta ﬁt on the pi’s
(i = 1, . . . , n). If a ﬁt is found at least at the
conﬁdence level 0.95, we take the parameters αp, βp
provided by the ﬁt and calculate µp, σ2
p by (12) and
(14), respectively. If the beta ﬁt is rejected, then µp

p are estimated from the pi’s as the empirical

and σ2
mean and variance:

This implies that the estimated number of ensemble
members up to time T is (cid:99)(cid:96)T =

(cid:108)
T βp−1

(cid:109)

.

αp+βp−1

7

µp =

1
n

n
(cid:88)

i=1

pi, σ2

p =

1
n − 1

n
(cid:88)

i=1

(pi − µp)2.

(15)

2) If the interarrival times τj correspond to a given T

and λ generated from Beta (βp, αp), then

To simplify our further notation we do not indicate
whether the mean and variance have been esti-
mated from the ﬁtted distribution or empirically.

4.2 Adding time constraints to the model

Now, we turn to the case when together with the
item accuracy pi, we consider its running time ti,
as well. The common distribution of a random time
is exponential, so let τ be an exponential distribu-
tion with density λ exp (−λt). If p is distributed as
Beta(αp, βp), then with setting λ = 1 − p for a given
p, the distribution of λ becomes Beta(βp, αp).

This is a reasonable behavior of time because
it is quite natural to assume that more accurate
components require more resources such as a larger
amount of computation times. On the other hand,
the selection procedure becomes trivial, if, e.g., the
time and accuracy are not inversely proportional,
since then the most accurate member is also the
fastest one; therefore, it should be selected ﬁrst by
following this strategy for the remaining members
until reaching the time limit. For some other possi-
ble simple accuracy–time relations, see our prelimi-
nary work [19].

For a given time constraint T , consider the ran-

dom number (cid:96)T such that

(cid:96)T(cid:88)

j=0

τj ≤ T.

(16)

With the following lemma, our purpose is to pro-
vide an estimation (cid:99)(cid:96)T for the expected size of the
composed ensemble and incorporate this informa-
tion in our stochastic characterization of q(cid:96).

Lemma 4.2. Let τ be an exponential distribution with
density λ exp (−λt) under the condition that the param-
eter λ is distributed as Beta(βp, αp), where 2 < βp < αp.
1) Then, the expected time for the sum of n variables is

E((cid:96)T ) =

βp
αp + βp

T.

(19)

3) If each component of pair (λj, τj) are independent

copies of λ and τj corresponds to λj, then

E((cid:96)T ) = T

βp − 1
αp + βp − 1

.

(20)

In both cases 2) and 3), (cid:96)T is distributed as Poisson
with parameter T /Eτ1, which implies that V ar ((cid:96)T ) =
E((cid:96)T ) and the estimation of (cid:96)T is

(cid:99)(cid:96)T = T /τ .

(21)

Proof. See Appendix F for the proof.

So far, we have assumed that p is distributed as
beta to calculate (cid:99)(cid:96)T by Lemma 4.2. If this is not the
case, we consider the following simple and obvious
calculation for the approximate number of (cid:96) under
the time constraint T :

(cid:38)

(cid:99)(cid:96)T =

nT

(cid:30) n
(cid:88)

(cid:39)

ti

i=1

= (cid:6)T /t(cid:7) ;

(22)

another alternative to derive (cid:99)(cid:96)T in this case is dis-
cussed in section 6. In either way it is derived, the
value (cid:99)(cid:96)T will be used in the stopping rule in our
ensemble selection procedure; the proper details
will be given next.

4.3 Stopping rule for ensemble selection
The procedure of ﬁnding (L0, (cid:96)0) is a selection task
that is NP-hard. We propose an algorithm such that
we stop the selection when the value of q(cid:96) (L) is
sufﬁciently close to the possible maximum, which
is not known. To be able to do so, we must give
a proper stochastic characterization of q(cid:96) by also
settling on the calculation of µq(cid:96) and σ2
q(cid:96) via Lemma
4.1. First, notice that the values of q(cid:96) are in (0, 1);
indeed, it is positive and

(cid:96)
(cid:88)

q(cid:96) =

(cid:88)

(cid:89)

pi

(cid:89)

(1 − pj)

n
(cid:88)

k=0

(cid:18)

Eτk = n

1 +

(cid:19)

αp
βp − 1

with variance
(cid:32) n
(cid:88)

V ar

k=0

(cid:33)

(cid:18)

τk

= n

1 +

(cid:19)

.

αp
βp − 2

(17)

k=(cid:98) (cid:96)

2 (cid:99)+1

I⊆N
|I|=k

i∈I

j∈N \I

(23)

(cid:89)

<

j

(pj + (1 − pj)) = 1.

(18)

For the case when pi’s are beta distributed, the
product of independent beta variates can be close

to beta again; see [22]. We have also performed MC
simulation and found that beta distributions ﬁt q(cid:96)
particularly well, compared to, e.g., the gamma,
normal, Weibull, and extreme-valued distributions.
Speciﬁcally, though the beta behavior of q(cid:96) was
naturally more stable for beta distributed pi’s, the
usual behavior of q(cid:96) was also the same for non-beta
pi’s.

TABLE 1: Probability values (cid:37)q
olds for different skewness coefﬁcients γ.

(cid:98)(cid:96) for stopping thresh-

8

γ
γ ≤ 1
1 < γ ≤ 2.5
2.5 < γ ≤ 3.5
3.5 < γ

(cid:37)q
(cid:98)(cid:96)
0.6
0.8
0.9
0.95

Thus, to provide a description of the stochastic
behavior of q, we consider the following strategy.
With a primary assumption on the Beta(αq, βq) dis-
tribution of q(cid:96), we calculate αq and βq as
(cid:18) 1
µq

(cid:18) 1 − µq
σ2
q

q, βq = αq

1
µq

αq =

(24)

− 1

µ2

−

(cid:19)

(cid:19)

.

If time information is provided for the pool items,
we calculate (cid:99)(cid:96)T by Lemma 4.2, and as a simpler
notation, we will write (cid:98)(cid:96) from now on. If time
information is not available, we will set (cid:98)(cid:96) = n.

Next, we decide whether q(cid:96) should be consid-
ered as beta with requiring 1 < βq < αq to be
fulﬁlled to have a mode that is larger than 1/2 and
ﬁnite. If this condition does not hold, we reject the
beta behavior of q(cid:96), and based on simulations, we
characterize it as a normal distribution and stop the
search if

(cid:112)

/
q(cid:96) ≥ κ0.9σq
(cid:98)(cid:96)

(cid:98)(cid:96) + µq
(cid:98)(cid:96)

= ST OP,

(25)

where κ0.9 is the 0.9 quantile of the standard normal
distribution. Otherwise, when q(cid:96) is considered beta,
we calculate the mode ν of Beta(αq, βq) for q(cid:96) as

ν =

αq − 1
αq + βq − 2

,

and the Pearson’s ﬁrst skewness coefﬁcient as

γ =

1 − ν
σq
(cid:98)(cid:96)

.

(26)

(27)

Then, we use Table 1 to select the appropriate
probability value (cid:37)q
(cid:98)(cid:96); the entries are determined by
simulation in the case of 2 ≤ βq < αq.

We stop the selection when the ensemble accu-
racy reaches the value of the inverse cumulative
distribution F −1
) of Beta(αq, βq) in the given
((cid:37)q
(cid:98)(cid:96)
probability, that is, when

αq,βq

q(cid:96) ≥ F −1

αq,βq

) = ST OP.
((cid:37)q
(cid:98)(cid:96)

(28)

In either via (25) or (28), an estimation for the en-
semble accuracy is gained; we obtain a STOP value
to stop the stochastic search. However, there is some
chance that STOP is not exceeded, though in our
experiments it has never occurred. Thus, to avoid an

inﬁnite loop, we consider a maximum allowed step
number MAXSTEP as an escaping stopping rule.
Namely, to obtain MAXSTEP, we apply Stirling’s
approximation

MAXSTEP =

(cid:19)

(cid:18)n
(cid:98)(cid:96)

∼ n(cid:98)(cid:96)/(cid:98)(cid:96)!,

(29)

assuming that (cid:98)(cid:96)/n → 0. This is a reasonable app-
roach since (cid:98)(cid:96) is calculated according to Lemma 4.2
or (22). The formal description of our proposed
ensemble selection method is enclosed in Algorithm
2.

i=1 ti for the ﬁrst, and 20% of (cid:80)100

Before providing our detailed empirical results
in section 5, in Table 2 we summarize our ﬁndings
for Algorithm 2 on simulations. Namely, in two
respective tests with i = 1, . . . , 30 and i = 1, . . . , 100,
we have generated the pi’s to come from Beta(17, 5)
and the execution times ti from conditional expo-
nential distributions with parameters λ = 1−pi. The
time constraint T was set in seconds to 30% of the
total time (cid:80)30
i=1 ti for
the second test. Both tests were repeated 100 times,
and we have taken the averages of the obtained
precisions. As our primary aim, we have checked
whether the stopping rule of the stochastic search
indeed led to a reasonable computational gain.
For the sake of completeness, in Table 2 we have
also shown the results regarding letting the search
continue in the long run (stopped by MAXSTEP),
though in each of our tests, the STOP value has
been exceeded much earlier. Secondarily, we have
compared SA with our selection method SHErLoCk
given in Algorithm 1. For Table 2, we can conclude
that applying our stopping rule by using STOP
saved considerable computational time compared
with the exhaustive search that culminated by stop-
ping it with MAXSTEP with a negligible drop in ac-
curacy. Moreover, our approach has found efﬁcient
ensembles quicker than SA. These impressions have
also been conﬁrmed by the empirical evaluations on
real data described in the next section.

Algorithm 2 Proposed Ensemble Creation Method.
Input: [NO-TIME]: Pool D = {pi}n
i=1.
Input: [TIME]: Pool D = {(pi, ti)}n
i=1,
Total allowed time T .

Output: An ensemble MAXENS ⊆ D to maximize
system accuracy (1) within time T as in (2).
1: Calculate the mean µp and std σp for {pi}n
by (12) and (14) (if a beta ﬁts to p) or
empirically (if p is not beta) by (15)

i=1

2: switch Input do
case NO-TIME
3:
4:
5:
6:

(cid:98)(cid:96) ← n
case TIME

Estimate # of members (cid:98)(cid:96) for T by

Lemma 4.2 if a beta ﬁts to p, or by (22)
if p is not beta

(cid:98)(cid:96) by (7) and σ2

7: Calculate µq
8: Calculate αq, βq by (24)
9: if 1 < βq < αq then
10:

Calculate cdf. Fαq,βq , ν, γ, (cid:37)q

q
(cid:98)(cid:96)

by (9)

(cid:98)(cid:96) by (26), (27)

and Table 1, and adjust STOP with (28)

11: else
12: Adjust STOP with (25)
13: end if
14: Calculate MAXSTEP by (29)
15: switch Input do
case NO-TIME
16:
17:

Compose ensemble by SA using STOP
and MAXSTEP for the stopping rule

18:
19:

case TIME

Compose ensemble either by Algorithm 1
(SHErLoCk) or SA using STOP and
MAXSTEP for the stopping rule

TABLE 2: Result of Algorithm 2 on simulations.

Search
method
SHErLoCk (n=30)
SA (n=30)
SHErLoCk (n=100)
SA (n=100)

Ensemble accuracy Comp. time (secs)
MAXSTEP STOP MAXSTEP STOP
0.08
0.30
1.54
1.58

99.56% 99.39%
98.97% 98.91%
99.66% 99.61%
99.38% 99.37%

60.03
87.40
294.58
638.39

5 EMPIRICAL ANALYSIS

9

scores of participants of Kaggle1 challenges without
cost/time information provided. Our second setup
for ensemble creation considers machine learning-
based binary classiﬁers as possible members; the
performance evaluation is performed on several
UCI Machine Learning Repository [23] datasets
with the training times considered as costs.

5.1 Kaggle challenges

Kaggle is an open online platform for predictive
modeling and analytics competitions with the aim
of solving real-world machine learning problems
provided by companies or users. The main idea be-
hind this crowd-sourcing approach is that a count-
less number of different strategies might exist to
solve a speciﬁc task, and it is not possible to know
beforehand which one is the most effective. Though
primarily only the scores of the participating algo-
rithms can be gathered from the Kaggle site, as a
possible future direction, we are curious regarding
whether creating ensembles from the various strate-
gies could lead to an improvement regarding the
desired task.

Not all the Kaggle competitions are suitable to
test our models since in the current content, we
focus on majority voting-based ensemble creation.
Consequently, we have collected only such com-
petitions and corresponding scores where majority
voting-based aggregation could take place. More
precisely, we have restricted our focus only to such
competition metrics based on which majority vot-
ing can be realized. Such metrics include quadratic
weighted kappa, area under the ROC curve (AUC),
log loss, normalized Gini coefﬁcient. For concrete
competitions where these metrics were applied, we
analyze the following ones: Diabetic Retinopathy
Detection2, DonorsChoose.org Application Screen-
ing 3, Statoil/C-CORE Iceberg Classiﬁer Challenge4,
WSDM - KKBox’s Churn Prediction Challenge5, and
Porto Seguros Safe Driver Prediction6.

For our analytics, on the one hand it is inter-
esting to observe the distribution of the ﬁnal score
of the competitors, which is often affected by the
volume of the prize money offered to the winner.
Moreover, accuracy measurement is usually scaled

In this section, we demonstrate the efﬁciency of our
models through an exhaustive experimental test on
publicly available data. Our ﬁrst experiment con-
siders the possibility of organizing competing ap-
proaches with different accuracies into an ensemble.
In this scenario, accuracy values correspond to ﬁnal

1. www.kaggle.com
2. www.kaggle.com/c/diabetic-retinopathy-detection
3. www.kaggle.com/c/donorschoose-application-screening
4. www.kaggle.com/c/statoil-iceberg-classiﬁer-challenge
5. www.kaggle.com/c/kkbox-churn-prediction-challenge
6. www.kaggle.com/c/porto-seguro-safe-driver-

prediction/data

to the interval [0, 1], with 0 for the worst and 1 for
the perfect performance, which allows us to test
our results regarding the beta distributions. As a
drawback of Kaggle data, access to the resource con-
straints corresponding to the competing algorithms
(e.g., training/execution times) is rather limited;
such data are provided for only a few competitions,
primarily in terms of execution time interval.

Thus, to summarize our experimental setup,
we interpret the competing solutions of a Kaggle
challenge as the {D1, D2, . . . , Dn} pool, where the
score of Di is used for the accuracy term pi ∈ [0, 1]
in our model. Then, we apply a beta ﬁt for each
investigated challenge to determine whether a beta
distribution ﬁts the corresponding scores or not. If
the test is rejected, we can still use the estimation
for the joint behavior q using (15) and (22). If the
beta test is accepted, we can also apply our corre-
sponding results using (12), (14), and Lemma 4.2.
Notice that reliably ﬁtting a model for the scores of
the competitors might lead to a better insight of the
true behavior of the data of the given ﬁeld, also for
the established expectations there.

As observed from Table 3, SA was able to stop
much earlier with a slight loss in accuracy using
the suggested stopping rule (STOP) in ﬁnding the
optimal ensemble. Our approach SHErLoCk given
in Algorithm 1 has been excluded from this analysis
since no cost information was available.

5.2 Binary classiﬁcation problems

The UCI Machine Learning Repository [23] is a pop-
ular platform to test the performances of machine
learning-based approaches, primarily for classiﬁca-
tion purposes. A large number of datasets are made
publicly available here among which our models
can be tested on binary classiﬁcation ones. That is,
in this experiment, the members D1, D2, . . . , Dn of a
pool for ensemble creation are interpreted as binary
classiﬁers, whose outputs can be aggregated by the
majority voting rule. Using the ground truth sup-
plied with the datasets, the accuracy term pi ∈ [0, 1]
stands for the classiﬁcation accuracy of Di.

The number of commonly applied classiﬁers is
relatively low; therefore to increase the cardinality
of the pool, we have also considered a synthetic
approach in a similar way to [24]. Namely, we have
trained the same base classiﬁer on different train-
ing datasets, by which we can synthesize several
”different” classiﬁers. Naturally, this method is able
to provide more independent classiﬁers only if the
base classiﬁer is unstable, i.e., minor changes in

10

the training set can lead to major changes in the
classiﬁer output; such an unstable classiﬁer is, for
example, the perceptron one.

To summarize our experimental setup for UCI
binary classiﬁcation problems, we have considered
base classiﬁers perceptron [25], decision tree [26],
Levenberg-Marquardt feedforward neural network
[27], random neural network [28], and discrimina-
tive restricted Boltzmann machine classiﬁer [29] for
the datasets MAGIC Gamma Telescope [30], HIGGS
[31], EEG Eye State [23], Musk (Version 2) [32], and
Spambase [23]; datasets of large cardinalities were
selected to be able to train synthetic variants of base
classiﬁers on different subsets. To check our models
for different numbers of possible ensemble mem-
bers, the respective pool sizes were set to n = 30
and n = 100; the necessary number of classiﬁers
has been reached via synthesizing the base clas-
siﬁers with training them on different subsets of
the training part of the given datasets. In contrast
to the Kaggle challenges, in these experiments we
were able to retrieve meaningful cost information to
devise a knapsack scenario. Namely, for a classiﬁer
Di, its training time was adjusted as its cost ti in our
model. Notice that for even the same classiﬁer, it
was possible to obtain different ti values with train-
ing its synthetic variants on datasets of different
sizes. Using this time information, for the estimated
size (cid:98)(cid:96) of the optimal ensemble, we could use Lemma
4.2 for n = 30, while (22) for the case n = 100.

As clearly visible from Tables 4 and 5, our
stochastic search strategy SHErLoCk described in
Algorithm 1 was reasonably faster than SA and
slightly dominant in accuracy, as well. Moreover,
it can be observed again that applying the stopping
rule with the threshold STOP led to an enormous
computational advantage for either search strate-
gies with only a small drop in accuracy.

5.3 Optic disc detection

The majority voting rule can be applied in a prob-
lem to aggregate the outputs of single object de-
tectors in the spatial domain [13]; the votes of the
members are given in terms of single pixels as
candidates for the centroid of the desired object.
In this extension, the shape of the desired object
deﬁnes a geometric constraint, which should be met
by the votes that can be aggregated. In [13], our
practical example relates to the detection of a disc-
like anatomical component, namely the optic disc
(OD) in retinal images. Here, the votes are required
to fall inside a disc of diameter dOD to vote together.

TABLE 3: Ensemble accuracies on the Kaggle datasets found by simulated annealing (SA).

11

Dataset
Name
Diabetic Retinopathy Detection
DonorsChoose.org Application Screening
Statoil/C-CORE Iceberg Classiﬁer Challenge
WSDM - KKBox’s Churn Prediction Challenge
Porto Seguros Safe Driver Prediction
Average

Ensemble accuracy
STOP
93.19%
91.96%
87.76%
96.32%
89.98%
90.45%

MAXSTEP
94.34%
94.78%
88.42%
96.96%
92.99%
92.29%

Computational time (secs)
MAXSTEP
194.12
206.89
191.91
203.88
214.28
202.21

STOP
1.31
1.67
2.23
1.45
1.95
1.72

TABLE 4: Comparing simulated annealing (SA) with the proposed search strategy (SHErLoCk) on binary
classiﬁcation problems of UCI datasets using an ensemble pool of n = 30 classiﬁers.

Dataset

Ensemble accuracy

Computational time (secs)

Name

Size

MAGIC
Spambase
HIGGS
EEG
Musk

19 020
4 601
20 000
14 980
6 598

Average

MAXSTEP

STOP

MAXSTEP

SA
99.34%
99.68%
78.03%
98.71%
99.95%
95.15%

SHErLoCk
99.57%
99.76%
77.99%
98.80%
99.99%
95.07%

SA
99.17%
98.95%
77.59%
95.62%
99.96%
94.26%

SHErLoCk
99.29%
98.77%
77.63%
97.43%
99.98%
94.62%

SA
171.9
100.98
158.90
345.57
178.21
191.11

SHErLoCk
56.47
67.90
69.46
90.20
58.89
68.06

STOP
SHErLoCk
0.18
0.39
0.93
0.38
0.32
0.44

SA
0.41
1.67
2.73
0.47
0.33
1.12

TABLE 5: Comparing simulated annealing (SA) with the proposed search strategy on binary classiﬁcation
problems of UCI datasets using an ensemble pool of n = 100 classiﬁers.

Dataset

Ensemble accuracy

Computational time (secs)

Name

Size

MAGIC
Spambase
HIGGS
EEG
Musk

19 020
4 601
20 000
14 980
6 598

Average

MAXSTEP

STOP

MAXSTEP

SA
99.57%
99.79%
78.08%
98.96%
99.98%
95.28%

SHErLoCk
99.59%
99.78%
78.16%
98.96%
99.99%
95.29%

SA
99.19%
98.96%
77.79%
97.32%
99.98%
94.65%

SHErLoCk
99.34%
98.88%
77.73%
97.64%
99.98%
94.71%

SA
349.62
390.89
378.56
453.59
475.71
409.67

SHErLoCk
194.12
206.89
191.91
203.88
214.28
202.21

STOP
SHErLoCk
1.32
1.41
2.11
1.33
1.56
1.55

SA
1.31
1.67
2.23
1.45
1.95
1.72

As more false regions are possible to be formed, the
correct decision can be made even if the true votes
are not in the majority, as in Figure 2. The geometric
restriction transforms (1) to the following form:








(cid:96)
(cid:88)

k=0

p(cid:96),k

q(cid:96)(L) =



(cid:88)

(cid:89)

(cid:89)

pi

i∈I

j∈L\I

I⊆L
|I|=k

(1 − pj)






.

(30)

In (30), the terms p(cid:96),k describe the probability that
a correct decision is made by supposing that we
have k correct votes out of (cid:96). For the terms p(cid:96),k (k =
0, 1, . . . , (cid:96)), in general, we have that 0 ≤ p(cid:96),0 ≤ p(cid:96),1 ≤
· · · ≤ p(cid:96),(cid:96) ≤ 1.

the pool

In our experiments,

consists of
eight OD detector algorithms with the following
accuracy and running time values: {(pi, ti)}8
i=1 =
{(0.220, 31), (0.304, 38), (0.319, 34), (0.643, 69),
(0.754, 11), (0.765, 7), (0.958, 21), (0.976, 90)}
(cid:80)8

with
i=1 ti = 301 secs. We can apply our theoretical

Fig. 2: Successful OD detection with the same num-
ber of correct/false ensemble member responses.

foundation with some slight modiﬁcations to
solve the same kind of knapsack problem for the
variant (30), transforming the model to reﬂect the
multiplication with the terms p(cid:96),k.

We have empirically derived the values p8,k =
{0, 0.11, 0.70, 0.93, 0.99, 1.00, 1.00, 1.00, 1.00} for (30)

in our task. To adopt our approach by following the
logic of Algorithm 2, we need to determine a STOP
value for the search based on µp and σp (calculated
by (15)), and (cid:98)(cid:96) (calculated by (22)). However, since
now the energy function is transformed by the
terms p(cid:96),k in (30), we must borrow the correspond-
ing theoretical results from [33] to derive the mean
µq
(cid:98)(cid:96) instead of (7) proposed in Algorithm 2. Accord-
ingly, we had to ﬁnd a continuous function F that
ﬁt to the values p(cid:96),k, which was evaluated by re-
gression and resulted in F(x) = b/(b + xa/(1 − x)a)
with a = −3.43 and b = 101.7, as also plotted in
Figure 3. Now, by using Theorem 1 from [33], we
have gained µq
(cid:98)(cid:96)

= F(µp).

12

accuracy p is not a beta distribution. As an alter-
native, notice that it is known that for a given λ, T
and an independent exponential distributed τj, (cid:96)T is
distributed as Poisson with parameter λT . We can
use Lemma 4.2 and conclude that for a starting size
of the ensemble, one may choose (cid:99)(cid:96)T such that the
remaining possible values are beyond the 5% error.
It follows that we apply formula either

P ((cid:96)T > m0.05) = 0.05,

(31)

where m0.05 is the upper quantile of the Poisson
distribution with parameter T / (cid:80)n
i=1 pi, or use the
normal approximation to the Poisson distribution

(cid:96)T − T / (cid:80)n
(cid:112)T / (cid:80)n

i=1 ti

i=1 pi − 0.5

> 1.64,

(32)

which provides us the inequality

(cid:96)T >

T
(cid:80)n
i=1 ti

(cid:115)

+ 0.5 + 1.64

T
(cid:80)n
i=1 ti

= (cid:99)(cid:96)T .

(33)

In our experiments we have used (22) instead of
(33) to obtain (cid:99)(cid:96)T , since the latter provided slightly
too large estimated size values. However, for other
scenarios, it might be worthwhile to try (33), as well.
As some additional arguments, we call attention
to the following issues regarding those elements of
our approach that might need special care or can be
adjusted differently in other scenarios:

• We have assumed independent member ac-
curacy behavior, providing solid estimation
power in our tests. However, in the case of
strong member dependencies, deeper discov-
ery of the joint behavior might be needed.

• Stirling’s approximation considered in (29) may
provide values that are too small for the param-
eter MAXSTEP in the case of small pools. Since
this is an escape parameter, a sufﬁciently large
value should be selected in such cases instead.
• The time proﬁle λ = 1 − p considered in sec-
tion 4.2 is suited to our data; however, any
other relationship between the member accu-
racy and time can be considered. Nevertheless,
the proper derivation of the estimation of the
ensemble accuracy might be slightly more la-
borious.

• In (25), we have used a one-tailed (left-side)
hypothesis since q(cid:96) was close to 1. However, if
it is not that close to 1, a two-tailed hypothesis
can be meaningful, as well. Furthermore, if µq
is even smaller (say 0.7), then we can search
above this mean by considering a right-side
hypothesis.

Fig. 3: Determining the constrained majority voting
probabilities p(cid:96),k for our OD detector ensemble.

For our experiment to search for the best ensem-
ble, we have set the time constraint to be 80% of the
total running time, with T = 4((cid:80)8
i=1 ti)/5. For this
= 0.969 for
setup, we could estimate (cid:98)(cid:96) = 7 and µq
(cid:98)(cid:96)
the expected ensemble size and mean accuracy, re-
spectively. Then, these values have been considered
for Algorithm 2 to compare the performance of our
stochastic search method SHErLoCk with SA. As
shown in Table 6, our search strategy outperformed
SA also for the object detection problem both in
accuracy and computational time.

TABLE 6: Comparing SA with the proposed search
strategy SHErLoCk on the OD detection problem.

Search
method
SHErLoCk
SA

Ensemble accuracy Comp. time (secs)

STOP
99.45%
99.43%

STOP
0.07
0.16

6 DISCUSSION
For the approximate number (cid:99)(cid:96)T of the ensemble
size, we have considered (22) when the member

ACKNOWLEDGMENTS

This work was supported by the project EFOP-3.6.2-
16-2017-00015 supported by the European Union,
co-ﬁnanced by the European Social Fund.

REFERENCES

[1] L. Lam and S. Y. Suen, “Application of majority voting
to pattern recognition: An analysis of its behavior and
performance,” Trans. Sys. Man Cyber. Part A, vol. 27, no. 5,
pp. 553–568, Sep. 1997.

[2] L. K. Hansen and P. Salamon, “Neural network ensem-
bles,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 12, no. 10,
pp. 993–1001, Oct. 1990.
S.-B. Cho and J. H. Kim, “Combining multiple neural
networks by fuzzy integral for robust classiﬁcation,” IEEE
Transactions on Systems, Man, and Cybernetics, vol. 25, no. 2,
pp. 380–384, Feb 1995.

[3]

[4] E. B. Kong and T. G. Dietterich, “Error-correcting output
coding corrects bias and variance,” in Machine Learning
Proceedings 1995, A. Prieditis and S. Russell, Eds.
San
Francisco (CA): Morgan Kaufmann, 1995, pp. 313 – 321.

[5] T. K. Ho, J. J. Hull, and S. N. Srihari, “Decision combina-
tion in multiple classiﬁer systems,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 16, no. 1, pp.
66–75, Jan 1994.

[6] Y. S. Huang and C. Y. Suen, “A method of combining mul-
tiple experts for the recognition of unconstrained hand-
written numerals,” IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. 17, no. 1, pp. 90–94, Jan 1995.
[7] L. Xu, A. Krzyzak, and C. Y. Suen, “Methods of combining
multiple classiﬁers and their applications to handwriting
recognition,” IEEE Transactions on Systems, Man, and Cyber-
netics, vol. 22, no. 3, pp. 418–435, May 1992.

[8] C. Szegedy, W. Liu, Y.

Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich,
“Going deeper with convolutions,” in 2015 IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
June 2015, pp. 1–9.

[9] B. Harangi, A. Baran, and A. Hajdu, “Classiﬁcation of skin
lesions using an ensemble of deep neural networks,” in
40th Annual International Conference of the IEEE Engineering
in Medicine and Biology Society, EMBC 2018, Honolulu, HI,
USA, July 18-21, 2018, 2018, pp. 2575–2578.

[10] B. Antal and A. Hajdu, “An ensemble-based system for
automatic screening of diabetic retinopathy,” Knowledge-
Based Systems, vol. 60, pp. 20 – 27, 2014.

[11] ——, “An ensemble-based system for microaneurysm de-
tection and diabetic retinopathy grading,” IEEE Trans. on
Biomed. Eng., vol. 59, no. 6, pp. 1720–1726, June 2012.
[12] L. I. Kuncheva, Combining Pattern Classiﬁers: Methods and

Algorithms. Wiley-Interscience, 2004.

[13] A. Hajdu, L. Hajdu, A. J ´on´as, L. Kov´acs, and H. Tom´an,
“Generalizing the majority voting scheme to spatially
constrained voting,” IEEE Transactions on Image Processing,
vol. 22, no. 11, pp. 4182–4194, Nov 2013.

[14] A. Hajdu, L. Hajdu, L. Kov´acs, and H. Tom´an, “Diversity
measures for majority voting in the spatial domain,” in
Hybrid Artiﬁcial Intelligent Systems, J.-S. Pan, M. M. Polycar-
pou, M. Wo´zniak, A. C. P. L. F. de Carvalho, H. Quinti´an,
and E. Corchado, Eds. Berlin, Heidelberg: Springer Berlin
Heidelberg, 2013, pp. 314–323.

[15] S. Martello and P. Toth, Knapsack Problems: Algorithms and
Computer Implementations. New York, NY, USA: John
Wiley & Sons, Inc., 1990.

13

[16] T. Klastorin, “On a discrete nonlinear and nonseparable
knapsack problem,” Operations Research Letters, vol. 9,
no. 4, pp. 233 – 237, 1990.

[17] T. C. Sharkey, H. E. Romeijn, and J. Geunes, “A class
of nonlinear nonseparable continuous knapsack and
multiple-choice knapsack problems,” Mathematical Pro-
gramming, vol. 126, no. 1, pp. 69–96, Jan 2011.

[18] M. Kurz, G. H ¨olzl, and A. Ferscha, “Enabling dynamic
sensor conﬁguration and cooperation in opportunistic
activity recognition systems,” International Journal of Dis-
tributed Sensor Networks, vol. 9, no. 6, p. 652385, 2013.
[19] A. Hajdu, H. Tom´an, L. Kov´acs, and L. Hajdu, “Compos-
ing ensembles by a stochastic approach under execution
time constraint,” in 2016 23rd International Conference on
Pattern Recognition (ICPR), Dec 2016, pp. 222–227.

[20] R. Tempo and H. Ishii, “Monte carlo and las vegas ran-
domized algorithms for systems and control*: An intro-
duction,” European Journal of Control, vol. 13, no. 2, pp. 189
– 203, 2007.

[21] K. Du and M. Swamy, Search and Optimization by Meta-
heuristics: Techniques and Algorithms Inspired by Nature.
Springer International Publishing, 2016.

[22] A. K. Tang, J.and Gupta, “On the distribution of the
product of independent beta random variables,” Statistics
& Probability Letters, vol. 2, no. 3, pp. 165–168, 1984.
[23] D. Dheeru and E. Karra Taniskidou, “UCI machine learn-

ing repository,” 2017.

[24] G. D. Cavalcanti, L. S. Oliveira, T. J. Moura, and G. V.
Carvalho, “Combining diversity measures for ensemble
pruning,” Patt. Rec. Lett., vol. 74, pp. 38 – 45, 2016.

[25] Y. Freund and R. E. Schapire, “Large margin classiﬁcation
using the perceptron algorithm,” Machine Learning, vol. 37,
pp. 277–296, 1999.

[26] J. R. Quinlan, “Induction of decision trees,” Machine Learn-

ing, vol. 1, pp. 81–106, 1986.

[27] A. A. Suratgar, M. B. Tavakoli, and A. Hoseinabadi, “Mod-
iﬁed levenberg-marquardt method for neural networks
training,” World Academy of Science, Engineering and Tech-
nology, pp. 24–48, 2005.

[28] S. Timotheou, “The random neural network: A survey,”

Comput. J., vol. 53, pp. 251–267, 2010.

[29] H. Larochelle and Y. Bengio, “Classiﬁcation using discrim-
inative restricted boltzmann machines,” Proceedings of the
25th International Conference on Machine Learning (ICML),
pp. 536–543, 2008.

[30] R. Bock, A. Chilingarian, M. Gaug, F. Hakl, T. Hengste-
beck, M. Jirina, J. Klaschka, E. Kotrc, P. Savicky, S. Towers,
A. Vaiciulis, and W. Wittek, “Methods for multidimen-
sional event classiﬁcation: a case study using images from
a cherenkov gamma-ray telescope,” Nuclear Instruments
and Methods in Physics Research Section A: Accelerators,
Spectrometers, Detectors and Associated Equipment, vol. 516,
no. 2, pp. 511 – 528, 2004.

[31] P. Baldi, P. D. Sadowski, and D. Whiteson, “Searching for
exotic particles in high-energy physics with deep learn-
ing.” Nature communications, vol. 5, p. 4308, 2014.

[32] T. G. Dietterich, A. N. Jain, R. H. Lathrop, and T. Lozano-
P´erez, “A comparison of dynamic reposing and tan-
gent distance for drug activity prediction,” in Advances
in Neural Information Processing Systems 6, J. D. Cowan,
G. Tesauro, and J. Alspector, Eds. Morgan-Kaufmann,
1994, pp. 216–223.

[33] A. Tiba, A. Hajdu, G. Terdik, and H. Toman, “Optimizing
majority voting based systems under a resource constraint
for multiclass problems,” in eprint arXiv:1904.04360, 2019.

APPENDICES

APPENDIX A
PROOF FOR LEMMA 1.1.

qn−1 (L), where L = {1, 2, . . . , 2(cid:96) − 1}. Set n = 2(cid:96);
then,

2(cid:96)
(cid:88)

q2(cid:96) (N ) =

(cid:88)

Q2(cid:96),k (N , I)

(38)

Proof. Consider a subset K when K = {1, 2, . . . , 2(cid:96)}
(otherwise we can renumerate pi). We have

k=(cid:96)+1

I⊆N
|I|=k

with N = {1, 2, . . . , 2(cid:96)} and put

2(cid:96)
(cid:88)

q2(cid:96) (K) =

(cid:88)

(cid:89)

(cid:89)

pi

(1 − pj)

k=(cid:96)+1

I⊆K
|I|=k

i∈I

j∈K\I

q2(cid:96)−1 (L) =

2(cid:96)−1
(cid:88)

(cid:88)

k=(cid:96)

I⊆L
|I|=k

Q2(cid:96)−1,k (L, I) ,

(39)

2(cid:96)
(cid:88)

(cid:88)

=

k=(cid:96)+1

I⊆K
|I|=k

Q2(cid:96),k (K, I) ,

(34)

notice that the number of terms is equal in both
sums. If k = 2(cid:96), then

Q2(cid:96),2(cid:96) (N , N ) = p2(cid:96)Q2(cid:96)−1,2(cid:96)−1 (L, L) ,

(40)

where

Q2(cid:96),k (K, I) =

(cid:89)

pi

(cid:89)

i∈I

j∈K\I

(1 − pj) ,

(35)

otherwise,

Q2(cid:96),k (N , I) =

that is, we consider a subset K ⊆ N with |K| = 2(cid:96),
and Q2(cid:96),k (K, I) is calculated for an index set I ⊆ K
with |I| = k. Now, choose an index a from the set
N \K, i.e., a > 2(cid:96), and obtain

Q2(cid:96),k (K, I) = Q2(cid:96),k (K, I) pa + Q2(cid:96),k (K, I) (1 − pa) .
(36)
The term Q2(cid:96),k (K, I) pa = Q2(cid:96)+1,k+1 ({K, a} , {I, a})
and Q2(cid:96),k (K, I) (1 − pa) = Q2(cid:96)+1,k ({K, a} , I); there-
fore,

q2(cid:96) (K) =

2(cid:96)
(cid:88)

(cid:88)

k=(cid:96)+1

I⊆K
|I|=k

Q2(cid:96),k (K, I)

(cid:26) p2(cid:96)Q2(cid:96)−1,k−1 (L, I\2(cid:96))
(1 − p2(cid:96)) Q2(cid:96)−1,k (L, I)

if
if

,

2(cid:96) ∈ I
2(cid:96) /∈ I
(41)

hence for k < 2(cid:96),

Q2(cid:96),k (N , I) = p2(cid:96)

(cid:88)

I⊆N
|I|=k

Q2(cid:96)−1,k−1 (L, I)

(cid:88)

I⊆L
|I|=k−1

+ (1 − p2(cid:96))

(cid:88)

I⊆L
|I|=k

Q2(cid:96)−1,k (L, I) .

(42)

We start summing up q (N , 2(cid:96)) from 2(cid:96); then, using
(40) and (42), we obtain for the ﬁrst two terms

2(cid:96)
(cid:88)

(cid:88)

k=2(cid:96)−1

I⊆N
|I|=2(cid:96)−1

Q2(cid:96),k (N , I) = p2(cid:96)Q2(cid:96)−1,2(cid:96)−1 (L, L)

2(cid:96)
(cid:88)

=

k=(cid:96)+1







(cid:88)

I⊆K
|I|=k

Q2(cid:96)+1,k+1 ({K, a} , {I, a}) +

(cid:88)

+p2(cid:96)

I⊆L
|I|=2(cid:96)−2

Q2(cid:96)−1,2(cid:96)−2 (L, I)+(1 − p2(cid:96)) Q2(cid:96)−1,2(cid:96)−1 (L, L)



= Q2(cid:96)−1,2(cid:96)−1 (L, L) + p2(cid:96)

Q2(cid:96)+1,k ({K, a} , I)





(cid:88)

+

I⊆K
|I|=k

Q2(cid:96)−1,2(cid:96)−2 (L, I) .

(cid:88)

I⊆L
|I|=2(cid:96)−2

(43)

2(cid:96)
(cid:88)

(cid:88)

<

k=(cid:96)+1

I⊆K
|I|=k

Q2(cid:96)+1,k (K, I) = q2(cid:96)+1 (K, a) ,

(37)

If we continue summing up one by one, then induc-
tion leads to

2(cid:96)−1
(cid:88)

(cid:88)

q2(cid:96) (N ) =

Q2(cid:96)−1,k−1 (L, I)

since q2(cid:96)+1 (K)
includes some extra additional
terms, say Q2(cid:96)+1,k ({K, a} , I), where I contains a.
Regarding that n is odd in the series of q(cid:96) (K), there
will be an element with odd (cid:96) following an element
of even (cid:96) and the lemma is proved for odd n. For
the case when n is even, we consider the qn (N ) and

k=(cid:96)+1

I⊆L
|I|=k
(cid:88)

+ p2(cid:96)

Q2(cid:96)−1,(cid:96) (L, I) < q2(cid:96)−1 (L) ,

(44)

I⊆L
|I|=(cid:96)

since p2(cid:96) < 1.

APPENDIX B
PROOF FOR PROPOSITION 2.1.

while an ensemble of lim
n→∞
found. Hence, the proposition follows.

qn(In) = 1 could also be

Proof. We prove the statement with an example
describing the worst-case scenario for the greedy
selection strategy. Let D = {D1 = (p1, t1), D2 =
(p2, t2), . . . , Dn = (pn, tn)} be the pool, where the
index set is denoted by In = {1, 2, . . . , n}. Let us

suppose that

ti ≤ T , that is, the time constraint

n
(cid:80)
i=1

should not be of concern. Let p1 = 1/2 + ε, where
0 < ε ≤ 1/2, and p2 = p3 = · · · = pn = 1/2 + α with
0 < α < ε, where the proper selection of α will be
given below.

The greedy strategy will move D1 to S as the
most accurate item in its ﬁrst step. Next, we try
to extend S by adding more members. Since we
require odd members, we try to add 2 items in every
selection step. Since all the remaining n − 1 features
have the same behavior, we can check whether S
should be extended via comparing the performance
of S1 = {D1} and S3 = {D1, D2, D3}. For the
performance of the ensemble S1, we trivially have
q1(I1) = p1 = 1/2 + ε, where I1 = {1}, while for S3
we can apply (1) for the 3-member ensemble, with
I3 = {1, 2, 3} to calculate q3(I3):

q3(I3) = p1p2(1 − p3) + p2p3(1 − p1) + p1p3(1 − p2)
ε
2

+ p1p2p3 =

+ α − 2α2ε

(45)

1
2

+

after the appropriate substitutions and simpliﬁca-
tions. Now, if we adjust α to have q1(I1) = q3(I3),
then via solving the equation

we obtain

1
2

+ ε =

1
2

+

ε
2

+ α − 2α2ε

1 −

α =

√

1 − 4ε2
4ε

.

(46)

(47)

That is, with a selection of α given in (47), the
ensemble S1 = {D1} is not going to be extended
since it does not lead to improvement. Thus, the
strategy stops after the ﬁrst step with an ensemble
accuracy 1/2 + ε.

On the other hand, with a sufﬁciently large n,
a very accurate ensemble could be achieved. More
precisely, it can be easily seen that qn(In) is strictly
monotonically increasing with

lim
n→∞

qn(In) = 1.

(48)

Now, by letting ε → 0, we can see that for the

ensemble accuracy found with this strategy

lim
ε→0

q1(S1) = 1/2,

(49)

APPENDIX C
PROOF FOR PROPOSITION 2.2.
Proof. We prove the statement with a similar exam-
ple to that given in the proof of Proposition 2.1 in
Appendix B to describe the worst case scenario. Let
D = {D1 = (p1, t1), D2 = (p2, t2), . . . , Dn = (pn, tn)}
be the pool and T be the time constraint. Put
p1 = 1/2 + ε, where 0 < ε ≤ 1/2, t1 = T , and
p2 = p3 = · · · = pm = 1/2 + α, t2 = t3 = · · · =
tn = T /(n − 1) with 0 < α < ε. If α is properly
selected, then q1(I1) = p1 < qn−1(In \ I1). However,
because of the time constraint, we must remove ele-
ments during the selection procedure, since initially
n
(cid:80)
i=1
approach in the ﬁrst step will remove any two
elements from D2, . . . , Dn by decreasing the time
with 2T /(n − 1). This selection will go on until only
D1 remains in the ensemble. With a proper selection
of α, we have lim
qn−1(In \ I1) = 1 and by letting
n→∞
ε → 0, the proposition follows.

ti = 2T > T . For this requirement, the greedy

APPENDIX D
PROOF FOR PROPOSITION 2.3.
Proof. Similar to the proof of Proposition 2.2, we
provide an example for the worst case scenario. Let
D1 = (1, T ), and D2 = D3 = D4 = (1/2 + ε, T /3)
with 0 < ε < 1/2. Now, since
1
T

3/2 + 3ε
T

= u2 = u3 = u4,

u1 =

(50)

<

the backward strategy will remove the less useful
component D1 ﬁrst to maintain the time constraint
and will keep the remaining ensemble {D2, D3, D4}
as the most accurate one, which also ﬁts the time

constraint with

ti = T . By letting ε → 0, we have

4
(cid:80)
i=2

limε→0 q3(I4 \ I1) = 1/2. Moreover, notice that the
most accurate ensemble would have been {D1} with
q1(I1) = 1 by meeting the time constraint, as well.
Thus, the statement follows.

APPENDIX E
PROOF FOR LEMMA 4.1
Proof. The ﬁrst part of the lemma corresponds to
Theorem 1 in [1]. For the rest, let us denote the
product of probabilities by

Π (I) =

(cid:89)

pi

(cid:89)

i∈I

j∈N \I

(1 − pj) ,

(51)

for simplifying the treatment below. The formula
(9) follows from expressing the variance in terms of
covariance

Var (q(cid:96)) =

(cid:96)
(cid:88)

(cid:88)

k,j=k(cid:96)

I,J⊆N
|I|=k,|J|=j

Cov (Π (I) , Π (J)) .

(52)

T ∪ I k

Now, we rewrite this expression into a more appro-
priate form. First, the notation is introduced, where
I k
T and I k
F for a partition of indices N = {1, . . . , (cid:96)},
such that N = I k
F where I k
T denotes indices of
those members voting true with accuracy p. Sim-
ilarly, I k
F contains indices of false votes. Observe
(cid:12)
(cid:12)
I k
F = N \I k
(cid:12)I k
(cid:12) = (cid:96) − k.
T . We have
F
T ∪ J j
F and J j
In the case of two partitions I k
F ,
let the number of the common elements of I k
T and
(cid:12)
(cid:12)
(cid:12)
J j
T ∩ J j
(cid:12)
(cid:12)
(cid:12)
(cid:12)I k
(cid:12) = nk.j; similarly,
(cid:12) = mk.j.
T be
According to this setup

(cid:12)
(cid:12) = k and
T ∪ I k
(cid:12)
F ∩ J j
(cid:12)
(cid:12)I k

(cid:12)
(cid:12)I k
T

F

T

Var (q(cid:96)) =

(cid:96)
(cid:88)

(cid:88)

k,j=k(cid:96)

T ,J j
I k

T

Cov (cid:0)Π (cid:0)I k
T

(cid:1) , Π (cid:0)J k
T

(cid:1)(cid:1) .

(53)

Observe I k
the product. Now, we consider the covariance

T when we apply the notation for

F = N \I k

We simplify (58), collecting similar terms and obtain
(9). Before we prove the limit (10), let us observe

sT + sT F = µp,
sF + sT F = 1 − µp,

sT + sF + 2sT F = 1.

(60)

(61)

(62)

i.e., the set {sT , sT F , sF , sT F } constitutes a proba-
bility distribution for sT F > 0; in other words,
p + σ2
µ2
p < µp. If it is so, we rewrite (9) in the form
of a multinomial distribution. The coefﬁcients in
(9) are actually multinomial coefﬁcients. The rest
of the proof is based on the approximation of the
binomial distribution by the normal distribution. It
is not complicated but slightly lengthy; we make it
available to the interested readers on request.

APPENDIX F
PROOF FOR LEMMA 4.2

Cov (cid:0)Π (cid:0)I k
(cid:1) , Π (cid:0)J k
(cid:1)(cid:1)
T
T
= E Π (cid:0)I k
(cid:1) Π (cid:0)J k
(cid:1) − E Π (cid:0)I k
(cid:1)
T
T
T
(cid:1) − µk+j (1 − µ)2(cid:96)−k−j .
(cid:1) Π (cid:0)J k
= E Π (cid:0)I k
T
T

(cid:1) E Π (cid:0)J k
T

The ﬁrst term contains three types of products:

E p2 = sT = σ2
E (1 − p)2 = sF = σ2
E p (1 − p) = sT F = µp (1 − µp) − σ2
p.

p + µ2
p,
p + (1 − µp)2 ,

(54)

(55)

(56)

(57)

Proof. We show only the ﬁrst statement; the rest of
the lemma is well known. If λ ∈ (0, 1) is distributed
as Beta (αp, βp), then 1 − λ is distributed as beta
(βp, αp). The expected value of time is calculated in
two steps; ﬁrst, we take the conditional expectation,
namely,

Eτ = EE ( τ | λ) =

1
(cid:90)

∞
(cid:90)

tλ exp (−λt) dtb (λ; βp, αp) dλ

The pool constitutes independent variables; there-
fore,

Var (q(cid:96)) =

(cid:96)
(cid:88)

(cid:88)

(cid:0)σ2

p + µ2
p

(cid:1)nk.j (cid:16)

p + (1 − µp)2(cid:17)mk.j
σ2

0

0
Γ (βp − 1)
Γ (αp + βp − 1)

=

Γ (αp + βp)
Γ (βp)

= 1 +

,

αp
βp − 1
(63)

k,j=k(cid:96)
× (cid:0)µp (1 − µp) − σ2

I k,J j

p

(cid:1)(cid:96)−nk.j −mk.j − (Eq(cid:96))2 .

(58)

since the sum of the second term gives the (Eq(cid:96))2,
indeed

(cid:96)
(cid:88)

k,j=k(cid:96)

(cid:18)(cid:96)
k

(cid:19)(cid:18)(cid:96)
j

(cid:19)

µk+j
p

(1 − µp)2(cid:96)−k−j



=



(cid:96)
(cid:88)

k=k(cid:96)

(cid:18)(cid:96)
k

(cid:19)

p (1 − µp)(cid:96)−k
µk



2



= (Eq(cid:96))2 .

(59)

where we assumed that 1 < βp < αp. Suppose 2 <
βp < αp to calculate the variance in a similar manner

V ar (τ ) = EE

=

(cid:90) 1

0

(cid:16)

(cid:17)

(τ − E ( τ | λ))2(cid:12)
(cid:12)
(cid:12) λ
1
λ2 b (λ; βp, αp) dλ = 1 +

(64)

αp
βp − 2

.

