UNSUPERVISED CLUSTERING OF SERIES USING DYNAMIC
PROGRAMMING

1
2
0
2

n
a
J

3
2

]

G
L
.
s
c
[

1
v
2
1
5
9
0
.
1
0
1
2
:
v
i
X
r
a

Karthigan Sinnathamby∗
Visium
Lausanne, Switzerland
karthigan.sinnathamby@visium.ch

Chang-Yu Hou
Schlumberger-Doll Research
Cambridge, MA
CHou2@slb.com

Lalitha Venkataramanan
Schlumberger-Doll Research
Cambridge, MA
LVenkataramanan@slb.com

Vasileios-Marios Gkortsas
Schlumberger-Doll Research
Cambridge, MA
VGkortsas@slb.com

François Fleuret
University of Geneva
Geneva, Switzerland
francois.fleuret@unige.ch

January 2021

ABSTRACT

We are interested in clustering parts of a given single multi-variate series in an unsupervised manner.
We would like to segment and cluster the series such that the resulting blocks present in each cluster
are coherent with respect to a known model (e.g. physics model). Data points are said to be coherent
if they can be described using this model with the same parameters. We have designed an algorithm
based on dynamic programming with constraints on the number of clusters, the number of transitions
as well as the minimal size of a block such that the clusters are coherent with this process. We present
an use-case: clustering of petrophysical series using the Waxman-Smits equation.

Keywords Unsupervised Clustering · Series · Dynamic Programming

1

Introduction

Unsupervised clustering is a branch of machine learning that aims to categorize the data based on the self-similarity. In
other word, data-points in the same group (called a cluster) are more similar to each other than to those in other groups.
This task can be achieved by various algorithms (the well-known K-means or spectral clustering but also hierarchical
clustering [1] or density-based clustering [2]) that differ signiﬁcantly in their understanding of what constitutes a cluster
and how to efﬁciently ﬁnd them.

In many cases, there exist models/functions, governed by a ﬁnite set of parameters, providing either physics or
phenomenology correlations between input data. The presence of these models can in principle be used to characterize
clusters (cluster characterization) because one can deﬁne a loss function to measure how well a point belongs to this
cluster (cluster afﬁliation). Hence, given a multi-variate series, one can cluster it such that the resulting blocks present
in each cluster are coherent with respect to prescribed physics correlations. In other words, data points are clustered
together if they can be described using the assumed model with the same modeling parameters.

Furthermore, we are interested in clustering multi-variate series which have temporal/spatial correlation such that
the sequential data points have higher likelihood to belong to the same cluster. Namely, we expect that the cluster
assignment should present a sparse clustering and consequently well deﬁned blocks of points. However, the generic
unsupervised clustering scheme often does not take this temporal/spatial correlation into account, which can overﬁt
the data and lead to the too fragmented clustering. Therefore, it is of interests to establish a clustering algorithm that
respect to the sequential correlation of the data.

∗Work done while in Master Data Science at EPFL (Lausanne, Switzerland)

 
 
 
 
 
 
In this work, we propose an unsupervised clustering algorithm that suits the following objective: (1) The clustering
criterion is based on the physics coherence. (2) The sequential correlation of data series is properly respected. The
general algorithm is inspired from the well-know K-means algorithm, which is a special case of the proposed algorithm
where the cluster characterization is merely the mean point of the cluster and the cluster afﬁliation the L2 loss with
this mean point. In order to properly taking into account the temporal/spatial correlation, the proposed algorithm will
constraint the number of cluster, the number of transition (number of times we change to a different cluster), and the
minimal size (number of data points) of any block in the ﬁnal assignment. To have an efﬁcient clustering algorithm,
we will derive a recurrence relation automatically satisfying all the required constraints, and make use of dynamic
programming to correctly ﬁnd the optimal assignment.

2 Problem Deﬁnition

We consider a multi-variate series X and we would like to cluster this series among C clusters. We suppose that each
point of the series is of dimension d, in other words X ∈ RT ×d where T is the total number of points. We note xt ∈ Rd
the vector data at step t and yt ∈ {1, ...C} the cluster assignment for the point xt. Moreover, we denote Y the vector
[y1, ...yT ].

We consider a modelisation function Mchar(X, Y, C) shortly noted Mchar and a cost function of the form f (x, w) :
Rd → R+. Given a clustering assignment Y , Mchar computes for each cluster c (among the C clusters) some weights
wc ∈ Rdw where dw is the dimension of the weights, obtained through a training procedure over the points in X
assigned to the cluster c (cluster characterization). We note W ∈ RC×dw the matrix where the rows consist of each
vector wc. f (x, wc) is the cost evaluating how bad the single point x is ﬁt to the cluster c (cluster afﬁliation) given its
weight wc.

The goal is to ﬁnd the best assignment Y that minimizes the following objective:

Lf (X, Y, W ) =

C−1
(cid:88)

(cid:88)

c=0

0≤t≤T −1
yt=c

f (xt, wc)

We detail what is Mchar(X, Y, C) and f (x, wc) in the case of K-means as an example in algorithm 1. The objective
function in the K-means is the pairwise L2 distance between the points.

Algorithm 1: Cluster characterization and afﬁliation for kmeans

1 f (x, w) = ||x − w||2
2 Function Mkmeans,char(X,Y,C)
3

W = empty array of size C

4

5

6

7

8

9

for c ← 1 to C do

tmp = {X[t] | Y [t] = c}
if len(tmp) ≥ 1 then

W[c] = Mean(tmp)

else

Remove cluster c

10

return W

3 Algorithm Deﬁnition

Given any Mchar(X, Y, C) and f (x, wc), we propose the clustering algorithm in algorithm 2 to minimize the given
objective. The parameter (cid:15) deﬁnes the maximum difference between two cost values to consider that the cost has
converged (to be chosen depending on the cost scale and the desired precision).

The idea is a generic version of the classical K-means algorithm. We start from a random assignment, we compute the
weights of the clusters (cluster characterization) using the Mchar. Then, for each point we assign the cluster that gives
the lowest value in cluster afﬁliation for this point using f . With the new assignment, we recompute the weights of the
clusters and we do the same thing until the main objective Lf (X, Y, W ) has converged.

2

The only supplement part is the function DynamicProgrammingPathFinder that comes handy in the study of series.
The goal of this function is to ﬁnd the clustering assignment optimal with respect to the objective function such that it
puts constraints on the number of cluster, the number of transitions and the minimal size of any block being M inφ
considered as an input. The function DynamicProgrammingPathFinder will constrain the assignment and return the
optimal one using Dynamic Programming by solving a recurrence relation that we detail now.

We deﬁne ωt(n, c): the optimal total cost (cumulatively summed over the steps {0, ..., t}) of assigning the point t to the
cluster c and having used n transitions over the steps {0, ..., t} among all the possible assignments of the steps {0, ..., t}
that use n transitions with blocks bigger than M inφ.

The following recurrence relation must be satisﬁed by ωt(n, c) (proof given in Appendix A):

∀t < M inφ, ∀n, ∀c,

ωt(n, c) =

(cid:26)(cid:80)t

k=0 f (xk, wc)

+∞

if n = 0 & t = M inφ − 1
else

∀t ≥ M inφ, ∀c,

ωt(0, c) = f (xt, wc) + ωt−1(0, c)

∀t ≥ M inφ, ∀n (cid:54)= 0, ∀c,

ωt(n, c) = min






f (xt, wc) + ωt−1(n, c)

t
(cid:80)
k=t−M inφ+1

f (xk, wc) + min
c(cid:48) (cid:54)=c

ωt−M inφ(n − 1, c

(cid:48)

)

(1)






The optimal cost we are interested in is min
∀c,∀n

ωT −1(n, c). When we want a transition, we look back at M inφ points in

the past and we check whether a transition M inφ points before would have been suitable. In the opposite case, we rely
only on ωt−1(n, c). Solving this recurrence relation yields the minimal cost. This is done using a classical bottom-up
approach. As a matter of fact, the optimal assignment is given in a linear time once this recurrence relation is solved.

The ﬁnal prediction has at most N transitions and at most C different clusters.

Algorithm 2: Clustering Algorithm - Final
Input: X, Mchar, f, C, N , M inφ, (cid:15)
Output: Y
Initialization:

1 oldLoss = ∞
2 currentLoss = 0
3 Y = randomAssignmentTransition(C,len(X),N,M inφ)
4 T = len(X)

5 while abs(currentLoss-oldLoss) >= (cid:15) do

/* Until convergence
{wc}1≤c≤C = W ← Mchar(X,Y,C,M inφ)
Y = DynamicProgrammingPathFinder(W,X,C,N,f)

oldLoss = currentLoss
currentLoss = Lf (X,Y,W)

6

7

8

9

4 Practical Considerations

4.1 Adjusted Rand Index

// Compute the weights

*/

// New assignment with recurrence relation 1

The algorithm begins with a random initialization. In order to have a stable answer, we will perform the previous
algorithm Ninit times. We thus have Ninit different assignments. Finally, to have one ﬁnal assignment, we will

3

compute the pairwise adjusted rand index (ARI [3]) between those Ninit assignments and take the one that has the
highest mean ARI. We brieﬂy review below the deﬁnition of ARI.

a+b
nsamples
C
2

Let us start by considering the unadjusted Rand Index (RI). Given two assignments A and B, the unadjusted RI is given
, where a is the number of pairs of elements that are in the same set in A and in the same set in B, b the
by
number of pairs of elements that are in different sets in A and in different sets in B, and C nsamples
possible pairs in the dataset (without ordering).

the total number of

2

The ARI adjusts RI by removing the expected RI of random labelings. We give the exact formulation below. We note Ai
the set of index of points that have been labeled i and in the same manner Bi. We note nij = |Ai ∩ Bj|, ai = (cid:80)
j ni,j
and bj = (cid:80)

i ni,j. The ARI between the assignments A and B is deﬁned as:

ARI(A, B) =

(cid:80)

ij

(cid:0)nij
2
(cid:1) + (cid:80)

(cid:1) − [(cid:80)
(cid:0)bj
2

(cid:0)ai
2
(cid:1)] − [(cid:80)

j

i

i

(cid:1) (cid:80)

(cid:0)bj
2
(cid:1) (cid:80)
j

(cid:1)]/(cid:0)n
2
(cid:0)bj
2

(cid:1)
(cid:1)]/(cid:0)n

2

j
(cid:0)ai
2

(cid:1)

1

2 [(cid:80)

i

(cid:0)ai
2

As a result, the ARI is a score between -1.0 and 1.0 where 1.0 is the perfect match score. Random uniform label
assignment have an ARI close to 0.0.

4.2 Grid-Search

Although for a given set of N and C, the proposed algorithm 2 together with sufﬁcient number of the random initializa-
tion provides a scheme to effectively cluster the multi-variate series, one common piece of puzzle for unsupervised
clustering is often the unknown values of the “correct” N and C in practical applications. Typically, when higher
values of C and N are used, lower total costs of the ﬁnal clustering assignment is expected. The unbounded values of
N and C could easily result in a structure more fragmented than the one that is desired. Hence, we need to perform a
grid-search over N and C and yield a ﬁnal answer by applying an additional criterion.

From a general perspective, the goal is to have a mean to determine a clustering pattern given by a set of N and C
that neither leads to over fragmented clusters nor has an unacceptably high total cost. Empirically, we have taken the
following procedure to give the ﬁnal assignment. We will take the most common assignment, using ARI as precised in
Sec. 4.1, among the plausible cluster patterns falling within a range of grid points satisfying the following criterion: A
region consists of high diversity of grid points parameterized in N and C with convergent total cost.

The selecting criterion for the grid points giving plausible cluster patterns can be clearly depicted by plots shown in
ﬁgure 1, where the red ticks along the y-axis indicate the region of the convergent total cost. In ﬁgure 1, Ngrid and Cgrid
represents the (N ,C) input to the algorithm (and not the number of transition/cluster of the associated assignment). On
the left of the ﬁgure, we have one blue tick on the cost axis per point (rug plot) - it enables us to see the distribution of
total cost from grid points. Using plots similar to ﬁgure 1, we can recast our selecting criterion as follows: the most
dense region of the distribution of points on the cost axis that contains points of high diversity in N and C.

The idea behind the criterion is two-fold:

• the most dense region of the distribution of points on the cost axis: we want a stable cost dense region. Indeed,
when doing the grid-search many (Ngrid,Cgrid) pairs comes back to the same assignment (or approximately
the same) yielding more or less the same cost. This is due to the fact that when starting with (Ngrid,Cgrid), we
might have an answer with N (cid:48) ≤ Ngrid and C (cid:48) ≤ Cgrid.

• that contains points of high diversity in Ngrid and Cgrid: we want a region not biased toward one C or one
N . In the example (ﬁgure 1), for C = 2 the cost is stable but it is mainly due that we need more clusters to
observe a change in the cost.

This criterion might be subjective when deciding for "the most dense" region but we did not ﬁnd another criterion
that gives better results for the studied datasets. Moreover the criterion depends on the size of the grid. Indeed, when
the grid gets bigger, that is going for higher N and C, the total cost can get lower with the issue of too fragmented
clustering. In such scenario, the criterion described above becomes unreliable as the selected cost range might lead to
more fragmented assignments. For many applications, this is not a problem as the N and C can be constrained within
a range based on prior information. So the algorithm will be computed on a realistic grid with respect to the studied
dataset. On the other hand, it is also possible that one cannot ﬁnd and deﬁne a convergent region for the total cost. In
such case, one can only take the lowest cost answer as the ﬁnal assignment.

4

Figure 1: Grid-search example: The y-axis is the MSE cost, the x-axis is the number of transitions and the color is the
number of clusters. On the left, we have the rug plot described in 4.2. The red ticks correspond to the most common
region found when applied the grid-search criterion described in 4.2

5 Application to petrophysics

We give here an application of the algorithm for clustering petrophysical well log data. The experiments and results
given in this paper are for the speciﬁc application that we detail in this section.

In the context of sub-surface characterization, petrophysicists have recorded many measurements to infer the formation
properties such as the porosity, the water saturation, the conductivity of the water-ﬁlled rock, grain density, gamma ray,...
We name these properties logs for the rest of the work. Moreover, petrophysicists have studied parametric equations
combining these different logs in order to characterize data-points belonging to the same formation type. In other words,
data-points belonging to the same formation type should follow these physics equations with the same parameters.

In the case of clustering petrophysical well log data, we better understand the usefulness of the constraints considered
in the algorithm:

• Unsupervised: there is not known label in any petrophysical dataset

• Number of cluster: there is a small ﬁnite number of different mineralogy formations that makes sense for the

exploitation

• Number of transition: the geological structure cannot be too fragmented

• Minimal size of a block: each block represent a formation layer that must be bigger than a prior minimal size.

One of the key physics relation that could help describing a cluster is the conductivity response of the water-ﬁlled rock
(σo) constrained by the water conductivity (σw), the porosity (φ), the volume fraction of clay (fclay) and the water
saturation fraction (Sw). In the literature, many equations try to study this relation. Among those, the Waxman-Smits
(WS) equation ([4]) cover a wide variety of the formation types and is used in many cases in the petrophysics domain.
As such, in this study, we will focus on this equation. A primary input to the Waxman-Smits (WS) equation is the
volume concentration of clay exchange cations (Qv) which is computed from the cation exchange capacity (CEC):

The WS equation is:

Qv =

CEC × fclay × (1 − φ)
φ

σo = φm × Sn

w ×

(cid:18)

σw +

(cid:19)

B × Qv
Sw

5

(2)

(3)
WS equation

where m and n are some parameters and B is a function of the temperature T which is considered constant throughout
the studied logs.

(cid:16)

B =

1 − 0.83 × e−σw×(−2.47+0.229×ln(T )2+ 1311

T 2 )−1 (cid:17)

(cid:16)

×

−9.2431 + 2.6146 ×

√

(cid:17)

T

(4)

We will leverage the WS equation (3) as a mean to characterize a cluster. Among all the variables involved in the
precedent equations, the logs of φ, Sw, σo, fclay are our input. In order to characterize a cluster, we perform a regression
), and CEC by minimizing the L2 loss function between the experimental (given well log) ρo (= 1
of m, n, ρw (= 1
)
σo
σw
and the predicted one computed with the WS equation (3) on the points of that cluster.

We describe here the exact values of the parameters for the algorithm 2 in the case of the WS regression.

• X are the 4 data logs X = [fclay, φ, Sw, ρo] (d = 4). Please keep in mind this speciﬁc order of the columns of

X.

• T is the number of data-points of the logs, T = len(X).

• C is the number of different types of formation layer.

• N is still the maximum number of transitions between formation layers.

• M inφ is the minimum size of a formation layer and is also the number of points that we judge enough for the

WS regression (for the computation of the weights wc to be meaningful.

• Ninit: experimentally set to 50.
• Mchar and f : Mchar(X, Y, C, M inφ) for the cluster characterization and the function f for the cluster
afﬁliation are detailed in algorithm 3. The W Sregression function is a regression using the L2-Loss over the
WS equation 3, regressing w = [m, n, ρw, CEC]. Here, we use the same L2-Loss for the regression and
for the function f . The regression is performed using Scipy function optimize.fmin (that implements the
downhill simplex algorithm). Finally, W Sf orward(x, w) is just a function applying the WS equation 3 with
the corresponding m, n, σw, CEC to get the predicted ρo.

Algorithm 3: Cluster characterization and afﬁliation for WS

// L2 Loss on ρo

1 f (x, w) = ||x[0] − W Sf orward(x, w)||2
2 Function Mws,char(X,Y,C,M inφ)
W = empty array of size C
3

4

5

6

7

8

9

for c ← 1 to C do

tmp = {X[t] | Y [t] = c}
if len(tmp) ≥ M inφ then

W[c] = W Sregression(tmp)

else

Remove cluster c

10

return W

6 Experiments

As mentioned previously, there is no dataset with groung-truth label in the adressed problem. As such, we have
simulated our own well logs with a noise level close to the one present in the ﬁeld well data.

In order to simulate data logs, we have ﬁrst deﬁned a way to simulate logs for a speciﬁc formation layer. For this
purpose, we have ﬁxed the expected value for m, n, CEC, σw and the interval of φ, Sw, fclay for each formation
type / cluster. With these inputs, we can directly generate logs for φ, Sw, and fclay but they will not show a realistic
"time-series" correlation. Therefore, we made an adjustment to the φ and Sw logs: we do not want points to jump
between largely different values at each depth, i.e. we would like to have a smooth behavior. Moreover for Sw the
adjustment is made such as to have the water at the bottom of the formation as it is usually the case in the nature: for

6

label

m

0
1
2
3
4
5
6
7

1.85
2.0
2.05
2.3
2.5
2.0
2.0
2.1

n

1.7
2.0
2.0
2.1
2.2
2.5
1.9
2.1

ρw
0.03
0.03
0.029
0.031
0.049
0.05
0.05
0.051

CEC

0
0
30
0
80
0
0
45

Table 1: Parameters for WS-0

the same block, Sw. The fraction of clay might not need such an adjustment. As such, we have a reference signal for φ,
Sw and fclay.

Then, we added noise of different level to these logs. Indeed in the real cases, we expect the ﬁeld measurements to have
inherent uncertainty for φ, Sw, and fclay. Furthermore, we added noise to m, n, CEC as there is a natural variation of
these value even within the same formation type. More precisely, we added approximately between 5% to 10% error to
φ, Sw, fclay and CEC, a Gaussian noise with a standard deviation 0.05 to m and 0.03 to n.

Finally, we have generated σo log using the WS (3) equation with these noise-added signals as input. This gives us a
way to generate realistic data logs given parameters describing a speciﬁc formation type or cluster. As such, we have
generated multiple clusters with several blocks per cluster. To have a full well log, we put together the blocks either in a
sharp manner (simple concatenation) or in a smooth manner (by interpolating between the blocks). The datasets named
with a sufﬁx "-smooth" present smooth transitions while the others present sharp transitions.

The grid of the grid-search (as mentioned in 4.2) is deﬁned as 1 ≤ C ≤ k × Ctrue and 0 ≤ N ≤ k × Ntrue where
Ctrue and Ntrue are the true values of respectively C and N , and k is a constant equal to approximately 1.5. The idea
behind this constant k is that geological priors refrain the prediction to have high N and C with respect to the real
datasets: the grid must be "realistic".

7 Results

We have tested our algorithm on seven simulated datasets. We present thoroughly here the results on two of them
(WS-3 and WS-3-smooth) and leave the others in the Appendix. In table 2, we give numeric results over all the datasets,
showing that the method present very reliable predictions looking at the ARI.

For each dataset, we ﬁrst give the table of parameters that were used to generate the data along with the cluster label.

Then, we give a scatter plot of the cost against N for each cluster to see where we apply the grid-search criterion
mentioned in section 4.2. The blue ticks on the scatter plot represent the rug plot. The red ticks designate the region of
the cost taken to compute the ﬁnal assignment by taking the most common pattern among the points of this region: this
is the region described in section 4.2. Ngrid and Cgrid represents the (N ,C) input to the algorithm.

Finally, we give the plots of the predictions: on each plot, the prediction on the left is either the most common answer
or the lowest cost one and the prediction on the right is the ground truth. Each color represents a different cluster within
the prediction (cluster 0 of the prediction and of the ground truth might have nothing in common). The y axis represents
the depth and the x axis is simply for illustration purpose. We also give their confusion matrix with the ground truth.

To better appreciate the results, one must look at the prediction plots for the transitions points and at the confusion
matrix for the cluster label attribution.

For the two datasets studied in this section, the parameters used to create the clusters are the same. The difference is
that in WS-3-smooth the transition are smooth as described in section 6.

• WS-3: in the most common answer, the cluster 1 and 2 are put together as well as 6 and 7. However, when we
look at the parameters of these clusters, there are really close. Therefore, the most common prediction is still
very good with respect to the high noise present in the simulation. The lowest cost answer is too fragmented as
expected.

7

Datasets

Prediction

WS-1

WS-2

Most common
Lowest cost
Most common
Lowest cost

WS-2-smooth Most common

WS-3

Lowest cost
Most common
Lowest cost

WS-3-smooth Most common
Lowest cost

costpred
0.170
0.169
0.161
0.159
0.164
0.161
0.170
0.166
0.173
0.167

costtrue ARI
0.76
0.72
0.91
0.79
0.82
0.68
0.81
0.73
0.71
0.84

0.170
0.170
0.165
0.165
0.163
0.163
0.166
0.166
0.164
0.164

Table 2: Results for simulated data. For each dataset, we have a line for the most common answer as deﬁned in section
4.2 and a line for the lowest cost answer. costpred is the cost of the prediction, costtrue is the cost of the ground truth
answer, ARI is the Adjusted Rand Index between the prediction and the ground truth as deﬁned in section 4.1. In bold,
we have the answer that have the highest ARI.

• WS-3-smooth: the only difference with WS-3 is that the cluster 3 is mixed with the cluster 5. Even though it
can be explained with the difference in ρw of these two clusters, we here have a case where the lowest cost
answer gives a (much) better prediction than the most common one.

8 Conclusions

Given a way to characterize a cluster and to quantify the mismatch of a point to a cluster, we have designed an algorithm
that enables the clustering, in an unsupervised manner, of any multi-variate series. This algorithm, based on dynamic
programming, allows the user to incorporate constraints on the number of clusters, the number of transitions as well as
the minimal size of a block in order to have an optimal assignment that minimizes the given objective while controlling
the sparsity of the answer.

References

[1] S. Patel, S. Sihmar, and A. Jatain. A study of hierarchical clustering algorithms. In 2015 2nd International

Conference on Computing for Sustainable Global Development (INDIACom), pages 537–541, 2015.

[2] Claudia Malzer and Marcus Baum. A hybrid approach to hierarchical density-based cluster selection, 2019.
[3] Jorge M. Santos and Mark Embrechts. On the use of the adjusted rand index as a metric for evaluating supervised
classiﬁcation. In Cesare Alippi, Marios Polycarpou, Christos Panayiotou, and Georgios Ellinas, editors, Artiﬁcial
Neural Networks – ICANN 2009, pages 175–184, Berlin, Heidelberg, 2009. Springer Berlin Heidelberg.

[4] M. H. Waxman and L. J. M. Smits. Electrical conductivities in oil-bearing shaly sands. Society of Petroleum

Engineers Journal, 8(02):107–122, 1968.

8

Figure 2: Grid-search criterion for WS-3
Most common: Ngrid = 8, Cgrid = 6

(a) Most common

(b) Lowest cost

Figure 3: Predictions for WS-3

(a) Most common

(b) Lowest cost

Figure 4: Confusion matrices for WS-3

9

(a) Grid-search criterion
Most common: Ngrid = 16, Cgrid = 7

Figure 5: Grid-search results for WS-3-smooth

(a) Most common

(b) Lowest cost

Figure 6: Predictions for WS-3-smooth

(a) Most common

(b) Lowest cost

Figure 7: Confusion matrices for WS-3-smooth

10

Appendix A Proof of the Recurrence Relation

We give here the proof that ωt(n, c) must satisfy 1.

We recall the deﬁnition of ωt(n, c): the optimal total cost (cumulatively summed over the timesteps {0, ..., t}) of
assigning the point t to the cluster c and having used n transitions over the timesteps {0, ..., t} among all the possible
assignments of the timesteps {0, ..., t} that use n transitions such that all blocks should be bigger than M inφ:

We name tk (1 ≤ k ≤ n) the n transitions points and t0 = 0 and tn+1 = t + 1. We name Yi the cluster label for
timestep i. Finally, we call ck (1 ≤ k ≤ c) the cluster label k.

ωt(n, c) =

min
0=t0<t1<...<tn<tn+1−M inφ+1<tn+1=t+1

Yt0 =Yt0+1=...(cid:54)=Yt1 =Yt1+1=...(cid:54)=Ytn−1 =...=c
∀k∈{0,..,n}, tk+1−tk≥M inφ

(cid:48)

(cid:54)=Ytn =....=Yt=c

n
(cid:88)

tk+1−1
(cid:88)

k=0

s=tk

f (xs, cs)

(5)

We omit "ωt(n, c) =" in the next equation lines for more readability.

If we take out the group for the group tn from the double sum, we have:

ωt(n, c) =

min
0=t0<t1<...<tn<tn+1−M inφ+1<tn+1=t+1

Yt0 =Yt0+1=...(cid:54)=Yt1 =Yt1+1=...(cid:54)=Ytn−1 =...=c
∀k∈{0,..,n}, tk+1−tk≥M inφ

(cid:48)

(cid:54)=Ytn =....=Yt=c

t
(cid:88)

s=tn

f (xs, cs) +

n−1
(cid:88)

tk+1−1
(cid:88)

k=0

s=tk

f (xs, cs),

or taking out the point at the step t from the ﬁrst sum:

min
0=t0<t1<...<tn<tn+1−M inφ+1<tn+1=t+1

(cid:48)

Yt0 =Yt0+1=...(cid:54)=Yt1 =Yt1+1=...(cid:54)=Ytn−1 =...=c

(cid:54)=Ytn =....=Yt−1=c

∀k∈{0,..,n}, tk+1−tk≥M inφ

= f (xt, c) +

• Case 1

t−1
(cid:88)

s=tn

f (xs, cs) +

n−1
(cid:88)

tk+1−1
(cid:88)

k=0

s=tk

f (xs, cs)

(6)

If the best nth transition is strictly before the last M inφ points, i.e., tn < t − M inφ + 1 and t − tn >= M inφ,
we can safely shift the tn+1 from t + 1 to t for all the summation terms in Eq. (6) without violating minimal
block size constraint. We would have in this case:

f (xt, c) +

min
0=t0<t1<...<tn<tn+1−M inφ+1<tn+1=t

(cid:48)

Yt0 =Yt0+1=...(cid:54)=Yt1 =Yt1+1=...(cid:54)=Ytn−1 =...=c

(cid:54)=Ytn =....=Yt−1=c

or in other words regrouping the sums:

∀k∈{0,..,n}, tk+1−tk≥M inφ

tn+1−1
(cid:88)

s=tn

f (xs, cs) +

n−1
(cid:88)

tk+1−1
(cid:88)

k=0

s=tk

f (xs, cs)

min
0=t0<t1<...<tn<tn+1−M inφ+1<tn+1=t

(cid:48)

Yt0 =Yt0+1=...(cid:54)=Yt1 =Yt1+1=...(cid:54)=Ytn−1 =...=c

(cid:54)=Ytn =....=Yt−1=c

∀k∈{0,..,n}, tk+1−tk≥M inφ

n
(cid:88)

tk+1−1
(cid:88)

k=0

s=tk

f (xs, cs)

f (xt, c) +

which is equal to:

• Case 2 If the best nth transition is exactly at the point tn = t − M inφ + 1, we can take out the ﬁrst sum. We

ωt(n, c) = f (xt, c) + ωt−1(n, c)

(7)

would have in this case:

ωt(n, c) =

t
(cid:88)

f (xs, cs) +

s=t−M inφ+1

min
0=t0<t1<...<tn−1<tn−M inφ+1<tn=t−M inφ+1
(cid:54)=c
Yt0 =Yt0+1=...(cid:54)=Yt1 =Yt1+1=...(cid:54)=Ytn−1 =...=c

(cid:48)

∀k∈{0,..,n−1}, tk+1−tk≥M inφ

n−1
(cid:88)

tk+1−1
(cid:88)

k=0

s=tk

f (xs, cs)

11

which is equal to (with c

(cid:48)

(cid:54)= c):

ωt(n, c) =

t
(cid:88)

s=t−M inφ+1

f (xs, cs) + ωt−M inφ(n − 1, c

(cid:48)

)

(8)

Finally, by realizing that the true ωt(n, c) can be obtained through the path either from Case 1 or from Case 2, we reach
the following equation as the selection criterion for the recurrence relation at the step t:

ωt(n, c) = min






f (xt, wc) + ωt−1(n, c)

t
(cid:80)
s=t−M inφ+1

f (xs, wc) + min
c(cid:48) (cid:54)=c

ωt−M inφ(n − 1, c

(cid:48)

)

which is exactly the last equation in 1.

Moreover, we need to initialize, for t < M inφ (∀n, ∀c) and n=0 (∀t, ∀c) in order to start this recurrence:

• t < M inφ the only case that can have a block bigger (in this case equal) to M inφ is for t = M inφ − 1 and

n=0,

• n=0, we can have only one cluster,
• in all other cases, the cost should be +∞

Appendix B Other Datasets

Here we present the clustering results for all the testing examples, beside WS-3 and WS-3-smooth, in the same fashion
indicated in themain text.

12

label

m

0
1
2
3
4
5

2.1
1.9
1.8
2.05
2.2
2.4

n

2.3
1.8
1.75
1.9
2.0
2.1

ρw
0.052
0.05
0.052
0.05
0.048
0.048

CEC

0
0
0
0
20
60

Table 3: Parameters for WS-1

Figure 8: Grid-search criterion for WS-1
Most common: Ngrid = 14, Cgrid = 10

(a) Most common

(b) Lowest cost

Figure 9: Predictions for WS-1

(a) Most common

(b) Lowest cost

Figure 10: Confusion matrices for WS-1

13

label

m

0
1
2
3
4
5

1.85
2.1
2.4
1.9
2.0
2.0

n

1.8
2.0
2.3
2.0
1.95
2.5

ρw
0.052
0.052
0.049
0.051
0.051
0.05

CEC

0
0
80
0
30
0

Table 4: Parameters for WS-2

Figure 11: Grid-search criterion for WS-2
Most common: Ngrid = 14, Cgrid = 8

(a) Most common

(b) Lowest cost

Figure 12: Predictions for WS-2

(a) Most common

(b) Lowest cost

Figure 13: Confusion matrices for WS-2

14

label

m

0
1
2
3
4
5

1.85
2.1
2.4
1.9
2.0
2.0

n

1.8
2.0
2.3
2.0
1.95
2.5

ρw
0.052
0.052
0.049
0.051
0.051
0.05

CEC

0
0
80
0
30
0

Table 5: Parameters for WS-2-smooth

Figure 14: Grid-search criterion for WS-2-smooth
Most common: Ngrid = 13, Cgrid = 6

(a) Most common

(b) Lowest cost

Figure 15: Predictions for WS-2-smooth

(a) Most common

(b) Lowest cost

Figure 16: Confusion matrices for WS-2-smooth

15

