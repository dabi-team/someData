Collaborative and Privacy-Preserving Machine
Teaching via Consensus Optimization

Yufei Han1, Yuzhe Ma2, Christopher Gates1, Kevin Roundy1, and Yun Shen1

1Symantec Research Labs
2University of Wisconsin-Madison

9
1
0
2

y
a
M
7

]

G
L
.
s
c
[

1
v
6
9
7
2
0
.
5
0
9
1
:
v
i
X
r
a

Abstract—In this work, we deﬁne a collaborative and privacy-
preserving machine teaching paradigm with multiple distributed
teachers. We focus on consensus super teaching. It aims at
organizing distributed teachers to jointly select a compact while
informative training subset from data hosted by the teachers
to make a learner learn better. The challenges arise from three
perspectives. First, the state-of-the-art pool-based super teaching
method applies mixed-integer non-linear programming (MINLP)
which does not scale well to very large data sets. Second, it is
desirable to restrict data access of the teachers to only their own
data during the collaboration stage to mitigate privacy leaks.
Finally, the teaching collaboration should be communication-
efﬁcient since large communication overheads can cause syn-
chronization delays between teachers.

To address these challenges, we formulate the collaborative
teaching as a consensus and privacy-preserving optimization
process to minimize teaching risk. We theoretically demonstrate
the necessity of collaboration between teachers for improving
the learner’s learning. Furthermore, we show that the proposed
method enjoys a similar property as the Oracle property of
adaptive Lasso. Empirical study illustrates that our teaching
method can deliver signiﬁcantly more accurate teaching results
with high speed, while the non-collaborative MINLP-based super
teaching becomes prohibitively expensive to compute.

I. INTRODUCTION

Machine teaching [1]–[5] studies the inverse problem of
machine learning, where a teacher already has a speciﬁc target
model (θ∗) it wants to teach some other student (learner),
and the teacher designs the optimal training set such that the
student can efﬁciently learn the target model. The constructed
training set does not need to be independent and identically
distributed. The teacher is allowed to design any instance in
the input space, which enables ﬂexibility when generating an
efﬁcient training set. The efﬁcacy of teaching is measured
by computational cost of the model training, accuracy of the
derived learner, robustness of the training process and so on.
In general, machine teaching places a strong emphasis on
the teacher and its power to control data. Machine teaching
is connected to machine learning fundamentals as it deﬁnes
abstractions and interfaces between the learning algorithm
and the teaching paradigm. Research on machine teaching
has not only great theoretical value, but also applications in
personalized education and human-in-the-loop learning.

Super teaching is an interesting phenomenon unveiled re-
cently in machine teaching. As stated in [6], a learner is super-
teachable if a teacher can trim down an i.i.d. training set while
enhancing the learning performance. Distinct from training set

reduction where the target model is hidden from the learner,
super teaching assumes the teacher knows the target model
and rely on such knowledge to select a training subset so that
a student learner can learn better on that subset.

Prior work in super teaching assumes that only one central
teacher is present and it has full knowledge of all data
instances used for teaching. As the privacy concern of data
security becomes increasingly important, research in super
teaching faces the following important challenges. First, in-
stead of transferring all data used to teach to the central
teacher where teaching is conducted, it is preferable to keep
data on the local devices and conduct teaching with multiple
distributed teachers. Each teacher only accesses the data
samples hosted by itself. It avoids heavy overheads of the
data transferring and prevents leaks of private information
contained in the local data set. However, it is not clear whether
organizing a consensus collaboration between teachers can
provide merits of teaching compared to independently con-
ducting teaching by each teacher in a stand-alone mode.
Little efforts have been devoted to discuss how to organize
an efﬁcient collaborative super teaching paradigm to achieve
good teaching performances and a privacy-preserving process
of teaching at the same time. Furthermore, the state-of-the-
art super teaching method proposed in [6] is formulated as
a mixed-integer non-linear programming (MINLP) problem.
The general computational complexity of MINLP problem
is undecidable in theory [7]. In the worst case, the popular
heuristic solver, such as Branch-and-Bound (BnB) method,
has an exponential time complexity thus becomes prohibitively
expensive given large-scale training data. Solving a MINLP
problem with distributed players is even more difﬁcult, as
it usually needs a central processor to allocate the resource
across multiple players to solve the overall problem [8]. There-
fore, the central processor can access the local private data,
which potentially violates the privacy regulation. Besides, fre-
quent synchronization between the central process and the end-
devices can cause severe latency given a low-communication
environment. Finally, limited computing capability of end-
devices in a distributed network can not afford to the intensive
computation of solving the MINLP problem.

We propose a novel computationally efﬁcient distributed
super teaching paradigm, which coherently facilitates collabo-
ration between multiple teachers in a privacy-preserving way.
Our study conﬁrms a well-known intuition: A carefully or-

 
 
 
 
 
 
ganized consensus collaboration between different teachers
can enhance the teaching performances. We also show that
independently conducting teaching in a non-colluded way can
even make the teaching performance deteriorate. Furthermore,
the privacy-preserving design of the proposed collaborative
teaching paradigm encourages information sharing between
teachers in the collaboration stage.

II. RELATED WORK

A. Machine Teaching

Machine teaching was originally proposed in [1], [2]. It has
attracted plenty of research interest, most of which focus on
studying a quantity called the teaching dimension, i.e., the size
of the minimal training set that is guaranteed to teach a target
model to the student. For example, [1] provides a discussion on
the teaching dimension of version space learners, [9] analyzes
the teaching dimension of linear classiﬁers, and [4] studies the
optimal teaching problem of Bayesian learners. In standard
machine teaching, the student is assumed to passively receive
the optimal training set from teacher. Later works consider
other variants of teaching setting, e.g., in [3], [10], the student
and the teacher are allowed to cooperate in order to achieve
better teaching performance. More studies of machine teaching
can be found in [5], [11]–[14].

Machine teaching as a theoretical regime also has many
applications in cognitive science and computer security. One
application is the personalized education, where a clairvoyant
teacher can help design minimal teaching curriculum for a
human student such that an educational goal is achieved [15].
As another popular application, machine teaching can be
used to perform data poisoning attacks of real-world machine
the teacher is
learning systems [16]–[18]. In such cases,
viewed as a nefarious attacker who has a speciﬁc attack goal
in his mind, while the student is some machine learner, then
the teaching procedure corresponds to minimally tampering
the clean dataset such that the machine learner learns some
sub-optimal target model on the corrupted training set.

Instead of artiﬁcially designing the training set, super teach-
ing [6] selects a subset from an i.i.d. training set to conduct
teaching. Mathematically, super teaching is deﬁned as below.

Deﬁnition 1 (Super Teaching). Let S be an n-item iid training
set, and T be a teacher who selects a subset T (S) ⊂ S as
the training subset for learner A. Let ˆθS and ˆθT (S) be the
model learned from S and T (S) respectively. Then T is a
super teacher for learner A if ∀δ > 0, ∃N such that ∀n ≥ N

(cid:104)

PS

R(ˆθT (S)) ≤ cnR(ˆθS)

(cid:105)

> 1 − δ,

(1)

where R is some teaching risk function, the probability is with
respect to the randomness of S, and cn ≤ 1 is a sequence
called the super teaching ratio.

The idea of selecting an informative training subset is also
explored in [19]. In the proposed learning-to-teach framework,
the teacher conducting subset selection is modeled with Deep
Neural Nets (DNN). The goal of teaching is to select training

samples to make faster convergence of the DNN based learner.
The teacher network is tuned via reinforcement learning with
reward signals encouraging fast descent of the classiﬁcation
loss of the learner. In contrast
to learning-to-teach, super
teaching in [6] focuses on a more general teaching goal, which
drives the student to learn the expected model. Although only
simple learners such as Logistic Regression are considered
in [6], the theoretical study over the teachability of super
teaching can be further applied to many advanced learners.

Inspired by the teachability theory proposed in [6], our work
extends the horizon of super teaching by studying applicability
of a collaborative and privacy-preserving teaching scenario.
Different from learning-to-teach, multiple teachers are present
as collaborative players in the teaching activity. Furthermore,
training data hosted by any one teacher can not be accessed
by the others in our problem setting, whereas learning-to-teach
assumes the teacher network can access all the training data.

B. Federated Learning

Another relevant branch of research is federated learning
[20]. Federated learning is a communication-efﬁcient and
privacy-preserving distributed model training method over dis-
tributed agents. Each agent hosts their own data instances and
is capable of computing local model update. In each round of
model training, the training process is ﬁrst conducted on each
node in parallel without inter-node communication. Only the
local model updates are aggregated on a centralized parameter
server to derive the global model update. The aggregation is
agnostic to data distribution of different agents. Neither the
centralized server, nor the local agents have visibility of the
data owned by any speciﬁc agent. In [21], a communication-
efﬁcient distributed optimization method named CoCoA is
proposed for training models in a privacy-preserving way.
CoCoA applies block-coordinate descent over the dual form of
the joint convex learning objective and guarantees sub-linear
convergence of the federated optimization. Furthermore, the
optimization process does not require to access data instances
hosted by each node. Only local dual variable updates need to
transfer from local nodes to the central server. This property
makes CoCoA inherently appropriate for federated training.

A federated data poisoning attack is recently proposed
in [22]. This work assumes that only one malicious agent
conducts non-colluding adversarial data poisoning over the
local data instances that it hosts. Our method is distinct from
this work since we study consensus collaboration of multiple
teachers. In addition, we investigate a more generous goal
of teaching than data a pre-speciﬁed target model with that
training set., which guides the learner to learn a pre-speciﬁed
yet potentially malicious target model.

III. COLLABORATIVE SUPER TEACHING

We assume K teachers and one central parameter server
as the learner. Each teacher hosts a local private dataset Di
(i ∈ [K]) of size Ni. As the output of super teaching, each
teacher selects a subset Si ⊂ Di. The learner runs the learning
algorithm L on the aggregated subsets S = ∪i∈[K]Si
to

obtain the model. Each teacher only accesses its own data
Di during the teaching process due to the privacy-preserving
regulations. Once the teaching stage terminates, the learner can
further conduct federated model training to keep the local data
subsets {Si} on the local machines, which protects teachers’
data privacy after teaching. Discussing how to conduct model
training is beyond our scope. Without loss of generality, we
assume that the learner choses federated training.

We set the teaching goal as the value of the model parameter
θ∗ that the teachers expect the learner to obtain, as the setting
in [6]. In the collaborative environment, the union of the
selected subsets {Si} should be jointly helpful in inducing
θ∗. Therefore we propose to deﬁne the collaborative super
teaching as in (2)

R∗(ˆθS)

ˆθS, bi : i ∈ [K] = arg min
ˆθS ,bi:i∈[K]
Ni(cid:88)

K
(cid:88)

s.t. ˆθS = arg min

θ

i=1

j=1

j(cid:96)(θ, xi
bi

j, yi

j) +

λ
2

Ω(θ),

(2)

bi ∈ {0, 1}Ni, ∀i ∈ [K],

j, yi

j, yi

where R∗(ˆθS) = (cid:107)ˆθS − θ∗(cid:107) measures the teaching risk as
Euclidean distance between ˆθS and θ∗, (xi
j) is the jth
data instances of Di hosted by the teacher i, and bi is an
Ni-dimensional binary-valued vector with bi
j = 1 denoting
the instance (xi
j) is selected and bi
j = 0 otherwise. (cid:96) is
the learning loss function, Ω(θ) is the regularization over the
model complexity of the learner and λ is the regularization
weight. Intuitively, there is a primitive solution to the proposed
distributed teaching problem: oblivious teaching, where each
teacher independently selects its own teaching set without col-
laborating with the other teachers. The independently selected
subsets are aggregated to form the training set of the learner.
The questions of interest are thus i) whether the oblivious
teaching can reduce the teaching risk. and ii) whether it
is possible to improve teaching performance by organising
appropriate collaboration between the teachers, compared to
the oblivious teaching.

For simplicity of analysis, we assume that Ω(θ) is the l2-
norm penalty (cid:107)θ(cid:107)2 and the loss (cid:96) is a convex and τ -smooth
function of θ, which holds in many cases such as the logistic
loss or squared loss with bounded input space. Thus, the
learning algorithm L of the learner takes the form of a convex
optimization. Based on such assumption, we provide an initial
answer to the question i) in Theorem 1:

Theorem 1. Assume the model space Θ is bounded, i.e.,
∀θ ∈ Θ, (cid:107)θ(cid:107) ≤ B. Also assume the convex learning loss
(cid:96) is τ -smooth, i.e., (cid:107)∇(cid:96)(θ) − ∇(cid:96)(θ(cid:48))(cid:107) ≤ τ (cid:107)θ − θ(cid:48)(cid:107), ∀θ, θ(cid:48).
The teaching risk is deﬁned as R∗(ˆθ) = 1
2 (cid:107)ˆθ − θ∗(cid:107)2, where
θ∗ is the target model. Each teacher independently solves (2)
on its own dataset Di with regularization weight λ
K , and let
S = ∪i∈[K]Si be the aggregated dataset. Then

R∗(ˆθS) ≤ (

τ
λK

+

1
K 2 )

K
(cid:88)

i=1

R∗(ˆθSi).

(3)

j, ˆyi
Proof. Let the (ˆxi
j) be the jth point in the selected Si.
Deﬁne gi(θ) = (cid:80)|Si|
j, ˆyi
j=1 (cid:96)(θ, ˆxi
2K (cid:107)θ(cid:107)2, where |Si| is the
number of items in Si. Then ˆθSi = arg minθ∈Θ gi(θ). Deﬁne

j) + λ

g(θ) =

1
K

K
(cid:88)

i=1

gi(θ) =

1
K

K
(cid:88)
(

|Si|
(cid:88)

i=1

j=1

(cid:96)(θ, ˆxi

j, ˆyi

j)+

λ
2

(cid:107)θ(cid:107)2). (4)

Then ˆθS = arg minθ∈Θ g(θ). Since g(θ) is λ-strongly convex,

g(θ∗) − g(ˆθS) ≥

λ
2

(cid:107)θ∗ − ˆθS(cid:107)2 = λR∗(ˆθS).

(5)

Thus R∗(ˆθS) ≤ 1
g(θ∗) − g(ˆθS). Note that g(θ∗) − g(ˆθS) =

λ (g(θ∗) − g(ˆθS)). Next we upper bound

1
K

K
(cid:88)

i=1

(gi(θ∗) − gi(ˆθS)) ≤

1
K

K
(cid:88)

(gi(θ∗) − gi(ˆθSi)),

(6)

i=1

where the last inequality is due to ˆθSi = arg minθ∈Θ gi(θ).
Since the loss (cid:96) is τ -smooth, one can easily show that gi(θ)
is (τ + λ
K )-smooth, thus we have the following upper bound

gi(θ∗) − gi(ˆθSi ) ≤

1
2

(τ +

λ
K

)(cid:107)θ∗ − ˆθSi (cid:107)2 = (τ +

λ
K

)R∗(ˆθSi ).

(7)

By (6) and (7), we have

R∗(ˆθS) ≤

1
λ

(g(θ∗)−g(ˆθS)) ≤ (

τ
λK

+

1
K 2 )

K
(cid:88)

i=1

R∗(ˆθSi). (8)

Theorem 1 implies that if ∀i, R∗(ˆθSi) ≤ c, then R∗(ˆθS) ≤
λ + 1
( τ
K )c, which is a constant change. Thus by simply
aggregating the selected subsets, one can achieve comparable
teaching performance as each individual teacher does. How-
ever, the joint teaching performance is never guaranteed to be
better than individual teaching. Therefore it is obvious that in
order to achieve better joint teaching performance, the teachers
should share information with each other and decide how to
tweak their own teaching subset based on the selection made
by the peers. This is also intuitive in real-world education,
where human teachers collaboratively teach student better via
communication with each other. Motivated by this observation,
we deﬁne a collaborative teaching strategy that encourages
information sharing between teachers to jointly minimize the
teaching risk, while keeping the local data private.

A. Regularized Dual Learning for Collaborative Teaching

The dual objective of the learning paradigm for the learner

gives:

α∗ = arg min

K
(cid:88)

Ni(cid:88)

(cid:96)∗(−αi

j) +

λ
2

α

(cid:107)Zα(cid:107)2

(9)

i=1

j=1
where (cid:96)∗ is the Fenchel dual of the loss function (cid:96). Let
N = (cid:80)K
i=1 Ni denote the number of training instances
delivered by the teachers. Z ∈ Rd∗N denotes aggregated data
matrix with each column corresponding to a data instance. The
duality comes with the mapping from dual to primal variable:

ω(α) = Zα as given by the KKT optimality condition. α is
the N -dimensional dual variable, where each αi
j denotes the
dual variable corresponding to the jth data instance hosted by
teacher i. If αi
j diminishes, the corresponding data instance
Z i
j consequently has no impact over the dual objective in (9).
Thus, only the data instances with non-zero αi
j dominates the
training process. Motivated by this observation, we propose to
optimize the dual objective and enforce sparsity structure of α
simultaneously to achieve selection of the informative training
samples in (10). Bearing in mind the goal of the collaborative
teaching, we also introduce an additional quadratic penalty
shrinking the gap between θ∗ and the learnt model Zα.

α = arg min
αi
j ,i∈[K]

1
n

K
(cid:88)

Ni(cid:88)

i=1

j=1

(cid:96)∗(−αi

j) +

λ
2

(cid:107)Zα(cid:107)2

+ λθ(cid:107)θ∗ − Zα(cid:107)2 + λα

K
(cid:88)

Ni(cid:88)

i=1

j=1

wi

j|αi
j|

(10)

j|. 1/|ˆαi

j can be set up as 1/|ˆαi

where λα and λθ are the weight coefﬁcients of the adaptive
l1-norm based penalization enforcing sparsity of α and the
quadratic penalty minimizing the teaching risk R∗. wi
j is data-
dependent per-variable weight assigned to each dual variable
αi
j. Based on [23], wi
j| denotes
a warm-start estimate of αi
j, which can be derived by simply
calculating the Ordinary-Least-Squares solution to (cid:107)θ∗−Zα(cid:107)2.
The teaching objective given in (10) is apparently convex
according to the property of Legendre-Fenchel transform. Thus
solving (10) with gradient descent guarantees fact conver-
gence. As enforced by the l1-norm regularization over α, the
non-zero entries of the optimal α of the objective function
in (10) correspond to the most useful data instances for the
learner to reach the expected teaching goal and minimize
the teaching risk R∗. In practice, the learned α has a small
fraction of entries with dominant magnitudes, and rest are
negligible. We thus rank the data instances Z i
j according to
the magnitude of |αi
j|. The top-ranked |S| data instances with
the largest |αi
j| are selected to form the ﬁnal training subset
for the learner. Since the selected data instances are distributed
over different teachers. Solving (10) helps to jointly identify
which data instances on each of the K teachers should be
used to teach the learner. In the consensus optimization, each
teacher learns to conduct the selection based on the decisions
of the other teachers. Compared to heuristically tuning each
teacher’s decision, solving (10) explicitly coordinates different
teachers to deliver collaborative subset selection to minimize
the teaching risk globally. Furthermore, we observe that the
solution to (10) enjoys a similar property as the Oracle
property of adaptive Lasso [23] with an appropriately chosen
λθ and λα, as given in Observation 1.

Observation 1. Given a training set {(xi, yi)}i∈[N ], where
xi ∈ Rd. We assume that the goal of the super teaching
satisﬁes θ∗ = (cid:80)
i xi, where A ⊂ [N ], and α∗ denotes
the dual variable. We further assume that λθ (cid:29) ϕ where
ϕ is the empirical upper limit for the learner’s classiﬁcation
loss on the training set. Given γ > 0, if λα
2 → 0 and
λθ

i∈A α∗

d− 1

Data: {zi

j i = 1, 2, 3..., K, j = 1, 2, 3, ..., Ni} hosted by

K teachers

Input: T ≥ 1 as the maximum iteration steps, scaling
parameter 1 ≤ βi ≤ K, by default βi = 1

Output: αi
Initialize: αi
for t = 1 to T do

j, i = 1, 2, ..., K, j = 1, 2, ..., Ni
j = 0 for all machines and (cid:101)θ(0) = 0

λ

for all teachers i = 1, 2, 3, ..., K in parallel do
jxi
jyi
j + ∆αi

2 (cid:107)(cid:101)θ(t−1) + 1
j=1 ∆αi
∆αi = arg min
λ
(cid:96)∗(−α(t−1),i − ∆αi) + (cid:80)Ni
j|αi
j=1 wi
(cid:80)Ni
λθ(cid:107)(cid:101)θt−1 + 1
j − θ∗(cid:107)2
jyi
λ
αt,i = αt−1,i + βi

j=1 ∆αi
K ∆αi

(cid:80)Ni

jxi

∆αi

j(cid:107)2 +

j| +

end
Reduce on the central parameter server
j=1 αt,i
(cid:101)θt = (cid:101)θt−1 + 1
λ
Broadcast (cid:101)θt to all K teachers

j xi
j

(cid:80)Ni

(cid:80)K

i=1

end
Algorithm 1: Block-Coordinate Descent for Collaborative
Super Teaching

2 + γ

d− 1

λα
2 → ∞ (see Theorem 2 in [23]), then the global
λθ
optimal solution α to (10) must satisfy the Oracle property:
limd→∞ P (α = α∗) = 1.

B. privacy-preserving teaching via block-coordinate descent

We propose to use block-coordinate descent to solve (10).
In each round of the descent process, we minimize (10) with
j belonging to the same teacher i, while
respect
ﬁxing all the other α as constants. The pseudo codes of the
optimization procedure is given in algorithm 1.

to all αi

We use αt,i

to denote the disjoint block {αi

j}, j =
1, 2, 3.., Ni corresponding to the data instances hosted by
teacher i, which are estimated at the t-th iteration. Z i denotes
the columns in the data matrix Z storing the data instances
of the teacher i. In each round of iteration, we update the
dual variable αi for each of the K teachers in parallel.
We assume an incremental update ∆αi based on the value
of αt−1,i. This incremental variation indicates the descent
direction minimizing the teaching loss with respect to the
block αi. It is estimated by minimizing the local approx-
imation to (10), where αi
is represented as the additional
combination αt−1,i + ∆αi. βi is the learning rate adjusting
the descent step length for the block αi. Note updating each
block αi does not require knowledge of the values for the other
blocks. All the local updates need is the local dual variable
value αt−1,i obtained from the last round and the global
aggregated variable (cid:101)θ broadcasted from the central server.
As such, update of each block can be conducted in parallel
without inter-teacher communication. Similarly, aggregating to
derive the global variable (cid:101)θ is also a parallel process. The
teachers forward the local aggregation (cid:80)Ni
j to the
central server, where simply summing up the local aggregation
gives the global variable value. It is worth noting that we
use (cid:101)θ to denote the global aggregation variable. It does not

j=1 αt,i

j xi

j xi

j xi

j=1 αt,i

j=1 αt,i

imply the primal-dual correspondence, as we are solving a
different problem from (10). Throughout the block-coordinate
descent process, it is easy to ﬁnd that i) private data hosted
by any teacher is kept on its own machine in the collaboration
stage. In other words, no training data is transferred directly
between teachers. Furthermore, updating (cid:101)θ only needs to
transfer the local aggregation (cid:80)Ni
j to the central sever.
It is difﬁcult to infer any statistical proﬁles about the local
data of the teachers based on solely on the local aggregation
(cid:80)Ni
j, which reduces the risk of unveiling local private
data of one teacher to the others in the collaboration step. ii)
sharing information between different teachers is conducted
in the proposed method by updating the global aggregation
variable (cid:101)θ and then broadcasting the updated value to all K
teachers. Communication for teaching collaboration is thus
efﬁcient, with the cost of O(Kd) in each round of iteration.
Moreover, according to [21], updating αi of local teachers can
be triggered with asynchronous parallelism, which allows to
organize efﬁcient teaching collaboration with large number of
teachers and tight communication budget.

We demonstrate how to apply the proposed super teach-
ing method to two prevalent learners, l2-regularized Logistic
Regression (LR) and Ridge Regression (RR).

j, yi

1) Collaborative Teaching for l2-regularized Logistic Re-
gression: (xi
j), i = 1, 2, 3, .., K, j = 1, 2, 3, .., Ni denote
the features and labels of the data instances hosted by all
K teachers. To instantiate (10) to l2-regularized Logistic
Regression, we concretize the deﬁnition of (cid:96)∗ with slight
modiﬁcation on the weight parameters, which gives:

α = arg min

α

λ
2

K
(cid:88)

Ni(cid:88)

i=1

j=1

(cid:107)

1
λ

jxi
αi

jyi

j(cid:107)2 +

K
(cid:88)

Ni(cid:88)

(cid:96)∗(−αi
j)

K
(cid:88)

Ni(cid:88)

+ λα

wi

j|αi

j| + λθ(cid:107)θ∗ −

i=1
s.t.0 ≤ αi

j=1

j ≤ 1

i=1

j=1

K
(cid:88)

Ni(cid:88)

i=1

j=1

1
λn

(11)

jyi
αi

jxi

j(cid:107)2

where yi
as +/-1 and (cid:96)∗(−αi

j is the binary class label of the data instance, valued

j) = αilog αi + (1 − αi) log(1 − αi)

The collaborative super teaching for LR is deﬁned as a box-
constrained convex quadratic programming problem following
the principle of algorithm 1. The optimization process is given
in algorithm 2: Π is the projection operator to make the
updated value of α(t),[k] satisfy the box constraint.

2) Collaborative Teaching for Ridge Regression: Given the
feature xi
j of each data instance, we
can deﬁne the objective of collaborative teaching for Ridge
Regression similarly.

j and regression target yi

α = arg min

α

1
2

K
(cid:88)

Ni(cid:88)

i=1

j=1

(cid:96)∗(−αi

j) +

+ λα

K
(cid:88)

Ni(cid:88)

i=1

j=1

wi

j|αi

j| + λθ(cid:107)θ∗ −

1
2λ

(cid:107)

K
(cid:88)

Ni(cid:88)

i=1

j=1

jαi
xi

j(cid:107)2

1
λ

K
(cid:88)

Ni(cid:88)

i=1

j=1

jαi
xi

j(cid:107)2

Initialize: αi
for t = 1 to T do

j = 0 for all teachers and (cid:101)θ(0) = 0

for all teachers i = 1, 2, 3, ..., K in parallel do

− ∆αi

(cid:80)Ni

∆αi =
j=1 (cid:96)∗(−αt−1,i
arg min
j
∆αi
(cid:80)Ni
(cid:80)Ni
1
j(cid:107)2 + λα
jxi
j=1 ∆αi
λ
(cid:80)Ni
j| + λθ(cid:107)(cid:101)θt−1 + 1
∆αi
j=1 ∆αi
λ
αt,[k] = Π(αt−1,[k] + βK
K ∆α[k])

jyi

j=1 wi
jxi

j) + λ

2 (cid:107)(cid:101)θt−1 +
+
j
j − θ∗(cid:107)2

j|αt−1,i
jyi

end
Reduce on the central parameter server
j=1 αt,i
(cid:101)θt = (cid:101)θt−1 + 1
λ
Broadcast (cid:101)θt to all K teachers

j xi
j

(cid:80)Ni

(cid:80)K

i=1

end
Algorithm 2: Block-Coordinate Descent for Collaborative
Super Teaching of l2-Regularized Logistic Regression

Initialize: αi
for t = 1 to T do

j = 0 for teachers and (cid:101)θ0 = 0

for all teachers i = 1, 2, 3, ..., K in parallel do

(cid:80)Ni

∆αi =
j=1 (cid:96)∗(−αt−1,i
arg min
j
∆αi
(cid:80)Ni
j(cid:107)2 + λα
j∆αi
j=1 xi
j| + λθ(cid:107)(cid:101)θt−1 + 1
λ
K ∆αi

1
λ
∆αi
αt,i = αt−1,i + βi

(cid:80)Ni

− ∆αi

(cid:80)Ni
j=1 wi
j=1 ∆αi

j) + λ
j|αt−1,i
jxi

2 (cid:107)(cid:101)θt−1 +
+
j
j − θ∗(cid:107)2

end
Reduce on the central parameter server
j=1 αt,i
(cid:101)θt = (cid:101)θt−1 + 1
λ
Broadcast (cid:101)θt to all K teachers

j xi
j

(cid:80)Ni

(cid:80)K

i=1

end
Algorithm 3: Block-Coordinate Descent for Collaborative
Super Teaching of Ridge Regression

C. Computational complexity and communication cost

As shown in algorithm 2 and algorithm 3, estimating the
incremental update of each block αi is a convex quadratic
programming problem. With appropriately set λθ and λα, the
quadratic programming problem is well scaled and can be
solved in a well scalable way using polynomial time interior
point algorithms, such as [24]. According to algorithm 1, only
the step of aggregating the global variable (cid:101)θ needs communi-
cation between the K teachers and the learner. Assuming that
in total T iterations are needed in the block-coordinate descent
in algorithm 1, the overall communication cost of running the
collaborative teaching paradigm for both models is O(T Kd).
In practices, T = 100 is enough to achieve convergence of the
block coordinate descent.

where (cid:96)∗(−αi
j(cid:107)2 − αi
collaboratively teaching ridge regression in algorithm 3

j. It is thus easy to deﬁne

j) = 1

2 (cid:107)αi

jyi

We test the proposed collaborative teaching method with
both synthetic data set and real-world benchmark datasets
(summarized in Table.I). For the synthetic classiﬁcation and

(12)

IV. EXPERIMENTAL STUDY

A. Experimental setup

TABLE I
SUMMARY OF PUBLIC REAL-WORLD BENCHMARK DATASETS.

Dataset
Higgs
Superconduct

No. of Instances No. of Features

1,000,000
21,263

28
81

regression data set, we create clusters of random data instances
following normal distribution. In the classiﬁcation dataset,
equal number of clusters are assigned to positive and negative
classes to construct a balanced labelled data set. In the
regression dataset, the regression target Y is given by applying
random linear regressor to X. The dimensionality of each
data instance is ﬁxed to 10 universally. In the experimental
study, we assume that each of the K teachers hosts (cid:98) N
K (cid:99)
data instances as the local data set. To generate i.i.d. data
instances, the mean and variance of the normal distribution
for data generation are kept the same for different teachers.
The summary of the real-world datasets is shown by Table.I,
which are used to evaluate practical performances of the
proposed method over large-scale real-world data samples. The
empirical study over the real-world data samples uniformly the
data set and assign (cid:98) N

K (cid:99) instances to each teacher.

To generate the target of teaching in the study, we run
standard LR and RR on all the data samples hosted by K
teachers to derive true model parameter θgt. The teaching
target θ∗ is given by adding a white Gaussian noise τ ∈ Rd, as
θ∗ = θgt + τ . We ﬁx the magnitude of τ as the same of that
of θgt in the following experiments to generate a teaching
target with reasonable difﬁculty. To measure the teaching
performances, we use the teaching risk R∗ as the major metric.
In addition, in the binary classiﬁcation scenario, we apply both
the teaching target θ∗ and the learned parameter ˆθS on the
whole data set. We count the fraction of the data instances
where the output labels of the teaching target and the learned
model are consistent. The higher the fraction value is, the
better the teaching performance is, as the goal of teaching is to
approximate the target model as close as possible. Similarly for
regression, we measure r-square score between the regression
output of the teaching target model and the learned model on
the whole synthetic regression data set as the metric measuring
the teaching quality for regression. The two additional metrics
are noted as ρlr and ρrr in the experiments.

We compare the proposed collaborative teaching method
to the primitive oblivious teaching strategy. To organize the
oblivious teaching, we further require that each teacher selects
|S|
K instances as the identiﬁed local subset, as there is no
heuristic preference over any speciﬁc teacher. The oblivious
teaching is conducted by running the MINLP based teaching
paradigm [6] on each teacher. The selected data instances
are aggregated to form the training set of the learner. The
proposed collaborative teaching method is implemented with
Spark TFOCS library on a AWS EC2 public cloud server,
with one core per teacher. For implementing the oblivious
teaching method, it is difﬁcult to ﬁnd an open-sourced MINLP

library tailored for parallel computing environments. We thus
use Spark to call the MINLP solver of NEOS [25] for each
teacher and aggregate the selected data instances to form the
learner’s training set. We record the running time to evaluate
and compare the scalability of both teaching methods, as
indicated by κ in the empirical study.

B. Benchmark with synthetic classiﬁcation and regression
datasets

For the tests of both classiﬁcation and regression scenarios,
we vary the total number N of synthetic data instances as
5000, 10000, 50000, 100000 and 500000 to cover intermediate
and large-scale data volumes. For each choice of N , we further
set the number of teachers K to be 5 and 10 respectively.
For a ﬁxed combination of N and K, we run 10 trials. In
each trial, we draw randomly an iid synthetic instances and
apply the proposed method of collaborative super teaching. We
show the fraction of the selected subset |S|/N that achieves
the minimum average teaching risk of the 10 trials in Table.II
and Table.III. λα and λθ are the parameters of the proposed
collaborative teaching method. In the experimental study of
both the classiﬁcation and regression scenario, both parameters
are tuned empirically using validation data instances that are
generated independently besides from the benchmark set. It
is interesting to ﬁnd out that the values of λα and λθ are
insensitive to the varying N and K. Therefore, we ﬁx λα
as 0.1 and λθ as 1000 in the binary classiﬁcation scenario.
they are ﬁxed as 1 and 2000
In the regression scenario,
respectively. We also run the MINLP based teaching paradigm
as a centralized teacher over all the training data instances,
as indicated by MINLP in both tables. We compare to the
centralized teaching to highlight the computing efﬁciency of
the proposed teaching paradigm.

The collaborative super teaching method selects less than
0.1 and 0.4 of the data instances to achieve accurate teaching
result in both the classiﬁcation and the regression test. Given
N ﬁxed, increasing K barely changes the teaching risk and
the decision consistency between the target model and the
model learned with the selected subset. However, it slightly
increases the running time due to the increased communication
cost during the global aggregation and broadcasting of (cid:101)θt.
In all of the tests, the collaborative super teaching method
runs for 85 to 150 iterations to reach convergences. With the
same N , more teachers (larger K) requires more iterations
before convergence. On one hand, collaborating with more
teachers leads to smaller block size of the block coordinate
descent, which causes slower convergence [26]. On the other
hand, more teachers help to reduce the computational cost on
each teacher. Depending on the computational resource budget
of the teachers, we can beneﬁt from the balance to organize
efﬁcient collaboration of the teachers. In general, compared to
the oblivious teaching and the centralized teaching method, the
collaborative teaching method provides signiﬁcantly lower or
similar teaching risk and better approximate the target model
with the selected training subset in both tests. It requires
distinctively less running time. The collaborative teaching

TABLE II
COMPARISON OF THE TEACHING PERFORMANCE IN THE BINARY CLASSIFICATION SCENARIO

Collaborative Super Teaching
κ
7.15s
8.13s
16.90s
18.85s
67.20s
72.52s
180.24s
179.12s
1064.15s
1100.75s

|S|/N R∗(ˆθS)
3.7e-2
4.5e-2
1.5e-2
1.0e-2
2.5e-2
2.4e-2
2.9e-2
3.0e-2
6.4e-2
7.2e-2

ρlr
0.95
0.92
0.94
0.97
0.93
0.96
0.95
0.93
0.98
0.98

0.43
0.37
0.23
0.26
0.35
0.36
0.33
0.41
0.15
0.12

Oblivious Super Teaching

MINLP

|S|/N R∗(ˆθS)
6.0e-2
5.8e-2
2.0e-2
2.0e-2
N/A
1.3e-2
N/A
N/A
N/A
N/A

0.54
0.62
0.63
0.54
N/A
0.22
N/A
N/A
N/A
N/A

ρlr
0.90
0.87
0.87
0.89
N/A
0.94
N/A
N/A
N/A
N/A

κ
200.53s
197.33s
320.69s
327.16s
N/A
2100s
N/A
N/A
N/A
N/A

|S|/N R∗(ˆθS)

ρlr

κ

4.0e-2

0.30

0.97

175.91s

N/A

N/A

N/A

N/A

N/A

N/A

N/A

N/A

N/A

N/A

N/A

N/A

N/A

N/A

N/A

N/A

TABLE III
COMPARISON OF THE TEACHING PERFORMANCE IN THE REGRESSION SCENARIO

Collaborative Super Teaching
κ
2.04s
2.38s
4.25s
5.15s
35.16s
32.53s
81.80s
101.03s
310.42s
395.91s

|S|/N R∗(ˆθS)
81.53
1.20e-1
80.43
1.12e-1
69.52
9.00e-2
67.46
7.00e-2
110.62
1.00e-1
118.36
1.12e-1
99.85
3.00e-1
101.36
3.00e-1
51.72
3.60e-1
49.67
3.60e-1

ρlr
0.82
0.83
0.86
0.87
0.84
0.82
0.88
0.88
0.94
0.95

Oblivious Super Teaching

MINLP

|S|/N R∗(ˆθS)
112.93
2.00e-1
120.54
1.80e-1
93.20
2.00e-1
94.20
2.50e-1
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A
N/A

ρlr
0.71
0.70
0.75
0.76
N/A
N/A
N/A
N/A
N/A
N/A

κ
230.53s
262.65s
320.28s
570.32s
N/A
N/A
N/A
N/A
N/A
N/A

|S|/N R∗(ˆθS)

ρlr

κ

1.20e-1

98.09

0.76

195.84s

2.00e-1

87.96

0.76

506.01s

N/A

N/A

N/A

N/A

N/A

N/A

N/A

N/A

N/A

N/A

N/A

N/A

N

5000

10000

50000

100000

500000

N

5000

10000

50000

100000

500000

K

5
10
5
10
5
10
5
10
5
10

k

5
10
5
10
5
10
5
10
5
10

method costs less than 5% of the running time compared
to both of the opponents according to Table.II and Table.III.
Notably, the time and storage cost of the oblivious teaching
becomes prohibitively expensive when N ≥ 10000. NEOS
can’t get results given the large N . We write N/A in case
NEOS fails to solve the MINLP problem. In contrast, the
proposed collaborative teaching method can still produce
accurate teaching output with acceptable time cost. Despite of
the difference of implementation details of the three teaching
methods, the difference of running time conﬁrms the superior
computational efﬁciency of the proposed collaborative teach-
ing paradigm. Beneﬁted from the consensus optimization pro-
cess, the proposed collaborative teaching paradigm provides a
highly scalable solver to the distributed super teaching task.
Moreover, it is interesting to ﬁnd out the centralized teaching
paradigm performs better than the oblivious teaching method.
This observation is consistent with what Theorem.1 unveils:
information sharing is the key to achieve good teaching co-
operation. Stand-alone teaching without inter-communication
between teachers can do harm to the teaching performance.

C. Benchmark with real-world data sets

Two real-world data sets, Higgs and Superconduct, are
the collaborative super teaching method
employed to test
for l2-regularized Logistic Regression and Ridge Regression
respectively. The number of the teachers is chosen to be 5 on
both data sets. The setting of λα, λθ, the computing platform

and the teaching goal follow the same setting as the test
on synthetic data. For Higgs data set, we randomly sample
1000000 instances from the whole set for 10 times and re-
run the proposed method on the sampled Higgs data samples.
Figure 1a illustrates the variation of the averaged teaching risk
of the proposed collaborative teaching method by incremen-
tally increasing the number of the jointly selected instances. In
Figure 2a, we demonstrate how the objective function value
of the proposed teaching method diminishes as the iterative
block-coordinate descent runs. On Higgs data set, the proposed
method selects only 15% of the 1000000 instances to achieve
the teaching risk of 3.28. The corresponding consistency score
ρlr is 0.99. It indicates that the learner manages to approximate
the expected target model perfectly with the selected subsets
given the teachers. Interestingly, the teaching risk declines
at ﬁrst as |S|/N increases to 15%. After this turning point,
the teaching risk begins to increase again. The observation is
consistent with our intuitive understanding about the teaching
process. Insufﬁcient and too many data instances can do harm
equally to the teaching performances. From Figure 2a, we can
ﬁnd the objective function value of the proposed collaborative
teaching method declines rapidly within 50 iterations. In
this experiment, the consensus optimization process of the
proposed collaborative teaching paradigm converges with 80
iteration steps. which costs 2095.48s. Similarly, Figure 1b
shows the declination of teaching risk by increasing gradually
the number of selected instances on Superconduct data set.

REFERENCES

[1] S. A. Goldman and M. J. Kearns, “On the complexity of teaching,”
Journal of Computer and System Sciences, vol. 50, no. 1, pp. 20–31,
1995.

[2] A. Shinohara and S. Miyano, “Teachability in computational learning,”

New Generation Computing, vol. 8, no. 4, pp. 337–347, 1991.

[3] S. Zilles, S. Lange, R. Holte, and M. Zinkevich, “Models of cooperative
teaching and learning,” Journal of Machine Learning Research, vol. 12,
no. Feb, pp. 349–384, 2011.

[4] J. Zhu, “Machine teaching for bayesian learners in the exponential
family,” in Advances in Neural Information Processing Systems, 2013,
pp. 1905–1913.

[5] Y. Chen, A. Singla, O. Mac Aodha, P. Perona, and Y. Yue, “Understand-
ing the role of adaptivity in machine teaching: The case of version space
learners,” arXiv preprint arXiv:1802.05190, 2018.

[6] Y. Ma, R. Nowak, P. Rigollet, X. Zhang, and X. Zhu, “Teacher improves
learning by selecting a training subset,” in International Conference on
Artiﬁcial Intelligence and Statistics, 2018, pp. 1366–1375.

[7] L. Liberti, “Undecidability and hardness in minlp,” RAIRO Operations

Research, to appear.

[8] E. Karabulut, “Distributed integer programming,” Phd Thesis, GeorgiaT-

ech, 2017.

[9] J. Liu, X. Zhu, and H. Ohannessian, “The teaching dimension of linear
learners,” in International Conference on Machine Learning, 2016, pp.
117–126.

[10] F. J. Balbach, “Measuring teachability using variants of the teaching
dimension,” Theoretical Computer Science, vol. 397, no. 1-3, pp. 94–
113, 2008.

[11] T. Doliwa, G. Fan, H. U. Simon, and S. Zilles, “Recursive teaching
dimension, vc-dimension and sample compression,” The Journal of
Machine Learning Research, vol. 15, no. 1, pp. 3107–3131, 2014.
[12] L. Haug, S. Tschiatschek, and A. Singla, “Teaching inverse reinforce-
ment learners via features and demonstrations,” in Advances in Neural
Information Processing Systems, 2018, pp. 8472–8481.

[13] W. Liu, B. Dai, X. Li, Z. Liu, J. M. Rehg, and L. Song, “Towards black-
box iterative machine teaching,” arXiv preprint arXiv:1710.07742, 2017.
[14] W. Liu, B. Dai, A. Humayun, C. Tay, C. Yu, L. B. Smith, J. M. Rehg,
and L. Song, “Iterative machine teaching,” in International Conference
on Machine Learning, 2017, pp. 2149–2158.

[15] K. R. Patil, J. Zhu, Ł. Kope´c, and B. C. Love, “Optimal teaching for
limited-capacity human learners,” in Advances in neural information
processing systems, 2014, pp. 2465–2473.

[16] S. Mei and X. Zhu, “Using machine teaching to identify optimal
training-set attacks on machine learners.” in AAAI, 2015, pp. 2871–2877.
[17] S. Alfeld, X. Zhu, and P. Barford, “Data poisoning attacks against

autoregressive models.” in AAAI, 2016, pp. 1452–1458.

[18] Y. Ma, K.-S. Jun, L. Li, and X. Zhu, “Data poisoning attacks in
contextual bandits,” in International Conference on Decision and Game
Theory for Security. Springer, 2018, pp. 186–204.

[19] Y. Fan, F. Tian, T. Qin, X.-Y. Li, and T.-Y. Liu, “Learning to teach,” in

International Conference on Learning Representations, 2018.

[20] J. Konecny, H. McMahan, F. X. Yu, P. Richtarik, A. T. Suresh, and
D. Bacon, “Federated learning: Strategies for improving communication
efﬁciency,” in NIPS Workshop on Private Multi-Party Machine Learning,
2016.

[21] M. Jaggi, V. Smith, M. Takac, J. Terhorst, S. Krishnan, T. Hofmann,
and M. I. Jordan, “Communication-efﬁcient distributed dual coordinate
ascent,” in Advances in neural information processing systems, 2014,
pp. 3068–3076.

[22] A. N. Bhahpji, S. Charkraborty, P. Mittal, and S. Calo, “Analyz-
ing federated learning through an adversarial lens,” in arXiv preprint
arXiv:1811.12470, 2018.

[23] H. Zhou, “The adaptive lasso and its oracle properties,” Journal of the

American Statistical Association, pp. 1418–1429, 2006.

[24] Y. Ye and E. Tse, “An extension of karmarkar’s projective algorithm
for convex quadratic programming,” Mathematical Programming, pp.
157–179, 1989.

[25] E. D. Dolan, “The neos server 4.0 administrative guide,” Mathematics
and Computer Science Division, Argonne National Laboratory, Techni-
cal Memorandum ANL/MCS-TM-250, 2001.

[26] A. Beck and L. Tetruashvili, “On the convergence of block coordinate
descent type methods,” SIAM Journal of Optimization, pp. 2037–2060,
2013.

(a) Higgs Data

(b) Superconduct Data

Fig. 1. Teaching risk variation with different number of the selected data
instances

(a) Higgs Data

(b) Superconduct Data

Fig. 2. Convergence of the quadratic programming based collaborative
teaching process

As seen in the ﬁgure, the proposed teaching method selects
25% of the data instances to reach the teaching risk of 2.23
and ρrr of 0.97. Figure 2b illustrates the declination of the
objective functions values on Superconduct achieves. Similar
pattern of the teaching risk variation is witnessed in Figure 1b,
compared to Figure 1a. The turning point of the teaching risk
curve conﬁrms empirically the existence of the optimal subset
for teaching. Based on the selected subset, the learner can
accurately ﬁt the target regression model. Minimizing the col-
laborative teaching objective for Ridge Regression converges
within 125 iterations, which costs only 35.12s.

V. CONCLUSION AND DISCUSSION

In this paper, we explore how to organize scalable, col-
laborative, and privacy-preserving super teaching with mul-
tiple teachers. We formulate a distributed convex optimiza-
tion problem for conducting consensus super teaching with
varying number of teachers, and adopt a block descent based
solver to optimize each teacher’s selection on teaching items.
Our approach preserves data privacy during the collaborative
teaching process. We show that the proposed collaborative
teaching scheme can achieve lower teaching risk than the
non-collaborative scheme. Empirical results on both synthetic
and real-word data sets conﬁrm the superior performance of
the proposed collaborative teaching method over the non-
collaborative solution. Future work will study practical use
of distributed and privacy-preserving super teaching based on
the proposed collaborative teaching framework, e.g., we plan
to explore the teaching goals that are realistic to practical use,
such as AUC-maximization oriented goals.

0.00.10.20.30.4|S|/n1020304050Teaching Risk0.00.10.20.30.4|S|/n1020304050Teaching Risk01020304050607080Iterations20040060080010001200Colluding Teaching Loss020406080100120Iterations100200300400500600700800Colluding Teaching Loss