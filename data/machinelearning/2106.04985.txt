Energy-Based Models for Code Generation
under Compilability Constraints

Tomasz Korbak,1,∗ Hady Elsahar,2 Marc Dymetman,2 Germ´an Kruszewski2
t.korbak@sussex.ac.uk
{hady.elsahar,marc.dymetman,german.kruszewski}@naverlabs.com
1University of Sussex, United Kingdom
2Naver Labs Europe, France

1
2
0
2

n
u
J

9

]

G
L
.
s
c
[

1
v
5
8
9
4
0
.
6
0
1
2
:
v
i
X
r
a

Abstract

Neural language models can be successfully
trained on source code, leading to applications
such as code completion. However, their ver-
satile autoregressive self-supervision objective
overlooks important global sequence-level fea-
tures that are present in the data such as syn-
tactic correctness or compilability.
In this
work, we pose the problem of learning to
generate compilable code as constraint satis-
faction. We deﬁne an Energy-Based Model
(EBM) representing a pre-trained generative
model with an imposed constraint of generat-
ing only compilable sequences. We then use
the KL-Adaptive Distributional Policy Gradi-
ent algorithm (Khalifa et al., 2021) to train
a generative model approximating the EBM.
We conduct experiments showing that our pro-
posed approach is able to improve compilabil-
ity rates without sacriﬁcing diversity and com-
plexity of the generated samples.

1

Introduction

Code completion is an essential feature of any mod-
ern Integrated Development Environment (IDEs).
It supports developers with recommendations about
the next token to write given a context, speed-
ing up software development and reducing the
number of mistakes. A large body of work has
relied on statistical language modeling, treating
programming languages as natural languages us-
ing probabilistic grammars (Raychev et al., 2014;
Bielik et al., 2016), and more recently relying on
neural language models (Liu et al., 2016a; Svy-
atkovskiy et al., 2020a,b; Arkesteijn et al., 2020;
Ciniselli et al., 2021).1 In particular, neural autore-

∗ Work done during a research internship at Naver Labs

Europe.

1See Allamanis et al. (2018) for a survey.

gressive language models have been favoured due
to their scalability and generic training procedure
that can exploit large codebases (e.g. open source
code repositories available on GitHub) through self-
supervised training.

Despite these desirable traits, neural language
models, trained in the standard way, are known
to suffer from myopia and to overlook global
sequence-level features that are present in the data
and which might be crucial for the quality of gen-
erated sequences (Parshakova et al., 2019b). This
leads to repetitions, hallucinations and failing to
capture long-distance consistency requirements. In
a code generation context, this is demonstrated in
compilation errors that are a common failure mode
in such tasks as translation between programming
languages (Roziere et al., 2020). This problem has
inspired a large body of work on different fronts
on injecting sequence-level priors by either directly
optimizing sequence-level features (Ranzato et al.,
2016) or through fusion with grammars and au-
tomata (Xiao et al., 2016). These techniques aim
to balance between the desirable traits and fast
inference of neural autoregressive models trained
in the standard way and the satisfaction of global
sequence-level features.

In this work, we formulate compilable code gen-
eration as a constraint satisfaction problem. We
show that this formulation leads to a unique dis-
tribution represented by an Energy-Based Model
(EBM). This unique distribution by deﬁnition fully
satisﬁes the compilability constraints while having
a minimal KL divergence from the original autore-
gressive generative model trained through cross en-
tropy. We then train an auto-regressive generative
model to approximate the underlying distribution
of this EBM using the KL-Adaptive Distributional

 
 
 
 
 
 
Policy Gradient algorithm (Khalifa et al., 2021).

In our experiments, we show that our approach
signiﬁcantly improves compilability rates without
sacriﬁcing diversity or complexity of the generated
This alleviates the drawbacks of
examples.
reinforcement learning ﬁne-tuning techniques that
maximize compilability but deviate signiﬁcantly
from the original generative model, which leads
to severe loss in diversity and complexity of the
generated samples. Finally, we complement our
experiments with a qualitative analysis of the
effect of several ﬁne-tuning approaches on the
distribution of compilation errors.

2 Related Work

Imposing compilability constraints on genera-
tive models There is a body of work focusing on
unconditional code generation or code completion:
generating a piece of source code given a preceding
piece of source code (Nguyen et al., 2013; Raychev
et al., 2014; Karpathy et al., 2015; Bielik et al.,
2016). That work, however, focuses on perplexity
and similarity with respect to ground truth comple-
tions (in terms of exact-match accuracy, Levens-
thein distance and ROUGE scores) (Svyatkovskiy
et al., 2020a; Lu et al., 2021), usually failing to
measure and control for compilability of generated
sequences or semantic and syntactic constraints in
general.2 On the other hand, semantic and syntactic
constraints are frequently considered in language-
to-code translation or program synthesis. For in-
stance, Zhong et al. (2017), who used policy gra-
dients to train a model for translating natural lan-
guage questions to corresponding SQL queries and
– in addition for rewarding for query execution re-
sults – added a penalty for syntactically invalid
queries. Taking that one step further, Kulal et al.
(2019) use compilation errors (with their precise
location) to guide search over the space of possible
programs.

Optimizing sequence-level rewards for text gen-
eration Most previous attempts at steering au-
toregressive model to conform to global constraints
deﬁned over entire sequence have employed re-
inforcement learning (RL). This includes using
Reinforce (Williams, 1992a) for machine transla-

2One exception is the work of Maddison and Tarlow
(2014), who augment neural probabilistic context free gram-
mars with semantic constraints and use them for unconditional
generation.

tion (Ranzato et al., 2016) or actor critic (Konda
and Tsitsiklis, 2000) for abstractive summariza-
tion (Paulus et al., 2018), caption generation (Liu
et al., 2016b), dialogue (Li et al., 2016b), and video
captioning (Pasunuru and Bansal, 2017). Some ap-
proaches (for instance, in machine translation and
summarization (Ranzato et al., 2016; Bahdanau
et al., 2017)) directly optimize performance met-
rics such as BLEU and ROUGE at training time.
Others use heuristic rewards (for instance Li et al.
(2016b) for dialogue generation and Tambwekar
et al. (2019) for story generation) in order to ob-
tain certain a priori desirable features of generated
sequences that then incentivize good performance
on target metrics. A weakness of using RL in ﬁne-
tuning generative models is the problem of catas-
trophic forgetting: maximizing global, sequence-
level rewards leads to very large deviations from
the original autoregressive model trained through
cross-entropy. This often results in signiﬁcant re-
ductions in ﬂuency and diversity of generated sam-
ples. The catastrophic forgetting problem is some-
times addressed by imposing a penalty term to the
rewards, such as the KL divergence between the
trained policy and the auto-regressive model. This
approach, termed “conservative ﬁne-tuning”, was
applied to generating melodies with music theory
rewards and organic molecules with synthesizabil-
ity rewards by Jaques et al. (2017) as well ﬁne-
tuning language models for controllable language
generation by Ziegler et al. (2019). This solution
doesn’t have an explicit notion of the optimal pol-
icy and often has hard time balancing between the
reward term and the KL penalty term, leading to
instability in training (Khalifa et al., 2021). Unlike
this approach, our formulation deﬁnes the optimal
distribution that satisﬁes both requirements.

Energy-based models for text Energy-based
models (EBMs) (Hinton, 2002; LeCun et al., 2006;
Ranzato et al., 2007) are a family of probabilistic
graphical models in which learning and inference
are done by associating an unnormalized probabil-
ity with each conﬁguration of observed and latent
variables. Early examples of EBMs applied to natu-
ral language processing include sequence labeling
problems (e.g. tagging) exploiting global proper-
ties of a sequence (Andor et al., 2016; Belanger
and McCallum, 2016). A recent surge of interest in
EBMs (Du and Mordatch, 2019) has not left text
generation unaffected (see (Bakhtin et al., 2020)
for a survey). Tu et al. (2020) proposed an energy-

based inference networks for non-autoregressive
machine translation. Parshakova et al. (2019b) and
Deng et al. (2020) augment a autoregressive lan-
guage models with an additional global factor to ob-
tain a lower perplexity on the training data. Khalifa
et al. (2021) develop a novel approach to distribu-
tional controllable text generation by constructing
an EBM satisfying desired statistical constraints
imposed on the set of generated sequences (such
as topic or gender statistics over the sequences)
and then train an autoregressive policy to approx-
imate it, which can be sampled from efﬁciently.
We build on Khalifa et al.’s approach by applying
it to a novel domain outside natural language and
deﬁning a new kind of constraint: compilability.

3 Method

Following Khalifa et al. (2021), we formulate com-
pilable code generation as a constraint satisfaction
problem over a space of generative models. There
are two constraints that a target generative model
p must satisfy. First, p must have minimal diver-
gence -in the distribution space- from an original
generative model a pre-trained using a standard
autoregressive language modeling objective. Sec-
ond, it must generate only sequences that satisfy
a certain sequence level constraint b. In our case,
b(x) = 1 iff x is a syntactically correct Python
program and b(x) = 0 otherwise. There two con-
straints can be represented as a product-of-experts
(Hinton, 2002) energy-based model

P (x) = a(x)b(x).

(1)

p(x) can be obtained from P (x) by dividing it by
a normalization constant Z:

where

p(x) =

1
Z

P (x),

.
=

Z

(cid:88)

P (x).

(2)

(3)

x
This EBM P is unique, it represents a distribution
p that optimally reconciles the two constraints. It
is a special case of the generalized maximum en-
tropy formulation presented in (Csisz´ar and Shields,
2004) for applying constraints over distributions.
However, one problem still remains: it is not
straightforward how to draw samples x ∼ p(x)
or even evaluating probability p(x) from this op-
timal unique distribution. A simple method for
drawing samples from the p distribution could be

sampling sequences from a and ﬁltering on b(x).
While this method sounds simple, there’s no di-
rect way of using it for interactive code completion
as sampling full sequences till the end is neces-
sary to ﬁlter through the sequence-level ﬁlter b(x).
Therefore our objective here is to obtain another
autoregressive policy πθ to directly approximate p.
To attain this, Khalifa et al. (2021) (following
Parshakova et al. (2019a)) developed a training pro-
cedure called KL-Adaptive Distributional Policy
Gradients (KL-DPG) to train πθ to minimize the
KL divergence between p and πθ. The gradient of
this KL turns out to be tractable:

∇θDKL(p, πθ) = ∇θEx∼p log

p(x)
πθ(x)

= −∇θEx∼p log πθ(x)
= −Ex∼p∇θ log πθ(x)

(4)

(5)

(6)

= −

1
Z

(cid:88)

x

P (x)∇θ log πθ(x)

(7)

Let us now absorb the constant −1/Z into a learn-
ing rate α(θ) and estimate the expectation over p(x)
using importance sampling (Owen, 2013) from yet
another generative model q:

∇θDKL(p, πθ) ∝ Ex∼q

P (x)
q(x)

∇θ log πθ(x).

(8)

During training, both πθ and q are initialized as a.
Then, q is periodically updated to πθ if πθ surpasses
q in being closer to p (in terms of KL). For a pseudo-
code of the whole KL-DPG training procedure, see
Algorithm 1.

The gradient in (8) is similar to an estimate ob-
tained using policy gradients methods in standard
reinforcement learning (Sutton et al., 1999) with
P (x)/q(x) playing the role of a pseudoreward.
This similarity, however, is superﬁcial. Our ob-
jective is approximating a target generative model
p by minimizing DKL(p, πθ) rather than maximiz-
ing expected reward b(x) or P (x) or P (x)/q(x).
As we show in Section 5, these objectives produce
vastly different policies which diverge from p and
catastrophically forget what the pretrained model a
knew about its training domain. Furthermore, since
q will always be close to πθ, our pseudoreward
P (x)/q(x) effectively depends on policy parame-
ters θ.

Algorithm 1 KL-DPG
Require: EBM P , initial generative model a

1: πθ ← a
2: q ← a
3: for each iteration do
4:

for each episode do

5:

6:

7:

8:

sample x from q(x)
θ ← θ + α(θ) P (x)

q(x) ∇θ log πθ(x)
if DKL(p||πθ) < DKL(p||q) then

q ← πθ

Ensure: πθ

4 Experiments

4.1 Setup

Dataset: To prepare the training dataset, we
started from the Python150 dataset, which consists
of 150k Python source code ﬁles obtained from
GitHub (Raychev et al., 2016). Then, using the
code from Roziere et al. (2020), we extracted 713k
Python functions (both methods and standalone
functions) from it (250 MB of raw text data). The
additional ﬁltering criteria were compilability (ac-
cording to b(x)) and being less than 128 BPE to-
kens long. The dataset was then split into a training
subset Dtrain and test subset Dtest.

Initial generative model a: We implemented a
using the GPT-2 (Radford et al., 2019) architecture
with 117m parameters (gpt2-small) and kept
all the original hyperparameters (see Table 1 in the
Appendix). We trained a byte-level BPE tokenizer
(Sennrich et al., 2016) with special BOS and EOS
tokens to obtain a vocabulary of 50k tokens. The
model was trained for one epoch.

Compilability Scorer b: To check for compi-
lability, we call the compile command func-
tion from codeop module of Python Standard Li-
brary3 with a sequence x as argument and check
if it returns a code object. We apply no postpro-
cessing other than removing BOS and EOS tokens.
codeop.compile command is the implemen-
tation that Python interactive interpreters use in
read-eval-print loop (REPL) to determine whether
a string is a valid Python code. The method tries to
compile a string of Python code and raise and ex-
ception if there is a problem with the Python code,
in particular a SyntaxError for invalid Python

3https://docs.python.org/3/library/

codeop.html

syntax and ValueError or OverflowError
if there is an invalid literal.

This notion of compilability is concerned only
with syntactic correctness and does not execute
the body of a function. However, we found the
initial compilability rate Ex∼ab(x) of functions x
sampled from a(x) to be only 0.56, which leaves a
large margin for improvement.4

KL-DPG training πθ and q share their architec-
ture with a but have separate weights which are
only initially identical to a’s. Throughout the train-
ing, πθ will be updated to approximate p. See Table
2 in the Appendix for a complete list of hyperpa-
rameters used for training πθ and q using KL-DPG.

4.2 Baselines

We compare our method to a common approach of
using standard reinforcement learning to ﬁne-tune a
generative model to conform to desired constraints.
We use the Reinforce algorithm (Williams, 1992b)
which instead of minimizing divergence from the
target distribution p tries to maximize expected
reward Eπθ R(x). We consider two kinds of reward
R(x):

• R(x) = b(x), where the generative model is
simply rewarded for generating sequences that
compile;

• R(x) = P (x), where the generative model is
simply rewarded proportionally to the score
our EBM assigns to x. Intuitively, this objec-
tive gives reward for both compilability and
respecting the original generative model a.

4.3 Evaluation Metrics

We evaluate KL-DPG and two baselines in terms
of the following metrics:

1. Ex∼πθ b(x), compilability rate of sequences

sampled from πθ(x),

2. DKL(p, πθ), the forward KL divergence from

the optimal distribution p,

3. DKL(πθ, a), the reverse KL divergence from
the original pretrained generative model,

4. Distinct-1 score, a measure of text diversity in
terms of the frequency of token repetitions in
a sample x, proposed in the context of NLP
by (Li et al., 2016a),

4Note that initial compilability rate will be equal to our Z
x a(x)b(x) = (cid:80)

because Ex∼ab(x) = (cid:80)

x P (x) = Z.

5. Self-BLEU-5, a measure of text diversity
across samples, proposed in the context of
NLP by (Zhu et al., 2018),

6. Perplexity measured on Dtest, a held-out sub-
set of the data used for training a, calculated
as

(cid:104)

exp

−

1
N

(cid:88)

log πθ(x)

(cid:105)
,

x∈Dtest
where N is the overall number of tokens in
Dtest.

7. Sequence length, the average number of char-
acters in generated sequence x after detok-
enization,

8. AST node count, the average number of nodes
in an abstract syntax tree (AST) of sequences
that compile. Samples are parsed to their cor-
responding ASTs using the ast module from
Python Standard Library.5
Intuitively, this
metric should indicate the logical (as opposed
to surface) complexity of generated programs,

9. PEP8 error frequency, the average number
of violations of PEP8, the style guide for
Python,6 measured using pycodestyle,7 an off-
the-shelf linter (static code analysis tool). We
report the average number of errors per charac-
ter to avoid confounding by sequence length.

While high compilability rate is the target, the
remaining metrics control for various aspects of
ﬂuency, quality and diversity of generated sam-
ples. Most but not all of these aspects reduce to the
constraint of staying close to a; for instance, it is
possible for πθ to actually outperform a in match-
ing the statistics of a’s own training distribution
p∗(x).

5 Results

We present the evolution of nine evaluation metrics
as a function of gradient updates on Figures 1 and 2.
Reinforce with R(x) = b(x) quickly improves
compilability by a large margin but this improve-
ment is mirrored by an equally large divergence
from p and a. This divergence translates into gener-
ating sequences much shorter (in terms of the num-
ber of characters) and logically simpler (in terms
of the number of nodes in its AST) than an average

5https://docs.python.org/3/library/ast.

html

6https://www.python.org/dev/peps/

pep-0008/

7https://github.com/PyCQA/pycodestyle

Figure 1: Compilability rate Ex∼πθ b(x) (↑ better) of sam-
ples from policies obtained from KL-DPG, and two base-
lines: Reinforce with reward R(x) = b(x) and with reward
R(x) = P (x).

sequence sampled from a. This heavily decreased
sequence length (most of the generated functions
are one-liners) seems to artiﬁcially increase diver-
sity metrics (Self-BLEU-5 and Distinct-1).

Reinforce with R(x) = P (x) doesn’t improve
compilability rate until an inﬂection point after
which it quickly reaches perfect compilability at a
price of heavily diverging from both a and (perhaps
counterintuitively) p. The reason behind that, how-
ever, is that the policy heavily peaks around a single
sequence that is compilable. To understand what
causes this behavior, ﬁrst note that the objective
for Reinforce with R(x) = P (x) is to maximize
Ex∼πθ [a(x)b(x)]. Because R(x) = 0 for uncom-
pilable sequences, compilation rate will improve.
But for compilable sequences, the effective reward
is R(x) = a(x) meaning that πθ is rewarded most
for generating the most probable sequences (ac-
cording to a(x)), making them even more probable.
Eventually, Ex∼πθ a(x) is maximized by a policy
peaking on a single sample x that was the most
probable one according to a(x). This failure mode
is reﬂected in diversity metrics and perplexity. The
sequence the policy peaks on is also shorter and
less complex than an average sequence sampled
from a.

KL-DPG is the only method that consistently
improves compilability rate while decreasing di-
vergence from p, maintaining the diversity of a
and only slightly decreasing sequence length and

0100200gradient updates0.60.70.80.91.0Eb(x)KL-DPGR(x)=b(x)R(x)=P(x)Figure 2: Evaluation metrics KL(p|πθ) (↓ better), KL(πθ|a) (↓ better), Self-BLEU-5 (↓ better), Distinct-1 (↑ better), AST node
count (↑ better), PEP8 error count (↓ better), sequence length (↑ better), and perplexity (↓ better) for policies obtained from
KL-DPG, and two baselines: Reinforce with reward R(x) = b(x) and with reward R(x) = P (x).

the number of nodes in ASTs. Moreover, as a by-
product of improving compilability, KL-DPG is
also able to slightly decrease the perplexity and the
frequency of PEP8 violations per character. We
conjecture the decrease in perplexity is because
compilability provides a training signal enabling
πθ to ﬁt the a’s training distribution p∗(x) better
than a was able to.8 The decrease in the frequency
of PEP8 violations might be due to the fact that
compilability is correlated with PEP8 compliance.

5.1 Qualitative evaluation

To further analyze effects of different ﬁne-tuning
approaches on sample diversity, we measured the
frequency of BPE tokens in generated samples. For
each of four analyzed generative models, we sam-
pled 1000 sequences using pure ancestral sampling.
We then computed the frequency for each BPE to-
ken (the number of times it occurs) and its rank (its
index in a sorted list of tokens). We plotted these re-

8This mirrors the results obtained by Parshakova et al.
(2019b), who also deﬁned an EBM augmenting an autoregres-
sive model with prior knowledge about features of the training
set and observed a decrease in perplexity compared to pure
autoregressive training.

sults on Figure 4. This qualitative evaluation paints
a similar picture: ﬁne-tuning using Reinforce in-
curs a large (with R(x) = b(x)) or extreme (with
R(x) = P (x)) decrease in token diversity. In con-
trast, KL-DPG is able to maintain a relatively long
tail of token frequencies, not departing too far from
a.

Moreover, in order to gain better understanding
of how different ﬁne-tuning methods affect genera-
tive models we measured the frequency of different
categories of compilation errors for samples from
a and from ﬁne-tuned policies. This analysis is pre-
sented on Figure 3. We categorized errors using er-
ror messages produced by Python interpreter trying
to compile an uncompilable sequence. invalid
syntax is the most common failure mode (30%
of all sequences sampled from a), with a long tail
of other error categories. We can see that both
KL-DPG and Reinforce with R(x) = b(x) consis-
tently decrease error frequency across almost all
the categories.

Finally, in the Appendix we present randomly
generated samples from each discussed policy. Ta-
bles 3-6 contain samples obtained through uncon-
ditional generation. In addition to that, to illustrate

0200gradient updates0.00.51.01.52.0KL(p, )0200gradient updates0.00.51.01.52.0KL(, a)0200gradient updates0.20.30.40.5Distinct-10200gradient updates0.800.850.900.951.00Self-BLEU-5KL-DPGR(x)=b(x)R(x)=P(x)0200gradient updates15202530AST node count0200gradient updates0.01250.01500.01750.02000.02250.02500.0275PEP8 error frequency0200gradient updates6080100120140160Sequence length0200gradient updates1.01701.01711.01721.01731.01741.0175PerplexityFigure 3: The frequency (measured as the percentage of samples from πθ(x) causing a given error) of each kind compilation
error for the original generative model a and policies ﬁne-tuned using KL-DPG and Reinforce with R(x) = b(x). The policy
ﬁne-tuned using Reinforce with R(x) = P (x) was excluded because the single sequence it produces causes no compilation
errors. Percentages were computed using 500 samples while conﬁdence intervals were based on 3 repeats of the sampling
procedure.

complexity of generated samples.

One obvious application of the presented ap-
proach is improving the accuracy of code com-
tools assisting in programming by
pletion, i.e.
predicting the next tokens based on context (Svy-
atkovskiy et al., 2020a). The fact that ﬁne-tuning
using KL-DPG has a beneﬁcial effect on perplex-
ity and PEP8 error frequency suggests that it can
provide a training signal complementary to that in
a language modeling objective. The beneﬁts of this
auxilary training signal would arguably diminish
with increased training time and datatset size, but
that still leaves room for signiﬁcant improvement
in low-resource domains.

A limitation of the current KL-DPG approach
is that it is restricted to unconditional generation.
This is because for a conditional EBM P (x, c) the
proportionality constant −1/Z from (4) would de-
pend on a context c. Nevertheless, one can imag-
ine using a policy πθ ﬁne-tuned using KL-DPG as
initialization of a decoder for conditional genera-
tion, e.g.
transpilation (translation between pro-
gramming languages) or program synthesis (trans-
lation from a natural language to a programming
language).

Figure 4: Token frequency against token rank computed
for tokens found in samples from from KL-DPG, and two
baselines. Longer tails imply more diverse samples.

the applicability of obtained policies for code com-
pletion, in Tables 7-9 we present samples obtained
through conditional generation, i.e. x ∼ πθ(x|c),
where the context c is a function name. In either
case, samples were obtained using pure ancestral
sampling.

6 Discussion

References

In the paper, we presented a new energy-based
model formulation for the problem of imposing
the constraint of compilability on an autoregressive
generative model for source code. In contrast with
standard reinforcement learning approaches, the
solution we propose – KL-DPG – is able to improve
compilability rate without sacriﬁcing diversity and

Miltiadis Allamanis, Earl T. Barr, Premkumar T. De-
vanbu, and Charles Sutton. 2018. A survey of ma-
chine learning for big code and naturalness. ACM
Comput. Surv., 51(4):81:1–81:37.

Daniel Andor, Chris Alberti, David Weiss, Aliaksei
Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally Nor-
malized Transition-Based Neural Networks.

0.0%10%20%30%40%invalid syntax0.0%2%4%6%8%EOL while scanningstring literal0.0%1.0%2%3%4%unexpected EOF whileparsing0.0%0.5%1.0%1.5%2%duplicate argument0.0%0.5%1.0%1.5%2%unindent does notmatch any outer indentation level0.0%0.5%1.0%1.5%2%unexpected indent0.0%0.2%0.5%0.8%1.0%keyword argument repeated0.0%0.2%0.5%0.8%1.0%unexpected character afterline continuation character0.0%0.2%0.4%0.6%0.8%positional argument followskeyword argument0.0%0.2%0.4%0.6%EOF while scanningtriple-quoted string literal0.0%0.1%0.2%0.3%invalid character inidentifier0.0%0.1%0.2%0.3%invalid token0.0%0.1%0.2%0.3%positional argument followskeyword argument unpacking0.0%0.1%0.2%0.3%non-default argument followsdefault argumentaKL-DPGR(x)=b(x)0200040006000token rank100101102103token frequencyaKL-DPGR(x)=b(x)R(x)=P(x)Youri Arkesteijn, Nikhil Saldanha,

Kostense. 2020.
ral attention and byte pair encoding.
abs/2004.06343.

and Bastijn
Code completion using neu-
CoRR,

Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu,
Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron C.
Courville, and Yoshua Bengio. 2017. An actor-critic
In 5th Inter-
algorithm for sequence prediction.
national Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Con-
ference Track Proceedings. OpenReview.net.

A. Bakhtin, Y. Deng, S. Gross, Myle Ott, Marc’Aurelio
Ranzato, and Arthur Szlam. 2020. Energy-based
models for text. ArXiv, abs/2004.10188.

David Belanger and Andrew McCallum. 2016. Struc-
In Proceedings
tured prediction energy networks.
of the 33rd International Conference on Interna-
tional Conference on Machine Learning - Volume 48,
ICML’16, pages 983–992. JMLR.org.

Pavol Bielik, Veselin Raychev, and Martin Vechev.
2016. Phog: Probabilistic model for code. In Pro-
ceedings of the 33rd International Conference on In-
ternational Conference on Machine Learning - Vol-
ume 48, ICML’16, page 2933–2942. JMLR.org.

Matteo Ciniselli, Nathan Cooper, Luca Pascarella,
Denys Poshyvanyk, Massimiliano Di Penta, and
Gabriele Bavota. 2021. An empirical study on the
usage of BERT models for code completion. CoRR,
abs/2103.07115.

Imre Csisz´ar and Paul C. Shields. 2004. Information
theory and statistics: A tutorial. Commun. Inf. The-
ory, 1(4):417–528.

Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam,
and Marc’Aurelio Ranzato. 2020. Residual energy-
In 8th Inter-
based models for text generation.
national Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020. OpenReview.net.

Yilun Du and Igor Mordatch. 2019.

Implicit genera-
tion and modeling with energy based models.
In
Advances in Neural Information Processing Systems,
volume 32. Curran Associates, Inc.

Geoffrey E. Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural Com-
put., 14(8):1771–1800.

Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, Jose
Miguel Hernandez Lobato, Richard E. Turner, and
Doug Eck. 2017. Tuning recurrent neural networks
with reinforcement learning.

A. Karpathy, J. Johnson, and Li Fei-Fei. 2015. Visual-
izing and understanding recurrent networks. ArXiv,
abs/1506.02078.

Muhammad Khalifa, Hady Elsahar, and Marc Dymet-
man. 2021. A distributional approach to controlled
In International Conference on
text generation.
Learning Representations.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Vijay Konda and John Tsitsiklis. 2000. Actor-critic al-
In Advances in Neural Information Pro-

gorithms.
cessing Systems, volume 12. MIT Press.

Sumith Kulal, Panupong Pasupat, Kartik Chandra,
Mina Lee, Oded Padon, Alex Aiken, and Percy S
Liang. 2019. Spoc: Search-based pseudocode to
In Advances in Neural Information Process-
code.
ing Systems, volume 32. Curran Associates, Inc.

Yann LeCun,

Sumit Chopra,

Raia Hadsell,
Marc’Aurelio Ranzato, and Fu Jie Huang. 2006. A
In Predicting
Tutorial on Energy-Based Learning.
Structured Data. MIT Press.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016a. A diversity-promoting ob-
jective function for neural conversation models. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 110–119, San Diego, California. Association
for Computational Linguistics.

Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,
Michel Galley, and Jianfeng Gao. 2016b. Deep rein-
forcement learning for dialogue generation. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2016,
Austin, Texas, USA, November 1-4, 2016, pages
1192–1202. The Association for Computational Lin-
guistics.

Chang Liu, Xin Wang, Richard Shin, Joseph E Gonza-
lez, and Dawn Song. 2016a. Neural code comple-
tion.

Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama,
and Kevin Murphy. 2016b. Optimization of image
description metrics using policy gradient methods.
CoRR, abs/1612.00370.

Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey
Svyatkovskiy, Ambrosio Blanco, Colin B. Clement,
Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Li-
dong Zhou, Linjun Shou, Long Zhou, Michele Tu-
fano, Ming Gong, Ming Zhou, Nan Duan, Neel Sun-
daresan, Shao Kun Deng, Shengyu Fu, and Shujie
Liu. 2021. Codexglue: A machine learning bench-
mark dataset for code understanding and generation.
CoRR, abs/2102.04664.

Chris J. Maddison and Daniel Tarlow. 2014. Structured
In Pro-
generative models of natural source code.
ceedings of the 31st International Conference on In-
ternational Conference on Machine Learning - Vol-
ume 32, ICML’14, page II–649–II–657. JMLR.org.

Tung Thanh Nguyen, Anh Tuan Nguyen, Hoan Anh
Nguyen, and Tien N. Nguyen. 2013. A statistical
In Pro-
semantic language model for source code.
ceedings of the 2013 9th Joint Meeting on Foun-
dations of Software Engineering, ESEC/FSE 2013,
page 532–542, New York, NY, USA. Association for
Computing Machinery.

Art B. Owen. 2013. Importance Sampling. In Monte
Carlo theory, methods and examples, chapter 9.

Tetiana Parshakova, Jean-Marc Andreoli, and Marc
Dymetman. 2019a. Distributional Reinforcement
Learning For Energy-Based Sequential Models.
CoRR.

Tetiana Parshakova, Jean-Marc Andreoli, and Marc
Dymetman. 2019b. Global Autoregressive Models
for Data-Efﬁcient Sequence Learning. In Proceed-
ings of the 23rd Conference on Computational Nat-
ural Language Learning (CoNLL), pages 900–909,
Hong Kong, China. Association for Computational
Linguistics.

Ramakanth Pasunuru and Mohit Bansal. 2017. Re-
inforced video captioning with entailment rewards.
In Proceedings of
the 2017 Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP 2017, Copenhagen, Denmark, September 9-
11, 2017, pages 979–985. Association for Computa-
tional Linguistics.

Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Te-
jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,
Junjie Bai, and Soumith Chintala. 2019.
Py-
torch: An imperative style, high-performance deep
learning library.
In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar-
nett, editors, Advances in Neural Information Pro-
cessing Systems 32, pages 8024–8035. Curran Asso-
ciates, Inc.

Romain Paulus, Caiming Xiong, and Richard Socher.
2018. A deep reinforced model for abstractive sum-
In 6th International Conference on
marization.
Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference
Track Proceedings. OpenReview.net.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. OpenAI
Blog, 1(8):9.

Marc’Aurelio Ranzato, Y-Lan Boureau, Sumit Chopra,
and Yann LeCun. 2007. A uniﬁed energy-based
In Pro-
framework for unsupervised learning.
ceedings of the Eleventh International Conference
on Artiﬁcial Intelligence and Statistics, AISTATS
2007, San Juan, Puerto Rico, March 21-24, 2007,
volume 2 of JMLR Proceedings, pages 371–379.
JMLR.org.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2016. Sequence level train-
In 4th Inter-
ing with recurrent neural networks.
national Conference on Learning Representations,
ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,
Conference Track Proceedings.

Veselin Raychev, Pavol Bielik, and Martin Vechev.
2016. Probabilistic model for code with decision
trees. SIGPLAN Not., 51(10):731–747.

Veselin Raychev, Martin Vechev, and Eran Yahav. 2014.
Code completion with statistical language models.
SIGPLAN Not., 49(6):419–428.

Baptiste Roziere, Marie-Anne Lachaux, Lowik
Chanussot, and Guillaume Lample. 2020. Un-
supervised translation of programming languages.
Advances in Neural Information Processing Systems,
33.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1715–
1725, Berlin, Germany. Association for Computa-
tional Linguistics.

Richard S. Sutton, David McAllester, Satinder Singh,
and Yishay Mansour. 1999. Policy gradient methods
for reinforcement learning with function approxima-
tion. In Proceedings of the 12th International Con-
ference on Neural Information Processing Systems,
NIPS’99, page 1057–1063, Cambridge, MA, USA.
MIT Press.

Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu,
and Neel Sundaresan. 2020a. Intellicode compose:
In Proceed-
Code generation using transformer.
ings of the 28th ACM Joint Meeting on European
Software Engineering Conference and Symposium
on the Foundations of Software Engineering, ES-
EC/FSE 2020, page 1433–1443, New York, NY,
USA. Association for Computing Machinery.

Alexey Svyatkovskiy, Sebastian Lee, Anna Hadjitoﬁ,
Maik Riechert, Juliana Franco, and Miltiadis Alla-
manis. 2020b. Fast and memory-efﬁcient neural
code completion. CoRR, abs/2004.13651.

Pradyumna Tambwekar, Murtaza Dhuliawala, Lara J.
Martin, Animesh Mehta, Brent Harrison, and
Mark O. Riedl. 2019. Controllable neural story plot
In Proceedings of
generation via reward shaping.
the Twenty-Eighth International Joint Conference on
Artiﬁcial Intelligence, IJCAI 2019, Macao, China,
August 10-16, 2019, pages 5982–5988. ijcai.org.

Lifu Tu, Richard Yuanzhe Pang, Sam Wiseman, and
Kevin Gimpel. 2020. Engine: Energy-based infer-
ence networks for non-autoregressive machine trans-
lation. ArXiv, abs/2005.00850.

Ronald J. Williams. 1992a. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Mach. Learn., 8:229–256.

Ronald J. Williams. 1992b. Simple statistical gradient-
following algorithms for connectionist reinforce-
In Machine Learning, pages 229–
ment learning.
256.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, R´emi Louf, Morgan Funtow-
icz, and Jamie Brew. 2019. Huggingface’s trans-
formers: State-of-the-art natural language process-
ing. CoRR, abs/1910.03771.

Chunyang Xiao, Marc Dymetman, and Claire Gardent.
2016. Sequence-based structured prediction for se-
In Proceedings of the 54th An-
mantic parsing.
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1341–
1350, Berlin, Germany. Association for Computa-
tional Linguistics.

Victor Zhong, Caiming Xiong, and Richard Socher.
2017.
Seq2sql: Generating structured queries
from natural language using reinforcement learning.
arXiv preprint arXiv:1709.00103.

Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo,
Weinan Zhang, Jun Wang, and Yong Yu. 2018. Texy-
gen: A benchmarking platform for text generation
models. In The 41st International ACM SIGIR Con-
ference on Research & Development in Information
Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-
12, 2018, pages 1097–1100. ACM.

Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.
Brown, Alec Radford, Dario Amodei, Paul Chris-
tiano, and Geoffrey Irving. 2019. Fine-tuning lan-
guage models from human preferences. CoRR,
abs/1909.08593.

A Hyperparameters and implementation

details

We implemented all models using PyTorch (Paszke
et al., 2019) and HuggingFace (Wolf et al., 2019).
Training the initial generative model took 10 days
on 3 Nvidia Tesla T4 GPUs. For a detailed list of
hyperparameter values, see Table 1.

Hyperparameter

base LM
number of params
number of layers
number of heads
vocabulary size
sequence length
hidden state size
activation function
optimizer
initial learning rate
learning rate scheduler
batch size
total gradient updates
dropout rate

Value
gpt2-small
117m
12
12
50257
128
768
gelu
Adam (Kingma and Ba, 2014)
5 × 10−5
linear
24
20069
0.1

Table 1: Hyperparameters used for training the initial
generative model a

The implementation of KL-DPG was based on
code published by Khalifa et al. (2021).9 Each ﬁne-
tuning run took approximately 5 days on 2 Nvidia
V100 GPUs. For a detailed list of hyperparameter
values, see Table 2.

Hyperparameter

Value

optimizer
learning rate α(θ)
learning rate scheduler
batch size
warmup gradient updates
total gradient updates
sequence length
dropout rate

Adam (Kingma and Ba, 2014)
1.41 × 10−6
linear
2048
100
250
128
0.1

Table 2: Hyperparameters used for training πθ using
KL-DPG and Reinforce

9https://github.com/naver/gdc

b(x)

Program

def test_3_invalid(self):

0

0

1

0

1

0

1

0

1

1

0

1

1

1

serializer = serializer.validated_manager['quarterly_ cred']
serializer.user = 'token'
self.verify_token(epsg = serializer.DBModes,[serializer.user])

def delete(self,username,password = None):

if username:

if username.startswith("oil",None)or username.startswith('"",True):

raise HttpRequest()

db.model.delete.assert_called_with(username,'password')

def mode(self):

self._mode = 'modeM_GB'
return self

def _update_update_tbl(self,new_worksheet):

self._merge_tbl(new_worksheet,old_worksheet)
self._create_where('x1')
self._update_tbl('x1',{ }).extend([str(new_fh.getvalue()))
self._clear_sql()
self.clear_lstrip()
self.pop.set('x1')[int(col)for param in['x1','y1']]
self.flush.update()

def _callResourceCost(self,server):

response = urllib.Request('GET','//api//log//%s//detected//' % server.id)
body = urllib. urllib2.urlencode(body)
response.headers['X-Basic-Control-Authorization']= self.oauth_client.Client.CertResponse(response.body)
return response

def _pre_save(self,data):

self.calculate_updates([item.resolve(data['output')]= yield
data['output'].find('top',['mybounce','geodeIB'])))
(cid:44)→

def read(self):

self.offset -= 1
start = O8(self)
while time.time()- start:

return self.get_index(start)

def Pub(self):

r = PCHAP()
r['where']= struct.unpack('!T',self.digest))
response = MKchronosOPS('R')
self.sendMessage(response)
return self.Response(response)

def __init__(self,current_node):

self.current_node = current_loadbalancer
self.assign_current_node = None
self.parenting = None
if self.menu:

self.getNodeSelector(Index(RemovelineToRow,self.parent.position),0,2.0,5.0)

self.show_parent()

def get_response_data(self):

return {
(cid:44)→
(cid:44)→

'from_blob_client':self.to_blob_key,'as_blob_secret':self.to_project_secret.to_secret(),'json':self.to_storage
}

def put(self,key,expire = True):

if not invert:
dict = { }
dict.update(key,self.__TestStepities[key])

self.cs.put(self._uZED_ATTRIBUTES_ =[("sequential_command","duration",key,expire)]= "//?modified:%r" %
(cid:44)→

key,queue_text = self.__kneeators["expires"])

def testPath(self):

t = Gaffer.Reader(self.callback)
dupe = ""
f.mkdir(t)
f = sys.stdout.tell()
f.write('_')
self.assertEqual(f,dataponCollision)

def get_count(self):

return self.get_implicit_count()

def is_alive(self):

return(self.pid,)and(self.pid == 400)

Table 3: Sequences sampled from the original generative model a

b(x)

Program

def fetch_size(self,page):

0

0

1

1

1

0

0

1

1

0

0

0

1

1

response = self.fetch(page,max((2))
constant(response.json(),response.pop('utf-8'))
payload = "%s//%s//%s//%s//%s" %(self.resource.id,page.format_from_bytes())
return payload

def setUp(self):

self.project_loader = testutil.FileSentenceDependencyGraph(extensions =['file','path'])
self.schema =RelatedPackage preserveLoader(root_loader)
self.extension_context = XMLLoader()

def __getattr__(self,perm):

return self._memo.get(perm)

def expand(self,text):
value.strip()
return extract_cseq(text)

def test_Obze(self):
w = Command()
self.assertEqual(w.callHeader.callHeader,self.result)

def start_stream(self,addressFamily,opcode):

logger.info("OpenlibwriteStructBegin chunkon.csv',OperationalError())
error_message = self.get_stream([None,None])
message,message = self.block_messages[0]
message = message[0]
self._process_message(message,message,message,message)

def set_dense(self,srs,fit_to):
if dup in self.scalar:

return

if not isinstance(modality,(pyobj):

self.sq =SUBNET

self.basic = asim.bin.sample(srs,rng = self.ctypes,trials = self.rng,dtype = self.dtype)

def _act(self,value):

self._result.set_argument('value',value)

def _verify_ssling_access_admin(self,ip_name):

self._check_proxy(ip_name)

def __str__(self):

r =[]
for s in self.__dict__.items():

if s[0]in BoundCacheContents():

break

if s[:- 1]:Elements([("Unsupported Ct%s]" % ','.join(self.__class__.__name__))
return "Data attribute '%s' % ','.join("%sCHOICES from %s" %(WARNING,str(r)))

def test_FaceIP_3D_14(self):

self.assertTrue(self.doTestFace(self.doTestFace([self.doTestFace([False,False)])

def __init__(self,** options):

super(_ChoiceTest,self).__init__(** options)
self.action_classes = options["cells_store"]
self.choices =(1.2,** options["mysql"]= FakeMissingTuple())
self.parser = Message(list.__init__(option_forms))

def main(self,client):

remove_home_config(client,"client_snapshot_url")
self.client.client_snapshot.update(client)

def _stop_signal(self,emitter,datafile,for_attachment):

vim.gui.target_cancel()

Table 4: Sequences sampled from a policy ﬁne-tuned using KL-DPG

b(x)

Program

1

1

1

1

1

1

1

1

1

1

1

1

1

1

def invalidateKey(self):

self.action.rooms = { }

def get(self):

return self.handler.identifier

def flush(self):

self.write("ready")

def get_flavor(self,resource,path,** metadata):

return self.context.get(resource,path,** metadata)

def test_api_set_to_result(self):

X = T.ListHead()
self.assertEquals(quantiles(X),self._cache.annotations)

def is_cmp(self,other):

return not self._safe_eq(other,self.link)

def __iter__(self):

return iter(self._reverse())

def cancel(self):

return self.enhanced_window.set_timeout()

def __str__(self):

return str(self.repository)

def summary(self):

return self._series

def Lazypeer(self):

return self._peer

def ByteSize(self):

n = 0
n += self.lengthString(len(self.parameters_))
return n + self.lengthString(number(self.value_))

def setUp(self):

super(TestMaUserRoleTestCase,self).setUp()
self.core =BER()
self.topsetup_existing = False

def __init__(self,** kwargs):

self.sourcemersListComp = kwargs.get('stretch {}'.format(self.__class__.twsourceCentOS_text))

Table 5: Sequences sampled from a policy ﬁne-tuned using Reinforce with R(x) = b(x)

b(x)

Program

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

def set_OwnerId(self,OwnerId):

self.add_query_param('OwnerId',OwnerId)

def set_OwnerId(self,OwnerId):

self.add_query_param('OwnerId',OwnerId)

def set_OwnerId(self,OwnerId):

self.add_query_param('OwnerId',OwnerId)

def set_OwnerId(self,OwnerId):

self.add_query_param('OwnerId',OwnerId)

def set_OwnerId(self,OwnerId):

self.add_query_param('OwnerId',OwnerId)

def set_OwnerId(self,OwnerId):

self.add_query_param('OwnerId',OwnerId)

def set_OwnerId(self,OwnerId):

self.add_query_param('OwnerId',OwnerId)

def set_OwnerId(self,OwnerId):

self.add_query_param('OwnerId',OwnerId)

def set_OwnerId(self,OwnerId):

self.add_query_param('OwnerId',OwnerId)

def set_OwnerId(self,OwnerId):

self.add_query_param('OwnerId',OwnerId)

def set_OwnerId(self,OwnerId):

self.add_query_param('OwnerId',OwnerId)

def set_OwnerId(self,OwnerId):

self.add_query_param('OwnerId',OwnerId)

def set_OwnerId(self,OwnerId):

self.add_query_param('OwnerId',OwnerId)

def set_OwnerId(self,OwnerId):

self.add_query_param('OwnerId',OwnerId)

def set_OwnerId(self,OwnerId):

self.add_query_param('OwnerId',OwnerId)

def set_OwnerId(self,OwnerId):

self.add_query_param('OwnerId',OwnerId)

Table 6: Sequences sampled from a policy ﬁnetuned using Reinforce with R(x) = P (x)

b(x)

Program

Sequences sampled from the original generative model a

1

1

1

1

1

1

1

1

1

1

1

1

def closeEvent(self):

self._isalive = False
self._original_resume = True

def close_file(self):

pass

def closeWorking(self):

pass

def close(self):

if not self.closed:

self.closed = True

self.translation.close()

Sequences sampled from a policy ﬁne-tuned using KL-DPG

def close(self):

self.queue.Importer.close(self.info)
self.open_input.close()
self.graph.close(self.gamma)

def close(self):

try:

self.srv.get_browser.mac(self.bus_process.name,vm_output = True)

except suspended as ex:

self.socket.stop(ex)

Sequences sampled from a policy ﬁne-tuned using Reinforce with R(x) = b(x)

def close(self):

self._stdout.close()

def close(self):

self.idb.close()

def close(self):

self.reuse = subprocess.Popen('CONNECTION','').unregisterProducer()
p = subprocess.Popen()
p.communicate().close()
return u.close()

Sequences sampled from a policy ﬁne-tuned using Reinforce with R(x) = P (x)

def close(self,object):

self.api.close(self.uid.length)

def close(self):

self.job_closed.remove(self)

def close(self):

self.buffer.flush()

Table 7: Samples obtained from policies conditioned on prompt def close

b(x)

Program

0

0

1

1

0

1

1

1

0

0

1

1

Sequences sampled from the original generative model a

def fit_pdf(self,hop,theta,theta):

asserttriangular is self._fit_rewrite(hop, kernel,theta,theta)- gtheta,70)
assertworkspace isTType.ACCEPTED_ignore
assert subset in(coeff,Y)
assert self._Xfd != xOpenStackBackendError
assert isinstance(750,Win,T,Vector)

def fit(self,X,y):

self._ y = y
self._children -= 1
assert isinstance(self._labels,_MOD_'")
x[:]= 0
y[:]=Bio_OFFSET
y *= self._labels
y * y * y
y //= y
return y

def fit(self,X = None,y = None,result = None):

sts = self.get_appId(self.mesh_filename,X,y = y,d = result)
self.mirror_logpdf([0x9]* indented)

Sequences sampled from a policy ﬁne-tuned using KL-DPG

def fit(self,X,y,* args,** kwargs):

X = self.transform(X,y,* args,** kwargs)
data = np.DataFrame(data)
for i in self.fallback_array.iteration_two(* data):

data[i].labels[i].tolist()

return data

def fit(self, initial_output = None):

if initial_output:

self.force_input = False

else:

self.cells_done = tuple(initial_output)

if initial_input == self.WK_MASK:

self.output_output += self.osfstorage_NORMAL
self.outputs = list([self.inputState.NORMAL_READ valid])

return 1

def fit(self,reshape,a,b):

return frappe. filediff(islice(a,b),b)

Sequences sampled from a policy ﬁne-tuned using Reinforce with R(x) = b(x)

def fit(self,X,y):
self.x = y

def fit(self,fit,d):

self.fit =followers
return super(PositionUntilLockedSequence,self).fit(marks)

def fit(self,X_acc):

X_exog = self.xc1.exog
y = self.instance.exog
y,= self.model.w2 preserve_uniform(os.environ.XMANllf,y_y))
y += self.model.t2le continX
y = self.transition.fit(y)
y.y = self.model.y * y
y.red = self.model.gw.urmpopow(y)
return y

def fit(self,fit,X,y,z):

Sequences sampled from a policy ﬁne-tuned using Reinforce with R(x) = P (x)

self.learning = indices[np.zeros(axis = 1Dot,y = y,motion = self. np.loss,y = res.scale)]
self.index = y

def fit(self,params):

self.params_param = params

def fit(self,X,y = None):

self.x = x
self.y = x

Table 8: Samples obtained from policies conditioned on prompt def fit

b(x)

Program

0

1

0

1

0

0

1

1

0

0

1

0

def generate_samples_with_prompt(self,input_value,decimal = False):

Sequences sampled from the original generative model a

use_full = False
full_input_string = escape_input[decimal]
newprefix = local_input_format.split("<%s__") % input_label.strip(),[formatted_full])
return newprefix

def generate_samples_with_prompt_publish(self):

self.overflow = self.percent

def generate_samples_with_prompt_line(self):

lines =[]
for line in rc:

if line.startswith('_','-'):

lines.append("{}0`%s))" % line.replace(".","\n")
lines.append(": ".join(lines))

lines.appenddsets()
lines.append_):
if len(lines)> 0:

lines.append(lines[0])

return lines

def generate_samples_with_prompt(self):

Sequences sampled from a policy ﬁne-tuned using KL-DPG

result = self._generate_blobs().generate(self._name,self._amount_in,lambda x:x.lower())
return result

def
(cid:44)→

generate_samples_with_prompt_token(self,impdly,red,name,restdeclarations,restid_with_mucmapreduce_access_reference,tpversion):
if prefix_to_acked_ level_per_pbfrom_account_version(MACRO256):

return 71212000x00 * c201402E64D + 204

self.generate_cant_rgb_signature(FLAG,name,comtop header, "0|02",["-20001500e6fsha"]

def generate_samples_with_prompt(self):

tsMAIN_SIZE =(0,1)
tsSBream_bin = self.1000
if if tsody_size is not None:

tsbleations = y

size = ts86.data.get_input(vid_ docs).get_language()
for address in data.SerializeToString()if not region:

cpu_ratio = np.zeros(freq.encode("Now"))
tsLOCATION_examples =[self.read_format(addr)for dir in tsningAssignmentInt()])

def generate_samples_with_prompt(self):

pass

Sequences sampled from a policy ﬁne-tuned using Reinforce with R(x) = b(x)

def generate_samples_with_prompt_indices(self):

return self.raw_results_with.raw_options.random_encoding

def generate_samples_with_prompt(self,* args,** kwargs):

return self.fit_sum(kwargs -(n))):

def generate_samples_with_prompt(self,cached_done,keep = False):

if not hasattr(upstream_show,'normalize'):

Sequences sampled from a policy ﬁne-tuned using Reinforce with R(x) = P (x)

return

sm =wb. cppProcessor(cached_TLS = False)
self.maxOccurs = self.concurrency. anno_DealList()
tool.is(csrf_restore,lazy = True)
self.salt_made(csrf)

def generate_samples_with_prompt(self):

start = back_start - self.start + self.test_samples().set_ofmid
result =[]
for step in range(start):

result.append(step)
result.append(step)

return result

def generate_samples_with_prompt(self,type::phone_shard = None):

return int(int(self.last_offsets_best_timescale,type_op = "0"))

Table 9: Samples obtained from policies conditioned on prompt def generate samples with prompt

