1
2
0
2

v
o
N
2
2

]

G
L
.
s
c
[

1
v
8
5
3
1
1
.
1
1
1
2
:
v
i
X
r
a

A Surrogate Objective Framework for
Prediction+Optimization with Soft Constraints

Kai Yan∗
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
kaiyan3@illinois.edu

Jie Yan
Microsoft Research
Beijing, China
jiey@microsoft.com

Chuan Luo
Microsoft Research
Beijing, china
chuan.luo@microsoft.com

Liting Chen∗
Microsoft Research
Beijing, China
98chenliting@gmail.com

Qingwei Lin†
Microsoft Research
Beijing, China
qlin@microsoft.com

Dongmei Zhang
Microsoft Research
Beijing, China
dongmeiz@microsoft.com

Abstract

Prediction+optimization is a common real-world paradigm where we have to pre-
dict problem parameters before solving the optimization problem. However, the
criteria by which the prediction model is trained are often inconsistent with the
goal of the downstream optimization problem. Recently, decision-focused pre-
diction approaches, such as SPO+ and direct optimization, have been proposed
to ﬁll this gap. However, they cannot directly handle the soft constraints with
the max operator required in many real-world objectives. This paper proposes
a novel analytically differentiable surrogate objective framework for real-world
linear and semi-deﬁnite negative quadratic programming problems with soft linear
and non-negative hard constraints. This framework gives the theoretical bounds
on constraints’ multipliers, and derives the closed-form solution with respect to
predictive parameters and thus gradients for any variable in the problem. We
evaluate our method in three applications extended with soft constraints: synthetic
linear programming, portfolio optimization, and resource provisioning, demon-
strating that our method outperforms traditional two-staged methods and other
decision-focused approaches.

1

Introduction

Mathematical optimization (a.k.a. mathematical programming), e.g., linear and quadratic program-
ming, has been widely applied in decision-making processes, such as resource scheduling [1], goods
production planning [2], portfolio optimization [3], and power scheduling [4]. In practice, problem
parameters (e.g., goods demands, and equity returns) are often contextual and predicted by models
with observed features (e.g., history time series). With the popularity of machine learning techniques
and increasing available data, prediction+optimization has become a normal paradigm [5].

∗Contributed during internship at Microsoft Research. † Corresponding Author.

35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.

 
 
 
 
 
 
Prediction becomes critical to the performance of the full prediction+optimization workﬂow since
modern optimization solvers (e.g., Gurobi [6] and CPLEX [7]) can already efﬁciently ﬁnd optimal
solutions for most large scale optimization problems. Traditionally, prediction is treated separately as
a general supervised learning problem and learned through minimizing a generic loss function (e.g.,
mean squared error for regression). However, studies have shown that minimization of the ﬁtting
errors does not necessarily lead to better ﬁnal decision performance [8, 9, 10, 11].

Recently, a lot of efforts on using optimization objective to guide the learning of prediction models
have been made, which are decision-focused when training prediction models instead of using
traditional prediction metrics, e.g. mean squared error losses. For linear objectives, the ‘Smart
Predict then Optimize’ ([10]) proposes the SPO+ loss function to measure the prediction errors
against optimization objectives, while direct optimization ([12]) updates the prediction model’s
parameters by perturbation. For quadratic objectives, OptNet [13, 14] implements the optimization
as an implicit layer whose gradients can be computed by differentiating the KKT conditions and then
back propagates to prediction neural network. CVXPY [15] uses similar technologies with OptNet
but extends to more general cases of convex optimization. However, all the above state-of-the-art
approaches do not contain the soft constraints in their objectives. In this paper, We consider linear
‘soft-constraints’, a penalty in the form of max(z, 0), where z = Cx − d is a projection of decision
variable x ∈ Rn and context variables C ∈ Rm×n, d ∈ Rm. Such soft constraints are often required
in practice: for example, they could be the waste of provisioned resources over demands or the extra
tax paid when violating regulatory rules. Unfortunately, the max(·, 0) operator is not differentiable
and thus cannot be directly handled by these existing approaches. To differentiate soft constraints is a
primary motivation of this paper.

In this paper, we derive a surrogate objective framework for a broad set of real-world linear and
quadratic programming problems with linear soft constraints and implement decision-focused dif-
ferentiable predictions, with the assumption of non-negativity of hard constraint parameters. The
framework consists of three steps: 1) rewriting all hard constraints into piece-wise linear soft
constraints with a bounded penalty multiplier; 2) using a differentiable element-wise surrogate to
substitute the piece-wise objective, and solving the original function numerically to decide which
segment the optimal point is on; 3) analytically solving the local surrogate and obtaining the gradient;
the gradient is identical to that of the piecewise surrogate since the surrogate is convex/concave such
that the optimal point is unique.

Our main contributions are summarized as follows. First, we propose a differentiable surrogate objec-
tive function that incorporates both soft and hard constraints. As the foundation of our methodology,
in Section 3, we prove that, with reasonable assumptions generally satisﬁed in the real world, for
linear and semi-deﬁnite negative quadratic programming problems, the constraints can be transformed
into soft constraints; then we propose an analytically differentiable surrogate function for the soft
constraints max(·, 0). Second, we present the derived analytical and closed-form solutions for three
representative optimization problems extended with soft constraints in Section 4 – linear programming
with soft constraints, quadratic programming with soft constraints, and asymmetric soft constraint
minimization. Unlike KKT-based differentiation methods, our method makes the calculation of
gradients straightforward for predicting context parameters in any part of the problem. Finally, we
apply with theoretical derivations and evaluate our approach in three scenarios, including synthetic
linear programming, portfolio optimization, and resource provisioning in Section 4, empirically
demonstrate that our method outperforms two-stage and other predict+optimization approaches.

2 Preliminaries

2.1 Real-world Optimization Problems with Soft Constraints

Our target is to solve the broad set of real-world mathematical optimization problems extended with
a max(z, 0) term in their objectives where z depends on decision variables and predicted context
parameters. In practice, max(z, 0) is very common; for example, it may model overhead of under-
provisioning, over-provisioning of goods, and penalty of soft regulation violations in investment
portfolios. We call the above max(z, 0) term in an objective as soft constraints, where z ≤ 0 is
allowed to violate as long as the objective improves.

2

The general problem formulation is

g(θ, x) − αT max(z, 0), z = Cx − d, s.t. Ax ≤ b, Bx = c, x ≥ 0

(1)

max
x

where g is a utility function, x ∈ Rn the decision variable, and θ ∈ Rn the predicted parameters.

Based on observations on a broad set of practical problem settings, we impose two assumptions on
the formulation, which serves as the basis of following derivations in this paper. First, we assume
A ∈ Rm1×n ≥ 0, b ∈ Rm1 ≥ 0, B ∈ Rm2×n ≥ 0 ,and c ∈ Rm2 ≥ 0 hold. This is because for
problems with constraint on weights, quantities or their thresholds, these parameters are naturally
non-negative. Second, we assume the linearity of soft constraints, that is z = Cx − d, where
z ∈ Rm3 , C ∈ Rm3×n, and d ∈ Rm3. This form of soft constraints make sense in wide application
situations when describing the penalty of goods under-provisioning or over-provisioning, a vanish in
marginal proﬁts, or running out of current materials.

Now we look into three representative instances of Eq.1, extracted from real-world applications.
Linear programming with soft constraints, where g(x, θ) = θT x. The problem formulation is

θT x − αT max(Cx − d, 0), s.t. Ax ≤ b, Bx = c, x ≥ 0

(2)

max
x

where α ≥ 0. Consider the application of logistics where θ represents goods’ prices, {A, B, C} are
the capability of transportation tools, and {b, c} are the thresholds. Obviously, A, b, B, c ≥ 0 hold.
Quadratic programming with soft constraints, where g(θ, x) = θT x − xT Qx. One example is
the classic minimum variance portfolio problem [16] with semi-deﬁnite positive covariance matrix Q
and expected return θ to be predicted. We extend it with soft constraints which, for example, may
represent regulations on portions of equities for some fund types. Formally with α ≥ 0, we have:

θT x − xT Qx − αT max(Cx − d, 0), s.t. Bx = c, x ≥ 0.

max
x

(3)

Optimization of asymmetric soft constraints. This set of optimization problems have the objective
to match some expected quantities by penalizing excess and deﬁciency with probably different
weights. Such formulation represents widespread resource provisioning problems, e.g., power[4] and
cloud resources[1], where we minimize the cost of under-provisioning and over-provisioning against
demands. Formally with α1, α2 > 0, we have:
1 max(Cx − d, 0) + αT

2 max(d − Cx, 0)), s.t. Bx = c, x ≥ 0.

−(αT

(4)

max
x

In this paper, we consider a challenging task where C is a matrix to be predicted with known constants
d. In reality, the Cx − d term may represent the "wasted" part when satisfying the actual need of d.

2.2 Prediction+Optimization

For compactness we write Eq.1 as maxx∈X f (x, θ), where f is the objective function and X is the
feasible domain. The solver for f is to solve x∗ = argmaxx∈X f (x, θ). With parameters θ known,
Eq.2–4 can be solved by mature solvers like Gurobi [6] and CPLEX [7].

In prediction+optimization, θ is unknown and needs to be predicted from some observed features
ξ ∈ Ξ. The prediction model is trained on a dataset D = {(ξi, θi)}N
i=1. In this paper, we consider the
prediction model, namely Φ, as a neural network parameterized with ψ. In traditional supervised
learning, Φψ is learned by minimizing a generic loss, e.g., L1 (Mean Absolute Error) or L2 (Mean
Squared Error), which measures the expected distance between predictions and real values. However,
such loss minimization is often inconsistent with the optimization objective f , especially when the
prediction model is biased [11].

Instead, decision-focused learning directly optimizes ψ with respect to the optimization objective
f , that is, maxψ E(ξ,θ)∼D[f ( ˆx∗(Φψ(ξ)), θ)]. The full computational ﬂow is illustrated in Fig.1. In
∂ ˆx∗ , where utility r = f ( ˆx∗, θ).
the gradient-based learning, update of ψ’s gradient is ∂r
The Jacobian ∂ ˆθ/∂ψ is computed implicitly by auto-differentiation of deep learning frameworks (e.g,
PyTorch [17]), and ∂r/∂ ˆx∗ is analytical. The main challenge is to compute ∂ ˆx∗/∂ ˆθ, which depends
on differentiating the argmax operation. One recent approach is to rewrite the objective to be convex
(by adding a quadratic regularizer if necessary), build and differentiate the optimality conditions (e.g.,

∂ψ = ∂ ˆθ

∂ ˆx∗
∂ ˆθ

∂ψ

∂r

3

Figure 1: Computation graph of the decision-focused prediction methods.

KKT conditions) [18] which map ˆθ to the solution ˆx∗, and then apply implicit function theorem to
obtain ∂ ˆx∗/∂ ˆθ. Alternatively, in this paper we propose a novel approach that rewrites the problem
as an unconstrained problem with soft constraints and derives analytical solutions, based on our
observations on the real-world problem structures and coefﬁcient properties.

3 Methodology

Our main idea is to derive a surrogate function for f with a closed-form solution such that the
Jacobian ∂x
∂θ is analytical, making the computation of gradient straightforward. Unlike other recent
work [13, 14, 15, 3], our method does not need to solve KKT optimality condition system. Instead,
by adding reasonable costs for infeasibility, we convert the constrained problem into an unconstrained
one. With the assumption of concavity, we prove that there exist constant vectors β1, β2, β3, such
that Eq.1 can be equivalently transformed into an unconstrained problem:

max
x

L(x) = max

x

g(x, θ) − αT max(Cx − d, 0) − βT

1 max(Ax − b, 0) − βT

2 |Bx − c| − βT

3 max(−x, 0) (5)

The structure of this section is as follows. Section 3.1 proves that the three types of hard constraints
can be softened by deriving bounds of β1, β2, β3; for this paper, we will assign each entry of the
three vectors the equal value (with a slight abuse of notation, we denote β1 = β2 = β3 = β for the
proofs of bounds; we align with the worst bound applicable to the problem formulation.) Section
3.2 proposes a novel surrogate function of max(·, 0), such that the analytical form of ∂x
∂θ can be
easily derived via techniques of implicit differentiation [18] and matrix differential calculus [19] on
equations derived by convexity [20]. Based on such derived ∂x
∂θ , we develop our end-to-end learning
algorithm of prediction+optimization whose detailed procedure is described in Appendix C.

3.1 Softening the Hard Constraints

For any hard constraints w = Ax − b ≤ 0, we denote its equivalent soft constraints as H(w) =
βT max(w, 0). H(w) should satisfy two conditions: 1) for w ≤ 0 (feasible x), H(w) = 0; 2) for
w ≥ 0 (infeasible x), H(w) is larger than the utility gain(i.e., improvement of the objective value)
R = f (x, θ) − maxx1:Ax1≤b f (x1, θ) by violating Ax − b ≤ 0. Intuitively, the second condition
requires a sufﬁciently large-valued β > 0 to ensure that the optimization on the unconstrained
surrogate objective never violates the original Ax ≤ b; to make this possible, we assume that the
l2-norm of the derivative of the objective f before conversion is bounded by constant E. The
difﬁculty of requirement 2) is that the distance of a point to the convex hull l is not bounded by the
sum of distances between the point and each hyper-plane in general cases, so the utility gain obtained
from violating constraints is unbounded. Fig. 2-(a) shows such an example which features the small
angle between hyper-planes of the activated cone. We will refer such kind of ’unbounding’ as "acute
angles" below.

The main effort of this subsection is to analyze and bound the effect caused by such acute angles.
Given a convex hull C = {z ∈ Rn|Az ≤ b} (A ≥ 0 is not required here) and any point x (cid:54)∈ C, let
x0 ∈ C be the nearest point of C to x, and A(cid:48)x ≥ b(cid:48) represent all active constraints at x, then all such
active constraints must pass through x0.2 The domain K = {z ∈ Rn|A(cid:48)z ≥ b(cid:48)} is a cone or degraded

2Otherwise, there must exist an active constraint i and a small neighbourhood of x0, say B(x0, (cid:15)), such that

∀z ∈ B(x0, (cid:15)), Aiz − bi (cid:54)= 0, which implies that either constraint i is inactive (< 0) or x0 (cid:54)∈ C (> 0).

4

(a) 2D acute angle

(b) 3D acute angle

(c) Non-degraded cone

(d) Degraded cone

Figure 2: (a) and (b) are 2 and 3-dimensional "acute angles"; (c) and (d) shows two corresponding
activated cones for given acute angles. The green area is the feasible region, x is the red point and x0
is the yellow point; the red θ is the derivative of an objective g(x, θ) = θT x.

cone where the tip of the cone is a subspace of Rn. For the rest of the paper, we will call K activated
cone, as shown in Fig. 2. Note that for any degraded activated cone, x − x0 is always perpendicular
to the tip subspace; therefore, we may project the cone onto the complementary space of the tip and
get the same multiplier bound on the projected activated cone with lower dimensions.
Ideally, we aim to eliminate the utility gain R obtained from violating A(cid:48)x ≤ b(cid:48) with the penalty
βT (A(cid:48)x − b(cid:48)), i.e., ensure βT (A(cid:48)x − b(cid:48)) ≥ R hold for any x. For the compactness of symbols and
simplicity, we will assume that the main objective is g(x, θ) = θT x in this section; however note that
our proofs apply with the existence of soft constraints and quadratic terms in g, which is discussed at
the beginning of Appendix A. With such assumption, we now give a crucial lemma, which is the core
of our proof for most of our theorem:
Lemma 1. (Bounding the utility gain) Let R = f (x, θ) − maxx1∈C f (x1, θ) be the utility gain,
(cid:80)n
then R ≤ f (x, θ) − f (x0, θ) ≤ E
i), where x is an infeasible point, A(cid:48)x ≤
i=1(A(cid:48)
ix − b(cid:48)
∗ and θ∗ are the optimal solution of
∗, θ∗) where A(cid:48)
b(cid:48) the active constraints at x, p0 = ∠(A(cid:48)
i
i
i, θ) (i.e., the maximin angle between θ and any hyperplane of the activated
maxθ minA(cid:48)
cone K = {z ∈ Rn|z − x0 ∈ cone(A(cid:48))}), and x0 the projection of x to the tip of cone C = {z ∈
Rn|A(cid:48)z ≤ b(cid:48)}. E is the upper bound of ||θ||2. ∠(·, ·) denotes the angle of two vectors.

cos∠(A(cid:48)

cos p0

i

cos p0

Thus, E
1 is a feasible choice of β, and it sufﬁces by ﬁnding the lower bound for cos p0. For the
rest of the section, we ﬁnd the lower bound of cos p0 by exploiting the assumed properties of A(cid:48), e.g.,
A(cid:48) ≥ 0; we give the explanation of the full proofs for all theoretical results in Appendix A.

3.1.1 Conversion of Inequality Constraints Ax ≤ b

Let us ﬁrst consider the constraints w = Ax − b ≤ 0, where A ≥ 0, b ≥ 0. It is easy to prove
that given A ≥ 0 and b ≥ 0, the distance of a point to the convex hull Ax ≤ b is bounded. More
rigorously, we have the following theorem, which guarantees the feasibility of softening the hard
constraints of inequalities Ax ≤ b:
Theorem 2. Assume the optimization objective θT x with constraints Ax ≤ b, where A ≥ 0, and
b ≥ 0. Then, the utility gain R obtained from violating Ax − b ≤ 0 has an upper bound of
O((cid:80)

i max(wi, 0)E), where w = A(cid:48)x − b(cid:48), and A(cid:48)x ≤ b(cid:48) is the active constraints.

3.1.2 Conversion of Inequality Constraints x ≥ 0

With inequality constraints Ax ≤ b converted, we now further enforce x ≥ 0 into soft constraints. It
seems that the constraint of this type may form a very small angle to those in w = Ax − b. However,
as −x is aligned with the axes, we can augment x ≥ 0 into soft constraints by proving the following
theorem:

Theorem 3. When there is at least one entry of x ≥ 0 in the activated cone, the utility gain R from
deviating the feasible region is bounded by O( n1.5E (cid:80)
), where p is the smallest angle
between axes and other constraint hyper-planes and w is the union of Ax − b and −x.

i max(wi,0)
sin p

Hence, we can set β = O( n1.5E

sin p ). Specially, for binary constraints we may set β = O(n1.5E):

5

𝑥!𝑥"𝜃𝜃𝑥!𝑥𝑥!𝑥Corollary 4. For binary constraints where the entries of A are either 0 or 1, the utility gain R of
violating x ≥ 0 constraint is bounded by O(n1.5E (cid:80)

i max(wi, 0)), where wi = Aix − bi or −x.

which gives a better bound for a set of unweighted item selection problem (e.g. select at most k items
from a particular subset).

3.1.3 Conversion of Equality Constraints Bx = c

Finally, we convert Bx = c into soft constraints. This is particularly difﬁcult, as Bx = c implies
both Bx ≤ c and −Bx ≤ −c, which will almost always cause acute angles. Let’s ﬁrst consider a
special case where there is only one equality constraints and A is an element matrix I n×n.
Theorem 5. If there is only one equality constraint Bx = c (i.e., B degrades as a row vector, such
like (cid:80)
i xi = 1) and special inequality constraints x ≥ 0, Ix ≤ b, then the utility gain R from
violating constraints is bounded by O( n1.5E (cid:80)
), where p is the same with theorem 3, w is
the union of Bx − c and −x.

i max(wi,0)
sin p

Intuitively, when there is only one equality constraint, the equality constraint can be viewed as an
unequal one, for at most one side of the constraint can be in a non-degraded activated cone. Thus, we
can directly apply the proof of Theorem 2 and 3, deriving the same bound O( n1.5E

sin p ) for β.

Finally, we give bounds for general Ax ≤ b, Bx = c with A, b, B, c ≥ 0 as below:
Theorem 6. Given constraints Ax ≤ b, x ≥ 0, and Bx = c, where A, B, b, c ≥ 0, the utility gain R
obtained from violating constraints is bounded by O(
i max(wi, 0)), where λmax is the
upper bound for eigenvalues of P T P (P : x → P x is an orthogonal transformation for an n-sized
subset of normalized row vectors in A, B and −I), and w is the union of all active constraints from
Ax ≤ b, x ≥ 0, Bx ≤ c and −Bx ≤ −c.

nλmax

(cid:80)

√

n

In this theorem, P is generated by taking an arbitrary n-sized subset from the union of row vectors
in A, B and −I, orthogonizing the subset, and using the orthogonal transformation matrix as P ;
there are (cid:0)n+m1+m2
(cid:1) different cases of P , and λmax is the upper bound of eigenvalues of P T P
over all possible cases of P . Note that there are no direct bounds on λmax with respect to n and the
angles between hyper-planes. However, empirical results (see Appendix A for details) show that for a
diverse set of synthetic data distributions, λmax = O(n2) follows. Therefore, empirically we can use
a bound O( n2.5E
sin p ) for β.3 So far, we have proven that all hard constraints can be transformed into
soft constraints with bounded multipliers. For compactness, Eq.5 is rewritten in a uniﬁed form:

L(x) = g(x, θ) − γT · max(C (cid:48)x − d(cid:48)), where γ =

(cid:20)

(cid:21)
α
, C (cid:48) =
O( n2.5E
sin p )1


, d(cid:48) =







C
A
−B
B
−I











d
b
−c
c
0

(6)

3.2 The Unconstrained Soft Constraints

As Eq.5 is non-differentiable for the max operator, we need to seek a relaxing surrogate for differ-
entiation. The most apparent choice of the derivative of such surrogate S(z) for z = C (cid:48)x − d(cid:48) is
sigmoidal functions; however, it is difﬁcult to derive a closed-form solution for such functions, since
sigmoidal functions yield z in the denominator, and z cannot be directly solved because C is not
invertible (referring to Appendix B for detailed reasons). Therefore, we have to consider a piecewise
roundabout where we can ﬁrst numerically solve the optimal point to determine which segment the
optimal point is on, and then expand the segment to the whole space. To make this feasible, two
assumptions must be made: 1) this function must be differentiable, and 2) the optimal point must be
unique; to ensure this, the surrogate should be a convex/concave piece-wise function. The second
property is for maintaining the optimal point upon segment expansion. Fortunately, there is one
simple surrogate function satisfying our requirement:

S(z) =


0

K(z + 1

z

4K )2

if z < − 1
4K
if − 1
if z ≥ 1
4K

4K ≤ z ≤ 1

4K

(7)

3For real optimization problems, we can sample A(cid:48) from A and estimate the largest eigenvalue of P T P .

6

Let M and U be diagonal matrices as the indicator of S(z), which are Mi,i = 2K[− 1
4K ]
and Ui,i = [ 1
4K < zi], where [·] is an indicator function. K > 0 is a hyper-parameter that needs to be
balanced. Larger K makes the function closer to original; however, if K is too large, then the training
process would be destabilized, because when the prediction error is large at the start of the training
process, ∂f
|ˆx might be too steep. Then consider the optimal point for the unconstrained optimization
∂ ˆθ
problem maximizing θT x − γT max(C (cid:48)x − d(cid:48), 0), by differentiaing on both sides, we can obtain:

4K ≤ zi ≤ 1

θ = C (cid:48)T M diag(γ)(C (cid:48)x − d(cid:48)) + C (cid:48)T (

1
4K

M + U )γ

(8)

This equation reveals the closed-form solution of x with respect to θ, and thus the calculation of ∂x
∂θ
becomes straightforward:

∂x
∂θ

= (C (cid:48)T M diag(γ)C (cid:48))−1

(9)

C (cid:48)T M diag(γ)C (cid:48) is invertible as long as at least n soft constraints are on the quadratic segment (i.e.,
active), which is the necessary condition to ﬁx a particular optimal point n in Rn. With such solution,
we can train our prediction model with stochastic gradient descent (SGD). For each batch of problem
instances, we ﬁrst solve optimization problems numerically using solvers like Gurobi to get the matrix
M and U , and then calculate gradients with the analytical solution. The parameters of the prediction
model are updated by such gradients. The sketch of our algorithm is outlined in Appendix C.

4 Applications and Experiments

We apply and evaluate our approach on the three problems described in Section 2, i.e.,linear program-
ming, quadratic programming, and asymmetric soft constraint minimization. These problems are
closely related to three applications respectively: synthetic linear programming, portfolio optimiza-
tion, and resource provisioning, which are constructed using synthetic or real-world datasets. The
detailed derivation of gradients for each application can be found in Appendix D.

In our experiments, the performance is measured in regret, which is the difference between the
objective value when solving optimization over predicted parameters and the objective value when
solving optimization over actual parameters. For each experiment, we choose two generic two-stage
methods with L1-loss and L2-loss, as well as decision-focused methods for comparison baselines.
We choose both SPO+[10] and DF proposed by Wilder et al. [9] for synthetic linear programming
and DF only for portfolio optimization,4 as the former is specially designed for for linear objective.
For resource provisioning, we use a problem-speciﬁc weighted L1 loss, as both SPO+ and DF are not
designed for gradients with respect to variables in the soft constraints. All reported results for each
method are obtained by averaging on 15 independent runs with different random seeds.

As real-world data is more lenient than extreme cases, in practice we use a much lower empirical
bound than the upper bound proved in section 3.1., e.g., constants of around 20 and 5
n where n
is the number of dimensions of decision variables. One rule of thumb is to start from a reasonable
n, where such "reasonable constant" is the product of a constant factor
constant or a constant times
(e.g. 5 − 10) and a roughly estimated upper bound of ||θ||2 (which corresponds to E in our bounds)
n while
with speciﬁc problem settings; then alternately increase the constant and time an extra
resetting the constant until the program stops diverging, and the hard constraints are satisﬁed. In our
experiments, we hardly observe situations where such process goes for two or more steps.

√

√

√

4.1 Synthetic Linear Programming

Problem setup. The prediction dataset {(ξi, θi)}N
i=1 is generated by a general structural causal model
([21]), ensuring it is representative and consistent with physical process in nature. The programming
parameters are generated for various scales in numbers of decision variables, hard constraints, and
soft constraints. Full details are given in Appendix E.

4The method [3] and its variant with dimension reduction [9] have same performance in terms of regret on

this problem, thus we choose the former for comparison convenience.

7

1000

N
100

Problem Size
(40, 40, 0)
(40, 40, 20)
(80, 80, 0)
(80, 80, 40)
(40, 40, 0)
(40, 40, 20)
(80, 80, 0)
(80, 80, 40)
(40, 40, 0)
(40, 40, 20)
(80, 80, 0)
(80, 80, 40)

L2
2.493±0.295
2.664±0.303
5.831±0.361
4.786±0.596
1.447±0.155
1.613±0.110
3.718±0.117
2.913±0.172
1.080±0.109
1.277±0.077
2.943±0.091
2.224±0.106
Table 1: Performance comparison (regret mean with std. deviation) for the synthetic linear pro-
gramming problem. N is the size of the training dataset, and problem size is a triplet (# of decision
variables’ dimension, # of hard constraints, # of soft constraints).

Regret (the lower, the better)
SPO+ [10]
2.506±0.294
2.667±0.281
5.711±0.309
4.939±0.382
1.454±0.148
1.618±0.103
3.573±0.113
2.879±0.148
1.090±0.105
1.298±0.077
2.926±0.079
2.234±0.122

L1
2.454±0.232
2.626±0.307
5.736±0.291
4.786±0.403
1.463±0.143
1.626±0.141
3.768±0.132
2.982±0.176
1.077±0.105
1.283±0.070
2.959±0.086
2.239±0.122

DF [9]
2.478±0.425
2.536±0.376
5.756±0.317
4.902±0.537
1.434±0.268
1.529±0.151
3.532±0.102
3.351±0.212
1.078±0.092
1.291±0.091
2.869±0.085
2.748±0.165

Ours
2.258±0.311
2.350±0.263
5.200±0.506
4.570±0.390
1.346±0.144
1.506±0.102
3.431±0.100
2.781±0.165
1.037±0.100
1.220±0.071
2.845±0.064
2.172±0.098

5000

Experimental setup. All ﬁve methods use the same prediction model – a fully connected neural
network of two hidden layers with 128 neurons for each and ReLU [22] for activation. We use
AdaGrad [23] as the optimizer, with learning rate 0.01 and gradient clipped at 1e − 4. We train each
method for 40 epochs, and early stop when valid performance degrades for 4 consecutive epochs.
Specially, to make DF[9] work right on the non-differentiable soft constraints, we ﬁrst use black-box
solvers to determine whether each soft constraint is active on the optimal point, and then optimize
with its local expression (i.e. 2nd-order Taylor expansion at optimal point).

Performance analysis. Our experiments cover four programming problem scales with three predic-
tion dataset sizes. Results are summarized in Table 1. In all cases, our method performs consistently
better than two-stage methods, DF and SPO+. Even for the cases with only hard constraints (i.e.,
the third parameter of problem size is 0), our method still has signiﬁcant advantage, demonstrating
its effectiveness on handling hard constraints. Surprisingly, although the main objective is linear,
SPO+ often performs even worse than two-stage methods. Detailed analysis (see the appendix) shows
that SPO+ quickly reaches the test optimality and then over-ﬁts. This may be due to that, unlike
our method, SPO+ loss is not designed to align with the soft constraints. This unawareness of soft
constraint is also why DF is performing worse than our method, as DF is working on an optimization
landscape that is non-differentiable at the boundary of soft constraints, on which the optimal point
usually lies. Besides, with the increment of the samples in train data, the performance of all methods
is improved signiﬁcantly and the performance gap among ours and two-stage methods becomes
narrow, which implies that prediction of two-stage methods becomes better and with lower biases.
Even so, our method has better sample efﬁciency than two-stage methods.

We also investigated effect of the hyper-parameter K in our surrogate max function, detailed in the
appendix. Through our experiments, K’s effect to regret is not monotonic, and its optimal value
varies for different problem sizes. Interestingly, K’s effect is approximately smooth. Thus, in practice,
we use simple grid search to efﬁciently ﬁnd the best setting of K.

4.2 Portfolio Optimization

Problem and experimental setup. The prediction dataset is daily price data of SP500 from 2004
to 2017 downloaded by Quandl API [24] with the same settings in [3]. We use the same ﬁx as that in
linear programming experiment to make DF[9] work with non-differentiable soft constraints, which
was also used in [3] for non-convex optimization applications. Most settings are aligned with those
in [3], including dataset conﬁguration, prediction model, learning rate (0.01), optimizer (Adam),
gradient clip (0.01) and number of training epochs (20). We set the number of soft constraints to 0.4
times of n, where n is the number of candidate equities. For the soft constraint αT max(Cx − d, 0),
α = 15
n v, where each element of v is generated randomly at uniform from (0, 1); the elements of
matrix C is generated independently from {0, 1}, where the probability of 0 is 0.9 and 1 is 0.1. K is
set as 100.

8

Regret measured in % (the lower, the better)

#Equities
50
100
150
200
250

L1
4.426±0.386
4.262±0.231
3.878±0.281
3.755±0.236
3.721±0.205

L2
4.472±0.385
4.320±0.229
3.950±0.287
3.822±0.273
3.751±0.212

DF [9]
4.016±0.389
3.500±0.252
3.419±0.281
3.406±0.287
3.335±0.175

ours(K = 100)
3.662±0.238
3.214±0.138
3.109±0.162
3.152±0.183
3.212±0.135

Table 2: Performance comparison (regret mean with std. deviation) for portfolio optimization.

α1/α2
100
10
1
0.1
0.01

L1
105.061±21.954
13.061±2.713
4.267±0.618
10.846±1.606
99.145±21.159

Regret (the lower, the better)

L2
93.193±29.815
13.275±6.208
5.136±0.722
13.619±2.195
118.112±29.957

Weighted L1
79.014±32.069
7.743±1.305
4.267±0.618
16.462±2.093
230.825±91.184

Ours(K = 0.05)
20.829±8.289
2.746±1.296
5.839±0.512
10.240±1.248
94.341±29.821

Table 3: Performance comparison (regret mean with std. deviation) for resource provisioning.

Performance analysis. Table 2 summarizes the experimental results. In total, on all problem sizes
(#equities), our method performs consistently and signiﬁcantly better than both two-stage (L1 and
L2) methods and the decision focused DF[9]. Among the three baselines, DF is signiﬁcantly better
than two-stage methods, similar to results in [3]. In fact, DF under this setting can be viewed as a
variant of our method with inﬁnite K and no conversion of softening (cid:80)
i xi = 1. The comparison
to DF also demonstrates the advantage of our method on processing such non-differentiable cases
against the simple 2nd-order Taylor expansion. Besides, with the increment of the number of equities,
regrets of all methods decease, which indicates that for the constraint (cid:80)
i xi = 1, larger number of
equities brings smaller entries of x on average (with the presence of Q, there are many non-zero
entries of x), lowering the impact of prediction error for any single entry.

4.3 Resource Provisioning

Problem setup. We use ERCOT energy dataset [25], which contains hourly data of energy output
from 2013 to 2018, 52535 data points in total. We use the last 20% samples for test. We aim to predict
the matrix C ∈ R24×8, the loads of 24 hours in 8 regions. The decision vairable x is 8-dimensional,
and d = 0.5 × 1 + 0.1N (0, 1). We test ﬁve sets of (α1, α2), with α1/α2 ranging from 100 to 0.01.

Experimental setup. We use AdaGrad with learning rate of 0.01, and clip the gradient with norm
0.01. For the prediction model, we use McElWee’s network [26] which was highly optimized for this
task, with (8 × 24 × 77)-dimensional numerical features and embedding ones as input.

Performance analysis. Table 3 shows the experimental results. The absolute value of regret
differs largely across different ratios of α1/α2. Our method is better than other methods, except
for α1/α2 = 1, where the desired objective is exactly L1-loss and thus L1 performs the best.
Interestingly, compared to L1/L2, the Weighted L1 performs better when α1/α2 > 1, but relatively
worse otherwise. This is probably due to the dataset’s inherent sample bias (e.g., asymmetric
distribution and occasional peaks), which causes the systematic bias (usually underestimation) of the
prediction model. This bias exacerbates, when superposed with weighted penalty multipliers which
encourage the existing bias to fall on the wrong side. Besides, the large variance for weighted L1 at
α1/α2 = 0.01 is caused by a pair of outliers.

5 Related Work

Differentiating argmin/argmax through optimality conditions. For convex optimization prob-
lems, the KKT conditions map coefﬁcients to the set of solutions, and thus can be differentiated
for argmin using implicit function theorem. Following this idea, existing work developed implicit
layers of argmin in neural network, including OptNet [13] for quadratic programs (QP) problems
and CVXPY [14] for more general convex optimization problems. Further with linear relaxation and

9

QP regularization, Wilder et al. derived an end-to-end framework for combinatorial programs [9],
which accelerates the computation by leverage the low-rank properties of decision vectors [3], and
is further extended to mixed integer linear programs in MIPaaL [27]. Besides, for the relaxed LP
problems, instead of differentiating KKT conditions, IntOpt [28] proposes an interior point based
approach which computes gradients by differentiating homogeneous self-dual formulation.

Optimizing surrogate loss functions. Elmachtoub and Grigas [10] proposed a convex surrogate
loss function, namely SPO+, measuring the decision error induced by a prediction, which can handle
polyhedral, convex and mixed integer programs with linear objectives. TOPNet [29] proposes a
learned surrogate approach for exotic forms of decision loss functions, which however is hard to
generalize to handle constrained programs.

Differentiating argmin is critical for gradient methods to optimize decision-focused prediction
models. Many kinds of efforts, including regularization for speciﬁc problems (e.g., differentiable
dynamic programming [30], differentiable submodular optimization [31]), reparameterization [32],
perturbation [33] and direct optimization ([12]), are spent for optimization with discrete or continuous
variables and still actively investigated.

As a comparison, our work proposes a surrogate loss function for constrained linear and quadratic
programs extended with soft constraints, where the soft constraints were not considered principally
in previous work. Also, unlike OptNet [13] and CVXPY [15], our method does not need to solve
KKT conditions. Instead, by adding reasonable costs for infeasibility, we convert the constrained
optimization problem into an unconstrained one while keeping the same solution set, and then
derive the required Jacobian analytically. To some degree, we reinvent the exact function [34] in
prediction+optimization.

6 Conclusion

In this paper, we have proposed a novel surrogate objective framework for prediction+optimization
problems on linear and semi-deﬁnite negative quadratic objectives with linear soft constraints. The
framework gives the theoretical bounds on constraints’ multipliers, and derives the closed-form
solution as well as their gradients with respect to problem parameters required to predict by a
model. We ﬁrst convert the hard constraints into soft ones with reasonably large constant multipliers,
making the problem unconstrained, and then optimize a piecewise surrogate locally. We apply and
empirically validate our method on three applications against traditional two-stage methods and
other end-to-end decision-focused methods. We believe our work is an important enhancement to
the current prediction+optimization toolbox. There are two directions for the future work: one is to
seek solutions which can deal with hard constraint parameters with negative entries, and the other
is to develop the distributional prediction model rather than existing point estimation, to improve
robustness of current prediction+optimization methods.

10

References

[1] Chuan Luo, Bo Qiao, Xin Chen, Pu Zhao, Randolph Yao, Hongyu Zhang, Wei Wu, Andrew
Zhou, and Qingwei Lin. Intelligent virtual machine provisioning in cloud computing.
In
Proceedings of IJCAI 2020, pages 1495–1502, 2020.

[2] Sirikarn Chansombat, Pupong Pongcharoen, and Christian Hicks. A mixed-integer linear
programming model for integrated production and preventive maintenance scheduling in the
capital goods industry. International Journal of Production Research, 57:1–22, 04 2018. doi:
10.1080/00207543.2018.1459923.

[3] Kai Wang, Bryan Wilder, Andrew Perrault, and Milind Tambe. Automatically learning compact
quality-aware surrogates for optimization problems. In NeurIPS 2020, Vancouver, Canada,
2020.

[4] Emir Demirovi, Peter Stuckey, Tias Guns, James Bailey, Christopher Leckie, Kotagiri
Ramamohanarao, and Jeffrey Chan. Dynamic programming for predict+optimise. Pro-
ceedings of the AAAI Conference on Artiﬁcial Intelligence, 34:1444–1451, 04 2020. doi:
10.1609/aaai.v34i02.5502.

[5] Gartheeban Ganeshapillai, John Guttag, and Andrew Lo. Learning connections in ﬁnancial time
series. In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 30th International
Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research,
pages 109–117, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR.

[6] LLC Gurobi Optimization. Gurobi optimizer reference manual, 2021. URL http://www.

gurobi.com.

[7] IBM ILOG Cplex. Release notes for cplex 20.1.0. 2020. URL https://www.ibm.com/docs/

en/icos/20.1.0?topic=2010-release-notes-cplex.

[8] Yoshua Bengio. Using a ﬁnancial training criterion rather than a prediction criterion. Int. J.
Neural Syst., 8(4):433–443, 1997. doi: 10.1142/S0129065797000422. URL https://doi.
org/10.1142/S0129065797000422.

[9] Bryan Wilder, Bistra Dilkina, and Milind Tambe. Melding the data-decisions pipeline: Decision-
focused learning for combinatorial optimization. In The Thirty-Third AAAI Conference on
Artiﬁcial Intelligence, pages 1658–1665. AAAI Press, 2019.

[10] Adam Elmachtoub and Paul Grigas. Smart "predict, then optimize". Management Science, 10

2017. doi: 10.1287/mnsc.2020.3922.

[11] Priya L. Donti, Brandon Amos, and J. Zico Kolter. Task-based end-to-end model learning
in stochastic optimization. In Proceedings of the 31st International Conference on Neural
Information Processing Systems, NIPS’17, page 5490–5500, Red Hook, NY, USA, 2017. Curran
Associates Inc. ISBN 9781510860964.

[12] Yang Song, Alexander Schwing, Richard, and Raquel Urtasun. Training deep neural networks
via direct loss minimization. In Proceedings of The 33rd International Conference on Machine
Learning, volume 48 of Proceedings of Machine Learning Research, pages 2169–2177, New
York, New York, USA, 20–22 Jun 2016. PMLR.

[13] Brandon Amos and J. Zico Kolter. OptNet: Differentiable optimization as a layer in neural
networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research,
pages 136–145, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.
URL http://proceedings.mlr.press/v70/amos17a.html.

[14] Brandon Amos. Differentiable Optimization-Based Modeling for Machine Learning. PhD

thesis, Carnegie Mellon University, May 2019.

[15] Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and J. Zico
Kolter. Differentiable convex optimization layers. In Advances in Neural Information Processing
Systems, volume 32, pages 9562–9574. Curran Associates, Inc., 2019.

11

[16] Harry M. Markowitz. Portfolio Selection: Efﬁcient Diversiﬁcation of Investments. Yale
University Press, 1959. ISBN 9780300013726. URL http://www.jstor.org/stable/j.
ctt1bh4c8h.

[17] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-
performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-
Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32,
pages 8024–8035. Curran Associates, Inc., 2019.

[18] A. Griewank and A. Walther. Evaluating Derivatives: Principles and Techniques of Algorithmic
Differentiation, Second Edition. Other Titles in Applied Mathematics. Society for Industrial
and Applied Mathematics (SIAM, 3600 Market Street, Floor 6, Philadelphia, PA 19104), 2008.
ISBN 9780898717761.

[19] Jan R. Magnus Jan R. Magnus. Matrix Differential Calculus with Applications in Statistics and
Econometrics, Third Edition. Wiley Series in Probability and Statistics. Wiley, 2019. ISBN
9781119541202.

[20] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press,

2004.

[21] Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference. The

MIT Press, 2017.

[22] Vinod Nair and Geoffrey E. Hinton. Rectiﬁed linear units improve restricted boltzmann
machines. In Proceedings of the 27th International Conference on International Conference
on Machine Learning, ICML’10, page 807–814, Madison, WI, USA, 2010. Omnipress. ISBN
9781605589077.

[23] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine Learning Research, 12:2121–2159, 07 2011.

[24] Sp500 data set. 2021. URL https://docs.quandl.com/.

[25] Ercot data set. 2021. URL https://github.com/kmcelwee/mediumBlog/tree/master/

load_forecast/data.

[26] Kevin McElwee. Predict daily electric consumption with neural networks, 2020. URL https:

//www.brownanalytics.com/load-forecasting.

[27] Aaron Ferber, Bryan Wilder, Bistra Dilkina, and Milind Tambe. Mipaal: Mixed integer program

as a layer. In AAAI Conference on Artiﬁcial Intelligence, 2020.

[28] Jayanta Mandi and Tias Guns. Interior point solving for lp-based prediction+optimisation. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural
Information Processing Systems, volume 33, pages 7272–7282. Curran Associates, Inc., 2020.

[29] Di Chen, Yada Zhu, Xiaodong Cui, and Carla Gomes. Task-based learning via task-oriented
prediction network with applications in ﬁnance. In Christian Bessiere, editor, Proceedings of
the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence, IJCAI-20, pages
4476–4482. International Joint Conferences on Artiﬁcial Intelligence Organization, 7 2020. doi:
10.24963/ijcai.2020/617. URL https://doi.org/10.24963/ijcai.2020/617. Special
Track on AI in FinTech.

[30] Arthur Mensch and Mathieu Blondel. Differentiable dynamic programming for structured
prediction and attention. In 35th International Conference on Machine Learning, volume 80,
pages 3459–3468, 2018.

[31] Josip Djolonga and Andreas Krause. Differentiable learning of submodular models. In Advances
in Neural Information Processing Systems, volume 30, pages 1013–1023. Curran Associates,
Inc., 2017.

12

[32] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation
and approximate inference in deep generative models. In Proceedings of the 31st International
Conference on International Conference on Machine Learning - Volume 32, ICML’14, page
II–1278–II–1286. JMLR.org, 2014.

[33] Quentin Berthet, Mathieu Blondel, Olivier Teboul, Marco Cuturi, Jean-Philippe Vert, and Francis
Bach. Learning with differentiable perturbed optimizers. arXiv preprint arXiv:2002.08676,
2020.

[34] Dimitri P Bertsekas. Nonlinear programming. Journal of the Operational Research Society, 48

(3):334–334, 1997.

Checklist

1. For all authors...

(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s

contributions and scope? [Yes]

(b) Did you describe the limitations of your work? [Yes]
(c) Did you discuss any potential negative societal impacts of your work? [Yes]
(d) Have you read the ethics review guidelines and ensured that your paper conforms to

them? [Yes]

2. If you are including theoretical results...

(a) Did you state the full set of assumptions of all theoretical results? [Yes]
(b) Did you include complete proofs of all theoretical results? [Yes]

3. If you ran experiments...

(a) Did you include the code, data, and instructions needed to reproduce the main experi-

mental results (either in the supplemental material or as a URL)? [Yes]

(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they

were chosen)? [Yes]

(c) Did you report error bars (e.g., with respect to the random seed after running experi-

ments multiple times)? [Yes]

(d) Did you include the total amount of compute and the type of resources used (e.g., type

of GPUs, internal cluster, or cloud provider)? [Yes]

4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...

(a) If your work uses existing assets, did you cite the creators? [Yes]
(b) Did you mention the license of the assets? [Yes]
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]
(d) Did you discuss whether and how consent was obtained from people whose data you’re

using/curating? [Yes]

(e) Did you discuss whether the data you are using/curating contains personally identiﬁable

information or offensive content? [Yes]

5. If you used crowdsourcing or conducted research with human subjects...

(a) Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A] We did not use crowdsourcing, nor did we conduct research with
human subjects.

(b) Did you describe any potential participant risks, with links to Institutional Review

Board (IRB) approvals, if applicable? [N/A]

(c) Did you include the estimated hourly wage paid to participants and the total amount

spent on participant compensation? [N/A]

13

A Mathematical Proofs

Throughout the following proofs, for the convenience of description, we denote two symbols:

• the symbol ∠(a, b), which means the angle of two vectors a, b ∈ Rn;

• cone(A), which means the conic combination of the row vectors of matrix A; cone(A, B)

stands for the conic combination of the union of the row vectors of matrix A, B.

Throughout this section, we will assume the same variables as in Eq.1-5 and Section 3 by default,
and further make the following assumptions or simpliﬁcation for ease of description.

• Assume the main objective g(x, θ) = θT x, where ||θ||2 ≤ E. In reality, elements of the
predictive vector θ have a known range and thus l2-norm of θ is bounded by a constant,
namely E ∈ R.

• We omit the soft constraints in original objective. Since all proofs in this section consider
∂x in Eq.5, for soft constraints which are linear, their derivatives are

the worst direction of ∂L
constant and can be integrated into E for any bounded penalty multiplier α.

• Similarly, we omit quadratic term Q. At ﬁrst glance, it seems that Q will bring unbounded
derivative of decision variable x. However, note that we are ﬁnding the maximum value of
function, and the matrix Q is semi-deﬁnite positive, we have ∂−xT Qx
< 0 for any i. Thus,
denote the maximum point of −xT Qx to be x0, we can ﬁnd a ﬁnite radius r > 0, such that
any point x ∈ Rn out of the circle with x0 as the center and r as radius is impossible to be
the global optimal point; intuitively, the norm of the derivative of Q becomes so large as to
"draw the optimal point back" from too far, and thus we can ignore the points outside the
circle. Since the circle is a bounded set, the norm of derivative E is also bounded inside the
circle.

∂xi

• When we consider the effect of violating constraints, we only consider non-degraded
activated cone, i.e.,, for an activated cone in a n-dimensional Euclidean space, there are
at least n non-redundant active constraints. Otherwise, as we stated in the main paper, we
can project the activated cone K = {z ∈ Rn|z − x0 ∈ cone(A(cid:48))} onto the complementary
space of the cone tip x0 (i.e., the solution space of A(cid:48)) and proceed on a low-dimensional
subspace.

• As for constraint itself, we assume that for any row vector of A in Ax ≤ b, and any row
vector of B in Bx = c, we have ||Ai||2 = 1 and ||Bi||2 = 1 for any row vector Ai, Bi,
i ∈ {1, 2, ..., m} where m is the number of constraints. If not, we can ﬁrst normalize the
row vectors of A and B while adequately scaling b, c, then apply our proofs.

Thus it is straightforward to apply our proofs to the general form of Eq.5.

A.1 Lemma 1

Lemma 1. (Bounding the utility gain) Let R = f (x, θ) − maxx1∈C f (x1, θ) be the utility gain,
(cid:80)n
then R ≤ f (x, θ) − f (x0, θ) ≤ E
i), where x is an infeasible point, A(cid:48)x ≤
i=1(A(cid:48)
ix − b(cid:48)
∗ and θ∗ are the optimal solution of
∗, θ∗) where A(cid:48)
b(cid:48) the active constraints at x, p0 = ∠(A(cid:48)
i
i
i, θ) (i.e., the maximin angle between θ and hyperplanes of the activated cone
maxθ minA(cid:48)
K = {z ∈ Rn|z − x0 ∈ cone(A(cid:48))}), and x0 the projection of x to the tip of cone C = {z ∈
Rn|A(cid:48)z ≤ b(cid:48)}. E is the upper bound of ||θ||2.

cos∠(A(cid:48)

cos p0

i

Proof. The ﬁrst inequality is trivial as x0 ∈ C and directly followed from the deﬁnition of utility gain
R.
For the second inequality, by the law of sines, we have ||x − x0||2 = di
, where di is the distance
sin τi
of x to the i-th hyper-plane in the activated cone cone(A(cid:48)) with A(cid:48)
i (i.e., the i-th row of A(cid:48)) being the
normal vector of the i-th hyper-plane in A(cid:48)x ≤ b(cid:48), and τi is the angle between i-th hyper-plane and θ.

14

Then the utility gain R from violating the constraints follows:

R ≤ θT (x − x0)

≤ ||θ||2||x − x0||2

= ||θ||2

= ||θ||2

di
sin τi
dj
sin τj
i=1 di
sin τj
i=1(A(cid:48)

(cid:80)n

(cid:80)n

≤ E

= E

(By the law of sines; i is arbitrary)

(By selecting j = argmin

i

sin τi = argmax

i

cos∠(A(cid:48)

i, x − x0))

(10)

(E is the upper bound of ||θ||2)

ix − b(cid:48)
i)

sin τj

(di =

|A(cid:48)

ix − b(cid:48)
i|
||A(cid:48)
i||2

= A(cid:48)

ix − b(cid:48)

i; note the ﬁfth assumption.)

Thus, setting all entries of penalty multiplier β in the hard constraint conversion to O( E
O(

j ,x−x0) ) will give us an upper bound of the minimum feasible β.

E
cos∠(A(cid:48)

sin τj

) =

The rest of the work is to ﬁnd the lower bound for minθ maxi cos∠(A(cid:48)
activated cone cone(A(cid:48)) and x0; as we have assumed ||A(cid:48)
minθ maxi cos∠(A(cid:48)
i, x − x0) can be equivalently written as5

i, x − x0) for any given
i|| = 1 ∀i ∈ {1, 2, ..., n}, the objective

F = min

θ

max
i

A(cid:48)
iθ

(11)

Deriving the lower bound of F is the core part of proving Theorem 3 and 6.

A.2 Lemma 7

In order to prove Theorem 2, we give a crucial lemma.
Lemma 7. Consider a set of hyper-planes with normal vectors A(cid:48)
i||2 = 1
for i ∈ {1, 2, ..., n} which forms a cone. Let di be the distance of a point x to the hyper-plane A(cid:48)
i,
where x is in the cone of A(cid:48)
1 + ... + knA(cid:48)
n, ki ≥ 0 ∀i ∈ {1, 2, ..., n}), then we have
||x||2 ≤ (cid:80)n
i=1 di.

n ∈ Rn ≥ 0, ||A(cid:48)

i (i.e., x = k1A(cid:48)

2, ..., A(cid:48)

1, A(cid:48)

Proof. Without loss of generality, let x = k1A(cid:48)
x if ||x||2 (cid:54)= 1). The distance di = A(cid:48)
= A(cid:48)
i
i
||A(cid:48)

T x
i||2

2 + ... + knA(cid:48)

1 + k2A(cid:48)
T x. Therefore, we have

n, and ||x||2 = 1 (we can scale

n
(cid:88)

i=1

di =

=

≥

n
(cid:88)

i=1
n
(cid:88)

j=1

n
(cid:88)

j=1

A(cid:48)
i

T (k1A(cid:48)

1 + ... + knA(cid:48)

n)

kj(A(cid:48)
j

T A(cid:48)

j +

n
(cid:88)

i=1,i(cid:54)=j

A(cid:48)
i

T A(cid:48)
j)

kj||A(cid:48)

j||2 (A(cid:48)

i ≥ 0, therefore A(cid:48)
i

T A(cid:48)

j ≥ 0)

We next prove that k1 + k2 + ... + kn ≥ 1. As ||x||2 = 1, we have

= k1 + k2 + ... + kn

T x + ... + knA(cid:48)
T x ∈ [0, 1], k1 +...+kn ≥ 1 must hold. Therefore, (cid:80)n

T x + k2A(cid:48)

k1A(cid:48)
1

2

As A(cid:48)
i

i=1 di ≥ k1 +...+kn ≥ 1 = ||x||2.

||x||2 = 1
T x = 1

n

(12)

(13)

5With an abuse of notation, the θ in Eq.11 represents x − x0, as the worst situation for deriving upper bound

of β is that x − x0 has the same direction with the objective θT x’s derivative θ.

15

A.3 Theorem 2

Theorem 2. Assume the optimization objective θT x with constraints Ax ≤ b, where A ≥ 0, and
b ≥ 0. Then, the utility gain R obtained from violating Ax − b ≤ 0 has an upper bound of
O((cid:80)

i max(wi, 0)E), where w = A(cid:48)x − b(cid:48), and A(cid:48)x ≤ b(cid:48) is the active constraints.

Proof. Let di = (di,1, di,2, ..., di,n)T be the vector of distances to the i-th hyper-plane of A(cid:48) with
normal vector A(cid:48)
i from x. By deﬁnition of x and x0, the distance vector of the point to the convex
hull C is x − x0. When A ≥ 0 and b ≥ 0, obviously Lemma 7 holds. Consider the active constraints
w = A(cid:48)x − b(cid:48) ≥ 0 which is the subset of active constraints in Ax ≤ b. The utility gain R from
violating the rules w = A(cid:48)x − b(cid:48) satisﬁes:

R ≤ θT (x − x0)

≤ ||θ||2||(x − x0)||2
(cid:88)

≤ ||θ||2

||di||2 (Lemma 7)

≤ E

= E

(cid:88)

i
(cid:88)

i

i

||di||2

max(wi, 0) (di =

|A(cid:48)

ix − b(cid:48)
i|
||A(cid:48)
i||2

(14)

= A(cid:48)

ix − b(cid:48)

i; note the ﬁfth assumption.)

This holds for any x. Therefore, we turn the hard constraint Ax ≤ b into a soft constraint with penalty
multiplier β = E.

A.4 Theorem 3

Theorem 3. Assume at least one constraint in x ≥ 0 is active, then the utility gain R by deviating
from the feasible region is bounded by O( n1.5E (cid:80)
), where p0 = mini,j ∠(Ai, ej) (i.e., the
(cid:20) A(cid:48)
−I (cid:48)
A(cid:48)x ≤ b(cid:48) and −I (cid:48)x ≤ 0 are active constraints in Ax ≤ b and x ≥ 0 respectively.

smallest corner between axes and other constraint hyper-planes), and w =

i max(wi,0)
sin p0

(cid:20)b(cid:48)
0

, where

x −

(cid:21)

(cid:21)

Proof. As Lemma 1 implies, the key point of proving this theorem is to prove that θ (the derivative
of the objective), is always close to some rows of A(cid:48) and −I (cid:48).
Consider the activated cone K = {z ∈ Rn|z − x0 ∈ cone(A(cid:48), −I (cid:48))}; all hyper-planes of A(cid:48)x = b(cid:48)
and violated entries of x ≥ 0 must pass through K’s tip x0. Figure 3 gives such an illustrative example.
For the rest of the proof, it is enough to assume that the activated cone K is a non-degraded cone, since
otherwise, as the main paper states, we can project the activated cone onto the supplementary space
of its tip. By the projection, we actually reduce the problem to the same one with lower dimensions
of x, and can apply the following proof in the same way. Let q be the number of rows in I (cid:48), i.e., the
number of active constraints in x ≥ 0; and r be the number of rows in A(cid:48), i.e., the number of active
constraints in Ax ≤ b. Since K is non-degraded, we have r + q ≥ n.

Without loss of generality, we transform the problem equivalently by rearranging the order of
dimensions of x so that I (cid:48) is the ﬁrst q rows of In×n. Let’s see an example after this transformation.
As shown in Figure 3, the three solid black vectors are (−1, 0, 0), (0, −1, 0), and A(cid:48)
1 ≥ 0, where the
ﬁrst and second entries of A(cid:48)
1 are strictly greater than 0 (otherwise the cone is degraded). Similarly,
for the n-dimensional non-degraded activated cone, we consider the maximin angle between θ and
all active inequality constraints A(cid:48) and −I (cid:48) (which is equivalent to ﬁnding the minimax cosine value
of angles between θ and all active inequality constraints). Then, guided by Equation 11 in Lemma 1,
the upper bound of distance between θ and hyper-planes with normal vector A(cid:48)
j, is the solution of the
following optimization problem w.r.t. θ ∈ Rn and A(cid:48)
j (since the cone is non-degraded, we assume
that A(cid:48)

j are linearly independent):

F = min

θ

max
j∈J={1,..,r},k∈K={1,..,q}

s.t. ||θ||2 = 1, θ ∈ K, r + q ≥ n.

16

{A(cid:48)

jθ, −eT

k θ},

(15)

Figure 3: A 3-dimensional illustration of the proof for Theorem 3. The green polytope is the feasible
region; the black solid vectors are the related vectors (i.e. rows of A(cid:48)) and the red line is the utility
vector θ. Apparently, (0, 0, 1)T θ ≥ 0, otherwise the intersection point will move down to origin. The
worse case appears when the red θ is on the circumcenter of the triangle formed by the three solid
unit vectors.

j ≥ 0, ||A(cid:48)

where ek is the unit vector where the k-th entry is 1 and others are 0. Note that according to our
presumptions, A(cid:48)
j||2 = 1 for j ∈ {1, 2, ..., r} have already satisﬁed. The constraints
θ ∈ K come from the following consideration: if with given θ we update x in an iterative manner
such as gradient ascending, x would eventually leave current activated cone where θi < 0 for i ∈
{q + 1, ..., n} (see Figure 3 for an illustration). Since for any dimension index i ∈ {q + 1, ..., n}, all
entries of A(cid:48)
j and −ek (j ∈ J, k ∈ K) are non-negative, we know that θi ≥ 0, ∀i ∈ {q + 1, ..., n}.
To further relax the objective F and derive the lower bound, we will assume r + q = n in the rest of
the paper; if r ≥ n − q, we can simply ignore all A(cid:48)

j in the max operator with j > n − q.

Given the signs of θ, we next derive a lower bound of F with respect to any given A(cid:48)
Now we relax F by setting all entries of A(cid:48)
to get A(cid:48)(cid:48)

j and set of k.
j to 0, except for the entries with index {1, 2, ..., q} and sj

j , where {sj}(j ∈ {q + 1, ..., n}) is a permutation of {q + 1, q + 2..., n}. We have:

F ≥ min

θ

max
j∈{1,..,r},k∈{1,..,q}

≥ min

θ

max
j∈{1,..,r},k∈{1,..,q}

{A(cid:48)(cid:48)T

j θ, −eT

k θ} (Relaxing within the max operator; note the sign of θ)

q
(cid:88)
{

A(cid:48)

j,iθi + A(cid:48)

j,sj

θsj , −θk}

≥ min

θ

max
sj ∈{q+1,n},k∈{1,..,q}

θi + α0θsj , −θk}

i=1
q
(cid:88)
{

i=1

(16)
where ||θ||2 = 1, α0 is the smallest non-zero entry among A(cid:48)
j,sj (and note that A ≥ 0 and θsj ≥ 0,
which means α0θsj ≥ 0). The inequalities hold for the relaxation within the max operator. Note
i ≤ 0 for i ∈ {1, 2, ..., q} for the optimal θ∗; otherwise, we
that the last line of inequality assumes θ∗
can scale further, ignore the index i of the optimal point θ∗ where θ∗
i > 0 and proceed with n − 1
dimensions for the following two facts:

1. (cid:80)q

i=1 θi + α0θsj term becomes smaller after ignoring such dimension;

2. θ ∈ K, which means for any θ and any vector y of the cone we have θT y ≥ 0, and the last
line still corresponds to a cone. Therefore the optimal value of last line is no less than 0; the
removal of −θk term given θk > 0 will not affect the optimal value.

Then, according to the property of minimax, (cid:80)q
if (cid:80)q
(cid:80)n

i=1 θi + α0θsj should be equal to −θk; Otherwise,
j=q+1 |θj| =
j=q+1 θj by a small amount such that the result is better and ||θ||2 = 1 is still satisﬁed, which can

i=1 θi + α0θsj is larger than −θk, we can adjust the value of (cid:80)q

i=1 |θi| and (cid:80)n

17

be repeated until no optimization is possible, and vice versa. With (cid:80)q
the entries of θ are equal to each other within each group by symmetry.
Therefore, the solution θ should be in the form of ( c1√
n , c2√
we have the following set of equations:

n , ..., c2√

n , c1√

i=1 |θi| and (cid:80)n

j=q+1 θj ﬁxed,

n ) where c1 < 0, c2 > 0, and

qc2

1 + (n − q)c2

2 = n

qc1 + α0c2 = −c1

(17)

With equations in Eq. 17, we get c2 = −(1 + α1q)c1/α0, c2
) = n. As q ≤ n,
and as ||αj|| = 1, α0 can be further relaxed to the sine of the smallest angle p0 between axes x ≥ 0
and the other inequality constraints, and thus we have the lower bound F ≥ c1√
n = O(sin p/n1.5) for
the optimization problem listed in Equation 16, which is also the cosine lower bound to the nearest
normal vector A(cid:48)
j and −ek of the activated cone. This is the denominator of the desired result; the
ﬁnal result follows as we apply such bound to Lemma 1.

1(q + (n − q) (1+q)2

α2
0

A.5 Corollary 4

Corollary 4. For binary constraints where the entries of A (before normalization) are either 0 or 1,
the utility gain R of violating x ≥ 0 constraint is bounded by O(n1.5E (cid:80)
i max(wi, 0)), where w is
the same as Theorem 3.

Proof. The proof of Corollary 4 is almost the same with Theorem 3, except that if the constraint A is
cardinal, then each entry of A is either 0 or 1. Thus, as all the non-zero entry are the same, we shall
replace the third line in Equation 16 with

(cid:80)q
i=1 θi
(cid:112)hj
where hj is the number of non-zero entry of A(cid:48)
By substituting hj with n6, this optimization problem can be further relaxed to

θsj
(cid:112)hj
j, each entry being 1√
hj

, −θk}

max
j

min
θ

+

{

(18)

as normalized ||A(cid:48)

j||2 = 1.

min
θ

max
j

{

(cid:80)q

i=1 θi√
n

+

θsj√
n

, −θk}

and we can change Equation 17 to

qc2

1 + (n − q)c2

2 = n
qc1 + c2 = −

√

nc1
(cid:113)

Therefore, similar to the proof of Theorem 3, we get c1√
bound O(n1.5E (cid:80)

1
q+(n−q)(
i max(wi, 0)), removing the sin p in the denominator.

n =

√

(19)

(20)

n+q)2 , which leads to the

A.6 Theorem 5
Theorem 5. If there is only one equality constraint BT x = c (e.g. (cid:80)
i xi = 1) and special
inequality constraints x ≥ 0, Ix ≤ b, then the utility gain R from violating constraints is bounded by
O( n1.5E (cid:80)
), where p is the same with theorem 3, w is the union of active BT x − c and
−x.7

i max(wi,0)
sin p

Proof. We ﬁrst prove the situation where the inequality constraint is only x ≥ 0. If we only have one
equality constraint, then it can be seen as two separate inequality constraints, which are BT x ≤ c
and −BT x ≤ −c; for any non-degraded activated cone, as BT x < c and BT x > c cannot hold
simutaneously, there is at most one normal vector of constraint in the activated cone.

6Note in Theorem 3 we have already mentioned the non-positivity of θi where i ∈ {1, 2, ..., q}, thus bigger

hj brings smaller objective.

7See Theorem 3 for the meaning of union.

18

If BT x ≤ c is active (i.e., this constraint is violated, now we have BT x > c), then with B ≥ 0,
the case is exactly the same with Theorem 3. Otherwise, if −BT x ≤ −c (i.e., BT x ≥ c) is active
(i.e., BT x ≥ c is violated, now BT x < c), then the normal vector of the current active constraint
−BT x ≤ −c is −B ≤ 0. Note that all other normal vectors of active constraints are −ek ≤ 0
(which is the same situation as that in Equation 15 in the proof of Theorem 3, except that A(cid:48)
j is
substituted with −BT ≤ 0); this indicates that the condition of Lemma 7 is satisﬁed under such
scenario. Therefore, we can apply the proof of Theorem 2 in this case and get a better bound than
that in the ﬁrst scenario.

Then, the full theorem is proved as follows: consider any activated cone K. For any i ∈ {1, 2, ..., n},
xi ≤ bi and xi ≥ 0 cannot be active simultaneously. If the former is activated, we replace xi with
bi − xi; if the latter (or neither) is active, we remain xi as normal. Then, for this activated cone,
the scenario is exactly the same with the situation where the inequality constraints are only x ≥ 0.
Similarly, if the equality constraint is cardinal, with the same proof of Corollary 4, we can remove
the sin p in the denominator.

A.7 Theorem 6

Theorem 6. Given constraints Ax ≤ b, x ≥ 0, and Bx = c, where A, B, b, c ≥ 0, the utility gain R
obtained from violating constraints is bounded by O(
i max(wi, 0)), where λmax is the
upper bound for eigenvalues of P T P (P : x → P x is an orthogonal transformation for an n-sized
subset of normalized row vectors in A, B and −I), and w is the union of all active constraints from
Ax ≤ b, x ≥ 0, Bx ≤ c and −Bx ≤ −c.

nλmax

(cid:80)

√

Proof. For any non-degraded activated cone K, we have the following optimization problem (it
is worth noting that reformulating our original problem to this optimization one is inspired by
Equation 11 in Lemma 1):

F = min

θ

max
{ max
i∈D,ni∈Rn
j∈J,k∈K
s.t.||θ||2 = 1, θ ∈ K

{nT

i θ}, A(cid:48)

jθ, −eT

k θ}

(21)

d + r + q ≥ n, ||ni||2 = 1 ∀i ∈ D = {1, 2, ..., d}, < B1, ..., Bd >=< n1, ..., nd >

where A(cid:48)
j is the normal vector of the j-th inequality constraints in Ax ≤ b, ni is the i-th normal
vector for the subspace of Bx = c (and thus represents the same subspace that is represented by the
rows of B). According to the assumption, we scale Ax ≤ b and Bx = c so that ||ni|| = ||A(cid:48)
j|| = 1
for any i, j. Note that different from the optimization problem stated in Theorem 3, in this problem
q can be 0, which means that there can be no entry of x ≥ 0 active. To get the lower bound of
F , we may relax the function by keeping the normal vectors {ni} in their original direction B to
F ≥ G = minθ maxi∈D,j∈J,k∈K{B(cid:48)
i = Bi or
B(cid:48)
i ≤ 0 (i.e., any
pair of elements in B(cid:48)
i||2 = 1. We denote the
positive B(cid:48)

i is decided by the direction of θ), and thus B(cid:48)
i will not have opposite signs) for any i. Moreover, ||B(cid:48)

k θ} where the i-th row of B satisﬁes B(cid:48)

i = −Bi (the sign of B(cid:48)

, i1 ∈ D1 and the negative B(cid:48)

i2 , i2 ∈ D2. Then similar to Theorem 3,

i ≥ 0 or B(cid:48)

jθ, −eT

i as Bn

i as Bp
i1

iθ, A(cid:48)

G = min

θ

max
i1∈D1,i2∈D2,j∈J,k∈K

{Bn
i1

θ, Bp
i2

θ, A(cid:48)

jθ, −eT

k θ}

where D1

(cid:84) D2 = ∅, D1

(cid:83) D2 = D. we aggregate A(cid:48), Bp and −ek, Bn into α, and get

G = min

θ

max
j∈S

{αT

j θ}

(22)

(23)

where |S| ≥ n, αj ≥ 0 or ≤ 0 for any j ∈ S. Therefore, we can further relax G by selecting n
linearly independent vectors that are the closest to the minimum product8 and ignore the others for
the max operator, and for the rest of the proof we may assume that |S| = n.

8The existence of such set of vectors comes from the non-degradation of the activated cone.

19

Figure 4: Empirical estimation of the max maximum eigenvalue over 1000 trials with D being
uniform. The x axis is n, and the y axis is λmax. The 2-degree polynomial ﬁtting curve is 0.1282x2 +
2.392x − 32.89.

We will next consider a linear transformation P : x → P x from Rn to Rn that transforms {αj} into
an orthogonal normal basis. Then we have

λmaxG = min

θ

max
j

λmaxαT
j θ

max
i
n
(cid:88)

αjP T P θ

αjP T P αk

k=1

αjP T P αj

≥ min

θ

=

=

=

1
√
n

1
√
n
1
√
n

(24)

where λmax is the maximum eigenvalue of P T P . The row vectors of A(cid:48)(cid:48) are {α(cid:48)
problem G has a lower bound of
consists of n of the vectors in A(cid:48) and has the largest maximum eigenvalue for P T P .

i}. Therefore, our
where λmax is the maximum eigenvalue for P T P ; A(cid:48)(cid:48)

nλmax

1√

Though we do not derive the bound of λmax with respect to the number of dimension n and the angle
p0 between hyper-planes in the activation cone, we empirically evaluate the behavior of λmax with
respect to n on randomly generated data. We ﬁrst generate a normal vector n ∈ Rn, with each entry
generated independently at random from the distribution D; then we generate n vectors {α1, ..., αn}
in Rn with their entries either all not smaller than 0 or all not greater than 0; the orthant is chosen
with probability 0.5. Each entry is independently generated from D and shifted by a constant to
enforce the signs. We ensure that ∀i, αT
i n ≥ 0 by discarding the vectors that do not satisfy such
constraint. D can be uniform distribution U (0, 1), the distribution for absolute value of normal
distribution |N (0, 1)|, or beta distribution B(2, 2). The αi is then normalized, and we record the

largest eigenvalue of RT R, where A =


, A = RQ is the RQ decomposition of A. We repeat

1000 times for each n and record the mean and maximum eigenvalue for RT R. Below is the result of
our evaluation; it shows that the maximum eigenvalue λmax is approximately O(n2). See Figure 4,5,
and 6 for illustration.








αT
1
αT
2
...
αT
n

20

Figure 5: Empirical estimation of the max maximum eigenvalue over 1000 trials with D being
Gaussian. The x axis is n, and the y axis is λmax. The 2-degree polynomial ﬁtting curve is
0.2693x2 + 13.95x − 716.8.

Figure 6: The max maximum eigenvalue over 1000 trials with D being Beta(2, 2). The x axis is n,
and the y axis is λmax. The 2-degree polynomial ﬁtting curve is 0.1325x2 + 3.403x − 69.95.

B Choices of Surrogate max Functions

The ﬁnal goal of surrogate max function is to relax the objective with the term αT max(z =
Cx − d, 0), making it differentiable over Rn. Such objective is a piecewise function with respect to z
with two segments: one is constant 0 with derivative 0, the other is linear with a constant derivative.

At ﬁrst glance, sigmoidal surrogates are seemingly the most straightforward candidate for modeling
the derivative of such a piecewise function. For a sigmoidal approximation of soft constraints, S(z)
should satisfy the following four conditions, among which the ﬁrst two are compulsory, and the third
and fourth can be slightly altered (e.g. by setting (cid:15)2 = 0 and remove the fourth condition).

1. When z → ∞, S(cid:48)(z) → M − = (1 + (cid:15)1)−. (cid:15)1 > 0 should be a small amount, and it serves
as a perturbation since otherwise g(cid:48)(z) would be always greater than 0. However, (cid:15)1 should
not be too small to let the optimal point be too far away from the original constraint (for the
gradient will reach α too late).

2. S must be differentiable, and must have a closed-form inverse function.

21

Figure 7: S(z), the proposed surrogate function of max(z, 0)

3. When z → −∞, S(cid:48)(z) → (cid:15)+

2 < 0, where (cid:15)2 is constant and very close to 0. The S(cid:48)(z)
should cause as small impact as possible when z < 0 (when there is no waste), and
meanwhile it should (very slightly) encourage z to grow to the limit instead of discouraging.
4. When z = 0, S(cid:48)(z) = 0, which means the penalty function S has no inﬂuence on the
clipping border. This condition is important for linear programming; it can be removed for
quadratic programming.

1

To satisfy the conditions above, we need a closed-form sigmoidal function with closed-form inverse
function to be S(cid:48)(z), the derivative of the surrogate. Such function can be Sigmoid function (S(cid:48)(z) =
1+e−z ), Tanh/arctan function, or fractional function (S(cid:48)(z) = 0.5(
+ 1), or equivalently
z2 + 1 + 0.5z). With the form of S(z) conﬁrmed, our function should look like9
S(z) = 0.5
∂x − ∂z
L(x) = g(x, θ) − αT S(z), with the derivative ∂L

∂x S(cid:48)(z)α.

∂x = ∂g

z√

z2+1

√

Take g(x) = θT x, S(z) = 0.5

√

1 + z2 + 0.5z as an example. The optimal point satisﬁes

θ = C T diag(

0.5z

(1 + z2)0.5 + 0.5)α

(25)

0.5z
(1+z2

0.5x
(1+x2

i )0.5 + 0.5)αi, wi = (

i )0.5 + 0.5), then θ = C T y. S(z) can be substituted by
Let yi = (
any function with sigmoidal function (e.g. arctan, tanh, and sigmoid) as its derivative. However,
C is impossible to be invertible, for C includes an identity matrix representing x ≥ 0 and must
have strictly more rows than columns if C has any other constraint. Therefore, we cannot solve z
by ﬁrst solving y exactly in a linear system; we need to solve the equation of fractional function, or
estimate y with pseudo-inverse. Hence, the closed-form solution with sigmoidal surrogate is at least
non-attractive.
To derive a closed-form solution x for the equation 0 = ∂L
∂x S(cid:48)(z)α, S(cid:48)(z) should be in a
simple form (e.g. polynomial form with degrees lower or equal to 4 with respect to x). Unfortunately,
no basic function except the sigmoid function can satisfy the conditions listed above; the degree limit
makes higher-order Taylor expansion infeasible. Thus, our only choice left is to ﬁnd a differentiable
piecewise function with linear parts on both sides; we shall ﬁrst solve the optimal point of the
surrogate numerically, and extend the segment where the optimal point locates to Rn; the gradient
∂x
∂θ is correct due to the uniqueness of optimality (which requires a convex surrogate).
Therefore, we choose the function S(z) to be

∂x = ∂g

∂x − ∂z

S(z) =


0

K(z + 1

z

4K )2

if z < − 1
4K
if − 1
if z ≥ 1
4K

4K ≤ z ≤ 1

4K

(26)

9To satisfy the requirements above, the function needs scaling and translation; we omit them for the simplicity

of formulae by simply set α = 1, (cid:15)1 = (cid:15)2 = 0.

22

432101234z0.00.51.01.52.02.53.03.54.0S(z)S(z) for varing KK=0.1K=0.5K=1.0K=10.0K=100.0as illustrated in Figure 7, where K is a constant. The surrogate is chosen for its simplicity. Not only
does such choice satisfy our conditions with minimum matrix computation, but moreover, S(cid:48)(z) is
linear, which means the equation can be solved in a linear system. The limitation of such method
is that it introduces a hyper-parameter K to tune: when K is large, if the predicted optimal point is
far from ground-truth, the gradient ∂f (x,θreal)
will be too large, making the training process instable.
On the other hand, if K is small, the optimal point may deviate from the feasible region too far, as
the hard constraints are not fully enforced within the region z ∈ [−1/4K, 4K]. Fortunately, our
empirical evaluation shows that the effect of K is smooth, and can be tuned by a grid search.

∂x

C Learning and Inference Algorithm

In general, the learning and inference procedures based on our method have the same predic-
tion+optimization workﬂow as the computational graph we gave in Fig.1. In this section, we propose
the detailed algorithm with emphasis on the speciﬁc steps in our method.

Alg. 1 is the learning algorithm of stochastic gradient descent in mini-batch. For each sample (ξi, θi),
it predicts the context variables as ˆθi (Statement 3) and then solve the program f to get the optimal
decision variable as ˆx∗
i (Statement 4). Note that here f is solved by solvers such as Gurobi and
CPLEX that are capable to get the deterministic optimal solutions. With ˆx∗
i , the piece of S(z) is
determined, and thus the surrogate objective ¯f . From Statement 5 to 7, we decide the form of
surrogate ¯f for ˆx∗
i to decide Mi and Ui in the surrogate. [·] is
an element-wise indicator function. Specially, if C or d is the predicted parameter, ˆz∗
i should be
calculated by the predicted value of C or d; moreover, in statement 8 we should calculate ∂r
∂x on the
estimated segment, but with ground-truth value of C or d. In Statement 8, we compute the gradient
by back-propagation. Speciﬁcally, we directly compute ∂x
∂x by their analytical forms, while
∂θ
∂ψ by auto-grad mechanism of end-to-end learning software such as PyTorch [17], which we use
throughout our experiments. Finally, in Statement 9, we update the parameters of ψ with accumulated
gradients in this mini-batch.

i by ﬁrst calculating ˆz∗

i , then use ˆz∗

∂θ and ∂r

Algorithm 1: SGD Learning with the surrogate ¯f
Input
Input
Input
Input
Output :the predictive model Φψ with parameters ψ
begin

:dataset D = {(ξi, θi)}N
:optimization settings {A, b, C, d, α}
:derived hyper parameters {K}
:learning rate a and batch size s

i=1

Sample mini-batch Dk ∼ D;
foreach (ξi, θi) ∈ Dk do

// Estimate θi with the model Φψ.
ˆθi ← Φψ(ξi);
// Solve the original f with ˆθi.
i ← arg maxx f (x, ˆθi);
ˆx∗
// decide surrogate ¯f for ˆx∗
ˆz∗
i ← C ˆx∗
i − d;
Mi ← diag([−1/4K ≤ ˆz∗
Ui ← diag([ ˆz∗
i ≥ 1/4K]);
// Compute ∂ ˆr∗
∂ψ , where ¯r∗
× ∂ ¯r∗
∂ψ × ∂ ˆx∗
∂ ¯r∗
∂ψ ← ∂ ˆθi
i
i
i
∂ ˆx∗
∂ ˆθi
i

i ≤ 1/4K]);

i = ¯f ( ˆx∗

|θi, ˆθi, ˆx∗

i , θi)

i

i

i by calculating Mi and Ui

// Update ψ with ascent gradients
s
(cid:80)
i=1

ψ ← ψ + a × 1

∂ ¯r∗
i
∂ψ

s ×

23

1

2

3

4

5

6

7

8

9

Alg. 2 is the program with context variable prediction for inference; it is the same with the traditional
predict-then-optimize paradigm. Note that the surrogate is no longer needed in this phase.

Algorithm 2: optimization with predictive variables

i=1

:dataset D = {ξi}N
:optimization settings {A, b, C, d, α}
:derived hyper parameters {K}
:learned predictive model Φψ

Input
Input
Input
Input
Output :Solutions {( ˆx∗
begin

i , ˆr∗

i )}

1

2

3

4

foreach ξi ∈ D do

// Estimate θi with the model Φψ.
ˆθi ← Φψ(ξi);
// Solve f with the estimated ˆθi.
i ← arg maxx f (x, ˆθi);
ˆx∗
// Evaluate f with the real θi.
i ← f ( ˆx∗
ˆr∗

i , θi);

D Derivation of Gradients

In this section, we will show the derivation process of the gradient with respect to the three problems
stated in the main paper.

D.1 Linear Programming with Soft Constraints

The problem formulation is:

θT x − αT max(Cx − d, 0), s.t. Ax ≤ b

max
x

(27)

where α ∈ Rn ≥ 0, A ∈ Rm1×n ≥ 0, b ∈ Rm1 ≥ 0, and θ ∈ Rn is to be predicted. In this
formulation and the respective experiment (synthetic linear programming), we assume that there is
no equality constraint.

Let C (cid:48) =

(cid:35)

(cid:34) C
A
−I

, d(cid:48) =

(cid:35)
(cid:34)d
b
0

, γ =








α
O( n1.5E
sin p )
O( n1.5E
sin p )


, where E is the upper bound of ||θ||2, p is the

minimum angle between hyper-planes of Ax ≤ b and the axes. Then, we can write the surrogate as

γT S(z) = γT (0.5M (z +

1
4K

)2 + U z)

(28)

Calculating the derivative of θT x − γT S(z) at the optimal point, we get

θ = C (cid:48)T (M (diag(z) +

1
4K

I) + U )γ = C (cid:48)T M diag(γ)(C (cid:48)x − d(cid:48)) + C (cid:48)T (

1
4K

M + U )γ

(29)

This equation gives us the analytical solution of x:

x = (C (cid:48)T M diag(γ)C (cid:48))−1(θ + C (cid:48)T M diag(γ)d(cid:48) − C (cid:48)T (

1
4K

M + U )γ)

(30)

Based on such derivative solution of x, we differentiate with respect to θ on both sides:

∂x
∂θ

= (C (cid:48)T M diag(γ)C (cid:48))−1

(31)

On the other hand, given real parameter θreal, we calculate the derivative of f (x, θreal):

∂f (x, θreal)
∂x

= θreal − C (cid:48)T M diag(γ)(C (cid:48)x − d(cid:48)) − C (cid:48)T (

1
4K

M + U )γ

(32)

24

D.2 Portfolio Selection with Soft Constraints

We consider minimum variance portfolio ([16]) which maximizes the return while minimizes risks of
variance. The problem formulation is:

θT x − xT Qx − αT max(Cx − d, 0)

max
x

s.t. xT 1 = 1, x ≥ 0, Q ≥ 0, α ≥ 0

(33)

where x ∈ Rn is the decision variable vector – equity weights, θ ∈ Rn is the equity returns, the
semi-deﬁnite positive Q ∈ Rn×n is the covariance matrix of returns θ ∈ Rn. Equivalently, we rewrite
(cid:35)
(cid:34)−1
1
0

the constraints of x to ﬁt our surrogate framework, as Ax ≤ b where A =

(cid:34)−11×n
11×n
−I

, b =

(cid:35)

.

Let C (cid:48) =

, d(cid:48) =

, γ =

, and we have surrogate γT S(z) = γT (0.5M (Z +

(cid:21)

(cid:20)C
A

(cid:21)
(cid:20)d
b

(cid:20)
(cid:21)
α
O(n1.5E)1

1

4K )2 + U z). Then, we may derive the optimal solution x and the gradient of f (x, θreal, Qreal) with
real data with respect to x as

x = (2Q + C (cid:48)T M diag(γ)C (cid:48))−1(θ + C (cid:48)T M diag(γ)d(cid:48) − C (cid:48)T (U +

M
4K

)γ)

∂f (x, θreal, Qreal)
∂x

= θreal − 2Qrealx − C (cid:48)T M diag(γ)(C (cid:48)x − d(cid:48)) − C (cid:48)T (

1
4K

M + U )γ

Differentiating on both sides of the analytical solution of x, we get:

∂x
∂θ
∂x
∂Q

= (2Q + C (cid:48)T M diag(γ)C (cid:48))−1

= (θ + C (cid:48)T M diag(γ)d(cid:48) − C (cid:48)T (U +

M
4K

)γ)

∂(2Q + C (cid:48)T M diag(γ)C (cid:48))−1
∂Q

(34)

(35)

To derive a simpliﬁed norm of ∂x
β = θ + C (cid:48)T M diag(γ)d(cid:48) − C (cid:48)T (U + M
results to

∂Q , let R = (2Q + C (cid:48)T M diag(γ)C (cid:48))−1, S = C (cid:48)T M diag(γ)C (cid:48),
4K )γ. With such notations, we can simplify the previous

x = (2Q + S)−1β = Rβ,

= RT = R,

∂f (x, θreal, Qreal)
∂x

= βreal − R−1
realx

(36)

and the derivative ∂xi
∂Qj,k

can be derived as follows:

∂x
∂θ
and ∂f (x,θreal,Qreal)

∂Qj,k

(cid:88)

(cid:88)

∂Rx,y
∂Qj,k

∂xi
∂Rx,y

y
∂Ri,y
∂Qj,k

βy

∂xi
∂Qj,k

=

=

=

=

x

(cid:88)

y

(cid:88)

y

(cid:88)

y

(cid:88)

(cid:88)

βy

∂(2Q + S)p,q
Qj,k

∂Ri,y
∂(2Q + S)p,q

p

q
∂Ri,y
∂(2Q + S)j,k

2βy

(cid:88)

= −

2βyRi,jRk,y

y

∂f (x, θreal, Qreal)
∂Qj,k

= −

(cid:88)

(cid:88)
(

2βyRi,jRk,y)(βreal,i − (R−1

realx)i)

i
(cid:88)

y
((R−1

realx)i − βreal,i)Ri,j

= 2

(cid:88)

βyRk,y

(37)

(38)

(39)

Let pj = (cid:80)

i RT

j,i((R−1

i

realx)i − βreal,i), tk = (cid:80)
∂f (x, θreal, Qreal)
∂Qj,k

y βyRk,y (p = RT (R−1
∂f (x, θreal, Qreal)
∂Q

= 2pjxk,

= 2pxT

y
realx − βreal), t = x), and ﬁnally

25

D.3 Resource Provisioning

The problem formulation of resource provisioning is

min
x

1 max(Cx − d, 0) + αT
αT

2 max(d − Cx, 0), s.t. xT 1 = 1, x ≥ 0

(40)










Let C (cid:48)

C
−C
−I
11×n
−11×n
(C (cid:48)T M diag(γ)d(cid:48) − C (cid:48)T ( M
∂f (x,C(cid:48)

, d(cid:48) =






real)















d
−d
0
1
−1

, γ =








α1
α2
O(n1.5E)1
O(n1.5E)
O(n1.5E)



, P = (C (cid:48)T M diag(γ)C (cid:48))−1, β =






4K + U )γ), η = M diag(γ)d(cid:48) − ( M

4K + U )γ. Then for the derivative

∂x

and the analytical solution of x, we have

∂f (x, C (cid:48)

∂x

real)

= −C (cid:48)T

realMrealdiag(γ)(C (cid:48)

1
4K
x = (C (cid:48)T M diag(γ)C (cid:48))−1(C (cid:48)T M diag(γ)d(cid:48) − C (cid:48)T (

M
4K
According to the analytical solution of optimal point x, the derivative ∂xi
∂C(cid:48)

realx − d(cid:48)) − C (cid:48)T

real(

is

k,l

Mreal + Ureal)γ

+ U )γ) = P β

∂xi
∂C (cid:48)

k,l

(cid:88)

=

j

∂Pi,j
∂C (cid:48)

k,l

βj +

(cid:88)

Pi,j

j

∂βj
∂C (cid:48)

k,l

For the ﬁrst term of the derivative above, we have:

(41)

(42)

∂Pi,j
∂C (cid:48)

k,l

=

=

=

=

=

∂(C (cid:48)T M diag(γ)C (cid:48))−1
i,j
C (cid:48)
∂(C (cid:48)T M diag(γ)C (cid:48))p,q
∂C (cid:48)

(cid:88)

(cid:88)

k,l

p

q

Pi,j
P −1
p,q

k,l
x,pM diag(γ)x,yC (cid:48)
y C (cid:48)
∂C (cid:48)

k,l

y,q

(−Pi,pPq,j)

(cid:88)

(cid:88)

∂ (cid:80)
x

(cid:80)

p
(cid:88)

q
(cid:88)

[p == l]M diag(γ)k,yC (cid:48)

y,q(−Pi,pPq,j) +

y
−Pi,l(M diag(γ))k,∗C (cid:48)

∗,qPq,j +

q
(cid:88)

q

(cid:88)

p

(cid:88)

(cid:88)

p

x

[q == l]M diag(γ)x,kC (cid:48)T

p,x(−Pi,pPq,j)

−Pl,jC (cid:48)T

p,∗(M diag(γ))∗,kPi,p

= −(Pi,l(M diag(γ))k,∗C (cid:48)P∗,j + Pl,jPi,∗C (cid:48)T (M diag(γ))∗,k)
= −(Pi,l(M diag(γ)C (cid:48)P )k,j + Pl,j(P C (cid:48)T M (diag(γ)))i,k)

Therefore, the simpliﬁed result for the ﬁrst term of the derivative in Equation 42 is:

(cid:88)

j

∂Pi,j
∂C (cid:48)

k,l

βj = −((M diag(γ)C (cid:48)P β)kPi,l + (P β)l(P C (cid:48)T M diag(γ))i,k)

For the second term, we have:

∂βj
∂C (cid:48)

k,l

=

∂(C (cid:48)T η)j
C (cid:48)

k,l

=

The simpliﬁed second term is thus

∂ (cid:80)

p,jηp

p C (cid:48)
∂C (cid:48)

k,l

= ηk[j == l]

(cid:88)

Pi,j

j

∂βj
∂C (cid:48)

k,l

= Pi,lηk

(43)

(44)

(45)

(46)

Finally, the derivative can be written as

∂xi
∂C (cid:48)

k,l

= −((M diag(γ)C (cid:48)P β)kPi,l + (P β)l(P C (cid:48)T M diag(γ))i,k) + Pi,lηk

(47)

26

E Benchmark Details : Dataset and Problem Settings

Our code is public in the repo: https://github.com/PredOptwithSoftConstraint/PredOptwithSoftConstraint.

E.1 Synthetic Linear Programming

E.1.1 Prediction Dataset

We generate the synthetic dataset {ξi, θi}N
original form, it is like:

i=1 under a general structural causal model ([21]). In the

z ∼ N (0, Σ)
ξ = g(z) + (cid:15)1
θ = h(z) + (cid:15)2

(48)

where z is the latent variable, ξ observed features of z, and θ the result variable caused by z.
According to physical knowledge, h can be a process of linear, quadratic, or bi-linear form. However,
it is difﬁcult to get an explicit form of reasonable g. Instead, g−1 can be well-represented by deep
neural networks.

Thus, alternatively, we use the following generative model:

ξ∗ ∼ N (0, Σ)
z = m(ξ∗)
θ = h(z) + (cid:15)2
ξ = ξ∗ + (cid:15)1

(49)

where m behaves as g−1. In our experiment settings, Σ = I + QQT , where each element of Q
is generated randomly at uniform from (0, 1). We set m(x) = sin(2πxB), where B is a matrix
whose elements are generated randomly at uniform in {0, 1}, and sin is applied element-wisely. We
implement h(z) as a MLP with two hidden layers, and the output is normalized to (0, 1] for each
dimension through different data points. Finally, we add a noise of 0.01(cid:15)x to x, where (cid:15)x ∼ N (0, 1);
and 0.01(cid:15)θ to θ, where (cid:15)θ follows a truncated normal distribution which truncates a normal distribution
N (0, 1) to [0, 1.5].

The dataset is split into training, validation and test sets with the proportions 50%, 25%, 25% in
respect. The batch size is set to 10 for N = 100, 50 for N = 1000, and 125 for N = 5000.

E.1.2 Problem Settings

We generate hard constraint Ax ≤ b, and soft constraint Cx ≤ d. Each element of A or C is ﬁrst
generated randomly at uniform within (0, 1), then set to 0 with probability of 0.5. We generate b, d
as b = 0.5A1 and d = 0.25C1. The soft constraint coefﬁcient α is generated randomly at uniform
from (0, 0.2) for each dimension.

E.2 Portfolio Optimization

E.2.1 Dataset

The prediction dataset is daily price data of SP500 from 2004 to 2017 downloaded by Quandl API
[24] with the same settings in [3]. Most settings are aligned with those in [3], including dataset
conﬁguration, prediction model, learning rate (initial 0.01 with scheduler), optimizer (Adam), gradient
clip (0.01), the number of training epochs (20), and the problem instance size (the number of equities
being {50, 100, 150, 200, 250}).

E.2.2 Problem Settings

We set the number of soft constraints to 0.4 times of n, where n is the number of candidate equities.
For the soft constraint αT max(Cx − d, 0), α = 15
n v, where each element of v is generated randomly
at uniform from (0, 1); the elements of matrix C are generated independently from {0, 1}, where the
probability of 0 is 0.9 and 1 is 0.1. K is set as 100.

27

E.3 Resource Provisioning

E.3.1 Dataset

The ERCOT energy dataset [25] contains hourly data of energy output from 2013 to 2018 with
52535 data points. We use the ﬁrst 70% as the training set, the middle 10% as the valida-
tion set, and the last 20% as the test set. We normalize the dataset by dividing the labels
by 104, which makes the typical label becomes value around (0.1, 1). We train our model
with a batch size of 256; each epoch contains 144 batches. We aim to predict the matrix
C ∈ R24×8, where 24 represents the following 24 hours and 8 represents the 8 regions, which
are {COAST, EAST, FWEST, NCENT, NORTH, SCENT, SOUTH, WEST}. The data is drawn
the decision variable x is 8-dimensional, and
from the dataset of the corresponding region.
d = 0.51 + 0.1N (0, 1).

E.3.2 Problem Settings

We test ﬁve sets of (α1, α2), which are (50 × 1, 0.5 × 1), (5 × 1, 0.5 × 1), (1, 1), (0.5 × 1, 5 × 1),
and (0.5 × 1, 50 × 1), against two-stage with L1-loss, L2-loss, and weighted L1-loss. We use
AdaGrad as optimizer with learning rate 0.01, and clip the gradient with norm 0.01. The feature is
a (8 × 24 × 77)-dimensional vector for each matrix C; we adopted McElWee’s Blog [26] for the
feature generation and the model. Weighted L1-loss has the following objective:
2 max(Cx − d, 0) + αT
αT

1 max(d − Cx, 0)

(50)

Note that α2 and α1 are exchanged in the objective. Intuitively, this is because an under-estimation of
entries of C will cause a larger solution of x, which in turn makes Crealx − d larger at test time, and
vice versa. Another thing worth noting is that SPO+ cannot be applied to the prediction of the matrix
C or vector d, for it is designed for the scenario where the predicted parameters are in the objective.

F Supplementary Experiment Results

F.1 Linear Programming with Soft Constraints

The effect of K. Table 4 provides the mean and standard deviation of regrets under varying values
of K. we can see that our method performs better than all other methods with most settings of K.

Empirically, to ﬁnd an optimal K for a given problem and its experimental settings, a grid search
with roughly adaptive steps sufﬁces. For example,in our experiments, we used a proposal where
neighbouring coefﬁcient has 5x difference (e.g. {0.2, 1, 5, 25, 125}) works well. Quadratic objective,
as in the second experiment, usually requires larger K than linear objective as in the ﬁrst and the
third experiment.

Regret

N
100

1000

5000

Problem Size
(40, 40, 0)
(40, 40, 20)
(80, 80, 0)
(80, 80, 40)
(40, 40, 0)
(40, 40, 20)
(80, 80, 0)
(80, 80, 40)
(40, 40, 0)
(40, 40, 20)
(80, 80, 0)
(80, 80, 40)

K = 1.0
2.378±0.293
2.574±0.254
5.529±0.307
4.747±0.470
1.387±0.130
1.553±0.114
3.431±0.100
2.817±0.167
1.065±0.098
1.248±0.076
2.845±0.064
2.222±0.111
Table 4: The mean and standard deviation of the regret of all K in our experiment.

ours(K = 0.2)
2.423±0.305
2.530±0.275
5.200±0.506
4.570±0.390
1.379±0.134
1.587±0.112
3.617±0.125
2.781±0.165
1.041±0.094
1.278±0.072
3.074±0.098
2.217±0.123

K = 25.0
2.301±0.325
2.321±0.272
5.563±0.265
4.780±0.450
1.346±0.144
1.507±0.102
3.513±0.097
2.820±0.157
1.038±0.102
1.220±0.071
2.889±0.083
2.174±0.113

K = 5.0
2.265±0.238
2.384±0.277
5.610±0.351
4.758±0.531
1.365±0.147
1.523±0.109
3.540±0.105
2.819±0.155
1.045±0.099
1.223±0.075
2.884±0.100
2.172±0.098

K = 125.0
2.258±0.311
2.350±0.263
5.502±0.353
4.796±0.624
1.352±0.140
1.506±0.102
3.507±0.117
2.826±0.197
1.037±0.100
1.221±0.074
2.864±0.086
2.177±0.105

Prediction error. Table 5 and Table 6 are the mean and standard deviation of prediction MSE for
all methods. We can ﬁnd that two-stage methods have an advantage over end-to-end methods on

28

MSE of θ

1000

N
100

Problem Size
(40, 40, 0)
(40, 40, 20)
(80, 80, 0)
(80, 80, 40)
(40, 40, 0)
(40, 40, 20)
(80, 80, 0)
(80, 80, 40)
(40, 40, 0)
(40, 40, 20)
(80, 80, 0)
(80, 80, 40)

L2
0.063±0.008
0.062±0.006
0.059±0.004
0.059±0.004
0.031±0.001
0.031±0.001
0.029±0.001
0.029±0.001
0.022±0.000
0.022±0.000
0.021±0.000
0.020±0.000
Table 5: The mean and standard deviation of the MSE loss of the predicted variable θ in the synthetic
experiment on the test set. The two-stage methods have better performance on the accuracy of
prediction.

DF
14.592±9.145
6.414±4.451
28.768±13.90
8.98± 9.99
34.931±14.309
9.255±8.497
90.590±35.298
1.060±2.395
72.083±46.685
235.789±842.939
147.144±51.101
0.372±0.311

L1
0.062±0.006
0.061±0.005
0.058±0.004
0.059±0.004
0.031±0.001
0.031±0.001
0.030±0.001
0.030±0.001
0.022±0.000
0.022±0.000
0.021±0.000
0.020±0.000

SPO+
0.079±0.150
0.071±0.006
0.066±0.011
0.085±0.008
0.034±0.002
0.033±0.001
0.032±0.002
0.033±0.001
0.025±0.001
0.238±0.001
0.023±0.001
0.022±0.001

ours (K = 0.2)
0.094±0.039
0.061±0.007
0.247±0.009
0.061±0.005
0.036±0.003
0.033±0.001
0.222±0.009
0.033±0.002
0.025±0.001
0.243±0.001
0.196±0.011
0.024±0.001

5000

MSE of θ

N
100

1000

5000

Problem Size
(40, 40, 0)
(40, 40, 20)
(80, 80, 0)
(80, 80, 40)
(40, 40, 0)
(40, 40, 20)
(80, 80, 0)
(80, 80, 40)
(40, 40, 0)
(40, 40, 20)
(80, 80, 0)
(80, 80, 40)

ours(K = 1.0)
0.068±0.008
0.094±0.031
0.129±0.049
0.094±0.013
0.036±0.002
0.060±0.010
0.114±0.070
0.039±0.002
0.026±0.001
0.036±0.004
0.152±0.079
0.026±0.001

ours(K = 5.0)
0.091±0.037
0.204±0.074
0.257±0.182
0.151±0.035
0.056±0.010
0.127±0.051
0.220±0.097
0.060±0.006
0.033±0.004
0.073±0.019
0.097±0.040
0.032±0.003

ours(K = 25.0)
0.188±0.089
0.244±0.105
0.641±0.476
0.172±0.091
0.143±0.048
0.172±0.071
1.124±0.341
0.078±0.010
0.076±0.025
0.110±0.038
0.703±0.177
0.038±0.004

ours(K = 125.0)
0.204±0.079
0.343±0.200
0.816±0.351
0.177±0.042
0.195±0.095
0.193±0.078
1.407±0.439
0.080±0.008
0.089±0.037
0.113±0.026
0.895±0.205
0.040±0.007

Table 6: The mean and standard deviation of the MSE loss of the predicted variable θ in the synthetic
experiment for different values of K.

prediction error. however, as demonstrated in our experiments, such advantage does not necessarily
lead to the advantage on the regret performance. Surprisingly, although DF[9] works generally on
par with other baselines in regret performance, it generates a very large MSE error compared with
our method; closer examination suggests that there are some outlier cases where the prediction
loss is magnitudes higher than others. Also, the prediction error of DF gets much higher when no
soft constraint exists, which is probably because the optimal solution remains the same when all
parameters to predict are scaled by a constant factor.

Detailed behaviour of SPO+. As mentioned in the main paper, SPO+ quickly becomes overﬁted in
our experiments, shown by Table 7 as empirical evidence. In our experiments, early stopping starts at
epoch 8, and maximum number of run epochs is 40; the results show that SPO+ stops much earlier
than other methods under most settings. Figure 8 plots the timely comparison of test performance
during training, where SPO+ get overﬁted quickly, although it performs better in the earlier period of
training.

Statistical signiﬁcance tests. Table 9 shows the results of signiﬁcance test (one-tailed paired t-test),
with the assumption that the means of two distributions are the same. The results show that our
method is signiﬁcantly better than other methods under almost all settings.

29

Figure 8: The average regret on test set of K = 25.0, training set size 5000 and problem size
(80, 80, 0) with respect to the number of epochs (methods other than ours does not reach 40 in this
ﬁgure, for all runs are early-stopped before epoch 40). While SPO+ is better than two-stage in the
best performance, it overﬁts rather quickly.

Average Episode

N
100

1000

5000

Problem Size
(40, 40, 0)
(40, 40, 20)
(80, 80, 0)
(80, 80, 40)
(40, 40, 0)
(40, 40, 20)
(80, 80, 0)
(80, 80, 40)
(40, 40, 0)
(40, 40, 20)
(80, 80, 0)
(80, 80, 40)

L1
20.46±10.20
21.47±9.58
22.6±5.78
25.2±8.38
14.67±2.61
13.2±2.37
19.07±2.25
19.93±2.66
16.47±2.59
17.13±2.59
20.67±3.20
20.47±2.64

L2
12.53±3.00
14.13±4.47
22.6±8.52
24.73±8.88
13.2±1.74
13.4±2.50
18.07±1.83
19±1.31
16.27±1.33
15.27±1.79
20.8±2.18
21±2.88

SPO+
19.6±8.58
17.8±9.98
20.07±7.23
19.33±764
10±2.04
10.33±1.40
11.87±2.00
12.33±1.95
11.87±2.10
12.8±2.88
13.7±2.16
12.2±1.61

DF
29.53±10.24
25.46±11.41
30.87±8.83
24±9.06
27.93±7.42
24.6±8.14
25.2±6.41
25.4±9.22
23.33±7.34
21.67±7.95
29.33±7.22
17.6±6.84

ours (K = 0.2)
20.53±10.39
22.47±10.29
24.53±8.02
28.33±8.89
13.6±2.53
10.67±1.23
24.27±11.02
17.73±4.08
11.8±2.60
13.4±2.77
30.07±9.48
18.53±5.26

Table 7: The mean and standard deviation of the number of epochs run in each instance (capped at
40). Note that SPO+ is signiﬁcantly more prone to overﬁtting in our experiment settings.

F.2 Portfolio Optimization

Statistical signiﬁcance tests. Table 10 shows the result of signiﬁcance test (one-tailed paired t-test),
with the assumption that the means of two distributions are the same. The results show that our
method is signiﬁcantly better than other methods under every setting.

F.3 Resource Provisioning

Statistical signiﬁcance tests. Table 11 shows the result of signiﬁcance test (one-tailed paired t-test),
with the assumption that the means of two distributions are the same.

G Computing Infrastructure

All experiments are conducted on Linux Ubuntu 18.04 bionic servers with 256G memory and 1.2T
disk space with no GPU, for GPU does not suit well with Gurobi.10 Each server has 32 CPUs, which
are Intel Xeon Platinum 8272CL @ 2.60GHz.
For the ﬁrst experiment, we use a few minutes to get one set of data 11 with training set size 100, and
about 4 − 5 hours to get one set of data with training set size 5000. For the second experiment, we use

10see Gurobi’s ofﬁcial support website: https://support.gurobi.com/hc/en-us/articles/360012237852-Does-

Gurobi-support-GPUs-

11Running all methods simultaneously under one particular parameter setting.

30

N
100

1000

5000

Problem Size
(40, 40, 0)
(40, 40, 20)
(80, 80, 0)
(80, 80, 40)
(40, 40, 0)
(40, 40, 20)
(80, 80, 0)
(80, 80, 40)
(40, 40, 0)
(40, 40, 20)
(80, 80, 0)
(80, 80, 40)

L1
3.884
5.173
6.024
2.308
8.438
6.547
12.620
7.116
7.130
10.410
8.361
6.550

t-value

L2
4.615
5.248
5.661
1.724
5.271
6.212
10.925
4.465
7.808
10.238
7.502
4.388

SPO+
5.194
6.836
4.523
3.622
4.860
8.562
7.979
6.223
7.546
6.555
6.874
5.066

DF
1.614
1.569
3.610
1.936
1.115
2.620
2.751
8.230
1.175
2.382
0.869
11.620

Table 8: The t-value of the one-tailed paired t-test between all other methods and our methods under
optimal K.

N
100

1000

5000

Problem Size
(40, 40, 0)
(40, 40, 20)
(80, 80, 0)
(80, 80, 40)
(40, 40, 0)
(40, 40, 20)
(80, 80, 0)
(80, 80, 40)
(40, 40, 0)
(40, 40, 20)
(80, 80, 0)
(80, 80, 40)

L1
8.272 ×10−4
7.076×10−5
1.561×10−5
0.018
3.662×10−7
6.489×10−6
2.444×10−9
2.600×10−6
2.545×10−6
2.834×10−8
4.082×10−7
6.460×10−6

p-value

L2
2.006×10−4
6.162×10−5
2.936×10−5
0.053
5.917×10−5
1.133×10−5
1.544×10−8
2.667×10−4
9.074×10−7
3.487×10−8
1.435×10−6
3.097×10−4

SPO+
6.809 ×10−5
4.054×10−6
2.388×10−4
0.001
1.263 ×10−4
3.078×10−7
7.064×10−7
1.114×10−5
1.342×10−6
6.406×10−6
3.818×10−6
8.606×10−5

DF
0.058
0.064
5×10−4
0.032
0.137
0.007
0.005
2.937×10−9
0.125
0.012
0.196
1.585×10−12

Table 9: The p-value of the one-tailed paired t-test between all other methods and our methods under
optimal K.

about an hour to get one set of data with N = 50, and 2 − 3 days to get one set of data with N = 250.
For the third experiment, we use around 6 − 7 hours to obtain one set of data. Though our method is
slower to train than two-stage methods, it is 2-3x faster to train than KKT-based decision-focused
method.

31

#Equities
50
100
150
200
250

L1
10.329
21.414
13.402
13.617
13.904

t-value
L2
10.628
23.995
14.503
13.104
14.483

DF
6.141
6.200
5.690
4.329
3.205

L1
3.213×10−8
2.125×10−12
1.119×10−9
9.086×10−10
6.915×10−10

p-value
L2
2.186×10−8
4.495×10−13
3.971×10−10
1.500×10−9
4.044×10−10

DF
1.279×10−5
1.157×10−5
2.789×10−5
3.465×10−4
0.003

Table 10: The t-value and p-value of the one-tailed paired t-test between all other methods and our
methods.

α1/α2
100
10
0.1
0.01

L1
13.133
12.484
1.184
0.535

t-value
L2
9.357
14.231
5.073
1.998

Weighted L1
6.429
10.897
11.001
5.083

L1
2.914×10−9
5.622×10−9
0.127
0.300

p-value
L2
2.115×10−7
1.493×10−6
8.486 ×10−5
0.032

Weighted L1
1.576×10−5
3.190 ×10−8
1.416 ×10−8
8.33×10−5

Table 11: The t-value (p-value) of the one-tailed paired t-test between all other methods and our
methods.

32

