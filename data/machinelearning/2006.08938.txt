JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

Online Reinforcement Learning Control by Direct
Heuristic Dynamic Programming: from
Time-Driven to Event-Driven

Qingtao Zhao, Jennie Si, Fellow, IEEE, and Jian Sun, Member, IEEE

0
2
0
2

n
u
J

6
1

]

Y
S
.
s
s
e
e
[

1
v
8
3
9
8
0
.
6
0
0
2
:
v
i
X
r
a

Abstract‚ÄîIn this paper time-driven learning refers to the
machine learning method that updates parameters in a prediction
model continuously as new data arrives. Among existing approx-
imate dynamic programming (ADP) and reinforcement learning
(RL) algorithms, the direct heuristic dynamic programming
(dHDP) has been shown an effective tool as demonstrated in
solving several complex learning control problems. It continu-
ously updates the control policy and the critic as system states
continuously evolve. It is therefore desirable to prevent the time-
driven dHDP from updating due to insigniÔ¨Åcant system event
such as noise. Toward this goal, we propose a new event-driven
dHDP. By constructing a Lyapunov function candidate, we prove
the uniformly ultimately boundedness (UUB) of the system states
and the weights in the critic and the control policy networks.
Consequently we show the approximate control and cost-to-go
function approaching Bellman optimality within a Ô¨Ånite bound.
We also illustrate how the event-driven dHDP algorithm works
in comparison to the original time-driven dHDP.

Index Terms‚Äîreinforcement learning (RL), direct heuristic
dynamic programming (dHDP), event-driven/time-driven dHDP.

I. INTRODUCTION

I N this paper time-driven learning refers to the machine

learning method that updates parameters in a prediction
model continuously as new data arrives. While important
applications such as autonomous robots and intelligent agents
on the web or in mobile devices require on-the-Ô¨Çy and
continuous adaptation to environment changes, catastrophic
forgetting [1], [2] can occur and thus drastically disrupt
prediction performance. From a human learning perspective,
such periodic, continuous update of the prediction model
does not best represent biological learning. Long established
theory based on extensive experimental data shows that short-
term memory decays very slowly when the environment is
undisturbed, but decays amazingly fast otherwise. In machine
learning, disruption of a learned model by new learning is a
recognized feature of neural networks [2]‚Äì[8].

Forgetting after learning has plagued the machine learning
Ô¨Åeld for many years and it has attracted attention from machine
learning researchers who seek scalable and effective learning
methods. A notable such progress was reported in [9] where
several Atari 2600 games were learned sequentially by elastic

Q. Zhao and J. Sun are with the Key Laboratory of Intelligent Control
and Decision of Complex System, Beijing Institute of Technology, Beijing,
100081, China (e-mail:zhaoqingtaophx@163.com; sunjian@bit.edu.cn).

J. Si is with the Department of Electrical, Computer, and Energy Engineer-
ing, Arizona State University, Tempe, AZ 85287 USA (e-mail: si@asu.edu).
The research of J. Si is supported in part by NSF #1563921 and #1808752.

weight consolidation (EWC) algorithm. To learn a new task,
EWC tempers the network parameters based on previous
task(s): it enables fast learning rates on parameters that are
poorly constrained by the previous tasks and slow learning
rates for those that are crucial. Incremental learning is an
idea of triggering learning only if new patterns are sufÔ¨Åciently
the
different from previous ones [10], [11]. For example,
Learn++ algorithm is to make the distribution update rule
contingent on the ensemble error instead of the previous hy-
pothesis error to allow for efÔ¨Åcient incremental learning of new
data that may introduce new classes. In [7], learning without
forgetting (LwF) was proposed by employing convolutional
neural networks. Given a set of shared weights across all tasks,
it optimizes the shared weights, old task weights and new
task weights simultaneously based on an objective function
including the predicted errors of old tasks, new tasks and a
regulation term. Similarly, to alleviate catastrophic forgetting,
Zenke et al. allowed individual synapses to estimate their
importance for learned task. When the weights learn a new
task, the cost-to-go function is modiÔ¨Åed by adding a penalty
term of the summed parameter error functions of all previous
tasks to reduce large changes in important parameters [8].

While the above discussions are pertinent to supervised
or unsupervised learning, forgetting after learning can also
plague RL or ADP. The problem has received some attention.
In [12] and [13],
the authors established a framework to
update neural network weights in an event-triggered manner
while the weight convergence and system stability were still
guaranteed. But the results were obtained for nonlinear afÔ¨Åne
systems. Additionally, an observer network was required in
[12] and a system identiÔ¨Åer network was required in [13]. In
[14]‚Äì[16], the authors considered a partially unknown afÔ¨Åne
nonlinear system. They showed the weight convergence of the
neural networks and controlled system stability while taking
into account control
input constraints and under different
event triggering conditions. Also dealing with nonlinear afÔ¨Åne
systems, [17]‚Äì[21] take into account an internal uncertainty or
an external disturbance by the proposed event-triggered robust
control using ADP structures. A different event triggering
condition based on value functions was proposed in [22] for
continuous-time nonlinear systems using ADP. The conver-
gence of the performance index was guaranteed while the
system stability and weight boundedness were also considered.
But such triggering condition is indirect and its dependence
on value function may result in false triggers as value function
approximation errors vary, sometimes to a large degree.

 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

2

Among different ADP algorithms [23], [24], heuristic dy-
namic programming (HDP) provides a basic structural frame-
work of adaptive critic designs to approximate the cost-to-go
function and the optimal control policy at the same time [25].
Such design approach has shown great promise to ameliorate
the curse of dimensionality in dynamic programming. Among
HDP algorithms, the dHDP [26] is an online continual learning
method that is intuitive and was motivated to be scalable
and effective in solving realistic, large scale decision and
control problems. Since its introduction, it has been applied
to some complex control problems such as Apache helicopter
stabilization and tracking control [27], [28], damping low
frequency oscillation in large power systems [29], and most
recently, robotic prosthesis control tested on amputee sub-
jects [30]. However as previously described, we also noticed
parameter drift due to continuous updates of learned neural
networks [30], [31]. To avoid learning from undesired sensor
and actuator noise as well as external disturbance, the control
designs included a termination condition on the action and
the critic network after sufÔ¨Åcient learning of the control tasks
during real life experimentation.

It

is therefore desirable to have a systematic RL con-
trol method that would only update a prediction model by
learning events but not by uncertain events or
signiÔ¨Åcant
noise. And preferably, such learning should not (signiÔ¨Åcantly)
compromise the performance of the learned model. In this
paper, we aim to develop an easy-to-implement event-based
dHDP to update the policy and critic network weights driven
by events directly reÔ¨Çected in system states.

Our proposed work differs from and introduces new re-
sults beyond [12]‚Äì[22]. Almost all previous event-triggered
ADP algorithms were developed for continuous-time systems
including [12]-[22], which also only deal with nonlinear
dynamics that can be described as afÔ¨Åne systems. Among the
few existing works for discrete-time systems, Sahoo et al. [32]
proposed a near optimal event-driven control for nonlinear
systems using an ADP structure under some assumptions.
However, this method was again, only applicable for afÔ¨Åne
systems and an identiÔ¨Åer neural network was also necessary.
More recently, Ha et al. [33] investigated an event based
controller for afÔ¨Åne discrete-time systems with constrained
inputs. The stability of the systems was guaranteed with
Lyapunov analysis tools but the approximation errors of the
neural networks were ignored. Additionally, a third model
neural network was needed. In [34], the authors proved the
convergence of the weights for general discrete-time systems
in an input to state stable framework but a model neural
network was still needed.

In this paper, our event-driven dHDP algorithm and its
analysis represents new contributions to the important problem
of event-driven ADP in the following aspects.

‚Ä¢ Compared with previous works [12]‚Äì[22], [32], [33],
which require the nonlinear dynamics under consideration
to be described as afÔ¨Åne systems, our event-driven dHDP
is applicable to general, discrete-time nonlinear systems.
‚Ä¢ Compared with [32]‚Äì[34] which require an identiÔ¨Åer
neural network in the controller design, our event-driven
dHDP directly learns from data without the requirement

of learning a dynamic system model either online or
ofÔ¨Çine. Consequently the algorithm is easy to implement.
Our previously demonstrated successes of time-driven
dHDP in several important applications [27]‚Äì[31] speaks
to the importance of an easy-to-implement algorithm.
‚Ä¢ In addition to introducing a new, event-driven dHDP
algorithm in this paper, we show the UUB of the neural
network weights, the approximate solution to the Bellman
equation approaching that of the Bellman optimality
equation, and the stability of the controlled system. Our
work is one of the few works that consider discrete-time
nonlinear systems. Compared to those existing works
addressing discrete-time nonlinear systems [32]‚Äì[34], we
require fewer and/or milder assumptions. As such, our
algorithm provides additional ease of implementation in
applications and makes our results less conservative.

II. PROBLEM FORMULATION

Consider a general nonlinear discrete-time system with

unknown dynamics

x(k + 1) = f (x(k), u(k)),

(1)

where x ‚àà Rm is the system state and u ‚àà Rn system input.
Assumption 1: System (1) is controllable. x(k) = 0 is a

unique equilibrium point and f (0, 0) = 0.

In time-driven ADP, a control input u(k) is generated at
each sampling time k as a function of system state x(k).
In event-driven ADP, which is considered in this paper, the
control input is updated only when there is a driving event
reÔ¨Çected in system states going beyond a threshold. As such,
the control inputs will be kept constant in a zero-order holder
(ZOH) after a driving event time instant. We deÔ¨Åne the event
indices as {Œ¥k}‚àû
k=0 (Œ¥0 = 0). The control law can then be
represented as

u(k) = u(Œ¥k), ‚àÄŒ¥k (cid:54) k < Œ¥k+1.

We introduce the event-driven state error as

e(k) = x(Œ¥k) ‚àí x(k), ‚àÄŒ¥k (cid:54) k < Œ¥k+1.

Then system (1) can be rewritten as

x(k + 1) = f (x(k), u(e(k) + x(k)).

(2)

(3)

(4)

Let the event instants be determined by the following condition
(cid:107) e(k) (cid:107)(cid:54) eT ,

(5)

where eT is a time varying threshold variable that will be
investigated in next section.

Consider the following cost-to-go function with event-

driven control

V (x(k)) =

where

‚àû
(cid:88)

j=k

r(x(j), u(x(j) + e(j))),

r(x(k), u(x(k) + e(k)) = r(x(k), u(x(Œ¥k))
= xT (k)Qx(k) + uT (x(Œ¥k))Ru(x(Œ¥k)).

(6)

(7)

In the above equation, both Q and R are positive deÔ¨Ånite
matrices whose maximum eigenvalue and minimum eigen-
value are Œªmax(¬∑) and Œªmin(¬∑), respectively. For simplicity,
we denote r(x(k), u(x(Œ¥k)) as r(k).

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

3

Note that the V (x(k)) reÔ¨Çects a measure of the performance

index at state x(k) and satisÔ¨Åes the Bellman equation

V (x(k)) = r(k) + V (x(k + 1)).

(8)

From Bellman‚Äôs optimality principle, the optimal cost-to-go

function is therefore

V ‚àó(x(k)) = min

{r(k) + V ‚àó(x(k + 1))}.

u(x(Œ¥k))

ÀÜœâc(k + 1) =

(9)

Under the event-driven learning mechanism (2), learning and
adaptation takes place only at event instants, i.e., k = Œ¥k and
the control input holds as a constant during the event intervals.
Then the critic network weights are updated as

Ô£±
Ô£¥Ô£≤

Ô£¥Ô£≥

ÀÜœâc(k) ‚àí lcœÜc(k)[ÀÜœâT
‚àíÀÜœâT
ÀÜœâc(k),

c (k ‚àí 1)œÜc(k ‚àí 1)]T , k = Œ¥k

c (k)œÜc(k) + r(k ‚àí 1)

Œ¥k < k < Œ¥k+1, (14)

where lc denotes the learning rate of the critic network.

Similar to [26], the action network weights are adjusted to

minimize the following approximation error:

Ea(k) =

eT
a (k)ea(k)
2

,

ea(k) = ÀÜV (x(k)).

(15)

(16)

Similar to the critic network, the action network weights are
updated as follows considering that the weight updates are
driven by signiÔ¨Åcant events in the system states.

ÀÜœâa(k + 1) =

Ô£±
Ô£¥Ô£≤

Ô£¥Ô£≥

ÀÜœâa(k) ‚àí laœÜa(k)[ÀÜœâT
√ó[ÀÜœâT
c (k)œÜc(k)]T ,
ÀÜœâa(k),

c (k)C(k)]

k = Œ¥k

Œ¥k < k < Œ¥k+1, (17)

Fig. 1. The relationship between event time instants Œ¥k and general time k is
shown in the upper left corner. As Eq. (2) and Eq. (3) show, only at event time
instants Œ¥k, the state x(k) = x(Œ¥k) is used to update u(Œ¥k). At other time
instants, the ZOH element provides the needed control input to interact with
the system. Correspondingly as shown in Eq. (14) and Eq. (17), the weights
in the critic and action networks learn at event time instants k = Œ¥k and
remain unchanged without triggering event. The critic network approximates
the cost-to-go as a function of x(Œ¥k) and u(Œ¥k), while the action network
approximates the input control as a function of x(Œ¥k).

To achieve event-driven learning control by dHDP, we adopt
the architecture as shown in Fig. 1, where both the critic
network and the action network are universal approximating
neural networks. The nonlinear thresholding functions œÜc(.)
and œÜa(.) in the hidden layer are hyperbolic tangents. We use
œÑc(k), œÑa(k) and œâc(k), œâa(k) to denote the input-to-hidden
layer weights and the hidden-to-output layer weights for the
critic and the action networks, respectively.

Given Fig. 1, we consider the approximate cost-to-go func-
tion ÀÜV (x(k)) and the approximate input ÀÜu(k) of the following
form,

ÀÜV (x(k)) = ÀÜœâT
ÀÜu(k) = ÀÜœâT

c (k)œÜc(k),
a (k)œÜa(k).

(10)

(11)

In this paper, the input-to-hidden layer weights œÑc(k), œÑa(k)
are chosen initially at random and kept constants as was the
case in [35] and only the output layer weights œâc(k), œâa(k)
are updated during learning. As shown in [35], such neural
networks can be universally approximating.

The critic network weights are updated in order to minimize

the following approximation error,

Ec(k) =

eT
c (k)ec(k)
2

,

ec(k) = ÀÜV (x(k)) ‚àí [ ÀÜV (x(k ‚àí 1) ‚àí r(k ‚àí 1)].

(12)

(13)

where la denotes the learning rate of the action network and
the components of C(k) ‚àà RNhc√ón are expressed as

1
2

(1 ‚àí œÜ2
Cli(k) =
cl
l = 1, ¬∑ ¬∑ ¬∑ , Nhc, i = 1, ¬∑ ¬∑ ¬∑ , n,

(k))œÑcl,m+i,

(18)

where l and i are the indices of the hidden and input neurons,
while m and n denote the dimension of system state and input.
We refer the critic and action update rules in (14) and (17)
c and œâ‚àó
as event-driven dHDP. Let œâ‚àó
a be the optimal weights
of the critic network and the action network, respectively, i.e.,

œâ‚àó

c = arg min
œâc

|| ÀÜV (x(k)) ‚àí [ ÀÜV (x(k ‚àí 1) ‚àí r(k ‚àí 1)]||,

(19)

œâ‚àó

a = arg min
œâa

(cid:107) ÀÜV (x(k)) (cid:107) .

Then we have

V ‚àó(x(k)) = œâ‚àóT
u‚àó(k) = œâ‚àóT

c (k)œÜc(k) + (cid:15)c,
a (k)œÜa(k) + (cid:15)a,

(20)

(21)

(22)

where V ‚àó(x(k)) and u‚àó(k) denote the optimal cost-to-go
function and optimal control input, while (cid:15)c and (cid:15)a are the
neural network reconstruction errors of the critic network and
the action network, respectively.

III. MAIN RESULTS

As previously described, our goal is to devise a dHDP-
based ADP online learning method that adapts the action and
the critic weights during signiÔ¨Åcant system events reÔ¨Çected
in system states. That is to say that learning and adaptation
does not necessarily take place regularly at each sampling time
when an observation is made. We therefore will provide a
sufÔ¨Åcient condition under which learning takes place as driven
by events while the closed-loop system stability is guaranteed.

Discrete-Time Nonlinear SystemEvent GeneratorZOH ùëü(ùõøùëò‚àí1) ùë•(ùõøùëò) ùë•(ùëò) ‚ààùëÖùëö  ùë¢(ùëò)‚ààùëÖùëõ ùúëùëê(‚Ä¢) ùë•(ùõøùëò)  ùë•(ùõøùëò)  ùúëùëê(‚Ä¢) ùúëùëê(‚Ä¢) ùúîùëê(ùëò) ùúèùëê(ùëò) ùúëùëé(‚Ä¢) ùúëùëé(‚Ä¢) ùúëùëé(‚Ä¢) ùúîùëé(ùëò) ùúèùëé(ùëò) ùë¢  ùõøùëò  ùë•(ùõøùëò)          ùëâ  ùë• ùõøùëò‚àí1   ‚àíùëü(ùõøùëò‚àí1)  ùëâ  ùë• ùõøùëò   + ‚àí ùëò ùë•(ùëò)  {ùõøùëò}  {ùõøùëò+1}  {ùõøùëò+2}  {ùë• ùõøùëò }  {ùë• ùõøùëò+2 }  {ùë• ùõøùëò+1 }  ùë¢  ùõøùëò   (ùëÅ‚Ñéùëé hidden nodes)   (ùëÅ‚Ñéùëê hidden nodes)  Action NetworkCritic NetworkJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4

SpeciÔ¨Åcally we will show that both the weight parameters and
the system states will remain UUB.

We deÔ¨Åne the weight estimation errors and some auxiliary

variables as follows:

Àúœâc(k) = ÀÜœâc(k) ‚àí œâ‚àó
c ,
c )T œÜc(k) = ÀúœâT
Œæc(k) = (ÀÜœâc(k) ‚àí œâ‚àó
Àúœâa(k) = ÀÜœâa(k) ‚àí œâ‚àó
a,
a)T œÜa(k) = ÀúœâT
Œæa(k) = (ÀÜœâa(k) ‚àí œâ‚àó

c (k)œÜc(k),

a (k)œÜa(k).

(23)

Assumption 2: For the critic network and the action
network, the optimal weights, the activation functions and
c (cid:107)(cid:54) œâcm,
the reconstruction errors are bounded, i.e., (cid:107) œâ‚àó
a (cid:107)(cid:54) œâam, (cid:107) œÜc (cid:107)(cid:54) œÜcm,(cid:107) œÜa (cid:107)(cid:54) œÜam, (cid:107) (cid:15)c (cid:107)(cid:54) (cid:15)cm,
(cid:107) œâ‚àó
(cid:107) (cid:15)a (cid:107)(cid:54) (cid:15)am. Besides, the activation function œÜa is Lipschitz
continuous and satisÔ¨Åes

(cid:107) œÜa(x1) ‚àí œÜa(x2) (cid:107)(cid:54) La (cid:107) x1 ‚àí x2 (cid:107),

(24)

for all x1, x2 ‚àà ‚Ñ¶ where La is a positive constant and ‚Ñ¶ is
the domain of function f (x, u).

Theorem 1: Consider nonlinear system (1) and let Assump-
tions 1-2 hold. Let the critic and action network weights update
iteratively according to (14) and (17) if a learning event occurs
based on the following event-driven state error criterion (25),
Œªmin(Q)Œ≤
2Œªmax(R) (cid:107) ÀÜœâa (cid:107)2 L2
a
and let the learning rates of the action and critic networks
satisfy

(cid:107) e(k) (cid:107)2(cid:54)

(cid:107) x(k) (cid:107)2,

(25)

1

lc <

(cid:107) œÜc(k) (cid:107)2 , la <
then we have the following results:

1
(cid:107) œÜa(k) (cid:107)2 ,

(26)

1. The errors between the optimal network weights œâ‚àó
and their estimates ÀÜœâc(k) and ÀÜœâa(k) are UUB.

c , œâ‚àó
a

2. The control u(k), which is parameterized by œâa(k), is a
stabilizing control to guarantee the UUB of the nonlinear
system under the proposed event-driven dHDP algorithm.
Proof: We consider the following two cases.

Case1: No learning event is observed at time instant k.
Consider the Lyapunov function candidate deÔ¨Åned as follows:
L(k) = L1(k) + L2(k) + L3(k) + L4(k),

(27)

where

L1(k) =

L3(k) =

1
lc
1
2

tr[ÀúœâT

c (k)Àúœâc(k)], L2(k) =

1
Œ≥la

tr[ÀúœâT

a (k)Àúœâa(k)],

(cid:107) Œæc(k ‚àí 1) (cid:107)2,

L4(k) = V (x(k)).

As Àúœâc(k + 1) = Àúœâc(k) and Àúœâa(k + 1) = Àúœâa(k), we Ô¨Ånd
(cid:52)L1(k) = (cid:52)L2(k) = 0.
1
(cid:52)L3(k) =
2

[(cid:107) Œæc(k) (cid:107)2 ‚àí (cid:107) Œæc(k ‚àí 1) (cid:107)2],

(28)

(cid:52)L4(k) = V (x(k + 1)) ‚àí V (x(k)) = ‚àír(k)
= ‚àíxT (k)Qx(k) ‚àí uT (x(Œ¥k))Ru(x(Œ¥k))
= ‚àíxT (k)Qx(k) ‚àí {‚àí[u‚àó(x(k)) ‚àí u(x(Œ¥k))]
+ u‚àó(x(k))}T R{‚àí[u‚àó(x(k)) ‚àí u(x(Œ¥k))] + u‚àó(x(k))}
= ‚àíxT (k)Qx(k) ‚àí u‚àóT (x(k))Ru‚àó(x(k))
‚àí [u‚àó(x(k)) ‚àí u(x(Œ¥k))]T R[u‚àó(x(k)) ‚àí u(x(Œ¥k))]
+ 2u‚àóT (x(k))R[u‚àó(x(k)) ‚àí u(x(Œ¥k))].

From (22), we have

u‚àó(x(k))‚àíu(x(Œ¥k)) = œâ‚àóT
= ÀÜœâT

a (k)œÜa(k)+(cid:15)a ‚àí ÀÜœâT
a (k)[œÜa(x(k))‚àíœÜa(x(Œ¥k))] ‚àí Œæa(k) + (cid:15)a.

a (k)œÜa(x(Œ¥k))

(30)

Substituting (24) and (30) into (29), we obtain

a (k)[œÜa(x(k)) ‚àí œÜa(x(Œ¥k))]

(cid:52)L4(k) (cid:54) ‚àíxT (k)Qx(k) + Œªmax(R) (cid:107) u‚àó(x(k)) (cid:107)2
+ Œªmax(R)[ÀÜœâT
‚àí ÀúœâT
(cid:54) ‚àíŒªmax(Q) (cid:107) x(k) (cid:107)2 +2Œªmax(R) (cid:107) ÀÜœâa (cid:107)2 L2
am + (cid:15)2
+ 6Œªmax(R)(œâ2

a (k)œÜa(x(k)) + (cid:15)a]2

amœÜ2

am).

a (cid:107) e(k) (cid:107)2

(31)

Recall (25), from (28) and (31), we have

(cid:52)L(k) (cid:54) ‚àí(1 ‚àí Œ≤)Œªmin(Q) (cid:107) x(k) (cid:107)2 +D2

1m,

(32)

where

D2

1m = 6Œªmax(R)(œâ2

amœÜ2

am + (cid:15)2

am) + 2œâ2

cmœÜ2

cm.

If

(cid:107) x(k) (cid:107)>

D1m
(cid:112)(1 ‚àí Œ≤)Œªmin(Q)

,

(33)

(34)

the Ô¨Årst difference (cid:52)L(k) ‚â§ 0. This demonstrates that the
closed-loop system state and the weight estimation errors are
UUB in this case.

Case2: A learning event is observed at time instant k with
an event index Œ¥k. In this case, the weights of the critic network
and the action network will be updated using (14) and (17).
With the same Lyapunov function candidate (27) and a similar
derivation as Theorem 4.3 in [35], we have

1
2
4
Œ≥

4
Œ≥
cmœÜ2

(cid:52)L(k) (cid:54) ‚àí(

‚àí

)||Œæc(k)||2 ‚àí Œªmax(Q)||x(k)||2 + D2

2m,

D2

2m = (12+

)œâ2

cm +

4
Œ≥

cmC 2
œâ2

mœâ2

amœÜ2

am + 8r2
m

(35)

where Cm and rm are the upper bounds of C(k) and r(k) in
(18) and (7), respectively, and Œ≥ > 8 is a weighting factor. If

(cid:107) x(k) (cid:107)>

D2m
(cid:112)Œªmin(Q)

or (cid:107) Œæc(k) (cid:107)>

(cid:114)
(

D2m
1
2

‚àí

,

(36)

4
Œ≥

)

the Ô¨Årst difference (cid:52)L(k) ‚â§ 0.

From Case 1 and Case 2, we conclude that the closed-
loop system state and the errors between the optimal network
weights œâ‚àó
a and their respective estimates ÀÜœâc(k) and ÀÜœâa(k)
are UUB. This completes the proof.

c , œâ‚àó

Remark 1: Theorem 1 presents a sufÔ¨Åcient condition to
guarantee the stability of an event-driven dHDP controlled
system with learning realized in the critic and action neural
networks. Without any special constraints on the nonlinear
system, we have obtained a system boundedness result under a
few mild assumptions. As there exist unavoidable approxima-
tion errors in both the cost-to-go function and the input control,
Theorem 2 below examines how an approximate solution of
the Bellman optimality equation can be achieved within a Ô¨Ånite
approximation error.

(29)

Theorem 2: Under the same conditions as in Theorem 1, the
Bellman optimality equation is approximated within a Ô¨Ånite

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

5

approximation error. Meanwhile, the adopted control law ÀÜu(k)
is uniformly convergent to a Ô¨Ånite neighborhood of the optimal
control u‚àó(k).

Proof: From the approximate cost-to-go in equation (10)
for the Bellman equation (8) and the optimal cost-to-go ex-
pressed in approximation form (21) for the Bellman optimality
equation (9), we have

(cid:107) ÀÜV (x(k)) ‚àí V ‚àó(x(k)) (cid:107)=(cid:107) ÀÜœâT
‚àí (cid:15)c (cid:107)(cid:54)(cid:107) Àúœâc(k) (cid:107) œÜcm + (cid:15)c (cid:54) ÀÜ(cid:15)c.

c (k)œÜc(k) ‚àí œâ‚àóT

c (k)œÜc(k)

(37)

Similarly, from (11) and (22), we have

(cid:107) ÀÜu(k) ‚àí u‚àó(k) (cid:107)(cid:54)(cid:107) Àúœâa(k) (cid:107) œÜam + (cid:15)a (cid:54) ÀÜ(cid:15)a.

(38)
This comes directly as (cid:107) Àúœâc(k) (cid:107) and (cid:107) Àúœâa(k) (cid:107) are both UUB
as shown in Theorem 1. This demonstrates that the Bellman
optimality is achieved within Ô¨Ånite approximation errors.

IV. AN ILLUSTRATIVE EXAMPLE

Fig. 2. State trajectories with event-driven dHDP controller (different Œ≤) and
time-driven dHDP controller (Œ≤ = 0).

.

(cid:20)

(cid:21)

+

=

0
0.1u(k)

In this section, we use a numerical example to demonstrate
the theoretical results in this paper and differences between
event-driven dHDP and its time-driven counter part. The
unknown system dynamics are assumed to be generated from
the following equation,
(cid:20)x1(k + 1)
(cid:20)0.9996x1(k) + 0.0099x2(k)
(cid:21)
(cid:21)
x2(k + 1)
‚àí0.0887x1(k) + 0.99x2(k)
Now we try to stabilize this system by the proposed event-
driven dHDP algorithm. For both the critic neural network and
the action neural network, we set the number of hidden layer
nodes to Nhc = Nha = 6. From Theorem 1, the learning rates
of these two neural networks should satisfy equation (26). In
this simulation, we choose la = lc = 0.1. The initial condition
of the system state is set as x(0) = [‚àí1, 1]T . The RL signal is
r(k) = xT (k)Imx(k) + uT (k)(0.1In)u(k), where Im and In
are identity matrices. The initial weights of both the critic and
the action neural network are set randomly within [‚àí0.4, 0.4].
For the learning event condition (25), we choose different Œ≤
values where Œ≤ = 0 corresponds to time-driven dHDP as in
[26].

Fig. 3. Numbers of learning events of event-driven dHDP (different Œ≤) and
time-driven dHDP (Œ≤ = 0).

Dynamic system state trajectories under different event-
driven learning conditions (i.e., different Œ≤ values) are shown
in Fig. 2. As expected, little difference is noticed between
the event-driven dHDP and the time-driven dHDP when Œ≤ is
small. Also as expected, the controlled system state trajectories
deviate further from those controlled by the time-driven dHDP
as Œ≤ increases. Fig. 3 illustrates reduced numbers of learning
events and consequently degraded learning control responses
as Œ≤ increases.

Next, we illustrate how event-driven dHDP may be used as
an effective tool to prevent converged critic and action network
weights from drifting too far from the learned controller due
to inevitable noise in the state measurements.

This simulation entails 500 time steps, the Ô¨Årst 100 of
which are for training the critic and action weights. During
this period, the weights are updated at each time instant k
while the system dynamics are not subject to any noise. From
time step 101 to 300, we introduce random Gaussian white
noise œâk ‚àº N (œâ0, 1) with œâ0 = 0.1 into the system. Then

Fig. 4. Average accumulated variation for every weight under event-driven
dHDP controller (different Œ≤) and time-driven dHDP controller (Œ≤ = 0) with
external noise.

02004006008001000Time steps-10123x1Time driven dHDPEventdrivendHDP(Œ≤=0.0001)EventdrivendHDP(Œ≤=0.001)EventdrivendHDP(Œ≤=0.005)EventdrivendHDP(Œ≤=0.01)Event driven dHDP (Œ≤=0.05)02004006008001000Time steps-2-10123x202004006008001000Timesteps020040060080010001200Number of eventsTime driven dHDPEvent driven dHDP (Œ≤=0.0001)Event driven dHDP (Œ≤=0.001)Event driven dHDP (Œ≤=0.005)Event driven dHDP (Œ≤=0.01)EventdrivendHDP(Œ≤=0.05)0100200300400500Timesteps00.10.20.30.40.50.6VariationofthecriticandtheactionnetworksTime driven dHDPEvent driven dHDP (Œ≤=0.0001)Event driven dHDP (Œ≤=0.001)Event driven dHDP (Œ≤=0.005)Event driven dHDP (Œ≤=0.01)EventdrivendHDP(Œ≤=0.05)JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

from time step 301 to 500, the random Gaussian white noise
is removed or the system dynamics evolve under noise free
condition. We use event-driven dHDP with different Œ≤ values
and consider the accumulated amount of variation in the critic
and action network weights as deÔ¨Åned below:
(cid:80)k

(cid:80)

j=0 (cid:107) ÀÜœâi(j + 1) ‚àí ÀÜœâi(j) (cid:107)2

i=a,c

,

(39)

Œ∑(k) =

Nhc + Nha

where (cid:107) ¬∑ (cid:107) denotes the 2-norm.

The Ô¨Årst 100 time steps in Fig. 4 shows actual learning in
the critic and action networks and such learning converges at
about time step 80. From steps 101 to 300, we notice from
small to large weight variations corresponding to small to large
Œ≤ values due to the added white Gaussian noise, and such
variation is the largest when Œ≤ = 0 or when learning takes
place continuously. Therefore we can see how learning event
condition (25) helps protect the critic and action networks from
drifting away due to random Ô¨Çuctuation in system dynamics.

V. CONCLUSION

We have proposed a new event-driven RL control method,
i.e., event-driven dHDP based on the time-driven dHDP for
a general discrete time nonlinear system. SpeciÔ¨Åcally we
introduced a learning event criterion that is directly related
to system states such that a learned dHDP controller could be
less affected by random Ô¨Çuctuations in the system dynamics.
We proved the stability of the closed-loop system under event-
driven dHDP control by using Lyapunov functions as well as
the approximate optimality of the dHDP control solution. We
demonstrated a signiÔ¨Åcant reduction of weight updating times
for event-driven learning from time-driven learning without
sacriÔ¨Åcing too much system performance.

REFERENCES

[1] M. McCloskey and N. Cohen, ‚ÄúCatastrophic interference in connec-
tionist networks: The sequential learning problem,‚Äù in Psychology of
learning and motivation. Elsevier, 1989, vol. 24, pp. 109‚Äì165.

[2] R. Ratcliff, ‚ÄúConnectionist models of recognition memory: constraints
imposed by learning and forgetting functions.‚Äù Psychological Review,
vol. 97, no. 2, pp. 285‚Äì308, 1990.

[3] G. Carpenter and S. Grossberg, ‚ÄúAdaptive resonance theory: Stable self-
organization of neural recognition codes in response to arbitrary lists of
input patterns,‚Äù in Proceedings of the Eighth Annual Conference of the
Cognitive Science Society. Erlbaum, 1986, pp. 45‚Äì62.

[4] G. Hinton, J. McClelland, D. Rumelhart et al., Distributed representa-

tions. Carnegie-Mellon University Pittsburgh, PA, 1984.

[5] G. Hinton and D. Plaut, ‚ÄúUsing fast weights to deblur old memories,‚Äù
in Proceedings of the ninth annual conference of the Cognitive Science
Society, 1987, pp. 177‚Äì186.

[6] R. Sutton, ‚ÄúTwo problems with back propagation and other steepest
descent learning procedures for networks,‚Äù in Proceedings of the Eighth
Annual Conference of the Cognitive Science Society, 1986, pp. 823‚Äì832.
[7] Z. Li and D. Hoiem, ‚ÄúLearning without forgetting,‚Äù IEEE transactions
on pattern analysis and machine intelligence, vol. 40, no. 12, pp. 2935‚Äì
2947, 2017.

[8] F. Zenke, B. Poole, and S. Ganguli, ‚ÄúContinual learning through synaptic
intelligence,‚Äù in Proceedings of the 34th International Conference on
Machine Learning-Volume 70.

JMLR. org, 2017, pp. 3987‚Äì3995.

[9] J. Kirkpatrick, R. Pascanu, N. Rabinowitz et al., ‚ÄúOvercoming catas-
the national

trophic forgetting in neural networks,‚Äù Proceedings of
academy of sciences, vol. 114, no. 13, pp. 3521‚Äì3526, 2017.

[11] R. Polikar, L. Upda, S. Upda, and V. Honavar, ‚ÄúLearn++: An incremental
learning algorithm for supervised neural networks,‚Äù IEEE Transactions
on Systems Man and Cybernetics Part C, vol. 31, no. 4, pp. 497‚Äì508,
2001.

[12] X. Zhong and H. He, ‚ÄúAn event-triggered adp control approach for
continuous-time system with unknown internal states,‚Äù IEEE Transac-
tions on Cybernetics, vol. 47, no. 3, pp. 683‚Äì694, 2016.

[13] X. Yang, H. He, and D. Liu, ‚ÄúEvent-triggered optimal neuro-controller
design with reinforcement learning for unknown nonlinear systems,‚Äù
IEEE Transactions on Systems, Man, and Cybernetics: Systems, no. 99,
pp. 1‚Äì13, 2017.

[14] L. Dong, X. Zhong, C. Sun, and H. He, ‚ÄúEvent-triggered adaptive
dynamic programming for continuous-time systems with control con-
straints,‚Äù IEEE transactions on neural networks and learning systems,
vol. 28, no. 8, pp. 1941‚Äì1952, 2017.

[15] Y. Zhu, D. Zhao, H. He, and J. Ji, ‚ÄúEvent-triggered optimal control
for partially unknown constrained-input systems via adaptive dynamic
programming,‚Äù IEEE Transactions on Industrial Electronics, vol. 64,
no. 5, pp. 4101‚Äì4109, 2017.

[16] H. Zhang, K. Zhang, G. Xiao, and H. Jiang, ‚ÄúRobust optimal control
scheme for unknown constrained-input nonlinear systems via a plug-n-
play event-sampled critic-only algorithm,‚Äù IEEE Transactions on Sys-
tems, Man, and Cybernetics: Systems, 2019.

[17] Q. Zhang, D. Zhao, and D. Wang, ‚ÄúEvent-based robust control for
uncertain nonlinear systems using adaptive dynamic programming,‚Äù
IEEE transactions on neural networks and learning systems, vol. 29,
no. 1, pp. 37‚Äì50, 2016.

[18] D. Wang, C. Mu, H. He, and D. Liu, ‚ÄúEvent-driven adaptive robust
control of nonlinear systems with uncertainties through ndp strategy,‚Äù
IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 47,
no. 7, pp. 1358‚Äì1370, 2016.

[19] D. Wang, C. Mu, D. Liu, and H. Ma, ‚ÄúOn mixed data and event
driven design for adaptive-critic-based nonlinear H‚àû control,‚Äù IEEE
transactions on neural networks and learning systems, vol. 29, no. 4,
pp. 993‚Äì1005, 2017.

[20] Q. Zhang, D. Zhao, and Y. Zhu, ‚ÄúEvent-triggered H‚àû control for
continuous-time nonlinear system via concurrent learning,‚Äù IEEE Trans-
actions on Systems, Man, and Cybernetics: Systems, vol. 47, no. 7, pp.
1071‚Äì1081, 2016.

[21] X. Yang and H. He, ‚ÄúAdaptive critic designs for event-triggered robust
control of nonlinear systems with unknown dynamics,‚Äù IEEE transac-
tions on cybernetics, vol. 49, no. 6, pp. 2255‚Äì2267, 2018.

approximate dynamic programming.

[22] B. Luo, Y. Yang, D. Liu, and H.-N. Wu, ‚ÄúEvent-triggered optimal control
with performance guarantees using adaptive dynamic programming,‚Äù
IEEE transactions on neural networks and learning systems, 2019.
[23] J. Si, A. Barto, W. Powell, and D. Wunsch, Handbook of learning and
John Wiley & Sons, 2004, vol. 2.
[24] F. Lewis and D. Liu, Reinforcement learning and approximate dynamic
John Wiley & Sons, 2013, vol. 17.
[25] P. J. Werbos, ‚ÄúNeural networks for control and system identiÔ¨Åcation,‚Äù in
Decision and Control, 1989., Proceedings of the 28th IEEE Conference
on.

programming for feedback control.

IEEE, 1989, pp. 260‚Äì265.

[26] J. Si and Y. Wang, ‚ÄúOnline learning control by association and rein-
forcement,‚Äù IEEE Transactions on Neural networks, vol. 12, no. 2, pp.
264‚Äì276, 2001.

[27] R. Ens and J. Si, ‚ÄúApache helicopter stabilization using neuro dynamic

programming,‚Äù Report G6316, AIAA, 2000.

[28] R. Enns and J. Si, ‚ÄúHelicopter trimming and tracking control using direct
neural dynamic programming,‚Äù IEEE Transactions on Neural Networks,
vol. 14, no. 4, pp. 929‚Äì939, 2003.

[29] C. Lu, J. Si, and X. Xie, ‚ÄúDirect heuristic dynamic programming for
damping oscillations in a large power system,‚Äù IEEE Transactions on
Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 38, no. 4, pp.
1008‚Äì1013, 2008.

[30] Y. Wen, J. Si, A. Brandt, X. Gao, and H. Huang, ‚ÄúOnline reinforcement
learning control for the personalization of a robotic knee prosthesis,‚Äù
IEEE transactions on cybernetics, pp. 1‚Äì11, 2019.

[31] X. Gao, J. Si, Y. Wen, M. Li, and H. Huang, ‚ÄúKnowledge-guided
reinforcement learning control for robotic lower limb prosthesis,‚Äù in
International Conference on Robotics and Automation.
IEEE, 2020, in
print.

[10] G. Carpenter, S. Grossberg, N. Markuzon, J. Reynolds, and D. Rosen,
‚ÄúFuzzy artmap: A neural network architecture for incremental supervised
learning of analog multidimensional maps,‚Äù IEEE Transactions on
Neural Networks, vol. 3, no. 5, pp. 698‚Äì713, 1992.

[32] A. Sahoo, H. Xu, and S. Jagannathan, ‚ÄúNear optimal event-triggered
control of nonlinear discrete-time systems using neurodynamic program-
ming,‚Äù IEEE transactions on neural networks and learning systems,
vol. 27, no. 9, pp. 1801‚Äì1815, 2016.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

7

[33] M. Ha, D. Wang, and D. Liu, ‚ÄúEvent-triggered adaptive critic control
design for discrete-time constrained nonlinear systems,‚Äù IEEE Transac-
tions on Systems, Man, and Cybernetics: Systems, pp. 1‚Äì11, 2018.
[34] L. Dong, X. Zhong, C. Sun, and H. He, ‚ÄúAdaptive event-triggered control
based on heuristic dynamic programming for nonlinear discrete-time
systems,‚Äù IEEE transactions on neural networks and learning systems,
vol. 28, no. 7, pp. 1594‚Äì1605, 2017.

[35] F. Liu, J. Sun, J. Si, W. Guo, and S. Mei, ‚ÄúA boundedness result for the
direct heuristic dynamic programming,‚Äù Neural Networks, vol. 32, pp.
229‚Äì235, 2012.

