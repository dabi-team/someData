9
1
0
2

g
u
A
8
1

]
T
E
.
s
c
[

2
v
0
1
4
9
0
.
6
0
9
1
:
v
i
X
r
a

A reaction network scheme which implements inference and
learning for Hidden Markov Models

Abhinav Singh1, Carsten Wiuf2, Abhishek Behera3, and Manoj Gopalkrishnan3

1 UM-DAE Centre for Excellence in Basic Sciences, Mumbai, India
2 Department of Mathematical Sciences, University of Copenhagen, Denmark
3 Indian Institute of Technology Bombay, Mumbai, India
{abhinavsns7, abhishek.enlightened, manoj.gopalkrishnan}@gmail.com
wiuf@math.ku.dk

Abstract. With a view towards molecular communication systems and molecular multi-agent
systems, we propose the Chemical Baum-Welch Algorithm, a novel reaction network scheme
that learns parameters for Hidden Markov Models (HMMs). Each reaction in our scheme
changes only one molecule of one species to one molecule of another. The reverse change
is also accessible but via a different set of enzymes, in a design reminiscent of futile cycles
in biochemical pathways. We show that every ﬁxed point of the Baum-Welch algorithm for
HMMs is a ﬁxed point of our reaction network scheme, and every positive ﬁxed point of our
scheme is a ﬁxed point of the Baum-Welch algorithm. We prove that the “Expectation” step
and the “Maximization” step of our reaction network separately converge exponentially fast.
We simulate mass-action kinetics for our network on an example sequence, and show that it
learns the same parameters for the HMM as the Baum-Welch algorithm.

1 Introduction

The sophisticated behavior of living cells on short timescales is powered by biochemical reaction
networks. One may say that evolution has composed the symphony of the biosphere, genetic machin-
ery conducts the music, and reaction networks are the orchestra. Understanding the capabilities and
limits of this molecular orchestra is key to understanding living systems, as well as to engineering
molecular systems that are capable of sophisticated life-like behavior.

The technology of implementing abstract reaction networks with molecules is a subﬁeld of
molecular systems engineering that has witnessed rapid advances in recent times. Several researchers
[1–8] have proposed theoretical schemes for implementing arbitrary reaction networks with DNA
oligonucleotides. There is a growing body of experimental demonstrations of such schemes [2, 7, 9–
11]. A stack of tools is emerging to help automate the design process. We can now compile abstract
reaction networks to a set of DNA oligonucleotides that will implement the dynamics of the network
in solution [12]. We can computationally simulate the dynamics of these oligonucleotide molecular
systems [13] to allow debugging prior to experimental implementation. In view of these rapid ad-
vances, the study of reaction networks from the point of view of their computational capabilities has
become even more urgent.

It has long been known that reaction networks can compute any computable function [14]. The
literature has several examples of reaction network schemes that have been inspired by known algo-
rithms [15–22]. Our group has previously described reaction network schemes that solve statistical
problems like maximum likelihood [23], sampling a conditional distribution and inference [24], and
learning from partial observations [25]. These schemes exploit the thermodynamic nature of the un-
derlying molecular systems that will implement these reaction networks, and can be expressed in
terms of variational ideas involving minimization of Helmholtz free energy [26–28].

 
 
 
 
 
 
In this paper, we consider situations where partial information about the environment is avail-
able to a cell in the form of a sequence of observations. For example, this might happen when an
enzyme is acting processively on a polymer, or a molecular walker [29–31] is trying to locate its
position on a grid. In situations like this, multiple observations are not independent. Such sequences
can not be summarized merely by the type of the sequence [32], i.e., the number of times various
symbols occur. Instead, the order of various observations carries information about state changes
in the process producing the sequence. The number of sequences grows exponentially with length,
and our previously proposed schemes are algorithmically inadequate. To deal with such situations
requires a pithy representation of sequences, and a way of doing inference and learning directly on
such representations. In Statistics and Machine Learning, this problem is solved by Hidden Markov
Models (HMMs) [33].

HMMs are a widely used model in Machine Learning, powering sequence analysis applications
like speech recognition [34], handwriting recognition, and bioinformatics. They are also essential
components of communication systems as well as of intelligent agents trained by reinforcement
learning methods. In this article, we describe a reaction network scheme which implements the
Baum-Welch algorithm. The Baum-Welch algorithm is an iterative algorithm for learning HMM
parameters. Reaction networks that can do such statistical analysis on sequences are likely to be an
essential component of molecular communication systems, enabling cooperative behavior among a
population of artiﬁcial cells. Our main contributions are:

1. In Section 2, we describe what the reader needs to know about HMMs and the Baum-Welch al-
gorithm to be able to follow the subsequent constructions. No prerequisites are assumed beyond
familiarity with matrices and probability distributions.

2. In Section 3, we describe a novel reaction network scheme to learn parameters for an HMM.
3. We prove in Theorem 1 that every ﬁxed point of the Baum-Welch algorithm is also a ﬁxed point

of the continuous dynamics of this reaction network scheme.

4. In Theorem 2, we prove that every positive ﬁxed point of the dynamics of our reaction network

scheme is a ﬁxed point of the Baum-Welch algorithm.

5. In Theorem 3 and Theorem 4, we prove that subsets of our reaction network scheme which
correspond to the Expectation step and the Maximization step of the Baum-Welch algorithm
both separately converge exponentially fast.

6. In Example 1, we simulate our reaction network scheme on an input sequence and show that
the network dynamics is successfully able to learn the same parameters as the Baum-Welch
algorithm.

7. We show in Example 2 that when the baum-welch ﬁxed point is on the boundary then our
scheme need not converge to a Baum-Welch ﬁxed point. However, we conjecture that if there
exists a positive Baum-Welch ﬁxed point then our scheme must always converge to a Baum-
Welch ﬁxed point. In particular, there would be a positive Baum-Welch ﬁxed point if the true
HMM generating the sequence has all parameters positive, and the observed sequence is long
enough.

2 Hidden Markov Models and the Baum Welch Algorithm

Fix two ﬁnite sets P and Q. A stochastic map is a |P | × |Q| matrix A = (apq)|P |×|Q| such that
q∈Q apq = 1 for all p ∈ P . Intuitively, stochastic maps
apq ≥ 0 for all p ∈ P and q ∈ Q, and
represent conditional probability distributions.

An HMM (H, V, θ, ψ, π) consists of ﬁnite sets H (for ‘hidden’) and V (for ‘visible’), a stochas-
tic map θ from H to H called the transition matrix, a stochastic map ψ from H to V called the

P

2

θ11

θ22

θ12

θ21

H2

H1

ψ11

ψ12

ψ21

ψ22

V1

V2

V1

V2

(a) Hidden Markov Model

βℓ−1,1

H1

H2

βℓ−1,2

vℓ−1

θ11ψ1vℓ βℓ1

θ12ψ1v

ℓβℓ2

θ 2 1 ψ 1 v ℓ β ℓ 1

θ22ψ2vℓ βℓ2

(c) Backward Algorithm

αℓ1

H1

H2

αℓ2

vℓ

βℓ1

H1

H2

βℓ2

vℓ

αℓ1θ11ψ1vℓ+1

αℓ1θ12ψ2v

ℓ+1

2 1 ψ 1 v ℓ + 1

α ℓ 2 θ

αℓ2θ22ψ2vℓ+1

αℓ+1,1

H1

H2

αℓ+1,2

vℓ+1

(b) Forward Algorithm
θ, ψ

θ, ψ

Forward

Backward

αlg

βlg

E-Step

ξlgh γlg

M-Step

θ, ψ
(d) Baum-Welch Algorithm

Fig. 1. Learning HMMs from sequences. (a) HMM: The hidden states H1 and H2 are not directly ob-
servable. Instead what are observed are elements V1, V2 from the set V = {V1, V2} of “visible states.” The
parameters θ11, θ12, θ21, θ22 denote the probability of transitions between the hidden states. The probabil-
ity of observing states V1, V2 depends on the parameters ψ11, ψ12, ψ21, ψ22 as indicated in the ﬁgure. (b)
The forward algorithm computes the position l + 1 likelihood αl+1,1 = αl1θ11ψ1vl+1 + αl2θ21ψ1vl+1 by
forward propagating the position l likelihoods αl1 and αl2. Here vl, vl+1 ∈ V are the observed emissions
at position l and l + 1. (c) The backward algorithm computes the position l − 1 conditional probability
βl−1,1 = θ11ψ1vl βl1 + θ12ψ2vl βl2 by propagating the position l conditional probabilities βl1 and βl2 back-
wards. (d) The Baum-Welch Algorithm is a ﬁxed point Expectation-Maximization computation. The E step
calls the forward and backward algorithm as subroutines and, conditioned on the entire observed sequence
(v1, v2, . . . , vL) ∈ V L, computes the probabilities γlg of being in states g ∈ H at position l and the prob-
abilities ξlgh of taking the transitions gh ∈ H 2 at position l. The M step updates the parameters θ and ψ to
maximize their likelihood given the observed sequence.

emission matrix, and an initial probability distribution π = (πh)h∈H on H, i.e., πh ≥ 0 for all
h ∈ H and

h∈H πh = 1. See Figure 1a for an example.

Suppose a length L sequence (v1, v2, . . . , vL) ∈ V L of visible states is observed due to a hid-
den sequence (x1, x2, . . . , xL) ∈ H L. The following questions related to an HMM are commonly
studied:

P

1. Likelihood: For ﬁxed θ, ψ, compute the likelihood Pr(v1, v2, . . . , vL | θ, ψ). This problem is

solved by the forward-backward algorithm.

3

2. Learning: Estimate the parameters ˆθ, ˆψ which maximize the likelihood of the observed se-
quence (v1, v2, . . . , vL) ∈ V L. This problem is solved by the Baum-Welch algorithm which
is an Expectation-Maximization (EM) algorithm. It uses the forward-backward algorithm as a
subroutine to compute the E step of EM.

3. Decoding: For ﬁxed θ, ψ, ﬁnd the sequence (ˆh1, ˆh2, . . . , ˆhl, . . . , ˆhL) ∈ H L that has the highest
probability of producing the given observed sequence (v1, v2, . . . , vL). This problem is solved
by the Viterbi algorithm.

The forward algorithm (Figure 1b) takes as input an HMM (H, V, θ, ψ, π) and a length L obser-
vation sequence (v1, v2, . . . , vL) ∈ V L and outputs the L×|H| likelihoods αlh = Pr[v1, v2, . . . , vl, xl =
h | θ, ψ] of observing symbols v1, . . . , vl and being in the hidden state h ∈ H at time l. It does so
using the following recursion.

– Initialisation: α1h = πhψhv1 for all h ∈ H,
– Recursion: αlh =

g∈H αl−1,gθghψhvl for all h ∈ H and l = 2, . . . , L.

The backward algorithm (Figure 1c) takes as input an HMM (H, V, θ, ψ, π) and a length L

P

observation sequence (v1, v2, . . . , vL) ∈ V L and outputs the L × |H| conditional probabilities
βlh = Pr[vl+1, vl+2, . . . , vL | xl = h, θ, ψ] of observing symbols vl+1, . . . , vL given that the
hidden state xl at time l has label h ∈ H.

– Initialisation: βLh = 1,
– Recursion: βlh =

for all h ∈ H,

g∈H θhgψgvl+1 βl+1,g for all h ∈ H and l = 1, . . . , L − 1.

The E step for the Baum-Welch algorithm takes as input an HMM (H, V, θ, ψ, π) and a length L
observation sequence (v1, v2, . . . , vL). It outputs the L × H conditional probabilities γlh = Pr[xl =
h | θ, ψ, v] of being in hidden state h at time l conditioned on the observed sequence v by:

P

γlh =

αlhβlh
g∈H αlgβlg

for all h ∈ H and l = 1, 2, . . . , L − 1. It also outputs the (L − 1) × H × H probabilities ξlgh =
Pr[xl = g, xl+1 = h | θ, ψ, v] of transitioning along the edge (g, h) at time l conditioned on the
observed sequence v by:

P

ξlgh =

αlgθghψhvl+1 βl+1,h
f ∈H αlf βlf

for all g, h ∈ H and l = 1, . . . , L − 1.

P

Remark 1. Note that the E-step uses the forward and backward algorithms as subroutines to ﬁrst
compute the α and β values. Further note that we don’t need the forward and backward algorithms
to return the actual values αlh and βlh. To be precise, let αl = (αlh)h∈H ∈ RH denote the vector of
forward likelihoods at time l. Then for the E step to work, we only need the direction of αl and not
the magnitude. This is because the numerator and denominator in the updates are both linear in αlh,
and the magnitude cancels out. Similarly, if βl = (βlh)h∈H ∈ RH denotes the vector of backward
likelihoods at time l then the E step only cares about the direction of βl and not the magnitude. This
scale symmetry is a useful property for numerical solvers. We will also make use of this freedom
when we implement a lax forward-backward algorithm using reaction networks in the next section.

The M step of the Baum-Welch algorithm takes as input the values γ and ξ that are output by the
E step as reconstruction of the dynamics on hidden states, and outputs new Maximum Likelihood

4

estimates of the parameters θ, ψ that best explain these values. The update rule turns out to be very
simple. For all g, h ∈ H and w ∈ V :

θgh ←

L−1
l=1 ξlgh

L−1
P
l=1

f ∈H ξlgf

,

ψhw ←

L
l=1 γlhδw,vl
L
l=1 γlh

P

where δw,vl =

P

1 if w = vl
0 otherwise

(

P
is the Dirac delta function.

P

Remark 2. Like in Remark 1, note that the M step does not require its inputs to be the actual values
γ and ξ. There is a scaling symmetry so that we only need the directions of the vectors γ(h) =
(γlh)l=1,2,...,L ∈ RL for all h ∈ H and ξ(g) = (ξlgh)1≤l≤L−1,h∈H ∈ R(L−1)×H for all g ∈ H.
This gives us the freedom to implement a lax E projection without affecting the M projection, and
we will exploit this freedom when designing our reaction network.

The Baum-Welch algorithm (Figure 1d) is a ﬁxed point EM computation that alternately runs
the E step and the M step till the updates become small enough. It is guaranteed to converge to a
ﬁxed point (ˆθ, ˆψ). However, the ﬁxed point need not always be a global optimum.

3 Chemical Baum-Welch Algorithm

3.1 Reaction Networks

Following [25], we recall some concepts from reaction network theory [24, 35–39].

Fix a ﬁnite set S of species. An S-reaction, or simply a reaction when S is understood from

context, is a formal chemical equation

yX X →

y′
X X

X∈S
X

X∈S
X

X ∈ Z≥0 are the stoichiometric coefﬁcients of species X on the reactant
where the numbers yX , y′
side and product side respectively. A reaction network is a pair (S, R) where R is a ﬁnite set of
S-reactions. A reaction system is a triple (S, R, k) where (S, R) is a reaction network and k : R →
R>0 is called the rate function.

As is common when specifying reaction networks, we will ﬁnd it convenient to explicitly specify

only a set of chemical equations, leaving the set of species to be inferred by the reader.

Fix a reaction system (S, R, k). Deterministic Mass Action Kinetics describes a system of

ordinary differential equations on the concentration variables {xi(t) | i ∈ S} according to:

˙xi(t) =

ka→b(bi − ai)

xj(t)aj

a→b∈R
X

j∈S
Y

3.2 Baum-Welch Reaction Network

In this section we will describe a reaction networks for each part of the Baum-Welch algorithm.

Fix an HMM M = (H, V, θ, ψ, π). Pick an arbitrary hidden state h∗ ∈ H and an arbitrary
visible state v∗ ∈ V . Picking these states h∗ ∈ H and v∗ ∈ V is merely an artiﬁce to break
symmetry akin to selecting leaders, and our results hold independent of these choices. Also ﬁx a
length L ∈ Z>0 of observed sequence.

5

We ﬁrst work out in full detail how the forward algorithm of the Baum-Welch algorithm may be
translated into chemical reactions. Suppose a length L sequence (v1, v2, . . . , vL) ∈ V L of visible
states is observed. Then recall that the forward algorithm uses the following recursion:

– Initialisation: α1h = πhψhv1 for all h ∈ H,
– Recursion: αlh =

g∈H αl−1,gθghψhvl for all h ∈ H and l = 2, . . . , L.

Notice this implies

P

– α1h × πh∗ ψh∗v1 = α1h∗ × πhψhv1 for all h ∈ H \ {h∗},
– αlh ×

g∈H αl−1,gθgh∗ ψh∗vl

= αlh∗ ×

g∈H αl−1,gθghψhvl

for all h ∈ H \ {h∗} and

l = 2, . . . , L.

(cid:16)P

(cid:17)

(cid:16)P

(cid:17)

This prompts the use of the following reactions for the initialization step:

α1h + πh∗ + ψh∗v1 −−→ α1h∗ + πh∗ + ψh∗v1
α1h∗ + πh + ψhv1 −−→ α1h + πh + ψhv1

for all h ∈ H \ {h∗} and w ∈ V . By design α1h × πh∗ ψh∗v1 = α1h∗ × πhψhv1 is the balance
equation for the pair of reactions corresponding to each h ∈ H \ {h∗}.
Similarly for the recursion step, we use the following reactions:

αlh + αl−1,g + θgh∗ + ψh∗vl −−→ αlh∗ + αl−1,g + θgh∗ + ψh∗vl
αlh∗ + αl−1,g + θgh + ψhvl −−→ αlh + αl−1,g + θgh + ψhvl

for all g ∈ H, h ∈ H \ {h∗}, l = 2, . . . , L and w ∈ V . Again by design

αlh ×





g∈H
X

αl−1,gθgh∗ ψh∗vl


= αlh∗ ×



αl−1,gθghψhvl


g∈H
X



is the balance equation for the set of reactions corresponding to each (h, l) ∈ H \{h∗}×{2, . . . , L}.
The above reactions depend on the observed sequence (v1, v2, . . . , vL) ∈ V L of visible state.
And this is a problem because one would have to design different reaction networks for different
observed sequences. To solve this problem we introduce the species Elw with l = 1, . . . , L and
w ∈ V . Now with the Elw species, we use the following reactions for the forward algorithm:

α1h + πh∗ + ψh∗w + E1w −−→ α1h∗ + πh∗ + ψh∗w + E1w
α1h∗ + πh + ψhw + E1w −−→ α1h + πh + ψhw + E1w

for all h ∈ H \ {h∗} and w ∈ V .

αlh + αl−1,g + θgh∗ + ψh∗w + Elw −−→ αlh∗ + αl−1,g + θgh∗ + ψh∗w + Elw
αlh∗ + αl−1,g + θgh + ψhw + Elw −−→ αlh + αl−1,g + θgh + ψhw + Elw

for all g ∈ H, h ∈ H \{h∗}, l = 2, . . . , L and w ∈ V . The Elw species are to be initialized such that
Elw = 1 iff w = vl. So different observed sequences can now be processed by the same reaction
network, by appropriately initializing the species Elw.

The other parts of the Baum-Welch algorithm may be translated into chemical reactions us-
ing a similar logic. We call the resulting reaction network as the Baum-Welch Reaction Network
BW (M, h∗, v∗, L). It consists of four subnetworks corresponding to the four parts of the Baum-
Welch algorithm, as shown in Table 1.

6

Table 1: Baum-Welch Reaction Network: The steps and reactions are
for all g, h ∈ H, w ∈ V and l = 1, . . . , L − 1. Notice there are some null
aiXi. As these null reaction have no
reactions of the form
effect on the dynamics, they can be ignored.
P

aiXi →

P

Baum-Welch Algorithm

Baum-Welch Reaction Network

α1h = πhψhv1

α1h + πh∗ + ψh∗w + E1w −−→ α1h∗ + πh∗ + ψh∗w + E1w
α1h∗ + πh + ψhw + E1w −−→ α1h + πh + ψhw + E1w

αl+1,h =

αlgθghψhvl+1

g∈H
X

βLh = 1

βlh =

θhgψgvl+1 βl+1,g

g∈H
X

γlh =

αlhβlh
g∈H αlgβlg

P

αl+1,h + αlg + θgh∗ + ψh∗w + El+1,w −−→
αl+1,h∗ + αlg + θgh∗ + ψh∗w + El+1,w
αl+1,h∗ + αlg + θgh + ψhw + El+1,w −−→
αl+1,h + αlg + θgh + ψhw + El+1,w

βlh + βl+1,g + θh∗g + ψgw + El+1,w −−→
βlh∗ + βl+1,g + θh∗g + ψgw + El+1,w
βlh∗ + βl+1,g + θhg + ψgw + El+1,w −−→
βlh + βl+1,g + θhg + ψgw + El+1,w

γlh + αlh∗ + βlh∗ −−→ γlh∗ + αlh∗ + βlh∗
γlh∗ + αlh + βlh −−→ γlh + αlh + βlh

ξlgh+αlh∗ + θh∗h∗ + βl+1,h∗ + ψh∗w + El+1,w −−→

ξlgh =

αlgθghψhvl+1βl+1,h
f ∈H αlf βlf

ξlh∗h∗ +αlh∗ + θh∗h∗ + βl+1,h∗ + ψh∗w + El+1,w
ξlh∗h∗ +αlg + θgh + βl+1,g + ψhw + El+1,w −−→

P

ξlgh+αlg + θgh + βl+1,g + ψhw + El+1,w

θgh ←

L−1
l=1 ξlgh

L−1
P
l=1

f ∈H ξlgf

P

P

ψhw ←

L
l=1 γlhδw,vl
L
l=1 γlh

P

P

θgh + ξlgh∗ −−→ θgh∗ + ξlgh∗
θgh∗ + ξlgh −−→ θgh + ξlgh

ψhw + γlh + Elv∗ −−→ ψhv∗ + γlh + Elv∗
ψhv∗ + γlh + Elw −−→ ψhw + γlh + Elw

The Baum-Welch reaction network described above has a special structure. Every reaction is
a monomolecular transformation catalyzed by a set of species. The reverse transformation is also
present, catalyzed by a different set of species to give the network a “futile cycle” [40] structure.

7

In addition, each connected component in the undirected graph representation of the network has a
topology with all transformations happening to and from a central species. This prompts the follow-
ing deﬁnitions.

αlh + αl−1,g + θih∗ + ψh∗w + Elw
αlh∗ + αl−1,g + θih∗ + ψh∗w + Elw

αlh∗ + αl−1,g + θgh + ψhw + Elw
αlh + αl−1,g + θgh + ψhw + Elw

kα+
lh−−−→

kα−
lh−−−→

αlh

kα
lh

αlh∗

lh = kα+
kα

lh = kα−

lh

|H| × |V |

αlh

|H| × |V |

βlh

|V |

ξlgh

Petal

αlh∗

kα
lh

βlh∗

kβ
lh

ξlh∗h∗

kξ
lgh

Flower

γlh

γlh∗

kγ
lh

L

θgh

L

ψhw

θgh∗

kθ
gh

ψhv∗

kψ
hw

Fig. 2. The Baum-Welch Reaction Network represented as an undirected graph. (a) Each reaction is a
unimolecular transformation driven by catalysts. The nodes represents the species undergoing transformation.
The edge represents a pair of reactions which drive this transformation backwards and forwards in a futile cycle.
Catalysts are omitted for clarity. (b) The network decomposes into a collection of disjoint ﬂowers. Nodes
represent species and edges represent pairs of reactions, species α, γ have L ﬂowers each, β, ξ have L − 1
ﬂowers each, and species θ and ψ have |H| ﬂowers each (not shown in ﬁgure). All reactions in the same petal
have the same speciﬁc rate constant, so the dimension of the space of permissible rate constants is equal to the
number of petals in the graph.

Deﬁnition 1 (Flowers, petals, gardens). A graph is a triple (Nodes, Edges, π) where Nodes and
Edges are ﬁnite sets and π is a map from Edges to unordered pairs of elements from Nodes. A ﬂower
is a graph with a special node n∗ such that for every edge e ∈ Edges we have n∗ ∈ π(e). A garden
is a graph which is a union of disjoint ﬂowers. A petal is a set of all edges e which have the same
π(e), i.e. they are incident between the same pair of nodes.

Figure 2 shows how the Baum-Welch reaction network can be represented as a garden graph, in

which species are nodes and reactions are edges.

A collection of speciﬁc rates is permissible if all reactions in a petal have the same rate. How-
ever, different petals may have different rates. We will denote the speciﬁc rate for a petal by su-
perscripting the type of the species and subscripting its indices. For example, the speciﬁc rate for
reactions in the petal for αlh would be denoted as kα
lh. The notation for the remaining rate constants
can be read from Figure 2.

8

Remark 3. The “ﬂower” topology we have employed for the Baum-Welch reaction network is only
one among several possibilities. The important thing is to achieve connectivity between different
nodes that ought to be connected, ensuring that the ratio of the concentrations of the species de-
noted by adjacent nodes takes the value as prescribed by the Baum-Welch algorithm. Therefore,
many other connection topologies can be imagined, for example a ring topology where each node is
connected to two other nodes while maintaining the same connected components. In fact, there are
obvious disadvantages to the star topology, with a single point of failure, whereas the ring topology
appears more resilient. How network topology affects basins of attraction, rates of convergence, and
emergence of spurious equilibria in our algorithm is a compelling question beyond the scope of this
present work.

The Baum-Welch reaction network with a permissible choice of rate constants k will deﬁne a
Baum-Welch reaction system (BW (M, h∗, v∗, L), k) whose deterministic mass action kinetics
equations will perform a continuous time version of the Baum-Welch algorithm. We call this the
Chemical Baum-Welch Algorithm, and describe it in Algorithm 1.

Algorithm 1: Chemical Baum-Welch Algorithm

Input: An HMM M = (H, V, θ, ψ, π) and an observed sequence v ∈ V L
Output: Parameters ˆθ ∈ R|H|×|H|
Initialization of concentrations at t=0:

, ˆψ ∈ R|H|×|V |
≥0

≥0

1. For w ∈ V and l = 1, . . . , L, initialize Elw(0) such that Elw(0) =

1 if w = vl
0 otherwise

(

2. For g ∈ H, initialize βLg = β
3. For every other species, initialize its concentration arbitrarily in R>0.

Algorithm:

Run the Baum-Welch reaction system with mass action kinetics until convergence.

4 Analysis and Simulations

The Baum-Welch reaction network has number of species of each type as follows:

Type
Number

α

π
|H| |H|L |H|L |H|2 |H||V | L|V | |H|L |H|2(L − 1)

E

ψ

β

γ

θ

ξ

The total number of species is |H| + 3|H|L + |H|2 + |H||V | + L|V | + |H|2(L − 1) which is
O(L|H|2 + |H||V | + L|V |). The number of reactions (ignoring the null reactions of the form

aiXi →

aiXi) in each part is:

P

P
Forward
2(|H| − 1)V +
2|H|(|H| − 1)(L − 1)|V |

Backward
(2|H|(|H| − 1)|V |)
(L − 1)

Expectation

Maximization

2L(|H| − 1)+ 2|H|(|H| − 1)(L − 1)

2(L − 1)(|H|2 − 1) +2|H|L(|V | − 1)

so that the total number of reactions is O(|H|2L|V |).

The ﬁrst theorem shows that the Chemical Baum-Welch Algorithm recovers all of the Baum-

Welch equilibria.

Theorem 1. Every ﬁxed point of the Baum-Welch algorithm for an HMM M = (H, V, θ, ψ, π) is a
ﬁxed point for the corresponding Chemical Baum-Welch Algorithm with permissible rates k.

9

See Appendix A.1 for proof.

We say a vector of real numbers is positive if all its components are strictly greater than 0. The
next theorem shows that positive equilibria of the Chemical Baum-Welch Algorithm are also ﬁxed
points of the Baum-Welch algorithm.

Theorem 2. Every positive ﬁxed point for the Chemical Baum-Welch Algorithm on a Baum-Welch
Reaction system (BW (M, h∗, v∗, L), k) with permissible rate k is a ﬁxed point for the Baum-Welch
algorithm for the HMM M = (H, V, θ, ψ, π).

See Appendix A.1 for proof.

The Baum-Welch algorithm is an iterative algorithm. The E step and the M step are run itera-
tively. In contrast, the Chemical Baum-Welch algorithm is a generalized EM algorithm [41] where
all the reactions are run at the same time in a single-pot reaction.

The next theorem shows that the E step consisting of the forward network, the backward net-
work, and the E network, converges exponentially fast to the correct equilibrium if the θ and ψ
species are held ﬁxed at a positive point.

Theorem 3. For the Baum-Welch Reaction System (BW (M, h∗, v∗, L), k) with permissible rates
k, if the concentrations of θ and ψ species are held ﬁxed at a positive point then the Forward,
Backward and Expection step reaction systems on α, β, γ and ψ species converge to equilibrium
exponentially fast.

See Appendix A.2 for proof.

The next theorem shows that if the α, β, γ, ξ species are held ﬁxed at a positive point, then the
M step consisting of reactions modifying the θ and ψ species converges exponentially fast to the
correct equilibrium.

Theorem 4. For the Baum-Welch Reaction System (BW (M, h∗, v∗, L), k) with permissible rates
k, if the concentrations of α, β, γ and ξ species are held ﬁxed at a positive point then the Maximiza-
tion step reaction system on θ and ψ converges to equilibrium exponentially fast.

See Appendix A.2 for proof.
The following examples demonstrate the behavior of the Chemical Baum-Welch Algorithm.

Example 1. Consider an HMM (H, V, θ, ψ, π) with two hidden states H = {H1, H2} and two
emitted symbols V = {V1, V2} where the starting probability is π = (0.6, 0.4), initial transition

probability is θ =

, and initial emission probability is ψ =

. Suppose we wish to

0.5 0.5
0.5 0.5

(cid:20)

(cid:21)

0.6 0.4
0.3 0.7

(cid:20)

(cid:21)

learn (θ, ψ) for the following observed sequence:
(V1, V1, V1, V2, V1, V1, V2, V2, V2, V1, V2, V2, V1, V1, V1, V1, V2, V2, V2, V1, V2, V1, V1, V2, V2). We ini-
tialize the corresponding reaction system by setting species El,vl = 1 and El,w = 0 for w 6= vl, and
run the dynamics according to deterministic mass-action kinetics. Initial conditions of all species
that are not mentioned are chosen to be nonzero, but otherwise at random.
For our numerical solution, we observe that the reaction network equilibrium point coincides with
the Baum-Welch steady state ˆθ =

0.5071 0.4928
0.0000 1.0000
(cid:21)
The next example shows that the Chemical Baum-Welch algorithm can sometimes get stuck
at points that are not equilibria for the Baum-Welch algorithm. This is a problem especially for
very short sequences, and is probably happening because in such settings, the best model sets many
parameters to zero. When many species concentrations are set to 0, many reactions get turned off,

1.0000 0.0000
0.4854 0.5145
(cid:21)

(See Figure 3).

and ˆψ =

(cid:20)

(cid:20)

10

Ψ

,

Θ

f
o

n
o
i
t
a
r
t
n
e
c
n
o
C

1.0

0.8

0.6

0.4

0.2

0.0

π1
π2
θ11
θ12
θ21
θ22
ψ11
ψ12
ψ21
ψ22

Ψ

,

Θ

1.0

0.8

0.6

0.4

0.2

0.0

π1
π2
θ11
θ12
θ21
θ22
ψ11
ψ12
ψ21
ψ22

0

250

500

750

1000
Time (s)

1250

1500

1750

2000

0

25

50

75

100
Iteration

125

150

175

200

(a) Reaction Network Dynamics

(b) Baum-Welch Algorithm

Fig. 3. Simulation of Example 1: Both simulations are started at exactly the same initial vector. This may not
be apparent in the ﬁgure because concentration of some species change rapidly at start in the reaction network.

and the network gets stuck away from the desired equilibrium. We believe this will not happen if the
HMM generating the observed sequence has nonzero parameters, and the observed sequence is long
enough.

Example 2. Consider an HMM (H, V, θ, ψ, π) with two hidden states H = {H1, H2} and two emit-
ted symbols V = {V1, V2}, initial distribution π = (0.6, 0.4) is ﬁxed, initial transition probability

, and initial emission probability ψ =

. Suppose we wish to learn (θ, ψ) for

the sequence (V1, V2, V1, V2, V1). We again simulate the corresponding reaction network and also
perform the Baum-Welch algorithm for comparison.

θ =

0.6 0.4
0.3 0.7

(cid:20)

(cid:21)

0.5 0.5
0.5 0.5

(cid:20)

(cid:21)

Ψ

,

Θ

f
o

n
o
i
t
a
r
t
n
e
c
n
o
C

1.0

0.8

0.6

0.4

0.2

0.0

π1
π2
θ11
θ12
θ21
θ22
ψ11
ψ12
ψ21
ψ22

1.0

0.8

Ψ

,

Θ

0.6

0.4

0.2

0.0

π1
π2
θ11
θ12
θ21
θ22
ψ11
ψ12
ψ21
ψ22

0

20

40

60

80

100

0

10

20

30

40

50

Time (s)

Iteration

(a) Reaction Network Dynamics

(b) Baum-Welch Algorithm

Fig. 4. Simulation of Example 2: Both simulations are started at exactly the same initial vector. This may not
be apparent in the ﬁgure because concentration of some species change rapidly at start in the reaction network.

Figure 4 shows that the reaction network equilibrium point does not coincide with the Baum-Welch
steady state. Both converge to ˆψ =

. However, the reaction network converges

1.0000 0.0000
0.0000 1.0000
(cid:21)
whereas the Baum-Welch algorithm converges to ˆθ =

(cid:20)

to ˆθ =

0.3489 0.6510
1.0000 0.0000
(cid:21)

(cid:20)

0.0000 1.0000
1.0000 0.0000
(cid:21)

(cid:20)

11

 
 
 
 
 
 
which happens to be the true maximum likelihood point. Note that this does not contradict Theorem
2 because the ﬁxed point of this system is a boundary point and not a positive ﬁxed point.

5 Related work

In previous work, our group has shown that reaction networks can perform the Expectation Maxi-
mization (EM) algorithm for partially observed log linear statistical models [25, 42]. That algorithm
also applies “out of the box” to learning HMM parameters. The problem with that algorithm is that
the size of the reaction network would become exponentially large in the length of the sequence,
so that even examples like Example 1 with an observation sequence of length 25 would become
impractical. In contrast, the scheme we have presented in this paper requires only a linear growth
with sequence length. We have obtained the savings by exploiting the graphical structure of HMMs.
This allows us to compute the likelihoods α in a “dynamic programming” manner, instead of having
to explicitly represent each path as a separate species.

Napp and Adams [20] have shown how to compute marginals on graphical models with reac-
tion networks. They exploit graphical structure by mimicking belief propagation. Hidden Markov
Models can be viewed as a special type of graphical model where there are 2L random variables
X1, X2, . . . , XL, Y1, Y2, . . . , YL with the X random variables taking values in H and the Y random
variables in V . The X random variables form a Markov chain X1 X2
. . . XL. In addition,
there are L edges from Xl to Yl for l = 1 to L denoting observations. Specialized to HMMs, the
scheme of Napp and Adams would compute the equivalent of steady state values of the γ species,
performing a version of the E step. They are able to show that true marginals are ﬁxed points of their
scheme, which is similar to our Theorem 1. Thus their work may be viewed as the ﬁrst example of
a reaction network scheme that exploits graphical structure to compute E projections. Our E step
goes further by proving correctness as well as exponential convergence. Their work also raises the
challenge of extending our scheme to all graphical models.

Poole et al. [43] have described Chemical Boltzmann Machines, which are reaction network
schemes whose dynamics reconstructs inference in Boltzmann Machines. This inference can be
viewed as a version of E projection. No scheme for learning is presented. The exact schemes pre-
sented there are exponentially large. The more realistically sized schemes are presented there with-
out proof. In comparison, our schemes are polynomially sized, provably correct if the equilibrium is
positive, and perform both inference and learning for HMMs.

Zechner et al. [11] have shown that Kalman ﬁlters can be implemented with reactions networks.
Kalman ﬁlters can be thought of as a version of Hidden Markov Models with continuous hidden
states [44]. It would be instructive to compare their scheme with ours, and note similarities and
differences. In passing from position l to position l + 1 along the sequence, our scheme repeats the
same reaction network that updates αl+1 using αl values. It is worth examining if this can be done
“in place” so that the same species can be reused, and a reaction network can be described that is
not tied to the length L of the sequence to be observed.

Recently Cherry et al. [10] have given a brilliant experimental demonstration of learning with
DNA molecules. They have empirically demonstrated a DNA molecular system that can classify
9 types of handwritten digits from the MNIST database. Their approach is based on the notion
of “winner-takes-all” circuits due to Maass [45] which was originally a proposal for how neural
networks in the brain work. Winner-take-all might also be capable of approximating HMM learning,
at least in theory [46], and it is worth understanding precisely how such schemes relate to the kind
of scheme we have described here. It is conceivable that our scheme could be converted to winner-
take-all by getting different species in the same ﬂower to give negative feedback to each other. This

12

might well lead to sampling the most likely path, performing a decoding task similar to the Viterbi
algorithm.

6 Discussion

We have described a one-pot one-shot reaction network implementation of the Baum-Welch algo-
rithm. Firstly, this involves proposing a reaction system whose positive ﬁxed points correspond to
equilibria of the Baum-Welch Algorithm. Secondly, this involves establishing the conditions un-
der which convergence of solutions to the equilibria can be accomplished. Further, from a practical
perspective, it is essential to obtain an implementation that does not rely on repeated iteration of a
reaction scheme but only requires to be run once.

As we observe in Remark 3, there is a whole class of reaction networks that implements the
Baum-Welch algorithm. We have proposed one such network and are aware that there are other
networks, potentially with more efﬁcient dynamical and analytical properties than the one proposed
here. Finding and characterizing efﬁcient reaction networks that can do complicated statistical tasks
will likely be of future concern.

We have only discussed deterministic dynamical properties of the network. However, in realistic
biological contexts one might imagine that the network is implemented by relatively few molecules
such that stochastic effects are signiﬁcant. Consequently, the study of the Baum-Welch reaction
system under stochastic mass-action kinetics is likely to be of interest.

Lastly, we have mentioned the Viterbi algorithm, but have made no attempt to describe how
the maximum likelihood sequence can be recovered from our reaction network. This decoding step
is likely to be of as much interest for molecular communication systems and molecular multi-agent
systems as it is in more traditional domains of communications and multi-agent reinforcement learn-
ing. Because of the inherent stochasticity of reaction networks, there might even be opportunities
for list decoding by sampling different paths through the hidden states that have high probability
conditioned on the observations. This might give an artiﬁcial cell the ability to “imagine” different
possible realities, and act assuming one of them to be the case, leading to an intrinsic stochasticity
and unpredictability to the behavior.

References

1. David Soloveichik, Georg Seelig, and Erik Winfree. DNA as a universal substrate for chemical kinetics.

Proceedings of the National Academy of Sciences, 107(12):5393–5398, 2010.

2. Niranjan Srinivas. Programming chemical kinetics: engineering dynamic reaction networks with DNA

strand displacement. PhD thesis, California Institute of Technology, 2015.

3. Lulu Qian, David Soloveichik, and Erik Winfree. Efﬁcient Turing-universal computation with DNA poly-

mers. In DNA computing and molecular programming, pages 123–140. 2011.

4. Luca Cardelli. Strand Algebras for DNA Computing. Natural Computing, 10:407–428, 2011.
5. Matthew R. Lakin, Simon Youssef, Luca Cardelli, and Andrew Phillips. Abstractions for DNA circuit

design. Journal of The Royal Society Interface, 9(68):470–486, 2011.

6. Luca Cardelli. Two-domain DNA strand displacement. Mathematical Structures in Computer Science,

23:02, 2013.

7. Yuan-Jyue Chen, Neil Dalchau, Niranjan Srinivas, Andrew Phillips, Luca Cardelli, David Soloveichik, and
Georg Seelig. Programmable chemical controllers made from DNA. Nature nanotechnology, 8(10):755–
762, 2013.

8. Matthew R. Lakin, Darko Stefanovic, and Andrew Phillips. Modular veriﬁcation of chemical reaction

network encodings via serializability analysis. Theoretical Computer Science, 632:21–42, 2016.

13

9. Niranjan Srinivas, James Parkin, Georg Seelig, Erik Winfree, and David Soloveichik. Enzyme-free nucleic

acid dynamical systems. Science, 358:6369, 2017.

10. Kevin M. Cherry and Lulu Qian. Scaling up molecular pattern recognition with DNA-based winner-take-all

neural networks. Nature, 559:7714, 2018.

11. Christoph Zechner, Georg Seelig, Marc Rullan, and Mustafa Khammash. Molecular circuits for dynamic

noise ﬁltering. Proceedings of the National Academy of Sciences, 113(17):4729–4734, 2016.

12. Stefan Badelt, Seung Woo Shin, Robert F. Johnson, Qing Dong, Chris Thachuk, and Erik Winfree. A
general-purpose CRN-to-DSD compiler with formal veriﬁcation, optimization, and simulation capabilities.
In International Conference on DNA-Based Computers, pages 232–248, 2017.

13. Matthew R. Lakin, Simon Youssef, Filippo Polo, Stephen Emmott, and Andrew Phillips. Visual DSD: a

design and analysis tool for dna strand displacement systems. Bioinformatics, 27(22):3211–3213, 2011.

14. Allen Hjelmfelt, Edward D. Weinberger, and John Ross. Chemical implementation of neural networks and

Turing machines. Proceedings of the National Academy of Sciences, 88(24):10983–10987, 1991.

15. H J Buisman, Huub MM ten Eikelder, Peter AJ Hilbers, and Anthony ML Liekens. Computing algebraic

functions with biochemical reaction networks. Artiﬁcial life, 15(1):5–19, 2009.

16. Kevin Oishi and Eric Klavins. Biomolecular implementation of linear I/O systems. Systems Biology, IET,

5(4):252–260, 2011.

17. David Soloveichik, Matthew Cook, Erik Winfree, and Jehoshua Bruck. Computation with ﬁnite stochastic

chemical reaction networks. natural computing, 7(4):615–633, 2008.

18. Ho-Lin Chen, David Doty, and David Soloveichik. Deterministic function computation with chemical

reaction networks. Natural computing, 13(4):517–534, 2014.

19. Lulu Qian and Erik Winfree. Scaling up digital circuit computation with DNA strand displacement cas-

cades. Science, 332(6034):1196–1201, 2011.

20. Nils E. Napp and Ryan P. Adams. Message passing inference with chemical reaction networks. Advances

in Neural Information Processing Systems, pages 2247–2255, 2013.

21. Lulu Qian, Erik Winfree, and Jehoshua Bruck. Neural Network Computation with DNA Strand Displace-

ment Cascades. Nature, 475(7356):368–372, 2011.

22. Luca Cardelli, Marta Kwiatkowska, and Max Whitby. Chemical reaction network designs for asynchronous

logic circuits. Natural computing, 17(1):109–130, 2018.

23. Manoj Gopalkrishnan. A scheme for molecular computation of maximum likelihood estimators for log-
linear models. In 22nd International Conference on DNA Computing and Molecular Programming, pages
3–18, 2016.

24. Muppirala Viswa Virinchi, Abhishek Behera, and Manoj Gopalkrishnan. A stochastic molecular scheme
for an artiﬁcial cell to infer its environment from partial observations. In 23rd International Conference on
DNA Computing and Molecular Programming, 2017.

25. Muppirala Viswa Virinchi, Abhishek Behera, and Manoj Gopalkrishnan. A reaction network scheme which
implements the EM algorithm. In International Conference on DNA Computing and Molecular Program-
ming, pages 189–207, 2018.

26. Shunichi Amari. Information geometry and its applications. Springer, 2016.
27. Imre Csisz´ar and Frantisek Matus. Information projections revisited. IEEE Transactions on Information

Theory, 49(6):1474–1490, 2003.

28. Edwin T. Jaynes. Information theory and statistical mechanics. Physical review, 106:4, 1957.
29. Jong-Shik Shin and Niles A. Pierce. A synthetic DNA walker for molecular transport. Journal of the

American Chemical Society, 126(35):10834–10835, 2004.

30. John Reif. The Design of Autonomous DNA Nano-mechanical Devices: Walking and Rolling DNA. DNA

Computing, pages 439–461, 2003.

31. William Sherman and Nadrian Seeman. A Precisely Controlled DNA Biped Walking Device. Nano Letters,

4:1203–1207, 2004.

32. Thomas M. Cover and Joy A. Thomas. Elements of information theory. Wiley, John & Sons, 2012.
33. L. R. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition.

Proceedings of the IEEE, 77(2):257–286, February 1989.

34. Biing Hwang Juang and Laurence R. Rabiner. Hidden Markov models for speech recognition. Technomet-

rics, 33(3):251–272, 1991.

14

35. Martin Feinberg. On chemical kinetics of a certain class. Arch. Rational Mech. Anal, 46, 1972.
36. Friedrich J. M. Horn. Necessary and sufﬁcient conditions for complex balancing in chemical kinetics.

Arch. Rational Mech. Anal, 49, 1972.

37. Martin Feinberg. Lectures on chemical reaction networks. http://www.che.eng.ohio-state.edu/FEINBERG/LecturesOnReactionNetworks/.

1979.

38. Manoj Gopalkrishnan. Catalysis in Reaction Networks. Bulletin of Mathematical Biology, 73(12):2962–

2982, 2011.

39. David F. Anderson, Gheorghe Craciun, and Thomas G. Kurtz. Product-form stationary distributions for
deﬁciency zero chemical reaction networks. Bulletin of mathematical biology, 72(8):1947–1970, 2010.
40. Benjamin P. Tu and Steven L. McKnight. Metabolic cycles as an underlying basis of biological oscillations.

Nature reviews Molecular cell biology, 7:9, 2006.

41. Geoffrey McLachlan and Thriyambakam Krishnan. The EM algorithm and extensions, volume 382. Wiley,

John & Sons, 2007.

42. Abhinav Singh and Manoj Gopalkrishnan. EM Algorithm with DNA Molecules. In Poster Presentations
of the 24-th edition of International Conference on DNA Computing and Molecular Programming, 2018.
43. William Poole, Andr´es Ortiz-Munoz, Abhishek Behera, Nick S. Jones, Thomas E. Ouldridge, Erik Winfree,
and Manoj Gopalkrishnan. Chemical boltzmann machines. In International Conference on DNA-Based
Computers, pages 210–231, 2017.

44. S. Roweis and Z. Ghahramani. A unifying review of linear Gaussian models. Neural Computation,

11(2):305–345, 1999.

45. Wolfgang Maass. On the computational power of winner-take-all. Neural computation, 12(11):2519–2535,

2000.

46. David Kappel, Bernhard Nessler, and Wolfgang Maass. STDP installs in winner-take-all circuits an online

approximation to hidden Markov model learning. PLoS computational biology, 10:3, 2014.

Appendix A

Appendix A.1 Comparing points of equilibria

We will now prove Theorem 1. For the sake of convenience, we ﬁrst recall the statement.

Theorem 1. Every ﬁxed point of the Baum-Welch algorithm for an HMM M = (H, V, θ, ψ, π) is a
ﬁxed point for the corresponding Chemical Baum-Welch Algorithm with permissible rates k.

Proof. Consider a point Φ = (α′, β′, γ′, ξ′, θ′, ψ′) with α′, β′, γ′ ∈ RL×H
θ′ ∈ RH×H

, ξ′ ∈ R(L−1)×H×H
≥0
. If Φ is a ﬁxed point of Baum-Welch Algorithm then it must satisfy:

and ψ′ ∈ RH×V

≥0

,

≥0

≥0

– α′

1h = πhψ′

hv1 for all h ∈ H. Then for the chemical Baum-Welch Algorithm we have

˙α1h

Φ = kα
(cid:12)
(cid:12)
ghψ′
g∈H α′
Welch Algorithm we have

for all h ∈ H \ {h∗} and ˙α1h∗

l−1,gθ′

lh =

hvl

1h

– α′

P

α′
1h∗ πhψ′

hv1 − α′

1hπh∗ ψ′

h∗v1

= 0

h6=h∗ ˙α1h

(cid:0)
Φ = −
for all h ∈ H and l = 2, . . . , L . Then for the chemical Baum-
(cid:12)
(cid:12)

Φ = 0.

P

(cid:12)
(cid:12)

(cid:1)

˙αlh

Φ = kα

lh 

α′

lh∗

l−1,gθ′
α′

ghψ′

hvl − α′
lh


for all h ∈ H \ {h∗} and l = 2, . . . , L and ˙αlh∗

(cid:12)
(cid:12)

g∈H
X

l−1,gθ′
α′

gh∗ ψ′

h∗vl

= 0



g∈H
X

Φ = −

h6=h∗ ˙αlh

Φ = 0.

(cid:12)
(cid:12)

15

P

(cid:12)
(cid:12)

– β′

lh =
Baum-Welch Algorithm we have

gvl+1 β′

g∈H θ′

hgψ′

P

l+1,g for all h ∈ H and l = 1, . . . , L − 1. Then for the chemical

˙βlh

Φ = kβ

lh 

β′
lh∗

(cid:12)
(cid:12)



h∗gψ′
θ′

gvl+1 β′

l+1,g − β′
lh

g∈H
X

g∈H
X

hgψ′
θ′

gvl+1β′

l+1,g

= 0

for all h ∈ H \ {h∗} and l = 1, 2, . . . , L − 1 and ˙βlh∗
l(h) =
lg β′
Algorithm we have

Φ = −
for all h ∈ H and l = 1, 2, . . . , L − 1. Then for the chemical Baum-Welch
(cid:12)
(cid:12)

α′
lhβ′
Pg∈H α′

h6=h∗ ˙βlh

P

lh

lg

– γ′


Φ = 0.
(cid:12)
(cid:12)

˙γlh = kγ
lh

γ′
lh∗ α′

lhβ′

lh − γ′

lhα′

lh∗ β′

lh∗

= 0

for all h ∈ H \ {h∗} and l = 1, 2, . . . , L − 1 and ˙γlh∗ = −

(cid:0)

(cid:1)
h6=h∗ ˙γlh = 0.

for all g, h ∈ H and l = 1, . . . , L − 1. Then for the chemical
P

– ξ′

lg θ′
α′

l(g, h) =
Baum-Welch Algorithm we have

lf

ghψ′
hvl+1
Pf ∈H α′

β′
lf β′

l+1,h

˙ξlgh

Φ = kξ

lgh

ξ′
lh∗h∗ α′

lgθ′

ghψ′

hvl+1 β′

l+1,h − ξ′

lghα′

lh∗ θ′

h∗h∗ ψ′

h∗vl+1 β′

l+1,h∗

= 0

(cid:12)
(cid:12)

(cid:16)

for all g, h ∈ H×H\{(h∗, h∗)} and l = 1, . . . , L−1 and ˙ξlh∗h∗
0.
– θ′
gh =
have

PL−1
l=1 ξ′
l=1 Pf ∈H ξ′

PL−1

l(g,f )

l(g,h)

Φ = −
(cid:12)
(cid:12)

P

for all g, h ∈ H. Then for the chemical Baum-Welch Algorithm we

Φ =
(cid:12)
(cid:12)

(cid:17)

(g,h)6=(h∗,h∗)

˙ξlgh

for all g ∈ H and h ∈ H \ {h∗} and ˙θgh∗

Φ = −

˙θgh

Φ = kθ

θ′
gh∗

gh 

(cid:12)
(cid:12)



L−1

l=1
X

(cid:12)
(cid:12)

L−1

lgh − θ′
ξ′
gh

l(g, h∗)
ξ′

= 0



l=1
X
h6=h∗ ˙θgh


Φ = 0.
(cid:12)
(cid:12)

Φ = kθ

gh 

θ′
gh∗

L−1

L−1

lgh − θ′
ξ′
gh

l(g, h∗)
ξ′

= 0



– ψ′

PL

hw =

l=1 γ′
PL
rithm we have

l(h)δw,vl
l (h)

l=1 γ′

˙θgh

for all h ∈ H and w ∈ V . Then for the chemical Baum-Welch Algo-

P

l=1
X
for all h ∈ H and w ∈ V \ {v∗} and ˙ψhw∗



(cid:12)
(cid:12)

l=1
X
w6=w∗ ˙ψhw

Φ = −

So Φ is ﬁxed point of the chemical Baum-Welch Algorithm.

P

(cid:12)
(cid:12)


Φ = 0.

(cid:12)
(cid:12)

⊓⊔

We will now prove Theorem 2. For the sake of convenience, we ﬁrst recall the statement.

Theorem 2. Every positive ﬁxed point for the Chemical Baum-Welch Algorithm on a Baum Welch
Reaction system (BW (M, h∗, v∗, L), k) with permissible rate k is a ﬁxed point for the Baum-Welch
algorithm for the HMM M = (H, V, θ, ψ, π).

Proof. Consider a positive point Φ = (α′, β′, γ′, ξ′, θ′, ψ′) with α′, β′, γ′ ∈ RL×H
θ′ ∈ RH×H
we must have:

and ψ′ ∈ RH×V

. If Φ is a ﬁxed point for the Chemical Baum-Welch Algorithm then

, ξ′ ∈ R(L−1)×H×H
>0

,

>0

>0

>0

16

– ˙α1h

Φ = 0 for all h ∈ H. This implies α′

1h × πh∗ψ′

h∗v1 = α′

1h∗ × πhψ′

hv1

for all h ∈ H \ h∗.

Since Φ is positive, this implies

(cid:12)
(cid:12)

α′

1h =

πhψ′

hv1

(cid:0)

(cid:1)

f ∈H α′
1f
f ∈H πf ψ′
P

f v1

for all h ∈ H

Φ = 0 for all h ∈ H and l = 2, . . . , L. This implies α′
– ˙αlh
α′
lh∗ ×
(cid:12)
implies
(cid:12)

h∗vl =
lh ×
for all h ∈ H \ {h∗} and l = 2, . . . , L. Since Φ is positive, this

g∈H α′

g∈H α′

l−1,gθ′

l−1,gθ′

gh∗ ψ′

ghψ′

P

P

hvl

P

α′

lh =



g∈H
X

– ˙βlh
β′
lh∗ ×
(cid:12)
this implies
(cid:12)

P

β′
lh =



g∈H
X



– ˙γlh
lhβ′
α′
lh
(cid:12)
(cid:12)

α′
l−1,gθ′

ghψ′

hvl 

f ∈H α′
lf
gf ψ′
l−1,gθ′

f vl

f,g∈H α′
P

for all h ∈ H and l = 2, . . . , L




P
Φ = 0 for all h ∈ H and l = 1, . . . , L. This implies β′

lh ×
l+1,g =
for all h ∈ H \{h∗} and l = 1, . . . , L−1. Since Φ is positive,

gvl+1 β′

g∈H θ′

hgψ′

g∈H θ′

h∗gψ′

gvl+1 β′

l+1,g

P

hgψ′
θ′

gvl+1 β′

l+1,g

f ∈H β′
lf
gvl+1 β′
f gψ′

l+1,g

f,g∈H θ′
P

for all h ∈ H and l = 1, . . . , L−1

P
Φ = 0 for all h ∈ H and l = 1, . . . , L. This implies γ′

lh∗ = γ′
for all h ∈ H \ {h∗} and l = 1, 2, . . . , L − 1. Since Φ is positive, this implies

l(h) × α′

lh∗ β′

l(h∗) ×



γ′
lh =

lhβ′
α′
lh
lgβ′
g∈H α′

lg !

γ′
lg

g∈H
X

for all h ∈ H and l = 1, 2, . . . , L − 1

P

– ˙ξlgh
ξ′
l(h∗, h∗) × α′
hvl+1
Φ is positive, this implies

ghψ′

lgθ′

(cid:12)
(cid:12)

Φ = 0 for all g, h ∈ H and l = 1, . . . , L−1. This implies ξ′

l(g, h)×α′

lh∗ θ′

h∗h∗ψ′

h∗vl+1

β′
l+1,h∗ =

l+1,h for all g, h ∈ H × H \ {(h∗, h∗)} and l = 1, . . . , L − 1. Since
β′

e,f ∈H
X
gh×

ξ′
lgh =

lgθ′
α′
e,f ∈H α′

ghψ′
lf θ′

hvl+1
ef ψ′

β′

l+1,h
β′

f vl+1

l+1,f !

ξ′
lef

for all g, h ∈ H×H and l = 1, . . . , L−1

P

– ˙θgh

L−1
l=1 ξ′
H and h ∈ H \ {h∗}. Since Φ is positive, this implies

Φ = 0 for all g, h ∈ H. This implies θ′
(cid:12)
(cid:12)

P

l(g, h∗) = θ′

gh∗ ×

L−1
l=1 ξ′

l(g, h)

for all g ∈

P

θ′
gh =





L−1
l=1 ξ′
lgh
L−1
l=1 ξ′

lgf

P
f ∈H





θ′
gf

for all g ∈ H and h ∈ H

f ∈H
X

P
Φ = 0 for all h ∈ H and w ∈ V . This implies ψ′

hv∗ ×
for all h ∈ H and w ∈ V \ {v∗}. Since Φ is positive, Elv = δv,vl and

l(h)δv∗,vl = ψ′

L
l=1 γ′

hw ×

P

– ˙ψhw
L
l=1 γ′
(cid:12)
(cid:12)
v∈V δv,vl = 1 this implies

l(h)δw,vl

P

P
P

ψ′

hw =

lhδw,vl

L
l=1 γ′
L
l=1 γ′

lh !

  P

ψ′

hv

for all h ∈ H and w ∈ V

v∈V
X
Because of the relaxation we get by Remark 1, the point Φ qualiﬁes as a ﬁxed point of the Baum-
Welch algorithm.
⊓⊔

P

17

 
 
Appendix A.2 Rate of convergence analysis

In this section we will prove Theorem 3 and Theorem 4, but ﬁrst we will state and prove two useful
lemmas.

Lemma 1. Let A be an arbitrary n × n matrix. Let W be an r × n matrix comprising of r linearly
independent left kernel vectors of A so that W A = 0r,n, where 0i,j denotes a i × j matrix with all
entries zero. Further suppose W is in the row reduced form, that it is,

W =

W ′ Ir

where Ij denotes the j × j identity matrix and W ′ is a r × (n − r) matrix. Let A be given as

(cid:0)

(cid:1)

A =

A11 A12
A21 A22(cid:19)
(cid:18)

,

where A11 is a (n − r) × (n − r) matrix, A12 is a (n − r) × r matrix, A21 is a r × (n − r) matrix,
and A22 is a r × r matrix.

Then the n − r eigenvalues (with multiplicity) of the matrix A11 − A12W ′ are the same as the

eigenvalues of A except for r zero eigenvalues.

Proof. Consider the n × n invertible matrix P given by

P =

In−r 0n−r,r
W ′

Ir

(cid:18)

, P −1 =

(cid:19)

In−r 0n−r,r
−W ′

Ir

(cid:18)

,

(cid:19)

with determinant det(P ) = det(P −1) = 1. We have

P AP −1 =

=

=

In−r 0n−r,r
−W ′

Ir

(cid:19)

A11 A12
A21 A22(cid:19) (cid:18)
In−r 0n−r,r
−W ′

Ir

In−r 0n−r,r
W ′

(cid:19) (cid:18)

Ir
A11 A12
0r,n−r 0r,r
A11 − A12W ′ A12
0r,r

0r,n−r

(cid:19) (cid:18)

(cid:19)

(cid:19)

(cid:18)

(cid:18)

(cid:18)

This implies that the characteristic polynomial of A fulﬁls

det(A − λIn) = det(P ) det(A − λIn) det(P −1) = det(P AP −1 − λIn)

= det

A11 − A12W ′ − λIn−r
A12
0r,r − λIr
0r,n−r
= (−1)rλr det(A11 − A12W ′ − λIn−r),

(cid:18)

(cid:19)

and the statement follows.

⊓⊔

Now we revisit the observation that every reaction in the Baum-Welch reaction network is a
monomolecular transformation catalyzed by a set of species. For the purposes of our analysis,
each reaction can be abstracted as a monomolecular reaction with time varying rate constants. This
prompts us to consider the following monomolecular reaction system with n species X1, . . . , Xn
and m reactions

Xrj

kj (t)
−−−→ Xpj ,

for

j = 1, . . . , m,

18

where rj
6= pj, and rj, pj ∈ {1, . . . , n}, and kj(t), j = 1, . . . , m, are mass-action reaction
rate constants, possibly depending on time. We assume kj(t) > 0 for t ≥ 0 and let k(t) =
(k1(t), . . . , km(t)) be the vector of reaction rate constants. Furthermore, we assume there is at most
one reaction j such that (rj , pj) = (r, p) ∈ {1, . . . , n}2 and that the reaction network is strongly
connected. The later means there is a reaction path from any species Xi to any other species Xi′ . (In
reaction network terms it means the network is weakly reversible and deﬁciency zero.)

The mass action kinetics of this reaction system is given by the ODE system

m

m

˙xi = −xi

kj (t) +

kj(t)xrj ,

i = 1, . . . , n.

Deﬁne the n × n matrix A(t) = (aii′ (t))i,i′=1,...,n by

j=1 : rj =i
X

j=1 : pj =i
X

m

aii(t) = −

kj (t),

j=1 : rj =i
X

aii′ (t) = kj(t),

if there is j ∈ {1, . . . , m} such that (rj , pj) = (i′, i).

Then the ODE system might be written as

˙x = A(t)x.

(1)

(2)

Note that the column sums of A(t) are zero, implying that

n
i=1 xi is conserved.

Lemma 2. Assume k(t) for t ≥ 0, converges exponentially fast towards k = (k1, . . . , km) ∈ Rm
P
>0
as t → ∞, that is, there exists γ1 > 0 and K1 > 0 such that

k(t) − k

≤ K1e−γ1t

for

t ≥ 0.

Let A(t) be the matrix as deﬁned in equation 1. And let A be the matrix obtained with k inserted for
k(t) in the matrix A(t) that is, A = limt→∞ A(t).

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Then solutions of ODE system ˙x = A(t)x starting at x(0) ∈ Rn

>0 of the ODE system ˙x = Ax starting at x(0) ∈ Rn

≥0 converges exponentially fast
≥0, that is, there

towards the equilibrium a ∈ Rn
exists γ > 0 and K > 0 such that

x(t) − a

≤ Ke−γt

for

t ≥ 0.

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Proof. We will ﬁrst rephrase the ODE system such that standard theory is applicable. Let rank of A
be n − r. Let W be as deﬁned in Lemma 1, that is, W be an r × n matrix comprising of r linearly
independent left kernel vectors of A so that W A = 0. Here since rank of A is n − r, the rows of
W would form a basis for the left kernel of A. And as in Lemma 1, further suppose W is in the row
reduced form, that is,

Then

W =

W ′ Ir

.

(cid:0)
˙x = Ax

(cid:1)

(3)

is a linear dynamical system with r conservation laws (one for each row of W ). Let W x(0) =
T ∈ Rr be the vector of conserved amounts. Let ˆx = (x1, . . . , xn−r) and ˜x = (xn−r+1, . . . , xn).
We will consider the (equivalent) dynamical system in which r variables are eliminated, expressed
through the conservation laws

T = W x =

W ′ Ir

x,

or

˜x = T − W ′ ˆx.

(cid:0)

(cid:1)

19

As in Lemma 1, let A be given as

A =

A11 A12
A21 A22(cid:19)
(cid:18)

,

where A11 is a (n − r) × (n − r) matrix, A12 is a (n − r) × r matrix, A21 is a r × (n − r) matrix,
and A22 is a r × r matrix. This yields

ˆx
˜x

˙ˆx =

A11 A12

=

A11 A12

(cid:0)

(cid:19)

(cid:18)
(cid:1)
= (A11 − A12W ′)ˆx + A12T
= C ˆx + DT,

(cid:0)

ˆx
T − W ′ ˆx

(cid:18)

(cid:19)

(cid:1)

(4)

with C = A11 − A12W ′ and D = A12. We call this as the reduced ODE system. Note that this
reduced system has only n − r variables and that the conservation laws are built directly into it. This
implies that the differential equation changes if T is changed. The role of this construction is so that
we can work only with the non-zero eigenvalues of the A.

As we also have W A(t) = 0 for all t ≥ 0, the ODE ˙x = A(t)x can also be similarly reduced to

˙ˆx = C(t)ˆx + D(t)T,

(5)

with C(t) = A11(t) − A12(t)W ′ and D(t) = A12(t), where analogous to A11, we deﬁne A11(t) to
be the top-left (n − r) × (n − r) sub-matrix of A(t) and analogous to A12, we deﬁne A12(t) to be
the top-right (n − r) × r sub-matrix of A(t).

Now if a is the equilibrium of the ODE system ˙x = Ax starting at x(0), then ˆa = (a1, . . . , an−r)
is an equilibrium of the reduced ODE sytem ˙ˆx = C ˆx+DT starting at ˆx(0) = (x1(0), . . . , xn−r(0)).
Suppose we are able to prove that solutions of reduced ODE ˙ˆx = C(t)ˆx + D(t)T starting at ˆx(0)
converges exponentially fast towards ˆa then because of the conservation laws ˜x = T − W ′ ˆx, we
would also have that solutions of ˙x = A(t)x starting at x(0) converges exponentially fast towards
a. So henceforth, we will work only with the reduced ODE systems. For notational convinience, we
will drop the hats off ˆx and ˆa and simply refer to them as x and a respectively.

By subtracting and adding terms to the reduced ODE system (in equation 5), we have

˙x = C(t)x + D(t)T

= Cx + DT + (C(t) − C)x + (D(t) − D)T.

As a is an equilibrium of the ODE system ˙x = Cx + DT , we have Ca + DT = 0.

Deﬁne y = x − a. Then

˙y = Cx + DT + (C(t) − C)x + (D(t) − D)T

= Cy + Ca + DT + (C(t) − C)x + (D(t) − D)T
= Cy + (C(t) − C)x(t) + (D(t) − D)T
= Cy + E(t)

where it is used that Ca + DT = 0, and E(t) = (C(t) − C)x(t) + (D(t) − D)T .

The solution to the above ODE system is known to be

y(t) = eCty(0) +

eC(t−s)E(s) ds.

t

(6)

0
Z

20

We have, using (6) and the triangle inequality,

t

+

eC(t−s)

E(s)

ds.

y(t)

≤

eCty(0)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

0
Z
Now A as deﬁned (see equation 1 with k inserted for k(t)) would form a Laplacian matrix over
a strongly connected graph and so it follows that all the eigenvalues of A are either zero or have
negative real part. And using C = A11 − A12W ′ and Lemma 1 it follows that all eigenvalues of C
have negative real part. Hence it follows that

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

≤ K2e−γ2t,

eCt
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

where 0 < γ2 < −ℜ(λ1) and K2 > 0. Here λ1 is the eigenvalue of C with the largest real part.

The matrices C(t) and D(t) are linear in k(t). And as k(t) converges exponentially fast to-
wards k, it follows that the matrices C(t) and D(t) converge exponentially fast towards C and D
respectively. Hence it follows that

E(t)

=

(C(t) − C)x(t) + (D(t) − D)T

(cid:13)
(cid:13)

(cid:13)
(cid:13)

where

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(C(t) − C)

x(t)
≤
≤ K3e−γ3t + K4e−γ4t
(cid:13)
(cid:13)
≤ K5e−γ5t

(cid:13)
(cid:13)

(cid:13)
(cid:13)

+

(D(t) − D)

(cid:13)
(cid:13)

kT k

(cid:13)
(cid:13)

(cid:13)
(cid:13)

–

–

x(t)

≤ K3e−γ3t for some K3, γ3 > 0 as C(t) converges exponentially fast
n
i=1 xi is conserved), and

(C0(t) − C)
towards C and x(t) is bounded (as in the original ODE
(cid:13)
(cid:13)
(cid:13)
kT k ≤ K4e−γ4t for some K4, γ4 > 0 as D(t) converges exponentially fast
(D0(t) − D)
(cid:13)
(cid:13)
(cid:13)
towards D, and
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2 max(K3, K4) and γ5 = min(γ3, γ4).

P

(cid:13)
(cid:13)

– K5 = 1

Collecting all terms we have for all t ≥ 0,

y(t)

≤

y(0)

K2e−γ2t +

t

K2e−γ2(t−s) × K5e−γ5sds

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)
≤ K0e−γ0t + K0

t

0

Z
e−γ0tds

0
Z
= K0e−γ0t(1 + t)
≤ Ke−γt

K2K5,

by choosing K0 = max
such that 0 < γ < γ0 and K is sufﬁciently large. Since y(t) = x(t) − a we have,
(cid:13)
(cid:13)
x(t) − a

≤ Ke−γt,

y(0)

K2

(cid:13)
(cid:13)

(cid:16)

(cid:17)

and γ0 = min(γ1, γ2). In the last line γ is chosen

as required.

(cid:13)
(cid:13)

(cid:13)
(cid:13)

21

⊓⊔

We will now prove Theorem 3. For the sake of convenience, we ﬁrst recall the statement.

Theorem 3. For the Baum Welch Reaction System (BW (M, h∗, v∗, L), k) with permissible rates k,
if the concentrations of θ and ψ species are held ﬁxed at a positive point then the Forward, Backward
and Expection step reaction systems on α, β, γ and ψ species converge to equilibrium exponentially
fast.

Proof. It follows by repeated use of Lemma 2. For l = 1 the forward reaction network can be
interpretated as the following molecular reactions:

α1h

πh∗ ψh∗ wE1w
−−−−−−−−→ α1h∗

α1h∗

πhψhwE1w
−−−−−−−→ α1h

for all h ∈ H \ {h∗} and w ∈ V , as they are dynamically equivalent. Here the effective rate
constants (πh∗ ψh∗wE1w or πhψhwE1w) are independent of time and so the conditions of Lemma 2
are fulﬁlled. Thus this portion of the reaction network converges exponentially fast.

The rest of the forward reaction network can be similarly interpretated as the following molecular

reactions:

αlh

αl−1,g θgh∗ ψh∗ wElw
−−−−−−−−−−−−−→ αlh∗

αlh∗

αl−1,g θghψhw Elw
−−−−−−−−−−−→ αlh

for all g ∈ H, h ∈ H \ {h∗}, l = 2, . . . , L and w ∈ V . For layers l = 2, . . . , L of the forward reac-
tion network we observe that the effective rate constants (αl−1,gθgh∗ ψh∗wElw or αl−1,gθghψhwElw)
for layer l depend on time only through αl−1,g. If we suppose that the concentration of αl−1,g con-
verges exponentially fast, then we can use Lemma 2 to conclude that the concentration of αlh also
converges exponentially fast. Thus using Lemma 2 inductively layer by layer we conclude that
forward reaction network converges exponentially fast. The backward reaction network converges
exponentially fast, similarly.

For the expectation reaction network it likewise follows by induction. But here, notice if we
interpreted the expectation network similarly into molecular reactions, the effective rate constants
would depend on time through the products such as αlhβlh or αlhβl+1,h. So to apply Lemma 2 we
need the following: If αl(t) and βl(t) converge exponentially fast towards al and bl then the product
αl(t)βl(t) converges exponentially fast towards albl as

αl(t)βl(t) − albl

(cid:13)
(cid:13)

=

≤

(cid:13)
(cid:13)

(αl(t) − al)(βl(t) − bl) + al(βl(t) − bl) + bl(αl(t) − al)
αl(t) − al
αl(t) − al
(cid:13)
(cid:13)
(cid:13)
(cid:13)

βl(t) − bl

βl(t) − bl

+ K

+ K

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

,

where K is some suitably large constant. We can further observe that αl(t)βl(t) converges expo-
nentially fast towards albl at rate γ = min(γa, γb), where γa and γb, respectively, are the expo-
nential convergence rates of αl(t) and βl(t). A similar argument goes for the products of the form
αl(t)βl+1(t). And thus the expectation reaction network, also converges exponentially fast.
⊓⊔

We will now prove Theorem 4. For the sake of convenience, we ﬁrst recall the statement.

Theorem 4. For the Baum Welch Reaction System (BW (M, h∗, v∗, L), k) with permissible rates k,
if the concentrations of α, β, γ and ξ species are held ﬁxed at a positive point then the Maximization
step reaction system on θ and ψ converges to equilibrium exponentially fast.

Proof. Exponential convergence of the maximisation network follows by a similar layer by layer
inductive use of Lemma 2.
⊓⊔

22

