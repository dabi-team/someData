Overﬁtting in Synthesis: Theory and Practice

Saswat Padhi 1∗
(cid:0)

, Todd Millstein 1, Aditya Nori 2, and Rahul Sharma 3

1 University of California, Los Angeles, USA
{padhi,todd}@cs.ucla.edu
2 Microsoft Research, Cambridge, UK
adityan@microsoft.com
3 Microsoft Research, Bengaluru, India
rahsha@microsoft.com

Abstract. In syntax-guided synthesis (SyGuS), a synthesizer’s goal is
to automatically generate a program belonging to a grammar of possi-
ble implementations that meets a logical speciﬁcation. We investigate
a common limitation across state-of-the-art SyGuS tools that perform
counterexample-guided inductive synthesis (CEGIS). We empirically ob-
serve that as the expressiveness of the provided grammar increases, the
performance of these tools degrades signiﬁcantly.

We claim that this degradation is not only due to a larger search
space, but also due to overﬁtting. We formally deﬁne this phenomenon
and prove no-free-lunch theorems for SyGuS, which reveal a fundamental
tradeoﬀ between synthesizer performance and grammar expressiveness.
A standard approach to mitigate overﬁtting in machine learning
is to run multiple learners with varying expressiveness in parallel. We
demonstrate that this insight can immediately beneﬁt existing SyGuS
tools. We also propose a novel single-threaded technique called hybrid
enumeration that interleaves diﬀerent grammars and outperforms the
winner of the 2018 SyGuS competition (Inv track), solving more problems
and achieving a 5× mean speedup.

1

Introduction

The syntax-guided synthesis (SyGuS) framework [3] provides a uniﬁed format to
describe a program synthesis problem by supplying (1) a logical speciﬁcation for
the desired functionality, and (2) a grammar of allowed implementations. Given
these two inputs, a SyGuS tool searches through the programs that are permitted
by the grammar to generate one that meets the speciﬁcation. Today, SyGuS is at
the core of several state-of-the-art program synthesizers [5, 14, 23, 24, 28], many
of which compete annually in the SyGuS competition [1, 4].

We demonstrate empirically that ﬁve state-of-the-art SyGuS tools are very
sensitive to the choice of grammar. Increasing grammar expressiveness allows the
tools to solve some problems that are unsolvable with less-expressive grammars.
However, it also causes them to fail on many problems that the tools are able
to solve with a less expressive grammar. We analyze the latter behavior both
theoretically and empirically and present techniques that make existing tools
much more robust in the face of increasing grammar expressiveness.

∗

Contributed during an internship at Microsoft Research, India.

9
1
0
2

n
u
J

8

]
L
P
.
s
c
[

3
v
7
5
4
7
0
.
5
0
9
1
:
v
i
X
r
a

 
 
 
 
 
 
2

Saswat Padhi, Todd Millstein, Aditya Nori, Rahul Sharma

We restrict our investigation to a widely used approach [6] to SyGuS called
counterexample-guided inductive synthesis (CEGIS) [36, §5]. In this approach,
the synthesizer is composed of a learner and an oracle. The learner iteratively
identiﬁes a candidate program that is consistent with a given set of examples
(initially empty) and queries the oracle to either prove that the program is correct,
i.e., meets the given speciﬁcation, or obtain a counterexample that demonstrates
that the program does not meet the speciﬁcation. The counterexample is added
to the set of examples for the next iteration. The iterations continue until a
correct program is found or resource/time budgets are exhausted.

Overﬁtting. To better understand the observed performance degradation, we
instrumented one of these SyGuS tools (§ 2.2). We empirically observe that for a
large number of problems, the performance degradation on increasing grammar
expressiveness is often accompanied by a signiﬁcant increase in the number of
counterexamples required. Intuitively, as grammar expressiveness increases so
does the number of spurious candidate programs, which satisfy a given set of
examples but violate the speciﬁcation. If the learner picks such a candidate, then
the oracle generates a counterexample, the learner searches again, and so on.

In other words, increasing grammar expressiveness increases the chances for
overﬁtting, a well-known phenomenon in machine learning (ML). Overﬁtting
occurs when a learned function explains a given set of observations but does not
generalize correctly beyond it. Since SyGuS is indeed a form of function learning,
it is perhaps not surprising that it is prone to overﬁtting. However, we identify
its speciﬁc source in the context of SyGuS — the spurious candidates induced by
increasing grammar expressiveness — and show that it is a signiﬁcant problem
in practice. We formally deﬁne the potential for overﬁtting (Ω), in Deﬁnition 7,
which captures the number of spurious candidates.

No Free Lunch. In the ML community, this tradeoﬀ between expressiveness
and overﬁtting has been formalized for various settings as no-free-lunch (NFL)
theorems [33, §5.1]. Intuitively such a theorem says that for every learner there
exists a function that cannot be eﬃciently learned, where eﬃciency is deﬁned by
the number of examples required. We have proven corresponding NFL theorems
for the CEGIS-based SyGuS setting (Theorems 1 and 2).

A key diﬀerence between the ML and SyGuS settings is the notion of m-
learnability. In the ML setting, the learned function may diﬀer from the true
function, as long as this diﬀerence (expressed as an error probability) is relatively
small. However, because the learner is allowed to make errors, it is in turn required
to learn given an arbitrary set of m examples (drawn from some distribution). In
contrast, the SyGuS learning setting is all-or-nothing — either the tool synthesizes
a program that meets the given speciﬁcation or it fails. Therefore, it would be
overly strong to require the learner to handle an arbitrary set of examples.

Instead, we deﬁne a much weaker notion of m-learnability for SyGuS, which
only requires that there exist a set of m examples for which the learner succeeds.
Yet, our NFL theorem shows that even this weak notion of learnability can always
be thwarted: given an integer m ≥ 0 and an expressive enough (as a function
of m) grammar, for every learner there exists a SyGuS problem that cannot be

Overﬁtting in Synthesis: Theory and Practice

3

learned without access to more than m examples. We also prove that overﬁtting
is inevitable with an expressive enough grammar (Theorems 3 and 4) and that
the potential for overﬁtting increases with grammar expressiveness (Theorem 5).

Mitigating Overﬁtting. Inspired by ensemble methods [13] in ML, which aggregate
results from multiple learners to combat overﬁtting (and underﬁtting), we propose
PLearn — a black-box framework that runs multiple parallel instances of a
SyGuS tool with diﬀerent grammars. Although prior SyGuS tools run multiple
instances of learners with diﬀerent random seeds [7, 20], to our knowledge, this
is the ﬁrst proposal to explore multiple grammars as a means to improve the
performance of SyGuS. Our experiments indicate that PLearn signiﬁcantly
improves the performance of ﬁve state-of-the-art SyGuS tools — CVC4 [7, 32],
EUSolver [5], LoopInvGen [28], SketchAC [20, 36], and Stoch [3, III F].
However, running parallel instances of a synthesizer is computationally ex-
pensive. Hence, we also devise a white-box approach, called hybrid enumeration,
that extends the enumerative synthesis technique [2] to eﬃciently interleave ex-
ploration of multiple grammars in a single SyGuS instance. We implement hybrid
enumeration within LoopInvGen4 and show that the resulting single-threaded
learner, LoopInvGen+HE, has negligible overhead but achieves performance
comparable to that of PLearn for LoopInvGen. Moreover, LoopInvGen+HE
signiﬁcantly outperforms the winner [27] of the invariant-synthesis (Inv) track
of 2018 SyGuS competition [4] — a variant of LoopInvGen speciﬁcally tuned
for the competition — including a 5× mean speedup and solving two SyGuS
problems that no tool in the competition could solve.

Contributions. In summary, we present the following contributions:
(§ 2) We empirically observe that, in many cases, increasing grammar expres-
siveness degrades performance of existing SyGuS tools due to overﬁtting.
(§ 3) We formally deﬁne overﬁtting and prove no-free-lunch theorems for the
SyGuS setting, which indicate that overﬁtting with increasing grammar
expressiveness is a fundamental characteristic of SyGuS.

(§ 4) We propose two mitigation strategies – (1) a black-box technique that runs
multiple parallel instances of a synthesizer, each with a diﬀerent grammar,
and (2) a single-threaded enumerative technique, called hybrid enumeration,
that interleaves exploration of multiple grammars.

(§ 5) We show that incorporating these mitigating measures in existing tools

signiﬁcantly improves their performance.

2 Motivation

In this section, we ﬁrst present empirical evidence that existing SyGuS tools
are sensitive to changes in grammar expressiveness. Speciﬁcally, we demonstrate
that as we increase the expressiveness of the provided grammar, every tool
starts failing on some benchmarks that it was able to solve with less-expressive
grammars. We then investigate one of these tools in detail.

4 Our implementation is available at https://github.com/SaswatPadhi/LoopInvGen.

4

Saswat Padhi, Todd Millstein, Aditya Nori, Rahul Sharma

2.1 Grammar Sensitivity of SyGuS Tools

We evaluated 5 state-of-the-art SyGuS tools that use very diﬀerent techniques:
– SketchAC [20] extends the Sketch synthesis system [36] by combining

both explicit and symbolic search techniques.

– Stoch [3, III F] performs a stochastic search for solutions.
– EUSolver [5] combines enumeration with uniﬁcation strategies.
– Reynolds et al. [32] extend CVC4 [7] with a refutation-based approach.
– LoopInvGen [28] combines enumeration and Boolean function learning.

We ran these ﬁve tools on 180 invariant-
synthesis benchmarks, which we describe in
§ 5. We ran the benchmarks with each of
the six grammars of quantiﬁer-free predicates,
which are shown in Fig. 1. These grammars
correspond to widely used abstract domains in
the analysis of integer-manipulating programs
— Equalities, Intervals [11], Octagons [25], Poly-
hedra [12], algebraic expressions (Polynomials)
and arbitrary integer arithmetic (Peano) [29].
operator denotes scalar multiplication,
The *S
e.g., (*S 2 x), and *N
denotes nonlinear multi-
plication, e.g., (*N x y).

hbi |= true | false | hBool variablesi
| (not b) | (or b b) | (and b b)
hii |= hInt constantsi | hInt variablesi

(cid:73) Additional rule in Equalities grammar :

hbi |=+ (= i i)

(cid:73) Additional rules in Intervals grammar :

hbi |=+ (> i i) | (>= i i)
| (< i i) | (<= i i)

(cid:73) Additional rules in Octagons grammar :

hii |=+ (+ i i) | (- i i)

(cid:73) Additional rule in Polyhedra grammar :

hii |=+ (*S i i)

(cid:73) Additional rule in Polynomials grammar :

hii |=+ (*N i i)

(cid:73) Additional rule in Peano grammar :
hii |=+ (div i i) | (mod i i)

In Fig. 2, we report our ﬁndings on run-
ning each benchmark on each tool with each
grammar, with a 30-minute wall-clock time-
out. For each htool, grammari pair, the y-axis
shows the number of failing benchmarks that
the same tool is able to solve with a less-
expressive grammar. We observe that, for each tool, the number of such failures
increases with the grammar expressiveness. For instance, introducing the scalar
) causes CVC4 to fail on 21 benchmarks that it is
multiplication operator ( *S
able to solve with Equalities (4/21), Intervals (18/21), or Octagons (10/21). Similarly,
adding nonlinear multiplication causes LoopInvGen to fail on 10 benchmarks
that it can solve with a less-expressive grammar.

Fig. 1. Grammars of quantiﬁer-
free predicates over integers 5

Fig. 2. For each grammar, each tool, the ordinate shows the number of bench-
marks that fail with the grammar but are solvable with a less-expressive grammar.
5 We use the |=+ operator to append new rules to previously deﬁned nonterminals.

Overﬁtting in Synthesis: Theory and Practice

5

Increase (↑)

Unchanged (=)

Decrease (↓)

Expressiveness ↑ ∧ Time ↑ → Rounds ?
Expressiveness ↑ ∧ Rounds ↑ → Time ?

27 %
79 %

67 %
6 %

6 %
15 %

Fig. 3. Observed correlation between synthesis time and number of rounds, upon
increasing grammar expressiveness, with LoopInvGen [28] on 180 benchmarks

2.2 Evidence for Overﬁtting

To better understand this phenomenon, we instrumented LoopInvGen [28] to
record the candidate expressions that it synthesizes and the number of CEGIS
iterations (called rounds henceforth). We compare each pair of successful runs of
each of our 180 benchmarks on distinct grammars.6 In 65 % of such pairs, we
observe performance degradation with the more expressive grammar. We also
report the correlation between performance degradation and number of rounds
for the more expressive grammar in each pair in Fig. 3.

In 67 % of the cases with degraded performance upon increased grammar
expressiveness, the number of rounds remains unaﬀected — indicating that this
slowdown is mainly due to a larger search space. However, there is signiﬁcant
evidence of performance degradation due to overﬁtting as well. We note an
increase in the number of rounds for 27 % of the cases with degraded performance.
Moreover, we notice performance degradation in 79 % of all cases that required
more rounds on increasing grammar expressiveness.

Thus, a more expressive grammar not only increases the search space, but also
makes it more likely for LoopInvGen to overﬁt — select a spurious expression,
which the oracle rejects with a counterexample, hence requiring more rounds. In
the remainder of this section, we demonstrate this overﬁtting phenomenon on
the veriﬁcation problem shown in Fig. 4, an example by Gulwani and Jojic [17],
which is the fib_19 benchmark in the Inv track of SyGuS-Comp 2018 [4].

For Fig. 4, we require an inductive in-
variant that is strong enough to prove that
the assertion on line 6 always holds. In
the SyGuS setting, we need to synthesize
a predicate I : Z 4 → B deﬁned on a sym-
bolic state σ = hm, n, x, yi, that satisﬁes
∀σ : ϕ(I, σ) for the speciﬁcation ϕ:7

1 assume (0 ≤ n ∧ 0 ≤ m ≤ n)
2 assume (x = 0 ∧ y = m)
3 while (x < n) do
4
5
6 assert (y = n)

x ← x + 1
if (x > m) then y ← y + 1

Fig. 4. The fib_19 benchmark [17]

ϕ(I, σ)

def

= (cid:0)0 ≤ n ∧ 0 ≤ m ≤ n ∧ x = 0 ∧ y = m(cid:1) =⇒ I(σ)
∧ ∀σ0 : (cid:0)I(σ) ∧ T (σ, σ0)(cid:1) =⇒ I(σ0)
∧ (cid:0)x ≥ n ∧ I(σ)(cid:1) =⇒ y = n

(precondition)

(inductiveness)

(postcondition)

where σ0 = hm0, n0, x0, y0i denotes the new state after one iteration, and T is a
transition relation that describes the loop body:

T (σ, σ0)

def

= (x < n) ∧ (x0 = x + 1) ∧ (m0 = m) ∧ (n0 = n)
∧ (cid:2) (x0 ≤ m ∧ y0 = y) ∨ (x0 > m ∧ y0 = y + 1) (cid:3)

6 We ignore failing runs since they require an unknown number of rounds.
7 We use B, N, and Z to denote the sets of all Boolean values, all natural numbers

(positive integers), and all integers respectively.

6

Saswat Padhi, Todd Millstein, Aditya Nori, Rahul Sharma

Increasing expressiveness →

Equalities

Intervals

Octagons

Polyhedra

Polynomials

Peano

×
FAIL

0.32 s
(19 rounds)

2.49 s
(57 rounds)

2.48 s
(57 rounds)

55.3 s
(76 rounds)

68.0 s
(88 rounds)

(a) Synthesis time and number of CEGIS iterations (rounds) with various grammars

16: (x ≥ n) ∨ (x + 1 < n) ∨ (m ≥ x ∧ m = y)

16: (x ≥ n) ∨ (x + 1 < n) ∨

(2y = n) ∨ (y (m − 1) = m)

28: (x = y) ∨ (y + m − n = x) ∨ (x + 2 < n)

28: (y = 1) ∨ (y = 0) ∨ (m < 1) ∨ (x2y > 1)

57: (m = y) ∨ (x ≥ m ∧ x ≥ y)

57: (x + 1 ≥ n) ∨ (x + 2 < n) ∨

((m − n)(x − y) = 1)

(b) Sample predicates with Polyhedra

(c) Sample predicates with Peano

Solution in both grammars: (n ≥ y) ∧ (y ≥ x) ∧ ((m = y) ∨ (x ≥ m ∧ x ≥ y))
Fig. 5. Performance of LoopInvGen [28] on the fib_19 benchmark (Fig. 4). In
(b) and (c), we show predicates generated at various rounds (numbered in bold).

In Fig. 5(a), we report the performance of LoopInvGen on fib_19 (Fig. 4)
with our six grammars (Fig. 1). It succeeds with all but the least-expressive
grammar. However, as grammar expressiveness increases, the number of rounds
increase signiﬁcantly — from 19 rounds with Intervals to 88 rounds with Peano.
LoopInvGen converges to the exact same invariant with both Polyhedra and
Peano but requires 30 more rounds in the latter case. In Figs. 5(b) and 5(c), we
list some expressions synthesized with Polyhedra and Peano respectively. These
expressions are solutions to intermediate subproblems — the ﬁnal loop invariant
is a conjunction of a subset of these expressions [28, §3.2]. Observe that the
expressions generated with the Peano grammar are quite complex and unlikely to
generalize well. Peano’s extra expressiveness leads to more spurious candidates,
increasing the chances of overﬁtting and making the benchmark harder to solve.

3 SyGuS Overﬁtting in Theory

In this section, ﬁrst we formalize the counterexample-guided inductive synthesis
(CEGIS) approach [36] to SyGuS, in which examples are iteratively provided
by a veriﬁcation oracle. We then state and prove no-free-lunch theorems, which
show that there can be no optimal learner for this learning scheme. Finally, we
formalize a natural notion of overﬁtting for SyGuS and prove that the potential
for overﬁtting increases with grammar expressiveness.

3.1 Preliminaries

We borrow the formal deﬁnition of a SyGuS problem from prior work [3]:
Deﬁnition 1 (SyGuS Problem). Given a background theory T, a function
symbol f : X → Y , and constraints on f : (1) a semantic constraint, also called a
speciﬁcation, φ(f, x) over the vocabulary of T along with f and a symbolic input x,
and (2) a syntactic constraint, also called a grammar, given by a (possibly inﬁnite)
set E of expressions over the vocabulary of the theory T; ﬁnd an expression e ∈ E
such that the formula ∀x ∈ X : φ(e, x) is valid modulo T.

Overﬁtting in Synthesis: Theory and Practice

7

We denote this SyGuS problem as hfX→Y | φ, Ei T and say that it is satisﬁable
iﬀ there exists such an expression e, i.e., ∃ e ∈ E : ∀x ∈ X : φ(e, x). We call e a
satisfying expression for this problem, denoted as e |= hfX→Y | φ, Ei T.

Recall, we focus on a common class of SyGuS learners, namely those that
learn from examples. First we deﬁne the notion of input-output (IO) examples
that are consistent with a SyGuS speciﬁcation:

Deﬁnition 2 (Input-Output Example). Given a speciﬁcation φ deﬁned on
f : X → Y over a background theory T, we call a pair hx, yi ∈ X × Y an input-
output (IO) example for φ, denoted as hx, yi (cid:112)≈ T φ iﬀ it is satisﬁed by some valid
interpretation of f within T, i.e.,
hx, yi (cid:112)≈ T φ def= ∃ e∗ ∈ T : e∗(x) = y ∧ (cid:0)∀x ∈ X : φ(e∗, x)(cid:1)

The next two deﬁnitions respectively formalize the two key components of a

CEGIS-based SyGuS tool: the veriﬁcation oracle and the learner.

Deﬁnition 3 (Veriﬁcation Oracle). Given a speciﬁcation φ deﬁned on a
function f : X → Y over theory T, a veriﬁcation oracle Oφ is a partial function
that given an expression e, either returns ⊥ indicating ∀x ∈ X : φ(e, x) holds, or
gives a counterexample hx, yi against e, denoted as e (cid:32)×φ hx, yi, such that

e (cid:32)×φ hx, yi

def= ¬ φ(e, x) ∧ e(x) 6= y ∧ hx, yi (cid:112)≈ T φ

We omit φ from the notations Oφ and (cid:32)×φ when it is clear from the context.
Deﬁnition 4 (CEGIS-based Learner). A CEGIS-based learner LO(q, E) is a
partial function that given an integer q ≥ 0, a set E of expressions, and access
to an oracle O for a speciﬁcation φ deﬁned on f : X → Y , queries O at most q
times and either fails with ⊥ or generates an expression e ∈ E. The trace
where 0 ≤ p ≤ q

(cid:2)e0 (cid:32)× hx0, y0i, . . . , ep−1 (cid:32)× hxp−1, yp−1i, ep

(cid:3)

summarizes the interaction between the oracle and the learner. Each ei denotes
the ith candidate for f and hxi, yii is a counterexample ei, i.e.,

(cid:0)∀j < i : ei(xj) = yj ∧ φ(ei, xj)(cid:1) ∧ (cid:0)ei (cid:32)×φ hxi, yii(cid:1)
Note that we have deﬁned oracles and learners as (partial) functions, and
hence as deterministic. In practice, many SyGuS tools are deterministic and this
assumption simpliﬁes the subsequent theorems. However, we expect that these
theorems can be appropriately generalized to randomized oracles and learners.

3.2 Learnability and No Free Lunch

In the machine learning (ML) community, the limits of learning have been
formalized for various settings as no-free-lunch theorems [33, §5.1]. Here, we
provide a natural form of such theorems for CEGIS-based SyGuS learning.

In SyGuS, the learned function must conform to the given grammar, which
may not be fully expressive. Therefore we ﬁrst formalize grammar expressiveness:

Deﬁnition 5 (k-Expressiveness). Given a domain X and range Y , a grammar
E is said to be k-expressive iﬀ E can express exactly k distinct X → Y functions.

8

Saswat Padhi, Todd Millstein, Aditya Nori, Rahul Sharma

A key diﬀerence from the ML setting is our notion of m-learnability, which
formalizes the number of examples that a learner requires in order to learn a
desired function. In the ML setting, a function is considered to m-learnable by a
learner if it can be learned using an arbitrary set of m i.i.d. examples (drawn from
some distribution). This makes sense in the ML setting since the learned function
is allowed to make errors (up to some given bound on the error probability), but
it is much too strong for the all-or-nothing SyGuS setting.

Instead, we deﬁne a much weaker notion of m-learnability for CEGIS-based
SyGuS, which only requires that there exist a set of m examples that allows the
learner to succeed. The following deﬁnition formalizes this notion.

Deﬁnition 6 (CEGIS-based m-Learnability). Given a SyGuS problem S =
hfX→Y | φ, Ei T and an integer m ≥ 0, we say that S is m-learnable by a CEGIS-
based learner L iﬀ there exists a veriﬁcation oracle O under which L can learn a
satisfying expression for S with at most m queries to O, i.e., ∃ O : LO(m, E) |= S.

Finally we state and prove the no-free-lunch (NFL) theorems, which make
explicit the tradeoﬀ between grammar expressiveness and learnability. Intuitively,
given an integer m and an expressive enough (as a function of m) grammar, for
every learner there exists a SyGuS problem that cannot be solved without access
to at least m + 1 examples. This is true despite our weak notion of learnability.
Put another way, as grammar expressiveness increases, so does the number
of examples required for learning. On one extreme, if the given grammar is
1-expressive, i.e., can express exactly one function, then all satisﬁable SyGuS
problems are 0-learnable — no examples are needed because there is only one
function to learn — but there are many SyGuS problems that cannot be satisﬁed
by this function. On the other extreme, if the grammar is |Y ||X|-expressive, i.e.,
can express all functions from X to Y , then for every learner there exists a SyGuS
problem that requires all |X| examples in order to be solved.

Below we ﬁrst present the NFL theorem for the case when the domain X
and range Y are ﬁnite. We then generalize to the case when these sets may be
countably inﬁnite. The proofs of these theorems can be found in Appendix A.1.

Theorem 1 (NFL in CEGIS-based SyGuS on Finite Sets). Let X and Y
be two arbitrary ﬁnite sets, T be a theory that supports equality, E be a grammar
over T, and m be an integer such that 0 ≤ m < |X|. Then, either:

– E is not k-expressive for any k > Pm
– for every CEGIS-based learner L, there exists a satisﬁable SyGuS problem
S = hfX→Y | φ, Ei T such that S is not m-learnable by L. Moreover, there exists
a diﬀerent CEGIS-based learner for which S is m-learnable.

|X|! |Y |i
(|X| − i)! , or

i = 0

Theorem 2 (NFL in CEGIS-based SyGuS on Countably Inﬁnite Sets).
Let X be an arbitrary countably inﬁnite set, Y be an arbitrary ﬁnite or countably
inﬁnite set,T be a theory that supports equality, E be a grammar over T, and m
be an integer such that m ≥ 0. Then, either:

– E is not k-expressive for any k > ℵ0, where ℵ0

def= |N|, or

Overﬁtting in Synthesis: Theory and Practice

9

– for every CEGIS-based learner L, there exists a satisﬁable SyGuS problem
S = hfX→Y | φ, Ei T such that S is not m-learnable by L. Moreover, there exists
a diﬀerent CEGIS-based learner for which S is m-learnable.

3.3 Overﬁtting
Last, we relate the above theory to the notion of overﬁtting from ML. In the
context of SyGuS, overﬁtting can potentially occur whenever there are multiple
candidate expressions that are consistent with a given set of examples. Some of
these expressions may not generalize to satisfy the speciﬁcation, but the learner
has no way to distinguish among them (using just the given set of examples) and
so can “guess” incorrectly. We formalize this idea through the following measure:

Deﬁnition 7 (Potential for Overﬁtting). Given a problem S = hfX→Y | φ, Ei T
and a set Z of IO examples for φ, we deﬁne the potential for overﬁtting Ω as the
number of expressions in E that are consistent with Z but do not satisfy S, i.e.,
∀z ∈ Z : z (cid:112)≈ T φ

( (cid:12)
(cid:8)e ∈ E | e 6|= S ∧ ∀hx, yi ∈ Z : e(x) = y(cid:9)(cid:12)
(cid:12)
(cid:12)
⊥

Ω(S, Z) def=

(undeﬁned)

otherwise

Intuitively, a zero potential for overﬁtting means that overﬁtting is not possible
on the given problem with respect to the given set of examples, because there is
no spurious candidate. A positive potential for overﬁtting means that overﬁtting
is possible, and higher values imply more spurious candidates and hence more
potential for a learner to choose the “wrong” expression.

The following theorems connect our notion of overﬁtting to the earlier NFL
theorems by showing that overﬁtting is inevitable with an expressive enough
grammar. We provide their proofs in Appendix A.2.

Theorem 3 (Overﬁtting in SyGuS on Finite Sets). Let X and Y be two
arbitrary ﬁnite sets, m be an integer such that 0 ≤ m < |X|, T be a theory
that supports equality, and E be a k-expressive grammar over T for some k >
|X|! |Y |m
m! (|X| − m)! . Then, there exists a satisﬁable SyGuS problem S = hfX→Y | φ, Ei T
such that Ω(S, Z) > 0, for every set Z of m IO examples for φ.

Theorem 4 (Overﬁtting in SyGuS on Countably Inﬁnite Sets). Let X be
an arbitrary countably inﬁnite set, Y be an arbitrary ﬁnite or countably inﬁnite set,
T be a theory that supports equality, and E be a k-expressive grammar over T for
some k > ℵ0. Then, there exists a satisﬁable SyGuS problem S = hfX→Y | φ, Ei T
such that Ω(S, Z) > 0, for every set Z of m IO examples for φ.

Finally, it is straightforward to show that as the expressiveness of the grammar

provided in a SyGuS problem increases, so does its potential for overﬁtting.

Theorem 5 (Overﬁtting Increases with Expressiveness). Let X and Y
be two arbitrary sets, T be an arbitrary theory, E1 and E2 be grammars over T
such that E1 ⊆ E2, φ be an arbitrary speciﬁcation over T and a function symbol
f : X → Y , and Z be a set of IO examples for φ. Then, we have
Ω(cid:0) hfX→Y | φ, E1i T , Z(cid:1) ≤ Ω(cid:0) hfX→Y | φ, E2i T , Z(cid:1)

10

Saswat Padhi, Todd Millstein, Aditya Nori, Rahul Sharma

func PLearn (T : Synthesis Tool, hfX→Y | φ, Ei T : Problem, E1...p : Subgrammars)

Algorithm 1 The PLearn framework for SyGuS tools.
1
2 (cid:73) Requires: ∀ Ei ∈ E1...p : Ei ⊆ E
parallel for i ← 1, . . . , p do
3
Si ← hfX→Y | φ, Eii T
ei ← T (Si)
if ei 6= ⊥ then return ei

4

6

5

7

return ⊥

4 Mitigating Overﬁtting

Ensemble methods [13] in machine learning (ML) are a standard approach to
reduce overﬁtting. These methods aggregate predictions from several learners
to make a more accurate prediction. In this section we propose two approaches,
inspired by ensemble methods in ML, for mitigating overﬁtting in SyGuS. Both
are based on the key insight from § 3.3 that synthesis over a subgrammar has a
smaller potential for overﬁtting as compared to that over the original grammar.

4.1 Parallel SyGuS on Multiple Grammars
Our ﬁrst idea is to run multiple parallel instances of a synthesizer on the same
SyGuS problem but with grammars of varying expressiveness. This framework,
called PLearn, is outlined in Algorithm 1. It accepts a synthesis tool T , a SyGuS
problem hfX→Y | φ, Ei T
, and subgrammars E1...p,8 such that Ei ⊆ E. The parallel
for construct creates a new thread for each iteration. The loop in PLearn creates
p copies of the SyGuS problem, each with a diﬀerent grammar from E1...p, and
dispatches each copy to a new instance of the tool T . PLearn returns the ﬁrst
solution found or ⊥ if none of the synthesizer instances succeed.

Since each grammar in E1...p is subsumed by the original grammar E, any ex-
pression found by PLearn is a solution to the original SyGuS problem. Moreover,
from Theorem 5 it is immediate that PLearn indeed reduces overﬁtting.

Theorem 6 (PLearn Reduces Overﬁtting). Given a SyGuS problem S =
hfX→Y | φ, Ei T, if PLearn is instantiated with S and subgrammars E1...p such that
∀ Ei ∈ E1...p : Ei ⊆ E, then for each Si = hfX→Y | φ, Eii T constructed by PLearn,
we have that Ω(Si, Z) ≤ Ω(S, Z) on any set Z of IO examples for φ.

A key advantage of PLearn is that it is agnostic to the synthesizer’s im-
plementation. Therefore, existing SyGuS learners can immediately beneﬁt from
PLearn, as we demonstrate in §5.1. However, running p parallel SyGuS instances
can be prohibitively expensive, both computationally and memory-wise. The
problem is worsened by the fact that many existing SyGuS tools already use
multiple threads, e.g., the SketchAC [20] tool spawns 9 threads. This motivates
our hybrid enumeration technique described next, which is a novel synthesis
algorithm that interleaves exploration of multiple grammars in a single thread.

8 We use the shorthand X1,...,n to denote the sequence hX1, . . . , Xni.

Overﬁtting in Synthesis: Theory and Practice

11

4.2 Hybrid Enumeration
Hybrid enumeration extends the enumerative synthesis technique, which enu-
merates expressions within a given grammar in order of size and returns the
ﬁrst candidate that satisﬁes the given examples [2]. Our goal is to simulate
the behavior of PLearn with an enumerative synthesizer in a single thread.
However, a straightforward interleaving of multiple PLearn threads would be
highly ineﬃcient because of redundancies – enumerating the same expression
(which is contained in multiple grammars) multiple times. Instead, we propose
a technique that (1) enumerates each expression at most once, and (2) reuses
previously enumerated expressions to construct larger expressions.

To achieve this, we extend a widely used [2, 15, 30] synthesis strategy, called
component-based synthesis [21], wherein the grammar of expressions is induced
by a set of components, each of which is a typed operator with a ﬁxed arity. For
example, the grammars shown in Fig. 1 are induced by integer components (such
as 1, +, mod, =, etc.) and Boolean components (such as true, and, or, etc.). Below,
we ﬁrst formalize the grammar that is implicit in this synthesis style.

Deﬁnition 8 (Component-Based Grammar). Given a set C of typed com-
ponents, we deﬁne the component-based grammar E as the set of all expressions
formed by well-typed component application over C, i.e.,

E = { c(e1, . . . , ea) | (c : τ1 × · · · × τa → τ ) ∈ C ∧ e1 . . . a ⊂ E

∧ e1 : τ1 ∧ · · · ∧ ea : τa }

where e : τ denotes that the expression e has type τ .
We denote the set of all components appearing in a component-based grammar E
as components(E). Henceforth, we assume that components(E) is known (explicitly
provided by the user) for each E. We also use values(E) to denote the subset of
nullary components (variables and constants) in components(E), and operators(E)
to denote the remaining components with positive arities.

The closure property of component-based grammars signiﬁcantly reduces the
overhead of tracking which subexpressions can be combined together to form
larger expressions. Given a SyGuS problem over a grammar E, hybrid enumeration
requires a sequence E1...p of grammars such that each Ei is a component-based
grammar and that E1 ⊂ · · · ⊂ Ep ⊆ E. Next, we explain how the subset relation-
ship between the grammars enables eﬃcient enumeration of expressions.

Given grammars E1 ⊂ · · · ⊂ Ep, observe that an expression of size k in Ei
may only contain subexpressions of size {1, . . . , (k − 1)} belonging to E1...i. This
allows us to enumerate expressions in an order such that each subexpression e is
synthesized (and cached) before any expressions that have e as a subexpression.
We call an enumeration order that ensures this property a well order.

Deﬁnition 9 (Well Order). Given arbitrary grammars E1...p, we say that a
strict partial order / on E1...p × N is a well order iﬀ
∀ Ea, Eb ∈ E1...p : ∀ k1, k2 ∈ N :

[ Ea ⊆ Eb ∧ k1 < k2 ] =⇒ (Ea, k1) / (Eb, k2)
Motivated by Theorem 5, our implementation of hybrid enumeration uses a
particular well order that incrementally increases the expressiveness of the space

12

Saswat Padhi, Todd Millstein, Aditya Nori, Rahul Sharma

of expressions. For a rough measure of the expressiveness (Deﬁnition 5) of a
pair (E, k), i.e., the set of expressions of size k in a given grammar E, we simply
overapproximate the number of syntactically distinct expressions:

Theorem 7. Let E1...p be component-based grammars and Ci = components(Ei).
Then, the following strict partial order / ∗ on E1...p × N is a well order

(Ea, m) / ∗ (Eb, n) ⇐⇒ | Ca |m < | Cb |n

∀ Ea, Eb ∈ E1...p : ∀ m, n ∈ N :
We now describe the main hybrid enumeration algorithm, which is listed in
Algorithm 2. The HEnum function accepts a SyGuS problem hfX→Y | φ, Ei T
, a set
E1...p of component-based grammars such that E1 ⊂ · · · ⊂ Ep ⊆ E, a well order
/, and an upper bound q ≥ 0 on the size of expressions to enumerate. In lines
4 –8, we ﬁrst enumerate all values and cache them as expressions of size one. In
general C[j, k][τ ] contains expressions of type τ and size k from Ej \ Ej−1. In line
9 we sort (grammar, size) pairs in some total order consistent with /. Finally, in
lines 10– 20, we iterate over each pair (Ej, k) and each operator from E1...j and
invoke the Divide procedure (Algorithm 3) to carefully choose the operator’s
argument subexpressions ensuring (1) correctness – their sizes sum up to k − 1,
(2) eﬃciency – expressions are enumerated at most once, and (3) completeness –
all expressions of size k in Ej are enumerated.

The Divide algorithm generates a set of locations for selecting arguments
to an operator. Each location is a pair (x, y) indicating that any expression
from C[x, y][τ ] can be an argument, where τ is the argument type required by

Algorithm 2 Hybrid enumeration to combat overﬁtting in SyGuS
func HEnum (hfX→Y | φ, Ei T : Problem, E1...p : Grammars, / : WO, q : Max. Size)
1
2 (cid:73) Requires: component-based grammars E1 ⊂ · · · ⊂ Ep ⊆ E and v as the input variable
3

C ← {}
for i ← 1 to p do

V ← if i = 1 then values(E1) else [ values(Ei) \ values(Ei−1) ]
for each (e : τ ) ∈ V do

C[i, 1][τ ] ← C[i, 1][τ ] ∪ {e}
if ∀x ∈ X : φ(λv. e, x) then return λv. e

R ← Sort(/, E1...p × {2, . . . , q})
for i ← 1 to | R | do
(Ej, k) ← R[i]
for l ← 1 to j do

O ← if l = 1 then operators(E1) else [ operators(El) \ operators(El−1) ]
for each (o : τ1 × · · · × τa → τ ) ∈ O do
L ← Divide(a, k − 1, l, j, hi)
for each (cid:10)(x1, y1), . . . , (xa, ya)(cid:11) ∈ L do

for each e1 . . . a ∈ C[x1, y1][τ1] × · · · × C[xa, ya][τa] do

e ← o(e1, . . . , ea)
C[j, k][τ ] ← C[j, k][τ ] ∪ {e}
if ∀x ∈ X : φ(λv. e, x) then return λv. e

return ⊥

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

Overﬁtting in Synthesis: Theory and Practice

13

Algorithm 3 An algorithm to divide a given size budget among subexpressions 9
func Divide (a : Arity, q : Size, l : Op. Level, j : Expr. Level, α : Accumulated Args.)

1
2 (cid:73) Requires: 1 ≤ a ≤ q ∧ l ≤ j
3

if a = 1 then

if l = j ∨ ∃ hx, yi ∈ α : x = j then return (cid:8)(1, q) (cid:5) α, . . . , (j, q) (cid:5) α(cid:9)
return (cid:8)(j, q) (cid:5) α(cid:9)

4

5

6

7

8

9
10

L = {}
for u ← 1 to j do

for v ← 1 to (q − a + 1) do

L ← L ∪ Divide(a − 1, q − v, l, j, (u, v) (cid:5) α)

return L

the operator. Divide accepts an arity a for an operator o, a size budget q, the
index l of the least-expressive grammar containing o, the index j of the least-
expressive grammar that should contain the constructed expressions of the form
o(e1, . . . , ea), and an accumulator α that stores the list of argument locations.
In lines 7 – 9, the size budget is recursively divided among a − 1 locations. In
each recursive step, the upper bound (q − a + 1) on v ensures that we have a size
budget of at least q − (q − a + 1) = a − 1 for the remaining a − 1 locations. This
results in a call tree such that the accumulator α at each leaf node contains the
locations from which to select the last a − 1 arguments, and we are left with some
size budget q ≥ 1 for the ﬁrst argument e1. Finally in lines 4 – 5, we carefully
select the locations for e1 to ensure that o(e1, . . . , ea) has not been synthesized
before — either o ∈ components(Ej) or at least one argument belongs to Ej \ Ej−1.
We conclude this section by stating some desirable properties satisﬁed by

HEnum. The proofs of the following theorems can be found in Appendix A.3.

Theorem 8 (HEnum is Complete up to Size q). Given a SyGuS problem
S = hfX→Y | φ, Ei T, let E1...p be component-based grammars over theory T such
that E1 ⊂ · · · ⊂ Ep = E, / be a well order on E1...p × N, and q ≥ 0 be an upper
bound on size of expressions. Then, HEnum(S, E1...p, /, q) will eventually ﬁnd a
satisfying expression if there exists one with size ≤ q.
Theorem 9 (HEnum is Eﬃcient). Given a SyGuS problem S = hfX→Y | φ, Ei T,
let E1...p be component-based grammars over theory T such that E1 ⊂ · · · ⊂ Ep ⊆ E,
/ be a well order on E1...p ×N, and q ≥ 0 be an upper bound on size of expressions.
Then, HEnum(S, E1...p, /, q) will enumerate each distinct expression at most once.

5 Experimental Evaluation

In this section we empirically evaluate PLearn and HEnum. Our evaluation
uses a set of 180 synthesis benchmarks,10 consisting of all 127 oﬃcial benchmarks
from the Inv track of 2018 SyGuS competition [4] augmented with benchmarks
from the 2018 Software Veriﬁcation competition (SV-Comp) [8] and challenging

9 We use (cid:5) as the cons operator for sequences, e.g., x (cid:5) hy, zi = hx, y, zi.
10 All benchmarks are available at https://github.com/SaswatPadhi/LoopInvGen.

14

Saswat Padhi, Todd Millstein, Aditya Nori, Rahul Sharma

(a) LoopInvGen [28]

(b) CVC4 [7, 32]

Solid blue curves ((cid:14)) show
original failure counts.

Dashed orange curves (•) show
failure counts with PLearn.

Timeout = 30 min.
(wall-clock)

(c) Stoch [3, III F]

(d) SketchAC [20, 36]

(e) EUSolver [5]

Fig. 6. The number of failures on increasing grammar expressiveness, for state-
of-the-art SyGuS tools, with and without the PLearn framework (Algorithm 1)

veriﬁcation problems proposed in prior work [9, 10]. All these synthesis tasks
are deﬁned over integer and Boolean values, and we evaluate them with the six
grammars described in Fig. 1. We have omitted benchmarks from other tracks of
the SyGuS competition as they either require us to construct E1...p (§ 4) by hand
or lack veriﬁcation oracles. All our experiments use an 8-core Intel ® Xeon ® E5
machine clocked at 2.30 GHz with 32 GB memory running Ubuntu ® 18.04.

5.1 Robustness of PLearn
For ﬁve state-of-the-art SyGuS solvers – (a) LoopInvGen [28], (b) CVC4 [7, 32],
(c) Stoch [3, III F], (d) SketchAC [8, 20], and (e) EUSolver [5] – we have
compared the performance across various grammars, with and without the
PLearn framework (Algorithm 1). In this framework, to solve a SyGuS problem
with the pth expressiveness level from our six integer-arithmetic grammars (see
Fig. 1), we run p independent parallel instances of a SyGuS tool, each with one of
the ﬁrst p grammars. For example, to solve a SyGuS problem with the Polyhedra
grammar, we run four instances of a solver with the Equalities, Intervals, Octagons
and Polyhedra grammars. We evaluate these runs for each tool, for each of the
180 benchmarks and for each of the six expressiveness levels.

Fig. 6 summarizes our ﬁndings. Without PLearn the number of failures
initially decreases and then increases across all solvers, as grammar expressiveness
increases. However, with PLearn the tools incur fewer failures at a given
level of expressiveness, and there is a trend of decreased failures with increased
expressiveness. Thus, we have demonstrated that PLearn is an eﬀective measure
to mitigate overﬁtting in SyGuS tools and signiﬁcantly improve their performance.

5.2 Performance of Hybrid Enumeration
To evaluate the performance of hybrid enumeration, we augment an existing
synthesis engine with HEnum (Algorithm 2). We modify our LoopInvGen
tool [28], which is the best-performing SyGuS synthesizer from Fig. 6. Internally,

Overﬁtting in Synthesis: Theory and Practice

15

Grammar M(cid:2)τ[P]
τ[H]

(cid:3)
(cid:3) M(cid:2)τ[H]
τ[L]

Equalities
Intervals
Octagons
Polyhedra
Polynomials
Peano

1.00
1.91
2.84
3.72
4.62
5.49

1.00
1.04
1.03
1.01
1.00
0.97

(a) Failures on increasing grammar expressiveness

(b) Median(M) overhead

Fig. 7. L - LoopInvGen, H - LoopInvGen+HE, P - PLearn (LoopInvGen).
H is not only signiﬁcantly robust against increasing grammar expressiveness, but
it also has a smaller total-time cost (τ) than P and a negligible overhead over L.

LoopInvGen leverages Escher [2], an enumerative synthesizer, which we replace
with HEnum. We make no other changes to LoopInvGen. We evaluate the
performance and resource usage of this solver, LoopInvGen+HE, relative to
the original LoopInvGen with and without PLearn (Algorithm 1).
Performance. In Fig. 7(a), we show the number of failures across our six grammars
for LoopInvGen, LoopInvGen+HE and LoopInvGen with PLearn, over our
180 benchmarks. LoopInvGen+HE has a signiﬁcantly lower failure rate than
LoopInvGen, and the number of failures decreases with grammar expressiveness.
Thus, hybrid enumeration is a good proxy for PLearn.
Resource Usage. To estimate how computationally expensive each solver is, we
compare their total-time cost (τ). Since LoopInvGen and LoopInvGen+HE are
single-threaded, for them we simply use the wall-clock time for synthesis as the
total-time cost. However, for PLearn with p parallel instances of LoopInvGen,
we consider the total-time cost as p times the wall-clock time for synthesis.

In Fig. 7(b), we show the median overhead (ratio of τ) incurred by PLearn
over LoopInvGen+HE and LoopInvGen+HE over LoopInvGen, at various
expressiveness levels. As we move to grammars of increasing expressiveness, the
total-time cost of PLearn increases signiﬁcantly, while the total-time cost of
LoopInvGen+HE essentially matches that of LoopInvGen.

5.3 Competition Performance
Finally, we evaluate the performance of LoopInvGen+HE on the benchmarks
from the Inv track of the 2018 SyGuS competition [4], against the oﬃcial winning
solver, which we denote LIG [27] — a version of LoopInvGen [28] that has been
extensively tuned for this track. In the competition, there are some invariant-
synthesis problems where the postcondition itself is a satisfying expression. LIG
starts with the postcondition as the ﬁrst candidate and is extremely fast on such
programs. For a fair comparison, we added this heuristic to LoopInvGen+HE
as well. No other change was made to LoopInvGen+HE.

LoopInvGen solves 115 benchmarks in a total of 2191 seconds whereas
LoopInvGen+HE solves 117 benchmarks in 429 seconds, for a mean speedup
of over 5×. Moreover, no entrants to the competition could solve [4] the two
additional benchmarks (gcnr_tacas08 and fib_20) that LoopInvGen+HE solves.

16

Saswat Padhi, Todd Millstein, Aditya Nori, Rahul Sharma

6 Related Work

The most closely related work to ours investigates overﬁtting for veriﬁcation
tools [35]. Our work diﬀers from theirs in several respects. First, we address
the problem of overﬁtting in CEGIS-based synthesis. Second, we formally deﬁne
overﬁtting and prove that all synthesizers must suﬀer from it, whereas they only
observe overﬁtting empirically. Third, while they use cross-validation to combat
overﬁtting in tuning a speciﬁc hyperparameter of a veriﬁer, our approach is to
search for solutions at diﬀerent expressiveness levels.

The general problem of eﬃciently searching a large space of programs for
synthesis has been explored in prior work. Lee et al. [24] use a probabilistic model,
learned from known solutions to synthesis problems, to enumerate programs in
order of their likelihood. Other approaches employ type-based pruning of large
search spaces [26, 31]. These techniques are orthogonal to, and may be combined
with, our approach of exploring grammar subsets.

Our results are widely applicable to existing SyGuS tools, but some tools fall
outside our purview. For instance, in programming-by-example (PBE) systems [18,
§7], the speciﬁcation consists of a set of input-output examples. Since any program
that meets the given examples is a valid satisfying expression, our notion of
overﬁtting does not apply to such tools. However in a recent work, Inala and
Singh [19] show that incrementally increasing expressiveness can also aid PBE
systems. They report that searching within increasingly expressive grammar
subsets requires signiﬁcantly fewer examples to ﬁnd expressions that generalize
better over unseen data. Other instances where the synthesizers can have a free
lunch, i.e., always generate a solution with a small number of counterexamples,
include systems that use grammars with limited expressiveness [16, 21, 34].

Our paper falls in the category of formal results about SyGuS. In one such
result, Jha and Seshia [22] analyze the eﬀects of diﬀerent kinds of counterexamples
and of providing bounded versus unbounded memory to learners. Notably, they
do not consider variations in “concept classes” or “program templates,” which
are precisely the focus of our study. Therefore, our results are complementary: we
treat counterexamples and learners as opaque and instead focus on grammars.

7 Conclusion

Program synthesis is a vibrant research area; new and better synthesizers are
being built each year. This paper investigates a general issue that aﬀects all
CEGIS-based SyGuS tools. We recognize the problem of overﬁtting, formalize it,
and identify the conditions under which it must occur. Furthermore, we provide
mitigating measures for overﬁtting that signiﬁcantly improve the existing tools.

Acknowledgement. We thank Guy Van den Broeck and the anonymous reviewers for
helpful feedback for improving this work, and the organizers of the SyGuS competition
for making the tools and benchmarks publicly available.

This work was supported in part by the National Science Foundation (NSF) under
grants CCF-1527923 and CCF-1837129. The lead author was also supported by an
internship and a PhD Fellowship from Microsoft Research.

Overﬁtting in Synthesis: Theory and Practice

17

References

1. The SyGuS Competition. http://sygus.org/comp/ (2019), Accessed: 2019-05-10
2. Albarghouthi, A., Gulwani, S., Kincaid, Z.: Recursive Program Synthesis.

In:
Computer Aided Veriﬁcation - 25th International Conference, CAV, Proceedings.
Lecture Notes in Computer Science, vol. 8044, pp. 934–950. Springer (2013),
https://doi.org/10.1007/978-3-642-39799-8_67

3. Alur, R., Bodík, R., Juniwal, G., Martin, M.M.K., Raghothaman, M., Seshia, S.A.,
Singh, R., Solar-Lezama, A., Torlak, E., Udupa, A.: Syntax-Guided Synthesis.
In: Formal Methods in Computer-Aided Design, FMCAD. pp. 1–8. IEEE (2013),
http://ieeexplore.ieee.org/document/6679385/

4. Alur, R., Fisman, D., Padhi, S., Singh, R., Udupa, A.: SyGuS-Comp 2018: Results
and Analysis. CoRR abs/1904.07146 (2019), http://arxiv.org/abs/1904.07146
5. Alur, R., Radhakrishna, A., Udupa, A.: Scaling Enumerative Program Synthesis
In: Tools and Algorithms for the Construction and
via Divide and Conquer.
Analysis of Systems - 23rd International Conference, TACAS, Held as Part of
the European Joint Conferences on Theory and Practice of Software, ETAPS,
Proceedings, Part I. Lecture Notes in Computer Science, vol. 10205, pp. 319–336
(2017), https://doi.org/10.1007/978-3-662-54577-5_18

6. Alur, R., Singh, R., Fisman, D., Solar-Lezama, A.: Search-Based Program Synthesis.

Commun. ACM 61(12), 84–93 (2018), https://doi.org/10.1145/3208071

7. Barrett, C., Conway, C.L., Deters, M., Hadarean, L., Jovanovic, D., King, T.,
Reynolds, A., Tinelli, C.: CVC4. In: Computer Aided Veriﬁcation - 23rd Interna-
tional Conference, CAV, Proceedings. Lecture Notes in Computer Science, vol. 6806,
pp. 171–177. Springer (2011), https://doi.org/10.1007/978-3-642-22110-1_14
8. Beyer, D.: Software Veriﬁcation with Validation of Results - (Report on SV-
COMP 2017). In: Tools and Algorithms for the Construction and Analysis of
Systems - 23rd International Conference, TACAS, Held as Part of the European
Joint Conferences on Theory and Practice of Software, ETAPS, Proceedings,
Part II. Lecture Notes in Computer Science, vol. 10206, pp. 331–349 (2017),
https://doi.org/10.1007/978-3-662-54580-5_20

9. Bounov, D., DeRossi, A., Menarini, M., Griswold, W.G., Lerner, S.: Inferring Loop
Invariants through Gamiﬁcation. In: Proceedings of the 2018 CHI Conference
on Human Factors in Computing Systems, CHI. p. 231. ACM (2018), https:
//doi.org/10.1145/3173574.3173805

10. Bradley, A.R., Manna, Z., Sipma, H.B.: The Polyranking Principle. In: Automata,
Languages and Programming, 32nd International Colloquium, ICALP, Proceedings.
Lecture Notes in Computer Science, vol. 3580, pp. 1349–1361. Springer (2005),
https://doi.org/10.1007/11523468_109

11. Cousot, P., Cousot, R.: Static Determination of Dynamic Properties of Generalized
Type Unions. In: Language Design for Reliable Software. pp. 77–94 (1977), https:
//doi.org/10.1145/800022.808314

12. Cousot, P., Halbwachs, N.: Automatic Discovery of Linear Restraints Among
Variables of a Program. In: Conference Record of the Fifth Annual ACM Symposium
on Principles of Programming Languages. pp. 84–96. ACM Press (1978), https:
//doi.org/10.1145/512760.512770

13. Dietterich, T.G.: Ensemble Methods in Machine Learning. In: Multiple Classi-
ﬁer Systems, First International Workshop, MCS, Proceedings. Lecture Notes in
Computer Science, vol. 1857, pp. 1–15. Springer (2000), https://doi.org/10.1007/
3-540-45014-9_1

18

Saswat Padhi, Todd Millstein, Aditya Nori, Rahul Sharma

14. Ezudheen, P., Neider, D., D’Souza, D., Garg, P., Madhusudan, P.: Horn-ICE
Learning for Synthesizing Invariants and Contracts. PACMPL 2(OOPSLA), 131:1–
131:25 (2018), https://doi.org/10.1145/3276501

15. Feng, Y., Martins, R., Geﬀen, J.V., Dillig, I., Chaudhuri, S.: Component-Based
Synthesis of Table Consolidation and Transformation Tasks From Examples. In:
Proceedings of the 38th ACM SIGPLAN Conference on Programming Language
Design and Implementation, PLDI. pp. 422–436. ACM (2017), https://doi.org/
10.1145/3062341.3062351

16. Godefroid, P., Taly, A.: Automated Synthesis of Symbolic Instruction Encodings
From I/O Samples. In: ACM SIGPLAN Conference on Programming Language
Design and Implementation, PLDI. pp. 441–452. ACM (2012), https://doi.org/
10.1145/2254064.2254116

17. Gulwani, S., Jojic, N.: Program Veriﬁcation as Probabilistic Inference. In: Proceed-
ings of the 34th ACM SIGPLAN-SIGACT Symposium on Principles of Program-
ming Languages, POPL. pp. 277–289. ACM (2007), https://doi.org/10.1145/
1190216.1190258

18. Gulwani, S., Polozov, O., Singh, R.: Program Synthesis. Foundations and Trends in
Programming Languages 4(1-2), 1–119 (2017), https://doi.org/10.1561/2500000010
19. Inala, J.P., Singh, R.: WebRelate: Integrating Web Data with Spreadsheets using
Examples. PACMPL 2(POPL), 2:1–2:28 (2018), https://doi.org/10.1145/3158090
20. Jeon, J., Qiu, X., Solar-Lezama, A., Foster, J.S.: Adaptive Concretization for
Parallel Program Synthesis. In: Computer Aided Veriﬁcation - 27th International
Conference, CAV, Proceedings, Part II. pp. 377–394. Lecture Notes in Computer
Science, Springer (2015), https://doi.org/10.1007/978-3-319-21668-3_22

21. Jha, S., Gulwani, S., Seshia, S.A., Tiwari, A.: Oracle-Guided Component-Based
Program Synthesis. In: Proceedings of the 32nd ACM/IEEE International Con-
ference on Software Engineering - Volume 1, ICSE. pp. 215–224. ACM (2010),
https://doi.org/10.1145/1806799.1806833

22. Jha, S., Seshia, S.A.: A Theory of Formal Synthesis via Inductive Learning. Acta
Informatica 54(7), 693–726 (2017), https://doi.org/10.1007/s00236-017-0294-5
23. Le, X.D., Chu, D., Lo, D., Le Goues, C., Visser, W.: S3: Syntax- and Semantic-
Guided Repair Synthesis via Programming by Examples. In: Proceedings of the 11th
Joint Meeting on Foundations of Software Engineering, ESEC/FSE. pp. 593–604.
ACM (2017), https://doi.org/10.1145/3106237.3106309

24. Lee, W., Heo, K., Alur, R., Naik, M.: Accelerating Search-Based Program Synthesis
using Learned Probabilistic Models. In: Proceedings of the 39th ACM SIGPLAN
Conference on Programming Language Design and Implementation, PLDI 2018.
pp. 436–449. ACM (2018), https://doi.org/10.1145/3192366.3192410

25. Miné, A.: The Octagon Abstract Domain. In: Proceedings of the Eighth Working
Conference on Reverse Engineering, WCRE. p. 310. IEEE Computer Society (2001),
https://doi.org/10.1109/WCRE.2001.957836

26. Osera, P., Zdancewic, S.: Type-and-Example-Directed Program Synthesis.

In:
Proceedings of the 36th ACM SIGPLAN Conference on Programming Language
Design and Implementation, PLDI. pp. 619–630. ACM (2015), https://doi.org/
10.1145/2737924.2738007

27. Padhi, S., Sharma, R., Millstein, T.: LoopInvGen: A Loop Invariant Generator
based on Precondition Inference. CoRR abs/1707.02029 (2018), http://arxiv.
org/abs/1707.02029

28. Padhi, S., Sharma, R., Millstein, T.D.: Data-Driven Precondition Inference with
Learned Features. In: Proceedings of the 37th ACM SIGPLAN Conference on

Overﬁtting in Synthesis: Theory and Practice

19

Programming Language Design and Implementation, PLDI. pp. 42–56. ACM
(2016), https://doi.org/10.1145/2908080.2908099

29. Peano, G.: Calcolo geometrico secondo l’Ausdehnungslehre di H. Grassmann: pre-
ceduto dalla operazioni della logica deduttiva, vol. 3. Fratelli Bocca (1888)
30. Perelman, D., Gulwani, S., Grossman, D., Provost, P.: Test-driven synthesis. In:
ACM SIGPLAN Conference on Programming Language Design and Implementation,
PLDI. pp. 408–418. ACM (2014), https://doi.org/10.1145/2594291.2594297
31. Polikarpova, N., Kuraj, I., Solar-Lezama, A.: Program Synthesis from Polymorphic
Reﬁnement Types. In: Proceedings of the 37th ACM SIGPLAN Conference on
Programming Language Design and Implementation, PLDI. pp. 522–538. ACM
(2016), https://doi.org/10.1145/2908080.2908093

32. Reynolds, A., Deters, M., Kuncak, V., Tinelli, C., Barrett, C.W.: Counterexample-
Guided Quantiﬁer Instantiation for Synthesis in SMT. In: Computer Aided Ver-
iﬁcation - 27th International Conference, CAV, Proceedings, Part II. Lecture
Notes in Computer Science, vol. 9207, pp. 198–216. Springer (2015), https:
//doi.org/10.1007/978-3-319-21668-3_12

33. Shalev-Shwartz, S., Ben-David, S.: Understanding Machine Learning: From Theory

to Algorithms. Cambridge University Press (2014)

34. Sharma, R., Gupta, S., Hariharan, B., Aiken, A., Liang, P., Nori, A.V.: A Data
Driven Approach for Algebraic Loop Invariants. In: Programming Languages and
Systems - 22nd European Symposium on Programming, ESOP, Held as Part of
the European Joint Conferences on Theory and Practice of Software, ETAPS,
Proceedings. Lecture Notes in Computer Science, vol. 7792, pp. 574–592. Springer
(2013), https://doi.org/10.1007/978-3-642-37036-6_31

35. Sharma, R., Nori, A.V., Aiken, A.: Bias-Variance Tradeoﬀs in Program Analysis.
In: The 41st Annual ACM SIGPLAN-SIGACT Symposium on Principles of Pro-
gramming Languages, POPL. pp. 127–138. ACM (2014), http://doi.acm.org/10.
1145/2535838.2535853

36. Solar-Lezama, A.: Program Sketching. STTT 15(5-6), 475–495 (2013), https:

//doi.org/10.1007/s10009-012-0249-7

20

Saswat Padhi, Todd Millstein, Aditya Nori, Rahul Sharma

A Proofs of Theorems

A.1 No-Free-Lunch Theorems (§ 3.2)

Theorem 1 (NFL in CEGIS-based SyGuS on Finite Sets). Let X and Y
be two arbitrary ﬁnite sets, T be a theory that supports equality, E be a grammar
over T, and m be an integer such that 0 ≤ m < |X|. Then, either:

– E is not k-expressive for any k > Pm
– for every CEGIS-based learner L, there exists a satisﬁable SyGuS problem
S = hfX→Y | φ, Ei T such that S is not m-learnable by L. Moreover, there exists
a diﬀerent CEGIS-based learner for which S is m-learnable.

|X|! |Y |i
(|X| − i)! , or

i = 0

Proof. First, note that there are t = Pm
distinct traces (sequences
of counterexamples) of length at most m over X and Y . Now, consider some
CEGIS-based learner L, and suppose E is k-expressive for some k > t. Then,
since the learner can deterministically choose at most t candidates for the t
traces, there must be at least one function f that is expressible in E, but does
not appear in the trace of LO(m, E) for any oracle O.

|X|! |Y |i
(|X| − i)!

i = 0

Let e be an expression in E that implements the function f . Then, we can deﬁne
the speciﬁcation φ(f, x) def= f (x) = e(x) and the SyGuS problem S = hfX→Y | φ, Ei T
.
By construction, S is satisﬁable since e |= S, but we have that LO(m, E) 6|= S for
all oracles O. So, by Deﬁnition 6, we have that S is not m-learnable by L.

However, we can construct a learner L0 such that S is m-learnable by L0. We
construct L0 such that L0 always produces e as its ﬁrst candidate expression for
any trace. The result then follows by Deﬁnition 6.
ut

Theorem 2 (NFL in CEGIS-based SyGuS on Countably Inﬁnite Sets).
Let X be an arbitrary countably inﬁnite set, Y be an arbitrary ﬁnite or countably
inﬁnite set, T be a theory that supports equality, E be a grammar over T, and m
be an integer such that m ≥ 0. Then, either:

– E is not k-expressive for any k > ℵ0, where ℵ0
– for every CEGIS-based learner L, there exists a satisﬁable SyGuS problem
S = hfX→Y | φ, Ei T such that S is not m-learnable by L. Moreover, there exists
a diﬀerent CEGIS-based learner for which S is m-learnable.

def= |N|, or

Proof. Consider some CEGIS-based learner L, and suppose E is k-expressive for
some k > ℵ0. Note that there are Pm
distinct traces of length at most
m over X and Y . Let us overapproximate each |X|! |Y |i
(|X| − i)!
the number of distinct traces as (m + 1) (|X| |Y |)m. We have two cases for Y :
1. Y is ﬁnite i.e., |X| = ℵ0 and |Y | < ℵ0. Then, the number of distinct traces is

as (|X| |Y |)m, and thus

|X|! |Y |i
(|X| − i)!

i = 0

at most (m + 1) (|X| |Y |)m = (ℵ0 |Y |)m = ℵ0. Or,

2. Y is countably inﬁnite i.e., |X| = |Y | = ℵ0. Then, the number of distinct

traces is at most (m + 1) (|X| |Y |)m = (ℵ0 ℵ0)m = ℵ0.
Thus, the number of distinct traces is at most ℵ0, i.e., countably inﬁnite. Since
the number of distinct functions k > ℵ0, the claim follows using a construction
similar to the proof of Theorem 1.
ut

Overﬁtting in Synthesis: Theory and Practice

21

A.2 Overﬁtting Theorems (§ 3.3)

Theorem 3 (Overﬁtting in SyGuS on Finite Sets). Let X and Y be two
arbitrary ﬁnite sets, m be an integer such that 0 ≤ m < |X|, T be a theory
that supports equality, and E be a k-expressive grammar over T for some k >
|X|! |Y |m
m! (|X| − m)! . Then, there exists a satisﬁable SyGuS problem S = hfX→Y | φ, Ei T
such that Ω(S, Z) > 0, for every set Z of m IO examples for φ.

Proof. First, note that there are t = |X|! |Y |m
distinct ways of constructing a
m! (|X| − m)!
set of m IO examples, over X and Y . Now, suppose E is k-expressive for some
k > t. Then, there must be at least one function f that is expressible in E, but
every set of m IO examples that f is consistent with is also satisﬁed by some
other expressible function.

Let e be an expression in E that implements the function f . Then, we can deﬁne
the speciﬁcation φ(f, x) def= f (x) = e(x) and the SyGuS problem S = hfX→Y | φ, Ei T
.
The claim then immediately follows from Deﬁnition 7.
ut

Theorem 4 (Overﬁtting in SyGuS on Countably Inﬁnite Sets). Let X be
an arbitrary countably inﬁnite set, Y be an arbitrary ﬁnite or countably inﬁnite set,
T be a theory that supports equality, and E be a k-expressive grammar over T for
some k > ℵ0. Then, there exists a satisﬁable SyGuS problem S = hfX→Y | φ, Ei T
such that Ω(S, Z) > 0, for every set Z of m IO examples for φ.

Proof. Let us overapproximate the number of distinct ways of constructing a
set of m IO examples,
as (|X| |Y |)m. Using cardinal arithmetic, as
shown in the the proof of Theorem 2, this number is always at most ℵ0. Then
the claim follows using a construction similar to the proof of Theorem 3.
ut

|X|! |Y |m
m! (|X| − m)!

Theorem 5 (Overﬁtting Increases with Expressiveness). Let X and Y
be two arbitrary sets, T be an arbitrary theory, E1 and E2 be grammars over T
such that E1 ⊆ E2, φ be an arbitrary speciﬁcation over T and a function symbol
f : X → Y , and Z be a set of IO examples for φ. Then, we have
Ω(cid:0) hfX→Y | φ, E1i T , Z(cid:1) ≤ Ω(cid:0) hfX→Y | φ, E2i T , Z(cid:1)

Proof. If E1 ⊆ E2, then for any set Z ⊆ X × Y of IO examples, we have

{e ∈ E1 | ∀hx, yi ∈ Z : e(x) = y} ⊆ {e ∈ E2 | ∀hx, yi ∈ Z : e(x) = y}

The claim immediately follows from this observation and Deﬁnition 7.

ut

A.3 Properties of Hybrid Enumeration (§ 4.2)

Lemma 1. Let E1 and E2 be two arbitrary component-based grammars. Then, if
E1 ⊆ E2, it must also be the case that components(E1) ⊆ components(E2), where
components(Ei) denotes the set of all components appearing in Ei.

22

Saswat Padhi, Todd Millstein, Aditya Nori, Rahul Sharma

Proof. Let C1 = components(E1), C2 = components(E2), and E1 ⊆ E2. Suppose
C1 6⊆ C2. Then, there must be at least one component c such that c ∈ C1 \ C2.
By deﬁnition of components(E1), the component c must appear in at least one
expression e ∈ E1. However, since c 6∈ C2, it must be the case that e 6∈ E2, thus
contradicting E1 ⊆ E2. Hence, our assumption C1 6⊆ C2 must be false.
ut

Theorem 7. Given component-based grammars E1...p, the following strict partial
order / ∗ on E1...p × N is a well order
∀ Ea, Eb ∈ E1...p : ∀ m, n ∈ N :

(Ea, m) / ∗ (Eb, n) ⇐⇒ | Ca |m < | Cb |n

where Ci = components(Ei) denotes the set of all components appearing in Ei.
Proof. Let Ea and Eb be two component-based grammars in E1...p. By Lemma 1,
we have that Ea ⊆ Eb =⇒ components(E1) ⊆ components(E2). The claim then
immediately follows from Deﬁnition 9.
ut

Deﬁnition 10 (jk-Uniqueness). Given grammars E1 ⊆ · · · ⊆ Ep, we say that
an expression e of size k is jk-unique with respect to E1...p if it is contained in
Ej but not in E(j−1). We deﬁne U[E1...p]k
j as the maximal such set of expressions,
i.e.,

U[E1...p]k
j

def= (cid:8)e ∈ Ej | size(e) = k ∧ e 6∈ E(j−1)

(cid:9)

Lemma 2. Let E1 ⊆ · · · ⊆ Ep be p component-based grammars. Then, for any
j , if the operator o belongs to operators(Eq)
expression o(e1, . . . , ea) ∈ U[E1...p]k
such that q < j, at least one argument must belong to Ej but not E(j−1), i.e.,

o ∈ operators(Eq) ∧ q < j =⇒ ∃ e ∈ e1 . . . a

: e ∈ Ej ∧ e 6∈ E(j−1)

such that
Proof. Consider an arbitrary expression e∗ = o(e1, . . . , ea) ∈ U[E1...p]k
j
(cid:3). Then, for
o ∈ operators(Eq) ∧ q < j. Suppose (cid:2)∀ e ∈ e1 . . . a
: e 6∈ Ej ∨ e ∈ E(j−1)
any argument subexpression e, we have the following three possibilities:

(cid:55) e 6∈ Ej ∧ e ∈ E(j−1) is impossible since E(j−1) ⊆ Ej.
(cid:55) e 6∈ Ej ∧ e 6∈ E(j−1) is also impossible, by Deﬁnition 8, due to the closure

property of component-based grammars.

(cid:55) e ∈ Ej ∧ e ∈ E(j−1) must be false for at least one argument subexpression.
Otherwise, since o ∈ operators(E(j−1)) and E(j−1) is closed under opera-
tor application by Deﬁnition 8, e∗ ∈ E(j−1) must be true. However, by
Deﬁnition 10, we have that e∗ ∈ U[E1...p]k
j
: e 6∈ Ej ∨ e ∈ E(j−1)

Therefore, our assumption (cid:2)∀ e ∈ e1 . . . a
Lemma 3. Let E0 = {} and E1 ⊆ · · · ⊆ Ep be p component-based grammars.
Then, for any l ≥ 1 and any operator o ∈ operators(El) \ operators(El−1) of arity
a, Divide(a, k − 1, l, j, hi) generates the following set L of all possible distinct
locations for selecting the arguments for o such that o(e1, . . . , ea) ∈ U[E1...p]k
j :
n(cid:10)(j1, k1), . . . , (ja, ka)(cid:11) | o(e1, . . . , ea) ∈ U[E1...p]k

(cid:3) must be false. ut

=⇒ e∗ 6∈ E(j−1).

L =

∧ ∀ 1 ≤ i ≤ a : ei ∈ U[E1...p]ki
ji

j

o

Overﬁtting in Synthesis: Theory and Practice

23

Proof. In lines 7– 9 of Algorithm 3, the top-level Divide(a, k − 1, l, j, hi) call ﬁrst
recursively creates a call tree of height a − 1 such that the accumulator α at each
leaf node contains the locations for selecting the last a − 1 arguments from. Since
u in line 7 ranges over {1, . . . , j} and v in line 8 ranges over {1, . . . , (k − 2)},
the call tree must be exhaustive by construction. Concretely, the values of α
at the the leaf nodes must capture every possible sequence of a − 1 locations,
(cid:10)(j1, k1), . . . , (j(a−1), k(a−1))(cid:11), such that k1 + · · · + k(a−1) ≤ k − 2.

Finally at the leaf nodes of the call tree, lines 4 –5 are triggered to select
locations for the ﬁrst argument. The naïve approach of simply assigning the
remaining size to each grammar in E1...j would be exhaustive, but may lead to
enumerating other expressions o(e1, . . . , ea) 6∈ U[E1...p]k
when l < j. Therefore,
j
we check if l < j and no location (x, y) in α satisﬁes x = j, in which case we
assign the remaining size to only Ej in line 5. Lemma 2 shows that this check is
suﬃcient to guarantee that we only enumerate expressions in U[E1...p]k
ut
j
Theorem 8 (HEnum is Complete up to Size q). Given a SyGuS problem
S = hfX→Y | φ, Ei T, let E1...p be component-based grammars over theory T such
that E1 ⊂ · · · ⊂ Ep = E, / be a well order on E1...p × N, and q ≥ 0 be an upper
bound on size of expressions. Then, HEnum(S, E1...p, /, q) will eventually ﬁnd a
satisfying expression if there exists one with size ≤ q.
Proof. First, we observe that every expression e ∈ E must belong to some maximal
set of jk-unique expressions with respect to E1...p:

.

We show that C[j, k] in HEnum (Algorithm 2) stores U[E1...p]k
j

∀ e ∈ E : ∃ j ∈ {1, . . . , p} : ∃ k ∈ {1, . . . , q} : e ∈ U[E1...p]k
j
, into various
C[j, k][τ ] lists based on the expression type τ . Since HEnum computes C[j, k] for
each j ∈ {1, . . . , p} and each k ∈ {1, . . . , q}, it must enumerate every expression
in E with size at most q, and thus eventually ﬁnd e.

The base cases C[i, 1] = U[E1...p]1
i

are straightforward. The inductive case
follows from Lemma 3. For each (j, k) ∈ {1, . . . , p} × {1, . . . , q} and each operator
in E1...j, we invoke Divide (Algorithm 3) to generate all possible locations for the
operator’s arguments such that the ﬁnal expression is contained in U[E1...p]k
. Lines
j
by applying the operator to
16 – 20 in HEnum then populate C[j, k] as U[E1...p]k
j
subexpressions of appropriate types drawn from these locations.
ut
Lemma 4. Given grammars E1 ⊆ · · · ⊆ Ep, for any distinct pairs (j, k) and
(j0, k0) the sets U[E1...p]k

j and U[E1...p]k0
∀ j, k, j0, k0 : j 6= j0 ∨ k 6= k0 =⇒ U[E1...p]k
Proof. When k 6= k0, it is straightforward to show that U[E1...p]k
since an expression cannot be of size k and k0 at the same time.

j0 must be disjoint, i.e.,

j ∩ U[E1...p]k0

j0 = {}
j ∩ U[E1...p]k0

j0 = {},

We now prove the claim for the case when j 6= j0 by contradiction. Suppose
there exists an expression e ∈ U[E1...p]k
j0. Without loss of generality,
assume j > j0, and therefore Ej ⊇ Ej0. But then, by Deﬁnition 10, it must be
j0. Therefore, our assumption that
the case that e 6∈ Ej0 and thus e 6∈ U[E1...p]k
U[E1...p]k
ut

j0 6= {} must be false.

j ∩ U[E1...p]k

j ∩ U[E1...p]k

24

Saswat Padhi, Todd Millstein, Aditya Nori, Rahul Sharma

Theorem 9 (HEnum is Eﬃcient). Given a SyGuS problem S = hfX→Y | φ, Ei T,
let E1...p be component-based grammars over theory T such that E1 ⊂ · · · ⊂ Ep ⊆ E,
/ be a well order on E1...p ×N, and q ≥ 0 be an upper bound on size of expressions.
Then, HEnum(S, E1...p, /, q) will enumerate each distinct expression at most once.

Proof. As shown in the proof of Theorem 8, C[j, k] in HEnum (Algorithm 2)
stores U[E1...p]k
. Then, by Lemma 4, we immediately have that all pairs C[j, k]
j
and C[j0, k0] of synthesized expressions are disjoint when j 6= j0 or k 6= k0.

Furthermore, although each C[j, k] is implemented as a list, we show that any
two expressions within any C[j, k] list must be syntactically distinct. The base
cases C[i, 1] are straightforward. For the inductive case, observe that if each list
C[j1, k1], . . . , C[ja, ka] only contains syntactically distinct expressions, then all
tuples within C[j1, k1] × · · · × C[ja, ka] must also be distinct. Thus, if an operator
o with arity a is applied to subexpressions drawn from the cross product, i.e.,
he1, . . . , eai ∈ C[j1, k1] × · · · × C[ja, ka], then all resulting expressions of the form
o(e1, . . . , ea) must be syntactically distinct. Thus, by structural induction, we
have that in any list C[j, k] all contained expressions are syntactically distinct. ut

