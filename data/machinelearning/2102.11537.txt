Revisiting the Role of Euler Numerical Integration
on Acceleration and Stability in Convex Optimization

Peiyuan Zhang*
ETH Zürich

Antonio Orvieto Hadi Daneshmand Thomas Hofmann
INRIA Paris, ETH Zürich

ETH Zürich

ETH Zürich

Roy Smith
ETH Zürich

1
2
0
2

b
e
F
3
2

]

C
O
.
h
t
a
m

[

1
v
7
3
5
1
1
.
2
0
1
2
:
v
i
X
r
a

Abstract

Viewing optimization methods as numerical
integrators for ordinary diﬀerential equations
(ODEs) provides a thought-provoking modern
framework for studying accelerated ﬁrst-order
optimizers. In this literature, acceleration is
often supposed to be linked to the quality of
the integrator (accuracy, energy preservation,
symplecticity). In this work, we propose a
novel ordinary diﬀerential equation that ques-
tions this connection: both the explicit and
the semi-implicit (a.k.a symplectic) Euler dis-
cretizations on this ODE lead to an acceler-
ated algorithm for convex programming. Al-
though semi-implicit methods are well-known
in numerical analysis to enjoy many desirable
features for the integration of physical sys-
tems, our ﬁndings show that these properties
do not necessarily relate to acceleration.

1

Introduction

Momentum methods are the state-of-the-art choice of
practitioners for the optimization of machine learning
models. The simplest of such algorithms is the Heavy-
ball (HB), ﬁrst proposed and analyzed in the context
of convex optimization by Polyak (1964):

xk+1 = xk + β(xk − xk−1) − s∇f (xk)

(HB)

where f : Rd → R is the L-smooth1 function we want
to minimize, s > 0 is the step-size and β ∈ [0, 1) the

*Correspondence to talantyeri@gmail.com.
1For all x, y ∈ Rd, (cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107), where

(cid:107) · (cid:107) is the standard Euclidean norm.

Proceedings of the 24th International Conference on Artiﬁ-
cial Intelligence and Statistics (AISTATS) 2021, San Diego,
California, USA. PMLR: Volume 130. Copyright 2021 by
the author(s).

momentum parameter. Using a novel and beautiful ar-
gument on ﬁxed point iterations, Polyak (1964) proved
that, if f is twice continuously diﬀerentiable and µ-
strongly-convex2, the sequence (xk)k≥0 produced by
if initialized close to the solution)
HB locally (i.e.
converges to the minimizer x∗ = arg minx∈Rd f (x) at
an accelerated rate. The keyword “accelerated” has a
precise meaning: an algorithm for µ-strongly-convex
and L-smooth problems is accelerated if and only if
the convergence rate of f (xk) to f ∗ := minx∈Rd f (x) is
O((1 − (cid:112)
µ/L)k). For instance, Gradient Descent (i.e.
β = 0) in this setting converges linearly but with con-
stant 1 − µ/L and is therefore not accelerated3.
Nesterov’s acceleration. Supported by the lower
bounds established by Nemirovsky and Yudin (1983),
many researchers in the early 80s tried to develop
an algorithm with a global accelerated convergence
rate. The problem was solved by Nesterov (1983), who
proposed the following modiﬁcation4 of HB:

xk+1 = xk + β(xk − xk−1) − s∇f (xk)
− βs(∇f (xk) − ∇f (xk−1)).

(NAG)

The intuition behind this algorithm puzzled researchers
for decades, and many articles are devoted to under-
standing the underlying mechanism (Allen-Zhu and
Orecchia, 2014; Defazio, 2019; Ahn, 2020) and the
role of the small yet crucial modiﬁcation5 compared
to HB (Flammarion and Bach, 2015; Lessard et al.,
2016; Hu and Lessard, 2017). Notwithstanding the the-
oretical value of these contributions, they are arguably
only of a descriptive nature and leave open more fun-
damental questions on the reason behind acceleration.

Continuous-time models for acceleration. A
new line of research bloomed from a seminal paper by
Su et al. (2014). This work gained a lot of attraction,

2∀x ∈ Rd, ∇2f (x) − µI is positive semideﬁnite.
3If L/µ is large, 1 − (cid:112)µ/L (cid:28) 1 − µ/L.
4In the original paper Nesterov (1983), the algorithm is
presented in a more general way. Our formulation is similar
to Shi et al. (2018).

5This is usually referred to as gradient extrapolation.

 
 
 
 
 
 
Revisiting the Role of Symplectic Integration on Acceleration and Stability in Convex Optimization

as it introduces6 a powerful way to look at acceleration
through the lens of second order ordinary diﬀerential
equations (ODEs). In the µ-strongly-convex case, this
equation is

√

¨X + 2

µ ˙X + ∇f (X) = 0

(NAG-ODE)

and retains the essence of acceleration: namely, con-
√
vergence with a rate O(e−
µt). Analogously to the
discrete-time case we just discussed, one can prove that
the continuous-time model of gradient descent, i.e. the
gradient ﬂow ˙X = −∇f (X), converges instead at the
non-accelerated rate O(e−µt). Other interesting prop-
erties of damped gradient systems such as NAG-ODE
can be found in the (stochastic) optimization litera-
ture (Krichene et al., 2015; Xu et al., 2018; Cabot et al.,
2009; Orvieto et al., 2020; Orvieto and Lucchi, 2019;
Diakonikolas and Jordan, 2019; Alecsa et al., 2019; Al-
imisis et al., 2020), and in the applied mathematics
literature (Sanz-Serna and Zygalakis, 2020; Attouch
et al., 2000; Attouch and Alvarez, 2000; Alvarez, 2000;
Bégout et al., 2015).

High-resolution ODEs. As ﬁrst noted by Wil-
son et al. (2016), while NAG-ODE is formally the
continuous-time limit (for some speciﬁc choice of β)
of NAG, it is also the continuous-time limit of HB. In
other words, NAG-ODE does not capture the vanish-
ing gradient correction (a.k.a gradient extrapolation)
term βs(∇f (xk) − ∇f (xk−1)), which is regarded to be
a fundamental piece of the acceleration machinery in
discrete-time. To solve this issue (i.e. to get a more
accurate model of Nesterov’s acceleration), Shi et al.
(2018) introduced a high-resolution model of NAG:

√

¨X + (2

+ (1 + 2

µ +
√

√

s∇2f (X)) ˙X
µs)∇f (X) = 0.

(NAG-ODE-HR)

Remarkably, here (1) the step-size s is included directly
in the model, and (2) the vanishing (as s → 0) term
√
s∇2f (X) ˙X is used to capture the gradient correc-
s∇2f (X) ˙X
tion βs(∇f (xk) − ∇f (xk−1)). The term
is referred to as Hessian damping, and can be seen
as a curvature-dependent viscosity correction. As a
validation for their new ODE, Shi et al. (2018) showed
that NAG-ODE-HR enjoys the same accelerated rate
of NAG-ODE — but it is empirically more faithful to
NAG compared to NAG-ODE, for ﬁnite values of s.

√

In a sec-
Connection to numerical integration.
ond article, Shi et al. (2019) showed that NAG

6We point out that, actually, the diﬀerential equations
proposed in Su et al. (2014) was already written down
and partly analyzed in the original 1963 paper by Polyak
(1964). Even more surprisingly, a ﬁrst study of damped
second order diﬀerential equations for optimization can be
found already in a 1958 paper of the soviet mathematician
Gavurin (1958).

can be approximately recovered7 through a semi-
implicit (a.k.a.
symplectic) Euler discretization of
NAG-ODE-HR. The authors also claim that if the same
system is integrated with the explicit Euler method, the
resulting optimizer might 8 not be accelerated because it
is only found stable for small values of s. Semi-implicit
methods are well-known to perform remarkably well for
integrating second-order ODEs in physics (Hairer et al.,
2006) and chemistry (Lubich, 2008); namely, one can
use big step-sizes while preserving the geometry of the
original ﬂow. Shi et al. suggested that the essence of
acceleration can be explained by the same phenomenon,
which is mathematically well understood in the Hamil-
tonian (i.e. energy-conserving) setting thanks to the
theory of backward error analysis (Hairer, 1994; Benet-
tin and Giorgilli, 1994).

Figure 1: Sketch of the storyline of Shi et al. (2019): while
semi-implicit discretization of NAG-ODE-HR yields an ac-
celerated method, explicit discretization results in a method
not known in the literature, which is claimed to be stable
only for very small step-sizes (s ≤ O(µ/L2), compared to
s ≤ O(1/L) of the semi-implicit method). This is used by
the authors to advocate that the structure provided by semi-
implicit integration is somethow critical for the construction
of accelerated methods. This storyline (and the associated
conclusion) is much diﬀerent from ours, sketched in Fig. 2.
Superiority of semi-implicit methods is also claimed/hinted
in several works (Shi et al., 2018; Bravetti et al., 2019; Be-
tancourt et al., 2018; França et al., 2020a; Muehlebach and
Jordan, 2019).

√

On a parallel line, Muehlebach and Jordan (2019)
derived a diﬀerent continuous-time model that con-
s ˙X) instead of
tains terms of the form ∇f (X +
√
s∇2f (X) ˙X. This ODE can also relate to Nesterov’s
method through semi-implicit integration. More-
over, inspired by the variational perspective presented
in Wibisono et al. (2016), many research papers (Be-
tancourt et al., 2018; Muehlebach and Jordan, 2020;
França et al., 2020a,b; Alecsa, 2020; Bravetti et al.,
2019) have been devoted to understanding the geomet-
ric properties of Nesterov’s method, seen as either (1)

7See approximations in Sec. 2.2 of Shi et al. (2019).
8We point out here a potential problem in the main
claim of Shi et al. (2019): the authors show that the explicit
integrator of NAG-ODE-HR is stable only for small step-
sizes by ﬁnding necessary conditions for the steady decrease
of a particular energy function. While this fact surely hints
at potential instabilities of the associated algorithm, it does
not per se provide a suﬃcient condition for slow convergence.
In other words, one could in principle ﬁnd a diﬀerent result
by choosing a diﬀerent Lyapunov function.

Acceleratedﬂow1<latexit sha1_base64="K93uaxogEdXw7evigAefzB+8vL8=">AAACAnicbVDLSgNBEJz1GeNr1ZN4GQyCp7ArAfUW9eIxgnlAsoTZSScZMvtgplcNS/Dir3jxoIhXv8Kbf+Mk2YMmFjQUVd10d/mxFBod59taWFxaXlnNreXXNza3tu2d3ZqOEsWhyiMZqYbPNEgRQhUFSmjECljgS6j7g6uxX78DpUUU3uIwBi9gvVB0BWdopLa930J4wPSCc5CgGEKHdmV0T91R2y44RWcCOk/cjBRIhkrb/mp1Ip4EECKXTOum68TopUyh4BJG+VaiIWZ8wHrQNDRkAWgvnbwwokdGMasjZSpEOlF/T6Qs0HoY+KYzYNjXs95Y/M9rJtg981IRxglCyKeLuomkGNFxHrQjFHCUQ0MYV8LcSnmfKcbRpJY3IbizL8+T2knRLRXPb0qF8mUWR44ckENyTFxySsrkmlRIlXDySJ7JK3mznqwX6936mLYuWNnMHvkD6/MHGjuXQA==</latexit>inTb.1,ﬁrstcolumn<latexit sha1_base64="ZhBvwTsKsHkow2BzIz+3VFBhP9k=">AAACBnicbVDLSsNAFJ34rPUVdSnCYBFcSEikoO6KblxW6AvaUCbTSTt0MgkzN2IJXbnxV9y4UMSt3+DOv3HaZqGtBwYO59x7594TJIJrcN1va2l5ZXVtvbBR3Nza3tm19/YbOk4VZXUai1i1AqKZ4JLVgYNgrUQxEgWCNYPhzcRv3jOleSxrMEqYH5G+5CGnBIzUtY86wB4g4xLXAsc7wyFXGrCZm0ZyjLt2yXXcKfAi8XJSQjmqXfur04tpGjEJVBCt256bgJ8RBZwKNi52Us0SQoekz9qGShIx7WfTM8b4xCg9HMbKPDldIla/OzISaT2KAlMZERjoeW8i/ue1UwgvfXNjkgKTdPZRmAoMMZ5kgntcMQpiZAihiptdMR0QRSiY5IomBG/+5EXSOHe8snN1Vy5VrvM4CugQHaNT5KELVEG3qIrqiKJH9Ixe0Zv1ZL1Y79bHrHTJynsO0B9Ynz8RnZhA</latexit>GM-ODEwithm,n,qas<latexit sha1_base64="LsCb8aMCgzhFIY231fzaxZiMnuw=">AAACDnicbVDJSgNBEO1xjXGLevTSGAIeYpiRgHoLLuhFjGAWSELo6fQkTXp6xu4aNQz5Ai/+ihcPinj17M2/sbMcNPFBweO9KqrquaHgGmz725qZnZtfWEwsJZdXVtfWUxubZR1EirISDUSgqi7RTHDJSsBBsGqoGPFdwSpu92TgV+6Y0jyQN9ALWcMnbck9TgkYqZnK1IE9QHx+uXd1eobvOXRwH/tZLLP4dmRhovvNVNrO2UPgaeKMSRqNUWymvuqtgEY+k0AF0brm2CE0YqKAU8H6yXqkWUhol7RZzVBJfKYb8fCdPs4YpYW9QJmSgIfq74mY+Fr3fNd0+gQ6etIbiP95tQi8w0bMZRgBk3S0yIsEhgAPssEtrhgF0TOEUMXNrZh2iCIUTIJJE4Iz+fI0Ke/nnHzu6DqfLhyP40igbbSDdpGDDlABXaAiKiGKHtEzekVv1pP1Yr1bH6PWGWs8s4X+wPr8ARoHmug=</latexit>GM-ODEwithm,n,qas<latexit sha1_base64="LsCb8aMCgzhFIY231fzaxZiMnuw=">AAACDnicbVDJSgNBEO1xjXGLevTSGAIeYpiRgHoLLuhFjGAWSELo6fQkTXp6xu4aNQz5Ai/+ihcPinj17M2/sbMcNPFBweO9KqrquaHgGmz725qZnZtfWEwsJZdXVtfWUxubZR1EirISDUSgqi7RTHDJSsBBsGqoGPFdwSpu92TgV+6Y0jyQN9ALWcMnbck9TgkYqZnK1IE9QHx+uXd1eobvOXRwH/tZLLP4dmRhovvNVNrO2UPgaeKMSRqNUWymvuqtgEY+k0AF0brm2CE0YqKAU8H6yXqkWUhol7RZzVBJfKYb8fCdPs4YpYW9QJmSgIfq74mY+Fr3fNd0+gQ6etIbiP95tQi8w0bMZRgBk3S0yIsEhgAPssEtrhgF0TOEUMXNrZh2iCIUTIJJE4Iz+fI0Ke/nnHzu6DqfLhyP40igbbSDdpGDDlABXaAiKiGKHtEzekVv1pP1Yr1bH6PWGWs8s4X+wPr8ARoHmug=</latexit>inTb.1,secondcolumn<latexit sha1_base64="O0zE6PObAtTlVsm3Zn7pUxK0KYw=">AAACB3icbVDLSgMxFM34rPVVdSlIsAgupMxIQd0V3bis0Be0Q8mkt21oJjMkd8RSunPjr7hxoYhbf8Gdf2PazkJbDwQO59yb5JwglsKg6347S8srq2vrmY3s5tb2zm5ub79mokRzqPJIRroRMANSKKiiQAmNWAMLAwn1YHAz8ev3oI2IVAWHMfgh6ynRFZyhldq5oxbCA46EopWg4J1RAzxSHWovTkI1pu1c3i24U9BF4qUkT1KU27mvVifiSQgKuWTGND03Rn/ENAouYZxtJQZixgesB01LFQvB+KNpjjE9sUqHdiNtj0I6VX9vjFhozDAM7GTIsG/mvYn4n9dMsHvp25BxgqD47KFuIilGdFIK7QgNHOXQEsa1sH+lvM8042iry9oSvPnIi6R2XvCKhau7Yr50ndaRIYfkmJwSj1yQErklZVIlnDySZ/JK3pwn58V5dz5mo0tOunNA/sD5/AHALpie</latexit>Acceleratedﬂow2<latexit sha1_base64="8zyO5a08hpel8DGC2x4t6hIOZ6w=">AAACAnicbVDJSgNBEO2JW4xb1JN4aQyCpzATAuot6sVjBLNAMoSeTk3SpGehu0YNQ/Dir3jxoIhXv8Kbf2NnOWjig4LHe1VU1fNiKTTa9reVWVpeWV3Lruc2Nre2d/K7e3UdJYpDjUcyUk2PaZAihBoKlNCMFbDAk9DwBldjv3EHSosovMVhDG7AeqHwBWdopE7+oI3wgOkF5yBBMYQu9WV0T0ujTr5gF+0J6CJxZqRAZqh28l/tbsSTAELkkmndcuwY3ZQpFFzCKNdONMSMD1gPWoaGLADtppMXRvTYKGZ1pEyFSCfq74mUBVoPA890Bgz7et4bi/95rQT9MzcVYZwghHy6yE8kxYiO86BdoYCjHBrCuBLmVsr7TDGOJrWcCcGZf3mR1EtFp1w8vykXKpezOLLkkByRE+KQU1Ih16RKaoSTR/JMXsmb9WS9WO/Wx7Q1Y81m9skfWJ8/G8CXQQ==</latexit>ExplicitEuler<latexit sha1_base64="SIrn/RuA1tsOIfpVfyDLM4St5+w=">AAAB/nicbVDLSgNBEJz1GeNrVTx5GQyCp7ArAfUWlIDHCOYBSQizk95kyOyDmV5JWAL+ihcPinj1O7z5N06SPWhiQUNR1U13lxdLodFxvq2V1bX1jc3cVn57Z3dv3z44rOsoURxqPJKRanpMgxQh1FCghGasgAWehIY3vJ36jUdQWkThA45j6ASsHwpfcIZG6trHbYQRppWRWcYF0koiQU26dsEpOjPQZeJmpEAyVLv2V7sX8SSAELlkWrdcJ8ZOyhQKLmGSbycaYsaHrA8tQ0MWgO6ks/Mn9MwoPepHylSIdKb+nkhZoPU48ExnwHCgF72p+J/XStC/6qQijBOEkM8X+YmkGNFpFrQnFHCUY0MYV8LcSvmAKcbRJJY3IbiLLy+T+kXRLRWv70uF8k0WR46ckFNyTlxyScrkjlRJjXCSkmfySt6sJ+vFerc+5q0rVjZzRP7A+vwBvE+WAQ==</latexit>Semi-implicitEuler<latexit sha1_base64="g0SzFtokwTBg2+DjEKkqyxe/8eE=">AAACA3icbVDLSgNBEJyNrxhfUW96GQyCF8OuBNRbUASPEc0DkhBmJ51kyMzuMtMrhiXgxV/x4kERr/6EN//GyeOgiQUNRVU33V1+JIVB1/12UguLS8sr6dXM2vrG5lZ2e6diwlhzKPNQhrrmMwNSBFBGgRJqkQamfAlVv3858qv3oI0IgzscRNBUrBuIjuAMrdTK7jUQHjC5BSWOhbIbuUB6FUvQw1Y25+bdMeg88aYkR6YotbJfjXbIYwUBcsmMqXtuhM2EaRRcwjDTiA1EjPdZF+qWBkyBaSbjH4b00Cpt2gm1rQDpWP09kTBlzED5tlMx7JlZbyT+59Vj7Jw1ExFEMULAJ4s6saQY0lEgtC00cJQDSxjXwt5KeY9pxtHGlrEheLMvz5PKSd4r5M9vCrnixTSONNknB+SIeOSUFMk1KZEy4eSRPJNX8uY8OS/Ou/MxaU0505ld8gfO5w9n7pgH</latexit>NAG-ODE-HR<latexit sha1_base64="bdEzHVDYMF7Tg1tDYOsqrJ8XDqg=">AAAB+nicbVDLTsJAFJ36RHwVXbppJCZuIK0hUXf4iqwUjTwSaMh0GGDCdNrM3Kqk8iluXGiMW7/EnX/jAF0oeJKbnJxzb+69xws5U2Db38bc/MLi0nJqJb26tr6xaWa2qiqIJKEVEvBA1j2sKGeCVoABp/VQUux7nNa8/tnIr91TqVgg7mAQUtfHXcE6jGDQUsvMNIE+Qnx1cpm7Pr/IlW6HLTNr5+0xrFniJCSLEpRb5lezHZDIpwIIx0o1HDsEN8YSGOF0mG5GioaY9HGXNjQV2KfKjcenD609rbStTiB1CbDG6u+JGPtKDXxPd/oYemraG4n/eY0IOkduzEQYARVksqgTcQsCa5SD1WaSEuADTTCRTN9qkR6WmIBOK61DcKZfniXVg7xTyB/fFLLF0ySOFNpBu2gfOegQFVEJlVEFEfSAntErejOejBfj3fiYtM4Zycw2+gPj8wfcw5Mc</latexit>(NAGwithoptimalparameters)<latexit sha1_base64="m+DzVgU7u1S766ygJR1nuJRgOoE=">AAACDXicbVC7SgNBFJ31GeNr1dJmMAqxCbsSULuohVYSwTwgWcLsZDYZMvtg5q4alvyAjb9iY6GIrb2df+NssoUmHhg4nHMPd+5xI8EVWNa3MTe/sLi0nFvJr66tb2yaW9t1FcaSshoNRSibLlFM8IDVgINgzUgy4ruCNdzBReo37phUPAxuYRgxxye9gHucEtBSx9xvA3uApHh9donvOfRxGAH3icARkcRnoKOHo45ZsErWGHiW2BkpoAzVjvnV7oY09lkAVBClWrYVgZMQCZwKNsq3Y8UiQgekx1qaBnqTcpLxNSN8oJUu9kKpXwB4rP5OJMRXaui7etIn0FfTXir+57Vi8E6chAdRDCygk0VeLDCEOK0Gd7lkFMRQE0Il13/FtK9roGkLeV2CPX3yLKkflexy6fSmXKicZ3Xk0C7aQ0Vko2NUQVeoimqIokf0jF7Rm/FkvBjvxsdkdM7IMjvoD4zPHyePm6U=</latexit>Acceleratedoptimizer<latexit sha1_base64="UZoNcBwee1GTco86w6EK5VvFI74=">AAACBXicbVA9SwNBEN3zM8avU0stFoNgFe4koHZRG8sI5gOSI+xtJsmSvQ9258R4pLHxr9hYKGLrf7Dz37hJrtDEBwOP92aYmefHUmh0nG9rYXFpeWU1t5Zf39jc2rZ3dms6ShSHKo9kpBo+0yBFCFUUKKERK2CBL6HuD67Gfv0OlBZReIvDGLyA9ULRFZyhkdr2QQvhHtMLzkGCYggdGsUoAvEAatS2C07RmYDOEzcjBZKh0ra/Wp2IJwGEyCXTuuk6MXopUyi4hFG+lWiIGR+wHjQNDVkA2ksnX4zokVE6tBspUyHSifp7ImWB1sPAN50Bw76e9cbif14zwe6Zl4owThBCPl3UTSTFiI4joR2hgKMcGsK4EuZWyvtMMY4muLwJwZ19eZ7UTopuqXh+UyqUL7M4cmSfHJJj4pJTUibXpEKqhJNH8kxeyZv1ZL1Y79bHtHXBymb2yB9Ynz948Jk4</latexit>(NAGwithoptimalparameters)<latexit sha1_base64="m+DzVgU7u1S766ygJR1nuJRgOoE=">AAACDXicbVC7SgNBFJ31GeNr1dJmMAqxCbsSULuohVYSwTwgWcLsZDYZMvtg5q4alvyAjb9iY6GIrb2df+NssoUmHhg4nHMPd+5xI8EVWNa3MTe/sLi0nFvJr66tb2yaW9t1FcaSshoNRSibLlFM8IDVgINgzUgy4ruCNdzBReo37phUPAxuYRgxxye9gHucEtBSx9xvA3uApHh9donvOfRxGAH3icARkcRnoKOHo45ZsErWGHiW2BkpoAzVjvnV7oY09lkAVBClWrYVgZMQCZwKNsq3Y8UiQgekx1qaBnqTcpLxNSN8oJUu9kKpXwB4rP5OJMRXaui7etIn0FfTXir+57Vi8E6chAdRDCygk0VeLDCEOK0Gd7lkFMRQE0Il13/FtK9roGkLeV2CPX3yLKkflexy6fSmXKicZ3Xk0C7aQ0Vko2NUQVeoimqIokf0jF7Rm/FkvBjvxsdkdM7IMjvoD4zPHyePm6U=</latexit>Acceleratedoptimizer<latexit sha1_base64="UZoNcBwee1GTco86w6EK5VvFI74=">AAACBXicbVA9SwNBEN3zM8avU0stFoNgFe4koHZRG8sI5gOSI+xtJsmSvQ9258R4pLHxr9hYKGLrf7Dz37hJrtDEBwOP92aYmefHUmh0nG9rYXFpeWU1t5Zf39jc2rZ3dms6ShSHKo9kpBo+0yBFCFUUKKERK2CBL6HuD67Gfv0OlBZReIvDGLyA9ULRFZyhkdr2QQvhHtMLzkGCYggdGsUoAvEAatS2C07RmYDOEzcjBZKh0ra/Wp2IJwGEyCXTuuk6MXopUyi4hFG+lWiIGR+wHjQNDVkA2ksnX4zokVE6tBspUyHSifp7ImWB1sPAN50Bw76e9cbif14zwe6Zl4owThBCPl3UTSTFiI4joR2hgKMcGsK4EuZWyvtMMY4muLwJwZ19eZ7UTopuqXh+UyqUL7M4cmSfHJJj4pJTUibXpEKqhJNH8kxeyZv1ZL1Y79bHtHXBymb2yB9Ynz948Jk4</latexit>(becauseonlystableforsmallstepsizes)<latexit sha1_base64="B8RKLXyEtoMiiwWHmKajfIQ7Fp4=">AAACGXicbVC7TgMxEPTxJrwClDQWERI00R1CAjoEDSVIJCAlUbTn7IGFzz7Ze4hwym/Q8Cs0FCBECRV/g/MoeE01mtnR7k6cKekoDD+DsfGJyanpmdnS3PzC4lJ5eaXuTG4F1oRRxl7E4FBJjTWSpPAiswhprPA8vj7q++c3aJ00+oy6GbZSuNQykQLIS+1y2CS8pWIzRgG5Q2606nJH4PM8MZa7FJTyAmZO3qHb6rXLlbAaDsD/kmhEKmyEk3b5vdkxIk9Rk1DgXCMKM2oVYEkKhb1S06/NQFzDJTY81ZCiaxWDz3p8wyudwSGJ0cQH6vdEAalz3TT2kynQlfvt9cX/vEZOyV6rkDrLCbUYLkpyxcnwfk28Iy0K8l10JAgr/a1cXIEFQb7Mki8h+v3yX1LfrkY71f3TncrB4aiOGbbG1tkmi9guO2DH7ITVmGD37JE9s5fgIXgKXoO34ehYMMqssh8IPr4AGFKhAA==</latexit>Slowoptimizer<latexit sha1_base64="TNAH39/hd+2jOl1DDnUD91nWYXk=">AAAB/nicbVDLSgNBEJz1GeMrKp68DAbBU9iVgHoLevEY0TwgCWF20kmGzO4sM71qXAL+ihcPinj1O7z5N06SPWhiQUNR1U13lx9JYdB1v52FxaXlldXMWnZ9Y3NrO7ezWzUq1hwqXEml6z4zIEUIFRQooR5pYIEvoeYPLsd+7Q60ESq8xWEErYD1QtEVnKGV2rn9JsIDJjdS3VMVoQjEI+hRO5d3C+4EdJ54KcmTFOV27qvZUTwOIEQumTENz42wlTCNgksYZZuxgYjxAetBw9KQBWBayeT8ET2ySod2lbYVIp2ovycSFhgzDHzbGTDsm1lvLP7nNWLsnrUSEUYxQsini7qxpKjoOAvaERo4yqEljGthb6W8zzTjaBPL2hC82ZfnSfWk4BUL59fFfOkijSNDDsghOSYeOSUlckXKpEI4ScgzeSVvzpPz4rw7H9PWBSed2SN/4Hz+ACvZlko=</latexit>(coincideswithNAG-ODE-HR)<latexit sha1_base64="g85SeguuMTANRdqXQG+/rs7BPhU=">AAACC3icbVDLSgNBEJz1GeNr1aOXwSDEg2FXBPUWX+jJF0aFJITZSScZnJ1dZnrVsOTuxV/x4kERr/6AN//GScxBowUNRVU33V1BLIVBz/t0hoZHRsfGMxPZyanpmVl3bv7CRInmUOKRjPRVwAxIoaCEAiVcxRpYGEi4DK53u/7lDWgjInWO7RiqIWsq0RCcoZVq7lIF4Q7TPI+E4qIOht4KbNGj7YPV47391cOzlU7NzXkFrwf6l/h9kiN9nNTcj0o94kkICrlkxpR9L8ZqyjQKLqGTrSQGYsavWRPKlioWgqmmvV86dNkqddqItC2FtKf+nEhZaEw7DGxnyLBlBr2u+J9XTrCxWU2FihMExb8XNRJJMaLdYGhdaOAo25YwroW9lfIW04yjjS9rQ/AHX/5LLtYK/nph63Q9V9zpx5Ehi2SJ5IlPNkiRHJITUiKc3JNH8kxenAfnyXl13r5bh5z+zAL5Bef9Cww6mcQ=</latexit>Semi-implicitEuler<latexit sha1_base64="g0SzFtokwTBg2+DjEKkqyxe/8eE=">AAACA3icbVDLSgNBEJyNrxhfUW96GQyCF8OuBNRbUASPEc0DkhBmJ51kyMzuMtMrhiXgxV/x4kERr/6EN//GyeOgiQUNRVU33V1+JIVB1/12UguLS8sr6dXM2vrG5lZ2e6diwlhzKPNQhrrmMwNSBFBGgRJqkQamfAlVv3858qv3oI0IgzscRNBUrBuIjuAMrdTK7jUQHjC5BSWOhbIbuUB6FUvQw1Y25+bdMeg88aYkR6YotbJfjXbIYwUBcsmMqXtuhM2EaRRcwjDTiA1EjPdZF+qWBkyBaSbjH4b00Cpt2gm1rQDpWP09kTBlzED5tlMx7JlZbyT+59Vj7Jw1ExFEMULAJ4s6saQY0lEgtC00cJQDSxjXwt5KeY9pxtHGlrEheLMvz5PKSd4r5M9vCrnixTSONNknB+SIeOSUFMk1KZEy4eSRPJNX8uY8OS/Ou/MxaU0505ld8gfO5w9n7pgH</latexit>ExplicitEuler<latexit sha1_base64="SIrn/RuA1tsOIfpVfyDLM4St5+w=">AAAB/nicbVDLSgNBEJz1GeNrVTx5GQyCp7ArAfUWlIDHCOYBSQizk95kyOyDmV5JWAL+ihcPinj1O7z5N06SPWhiQUNR1U13lxdLodFxvq2V1bX1jc3cVn57Z3dv3z44rOsoURxqPJKRanpMgxQh1FCghGasgAWehIY3vJ36jUdQWkThA45j6ASsHwpfcIZG6trHbYQRppWRWcYF0koiQU26dsEpOjPQZeJmpEAyVLv2V7sX8SSAELlkWrdcJ8ZOyhQKLmGSbycaYsaHrA8tQ0MWgO6ks/Mn9MwoPepHylSIdKb+nkhZoPU48ExnwHCgF72p+J/XStC/6qQijBOEkM8X+YmkGNFpFrQnFHCUY0MYV8LcSvmAKcbRJJY3IbiLLy+T+kXRLRWv70uF8k0WR46ckFNyTlxyScrkjlRJjXCSkmfySt6sJ+vFerc+5q0rVjZzRP7A+vwBvE+WAQ==</latexit>Zhang, Orvieto, Daneshmand, Hofmann, Smith

a (Strang/Lie-Trotter) splitting scheme for structure-
preserving integration of conformal Hamiltonian sys-
tems (McLachlan and Perlmutter, 2001; McLachlan and
Quispel, 2002) or (2) the composition of a map derived
from a contact Hamiltonian (de León and Lainz Val-
cázar, 2019; Bravetti et al., 2017) and a gradient descent
step. Finally, the application of Runge-Kutta schemes
was explored (Zhang et al., 2018, 2019; Sanz Serna
and Zygalakis, 2020); in particular, Zhang et al. (2018)
ﬁrst showed that fast rates can be also achieved via
high-order explicit methods.

To sum it up, to the best of our knowledge, all recent
convex optimization literature advocates that, in order
to achieve acceleration from an ODE model, one needs
to use either structure-preserving integrators (Bravetti
et al., 2019; Shi et al., 2018; França et al., 2020a), high-
order explicit methods (Zhang et al., 2018, 2019), or
implicit methods (Shi et al., 2019; Wilson et al., 2016;
Diakonikolas and Orecchia, 2017).

Our contribution. We show that, contrary to what
is often claimed (or hinted at) in recent literature (see
paragraph above) acceleration can also be achieved
by means of simple low-order explicit numerical inte-
grators — such as the explicit Euler method. While
explicit Euler is well-known to be provably subopti-
mal for accurate integration of Hamiltonian systems
(Hairer et al., 2006; Hairer, 1994; Benettin and Giorgilli,
1994), we show that this does not necessarily imply
slow convergence of the resulting optimizer. In partic-
ular, our work suggests that the structure-preserving
properties of semi-implicit (symplectic) methods are
not a necessary component of accelerated algorithms.

We start by introducing a generalized momentum ODE
(GM-ODE), dependent on three parameters, which re-
covers both NAG-ODE and NAG-ODE-HR as special
cases. In Sec. 3 we study the convergence of this ODE.
Next, in Sec. 4 we show that both the explicit and the
semi-implicit Euler methods, when applied for numeri-
cal integration of GM-ODE, can achieve an accelerated
rate. Finally, in Sec. 5, we go one step further and show
that there exist damped gradient systems for which
the semi-implicit Euler method is unstable, while the
explicit Euler method (with the same step-size) is sta-
ble. Of course, for other ODE systems, we observe the
opposite behavior. This showcases that the stability of
the integrator depends on the underlying accelerated
ODE.
At its core, our work showcases some unintuitive aspects
of the connection between the ﬁelds of numerical inte-
gration and optimization. Namely, while for accurate
integration of physical systems symplectic integrators
are provably superior to explicit methods (Hairer et al.,
2006; Benettin and Giorgilli, 1994), we show that the
same ranking might not hold when seeking fast op-

Figure 2: The main conceptual ﬁnding of our paper: both
the semi-implicit and the explicit Euler integrators are
able to recover Nesterov’s method, when used to discretize
diﬀerent accelerated ﬂows (ODEs are given in Sec. 4).

timizers through ODE discretization. We think that
clarifying this critical point makes the (already vast) lit-
erature on this topic richer, motivating future research
on the connections between the ﬁelds of optimization
and numerical analysis.

2 Summary of the results

Our work is based on the study of a novel continuous-
time model for momentum algorithms, namely the
following ordinary diﬀerential equation that is indexed
by non-negative parameters m, n, q:
(cid:40) ˙X = −m∇f (X) − nV
˙V = ∇f (X) − qV,

(GM-ODE)

We show that the above ODE includes both NAG-
ODE and NAG-ODE-HR as special cases and recovers
a large set of momentum methods through the appli-
cation of two classical numerical integrators, i.e. semi-
implicit and explicit Euler. In Lemma 2 we show that
these integrators are equivalent and can both lead to
acceleration (see also Fig. 2). Equivalence is shown
by reparameterization: any semi-implicit discretiza-
tion of GM-ODE with parameters (mSIE, nSIE, qSIE)
can be viewed as explicit Euler discretization of GM-
ODE with diﬀerent parameters (mEE, nEE, qEE). These
parameters can be computed in closed form starting
from (mSIE, nSIE, qSIE) (see Lemma 2 for precise formu-
las). Such an equivalence suggests that the energy-
preservation properties of semi-implicit integration
might no be strictly necessary to achieve acceleration,
as instead hinted in recent works (Bravetti et al., 2017;
McLachlan and Perlmutter, 2001; Shi et al., 2019).

To make our analysis complete, we establish an acceler-
ated convergence rate (in Thm. 3 and Cor. 6) for a set
of algorithms that can be interpreted as (both the two)
Euler discretizations of GM-ODE with a proper param-
eters choice. As a side product of our novel analysis of
semi-implicit and explicit methods, we derive a novel
accelerated convergence rate for the quasi-hyperbolic
momentum (QHM) method introduced by Ma and
Yarats (2018). Indeed, along with HB and NAG, the

Acceleratedﬂow1<latexit sha1_base64="K93uaxogEdXw7evigAefzB+8vL8=">AAACAnicbVDLSgNBEJz1GeNr1ZN4GQyCp7ArAfUW9eIxgnlAsoTZSScZMvtgplcNS/Dir3jxoIhXv8Kbf+Mk2YMmFjQUVd10d/mxFBod59taWFxaXlnNreXXNza3tu2d3ZqOEsWhyiMZqYbPNEgRQhUFSmjECljgS6j7g6uxX78DpUUU3uIwBi9gvVB0BWdopLa930J4wPSCc5CgGEKHdmV0T91R2y44RWcCOk/cjBRIhkrb/mp1Ip4EECKXTOum68TopUyh4BJG+VaiIWZ8wHrQNDRkAWgvnbwwokdGMasjZSpEOlF/T6Qs0HoY+KYzYNjXs95Y/M9rJtg981IRxglCyKeLuomkGNFxHrQjFHCUQ0MYV8LcSnmfKcbRpJY3IbizL8+T2knRLRXPb0qF8mUWR44ckENyTFxySsrkmlRIlXDySJ7JK3mznqwX6936mLYuWNnMHvkD6/MHGjuXQA==</latexit>inTb.1,ﬁrstcolumn<latexit sha1_base64="ZhBvwTsKsHkow2BzIz+3VFBhP9k=">AAACBnicbVDLSsNAFJ34rPUVdSnCYBFcSEikoO6KblxW6AvaUCbTSTt0MgkzN2IJXbnxV9y4UMSt3+DOv3HaZqGtBwYO59x7594TJIJrcN1va2l5ZXVtvbBR3Nza3tm19/YbOk4VZXUai1i1AqKZ4JLVgYNgrUQxEgWCNYPhzcRv3jOleSxrMEqYH5G+5CGnBIzUtY86wB4g4xLXAsc7wyFXGrCZm0ZyjLt2yXXcKfAi8XJSQjmqXfur04tpGjEJVBCt256bgJ8RBZwKNi52Us0SQoekz9qGShIx7WfTM8b4xCg9HMbKPDldIla/OzISaT2KAlMZERjoeW8i/ue1UwgvfXNjkgKTdPZRmAoMMZ5kgntcMQpiZAihiptdMR0QRSiY5IomBG/+5EXSOHe8snN1Vy5VrvM4CugQHaNT5KELVEG3qIrqiKJH9Ixe0Zv1ZL1Y79bHrHTJynsO0B9Ynz8RnZhA</latexit>GM-ODEwithm,n,qas<latexit sha1_base64="LsCb8aMCgzhFIY231fzaxZiMnuw=">AAACDnicbVDJSgNBEO1xjXGLevTSGAIeYpiRgHoLLuhFjGAWSELo6fQkTXp6xu4aNQz5Ai/+ihcPinj17M2/sbMcNPFBweO9KqrquaHgGmz725qZnZtfWEwsJZdXVtfWUxubZR1EirISDUSgqi7RTHDJSsBBsGqoGPFdwSpu92TgV+6Y0jyQN9ALWcMnbck9TgkYqZnK1IE9QHx+uXd1eobvOXRwH/tZLLP4dmRhovvNVNrO2UPgaeKMSRqNUWymvuqtgEY+k0AF0brm2CE0YqKAU8H6yXqkWUhol7RZzVBJfKYb8fCdPs4YpYW9QJmSgIfq74mY+Fr3fNd0+gQ6etIbiP95tQi8w0bMZRgBk3S0yIsEhgAPssEtrhgF0TOEUMXNrZh2iCIUTIJJE4Iz+fI0Ke/nnHzu6DqfLhyP40igbbSDdpGDDlABXaAiKiGKHtEzekVv1pP1Yr1bH6PWGWs8s4X+wPr8ARoHmug=</latexit>GM-ODEwithm,n,qas<latexit sha1_base64="LsCb8aMCgzhFIY231fzaxZiMnuw=">AAACDnicbVDJSgNBEO1xjXGLevTSGAIeYpiRgHoLLuhFjGAWSELo6fQkTXp6xu4aNQz5Ai/+ihcPinj17M2/sbMcNPFBweO9KqrquaHgGmz725qZnZtfWEwsJZdXVtfWUxubZR1EirISDUSgqi7RTHDJSsBBsGqoGPFdwSpu92TgV+6Y0jyQN9ALWcMnbck9TgkYqZnK1IE9QHx+uXd1eobvOXRwH/tZLLP4dmRhovvNVNrO2UPgaeKMSRqNUWymvuqtgEY+k0AF0brm2CE0YqKAU8H6yXqkWUhol7RZzVBJfKYb8fCdPs4YpYW9QJmSgIfq74mY+Fr3fNd0+gQ6etIbiP95tQi8w0bMZRgBk3S0yIsEhgAPssEtrhgF0TOEUMXNrZh2iCIUTIJJE4Iz+fI0Ke/nnHzu6DqfLhyP40igbbSDdpGDDlABXaAiKiGKHtEzekVv1pP1Yr1bH6PWGWs8s4X+wPr8ARoHmug=</latexit>inTb.1,secondcolumn<latexit sha1_base64="O0zE6PObAtTlVsm3Zn7pUxK0KYw=">AAACB3icbVDLSgMxFM34rPVVdSlIsAgupMxIQd0V3bis0Be0Q8mkt21oJjMkd8RSunPjr7hxoYhbf8Gdf2PazkJbDwQO59yb5JwglsKg6347S8srq2vrmY3s5tb2zm5ub79mokRzqPJIRroRMANSKKiiQAmNWAMLAwn1YHAz8ev3oI2IVAWHMfgh6ynRFZyhldq5oxbCA46EopWg4J1RAzxSHWovTkI1pu1c3i24U9BF4qUkT1KU27mvVifiSQgKuWTGND03Rn/ENAouYZxtJQZixgesB01LFQvB+KNpjjE9sUqHdiNtj0I6VX9vjFhozDAM7GTIsG/mvYn4n9dMsHvp25BxgqD47KFuIilGdFIK7QgNHOXQEsa1sH+lvM8042iry9oSvPnIi6R2XvCKhau7Yr50ndaRIYfkmJwSj1yQErklZVIlnDySZ/JK3pwn58V5dz5mo0tOunNA/sD5/AHALpie</latexit>Acceleratedﬂow2<latexit sha1_base64="8zyO5a08hpel8DGC2x4t6hIOZ6w=">AAACAnicbVDJSgNBEO2JW4xb1JN4aQyCpzATAuot6sVjBLNAMoSeTk3SpGehu0YNQ/Dir3jxoIhXv8Kbf2NnOWjig4LHe1VU1fNiKTTa9reVWVpeWV3Lruc2Nre2d/K7e3UdJYpDjUcyUk2PaZAihBoKlNCMFbDAk9DwBldjv3EHSosovMVhDG7AeqHwBWdopE7+oI3wgOkF5yBBMYQu9WV0T0ujTr5gF+0J6CJxZqRAZqh28l/tbsSTAELkkmndcuwY3ZQpFFzCKNdONMSMD1gPWoaGLADtppMXRvTYKGZ1pEyFSCfq74mUBVoPA890Bgz7et4bi/95rQT9MzcVYZwghHy6yE8kxYiO86BdoYCjHBrCuBLmVsr7TDGOJrWcCcGZf3mR1EtFp1w8vykXKpezOLLkkByRE+KQU1Ih16RKaoSTR/JMXsmb9WS9WO/Wx7Q1Y81m9skfWJ8/G8CXQQ==</latexit>ExplicitEuler<latexit sha1_base64="SIrn/RuA1tsOIfpVfyDLM4St5+w=">AAAB/nicbVDLSgNBEJz1GeNrVTx5GQyCp7ArAfUWlIDHCOYBSQizk95kyOyDmV5JWAL+ihcPinj1O7z5N06SPWhiQUNR1U13lxdLodFxvq2V1bX1jc3cVn57Z3dv3z44rOsoURxqPJKRanpMgxQh1FCghGasgAWehIY3vJ36jUdQWkThA45j6ASsHwpfcIZG6trHbYQRppWRWcYF0koiQU26dsEpOjPQZeJmpEAyVLv2V7sX8SSAELlkWrdcJ8ZOyhQKLmGSbycaYsaHrA8tQ0MWgO6ks/Mn9MwoPepHylSIdKb+nkhZoPU48ExnwHCgF72p+J/XStC/6qQijBOEkM8X+YmkGNFpFrQnFHCUY0MYV8LcSvmAKcbRJJY3IbiLLy+T+kXRLRWv70uF8k0WR46ckFNyTlxyScrkjlRJjXCSkmfySt6sJ+vFerc+5q0rVjZzRP7A+vwBvE+WAQ==</latexit>Semi-implicitEuler<latexit sha1_base64="g0SzFtokwTBg2+DjEKkqyxe/8eE=">AAACA3icbVDLSgNBEJyNrxhfUW96GQyCF8OuBNRbUASPEc0DkhBmJ51kyMzuMtMrhiXgxV/x4kERr/6EN//GyeOgiQUNRVU33V1+JIVB1/12UguLS8sr6dXM2vrG5lZ2e6diwlhzKPNQhrrmMwNSBFBGgRJqkQamfAlVv3858qv3oI0IgzscRNBUrBuIjuAMrdTK7jUQHjC5BSWOhbIbuUB6FUvQw1Y25+bdMeg88aYkR6YotbJfjXbIYwUBcsmMqXtuhM2EaRRcwjDTiA1EjPdZF+qWBkyBaSbjH4b00Cpt2gm1rQDpWP09kTBlzED5tlMx7JlZbyT+59Vj7Jw1ExFEMULAJ4s6saQY0lEgtC00cJQDSxjXwt5KeY9pxtHGlrEheLMvz5PKSd4r5M9vCrnixTSONNknB+SIeOSUFMk1KZEy4eSRPJNX8uY8OS/Ou/MxaU0505ld8gfO5w9n7pgH</latexit>NAG-ODE-HR<latexit sha1_base64="bdEzHVDYMF7Tg1tDYOsqrJ8XDqg=">AAAB+nicbVDLTsJAFJ36RHwVXbppJCZuIK0hUXf4iqwUjTwSaMh0GGDCdNrM3Kqk8iluXGiMW7/EnX/jAF0oeJKbnJxzb+69xws5U2Db38bc/MLi0nJqJb26tr6xaWa2qiqIJKEVEvBA1j2sKGeCVoABp/VQUux7nNa8/tnIr91TqVgg7mAQUtfHXcE6jGDQUsvMNIE+Qnx1cpm7Pr/IlW6HLTNr5+0xrFniJCSLEpRb5lezHZDIpwIIx0o1HDsEN8YSGOF0mG5GioaY9HGXNjQV2KfKjcenD609rbStTiB1CbDG6u+JGPtKDXxPd/oYemraG4n/eY0IOkduzEQYARVksqgTcQsCa5SD1WaSEuADTTCRTN9qkR6WmIBOK61DcKZfniXVg7xTyB/fFLLF0ySOFNpBu2gfOegQFVEJlVEFEfSAntErejOejBfj3fiYtM4Zycw2+gPj8wfcw5Mc</latexit>(NAGwithoptimalparameters)<latexit sha1_base64="m+DzVgU7u1S766ygJR1nuJRgOoE=">AAACDXicbVC7SgNBFJ31GeNr1dJmMAqxCbsSULuohVYSwTwgWcLsZDYZMvtg5q4alvyAjb9iY6GIrb2df+NssoUmHhg4nHMPd+5xI8EVWNa3MTe/sLi0nFvJr66tb2yaW9t1FcaSshoNRSibLlFM8IDVgINgzUgy4ruCNdzBReo37phUPAxuYRgxxye9gHucEtBSx9xvA3uApHh9donvOfRxGAH3icARkcRnoKOHo45ZsErWGHiW2BkpoAzVjvnV7oY09lkAVBClWrYVgZMQCZwKNsq3Y8UiQgekx1qaBnqTcpLxNSN8oJUu9kKpXwB4rP5OJMRXaui7etIn0FfTXir+57Vi8E6chAdRDCygk0VeLDCEOK0Gd7lkFMRQE0Il13/FtK9roGkLeV2CPX3yLKkflexy6fSmXKicZ3Xk0C7aQ0Vko2NUQVeoimqIokf0jF7Rm/FkvBjvxsdkdM7IMjvoD4zPHyePm6U=</latexit>Acceleratedoptimizer<latexit sha1_base64="UZoNcBwee1GTco86w6EK5VvFI74=">AAACBXicbVA9SwNBEN3zM8avU0stFoNgFe4koHZRG8sI5gOSI+xtJsmSvQ9258R4pLHxr9hYKGLrf7Dz37hJrtDEBwOP92aYmefHUmh0nG9rYXFpeWU1t5Zf39jc2rZ3dms6ShSHKo9kpBo+0yBFCFUUKKERK2CBL6HuD67Gfv0OlBZReIvDGLyA9ULRFZyhkdr2QQvhHtMLzkGCYggdGsUoAvEAatS2C07RmYDOEzcjBZKh0ra/Wp2IJwGEyCXTuuk6MXopUyi4hFG+lWiIGR+wHjQNDVkA2ksnX4zokVE6tBspUyHSifp7ImWB1sPAN50Bw76e9cbif14zwe6Zl4owThBCPl3UTSTFiI4joR2hgKMcGsK4EuZWyvtMMY4muLwJwZ19eZ7UTopuqXh+UyqUL7M4cmSfHJJj4pJTUibXpEKqhJNH8kxeyZv1ZL1Y79bHtHXBymb2yB9Ynz948Jk4</latexit>(NAGwithoptimalparameters)<latexit sha1_base64="m+DzVgU7u1S766ygJR1nuJRgOoE=">AAACDXicbVC7SgNBFJ31GeNr1dJmMAqxCbsSULuohVYSwTwgWcLsZDYZMvtg5q4alvyAjb9iY6GIrb2df+NssoUmHhg4nHMPd+5xI8EVWNa3MTe/sLi0nFvJr66tb2yaW9t1FcaSshoNRSibLlFM8IDVgINgzUgy4ruCNdzBReo37phUPAxuYRgxxye9gHucEtBSx9xvA3uApHh9donvOfRxGAH3icARkcRnoKOHo45ZsErWGHiW2BkpoAzVjvnV7oY09lkAVBClWrYVgZMQCZwKNsq3Y8UiQgekx1qaBnqTcpLxNSN8oJUu9kKpXwB4rP5OJMRXaui7etIn0FfTXir+57Vi8E6chAdRDCygk0VeLDCEOK0Gd7lkFMRQE0Il13/FtK9roGkLeV2CPX3yLKkflexy6fSmXKicZ3Xk0C7aQ0Vko2NUQVeoimqIokf0jF7Rm/FkvBjvxsdkdM7IMjvoD4zPHyePm6U=</latexit>Acceleratedoptimizer<latexit sha1_base64="UZoNcBwee1GTco86w6EK5VvFI74=">AAACBXicbVA9SwNBEN3zM8avU0stFoNgFe4koHZRG8sI5gOSI+xtJsmSvQ9258R4pLHxr9hYKGLrf7Dz37hJrtDEBwOP92aYmefHUmh0nG9rYXFpeWU1t5Zf39jc2rZ3dms6ShSHKo9kpBo+0yBFCFUUKKERK2CBL6HuD67Gfv0OlBZReIvDGLyA9ULRFZyhkdr2QQvhHtMLzkGCYggdGsUoAvEAatS2C07RmYDOEzcjBZKh0ra/Wp2IJwGEyCXTuuk6MXopUyi4hFG+lWiIGR+wHjQNDVkA2ksnX4zokVE6tBspUyHSifp7ImWB1sPAN50Bw76e9cbif14zwe6Zl4owThBCPl3UTSTFiI4joR2hgKMcGsK4EuZWyvtMMY4muLwJwZ19eZ7UTopuqXh+UyqUL7M4cmSfHJJj4pJTUibXpEKqhJNH8kxeyZv1ZL1Y79bHtHXBymb2yB9Ynz948Jk4</latexit>(becauseonlystableforsmallstepsizes)<latexit sha1_base64="B8RKLXyEtoMiiwWHmKajfIQ7Fp4=">AAACGXicbVC7TgMxEPTxJrwClDQWERI00R1CAjoEDSVIJCAlUbTn7IGFzz7Ze4hwym/Q8Cs0FCBECRV/g/MoeE01mtnR7k6cKekoDD+DsfGJyanpmdnS3PzC4lJ5eaXuTG4F1oRRxl7E4FBJjTWSpPAiswhprPA8vj7q++c3aJ00+oy6GbZSuNQykQLIS+1y2CS8pWIzRgG5Q2606nJH4PM8MZa7FJTyAmZO3qHb6rXLlbAaDsD/kmhEKmyEk3b5vdkxIk9Rk1DgXCMKM2oVYEkKhb1S06/NQFzDJTY81ZCiaxWDz3p8wyudwSGJ0cQH6vdEAalz3TT2kynQlfvt9cX/vEZOyV6rkDrLCbUYLkpyxcnwfk28Iy0K8l10JAgr/a1cXIEFQb7Mki8h+v3yX1LfrkY71f3TncrB4aiOGbbG1tkmi9guO2DH7ITVmGD37JE9s5fgIXgKXoO34ehYMMqssh8IPr4AGFKhAA==</latexit>Slowoptimizer<latexit sha1_base64="TNAH39/hd+2jOl1DDnUD91nWYXk=">AAAB/nicbVDLSgNBEJz1GeMrKp68DAbBU9iVgHoLevEY0TwgCWF20kmGzO4sM71qXAL+ihcPinj1O7z5N06SPWhiQUNR1U13lx9JYdB1v52FxaXlldXMWnZ9Y3NrO7ezWzUq1hwqXEml6z4zIEUIFRQooR5pYIEvoeYPLsd+7Q60ESq8xWEErYD1QtEVnKGV2rn9JsIDJjdS3VMVoQjEI+hRO5d3C+4EdJ54KcmTFOV27qvZUTwOIEQumTENz42wlTCNgksYZZuxgYjxAetBw9KQBWBayeT8ET2ySod2lbYVIp2ovycSFhgzDHzbGTDsm1lvLP7nNWLsnrUSEUYxQsini7qxpKjoOAvaERo4yqEljGthb6W8zzTjaBPL2hC82ZfnSfWk4BUL59fFfOkijSNDDsghOSYeOSUlckXKpEI4ScgzeSVvzpPz4rw7H9PWBSed2SN/4Hz+ACvZlko=</latexit>(coincideswithNAG-ODE-HR)<latexit sha1_base64="g85SeguuMTANRdqXQG+/rs7BPhU=">AAACC3icbVDLSgNBEJz1GeNr1aOXwSDEg2FXBPUWX+jJF0aFJITZSScZnJ1dZnrVsOTuxV/x4kERr/6AN//GScxBowUNRVU33V1BLIVBz/t0hoZHRsfGMxPZyanpmVl3bv7CRInmUOKRjPRVwAxIoaCEAiVcxRpYGEi4DK53u/7lDWgjInWO7RiqIWsq0RCcoZVq7lIF4Q7TPI+E4qIOht4KbNGj7YPV47391cOzlU7NzXkFrwf6l/h9kiN9nNTcj0o94kkICrlkxpR9L8ZqyjQKLqGTrSQGYsavWRPKlioWgqmmvV86dNkqddqItC2FtKf+nEhZaEw7DGxnyLBlBr2u+J9XTrCxWU2FihMExb8XNRJJMaLdYGhdaOAo25YwroW9lfIW04yjjS9rQ/AHX/5LLtYK/nph63Q9V9zpx5Ehi2SJ5IlPNkiRHJITUiKc3JNH8kxenAfnyXl13r5bh5z+zAL5Bef9Cww6mcQ=</latexit>Semi-implicitEuler<latexit sha1_base64="g0SzFtokwTBg2+DjEKkqyxe/8eE=">AAACA3icbVDLSgNBEJyNrxhfUW96GQyCF8OuBNRbUASPEc0DkhBmJ51kyMzuMtMrhiXgxV/x4kERr/6EN//GyeOgiQUNRVU33V1+JIVB1/12UguLS8sr6dXM2vrG5lZ2e6diwlhzKPNQhrrmMwNSBFBGgRJqkQamfAlVv3858qv3oI0IgzscRNBUrBuIjuAMrdTK7jUQHjC5BSWOhbIbuUB6FUvQw1Y25+bdMeg88aYkR6YotbJfjXbIYwUBcsmMqXtuhM2EaRRcwjDTiA1EjPdZF+qWBkyBaSbjH4b00Cpt2gm1rQDpWP09kTBlzED5tlMx7JlZbyT+59Vj7Jw1ExFEMULAJ4s6saQY0lEgtC00cJQDSxjXwt5KeY9pxtHGlrEheLMvz5PKSd4r5M9vCrnixTSONNknB+SIeOSUFMk1KZEy4eSRPJNX8uY8OS/Ou/MxaU0505ld8gfO5w9n7pgH</latexit>ExplicitEuler<latexit sha1_base64="SIrn/RuA1tsOIfpVfyDLM4St5+w=">AAAB/nicbVDLSgNBEJz1GeNrVTx5GQyCp7ArAfUWlIDHCOYBSQizk95kyOyDmV5JWAL+ihcPinj1O7z5N06SPWhiQUNR1U13lxdLodFxvq2V1bX1jc3cVn57Z3dv3z44rOsoURxqPJKRanpMgxQh1FCghGasgAWehIY3vJ36jUdQWkThA45j6ASsHwpfcIZG6trHbYQRppWRWcYF0koiQU26dsEpOjPQZeJmpEAyVLv2V7sX8SSAELlkWrdcJ8ZOyhQKLmGSbycaYsaHrA8tQ0MWgO6ks/Mn9MwoPepHylSIdKb+nkhZoPU48ExnwHCgF72p+J/XStC/6qQijBOEkM8X+YmkGNFpFrQnFHCUY0MYV8LcSvmAKcbRJJY3IbiLLy+T+kXRLRWv70uF8k0WR46ckFNyTlxyScrkjlRJjXCSkmfySt6sJ+vFerc+5q0rVjZzRP7A+vwBvE+WAQ==</latexit>Revisiting the Role of Symplectic Integration on Acceleration and Stability in Convex Optimization

QHM method can also be seen as a numerical integrator
on GM-ODE. QHM was shown to be very competitive
in deep learning tasks (Choi et al., 2019) as well as in
the strongly-convex setting (see Appendix J in (Ma and
Yarats, 2018)). However, to the best of our knowledge,
QHM has only been studied in the quadratic case (Git-
man et al., 2019) (hence the novelty of our rate). We
like to point out that this is not the main contribu-
tion of our paper, but is presented here nonetheless to
showcases the ﬂexibility of our novel ODE and of the
numerical integration approach.

Finally, we go beyond convergence analysis and study
the discretization errors in Sec. 5. Under some condi-
tions on the choice of parameters, we show that the
explicit Euler method enjoys the same integration error
as the semi-implicit Euler method when integrating
GM-ODE (see Lemma 8).

3 Continuous-time analysis

Before discussing numerical integration, we provide
here a continuous-time analysis of GM-ODE, in line
with most related works on acceleration and numerical
integration (Shi et al., 2018; Su et al., 2014). The
results in this section are not fundamental for the
understanding of our claims on the discretization of
GM-ODE. Hence, for a quick read, this section can be
safely skipped.

GM-ODE can be seen as as a linear combination of
the gradient ﬂow ˙X = −∇f (X) (obtained for n = 0)
and NAG-ODE (obtained for n = 1). Assuming the
objective function f is L-smooth, one can check that
GM-ODE admits a unique solution (follows directly
from Thm. 3.2 in Khalil and Grizzle (2002)). The
model above is inspired by the quasi-hyperbolic momen-
tum (QHM) algorithm9 developed in Ma and Yarats
(2018). We discuss the connection to QHM later in
Sec. 4.

Connections to existing ODE models. GM-
ODE recovers existing continuous momentum mod-
els under diﬀerent choices of parameters. To see
¨X =
this, let us take the second derivative of X:
−m∇2f (X) ˙X − n ˙V .

¨X + (cid:0)q + m∇2f (X)(cid:1) ˙X + (n + qm)∇f (X) = 0.

(1)

The choice10 m = 0, n = 1, q = 2
µ recovers NAG-
ODE by Polyak (1964). Moreover, the choice m =
√
µ recovers NAG-ODE-HR, pro-
posed by Shi et al. (2018, 2019). That is, GM-ODE

s, n = 1, q = 2

√

√

9QHM was introduced as weighted average of momentum
and gradient descent methods. It is shown to recover both
HB and NAG as special cases.

10Proofs for discretized GM-ODE will rely on condition

m > 0. This discussion will be elaborated in Sec. 4.

includes as special cases both the high-resolution and
low-resolution models of Nesterov’s method (see dis-
cussion in the introduction). We note that, contrary
to Shi et al. (2018), the Hessian of f is not explicitly
included in the model. Also, contrary to Muehlebach
and Jordan (2019), the gradient is evaluated only at
the current position X. This feature arguably gives
GM-ODE higher interpretability than existing models –
a simple linear combination of gradient and momentum
can also achieve high resolution11.

Gradient Flow
NAG-ODE (Su et al., 2014)
NAG-ODE-HR (Shi et al., 2019)

m n
1
0
1
0
√
1
s

q
any
√
µ
2
√
µ
2

Stability and convergence rate. The equilibria
of GM-ODE are easy to characterize: since m, n and
q are non-negative, we have ˙X = 0 and ˙V = 0 if and
only if both ∇f (X) = 0 and V = 0. Under the as-
sumption that f is strongly-convex, only its unique
minimizer x∗ is such that ∇f (x∗) = 0. Therefore the
point (x∗, 0) ∈ R2d is the only equilibrium of GM-ODE.
Next, we want to show that (x∗, 0) is asymptotically sta-
ble and characterize the convergence rate of our model.
Borrowing some inspiration from Su et al. (2014); Shi
et al. (2019), we propose the following energy function:

E(X, V ) = (qm + n)(cid:0)f (X) − f (x∗)(cid:1)

+

1
4

(cid:107)q(X − x∗) − nV (cid:107)2 +

n(qm + n)
4

(cid:107)V (cid:107)2.

The next theorem states our result about Lyapunov sta-
bility, of which the proof is presented in the appendix.

Theorem 1 (Continuous-time stability). Let f be µ-
strongly-convex and L-smooth. If n, m, q ≥ 0 then, for
any value of the strong-convexity modulus µ ≥ 0, the
point (x∗, 0) ∈ R2d is globally asymptotically stable
for GM-ODE, as

E(X(t), V (t)) ≤ e−γ1t · E(X(0), V (0)),

(2)

where γ1 := min

(cid:32)

µ(n + qm)
,
2q

q
2

(cid:33)
.

Remarkably, the stability analysis in the proof can
be used to guide the analysis of diﬀerent momentum
methods (see Sec. 4) — obtained by the application of
standard Euler integrators of our model.
Remark 1. The rate in Thm. 1 is not aﬀected by the
gradient Lipshitz constant L. This might look strange

11That is, a ﬁner, compared to the original ODE in Su
et al. (2014) approximation of Nesterov’s method. For a
detailed discussion on this terminology, we refer the reader
to Shi et al. (2018).

Zhang, Orvieto, Daneshmand, Hofmann, Smith

at ﬁrst for a reader familiar with the optimization
literature. However, we point to the fact that this is a
feature of most continuous-time models (see e.g. rates
in Shi et al. (2018)). The Lipschitz constant comes
back into the rate after discretization, since one has to
introduce a bound on the maximum integrator step-size,
usually proportional to 1/L (see Eq. 3 and 4).

How do m, n, q aﬀect the ODE dynamics? One
can readily check that Thm. 1 implies a linear rate in
function value of the form f (X(t)) − f (x∗) ≤ O(−eγ1t).
This result recovers exactly the rates in Shi et al. (2018)
as a special case. However, we note that our result
is more general and leads to novel insights on the in-
terplay between gradient ampliﬁcation (controlled by
n), momentum (controlled by q) and Hessian damp-
ing (controlled by m). Indeed, given the expression for
γ1 in Eq. 2, we can make the following conclusions.
• For ﬁxed m, n ≥ 0, the value of q which maximizes
γ1 also solves µ(n + qm)/q = q, which implies q =
4µn + m2)/2. If we restrict q to be a power of
(m+
µ, set n = 1 and ignore the eﬀect of m, then we get
the popular choice (Shi et al., 2018, 2019; Muehlebach
√
and Jordan, 2019) q = O(
µ) (see the ﬁrst panel of
Fig. 3). Sanz-Serna and Zygalakis (2020) recently
showed that this choice is optimal using the linear
matrix inequalities framework (Lessard et al., 2016;
Fazlyab et al., 2018).

(cid:112)

• For any q ≥ 0, if n ≥ 0 is chosen small enough such
that q2 −µn ≥ 0, then by picking m = (q2 −µn)/q we
have γ1 = q/2. Hence, by increasing q (and adapting
m accordingly) the convergence in continuous-time
can be sped-up arbitrarily (see the second panel of
Fig. 3).

• If n = q2/µ, then γ1 = q/2 for all q ≥ 0 and any
m ≥ 0. Again, by increasing q the convergence can
be sped-up arbitrarily (bottom panel of Fig. 3).

Remark 2. If n or m are increased, one can guarantee
arbitrarily fast convergence to the minimizer. This
result only holds true in continuous-time, as noted also
in a similar setting by Wilson et al. (2016). Indeed, as
we will see in Thm. 3, in the discrete word, to ensure
stability, n and m have to be bounded by a constant
which is inversely proportional to the discretization
accuracy.

4 Discretization and acceleration

We now jump to the discrete world and show how
both explicit and semi-implicit numerical integration,
applied to GM-ODE, can yield accelerated gradient
iterations.

Discretization schemes. We consider two well-
understood (Hairer et al., 2006) and practical ﬁrst-order

Figure 3: Role of parameters in GM-ODE. The objective
function is a 10-dimensional quadratic function f with
µI (cid:22) ∇2f (cid:22) LI where µ = 0.01 and L = 1. The panels
depict, from left to right, the inﬂuence of q, m and n, as
suggested in above discussion. In each ﬁgure we vary the
parameter we are interested in (as in the legends) and keep
the others ﬁxed. For left panel we use m = 0.2 and n = 1.
For the middle panel we use n = 0.1. For the right panel we
use m = 0.2. Numerical integration of GM-ODE performed
using a fourth-order Runge-Kutta with step-size 10−4.

numerical integration schemes applied to GM-ODE
s (see discussion in Su
with discretization step-size
et al. (2014); Shi et al. (2019)): Explicit Euler (EE)
and Semi-Implicit12 Euler (SIE).

√

(cid:40)

(cid:40)

(EE) :

(SIE) :

xk+1 − xk = −m
vk+1 − vk =

√

s∇f (xk) − n

√

s∇f (xk) − q

xk+1 − xk = −m
vk+1 − vk =

√

s∇f (xk) − n
√

svk

s∇f (xk+1) − q

svk.

√

svk

svk.
√

√

√

Even though the second equation in SIE is written in
an implicit way, it can be trivially solved: indeed, one
shall ﬁrst ﬁnd xk+1 and then plug the solution into the
second equation. Since the gradient computed at xk+1
can be used at the next iteration, the two algorithms
have the same complexity. Indeed, for n (cid:54)= 0 (gradient
descent is recovered for n = 0), by simplifying the
variable v, both schemes can be written in one line:

xk+1 = xk + (1 − q
+ ((1 − q

s)(xk − xk−1) − m

s∇f (xk)

s)m

s − ns)∇f (xk−1);

(EE)

√

√

√

√

12Actually, there exist many semi-implicit methods that
go under the name of “semi-implicit Euler”. We expect many
of those integrators to work equally well for the sake of our
discussion on equivalence. For a more detailed discussion,
we refer the reader to Chapter 1 of Hairer et al. (2006).

Revisiting the Role of Symplectic Integration on Acceleration and Stability in Convex Optimization

√

xk+1 = xk + (1 − q
− (cid:0)m

s + ns(cid:1) ∇f (xk) + (1 − q

√

s)(xk − xk−1)

√

s)m

√

s∇f (xk−1).

(SIE)

Remarkably, diﬀerent choices of parameters yield a rich
set of momentum methods, and the reader can probably
already notice some conﬁgurations which recover well-
known optimizers (see introduction). We explore this
in the next subsection.

4.1 Equivalence between SIE and EE
We show that algorithms obtained from semi-implicit
discretization of an accelerated ﬂow can also be seen
as explicit discretization of a diﬀerent accelerated ﬂow.

Lemma 2 (Equivalence between SIE and EE). For
n = 0 both EE and SIE reduce to gradient descent.
For n (cid:54)= 0, consider parameters (mSIE, nSIE, q) and set

mEE = mSIE +
√

nEE = (1 − q

√

snSIE,

s)nSIE.

√

EE with stepsize
s > 0 on GM-ODE with parameters
(mEE, nEE, q) leads to the same exact algorithm as the
one obtained using SIE with stepsize
s > 0 on GM-
ODE with parameters (mSIE, nSIE, q).

√

Proof. We start from the one-line representation. We
get the following conditions for n (cid:54)= 0:

(cid:40)

√

√

mSIE
(1 − q

s + snSIE = mEE
√
√
s)mSIE

s
s = (1 − q

√

s)mEE

√

s − snEE.

We substitute the ﬁrst equation into the second.

As a crucial consequence of the last lemma, Heavy-ball
and Nesterov method can be seen both as semi-implicit
and explicit integrators on GM-ODE. This is illustrated
in Tb. 1. Since, as it is well known, NAG is accelerated,
Lemma 2 shows that both explicit and semi-implicit
Euler integrators can lead to acceleration under well-
chosen parameters. In the next subsection, we elaborate
more on this ﬁnding and recover parameters which lead
to acceleration for EE and SIE.

An ODE which gives NAG under the explicit
Euler method. From Tb. 1 and Eq. 1, we get that
√

√

s∇2f (X)(cid:1) ˙X + ∇f (X) = 0

µ + 2(1 −

µs)

¨X + (cid:0)2

√

leads to NAG through EE (choosing q = 2
√

√

¨X + (cid:0)2

µ +

s∇2f (X)(cid:1) ˙X + ∇f (X) = 0

√

µ), while

recovers NAG through SIE discretization. These pa-
rameter choices lead to acceleration (see Cor. 4). Note

EE discretization

SIE discretization

HB

s

q = (1 − β)/
√
m =
n = β
NAG q = (1 − β)/
m = (1 + β)
n = β2

√

s

√
√

s
s

√

√

s

s

q = (1 − β)/
m = 0
n = 1
q = (1 − β)/
√
m =
n = β

s

Table 1: HB and NAG with any stepsize s > 0 and mo-
mentum β ∈ (0, 1) (see deﬁnition in the introduction) can
be seen as both EE or SIE numerical integrators.

that the last equation is equivalent13 to NAG-ODE-HR,
while the ﬁrst is not known in the literature. However,
Thm. 1 ensures that both ODEs are accelerated. This
is enough to show that the sketch in Fig. 2 is correct.

4.2 Semi-implicit Euler is accelerated

Leveraging insights from the ODE stability analysis in
Thm. 1 and the lessons learned from semi-implicit Lya-
punov function design in recent literature Shi et al.
(2018, 2019), our next result establishes a general
convergence rate for the semi-implicit Euler method
on GM-ODE. In the next subsection, we also provide
a similar result for EE, using Lemma 2.

Theorem 3 (Convergence of SIE). Assume f L-
smooth and µ-strongly-convex. Let (xk)∞
k=1 be the se-
quence obtained from semi-implicit discretization of
√
GM-ODE with step
√

s. Let

√

√

0 < m

s ≤

, 0 < ns ≤ m

s, 0 < q

s ≤

. (3)

1
2L

1
2

There exists a constant C > 0 such that, for any k ∈ N,
it holds that

f (xk) − f (x∗) ≤ (cid:0)1 + γ2

s(cid:1)−k

C,

√

where γ2 := 1

5 min

(cid:32)

nµ
,
q

q
1 + q2/(nL)

(cid:33)
.

Proof Sketch. The proof is based on the following en-
ergy function inspired by the ODE model (cf. Sec. 3):

E(k) =r1r2(f (xk) − f (x∗)) −

√

r1r2m
2

s

(cid:107)∇f (xk)(cid:107)2

+

nr2
1r2
4

(cid:107)vk(cid:107)2 +
√

1
4

(cid:107)q(xk+1 − x∗) − nr1vk(cid:107)2,

where r1 = 1 − q
s, r2 = n + mq and the last term
is a vanishing (as s → 0) correction that accounts for

13The careful reader might notice a factor 1 +

µs in
front of the gradient for the ODE in Shi et al. (2019). This
small diﬀerence is only due to the particular deﬁnition of
semi-implicit integration. If one replaces q
svk in the RHS
svk+1, then we have complete equivalence.
of SIE with q

√

√

√

Zhang, Orvieto, Daneshmand, Hofmann, Smith

the discretization error (cf. Shi et al. (2019)). We show
sE(k +1) in App. B, completing
E(k +1)−E(k) ≤ −γ2
the proof.

√

The generality of the convergence result allows us to
derive accelerated rates for diﬀerent momentum meth-
ods whose convergence rates may even be unknown.
We illustrate this by deriving the well-known rate of
Nesterov’s method in just a few lines. We note that
known results on semi-implicit integration such as the
ones presented in Shi et al. (2019) are less general since
are limited to high/low resolution or to a ﬁxed viscosity
O(

µ).

√

From Thm. 3 to the well-known rate for NAG.
By invoking Thm. 3, we can recover acceleration of
NAG since it can be written as SIE discretization
of GM-ODE (see Tb. 1).

Corollary 4 (NAG is accelerated). Let f be L-
smooth and µ-strongly-convex with large14 condition
number L/µ ≥ 9. Consider the SIE discretization
of GM-ODE with s ≤ 1
s (with
4L , q = (1 − β)/
√
√
β = 1 − 2
s, n = β (i.e. NAG, see
Tb. 1). The algorithm enjoys the accelerated conver-
gence rate O((1 − (cid:112)
µ/L)k). Namely, ∃C > 0 such
that
µs/15(cid:1)−k

f (xk) − f (x∗) ≤ (cid:0)1 +

µs), m =

C.

√

√

√

Proof. The conditions in in Eq. 3 are satisﬁed since
s = m
s =
√
µ

s ≤ 1/(4L), n = β < 1 = m√
s
(cid:112)
(1−2
Ls/9 ≤ 1/3. Thus, nµ
5q =
√
√
µ
5+5q2/(nL) ≥ 2
5+6µ/L ≥

µs ≤ 2
√
µ

and q
√
µs)

2
(1−1/3)

10
√

µ
9

.

and

µ
15

√

≥

≥

10

√

q

From Thm. 4 to a new rate for QHM. The gen-
erality of our model and our discretization analysis pro-
vides an accelerated convergence rate for a broad class
of momentum methods. Among these methods is quasi-
hyperbolic momentum (Ma and Yarats, 2018), which15
shows promises in optimization for neural nets (Choi
et al., 2019).

(cid:40)

xk+1 = xk − s((1 − a)∇f (xk) + agk+1)
gk+1 = bgk + ∇f (xk),

(QHM)

where a, b ∈ (0, 1). For classiﬁcation tasks, QHM yields
an accelerated rate on real-world datasets (even better
than NAG) (Ma and Yarats, 2018). Despite empirical
beneﬁts, the convergence analysis for this algorithm
is limited to quadratics (Gitman et al., 2019). Using

14The lower bound assumption for conditional number
here and in Cor. 5 is purely technical and only serves for a
simple illustration of these corollaries.

15Ma and Yarats (2018) presented a normalized second
iteration, i.e. gk+1 = bgk +(1−b)∇f (xk), which is generally
equivalent to the one we present here by factor rescaling.

Figure 4: Convergence of QHM. The left plot shows the
convergence of 10-dimensional quadratic with µ = 0.01 and
L = 1; the right plot reports 10-dimensional regularized
logistic regression (random data and labels) with regular-
ization weight l = 10−4. We speciﬁcally used s = 0.5,
β = 1 − 2
ls, respectively, in
the two experiments.

µs and s = 0.5, β = 1 − 2

√

√

Thm. 3, the next corollary establishes an accelerated
rate for QHM (proof in the appendix).

Corollary 5 (Convergence of QHM). Let f be L-
smooth and µ-strongly-convex with L/µ ≥ 9. The
iterates of enjoy a linear convergence rate for s ≤ 1
4L
and a ≤ 1/2. In particular, also enjoys convergence
rate O((1 − (cid:112)
µ/L)k) for b = 1 − 2
µs. Namely,
∃C > 0 such that

√

f (xk) − f (x∗) ≤

(cid:16)

√

1 + a

(cid:17)−k

µs/10

C.

Fig. 4 shows the accelerated rate established in the
corollary, and its dependency on the parameter a. We
leave the extension to the stochastic case (possible
with the methodology in Assran and Rabbat (2020)) to
future work, for the sake of continuing our discussion
on numerical integration.

Thm. 3 fails to prove accelerated rate for HB.
An interesting question may arise as a consequence of
our results: since HB can be recast as semi-implicit
discretization of GM-ODE, then does invoking Thm. 3
produce a global acceleration proof for HB? The an-
swer is no, since the convergence result in Thm. 3 is
conditioned on m > 0; while one needs to set m = 0 to
obtain HB by SIE integration. This is not surprising
since the Lyapunov functions used in the literature
to prove acceleration for NAG often diﬀer from the
one used for convergence of HB (see Eq. 3.3 in Shi
et al. (2018)). Nonetheless, It is possible to construct
an analogue of Thm. 3, using a diﬀerent Lyapunov
function, to derive (non accelerated) convergence for
an HB-like method.

The trade-oﬀ speed-stability. As noted in Re-
mark 2, in continuous time one can increase either
m or n to inﬁnity and get an arbitrarily fast rate.
Thm. 3 shows why a similar phenomenon is not pos-
sible in discrete time (would violate the lower bound

Revisiting the Role of Symplectic Integration on Acceleration and Stability in Convex Optimization

√

in Nemirovsky and Yudin (1983)): for a speciﬁc dis-
cretization step-size
s, Eq. 3 gives us a bound on
the maximum m and n we can choose to have guar-
anteed stability. In other words, if we choose a large
value for either m and n to get a faster rate, we would
end up with a slow algorithm since numerical stability
would require a very small integration step-size. Hence,
as expected by the classic theory of convex optimiza-
tion (Nemirovsky and Yudin, 1983), there is a sweet
(cid:112)
µ/L) — a.k.a acceleration.
spot which yields γ2 = O(

4.3 Explicit Euler is also accelerated!

In the last subsection, we provided a convergence rate
for semi-implicit discretization of GM-ODE and showed
how this general result can be applied to derive (old
and new) convergence rates for momentum methods.
However, as already noted a few times, Lemma 2 im-
plies that an equivalent theorem can be written for the
explicit Euler method.

Corollary 6 (Convergence of EE). Assume f L-
smooth and µ-strongly-convex. Let (xk)∞
k=1 be the se-
quence obtained from semi-implicit discretization of
√
GM-ODE with step

s. Let

√

0 < m

s − ns/(1 − q
√

√

s) ≤

1
2L

,

0 < ns ≤

1 − q
2

√

s

m

s,

0 < q

√

s ≤

1
2

.

(4)

There exists a constant C > 0 such that, for any k ∈ N,
it holds that

f (xk) − f (x∗) ≤ (cid:0)1 + γ3

√

s(cid:1)−k

C,

where γ3 := 1

5 min

(cid:32)

nµ
q(1 − µ

√

,
s)

q
1 + q2/(nL)

(cid:33)
.

Proof. Consider an explicit method with parameters
(mEE, nEE, q) and a semi-implicit method with param-
eters (mSIE, nSIE, q). Thm. 3 holds if 0 < mSIE
s ≤
√
s ≤ 1/2, then it
1/(2L), 0 < snSIE ≤ mSIE
is convergent. By Lemma 2, we can recover the pa-
rameter of an equivalent explicit method by setting
snSIE. Com-
s)nSIE and mEE = mSIE +
nEE = (1 − q
bining these conditions with the theorem requirements
on nSIE, we get:

s and q

√

√

√

√

0 <

snEE
√

1 − q

s

√

≤ mSIE

s = mEE

√

s −

snEE
√

1 − q

,

s

which implies the condition on nEE. For the condition
on mEE, just note that the condition on mSIE from
√
Thm. 3 implies
s nEE ≤ 1
2L .

smEE − s
1−q

smSIE =

√

√

For the integration of
Stability of EE and SIE.
Hamiltonian systems, semi-implicit Euler is provably
more stable than explicit Euler (Hairer et al., 2006). For
example, a linearized pendulum integrated with explicit
Euler diverges in phase space, while the semi-implicit
Euler method is stable and conserves the structure of
the ODE system (energy, volume). In Fig. 5, we show
that for a dissipative (hence not Hamiltonian) system
such as GM-ODE the situation can be very diﬀerent:
in complete agreement with our equivalence result in
Lemma 2, there exists parameter conﬁgurations for
which EE is stable but SIE is not, and vice versa.

Figure 5: EE vs. SIE. To show that SIE and EE are
neither superior nor inferior to each other, in each subplot,
we use the same parameters m, n, q for both SIE and EE
discretization. We observe very diﬀerent behaviours. This
suggests the stability and convergence is determined by
the joint choice of parameters and numerical integrator
together. The objective function here is a 2-dimensional
quadratic with µ = 0.01, L = 1 and the step-size is s = 1.
√
µ and in
In the left plot we use m =
√
√
the right plot we use m = 2

s, n = 1 and q = 2
s, n = 0.5 and q = 2

µ.

√

5 Behaviour of the discretization error

In the last sections, we studied the properties of ex-
plicit and semi-implicit integration of GM-ODE and
showed that both can lead to acceleration. Yet, most
recent literature (Wilson et al., 2016; Shi et al., 2019;
Muehlebach and Jordan, 2019, 2020) claims that semi-
implicit integration is somehow more natural for the
approximation of partitioned dissipative systems such
as GM-ODE. Indeed, recent works (França et al., 2020a;
Muehlebach and Jordan, 2020) showed that the geomet-
ric properties of semi-implicit methods combined with
backward error analysis (Hairer et al., 2006) can be used
to successfully prove the preservation of continuous-
time rates of convergence up to a controlled error. In-
stead, our results in Thm. 6 show that explicit Euler
discretization — of a proper ODE — also leads to an
accelerated method (see also Tb. 1). To conclude our
study, we compare semi-implicit and explicit Euler in
terms of their approximation error, speciﬁcally for the
integration of GM-ODE. For this particular ODE, EE
suﬀers from a worse local discretization error compared
to SIE for the general choice of parameters. Under

Zhang, Orvieto, Daneshmand, Hofmann, Smith

particular choices of parameters, EE and SIE yield
contractive algorithms. In this case, the error of the
both discretization schemes decays exponentially fast.

A trap: local error analysis for the general case.
Consider the following discretization errors:

Lemma 8. Let f be µ-strongly-convex and L-smooth.
For EE discretization of GM-ODE obeying Eq. 4, the
discretization error decays as ∆(EE)
s)−k)
where γ3 is deﬁned in Thm. 6. Furthermore, SIE also
√
enjoys ∆(SIE)
s)−k) where γ2 is deﬁned
in Thm. 3 as long as conditions in Eq. 3 are satisﬁed.

= O((1 + γ2

= O((1+γ3

√

k

k

∆(EE)
k
∆(SIE)
k

:= (cid:107)X(k

:= (cid:107)X(k

√

s) − x(EE)
(cid:107),
k
√
s) − x(SIE)
k+1 (cid:107).

(5)

(6)

The proof of the last lemma is postponed to the ap-
pendix. According to this result, SIE and EE discretiza-
tion have the same asymptotic integration error prop-
erties — under particular choice of parameters. This
similarity is also reﬂected in the convergence rates.

We compare the above errors for k = 1 (for one step).
Proof/details are provided in the appendix.

6 Conclusion

Lemma 7. Let f be L-smooth and of class C 2. If
= O(s).
m = O(

= O(s3/2) and ∆(EE)

s), then ∆(SIE)

√

1

1

The above lemma holds for any ﬁnite choice of the
parameters, and shows that SIE provides a better one-
step integration error in the position variable16. This
result may lead to a wrong conclusion: semi-implicit
integration leads to faster algorithm when discretizing
GM-ODE. However, this analysis does not provide
us a complete picture. Indeed, as we proved in the
last section, explicit discretization can also lead to
acceleration — in particular, it can recover Nesterov’s
method. To provide some intuition on why a local error
analysis leads to misleading conclusions, we provide a
tighter analysis of the integration error for a narrowed
set of parameters in GM-ODE.

Analysis for contractive cases. A line of recent
works around the connection between acceleration
and numerical integration (Orvieto and Lucchi, 2019;
Muehlebach and Jordan, 2020; França et al., 2020a)
studied the behavior of the discretization error of NAG-
ODE as k → ∞, showing interesting shadowing 17 prop-
erties. The main idea behind shadowing is studying
the discretization error when the choice of parameters
leads to a contractive algorithm. In this case, one can
provide a tighter analysis for the discretization error.
The next lemma proves that the integration error of
semi-implicit and explicit Euler discretization of GM-
ODE decays exponentially fast if one properly chooses
the parameters.

16It is well known (Hairer et al., 2006) that these methods
actually have the same order, since they are O(s) in the
velocity.

17That is, the discretization bound does not explode expo-
nentially due to error accumulation (Chow and Van Vleck,
1994) if the objective is convex, due to the contraction
provided by the landscape.

In this paper, we proposed a general ODE model
of momentum-based methods for optimizing smooth
strongly-convex functions. The generality of our model
allows to view diﬀerent old and new momentum meth-
ods as semi-implicit or explicit Euler integrators and to
establish novel accelerated convergence rates for both
integrators. In particular, our new ﬁndings overturn
the following old notion: explicit Euler is inferior to
semi-implicit (a.k.a symplectic) Euler because of its
unstable nature. Instead, we show that the stability of
these integrators is tied to the underlying accelerated
ODE. At a deeper level, our methodology provides new
challenging insights on the link between accelerated
optimization, and numerical integration.

Acknowledgements. We are grateful for the en-
lightening discussions with Christian Lubich and Au-
relien Lucchi on the connection between integration
accuracy and optimization speed.

References

Kwangjun Ahn. From proximal point method to Nes-

terov’s acceleration. arXiv:2005.08304, 2020.

Cristian Daniel Alecsa. The long time behavior and the
rate of convergence of symplectic convex algorithms
obtained via splitting discretizations of inertial damp-
ing systems. arXiv preprint arXiv:2001.10831, 2020.
Cristian Daniel Alecsa, Szilárd Csaba László, and Titus
Pinta. An extension of the second order dynami-
cal system that models Nesterov’s convex gradient
method. arXiv preprint arXiv:1908.02574, 2019.
Foivos Alimisis, Antonio Orvieto, Gary Bécigneul, and
Aurelien Lucchi. A continuous-time perspective for
modeling acceleration in Riemannian optimization.
In International Conference on Artiﬁcial Intelligence
and Statistics, pages 1297–1307. PMLR, 2020.

Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear cou-

Revisiting the Role of Symplectic Integration on Acceleration and Stability in Convex Optimization

pling: An ultimate uniﬁcation of gradient and mirror
descent. arXiv preprint arXiv:1407.1537, 2014.
Felipe Alvarez. On the minimizing property of a second
order dissipative system in Hilbert spaces. SIAM
Journal on Control and Optimization, 38(4):1102–
1119, 2000.

Mahmoud Assran and Michael Rabbat. On the conver-
gence of Nesterov’s accelerated gradient method in
stochastic settings. arXiv preprint arXiv:2002.12414,
2020.

Hedy Attouch and Felipe Alvarez. The heavy ball with
friction dynamical system for convex constrained
minimization problems. In Optimization, pages 25–
35. Springer, 2000.

Hedy Attouch, Xavier Goudou, and Patrick Redont.
The heavy-ball with friction method, i. the contin-
uous dynamical system: global exploration of the
local minima of a real-valued function by asymptotic
analysis of a dissipative dynamical system. Commu-
nications in Contemporary Mathematics, 2(01):1–34,
2000.

Pascal Bégout, Jérôme Bolte, and Mohamed Ali Jen-
doubi. On damped second-order gradient systems.
Journal of Diﬀerential Equations, 259(7):3115–3143,
2015.

Giancarlo Benettin and Antonio Giorgilli. On the
Hamiltonian interpolation of near-to-the identity
symplectic mappings with application to symplectic
integration algorithms. Journal of Statistical Physics,
74(5-6):1117–1143, 1994.

Michael Betancourt, Michael I Jordan, and Ashia C
Wilson. On symplectic optimization. arXiv preprint
arXiv:1802.03653, 2018.

Alessandro Bravetti, Hans Cruz, and Diego Tapias.
Contact Hamiltonian mechanics. Annals of Physics,
376:17–39, 2017.

Alessandro Bravetti, Maria L Daza-Torres, Hugo Flores-
Arguedas, and Michael Betancourt. Optimization
algorithms inspired by the geometry of dissipative
systems. arXiv preprint arXiv:1912.02928, 2019.
Alexandre Cabot, Hans Engler, and Sébastien Gadat.
On the long time behavior of second order diﬀeren-
tial equations with asymptotically small dissipation.
Transactions of the American Mathematical Society,
361(11):5983–6017, 2009.

Dami Choi, Christopher J Shallue, Zachary Nado, Jae-
hoon Lee, Chris J Maddison, and George E Dahl. On
empirical comparisons of optimizers for deep learning.
arXiv preprint arXiv:1910.05446, 2019.

Shui-Nee Chow and Erik S Van Vleck. A shadowing
lemma approach to global error analysis for initial

value odes. SIAM Journal on Scientiﬁc Computing,
15(4):959–976, 1994.

Manuel de León and Manuel Lainz Valcázar. Con-
tact Hamiltonian systems. Journal of Mathematical
Physics, 60(10):102902, 2019.

Aaron Defazio. On the curved geometry of accelerated
optimization. In Advances in Neural Information
Processing Systems, pages 1764–1773, 2019.

Jelena Diakonikolas and Michael I Jordan. Generalized
momentum-based methods: A Hamiltonian perspec-
tive. arXiv preprint arXiv:1906.00436, 2019.

Jelena Diakonikolas and Lorenzo Orecchia. Acceler-
ated extra-gradient descent: A novel accelerated
ﬁrst-order method. arXiv preprint arXiv:1706.04680,
2017.

Mahyar Fazlyab, Alejandro Ribeiro, Manfred Morari,
and Victor M Preciado. Analysis of optimization
algorithms via integral quadratic constraints: Non-
strongly convex problems. SIAM Journal on Opti-
mization, 28(3):2654–2689, 2018.

Nicolas Flammarion and Francis Bach. From averaging
to acceleration, there is only a step-size. In Confer-
ence on Learning Theory, pages 658–695, 2015.

Guilherme França, Michael I Jordan, and René Vidal.
On dissipative symplectic integration with applica-
tions to gradient-based optimization. arXiv preprint
arXiv:2004.06840, 2020a.

Guilherme França, Jeremias Sulam, Daniel Robinson,
and René Vidal. Conformal symplectic and relativis-
tic optimization. Advances in Neural Information
Processing Systems, 33, 2020b.

Mark Konstantinovich Gavurin. Nonlinear functional
equations and continuous analogues of iteration
methods. Izvestiya Vysshikh Uchebnykh Zavedenii.
Matematika, pages 18–31, 1958.

Igor Gitman, Hunter Lang, Pengchuan Zhang, and
Lin Xiao. Understanding the role of momentum in
stochastic gradient methods. In Advances in Neural
Information Processing Systems, pages 9630–9640,
2019.

Ernst Hairer. Backward analysis of numerical integra-
tors and symplectic methods. Annals of Numerical
Mathematics, 1:107–132, 1994.

Ernst Hairer, Christian Lubich, and Gerhard Wan-
ner. Geometric numerical integration: structure-
preserving algorithms for ordinary diﬀerential equa-
tions, volume 31. Springer Science & Business Media,
2006.

Bin Hu and Laurent Lessard. Dissipativity theory for
Nesterov’s accelerated method. In Proceedings of the
34th International Conference on Machine Learning-
Volume 70, pages 1549–1557. JMLR. org, 2017.

Zhang, Orvieto, Daneshmand, Hofmann, Smith

JM Sanz Serna and Konstantinos C Zygalakis. Contrac-
tivity of Runge–Kutta methods for convex gradient
systems. SIAM Journal on Numerical Analysis, 58
(4):2079–2092, 2020.

Bin Shi, Simon S Du, Michael I Jordan, and Weijie J
Su. Understanding the acceleration phenomenon via
high-resolution diﬀerential equations. arXiv preprint
arXiv:1810.08907, 2018.

Bin Shi, Simon S Du, Weijie Su, and Michael I Jordan.
Acceleration via symplectic discretization of high-
resolution diﬀerential equations.
In Advances in
Neural Information Processing Systems, pages 5744–
5752, 2019.

Weijie Su, Stephen Boyd, and Emmanuel Candes. A
diﬀerential equation for modeling Nesterov’s accel-
erated gradient method: Theory and insights. In
Advances in Neural Information Processing Systems,
pages 2510–2518, 2014.

Andre Wibisono, Ashia C Wilson, and Michael I Jordan.
A variational perspective on accelerated methods in
optimization. Proceedings of the National Academy
of Sciences, 113(47):E7351–E7358, 2016.

Ashia C Wilson, Benjamin Recht, and Michael I Jor-
dan. A Lyapunov analysis of momentum methods
in optimization. arXiv preprint arXiv:1611.02635,
2016.

Pan Xu, Tianhao Wang, and Quanquan Gu. Continu-
ous and discrete-time accelerated stochastic mirror
descent for strongly convex functions. In Interna-
tional Conference on Machine Learning, pages 5492–
5501, 2018.

Jingzhao Zhang, Aryan Mokhtari, Suvrit Sra, and
Ali Jadbabaie. Direct Runge-Kutta discretization
achieves acceleration. In Advances in neural infor-
mation processing systems, pages 3900–3909, 2018.
Jingzhao Zhang, Suvrit Sra, and Ali Jadbabaie. Ac-
celeration in ﬁrst order quasi-strongly convex opti-
mization by ode discretization. In 2019 IEEE 58th
Conference on Decision and Control (CDC), pages
1501–1506. IEEE, 2019.

Hassan K Khalil and Jessy W Grizzle. Nonlinear sys-
tems, volume 3. Prentice Hall Upper Saddle River,
NJ, 2002.

Walid Krichene, Alexandre Bayen, and Peter L Bartlett.
Accelerated mirror descent in continuous and discrete
time. Advances in neural information processing
systems, 28:2845–2853, 2015.

Laurent Lessard, Benjamin Recht, and Andrew
Packard. Analysis and design of optimization al-
gorithms via integral quadratic constraints. SIAM
Journal on Optimization, 26(1):57–95, 2016.

Christian Lubich. From quantum to classical molecular
dynamics: reduced models and numerical analysis.
European Mathematical Society, 2008.

Jerry Ma and Denis Yarats. Quasi-hyperbolic momen-
tum and adam for deep learning. arXiv preprint
arXiv:1810.06801, 2018.

Robert McLachlan and Matthew Perlmutter. Confor-
mal Hamiltonian systems. Journal of Geometry and
Physics, 39(4):276–300, 2001.

Robert I McLachlan and G Reinout W Quispel. Split-

ting methods. Acta Numerica, 11:341, 2002.

Michael Muehlebach and Michael I Jordan. A dynam-
ical systems perspective on Nesterov acceleration.
arXiv preprint arXiv:1905.07436, 2019.

Michael Muehlebach and Michael I Jordan. Op-
timization with momentum: Dynamical, control-
theoretic, and symplectic perspectives.
arXiv
preprint arXiv:2002.12493, 2020.

Arkadi Semenovich Nemirovsky and David Borisovich
Yudin. Problem complexity and method eﬃciency in
optimization. Wiley, 1983.

Yurii E Nesterov. A method for solving the convex pro-
gramming problem with convergence rate O(1/k2).
In Dokl. akad. nauk Sssr, volume 269, pages 543–547,
1983.

Antonio Orvieto and Aurelien Lucchi. Shadowing prop-
erties of optimization algorithms. In Advances in
Neural Information Processing Systems, pages 12692–
12703, 2019.

Antonio Orvieto, Jonas Kohler, and Aurelien Lucchi.
The role of memory in stochastic optimization. In
Uncertainty in Artiﬁcial Intelligence, pages 356–366.
PMLR, 2020.

Boris T Polyak. Some methods of speeding up the con-
vergence of iteration methods. USSR Computational
Mathematics and Mathematical Physics, 4(5):1–17,
1964.

JM Sanz-Serna and Konstantinos C Zygalakis. The
connections between Lyapunov functions for some
optimization algorithms and diﬀerential equations.
arXiv preprint arXiv:2009.00673, 2020.

Revisiting the Role of Symplectic Integration on Acceleration and Stability in Convex Optimization

Appendix: Proofs and Supplementaries

A Proof for Theorem 1

For convenience of the reader, we report here our generalized model for momentum methods (GM-ODE), motivated
in the main paper.

(cid:40) ˙X = −m∇f (X) − nV
˙V = ∇f (X) − qV.

(GM-ODE)

Theorem 1 (Continuous-time stability). Let f be µ-strongly-convex and L-smooth. If n, m, q ≥ 0 then, for any
value of the strong-convexity modulus µ ≥ 0, the point (x∗, 0) ∈ R2d is globally asymptotically stable for GM-ODE,
as

E(X(t), V (t)) ≤ e−γ1t · E(X(0), V (0)),

(2)

where γ1 := min

(cid:32)

µ(n + qm)
,
2q

q
2

(cid:33)
.

Proof. We propose the Lyapunov function

E(t) = (qm + n)
(cid:124)
(cid:125)

(cid:123)(cid:122)
c1

(cid:0)f (X(t)) − f (x∗)(cid:1) +

consisting of quadratic and mixing parts

n(qm + n)
4
(cid:123)(cid:122)
c2

(cid:125)

(cid:124)

(cid:107)V (t)(cid:107)2 +

1
4
(cid:124)(cid:123)(cid:122)(cid:125)
c3

(cid:107)q(X(t) − x∗) − nV (t)(cid:107)2,

(7)

E1(t) = f (X(t)) − f (x∗),

E2(t) = (cid:107)V (t)(cid:107)2,

E3(t) = (cid:107) − nV (t) + q(X(t) − x∗)(cid:107)2.

The derivatives of each quadratic part are

and

d
dt

E1(t) = −m(cid:107)∇f (X(t)(cid:107)2 − n(cid:104)∇f (X(t)), V (t)(cid:105)

d
dt

E2(t) = −2q(cid:107)V (t)(cid:107)2 + 2(cid:104)∇f (X(t)), V (t)(cid:105),

along with that of the mixing term:

d
dt

E3(t) =2(cid:104)−n ˙V (t) + q ˙X(t), −nV (t) + q(X(t) − x∗)(cid:105)

= − 2(qm + n)(cid:104)∇f (X(t)), −nV (t) + q(X(t) − x∗)(cid:105)
= − 2q(qm + n)(cid:104)∇f (X(t)), X(t) − x∗(cid:105) + 2n(qm + n)(cid:104)∇f (X(t)), V (t)(cid:105)
(cid:17)

(cid:16)

f (X(t)) − f (x∗)

− µq(qm + n)(cid:107)X(t) − x∗(cid:107)2

≤ − 2q(qm + n)

+ 2n(qm + n)(cid:104)∇f (X(t)), V (t)(cid:105),

where last inequality is due to the strong convexity. Plugging the value of c1, c2 and c3, we have

d
dt

E(t) ≤ −

q(n + qm)
2

(cid:16)(cid:0)f (X(t)) − f (x∗)(cid:1) +

µ
2

(cid:107)X(t) − x∗(cid:107)2 + n(cid:107)V (t)(cid:107)2(cid:17)

.

Besides, the mixing term can be upper-bounded by

E3(t) ≤ 2q2(cid:107)X(t) − x∗(cid:107)2 + 2n2(cid:107)V (t)(cid:107)2.

(8)

(9)

(10)

(11)

(12)

(13)

Zhang, Orvieto, Daneshmand, Hofmann, Smith

Therefore we have E(t) satisfying

E(t) ≤ (qm + n)(cid:0)f (X(t)) − f (x∗)(cid:1) + q2(cid:107)X(t) − x∗(cid:107)2/2 +

(cid:16)
n2/2 +

n(n + qm)
4

(cid:17)

(cid:107)V (t)(cid:107)2,

which implies

d
dt

E(t) ≤ − min

(cid:26) µ(n + qm)
2q

,

q
2

(cid:27)

· E(t).

We then conclude using Gronwall’s lemma (Khalil and Grizzle, 2002).

B Proof for Theorem 3

For convenience of the reader, we repeat here the semi-implicit integrator of GM-ODE we seek to study:

(cid:40)

(SIE) :

xk+1 − xk = −m
vk+1 − vk =

√

s∇f (xk) − n
√

s∇f (xk+1) − q

svk.

√

svk

√

In compact notation, the second iteration can be written as

or

where r1 = 1 − q

√

s.

r1(vk+1 − vk) =

√

s∇f (xk+1) − q

√

svk+1

r1vk = vk+1 −

√

s∇f (xk+1),

(14)

(15)

(16)

(17)

Theorem 3 (Convergence of SIE). Assume f L-smooth and µ-strongly-convex. Let (xk)∞
obtained from semi-implicit discretization of GM-ODE with step

√

k=1 be the sequence

√

0 < m

s ≤

1
2L

, 0 < ns ≤ m

√

1
2

.

(3)

s. Let
√

s, 0 < q

s ≤

There exists a constant C > 0 such that, for any k ∈ N, it holds that
s(cid:1)−k

f (xk) − f (x∗) ≤ (cid:0)1 + γ2

√

C,

where γ2 := 1

5 min

(cid:32)

nµ
,
q

q
1 + q2/(nL)

(cid:33)
.

Proof. We propose the discrete Lyapunov function deﬁned as

E(k) = r1r2(f (xk) − f (x∗)) +

1
4

(cid:107)q(xk+1 − x∗) − nr1vk(cid:107)2 +

nr2
1r2
4

(cid:107)vk(cid:107)2 −

√

r1r2m
2

s

(cid:107)∇f (xk)(cid:107)2.

(18)

We use colors for diﬀerent parts to keep track of related terms in the derivation. As the ﬁrst step, thanks to
L-Lipshitz smoothness, we have

f (xk+1) − f (xk) ≤(cid:104)∇f (xk+1), xk+1 − xk(cid:105) −

s(cid:104)∇f (xk), ∇f (xk+1)(cid:105) − n

1
2L

(cid:107)∇f (xk+1) − ∇f (xk)(cid:107)2

√

s(cid:104)vk, ∇f (xk+1)(cid:105)

√

= − m
1
2L

−

(cid:107)∇f (xk+1) − ∇f (xk)(cid:107)2.

(19)

We proceed by computing the diﬀerence in E in two subsequent iterations. Denote r2 = n + mq, we have

E(k + 1) − E(k)

(A)
≤ − r1r2m

√

s(cid:104)∇f (xk), ∇f (xk+1)(cid:105) − r1r2n

√

s(cid:104)vk, ∇f (xk+1)(cid:105)

Revisiting the Role of Symplectic Integration on Acceleration and Stability in Convex Optimization

(cid:107)q(xk+2 − xk+1) − nr1(vk+1 − vk)(cid:107)2

−

+

+

−

(cid:107)∇f (xk+1) − ∇f (xk)(cid:107)2 +

r1r2
1
2L
4
1
(cid:104)q(xk+2 − xk+1) − nr1(vk+1 − vk), q(xk+1 − x∗) − nvk+1 + n
2
nr2
1r2
4
r1r2m
2
√

(cid:107)vk+1 −
(cid:107)∇f (xk+1)(cid:107)2 − (cid:107)∇f (xk)(cid:107)2(cid:17)
√

(cid:107)vk+1(cid:107)2 −
√

s∇f (xk+1)(cid:107)2

nr2
4

√

(cid:16)

s

s(cid:104)∇f (xk), ∇f (xk+1)(cid:105) − r1r2n

s(cid:104)vk, ∇f (xk+1)(cid:105)

(B)
= −r1r2m

√

s∇f (xk+1)(cid:105)

−

r1r2
2L
r2
2
nr2(1 − r2
1)
4
√
r1r2m
2

(cid:16)

s

−

−

−

(cid:107)∇f (xk+1) − ∇f (xk)(cid:107)2 −
√

s(cid:104)∇f (xk+1), q(xk+1 − x∗) − nvk+1(cid:105)

r2(2n − r2)
4

s(cid:107)∇f (xk+1)(cid:107)2

nr2
(cid:107)vk+1(cid:107)2 −
4
(cid:107)∇f (xk+1)(cid:107)2 − (cid:107)∇f (xk)(cid:107)2(cid:17)

s(cid:107)∇f (xk+1)(cid:107)2 +

√

nr2
2

s(cid:104)∇f (xk+1), vk+1(cid:105)

√

(C)
= nr2

s(cid:104)∇f (xk+1), vk+1/2 + vk+1/2 − r1vk(cid:105)

√

m

(cid:16)
s

r1r2
2

(cid:16) r2(2n − r2)
4

(cid:107)∇f (xk+1)(cid:107)2 − 2(cid:104)∇f (xk+1), ∇f (xk)(cid:105) + (cid:107)∇f (xk)(cid:107)2(cid:17)
nr2(1 − r2
1)
4

s + r1r2m

nr2
4

(cid:107)∇f (xk+1)(cid:107)2 −
√

s +

√

(cid:107)∇f (xk+1) − ∇f (xk)(cid:107)2 −

s(cid:104)∇f (xk+1), xk+1 − x∗(cid:105).

+

−

−

(cid:17)
s
r2
2

q

r1r2
2L

(cid:107)vk+1(cid:107)2

(20)

In step (A), we use smoothness of f as stated in Eq. 19 for the blue term. Also, we used the inequality
(cid:107)a(cid:107)2 − (cid:107)b(cid:107)2 = (cid:107)a − b(cid:107)2 + 2(cid:104)a − b, b(cid:105) where a = q(xk+2 − x∗) − nr1vk and b = q(xk+1 − x∗) − nr1vk to obtain the
red term. In particular,

a − b = q(xk+2 − xk+1) − nr1(vk+1 − vk)

√

= −mq
√

= −r2

s∇f (xk+1) − nq
s∇f (xk+1).

√

svk+1 − n

√

s∇f (xk+1) + nq

√

svk+1

In step (B), we incorporate the recurrence of SIE. Step (C) is a simple re-arrangement of terms.

We can easily verify the following identities:

√

s(cid:104)∇f (xk+1), vk+1 − r1vk(cid:105) = s(cid:107)∇f (xk+1)(cid:107)2

and

We have

(cid:107)∇f (xk+1)(cid:107)2 − 2(cid:104)∇f (xk+1), ∇f (xk)(cid:105) + (cid:107)∇f (xk)(cid:107)2 = (cid:107)∇f (xk+1) − ∇f (xk)(cid:107)2.

E(k + 1) − E(k) ≤nr2s(cid:107)∇f (xk+1)(cid:107)2 +

s(cid:107)∇f (xk+1) − ∇f (xk)(cid:107)2

√

m

r1r2
2
r1m
√
s

(cid:17)

+

(cid:16) 2n − r2
4

n
4
(cid:107)∇f (xk+1) − ∇f (xk)(cid:107)2 −

+

− r2s

−

r1r2
2L

√

r2
2

q

(cid:107)∇f (xk+1)(cid:107)2 −

nr2(1 − r2
1)
4
s(cid:104)∇f (xk+1), xk+1 − x∗(cid:105).

(cid:107)vk+1(cid:107)2

We leverage µ-strong convexity of f to get

(cid:104)∇f (xk+1), xk+1 − x∗(cid:105) ≥ f (xk+1) − f (x∗) +

µ
2

(cid:107)xk+1 − x∗(cid:107)2.

Applying the above inequality to the last term of Eq. 24, we obtain

E(k + 1) − E(k) ≤ −

√

r2
2

q

s(f (xk+1) − f (x∗)) −

√

r2µ
4

q

s(cid:107)xk+1 − x∗(cid:107)2

(21)

(22)

(23)

(24)

(25)

Zhang, Orvieto, Daneshmand, Hofmann, Smith

−

r1r2
2

− r2s

√

(1/L − m

(cid:16) 2n − r2
4

+

n
4

s)(cid:107)∇f (xk+1) − ∇f (xk)(cid:107)2 −
(cid:17)

− n

(cid:107)∇f (xk+1)(cid:107)2.

+

r1m
√
s

nr2(1 − r2
1)
4

(cid:107)vk+1(cid:107)2

(26)

(27)

√

s ≥ 1/2, it holds

Now we plug in the the value of r1, r2 and calculate
√

where we used the condition q
that

√

1 − r2
1 = 1 − (1 − q
s ≤ 1/2. Next, since m

s)2 = q
√

s(2 − q
s) ≥ q
√
s ≤ 1/(2L), n ≤ m/

s,
s and r1 = 1 − q

√

√

√

3n
4
Hence, the diﬀerence between two iterations can be upper-bounded as follows:

2n − r2
4

n − mq
4

r1m
√
s

r1m
√
s

r1m
√
s

3n
4

n
2

−

−

−

=

+

+

=

−

mq
4

≥ −

mq
4

.

E(k + 1) − E(k) ≤ −

= −

√

(cid:16)

s

µ
2

f (xk+1) − f (x∗) +

r2q
2
√
r2q
2
+ n(cid:107)vk+1(cid:107)2/2 + r3[f (xk+1) − f (x∗) −

(1 − r3)[f (xk+1) − f (x∗)] +

µ
2

(cid:16)

s

(cid:107)xk+1 − x∗(cid:107)2

(cid:17)
(cid:107)∇f (xk+1)(cid:107)2]

,

1
2L

(cid:107)xk+1 − x∗(cid:107)2 + n(cid:107)vk+1(cid:107)2/2 −

√

m
2

s

(cid:107)∇f (xk+1)(cid:107)2(cid:17)

√

where r3 = Lm
On the other hand, our candidate Lyapunov function at iteration k itself can be upper-bounded as

s ≤ 1/2 and the bound remains legal since 1 − r3 ≥ 1/2.

E(k) = r1r2(f (xk) − f (x∗) +

(cid:107)q(xk+1 − x∗) − nr1vk(cid:107)2 +

nr2
1r2
4

(cid:107)vk(cid:107)2 −

r1r2m
2

√

s

(cid:107)∇f (xk)(cid:107)2

(cid:107)q(xk − x∗) − nvk − mq

√

s∇f (xk)(cid:107)2 +

nr2
1r2
4

(cid:107)vk(cid:107)2

1
4
1
4
s(cid:107)∇f (xk)(cid:107)2/2

(A)
= r1r2(f (xk) − f (x∗)) +
√

− r1r2m

(B)
≤ r1r2(f (xk) − f (x∗)) + q2(cid:107)xk − x∗(cid:107)2 + n2(cid:107)vk(cid:107)2 +

√

− r1r2m

s(cid:107)∇f (xk)(cid:107)2/2

q2m2s
2

(cid:107)∇f (xk)(cid:107)2 +

nr2
1r2
4

(cid:107)vk(cid:107)2

= r1r2(1 − r3 + r4)(f (xk) − f (x∗)) + q2(cid:107)xk − x∗(cid:107)2 + (n2 + nr2

1r2/4)(cid:107)vk(cid:107)2

+ r1r2(r3 − r4)[f (xk) − f (x∗) −

1
2L

(cid:107)∇f (xk)(cid:107)2],

(28)

(29)

(30)

with r4 = Lq2m2s/(r1r2). Precisely, step (A) is obtained by replacing SIE update for the term xk+1. (B) is obtained
by repeatedly using the inequality (cid:107)a + b(cid:107)2 ≤ 2(cid:107)a(cid:107)2 + 2(cid:107)b(cid:107)2. Finally, noting that f (xk) − f (x∗) ≥ 1
2L (cid:107)∇f (xk)(cid:107)2,
we have

(cid:16)

E(k) ≤ r2

r1(1 − r3 + r4)[f (xk) − f (x∗)] +

q2
n

(cid:107)xk − x∗(cid:107)2 + 5n(cid:107)vk(cid:107)2/4

+ r1(r3 − r4)[f (xk) − f (x∗) −

(cid:17)
(cid:107)∇f (xk)(cid:107)2]

,

(31)

1
2L

since r2 = n + mq ≥ n. It is reckoned that E(k + 1) − E(k) and E(k) share identical parts except for diﬀerent
coeﬃcients. Now we aim at obtaining following inequality

E(k + 1) − E(k) ≤ −γ2

√

sE(k + 1).

(32)

To achieve this, γ2 should be the minimal ratio for coeﬃcients of each parts of E(k + 1) − E(k) to those of E(k).
It is easy then to notice that γ2 should be smaller than q/5 and nµ/(4q). Besides it should also hold that
1 − r3
1 − r3(1 − r4/r3)

1 − r3
1 − (r3 − r4)

1 − 1/2
1 − 1/2(1 − q2

r3
r3 − r4

r2q
2r1r2

≥ γ2,

(33)

q
2

q
2

q
2

q
2

≥

≥

≥

=

1
1 + q2
nL

nL )
q } satisﬁes the above inequality

, nµ

due to the fact r4
r3

= q2m
r1r2
and completes the proof.

√

s

≤ q2
nL

and r3 ≤ 1/2. Therefore γ2 = 1

5 min{

q
1+ q2

nL

Revisiting the Role of Symplectic Integration on Acceleration and Stability in Convex Optimization

We now use the above result to prove the convergence of QHM iterations (see Sec. 4).

Corollary 5 (Convergence of QHM). Let f be L-smooth and µ-strongly-convex with L/µ ≥ 9. The iterates of
enjoy a linear convergence rate for s ≤ 1
µ/L)k)
for b = 1 − 2

4L and a ≤ 1/2. In particular, also enjoys convergence rate O((1−(cid:112)

µs. Namely, ∃C > 0 such that

√

f (xk) − f (x∗) ≤

(cid:16)

√

1 + a

(cid:17)−k

µs/10

C.

Proof. First, we show how one can alternatively write QHM as one-line scheme. The original QHM algorithm is
reported here for convenience of the reader

(cid:40)

xk+1 = xk − s((1 − a)∇f (xk) + agk+1)
gk+1 = bgk + ∇f (xk).

We replace the second line of QHM into the ﬁrst one :

xk+1 = xk − s(1 − a)∇f (xk) − s · b · a · gk − as∇f (xk).

Using the ﬁrst iterate we get:

Replacing this into the result of ﬁrst equation, we get:

−(xk − xk−1)/s − (1 − α)∇f (xk−1) = agk.

xk+1 = xk − s(1 − a)∇f (xk) + b((xk − xk−1) + s(1 − a)∇f (xk−1)) − as∇f (xk).

By rearrangment, we ﬁnally obtain

xk+1 = xk + b(xk − xk−1) − s∇f (xk) + sb(1 − a)∇f (xk−1).

(QHM)

(34)

(35)

(36)

(37)

The above iterates can be viewed as SIE discretization of GM-ODE with the following speciﬁc choice of parameters
(see the single sequence of iterates of SIE in the last section):

Invoking Thm. 3, we get the convergence rate for QHM. More precisely, choosing b = 1 − 2

√

µs we obtain

m = (1 − a)

√

s, n = a,

q =

1 − b
√
s

.

The above choice of parameters obeys the constraints in Thm. 3:

√

q = 2

µ.

and

√

m

s = (1 − a)s < s ≤

1
4L

, n = a ≤ (1 − a) =

m
√
s

,

√

q

√

s = 2

µs ≤ 2

(cid:112)

sL/9 ≤ 1/3,

(38)

(39)

(40)

(41)

5 min{ nµ
q ,

since we assumed s ≤ 1/(4L), a ≤ 1/2 and L/µ ≥ 9. The rate — thanks to Thm. 3 — is determined by
µ/8 in the case of QHM. First,
γ2 = 1
one can readily check that (nµ)/(5q) = aµ/(10
µ) holds due to the choice of parameters. Second, with some
patience, one can check that the following chain of inequality holds:

1+q2/(nL) )}. We conclude the proof by showing that γ2 = a
√

√

q

1
5

·

q
1 + q2
nL

=

√
2

µ
5 + 20µ
aL

≥

√

2a

µ

5a + 20/9

√

a
µ
10

.

≥

Zhang, Orvieto, Daneshmand, Hofmann, Smith

C Proofs for Section 5

As stated in the main paper, we consider the following discretization errors:

∆(EE)
k
∆(SIE)
k

√

√

:= (cid:107)X(k

s) − xk(cid:107),

xk obtained by EE

:= (cid:107)X(k

s) − xk+1(cid:107),

xk obtained by SIE.

We deﬁne wk := xk for EE and wk := xk+1 for SIE. We compare the error ∆k = (cid:107)X(k
next lemma, assuming ∆0 = 0 and v0 = V (0). This is also called local (or one-step) integration error.

s) − wk(cid:107) for k = 1 in the

√

Lemma 7. Let f be L-smooth and of class C 2. If m = O(

√

s), then ∆(SIE)

1

= O(s3/2) and ∆(EE)

1

= O(s).

Proof. We introduce the notation Xk := X(k
v0 = V0. For SIE, wk = xk+1 and we begin from Taylor expansion of X as

s), Vk := V (k

√

√

s). Our problem setting requires w0 = X0 and

and therefore

X1 − X0 =

√

s ˙X0 + s ¨X0 + O(s3/2),

(42)

X1 − w1 =X1 − X0 − (w1 − w0) + X0 − w0
=X1 − X0 − (x2 − x1) + X0 − x1
s∇f (x1) + n
=
(cid:16)

s ˙X0 + s ¨X0 + m
(cid:16)
s

√

√

√

=

(cid:17)

√

− m∇f (X0) − nV0
√

√

s∇f (x1) + n

+ s
(cid:16)√
s

+ m

d
dt

sv1 + O(s3/2)

(cid:17)

− m∇f (X0) − nV0
(cid:17)

√

s∇f (x1) + (1 − q

s)v0

+ O(s3/2).

(43)

where in the third equality we used the fact that, by hypothesis, X0 − x1 = 0. And in particular, since
d∇f (X)

dt = ∇2f (X) ˙X,

(cid:16)

s

d
dt

− m∇f (X0) − nV0

(cid:17)

= − sm∇2f (X0) ˙X0 − sn ˙V0

= − sn∇f (X0) + snqV0 + sm2∇2f (X0)∇f (X0) + smn∇2f (X0)V0.

(44)

Then it holds that

X1 − w1 = − (m

√

(cid:16)

s + ns)

∇f (X0) − ∇f (x1)

(cid:17)

√

− n

s(V0 − v0) + snq(V0 − v0) + O(s3/2) ≤ O(s3/2)

(45)

1

≤ O(s3/2).

and ∆(SIE)
We proceed with the EE iterations (remember: wk = xk). We expand ∆(EE)

1

as

X1 − w1 =X1 − X0 − (w1 − w0) + X0 − x0 + O(s3/2)
=X1 − X0 − (x1 − x0) + X0 − x0 + O(s3/2)
√
=

√

√

s ˙X0 + s ¨X0 + m
(cid:16)
s

− m∇f (X0) − nVk

s∇f (x0) + n
√

(cid:17)

+ m

√

=

sv0 + O(s3/2)
√

m∇2f (X0)∇f (X0) + n∇2f (Xk)V0

s∇f (x0) + n
(cid:17)

sv0

(cid:16)

+ sm
(cid:16)

− sn
√

∇f (X0) − qV0
(cid:16)
s

∇f (X0) − ∇f (x0)

+ O(s3/2)
√

(cid:17)

− n

= − m

(cid:17)

(cid:16)
s

V0 − v0

(cid:17)

+ O(s).

(46)

Therefore, we conclude that ∆(EE)

1 ≤ O(s).

Revisiting the Role of Symplectic Integration on Acceleration and Stability in Convex Optimization

Lemma 8. Let f be µ-strongly-convex and L-smooth. For EE discretization of GM-ODE obeying Eq. 4, the
discretization error decays as ∆(EE)
s)−k) where γ3 is deﬁned in Thm. 6. Furthermore, SIE also
enjoys ∆(SIE)

s)−k) where γ2 is deﬁned in Thm. 3 as long as conditions in Eq. 3 are satisﬁed.

= O((1 + γ3

= O((1 + γ2

√

√

k

k

Proof. The proof is based on the following consequence of strong convexity

Using the above inequality together with a straightforward application of triangular inequality we complete the
proof:

µ(cid:107)x − x∗(cid:107)2/2 ≤ f (x) − f (x∗).

(47)

√

(cid:107)X(k

s) − xk(cid:107) = (cid:107)X(k
≤ (cid:107)X(k
√

s) − x∗ + x∗ − xk(cid:107)
s) − x∗(cid:107) + (cid:107)xk − x∗(cid:107)
√

√

√

2µ−1/2 (cid:16)(cid:0)f (X(k

≤

s)) − f (x∗)(cid:1)1/2

+ (f (xk) − f (x∗))

1/2(cid:17)

.

(48)

Replacing the convergence results in Thm. 1, 3, and 6 into the the above bound concludes the proof.

