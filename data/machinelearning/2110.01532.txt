DIFFERENTIABLE SPLINE APPROXIMATIONS

1
2
0
2

t
c
O
4

]

G
L
.
s
c
[

1
v
2
3
5
1
0
.
0
1
1
2
:
v
i
X
r
a

Minsu Cho2†, Aditya Balu1†, Ameya Joshi2, Anjana Deva Prasad1, Biswajit Khara1, Soumik Sarkar1,
Baskar Ganapathysubramanian1, Adarsh Krishnamurthy1, Chinmay Hegde2 ∗

ABSTRACT

The paradigm of differentiable programming has signiﬁcantly enhanced the scope of machine learning
via the judicious use of gradient-based optimization. However, standard differentiable programming
methods (such as autodiff) typically require that the machine learning models be differentiable,
limiting their applicability. Our goal in this paper is to use a new, principled approach to extend
gradient-based optimization to functions well modeled by splines, which encompass a large family of
piecewise polynomial models. We derive the form of the (weak) Jacobian of such functions and show
that it exhibits a block-sparse structure that can be computed implicitly and efﬁciently. Overall, we
show that leveraging this redesigned Jacobian in the form of a differentiable “layer” in predictive
models leads to improved performance in diverse applications such as image segmentation, 3D point
cloud reconstruction, and ﬁnite element analysis.

Keywords
Differentiable NURBS Layer | NURBS | Geometric Deep Learning | Surface Modeling

1

Introduction

Motivation: Differentiable programming has been a paradigm shift in algorithm design. The main idea is to leverage
gradient-based optimization to optimize the parameters of the algorithm, allowing for end-to-end trainable systems
(such as deep neural networks) to exploit structure in data and achieve better performance. This approach has found
use in a large variety of applications such as scientiﬁc computing [1–3], image processing [4], physics engines [5],
computational simulations [6], and graphics [7, 8]. One way to leverage differentiable programming modules is to
encode additional structural priors as “layers” in a larger machine learning model. Inherent structural constraints such
as monotonicity, or piecewise constancy, are particularly prevalent in applications such as physics simulations, graphics
rendering, and network engineering. In such applications, it may be beneﬁcial to build models that obey such priors by
design.

Challenges: For differentiable programming to work, all layers within the model must admit simple gradient calcula-
tions; however, this poses a major limitation in many settings. For example, consider computer graphics applications for
rendering 3D objects [9–11]. A common primitive in such cases is a spline (or a piecewise polynomial) function which
either exactly or approximately interpolates between a discrete set of points to produce a continuous shape or surface.
Similar spline (or other piecewise polynomial) approximations arise in partial differential equation (PDE) solvers [12],
network ﬂow problems [13], and other applications.

For such problems, we would like to compute gradients “through” operations involving spline approximation. However,
algorithms for spline approximation often involve discontinuous (or even discrete) co-domains and may introduce
undeﬁned (or even zero) gradients. Generally, embedding such functions as layers in a differentiable program, and
running automatic differentiation on this program, requires special care. A popular solution is to relax these non-
differentiable, discrete components into continuous approximations for which gradients exist. This has led to recent
advances in differentiable sorting [14, 15], dynamic programming [16], and optimization [17–19].

Our contributions: We propose a principled approach for differentiable programming for spline functions without the
use of continuous relaxation2. For the forward pass, we leverage fast algorithms for computing the optimal projection

∗1The author is with Iowa State University, 2The author is with New York University. †Equal contribution.
2While tricks such as straight-through gradient estimation [20] also avoid continuous relaxation, they are heuristic in nature and

may be inaccurate for speciﬁc problem instances [21].

 
 
 
 
 
 
of any given input onto the space of piecewise polynomial functions. For the backward pass, we leverage a fundamental
locality property in splines that every piece (or basis function) in the output approximation only interacts with a few
other elements. Using this, we derive a weak form of the Jacobian for the spline operation, and show that it exhibits a
particular block-structured form. While we focus on spline approximation in this paper, our approach can be generalized
to any algorithmic module with piecewise smooth outputs. Our speciﬁc contributions are as follows:

1. We propose the use of spline function approximations as “layers” in differentiable programs.
2. We derive efﬁcient (nearly-linear time) methods for computing forward and backward passes for various spline
approximation problems, showing that the (weak) Jacobian in each case can be represented using a block sparse
matrix that can be efﬁciently used for backpropagation.

3. We show applications of our approach in three stylized applications: image segmentation, 3D point cloud reconstruc-

tion, and ﬁnite element analysis for the solution of partial differential equations.

Related Work Before proceeding, we brieﬂy review related work.

Extensions of autodiff: Automatic differentiation (autodiff) algorithms enable gradient computations over basic
algorithmic primitives such as loops, recursion, and branch conditions [22]. However, introducing more complex
non-differentiable components requires careful treatment due to undeﬁned or badly behaved gradients. For example, in
the case of sorting and ranking operators, it can be shown that the corresponding gradients are either uninformative or
downright pathological, and it is imperative the operators obey a ‘soft’ differentiable form. Cuturi et al. [15] propose
a differentiable proxy for sorting based on optimal transport. Blondel et al. [14] improve this by proposing a more
efﬁcient differentiable sorting/ranking operator by appealing to isotonic regression. Berthet et al. [23] introduce the use
of stochastic perturbations to construct smooth approximations to discrete functions, and other researchers have used
similar approaches to implement end-to-end trainable top-k ranking systems [24, 25]. Several approaches for enabling
autodiff in optimization have also been researched [16, 18, 26, 27].

Structured priors as neural “layers”: As mentioned above, one motivation for our approach arises from the need for
enforcing structural priors for scientiﬁc computing applications. Encoding non-differentiable priors such as the solutions
to speciﬁc partial differential equations [28], geometrical constraints [8, 29], and spatial consistency measures [17]
perform well but typically require massive amounts of structured training examples.

Spline approximation: Non-Uniform Rational B-splines (NURBS) are commonly used for deﬁning spline surfaces
for geometric modeling [30]. NURBS surfaces offer a high level of control and versatility; they can also compactly
represent the surface geometry. The versatility of NURBS surfaces enables them to represent more complex shapes
than Bèzier or B-splines. Several frameworks that leverage deep learning are beginning to use NURBS representations.
Minto et al. [31] use NURBS surfaces ﬁtted over the 3D geometry as an input representation for the object classiﬁcation
task of ModelNet10 and ModelNet40 datasets. Erwinski et al. [32] presented a neural-network-based contour error
prediction method for NURBS paths. Fey et al. [33] present a new convolution operator based on B-splines for irregular
structured and geometric input, e.g., graphs or meshes. Very recently, [34] perform point cloud reconstruction to predict
a B-spline surface, which is later processed to obtain a complete CAD model with other primitives “stitched” together.

Differentiable PDE solvers: With the advent of deep learning, there has been a recent rise in the development
of differentiable programming libraries for physics simulations [35, 36]. Most often, the physics phenomena are
represented using partial differential equations (PDEs) [37, 38]. Considerable effort has gone into designing physics-
informed loss functions [39–41] whose optimization leads to desired solutions for PDEs. Due to space limitations, we
defer to a detailed survey of this (vast) area by Cai et al. [42].

2 Differentiable Spline Approximation

We now introduce our framework, Differentiable Spline Approximation (DSA), as an approach to estimate gradients
over piecewise polynomial operations. Our main goal will be to estimate easy-to-compute forms of the (weak) Jacobian
for several spline approximation problems, enabling their use within backward passes in general differentiable programs.
Setup. We begin with some basic deﬁnitions and notation. Let f ∈ Rn be a vector where the ith element is denoted
as fi. Let us use [n] = {1, 2, . . . , n} to denote the set of all coordinate indices. For a vector f ∈ Rn and an index set
I ⊆ [n], let fI be the restriction of f to I, i.e., for i ∈ I, we have fI (i) := fi, and fI (i) := 0 for i /∈ I. Now, consider
any ﬁxed partition of [n] into a set of disjoint intervals I = {I1, . . . , Ik} where the number of intervals |I| = k. The
(cid:96)2-norm of f is written as (cid:107)f (cid:107)2 := (cid:112)(cid:80)n
We ﬁrst deﬁne the notion of a discretized k-spline. Note that the use of “spline" here is non-standard and somewhat
more general than what is typically encountered in the literature. (Indeed, the spline concept used in computer graphics
is a special instance of this deﬁnition; we explain further below.)

i while the (cid:96)2 distance between f, g is written as (cid:107)f − g(cid:107)2.

i=1 f 2

2

Def. 2.1 (Discretized k-spline). A vector h ∈ Rn is called a discretized k-spline with degree d if: (i) there exists a
partition of [n] into k disjoint intervals I1, . . . , Ik; (ii) within each interval Ii, the coefﬁcients of hj, j ∈ Ii, can be
perfectly interpolated by some polynomial function of degree d.

Let us illustrate this by an example. Suppose that d = 1 and k = 5. Then, h is a discretized k-spline with degree d if,
in a “line plot” of the vector h (i.e., we interpolate the 2D points (j, hj) for all j ∈ [n]), we see up to k = 5 distinct
linear pieces. A different way to interpret this deﬁnition is that we start with a piecewise degree-d polynomial function
H : R → R with k = 5 pieces (with suitably deﬁned knot points, which are the location of the intervals I), and evaluate
H at any n equally spaced points in its domain. This gives us a vector h ∈ Rn, which we call a discretized k-spline.
In contrast with traditional splines, we allow H to be arbitrarily deﬁned at the knot points and require no speciﬁc
continuity or differentiability properties. Therefore, our deﬁnition encompasses all standard spline families (including
interpolating/approximating splines such as smoothing-, cubic-, and B-splines).

2.1 Spline Approximation

Our focus in this paper is the problem of computing the best possible spline ﬁt to a given set of data points (where both
the parameters of the spline as well as the knot vectors are allowed to be variable).
We provide an algebraic interpretation of this problem. For a given vector space Rn, consider Sk
d , the set of all
discretized k-splines with degree d. Since (standard) splines are vector spaces for a ﬁxed set of knots, one can easily see
that for any ﬁxed partition of [n] into k subsets, the family of discretized k-splines is a k(d + 1)-dimensional subspace
of Rn. Now suppose that the knot indices are allowed to vary. The number of possible partitions is ﬁnite (of the order
of (cid:0)n
Therefore, the problem of discretized k-spline approximation can be viewed as an orthogonal projection onto this
nonlinear manifold. Consider any arbitrary vector x ∈ Rn (we can think of (i, xi) as a set of n data points to which
we are trying to ﬁt a k-spline). Then, the best k-spline ﬁt to x (in the sense of (cid:96)2 distance) amounts to solving the
optimization problem:

d is a ﬁnite union of subspaces, or a nonlinear submanifold, embedded in Rn.

(cid:1)), and therefore the set Sk

k

F (x) = arg min

h

1
2

(cid:107)x − h(cid:107)2

2 =

1
2

n
(cid:88)

(xi − hi)2 s.t. h ∈ Sk
d

i=1

(1)

This operation resembles standard spline regression. But it is strictly more general since this requires not only optimizing
piecewise spline parameters but also the knot indices. Crucially, we note that F is both a non-differentiable and a
non-convex map. Nevertheless, such an orthogonal projection can be computed in polynomial (in fact, nearly-linear)
time [43, 44] using many different techniques, including dynamic programming. This forms the forward pass of our
DSA “layer”.

Our ﬁrst main conceptual contribution is a formal derivation of the backward pass of the orthogonal projection operation.
Strictly speaking, the Jacobian is not well-deﬁned due to the non-differentiable nature of the forward pass (owing to the
non-differentiability built into the deﬁnition of the k-spline). Therefore, we will instead be deriving the so-called “weak”
form of the Jacobian (borrowing terminology from Blondel et al. [14]).

We leverage two properties of the projection operation: (1) the output of the forward pass h corresponds to a partition
of [n], that is, each element of hj corresponds to a single interval, Ij, and (2) within each interval, the least-squares
operation is continuous and differentiable. The ﬁrst property ensures that every element xi contributes to only a single
piece in the output h. Given that the sub-functions from the piecewise partitioning function are smooth, we also observe
that the size of each block corresponds to the size of the partition, Ii. Using this observation, we get:

Theorem 1. The Jacobian of the operation F with respect to x ∈ Rn can be expressed as a block diagonal matrix,
J ∈ Rn×n, whose (s, t)th entry obeys:

Jx(F (x))(s, t) =

∂h(x)s
∂xt

=

(cid:40) ∂hIi (x)s
∂xt

0

if s, t ∈ Ii
otherwise

(2)

As a concrete instantiation of this result, consider the case d = 0. This is the case where we wish to best approximate
the entries of x with at most k “horizontal” pieces, where the break-points are obtained during the forward pass3. Call

3In the data summarization literature, this class of functions is sometimes called k-histograms [43]

3

this approximation h. Then, the Jacobian of h with respect to x forms the block-diagonal matrix J ∈ Rn×n:

J =







J1
0
...
0

0
J2
...
0







0
. . .
0
. . .
...
. . .
. . . Jk

(3)

where all entries of each block, Ji ∈ R|Ii|×|Ii| are constant and equal to 1/|Ii|, i.e., they are row/column-stochastic.
Note that the sparse structure of the Jacobian allows for fast computation and that computing the Jacobian vector
product JT ν for any input ν requires O(n) running time. As an additional beneﬁt, the decoupling induced by the
partition enables further speed up in computation via parallelization. See the Appendix for proofs, as well as derivations
of similar Jacobians for k-spline approximation of any degree d ≥ 1, and generalization to 2D domains (surface
approximation). In Section 3 we demonstrate the utility of this approach for a 2D segmentation (i.e., piecewise constant
approximation) problem, similar to the setting studied in [17].

2.2 Differentiable NURBS
We now switch to a slightly different setting involving a special spline family known as non-uniform rational B-splines
(NURBS), which are common in geometric modeling. Mathematically, a NURBS curve is a continuous function
C : R → R deﬁned as follows. Construct any knot vector u (i.e. a non-decreasing sequence of real coordinate values)
: R → R computed using the Cox-de Boor
and ﬁx degree d. Recursively deﬁne a sequence of basis functions, N d
i
formula:

N d

i (u) =

u − ui
ui+d − ui

N d−1
i

(u) +

ui+d+1 − u
ui+d+1 − ui+1

N d−1

i+1 (u), N 0

i (u) =

(cid:26) 1
0

if ui ≤ u ≤ ui+1
otherwise

(4)

for d = 1, 2, . . .. In the uniform case (where the knots are equally spaced), each N d
by recursively convolving a box function with N d−1
intuition is similar. With these basis functions in hand, the NURBS curve C is deﬁned as the rational function:
(cid:80)n

i can be viewed as being generated
. The non-uniform case cannot be written as a convolution, but the

i

C(u) =

,

(5)

i=0 N d
(cid:80)n
i=0 N d

i (u)wiPi
i (u)wi

where Pi, i = 0, 1, . . . , t are called control points and wi are corresponding non-negative weights. The number of
control points is related to the number of knots k and curve degree d as follows: k = t + d + 1. For simplicity, assume
that all weights are equal to one. The basis functions in NURBS add up to one uniformly for each u (this is called the
partition of unity property). Therefore:

t
(cid:88)

C(u) =

N d

i (u)Pi,

(6)

i=0
In summary, the NURBS curve is parametrically deﬁned via the control points and the knot positions. This discussion
is for 1D curves, but extension to higher-order surfaces is conceptually similar.

Consider implementing NURBS as a differentiable “layer” where the inputs are the knot positions and control points.
The forward pass through this layer simply consists of evaluating Equation 6 via the recursive Equation 4, and storing
the various basis functions (and their spans) for further use.

However, the backward pass is a bit more tricky, once again due to the non-differentiable nature of C. The gradient
with respect to the control point coordinates, P is immediate (since the mapping from P to C is linear). However, the
gradient with respect to the knot positions, ui, is not well-deﬁned due to the non-differentiable nature of the base cases
of the recursion (which are box functions speciﬁed in terms of ui). Once again, we see that the non-differentiability of
NURBS is built into its very deﬁnition, and this affects numerics.

To resolve this, we propose the following approach to compute an (approximate) Jacobian of C. The main source
of the issue is the derivative of the box-car function N 0
i (u) = 1[ui,ui+1) with respect to the knot points, which is not
well deﬁned. However, N 0
i (u) can be viewed as the difference between convolutions of the unit step function with
δui and δui+1, where δ is the Dirac delta deﬁned over the real line. We smoothly approximate the delta function by
a Gaussian function with small enough bandwidth hyperparameter σ: δ(ui) ≈ g(u) = exp(−(u − ui)/2σ2). This
function is now differentiable with respect to ui, with g(cid:48)(u) = u−ui
σ2 g(u). Convolutions and differences are linear, and
hence the derivative is the basis function times a multiplicative factor. Finally, a similar approach as the Cox-de Boor
recursion (Equation 4) can be used to reconstruct the derivatives for all basis functions of higher order. See Algorithm 1
for pseudocode and the Appendix for details.

4

i , C(u) calculated during forward pass

Algorithm 1 Backward pass for NURBS Jacobian (for one curve point , C(u))
P(cid:48), U(cid:48): gradients of C w.r.t. P, U
Initialize: P(cid:48), U(cid:48) → 0
Retrieve uspan, N d
/* uspan is the index of knot position */
/* N d
/* C(u) is the evaluated curve point */
for h = 0 : d + 1 do
uspan+h = N d
uspan+h = N d

i is the basis function of degree d */

P(cid:48)
U(cid:48)

h // easy since C is a linear function of P.
h Uuspan+h // due to Gaussian approximation; see discussion below.

Let us probe the structure of this Jacobian a bit further. Suppose we evaluate the curve C at n arbitrary domain points.
There are slightly less than k control points, and therefore the Jacobian is roughly of size n × O(k). However, due to
the recursive nature of the deﬁnition of basis functions, the span (or support) of each basis function is small and only
touches d + 1 knots; for example, only 2 knots affect N 0
i , and so on. This endows a natural
sparse structure on the Jacobian. Moreover, for a ﬁxed order parameter d + 1, the span is constant [30]; therefore,
assuming evenly spaced evaluation points, we have the same number of nonzeros. Therefore, the Jacobian exhibits an
interesting Toeplitz structure (unlike the block diagonal matrix in the case of Equation 3), thereby enabling efﬁcient
evaluation during any gradient calculations. We show below in Section 3 that automatic differentiation using this
approach surpasses existing NURBS baselines.

i , only 3 knots impact N 1

2.3 Differentiable Finite Element PDE Solvers
Next, we see how spline approximations can be used to improve ﬁnite element analysis for solving PDEs. Popular
recent efforts for solving PDEs using autodiff construct “physics-informed” solvers [39, 40], while other efforts have
been made to utilize variational [41] or adjoint-based derivative methods [38]. However, these approaches come with
challenges while used in conjunction with autodiff packages, and gradient pathologies pose a major barrier [45].

Using our principles developed above, we propose an alternative PDE solution approach via differentiable ﬁnite elements.
PDE solvers based on Finite Element Methods (FEM) are ubiquitous, and we provide a very brief primer here. Consider
a domain Ω and a differential system of equations:

N [U(u)] = F (u),

(7)
where N denotes the differential operator and U : Ω → R is a continuous ﬁeld variable; it is common to specify
additional boundary constraints on U. The Galerkin method converts solving for the best possible U (which is a
continuous variable) into a discrete problem by ﬁrst looking at the weak form: R(U) = (cid:82)
Ω V [N (U) − F ] du, where
V is called a test function (and the weak form may involve some integration by parts), and rewriting this weak form in
terms of a ﬁnite set of basis coefﬁcients. A typical set of basis functions Φj is obtained by (piecewise) concatenation of
polynomials, each deﬁned over elements of a given partition of Ω (also called a mesh). Commonly used choices include
Lagrange polynomials, deﬁned by:

u ∈ Ω,

pr
i,d(u) =

d
(cid:88)

r=1

Ur

(cid:89)

0≤m≤d
m(cid:54)=r

u − um
ur − um

s.t. xr ∈ [−1, 1]

(8)

where {u0, u1, . . . , ud} are a ﬁnite set of nodes (akin to control points in our above discussion, except in this case the
splines interpolate the control points) and Ur is the corresponding coefﬁcient. We use this collection of basis functions
Φj to represent U:

U(u) =

#nodes
(cid:88)

j=1

Φj(u)Ud
j

(9)

and likewise for V . (The resemblance with Equation 5 above should be clear, and indeed NURBS basis functions could
be an alternative choice.) Plugging the discrete coefﬁcient representation Uc := {Uc
j} into the deﬁnition of R, we get a
standard Finite Element form,

R(Uc, V c) = B(Uc, V c) − L(V c)
(10)
where B(Uc, V c) is the discrete form (bilinear for linear operators) that encodes the differential operator and L(v) is a
linear functional involving the forcing function. For most PDE operators (including linear elliptic operators), one can

5

Original

Ground

Mbaseline

MDSA

Original

Ground

Mbaseline

MDSA

Figure 1: Segmentation results. The two models, MDSA and Mbaseline were trained with and without the DSA layer, respectively.
Note that MDSA generates better segmentation masks with fewer holes and enforced connectivity. Also note the cleaner edges
compared to the standard segmentation results. Additional ﬁgures are in the Appendix.

form the energy functional by using U as the test function:

J(Uc) =

1
2

B(Uc, Uc) − L(Uc).

(11)

Optimization of this energy functional can now be performed using gradient-based iterations evaluated by automatic
differentiation. This is powerful since formal techniques exist (e.g., Galerkin Least Squares [46]) that reformulate
weak forms of PDEs into equivalent energy functionals. The key aspect to note here is that differentiating “through"
the differential operator N (embedded within B) requires derivative computations of the piecewise polynomial basis
functions Φjs, and therefore our techniques developed above are applicable.

3 Experiments

We have implemented the DSA framework (and its different applications provided below) by extending autograd
functions in Pytorch. We also provide the capability to run the code using CUDA for GPU support. All the experiments
were performed using a local cluster with 6 compute nodes and each node having 2 GPUs (Tesla V100s with 32GB
GPU memory). All training were performed using a single GPU. Each experiment shown below is performed multiple
times with different random seeds, and the average value with error bars is provided. Due to limited space, we provide
three interesting applications of spline approximations here (see Appendix for additional examples).

Image segmentation: We begin with implementing a 2D piecewise constant splines regression approach for the image
segmentation problem using a UNet [47]. For differentiation, we use the formulation of splines discussed in Section 2.
We analyze the efﬁcacy of our approach by adding a piecewise constant DSA layer as the ﬁnal layer of our network
(MDSA). We compare this approach with the baseline model without the piecewise constant layer (Mbaseline).
We train two models (MDSA, Mbaseline) on two different segmentation tasks: the Weizmann horse dataset [48] and the
Broad Bioimage Benchmark Collection dataset [49] (publicly available under Creative Commons License). We split
both the Weizmann horse dataset and Broad Bioimage Benchmark Collection dataset into train and test with 85% and
15% of the dataset. We use binary cross-entropy error between the ground truth and the predicted segmentation map.
We use the same architecture and hyper-parameters for both models (see Appendix for details.)

We observe that our DSA layer provides more consistent segmentation maps and higher Jaccard scores than the baseline
model; see Figure 1. For the Weizmann horse dataset, Mconn enforces the connectivity of the segmented objects while
also limiting noise in the segmentation map. In the cell segmentation task, we note that the number of segments
is high while the objects are small. Since the size of the components is small, our DSA layer Jacobian exhibits
substantial differences from the commensurate identity gradient for the baseline models. Table 1 also shows the further
improvement in Jaccard score on cell segmentation tasks over the Weizmann horse dataset.

3D point cloud reconstruction using NURBS: Next, we provide results for two experiments using DSA with NURBS
discussed in Section 2.2. The ﬁrst application is surface ﬁtting for a complex benchmark surface represented by a
mesh of surface points obtained by evaluating the benchmark test function at these points. We use Bukin function N.6

6

Table 1: Results for the horse and cell segmentation dataset: Jaccard scores for the baseline and connected component models
for the cell and horse segmentation task. From independent three runs with random seeds and the table reports mean and standard
deviation. As the objects of interest (piecewise constant components) are smaller, the model with the DSA layer learns a better
representation. Predictions are thresholded at 0.5.

Dataset

Baseline (Mbaseline) Baseline + DSA (MDSA)

Weizmann Horse [48]
Broad Bioimage Benchmark [49]

72.06 ± 0.60
79.34 ± 0.43

73.13 ± 0.31
81.56 ± 0.24

Figure 2: NURBS surface ﬁtting results: Surface ﬁtting to point cloud generated using the Bukin’s function N.6 given by
z = 100(cid:112)|y − 0.01x2| + 0.01|x + 10|; −15 < x < −5, −3 < y < 3. The center image shows the surface ﬁt obtained without
reparameterization of the knots. We obtain better ﬁt by reparameterizing the knots.

Table 2: NURBS surface ﬁtting results: Comparison of mean squared error between the target surface point cloud and the surface
generated using DSA with and without reparameterization.

Number of Points MDSA(without reparameterization) MDSA(with reparameterization)

128 × 128
256 × 256

19.83 ± 0.001
19.85 ± 0.001

8.25 ± 0.01
8.23 ± 0.02

(publicly available here) for generating a grid of 256 × 256 points as shown on the left of Figure 2. For ﬁtting a NURBS
surface from the deﬁned target point cloud, we initialize a uniform clamped knot vector for a cubic basis function
and random control points of size 8 × 8 points. Using DSA, we evaluate the NURBS surface for a uniform grid of
256 × 256 parametric points. We now evaluate the surface and use mean squared error for ﬁtting the surface point cloud
using NURBS. We consider two scenarios: (i) we do not update the knot vectors (i.e., no reparameterization), and (ii)
we compute the gradients for the knot vectors and allow for reparameterization (i.e., change of knot locations). We
provide the comparison of these scenarios in Table 2. We see that the reparameterization helps in reducing the error in
ﬁt by half. Also, we notice that the density of points evaluated has a very minimal impact on the performance (see more
details in Appendix). Visually, in Figure 2, we see that two knots in the “v” direction come close to each other around
0.06, enabling a sharp edge in the evaluated surface.

The next experiment we present involves surface reconstruction from point clouds using a graph convolutional neural
network and DSA for unsupervised training. We use the SplineNet method proposed by Sharma et al. [34] to be the
baseline for point cloud reconstruction using splines. SplineNet uses a dynamic graph convolutional neural network
(DGCNN) to predict the control points for a spline surface. The authors use a supervised control point loss to perform
the training and also include regularizations such as the Laplacian loss and a patch distance (using Chamfer distance)
loss. Instead, we perform this training in an unsupervised manner by not using the control points prediction loss and
only using DSA to evaluate the surface and then apply regularization of minimizing the Laplacian of the surface. Since
we can train this in an unsupervised manner, we can even use an arbitrary number of control points and are not restricted
to the target control points.

For a fair comparison, we use the same network, dataset, and hyperparameters as Sharma et al. [34] and change the loss
functions by removing the control point regression loss. For comparison, we compute the chamfer distance between the
input point cloud and the NURBS surface ﬁt by the DGCNN model (MDSA) (see Appendix for details of training). We
use the Spline Dataset, which is a subset of surfaces extracted from the ABC dataset (available for public use under this
license). In Table 3, we provide a comparison of chamfer distance obtained between the predicted surface points from
splines and the input point cloud for the test dataset. In our experiments, we observe that we get signiﬁcantly better

7

xyzSurfacePredicted NURBS (no reparameterization)Predicted NURBS (with reparameterization)0.0640.058-0.0050.005-0.0050.005Pointwise Normalized MSEPointwise Normalized MSETable 3: Point-cloud reconstruction results: Comparison between the model proposed by Sharma et al. [34] and its extension
using DSA (with different number of control points). We compare the two sided chamfer distance (scaled by 100) between the input
point cloud and the ﬁtted surface.

Experiment

Chamfer Distance

Mbaseline
(20 × 20)
1.18 ± 0.10

MDSA
(20 × 20)
0.03 ± 0.02

MDSA
(5 × 5)
0.14 ± 0.07

MDSA
(4 × 4)
0.02 ± 0.01

Table 4: Quantitative comparison of Solving PDEs: L2 Norm between the analytical exact solution uex and predicted u using
PINNs [39] and DSA with different degrees of the Lagrange polynomials.

Model

L2 Norm

P IN N

DSA (d = 1)

DSA (d = 2)

DSA (d = 3)

128 × 128
256 × 256

3.72 ± 0.20 E-4
2.63 ± 0.20 E-4

3.32 ± 0.05 E-5
2.57 ± 0.01 E-5

2.16 ± 0.04 E-5
2.79 ± 0.20 E-5

2.37 ± 0.10 E-5
2.59 ± 0.10 E-5

performance with fewer control points. This is because most of the surfaces in the dataset are simply curved surfaces
that can be easily ﬁt with fewer control points.

PDE based surrogate physics priors: Finally, we leverage DSA in the context of solving PDEs as a prior. In particular,
we consider the Poisson equation solved for u:

−∇ · (ν(x)∇u) = f (x) in D
u|∂D = 0

(12)
(13)

where D = [0, 1]2, a 2D square domain, ν is the diffusivity and f is the forcing function. We consider two experiments
here: (1) validation of our approach with an analytically known solution, and (2) extending this to learn the solutions
for the parametric Poisson equation parameterized using ν.
For the ﬁrst experiment, we set ν to 1 and the forcing f = f (x) = f (x, y) = 2π2 sin(πx) sin(πy), and minimize the
residual using the approach described in Section 2.3. We know that for this PDE and the conditions provided, the
exact solution is given by uex(x, y) = sin(πx) sin(πy). We compare our results (uDSA) with the exact solution uex.
Also, we perform this experiment with Lagrange polynomials of different degrees. Further, we compare our results
with results obtained using PINNs [39]. We obtain signiﬁcantly better performance (lesser (cid:96)2-error by an order of
magnitude) compared to PINNs, owing to more accurate gradients computed using our DSA approach. The performance
improvement with increase in degree of polynomial in lower resolutions is more pronounced than at higher resolutions.

Next, we present results for training a deep learning network with a prior for solving a parametric Poisson’s equation.
The input to the network are different diffusivity maps ν sampled from

ν(x; ω) = exp

(cid:33)

ωiλiξi(x)ηi(y)

(cid:32) m
(cid:88)

i=1

(14)

where ωi is an m-dimensional parameter, λ is a vector of real numbers with monotonically decreasing values arranged
1
in order; and ξ and η are functions of x and y respectively. We take m = 4, ω = [−3, 3]4 and λi =
i ) ,
(1+0.25a2
where a = (1.72, 4.05, 6.85, 9.82). Also ξi(x) = ai
2 cos(aiy) + sin(aiy). We

2 cos(aix) + sin(aix) and η(y) = ai

Figure 3: Learning a parametric family of PDE solutions: Poisson’s equation with log permeability coefﬁcients ω =
(−0.26, −0.77, −0.37, −0.92) in the Poisson’s equation.

8

0.03.00.01.0-0.010.010.01.01.00.0νuDSAugrounduDSA-ugroundgenerate several diffusivity maps by sampling this function with different values of ω. We use a UNet [47] that takes
these diffusivity maps and predicts the solution u, which is further optimized with the residual minimizing prior to
the Poisson’s equation. Thus, we obtain a trained neural network that predicts the solution ﬁeld u for any unknown
diffusivity maps from the data distribution. We provide the predicted result along with its comparison with traditional
numerical FEM results in Figure 3. Visually, we see both the predicted solution ﬁeld map (uDSA) and the actual
solution ﬁeld (uground) obtained using traditional numerical methods match each other. The right most image shows
the difference between both with the maximum deviation to be 0.01, showing the accuracy of our (easy-to-implement)
DSA-based FEM solver.

4 Broader Impact and Discussion

We introduce a principled approach to estimate gradients for spline approximations. Speciﬁcally, we derive the (weak)
Jacobian in the form of a block-sparse matrix based on the partitions generated by any spline approximation algorithm
(which serves as the forward pass). The block structure allows for fast computation of the backward pass, thereby
extending the application of differentiable programs (such as deep neural networks) to tasks involving splines. Our
methods show superior performance than the state-of-the-art curve ﬁtting methods by reducing the chamfer distance
by an order of magnitude and the mean squared error in the case of surface ﬁtting by a factor of two. Further, with
the application of our methods in ﬁnite element analysis, we show signiﬁcantly better performance compared to
state-of-the-art physics-informed neural networks.

Our method is quite generic and may impact applications such as computer graphics, physics simulations, and
engineering design. Care should be taken to ensure that these applications are deployed responsibly. Future works
include further algorithmic understanding of the inductive bias encoded by DSA layers, and dealing with splines having
a dynamically chosen number of parameters (control points and knots).

Acknowledgements

This work was supported in part by the National Science Foundation under grants CCF-2005804, LEAP-HI:2053760,
CMMI:1644441, CPS-FRONTIER:1954556, USDA-NIFA:2021-67021-35329 and ARPA-E DIFFERENTIATE:DE-
AR0001215. Any information provided and opinions expressed in this material are those of the author(s) and do not
necessarily reﬂect the views of, nor any endorsements by, the funding agencies.

References
[1] Michael J. Innes. Algorithmic differentiation. In Machine Learning and Systems, pages 1–12, 2020.

[2] Mike Innes, A. Edelman, K. Fischer, C. Rackauckas, E. Saba, V. B. Shah, and Will Tebbutt. A differentiable

programming system to bridge machine learning and scientiﬁc computing. ArXiv, abs/1907.07587, 2019.

[3] F. Schafer, M. Kloc, C. Bruder, and N. Lorch. A differentiable programming method for quantum control. ArXiv,

2020.

[4] Tzu-Mao Li, Michaël Gharbi, Andrew Adams, Frédo Durand, and Jonathan Ragan-Kelley. Differentiable
programming for image processing and deep learning in halide. ACM Transactions on Graphics, 37(4):1–13,
2018.

[5] J. Degrave, Michiel Hermans, J. Dambre, and F. Wyffels. A differentiable physics engine for deep learning in

robotics. Frontiers Neurorobotics, 13, 2017.

[6] Martin Alnæs, Jan Blechta, Johan Hake, August Johansson, Benjamin Kehlet, Anders Logg, Chris Richardson,
Johannes Ring, Marie E Rognes, and Garth N Wells. The FEniCS project version 1.5. Archive of Numerical
Software, 3(100), 2015.

[7] Tzu-Mao Li, Miika Aittala, Frédo Durand, and Jaakko Lehtinen. Differentiable Monte-Carlo ray tracing through

edge sampling. ACM Transactions on Graphics, 37(6):1–11, 2018.

[8] Wenzheng Chen, Huan Ling, Jun Gao, Edward Smith, Jaakko Lehtinen, Alec Jacobson, and Sanja Fidler. Learning
to predict 3D objects with an interpolation-based differentiable renderer. In Advances in Neural Information
Processing Systems, volume 32, pages 1–11, 2019.

[9] Gordon Kindlmann, Ross Whitaker, Tolga Tasdizen, and Torsten Moller. Curvature-based transfer functions for
direct volume rendering: Methods and applications. In IEEE Visualization, 2003. VIS 2003., pages 513–520.
IEEE, 2003.

9

[10] Markus H Gross, Lars Lippert, A Dreger, and R Koch. A new method to approximate the volume-rendering

equation using wavelet bases and piecewise polynomials. Computers & Graphics, 19(1):47–62, 1995.

[11] Charles Loop and Jim Blinn. Real-time GPU rendering of piecewise algebraic surfaces. In SIGGRAPH, pages

664–670. ACM, 2006.

[12] Thomas JR Hughes, John A Cottrell, and Yuri Bazilevs. Isogeometric analysis: CAD, ﬁnite elements, NURBS,
exact geometry and mesh reﬁnement. Comp. methods in Applied Mechanics and Engineering, 194(39-41):
4135–4195, 2005.

[13] Anantharam Balakrishnan and Stephen C Graves. A composite algorithm for a concave-cost network ﬂow problem.

Networks, 19(2):175–202, 1989.

[14] Mathieu Blondel, O. Teboul, Quentin Berthet, and Josip Djolonga. Fast differentiable sorting and ranking. ArXiv,

abs/2002.08871, 2020.

[15] Marco Cuturi, O. Teboul, and Jean-Philippe Vert. Differentiable ranking and sorting using optimal transport. In

Neural Information Processing Systems, 2019.

[16] A. Mensch and Mathieu Blondel. Differentiable dynamic programming for structured prediction and attention.

ArXiv, abs/1802.03676, 2018.

[17] Josip Djolonga and Andreas Krause. Differentiable learning of submodular models. In Adv. Neural Inf. Proc. Sys.

(NeurIPS), pages 1013–1023, 2017.

[18] A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and Z. Kolter. Differentiable convex optimization layers.

In Adv. Neural Inf. Proc. Sys. (NeurIPS), 2019.

[19] Boyang Deng, Kyle Genova, Soroosh Yazdani, Soﬁen Bouaziz, Geoffrey Hinton, and Andrea Tagliasacchi.
CvxNet: Learnable convex decomposition. In IEEE Conf. Comp. Vision and Pattern Recog. IEEE, 2020.

[20] Yoshua Bengio. Estimating or propagating gradients through stochastic neurons. ArXiv, abs/1305.2982, 2013.

[21] Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack Xin. Understanding straight-
through estimator in training activation quantized neural nets. In Proc. Int. Conf. Learning Representations (ICLR),
2019.

[22] Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic
differentiation in machine learning: a survey. J. Machine Learning Research, 18(153):1–43, 2018. URL
http://jmlr.org/papers/v18/17-468.html.

[23] Quentin Berthet, Mathieu Blondel, O. Teboul, Marco Cuturi, Jean-Philippe Vert, and Francis R. Bach. Learning

with differentiable perturbed optimizers. ArXiv, abs/2002.08676, 2020.

[24] Yujia Xie, Hanjun Dai, M. Chen, Bo Dai, Tuo Zhao, H. Zha, Wei Wei, and T. Pﬁster. Differentiable top-k operator

with optimal transport. ArXiv, abs/2002.06504, 2020.

[25] Hyunsung Lee, Yeongjae Jang, Jaekwang Kim, and Honguk Woo. A differentiable ranking metric using relaxed

sorting opeartion for top-k recommender systems. ArXiv, abs/2008.13141, 2020.

[26] Marin Vlastelica Poganˇci´c, Anselm Paulus, Vit Musil, Georg Martius, and Michal Rolinek. Differentiation
of blackbox combinatorial solvers. In Proc. Int. Conf. Learning Representations (ICLR), 2020. URL https:
//openreview.net/forum?id=BkevoJSYPB.

[27] Brandon Amos and J. Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks. ArXiv,

1703.00443, 2019.

[28] Sheroze Sheriffdeen, J. Ragusa, J. Morel, M. Adams, and T. Bui-Thanh. Accelerating PDE-constrained inverse

solutions with deep learning and reduced order models. ArXiv, abs/1912.08864, 2019.

[29] Ameya Joshi, Minsu Cho, Viraj Shah, B. Pokuri, Soumik Sarkar, Baskar Ganapathysubramanian, and Chinmay
Hegde. InvNet: Encoding geometric and statistical invariances in deep generative models. In Association for the
Advancement of Artiﬁcial Intelligence Conference, pages 1–8, 2020.

[30] Les Piegl and Wayne Tiller. The NURBS Book (2nd Ed.). Springer-Verlag, Berlin, Heidelberg, 1997. ISBN

3540615458.

10

[31] Ludovico Minto, Pietro Zanuttigh, and Giampaolo Pagnutti. Deep learning for 3D shape classiﬁcation based on
volumetric density and surface approximation clues. In VISIGRAPP (5: VISAPP), pages 317–324, 2018.

[32] Krystian Erwinski, Marcin Paprocki, Andrzej Wawrzak, and Lech M Grzesiak. Neural network contour error
predictor in CNC control systems. In 2016 21st International Conference on Methods and Models in Automation
and Robotics (MMAR), pages 537–542. IEEE, 2016.

[33] Matthias Fey, Jan Eric Lenssen, Frank Weichert, and Heinrich Müller. SplineCNN: Fast geometric deep learning
with continuous B-spline kernels. In Conference on Computer Vision and Pattern Recognition, pages 869–877,
2018.

[34] Gopal Sharma, Difan Liu, Subhransu Maji, Evangelos Kalogerakis, Siddhartha Chaudhuri, and Radomír Mˇech.

ParSeNet: A parametric surface ﬁtting network for 3D point clouds, 2020.

[35] Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan Ragan-Kelley, and Frédo Durand.
Difftaichi: Differentiable programming for physical simulation. arXiv preprint arXiv:1910.00935, 2019.

[36] Yi-Ling Qiao, Junbang Liang, Vladlen Koltun, and Ming C Lin. Scalable differentiable physics for learning and

control. arXiv preprint arXiv:2007.02168, 2020.

[37] Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter Battaglia. Learning
to simulate complex physics with graph networks. In International Conference on Machine Learning, pages
8459–8468. PMLR, 2020.

[38] Philipp Holl, Vladlen Koltun, and Nils Thuerey. Learning to control pdes with differentiable physics. arXiv

preprint arXiv:2001.07457, 2020.

[39] M. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: A deep learning framework for
solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational
Physics, 378:686 – 707, 2019. ISSN 0021-9991. doi: https://doi.org/10.1016/j.jcp.2018.10.045. URL http:
//www.sciencedirect.com/science/article/pii/S0021999118307125.

[40] Maziar Raissi and George Em Karniadakis. Hidden physics models: Machine learning of nonlinear partial

differential equations. Journal of Computational Physics, 357:125–141, 2018.

[41] Ehsan Kharazmi, Zhongqiang Zhang, and George Em Karniadakis. hp-VPINNs: Variational physics-informed
neural networks with domain decomposition. Computer Methods in Applied Mechanics and Engineering, 374:
113547, 2021.

[42] Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, and George Em Karniadakis. Physics-informed neural

networks (pinns) for ﬂuid mechanics: A review. arXiv preprint arXiv:2105.09506, 2021.

[43] H. V. Jagadish, Nick Koudas, S. Muthukrishnan, Viswanath Poosala, Kenneth C. Sevcik, and Torsten Suel. Optimal
histograms with quality guarantees. In Proc. of Int. Conference on Very Large Data Bases (VLDB), 1998.

[44] Jayadev Acharya, Ilias Diakonikolas, Chinmay Hegde, Jerry Zheng Li, and Ludwig Schmidt. Fast and near-optimal
algorithms for approximating distributions by histograms. In Proc. ACM SIGMOD-SIGACT-SIGAI Symp. on
Principles of Database Systems, 2015.

[45] Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient pathologies in physics-

informed neural networks. arXiv preprint arXiv:2001.04536, 2020.

[46] Pavel B Bochev and Max D Gunzburger. Least-squares ﬁnite element methods, volume 166. Springer Science &

Business Media, 2009.

[47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image
segmentation. In International Conference on Medical image computing and computer-assisted intervention,
pages 234–241. Springer, 2015.

[48] Eran Borenstein and Shimon Ullman. Learning to segment. In Euro. Conf. Comp. Vision, pages 315–328. Springer,

2004.

[49] Vebjorn Ljosa, Katherine L Sokolnicki, and Anne E Carpenter. Annotated high-throughput microscopy image sets

for validation. Nature methods, 9(7):637–637, 2012.

[50] Sudipto Guha, Nick Koudas, and Kyuseok Shim. Approximation and streaming algorithms for histogram

construction problems. ACM Trans. on Database Systems (TODS), 31(1):396–438, 2006.

11

[51] Kesheng Wu, Ekow Otoo, and Arie Shoshani. Optimizing connected component labeling algorithms. In Medical
Imaging 2005: Image Processing, volume 5747, pages 1965–1976. International Society for Optics and Photonics,
2005.

[52] Adarsh Krishnamurthy, Rahul Khardekar, Sara McMains, Kirk Haller, and Gershon Elber. Performing efﬁcient
NURBS modeling operations on the GPU. IEEE Transactions on Visualization and Computer Graphics, 15(4):
530–543, 2009.

Appendix

A Proofs and derivations
Theorem 1. The Jacobian of the operation F with respect to x ∈ Rn can be expressed as a block diagonal matrix,
J ∈ Rn×n, whose (s, t)th entry obeys:

Jx(F (x))(s, t) =

∂h(x)s
∂xt

=

(cid:40) ∂hIi (x)s
∂xt

0

if s, t ∈ Ii
otherwise

(15)

Proof. The proof follows similar arguments as in Proposition 4 from Blondel et al. [14].
Let I = {I1, I2, · · · , Ik} be k partitions induced by some H : R → R for some input, x ∈ Rn and h ∈ Rn be a vector
from n equally spaced evaluated H in its domain. Then, each element, xi uniquely belongs to some partition Ir.

Now,

Jx(F (x))(s, t) =

∂ (cid:80)k

j=1 h(x)s (cid:12) 1(s ∈ Ij)

(cid:40) ∂h(x)s
∂xt

0

=

∂xt

if s, t ∈ Ir
otherwise

Note that this is a block-diagonal matrix with each block being |Ir| × |Ir|, giving us the required statement.

B Application of DSA to piecewise polynomial regression

1D piecewise constant regression: We ﬁrst provide the notations we provided in Section 2.
Let f ∈ Rn be a vector where the ith element is denoted as fi. Let us use [n] = {1, 2, . . . , n} to denote the set of all
coordinate indices. For a vector f ∈ Rn and an index set I ⊆ [n], let fI be the restriction of f to I, i.e., for i ∈ I,
we have fI (i) := fi, and fI (i) := 0 for i /∈ I. Now, consider any ﬁxed partition of [n] into a set of disjoint intervals
I = {I1, . . . , Ik} where the number of intervals |I| = k. The (cid:96)2-norm of f is written as (cid:107)f (cid:107)2 := (cid:112)(cid:80)n
i while the
(cid:96)2 distance between f, g is written as (cid:107)f − g(cid:107)2. Finally, 1I ∈ {0, 1}n is a indicator vector where for i ∈ I, 1I (i) = 1
and for i /∈ I, 1I (i) = 0.
We consider the case of k-piecewise regression in 1D, where we can use any algorithm to approximate a given input
vector with a ﬁxed number of piecewise polynomial functions. The simplest example is that of k-piecewise constant
regression, where a given input vector is approximated by a set of constant segments.
Formally, consider a piecewise constant function H : R → R with k pieces. Similar to spline, we evaluate H at any
n equally spaced points in its domain. This gives us a vector h ∈ Rn, which we call a k-piecewise constant vector.
Since the best (in terms of (cid:96)2-norm) constant approximation to a function is its mean, a k-piecewise constant function
approximation can be reparameterized over the collection of all disjoint intervals I = {I1, . . . , Ik} of [n] such that
given x:

i=1 f 2

min
I1,...,Ik

n
(cid:88)

k
(cid:88)

i=1

j=1

(hIj (i) − xi)2 = min
I1,...,Ik

k
(cid:88)

(cid:88)

j=1

i∈Ij

(

1
|Ij|

(cid:88)

l∈Ij

xl − xi)2

(16)

We assume an optimal H (parameterized by {Ii} that can be obtained using many existing methods (a classical approach
by dynamic programming [43]). The running time of such approaches is typically O(nk), which is constant for ﬁxed k;
see Acharya et al. [44] for a more detailed treatment.

12

Using Theorem 1, the Jacobian of the output k-histogram with respect to x assumes the following form:

∂h
∂xi

=

∂
∂xi

k
(cid:88)

j=1

(

1
|Ij|

(cid:88)

l∈Ij

xl)) =

=

∂
∂xi

k
(cid:88)

j=1

k
(cid:88)

j=1

(

1
|Ij|

(

(cid:88)

l∈Ij

xl)1Ij )

∂
∂xi

1
|Ij|

(cid:88)

(
l∈Ij

1Ij ) =

1
|Ij|

1Ij

(17)

(18)

Therefore, the Jacobian of h with respect to x forms the block-diagonal matrix J ∈ Rn×n:

J =







J1
0
...
0

0
J2
...
0







. . .
0
. . .
0
...
. . .
. . . Jk

where all entries of Ji ∈ R|Ii|×|Ii| equal to 1/|Ii|. Note here that the sparse structure of the Jacobian allows for fast
computation, and it can be easily seen that computing the Jacobian vector product JT ν for any input ν requires O(n)
running time. As an additional beneﬁt, the decoupling induced by the partition enables further speed up in computation
via parallelization.

Generalization to 1D piecewise polynomial ﬁtting: We now derive differentiable forms of generalized piecewise
d-polynomial regression, which is used in applications such as spline ﬁttings.
As before, H : R → R is any algorithm to compute the k-piecewise d polynomial approximation of an input vector
x ∈ Rd that outputs partition I = {I1, . . . , Ik}. Similarly, the function H gives us a vector h ∈ Rn, a k-piecewise
polynomial vector. Then, for each partition, we are required to solve a d-degree polynomial regressions. Generally, the
polynomial regression problem is simpliﬁed to linear regression by leveraging a Vandermonde matrix. We get a similar
closed-form expression for the coefﬁcient as in Section 2.2.
Assume that for partition Ij, the input indices tIj (i) is ith element in an index vector corresponding to the Ij partition.
Then, the input indices tIj (i) are represented as a Vandermonde matrix, VIj :

VIj =


1
1


...


1

tIj (1)
tIj (2)
...
tIj (|Ij|)

tIj (1)2
tIj (2)2
...
tIj (|Ij|)2

· · ·
· · ·
. . .
· · ·








tIj (1)d
tIj (2)d
...
tIj (|Ij|)d

.

It can be shown that the optimal polynomial coefﬁcient αIj corresponding to the partition (or disjoint interval) Ij have
the following closed form:

αIj = (VT
Ij

VIj )−1VT
Ij

xIj ,

where xIj ∈ R|Ij | is a vector x length of |Ij| corresponding to the Ij partition such that xIj (i) = xi if i ∈ Ij and
undeﬁned if i /∈ Ij. This can be computed in O(kndw) time where w is the matrix-multiplication exponent [50]. Then
using Theorem 1 and the gradient for polynomial regression, the Jacobian of hIj with respect to x forms a blockwise
sparse matrix:

((cid:104)(VT
Ij

VIj )−1VT
Ij

xIj , [VT
Ij

]s(cid:105))

∂hIj (s)
∂xl

=

=

=

((cid:104)αIj , [VT
Ij

∂
∂xl
∂
[VT
]T
s (VT
Ij
Ij
∂xl
(cid:40)(cid:2)VIj (VT
0

Ij

]s(cid:105)) =

∂
∂xl
VIj )−1VT
Ij

xIj

VIj )−1[VT
Ij

])s

(cid:3)
l

if l, s ∈ Ij
otherwise.

The two main takeaways here are as follows: (1) VIi can be precomputed for all possible n − 1 partition sizes,
thus allowing for fast (O(n)) computation of Jacobian-vector products; and (2) an added ﬂexibility is that we can
independently control the degree of the polynomial used in each of the partitions. The second advantage could be very
useful for heterogeneous data as well as considering boundary cases in data streams.

13

B.1 2D piecewise constant functions
Our 1D piecewise spline approximation can be (heuristically) extended to 2D data. We provide the detailed descriptions.
We consider the problem of image segmentation, which can be viewed as representing the domain of an image into a
disjoint union of subsets. Neural-network based segmentation involves training a model (deep or otherwise) to map the
input image to a segmentation map, which is a piecewise constant spline function. However, standard neural models
trained in a supervised manner with image-segmentation map pairs would generate pixel-wise predictions, which
could lead to disconnected regions (or holes) as predictions. We leverage our approach to enforce deep models to
predict piecewise constant segmentation maps. In case of 2D images, note that we do not have a standard primitive (for
piecewise constant ﬁtting) to serve as the forward pass. Instead, we leverage connected-component algorithms (such as
Hoshen-Kopelman, or other, techniques [51]) to produce a partition, and the predicted output is a piecewise constant
image with values representing the mean of input pixels in the corresponding piece. For the backward pass, we use a
tensor generalization of the block Jacobian where each partition is now represented as a channel which is only non-zero
in the positions corresponding to the channel. Formally, if the image x ∈ Rn is represented as the union of k partitions,
h = (cid:83)k

i=1 Ii, the Jacobian, Jx = ∂h/∂x ∈ Rn×n and,

Jx(F (x))(s, t) =

(cid:40) ∂h(x)s
∂xt

= 1
|Ii|

0

if s, t ∈ Ii,
otherwise.

(19)

Note that Ii here no longer correspond to single blocks in the Jacobian. Here, they will reﬂect the positions of
pixels associated with the various components. However, the Jacobian is still sparsely structured, enabling fast vector
operations.

C Implementing DSA with NURBS

C.1 Backward evaluation for NURBS surface
In a modular machine learning system, each computational layer requires the gradient of a loss function with respect
to the output tensor for the backward computation or the backpropagation. For our NURBS evaluation layer this
corresponds to ∂L/∂S . As an output to the backward pass, we need to provide ∂L/∂Ψ. While we represent S for the
boundary surface, computationally, we only compute S (the set of surface points evaluated from S). Therefore, we
would be using the notation of ∂S instead of ∂S to represent the gradients with respect to the boundary surface. Here,
we make an assumption that with increasing the number of evaluated points, ∂S will asymptotically converge to ∂S.
Now, we explain the computation of ∂S/∂Ψ in order to compute ∂L/∂Ψ using the chain rule. In order to explain the
implementation of the backward algorithm, we ﬁrst explain the NURBS derivatives for a given surface point with
respect to the different NURBS parameters.

C.2 NURBS derivatives
We rewrite the NURBS formulation as follows:

S(u, v) =

NR(u, v)
w(u, v)

(20)

where,

NR(u, v) =

n
(cid:88)

m
(cid:88)

i=0

j=0

N p

i (u)N q

j (v)wijPij

w(u, v) =

n
(cid:88)

m
(cid:88)

i=0

j=0

N p

i (u)N q

j (v)wij

For the forward evaluation of S(u, v) = f (P , U , V , W), we can deﬁne four derivatives for a given surface evaluation
point: S,u := ∂S(u,v)/∂u, S,v := ∂S(u,v)/∂v, S,P := ∂S(u,v)/∂P, and S,W := ∂S(u,v)/∂W. Note that, S,P and S,W are
represented as a vector of gradients {S,Pij ∀Pij ∈ P} and {Swij ∀wij ∈ W}. Now, we show the mathematical form of
each of these four derivatives. The ﬁrst derivative is traditionally known as the parametric surface derivative, S,u. Here,
N p

i,u(u) refers to the derivative of basis functions with respect to u.

S,u(u, v) =

NR,u(u, v)w(u, v) − NR(u, v)w,u(u, v)
w(u, v)2

(21)

where,

14

NR,u(u, v) =

n
(cid:88)

m
(cid:88)

N p

i,u(u)N q

j (v)wijPij

w,u(u, v) =

i=0

j=0

n
(cid:88)

m
(cid:88)

i=0

j=0

N p

i,u(u)N q

j (v)wij

A similar surface point derivative could be deﬁned for S,v. These derivatives are useful in the sense of differential
geometry of NURBS for several CAD applications [52]. However, since many deep learning applications such as
surface ﬁtting are not dependent on the (u, v) parametric coordinates, we do not use it in our layer. Also, note that S,u
and S,v are not the same as S,U and S,V. A discussion about S,U and S,V is provided later in this section. Now, let us
deﬁne S,pij (u, v).

S,Pij (u, v) =

(cid:80)n

k=0

N p
(cid:80)m

i (u)N q
l=0 N p

j (v)wij
k (u)N q

l (v)wkl

(22)

S,Pij (u, v) is the rational basis functions themselves. Computing S,wij (u, v) is more involved with wij terms in both
the numerator and the denominator of the evaluation.

S,wij (u, v) =

NR,wij (u, v)w(u, v) − NR(u, v)w,wij (u, v)
w(u, v)2

(23)

where,

NR,wij (u, v) = N p

i (u)N q

j (v)Pij

w,wij (u, v) = N p

i (u)N q

j (v)

C.3 Derivatives with respect to knot points
For simplicity, we will stick to 1D NURBS curves. The extension to 2D surfaces is straightforward by taking Kronecker
products.

We recall the deﬁnition of the NURBS basis:

N d

i (u) =

u − ui
ui+d − ui

N d−1
i

(u) +

ui+d+1 − u
ui+d+1 − ui+1

N d−1

i+1 (u), N 0

i (u) =

(cid:26) 1
0

if ui ≤ u ≤ ui+1
otherwise

(24)

The goal is to evaluate the derivative of N d
nature of the deﬁnition, we can accordingly compute the derivatives of N d
provided we can evaluate:

i (u) with respect to the knot points {ui}. We observe that due to the recursive
i (u) in a recursive fashion using chain rule,

∂N 0
i (u)
∂ui

=

∂1([ui, ui+1])
∂ui

(and likewise for ui+1) where 1 denotes the indicator function over an interval. However, this derivative is not
well-deﬁned since the gradient is zero everywhere and undeﬁned at the interval edges.

We propose to approximate this derivative using Gaussian smoothing. Rewrite the interval as the difference between
step functions convolved with deltas shifted by ui and ui+1 respectively:

and approximate the delta function with a Gaussian of sufﬁciently small (but constant) bandwidth:

1([ui, ui+1))(u) = sign(u) (cid:63) δ(u − ui) − sign(u) (cid:63) δ(u − ui+1)

1([ui, ui+1])(u) = sign(u) (cid:63) Gσ(u − ui) − sign(u) (cid:63) Gσ(u − ui+1)

where

The derivative with respect to µ is therefore given by:

Gσ(u − µ) =

√

1
2πσ2

exp(−

(u − µ)2
2σ2

).

G(cid:48)

σ(u = µ) =

(u − µ)
2σ2 Gσ(u − µ),

which means that the approximate gradient introduces a multiplicative (u − µ) factor with the original basis function.
Propagating this through the chain rule and applying a similar strategy as Cox-de Boor recursion gives us Algorithm 1.

15

D Experimental details

D.1 Segmentation
Weizmann Horse dataset: The dataset consists of 378 images of single horses with varied backgrounds and their
corresponding ground truth. We divide the dataset into 85:15 ratios for training and testing, respectively. Further, each
image is normalized to a [0, 1] domain by dividing it by 256. 5443

Cell dataset: The dataset consists of 19K gray-scale images containing various cells, and we take 1900 subset images
as the dataset. We divide the dataset into 85:15 ratios for training and testing, respectively. Similarly, we normalize the
image to a [0, 1] by dividing each pixel by 256.

Architecture and training: We use the following U-Net architecture for training our segmentation networks. While
we use the equivalent model skeleton reported by Ronneberger et al. [47], we scale down the network size starting the
initial channels C = 8 (default channel is C = 64). In both dataset, we train the network 1000 epochs with initial
learning rate 0.0003. We leverage Adam optimizer with β = (0.9, 0.999) and weight decay 0.0001. We use a binary
cross entropy loss function as the objective function.

D.2 NURBS surface ﬁtting implementation
The complete algorithm for forward evaluation of S(u, v) as described in Piegl and Tiller [30] can be divided into three
steps:

1. Finding the knot span of u ∈ [ui, ui+1) and the knot span of v ∈ [vj, vj+1), where ui, ui+1 ∈ U and

vj, vj+1 ∈ V. This is required for the efﬁcient computation of only the non-zero basis functions.

2. Now, we compute the non-zero basis functions N p

j (v) using the knot span. The basis functions
have speciﬁc mathematical properties that help us in evaluating them efﬁciently. The partition of unity and
the recursion formula ensures that the basis functions are non-zero only over a ﬁnite span of p + 1 control
points. Therefore, we only compute those p + 1 non-zero basis functions instead of the entire n basis function.
Similarly in the v direction we only compute q + 1 basis functions instead of m.

i (u) and N q

3. We ﬁrst compute the weighted control points Pw

ij for a given control point Pij = {Px, Py, Pz} and weight
wij as {Pxw, Pyw, Pzw} representing the surface after homogeneous transformation for ease of computation.
Once the basis functions are computed we multiply the non-zero basis functions with the corresponding
ij. This result, S(cid:48) is then used to compute S(u, v) as {S(cid:48)
weighted control points, Pw

w, S(cid:48)

w, S(cid:48)

x/S(cid:48)

y/S(cid:48)

z/S(cid:48)

w}.

Algorithm 2 Forward algorithm for multiple surfaces
:U, V, P, W, output resolution ngrid, mgrid
Input
Output :S
Initialize a meshgrid of parametric coordinates

uniformly from [0, 1] using ngrid × mgrid : ugrid × vgrid

Initialize: S → 0
for k = 1 : surf aces in parallel do

for j = 1 : mgrid points in parallel do

for i = 1 : ngrid points in parallel do

Compute uspan and vspan for the corresponding ui and vi using knot vectors Uk and Vk
Compute basis functions Ni and Nj basis functions using uspan and vspan and knot vectors Uk and Vk
Compute surface point S(ui, vj) (in x, y, and z directions).
Store uspan, vspan, Ni, Nj, and S(ui, vj) for backward computation

In a deep learning system, each layer is considered as an independent unit performing the computation. The layer takes a
batch of input during the forward pass and transforms them using the parameters of the layer. Further, in order to reduce
the computations needed during the backward pass, we store extra information for computing the gradients during the
forward computation. The NURBS layer takes as input the control points, weights, and knot vectors for a batch of
NURBS surfaces. We deﬁne a parameter to control the number of points evaluated from the NURBS surface. We deﬁne
a mesh grid of a uniformly spaced set of parametric coordinates ugrid × vgrid. We perform a parallel evaluation of
each surface point S(u, v) in the ugrid × vgrid for all surfaces in the batch and store all the required information for
the backward computation. The complete algorithm is explained in Algorithm 2. Our implementation is robust and
modular for different applications. For example, if an end-user desires to use this for a B-spline evaluation, they need
to set the knot vectors to be uniform and weights W to be 1.0. In this case, the forward evaluation can be simpliﬁed
to S(u, v) = f (P). Further, we can also pre-compute the knot spans and basis functions during the initialization of

16

Figure 4: UNet architecture used for training

the NURBS layer. During computation, we could make use of tensor comprehension that signiﬁcantly increases the
computational speed. We can also handle NUBS (Non-Uniform B-splines), where the knot vectors are still non-uniform,
but the weights W are set to 1.0. Note in the case of B-splines Ψ = {P} (the output from the deep learning framework)
and in the case of NUBS Ψ = {P, U, V}.

SplineNet training details: The SplineNet architecture made of a series of dynamic graph convolution layers,
followed by an adaptive max pooling and conv1d layers. We use the Chamfer distance as the loss function. The Chamfer
distance (LCD) is a global distance metric between two sets of points as shown below.

LCD =

(cid:88)

Pi∈P

min
Qj∈Q

||Pi − Qj||2 +

(cid:88)

Qj∈Q

min
Pi∈P

||Pi − Qj||2

(25)

For training and testing our experiments, we use the SplineDataset provided by Sharma et al. [34]. The SplineDataset is
a diverse collection of open and closed splines that have been extracted from one million CAD geometries included in
the ABC dataset. We run our experiments on open splines that are split into 3.2K, 3K, and 3K surfaces for training,
testing, and validation.

D.3 PDE solver implementation with DSA prior

Deep convolutional neural networks are a natural choice for the network architecture for solving PDEs due to the
structured grid representation of S d and similarly structured representation of U d
θ . The spatial localization of convolu-
tional neural networks helps in learning the interaction between the discrete points locally. Since the network takes an
input of a discrete grid representation (similar to an image, possibly with multiple channels) and predicts an output
of the solution ﬁeld of a discrete grid representation (similar to an image, possibly with multiple channels), this is
considered to be similar to an image segmentation or image-to-image translation task in computer vision. U-Nets [47]
have been known to be effective for applications such as semantic segmentation and image reconstruction. Due to its
success in diverse applications, we choose U-Net architecture for solving the PDE. The architecture of the network is
shown in Figure 4. First, a block of convolution and instance normalization is applied. Then, the output is saved for
later use during skip-connection. This intermediate output is then downsampled to a lower resolution for a subsequent
convolution block and instance normalization layers. This process is continued twice. The upsampling starts where the
saved outputs of similar dimensions are concatenated with the output of upsampling for creating the skip-connections
followed by a convolution layer. LeakyReLU activation was used for all the intermediate layers. The ﬁnal layer has a
Sigmoid activation.

D.3.1 Applying boundary conditions
The Dirichlet boundary conditions are applied exactly. The query result from U d
θ from the network pertains only to the
interior of the domain. The boundary conditions need to be taken into account separately. There are two ways of doing
this:

• Applying the boundary conditions exactly (this is possible only for Dirichlet conditions in FEM/FDM, and the

zero-Neumann case in FEM)

• Taking the boundary conditions into account in the loss function, thereby applying them approximately.

17

Figure 5: Solution to the linear Poisson’s equation with forcing. From left to right: f , uDSA, unum and (uDSA − unum). Here
unum is a conventional numerical solution obtained through FEM. Diffusivity ν = 1

We take the ﬁrst approach of applying the Dirichlet conditions exactly (subject to the mesh). Since the network
architecture is well suited for 2d and 3d matrices (which serve as an adequate representation of the discrete ﬁeld in
2D/3D on regular geometry), the imposition of Dirichlet boundary conditions amounts to simply padding the matrix
by the appropriate values. A zero-Neumann condition can be imposed by taking the “edge values" of the interior and
copying them as padding. A nonzero Neumann condition is slightly more involved in the FDM case since additional
equations need to be constructed, but if using FEM loss, this can be done with another surface integration on the relevant
boundary.

18

E Additional results

Original

Ground

Mbaseline

MDSA

Figure 6: Image Segmentation Tasks Adding DSA layers (MDSA) on top of U-Net (Mbaseline) improves the segmentation tasks on
both datasets.

19

Original

Ground

Mbaseline

MDSA

Figure 7: Additional cell segmentations results. Mbaseline and MDSA correspond to U-Net and U-Net+DSA layers, respectively.

20

