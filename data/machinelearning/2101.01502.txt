1
2
0
2

n
a
J

8
2

]

G
L
.
s
c
[

2
v
2
0
5
1
0
.
1
0
1
2
:
v
i
X
r
a

Control-Data Separation and Logical Condition
Propagation for Efﬁcient Inference on Probabilistic
Programs

Ichiro Hasuo, Yuichiro Oyabu, Clovis Eberhart∗
National Institute of Informatics
Tokyo 101-8430, Japan
i.hasuo@acm.org, oyabu82@gmail.com, eberhart@nii.ac.jp

Kohei Suenaga
Kyoto University
Kyoto 606-8501, Japan
ksuenaga@fos.kuis.kyoto-u.ac.jp

Kenta Cho, Shin-ya Katsumata
National Institute of Informatics
Tokyo 101-8430, Japan
{cho, s-katsumata}@nii.ac.jp

Abstract

We introduce a novel sampling algorithm for Bayesian inference on imperative
probabilistic programs. It features a hierarchical architecture that separates control
ﬂows from data: the top-level samples a control ﬂow, and the bottom level samples
data values along the control ﬂow picked by the top level. This separation allows
us to plug various language-based analysis techniques in probabilistic program
sampling; speciﬁcally, we use logical backward propagation of observations for
sampling efﬁciency. We implemented our algorithm on top of Anglican. The
experimental results demonstrate our algorithm’s efﬁciency, especially for programs
with while loops and rare observations.

1

Introduction

Probabilistic Programs In the recent rise of statistical machine learning, probabilistic programming
languages are attracting a lot attention as a programming infrastructure for data processing tasks.
Probabilistic programming frameworks allow users to express statistical models as programs, and
offer a variety of methods for analyzing the models.

Probabilistic programs feature randomization
and conditioning (also called observation or
evidence). Randomization can take different
forms, such as probabilistic branching (ifp
in Prog. 1) and assignment from a probability
distribution (denoted by ∼, see Prog. 2).

Prog. 1: coin

Prog. 2: obsLoop

bool c1 , c2 := true ;
ifp (0.36)

then c1 := true ;
else c1 := false ;

ifp (0.36)

then c2 := true ;
else c2 := false ;
observe (!( c1 = c2 ));
return ( c1 );

double x := 0;
double y := 0;
int n := 0;
while ( x < 3) {
n := n + 1;
y ∼ normal (1 ,1);
observe (0 <= y <= 2);
x := x + y ;}
observe ( n >= 5);
return ( n );

1
2
3
4
5
6
7
8
9
10

Conditioning makes the operational meaning
of probabilistic programs unique and distinct
from other programs. Here, the meaning of “execution” is blurry since some execution traces get
discarded due to observation violation. For example, in Prog. 1, in Line 8 we have (c1, c2) =
(true, true) with probability 0.362, which violates the observation in Line 8 and is thus discarded.

∗I.H. and Y.O. are also afﬁliated with SOKENDAI (The Graduate University for Advanced Studies), Tokyo,
Japan. The work was done when Y.O. was a student at National Inst. Inform. & SOKENDAI. I.H., Y.O. and C.E.
made equal contribution to the work.

Preprint

 
 
 
 
 
 
It is suitable to think of a probabilistic programming language as a modeling language: a probabilistic
program is not executed but is inferred on. Speciﬁcally, the randomization constructs in a program
deﬁne the prior distribution; it is transformed by other commands such as deterministic assignment;
and the semantics of the program is the posterior distribution, conditioned by the observation
commands therein. This Bayesian view makes probabilistic programming a useful infrastructure for
various statistical inference tasks. It poses a challenge to the programming language community, too,
namely to come up with statistical inference techniques that are efﬁcient, generic, and language-based.

The community’s efforts have produced a number of languages and their inference frameworks. Many
of them are sampling-based, such as Anglican [Tolpin et al., 2016], Venture [Mansinghka et al.,
2014], Stan [Carpenter et al., 2017], and Pyro [Bingham et al., 2019]; others feature symbolic and
exact inference, such as Hakaru [Shan and Ramsey, 2017], PSI [Gehr et al., 2016] and EfProb [Cho
and Jacobs, 2017]. Recent topics include ﬂexibility of a framework [Cusumano-Towner et al., 2019],
assistance by DNNs [Le et al., 2017], and use of variational inference [Bingham et al., 2019].

Approximate Inference by Sampling, and Its Challenges Exact inference methods apply mainly
to simple models with symbolically expressible posteriors. Therefore, in this paper, we pursue
sampling-based approximate inference. In doing so, we encounter the following two major challenges.

The ﬁrst is weight collapse, a challenge widely recognized in the community. In sampling a statistical
model, one typically sweeps the model with a number of particles (i.e. potential samples). However,
in case the model imposes rare observations, the particles’ weights (i.e. likelihoods) quickly decay to
zero, making the effective number of samples very small. See e.g. Tsay and Chen [2018].

Coping with weight collapse is an established topic in statistics, and a number of advanced methods
have been introduced. Two major classes of such methods are sequential Monte Carlo (SMC) and
Markov chain Monte Carlo (MCMC). SMC features resampling in the course of a sweep; MCMC
collects samples through a random walk suitably deﬁned with proposals and acceptance probabilities.

The second challenge is speciﬁc to probabilistic programs: the compatibility between advanced
sampling methods (such as SMC and MCMC) and control structures of probabilistic programs.
For example, Hur et al. [2015] observed that the implementations of Stan and R2—which are both
based on the MCMC algorithm in [Wingate et al., 2011]—can yield incorrect sampling results. The
fundamental issue here is the difﬁculty for MCMC walks to traverse different control ﬂows. See
also [Kiselyov, 2016] for further discussions.

This last challenge occurs also in the state-of-the-art framework Anglican [Tolpin et al., 2016].
Anglican offers a number of sampling methods (MCMC, SMC, etc.), but some advanced ones
disallow “non-global” observations. This roughly means the number of observations must not depend
on control ﬂows—Prog. 2 is disallowed since there is an observation in the while loop. Discontinuities
arising from different control ﬂows make it hard to employ gradient-based sampling methods, too
(such as Hamiltonian MC and variational inference). See e.g. van de Meent et al. [2018, §3.4.2].

Hierarchical Sampler via Control-Data Separa-
tion In order to alleviate the last challenge (com-
patibility between sampling and control), we pro-
pose a hierarchical sampling framework that sep-
arates control ﬂows and data values. See Fig. 1.

Multi-armed sampling of control ﬂows

estimated
likelihood of (cid:126)l

a control ﬂow (cid:126)l

Our framework consists of two levels of sampling:
the top level is for control ﬂows, and the bottom
one is for data values. The bottom level focuses on
a speciﬁc control ﬂow (cid:126)l that is chosen in the top
level. This way we free the data value sampling from the duty of dealing with different control ﬂows.

Figure 1: the hierarchical architecture

Sampling of data along the
straight-line program induced by (cid:126)l
(simpliﬁed by cond. propagation)

(cid:47)accumulate
data
samples

Another beneﬁt of our control-data separation is to ease the application of language-based, symbolic
and static techniques for sampling efﬁciency. In this paper, as the ﬁrst example, we use condition
propagation. Condition propagation, originally from R2 [Nori et al., 2014], propagates observations
upwards in a program, so that sample rejection happens earlier and unnecessary samples are spared.

Condition propagation is much like a weakest precondition calculus, a well-known technique in
program veriﬁcation (see e.g. [Winskel, 1993]). The original condition propagation in R2 is targeted
at arbitrary programs, which limits its applicability. For example, it requires explicit loop invariants
for condition propagation over while loops, but such loop invariants are hard to ﬁnd. In contrast, in

2

(cid:13)
(cid:13)
(cid:75)
(cid:75)
(cid:47)
our framework (Fig. 1), condition propagation is only applied to straight-line programs (without if
branchings or while loops), making the application of condition propagation easier and more robust.

In our framework, the top-level sampling is identiﬁed as what we call the inﬁnite-armed sampling
problem: from a (potentially inﬁnite) set of control ﬂows, we aim to draw samples according to
their likelihoods, but the likelihoods are unknown and we learn them as the sampling goes on. This
problem is a variation of the classic problem of multi-armed bandit (MAB), with the differences that
1) we sample rather than optimize, and 2) we have potentially inﬁnitely many arms. We show how
the well-known ε-greedy algorithm for MAB can be adapted for our purpose.

The bottom-level data sampling can be by various algorithms. We use SMC in our framework. This
is because SMC, unlike MCMC, can estimate the likelihood of a control ﬂow (cid:126)l via the average
weight of particles. This estimated likelihood is used in the top-level ﬂow sampling (Fig. 1). Use of
other algorithms for the bottom-level data sampling is future work. We expect that the control-data
separation will ease the use of gradient-based algorithms (such as Hamiltonian MC and variational
inference, [van de Meent et al., 2018]), since often each control ﬂow denotes a differentiable function.

Comparison with Zhou et al. [2020] In the context of approximate inference for probabilistic
programs, the idea of sampling from individual control ﬂows and combining the obtained samples
is already presented in the recent work by Zhou et al. [2020]. While the current work shares the
principle of control-data separation, principal differences from Zhou et al. [2020] are the following.

• One of our main claims is that control-data separation boosts the applicability of language-
based and symbolic reasoning techniques. We use condition propagation from Hur et al.
[2015] as the ﬁrst example of such techniques.

• Our top-level sampling systematically ﬁnds new control ﬂows by statically analyzing the

program code (its control-ﬂow graph to be precise).

Overall, our current incarnation of the idea of control-ﬂow separation emphasizes language-based
and static analysis of program code, while the one in Zhou et al. [2020] emphasizes techniques from
statistics. This difference in “backgrounds” makes the two frameworks suited to different classes of
probabilistic programs.

• Our current framework is particularly suited to programs in which many control ﬂows have
zero likelihood. These control ﬂows are identiﬁed to be logically infeasible by condition
propagation and then get blacklisted, preventing waste of data-sampling efforts.

• The framework in Zhou et al. [2020] is suited to programs in which many control ﬂows have
non-zero likelihood. For such programs, the hill-climbing behavior of their control ﬂow
sampler allows quick discovery of control ﬂows with dominant likelihood; see [Zhou et al.,
2020, §7.2]. The beneﬁt of condition propagation would be limited for such programs, since
control ﬂows with non-zero likelihood never gets logically blacklisted, no matter how small
its likelihood is.
Note, in turn, that the hill climbing-style control ﬂow sampler in Zhou et al. [2020] has
limited use for those programs which are suited for our framework. With many control ﬂows
forming a plateau of zero likelihood, their sampler cannot know which direction would lead
to a control ﬂow with non-zero likelihood.

Contributions Our main contribution is a hierarchical sampling algorithm (Fig. 1). Our theoretical
contributions include the mathematical derivation of the sampler—it uses SMC for both data sampling
and estimation of ﬂow likelihoods—and a convergence proof of multi-armed sampling (restricting to
ﬁnite arms). Our implementation is built on top of Anglican [Tolpin et al., 2016] and is called Schism
(for SCalable HIerarchical SaMpling). In our experimental comparison with Anglican, we observe
that Schism successfully addresses the challenge of compatibility between sampling and control. Its
performance advantage is especially with programs with while loops and rare observations. For those
programs, condition propagation successfully rules out many control ﬂows, allowing the sampler to
focus on those ﬂows whose prior likelihood is small but posterior likelihood is large.

Organization In §2 we introduce the syntax and semantics of our target language PROB. Condition
propagation is introduced there, too. We introduce the inﬁnite-armed sampling problem in §3.1,
and present our main algorithm as its reﬁnement in §3.2. In §4 we discuss our implementation and
experiments. Many details are deferred to the appendix, provided in a supplementary material.

3

x
c

∈ Var
::= constants, e.g. 0, −1.2 and true
uop ::= unary operators, e.g. “−” in −3
bop ::= binary operations, e.g. +, &&

ϕ
f

e

::= Boolean formulas
::= fuzzy predicates,

i.e. expressions that return
nonnegative real values

::= x | c | e1 bop e2 | uop e

expressions
(int, bool, double, etc.)

c

::=

skip
| x := e
| x ∼ Dist((cid:126)e)
| weight f
| c1; c2
| if ϕ then c1 else c2
| while(ϕ){c}

commands
skip
deterministic assignment
probabilistic assignment
soft conditioning by a fuzzy predicate
sequential composition
conditional branching
while loop

π ::= c ; return e

program

Figure 2: the probabilistic programming language PROB, syntax. The set Var of variables is ﬁxed.

Notations The sets of real numbers and nonnegative reals are R and R≥0, respectively. We follow
the statistics convention to write x1:N for a sequence x1, x2, . . . , xN . For a measurable space X, the
set of probability distributions over X is denoted by D(X). The set of subprobability distributions µ
over X—for which µ(X) is required to be ≤ 1 rather than = 1—is denoted by D≤1(X).

2 The Language PROB and pCFGs

2.1 Probabilistic Programming Language PROB

We use an imperative programming language whose syntax (Fig. 2) closely follows Gordon et al.
[2014]. We therefore use the same name PROB. The program in Prog.s 1 & 2 are examples. The
language PROB adds the following constructs to a usual imperative programming language.

The probabilistic assignment command x ∼ Dist((cid:126)e) samples a value from the designated distribution
and assigns it to the variable x. Here Dist speciﬁes a family of probability distributions (such as
normal, Bernoulli, etc.; they can be discrete or continuous), and (cid:126)e is a vector of parameters for the
family Dist, given by expressions for real numbers. An example is x ∼ normal(0, 1).

The probabilistic branching construct ifp p then c1 else c2 has a real number p ∈ [0, 1], and
chooses c1 and c2 with the probabilities p and 1 − p, respectively. We introduce this as a shorthand
for b ∼ Bernoulli(p); if b then c1 else c2, where b is a fresh Boolean variable.

Soft conditioning weight f is a primitive in PROB, as is common in probabilistic programming
languages. Here, f is a fuzzy predicate—a function that returns a nonnegative real number—that tells
how much weight the current execution should acquire.

We also use sharp conditioning observe ϕ—where ϕ is a Boolean formula rather than a fuzzy
predicate—in our examples (see e.g. Prog.s 1 & 2). This is a shorthand for weight 1ϕ, where 1ϕ is
the characteristic function of the formula ϕ (1ϕ returns 1 if ϕ is true, and 0 if ϕ is false).

Note that we allow only (sharp) Boolean formulas ϕ as guards of if branchings and while loops.

2.2 Probabilistic Control Flow Graphs (pCFGs)

We use the notion of probabilistic control ﬂow graph
(pCFG), adapted from [Agrawal et al., 2018], for pre-
senting PROB programs as graphs.
It is a natural
probabilistic adaptation of the conventional notion of
control ﬂow graphs for imperative programs.

x := 0, y := 0
n := 0

linit

x < 3 (cid:47)

l(2)

n :=
n + 1

l(3)

!(x < 3)

x :=
x + y

y ∼
normal(1, 1)

n

lﬁnal

observe
n ≥ 5

l(6)

l(5)

l(4)

observe
0 ≤ y ≤ 2

For space reasons, here we only show an example,
deferring the deﬁnition to Appendix A.1. A pCFG is
a ﬁnite graph, its nodes roughly correspond to lines of
a PROB program, and its edges have transition labels that correspond to atomic commands of PROB.
See Fig. 3: linit is the initial location; the label into linit speciﬁes the initial memory state σinit; lﬁnal
is the ﬁnal location; and the label out of lﬁnal (namely n) speciﬁes the return expression eﬁnal. The
translation from PROB programs to pCFGs is straightforward, following Agrawal et al. [2018]. Its
details are therefore omitted.

Figure 3: pCFG for obsLoop (Prog. 2).

4

(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:98)
(cid:98)
(cid:111)
(cid:111)
2.3 Semantics of pCFGs

We introduce formal semantics of pCFGs in a denotational style (see Winskel [1993] for basics).
We need semantics to formulate soundness of condition propagation (§2.5); we will also use the
semantics in the description of our sampling algorithm. We follow Staton et al. [2016] and introduce
two semantics: weighted state transformer and (unweighted, normalized) state transformer semantics.
They are both deﬁned in terms of memory states, that are assignments of values to variables.

The deﬁnitions of these semantics are by induction on the construction of programs, exploiting
the ω-cpo structure of D≤1X (the set of subprobability distributions, §1). The deﬁnitions are in
Appendix A.2. Here we focus on notations, types and illustrations, which should sufﬁce most readers.

The Weighted State Transformer

Given a pCFG Γ, the weighted semantics of Γ is of the type

Γ

w-st ∈ D≤1
(cid:75)

(cid:74)

(cid:0)R≥0 × D(cid:1),

(1)

where D is the value domain (R, Bool, . . . ) of the return expression eﬁnal. It is a subprobability
distribution of weighted samples, of the value of eﬁnal, obtained at the ﬁnal location lﬁnal, after an
execution of the program Γ that starts at linit with the memory state σinit. See Example 2.1.

(cid:74)

Γ
(cid:75)

w-st is randomized, since an execution of Γ has uncertainties that come
The weighted semantics
from the randomizing constructs x ∼ Dist((cid:126)e). It is a subprobability distribution (D≤1 in (1))—
meaning that the probabilities need not sum up to 1—since an execution of Γ may not terminate. See
e.g. [Morgan et al., 1996]. Note that violation of observations observe ϕ is recorded as the zero
weight (in R≥0 in (1)), rather than making the sample disappear.
The (Unweighted, Normalized) Semantics By normalizing the weighted semantics
i.e. by letting samples’ weights discount probabilities—we obtain the (unweighted) semantics

w-st in (1)—

Γ
(cid:75)

(cid:74)

st ∈ D(D),

where D is the value domain of the return expression eﬁnal.

(2)

Γ
(cid:74)

(cid:75)

This semantics is the posterior distribution (in the Bayesian sense) of the value of the return expression
eﬁnal in Γ. The distribution

st is therefore the one that we would like to sample from.

Example 2.1. The program coin in Prog. 1 emulates a fair coin using a biased one; the observe
command (Line 8) is crucial. Let Γ be the pCFG induced by it. The return value domain is D = Bool.
w-st = (cid:2)(0, true) (cid:55)→ 0.362, (1, true) (cid:55)→ 0.36 × 0.64, (1, false) (cid:55)→
The weighted semantics is
(cid:0)R≥0 × Bool(cid:1). Here the ﬁrst sample (0, true)—it
0.64 × 0.36, (0, false) (cid:55)→ 0.642(cid:3) ∈ D≤1
(cid:75)
comes from the memory state [c1 (cid:55)→ true, c2 (cid:55)→ true]—has a weight 0 due to its violation of
the observation !(c1 = c2). After normalization (which wipes out the contribution of the weighted
sample (0, true)), we obtain the unweighted semantics

st = [true (cid:55)→ 1/2, false (cid:55)→ 1/2].

Γ

(cid:74)

Γ
(cid:74)

(cid:75)

Γ
(cid:75)

(cid:74)

2.4 Control Flows and Straight-Line Programs

In our hierarchical architecture (Fig. 1), the top-level chooses a complete control ﬂow (cid:126)l of a pCFG,
which induces a straight-line program StrLn((cid:126)l), on which the bottom-level data sampler works.
These notions are deﬁned in Appendix A.3: intuitively, a control ﬂow is a path in a pCFG that starts
at the initial state linit; and a complete control ﬂow is one that ends at the ﬁnal location lﬁnal. In the
deﬁnition of the straight-line program StrLn((cid:126)l), the main point is to turn guards (for if branchings
and while loops) into suitable observations. The following example shall convey the ideas.
Example 2.2. For the pCFG in Fig. 3, the set of complete control ﬂows is {(cid:126)ln | n ≥ 0} where
l(6) lﬁnal. The ﬂow (cid:126)l1 induces the following straight-line program:
(cid:126)ln := linit

(cid:0)l(2)l(3)l(4)l(5)linit

(cid:1)n

StrLn((cid:126)l1) =

(cid:18) σinit−−−→ linit

−−−−−→ l(3) y∼normal(1,1)
observe x<3
−−−−−−−−→ l(2) n:=n+1
observe !(x<3)
−−−−−−−−−→ l(6) observe n≥5
−−−−−→ linit

−−−−−−−−−→ l(4) observe 0≤y≤2
−−−−−−−−−→
−−−−−−−−→ lﬁnal

n−→

l(5) x:=x+y

(cid:19)

Here σinit = [ x (cid:55)→ 0, y (cid:55)→ 0, n (cid:55)→ 0]. Note that the label x < 3 going out of linit in Fig. 3 is turned
into the observation command observe x < 3 in StrLn((cid:126)l1).

5

Prog. 3: cond-prop-demo
x ∼ unif (0 ,20) ;
while ( x < 10) {
y ∼ Beta (1 ,1);
x := x + y ;

}

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

x ∼ unif (0 ,20);

obs ( x < 10);
y ∼ Beta (1 ,1);

x := x + y ;
obs ( x < 10);
y ∼ Beta (1 ,1);

x := x + y ;
obs ( x < 10);
y ∼ Beta (1 ,1);

x := x + y ;
obs (10 ≤ x );

7 < x < 10
7 < x < 10, 0 ≤ x ≤ 20
7 < x < 10
7 < x < 10
8 < x + y < 10, 0 ≤ y < 1
8 < x + y < 10
8 < x < 10
8 < x < 10
9 < x + y < 10, 0 ≤ y < 1
9 < x + y < 10
9 < x < 10
9 < x
10 ≤ x + y, 0 ≤ y < 1
10 ≤ x + y
10 ≤ x

x ∼ unif (7 ,10);
weight (3/20)
// no observation
y ∼ Beta (1 ,1);
obs (8 < x + y < 10);
x := x + y ;
// no observation
y ∼ Beta (1 ,1);
obs (9 < x + y < 10);
x := x + y ;
// no observation
y ∼ Beta (1 ,1);
obs (10 ≤ x + y );
x := x + y ;
// no observation

Figure 4: condition propagation. In Prog. 3, let (cid:126)l be the control ﬂow that executes the loop exactly
three times. The corresponding straight-line program StrLn((cid:126)l) is the second column. The third
column illustrates condition propagation, which should be read from bottom to top. The last column
is the resulting program CdPg(StrLn((cid:126)l)), where changes are highlighted.

2.5 Condition Propagation and Domain Restriction

Condition propagation pushes observations upwards in a program, so that those samples which
eventually violate observations get ﬁltered away earlier. Its contribution to sampling efﬁciency is
demonstrated in R2 [Nori et al., 2014]. Our propagation rules are, similarly to R2, essentially the
weakest precondition calculus (see [Winskel, 1993]). Condition propagation is more universally
applicable here than in R2, since in our framework (Fig. 1), data sampling only concerns straight-line
programs. We do not need manual loop invariant annotations, for example, that are needed in R2.

In this paper, we also introduce a technique called domain restriction. It restricts a distribution to a
suitable domain, so that we never generate samples that are logically deemed unnecessary. Sampling
from domain-restricted distributions is easy to implement, via the inverse transform sampling.

The condition propagation operation is denoted by CdPg: it transforms a straight-line program into
another semantically equivalent straight-line program, applying domain restriction if possible. A
precise deﬁnition of CdPg is in Appendix A.4–A.5; for space reasons, we only show an example.

Example 2.3. See Fig. 4, where a straight-line program (Column 2) gets optimized into the one on
Column 4. Column 3 illustrates condition propagation: observations get propagated upwards.

Many propagation steps follow the weakest precondition calculus. For deterministic assignment
commands (Line 6, 10, 14), occurrences of x in the conditions are replaced by x + y. For observation
commands, we take the conjunction of the propagated condition and the observation there.

When we encounter a probabilistic assignment (say Line 12), we do the following: 1) we make
an observation of the condition so far (10 ≤ x + y in Line 13); 2) we collect the support of the
distribution as a new condition (0 ≤ y < 1 in Line 13); and 3) we compute a logical consequence ψ
of the last two conditions (ψ = (9 < x) in Line 12), and pass it further up.

For the probabilistic assignment in Line 1, we can moreover apply the domain restriction operation.
The propagated condition 7 < x < 10 allows us to restrict the original distribution unif(0,20)
to unif(7,10), as is done in Column 4. This way we spare generation of samples of x that are
eventually conditioned out. The idea is much like that of importance sampling; similarly, we
need to discount the weights of the obtained samples. This is done by the new weight command
weight(3/20) in Line 2, Column 4, where 3/20 is the constant fuzzy predicate that returns the
weight 3/20. See Appendix A.5 for the mathematical deﬁnition of the operation.

3 A Hierarchical Sampling Algorithm

The hierarchical sampling scheme (Fig. 1) arises from the equality (3). The left-hand side is the
probability we want—the probability of the return expression eﬁnal taking a certain value
σN ,
under the ﬁnal memory state σN . It is expressed using two nested integrals: the inner integral is over

eﬁnal

(cid:75)

(cid:74)

6

data samples σ1:N under a ﬁxed ﬂow l1:N ; and the outer integral is over control ﬂows l1:N .

eﬁnal

p(
(cid:74)

σN ) = (cid:82) (cid:16)(cid:82) p(
(cid:74)
(cid:75)

eﬁnal

σN | σN ) p(σ1:N | N, l1:N ) dσ1:N
(cid:75)

(cid:17)

p(N, l1:N ) dl1:N dN

(3)

The proof of (3) is in Appendix A.6. We estimate the two integrals by sampling. We use SMC for the
bottom-level data sampling, i.e. for the inner integral in (3). See §3.2. We ﬁrst discuss the top level.

3.1 The Inﬁnite-Armed Sampling Problem

The top-level control ﬂow sampling (Fig. 1) is formulated as inﬁnite-armed sampling (IAS), a problem
we shall now describe. Formal deﬁnitions, discussions and proofs are in Appendix A.8.

In the inﬁnite-armed sampling problem, inﬁnite arms {1, 2, . . . } are given, and each arm k has its
likelihood pk. Our goal is to sample arms, pulling one arm each time, so that the resulting histograms
converge to the distribution (p1, p2, . . . ). The challenge, however, is that the likelihoods p1, p2, . . .
are not know a priori. We assume that, when we pull an arm k at time t, we observe a random value
Xk,t whose mean is the (unknown) true likelihood pk. The random variables Xk,1, Xk,2, . . . are i.i.d.

Note that the IAS problem is an inﬁnite and sampling variant of the multi-armed bandit problem
(MAB). The goal in MAB is to optimize, while our goal is to sample. The IAS problem indeed
describes our top-level sampling in Fig. 1: a control ﬂow is an arm; there are countably many of them
in general; and the likelihood of each control ﬂow is only estimated by sampling the corresponding
straight-line program and measuring the weights of the samples. We come back to this in §3.2.

Algorithm 1 is our algorithm for the IAS problem. It is an adaptation of the well-known ε-greedy
algorithm for MAB. In each iteration, it conducts one of the following: (proportional sampling,
Line 9) sampling a known arm according to the empirical likelihoods; (random sampling, Line 7)
sampling a known arm uniformly randomly; and (expansion, Line 5) sampling an unknown arm
and making it known. In Line 13, the empirical likelihood ˆpk is updated using the newly observed
likelihood p, so that the result is the mean of all the likelihoods of k observed so far.

t

Comparing to the original ε-greedy algorithm for MAB, proportional sampling corresponds to the
exploitation action, while random sampling corresponds to the exploration action. The exploration
) 1
rate εt = ( K log t
3 in Algorithm 1 is the one commonly used for MAB. See e.g. [Slivkins, 2019].
We give a theoretical guarantee, restricting to the ﬁnite-armed setting. Its proof is in Appendix A.8.
Theorem 3.1 (convergence, ﬁnite-armed). In Algorithm 1, assume that K = {1, . . . , K}, Kknown is
initialized to K, and no expansion is conducted. Then Algorithm 1 satisﬁes, for each arm k ∈ K,
(cid:12)
(cid:12)

(cid:1) 1
4 (cid:1), where Tk(T ) = |{t | k(t) = k}|.

(cid:12)
(cid:12) = O(cid:0)K 7

− pk
(cid:80)

E(Tk(T ))
T

3 (cid:0) log T
T

k pk

We note that the above convergence speed is slower than in the MAB case (optimization, see [Slivkins,
2019]). Extension of the convergence theorem to inﬁnite arms is left as future work. It would require
some assumption on the distribution (p1, p2, . . . ), which is hard to check for probabilistic programs.

3.2 Our Hierarchical Sampling Algorithm

Our hierarchical sampling algorithm is Algorithm 2. It reﬁnes Algorithm 1. Here are some highlights.
Estimating Control Flow Likelihoods by SMC In Algorithm 2, an arm k = (cid:126)l is a complete control
ﬂow. The algorithm is designed so that different arms (cid:126)l are pulled in proportion to the control ﬂows’
likelihood p(cid:126)l = p((cid:126)l) in Γ’s execution. This p((cid:126)l) is expressed as follows; a proof is in Appendix A.7.
p(l1:N ) = (cid:82) p(l1, σ1) (cid:0) (cid:81)N
(4)
It can be shown by induction on N that the right-hand side is (cid:82) w ·
a value estimated by sampling (wt,1:J , xt,1:J ) from the weighted semantics
(from (1)) and taking the average of wt,1:J . This is what we do in Lines 10–11, using SMC.

w-st(w, x) dwdx,
StrLn(l1:N )
(cid:75)
w-st
StrLn(l1:N )

k=2 p(σk | lk−1:k, σk−1) (cid:1) dσ1:N

k=2 p(lk | lk−1, σk−1) (cid:1)(cid:0) (cid:81)N

(cid:74)

(cid:75)

(cid:74)

Data Sampling After estimating the ﬂow likelihood p(l1:N ) = p(N, l1:N ), we sample val-
σN from the inner integral in (3). The inner integral is proportional to (cid:82) w ·
eﬁnal
ues of
(cid:75)
(cid:74)
w-st(w, x) dw, where we write x for a choice of the value of
eﬁnal
StrLn(l1:N )
σN . Therefore,
(cid:74)
(cid:75)
(cid:75)
w-st—which we
StrLn(l1:N )
eﬁnal
samples of
(cid:75)
(cid:74)
already did in Line 10—and weighting the sample values xt,1:J by wt,1:J . This justiﬁes Line 12.

σN can be given by sampling (wt,1:J , xt,1:J ) from

(cid:74)

(cid:74)

(cid:75)

7

3

t

) 1

(cid:46) empirical likelih.

Algorithm 1 Our ε-greedy algorithm for inﬁnite-
armed sampling. Here εt = ( |Kknown| log t
Input: arms K = {1, 2, . . . }, budget T
Output: sequence k(1), k(2), . . . , k(T ) of arms
1: t ← 1; Kknown ← ∅;
2: ˆpk ← 0 for ∀k ∈ K
3: while t ≤ T do
4:
5:
6:
7:
8:
9:
10:
13:
14:

pull arm k and observe a likelihood p
update empirical likelihood ˆpk using p
k(t) ← k;

(cid:46) proportional
pick k ∈ Kknown proportionally to (ˆpk)k

pick k (cid:54)∈ Kknown, add k to Kknown

pick k ∈ Kknown unif. randomly

else if (with prob. εt) then

if |Kknown| < t2/3 then

t ← t + 1

(cid:46) random

(cid:46) expand

else

Algorithm 2 Our hierarchical sampler

Input: a pCFG Γ, constant J
−−−−→
(w(cid:48), x) of weighted samples
Output: a sequence

Take Algorithm 1, and apply the following adaptation: 1) K is
the set of complete control ﬂows of Γ; 2) we maintain a pool
−−−→
(w, x), initialized to empty; 3) we reﬁne
of weighted samples
Line 10 into Lines 10–12 shown below; and 4) we add Line 15,
shown below, for adjusting weights

10: draw J weighted samples (wt,1:J , xt,1:J ) from the weighted
w-st ∈ D≤1(R≥0 × D) (cf. (1)). Con-
semantics
this is done by running SMC for the program
cretely,
CdPg(StrLn(k)). We require that at least one of wt,1:J is
nonzero

StrLn(k)
(cid:75)
(cid:74)

11: p ← (cid:80) wt,1:J /J
12: append (k, wt,1:J , xt,1:J ) to the pool

(cid:46) used in Line 13
−−−−−→
(k, w, x) (where we also

record the control ﬂow k)

15: replace each entry (k, w, x) in the pool

to obtain the output samples

−−−−→
(w(cid:48), x)

−−−−−→
(k, w, x) with ( w
ˆpk

, x)

Prog. 4: unifCd(t0)

Prog. 5: unifCd2(t0)

p ∼ unif (0 ,1);
q = 1; t = 0;
while ( p <= q ) {

q = q / 2;
t = t + 1; }
observe ( t >= t0 );
return p ;

p ∼ unif (0 ,1);
q = 1; x = 0;
while ( p <= q ) {

q = q / 2;
y ~ normal (1 ,1);
x = x + y ; t = t +1;}

observe ( t >= t0 );
return x ;

Prog. 6: poisCd(p,x0)
m ∼ poisson ( p );
x = 0; n = m ;
while (0 < n ) {

x = x +1; n = n -1; }

observe ( x >= x0 );
return m ;

Prog. 7: poisCd2(p,x0)
m ∼ poisson ( p );
x = 0; n = m ;
while (0 < n ) {

y ∼ unif (1 ,1.25);
x = x + y ; n = n -1; }

observe ( x >= x0 );
return m ;

Prog. 8: geomIt(r,x0)
n = 0; c ∼ unif (0 ,1);
while ( c <= r ) {

n = n +1; x = x +1;
c ∼ unif (0 ,1); }

observe ( x >= x0 );
return n ;

Prog. 9: geomIt2(r,x0)
n = 0; c ∼ unif (0 ,1);
while ( c <= r ) {
y ∼ beta (n ,1);
n = n +1; x = x + y ;
c ∼ unif (0 ,1); }

observe ( x >= x0 );
return n ;

Prog. 10: mixed(p)

x ∼ normal (0 ,1);
if x > p {

y ∼ normal (10 ,2);

} else {

y ∼ gamma (3 ,3); }

return y ;

Figure 5: convergence

Weight Adjustment In Line 15, from the weight w of each (k, w, x), we discount the empirical
likelihood ˆpk of k since it is already accounted for by the frequency of k in
For faster convergence, an alternative to Line 15 is to replace each (k, w, x) with (ˆpk · w
, x) to
ˆwk
w(cid:48) is the sum of the weights associated to
obtain the output
k. This means that we use the inﬁnite-armed sampling as a proposal in importance sampling. This is
a good proposal distribution, as shown by the convergence discussed in §3.1,

−−−−→
(w(cid:48), x), where ˆwk = (cid:80)

−−−−−→
(k, w, x).

−−−−−→
(k,w,x)

(k,w(cid:48),x(cid:48))∈

Logical Blacklisting of Control Flows In Algorithm 2, running Line 10 for ﬂows k with zero
likelihood is useless. We adopt logical blacklisting to soundly remove some of such ﬂows: if a ﬂow’s
straight-line program has obs(false) after condition propagation (§2.5), it is blacklisted.

4

Implementation and Experiments

Our implementation in Clojure is called Schism (for SCalable HIerarchical SaMpling). It builds on
top of Anglican (ver. 1.0.0) [Tolpin et al., 2016]. It receives a PROB program, translates it to a pCFG,
and runs Algorithm 2. Its parameters concern the SMC sampling in Line 10, Algorithm 2, namely 1)
number of particles in SMC (we set it to 100); 2) timeout for each SMC run (set to 2 seconds).

We conducted experiments to assess the performance of Schism. We compare with Anglican [Tolpin
et al., 2016], a state-of-the-art probabilistic programming system. In Anglican experiments, we
used RMH, SMC and IPMCMC as sampling algorithms (with 100 particles for the latter two); the
choice follows the developers’ recommendation (probprog.github.io/anglican/inference).
We also implemented a translator from PROB programs to Anglican queries. The experiments were

8

Table 1: experimental results. Experiments ran for designated timeout seconds, or until 500K samples
were obtained (“≥500K”). For some programs, the ground truth is known, and the KL-divergence
from it is shown. When the ground truth is unknown, the mean and standard deviation is shown. The
numbers are the average of ten runs. Anglican-IPMCMC is not shown since for many programs it
returned obviously wrong samples. See Appendix B for more details.

method (timeout)
↓ target program
unifCd(10)
unifCd(15)
unifCd(18)
unifCd(20)
poisCd(6,20)
poisCd(6,30)
geomIt(0.5,5)
geomIt(0.5,20)
geomIt(0.1,5)
geomIt(0.1,20)
mixed(0)
mixed(5)
coin(0.1)
coin(0.001)

unifCd2(18)
unifCd2(20)
poisCd2(6,20)
poisCd2(6,30)
geomIt2(0.5,5)
geomIt2(0.5,20)
geomIt2(0.1,5)
geomIt2(0.1,20)
obsLoop(3,10)
obsLoop(3,12)

Schism (10 sec.)

samples KL-div.
3.58K 0.256
2.84K 0.318
2.28K 0.408
1.98K 0.457
1.97K 0.116
0.18
820
3.66K 0.0939
910
0.344
3.66K 1.82
2.01
990
41K
0.0792
42.2K 0.379
63.2K 0.000924
62.8K 0.00075
samples mean ± std
1.72K 18.3 ± 4.34
1.36K 20.4 ± 4.58
1.08K 17.9 ± 0.706
25.3 ± 0.355
470
2.33K 7.26 ± 1.18
21.2 ± 0.324
429
2.38K 6.43 ± 0.615
21.3 ± 0.261
422
10.1 ± 0.312
1.9K
12.2 ± 0.588
1.6K

Schism (60 sec.)

samples KL-div.
12.4K 0.079
9.79K 0.0996
9.12K 0.103
8.43K 0.107
7.57K 0.0459
5.08K 0.0479
10.4K 0.0399
4.31K 0.101
10.3K 1.9
4.87K 1.87
227K
229K
304K
289K
samples mean ± std
6.75K 18.8 ± 4.53
6.08K 20.9 ± 4.75
5.27K 18.3 ± 0.869
3.41K 26.7 ± 0.93
7.19K 7.61 ± 1.44
2.81K 23.0 ± 1.33
7.18K 6.48 ± 0.648
2.99K 22.2 ± 0.797
5.47K 10.2 ± 0.531
4.68K 12.2 ± 0.452

0.0734
0.22
0.00019
0.000212

Schism (600 sec.)

samples KL-div.
34.1K 0.0293
32.9K 0.0287
30K
0.0332
28.5K 0.0361
23.5K 0.0212
20.4K 0.0135
33.5K 0.0164
23.9K 0.0243
34.7K 1.86
24.1K 1.86
≥500K 0.0724
≥500K 0.162
≥500K 3.69e-05
≥500K 8.33e-05
samples mean ± std
19.0 ± 4.55
25K
23.6K 21.0 ± 4.8
17.3K 18.4 ± 0.883
14.5K 27.0 ± 0.877
24.5K 7.88 ± 1.63
16.1K 23.6 ± 1.56
25.6K 6.5 ± 0.712
16.2K 22.4 ± 0.791
14.6K 10.1 ± 0.482
15.1K 12.1 ± 0.41

Anglican-RMH (60 sec.)
samples KL-div.
≥500K 0.864
3.92
492K
5.88
447K
6.66
326K
0.00017
298K
—
0
0.00015
365K
0.000994
133K
1.95
464K
—
0
0.0735
496K
472K
0.00075
≥500K 3.8e-05
≥500K 0.00287
samples mean ± std
19.1 ± 4.35
123K
71.6K 21.0 ± 4.64
18.5 ± 0.799
146K
—
0
7.95 ± 1.55
208K
—
0
6.51 ± 0.637
132K
—
0
10.1 ± 0.29
122K
—
0

—

Anglican-SMC (60 sec.)
samples KL-div.
≥500K 0.0958
18.6K 1.82
3.93
2.1K
500
5.39
1.95K 0.0545
—
0
≥500K 0.000474
600
0.144
2.97K 1.92
0
≥500K 0.0722
≥500K 0.000207
≥500K 7.73e-06
297K
0.000214
samples mean ± std
1.53K 17.7 ± 3.0
22.0 ± 3.18
400
4.29K 18.4 ± 0.713
0
—
≥500K 7.91 ± 1.55
23.0 ± 0.0
100
—
0
0
—
(obs in loop not allowed)
(obs in loop not allowed)

on c4.xlarge instances of Amazon Web Service (4 vCPUs, 7.5 GB RAM), Ubuntu 18.04. The target
programs are in Prog. 4–10. The results are summarized in Table 1; more results are in Appendix B.

The programs mixed and coin have simple control structures (a couple of if branchings). For such
programs, our control-data separation tends to have more overhead than advantage—compare the
number of samples after 60 sec. For mixed(5) we observe detrimented precision, too. The ‘if’
branch must be rarely taken for p = 5; yet random sampling (Line 7, Algorithm 1) forces taking the
branch.

The other programs feature a while loop. With those programs, we observe beneﬁts of the control-data
separation. This is especially the case with harder instances, i.e. with more restrictive conditioning
(instances are sorted from easy to hard in Table 1). Schism can return samples of reasonable quality
while Anglican struggles: see the results with poisCd and geomIt. See also unifCd, where KL-
divergence differs a lot. Condition propagation (§2.5) is crucial here: by logical reasoning, Schism
blacklists a number of shallow control ﬂows, quickly digging into deeper ﬂows. Those deeper ﬂows
have tiny likelihoods, making it hard for Anglican to ﬁnd them.

At the same time, the results with geomIt(0.5, 5), (0.5, 20) show that Anglican can be much faster
and more precise when conditioning is not harsh, even in presence of a while loop.

For many instances of the programs with while loops, IPMCMC ran very quickly but returned
obviously wrong samples; see Appendix B. This seems to be because of the general challenge with
MCMC walks traversing different control ﬂows. This challenge is discussed in Hur et al. [2015],
Kiselyov [2016]; see also van de Meent et al. [2018, §4.2].

We also note that convergence is generally slow with Schism: for many programs, KL-div. dropped
sharply from 10 to 60 seconds, and to 600. This is no surprise: we need to sample many control ﬂows
before convergence of Algorithm 1, but each ﬂow sampling involves SMC (Line 10, Algorithm 2)
and takes time. In Fig. 5 are the KL-div. (red), and the mean and the standard deviation (blue), plotted
over samples for unifCd(18), Schism, 600 sec. More of such plots are found in Appendix B.

In summary, we observe that the control-data separation in the hierarchical sampler Schism success-
fully addresses the general challenge of the compatibility between sampling and control structures.
Its performance is superior for programs with 1) while loops and 2) restrictive conditioning (“rare
events”). We expect problems with these features are found in many application domains; an example
is safety of automotive systems (see e.g. Dreossi et al. [2019]).

9

Acknowledgments and Disclosure of Funding

Thanks are due to Bart Jacobs, Ohad Kammar, Adam Scibior, and Akihisa Yamada for useful
discussions; to Darren Wilkinson for his extensive set of blog entries that served us as a valuable
information source on sampling; and to the anonymous reviewers for previous versions of the paper
for their wonderfully extensive and insightful comments and suggestions. The authors are supported
by ERATO HASUO Metamathematics for Systems Design Project (No. JPMJER1603) JST.

References

Sheshansh Agrawal, Krishnendu Chatterjee, and Petr Novotný. Lexicographic ranking supermartingales: an

efﬁcient approach to termination of probabilistic programs. PACMPL, 2(POPL):34:1–34:32, 2018.

Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis Karaletsos, Rohit
Singh, Paul A. Szerlip, Paul Horsfall, and Noah D. Goodman. Pyro: Deep universal probabilistic programming.
J. Mach. Learn. Res., 20:28:1–28:6, 2019. URL http://jmlr.org/papers/v20/18-403.html.

Sébastien Bubeck and Nicolò Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit
problems. Foundations and Trends in Machine Learning, 5(1):1–122, 2012. doi: 10.1561/2200000024. URL
https://doi.org/10.1561/2200000024.

Bob Carpenter, Andrew Gelman, Matthew Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus
Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. Stan: A probabilistic programming language. Journal
of Statistical Software, Articles, 76(1):1–32, 2017. ISSN 1548-7660. doi: 10.18637/jss.v076.i01. URL
https://www.jstatsoft.org/v076/i01.

Kenta Cho and Bart Jacobs. The efprob library for probabilistic calculations. In Filippo Bonchi and Barbara
König, editors, 7th Conference on Algebra and Coalgebra in Computer Science, CALCO 2017, June 12-16,
2017, Ljubljana, Slovenia, volume 72 of LIPIcs, pages 25:1–25:8. Schloss Dagstuhl - Leibniz-Zentrum
fuer Informatik, 2017. ISBN 978-3-95977-033-0. doi: 10.4230/LIPIcs.CALCO.2017.25. URL https:
//doi.org/10.4230/LIPIcs.CALCO.2017.25.

Marco F. Cusumano-Towner, Feras A. Saad, Alexander K. Lew, and Vikash K. Mansinghka. Gen: a general-
purpose probabilistic programming system with programmable inference. In Kathryn S. McKinley and
Kathleen Fisher, editors, Proceedings of the 40th ACM SIGPLAN Conference on Programming Language
Design and Implementation, PLDI 2019, Phoenix, AZ, USA, June 22-26, 2019, pages 221–236. ACM, 2019.
doi: 10.1145/3314221.3314642. URL https://doi.org/10.1145/3314221.3314642.

Tommaso Dreossi, Daniel J. Fremont, Shromona Ghosh, Edward Kim, Hadi Ravanbakhsh, Marcell Vazquez-
Chanlatte, and Sanjit A. Seshia. Verifai: A toolkit for the formal design and analysis of artiﬁcial intelligence-
based systems. In Isil Dillig and Serdar Tasiran, editors, Computer Aided Veriﬁcation - 31st International
Conference, CAV 2019, New York City, NY, USA, July 15-18, 2019, Proceedings, Part I, volume 11561 of
Lecture Notes in Computer Science, pages 432–442. Springer, 2019. doi: 10.1007/978-3-030-25540-4\_25.
URL https://doi.org/10.1007/978-3-030-25540-4_25.

Timon Gehr, Sasa Misailovic, and Martin T. Vechev. PSI: exact symbolic inference for probabilistic programs. In
Swarat Chaudhuri and Azadeh Farzan, editors, Computer Aided Veriﬁcation - 28th International Conference,
CAV 2016, Toronto, ON, Canada, July 17-23, 2016, Proceedings, Part I, volume 9779 of Lecture Notes in
Computer Science, pages 62–83. Springer, 2016. ISBN 978-3-319-41527-7. doi: 10.1007/978-3-319-41528-4\
_4. URL https://doi.org/10.1007/978-3-319-41528-4_4.

Michele Giry. A categorical approach to probability theory. In Proc. Categorical Aspects of Topology and

Analysis, volume 915 of Lect. Notes Math., pages 68–85, 1982.

Andrew D. Gordon, Thomas A. Henzinger, Aditya V. Nori, and Sriram K. Rajamani. Probabilistic programming.
In James D. Herbsleb and Matthew B. Dwyer, editors, Proceedings of the on Future of Software Engineering,
FOSE 2014, Hyderabad, India, May 31 - June 7, 2014, pages 167–181. ACM, 2014. ISBN 978-1-4503-2865-4.
doi: 10.1145/2593882.2593900. URL http://doi.acm.org/10.1145/2593882.2593900.

Chung-Kil Hur, Aditya V. Nori, Sriram K. Rajamani, and Selva Samuel. A provably correct sampler for
probabilistic programs. In Prahladh Harsha and G. Ramalingam, editors, 35th IARCS Annual Conference
on Foundation of Software Technology and Theoretical Computer Science, FSTTCS 2015, December 16-
18, 2015, Bangalore, India, volume 45 of LIPIcs, pages 475–488. Schloss Dagstuhl - Leibniz-Zentrum
fuer Informatik, 2015. ISBN 978-3-939897-97-2. doi: 10.4230/LIPIcs.FSTTCS.2015.475. URL https:
//doi.org/10.4230/LIPIcs.FSTTCS.2015.475.

10

Oleg Kiselyov. Problems of the lightweight implementation of probabilistic programming. In proc. ACM

SIGPLAN Workshop on Probabilistic Programming Semantics (PPS2016), 2016.

Tuan Anh Le, Atilim Gunes Baydin, and Frank Wood. Inference Compilation and Universal Probabilistic
Programming. In Aarti Singh and Jerry Zhu, editors, Proceedings of the 20th International Conference
on Artiﬁcial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages
1338–1348, Fort Lauderdale, FL, USA, 20–22 Apr 2017. PMLR. URL http://proceedings.mlr.press/
v54/le17a.html.

Vikash K. Mansinghka, Daniel Selsam, and Yura N. Perov. Venture: a higher-order probabilistic programming
platform with programmable inference. CoRR, abs/1404.0099, 2014. URL http://arxiv.org/abs/1404.
0099.

Carroll Morgan, Annabelle McIver, and Karen Seidel. Probabilistic predicate transformers. ACM Trans. Program.

Lang. Syst., 18(3):325–353, 1996.

Aditya V. Nori, Chung-Kil Hur, Sriram K. Rajamani, and Selva Samuel. R2: an efﬁcient MCMC sampler for
probabilistic programs. In Carla E. Brodley and Peter Stone, editors, Proceedings of the Twenty-Eighth AAAI
Conference on Artiﬁcial Intelligence, July 27 -31, 2014, Québec City, Québec, Canada., pages 2476–2482.
AAAI Press, 2014.
ISBN 978-1-57735-661-5. URL http://www.aaai.org/ocs/index.php/AAAI/
AAAI14/paper/view/8192.

Tom Rainforth, Yuan Zhou, Xiaoyu Lu, Yee Whye Teh, Frank Wood, Hongseok Yang, and Jan-Willem van de

Meent. Inference trees: Adaptive inference with exploration, 2018.

Chung-chieh Shan and Norman Ramsey. Exact bayesian inference by symbolic disintegration. In Giuseppe
Castagna and Andrew D. Gordon, editors, Proceedings of the 44th ACM SIGPLAN Symposium on Principles
of Programming Languages, POPL 2017, Paris, France, January 18-20, 2017, pages 130–144. ACM,
2017. ISBN 978-1-4503-4660-3. doi: 10.1145/3009837. URL http://dl.acm.org/citation.cfm?id=
3009852.

Aleksandrs Slivkins. Introduction to multi-armed bandits. CoRR, abs/1904.07272, 2019. URL http://arxiv.

org/abs/1904.07272.

Sam Staton, Hongseok Yang, Frank D. Wood, Chris Heunen, and Ohad Kammar. Semantics for probabilistic
programming: higher-order functions, continuous distributions, and soft constraints. In Martin Grohe, Eric
Koskinen, and Natarajan Shankar, editors, Proceedings of the 31st Annual ACM/IEEE Symposium on Logic in
Computer Science, LICS ’16, New York, NY, USA, July 5-8, 2016, pages 525–534. ACM, 2016. ISBN 978-1-
4503-4391-6. doi: 10.1145/2933575.2935313. URL http://doi.acm.org/10.1145/2933575.2935313.

David Tolpin, Jan-Willem van de Meent, Hongseok Yang, and Frank D. Wood. Design and implementation
In Tom Schrijvers, editor, Proceedings of the 28th
of probabilistic programming language Anglican.
Symposium on the Implementation and Application of Functional Programming Languages, IFL 2016,
Leuven, Belgium, August 31 - September 2, 2016, pages 6:1–6:12. ACM, 2016. ISBN 978-1-4503-4767-9.
doi: 10.1145/3064899.3064910. URL https://doi.org/10.1145/3064899.3064910.

R.S. Tsay and R. Chen. Nonlinear Time Series Analysis. Wiley Series in Probability and Statistics. Wiley, 2018.

ISBN 9781119264071.

Jan-Willem van de Meent, Brooks Paige, Hongseok Yang, and Frank Wood. An introduction to probabilistic

programming. CoRR, abs/1809.10756, 2018. URL http://arxiv.org/abs/1809.10756.

David Wingate, Andreas Stuhlmüller, and Noah D. Goodman. Lightweight implementations of probabilistic pro-
gramming languages via transformational compilation. In Geoffrey J. Gordon, David B. Dunson, and Miroslav
Dudík, editors, Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics,
AISTATS 2011, Fort Lauderdale, USA, April 11-13, 2011, volume 15 of JMLR Proceedings, pages 770–778.
JMLR.org, 2011. URL http://www.jmlr.org/proceedings/papers/v15/wingate11a/wingate11a.
pdf.

Glynn Winskel. The Formal Semantics of Programming Languages. MIT Press, 1993.

Yuan Zhou, Hongseok Yang, Yee Whye Teh, and Tom Rainforth. Divide, conquer, and combine: a new
inference strategy for probabilistic programs with stochastic support. In Proceedings of the 37th International
Conference on International Conference on Machine Learning, ICML’20, 2020.

11

l

ϕ

! ϕ

· · ·

· · ·

x ∼ Dist((cid:126)e) (cid:47)

(cid:47) · · ·

l

x := e (cid:47)

(cid:47) · · ·

l

weight f (cid:47)

(cid:47) · · ·

l

(a) A deterministic
location l ∈ LD.

(b) A probabilistic
assignment location
l ∈ LPA.

(c) A deterministic
assignment location
l ∈ LDA.

(d) A weight loca-
tion l ∈ LW.

Figure 6: pCFG locations

A Omitted Details

A.1 Deﬁnition of pCFG

Our deﬁnition (Def. A.1) differs from Agrawal et al. [2018] mainly in the following: 1) presence of
weight commands; and 2) absence of nondeterminism.
Deﬁnition A.1 (pCFG, adapted from Agrawal et al. [2018]). A probabilistic control ﬂow graph
(pCFG) is a tuple Γ = (L, V, linit, σinit, lﬁnal, eﬁnal, →, λ) that consists of the following components.

• A ﬁnite set L of locations, equipped with a partition L = LD + LPA + LDA + LW + {lﬁnal}
into deterministic, probabilistic assignment, deterministic assignment, weight and ﬁnal
locations.

• A ﬁnite set V = {x1, . . . , x|V |} of program variables. It is a subset of the set Var of

variables.

• An initial location linit ∈ L, and an initial memory state σinit : V → R.

• A ﬁnal location lﬁnal ∈ L, and a return expression eﬁnal.

• A transition relation → ⊆ L × L.

• A labeling function λ.

Labeling λ assigns, to each transition l → l(cid:48), a command, a formula or a real number. It is subject to
the following conditions.

• Each deterministic location l ∈ LD has two outgoing transitions. One is labeled with a

(sharp) Boolean formula ϕ; the other is labeled with its negation ! ϕ.

• Each probabilistic assignment location l ∈ LPA has one outgoing transition. It is labeled

with a probabilistic assignment command x ∼ Dist((cid:126)e).

• Each deterministic assignment location l ∈ LDA has one outgoing transition. It is labeled

with a deterministic assignment command x := e.

• Each weight location l ∈ LW has one outgoing transition, labeled with a command weight f .
We also use observe ϕ as a label from time to time. Recall that observe ϕ is a shorthand
for weight 1ϕ, where 1ϕ is the characteristic function for the Boolean formula ϕ.

• The ﬁnal location lﬁnal has no successor with respect to →.

In the last deﬁnition, we assume that the values of all the basic types (int, bool, double, etc.) are
embedded in R. Using different value domains is straightforward but cumbersome.

Fig. 6 illustrates the ﬁve types of pCFG locations.

A.2 Semantics of pCFGs

There are a few standard ways of deﬁning semantics for imperative probabilistic programs: small-step
and big-step operational semantics, forward state-transformer semantics, and backward predicate-
transformer semantics. This is much like for non-probabilistic imperative programs.

We follow Staton et al. [2016] and introduce two denotational semantics: weighted state transformer
semantics and (unweighted, normalized) state transformer semantics.

12

(cid:44)
(cid:44)
(cid:50)
(cid:50)
(cid:74)

(cid:74)

(cid:74)

Γ, l

w-st(σ) =
(cid:75)

Γ, l

w-st(σ) =
(cid:75)

(cid:26)

(cid:74)
(cid:74)

(cid:90)

Γ, l(cid:48)
(cid:75)
Γ, l(cid:48)(cid:48)
(cid:75)

Γ, l(cid:48)

(cid:75)

(cid:74)

if
if

w-st(σ)
w-st(σ)

ϕ
(cid:74)
ϕ
(cid:74)
w-st(cid:0)σ[x (cid:55)→ v](cid:1) · Dist(cid:0)

σ = true
(cid:75)
σ = false
(cid:75)

σ

(cid:126)e
(cid:75)
(cid:74)

if l ∈ LD, with transitions l

ϕ
−→ l(cid:48) and l

! ϕ
−→ l(cid:48)(cid:48).

(cid:1)(v) dv

x∼Dist((cid:126)e)
−−−−−−−→ l(cid:48). Here
σ) is the distribution Dist instantiated with
σ. A state update, assigning v to x, is

if l ∈ LPA, with a transition l
(cid:126)e
Dist(
(cid:74)
parameters
denoted by σ[x (cid:55)→ v].
if l ∈ LDA, with a transition l x:=e−−−→ l(cid:48).

(cid:126)e
(cid:75)
(cid:74)

(cid:75)

(cid:3) (cid:1)

w-st(cid:0) σ(cid:2)x (cid:55)→
σ · dr, dσ(cid:48) (cid:1) (cid:55)−→

e
(cid:74)

(cid:75)

σ

Γ, l

Γ, l

Γ, l(cid:48)
(cid:74)
(cid:104) (cid:0)

w-st(σ) =
(cid:75)
(cid:75)
w-st(σ) =
f
(cid:75)
(cid:74)
w-st(σ) = δ(1,σ)
(cid:75)

(cid:75)

(cid:74)
Γ, lﬁnal

(cid:74)
Figure 7: Recursive deﬁnition of

Γ, l(cid:48)
(cid:74)

w-st(σ)(dr, dσ(cid:48))
(cid:75)

(cid:105)

if l ∈ LW, with a transition l

weight f
−−−−−→ l(cid:48).

(6)

where δ(1,σ) is the Dirac distribution at (1, σ).

Γ, l

w-st used in the weighted state transformer semantics of pCFGs
(cid:75)

(cid:74)

• The weighted state transformer semantics

w-st (Def. A.4), where and weights from soft

conditioning are recorded by the weights of samples.

(cid:74) (cid:75)

• The (unweighted, normalized) state transformer semantics

st (Def. A.5), obtained
w-st. In particular, the normalization process
by normalizing the weighted semantics
removes samples of weight 0, i.e. those which violate observations. This is the semantics
we wish to sample from.

(cid:74) (cid:75)

(cid:74) (cid:75)

Deﬁnition A.2. Let X be a measurable space. The set of probability distributions over X is denoted
by D(X). The set of subprobability distributions µ over X—for which µ(X) is required to be ≤ 1
rather than = 1—is denoted by D≤1(X). We equip both D(X) and D≤1(X) with suitable σ-algebras
given by Giry [1982]. For a distribution µ, its support is denoted by supp(µ). The Dirac distribution
at x ∈ X is denoted by δx.
Deﬁnition A.3 (memory state, interpretation of expressions). Let Γ = (L, V, . . . ) be a pCFG. A
memory state σ for Γ is a function σ : V → R that maps each variable x ∈ V to its value σ(x).
(Recall from Appendix A.1 that, for simplicity, we embed the values of all basic types in R.) The
interpretation
σ of
a Boolean formula.
We deﬁne St to be the measurable space over the set of functions of type V → R, equipped with
the coarsest σ-algebra making the evaluation function −(x) : (V → R) → R measurable for each
x ∈ V . This makes St isomorphic to the product of |V |-many copies of R.
=
Deﬁnition A.4
transformer
state
(L, V, linit, σinit, lﬁnal, eﬁnal, →, λ) be a pCFG. For each location l ∈ L of the pCFG Γ, we
deﬁne its interpretation

σ of an expression e under σ is deﬁned inductively; so is the interpretation

w-st). Let

(weighted

semantics

ϕ
(cid:75)

e
(cid:74)

(cid:74) (cid:75)

Γ

(cid:74)

(cid:75)

Γ, l

w-st : St −→ D≤1

(cid:0)R≥0 × St(cid:1),

(cid:74)

(cid:75)

which is a measurable function, by the least solution of the system of recursive equations shown in
w-st(σ) is the subprobability distribution of weighted samples of memory
Fig. 7. Intuitively,
(cid:75)
states, obtained at the ﬁnal state lﬁnal after an execution of Γ that starts at the location l with a memory
state σ. It is a sub-probability distribution since an execution might be nonterminating.

Γ, l

(cid:74)

For the whole pCFG Γ, its weighted state transformer semantics
as follows, using an intermediate construct

Γ
(cid:74)
(cid:0)R≥0 × St(cid:1).

Γ

w-st ∈ D≤1
(cid:75)

(cid:0)R≥0 × R(cid:1) is deﬁned

(cid:74)

w-st ∈ D≤1
(cid:75)
w-st(σinit),

w-st

:=

Γ, linit
(cid:75)
(cid:90)

(cid:74)

w-st(w, v) :=

Γ
(cid:75)
Γ
(cid:75)

(cid:74)

(cid:74)

1

efinal
(cid:74)

σ=v
(cid:75)

Γ
(cid:75)

(cid:74)

w-st(w, σ) dσ .

(5)

Here 1
(cid:74)

efinal

σ=v is the characteristic function for the designated equality.
(cid:75)

Intuitively, the deﬁnition (5) is the continuous variation of the following one (that only makes sense if
all the value domains are discrete).

w-st(w, v) :=

Γ
(cid:74)

(cid:75)

(cid:88)

σ s.t.

efinal
(cid:74)

Γ, linit

σ=v(cid:74)
(cid:75)

(cid:75)

w-st(σinit)(w, σ) .

13

Γ, l(cid:48)
(cid:75)
Γ, l

i) (cid:55)→ pi
f

Similarly, the case (6) in Fig. 7 can be described more simply if the distributions involved are discrete.
w-st(σ) be (cid:2) (ri, σ(cid:48)
(cid:3)
i, where ri is a weight and pi is a probability.
Let the distribution
(cid:3)
w-st(σ) is given by (cid:2) (cid:0)
(cid:74)
σ · ri, σ(cid:48)
Then the distribution
i. Here we multiply all the
i
(cid:75)
(cid:74)
weights ri by the nonnegative real number
f
σ.
(cid:75)
(cid:74)
The least solution of the recursive equation for (cid:0)
Γ, l
l∈L in Fig. 7 exists. It can be constructed as
the supremum of a suitable increasing ω-chain, exploiting the ω-cpo structure of the set of measurable
(cid:0)R≥0 × St(cid:1). See e.g. Gordon et al. [2014]; note that D≤1(X) for
functions of type St × L → D≤1
any measurable set X is an ω-cpo, with its bottom element given by the zero distribution (assigning 0
to every measurable subset).

(cid:1) (cid:55)→ pi

w-st(cid:1)

(cid:74)

(cid:74)

(cid:75)

(cid:75)

This construction via a supremum also matches the operational intuition of collecting the return
values of all the execution paths of length 0, 1, 2, . . . .

By applying normalization to the weighted semantics in Def. A.4, we obtain the following (un-
weighted) state transformer semantics. This semantics is the posterior distribution in the sense of
Bayesian inference. It is therefore the distribution that we would like to sample from.
Deﬁnition A.5 ((unweighted) state transformer semantics
former semantics is the probability distribution
st ∈ D(St).
ate construct

st). Let Γ be a pCFG. Its state trans-
st ∈ D(R) deﬁned as follows, using an intermedi-

(cid:74) (cid:75)

(cid:75)

Γ
(cid:75)

(cid:74)

Γ
(cid:74)
Γ
(cid:74)
Here 1
efinal
in case the denominator (cid:82) r ·
(cid:74)

(cid:75)

Γ
(cid:74)
(cid:82) r ·
w-st(r, σ) dr
Γ
(cid:74)
(cid:75)
(cid:82) r ·
w-st(r, σ(cid:48)) dr dσ(cid:48)
Γ
(cid:74)
(cid:75)
st(v) := (cid:82) 1
efinal
(cid:74)

σ=v ·
(cid:75)

st(σ) :=

Γ
(cid:75)

st(σ) dσ .

w-st(r, σ(cid:48)) dr dσ(cid:48) is 0.

Γ
(cid:75)

(cid:74)

(cid:74)
σ=v is the characteristic function for the designated equality. The semantics is undeﬁned
(cid:75)

(cid:75)

;

(7)

A.3 Control Flows and Straight-Line Programs

Deﬁnition A.6 (control ﬂow). Let Γ = (L, V, linit, σinit, lﬁnal, eﬁnal, →, λ) be a pCFG. A control
ﬂow of Γ is a ﬁnite sequence (cid:126)l = l1l2 . . . ln of locations, where we require l1 = linit and (li, li+1) ∈
→ for each i ∈ [1, n − 1]. A control ﬂow is often denoted together with transition labels, that is,

linit = l1

λ(l1,l2)
−−−−−→ l2

λ(l2,l3)
−−−−−→ · · ·

λ(ln−1,ln)
−−−−−−−→ ln .

A control ﬂow (cid:126)l = l1l2 . . . ln is said to be complete if ln = lﬁnal.
The set of control ﬂows of a pCFG Γ is denoted by CF(Γ); the set of complete ones is denoted by
CCF(Γ).
Deﬁnition A.7 (straight-line program). A straight-line program is a pCFG that has no deterministic
locations.

Therefore, a straight-line program is identiﬁed with a triple

(cid:0) linit
that shall be also denoted by

λ1−→ l2

λ2−→ · · ·

λn−1−−−→ lﬁnal, σinit, eﬁnal

(cid:1) ,

σinit−−−→ linit

λ1−→ l2

λ2−→ · · ·

λn−1−−−→ lﬁnal

efinal−−−→ .

The ﬁrst component of the above triple is a chain that consists of the following types of locations: 1)

x∼Dist((cid:126)e)
−−−−−−−→ · and l x:=e−−−→ · ; 2) weight locations l

weight f
assignment locations l
−−−−−→ · (where we might
observe ϕ
use l
−−−−−−→ · as a shorthand); and 3) a ﬁnal location. The remaining components are an initial
memory state σinit and a return expression eﬁnal.
Deﬁnition A.8 (the straight-line program StrLn((cid:126)l) ). Let Γ be a pCFG, and (cid:126)l = l1 . . . ln be
a complete control ﬂow of Γ. The straight-line program StrLn((cid:126)l) is obtained by applying the
following operation to (cid:126)l = l1 . . . ln.

1. Each deterministic location li ∈ LD occurring in (cid:126)l is changed into an observation location.
ϕ
−→ li+1 is made into an observation command,

Accordingly, the label of the transition li
observe ϕ
−−−−−−→ li+1.
resulting in li

14

A.4 Condition Propagation

Here we present a detailed account on our condition propagation rules. We continue §2.5.

The following deﬁnition closely follows the one in Nori et al. [2014]. It is much simpler though,
since we deal only with straight-line programs.
Deﬁnition A.9 (condition propagation CdPg). We deﬁne the condition propagation operation on
straight-line programs, denoted by CdPg, as follows.
The deﬁnition is via an extended operation CdPg that takes a straight-line program (cid:126)l, and returns a
pair ((cid:126)l(cid:48), f ) of a straight-line program (cid:126)l(cid:48) and a “continuation” fuzzy predicate f . The operation CdPg
is deﬁned in the following backward inductive manner.

• The base case: CdPg(lﬁnal) := (lﬁnal, 1), where 1 is the constant fuzzy predicate that

returns the real number 1.

• The step case: we shall deﬁne CdPg(l λ1−→ (cid:126)l(cid:48))—where λ1 is the label for the ﬁrst transition
in the straight-line program (it goes out of l)—assuming that CdPg((cid:126)l(cid:48)) is already deﬁned
by induction.

– Let l be a probabilistic assignment location, and let ((cid:126)l•, f •) = CdPg((cid:126)l(cid:48)). We deﬁne

CdPg(cid:0) l



(cid:0) l
(cid:0) l



x∼Dist((cid:126)e)

x∼Dist((cid:126)e)

−−−−−−−→ (cid:126)l(cid:48) (cid:1) :=
−−−−−−−→ l∗ weight f •
−−−−−−−→ (cid:126)l• , f • (cid:1)

x∼Dist((cid:126)e)

−−−−−−→ (cid:126)l• , 1ψ
otherwise.

(cid:1)

if x occurs in the fuzzy predicate f •,

(8)
In the ﬁrst case of the above, l∗ is a fresh location, and ψ is a choice of a (sharp)
Boolean formula that makes the following valid.

(cid:0) ∃x ∈ supp(cid:0)Dist((cid:126)e)(cid:1). f • > 0 (cid:1) =⇒ ψ

(9)

The intuition behind the above deﬁnition—it follows Nori et al. [2014]—is described
later in Rem. A.11. The choice of ψ in (9) will be discussed in Rem. A.12.
In Appendix A.5 we discuss a technique that we call domain restriction that can be
used in (8). The technique utilizes (part of) f • to restrict the distribution Dist((cid:126)e),
preventing unnecessary samples from being generated at all.

– Let l be a deterministic assignment location, and let ((cid:126)l•, f •) = CdPg((cid:126)l(cid:48)). We deﬁne
CdPg(cid:0) l x:=e−−−→ (cid:126)l(cid:48) (cid:1) := (cid:0) l x:=e−−−→ (cid:126)l• , f •[e/x] (cid:1),
where the predicate f •[e/x] is obtained by replacing every free occurrences of x with
the expression e.

– Let l be a weight location, and let ((cid:126)l•, f •) = CdPg((cid:126)l(cid:48)). We deﬁne

CdPg(cid:0) l

where the “conjunction” fuzzy predicate f ×f • is deﬁned by
for each memory state σ.

−−−−−→ (cid:126)l(cid:48) (cid:1) := (cid:0) (cid:126)l• , f × f • (cid:1) ,
weight f
f ×f •
(cid:74)

σ :=

(cid:75)

σ×

f

(cid:74)

(cid:75)

(cid:74)

f •

σ,
(cid:75)

Finally, the condition propagation operation CdPg on a straight-line program

σinit−−−→ linit

λ1−→

· · ·

λn−1−−−→ lﬁnal

efinal−−−→ is deﬁned as follows. Let

(cid:0) l•

1

λ•
1−→ · · ·

λ•
n•−1−−−−→ l•

Then we deﬁne, using a fresh initial location l•

ﬁnal , f • (cid:1) = CdPg(cid:0) linit
init,
λn−1−−−→ lﬁnal
weight f •
−−−−−−→ l•
1

efinal−−−→ (cid:1)
λ•
1−→ · · ·

init

λ1−→ · · ·

λn−1−−−→ lﬁnal

(cid:1) .

λ•
n•−1−−−−→ l•

ﬁnal

efinal−−−→ (cid:1) .

CdPg(cid:0) σinit−−−→ linit

λ1−→ · · ·

:= (cid:0) σinit−−−→ l•

15

Proposition A.10 (soundness of CdPg). The operation CdPg preserves the semantics of straight-
line programs. That is,

(cid:113) ((cid:126)l, σinit, eﬁnal) (cid:121)
(cid:113) ((cid:126)l, σinit, eﬁnal) (cid:121)

st

w-st

= (cid:113) CdPg((cid:126)l, σinit, eﬁnal) (cid:121)

w-st

,

and

= (cid:113) CdPg((cid:126)l, σinit, eﬁnal) (cid:121)

st

.

Proof. The ﬁrst statement (coincidence of the weighted semantics, Def. A.4) is shown easily by
induction on the deﬁnition of CdPg. Restriction of ψ (in (9)) to a sharp (i.e. {0, 1}-valued) predicate
is crucial here: we rely on the nilpotency and idempotency of 0 and 1, respectively. The second
statement (on
Remark A.11. The deﬁnition (8) follows Nori et al. [2014]; its intuition is as follows. The continua-
tion predicate f • is the observation that is passed over from the remaining part (cid:126)l(cid:48) of the program. We
would like to pass as much of the content of f • as possible to the further left (i.e. as the continuation

st) follows from the ﬁrst.

(cid:74) (cid:75)

predicate of CdPg(l

x∼Dist((cid:126)e)
−−−−−−−→ (cid:126)l(cid:48))), since early conditioning should aid efﬁcient sampling.

• In case x does not occur in f •, the probabilistic assignment x ∼ Dist((cid:126)e) does not affect
the value of f •, hence we can pass the predicate f • itself to the left. This is the second case
of (8).

• If x occurs in f •, the conditioning is blocked by the probabilistic assignment x ∼ Dist((cid:126)e),
and the conditioning by f • at this stage is mandated. This results in the straight-line program
in the ﬁrst case of (8).

However, it still makes sense to try to reject those earlier samples which would eventually
“violate” f • (i.e. yield 0 as the value of f •, to be precise, since f • is fuzzy). The Boolean
formula ψ in (8) serves this purpose.
The strongest choice for ψ is the Boolean formula ∃x ∈ supp(cid:0)Dist((cid:126)e)(cid:1). f • > 0 itself.
However, the quantiﬁer therein makes it hard to deal with in implementation. The weakest
choice of ψ is true, which means we do not pass any content of f • further to the left.

Remark A.12. On the choice of a predicate ψ in (9), besides the two extremes discussed in Rem. A.11
(true and ∃x ∈ supp(cid:0)Dist((cid:126)e)(cid:1). f • > 0), we ﬁnd the following choice useful.
Assume that the truth of f • > 0 is monotone in x, that is, f •(x1) > 0 and x1 ≤ x2 imply f •(x2) > 0.
Assume further that the support supp(cid:0)Dist((cid:126)e)(cid:1) has a supremum xsup. Then the implication

(cid:0) ∃x ∈ supp(cid:0)Dist((cid:126)e)(cid:1). f • > 0 (cid:1) =⇒ f •(xsup) > 0

(10)

is obviously valid, making f •(xsup) > 0 a viable candidate of ψ.
Here is an example. Assume Dist((cid:126)e) = Beta(1, 1), for which we have supp(Beta(1, 1)) = [0, 1).
Let f • = 1x+z≥0, the characteristic function for the formula x + z ≥ 0, where z is another variable.
Then we can take ψ = (1 + z ≥ 0), that is, ψ = (z ≥ −1).

A.5 Domain Restriction

The ﬁrst case in (8) can be further transformed as follows: if ((cid:126)l•, f •) = CdPg((cid:126)l(cid:48)),

CdPg(l

x∼Dist((cid:126)e)
−−−−−−−→ (cid:126)l(cid:48)) :=
(cid:12)
x∼(cid:0)Dist((cid:126)e)
(cid:12) ξ(cid:1)
−−−−−−−−−−→ l(cid:93) weight p(ξ|x∼Dist((cid:126)e))

−−−−−−−−−−−−−−→ l∗ weight f •

−−−−−−→ (cid:126)l• , 1ψ

(cid:1) ,

(cid:0) l

where

• l(cid:93) and l∗ are fresh locations,
• ξ is a choice of a (sharp) Boolean formula such that the following is valid:

∀x ∈ supp(cid:0)Dist((cid:126)e)(cid:1). (cid:0) f • > 0 =⇒ ξ (cid:1),

(11)

(12)

• p(ξ | x ∼ Dist((cid:126)e)) is the fuzzy predicate that returns the probability of a sample x drawn

from Dist((cid:126)e) satisfying ξ, and

16

• Dist((cid:126)e) | ξ denotes the distribution Dist((cid:126)e) conditioned by ξ, whose probability density

function is given as follows:

(cid:0)Dist((cid:126)e) (cid:12)

(cid:12) ξ(cid:1)(x) =

(cid:26)Dist((cid:126)e)(x)

0

if x satisﬁes ξ,
otherwise,

• and ψ is, much like in (9), a (sharp) Boolean predicate such that the following is valid.

(cid:0) ∃x ∈ supp(cid:0)Dist((cid:126)e)(cid:1). f • > 0 (cid:1) =⇒ ψ

The transformation (11) restricts the domain of distribution Dist((cid:126)e) by ξ; therefore we call this
transformation domain restriction. Its essence is really that of importance sampling: in (11), the
restriction to Dist((cid:126)e) | ξ must be compensated by discounting the weights of the obtained samples,
which is done by weight p(ξ | x ∼ Dist((cid:126)e)).
An example is as follows. If Dist((cid:126)e) = unif(1, 5) and f • = 12≤x≤4, then we can choose ξ to be
ξ = (2 ≤ x ≤ 4), in which case the conditioned distribution Dist((cid:126)e) | ξ is unif(2, 4). In this way,
the above simpliﬁcation allows direct sampling without conditioning. The following “discounting”
conditioning weight p(ξ | x ∼ Dist((cid:126)e)) uses the discounting factor p(ξ | x ∼ Dist((cid:126)e)) = 1/2.

We note that sampling from a restricted distribution Dist((cid:126)e) | ξ is often not hard. For example,
assume that ξ is given in the form of excluded intervals, that is,

ξ = (cid:8) x (cid:12)

(cid:12) x (cid:54)∈ (a1, b1] ∪ (a2, b2] ∪ · · · ∪ (am, bm] (cid:9),

(13)

where b1 ≤ a2, . . . , bm−1 ≤ am. Assuming that we know the inverse F of the cumulative density
function of Dist((cid:126)e) (which is the case with most common distributions), we can

• transform F into the inverse Fξ of the CDF of the restricted distribution Dist((cid:126)e) | ξ, via

case distinction and rescaling,

• and use this Fξ in the inverse transform sampling for the distribution Dist((cid:126)e) | ξ.

This method of inverse transform sampling from Dist((cid:126)e) | ξ, when ξ is given in the form of (13), is
implemented in our prototype Schism.

A.6 Derivation of Our Sampling Algorithm

(cid:74)

(cid:75)

st in (2). The relevant
Let Γ be a pCFG; our goal is to draw samples from the distribution
random variables are as follows: N (the length of a complete control ﬂow (cid:126)l = l1 . . . lN ); l1, . . . , lN
(cid:75)
(the locations therein); and σ1, . . . , σN (the memory states at those locations). Then the desired
distribution is p(

Γ

(cid:74)

eﬁnal

σN ).

eﬁnal

By marginalization and the deﬁnition of conditional probability, we can transform our target distribu-
tion p(
(cid:74)
eﬁnal
(cid:74)

σN ) as follows.
(cid:75)
σN ) = (cid:82) p(N, l1:N , σ1:N ,

σN | σN ) p(σ1:N | N, l1:N ) p(N, l1:N ) dσ1:N dl1:N dN

σN ) dσ1:N dl1:N dN

eﬁnal

eﬁnal

p(

(cid:75)

(cid:74)

(cid:75)

σN | σN ) p(σ1:N | N, l1:N ) dσ1:N

p(N, l1:N ) dl1:N dN

(cid:17)

= (cid:82) p(
= (cid:82) (cid:16)(cid:82) p(
(cid:74)
(cid:74)

(cid:75)
eﬁnal

(cid:75)

(14)
The last expression justiﬁes the following hierarchical sampling scheme. At the top level, we sample
a control ﬂow N, l1:N from the distribution p(N, l1:N ); at the bottom level, for each control ﬂow
sample N, l1:N , we sample data sequences σ1:N , in the way that is conditioned by the ﬁxed control
ﬂow N, l1:N . The latter corresponds to sampling data from the straight-line program StrLn(l1:N ).

A.7

Justiﬁcation of Flow Likelihood Estimation

The equality (4) in §3.2 is shown as follows. Here we use the deﬁnition of conditional probability; we
also use the principle of conditional independence (i.e. the Markovian property of pCFGs) to derive

17

equalities such as p(σ3 | l1:3, σ1:2) = p(σ3 | l2:3, σ2).

p(l1:N )
= (cid:82) p(σ1:N , l1:N ) dσ1:N
= (cid:82) p(l1, σ1) p(σ2:N , l2:N | l1, σ1) dσ1:N
= (cid:82) p(l1, σ1) p(l2 | l1, σ1) p(σ2 | l1:2, σ1)
p(σ3:N , l3:N | l1:2, σ1:2) dσ1:N

= · · ·
= (cid:82) p(l1, σ1)
(cid:16)(cid:81)N

(cid:16)(cid:81)N

(cid:17)
k=2 p(σk | lk−1:k, σk−1)

k=2 p(lk | lk−1, σk−1)
dσ1:N

(cid:17)

(15)

A.8 Multi-Armed Sampling

In our hierarchical sampling framework in Fig. 1, we formulate the top-level control ﬂow sampling as
the inﬁnite-armed sampling problem. As a step towards this problem, here we introduce and study the
multi-armed sampling problem; the inﬁnite-armed sampling problem is its variation with inﬁnitely
many arms.

A.8.1 The Multi-Armed Sampling Problem

The setting is informally described as follows. We have arms {1, 2, . . . , K}, and each arm k comes
with its likelihood pk. Our goal is to sample (or “pull”) arms from the set {1, 2, . . . , K} according to
the likelihoods p1, . . . , pK.

The challenge, however, is that the likelihoods p1, . . . , pK are not know a priori. We assume that, if
we pull an arm k at time t, we observe a value represented by a random variable Xk,t. We further
assume that Xk,t is a random variable such that

• its mean is the (unknown) true likelihood pk of the arm k, and

• it is time-homogeneous, i.e. Xk,1, Xk,2, . . . are i.i.d.

Note that the problem is a “sampling-variant” of the classic multi-armed bandit (MAB) problem in
reinforcement learning (RL). In MAB, the goal is to optimize—speciﬁcally, to maximize cumulative
likelihood values—instead of to sample. MAB is an exemplar of the exploration-exploitation trade-
off in RL: by engaging the greedy (or “exploitation-only”) strategy of keep pulling the empirically
best-performing arm, one runs the risk of missing the actual best-performing arm, in case the latter
happens to have performed empirically worse.

We formally state the problem. For the ease of theoretical analysis, we assume that the true likelihoods
pk, as well as the observed ones Xk,t, all take their values in the unit interval [0, 1].

Deﬁnition A.13 (the (ﬁnite) multi-armed sampling problem).

• Given: a ﬁnite set K =
{1, . . . , K} of arms. Each arm k ∈ K, when pulled, returns an observed likelihood that is
given by a random variable Xk,t. We assume that {Xk,t}t∈Z>0 is i.i.d., and that the mean
of Xk,t is pk.

• Goal: pull one arm at each time t ∈ Z>0, producing a sequence of arms k(1), k(2), . . .

(where k(t) ∈ K for each t ∈ Z>0), so that the vector

(cid:18) T1(T )
T

, . . . ,

(cid:19)

TK(T )
T

, where Tk(T ) = (cid:12)

(cid:12){t ∈ [1, T ] | k(t) = k}(cid:12)

(cid:12) is the visit count,

(16)

as T → ∞. Here, the two vectors of length K
converges to the vector
are understood as categorical distributions over the set {1, 2, . . . , K} of arms. Convergence
here precisely means the one in Thm. A.14.

, . . . ,

k pk

(cid:80)

(cid:16) p1
(cid:80)

(cid:17)

pK
k pk

18

(cid:46) Initialization

Algorithm 3 An ε-greedy algorithm for (ﬁnite) multi-armed sampling
Input: The setting of Def. A.13 (with arms 1, . . . , K), T ∈ Z>0, and εt = ( K log t

) 1
3 for each

t

t ∈ Z>0

Output: a sequence k(1), k(2), . . . , k(T ) of arms from K = {1, . . . , K}
1: t ← 0
2: while t < T do
3:
4:
5:
6:
7:
8:

t ← t + 1
explore? ← (sample from [true (cid:55)→ εt, false (cid:55)→ 1 − εt])
if explore? = true then

k(t) ← (sampled from the uniform distribution over K)

else

k(t) ← (sampled according to the empirical likelihoods (cid:2)ˆp1,t−1, . . . , ˆpK,t−1

(cid:3))

9:

p(t) ← (sampled from Xk,t)

A.8.2 An ε-Greedy Algorithm

We propose the algorithm shown in Algorithm 3, where Tk(t) is the visit count from (16), and ˆpk,t is
the empirical mean of likelihoods observed by pulling the arm k, that is,

ˆpk,t =

1
Tk(t)

(cid:88)

p(s) .

s∈[1,t] such that k(s)=k

(17)

The algorithm is exactly the same as the ε-greedy algorithm for MAB (for optimization), except
that in Line 8, we sample according to the empirical likelihoods instead of pulling the empirically
) 1
best-performing arm. The “exploration” rate εt = ( K log t
3 is the same as the one commonly used
for MAB, too; see e.g. Slivkins [2019], Bubeck and Cesa-Bianchi [2012].

t

A.8.3 Convergence of the ε-Greedy Algorithm

It turns out that our ε-Greedy algorithm (Algorithm 3) indeed achieves the goal described in Def. A.13,
as we show in Thm. A.14. The convergence speed is slower, however, compared to the MAB
(optimization) case. For MAB, the ε-Greedy algorithm achieves a regret bound T 2/3 · O(K log T )1/3,
that is, regret O(K log T /T )1/3 per trial. See Slivkins [2019].
Theorem A.14. The output k(1), k(2), . . . , k(T ) of Algorithm 3 achieves the goal of Def. A.13.
Speciﬁcally, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)

E(Tk(T ))
T

−

pk
k pk

(cid:80)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:16)

7
3

K

= O

(cid:16) log T
T

(cid:17) 1

4 (cid:17)

for each k ∈ K.

(18)

Proof. We follow the proof structure outlined in Slivkins [2019]—it emphasizes the role of so-called
clean events—adapting it to the current sampling setting.

In what follows, we proceed leaving two constants α, β as parameters—α will appear in (25), and β
occurs in the deﬁnition of the exploration rate

εt =

(cid:19)β

(cid:18) K log t
t

.

We assume 0 < α, β < 1. In the course of the proof, we collect constraints on α and β, and in the
end we choose the values of α, β so that the resulting error bound is optimal. We will see that α = 1
4
and β = 1

3 are optimal, yielding the exploration rate εt in Algorithm 3.

Let us consider the following events.

E1(t)

(where t ∈ [1, T ]) : ∀k ∈ K. ∀s ∈ [1, t].

(cid:12)
(cid:12) ˆpk,s − pk

(cid:12)
(cid:12) ≤
(cid:113)

(cid:113) log T
Tk(s) ,

and

t(cid:0)log t − log(log t)(cid:1)

(19)

−

∀k ∈ K. Tk(t) ≥

t1−β(cid:0)log t(cid:1)β
K1−β

19

We claim that

To see this, ﬁrst observe that

P(E1(t)) = 1 − O

(cid:16)

K

(cid:16) log t
t

(cid:17)2(cid:17)

.

(cid:32)
(cid:12)
(cid:12) ˆpk,s − pk

P

(cid:12)
(cid:12) ≤

(cid:115)

(cid:33)

log T
Tk(s)

≥ 1 −

2
T 2

(20)

(21)

holds by Hoeffding’s inequality for each k and s. For the second condition in E1(t) in (19), we
argue as follows. According to Algorithm 3, the visit count Tk(t) is the sum of the number of the
exploration picks (Line 6) and that of the usual picks (Line 8). By counting only the exploration
picks, and also noting that εt = ( K log t

)β is decreasing with respect to t, we observe that

t

Tk(t) ≥ RB(εtK −1, t) = RB

(cid:18) (log t)β

K 1−βtβ , t

(cid:19)

,

(22)

where RB(α, s) is the “repeated Bernoulli” random variable for the number of heads when a coin
with bias α is tossed s times. This observation is used in the following reasoning.
(cid:113)

(cid:21)

−

t(cid:0)log t − log(log t)(cid:1)

(cid:20)
Tk(t) ≥

P

(cid:20)

RB

≥ P

t1−β(log t)β
K 1−β
(cid:18) (log t)β

(cid:19)

K 1−βtβ , t

(cid:32)(cid:114)

≥ 1 − exp

−2

= 1 −

(cid:19)2

(cid:18) log t
t

.

≥

t1−β(log t)β
K 1−β

(cid:113)

−

t(cid:0)log t − log(log t)(cid:1)

(cid:21)

log t − log(log t)
t

(cid:33)2



t


by Hoeffding’s inequality

Combining (21) and (23) we obtain

P(E1(t)) ≥ 1 − KT

2
T 2 − K

(cid:18) log t
t

(cid:19)2

which proves the claim (20). Note that, in deriving (24), we used the following basic principle:
n
(cid:95)

n
(cid:88)

n
(cid:94)

P(

Ai) = 1 − P(

Ai) ≥ 1 −

P(Ai) .

i=1

i=1

i=1

Towards the statement (18) of the proposition, we introduce a constant

L = (cid:100)T 1−α(log T )α(cid:101) ,
where α is a parameter that will later be chosen by optimization (as we discussed at the beginning of
the proof). The intuition of L is as follows: the ﬁrst L samples k(1), . . . , k(L) (out of T samples in
total) are transient ones, and they are inessential in the convergence guarantee in Thm. A.14.

(25)

We focus on suitable clean events (namely E1(L) ∧ E1(T )), and estimate the deviation from the true
likelihood

.

(cid:80)

pk
k∈K pk

E

(cid:18) Tk(T )
T
(cid:32)

= E

−

(cid:80)

Tk(L)
T

+

1
T

(cid:12)
(cid:12)
(cid:12)
(cid:12)

pk
k∈K pk
T
(cid:88)

t=L+1

E1(L) ∧ E1(T )

(cid:19)

εt
K

+

1
T

T
(cid:88)

t=L+1

(1 − εt)

ˆpk,t
k∈K ˆpk,t

(cid:80)

−

pk
k∈K pk

(cid:80)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

E1(L) ∧ E1(T )

where the ﬁrst three terms stand for samples from t ≤ L, exploration picks after
L, and exploitation picks after L, respectively


T
(cid:88)

(cid:80)

t=L+1

k∈K

pk +
(cid:16)

(cid:113) log T
Tk(t)
(cid:113) log T
Tk(t)

pk −

(cid:17) −

(cid:80)

pk
k∈K pk

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E1(L) ∧ E1(T )





≤ E



L
T

+

εL(T − L)
KT

+

1
T

20

(23)

(24)

by Tk(L) ≤ L, εt ≤ εL for t ∈ [L + 1, T ], and E1(T )


≤ E



L
T

+

εLT
KT

+

1
T

T
(cid:88)

(cid:80)

t=L+1

k∈K

pk +
(cid:16)

(cid:113) log T
Tk(L)
(cid:113) log T
Tk(L)

pk −

(cid:17) −

(cid:80)

pk
k∈K pk

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)



E1(L) ∧ E1(T )



by Tk(L) ≤ Tk(t) for t ∈ [L + 1, T ]




≤ E



L
T

+

εL
K

+



(cid:80)

pk +
(cid:16)

(cid:113) log T
Tk(L)
(cid:113) log T
Tk(L)

pk −

k∈K

(cid:17) −

pk
k∈K pk

(cid:80)





(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)



E1(L) ∧ E1(T )



(26)

(27)

(28)

(29)

(30)

We shall bound, from above, the three terms in the last expression.

L
T

εL
K

= O

=

1
K

(cid:18)(cid:18) log T

(cid:19)α(cid:19)

T
(cid:18) K log L
L

(cid:19)β

= O

= O

(cid:32)

(cid:32)

1
K 1−β

1
K 1−β

(cid:0)(1 − α) log T + α log(log T )(cid:1)β
T β(1−α)(log T )αβ
(cid:19)β(1−α)(cid:33)

(cid:18) log T
T

(cid:33)

On the third term in (26), we proceed as follows.

Tk(L) ≥

(cid:113)

L1−β(cid:0)log L(cid:1)β
K 1−β
T (1−α)(1−β)(log T )α(1−β)(cid:0)log(T 1−α(log T )α)(cid:1)β

L(cid:0)log L − log(log L)(cid:1)

−

(cid:16)

by E1(L),

= Θ

L1−β(log L)β
K 1−β

(cid:17)

1
K 1−β

(cid:16)

= Θ

T (1−α)(1−β)(log T )α(1−β)(cid:0)(1 − α) log T + α log(log T )(cid:1)β

(cid:17)

1
K 1−β

= Θ(cid:0)T (1−α)(1−β)(log T )α(1−β)+β

1
K 1−β

(cid:1)

= Θ(cid:0)T (1−α)(1−β)(log T )1−(1−α)(1−β)

1
K 1−β
by α(1 − β) + β = 1 − (1 − α)(1 − β),

(cid:1)

(cid:113)

L(cid:0)log L − log(log L)(cid:1)

2 (cid:17)
(cid:16)(cid:0)T 1−α(log T )α(cid:0)(1 − α) log T + α log(log T ) − log((1 − α) log T + α log(log T ))(cid:1)(cid:1) 1
(cid:16)

(cid:17)

1−α
2 (log T )

1+α
2

T

.

= Θ

= Θ

Let us now impose a condition

(1 − α)(1 − β) >

1 − α
2

,

(31)

which will be taken into account when we choose the values of α, β in the end. This assumption
implies that (29) dominates (30), and hence that

(cid:18)

Tk(L) ≥ Θ

T (1−α)(1−β)(log T )1−(1−α)(1−β)

(cid:19)

.

1
K 1−β

(32)

We continue the estimation of the third term in (26). Let δ denote

(cid:113) log T

Tk(L) occurring therein. It

follows from (32) that δ ≤ Θ

(cid:16)

1−β
2

K

(cid:16) log T
T

(cid:17) (1−α)(1−β)

2

(cid:17)

; in particular, δ tends to 0 as T → ∞ since

21

α, β ∈ (0, 1).

(cid:18)

pk + δ
k∈K (pk − δ)

(cid:80)

−

pk
k∈K pk

(cid:80)

(cid:19)

(cid:18)

=

−

(cid:80)

(cid:19)

pk
k∈K pk

pk + δ

((cid:80)
(cid:32)

k∈K pk) − Kδ
(cid:32)
1
k∈K pk

(pk + δ)

(cid:80)

δ
k∈K pk

(cid:80)

+

2Kδ(pk + δ)
(cid:1)2
(cid:0)(cid:80)
k∈K pk

(cid:33)

= O

(cid:32)

= O

+

(cid:0)(cid:80)

2Kδ
k∈K pk

(cid:1)2

(cid:33)

(cid:33)

−

pk
k∈K pk

(cid:80)

(33)

= O (Kδ)

(cid:16)

3−β
2

K

= O

(cid:18) log T
T

(cid:19) (1−α)(1−β)

2

(cid:17)

.

(34)

In (33) we used the following general fact. Let a a constant, and ϕ be a function of T that takes
nonnegative values and tends to 0 as T → ∞. Then 1

a−ϕ = O( 1

a + 2ϕ

a2 ).

Combining (26) and (27,28,34), we obtain, for each k ∈ K = {1, 2, . . . , K},

Since (cid:80)

E

(cid:18) Tk(T )
T


= O



(cid:18) log T
T

−

(cid:80)

pk
k∈K pk
(cid:19)α

+

1
K 1−β

k∈K
(cid:18)

E

=

Tk(T )

T = (cid:80)
pk
k∈K pk
(cid:88)

E

(cid:80)

pk
k∈K pk
(cid:12)
(cid:12)
(cid:12)
(cid:12)

k∈K

(cid:80)

−

Tk(T )
T
(cid:18) Tk(cid:48)(T )
T

k(cid:48)∈K\{k}


= O

K

E1(L) ∧ E1(T )

(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)β(1−α)

(cid:18) log T
T

+ K

3−β
2

(cid:18) log T
T

(cid:19) (1−α)(1−β)

2



 .

= 1, we also obtain a lower bound of the difference, namely

(cid:19)

E1(L) ∧ E1(T )

−

pk(cid:48)
k∈K pk

(cid:80)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

E1(L) ∧ E1(T )

(cid:19)

(cid:18) log T
T

(cid:19)α

+ K β

(cid:18) log T
T

(cid:19)β(1−α)

+ K

5−β
2

(cid:18) log T
T

(cid:19) (1−α)(1−β)

2



 .

Combining the above two, we obtain
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Tk(T )
T

(cid:18) (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:80)

−

E

E1(L) ∧ E1(T )

(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

pk
k∈K pk
(cid:19)α



= O

K

(cid:18) log T
T

+ K β

(cid:19)β(1−α)

(cid:18) log T
T

+ K

5−β
2

(cid:18) log T
T

(cid:19) (1−α)(1−β)

2



 .

(35)

We are ready to show the statement of the proposition.

−

pk
k pk

(cid:80)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Tk(T )
T

−

pk
k pk

(cid:80)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= E

E(Tk(T ))
T
(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ E

Tk(T )
T

−

≤ E

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)

Tk(T )
T

−

(cid:80)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:80)

pk
k pk
(cid:12)
(cid:12)
(cid:12)
(cid:12)

pk
k pk

(cid:33)

(cid:33)

E1(L) ∧ E1(T )

· P(cid:0) E1(L) ∧ E1(T ) (cid:1)

(cid:33)

E1(L) ∨ E1(T )

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

· P(cid:0) E1(L) ∨ E1(T ) (cid:1)

E1(L) ∧ E1(T )

+ P(cid:0) E1(L) ∨ E1(T ) (cid:1)

22

Prog. 11: coin(bias)

Prog. 12: obsLoop(x0,n0)

bool c1 , c2 := true ;
ifp ( bias )

then c1 := true ;
else c1 := false ;

ifp ( bias )

then c2 := true ;
else c2 := false ;
observe (!( c1 = c2 ));
return ( c1 );

double x := 0;
double y := 0;
int n := 0;
while ( x < x0 ) {
n := n + 1;
y ∼ normal (1 ,1);
observe (0 <= y <= 2);
x := x + y ;}
observe ( n >= n0 );
return ( n );



= O

K

(cid:19)α

(cid:18) log T
T

+ K β

(cid:18) log T
T

(cid:19)β(1−α)

+ K

5−β
2

(cid:18) log T
T

(cid:19) (1−α)(1−β)

2





(cid:18)

+ O

K

(cid:17)2

(cid:16) log L
L

+ K

(cid:16) log T
T

(cid:17)2(cid:19)

by (20,35)



= O

K

(cid:19)α

(cid:18) log T
T

+ K β

(cid:18) log T
T

(cid:19)β(1−α)

+ K

5−β
2

(cid:18) log T
T

(cid:19) (1−α)(1−β)

2

(cid:19)2(1−α)

+K

(cid:18) log T
T

(cid:17)2(cid:33)

+ K

(cid:16) log T
T

(36)

We now choose the parameters α, β ∈ (0, 1) so that (36) is optimal, that is, so that the minimum
of the powers of the ﬁve terms (namely α, β(1 − α), (1−α)(1−β)
, 2(1 − α), 2) is maximal. The
4 , β = 1
choice turns out to be α = 1
= 1/4, and
2(1 − α) = 3/2. The choice of α, β satisﬁes the additional condition (31) that we introduced in the
course of the proof, too.

3 ,2 which yields α = β(1 − α) = (1−α)(1−β)

2

2

The bound (36) under the above choice of α, β yields the desired bound. This concludes the proof.

B Supplementary Experimental Results

The following experiment data is supplementary to §4. The new target programs (with parameters)
are in Prog. 11–12.

2Obtained by a numeric solver in MATLAB.

23

Table 2: experimental results with more details, supplementing Table 1.

t
s
r
ﬁ
e
h
t

r
o
F

.
)
”
K
0
0
5
≥

“

h
t
i

w
d
e
k
r
a
m

(

d
e
n
i
a
t
b
o

e
r
e
w
s
e
l
p
m
a
s
K
0
0
5

l
i
t
n
u

r
o

,
s
d
n
o
c
e
s

t
u
o
e
m

i
t

d
e
t
a
n
g
i
s
e
d

r
o
f

n
a
r

s
t
n
e
m

i
r
e
p
x
E

.
)
s
m
h
t
i
r
o
g
l
a

g
n
i
l
p
m
a
s

t
n
e
r
e
f
f
i
d

h
t
i

w

(

n
a
c
i
l
g
n
A
h
t
i

w

)

m
s
i
h
c
S
(

l
a
s
o
p
o
r
p

r
u
o

g
n
i
r
a
p
m
o
C

e
h
T

.
n
w
o
h
s

s
i

n
o
i
t
a
i
v
e
d

d
r
a
d
n
a
t
s

d
n
a

n
a
e
m
e
h
t

o
s

,
n
w
o
n
k

t
o
n

s
i

h
t
u
r
t

d
n
u
o
r
g

e
h
t

,
s
m
a
r
g
o
r
p

r
e
t
t
a
l

e
h
t

r
o
F

.

n
w
o
h
s

s
i

s
e
l
p
m
a
s

e
h
t

o
t

t
i

m
o
r
f

e
c
n
e
g
r
e
v
i
d
-
L
K
e
h
t

o
s

,
n
w
o
n
k

s
i

h
t
u
r
t

d
n
u
o
r
g

e
h
t

,
s
m
a
r
g
o
r
p

f
o

p
u
o
r
g

.
s
n
u
r

n
e
t

f
o

e
g
a
r
e
v
a

e
h
t

e
r
a

s
r
e
b
m
u
n

)
.
c
e
s

0
6
(

C
M
C
M
P
I
-
n
a
c
i
l
g
n
A

)
.
c
e
s

0
6
(

C
M
S
-
n
a
c
i
l
g
n
A

)
.
c
e
s
0
6
(

H
M
R
-
n
a
c
i
l
g
n
A

)
.
c
e
s
0
0
6
(

m
s
i
h
c
S

—

—

—

—

f
n
i

f
n
i

f
n
i

n
a
n

2
7
6
0
0
0
.
0

f
n
i

f
n
i

f
n
i

1
9
2
0
0
0
.
0

5
0
-
e
5
4
.
1

5
2
3
0
0
0
.
0

3
7
0
.
0

—

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

0

0

0

0

0

.
v
i
d
-
L
K

s
e
l
p
m
a
s

.
v
i
d
-
L
K

s
e
l
p
m
a
s

8
5
9
0
.
0

K
0
0
5
≥

2
8
.
1

3
9
.
3

9
3
.
5

K
6
8
1

.

K
1
2

.

0
0
5

5
4
5
0
.
0

K
5
9
1

.

—

—

—

0

0

0

4
7
4
0
0
0
.
0

K
0
0
5
≥

7
0
2
0
0
0
.
0

6
0
-
e
3
7
.
7

4
1
2
0
0
0
.
0

2
2
7
0
.
0

—

4
4
1
.
0

2
9
.
1

—

K
7
9
2

.

0
0
6

0

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

K
7
9
2

0

.

v
i
d
-
L
K

s
e
l
p
m
a
s

4
6
8
0

.

K
0
0
5
≥

7
1
0
0
0
0

.

2
9
3

.

8
8
5

.

6
6
6

.

—

—

—

4
9
9
0
0
0
0

.

5
1
0
0
0
0

.

5
7
0
0
0
0

.

5
0
-
e
8
3

.

7
8
2
0
0
0

.

5
3
7
0
0

.

—

5
9
1

.

—

K
2
9
4

K
7
4
4

K
6
2
3

K
8
9
2

0

0

0

K
5
6
3

K
3
3
1

K
4
6
4

0

K
6
9
4

K
2
7
4

K
0
0
5
≥

K
0
0
5
≥

0

.

v
i
d
-
L
K

s
e
l
p
m
a
s

3
9
2
0
0

.

7
8
2
0
0

.

2
3
3
0
0

.

1
6
3
0
0

.

2
1
2
0
0

.

5
3
1
0
0

.

3
3
0
0

.

—

4
6
1
0
0

.

3
4
2
0
0

.

6
8
1

.

6
8
1

.

K
1
4
3

.

K
9
2
3

.

K
0
3

K
5
8
2

.

K
5
3
2

.

K
4
0
2

.

K
2
3
2

.

0

K
5
3
3

.

K
9
3
2

.

K
7
4
3

.

K
1
4
2

.

4
2
7
0
0

.

2
6
1
0

.

5
0
-
e
9
6
3

.

5
0
-
e
3
3
8

.

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

—

0

)
.
c
e
s
0
6
(

m
s
i
h
c
S

.

v
i
d
-
L
K

s
e
l
p
m
a
s

)
.
c
e
s

0
1
(

m

s
i
h
c
S

.
v
i
d
-
L
K

s
e
l
p
m
a
s

m
a
r
g
o
r
p

t
e
g
r
a
t

)
t
u
o
e
m

i
t
(

d
o
h
t
e
m

6
9
9
0
0

.

9
7
0
0

.

3
0
1
0

.

7
0
1
0

.

9
5
4
0
0

.

9
7
4
0
0

.

1
3
5
0
0

.

—

9
9
3
0
0

.

1
0
1
0

.

7
8
1

.

9
1

.

4
3
7
0
0

.

2
2
0

.

2
1
2
0
0
0
0

.

9
1
0
0
0
0

.

—

K
4
2
1

.

K
9
7
9

.

K
2
1
9

.

K
3
4
8

.

K
7
5
7

.

K
8
0
5

.

K
4
3
7

.

0

K
4
0
1

.

K
1
3
4

.

K
3
0
1

.

K
7
8
4

.

K
7
2
2

K
9
2
2

K
4
0
3

K
9
8
2

0

6
5
2
.
0

8
1
3
.
0

8
0
4
.
0

7
5
4
.
0

6
1
1
.
0

8
1
.
0

K
8
5
.
3

K
4
8
.
2

K
8
2
.
2

K
8
9
.
1

K
7
9
.
1

0
2
8

4
0
5
0
.
0

K
6
9
.
1

9
3
9
0
.
0

4
4
3
.
0

—

2
8
.
1

1
0
.
2

2
9
7
0
.
0

9
7
3
.
0

4
2
9
0
0
0
.
0

5
7
0
0
0
.
0

K
6
6
.
3

0
1
9

K
6
6
.
3

0
9
9

K
1
4

K
2
.
2
4

K
2
.
3
6

K
8
.
2
6

0

)
0
1
(
d
C
f
i
n
u

)
5
1
(
d
C
f
i
n
u

)
8
1
(
d
C
f
i
n
u

)
0
2
(
d
C
f
i
n
u

)
0
2
,
6
(
d
C
s
i
o
p

)
0
3
,
6
(
d
C
s
i
o
p

)
0
2
,
3
(
d
C
s
i
o
p

)
0
3
,
3
(
d
C
s
i
o
p

)
0
(
d
e
x
i
m

)
5
(
d
e
x
i
m

)
1
.
0
(
n
i
o
c

)
1
0
0
.
0
(
n
i
o
c

)
5
,
5
.
0
(
t
I
m
o
e
g

)
0
2
,
5
.
0
(
t
I
m
o
e
g

)
5
,
1
.
0
(
t
I
m
o
e
g

)
0
2
,
1
.
0
(
t
I
m
o
e
g

—

0

)
1
0
0
0
0
.
0
(
n
i
o
c

d
t
s

±
n
a
e
m

s
e
l
p
m
a
s

d
t
s

±
n
a
e
m

s
e
l
p
m
a
s

d
t
s

±
n
a
e
m

s
e
l
p
m
a
s

d
t
s

±
n
a
e
m

s
e
l
p
m
a
s

d
t
s

±
n
a
e
m

s
e
l
p
m
a
s

d
t
s

±
n
a
e
m

s
e
l
p
m
a
s

24

5
6
.
3
±
0
.
1
1

2
0
.
7
±
5
.
2
1

8
2
.
5
±
4
6
.
3

4
8
.
3
±
9
.
2

0
.
6
±
4
.
4
1

5
4
.
2
±
0
.
6

3
7
.
1
±
0
.
3

3
7
.
1
±
0
.
3

1
4
.
1
±
9
9
9
.
0

4
5
.
1
±
2
9
.
7

8
5
4
.
0
±
5
4
1
.
0

1
5
3
.
0
±
1
1
1
.
0

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

K
0
0
5
≥

)
d
e
w
o
l
l
a

)
d
e
w
o
l
l
a

)
d
e
w
o
l
l
a

)
d
e
w
o
l
l
a

)
d
e
w
o
l
l
a

)
d
e
w
o
l
l
a

t
o
n

t
o
n

t
o
n

t
o
n

t
o
n

t
o
n

p
o
o
l

p
o
o
l

p
o
o
l

p
o
o
l

p
o
o
l

p
o
o
l

n
i

n
i

n
i

n
i

n
i

n
i

s
b
o
(

s
b
o
(

s
b
o
(

s
b
o
(

s
b
o
(

s
b
o
(

2
1
.
4
±
1
.
6
1

0
.
3
±
7
.
7
1

8
1
.
3
±
0
.
2
2

6
.
3
±
0
.
1
1

3
1
7
.
0
±
4
.
8
1

—

—

—

K
2
7
2

K
7
9
9

.

K
3
5
1

.

0
0
4

K
9
2
4

.

0

0

0

5
5
.
1
±
1
9
.
7

K
0
0
5
≥

0
.
0
±
0
.
3
2

0
0
1

—

—

0

0

)
d
e
w
o
l
l
a

)
d
e
w
o
l
l
a

)
d
e
w
o
l
l
a

)
d
e
w
o
l
l
a

)
d
e
w
o
l
l
a

)
d
e
w
o
l
l
a

t
o
n

t
o
n

t
o
n

t
o
n

t
o
n

t
o
n

p
o
o
l
n
i

p
o
o
l
n
i

p
o
o
l
n
i

p
o
o
l
n
i

p
o
o
l
n
i

p
o
o
l
n
i

s
b
o
(

s
b
o
(

s
b
o
(

s
b
o
(

s
b
o
(

s
b
o
(

.

9
6
3
±
2
1
1

.

.

1
4
±
1
6
1

.

.

5
3
4
±
1
9
1

.

.

4
6
4
±
0
1
2

.

.

9
9
7
0
±
5
8
1

.

.

5
5
1
±
5
9
7

.

—

—

—

—

.

7
3
6
0
±
1
5
6

.

.

2
9
4
0
±
2
2
5

.

—

.

6
4
3
0
±
1
1
8

.

.

9
2
0
±
1
0
1

.

—

—

—

K
0
4
2

K
8
8
1

K
3
2
1

K
6
1
7

.

K
6
4
1

K
8
0
2

K
2
3
1

0

0

K
3
0
3

K
4
2
2

K
2
2
1

0

0

0

0

0

0

.

9
5
3
±
0
1
1

.

.

3
2
4
±
0
6
1

.

.

5
5
4
±
0
9
1

.

.

8
4
±
0
1
2

.

.

3
8
8
0
±
4
8
1

.

.

7
7
8
0
±
0
7
2

.

.

7
7
5
0
±
1
8
1

.

.

3
5
2
0
±
9
5
2

.

.

3
6
1
±
8
8
7

.

.

6
5
1
±
6
3
2

.

.

2
1
7
0
±
5
6

.

.

1
9
7
0
±
4
2
2

.

.

8
0
6
0
±
7
2
5

.

.

8
4
4
0
±
5
1
8

.

.

2
8
4
0
±
1
0
1

.

.

1
4
0
±
1
2
1

.

.

4
0
±
1
5
1

.

.

5
9
4
0
0
±
1
0
2

.

K
3
2
3

.

K
2
7
2

.

K
5
2

K
6
3
2

.

K
3
7
1

.

K
5
4
1

.

K
5
6
1

.

K
7
4
1

.

K
5
4
2

.

K
1
6
1

.

K
6
5
2

.

K
2
6
1

.

K
8
7
1

.

K
4
6
1

.

K
6
4
1

.

K
1
5
1

.

K
9
3
1

.

K
9
0
5

.

.

9
5
3
±
0
1
1

.

.

8
1
4
±
9
5
1

.

.

3
5
4
±
8
8
1

.

.

5
7
4
±
9
0
2

.

.

9
6
8
0
±
3
8
1

.

.

3
9
0
±
7
6
2

.

.

4
0
7
0
±
0
8
1

.

.

9
0
3
0
±
9
5
2

.

.

4
4
1
±
1
6
7

.

.

3
3
1
±
0
3
2

.

.

8
4
6
0
±
8
4
6

.

.

7
9
7
0
±
2
2
2

.

.

7
5
6
0
±
5
2
5

.

.

6
0
5
0
±
7
1
8

.

.

1
3
5
0
±
2
0
1

.

.

2
5
4
0
±
2
2
1

.

.

7
3
0
±
1
5
1

.

.

6
9
2
0
0
±
2
0
2

.

K
9
4
9

.

K
5
6
7

.

K
5
7
6

.

K
8
0
6

.

K
7
2
5

.

K
1
4
3

.

K
3
7
4

.

K
2
4
3

.

K
9
1
7

.

K
1
8
2

.

K
8
1
7

.

K
9
9
2

.

K
4
7
6

.

K
8
8
5

.

K
7
4
5

.

K
8
6
4

.

K
9
3

.

2
2
8

5
4
.
3
±
7
.
0
1

8
0
.
4
±
6
.
5
1

4
3
.
4
±
3
.
8
1

8
5
.
4
±
4
.
0
2

6
0
7
.
0
±
9
.
7
1

5
5
3
.
0
±
3
.
5
2

6
1
6
.
0
±
6
.
7
1

9
4
3
.
0
±
4
.
5
2

4
2
3
.
0
±
2
.
1
2

5
1
6
.
0
±
3
4
.
6

1
6
2
.
0
±
3
.
1
2

4
6
5
.
0
±
6
2
.
5

2
9
3
.
0
±
3
1
.
8

2
1
3
.
0
±
1
.
0
1

8
8
5
.
0
±
2
.
2
1

8
1
.
1
±
6
2
.
7

7
4
7
0
.
0
±
1
.
5
1

0
.
0
±
0
.
0
2

K
3
8
.
2

K
1
0
.
2

K
2
7
.
1

K
6
3
.
1

K
8
0
.
1

K
5
3
.
1

0
7
4

0
1
5

K
3
3
.
2

K
8
3
.
2

9
2
4

2
2
4

K
4
0
.
3

K
2
3
.
2

K
9
.
1

K
6
.
1

K
1
2
.
1

7
1
2

)
0
1
(
2
d
C
f
i
n
u

)
5
1
(
2
d
C
f
i
n
u

)
8
1
(
2
d
C
f
i
n
u

)
0
2
(
2
d
C
f
i
n
u

)
0
2
,
6
(
2
d
C
s
i
o
p

)
0
3
,
6
(
2
d
C
s
i
o
p

)
0
2
,
3
(
2
d
C
s
i
o
p

)
0
3
,
3
(
2
d
C
s
i
o
p

)
5
,
5
.
0
(
2
t
I
m
o
e
g

)
5
,
1
.
0
(
2
t
I
m
o
e
g

)
0
2
,
5
.
0
(
2
t
I
m
o
e
g

)
0
2
,
1
.
0
(
2
t
I
m
o
e
g

)
5
,
3
(
p
o
o
L
s
b
o

)
8
,
3
(
p
o
o
L
s
b
o

)
0
1
,
3
(
p
o
o
L
s
b
o

)
2
1
,
3
(
p
o
o
L
s
b
o

)
5
1
,
3
(
p
o
o
L
s
b
o

)
0
2
,
3
(
p
o
o
L
s
b
o

Figure 8: KL divergence (red), and mean
and standard deviation (blue), unifCd(18),
Schism, 600 seconds

Figure 9: KL divergence (red), and mean and
standard deviation (blue), mixed(5), Schism,
600 seconds

Figure 10: KL divergence (red), and mean
and standard deviation (blue), poisCd(3, 20),
Schism, 600 seconds

Figure 11: KL divergence (red), and
mean
(blue),
standard
geomIt(0.5, 20), Schism, 600 seconds

deviation

and

25

