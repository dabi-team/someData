1
2
0
2
c
e
D
9
1

]

C
D
.
s
c
[

1
v
4
3
0
0
1
.
2
1
1
2
:
v
i
X
r
a

COX: CUDA ON X86 BY EXPOSING WARP-LEVEL FUNCTIONS
TO CPUS

Ruobing Han
Georgia Institute of Technology
USA
hanruobing@gatech.edu

Jaewon Lee
Georgia Institute of Technology
USA
jaewon.lee@gatech.edu

Jaewoong Sim
Seoul National University
Korea
jaewoong@snu.ac.kr

Hyesoon Kim
Georgia Institute of Technology
USA
hyesoon@cc.gatech.edu

ABSTRACT

As CUDA programs become the de facto program among data parallel applications such as high-
performance computing or machine learning applications, running CUDA on other platforms has
been a compelling option. Although several efforts have attempted to support CUDA on other
than NVIDIA GPU devices, due to extra steps in the translation, the support is always behind a
few years from supporting CUDA’s latest features. The examples are DPC, Hipfy, where CUDA
source code have to be translated to their native supporting language and then they are supported.
In particular, the new CUDA programming model exposes the warp concept in the programming
language, which greatly changes the way the CUDA code should be mapped to CPU programs. In
this paper, hierarchical collapsing that correctly supports CUDA warp-level functions on CPUs is
proposed. Based on hierarchical collapsing, a framework, COX, is developed that allows CUDA
programs with the latest features to be executed efﬁciently on CPU platforms. COX consists of
a compiler IR transformation (new LLVM pass) and a runtime system to execute the transformed
programs on CPU devices. COX can support the most recent CUDA features, and the application
coverage is much higher (90%) than for previous frameworks (68%) with comparable performance.
We also show that the warp-level functions in CUDA can be efﬁciently executed by utilizing CPU
SIMD (AVX) instructions.

Keywords GPU, code migration, compiler transformations

1

Introduction

The high-performance computing power in GPUs has developed a strong software eco-system based on GPU pro-
grams.Although there are other choices for GPU programming such as HIP [1], OpenMP and DPC [2], in recent years,
CUDA is still in the dominant place. In the realm of Deep Learning, both of the two most popular frameworks, Pytorch
and TensorFlow, support only CUDA for GPU backend [3]. In the multimedia realm, for the video editing applications,
as lists in [4], CUDA is compatibility in 14 of the 17 applications, while OpenCL is only supported by 9 of them.

Unfortunately, despite the popularity of CUDA programs, NVIDIA GPUs are the main hardware platforms to run them.
Although there have been several efforts to run CUDA on non-NVIDIA GPUs, they are lack supporting newer CUDA
features. There are mainly two challenges to run CUDA on other platforms are. The ﬁrst is to convert Single Program
Multiple Data (SPMD) programs to programs for non-SPMD friendly architectures. In the SPMD programming model,
the same kernel is executed by many threads at runtime, and the GPU is built on for through-put-oriented architectures
by supporting many threads (or warps). However, other architectures often do not have that many threads, so the CUDA
programs need to be converted to a fewer number of threads efﬁciently. The second problem is a continuous support for

 
 
 
 
 
 
still evolving programming models like CUDA. To address the second problem, we utilize the open-source compiler
frame LLVM as much as possible. In this paper, we tackle the ﬁrst problem: Supporting CUDA with fewer number
threads, which is an essential component for running CUDA on X86, ARM, or Intel-GPU, which has fewer hardware
threads than NVIDIA GPUs.

Several projects aim to support this transformation: running GPU programs on CPUs [2, 5–16]. While a few projects
focus on narrowing the gap between SPMD and MPMD by adding a hardware extension [15] or adding system-level
support for faster context switching [16], most projects try to do compiler-level transformation to translate GPU functions
to be suitable for CPUs. These projects use the same granularity of transformation: a CPU thread is responsible for
executing all the threads in a CUDA block (or OpenCL work-group). The CUDA-block-to-CPU-thread mapping
is optimal based on three observations: 1) it has a fewer CPU threads compared with CUDA-thread-to-CPU-thread
mapping, which can lead to low overhead for context switching; 2) the memory access within a CUDA block utilizes
GPU caches. Both shared memory in the CUDA programming model (or local memory in OpenCL) and global memory
with spatial/temporal locality within a CUDA block utilize caches. These memory accesses from a CUDA block are
mapped into a CPU thread, so those memory accesses would also utilize CPU caches [17]; 3) threads within a block
have similar computation patterns, which makes them amenable to the SIMD instructions [12, 18] common in the
current CPU architectures for further optimizations [19–25]. The transformation is shown in Figure 1(b): for an
original GPU kernel, the translator ﬁrst splits it into different regions according to synchronization instructions and then
wraps each region with a loop whose size equals to the GPU block size. However, this transformation was proposed
based on early GPU programming models and cannot support several important features that were proposed in recent
GPU programming models. One of the signiﬁcant changes is the warp-level programming model in CUDA 1. And this
new feature is critical for achieving high performance. hierarchical collapsing is proposed to support these warp-level
features on CPUs. Although this might sound like a trivial extension, it is critical to identify new types of barriers
in the warp level. And warp- and block-level barriers form a hierarchical relationship that complicates translating
loops or branches into a CPU-friendly version. Throughout the paper, the focus is on the CUDA programming model.
However, the same techniques would also be applicable to other SPMD programming models (e.g., OpenCL [26],
HIP [1], DPC [2]). Based on hierarchical collapsing, a framework, COX, is implemented that efﬁciently executes
CUDA programs with the latest features on X86 devices. COX also uses SIMD instructions explicitly to take advantage
of the hardware features in the latest CPUs.

The main contributions of this paper as follows:

• propose hierarchical collapsing, which provides the correctness for mapping GPU programs that use warp-level

functions to CPU programming models, and implement it with LLVM passes.

• extend the Parallel Region concept into the Hierarchical Parallel Region to provide the correct translation

when the GPU programs have warp-level functions.

• implement the COX framework, which executes CUDA source code on CPUs. The framework includes a new

LLVM pass that contains hierarchical collapsing and a lightweight runtime system. 2

(a) GPU SPMD

(b) Output of ﬂat collapsing

(c) Output of hierarchical collapsing

Figure 1: The programming model for input CUDA SPMD and output CPU programs by ﬂat collapsing and hierarchical
collapsing

1Warp is now ofﬁcially a part of the programming model instead of being a microarchitecture concept.
2the frame will be released as an open source once the paper is accepted.

2

statements-A__syncthreadsstatements-B# of threads..................# of blocks...for-loop ( i = 0 ; i < # of threads, i ++)        Statement-Afor-loop ( i = 0 ; i < # of threads, i ++)        Statement-B# of blocksfor-loop ( i = 0 ; i < # of warps; i ++)    for-loop ( j = 0 ; j < # of threads_per_warp, j ++)        Statement-Afor-loop ( i = 0 ; i < # of warps; i ++)    for-loop ( j = 0 ; j < # of threads_per_warp, j ++)        Statement-B...# of blocks# of warps# of threads per warp...2 Background and Motivation

2.1 Running SPMD Programming Models on CPUs

The basic mechanism to support SPMD programming models on CPUs is to map a thread block/work-group to a CPU
thread and iterate for the thread block/work-group using a loop [5–7, 12, 21]. Figure 1(a) and (b) show the input and
output of the basic process. This transformation has different names in different projects: microthreading [8], thread
aggregation [27], thread-fusion [6], region-based serialization [17], loop chunking [28], and kernel serialization [29].
In this paper, this transformation is called ﬂat collapsing, as it uses a single loop to represent all threads within a
block. The loop can be vectorized by the compiler, and multiple CPU threads (essentially multiple CUDA blocks)
are executed in parallel on multiple cores using runtime system such as p-thread or openMP. When a GPU program
contains synchronization primitives inside (e.g., synchthreads()), the loop needs to be split (loop ﬁssion).

Below is some of the terminologies used in [5, 7, 12] to support this transformation in general cases:

• Parallel Region (PR): These are the regions between barriers that must be executed by all the threads within
a block before proceeding to the next region. Typically, loops are generated to wrap each PR. In Figure 1,
statements-A and statements-B form two PRs.

• Extra barrier: Unlike explicit barriers that are inserted by programmers (e.g., synchthreads()), the ﬂat
collapsing inserts extra barriers that are necessary to deﬁne the Parallel Region. The ﬂat collapsing groups
instructions between barriers as PRs and wraps these PRs by loops. However, when barriers are present in the
conditional statements, the situation becomes more complex. For example, to transform a CUDA kernel that
has a barrier within an if–then construct, ﬂat collapsing has to insert extra barriers in the original CFG so that
it can get correct PRs in the further step.

The deﬁnition of Parallel Region in previous project for ﬂat collapsing cannot support wrap-level features. The ﬂat
collapsing generates a single loop for each PR to simulate all threads within a block. This coarse-grain simulation
cannot distinguish threads among different wraps. In this paper, an extension deﬁnition is proposed and used to support
warp-level features. (See Section 3.5 for details.)

2.2 Warp-level Programming Features

2.2.1 Warp-Level Collectives

CUDA provides a list of warp-level collective functions, 3 which are necessary when achieving high performance for
reduction. This section introduces two of them that are commonly used in existing benchmarks.

• Warp shufﬂe: In early versions, although most GPUs have local memory and global memory to support data
exchange between threads, there was no efﬁcient way to exchange data among threads within a warp. To
efﬁciently exchange data that is stored in registers, CUDA provides a series of warp shufﬂe instructions. When
the warp shufﬂe instructions are invoked, a thread can send its local data to another thread in the same warp.
Warp shufﬂe can be used with warp vote to write more ﬂexible programs.

• Warp Vote: Instead of only exchanging data among threads within a warp, warp vote instructions can directly
make logical reductions (e.g., all, any) for the local variables, controlled by the mask argument. These features,
used with warp shufﬂe and cooperative group, are necessary to implement high-performance reduction kernels.

2.2.2 Cooperative group

In the early CUDA programming models, there are only two explicit groups of threads: block and grid. Users cannot
easily organize a sub-group among a small group of threads within the block. NVIDIA proposed a new concept in
CUDA 7.0 called cooperative group. The corresponding instructions allow users to group threads within a block
and this group can further be used for data exchange. There are two kinds of grouping strategies: static and dynamic.
For the static grouping, it is known whether a thread belongs to a group in compile time (e.g., group threads with index
0 and 1), while for dynamic grouping, it can only be known during runtime (e.g., group all activated threads).

2.2.3 Limitation of COX

This project focuses mainly on the compile-level transformation. Thus, only the static features can be addressed. The
latest CUDA supports several dynamical features. For example, for warp-level collective operations, users can assign

3https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions

3

only a sub-group of threads within the wrap to do the collective operations. The sub-group is organized by a mask
argument at runtime. For the cooperative group, users can also group all activated threads into a group at runtime. For
these warp-level features, although these dynamic features provide more ﬂexibility, they can be harmful for performance,
as they may incur warp-divergence. Thus, most high-performance implementations [30, 31] use warp-level collectives
and the cooperative group without warp-divergence. In the following sections, only the non warp-divergence use cases
for these new features are of concern. For the same reason, only the aligned barriers4 are of concern. In other words, for
a block/wrap barrier, it is assumed that all or none of the threads within a block/wrap can reach this barrier.

2.3 Motivation

Section 2.1 introduced the concepts of Parallel Region (PR) and extra barrier. This section discusses, with examples, the
limitation of these concepts and how to extend them. Assume the input kernel shown in Code 1,5 and its block size is
b_size. The code accumulates the variable val within the ﬁrst warp and stores the accumulated value in the ﬁrst thread
of this warp. Figure 2 illustrates an example of the last two iterations.

1
2
3
4
5

int val = 1;
if ( threadIdx . x < 32) {

for ( int offset = 16; offset > 0; offset /= 2)

val += __shfl_down_sync ( -1 , val , offset ) ;

}

Code 1: Input GPU reduction kernel

Figure 2: Explanation of using shfl_down_sync to implement reduction.

Although none of the existing projects can support this kernel, it is assumed that ﬂat collapsing would generate code
with the following steps:

• group consecutive instructions between two barriers, and wrap each group with a for-loop whose length equals
to the block size. In Code 2, there are three groups that are separately wrapped by three for-loops in line 3,
line 6, and line 9. Please note that the loop in line 5 is from the source input;

• replace the use of threadIdx.x by the loop iteration variable tx;
• replicate variables (e.g., val) used by more than one group into an array form;

Unfortunately, these steps are insufﬁcient to generate the correct CPU program for this GPU kernel. The key reason is
that there are implicit warp-level barriers derived from shfl_down(): each thread within a warp has to ﬁrst calculate its
val and then invoke shfl_down() at the same time (see Section. 3.2 for details). These warp-level barriers are inside a
branch of an if–then construct, which creates a more complex situation. In previous projects, there are only block-level
barriers, so it can safely be assumed that for each barrier instruction, all or no threads within the block can access it.
Thus, it will just wrap the barrier instructions with for-loops with lengths b_size, and these for-loops are located in
the branch of a if–then construct: if it is knwon at runtime that this barrier will be accessed, then the control ﬂow will
directly runs into this branch and execute the for-loop; or go to another branch otherwise. However, in the example,
these warp-level barriers are only accessible for the threads in the ﬁrst wrap. To get the correct result, one needs to not
only wrap the barrier instructions with for-loops, but also replicate the control ﬂow instruction in if–then construct (line
2 in Code 1) and insert them into line 7 and line 10 in Code 2. With the above modiﬁcations, Code 1 would become
Code 2. These transformations are quite complex, even for the demo example, not to mention implementing it in
compilers used for all possible CFGs. Based on the above analysis, hierarchical collapsing is proposed which produces
Code 3. The concept is also illustrated in Figure 1(c). Compared with Code 2, Code 3 has two types of generated loops:
the loop with induction variable wid (line 4) is for the block level called inter-warp loop , while the inner loops with
induction variable tx (lines 5, 7, 12, 14) are for the warp level called intra-warp loop. With inter/intra warp loops,
compared with a single-level loop for all threads within a block, the complexity of the generated code can be reduced:

4https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-synchronization-and

-communication-instructions-bar

5This example is a simpliﬁed version of reduction_kernel.cu in CUDA 10.1 SDK.

4

no longer needs to replicate and insert the if-then construct. Instead, it is maintained only with a simple loop peeling
(line 10 in Code 3). With the low complexity, hierarchical collapsing can easily be implemented and integrated into
compilers. In COX, hierarchical collapsing is implemented as a new LLVM pass to automatically transform the GPU
kernels.

int shfl_arr [32];
int val [ b_size ];
for ( int tx = 0; tx < b_size ; tx ++)

val [ tx ] = 1;

for ( int offset = 16; offset > 0; offset /= 2) {

for ( int tx = 0; tx < b_size ; tx ++)

if ( tx < 32)

shfl_arr [ tx ] = val [ tx ];

for ( int tx = 0; tx < b_size ; tx ++)

if ( tx < 32)

if ( tx + offset < 32)

val [ tx ] += shfl_arr [ tx + offset ];

}

Code 2: CPU warp shufﬂe program generated by ﬂat collapsing

int shfl_arr [32];
int val [ b_size ];
bool flag [32];
for ( int wid = 0; wid < b_size / 32; wid ++) {

for ( int tx = 0; tx < 32; tx ++)

val [ wid * 32 + tx ] = 1;

for ( int tx = 0; tx < 32; tx ++)

flag [ tx ] = ( wid * 32 + tx ) < 32;

// loop peeling
if ( flag [0]) {

for ( int offset = 16; offset > 0; offset /= 2) {

for ( int tx = 0; tx < 32; tx ++)

shfl_arr [ tx ] = val [ wid * 32 + tx ];

for ( int tx = 0; tx < 32; tx ++)

if ( tx + offset < 32)

val [ wid * 32 + tx ] += shfl_arr [ tx + offset ];

}

}

}

1
2
3
4
5
6
7
8
9
10
11
12
13

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

Below are several details worth mentioning:

Code 3: CPU warp shufﬂe program by hierarchical collapsing

• As the input CUDA kernel has a shfl_down inside an if-then construct, this is quite a complex situation: not
all warps can access the implicit warp-level barriers derived from shfl_down. According to CUDA document,
6 for a given warp-barrier, none or all threads within a warp can access it. Thus, loop peeling (line 10 in Code
3) is used to evaluate the condition of the if-then construct only for the ﬁrst thread in the warp, and then all
other threads in the warp just follow the same path. (See Section 3.3 for details);

• Although only flag[0] is needed, the instructions to calculate other elements in flag are also executed.
Because these instructions may have a side effect, to guarantee the correctness, they have to be executed even
these outputs are not needed;

The rest of this paper is organized as follows: Section 3 introduces the key part of the hierarchical collapsing. The
runtime system is introduced in Section 4. Section 5 shows the evaluation of COX with CUDA SDK, heter-Mark, and
GraphBig benchmarks and the comparison of the performance with POCL and DPC, which are the state-of-the-art
open source frameworks. Section 6 provides a survey that describes various attempts to migrate GPU programs to CPU
devices. Finally, concluding thoughts are presented in Section 7.

3

IR Transformation

3.1 Overview of COX

Figure 3 shows an overview of the COX framework. At a high level, a CUDA kernel is compiled with Clang, which
produces NVVM IR [32] for the NVIDIA GPU. Then, COX transforms the NVVM IR into the CPU-friendly LLVM IR.
The hierarchical collapsing is implemented in this transformer. After that, the LLVM IR is linked with host programs
and runtime libraries to generate a CPU-executable ﬁle.

6https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#synchronization-functions

5

Figure 3: COX pipeline for generating CPU executable ﬁles from CUDA kernel codes.

1 __global__ void VoteAll ( int * result ) {
2
3
4 }

int tx = threadIdx . x ;
result [ tx ] = __all_sync ( -1 , tx % 2) ;

Code 4: CUDA Warp Vote example

(a) Original Code.

(b) Step 1: Replace warp function.

(c) Step 2: Insert extra barriers.

(d) Step 3: Split blocks by bar-
riers.

(e) Step 4: Wrap the current CFG with intra-warp loop.

(f) Step 5: Wrap the current
CFG with inter-warp loop.

Figure 4: Steps of NVVM to LLVM-IR transformer in Figure 3.

Figure 4 shows an example of transforming CUDA kernels shown in Code 4 into a LLVM-IR for CPU. First, warp-level
functions are replaced with built-in functions deﬁned in the runtime library, as shown in Step 1 (Section 3.2). Second, in
Step 2, the extra barriers are identiﬁed and inserted (Section 3.3). Last, through Steps 3 to 5, hierarchical parallel regions
are identiﬁed, and intra/inter-warp loops are generated accordingly to create a CPU-friendly version (Section 3.4). After
Step 5, the generated LLVM IR will be compiled and linked with host programs and runtime libraries to generate a
CPU-executable ﬁle.

6

NVVM-IRfor GPUNVVM-IRfor GPUNVVM-IRfor GPUClangCUDA FETransformerLLVMBackendBuilt-in LibraryHostCodeKernelCodeTransformedLLVM-IRfor CPUTransformedLLVM-IRfor CPUTransformedLLVM-IRfor CPUObjectFileObjectFileLinker.cu.ll.llOur workMigration& compileExecutableFileExecutableFileentry:   %0 = call @llvm.nvvm.read.ptx.sreg.tid.x()   %rem = and %0, 1   %1 = call @llvm.nvvm.vote.all.sync(-1, %rem)   %arrayidx = getelementptr %result, %0   store %1, %arrayidx   ret void  entry:     %0 = call @llvm.nvvm.read.ptx.sreg.tid.x()     %rem = and %0, 1     %intra_warp_idx = and %0, 31    %addr = getelementptr @warp_vote, %intra_warp_idx     store %rem, %addr    call @warp.barrier()     %2 = call @warp_all(-1, @warp_vote)    call @warp.barrier()     %arrayidx = getelementptr %result, %0     store %2, %arrayidx     ret void  entry:     call @block.barrier()    %0 = call @llvm.nvvm.read.ptx.sreg.tid.x()     %rem = and %0, 1     %intra_warp_idx = and %0, 31    %addr = getelementptr @warp_vote, %intra_warp_idx     store %rem, %addr    call @warp.barrier()     %2 = call @warp_all(-1, @warp_vote)    call @warp.barrier()     %arrayidx = getelementptr %result, %0     store %2, %arrayidx     call @block.barrier()    ret voidentry:  call @block.barrier()  Br block1block1:    %0 = call @llvm.nvvm.read.ptx.sreg.tid.x()     %rem = and %0, 1     %intra_widx = and %0, 31    %addr = getelementptr @warp_vote, %intra_widx    store %rem, %addr    call @warp.barrier()     br block2block2:     %2 = call @warp_all(-1, @warp_vote)    call @warp.barrier()     br block3exit:​  ret void​block3:    %arrayidx = getelementptr %result, %0     store %2, %arrayidx     call @block.sync()    br exitentryintra_warp_init:​  @intra_widx = 0​  br intra_warp_cond​intra_warp_cond:​  %flag = icmp ult @intra_widx, 32​  br %flag, block1, intra_warp_init2​block1intra_warp_inc:​  @intra_widx = add @intra_widx, 1​  br intra_warp_cond​intra_warp_init2:​  @intra_widx = 0​  br intra_warp_cond​2intra_warp_cond2:​  %flag = icmp ult @intra_widx, 32​  br %flag, block2, exit​block2intra_warp_inc2:​  @intra_widx = add @intra_widx, 1​  br intra_warp_cond2​intra_warp_init3:​  @intra_widx = 0​  br intra_warp_cond​2intra_warp_cond3:​  %flag = icmp ult @intra_widx, 32​  br %flag, block3, exit​block3intra_warp_inc3:​  @intra_widx = add @intra_widx, 1​  br intra_warp_cond3LOOP 1LOOP 2LOOP 3Exitentryinter_warp_init:​  @inter_widx = 0​  br inter_warp_cond​inter_warp_cond:​  %flag = icmp ult @inter_widx, WARP_NUM  br %flag, inter_warp_init, exit​inter_warp_inc:​  @inter_widx = add @inter_widx, 1​  br inter_warp_condexitLOOP 1LOOP 2LOOP 31
2
3
4
5
6
7
8
9
10

; the first warp vote instruction
@warp_vote [ tx ] = 1 ;
call @warp.sync () ; for RAW hazard
% res1 = call @warp_all () ; read from @warp_vote
call @warp.sync () ; for WAR hazard
; the second warp vote instruction
@warp_vote [ tx ] = 2 ;
call @warp.sync () ; for RAW hazard
% res2 = call @warp_all () ; read from @warp_vote
call @warp.sync () ; for WAR hazard

Code 5: Insert implicit Barriers to avoid RAW/WAR hazards

3.2 Support Warp-level Functions

In a GPU architecture, when threads within a warp invoke the warp functions, the GPU will have internal communica-
tions to accumulate and/or communicate the local variables among the warp. To support these features on CPUs, the
corresponding accumulation and communication need to be explicitly performed. This section describes how to support
warp-level collectives.
In the initialization, COX allocates an array warp_vote with length 32. The warp_vote should be stored in CPU
thread local memory, as a CPU thread is used to simulate a GPU block. Otherwise, if warp_vote is stored in global
memory that is accessible to all CPU threads, a data race can occur when multi CPU threads read/write the warp_vote
variable at the same time. A GPU warp vote instruction is translated to the following CPU instructions: for threads
within a warp, ﬁrst, each thread stores its local ﬂag into a different element in warp_vote. After all the elements are
set, the result for this warp vote can easily be computed. The function warp_all is deﬁned in a runtime library that
will be linked at the ﬁnal compilation. To utilize the computation resource of X86, warp_all is implemented with the
AVX instructions. The beneﬁts brought by AVX are evaluated in Section 5.2.3. The ways to support warp shufﬂe are
quite similar. See Code 1 and Code 3 for example.
COX also needs to insert the implicit warp-level barriers when supporting these warp-level functions. As discussed
in [33], two warp-level barriers are required: barriers for the Read-after-Write (RAW) hazard and barriers for the
Write-after-Read (WAR) hazard. The use of these two barriers is shown in Code 5. There are two consecutive warp
vote instructions and the inserted barriers 1) without the barriers for RAW hazard, a thread will invoke warp_all
before other threads set warp_vote[tx] to 1 (the ﬁrst vote) or 2 (the second vote); 2) without the barriers for the WAR
hazard, a thread will set warp_vote[tx] to 2 before other threads invoke the ﬁrst vote function. The use of consecutive
warp-level collective functions is really common when implementing reduction.

3.3

Insert extra Barriers

In Steps 3, 4, and 5, hierarchical collapsing needs barrier information to identify the Parallel Region and generate
intra/inter-warp loops accordingly. Thus, it is important to insert extra barriers that are not shown in the input GPU codes
but necessary for identifying the Parallel Region. Some researchers [12] proposes a similar concept and corresponding
algorithm, but they cannot support warp-level functions. The extra barriers are sourced from barriers in conditional
statements. An example is shown in Figure 5.

(a) Input CUDA kernel

(b) After extra barrier inser-
tion

(c) After loop generation

Figure 5: An example of extra barriers needed for identifying PRs. a) the input CUDA kernel, which has a barrier in
for-loop construct; b) as there is a barrier in the conditional statement, extra barriers are inserted to guide the generation
of intra/inter-warp loops in future steps; c) according to the barriers, two PRs are identiﬁed and two for-loops are
generated separately. Note, all transformations in COX are done in the LLVM IR level. This source code level example
is only used for explanation.

7

To make the hierarchical collapsing work, extra block-level barriers are inserted at the beginning of the entry block and
at the end of the exit block as POCL does [7].
The two most common conditional statements are If-Then construct and 2) For-loop construct.

3.3.1 Barriers in if–then construct

The CFG of a classical if–then construct is shown in left side of Figure 6(a). In the block if.body, there is a barrier.
According to [34], for a block/wrap barrier, none or all threads within the block/wrap can reach it7. Thus, COX can
safely apply loop peeling on the CFG; COX peels the ﬁrst thread to evaluate the branch direction and the rest of the
threads within the warp/block can just follow the same direction. See Code 3 for a loop peeling example. The result after
inserting extra barriers and block split is shown in the right side of Figure 6(a). Several details are worth mentioning:

• insert extra barriers with the same type as the barrier in if.body. In the example, there is a warp barrier in

if.body, thus, hierarchical collapsing also inserts warp barriers as extra barriers;

• after transformation, all blocks will be wrapped by intra-warp loops, except if.cond, which is used for loop

peeling;

• if.cond should contain only a single conditional-branch instruction and it should not have any side-effect. In
Figure 6(a), all computation instructions are put into if.head so that they are executed b_size times, as the
original GPU program does.

(a) Barriers in if-then construct.

Figure 6: After transformation, the inserted barriers are shown in bold. The PRs identiﬁed in further step are also shown
in the ﬁgure.

(b) Barriers in for-loop construct

The detailed algorithm for inserting extra barriers derived from barriers in if-then construct is described in Algorithm 1.
COX has to do some additional checking to avoid an inﬁnite loop caused by a for-loop construct in CFG. For simplicity
this checking part is not shown in Algorithm 1.

7This rule does not exist for non-aligned barriers in CUDA, which is beyond the scope of this paper.

8

Algorithm 1 Transformation for inserting extra barriers for barrier in if-then construct

Input: K: The CFG for the input kernel
Input: P DT : The Post Dominator Tree for the input CFG
Input: DT : The Dominator Tree for the input CFG
1: conditional_block ← []

(cid:46) Find all barriers in if-body construct

if has_barrier(block) then

if !P DT.dominates(block, K.entry) then
conditional_block.insert(block)

2: for all block ∈ K do
3:
4:
5:
6:
7:
8: end for

end if

end if

(cid:46) Insert extra barriers

9: for all block ∈ conditional_block do
10:
11:
12:
13:

N earestEntry ← block.precessor
while P DT.dominates(block, N earestEntry) do
N earestEntry ← N earestEntry.precessor

end while

14:
15:
16:
17:
18:
19:
20:

21:

22:

(cid:46) Insert barrier in the end of if-head

insert_barrier_bef ore(precessor.terminator)
pre_successor ← block
successor ← block.successor
while DT.dominates(block, successor) do

pre_successor ← successor
successor ← successor.successor

end while

(cid:46) Insert barrier in the beginning of if-exit

insert_barrier_bef ore(successor.begin)

(cid:46) Insert barrier in the end of if-body

insert_barrier_bef ore(pre_successor.terminator)

(cid:46) Inserted extra barriers may generate another if-then construct that contains barriers

if !P DT.dominates(N earestEntry, K.entry) then
conditional_block.insert(N earestEntry)

23:
24:
25:
26: end for

end if

3.3.2 Barriers in for-loop construct

Although CUDA supports several loop styles (e.g., for-loop, while-loop, do-while-loop), after LLVM’s transformation,
all loops will be simpliﬁed to the canonical format, which 1) has single latch; 2) has the loop headers dominating all
exit blocks.8 Thus, COX only needs to concern these canonical loops.

COX inserts extra barriers before/after the branch instructions (back edge of the loop). Figure 6(b) shows the example
of inserting extra barriers for a for-loop construct which contains a block barrier. The same as with an if–then construct,
all these inserted extra barriers (shown by bold text in the ﬁgure) should have the same type as with the barriers in
for.header (block barrier in the example).

3.3.3 Supporting other conditional statements

CUDA is a high-level ﬂexible language; even a single concept can generate quite a different CFG. For example, the
loop concept can be implemented by different CFGs, such as do-while-loop, while-loop, and for-loop. However, with
the existing LLVM transformations, COX can automatically convert the input CFGs to canonical formats and only
focus on these canonical formats in the above discussion. Below are some important features in the canonical format:

8A latch is a node in the loop that has an edge to the loop header. An exiting edge is an edge from inside the loop to a node

outside of the loop. The source of such an edge is called an exiting block, its target is an exit block. [35]

9

• Each branch instruction has only two successors; most input CFGs already have this feature, except the CFG
which uses switch-case construct. For these exceptions, COX uses LLVM’s lowerswitch transformation to
convert the switch-case construct into if–then constructs.

• All loops are in canonical format that 1) they all have pre-headers; 2) each loop has only a single latch, in
other words, a single backedge; and 3) the loop header will dominate all exit blocks. COX calls LLVM’s
loop − simplif y transformation to translate the input CFG loops to the canonical format.

3.4 Split Blocks Before/After Each Barrier

As the instructions before/after a barrier in a block need to be wrapped by different intra/inter-warp loops, in this step,
COX splits the blocks that have barriers inside. See Step 3 in Figure 4 for an example.

3.5 Hierarchical Parallel Region

As discussed in Section 2.3, each parallel region (PR) becomes a for-loop. Due to the warp-level functions, COX has to
generate two kinds of for-loops: inter-warp loop and intra-warp loop. Thus, COX needs two kinds of Parallel Regions:
1) warp-level Parallel Region, which will be wrapped by intra-warp loop and 2) block-level Parallel Region, which will
be wrapped by inter-warp loop. It is obvious that a warp-level PR will always be a subset for a block-level PR (a GPU
warp is always within a GPU block); thus, the new concept is called a Hierarchical Parallel Region. An example of a
Hierarchical Parallel Region is shown in Figure 7.

Figure 7: As there is a warp barrier in block1, after transformation, there are two warp-level PR ({ block1},{block2})
and a single block-level PR ({block1, block2}).

Thus, the rest of the steps are for ﬁnding the block/warp level PRs and wrapping them with inter/intra-warp loops.
The algorithm (Alg. 2) is used for ﬁnding the set of warp-level PRs. The algorithm for ﬁnding the block-level PRs
is very similar, except it only concerns the block barrier. COX cannot ﬁnd the PR for the warp level and block level
simultaneously: COX ﬁrst ﬁnds all warp-level PRs and generates intra-warp loops to wrap these PRs. Then, COX ﬁnds
the block-level PRs in the new CFG and wrap them with inter-warp loops.

3.6 Wrap PR with For-Loop

In this step, COX wraps warp/block-level PRs by intra/inter-warp loop. Please see Figure 4(e)(f) for an example.
Although this step is quite straightforward, it requires proving the correctness 9: after inserting intra/inter-warp loops,
each instruction from the input GPU kernel is executed b_size times (b_size is the block size), except the instructions
used for loop peeling.

Finally, after adding intra/inter-warp loops, some local variables are needed to be replicated: for local variables that are
used in several warp-level PRs but only used in a single block-level PR, they are replicated with an array of length 32.
For local variables that are used among different block-level PRs, they are replicated by an array of length equals to
block size.

9Due to page limitation, we move the proof into Appendix

10

Algorithm 2 Find all warp-level PRs

Input: K: The CFG after Step3
Output: P R_set: The set of PRs.
1: P R_set ← {}
2: end_block ← []
3: for all block ∈ K do
4:
5:
6:
7: end for

end if

end_block.insert(block)

if block contains warp/block barrier then

(cid:46) Find the PR that block belongs to

if block has more than one precessors then

end if
P R ← {block}
pending_block ← block.precessors
while !pending_block.empty() do

current ← pending_block.f ront()
pending_block.pop()
if has visited current then

continue

8: for all block ∈ end_block do
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:

end while

continue

continue

end if
if current has warp/block barriers then

end if
P R.insert(current)
pending_block.insert(current.preprocessors)

(cid:46) This is the exit of an if-then construct

(cid:46) Blocks for loop peeling do not belong to any PR

if PR only has a single block which only contains a conditional branch then

26:
27:
28:
29:
30: end for

continue

end if
P R_set.insert(P R)

4 Runtime System

The above section describes only the CUDA device part. As for the CUDA host part which involves memory allocation,
memory transfer, and kernel launch, these features has to be manually migrated. The automatic translation from
CUDA host code to CPU is left for future work. In the runtime system, p-thread is used for multi-threads. In this
paper, both host and device are x86; thus, CUDA malloc and memcpy are replaced by C malloc and memcpy. In
Figure 8(a), a CUDA host example is presented for vector copy. The migrated COX host code is shown in Figure 8(b),
with the corresponding CUDA operations recorded in the comments. Compared with the CUDA host program, the
COX host program has the following differences: 1) COX uses thread-local variable block_index to store the block
index, and the block index is explicitly set during invocation; 2) COX replaces the CUDA memory operations with
corresponding CPU operations; 3) COX uses pthread fork/join to replace kernel launch in CUDA. There are several
potential optimization for the runtime system, such as using thread-pool instead of fork/join for kernel launching and
using regular expression or LLVM transformation to automatically generate COX host programs from the CUDA source
code. These optimizations are beyond the scope of this paper and are open for future research.

The following steps make up the workﬂow for COX: It 1) compiles the input CUDA source code with Clang and gets the
NVVM IR of kernel functions and; 2) transforms the NVVM IR with hierarchical collapsing; 3) links the transformed
kernel with the COX host program (manually migrated from CUDA host program) and generates the CPU-executable
ﬁle. COX has two modes: 1) normal mode to maintain the runtime conﬁguration as variables (e.g., grid size, block size)
and 2) JIT mode to compile the program with the given runtime conﬁguration. In the normal mode, COX only needs
to compile programs once, and it can be used for different runtime conﬁgurations. In JIT mode, a program has to be
recompiled when executed with different conﬁgurations. Although the JIT mode requires recompiling, in some cases,

11

(a) CUDA host code.

Figure 8: The host code in COX is similar with the original CUDA host code. The automatic migration from CUDA
host code to COX host code is open for future work.

(b) Migrated COX host code.

12

it can generate higher-performance programs, as it provides more opportunities for optimizations. For more details,
please see Section 5.2.2.

5 Experimental Results

To verify the correctness and performance, this section describes the experiments for supporting the CUDA kernel in
several benchmarks on X86 and ARM architectures. Although several frameworks also support executing CUDA on
CPU, most of them were developed decades ago and cannot really support programs that use new CUDA features.
Below are the hardware and software environments for the experiments:

• Software: Ubuntu 18.04, LLVM 10.0, gcc 7.5.0, CUDA 10.1, POCL 1.4, DPCT [2] Ver. : 2021.3.0.
• X86-Hardware: 8 x Intel(R) Xeon(R) Silver 4210 CPU @ 2.20GHz
• ARM-hardware: 48 x ARM(R) A64FX CPU @ BogoMIPS 200.00
• Benchmarks: CUDA SDK 10.1, Hetero-mark [36], GraphBig [31];
• Time: average time for running more than 1000 times; fork/join time of threads is also included.

5.1 Coverage

Table 1 analyzes examples in CUDA SDK10.1 that use but do not require special hardware support (e.g., tensorcore,
uniﬁed memory). POCL and DPCT are chosen for the coverage comparisons since they are the currently activated
projects that support executing CUDA programs on CPUs. As POCL is designed for OpenCL and cannot directly
execute CUDA programs, a third-party translator [37] is used to support executing CUDA with POCL. Besides, although
POCL has both compilation and runtime parts, only the compilation is used in this experiment. For POCL evaluation,
the GPU programs is compiled by POCL and then executed on COX. Thus, it’s fair to compare the execution time to
show the results of compilation and avoid the effect of runtime system.
As shown in the table, the existed frameworks can only automatically support at most 21 kernels (coverage=68%).
Those failed kernels are using new CUDA features. On the other hand, COX supports 28 kernels (coverage=90%).

kernel name
initVectors
gpuDotProduct
gpuSpMV
r1_div_x
a_minus
gpuConjugateGradient
multigpuConjugateGradient
MatrixMulCUDA
matrixMul
copyp2p
reduce0
reduce1
reduce2
reduce3
reduce4
reduce5
reduce6
shﬂ_intimage_rows
shﬂ_vertical_shﬂ
shﬂ_scan_test
uniform_add
reduce
reduceFinal
simpleKernel
VoteAnyKernel1
VoteAllKernel2
VoteAnyKernel3
spinWhileLessThanone
matrixMultiplyKernel
vectorAdd
ﬁlter_arr
Coverage

features

warp cooperative group

grid sync
multi grid sync

block cooperative group
block cooperative group
block cooperative group
block cooperative group
warp cooperative group
warp cooperative group
warp cooperative group
warp shufﬂe
warp shufﬂe
warp shufﬂe

warp cooperative group
warp cooperative group

warp vote
warp vote
warp vote

activated thread sync

POCL
(cid:51)
(cid:55)
(cid:51)
(cid:51)
(cid:51)
(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:51)
(cid:55)
39%

DPCT
(cid:51)
(cid:55)
(cid:51)
(cid:51)
(cid:51)
(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:55)*
(cid:51)
(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:55)
68%

COX
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:55)
90%

Table 1: Coverage of COX compared to other frameworks. *enabled by manual code migration [38]

The CUDA features supported by POCL, DPCT and COX is also shown in Figure 9.

Although the coverage with COX can be signiﬁcantly improved, there are still three kernels that cannot be supported
yet. gpuConjugateGradient and multiGpuConjugateGradient rely on synchronization between different grids
and devices, which utilize the grid cooperative group and multi-grid cooperative group separately. f ilter_arr uses a
dynamic cooperative group: it dynamically groups all activated threads. As discussed in Section 2.2.2, all these features

13

Figure 9: The CUDA features supported by POCL, DPCT and COX.

should be supported at the runtime level: frameworks should schedule threads accordingly at runtime, and each thread
can only know whether it is activated during runtime. Supporting runtime features is included for future work.

5.2 Performance

Figure 10 shows a performance comparison of POCL, DPC, and COX on CUDA SDK, Hetero-Mark, and GraphBig
benchmark on X86 architecture.

Figure 10: The normalized execution time of POCL and DPC on X86 architecture. The normalized execution time is
the POCL/DPC execution time divide by COX’s execution time. Thus, the COX’s normalized execution time is always
1.

In most cases, COX and POCL have close execution time. Thus, the POCL’s normalized execution times are always
close to 1. However, DPC’s normalized execution time has large variance. This is due to: 1) DPC’s most optimizations
are for multiple block cases, which are runtime optimization. While in the evaluation, to shown the compile-level
optimization, there is only a single block in each application; 2) DPC has optimizations on new Intel CPUs, while
POCL and COX do not have special optimizations for the new Intel architectures.

The evaluation results for ARM CPU with AArch64 architecture is shown in Figure 11. As DPC does not support ARM
CPU, only POCL and COX are evaluated. The performance between COX and POCL are close among all experiments.

5.2.1 Performance effects of ﬂat collapsing vs. hierarchical collapsing

Although hierarchical collapsing can support wrap-level features, it generates nested loops instead of a single loop
as ﬂat collapsing does. The complex nested loops incurs more instructions and also makes it difﬁcult to do some
optimizations. Figure 12 shows the overhead of hierarchical collapsing over ﬂat collapsing on X64 architecture, for three
micro-benchmarks by varying the vector/matrix sizes, and none of these three benchmarks use warp-level functions. As
the results show, hierarchical collapsing downgrades performance by 13% on average due to additional instructions.
Hence, COX uses hybrid-mode: for each input kernel, ﬁrst checkes whether there are warp-level functions or other
features for which cannot be supported by ﬂat collapsing. If not, ﬂat collapsing is used in default.

14

Figure 11: The normalized execution time of COX (normalized with POCL execution time) on ARM CPU. In all cases,
COX and POCL has close performance. DPC does not support ARM CPU.

Figure 12: Performance comparisons of ﬂat collapsing and hierarchical collapsing

5.2.2 Normal mode vs JIT mode

Loop optimization is an important optimizations for high-performance programs. Although the intra-warp loop’s
length is always 32, the inter-warp loop’s length depends on the block size, a runtime conﬁguration. COX supports
two compile modes: normal mode and JIT mode. Although the programs generated by these two modes will both
forward to LLVM’s optimizer (with −O3 ﬂag), they have an obvious difference, especially when compiling complex
kernels. Figure 13 shows the difference in execution time between two modes. These two modes have a relatively small
difference for the VectorAdd kernel, as it is quite simple and can easily be vectorized with compiler optimization even
the block size is not provided at compile time. However, for more complex kernels, JIT mode generates programs with
higher performance.

Figure 13: JIT mode can generate faster programs, especially for complex kernels.

15

function

vote any

vote all

time (µs)
instructions
branches
time (µs)
instructions
branches

w/ AVX
0.241
1,447,901,852
100,110,593
0.236
1,384,476,021
100,108,177

w/o AVX
2.542
23,472,339,251
4,260,452,162
2.992
29,552,745,486
5,220,517,219

Table 2: Both functions gain around 10x speed up when using AVX instructions.

5.2.3 SIMD instructions

For CPU programs, SIMD instructions are necessary to achieve high performance [18, 39–41]. The warp vote function
execution time with/without AVX is shown in Table 2. With AVX instructions, around 10x speedup is achieved for both
functions. The beneﬁt is due to fewer instructions and branches.

5.2.4 Scalability

Besides the single block execution time, the execution time for multi-block cases are also measured and the result is
shown in Figure 14. In the Hetero-mark benchmark, the kernels have ﬁxed block sizes. Thus, to enlarge the grid size,
workload size is also enlarged. As the X86 platform has eight CPU cores; the speed up signiﬁcantly degrades when the
grid sizes are larger than eight. Up to eight, it shows good scalability.

Figure 14: Multi-core execution time with COX.

6 Related Work

The CPU architecture belongs to MPMD, while the GPU architecture is SPMD. Although users can naively execute
a GPU thread with a CPU thread, due to the limited parallelism in CPU, the system can only execute around 100
CPU threads simultaneously, which is much smaller than the parallelism in the GPU architecture. Thus, to achieve the
same number of threads as a GPU, the CPU has to create more threads than it can actually execute simultaneously,
which will incur a large amount of thread context switching overhead. Two methods solve this issue. The ﬁrst is to
accelerate the context-switching time in the CPU. Some researchers extend the CPU architecture to accelerate the
context switching [15], these hardware-level extensions are beyond the scope of this paper. In the software level, [16]
proposes to use lightweight threading to accelerate the context switching. Context switching only stores and reloads a
few registers, while maintaining the stack memory. Most modiﬁcations are in the runtime level, and users can directly
use the original GPU source code. As reported in [17], the AMD CPU OpenCL implementation is based on this
technology. However, even with these optimizations, there is still signiﬁcant overhead for context switching, around
10ns per switching.
Thus, another direction is being explored: increasing the workload of each CPU thread. For each CPU thread,
instead of executing a single GPU thread, it executes whole GPU threads within a block. This mechanism can elicit
two beneﬁts: 1) it can increase the CPU execution time to make it much larger so that context switching overload
becomes negligible; 2) with more workload in a single thread, there are more opportunities to do optimizations
(e.g., vectorization, loop transformation). This mechanism has several different names: microthreading [8], thread
aggregation [27], thread-fusion [6], region-based serialization [17], loop chunking [28], and kernel serialization [29]. In
this paper, this mechanism is given a new name: ﬂat collapsing. In [5, 8], the authors propose to wrap an SPMD kernel

16

with a loop, and the loop size is equal to the block size. Thus, each loop iteration can simulate a GPU thread within a
block, and the a CPU thread is mapped to a GPU block. An important technical detail is supporting synchronization
instructions: compilers should separately wrap instructions before/after a synchronization instruction into different loops
to maintain the correctness. A similar technology is also discussed in [28] which utilizes loop transformations (e.g.,
loop strip-mining, interchange, distribution, unswitching) to transform SPMD execution models with synchronization
to Task Parallel execution models. The authors in [27] propose improved static analysis to vectorize the generated
loop-programs to improve the performance, and also propose another algorithm to wrap the original kernels with loops
to avoid additional synchronization points in previous works. In some GPU architectures, such as NVIDIA GPU,
there is implicit lock step within a group of threads. [42] proposed transformations to detect these implicit warp-level
synchronizations and maintained them during transformations. The authors in [17] propose to use C Extensions for
Array Notation to further accelerate the generated CPU programs with SIMD execution and better spatial locality.
Several projects have been proposed to execute CUDA on non-NVIDIA devices. In the early days, NVIDIA provided
an emulation framework [14] to execute CUDA on a CPU; each thread within a GPU block is executed by a CPU
thread. Horus [43] is another emulator. It supports parsing and executing NVIDIA PTX instructions on CPU devices.
These emulators are for debugging rather than for performance. In MCUDA [5], the authors also use a source-to-source
translation to translate CUDA to C with the ﬂat collapsing mechanism. Ocelot [6] uses the same mechanism, but
instead of source-to-source translation, it converts in the PTX level to avoid recompiling. MapCG [44] is a hybrid
computing framework. It uses source-to-source translation to translate CUDA kernels to C programs, which can be
executed on a CPU. [45] proposes another framework for hybird-computing based on Ocelot to translate GPU programs
on the PTX level. Cumuls [29] uses Clang to parse the CUDA programs and modiﬁes them on AST level. Cumuls is
mainly concern on CUDA runtime support, as for compilation part, it reuses the transformation in MCUDA. Instead
of directly translating CUDA/PTX to CPU executable ﬁles, other projects utilize the portability of other front-end
languages. The authors in [46, 47] propose using source-to-source translation to translate CUDA to OpenCL. Instead of
source-to-source translation, [37, 48] implements the translations with LLVM IR. DPC++ Compatibility Tool [2] and
HIPIFY [9] are frameworks that translate CUDA to source languages for Intel and AMD devices.
Most related works only focus on supporting old CUDA features. However, the rapid evolution of GPU hardware
and software stacks bring lots of new features which are important to achieve high performance, such as warp-level
collectives, uniﬁed memory and CudaGraph. Achieving high coverage on these new features is still an ongoing project.
The researchers in [33] propose to use explicitly barriers and memory exchanges to support warp shufﬂe on OpenMP,
which shares the same insight with COX.
OpenCL is another framework that supports executing SPMD programs on MPMD architectures. POCL [7] is an open
source OpenCL implementation which supports CPU backend. To support SPMD programs on CPU, POCL implements
ﬂat collapsing on LLVM IR level. The authors in [12] also proposes to use ﬂat collapsing on OpenCL, but with a
different method to insert extra synchronization and ﬁnd Parallel Region which result in fewer extra synchronization
barriers. However, this method is not extendable for Hierarchical Parallel Region, thus, cannot be utilized to support
warp-level features. In [49], another OpenCL implementation has been proposed, which mainly focus on support
OpenCL programs on multi-device clusters with heterogeneous devices.

7 Conclusion

This project proposes and builds COX, a framework that supports executing CUDA kernels on CPU devices. It also
proposes hierarchical collapsing which can be used to transform SPMD programs to MPMD friendly programs and it
supports the latest warp-level functions in CUDA. Using CUDA 10.1 Sample as a benchmark, the previous projects can
only support 21 of 31 kernels (coverage=68%), while COX can support 28 (coverage=90%). The kernel performance is
also compared on X86 and AArch64 architectures and shows that the performance is comparable. COX is based on
compile-time transformation. Future work will provide a runtime system to support other CUDA features.

17

References

[1] AMD. Hip. https://github.com/ROCm-Developer-Tools/HIP, 2021.

[2] Intel. Dpct. https://software.intel.com/content/www/cn/zh/develop/tools/oneapi/components

/dpc-compatibility-tool.html, 2021.

[3] Abu Asaduzzaman, Alec Trent, S Osborne, C Aldershof, and Fadi N Sibai. Impact of cuda and opencl on parallel
and distributed computing. In 2021 8th International Conference on Electrical and Electronics Engineering
(ICEEE), pages 238–242. IEEE, 2021.

[4] MacFinder. 2020gpgpu roundup. https://macfinder.co.uk/blog/2020-gpgpu-roundup-metal-vs-c

uda-vs-opencl-amd-vs-nvidia/, 2020.

[5] John A Stratton, Sam S Stone, and W Hwu Wen-mei. Mcuda: An efﬁcient implementation of cuda kernels for
multi-core cpus. In International Workshop on Languages and Compilers for Parallel Computing, pages 16–30.
Springer, 2008.

[6] Gregory Diamos, Andrew Kerr, Sudhakar Yalamanchili, and Nathan Clark. Ocelot: a dynamic optimization
framework for bulk-synchronous applications in heterogeneous systems. In 2010 19th International Conference
on Parallel Architectures and Compilation Techniques (PACT), pages 353–364. IEEE, 2010.

[7] Pekka Jääskeläinen, Carlos Sánchez de La Lama, Erik Schnetter, Kalle Raiskila, Jarmo Takala, and Heikki
Berg. pocl: A performance-portable opencl implementation. International Journal of Parallel Programming,
43(5):752–785, 2015.

[8] John A Stratton, Vinod Grover, Jaydeep Marathe, Bastiaan Aarts, Mike Murphy, Ziang Hu, and Wen-mei W
Hwu. Efﬁcient compilation of ﬁne-grained spmd-threaded programs for multicore cpus. In Proceedings of the 8th
annual IEEE/ACM international symposium on Code generation and optimization, pages 111–119, 2010.

[9] AMD. Hipify. https://github.com/ROCm-Developer-Tools/HIPIFY, 2021.

[10] Intel. Intel-opencl. https://software.intel.com/content/www/us/en/develop/tools/opencl-sdk.

html, 2021.

[11] NVIDIA. Pgi. https://developer.nvidia.com/pgi-cuda-cc-x86, 2011.
[12] Ralf Karrenberg and Sebastian Hack. Improving performance of opencl on cpus. In International Conference on

Compiler Construction, pages 1–20. Springer, 2012.

[13] GF Diamos, AR Kerr, S Yalamanchili, and NC Ocelot. A dynamic optimization framework for bulk-synchronous
applications in heterogeneous systems. In Proceedings of the 19th International Conference on Parallel Architec-
tures and Compilation Techniques, PACT, volume 10, pages 353–364, 2010.

[14] Compiling and executing cuda programs in emulation mode. https://developer.nvidia.com/cuda-tool

kit.

[15] Kuan-Chung Chen and Chung-Ho Chen. Enabling simt execution model on homogeneous multi-core system.

ACM Transactions on Architecture and Code Optimization (TACO), 15(1):1–26, 2018.

[16] Jayanth Gummaraju, Ben Sander, Laurent Morichetti, Benedict R Gaster, Michael Houston, and Bixia Zheng.
Twin peaks: a software platform for heterogeneous computing on general-purpose and graphics processors. In
2010 19th International Conference on Parallel Architectures and Compilation Techniques (PACT), pages 205–215.
IEEE, 2010.

[17] John A Stratton, Hee-Seok Kim, Thoman B Jablin, and Wen-Mei W Hwu. Performance portability in accelerated

parallel kernels. Center for Reliable and High-Performance Computing, 2013.

[18] Hwancheol Jeong, Sunghoon Kim, Weonjong Lee, and Seok-Ho Myung. Performance of sse and avx instruction

sets. In proceedings of the 30th International Symposium on Lattice Field Theory, 2012.

[19] Dorit Nuzman, Ira Rosen, and Ayal Zaks. Auto-vectorization of interleaved data for simd. ACM SIGPLAN Notices,

41(6):132–143, 2006.

[20] Dorit Nuzman and Richard Henderson. Multi-platform auto-vectorization. In International Symposium on Code

Generation and Optimization (CGO’06), pages 11–pp. IEEE, 2006.

[21] Ralf Karrenberg and Sebastian Hack. Whole-function vectorization.
Generation and Optimization (CGO 2011), pages 141–150, 2011.

In International Symposium on Code

[22] Dorit Nuzman and Ayal Zaks. Autovectorization in gcc–two years later. In Proceedings of the 2006 GCC

Developers Summit, pages 145–158, 2006.

18

[23] Saeed Maleki, Yaoqing Gao, Maria J Garzar, Tommy Wong, David A Padua, et al. An evaluation of vectorizing
compilers. In 2011 International Conference on Parallel Architectures and Compilation Techniques, pages
372–382. IEEE, 2011.

[24] Vasileios Porpodas. Supergraph-slp auto-vectorization.

In 2017 26th International Conference on Parallel

Architectures and Compilation Techniques (PACT), pages 330–342. IEEE, 2017.

[25] Ira Rosen, Dorit Nuzman, and Ayal Zaks. Loop-aware slp in gcc. In GCC Developers Summit. Citeseer, 2007.
[26] Aaftab Munshi. The opencl speciﬁcation. In 2009 IEEE Hot Chips 21 Symposium (HCS), pages 1–314. IEEE,

2009.

[27] Yao Zhang, Mark Sinclair, and Andrew A Chien. Improving performance portability in opencl programs. In

International Supercomputing Conference, pages 136–150. Springer, 2013.

[28] Jun Shirako, Jisheng M Zhao, V Krishna Nandivada, and Vivek N Sarkar. Chunking parallel loops in the presence
of synchronization. In Proceedings of the 23rd international conference on Supercomputing, pages 181–192,
2009.

[29] Vera Blomkvist Karlsson. Cumulus-translating cuda to sequential c++: Simplifying the process of debugging

cuda programs, 2021.

[30] NVIDIA. Cub. https://nvlabs.github.io/cub/, 2021.
[31] Lifeng Nai, Yinglong Xia, Ilie G Tanase, Hyesoon Kim, and Ching-Yung Lin. Graphbig: understanding graph
computing in the context of industrial solutions. In SC’15: Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis, pages 1–12. IEEE, 2015.

[32] NVIDIA. Nvvm. https://docs.nvidia.com/cuda/nvvm-ir-spec/index.html#introduction, 2016.
[33] Atmn Patel, Shilei Tian, Johannes Doerfert, and Barbara Chapman. A virtual gpu as developer-friendly openmp

ofﬂoad target. In 50th International Conference on Parallel Processing Workshop, pages 1–7, 2021.

[34] NVIDIA. Cuda-sync. https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#syn

chronization-functions, 2021.

[35] LLVM. Llvm loop terminology. https://llvm.org/docs/LoopTerminology.html, 2013.
[36] Yifan Sun, Xiang Gong, Amir Kavyan Ziabari, Leiming Yu, Xiangyu Li, Saoni Mukherjee, Carter McCardwell,
Alejandro Villegas, and David Kaeli. Hetero-mark, a benchmark suite for cpu-gpu collaborative computing. In
2016 IEEE International Symposium on Workload Characterization (IISWC), pages 1–10. IEEE, 2016.

[37] Ruobing Han, Blaise Tine, Jaewon Lee, Jaewoong Sim, and Hyesoon Kim. Supporting cuda for an extended risc-v

gpu architecture. arXiv preprint arXiv:2109.00673, 2021.

[38] Yuhsiang M. Tsai, Terry Cojean, and Hartwig Anzt. Porting a sparse linear algebra math library to intel gpus,

2021.

[39] Jose-Ignacio Agulleiro and Jose-Jesus Fernandez. Tomo3d 2.0–exploitation of advanced vector extensions (avx)

for 3d reconstruction. Journal of structural biology, 189(2):147–152, 2015.

[40] Berenger Bramas. Fast sorting algorithms using avx-512 on intel knights landing. arXiv preprint arXiv:1704.08579,

305:315, 2017.

[41] Matt Pharr and William R Mark. A spmd compiler for high-performance cpu programming. Proceedings of
the 2012 Innovative Parallel Computing: Foundations & Applications of GPU, Manycore, and Heterogeneous
Systems, 2012.

[42] Ziyu Guo, Eddy Zheng Zhang, and Xipeng Shen. Correctly treating synchronizations in compiling ﬁne-grained
spmd-threaded programs for cpu. In 2011 International Conference on Parallel Architectures and Compilation
Techniques, pages 310–319. IEEE, 2011.

[43] Amr S Elhelw and Sreepathi Pai. Horus: A modular gpu emulator framework. In 2020 IEEE International

Symposium on Performance Analysis of Systems and Software (ISPASS), pages 104–106. IEEE, 2020.

[44] Chuntao Hong, Dehao Chen, Wenguang Chen, Weimin Zheng, and Haibo Lin. Mapcg: Writing parallel program
portable between cpu and gpu. In Proceedings of the 19th international conference on Parallel architectures and
compilation techniques, pages 217–226, 2010.

[45] Changmin Lee, Won Woo Ro, and Jean-Luc Gaudiot. Boosting cuda applications with cpu–gpu hybrid computing.

International Journal of Parallel Programming, 42(2):384–404, 2014.

[46] Matt J Harvey and Gianni De Fabritiis. Swan: A tool for porting cuda programs to opencl. Computer Physics

Communications, 182(4):1093–1099, 2011.

19

[47] Paul Sathre, Mark Gardner, and Wu-chun Feng. On the portability of cpu-accelerated applications via automated
source-to-source translation. In Proceedings of the International Conference on High Performance Computing in
Asia-Paciﬁc Region, pages 1–8, 2019.

[48] Hugh Perkins. Cuda-on-cl: a compiler and runtime for running nvidia® cuda™ c++ 11 applications on opencl™

1.2 devices. In Proceedings of the 5th International Workshop on OpenCL, pages 1–4, 2017.

[49] Jungwon Kim, Sangmin Seo, Jun Lee, Jeongho Nah, Gangwon Jo, and Jaejin Lee. Snucl: an opencl framework
for heterogeneous cpu/gpu clusters. In Proceedings of the 26th ACM international conference on Supercomputing,
pages 341–352, 2012.

A Proof of the generation kernel

In this section, we prove that after inserting intra-wap loops and inter-warp loops, each instruction from the input GPU
kernel is executed b_size times (b_size is the block size), except the instructions used for loop peeling.

Proof1: For if-body/for-body that contains a warp/block barrier, their conditional branch instructions do not
belong to any warp/block-level PR
For if-then construct after Step 3, we insert barriers in the end of if-head (barrier in if.head), the beginning of if-exit
(barrier in if.exit) and the end of if-body (barrier in if.body2), shown in Figure 6(a). As for for-latch, we insert two
barriers around the conditional branch instruction, as shown in Figure. 6(b). For if.cond, it can only be included in PR
together with if.body1 or if.exit. For if.exit, it has two precessors, thus we will ignore it line10 in Alg. 2. As
for if.body1, as it does not post-dominate if.cond, it will not include if.cond into the PR. The proof of for.cond
is quite same.
Proof2: All other blocks must belong to one and only one warp/block-level PR
This paragraph only proof the block-level PR. The warp-level PR is quite similar. First, let us prove a block can only
belong to at most one block-level PR. If a block B belongs to more than one PRs, this block must be the ancestor of
two blocks that both have block-level barriers. Assuming the block NCA is the nearest common ancestor of these two
blocks in the control ﬂow graph, it is obvious that NCA is the head of an if-then-construct or the latch of a for-construct.
However, from the Proof1, we know neither of them belongs to any PR. In other situations, if a block B doesn’t belong
to any PR, it means all nodes that post-dominates B do not have a block-level barrier. However, in Step2, we insert a
block-level barrier at the end of the exit block. And exit block will always post-dominates B.

20

