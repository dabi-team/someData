1
2
0
2

b
e
F
1
1

]

G
L
.
s
c
[

2
v
3
2
1
0
0
.
2
1
0
2
:
v
i
X
r
a

A Hypergradient Approach to Robust Regression
without Correspondence

Yujia Xie(cid:42), Yixiu Mao(cid:42), Simiao Zuo, Hongteng Xu,
Xiaojing Ye, Tuo Zhao, Hongyuan Zha†

Abstract

We consider a variant of regression problem, where the correspondence between input and
output data is not available. Such shuﬄed data is commonly observed in many real world problems.
Taking ﬂow cytometry as an example, the measuring instruments may not be able to maintain
the correspondence between the samples and the measurements. Due to the combinatorial
nature of the problem, most existing methods are only applicable when the sample size is small,
and limited to linear regression models. To overcome such bottlenecks, we propose a new
computational framework — ROBOT — for the shuﬄed regression problem, which is applicable
to large data and complex nonlinear models. Speciﬁcally, we reformulate the regression without
correspondence as a continuous optimization problem. Then by exploiting the interaction between
the regression model and the data correspondence, we develop a hypergradient approach based
on diﬀerentiable programming techniques. Such a hypergradient approach essentially views the
data correspondence as an operator of the regression, and therefore allows us to ﬁnd a better
descent direction for the model parameter by diﬀerentiating through the data correspondence.
ROBOT can be further extended to the inexact correspondence setting, where there may not be
an exact alignment between the input and output data. Thorough numerical experiments show
that ROBOT achieves better performance than existing methods in both linear and nonlinear
regression tasks, including real-world applications such as ﬂow cytometry and multi-object tracking.

1 Introduction

Regression analysis has been widely used in various machine learning applications to infer the
the relationship between an explanatory random variable (i.e., the input) X ∈ Rd and a response
random variable (i.e., the output) Y ∈ Ro (Stanton, 2001). In the classical setting, regression is

(cid:42)

Equal contributions.
†Yujia Xie, Simiao Zuo, Tuo Zhao are aﬃliated with Georgia Institute of Technology. Emails: {xieyujia, simiaozuo,
tourzhao}@gatech.edu. Yixiu Mao is aﬃliated with Shanghai Jiao Tong University. Email: 956986044myx@gmail.com.
Hongteng Xu is aﬃliated with Duke University. Email: hongteng.xu@duke.edu. Xiaojing Ye is aﬃliated with Georgia
State University. Email: xye@gsu.edu. Hongyuan Zha is aﬃliated with School of Data Science at Shenzhen Research
Institute of Big Data and Shenzhen Institute of Artiﬁcial Intelligence and Robotics for Society. On leave from Georgia
Institute of Technology. Email: zhahy@cuhk.edu.cn.

1

 
 
 
 
 
 
used on labeled datasets that contain paired samples {xi, yi
respectively.

}n
i=1, where xi, yi are realizations of X, Y ,

Unfortunately, such an input-output correspondence is not always available in some applica-
tions. One example is ﬂow cytometry, which is a physical experiment for measuring properties
of cells, e.g., aﬃnity to a particular target (Abid and Zou, 2018). Through this process, cells are
suspended in a ﬂuid and injected into the ﬂow cytometer, where measurements are taken using the
scattering of a laser. However, the instruments are unable to diﬀerentiate the cells passing through
the laser, such that the correspondence between the cell proprieties (i.e., the measurements) and the
cells is unknown. This prevents us from analyzing the relationship between the instruments and
the measurements using classical regression analysis, due to the missing correspondence. Another
example is multi-object tracking, where we need to infer the motion of objects given consecutive
frames in a video. This requires us to ﬁnd the correspondence between the objects in the current
frame and those in the next frame.

The two examples above can be formulated as a shuﬄed regression problem. Speciﬁcally, we

consider a multivariate regression model

Y = f (X, Z; w) + ε,

where X ∈ Rd, Z ∈ Re are two input vectors, Y ∈ Ro is an output vector, f : Rd+e → Ro is the
unknown regression model with parameters w and ε is the random noise independent on X and Z.
When we sample realizations from such a regression model, the correspondence between (X, Y ) and
Z is not available. Accordingly, we collect two datasets D
}n
j=1, and there
∗ such that (xi, zπ(i)) corresponds to yi in the regression model. Our goal is to
exists a permutation π
recover the unknown model parameter w. Existing literature also refer to the shuﬄed regression
problem as unlabeled sensing, homomorphic sensing, and regression with an unknown permutation
(Unnikrishnan et al., 2018). Throughout the rest of the paper, we refer to it as Regression WithOut
Correspondence (RWOC).

i=1 and D
}n

1 = {xi, yi

2 = {zj

A natural choice of the objective for RWOC is to minimize the sum of squared residuals with
respect to the regression model parameter w up to the permutation π(·) over the training data, i.e.,

L(w, π) =

min
w,π

n(cid:88)

i=1

(cid:107)yi

− f

(cid:16)
xi, zπ(i); w

(cid:17) (cid:107)2
2.

(1)

Existing works on RWOC mostly focus on theoretical properties of the global optima to equation 1
for estimating w and π (Pananjady et al., 2016, 2017b; Abid et al., 2017; Elhami et al., 2017;
Hsu et al., 2017; Unnikrishnan et al., 2018; Tsakiris and Peng, 2019; Zhang and Li, 2020). The
development of practical algorithms, however, falls far behind from the following three aspects:
• Most of the works are only applicable to linear regression models.
• Some of the existing algorithms are of very high computational complexity, and can only handle
small number of data points in low dimensions (Elhami et al., 2017; Pananjady et al., 2017a;
Tsakiris et al., 2018; Peng and Tsakiris, 2020). Other algorithms choose to optimize with respect to
w and π in an alteranting manner, e.g., alternating minimization in Abid et al. (2017). However,
as there exists a strong interaction between w and π, the optimization landscape of equation 1 is
ill-conditioned. Therefore, these algorithms are not eﬀective and often get stuck in local optima.

2

1 and D

1 and D

• Most of the works only consider the case where there exists an exact one-to-one correspondence
between D
2. For many more scenarios, however, these two datasets are not necessarily well
aligned. For example, consider D
2 collected from two separate databases, where the users
overlap, but are not identical. As a result, there exists only partial one-to-one correspondence. A
similar situation also happens to multiple-object tracking: Some objects may leave the scene in
one frame, and new objects may enter the scene in subsequent frames. Therefore, not all objects
in diﬀerent frames can be perfectly matched. The RWOC problem with partial correspondence is
known as robust-RWOC, or rRWOC (Varol and Nejatbakhsh, 2019), and is much less studied in
existing literature.

To address these concerns, we propose a new computational framework – ROBOT (Regression
withOut correspondence using Bilevel OptimizaTion). Speciﬁcally, we propose to formulate the
regression without correspondence as a continuous optimization problem. Then by exploiting the
interaction between the regression model and the data correspondence, we propose to develop a
hypergradient approach based on diﬀerentiable programming techniques (Duchi et al., 2008; Luise
et al., 2018). Our hypergradient approach views the data correspondence as an operator of the
regression, i.e., for a given w, the optimal correspondence is

(cid:98)π(w) = arg min

π

L(w, π).

(2)

Accordingly, when applying gradient descent to (1), we need to ﬁnd the gradient with respect to w
by diﬀerentiating through both the objective function L and the data correspondence (cid:98)π(w). For
simplicity, we refer as such a gradient to “hypergradient”. Note that due to its discrete nature, (cid:98)π(w)
is actually not continuous in w. Therefore, such a hypergradient does not exist. To address this
issue, we further propose to construct a smooth approximation of (cid:98)π(w) by adding an additional
regularizer to equation 2, and then we replace (cid:98)π(w) with our proposed smooth replacement
when computing the hyper gradient of w. Moreover, we also propose an eﬃcient and scalable
implementation of hypergradient computation based on simple ﬁrst order algorithms and implicit
diﬀerentiation, which outperforms conventional automatic diﬀerentiation in terms of time and
memory cost.

ROBOT can also be extended to the robust RWOC problem, where D

2 are not necessarily
exactly aligned, i.e., some data points in D
2. Speciﬁcally,
we relax the constraints on the permutation π(·) (Liero et al., 2018) to automatically match related
data points and ignore the unrelated ones.

1 may not correspond to any data point in D

1 and D

At last, we conduct thorough numerical experiments to demonstrate the eﬀectiveness of ROBOT.
For RWOC (i.e., exact correspondence), we use several synthetic regression datasets and a real gated
ﬂow cytometry dataset, and we show that ROBOT outperforms baseline methods by signiﬁcant
margins. For robust RWOC (i.e., inexact correspondence), we consider a vision-based multiple-
object tracking task, and then we show that ROBOT also achieves signiﬁcant improvement over
baseline methods.
Notations. Let (cid:107) · (cid:107)
(cid:80)
an n-dimensional vector of all ones. Denote d(·)

2 denote the (cid:96)2 norm of vectors, (cid:104)·, ·(cid:105) the inner product of matrices, i.e., (cid:104)A, B(cid:105) =
i,j AijBij for matrices A and B. ai:j are the entries from index i to index j of vector a. Let 1n denote
(·)(·) the Jacobian of

d(·) the gradient of scalars, and ∇

3

tensors. We denote [v1, v2] the concatenation of two vectors v1 and v2. N (µ, σ 2) is the Gaussian
distribution with mean µ and variance σ 2.

2 ROBOT: A Hypergradient Approach for RWOC

We develop our hypergradient approach for RWOC. Speciﬁcally, we ﬁrst introduce a continuous
formulation equivalent to (1), and then propose a smooth bi-level relaxation with an eﬃcient
hypergradient descent algorithm.

2.1 Equivalent Continuous Formulation

We propose a continuous optimization problem equivalent to (1). Speciﬁcally, we rewrite an
equivalent form of (1) as follows,

min
w

min
S∈Rn×n

L(w, S) = (cid:104)C(w), S(cid:105)

subject to S ∈ P ,

(3)

where P denotes the set of all n × n permutation matrices, C(w) ∈ Rn×n is the loss matrix with

Cij(w) = (cid:107)yi

− f

(cid:16)
xi, zj; w

(cid:17) (cid:107)2
2.

Note that we can relax S ∈ P , which is the discrete feasible set of the inner minimization problem
of (3), to a convex set, without aﬀecting the optimality, as suggested by the next theorem.

Proposition 1. Given any a ∈ Rn and b ∈ Rm, we deﬁne

Π(a, b) = {A ∈ Rn×m : A1m = a, A

(cid:62)1n = b, Aij

≥ 0}.

The optimal solution to the inner discrete minimization problem of (3) is also the optimal solution
to the following continuous optimization problem,

min
S∈Rn×n

(cid:104)C(w), S(cid:105), s.t. S ∈ Π(1n, 1n).

(4)

This is a direct corollary of the Birkhoﬀ-von Neumann theorem (Birkhoﬀ, 1946; Von Neumann,
1953), and please refer to Appendix A for more details. Theorem 1 allows us to replace P in (3)
with Π(1n, 1n), which is also known as the Birkhoﬀ polytope1(Ziegler, 2012). Accordingly, we
obtain the following continuous formulation,

min
w

min
S∈Rn×n

(cid:104)C(w), S(cid:105)

subject to S ∈ Π(1n, 1n).

(5)

Remark 1. In general, equation 3 can be solved by linear programming algorithms (Dantzig, 1998).

1This is a common practice in integer programming (Marcus and Ree, 1959).

4

2.2 Conventional Wisdom: Alternating Minimization

Conventional wisdom for solving (5) suggests to use alternating minimization (AM, Abid et al.
(2017)). Speciﬁcally, at the k-th iteration, we ﬁrst update S by solving

S(k) = arg min
S∈Π(1n,1n)

L(w(k−1), S),

and then given S(k), we update w using gradient descent or exact minimization, i.e.,

w(k) = w(k−1) − η∇
w

L(w(k−1), S(k)).

However, AM works poorly for solving (5) in practice. This is because w and S have a strong
interaction throughout the iterations: A slight change to w may lead to signiﬁcant change to S.
Therefore, the optimization landscape is ill-conditioned, and AM can easily get stuck at local
optima.

2.3 Smooth Bi-level Relaxation

To tackle the aforementioned computational challenge, we propose a hypergradient approach,
which can better handle the interaction between w and S. Speciﬁcally, we ﬁrst relax (5) to a smooth
bi-level optimization problem, and then we solve the relaxed bi-level optimization problem using
the hypergradient descent algorithm.

We rewrite (5) as a smoothed bi-level optimization problem,

(cid:15)(w) = (cid:104)C(w), S
F

min
w

∗
(cid:15)(w)(cid:105), subject to S

∗
(cid:15)(w) = arg min
S∈Π(1n,1n)

(cid:104)C(w), S(cid:105) + (cid:15)H(S),

(6)

where H(S) = (cid:104)log S, S(cid:105) is the entropy of S. The regularizer H(S) in (6) alleviates the sensitivity of
∗(w) to w. Note that if without such a regularizer, we solve
S

∗

S

(w) = arg min
S∈Π(1n,1n)

(cid:104)C(w), S(cid:105).

(7)

∗(w) can be discontinuous in w. This is because S

∗(w) is the optimal solution of a
The resulting S
linear optimization problem, and usually lies on a vertex of Π(1n, 1n). This means that if we change
∗(w)
∗(w) either stays the same or jumps to another vertex of Π(1n, 1n). The jump makes S
w, S
∗(w) by adding an entropy
highly sensitive to w. To alleviate this issue, we propose to smooth S
∗
regularizer to the lower level problem. The entropy regularizer enforces S
(cid:15)(w) to stay in the interior
∗
of Π(1n, 1n), and S
(cid:15)(w) changes smoothly with respect to w, as suggested by the following theorem.

Theorem 1. For any (cid:15) > 0, S
w. Consequently, the objective F

∗
(cid:15)(w) is diﬀerentiable, if the cost C(w) is diﬀerentiable with respect to

(cid:15)(w) = (cid:104)C(w), S

∗
(cid:15)(w)(cid:105) is also diﬀerentiable.

The proof is deferred to Appendix C. Note that (6) provides us a new perspective to interpret
the relationship between w and S. As can be seen from (6), w and S have diﬀerent priorities: w
is the parameter of the leader problem, which is of the higher priority; S is the parameter of the
follower problem, which is of the lower priority, and can also be viewed as an operator of w –

5

denoted by S
we should also diﬀerentiate through S
follows,

∗
(cid:15)(w). Accordingly, when we minimize (6) with respect to w using gradient descent,
∗
(cid:15). We refer to such a gradient as “hypergradient” deﬁned as

∇

w

F

(cid:15)(w) =

∂F
(cid:15)(w)
∂C(w)

∂C(w)
∂w

+

∂F
∂S

(cid:15)(w)
∗
(cid:15)(w)

∂S

∗
(cid:15)(w)
∂w

= ∇
w

L(w, S) +

∂F
∂S

(cid:15)(w)
∗
(cid:15)(w)

∂S

∗
(cid:15)(w)
∂w

.

w

We further examine the alternating minimization algorithm from the bi-level optimization perspec-
L(w(k−1), S(k)) is not diﬀerentiable through S(k), AM is essentially using an inexact
tive: Since ∇
gradient. From a game-theoretic perspective2, (6) deﬁnes a competition between the leader w and
the follower S. When using AM, S only reacts to what w has responded. In contrast, when using
the hypergradient approach, the leader essentially recognizes the follower’s strategy and reacts to
∂S∗
what the follower is anticipated to response through ∂F
(cid:15)(w)
∂w . In this way, we can ﬁnd a better
∂S∗
descent direction for w.

(cid:15)(w)
(cid:15)(w)

Remark 2. We use a simple example of quadratic minimization to illustrative why we expect the
bilevel optimization formulation in (6) to enjoy a benign optimization landscape. We consider a
quadratic function

L(a1, a2) = a

(cid:62)

P a + b

(cid:62)

a,

(8)

∈ Rd1, a2

+ (1 −
where a1
ρ)Id1+d2, where Id1+d2 is the identity matrix, and ρ is a constant. We solve the following bilevel
optimization problem,

∈ Rd2, a = [a1, a2], P ∈ R(d1+d2)×(d1+d2), b ∈ Rd1+d2. Let P = ρ1d1+d2

1(cid:62)
d1+d2

mina1 F(a1) = L(a1, a

∗
2(a1))

∗
2(a1) = arg mina2
subject to a

L(a1, a2) + λ(cid:107)a2

(cid:107)2
2,

(9)

where λ is a regularization coeﬃcient. The next proposition shows that ∇2F(a1) enjoys a smaller
condition number than ∇2

a1a1L(a1, a2), which corresponds to the problem that AM solves.

Proposition 2. Given F deﬁned in (9), we have

λmax(∇2F(a1))
λmin(∇2F(a1))

= 1 +

1 − ρ + λ
d2ρ − ρ + λ + 1

· d1ρ
1 − ρ

and

λmax(∇2
λmin(∇2

a1a1L(a1, a2))
a1a1L(a1, a2))

= 1 +

d1ρ
1 − ρ

.

The proof is deferred to Appendix B. As suggested by Proposition 2, F(a1) is much better-

conditioned than L(a1, a2) in terms of a1 for high dimensional settings.

2.4 Solving rWOC by Hypergradient Descent

We present how to solve (6) using our hypergradient approach. Speciﬁcally, we compute the
“hypergradient” of F

(cid:15)(w) based on the following theorem.

Theorem 2. The gradient of F


(cid:15) with respect to w is

∇

w

F

(cid:15)(w) =

(1 − Cij)S

∗
(cid:15),ij +

1
(cid:15)

n,n(cid:88)

i,j=1



n,n(cid:88)

h,(cid:96)=1

Ch(cid:96)S

∗
(cid:15),h(cid:96)Phij +

n,n(cid:88)

h,(cid:96)=1

Ch(cid:96)S

∗
(cid:15),h(cid:96)Q(cid:96)ij





∇

wCij.

(10)

2The bilevel formulation can be viewed as a Stackelberg game.

6

where





P
Q






−1D
−H

0





:=

with P , Q ∈ Rn×n×n, −H

−1D ∈ R(2n−1)×n×n, 0 ∈ R1×n×n,

1
n(cid:15)

D(cid:96)ij =



δ(cid:96)iS

δ(cid:96)jS
and K = In−1

∗
(cid:15),ij,
∗
(cid:15),ij,
∗
− ¯S
(cid:15)

(cid:96) = 1, · · · , n;
(cid:96) = n + 1, · · · , 2n − 1,

−1 = −(cid:15)n

H


∗
∗
(cid:15)K−1 ¯S
In + ¯S

(cid:15)
∗
T
−K−1 ¯S
(cid:15)

∗
T − ¯S
(cid:15)K−1
K−1





,

∗
T ¯S
(cid:15),

∗
¯S
(cid:15) = S

∗
(cid:15),1:n,1:n−1.

The proof is deferred to Appendix C. Theorem 2 suggests that we ﬁrst solve the lower level

problem in (6),

and then substitute S

∗
(cid:15) = arg min
S
S∈Π(1n,1n)
F

∗
(cid:15) into (10) to obtain ∇
w

(cid:15)(w).

(cid:104)C(w), S(cid:105) + (cid:15)H(S),

(11)

Note that the optimization problem in (11) can be eﬃciently solved by a variant of Sinkhorn
algorithm (Cuturi, 2013; Benamou et al., 2015). Speciﬁcally, (11) can be formulated as an entropic
optimal transport (EOT) problem (Monge, 1781; Kantorovich, 1960), which aims to ﬁnd the optimal
way to transport the mass from a categorical distribution with weight µ = [µ1, . . . , µn](cid:62) to another
categorical distribution with weight ν = [ν1, . . . , νm](cid:62),

∗

Γ

= arg min
Γ ∈Π(µ,ν)
with Π(µ, ν) = {Γ ∈ Rn×m : Γ 1m = µ, Γ

(cid:104)M, Γ (cid:105) + (cid:15)H(Γ ),

(cid:62)1n = ν, Γij

≥ 0},

(12)

where M ∈ Rn×m is the cost matrix with Mij the transport cost. When we set the two categorical
distributions as the empirical distribution of D

2, respectively,

1 and D

M = C(w)

and µ = ν = 1n/n,

∗

one can verify that (12) is a scaled lower problem of (6), and their optimal solutions satisﬁes
(cid:15) = nΓ ∗. Therefore, we can apply Sinkhorn algorithm to solve the EOT problem in equation 12: At
S
the (cid:96)-th iteration, we take

p((cid:96)+1) =

µ
Gq((cid:96))

and q((cid:96)+1) =

ν
G(cid:62)p((cid:96)+1)

, where q(0) =

1
n

1n and Gij = exp

(cid:32) −Cij(w)
(cid:15)

(cid:33)

,

G ∈ Rn×n, and the division here is entrywise. Let p
obtain S

∗
(cid:15),ij = np

∗
i Gijq

∗
j.

∗ and q

∗ denote the stationary points. Then we

Remark 3. The Sinkhorn algorithm is iterative and cannot exactly solve (11) within ﬁnite steps.
As the Sinkhorn algorithm is very eﬃcient and attains linear convergence, it suﬃces to well
approximate the gradient ∇
(cid:15)(w) using the output inexact solution.
w

F

3 ROBOT for Robust Correspondence

We next propose a robust version of ROBOT to solve rRWOC (Varol and Nejatbakhsh, 2019). Note
that in (6), the constraint S ∈ Π(1n, 1n) enforces a one-to-one matching between D
2. For

1 and D

7

rRWOC, however, such an exact matching may not exist. For example, we have n < m, where
n = |D

|. Therefore, we need to relax the constraint on S.

|, m = |D

1

2

Motivated by the connection between (6) and (12), we propose to solve the following lower

problem3,

(S

∗
∗
r (w), ¯µ

, ¯ν

∗

) = arg min
S∈Π( ¯µ, ¯ν)

(cid:104)C(w), S(cid:105) + (cid:15)H(S),

(13)

subject to ¯µ

(cid:62)1n = n, ¯ν

(cid:62)1m = m, (cid:107) ¯µ − 1n

(cid:107)2
2

≤ ρ1, (cid:107) ¯ν − 1m

(cid:107)2
2

≤ ρ2,

∗

r (w) ∈ Rn×m denotes an inexact correspondence between D

where S
2. As can be seen in (13),
we relax the marginal constraint Π(1, 1) in (6) to Π( ¯µ, ¯ν), where ¯µ, ¯ν are required to not deviate
much from 1. Problem (13) relaxes the marginal constraints Π(1, 1) in the original problem to
Π( ¯µ, ¯ν), where ¯µ, ¯ν are picked such that they do not deviate too much from 1. Illustrative examples
of the exact and robust alignments are provided in Figure 1.

1 and D

Computationally, (13) can be solved by taking the Sinkhorn iteration and the projected gradient
∗
r (w), we solve the

iteration in an alternating manner (See more details in Appendix D). Given S
upper level optimization in (6) to obtain w

∗, i.e.,

∗

w

= arg min
w

(cid:104)C(w), S

∗
r (w)(cid:105).

Similar to the previous section, we use a ﬁrst-order algorithm to solve this problem, and we derive
explicit expressions for the update rules. See Appendix E for details.

(a) Original

(b) Robust

Figure 1: Illustrative example of exact (L) and robust (R) alignments. The robust alignment can drop
potential outliers and only match data points close to each other.

4 Experiment

We evaluate ROBOT and ROBOT-robust on both synthetic and real-world datasets, including
ﬂow cytometry and multi-object tracking. We ﬁrst present numerical results and then we provide
insights in the discussion section. Experiment details and auxiliary results can be found in
Appendix F.

3The idea is inspired by the marginal relaxation of optimal transport, ﬁrst independently proposed by Kondratyev
et al. (2016) and Chizat et al. (2018a), and later developed by Chizat et al. (2018c) and Liero et al. (2018). Chizat et al.
(2018b) share the same formulation as ours.

8

4.1 Unlabeled Sensing

Data Generation. We follow the unlabeled sensing setting (Tsakiris and Peng, 2019) and generate
n = 1000 data points {(yi, zi)}n
∈ Re. Note here we take d = 0. We ﬁrst generate
(cid:62)
zi, w ∼ N (0e, Ie), and εi
i w + εi. We randomly permute the
order of 50% of zi so that we lose the Z-to-Y correspondence. We generate the test set in the same
way, only without permutation.

noise). Then we compute yi = z

i=1, where zi

∼ N (0, ρ2

Baselines and Training. We consider the following scalable methods:

1. Oracle: Standard linear regression where no data are permuted.

2. Least Squares (LS): Standard linear regression, i.e., treating the data as if they are not permuted.

3. Alternating Minimization (AM, Abid et al. (2017)): We iteratively solve the correspondence

given w, and update w using gradient descent with the correspondence.

4. Stochastic EM (Abid and Zou, 2018): A stochastic EM approach to recover the permutation.

5. Robust Regression (RR, Slawski and Ben-David (2019); Slawski et al. (2019a)). A two-stage

block coordinate descent approach to discard outliers and ﬁt regression models.

6. Random Sample (RS, Varol and Nejatbakhsh (2019)): A random sample consensus (RANSAC)

approach to estimate w.

We initialize AM, EM and ROBOT using the output of RS with multi-start. We adopt a linear
model f (Z; w) = Z
w. Models are evaluated by the relative error on the test set, i.e., error =
(cid:80)
− yi)2/

− ¯y)2, where (cid:98)yi is the predicted label, and ¯y is the mean of {yi

i((cid:98)yi

i(yi

(cid:80)

}.

(cid:62)

(a) ρ2

noise = 0.1

(b) e = 10

Figure 2: Unlabeled sensing. Results are the mean over 10 runs. SNR= (cid:107)w(cid:107)2
ratio.

2/ρ2

noise is the signal-to-noise

Results. We visualize the results in Figure 2. In all the experiments, ROBOT achieves better
results than the baselines. Note that the relative error is larger for all methods except Oracle as
the dimension and the noise increase. For low dimensional data, e.g., e = 5, our model achieves
even better performance than Oracle. We include more discussions on using RS as initializations in
Section 5.

9

e=5e=10e=200.0000.0050.0100.0150.020Rel.ErrorSNR=100SNR=10000.000.010.020.030.04Rel.ErrorLSRRRSAMEMROBOTOracleFigure 3: Nonlinear regression. We use n = 1000, d = 2, e = 3, ρ2

noise = 0.1 as defaults.

4.2 Nonlinear Regression

i=1, where xi

∼ N (0d, Id), zi

Data Generation. We mimic the scenario where the dataset is collected from diﬀerent platforms.
Speciﬁcally, we generate n data points {(yi, [xi, zi])}n
∈ Re. We ﬁrst generate
∼ N (0e, Ie), w ∼ N (0d+e, Id+e), and εi
∼ N (0, ρ2
noise). Then we compute yi =
xi
f ([xi, zi]; w)+εi. Next, we randomly permute the order of {zi
} so that we lose the data correspondence.
} mimic two parts of data collected from two separate platforms.
2 = {zj
Here, D
Since we are interested in the response on platform one, we treat all data from platform two, i.e.,
D
1 are the test data.
Notice that we have diﬀerent number of data on D
2, i.e., the correspondence is not exactly
one-to-one.

1 as the training data. The remaining data from D
1 and D

2, as well as 80% of data in D

1 = {(xi, yi)} and D

∈ Rd and zi

Baselines and Training. We consider a nonlinear function f (X, Z; w) =
k=1 sin ([X, Z]kwk). In this
case, we consider only two baselines — Oracle and LS, since the other baselines in the previous
section are designed for linear models. We evaluate the regression models by the transport cost
divided by

− ¯y)2 on the test set.

(cid:80)

i(yi

(cid:80)d

Results. As shown in Figure 3, ROBOT-robust consistently outperforms ROBOT and LS, demon-
strating the eﬀectiveness of our robust formulation. Moreover, ROBOT-robust achieves better
performance than Oracle when the number of training data is large or when the noise level is high.

(a) FC

(b) GFC

Figure 4: Relative error of diﬀerent methods.

10

0246810d2.52.01.51.00.50.0log10(Rel.Error)0246810e2.52.01.51.00.50.010020050010002000n-2.5-2.0-1.5-1.0-0.50.000.10.20.512noise-6-5-4-3-2-10OracleLSROBOTROBOT-robustTable 1: Experiment results on MOT. Here, ↑ suggests the larger the better, and ↓ suggests the smaller the
better.

Data

MOT17 (train)

MOT17 (dev)

MOT20 (train)

MOT20 (dev)

FN↓

MOTA↑ MOTP↑ IDF1↑ MT↑ ML↓ FP↓
Method
48.3
55.3
ROBOT
49.9
w/o ROBOT 44.0
48.2
43.4
ROBOT
36.8
w/o ROBOT 42.1
39.8
43.1
SORT
56.2
47.6
ROBOT
40.2
w/o ROBOT 48.8
45.0
34.0
ROBOT
27.0
w/o ROBOT 38.5
45.1
42.7
SORT

IDS↓
407 553 22,443 149,988 1,811
404 550 36,187 149,131 3,204
455 904 29,419 259,714 3,228
414 890 61,210 259,318 6,138
295 997 28,398 287,582 4,852
805 288 113,752 377,247 5,888
769 290 186,245 384,562 10,153
394 257 70,416 210,425 3,683
383 233 104,958 207,627 5,696
208 326 27,521 264,694 4,470

82.6
81.3
76.6
75.0
77.8
84.9
81.5
76.9
75.1
78.5

4.3 Flow Cytometry

In ﬂow cytometry (FC), a sample containing particles is suspended in a ﬂuid and injected into the
ﬂow cytometer, but the measuring instruments are unable to preserve the correspondence between
the particles and the measurements. Diﬀerent from FC, gated ﬂow cytometry (GFC) uses “gates”
to sort the particles into one of many bins, which provides partial ordering information since the
measurements are provided individually for each bin. In practice, there are usually 3 or 4 bins.

Settings. We adopt the dataset from Knight et al. (2009). Following Abid et al. (2017), the outputs
yi’s are normalized, and we select the top 20 signiﬁcant features by a linear regression on the top
1400 items in the dataset. We use 90% of the data as the training data, and the remaining as test
data. For ordinary FC, we randomly shuﬄe all the labels in the training set. For GFC, the training
set is ﬁrst sorted by the labels, and then divided into equal-sized groups, mimicking the sorting by
gates process. The labels in each group are then randomly shuﬄed. To simulate gating error, 1% of
the data are shuﬄed across the groups. We compare ROBOT with Oracle, LS, Hard EM (a variant
of Stochastic EM proposed in Abid and Zou (2018)), Stochastic EM, and AM. We use relative error
on the test set as the evaluation metric.

Results. As shown in Figure 4, while AM achieves good performance on GFC when the number of
groups is 3, it behaves poorly on the FC task. ROBOT, on the other hand, is eﬃcient on both tasks.

4.4 Multi-Object Tracking

In this section we extend our method to vision-based Multi-Object Tracking (MOT), a task with
broad applications in mobile robotics and autonomous driving. Given a video and the current
frame, the goal of MOT is to predict the locations of the objects in the next frame. Speciﬁcally,
object detectors (Felzenszwalb et al., 2009; Ren et al., 2015) ﬁrst provide us the potential locations
of the objects by their bounding boxes. Then, MOT aims to assign the bounding boxes to trajectories

11

Figure 5: One frame in MOT20 with detected bounding boxes in yellow.

that describe the path of individual objects over time. Here, we formulate the current frame and the
objects’ locations in the current frame as D
}, while we treat the next frame and the locations
in the next frame as D

2 = {zj

1 = {(xi, yi)}.

Existing deep learning based MOT algorithms require large amounts of annotated data, i.e., the
ground truth of the correspondence, during training. Diﬀerent from them, our algorithm does not
require the correspondence between D
2, and all we need is the video. This task is referred
to as unsupervised MOT (He et al., 2019).

1 and D

Related Works. To the best of our knowledge, the only method that accomplishes unsupervised
end-to-end learning of MOT is He et al. (2019). However, it targets tracking with low densities,
e.g., Sprites-MOT, which is diﬀerent from our focus.

Settings. We adopt the MOT17 (Milan et al., 2016) and the MOT20 (Dendorfer et al., 2020) datasets.
Scene densities of the two datasets are 31.8 and 170.9, respectively, which means the scenes are
pretty crowded as we illustrated in Figure 5. We adopt the DPM detector (Felzenszwalb et al., 2009)
on MOT17 and the Faster-RCNN detector (Ren et al., 2015) on MOT20 to provide us the bounding
boxes. Inspired by Xu et al. (2019b), the cost matrix is computed as the average of the Euclidean
center-point distance and the Jaccard distance between the bounding boxes,

Cij(w) =

1
2

(cid:32) (cid:107)c(f (zj; w)) − c(yi)(cid:107)

√

H 2 + W 2

2

(cid:33)
+ J (f (zj; w), yi)

,

where c(·) is the location of the box center, H and W are the height and the width of the video
frame, and J (·, ·) is the Jaccard distance deﬁned as 1-IoU (Intersection-over-Union). We utilize
the single-object tracking model SiamRPN4 (Li et al., 2018) as our regression model f . We apply
ROBOT-robust with ρ1 = ρ2 = 10−3. See Appendix F for more detailed settings.
4The initial weights of f are obtained from https://github.com/foolwood/DaSiamRPN.

12

Results. We demonstrate the experiment results in Table 1, where the evaluation metrics follow
Ristani et al. (2016). In the table, ↑ represents the higher the better, and ↓ represents the lower
the better. ROBOT signiﬁes the model trained by ROBOT-robust, and w/o ROBOT means the
pretrained model in Li et al. (2018). The scores are improved signiﬁcantly after training with
ROBOT-robust.

We also include the scores of the SORT model (Bewley et al., 2016) obtained from the dataset
platform. Diﬀerent from SiamRPN and SiamRPN+ROBOT, SORT is a supervised learning model.
As shown, our unsupervised training framework achieves comparable or even better performance.

5 Discussion

• Sensitivity to initialization. As stated in Pananjady et al. (2017b), obtaining the global optima
of (1) is in general an NP-hard problem. Some “global” methods methods use global optimization
techniques and have exponential complexity, e.g., Elhami et al. (2017), which is not applicable to
large data. The other “local” methods only guarantee converge to local optima, and the conver-
gence is very sensitive to initialization. Compared with existing “local” methods, our method is
computationally eﬃcient and greatly reduces the sensitivity to initialization.

To demonstrate such an advantage, we run AM and ROBOT with 10 diﬀerent initial solutions,
and then we sort the results based on (a) the averaged residual on the training set, and (b) the
relative prediction error on the test set. We plot the percentiles in Figure 6. Here we use fully
shuﬄed data under the unlabeled sensing setting, and we set n = 1000, d = 5, ρ2
noise = 0.1, and
(cid:15) = 10−2. We can see that ROBOT is able to ﬁnd “good” solutions in 30% of the cases (The relative
prediction error is smaller than 1), but AM is more sensitive to the initialization and cannot ﬁnd
“good” solutions.

(a) Training residual

(b) Test error

Figure 6: ROBOT and AM with diﬀerent initial solutions.

• ROBOT v.s. Automatic Diﬀerentiation (AD). Our algorithm computes the Jacobian matrix
directly based on the KKT condition of the lower problem (11). An alternative approach to
approximate the Jacobian is the automatic diﬀerentiation through the Sinkhorn iterations for
updating S when solving (11). As suggested by Figure 7 (a), running Sinkhorn iterations until

13

255075Percentile012345Avg.Residual255075Percentile012Rel.ErrorAMROBOTconvergence (200 Sinkhorn iterations) can lead to a better solution5. In order to apply AD, we need
to store all the intermediate updates of all the Sinkhorn iterations. This require the memory usage
to be proportional to the number of iterations, which is not necessarily aﬀordable. In contrast,
applying our explicit expression for the backward pass is memory-eﬃcient. Moreover, we also
observe that AD is much more time-consuming than our method. The timing performance and
memory usage are shown in Figure 7 (b)(c), where we set n = 1000.

(a)

(b)

(c)

Figure 7: The comparisons to AD. (a) Convergence under diﬀerent number of Sinkhorn iterations of AD.
(b) Time comparison. (c) Memory comparison.

• Connection to EM. Abid and Zou (2018) adopt an Expectation Maximization (EM) method for
RWOC, where S is modeled as a latent random variable. Then in the M-step, one maximizes the
expected likelihood of the data over S. This method shares the same spirit as ours: We avoid
updating w using one single permutation matrix like AM. However, this method is very dependent
on a good initialization. Speciﬁcally, if we randomly initialize w, the posterior distribution of
S in this iteration would be close to its prior, which is a uniform distribution. In this way, the
follow-up update for w is not informative. Therefore, the solution of EM would quickly converge
to an undesired stationary point. Figure 8 illustrates an example of converged correspondence,
where we adopt n = 30, o = e = 1, d = 0. For this reason, we initialize EM with good initial points,
either by RS or AM throughout all experiments.

Table 2: Pairwise comparisons between RS alone and the combination of RS and ROBOT. The relative
error ratio is the ratio of the relative errors of RS alone and RS+ROBOT combination. Ratios larger than
1 suggest that RS performs worse than RS+ROBOT combination.

Proportion

25%

50%

75%

Rel. error ratio

1.04 ± 0.20

1.29 ± 0.32

1.27 ± 0.34

• Combination with RS. As suggested in Figure 2, although RS cannot perform well itself, re-
training the output of RS using our algorithms increases the performance by a large margin. To

5We remark that running one iteration sometimes cannot converge.

14

050100150200#iterations0.0040.0060.0080.0100.012Residual/Var(y)1Sinkhorniterations10Sinkhorniterations100Sinkhorniterations1005001000BatchSize2468Timeperepoch(s)ROBOTAD1005001000BatchSize050010001500Memory(M)ROBOTADFigure 8: Expected correspondence in EM.

show that combining RS and ROBOT can achieve better results than RS alone, we compare the
following two cases: i). Subsample 2 × 105 times using RS; ii). Subsample 105 times using RS
followed by ROBOT for 50 training steps. The result is shown in Table 2. For a larger permutation
proportion, RS alone cannot perform as well as RS+ROBOT combination. Here, we have 10 runs for
each proportion. We adopt SNR= 100, d = 5 for data, and (cid:15) = 10−4, learning rate 10−4 for ROBOT
training.

• Related works with additional constraints. There is another line of research which improves
the computational eﬃciency by solving variants of RWOC with additional constraints. Speciﬁcally,
Haghighatshoar and Caire (2017); Rigollet and Weed (2018) assume an isotonic function (note
that such an assumption may not hold in practice), and Shi et al. (2018); Slawski and Ben-David
(2019); Slawski et al. (2019a,b); Varol and Nejatbakhsh (2019) assume only a small fraction of
the correspondence is missing. Our method is also applicable to these problems, as long as the
additional constraints can be adapted to the implicit diﬀerentiation step.

• More applications of RWOC. RWOC problems generally appear for two reasons. First, the
measuring instruments are unable to preserve the correspondence.
In addition to GFC and
MOT, we list a few more examples: SLAM tracking (Thrun, 2007), archaeological measurements
(Robinson, 1951), large sensor networks (Keller et al., 2009), pose and correspondence estimation
(David et al., 2004), and the genome assembly problem from shotgun reads (Huang and Madan,
1999). Second, the data correspondence is masked for privacy reasons. For example, we want to
build a recommender system for a new platform, borrowing user data from a mature platform.

References

Abid, A., Poon, A. and Zou, J. (2017). Linear regression with shuﬄed labels. arXiv preprint

arXiv:1705.01342.

Abid, A. and Zou, J. (2018). A stochastic expectation-maximization approach to shuﬄed linear
regression. In 2018 56th Annual Allerton Conference on Communication, Control, and Computing
(Allerton). IEEE.

15

Benamou, J.-D., Carlier, G., Cuturi, M., Nenna, L. and Peyr´e, G. (2015). Iterative bregman
projections for regularized transportation problems. SIAM Journal on Scientiﬁc Computing, 37
A1111–A1138.

Bewley, A., Ge, Z., Ott, L., Ramos, F. and Upcroft, B. (2016). Simple online and realtime tracking.

In 2016 IEEE International Conference on Image Processing (ICIP). IEEE.

Birkhoff, G. (1946). Three observations on linear algebra. Univ. Nac. Tacuman, Rev. Ser. A, 5

147–151.

Bottou, L., Curtis, F. E. and Nocedal, J. (2018). Optimization methods for large-scale machine

learning. Siam Review, 60 223–311.

Chizat, L., Peyr´e, G., Schmitzer, B. and Vialard, F.-X. (2018a). An interpolating distance between
optimal transport and ﬁsher–rao metrics. Foundations of Computational Mathematics, 18 1–44.

Chizat, L., Peyr´e, G., Schmitzer, B. and Vialard, F.-X. (2018b). Scaling algorithms for unbalanced

optimal transport problems. Mathematics of Computation, 87 2563–2609.

Chizat, L., Peyr´e, G., Schmitzer, B. and Vialard, F.-X. (2018c). Unbalanced optimal transport:

Dynamic and kantorovich formulations. Journal of Functional Analysis, 274 3090–3123.

Cuturi, M. (2013). Sinkhorn distances: Lightspeed computation of optimal transport. In Advances

in neural information processing systems.

Dantzig, G. B. (1998). Linear programming and extensions, vol. 48. Princeton university press.

David, P., Dementhon, D., Duraiswami, R. and Samet, H. (2004). Softposit: Simultaneous pose and

correspondence determination. International Journal of Computer Vision, 59 259–284.

Dendorfer, P., Rezatofighi, H., Milan, A., Shi, J., Cremers, D., Reid, I., Roth, S., Schindler, K.
and Leal-Taix´e, L. (2020). Mot20: A benchmark for multi object tracking in crowded scenes.
arXiv:2003.09003[cs]. ArXiv: 2003.09003.
http://arxiv.org/abs/1906.04567

Duchi, J., Shalev-Shwartz, S., Singer, Y. and Chandra, T. (2008). Eﬃcient projections onto the
l 1-ball for learning in high dimensions. In Proceedings of the 25th international conference on
Machine learning.

Elhami, G., Benjamin, A. J., Haro, B. and Vetterli, M. (2017). Unlabeled sensing: Reconstruction
algorithm and theoretical guarantees. In 2017 42nd IEEE International Conference on Acoustics,
Speech and Signal Processing.

Felzenszwalb, P. F., Girshick, R. B., McAllester, D. and Ramanan, D. (2009). Object detection
with discriminatively trained part-based models. IEEE transactions on pattern analysis and machine
intelligence, 32 1627–1645.

16

Haghighatshoar, S. and Caire, G. (2017). Signal recovery from unlabeled samples. IEEE Transac-

tions on Signal Processing, 66 1242–1257.

He, Z., Li, J., Liu, D., He, H. and Barber, D. (2019). Tracking by animation: Unsupervised learning
of multi-object attentive trackers. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition.

Hsu, D. J., Shi, K. and Sun, X. (2017). Linear regression without correspondence. In Advances in

Neural Information Processing Systems.

Huang, X. and Madan, A. (1999). Cap3: A dna sequence assembly program. Genome research, 9

868–877.

Kantorovich, L. V. (1960). Mathematical methods of organizing and planning production. Man-

agement science, 6 366–422.

Keller, L., Siavoshani, M. J., Fragouli, C., Argyraki, K. and Diggavi, S. (2009). Identity aware

sensor networks. In IEEE INFOCOM 2009. IEEE.

Knight, C. G., Platt, M., Rowe, W., Wedge, D. C., Khan, F., Day, P. J., McShea, A., Knowles, J.
and Kell, D. B. (2009). Array-based evolution of dna aptamers allows modelling of an explicit
sequence-ﬁtness landscape. Nucleic acids research, 37 e6–e6.

Kondratyev, S., Monsaingeon, L., Vorotnikov, D. et al. (2016). A new optimal transport distance

on the space of ﬁnite radon measures. Advances in Diﬀerential Equations, 21 1117–1164.

Li, B., Yan, J., Wu, W., Zhu, Z. and Hu, X. (2018). High performance visual tracking with siamese
region proposal network. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition.

Liero, M., Mielke, A. and Savar´e, G. (2018). Optimal entropy-transport problems and a new
hellinger–kantorovich distance between positive measures. Inventiones mathematicae, 211 969–
1117.

Luise, G., Rudi, A., Pontil, M. and Ciliberto, C. (2018). Diﬀerential properties of sinkhorn ap-
proximation for learning with wasserstein distance. In Advances in Neural Information Processing
Systems.

Marcus, M. and Ree, R. (1959). Diagonals of doubly stochastic matrices. The Quarterly Journal of

Mathematics, 10 296–302.

Milan, A., Leal-Taix´e, L., Reid, I., Roth, S. and Schindler, K. (2016). MOT16: A benchmark for

multi-object tracking. arXiv:1603.00831 [cs]. ArXiv: 1603.00831.
http://arxiv.org/abs/1603.00831

Monge, G. (1781). M´emoire sur la th´eorie des d´eblais et des remblais. Histoire de l’Acad´emie Royale

des Sciences de Paris.

17

Pananjady, A., Wainwright, M. J. and Courtade, T. A. (2016). Linear regression with an unknown
permutation: Statistical and computational limits. In 2016 54th Annual Allerton Conference on
Communication, Control, and Computing.

Pananjady, A., Wainwright, M. J. and Courtade, T. A. (2017a). Denoising linear models with

permuted data. In 2017 IEEE International Symposium on Information Theory (ISIT). IEEE.

Pananjady, A., Wainwright, M. J. and Courtade, T. A. (2017b). Linear regression with shuﬄed data:
Statistical and computational limits of permutation recovery. IEEE Transactions on Information
Theory, 64 3286–3300.

Peng, L. and Tsakiris, M. C. (2020). Linear regression without correspondences via concave

minimization. arXiv preprint arXiv:2003.07706.

Ren, S., He, K., Girshick, R. and Sun, J. (2015). Faster r-cnn: Towards real-time object detection

with region proposal networks. In Advances in neural information processing systems.

Rigollet, P. and Weed, J. (2018). Uncoupled isotonic regression via minimum wasserstein decon-

volution. arXiv preprint arXiv:1806.10648.

Ristani, E., Solera, F., Zou, R., Cucchiara, R. and Tomasi, C. (2016). Performance measures and
a data set for multi-target, multi-camera tracking. In European Conference on Computer Vision.
Springer.

Robinson, W. S. (1951). A method for chronologically ordering archaeological deposits. American

antiquity, 16 293–301.

Shi, X., Lu, X. and Cai, T. (2018). Spherical regresion under mismatch corruption with application

to automated knowledge translation. arXiv preprint arXiv:1810.05679.

Sinkhorn, R. and Knopp, P. (1967). Concerning nonnegative matrices and doubly stochastic

matrices. Paciﬁc Journal of Mathematics, 21 343–348.

Slawski, M. and Ben-David, E. (2019). Linear regression with sparsely permuted data. Electronic

Journal of Statistics.

Slawski, M., Ben-David, E. and Li., P. (2019a). A two-stage approach to multivariate linear

regression with sparsely mismatched data. arXiv preprint arXiv:1907.07148.

Slawski, M., Rahmani, M. and Li, P. (2019b). A sparse representation-based approach to linear
regression with partially shuﬄed labels. In 35th Conference on Uncertainty in Artiﬁcial Intelligence,
UAI 2019.

Stanton, J. M. (2001). Galton, pearson, and the peas: A brief history of linear regression for

statistics instructors. Journal of Statistics Education, 9.

Thrun, S. (2007). Simultaneous localization and mapping. In Robotics and cognitive approaches to

spatial mapping. Springer, 13–41.

18

Tsakiris, M. and Peng, L. (2019). Homomorphic sensing. In Proceedings of the 36th International
Conference on Machine Learning (K. Chaudhuri and R. Salakhutdinov, eds.), vol. 97 of Proceedings
of Machine Learning Research. PMLR, Long Beach, California, USA.
http://proceedings.mlr.press/v97/tsakiris19a.html

Tsakiris, M. C., Peng, L., Conca, A., Kneip, L., Shi, Y., Choi, H. et al. (2018). An algebraic-

geometric approach to shuﬄed linear regression. arXiv preprint arXiv:1810.05440.

Unnikrishnan, J., Haghighatshoar, S. and Vetterli, M. (2018). Unlabeled sensing with random

linear measurements. IEEE Transactions on Information Theory, 64 3237–3253.

Varol, E. and Nejatbakhsh, A. (2019). Robust approximate linear regression without correspon-

dence. arXiv preprint arXiv:1906.00273.

Vayer, T., Flamary, R., Tavenard, R., Chapel, L. and Courty, N. (2019). Sliced gromov-wasserstein.

arXiv preprint arXiv:1905.10124.

Von Neumann, J. (1953). A certain zero-sum two-person game equivalent to the optimal assignment

problem. Contributions to the Theory of Games, 2 5–12.

Wang, M., Chen, Y., Liu, J. and Gu, Y. (2015). Random multi-constraint projection: Stochastic gra-
dient methods for convex optimization with many constraints. arXiv preprint arXiv:1511.03760.

Xie, Y., Dai, H., Chen, M., Dai, B., Zhao, T., Zha, H., Wei, W. and Pfister, T. (2020). Diﬀerentiable

top-k operator with optimal transport. arXiv preprint arXiv:2002.06504.

Xu, H., Luo, D., Zha, H. and Carin, L. (2019a). Gromov-wasserstein learning for graph matching

and node embedding. arXiv preprint arXiv:1901.06003.

Xu, Y., Ban, Y., Alameda-Pineda, X. and Horaud, R. (2019b). Deepmot: A diﬀerentiable framework

for training multiple object trackers. arXiv preprint arXiv:1906.06618.

Zhang, H. and Li, P. (2020). Optimal estimator for unlabeled linear regression. In International

Conference on Machine Learning. PMLR.

Ziegler, G. M. (2012). Lectures on polytopes, vol. 152. Springer Science & Business Media.

19

A Connection between OT and RWOC

Theorem 1. Denote Π(a, b) = {S ∈ Rn×m : S1m = a, S
Then at least one of the optimal solutions of the following problem lies in P .

(cid:62)1n = b, Sij

≥ 0} for any a ∈ Rn and b ∈ Rm.

minS∈Rn×n(cid:104)C(w), S(cid:105), s.t. S ∈ Π(1n, 1n).

(14)

∗. As we mentioned earlier, this is a direct corollary
Proof. Denote the optimal solution of (14) as Z
of Birkhoﬀ–von Neumann theorem (Birkhoﬀ, 1946; Von Neumann, 1953). Speciﬁcally, Birkhoﬀ–von
Neumann theorem claims that the polytope Π(1n, 1n) is the convex hull of the set of n × n per-
mutation matrices, and furthermore that the vertices of Π(1n, 1n) are precisely the permutation
matrices.

On the other hand, (14) is a linear optimization problem. There would be at least one optimal
solutions lies at the vertices given the problem is feasible. As a result, there would be at least one
Z

∗ being a permutation matrix.

B Proof of Proposition 2

∈ Rd1, a2

The bilevel optimization formulation has a better gradient descent iteration complexity than
(cid:62)
a, where
alternating minimization. To see this, consider a quadratic function F(a1, a2) = a
(cid:62)
2 ](cid:62) ∈ R(d1+d2), P ∈ R(d1+d2)×(d1+d2), b ∈ R(d1+d2). To further simplify the
∈ Rd2, a = [a
1 , a
a1
discussion, we assume P = ρ1(d1+d2)1(cid:62)
(d1+d2) + (1 − ρ)Id1+d2, where Id1+d2 is the identity matrix. Then
we have the following proposition.
Proposition 1. Given F deﬁned in (9), we have

P a + b

(cid:62)

(cid:62)

λmax(∇2F(a1))
λmin(∇2F(a1))

= 1 +

1 − ρ + λ
d2ρ − ρ + λ + 1

· d1ρ
1 − ρ

and

λmax(∇2
λmin(∇2

a1a1L(a1, a2))
a1a1L(a1, a2))

= 1 +

d1ρ
1 − ρ

.

Proof. For alternating minimization, the Hessian for a1 is a submatrix of P , i.e.,

whose condition number is

HAM = ρ1d1

1(cid:62)
d1

+ (1 − ρ)Id1,

CAM = 1 +

d1ρ
1 − ρ

.

We now compute the condition number for ROBOT. Denote

P =


P11 P12

P21 P22





,

b =





b1
b2





,

∈ Rd1

where P11
minimize over a2,

×d1, P12

∈ Rd1

×d2, P21

∈ Rd2

×d1, P22

∈ Rd2

×d2, and b1

∈ Rd1, b2

∈ Rd2. ROBOT ﬁrst

a

∗
2(a1) = arg min

a2

F(a1, a2) = −(P22 + λId2)

−1(P21a1 + b2/2).

20

Substituting a

∗
2(a1) into F(a1, a2), we can obtain the Hessian for a1 is
−1P21.

− P12(P22 + λId2)

HROBOT = P11

Using Sherman–Morrison formula, we can explicitly express P

−1
22 as

−1
22 =
P

1
1 − ρ + λ

−

Id2

ρ
(1 − ρ + λ)(1 − ρ + λ + ρd2)

1d2

1(cid:62)
d2

.

Substituting it into HROBOT,

HROBOT = P11

− P12P

−1
22 P21 = (1 − ρ)Id1 +

(cid:32)
ρ −

d2ρ2
d2ρ − ρ + λ + 1

(cid:33)

1d1

1(cid:62)
d1

.

Therefore, the condition number is

CROBOT = 1 +

1 − ρ + λ
1 − ρ

d1ρ
d2ρ − ρ + λ + 1

.

Note that CAM increases linearly with respect to d1. Therefore, the optimization problem
inevitably becomes ill-conditioned as dimension increase. In contrast, CROBOT can stay in the same
order of magnitude when d1 and d2 increase simultaneously.

Since the iteration complexity of gradient descent is proportional to the condition number

(Bottou et al., 2018), ROBOT needs fewer iterations to converge than AM.

C Diﬀerentiability

Theorem 2. For any (cid:15) > 0, S
respect to w. As a result, the objective L

∗
(cid:15)(w) is diﬀerentiable, as long as the cost C(w) is diﬀerentiable with

(cid:15)(w) = (cid:104)C(w), S

∗
(cid:15)(w)(cid:105) is also diﬀerentiable.

Proof. The proof is analogous to Xie et al. (2020).

We ﬁrst prove the diﬀerentiability of S

∗
(cid:15)(w). This part of proof mirrors the proof in Luise et al.

(2018). By Sinkhorn’s scaling theorem (Sinkhorn and Knopp, 1967),

∗
(cid:15)(w) = diag(e
S

ξ

∗

(w)
(cid:15)

)e

− C(w)

(cid:15) diag(e

ζ

∗

(w)
(cid:15)

).

Therefore, since Cij(w) is diﬀerentiable, Γ ∗,(cid:15) is diﬀerentiable if (ξ
function of w.
Let us set

∗(w), ζ

∗(w)) is diﬀerentiable as a

L(ξ, ζ; µ, ν, C) = ξT µ + ζT ν − (cid:15)

− Cij

−ζj

−ξi
(cid:15)

e

.

n,m(cid:88)

i,j=1

∗

and recall that (ξ
the Implicit Function theorem and follows from the diﬀerentiability and strict convexity in (ξ
of the function L.

∗) = arg maxξ,ζ L(ξ, ζ; µ, ν, C). The diﬀerentiability of (ξ

∗) is proved using
∗)
, ζ

, ζ

, ζ

∗

∗

21

Theorem 3. The gradient of F

(cid:15) with respect to w is

∇

F

(cid:15) =

w





1
(cid:15)

n,n(cid:88)

i,j=1

(1 − Cij)S

∗
(cid:15),ij +

n,n(cid:88)

h,(cid:96)=1

Ch(cid:96)S

∗
(cid:15),h(cid:96)

∗
dξ
h
dCij

+

n,n(cid:88)

h,(cid:96)=1

Ch(cid:96)S

∗
(cid:15),h(cid:96)

∗
dζ
(cid:96)
dCij





∇

wCij,

(15)

where





∇
∇

Cξ
Cζ

∗

∗






−1D
−H

0





=

with − H

−1D ∈ R(2n−1)×n×n, 0 ∈ R1×n×n,

1
n(cid:15)

D(cid:96)ij =



δ(cid:96)iS

δ(cid:96)jS
and K = In−1

∗
(cid:15),ij,
∗
(cid:15),ij,
∗
− ¯S
(cid:15)

(cid:96) = 1, · · · , n;
(cid:96) = n + 1, · · · , 2n − 1,

−1 = −(cid:15)n

H


∗
∗
(cid:15)K−1 ¯S
In + ¯S

(cid:15)
∗
T
−K−1 ¯S
(cid:15)

∗
T − ¯S
(cid:15)K−1
K−1





,

∗
T ¯S
(cid:15),

∗
¯S
(cid:15) = S

∗
(cid:15),1:n,1:n−1.

Proof. This result is straightforward combining the Sinkhorn’s scaling theorem and Theorem 3 in
Xie et al. (2020). Speciﬁcally, notice the similarity between the lower-level optimization and (12),

Γ = arg min
Γ ∈Π(µ,ν)

(cid:104)C(w), Γ (cid:105) + (cid:15)

(cid:88)

i,j

Γij ln Γij.

We will ﬁrst derive ∇
(cid:15) using the chain rule. To avoid possible confusion, we
will derive the case where µ ∈ Rn and ν ∈ Rm, and then take µ = ν = 1n/n. The dual problem of the
above optimization problem is

CΓ , then derive ∇

F

w

where

∗

∗

ξ

, ζ

= arg max
ξ,ζ

L(ξ, ζ; C),

L(ξ, ζ; C) = ξ

(cid:62)

µ + ζ

(cid:62)

ν − (cid:15)

− Cij

−ζj

−ξi
(cid:15)

e

.

n,m(cid:88)

i,j=1

And it is connected to the prime form by

∗,(cid:15) = diag(e

Γ

∗
ξ
(cid:15) )e

− C

(cid:15) diag(e

∗
ζ
(cid:15) ).

Notice that there is one redundant dual variable, since µ1n = ν1m = 1. Therefore, we can rewrite

L(ξ, ζ; C) as

Denote

L(ξ, ¯ζ; C) = ξT µ + ¯ζT ¯ν − (cid:15)

n,m−1(cid:88)

i,j=1

−Cij +ξi +ζj
(cid:15)

e

− (cid:15)

−Cim+ξi
(cid:15)

e

.

n(cid:88)

i=1

φ(ξ, ¯ζ, C) =

ψ(ξ, ¯ζ, C) =

dL(ξ, ¯ζ; C)
dξ
dL(ξ, ¯ζ; C)
d ¯ζ

= µ − F1m,

= ¯ν − ¯F

(cid:62)1n,

22

(16)

(17)

where

−Cij +ξi +ζj
(cid:15)

Fij = e

, ∀i = 1, · · · , n,

j = 1, · · · , m − 1

−Cim+ξi
(cid:15)

Fim = e
¯F = F:,:−1.

, ∀i = 1, · · · , n,

Since (ξ

∗

, ¯ζ

∗) is a maximum of L(ξ, ¯ζ; C), we have

φ(ξ

ψ(ξ

∗

∗

∗

∗

, ¯ζ
, ¯ζ

, C) = 0,

, C) = 0.

Therefore,

Therefore,

dφ(ξ

dψ(ξ

∗

∗

, C)

, C)

=

=

∗

, ¯ζ
dC
∗
, ¯ζ
dC

∂φ(ξ

∂ψ(ξ

∗

∗

, C)

, C)

+

+

∗

, ¯ζ
∂C
∗
, ¯ζ
∂C

∂φ(ξ

∗

, C)

∂φ(ξ

∗

, C)

∗

, ¯ζ
∂ξ∗
∗
, ¯ζ
∂ξ∗

∗

dξ
dC
∗
dξ
dC

+

+

∗

, ¯ζ
∂ ¯ζ∗
∗
, ¯ζ
∂ ¯ζ∗

∗

d ¯ζ
dC
∗
d ¯ζ
dC

= 0,

= 0.

∂ψ(ξ

∗

, C)

∂ψ(ξ

∗

, C)

∂φ(ξ∗, ¯ζ∗,C)
∂ ¯ζ∗
∂ψ(ξ∗, ¯ζ∗,C)
∂ ¯ζ∗

−1 





∂φ(ξ∗, ¯ζ∗,C)
∂C
∂ψ(ξ∗, ¯ζ∗,C)
∂C









dξ∗
dC
d ¯ζ∗
dC









= −

(cid:44) −H

∂φ(ξ∗, ¯ζ∗,C)
∂ξ∗
∂ψ(ξ∗, ¯ζ∗,C)
∂ξ∗

D(1)
D(2)


−1D.

−1





(cid:44) −H

After some derivations, we have

H = − 1
(cid:15)





diag(µ)
¯Γ T

¯Γ
diag( ¯ν)





.

Following the formula for inverse of block matrices,





A B
C D

−1





=


A−1 + A−1B(D − CA−1B)−1CA−1 −A−1B(D − CA−1B)−1


−(D − CA−1B)−1CA−1

(D − CA−1B)−1





,

denote

Finally we have

−1 = −(cid:15)

H

K = diag( ¯ν) − ¯Γ T (diag(µ))

−1 ¯Γ .


(diag(µ))−1 + (diag(µ))−1 ¯Γ K−1 ¯Γ T (diag(µ))−1 −(diag(µ))−1 ¯Γ K−1


−K−1 ¯Γ T (diag(µ))−1

K−1





.

23

And also

D

(1)
hij =

D

(2)
(cid:96)ij =

1
(cid:15)
1
(cid:15)

δhiΓij

δ(cid:96)jΓij.

The above derivation can actually be viewed as we explicitly force ζm = 0, i.e., no matter how C
changes, ζm does not change. Therefore, we can treat dζm
dC = 0n×m.
dC , we can now compute dΓ
dC .

After we obtain dξ∗

dC and dζ∗

dΓh(cid:96)
dCij

=

d
dCij

e

∗
h

∗
+ζ
(cid:96)

−Ch(cid:96) +ξ
(cid:15)

(cid:32)

=

1
(cid:15)

−Γh(cid:96)δihδj(cid:96) + Γh(cid:96)

∗
dξ
h
dCij

+ Γh(cid:96)

(cid:33)

.

∗
dζ
(cid:96)
dCij

Note that F
in the theorem.

(cid:15)(w) = (cid:104)C(w), nΓ (cid:105). Substituting dΓh(cid:96)
dCij

into the expression of ∇
w

F

(cid:15)(w), we get the equation

D Algorithm of the Forward Pass for ROBOT-robust

For better numerical stability, in practice we add two more regularization terms,

∗
r (w), ¯µ
S

∗

, ¯ν

∗

= arg minS∈Π( ¯µ, ¯ν), ¯µ, ¯ν∈∆n
s.t. F ( ¯µ, µ) ≤ ρ1, F ( ¯ν, ν) ≤ ρ2,

(cid:104)C(w), S(cid:105) + (cid:15)H(S) + (cid:15)1h( ¯µ) + (cid:15)2h( ¯ν),

(18)

(cid:80)

i ¯µi log ¯µi is the entropy function for vectors. This can avoid the entries of ¯µ and ¯ν
where h( ¯µ) =
shrink to zeros when updated by gradient descent. We remark that since we have entropy term H(S),
the entries of S would not be exactly zeros. Furthermore, we have ¯µ = S1 and ¯µ = S1. Therefore,
theoretically the entries of ¯µ and ¯ν will not be zeros. We only add the two more entropy terms for
numerical consideration. The detailed algorithm is in Algorithm 1. Although the algorithm is not
guaranteed to converge to a feasible solution, in practice it usually converges to a good solution
(Wang et al., 2015).

E Algorithm of the Backward Pass for ROBOT-robust

We ﬁrst summarize the outline of the derivation, then provide the detailed derivation.

E.1 Summary

∗

∗
∗
r (w)/dw using implicit diﬀerentiation and
Given ¯µ
diﬀerentiable programming techinques. Speciﬁcally, the Lagrangian function of Problem (18) is

∗
r (w), we compute the Jacobian matrix dS

, ¯ν

, S

L =(cid:104)C, S(cid:105) + (cid:15)H(S) + (cid:15)1h( ¯µ) + (cid:15)2h( ¯ν) − ξ
− 1) + λ2( ¯ν

+ λ1( ¯µ

(cid:62)1m

(cid:62)1n

− 1) + λ3((cid:107) ¯µ − µ(cid:107)2
2

(cid:62)

(Γ 1m

(cid:62)

− µ) − ζ

(Γ

(cid:62)1n
− ν)
− ρ1) + λ4((cid:107) ¯ν − ν(cid:107)2
2

− ρ2).

24

Algorithm 1 Solving S
Require: C ∈ Rm×n, µ, ν, K, (cid:15), L, η

∗
r for robust matching

− Cij
(cid:15)

Gij = e
¯µ = µ, ¯ν = ν
b = 1n
for l = 1, · · · , L do

∗ log ¯µ), ¯ν = ¯ν − η(e

b

(cid:15) + (cid:15)2

∗ log ¯ν)

a

a = ¯µ/(Gb), b = ¯ν/(GT a)
¯µ = ¯µ − η(e
(cid:15) + (cid:15)1
¯µ = max{ ¯µ, 0}, ¯ν = max{ ¯ν, 0}
(cid:62)1), ¯ν = ¯ν/( ¯ν
¯µ = ¯µ/( ¯µ
if (cid:107) ¯µ − µ(cid:107)2
2 > ρ1 then
√
¯µ = µ +

(cid:62)1)

¯µ−µ
(cid:107) ¯µ−µ(cid:107)

ρ1

2

end if
if (cid:107) ¯ν − ν(cid:107)2
2 > ρ2 then
√
¯ν = ν +

¯ν−ν
(cid:107) ¯ν−ν(cid:107)
2

ρ2

end if
end for
S = diag(a) (cid:12) G (cid:12) diag(b)

where ξ and ζ are dual variables. The KKT conditions (Stationarity condition) imply that the
optimal solution Γ ∗,(cid:15) can be formulated using the optimal dual variables ξ

∗ and ζ

∗ as,

∗
r = diag(e
S

∗
ξ
(cid:15) )e

− C

(cid:15) diag(e

∗
ζ
(cid:15) ).

(19)

By the chain rule, we have

∗
dS
r
dw

Therefore, we can compute dS

(cid:32)

=

∗

=

dC
dw

∗
∂S
r
∂C

∗
∗
dS
∂S
r
r
∂ξ∗
dC
r (w)/dw if we obtain dξ∗

∗
∂S
dξ
r
∂ζ∗
dC
dC and dζ∗
dC .

+

+

∗

(cid:33)

∗

dζ
dC

dC
dw

.

Substituting (19) into the Lagrangian function, at the optimal solutions we obtain

L = L(ξ

∗

∗

∗

, ¯µ

, ¯ν

∗

, ζ

∗
4; C).

, λ

∗
∗
∗
1, λ
2, λ
3, λ
4](cid:62), and φ(r

∗

Denote r
dual variable r

∗ = [(ξ

∗)(cid:62)

, (ζ

∗)(cid:62)

, ( ¯µ)(cid:62)

, ( ¯ν)(cid:62)

, λ

∗
1, λ

∗
2, λ

∗
3, λ

∗, the KKT condition immediately yields φ(r
∗; C)

∗; C)

∂φ(r

dφ(r

∗; C)

=

∂C

+

∂φ(r
∂r∗

∗

dr
dC

= 0.

Rerranging terms, we obtain

dC

∗

dr
dC

= −

(cid:32)

∗; C)

∂φ(r
∂r∗

(cid:33)−1 ∂φ(r
∂C

∗; C)

.

Combining (19), (20), and (21), we can then obtain dS

∗
r (w)/dw.

25

∗; C) = ∂L(r

∗; C)/∂r
∗; C) ≡ 0. By the chain rule, we have

∗. At the optimal

(20)

(21)

E.2 Details

Now we provide the detailed derivation for computing dS

∗
r /dw.

Since S

∗
r is the optimal solution of an optimization problem, we can follow the implicit function
theorem to solve for the closed-form expression of the gradient. Speciﬁcally, we adopt F ( ¯µ, ν) =
(cid:80)

i( ¯µi

− µi)2, and rewrite the optimization problem as
(cid:88)
(cid:88)

(cid:104)C, S(cid:105) + (cid:15)

Sij(log Sij

− 1) + (cid:15)1

s.t.,

Sij = ¯µi,

ij
(cid:88)

Sij = ¯νj,

i

¯µi(log ¯µi

− 1) + (cid:15)2

(cid:88)

j

¯νj(log ¯νj

− 1),

min
¯µ, ¯ν,S
(cid:88)

j
(cid:88)

i
(cid:88)

i

¯µi = 1,

i
(cid:88)

j

( ¯µi

− µi)2 ≤ ρ1,

¯νj = 1,

(cid:88)

j

( ¯νj

− νj)2 ≤ ρ2.

The Language of the above problem is

L(C, S, ¯µ, ¯ν, ξ, ζ, λ1, λ2, λ3, λ4)
(cid:88)
= (cid:104)C, S(cid:105) + (cid:15)

Sij(log Sij

− 1) + (cid:15)1

(cid:62)

− ξ

+ λ1(

(S1m
(cid:88)

(cid:62)

ij
− ¯µ) − ζ
(S
− 1) + λ2(

¯µi

(cid:62)1n
(cid:88)
¯νj

− ¯ν)
− 1) + λ3(

(cid:88)

i

¯µi(log ¯µi

− 1) + (cid:15)2

(cid:88)

j

¯νj(log ¯νj

− 1)

(cid:88)

− µi)2 − ρ1) + λ4(

(cid:88)

( ¯νj

− νj)2 − ρ2).

( ¯µi

i

j

i

j

Easy to see that the Slater’s condition holds. Denote

L∗

= L(C, S

∗
∗
r , ¯µ

, ¯ν

∗

, ξ

∗

, ζ

∗

, λ

∗
1, λ

∗
2, λ

∗
3, λ

∗
4).

Following the KKT conditions,

dL∗
∗
dS
r,ij

= Cij + (cid:15) log S

∗
r,ij

∗
− ξ
i

− ζ

∗
j = 0.

Therefore, S

∗
r,ij = e

−Cij

ξ

∗
i

∗
+ζ
j
(cid:15)

. Then we have

∗
dS
r
dw

= (

∗
∂S
r
∂C

∗
∂S
r
∂ξ∗

∗

dξ
dC

∗
∂S
r
∂ζ∗

∗

dζ
dC

)

dC
dw

.

+

+

26

So all we need to do is to compute dξ∗

dC and dζ∗

dC . Denote Fij = e

−Cij
ξi +ζj
(cid:15)

. Denote

φ =

ψ =

p =

q =

dL
dξ
dL
dζ
dL
d ¯µ
dL
d ¯ν

= ¯µ − F1m,

= ¯ν − F

(cid:62)1n,

= ξ + λ11n + 2λ3( ¯µ − µ) + (cid:15)1 log ¯µ,

= ζ + λ21m + 2λ4( ¯ν − ν) + (cid:15)2 log ¯ν,

(cid:62)1n

= ¯µ

χ1 =

χ2 =

dL
dλ1
dL
dλ2
χ3 = λ3((cid:107) ¯µ − µ(cid:107)2
2
χ4 = λ4((cid:107) ¯ν − ν(cid:107)2
2

= ¯ν

(cid:62)1m

− 1,

− 1,

− ρ1),
− ρ2).

Denote χ = [χ1, χ2, χ3, χ4], and λ = [λ1, λ2, λ3, λ4]. Following the KKT conditions, we have

at the optimal solutions. Therefore, for the optimal solutions we have

φ = 0, ψ = 0, p = 0, q = 0, χ = 0,

dφ
dC
dψ
dC
dp
dC
dq
dC
dχ
dC

=

=

=

=

=

∂φ
∂C
∂ψ
∂C
∂p
∂C
∂q
∂C
∂χ
∂C

∂φ
∂ξ∗
∂ψ
∂ξ∗
∂p
∂ξ∗
∂q
∂ξ∗
∂χ
∂ξ∗

∗

dξ
dC
∗
dξ
dC
∗
dξ
dC
∗
dξ
dC
∗
dξ
dC

∂φ
∂ζ∗
∂ψ
∂ζ∗
∂p
∂ζ∗
∂q
∂ζ∗
∂χ
∂ζ∗

∗

dζ
dC
∗
dζ
dC
∗
dζ
dC
∗
dζ
dC
∗
dζ
dC

+

+

+

+

+

+

+

+

+

+

∂φ
∂ ¯µ∗
∂ψ
∂ ¯µ∗
∂p
∂ ¯µ∗
∂q
∂ ¯µ∗
∂χ
∂ ¯µ∗

∗

d ¯µ
dC
∗
d ¯µ
dC
∗
d ¯µ
dC
∗
d ¯µ
dC
∗
d ¯µ
dC

∂φ
∂ ¯ν∗
∂ψ
∂ ¯ν∗
∂p
∂ ¯ν∗
∂q
∂ ¯ν∗
∂χ
∂ ¯ν∗

∗

d ¯ν
dC
∗
d ¯ν
dC
∗
d ¯ν
dC
∗
d ¯ν
dC
∗
d ¯ν
dC

∂φ
∂λ∗
∂ψ
∂λ∗
∂p
∂λ∗
∂q
∂λ∗
∂χ
∂λ∗

∗

dλ
dC
∗
dλ
dC
∗
dλ
dC
∗
dλ
dC
∗
dλ
dC

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

= 0,

= 0,

= 0,

= 0

= 0.

Therefore, we have





∗

dξ
dC
∗
dζ
dC
∗
d ¯µ
dC
∗
d ¯ν
dC
∗
dλ
dC





= −





∂φ
∂ξ∗
∂ψ
∂ξ∗
∂p
∂ξ∗
∂q
∂ξ∗
∂χ
∂ξ∗

∂φ
∂ζ∗
∂ψ
∂ζ∗
∂p
∂ζ∗
∂q
∂ζ∗
∂χ
∂ζ∗

∂φ
∂ ¯ν∗
∂ψ
∂ ¯ν∗
∂p
∂ ¯ν∗
∂q
∂ ¯ν∗
∂χ
∂ ¯ν∗

−1




∂φ
∂λ∗
∂ψ
∂λ∗
∂p
∂λ∗
∂q
∂λ∗
∂χ
∂λ∗









∂φ
∂C
∂ψ
∂C
∂p
∂C
∂q
∂C
∂χ
∂C

.

∂φ
∂ ¯µ∗
∂ψ
∂ ¯µ∗
∂p
∂ ¯µ∗
∂q
∂ ¯µ∗
∂χ
∂ ¯µ∗

27

After some derivation, we have





∗

dξ
dC
∗
dζ
dC
∗
d ¯µ
dC
∗
d ¯ν
dC
∗
dλ
1
dC
∗
dλ
2
dC
∗
dλ
3
dC
∗
dλ
4
dC









= −

and

− 1
(cid:15)
− 1
(cid:15)

diag( ¯µ)
∗
(cid:62)
r )

(S

In
0
0
0
0

0

S

∗
r

− 1
(cid:15)
diag( ¯ν)

− 1
(cid:15)

0

Im
0
0
0

0

In

0

2λ3In + diag(

(cid:15)1
¯µ )

0
(cid:62)
1
n
0
2λ3( ¯µ − µ)
0

(cid:62)

0

Im
0

2λ4Im + diag(

(cid:15)2
¯ν )

0
(cid:62)
1
m
0
2λ4( ¯ν − ν)

(cid:62)

0

0

1n
0
0
0
0

0

0

0

0

1m
0
0
0

0

0

0
2( ¯µ − µ)

0
0
0
(cid:107) ¯µ − µ(cid:107)2
2
0

− ρ1

−1









∂φ
∂C
∂ψ
∂C
0
0
0
0
0
0





,

0

0

0
2( ¯ν − ν)
0
0
0
(cid:107) ¯ν − ν(cid:107)2
2

− ρ2

∂φh
∂Cij
∂ψ(cid:96)
∂Cij

=

=

1
(cid:15)

1
(cid:15)

δhiSij, ∀h = 1, · · · , n,

i = 1, · · · , n,

j = 1, · · · , m

δ(cid:96)jSij, ∀(cid:96) = 1, · · · , m − 1,

i = 1, · · · , n,

j = 1, · · · , m.

To eﬃciently solve for the inverse in the above equations, we denote





A =

− 1
(cid:15)
− 1
(cid:15)

diag( ¯µ)
r )(cid:62)

(S

∗

In
0

∗
S
r

− 1
(cid:15)
diag( ¯ν)

− 1
(cid:15)

In

0

0
2λ3In + diag( (cid:15)1
¯µ )
0

Im
0
2λ4Im + diag( (cid:15)2
¯ν )





,

0
Im


1n

0

B1 =

0
1m

2( ¯µ − µ)
0

0
2( ¯ν − ν)





,

C1 =





1(cid:62)
n
0
2λ3( ¯µ − µ)(cid:62)
0

0
1(cid:62)
m
0
2λ4( ¯ν − ν)(cid:62)





,

D =


0
0 0
0 0
0
0 0 (cid:107) ¯µ − µ(cid:107)2
2
0
0 0



− ρ1

0
0
0
(cid:107) ¯ν − ν(cid:107)2
2

− ρ2





.

We ﬁrst A

−1 using the rules for inverting a block matrix,

−1 =
A







−KL

K

−LK L + LKL


A1 A2

A3 A4





=:

28

where


2λ3In + diag( (cid:15)1
¯µ )

0

L =

0
2λ4Im + diag( (cid:15)1
¯ν )

−1




, K =





1
(cid:15)


diag( ¯µ)
r )(cid:62)
∗
(S



Then using the rules of inverting a block matrix again, we have





∗

dξ
dC
∗
dζ
dC





= (A1 + A2B1(D − C1A4B1)

−1C1A3)





.





∂φ
∂C
∂ψ
∂C

∗
S
r
diag( ¯ν)






−1



.

+ L

Therefore, the bottleneck of computation is the inverting step in computing K. Note L is a diagonal
matrix, we can further lower the computation cost by applying the rules for inverting a block
matrix again. The value of λ3 and λ4 can be estimated from the fact p = 0, q = 0 . We detail the
algorithm in Algorithm 2.

F Details on Experiments

F.1 Unlabeled Sensing

We now provide more training details for experiments in Section 4.1. Here, AM and ROBOT is
trained with batch size 500 and learning rate 10−4 for 2, 000 iterations. For the Sinkhorn algorithm
in ROBOT we set (cid:15) = 10−4. We run RS for 2 × 105 iterations with inlier threshold as 10−2. Other
settings for the hyper-parameters in the baselines follows the default settings of their corresponding
papers.

F.2 Nonlinear Regression

For the nonlinear regression experiment in Section 4.2, ROBOT and ROBOT-robust is trained
with learning rate 10−4 for 80 iterations. For n = 100, 200, 500, 1000, 2000, we set batch size
10, 30, 50, 100, 300, respectively.We set (cid:15) = 10−4 for the Sinkhorn algorithm in ROBOT. For Oracle
and LS, we perform ordinary regression model and ensure convergence, i.e., learning rate 5 × 10−2
for 100 iterations.

F.3 Flow Cytometry

We provide more details for the Flow Cytometry experiment in Section 4.3. In the FC seting,
ROBOT is trained with batch size 1260 and learning rate 10−4 for 80 iterations. In the GFC seting,
ROBOT is trained with batch size 1260 and learning rate 6 × 10−4 for 60 iterations. We set (cid:15) = 10−4
for the Sinkhorn algorithm in ROBOT. Other settings for the hyper-parameters in the baselines
follows the default settings of their corresponding papers. EM is initialized by AM.

F.4 Multi-Object Tracking

For the MOT experiments in Section 4.4, the reported results of MOT17 (train) and MOT17 (dev)
is trained on MOT17 (train), and the reported results of MOT20 (train) and MOT20 (dev) is trained

29

− µi), b1 = − (cid:80)(cid:100)n/2(cid:101)

i=1 ξi, b2 = − (cid:80)n

i=(cid:100)n/2(cid:101) ξi

− νj), b1 = − (cid:80)(cid:100)m/2(cid:101)

j=1 ζj, b2 = − (cid:80)m

j=(cid:100)m/2(cid:101) ζj

(cid:48))(cid:62)(diag( ¯µ))−1

Algorithm 2 Computing the gradient for w
Require: C ∈ Rm×n, µ, ν, (cid:15), dC
dw

∗
r , ¯µ, ¯ν, ξ, ζ

(cid:80)n

(cid:80)m

(cid:48)K−1

(cid:80)(cid:100)n/2(cid:101)

(cid:80)(cid:100)m/2(cid:101)

(cid:48)) − (S

i=1 ( ¯µi

j=1 ( ¯νj

i=(cid:100)n/2(cid:101)( ¯µi

j=(cid:100)m/2(cid:101)( ¯νj

− νj), x2 =

− µi), x2 =

(cid:48) = S[:, : −1]

(cid:48)
(cid:48)K−1(S

¯µ )−1, ¯ν = ¯ν + (cid:15)(2λ41m + (cid:15)2

(cid:48))T (diag( ¯µ))−1S
← (diag( ¯µ))−1 + (diag( ¯µ))−1S
← −(diag( ¯µ))−1S
← (H2)(cid:62)
← K−1

Run forward pass to get S = S
x1 =
[λ1, λ3](cid:62) = [(cid:100)n/2(cid:101), x1; n − (cid:100)n/2(cid:101), x2]−1[b1, b2](cid:62)
x1 =
[λ2, λ4](cid:62) = [(cid:100)m/2(cid:101), x1; m − (cid:100)m/2(cid:101), x2]−1[b1, b2](cid:62)
¯ν )−1
¯µ = ¯µ + (cid:15)(2λ31n + (cid:15)1
(cid:48) = ¯ν[: −1], S
¯ν
K ← diag( ¯ν
H1
H2
H3
H4
Pad H2 to be [n, m] with value 0
Pad H3 to be [m, n] with value 0
Pad H4 to be [m, m] with value 0
L = diag([(cid:15)(2λ31n + (cid:15)1
A1 = [H1, H2; H3, H4]
· L
A2 = −A1
(cid:62)
A3 = A
2
A4 = L + L · A1
· B1(D − C · A4
E = A1 + A2
[J1, J2; J3, J4] = E, where J1
[ dξ∗
dC ]nij
[ dζ∗
dC ]mij
Pad dζ∗
[ dL
dC ]ij
return

← [J1]niSij + [J2]njSij
← [J3]miSij + [J4]mjSij
dC to be [m, n, m] with value 0

¯µ )−1, (cid:15)(2λ41m + (cid:15)2

n,m CnmSnm[ da∗

← 1
(cid:15) (−CijSij +
dL
dC
dw
dC

∈ Rn×m, J3

dC ]nij +

¯ν )−1])

· L

(cid:80)

(cid:80)

· B)−1C · A3, where B1, C1, D deﬁned above
∈ Rn×n, J2

∈ Rm×n, J4

∈ Rm×m

n,m CnmSnm[ db∗

dC ]mij) + Sij

30

on MOT20 (train). Each model is trained for 1 epoch. We adopt Adam optimizer with learning
rate= 10−5, (cid:15) = 10−4, and η = 10−3. To track the birth and death of the tracks, we adapt the inference
code of Xu et al. (2019b).

F.5 The Eﬀect of ρ1 and ρ2

∗
We visualize S
r computed from the robust optimal transport problem in Figure 9. The two input
distributions are Unif(0, 2) and Unif(0, 1). We can see that with large enough ρ1 and ρ2, Unif(0, 1)
would be aligned with the ﬁrst half of Unif(0, 2).

(a) ρ1 = 0, ρ2 = 0

(b) ρ1 = 0.1, ρ2 = 0.1

(c) ρ1 = 0.2, ρ2 = 0.2

Figure 9: Computed S

∗ for robust optimal transport problem.

F.6 Comparison of Residuals in Linear Regression

∼ N (0d, Id), zi

Settings. We generate n data points {(yi, [xi, zi])}n
∼ N (0e, Ie), w ∼ N (0d+e, Id+e), and εi
xi
f ([xi, zi]; w)+εi. Next, we randomly permute the order of {zi
2 = {zj
Here, D

∈ Re. We ﬁrst generate
∼ N (0, ρ2
noise). Then we compute yi =
} so that we lose the data correspondence.
} mimic two parts of data collected from two separate platforms.
−
i((cid:98)yi
w. To evaluate model performance, we use error=

We adopt a linear model f (x; w) = x

1 = {(xi, yi)} and D

i=1, where xi

∈ Rd and zi

(cid:80)

(cid:62)

(cid:80)

yi)2/

i(yi

− ¯y)2, where (cid:98)yi is the predicted label, and ¯y is the mean of {yi

}.

Baselines. We use Oracle, LS, Stochastic-EM as the baselines. Notice that without a proper
initialization, Stochastic-EM performs well in partially permuted cases, but not in fully shuﬄed
cases. For better visualization, we only include this baseline in one experiment. Furthermore,
we adopt two new baselines: Sliced-GW (Vayer et al., 2019) and Sinkhorn-GW (Xu et al., 2019a),
which can be used to align distributions and points sets.

Results. We visualize the ﬁtting error of regression models in Figure 10. We can see that ROBOT
outperforms all the baselines except Oracle. Also, our model can beat the Oracle model when the
dimension is low or when the noise is large.

31

0.040.140.170.770.850.881.061.091.101.141.211.291.431.561.581.671.741.781.851.930.120.140.410.460.520.640.780.800.940.980.040.140.170.770.850.881.061.091.101.141.211.291.431.561.581.671.741.781.851.930.120.140.410.460.520.640.780.800.940.980.040.140.170.770.850.881.061.091.101.141.211.291.431.561.581.671.741.781.851.930.120.140.410.460.520.640.780.800.940.98Figure 10: Linear regression. We use n = 1000, d = 2, e = 3, ρ2

noise = 0.1 as defaults.

32

