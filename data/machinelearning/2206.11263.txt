2
2
0
2

n
u
J

2
2

]

G
L
.
s
c
[

1
v
3
6
2
1
1
.
6
0
2
2
:
v
i
X
r
a

Optimally Weighted Ensembles of Regression Models:
Exact Weight Optimization and Applications

Patrick Echtenbruck∗, Martina Echtenbruck∗, Joost Batenburg∗, Thomas
B¨ack∗, Michael Emmerich∗, Boris Naujoks+

∗LIACS, Leiden University, Niels Bohrweg 1, The Netherlands,
+IDE+A, TH Cologne, 51643 Cologne Gummersbach, Germany

Abstract

Automated model selection is often proposed to users to choose which machine
learning model (or method) to apply to a given regression task. In this paper we
show that combining diﬀerent regression models can yield better results than
selecting a single (’best’) regression model, and outline an eﬃcient method that
obtains optimally weighted convex linear combination from a heterogeneous set
of regression models. More speciﬁcally, in this paper a heuristic weight opti-
mization, used in a preceding conference paper, is replaced by an exact opti-
mization algorithm using convex quadratic programming. We prove convexity
of the quadratic programming formulation for the straightforward formulation
and for a formulation with weighted data points. The novel weight optimization
is not only (more) exact but also more eﬃcient. The methods we develop in
this paper are implemented and made available via github-open source. They
can be executed on commonly available hardware and oﬀer a transparent and
easy to interpret interface. The results indicate that the approach outperforms
model selection methods on a range of data sets, including data sets with mixed
variable type from drug discovery applications.

1. Introduction

A common task in machine learning is to build a regression (or surrogate) model
of a black-box function based on a set of known evaluation results of that func-
tion at some points. The regression model can be used to partially replace the
original function, for instance in cases where the original function is expensive
to evaluate or evaluations are diﬃcult to obtain for other reasons.

Surrogate models play a signiﬁcant role in modern optimization, prediction,
modeling or simulation tools. In recent years various types of surrogate models
have been proposed in the machine learning literature and integrated as options
in machine learning libraries. It remains however diﬃcult for users to select the

Preprint submitted to Elsevier

June 24, 2022

 
 
 
 
 
 
right method for the given data-set and often this model selection problem is
solved by experimenting with diﬀerent models, looking at the training error.

Automatic model selection methods relief the user from this task.
It can be
accomplished by, for instance, ranking models based on the cross-validation er-
ror on the training data set. However, recent research has shown that one can
do better than merely selecting the best model from the ensemble by combin-
ing several of the available regression models. Whereas many of these methods
rely on overly simplistic assumptions (majority vote, averaging) or introduce a
large amount of additional complexity (genetic programming, symbolic regres-
sion), the recently proposed optimally weighted model mixtures provide a good
compromise between simplicity, ﬂexibility and integration of various regression
models [12]. Moreover, it is a model-agnostic approach and can combine re-
gression models of various types, such as for instance artiﬁcial neural networks,
Gaussian process regression, piecewise linear regression, and random forests.
This paper provides a signiﬁcant extension of our earlier work on such model
mixtures [18].

In particular, this paper deals with ensembles of regression models that use
a linear combination and oﬀer an interpretable, generally applicable, and eﬃ-
cient approach for combining models. Instead of simply selecting the prediction
of the best model based on the cross-validation error, we propose to combine
the model predictions of various regression models in an optimal way consider-
ing covariance information. In the formulation we restrict ourselves to convex
linear model combinations, which are easy to interpret and can be conﬁgured
eﬃciently. This article has three novel contributions:

• Firstly, we show how to adapt the approach to data sets where sample
points are unevenly distributed. This is accomplished by a linear weighting
scheme for data points based on point density that can be integrated into
the exact solution method without a signiﬁcant increase in complexity.

• Secondly, we replace the heuristic optimization procedure used in a pre-
vious paper by an exact method, using a quadratic programming (QP)
formulation that can be solved eﬃciently by standard QP solvers.

• Thirdly, we apply and test the method on a range of data sets, including
examples from drug discovery that require the combination of regression
models for high dimensional functions with discrete variable types.

This article is structured as follows: In Section 2 we introduce the preliminaries
needed for the theoretical part of the paper. In Section 3 we discuss previous
and related works. In the Sections 4 to 6 we develop the approach presented in
this paper. Firstly, Section 4 discusses the relevance of clustering in the data
and presents a method to eﬀectively handle clustered data. Secondly, Section 5
discusses the need for large ensemble sets and illustrates that the search space
for the optimal ensemble setup is convex, while Section 6 gives the mathemat-
ical proof that the problem of ﬁnding optimal weights is convex and the error

2

minimizing weights combination can directly be calculated by quadratic pro-
gramming. In Section 7 the adapted ensemble algorithm is tested on classical
regression data sets as well as on diﬀerent data sets from drug property predic-
tion. The results of these experiments are presented and discussed in Section
8. Main aspects and ﬁndings of this paper are summarized and possible future
works are discussed in Section 9.

2. Preliminaries

In machine learning, it is a common task to model a given objective function
in order to classify unknown points or to predict promising parameter settings.
In both cases, costly function evaluations on the original models are reduced by
partly replacing them with fast approximate evaluations on surrogate models.
By surrogate model, we understand a function ˆf : Rd → R that is an approxima-
tion to the original function f : Rd → R, learned from a ﬁnite set of evaluations
of the original function.

The number of available modeling algorithms, all featuring diﬀerent strengths
and weaknesses, for the user to choose from is large. However, the choice of
the model is crucial for the solution quality. Burnham et al. even state that
the selection of the right surrogate model is the most crucial question in mak-
ing statistical inferences [8]. To choose the best surrogate model for a given
objective function, often expert knowledge about the surrogate model as well
as the objective function is needed. But if no preliminary knowledge about the
surrogate model or the objective function is available, it would be beneﬁcial if
an algorithm could learn which surrogate model suits best to a given problem.

Model selection approaches, which a priori train a set of models on training
data and then choose the best surrogate model using statistical approaches, are
already well established. However, recent results show that it can be beneﬁcial to
linearly combine several surrogate models into one better-performing ensemble
model [12, 2, 3]. Further constraints on the ensemble coeﬃcients lead to convex
combinations, not to be confused with convex functions, and can be deﬁned as
follows. Given s diﬀerent surrogate models ˆfi : Rd → R, i = 1, . . . , s, d the
input dimension of the approximated functions and αi the weights for the i-th
surrogate model, a convex combination of models (CCM) speciﬁes an ensemble
of surrogate models as follows:

αi

ˆfi

s.t.

(cid:88)

αi = 1 and αi ≥ 0, i = 1, . . . , s

s
(cid:88)

i=1

Due to the summing up to unity constraint, the search space of model weights
is an (s − 1)-dimensional simplex. Special solutions, where only one model
is used in the ”ensemble”, are located in the corners of the simplex. To ﬁnd
the optimally weighted ensemble a minimization of the cross-validation error

3

i=1 αi = 1, αi ≥ 0}.

over the set of possible CCMs can be performed by searching over the simplex
{α ∈ Rs| (cid:80)s
As cross-validation error or ﬁtness function resp., an adaptation of the root mean
squared error (RMSE) is regularly used. The RMSE of the predictions of a single
i=1 (yi − ˆyi)2, where n is the number of
model is deﬁned as RM SE =
predictions of this model. As a remark, it suﬃces to ﬁnd the minimizer of the
squared sum (cid:80)n
i=1 (yi − ˆyi)2 and it is equivalent to the minimizer of the RM SE.

(cid:113) 1
n

(cid:80)n

3. Related Work

The general scenario that is investigated in our studies is the automated selection
and optimal mixture of prediction or regression models. Ensemble methods have
a long history and some early work has already been presented in the 1970ties
[3] for time series predictions.

The work presented in this article is a follow up and extension to an earlier
conference paper [12], where the idea of building sparse ensembles by mixtures
or convex combination of heterogeneous models has been proposed and studied
on a benchmark set with multi-modal regression problems and a heuristic opti-
mization with an (1+1)-ES is used to optimize the weights of the ensemble. In
[18] a review of other types of ensemble approaches is given.

The approach in [12] was recently taken up in [4] in the context of predicting
time series from the COVID-19 pandemic and an alternative penalizing term
was introduced that is similar to the approach used in the LASSO regression
model, where instead of models, variables are selected based on their marginal
distribution.

The new approach of our article extends the work by Friese et al. [12] by the
following major contributions. Firstly we show that ﬁnding optimal model mix-
tures is a convex quadratic optimization problem that can be solved to opti-
mality, even if density of points is considered (weighted training set). Secondly,
the approach is used on a broader data-set including discrete variables and data
from the important application domain of molecular property prediction in drug
discovery.

Acar and Rais-Rohani [1] proposed some adaptations of previously deﬁned ap-
proaches of weighted sum ensembles for better generalization. Like in previous
works they also required the weights to be positive and sum up to one as the
only constraints on the weights. Building on the approach of Bishop et al. [7],
they proposed to use k-fold cross-validation to allow for an evaluation of models
that have per deﬁnition no error at the training points. For the works of Goel
et al. [15] they suggested to generalize the mean squared error1(GMSE) of the
ensemble.

1The GMSE refers to the MSE applied in a leave-one-out cross-validation process.

4

4. Local Density Weighted Cross-Validation

The ensemble method, as ﬁrst presented by Friese et al. [12], performed all ex-
periments on mathematical test functions generated by Max-Set of Gaussian
Landscape Generator [14]. These test functions are trained on data-sets gener-
ated automatically using Latin Hypercube Design [19]. However, in real-world
problem deﬁnitions, we cannot rely on the assumption that the available data is
likely evenly spread. In sequential parameter optimization, it is even expected
to build clusters of evaluated points at local optima. Without taking this pos-
sible clustering of the points into account during cross-validation we risk to put
too much emphasis on prediction errors in the area of clustered points, which
leads to over-ﬁtting in these areas.

One potential solution could be to exclude individual points situated in clustered
areas from the set before evaluation to ensure an even distribution of the points
in the data set before cross-validation. However, to do so it must be speciﬁed
how many and which points will be excluded from the set. But, by doing so
important information may be dropped from the data set and the surrogate
models may perform better with the complete data-set.

Therefore, we propose to keep all data points and weight the squared error of
points in denser areas to reduce the importance of these points. The given
weights depend on the density of the direct neighborhood of each point and
hence of the position of the regarded point. This way, the weighting occurs
smoothly and without harsh steps between the weights of neighboring points.
To this end, we use density weighted cross-validation applying a weighted Root
Mean Square Error (wRMSE) as a quality indicator:

wRM SE =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1

(βi(yi − ˆyi))2

(1)

The weights βi ∈ [0, 1] that are applied to the squared prediction errors (yi−ˆyi)2
of their related predictions ˆy at the positions xi ∈ Rd, i = 1, . . . , n, n the number
of predictions, are derived from the proximity of the point’s nearest neighbors,
and thus the density of the points direct neighborhood. For the calculation of
this density, the k nearest neighbors of each point are utilized. Per default, k is
set to 20.

These k nearest neighbors are utilized to calculate the density densi of a point
xi as their median Euclidean distance to this point:

densi = median






(cid:118)
(cid:117)
(cid:117)
(cid:116)

d
(cid:88)

j=1

(xij − xlj)2 | l = 1, . . . , k






(2)

Density values that are exceeding the overall mean density are truncated to the
mean value. This way, it is ensured that points not located in sparse areas get

5

higher weight during cross-validation, and on the other hand points in highly
crowded regions receive a lower weight to prevent over-representation of these
regions in the global modelling of the response function. The resulting density
values at the point locations will be denoted with densi. They are normalized
to the [0, 1] range by computing βi = densi/ max{dens0, . . . , densn} in order
to obtain the weights βi ∈ [0, 1]. Here, the lower bound does not have to be
considered since the density values densi are calculated from distance values
and thus are positive per deﬁnition. Also, it is not intended to force zero weight
on points with the highest density.

Applying these weights to the prediction errors in the calculation of the RMSE
during cross-validation ensures that all points with a neighborhood not denser
than the mean density are considered with full weight and only points located in
denser neighborhoods are weighted according to the density of their neighbor-
hoods, while the weighting during the transition from sparser areas to clustered
areas is smooth.

Figure 1 illustrates the impact of the weighting on the points on an exemplary
optimization situation. The example uses two states of the same optimization
process that used a Latin Hypercube Design for the initial setup. Both plots
on the left-hand side show the situation at the beginning of the optimization
process, while the ﬁgures on the right-hand side illustrate the situation at the
end of the optimization process. On the left-hand side, the points are evenly
distributed over the search space, and, as the histogram in the lower row shows,
most points get the full weight. Only a few points are lightly weighted. On the
right-hand side, the points have clustered around the local optima. Again the
histogram shows that most points get the full weight, but some points, colored
turquoise in the upper row, are considered with a higher weight.

5. Ensemble Methods

In real-world regression or optimization problems and in black-box regression or
optimization, the user often faces the problem that the main characteristics of
the objective function are not known. Additionally, a wide range of potentially
applicable surrogate models is available. Each of these models is based on
speciﬁc model assumptions that deﬁne its strengths and weaknesses.
If any,
only very few users are acquainted with all of them. However, the right choice
of a surrogate model for a speciﬁc problem deﬁnition is crucial for the quality
of the regression or optimization result, respectively. The same applies for the
use of ensembles of surrogate models. Or even more so, since not only the most
appropriate models need to be in the set but also weaker performing models,
since they may still be able to compensate weaknesses of the stronger models.
Hence, a large set of available surrogate models ought to be the best starting
point for a best performing ensemble approach.

However, as the number of models grows, the complexity of ﬁnding the best
model combination grows. Friese et al. [12] applied a simple (1+1)-ES to search

6

Figure 1: The plots show the impact of the weighting procedure on the points at the beginning
and the end of an optimization process on a 2D Ackley function. Points that are colored black
are fully taken into account, and lighter blue points are weighted. The lower row shows the
related distribution of weights used. By the light blue color of the cluster it can be seen that
points near the cluster are rigorously weighted.

for the best model mixture on a set of three surrogate models, which showed
promising results in reasonable time. Nevertheless, this approach deteriorates
as the number of surrogate models, and with it, the number of dimensions of
the search space grows.

Echtenbruck [18] showed that the algorithm completely failed to ﬁnd a known
good solution for three models in a set with additional ten models. Echtenbruck
presented an incremental adaptation of the search strategy that searches for
improving solutions in the diﬀerent search dimensions successively. However,
the improvement of the performance of the search algorithm goes along with
an increased computation time for the search, linear in the number of surrogate
models in the set. A more eﬃcient approach is desirable.

7

Originally, the (1+1)-ES was chosen for its capability to perform well on convex
functions as well as on non-convex or even multi-modal functions. However,
previously realised experiments suggested the assumption that the search space
for the optimal weighting might be convex [12, 18]. Also closer inspection of the
applied error function conjectures that the optimization problem, speciﬁed by
the MSE, is convex, noting that the minimizer of the MSE function coincides
with the minimizer of the RMSE function.

In the following we show that the function is indeed convex. First we will
motivate this heuristically, followed by a rigorous mathematical proof.

The RMSE is calculated based on an unordered set of prediction errors (devia-
tion between prediction and true value) and is therefore independent from the
structure of the underlying objective functions and the models used. Only the
(weighted) mean of the squared deviations is to be minimized. Considering the
mixture of two models, the three following cases can be distinguished when a
prediction for a single point is made:

Case 1: both models deliver the same prediction.

Case 2: they deliver diﬀerent predictions but both are either smaller or
larger than the objective function value or,

Case 3: they deliver diﬀerent predictions, but one model predicts larger
and one smaller values than the objective function value.

Examples for these cases are shown in Figure 2. The Figure depicts predictions
and errors of the ensembles that would result from a convex linear combination,
using the mixture of models approach, as well as their resulting prediction errors.

Figure 2: The plots show the three diﬀerent cases of prediction combinations of two base
models for a single point. The horizontal line marks the distance zero to the objective function
value , the dots mark the predictions of the diﬀerent models. Here the red and the blue dot
depict the base models, while the white dots depict the predictions of the ensembles that
originate from the convex linear combination of the two base models. The red lines illustrate
the prediction errors that result from the predictions of these models.

For the calculation of the RMSE, the mean of the squared errors is considered.
x is a bijective function and strictly monotonic for x > 0, the square root
Since

√

8

0.00.10.20.30.40.50.60.70.80.91.0Base model combinationsDistance to objective function value0.00.10.20.30.40.50.60.70.80.91.0Base model combinationsDistance to objective function value0.00.10.20.30.40.50.60.70.80.91.0Base model combinationsDistance to objective function valuecan be omitted since it has no inﬂuence on the position of the minimizer (the
point where the function obtains its minimum). Following previous thoughts,
there are also three possible types of functions for the squared errors that have
to be considered, these are shown in Figure 3.

Figure 3: The plots show the corresponding squared errors for the predictions shown in Figure
2, depicted as red lines. The black line depicts the resulting search space for the optimal convex
combination of the two base models.

If both models predict the same function value (Case 1) all predictions of the
related convex combination models will be the same, and the resulting function
of squared prediction errors is therefore also constant. In Case 2, where both
models predict diﬀerent values, but both are either larger or smaller than the
actual objective function value, the resulting function of squared errors will
be quadratic, monotonic, and the optimum will be obtained at the boundary
(meaning that in this case one of the models will be weighted with zero). In
Case 3, as depicted in the right hand side plot of Figure 3, one model predicts
larger and one smaller values than the actual objective function value. The
resulting function of squared errors will be a quadratic function which obtains
its minimum (of zero) in the interior of the interval, at the position where the
convex combination of the base models would predict the actual function value.

The RMSE function that deﬁnes the search space for the optimal mixture of
these two models is given by building the mean of n squared-error functions,
where n is the number of training points. Adding a constant function type
prediction of a new point (Case 1) to this mean would result in a compression
of the original function that does not change the position of its minimizer.
Adding an ascending or descending function type prediction (Case 2) to the
mixture would result in a horizontal shift and if any in a horizontal compression
of the mixture function. Adding a function, containing a minimizer at another
position than contained in the mixture (Case 3), would result in a horizontal
shift of the minimizer and if any in a horizontal dilation or compression of
the existing mixture function. Figure 4 illustrates these cases. Algebraically,
the superposition of a quadratic function and a quadratic function results in a
quadratic function.

9

0.000.250.500.751.00Base model combinationsSquared Erro of the predictions0.000.250.500.751.00Base model combinationsSquared Erro of the predictions0.000.250.500.751.00Base model combinationsSquared Erro of the predictionsFigure 4: In these plots the black lines represent a function that was created by building
the mean of several functions. The green lines represent the functions from the three cases
introduced in Figure 3. The dashed lines illustrate how the mean function would evolve if the
function represented by the green line was added to the mean. Diﬀerent examples are given in
case the mean of the black line was built from 1,2,3,4 or 5 functions. Of course the inﬂuence
of the added function would decrease with a higher number of already contained functions.

These thoughts are so far restricted to the combination of two models. However,
any line segment in the search space is bounded by two end points that in itself
are model-mixtures and thus models. Therefore also on this line segment the
function is convex and quadratic, as in the case of two models discussed above.

A mathematical proof, that the regarded search space is actually convex, is
given in the following chapter.

6. Exact Quadratic Programming Method

Let ˆf1, . . . , ˆfs : Rd → R be the set of surrogate models.
Let x1, . . . , xn ∈ Rd be the set of sample points where the models are evaluated.
We denote the vector of coeﬃcients for the linear combination of surrogate
models by α ∈ Rs. Deﬁne Ω = {α ∈ Rs| (cid:80)s
i=1 αi = 1, αi ≥ 0 (i = 1, . . . , s)}, the
set of valid weight vectors. Note that Ω is deﬁned as an intersection of convex
sets (one halfspace for each nonnegativity constraint and a hyperplane for the
summation constraint) and is therefore convex.
Deﬁne A = (aij) ∈ Rs×n by aij = ˆfj(xi). The matrix A contains the evaluations
of the surrogate models
in the sample points, where each column of A is an
s-dimensional vector containing the evaluations for one of the surrogate models.
For a given vector of coeﬃcients α and vector y ∈ Rn of desired outcomes,
deﬁne

RMSE (α, y) =

||Aα − y||2

(3)

(cid:114) 1
n

Note that this deﬁnition is equivalent to the RMSE deﬁnition given in Section
2, except for a change in notation.

10

05101520250.000.250.500.751.00XY05101520250.000.250.500.751.00XY05101520250.000.250.500.751.00XYConsider the following problem:

Or, equivalently, deﬁne:

minimizeα∈Ω RMSE (α, y)

α∗ = arg min

RMSE (α, y)

α∈Ω

(4)

(5)

In the following we will show that the above problem is a convex quadratic
programming problem and therefore it has a unique solution and can be solved
with eﬃcient standard solvers.

Theorem 1. The optimization problem stated in Equation 4 and 5 is a convex
quadratic problem.

√

x is a strictly monotonic function for x ≥ 0, this
Proof. As n is ﬁxed and
minimization problem is equivalent to minimizing ||Aα − y||2. More speciﬁcally,
for any α1, α2 ∈ Rn, we have that RMSE (α1, y) ≥ RMSE (α2, y) if and only if
||Aα1 − y||2 ≥ ||Aα2 − y||2. We therefore need to solve:

minimizeα∈Ω ||Aα − y||2

(6)

Deﬁne Q = AT A and c = AT y. Then Problem 6 can be written as a quadratic
programming problem in the following form:

minimizeα∈Ω

αT Qα + cT α

(cid:19)

.

(cid:18) 1
2

(7)

Problem 7 is convex as Q is a positive semideﬁnite matrix, and the feasible set
is the convex set Ω.

A variation of the problem, weighted RMSE (wRMSE) minimization, is based
on the following deﬁnition:

wRM SE =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
n

n
(cid:88)

i=1

(βi(yi − ˆyi))2

(8)

Like in the unweighted case it can be easily shown that this is a convex quadratic
problem.

Theorem 2. Given a set of non-negative weights βi, i = 1, . . . , n, the problem

α∗ = arg min

wRMSE (α, y)

α∈Ω

(9)

is a convex quadratic problem.

11

(a) Search trajectory and optimal solution found
by an heuristic method ((1+1)-Evolutionary
Strategy)

(b) Optimal solution found by Quadratic Pro-
gramming

Figure 5: All considered individuals are shown as points, the optimal solution is marked with
a white circle. The Quadratic Programming solution is located close to the optimal solution
found by the heuristic method.

Proof. To show this, the formulation (of Eq. (6) can be modiﬁed in a straight-
forward way. Each row i of the matrix A (and each entry of the vector y) will
have weight βi. All these weights jointly form the vector β. We then multiply in
Eq. (6) the rows of A by β (replacing A by ˜A = βA), and also the corresponding
entries of y (replacing them by ˜y = βy). We can then continue with the same
reasoning as before starting from Eq. (7) , but with a new matrix ˜A and vector
˜y.

For positive deﬁnite Q, the ellipsoid method solves the problem in (weakly)
polynomial time [16].

To depict the diﬀerences between results received by exactly solving the above
QP problem and an heuristic method, these methods are applied to the prob-
lem of ﬁnding the optimal convex combination of three base models on a given
objective function. The experiment setup relies on the experiment setup ini-
tially introduced in [12]: We use the Max-Set of Gaussian Landscape Generator
(MSG) [12, 13], to generate a four dimensional objective function using 160
Gaussian process realizations, and Kriging surrogate models using three diﬀer-
ent kernels: gaussian, spline and exponential, following the deﬁnitions of [17],
as base models. To generate the experiment data, a Latin Hypercube Design
of 160 points is evaluated on the objective function. The three base models
are then evaluated on these data using a Leave-One-Out Cross-Validation to
obtain 160 model predictions on the objective function for each model. For the
heuristic method we used a simple (1+1)-Evolution Strategy with 1/5th success
rule [6], also as proposed by [12].

12

Figure 5 shows the results of these two weight optimizations in a ternary mixture
diagram. The results of the (1 + 1)-ES and the QP method are similar, but due
to its stochastic nature the (1 + 1)-ES is not guaranteed to converge, why it is
preferred to use the exact solver, which has now been made available.

7. Application

A real world application of the optimally weighted ensemble approach is the
ﬁeld of drug discovery. De novo drug discovery mainly targets the prediction of
bio-activity. This way it can be determined whether a component is active or
inactive on a speciﬁc target protein. From a chemist’s perspective developing
new drugs is a complex and costly task. The reason for that is that a lot of
compounds fail compared to the small number of successful candidates. There
are two main goals in in silico classiﬁcation that are mainly considered. The
prediction of activity on targets to identify compounds of high eﬃcacy, and the
prediction of activity on oﬀ-targets to avoid or exclude compounds that likely
have unwanted side-eﬀects. In silico drug discovery can provide promising com-
ponents in reasonable time. This procedure can minimize drug discovery costs
by pre-selecting components that chemists need to test in ”wet lab” experiments
and exclude less promising components beforehand.

In our experiments drug data is represented as functional class ﬁngerprints
(FCFP) [20] which is an established way of representing molecule data in a
computer readable form and also retain comparability between molecules.

Promising results in combination with multiobjective optimization for classi-
ﬁcation are available [10]. Furthermore, optimally weighted ensembles were
successfully applied to this kind of data [9]. Ensembles of heterogeneous models
lead to better results than the single model approach.
The dataset is derived from the publicly available ChEMBL2 database. A tool
written for this purpose gets all molecules with a known activity for a speciﬁc
target protein from the ChEMBL API [9]. This data is converted to a dataset
containing FCFPs. The activity of a molecule is deﬁned by a proprietary con-
tinuous pChembl value [5].

The dataset we analysed further has the ChEMBL Id CHEMBL4159 and has
the Endoplasmic reticulum-associated amyloid beta-peptide-binding protein as a
target. This target was mainly chosen because of the large count (>20,000) of
known molecules that have an activity on this target.

To determine the inﬂuence of clustering and therefore reducing the weight of
single points in cluster-regions we analysed the weights in detail. The plots in
Figure 6 show the distribution of diﬀerent weights in our CHEMBL4159 dataset.

2Chemical database by EMBL-EBI; European Bioinformatics Institute at the European

Molecular Biology Laboratory.

13

We have a total of 18 452 points, 9 572 of these have a weight of 1. Since the
histogram clearly shows that the weights tend towards a value of 1 and almost
no values are below 0.50, there is not much clustering inside the CHEMBL4159
dataset. To further check this we generated a test-set with maximized coverage
of the data points. This lead to almost identical results. The plots in Figure 7
show the distribution of diﬀerent weights in our CHEMBL4159 dataset with a
maximized distribution of data in the test set. We have a total of 18 452 point
of which 9 586 have a weight of 1.

This ﬁndings are also reﬂected in the ROC (Receiver Operating Characteris-
tics) plots in Figures 8 and 9. The plots in Figure 8 show the mean result of
unweighted RMSEs and the plots in Figure 9 show the mean result of weighted
RMSEs. The results are almost identical and the optimal point deﬁned by the
Youden-Index J or Youden’s J statistic which was originally proposed in [21]
is nearly in the same spot. This results from the lack of clustering inside the
CHEMBL4159 dataset where a weighting of point is obviously not necessary.
This shows that the beneﬁt of using the weighted RMSE is negligible when
the data is distributed homogeneously. The weighted RMSE approach does
not guarantee better results and mainly depends on the existence of clusters in
the data. However, since it is generally not known beforehand how the data is
distributed, the wRMSE should be the means of choice.

Also, experiments were conducted to determine if the quadratic programming
approach leads to equally good or better results. The plots in Figure 8 and Fig-
ure 9 show these results. There is also the optimal Youden Index point marked.
The results are similar to the ensemble results from the ES approach. What can
be deduced from the plots the optimal Youden Index is almost identical with
the beneﬁt of a hugely improved runtime.

As described in Echtenbruck et al. [9], the mixture of models could indeed
improve the results. One model that was used as part of the ensemble is an
optimized ranger/random forest. This optimized model contributed hugely to
the overall result with around 90%, but the overall result could be improved
with an added Lasso model. A Ridge model was also used, but could not
contribute in a meaningful manner. It was excluded in 9 out of 10 experiments.
With standard models that were not optimized for the problem beforehand, the
ensemble has a more even distribution of contributing models. These results
are conﬁrmed when using the quadratic programming approach instead of an
(1+1)-ES. Nevertheless, better results could be achieved if all the contributing
models are optimized beforehand.

Since overﬁtting can be a problem in model building when the model corresponds
too closely to the training set, we analysed if this is the case in our ensemble
for drug datasets. We used y − y-Plots [11] to plot the actual value and the
predicted value as x- and y-axis. The plots for the training and test set are
depicted in Figure 10.

14

(a) Weights applied to the points of
the
CHEMBL4159 dataset. Nearly 10 000 points are
weighted.

(b) Weights applied to the points of
the
CHEMBL4159 dataset, points with weight=1
excluded.

Figure 6: Histogram of the applied weights on the CHEMBL4159 dataset.

(a) Weights applied to the points of
the
CHEMBL4159 dataset with max coverage inside
the dataset. Nearly 10 000 points are weighted.

(b) Weights applied to the points of
the
CHEMBL4159 dataset with optimized distribu-
tion, points with weight=1 excluded.

Figure 7: Histogram of the applied weights on the CHEMBL4159 dataset with max coverage
inside the dataset.

15

0250050007500100000.000.250.500.751.00weightcount030060090012000.000.250.500.751.00weightcount0250050007500100000.000.250.500.751.00weightcount03006009000.000.250.500.751.00weightcount(a) Mean curve calculated by ES with un-
weighted RMSE and Youden Index marked

(b) Mean curve calculated by QP with un-
weighted RMSE and Youden Index marked

Figure 8: The mean result of all 10 experiments with Youden Index marked (unweighted
RMSE, max coverage in test set).

(a) Mean curve calculated by ES with weighted
RMSE and Youden Index marked

(b) Mean curve calculated by QP with weighted
RMSE and Youden Index marked

Figure 9: The mean result of all 10 experiments with Youden Index marked (weighted RMSE,
max coverage in test set).

16

0.00.20.40.60.81.00.00.20.40.60.81.01−Specificity (FPR)Sensitivity (TPR)lOptimal (Youden Index) point0.00.20.40.60.81.00.00.20.40.60.81.01−Specificity (FPR)Sensitivity (TPR)lOptimal (Youden Index) point0.00.20.40.60.81.00.00.20.40.60.81.01−Specificity (FPR)Sensitivity (TPR)lOptimal (Youden Index) point0.00.20.40.60.81.00.00.20.40.60.81.01−Specificity (FPR)Sensitivity (TPR)lOptimal (Youden Index) pointFigure 10: Actual values compared to predicted values for training set (left) and test set
(right).

8. Discussion

In summary, the results on the approximation of mathematical benchmark func-
tions and on drug property prediction show that the usage of quadratic program-
ming has three advantages compared to the state-of the art heuristic (1+1)-ES
method:

• it oﬀers a run-time beneﬁt providing a solution almost instantaneously,

while the (1+1)-ES took several seconds to converge;

• it provides slightly better results regarding model weighted models;

• it is more reliable, because ”bad surprises” are avoided in cases where the

heuristic method might converge prematurely.

Moreover, the results on the drug discovery data show that the used of heteroge-
neous data is possible, and the input does not necessarily be from a continuous
domain, as in the previously published results of optimally weighted model mix-
tures. Even though the results of the weighted RMSE method is better with the
exact approach than with the heuristic approach, it is surprisingly not always
the case that it outperforms the non-weighted method that does not compensate
for the clustering of data points.

9. Summary and Outlook

This research article proposes an exact and eﬃcient convex quadratic program-
ming method on how to ﬁnd optimally weighted ensembles of models, using
convex combinations. The approach is elegant, yet powerful and is shown to

17

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.02.55.07.50.02.55.07.5training_set.V2result123lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.02.55.07.50.02.55.07.5test_set.V2activity1yield better results than using only model selection strategies. We also study
how we can compensate for overrepresentation of highly clustered regions in the
training data and suggested density-based weighting, which can be integrated
straightforwardly, and without introducing non-convexity, in the quadratic pro-
gramming method.

The results on the approximation of mathematical benchmark functions and on
drug property prediction show that the usage of quadratic programming has
not only a run-time beneﬁt, but also leads to slightly better results regarding
model weights, if compared with heuristic methods. However, apart from mere
benchmarking results, the replacement of the heuristic method by a exact convex
quadratical programming method has also the advantage that ”bad surprises”
are avoided in cases where the heuristic method might converge prematurely and
thereby we can with the new exact method provide a more reliable approach.

The idea of using weighted training data, so-far, did not yield a strong advan-
tage in the observed test results. But on the other hand the results are also not
much worse, and we showed that the complexity of the mathematical program-
ming problem increases only slightly and it remains convex, and thus eﬃciently
solvable. Future work will need to be conducted to gain a better understanding
of how the clustering of data points may inﬂuence the result of the optimization,
and if alternative weighting schemes can be derived that take this eﬀect better
into account.

When it comes to applications, it will be beneﬁcial to test the approach on a
broader range of problems and machine learning methods. Our ﬁrst results on
heterogeneous data for drug property prediction show ﬁrst promising result in
that direction.

References

[1] E. Acar and M. Rais-Rohani. Ensemble of metamodels with optimized
weight factors. Structural and Multidisciplinary Optimization, 37(3):279–
294, 2009.

[2] T. Bartz-Beielstein and M. Zaeﬀerer. “Model-based methods for continuous
and discrete global optimization”. Applied Soft Computing, 55:154 – 167,
2017.

[3] J. M. Bates and C. W. J. Granger. The combination of forecasts. OR,

20(1):451–468, 1969.

[4] S. Ben´ıtez-Pe˜na, E. Carrizosa, V. Guerrero, M. D. Jim´enez-Gamero,
B. Mart´ın-Barrag´an, C. Molero-R´ıo, P. Ram´ırez-Cobo, D. R. Morales, and
M. R. Sillero-Denamiel. On sparse ensemble methods: an application to
short-term predictions of the evolution of covid-19. European Journal of
Operational Research, 295(1), 2021.

18

[5] A. P. Bento, A. Gaulton, A. Hersey, L. J. Bellis, J. Chambers, M. Davies,
F. A. Kr¨uger, Y. Light, L. Mak, S. McGlinchey, M. Nowotka, G. Papadatos,
R. Santos, and J. P. Overington. The chembl bioactivity database: an
update. Nucleic acids research, 42(D1):D1083–D1090, 2014.

[6] H.-G. Beyer and H.-P. Schwefel. Evolution Strategies: A Comprehensive

Introduction. Natural Computing, 1(1):3–52, 2002.

[7] C. M. Bishop. Neural Networks for Pattern Recognition. Oxford University

Press, New York, 1995.

[8] K. P. Burnham and D. R. Anderson. Model Selection and Multimodel
Inference - A Practical Information-Theoretic Approach. Springer, New
York, 2002.

[9] P. Echtenbruck, M. T. M. Emmerich, M. Echtenbruck, and B. Naujoks.
Optimally weighted ensembles in model-based regression for drug discovery.
In Congress on Evolutionary Computation (CEC), pages 2251–2258. IEEE
Press, 2021.

[10] P. Echtenbruck, M. T. M. Emmerich, and B. Naujoks. A multiobjective
approach to classiﬁcation in drug discovery. In Computational Intelligence
in Bioinformatics and Computational Biology (CIBCB), pages 1–8. IEEE
Press, 2019.

[11] M. T. M. Emmerich, K. C. Giannakoglou, and B. Naujoks. Single-and
multiobjective evolutionary optimization assisted by gaussian random ﬁeld
metamodels. IEEE Transactions on Evolutionary Computation, 10(4):421–
439, 2006.

[12] M. Friese, T. Bartz-Beielstein, and M. T. M. Emmerich. Building ensembles
of surrogates by optimal convex combination. In G. Papa and M. Mernik,
editors, Bioinspired Optimization Methods and their Applications, pages
131–143. Joˇzef Stefan Institute, Ljubljana, Slovenia, 2016.

[13] M. Gallagher and B. Yuan. “A general-purpose tunable landscape genera-
tor”. IEEE Trans. Evolutionary Computation, 10(5):590–603, (2006).

[14] M. Gallagher and B. Yuan. A general-purpose tunable landscape generator.
IEEE Transactions on Evolutionary Computation, 10(5):590–603, 2006.

[15] T. Goel, R. T. Haftka, W. Shyy, and N. V. Queipo. Ensemble of surrogates.
Structural and Multidisciplinary Optimization, 33(3):199–216, 2007.

[16] M. K. Kozlov, S. P. Tarasov, and L. G. Khachiyan. The polynomial solvabil-
ity of convex quadratic programming. USSR Computational Mathematics
and Mathematical Physics, 20(5):223–228, 1980.

[17] S. N. Lophaven, H. B. Nielsen, and J. Søndergaard. Dace - a matlab kriging

toolbox. Technical report, Technical University of Denmark, 2002.

19

[18] Martina Echtenbruck. Optimally weighted ensembles of surrogate models
for sequential parameter optimization. phdthesis, Leiden University, Leiden,
NL, 2020.

[19] M. D. McKay, R. J. Beckman, and W. J. Conover. A comparison of three
methods for selecting values of input variables in the analysis of output
from a computer code. Technometrics, 21(2):239–245, 1979.

[20] D. Rogers and M. Hahn. Extended-connectivity ﬁngerprints. Journal of

Chemical Information and Modeling, 50(5):742–754, 2010.

[21] W. J. Youden. Index for rating diagnostic tests. Cancer, 3(1):32–35, 1950.

20

