9
1
0
2

v
o
N
5
2

]

Y
S
.
s
s
e
e
[

1
v
9
8
7
0
1
.
1
1
9
1
:
v
i
X
r
a

A Neural Network Architecture to Learn
Explicit MPC Controllers from Data∗

E. T. Maddalena, C. G. da S. Moraes, G. Waltrich and C. N. Jones

Abstract

We present a methodology to learn explicit Model Predictive Control (eMPC) laws
from sample data points with tunable complexity. The learning process is cast in a
special Neural Network setting where the coeﬃcients of two linear layers and a para-
metric quadratic program (pQP) implicit layer are optimized to ﬁt the training data.
Thanks to this formulation, powerful tools from the machine learning community can
be exploited to speed up the oﬀ-line computations through high parallelization. The
ﬁnal controller can be deployed via low-complexity eMPC and the resulting closed-loop
system can be certiﬁed for stability using existing tools available in the literature. A
numerical example on the voltage-current regulation of a multicell DC-DC converter is
provided, where the storage and on-line computational demands of the initial controller
are drastically reduced with negligible performance impact.

Keywords: Explicit model predictive control, machine learning, data-driven control,
neural networks, power electronics.

1

Introduction

Model Predictive Control (MPC) is a valuable control methodology to be considered when-
ever performance needs to be maximized, possibly operating the system close to its physical
constraints. For certain classes of problems – e.g. when the dynamics and constraints are
aﬃne and the objective is quadratic – a closed-form solution exists, mapping the current
state to the control inputs through a continuous piecewise-aﬃne (PWA) function. This ap-
proach is known as explicit Model Predictive Control (eMPC) and is particularly employed
when the hardware where such a control law is to be implemented cannot solve the MPC
mathematical program on-line (either for time limitations or even safety regulations regard-
ing the ﬁnal application). Unfortunately, eMPC controllers might also suﬀer from having
high complexity due to the exponential growth of the number of regions in the worst case
[1], thus leading to not only high memory requirements but also high computational times.
Scaling down the memory footprint and computational burden of eMPC is possible
through a variety of techniques. Given a measurement of the current state, deciding in
which region of the polyhedral partition the system is can be eﬃciently accomplished us-
ing search trees, hash tables, among other data structures [2, 3, 4].
If some information
is known about the initial condition of the plant (even if only with respect to a subset of
the states), reachability arguments can be used to drop polytopic cells that are guaranteed
not to be visited during operation; this was recently studied in [5] and in [6]. Additional
region elimination schemes were presented in [7] and [8], where an interpolation strategy
and a backup control law were respectively deﬁned to be used in case the system visited a
state whose region had been eliminated. Finally, approximation methods for eMPC were
investigated using polynomial functions [9], multi-scale basis [10], as well as piecewise-aﬃne
functions [11].

The use of Neural Networks (NN) to learn nonlinear MPC controllers was studied in
[12], where one hidden layer and sigmoid activation functions were employed. The authors

∗This work has received support from the Swiss National Science Foundation under the RISK project

(Risk Aware Data-Driven Demand Response, grant number 200021 175627.

E. T. Maddalena and C. N. Jones are with ´Ecole Polytechnique F´ed´erale de Lausanne (EPFL), Switzer-
land (e-mails: emilio.maddalena@epfl.ch, colin.jones@epfl.ch). C. G. da S. Moraes and G. Waltrich
are with Universidade Federal de Santa Catarina (UFSC), Brazil (e-mails: caio.moraes@posgrad.ufsc.com,
gierri.waltrich@ufsc.br).

 
 
 
 
 
 
assumed a speciﬁc degree of regularity of the obtained approximator in terms of a bound
of an integral weighted by its Fourier decomposition coeﬃcients. It was then shown that
the ﬁnal NN approximation error can be bounded. More recently, [13] proposed training a
NN with rectiﬁed linear units by means of reinforcement learning instead of fully supervised
learning. The context was that of quadratic MPC for linear systems. Their approach was
shown to alleviate the training phase computations, especially since the system model can
be exploited at this stage. Nevertheless, no guarantees of closed-loop stability are given
or even tools to analyze it a posteriori – a rather common drawback of machine learning
approximators.

in implicit or explicit

form – with a particular

Contribution: Herein we propose a method to ﬁt datapoints sampled from an MPC con-
type of Neural
troller – either
Netowrk. The architecture is deﬁned by two linear layers and an implicit parametric
quadratic program (pQP) layer, which together are able to capture any linear MPC problem
with quadratic cost. In contrast with the majority of previous approaches such as [7, 8, 14],
the ﬁnal complexity is incremental and can be ﬂexibly adjusted. The resulting approxima-
tor can be deployed essentially as a new simpler eMPC controller, and closed-loop stability
can be certiﬁed a posteriori with tools already available in the literature. A case study in
the power electronics domain, where several successful implementations of eMPC have been
reported (see e.g. [15]), is presented to illustrate the potential of the proposed methodology.
Although the proposed simpliﬁcation technique gives up the original controller optimality,
our numerical investigations show that signiﬁcant complexity reduction is possible with little
to negligible performance degradation1.

Notation: Rn is the n-dimensional Euclidean space equipped with its usual norm. Given
a matrix A, its transpose is denoted by A(cid:48), and A (cid:31) ((cid:23)) 0 means it is positive-deﬁnite
A is deﬁned as ||v(cid:48)Av||2, I denotes an identity matrix of
(semideﬁnite). Furthermore, ||v||2
appropriate size, and 0 denotes a zero matrix of appropriate size. diag(a1, . . . , an) represents
a diagonal matrix with entries a1, . . . , an.

2 Learning Predictive Controllers

Consider the following standard MPC formulation for linear dynamical systems

P1 : min
X,U

H−1
(cid:88)

k=0

(cid:0)x(cid:48)

kQxk + u(cid:48)

kRuk

s.t. ∀k = 0, . . . , H − 1
xk+1 = Axk + Buk
xk ∈ X
uk ∈ U
xH ∈ XH
x0 = x(0)

(cid:1) + x(cid:48)

H P xH

(1a)

(1b)

(1c)

(1d)

(1e)

(1f)

(1g)

where X := {x1, . . . , xH }, U := {u0, . . . , uH−1}, Q (cid:23) 0, P (cid:23) 0, R (cid:31) 0, and the constraints
are all described by aﬃne equalities and inequalities. Denote by π : X → U the optimal
solution of (1) in its parametric form with respect to the initial conditions x(0), where
X ⊆ Rn is the feasible state space of P1 and U ⊆ Rm is the control space. We assume a set
of N samples can be acquired from the original control law2

D = {xi, ui}N

i=1

ui = π(xi), i = 1, . . . , N

(2a)

(2b)

The process of acquiring the training dataset does not have to follow a particular distribution,
nor do the samples have to be independent.

1All MATLAB scripts and python code can be downloaded from www.c4science.ch/source/QPFit.
2The dataset can also be directly obtained from the implicit controller; depending on the size of the

problem at hand, computing the explicit solution might be intractable.

2

Figure 1: An illustration of the NN eMPC controller approximator. The parameters to be
optimized are shown on the left-hand side.

2.1 The proposed architecture

The key idea behind the proposed approximator is the use of a parametric quadratic program
layer as part of the Neural Network, and optimizing over its parameters in order to ﬁt the
available dataset. This layer is implicitly described by the quadratic program

z(cid:63) = arg min
z≥0

||Lz + y1(x)||2 + (cid:15)||z||2

(3)

which is always feasible and bounded from below. The size of this mathematical program,
i.e. the dimension of z, can be tuned to attain approximations with diﬀerent complexity.
Moreover, as notation suggests, the parameter y1 depends on a previous aﬃne layer that
maps the system states into the z space, y1 := F x + f . Let y2 := z(cid:63), then another aﬃne
layer maps the optimal solution to the input space y3 := Gy2 + g, and a projection onto
the feasible input set produces the ﬁnal control action ˆu := Proj U(y2). This last step is
necessary to guarantee feasibility of the control moves (see e.g. [13]). An illustration of the
proposed architecture is presented in Fig. 1, where the projection layer was particularized
to a familiar element-wise saturation operation sat(·), valid for box input constraints.

Let the chosen number of decision variables in the pQP be z ∈ Rnz . We choose L ∈
Rnz×nz to be square, and therefore F ∈ Rnz×n, f ∈ Rnz . Moreover, G ∈ Rm×nz and
g ∈ Rm. If nz ≥ n, the ﬁrst layer lifts the input data into a higher dimensional space before
it is passed through the optimization layer. A last aﬃne function then projects it onto the
control space. These facts will be later employed to analyze the representative power of the
network.

The parameters to be trained are therefore F , f , L, G and g. This process can be carried
out via a stochastic gradient descent algorithm applied to an appropriate loss function.
Diﬀerentiability of all layers is trivial with the exception of the pQP one [16, 17]. Regarding
the later, note that the objective in (3) can be rewritten as

V (z) := z(cid:48)((cid:15)I + L(cid:48)L)z + (2L(cid:48)y1(x))(cid:48) z + y1(x)(cid:48)y1(x)

(4)

whose Lagrangian is simply

L(z, λ) = V (z) − λ(cid:48)z
The Karush-Kuhn-Tucker (KKT) conditions for primal and dual feasibility, complementary
slackness, and stationarity then read

(5)

z(cid:63) ≥ 0
λ(cid:63) ≥ 0
i z(cid:63)
λ(cid:63)
2((cid:15)I + L(cid:48)L)z(cid:63) + (2L(cid:48)y1(x)) − λ(cid:63) = 0

i = 0, ∀i = 1, . . . , nz

(6a)

(6b)

(6c)

(6d)

where λi and zi denote the components of the Lagrange multipliers and decision variables
vectors. The following proposition presents the diﬀerentiability properties of the pQP layer,
and holds since (3) is a particular instance of the OptNet layer [17] with strictly convex
objective function.

3

y1=F x+f     y3= Gy2+gy4  = sat(y3) u x Linear layer: F, fpQP layer: LLinear layer: G, gProjection layer:y2 = arg min ||Lz+y1 ||2+"||z||2z≥0^Proposition 1: Let θ := (L, y1). The parametric solution z(cid:63)(θ) of (3) is subdiﬀeren-
tiable everywhere in its domain, i.e., ∂z(cid:63)(θ) (cid:54)= { }, and ∂z(cid:63)(θ) has a unique element (the
jacobian) everywhere but in a set of measure zero.

As shown in [17], the relevant gradients with respect to the parameters to be trained can
be obtained from the KKT set of equations (6). Hence, backward passes are possible and
backpropagation can be performed to optimize all of the NN parameters.

2.2 Properties of the approximator

The authors of [18] showed that any continuous PWA function can be obtained as the
solution of a particular parametric linear program (pLP) transformed by a linear map.
Even though this view could be adopted herein, we instead prove a diﬀerent result that is
enough in the context of linear eMPC.

Theorem 1: (The proposed NN architecture can learn any linear quadratic MPC con-
troller) Let ˆπ : X → U be the map deﬁned by the composition of all four layers, i.e.,
ˆπ(x) := y4 ◦ y3 ◦ y2 ◦ y1(x). Set (cid:15) = 0, then ∃F , f , L, G and g with appropriate dimensions
such that ∀x ∈ X, ˆπ(x) = π(x).

Proof : Deﬁne the following parametric problem

P2 : min

U (cid:48) Λ U + x(0)(cid:48) Γ U

U
s.t. Φ U ≤ Ω x(0) + ω

(7a)

(7b)

obtained from P1 by using the equality constraints to eliminate all state decision variables
(see [19] for the construction of each matrix and vector). We also have that Λ (cid:31) 0. Problems
P2 and P1 are then equivalent in the sense that the solution U (cid:63) of P2 and {X (cid:63), U (cid:63)} of P1
have the same U (cid:63) component. Next, the dual problem of P2 can be shown to be

D2 : min
λ≥0

1
4

(cid:2)λ(cid:48)Φ Λ−1Φ(cid:48)λ + (4x(0)(cid:48)Ω(cid:48) + 2x(0)(cid:48)ΓΛ−1 Φ(cid:48)

· · · + 4ω(cid:48))λ + x(0)(cid:48)ΓΛ−1Γ(cid:48)x(0)(cid:3)

(8)

From the stationarity optimality condition of P2, it is possible to recover the primal optimal
solution U (cid:63) from the dual optimal solution λ(cid:63)

U (cid:63) = −0.5 Λ−1Φ(cid:48)λ(cid:63) − 0.5 Λ−1Γ(cid:48)x(0)

(9)

It would be possible to enforce the ﬁrst two layers to match the dual problem D2, and
the third to implement (9) and recover the primal solution. Nevertheless, this would require
the third layer to have a so called skip connection directly from the NN input, i.e., access
to x(0). We instead deﬁne slightly diﬀerent linear and pQP layers that not only learn D2,
but also let x(0) pass through them and arrive at the second linear layer.

Let the auxiliary variable ˜L and function ˜y1(x) := ˜F x + ˜f be the solution to (compare

(4) and (8))

˜L(cid:48) ˜L + (cid:15)I = 0.25 ΦΛ−1Φ(cid:48)
2 ˜L(cid:48) ˜y1(x) = Ω x(0) + 0.5 ΦΛ−1x(0) + ω

(10a)

(10b)

which leads to (cid:15) = 0, ˜L = 0.5 (Φ˜Λ)(cid:48), where ˜Λ is the unique square root of Λ−1, guaranteed
to exist as Λ−1 (cid:31) 0. Then, ˜y1(x) = (Φ˜Λ)−1(Ω x(0) + 0.5 ΦΛ−1x(0) + ω) =⇒ ˜F =
(Φ˜Λ)−1(Ω + 0.5 ΦΛ−1), ˜f = (Φ˜Λ)−1ω.

Set the ﬁrst layer weights to F = [−I I ˜F ](cid:48) and f = [0 0 ˜f ](cid:48) so that y1(x) = [−x; x; ˜F x+
˜f ]. Set the weights of the pQP layer (3) to (cid:15) = 0 and L = [I 0 0; 0 I 0; 0 0 ˜L]. If we
partition the decision variable as z = [˜z xp xn](cid:48), this results in

min
˜z,xp,xn≥0

||xp − x(0)||2 + ||xn + x(0)||2 + || ˜L˜z + ˜y1(x)||2

(11)

which is a separable objective in ˜z, ˜xp and ˜xn. Due to the choice of ˜L and ˜y1(x) in (10),
the last term of the pQP matches the dual D2 with the exception of its constant term – not

4

relevant for determining the optimal solution. Therefore, we have that ˜z(cid:63) in (11) matches
λ(cid:63) in D2. Regarding xp(cid:63), the n optimizer components will satisfy ∀i = 1, . . . , n, xp(cid:63)
i = xi(0)
if xi(0) ≥ 0, else xp(cid:63)
i = 0. Therefore,
xp(cid:63) − xn(cid:63) = x(0).

i = −xi(0) if xi(0) ≤ 0, else xn(cid:63)

i = 0. Similarly, xn(cid:63)

Next set the weights of the second linear layer y3 to G = [−0.5Λ−1Φ(cid:48) −0.5Λ−1Γ(cid:48) 0.5Λ−1Γ(cid:48)]
and g = 0. Therefore, y3 = G [˜z(cid:63) xp(cid:63) xn(cid:63)](cid:48) = G [λ(cid:63) xp(cid:63) xn(cid:63)](cid:48) = U (cid:63), where equality (9) was
used in the last step. Finally, note that the last layer y4 = ProjU(y3) will simply evaluate
to y3 since y3 is the optimal primal solution U (cid:63), which satisﬁes the constraints (7b) and
necessarily belongs to U. The theorem then follows from the fact that x(0) in the above
calculations can be taken to be any point x in X. (cid:3)

Exactly matching the original MPC controller would require L to have the same size of
Φ˜Λ and (cid:15) = 0 as shown. Nevertheless, we are interested precisely in reducing the complexity
of the resulting controller through employing less parameters. In this process, choosing a
regularizer (cid:15) > 0 is beneﬁcial since it ensures that the QP is bounded during the training
phase for all possible parameters.

2.3 Stability certiﬁcation

After the NN has been trained and its weights have been deﬁned, closed-loop stability in
the sense of Lyapunov can be veriﬁed through sum-of-squares (SoS) programming. First
note that the NN is a composition of linear maps and optimization problems. The central
idea is that the forward pass involves calculating the optimal solution to these mathematical
programs, which necessarily satisfy their respective KKT conditions, e.g. (6). Furthermore,
these conditions are sets of polynomial inequalities in the primal-dual lifted space; hence,
the control moves are deﬁned by polynomial and linear functions. As the system being
controlled is linear, the certiﬁcation technique presented in [20] can be readily applied.

2.4 Controller deployment

The ﬁnal approximated control law u = ˆπ(x) is evaluated by a forward pass in the NN. If
the ﬁrst and second linear layers are incorporated respectively into the pQP and projection
ones, this can be done by solving the two (simpler) optimization problems on-line. Alterna-
tively, two explicit PWA solutions can be calculated oﬀ-line with a bound on the number of
regions dependent on the chosen size nz. If the constraints u ∈ U are simply a box – as in
many practical applications – an element-wise saturation is the closed-form solution to the
projection layer u = y3 = ProjU(y2), further simplifying the ﬁnal controller. We illustrate
the use of the proposed technique in a power electronics context in the next section.

Any discrepancy between ˆπ(x) and π(x) in the region that contains the origin will result
in steady-state errors. Several techniques can be used to overcome this issue. As discussed
in [12], the LQR solution associated with (1) can be stored and used whenever the system
is inside the associated invariant set, where no constraints are active. Alternatively, an
external disturbance estimator could be adopted to account for the optimal-approximate
controller mismatch.

3 Control of a step-down converter

In order to enhance readability and stay consistent with the standard circuits convention,
notation will be overloaded.

3.1 Analysis and controller design

Parallelism is a key concept to increase the eﬃciency and power levels of electronic con-
verters. Still, this design choice has to be followed by proper current/voltage balancing
techniques to ensure that no stage is subjected to a higher electrical stress when compared
to the others.

A schematic representation of a multicell step-down converter is shown in Fig. 2, and its
parameters can be found in Table 1. The topology features three arms that are connected
to a coupled inductor, and an L-C output ﬁlter. All self inductances are assumed equal

5

Table 1: DC-DC converter parameters

R
Vin
350 V 4 mH -2 mH 10 mΩ 270 µH 20 µF 6.25 Ω

Lm

Ro

Co

Lf

Ls

Figure 2: Circuit diagram of the multicell step-down DC-DC converter.

L1 = L2 = L3 = Ls, and all mutual inductances have value Lm. The switches of each arm
operate in a complementary fashion at a ﬁxed frequency f = 15 kHz, and with variable but
constrained duty cycle 0 ≤ di ≤ 0.9, i = 1, 2, 3. Let the average voltage applied by the
arms over one switching period be denoted by vi := diVin, i = 1, 2, 3. In order to ease the
analysis, we apply the Lunze transform Ψ to all variables, decomposing the phase voltages
and currents into diﬀerential and common mode components

(cid:2)idm1
(cid:2)vdm1

idm2

icm

vdm2

vcm

(cid:3)(cid:48)

(cid:3)(cid:48)

:= Ψ (cid:2)i1
:= Ψ (cid:2)v1

i2

v2

(cid:3)(cid:48)

i3

(cid:3)(cid:48)

v3

(12)

(13)

where Ψ = (1/3) [2 − 1 − 1; −1 2 − 1; 1 1 1].

The control input is deﬁned as u := [vdm1 vdm2 vcm](cid:48) and the continuous-time state vec-
tor, by appending the output voltage to the transformed currents x := [idm1 idm2 icm vout](cid:48).
By using Kircchoﬀ’s circuit laws, a linear model of the form ˙x = Actx + Bctu can be derived
with

Act =








−R
Ls+Lm
0
0
0

0
−R
Ls+Lm
0
0

0
0
−R
Ls+2Lm+3Lf
3
Co

0
0
−1
Ls+2Lm+3Lf
−1
RoCo








Bct =







1
Ls
0
0
0

0
1
Ls
0
0

0
0
1
Ls+2Lm+3Lf
0







(14)

(15)

Finally, discretization at frequency f is carried out using the zero-order hold method, yielding
xk+1 = Axk + Buk.

The control goal is to regulate the output voltage vout to 300 V while maintaining the
phase currents balanced at all times, which translates to driving the diﬀerential currents to
zero. More speciﬁcally we have the following ﬁxed reference xeq = [0 0 16 300](cid:48) with ueq =
B†(I − A)xeq, where B† is the pseudo-inverse of B. Moreover, the controller approximation
procedure must not incur a steady-state error larger than 200 mA for idm1 and idm2, and
5% for the common mode component icm and output voltage vout. The chosen MPC cost
function was

H−1
(cid:88)

J =

(||xk − xeq||2

Q + ||uk − ueq||2

R) + ||xN − xeq||2
P

(16)

k=0

where Q = diag(10, 10, 0.1, 0.1), R = 0.1 I, P is the solution to the associated the discrete-
time algebraic Riccati equation, and H = 10. For all time instants, box state constraints

6

Q1Q1Q2Q2Q3Q3L2VinRRR___L1L3LfvoutCoRoi2i1i3Figure 3: Neural network training loss as a function of the pQP layer size, and storage
requirements associated with their PWA representations.

≤ xk ≤ (cid:2)5

were imposed (cid:2)−5 −5 −10 −20(cid:3)(cid:48)
and polyhedron constraints
on the controls Hu uk ≤ hu that simply mapped the duty cycle saturation to the Lunze
domain. Due to the polytopic input constraints, the system cannot be decomposed into
three decoupled parts as the structure of matrices Act and Bct suggest. Furthermore, the
standard terminal set constraint was imposed on xH , deﬁned as the invariant set associated
to the unconstrained inﬁnite-time problem formulation.

400(cid:3)(cid:48)

30

5

3.2 Learning the optimal controller

With the aid of the Multi-Parametric Toolbox (MPT) [21], the optimal eMPC solution π(x)
was calculated and consisted of 2’337 critical regions. By counting the number of parameters
necessary to describe each halfspace and control gain, the memory requirement of this PWA
function was found to be 518 kB. In the previous calculation, a 4 byte representation was
considered for both integers and ﬂoating point numbers.

Next, 5’000 samples were randomly acquired from the eMPC controller using a uniform
distribution. The ﬁrst and second components of the sampled control moves had consider-
ably smaller amplitudes compared to the third due to the structure of the Lunze transform
Ψ. The dataset labels {ui}5000
i=1 had therefore to be scaled to ensure a similar learning of
all control components. Moreover, instead of ProjU(y3), the last NN layer was simpliﬁed to
y4 = Ψ sat(y3) with saturation limits 0 and 0.9 Vin. This clearly guarantees control feasibil-
ity without the need of a second quadratic program. NN approximators were trained using
PyTorch and the OptNet framework [17]. A mean squared error loss function was minimized
by employing the Adam algorithm, mini-batch stochastic gradient descent with batch size
50, and 150 epochs. The size nz of the pQP layer was varied from 1 to 7 and a total of
10 models were trained for each size; the lowest obtained losses are shown in Figure 3. On
average, training a model required 42 minutes on a 3.1 GHz Intel Core i7 machine with-
out GPU acceleration, and 23 minutes with a single NVIDIA Tesla T4 graphics card. The
learned parameters were then exported to MATLAB in order to calculate the PWA solution
of the pQP layer.

An increase in the nz size clearly expands the representation capabilities of the neural
network. This however does not always translate to a decrease in the ﬁnal loss since the
training process is aﬀected by the weights initialization among other factors. Although not
monotonic, we see a decrease of the overall training loss in Figure 3 as nz grows. In order to
validate the models, the start-up response of the converter was analyzed under all 7 diﬀerent
approximate controllers, and only the two largest ones (nz = 6 and nz = 7) met the target
speciﬁcations given in Section 3.1. We refer to these two solutions as the viable learned
controllers. Slices of their control surfaces are shown in Figure 43, and a phase portrait
of the closed-loop system evolution over 50 steps starting from four initial conditions is
depicted in Figure 5. A summary of the two viable learned controller features is presented
in Table 2, including the number of polytopic regions, the storage requirements, the worst-

3The vout axis was extended to -100 V for visualization purposes.

7

Mean squared error loss10-410-210-31234567Size nz of the NN pQP layer34 kB17 kB8 kB3 kB1 kB1 kB0.29 kBOptimalLoss = 0Size = 518 kBFigure 4: Slices of the optimal eMPC controller π(x) and several PWA NN approximations
ˆπ(x). The left plots are associated to vcm and the right plots, to vdm1.

case computation time4 and the steady state (SS) error for icm and vout – both clearly
always equal. Even though four initial states were given, the systems always converged to
the same points and, hence, only one SS error is reported. Plus, the storage numbers also
take into account all the remaining layers parameters. Analyzing the obtained results we see
that the approximations drastically reduced the storage requirements by 93.4% and 96.7%
and sped up the worst-case evaluation time by 83.7% and 88.4%, respectively for the nz = 7
and nz = 6 cases. The closed-loop trajectories with the proposed ˆπ(x) remained reasonably
close to the scenario with the optimal π(x), converging to nearby equilibrium points. In
practice, steady-state errors could be removed by using the tools mentioned in Section 2.4.

4Before proceeding to implementation, a further speed up is possible through the methods listed in the

Introduction.

8

vcm400200250150300100350vout-1000100200300icm-100102030vcm400200250150300100350vout-1000100200300icm-100102030vcm400200250150300100350vout-1000100200300icm-100102030vcm400200250150300100350vout-1000100200300icm-100102030vcm400200250150300100350vout-1000100200300icm-100102030vdm1-2010-3030-50-5-2.502.55idm1idm2vdm1-2010-3030-50-5-2.502.55idm1idm2vdm1-2010-3030-50-5-2.502.55idm1idm2vdm1-2010-3030-50-5-2.502.55idm1idm2vdm1-2010-3030-50-5-2.502.55idm1idm2π(x), nz= 7^π(x), nz= 7^π(x), nz= 6^π(x), nz= 6π(x), nz= 2^π(x), nz= 2π(x), nz= 1^π(x), nz= 1^^^......π(x)Optimal solutionπ(x)Optimal solution-5-2.502.55-5-2.502.55-5-2.502.55-5-2.502.55-5-2.502.55Table 2: Optimal and viable learned controllers information
SS error
0%
0.59%
1.25%

Storage Comp. time
518 kB
34 kB
17 kB

π(x)
ˆπ(x), nz = 7
ˆπ(x), nz = 6

Regions
2’337
107
56

12.9 ms
2.1 ms
1.5 ms

Figure 5: Output voltage and common mode current phase portraits when employing the
optimal controller and the two viable approximations.

4 Conclusion

We proposed a novel method to approximate explicit MPC controllers from samples of the
optimal control law. The essence of the approach is a Neural Network architecture featuring
a pQP layer incorporated to learn the MPC dual problem.
It was shown that, with an
appropriate pQP size, it is possible to learn any linear quadratic MPC exactly. After its
training, an equivalent representation of the NN can be found oﬀ-line as a new and simpler
controller. Steady-state errors due to approximation inaccuracies can be mitigated by em-
ploying standard tools. A numerical example was provided where the storage requirements
and computational burden of the approximate PWA controllers were signiﬁcantly lower than
their optimal counterpart with minor performance degradation.

References

[1] A. Alessio and A. Bemporad, “A survey on explicit model predictive control,” in Non-

linear model predictive control. Springer, 2009, pp. 345–369.

[2] C. N. Jones, P. Grieder, and S. V. Rakovi´c, “A logarithmic-time solution to the point
location problem for parametric linear programming,” Automatica, vol. 42, no. 12, pp.
2215–2218, 2006.

[3] F. Bayat, T. A. Johansen, and A. A. Jalali, “Using hash tables to manage the time-
storage complexity in a point location problem: Application to explicit model predictive
control,” Automatica, vol. 47, no. 3, pp. 571–577, 2011.

[4] M. M¨onnigmann and M. Kastsian, “Fast explicit MPC with multiway trees,” IFAC

Proceedings Volumes, vol. 44, no. 1, pp. 1356–1361, 2011.

9

icmvout302520151050-5050100150200250300350400π(x)π(x), nz= 7^π(x), nz= 6^[5] E. T. Maddalena, R. K. H. Galv˜ao, and R. J. M. Afonso, “Robust region elimination

for piecewise aﬃne control laws,” Automatica, vol. 99, pp. 333–337, 2019.

[6] M. Kvasnica, P. Bakar´aˇc, and M. Klauˇco, “Complexity reduction in explicit MPC: A

reachability approach,” Systems & Control Letters, vol. 124, pp. 19–26, 2019.

[7] J. A. Rossiter and P. Grieder, “Using interpolation to improve eﬃciency of multipara-

metric predictive control,” Automatica, vol. 41, no. 4, pp. 637–643, 2005.

[8] F. J. Christophersen, M. N. Zeilinger, C. N. Jones, and M. Morari, “Controller com-
plexity reduction for piecewise aﬃne systems through safe region elimination,” in 2007
46th IEEE Conference on Decision and Control, 2007, pp. 4773–4778.

[9] M. Kvasnica, J. L¨ofberg, and M. Fikar, “Stabilizing polynomial approximation of ex-

plicit MPC,” Automatica, vol. 47, no. 10, pp. 2292–2297, 2011.

[10] S. Summers, C. N. Jones, J. Lygeros, and M. Morari, “A multiresolution approximation
method for fast explicit model predictive control,” IEEE Transactions on Automatic
Control, vol. 56, no. 11, pp. 2530–2541, 2011.

[11] C. N. Jones and M. Morari, “Polytopic approximation of explicit model predictive
controllers,” IEEE Transactions on Automatic Control, vol. 55, no. 11, pp. 2542–2553,
2010.

[12] T. Parisini and R. Zoppoli, “A receding-horizon regulator for nonlinear systems and a

neural approximation,” Automatica, vol. 31, no. 10, pp. 1443–1451, 1995.

[13] S. Chen, K. Saulnier, N. Atanasov, D. D. Lee, V. Kumar, G. J. Pappas, and M. Morari,
“Approximating explicit model predictive control using constrained neural networks,”
in 2018 Annual American Control Conference (ACC), 2018, pp. 1520–1527.

[14] M. Kvasnica and M. Fikar, “Clipping-based complexity reduction in explicit MPC,”

IEEE Transactions on Automatic Control, vol. 57, no. 7, pp. 1878–1883, 2011.

[15] S. Mari´ethoz and M. Morari, “Explicit model-predictive control of a PWM inverter
with an LCL ﬁlter,” IEEE Transactions on Industrial Electronics, vol. 56, no. 2, pp.
389–399, 2008.

[16] S. Gould, B. Fernando, A. Cherian, P. Anderson, R. S. Cruz, and E. Guo, “On dif-
ferentiating parameterized argmin and argmax problems with application to bi-level
optimization,” arXiv preprint arXiv:1607.05447, 2016.

[17] B. Amos and J. Z. Kolter, “Optnet: Diﬀerentiable optimization as a layer in neural
networks,” in Proceedings of the 34th International Conference on Machine Learning-
Volume 70, 2017, pp. 136–145.

[18] A. B. Hempel, P. J. Goulart, and J. Lygeros, “Every continuous piecewise aﬃne function
can be obtained by solving a parametric linear program,” in 2013 European Control
Conference (ECC), 2013, pp. 2657–2662.

[19] S. J. Wright, “Eﬃcient convex optimization for linear MPC,” in Handbook of Model

Predictive Control. Springer, 2019, pp. 287–303.

[20] M. Korda and C. N. Jones, “Stability and performance veriﬁcation of optimization-

based controllers,” Automatica, vol. 78, pp. 34–45, 2017.

[21] M. Herceg, M. Kvasnica, C. N. Jones, and M. Morari, “Multi-parametric toolbox 3.0,”

in 2013 European control conference (ECC), 2013, pp. 502–510.

10

