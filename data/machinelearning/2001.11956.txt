Documentation of Machine Learning Software 

Yalda Hashemi∗, Maleknaz Nayebi∗, Giuliano Antoniol∗
∗Dept. computer and software engineering
Ecole Polytechnique de Montreal, Canada 

{yalda.hashemi, maleknaz.nayebi, giuliano.antoniol}@polymtl.ca 

Abstract—Machine  Learning  software  documentation  is  dif- 
ferent  from  most  of  the  documentations  that  were  studied  in 
software engineering research. Often, the users of these documen- 
tations are not software experts. The increasing interest in using 
data science and in particular, machine learning in different fields 
attracted scientists and engineers with various levels of knowledge 
about programming and software engineering. Our ultimate goal 
is  automated  generation  and  adaptation  of  machine  learning 
software  documents  for  users  with  different  levels  of  expertise. 
We are interested in understanding the nature and triggers of the 
problems  and  the  impact  of  the  users’  levels  of  expertise  in  the 
process of documentation evolution. We will investigate the Stack 
Overflow  Q&As  and  classify  the  documentation  related  Q/As 
within the machine learning domain to understand the types and 
triggers of the problems as well as the  potential change requests 
to the documentation. We intend to use the results for building on 
top of the state of the art techniques for automatic documentation 
generation  and  extending  on  the  adoption,  summarization,  and 
explanation of software  functionalities. 

Index Terms—Software Engineering, Machine Learning, Soft- 

ware Documentation, Mining Software Repositories. 

I. INTRODUCTION

Documentation  is  meant  to  guide  users  about  software 
functionality. Documentation is “any artifact which its purpose 
is  to  communicate  information  about  the  software  system  to 
which it belongs, to individuals involved in the production  of 
that  software"  [2].  State  of  the  art  of  software  engineering 
discussed  methods  for  automated  generation  of  software 
documentation, for example, API descriptions. 

The increasing growth in the use of machine learning  (ML) 
in a  variety  of  domains  requires  data scientists  to  familiarize 
themselves with the code and software structure. This says, for 
proper  use  of  ML  techniques,  a  data  scientist  in  biomedical  
or  climate  engineering  should  be  able  to  ideally  refer  to  the 
software documentation to understand how to use the product 
(for  example,  an  API  in  TensorFlow).  However,  the  level  of 
technicality in current software documentation urges the users 
to look for additional resources continuously. These questions 
often find their way to StackOverflow (SO). 

Our research is intended to provide the users of ML software 
with clear and thorough information about the software product 
in a way being understandable in consideration of their level of 
expertise [5]. We are intent to ultimately adopt state of the art 
methods for automated construction of software documentation 
to  develop  expertise-aware  software  documentation  for  ML 
products. We answer three research questions: 

RQ1: What type of problems do users face when using 

documentation of ML software? 

RQ2: How does the expertise level of users impact their 

understandings of the documentation of ML software? 
RQ3: How does documentation evolve in relation to the SO 

questions? 

II. METHODOLOGY

A. RQ1: Nature and triggers of ML documentation problems

SO Q/As has been a common source for researchers to under- 
stand developers’ problems and pain points, in general, and in 
particular with ML software tools [1], [3]. Being interested in 
understanding the current usefulness of documentation related 
to ML software, we mine the SO questions to identify: 

What types of problems do users have with ML software, and 

in what areas? 

Which types of documentations are used and questioned? 
Why the documentations are referred to, and in which area? 
How the documentation referrals happen? 

Figure  1  shows  a  sample  question  from  SO  and  the  anno- 
tation  showing  what  the  problem  is,  which  documentation  is 
used and how it has been referred to. To systematically answer 
these questions, we start  by  gathering  the  SO  Q/As tagged  as 
ML  or  popular  ML  libraries  such  as  Tensorflow  or  Pytorch. 
We then manually annotate a representative sample of this data 
as  being  related  or  unrelated  to  the  software  documentation, 
using at least two annotators. Having the questions identified as 
documentation related, we perform thematic analysis to identify 
what, why, how, and which questions by manual analysis of the 
categories using combination of open and closed card sorting. 
The  results would  provide  a  taxonomy  of problems  and their 

Mo d el  cr eat io n  (What?) 

Hy p erlin k  r ef er en ce (How?) T en so rF lo w  gu id e (Which?) 

Rep li catin g  ex amp le  in  o th er d o m ain  (Why?) 

Fig. 1. Example question and the information being analyzed in this study 

This is author's version of the paper accepted to the 27th IEEE International Conference on Software 
Analysis, Evolution and Reengineering (SANER 2020), London, Ontario

 
StackOverflow 
Q/As 

RQ1 

Manual annota- 
tion of questions 

Open  card sorting to  
define what, which, 
why &  how categories 

  Close card sorting 
to identify frequency 
of each  category 

D o cu m e nt .  
un rel at ed  
q u estio ns  

D o cu m e nt .  
rel at ed  
q u estio ns  

RQ2 

RQ3 

Retrieving questioners 
profile & reputation 

R el e as es ,  
issues  
&  c o m mits 

        Associating questions 

time stamps to 
software release dates 

re positor y  of  M L  
so ftw a r e  

Clustering to identify 
expertise groups 

Statistical analysis 
for problem spec- 
ification for each 
expertise group 

Calculating cosine 
similarity between ques- 
tions, issues, & commits 

Analyzing the at- 
tributes of questions 
which triggered changes 
and the ones not 

Fig. 2. 

Methodology for analyzing documentation evolution in ML software 

attributes. To answer the “What" question with regards to the 
ML problems, we use the categories introduced in [3]. 

B.  RQ2:  the  impact  of  level  of  expertise  of  users  on  their 
understanding from ML software  documentation 

We are interested in evaluating if developers understanding of 
software and the domain (here ML) impacts their question types. 
Learning  this,  we  intend  to  ultimately  adapt  the  software 
documentation for different user groups and their information 
needs.  To    this  aim,  we  use  the  factors  introduced  by 
Movashovitz       et  al.  [4]  (the number of up voted questions, 
number  of  up  voted  answers,  number  of  accepted  answers, 
number  of  down  voted  answers).  To  identify  users  level  of 
expertise in ML, we also consider the number of posts tagged 
in 
tensorflow  or 
machinelearning). Having this, we perform clustering using k-
means  (identifying  k with  the elbow  method). As  for  the last 
stage,  we  perform  statistical  test  to  compare  groups  with 
different  levels  of  expertise  and  reason  out  the  extent  of 
difference between their needs  from  software documentation. 
Figure 2 shows the main steps of this process. 

(for  example, 

to  ML 

relation 

analyze  the  similarity  between  the  commit  messages  and  the 
questions. Tracing a question to the documentation changes, we 
look into the question attributes in RQ1 and RQ2 to understand 
the co-evolution of users understandings and documentations. 

III.  SUMMARY AND FUTURE  WORK 

We  intend  to  automate  the  generation  of  software  docu- 
mentation  in  the  ML  domain  in  consideration  of  different 
levels of user  expertise. To  achieve that, we take initial steps 
by  understanding  users  behavior  in  using  ML  documentation 
and  understanding  the  process  of  evolution  of  the  software 
documents in ML domain. 

So far, we took steps toward answering RQ1 and performed 
a  preliminary  study  on  “TensorFlow"  as  one  of  the  popular 
ML  libraries  [3]  (TensorFlow  has  48,122  questions  on    SO). 
By randomly  sampling 500 questions  with  the  tensorflow tag 
and  manually  categorizing the  questions, we  found  16.6% of 
these questions are related to documentation.  We performed a 
two annotator light-weighted card sorting process to check our 
hypothesis of RQ1. We observed that SO questions are mainly 
concerned  with  parameter  tuning  (22.6%),  model  creation 
(14.2%), and error/exception (10.7%) when categorized along 
with ML domain as defined by Islam et al. [3] (what). Most of 
the questions have been triggered as the users were not able to 
replicate the examples provided in the documentation (24.8%) 
and  12.9%  were  concerned  with  the  lack  of  description  on    
the implementation  and use of the software  in  documentation 
(why). Also, we saw 60.7% of these documentations are related 
to  the  official  TensorFlow  documentation,  while  others  refer  
to  third  party  material  such  as  tutorials,  videos,  books  and 
scientific  papers  (which).  We  also  observed  that  the  majority  
of  these  questions  (72.8%)  hyperlinked  to  the  mentioned 
documentation, while several others  used  a  screenshot  or  just 
mentioned the name of the documentation. We extend the study 
by enlarging the sample size and across other popular libraries, 
including “TensorFlow" and “Pytorch". The result of this study 
will provide us  with  the knowledge  to develop,  simplify,  and 
summarize  the  documentation  for  a  better  understanding  of   
its users and later developing an automated tool for  expertise- 
aware software documentation for ML products. 

C.  RQ3: Co-evolution of documentation and questions 

REFERENCES 

To  understand  if  and  to  what  extent  the  raised  questions 
triggered a  change in the documentation of  ML software,  we 
connect  the  questions  to  the  opened  issues  in  the  respective 
software repository and the commit messages while changing 
the  documentation1.  In this  RQ, we are interested  to see if  a 
question from SO has triggered a change in the documentation 
or  not  and  if  so  what  is  the  characteristics  of  these  types  of 
questions. The process is shown in Figure 2. First, we associate 
each question to a release of a software assuming that a valid 
question that is asked before Releasei can not be responded 
in  Releasej,  where  j  <  i.  After,  we  calculate  the  cosine 
similarity between the text of the SO question and the issues in 
the repository. For those questions not matching any issues, we 

1ML software often have document repository for example https://github. 

com/tensorflow/docs. 

[1]  A.  A.  Bangash,  H.  Sahar,  S.  Chowdhury,  A.  W.  Wong,  A.  Hindle,  and 

K.  Ali.    What  do  developers  know  about  machine  learning:  a  study  of   
ml discussions on stackoverflow. In International Conference on Mining 
Software Repositories, pages 260–264. IEEE Press, 2019. 

[2]  A. Forward and T. C. Lethbridge. The relevance of software documentation, 
tools  and  technologies:  a  survey.  In  ACM  symposium  on  Document 
engineering, pages 26–33. ACM, 2002. 

[3]  M.  J.  Islam,  H.  A.  Nguyen,  R.  Pan,  and  H.  Rajan.  What  do  developers  
ask  about  ml  libraries?  a  large-scale  study  using  stack  overflow.  arXiv 
preprint arXiv:1906.11940, 2019. 

[4]  D. Movshovitz-Attias, Y. Movshovitz-Attias, P. Steenkiste, and C. Falout- 
sos.  Analysis  of  the  reputation  system  and  user  contributions  on  a 
question answering website: Stackoverflow. In  International Conference 
on  Advances  in  Social  Networks  Analysis  and  Mining,  pages  886–893. 
ACM, 2013. 

[5]  M. Nayebi, G. Ruhe, R.  C. Mota, and M. Mufti. Analytics for software 
project  management–where  are  we  and  where  do  we  go?  In  30th 
IEEE/ACM International Conference on Automated Software Engineering 
Workshop (ASEW), pages 18–21. IEEE, 2015. 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
