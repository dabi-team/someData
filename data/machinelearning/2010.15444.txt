1
2
0
2

t
c
O
5
2

]

C
D
.
s
c
[

2
v
4
4
4
5
1
.
0
1
0
2
:
v
i
X
r
a

Advanced Python Performance
Monitoring with Score-P

Andreas Gocht

, Robert Schöne, and Jan Frenzel

Abstract Within the last years, Python became more prominent in the
scientiﬁc community and is now used for simulations, machine learning,
and data analysis. All these tasks proﬁt from additional compute power
oﬀered by parallelism and oﬄoading. In the domain of High Performance
Computing (HPC), we can look back to decades of experience exploiting
diﬀerent levels of parallelism on the core, node or inter-node level, as well
as utilising accelerators. By using performance analysis tools to investigate
all these levels of parallelism, we can tune applications for unprecedented
performance. Unfortunately, standard Python performance analysis tools
cannot cope with highly parallel programs. Since the development of such
software is complex and error-prone, we demonstrate an easy-to-use solution
based on an existing tool infrastructure for performance analysis. In this
paper, we describe how to apply the established instrumentation framework
Score-P to trace Python applications. We ﬁnish with a study of the overhead
that users can expect for instrumenting their applications.

Key words: python, tools, performance analysis, Score-P

Andreas Gocht
Center for Information Services and High Performance Computing (ZIH), Technische Uni-
versität Dresden, 01062 Dresden, Germany e-mail: andreas.gocht@tu-dresden.de

Robert Schöne
Center for Information Services and High Performance Computing (ZIH), Technische Uni-
versität Dresden, 01062 Dresden, Germany e-mail: robert.schoene@tu-dresden.de

Jan Frenzel
Center for Information Services and High Performance Computing (ZIH), Technische Uni-
versität Dresden, 01062 Dresden, Germany e-mail: jan.frenzel@tu-dresden.de

1

 
 
 
 
 
 
2

Andreas Gocht

, Robert Schöne, and Jan Frenzel

1 Introduction

Python is one of the Top 5 programming languages1, and it is not surprising
that more and more scientiﬁc software is written in Python. But the standard
implementation CPython interprets Python source code, rather than compiling
it. Hence, it is deemed to be less performant than other programming languages
like C or C++. Moreover, as CPython employs a Global Interpreter Lock
(GIL) [9], it is often stated that Python does not support parallelism. While
there are diﬀerent Python implementations like pypy2 or IronPython3, which
try to counter these drawbacks, these approaches do not represent the standard
implementation.

However, CPython is easily extensible, e.g., by using its C-API or foreign
function interfaces. These interfaces allow programmers to exploit the paral-
lelism of a problem with traditional programming languages like C without
losing the ﬂexibility and the power of the standard Python implementation.
Moreover, it is possible to oﬄoad computation to accelerators like graphic
cards. Nevertheless, these extensions and the Python source code itself need
to be optimised to exploit the full performance of a computing system. To
optimize the application, it has to be monitored. To monitor the application,
performance-related information has to be collected and recorded.

While collecting performance information is possible to some extent with
tools that are part of the standard Python installation, none of these tools
makes it easy to gain knowledge about the eﬃciency of thread parallel, process
parallel, and accelerator-supported workloads. However, such tools exist for
traditional programming languages used in High Performance Computing
(HPC). Here, Score-P [8], Extrae [16], TAU [15], and others allow users to
record the performance of their applications and analyze them with scalable
interfaces.

In this paper, we present the Python bindings for Score-P, which make it
easy for users to trace and proﬁle4 their Python applications, including the
usage of (multi-threaded) libraries, MPI parallelism and accelerator usage. The
paper is structured as follows: We describe our concept and implementation in
Section 2 and evaluate the overhead in Section 3. We present related work in
Section 4 and ﬁnalize this paper with a conclusion and an outlook in Section 5.

1 According to the TIOBE Index Oktober 2019: https://www.tiobe.com/tiobe-index/
2 https://pypy.org/
3 https://ironpython.net/
4 As deﬁned in [6, Section 2]

Advanced Python Performance Monitoring with Score-P

3

2 The Score-P Python Bindings

The Python module, which is used to invoke Score-P and allows tracing and
proﬁling of Python code, is called Score-P Python bindings. The module can be
split into three basic blocks, which are used in two phases: The initialisation,
which is executed in a preparation phase, prepares the measurement and
executes the application. The instrumenter is registered with the Python
instrumentation hooks and used during execution. The Score-P C-bindings
connect Python with C and Score-P and are also used during execution. The
workﬂow of the overall process, including preparation phase and the execution
phase, is depicted in Figure 1.

Fig. 1: Overview of the instrumentation process with Score-P. In the ﬁrst
phase, the Score-P Python module initializes the Score-P measurement system
and attaches Score-P libraries. In the second phase, the bindings use the
preloaded Score-P libraries and instrument the Python code to record events
with Score-P. In addition to the Python instrumentation, other parts of the
application, such as MPI, pthreads, and CUDA functions, are automatically
instrumented by Score-P (not depicted).

Python Instance (scorep -mpp=mpi ...)Score-P ModulePython Instance (scorep … ./run.py)Dependencies (MPI, pthread)./run.pycreatecompilepython -m scorep  --mpp=mpi –thread=pthread ./run.pympi4pyscipycallcallwritepreloadPhase 1: PreparationPhase 2: Execution, RecordingShared Library ObjectInitialisationC FileMKLcallScore-P ModuleRegisterinstrumenterstartScore-P User Instrumen-tationWrapped pthread libraryWrapped MPI librarydummysettracesetprofileShared Library Object:Translation to Score-P interfaceOTF2CubeX4

Andreas Gocht

, Robert Schöne, and Jan Frenzel

-> run two parallel MPI processes
-> each of these runs python
-> run ’ scorep ’ module before actual script

1 # mpirun -n 2
2 # python
3 # -m scorep
4 # -- mpp = mpi -- thr ... -> use MPI & pthread instrumentation
5 # ./ run . py
6 # -app - arg
7 mpirun -n 2 \
8 python -m scorep -- mpp = mpi -- thread = pthread ./ run . py -app - arg

-> the script or application to run
-> an argument to ./ run . py

Listing 1: Calling an MPI-parallel application using the Score-P Python
bindings

2.1 Preparation Phase

Since version 2.5, Python allows running modules as scripts [3]. This approach
can be used to record traces of a Python application. Instead of starting
the Python application directly, the script and its parameters are passed as
arguments to the Score-P Python module. The recording can be conﬁgured
by preﬁxing additional parameters to the parameter specifying the original
Python application. An example is given in Listing 1.

In the ﬁrst step, all Score-P related parameters are parsed. Score-P supports
a variety of programming models like OpenMP, MPI, and CUDA. However,
increasing the monitoring detail leads to more information in a trace or proﬁle
but also to a higher instrumentation overhead. Therefore, we allow the user to
choose which functionality should be monitored. Based on the chosen features,
the Score-P initialisation code is generated. This code is then compiled and
added together with some dependencies to the LD_PRELOAD environment vari-
able. As LD_PRELOAD is evaluated by the linker, the whole Python interpreter
needs to be restarted, which is done using os.execve() [12].

Once restarted, the module starts the second step: The instrumenter is
created, and the arguments, which are succeeding the Score-P arguments
are utilised. The ﬁrst non-Score-P argument is the Python application that
shall be executed, followed by its arguments. The Python application is read,
compiled, and executed [10], and its arguments are passed to the application.

2.2 Execution Phase

As described before, the execution phase uses two diﬀerent software parts:
the instrumenter and the Score-P C-bindings that hand over the events from
the instrumenter to Score-P.

Advanced Python Performance Monitoring with Score-P

5

The Instrumenter

The instrumenter represents a component that is registered with CPython
and supposed to be called for speciﬁc events during the execution of an
application. Python oﬀers two registration alternatives for such callback
functions: sys.settrace() and sys.setprofile() [14]. However, diﬀerent
events are raised and forwarded to the instrumenter depending on which of
these functions is used. A summary of these events is shown Table 1. Obviously,
both functions can be used to instrument function calls, but both also oﬀer
diﬀerent functionality. While sys.setprofile() can be used to trace also
calls to C-functions, sys.settrace() can be used to record lines of code or
operations executed.

Please note that tracing has diﬀerent meanings in the Python documen-
tation and in the HPC community. In the former, tracing describes the
investigation of per line execution of the source code, which can be used
to implement debuggers [14]. In contrast, the HPC community understands
tracing as the recording of events like entering or exiting a region over time [6].
In this paper, we use the term tracing for the HPC notion of tracing. If we
refer to the python notion of tracing we use sys.settrace().

However, for each callback, sys.settrace() and sys.setprofile(),
Python also issues the Python frame causing the event and some additional
arguments. The Python frame holds information like the current line number
of the associated module. The instrumenter passes this information to the
Score-P C-bindings.

Table 1: Supported events for Python proﬁling/debugging interfaces.

Event

Description

Supported by sys_set...
...profile() ...trace()

A function is called
A code block (e.g., a function) is about to return
A C function is about to be called
A C function has returned

call
return
c_call
c_return
c_exception A C function has raised an exception
The interpreter is about to a new line
of code or re-execute the condition of a loop
An [Python] exception has occurred
The interpreter is about to execute a new opcode

exception
opcode

line

(cid:51)
(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:55)

(cid:55)
(cid:55)

(cid:51)
(cid:51)
(cid:55)
(cid:55)
(cid:55)

(cid:51)

(cid:51)
(cid:51)

Score-P C-bindings

The Score-P C-bindings between Python and Score-P use the Python C-
interface [11] and the user instrumentation from Score-P [2, Section J.1.2].
The bindings do not only forward events regarding entering or exiting of

6

Andreas Gocht

, Robert Schöne, and Jan Frenzel

functions, but also group these functions based on their associated module.
Moreover, they also pass information like line number or the path to the
source ﬁle to Score-P. Score-P then uses these instrumentation events to
create Cube4-proﬁles, OTF2-traces or to call substrate plugins for an online
interpretation. Resulting traces can be viewed in Vampir [7], as shown in
Figure 2 for the small example code in Listing 2. A more complex parallel
application is visualized in Figure 3.

1 def baz () :

print ( " Hello World " )

2
3 def foo () :

baz ()

4
5 if __name__ == \
" __main__ " :

6

Fig. 2: Trace of a simple application using
the Score-P Python bindings and Vampir.
__main__ indicates that the function is part
of the currently run script.

foo ()

7

8

Listing 2: Simple Python
example

Fig. 3: Trace of a Python application [5] using CUDA and MPI. Traced using
the Score-P Python bindings. Green are TensorFlow functions; red are MPI
operations; blue are CUDA operations; black lines are CUDA communication.

Advanced Python Performance Monitoring with Score-P

7

3 Performance Evaluation

To evaluate the overhead caused by the instrumentation, we designed two
test cases. The ﬁrst test case, shown in Listing 3, increments a value in a
loop. We expect that the overhead introduced by the sys.setprofile()
instrumenter does not depend on the number of iterations about this loop,
since no functions are entered or exited. In contrast, we expect that the
instrumenter using sys.settrace() causes an overhead depending on the
iterations, since it is called for each executed line.

The second test case (Listing 4) uses a function to increment the value.
Here, we expect a strong dependency on the number of iterations for both
instrumenters.

1 import sys

2
3 result = 0

4
5 iterations = \

6

int ( sys . argv [1])

7
8 iteration_list = \

1 import sys

2
3 def add ( val ) :

4

return val + 1

5
6 result = 0
7 iterations = int ( sys . argv [1])
8 iteration_list = \

9

list ( range ( iterations ) )

9

list ( range ( iterations ) )

10
11 for i in iteration_list :

12

result += 1

10
11 for i in iteration_list :
result = add ( result )

12

13
14 assert ( result == iterations )

13
14 assert ( result == iterations )

Listing 3: Test case 1: loop only

Listing 4: Test case 2: function calls

We performed our experiments on the Haswell partition of the Taurus
Cluster at TU Dresden. Each node is equipped with two Intel Xeon CPU E5-
2680 v3 with 12 cores per CPU, and at least 64 GB of main memory per node [1].
Measurements are taken for each instrumenter, i.e. sys.setprofile() and
sys.settrace(), as well as without the Score-P module, marked with None.
Each experiment is repeated 51 times. The results are depicted in Figure 4. We
use linear interpolation to calculate the costs for (a) enabling instrumentation
and (b) using the instrumentation. While the former includes setting up the
Python environment and starting and ﬁnalizing Score-P, the latter represents
the costs to execute one loop iteration. We disabled the Score-P measurement
substrates proﬁling and tracing to represent only the overhead of instrumenting
the code. The linear interpolation uses the median of each measurement and
the polyfit function from numpy to create t = α + βN where t represents
the runtime, N is the number of iterations, α is the one-time overhead for

8

Andreas Gocht

, Robert Schöne, and Jan Frenzel

(a) Test case 1 (Listing 3)

(b) Test case 2 (Listing 4)

Fig. 4: Runtime of three instrumenters and non-instrumented code (None) for
diﬀerent test cases. Dotted lines represent a linear interpolation of the medians
of each measurement point. The overhead for setting up the measurement
and starting the Python environment is 0.6 seconds and independent of the
instrumenter. Please note the diﬀerent x-axis.

Table 2: Overhead for test cases (median results): α: constant overhead; β:
per loop iteration overhead

Instrumenter
None
sys.setprofile()
sys.settrace()

Test case 1
β
0.17 us
0.18 us
0.98 us

α
0.05 s
0.58 s
0.63 s

Test case 2
β
0.3 us
15.0 us
17.9 us

α
0.05 s
0.61 s
0.58 s

enabling the instrumentation and β is the cost per loop iteration. The results
of this interpolation are presented in Table 2.

For the ﬁrst test case (Figure 4a), we see that the instrumentation cost is
about 0.6 s. This cost will apply every time the instrumentation is enabled. Ex-
ecuting one loop will consume about 0.17 µs. Capturing the loop execution on a
per-line scale without forwarding the information to Score-P costs additionally
0.8 µs. This cost only appears for thesys.settrace() instrumenter.

For the second case (Figure 4b), we see the same initial costs. However, the
per-iteration costs are higher since we call functions. The general overhead
without instrumentation (None) increases by about 0.13 µs to about 0.3 µs.
The overhead for function instrumentation increases even more. Here each
function call adds about 14.7 µs (for sys.setprofile()). Due to the per-

10000002000000300000040000005000000Iterations [#]0123456Total runtime [s]Noneprofiletrace100000200000300000400000500000Iterations [#]0246810Total runtime [s]NoneprofiletraceAdvanced Python Performance Monitoring with Score-P

9

line overhead, we can say that sys.settrace() should not be used in the
current implementation where the same data is given to Score-P by both
available instrumenters. Therefore, we choose to set sys.setprofile() as
default instrumenter. In future versions of our software, we plan to include
information on exceptions or executed lines in proﬁles and traces. The user
will have to choose whether the additional information is important enough
for the added overhead.

4 Related Work

There are diﬀerent tools to proﬁle or trace Python code. The most common
ones are the built-in proﬁling tools proﬁle and cProﬁle [13]. While both
share the same command-line interface, cProﬁle is preferable, since it is
implemented in C and therefore faster. The output of both tools is usually
written to the command line, but can also be re-directed to a ﬁle. The output
can be converted and visualised by several third-party tools. For example,
pyprof2calltree [17] enables users to convert the output for later analysis with
Kcachegrind [18]. An alternative is SnakeViz [4], which visualises the output
of the built-in proﬁlers in a web application.

All these tools are only focussed at single node analysis and do not support
parallel programming paradigms used in HPC, like MPI or OpenMP. This
is diﬀerent for Extrae [16] and TAU [15]. Extrae uses sys.setprofile() to
register callbacks from Python. The developers implemented their interface
using ctypes, which is a foreign function interface for Python. TAU version
2.28.1 utilises PyEval_SetProfile from the C-API and register a callback
function that is written in C.

5 Conclusion and Future Work

In this paper, we introduced a module that enables performance engineers
to instrument Python applications with Score-P. We described and justiﬁed
diﬀerent design decisions that we encountered during development. To quantify
the runtime overhead, we presented measurements of two benchmark kernels.
Based on these measurements, we decided to use sys.setprofile() as the
default instrumenter, as the runtime overhead is smaller than the overhead
caused by sys.settrace().

Further work might include ways to control the runtime overhead, be-
sides manual instrumentation. One approach could be to sample Python
applications.

The Score-P Python bindings are available online at https://github.com/

score-p/scorep_binding_python.

10

Andreas Gocht

, Robert Schöne, and Jan Frenzel

Acknowledgments

This work is supported by the European Union’s Horizon 2020 program in
the READEX project (grant agreement number 671657).

References

1. HPC System Taurus (2019), https://doc.zih.tu-dresden.de/hpc-wiki/bin/view/

Compendium/HardwareTaurus

2. Score-P User Manual 6.0 (2019), http://scorepci.pages.jsc.fz-juelich.de/

scorep-pipelines/docs/scorep-6.0/pdf/scorep.pdf

3. Coghlan, N.: Executing modules as scripts. PEP 338, Python (2004), https://www.

python.org/dev/peps/pep-0338/

4. Davis, M., Bray, E.M., Schlömer, N., Xiong, Y.: SnakeViz (2019), https://github.

com/jiffyclub/snakeviz/

5. Horovod: tensorﬂow_keras_mnist.py. GitHub (Nov 2019), https://github.com/
horovod/horovod/blob/master/examples/tensorflow_keras_mnist.py, sha:c6ed366
6. Ilsche, T., Schuchart, J., Schöne, R., Hackenberg, D.: Combining Instrumentation and
Sampling for Trace-Based Application Performance Analysis. In: Tools for High Per-
formance Computing 2014 (2015), DOI: 10.1007/978-3-319-16012-2_6

7. Knüpfer, A., Brunst, H., Doleschal, J., Jurenz, M., Lieber, M., Mickler, H., Müller,
M.S., Nagel, W.E.: The Vampir Performance Analysis Tool-Set. In: Tools for High
Performance Computing (2008), DOI: 10.1007/978-3-540-68564-7_9

8. Knüpfer, A., Rössel, C., Mey, D.a., Biersdorﬀ, S., Diethelm, K., Eschweiler, D.,
Geimer, M., Gerndt, M., Lorenz, D., Malony, A., Nagel, W.E., Oleynik, Y., Philip-
pen, P., Saviankou, P., Schmidl, D., Shende, S., Tschüter, R., Wagner, M., Wesarg,
B., Wolf, F.: Score-P: A Joint Performance Measurement Run-Time Infrastructure
for Periscope,Scalasca, TAU, and Vampir. In: Tools for High Performance Computing
2011 (2012), DOI: 10.1007/978-3-642-31476-6_7

9. Python 3 Documentation: Glossary (2019), https://docs.python.org/3/glossary.

html

10. Python 3.7: Built-in Functions, https://docs.python.org/3/library/functions.

html

11. Python 3.7: Extending and Embedding the Python Interpreter (Oct.), https://docs.

python.org/3/extending/index.html

12. Python 3.7: os — Miscellaneous operating system interfaces, https://docs.python.

org/3/library/os.html#os.execve

13. Python 3.7: The Python Proﬁlers, https://docs.python.org/3/library/profile.

html

14. Python 3.7: sys — System-speciﬁc parameters and functions, https://docs.python.

org/3/library/sys.html

15. Shende, S.S., Malony, A.D.: The Tau Parallel Performance System. The In-
ternational Journal of High Performance Computing Applications (2006), DOI:
10.1177/1094342006064482

16. Wagner, M., Llort, G., Mercadal, E., Gimenez, J., Labarta, J.: Performance Anal-
ysis of Parallel Python Applications. Procedia Computer Science (Jun 2017), DOI:
10.1016/j.procs.2017.05.203

17. Waller, P., Dufresne, J., Grisel, O., Benjamin, Z.: pyprof2calltree (2019), https://

github.com/pwaller/pyprof2calltree/

18. Weidendorfer, J.: Sequential Performance Analysis with Callgrind and KCachegrind.
In: Tools for High Performance Computing (2008), DOI: 10.1007/978-3-540-68564-7_7

