RISE & Shine: Language-Oriented Compiler Design

Michel Steuwer∗ Thomas Kœhler† Bastian Köpcke‡ Federico Pizzuti∗
∗University of Edinburgh, Scotland, UK †University of Glasgow, Scotland, UK ‡University of Münster, Germany
Email: michel.steuwer@ed.ac.uk, thomas.koehler@thok.eu, bastian.koepcke@wwu.de, federico.pizzuti@ed.ac.uk

2
2
0
2

n
a
J

0
1

]
L
P
.
s
c
[

1
v
1
1
6
3
0
.
1
0
2
2
:
v
i
X
r
a

Abstract—The trend towards specialization of software and
hardware — fueled by the end of Moore’s law and the still
accelerating interest in domain-speciﬁc computing, such as ma-
chine learning — forces us to radically rethink our compiler
designs. The era of a universal compiler framework built around
a single one-size-ﬁts-all intermediate representation (IR) is over.
This realization has sparked the creation of the MLIR compiler
framework that empowers compiler engineers to design and
integrate IRs capturing speciﬁc abstractions. MLIR provides a
generic framework for SSA-based IRs, but it doesn’t help us to
decide how we should design IRs that are easy to develop, to
work with and to combine into working compilers.

To address the challenge of IR design, we advocate for a
language-oriented compiler design that understands IRs as formal
programming languages and enforces their correct use via an
accompanying type system. We argue that programming lan-
guage techniques directly guide extensible IR designs and provide
a formal framework to reason about transforming between
multiple IRs. In this paper, we discuss the design of the Shine
compiler that compiles the high-level functional pattern-based
data-parallel language RISE via a hybrid functional-imperative
intermediate language to C, OpenCL, and OpenMP.

We compare our work directly with the closely related pattern-
based LIFT IR and compiler. We demonstrate that our language-
oriented compiler design results in a more robust and predictable
compiler that is extensible at various abstraction levels. Our
experimental evaluation shows that this compiler design is able
to generate high-performance GPU code.

Index Terms—Compiler Design, Intermediate Representation

I. INTRODUCTION
The end of Moore’s law and Dennard’s scaling has opened
the door to a new golden age for computer architecture [11]
that is characterized by specialization of hardware, which we
already see in the diverse landscape of AI and ML accelerators.
Together with the specialization in hardware, we observe
an accelerating trend of specialized software solutions as well,
again most prominently in the domain of machine learning
with popular frameworks such as TensorFlow and PyTorch.

To exploit the tremendous efﬁciency gains of specialized
hardware, we need specialized compilers capable of translating
the specialized software to the hardware by employing opti-
mizations depending on the software domains and hardware
capabilities. Gone are the days of a one-size-ﬁts-all compiler
with a single intermediate representation (IR) sufﬁcient to
optimize for the few standard hardware architectures of interest.
These observations sparked the creation of the MLIR [16]
compiler framework as a natural evolution of the well-known
LLVM [15] compiler framework that embraced the idea of a
single static assignment (SSA)-based IR and with it managed
to unify community development efforts in the compiler
community at an unprecedented scale.

MLIR — in contrast to LLVM — embraces diversity and
specialization of IRs and their optimizations. With only few
core constraints on the IR designs, most notably the insistence
on SSA as a formal foundation, MLIR has already proven
to be a versatile framework for expressing various IRs1. This
includes traditional generic IRs (such as the LLVM IR itself),
IRs reﬂecting speciﬁc programming models (such as OpenMP
and TensorFlow), IRs specialized towards particular styles of
optimizations (such as the polyhedral model), IRs reﬂecting
speciﬁc hardware features (such as vectorization and GPU-
speciﬁc features), all the way to IRs for hardware circuit design.
But besides its core foundations, MLIR provides little
guidance on how to design good IRs that are easily extended
and easy to work with, which are robust by ruling out ill-formed
programs, and which compose well when integrated in full
compilers. MLIR provides a framework for designing compilers
as compositions of various SSA-based IRs, but what should
be our mental framework for designing the IRs themselves?

In this paper, we argue that we should look at programming
language design for inspiration. Of course, this observation is
not new, but we believe that this paper adds a new perspective
by providing insights into our experience of following this
approach. We present our experience of designing the opti-
mizing Shine compiler for RISE — a functional data-parallel
programming language in the spirit of LIFT [31, 32]. RISE
builds on a small functional core language that is enriched
with data-parallel patterns such as map and reduce suitable
for expressing numerical computations over tensors relevant
in many domains ranging from machine learning to scientiﬁc
computing. The functional and pattern-based design makes
it easy to exploit high-level optimization opportunities via
rewriting and to generate high-performance code as explored
before in the context of LIFT [9, 24] and RISE [10, 14].

To summarize, this paper makes the following contributions:

• We advocate a language-oriented IR and compiler design
by reporting our experience of engineering the Shine
compiler and discussing its design (Section III);

• we describe the beneﬁts of our compiler design: a clear
separation of concerns between optimizing and code
generation (Section IV), formalizing of invariants and
assumptions about IRs in type systems (Section V), and its
extensibility at various levels in the compiler (Section VI);
• we present an experimental evaluation demonstrating the

quality of the generated code (Section VII).

1https://mlir.llvm.org/docs/Dialects/

1

 
 
 
 
 
 
II. BACKGROUND IN IR AND COMPILER DESIGN

Compiler IRs have been studied extensively before. In the
community of optimizing compilers for imperative languages,
SSA [23] — developed in the late 1980s at IBM research [2, 27]
— has by now been established as the standard form for IRs.
SSA makes def-use-chains explicit and guarantees that def s
are introduced before they are used. Since the mid-1970s, the
functional community used lambda calculus (introduced in
the 1930s by Alonzo Church) as a compiler IR. The Lambda
Papers series even advocated the ﬂexibility of lambda calculus
to model non-functional languages [29, 30].

In the mid/late 1990s Kelsey and Appel pointed out a
direct correspondence between SSA and the functional lambda
calculus representations [3, 13]. Despite this fundamental corre-
spondence, the functional and imperative compiler communities
have continued to develop largely independently.

The rise of machine learning in the last decade has sparked
interest in domain-speciﬁc compilers and their IRs, speciﬁcally
for numerical computations over tensors — multidimensional
arrays. These specialized compilers such as TensorFlow [1],
PyTorch [20], or TVM [8] often use a graph-based representa-
tion. These systems have developed into complex optimizing
compilers having taken on inspirations from related compilers
such as Halide [22] and LIFT [31, 32]. LIFT builds on a
tradition of similar functional languages and compilers that
existed before, such as SaC [28]. More have emerged recently,
including Accelerate [7] and Futhark [12] as well as Dex [21]
by Google research.

An advantage of the functional representation is that op-
timizations are naturally expressed via rewriting allowing
the automatic exploration of optimizations for example by
stochastic search as explored by LIFT [24, 31]. Stratego [5]
and more recently Elevate [10] show how to describe complex
program transformations as composition of rewrites expressed
in strategy languages that provide a more formal treatment of
the scheduling languages found in Halide and TVM.

To accommodate the rising need for specialized IRs,
MLIR [16] provides a common framework for designing IRs
and transformations between them. The different IRs, called
dialects, must follow the SSA form. But in contrast to LLVM’s
IR, the awkward φ-nodes are replaced with so-called regions
that allow for directly representing nested structures, such as
control ﬂow with multiple branches, e.g., an if-then-else
operation. MLIR allows for a broad range of IRs to be
represented and provides itself little guidance on how to design
dialects that are well-behaved, that is, dialects that: 1) providing
a clear purpose; 2) have formally checked invariants and
assumptions; 3) are easily extensible.

In this paper, we provide a perspective on how to design IRs
that have these positive properties. We share our experience
in developing the Shine compiler that composes a LIFT-
like functional IR with a hybrid functional-imperative IR for
compiling high-level programs to high-performance code.

Fig. 1: Overview of the language-oriented designed compiler
Shine: High-level programs expressed in the functional lan-
guage RISE are rewritten to encode all major optimization
and implementation choices. The resulting low-level program
is translated into DPIA — a language combining functional
and imperative aspects, from which high-performance code
is generated. The language-oriented design results in a clear
separation of concerns (optimizing vs. code generation), it
simpliﬁes the formalization of invariants and assumptions about
the IRs in their type systems, and it’s easily extensible by
introducing abstractions at multiple levels in the compiler.

III. SHINE: A LANGUAGE-ORIENTED COMPILER
Figure 1 shows an overview of the Shine compiler. The
overall design follows the compiler described by Hagedorn et al.
in [10] where an optimization strategy written in a strategy
language (aka, scheduling language) called Elevate speciﬁes
the optimizations to be applied to the input program. Elevate is
described in detail in [10] and we will not discuss it further in
this paper, but we highlight that it allows for experts (human or
machine) to directly control the optimizations to be performed.
The input program of the Shine compiler is written in the
functional data-parallel language RISE that uses a restricted
form of dependent types to track the size of multi-dimensional
arrays in the type. This follows our language-oriented design
principle in that this information is embedded into the type
system of the IR rather than being represented as separate
meta-data. This has the main advantage that we have a formal
account of the structure of the meta-data (here tracking array
lengths) allowing us to ensure its consistency by the soundness
of the type system, for example, the type system accounts for
the basic fact that variables referred to in the types must be
well-scoped.

The process of compiling a high-level RISE program to high-
performance code is shown in Figure 1. The process is broken
into two steps, each with a corresponding language and type
system serving as the IR.

2

CodegenRewritingHigh-Performance CodeC/OpenMP/OpenCLOptimization StrategyHigh-Level ProgramRISELow-Level ProgramRISELow-Level ProgramDPIAFunctionalImperativeFirst, the functional high-level program is transformed via
rewriting into a low-level program that encodes optimiza-
tion and implementation decisions. The Elevate optimization
strategy, given as an input to the compiler, describes these
decisions, such as: Should we fuse multiple patterns to avoid
intermediate buffers? In which address space should we allocate
intermediate buffers? Should we perform map patterns in
parallel or sequential? In this rewriting step, choices for these
decisions are directly encoded, resulting in a functional low-
level program.

The remaining steps translate the functional

low-level
program into an imperative low-level program. For this, we
introduce a new typed language called DPIA as IR that
combines functional and imperative aspects while preventing
simultaneous sharing and mutation similar to Rust. After
low-level RISE program into a
translating the functional
functional DPIA program, we gradually translate it into an
imperative program using a formal translation. This translation
preserves the implementation and optimization choices made
before and does not make signiﬁcant implementation choices
itself. Finally, high-performance code is generated (C, OpenMP,
or, OpenCL depending on the implementation choices made).

IV. CLEARLY SEPARATING OPTIMIZING FROM
CODE GENERATION
One main principle observed in the Shine compiler is the
clear separation of IRs for optimizing and code generation.
In this design, two languages (RISE and DPIA) are composed,
each serving as the IR for one task: all optimizations and
major implementation decisions are performed on RISE; the
code generation process of translating the functional to an
imperative program is performed in DPIA.

Keeping these two fundamental tasks separated has the key
advantage that the IR for optimizing is kept simple and free of
unnecessary low-level details. These are only added after all
optimization decisions are encoded in the functional program.
To understand the optimization and code generation process
of the Shine compiler, we will follow an easy-to-understand
running example: matrix-vector-multiplication (Listing 1). This
functional program describes the computation algorithmically
without committing to a particular implementation strategy: for
each row of the matrix M, we compute the dot-product with
the input vector x. The dot-product is expressed in lines 5–7

1
2
3
4
5
6
7

def mv = depFun((n: Nat, m: Nat) =>
fun(M: Array[n, Array[m, f32]] =>

fun(x: Array[m, f32] =>
M |> map(fun(row =>
zip(row)(x) |>

map(fun(ax => fst(ax) * snd(ax))) |>

reduce(add)(0.0f) )) )) )

Listing 1: Matrix-Vector-Multiplication expressed as a high-
level program in RISE. No optimization or implementation
choices have been made yet, there are many ways to compile
this program to high-performance code.

E := x | 0.0f |

fun(x => E) |
E |> E | E(E) |
depFun(x: K => E) |
E(N ) | E(DT ) | E(A) |
map | reduce | reduceSeq | zip | ...

variables and literals
function abstraction
function application
dependent fun. abstraction
dependent fun. application
primitives

Expr

t: Type

Identifier name: String

Literal value: String

Lambda x: Identifier; e: Expr

Apply f: Expr; e: Expr

DepLambda<K,I> kind: K; x: I; e: Expr

DepApply<K,T> kind: K; f: Expr; x: T

Primitive

Fig. 2: Grammar of RISE expressions (above) and correspond-
ing diagram of classes (below) representing them.

by composition of the zip primitive that aligns the matrix row
and the vector element wise, followed by the map primitive
that multiplies each aligned pair, before the resulting array
is summed up using the reduce primitive. In RISE, sizes of
multi-dimensional arrays can be tracked symbolically in their
types. For this, we introduce a dependent function in line 1
that abstracts over the type-level size values in the same way
as ordinary functions abstract over computational values. The
introduced variables n and m are now allowed to appear in the
types of the nested expression. In a non-language-oriented IR
design, such information is often modeled as meta-data without
a clear understanding of scoping rules and how it interacts with
the computational operations of the IR. Next, let us investigate
the RISE language itself.

A. RISE expressions, types, and primitives

From a users’ perspective, RISE shares many common
features with the LIFT IR, but their implementations differ
signiﬁcantly. The LIFT implementation separates expressions
and function declarations and lacks a formalized type system,
in particular LIFT lacks the concept of a function type. In
contrast, RISE uniﬁes all language constructs under a single
Expr superclass. This simpliﬁed design follows the usual
implementation of languages in the functional community by
using an algebraic data type (ADT) to represent the various
language constructs. ADTs model data in a structured way by
organizing it in a ﬁnite set of alternative cases, where each case
is represented as a struct containing multiple ﬁelds. ADTs
are a perfect ﬁt to represent grammars, such as RISE’s grammar
shown at the top of Figure 2. Support for ADTs is directly built
into many functional languages, but support has recently also
been added in non-functional languages, e.g., they are known
as enum in Rust and Swift and as std::variant in C++.
A straightforward way to encode ADTs in an object-oriented
language is to build a ﬂat class hierarchy, as shown at the
bottom of Figure 2.

3

T := t | DT |

T → T | (x: K) → T

type variables & data types
function types

DT := f32 | ... |

Array[N ,DT ] | Tuple[DT ,DT ]

scalar types
array & tuple types

N := 0|1| ... | N + N | N * N | ...
A := Global | Local | Private
K := Nat | DataType | AddrSp

natural numbers
OpenCL address spaces
kinds

TypeIdentifier name: String

FunType in: Type; out: Type

Type

DepFunType<K,I> kind: K; x: I; t: T

DataType
DataType

ScalarType

Array

Tuple

size: Nat;
elem: DataType

fst: DataType;
snd: DataType

f32

. . .

Literal value: Int

Nat

BinOp op: String; lhs: Nat; rhs: Nat

. . .

Fig. 3: Grammar of RISE types and type-level values (above)
and diagram of classes (below) representing them.

The Expr class is abstract with seven subclasses modeling
the language constructs, such as: Lambda and Apply to
model function abstraction (aka, lambdas) and application,
DepLambda/Apply to model function abstraction/application
over variables at the type level (e.g, for reasoning about
the length of arrays), and ﬁnally, Primitive to represent
primitives, such as map and reduce.

Figure 3 shows the grammar and corresponding class
hierarchies of RISE types. Type level values are organized into
distinct and interconnected classes following the formalization
given by the grammar and the formal typing rules (not shown in
this paper, but described in [4, 9]). DataTypes are kept distinct
from general types and represent types that can be stored in
memory. As we compile to GPUs, this design carefully avoids
the challenge of storing functions (or closures) in memory,
enforcing an important invariant of our IR.

Array types track their length with the member size that is
a natural number and forms a distinct category of expressions
represented by the Nat class and subclasses.

Finally, Listing 2 shows some important high-level data-
parallel primitives modeled as functions. We use a functional
notation for their types, i.e., every argument is separated by
→. For example, map expects 5 arguments, 3 at the type-level:
a natural number n, 2 data types s and t; and 2 ordinary
arguments: a function and an array to whose elements the
function is applied to produce the output array. Parameters in
curly braces are implicit and automatically inferred.

map: {n: Nat} -> {s: DataType} -> {t: DataType} ->

(s -> t) -> Array[n, s] -> Array[n, t]

reduce: {n: Nat} -> {t: DataType} ->

(t -> t -> t) -> t -> Array[n, t] -> t

zip: {n: Nat} -> {s: DataType} -> {t: DataType} ->

Array[n, s] -> Array[n, t] -> Array[n, Tuple[s, t]]

Listing 2: Selection of RISE functional data-parallel primitives

B. Optimizing RISE via rewriting

Optimizing is the primary focus of RISE, and it is clearly
separated from compiling the functional abstractions away.
Translation to imperative constructs happens in later stages.

The rewrite rules used in the Shine compiler follow closely
the rules introduced by LIFT, for example, three algorithmic
rules for splitting and fusing map and reduce:

map(f) (cid:55)→ split(n) >> map(map(f)) >> join
map(f) >> map(g) (cid:55)→ map(f >> g)
map(f) >> reduce(fun(acc, y => acc ⊕ y), init)

(cid:55)→ reduceSeq(fun(acc, y => acc ⊕ f(y)), init)

These rules encode algorithmic optimization choices that
when applied to an expression directly encode that choice in
the rewritten expression. This becomes even more clear for
rules translating a high-level RISE primitive into a low-level
implementation-speciﬁc primitive. For example, the following
rules provide different options to implement the high-level map
primitive in the OpenCL programming model:

map (cid:55)→ mapSeq
map (cid:55)→ mapWorkGroup

map (cid:55)→ mapGlobal
map (cid:55)→ mapLocal

Applying one of these rules to a map primitive encodes the
explicit choice that this primitive should be implemented either
sequentially or in one of three different parallel ways, each
exploiting different aspects of the GPU.

Rewrite rules transforming RISE are expressed in Ele-
vate [10] a language for composing rewrite rules into larger op-
timization strategies. In this design, rewrite rules are functions
taking a RISE expression as input and returning the transformed
expression or a failure state if the rewrite rule could be applied.
An Elevate optimization strategy that describes a particular
composition of rewrite rules corresponds to a schedule in
systems such as TVM or Halide describing precisely how the
high-level RISE program should be optimized and implemented.

C. Optimizing Matrix-Vector-Multiplication

There are many choices to be made when deciding how
to optimize the matrix-vector-multiplication example from
Listing 1. A key idea of optimizing by rewriting in the Shine
compiler is to expose these choices rather than make them
internally.

Listing 3 shows one possible (and for illustration purposes
simple) way to optimize. This Elevate optimization strategy
describes a sequence of rewrite steps that transforms the high-
level description of matrix-vector-multiplication into the low-

4

splitJoinMap
toMapWorkGroup
toMapLocal
fuseReduceMap
toReduceSeq

`@` outermost(isMap)
`@` outermost(isMap)
`@` outermost(isMap)
`@` every(isReduce)
`@` every(isReduce)

`;`
`;`
`;`
`;`

Listing 3: One possible way to optimize Matrix-Vector-
Multiplication encoded as an Elevate optimization strategy.

1
2
3
4
5
6
7
8
9
10

def mvOpt = depFun((n: Nat, m: Nat) =>
fun(M: Array[n, Array[m, f32]] =>

fun(x: Array[m, f32] =>

M |> split(s) |> mapWorkGroup(fun(rows =>

rows |> mapLocal(fun(row =>

zip(row)(x) |>

reduceSeq(Private)(fun(acc, ax =>

acc + (fst(ax) * snd(ax))))(0.0f) ))

)) |> join

)) )

Listing 4: Optimized Matrix-Vector-Multiplication in RISE.

level version shown in Listing 4. The outermost map is split
into two maps (using the splitJoinMap rule) that are then
lowered into two speciﬁc versions of map reﬂecting the OpenCL
parallelism model. The reduce and map primitives describing
the dot product computation are fused into a single reduction
(using the fuseReduceMap rule). This reduction can no longer be
parallelized (as the operator is not associative), but we avoid
the need for an intermediate buffer and two separate loops.

Summary

In this section, we have discussed the design of the RISE
language that is used in the Shine compiler for explicitly
encoding optimization and implementation choices. This clear
purpose heavily inﬂuences the design of the language: optimiza-
tions are expressed as compositions of rewrite rules, allowing
them to be easily extended and externally controlled; to enable
rewriting, the IR is functional and free of side effects to be
referentially transparent (i.e., to allow any expression to be
replaced by an equivalent expression).

Deliberately, RISE does not have imperative features or
concepts, such as assignments into memory, or for loops.
This still leaves a large gap to close between the optimized
RISE program and the targeted imperative programming models,
such as C, OpenMP, or OpenCL. To translate the functional to
imperative code, we introduce a separate dedicated language
called DPIA that does provide functional and imperative
concepts and is, therefore, suited to serve for the further code
generation process. While existing functional solutions, such
as LIFT, have RISE-like languages, are missing a dedicated IR
for code generation, such as DPIA, and, therefore, combine
optimizing and code generation in one step, complicating both
steps and the overall compiler design.

V. TYPE SYSTEMS FOR FORMALIZING
INVARIANTS AND ASSUMPTIONS

Each compiler IR has invariants and assumptions that must
hold for optimizations and translations to work as expected.
For example, a polyhedral-speciﬁc IR must ensure that array
indices are afﬁne for polyhedral optimizations to be valid. In
this section, we will discuss how a language-oriented view
helps to formalize such invariants and assumptions in the type
system of the language used as the IR.

We already discussed in Section IV that RISE separates
data types whose values are stored in memory from function
types, enforcing an important invariant and ruling out programs
that we are unable to compile into efﬁcient GPU code. We
also formulated the clear goal,
that all optimization and
implementation decisions have to be encoded directly into
the RISE program. This is a crucial assumption we make in
the further compilation process. But how do we enforce it?

It is easy to check that implementations have been chosen for
high-level primitives, for example, it is easy to check that all
map primitives have been replaced with their implementation-
speciﬁc counterparts (e.g., mapSeq or mapGlobal). But there
is a crucial hidden choice that cannot be overlooked: memory.
Let’s consider for example the following RISE program:

depFun((n: Nat, m: Nat) =>

fun(M: Array[n, Array[m, f32]] =>
M |> mapWorkGroup(fun(row =>

row |> mapLocal(f) |> mapLocal(g)) )))

Listing 5: RISE program without choice of temporary memory.

For every element in every row of the matrix M, ﬁrst the function
f is applied and then the function g. The parallelization strategy
is clearly described here and also the fact that f and g will be
computed in two separate steps rather than a single one. But this
leaves the choice of where to allocate the intermediate memory.
On a GPU, there are different address spaces (e.g., local/shared
vs global memory) with vastly different size and performance
characteristics. In LIFT, this choice is already modeled via
speciﬁc primitives such as toLocal and toGlobal [32], but
making this choice explicitly is not enforced. In the following,
we introduce the DPIA language, and its type system that
formalizes the intuition that everywhere where we need to
write to memory the address space choice has been made.
Furthermore, DPIA combines functional and imperative aspects
to facilitate the translation of the optimized functional program
to imperative high-performance code.

A. DPIA phrases, types, and primitives

DPIA is a variation of Reynolds’s idealized ALGOL [4, 25]
and aims to integrate functional and imperative concepts in a
single language. For this, we separate our program phrases
into three categories: (functional) expressions, (imperative)
commands (like C statements), and (imperative) acceptors. One
can think of an acceptor as a pointer that we can manipulate
and ultimately use for writing to a memory location.

5

P := x | 0.0f |

fun(x => P ) |
P |> P | P (P ) |
depFun(x: K(cid:48) => P ) |
P (N ) | P (DT ) | P (A) |
mapWorkGroup|reduceSeq|...
P =P |(;)|new|parForWorkGroup|...

variables and literals
function abstraction
function application
dependent fun. abstraction
dependent fun. application
functional primitives
imperative primitives

T (cid:48) := t | T (cid:48) → T (cid:48) | (x:K) → T (cid:48)|

T (cid:48) × T (cid:48) |
Exp[DT , RW ] |
Acc[DT ] |
Comm
RW := Rd | Wr
K(cid:48) := K | ReadWrite

type var. & function types
phrase pair type
expression type
acceptor type
command type
read-write annotations
read-write & RISE kinds

kinds:

Fig. 4: Grammar of DPIA phrases, types, and type-level values.

Figure 4 shows the grammar of DPIA phrases and types.
The language looks similar to RISE and in fact uses RISE’s
functional language constructs and data types. New are the
imperative constructs, such as assignment (P =P ). The type
system reﬂects the separation into expressions (Exp[DT ,RW ]),
acceptors (Acc[DT ]), and commands (Comm). (Dependent) func-
tions and pairs can freely combine phrases of all categories. The
expression type has a ReadWrite annotation used for checking
that an input program has encoded all choices about memory
address spaces explicitly, as we will discuss in Section V-B.
Listing 6 and 7 show a selection of DPIA primitives. In RISE,
we introduced primitives with a function type. In contrast,
in DPIA primitives are fully applied values. For example, in
RISE mapSeq(f) represents the function application of mapSeq
to f. The equivalent program in DPIA, must use a lambda:
fun(x => mapSeq(f, x)). Insisting on fully applied primitives
simpliﬁes the translation and code generation process, while
allowing partially applied primitives (such as map(f)) simpliﬁes
the speciﬁcation of rewrite rules. With two separate IRs, we
can choose the most convenient style for each task.

The functional primitives in Listing 6 only contain low-
level variations of the high-level map and reduce primitives, as
for programs at this stage, all implementation choices must
have been made. Some functional primitives, such as zip,
join, and split will always be fused into successive primitives
and, therefore, remain unchanged. There are also low-level
primitives such as indexing into arrays (idx) and toMem which
we will discuss in the next section.

Listing 7 shows a selection of DPIA’s imperative primitives
and their types. These are commands, such as writing to
memory via an assignment, sequencing commands, memory
allocation (new), and sequential and parallel for loops. zipAcc1/2
and joinAcc represent computations over acceptors, for example,
treating a one-dimensional array as a two-dimensional one
(joinAcc). Eventually, these computations will correspond to
pointer manipulations in the generated code.

B. Enforcing Presence of Necessary Address Space Choices

In Listing 5 we introduced an example of a RISE program
where a necessary choice to clarify the address space of a
temporary memory buffer was missing. If we attempt to compile

6

mapLocal(n: Nat, s: DataType, t: DataType,

f: Exp[s,Rd] -> Exp[t,Wr],
x: Exp[Array[n,s],Rd]): Exp[Array[n,t],Wr]

reduceSeq:(n: Nat, a: AddrSp, s: DataType, t: DataType,

f: Exp[t,Rd] -> Exp[s,Rd] -> Exp[t,Wr],
init: Exp[t,Wr], x: Exp[Array[n,s],Rd]): Exp[t,Rd]
zip:(n: Nat, s: DataType, t: DataType, w: ReadWrite,

lhs: Exp[Array[n,s],w],
rhs: Exp[Array[n,t],w]): Exp[Array[n,Tuple[s,t]],w]

join:(n: Nat, m: Nat, t: DataType, w: ReadWrite,

x: Exp[Array[n,Array[m,t]],w]): Exp[Array[n*m,t],w]

split:(n: Nat, m: Nat, t: DataType, w: ReadWrite,

x: Exp[Array[n*m,t],w]): Exp[Array[m,Array[n,t]],w]

toMem:(a: AddrSp, t: DataType, x: Exp[t,Wr]): Exp[t,Rd]
idx:(n: Nat, t: DataType,

i: Exp[Idx[n],Rd], arr: Exp[Array[n,t],Rd]): Exp[t,Rd]

Listing 6: Selection of DPIA functional primitives

assign:(t: DataType, lhs: Acc[t], rhs: Expr[t, Rd]): Comm
seq:(c1: Comm, c2: Comm): Comm
new:(a: AddrSp, t: DataType,

body: (Exp[t,Rd] x Acc[t]) -> Comm): Comm

for:(n: Nat, body: Exp[Idx[n],Rd] -> Comm): Comm
parForLocal(n: Nat, t: DataType,

out: Acc[Array[n,t]],
body: Exp[Idx[n],Rd] -> Acc[t] -> Comm): Comm

zipAcc1:(n: Nat, s: DataType, t: DataType,

array: Acc[Array[n,Tuple[s,t]]]): Acc[Array[n,s]]

zipAcc2:(n: Nat, s: DataType, t: DataType,

array: Acc[Array[n,Tuple[s,t]]]): Acc[Array[n,t]]

joinAcc:(n: Nat, m : Nat, t : DataType,

array: Acc[Array[n*m,t]]): Acc[Array[n,Array[m,t]]]

Listing 7: Selection of DPIA imperative primitives

this program, we have to make a decision on behalf of the user.
We want to avoid such situations, and DPIA’s type system
formalizes this: when translating a program from RISE to
DPIA the type system checks the consistency of all ReadWrite
annotations and rejects programs with inconsistent annotations.
Let’s revisit the example from Listing 5, after it has been
translated to DPIA:

depFun((n: Nat, m: Nat) =>

fun(M: Exp[Array[n, Array[m, f32]], Rd] =>

mapWorkGroup(n, Array[m, f32], Array[m, f32],

fun(row => mapLocal(m, f32, f32, g,

mapLocal(m, f32, f32, f, row))),

M )))

The translation process from low-level RISE to functional
DPIA is straightforward, as there exists a corresponding DPIA
primitive for each low-level primitive in RISE.

The program should not type check in DPIA as a choice of
address space is missing. Let us understand why by looking
at the crucial part of the program:

mapLocal(m, f32, f32, g,

// expected:
Exp[Array[m, f32], Rd]
mapLocal(m, f32, f, row) : Exp[Array[m, f32], Wr])

As indicated by the comment, we can see that the type of the
second mapLocal does not match the type expected by the ﬁrst
mapLocal. The precise type of mapLocal is given in Listing 6.
Clearly, the ReadWrite annotations do not match: we expect a
value that we can read from memory (as indicated by Rd), but
instead we have been given a value that ﬁrst has to be written
to memory (as indicated by Wr) before we can read it.

The ReadWrite annotations are introduced in the translation
process from RISE to DPIA and the functional primitives in
Listing 6 are explicitly annotated with them. Some primitives,
such as zip, are polymorphic over ReadWrite annotations, i.e.,
their annotation depends on the context they are used in. For
these primitives, a simple inference is performed.

There exists only one primitive to convert an expression that
needs to be written to memory (i.e., with Wr) to an expression
from which we can read (i.e., with Rd): toMem which expects
the address space to write into as an argument.

Therefore, to ﬁx our program, we need to inject a toMem in
between the two mapLocals. This choice has to be encoded in
the RISE program, e.g., via an appropriate rewrite rule, before
we translate to DPIA. Once we are in DPIA, we can only check
that address space annotations are consistent.

After adding toMem in the RISE program and translating to

DPIA we see that the type checking succeeds:

mapLocal(m, f32, f32, g,

// expected: Exp[Array[m, f32], Rd]
toMem(Private, Array[m, f32],

Exp[Array[m, f32], Wr]
// expected:
mapLocal(m, f32, f, row) : Exp[Array[m, f32], Wr]

)

: Exp[Array[m, f32], Rd] )

It is important to note that toMem only accepts arguments
that are not yet written to memory, as indicated by the Wr
annotation on its input. Otherwise, we would have to make a
decision on how to copy the value from one memory location
to another. But there are many ways to copy, for example, an
array: sequentially, vectorized, or in parallel using threads?
As we refuse to make such choices in DPIA, we reject such
programs and insist that the choice on how to perform the
copy must be encoded in RISE explicitly.

C. Translating Functional into Imperative Programs

Now that we are conﬁdent that all implementation decisions
are encoded in the functional DPIA program, we discuss how
it is translated into a program using DPIA’s imperative features.
For example, we want to translate a primitive such as reduceSeq
into a for loop.

This translation process is performed by two intertwined
translation functions, shown in Listing 8 and 9. The acceptor
translation accT takes a functional expression that needs to be
written to memory (i.e., with Wr) plus an acceptor representing
the memory to write into. The continuation translation conT
takes expressions that we can read from (i.e., with Rd) plus
a continuation function that knows how to continue the
translation once we have translated the current expression.
Both translations return an imperative command.

def accT(expr: Phrase[Exp[d,Wr]],

output: Phrase[Acc[t]]
): Phrase[Comm] = expr match {
case mapWorkGroup(n, s, t, f, arr) =>

conT(arr, fun(arrT =>

parForWorkGroup(n, t, output, fun((i, o) =>

accT(f(idx(n, s, i, arrT)), o) ))))

case mapSeq(n, s, t, f, arr) =>

conT(arr, fun(arrT =>
for(n, fun(i =>

accT(f(idx(n,s,i,arrT)), idxAcc(n,t,i,output)) ))))

case zip(n, s, t, Wr, lhs, rhs) =>

accT(lhs, zipAcc1(n, s, t, output)) ;
accT(rhs, zipAcc2(n, s, t, output))

case join(n, m, t, Wr, arr) =>

accT(arr, joinAcc(n, m, t, output))

case ...

}

Listing 8: Deﬁnition of acceptor translation

def conT(expr: Phrase[Exp[t,Rd]],

continuation: Phrase[Exp[t,Rd]] -> Phrase[Comm]
): Phrase[Comm] = expr match {

case toMem(a, t, e) =>
new(a, t, fun((tmpAcc, tmpExp) =>

accT(e, tmpAcc) ; continuation(tmpExp) ))
case reduceSeq(n, a, t, s, f, init, arr) =>

conT(arr, fun(arrT =>

new(a, t, fun((accumAcc, accumExp) =>

accT(init, accumAcc) ;
for(n, fun(i =>

accT(f(accumExp, idx(n, s, i, arrT)),

accumAcc) )) ;

conT(accumExp, continuation) )) ))

case zip(n, s, t, Rd, lhs, rhs) =>

conT(lhs, fun(lhsT =>

conT(rhs, fun(rhsT =>

continuation(zip(n, s, t, Rd, lhsT, rhsT))))))

case join(n, m, t, Rd, x) =>

conT(x, fun(xT => continuation(join(n, m, t, Rd, xT))))

case split(n, m, t, Rd, x) =>

conT(x, fun(xT => continuation(split(n, m, t, Rd, xT))))

case fst(s, t, Rd, x) =>

conT(x, fun(xT => continuation(fst(s, t, Rd, xT))))

case snd(s, t, Rd, x) =>

conT(x, fun(xT => continuation(snd(s, t, Rd, xT))))

case ...

}

Listing 9: Deﬁnition of continuation translation

To start the overall translation process, we allocate output
memory according to the data type of the value that the
functional program computes. When compiling to OpenCL,
the results of a kernel must reside in global memory so that
we have no choice of address space to make here. With the
given output, we invoke the accT translation. This corresponds
to the intuition of translating a program that computes a value
functionally and afterwards writes it to memory.

For the matrix-vector-multiplication translated from RISE

(Listing 4) to DPIA we invoke accT with an output array:

mvOptImp = accT(mvOpt, output : Acc[Array[n,f32]])

7

Following the intuition of the acceptor translation, we
traverse the functional program backwards, starting with the
last primitive; the primitive that writes into the output. In case
of the matrix-vector multiplication, the last primitive is join
with mapWorkGroup as its argument. join changes our view from
a two-dimensional into a one-dimensional array. The acceptor
translation indicates that we call accT with join’s argument
(here mapWorkGroup) and wrap the output in a joinAcc:

accT(join(mapWorkGroup(f, arr)), output)

= accT(mapWorkGroup(f, arr), joinAcc(output))

The joinAcc primitive has the reverse effect on an acceptor
to that of join on an expression: we transform our view on the
output memory, instead of a ﬂat one-dimensional array we treat
it as two-dimensional. Then we translate mapWorkGroup:

accT(mapWorkGroup(f, arr), joinAcc(output))

= conT(arr, fun(arrT =>

parForWorkGroup(n/s, joinAcc(output), fun((i, o) =>

accT(f(idx(i, arrT)), o) ))))

In the imperative program, the code computing the input
of mapWorkGroup must appear before the code for mapWorkGroup
itself, therefore, we must continue to translate the input ﬁrst by
using the continuation translation. This is the right choice, as
we must be able to read from this expression based on the type
of mapWorkGroup. To describe how the translation continues,
we pass along a continuation function that will be called
once we have translated the input. This function will be given
the translated input array (arrT) and inside we describe the
translation of mapWorkGroup itself into a parForWorkGroup. This
parallel for loop is parameterized with the length of the array,
it is mapping over; the output it is writing into; and a function
representing the loop body. The loop body has access to the
loop index i as well as an acceptor o that guarantees that each
iteration writes to a distinct location in the output array. In the
loop body, we invoke the acceptor translation of the function
f applied to a single value of the (translated) input array by
indexing into it and passing the acceptor to write into along.
To translate the full program, we continue the translation
process by calling the accT and conT functions as indicated in
Listing 8 and 9 for each primitive, until we have traversed the
entire functional program.

The result of the translation process is the imperative DPIA
program shown in Listing 10, from which we generate the
OpenCL code shown in Listing 11. This ﬁnal code generation
step is small, as the structure of the imperative DPIA program
directly corresponds to the OpenCL program. For example,
parForWorkGroup and parForLocal correspond to the two outer
loops in line 6 and 8 in the OpenCL code. The remaining gap is
to resolve some functional expressions into multi-dimensional
array indices. This process is similar to the views in LIFT [32]
and involves simplifying arithmetic expressions to generate
concise indices. The only conceptual difference to LIFT is
that in DPIA index computations also happen on acceptors to
manipulate the location where we write into memory.

1
2
3
4
5
6
7
8
9
10
11
12

depFun((n: Nat, m: Nat) =>

fun(M:Array[n,Array[m,f32]] => fun(x:Array[m,f32] =>

parForWorkGroup(n/s, Array[m,f32], joinAcc(output),
fun((wgId, wgOut) =>
parForLocal(s,f32, wgOut, fun((lId, lOut) =>
new(Private, f32, fun((accumAcc, accumExp) =>
accumAcc = 0.0f;
for(m, fun(i =>
accumAcc = accumExp +
fst(idx(i,zip(idx(lId,idx(wgId,split(s,M))),x)))
* snd(idx(i,zip(idx(lId,idx(wgId,split(s,M))),x)))
)) ; lOut = accumExp )))))))))

Listing 10: MV-Mult. after translation into imperative DPIA

1
2
3
4
5
6
7
8
9
10
11
12
13
14

__kernel

void mvOptKernel(global float* restrict output,

for (int wgId = get_group_id(0);

int n, int m, int s,
const global float* restrict M,
const global float* restrict x) {
// parForWorkGroup

wgId < n/s; wgId += get_num_groups(0)) {

for (int lId = get_localId(0);

//

parForLocal

lId < s; lId += get_local_size(0)) {

float accum;
accum = 0.0f;
for (int i = 0; i < m; i += i) {

//
new
// assign
for
//

accum += (M[(i+lId*m)+(m*s*wgId)] * x[i]); }

output[lId + (s * wgId)] = accum; }}}

// assign

Listing 11: OpenCL code generated for MV-Mult.

Summary

In this section, we discussed the DPIA language that is
used as an IR in the Shine compiler to translate functional
to imperative programs. We have seen, how our language-
oriented compiler design enables the formalization of important
invariants and assumptions in the type system and prevents ad-
hoc implementation choices in the compilation process. Instead,
all implementation choices are pushed upwards and must have
been encoded explicitly beforehand.

VI. COMPILER EXTENSIBILITY AT MULTIPLE LEVELS

Extensibility is a crucial consideration in compiler design.
By having a slim core language, both RISE and DPIA are easy
to extend by adding new primitives. A crucial advantage over
purely functional IRs, such as LIFT, is that we can extend
DPIA with new imperative primitives, directly capturing code
patterns or interfaces in the target programming model.

We illustrate the extensibility by adding support for iterative
computations. LIFT proposes an iterate primitive, that repeat-
edly applies a function to an array, passing the output of one
iteration as the input of the next. This can be used to describe
tree-based reductions and similar iterative algorithms [31].

One efﬁcient way to implement such computations in a C-like
programming language is to use double buffering by swapping
pointers to two buffers after each iteration. But how would we
represent this in DPIA, as there is no support for dealing with
pointers directly? To solve this, we add two primitives to DPIA:
the functional iterate and the imperative newDoubleBuffer.

8

1
2
3
4
5
6
7
8
9
10
11
12

float buffer1[size]; float buffer2[size];
float* in_ptr = input; float* out_ptr = buffer1;
unsigned char flag = 1;
for (int i = 0; i < k; i += 1) {

body(in_ptr, out_ptr);
if (i <= (k-1)) {

// body
// swap

in_ptr = flag ? buffer1 : buffer2;
out_ptr = flag ? buffer2 : buffer1;
flag = flag ^ 1;

} else {

// done

in_ptr = flag ? buffer1 : buffer2;

out_ptr = output; } }

Listing 12: C code expressing double buffering.

To start, let’s look at the C code we would like to generate for
an iterative computation implemented using double buffering,
as shown in Listing 12. First we need to allocate temporary
buffers and then initialize two pointers in_ptr and out_ptr who
are pointing to the input and output of each iteration. The for
loop iterates for a ﬁxed number of k iterations and performs
the iterative computation in line 5. Afterwards, we need to
swap the pointers to prepare for the next iteration. Because
we want to avoid the need for unnecessary copies, in the ﬁrst
iteration in_ptr points to the input of the overall computation
and in the last out_ptr to the output. To achieve this, we use
a flag indicating how we have to swap and an if branch to
check if the last iteration has been reached.

When adding new primitives, we need to model these
implementation aspects in DPIA. For double buffering, we
want to avoid exposing pointers directly, and thus we hide the
pointer manipulation behind two commands that we expose in
DPIA. Listing 13 shows the added newDoubleBuffer primitive.
It expects multiple arguments: the data types and sizes of
the input, output, and temporary buffers, the input array, the
output array, and a body function that has access to the double
buffer. The body expects four arguments: an acceptor and
expression to write/read into/from the current active buffer,
and two commands corresponding to the swap and done code
parts from Listing 12. The body may use swap and done freely.
In the ﬁnal code generation process, these opaque commands
are replaced with the code shown in the listing.

Listing 14 shows the translation of the functional iterate
primitive (List. 13) to imperative DPIA. The iterate primitive
is translated into a combination of newDoubleBuffer, to allocate
a double buffer, and for, to express the iterative computation.
Speciﬁcally, the swap and done commands are used inside the
if, directly corresponding to the C code in Listing 12.

Summary

This case study of extensibility highlights two important
points: 1) the language-oriented compiler design with multiple
single-purpose IRs allows adding abstractions at multiple
levels, such as the high-level iterate and the low-level
newDoubleBuffer; 2) the abstractions stand on their own: we
did not design an imperative version of iterate, but instead
designed a double buffer abstraction that can be used in other
ways by using the exposed interface of swap and done.

newDoubleBuffer(t: DataType, n: Nat, m: Nat, k: Nat,
input: Exp[Array[m,t],Rd], output: Acc[Array[k,t]],
body: (Acc[Array[n,t] x Exp[Array[n,t],Rd] x Comm x Comm)

-> Comm): Comm

iterate(n: Nat, m: Nat, k: Nat, t: DataType,

body: (l:Nat)->Exp[Array[l*n,t],Rd]->Exp[Array[l,t],Wr],
arr: Exp[Array[(n^k)*m,t],Rd]): Exp[Array[m,t],Wr]

Listing 13: Extension to support iterative computations using
double buffering.

def accT(expr, output) = expr match {
case ...
case iterate(n, m, k, t, body, arr) =>

conT(arr, fun(arrT =>

newDoubleBuffer(t, (n^k)*m, m, (n^k)*m,

arr, output, fun((tmpAcc, tmpExp, swap, done) =>

for(k, fun(i =>

body(n^(k-i-1)*m, tmpAcc, tmpExp) ;
if (i < k) { swap } else { done } )))))

}

Listing 14: Extension of the acceptor translation

VII. EVALUATION

We now want to evaluate our language-oriented compiler
design. We will do this in two ways: 1) we will describe a
qualitative comparison with the LIFT compiler highlighting the
beneﬁts of our approach over a compiler with similar design
goals; 2) we perform an experimental evaluation to conﬁrm
that the code generated by the Shine compiler has the same
runtime performance as code generated by LIFT.

A. Qualitative comparison with LIFT

From the user perspective, the LIFT compiler and Shine
have the same goal and RISE has almost the same interface as
the LIFT IR. But their compiler design and implementations
differ greatly.

Speciﬁcally, LIFT does not follow a language-oriented
compiler design. It uses a single intermediate representation
that lacks a formal type system. For example, LIFT lacks the
concept of a function type despite primitives being modeled
as functions. This makes specifying primitives cumbersome
and the type inference implementation ad-hoc. Shine on the
other hand, is implemented with two distinct languages that
have designated purposes (optimizing vs. code generation).
This more modular design comes with signiﬁcant beneﬁts for
engineering, but also conceptually.

First, optimizing by rewriting is performed in a purely
functional language without compromising the language design
for other tasks. This is not the case in LIFT, where a single
IR is used for both tasks and primitives are modelled not
as functions, but as fully applied values, which makes the
implementation of a rewrite system inconvenient.

Second, DPIA – the language for code generation – formal-
izes and enforces the necessary assumptions of code generation.
This is not the case in LIFT where the compiler will fail with

9

by the lack of a memory re-use mechanism in Shine, whereas
the LIFT generated code uses less memory. Reusing memory
is currently not modelled in the more formal DPIA, but could
be added in the future.

To summarize, Shine shows the same performance as
the LIFT compiler, while following a much better and more
principled language-oriented compiler design.

VIII. RELATED WORK

Functional Compiler IRs: We have extensively compared
our work to the closest related work, LIFT [31, 32], throughout
this paper. There exist several similar compilers with functional
IRs, including Futhark [12], Accelerate [7], and Dex [21].
Delite [6] uses parallel patterns similar to RISE to provide a
framework for building DSL compilers. Lücke et al. presents an
implementation of a RISE-like language as a MLIR dialect [19].
The MLIR [16] dialect linalg cites LIFT as a direct inﬂuence.
Of course there exist other IRs that incorporate functional
aspects, such as the functional graph-based Thorin [17] IR that
provides explicit support for higher-order functions. Many
machine learning compilers, such as TensorFlow [1] and
PyTorch [20], follow a functional graph-based IR that is
inspired by ideas from data-ﬂow programming.

Extensible Compilers: Rompf et al. present a technique
based on staging for extensible compiler design [26]. Similarly,
the Any DSL framework sees partial evaluation as the key to
expressing compiler optimizations as easily extensible library
code rather than internal IR transformations [18]. AnyDSL
uses the functional Thorin IR mentioned above. Koehler and
Steuwer argues for domain extensible compilers by extending
the compiler with new image processing speciﬁc primitives
and rewrite rules [14].

IX. CONCLUSION
In this paper, we present the Shine compiler that follows a
language-oriented compiler design. The compiler is composed
of multiple formal programming languages that are used as
intermediate representations.

Each language has a clear purpose, separating optimizing
clearly from the code generation process. This simpliﬁes both
phases: optimizing is expressed exclusively by rewriting the
higher-level functional RISE program encoding implementation
and optimization choices; code generation bridges from the
functional to the imperative paradigm and preserves the encoded
implementation choices.

DPIA, the language for code generation, formalizes and
checks the assumption that all implementation choices have
been made before the code generation process starts.

By composing multiple languages, the compiler becomes
easily extensible at various abstraction levels. We demonstrated
extending with an imperative primitive that captures a desirable
low-level double buffering code pattern.

Finally, our evaluation showed the beneﬁts of our compiler
design over the closely related LIFT compiler while maintaining
the same runtime code performance.

Fig. 5: Relative runtime performance of the code generated by
Shine and LIFT. A higher bar indicates better performance.

internal errors or is forced to make ad-hoc implementation
choices when these have not been encoded in the input program.
Finally, extensibility in LIFT is limited to functional prim-
itives, whereas in Shine we can easily extend DPIA with
imperative primitives that provide a safe interface re-usable for
compiling many high-level primitives. The bottom up process
of ﬁrst establishing an imperative primitive in DPIA and then
thinking about how to compile a functional primitive to it,
provides a principled pathway to incorporate implementation
patterns and library interfaces of the low-level target language.

B. Runtime performance compared to LIFT

We now evaluate the quality of the code generated by Shine
and LIFT using the benchmarks used in a previous LIFT
paper [32]. We like to thank the authors for making their artifact
publicly available. Figure 5 shows the runtime performance on
two Nvidia GPUs. The benchmarks use the same optimizations
in both compilers, even though LIFT makes some ad-hoc
implementation choices, e.g., about memory allocation and
vectorization, internally in the compiler rather than encoding
them in the functional program as we do in Shine.

We expect that the generated code has the same performance
for both compilers, as (mostly) the same optimizations are
applied. For the mean across all benchmarks, this is indeed
the case: the code generated by Shine is slightly faster (about
5%). There are two signiﬁcant outliers, for which Shine
generates signiﬁcantly faster code: MD and N-Body AMD. Both
benchmarks use vectorization and receive as input or produce
as output arrays of float4 values. While Shine honors this
interface and generates OpenCL kernels with float4 pointers,
the LIFT compiler produces code with float pointers and uses
vload/vstore vector instructions to load/store values from
these arrays. In this case, this small code difference produces a
signiﬁcant performance gap. The NBody NVIDIA benchmark
shows a clear performance beneﬁt for the LIFT generated code.
After inspection of the generated code, this can be explained

10

N−BodyNVIDIAN−BodyAMDMDK−MeansNNMRI−QConvATAXGEMVGESUMMVMMMeanTitan XpTesla K40csmalllargesmalllargesmalllargesmalllargesmalllargesmalllargesmalllargesmalllargesmalllargesmalllargesmalllargemean0.51.02.00.51.02.0input sizeLiftShineREFERENCES

[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng
Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay
Ghemawat, Geoffrey Irving, Michael Isard, Manjunath
Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore,
Derek Gordon Murray, Benoit Steiner, Paul A. Tucker,
Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan
Yu, and Xiaoqiang Zheng. Tensorﬂow: A system for
large-scale machine learning. In OSDI, pages 265–283.
USENIX Association, 2016.

[2] Bowen Alpern, Mark N. Wegman, and F. Kenneth Zadeck.
Detecting equality of variables in programs. In POPL,
pages 1–11. ACM Press, 1988.

[3] Andrew W. Appel. SSA is functional programming. ACM

SIGPLAN Notices, 33(4):17–20, 1998.

[4] Robert Atkey, Michel Steuwer, Sam Lindley, and
Christophe Dubach. Strategy preserving compilation for
parallel functional code. CoRR, abs/1710.08332, 2017.

[5] Martin Bravenboer, Karl Trygve Kalleberg, Rob Vermaas,
and Eelco Visser. Stratego/xt 0.17. A language and toolset
for program transformation. Sci. Comput. Program., 72
(1-2):52–70, 2008.

[6] Kevin J. Brown, HyoukJoong Lee, Tiark Rompf, Arvind K.
Sujeeth, Christopher De Sa, Christopher R. Aberger, and
Kunle Olukotun. Have abstraction and eat performance,
too: optimized heterogeneous computing with parallel
patterns. In CGO, pages 194–205. ACM, 2016.

[7] Manuel M. T. Chakravarty, Gabriele Keller, Sean Lee,
Trevor L. McDonell, and Vinod Grover. Accelerating
haskell array codes with multicore gpus. In DAMP, pages
3–14. ACM, 2011.

[8] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin
Zheng, Eddie Q. Yan, Haichen Shen, Meghan Cowan,
Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and
Arvind Krishnamurthy. TVM: an automated end-to-end
optimizing compiler for deep learning. In OSDI, pages
578–594. USENIX Association, 2018.

[9] Bastian Hagedorn, Larisa Stoltzfus, Michel Steuwer,
Sergei Gorlatch, and Christophe Dubach. High perfor-
mance stencil code generation with lift. In CGO, pages
100–112. ACM, 2018.

[10] Bastian Hagedorn, Johannes Lenfers, Thomas Koehler,
Xueying Qin, Sergei Gorlatch, and Michel Steuwer.
Achieving high-performance the functional way: a func-
tional pearl on expressing high-performance optimizations
as rewrite strategies. Proc. ACM Program. Lang., 4(ICFP):
92:1–92:29, 2020.

[11] John L. Hennessy and David A. Patterson. A new golden
age for computer architecture. Commun. ACM, 62(2):
48–60, 2019.

[12] Troels Henriksen, Niels G. W. Serup, Martin Elsman,
Fritz Henglein, and Cosmin E. Oancea. Futhark: purely
functional gpu-programming with nested parallelism and
in-place array updates. In PLDI, pages 556–571. ACM,
2017.

[13] Richard Kelsey. A correspondence between continuation
passing style and static single assignment form.
In
Intermediate Representations Workshop, pages 13–23.
ACM, 1995.

[14] Thomas Koehler and Michel Steuwer. Towards a domain-
extensible compiler: Optimizing an image processing
pipeline on mobile cpus. In CGO, pages 27–38. IEEE,
2021.

[15] Chris Lattner and Vikram S. Adve. LLVM: A compilation
framework for lifelong program analysis & transformation.
In CGO, pages 75–88. IEEE Computer Society, 2004.

[16] Chris Lattner, Mehdi Amini, Uday Bondhugula, Albert
Cohen, Andy Davis, Jacques A. Pienaar, River Riddle,
Tatiana Shpeisman, Nicolas Vasilache, and Oleksandr
Zinenko. MLIR: scaling compiler infrastructure for
domain speciﬁc computation. In CGO, pages 2–14. IEEE,
2021.

[17] Roland Leißa, Marcel Köster, and Sebastian Hack. A
graph-based higher-order intermediate representation. In
CGO, pages 202–212. IEEE Computer Society, 2015.

[18] Roland Leißa, Klaas Boesche, Sebastian Hack, Arsène
Pérard-Gayot, Richard Membarth, Philipp Slusallek, An-
dré Müller, and Bertil Schmidt. Anydsl: a partial
evaluation framework for programming high-performance
libraries. Proc. ACM Program. Lang., 2(OOPSLA):119:1–
119:30, 2018.

[19] Martin Lücke, Michel Steuwer, and Aaron Smith. Inte-
grating a functional pattern-based IR into MLIR. In CC,
pages 12–22. ACM, 2021.

[20] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,
Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison,
Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An
imperative style, high-performance deep learning library.
CoRR, abs/1912.01703, 2019.

[21] Adam Paszke, Daniel Johnson, David Duvenaud, Dim-
itrios Vytiniotis, Alexey Radul, Matthew Johnson,
Jonathan Ragan-Kelley, and Dougal Maclaurin. Getting to
the point. index sets and parallelism-preserving autodiff
for pointful array programming. Proc. ACM Program.
Lang., 5(ICFP), 2021.

[22] Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams,
Sylvain Paris, Frédo Durand, and Saman P. Amarasinghe.
Halide: a language and compiler for optimizing paral-
lelism, locality, and recomputation in image processing
pipelines. In PLDI, pages 519–530. ACM, 2013.
[23] Fabrice Rastello and Florent Bouchez Tichadou, editors.

SSA-based Compiler Design. Springer, 2018.

[24] Toomas Remmelg, Thibaut Lutz, Michel Steuwer, and
Christophe Dubach. Performance portable GPU code
generation for matrix multiplication. In GPGPU@PPoPP,
pages 22–31. ACM, 2016.

[25] John C. Reynolds. The Essence of Algol, pages 67–88.

Birkhäuser Boston, 1997.

11

[26] Tiark Rompf, Arvind K. Sujeeth, Nada Amin, Kevin J.
Brown, Vojin Jovanovic, HyoukJoong Lee, Manohar
Jonnalagedda, Kunle Olukotun, and Martin Odersky.
Optimizing data structures in high-level programs: new
directions for extensible compilers based on staging. In
POPL, pages 497–510. ACM, 2013.

[27] Barry K. Rosen, Mark N. Wegman, and F. Kenneth
Zadeck. Global value numbers and redundant computa-
tions. In POPL, pages 12–27. ACM Press, 1988.
[28] Sven-Bodo Scholz. On programming scientiﬁc ap-
plications in SAC - A functional language extended
by a subsystem for high-level array operations.
In
Implementation of Functional Languages, volume 1268
of Lecture Notes in Computer Science, pages 85–104.
Springer, 1996.

[29] Guy Lewis Steele. Lambda: The ultimate declarative.

volume 379, 1976.

[30] Guy Lewis Steele and Gerald Jay Sussman. Lambda: The

ultimate imperative. volume 353, 1976.

[31] Michel Steuwer, Christian Fensch, Sam Lindley, and
Christophe Dubach. Generating performance portable
code using rewrite rules: from high-level functional
expressions to high-performance opencl code. In ICFP,
pages 205–217. ACM, 2015.

[32] Michel Steuwer, Toomas Remmelg, and Christophe
Dubach. Lift: a functional data-parallel IR for high-
performance GPU code generation. In CGO, pages 74–85.
ACM, 2017.

12

