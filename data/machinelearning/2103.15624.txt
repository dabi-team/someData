1
2
0
2

y
a
M
1
3

]
E
N
.
s
c
[

2
v
4
2
6
5
1
.
3
0
1
2
:
v
i
X
r
a

Shape-constrained Symbolic Regression –
Improving Extrapolation with Prior
Knowledge

G. Kronberger
Josef Ressel Center for Symbolic Regression, University of Applied Sciences Upper
Austria, Softwarepark 11, 4232 Hagenberg, Austria

gabriel.kronberger@fh-ooe.at

F. O. de Franca
Center for Mathematics, Computation and Cognition (CMCC), Heuristics, Analysis
and Learning Laboratory (HAL), Federal University of ABC, Santo Andre, Brazil
B. Burlacu and C. Haider and M. Kommenda
Josef Ressel Center for Symbolic Regression, University of Applied Sciences Upper Aus-
tria, Softwarepark 11, 4232 Hagenberg, Austria

folivetti@ufabc.edu.br

Abstract

We investigate the addition of constraints on the function image and its derivatives for
the incorporation of prior knowledge in symbolic regression. The approach is called
shape-constrained symbolic regression and allows us to enforce e.g. monotonicity of
the function over selected inputs. The aim is to ﬁnd models which conform to expected
behaviour and which have improved extrapolation capabilities. We demonstrate the
feasibility of the idea and propose and compare two evolutionary algorithms for shape-
constrained symbolic regression: i) an extension of tree-based genetic programming
which discards infeasible solutions in the selection step, and ii) a two population
evolutionary algorithm that separates the feasible from the infeasible solutions. In
both algorithms we use interval arithmetic to approximate bounds for models and
their partial derivatives. The algorithms are tested on a set of 19 synthetic and
four real-world regression problems. Both algorithms are able to identify models
which conform to shape constraints which is not the case for the unmodiﬁed symbolic
regression algorithms. However, the predictive accuracy of models with constraints is
worse on the training set and the test set. Shape-constrained polynomial regression
produces the best results for the test set but also signiﬁcantly larger models.1

Keywords

Symbolic regression, Genetic programming, Shape-constrained regression

1

Introduction and Motivation

Dynamical systems and processes in critical application areas such as control engineer-
ing require trustworthy, robust and reliable models that strictly conform to behavioral
expectations according to domain-speciﬁc knowledge, as well as safety and performance
criteria. However, such models cannot always be derived from ﬁrst principles and the-
oretical considerations alone. Thus, they must be determined empirically. From this
perspective, models used in system identiﬁcation can be categorized as:

1This work has been accepted to be published by Evolutionary Computation, MIT Press

 
 
 
 
 
 
• White-box. Derived from ﬁrst principles, explicitly include domain knowledge, can
be valid for a wide range of inputs even beyond available observations and allow to
make far-reaching predictions (extrapolation).

• Gray-box. Derived from data, with known internals, open for further inspection.
Can be validated against domain knowledge. Only valid for inputs which are similar
to observations that are used for model ﬁtting.

• Black-box. Derived from data, with unknown internals (e.g., neural networks),

diﬃcult or impossible to inspect and/or validate.

In-between the two extremes there is a whole spectrum of gray-box models which
combine aspects of both extremes (Ljung, 2010). The proponents of “explainable AI”
propose to apply models on the white end of this spectrum either directly (Rudin, 2019)
or as an approximation of black-box models in AI technologies.

Empirical, data-driven models are required in scenarios where ﬁrst principles models
are infeasible due to engineering or ﬁnancial considerations. Ensuring that empirical
models conform to expected system behavior, correctly reﬂect physical principles, and
can extrapolate well on unseen data remains an open challenge in this area.

Symbolic regression (SR) is especially interesting in this context because SR models
are closed-form expressions which are structurally similar to white-box models derived
from ﬁrst principles. The aim in SR is to identify the best function form and parameters
for a given data set using common mathematical operators and functions which serve
as building blocks for the function form (Koza, 1992). This is in contrast to other forms
of regression analysis where the function form is pre-speciﬁed and only the numerical
coeﬃcients are optimized through model ﬁtting.

It should be noted however that SR is fundamentally a purely data-driven approach.
It does not require prior information about the modelled process or system and leads
to gray-box models which can be hard to understand. SR algorithms therefore favor
shorter expressions and use simpliﬁcation to facilitate detailed analysis of their models.
The most well-known solution methods for SR includes tree-based genetic pro-
gramming (GP) variants (Koza, 1992), linear GP (Oltean and Grosan, 2003) includ-
ing Pareto-GP (Smits and Kotanchek, 2005), Cartesian GP (Miller and Harding, 2008),
grammatical evolution (GE) (O’Neill and Ryan, 2001), and gene expression programming
(GEP) (Ferreira, 2001). More recent developments of SR include Geometric Semantic
GP (Moraglio et al., 2012; Pawlak and Krawiec, 2018; Ruberto et al., 2020), Multiple-
regression GP (Arnaldo et al., 2014), and a memetic evolutionary algorithm using a novel
fractional representation for SR (Sun and Moscato, 2019). There are also several non-
evolutionary algorithms for SR including fast function extraction (FFX) (McConaghy,
2011), SymTree (de Fran¸ca, 2018), prioritized grammar enumeration (PGE) (Worm and
Chiu, 2013). Recently several neural network architectures have been used for symbolic
regression (Sahoo et al., 2018; Kim et al., 2019; Petersen et al., 2019; Udrescu and
Tegmark, 2020).

This contribution introduces an extension of SR, which allows to include vague prior
knowledge via constraints on the shape of SR model image and derivatives. We therefore
call this approach shape-constrained SR as it is a speciﬁc form of shape-constrained
regression. The main motivation is, that even in data-based modelling tasks, partial
knowledge about the modelled system or process is often available, and can be helpful
to produce better models which conform to expected behaviour and might improve
extrapolation. Such information could be used for:

2

1. data augmentation to decrease the eﬀects of noisy measurements or for detecting

implausible outliers

2. ﬁtting the model in input space regions where observations are scarce or unavailable

3. distinguishing between correlations in observation data and causal dependencies

4. increasing the eﬃciency of the statistical modelling technique.

1.1 Problem Statement
We focus on the integration of prior knowledge in SR solvers in order to ﬁnd expressions
compatible with the expected behavior of the system.

Our assumptions are that we have data in the form of measurements as well as
side information about the general behaviour of the modelled system or process that
however is insuﬃcient to formulate an explicit closed-form expression for the model. We
additionally assume that the information can be expressed in the form of constraints on
the function output as well as its partial derivatives.

The task is therefore to ﬁnd a closed-form expression which ﬁts the data, possibly
with a small error, and conforms to our expectation of the general behaviour of the
system. In particular, the model must not violate shape constraints when interpolating
and extrapolating within a bounded input space.

We investigate the hypotheses that (i) interval arithmetic can be used to approx-
imate bounds for the image of SR models and their partial derivatives and therefore
reject non-conforming models within SR solvers, and (ii) that the predictive accuracy of
SR models can be improved by using side information in the form of shape constraints.

2 Related Work

The issue of knowledge integration into machine learning methods including genetic
programming has been discussed before.

Closely related to this work is the work of Bladek and Krawiec (2019) who have
discussed using formal constraints in combination with symbolic regression to include
domain knowledge about the monotonicity, convexity or symmetry of functions. Their
approach called Counterexample-Driven GP utilizes satisﬁability modulo theories (SMT)
solvers to check whether candidate SR models satisfy the constraints and to extend the
training set with counter examples. Bladek and Krawiec (2019) observe that is very
unlikely for GP to synthesize models which conform to constraints. The diﬀerence to
this work is that we do not support symmetry constraints and instead of a SAT solver
we use interval arithmetic to calculate bounds for the function image and its partial
derivatives. We test our approach on a larger set of harder benchmark problems.

Versino et al. (2017) have speciﬁcally studied the generation of ﬂow stress models
for copper using GP with and without knowledge integration. They have investigated
four diﬀerent scenarios: i) a canonical GP solver that returns a Pareto front of model
quality and complexity, ii) knowledge integration by means of transformed variables,
samples weighting, introduction of artiﬁcial points, introduction of seed solutions, iii)
search for the derivative of the expression and then reconstructing the original equation
by integration, iv) search derivatives but with additional artiﬁcial data points.

This work emphasized the importance of integrating prior knowledge in the model
ﬁtting process. The authors observed that the canonical GP solver presented unpre-
dictable behavior w.r.t. the physical constraints, thus the sampled data set alone was
insuﬃcient to guide the solver into a model with both low error and physical feasibility.

3

In contrast to our approach which guarantees that solutions conform to prior knowledge,
all techniques tried in (Versino et al., 2017) are optimistic and might produce infeasible
solutions. Additionally, adding artiﬁcial data points is subjective and does not scale to
high-dimensional problems.

Schmidt and Lipson (2009) studied the inﬂuence of integrating expert knowledge
into the evolutionary search process through a process called seeding. First the authors
generated approximation solutions by either solving a simpler problem or ﬁnding an
approximate solution to a more complex problem. These solutions are then used during
the seeding procedure by inserting approximations into the initial population, shuﬄed
expressions and building blocks. The authors found that seeding signiﬁcantly improves
the convergence and ﬁtness average performance when compared to no seeding. Among
the seeding procedures, the seeding of building blocks was the most successful allowing
faster convergence even in more complex problems. Again, seeding is an optimistic
approach and does not guarantee that the ﬁnal SR solution conforms to prior knowledge.
Stewart and Ermon (2017) investigated how to use prior knowledge in artiﬁcial
neural networks. Speciﬁcally, they incorporate physical knowledge into motion detection
and tracking and causal relationships in object detection.

Even though the experiments were all small scaled, they showed promising results
and opened up the possibility of teaching an artiﬁcial neural network by only describing
the properties of the approximation function. This idea is similar to the idea of shape-
constraints.

Recently an approach for knowledge integration for symbolic regression with a sim-
ilar motivation to this work has been described in (Li et al., 2019). Instead of shape
constraints the authors make use of semantic priors in the form of leading powers for
symbolic expressions. They describe a Neural-Guided Monte Carlo Tree Search (MCTS)
algorithm to search of SR models which conform to prior knowledge. The authors use a
context-free grammar for generating symbolic expressions and use MCTS for generating
solutions whereby a neural network predicts the next production rule given a sequence
of already applied rules and the leading power constraints.

The algorithm was compared against variations of MCTS and a GP implementation

using the DEAP framework (Fortin et al., 2012)

The results show a superior performance of neural-guided MCTS with a high success
rate for easier instances but much lower rates on harder instances which were however
still higher than the success rate of GP.

One aim of our work is improving extrapolation of SR solutions. Similarly to poly-
nomial regression, SR models might produce extreme outputs values when extrapolating.
Interestingly, extrapolation behaviour of SR models has been largely ignored in the lit-
erature with only a few exceptions.

Castillo et al. (2013) investigated the diﬀerence in the response variation of diﬀerent
models generated by GP to three diﬀerent data sets. They have shown that in some
situations a GP algorithm can generate models with diﬀerent behavior while presenting
a similar R2 value. These results motivate the need of a post analysis of the validity of
the model w.r.t. the system being studied and a veriﬁcation of how well such models
extrapolate.

The extrapolation capability of GP is also studied in (Castillo et al., 2003). In this
work, the authors evaluate and compare the best solution obtained by a GP algorithm
to a linear model which uses the transformed variables found by the GP model.

The experiments suggest that the GP models have good interpolation capabilities
but with a moderate extrapolation error. The proposed approach presented good inter-

4

polation and extrapolation capabilities, suggesting that at the very least, the GP models
can guide the process of building a new transformed space of variables.

Another relevant result was obtained by Kurse et al. (2012) who used Eureqa (a
commercial SR solver) to ﬁnd analytical functions that correctly modelled complex neu-
romuscular systems. The experiments showed that the GP solver was capable of ex-
trapolating the data set much better than the traditional polynomial regression, used
to model such systems.

Stewart and Ermon (2017) trained a neural network by introducing prior knowledge
through a penalty term of the loss function such that the generated model is consistent
to a physics behavior. Diﬀerent from our work, this generates a black box model of the
data. Finally, Zhu et al. (2019) incorporate the appropriate partial diﬀerential equations
into the loss functions of a neural network and, by doing so, they can train their model
on unlabeled data, this approach requires more information than usually available when
modelling studied observations.

3 Methods

In this section we describe in detail the methods proposed in this paper to integrate
prior knowledge into SR solvers using interval arithmetic and shape constraints.
In
summary, we extend two GP variants to support shape-constrained SR: a tree-based GP
approach with optional memetic improvement of models parameters and the Interaction-
Transformation Evolutionary Algorithm (ITEA) (de Franca and Aldeia, 0). The ex-
tended algorithms are compared against the original versions on a set of benchmark
problems. For the comparison we calculate the number of constraint violations and the
training as well as the test errors and qualitatively assess the extrapolation behaviour.

3.1 Shape-constrained Regression
Shape-constrained regression allows to ﬁt a model whereby certain characteristics of the
model can be constrained. It is a general concept, that encompasses a diverse set of meth-
ods including parametric and non-parametric, as well as uni-variate and multi-variate
methods. Examples include isotonic regression (Wright et al., 1980; Tibshirani et al.,
2011), monotonic lattice regression (Gupta et al., 2016), nonparametric shape-restricted
regression (Guntuboyina et al., 2018), non-negative splines (Papp and Alizadeh, 2014),
and shape-constrained polynomial regression (Hall, 2018).

In shape-constrained regression, the model is a function mapping real-valued inputs
to a real-valued output and the constraints refer to the shape of the function. For
instance if the function f (x) should be monotonically increasing over a given input
variable x1 then we would introduce the monotonicity constraint

∂f
∂x1

(x) ≥ 0, x ∈ S ⊆ Rd

Similarly, we could enforce concavity/convexity of a function by constraining second
order derivatives. Usually the input space for the model is limited for example to a
d-dimensional box S = [inf 1, sup
1

] × [inf 2, sup
2
In general it is possible to include constraints for the model and its partial derivatives
of any order. However, in practice ﬁrst and second order partial derivatives are most
relevant. The set of constraints C contains expressions that are derived from the model
via one of the operators Op and linearly transformed using a sign s and threshold c.
We use this representation of shape constraints to simplify checking of constraints. All

] × · · · × [inf d, sup
d

] ⊆ Rd.

5

constraint expressions ci ∈ C must not be positive for feasible solutions.

(cid:26)

C =

s · Op(f )(x) − c ≤ 0

Op ∈

(cid:26)

id(f ),

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂nf
∂nx1

, . . .

(cid:27)

∂nf
∂nxd

, c ∈ R, s ∈ {1, −1}, n > 0

(cid:27)

It is important to mention that shape constraints limit the function outputs and
partial derivatives, but the optimal function which ﬁts the observed data still needs to
be identiﬁed by the algorithm. Shape-constrained regression is a general concept which
is applicable to diﬀerent forms of regression analysis. For speciﬁc models like multi-
variate linear regression or tree-based methods (e.g., XGBoost), it is easy to integrate
shape constraints. For other models, for instance, polynomial regression, it is harder to
incorporate shape constraints (Hall, 2018; Ahmadi and Majumdar, 2019).

Generally, one can distinguish optimistic and pessimistic approaches to shape-
constrained regression (Gupta et al., 2016). Optimistic approaches check the constraints
only for a ﬁnite set of points from the input space and accept that the identiﬁed model
might violate the constraints for certain elements from the input space. Pessimistic
approaches calculate bounds for the outputs of the model and its partial derivatives to
guarantee that identiﬁed solutions are in fact feasible. However, pessimistic approaches
might reject optimal models as a consequence of overly wide bounds. In this paper, we
decided to study a pessimistic approach and calculate bounds for SR models and their
derivatives using interval arithmetic.

3.2 Interval Arithmetic
Interval Arithmetic (IA) is a method for calculating output ranges for mathematical
expressions (Hickey et al., 2001). It has many uses such as dealing with uncertainties
stemming from inaccurate representations and verifying boundary conditions. An inter-
val is represented as [a, b] with a ≤ b, named lower and upper endpoints, respectively.

If we have a function f (x1, x2) = x1 + x2, for example, and knowing the intervals
for each variable to be x1 ∈ [a, b], x2 ∈ [c, d], we can say that the image of function f
will be in the interval [a + c, b + d]. IA deﬁnes the common mathematical operations
and functions for such intervals. It is easy to see that IA can only give bounds for the
expression instead of an accurate range because it does not track dependencies between
arguments of operators. For instance, the IA result for the expression x1 − x1 with
x1 ∈ [a, b] is the interval [a − b, b − a] instead of [0, 0].

IA has previously been used for SR for improving model quality as it allows to detect
and reject partially deﬁned functions resulting e.g. from division by zero or taking the
logarithm of a non-positive number. The use of IA for SR was ﬁrst proposed in (Keijzer,
2003) where IA was used to verify that a given expression is deﬁned within the domain
spanned by the variable values in the data set. The author showed that this indeed
helps avoiding partially deﬁned functions even when using standard division instead of
the commonly used protected division operator.

Pennachin et al. (2010) used Aﬃne Arithmetic, an extension to IA, for ﬁnding
robust solutions. The authors observed that the models generated by their algorithm
were robust w.r.t. extrapolation error.

In (Dick, 2017) the approach of (Keijzer, 2003) was extended by introducing
crossover and mutation operators that make use of IA to generate feasible expressions.
The experiments showed a faster convergence rate with interval-aware operators.

We also include IA into SR. Our work is in fact a generalization of the idea dis-
cussed in (Keijzer, 2003) and (Pennachin et al., 2010), but instead of limiting the use
of IA to detect only whether a function is partial or not, we also expand it to test for

6

other properties such as monotonicity. Another diﬀerence is that we assume that the
considered intervals are derived from prior knowledge, instead of being inferred from the
data.

3.3 Genetic Programming for Shape-constrained Symbolic Regression
We integrate IA into two solvers for SR. In this section we describe the integration into
tree-based GP (Koza, 1992) and in the next section we describe the integration into
ITEA.

We use a tree-based GP variation with optional memetic local optimization of SR
model parameters which has produced good results for a diverse set of regression bench-
mark problems (Kommenda et al., 2020).

The pseudo-code for our GP algorithm for shape-constrained symbolic regression
is shown in Algorithm 1. Solution candidates are symbolic expressions encoded as ex-
pression trees. Parameters of the algorithm are the function set F, the terminal set
T , the maximum length (number of nodes) Lmax and maximum depth Dmax of expres-
sion trees, the number of generations Gmax, the population size N , the mutation rate
m, the tournament group size p, and the number of iterations for memetic parameter
optimization nOpt. The algorithm uses tournament selection, generational replacement
and elitism. New solution candidates are produced via sub-tree crossover and optional
mutation. The initial population is generated with the PTC2 algorithm (Luke, 2000).
All evolutionary operators respect the length and depth limits for trees.

Shape constraints are handled during ﬁtness evaluation. First we calculate intervals
for the function output as well as the necessary partial derivatives using the known inter-
vals for each of the input variables and check the output intervals against the constraint
bounds. If a solution candidate violates any of the shape constraints the solution candi-
date is assigned the worst possible ﬁtness value. In a second step, the prediction error
is calculated only for the remaining feasible solution candidates. The solution candidate
with the smallest error within a group is selected as the winner in tournament selection.
We assume that the constraints are provided via a set C as given in Section 3.1.

3.3.1 Fitness Evaluation
Function Evaluate calculates the vector of residuals from the predictions for inputs X
and the observed target values y. We use the normalized mean of squared errors (NMSE)
as the ﬁtness indicator. NMSE is the mean of squared residuals scaled with the inverse
variance of y. As described in (Keijzer, 2003) we implicitly scale all symbolic regression
solutions to match the mean and variance of y. Therefore, we know that the worst
possible prediction (i.e. a constant value) has an NMSE of one and we use this value for
infeasible solution candidates.

3.3.2 Optional Local Optimization
Procedure Optimize locally improves the vector of numerical coeﬃcients θ ∈ Rdim of each
model using non-linear least-squares ﬁtting using the Levenberg-Marquardt algorithm.
It has been demonstrated that gradient-based local improvement improves symbolic
regression performance (Topchy and Punch, 2001; Kommenda et al., 2020). Here we
want to investigate whether it can also be used in combination with shape constraints.
To test the algorithms with and without local optimization we have included the number
of local improvement iterations nOpt as a parameter for the GP algorithm.

The local improvement operator extracts the initial parameter values θinit from the
solution candidates and updates the values after optimization. Therefore, improved
parameter values θ? become part of the genome and can be inherited to new solution

7

Algorithm 1: GP(F, T , Lmax, Dmax, Gmax, N, m, p, nOpt)

X and y are input and target variable values, C is the vector of constraints
(operators and thresholds), Intervals are intervals for all input variables, pop
and nextPop are solution candidate vectors, and err and nextErr are error
vectors of solution candidates
pop ← InitRandomPopulation(F, T , Lmax, Dmax);
for i ← 1 to N do

err[i] ← Evaluate(pop[i], X, y, C, Intervals)

for g ← 1 to Gmax do

nextPop[1 ], nextErr[1 ] ← FindElite(pop, err);
for i ← 2 to N do

parent 1 ← SelectTournament(pop, err, p);
parent 2 ← SelectTournament(pop, err, p);
child ← Crossover(parent 1, parent 2, Lmax, Dmax);
if rand() < m then

Mutate(child, F, T , Lmax, Dmax)

if nOpt > 0 then

Optimize(child, nOpt, X, y, C, Intervals)

nextPop[i] ← child;
nextErr[i] ← Evaluate(child, X, y, C, Intervals)

pop ← nextPop;
err ← nextErr;

best ← pop[1 ];
return best

Function Evaluate(expr, X, y, C, Intervals)

V ← {c ∈ C | evalConstraint(c, Intervals) > 0};
if V 6= ∅ then
nmse ← 1

/* return worst nmse for expr with violations */

else

s ← ky − mean(y)k2 ;
nmse ← min(s−1keval(expr, X) − yk2, 1) ;

/* normalization factor */
/* worst nmse is 1 */

return nmse

candidates (Lamarckian learning (Houck et al., 1997)).

3.4 Interaction-Transformation Evolutionary Algorithm

The second SR method that we have extended for shape-constrained SR is the
Interaction-Transformation Evolutionary Algorithm (ITEA). ITEA is an evolutionary
algorithm that relies on mutation to search for an optimal Interaction-Transformation ex-
pression (de Fran¸ca, 2018) that ﬁts a provided data set. The Interaction-Transformation
(IT) representation restricts the search space to simple expressions following a common
pattern that greatly simpliﬁes optimization of model parameters and handling shape
constraints. The IT representation speciﬁes a pattern of function forms:

8

Procedure Optimize(expr, nOpt, X, y C, Intervals)

θinit ← ExtractParam(expr) ;
fx ← (θi) ⇒ begin

UpdateParam(expr,θi);
eval(expr, X)

/* use initial params from expr */
/* eval expression for θi */

θ? ← LevenbergMarquardt(θinit, nOpt, y, fx);
UpdateParam(expr, θ?);

Algorithm 2: ITEA (de Franca and Aldeia, 0).

input : data points X and corresponding set of target variable y.
output: symbolic function f
pop ← GenRandomPopulation();
while stopping criteria not met do

children ← Mutate(pop);
pop ← Select(pop, children);

return arg maxp.f it pop;

ˆf (x) = X

wi · ti(

d
Y

xkij
j

),

i

j=1

where wi is the weight of the i-th term of the linear combination, ti is called a transfor-
mation function and can be any univariate function that is reasonable for the studied
system and kij is the strength of interaction for the j-th variable on the i-th term.

The algorithm is a simple mutation-based evolutionary algorithm depicted in Algo-
rithm 2. It starts with a random population of expressions and repeats mutation and
selection until convergence.

The mutation procedure chooses one of the following actions at random:

• Remove one random term of the expression.

• Add a new random term to the expression.

• Replace the interaction strengths of a random term of the expression.

• Replace a random term of the expression with the positive interaction of another

random term.

• Replace a random term of the expression with the negative interaction of another

random term.

The positive and negative interactions of terms is the sum or subtraction of the
, and
strengths of the variables. For example, if we have two terms x2
we want to apply the positive interaction to them, we would perform the operation
, similarly with the negative interactions we divide
1 · x−1
x2
1
, resulting
the ﬁrst polynomial by the other, so performing the operation x2
in x3

that results in x1x4
2

and x−1

2 · x−1
2

2 · x1
2

1 x1
2

1 · x3

1 · x1

1x3
2

· x3

.

1x2
2

9

Algorithm 3: FI-2POP-IT: Interaction-Transformation Evolutionary Algo-
rithm with Feasible-Infeasible Two-Population.

input : data points X and corresponding set of target variable y.
Data: feas is the feasible population, infeas is the infeasible population,

child-feas and child-infeas are the feasible and infeasible child
population, respectively.

output: symbolic function f
feas, infeas ← GenRandomPopulation();
while stopping criteria not met do

child-feas, child-infeas ← Mutate(feas ∪ infeas);
feas, infeas ← Select(feas ∪ child-feas ∪ infeas ∪ child-infeas);

return arg maxs∈feas ﬁtness(s);

Weights within IT expressions can be eﬃciently optimized using ordinary least

squares (OLS) after the mutation step.

This algorithm was reported to surpass diﬀerent variants of GP and stay on par

with nonlinear regression algorithms (de Franca and Aldeia, 0).

3.5 Feasible-Infeasible Two-population with Interaction-Transformation
In order introduce the shape-constraints into ITEA, we will use the approach called
Feasible-Infeasible Two-population (FI-2POP) (Kimbrough et al., 2008) that works with
two populations: one for feasible models and another for infeasible models. Despite its
simplicity, it has been reported to work well on diﬀerent problems (Liapis et al., 2015;
Scirea et al., 2016; Cov˜oes and Hruschka, 2018). We have chosen this approach to
deal with constraints for the ITEA algorithm since it does not demand ﬁne tuning of a
penalty function and a penalty coeﬃcient, does not require any change to the original
algorithm (as the repairing and constraint-aware operators), and does not introduce a
high computational cost (i.e., multi-objective optimization).

Speciﬁcally for ITEA, the adaptation (named FI-2POP-IT) is depicted in Algo-
rithm 3. The main diﬀerence from Algorithm 2 is that in FI-2POP-IT every function
now produces two distinct populations, one for feasible solutions and another for the
infeasible solutions. For example, the mutation operator when applied to an individual
can produce either a feasible solution or an infeasible one, this new individual will be
assigned to its corresponding population.

The ﬁtness evaluation diﬀers for each population, the feasible population minimizes

the error function while the infeasible population minimizes the constraint violations.

3.5.1 Deﬁning Constraints with Interval Arithmetic
The IT representation allows for a simple algorithm to calculate the n-th order partial
derivatives of the expression and to perform IA. Additionally, only linear real-valued
parameters are allowed for IT expressions. Therefore parameters can be optimized eﬃ-
ciently using ordinary least squares (OLS) and all shape constraints can be transformed
to functions which are linear in the parameters.

Given an IT expression and the set of domains for each of the input variables, it
is possible to calculate the image of the expression as the sum of the images of each
term multiplied by their corresponding weight. The image of an interaction term Im(.)
is the product of the exponential of each variable interval D(.) with the corresponding

10

strength:

Im(p(x)) =

d
Y

i=1

D(xi)ki,

Arithmetic operators are well deﬁned in interval arithmetic and the images of univariate
non-linear functions can be easily determined.

The derivative of an IT expression can be calculated by the chain rule as follows

(gi = ti ◦ pi):

∂IT (x)
∂xj
∂gi(x)
∂xj

= w1 · g0
1

(x) + . . . + wn · g0
n

(x)

= t0
i

(pi(x)) · kj

pi(x)
xj

All of the aforementioned transformation functions have derivatives representable
by the provided functions. Following the same rationale the second order derivatives
can be calculated as well.

4 Experimental Setup

For testing the eﬀectiveness of the proposed approach we have created some artiﬁ-
cial data sets generated from ﬂuid dynamics engineering (FDE) taken from (Chen
et al., 2018) and some selected physics models from the Feynman Symbolic Regres-
sion Database2 (Udrescu and Tegmark, 2020) (FEY). Additionally, we have used data
sets built upon real-world measurements from physical systems and engineering (RW).
We have chosen these data sets because they are prototypical examples of non-trivial
non-linear models which are relevant in practical applications.

For each of the problem instances we have deﬁned a set of shape constraints that
must be fulﬁlled by the SR solutions. For the FDE and FEY data sets, we have used
constraints that can be derived from the known expressions over the given input space.
For the physical systems we have deﬁned the shape constraints based on expert knowl-
edge. The input space and monotonicity constraints for all problem instances are given
in the supplementary material.

In the following subsections we give more details on the problem instances including

the constraint deﬁnitions and describe the methods to test the modiﬁed algorithms.

4.1 Problem Instances
All the data sets are described by the input variables names, the target variable, the
true expression if it is known, domains of the variables for the training and test data,
expected image of the function and monotonic constraints. All data set speciﬁcations
are provided as a supplementary material.

Details for Aircraft Lift, Flow Psi, Fuel Flow models can be found in (Anderson,
2010, 1982). Additionally, we have selected a subset of the FEY problem instances
(Udrescu and Tegmark, 2020). The selection criteria was those problem instances which
have been reported as unsolved without dimensional analysis or for which a required
runtime of more than ﬁve minutes was reported by Udrescu and Tegmark (2020). We
have not included the three benchmark problems from the closely related work of Bladek

2https://space.mit.edu/home/tegmark/aifeynman.html

11

and Krawiec (2019), because we could easily solve them in preliminary experiments and
additionally, the results would not be directly comparable because we do not support
symmetry constraints.

All constraints for FDE and FYE have been determined by analysing the known
expression and its partial derivatives. The formulas for the FDE and FYE problem
instances are shown in Table 1. For all these problem instances we have generated data
sets from the known formula by randomly sampling the input space using 100 data points
for training and 100 data points for testing. For each of the data sets we generated two
versions: one without noise and one where we have added normally distributed noise to
the target variable y0 = y + N (0, 0.05σy). Accordingly, the optimally achievable NMSE
for the noisy problem instances is 0.25%.

We have included three real-world data sets (RW). The dataset for Friction µstat
and Friction µdyn has been collected in a standardized testing procedure for friction
plates and has two target variables that we model independently. The Flow Stress data
set stems from hot compression tests of aluminium cylinders (Kabliman et al., 2019).
The constraints for these data sets where set by specialists in the ﬁeld.

The Cars data set has values for displacement, horsepower, weight, acceleration time
and fuel eﬃciency of almost 400 diﬀerent car models. This set was ﬁrst used by Mammen
et al. (2001) and then evaluated by Shah et al. (2016) to assess the performance of
Support Vector Regression with soft constraints.

4.2 Testing Methodology
Each one of the following experiments has been repeated 30 times and, for every rep-
etition, we have measured the NMSE in percent for both training and test data. The
algorithms receive only the training set as the input and only the best found model is
applied to the test set. Runs with and without shape constraints have been executed on
the same hardware.

The ﬁrst experiment tests the performance of the unmodiﬁed algorithms, i.e., with-
out shape constraints. This test veriﬁes whether unmodiﬁed SR algorithms identify
conforming models solely from the observed data. If this result turns out negative, it
means that constraint handling mechanisms are required for ﬁnding conforming models.
In the second experiment, we test the performance of the two modiﬁed algorithms.
This will verify whether the algorithms are able to identify models conforming to prior
knowledge encoded as shape constraints.

This testing approach allows us to compare the prediction errors (NMSE) of models
produced with and without shape constraints to assess whether the prediction error
improves or deteriorates. We perform the same analysis for the two groups of instances
with and without noise separately.

4.3 Algorithms Conﬁgurations
In the following, the abbreviation GP refers to tree-based GP without local optimization
and GPC refers to tree-based GP with local optimization. Both algorithms can be
used with and without shape constraints. Table 2 shows the parameter values that
have been used for the experiments with GP and GPC. ITEA refers to the Interaction-
Transformation Evolutionary Algorithm and FI-2POP-IT (short form: FIIT) refers to
the two-population version of ITEA which supports shape constraints. Table 3 shows
the parameters values for ITEA and FI-2POP-IT.

For comparison we have used auto-sklearn (Feurer et al., 2015) (AML) and an
implementation of shape-constrained polynomial regression (SCPR) (Curmei and Hall,

12

Name
Aircraft lift

Flow psi

Fuel ﬂow

Jackson 2.11

Wave power

I.6.20

I.9.18
I.15.3x

I.15.3t

I.30.5
I.32.17

I.41.16

I.48.20

II.6.15a
II.11.27
II.11.28
II.35.21

III.9.52

III.10.19

Formula

+ Γ
2π

log r
R

r
(cid:17)(γ+1)/(γ−1)

CL = CLα(α + α0) + CLδe δe
(cid:16)

SHT
Sref

)
Ψ = V∞r sin( θ
2π
r

˙m = p0A?√
T0
(cid:16)

q
4π(cid:15)y2

−32
5

(cid:1)2(cid:17)

γ
R

1 − (cid:0) R
(cid:16) 2
1+γ
4π(cid:15)Volt d − qd y3
(m1 m2 )2 (m1 +m2 )
G4
c5
r5
(cid:18) −( θ
(cid:19)
exp
σ
2

1√

)2

2πσ

(y2−d2)2

(cid:17)

G m1 m2
(x2 −x1 )2+(y2 −y1 )2+(z2 −z1 )2
x−ut
p1− u2
(cid:1)

c2

(cid:0)t − ux
c2

1
p1− u2
c2
(cid:1)
ω4
(ω2−ω0

2)2

asin (cid:0) lambd
nd

1

2 (cid:15)c Ef 2 8πr2
3
h ω3

π2 c2 (exp( hω

)−1)

kbT

c2

pd
4π(cid:15) 3z
r5

m c2
p1− v2
px2 + y2
nα
(cid:15)Ef
1− nα
3
1 + nα
1− nα
3
nrhomom tanh (cid:0) momB
(cid:16) (ω−ω0)t
2

sin

(cid:17)2

kbT

(cid:1)

pdEf t
h

1
(cid:0) (ω−ω0)t
2
Bx 2 + By2 + Bz 2

(cid:1)2

p

mom

Table 1: Synthetic problem instances used for testing. The ﬁrst three functions are from
the FDE data sets, the rest from the FEY data sets.

2020)3. Auto-sklearn does not allow to set monotonicity constraints. Therefore, we can
only use it as a comparison for symbolic regression models without constraints. We
execute 30 independent repetitions for each problem with each run limited to one hour.
The degree for SCPR has been determined for each problem instance using 10-
fold cross-validation and using the best CV-RMSE as the selection criteria. We tested
homogeneous polynomials up to degree eight (except for I.9.18 where we used a maximum
degree of ﬁve because of runtime limits). The runtime for the grid search was limited to
one hour for each problem instance. The ﬁnal model is then trained with the selected
degree on the full training set. SCPR is a deterministic algorithm therefore only a single
run was required for each problem instance.

3We have initially also used XGboost, a popular implementation of gradient boosted trees, because it
has previously been found that it compares well with symbolic regression methods (Orzechowski et al.,
2018) and it supports monotonicity constraints. However, the results were much worse compared to
the symbolic regression results which indicates that the method is not particularly well-suited for the
problem instances in the benchmark set which are smooth and of low dimensionality.

13

Parameter

Value

Population size
Generations

1000
200
20 (for GPC with memetic optimization)
PTC2
Initialization
Lmax, Dmax
50, 20
Fitness evaluation NMSE with linear scaling
Tournament (group size = 5)
Selection
Crossover
Subtree crossover
Mutation (one of) Replace subtree with random branch

Add x ∼ N (0, 1) to all numeric parameters
Add x ∼ N (0, 1) to a single numeric parameter
Change a single function symbol
100%
15%
Generational with a single elite
real-valued parameters and input variables
+, ∗, %, log, exp, sin, cos, tanh, x2,
max. 10 iterations of Levenberg-Marquardt (LM)

√

x

Crossover rate
Mutation rate
Replacement
Terminal set
Function set
GPC

Table 2: GP parameter conﬁguration used for the experiments (% refers to protected
division). For the GPC runs we need fewer generations because of the faster convergence
with local optimization.

Parameter

Value

Population size
Number of Iterations
Function set
Fitness evaluation
Maximum number of terms (init. pop.)
Range of strength values (init. pop.)
Min. and max. term length
Regression model

200
500
sin, cos, tanh, √, log, log 1p, exp
RMSE
4
[−4, 4]
2, 6
OLS

Table 3: Parameter values for the ITEA algorithm that have been used for all experi-
ments.

5 Results

In this section we report the obtained results in tabular and graphical forms focusing
on the violations of the shape constraints, goodness of ﬁt, computational overhead, and
extrapolation capabilities with and without shape constraints.

5.1 Constraint Violations
The procedure for checking of constraint violations is as follows: for each problem in-
stance we sample a million points uniformly from the full input space to produce a data
set for checking constraint violations. The ﬁnal SR solutions as well as their partial
derivatives are then evaluated on this data set. If there is at least one point for which

14

(a)

(b)

(c)

Figure 1: Constraint violation frequency for the solutions of every algorithm over the
30 executions for the FDE data sets. Only for the Fuel Flow problem a feasible solution
can be identiﬁed even without shape constraints. FI-2POP always produces feasible
solutions due to the nature of its constraint handling mechanism.

the shape constraints are violated then we count the SR solution as infeasible.

Figures 1a to 1c show exemplarily the number of infeasible SR models for each algo-
rithm and constraint, over the 30 runs for the FDE data sets. The picture is similar for
all problem instances. We observe that without shape constraints none of the algorithms
produce feasible solutions in every run.

Only for the easiest data sets (e.g. Fuel Flow) we re-discovered the data-generating
functions – which necessarily conform to the constraints – in multiple runs. Over all
data sets, ITEA has the highest probability of producing infeasible models. We observe
that even a small amount of noise increases the likelihood to produce infeasible models.
In short, we can see that we cannot hope to ﬁnd feasible solutions using SR algorithms
without shape constraints for noisy data sets. This is consistent with the observation
in (Bladek and Krawiec, 2019).

The results for the extended algorithms show, that with few exceptions, all three

SR algorithms are capable of ﬁnding feasible solutions most of the time.

5.2 Interpolation and Extrapolation
The diﬀerence between SR models and shape-constrained SR models becomes clearer
when we visualize their outputs.

Figures 2a to 2f show the predictions of every SR model identiﬁed for the Friction
µstat data set. The partial dependence plots show the predictions for µstat for all 30
models over the complete range of values allowed for the two variables p and T . The
plot of µstat over p shows the predictions when v and T are ﬁxed to their median values
and the plot of µstat over T shows the predictions when p and v are ﬁxed. The dashed
vertical lines mark the subspace from which we have sampled points for the training and
test data sets.

The algorithms without shape constraints produce extreme predictions when ex-
trapolating (Figures 2a, 2c, 2e). For instance many of the functions have poles at a
temperature T close to zero which are visible in the plots as vertical lines. GP and GPC
without shape constraints produced a few solutions which are wildly ﬂuctuating over p
even within the interpolation range. Within the interpolation range ITEA produced the
best SR solutions for the Friction data sets (see Figure 2e as well as Table 4). However,
the models do not conform to prior knowledge as we would expect that µstat decreases
with increasing pressure and temperature and the models show a slight increase in µstat

15

GPGPCITEAGP (info)GPC (info)FI-2POPCL>0CL<20∂CL∂CLα>0∂CL∂α>0∂CL∂CLδe>0∂CL∂δe>0∂CL∂SHT>0∂CL∂Sref<0 19262600012101200011281900010790001014190008916000131620000221316000Aircraft LiftGPGPCITEAGP (info)GPC (info)FI-2POPΨ>0Ψ<200∂Ψ∂r>0∂Ψ∂R<0∂Ψ∂Γ>0∂Ψ∂θ>0∂Ψ∂V∞>02927300001010201001416120002518251006425000212329000181923000Flow PsiGPGPCITEAGP (info)GPC (info)FI-2POṖm>0̇m<100000∂̇m∂p0>0∂̇m∂A⋆>0∂̇m∂T0<0003010211010121101020110103320010Fuel Flowwhen increasing p.

The model predictions for shape-constrained SR are shown in Figures 2b, 2d, and
2f. The visualization clearly shows that there is higher variance and that all algorithms
produced a few bad or even constant solutions. This invalidates our hypothesis that
shape-constrained SR leads to improved predictive accuracy and instead indicates that
there are convergence issues with our approach of including shape constraints. We will
discuss this in more detail in the following sections. The visualization also shows that
the solutions with shape constraints have better extrapolation behaviour and conform
to shape constraints.

5.3 Goodness-of-ﬁt
Table 4 shows the median NMSE of best solutions over 30 runs obtained by each algo-
rithm, with and without shape constraints, on the test sets. The training results can be
found in the supplementary material.

The table has 6 quadrants: the left block shows the results without side information,
the right block shows the results with shape constraints; the top panel shows results for
RW instances, the middle panel the results for the FDE and FYE instances, and the
bottom panel the results for the same instances including 0.25% noise.

Analysing the detailed results we observe that the best result of all models with
information is better for 18 of 42 instances (RW: 2, no noise: 6, noisy: 10). While the
best result of models without information is better for 14 of 42 instances (RW: 2, no
noise: 3, noisy: 9).

Within both groups there are algorithms with signiﬁcantly diﬀerent test errors
(without info: p-value: 0.0103, with info: p-value: 6.467 · 10−6, Friedman’s rank sum
test with Davenport correction). Pairwise comparison of results without info shows that
GPC is better than GP (p-value: 0.011) and AML (p-value: 0.043) using Friedman’s
rank sum test with Bergman correction. For the problem instances with noise AML
produced the best result for only 1 out of 19 instances. For the instances without noise
the results of AML are similar to results of GP, GPC, and ITEA. Pairwise comparison
of results with info shows that SCPR is better than the other algorithms and no statis-
tically signiﬁcant diﬀerence was found between GP, GPC and FIIT. The p-values for all
pairwise tests are shown in Table 5.

Comparing the results with and without constraints for each algorithm individually,
we ﬁnd that the results are in general worse when using constraints. GP is better than
GP (info) for 27 instances. GPC is better than GPC (info) for 32 instances. ITEA is
better than FIIT for 19 instances. For the RW data sets, ITEA managed to ﬁnd the
best models on two out of the four instances. For Flow Stress, ITEA returned a solution
that produced numerical errors for the test set. This is not the case when we include
the shape-constraints, as we can see on the top-right quadrant. In this situation, FI-
2POP-IT was capable of ﬁnding expressions that did not return invalid results for the
test set.

5.4 Computational Overhead
Another important impact of introducing constraints that should be considered is the
computational overhead introduced to each approach.

For ITEA the execution time is approximately doubled when using the constraint
handling. The reason being that parameter optimization is much easier for the ITEA rep-
resentation and the calculation of the partial derivatives is a simple mechanical process
as shown in Section 3.5.1. For GP and GPC the execution time factor is approximately 5

16

(a) GP

(b) GP (info)

(c) GPC

(d) GPC (info)

(e) ITEA

(f) FI-2POP-IT

Figure 2: Partial dependence plots for the Friction µstat models found by each algo-
rithm over the 30 runs. Dashed lines mark the subspace from which training and test
points were sampled. Algorithms with shape constraints (b, d, f) produce SR solutions
which conform to prior knowledge and have better extrapolation behaviour but increased
prediction error (cf. Table 4).

17

0.000.050.100.150.02.55.07.510.0pmu_stat0100200T0.000.050.100.150.02.55.07.510.0pmu_stat0100200T0.000.050.100.150.02.55.07.510.0pmu_stat0100200T0.000.050.100.150.02.55.07.510.0pmu_stat0100200T0.000.050.100.150.02.55.07.510.0pmu_stat0100200T0.000.050.100.150.02.55.07.510.0pmu_stat0100200TTable 4: Median NMSE values for the test data. Values are multiplied by 100 (percent-
age) and truncated at the second decimal place.

Friction µdyn
Friction µstat
Flow stress
Cars
Aircraft lift
Flow psi
Fuel ﬂow
Jackson 2.11
Wave Power
I.6.20
I.9.18
I.15.3x
I.15.3t
I.30.5
I.32.17
I.41.16
I.48.20
II.6.15a
II.11.27
II.11.28
II.35.21
III.9.52
III.10.19
Aircraft lift
Flow psi
Fuel ﬂow
Jackson 2.11
Wave Power
I.6.20
I.9.18
I.15.3x
I.15.3t
I.30.5
I.32.17
I.41.16
I.48.20
II.6.15a
II.11.27
II.11.28
II.35.21
III.9.52
III.10.19

e
s
i
o
n

t
u
o
h
t
i
w

e
s
i
o
n

h
t
i
w

w/o. info

GP GPC ITEA AML
12.99
8.28
6.82
7.22
8.36
0.15
75.18
74.72
0.63
0.00
0.75
0.00
0.00
0.00
0.62
0.00
14.55
2.74
0.46
0.00
4.98
2.88
0.02
0.34
0.21
0.00
0.18
0.00
42.36
0.76
15.14
2.78
0.00
0.00
16.17
3.55
0.00
0.00
0.00
0.00
1.40
3.29
116.25
19.88
0.41
0.01
0.32
0.45
0.37
0.75
0.34
0.21
3.18
0.28
44.83
21.23
0.45
1.09
4.02
3.77
0.37
0.55
0.53
0.65
0.81
0.34
47.60
0.78
15.19
3.13
0.37
0.35
19.29
3.08
0.62
0.35
0.51
0.38
2.10
3.88
24.81
126.84
0.64
0.85

7.73
5.44
4.66
76.23
0.15
0.13
0.00
0.00
30.82
0.00
2.49
0.01
0.01
0.00
1.13
2.29
0.00
2.50
0.00
0.00
1.18
20.44
0.04
0.26
0.29
0.24
0.31
51.36
0.40
3.55
0.36
0.48
0.35
3.14
2.32
0.36
2.88
0.39
0.44
1.33
18.91
0.38

4.35
4.46
−−
75.06
0.22
0.05
0.00
0.00
2.26
0.31
0.91
0.01
0.00
0.00
8.07
1.08
0.00
4.66
0.00
0.00
2.61
66.16
0.17
0.25
0.21
0.18
0.38
99.88
0.56
1.56
0.38
0.58
0.62
8.50
3.47
0.51
7.56
1.06
0.39
2.43
74.08
0.70

18

GP
12.53
9.98
34.05
76.86
0.80
4.80
0.00
0.00
18.71
1.61
4.03
0.36
0.15
0.00
2.07
8.99
0.00
4.67
0.00
0.00
3.67
104.56
0.55
1.24
5.90
0.30
0.24
22.36
2.14
5.25
0.56
0.59
0.32
3.95
6.68
0.32
3.87
0.37
0.27
4.38
106.56
0.91

w. info

GPC FIIT SCPR
35.90
16.30
8.07
11.83
7.76
1.77
68.16
26.04
19.46
76.64
77.67
73.83
0.14
1.01
0.00
2.91
5.36
0.00
0.00
0.00
0.00
0.90
0.00
0.00
21.31
80.34
13.50
3.20
0.42
0.01
1.20
16.16
0.74
0.04
0.01
0.01
0.03
0.00
0.00
0.00
0.00
0.00
7.79
2.42
12.76
5.15
17.72
1.56
0.00
0.00
0.00
32.12
7.30
1.01
0.07
0.00
0.00
0.00
0.00
0.00
3.22
6.10
1.34
71.93
106.48
33.41
0.31
0.33
0.00
0.46
1.30
0.28
4.65
6.02
0.57
0.30
0.25
0.25
0.83
0.30
0.25
21.39
68.96
11.88
3.61
0.78
0.55
1.62
15.70
1.33
0.42
0.36
0.35
0.48
0.51
0.45
0.39
0.34
0.33
6.22
14.02
2.53
5.05
19.72
2.93
0.34
0.32
0.32
45.32
6.05
1.76
0.47
0.41
0.61
0.30
0.38
0.29
4.27
7.49
1.34
73.44
90.18
32.69
0.70
0.64
0.46

GP
n/a
0.011
0.325
0.499

without info
GPC ITEA AML
0.499
0.325
0.011
0.325
n/a
0.043
0.353
n/a
0.325
n/a
0.353
0.043

with info

GP
n/a
0.151
0.353
0.002

GPC FIIT SCPR
0.353
0.151
0.002
0.054
n/a
0.000
n/a
0.054
0.028
n/a
0.028
0.000

Table 5: p-values for pairwise comparison of algorithms in both groups (Friedman’s rank
sum test with Bergman correction)

when including shape constraints. The increased execution time for GP results from the
additional eﬀort for building the partial derivatives for each solution candidate and for
the interval evaluation. We observed that the increase in execution time is less extreme
for problem instances with a large number of rows where the relative eﬀort for symbolic
derivation of solution candidates becomes smaller.

6 Discussion

The results presented on the previous section largely corroborate our initial assumptions
for shape-constrained symbolic regression. First of all, when we do not explicitly consider
shape constraints within SR algorithms we are unlikely to ﬁnd solutions which conform to
expected behaviour. We showed that the results produced by the two newly-introduced
algorithms in fact conform to shape constraints. Our assessment of the extrapolation
and interpolation behaviour of SR models highlighted the bad extrapolation behaviour as
well as occasional problems even for interpolation. The improved results when including
shape constraints support the argument to include interval arithmetic to improve the
robustness of SR solutions (Keijzer, 2003; Pennachin et al., 2010).

However, our results also show that including shape-constraints via interval arith-
metic leads to SR solutions with higher prediction errors on training and test sets. While
the increased error on the training set is expected, we hoped we would be able to improve
prediction errors on the test set. Assuming that the constraints are correct, this should
hypothetically be possible because the shape constraints provide additional information
for improving the ﬁt on noisy data sets. In fact, we observed that the results got worse
when using information for tree-based GP with and without local optimization (GPC).
There are several possible explanations such as slower convergence, more rapid loss of
diversity, or rejection of feasible solutions because of the pessimistic bounds produced by
interval arithmetic. Another hypothesis is that the positive eﬀect of shape constraints
becomes more relevant with higher noise levels. We are not able to give a conclusive
answer for the main cause of the higher prediction errors with side information and leave
this question open for future research.

Comparison with AutoML as implemented by auto-sklearn showed that GP with pa-
rameter optimization (GPC) produced better test results than AutoML (p-value: 0.011)
over the benchmark set without shape-constraints. However, AutoML does not support
monotonicity constraints and, because of that, we cannot use it to compare with the
results using side information. Therefore, we compared the results of our proposed al-
gorithms with shape-constrained polynomial regression (SCPR). The results show that
SCPR performs better than the evolutionary algorithms for this benchmark set, which
indicates that we can ﬁnd a good approximation of many of our benchmark instances
using polynomials.

19

An advantage of SCPR is that it is formulated as a convex optimization problem
that can be solved eﬃciently and deterministically by highly-tuned solvers. A drawback
of SCPR is the potentially large model size. The number of terms of a homogeneous n-
variate polynomial of degree d is (cid:0)n+d
(cid:1). Our experiments found that polynomial degrees
of up to eight were required to ﬁnd a good ﬁt. The studied problems had ﬁve variables
on average, which led to more than a thousand terms in our polynomial models. The
models produced by the evolutionary algorithms were, on average, much smaller as we
used a limit of 50 nodes for GP expression trees and 6 terms for ITEA/FI-2POP-IT.

n

7 Conclusions

In this paper we have introduced shape-constrained symbolic regression which allows
to include prior knowledge into SR. Shape-constrained symbolic regression allows to
enforce that the model output must be within given bounds, or that outputs must
be monotonically increasing or decreasing over selected inputs. The structure and the
parameters of the symbolic regression model are however still identiﬁed by the algorithm.
We have described two algorithms for shape-constrained symbolic regression which
are extensions of tree-based genetic programming with optional local optimization and
the Interaction-Transformation Evolutionary Algorithm that uses a more restrictive rep-
resentation with the goal of returning simpler expressions.

The extensions add the ability to calculate partial derivatives for any expression
generated by the algorithms and use interval arithmetic to determine bounds and de-
termine if models conform to shape constrains. Two approaches for handling constraint
violations have been tested. The ﬁrst approach simply adjusts ﬁtness for infeasible
solutions while the second approach splits the population into feasible and infeasible
solutions.

The results showed the importance of treating the shape constraints inside the
algorithms. First of all, we have collected more evidence that without any feasibility
control, is unlikely to ﬁnd feasible solutions for most of the problems. Following, we
veriﬁed the eﬃcacy of our approach by measuring the frequency of infeasible solutions
and reporting the median numerical error of our models. The modiﬁed algorithms were
all capable of ﬁnding models conforming to the shape constraints. This shows that the
introduction of shape constraints can help us ﬁnding more realistic models. However,
we have also found that the extended algorithms with shape constraints produce worse
solutions on the test set on average.

For the next steps we intend to analyse in detail the causes for the worse solu-
tions with shape-constrained SR. The bounds determined via interval arithmetic are
very wide and might lead to rejection of feasible solutions as well as premature con-
vergence. This is an issue that could potentially be solved by using more elaborate
bound estimation schemes such as aﬃne arithmetic or recursive splitting. Other pos-
sibilities for the constraint-handling include multi-objective optimization and penalty
functions. Alternatively, optimistic approaches (e.g. using sampling) or a hybridization
of pessimistic and optimistic approaches for shape-constrained regression can be used
to potentially improve the results. Additionally, it would be worthwhile to study the
eﬀects of constraint-handling mechanisms on population diversity in more detail.

8 Acknowledgments

This project is partially funded by Funda¸c˜ao de Amparo `a Pesquisa do Estado de S˜ao
Paulo (FAPESP), grant number 2018/14173-8. And some of the experiments (ITEA)
made use of the Intel®AI DevCloud, which Intel®provided free access.

20

The authors gratefully acknowledge support by the Christian Doppler Research
Association and the Federal Ministry of Digital and Economic Aﬀairs within the Josef
Ressel Centre for Symbolic Regression.

References

Ahmadi, A. A. and Majumdar, A. (2019). DSOS and SDSOS optimization: More
tractable alternatives to sum of squares and semideﬁnite optimization. SIAM Journal
on Applied Algebra and Geometry, 3(2):193–230.

Anderson, J. D. (1982). Modern Compressible Flow: With Historical Perspective

(McGraw-Hill series in mechanical engineering). McGraw-Hill New York.

Anderson, J. D. (2010). Fundamentals of Aerodynamics. McGraw-Hill Education, 5th

edition.

Arnaldo, I., Krawiec, K., and O’Reilly, U.-M. (2014). Multiple regression genetic pro-
gramming. In Proceedings of the 2014 Annual Conference on Genetic and Evolutionary
Computation, GECCO ’14, pages 879–886, New York, NY, USA. ACM.

Bladek, I. and Krawiec, K. (2019). Solving symbolic regression problems with formal
constraints. In Proceedings of the Genetic and Evolutionary Computation Conference,
pages 977–984.

Castillo, F., Marshall, K., Green, J., and Kordon, A. (2003). A methodology for com-
bining symbolic regression and design of experiments to improve empirical model
building.
In Genetic and Evolutionary Computation Conference, pages 1975–1985.
Springer.

Castillo, F. A., Villa, C. M., and Kordon, A. K. (2013). Symbolic regression model
comparison approach using transmitted variation. In Genetic Programming Theory
and Practice X, pages 139–154. Springer.

Chen, C., Luo, C., and Jiang, Z. (2018). A multilevel block building algorithm for fast
modeling generalized separable systems. Expert Systems with Applications, 109:25–34.

Cov˜oes, T. F. and Hruschka, E. R. (2018). Classiﬁcation with multi-modal classes
using evolutionary algorithms and constrained clustering. In 2018 IEEE Congress on
Evolutionary Computation (CEC), pages 1–8. IEEE.

Curmei, M. and Hall, G. (2020). Shape-constrained regression using sum of squares

polynomials.

de Fran¸ca, F. O. (2018). A greedy search tree heuristic for symbolic regression. Infor-

mation Sciences, 442:18–32.

de Franca, F. O. and Aldeia, G. S. I. (0).

Interaction-transformation evolutionary
algorithm for symbolic regression. Evolutionary Computation, 0(ja):1–25. PMID:
33306435.

Dick, G. (2017). Revisiting interval arithmetic for regression problems in genetic pro-
gramming. In Proceedings of the Genetic and Evolutionary Computation Conference
Companion, pages 129–130. ACM.

21

Ferreira, C. (2001). Gene expression programming: A new adaptive algorithm for solving

problems. Complex Systems, 13(2):87–129.

Feurer, M., Klein, A., Eggensperger, K., Springenberg, J., Blum, M., and Hutter, F.
(2015). Eﬃcient and robust automated machine learning. In Cortes, C., Lawrence,
N. D., Lee, D. D., Sugiyama, M., and Garnett, R., editors, Advances in Neural Infor-
mation Processing Systems 28, pages 2962–2970. Curran Associates, Inc.

Fortin, F.-A., Rainville, F.-M. D., Gardner, M.-A., Parizeau, M., and Gagn´e, C. (2012).
Deap: Evolutionary algorithms made easy. Journal of Machine Learning Research,
13(Jul):2171–2175.

Guntuboyina, A., Sen, B., et al. (2018). Nonparametric shape-restricted regression.

Statistical Science, 33(4):568–594.

Gupta, M., Cotter, A., Pfeifer, J., Voevodski, K., Canini, K., Mangylov, A., Moczyd-
lowski, W., and van Esbroeck, A. (2016). Monotonic calibrated interpolated look-up
tables. Journal of Machine Learning Research, 17(109):1–47.

Hall, G. (2018). Optimization over nonnegative and convex polynomials with and without

semideﬁnite programming. PhD thesis, Princeton University.

Hickey, T., Ju, Q., and Van Emden, M. H. (2001). Interval arithmetic: From principles

to implementation. Journal of the ACM (JACM), 48(5):1038–1068.

Houck, C. R., Joines, J. A., Kay, M. G., and Wilson, J. R. (1997). Empirical investigation
of the beneﬁts of partial lamarckianism. Evolutionary Computation, 5(1):31–60.

Kabliman, E., Kolody, A. H., Kommenda, M., and Kronberger, G. (2019). Prediction of
stress-strain curves for aluminium alloys using symbolic regression. AIP Conference
Proceedings, 2113(1):180009.

Keijzer, M. (2003). Improving symbolic regression with interval arithmetic and linear
scaling. In European Conference on Genetic Programming, pages 70–82. Springer.

Kim, S., Lu, P. Y., Mukherjee, S., Gilbert, M., Jing, L., Ceperic, V., and Soljaˇci´c, M.
(2019). Integration of neural network-based symbolic regression in deep learning for
scientiﬁc discovery. ArXiv, abs/1912.04825.

Kimbrough, S. O., Koehler, G. J., Lu, M., and Wood, D. H. (2008). On a feasible–
infeasible two-population (ﬁ-2pop) genetic algorithm for constrained optimization:
Distance tracing and no free lunch. European Journal of Operational Research,
190(2):310–327.

Kommenda, M., Burlacu, B., Kronberger, G., and Aﬀenzeller, M. (2020). Parameter
identiﬁcation for symbolic regression using nonlinear least squares. Genetic Program-
ming and Evolvable Machines, Special Issue on Integrating Numerical Optimization
Methods with Genetic Programming, pages 1–31.

Koza, J. R. (1992). Genetic programming: on the programming of computers by means

of natural selection, volume 1. MIT press.

Kurse, M. U., Lipson, H., and Valero-Cuevas, F. J. (2012). Extrapolatable analyti-
cal functions for tendon excursions and moment arms from sparse datasets. IEEE
Transactions on Biomedical Engineering, 59(6):1572–1582.

22

Li, L., Fan, M., Singh, R., and Riley, P. (2019). Neural-guided symbolic regression with

semantic prior. arXiv preprint arXiv:1901.07714.

Liapis, A., Holmg˚ard, C., Yannakakis, G. N., and Togelius, J. (2015). Procedural per-
sonas as critics for dungeon generation. In European Conference on the Applications
of Evolutionary Computation, pages 331–343. Springer.

Ljung, L. (2010). Perspectives on system identiﬁcation. Annual Reviews in Control,

34(1):1–12.

Luke, S. (2000). Two fast tree-creation algorithms for genetic programming. Evolutionary

Computation, IEEE Transactions on, 4:274–283.

Mammen, E., Marron, J., Turlach, B., Wand, M., et al. (2001). A general projection

framework for constrained smoothing. Statistical Science, 16(3):232–248.

McConaghy, T. (2011). FFX: Fast, scalable, deterministic symbolic regression technol-
ogy. In Genetic Programming Theory and Practice IX, pages 235–260. Springer.

Miller, J. F. and Harding, S. L. (2008). Cartesian genetic programming. In Proceedings
of the 10th annual conference companion on Genetic and evolutionary computation,
pages 2701–2726. ACM.

Moraglio, A., Krawiec, K., and Johnson, C. G. (2012). Geometric semantic genetic
programming. In International Conference on Parallel Problem Solving from Nature,
pages 21–31. Springer.

Oltean, M. and Grosan, C. (2003). A comparison of several linear genetic programming

techniques. Complex Systems, 14(4):285–314.

O’Neill, M. and Ryan, C. (2001). Grammatical evolution. IEEE Transactions on Evo-

lutionary Computation, 5(4):349–358.

Orzechowski, P., La Cava, W., and Moore, J. H. (2018). Where are we now? a large
benchmark study of recent symbolic regression methods. In Proceedings of the Genetic
and Evolutionary Computation Conference, pages 1183–1190.

Papp, D. and Alizadeh, F. (2014). Shape-constrained estimation using nonnegative

splines. Journal of Computational and graphical Statistics, 23(1):211–231.

Pawlak, T. P. and Krawiec, K. (2018). Competent geometric semantic genetic program-
ming for symbolic regression and boolean function synthesis. Evolutionary Computa-
tion, 26(2):177–212. PMID: 28207295.

Pennachin, C. L., Looks, M., and de Vasconcelos, J. A. (2010). Robust symbolic regres-
sion with aﬃne arithmetic. In Proceedings of the 12th Annual Conference on Genetic
and Evolutionary Computation, pages 917–924. ACM.

Petersen, B. K., Larma, M. L., Mundhenk, T. N., Santiago, C. P., Kim, S. K., and Kim,
J. T. (2019). Deep symbolic regression: Recovering mathematical expressions from
data via risk-seeking policy gradients. arXiv preprint arXiv:1912.04871.

Ruberto, S., Terragni, V., and Moore, J. H. (2020). Sgp-dt: Semantic genetic program-
ming based on dynamic targets. In Hu, T., Louren¸co, N., Medvet, E., and Divina, F.,
editors, Genetic Programming, pages 167–183, Cham. Springer International Publish-
ing.

23

Rudin, C. (2019). Stop explaining black box machine learning models for high stakes
decisions and use interpretable models instead. Nature Machine Intelligence, 1(5):206–
215.

Sahoo, S. S., Lampert, C. H., and Martius, G. (2018). Learning equations for extrapo-

lation and control. arXiv preprint arXiv:1806.07259.

Schmidt, M. D. and Lipson, H. (2009). Incorporating expert knowledge in evolutionary
search: a study of seeding methods. In Proceedings of the 11th Annual conference on
Genetic and evolutionary computation, pages 1091–1098. ACM.

Scirea, M., Togelius, J., Eklund, P., and Risi, S. (2016). Metacompose: A composi-
tional evolutionary music composer. In International Conference on Computational
Intelligence in Music, Sound, Art and Design, pages 202–217. Springer.

Shah, S., Sardeshmukh, A., Ahmed, S., and Reddy, S. (2016). Soft monotonic constraint

support vector regression. In COMAD, pages 64–73.

Smits, G. F. and Kotanchek, M. (2005). Pareto-front exploitation in symbolic regression.

In Genetic programming theory and practice II, pages 283–299. Springer.

Stewart, R. and Ermon, S. (2017). Label-free supervision of neural networks with physics
and domain knowledge. In Thirty-First AAAI Conference on Artiﬁcial Intelligence,
AAAI’17, pages 2576–2582. AAAI Press.

Sun, H. and Moscato, P. (2019). A memetic algorithm for symbolic regression. In 2019

IEEE Congress on Evolutionary Computation (CEC), pages 2167–2174.

Tibshirani, R. J., Hoeﬂing, H., and Tibshirani, R. (2011). Nearly-isotonic regression.

Technometrics, 53(1):54–61.

Topchy, A. and Punch, W. F. (2001). Faster genetic programming based on local gradient
search of numeric leaf values.
In Spector, L., Goodman, E. D., Wu, A., Langdon,
W. B., Voigt, H.-M., Gen, M., Sen, S., Dorigo, M., Pezeshk, S., Garzon, M. H.,
and Burke, E., editors, Proceedings of the Genetic and Evolutionary Computation
Conference (GECCO-2001), pages 155–162, San Francisco, California, USA. Morgan
Kaufmann.

Udrescu, S.-M. and Tegmark, M. (2020). AI Feynman: A physics-inspired method for

symbolic regression. Science Advances, 6(16):eaay2631.

Versino, D., Tonda, A., and Bronkhorst, C. A. (2017). Data driven modeling of plastic
deformation. Computer Methods in Applied Mechanics and Engineering, 318:981–
1004.

Worm, T. and Chiu, K. (2013). Prioritized grammar enumeration: symbolic regression
by dynamic programming. In Proceedings of the 15th annual conference on Genetic
and evolutionary computation, pages 1021–1028. ACM.

Wright, I. W., Wegman, E. J., et al. (1980). Isotonic, convex and related splines. The

Annals of Statistics, 8(5):1023–1035.

Zhu, Y., Zabaras, N., Koutsourelakis, P.-S., and Perdikaris, P. (2019). Physics-
constrained deep learning for high-dimensional surrogate modeling and uncertainty
quantiﬁcation without labeled data. Journal of Computational Physics, 394:56–81.

24

Name
Aircraft lift

Flow psi

Fuel ﬂow

Jackson 2.11

Wave power

I.6.20
I.9.18

I.15.3x
I.15.3t
I.30.5
I.32.17

I.41.16
I.48.20
II.6.15a
II.11.27
II.11.28
II.35.21
III.9.52

III.10.19

Friction µdyn
Friction µstat
Flow stress
Cars

Input space
(CLα, α, CLδe , δe, SHT, Sref )
∈ [0.3..0.9] × [2..12] × [0.3..0.9] × [0..12] × [0.5..2] × [3..10]
(V∞, R, Γ, r, θ)
∈ [30..100] × [0.1..0.5] × [2..15] × [0.5..1.5] × [10..90]
(A?, p0, T0)
∈ [0.2..2] × [3 · 105..7 · 105] × [200..400]
(q, y, Volt, d, (cid:15))
∈ [1..5] × [1..3] × [1..5] × [4..6] × [1..5]
(G, c, m1 , m2 , r)
∈ [1..2] × [1..2] × [1..5] × [1..5] × [1..2]
(σ, θ) ∈ [1..3]2
(m1 , m2 , G, x1 , x2 , y1 , y2 , z1 , z2 )
∈ [1..2] × [1..2] × [1..2] × [3..4]
×[1..2] × [3..4] × [1..2] × [3..4] × [1..2]
(x, u, c, t) ∈ [5..10] × [1..2] × [3..20] × [1..2]
(x, c, u, t) ∈ [1..5] × [3..10] × [1..2] × [1..5]
(lambd, d, n) ∈ [1..5] × [2..5] × [1..5]
((cid:15), c, Ef , r, ω, ω0)
∈ [1..2] × [1..2] × [1..2] × [1..2] × [1..2] × [3..5]
(ω, T, h, kb, c) ∈ [1..5]5
(m, v, c) ∈ [1..5] × [1..2] × [3..20]
((cid:15), pd, r, x, y, z) ∈ [1..3]6
(n, α, (cid:15), Ef ) ∈ [0..1] × [0..1] × [1..2] × [1..2]
(n, alpha) ∈ [0..1]2
(nrho, mom, B, kb, T ) ∈ [1..5]5
(pd, Ef , t, h, ω, ω0)
∈ [1..3] × [1..3] × [1..3] × [1..3] × [1..5] × [1..5]
(mom, Bx, By, Bz) ∈ [1..5]4

(p, v, T ) ∈ [0.1..15] × [0.01..3] × [−50..250]
(p, v, T ) ∈ [0.1..15] × [0.01..3] × [−50..250]
(φ, ˙φ, T ) ∈ [0..1] × [0.001..10] × [250..600]
(cyl, dis, hp, w, acc)
∈ [3..8] × [68..455] × [46..230] × [8..24.8]

Constraints
(1, 1, 1, 1, 1, −1)

(1, 1, 1, −1, 1)

(1, 1, −1)

(1, −1, 1, 1, 1)

(−1, 1, −1, −1, 1)

(0, −1)
(1, 1, 1, −1, 1,
−1, 1, −1, 1)

(1, 0, −1, −1)
(0, 0, 0, 1)
(1, −1, −1)
(1, 1, 1, 1, 1, −1)

(0, 1, −1, 1, −1)
(1, 1, 1)
(−1, 1, −1, 1, 1, 1)
(1, 1, 1, 1)
(1, 1)
(1, 1, 1, −1, −1)
(1, 1, 0, −1, 0, 0)

(1, 1, 1, 1)

(−1, −1, −1)
(−1, 0, −1)
(0, 1, −1)
(0, −1, −1, −1, 0)

Table 6: Input space and the monotonicity constraints we used for all benchmark in-
stances. In the constraints tuple a negative value for ci means that the model is non-
increasing over the i-th variable, a positive value means that the model is non-decreasing
and a zero means that there is no monotonicity constraint.

9 Supplementary

9.1 Problem deﬁnition details
Table 6 shows the input space and the monotonicity constraints that we have used for
the problem instances.

9.2 Training results
Table 7 shows the median NMSE values (in percent) on the training set for the best
solutions over 30 runs of each algorithm. The test results are given in the paper.

9.3 Sensitivity to population size
For the experiments in the paper we have used parameter settings based on prior knowl-
edge about settings which tend to produce good results on a wide range of problems. For
instance we have set the population size to 1000 individuals for GP and 200 for ITEA
and FIIT. Similarly, we have chosen other parameter values. It should be noted that we
have not tried to tune the hyper-parameters (globally or for individual instances). Since

25

Table 7: Median of the NMSE for the training data without and with shape constraints.
Values are multiplied by 100 (percentage) and truncated at the second decimal place.

Friction µdyn
Friction µstat
Flow stress
Cars
Aircraft lift
Flow psi
Fuel ﬂow
Jackson 2.11
Wave Power
I.6.20
I.9.18
I.15.3x
I.15.3t
I.30.5
I.32.17
I.41.16
I.48.20
II.6.15a
II.11.27
II.11.28
II.35.21
III.9.52
III.10.19
Aircraft lift
Flow psi
Fuel ﬂow
Jackson 2.11
Wave Power
I.6.20
I.9.18
I.15.3x
I.15.3t
I.30.5
I.32.17
I.41.16
I.48.20
II.6.15a
II.11.27
II.11.28
II.35.21
III.9.52
III.10.19

e
s
i
o
n

t
u
o
h
t
i
w

e
s
i
o
n

h
t
i
w

w/o. info
GP GPC ITEA AML
0.80
0.07
0.41
0.11
4.73
0.05
9.34
6.42
0.33
0.00
0.54
0.00
0.00
0.00
0.10
0.00
0.41
0.34
0.18
0.00
0.47
0.94
0.18
0.00
0.11
0.00
0.22
0.00
20.19
0.18
12.11
0.19
0.00
0.00
19.98
0.64
0.00
0.00
0.00
0.00
1.75
0.17
23.69
5.44
0.31
0.00
0.45
0.22
0.75
0.21
0.21
0.24
0.66
0.28
2.42
0.52
0.43
0.56
0.46
1.15
0.28
0.35
0.25
0.34
0.76
0.20
28.23
0.85
13.37
0.40
0.23
0.16
20.72
0.58
0.19
0.18
0.38
0.30
2.21
0.55
29.77
5.50
0.49
0.17

0.48
0.30
4.85
10.78
0.08
0.14
0.00
0.00
0.69
0.00
0.63
0.00
0.00
0.00
0.13
0.18
0.00
0.15
0.00
0.00
0.49
7.14
0.02
0.26
0.29
0.24
0.29
1.12
0.23
0.87
0.16
0.23
0.19
0.81
0.30
0.16
0.33
0.17
0.29
0.62
5.65
0.21

0.24
0.25
4.70
7.48
0.08
0.02
0.00
0.00
0.01
0.01
0.17
0.00
0.00
0.00
0.05
0.03
0.00
0.18
0.00
0.00
0.44
8.13
0.08
0.25
0.21
0.18
0.20
7.24
0.25
0.25
0.15
0.19
0.15
0.65
0.16
0.14
0.30
0.15
0.24
0.55
9.22
0.32

26

w. info

GP GPC FIIT SCPR
3.05
2.02
1.84
0.61
23.57
14.27
10.79
9.78
0.49
0.00
4.54
0.00
0.00
0.00
0.00
0.00
1.07
0.00
1.47
0.00
1.15
0.00
0.19
0.00
0.10
0.00
0.00
0.00
0.11
0.43
1.13
0.00
0.00
0.00
1.16
0.00
0.00
0.00
0.00
0.00
2.43
0.00
32.27
0.20
0.37
0.00
3.94
0.76
4.78
5.69
5.32
0.26
0.32
0.09
0.92
0.07
1.79
0.21
1.47
0.12
0.18
0.36
0.21
0.15
0.21
0.14
1.74
0.11
1.03
0.05
0.20
0.19
1.03
0.00
0.18
0.21
0.35
0.36
3.22
0.20
33.68
0.28
0.68
0.16

1.98
5.81
42.55
11.56
0.13
3.63
0.00
0.00
0.93
3.61
0.18
0.00
0.00
0.00
0.06
0.24
0.00
0.61
0.00
0.00
2.08
21.45
0.21
0.28
4.65
0.25
0.31
0.94
3.86
0.26
0.15
0.21
0.20
0.71
0.49
0.19
0.61
0.17
0.33
2.37
22.17
0.45

1.51
0.95
22.76
11.26
0.68
4.59
0.00
0.00
6.88
0.34
4.48
0.01
0.01
0.00
4.89
1.52
0.00
1.24
0.04
0.00
3.95
32.56
0.23
0.87
5.22
0.27
0.34
4.74
0.77
4.74
0.19
0.26
0.21
6.36
2.10
0.21
1.30
0.38
0.35
4.52
29.77
0.48

Figure 3: Convergence graphs of best quality over evaluations for diﬀerent population
sizes (top: GP, bottom: FI2POP, left: without noise, right with noise).

population size is potentially a sensitive parameter it is worthwhile to analyse how the
algorithm convergence is inﬂuenced by diﬀerent parameter settings. On the one hand
smaller population sizes can improve the runtime on the other hand there is a danger
that the population size becomes too small leading to loss of diversity and premature
convergence.

For the experiment we have chosen one of the harder problem instances (Wave
power) and have used the population sizes: 250, 500, 1000, 2000, and 4000. Figure 3
shows the best error over the number of evaluations (note:
log-log scale) for GP and
FI2POP with and without noise. It can be seen that comparable results can also be
achieved with smaller population sizes. Overall, the population size of 1000 individuals,
that we have originally used for the GP experiments, performs well compared to the
other population sizes. The plots show no overly long plateaus in quality which would
be indicative of premature convergence.

27

103104105Evaluations102101100NMSE (%)Pop. size250500100020004000103104105Evaluations102101100NMSE (%)Pop. size250500100020004000103104105Evaluations102101100NMSE (%)Pop. size250500100020004000103104105Evaluations102101100NMSE (%)Pop. size250500100020004000