2
2
0
2

p
e
S
5
1

]

G
L
.
s
c
[

2
v
5
2
0
8
0
.
1
0
0
2
:
v
i
X
r
a

Optimal binning: mathematical programming
formulation

Guillermo Navas-Palencia

g.navas.palencia@gmail.com

September 19, 2022∗

Abstract

The optimal binning is the optimal discretization of a variable into bins given a dis-
crete or continuous numeric target. We present a rigorous and extensible mathematical
programming formulation to solve the optimal binning problem for a binary, contin-
uous and multi-class target type, incorporating constraints not previously addressed.
For all three target types, we introduce a convex mixed-integer programming formula-
tion. Several algorithmic enhancements, such as automatic determination of the most
suitable monotonic trend via a Machine-Learning-based classiﬁer, and implementation
aspects are thoughtfully discussed. The new mathematical programming formulations
are carefully implemented in the open-source python library OptBinning.

1

Introduction

Binning (grouping or bucketing) is a technique to discretize the values of a continuous
variable into bins (groups or buckets). From a modeling perspective, the binning technique
may address prevalent data issues such as the handling of missing values, the presence
of outliers and statistical noise, and data scaling. Furthermore, the binning process is
a valuable interpretable tool to enhance the understanding of the nonlinear dependence
between a variable and a given target while reducing the model complexity. Ultimately,
resulting bins can be used to perform data transformations.

Binning techniques are extensively used in machine learning applications, exploratory
data analysis and as an algorithm to speed up learning tasks; recently, binning has been
applied to accelerate learning in gradient boosting decision tree [13]. In particular, binning
is widely used in credit risk modeling, being an essential tool for credit scorecard modeling
to maximize diﬀerentiation between high-risk and low-risk observations.

There are several unsupervised and supervised binning techniques. Common unsuper-
vised techniques are equal-width and equal-size or equal-frequency interval binning. On the
other hand, well-known supervised techniques based on merging are Monotone Adjacent
Pooling Algorithm (MAPA), also known as Maximum Likelihood Monotone Coarse Classi-
ﬁer (MLMCC) [22] and ChiMerge [14], whereas other techniques based on decision trees are
CART [2], Minimum Description Length Principle (MDLP) [3] and, more recently, condition
inference trees (CTREE) [10].

The binning process might require to satisfy certain constraints. These constraints might
range from requiring a minimum number of records per bin to monotonicity constraints. This
variant of the binning process is known as the optimal binning process. The optimal binning

∗This version contains a new objective function and the maximum p-value constraint for the continuous

target. The initial version date is January 22, 2020.

1

 
 
 
 
 
 
is generally solved by iteratively merging an initial granular discretization until imposed
constraints are satisﬁed. Performing this ﬁne-tuning manually is likely to be unsatisfactory
as the number of constraints increases, leading to suboptimal or even infeasible solutions.
However, we note that this manual adjustment has been encouraged by some authors [21],
legitimating the existing interplay of “art and science” in the binning process.

There are various commercial software tools for solving the optimal binning problem1.
Software IBM SPSS and the MATLAB Financial Toolbox, use MDLP and MAPA as default
algorithm, respectively. The most advanced tool to solve the optimal binning problem is
available in the SAS Enterprise Miner software. A limited description of the proprietary al-
gorithm can be found in [18], where two mixed-integer programming (MIP) formulations are
sketched: a mixed-integer linear programming (MILP) formulation to obtain a fast proba-
bly suboptimal solution, and a mixed-integer nonlinear programming (MINLP) formulation
to obtain an optimal solution. The suboptimal formulation is the default method due to
computational time limitations (MILP techniques are considerably more mature). We note
that the SAS implementation allows most of the constraints required in credit risk model-
ing, becoming an industry standard. Besides, there exist a few open-source solutions, but
the existing gap comparing to the commercial options in terms of capabilities is still signif-
icant. Among the available alternatives, we mention the MATLAB implementation of the
monotone optimal binning in [17], and the R specialized packages smbinning [8], relying on
CTREE, and MOB [23], which merely include basic functionalities.

In this paper, we develop a rigorous and extensible mathematical programming formu-
lation for solving the optimal binning problem. This general formulation can eﬃciently
handle binary, continuous, and multi-class target type. The presented formulations incorpo-
rate the constraints generally required to produce a good binning [21], and new constraints
not previously addressed. For all three target types, we introduce a convex mixed-integer
programming formulation, ranging from a integer linear programming (ILP) formulation for
the simplest cases to a mixed-integer quadratic programming (MIQP) formulation for those
cases adding more involved constraints.

The remainder of the paper is organized as follows. Section 2 introduces our general
problem formulation and the corresponding mixed-integer programming formulation for each
supported target. We focus on the formulation for binary target, investigating various
formulation variants. Then, in Section 3 we discuss in detail several algorithmic aspects
such as the automatic determination of the optimal monotonic trend and the development of
presolving algorithms to eﬃciently solve large size instances. Section 4 includes experiments
with real-world datasets and compares the performance of supported solvers for large size
instances. Finally, in Section 5, we present our conclusions and discuss possible research
directions.

2 Mathematical programming formulation

The optimal binning process comprises two steps: A pre-binning process that generates
an initial granular discretization, and a subsequent reﬁnement or optimization to satisfy
imposed constraints. The pre-binning process uses, for example, a decision tree algorithm to
calculate the initial split points. The resulting m split points are sorted in strictly ascending
order, s1 < s2 < . . . < sm to create n = m + 1 pre-bins. These pre-bins are deﬁned by the
intervals (−∞, s1), [s1, s2), . . . , [sm, ∞).

Given n pre-bins, the decision variables consist of a binary lower triangular matrix (in-
dicator variables) of size n, Xij ∈ {0, 1}, ∀(i, j) ∈ {1, . . . , n : i ≥ j}. The starting point is
a diagonal matrix, meaning that initially all pre-bins are selected. A basic feasible solution
must satisfy the following constraints:

1To the author’s knowledge, at the time of writing, these tools are restricted to the problem of discretizing

a variable with respect to a binary target.

2

• All pre-bins are either isolated or merged to create a larger bin interval, but cannot

be erased. Each column must contain exactly one 1.

n
(cid:88)

i=1

Xij = 1,

j = 1, . . . , n.

(1)

• Only consecutive pre-bins can be merged. Continuity by rows, no 0 − 1 gaps are

allowed.

Xij ≤ Xij+1,

i = 1, . . . , n; j = 1, . . . , i − 1.

(2)

• A solution has a last bin interval of the form [sk, ∞), for k ≤ n. The binary decision

variable Xnn = 1.

To clarify, Figure 1 shows an example of a feasible solution. In this example, pre-bins
corresponding to split points (s2, s3, s4) and (s5, s6) are merged, thus having an optimal
binning with bin intervals (−∞, s1), [s1, s4), [s4, s6), [s6, ∞).

Figure 1: Lower triangular matrix X.
solution with 4 bins after merging pre-bins (right).

Initial solution after pre-binning (left). Optimal

The described problem can be seen as a generalized assignment problem. A direct for-
mulation of metrics involving ratios such as the mean or most of the divergence measures on
merged bins leads to a non-convex MINLP formulation, due to the ratio of sums of binary
variables. Solving non-convex MINLP problems to optimality is a challenging task requiring
the use of global MINLP solvers, especially for large size instances.

Investigating the binary lower triangular matrix in Figure 1, it can be observed, by
analyzing the constraints in Equations (1) and (2) imposing continuity by rows, that a
feasible solution is entirely characterized by the position of the ﬁrst 1 for each row. This
observation permits the pre-computation of the set of possible solutions by rows, obtaining
an aggregated matrix with the shape of X for each involved metric. Consequently, the non-
convex objective function and constraints are linearized, resulting in a convex formulation
by exploiting problem information. Using this reformulation, we shall see that the deﬁnition
of constraints for binary, continuous and multi-class target are almost analogous.

2.1 Mixed-integer programming formulation for binary target

Given a binary target y used to discretize a variable x into n bins, we deﬁne the normalized
count of non-events (NE) pi, case y = 0, and events (E) qi, case y = 1, for each bin i as

pi =

rN E
i
rN E
T

,

qi =

rE
i
rE
T

,

3

10000000010000100010000101011000000100000100001000100101i

, rE

i , rN E
T

where rN E
T are the number of non-event and event records per bin, and
the total number of non-event records and event records, respectively. Next, we deﬁne the
Weight of Evidence (WoE) and event rate (D) for each bin,

and rE

WoEi = log

(cid:19)

(cid:18) rN E
/rN E
i
T
i /rE
rE
T

, Di =

rE
i
i + rN E
rE

i

.

The Weight of Evidence WoEi and event rate Di for each bin are related by means of the
functional equations

WoEi = log

(cid:19)

(cid:18) 1 − Di
Di

+ log

(cid:18)

Di =

1 +

rN E
T
rE
T

eWoEi

(cid:19)−1

(cid:19)

= log

(cid:18) rE
T
rN E
T
(cid:32)

=

1 + e

WoEi−log

(cid:19)

(cid:18) rE
T
rN E
T
(cid:18) rE
T
rN E
T

− logit(Di)

(cid:19)(cid:33)−1

,

where Di can be characterized as a logistic function of WoEi, and WoEi can be expressed
in terms of the logit function of Di. This shows that WoE is inversely related to the event
rate. The constant term log(rE
T ) is the log ratio of the total number of event and the
total number of non-events.

T /rN E

Divergence measures serve to assess the discriminant power of a binning solution. The
Jeﬀreys’ divergence [11], also known as Information Value (IV) within the credit risk in-
dustry, is a symmetric measure expressible in terms of the Kullback-Leibler divergence
DKL(P ||Q) [15] deﬁned by

J(P ||Q) = IV = DKL(P ||Q) + DKL(Q||P ) =

n
(cid:88)

(pi − qi) log

i=1

(cid:19)

.

(cid:18) pi
qi

The IV statistic is unbounded, but some authors have proposed rules of thumb to settings
quality thresholds [21]. Alternatively, the Jensen-Shannon divergence is a bounded symmet-
ric measure also expressible in terms of the Kullback-Leibler divergence

JSD(P ||Q) =

1
2

(D(P ||M ) + D(Q||M )) , M =

1
2

(P + Q),

and bounded by JSD(P ||Q) ∈ [0, log(2)]. Note that these divergence measures cannot be
computed when rN E
i = 0. Other divergences measures without this limitation
are described in [24].

i = 0 and/or rE

A good binning algorithm for binary target should be characterized by the following

properties [21]:

1. Missing values are binned separately.

2. Each bin should contain at least 5% observations.

3. No bins should have 0 non-events or events records.

Property 1 is adequately addressed in many implementations, where missing and special
values are incorporated as additional bins after the optimal binning terminates. Property 2
is a usual constraint to enforce representativeness. Property 3 is required to compute the
above divergence measures.

4

Let us deﬁne the parameters of the mathematical programming formulation:

i + rE
i

∈ N

n ∈ N
T ∈ N
rN E
T ∈ N
rE
rN E
i
i ∈ N
rE
ri = rN E
min ∈ N
rN E
max ∈ N
rN E
min ∈ N
rE
max ∈ N
rE
bmin ∈ N
bmax ∈ N

number of pre-bins.

total number of non-event records.

total number of event records.

number of non-event records per pre-bin.

number of event records per pre-bin.

number of records per pre-bin.

minimum number of non-event records per bins.

maximum number of non-event records per bins.

minimum number of event records per bins.

maximum number of event records per bins.

minimum number of bins.

maximum number of bins.

The objective function is to maximize the discriminant power among bins, therefore,
maximize a divergence measure. The IV can be computed using the described parameters
and the decision variables Xij, yielding

n
(cid:88)

IV =





i
(cid:88)

i=1

j=1

(cid:18) rN E
z
rN E
T

−

(cid:19)

rE
z
rE
T



Xij

 log

(cid:32) (cid:80)i

j=1 rN E
j
(cid:80)i
j=1 rE

/rN E

T Xij
T Xij

j /rE

(cid:33)

,

The IV is the sum of the IV contributions per bin, i.e., the sum by rows. As previously
stated, given the constraints in Equations (1) and (2), an aggregated low triangular matrix
Vij ∈ R+
0 , ∀(i, j) ∈ {1, . . . , n : i ≥ j} with all possible IV values from bin merges can be
pre-computed as follows

Vij =





i
(cid:88)

z=j

rN E
z
rN E
T

−

rE
z
rE
T



 log

(cid:32) (cid:80)i

z=j rN E
z
(cid:80)i
z=j rE

/rN E
T
z /rE
T

(cid:33)

,

i = 1, . . . , n; j = 1, . . . , i.

(3)

The optimal IV for each bin is determined by using the remarked observation that a solu-
tion is characterized by the position of the ﬁrst 1 for each row, thus, using the continuity
constraint in (2), we obtain

Vi· = Vi1Xi1 +

i
(cid:88)

j=2

Vij(Xij − Xij−1) ⇐⇒ ViiXii +

i−1
(cid:88)

j=1

(Vij − Vij+1)Xij.

(4)

for i = 1, . . . , n. The latter formulation is preferred to reduce the ﬁll-in of the matrix
of constraints. Similarly, a lower triangular matrix of event rates Dij ∈ [0, 1], ∀(i, j) ∈
{1, . . . , n : i ≥ j} can be pre-computed as follows

Dij =

(cid:80)i

(cid:80)i

z=j rE
z
z=j rz

,

i = 1, . . . , n; j = 1, . . . , i.

The ILP formulation, with no additional constraints such as monotonicity constraints,

5

can be stated as follows

n
(cid:88)

i=1
n
(cid:88)

max
X

s.t.

ViiXii +

i−1
(cid:88)

(Vij − Vij+1)Xij

j=1

Xij = 1,

i=j
Xij − Xij+1 ≤ 0,
n
(cid:88)

bmin ≤

Xii ≤ bmax

(5a)

j = 1, . . . , n

(5b)

i = 1, . . . , n; j = 1, . . . , i − 1

(5c)

(5d)

i=1

rminXii ≤

rN E
minXii ≤

rE
minXii ≤

i
(cid:88)

j=1

i
(cid:88)

j=1

i
(cid:88)

j=1
Xij ∈ {0, 1},

rjXij ≤ rmaxXii,

i = 1, . . . , n

(5e)

j Xij ≤ rN E
rN E

maxXii,

i = 1, . . . , n

(5f)

j Xij ≤ rE
rE

maxXii,

i = 1, . . . , n

(5g)

∀(i, j) ∈ {1, . . . , n : i ≥ j}

(5h)

Apart from the constraints (5b) and (5c) already described, constraint (5d) imposes a
lower and upper bound on the number of bins. Other range constraints (5e-5g) limit the
number of total, non-event and event records per bin. Note that to increase sparsity, the
range constraints are not implemented following the standard formulation to avoid having
the data twice in the model. For example, constraint (5d) is replaced by

d +

n
(cid:88)

i=1

Xii − bmax = 0,

0 ≤ d ≤ bmax − bmin.

2.1.1 Monotonicity constraints

Monotonicity constraints between the event rates of consecutive bins can be imposed to
ensure legal compliance and business constraints. Three types of monotonic trends are con-
sidered: the usual ascending/descending, and two types of unimodal forms, concave/convex
and peak/valley. This modeling ﬂexibility can help to capture overlooked or unexpected
patterns, providing new insights to enrich models. Note that work in [18] uses the WoE
approach instead.

Applying Equation (4), the optimal event rate for each bin is given by

Di· = DiiXii +

i−1
(cid:88)

(Dij − Dij+1)Xij,

j=1

i = 1, . . . , n.

(6)

Monotonic trend: ascending and descending. The formulation for monotonic as-
cending trend can be stated as follows,

DzzXzz +

z−1
(cid:88)

(Dzj − Dzj+1)Xzj + β(Xii + Xzz − 1)

j=1

≤ 1 + (Dii − 1)Xii +

i−1
(cid:88)

j=1

(Dij − Dij+1)Xij,

i = 2, . . . , n; z = 1, . . . i − 1.

6

The term 1 + (Dii − 1)Xii or simply 1 − Xii, is used to ensure that event rates are in
[0, 1], and the ascending constraint is satisﬁed even if bin i is not selected. Note that this
is a big-M formulation M + (Dii − M )Xii using M = 1, which suﬃces given D ∈ [0, 1],
however, a tighter (non integer) M = max({Dij : i = 1, . . . , n; i ≥ j}), can be used instead.
The parameter β is the minimum event rate diﬀerence between consecutive bins. The term
β(Xii + Xzz − 1) is required to ensure that the diﬀerence between two selected bins i and z
is greater or equal than β. Similarly, for the descending constraint,

DiiXii +

i−1
(cid:88)

(Dij − Dij+1)Xij + β(Xii + Xzz − 1)

j=1

≤ 1 + (Dzz − 1)Xzz +

z−1
(cid:88)

(Dzj − Dzj+1)Xzj,

j=1

i = 2, . . . , n; z = 1, . . . i − 1.

Monotonic trend: concave and convex. The concave and convex trend can be achieved
by taking the deﬁnition of concavity/convexity on equally spaced points:

−xi+1 + 2xi − xi−1 ≥ 0
xi+1 − 2xi + xi−1 ≥ 0

concave

convex

Thus, replacing Equation (6) in the previous deﬁnition of concavity we obtain the concave
trend constraints,

(cid:32)

−

DiiXii +

i−1
(cid:88)

(Diz − Diz+1)Xiz

(cid:33)

(cid:32)

+ 2

DjjXjj +

j−1
(cid:88)

(Djz − Djz+1)Xjz

(cid:33)

z=1

z=1

(cid:32)

−

DkkXkk +

k−1
(cid:88)

(Dkz − Dkz+1)Xkz

(cid:33)

≥ Xii + Xjj + Xkk − 3,

z=1

for i = 3, . . . n; j = 2, . . . , i − 1 and k = 1, . . . , j − 1. Similarly, for convex trend we get

(cid:32)

DiiXii +

i−1
(cid:88)

z=1

(cid:33)

(cid:32)

(Diz − Diz+1)Xiz

− 2

DjjXjj +

j−1
(cid:88)

(Djz − Djz+1)Xjz

(cid:33)

z=1

(cid:32)

DkkXkk +

k−1
(cid:88)

(Dkz − Dkz+1)Xkz

(cid:33)

≥ Xii + Xjj + Xkk − 3,

z=1

for i = 3, . . . n; j = 2, . . . , i − 1 and k = 1, . . . , j − 1. Note that term Xii + Xjj + Xkk − 3 is
used the preserve redundancy of constraints when not all bins i, j and k are selected, given
that D ∈ [0, 1].

Monotonic trend: peak and valley. The peak and valley trend2 deﬁne an event rate
function exhibiting a single trend change or reversal. The optimal trend change position
is determined by using disjoint constraints, which can be linearized using auxiliary binary
variables. The resulting additional constraints are as follows,

i − n(1 − yi) ≤ t ≤ i + nyi,

i = 1, . . . , n

t ∈ [0, n]
yi ∈ {0, 1},

i = 1, . . . , n

(7a)

(7b)

(7c)

2In some commercial tools, peak and valley trend are called inverse U-shaped and U-shaped, respectively.

7

where t is the position of the optimal trend change bin, yi are auxiliary binary variables and
n in (7a) is the smallest big-M value for this formulation while preserving the redundancy
of constraints. Furthermore, for the peak trend we incorporate the following constraints,

yi + yz + 1 + (Dzz − 1)Xzz +

z−1
(cid:88)

(Dzj − Dzj+1)Xzj

j=1

≥ DiiXii +

i−1
(cid:88)

j=1

(Dij − Dij+1)Xij,

i = 2, . . . , n; z = 1, . . . , i − 1,

2 − yi − yz + 1 + (Dii − 1)Xii +

i−1
(cid:88)

j=1

(Dij − Dij+1)Xij

≥ DzzXzz +

z−1
(cid:88)

(Dzj − Dzj+1)Xzj,

j=1

i = 2, . . . , n; z = 1, . . . , i − 1.

Similarly, for the valley trend we include,

yi + yz + 1 + (Dii − 1)Xii +

i−1
(cid:88)

(Dij − Dij+1)Xij

j=1

≥ DzzXzz +

z−1
(cid:88)

j=1

(Dzj − Dzj+1)Xzj,

i = 2, . . . , n; z = 1, . . . , i − 1,

2 − yi − yz + 1 + (Dzz − 1)Xzz +

≥ DiiXii +

i−1
(cid:88)

(Dij − Dij+1)Xij,

j=1

z−1
(cid:88)

j=1

(Dzj − Dzj+1)Xzj

i = 2, . . . , n; z = 1, . . . , i − 1.

Note that none of these constraints are necessary if the position of the change bin t is
ﬁxed in advance. For example, given t, the valley trend constraints are replaced by two sets
of constraints; one to guarantee a descending monotonic trend before t and another to guar-
antee an ascending monotonic trend after t. Devising an eﬀective heuristic to determine the
optimal t can yield probably optimal solutions while reducing the problem size signiﬁcantly.

2.1.2 Additional constraints

Reduction of dominating bins. To prevent any particular bin from dominating the
results, it might also be required that bins have at most a certain number of (total/non-
event/event) records using constraints (5e - 5g). Furthermore, we might produce more
homogeneous solutions by reducing a concentration metric such as the standard deviation of
the number of total/non-event/event records among bins. Three concentration metrics are
considered: standard deviation, Herﬁndahl-Hirschman Index (HHI) [17] and the diﬀerence
between the largest and smallest bin.

The standard deviation among the number of records for each bin is given by






1
m − 1

std =

n
(cid:88)





i
(cid:88)

i=1

j=1

rjXij −

Xii
m

n
(cid:88)

i
(cid:88)

i=1

j=1

2



1/2

rjXij






,

where m = (cid:80)n
variables

i=1 Xii is the optimal number of bins. Let us deﬁne the following auxiliary

µ =

1
m

n
(cid:88)

i
(cid:88)

i=1

j=1

rjXij, wi =

i
(cid:88)

j=1

8

rjXij − µXii,

i = 1, . . . n.

Taking w = (w1, . . . , wn)T , the standard deviation t can be incorporated to the formulation
with a diﬀerent representation. Since std = (wT w/(m − 1))1/2 then

||w||2
(m − 1)1/2

≤ t ⇐⇒ ||w||2

2 ≤ (m − 1)t2.

The non-convex MINLP formulation using the parameter γ to control the importance of the
term t,

max
X,µ,w

n
(cid:88)

i=1

ViiXii +

s.t.

(5b - 5h)

i−1
(cid:88)

(Vij − Vij+1)Xij − γt

j=1

n
(cid:88)

i=1

µ =

w2

i ≤ (m − 1)t2

1
m

n
(cid:88)

i
(cid:88)

i=1

j=1

rjXij

i
(cid:88)

wi =

rjXij − µXii,

j=1
n
(cid:88)

i=1

Xii

m =

m ≥ 0

µ ≥ 0
wi ∈ R,

(10a)

(10b)

(10c)

(10d)

i = 1, . . . n

(10e)

(10f)

(10g)

(10h)

(10i)

i = 1, . . . , n.

A widely used metric to quantify concentration is HHI, which can be employed to asses
the quality of a binning solution. Lower values of HHI correspond to more homogeneous
bins. The HHI of the number of records for each bin is given by

HHI =

1
r2
T

n
(cid:88)





i
(cid:88)


2

rjXij



,

i=1

j=1

where rT = (cid:80)n
parameter γ to control the importance of HHI is stated as

i=1 ri is the total number of records. The MIQP formulation using the

max
X

n
(cid:88)

i=1

ViiXii +

s.t.

(5b - 5h)

i−1
(cid:88)

(Vij − Vij+1)Xij −

j=1

γ
r2
T

n
(cid:88)





i
(cid:88)

i=1

j=1


2

rjXij



(11a)

(11b)

An eﬀective MILP formulation can be devised using a simpliﬁcation of the standard
deviation approach based on reducing the diﬀerence between the largest and smallest bin.

9

The MILP formulation is given by

max
X,pmin,pmax

n
(cid:88)

i=1

ViiXii +

s.t.

(5b - 5h)

i−1
(cid:88)

(Vij − Vij+1)Xij − γ(pmax − pmin)

j=1

pmin ≤ rT (1 − Xii) +

i
(cid:88)

j=1

rjXij,

pmax ≥

i
(cid:88)

rjXij,

j=1
pmin ≤ pmax
pmin ≥ 0.
pmax ≥ 0.

(12a)

(12b)

i = 1, . . . , n

(12c)

i = 1, . . . , n

(12d)

(12e)

(12f)

(12g)

Maximum p-value constraint. A necessary constraint to guarantee that event rates be-
tween consecutive bins are statistically diﬀerent is to impose a maximum p-value constraint
setting a signiﬁcance level α. Suitable statistical tests are the Z-test, Pearson’s Chi-square
test or Fisher ’s exact test. To perform these statistical tests we require an aggregated
matrix of non-event and event records per bin,

RN E

ij =

i
(cid:88)

z=j

rN E
z

, RE

ij =

i
(cid:88)

z=j

rE
z ,

i = 1, . . . , n; j = 1, . . . , i.

The preprocessing procedure to detect pairs of pre-bins that do not satisfy the p-value

constraints using the Z-test is shown in Algorithm 1.

Algorithm 1 Maximum p-value constraint using Z-test
1: procedure p-value violation indices(n, RN E, RE, α)
2:
3:
4:
5:
6:
7:

zscore = Φ−1(1 − α/2)
I = {}
for i = 1, . . . , n − 1 do

l = i + 1
for j = 1, . . . , i do

8:
9:
10:
11:
12:

x = RE
ij
y = RN E
for k = l, . . . , n do

ij

w = RE
kl
z = RN E
if Z-test(x, y, w, z) < zscore then

kl

I = I ∪ (i, j, k, l)

end if

13:
14:
15:
16:
end for
17:
18: end procedure

end for

end for

These constraints are added to the formulation by imposing that, at most, one of the

bins violating the maximum p-value constraint can be selected,

Xij + Xkl ≤ 1,

∀(i, j, k, l) ∈ I.

10

2.1.3 Mixed-integer programming reformulation for local and heuristic search

The number of binary decision variables X is n(n + 1)/2. For large n the N P-hardness of
the combinatorial optimization problem might limit the success of tree-search techniques. A
ﬁrst approach to tackle this limitation is reformulating the problem to reduce the number
of decision variables. First, observe in Figure 1 that a solution is fully characterized by the
diagonal of X. Thus, having the diagonal we can place the ones on the positions satisfying
unique assignment (1) and continuity (2) constraints. On the other hand, to return indexed
elements in any aggregated matrix in the original formulation, we require the position of the
ﬁrst one by row. We note that this information can be retrieved by counting the number of
consecutive zeros between ones (selected bins) of the diagonal. To perform this operation we
use two auxiliary decision variables: an accumulator of preceding zeros ai and the preceding
run-length of zeros zi. A similar approach to counting consecutive ones is introduced in [12].
The described approach is illustrated in Figure 2.

Figure 2: New decision variables suitable for counting consecutive zeros.

The positions in z are zero-based indexes of the reversed rows of the aggregated matrices.
ij =
z for i = 1, . . . , n; j = 1, . . . , i. Same for the aggregated matrices V , D, R and RN E.

For example, the aggregated lower triangular matrix RE is now computed backward: RE
(cid:80)j

z=i rE

Let us deﬁne the parameters of the mathematical programming formulation:

n ∈ N
V[i,zi] ∈ R+
0
D[i,zi] ∈ [0, 1]
R[i,zi] ∈ N
[i,zi] ∈ N
RN E
[i,zi] ∈ N
RE
min ∈ N
rN E
max ∈ N
rN E
min ∈ N
rE
max ∈ N
rE
bmin ∈ N
bmax ∈ N

number of pre-bins.

Information value.

event rate.

number of records.

number of non-event records.

number of event records.

minimum number of non-event records per bins.

maximum number of non-event records per bins.

minimum number of event records per bins.

maximum number of event records per bins.

minimum number of bins.

maximum number of bins.

11

1000000001000010001000010101100101101201000002010      and the decision variables:

xi ∈ {0, 1}
ai ∈ N0
zi ∈ N0

binary indicator variable.

accumulator of preceding zeros.

preceding run-length of zeros.

The new formulation with 3n decision variables is stated as follows

max
X

n
(cid:88)

i=1

V[i,zi]xi

s.t. xn = 1

ai = (ai−1 + 1)(1 − xi),
zi = ai−1(1 − xi−1)xi,

i = 1, . . . , n

i = 1, . . . , n

(13a)

(13b)

(13c)

(13d)

(13e)

xi ≤ bmax

RE

[i,zi]xi ≤ rE

max,

n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

bmin ≤

rmin ≤

rN E
min ≤

rE
min ≤

i=1
xi ∈ {0, 1},
ai ∈ N0,
zi ∈ N0,

R[i,zi]xi ≤ rmax,

i = 1, . . . , n

(13f)

RN E

[i,zi]xi ≤ rN E
max,

i = 1, . . . , n

(13g)

i = 1, . . . , n

i = 1, . . . , n

i = 1, . . . , n

i = 1, . . . , n

(13h)

(13i)

(13j)

(13k)

This MINLP formulation is particularly suitable for Local Search (LS) and heuristic tech-
niques, where decision variable zi can be used as an index, for example, V[i,zi]. The nonlinear
constraints (13d) and (13e) are needed for counting consecutive zeros. After the lineariza-
tion of these constraints via big-M inequalities or indicator constraints [12], the formulation
is adequate for Constraint Programming (CP). Additional constraints such as monotonicity
constraints can be incorporated to (13b - 13k) in a relatively simple manner:

Monotonic trend ascending

D[i,zi]xi + 1 − xi ≥ D[j,zj ]xj + β(xi + xj − 1),

i = 2, . . . , n; z = 1, . . . , i − 1.

Monotonic trend descending

D[i,zi]xi + β(xi + xj − 1) ≤ 1 − xj + D[j,zj ]xj,

i = 2, . . . , n; z = 1, . . . , i − 1.

Monotonic trend concave

−D[i,zi]xi + 2D[j,zj ]xj − D[k,zk]xk ≥ xi + xj + xk − 3,

for i = 3, . . . , n; j = 2, . . . , i − 1; k = 1, . . . , j − 1.

Monotonic trend convex

D[i,zi]xi − 2D[j,zj ]xj + D[k,zk]xk ≥ xi + xj + xk − 3,

for i = 3, . . . , n; j = 2, . . . , i − 1; k = 1, . . . , j − 1.

Monotonic trend peak: constraints (7a - 7c) and

yi + yj + 1 + (D[j,zj ] − 1)xj − D[i,zi]xi ≥ 0
2 − yi − yj + 1 + (D[i,zi] − 1)xi − D[j,zj ]xj ≥ 0,

12

for i = 2, . . . , n; z = 1, . . . , i − 1.

Monotonic trend valley: constraints (7a - 7c) and

yi + yj + 1 + (D[i,zi] − 1)xi − D[j,zj ]xj ≥ 0
2 − yi − yj + 1 + (D[j,zj ] − 1)xj − D[i,zi]xi ≥ 0

for i = 2, . . . , n; z = 1, . . . , i − 1.

In Section 4, we compare the initial CP/MIP formulation to the presented LS formulation

for large size instances.

2.2 Mixed-integer programming formulation for continuous target

The presented optimal binning formulation given a binary target can be seamlessly extended
to a continuous target. Following the methodology developed for Equations (3) and (4), we
could adapt the IV statistic for a continuous target as described in [5]

IV =

n
(cid:88)

i=1

|µ − ui|

ri
rT

,

where µ ∈ R is the target mean for all records (global target mean), rT is total number
of records, and ui and ri are the target mean and number of records for each bin, re-
spectively. The goal of this metric is to obtain bins with a target mean as diﬀerent as
possible from the global target mean. However, we note that the usage of this metric
as objective function might tend to reduce granularity by selecting only a few bins with
large diﬀerences. Therefore, we use only the p-norm distance (L1-norm of L2-norm) term
and exclude the relative number of records term. An aggregated lower triangular matrix
Lij ∈ R+

0 , ∀(i, j) ∈ {1, . . . , n : i ≥ j} can be pre-computed as follows,

Lij = (cid:107)µ − Uij(cid:107)p , Uij =

(cid:80)j

(cid:80)j

z=i sz
z=i rz

,

where Uij ∈ R is the aggregated matrix of target mean values, and si is the sum of target
values for each pre-bin. A more robust approach might replace the mean by an order statistic,
but unfortunately, the aggregated matrix computed from the pre-binning data would require
approximation methods. Finally, replacing Lij in the objective function (5a), the resulting
formulation is given by

max
X

n
(cid:88)

i=1

LiiXii +

i−1
(cid:88)

j=1

s.t.

(5b - 5h)

(Lij − Lij+1)Xij

(16a)

(16b)

2.2.1 Monotonicity constraints

As for the binary target case, we can impose monotonicity constraints between the mean
value of consecutive bins. Since establishing tight bounds for Uij ∈ R is not trivial, we
discard a big-M formulation. Another traditional technique such as the use of SOS1 sets
is also discarded to avoid extra variables and constraints. Instead, we state the ascending
monotonic trend in double implication form as follows

Xii = 1 and Xzz = 1 =⇒ UzzXzz +

z−1
(cid:88)

(Uzj − Uzj+1)Xzj + β

j=1

≤ UiiXii +

i−1
(cid:88)

(Uij − Uij+1)Xij,

j=1

i = 2, . . . , n; z = 1, . . . i − 1.

13

The enforced constraint must be satisﬁed iﬀ the two literals Xii and Xzz are true, otherwise
the constraint is ignored. This is half-reiﬁed linear constraint [4]. The parameter β is the
minimum mean diﬀerence between consecutive bins. Similarly, for the descending constraint,

Xii = 1 and Xzz = 1 =⇒ UiiXii +

i−1
(cid:88)

j=1

(Uij − Uij+1)Xij + β

≤ UzzXzz +

z−1
(cid:88)

(Uzj − Uzj+1)Xzj,

j=1

i = 2, . . . , n; z = 1, . . . i − 1.

Furthermore, the concave and convex trend can be written in triple implication form

using literals Xii, Xjj and Xkk. The concave trend constraints are

Xii = 1, Xzz = 1 and Xkk = 1 =⇒ −

UiiXii +

(cid:32)

i−1
(cid:88)

(Uiz − Uiz+1)Xiz

(cid:33)

(cid:33)

z=1

(cid:32)

+ 2

UjjXjj +

j−1
(cid:88)

z=1

(Ujz − Ujz+1)Xjz

(cid:32)

−

UkkXkk +

k−1
(cid:88)

(Ukz − Ukz+1)Xkz

(cid:33)

≥ 0,

z=1

for i = 3, . . . n; j = 2, . . . , i − 1 and k = 1, . . . , j − 1. Similarly, for convex trend we get

Xii = 1, Xzz = 1 and Xkk = 1 =⇒

UiiXii +

(cid:32)

i−1
(cid:88)

(Uiz − Uiz+1)Xiz

(cid:33)

z=1

(cid:32)

− 2

UjjXjj +

j−1
(cid:88)

(Ujz − Ujz+1)Xjz

(cid:33)

z=1

(cid:32)

UkkXkk +

k−1
(cid:88)

(Ukz − Ukz+1)Xkz

(cid:33)

≥ 0,

z=1

for i = 3, . . . n; j = 2, . . . , i − 1 and k = 1, . . . , j − 1.

The formulation can be extended to support valley and peak trend. The peak trend

requires constraints (7a - 7c) and

Xii = 1 and Xzz = 1 =⇒M (yi + yz) + UzzXzz +

z−1
(cid:88)

j=1

(Uzj − Uzj+1)Xzj

≥ UiiXii +

i−1
(cid:88)

(Uij − Uij+1)Xij,

j=1

M (2 − yi − yz) + UiiXii +

i−1
(cid:88)

j=1

(Uij − Uij+1)Xij

≥ UzzXzz +

z−1
(cid:88)

j=1

(Uzj − Uzj+1)Xzj,

for i = 2, . . . , n; z = 1, . . . , i − 1. The big-M formulation to handle disjoint constraints in
(7a - 7c) requires an eﬀective bound, we suggest M = max({|Uij| : i = 1, . . . , n : i ≥ j}).

14

Similarly, for the valley trend we include constraints and

Xii = 1 and Xzz = 1 =⇒M (yi + yz) + UiiXii +

i−1
(cid:88)

(Uij − Uij+1)Xij

j=1

≥ UzzXzz +

z−1
(cid:88)

j=1

(Uzj − Uzj+1)Xzj,

M (2 − yi − yz) + UzzXzz +

z−1
(cid:88)

j=1

(Uzj − Uzj+1)Xzj,

≥ UiiXii +

i−1
(cid:88)

(Uij − Uij+1)Xij,

j=1

for i = 2, . . . , n; z = 1, . . . , i − 1.

2.2.2 Additional constraints

Maximum p-value constraint. Similar to the binary target case, we can impose a maxi-
mum p-value constraint between consecutive bins to ensure that their means are statistically
diﬀerent. In this case, the T-test for means is appropriate, although it requires the standard
deviation. To compute an aggregate matrix of standard deviations, SDij, we could use

SDij =

(cid:80)j

z=i ssz
z=i rz

(cid:80)j

−

(cid:32) (cid:80)j
(cid:80)j

z=i sz
z=i rz

(cid:33)2

,

where ssi is the sum of squared target values for each pre-bin.

Finally, the algorithm and procedure described in Section 2.1.2 to incorporate these

constraints can be readily reused.

2.3 Mixed-integer programming formulation for multi-class target

A simple approach to support a multi-class target is to use the one-vs-rest scheme with
nC distinct classes. This scheme consists of building a binary target for each class. The
resulting mathematical formulation closely follows the formulation for binary target,

max
X

nC(cid:88)

n
(cid:88)

c=1

i=1

V c
iiXii +

i−1
(cid:88)

j=1

s.t.

(5b - 5e)

(V c

ij − V c

ij+1)Xij

(19a)

(19b)

Note that for this formulation we need an aggregated matrix V and D for each class c. It
is important to emphasize that the monotonicity constraints in Section 2.1.1 act as link-
ing constraints among classes, otherwise, nC optimal binning problems with binary target
could be solved separately. Again, additional constraints described in Section 2.1.2 can be
naturally incorporated with minor changes.

3 Algorithmic details and implementation

3.1 Automatic monotonic trend algorithm

Our approach to automate the monotonic trend decision employs a Machine Learning (ML)
classiﬁer that predicts, given the pre-binning data, the most suitable monotonic trend to
In particular, we aim to integrate an oﬀ-line classiﬁer,
maximize discriminatory power.

15

hence we are merely interested in ML classiﬁcation algorithms easily embeddable. Recently,
a similar approach was implemented in the commercial solver CPLEX to make automatic
decisions over some algorithmic choices [19].

For this study, a dataset is generated with 417 instances collected from public datasets.
We design a set of 16 numerical features, describing the pre-binning instances in terms of
number of pre-bins, distribution of records per pre-bin and trend features. The most relevant
trend features are: number of trend change points, linear regression coeﬃcient sense, area
of the convex hull, and area comprised among extreme trend points.

The labeling procedure consists of solving all instances selecting the ascending (A), de-
scending (D), peak (P) and valley (V) monotonic trend. Concave and convex trends are
In what follows,
discarded due to being a special case of peak and valley, respectively.
without loss of generality, we state the procedure for the binary target case: if the relative
diﬀerence between IV with ascending/descending monotonic trend and IV with peak/valley
trend is less than 10%, the ascending/descending monotonic trend is selected, due to the
lesser resolution times. Table 1 summarizes the composition of the dataset with respect to
assigned labels. We note that the dataset is slightly unbalanced, being predominant the
descending label (D).

Label
A
D
P
V
Total

Instances Frequency (%)

84
200
76
57
417

20
48
18
14

Table 1: Number of instances and percentage for each label.

To perform experiments, the dataset is split into train and test subsets in a stratiﬁed
manner to treat unbalanced data. The proportion of the test set is 30%. Three interpretable
multi-class classiﬁcation algorithms are tested, namely, logistic regression, decision trees
(CART) and Support vector Machine (SVM) using the Python library Scikit-learn [20].
All three algorithms are trained using option class weight="balanced". Throughout the
learning process, we discard 8 features and perform hyperparameter optimization for all
three algorithms. These experiments show that SVM and CART have similar classiﬁcation
measures, and we decide to choose CART (max depth 5) to ease implementation.

On the test set, the trained CART has a weighted average accuracy, precision and recall
of 88%. See classiﬁcation measures and the confusion matrix in Table 2. We observe that
various instances of the minority class (V) are misclassiﬁed, indicating that more instances or
new features might be required to improve classiﬁcation measures. Improving this classiﬁer
is part of ongoing research.

Label
A
D
P
V
weighted avg

Precision Recall F1-score Support

0.85
0.97
0.81
0.71
0.88

0.88
0.93
0.88
0.59
0.88

0.86
0.95
0.88
0.65
0.88

25
61
23
17
126

A D P V
2
2
0
10

0
57
0
2

1
2
22
2

A 22
D 0
P 1
V 3

Table 2: Classiﬁcation measures (left) and confusion matrix (right) for CART on the test
set.

3.2 Presolving algorithm

The mathematical programming formulation is a hard combinatorial optimization problem
that does not scale well as the number of pre-bins increases. To reduce solution times we

16

need to reduce the search space to avoid deep tree searches during branching. The idea is to
develop a presolving algorithm to ﬁx bins not satisfying monotonicity constraints, after that
the default presolver may be able to reduce the problem size signiﬁcantly. The presolving
algorithm applies to the binary target case and was developed after several observations
about the aggregated matrix of event rates D. Algorithm 2 shows the implemented approach
for the ascending monotonicity trend. Presolving algorithm for the descending monotonicity
is analogous, only requiring inequalities change.

for i = 1, . . . , n − 1 do

Algorithm 2 Preprocessing ascending monotonicity
1: procedure PreprocessingAscending(D, X, β)
2:
3:
4:
5:
6:
7:

end if
for j = 1, . . . , n − i − 1 do

if Di+1+j,i − Di+1+j,i+1+j > 0 then

if Di+1,i − Di+1,i+1 > 0 then

ﬁx Xi,i = 0

ﬁx Xi+j,i+j = 0

end if

8:
9:
10:
end for
11:
12: end procedure

end for

3.3 Binning quality score

To assess the quality of binning for binary target, we develop a binning quality score con-
sidering the following aspects:

• Predictive power: IV rule of thumb [21] in Table 3.

• Statistical signiﬁcance: bin event rates must be statistically diﬀerent, therefore large

p-values penalize the quality score.

• Homogeneity: binning with homogeneous bin sizes or uniform representativeness, in-

creases reliability.

IV
[0, 0.02)
[0.02, 0.1)
[0.1, 0.3)
[0.3, 0.5)
[0.5, ∞)

predictive power
not useful
weak
medium
strong
over-prediction

Table 3: Information Value rule of thumb.

To account for all these aspects, we propose a rigorous binning quality score function

Proposition 3.1 Given a binning with Information Value ν, p-values between consecutive
bins pi, i = 1, . . . , n − 1 and normalized bins size si, i = 1, . . . , n, the binning quality score
function is deﬁned as

Q(ν, p, s) =

ν
c

exp (cid:0)−ν2/(2c2) + 1/2(cid:1)

(cid:32)n−1
(cid:89)

(1 − pi)

i=1

(cid:33) (cid:18) 1 − (cid:80)n

i=1 s2
i

1 − 1/n

(cid:19)

,

(20)

where Q(ν, p, s) ∈ [0, 1] and c = 1
5

(cid:113) 2

log(5/3) is the best a priori IV value in [0.3, 0.5).

17

Proof: Given the rule of thumb in Table 3, let us consider the set of statistical distributions
with positive skewness, positive fat-tail, and support on the semi-inﬁnite interval [0, ∞). The
function should penalize large values of Information Value ν, and fast decay is expected after
a certain threshold indicating over-prediction. This fast decay is a required property that
must be accompanied by the following statement: limν→0 f (ν) = limν→∞ f (ν) = 0 Among
the available distributions satisfying aforementioned properties, we select the Rayleigh dis-
tribution, which probability density function is given by

ν
c2 e−ν2/(2c2),
This is a statistical distribution, not a function, hence we need a scaling factor so that
maxν∈[0,∞) f (ν; c) = 1: the maximum value of a unimodal probability distribution is the
mode c, thus

f (ν; c) =

ν ≥ 0.

γ = f (c, c) =

1
√

c

e

=⇒

f (ν, c)
γ

=

ν exp (cid:0)−ν2/(2c2) + 1/2(cid:1)
c

.

The optimal c such that f (a) = f (b) for b > a can be obtained by solving f (b; c)−f (a; c) = 0
for c, which yields

c∗ =

√

b2 − a2
(cid:112)2 log(b/a)

.

Term (cid:81)n−1
1−(cid:80)n
i=1 s2

i

the homogeneity/uniformity of the bin sizes.

i=1 (1 − pi) assesses the statistical signiﬁcance of the bins. Furthermore, term
1−1/n = 1−HHI ∗, where HHI ∗ is the normalized Herﬁndahl Hirschman Index, assesses
(cid:3)
For example, if we consider that the boundaries of the interval with strong IV predictive
power in Table 3, a = 0.3 and b = 0.5, should produce the same quality score, c∗(a, b) =
1
log(5/3) . Table 4 shows the value of f (ν, c∗) for various IV values ν; note the fast decay
5
of f (ν, c∗) when ν > 0.5.

(cid:113) 2

ν
f (ν, c∗)

0
0

0.02
0.083

0.1
0.404

0.3
0.938

0.5
0.938

0.7
0.610

0.9
0.282

1
0.171

1.5
0.005

Table 4: Function values for various ν values.

3.4 Implementation

The presented mathematical programming formulations are implemented using Google OR-
Tools [16] with the open-source MILP solver CBC [7], and Google’s BOP and CP-SAT
solvers. Besides, the specialized formulation in Section 2.1.3 is implemented using the
commercial solver LocalSolver [1]. The python library OptBinning3 has been developed
throughout this work to ease usability and reproducibility.

Much of the implementation eﬀort focuses on the careful implementation of constraints
and the development of fast algorithms for preprocessing and generating the model data.
A key preprocessing algorithm is a pre-binning reﬁnement developed to guarantee that no
bins have 0 non-events and events records in the binary target case.

Categorical variables require special treatment: pre-bins are ordered in ascending order
with respect to a given metric; the event rate for binary target and the target mean for a
continuous target. The original data is replaced by the ordered indexes and is then used as
a numerical (ordinal) variable. Furthermore, during preprocessing, the non-representative
categories may be binned into an “others” bin. Similarly, missing values and special values
are incorporated naturally as additional bins after the optimal binning is terminated.

3https://github.com/guillermo-navas-palencia/optbinning

18

4 Experiments

The experiments were run on an Intel(R) Core(TM) i5-3317 CPU at 1.70GHz, using a
single core, running Linux. Two binning examples are shown in Tables 5 and 6, using Fair
Isaac (FICO) credit risk dataset [6] (N = 10459) and Home Credit Default Risk Kaggle
competition dataset [9] (N = 307511), respectively.

Example in Table 5 uses the variable AverageMInFile (Average Months in File) as an
risk driver. FICO dataset imposes monotonicity constraints to some variables, in particular,
for this variable, the event rate must be monotonically decreasing. Moreover, the dataset
includes three special values/codes deﬁned as follows:

• -9: No Bureau Record or No Investigation

• -8: No Usable/Valid Trades or Inquiries

• -7: Condition not Met (e.g. No Inquiries, No Delinquencies)

For the sake of completeness, we also include a few random missing values on the dataset. As
shown in Table 5, these values are separately treated by incorporating a Special and Missing
bin. Regarding computation time, this optimal binning instance is solved in 0.08 seconds.
The optimization time accounts for 91% of the total time, followed by the pre-binning time
representing about 6%. The remaining 3% is spent in pre-processing and post-processing
operations.

Bin
(−∞, 30.5)
[30.5, 48.5)
[48.5, 54.5)
[54.5, 64.5)
[64.5, 70.5)
[70.5, 74.5)
[74.5, 81.5)
[81.5, 101.5)
[101.5, 116.5)
[116.5, ∞)
Special
Missing

Count Count (%) Non-event Event Event rate WoE

544
1060
528
1099
791
536
912
2009
848
1084
558
490

0.052013
0.101348
0.050483
0.105077
0.075629
0.051248
0.087198
0.192083
0.081078
0.103643
0.053351
0.046850

99
286
184
450
369
262
475
1141
532
702
252
248

445
774
344
649
422
274
437
868
316
382
306
242

0.818015
0.730189
0.651515
0.590537
0.533502
0.511194
0.479167
0.432056
0.372642
0.352399
0.548387
0.493878

-1.41513
-0.907752
-0.537878
-0.278357
-0.046381
0.0430441
0.171209
0.361296
0.608729
0.696341
-0.106328
0.112319

IV
0.087337
0.076782
0.014101
0.008041
0.000162
0.000095
0.002559
0.025000
0.029532
0.049039
0.000601
0.000592

JS
0.010089
0.009281
0.001742
0.001002
0.000020
0.000012
0.000320
0.003108
0.003636
0.006009
0.000075
0.000074

Table 5: Example optimal binning using variable AverageMInFile from FICO dataset.

Example in Table 6 uses the categorical variable ORGANIZATION TYPE from the
Kaggle dataset. This variable has 58 categories, and we set the non-representative cate-
gories cut-oﬀ to 0.01. Note that the bin just before the Special bin corresponds to the bin
with non-representative categories, which is excluded from the optimization problem, hence
monotonicity constraint does not apply. This optimal binning instance is solved in 0.25
seconds. For categorical variables, most of the time is spent on pre-processing, 71% in this
particular case, whereas the optimization problem is solved generally faster.

Bin
[XNA, School]
[Medicine, ...]
[Other]
[Business ...]
[Transport: ...]
[Security, ...]
[Housing, ...]
Special
Missing

Count Count (%) Non-event Event Event rate WoE
64267
31845
16683
16537
81221
55150
41808
0
0

0.208991
0.103557
0.054252
0.053777
0.264124
0.179343
0.135956
0
0

0.054709
0.068205
0.076425
0.083873
0.093129
0.103826
0.076182
0
0

0.416974
0.182104
0.0594551
-0.0416281
-0.156466
-0.277067
0.0629101
0
0

60751
29673
15408
15150
73657
49424
38623
0
0

3516
2172
1275
1387
7564
5726
3185
0
0

IV
0.030554
0.003182
0.000187
0.000095
0.006905
0.015465
0.000524
0
0

JS
0.003792
0.000397
0.000023
0.000012
0.000862
0.001927
0.000065
0
0

Table 6: Example optimal binning using categorical variable ORGANIZATION TYPE from
Home Credit Default Risk Kaggle competition dataset.

19

4.1 Benchmark CP/MIP vs local search heuristic

For large instances, we compare the performance of Google OR-Tools’ solvers BOP (MIP)
and CP-SAT against LocalSolver. For these tests we select two variables from Home Credit
Default Risk Kaggle competition dataset [9] (N = 307511). We aim to perform a far ﬁner
binning than typical in many applications to stress the performance of classical solvers for
large combinatorial optimization problems.

Tables 7 and 8 show results for varying number of pre-bins n and monotonic trends. In
test 1 from Table 7 , LocalSolver does not improve after 10 seconds, not being able to reduce
the optimality gap. In test 2, LocalSolver outperforms CP-SAT, ﬁnding the optimal solution
after 5 seconds, 28x faster. In test 3, solution times are comparable. Results reported in
Table 8 are also interesting; in test 1, BOP and CP-SAT solvers cannot ﬁnd an optimal
solution after 1000 seconds. LocalSolver ﬁnds the best found feasible solution after 30
seconds. Nevertheless, we recall that the described heuristic for peak/valley trend introduced
in Section 2.1.1 could reduce resolution times substantially, obtaining times comparable to
those when choosing ascending/descending monotonic trend.

n monotonic trend solver variables
48
48
48
48
77
77
77
77
77

peak
peak
peak
peak
peak
peak
peak
descending
descending

1225
193
193
193
3081
309
309
3003
231

cp
ls
ls
ls
cp
ls
ls
cp
ls

constraints
3528
2352
2352
2352
9009
6007
6007
6706
2969

time
12.7
1
5
10
140.9
1
5
0.9
1

solution
0.03757878
0.03373904
0.03386574
0.03725560
0.03776231
0.03078212
0.03776231
0.03386574
0.03386574

gap
-
10.2%
9.9%
0.9%
-
18.5%
0.0%
-
0.0%

Table 7: Variable REGION POPULATION RELATIVE. Performance comparison Google
OR-Tools’ CP-SAT vs LocalSolver. Time in seconds.

n monotonic trend solver variables
100
100
100
100
100
100
100
100

peak
peak
peak
peak
peak
peak
ascending
ascending

5151
5151
401
401
401
401
5050
300

cp
mip
ls
ls
ls
ls
cp
ls

constraints
15150
15150
10101
10101
10101
10101
10933
5000

time
t
t
1
5
10
30
2.3
1

solution
0.11721972
0.11786335∗
0.11666556
0.11735812
0.11771822
0.11786335
0.05175782
0.05175782

gap
-
-
1.0%
0.4%
0.1%
0.0%
-
0.0%

Table 8: Variable DAYS EMPLOYED. Performance comparison Google OR-Tools’ CP-
SAT/BOP vs LocalSolver. Time in seconds. *: Best feasible solution. t: 1000 seconds
exceeded.

5 Conclusions

We propose a rigorous and ﬂexible mathematical programming formulation to compute the
optimal binning. This is the ﬁrst optimal binning algorithm to achieve solutions for nontriv-
ial constraints, supporting binary, continuous and multi-class target, and handling several
monotonic trends rigorously. Importantly, the size of the decision variables and constraints
used in the presented formulations is independent of the size of the datasets; they are en-
tirely controlled by the starting solution computed during the pre-binning process. In the
future, we plan to extend our methodology to piecewise-linear binning and multivariate bin-
ning. Lastly, the code is available at https://github.com/guillermo-navas-palencia/
optbinning to ease reproducibility.

20

References

[1] T. Benoist, B. Estellon, Gardi F., R. Megel, and K. Nouioua. Localsolver 1.x: a black-

box local-search solver for 0-1 programming. 4OR-Q J Oper Res, 9(299), 2011.

[2] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classiﬁcation and Regression Trees.

1984.

[3] U. M. Fayyad and K. B. Irani. Multi-Interval Discretization of Continuous-Valued
International Joint Conferences on Artiﬁcial

Attributes for Classiﬁcation Learning.
Intelligence, 13:1022–1027, 1993.

[4] T. Feydy, Z. Somogyi, and P. J. Stuckey. Half reiﬁcation and ﬂattening. In Principles
and Practice of Constraint Programming – CP 2011, pages 286–301, Berlin, Heidelberg,
2011. Springer Berlin Heidelberg.

[5] FICO. Building powerful, predictive scorecards. 2014.

[6] FICO, Google, Imperial College London, MIT, University of Oxford, UC Irvine and UC
Berkeley. Explainable Machine Learning Challenge. https://community.fico.com/
s/explainable-machine-learning-challenge, 2018.

[7] J. Forrest, T. Ralphs, S. Vigerske, and et al. coin-or/cbc: Version 2.9.9. 2018.

[8] J. Herman. smbinning: Scoring Modeling and Optimal Binning, 2019.

[9] Home Credit Group. Kaggle competition: Home Credit Default Risk. https://www.

kaggle.com/c/home-credit-default-risk/overview, 2018.

[10] T. Hothorn, K. Hornik, and A. Zeileis. Unbiased recursive partitioning: A conditional
inference framework. Journal of Computational and Graphical statistics, 15(3):651–674,
2006.

[11] H. Jeﬀreys. An invariant form for the prior probability in estimation problems. Pro-
ceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences,
186(1007):453–461, 1946.

[12] E. Kalvelagen.

A diﬃcult MIP construct:
https://yetanothermathprogrammingconsultant.blogspot.com/2018/04/
a-difficult-mip-construct-counting.html, 2018.

counting

consecutive

1’s.

[13] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, and T. Liu. LightGBM:
A Highly Eﬃcient Gradient Boosting Decision Tree.
In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances
in Neural Information Processing Systems 30, pages 3146–3154. Curran Associates,
Inc., 2017.

[14] R. Kerber. ChiMerge: Discretization of Numeric Attributes. AAAI-92 Proceedings,

1992.

[15] S. Kullback and R. A. Leibler. On Information and Suﬃciency. Ann. Math. Statist.,

22(1):79–86, 1951.

[16] P. Laurent and F. Vincent. Google OR-Tools 7.7. https://developers.google.com/

optimization/, 2020.

[17] P. Mironchyk and V. Tchistiakov. Monotone optimal binning algorithm for credit risk

modeling. 2017.

21

[18] I. Oliveira, M. Chari, and S. Haller. Rigorous Constrained Optimization Binning for
Credit Scoring. SAS Global Forum 2008 - Data Mining and Predictive Modelling, 2008.

[19] A. Lodi P. Bonami and G. Zarpellon. Learning a Classiﬁcation of Mixed-Integer
Quadratic Programming Problems.
In W.-J. van Hoeve, editor, Integration of Con-
straint Programming, Artiﬁcial Intelligence, and Operations Research – CPAIOR 2018,
Lecture Notes in Computer Science, pages 595–604. Springer-Verlag, 2018.

[20] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine Learning in Python .
Journal of Machine Learning Research, 12:2825–2830, 2011.

[21] N. Siddiqi. Credit Risk Scorecards: Developing And Implementing Intelligent Credit

Scoring. Wiley and SAS Business Series. Wiley, 2005.

[22] L. C. Thomas, D. B. Edelman, and J. N. Crook. Credit Scoring and Its Applications.

Society for Industrial and Applied Mathematics, 2002.

[23] L. WenSui. Monotonic Optimal Binning (MOB) for Risk Scorecard Development, 2020.

[24] G. Zeng. Metric Divergence Measures and Information Value in Credit Scoring. Journal

of Mathematics, 2013:1–10, 2013.

22

