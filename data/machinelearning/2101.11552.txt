1
2
0
2

n
a
J

7
2

]

G
L
.
s
c
[

1
v
2
5
5
1
1
.
1
0
1
2
:
v
i
X
r
a

Efficient Graph Deep Learning in TensorFlow with tf_geometric

Jun Hu1, Shengsheng Qian1,2, Quan Fang1,2, Youze Wang3
Quan Zhao3, Huaiwen Zhang1,2,4, Changsheng Xu1,2
1National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences
2School of Artificial Intelligence, University of Chinese Academy of Sciences
3Hefei University of Technology
4Key Laboratory of Artificial Intelligence Scenario Application and Intelligent System Evaluation (China Center for
Information Industry Development), Ministry of Industry and Information Technology
hujunxianligong@gmail.com,{shengsheng.qian,qfang}@nlpr.ia.ac.cn
{youze.wang,2019111002}@mail.hfut.edu.cn,{huaiwen.zhang,csxu}@nlpr.ia.ac.cn

ABSTRACT
We introduce tf_geometric1, an efficient and friendly library for
graph deep learning, which is compatible with both TensorFlow 1.x
and 2.x. tf_geometric provides kernel libraries for building Graph
Neural Networks (GNNs) as well as implementations of popular
GNNs. The kernel libraries consist of infrastructures for building
efficient GNNs, including graph data structures, graph map-reduce
framework, graph mini-batch strategy, etc. These infrastructures
enable tf_geometric to support single-graph computation, multi-
graph computation, graph mini-batch, distributed training, etc.;
therefore, tf_geometric can be used for a variety of graph deep
learning tasks, such as transductive node classification, inductive
node classification, link prediction, and graph classification. Based
on the kernel libraries, tf_geometric implements a variety of popular
GNN models for different tasks. To facilitate the implementation of
GNNs, tf_geometric also provides some other libraries for dataset
management, graph sampling, etc. Different from existing popular
GNN libraries, tf_geometric provides not only Object-Oriented
Programming (OOP) APIs, but also Functional APIs, which enable
tf_geometric to handle advanced graph deep learning tasks such
as graph meta-learning. The APIs of tf_geometric are friendly, and
they are suitable for both beginners and experts. In this paper,
we first present an overview of tf_geometric’s framework. Then,
we conduct experiments on some benchmark datasets and report
the performance of several popular GNN models implemented by
tf_geometric.

KEYWORDS
Graph Neural Networks, Graph Deep Learning, Network Represen-
tation Learning

1 INTRODUCTION
Graph is a powerful data structure that can be used to model re-
lational data, and it is widely used by real-world applications. In
recent years, Graph Neural Networks (GNNs) emerge as power-
ful tools for deep learning on graphs, which aims to understand
the semantics of graph data. GNNs have been successfully applied
to a variety of tasks in different fields, such as recommendation
systems [5, 24, 30], question answering systems [6, 12, 16], neural

1https://github.com/CrawlScript/tf_geometric

Figure 1: An Example of Message Aggregation

machine translation [1, 18], traffic prediction [3, 31], drug discov-
ery and design [9, 10], diagnosis prediction [17, 21], and physical
simulation [19, 22].

Due to the properties of graph data, such as sparsity and irreg-
ularity, it is challenging to implement efficient and friendly GNN
libraries. It is known that the most challenging problem for imple-
menting GNNs is the aggregation of graph data. Aggregation is the
fundamental operation for most GNNs. There are mainly two types
of aggregation in GNNs: message aggregation and graph pooling.
(1) The message aggregation, which is also called message pass-
ing, aims to aggregate multiple messages between a node and its
context and reduce them into one element. Fig. 1 shows an example
of message aggregation in Graph Convolutional Networks (GCNs).
In the example, the context of node 𝑣𝑖 consists of its neighbor nodes
and itself. The context nodes pass multiple messages to 𝑣𝑖 and these
messages are reduced to a feature vector, which is then used as the
high-order representation of node 𝑣𝑖 . (2) The graph pooling aims
to aggregate elements in graphs or clusters and reduce them into
high-order graph-level or cluster-level representations. Fig. 2(a)
shows an example of graph pooling for learning graph-level rep-
resentations. The representations of all the nodes in the graph are
aggregated to generate the representation of the graph. In some
complex graph pooling models, the graph pooling layers are used
to obtain a pooled graph rather than a graph representation vec-
tor [15, 20, 34]. For example, as shown in the hierarchical graph
pooling example in Fig. 2(b), the graph pooling operation aggre-
gates nodes in each cluster and reduces them as a node in the pooled
graph. The two main types of aggregation allow researchers and
engineers to design complex GNNs, and thus proper solutions for
aggregation on graphs are imperative for building elegant GNN
models. However, it is non-trivial to design proper aggregation so-
lutions for sparse and irregular graph data. Most intuitive solutions,

vimsgmsgmsgmsgmsgmsgMessage AggregationviReduced InformationMessagesmsg 
 
 
 
 
 
Jun Hu, Shengsheng Qian, Quan Fang, Youze Wang, Quan Zhao, Huaiwen Zhang, Changsheng Xu

(a) Common Graph Pooling

(b) Hierarchical Graph Pooling

Figure 2: Examples of Graph Pooling

2.1 Kernel Libraries
The framework of the kernel libraries is shown in Fig. 3. As shown
in Fig. 3, the kernel libraries consist of several fundamental compo-
nents as infrastructures for building efficient GNNs, including graph
data structures, graph map-reduce framework, graph mini-batch
strategy, etc. These infrastructures enable tf_geometric to support
single-graph computation, multi-graph computation, graph mini-
batch, distributed training, etc., and therefore tf_geometric can be
used for a variety of graph deep learning tasks, such as transductive
node classification, inductive node classification, link prediction,
and graph classification. In this section, we will introduce these
infrastructures in detail.

2.1.1 Graph Data Structure. tf_geometric has two core graph data
structures (classes): Graph and BatchGraph, which are used to
model a single graph and a batch of graphs, respectively. In this
section, we first introduce some notations for graph data in graph
deep learning and then show how tf_geometric organizes graph
data with its graph data structures.

Generally, a graph can be represented as 𝐺 = (𝑉 , 𝐸), where
𝑉 = {𝑣1, 𝑣2, ..., 𝑣 |𝑉 | } is the set of nodes and 𝐸 = {𝑒1, 𝑒2, ..., 𝑒 |𝐸 | }
denotes the set of edges. In graph deep learning, the graph 𝐺 is
usually presented as G = (𝑋, 𝐴), where 𝑋 and 𝐴 are the node
feature matrix and adjacency matrix, respectively. The node feature
matrix 𝑋 ∈ R|𝑉 |×𝑑 contains features of all the nodes in the graph,
and its 𝑖𝑡ℎ row represents the 𝑑-dimensional feature vector of the 𝑖𝑡ℎ
node in the graph. The adjacency matrix 𝐴 ∈ R |𝑉 |×|𝑉 | contains the

such as padding and masking, usually suffer from the memory and
efficiency problem, whereas many efficient solutions, such as sparse
matrix multiplication, require a lot of complex tricks to accomplish
advanced aggregations. Moreover, most efficient solutions require
users to use specific data structures to organize the graph data.

We develop tf_geometric, an efficient and friendly GNN library
for deep learning on sparse and irregular graph data, which is com-
patible with both TensorFlow 1.x and 2.x. tf_geometric provides a
unified solution for GNNs, which mainly consists of kernel libraries
for building graph neural networks and implementations of popu-
lar GNNs. The kernel libraries contain infrastructures for building
efficient GNNs, including graph data structures, graph map-reduce
framework, graph mini-batch strategy, etc. In particular, the graph
data structure and map-reduce framework provide an elegant and
efficient way for aggregation on graphs. The kernel libraries enable
tf_geometric to support single-graph computation, multi-graph
computation, graph mini-batch, distributed training, etc.; therefore,
tf_geometric can be used for a variety of graph deep learning tasks,
such as transductive node classification, inductive node classifica-
tion, link prediction, and graph classification. Based on the kernel
libraries, a variety of popular GNN models for different tasks are
implemented as APIs for tf_geometric. To facilitate the implemen-
tation of GNNs, tf_geometric also provides some other libraries for
dataset management, graph sampling, etc. Different from existing
popular GNN libraries, tf_geometric provides not only OOP APIs,
but also Functional APIs, which enable tf_geometric to handle ad-
vanced graph deep learning tasks such as graph meta-learning. The
APIs of tf_geometric are friendly, and they are suitable for both
beginners and experts. tf_geometric is available on GitHub2. The
features of tf_geometric are thoroughly documented3 and a collec-
tion of accompanying tutorials and examples are also provided in
the documentation.

2 OVERVIEW
tf_geometric mainly consists of kernel libraries for building graph
neural networks and implementations of popular GNNs. Besides,
some other libraries such as dataset management and graph sam-
pling are also provided to facilitate the implementation of GNNs.
In this section, we provide an overview of different parts of the
tf_geometric.

2https://github.com/CrawlScript/tf_geometric
3https://tf-geometric.readthedocs.io

Figure 3: The Framework of tf_geometric

GraphReduced GraphPoolingGraphPoolingReduced GraphclusterclusterclusterclusterclusterPoolingReduced GraphTensorFlow Runtimetf_geometricKernelGraph Data StructureGraph Map-Reduce FrameworkGraph Mini-batch StrategyCore OOP APICore Functional APIDistribution StrategyDataset ManagementGraph UtilitiesFunctional APIgcngatsag_pooldiff_poolappnp…OOP APIGCNGATSAGPoolDiffPoolAPPNP…Other APIEfficient Graph Deep Learning in TensorFlow with tf_geometric

Figure 4: tfg.Graph and tfg.BatchGraph

edge information, where a positive entry 𝐴𝑖 𝑗 indicates these exists
an edge from the 𝑖𝑡ℎ node 𝑣𝑖 to the 𝑗𝑡ℎ node 𝑣 𝑗 with weight 𝐴𝑖 𝑗 .
In some tasks, such as node classification and graph classification,
the node label or graph label information is also required. The
label information is denoted as 𝑌 , and the graph can be further
represented as G = (𝑋, 𝐴, 𝑌 ). Usually, 𝑌 is presented as a list of
integer label indices or a matrix of encoded label vectors.

The Graph class is used to model a single graph. A graph G =
(𝑋, 𝐴, 𝑌 ) can be modeled as a Graph object 𝑔𝑟𝑎𝑝ℎ = (𝑥, 𝑒𝑑𝑔𝑒_𝑖𝑛𝑑𝑒𝑥,
𝑒𝑑𝑔𝑒_𝑤𝑒𝑖𝑔ℎ𝑡, 𝑦) , where 𝑥, (𝑒𝑑𝑔𝑒_𝑖𝑛𝑑𝑒𝑥, 𝑒𝑑𝑔𝑒_𝑤𝑒𝑖𝑔ℎ𝑡), and 𝑦 cor-
respond to 𝑋 , 𝐴 and 𝑌 , respectively. The node feature matrix 𝑋 and
label information 𝑌 are modeled as dense tensors 𝑥 and 𝑦 respec-
tively, while the adjacency matrix 𝐴 is presented as a sparse matrix
in coordinate (COO) format, which consists of the indices of entries
𝑒𝑑𝑔𝑒_𝑖𝑛𝑑𝑒𝑥 ∈ R2×|𝐸 | and the values of entries 𝑒𝑑𝑔𝑒_𝑤𝑒𝑖𝑔ℎ𝑡 ∈ R |𝐸 |.
It is known that many aggregation operations in GNNs can benefit
a lot from the COO format sparse adjacency matrix. Moreover, the
COO format data is friendly to many advanced aggregation oper-
ations of TensorFlow, such as the tf.math.segment_xxxx opera-
tions, which are important for building efficient GNN models. Note
that either the input data or the intermediate output tensors can be
used to construct graph objects. Especially, since the construction
of Graph objects does not involve any deep copy operations, it is a
differentiable operation that can be applied to any intermediate out-
put tensors that require gradients. For each Graph object, the GNN
models in tf_geometric can take advantage of parallelism capabili-
ties of deep learning frameworks to efficiently process information
in the graph. However, due to the limitation of most deep learning
frameworks, it is difficult to process information in different graph
objects simultaneously. Therefore, for tasks dealing with multiple
graphs, such as inductive node classification and graph classifica-
tion, tf_geometric introduces the BatchGraph class, which allow
the GNN models to process information in multiple graphs in par-
allel.

A BatchGraph object stores the information in multiple graph
objects, and it enables the parallel processing of data from different
graphs by virtualizing multiple graphs as a single graph. The Batch-
Graph class is a subclass of the Graph class, and it can be denoted as

𝑏𝑎𝑡𝑐ℎ_𝑔𝑟𝑎𝑝ℎ = (𝑥, 𝑒𝑑𝑔𝑒_𝑖𝑛𝑑𝑒𝑥, 𝑒𝑑𝑔𝑒_𝑤𝑒𝑖𝑔ℎ𝑡, 𝑦, 𝑛𝑜𝑑𝑒_𝑔𝑟𝑎𝑝ℎ_𝑖𝑛𝑑𝑒𝑥),
where 𝑥, 𝑒𝑑𝑔𝑒_𝑖𝑛𝑑𝑒𝑥, 𝑒𝑑𝑔𝑒_𝑤𝑒𝑖𝑔ℎ𝑡, and 𝑦 are attributes inherited
from the superclass Graph, and 𝑛𝑜𝑑𝑒_𝑔𝑟𝑎𝑝ℎ_𝑖𝑛𝑑𝑒𝑥 is a list of in-
tegers indicating which graph each node belong to. tf_geometric
first leverages a reindexing trick to reassign indices to nodes from
different graphs such that each node has a unique index in the
BatchGraph. The left part of Fig. 4 shows an example of combining
multiple Graph objects into a BatchGraph object. The 𝑗𝑡ℎ node of
the 𝑖𝑡ℎ graph is reindexed by adding an offset value, which is the
number of nodes in the previous 𝑖 − 1𝑡ℎ graphs. In the example,
the offset of the second graph and third graph is 4 and 6 respec-
tively. Therefore, the index of the first node in the second graph
is reindexed as 1 + 4 = 5, and the index of the second node in
the third graph is reassigned as 2 + 6 = 8. After reindexing, the
attributes of the graphs such as 𝑥 and 𝑒𝑑𝑔𝑒_𝑖𝑛𝑑𝑒𝑥 are then adjusted
and combined based on the reassigned node indices. For example,
the 𝑒𝑑𝑔𝑒_𝑖𝑛𝑑𝑒𝑥 of the given graphs are first replaced with the rein-
dexed node index and they are then stacked together to form a new
𝑒𝑑𝑔𝑒_𝑖𝑛𝑑𝑒𝑥 for the BatchGraph. Note that converting Graph objects
into a BatchGraph object does not modify node features and the
connectivity between nodes. Thus, for most GNNs, applying them
on multiple graphs iteratively is equivalent to applying them on the
corresponding BatchGraph. As a result, applying GNN operations
on a BatchGraph automatically enables the parallel processing of
multiple graphs, which brings dramatic performance improvement
in computational efficiency. As shown in the right part of Fig. 4, the
GNNs learn high-order features for nodes from different graphs,
which can be further used for different multi-graph tasks. For ex-
ample, the learned node features can be directly used for inductive
node classification tasks. Moreover, the learned node features can
be aggregated into graph representations (graph pooling) based on
the 𝑛𝑜𝑑𝑒_𝑔𝑟𝑎𝑝ℎ_𝑖𝑛𝑑𝑒𝑥 of the BatchGraph, which can be used for
graph-level tasks such as graph classification.

2.1.2 Graph Map-Reduce Framework. Many complex GNNs can
be considered as a combination of simple map-reduce operations
on graphs. Usually, map and reduce operations correspond to trans-
formation and aggregation operations on graphs. The tf_geometric
kernel provides a graph map-reduce framework, including basic

v3v2v4v1v2v1tfg.GraphObjectsxxedge_index, edge_weightedge_index, edge_weight0.40.21.00.70.6yyedge_index, edge_weightxyv3v2v10.10.8A tfg.BatchGraphObjectxedge_index, edge_weightMergev3v2v4v1v6v50.40.21.00.70.6v9v8v70.10.8yHigh-order Node Representationsxedge_index, edge_weightv3v2v4v1v6v50.40.21.00.70.6v9v8v70.10.8yv11v21v31v41v52v62v73v83v93node_graph_indexv11v21v31v41v52v62v73v83v93node_graph_indexGNNsHigh-order GraphRepresentationsNode-levelTasksGraph-levelTasksTasksPoolingJun Hu, Shengsheng Qian, Quan Fang, Youze Wang, Quan Zhao, Huaiwen Zhang, Changsheng Xu

of the neighbor nodes of a node share the same reduce key. Dif-
ferent from common deep learning models for images and text,
where most reduce operations are designed for tensors with regular
shapes, the graph deep learning models usually require the reducer
to deal with irregular data. Thus, many general reduce operations,
such as tf.reduce_sum and tf.reduce_max, can not be used as
reducers for GNNs. To address this problem, tf_geometric takes
advantage of several advanced APIs of TensorFlow to build efficient
reducers for irregular graph data. In the example, the sum reducer
is implemented with the tf.math.unsorted_segment_sum API,
and it can efficiently aggregate the unnormalized attention scores
from different numbers of neighbors for each node. The last map
operation is easier than the aforementioned operations, and it can
be implemented by a simple TensorFlow division function.

2.1.3 Graph Mini-batch Strategy. As with general deep learning
models, GNNs can benefit from mini-batch training and inference
of graphs. Given a batch of graphs, tf_geometric combine them into
a BatchGraph and apply GNNs on it. Since the label information 𝑦
is also combined in the BatchGraph, the combined 𝑦 can directly
be used as the node/graph labels of the batch.

The mini-batch strategy in tf_geometric is flexible, and you can
mini-batch not only the input graph data, but also the intermediate
output graphs. In the mini-batch process, since the operation of
combining graphs into a BatchGraph is differentiable, the gradients
will pass from the BatchGraph back to the given batch of graphs.
Moreover, the mini-batch construction operation is fast enough
and can be executed during each forward propagation process.

2.1.4 Distribution Strategy. To take advantage of the powerful dis-
tribution ability of TensorFlow, all the GNN models in tf_geometric
are implemented as standard TensorFlow models, which can be
distributed with minimal code changes on the data processing.
The distribution of tf_geometric GNN models can be easily han-
dled by TensorFlow distribution strategies, and the model can be
distributed in different ways by choosing different distribution
strategies. However, the distribution of graph data cannot be solved
by simply applying the distribution strategies. This is because the
built-in data sharding mechanism of the distribution strategies are
designed for regular tensors, which is not able to deal with irregular
graph data. Nonetheless, we can still easily distribute graph data
by customizing distributed graph datasets with tf.data.Dataset for
distribution strategies. The customizing usually only requires a few
small changes on the code for local data processing.

2.1.5 Core OOP APIs and Functional APIs. tf_geometric provides
both Object Oriented Programming (OOP) APIs and Functional
APIs, with which users can built advanced graph deep learning
models:

• OOP APIs are class-level interfaces for graph neural net-
works. The GNN classes in tf_geometric are implemented as
standard TensorFlow models by subclassing the tf.keras.Model
class, where each GNN class defines how to maintain the
model parameters and the computational process. An in-
stance of GNN classes holds the parameters of a GNN model
and it can be called as a function to process the input data
with the GNN algorithm. OOP APIs are convenient since
users can apply them on graph data as black boxes without

(a) Map-Reduce Workflow for Normalized Attention Scores in GAT

(b) Reduce by Key

Figure 5: Graph Map-Reduce Framework

and advanced map and reduce operations on graphs and graph
map-reduce workflows.

Fig. 5(a) shows an example of map-reduce workflow, which com-
putes the normalized attention scores for a Graph Attention Net-
work (GAT). In the example, each colored edge contains the feature
vectors of a node and one of its neighbor nodes. The first map
operation parallelly transforms a batch of edges into unnormalized
attention scores. Map operations do not involve the interaction
between elements, and most of them can be implemented with gen-
eral TensorFlow operations. Here, the first map operation is imple-
mented with a TensorFlow dense layer. GAT requires the attention
score to be normalized by softmax normalization. To achieve this,
a reduce operation is introduced to aggregate the unnormalized
attention scores of neighbors and obtain the denominator for the
softmax normalization. Details of the reducer are shown in Fig. 5(b).
The reducer aggregates information for each group and the reduce
key indicates which group each element belongs to. In this case,
the reduce key is node index and the unnormalized attention scores

v3v2v4v1Graphv1v1v1v2v1v4v2v2v2v1v2v4……v4v3EdgesRowColv1v1e2v1v2e3v1v4e2v2v2e2v2v1e1v2v4e4………v4V3e2ScoreRowColAttention Score(Unnormalized) Reduce Sum by Rowv1e2+e3+e2v2e2+e1+e4v3…v4…Score_SumRowSum of Scoresv1v1e2/(e2+e3+e2)v1v2e3/(e2+e3+e2)v1v4e2/(e2+e3+e2)v2v2e2/(e2+e1+e4)v2v1e1/(e2+e1+e4)v2v4e4/(e2+e1+e4)………v4v3…Attention Score(Normalized) Score/Score_SumRowColGatherMapv1v1e2e2+e3+e2v1v2e3e2+e3+e2v1v4e2e2+e3+e2v2v2e2e2+e1+e4v2v1e1e2+e1+e4v2v4e4e2+e1+e4…………v4v3e2…Attention Score(Zipped) ScoreRowColScore_SumMapGatherZipv1v1e2v1v2e3v1v4e2v2v2e2v2v1e1v2v4e4………v4V3e2ScoreRowColAttention Score(Unnormalized) v1e2+e3+e2v2e2+e1+e4v3…v4…Score_SumRowSum of Scoresv1v1v1v2v2v2…v4RowReduce KeyEfficient Graph Deep Learning in TensorFlow with tf_geometric

knowing details of the GNN model, such as the initializa-
tion of model parameters and the algorithm. Due to the
convenience and customizability of OOP APIs, most popu-
lar GNN libraries provide OOP APIs as the main interface
for GNNs. However, OOP APIs are insufficient for some ad-
vanced tasks, and therefore tf_geometric provides functional
APIs to solve the problem, which will be introduced in the
next paragraphs.

• Functional APIs provide function-level interfaces for graph
neural networks. Functional APIs are functions that imple-
ment GNN operations. Different from OOP APIs, which auto-
matically maintain model parameters in GNN layer instances,
functional APIs require users to maintain model parameters
outside the GNN functions and use them as the input of GNN
functions together with graph data. That is, instead of using
fixed tensors as model parameters in OOP APIs, functional
APIs can dynamically use different tensors as model param-
eters for each call. This feature of functional APIs is critical
for advanced tasks that require complex maintenance strate-
gies of model parameters, such as graph meta-learning. For
example, to implement MAML [8] on graphs, a GNN func-
tion will be called multiple times with different parameters
during each forward propagation. The GNN function is first
called with variable tensors as initial parameters and then
called multiple times with temporary tensors as updated
model parameters. Obviously, functional APIs are elegant
solutions for this task, since dynamic parameters are natively
supported by functional APIs.

Note that the core OOP APIs and functional APIs in the kernel
do not involve the implementation of specific GNNs. Instead, they
provide some infrastructures that are essential for implementing
specific GNN classes or functions, such as abstract classes and
functions for graph map-reduce.
2.2 Implementation of Popular GNN Models
Based on the kernel libraries, tf_geometric implements a vari-
ety of popular GNN models for different tasks, including node-
level models such as Graph Convolutional Network (GCN) [13],
Graph Attention Network (GAT) [25], Simple Graph Convolution
(SGC) [32], Approximate Personalized Propagation of Neural Predic-
tions (APPNP) [14], and Deep Graph Infomax (DGI) [26], and graph-
level models such as Set2Set [27], SortPool [35], Differentiable
Pooling (DiffPool) [34], and Self-Attention Graph Pooling (SAG-
Pool) [15]. To avoid redundancy, all the GNN models in tf_geometric
are first implemented as Functional APIs, and the OOP APIs are just
wrappers of the corresponding Functional APIs. We carefully imple-
ment these models and make sure that they can achieve competitive
performance with other implementations.

Besides, tf_geometric also provides demos that reproduce the
performance of GNNs reported in the literature. The demos contain
the complete code for data loading, training, and evaluation. They
are implemented in an elegant way and also act as the style guide
for tf_geometric.

2.3 Dataset Management Mechanism
tf_geometric provides customizable dataset APIs and a lot of ready-
to-use public benchmark datasets.

2.3.1 Dataset Classes and Dataset Instances. Each dataset has a
corresponding dataset class and different instances of a dataset class
(dataset instances) can represent different configurations for the
same dataset. Each dataset instance can automatically download the
raw dataset from the Web and then pre-process it into convenient
data formats, which can benefit not only tf_geometric, but also other
graph deep learning frameworks. Besides, a caching mechanism is
provided by dataset classes, which allow you to only process each
raw dataset once and load it from the cache on-the-fly.

Dataset classes are not just simple wrappers of the raw graph
datasets, and they may also involve complex feature engineering
in the pre-processing. For example, node degrees are frequently
used features in graph classification tasks [33]. By encapsulating
the computation of node degrees in the pre-processing method of
dataset classes, users can directly load node degrees as features from
the datasets without considering the complex feature engineering
process.

2.3.2 Built-in Datasets. The provided datasets, which are also
called built-in datasets, consist of lots of public benchmark datasets
that are frequently used in graph deep learning research. Moreover,
the built-in datasets cover datasets for various graph deep learning
tasks, such as node classification and graph classification.

2.3.3 Customizable Datasets. Users can customize their datasets
by simply subclassing built-in abstract dataset classes. The built-in
abstract dataset classes manage the workflow of dataset processing
and already encapsulate the implementation of general processes,
such as downloading, file management, and caching. These gen-
eral processes can be customized by the configuration parameters
defined in subclasses, such as the URL of the dataset and whether
the pre-processing result should be cached. Since the data pre-
processing processes are usually different across different datasets,
the data pre-processing is defined as abstract methods in the su-
perclasses and users can implement them for their datasets in the
subclasses by overriding the abstract methods.

2.4 Utilities
Some important utilities are required for implementing graph deep
learning models. These utilities include tools for common graph
data processing, type conversion, graph sampling, etc. The tools
are put in the utils module of tf_geometric, and most of them are
designed not only for tf_geometric, but also for general graph deep
learning implementations.

3 COMPARISON TO OTHER GNN LIBRARIES
In recent years, several GNN libraries have been developed for dif-
ferent deep learning frameworks. Among them, popular libraries
such as PyTorch Geometric (PyG)4 [7] and Deep Graph Library
(DGL)5 [29] have been widely used by researchers to deal with
graph deep learning in different fields. They provide extensible
OOP APIs and implement a variety of GNN classes, with which
users can easily handle general graph deep learning tasks. As men-
tioned before, OOP APIs are insufficient for several advanced tasks
such as graph meta-learning. Different from these GNN libraries,

4https://github.com/rusty1s/pytorch_geometric
5https://github.com/dmlc/dgl

Jun Hu, Shengsheng Qian, Quan Fang, Youze Wang, Quan Zhao, Huaiwen Zhang, Changsheng Xu

tf_geometric provide not only OOP APIs, but also Functional APIs,
which can be used to deal with advanced graph deep learning tasks.
Moreover, popular GNN libraries for TensorFlow such as Spek-
tral 6 [11] and StellarGraph 7 usually only support TensorFlow
2.x, whereas tf_geometric is compatible with both TensorFlow 1.x
and 2.x. Furthermore, tf_geometric provides a flexible and friendly
caching system that can speed up some GNNs, while existing GNN
libraries do not support caching or only support caching for few
special cases. For example, PyG only supports layer-level caching
for GCN, which means that each PyG GCN layer with caching en-
abled is bound to a constant graph structure and usually it can only
be used for transductive learning tasks on a single graph. Instead,
the tf_geometric GCN adopts a graph-level caching mechanism,
and it can cache for different graph structures with different GCN
normalization configurations.

4 EMPIRICAL EVALUATION
To provide an overview of how GNN models implemented by
tf_geometric perform on common research scenarios, we conduct
experiments with several public benchmark datasets on two differ-
ent tasks.

4.1 Tasks and Evaluation Metrics
We evaluate several tf_geometric models on two different tasks:
node classification and graph classification.

Node Classification We first conduct experiments on a semi-
supervised node classification task with three benchmark datasets:
Cora, CiteSeer, and Pubmed [23]. We evaluate GCN [13], GAT [25],
SGC [32], APPNP [14], and DGI [26] on the task, where GCN, GAT,
SGC, APPNP are end-to-end classification models, while DGI is a
self-supervised model node representation learning model, where
an extra logistic regression model is utilized for classification based
on the learned node representations. For the benchmark datasets,
we use the same dataset splits as in [13], where each dataset is split
into a train set, a test set, and a validation set. The validation set
is used for early stopping and its label information is not used for
training. We report the classification accuracy scores on the test
set.

Graph Classification We also evaluate tf_geometric on a graph
classification task with three benchmark datasets: NCI1, NCI109 [28],
and PROTEINS [2, 4]. We evaluate several graph pooling models,
including Mean-Max Pool, Set2Set [27], SortPool [35], DiffPool [34],
and SAGPoolℎ [15]. The Mean-Max Pool is a naive graph pooling
model, which obtains graph representations by concatenating the
mean pooling and max pooling results of GCNs. These classification
accuracy scores of these models are evaluated on three benchmark
datasets using 10-fold cross-validation, where a training fold is ran-
domly sampled as the validation set. As with the node classification
task, the validation set is only used for early stopping. The architec-
tures of graph pooling models are complex, and they may involve
components other than the core graph pooling layers. Some of
these components are model-agnostic and can be utilized by some
other GNN models to obtain better performance. For example, the
hierarchical graph pooling models may benefit from the mean-max

6https://github.com/danielegrattarola/spektral
7https://github.com/stellargraph/stellargraph

Table 1: Performance on Node Classification Tasks.

Dataset

Model
GCN
GAT
SGC
APPNP
DGI

Cora

CiteSeer

Pubmed

81.7 ± 0.5
83.0 ± 0.8
81.1 ± 0.0
83.6 ± 0.7
82.4 ± 0.5

71.0 ± 0.8
72.9 ± 0.8
72.1 ± 0.0
72.2 ± 0.9
72.2 ± 0.5

79.1 ± 0.6
79.0 ± 0.2
79.2 ± 0.1
79.0 ± 0.7
78.0 ± 0.6

Table 2: Performance on Graph Classification Tasks.

Dataset

Model
Mean-Max Pool
Set2Set
SortPool
DiffPool
SAGPoolℎ

NCI1

NCI109

PROTEINS

76.03 ± 0.7
72.01 ± 0.6
74.65 ± 0.6
75.27 ± 0.8
69.41 ± 1.1

75.62 ± 0.7
69.54 ± 0.6
73.22 ± 0.7
73.73 ± 0.6
69.12 ± 0.5

74.18 ± 0.6
71.69 ± 0.6
68.09 ± 2.6
74.23 ± 1.0
73.33 ± 0.8

pooling on both hidden and output layers, whereas the official im-
plements may only consider using mean pooling. Therefore, we
update the architectures for some models for a fair comparison.

4.2 Performance
The model performance on node classification is reported in Table 1.
The results show that the GNN models provided by tf_geometric can
achieve competitive performance with the official implementations.
Particularly, although tf_geometric adopts a Transformer-based
GAT algorithm rather than the official version, it still obtains almost
the same accuracy scores reported in [25].

For the graph classification task, the results are listed in Table 2.
Since the architectures of some models are adjusted, the model
performance is sometimes better than that reported in the literature.
Note that by optimizing the architecture, the naive Mean-Max Pool
outperforms some other graph pooling models in some cases.

5 CONCLUSIONS
We introduce tf_geometric, an efficient and friendly library for
graph deep learning, which is compatible with both TensorFlow 1.x
and 2.x. tf_geometric provides kernel libraries for building graph
neural networks as well as implementations of popular GNNs. In
particular, the kernel libraries consist of infrastructures for build-
ing efficient GNNs, which enable tf_geometric to support single-
graph computation, multi-graph computation, graph mini-batch,
distributed training, etc. Therefore, tf_geometric can be used for a
variety of graph deep learning tasks, such as transductive node clas-
sification, inductive node classification, link prediction, and graph
classification. tf_geometric exposes both OOP APIs and Functional
APIs, with which users can deal with advanced graph deep learning
tasks. Moreover, the APIs are friendly, and they are suitable for both
beginners and experts. We are actively working to further optimize
the kernel libraries and integrate more existing GNN models for
tf_geometric. In the future, we will keep tf_geometric up-to-date
with the latest research findings of GNNs and continually integrate
future models into it.

Efficient Graph Deep Learning in TensorFlow with tf_geometric

REFERENCES
[1] Jasmijn Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil
Sima’an. 2017. Graph Convolutional Encoders for Syntax-aware Neural Ma-
chine Translation. In Proceedings of the 2017 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September
9-11, 2017. Association for Computational Linguistics, 1957–1967.

[2] K. Borgwardt, Cheng Soon Ong, S. Schönauer, S. Vishwanathan, Alex Smola, and
H. Kriegel. 2005. Protein function prediction via graph kernels. Bioinformatics
21 Suppl 1 (2005), i47–56.

[3] Zhiyong Cui, Kristian Henrickson, Ruimin Ke, and Yinhai Wang. 2020. Traffic
Graph Convolutional Recurrent Neural Network: A Deep Learning Framework
for Network-Scale Traffic Learning and Forecasting. IEEE Trans. Intell. Transp.
Syst. 21, 11 (2020), 4883–4894.

[4] P. Dobson and A. Doig. 2003. Distinguishing enzyme structures from non-
enzymes without alignments. Journal of molecular biology 330 4 (2003), 771–83.
[5] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Yihong Eric Zhao, Jiliang Tang, and Dawei
Yin. 2019. Graph Neural Networks for Social Recommendation. In The World
Wide Web Conference, WWW 2019, San Francisco, CA, USA, May 13-17, 2019. ACM,
417–426.

[6] Yuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuohang Wang, and Jingjing Liu.
2020. Hierarchical Graph Network for Multi-hop Question Answering. In Proceed-
ings of the 2020 Conference on Empirical Methods in Natural Language Processing,
EMNLP 2020, Online, November 16-20, 2020. Association for Computational Lin-
guistics, 8823–8838.

[7] Matthias Fey and Jan Eric Lenssen. 2019. Fast Graph Representation Learning with
PyTorch Geometric. CoRR abs/1903.02428 (2019). http://arxiv.org/abs/1903.02428
[8] Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-Agnostic Meta-
Learning for Fast Adaptation of Deep Networks. In Proceedings of the 34th In-
ternational Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia,
6-11 August 2017 (Proceedings of Machine Learning Research, Vol. 70). PMLR,
1126–1135.

[9] Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. 2017. Protein Interface
Prediction using Graph Convolutional Networks. In Advances in Neural Informa-
tion Processing Systems 30: Annual Conference on Neural Information Processing
Systems 2017, December 4-9, 2017, Long Beach, CA, USA. 6530–6539.

[10] Thomas Gaudelet, Ben Day, Arian R. Jamasb, Jyothish Soman, Cristian Regep,
Gertrude Liu, Jeremy B. R. Hayter, Richard Vickers, Charles Roberts, Jian Tang,
David Roblin, Tom L. Blundell, Michael M. Bronstein, and Jake P. Taylor-King.
2020. Utilising Graph Machine Learning within Drug Discovery and Development.
CoRR abs/2012.05716 (2020). https://arxiv.org/abs/2012.05716

[11] Daniele Grattarola and Cesare Alippi. 2021. Graph Neural Networks in Tensor-
Flow and Keras with Spektral [Application Notes]. IEEE Comput. Intell. Mag. 16,
1 (2021), 99–106.

[12] Jun Hu, Shengsheng Qian, Quan Fang, and Changsheng Xu. 2019. Hierarchical
Graph Semantic Pooling Network for Multi-modal Community Question Answer
Matching. In Proceedings of the 27th ACM International Conference on Multimedia,
MM 2019, Nice, France, October 21-25, 2019. ACM, 1157–1165.

[13] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
Proceedings. OpenReview.net.

[14] Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. 2019. Pre-
dict then Propagate: Graph Neural Networks meet Personalized PageRank. In
7th International Conference on Learning Representations, ICLR 2019, New Orleans,
LA, USA, May 6-9, 2019. OpenReview.net.

[15] Junhyun Lee, Inyeop Lee, and Jaewoo Kang. 2019. Self-Attention Graph Pooling.
In Proceedings of the 36th International Conference on Machine Learning, ICML
2019, 9-15 June 2019, Long Beach, California, USA (Proceedings of Machine Learning
Research, Vol. 97). PMLR, 3734–3743.

[16] Linjie Li, Zhe Gan, Yu Cheng, and Jingjing Liu. 2019. Relation-Aware Graph
Attention Network for Visual Question Answering. In 2019 IEEE/CVF International
Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 -
November 2, 2019. IEEE, 10312–10321.

[17] Yang Li, Buyue Qian, Xianli Zhang, and Hui Liu. 2020. Graph Neural Network-

Based Diagnosis Prediction. Big Data 8, 5 (2020), 379–390.

[18] Diego Marcheggiani, Jasmijn Bastings, and Ivan Titov. 2018. Exploiting Semantics
in Neural Machine Translation with Graph Convolutional Networks. In Proceed-
ings of the 2018 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, NAACL-HLT, New Or-
leans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short Papers). Association for
Computational Linguistics, 486–492.

[19] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter Battaglia. 2021.
Learning Mesh-Based Simulation with Graph Networks. In International Confer-
ence on Learning Representations. https://openreview.net/forum?id=roNqYL0_XP
[20] Ekagra Ranjan, Soumya Sanyal, and Partha P. Talukdar. 2020. ASAP: Adaptive
Structure Aware Pooling for Learning Hierarchical Graph Representations. In The

Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-
Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The
Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI
2020, New York, NY, USA, February 7-12, 2020. AAAI Press, 5470–5477.

[21] Vishnu Ram Sampathkumar. 2021. ADiag: Graph Neural Network Based Diagno-
sis of Alzheimer’s Disease. CoRR abs/2101.02870 (2021). https://arxiv.org/abs/
2101.02870

[22] Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure
Leskovec, and Peter W. Battaglia. 2020. Learning to Simulate Complex Physics
with Graph Networks. In Proceedings of the 37th International Conference on Ma-
chine Learning, ICML 2020, 13-18 July 2020, Virtual Event (Proceedings of Machine
Learning Research, Vol. 119). PMLR, 8459–8468.

[23] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and
Tina Eliassi-Rad. 2008. Collective Classification in Network Data. AI Mag. 29, 3
(2008), 93–106.

[24] Qiaoyu Tan, Ninghao Liu, Xing Zhao, Hongxia Yang, Jingren Zhou, and Xia Hu.
2020. Learning to Hash with Graph Neural Networks for Recommender Systems.
In WWW ’20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020. ACM /
IW3C2, 1988–1998.

[25] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Liò, and Yoshua Bengio. 2018. Graph Attention Networks. In 6th International
Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April
30 - May 3, 2018, Conference Track Proceedings. OpenReview.net.

[26] Petar Velickovic, William Fedus, William L. Hamilton, Pietro Liò, Yoshua Bengio,
and R. Devon Hjelm. 2019. Deep Graph Infomax. In 7th International Conference
on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.
OpenReview.net.

[27] Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. 2016. Order Matters: Se-
quence to sequence for sets. In 4th International Conference on Learning Rep-
resentations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track
Proceedings. http://arxiv.org/abs/1511.06391

[28] Nikil Wale, Ian A. Watson, and George Karypis. 2008. Comparison of descriptor
spaces for chemical compound retrieval and classification. Knowl. Inf. Syst. 14, 3
(2008), 347–375.

[29] Minjie Wang, Lingfan Yu, Da Zheng, Quan Gan, Yu Gai, Zihao Ye, Mufei Li,
Jinjing Zhou, Qi Huang, Chao Ma, Ziyue Huang, Qipeng Guo, Hao Zhang, Haibin
Lin, Junbo Zhao, Jinyang Li, Alexander J. Smola, and Zheng Zhang. 2019. Deep
Graph Library: Towards Efficient and Scalable Deep Learning on Graphs. CoRR
abs/1909.01315 (2019). http://arxiv.org/abs/1909.01315

[30] Wen Wang, Wei Zhang, Shukai Liu, Qi Liu, Bo Zhang, Leyu Lin, and Hongyuan
Zha. 2020. Beyond Clicks: Modeling Multi-Relational Item Graph for Session-
Based Target Behavior Prediction. In WWW ’20: The Web Conference 2020, Taipei,
Taiwan, April 20-24, 2020. ACM / IW3C2, 3056–3062.

[31] Xiaoyang Wang, Yao Ma, Yiqi Wang, Wei Jin, Xin Wang, Jiliang Tang, Caiyan Jia,
and Jian Yu. 2020. Traffic Flow Prediction via Spatial Temporal Graph Neural
Network. In WWW ’20: The Web Conference 2020, Taipei, Taiwan, April 20-24,
2020. ACM / IW3C2, 1082–1092.

[32] Felix Wu, Amauri H. Souza Jr., Tianyi Zhang, Christopher Fifty, Tao Yu, and
Kilian Q. Weinberger. 2019. Simplifying Graph Convolutional Networks. In
Proceedings of the 36th International Conference on Machine Learning, ICML 2019,
9-15 June 2019, Long Beach, California, USA (Proceedings of Machine Learning
Research, Vol. 97). PMLR, 6861–6871.

[33] Jun Wu, Jingrui He, and Jiejun Xu. 2019. DEMO-Net: Degree-specific Graph
Neural Networks for Node and Graph Classification. In Proceedings of the 25th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,
KDD 2019, Anchorage, AK, USA, August 4-8, 2019. ACM, 406–415.

[34] Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L. Hamilton,
and Jure Leskovec. 2018. Hierarchical Graph Representation Learning with
Differentiable Pooling. In Advances in Neural Information Processing Systems 31:
Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
December 3-8, 2018, Montréal, Canada. 4805–4815.

[35] Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. 2018. An
End-to-End Deep Learning Architecture for Graph Classification. In Proceedings
of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the
30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI
Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New
Orleans, Louisiana, USA, February 2-7, 2018. AAAI Press, 4438–4445.

