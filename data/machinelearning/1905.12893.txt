9
1
0
2

y
a
M
1
3

]

G
L
.
s
c
[

2
v
3
9
8
2
1
.
5
0
9
1
:
v
i
X
r
a

A General Optimization Framework for Dynamic Time
Warping

Dave Deriso

Stephen Boyd

June 3, 2019

Abstract

The goal of dynamic time warping is to transform or warp time in order to ap-
proximately align two signals together. We pose the choice of warping function as an
optimization problem with several terms in the objective. The ﬁrst term measures the
misalignment of the time-warped signals. Two additional regularization terms penalize
the cumulative warping and the instantaneous rate of time warping; constraints on the
warping can be imposed by assigning the value +
to the regularization terms. Diﬀer-
ent choices of the three objective terms yield diﬀerent time warping functions that trade
oﬀ signal ﬁt or alignment and properties of the warping function. The optimization
problem we formulate is a classical optimal control problem, with initial and termi-
nal constraints, and a state dimension of one. We describe an eﬀective general method
that minimizes the objective by discretizing the values of the original and warped time,
and using standard dynamic programming to compute the (globally) optimal warping
function with the discretized values. Iterated reﬁnement of this scheme yields a high
accuracy warping function in just a few iterations. Our method is implemented as an
open source Python package GDTW.

∞

1 Background

The goal of dynamic time warping (DTW) is to ﬁnd a time warping function that transforms,
or warps, time in order to approximately align two signals together [1]. At the same time,
we prefer that the time warping be as gentle as possible, in some sense, or we require that
it satisfy some requirements.

DTW is a versatile tool used in many scientiﬁc ﬁelds, including biology, economics, signal
processing, ﬁnance, and robotics. It can be used to measure a realistic distance between two
signals, usually by taking the distance between them after one is time-warped. In another
case, the distance can be the minimum amount of warping needed to align one signal to
the other with some level of ﬁdelity. Time warping can be used to develop a simple model
of a signal, or to improve a predictor; as a simple example, a suitable time warping can
lead to a signal being well ﬁt by an auto-regressive or other model. It can be employed in

1

 
 
 
 
 
 
any machine-learning application that relies on signals, such as PCA, clustering, regression,
logistic regression, or multi-class classiﬁcation. (We return to this topic in

7.)

Almost all DTW methods are based on the original DTW algorithm [1], which uses
dynamic programming to compute a time warping path that minimizes misalignments in
the time-warped signals while satisfying monotonicity, boundary, and continuity constraints.
The monotonicity constraint ensures that the path represents a monotone increasing function
of time. The boundary constraint enforces that the warping path beings with the origin
point of both signals and ends with their terminal points. The continuity constraint restricts
transitions in the path to adjacent points in time.

§

Despite its popularity, DTW has a longstanding problem with producing sharp irregular-
ities in the time warp function that cause many time points of one signal to be erroneously
mapped onto a single point, or “singularity,” in the other signal. Most of the literature on
reducing the occurrence of singularities falls into two camps: preprocessing the input signals,
and variations on continuity constraints. Preprocessing techniques rely on transformations
of the input signals, which make them smoother or emphasize features or landmarks, to
indirectly inﬂuence the smoothness of the warping function. Notable approaches use combi-
nations of ﬁrst and second derivatives [2, 3, 4], square-root velocity functions [5], adaptive
down-sampling [6], and ensembles of features including wavelet transforms, derivatives, and
several others [7]. Variations of the continuity constraints relax the restriction on transitions
in the path, which allows smoother warping paths to be chosen. Instead of only restricting
transitions to one of three neighboring points in time, as in the original DTW algorithm,
these variations expand the set of allowable points to those speciﬁed by a “step pattern,”
of which there are many, including symmetric or asymmetric, types I-IV, and sub-types
a-d [1, 8, 9, 10]. While preprocessing and step patterns may result in smoother warping
functions, they are ad-hoc techniques that often require hand-selection for diﬀerent types of
input signals.

We propose to handle these issues entirely within an optimization framework in contin-
uous time. Here we pose DTW as an optimization problem with several penalty terms in
the objective. The basic term in our objective penalizes misalignments in the time-warped
signals, while two additional terms penalize (and constrain) the time warping function. One
of these terms penalizes the cumulative warping, which limits over-ﬁtting similar to “ridge”
or “lasso” regularization [11, 12]. The other term penalizes the instantaneous rate of time
warping, which produces smoother warping functions, an idea that previously proposed in
[13, 14, 15, 16].

Our formulation oﬀers almost complete freedom in choosing the functions used to compare
the sequences, and to penalize the warping function. We include constraints on the ﬁt and
warping functions by allowing these functions to take on the value +
. Traditional penalty
functions include the square or absolute value. Less traditional but useful ones include
for example the fraction of time the two signals are within some threshold distance, or a
minimum or maximum on the cumulative warping function. The choice of these functions,
and how much they are scaled with respect to each other, gives a very wide range of choices
for potential time warpings.

∞

2

Our continuous time formulation allows for non-uniformly sampled signals, which allows
us to use simple out-of-sample validation techniques to help guide the choice of time warping
penalties; in particular, we can determine whether a time warp is ‘over-ﬁt’. Our handling of
missing data in the input signals is useful in itself since real-world data often have missing
entries. To the best of our knowledge, we are the ﬁrst use of out-of-sample validation for
performing model selection in DTW.

We develop a single, eﬃcient algorithm that solves our formulation, independent of the
particular choices of the penalty functions. Our algorithm uses dynamic programming to
exactly solve a discretized version of the problem with linear time complexity, coupled with
iterative reﬁnement at higher and higher resolutions. Our discretized formulation can be
thought of as generalizing the Itakura parallelogram [8]; the iterated reﬁnement scheme is
similar in nature to FastDTW [17]. We oﬀer our implementation as open source C++ code
with an intuitive Python package called GDTW that runs 50x faster than other methods on
standard problem sizes.

We describe several extensions and variations of our method. In one extension, we extend
our optimization framework to ﬁnd a time-warped center of or template for a set of signals;
in a further extension, we cluster a set of signals into groups, each of which is time-warped
into one of a set of templates or prototypes.

2 Dynamic time warping

Rd, with argument time. A
Signals. A (vector-valued) signal f is a function f : [a, b]
signal can be speciﬁed or described in many ways, for example a formula, or via a sequence
of samples along with a method for interpolating the signal values in between samples.
Rd, at points (times)
For example we can describe a signal as taking values s1, . . . , sN ∈
a
b, with linear interpolation in between these values and a constant
< tN ≤
≤
extension outside the ﬁrst and last values:



t1 < t2 <

· · ·

→

f (t) =

s1
ti+1
ti+1
sN



t
−
ti
−

si + t
−
ti+1

ti

−

ti

si+1

t < t1
t < ti+1,

a
≤
ti ≤
tN < t

b,

≤

i = 1, . . . , N

1,

−

For simplicity, we will consider signals on the time interval [0, 1].

Time warp function. Suppose φ : [0, 1]
[0, 1] is increasing, with φ(0) = 0 and φ(1) = 1.
We refer to φ as the time warp function, and τ = φ(t) as the warped time associated with
real or original time t. When φ(t) = t for all t, the warped time is the same as the original
time. In general we can think of

→

−
as the amount of cumulative warping at time t, and

−

τ

t = φ(t)

t

dτ
dt

(τ

−

t) = φ(cid:48)(t)

1

−

3

as the instantaneous rate of time warping at time t. These are both zero when φ(t) = t for
all t.

Time-warped signal.

If x is a signal, we refer to the signal ˜x = x

φ, i.e.,

◦

˜x(t) = x(τ ) = x(φ(t)),

as the time-warped signal, or the time-warped version of the signal x.

Dynamic time warping. Suppose we are given two signals x and y. Roughly speaking,
y.
the dynamic time warping problem is to ﬁnd a warping function φ so that ˜x = x
In other words, we wish to warp time so that the time-warped version of the ﬁrst signal is
close to the second one. We refer to the signal y as the target, since the goal is warp x to
match, or align with, the target.

≈

φ

◦

(a) Top. x and y. Middle. φ. Bottom. ˜x and y. (b) Top. φ(t). Middle. φ(t)

t. Bottom. φ(cid:48)(t)

1.

−

−

Example. An example is shown in ﬁgure 1a. The top plot shows a scalar signal x and
target signal y, and the bottom plot shows the time-warped signal ˜x = x
φ and y. The
middle plot shows the correspondence between x and y associated with the warping function
φ. Figure 1b shows the time warping function; the next plot is the cumulative warp, and
the next is the instantaneous rate of time warping.

◦

3 Optimization formulation

We will formulate the dynamic time warping problem as an optimization problem, where
the time warp function φ is the (inﬁnite-dimensional) optimization variable to be chosen.
Our formulation is very similar to those used in machine learning, where a ﬁtting function

4

0.00.20.40.60.81.0t−101y(t)andx(t)y(t)x(t)0.00.20.40.60.81.0t−101y(t)and(x◦φ)(t)y(t)(x◦φ)(t)0.00.20.40.60.81.0t−101φ(t)Mappingx(t)toy(t)y(t)x(t)φ(t)0.00.20.40.60.81.0t0.00.51.0τφ(t)0.00.20.40.60.81.0t−0.10.00.1φ(t)−t0.00.20.40.60.81.0t−0.50.00.5φ0(t)−1is chosen to minimize an objective that includes a loss function that measures the error in
ﬁtting the given data, and regularization terms that penalize the complexity of the ﬁtting
function [18].

Loss functional. Let L : Rd
→
associated with a time warp function φ, on the two signals x and y, as

R be a vector penalty function. We deﬁne the loss

(φ) =

L

(cid:90) 1

0

L(x(φ(t))

y(t))dt,

−

(1)

the average value of the penalty function of the diﬀerence between the time-warped ﬁrst
φ to
signal and the second signal. The smaller
approximate y.

(φ) is, the better we consider ˜x = x

L

◦

Simple choices of the penalty include L(u) =

u
(cid:107)1. The corresponding
losses are the mean-square deviation and mean-absolute deviation, respectively. One useful
variation is the Huber penalty [19, 20],

2
2 or L(u) =

u
(cid:107)

(cid:107)

(cid:107)

L(u) =

(cid:26)

2
2

u
(cid:107)
(cid:107)
2M
(cid:107)

u
M
(cid:107)2 ≤
(cid:107)2 > M,
u

(cid:107)
(cid:107)

u
(cid:107)2 −
where M > 0 is a parameter. The Huber penalty coincides with the least squares penalty
for small u, but grows more slowly for u large, and so is less sensitive to outliers. Many other
choices are possible, for example

M 2

L(u) =

(cid:26) 0

u
(cid:107)

(cid:107) ≤
1 otherwise,

(cid:15)

where (cid:15) is a positive parameter. The associated loss
warped signal is farther than (cid:15) from the second signal (measured by the norm

(φ) is the fraction of time the time-

The choice of penalty function L (and therefore loss functional

(cid:107) · (cid:107)
) will inﬂuence the
warping found, and should be chosen to capture the notion of approximation appropriate
for the given application.

L

L

).

Cumulative warp regularization functional. We express our desired qualities for or
requirements on the time warp function using a regularization functional for the cumulative
warp,

cum(φ) =

R

Rcum(φ(t)

t)dt,

−

(2)

(cid:90) 1

0

R

where Rcum : R
is a penalty function on the cumulative warp. The function
∪ {∞}
Rcum can take on the value +
, which allows us to encode constraints on φ. While we do
not require it, we typically have Rcum(0) = 0, i.e., there is no cumulative regularization cost
when the warped time and true time are the same.

∞

→

5

Instantaneous warp regularization functional. The regularization functional for the
instantaneous warp is

inst(φ) =

R

Rinst(φ(cid:48)(t)

1)dt,

−

(3)

(cid:90) 1

0

R

→

∪ {∞}

where Rinst : R
is the penalty function on the instantaneous rate of time
warping. Like the function Rcum, Rinst can take on the value +
, which allows us to encode
for u < smin, for example, we require that
constraints on φ(cid:48). By assigning Rinst(u) = +
smin for all t. We will assume that this is the case for some positive smin, which
φ(cid:48)(t)
ensures that φ is invertible. While we do not require it, we typically have Rinst(0) = 0, i.e.,
there is no instantaneous regularization cost when the instantaneous rate of time warping is
one.

∞

∞

≥

As a simple example, we might choose

Rcum(u) = u2,

Rinst(u) =

(cid:26) u2 smin

u

smax

≤
≤
otherwise,

∞

i.e., a quadratic penalty on cumulative warping, and a square penalty on instantaneous
warping, plus the constraint that the slope of φ must be between smin and smax. A very
wide variety of penalties can be used to express our wishes and requirements on the warping
function.

Dynamic time warping via regularized loss minimization. We propose to choose φ
by solving the optimization problem

f (φ) =
minimize
R
subject to φ(0) = 0, φ(1) = 1,

(φ) + λcum

L

cum(φ) + λinst

inst(φ)

R

(4)

where λcum and λinst are positive hyper-parameters used to vary the relative weight of the
three terms. The variable in this optimization problem is the time warp function φ.

Optimal control formulation. The problem (4) is an inﬁnite-dimensional, and generally
non-convex, optimization problem. Such problems are generally impractical to solve exactly,
but we will see that this particular problem can be eﬃciently and practically solved.

It can be formulated as a classical continuous-time optimal control problem [21], with

scalar state φ(t) and action or input u(t) = φ(cid:48)(t):

minimize
subject to φ(0) = 0, φ(1) = 1, φ(cid:48)(t) = u(t),

(cid:0)(cid:96)(φ(t), u(t), t) + λinstRinst(u(t))(cid:1) dt

(cid:82) 1
0

0

t

≤

≤

1,

(5)

where (cid:96) is the state-action cost function

(cid:96)(u, v, t) = L(x(u)

−

y(t)) + λcumRcum(u).

There are many classical methods for numerically solving the optimal control problem (5),
but these generally make strong assumptions about the loss and regularization functionals

6

(such as smoothness), and do not solve the problem globally. We will instead solve (5) by
brute force dynamic programming, which is practical since the state has dimension one, and
so can be discretized.

Lasso and ridge regularization. Before describing how we solve the optimal control
problem (5), we mention two types of regularization that are widely used in machine learning,
and what types of warping functions typically result when using them. They correspond to
Rcum and Rinst being either u2 (quadratic, ridge, or Tikhonov regularization [11, 22]) or
u
|
|
(absolute value, (cid:96)1 regularization, or Lasso [23, p564] [12])

With Rcum(u) = u2, the regularization discourages large deviations between τ and t,
but the not the rate at which τ changes with t. With Rinst(u) = u2, the regularization
discourages large instantaneous warping rates. The larger λcum is, the less τ deviates from
t; the larger λinst is, the more smooth the time warping function φ is.

Using absolute value regularization is more interesting.

It is well known in machine
learning that using absolute value or (cid:96)1 regularization leads to solutions with an argument
of the absolute value that is sparse, that is, often zero [20]. When Rcum is the absolute
value, we can expect many times when τ = t, that is, the warped time and true time are
the same. When Rinst is the absolute value, we can expect many times when φ(cid:48)(t) = 1, that
is, the instantaneous rate of time warping is zero. Typically these regions grow larger as we
increase the hyper-parameters λcum and λinst.

Discretized time formulation. To solve the problem (4) we discretize time with the N
values

0 = t1 < t2 <

< tN = 1.

· · ·

We will assume that φ is piecewise linear with knot points at t1, . . . , tN ; to describe it
we only need to specify the warp values τi = φ(ti) for i = 1, . . . , N , which we express
RN . We assume that the points ti are closely enough spaced that the
as a vector τ
restriction to piecewise linear form is acceptable. The values ti could be taken as the values
at which the signal y is sampled (if it is given by samples), or just the default linear spacing,
ti = (i
1). The constraints φ(0) = 0 and φ(1) = 1 are expressed as τ1 = 0 and
τN = 1.

1)/(N

−

−

∈

Using a simple Riemann approximation of the integrals and the approximation

φ(ti+1)

−
ti+1 −
we obtain the discretized objective

φ(cid:48)(ti) =

φ(ti)
ti

=

τi+1 −
ti+1 −

τi
ti

,

i = 1, . . . , N

1,

−

ˆf (τ ) =

N
1
(cid:88)
−

i=1

(cid:18)

(ti+1 −

ti)

L(x(τi)

y(ti)) + λcumRcum(τi −

−

ti) + λinstRinst

(cid:19)(cid:19)

(cid:18) τi+1 −
ti+1 −

τi
ti

. (6)

RN that minimizes ˆf (τ ), subject to
The discretized problem is to choose the vector τ
τ1 = 0, τN = 1. We call this vector τ (cid:63), with which we can construct an approximation

∈

7

to function φ using piecewise-linear interpolation. The only approximation here is the dis-
cretization; we can use standard techniques based on bounds on derivatives of the functions
involved to bound the deviation between the continuous-time objective f (φ) and its dis-
cretized approximation ˆf (τ ).

4 Dynamic programming with reﬁnement

In this section we describe a simple method to minimize ˆf (τ ) subject to τ1 = 0 and τN = 1,
i.e., to solve the optimal control problem (5) to obtain τ ∗. We ﬁrst discretize the possible
values of τi, whereupon the problem can be expressed as a shortest path problem on a graph,
and then eﬃciently and globally solved using standard dynamic programming techniques.
To reduce the error associated with the discretization of the values of τi, we choose a new
discretization with the same number of values, but in a reduced range (and therefore, more
ﬁnely spaced values) around the previously found values. This reﬁnement converges in a few
steps to a highly accurate solution of the discretized problem. Subject only to the reasonable
assumption that the discretization of the original time and warped time are suﬃciently ﬁne,
this method ﬁnds the global solution.

4.1 Dynamic programming

We now discretize the values that τi is allowed to take:

τi ∈ Ti =

τi1, . . . , τiM }

,

{

i = 1, . . . , N.

One choice for these discretized values is linear spacing between given lower and upper
bounds on τi, 0

1:

≤

ui ≤
li ≤
τij = li +

j
M

1
1

−
−

(ui −

li),

j = 1, . . . , M,

i = 1, . . . , N.

Here M is the number of values that we use to discretize each value of τi (which we take
to be the same for each i, for simplicity). We will assume that 0
∈ TN , so the
constraints τ1 = 0 and τN = 1 are feasible.

∈ T1 and 1

The bounds can be chosen as

li = max
{

sminti, 1

smax(1

,

ti)
}

−

smaxti, 1
ui = min
{

−

smin(1

,

ti)
}

−

−

i = 1, . . . , N, (7)

where smin and smax are the given minimum and maximum allowed values of φ(cid:48). This is
are drawn at position (ti, τij), for N = 30, M = 20
illustrated in ﬁgure 2, where the nodes of
and various values of smin and smax. Note that since N
M is the minimum slope, M should be
chosen to satisfy M < N

smax , a consideration that is automated in the provided software.

The objective (6) splits into a sum of terms that are functions of τi, and terms that are
τi. (These correspond to the separable state-action loss function terms
functions of τi+1 −
in the optimal control problem associated with φ(t) and φ(cid:48)(t), respectively.) The problem is

T

8

Figure 2: Left. Unconstrained grid. Left center. Eﬀect of introducing smin. Right center.
Eﬀect of smax. Right. Typical parameters that work well for our method.

then globally solved by standard methods of dynamic programming [24], using the methods
we now describe.

We form a graph with M N nodes, associated with the values τij, i = 1, . . . , N and
j = 1, . . . , M . (Note that i indexes the discretized values of t, and j indexes the discretized
values of τ .) Each node τij with i < N has M outgoing edges that terminate at the nodes of
1)M 2. This is
the form τi+1,k for k = 1, . . . , M . The total number of edges is therefore (N
illustrated in ﬁgure 2 for M = 25 and N = 100, where the nodes are shown at the location
(ti, τij). (In practice M and N would be considerably larger.)

−

At each node τij we associate the node cost

and on the edge from τij to τi+1,k we associate the edge cost

L(x(τij)

−

y(ti)) + λcumRcum(τij)

λinstRinst

(cid:18) τi+1,k −
ti+1 −

τij
ti

(cid:19)

.

With these node and edge costs, the objective ˆf (τ ) is the total cost of a path starting at node
τ11 = 0 and ending at τN M = 1. (Infeasible paths, for examples ones for which τi+1,k < τi,j,
.) Our problem is therefore to ﬁnd the shortest weighted path through a graph,
have cost +
which is readily done by dynamic programming.

∞

The computational cost of dynamic programming is order N M 2 ﬂops (not counting
the evaluation of the loss and regularization terms). With current hardware, it is entirely
practical for M = N = 1000 or even (much) larger. The path found is the globally optimal
one, i.e., τ ∗ minimizes ˆf (τ ), subject to the discretization constraints on the values of τi.

4.2 Iterative reﬁnement

After solving the problem above by dynamic programming, we can reduce the error induced
by discretizing the values of τi by updating li and ui. We shrink them both toward the current
value of τ ∗i , thereby reducing the gap between adjacent discretized values and reducing the

9

0.000.250.500.751.00t0.000.250.500.751.00τsmin=0,smax=+∞0.000.250.500.751.00t0.000.250.500.751.00τsmin=0.25,smax=+∞0.000.250.500.751.00t0.000.250.500.751.00τsmin=0,smax=40.000.250.500.751.00t0.000.250.500.751.00τsmin=0.001,smax=5discretization error. One simple method for updating the bounds is to reduce the range
ui −

li by a ﬁxed fraction η, say 1/2 or 1/8.

To do this we set

l(q+1)
i

(q)
τ ∗
= max
i −
{

η

l(q)
i

u(q)
i −
2

, l(0)
,
i }

u(q+1)
i

= min
{

(q)
i + η
τ ∗

l(q)
i

u(q)
i −
2

, u(0)
i }

in iteration q + 1, where the superscripts in parentheses above indicate the iteration. Using
the same data as ﬁgure 1b, ﬁgure 3 shows the iterative reﬁnement of τ ∗. Here, nodes of
are plotted at position (ti, τij), as it is iteratively reﬁned around τ ∗i .

T

Figure 3: Left to right. Iterative reﬁnement of τ ∗ for iterations q = 0, 1, 2, 3, with τ ∗ colored
orange.

4.3 Implementation

GDTW package. The algorithm described above has been implemented as the open
source Python package GDTW, with the dynamic programming portion written in C++ for
improved eﬃciency. The node costs are computed and stored in an M
N array, and the
N array. For multiple iterations
edge costs are computed on the ﬂy and stored in an M
on group-level alignments (see
7), multi-threading is used to distribute the program onto
worker threads.

M

×

×

×

§

Performance. We give an example of the performance attained by GDTW using real-world
5, which are uniformly sampled with N = 1000. Although it has no
signals described in
eﬀect on method performance, we take square loss, square cumulative warp regularization,
and square instantaneous warp regularization. We take M = 100.

§

The computations are carried on a 4 core MacBook. To compute the node costs requires
0.0055 seconds, and to compute the shortest path requires 0.0832 seconds. With reﬁnement
factor η = .15, only three iterations are needed before no signiﬁcant improvement is obtained,
and the result is essentially the same with other choices for the algorithm parameters N ,
M , and η. Over 10 trials, our method only took an average of 0.25 seconds, a 50x speedup
over FastDTW, which took an average of 14.1 seconds to compute using a radius of 50, which
is equivalent to M = 100. All of the data and example code necessary to reproduce these

10

0.000.250.500.751.00t0.000.250.500.751.00τk=00.000.250.500.751.00t0.000.250.500.751.00τk=10.000.250.500.751.00t0.000.250.500.751.00τk=20.000.250.500.751.00t0.000.250.500.751.00τk=3results are available in the GDTW repository. Also available are supplementary materials that
contain step-by-step instructions and demonstrations on how to reproduce these results.

4.4 Validation

To test the generalization ability of a speciﬁc time warping model, parameterized by L, λcum, Rcum, λinst,
and Rinst, we use out-of-sample validation by randomly partitioning N discretized time val-
ues 0 = t1, . . . , tN = 1 into two sorted ordered sets that contain the boundaries, ttrain
0, 1
}
and ttest
. Using only the time points in ttrain, we obtain our time warping function
φ by minimizing our discretized objective (6). (Recall that our method does not require
signals to be sampled at regular intervals, and so will work with the irregularly spaced time
points in ttrain.)

0, 1
}

∪ {

∪ {

We compute two loss values: a training error

(cid:96)train =

ttrain
|−
(cid:88)

|

i=1

1
(ttrain

i+1 −

ttrain
i

) (cid:0)L(x(φ(ttrain

i

))

y(ttrain
i

))(cid:1) ,

−

and a test error

(cid:96)test =

ttest
|−
(cid:88)

|

i=1

1
(ttest

i+1 −

ttest
i

) (cid:0)L(x(φ(ttest

i

))

y(ttest
i

))(cid:1) .

−

Figure 4 shows (cid:96)test over a grid of values of λcum and λinst, for a partition where ttrain and ttest
each contain 50% of the time points. In this example, we use the signals shown ﬁgure 1a.

Figure 4: Test loss.

11

10−610−410−2100λcum10−610−510−410−310−210−1100101λinst‘test10−510−410−310−210−1100Ground truth estimation. When a ground truth warping function, φtrue, is available,
we can score how well our φ approximates φtrue by computing the following errors:

and

(cid:15)train =

ttrain
|−
(cid:88)

|

i=1

1
(ttrain

i+1 −

ttrain
i

) (cid:0)L(φtrue(ttrain

i

)

φ(ttrain
i

))(cid:1) ,

−

(cid:15)test =

ttest
|−
(cid:88)

|

i=1

1
(ttest

i+1 −

ttest
i

) (cid:0)L(φtrue(ttest

i

)

φ(ttest
i

))(cid:1) .

−

In the example shown in ﬁgure 4, target signal y is constructed by composing x with a known
warping function φtrue, such that y(t) = (x
φtrue)(t). Figure 5 shows the contours of (cid:15)test
for this example.

◦

Figure 5: Test error.

5 Examples

We present a few examples of alignments using our method. Figure 6 is a synthetic example
of diﬀerent types of time warping functions. Figure 7 is real-world example using biological
signals (ECGs). We compare our method using varying amounts of regularization λinst
∈
, N = 1000, M = 100 to those using with FastDTW [17], as implemented in
0.01, 0.1, 0.5
{
}
the Python package FastDTW [25] using the equivalent graph size N = 1000, radius = 50. As
expected, the alignments using regularization are smoother and less prone to singularities
than those from FastDTW, which are unregularized. Figure 8 shows how the time warp
functions become smoother as λinst grows.

12

10−610−410−2100λcum10−610−510−410−310−210−1100101λinst(cid:15)test10−510−410−310−210−1100Figure 6: Left. Signal x and target signal y. Middle. Warping function φ and the ground
truth warping φtrue. Right. The time-warped x and y.

6 Extensions and variations

We will show how to extend our formulation to address complex scenarios, such as aligning
a portion of a signal to the target, regularization of higher-order derivatives, and symmetric
time warping, where both signals align to each other.

6.1 Alternate boundary and slope constraints

We can align a portion of a signal with the target by adjusting the boundary constraints to
allow 0
. We
incorporate this by reformulating (7) as

0 < x < 1
}

1, for margin β =

β and (1

φ(0)

φ(1)

β)

R

≤

≥

≤

≥

−

∈

x

{

|

sminti, (1
li = max
{

−

smax(1

ti))

β

,

}

−

−

smaxti +β, 1
ui = min
{

−

smin(1

ti)

,
}

−

i = 1, . . . , N.

We can also allow the slope of φ to be negative, by choosing smin < 0. These modiﬁcations are
are drawn at position (ti, τij), for N = 30, M = 20
illustrated in ﬁgure 9, where the nodes of
and various values of β, smin, and smax.

T

6.2 Penalizing higher-order derivatives

We can extend the formulation to include a constraint or objective term on the higher-order
derivatives, such as the second derivative φ(cid:48)(cid:48). This requires us to extend the discretized state
space to include not just the current M values, but also the last M values, so the state space
size grows to M 2 in the dynamic programming problem.

13

0.00.20.40.60.81.0t−1.0−0.50.00.51.0y(t)andx(t)y(t)x(t)0.00.20.40.60.81.0t−1.0−0.50.00.51.0y(t)and(x◦φ)(t)y(t)(x◦φ)(t)0.00.20.40.60.81.0t0.000.250.500.751.00φ(t)andφtrue(t)φ(t)=tφ(t)φtrue(t)0.00.20.40.60.81.0t−1.0−0.50.00.51.0y(t)andx(t)y(t)x(t)0.00.20.40.60.81.0t−1.0−0.50.00.51.0y(t)and(x◦φ)(t)y(t)(x◦φ)(t)0.00.20.40.60.81.0t0.000.250.500.751.00φ(t)andφtrue(t)φ(t)=tφ(t)φtrue(t)0.00.20.40.60.81.0t−1.0−0.50.00.51.0y(t)andx(t)y(t)x(t)0.00.20.40.60.81.0t−1.0−0.50.00.51.0y(t)and(x◦φ)(t)y(t)(x◦φ)(t)0.00.20.40.60.81.0t0.000.250.500.751.00φ(t)andφtrue(t)φ(t)=tφ(t)φtrue(t)Figure 7: Top four. ECGs warped using our method while increasing λinst. Bottom. Results
using FastDTW, with a few of the singularities circled in red.

Figure 8: Left. φ(t) Right. φ(t)

−

t for ECGs. (Smoother lines correspond to larger λinst.)

The regularization functional for the second-order instantaneous warp is

inst2

R

(φ) =

(cid:90) 1

0

Rinst2

(φ(cid:48)(cid:48)(t))dt,

: R

where Rinst2
of time warping. Like the function Rinst, Rinst2
to encode constraints on φ(cid:48)(cid:48).

∪ {∞}

→

R

is the penalty function on the second-order instantaneous rate
, which allows us
can take on the value +

∞

14

0.00.20.40.60.81.0t−5000500λinst=0.01φ(t)Mappingx(t)toy(t):GDTWy(t)x(t)φ(t)0.00.20.40.60.81.0t−5000500λinst=0.1φ(t)Mappingx(t)toy(t):GDTW0.00.20.40.60.81.0t−5000500λinst=0.5φ(t)Mappingx(t)toy(t):GDTW0.00.20.40.60.81.0t 5000500 (t)Mappingx(t)toy(t):FastDTWy(t)x(t) (t)0.00.20.40.60.81.0t0.000.250.500.751.00τφ(t)FastDTWGDTW0.00.20.40.60.81.0t−0.10−0.050.000.050.10φ(t)−tφ(t)−tFastDTWGDTWFigure 9: Left. Eﬀect of introducing β to unconstrained grid. Left center. Eﬀect of
introducing β using typical parameters. Right center. Eﬀect of introducing β using larger
smin. Right. Eﬀect of negative smin.

We use a three-point central diﬀerence approximation of the second derivative for evenly

spaced time points

φ(ti+1)

φ(cid:48)(cid:48)(ti) =

−
(ti+1 −
and unevenly spaced time points

2φ(ti) + φ(ti
ti)2

1)

−

=

τi+1 −

2τi + τi
ti)2

(ti+1 −

1

−

,

i = 1, . . . , N

1,

−

2(δ1φ(ti+1)

φ(cid:48)(cid:48)(ti) =

−

(δ1 + δ2)φ(ti) + δ2φ(ti
δ1δ2(δ1 + δ2)
ti
1, where δ1 = ti −

−

1 and δ2 = ti+1 −

for i = 1, . . . , N
obtain the discretized objective

−

1))

−

=

2(δ1τi+1 −

(δ1 + δ2)τi + δ2τi

1)

−

,

δ1δ2(δ1 + δ2)

ti. With this approximation, we

ˆf (τ ) =

N
1
(cid:88)
−

i=1

(ti+1−

ti)

(cid:16)

L(x(τi)

y(ti)) + λcumRcum(τi −

−

ti) + λinstRinst(φ(cid:48)(ti)) + λinst2

Rinst2

(cid:17)

(φ(cid:48)(cid:48)(ti))

.

6.3 General loss

The two signals need not be vector valued; they could have categorical values, for example

L(τi, ti) =

(cid:26) 1 τi (cid:54)

= ti
0 otherwise,

or

L(τi, ti) =

(cid:26) g(τi, ti) τi (cid:54)

= ti
otherwise,

0

R++ →
where g : R++ ×
certain mismatches or a similarity matrix [26].

R is a categorical distance function that can specify the cost of

Another example could use the Earth mover’s distance, EMD : Rn

two short-time spectra

Rn

×

→

R, between

L(φ, ti) = EMD(
{

φ(ti −
R is a radius around time point ti.

ρ), . . . , φ(ti), . . . , φ(ti + ρ)
}

where ρ

∈

,

ti −

{

ρ, . . . , ti, . . . , ti + ρ

),
}

15

0.000.250.500.751.00t0.000.250.500.751.00τsmin=0,smax=+∞,β=.250.000.250.500.751.00t0.000.250.500.751.00τsmin=0.001,smax=5,β=.250.000.250.500.751.00t0.000.250.500.751.00τsmin=0.25,smax=4,β=.2500.250.50.751.0t−0.250.000.250.500.751.001.25τsmin=−0.25,smax=4,β=.25(φ), as in (1).

L

6.4 Symmetric time warping

Until this point, we have used unidirectional time warping, where signal x is time-warped
y. We can also perform bidirectional time warping, where
to align with y such that x
signals x and y are time-warped each other. Bidirectional time warping results in two time
ψ.
warp functions, φ and ψ, where x

≈

φ

φ

◦

y

Bidirectional time warping requires a diﬀerent loss functional. Here we deﬁne the bidi-
rectional loss associated with time warp functions φ and ψ, on the two signals x and y,
as

◦

≈

◦

(cid:90) 1

where we distinguish the bidirectional case by using two arguments,

(φ, ψ) =

L(x(φ(t))

0

L

y(ψ(t)))dt,

−

(φ, ψ), instead of one,

L

Bidirectional time warping can be symmetric or asymmetric. In the symmetric case, we

choose φ, ψ by solving the optimization problem

minimize
R
subject to φ(0) = 0, φ(1) = 1, ψ(t) = 2t

cum(φ) + λinst

(φ, ψ) + λcum

R

L

inst(φ)

φ(t),

−

where the constraint ψ(t) = 2t
φ(t) ensures that φ and ψ are symmetric about the identity.
The symmetric case does not add additional computational complexity, and can be readily
solved using the iterative reﬁnement procedure described in

−

4.

In the asymmetric case, φ, ψ are chosen by solving the optimization problem

§

cum(ψ) + λinst
minimize
R
subject to φ(0) = 0, φ(1) = 1, ψ(0) = 0, ψ(1) = 1.

cum(φ) + λcum

(φ, ψ) + λcum

R

L

inst(φ) + λinst

R

inst(ψ)

R

The asymmetric case requires Rcum, Rinst to allow negative slopes for ψ. Further, it requires
a modiﬁed iterative reﬁnement procedure (not described here) with an increased complexity
of order N M 4 ﬂops, which is impractical when M is not small.

7 Time-warped distance, centering, and clustering

In this section we describe three simple extensions of our optimization formulation that yield
useful methods for analyzing a set of signals x1, . . . , xM .

7.1 Time-warped distance

For signals x and y, we can interpret the optimal value of (4) as the time-warped distance
between x and y, denoted D(x, y). (Note that this distance measures takes into account
both the loss and the regularization, which measures how much warping was needed.) When
λcum and λinst are zero, we recover the unconstrained DTW distance [1]. This distance is
= D(y, x). If a symmetric distance is
not symmetric; we can (and usually do) have D(x, y)

16

(cid:54)
preferred, we can take (D(x, y) + D(y, x))/2, or the optimal value of the group alignment
problem (8), with a set of original signals x, y.

The warp distance can be used in many places where a conventional distance between
two signals is used. For example we can use warp distance to carry out k nearest neighbors
regression [27] or classiﬁcation. Warp distance can also be used to create features for further
machine learning. For example, suppose that we have carried out clustering into K groups,
as discussed above, with target or group centers or exemplar signals y1, . . . , yK. From these
we can create a set of K features related to the warp distance of a new signal x to the centers
y1, . . . , yK, as

zi =

edi/σ
j=1 edj /σ

(cid:80)K

,

i = 1, . . . , K,

where di = D(x, yi) and σ is a positive (scale) hyper-parameter.

7.2 Time-warped alignment and centering

In time-warped alignment, the goal is to ﬁnd a common target signal µ that each of the
original signals can be warped to, at low cost. We pose this in the natural way as the
optimization problem

minimize (cid:80)M
−
subject to φi(0) = 0, φi(1) = 1,

0 L(xi(φi(t))

i=1

(cid:16)(cid:82) 1

µ(t)) dt + λcum

cum(φi) + λinst

R

(cid:17)

inst(φi)

R

(8)

where the variables are the warp functions φ1, . . . , φM and the target µ, and λcum and λinst
are positive hyper-parameters. The objective is the sum of the objectives for time warping
each xi to µ. This is very much like our basic formulation (4), except that we have multiple
signals to warp, and the target µ is also a variable that we can choose.

The problem (8) is hard to solve exactly, but a simple iterative procedure seems to work
well. We observe that if we ﬁx the target µ, the problem splits into M separate dynamic time
warping problems that we can solve (separately, in parallel) using the method described in
4.
Conversely, if we ﬁx the warping functions φ1, . . . , φM , we can optimize over µ by minimizing

§

M
(cid:88)

(cid:90) 1

i=1

0

L(xi(φi(t))

µ(t)) dt.

−

This is turn amounts to choosing each µ(t) to minimize

M
(cid:88)

i=1

L(xi(φi(t))

µ(t)).

−

This is typically easy to do; for example, with square loss, we choose µ(t) to be the mean of
xi(φi(t)); with absolute value loss, we choose µ(t) to be the median of xi(φi(t)).

This method of alternating between updating the target µ and updating the warp func-
tions (in parallel) typically converges quickly. However, it need not converge to the global

17

minimum. One simple initialization is to start with no warping, i.e., φi(t) = t. Another is
to choose one of the original signals as the initial value for µ.

As a variation, we can also require the warping functions to be evenly arranged about a
common time warp center, for example φ(t) = t. We can do this by imposing a “centering”
constraint on (8),

minimize (cid:80)M
−
subject to φi(0) = 0, φi(1) = 1,

0 L(xi(φi(t))

i=1

(cid:16)(cid:82) 1

µ(t)) dt + λcum

cum(φi) + λinst

(cid:80)M

1
M

i=1 φi(t) = t,

R

(cid:17)

inst(φi)

R

(9)

(cid:80)M

where 1
i=1 φi(t) = t forces φ1, . . . , φM to be evenly distributed around the identity φ(t) =
M
t. The resulting centered time warp functions, can be used to produce a centered time-warped
mean. Figure 10 compares a time-warped mean with and without centering, using synthetic
data consisting of multi-modal signals from [5].

Figure 10: Top. Time-warped mean. Bottom. Centered time-warped mean. Left. Original
signals. Left center. Warped signals after iteration 1. Right center. Warped signals after
iteration 2. Right. Time warp functions after iteration 2.

Figure 11 shows examples of centered time-warped means of real-world data (using our
default parameters), consisting of ECGs and sensor data from an automotive engine [28].
The ECG example demonstrates that subtle features of the input sequences are preserved
in the alignment process, and the engine example demonstrates that the alignment process
can ﬁnd structure in noisy data.

7.3 Time-warped clustering

A further generalization of our optimization formulation allows us to cluster set of signals
x1, . . . , xM into K groups, with each group having a template or center or exemplar. This
can be considered a time-warped version of K-means clustering; see, e.g., [29, Chapter 4]. To
describe the clusters we use the M -vector c, with ci = j meaning that signal xi is assigned

18

0.000.250.500.751.00t−1.0−0.50.00.51.0xj(t)forj=1,...,20Original0.000.250.500.751.00t−1.0−0.50.00.51.0Iteration10.000.250.500.751.00t−1.0−0.50.00.51.0Iteration20.000.250.500.751.00t0.000.250.500.751.00τφj(t)0.000.250.500.751.00t−1.0−0.50.00.51.0xj(t)forj=1,...,20Original0.000.250.500.751.00t−1.0−0.50.00.51.0Iteration10.000.250.500.751.00t−1.0−0.50.00.51.0Iteration20.000.250.500.751.00t0.000.250.500.751.00τφj(t)Figure 11: Top. ECG signals. Bottom. Engine sensor signals. Left. Original signals. Left
center. Warped signals after iteration 1. Right center. Warped signals after iteration 2.
Right. Time warp functions after iteration 2.

to group j, where j
y1, . . . , yK.

1, . . . , M

}

∈ {

. The exemplars or templates are the signals denoted

minimize (cid:80)M
−
subject to φi(0) = 0, φi(1) = 1,

0 L(xi(φi(t))

i=1

(cid:16)(cid:82) 1

yci(t)) dt + λcum

cum(φi) + λinst

R

R

(cid:17)

inst(φi)

(10)

where the variables are the warp functions φ1, . . . , φM , the templates y1, . . . , yK, and the
assignment vector c. As above, λcum and λinst are positive hyper-parameters.

We solve this (approximately) by cyclically optimizing over the warp functions, the tem-
plates, and the assignments. Figure 12 shows an example of this procedure (using our
default parameters) on a set of sinusoidal, square, and triangular signals of varying phase
and amplitude.

Figure 12: K-means alignment on synthetic data.

19

0.000.250.500.751.00t−500−2500250500xj(t)forj=1,...,63Original0.000.250.500.751.00t−500−2500250500Iteration10.000.250.500.751.00t−500−2500250500Iteration20.000.250.500.751.00t0.000.250.500.751.00τφj(t)0.000.250.500.751.00t−4−2024xkj(t)forj=1,...,400Original0.000.250.500.751.00t−4−2024k=10.000.250.500.751.00t−4−2024k=20.000.250.500.751.00t0.000.250.500.751.00τφkj(t)0.000.250.500.751.00t0.20.40.60.8xkj(t)forj=1,...,75Original0.000.250.500.751.00t0.20.40.60.8Ck=00.000.250.500.751.00t0.20.40.60.8Ck=10.000.250.500.751.00t0.20.40.60.8Ck=28 Conclusion

We claim three main contributions. We propose a full reformulation of DTW in contin-
uous time that eliminates singularities without the need for preprocessing or step func-
tions. Because our formulation allows for non-uniformly sampled signals, we are the ﬁrst
to demonstrate how validation can be used for DTW model selection. Finally, we oﬀer an
implementation that runs 50x faster than state-of-the-art methods on typical problem sizes,
and distribute our C++ code (as well as all of our example data) as an open-source Python
package called GDTW.

20

References

[1] H. Sakoe and S. Chiba. Dynamic programming algorithm optimization for spoken word
recognition. IEEE transactions on acoustics, speech, and signal processing, 26(1):43–49,
1978.

[2] E. Keogh and M. Pazzani. Derivative dynamic time warping. In Proceedings of the 2001

SIAM International Conference on Data Mining, pages 1–11. SIAM, 2001.

[3] J. Marron, J. Ramsay, L. Sangalli, and A. Srivastava. Functional data analysis of

amplitude and phase variation. Statistical Science, 30(4):468–484, 2015.

[4] M. Singh, I. Cheng, M. Mandal, and A. Basu. Optimization of symmetric transfer error
In European Conference on Computer Vision,

for sub-frame video synchronization.
pages 554–567. Springer, 2008.

[5] A. Srivastava, W. Wu, S. Kurtek, E. Klassen, and J. Marron. Registration of functional

data using Fisher-Rao metric. arXiv preprint arXiv:1103.3817, 2011.

[6] M. Dupont and P. Marteau. Coarse-DTW for sparse time series alignment.

In In-
ternational Workshop on Advanced Analytics and Learning on Temporal Data, pages
157–172. Springer, 2015.

[7] J. Zhao and L. Itti.

ShapeDTW: Shape dynamic time warping.

arXiv preprint

arXiv:1606.01601, 2016.

[8] F. Itakura. Minimum prediction residual principle applied to speech recognition. IEEE

Transactions on Acoustics, Speech, and Signal Processing, 23(1):67–72, 1975.

[9] C. Myers, L. Rabiner, and A. Rosenberg. Performance tradeoﬀs in dynamic time warping
algorithms for isolated word recognition. IEEE Transactions on Acoustics, Speech, and
Signal Processing, 28(6):623–635, 1980.

[10] L. Rabiner and B. Juang. Fundamentals of Speech Recognition. Prentice Hall, 1993.

[11] A. Tikhonov and V. Arsenin. Solutions of Ill-Posed Problems, volume 14. Winston,

1977.

[12] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal

Statistical Society, 58(1):267–288, 1996.

[13] P. Green and B. Silverman. Non-Parametric Regression and Generalized Linear Models:

A Roughness Penalty Approach. CRC Press, 1993.

[14] J. Ramsay and B. Silverman. Functional Data Analysis. Springer, 2005.

[15] J. Ramsay and B. Silverman. Applied Functional Data Analysis: Methods and Case

Studies. Springer, 2007.

21

[16] A. Srivastava and E. Klassen. Functional and Shape Data Analysis. Springer, 2016.

[17] S. Salvador and P. Chan. Toward accurate dynamic time warping in linear time and

space. Intelligent Data Analysis, 11(5):561–580, 2007.

[18] J. Friedman, T. Hastie, and R. Tibshirani. The Elements of Statistical Learning, vol-

ume 1. Springer, 2001.

[19] P.J. Huber. Robust statistics. In International Encyclopedia of Statistical Science, pages

1248–1251. Springer, 2011.

[20] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.

[21] D. P. Bertsekas. Dynamic Programming and Optimal Control, volume 1. Athena Scien-

tiﬁc, 2005.

[22] P. Hansen. Rank-Deﬁcient and Discrete Ill-Posed Problems: Numerical Aspects of Lin-

ear Inversion, volume 4. SIAM, 2005.

[23] G. Golub and C. Van Loan. Matrix Computations, volume 3. JHU Press, 2012.

[24] R. Bellman and S. Dreyfus. Applied Dynamic Programming, volume 2050. Princeton

University Press, 2015.

[25] K. Tanida. FastDTW. GitHub Repository https: // github. com/ slaypni/ fastdtw ,

2015.

[26] S. Needleman and C. Wunsch. A general method applicable to the search for similarities
in the amino acid sequence of two proteins. Journal of molecular biology, 48(3):443–453,
1970.

[27] X. Xi, E. Keogh, C. Shelton, L. Wei, and C. A. Ratanamahatana. Fast time series classi-
ﬁcation using numerosity reduction. In Proceedings of the 23rd international conference
on Machine learning, pages 1033–1040. ACM, 2006.

[28] M. Abou-Nasr and L. Feldkamp. Ford Classiﬁcation Challenge. Zip Archive http: //
www. timeseriesclassification. com/ description. php? Dataset= FordA , 2008.

[29] S. Boyd and L. Vandenberghe. Introduction to Applied Linear Algebra: Vectors, Matri-

ces, and Least Squares. Cambridge University Press, 2018.

22

