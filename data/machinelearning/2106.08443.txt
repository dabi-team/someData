To appear as a part of an upcoming textbook on dimensionality reduction and manifold learning.

1
2
0
2

n
u
J

5
1

]
L
M

.
t
a
t
s
[

1
v
3
4
4
8
0
.
6
0
1
2
:
v
i
X
r
a

Reproducing Kernel Hilbert Space, Mercer’s Theorem, Eigenfunctions,
Nystr¨om Method, and Use of Kernels in Machine Learning:
Tutorial and Survey

Benyamin Ghojogh
Department of Electrical and Computer Engineering,
Machine Learning Laboratory, University of Waterloo, Waterloo, ON, Canada

BGHOJOGH@UWATERLOO.CA

Ali Ghodsi
Department of Statistics and Actuarial Science & David R. Cheriton School of Computer Science,
Data Analytics Laboratory, University of Waterloo, Waterloo, ON, Canada

ALI.GHODSI@UWATERLOO.CA

Fakhri Karray
Department of Electrical and Computer Engineering,
Centre for Pattern Analysis and Machine Intelligence, University of Waterloo, Waterloo, ON, Canada

KARRAY@UWATERLOO.CA

Mark Crowley
Department of Electrical and Computer Engineering,
Machine Learning Laboratory, University of Waterloo, Waterloo, ON, Canada

MCROWLEY@UWATERLOO.CA

Abstract

This is a tutorial and survey paper on kernels,
kernel methods, and related ﬁelds. We start with
reviewing the history of kernels in functional
analysis and machine learning. Then, Mercer
kernel, Hilbert and Banach spaces, Reproduc-
ing Kernel Hilbert Space (RKHS), Mercer’s the-
orem and its proof, frequently used kernels, ker-
nel construction from distance metric, important
classes of kernels (including bounded, integrally
positive deﬁnite, universal, stationary, and char-
acteristic kernels), kernel centering and normal-
ization, and eigenfunctions are explained in de-
tail. Then, we introduce types of use of kernels
in machine learning including kernel methods
(such as kernel support vector machines), kernel
learning by semi-deﬁnite programming, Hilbert-
Schmidt independence criterion, maximum mean
discrepancy, kernel mean embedding, and ker-
nel dimensionality reduction. We also cover rank
and factorization of kernel matrix as well as the
approximation of eigenfunctions and kernels us-
ing the Nystr¨om method. This paper can be use-
ful for various ﬁelds of science including ma-

chine learning, dimensionality reduction, func-
tional analysis in mathematics, and mathematical
physics in quantum mechanics.

1. Introduction
1.1. History of Kernels
It is 1904 when David Hilbert proposed his work on ker-
nels and deﬁned a deﬁnite kernel (Hilbert, 1904), and
later, his and Erhard Schmidt’s works proposed integral
equations such as Fredholm integral equations (Hilbert,
1904; Schmidt, 1908). This was the introduction of a
new space which was named the Hilbert space. Later, the
Hilbert space was found to be very useful for the formu-
lations in quantum mechanics (Prugovecki, 1982). After
the initial works on Hilbert space by Hilbert and Schmidt
(Hilbert, 1904; Schmidt, 1908), James Mercer improved
Hilbert’s work and proposed his theorem in 1909 (Mer-
cer, 1909) which was named the Mercer’s theorem later.
In the mean time, Stefan Banach, Hans Hahn, and Eduard
Helly proposed the concepts of another new space in years
1920–1922 (Bourbaki, 1950) which was named the Banach
space later by Maurice Ren´e Fr´echet (Narici & Becken-
stein, 2010). The Hilbert space is a subset of the Banach
space.
Reproducing Kernel Hilbert Space (RKHS) is a special
case of Hilbert space with some properties. It is a Hilbert
space of functions with reproducing kernels (Berlinet &

 
 
 
 
 
 
Thomas-Agnan, 2011). The ﬁrst work on RKHS was
(Aronszajn, 1950). Later, the concepts of RKHS were im-
proved further in (Aizerman et al., 1964). The RKHS re-
mained in pure mathematics until this space was used for
the ﬁrst time in machine learning by introduction of ker-
nel Support Vector Machine (SVM) (Boser et al., 1992;
Vapnik, 1995). Eigenfunctions were also developed for
eigenvalue problem applied on operators and functions
(Williams & Seeger, 2000) and were used in machine learn-
ing (Bengio et al., 2003c) and physics (Kusse & Westwig,
2006). This is related to RKHS because it uses weighted
inner product in Hilbert space (Williams & Seeger, 2000)
and RKHS is a Hilbert space of functions with a reproduc-
ing kernel.
Using kernels was widely noticed when linear SVM (Vap-
nik & Chervonenkis, 1974) was kernelized (Boser et al.,
1992; Vapnik, 1995). Kernel SVM showed off very suc-
cessfully because of its merits. Two competing models,
kernel SVM and neural network were the models which
could handle nonlinear data (see (Fausett, 1994) for history
of neural networks). Kernel SVM transformed nonlinear
data to RKHS to make the pattern of data linear hopefully
and then applied linear SVM on it. However, the approach
of neural network was different because the model itself
was nonlinear (see Section 8 for more details). The suc-
cess of kernel SVM plus the problem of vanishing gradi-
ents in neural networks (Goodfellow et al., 2016) resulted
in the winter of neural network around years 2000 to 2006.
However, the problems of training deep neural networks
started to be resolved (Hinton & Salakhutdinov, 2006) and
their success plus two problems of kernel SVM helped neu-
ral networks take over kernel SVM gradually. One prob-
lem of kernel SVM was not knowing the suitable kernel
type for various learning problems.
In other words, ker-
nel SVM still required the user to choose the type of ker-
nel but neural networks were end-to-end and almost robust
to hyperparameters such as the number of layers or neu-
rons. Another problem was that kernel SVM could not han-
dle big data, although Nystr¨om method, ﬁrst proposed in
(Nystr¨om, 1930), was used to resolve this problem of ker-
nel methods by approximating kernels from a subset of data
(Williams & Seeger, 2001). Note that kernels have been
used widely in machine learning such as in SVM (Vap-
nik, 1995), Gaussian process classiﬁers (Williams & Bar-
ber, 1998), and spline methods (Wahba, 1990). The types
of use of kernels in machine learning will be discussed in
Section 9.

1.2. Useful Books on Kernels
There exist several books about use of kernels in machine
learning. Some examples are (Smola & Sch¨olkopf, 1998;
Sch¨olkopf et al., 1999a; Sch¨olkopf & Smola, 2002; Shawe-
Taylor & Cristianini, 2004; Camps-Valls, 2006; Steinwart
& Christmann, 2008; Rojo- ´Alvarez et al., 2018; Kung,

2

2014). Some survey papers about kernel-based machine
learning are (Hofmann et al., 2006; 2008; M¨uller et al.,
2018). In addition to some of the above-mentioned books,
there exist some other books/papers on kernel SVM such as
(Sch¨olkopf et al., 1997b; Burges, 1998; Hastie et al., 2009).

1.3. Kernel in Different Fields of Science
The term kernel has been used in different ﬁelds of science
for various purposes. In the following, we brieﬂy introduce
the different uses of kernel in science to clarify which use
of kernel we are focusing on in this paper.

1. Kernel of ﬁlter in signal processing: In signal pro-
cessing, one can use ﬁlters to ﬁlter a price of signal,
such as an image (Gonzalez & Woods, 2002). Dig-
ital ﬁlters have a kernel which determine the values
of ﬁlter in the window of ﬁlter (Schlichtharle, 2011).
In convolutional neural networks, the ﬁlter kernels are
learned in deep learning (Goodfellow et al., 2016).
2. Kernel smoothing for density estimation: Kernel
density estimation can be used for ﬁtting a mixture
of distributions to some data instances (Scott, 1992).
For this, a histogram with inﬁnite number of bins is
utilized. In limit, this histogram is converged to a ker-
nel smoothing (Wand & Jones, 1994) where the kernel
determines the type of distribution. For example, if a
Radial Basis Function (RBF) kernel is used, a mixture
of Gaussian distributions is ﬁtted to data.

3. Kernelization in complexity theory: Kernelization is
a pre-processing technique where the input to an algo-
rithm is replaced by a part of the input named kernel.
The output of the algorithm on kernel should either be
the same as or be able to be transformed to the out-
put of the algorithm for the whole input (Fomin et al.,
2019). An example usage of kernelization is in vertex
cover problem (Abu-Khzam et al., 2004).

4. Kernel in operating system: Kernel is the core of an
operating system, such as Linux, which connects the
hardware including CPU, memory, and peripheral de-
vices to applications (Anderson & Dahlin, 2014).
5. Kernel in linear algebra and graphs: Consider a map-
ping from the vector space V to the vector space W as
L : V → W. The kernel, also called the nullspace, of
this mapping is deﬁned as ker(L) := {v ∈ V | L(v) =
0}. For example, for a matrix A ∈ Ra×b, the ker-
nel of A is ker(A) = {x ∈ Rn | Ax = 0}. The
four fundamental subspaces of a matrix are its kernel,
row space, column space, and left null space (Strang,
1993). Note that the kernel (nullspace) of adjacency
matrix in graphs has also been well developed (Ak-
bari et al., 2006).

6. Kernel in other domains of mathematics: There ex-
ist kernel concepts in other domains of mathematics
and statistics such as geometry of polygon (Icking &
Klein, 1995), set theory (Bergman, 2011, p. 14), etc.

7. Kernel in feature space for machine learning: In sta-
tistical machine learning, kernels pull data to a feature
space for the sake of better discrimination of classes or
simpler representation of data (Hofmann et al., 2006;
2008).
In this paper, our focus is on this category
which is kernels for machine learning.

1.4. Organization of Paper
This paper is a tutorial and survey paper on kernels and ker-
nel methods. It can be useful for several ﬁelds of science in-
cluding machine learning, functional analysis in mathemat-
ics, and mathematical physics in quantum mechanics. The
remainder of this paper is organized as follows. Section
2 introduces the Mercer kernel, important spaces in func-
tional analysis including the Hilbert and Banach spaces,
and Reproducing Kernel Hilbert Space (RKHS). Mercer’s
theorem and its proof are provided in Section 3. Character-
istics of kernels are explained in Section 4. We introduce
frequently used kernels, kernel construction from distance
metric, and important classes of kernels in Section 5. Ker-
nel centering and normalization are explained in Section 6.
Eigenfunctions are then introduced in Section 7. We ex-
plain two techniques for kernelization in Section 8. Types
of use of kernels in machine learning are reviewed in Sec-
tion 9. Kernel factorization and Nystr¨om approximation
are introduced in Section 10. Finally, Section 11 concludes
the paper.

Required Background for the Reader
This paper assumes that the reader has general knowledge
of calculus, linear algebra, and basics of optimization. The
required basics of functional analysis are explained in the
paper.

2. Mercer Kernel and Spaces In Functional

Analysis

2.1. Mercer Kernel and Gram Matrix
Deﬁnition 1 (Mercer Kernel (Mercer, 1909)). The function
k : X 2 → R is a Mercer kernel function (also known as
kernel function) where:

1. it is symmetric: k(x, y) = k(y, x),
2. and its corresponding kernel matrix K(i, j) =
k(xi, xj), ∀i, j ∈ {1, . . . , n} is positive semi-deﬁnite:
K (cid:23) 0.

The corresponding kernel matrix of a Mercer kernel is a
Mercer kernel matrix.

The two properties of a Mercer kernel will be proved in
Section 4. By convention, unless otherwise stated, the term
kernel refers to Mercer kernel. The effectiveness of Mercer
kernel will be shown and proven in the Mercer’s theorem,
i.e., Theorem 2.

Deﬁnition 2 (Gram Matrix or Kernel Matrix). The matrix

3

K ∈ Rn×n is a Gram matrix, also known as a Gramian
matrix or a kernel matrix, whose (i, j)-th element is:

K(i, j) := k(xi, xj),

∀i, j ∈ {1, . . . , n}.

(1)

Here, we deﬁned the square kernel matrix applied on a set
of n data instances; hence, the kernel is a n × n matrix.
We may also have a kernel matrix between two sets of data
instances. This will be explained more in Section 8. More-
over, note that the kernel matrix can be computed using the
inner product between pulled data to the feature space. This
will be explained in detail in Section 3.2.

2.2. Hilbert, Banach, Lp, and Sobolev Spaces
Before deﬁning the RKHS and details of kernels, we need
to introduce Hilbert, Banach, Lp, and Sobolev spaces,
which are well-known spaces in functional analysis (Con-
way, 2007).

Deﬁnition 3 (Metric Space). A metric space is a set where
a metric, for measuring the distance between instances of
set, is deﬁned on it.

Deﬁnition 4 (Vector Space). A vector space is a set of vec-
tors equipped with some available operations such as ad-
dition and multiplication by scalars.

Deﬁnition 5 (Complete Space). A space F is complete
if every Cauchy sequence converges to a member of this
space f ∈ F. Note that the Cauchy sequence is a sequence
whose elements become arbitrarily close to one another as
the sequence progresses (i.e., it converges in limit).

Deﬁnition 6 (Compact Space). A space is compact if it
is closed (i.e., it contains all its limit points) and bounded
(i.e., all its points lie within some ﬁxed distance of one an-
other).

Deﬁnition 7 (Hilbert Space (Reed & Simon, 1972)). A
Hilbert space H is an inner product space that is a com-
plete metric space with respect to the norm or distance
function induced by the inner product.

The Hilbert space generalizes the Euclidean space to a ﬁ-
nite or inﬁnite dimensional space. Usually, the Hilbert
space is high dimensional. By convention in machine learn-
ing, unless otherwise stated, Hilbert space is also referred
to as the feature space. By feature space, researchers of-
ten speciﬁcally mean the RKHS space which will be intro-
duced in Section 2.3.

Deﬁnition 8 (Banach Space (Beauzamy, 1982)). A Banach
space is a complete vector space equipped with a norm.

Remark 1 (Difference of Hilbert and Banach Spaces).
Hilbert space is a special case of Banach space equipped
with a norm deﬁned using an inner product notion. All
Hilbert spaces are Banach spaces but the converse is not
true.

Suppose Rn, H, B, Mc, M, T denote the Euclidean space,
Hilbert space, Banach space, complete metric space, met-
ric space, and topological space (containing both open and
closed sets), respectively. Then, we have:

Rn ⊂ H ⊂ B ⊂ Mc ⊂ M ⊂ T.

(2)

Deﬁnition 9 (Lp Space). Consider a function f with do-
main [a, b]. For p > 0, let the Lp norm be deﬁned as:

(cid:107)f (cid:107)p :=

(cid:16) (cid:90)

|f (x)|p dx

(cid:17) 1

p

.

(3)

The Lp space is deﬁned as the set of functions with bounded
Lp norm:

Lp(a, b) := {f : [a, b] → R | (cid:107)f (cid:107)p < ∞}.

(4)

Deﬁnition 10 (Sobolev Space (Renardy & Rogers, 2006,
Chapter 7)). A Sobolev space is a vector space of functions
equipped with Lp norms and derivatives:

Wm,p := {f ∈ Lp(0, 1) | Dmf ∈ Lp(0, 1)},

(5)

where Dmf denotes the m-th order derivative.

Note that the Sobolev spaces are RKHS with some speciﬁc
kernels (Novak et al., 2018). RKHS will be explained in
Section 2.3.

2.3. Reproducing Kernel Hilbert Space
2.3.1. DEFINITION OF RKHS
Reproducing Kernel Hilbert Space (RKHS), ﬁrst proposed
in (Aronszajn, 1950), is a special case of Hilbert space with
some properties. It is a Hilbert space of functions with re-
producing kernels (Berlinet & Thomas-Agnan, 2011). Af-
ter the initial work on RKHS (Aronszajn, 1950), another
work (Aizerman et al., 1964) developed the RKHS con-
cepts. In the following, we introduce this space.

Deﬁnition 11 (RKHS (Aronszajn, 1950; Berlinet &
Thomas-Agnan, 2011)). A Reproducing Kernel Hilbert
Space (RKHS) is a Hilbert space H of functions f : X → R
with a reproducing kernel k : X 2 → R where k(x, .) ∈ H
and f (x) = (cid:104) k(x, .), f (cid:105).

The RKHS is explained in more detail in the following.
Consider the kernel function k(x, y) which is a function
of two variables. Suppose, for n points, we ﬁx one of the
variables to have k(x1, y), k(x2, y), . . . , k(xn, y). These
are all functions of the variable y. RKHS is a function
space which is the set of all possible linear combinations
of these functions (Kimeldorf & Wahba, 1971), (Aizerman
et al., 1964, p. 834), (Mercer, 1909):

H :=

(cid:110)

f (.) =

n
(cid:88)

i=1

αi k(xi, .)

(cid:111) (a)
=

(cid:110)

f (.) =

αi kxi(.)

(cid:111)
,

n
(cid:88)

i=1

(6)

4

where (a) is because we deﬁne kx(.) := k(x, .). This
equation shows that the bases of an RKHS are kernels. The
proof of this equation is obtained by considering both Eqs.
(21) and (34) together (n.b.
for better organization, it is
better that we provide those equations later). It is also note-
worthy that this equation will also appear in Theorem 1.
According to Eq. (6), every function in the RKHS can be
written as a linear combination. Consider two functions in
this space represented as f = (cid:80)n
i=1 αi k(xi, y) and g =
(cid:80)n
j=1 βj k(x, yj). Hence, the inner product in RKHS is

calculated as:

(cid:104)f, g(cid:105)k

(6)
=

(a)
=

(cid:68) n
(cid:88)

i=1
(cid:68) n
(cid:88)

αi k(xi, .),

αi k(xi, .),

n
(cid:88)

j=1
n
(cid:88)

j=1

βj k(yj, .)

(cid:69)

k

βj k(., yj)

(cid:69)

k

i=1
n
(cid:88)

n
(cid:88)

i=1

j=1

=

αi βj k(xi, yj),

(7)

where (a) is because kernel is symmetric (it will be proved
in Section 4). Hence, the norm in RKHS is calculated as:
(cid:107)f (cid:107)k := (cid:112)(cid:104)f, f (cid:105)k.

(8)

The subscript of norm and inner product in RKHS has var-
ious notations in the research papers. Some most famous
notations are (cid:104)f, g(cid:105)k, (cid:104)f, g(cid:105)H, (cid:104)f, g(cid:105)Hk , (cid:104)f, g(cid:105)F where Hk
denotes the Hilbert space associated with kernel k and F
stands for the feature space because RKHS is sometimes
referred to as the feature space.

Remark 2 (RKHS Being Unique for a Kernel). Given a
kernel, the corresponding RKHS is unique (up to isometric
isomorphisms). Given an RKHS, the corresponding kernel
is unique.
In other words, each kernel generates a new
RKHS.

Remark 3. As we also saw in Mercer’s theorem, the bases
of RKHS space is the eigenfunctions {ψi(.)}∞
i=1 which are
functions themselves. This, along with Eq. (6), show that
the RKHS space is a space of functions and not a space
of vectors. In other words, the basis vectors of RKHS are
basis functions named eigenfunctions. Because the RKHS
is a space of functions rather than a space of vectors, we
usually do not know the exact location of pulled points to
the RKHS but we know the relation of them as a function.
This will be explained more in Section 3.2 and Fig. 1.

2.3.2. REPRODUCING PROPERTY
(7), consider only one component for g to have
In Eq.
g(x) = (cid:80)n
j=1 βj k(xi, x) = βk(x, x) where we take
β = 1 to have g(x) = k(x, x) = kx(.). In other words,
assume the function g(x) is a kernel in the RKHS space.
Also consider the function f (x) = (cid:80)n
i=1 αi k(xi, x) in

the space. According to Eq. (7), the inner product of these
functions is:

components along and orthogonal to this subspace, respec-
tively denoted by f(cid:107) and f⊥:

5

αi k(xi, x)

(a)
= f (x),

(9)

f (xi) = (cid:104)f, k(xi, .)(cid:105)k

(cid:104)f (x), g(x)(cid:105)k = (cid:104)f, kx(.)(cid:105)k

(cid:68) n
(cid:88)

i=1

αi k(xi, x), k(x, x)

(cid:69)

k

=

=

n
(cid:88)

i=1

where (a) is because the term before equation is the previ-
ously considered function f . As Eq. (9) shows, the func-
tion f is reproduced from the inner product of that function
with one of the kernels in the space. This shows the repro-
ducing property of the RKHS space. A special case of Eq.
(9) is (cid:104)kx, kx(cid:105)k = k(x, x).
The name of RKHS consists of several parts becuase of the
following justiﬁcations:

f = f(cid:107) + f⊥
=⇒ (cid:107)f (cid:107)2

k = (cid:107)f(cid:107)(cid:107)2

k + (cid:107)f⊥(cid:107)2

k ≥ (cid:107)f(cid:107)(cid:107)2
k.

(12)

(13)

Moreover, using the reproducing property of the RKHS, we
have:

(12)
= (cid:104)f(cid:107), k(xi, .)(cid:105)k + (cid:104)f⊥, k(xi, .)(cid:105)k
(9)
(a)
= f(cid:107)(xi),
= (cid:104)f(cid:107), k(xi, .)(cid:105)k

(14)

where (a) is because the orthogonal component has zero
inner product with the bases of subspace. According to Eq.
(14), we have:

n
(cid:88)

i=1

(cid:96)(f (xi), yi) =

n
(cid:88)

i=1

(cid:96)(f(cid:107)(xi), yi).

(15)

1. “Reproducing”: because if the reproducing property

Using Eqs. (13) and (15), we can say:

of RKHS which was proved above.

2. “Kernel”: because of the kernels associated to RKHS

as stated in Deﬁnition 11 and Eq. (6).

3. “Hilbert Space”: because RKHS is a Hilbert space of
functions with a reproducing kernel, as stated in Deﬁ-
nition 11.

2.3.3. REPRESENTATION IN RKHS
In the following, we provide a proof for Eq. (6) and explain
why that equation deﬁnes the RKHS.

Theorem 1 (Representer Theorem (Kimeldorf & Wahba,
1971), simpliﬁed in (Rudin, 2012)). For a set of data
i=1, consider a RKHS H of functions f : X → R
X = {xi}n
with kernel function k. For any function (cid:96) : R2 → R
(usually called the loss function), consider the optimization
problem:

f ∗ ∈ arg min
f ∈H

n
(cid:88)

i=1

(cid:96)(f (xi), yi) + η Ω((cid:107)f (cid:107)k),

(10)

where η ≥ 0 is the regularization parameter and Ω((cid:107)f (cid:107)k)
is a penalty term such as (cid:107)f (cid:107)2
k. The solution of this opti-
mization can be expressed as:

f ∗ =

n
(cid:88)

i=1

αi k(xi, .) =

n
(cid:88)

i=1

αi kxi(.).

(11)

P.S.: Eq. (11) can also be seen in (Aizerman et al., 1964,
p. 834).

Proof. Proof is inspired by (Rudin, 2012). Assume
we project the function f onto a subspace spanned by
{k(xi, .)}n
i=1. The function f can be decomposed into

min
f ∈H

n
(cid:88)

i=1

(cid:96)(f (xi), yi) + η (cid:107)f (cid:107)2
k

= min
f ∈H

n
(cid:88)

i=1

(cid:96)(f(cid:107)(xi), yi) + η (cid:107)f(cid:107)(cid:107)2
k.

Hence, for this minimization, we only require the compo-
nent lying in the space spanned by the kernels of RKHS.
Therefore, we can represent the function (solution of opti-
mization) to lie in the space as linear combination of basis
vectors {k(xi, .)}n

i=1. Q.E.D.

Corollary 1. In Section 2.2, we mentioned that Hilbert
space can be inﬁnite dimensional. According to Deﬁnition
11, RKHS is a Hilbert space so it may be inﬁnite dimen-
sional. The representer theorem states that, in practice,
we only need to deal with a ﬁnite-dimensional space; al-
though, that ﬁnite number of dimensions is usually a large
number.

Note that the representer theorem has been used in kernel
SVM where αi’s are the dual variables which are non-zero
for support vectors (Boser et al., 1992; Vapnik, 1995). Ac-
cording to this theorem, kernel SVM only requires to learn
the dual variables, αi’s, to ﬁnd the optimal boundary be-
tween classes.

3. Mercer’s Theorem and Feature Map
3.1. Mercer’s Theorem
Deﬁnition 12 (Deﬁnite Kernel (Hilbert, 1904)). A kernel
k : [a, b] × [a, b] → R is a deﬁnite kernel where the follow-
ing double integral:

J(f ) =

(cid:90) b

(cid:90) b

a

a

k(x, y)f (x)f (y) dx dy,

(16)

satisﬁes J(f ) > 0 for all f (x) (cid:54)= 0.

Mercer improved over Hilbert’s work (Hilbert, 1904) to
propose his theorem, the Mercer’s theorem (Mercer, 1909),
introduced in the following.

Theorem 2 (Mercer’s Theorem (Mercer, 1909)). Suppose
k : [a, b] × [a, b] → R is a continuous symmetric positive
semi-deﬁnite kernel which is bounded:

6

provides a spectral (or eigenvalue) decomposition for the
operator Tk (Ghojogh et al., 2019a):

Tkψi(x) = λi ψi(x),

(22)

i=1 and {λi}∞

where {ψi(.)}∞
i=1 are the eigenvectors and
eigenvalues of the operator Tk, respectively. Noticing the
deﬁned Eq.
(18) and the eigenvalue decomposition, Eq.
(22), we have:

k(x, y) < ∞.

sup
x,y

(17)

(cid:90)

k(x, y) ψi(y) dy

(18)
= Tkψi(x)

(22)
= λi ψi(x).

(23)

Assume the operator Tk takes a function f (x) as its argu-
ment and outputs a new function as:

Tkf (x) :=

(cid:90) b

a

k(x, y)f (y) dy,

(18)

which is a Fredholm integral equation (Schmidt, 1908).
The operator Tk is called the Hilbert–Schmidt integral op-
erator (Renardy & Rogers, 2006, Chapter 8). This output
function is positive semi-deﬁnite:

(cid:90) (cid:90)

k(x, y)f (y) dx dy ≥ 0.

(19)

Then, there is a set of orthonormal bases {ψi(.)}∞
i=1 of
L2(a, b) consisting of eigenfunctions of TK such that the
corresponding sequence of eigenvalues {λi}∞
i=1 are non-
negative:

(cid:90)

k(x, y) ψi(y) dy = λi ψi(x).

(20)

The eigenfunctions corresponding to the non-zero eigen-
values are continuous on [a, b] and k can be represented as
(Aizerman et al., 1964):

k(x, y) =

∞
(cid:88)

i=1

λi ψi(x) ψi(y),

(21)

where the convergence is absolute and uniform.

Proof. A roughly high-level proof for the Mercer’s theo-
rem is as follows.
Step 1 of proof: According to assumptions of theorem,
the Hilbert-Schmidt integral operator Tk is a symmet-
ric operator on L2(a, b) space. Consider a unit ball in
L2(a, b) as input to the operator. As the kernel is bounded,
supx,y k(x, y) < ∞, the sequence f1, f2, . . . converges
in norm, i.e. (cid:107)fn − f (cid:107) → 0 as n → 0. Therefore, accord-
ing to the Arzel`a-Ascoli theorem (Arzel`a, 1895), the image
of the unit ball after applying the operator is compact. In
other words, the operator Tk is compact.
Step 2 of proof: According to the spectral
theorem
(Hawkins, 1975), there exist several orthonormal bases
{ψi(.)}∞
i=1 in L2(a, b) for the compact operator Tk. This

This proves the Eq.
(20) which is the eigenfunction de-
composition of the operator Tk. Note that the eigenvectors
{ψi(.)}∞
i=1 are referred to as the eigenfunctions because the
decomposition is applied on a function or operator rather
than a matrix. Note that eigenfunctions will be explained
more in Section 7.
Step 3 of proof: According to Parseval’s theorem (Parse-
val des Chenes, 1806), the Bessel’s inequality can be con-
verted to equality (Saxe, 2002). For the orthonormal bases
{ψi(.)}∞
i=1 in the Hilbert space H associated with kernel k,
we have for any function f ∈ L2(a, b):

∞
(cid:88)

(cid:104)f, ψi(cid:105)k ψi.

f =

i=1

(24)

If we replace ψi with f in Eq. (22) and consider Eq. (24),
we will have:

Tkf =

∞
(cid:88)

i=1

λi(cid:104)f, ψi(cid:105)k ψi.

(25)

One can consider Eq. (18) as Tkf = kf . Noticing this and
Eq. (25) results in:

kf =

∞
(cid:88)

i=1

λi(cid:104)f, ψi(cid:105)k ψi.

(26)

Ignoring f from Eq. (26) gives:

k(x, y) =

∞
(cid:88)

i=1

λi ψi(x) ψi(y),

(27)

which is Eq. (21); hence, that is proved.
Step 4 of proof: We deﬁne the truncated kernel rn (with
parameter n) as:

rn(x, y) := k(x, y) −

n
(cid:88)

i=1

λi ψi(x) ψi(y)

∞
(cid:88)

=

i=n+1

λi ψi(x) ψi(y).

(28)

As Tk is an integral operator, this truncated kernel has pos-
itive kernel, i.e., for every x ∈ [a, b], we have:

rn(x, x) = k(x, x) −

n
(cid:88)

i=1

λi ψi(x) ψi(x) ≥ 0

=⇒

n
(cid:88)

i=1

λi ψi(x) ψi(x) ≤ k(x, x) ≤ sup
x∈[a,b]

k(x, x).

(29)

By Cauchy-Schwartz inequality, we have:

(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

λi ψi(x) ψi(y)

(cid:12)
2
(cid:12)
(cid:12)

(cid:16) n
(cid:88)

≤

i=1

λi ψi(x) ψi(x)

(cid:17)(cid:16) n
(cid:88)

λi ψi(y) ψi(y)

(cid:17)

i=1

(29)
≤ sup

(cid:0)k(x, x)(cid:1)2

.

x∈[a,b]

Taking second root from the sides of inequality gives:

n
(cid:88)

i=1

λi

(cid:12)ψi(x) ψi(x)(cid:12)
(cid:12)

(cid:12) ≤ sup
x∈[a,b]

|k(x, x)|

(17)
≤ ∞.

(30)

This shows that the sequence (cid:80)n
verges absolutely and uniformly. Q.E.D.

i=1 λi ψi(x) ψi(x) con-

The Mercer’s theorem is very important. Many of its equa-
tions, such as Eqs. (18), (20), and (21) are used in theory
of kernels and kernel methods.

3.2. Feature Map and Pulling Function
Let X := {xi}n
i=1 be the set of data in the input space
(note that the input space is the original space of data). The
t-dimensional (perhaps inﬁnite dimensional) feature space
(or Hilbert space) is denoted by H.

Deﬁnition 13 (Feature Map or Pulling Function). We de-
ﬁne the mapping:

φ : X → H,

(31)

to transform data from the input space to the feature space,
i.e. Hilbert space. In other words, this mapping pulls data
to the feature space:

x (cid:55)→ φ(x).

(32)

The function φ(x) is called the feature map or pulling func-
tion. The feature map is a (possibly inﬁnite-dimensional)
vector whose elements are (Minh et al., 2006):

φ(x) = [φ1(x), φ2(x), . . . ](cid:62)

√

√

:= [

λ1 ψ1(x),

λ2 ψ2(x), . . . ](cid:62),

(33)

7

where {ψi} and {λi} are eigenfunctions and eigenvalues
of the kernel operator (see Eq. (20)). Note that eigenfunc-
tions will be explained more in Section 7.

Let t denote the dimensionality of φ(x). The feature map
may be inﬁnite or ﬁnite dimensional, i.e. t can be inﬁnity;
it is usually a very large number (recall Deﬁnition 7 where
we said Hilbert space may have inﬁnite number of dimen-
sions).
Considering both Eqs. (21) and (33) shows that:

k(x, y) = (cid:10)φ(x), φ(y)(cid:11)

k = φ(x)(cid:62)φ(y).

(34)

Hence, the kernel between two points is the inner product
of pulled data points to the feature space. Suppose we stack
the feature maps of all points X ∈ Rd×n column-wise in:

Φ(X) := [φ(x1), φ(x2), . . . , φ(xn)],

(35)

which is t × n dimensional and t may be inﬁnity or a large
number. The kernel matrix deﬁned in Deﬁnition 2 can be
calculated as:
Rn×n (cid:51) K = (cid:10)Φ(X), Φ(X)(cid:11)

k = Φ(X)(cid:62)Φ(X). (36)

Eqs. (34) and (36) show that there is no need to compute
kernel using eigenfunctions but a simple inner product suf-
ﬁces for kernel computation. This is the beauty of kernel
methods which are simple to compute.

Deﬁnition 14 (Input Space and Feature Space (Sch¨olkopf
et al., 1999b)). The space in which data X exist is called
the input space, also known as the original space. This
space is denoted by X and is usually an Rd Euclidean
space. The RKHS to which the data have been pulled is
called the feature space. Data can be pulled from the input
to feature space using kernels.

Remark 4 (Kernel is a Measure of Similarity). Inner prod-
uct is a measure of similarity in terms of angles of vectors
or in terms of location of points with respect to origin. Ac-
cording to Eq. (34), kernel can be seen as inner product
between feature maps of points; hence, kernel is a measure
of similarity between points and this similarity is computed
in the feature space rather than input space.

Pulling data to the feature space is performed using kernels
which is the inner product of points in RKHS according
to Eq. (34). Hence, the relative similarity (inner product)
of pulled data points is known by the kernel. However,
in most of kernels, we cannot ﬁnd an explicit expression
for the pulled data points. Therefore, the exact location of
pulled data points to RKHS is not necessarily known but
the relative similarity of pulled points, which is the ker-
nel, is known. An exceptional kernel is the linear kernel
in which we have φ(x) = x. Figure 1 illustrates what we
mean by not knowing the explicit location of pulled points
to RKHS.

8

Figure 1. Pulling data from the input space to the feature space (RKHS). The explicit locations of pulled points are not necessarily known
but the relative similarity (inner product) of pulled data points is known in the feature space.

4. Characteristics of Kernels
In this section, we review some of the characteristics of ker-
nels including the symmetry and positive semi-deﬁniteness
properties of Mercer kernel (recall Deﬁnition 1).

Lemma 1 (Symmetry of Kernel). A square Mercer kernel
matrix is symmetric, so we have:

(cid:104)f, g(cid:105)k = (cid:104)g, f (cid:105)k,
K ∈ Sn,

or

i.e., k(x, y) = k(y, x).

(37)

(38)

Proof.

k(x, y)

(34)

= (cid:10)φ(x), φ(y)(cid:11)

k = φ(x)(cid:62)φ(y)

(a)
= φ(y)(cid:62)φ(x) = k(y, x),

where (a) is because φ(x)(cid:62)φ(y) and φ(y)(cid:62)φ(x) are
scalars and are equivalent according to the deﬁnition of dot
product between vectors. Q.E.D.

Lemma 2 (Zero Kernel). We have:

(cid:104)f, f (cid:105)k = 0 iff

f = 0.

(39)

Proof.

0 ≤ f 2(x)

(9)
= (cid:104)f, kx(cid:105)k (cid:104)f, kx(cid:105)k

(a)
≤ (cid:107)f (cid:107)k(cid:107)kx(cid:107)k (cid:107)f (cid:107)k(cid:107)kx(cid:107)k = (cid:107)f (cid:107)2

k (cid:107)kx(cid:107)2
k

(b)
= 0,

where (a) is because Cauchy-Schwarz inequality and (b) is
because we had assumed (cid:104)f, f (cid:105)k = (cid:107)f (cid:107)k = 0. Hence:

0 ≤ f 2(x) = 0 =⇒ f (x) = 0.

Lemma 3 (Positive Semi-deﬁniteness of Kernel). The
Mercer kernel matrix is positive semi-deﬁnite:

K ∈ Sn
+,

i.e., K (cid:23) 0.

(40)

Proof. Let v(i) denote the i-th element of vector v.

v(cid:62)Kv

(1)
=

(7)
=

=

=

n
(cid:88)

n
(cid:88)

i=1
n
(cid:88)

j=1
n
(cid:88)

i=1
n
(cid:88)

j=1
n
(cid:88)

(cid:68)

j=1

i=1
(cid:68) n
(cid:88)

v(i) v(j) k(x(i), x(j))

v(i) v(j)

φ(cid:0)x(i)(cid:1), φ(cid:0)x(j)(cid:1)(cid:69)
(cid:68)

k

v(i) φ(cid:0)x(i)(cid:1), v(j) φ(cid:0)x(j)(cid:1)(cid:69)

k

v(i) φ(cid:0)x(i)(cid:1),

n
(cid:88)

j=1

v(j) φ(cid:0)x(j)(cid:1)(cid:69)

k

v(i) φ(cid:0)x(i)(cid:1)(cid:13)
2
(cid:13)
(cid:13)
k

≥ 0,

∀v ∈ Rn.

i=1
n
(cid:88)

i=1

=

(cid:13)
(cid:13)
(cid:13)

Hence, according to the deﬁnition of positive semi-
deﬁniteness (Bhatia, 2009), we have K (cid:23) 0. Q.E.D.

5. Well-known Kernel Functions
5.1. Frequently Used Kernels
There exist many different kernel functions which are
widely used in machine learning (Rojo- ´Alvarez et al.,
2018).
In the following, we list some of the most well-
known kernels.
– Linear Kernel:
Linear kernel is the simplest kernel which is the inner prod-
uct of points:

k(x, y) := x(cid:62)y.

(41)

Comparing this with Eq. (34) shows that in linear kernel we
have φ(x) = x. Hence, in this kernel, the feature map is
explicitly known. Note that φ(x) = x shows that data are
not pulled to any other space in linear kernel but in the input
space, the inner products of points are calculated to obtain
the feature space. Moreover, recall Remark 9 which states
that, depending on the kernelization approach, using lin-

ear kernel may or may not be equivalent to non-kernelized
method.
– Radial Basis Function (RBF) or Gaussian Kernel:
RBF kernel has a scaled Gaussian (or normal) distribution
where the normalization factor of distribution is usually ig-
nored. Hence, it is also called the Gaussian kernel. The
RBF kernel is formulated as:

k(x, y) := exp(−γ (cid:107)x − y(cid:107)2

2) = exp(−

(cid:107)x − y(cid:107)2
2
σ2

),

(42)

where γ := 1/σ2 and σ2 is the variance of kernel. A
proper value for this parameter is γ = 1/d where d is the
dimensionality of data. Note that RBF kernel has also been
widely used in RBF networks (Orr, 1996) and kernel den-
sity estimation (Scott, 1992).
– Laplacian Kernel:
The Laplacian kernel, also called the Laplace kernel, is
similar to the RBF kernel but with (cid:96)1 norm rather than
squared (cid:96)2 norm. The Laplacian kernel is:

k(x, y) := exp(−γ (cid:107)x − y(cid:107)1) = exp(−

(cid:107)x − y(cid:107)1
σ2

),

(43)

where (cid:107)x − y(cid:107)1 is also called the Manhattan distance. A
proper value for this parameter is γ = 1/d where d is the
dimensionality of data. In some speciﬁc ﬁelds of science,
the Laplacian kernel has been found to perform better than
Gaussian kernel (Rupp, 2015). This makes sense because
of betting on sparsity principal (Hastie et al., 2009) since (cid:96)1
norm makes algorithm sparse. Note that (cid:96)2 norm in RBF
kernel is also more sensitive to noise; however, the com-
putation and derivative of (cid:96)1 norm is more difﬁcult than (cid:96)2
norm.
– Sigmoid Kernel:
Sigmoid kernel is a hyperbolic tangent function applied on
inner product of points. It is formulated as:

k(x, y) := tanh(γx(cid:62)y + c),

(44)

where γ > 0 is the slope and c is the intercept. Some proper
values for these parameters are γ = 1/d and c = 1 where
d is the dimensionality of data. Note that the hyperbolic
tangent function is also used widely for activation functions
in neural networks (Goodfellow et al., 2016).
– Polynomial Kernel:
Polynomial kernel applies a polynomial function with de-
gree δ (a positive integer) on inner product of points:

k(x, y) := (γx(cid:62)y + c)d,

(45)

9

– Cosine Kernel:
According to Remark 4, kernel is a measure of similarity
and computes the inner product between points in the fea-
ture space. Cosine kernel computes the similarity between
points. It is obtained from the formula of cosine and inner
product:

k(x, y) := cos(x, y) =

x(cid:62)y
(cid:107)x(cid:107)2 (cid:107)y(cid:107)2

.

(46)

The normalization in the denominator projects the points
onto a unit hyper-sphere so that the inner product measures
the similarity of their angles regardless of their lengths.
Note that angle-based measures such as cosine are found
to work better for face recognition compared to Euclidean
distances (Perlibakas, 2004).
– Chi-squared Kernel:
the d-
Assume x(j) denotes the j-th dimension of
dimensional point x. The Chi-squared (χ2) kernel is
(Zhang et al., 2007):

k(x, y) := exp

(cid:16)
−γ

d
(cid:88)

j=1

(cid:0)x(j) − y(j)(cid:1)2
x(j) + y(j)

(cid:17)

,

(47)

where γ > 0 is a parameter (a proper value is γ = 1).
Note that the summation term inside exponential (without
the minus) is the Chi-squared distance which is related to
the Chi-squared test in statistics.

5.2. Kernel Construction from Distance Metric
Consider d2
tance between xi and xj. We have:

ij = ||xi − xj||2

2 as the squared Euclidean dis-

ij = ||xi − xj||2
d2
i xi − x(cid:62)
i xi − 2x(cid:62)

= x(cid:62)
= x(cid:62)

2 = (xi − xj)(cid:62)(xi − xj)
j xi + x(cid:62)
i xj − x(cid:62)
i xj + x(cid:62)
j xj = Gii − 2Gij + Gjj,

j xj

where Rn×n (cid:51) G := X (cid:62)X is the linear Gram matrix. If
Rn (cid:51) g := [g1, . . . , gn] = [G11, . . . , Gnn] = diag(G),
we have:

d2
ij = gi − 2Gij + gj,
D = g1(cid:62) − 2G + 1g(cid:62) = 1g(cid:62) − 2G + g1(cid:62),

where 1 is the vector of ones and D is the distance matrix
with squared Euclidean distance (d2
ij as its elements). Let
H denote the centering matrix:

Rn×n (cid:51) H := I −

1
n

1n1(cid:62)
n ,

(48)

where γ > 0 is the slope and c is the intercept. Some proper
values for these parameters are γ = 1/d and c = 1 where
d is the dimensionality of data.

and I is the identity matrix, 1n := [1, . . . , 1](cid:62) ∈ Rn and
n ∈ Rn×n. Refer to (Ghojogh & Crowley,
1n×n := 1n1(cid:62)
2019, Appendix A) for more details about the centering

matrix. We double-center the matrix D as follows (Old-
ford, 2018):

where (a) is because H and D are symmetric matrices.
Moreover, the kernel is positive semi-deﬁnite because:

10

1
n

11(cid:62))

K = −

1
2

HDH = Φ(X)(cid:62)Φ(X)

=⇒ v(cid:62)Kv = v(cid:62)Φ(X)(cid:62)Φ(X)v

= (cid:107)Φ(X)v(cid:107)2

2 ≥ 0,

∀v ∈ Rn.

HDH = (I −

= (I −

1
n
1
n
= (cid:2) (I −
(cid:124)

+ (I −

= −2(I −

+ (I −

11(cid:62))D(I −

1
n
11(cid:62))(1g(cid:62) − 2G + g1(cid:62))(I −

11(cid:62))

1
n

11(cid:62))G

g(cid:62) − 2(I −

11(cid:62))g1(cid:62)(cid:3)(I −

1
11(cid:62))1
n
(cid:123)(cid:122)
(cid:125)
= 0
1
n
1
1
n
n
1
1
11(cid:62))g 1(cid:62)(I −
n
n
(cid:123)(cid:122)
(cid:124)
= 0
1
n

11(cid:62))G(I −

11(cid:62))G(I −

1
n

11(cid:62))

1
n
11(cid:62))

11(cid:62))
(cid:125)

= −2(I −

11(cid:62)) = −2 HGH.

∴

HGH = HX (cid:62)XH = −

1
2

HDH.

(49)

n 11(cid:62))1 = 0 and 1(cid:62)(I − 1

n 11(cid:62)) = 0
Note that (I − 1
because removing the row mean of 1 and column mean of
of 1(cid:62) results in the zero vectors, respectively.
If data X are already centered, i.e., the mean has been re-
moved (X ← XH), Eq. (49) becomes:

X (cid:62)X = −

1
2

HDH.

(50)

According to the kernel trick, Eq. (104), we can write a
general kernel matrix rather than the linear Gram matrix in
Eq. (50), to have (Cox & Cox, 2008):

Rn×n (cid:51) K = Φ(X)(cid:62)Φ(X) = −

1
2

HDH.

(51)

This kernel is double-centered because of HDH. It is also
noteworthy that Eq. (51) can be used for unifying the spec-
tral dimensionality reduction methods as special cases of
kernel principal component analysis with different kernels.
See (Ham et al., 2004; Bengio et al., 2004) and (Strange &
Zwiggelaar, 2014, Table 2.1) for more details.

Lemma 4 (Distance-based Kernel is a Mercer Kernel). The
kernel constructed from a valid distance metric, i.e. Eq.
(51), is a Mercer kernel.

Proof. The kernel is symmetric because:

Hence, according to Deﬁnition 1, this kernel is a Mercer
kernel. Q.E.D.

Remark 5 (Kernel Construction from Metric). One can
use any valid distance metric, satisfying the following prop-
erties:

1. non-negativity: D(x, y) ≥ 0,
2. equal points: D(x, y) = 0 ⇐⇒ x = y,
3. symmetry: D(x, y) = D(y, x),
4. triangular inequality: D(x, y) ≤ D(x, z)+D(z, y),
to calculate elements of distance matrix D in Eq. (50). It
is important that the used distance matrix should be a valid
distance matrix. Using various distance metrics in Eq. (50)
results in various useful kernels.

Some examples are the geodesic kernel and Structural Sim-
ilarity Index (SSIM) kernel, used in Isomap (Tenenbaum
et al., 2000) and image structure subspace learning (Gho-
jogh et al., 2019c), respectively. The geodesic kernel is de-
ﬁned as (Tenenbaum et al., 2000; Ghojogh et al., 2020b):

K = −

1
2

HD(g)H,

(52)

where the approximation of geodesic distances using piece-
wise Euclidean distances is used in calculating the geodesic
distance matrix D(g). The SSIM kernel is deﬁned as (Gho-
jogh et al., 2019c):

K = −

1
2

HD(s)H,

(53)

where the distance matrix D(s) is calculated using the
SSIM distance (Brunet et al., 2011).

5.3. Important Classes of Kernels
In the following, we introduce some of the important
classes of kernels which are widely used in statistics and
machine learning. A good survey on the classes of kernels
is (Genton, 2001).

5.3.1. BOUNDED KERNELS
Deﬁnition 15 (Bounded Kernel). A kernel function k is
bounded if:

sup
x,y∈X

k(x, y) < ∞,

(54)

K(cid:62) = −

1
2

H (cid:62)D(cid:62)H (cid:62) (a)

= −

1
2

HDH = K,

where X is the input space. Likewise, the kernel matrix K
is bounded if supx,y∈X K(x, y) < ∞.

5.3.2. INTEGRALLY POSITIVE DEFINITE KERNELS
Deﬁnition 16 (Integrally Positive Deﬁnite Kernel). A ker-
nel matrix K is integrally positive deﬁnite ((cid:82) p.d.) on Ω×Ω
if:

(cid:90)

(cid:90)

Ω

Ω

K(x, y)f (x)f (y) ≥ 0,

∀f ∈ L2(Ω).

(55)

A kernel matrix K is integrally strictly positive deﬁnite ((cid:82)
s.p.d.) on Ω × Ω if:

(cid:90)

(cid:90)

Ω

Ω

K(x, y)f (x)f (y) > 0,

∀f ∈ L2(Ω).

(56)

5.3.3. UNIVERSAL KERNELS
Deﬁnition 17 (Universal Kernel (Steinwart, 2001, Deﬁni-
tion 4), (Steinwart, 2002, Deﬁnition 2)). Let C(X ) denote
the space of all continuous functions on space X . A con-
tinuous kernel k on a compact metric space X is called
universal if the RKHS H, with kernel function k, is dense
in C(X ). In other words, for every function g ∈ C(X )
and all (cid:15) > 0, there exists a function f ∈ H such that
(cid:107)f − g(cid:107)∞ ≤ (cid:15).
Remark 6. We can approximate any function, including
continuous functions and functions which can be approxi-
mated by continuous functions, using a universal kernel.

Lemma 5 ((Steinwart, 2001, Corollary 10)). Consider a
function f : (−r, r) → R where 0 < r ≤ ∞ and f ∈
C∞ (C∞ denotes the differentiable space for all degrees
of differentiation). Let X := {x ∈ Rd | (cid:107)x(cid:107)2 <
r}. If
the function f can be expanded by Taylor expansion in 0
as:

√

f (x) =

∞
(cid:88)

j=0

aj xj,

∀x ∈ (−r, r),

(57)

and aj > 0 for all j ≥ 0, then k(x, y) = f ((cid:104)x, y(cid:105)) is a
universal kernel on every compact subset of X .

Proof. For proof, see (Steinwart, 2001, proof of Corollary
10). Note that the Stone-Weierstrass theorem (De Branges,
1959) is used for the proof of this lemma.

An example for universal kernel is RBF kernel (Steinwart,
2001, Example 1) because its Taylor series expansion is:

exp(−γr) ≈ 1 − γr +

γ2
2

r2 −

γ3
6

r3 + . . . ,

where r := (cid:107)x − y(cid:107)2
2. Considering Eq. (34) and notic-
ing that this Taylor series expansion has inﬁnite number
of terms, we see that the RKHS for RBF kernel is inﬁnite
dimensional because φ(x), although cannot be calculated
explicitely for this kernel, will have inﬁnite dimensions.
Another example for universal kernel is the SSIM kernel

11

(Ghojogh et al., 2020d), denoted by Ks, whose Taylor se-
ries expansion is (Ghojogh et al., 2019c):

Ks ≈ −

5
16

−

15
16

r +

5
16

r2 −

1
16

r3 + . . . ,

where r is the squared SSIM distance (Brunet et al., 2011)
between images. Note that polynomial kernels are not uni-
versal. Universal kernels have been widely used for kernel
SVM. More detailed discussion and proofs for use of uni-
versal kernels in kernel SVM can be found in (Steinwart &
Christmann, 2008).

Lemma 6 ((Borgwardt et al., 2006), (Song, 2008, Theorem
10)). A kernel is universal if for arbitrary sets of distinct
points, it induces strictly positive deﬁnite kernel matrices
(Borgwardt et al., 2006; Song, 2008). Conversely, if a ker-
nel matrix can be written as K = K(cid:48) + (cid:15)I where K(cid:48) (cid:23) 0,
(cid:15) > 0, and I is the identity matrix, the kernel function cor-
responding to K is universal (Pan et al., 2008).

5.3.4. STATIONARY KERNELS
Deﬁnition 18 (Stationary Kernel (Genton, 2001; Noack &
Sethian, 2021)). A kernel k is stationary if it is a positive
deﬁnite function of the form:

k(x, y) = k((cid:107)x − y(cid:107)),

(58)

where (cid:107).(cid:107) is some norm deﬁned on the input space.

An example for stationary kernel is the RBF kernel deﬁned
in Eq. (42) which has the form of Eq. (58). Stationary
kernels are used for Gaussian processes (Noack & Sethian,
2021).

5.3.5. CHARACTERISTIC KERNELS
The characteristic kernels, which are widely used for dis-
tribution embedding in the Hilbert space, will be deﬁned
and explained in Section 9.4. Examples for characteristic
kernels are RBF and Laplacian kernels. Polynomial ker-
nels. however, are not characteristic. Note that the relation
between universal kernels, characteristic kernels, and inte-
grally strictly positive deﬁnite kernels has been studied in
(Sriperumbudur et al., 2011).

6. Kernel Centering and Normalization
6.1. Kernel Centering
In some cases, there is a need to center the pulled data in the
feature space. For this, the kernel matrix should be centered
in a way that the mean of pulled dataset becomes zero. Note
that this will restrict the place of pulled points in the feature
space further (see Fig. 2); however, because of different
possible rotations of pulled points around origin, the exact
positions of pulled points are still unknown.
For kernel centering, one should follow the following
theory, which is based on (Sch¨olkopf et al., 1997a) and

12

feature space are centered. Also, double-centered kernel
has zero row-wise and column-wise mean (so its row and
column summations are zero). Therefore, after this kernel
centering, we will have:

1
n

n
(cid:88)

i=1

˘φ(xi) = 0,

n
(cid:88)

n
(cid:88)

i=1

j=1

˘K(i, j) = 0.

(62)

(63)

6.1.2. CENTERING THE KERNEL BETWEEN TRAINING

AND OUT-OF-SAMPLE DATA

Now, consider the kernel matrix between the training
data and the out-of-sample data Rn×nt (cid:51) Kt
:=
is R (cid:51)
Φ(X)(cid:62)Φ(X t).
Kt(i, j) = φ(xi)(cid:62)φ(xt,j). We want to center the pulled
training data in the feature space, i.e., Eq. (59). Moreover,
the out-of-sample data should be centered using the mean
of training (and not out-of-sample) data:

whose (i, j)-th element

˘φ(xt,i) := φ(xt,i) −

1
n

n
(cid:88)

k=1

φ(xk).

(64)

If we center the pulled training and out-of-sample data, the
(i, j)-th element of kernel matrix becomes:

˘Kt(i, j) := ˘φ(xi)(cid:62) ˘φ(xt,j)
1
n

= (cid:0)φ(xi) −

n
(cid:88)

(a)

k1=1

φ(xk1 )(cid:1)(cid:62)(cid:0)φ(xt,j) −

1
n

n
(cid:88)

k2=1

φ(xk2)(cid:1)

= φ(xi)(cid:62)φ(xt,j) −

1
n

n
(cid:88)

k1=1

φ(xk1)(cid:62)φ(xt,j)

−

1
n

n
(cid:88)

k2=1

φ(xi)(cid:62)φ(xk2 ) +

1
n2

n
(cid:88)

n
(cid:88)

k1=1

k2=1

φ(xk1)(cid:62)φ(xk2),

where (a) is because of Eqs.
(59) and (64). Therefore,
the double-centered kernel matrix over training and out-of-
sample data is:

1
n

1n×nKt −

Rn×nt (cid:51) ˘Kt = Kt −
1
n2 1n×nK1n×nt,
:= 1n1(cid:62)

+

1
n

K1n×nt

(65)

nt and Rnt (cid:51) 1nt

where Rn×nt (cid:51) 1n×nt
:=
[1, . . . , 1](cid:62). The Eq. (65) is the kernel matrix when the
pulled training data in the feature space are centered and
the pulled out-of-sample data are centered using the mean
of pulled training data.
If we have one out-of-sample xt, the Eq. (65) becomes:

Figure 2. Centered pulled data the feature space (RKHS). This
happens after kernel centering where the mean of cloud of pulled
data becomes zero in RKHS. Even by kernel centering, the ex-
plicit locations of pulled points are not necessarily known because
of not knowing the rotation of pulled data in that space.

(Sch¨olkopf et al., 1998, Appendix A). An example of use
of kernel centering in machine learning is kernel principal
component analysis (see (Ghojogh & Crowley, 2019) for
more details).

have

some

training

data X

6.1.1. CENTERING THE KERNEL OF TRAINING DATA
=
Assume we
[x1, . . . , xn] ∈ Rd×n and some out-of-sample data
X t = [xt,1, . . . , xt,nt] ∈ Rd×nt. Consider the kernel
matrix for the training data Rn×n (cid:51) K := Φ(X)(cid:62)Φ(X),
whose (i, j)-th element is R (cid:51) K(i, j) = φ(xi)(cid:62)φ(xj).
We want to center the pulled training data in the feature
space:

˘φ(xi) := φ(xi) −

1
n

n
(cid:88)

k=1

φ(xk).

(59)

If we center the pulled training data, the (i, j)-th element
of kernel matrix becomes:

˘K(i, j) := ˘φ(xi)(cid:62) ˘φ(xj)

(59)

= (cid:0)φ(xi) −

1
n

n
(cid:88)

k1=1

φ(xk1 )(cid:1)(cid:62)(cid:0)φ(xj) −

(60)

φ(xk2)(cid:1)

1
n

n
(cid:88)

k2=1

= φ(xi)(cid:62)φ(xj) −

1
n

n
(cid:88)

k1=1

φ(xk1 )(cid:62)φ(xj)

−

1
n

n
(cid:88)

k2=1

φ(xi)(cid:62)φ(xk2 ) +

1
n2

n
(cid:88)

n
(cid:88)

k1=1

k2=1

φ(xk1)(cid:62)φ(xk2).

Writing this in the matrix form gives:

Rn×n (cid:51) ˘K = K −

1
n

1n×nK −

1
n

K1n×n

+

1
n2 1n×nK1n×n = HKH,

(61)

where H is the centering matrix (see Eq. (48)). The Eq.
(61) is called the double-centered kernel. This equation
is the kernel matrix when the pulled training data in the

Rn (cid:51) ˘kt = kt −

1
n

1n×nkt −

1
n

K1n +

1
n2 1n×nK1n,
(66)

where:

(which is already normalized) is kernelized as:

13

Rn (cid:51) kt = kt(X, xt) := Φ(X)(cid:62)φ(xt)

(67)

= [φ(x1)(cid:62)φ(xt), . . . , φ(xn)(cid:62)φ(xt)](cid:62),

Rn (cid:51) ˘kt = ˘kt(X, xt) := ˘Φ(X)(cid:62) ˘φ(xt),

(68)

= [ ˘φ(x1)(cid:62) ˘φ(xt), . . . , ˘φ(xn)(cid:62) ˘φ(xt)](cid:62),

where ˘Φ(X) and ˘φ(xt) are according to Eqs.
(64), respectively.
Note that Eq. (61) or (65) can be restated as the following
lemma.

(59) and

Lemma 7 (Kernel Centering (Bengio et al., 2003b;c)). The
pulled data to the feature space can be centered by kernel
centering. The kernel matrix K(x, y) is centered as:

˘K(x, y) = (cid:0)φ(x) − Ex[φ(x)](cid:1)(cid:62)(cid:0)φ(x) − Ex[φ(x)](cid:1)
= K(x, y) − Ex[K(x, y)] − Ey[K(x, y)]

+ Ex[Ey[K(x, y)]].

(69)

Proof. The explained derivations for Eqs.
and deﬁnition of expectation complete the proof.

(59) and (64)

(69), Ex[K(x, y)], Ey[K(x, y)], and
Note that in Eq.
Ex[Ey[K(x, y)]] are average of rows, average of columns,
and total average of rows and columns of the kernel matrix,
respectively.

6.2. Kernel Normalization
(34), kernel value can be large if the
According to Eq.
pulled vectors to the feature map have large length. Hence,
in practical computations and optimization, it is sometimes
required to normalize the kernel matrix.

Lemma 8 (Cosine Normalization of Kernel (Rennie, 2005;
Ah-Pine, 2010)). The kernel matrix K ∈ Rn×n can be
normalized as:

K(i, j) ←

K(i, j)
(cid:112)K(i, i)K(j, j)

,

∀i, j ∈ {1, . . . , n}.

(70)

Proof. Cosine normalizes points onto a unit hyper-sphere
and then computes the similarity of points using inner prod-
uct. Cosine is computed by Eq. (46) and according to the
relation of norm and inner product, it is:

cos(xi, xj) =

=

x(cid:62)
i xj
(cid:107)xi(cid:107)2 (cid:107)xj(cid:107)2
x(cid:62)
i xj
i xi x(cid:62)
x(cid:62)

(cid:113)

j xj

x(cid:62)
(cid:112)(cid:107)xi(cid:107)2

i xj
2 (cid:107)xj(cid:107)2
2

=

.

According to Remark 4, kernel is also a measure of simi-
larity. Using kernel trick, Eq. (103), the cosine similarity

K(i, j) =

φ(xi)(cid:62)φ(xj)
(cid:112)φ(xi)(cid:62)φ(xi) φ(xj)(cid:62)φ(xj)

(103)
=

K(i, j)
(cid:112)K(i, i)K(j, j)

.

Q.E.D.

Deﬁnition 19 (generalized mean with exponent t (Ah-Pine,
2010)). The generalized mean with exponent t as:

mt(a1, . . . , an) :=

(cid:16) 1
p

p
(cid:88)

i=1

(cid:17) 1

t

.

at
i

(71)

The generalized mean becomes the harmonic, geometric,
and arithmetic mean for t = −1, t → 0, and t = 1, respec-
tively.

Deﬁnition 20 (Generalized Kernel Normalization of order
t (Ah-Pine, 2010)). The generalized kernel normalization
of order t > 0 normalizes the kernel K ∈ Rn×n as:

K(i, j) ←

K(i, j)
(cid:0)K(i, i), K(j, j)(cid:1) ,

mt

∀i, j ∈ {1, . . . , n}.

(72)

Both cosine normalization and generalized normalization
make the kernel of every point with itself one.
In other
words, after normalization, we have:

k(x, x) = 1,

∀x ∈ X .

(73)

As the most similar point to a point is itself, the values of a
normalized kernel will be less than or equal to one. In other
words, after normalization, we have:

k(xi, xj) ≤ 1,

∀i, j ∈ {1, . . . , n}.

(74)

This helps the values not explode to large values in algo-
rithms, especially in the iterative algorithms (e.g., algo-
rithms which use gradient descent for optimization).

7. Eigenfunctions
7.1. Inner Product in Hilbert Space
Lemma 9 (Inner Product in Hilbert Space). If the domain
of functions in a Hilbert space H is [a, b], the inner product
of two functions in the Hilbert space is calculated as:

(cid:104)f (x), g(x)(cid:105)H =

(cid:90) b

a

f (x) g∗(x)dx

(cid:90) b

(37)
=

a

f ∗(x) g(x)dx,

(75)

where g∗ is the complex conjugate of function g.
functions are real,
(cid:82) b
a f (x) g(x)dx.

the inner product

If
is simpliﬁed to

Proof. If we discretize the domain [a, b], for example by
sampling, with step ∆x, the function values become vec-
tors as f (x) = [f (x1), f (x2), . . . , f (xn)](cid:62) and g(x) =
[g(x1), g(x2), . . . , g(xn)](cid:62). According to the inner prod-
uct of two vectors, we have:

then the function f is an eigenfunction for the operator O
and the constant λ is the corresponding eigenvalue. Note
that the form of eigenfunction problem is:

Operator (function f ) = constant × function f.

(79)

14

(cid:104)f (x), g(x)(cid:105) = gH f =

n
(cid:88)

i=1

f (xi)g(xi),

where gH denotes the conjugate transpose of g (it is trans-
pose if functions are real). Multiplying the sides of this
equation by the setp ∆x gives:

(cid:104)f (x), g(x)(cid:105)∆x = gH f =

n
(cid:88)

i=1

f (xi)g(xi)∆x,

which is a Riemann sum. This is the Riemann approxima-
tion of the Eq. (75). This approximation gets more accurate
by ∆x → 0 or n → ∞. Hence, that equation is a valid in-
ner product in the Hilbert space. Q.E.D.

Remark 7 (Interpretation of Inner Product of Functions
in Hilbert Space). The inner product of two functions, i.e.
(75), measures how similar two functions are. The
Eq.
more similar they are in their domain [a, b], the larger in-
ner product they have. Note that this similarity is more
about the pattern (or changes) of functions and not the ex-
act value of functions. If the pattern of functions is very
similar, they will have a large inner product.

Corollary 2 (Weighted Inner Product in Hilbert Space
(Williams & Seeger, 2000), (Bengio et al., 2003c, Section
2)). The Eq. (75) is the inner product with uniform weight-
ing. With density function p(x), one can weight the inner
product in the Hilbert space as (assuming the functions are
real):

(cid:104)f (x), g(x)(cid:105)H =

(cid:90) b

a

f (x) g(x) p(x) dx.

(76)

7.2. Eigenfunctions
Recall eigenvalue problem for a matrix A (Ghojogh et al.,
2019a):

A φi = λi φi,

∀i ∈ {1, . . . , d},

(77)

where φi and λi are the i-th eigenvector and eigenvalue of
A, respectively. In the following, we introduce the Eigen-
function problem which has a similar form but for an oper-
ator rather than a matrix.

Deﬁnition 21 (Eigenfunction (Kusse & Westwig, 2006,
Chapter 11.2)). Consider a linear operator O which can
be applied on a function f . If applying this operator on
the function results in a multiplication of function to a con-
stant:

Of = λf,

(78)

Some examples of operator are derivative, kernel function,
etc. For example, eλx is an eigenfunction of derivative be-
cause d
dx eλx = λeλx. Note that eigenfunctions have appli-
cation in many ﬁelds of science including machine learning
(Bengio et al., 2003c) and quantum mechanics (Reed & Si-
mon, 1972).
Recall that in eigenvalue problem, the eigenvectors show
the most important or informative directions of matrix and
the corresponding eigenvalue shows the amount of impor-
tance (Ghojogh et al., 2019a). Likewise, in eigenfunc-
tion problem of an operator, the eigenfunction is the most
important function of the operator and the corresponding
eigenvalue shows the amount of this importance. This con-
nection between eigenfunction and eigenvalue problems is
proved in the following theorem.
Theorem 3 (Connection of Eigenfunction and Eigenvalue
Problems). If we assume that the operator and the func-
tion are a matrix and a vector, eigenfunction problem is
converted to an eigenvalue problem where the vector is the
eigenvector of the matrix.

Proof. Consider any function space such as a Hilbert
space. Let {ej}n
j=1 be the bases (basis functions) of this
function space where n may be inﬁnite. The function f in
this space can be represented as a linear combination bases:

f (x) =

n
(cid:88)

j=1

αj ej(x).

(80)

An example of this linear combination is Eq. (6) in RKHS
where the bases are kernels. Consider the operator O which
can be applied on the functions in this function space. Ap-
plying this operator on Eq. (80) gives:

Of (x) = O

n
(cid:88)

j=1

αj ej(x)

(a)
=

n
(cid:88)

j=1

αj Oej(x),

(81)

where (a) is because the operator O is a linear operator
according to Deﬁnition 21. Also, we have:

Of (x)

(78)
= λf (x)

(a)
=

n
(cid:88)

j=1

λ αj ej(x),

(82)

where (a) is because λ is a scalar.
On the other hand, the output function from applying the
operator on a function can also be written as a linear com-
bination of the bases:

Of (x) =

n
(cid:88)

j=1

βj ej(x).

(83)

From Eqs. (81) and (83), we have:

n
(cid:88)

j=1

αj Oej(x) =

n
(cid:88)

j=1

βj ej(x).

(84)

In parentheses, consider an n × n matrix A whose (i, j)-th
element is the inner product of ei and Oej:

A(i, j) := (cid:104)ei, Oej(cid:105)k

(cid:90)

(75)
=

e∗
i (x) Oej(x)dx,

(85)

where integral is over the domain of functions in the func-
tion space.
Using Eq. (75), we take the inner product of sides of Eq.
(84) with an arbitrary basis function ei:

(cid:90)

αj

n
(cid:88)

j=1

e∗
i (x) Oej(x) dx =

(cid:90)

βj

n
(cid:88)

j=1

e∗
i (x) ej(x) dx.

According to Eq. (85), this equation is simpliﬁed to:

n
(cid:88)

j=1

αj A(i, j) =

(cid:90)

βj

n
(cid:88)

j=1

e∗
i (x) ej(x) dx

(a)
= βi,

(86)

which is true for ∀i ∈ {1, . . . n} and (a) is because the
bases are orthonormal, so:

(cid:104)ei, ej(cid:105)k

(cid:90)

(75)
=

e∗
i (x) ej(x) dx =

(cid:26) 1

if i = j,
0 Otherwise.

The Eq. (86) can be written in matrix form:

Aα = β,

(87)

where α := [α1, . . . , αn](cid:62) and β := [β1, . . . , βn](cid:62).
From Eqs. (82) and (83), we have:

15

7.3. Use of Eigenfunctions for Spectral Embedding
Consider a Hilbert space H of functions with the inner
product deﬁned by Eq. (76). Let the data in the input space
be X = {xi ∈ Rd}n
i=1. In this space, we can consider an
operator for the kernel function Kp as (Williams & Seeger,
2000), (Bengio et al., 2003a, Section 3):

(cid:90)

(Kpf )(x) :=

k(x, y) f (y) p(y) dy,

(89)

where f ∈ H and the density function p(y) can be ap-
proximated empirically. A discrete approximation of this
operator is (Williams & Seeger, 2000):

(Kp,nf )(x) :=

1
n

n
(cid:88)

i=1

k(x, xi) f (xi),

(90)

(89) if n → ∞. Note that this
which converges to Eq.
equation is also mentioned in (Bengio et al., 2003c, Section
2), (Bengio et al., 2004, Section 4), (Bengio et al., 2006,
Section 3.2).
Lemma 10 (Relation of Eigenvalues of Eigenvalue Prob-
lem and Eigenfunction Problem for Kernel (Bengio et al.,
2003a, Proposition 1), (Bengio et al., 2003c, Theorem 1),
(Bengio et al., 2004, Section 4)). Assume λk denotes the k-
th eigenvalue for eigenfunction decomposition of the oper-
ator Kp and δk denotes the k-th eigenvalue for eigenvalue
problem of the matrix K ∈ Rn×n. We have:

δk = n λk.

(91)

Proof. This proof gets help from (Bengio et al., 2003b,
proof of Proposition 3). According to Eq. (78), the eigen-
function problems for the operators Kp and Kp,n (discrete
version) are:

(Kpfk)(x) = λkfk(x),
(Kp,nfk)(x) = λkfk(x),

∀k ∈ {1, . . . , n},

∀k ∈ {1, . . . , n},

(92)

where fk(.) is the k-th eigenfunction and λk is the corre-
sponding eigenvalue. Consider the kernel matrix deﬁned
by Deﬁnition 2. The eigenvalue problem for the kernel ma-
trix is (Ghojogh et al., 2019a):

n
(cid:88)

j=1

λ αj ej(x) =

n
(cid:88)

j=1

βj ej(x) =⇒ λ α = β.

(88)

Kvk = δkvk,

∀k ∈ {1, . . . , n},

(93)

Comparing Eqs. (87) and (88) shows:

Aα = λ α,

which is an eigenvalue problem for matrix A with eigen-
vector α and eigenvalue λ (Ghojogh et al., 2019a). Note
that, according to Eq. (80), the information of function f
is in the coefﬁcients αj’s of the basis functions of space.
Therefore, the function is converted to the eigenvector (vec-
tor of coefﬁcients) and the operator O is converted to the
matrix A. Q.E.D.

where vk is the k-th eigenvector and δk is the correspond-
ing eigenvalue. According to Eqs. (90) and (92), we have:

1
n

n
(cid:88)

i=1

k(x, xi) f (xi) = λkfk(x),

∀k ∈ {1, . . . , n}.

When this equation is evaluated only at xi ∈ X , we have
(Bengio et al., 2004, Section 4), (Bengio et al., 2006, Sec-
tion 3.2):

Kfk = λkfk,

1
n
=⇒ Kfk = nλkfk.

∀k ∈ {1, . . . , n},

According to Theorem 3, eigenfunction can be seen as an
eigenvector. If so, we can say:

allowed to re-arrange them. Re-arranging the terms in this
equation gives:

16

Kfk = nλkfk =⇒ Kvk = nλkvk,

(94)

Comparing Eqs. (93) and (94) results in Eq. (91). Q.E.D.

Lemma 11 (Relation of Eigenvalues of Kernel and Covari-
ance in the Feature Space (Sch¨olkopf et al., 1998)). Con-
sider the covariance of pulled data to the feature space:

CH :=

1
n

n
(cid:88)

i=1

˘φ(xi) ˘φ(xi)(cid:62),

(95)

where ˘φ(xi) is the centered pulled data deﬁned by Eq. (64).
which is t × t dimensional where t may be inﬁnite. Assume
ηk denotes the k-th eigenvalue CH and δk denotes the k-th
eigenvalue of centered kernel ˘K. We have:

δk = n ηk.

(96)

Proof. This proof is based on (Sch¨olkopf et al., 1998, Sec-
tion 2). The eigenvalue problem for this covariance matrix
is:

ηk uk = CH uk,

∀k ∈ {1, . . . , n},

where uk is the k-th eigenvector and ηk is its corresponding
eigenvalue (Ghojogh et al., 2019a). Left multiplying this
equation with ˘φ(xj)(cid:62) gives:

ηk

˘φ(xj)(cid:62)uk = ˘φ(xj)(cid:62)CH uk,

∀k ∈ {1, . . . , n}.

(97)

As uk is the eigenvector of the covariance matrix in the
feature space, it lies in the feature space; hence, according
to Lemma 13 which will come later, we can represent it as:

uk =

1
√
δk

n
(cid:88)

(cid:96)=1

v(cid:96)

˘φ(x(cid:96)),

(98)

where pulled data to feature space are assumed to be cen-
tered, v(cid:96)’s are the coefﬁcients in representation, and the
normalization by 1/
δk is because of a normalization used
in (Bengio et al., 2003c, Section 4). Substituting Eq. (98)
and Eq. (95) in Eq. (97) results in:

√

ηk

˘φ(xj)(cid:62)

n
(cid:88)

(cid:96)=1

v(cid:96)

˘φ(x(cid:96))

= ˘φ(xj)(cid:62) 1
n

n
(cid:88)

i=1

˘φ(xi) ˘φ(xi)(cid:62)

n
(cid:88)

(cid:96)=1

v(cid:96)

˘φ(x(cid:96)),

where normalization factors are simpliﬁed from sides. In
the right-hand side, as the summations are ﬁnite, we are

ηk

n
(cid:88)

(cid:96)=1

=

v(cid:96)

˘φ(xj)(cid:62) ˘φ(x(cid:96))

1
n

n
(cid:88)

(cid:96)=1

(cid:16) ˘φ(xj)(cid:62)

v(cid:96)

n
(cid:88)

i=1

˘φ(xi)

(cid:17)
(cid:17)(cid:16) ˘φ(xi)(cid:62) ˘φ(x(cid:96))

.

Considering Eqs. (36) and (60), we can write this equa-
tion in matrix form ηk ˘Kvk = 1
vk where vk :=
n
[v1, . . . , vn](cid:62). As ˘K is positive semi-deﬁnite (see Lemma
3), it is often non-singular. For non-zero eigenvalues, we
can left multiply this equation to ˘K

to have:

˘K

−1

2

n ηk vk = ˘Kvk,

which is the eigenvalue problem for ˘K where v is the
eigenvector and δk = n ηk is the eigenvalue (cf. Eq. (93)).
Q.E.D.

Lemma 12 (Relation of Eigenfunctions and Eigenvectors
for Kernel (Bengio et al., 2003a, Proposition 1), (Bengio
et al., 2003c, Theorem 1)). Consider a training dataset
{xi ∈ Rd}n
i=1 and the eigenvalue problem (93) where
vk ∈ Rn and δk are the k-th eigenvector and eigenvalue
of matrix K ∈ Rn×n. If vki is the i-th element of vector
vk, the eigenfunction for the point x and the i-th training
point xi are:

n
(cid:88)

vki

˘k(xi, x),

√

n
δk
√

i=1
n vki,

fk(x) =

fk(xi) =

(99)

(100)

respectively, where ˘k(xi, x) is the centered kernel.
If x
is a training point, ˘k(xi, x) is the centered kernel over
training data and if x is an out-of-sample point,
then
˘k(xi, x) = ˘kt(xi, x) is between training set and the out-
of-sample point (n.b. kernel centering is explained in Sec-
tion 6.1).

Proof. For proof of Eq.
(99), see (Bengio et al., 2003c,
proof of Theorem 1) or (Williams & Seeger, 2001, Section
1.1). The Eq. (100) is claimed in (Bengio et al., 2003c,
Proposition 1). For proof of this equation, see (Bengio
et al., 2003c, proof of Theorem 1, Eq. 7).

It is noteworthy that Eq. (99) is similar and related to the
Nystr¨om approximation of eigenfunctions of kernel opera-
tor which will be explained in Lemma 16.

Theorem 4 (Embedding from Eigenfunctions of Kernel
Operator (Bengio et al., 2003a, Proposition 1), (Bengio
et al., 2003c, Section 4)). Consider a dimensionality reduc-
tion algorithm which embeds data into a low-dimensional

embedding space. Let the embedding of the point x be
Rp (cid:51) y(x) = [y1(x), . . . , yp(x)](cid:62) where p ≤ n. The
k-th dimension of this embedding is:

yk(x) =

(cid:112)

δk

fk(x)
√
n

=

1
√
δk

n
(cid:88)

i=1

vki

˘k(xi, x),

(101)

where ˘k(xi, x) is the centered training or out-of-sample
kernel depending on whether x is a training or an out-of-
sample point (n.b. kernel centering will be explained in
Section 6.1).

Proof. We can embed data point x by pulling it to the fea-
ture space and centering the pulled dataset to have ˘φ(x)
and then projecting it onto the eigenvector of covariance
matrix in the feature space (Sch¨olkopf et al., 1998, Section
2), (Bengio et al., 2003c, Section 4):

yk(x) = u(cid:62)
k

˘φ(x)

(98)
=

1
√
δk

n
(cid:88)

i=1

vi

˘φ(xi)(cid:62) ˘φ(x)

(60)
=

1
√
δk

n
(cid:88)

i=1

vki

˘k(xi, x).

Q.E.D.

The Theorem 4 has been widely used for out-of-sample
(test data) embedding in many spectral dimensionality re-
duction algorithms (Bengio et al., 2003a).

Corollary 3 (Embedding from Eigenvectors of Kernel Ma-
trix). Consider the eigenvalue problem for the kernel ma-
(93), where vk = [vk1, . . . , vkn](cid:62) and
trix, i.e. Eq.
δk are the k-th eigenvector and eigenvalue of kernel, re-
spectively. According to Eqs.
(100) and (101), we can
compute the embedding of point x, denoted by y(x) =
[y1(x), . . . , yp(x)](cid:62) (where p ≤ n) using the eigenvector
of kernel as:

yk(x) =

(cid:112)

δk

√
(

1
√
n

n)vki =

(cid:112)

δk vki.

(102)

The Eq. (102) is used in several dimensionality reduction
methods such as maximum variance unfolding (or semidef-
inite embedding) (Weinberger et al., 2005; Weinberger &
Saul, 2006b;a). We will introduce this method in Section
9.2.

8. Kernelization Techniques
Linear algorithms cannot properly handle nonlinear pat-
terns of data obviously. When dealing with nonlinear data,
if the algorithm is linear, two solutions exist to have accept-
able performance:

1. Either the linear method should be modiﬁed to be-
come nonlinear or a completely new nonlinear algo-

17

Figure 3. Transforming data to RKHS using kernels to make the
nonlinear pattern of data more linear. For example, here the
classes have become linearly separable (by a linear hyperplane)
after kernelization.

rithm should be proposed to be able to handle nonlin-
ear data. Some examples of this category are nonlin-
ear dimensionality methods such as locally linear em-
bedding (Ghojogh et al., 2020a) and Isomap (Ghojogh
et al., 2020b).

2. Or the nonlinear data should be modiﬁed in a way to
become more linear in pattern. In other words, a trans-
formation should be applied on data so that the pattern
of data becomes roughly linear or easier to process
by the linear algorithm. Some examples of this cate-
gory are kernel versions of linear methods such as ker-
nel Principal Component Analysis (PCA) (Sch¨olkopf
et al., 1997a; 1998; Ghojogh & Crowley, 2019), ker-
nel Fisher Discriminant Analysis (FDA) (Mika et al.,
1999; Ghojogh et al., 2019b), and kernel Support
Vector Machine (SVM) (Boser et al., 1992; Vapnik,
1995).

The second approach is called kernelization in machine
learning which we deﬁne in the following. Figure 3 shows
how kernelization for transforming data can help separate
classes for better classiﬁcation.

Deﬁnition 22 (Kernelization). In machine learning and
data science, kernelization means a slight change in algo-
rithm formulation (without any modiﬁcation in the idea of
algorithm) so that the pulled data to the RKHS, rather than
the raw data, are used as input of algorithm.

Note that kernelization can be useful for enabling linear al-
gorithms to handle nonlinear data better. Nevertheless, it
should be noted that nonlinear algorithms can also be ker-
nelized to be able to handle nonlinear data perhaps better
by transforming data.
Generally, there exist two main approaches for kerneliza-
tion in machine learning. These two approaches are related
in theory but have two ways for kernelization. In the fol-
lowing, we explain these methods which are kernel trick
and kernelization using representation theory.

8.1. Kernelization by Kernel Trick
Recall Eqs. (34) and (36) where kernel can be computed by
inner product between pulled data instances to the RKHS.
One technique to kernelize an algorithm is kernel trick.
In this technique, we ﬁrst try to formulate the algorithm
formulas or optimization in a way that data always ap-
pear as inner product of data instances and not a data in-
stance alone. In other words, the formulation of algorithm
should only have x(cid:62)x, x(cid:62)X, X (cid:62)x, or X (cid:62)X and not a
lonely x or X. In this way, kernel trick replaces x(cid:62)x with
φ(x)(cid:62)φ(x) and uses Eq. (34) or (36). To better explain,
kernel trick applies the following mapping (Burges, 1998):

x(cid:62)x (cid:55)→ φ(x)(cid:62)φ(x)

(34)
= k(x, x).

(103)

Therefore, the inner products of points are all replaced with
the kernel between points. The matrix form of kernel trick
is:

X (cid:62)X (cid:55)→ Φ(X)(cid:62)Φ(X)

(36)
= K(X, X) ∈ Rn×n.

(104)

Most often, kernel matrix is computed over one dataset;
hence, its dimensionality is n × n. However, in some
cases, the kernel matrix is computed between two sets
of data instances with sample sizes n1 and n2 for exam-
ple, i.e. datasets X 1 := [x1,1, . . . , x1,n1] and X 2 :=
[x2,1, . . . , x2,n2]. In this case, the kernel matrix has size
n1 × n2 and the kernel trick is:

1,ix1,j (cid:55)→ φ(x1,i)(cid:62)φ(x1,j)
x(cid:62)
X (cid:62)

1 X 2 (cid:55)→ Φ(X 1)(cid:62)Φ(X 2)

(34)
= k(x1,i, x1,j),

(105)

(36)
= K(X 1, X 2) ∈ Rn1×n2.
(106)

An example for kernel between two sets of data is the ker-
nel between training data and out-of-sample (test) data. As
stated in (Sch¨olkopf, 2001), the kernel trick is proved to
work for Mercer kernels in (Boser et al., 1992; Vapnik,
1995) or equivalently for the positive deﬁnite kernels (Berg
et al., 1984; Wahba, 1990).
Some examples of using kernel trick in machine learning
are kernel PCA (Sch¨olkopf et al., 1997a; 1998; Ghojogh &
Crowley, 2019) and kernel SVM (Boser et al., 1992; Vap-
nik, 1995). More examples will be provided in Section 9.
Note that in some algorithms, data do not not appear only
by inner product which is required for the kernel trick. In
these cases, if possible, a “dual” method for the algorithm is
proposed which only uses the inner product of data. Then,
the dual algorithm is kernelized using kernel trick. Some
examples for this are kernelization of dual PCA (Ghojogh
& Crowley, 2019) and dual SVM (Burges, 1998). As an ad-
ditional point, it is noteworthy that it is possible to replace
kernel trick with function replacement (see (Ma, 2003) for
more details on this).

18

8.2. Kernelization by Representation Theory
As was explained in Section 8.1, if the formulation of algo-
rithm has data only as inner products of points, kernel trick
can be used. In some cases, a dual version of algorithm is
used to have only inner products. Some algorithms, how-
ever, cannot be formulated in a way to have data only in
inner product form, nor does their dual have this form. An
example is Fisher Discriminant Analysis (FDA) (Ghojogh
et al., 2019b) which uses another technique for kerneliza-
tion (Mika et al., 1999). In the following, we explain this
technique.

Lemma 13 (Representation of Function Using Bases
(Mika et al., 1999)). Consider a RKHS denoted by H. Any
function f ∈ H lies in the span of all points in the RKHS,
i.e.,

f =

n
(cid:88)

i=1

αi φ(xi).

(107)

Proof. Consider Eq. (6) which can be restated as:

f (y)

(6)
=

n
(cid:88)

i=1

αi k(xi, y)

(34)
=

n
(cid:88)

i=1

=⇒ f (.) =

n
(cid:88)

i=1

αi φ(xi).

αi φ(xi)(cid:62)φ(y)

Q.E.D.

Remark 8 (Justiﬁcation by Representation Theory). Ac-
cording to representation theory (Alperin, 1993), any func-
tion in the space can be represented as a linear combina-
tion of bases of the space. This makes sense because the
function is in the space and the space is spanned by the
bases. Now, assume the space is RKHS. Hence, any func-
tion should lie in the RKHS spanned by the pulled data
points to the feature space. This justiﬁes Lemma 13 using
representation theory.

8.2.1. KERNELIZATION FOR VECTOR SOLUTION
Now, consider an algorithm whose optimization variable or
solution is the vector/direction u ∈ Rd in the input space.
For kernelization, we pull this solution to RKHS by Eq.
(32) to have φ(u). According to Lemma 13, this pulled
solution must lie in the span of all pulled training points
{φ(xi)}n

i=1 as:

φ(u) =

n
(cid:88)

i=1

αi φ(xi) = Φ(X) α,

(108)

which is t dimensional and t may be inﬁnite. Note that
Φ(X) is deﬁned by Eq. (35) and is t × n dimensional. The
vector α := [α1, . . . , αn](cid:62) ∈ Rn contains the coefﬁcients.
According to Eq. (32), we can replace u with φ(u) in the
algorithm. If, by this replacement, the terms φ(xi)(cid:62)φ(xi)

or Φ(X)(cid:62)Φ(X) appear, then we can use Eq.
(34) and
replace φ(xi)(cid:62)φ(xi) with k(xi, xi) or use Eq. (36) to re-
place Φ(X)(cid:62)Φ(X) with K(X, X). This kernelizes the
method. The steps of kernelization by representation the-
ory are summarized below:
• Step 1: u → φ(u)
• Step 2: Replace φ(u) with Eq. (108) in the algorithm

formulation

• Step 3: Some φ(xi)(cid:62)φ(xi) or Φ(X)(cid:62)Φ(X) terms

appear in the formulation
• Step 4: Use Eq. (34) or (36)
• Step 5: Solve (optimize) the algorithm where the vari-

able to ﬁnd is α rather than u

Usually, the goal of algorithm results in kernel. For exam-
ple, if u is a projection direction, the desired projected data
are obtained as:

u(cid:62)xi

(32)
(cid:55)→ φ(u)(cid:62)φ(xi)

(108)
= α(cid:62)Φ(X)(cid:62)φ(xi)
(36)
= α(cid:62)k(X, xi),

(109)

where k(X, xi) ∈ Rn is the kernel between all n train-
ing points with the point xi. As this equation shows, the
desired goal is based on kernel.

8.2.2. KERNELIZATION FOR MATRIX SOLUTION
Usually, the algorithm has multiple directions/vectors as
its solution. In other words, its solution is a matrix U =
[u1, . . . , up] ∈ Rd×p. In this case, Eq. (108) is used for all
p vectors and in a matrix form, we have:

Φ(U ) = Φ(X) A,

(110)

where Rn×p (cid:51) A := [α1, . . . , αp](cid:62) and Φ(U ) is t × p di-
mensional where t may be inﬁnite. Similarly, the following
steps should be performed to kernelize the algorithm:

• Step 1: U → φ(U )
• Step 2: Replace φ(U ) with Eq. (110) in the algorithm

formulation

• Step 3: Some Φ(X)(cid:62)Φ(X) terms appear in the for-

mulation

• Step 4: Use Eq. (36)
• Step 5: Solve (optimize) the algorithm where the vari-

able to ﬁnd is A rather than U

Again the goal of algorithm usually results in kernel. For
example, if U is a projection matrix onto its column space,
we have:

U (cid:62)xi

(32)
(cid:55)→ φ(U )(cid:62)φ(xi)

(110)
= A(cid:62)Φ(X)(cid:62)φ(xi)
(36)
= A(cid:62)k(X, xi),

(111)

where k(X, xi) ∈ Rn is the kernel between all n train-
ing points with the point xi. As this equation shows, the
desired goal is based on kernel.

19

As was mentioned, in many machine learning algorithms,
the solution U ∈ Rd×p is a projection matrix for projecting
d-dimensional data onto a p-dimensional subspace. Some
example methods which have used kernelization by rep-
resentation theory are kernel Fisher discriminant analysis
(FDA) (Mika et al., 1999; Ghojogh et al., 2019b), kernel
supervised principal component analysis (PCA) (Barshan
et al., 2011; Ghojogh & Crowley, 2019), and direct ker-
nel Roweis discriminant analysis (RDA) (Ghojogh et al.,
2020c).

Remark 9 (Linear Kernel in Kernelization). If we use ker-
nel trick, the kernelized algorithm with a linear kernel is
equivalent to the non-kernelized algorithm. This is because
in linear kernel, we have φ(x) = x and k(x, y) = x(cid:62)y
according to Eq. (41). So, the kernel trick, which is Eq.
(105), maps data as x(cid:62)y (cid:55)→ φ(x)(cid:62)φ(y) = x(cid:62)y for lin-
ear kernel. Therefore, linear kernel does not have any effect
when using kernel trick. Examples for this are kernel PCA
(Sch¨olkopf et al., 1997a; 1998; Ghojogh & Crowley, 2019)
and kernel SVM (Boser et al., 1992; Vapnik, 1995) which
are equivalent to PCA and SVM, respectively, if linear ker-
nel is used.
However, kernel trick does have impact when using kernel-
ization by representation theory because it ﬁnds the inner
products of pulled data points after pulling the solution and
representation as a span of bases. Hence, kernelized algo-
rithm using representation theory with linear kernel is not
equivalent to non-kernelized algorithm. Examples of this
are kernel FDA (Mika et al., 1999; Ghojogh et al., 2019b)
and kernel supervised PCA (Barshan et al., 2011; Ghojogh
& Crowley, 2019) which are different from FDA and super-
vised PCA, respectively, even if linear kernel is used.

9. Types of Use of Kernels in Machine

Learning

There are several types of using kernels in machine learn-
ing. In the following, we explain these types of usage of
kernels.

9.1. Kernel Methods
The ﬁrst type of using kernels in machine learning is ker-
nelization of algorithms using either kernel trick or rep-
resentation theory. As was discussed in Section 8, linear
methods can be kernelized to handle nonlinear data better.
Even nonlinear algorithms can be kernelized to perform in
the feature space rather than the input space. In machine
learning both kernel trick and kernelization by representa-
tion theory have been used. We provide some examples for
each of these categories:

• Examples for kernelization by kernel trick: ker-
nel Principal Component Analysis (PCA) (Sch¨olkopf
et al., 1997a; 1998; Ghojogh & Crowley, 2019), kernel
Support Vector Machine (SVM) (Boser et al., 1992;

Vapnik, 1995).

• Examples for kernelization by representation the-
ory: kernel supervised PCA (Barshan et al., 2011;
Ghojogh & Crowley, 2019), kernel Fisher Discrim-
inant Analysis (FDA) (Mika et al., 1999; Ghojogh
et al., 2019b), direct kernel Roweis Discriminant
Analysis (RDA) (Ghojogh et al., 2020c).

As was discussed in Section 1, using kernels was widely
noticed when linear SVM (Vapnik & Chervonenkis, 1974)
was kernelized in (Boser et al., 1992; Vapnik, 1995). More
discussions on kernel SVM can be found in (Sch¨olkopf
et al., 1997b; Hastie et al., 2009). A tutorial on kernel SVM
is (Burges, 1998). Universal kernels, introduced in Section
5.3, are widely used in kernel SVM. More detailed discus-
sions and proofs for use of universal kernels in kernel SVM
can be read in (Steinwart & Christmann, 2008).
As was discussed in Section 8.1, many machine learning
algorithms are developed to have dual versions because in-
ner products of points usually appear in the dual algorithms
and kernel trick can be applied on them. Some examples of
these are dual PCA (Sch¨olkopf et al., 1997a; 1998; Gho-
jogh & Crowley, 2019) and dual SVM (Boser et al., 1992;
Vapnik, 1995) yielding to kernel PCA and kernel SVM, re-
spectively. In some algorithms, however, either a dual ver-
sion does not exist or formulation does not allow for merely
having inner products of points. In those algorithms, ker-
nel trick cannot be used and representation theory should be
used. An example for this is FDA (Ghojogh et al., 2019b).
Moreover, some algorithms, such as kernel reinforcement
learning (Ormoneit & Sen, 2002), use kernel as a measure
of similarity (see Remark 4).

9.2. Kernel Learning
After development of many spectral dimensionality reduc-
tion methods in machine learning, it was found out that
many of them are actually special cases of kernel Principal
Component Analysis (PCA) (Bengio et al., 2003c; 2004).
Paper (Ham et al., 2004) has shown that PCA, multidi-
mensional scaling, Isomap, locally linear embedding, and
Laplacian eigenmap are special cases of kernel PCA with
kernels in the formulation of Eq. (51). A list of these ker-
nels can be seen in (Strange & Zwiggelaar, 2014, Chapter
2) and (Ghojogh et al., 2019d).
Because of this, some generalized dimensionality reduc-
tion methods, such as graph embedding (Yan et al., 2005),
were proposed.
In addition, as many spectral methods
are cases of kernel PCA, some researchers tried to learn
the best kernel for manifold unfolding. Maximum Vari-
ance Unfolding (MVU) or Semideﬁnite Embedding (SDE)
(Weinberger et al., 2005; Weinberger & Saul, 2006b;a) is
a method for kernel learning using Semideﬁnite Program-
ming (SDP) (Vandenberghe & Boyd, 1996). MVU is used
for manifold unfolding and dimensionality reduction. Note

20

that kernel learning by SDP has also been used for labeling
a not completely labeled dataset and is also used for ker-
nel SVM (Lanckriet et al., 2004; Karimi, 2017). Our focus
here is on MVU. In the following, we brieﬂy introduce the
MVU (or SDE) algorithm.
Lemma 14 (Distance in RKHS (Sch¨olkopf, 2001)). The
squared Euclidean distance between points in the feature
space is:

(cid:107)φ(xi) − φ(xj)(cid:107)2

k = k(xi, xi) + k(xj, xj) − 2k(xi, xj).
(112)

Proof.

(cid:107)φ(xi) − φ(xj)(cid:107)2
= φ(xi)(cid:62)φ(xi) + φ(xj)(cid:62)φ(xj) − φ(xi)(cid:62)φ(xj)

k = (cid:0)φ(xi) − φ(xj)(cid:1)(cid:62)(cid:0)φ(xi) − φ(xj)(cid:1)

− φ(xj)(cid:62)φ(xi)

− 2φ(xi)(cid:62)φ(xj)

(37)
= φ(xi)(cid:62)φ(xi) + φ(xj)(cid:62)φ(xj)
(34)
= k(xi, xi) + k(xj, xj) − 2k(xi, xj).

Q.E.D.

MVU desires to unfolds the manifold of data in its maxi-
mum variance direction. For example, consider a Swiss roll
which can be unrolled to have maximum variance after be-
ing unrolled. As trace of matrix is the summation of eigen-
values and kernel matrix is a measure of similarity between
points (see Remark 4), the trace of kernel can be used to
show the summation of variance of data. Hence, we should
maximize tr(K) where tr(.) denotes the trace of matrix.
MVU pulls data to the RKHS and then unfolds the mani-
fold. This unfolding should not ruin the local distances be-
tween points after pulling data to the feature space. Hence,
we should preserve the local distances as:

set= (cid:107)φ(xi) − φ(xj)(cid:107)2

(cid:107)xi−xj(cid:107)2
2
(112)
= k(xi, xi) + k(xj, xj) − 2k(xi, xj).

k

(113)

Moreover, according to Lemma 3, the kernel should be pos-
itive semideﬁnite, i.e. K (cid:23) 0. MVU also centers kernel to
have zero mean for the pulled dataset in the feature space
(see Section 6.1). According to Eq.
(63), we will have
(cid:80)n
j=1 K(i, j) = 0. In summary, the optimization
of MVU is (Weinberger et al., 2005; Weinberger & Saul,
2006b;a):

(cid:80)n

i=1

maximize
K
subject to

tr(K)

(cid:107)xi − xj(cid:107)2

2 = K(i, i) + K(j, j) − 2K(i, j),
∀i, j ∈ {1, . . . , n},

K(i, j) = 0,

n
(cid:88)

n
(cid:88)

i=1

j=1

K (cid:23) 0,

(114)

which is a SDP problem (Vandenberghe & Boyd, 1996).
Solving this optimization gives the best kernel for maxi-
mum variance unfolding of manifold. Then, MVU consid-
ers the eigenvalue problem for kernel, i.e. Eq. (93), and
ﬁnds the embedding using Eq. (102).

9.3. Use of Kernels for Difference of Distributions
There exist several different measures for difference of dis-
tributions (i.e., PDFs). Some of them make use of kernels
and some do not. A measure of difference of distributions
can be used for (1) calculating the divergence (difference)
of a distribution from another reference distribution or (2)
convergence of a distribution to another reference distribu-
tion using optimization. We will explain the second use of
this measure better in Corollary 5.
For the information of reader, we ﬁrst enumerate some
of the measures without kernels and then introduce the
kernel-based measures for difference of distributions. One
of methods which do not use kernels is the Kullback-
Leibler (KL) divergence (Kullback & Leibler, 1951). KL-
divergence, which is a relative entropy from one distribu-
tion to the other one and has been widely used in deep
learning (Goodfellow et al., 2016). Another measure is the
Wasserstein metric which has been used in generative mod-
els (Arjovsky et al., 2017). The integral probability metric
(M¨uller, 1997) is another measure for difference of distri-
butions.
In the following, we introduce some well-known measures
for difference of distributions, using kernels.

9.3.1. HILBERT-SCHMIDT INDEPENDENCE CRITERION

(HSIC)

Suppose we want to measure the dependence of two ran-
dom variables. Measuring the correlation between them is
easier because correlation is just “linear” dependence.
According to (Hein & Bousquet, 2004), two random vari-
ables X and Y are independent if and only if any bounded
continuous functions of them are uncorrelated. Therefore,
if we map the samples of two random variables {x}n
i=1
and {y}n
i=1 to two different (“separable”) RKHSs and have
φ(x) and φ(y), we can measure the correlation of φ(x) and
φ(y) in Hilbert space to have an estimation of dependence
of x and y in the input space.
The correlation of φ(x) and φ(y) can be computed by
the Hilbert-Schmidt norm of the cross-covariance of them
(Gretton et al., 2005). Note that the squared Hilbert-
Schmidt norm of a matrix A is (Bell, 2016):

||A||2

HS := tr(A(cid:62)A),

(115)

and the cross-covariance matrix of two vectors x and y is
(Gubner, 2006; Gretton et al., 2005):

Cov(x, y) := E

(cid:104)(cid:0)x − E(x)(cid:1)(cid:0)y − E(y)(cid:1)(cid:105)

.

(116)

21

Using the explained intuition, an empirical estimation of
the Hilbert-Schmidt Independence Criterion (HSIC) is in-
troduced (Gretton et al., 2005):

HSIC(X, Y ) :=

1

(n − 1)2 tr(KxHKyH),

(117)

where Kx := φ(x)(cid:62)φ(x) and Ky := φ(y)(cid:62)φ(y) are the
kernels over x and y, respectively. The term 1/(n − 1)2
is used for normalization. The matrix H is the centering
matrix (see Eq. (48)). Note that HSIC double-centers one
of the kernels and then computes the Hilbert-Schmidt norm
between kernels.
HSIC measures the dependence of two random variable
vectors x and y. Note that HSIC = 0 and HSIC > 0 mean
that x and y are independent and dependent, respectively.
The greater the HSIC, the greater dependence they have.
Lemma 15 (Independence of Random Variables Using
Cross-Covariance (Gretton & Gy¨orﬁ, 2010, Theorem 5)).
Two random variables X and Y are independent if and
only if Cov(f (x), f (y)) = 0 for any pair of bounded con-
tinuous functions (f, g). Because of relation of HSIC with
the cross-covariance of variables, two random variables
are independent if and only if HSIC(X, Y ) = 0.

9.3.2. MAXIMUM MEAN DISCREPANCY (MMD)
MMD, also known as the kernel two sample test and pro-
posed in (Gretton et al., 2006; 2012), is a measure for dif-
ference of distributions. For comparison of two distribu-
tions, one can ﬁnd the difference of all moments of the two
distributions. However, as the number of moments is inﬁ-
nite, it is intractable to calculate the difference of all mo-
ments. One idea to do this tractably is to pull both distri-
butions to the feature space and then compute the distance
of all pulled data points from distributions in RKHS. This
difference is a suitable estimate for the difference of all mo-
ments in the input space. This is the idea behind MMD.
MMD is a semi-metric (Simon-Gabriel et al., 2020) and
uses distance in the RKHS (Sch¨olkopf, 2001) (see Lemma
i=1 ∼ P and
14). Consider PDFs P and Q and samples {xi}n
i=1 ∼ Q. The squared MMD between these PDFs is:
{yi}n

MMD2(P, Q) :=

(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

i=1

φ(xi) −

1
n

(112)
=

1
n2

n
(cid:88)

n
(cid:88)

k(xi, xj) +

n
(cid:88)

1
n2

i=1

j=1

n
(cid:88)

i=1
n
(cid:88)

(cid:13)
2
(cid:13)
φ(yi)
(cid:13)
k

k(yi, yj)

i=1

−

2
n2

j=1
n
(cid:88)

n
(cid:88)

i=1

j=1

k(xi, yj)

= Ex[K(x, y)] + Ey[K(x, y)] − 2Ex[Ey[K(x, y)]],

(118)

where Ex[K(x, y)], Ey[K(x, y)], and Ex[Ey[K(x, y)]]
are average of rows, average of columns, and total aver-

age of rows and columns of the kernel matrix, respectively.
Note that MMD ≥ 0 where MMD = 0 means the two dis-
tributions are equivalent if the used kernel is characteristic
(see Corollary 5 which will be provided later). MMD has
been widely used in machine learning such as generative
moment matching networks (Li et al., 2015).

Remark 10 (Equivalence of HSIC and MMD (Sejdinovic
et al., 2013)). After development of HSIC and MMD mea-
sures, it was found out that they are equivalent.

9.4. Kernel Embedding of Distributions (Kernel Mean

Embedding)

Deﬁnition 23 (Kernel Embedding of Distributions (Smola
et al., 2007)). Kernel embedding of distributions, also
called the Kernel Mean Embedding (KME) or mean
map, represents (or embeds) Probability Density Functions
(PDFs) in a RKHS.

Corollary 4 (Distribution Embedding in Hilbert Space).
Inspired by Eq. (6) or (11), if we map a PDF P from its
space X to the Hilbert space H, its mapped PDF, denoted
by φ(P), can be represented as:

P (cid:55)→ φ(P) =

(cid:90)

X

k(x, .) dP(x).

(119)

This integral is called the Bochner integral.

KME and MMD were ﬁrst proposed in the ﬁeld of pure
mathematics (Guilbart, 1978). Later on, KME and MMD
were used in machine learning, ﬁrst in (Smola et al., 2007).
KME is a family on methods which use Eq. (119) for em-
bedding PDFs in RKHS. This family of methods is more
discussed in (Sriperumbudur et al., 2010). A survey on
KME is (Muandet et al., 2016).
Universal kernels, introduced in Section 5.3.3, can be used
for KME (Sriperumbudur et al., 2011; Simon-Gabriel &
Sch¨olkopf, 2018). In addition to universal kernels, char-
acteristic kernels and integrally strictly positive deﬁnite
are useful for KME (Sriperumbudur et al., 2011; Simon-
Gabriel & Sch¨olkopf, 2018). The integrally strictly posi-
tive deﬁnite kernel was introduced in Section 5.3.2. In the
following, we introduce the characteristic kernels.

Deﬁnition 24 (Characteristic Kernel (Fukumizu et al.,
2008)). A kernel is characteristic if the mapping (119) is
injective. In other words, for a characteristic kernel k, we
have:

EX∼P[k(., X)] = EY ∼Q[k(., Y )] ⇐⇒ P = Q,

(120)

where P and Q are two PDFs and X and Y are random
variables from these distributions, respectively.

Some examples for characteristic kernels are RBF and
Laplacian kernels. Polynomial kernels are not character-
istic kernels.

22

Corollary 5 (Convergence of Distributions to Each Other
Using Characteristic Kernels (Simon-Gabriel et al., 2020)).
Let Q be the PDF for a theoretical or sample reference
distribution. Following Deﬁnition 24, if the kernel used in
measures for difference of distributions is characteristic,
the measure can be used in an optimization framework to
converge a PDF P to the reference distribution Q as:

dk(P, Q) = 0 ⇐⇒ P = Q.

(121)

where dk(., .) denotes a measure for difference of distribu-
tions such as MMD.

Characteristic kernels have been used for dimensionality
reduction in machine learning. For example, see (Fuku-
mizu et al., 2004; 2009).
So far, we have introduced three different types of embed-
ding in RKHS. In the following, we summarize these three
types available in the literature of kernels.

Remark 11 (Types of Embedding in Hilbert Space). There
are three types of embeddings in Hilbert space:

1. Embedding of points in the Hilbert space: This em-
bedding maps x (cid:55)→ k(x, .) as stated in Sections 2.3
and 8.

2. Embedding of functions in the Hilbert space: This em-
bedding maps f (x) (cid:55)→ (cid:82) K(x, y) f (y) p(y) dy as
stated in Section 7.

3. Embedding of distributions (PDF’s) in the Hilbert
space: This embedding maps P (cid:55)→ (cid:82) k(x, .) dP(x)
as stated in Section 9.4.

Researchers are expecting that a combination of these types
of embedding might appear in the future.

9.5. Kernel Dimensionality Reduction for Sufﬁcient

Dimensionality Reduction

Kernels can also be used directly for dimensionality reduc-
tion. Assume X is the random variable of data and Y is
the random variables of labels of data. The labels can be
discrete ﬁnite for classiﬁcation or continuous for regres-
sion. Sufﬁcient Dimensionality Reduction (SDR) (Adragni
& Cook, 2009) is a family of methods which ﬁnd a trans-
formation of data to a lower dimensional space, denoted
by R(x), which does not change the conditional of labels
given data:

PY |X (y | x) = PY |R(X)(y | R(x)).

(122)

Kernel Dimensionality Reduction (KDR) (Fukumizu et al.,
2004; 2009; Wang et al., 2010b) is a SDR method with lin-
ear projection for transformation, i.e. R(x) : x (cid:55)→ U (cid:62)x
which projects data onto the column space of U . The goal
of KDR is:

PY |X (y | x) = PY |U,X (y | U , x).

(123)

Deﬁnition 25 (Dual Space). A dual space of a vector
space V, denoted by V ∗, is the set of all linear function-
als φ : V → F where F is the ﬁeld on which vector space
is deﬁned.

Theorem 5 (Riesz (or Riesz–Fr´echet) representation theo-
rem (Garling, 1973)). Let H be a Hilbert space with norm
(cid:104)., .(cid:105)H. Suppose φ ∈ H∗ (e.g., φ : f → R). Then, there
exists a unique f ∈ H such that for any x ∈ H, we have
φ(x) = (cid:104)f, g(cid:105):

∃f ∈ H : φ ∈ H∗, ∀x ∈ H, φ(x) = (cid:104)f, x(cid:105)H.

(124)

Corollary 6. According to Theorem 5, we have:

EX [f (x)] = (cid:104)f, φ(P)(cid:105)H,

∀f ∈ H,

(125)

where φ(P) is deﬁned by Eq. (119).

KDR uses Theorem 5 and Corollary 6 in its formualtions.
Note that characteristic kernels (Fukumizu et al., 2008), in-
troduced in Deﬁnition 24, are used in KDR.

10. Rank and Factorization of Kernel and the

Nystr¨om Method

10.1. Rank and Factorization of Kernel Matrix
Usually, the rank of kernel is small. This is because the
manifold hypothesis which states that data points often
do not cover the whole space but lie on a sub-manifold.
Nystr¨om approximation of the kernel matrix also works
well because kernels are often low-rank matrices (we will
discuss it in Corollary 7). Because of low rank of kernels,
they can be approximated (Kishore Kumar & Schneider,
2017), learned (Kulis et al., 2006; 2009), and factorized.
Kernel factorization has also be used for the sake of clus-
tering (Wang et al., 2010a). In the following, we introduce
some of the most well-known decompositions for the ker-
nel matrix.

10.1.1. SINGULAR VALUE AND EIGENVALUE

DECOMPOSITIONS

The Singular Value Decomposition (SVD) of the pulled
data to the feature space is:

Rt×n (cid:51) Φ(X) = U ΣV (cid:62),

(126)

where U ∈ Rt×n and V ∈ Rn×n are orthogonal matrices
and contain the left and right singular vectors, respectively,
and Σ ∈ Rn×n is a diagonal matrix with singular values.
Note that here, we are using notations such as Rt×n for
showing the dimensionality of matrices and this notation
does not imply a Euclidean space.
As mentioned before, the pulled data are not necessarily
available so Eq.
(126) cannot necessarily be done. The
kernel, however, is available. Using the SVD of pulled data

in the formulation of kernel gives:

23

(126)
= (U ΣV (cid:62))(cid:62)(U ΣV (cid:62))
ΣV (cid:62) = V ΣΣV (cid:62) = V Σ2V (cid:62),

K

(36)
= Φ(X)(cid:62)Φ(X)
= V Σ U (cid:62)U
(cid:124) (cid:123)(cid:122) (cid:125)
= I
=⇒ KV = V Σ2 V (cid:62)V
(cid:124) (cid:123)(cid:122) (cid:125)
= I

=⇒ KV = V Σ2.

(127)

If we take ∆ = diag([δ1, . . . , δn](cid:62)) := Σ2, this equation
becomes:

KV = V ∆,

(128)

which is the matrix of Eq. (93), i.e. the eigenvalue problem
(Ghojogh et al., 2019a) for the kernel matrix with V and
∆ as eigenvectors and eigenvalues, respectively. Hence,
for SVD on the pulled dataset, one can apply Eigenvalue
Decomposition (EVD) on the kernel where the eigenvec-
tors of kernel are equal to right singular vectors of pulled
dataset and the eigenvalues of kernel are the squared singu-
lar values of pulled dataset. This technique has been used
in kernel PCA (Ghojogh & Crowley, 2019).

10.1.2. CHOLESKY AND QR DECOMPOSITIONS
The kernel matrix can be factorized using LU decompo-
sition; however, as the kernel matrix is symmetric pos-
itive semi-deﬁnite matrix (see Lemmas 1 and 3), it can
be decomposed using Cholesky decomposition which is
much faster than LU decomposition. The Cholesky de-
composition of kernel is in the form Rn×n (cid:51) K = LL(cid:62)
where L ∈ Rn×n is a lower-triangular matrix. The ker-
nel matrix can also be factorized using QR decomposition
as Rn×n (cid:51) K = QR where Q ∈ Rn×n is an orthogo-
nal matrix and R ∈ Rn×n is an upper-triangular matrix.
The paper (Bach & Jordan, 2005) has incorporated side in-
formation, such as class labels, in the Cholesky and QR
decompositions of kernel matrix.

10.2. Nystr¨om Method for Approximation of

Eigenfunctions

Nystr¨om method, ﬁrst proposed in (Nystr¨om, 1930), was
initially used for approximating the eigenfunctions of an
operator (or of a matrix corresponding to an operator). The
following lemma provides the Nystr¨om approximation for
the eigenfunctions of the kernel operator deﬁned by Eq.
(89). For more discussion on this, reader can refer to
(Baker, 1978), (Williams & Seeger, 2001, Section 1.1), and
(Williams & Seeger, 2000).

Lemma 16 (Nystr¨om Approximation of Eigenfunction
(Baker, 1978, Chapter 3), (Williams & Seeger, 2001, Sec-
tion 1.1)). Consider a training dataset {xi ∈ Rd}n
i=1 and
the eigenfunction problem (78) where fk ∈ H and λk are
the k-th eigenfunction and eigenvalue of kernel operator
deﬁned by Eq. (89) or (90). The eigenfunction can be ap-

proximated by Nystr¨om method as:

fk(x) ≈

1
nλk

n
(cid:88)

i=1

k(xi, x) fk(xi),

(129)

where k(xi, x) is the kernel (or centered kernel) corre-
sponding to the kernel operator.

(89), in Eq.

Proof. This Lemma is somewhat similar and related to
Lemma 12. Consider the kernel operator, deﬁned by
(78): Kf = λf . Combining this
Eq.
with the discrete version of operator, Eq.
(90), gives
λkfk(x) = (1/n) (cid:80)n
i=1 k(xi, x)fk(xi) Dividing the sides
of this equation by λk brings Eq. (129). Q.E.D.

10.3. Nystr¨om Method for Kernel Completion and

Approximation

As explained before, the kernel matrix has a low rank
often. Because of its low rank, it can be approximated
(Kishore Kumar & Schneider, 2017). This is important
because in big data when n (cid:29) 1, constructing the kernel
matrix is both time-consuming and also intractable to store
in computer; i.e., its computation will run forever and will
raise a memory error ﬁnally. Hence, it is desired to com-
pute the kernel function between a subset of data points
(called landmarks) and then approximate the rest of kernel
matrix using this subset of kernel matrix. Nystr¨om approx-
imation can be used for this goal.
The Nystr¨om method can be used for kernel approxima-
tion. It is a technique used to approximate a positive semi-
deﬁnite matrix using merely a subset of its columns (or
rows) (Williams & Seeger, 2001, Section 1.2), (Drineas
et al., 2005). Hence, it can be used for kernel completion
in big data where computation of the entire kernel matrix
is time consuming and intractable. One can compute some
of the important important columns or rows of a kernel ma-
trix, called landmarks, and approximate the rest of columns
or rows by Nystr¨om approximation.
Consider a positive semi-deﬁnite matrix Rn×n (cid:51) K (cid:23) 0
whose parts are:

Rn×n (cid:51) K =

(cid:20) A B
B(cid:62) C

(cid:21)

,

(130)

where A ∈ Rm×m, B ∈ Rm×(n−m), and C ∈
R(n−m)×(n−m) in which m (cid:28) n. This positive semi-
deﬁnite matrix can be a kernel (or Gram) matrix.
The Nystr¨om approximation says if we have the small parts
of this matrix, i.e. A and B, we can approximate C and
thus the whole matrix K. The intuition is as follows. As-
sume m = 2 (containing two points, a and b) and n = 5
(containing three other points, c, d, and e). If we know the
similarity (or distance) of points a and b from one another,
resulting in matrix A, as well as the similarity (or distance)

24

of points c, d, and e from a and b, resulting in matrix B,
we cannot have much freedom on the location of c, d, and
e, which is the matrix C. This is because of the positive
semi-deﬁniteness of the matrix K. The points selected in
submatrix A are named landmarks. Note that the land-
marks can be selected randomly from the columns/rows of
matrix K and, without loss of generality, they can be put
together to form a submatrix at the top-left corner of matrix.
For Nystr¨om approximation, some methods have been pro-
posed for sampling more important columns/rows of matrix
more wisely rather than randomly. We will mention some
of these sampling methods in Section 10.5.
As the matrix K is positive semi-deﬁnite, by deﬁnition, it
can be written as K = O(cid:62)O.
If we take O = [R, S]
where R are the selected columns (landmarks) of O and S
are the other columns of O. We have:

K = O(cid:62)O =

(cid:21)

(cid:20)R(cid:62)
S(cid:62)
(cid:20)R(cid:62)R R(cid:62)S
S(cid:62)R S(cid:62)S

=

[R, S]

(cid:21) (130)
=

(cid:21)

(cid:20) A B
B(cid:62) C

.

(131)

(132)

Hence, we have A = R(cid:62)R. The eigenvalue decomposi-
tion (Ghojogh et al., 2019a) of A gives:

A = U ΣU (cid:62)
=⇒ R(cid:62)R = U ΣU (cid:62) =⇒ R = Σ(1/2)U (cid:62).

(133)

(134)

Moreover, we have B = R(cid:62)S so we have:

B = (Σ(1/2)U (cid:62))(cid:62)S = U Σ(1/2)S

(a)
=⇒ U (cid:62)B = Σ(1/2)S =⇒ S = Σ(−1/2)U (cid:62)B,

(135)

where (a) is because U is orthogonal (in the eigenvalue
decomposition). Finally, we have:

C = S(cid:62)S = B(cid:62)U Σ(−1/2)Σ(−1/2)U (cid:62)B

= B(cid:62)U Σ−1U (cid:62)B

(133)
= B(cid:62)A−1B.

(136)

Therefore, Eq. (130) becomes:

K ≈

(cid:20) A

B

B(cid:62) B(cid:62)A−1B

(cid:21)

.

(137)

Lemma 17 (Impact of Size of sub-matrix A on Nystr¨om
approximation). By increasing m, the approximation of
Eq. (137) becomes more accurate. If rank of K is at most
m, this approximation is exact.

Proof. In Eq. (136), we have the inverse of A. In order to
have this inverse, the matrix A must not be singular. For
having a full-rank A ∈ Rm×m, the rank of A should be m.
This results in m to be an upper bound on the rank of K

and a lower bound on the number of landmarks. In practice,
it is recommended to use more number of landmarks for
more accurate approximation but there is a trade-off with
the speed.

Corollary 7. As we usually have m (cid:28) n, the Nystr¨om
approximation works well especially for the low-rank ma-
trices (Kishore Kumar & Schneider, 2017) because we will
need a small A (so small number of landmarks) for ap-
proximation. Usually, because of the manifold hypothe-
sis, data fall on a submanifold; hence, usually, the ker-
nel (similarity) matrix or the distance matrix has a low
rank. Therefore, the Nystr¨om approximation works well
for many kernel-based or distance-based manifold learn-
ing methods.

10.4. Use of Nystr¨om Approximation for Landmark

Spectral Embedding

The spectral dimensionality reduction methods (Saul et al.,
2006) are based on geometry of data and their solutions of-
ten follow an eigenvalue problem (Ghojogh et al., 2019a).
Therefore, they cannot handle big data where n (cid:29) 1. To
tackle this issue, there exist some landmark methods which
approximate the embedding of all points using the embed-
ding of some landmarks. Big data, i.e. n (cid:29) 1, results
in large kernel matrices. Selecting some most informative
columns or rows of the kernel matrix, called landmarks, can
reduce computations. This technique is named the Nystr¨om
approximation which is used for kernel approximation and
completion.
Nystr¨om approximation, introduced below, can be used to
make the spectral methods such as locally linear embed-
ding (Ghojogh et al., 2020a) and Multidimensional Scaling
(MDS) (Ghojogh et al., 2020b) scalable and suitable for
big data embedding. It is shown in (Platt, 2005) that all the
landmark MDS methods are Nystr¨om approximations. For
more details on usage of Nystr¨om approximation in spec-
tral embedding, refer to (Ghojogh et al., 2020a;b).

10.5. Other Improvements over Nystr¨om

Approximation of Kernels

The Nystr¨om method has been improved for kernel approx-
imation in a line of research. For example, it has been
used for clustering (Fowlkes et al., 2004) and regulariza-
tion (Rudi et al., 2015). Greedy Nystr¨om (Farahat et al.,
2011; 2015) and large scale Nystr¨om (Li et al., 2010) are
other examples. There is a trade-off between the approxi-
mation accuracy and computational efﬁciency and they are
balanced in (Lim et al., 2015; 2018). The error analysis
of Nystr¨om method can be found in (Zhang et al., 2008;
It is better to sample the land-
Zhang & Kwok, 2010).
marks wisely rather than randomly. This ﬁeld of research
is named “column subset selection” or “landmark selec-
tion” for Nystron approximation. Some of these methods

25

are (Kumar et al., 2009; 2012) and landmark selection on a
sparse manifold (Silva et al., 2006).

11. Conclusion
This paper was a tutorial and survey paper on kernels
and kernel methods. We covered various topics including
Mercer kernels, Mercer’s theorem, RKHS, eigenfunctions,
Nystr¨om methods, kernelization techniques, and use of ker-
nels in machine learning. This paper can be useful for dif-
ferent ﬁelds of science such as machine learning, functional
analysis, and quantum mechanics.

Acknowledgement
Some parts of this tutorial, particularly some parts of the
RKHS, are covered by Prof. Larry Wasserman’s statisti-
cal machine learning course at the Department of Statistics
and Data Science, Carnegie Mellon University (watch his
course on YouTube). The video of RKHS in Statistical Ma-
chine Learning course by Prof. Ulrike von Luxburg at the
University of T¨ubingen is also great (watch on YouTube,
T¨ubingen Machine Learning channel). Also, some parts
such as the Nystr¨om approximation and MVU are covered
by Prof. Ali Ghodsi’s course, at University of Waterloo,
available on YouTube. There are other useful videos on
this ﬁeld which can be found on YouTube.

References
Abu-Khzam, Faisal N, Collins, Rebecca L, Fellows,
Micheal R, Langston, Micheal A, Suters, W Henry, and
Symons, Christopher T. Kernelization algorithms for the
vertex cover problem: Theory and experiments. Techni-
cal report, University of Tennessee, 2004.

Adragni, Koﬁ P and Cook, R Dennis. Sufﬁcient dimen-
sion reduction and prediction in regression. Philosoph-
ical Transactions of the Royal Society A: Mathematical,
Physical and Engineering Sciences, 367(1906):4385–
4405, 2009.

Ah-Pine, Julien. Normalized kernels as similarity indices.
In Paciﬁc-Asia Conference on Knowledge Discovery and
Data Mining, pp. 362–373. Springer, 2010.

Aizerman, Mark A, Braverman, E. M., and Rozonoer, L. I.
Theoretical foundations of the potential function method
in pattern recognition learning. Automation and remote
control, 25:821–837, 1964.

Akbari, Saieed, Ghareghani, Narges, Khosrovshahi, Gho-
lamreza B, and Maimani, Hamidreza. The kernels of the
incidence matrices of graphs revisited. Linear algebra
and its applications, 414(2-3):617–625, 2006.

Alperin, Jonathan L. Local representation theory: Modu-
lar representations as an introduction to the local repre-

26

sentation theory of ﬁnite groups. Cambridge University
Press, 1993.

Anderson, Thomas and Dahlin, Michael. Operating Sys-
tems: Principles and Practice. Recursive Books, 2014.

Arjovsky, Martin, Chintala, Soumith, and Bottou, L´eon.
In Inter-
Wasserstein generative adversarial networks.
national conference on machine learning, pp. 214–223.
PMLR, 2017.

Aronszajn, Nachman.

Theory of reproducing kernels.
Transactions of the American mathematical society, 68
(3):337–404, 1950.

Arzel`a, Cesare. Sulle Funzioni Di Linee. Tipograﬁa Gam-

berini e Parmeggiani, 1895.

Bach, Francis R and Jordan, Michael I. Predictive low-rank
decomposition for kernel methods. In Proceedings of the
22nd international conference on machine learning, pp.
33–40, 2005.

Bengio, Yoshua, Delalleau, Olivier, Roux, Nicolas Le,
Paiement, Jean-Franc¸ois, Vincent, Pascal, and Ouimet,
Marie. Learning eigenfunctions links spectral embed-
ding and kernel PCA. Neural computation, 16(10):
2197–2219, 2004.

Bengio, Yoshua, Delalleau, Olivier, Le Roux, Nicolas,
Paiement, Jean-Franc¸ois, Vincent, Pascal, and Ouimet,
In Feature
Marie. Spectral dimensionality reduction.
Extraction, pp. 519–550. Springer, 2006.

Berg, Christian, Christensen, Jens Peter Reus, and Ressel,
Paul. Harmonic analysis on semigroups: theory of posi-
tive deﬁnite and related functions, volume 100. Springer,
1984.

Bergman, Clifford. Universal algebra: Fundamentals and

selected topics. CRC Press, 2011.

Berlinet, Alain and Thomas-Agnan, Christine. Reproduc-
ing kernel Hilbert spaces in probability and statistics.
Springer Science & Business Media, 2011.

Baker, Christopher TH. The numerical treatment of inte-

Bhatia, Rajendra. Positive deﬁnite matrices. Princeton Uni-

gral equations. Clarendon press, 1978.

versity Press, 2009.

Barshan, Elnaz, Ghodsi, Ali, Azimifar, Zohreh, and
Jahromi, Mansoor Zolghadri. Supervised principal com-
ponent analysis: Visualization, classiﬁcation and regres-
sion on subspaces and submanifolds. Pattern Recogni-
tion, 44(7):1357–1371, 2011.

Borgwardt, Karsten M, Gretton, Arthur, Rasch, Malte J,
Kriegel, Hans-Peter, Sch¨olkopf, Bernhard, and Smola,
Alex J. Integrating structured biological data by kernel
maximum mean discrepancy. Bioinformatics, 22(14):
e49–e57, 2006.

Beauzamy, Bernard.

Introduction to Banach spaces and

their geometry. North-Holland, 1982.

Bell, Jordan. Trace class operators and Hilbert-Schmidt
operators. Department of Mathematics, University of
Toronto, Technical Report, 2016.

Bengio, Yoshua, Paiement, Jean-franc¸cois, Vincent, Pas-
cal, Delalleau, Olivier, Roux, Nicolas, and Ouimet,
Marie. Out-of-sample extensions for LLE, Isomap,
MDS, eigenmaps, and spectral clustering. Advances
in neural information processing systems, 16:177–184,
2003a.

Bengio, Yoshua, Vincent, Pascal, Paiement, Jean-Franc¸ois,
Delalleau, O, Ouimet, M, and LeRoux, N. Learning
eigenfunctions of similarity:
linking spectral cluster-
ing and kernel PCA. Technical report, Departement
d’Informatique et Recherche Operationnelle, 2003b.

Bengio, Yoshua, Vincent, Pascal, Paiement, Jean-Franc¸ois,
Delalleau, Olivier, Ouimet, Marie, and Le Roux, Nico-
Spectral clustering and kernel PCA are learn-
las.
ing eigenfunctions.
Technical report, Departement
d’Informatique et Recherche Operationnelle, Technical
Report 1239, 2003c.

Boser, Bernhard E, Guyon,

Isabelle M, and Vapnik,
Vladimir N. A training algorithm for optimal margin
classiﬁers. In Proceedings of the ﬁfth annual workshop
on Computational learning theory, pp. 144–152, 1992.

Bourbaki, Nicolas.

Sur certains espaces vectoriels
topologiques. In Annales de l’institut Fourier, volume 2,
pp. 5–16, 1950.

Brunet, Dominique, Vrscay, Edward R, and Wang, Zhou.
On the mathematical properties of the structural similar-
ity index. IEEE Transactions on Image Processing, 21
(4):1488–1499, 2011.

Burges, Christopher JC. A tutorial on support vector ma-
chines for pattern recognition. Data mining and knowl-
edge discovery, 2(2):121–167, 1998.

Camps-Valls, Gustavo. Kernel methods in bioengineering,

signal and image processing. Igi Global, 2006.

Conway, John B. A course in functional analysis. Springer,

2 edition, 2007.

Cox, Michael AA and Cox, Trevor F. Multidimensional
scaling. In Handbook of data visualization, pp. 315–347.
Springer, 2008.

De Branges, Louis. The Stone-Weierstrass theorem. Pro-
ceedings of the American Mathematical Society, 10(5):
822–824, 1959.

Ghojogh, Benyamin, Karray, Fakhri, and Crowley, Mark.
Eigenvalue and generalized eigenvalue problems: Tuto-
rial. arXiv preprint arXiv:1903.11240, 2019a.

27

Drineas, Petros, Mahoney, Michael W, and Cristianini,
Nello. On the Nystr¨om method for approximating a
Gram matrix for improved kernel-based learning. jour-
nal of machine learning research, 6(12), 2005.

Farahat, Ahmed, Ghodsi, Ali, and Kamel, Mohamed. A
novel greedy algorithm for Nystr¨om approximation. In
Proceedings of the Fourteenth International Conference
on Artiﬁcial Intelligence and Statistics, pp. 269–277.
JMLR Workshop and Conference Proceedings, 2011.

Farahat, Ahmed K, Elgohary, Ahmed, Ghodsi, Ali, and
Kamel, Mohamed S. Greedy column subset selection
for large-scale data sets. Knowledge and Information
Systems, 45(1):1–34, 2015.

Fausett, Laurene V. Fundamentals of neural networks: ar-
chitectures, algorithms and applications. Prentice-Hall,
Inc., 1994.

Fomin, Fedor V, Lokshtanov, Daniel, Saurabh, Saket, and
Zehavi, Meirav. Kernelization: theory of parameterized
preprocessing. Cambridge University Press, 2019.

Fowlkes, Charless, Belongie, Serge, Chung, Fan, and Ma-
Spectral grouping using the Nystr¨om
lik, Jitendra.
method. IEEE transactions on pattern analysis and ma-
chine intelligence, 26(2):214–225, 2004.

Fukumizu, Kenji, Bach, Francis R, and Jordan, Michael I.
Dimensionality reduction for supervised learning with
reproducing kernel Hilbert spaces. Journal of Machine
Learning Research, 5(Jan):73–99, 2004.

Fukumizu, Kenji, Sriperumbudur, Bharath K, Gretton,
Arthur, and Sch¨olkopf, Bernhard. Characteristic kernels
on groups and semigroups. In Advances in neural infor-
mation processing systems, pp. 473–480, 2008.

Fukumizu, Kenji, Bach, Francis R, Jordan, Michael I, et al.
Kernel dimension reduction in regression. The Annals of
Statistics, 37(4):1871–1905, 2009.

Garling, DJH. A ‘short’ proof of the Riesz representa-
tion theorem. In Mathematical Proceedings of the Cam-
bridge Philosophical Society, volume 73, pp. 459–460.
Cambridge University Press, 1973.

Genton, Marc G. Classes of kernels for machine learning:
a statistics perspective. Journal of machine learning re-
search, 2(Dec):299–312, 2001.

Ghojogh, Benyamin and Crowley, Mark. Unsupervised
and supervised principal component analysis: Tutorial.
arXiv preprint arXiv:1906.03148, 2019.

Ghojogh, Benyamin, Karray, Fakhri, and Crowley, Mark.
Fisher and kernel Fisher discriminant analysis: Tutorial.
arXiv preprint arXiv:1906.09436, 2019b.

Ghojogh, Benyamin, Karray, Fakhri, and Crowley, Mark.
Image structure subspace learning using structural simi-
larity index. In International Conference on Image Anal-
ysis and Recognition, pp. 33–44. Springer, 2019c.

Ghojogh, Benyamin, Samad, Maria N, Mashhadi,
Sayema Asif, Kapoor, Tania, Ali, Wahab, Karray,
Fakhri, and Crowley, Mark. Feature selection and fea-
ture extraction in pattern analysis: A literature review.
arXiv preprint arXiv:1905.02845, 2019d.

Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
Locally linear embedding and
arXiv preprint

Crowley, Mark.
its variants: Tutorial and survey.
arXiv:2011.10925, 2020a.

Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
Crowley, Mark. Multidimensional scaling, Sammon
arXiv
mapping, and Isomap: Tutorial and survey.
preprint arXiv:2009.08136, 2020b.

Ghojogh, Benyamin, Karray, Fakhri, and Crowley, Mark.
Generalized subspace learning by Roweis discriminant
analysis. In International Conference on Image Analysis
and Recognition, pp. 328–342. Springer, 2020c.

Ghojogh, Benyamin, Karray, Fakhri, and Crowley, Mark.
Theoretical insights into the use of structural similar-
ity index in generative models and inferential autoen-
coders. In International Conference on Image Analysis
and Recognition, pp. 112–117. Springer, 2020d.

Gonzalez, Rafael C and Woods, Richard E. Digital image
processing. Prentice hall Upper Saddle River, NJ, 2002.

Goodfellow, Ian, Bengio, Yoshua, Courville, Aaron, and
Bengio, Yoshua. Deep learning, volume 1. MIT press
Cambridge, 2016.

Gretton, Arthur and Gy¨orﬁ, L´aszl´o. Consistent nonpara-
metric tests of independence. The Journal of Machine
Learning Research, 11:1391–1423, 2010.

Gretton, Arthur, Bousquet, Olivier, Smola, Alex, and
Sch¨olkopf, Bernhard. Measuring statistical dependence
with Hilbert-Schmidt norms. In International conference
on algorithmic learning theory, pp. 63–77. Springer,
2005.

Gretton, Arthur, Borgwardt, Karsten, Rasch, Malte,
Sch¨olkopf, Bernhard, and Smola, Alex. A kernel method
for the two-sample-problem. Advances in neural infor-
mation processing systems, 19:513–520, 2006.

Gretton, Arthur, Borgwardt, Karsten M, Rasch, Malte J,
Sch¨olkopf, Bernhard, and Smola, Alexander. A kernel
two-sample test. The Journal of Machine Learning Re-
search, 13(1):723–773, 2012.

Gubner, John A. Probability and random processes for
electrical and computer engineers. Cambridge Univer-
sity Press, 2006.

Guilbart, Christian.

Etude des produits scalaires sur
l’espace des mesures: estimation par projections. PhD
thesis, Universit´e des Sciences et Techniques de Lille,
1978.

Ham,

Jihun, Lee, Daniel D, Mika, Sebastian, and
Sch¨olkopf, Bernhard. A kernel view of the dimensional-
ity reduction of manifolds. In Proceedings of the twenty-
ﬁrst international conference on Machine learning, pp.
47, 2004.

Hastie, Trevor, Tibshirani, Robert, and Friedman, Jerome.
The elements of statistical learning: data mining, infer-
ence, and prediction. Springer Science & Business Me-
dia, 2009.

Hawkins, Thomas. Cauchy and the spectral theory of ma-

trices. Historia mathematica, 2(1):1–29, 1975.

28

Karimi, Amir-Hossein. A summary of the kernel matrix,
and how to learn it effectively using semideﬁnite pro-
gramming. arXiv preprint arXiv:1709.06557, 2017.

Kimeldorf, George and Wahba, Grace. Some results on
Tchebychefﬁan spline functions. Journal of mathemati-
cal analysis and applications, 33(1):82–95, 1971.

Kishore Kumar, N and Schneider, Jan. Literature survey on
low rank approximation of matrices. Linear and Multi-
linear Algebra, 65(11):2212–2244, 2017.

Kulis, Brian, Sustik, M´aty´as, and Dhillon, Inderjit. Learn-
ing low-rank kernel matrices. In Proceedings of the 23rd
international conference on Machine learning, pp. 505–
512, 2006.

Kulis, Brian, Sustik, M´aty´as A, and Dhillon, Inderjit S.
Low-rank kernel learning with Bregman matrix diver-
gences. Journal of Machine Learning Research, 10(2),
2009.

Kullback, Solomon and Leibler, Richard A. On informa-
tion and sufﬁciency. The annals of mathematical statis-
tics, 22(1):79–86, 1951.

Kumar, Sanjiv, Mohri, Mehryar, and Talwalkar, Ameet.
In Ar-
Sampling techniques for the Nystr¨om method.
tiﬁcial Intelligence and Statistics, pp. 304–311. PMLR,
2009.

Hein, Matthias and Bousquet, Olivier. Kernels, associated
structures and generalizations. Max-Planck-Institut fuer
biologische Kybernetik, Technical Report, 2004.

Kumar, Sanjiv, Mohri, Mehryar, and Talwalkar, Ameet.
Sampling methods for the Nystr¨om method. The Journal
of Machine Learning Research, 13(1):981–1006, 2012.

Hilbert, David.

Grundz¨uge einer allgemeinen theo-
Nachrichten
rie der linearen integralrechnungen I.
von der Gesellschaft der Wissenschaften zu G¨ottingen,
Mathematisch-Physikalische Klasse, pp. 49–91, 1904.

Hinton, Geoffrey E and Salakhutdinov, Ruslan R. Reduc-
ing the dimensionality of data with neural networks. Sci-
ence, 313(5786):504–507, 2006.

Hofmann, Thomas, Sch¨olkopf, Bernhard, and Smola,
Alexander J. A review of kernel methods in machine
learning. Max-Planck-Institute Technical Report, 156,
2006.

Hofmann, Thomas, Sch¨olkopf, Bernhard, and Smola,
Alexander J. Kernel methods in machine learning. The
annals of statistics, pp. 1171–1220, 2008.

Kung, Sun Yuan. Kernel methods and machine learning.

Cambridge University Press, 2014.

Kusse, Bruce R and Westwig, Erik A. Mathematical
physics: applied mathematics for scientists and engi-
neers. Wiley-VCH, 2 edition, 2006.

Lanckriet, Gert RG, Cristianini, Nello, Bartlett, Peter,
Ghaoui, Laurent El, and Jordan, Michael I. Learning the
kernel matrix with semideﬁnite programming. Journal
of Machine learning research, 5(Jan):27–72, 2004.

Li, Mu, Kwok, James Tin-Yau, and L¨u, Baoliang. Mak-
ing large-scale Nystr¨om approximation possible.
In
ICML 2010-Proceedings, 27th International Conference
on Machine Learning, pp. 631, 2010.

Icking, Christian and Klein, Rolf. Searching for the kernel
of a polygon—a competitive strategy. In Proceedings of
the eleventh annual symposium on Computational geom-
etry, pp. 258–266, 1995.

Li, Yujia, Swersky, Kevin, and Zemel, Rich. Genera-
tive moment matching networks. In International Con-
ference on Machine Learning, pp. 1718–1727. PMLR,
2015.

Lim, Woosang, Kim, Minhwan, Park, Haesun, and Jung,
Kyomin. Double Nystr¨om method: An efﬁcient and ac-
curate Nystr¨om scheme for large-scale data sets. In In-
ternational Conference on Machine Learning, pp. 1367–
1375. PMLR, 2015.

Lim, Woosang, Du, Rundong, Dai, Bo, Jung, Kyomin,
Song, Le, and Park, Haesun. Multi-scale Nystr¨om
method. In International Conference on Artiﬁcial Intel-
ligence and Statistics, pp. 68–76. PMLR, 2018.

Ma, Junshui. Function replacement vs. kernel trick. Neu-

rocomputing, 50:479–483, 2003.

Mercer, J. Functions of positive and negative type and their
connection with the theory of integral equations. Philo-
sophical Transactions of the Royal Society, A(209):415–
446, 1909.

Mika, Sebastian, Ratsch, Gunnar, Weston,

Jason,
Scholkopf, Bernhard, and Mullers, Klaus-Robert. Fisher
discriminant analysis with kernels. In Neural networks
for signal processing IX: Proceedings of the 1999 IEEE
signal processing society workshop, pp. 41–48. Ieee,
1999.

Minh, Ha Quang, Niyogi, Partha, and Yao, Yuan. Mer-
cer’s theorem, feature maps, and smoothing. In Interna-
tional Conference on Computational Learning Theory,
pp. 154–168. Springer, 2006.

Muandet, Krikamol, Fukumizu, Kenji, Sriperumbudur,
Bharath, and Sch¨olkopf, Bernhard. Kernel mean em-
bedding of distributions: A review and beyond. arXiv
preprint arXiv:1605.09522, 2016.

M¨uller, Alfred. Integral probability metrics and their gen-
erating classes of functions. Advances in Applied Prob-
ability, pp. 429–443, 1997.

M¨uller, Klaus-Robert, Mika, Sebastian, Tsuda, Koji, and
Sch¨olkopf, Bernhard. An introduction to kernel-based
learning algorithms. Handbook of Neural Network Sig-
nal Processing, 2018.

Narici, Lawrence and Beckenstein, Edward. Topological

vector spaces. CRC Press, 2010.

Noack, Marcus M and Sethian, James A. Advanced station-
ary and non-stationary kernel designs for domain-aware
Gaussian processes. arXiv preprint arXiv:2102.03432,
2021.

Novak, Erich, Ullrich, Mario, Wo´zniakowski, Henryk, and
Zhang, Shun. Reproducing kernels of Sobolev spaces
on Rd and applications to embedding constants and
tractability. Analysis and Applications, 16(05):693–715,
2018.

29

Nystr¨om, Evert J.

¨Uber die praktische auﬂ¨osung von
integralgleichungen mit anwendungen auf randwertauf-
gaben. Acta Mathematica, 54(1):185–204, 1930.

Oldford, Wayne. Lecture: Recasting principal compo-
nents. Lecture notes for Data Visualization, Department
of Statistics and Actuarial Science, University of Water-
loo, 2018.

Ormoneit, Dirk and Sen, ´Saunak. Kernel-based reinforce-
ment learning. Machine learning, 49(2):161–178, 2002.

Orr, Mark J. L. Introduction to radial basis function net-
works. Technical report, Center for Cognitive Science,
University of Edinburgh, 1996.

Pan, Sinno Jialin, Kwok, James T, and Yang, Qiang. Trans-
fer learning via dimensionality reduction. In AAAI, vol-
ume 8, pp. 677–682, 2008.

Parseval des Chenes, MA. M´emoires pr´esent´es `a l’institut
des sciences, lettres et arts, par divers savans, et lus dans
ses assembl´ees. Sciences, math´ematiques et physiques
(Savans ´etrangers), 1:638, 1806.

Perlibakas, Vytautas. Distance measures for PCA-based
face recognition. Pattern recognition letters, 25(6):711–
724, 2004.

Platt, John. FastMap, MetricMap, and landmark MDS are

all Nystrom algorithms. In AISTATS, 2005.

Prugovecki, Eduard. Quantum mechanics in Hilbert space.

Academic Press, 1982.

Reed, Michael and Simon, Barry. Methods of modern
mathematical physics: Functional analysis. Academic
Press, 1972.

Renardy, Michael and Rogers, Robert C. An introduction
to partial differential equations, volume 13. Springer
Science & Business Media, 2006.

Rennie, Jason. How to normalize a kernel matrix. Tech-
nical report, MIT Computer Science & Artiﬁcial Intelli-
gence Lab, 2005.

Rojo- ´Alvarez, Jos´e Luis, Mart´ınez-Ram´on, Manel, Mar´ı,
Jordi Mu˜noz, and Camps-Valls, Gustavo. Digital signal
processing with Kernel methods. Wiley Online Library,
2018.

Rudi, Alessandro, Camoriano, Raffaello, and Rosasco,
Lorenzo. Less is more: Nystr¨om computational regu-
larization. In Advances in neural information processing
systems, pp. 1657–1665, 2015.

Rudin, Cynthia. Prediction: Machine learning and statis-
tics (MIT 15.097), lecture on kernels. Technical report,
Massachusetts Institute of Technology, 2012.

Rupp, Matthias. Machine learning for quantum mechanics
in a nutshell. International Journal of Quantum Chem-
istry, 115(16):1058–1073, 2015.

Saul, Lawrence K, Weinberger, Kilian Q, Sha, Fei, Ham,
Jihun, and Lee, Daniel D. Spectral methods for dimen-
sionality reduction. Semi-supervised learning, 3, 2006.

Saxe, Karen. Beginning functional analysis. Springer,

2002.

Schlichtharle, Dietrich. Digital Filters: Basics and Design.

Springer, 2 edition, 2011.

Schmidt, Erhard. ¨Uber die auﬂ¨osung linearer gleichungen
mit unendlich vielen unbekannten. Rendiconti del Cir-
colo Matematico di Palermo (1884-1940), 25(1):53–77,
1908.

Sch¨olkopf, Bernhard. The kernel trick for distances. Ad-
vances in neural information processing systems, pp.
301–307, 2001.

Sch¨olkopf, Bernhard and Smola, Alexander J. Learning
with kernels: support vector machines, regularization,
optimization, and beyond. MIT press, 2002.

Sch¨olkopf, Bernhard, Smola, Alexander, and M¨uller,
Klaus-Robert. Kernel principal component analysis. In
International conference on artiﬁcial neural networks,
pp. 583–588. Springer, 1997a.

Sch¨olkopf, Bernhard, Sung, Kah-Kay, Burges, Christo-
pher JC, Girosi, Federico, Niyogi, Partha, Poggio,
Tomaso, and Vapnik, Vladimir. Comparing support vec-
tor machines with Gaussian kernels to radial basis func-
tion classiﬁers. IEEE transactions on Signal Processing,
45(11):2758–2765, 1997b.

30

Sejdinovic, Dino, Sriperumbudur, Bharath, Gretton,
Arthur, and Fukumizu, Kenji. Equivalence of distance-
based and RKHS-based statistics in hypothesis testing.
The Annals of Statistics, pp. 2263–2291, 2013.

Shawe-Taylor, John and Cristianini, Nello. Kernel methods
for pattern analysis. Cambridge university press, 2004.

Silva, Jorge, Marques, Jorge, and Lemos, Jo˜ao. Select-
ing landmark points for sparse manifold learning.
In
Advances in neural information processing systems, pp.
1241–1248, 2006.

Simon-Gabriel, Carl-Johann and Sch¨olkopf, Bernhard.
Kernel distribution embeddings: Universal kernels, char-
acteristic kernels and kernel metrics on distributions. The
Journal of Machine Learning Research, 19(1):1708–
1736, 2018.

Simon-Gabriel, Carl-Johann, Barp, Alessandro,

and
Mackey, Lester. Metrizing weak convergence with
arXiv preprint
maximum mean discrepancies.
arXiv:2006.09268, 2020.

Smola, Alex, Gretton, Arthur, Song, Le, and Sch¨olkopf,
Bernhard. A Hilbert space embedding for distributions.
In International Conference on Algorithmic Learning
Theory, pp. 13–31. Springer, 2007.

Smola, Alex J and Sch¨olkopf, Bernhard. Learning with

kernels, volume 4. Citeseer, 1998.

Song, Le. Learning via Hilbert space embedding of distri-
butions. PhD thesis, The University of Sydney, 2008.

Sriperumbudur, Bharath K, Gretton, Arthur, Fukumizu,
Kenji, Sch¨olkopf, Bernhard, and Lanckriet, Gert RG.
Hilbert space embeddings and metrics on probability
measures. The Journal of Machine Learning Research,
11:1517–1561, 2010.

Sch¨olkopf, Bernhard, Smola, Alexander, and M¨uller,
Klaus-Robert. Nonlinear component analysis as a kernel
eigenvalue problem. Neural computation, 10(5):1299–
1319, 1998.

Sriperumbudur, Bharath K, Fukumizu, Kenji, and Lanck-
riet, Gert RG. Universality, characteristic kernels and
Journal of Machine
RKHS embedding of measures.
Learning Research, 12(7), 2011.

Sch¨olkopf, Bernhard, Burges, Christopher JC, and Smola,
Alexander J. Advances in kernel methods: support vec-
tor learning. MIT press, 1999a.

Steinwart, Ingo. On the inﬂuence of the kernel on the con-
sistency of support vector machines. Journal of machine
learning research, 2(Nov):67–93, 2001.

Sch¨olkopf, Bernhard, Mika, Sebastian, Burges, Chris JC,
Knirsch, Philipp, Muller, K-R, Ratsch, Gunnar, and
Smola, Alexander J.
Input space versus feature space
in kernel-based methods. IEEE transactions on neural
networks, 10(5):1000–1017, 1999b.

Scott, David W. Multivariate density estimation: theory,
practice, and visualization. John Wiley & Sons, 1992.

Steinwart, Ingo. Support vector machines are universally
consistent. Journal of Complexity, 18(3):768–791, 2002.

Steinwart, Ingo and Christmann, Andreas. Support vector
machines. Springer Science & Business Media, 2008.

Strang, Gilbert. The fundamental theorem of linear algebra.
The American Mathematical Monthly, 100(9):848–855,
1993.

31

Williams, Christopher KI and Barber, David. Bayesian
classiﬁcation with Gaussian processes. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 20
(12):1342–1351, 1998.

Yan, Shuicheng, Xu, Dong, Zhang, Benyu, and Zhang,
Hong-Jiang. Graph embedding: A general framework
for dimensionality reduction. In 2005 IEEE Computer
Society Conference on Computer Vision and Pattern
Recognition (CVPR’05), volume 2, pp. 830–837. IEEE,
2005.

Zhang, Jianguo, Marszałek, Marcin, Lazebnik, Svetlana,
and Schmid, Cordelia. Local features and kernels for
classiﬁcation of texture and object categories: A com-
prehensive study. International journal of computer vi-
sion, 73(2):213–238, 2007.

Zhang, Kai and Kwok, James T. Clustered Nystr¨om
method for large scale manifold learning and dimension
reduction. IEEE Transactions on Neural Networks, 21
(10):1576–1587, 2010.

Zhang, Kai, Tsang, Ivor W, and Kwok, James T. Improved
Nystr¨om low-rank approximation and error analysis. In
Proceedings of the 25th international conference on ma-
chine learning, pp. 1232–1239, 2008.

Strange, Harry and Zwiggelaar, Reyer. Open Problems in
Spectral Dimensionality Reduction. Springer, 2014.

Tenenbaum, Joshua B, De Silva, Vin, and Langford,
John C. A global geometric framework for nonlinear di-
mensionality reduction. Science, 290(5500):2319–2323,
2000.

Vandenberghe, Lieven and Boyd, Stephen. Semideﬁnite

programming. SIAM review, 38(1):49–95, 1996.

Vapnik, Vladimir. The nature of statistical learning theory.

Springer science & business media, 1995.

Vapnik, Vladimir and Chervonenkis, Alexey. Theory of

pattern recognition. Nauka, Moscow, 1974.

Wahba, Grace.
SIAM, 1990.

Spline models for observational data.

Wand, Matt P and Jones, M Chris. Kernel smoothing. CRC

press, 1994.

Wang, Lijun, Rege, Manjeet, Dong, Ming, and Ding, Yong-
sheng. Low-rank kernel matrix factorization for large-
IEEE Transactions on
scale evolutionary clustering.
Knowledge and Data Engineering, 24(6):1036–1050,
2010a.

Wang, Meihong, Sha, Fei, and Jordan, Michael. Unsuper-
vised kernel dimension reduction. Advances in neural
information processing systems, 23:2379–2387, 2010b.

Weinberger, Kilian Q and Saul, Lawrence K. An introduc-
tion to nonlinear dimensionality reduction by maximum
variance unfolding. In Proceedings of the AAAI Confer-
ence on Artiﬁcial Intelligence, volume 6, pp. 1683–1686,
2006a.

Weinberger, Kilian Q and Saul, Lawrence K. Unsupervised
learning of image manifolds by semideﬁnite program-
ming. International journal of computer vision, 70(1):
77–90, 2006b.

Weinberger, Kilian Q, Packer, Benjamin, and Saul,
Lawrence K. Nonlinear dimensionality reduction by
semideﬁnite programming and kernel matrix factoriza-
tion. In AISTATS, 2005.

Williams, Christopher and Seeger, Matthias. The effect of
the input density distribution on kernel-based classiﬁers.
In Proceedings of the 17th international conference on
machine learning, 2000.

Williams, Christopher and Seeger, Matthias. Using the
Nystr¨om method to speed up kernel machines. In Pro-
ceedings of the 14th annual conference on neural infor-
mation processing systems, number CONF, pp. 682–688,
2001.

