2
2
0
2

n
u
J

0
2

]

G
L
.
s
c
[

3
v
6
8
7
1
0
.
6
0
9
1
:
v
i
X
r
a

Global Optimality Guarantees For Policy Gradient Methods

Jalaj Bhandari and Daniel Russo

Columbia University

Abstract

Policy gradients methods apply to complex, poorly understood, control problems by performing stochastic
gradient descent over a parameterized class of polices. Unfortunately, even for simple control problems solvable
by standard dynamic programming techniques, policy gradient algorithms face non-convex optimization
problems and are widely understood to converge only to a stationary point. This work identiﬁes structural
properties – shared by several classic control problems – that ensure the policy gradient objective function
has no suboptimal stationary points despite being non-convex. When these conditions are strengthened, this
objective satisﬁes a Polyak-lojasiewicz (gradient dominance) condition that yields convergence rates. We also
provide bounds on the optimality gap of any stationary point when some of these conditions are relaxed.
Keywords: Reinforcement learning, policy gradient methods, policy iteration, dynamic programming, gradient
dominance.

1 Introduction

Many recent successes in reinforcement learning are driven by a class of algorithms called policy gradient
methods. These methods search over a parameterized class of polices by performing stochastic gradient
descent on a cost function capturing the cumulative expected cost incurred. Speciﬁcally, for discounted or
episodic problems, they treat the scalar cost function (cid:96)(π) = (cid:82) Jπ(s)dρ(s), which averages the total cost-to-go
function Jπ over a random initial state distribution ρ. Policy gradient methods aim to optimize over a smooth,
and often stochastic, class of parameterized policies {πθ}θ∈Θ by performing stochastic gradient descent on
(cid:96)(·), as in the iteration

θk+1 = θk − αk (∇θ(cid:96)(πθk ) + noise) .

Stochastic gradients can be generated by monte-carlo simulation, even in complex environments and with
policies represented by deep neural networks [Schulman et al., 2015, 2016, 2017]. This approach is especially
appealing when one has an inductive bias about the form of an effective policy. For example, Glasserman
and Tayur [1995] use gradient descent to optimize over simple structured policies in a realistic simulator of a
multi-echelon inventory control problem. Direct policy search has been used in control problems in robotics
[Peters and Schaal, 2006], manufacturing [Caramanis and Liberopoulos, 1992], arcade games [Schulman et al.,
2015], revenue management [Talluri and Van Ryzin, 2006, Section 3.5.1], ambulance redeployment [Maxwell
et al., 2013], scheduling in queues [L’Ecuyer and Glynn, 1994, L’Ecuyer et al., 1994], and many other areas.
Unfortunately, while policy gradient methods can be applied to a very broad class of problems, it is not
clear whether they adequately address even simple control problems solvable by classical methods. A key
challenge is that the total cost (cid:96)(·) is a non-convex function of the chosen policy. Typical of results concerning
black-box optimization of non-convex functions, policy gradient methods are widely understood to converge
asymptotically to a stationary point or a local minimum. Important theory guarantees this under technical
conditions [Baxter and Bartlett, 2001, Marbach and Tsitsiklis, 2001, Sutton et al., 1999] and it is widely
repeated in textbooks and surveys [Grondman et al., 2012, Peters and Schaal, 2006, Sutton and Barto, 2018].
But the literature seems to provide almost no guarantees into the quality of these stationary points. Worse yet,
Example 1 shows that policy gradient methods can get stuck in a bad local minimum in very simple examples
even when the policy class contains the optimal policy.

1

 
 
 
 
 
 
g(sR, L) = 2

g(sL, L) = 1

sL

g(sR, R) = 0

sR

g(sL, R) = 2

(a) Two state, two action MDP

Figure 1: Presence of bad local minima with a constrained policy class.

(b) (cid:96)(πθ) when γ = 0.8 and ρ = [0.6, 0.4]

Example 1. Consider the MDP depicted in Figure 1a. There are two states, left (sL) and right (sR), and
two possible actions, L and R, which move the agent to the desired state in the next period. Staying in the
state L incurs a cost g(sL, L) = 1 per period, whereas staying in the right state is costless with g(sR, R) = 0.
Moving between states incurs a per-period cost of 2. When the discount factor exceeds 1/2, it is easy to show
that the optimal policy chooses action R in either state. In that case, it is reasonable to search in a constrained
policy class, {πθ : θ ∈ [0, 1]} that plays action R with probability θ ∈ [0, 1] regardless of the current state.
Setting θ = 1 yields an optimal policy. Unfortunately, as shown in Figure 1, the total discounted cost incurred,
(cid:96)(πθ), is a nonconvex function of θ. When initialized with small value of θ, cost is locally increasing as a
function of θ, and so a gradient method updates the policy toward a bad local minimum at θ = 0. Once there,
any local policy search approach gets stuck as there are no descent directions that reduce cost. It is worth
noting here that in general, policy gradient methods face many additional challenges, for instance due to
unsophisticated exploration or policy parameterization. This example instead highlights the risk of bad local
minima due to the non-convexity of the inﬁnite horizon cost function (cid:96)(·).

In marked contrast to the example above, important recent work of Fazel et al. [2018] showed that for the
deterministic linear quadratic control problem, policy gradient with the class of linear policies converges to
the global optimum, despite non-convexity of the objective. Here, the authors provided an intricate analysis,
leveraging a variety of closed form expressions available for linear-quadratic problems. Separately, in the
operations literature, Kunnumkal and Topaloglu [2008] propose a stochastic approximation method for setting
base-stock levels in inventory control. In this example too, the objective is non-convex, but the authors establish
convergence to the globally optimal solution using an intricate analysis quite different than that of Fazel et al.
[2018]. How do we reconcile these success stories with the simple counterexample given in Example 1?

1.1 Our Contribution

Policy gradient methods aim to directly minimize the multi-period total discounted cost by applying ﬁrst-order
optimization methods. By contrast, classical dynamic programming methods, like value and policy iteration,
indirectly minimize the total cost by solving a sequence of simpler single period problems. We uncover an
indirect analysis of policy gradient methods, deducing global convergence properties from conditions on the
single period problems solved by policy iteration.

As a consequence of our general framework, we show that for several classic control problems, policy
gradient methods performed with respect to natural structured policy classes face no suboptimal local minima.
More precisely, despite its non-convexity, any stationary point1 of the policy gradient cost function is a global
optimum. The examples we treat include:

• Finite state and action MDPs with the class of all stochastic policies.

1For unconstrained problems, stationary points of a function f satisfy ∇f (x) = 0. More generally, for constrained optimization over

some set X , any stationary point x satisﬁes the ﬁrst order necessary conditions for optimality, ∇f (x)(cid:62)(x(cid:48) − x) ≥ 0 ∀x(cid:48) ∈ X .

2

0.00.20.40.60.81.01.22.64.05.36.7()• Linear quadratic (LQ) control problems with the class of linear policies.

• An optimal stopping problem with the class of threshold policies.

• A ﬁnite horizon inventory control problems with the class of non-stationary base-stock policies.

These canonical control problems provide an important benchmark and sanity check. But why do policy
gradient methods avoid suboptimal local minima in these cases as opposed to the simple case in Example 1?
Interestingly, the examples above share some important structural properties. Consider the following properties
of the LQ control example:

1. The policy class is closed under policy improvement. That is, starting with a linear policy and performing

a policy iteration step yields another linear policy.

2. The single period optimization problem deﬁning a policy iteration update has no suboptimal stationary
points. In particular, the objective is a simple convex quadratic that can be easily optimized using
ﬁrst-order methods.

These same properties hold for ﬁnite MDPs as well as the optimal stopping problem. Our result in Theorem 1
shows that these two properties, together with mild regularity conditions, imply that any stationary point of the
policy gradient loss function is globally optimal.

We remark that the closure condition is much weaker than requiring the policy class to contain all possible
policies. This is useful, for example, in problems where simple structured policy classes may be naturally
aligned with the problem objective. However, the closure property is stronger than only requiring the policy
class to contain (near) optimal policies. The presence of bad local minima in Example 1 illustrates why a
stronger condition is necessary. In that case, the policy class contains the optimal policy, but it is not closed
under policy improvement (see Section 5.1).

We extend our result in Theorem 1 in several ways. Theorem 2 studies a strengthening of the second
condition above that leads to fast converge rates. In particular, when the single period objective deﬁned by
a (weighted) policy iteration problem satisﬁes a Polyak-lojasiewicz (PL) condition (also popularly known
as “gradient dominance”), this property is inherited by the policy gradient objective (cid:96)(·). The PL conditions
are relaxations of (strong) convexity that guarantee fast global convergence of ﬁrst-order methods even for
non-convex objectives [Nesterov and Polyak, 2006, Polyak, 1963].

Next, in Theorem 3, we show that for ﬁnite horizon problems with non-stationary policy classes, like
the ﬁnite horizon inventory control problem, we only require that the policy class contains an optimal policy
instead of the closure condition. In addition, we only require a weaker version of condition 2 above. See
Section 5 for more details. Finally, Theorem 5, studies a weakening of the closure condition more generally. It
assumes that the policy class is approximately closed under policy improvement – meaning that the policy
iteration update can be solved in the given policy class up to a small error. In this case, we show how bound
the optimality gap of any stationary point. Many of our intermediate results may also be of interest, especially
our approach to concentrability coefﬁcients in Section 7 and the corresponding bounds in Theorem 4.

1.2 Limited scope of this work

This paper is focused on understanding the optimization landscape of the non-convex policy gradient objective
(cid:96)(·), which is a fundamental challenge for local policy search methods. Such an investigation is simultaneously
relevant to many strategies for searching locally over the policy space, including policy gradient methods
[Sutton et al., 1999], natural gradient methods [Kakade, 2002], ﬁnite difference schemes [Riedmiller et al.,
2007], evolutionary strategies [Salimans et al., 2017] etc.

However, we do sidestep many other issues that could lead to poor performance for speciﬁc local policy
search methods. One such notable issue is that of exploration. It is well known that the naive random
exploration provided by stochastic policies may be insufﬁcient to guarantee convergence to an optimal policy.
See Kakade and Langford [2002] or our discussion in Appendix C for examples. We imagine that we have

3

access to a simulator where a restart distribution ensures that important regions of the state space are visited
under all policies (see Assumption 1).

Another issue involves the choice of policy parameterization. In particular, for the popular softmax
parameterization, the Jacobian matrix becomes ill conditioned near corners of the probability simplex, making
a policy nearly insensitive to changes in the parameter space. Successful policy gradient methods must perform
a local change of variables, like the popular natural policy gradient methods proposed by Kakade [2002]. See
Section 6 for a detailed discussion on softmax policies. In addition, we do not address the choice of a gradient
estimator, which is the subject of a large literature on stochastic simulation and reinforcement learning. See
Sutton and Barto [2018], Mohamed et al. [2020], Glasserman and Ho [1991] or Fu [2006] for reviews from
different perspectives as well as recent work by Kallus and Uehara [2020a,b] on policy gradient estimation
using limited off-policy data.

For the rest of this paper, we will consider an idealized policy gradient update with access to exact gradient
evaluations. Generalizations to treat stochastic noise in gradient evaluations are possible using classical
results from stochastic approximation literature, which show that under regularity conditions and appropriately
decaying step-sizes most noisy iterative algorithms converge to the same limit as their deterministic counterparts
[see e.g. Bertsekas and Tsitsiklis, 1996, Borkar, 2009]. We do not pursue such extensions for brevity.

1.3 Related Literature

Realizability and closure Closure conditions play an important role in the study of methods which ﬁt
parametric approximations to value functions. Such conditions appear implicitly already in the classic textbook
of Bertsekas and Tsitsiklis [1996] and reﬁned notions were introduced in Munos [2003, 2005, 2007] and
Munos and Szepesv´ari [2008]. Closure conditions are sometimes called completeness assumptions in the
literature. They are stronger than realizability conditions, which assume that the parametric class of value
functions (nearly) contains the true value function. There is some evidence that conditions stronger than
realizability are necessary for any statistically efﬁcient RL algorithm to exist. Recent papers of Chen and Jiang
[2019], Du et al. [2020], Weisz et al. [2021] and Wang et al. [2021] construct examples where realizability
holds but no algorithm which uses a polynomial number of samples from the environment can produce a near
optimal policy. The insufﬁciency of realizability does not imply that closure conditions are necessary. For
instance, in an off-policy estimation problem, Uehara et al. [2021] identify a relaxed condition.

Our paper is focused on studying a seemingly distinct issue – landscape of the policy gradient loss function
– so it is intruiging that many of the same issues arise. A realizability assumption holds in Example 1, but the
optimization landspace is poor and policy gradient could get stuck in a bad local minimum. The optimization
landscape improves when the policy class is closed under policy improvement.

Prior work on analysis of policy gradient methods. Apart from the aforementioned works of Kunnumkal
and Topaloglu [2008] and Fazel et al. [2018], there has been limited prior work in theoretical guarantees for
policy gradient methods, especially beyond tabular MDPs. See Agarwal et al. [2020] for a detailed literature
review of results for the tabular setting.

One notable exception is the work by Scherrer and Geist [2014] which provides guarantees on the quality
of (approximate) stationary points obtained by local policy search algorithms with a convex policy class, such
as Conservative Policy Iteration [Kakade and Langford, 2002]. While Conservative Policy Iteration does not
explicitly place convexity conditions on the policy class, Scherrer and Geist [2014] argue that it can be viewed
as a special case of local policy search over an expanded policy class which is the convex hull of the original
policy class (i.e. the set of all mixture distributions over policies). Relative to these works, our result in
Theorem 1 is more general as it applies to problem settings with deterministic policies, inﬁnite action spaces,
structured cost functions, and policy classes that are not convex, such as the class of threshold or base-stock
policies.

Concurrent work. Concurrently with this work, Agarwal et al. [2020] provide a detailed study of policy
gradient methods, primarily focusing on convergence rates in the tabular case for speciﬁc algorithms and

4

with different policy parameterizations. They also extend their insights to the function approximation setting,
focusing on natural gradient descent with a restricted class of log-linear policies in ﬁnite action spaces. We
provide a detailed comparison with their concurrent work in Appendix B. There we also establish a precise
link between their conditions on the quality of parametric approximations to the value function and our
conditions on closure of the policy class. Along similar lines, to Agarwal et al. [2020], Shani et al. [2020]
also provide convergence rates for trust-region based policy optimization methods for regularized tabular
MDPs using ideas from analysis of the classic mirror descent method [Beck and Teboulle, 2003]. Although
we do not analyze speciﬁc algorithms, intellectually we view regularized MDPs as satisfying a second-order
gradient dominance condition, for which basic results in optimization theory imply fast convergence rates
with ﬁrst-order methods. See Section 6 for details. Finally, a recent paper by Wang et al. [2019] studies global
convergence properties of actor-critic methods in the compatible function approximation setting of Sutton
et al. [1999] with over-parameterized two layer neural networks, a regime in which the linearization error is
bounded and neural networks essentially behave like kernel functions [Jacot et al., 2018]. Complimentary
to these works, we focus primarily on novel insights into when, why, and how local policy search methods
succeed in ﬁnding a near-optimal policy. Our results apply to classic control problems, including those with
continuous state and action spaces, and structured classes of deterministic policies. A unique feature of the
work is that it applies in a uniﬁed way not only to a problem like the linear MDPs (Example 6) which are
popular among reinforcement learning theorists, but also to problems like optimal stopping, linear quadratic
control, and inventory control, where it is especially natural to restrict policy search to a particular structured
class of policies.

On non-convex optimization in machine learning. Beyond reinforcement learning, our work connects to
an emerging body of literature on ﬁrst-order methods for non-convex optimization problems, giving rates
of convergence to stationary-points for ﬁrst-order methods [Carmon et al., 2018, Davis and Grimmer, 2019,
Davis et al., 2020, Defazio et al., 2014, Ghadimi and Lan, 2013, 2016, Reddi et al., 2016a,b,c, Xiao and Zhang,
2014] and ensuring convergence to approximate local minima rather than saddle points [Agarwal et al., 2017,
Jin et al., 2017, Lee et al., 2016]. A complementary line of research studies the optimization landscape of
speciﬁc problems to essentially ensure that any local minima is globally optimal [Bhojanapalli et al., 2016, Ge
et al., 2015, 2016, Kawaguchi, 2016, Sun et al., 2017]. Taken together, these results show how interesting
non-convex optimization problems can be efﬁciently solved using ﬁrst-order methods. Our work contributes to
the second line of research, offering insight into landscape of the optimization objective (cid:96)(·). It is worth noting
that, unlike some of the aforementioned work, we do not focus on ensuring convergence to local minima rather
than saddle points. Example 1 shows that local minima could be far from optimal in simple examples. The
conditions we identify ensure that there are no suboptimal stationary points – including suboptimal saddle
points.

2 Problem formulation

We defer measurability assumptions required for a rigorous presentation until the end of this section, keeping
most of the formulation less formal but more accessible. A Markov decision process (MDP) is a six-tuple
(S, (As)s∈S , g, P, γ, ρ), consisting of a state space S ⊂ Rn, action spaces (As)s∈S , cost function g, transition
kernel P , discount factor γ ∈ (0, 1) and initial distribution ρ. For each state s ∈ S, As ⊂ Rk is the set of
feasible actions. We take A = ∪sAs. The transition kernel P speciﬁes a probability distribution P (·|s, a) over
S for any given state s ∈ S and action a ∈ As. The cost function g(·) encodes the instantaneous expected cost
g(s, a) incurred when selecting action a in state s. We assume that per-period costs are uniformly bounded,
meaning sups∈S,a∈As |g(s, a)| < ∞.

A stationary policy π : S → A is a function that prescribes a feasible action2 π(s) ∈ As for each state
s ∈ S. Let Π denote the set of all (measurable) stationary polices and M ⊂ S be any measurable set. For

2Rather than develop separate notation for randomized and deterministic policies, in speciﬁc settings like Example 3, we accommodate

randomized policies by letting a ∈ As denote a choice of some probability vector.

5

any π ∈ Π, let gπ(s) = g(s, π(s)) denote the per step cost function and Jπ(s) = Eπ
t=0 γtgπ(st)] to be
the corresponding cost-to-go function. Here, the notation Eπ
s [·] indicates that expectation is taken over the
Markovian sequence of states (s0, s1, · · · ) with s0 = s and the transition kernel Pπ(st+1 ∈ M|st, · · · , s0) =
P (M|st, π(st)). More generally, we write Eπ
ν [·] when the initial state is randomly drawn from some distribu-
tion ν over S and let Pπ

ν (·) denote the corresponding probability measure.

s [(cid:80)∞

Loss function and policy gradient methods. Under the initial distribution ρ, a stationary policy π ∈ Π has
discounted average cost

(cid:96)(π) = (1 − γ)Eπ
ρ

(cid:34) ∞
(cid:88)

(cid:35)
γtgπ(st)

(cid:90)

= (1 − γ)

Jπ(s)ρ(ds),

t=0

and discounted state-occupancy measure given by

ηπ(M) = (1 − γ)

∞
(cid:88)

t=0

γtPπ

ρ (st ∈ M) M ⊂ S.

(1)

(2)

Since Pπ
ρ (st ∈ ·) is the distribution of the state at time t under the policy π and initial distribution ρ, the
discounted state occupancy measure gives the discounted fraction of time the system spends in a given part
of the state space. It satisﬁes the balance equation ηπ(M) = (cid:82) [(1 − γ)ρ(M) + γP (M|s, π(s))] ηπ(ds),
allowing it to also be interpreted as the steady state distribution of an equivalent average cost problem where
in every period the state resets according to ρ with probability (1 − γ) and otherwise evolves according to the
transition kernel. See Lemma 17 in Appendix D for a proof. Note that (cid:96)(π) can be equivalently written as
(cid:96)(π) = (cid:82) gπ(s) ηπ(ds), allowing it to be interpreted as an average per-period cost.

Policy gradient methods directly apply ﬁrst-order optimization methods to minimize (cid:96)(·) over a chosen
parameterized class of policies, ΠΘ = {πθ(·) : θ ∈ Θ} ⊂ Π. For this reason, we often refer to (cid:96) as the policy
gradient objective function. We assume Θ ⊂ Rd is convex and denote JΘ = {Jπθ : θ ∈ Θ} to be the set
of cost-to-go functions corresponding to ΠΘ. To indicate that we are referring to a policy in the restricted
policy class, rather than an arbitrary stationary policy π ∈ Π, we typically either write πθ or specify that
π ∈ ΠΘ. We overload notation, writing (cid:96)(θ) = (cid:96)(πθ). We will later impose smoothness conditions to ensure
differentiability of (cid:96)(θ).

The optimal cost-to-go function is deﬁned as J ∗(s) = inf π∈Π Jπ(s). A stationary policy π is said to be
optimal if Jπ(s) = J ∗(s) for every s ∈ S. Under the technical conditions stated below in Assumption 2, at
least one optimal policy exists, which we denote by π∗ throughout the paper. The next lemma establishes that
optimal policies are minimizers of the average cost function (cid:96) and that the reverse direction essentially holds
when ρ places positive weight on all parts of the state space. For example, if S is discrete and ρ(s) > 0 for all
s ∈ S, then any minimizer of (cid:96) is an optimal policy. The proof can be found in Appendix D.

Lemma 1. A policy satisﬁes π ∈ arg minπ(cid:48)∈Π (cid:96)(π(cid:48)) if and only if Jπ = J ∗ ρ–almost surely, i.e.
ρ ({s ∈ S : Jπ(s) = J ∗(s)}) = 1.

Exploratory initial distribution. Policy gradient methods have poor convergence properties if applied
without an exploratory initial distribution. See Appendix C for a full discussion.
Inspired by Kakade
and Langford [2002], we assume that the discounted state occupancy measure under an optimal policy is
absolutely continuous with respect to the initial distribution, mathematically denoted as ηπ∗ (cid:28) ρ. Roughly, an
assumption of this form ensures that the policy gradient loss function in (1) is sensitive to policy performance
in all important parts of the state space. When S is discrete, it sufﬁces to assume ρ(s) > 0 for each s ∈ S.
When ρ and ηπ both possess probability density functions (PDFs) over S, it sufﬁces to assume that ρ is
supported over S. In Section 7, we discuss more about the dependence of our results on speciﬁc choices of the
initial distribution.

6

Assumption 1. We assume that ηπ∗ is absolutely continuous with respect to ρ. That is, for any M ⊂ S such
that ρ(M) = 0, we have ηπ∗ (M) = 0.

By (2), ηπ (cid:23) (1 − γ)ρ for any stationary policy π ∈ Π where (cid:23) is used to denote an inequality that holds
element-wise. Therefore, under Assumption 1, ηπ∗ (cid:28) ηπ for any policy π. Many of our results actually rely
on this consequence of Assumption 1, rather than Assumption 1 itself.

Bellman operators. Let J denote the set of bounded (measurable) functions on the state space. Deﬁne the
Bellman operator Tπ : J → J and Bellman optimality operator T : J → J by

(TπJ) (s) := g(s, π(s)) +

(cid:90)

(cid:20)

(T J)(s) := min
a∈As

g(s, a) +

J(s(cid:48))P (ds(cid:48)|s, π(s)),

(cid:90)

J(s(cid:48))P (ds(cid:48)|s, a)

(cid:21)

.

(3)

(4)

The Bellman optimality operator in (4) can be equivalently deﬁned as T J(s) = minπ∈Π TπJ(s). Assumption
2 below ensures that the minimum in (4) is attained and in particular there exists policy π ∈ Π such that
TπJ = T J. It is well known that when the per-period costs are uniformly bounded (as we assumed), T and
Tπ are monotone and contraction operators with respect to the maximum norm. Their unique ﬁxed points are
J ∗ and Jπ, respectively. We repeatedly use the following element-wise inequalities, which hold for any policy
π and J ∈ J and can be deduced from (3) and (4) above:

T J (cid:22) TπJ and T Jπ (cid:22) Jπ.

The state-action cost-to-go function (“Q-function”) corresponding to a policy π ∈ Π is given by,

Qπ(s, a) = g(s, a) + γ

(cid:90)

Jπ(s(cid:48))P (ds(cid:48) | s, a).

(5)

(6)

Deﬁne Q∗(·) = Qπ∗ (·). Notice that for any polices π, π(cid:48) ∈ Π, we have the following relations,

Qπ(s, π(s)) = Jπ(s),

Qπ(s, π(cid:48)(s)) = (Tπ(cid:48)Jπ)(s),

min
a∈As

Qπ(s, a) = (T Jπ)(s).

(7)

Notation. For any J ∈ J , we deﬁne the weighted p-norm as, (cid:107)J(cid:107)p,ν = (cid:0)(cid:82) |J(s)|pν(ds)(cid:1)1/p
for a given
probability distribution ν over S and p ≥ 1. Similarly, the maximum norm is given by (cid:107)J(cid:107)∞ = sups∈S |J(s)|.
For a matrix A ∈ Rn×m, we write (cid:107)A(cid:107)p = maxx:(cid:107)x(cid:107)p=1 (cid:107)Ax(cid:107)p for the operator norm and, in the case
where m = n and A = A(cid:62), let λmin(A) and λmax(A) denote the minimum and maximum eigenvalue. Let
(cid:104)x, y(cid:105) = x(cid:62)y = (cid:80)
i xiyi denote the standard inner product. We write element-wise inequalities as (cid:22) or (cid:23), so
J (cid:22) J (cid:48) if and only if J(s) ≤ J (cid:48)(s) for each s ∈ S.

Measurability assumptions. Here we state assumptions which avoid pathological measurability issues
that can arise in dynamic programming with general state and action spaces [Bertsekas and Shreve, 1978,
Blackwell, 1965], i.e. when both state and actions spaces are uncountably inﬁnite. Readers unfamiliar with
measure theory can safely skip this while still understanding the paper’s main insights. The key condition
is the existence of a measurable selection rule, which is a measurable rule that associates each state with an
action attaining the minimum in a Bellman update. All of our examples satisfy appropriate smoothness and
compactness conditions that ensure such a condition holds. We refer the readers to [Hern´andez-Lerma and
Lasserre, 2012, Section 3.3] for a detailed account. A brief introduction is also given in [Puterman, 2014,
Section 6.2.5]. We use the term measurable to refer to a Borel measurable set or function.

Assumption 2 (Measurable selection). Assume the sets S, A and K := {(s, a) : s ∈ S, a ∈ As} as well as
the function g : K → R are measurable. The transition kernel P is a stochastic kernel on S given K, meaning

7

that for each (s, a) ∈ K, P (·|s, a) is a probability measure on S and for each measurable set M ⊂ S, P (M|·)
is a measurable function. Assume that for each bounded measurable function J on S, there is a measurable
policy π ∈ Π such that

g(s, π(s)) + γ

(cid:90)

J(s(cid:48))P (ds(cid:48)|s, π(s)) = inf
a∈As

(cid:20)

g(s, a) + γ

(cid:90)

(cid:21)
J(s(cid:48))P (ds(cid:48)|s, a)

.

3 Background on smooth nonconvex optimization

Convergence to stationary points. Given that the policy gradient objective is almost always non-convex,
optimization algorithms generally will not converge to a global minimum. Instead, classical theory suggests
that under appropriate smoothness conditions many algorithms will converge to stationary points of the
objective, i.e. those satisfying the ﬁrst-order necessary conditions for optimality as in Deﬁnition 1. This
motivates our approach of studying the landscape of the policy gradient objective – and in particular the quality
of its (approximate) stationary points – rather than studying convergence properties of speciﬁc algorithms.

Deﬁnition 1. Consider the optimization problem minx∈X f (x) where X ⊂ Rd is a closed convex set and
f is continuously differentiable on an open set containing X . A point x ∈ X is called a stationary point if
(cid:104)x(cid:48) − x, ∇f (x)(cid:105) ≥ 0 for all x(cid:48) ∈ X . For X = Rd, a stationary point satisﬁes ∇f (x) = 0.

We include an illustrative result showing that projected gradient descent converges asymptotically to a
stationary point under appropriate smoothness and regularity conditions. This result can be generalized in
numerous ways. For example, [Beck, 2017, Theorem 10.15] provides rates of convergence. A more complete
treatment can be found in textbooks on nonlinear optimization [see e.g Bertsekas, 1997].

The result below covers two cases. The ﬁrst assumes ∇f is Lipschitz, or, equivalently for twice differ-
entiable functions, that the maximum eigenvalue of its Hessian is bounded above. The second relaxes this
condition, only requiring regularity properties on the sublevel set of the initial iterate. This accommodates
functions like f (x) = x4, whose second derivative is unbounded. This result is possible because projected
gradient descent with sufﬁciently small step-sizes is guaranteed to reduce cost in each iteration, so all iterates
lie in certain sublevel sets. The restriction that f has bounded sublevel sets is satisﬁed if the feasible region X
is itself a bounded set or if the function is coercive, meaning f (x) → ∞ as (cid:107)x(cid:107) → ∞. In problems where this
is not naturally satisﬁed, it can sometimes be enforced by adding a small penalty function (e.g. a quadratic
regularizer) to the objective. Recall that a point x∞ is said to be a limit point of a sequence {xk} if some
subsequence converges to x∞.

Lemma 2. Consider the optimization problem minx∈X f (x) where X ⊂ Rd is a closed convex set. Assume
f is bounded below and its β–sublevel set {x ∈ X : f (x) ≤ β} is bounded for each β ∈ R. Consider the
sequence xk+1 = ProjX (xk − α∇f (xk)) for k ∈ N.

1. [Beck, 2002, 2017] Assume f is differentiable on an open set containing X and ∇f is Lipschitz
continuous on X with Lipschitz constant L. If α ∈ (0, 1/L], the sequence {xk} has at least one limit
point and any limit point x∞ is a stationary point of f (·) on X satisfying f (xk) ↓ f (x∞).

2. Given a ﬁxed initial iterate x0, suppose f is continuously twice differentiable on an open set containing
the sublevel set {x ∈ X : f (x) ≤ f (x0)}. For a sufﬁciently small α > 0, the sequence {xk} has at least
one limit point and any limit point x∞ is a stationary point of f (·) on X satisfying f (xk) ↓ f (x∞).

Proof. The proof for of the second part closely follows the proof for the ﬁrst part as shown in [Beck, 2002,
2017]. For brevity this is omitted, but to ensure reproducibility, details are given in the technical report
Bhandari and Russo [2021].

8

Convergence rates under gradient dominance. Results like Lemma 2 ensure that ﬁrst order methods
converge asymptotically to stationary points under mild regularity conditions. Then, even if the cost function
is non-convex, such algorithms will converge toward a global optimum if all stationary points are optimal.
A stronger property, called the Polyak-Lojasiewicz (PL) inequality and also commonly known as gradient
dominance, effectively requires that approximate stationary points are also approximately optimal. Combined
with regularity conditions, this yields rates of convergence for ﬁrst-order methods. Below, we introduce
a notion of gradient dominance which might seem somewhat nonstandard, since many authors treat only
unconstrained problems. For the unconstrained case, this result reduces to the well known PL inequality of
Polyak [1963], minx(cid:48)∈Rd f (x) ≥ f (x) − c2
2µ (cid:107)∇f (x)(cid:107)2
2.
Deﬁnition 2. For X ⊆ Rd, we say f is (c, µ)–gradient dominated over X if there exists constants c > 0 and
µ ≥ 0 such that

min
x(cid:48)∈X

f (x(cid:48)) ≥ f (x) + min
x(cid:48)∈X

(cid:104)

c (cid:104)∇f (x), x(cid:48) − x(cid:105) +

(cid:107)x − x(cid:48)(cid:107)2
2

(cid:105)

µ
2

∀ x ∈ X .

(8)

The function is said to be gradient dominated with degree one if µ = 0 and gradient dominated with degree
two if µ > 0.

Any stationary point of a gradient dominated function is globally optimal. To see this, note that if a point x
is stationary, meaning (cid:104)∇f (x), x(cid:48) − x(cid:105) ≥ 0 for every x(cid:48) ∈ X , then the minimizer of the right hand side in (8) is
x, implying minx(cid:48)∈X f (x(cid:48)) ≥ f (x). More broadly, note how in (8), the optimality gap minx(cid:48)∈X f (x(cid:48)) − f (x)
can be bounded by a measure of how far x is from stationarity, which is captured by the minimization problem
on the right hand side.

Convex and strongly convex functions are gradient dominated. Recall that a differentiable function f is

said to be µ–strongly–convex if it satisﬁes the inequality

f (x(cid:48)) ≥ f (x) + (cid:104)∇f (x) , x(cid:48) − x(cid:105) +

µ
2

(cid:107)x − x(cid:48)(cid:107)2
2

(9)

for every x, x(cid:48) ∈ X . Minimizing over x(cid:48) on each side of (9) shows that a µ–strongly-convex function is
(1, µ)–gradient dominated. Similarly, convex functions satisfy (9) with µ = 0, and therefore are (1, 0)–gradient
dominated. Thus, gradient dominated functions satisfy a critical property of convex functions (relating the
optimality gap to distance from stationarity). However, there are important classes of functions that are
gradient dominated despite being nonconvex.

Under gradient dominance conditions, popular ﬁrst order optimization algorithms are assured to converge
to the global minimum and a simple analysis provides ﬁnite time rates of convergence. As an illustrative result,
part (a) of Lemma 3 strengthens Lemma 2 by providing an O(1/
T ) convergence rate. While part (b) of this
lemma assumes X = Rd, Karimi et al. [2016] also give a geometric convergence rate for projected gradient
descent on constrained subsets, X (cid:54)= Rd for (1, L)–gradient dominated functions with L-Lipschitz continuous
gradients. Using similar arguments, it seems possible to also show a geometric rate for (c, µ)–gradient
dominated functions on constrained spaces when µ > 0.

√

Lemma 3 (Convergence rates for gradient dominated smooth functions). Consider the problem, minx∈X f (x)
where X ⊆ Rd is nonempty. Assume ∇f is L–Lipschitz continuous on X . Denote f ∗ = inf x(cid:48)∈X f (x(cid:48)).
Consider the sequence xt+1 = ProjX (xt − α∇f (xt)).

1. Let X ⊂ Rd be bounded. Set R = supx,x(cid:48)∈X (cid:107)x−x(cid:48)(cid:107)2 and k = supx∈X (cid:107)∇f (x)(cid:107)2. If α ≤ min{ 1

k , 1
L }

and f is (c, 0)–gradient-dominated, then,

f (xT ) − f ∗ ≤

(cid:114)

2R2c2 (f (x0) − f ∗)
αT

.

2. Assume X = Rd and α = 1/L. If f is (c, µ)–gradient-dominated for µ > 0, then,

f (xT ) − f ∗ ≤

(cid:16)

1 −

(cid:17)T

µ
c2L

(f (x0) − f ∗) .

9

Proof. The proof of part (2) can be found in [Karimi et al., 2016, Polyak, 1963]. A result similar to Part (1)
can be extracted from the textbook [Beck, 2017], which gives convergence rates to approximate stationary
points. For brevity, this is omitted, but to ensure reproducibility, we give a detailed proof in the technical report
[Bhandari and Russo, 2021].

4 Motivation from linear quadratic control

We ﬁrst motivate and instantiate our general results for the special case of linear quadratic (LQ) control.
Leveraging many of the closed form expressions available in this case, recent work of Fazel et al. [2018]
showed that policy gradient methods converge to the globally optimal policy under some technical conditions.
The key to their result is showing that the inﬁnite horizon cost function, despite being non-convex, has no
suboptimal stationary points (and is in fact gradient dominated). Given the presence of bad stationary points in
Example 1, there must be some special problem structure driving this, but what? Quite different from Fazel
et al. [2018], our arguments involve classical understanding of the single period cost function underlying
policy iteration, avoiding the complications of directly analyzing the inﬁnite horizon cost function (cid:96)(·).

We highlight the two key properties of LQ control identiﬁed in Section 1.1. That is, we show that (a) the
class of linear policies is closed under policy improvement and (b) the policy iteration problem can be solved
to optimality by a gradient method, since it is convex quadratic and therefore has no suboptimal stationary
points. A short proof then shows how these two conditions imply that (cid:96)(·) has no suboptimal station points.
Like Fazel et al. [2018], we simplify the presentation by studying deterministic LQ control, but it is easy to
allow for noisy dynamics.

Example 2 (Linear Quadratic Control). For symmetric positive deﬁnite matrices R and C, we have the
following optimal control problem:

Minimize

∞
(cid:88)

γt (cid:0)a(cid:62)

t Rat + s(cid:62)

t Cst

(cid:1)

t=0
Subject to st+1 = Ast + Bat,

s0 ∼ ρ

where st ∈ Rn is a continuous state variable and at ∈ Rk is the action chosen at time t. We assume that
(cid:3) is ﬁnite and positive deﬁnite. In this setting, a linear
the second moment of the initial distribution Eρ
0
policy πθ(s) = θs is known to be optimal for some θ ∈ Rk×n. See for example Bertsekas [1995, 2011], Evans
[2005]. We consider searching for the optimal θ via a gradient method. Unfortunately, the loss function
(cid:96)(θ) = (1 − γ)Eρ[Jπθ (s0)] is non-convex (see Appendix B in Fazel et al. [2018]), making it unclear if gradient
descent on (cid:96)(θ) would reach the global minimum.

(cid:2)s0s(cid:62)

For LQ control, if a linear policy πθ is applied from a state s0, then unrolling the linear dynamics we have

st = (A + Bθ)st−1 = · · · = (A + Bθ)ts0. Then, we can write the cost-to-go function as:

Jπθ (s0) =

∞
(cid:88)

t=0

γt (cid:0)s(cid:62)

t θ(cid:62)Rθst + s(cid:62)

t Cst

(cid:1) = s(cid:62)

0

(cid:34) ∞
(cid:88)

t=0

(cid:124)

γt (cid:0)(A + Bθ)t(cid:1)(cid:62) (cid:0)θ(cid:62)Rθ + C(cid:1) (A + Bθ)t

(cid:123)(cid:122)
:=Kθ (cid:23) 0

s0.

(cid:35)

(cid:125)

A linear policy πθ is said to be stable if its cost-to-go is ﬁnite from all initial states, or equivalently, if all
γ(A + Bθ) lie strictly within the unit circle. We let ΘS ⊂ Rk×n denote the set
eigenvalues of the matrix
of all parameters deﬁning stable linear policies and assume that this set is nonempty. One can show3 that
(cid:96)(θ) < ∞ if θ ∈ ΘS and (cid:96)(θ) = ∞ if θ /∈ ΘS.

√

3This is stated formally in Lemma 21 in Appendix E. For completeness a proof is given in the technical report [Bhandari and Russo,
2021], but this is really a minor modiﬁcation of standard linear systems theory to account for the effect of discounting. Indeed, if we
γtst → ∞ for
consider the linear dynamical system st = (A + Bθ)st−1, then our condition is precisely equivalent to requiring that
every initial state. This is the appropriate deﬁnition of stability in discounted problems and, when γ = 1, our deﬁnition reduces to the
standard deﬁnition of a stable linear policy in undiscounted problems.

√

10

Even though the total cost function (cid:96)(θ) is non-convex, it can be shown that starting from a stable linear
policy, policy iteration (PI) converges to an optimal policy by solving a sequence of simpler single period
optimization problems [Hewer, 1971, Kleinman, 1968] that arise when applying the Bellman operator, which
in this case can be written as

(T Jπθ )(s) = min
a∈Rk


a(cid:62)Ra + s(cid:62)Cs + γ(As + Ba)(cid:62)Kθ(As + Ba)

(cid:123)(cid:122)
(cid:125)
(cid:124)
= Qπθ (s,a)




 .

(10)

A single iteration of PI updates the stable linear policy πθ to a new policy π+ that selects the action
π+(s) = arg mina Qπθ (s, a). This can equivalently be expressed as Tπ+Jπθ = T Jπθ . Typically, a PI update
requires solving a unique optimization problem for each state, but given the convex quadratic nature of
the problem in (10), it can be easily checked that π+(s) = θs where θ = −γ(R + γB(cid:62)KθB)−1B(cid:62)KθA.
Thus, starting from a linear policy, a PI update yields yet another linear policy implying that for LQ control,
the class of linear policies is closed under policy improvement. Policy iteration steps are sometimes called
policy improvement steps because the new policy π+ is assured to have lower cost-to-go, meaning that
Jπ+(s) ≤ Jπθ (s) for all s ∈ S and the improvement is strict at some set of states if πθ is not an optimal policy.
Crucially, this implies that for LQ control the PI update π+ is also a stable policy (see Lemma 4 for a formal
statement).

A useful interpretation of the PI update is to view it as the minimizer of a weighted policy iteration cost,

B(θ(cid:48)|η, Jπθ ) :=

(cid:90)

(Tπθ Jθ)(s) η(ds) =

(cid:90)

Qπθ (s, πθ(cid:48)(s)) η(ds),

(11)

over policy parameters θ(cid:48). Under appropriate conditions on η, the solution is unique4. From (10), note that
Qπθ (s, a) is a convex quadratic function of action a, which implies Qπθ (s, θ(cid:48)s) is a convex quadratic function
of θ(cid:48) and so is (11). This shows that the weighted PI objective has no suboptimal stationary points, the second
key property we identiﬁed in Section 1.1.

Because the per-stage cost functions in LQ control are unbounded, this example is technically beyond the
scope of the problem formulation in Section 2. Thankfully, the properties of Bellman operators that underlie
our analysis (for example, monotonicity) hold for LQ control. See Lemma 18 in Section E.4 for details. As a
consequence, the proofs of our general results will essentially apply without modiﬁcation to stable policies in
LQ control. Nevertheless, to be formal, any results about LQ control will clearly specify that they apply to
Example 2 and standalone proofs are given for completeness.

To discuss policy gradient methods, we ﬁrst need some smoothness properties of (cid:96)(·). Beginning with an
initial stable policy θ0 which incurs ﬁnite cost, ﬁrst-order algorithms with an appropriate step-size are assured
to decrease cost on every iteration, meaning that iterates remain in the sublevel set {θ ∈ Θ : (cid:96)(θ) ≤ (cid:96)(θ0)} ⊂
ΘS. The next lemma establishes regularity conditions on these sublevel sets which are sufﬁcient to apply
optimization results, like Lemma 2, to show that gradient descent converges to a stationary point of (cid:96)(·).

Lemma 4. Consider the LQ control problem formulated in Example 2. The set ΘS is open and (cid:96) is twice
continuously differentiable on ΘS. For any α ∈ R, the sublevel set Cα := (cid:8)θ ∈ Rn×k : (cid:96)(θ) ≤ α(cid:9) is a
compact subset of ΘS and, if it is nonempty, supθ∈Cα (cid:107)∇2(cid:96)(θ)(cid:107)2 < ∞.

Proof. These properties follow from [Rautert and Sachs, 1997, Toivonen, 1985]. Some additional details are
provided in Appendix E.4

With this background, a simple proof shows that for LQ control, the policy gradient loss function has
no suboptimal stationary points despite being non-convex. Essentially, for any stable linear policy that is

4Due to the quadratic structure of the objective, it is enough that η has a ﬁnite and strictly positive deﬁnite second moment matrix.
This is similar to our assumption about the initial distribution ρ and it can be established using the same argument as given in the proof of
Lemma 5.

11

suboptimal, we show that moving along the line segment toward a policy iteration update forms a descent
direction, implying that it cannot be a stationary point. The two properties we identiﬁed, convexity of the
weighted PI objective and closure with respect to the class of linear policies, are critical for this argument.

Lemma 5. For the LQ control problem formulated in Example 2, any stable linear policy θ satisﬁes ∇(cid:96)(θ) = 0
if and only if Jπθ = J ∗.

Proof sketch. Consider a stable linear policy πθ and take πθ to be a policy iteration update. Standard analysis
of policy iteration, using monotonicty of the Bellman operator, shows that Jπθ
(cid:22) T Jπθ (cid:22) Jπθ . Here, the
second inequality is strict at some set of states unless πθ is an optimal policy. This implies (cid:96)(θ) < (cid:96)(θ) when
πθ is suboptimal. Now, consider a soft policy iteration update θα = (1 − α)θ + αθ ∀ α ∈ [0, 1]. Leveraging
convexity of the policy iteration objective in (11), a short argument shows d
dα (cid:96)(θα)|α=0 ≤ 0 and this inequality
is strict unless πθ is an optimal policy. See Appendix E.4 for a detailed proof.

This idea of constructing a descent direction is strongly reminiscent to the arguments in Kakade and
Langford [2002] for ﬁnite MDPs. While we ﬁnd this proof to be intuitive, it relies not just on the closure
property, but on convexity of the policy class5 as well as convexity of the policy iteration cost function, which
will not hold in all of our examples. One contribution of this paper is to ﬁnd a clean generalization of this
argument, relaxing convexity conditions into Condition 2 in the next section.

5 General results

We now generalize some of the insights discussed above for the LQ control example to identify properties
which ensure the policy gradient objective has no suboptimal stationary points. In the next section, we show
these properties hold for various problems settings beyond LQ control.

5.1 Conditions on the policy iteration cost function

Consider the weighted policy iteration or the “Bellman” cost function introduced in (11).

B(¯π | η, Jπ) =

(cid:90)

(T¯πJπ)(s) η(ds) =

(cid:90)

Qπ(s, ¯π(s)) η(ds),

for a probability distribution η over S and Jπ ∈ J . Here, the ﬁnal equality follows by noting that (T¯πJπ)(s) ≡
Qπ(s, ¯π(s)) from (7). This Bellman cost function is a single period objective, considering the cost-to-go
of following ¯π for a single period and following π thereafter. We overload notation to write B(θ | η, J) =
B(πθ | η, J). When the state space is discrete and η(s) > 0 for all s ∈ S, classic policy iteration update can
be equivalently written as,

πk+1 = arg min

π∈Π

B(π|η, Jπk ).

(12)

Policy iteration, like value iteration, indirectly optimizes the inﬁnite horizon cost-to-go, (cid:96)(·) by solving the
sequence of simpler single period problems in (12). On the other hand, policy gradient methods aim to directly
minimize (cid:96)(πθ). Despite this crucial difference, our approach is to infer properties of the complex multi-period
objective (cid:96)(·) using some structure present in the single period problems. We outline this below.

5The class of linear policies is convex. That is, the policy απθ + (1 − α)πθ is a linear policy for any given linear policies πθ, πθ and
some α ∈ [0, 1]. However, the class of threshold policies, used in the optimal stopping and the inventory control problems is not convex.
If πθ(s) = 1(s ≤ θ) for θ ∈ R is a threshold policy, then 1

2 πθ(cid:48) is not a threshold policy when θ (cid:54)= θ(cid:48).

2 πθ + 1

12

Differentiability. Before arguing about any convergence properties, we ﬁrst need conditions for the policy
gradient itself to be well deﬁned. Condition 0 below states smoothness conditions, related to partial differen-
tiability of the Bellman objective B(·), that ensure (cid:96)(·) is differentiable and its gradients satisfy a convenient
formula used in practical implementations [Marbach and Tsitsiklis, 2001, Silver et al., 2014, Sutton and Barto,
2018]. Condition 0 arises quite naturally in calculating the derivatives of (cid:96)(θ). To see this, we refer the readers
to a short proof of Lemma 6 in Appendix D.

Condition 0 (Differentiability). For each θ ∈ Θ, the functions θ (cid:55)→ B(θ|ηπθ , Jπθ ) and θ (cid:55)→ B(θ|ηπθ
are continuously differentiable on an open set containing θ.

, Jπθ )

As B(θ|ηπθ , Jπθ ) = (cid:82) Qπθ (s, πθ(s)) η(ds), differentiability in θ follows if Qπθ (s, πθ(s)) is differentiable
almost everywhere and the exchange of derivative and integral is permitted. Also note that B(θ|ηπθ
, Jπθ ) =
(cid:82) Jπθ (s) ηπθ
(ds). Thus, differentiability in θ is related to the existence of a weak derivative of the state
occupancy measure [Pﬂug, 1988, 1990]. A large literature studies sufﬁcient conditions for differentiability
[Asmussen and Glynn, 2007, Glasserman and Ho, 1991, Rhee and Glynn, 2017]. We do not try to advance
that literature, instead focusing on the convergence of policy gradient methods when these derivatives are well
deﬁned.

The next lemma writes the gradients of (cid:96)(θ) in a form that may illuminate the sharp connections with
policy iteration. In making an update to the policy parameter θ, policy iteration within the parameterized
policy class would solve minθ∈Θ B(θ | ηπθ , Jπθ ). A policy gradient update takes a gradient step with respect
to θ instead of solving this single period problem to optimality. It is through this viewpoint that we avoid
analyzing (cid:96)(·) directly.

Lemma 6 (Policy gradient theorem). Under Condition 0, (cid:96)(θ) is continuously differentiable and

∇(cid:96)(θ) = ∇θ B(θ | ηπθ , Jπθ )

(cid:12)
(cid:12)
(cid:12)
(cid:12)θ=θ

Remark 1. When the exchange of a certain integral and derivative is permitted, this statement reduces
to ∇(cid:96)(θ) = (cid:82) ∇θQπθ (s, πθ(s))|θ=θηπθ (ds). Our presentation differs from the familiar form of the policy
gradient theorem [Sutton and Barto, 2018], which is written as ∇(cid:96)(θ) = E [Qπθ (s, a)∇ log πθ(a|s)] where
πθ(a|s) is the probability of selecting a deterministic action a in state s and the expectation is taken over the
distribution of states and actions under πθ. This form is useful for gradient estimation, but it applies only
to stochastic policies and seems to obscure connections with the gradient of the weighted PI objective. The
expression in Lemma 6 is more general and can be applied to both deterministic and stochastic policies (by
taking πθ(s) to be a probability vector as in Section 6). Lemma 6 was previously derived under somewhat
stringent regularity conditions by Silver et al. [2014]. Condition 0 and our short proof appear to be new.

Closure under policy improvement. We now introduce one of our main conditions, which we call closure
under policy improvement. This is a consistency condition which essentially says that the policy improvement
update can be solved within the policy class.

Condition 1 (Closure under policy improvement). For each π ∈ ΠΘ , there exists π+ ∈ ΠΘ such that
B(π+|ηπ, Jπ) = minπ(cid:48)∈Π B(π(cid:48)|ηπ, Jπ).

It is not hard to show that Condition 1 is equivalent to the following one: for each π ∈ ΠΘ there exists
π+ ∈ ΠΘ such that (Tπ+Jπ)(s) = (T Jπ)(s) almost surely under s drawn from the occupancy measure ηπ.
Under assumption 1, ηπ∗ (cid:28) ηπ and so this condition can be thought of as imposing a closure property at all
relevant parts of the state space.

This closure assumption accommodates interesting examples in which a restricted class of policies is
naturally aligned with the decision task, like the class of linear policies in LQ control or threshold policies
for optimal stopping. We emphasize that this condition is weaker than requiring the policy class to contain
nearly all stochastic policies. However, it is stronger than just requiring the policy class to contain an optimal

13

policy. An extension in section 8 bounds the optimality gap of stationary points when the policy is not closed
but satisﬁes a relaxed closure condition.

Some condition along these lines appears to be necessary. Indeed, Example 1 in the introduction showed an
extremely simple problem for which policy gradient methods can get stuck in a bad local minimum even though
the policy class contains an optimal policy. There, we consider a two state (S = {sL, sR}) deterministic
MDP with actions corresponding to moving left (L) and right (R) respectively. An action a ∈ A = [0, 1]
indicates the probability of choosing the action right. We consider a restricted class of policies of the form
πθ(sL) = πθ(sR) = θ, which plays action R in either state with probability θ ∈ [0, 1]. Simple calculations
show why this policy class is not closed under policy improvement. For any policy πθ we can write the
Q-function as

Qπθ (sL, a) = (1 − a) · 1 + a · 2 + γ ((1 − a)Jπθ (sL) + aJπθ (sR))
Qπθ (sR, a) = (1 − a) · 2 + a · 0 + γ ((1 − a)Jπθ (sL) + aJπθ (sR))

Consider a policy where θ is nearly zero, so πθ moves left with high probability. It is easy to check that
0 = arg mina∈[0,1] Qπθ (sL, a). That is, in state sL, it is optimal for the decision maker to move left assuming
that the policy πθ (which almost always moves left) will be applied in future periods. A similar argument shows
that 1 = arg mina∈[0,1] Qπθ (sR, a). Clearly, this policy iteration update is not contained in the restricted one
dimensional policy class {πθ : θ ∈ [0, 1]}.

Stationary points of the weighted PI objective. As a ﬁrst order method, policy gradients require additional
local optimization structure to succeed. The following conditions ensure that ﬁrst order methods are suitable
for solving the weighted policy iteration problem.

Condition 2.A (Stationary points of the weighted PI objective). For each π ∈ ΠΘ, the function θ (cid:55)→ B(θ |
ηπ, Jπ) has no sub-optimal stationary points.

Condition 2.B (Gradient dominance of the weighted PI objective). For any π ∈ ΠΘ, the function θ (cid:55)→ B(θ |
ηπ, Jπ) is (c, µ)–gradient-dominated over Θ.

It is worth emphasizing that the single period Bellman objective ¯θ (cid:55)→ B(¯θ|ηπθ , Jπθ ) is often much simpler
than the inﬁnite horizon objective (cid:96)(θ). In LQ control it is a convex quadratic function and is therefore gradient
dominated. For ﬁnite state and action MDPs, say for instance in Example 1, it is linear and hence is also
gradient dominated, even though we showed that the total cost function (cid:96)(θ) is non-convex and can have
suboptimal local minima. Even for complex neural networks, a very active literature studies the quality of
stationary points and local minima for certain single period loss functions [Du and Lee, 2018, Livni et al.,
2014].

5.2 Closed policy classes and optimality of stationary points.

Our ﬁrst result establishes that the policy gradient objective has no suboptimal stationary points when the
policy class is closed under policy improvement and the single period Bellman objective has no suboptimal
stationary points. Having identiﬁed the right conditions and notation, the proof falls into place.

Theorem 1. Suppose Conditions 0, 1, and 2.A hold. Then, (cid:96) is continuously differentiable and θ ∈ Θ is a
stationary point of (cid:96)(·) if and only if (cid:96)(πθ) = (cid:96)(π∗).

Proof of Theorem 1. We ﬁrst give a key lemma which establishes a Bellman-type equation that holds when
the single period objective θ (cid:55)→ B(θ | ηπθ , Jπθ ) has no bad stationary points.
Lemma 7. Suppose Condition 2.A is satisﬁed. If θ is a stationary point of (cid:96) : Θ → R, then

(cid:90)

Jπθ dηπθ = min
π∈ΠΘ

(cid:90)

(TπJπθ ) dηπθ .

14

Proof. If θ is a stationary point of (cid:96) : Θ → R, then by the policy gradient theorem in Lemma 6, it is also a
stationary point of the function θ (cid:55)→ B(θ | ηπθ , Jπθ ). Since Condition 2.A holds, this implies

B(θ | ηπθ , Jπθ ) = min
θ∈Θ

B(θ | ηπθ , Jπθ ).

Recalling the deﬁnition of B(θ | η, Jπ) in (11) lets us rewrite both sides of this equation as,

(cid:90)

(cid:90)

Jπθ dηπθ =

[Tπθ Jπθ ] dηπθ = B(θ | ηπθ , Jπθ ) = min
¯θ∈Θ

B(¯θ | ηπθ , Jπθ ) = min
¯θ∈Θ

(cid:90)

(cid:2)Tπθ

Jπθ

(cid:3) dηπθ .

Next, in Lemma 8, we state an “average” form of Bellman’s equation which shows that under an exploratory

initial distribution (see Assumption 1), an optimal policy has zero average Bellman error.

Lemma 8 (On average Bellman equation). For any π ∈ Π,

(cid:96)(π) = (cid:96)(π∗) ⇐⇒

(cid:90)

(Jπ − T Jπ) dρ = 0.

Proof. See Appendix D.1 for a detailed proof.

We now complete the proof of Theorem 1.

Proof of Theorem 1. To show the ﬁrst direction, note that (cid:96)(θ) = (cid:96)(π∗) implies that θ is a minimizer of (cid:96)(·).
By the ﬁrst order necessary conditions of optimality, θ must be a stationary point of (cid:96)(·). To prove the other
direction, suppose that θ is a stationary point of (cid:96)(·). Then,

(cid:90)

Jπθ dηπθ = min
π∈ΠΘ

(cid:90)

[TπJπθ ] dηπθ =

(cid:90)

[T Jπθ ] dηπθ .

where the ﬁrst equality follows from Lemma 7 (implied by Condition 2.A) while the second equality uses the
closure property in Condition 1. By the deﬁnition in (2), ηπ (cid:23) (1 − γ)ρ. Using that Jπ (cid:23) T Jπ, we have

(cid:90)

0 =

[Jπθ − T Jπθ ] dηπθ ≥ (1 − γ)

(cid:90)

[Jπθ − T Jπθ ] dρ ≥ 0.

The “on average Bellman equation” in Lemma 8 lets us conclude that (cid:96)(θ) = (cid:96)(π∗).

5.3 Convergence rates for policy gradient methods.

Theorem 1, which guarantees that the policy gradient objective has no suboptimal stationary points, is only an
asymptotic result when viewed in context of Lemma 2. Our next result strengthens this by showing that (cid:96)(θ) is
gradient dominated (though possibly non-convex) for closed policy classes if the simpler weighted PI objective
is gradient dominated. Recall, gradient dominance and smoothness of (cid:96)(·) often imply fast convergence rates
for ﬁrst-order methods, using results like Lemma 3. Our investigation here is inspired by Fazel et al. [2018],
who showed gradient dominance for LQ control by a careful manipulation of closed form expressions available
in linear systems.

Our result in Theorem 2 below relies on a constant κρ that measures the efﬁcacy of an exploratory initial
distribution in a more reﬁned way than Assumption 1. We call this the effective concentrability coefﬁcient, since
it plays a role similar to the concentrability coefﬁcients that are widely used in the analysis of approximate
value and policy iteration algorithms [Farahmand et al., 2010, Geist et al., 2017, Kakade and Langford,
2002, Munos, 2003, 2007, Munos and Szepesv´ari, 2008, Scherrer and Geist, 2014]. Intuitively, κρ captures
how errors in the cost-to-go functions manifest in Bellman errors that are detectable by sampling from the
exploratory initial distribution ρ. The somewhat opaque deﬁnition in (13) is precisely the quantity we need in
our analysis. Section 7 provides more insights into the deﬁnition along with more interpretable bounds for
κρ. For example, we show it is always bounded as κρ ≤ (cid:107) dηπ∗
dρ (cid:107)∞, where this worst-case likelihood ratio has
featured prominently in prior work as a measure of distribution shift.

15

Deﬁnition 3. Deﬁne the effective concentrability coefﬁcient κρ for the class of cost-to-go functions JΘ =
{Jπθ : θ ∈ Θ} to be the smallest scalar such that

(cid:107)J − J ∗(cid:107)1,ρ ≤

κρ
(1 − γ)

(cid:107)J − T J(cid:107)1,ρ ∀J ∈ JΘ.

(13)

If no such scalar exists then we say κρ = ∞.

Theorem 2 gives a gradient dominance condition on (cid:96)(·) for closed policy classes under Condition 2.B.

Subsequently, Corollary 1 holds as (strongly) convex functions are gradient dominated.

Theorem 2. If Conditions 0, 1, and 2.B hold, then (cid:96)(·) is

(cid:16) κρ

(1−γ) · c ,

(cid:17)
κρ
(1−γ) · µ

–gradient dominated.

Corollary 1. Suppose Conditions 0 and 1 hold. If, for every π ∈ ΠΘ, the function θ (cid:55)→ B(θ | ηπ, Jπ) is
convex, then (cid:96)(θ) is gradient dominated with degree one. If θ (cid:55)→ B(θ | ηπ, Jπ) is strongly convex, then (cid:96)(θ) is
gradient dominated with degree two.

Proof of Theorem 2. Our proof can be divided into two key steps. First, we use closure property (Condition
1) to bound the optimality gap of a policy, (cid:96)(π) − (cid:96)(π∗), by the improvement under a weighted PI update. The
second step uses the policy gradient theorem in Lemma 6 to translate this inequality into a gradient dominance
condition on (cid:96)(·). It is noteworthy that our results crucially depend on using an exploratory initial distribution
under which κρ < ∞.

Proof. We ﬁrst derive a consequence of the closure condition:

(cid:96)(πθ) − min
π∈Π

(cid:96)(π) = (1 − γ)

(cid:90)

[Jπθ − J ∗] dρ

(a)
= (1 − γ)(cid:107)Jπθ − J ∗(cid:107)1,ρ

(cid:107)Jπθ − T Jπθ (cid:107)1,ηπθ
(cid:90)

[Jπθ − T Jπθ ] dηπθ

(b)
≤ κρ(cid:107)Jπθ − T Jπθ (cid:107)1,ρ
(c)
≤

κρ
(1 − γ)
κρ
(1 − γ)
κρ
(1 − γ)
κρ
(1 − γ)

=

(d)
=

=

(cid:18)(cid:90)

(cid:18)

Jπθ dηπθ − min
π∈ΠΘ

(cid:90)

(cid:19)

[TπJπθ ] dηπθ

B(θ | ηπθ , Jπθ ) − min
θ(cid:48)∈Θ

B(θ(cid:48) | ηπθ , Jπθ )

(cid:19)

.

Here (a) uses that Jπθ (cid:23) J ∗, (b) applies the deﬁnition of κρ in (13), (c) uses that ηπθ (cid:23) (1 − γ)ρ (see the
deﬁnition in (2)) and (d) uses closure property of the policy class.

By Condition 2.B, θ (cid:55)→ B(θ | ηθ, Jπθ ) is (c, µ)–gradient dominated. Using gradient dominance and the

policy gradient theorem in Lemma 6, we ﬁnd

B(θ | ηπθ , Jπθ ) − min
θ(cid:48)∈Θ

B(θ(cid:48) | ηπθ , Jπθ ) ≤ − min
v∈Θ

≤ − min
v∈Θ

(cid:12)
(cid:20)
(cid:12)
c (cid:104)∇θ B(θ | ηπθ , Jπθ )
(cid:12)
(cid:12)θ=θ
µ
(cid:107)v − θ(cid:107)2
2
2

c (cid:104)∇θ(cid:96)(θ), v − θ(cid:105) +

(cid:104)

(cid:105)

.

, v − θ(cid:105) +

(cid:21)

µ
2

(cid:107)v − θ(cid:107)2
2

Combining this with the preceding calculation yields the desired result.

16

5.4 Beyond closed policy classes: the case of non-stationary policies.

For ﬁnite horizon problems with a class of non-stationary policies, we can guarantee that the policy gradient
objective has no spurious local minima under a much weaker condition. Rather than require the policy class to
be closed under improvement, it is sufﬁcient that the policy class contains an optimal policy6. For this reason,
our theory will cover a broad variety of ﬁnite horizon dynamic programming problems for which structured
policy classes are known to be optimal, even if the policy class is not closed under policy improvement.
Interestingly, this result relies critically on the use of a non-stationary policy class. Recall how Example 1
shows that for stationary policy classes, policy gradient methods can get stuck in bad local minima even if the
policy class contains an optimal policy.

We can state our formal result without introducing new notation for the ﬁnite horizon setting by a well
known trick that treats ﬁnite-horizon time-inhomogeneous MDPs as a special case of inﬁnite horizon MDPs
(see e.g. Osband et al. [2019]). Essentially, one can imagine that the state space factorizes into H + 1
components, thought of as stages of the decision problem. For any policy, a state s ∈ Si transitions to a state
in Si+1 until stage H + 1 is reached and the interaction effectively ends. We also assume the policy class
factors into separate components. This structure allows us to change the policy in stage h without inﬂuencing
the policy at other stages, essentially encoding time-inhomogeneous policies.

Condition 3. Suppose the state space factors as S = S1 ∪ · · · ∪ SH ∪ SH+1, where for a state s ∈ Sh with
h ≤ H, P (Sh+1|s, a) = 1 for all a ∈ As. The ﬁnal subset SH+1 = {τ } contains a single costless absorbing
state, with P ({τ }|τ, a) = 1 and g(τ, a) = 0 for any action a. The parameter space is the product set
Θ = Θ1 × · · · × ΘH , where a policy parameter θ = (θ1, . . . , θH ) ∈ Θ is the concatenation of H sub-vectors.
For any ﬁxed s ∈ Sh, πθ(s) depends only on θh.

We now state the main result for this subsection which applies under conditions much weaker than those for
Theorem 1. First, we only require the policy class to contain the optimal policy. Second, we relax Condition
2.A, which considered stationary points of the single-period Bellman objective θ (cid:55)→ B(θ|ηπ, Jπ), induced
by any policy π ∈ ΠΘ. Instead, we only need to impose a regularity condition on the Bellman objective
corresponding to the optimal cost-to-go function, B(θ|η, J ∗). This is helpful as the cost-to-go function of
an optimal policy often satisﬁes nice regularity properties (e.g. convexity) than that of an arbitrary policy.
Example 8 illustrates how we exploit both these properties for ﬁnite horizon inventory control with the class of
base-stock policies.

Condition 4. For any η ∈ {ηπ : π ∈ ΠΘ}, the problem minθ∈Θ B(θ|η, J ∗) has no suboptimal stationary
points.

However, for our argument to work only with Condition 4, we do require a stronger regularity property of

the initial distribution ρ as compared to Assumption 1.

Assumption 3. For any π ∈ ΠΘ, ηπ is absolutely continuous with respect to ρ.

Assumption 3 is crucial for our proof as it enables us to relate stationary points of B(θ|η, Jπ) to those of
B(θ|η, J ∗). See Appendix D.2 for details. Also note that in context of ﬁnite horizon problems, Assumption 3
implies that the agent may begin in a sub-problem with fewer than H periods remaining. This would typically
be possible in simulation based optimization and it is necessary for the results we derive. Intuitively, it ensures
the simulated agent encounters all possible ‘scenarios’ when the policy is being trained.

Theorem 3. Suppose Conditions 3 and 4 hold. If the parameterized policy class ΠΘ contains an optimal
policy, then any stationary point θ of (cid:96) : Θ → R satisﬁes (cid:96)(πθ) = (cid:96)(π∗).

Proof Sketch. The proof is given in Appendix D.2 and proceeds by backward induction. We ﬁrst show that at
any stationary point θ of (cid:96)(·), Jπθ (s) = J ∗(s) holds ρ-almost surely for s ∈ SH . We then argue that the same
statement holds for s ∈ Sh for all h < H as well. Our result then follows by invoking Lemma 1.

6This is a weaker property as closure of the policy class implies that it contains an optimal policy.

17

Note that Theorem 3 only gives a characterization of the stationary points of (cid:96)(·). We leave the study of a

gradient dominance condition for ﬁnite horizon problems as future work.

Remark 2 (On comparison with Policy Search by Dynamic Programming). We brieﬂy remark on how
Theorem 3 compares to results of Bagnell et al. [2004] on the Policy Search by Dynamic Programming
(PSDP) algorithm. Note that the spirit of our work and PSDP is quite different. While we focus on the
optimization landscape of policy gradient methods, PSDP is a dynamic programming based approach which
approximately solves the policy iteration problem, minπ∈ΠΘ B(π|µ, Jπ) over some base distribution µ. It
does this recursively, starting from the end of the horizon. If µ is a good guess of the state occupancy measure
of the optimal policy ηπ∗ , and the policy class contains a near-optimal policy, PSDP returns an approximately
optimal policy. Therefore, PSDP requires the base distribution µ to be equal to the state-occupancy measure
of the optimal policy, i.e. d(ηπ∗ , µ) = 0 for an appropriate metric d(·, ·) between two distributions. This is a
much stronger condition than Assumption 3, which only requires ηπ << ρ for all π ∈ ΠΘ and can be ensured
for example by letting ρ to be a density supported on the state space in each period, S1 × S2 × . . . × SH .

6 Examples

We now show how our general results in Section 5 apply to several examples.

Finite state and action MDPs with natural parameterization.

Example 3 (Finite state-action MDPs). Consider a problem with ﬁnite state space S = {1, · · · , n}. For
simplicity, we assume the set of feasible actions As is the same for every state s and denote this by A. We
also assume there is a ﬁnite set of k deterministic actions to choose from and take A = ∆k−1 to be the set of
all probability distributions over these actions. That is, any action a ∈ A is a probability vector where each
component ai denotes the probability of taking the i-th deterministic action. Cost and transition functions can
be naturally extended to functions on the probability simplex by deﬁning:

g(s, a) =

k
(cid:88)

i=1

g(s, ei) ai

P (s(cid:48)|s, a) =

k
(cid:88)

i=1

P (s(cid:48)|s, ei) ai,

(14)

where ei is the i-th standard basis vector, representing one of the k possible deterministic actions.

For this tabular setting, a natural parameterization considers the policy πθ(s) = θs ∈ ∆k−1 which
associates each state with a probability distribution over actions. Rather than track the policy parameter
θ = (θs : s = 1, · · · , n) ∈ Rn×k we work directly with a stochastic policy π ∈ Rn×k, viewed as a matrix
whose rows are probability vectors. In this case, the set of all stationary randomized policies can be written as
Π = {π ∈ Rn×k
i=1 πs,i = 1 ∀s ∈ {1, · · · , n}}. (When taking gradients, it can be helpful to view π as a
stacked vector rather than a matrix.)

: (cid:80)k

+

Since Π contains all stationary policies, it is clearly closed under policy improvement. It is also worth
noting that for any π ∈ Π, s ∈ S and a ∈ ∆k−1, the Q-function is linear in a, as we can write: Qπ(s, a) =
(cid:80)k

i=1 Qπ(s, ei)ai = (cid:104)Qπ(s, ·), a(cid:105). Therefore, the weighted policy iteration objective,

B(π(cid:48)|ηπ, Jπ) =

(cid:88)

s∈S

ηπ(s)

k
(cid:88)

i=1

Qπ (s, ei) π(cid:48)

s,i

(15)

is convex (linear) in π(cid:48). This implies that the weighted Bellman objective has no suboptimal stationary points
(Condition 2.A) and in fact is (1,0)-gradient dominated (Condition 2.B). Condition 0 is not hard to verify7.

7We sketch the argument. It is clear that (15) is continuously differentiable in π(cid:48), since it is linear in π(cid:48). On the other hand, one can
show to show that B(π|ηπ(cid:48) , Jπ) is continuously differentiable in π(cid:48) by observing that it is linear in ηπ(cid:48) and that ηπ(cid:48) can be written in
matrix form as ηπ(cid:48) = (1 − γ)ρ(I − γP π(cid:48))−1 where ρ is viewed as a row vector and Pπ(cid:48) is the transition matrix of the Markov chain
induced by π(cid:48). Here I − γPπ(cid:48) is invertible and linear in π(cid:48), so the derivative of this inverse matrix exists and is continuous.

18

Therefore, Theorems 1 and 2 conﬁrm that for tabular MDPs, (cid:96)(·) has no suboptimal stationary points and is
gradient dominated with degree one.

We give an illustrative convergence rate result for this example by appealing to part (1) of Lemma 3
and computing all the relevant quantities. Notice that application of Lemma 3 also requires smoothness
properties. For this, we use a result in [Agarwal et al., 2020] which shows that for tabular MDPs with natural
parameterization, ∇(cid:96) is Lipschitz continuous with constant L = 2γ|A|
(1−γ)2 . See Appendix E.1 for a detailed
proof.

Lemma 9 (Convergence rate for tabular MDPs). Consider the ﬁnite state action MDP with natural parame-
terization as formulated in Example 3. Assume γ ≥ 1/3 and per-period costs are normalized with (cid:107)g(cid:107)∞ ≤ 1.
For projected gradient descent, π(t+1) = ProjΠ(πt − α∇(cid:96)(πt))) with α = (1−γ)2
2γ|A| ,

(cid:96)(πT ) − (cid:96)∗ ≤

(cid:115)

8γ|S||A|κ2
ρ
(1 − γ)4

((cid:96)(π0) − (cid:96)∗)
T

,

where (cid:96)∗ = inf π∈Π (cid:96)(π).

Remark 3. Note that the dependence on various factors such as γ, |S|, |A|, κρ, T is essentially the same as in
the result of [Agarwal et al., 2020] except for a factor of (1 − γ)2 due to our deﬁnition of the policy gradient
objective (cid:96)(·) as a discounted average cost.

Regularized ﬁnite state and action MDPs with natural parameterization.

We now consider a modiﬁcation of example 3 where a strongly convex regularizer is added to the single period
cost functions. This smooths the problem and ensures that optimal policies are strictly stochastic. See Geist
et al. [2019] and the references therein for a formulation that is analogous to ours, except that while they use a
negative entropy regularizer, we propose to use a more aggressive log-barrier based regularizer8.

Example 4. (Regularized Finite MDPs) As in Example 3, let S = {1, · · · , n} and A = ∆k−1. Deﬁne
R(a) := λDKL(U ||a) = λ (cid:80)k
to be proportional to the Kullback–Leibler (KL) divergence
(a.k.a relative entropy) between a uniform distribution and a. Assume that for each s there exists gs ∈ Rk
+
such that

(cid:16) 1/k
ai

(cid:1) log

(cid:0) 1
k

i=1

(cid:17)

g(s, a) = g(cid:62)

s a + R(a)

P (s(cid:48)|s, a) =

k
(cid:88)

i=1

P (s(cid:48)|s, ei) ai.

(16)

The regularizer R(a) is a strongly-convex extended-real-valued function with dom(R) = {a ∈ ∆k−1 :
R(a) < ∞} = {a ∈ ∆k−1 : mini ai > 0}.

Under this example, the weighted Bellman objective takes the form

B(π(cid:48) | ηπ, Jπ) =

(cid:88)

s∈S

ηπ(s)

k
(cid:88)

i=1

Qπ (s, ei) π(cid:48)

s,i +

(cid:88)

s∈S

ηπ(s)R((π(cid:48)

s,1, · · · , π(cid:48)

s,k)),

(17)

which is the sum of a linear function and a strongly convex regularizer. Strong convexity of B implies it has
no suboptimal stationary points, and in fact it is gradient dominated with degree 2 (Condition 2). The class
of policies is trivially closed under policy improvement (Condition 1). Together, these imply π (cid:55)→ (cid:96)(π) is
gradient dominated with degree two.

8Like Geist et al. [2019], we regularize single stage cost functions, which produces a modiﬁed MDP with continuous actions to which
our results apply. Directly adding a generic regularizer to (cid:96)(θ) before optimizing over policies might produce objectives that are not
additive across time.

19

Notice that relative-entropy regularization is incorporated into the single-period cost functions. Our results
imply that appropriate ﬁrst-order methods applied to a regularized objective converge to the optimum of
that objective. We complement this by establishing that, when λ is small, an optimal policy for an entropy
regularized objective is near-optimal for an un-regularized one. See Appendix E.2 for a proof. Geist et al.
[2019] provides similar bounds albeit with a negative entropy regularizer; these bounds do not apply here as
the log-barrier regularizer is unbounded.

Lemma 10 (Impact of regularization). Let (cid:96)λ(π) denote the average cost function for the problem described
in Example 4 with a given regularization parameter λ ≥ 0. Then, if π∗

λ ∈ arg minπ∈Π (cid:96)λ(π),

(cid:96)0 (π∗

λ) ≤ min
π∈Π

(cid:96)λ(π) + λ

(cid:16)

1 + log

(cid:16)

1 +

(cid:17)(cid:17)

c
λ

where

c = 2(max
s,i

|gs,i|)/(1 − γ).

Regularized ﬁnite state and action MDPs with nonlinear parameterization.

Our next example discusses the popular softmax policy parameterization for tabular MDPs.

Example 5 (Softmax policies). Consider the setting of Example 4 but with a different policy parameterization.
Suppose policies are parameterized by θ ∈ Rn×(k−1) where for any state s, πθ(s) ∈ ∆k−1 is a probability
distribution whose components πθ(s) ≡ (πθ(1|s), · · · , πθ(k|s)) satisfy

πθ(1|s) =

1
1 + (cid:80)k
j=2 eθs,j

&

πθ(i|s) =

eθs,i
j=2 eθs,j

1 + (cid:80)k

i (cid:54)= 1.

Here, we have simpliﬁed our discussion by tracking k − 1 parameters per state, denoted (θs,2, · · · , θs,k), and
effectively ﬁxing θs,1 = 0. This convenient as it implies that each policy in the policy class corresponds to a
unique parameter vector.

The cost-function θ (cid:55)→ (cid:96)(θ) is coercive, since (cid:96)(πθ) ≥ (cid:80)

s ρ(s)R(πθ(s)) → ∞ as any component
θs,i → ∞. Lemma 2 therefore implies that gradient descent converges to a stationary point of θ (cid:55)→ (cid:96)(πθ). Our
goal is to show that any stationary point corresponds to an optimal policy.
First, note that with this relative entropy regularizer, the function π(cid:48)

(cid:55)→ B(π(cid:48) | ηπ, Jπ) which takes
the form in (17), is an extended-real-valued strongly-convex function [Boyd and Vandenberghe, 2004]. Its
domain dom(B) = {π(cid:48) ∈ Π : B(π(cid:48)|ηπ, Jπ) < ∞} is the set of strictly stochastic policies Π◦ = {π(cid:48) ∈ Π :
mins,i π(cid:48)
s,i > 0}. For any π, arg minπ(cid:48) B(π(cid:48)|ηπ, Jπ) ∈ Π◦, so the class of policies Π◦ is closed under policy
improvement. Clearly, θ (cid:55)→ πθ is an isomorphism9 between Rn·(k−1) and the set of strictly stochastic policies,
Π◦. Closure under policy improvement (Condition 1) follows from this.

Next, we show Condition 2.A, that θ (cid:55)→ B(πθ|ηπ, Jπ) has no suboptimal stationary points. The intuition
behind our argument is that optimizing the single period objective in (17) locally over π(cid:48) ∈ Π◦ is, in a suitable
sense, equivalent to optimizing locally over the parameter of a softmax policy. Since the arbitrary policy π is
ﬁxed throughout, we use the shorthand notation B(πθ) ≡ B(πθ | ηπ, Jπ). Suppose θ0 is a stationary point of
θ (cid:55)→ B(πθ). Then, Corollary 2 shows πθ0 is a stationary point of π(cid:48) (cid:55)→ B(π(cid:48)). Since B(π(cid:48)) is strictly convex
in π(cid:48), it has no suboptimal stationary points. Hence, πθ0 must be a minimizer of B.

In the next result, ∂πθ/∂θ is a Jacobian matrix. For completeness, a proof is given in Appendix E.3. It acts
a local change of coordinates. This can also be critical in practical policy gradient implementations [Kakade,
2002, Schulman et al., 2015].

Lemma 11 (Diffeomorphism). Given a softmax policy πθ as in Example (5) and any vector D such that
πθ + αD ∈ Π for sufﬁciently small α > 0, there exists a vector N such that (cid:2) ∂πθ

(cid:3) N = D.

∂θ

9That is, the function is one-to-one, meaning no two distinct choices of parameter yield the same policy, and onto, meaning that every

strictly stochastic policy can be represented by some choice of θ.

20

Corollary 2 (Correspondence of stationary points). If ∇θB(πθ) = 0, then (cid:104)∇πθ B(πθ) , π(cid:48) − πθ(cid:105) ≥ 0 for
each π(cid:48) ∈ Π.

Proof. Suppose otherwise and there exists π(cid:48) ∈ Π and a descent direction D = π(cid:48)−πθ with (cid:104)∇πθ B(πθ) , D(cid:105) <
0. By convexity of Π, D is feasible, i.e. πθ + αD = (1 − α)πθ + απ(cid:48) ∈ Π for α ∈ (0, 1). Lemma 11 therefore
applies and we can take N as in Lemma 11. Then, by the chain rule

(cid:104)∇θB(πθ) , N (cid:105) =

(cid:21)(cid:62)

(cid:42)(cid:20) ∂πθ
∂θ

(cid:43)

(cid:28)

∇πθ B (πθ) , N

=

∇πθ B (πθ) ,

(cid:20) ∂πθ
∂θ

(cid:21)

(cid:29)

N

= (cid:104)∇πθ B (πθ) , D(cid:105) < 0,

yielding a contradiction.

Therefore, under Conditions 1 and 2.A, Theorem 1 implies that θ (cid:55)→ (cid:96)(πθ) has no suboptimal stationary
points. Lemma 10 shows that solving this relative entropy regularized problem with a very small regularization
parameter λ will produce a near-optimal policy for the unregularized objective as well. It is worth emphasizing,
however, that our results in Theorem 1 do not apply to the case of softmax policies with unregularized ﬁnite
MDP (i.e if we take λ = 0). In that case the cost function θ (cid:55)→ (cid:96)(πθ) has no minimizer, since an optimal
policy is not strictly stochastic. Similarly, it has no stationary points. A minimum can be approached as
certain components of θ tend to inﬁnity, but is never attained. The choice of policy parameterization leads to a
degenerate optimization problem, even in a seemingly trivial problem with just a single state and single time
period (γ = 0).

Linear MDPs

Since their introduction in Jin et al. [2020], the class of linear MDPs has been widely used in theoretical
analysis of reinforcement learning. This model assumes instantaneous cost functions and transitions kernels
have a low-dimensional structure that ensures cost-to-go functions lie in a low dimensional function class. The
next example treats a regularized linear MDP, where a small entropy regularizer is added to the single-period
costs. This choice smooths the problem, making it easy to apply and analyze policy gradient methods.

Example 6 (Entropy regularized linear MDPs). As in Examples 3, 4, let A = ∆k−1 be the set probability
distributions over k deterministic actions. The standard basis vectors e1, · · · , ek correspond to deterministic
actions. Suppose there exists a feature map φ : S × A → Rm+1, a parameter β ∈ Rm+1, and signed measures
µ0, · · · , µm over S such that for each state s ∈ S, action distribution a ∈ A and subset M ⊂ S,

g(s, a) =

m
(cid:88)

j=0

φj(s, a)βj

and P (M|s, a) =

m
(cid:88)

j=0

(cid:90)

φj(s, a)

µi(ds(cid:48)).

M

In keeping with the interpretation of an action a as encoding a distribution over a set of k deterministic actions,
we assume φj(s, a) = (cid:80)k

i=1 φj(s, ei)ai for j ∈ {1, · · · , m}.
To smooth the problem, we include a pair of basis elements (φ0, µ0) satisfying:

φ0(s, a) = R(a) := λDKL(U ||a) and µ0(M) = 0 ∀s ∈ S, a ∈ A, M ⊂ S.

Take β0 = 1. As in Example 4, one should interpret this as modifying the original objective of the MDP
by adding the relative-entropy term R(a) to the single period cost function. This is useful for us because it
ensures that the optimal policy is a smooth function of problem parameters (like β). Lemma 10 shows that
when λ is very small, a policy that is optimal with respect to this modiﬁed objective will be near optimal for
the original objective.

The inherent low-dimensional structure of linear MDPs ensures that each state action value function lies

in a low dimensional class of functions spanned by the basis vectors φ0, · · · , φd. In particular,

Qπ(s, a) =

m
(cid:88)

j=0

φj(s, a)βj + γ

m
(cid:88)

j=0

φj(s, a)

(cid:90)

S

Jπ(s(cid:48))µj(ds(cid:48)) = R(a) +

m
(cid:88)

j=1

φj(s, a)θj

(18)

21

where we choose θj = βj + γ (cid:82)

S Jπθ (s(cid:48))µj(ds(cid:48)).

Motivated by (18), deﬁne the low dimensional class of state-action value function of the form

Qθ(s, a) = a(cid:62)Φ(s)θ + R(a).

where θ ∈ Rm and Φ(s) ∈ Rk×m has elements Φ(s)ij = φj(s, ei). Take πθ(s) = arg mina∈∆k−1 Qθ(s, a)
to be the policy induced by minimizing such a state-action value function.

Lemma 12. The class of policies ΠΘ = {πθ : θ ∈ Rm} satisﬁes Condition 1 (closure) and for any π ∈ Π,
θ (cid:55)→ B (πθ | ηπ, Jπ) satisﬁes Condition 2.A (no sub-optimal stationary points).

Proof sketch. To establish the closure condition, ﬁx an arbitrary policy π. By (18), one can pick θ+ such that
Qπ = Qθ+
. Then, by construction, πθ+(s) is a policy iteration update to π. Now, we want to show that the
single period objective θ (cid:55)→ B(πθ | ηπ, Jπ) does not have any suboptimal stationary points. Given θ, take
θα = θ + α(θ+ − θ). We show in Appendix E.7 that

d
dα

B(πθα | ηπ, Jπ) =

(cid:90) d
dα

Qπ(s, πθα (s))ηπ(ds) =

(cid:90) d
dα

Qθ+

(s, πθα (s))ηπ(ds) < 0.

. Precisely, πθ+(s) ∈ arg mina∈A Qθ+

To see this intuitively, consider some ﬁxed s. The policy iteration update to π, is a greedy policy deﬁned
by optimizing Qπ = Qθ+
(s, a) minimizes an entropy regularized
linear cost function over the probability simplex. The current (sub-optimal) policy πθ optimizes a different
(incorrect) cost function. It is a greedy policy to Qθ, with πθ(s) ∈ arg mina∈A Qθ(s, a). Changing the
optimization objective marginally, from Qθ(s, ·) towards the (correct) cost function Qθ+
(s, ·), reduces the
cost of the selected action.

Remark 4. Unlike other examples, here a parameterized policy is deﬁned implicitly as the solution of a
certain optimization problem. One way to derive gradients of this policy is to use implicit differentiation, as in
our proof in Appendix E.7.

Linear quadratic control with linear policies.

We brieﬂy revisit the linear quadratic control example in Section 4. As discussed there, this example technically
falls outside the scope of our formulation because single-period costs are not uniformly bounded. At the same
time, the proofs of Theorems 1 and 2 apply essentially without modiﬁcation to the class of stable policies. In
particular, by Corollary 1, because the policy class is closed and the single period weighted PI objective is
quadratic, the policy gradient cost function is gradient dominated with degree two. This may help illuminate
the gradient dominance result previously proved directly by Fazel et al. [2018]. Fazel et al. [2018] also
give a detailed analysis of the rate of convergence attained by several speciﬁc algorithms. Despite gradient
dominance, a subtlety that complicates such convergence analysis is that the smoothness properties of the loss
function in Lemma 4 only hold over sublevel sets. Smoothness over sublevel sets is sufﬁcient to apply Lemma
2, implying convergence of gradient descent to the global optimum. But as currently stated, the convergence
rate in Lemma 3 does not apply since that lemma assumes a global bound on the norm of the Hessian. We
believe Lemma 3 can be extended along the lines of Lemma 2 to give a geometric convergence rate under
constant stepsizes chosen to ensure that iterates always remain in sublevel sets.

Optimal stopping with threshold policies.

We now turn to an example with a structured policy class.

22

Example 7 (Optimal Stopping). We formulate the optimal stopping problem as a reward maximization
problem10. In each round the agent observes a state variable xt taking values in a ﬁnite set X , which evolves
according to an uncontrolled Markov chain with time-homogeneous transition probabilities P(xt+1 = x(cid:48)|xt =
x) = p(x(cid:48)|x). Conditioned on xt, the agent receives an offer yt ∈ R drawn i.i.d from some probability density
function qxt(·). We assume that for each x ∈ X , qx(·) has support {y ∈ R : qx(y) > 0} = [ymin, ymax] where
0 < ymin < ymax < ∞ and has a continuous derivative throughout its support11. Set Y = [ymin, ymax]. If
the offer is accepted in round t, the process terminates and the agent accrues a reward of γtyt. Rejecting the
offer in any round is costless. The agent’s objective is to maximize the expected revenue.

This problem can be formalized as a Markov decision process with the state-space S = SC∪{τ }, consisting
of a set of continuation states SC = (X ×Y) and a terminal state τ that is costless (g(τ, a) = 0) and absorbing
(P ({τ }|τ, a) = 1). For convenience, we assume the initial distribution ρ assigns zero probability of trivial
problem instances that start in the terminal state. We also assume that ρ factorizes over continuation states as
ρ(x, dy) = ν(x)qx(y)dy where ν(x) > 0 for every x ∈ X . The action a = 0 corresponds to accepting the
offer and terminating while action a = 1 continues the game by transitioning to a new state with probabilities
given by

P [st+1 = (x(cid:48), dy(cid:48)) | st = (x, y), a = 1] = p(x(cid:48)|x)qx(cid:48)(y(cid:48))dy(cid:48).
We consider the class of threshold policies where the vector θ ∈ Θ := [ymin, ymax]|X | speciﬁes one stopping
threshold per context. The policy πθ(x, y) = 1 (y < θx) rejects all offers below θx.

It is easy to show closure of the policy class. For a threshold policy π, a policy iteration step updates
to yet another threshold policy which accepts an offer y in x if and only if it exceeds the continuation
value, cπ(x) = γE[Jπ(xt+1, yt+1)|xt = x, at = 1]. See Appendix E.5 for details where we also show that
Conditions 0, 1 and 2.A hold. It is also possible to prove gradient dominance and smoothness results that, due
to Lemma 3, imply convergence rates for policy gradient methods. We omit those results for brevity, but the
interested reader can ﬁnd precise statements in the Appendix and the proofs in the technical report [Bhandari
and Russo, 2021].

Finite horizon inventory control with base stock policies.

We now apply Theorem 3 to a ﬁnite horizon inventory control problem with the class of multi-period base
stock policies. Kunnumkal and Topaloglu [2008] previously showed through a somewhat intricate analysis that
a stochastic approximation algorithm converges to the optimal policy, despite non-convexity of the objective.
See also the follow up work by Huh and Rusmevichientong [2013].

One unconventional feature of our formulation is that it involves both time periods t ∈ (0, 1, · · · ) and
“stages” h0, · · · , ht, · · · , H. This is consistent with the way we formulated ﬁnite-horizon problems in Section
5.4. We have in mind an inventory control problem that is optimized in simulation. Consistent with Assumption
3, the initial stage is random, meaning that different simulated scenarios may begin with a different number of
periods remaining. This effectively ensures the policy is optimized from a diverse set initial scenarios. This
appears to be necessary for general ﬁnite horizon MDPs, but an exploratory initial distribution may not be
necessary due to the special structure of this particular problem.

Example 8 (Finite horizon inventory control). Consider a multi-period inventory control problem (also
popularly known as the multi-period newsvendor problem) with backlogged demands. In time period t, the
seller selects a non-negative quantity at ≥ 0 of inventory to order on the basis of the current inventory level
xt ∈ R. After ordering inventory, a random i.i.d demand wt is realized and the inventory level evolves as:
xt+1 = xt + at − wt. We assume the demand distribution has a twice differentiable12 PDF supported over
some bounded set [0, wmax]. Negative inventory levels correspond to backlogged demand that is ﬁlled when
additional inventory becomes available.

10One could imagine costs to be the negative of reward in order to be consistent with the formulation in Section 2.
11To be rigorous, at the boundaries it has a directional derivative only.
12That this PDF is twice differentiable is used to simplify a proof of Condition 0 given in the appendix. We give a remark there on how

the proof could be completed without that condition.

23

The seller begins at some stage h0 in the initial period and the stage advances by one in each time period,
i.e. ht+1 = ht + 1, until a ﬁnal state H is reached. The seller’s objective is to minimize total expected cost
over the horizon,

(cid:34)H−h0(cid:88)

E

t=0

γt(cat + b max{xt + at − wt, 0} + p max{−xt − at + wt, 0})

(19)

(cid:35)

where c, b, p > 0 denote the per unit costs of ordering, holding and backlogging items, respectively. We assume
that p > c. Otherwise, the optimal policy may never order in any period. It is well known that a (multi-period)
base-stock policy is optimal for this setting [Bertsekas, 1995].

Let st = (xt, ht) denote the state at time t, encoding all information needed to make an optimal ordering
decision. A base-stock policy depends on a vector of target inventory levels (θ1, · · · , θH ) ∈ RH
+ At state
(x, h), the policy orders inventory πθ((x, h)) = max{0, θh − x}. That is, it orders enough inventory to reach a
target level θh, whenever feasible. We restrict to the policy class ΠΘ = {πθ : θ ∈ Θ} with bounded parameter
space Θ = [0, 2wmax]H . With a selection of θht = 2wmax, the seller can ensure xt+1 ≥ wmax, which is
large enough to meet all demand with probability one. Inventory levels above this are clearly suboptimal, due
to their excessive holding costs. As the demand distribution is bounded and the target inventory level is in
[0, 2wmax] the feasible inventory levels at every period are trivially bounded in I = [−wmax, 2wmax], as long
as the initial inventory level does not fall outside this set. The initial state (x0, h0) is drawn from the initial
distribution ρ. We assume that h0 has a positive probability of taking on any value in {1, · · · , H} and that,
conditioned on h0, the distribution of x0 has a twice differentiable PDF supported on I.

This is clearly an example of a ﬁnite horizon problem with a non-stationary policy class and hence has
the structure noted in Condition 3. Condition 0 follows essentially from using the Leibniz rule along with
the fact that base-stock policies are differentiable everywhere except at a single point. Condition 4 follows
from a classic result in inventory control theory which shows that the optimal state-action cost-to-go function
Q∗(s, a) is convex in a [Bertsekas, 1995]. Details are given in Appendix E.

7 Bounds on the concentrability coefﬁcient

To show a gradient dominance result, we need to relate the optimality gap to the magnitude of errors in
the Bellman equation. In Section 5.3, we deﬁned the effective concentrability coefﬁcient κρ for the set of
cost-to-go functions JΘ = {Jπθ : θ ∈ Θ} to be the smallest scalar such that

(cid:107)J − J ∗(cid:107)1,ρ ≤

κρ
(1 − γ)

(cid:107)J − T J(cid:107)1,ρ ∀J ∈ JΘ.

(20)

To motivate our deﬁnition, note that whenever the Bellman operator is a contraction in some norm (cid:107) · (cid:107) with
modulus γ, the following inequality holds: (cid:107)J − J ∗(cid:107) ≤ (1 − γ)−1(cid:107)J − T J(cid:107) (see (26) in Appendix D). For
bounded cost problems, T is a contraction in the maximum norm and can sometimes also be shown to be
contractive in a weighted norm which reﬂects state relevance. See Bertsekas [1995] or Puterman [2014].
Essentially, the constant κρ enables the above inequality in the weighted norm (cid:107) · (cid:107)1,ρ, in which T is typically
not contractive.

The focus on this norm is motivated by two factors. First, the optimality gap can be written as (cid:96)(πθ) −
minπ∈Π (cid:96)(π) = (1 − γ)(cid:107)Jπθ − J ∗(cid:107)1,ρ, mirroring the left hand side of (20) modulo a constant factor. Second,
the policy gradient theorem in Lemma 6 reveals a natural dependence on the errors in Bellman’s equation
weighted under the state occupancy measure ηπ. As ηπ (cid:23) (1 − γ)ρ, it makes sense to measure the Bellman
errors in (cid:107) · (cid:107)1,ρ. It is worth noting that, because our deﬁnition of κρ depends only on the subclass of cost-to-go
functions JΘ, it allows for stronger bounds when these functions obey certain regularity properties. We now
provide several useful upper bounds on κρ. See Appendix D.3 for proof details.

Theorem 4. The following results apply under the general problem formulation in Section 2.

24

(a) If S is ﬁnite, then κρ ≤ 1/ (mins∈S ρ(s)).

(b) Let π∗ denote any optimal stationary policy. Then, κρ ≤

(cid:13)
(cid:13)
(cid:13)

dηπ∗
dρ

(cid:13)
(cid:13)
(cid:13)∞

.

(c) The bound κρ ≤ C/c holds if T is a contraction with modulus γ in a norm (cid:107) · (cid:107) that satisﬁes

c(cid:107)J(cid:107) ≤ (cid:107)J(cid:107)1,ρ ≤ C(cid:107)J(cid:107)

∀J ∈ JΘ.

(21)

The bound in part (a) is simple and can be derived as a special case of the result in part (b). The bound in
part (b) depends on the worst-case likelihood ratio between the state occupancy measure under the optimal
policy and the initial distribution. Note, dηπ∗
dρ is the Radon-Nikodym derivative term, which exists because of
Assumption 1. Put differently, this result implies that κρ ≤ C if ηπ∗ (M) ≤ Cρ(M) for every measurable
set M ⊂ S. This is, essentially, a restatement of a key observation in Kakade and Langford [2002]. Such
distributional mismatch terms also appears in the works of Agarwal et al. [2020], Scherrer and Geist [2014].
The result in part (c) gives an alternative approach to bounding κρ. It is potentially useful for many
problems where the Bellman operator is a contraction with respect to a certain weighed norm, as it suggests ρ
should be chosen in a manner which aligns with that norm’s state weighting. The optimal stopping problem
is one such special case where it can be shown that T is a contraction in (cid:107) · (cid:107)1,µ, where µ is the stationary
distribution of the underlying Markov chain – assuming it is never interrupted by stopping. Choosing ρ = µ
implies κρ ≤ 1 using (21). In practical problems, one could easily sample initial states from ρ by simulating
this Markov process.

Lemma 13. For the optimal stopping problem in Example 7, consider a policy πC that never stops, i.e.
πC(s) = 1 for all s ∈ SC. Let µ be a stationary distribution of the induced Markov process, meaning
µ(M) = (cid:82) P (M|s(cid:48), 1)µ(ds(cid:48)) for any M ⊂ S. Then, choosing ρ = µ implies κρ ≤ 1.

The LQ control problem technically falls outside the scope of our general formulation as per-stage costs
are not bounded. Therefore, we restrict our attention to the cost-to-go functions corresponding to stable
linear policies. For this result, we are able to leverage certain regularity properties of this which imply better
bounds than the generic bound implied by part (b) of Theorem 4. In particular, because the class of cost-to-go
functions induced by linear policies are quadratic, we need only the initial distribution to explore the basis of
the state space sufﬁciently, rather than requiring it to almost perfectly mimic the steady state distribution of the
(unknown) optimal policy.

Lemma 14. Consider the LQ control problem formulated in Example 2. Let θ∗ ∈ Rn×k denote the parameter
of an optimal policy and deﬁne Σρ := Eρ

(cid:3). Then,

(cid:3) and Σηπθ∗ := Eηπθ∗

(cid:2)s0s(cid:62)

(cid:2)s0s(cid:62)

0

0

(cid:107)J − J ∗(cid:107)1,ρ ≤

κ
(1 − γ)

(cid:107)J − T J(cid:107)1,ρ

∀J ∈ {Jπθ : θ ∈ ΘS}

when

κ =

(cid:16)

λmax

Σηπθ∗
λmin (Σρ)

(cid:17)

.

(22)

In the claim above, Σρ is the second moment matrix of the initial state distribution, which we assumed to
be non-singular, and Σηπθ∗ is the second moment matrix of the state-occupancy measure under the optimal
policy. That is, Σηπθ∗ = (1 − γ) (cid:80)∞
t ] where st = [A + Bθ∗]ts0 evolves according to linear
dynamics from the initial state s0 ∼ ρ. Since the optimal policy is stable, this inﬁnite sum converges to a
ﬁnite limit. Generally, this term is large when the system is only barely stable even under the optimal policy,
meaning the spectral radius of

γ(A + Bθ∗) is close to one.

t=0 γtE[sts(cid:62)

√

25

8 Closure under approximate policy improvement.

So far, our results crucially depend on the closure property of the policy class, which applies to many classical
dynamic programming problems with structured policy classes. A natural question to ask is whether we
can relax this closure condition. In this section, we present results for the case where our policy class is
only approximately closed under improvement. One would expect expressive policy classes such as those
parameterized by a deep neural network, a Kernel method [Rajeswaran et al., 2017], or using state aggregation
Bertsekas [2019], Ferns et al. [2004], Singh et al. [1994] to follow this condition. We discuss the example of
state-aggregation at the end of this section. Recall that Π denotes the class of all stationary policies and ΠΘ
denotes the parameterized policy class over which we search.

Condition 5 (Closure under approximate policy improvement). There exists (cid:15) ≥ 0 such that for every π ∈ ΠΘ,

min
π+∈ΠΘ

B(π+|ηπ, Jπ) ≤ min
π(cid:48)∈Π

B(π(cid:48)|ηπ, Jπ) + (cid:15).

(23)

If ΠΘ were closed under policy improvement steps, the approximation error would be zero since there
would exist a π+ ∈ ΠΘ such that Tπ+Jπ(s) = T Jπ(s) almost surely for s drawn from ηπ. Condition 5
measures the deviation from this ideal case, in a norm that weights states by the discounted state occupancy
measure under the current policy. We refer to (cid:15) as the inherent Bellman error of the policy class. We discuss
this condition further after the next theorem.

We state our formal result in Theorem 5 below to show that for approximately closed policy classes, any
stationary point of the policy gradient objective is nearly optimal, where this optimality gap is a function of the
inherent Bellman error in Condition 5. Our result is reminiscent of results in the study of approximate policy
iteration methods, pioneered by Antos et al. [2008], Bertsekas [2011], Bertsekas and Tsitsiklis [1996], Munos
[2003], Munos and Szepesv´ari [2008], among others. The primary differences are that (1) we directly consider
an approximate policy class whereas that line of work considers the error in parametric approximations to the
Q-function and (2) we make a speciﬁc link with the stationary points of a policy gradient method. Recall the
deﬁnition of κρ in (13), which relates the optimality gap to errors in the Bellman equation weighted under ρ.

Theorem 5. Suppose Conditions 0, 2.A and 5 hold. Then, (cid:96) is continuously differentiable and any stationary
point θ of (cid:96)(·) satisﬁes,

(cid:96)(πθ) − (cid:96)(π∗) ≤

κρ
(1 − γ)

· (cid:15)

Proof. Suppose θ is a stationary point of (cid:96) : Θ → R. Under Conditions 2.A and 5, we have
(cid:90)

(cid:18)(cid:90)

(cid:19)

(cid:15) ≥ min
π+∈ΠΘ

B(π+|ηπθ , Jπθ ) − min
π(cid:48)∈Π

B(π(cid:48)|ηπθ , Jπθ ) = min
π+∈ΠΘ
(cid:90)

[Tπ+Jπθ ] dηπθ

−

[T Jπθ ] dηπθ

=

[Jπθ − T Jπθ ] dηπθ

= (cid:107)Jπθ − T Jπθ (cid:107)1,ηπθ

,

where the second equality follows from Lemma 7 and the ﬁnal equality uses that Jπθ (cid:23) T Jπθ for any πθ ∈ ΠΘ.
Then, we have

(cid:96)(πθ) − min

π

(cid:96)(π) = (1 − γ)

(cid:90)

[Jπθ − J ∗] dρ = (1 − γ)(cid:107)Jπθ − J ∗(cid:107)1,ρ

≤ κρ(cid:107)Jπθ − T Jπθ (cid:107)1,ρ

≤

≤

κρ
(1 − γ)
κρ · (cid:15)
(1 − γ)

(cid:107)Jπθ − T Jπθ (cid:107)1,ηπθ

.

The ﬁrst inequality follows from the deﬁnition of κρ while the second uses that ηπθ (cid:23) (1 − γ)ρ.

26

Intuitively, it seems we might expect the inherent Bellman error in condition (5) to be small if the policy
class is sufﬁciently rich. Indeed, it holds with (cid:15) = 0 when ΠΘ contains all possible policies. For the linear MDP
problem in Example 6, the condition also holds with (cid:15) = 0 once the policy class contains the low-dimensional
family referenced in Lemma (12). The situation is not so simple, however. For a given policy π, it is easier to
satisfy the policy improvement property (23) when ΠΘ contains more policies, but if ΠΘ is larger we require
(23) to hold for more choices of π. For this reason, the inherent Bellman error does not necessarily reduce
monotonically as the policy class grows.

Let us explore the illustrative example of state aggregation, which has a long history of being employed in
reinforcement learning [Tsitsiklis and Van Roy, 1996, Van Roy, 2006]. Consider a continuous state, ﬁnite
action MDP. When the cost functions and transitions dynamics are appropriately smooth, we expect there is
little beneﬁt to distinguishing between states that are very close together. A state aggregated policy is one that
partitions the state space into disjoint regions and assigns an action distribution to each region. Now consider
an increasing sequence of policy classes generated by taking progressively ﬁner partitions of the state space.
Does condition (5) hold once the partition is sufﬁciently ﬁne? The next lemma provides a coarse upper bound
conﬁrming that inherent Bellman error is small once the policy class is rich enough. To see this, imagine a
sequence of state aggregation rules where the distance (cid:107)s − φ(s)(cid:107) between a state and its representer tends to
zero uniformly. If g(s, a) and P (·|s, a) are Lipshitz continuous in s (in total-variation metric for P ), then (24)
tends to zero.

Lemma 15. (Approximate closure of state-aggregated policies) Suppose S = ∪m
i=1Si is the union of m
disjoint subsets, A = ∆k−1 is the set of probability distributions over k elements, and (cid:107)g(cid:107)∞ ≤ 1. From
each Si, pick a representer ¯si ∈ Si and deﬁne φ : S → Si such that φ(s) = ¯si if and only if s ∈ Si. For
any θ = (θ1, · · · , θm) ∈ Θ := ∆k−1 × · · · × ∆k−1, deﬁne πθ(s) = θφ(s). The class of policies ΠΘ satisﬁes
Condition 5 with

(cid:104)

(cid:15) = 2

sup
s∈S,a∈A

|g(s, a) − g(φ(s), a)| +

γ
1 − γ

(cid:107)P (· | s, a) − P (· | φ(s), a)(cid:107)TV

(cid:105)

.

(24)

Proof. Fix π ∈ ΠΘ. Applying (24) with the deﬁnition in (6) ensures |Qπ(s, a) − Qπ(φ(s), a)| ≤ (cid:15)/2
for all s ∈ S and a ∈ A. Take π+ to a be an unconstrained policy iteration update, with π+(s) ∈
arg mina∈A Qπ(s, a) , and πθ to be a state-aggregated policy which satisﬁes πθ(s) ∈ arg mina∈A Qπ(φ(s), a).
Then,

Qπ(s, πθ(s)) ≤ Qπ(φ(s), πθ(s)) + (cid:15)/2 = min
a∈A

Qπ(φ(s), a) + (cid:15)/2 ≤ min
a∈A

Qπ(s, a) + (cid:15) = Qπ(s, π+(s)) + (cid:15).

Taking an expectation over s ∼ ηπ and using the deﬁnition of B (· | ηπ, Jπ) establishes that B(πθ|ηπ, Jπ) ≤
B(π+|ηπ, Jπ) + (cid:15).

9 Conclusion

In this paper, we uncover structural properties of the underlying MDP which guarantee that policy gradient
methods converge to globally optimal solutions as well as characterize their convergence rates, even though
the optimization objective is non-convex. Our results rely on a key connection with policy iteration, a classic
dynamic programming algorithm which solves a single period optimization problem at every step that often has
special structure. We show how the policy gradient objective inherits this nice structure, making it amenable
for gradient based algorithms to ﬁnd the optimal policy at a fast rate. There are a number of research directions
to extend our work, including results for the function approximation setting with neural network or kernel
based parameterization of the cost-to-go function as well as designing principled exploration approaches which
can be efﬁciently combined with policy gradient methods.

27

References

Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation with policy gradient methods in
markov decision processes. In J. Abernethy and S. Agarwal, editors, Proceedings of Thirty Third Conference on Learning Theory,
volume 125, pages 64–66. (PMLR), 2020.

Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approximate local minima faster than gradient
descent. In P. McKenzie H. Hatami and V. King, editors, Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of
Computing, pages 1195–1199. (Association for Computing Machinery, NY, USA), 2017.

Andr´as Antos, Csaba Szepesv´ari, and R´emi Munos. Learning near-optimal policies with bellman-residual minimization based ﬁtted

policy iteration and a single sample path. Machine Learning, 71(1):89–129, 2008.

Søren Asmussen and Peter W Glynn. Stochastic simulation: algorithms and analysis, volume 57. (Springer Science & Business Media,

Berlin, Germany), 2007.

Mohammad Gheshlaghi Azar, R´emi Munos, Mohammad Ghavamzadeh, and Hilbert Kappen. Reinforcement learning with a near optimal

rate of convergence. Technical report, INRIA Lille, 2011. HAL Id: inria-00636615v2.

Andrew Bagnell, Sham M Kakade, Jeff G Schneider, and Andrew Y Ng. Policy search by dynamic programming. In S. Thrun, L. Saul,
and B. Sch¨olkopf, editors, Advances in neural information processing systems, volume 16, pages 831–838. (MIT Press, MA, USA),
2004.

Jonathan Baxter and Peter L Bartlett. Inﬁnite-horizon policy-gradient estimation. Journal of Artiﬁcial Intelligence Research, 15(1):

319–350, 2001.

Amir Beck. Convergence rate analysis of gradient based algorithms. PhD thesis, Tel-Aviv University, 2002.
Amir Beck. First-order methods in optimization, volume 25. (SIAM, PA, USA), 2017.
Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations

Research Letters, 31(3):167–175, 2003.

Dimitir P Bertsekas and Steven Shreve. Stochastic optimal control: the discrete-time case. (Athena Scientiﬁc, MA, USA), 1978.
Dimitri P Bertsekas. Dynamic programming and optimal control. (Athena scientiﬁc, MA, USA), 1995.
Dimitri P Bertsekas. Nonlinear programming. Journal of the Operational Research Society, 48(3):334–334, 1997.
Dimitri P Bertsekas. Approximate policy iteration: A survey and some new methods. Journal of Control Theory and Applications, 9(3):

310–335, 2011.

Dimitri P Bertsekas. Feature-based aggregation and deep reinforcement learning: A survey and some new implementations. IEEE/CAA

Journal of Automatica Sinica, 6(1):1–31, 2019.

Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming, volume 5. (Athena Scientiﬁc, MA, USA), 1996.
Jalaj Bhandari and Daniel Russo. Global optimiality guarantees for policy gradient methods: Technical report with supplementary

materials. 2021. URL https://djrusso.github.io/docs/policy_grad_optimality_EC.pdf.

Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. In D. Lee,
M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29, pages
3873–3881. (Curran Associates, NY, USA), 2016.

David Blackwell. Discounted dynamic programming. The Annals of Mathematical Statistics, 36(1):226–235, 1965.
Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint, volume 48. (Springer, Berlin, Germany), 2009.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Jingjing Bu, Afshin Mesbahi, Maryam Fazel, and Mehran Mesbahi. Lqr through the lens of ﬁrst order methods: Discrete-time case. arXiv

preprint arXiv:1907.08921, 2019.

Michael Caramanis and George Liberopoulos. Perturbation analysis for the design of ﬂexible manufacturing system ﬂow controllers.

Operations Research, 40(6):1107–1125, 1992.

Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for nonconvex optimization. SIAM Journal on

Optimization, 28(2):1751–1772, 2018.

Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In International Conference on

Machine Learning, pages 1042–1051. PMLR, 2019.

Damek Davis and Benjamin Grimmer. Proximally guided stochastic subgradient method for nonsmooth, nonconvex problems. SIAM

Journal on Optimization, 29(3):1908–1930, 2019.

Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, and Jason D Lee. Stochastic subgradient method converges on tame functions.

Foundations of computational mathematics, 20(1):119–154, 2020.

Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly
convex composite objectives. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger, editors, Advances in
neural information processing systems, volume 27, pages 1646–1654. (Curran Associates, NY, USA), 2014.

Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with quadratic activation. In Jennifer Dy and
Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 1329–1338.
(PMLR), 2018.

28

Simon S. Du, Sham M. Kakade, Ruosong Wang, and Lin F. Yang. Is a good representation sufﬁcient for sample efﬁcient reinforcement
learning? In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=
r1genAVKPB.

Lawrence C Evans. An introduction to mathematical optimal control theory. Lecture Notes, University of California, Department of

Mathematics, Berkeley, 2005.

Yuguang Fang, Kenneth A Loparo, and Xiangbo Feng. Inequalities for the trace of matrix product. IEEE Transactions on Automatic

Control, 39(12):2489–2490, 1994.

Amir-massoud Farahmand, Csaba Szepesv´ari, and R´emi Munos. Error propagation for approximate policy and value iteration. In
J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems,
volume 23, pages 568–576. (Curran Associates, NY, USA), 2010.

Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy gradient methods for the linear quadratic
regulator. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning,
volume 80, pages 1467–1476. (PMLR), 2018.

Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics for ﬁnite markov decision processes. In C. Meek, M. Chickering, and
J. Halpern, editors, Proceedings of the 20th conference on Uncertainty in artiﬁcial intelligence, pages 162–169. (AUAI Press, Virginia,
USA), 2004.

Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, and Sergey Levine. Variational inverse control with events: A general framework
for data-driven reward deﬁnition. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors,
Advances in Neural Information Processing Systems, volume 31, pages 8538–8547. (Curran Associates, NY, USA), 2018.

Michael C Fu. Gradient estimation. Handbooks in operations research and management science, 13:575–616, 2006.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic gradient for tensor decomposition. In
P. Gr¨unwald, E. Hazan, and S. Kale, editors, Proceedings of The 28th Conference on Learning Theory, volume 40, pages 797–842.
(PMLR), 2015.

Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In D. Lee, M. Sugiyama, U. Luxburg,
I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29, pages 2973–2981. (Curran
Associates, NY, USA), 2016.

Matthieu Geist, Bilal Piot, and Olivier Pietquin. Is the bellman residual a bad proxy? In I. Guyon, U.V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30, pages 3208–3217.
(Curran Associates, NY, USA), 2017.

Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision processes. In ICML 2019-Thirty-sixth

International Conference on Machine Learning, 2019.

Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on

Optimization, 23(4):2341–2368, 2013.

Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic programming. Mathematical

Programming, 156(1):59–99, 2016.

Paul Glasserman and Yu-Chi Ho. Gradient estimation via perturbation analysis, volume 116. (Springer Science & Business Media,

Berlin, Germany), 1991.

Paul Glasserman and Sridhar Tayur. Sensitivity analysis for base-stock levels in multiechelon production-inventory systems. Management

Science, 41(2):263–281, 1995.

Ivo Grondman, Lucian Busoniu, Gabriel AD Lopes, and Robert Babuska. A survey of actor-critic reinforcement learning: Standard and
natural policy gradients. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 42(6):1291–1307,
2012.

Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta,

Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018.

On´esimo Hern´andez-Lerma and Jean B Lasserre. Discrete-time Markov control processes: basic optimality criteria, volume 30. (Springer

Science & Business Media, Berlin, Germany), 2012.

G Hewer. An iterative technique for the computation of the steady state gains for the discrete optimal regulator. IEEE Transactions on

Automatic Control, 16(4):382–384, 1971.

Woonghee Tim Huh and Paat Rusmevichientong. Online sequential optimization with biased gradients: theory and applications to

censored demand. INFORMS Journal on Computing, 26(1):150–159, 2013.

Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In

Advances in neural information processing systems, pages 8571–8580, 2018.

Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efﬁciently. In D. Precup and
Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1724–1732. (JMLR. org),
2017.

Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efﬁcient reinforcement learning with linear function approximation.

In Conference on Learning Theory, pages 2137–2143. PMLR, 2020.

29

Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In C. Sammut and A. G. Hoffmann,
editors, Proceedings of the Nineteenth International Conference on Machine Learning, volume 2, pages 267–274. (Morgan Kaufmann
Publishers, CA, USA), 2002.

Sham M Kakade. A natural policy gradient. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in neural information

processing systems, volume 14, pages 1531–1538. (MIT Press, MA, USA), 2002.

Nathan Kallus and Masatoshi Uehara. Statistically efﬁcient off-policy policy gradients. In Hal Daum´e III and Aarti Singh, editors,
Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research,
pages 5089–5100. PMLR, 13–18 Jul 2020a.

Nathan Kallus and Masatoshi Uehara. Doubly robust off-policy value and gradient estimation for deterministic policies. In H. Larochelle,
M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages
10420–10430. Curran Associates, Inc., 2020b.

Rudolf Emil Kalman et al. Contributions to the theory of optimal control. Bol. soc. mat. mexicana, 5(2):102–119, 1960.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-gradient methods under the polyak-
łojasiewicz condition. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 795–811.
(Springer, Berlin, Germany), 2016.

Kenji Kawaguchi. Deep learning without poor local minima. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors,

Advances in neural information processing systems, volume 29, pages 586–594. (Curran Associates, NY, USA), 2016.

D. Kleinman. On an iterative technique for riccati equation computations. IEEE Transactions on Automatic Control, 13(1):114 – 115,

1968.

Sven Koenig and Reid G Simmons. Complexity analysis of real-time reinforcement learning. In R. Fikes and W. Lehnert, editors,

Proceedings of the Eleventh National Conference on Artiﬁcial Intelligence, pages 99–107. (AAAI Press, CA, USA), 1993.

Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information processing systems, pages 1008–1014,

2000.

Sumit Kunnumkal and Huseyin Topaloglu. Using stochastic approximation methods to compute optimal base-stock levels in inventory

control problems. Operations Research, 56(3):646–664, 2008.

Pierre L’Ecuyer and Peter W Glynn. Stochastic optimization by simulation: Convergence proofs for the gi/g/1 queue in steady-state.

Management Science, 40(11):1562–1578, 1994.

Pierre L’Ecuyer, Nataly Giroux, and Peter W Glynn. Stochastic optimization by simulation: numerical experiments with the m/m/1 queue

in steady-state. Management science, 40(10):1245–1261, 1994.

Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only converges to minimizers. In V. Feldman,
A. Rakhlin, and O. Shamir, editors, 29th Annual Conference on Learning Theory, volume 49, pages 1246–1257. (PMLR), 2016.
Timoth´ee Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat, and Natalia D´ıaz-Rodr´ıguez. Continual learning for

robotics: Deﬁnition, framework, learning strategies, opportunities and challenges. Information Fusion, 58:52–68, 2020.

Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efﬁciency of training neural networks. In Z. Ghahramani,
M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger, editors, Advances in neural information processing systems, volume 27,
pages 855–863. (Curran Associates, NY, USA), 2014.

Peter Marbach and John N Tsitsiklis. Simulation-based optimization of markov reward processes. IEEE Transactions on Automatic

Control, 46(2):191–209, 2001.

Matthew S Maxwell, Shane G Henderson, and Huseyin Topaloglu. Tuning approximate dynamic programming policies for ambulance

redeployment via direct search. Stochastic Systems, 3(2):322–361, 2013.

Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine learning. Journal of

Machine Learning Research, 21(132):1–62, 2020.

R´emi Munos. Error bounds for approximate policy iteration. In T. Fawcett and N. Mishra, editors, Proceedings of the Twentieth

International Conference on International Conference on Machine Learning, pages 560–567. (AAAI Press, CA, USA), 2003.

R´emi Munos. Error bounds for approximate value iteration. In Proceedings of the National Conference on Artiﬁcial Intelligence,

volume 20, page 1006. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2005.

R´emi Munos. Performance bounds in L p-norm for approximate value iteration. SIAM Journal on Control and Optimization, 46(2):

541–561, 2007.

R´emi Munos and Csaba Szepesv´ari. Finite-time bounds for ﬁtted value iteration. Journal of Machine Learning Research, 9(27):815–857,

2008.

Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global performance. Mathematical Programming, 108

(1):177–205, 2006.

Ian Osband, Daniel Russo, and Benjamin Van Roy. (More) efﬁcient reinforcement learning via posterior sampling. In C. J. C. Burges,
L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26,
pages 3003–3011. (Curran Associates, NY, USA), 2013.

Ian Osband, Benjamin Van Roy, Daniel J Russo, and Zheng Wen. Deep exploration via randomized value functions. Journal of Machine

Learning Research, 20(124):1–62, 2019.

Anthony L Peressini, Francis E Sullivan, and J Jerry Uhl. The mathematics of nonlinear programming. (Springer-Verlag, NY, USA), 1988.

30

Jan Peters and Stefan Schaal. Policy gradient methods for robotics. In 2006 IEEE/RSJ International Conference on Intelligent Robots and

Systems, pages 2219–2225. (IEEE, NY, USA), 2006.

G. Ch. Pﬂug. Derivatives of probability measures-concepts and applications to the optimization of stochastic systems. In P. Varaiya
and A.B. Kurzhanski, editors, Discrete Event Systems: Models and Applications. Lecture Notes in Control and Information Sciences,
volume 103, pages 252–274. (Springer, Berlin, Germany), 1988.

G. Ch. Pﬂug. On-line optimization of simulated markovian processes. Mathematics of Operations Research, 15(3):381–395, 1990.
Boris T Polyak. Gradient methods for the minimisation of functionals. USSR Computational Mathematics and Mathematical Physics, 3

(4):864–878, 1963.

Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. (John Wiley & Sons, NJ, USA), 2014.
Aravind Rajeswaran, Kendall Lowrey, Emanuel V Todorov, and Sham M Kakade. Towards generalization and simplicity in continuous
control. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural
Information Processing Systems, volume 30, pages 6550–6561. (Curran Associates, NY, USA), 2017.

Tankred Rautert and Ekkehard W Sachs. Computational design of optimal output feedback controllers. SIAM Journal on Optimization, 7

(3):837–852, 1997.

Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnab´as P´oczos, and Alex Smola. Stochastic variance reduction for nonconvex optimization.
In M. F. Balcan and K. Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48,
pages 314–323. (PMLR), 2016a.

Sashank J Reddi, Suvrit Sra, Barnab´as P´oczos, and Alex Smola. Stochastic frank-wolfe methods for nonconvex optimization. In 54th
Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 1244–1251. (IEEE, NY, USA), 2016b.
Sashank J Reddi, Suvrit Sra, Barnabas Poczos, and Alexander J Smola. Proximal stochastic methods for nonsmooth nonconvex ﬁnite-sum
optimization. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing
Systems, volume 29, pages 1145–1153. (Curran Associates, NY, USA), 2016c.

Chang-Han Rhee and Peter Glynn. Lyapunov conditions for differentiability of markov chain expectations: the absolutely continuous

case. arXiv preprint arXiv:1707.03870, 2017.

Martin Riedmiller, Jan Peters, and Stefan Schaal. Evaluation of policy gradient methods and variants on the cart-pole benchmark. In 2007
IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning, pages 254–261. (IEEE, NY,
USA), 2007.

Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement

learning. arXiv preprint arXiv:1703.03864, 2017.

Bruno Scherrer and Matthieu Geist. Local policy search in a convex space and conservative policy iteration as boosted policy search. In
Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 35–50. (Springer, Berlin, Germany),
2014.

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In F. Bach and
D. Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37, pages 1889–1897. (PMLR),
2015.

John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using
generalized advantage estimation. In Y. Bengio and Y. LeCun, editors, Proceedings of the 4th International Conference on Learning
Representations (ICLR), 2016.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv

preprint arXiv:1707.06347, 2017.

Lior Shani, Yonathan Efroni, and Shie Mannor. Adaptive trust region policy optimization: Global convergence and faster rates for
regularized mdps. In F. Rossi, V. Conitzer, and F. Sha, editors, Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, volume 34.
(AAAI Press, CA, USA), 2020.

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms.
In E. P. Xing and T. Jebara, editors, Proceedings of the 31st International Conference on Machine Learning, volume 32. (PMLR),
2014.

Satinder P Singh, Tommi Jaakkola, and Michael I Jordan. Reinforcement learning with soft state aggregation. In G. Tesauro, D. Touretzky,
and T. Leen, editors, Advances in neural information processing systems, volume 7, pages 361–368. (MIT Press, MA, USA), 1994.
Alexander L Strehl and Michael L Littman. An analysis of model-based interval estimation for markov decision processes. Journal of

Computer and System Sciences, 74(8):1309–1331, 2008.

J. Sun, Q. Qu, and J. Wright. Complete dictionary recovery over the sphere I: Overview and the geometric picture. IEEE Transactions on

Information Theory, 63(2):853–884, 2017.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. (MIT Press, MA, USA), 2018.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with
function approximation. In S. Solla, T. Leen, and K. M¨uller, editors, Advances in neural information processing systems, volume 12,
pages 1057–1063. (MIT Press, MA, USA), 1999.

Kalyan T Talluri and Garrett J Van Ryzin. The theory and practice of revenue management, volume 68. (Springer Science & Business

Media, Berlin, Germany), 2006.

31

Sebastian B Thrun. Efﬁcient exploration in reinforcement learning. Technical report, CMU-CS-92-102, School of Computer Science,

Carnegie Mellon University, 1992.

Hannu T Toivonen. A globally convergent algorithm for the optimal constant output feedback problem. International Journal of Control,

41(6):1589–1599, 1985.

John N Tsitsiklis and Benjamin Van Roy. Feature-based methods for large scale dynamic programming. Machine Learning, 22(1-3):

59–94, 1996.

Masatoshi Uehara, Masaaki Imaizumi, Nan Jiang, Nathan Kallus, Wen Sun, and Tengyang Xie. Finite sample analysis of minimax ofﬂine

reinforcement learning: Completeness, fast rates and ﬁrst-order efﬁciency. arXiv preprint arXiv:2102.02981, 2021.

Benjamin Van Roy. Performance loss bounds for approximate value iteration with state aggregation. Mathematics of Operations Research,

31(2):234–244, 2006.

Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global optimality and rates of convergence.

In Y. Bengio and Y. LeCun, editors, Proceedings of 7th International Conference on Learning Representations (ICLR), 2019.

Ruosong Wang, Dean Foster, and Sham M. Kakade. What are the statistical limits of ofﬂine RL with linear function approximation? In
International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=30EvkP2aQLD.
Gell´ert Weisz, Philip Amortila, and Csaba Szepesv´ari. Exponential lower bounds for planning in mdps with linearly-realizable optimal
action-value functions. In ALT, volume 132 of Proceedings of Machine Learning Research, pages 1237–1264. PMLR, March 2021.
Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization,

24(4):2057–2075, 2014.

Henry Zhu, Justin Yu, Abhishek Gupta, Dhruv Shah, Kristian Hartikainen, Avi Singh, Vikash Kumar, and Sergey Levine. The ingredients
of real-world robotic reinforcement learning. In Y. Bengio and Y. LeCun, editors, Proceedings of 8th International Conference on
Learning Representations (ICLR), 2020.

32

A Notation

Table 1: Table of Notation

γ (cid:44) Discount factor.
S (cid:44) State space.

As ⊂ Rk (cid:44) Convex set of feasible actions when in state s.

Π (cid:44) Set of all stationary policies.
J (cid:44) Set of bounded measurable functions on S.
g(s, a) (cid:44) Single period expected cost of action a in state s.

P (M|s, a) (cid:44) Transition probability to set M ⊂ S

gπ ∈ J (cid:44) Single period cost function under policy π.
Jπ ∈ J (cid:44) Cost-to-go function under policy π.
Qπ : S × A → R (cid:44) State-action cost-to-go function under policy π.

J ∗ ∈ J (cid:44) Optimal cost-to-go function.

π∗ (cid:44) An optimal policy (satisfying Jπ∗ = J ∗).

Q∗ = Qπ∗ (cid:44) State-action cost-to go function associated with an optimal policy.

Tπ : J → J (cid:44) Bellman operator associated with policy π.
T : J → J (cid:44) Bellman optimality operator.
ρ (cid:44) Initial distribution.
ηπ (cid:44) The discounted state occupancy measure under policy π. See (2).
(cid:96)(π) = (1 − γ) (cid:82) Jπdρ (cid:44) Expected discounted cost under a random initial state, policy π.

Θ ⊂ Rd (cid:44) Convex set of policy parameters.

ΠΘ = {πθ : θ ∈ Θ} (cid:44) Parameterized policy class.

JΘ = {Jπ : π ∈ πΘ} (cid:44) Set of cost-to-go functions under parameterized policies.

(cid:96)(θ) = (cid:96)(πθ) (cid:44) Overloaded notation for (cid:96)(πθ).
B(π(cid:48)|η, Jπ) (cid:44) “Bellman” objective or the weighted policy iteration objective

B(θ|η, Jπ) = B(πθ|η, Jπ) (cid:44) Overloaded notation for the policy iteration objective at πθ.

κρ (cid:44) Effective concentrability coefﬁcient. See (13).

(cid:107)J(cid:107)∞ (cid:44) Max-norm sups |J(s)|.
(cid:107)J(cid:107)1,η (cid:44) Weighted 1-norm (cid:82) |J(s)| dη.

(cid:107)A(cid:107)p for A ∈ Rn×m (cid:44) Matrix operator norm max{(cid:107)Ax(cid:107)p : (cid:107)x(cid:107)p = 1}

λmin(A) for A = AT ∈ Rn×n (cid:44) Minimum eigenvalue of A
λmax(A) for A = AT ∈ Rn×n (cid:44) Maximum eigenvalue of A
f (cid:23) g, f (cid:31) g for functions f, g (cid:44) Elementwise inequality, f (x) ≥ g(x) or f (x) > g(x) ∀x

A (cid:23) B, A (cid:31) B for A, B ∈ Rn×n (cid:44) Indicates A − B is positive (semi)deﬁnite.

B Discussion on concurrent work of Agarwal et al. [2020]

Here we provide some comparisons to the approach of Agarwal et al. [2020], offering perspective on the
strengths and shortcomings of our approach and highlighting some connections between our technical results
and theirs.

The work of Agarwal et al. [2020] is mostly focused on analyzing speciﬁc algorithms, with special
attention given to natural-gradient actor-critic algorithms. Explicit bounds on the convergence rates of different
algorithms are also given. In comparison, we take a broad algorithm-independent view, focusing primarily
on the landscape of the loss function (cid:96)(·) to understand when local policy search is a sensible approach for

33

reaching a (near) optimal policy. Our choice of focus yields several results and insights that are not covered by
their work:

• In Example 1, we provide a simple illustration of how the multi-period nature of the decision problem
can lead to bad local minima. With policy closure, we show how challenges raised in that example
largely disappear, and the problem is reduced to studying the single period objective function in (11).
This approach of reasoning about the landscape of long-horizon objective by analyzing structure in
single period problems offers new insight and greatly simpliﬁes our analysis.

• In addition to treating some examples covered by Agarwal et al. [2020], like tabular and linear MDPs,
our analysis seamlessly covers problems with inﬁnite action spaces, structured cost functions, and
deterministic policies – for example linear quadratic control and optimal stopping.

• Our approach extends to ﬁnite horizon problems with nonstationary policy classes as shown in Theorem
3. We instantiate this theory for an important practical problem of ﬁnite horizon inventory control.

• In stating gradient dominance conditions for convergence rates, our approach to concentration coefﬁ-
cients implies tighter bounds for some examples as compared to the distribution mismatch coefﬁcient in
Agarwal et al. [2020].

Our focus on an algorithm-agnostic study of the landscape of (cid:96)(·) comes at a cost. This is felt most clearly
when studying the softmax parameterization which poses unique optimization challenges even in the seemingly
simple single-state single-period problem with a ﬁnite number of actions. There is no optimal solution; instead
optimal performance is attained only as certain components of the parameter vector tend to inﬁnity. Although
our theory applies easily to problems with entropy regularization (see e.g. Example 5), a more specialized
analysis is required for the case without regularization. Agarwal et al. [2020] do this with a detailed study of
speciﬁc algorithms and precisely characterize the convergence behavior for both the cases of with and without
regularization. In addition, their work also shows how natural policy gradients can be particularly useful in
alleviating conditioning issues arising with softmax policies and result in faster convergence.

The next subsection tries to clarify some connections between our theory and that appearing in Agarwal

et al. [2020].

Connecting closure conditions to conditions on value function approximation Compared to our work,
a novel theory appears in Agarwal et al. [2020] on function approximaiton. Speciﬁcally, they analyze a
natural gradient actor critic method working with a class of softmax linear policies and compatible function
approximation [Konda and Tsitsiklis, 2000, Sutton et al., 1999], i.e. deriving features for approximating
the Q-function from score function of a stochastic policy. Although we do not treat this case in detail, we
brieﬂy remark on an interesting connection which shows that in one important setting (linearly parameterized
Q-functions and the corresponding softmax linear policies), accuracy of value function approximation error
implies approximate closure as shown in Condition 5.

This observation follows from a natural duality between a parametric class of policies and a parametric
class of value functions. In Example 6, for instance, we leveraged the fact that if a parametric class of value
functions {Qθ : θ ∈ Θ} can approximate each Qπ, then the class of greedy policies induced by this class of
value functions is closed under policy improvement. A similar observation holds for approximate closure as
shown below in Example 9.

Example 9. As in Example 3, we assume the set of feasible actions As is the same for every state s and denote
this by A. We also assume there is a ﬁnite set of k deterministic actions to choose from and take A = ∆k−1 to
be the set of all probability distributions over these actions. Suppose g(s, a) and P (s(cid:48)|s, a) are linear in a, as
in (14). Fix a feature mapping φ : S × {1, · · · , k} → Rd. Consider the class of policies, ΠΘ = {πθ : θ ∈ Rd}
where πθ(s) = (πθ(s, 1), · · · πθ(s, k)) ∈ ∆k−1 has components πθ(s, i) ∝ exp{θ(cid:62)φ(s, i)}.

34

Lemma 16 (Accurate function approximation implies approximate closure). In the setting of Example 9,
suppose that for each π ∈ ΠΘ,

min
w∈Rd

E(s,i)∼ηπ⊗Unif

(cid:104)(cid:0)Qπ(s, ei) − w(cid:62)φ(s, i)(cid:1)2(cid:105)

< (cid:15)2/k2.

(25)

Then ΠΘ satisﬁes Condition 5.

Proof. Given π, let ˆw satisfy E(s,i)∼ηπ⊗Unif

(cid:104)(cid:0)Qπ(s, ei) − w(cid:62)φ(s, i)(cid:1)2(cid:105)

= (cid:15)2

0/k2 where (cid:15)0 < (cid:15)/2. Set

ˆQπ(s, a) =

k
(cid:88)

i=1

ai ˆQπ(s, ei) =

k
(cid:88)

i=1

ai( ˆw(cid:62)φ(s, i)).

to be the resulting approximate Q-function. For a scalar c > 0, consider the policy πc ˆw which assigns
probability πc ˆw(s, i) ∝ exp{c · ˆQπ(s, ei)} to base action i in state s. We show that πc ˆw approximately
optimizes the policy iteration objective when c is large. Note,

B(πc ˆw | ηπ, Jπ) =

(cid:90)

(cid:0)Qπ(s, πc ˆw(s))(cid:1) ηπ(ds)

(cid:90) (cid:16) ˆQπ(s, πc ˆw(s))
(cid:90) (cid:16) ˆQπ(s, πc ˆw(s))

(cid:17)

(cid:17)

≤

≤

(cid:90)

ηπ(ds) +

(cid:12)
(cid:12)
(cid:12)

(cid:12)
ˆQπ(s, a) − Qπ(s, a)
(cid:12)
(cid:12) ηπ(ds)

max
a∈A

ηπ(ds) + (cid:15)0

where the ﬁnal inequality follows from using the identity E[X 2] ≥ (E[|X|])2 for any random variable X and
observing that

(cid:90)

(cid:12)
(cid:12)
(cid:12)

max
a∈A

ˆQπ(s, a) − Qπ(s, a)

(cid:12)
(cid:12)
(cid:12) ηπ(ds) ≤

(cid:90)

k
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)

ˆQπ(s, ei) − Qπ(s, ei)

(cid:12)
(cid:12)
(cid:12) ηπ(ds)

(cid:90)

= k

k−1

k
(cid:88)

i=1

(cid:12)
(cid:12)
ˆQπ(s, ei) − Qπ(s, ei)
(cid:12)
(cid:12)
(cid:12) ηπ(ds)
(cid:12)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:90)

≤ k

k−1

k
(cid:88)

i=1

(cid:16) ˆQπ(s, ei) − Qπ(s, ei)

(cid:17)2

ηπ(ds) ≤ (cid:15)0.

This implies,

lim
c→∞

B(πc ˆw | ηπ, Jπ) ≤ lim
c→∞
(cid:90)

(cid:17)

(cid:17)

ηπ(ds) + (cid:15)0

ηπ(ds) + (cid:15)0

(cid:90) (cid:16) ˆQπ(s, πc ˆw(s))
(cid:16) ˆQπ(s, πc ˆw(s))
(cid:16) ˆQπ(s, a)

(cid:17)

lim
c→∞

min
a∈A

ηπ(ds) + (cid:15)0

=

(cid:90)

(cid:90)

=

≤

min
a∈A

(Qπ(s, a)) ηπ(ds) +

(cid:90)

(cid:12)
(cid:12)
(cid:12)

max
a∈A

ˆQπ(s, a) − Qπ(s, a)

(cid:12)
(cid:12)
(cid:12) ηπ(ds) + (cid:15)0

≤ min
π+∈Π

B(π+ | ηπ, Jπ) + 2(cid:15)0.

The ﬁrst and second equality use the monotone convergence theorem together with the fact that, for each s,
ˆQπ(s, πc ˆw(s)) ↑ maxa∈A ˆQπ(s, a). Since (cid:15)0 < (cid:15)/2 we can choose a ˆc ∈ R such that B(πˆc ˆw | ηπ, Jπ) ≤
minπ+∈Π B(π+ | ηπ, Jπ) + (cid:15).

35

The theory in Agarwal et al. [2020] is a tour-de-force, containing many ideas which are not reﬂected in the

result above. Let us remark on two of them:

1. Agarwal et al. [2020] treat generalizations of Example 9 where policies are still stochastic but are not
log-linear. If πθ(s, i) ∝ exp{fθ(s, i)}, then the theory of compatible function approximation [Sutton
et al., 1999] implies modeling Qπ(s, ei) ≈ w(cid:62)∇θfθ(s, i) for some choice of weights w ∈ Rd. When
this approximation is accurate, natural gradient updates to the parameter vector will tilt the policies’
action probabilities towards selecting actions with higher Q-values. Because it is local in nature, such a
property appears to no longer imply closure of the policy class under policy improvement steps, which
involves global properties of the policy class. The choice to focus on natural gradient methods helps
Agarwal et al. [2020] to analyze this example, since there is an explicit formula for the parameter updates
which has a very close connection to compatible function approximation of the Q–function.

2. Agarwal et al. [2020] state their results in terms of a notion called transfer error. This measures implicitly
depends on both issues of approximation, as in (25), and issues of distribution shift, as in our term κρ. It
is a very nice insight that this term is what is really needed in the analysis.

C On the necessity of an exploratory initial distribution

Our results critically rely on using an exploratory initial distribution (see Assumption 1). This is not an artifact
of the proof techniques and it is well known that, in the absence of strong assumptions on the transition kernel,
policy gradient methods have poor convergence properties if applied without some form of sophisticated
exploration. While this aspect of policy gradient methods is not always highlighted in the literature, many
applied papers assume access to a diverse set of starting states using either explicit restarts [Fu et al., 2018,
Haarnoja et al., 2018] or some form of continual learning that aims to increase the support of a training
distribution [Lesort et al., 2020, Zhu et al., 2020].

The following example, which is commonly known as a “chain” MDP [Kakade and Langford, 2002, Thrun,
1992] or the “river swim” problem [Osband et al., 2013, Strehl and Littman, 2008], illustrates the challenges
for policy gradient in the absence of sufﬁcient exploration. Many other examples in the reinforcement learning
literature, like the “combination lock” problem [Koenig and Simmons, 1993] and the “grid world” problem
[Azar et al., 2011] highlight the same issue. While these examples are typically used to highlight a statistical
challenge, here we focus on the optimization landscape. This example is partly inspired by one in Kakade and
Langford [2002]. A similar discussion appears also in Agarwal et al. [2020]. We include this section to keep
the paper self contained. In addition, it does not seem that past work has shown clearly that (cid:96)(·) may have
suboptimal local minima in the absence of an exploratory initial distribution, instead showing the existence of
suboptimal polices with small but nonzero gradient norm.

Example 10. Consider the MDP shown in Figure 2. There are N states and the agent can move either left (L)
or right (R) from each state. The agent always begins in the leftmost state (i.e. ρ(s1) = 1). She incurs a cost of
2 per-period when in any state other than the leftmost or rightmost state, a cost g(s1) = 1 from the leftmost
state and a cost of g(sN ) = 0 per period in the rightmost state. A stationary policy π ∈ [0, 1]N is a vector13
where π(s) speciﬁes the probability of choosing the action R in state s. When the horizon is sufﬁciently long,
the optimal policy moves right in each period. From Lemma 6, one can calculate the policy gradient as

∂(cid:96)(π)
∂π(s)

= ηπ(s) (Qπ(s, R) − Qπ(s, L)) .

We argue that a suboptimal policy π that always moves left, i.e. π(si) = 0 ∀ i ∈ {1, . . . , N }, is a local
minimum of (cid:96)(·). To see this, ﬁrst note that the agent will always start and stay in the leftmost state, so

13Note that unlike Example 1, this policy class is closed under policy improvement.

36

R

L

s2

R

L

. . .

RR

L

sN −1

R

L

g(s1) = 1

s1

L

R

sN

g(sN ) = 0

Figure 2: A simple chain MDP example to illustrate how policy gradient methods face suboptimal local
minima in the absence of an exploratory initial distribution.

ηπ(si) = 0 when i ≥ 2. The only possible nonzero component of ∇(cid:96)(π) is the ﬁrst term corresponding to
state s1. Therefore, for any policy π(cid:48) ∈ [0, 1]N ,

(cid:104)∇(cid:96)(π), π(cid:48) − π(cid:105) = ηπ(s1) (Qπ(s1, R) − Qπ(s1, L)) (π(cid:48)(s1) − π(s1)) ≥ 0,

which follows as Qπ(s1, R) > Qπ(s1, L), given that moving to s2 for a single period is more costly than
staying in s1 and the fact that π(s1) = 0, so π(cid:48)(s1) − π(s1) ≥ 0 for any feasible policy π(cid:48).

Similar issues arise under a (non-degenerate) stochastic policy. The main idea is that policies which are
more likely to move left from every state are expected to require exponentially (in the number of states) many
periods to reach the rightmost state. An explicit bound conﬁrming that the policy gradient can be exponentially
small in N is shown in Agarwal et al. [2020].

D Omitted proofs.

In this section, we provide proofs for some of the main results along with the supporting lemmas.

D.1 General results.

We prove some key lemmas which are used to show our general results in Theorems 1 and 2. We start with
some useful background on Bellman operators.

Bellman operators. For bounded cost-to-go functions, Bellman operators are monotone, meaning that
J (cid:22) J (cid:48) implies T J (cid:22) T J (cid:48) and TπJ (cid:22) TπJ (cid:48), and contractive in (cid:107) · (cid:107)∞ with modulus γ. A useful consequence
of contractivity relates optimality gap to errors in the cost-to-go functions.

(cid:107)Jπ − J ∗(cid:107)∞ ≤

1
1 − γ

(cid:107)Jπ − T Jπ(cid:107)∞

(26)

where J ∗ is the optimal cost-to-go function. A simple argument [Bertsekas, 1995] shows (26).

(cid:107)Jπ − J ∗(cid:107)∞ = (cid:107)TπJπ − T Jπ + T Jπ − J ∗(cid:107)∞ ≤ (cid:107)TπJπ − T Jπ(cid:107)∞ + (cid:107)T Jπ − T J ∗(cid:107)∞
≤ (cid:107)TπJπ − T Jπ(cid:107)∞ + γ(cid:107)Jπ − J ∗(cid:107)∞.

Balance equation The best way to understand equation (17) below is by analogy to an equivalent undis-
counted problem: ηπ is the steady state distribution in a problem in which the next state is drawn from the
restart distribution ρ(·) with probability 1 − γ and otherwise is drawn according to the transition kernel under
the policy π. This result is only used explicitly in one place, but may provide helpful intuition throughout.

37

Lemma 17. The discounted state occupancy measure satisﬁes the balance equation,

ηπ(M) =

(cid:90)

S

[(1 − γ)ρ(M) + γP (M|s, π(s))] ηπ(ds)

∀M ⊂ S.

Proof. This can be directly veriﬁed by using the tower property of conditional expectation as follows:

ηπ(M) = (1 − γ)

∞
(cid:88)

t=0

γtEπ

ρ [1 (st ∈ M)]

= (1 − γ)ρ(M) + (1 − γ)Eπ
ρ

= (1 − γ)ρ(M) + (1 − γ)Eπ
ρ

(cid:34) ∞
(cid:88)

(cid:35)
ρ [st ∈ M | st−1]

γtPπ

t=1
(cid:34) ∞
(cid:88)

γtP (M|st−1, π(st−1))

(cid:35)

t=1
(cid:34) ∞
(cid:88)

t=0

(cid:35)

γtP (M|st, π(st))

= (1 − γ)ρ(M) + γ(1 − γ)Eπ
ρ

= (1 − γ)ρ(M) + γ

(cid:90)

S

P (M|s, π(s))ηπ(ds).

Optimal policies and minimizers of the policy gradient loss. For the reader’s convenience, we recall
Lemma 1, which relates minimizers of (cid:96)(·) to the classic deﬁnition of optimal policies in dynamic programming.

Lemma 1. A policy satisﬁes π ∈ arg minπ(cid:48)∈Π (cid:96)(π(cid:48)) if and only if Jπ = J ∗ ρ–almost surely, i.e.
ρ ({s ∈ S : Jπ(s) = J ∗(s)}) = 1.
Proof. Recall (cid:96)(π) = (1 − γ) (cid:82) Jπ(s)ρ(ds). An optimal policy π∗ satisﬁes Jπ∗ (s) = J ∗(s) for every state
s ∈ S. Since Jπ(s) ≥ J ∗(s) for each s ∈ S, we have
(cid:90)

(cid:96)(π) − (cid:96)(π∗) = (1 − γ)

(Jπ(s) − J ∗(s)) ρ(ds) ≥ 0.

(27)

Since this holds for every policy π, it is clear that (cid:96)(π∗) = minπ∈Π (cid:96)(π).

A basic fact in measure theory states that, for a non-negative function J : S → R+, (cid:82) Jdρ = 0 if and
only if J = 0 ρ-almost surely. Since Jπ(s) ≥ J ∗(s) for each s ∈ S, applying this fact with a choice of
J = Jπ − J ∗ implies equality holds in (27) if and only if Jπ − J ∗ = 0 ρ-almost-surely.

Performance difference and telescoping sums. Throughout the analysis, we use a basic result which
relates the difference in cost-to-go functions to the gap in Bellman’s equation at future states. For any two
cost-to-go functions, Jπ, J ∈ J , and any starting state s0 ∈ S, we have

Jπ(s0) − J(s0) = TπJ(s0) − J(s0) + TπJπ(s0) − TπJ(s0)
= TπJ(s0) − J(s0) + γPπ (Jπ(s0) − J(s0))

where we use the notation (PπJ)(s) = (cid:82) J(s(cid:48))P (ds(cid:48)|s, π(s)). Unrolling this recursion, taking expectation
over some initial distribution ν, and noting that (PπJ)(st) = Eπ [J(st+1)|st], we have

ν [Jπ(s0) − J(s0)] = Eπ
Eπ
ν

(cid:34) ∞
(cid:88)

(cid:35)
γt [TπJ(st) − J(st)]

,

(28)

t=0

38

where we use the tower property of conditional expectation to simplify the telescoping sum. Kakade and
Langford [2002] use this to give a particularly convenient form, which is commonly known as the performance
difference lemma. Choosing J = J¯π, ν = ρ in (28) and recalling that (cid:96)(π) = (1 − γ)Eρ[Jπ(s0)] gives

(cid:96)(π) − (cid:96)(¯π) = (1 − γ)Eπ
ρ

γt (TπJ¯π(st) − J¯π(st))

=

(cid:35)

(cid:90)

(cid:34) ∞
(cid:88)

t=0

[TπJ¯π − J¯π] dηπ.

(29)

The second equality follows using the deﬁnition of the discounted state occupancy measure, ηπ(M) =
(1 − γ)Eπ

t=0 γt1(st ∈ M)] for any measurable set M ⊂ S.

ρ [(cid:80)∞

A policy gradient formula. We give a short of the policy gradient theorem in Lemma 6 assuming the
differentiability conditions hold.

Lemma 6 (Policy gradient theorem). Under Condition 0, (cid:96)(θ) is continuously differentiable and

∇(cid:96)(θ) = ∇θ B(θ | ηπθ , Jπθ )

(cid:12)
(cid:12)
(cid:12)
(cid:12)θ=θ

Proof. Recall that B(θ|ηθ, Jθ) = (cid:82) Jπθ dηπθ

. From the performance difference lemma in (29), we have

(cid:96)(θ) − (cid:96)(θ) =

(cid:90)

(cid:2)TθJθ − Jθ

(cid:3) dηθ = B(θ|ηθ, Jθ) − B(θ|ηθ, Jθ).

Expanding the total derivative in terms of partial derivatives gives,

∇(cid:96)(θ) = ∇θ B(θ|ηθ, Jθ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)θ=θ

− ∇θ B(θ|ηθ, Jθ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)θ=θ

= ∇θ B(θ|ηθ, Jθ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)θ=θ

.

On average Bellman equation. We prove the on average Bellman equation which is a key lemma we used
for proving Theorem 1.

Lemma 8 (On average Bellman equation). For any π ∈ Π,

(cid:96)(π) = (cid:96)(π∗) ⇐⇒

(cid:90)

(Jπ − T Jπ) dρ = 0.

Proof. Recall that a non-negative function f satisﬁes (cid:82) f dµ = 0 if and only if f = 0 almost surely under
the probability distribution µ. We also use the fact that Jπ (cid:23) J ∗, by deﬁnition of the optimal cost-to-go, and
Jπ (cid:23) T Jπ, as shown in (5).

Take π+ to be the policy iteration update at π, i.e. Tπ+Jπ = T Jπ. To show the left hand side, recall that

by deﬁnition, (cid:96)(π) − (cid:96)(π(cid:48)) = (1 − γ) (cid:82) (Jπ − Jπ(cid:48)) dρ.Therefore, (cid:96)(π) = (cid:96)(π∗) implies that,

(cid:90)

0 =

(Jπ − J ∗) dρ ≥

(cid:90)

(Jπ − Jπ+) dρ

(a)
= (1 − γ)−1

(cid:90)

(Jπ − Tπ+Jπ) dηπ+

= (1 − γ)−1

(cid:90)

(Jπ − T Jπ) dηπ+

(cid:90)

≥

(Jπ − T Jπ) dρ ≥ 0.

where (a) follows by using the performance difference lemma in (29) with ¯π = π+. The penultimate inequality
uses that ηπ+ (cid:23) (1 − γ)ρ while the ﬁnal inequality follows by using that Jπ (cid:23) T Jπ.

39

To show the other side, suppose (cid:82) (Jπ − T Jπ)dρ = 0. Let S0 = {s : Jπ(s) − T Jπ(s) = 0} and Sc

0 denote
0) = 0. But as we assumed ηπ∗ to be absolutely

its complement. Since Jπ − T Jπ (cid:23) 0, we must have ρ(Sc
continuous with respect to ρ, we have that ρ(Sc

0) = 0 =⇒ ηπ∗ (Sc

0) = 0. Therefore,

(cid:90)

(Jπ − T Jπ)dηπ∗ = 0.

As Jπ (cid:23) J ∗, we have (cid:96)(π) − (cid:96)(π∗) = (1 − γ) (cid:82) (Jπ − J ∗) dρ ≥ 0. Then, we get our result by noting

0 ≤ (cid:96)(π) − (cid:96)(π∗)

(cid:90)

(b)
=

(Jπ − Tπ∗ Jπ) dηπ∗ ≤

(cid:90)

(Jπ − T Jπ) dηπ∗ = 0

where (b) follows from the performance difference lemma in (29).

D.2 Non-stationary policy classes: Proof of Theorem 3

For the reader’s convenience, we restate Theorem 3.

Theorem 3. Suppose Conditions 3 and 4 hold. If the parameterized policy class ΠΘ contains an optimal
policy, then any stationary point θ of (cid:96) : Θ → R satisﬁes (cid:96)(πθ) = (cid:96)(π∗).

Proof. To give a more transparent proof, it is helpful to develop some notation that highlights a (limited) sense
in which the problem decomposes across time periods. With some abuse of notation14, for any parameter
vector θ = (θ1, · · · , θH ), deﬁne πθh : Sh → A to be the restriction of the policy πθ to Sh, i.e. πθ(s) = πθh (s)
for all s ∈ Sh.

Single period PI objectives. Similarly, deﬁne

so that

Bh(θh | η, Jπ) =

(cid:90)

Sh

Qπ(s, πθh(s))η(ds)

B(θ | η, Jπ) =

H
(cid:88)

Bh(θh | η, Jπ).

h=1
Because the parameter space factorizes as Θ = Θ1 × · · · × ΘH , this separability of the weighted Bellman
objective implies,

θ ∈ arg min

θ∈Θ

B(θ | η, Jπ) ⇐⇒ θh ∈ arg min
θh∈Θh

Bh(θh | η, Jπ) ∀h.

(30)

Single period characterization of stationary points. The policy gradient formula in Lemma 6 states
∇θ(cid:96)(θ) = ∇θB(θ | ηπθ , Jπθ )|θ=θ. Therefore, θ is a stationary point of minθ∈Θ (cid:96)(θ) if and only if it is a the
stationary points of the optimization problem minθ∈Θ B(θ | ηπθ , Jπθ ). Because Θ = Θ1 × · · · × ΘH , this
problem separates across time periods, and we ﬁnd θ is a stationary point of minθ∈Θ (cid:96)(θ) if and only if θh is a
stationary point of minθh∈Θh

Bh(θh | ηπθ , Jπθ ) ∀h. That is,

(cid:104)∇(cid:96)(θ), θ(cid:48) − θ(cid:105) ≥ 0 ∀θ(cid:48) ∈ Θ ⇐⇒

(cid:34)

∂
∂θh

Bh(θh | ηπθ , Jπθ )

(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:12)θh=θh

(θ(cid:48)

h − θh) ≥ 0 ∀θ(cid:48)

h ∈ Θh, ∀h (31)

14Technically, for this to be appropriate we should imagine Θ1, · · · , ΘH are disjoint, which we could assume without loss of generality.

40

Inductive proof. We argue that any stationary point θ of (cid:96)(·) must satisfy Jπθ = J ∗ ρ-almost surely. By
Lemma 1, this implies it is a minimizer of (cid:96)(·). By assumption 3, ηπ (cid:28) ρ for any π ∈ ΠΘ. In the reverse
direction, ηπ (cid:23) (1 − γ)ρ by deﬁnition, which implies ρ (cid:28) ηπ for each π ∈ ΠΘ. Therefore, we can throughout
claim that certain events hold “almost surely” without reference to whether the base measure is ρ or ηπ.

Let θ be a stationary point of (cid:96)(·). We proceed by backward induction, showing Jπθ (s) = J ∗(s) almost
surely for s ∈ Sh. As a base case, consider h = H + 1. By deﬁnition, Jπθ (s) = J ∗(s) = 0 for all s ∈ SH+1
as SH+1 = {τ } contains a single costless absorbing state.

Now, for any h ≤ H, suppose Jπθ (s) = J ∗(s) almost surely for s ∈ Sh+1. We ﬁrst claim that
Bh(θh|ηπθ , Jπθ ) = Bh(θh|ηπθ , J ∗) for all θh ∈ Θh. This is a consequence of our induction hypothesis and
Assumption 3. In particular,

0 ≤ Bh(θh|ηπθ , Jπθ ) − Bh(θh|ηπθ , J ∗) =

(cid:90)

(cid:104)

Qπθ (s, πθh

(s)) − Q∗(s, πθh

(cid:105)
(s))

ηπθ (ds)

Sh

= γ

(cid:90)

(cid:90)

[Jπθ (s(cid:48)) − J ∗(s(cid:48))] P (ds(cid:48)|s, πθh

(s)) ηπθ (ds)

s(cid:48)∈Sh+1
Sh
(cid:90)

[Jπθ (s(cid:48)) − J ∗(s(cid:48))] ηπθ

(a)
≤

(ds(cid:48))

s(cid:48)∈Sh+1

(b)
= 0,

where we use throughout that Qπθ (cid:23) Q∗ or Jπθ (cid:23) J ∗. Inequality (a) is justiﬁed by the following balance
equation that holds for the discounted state occupancy measure: for any h ∈ {1, · · · , H} and and M ⊂ Sh+1,

ηπ(M) = (1 − γ)ρ(M) + γ

P (M|s, π(s))ηπ(ds).

(cid:90)

Sh

This follows from the general balance equation of Lemma 17, which states ηπ(M) = (1 − γ)ρ(M) +
(cid:82) P (M|s, π(s))ηπ(ds) holds for all M ⊂ S, and the fact that P (Sh+1|s, π(s)) = 0 for s /∈ Sh due to
Condition 3. Inequality (b) uses the induction hypothesis that Jπθ (s) = J ∗(s) for s ∈ Sh+1 almost surely.

Having shown Bh(θh|ηπθ , Jπθ ) = Bh(θh|ηπθ , J ∗) for all θh ∈ Θh, we now complete the induction step.
As θ is a stationary point of (cid:96)(·), the characterization of stationary points in (31) implies that θh is a stationary
Bh(θh|ηπθ , J ∗).
point of the optimization problem minθh∈Θh
But in Condition 4 we assumed that θ (cid:55)→ B(θ | ηπθ , J ∗) has no suboptimal stationary points. By the
separability structure highlighted in (30), this implies that θh (cid:55)→ Bh(θh | ηπθ , J ∗) also has no suboptimal
stationary points, so we have shown θh ∈ arg minθh∈Θh

(cid:0)θh | ηπθ , J ∗(cid:1). Putting it all together, we get,

Bh(θh|ηπθ , Jπθ ), or equivalently, of minθh∈Θh

Bh

(cid:90)

Sh

Jπθ (s)ηπθ (ds) =

(cid:90)

Sh

Qπθ (s, πθh (s))ηπθ (ds) = Bh(θh | ηπθ , Jπθ ) = Bh(θh | ηπθ , J ∗)

= min
θh∈Θh

= min
θh∈Θh

Bh

(cid:0)θh | ηπθ , J ∗(cid:1)

(cid:90)

Sh

Q∗(s, πθh

(s))ηπθ (ds)

(cid:90)

(c)
=

Sh

J ∗(s)ηπθ (ds),

where equality (c) applies our assumption that the policy class contains an optimal policy, i.e. there exists
θh ∈ Θh such that Q∗(s, πθh (s)) = mina∈As Q∗(s, a) = J ∗(s) for all s ∈ Sh. Since Jπθ (cid:23) J ∗, we conclude

41

that Jπθ (s) = J ∗(s) for s ∈ Sh almost surely, completing the induction step. The statement in Theorem 3
follows by invoking Lemma 1.

D.3 Concentrability coefﬁcients

Theorem 4. The following results apply under the general problem formulation in Section 2.

(a) If S is ﬁnite, then κρ ≤ 1/ (mins∈S ρ(s)).

(b) Let π∗ denote any optimal stationary policy. Then, κρ ≤

(cid:13)
(cid:13)
(cid:13)

dηπ∗
dρ

(cid:13)
(cid:13)
(cid:13)∞

.

(c) The bound κρ ≤ C/c holds if T is a contraction with modulus γ in a norm (cid:107) · (cid:107) that satisﬁes

c(cid:107)J(cid:107) ≤ (cid:107)J(cid:107)1,ρ ≤ C(cid:107)J(cid:107)

∀J ∈ JΘ.

(21)

Proof. The proof of part (a) follows as a simple corollary of the result in part (b).

Proof of part (b). Recall that π∗ denotes an optimal policy. Using that Jπ (cid:23) J ∗ and the performance
difference lemma in (29), we get

(cid:90)

(1 − γ)

(Jπ − J ∗) dρ = (1 − γ)(cid:107)Jπ − J ∗(cid:107)1,ρ = (cid:96)(π) − (cid:96)(π∗) =

(cid:90)

(Jπ − Tπ∗ Jπ) dηπ∗

(cid:90)

(cid:90)

(a)
≤

(b)
=

(Jπ − T Jπ) dηπ∗

(Jπ − T Jπ)

(cid:19)

(cid:18) dηπ∗
dρ

dρ

≤

(c)
=

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

dηπ∗
dρ
dηπ∗
dρ

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:90)

(Jπ − T Jπ) dρ

(cid:107)Jπ − T Jπ(cid:107)1,ρ

where (a) follows by using that Tπ(cid:48)J (cid:23) T J for any policy π(cid:48) and each J ∈ J , (b) uses deﬁnition of the
Radon-Nikodym derivative and (c) follows as Jπ (cid:23) T Jπ.

Proof of part (c). From (26), if T is contraction with modulus γ in (cid:107) · (cid:107), then (cid:107)J − J ∗(cid:107) ≤ 1
Our result follows by noting,

(1−γ) (cid:107)J − T J ∗(cid:107).

(cid:107)J − J ∗(cid:107)1,ρ ≤ C(cid:107)J − J ∗(cid:107) ≤

C
(1 − γ)

(cid:107)J − T J(cid:107) ≤

C
c(1 − γ)

(cid:107)J − T J(cid:107)1,ρ

Lemmas 13 and 14 which state the concentrability coefﬁcients for optimal stopping and LQ control are

proved in Appendix E where both these examples are treated in detail.

E Example details.

E.1 Finite state and action MDPs with natural parameterization.

We restate and prove the convergence rate result for tabular MDPs given in Lemma 9. We use a result in
[Agarwal et al., 2020] which shows that for the policy gradient objective with natural parameterization, ∇(cid:96) is
Lipschitz continuous with constant L = 2γ|A|
(1−γ)2 .

42

Lemma 9 (Convergence rate for tabular MDPs). Consider the ﬁnite state action MDP with natural parame-
terization as formulated in Example 3. Assume γ ≥ 1/3 and per-period costs are normalized with (cid:107)g(cid:107)∞ ≤ 1.
For projected gradient descent, π(t+1) = ProjΠ(πt − α∇(cid:96)(πt))) with α = (1−γ)2
2γ|A| ,

(cid:96)(πT ) − (cid:96)∗ ≤

(cid:115)

8γ|S||A|κ2
ρ
(1 − γ)4

((cid:96)(π0) − (cid:96)∗)
T

,

where (cid:96)∗ = inf π∈Π (cid:96)(π).

Proof. We assumed that the per-step costs are normalized, i.e. sups∈S,a∈A |g(s, a)| ≤ 1. As the weighted PI
objective π(cid:48) (cid:55)→ B(π(cid:48)|ηπ, Jπ) in (15) is (1,0)-gradient dominated, Theorem 2 implies that (cid:96)(·) is ( κρ
(1−γ) , 0)-
ηπ∗ (s)
gradient dominated. For ﬁnite state-actions MDPs, Theorem 4 implies κρ ≤ sups∈S
ρ(s) . Application of
Lemma 3 also requires smoothness conditions. For this we appeal to Lemma D3 in [Agarwal et al., 2020]
which shows that ∇(cid:96) is Lipschitz continuous with constant15 L = 2γ|A|
(1−γ)2 . Finally, it is easy to compute the
constants R and k in part (1) of Lemma 3 for tabular MDPs with natural parameterization as:

R = sup

π,π(cid:48)∈Π

(cid:107)π − π(cid:48)(cid:107)2 ≤ (cid:112)2|S|,

k = sup
π∈Π

(cid:107)∇(cid:96)(π)(cid:107)2 ≤

(cid:112)|A|
(1 − γ)

(32)

(33)

where (32) follows from the fact that (cid:80)k
∂(cid:96)(π)
(33), we use that
normalized and 0 ≤ ηπ(s) ≤ 1 by our deﬁnition of the discounted state occupancy measure. As 1
L and c = κρ
γ ≥ 1/3, the claim follows by letting α = 1
(1−γ) in the statement of Lemma 3 part (1).

j=1(π(s, ej) − π(cid:48)(s, ej))2 ≤ 2 for any s ∈ S and π ∈ Π. For
(1−γ) since we assume per-step costs are
L for

∂π(s,ej ) = ηπ(s)Qπ(s, ej), where |Qπ(s, ej)| ≤ 1

k > 1

E.2 Regularized ﬁnite state and action MDPs with natural parameterization.

Lemma 10 (Impact of regularization). Let (cid:96)λ(π) denote the average cost function for the problem described
in Example 4 with a given regularization parameter λ ≥ 0. Then, if π∗

λ ∈ arg minπ∈Π (cid:96)λ(π),

(cid:96)0 (π∗

λ) ≤ min
π∈Π

(cid:96)λ(π) + λ

(cid:16)

1 + log

(cid:16)

1 +

(cid:17)(cid:17)

c
λ

where

c = 2(max
s,i

|gs,i|)/(1 − γ).

Proof. To make the dependence on λ explicit, we consider the MDP Mλ = (S, A, gλ, P, γ, ρ) where

gλ(s, a) = g(cid:62)

s a + λ · DKL(U ||a) =

k
(cid:88)

i=1

g0(s, ei)ai + λ · DKL(U ||a)

For the regularized problem, we write the appropriate terms as T π
λ, J ∗
notation, we write D(·||·) to denote DKL(·||·). We prove this result in a sequence of steps.

λ , Tλ, (cid:96)λ(π), Qπ

λ, Q∗

λ, J π

λ . For ease of

Step 1: Cost-function decomposition: Using the relationship between the per-step cost function and the average
cost function, we have that for any π ∈ Π,

(cid:96)λ(π) =

(cid:88)

s∈S

ηπ(s)gλ(s, π(s)) =

(cid:88)

s∈S

ηπ(s) (g0(s, π(s)) + λ · D(U ||π(s))) = (cid:96)0(π)+λ

(cid:88)

s∈S

ηπ(s)D(U ||π(s)).

(34)

15Note that there is a difference of a factor of (1−γ) in the denominator as compared to the result in Lemma D3 of Agarwal et al. [2020]
s∈S Jπ(s)ρ(s),

due to our deﬁnition of the policy gradient objective. While we consider the discounted average cost, (cid:96)(π) = (1 − γ) (cid:80)
Agarwal et al. [2020] consider the undiscounted objective, (cid:96)(π) = (cid:80)

s∈S Jπ(s)ρ(s).

43

Step 2: We construct a policy πλ with bounded suboptimality gap for the unregularized objective:
Essentially, we show that for πλ,

(cid:96)0(πλ) ≤ min
π∈Π

(cid:96)0(π) + λ

To do this, deﬁne πλ as the solution to the following regularized optimization problem,

πλ(s) = arg min
a(cid:62)e=1

k
(cid:88)

i=1

Q∗

0(s, ei)ai + λ · D(U ||a).

(35)

(36)

(cid:80)k

where Q∗
0 denotes the optimal state-action cost-to-go function of the unregularized MDP. We can interpret
D(U ||a) = − 1
i=1 log(ai) − log(k) as a log-barrier penalty function for the probability simplex with
k
effective regularization parameter λ/k. The log-barrier regularization plays a key role in the theory of
interior point methods [Boyd and Vandenberghe, 2004, Chaper 11]. In particular, we use a result [Boyd and
Vandenberghe, 2004, page 566] which shows that optimal solutions to log-barrier regularized problems are
near-optimal for un-regularized ones. For our construction in (36), this implies

k
(cid:88)

i=1

Q∗

0(s, ei)πλ(i|s) ≤ min
a∈∆k−1

k
(cid:88)

i=1

Q∗

0(s, ei)ai + λ

where πλ(s) = (πλ(1|s), · · · , πλ(k|s)). In terms of Bellman operators, this means

T πλ
0 J ∗

0 ≤ T0J ∗

0 + λe = J ∗

0 + λe

(37)

where e is a vector of 1’s. Repeatedly applying T πλ
and constant shift properties of the Bellman operator [Bertsekas, 1995] shows J πλ
this with deﬁnition of the average cost, (cid:96)0(π) = (1 − γ) (cid:80)

to each side of this expression and using the monotonicity
1−γ e. Combining

s ρ(s)J0,π(s), shows our result in (35).

0 (cid:22) J ∗

0 + λ

0

Step 3: Bound D(U ||πλ(s)): Combining the results in (35) with the cost decomposition in (34) implies,

(cid:96)λ(πλ) ≤ min
π∈Π

(cid:96)0(πλ) + λ

(cid:88)

s∈S

ηπ(s)D(U ||πλ(s))

(38)

We now show an upper bound on D(U ||πλ(s)) which shows our result. For simplicity, focus on a single ﬁxed
state s and take

a∗ = πλ(s) = arg min
a(cid:62)e=1

k
(cid:88)

Q∗

0(s, ei)ai −

log(ai).

k
(cid:88)

λ
k

Without loss of generality, assume actions are ordered such that Q∗
j for each j, i.e. a∗
we must have that a∗

1 ≤ a∗

i=1
0(s, e1) ≥ Q∗
1 ≤ 1/k. From the ﬁrst order optimality conditions, we have

0(s, ej) for all j. In this case

i=1

Q∗

0(s, e1) −

(λ/k)
a∗
1

= Q∗

0(s, ej) −

(λ/k)
a∗
j

∀j ≥ 2.

This implies,

Q∗

0(s, e1) −

(λ/k)
a∗
1

=

1
1 − a∗
1





k
(cid:88)

j=2

(cid:18)

Q∗

0(s, e1) −



a∗
j

 =

(cid:19)

(λ/k)
a∗
1

1
1 − a∗
1





k
(cid:88)

j=2

(cid:32)

Q∗

0(s, ej) −

(cid:33)



a∗
j



(λ/k)
a∗
j

=

1
1 − a∗
1

k
(cid:88)

j=2

44

Q∗

0(s, ej)a∗

j −

λ · (k − 1)/k
1 − a∗
1

Rearranging terms gives

λ/k
a∗
1

−

λ · (k − 1)/k
1 − a∗
1

= Q∗

0(s, e1) −

1
1 − a∗
1

k
(cid:88)

j=2

Q∗

0(s, ej)a∗

j := ∆.

Here, ∆ is roughly interpreted as the excess cost of action 1 over the weighted average of other actions. We use
the uniform bound ∆ ≤ c. Multiplying each side by k/λ and using that a∗
1 ≥ (k − 1)/k,

1 ≤ 1/k =⇒ 1 − a∗

1
a∗
1

=

k
λ

· ∆ +

k − 1
1 − a∗
1

≤

k
λ

· ∆ + k ≤ k

(cid:16)

1 +

(cid:17)

.

c
λ

Since D(U ||a∗) = 1
k

(cid:80)k

j=1 log

(cid:17)

(cid:16) 1
a∗
j

− log(k) ≤ log(1/a∗

1) − log(k) ≤ log (cid:0)1 + c

λ

(cid:1), we have that

λ · D(U ||πλ(s)) ≤ λ log

(cid:16)

1 +

(cid:17)

.

c
λ

Since this holds for every s, combining it with (38) gives the desired bound

min
π∈Π

(cid:96)λ(π) ≤ (cid:96)λ(πλ) ≤ min
π∈Π

(cid:96)0(π) + λ + λ log

(cid:16)

1 +

(cid:17)

.

c
λ

E.3 Regularized ﬁnite state and action MDPs with nonlinear parameterization.

∂θ

We prove Lemma 11 as discussed in Example 5. Given a feasible descent direction D, which by deﬁnition
means πθ + αD ∈ Π is a feasible policy for sufﬁciently small α, our goal is to verify that there exists N
solving the linear system (cid:2) ∂πθ
(cid:3) N = D. For simplicity, imagine there is only a single state, so that a policy
is described by the vector (πθ(1), · · · , πθ(k)). If there were multiple states, the same argument could be
repeated for each block of parameters corresponding to each distinct state.
Since we have effectively ﬁxed a choice of θ1 = 0 in Example 5, (cid:2) ∂πθ

(cid:3) N = D is a system of k equations
with k − 1 variables, denoted N = (N2, · · · , Nk). We ﬁrst temporarily ignore the ﬁrst linear equality and
show we can solve the remaining k − 1 equations: ∂πθ(i)
Nk = Di for each i ≥ 2. To see
∂θ2
this, consider the (k − 1) × (k − 1) submatrix of the Jacobian that comes from dropping the ﬁrst row:

N2 + · · · + ∂πθ(i)
∂θk

∂θ

(cid:20) ∂πθ(i)
∂θj

(cid:21)

=

i,j≥2








πθ(2)(1 − πθ(2))
−πθ(2)πθ(3)
...
−πθ(2)πθ(k)

−πθ(2)πθ(3)
πθ(3)(1 − πθ(3))
...
−πθ(3)πθ(k)

−πθ(2)πθ(k)
−πθ(3)πθ(k)
...

· · ·
· · ·
...
· · · πθ(k)(1 − πθ(k))








.

This matrix is diagonally dominant and therefore non-singular, implying a solution to these k − 1 linear
equations exist.

We now show that the ﬁrst linear equation is redundant and follows from the other k − 1. In particular, we

have

k
(cid:88)

j=2

∂πθ(1)
∂θj

Nj =

k
(cid:88)

(cid:32)

−

k
(cid:88)

j=2

i=2

∂πθ(i)
∂θj

(cid:33)

Nj

=



−

k
(cid:88)

i=2

k
(cid:88)

j=2

∂πθ(i)
∂θj



Nj

 = −

k
(cid:88)

i=2

Di = D1,

where the ﬁrst equality uses that πθ(1) + · · · + πθ(k) = 1 and the ﬁnal equality uses that D1 + · · · + Dk = 0
for any feasible descent direction.

45

E.4 LQ control

Preliminaries. We consider the LQ control problem as described in Example 2 with all the notations and
assumptions introduced there. Even though this example doesn’t ﬁt our general formulation as the per period
costs are not uniformly bounded, the important properties of Bellman operators that are used in our proofs
hold when restricting attention to stable linear policies and quadratic value functions. Deﬁne the set of strictly
convex quadratic cost to go functions as

Jq = {J : s ∈ Rn (cid:55)→ s(cid:62)Ks | K ∈ Rn×n, K (cid:31) 0}.

Lemma 18 (Bellman operators for LQ control). Consider the LQ control problem formulated in Example 2.
For J, ¯J ∈ Jq and a stable linear policy π ∈ {πθ : θ ∈ ΘS}, the following hold:

1. (Closure on the set of quadratic cost-to-go functions) TπJ ∈ Jq and T J ∈ Jq.
2. (Monotonicity) If J (cid:22) ¯J, then TπJ (cid:22) Tπ ¯J and T J (cid:22) T ¯J.

3. (Bellman equation) Jπ = TπJπ and Jπ = limk→∞ T k

π J. Moreover, J = T J if and only if J = J ∗.

We use these properties extensively for our analysis but omit the proofs as these results can be found16 in
standard textbooks [e.g. Bertsekas, 1995]. In addition, we use the following standard property of the trace
operator repeatedly in our analysis. This can be found in [Fang et al., 1994], for example. Let λmin(A) and
λmax(A) denote the minimum and maximum eigenvalues of a symmetric matrix respectively.

Lemma 19. For any two symmetric and positive semi-deﬁnite symmetric matrices A, B ∈ Rn×n,

λmin(A)Trace(B) ≤ Trace(AB) ≤ λmax(A)Trace(B)

Finally, we use several times the following observation. Recall that the second moment matrix of the initial

state, denoted Σρ = Eρ
Lemma 20. For a square matrix M with Frobenius norm (cid:107)M (cid:107)F = (cid:112)Trace(M (cid:62)M ),

(cid:3) is assumed to be ﬁnite and positive deﬁnite.

(cid:2)s0s(cid:62)

0

λmin (Σρ) (cid:107)M (cid:107)2

F ≤ Eρ

(cid:2)(cid:107)M s0(cid:107)2

2

(cid:3) ≤ λmax (Σρ) (cid:107)M (cid:107)2
F .

Proof. Using Lemma 19 gives

Eρ

(cid:2)(cid:107)M s0(cid:107)2

2

(cid:3) = Eρ

(cid:2)s(cid:62)

0 M (cid:62)M s0

(cid:3) = E (cid:2)Trace (cid:0)s(cid:62)

0 M (cid:62)M s0

(cid:1)(cid:3) = Trace (cid:0)M (cid:62)M Σρ

(cid:1)

≥ λmin (Σρ) Trace(M (cid:62)M )
= λmin (Σρ) (cid:107)M (cid:107)2
F .

An identical argument yields a corresponding upper bound.

Stability in LQ control. The following lemma is a straightforward adaption of classical understanding
of stability in linear dynamical systems to our setting, which considers the cumulative discounted cost
incurred from a random initial state. To ensure reproducibility, a proof of the following result is given in the
supplementary Technical report [Bhandari and Russo, 2021].

Lemma 21. In the LQ control problem formulated in Example 2, (cid:96)(θ) < ∞ if and only if θ ∈ ΘS.

16It is worth mentioning that many references state such results in terms of the cost matrices instead of the functions J ∈ Jq. For
example, the uniqueness of solutions to the Bellman optimality equation within Jq is identical to the more common statement that the
algebraic Riccatti equation has a unique positive deﬁnite solution.

46

Smoothness properties for LQ control. Recall the cost cost function in LQ control is

Jπθ (s0) = lim
T →∞

T
(cid:88)

t=0

γts(cid:62)

t (θ(cid:62)Rθ + C)st where

st = [A + Bθ]ts0.

√

Since st is differentiable in θ, differentiability properties for ﬁnite T follow almost immediately. For stable
γ)tst → 0 at a geometric rate, so later terms in the sum have a negligible contribution to the
policies, (
total cost. It is then natural that smoothness properties hold for stable policies, which is shown carefully by
Rautert and Sachs [1997] (though derivative calculations seem to date back even to Kalman et al. [1960]). The
next lemma states this ﬁnding in a form that is sufﬁcient to apply results on the convergence of ﬁrst-order
algorithms like Lemma 2. The idea is that, beginning with a stable linear policy θ0, iterates produced by ﬁrst
order methods with appropriate step-sizes are assured to stay in the sublevel set C(cid:96)(θ0), which only contains
stable policies.

Lemma 4. Consider the LQ control problem formulated in Example 2. The set ΘS is open and (cid:96) is twice
continuously differentiable on ΘS. For any α ∈ R, the sublevel set Cα := (cid:8)θ ∈ Rn×k : (cid:96)(θ) ≤ α(cid:9) is a
compact subset of ΘS and, if it is nonempty, supθ∈Cα (cid:107)∇2(cid:96)(θ)(cid:107)2 < ∞.

Proof. That any sublevel set only contains stable policies, i.e. Cα ⊆ ΘS, is an immediate consequence of
Lemma 21. With a little bit of algebraic simpliﬁcation (see Rautert and Sachs [1997] in continuous time and
Bu et al. [2019] in discrete time), one can show that (cid:96)(θ) is twice continuously differentiable for any θ ∈ ΘS
and hence over sublevel sets (as Cα ⊆ ΘS).

We show that sublevel sets are compact by showing that they are closed and bounded. As (cid:96)(·) is con-
tinuous over ΘS, by deﬁnition its sublevel sets are closed. We show (cid:96)(θ) is a coercive function, meaning
lim(cid:107)θ(cid:107)→∞ (cid:96)(θ) = ∞. (By the equivalence of norms in ﬁnite dimensional spaces, the deﬁnition does not
depend on the choice of matrix norm.) By deﬁnition, sublevel sets of a coercive function are bounded, (see for
example [Peressini et al., 1988]) so this completes our argument. To show (cid:96)(·) is coercive, we lower bound it
using Lemma 20 as

(1 − γ)−1(cid:96)(θ) = Eρ

γts(cid:62)

t (θ(cid:62)Rθ + C)st

(cid:35)

(cid:34) ∞
(cid:88)

t=0

≥ Eρ

(cid:2)(θs0)(cid:62)R(θs0)(cid:3) ≥ λmin(R)Eρ

(cid:2)(cid:107)θs0(cid:107)2

2

(cid:3)

which clearly tends to inﬁnity as (cid:107)θ(cid:107)2

F → ∞. Recall that Σρ = Eρ

(cid:2)s0s(cid:62)

0

To prove the ﬁnal claim, observe that as (cid:96)(·) is twice continuously differentiable, (cid:107)∇2(cid:96)(θ)(cid:107)2 is a continuous
function. Because any sublevel set Cα of (cid:96)(·) is compact, the Extreme Value Theorem implies (cid:107)∇2(cid:96)(θ)(cid:107)2 is
bounded on any sublevel set, i.e. maxθ∈Cα (cid:107)∇2(cid:96)(θ)(cid:107)2 < ∞.

≥ λmin(R)λmin(Σρ)(cid:107)θ(cid:107)2
F ,

(cid:3).

Optimality of stationary points for LQ control: Proof of Lemma 5.

Lemma 5. For the LQ control problem formulated in Example 2, any stable linear policy θ satisﬁes ∇(cid:96)(θ) = 0
if and only if Jπθ = J ∗.

Proof. If θ deﬁnes an optimal policy, then the ﬁrst order necessary conditions imply ∇(cid:96)(θ) = 0. Now consider
a sub-optimal stable linear policy πθ and let πθ be the policy iteration update to πθ. That is, θ satisﬁes
Jπθ = T Jπθ . Set θα = (1 − α)θ + αθ for some α ∈ [0, 1]. As both πθ and πθ are linear policies, this
Tπθ

47

implies, πθα (s) = (1 − α)θs + αθs . For every s ∈ Rn,

Tπθα Jπθ (s) = Qπθ (s, πθα (s)) = Qπθ (s, (1 − α)θs + αθs)

≤ (1 − α)Qπθ (s, θs) + αQπθ (s, θs)
= (1 − α)Tπθ Jπθ (s) + αTπθ
Jπθ (s)
= (1 − α)Jπθ (s) + αT Jπθ (s)
= Jπθ (s) − α (Jπθ (s) − T Jπθ (s))

(39)

where the ﬁrst inequality uses that a (cid:55)→ Qπθ (s, a) is convex, as noted in Section 4. As Jπθ (cid:23) T Jπθ , we
conclude from (39) that Jπθ (cid:23) Tπθα Jπθ . Repeatedly applying the Bellman operator and using the monotonicity
property gives,

Jπθ (cid:23) Tπθα Jπθ (cid:23) T 2

πθα Jπθ (cid:23) · · · (cid:23) lim
k→∞

T k
πθα Jπθα = Jπθα .

(40)

As, Jπθα (cid:22) Jπθ , the interpolated policy πθα is stable. Then from (39) and (40), we have

Jπθα − Jπθ
α

(cid:22)

Tπθα Jπθ − Jπθ
α

(cid:22) [T Jπθ − Jπθ ] .

Multiplying each side by (1 − γ), taking the expectation over s0 drawn from the initial distribution ρ, and then
taking α → 0 gives

d
dα

(cid:96)(θα)

(cid:12)
(cid:12)
(cid:12)
(cid:12)α=0

≤ (1 − γ) Eρ [T Jπθ (s0) − Jπθ (s0)] = −(1 − γ)Eρ [E(s0)]

(41)

where we have denoted the error in Bellman’s equation by E(s) (cid:44) Jπθ (s) − T Jπθ (s).

We know E(s) ≥ 0 for all s because Jπθ (cid:23) T Jπθ and the inequality is strict at some s because the policy
πθ is sub-optimal by assumption and therefore Jπθ (cid:54)= T Jπθ . We argue that Eρ[E(s0)] > 0, showing that the
right hand side of (41) is negative and hence θ cannot be a stationary point. To show this, we ﬁrst observe that,
since Jπθ ∈ Jq and T Jπθ ∈ Jq are both quadratic functions (See Lemma 18), E(s) is a quadratic function.
We can then write E(s) = s(cid:62)Ks for some symmetric matrix K satisfying K (cid:23) 0 and K (cid:54)= 0. Applying
Lemma 20 gives,

Eρ [E(s0)] = Eρ

(cid:2)s(cid:62)

0 Ks0

(cid:3) = Eρ

(cid:104)

(cid:107)K 1/2s0(cid:107)2
2

(cid:105)

≥ λmin(Σρ)(cid:107)K 1/2(cid:107)2

F = λmin(Σρ) · Trace(K) > 0.

Concentrability coefﬁcient for LQ control. We ﬁrst recall the lemma statement from Section 7.

Lemma 14. Consider the LQ control problem formulated in Example 2. Let θ∗ ∈ Rn×k denote the parameter
of an optimal policy and deﬁne Σρ := Eρ

(cid:3). Then,

(cid:3) and Σηπθ∗ := Eηπθ∗

(cid:2)s0s(cid:62)

(cid:2)s0s(cid:62)

0

0

(cid:107)J − J ∗(cid:107)1,ρ ≤

κ
(1 − γ)

(cid:107)J − T J(cid:107)1,ρ

∀J ∈ {Jπθ : θ ∈ ΘS}

when

κ =

(cid:16)

λmax

Σηπθ∗
λmin (Σρ)

(cid:17)

.

(22)

The proof leverages the performance difference lemma, described in Subsection D.1 and equation (29) in
particular. Here, to make precise the result used in the proof, we state an analogue of that formula in linear
quadratic control, which restricts to stable policies to rule out divergent sums. The proof is essentially identical
to that leading to (29) and is therefore omitted. Observe that, since the dynamics are deterministic with states
evolving according to st+1 = (A + Bθ)st, all randomness is due to the initial state s0 drawn from ρ.

48

Lemma 22 (Performance difference lemma for LQ control). Consider the LQ control problem formulated in
Example 2. For any θ, θ ∈ ΘS,

(cid:96)(θ) − (cid:96)(θ) = (1 − γ)Eπθ
ρ

γt (cid:0)Tπθ

Jπθ (st) − Jπθ (st)(cid:1)

(cid:35)

(cid:90)

=

(cid:2)Tπθ

Jπθ − Jπθ

(cid:3) dηπθ

.

(cid:34) ∞
(cid:88)

t=0

Proof of Lemma 14. For any πθ ∈ ΘS, by Lemma 18, Jπθ ∈ Jq and T Jπθ ∈ Jq. Therefore Jπθ −T Jπθ ∈ Jq,
which means there exists symmetric K ∈ Rn×n such that Jπθ (s) − T Jπθ (s) = s(cid:62)Ks for all s. Since
Jπθ (cid:23) T Jπθ , we have that K (cid:23) 0. We get

(cid:107)Jπθ − T Jπθ (cid:107)1,ρ = Eρ [Jπθ (s0) − T Jπθ (s0)] = Eρ

(cid:2)s(cid:62)

0 Ks0

(cid:3) = Trace(KΣρ).

(42)

This simpliﬁes the right hand side in the deﬁnition of κρ. To simplify the left hand side, we use the performance
difference lemma above (with a choice θ = θ∗),

(1 − γ)(cid:107)Jπθ − J ∗(cid:107)1,ρ = − ((cid:96)(θ∗) − (cid:96)(θ)) =

(cid:90)

(cid:2)Jπθ − Tπθ

Jπθ

(cid:3) dηπθ∗ ≤

(cid:90)

[Jπθ − T Jπθ ] dηπθ∗

= Eηθ∗

= Trace

(cid:3)

(cid:2)s(cid:62)
0 Ks0
(cid:16)
KΣηπθ∗

(cid:17)

,

where the inequality used that T J (cid:22) Tπθ∗ J holds for all cost-to-go functions J. Combining this with (42) and
applying Lemma 19 gives,

(cid:107)Jπθ − J ∗(cid:107)1,ρ
(cid:107)Jπθ − T J(cid:107)1,ρ

≤

1
1 − γ

·

Trace(KΣηπθ∗ )
Trace(KΣρ)

≤

1
1 − γ

·

(cid:16)

λmax

Σηπθ∗
λmin(Σρ)

(cid:17)

.

E.5 Optimal Stopping

We now consider the optimal stopping problem as described in Example 7, continuing with the notation and
assumptions introduced there. Recall that in our formulation, for each context x ∈ X , the offer distribution is
assumed to have a density, qx(·), with continuous derivative and support {y ∈ R : qx(y) > 0} = (ymin, ymax)
where ymin > 0. We set Y = [ymin, ymax]. We also assume the initial distribution places zero probability
on trivial instances that begin in the terminal state (i.e ρ(τ ) = 0) and factorizes over continuation states as
ρ(x, dy) = ν(x)qx(y)dy where ν(x) > 0 for all x ∈ X .

Preliminaries and notation. Technically, the state at time t consists of both the context xt and the offer
yt. But since yt depends only on xt and not on the previous state, it is helpful to work directly with the state
variable xt and then separately take expectations over offers drawn from qxt(·). To this end, we develop
some specialized notation. Let η(cid:48)
π denote the marginal distribution over X ∪ {τ } under the discounted state
occupancy measure ηπ, and note that

(cid:16)

ηπ

{x} × (y1, y2)

(cid:17)

= η(cid:48)

π(x)

y2(cid:90)

y1

qx(y)dy.

(43)

We ﬁnd it convenient to directly work with η(cid:48)

π and
qx(y) that will be used throughout the analysis. For an action a ∈ [0, 1], indicating a probability of stopping,

π and qx(y). We now state several formulas in terms of η(cid:48)

(cid:34)

Qπ((x, y), a) =

ay + (1 − a)γ

p(x(cid:48)|x)

(cid:88)

x(cid:48)∈X

(cid:90)

Y

49

Jπ((x(cid:48), y(cid:48)))qx(cid:48)(y(cid:48))dy(cid:48)

= ay + (1 − a)cπ(x)

(cid:35)

where

cπ(x) := γ

p(x(cid:48)|x)

(cid:88)

x(cid:48)∈X

(cid:90)

Y

Jπ(x(cid:48), y(cid:48))qx(cid:48)(y(cid:48))dy(cid:48) ∈ [0, γymax]

(44)

is called the “continuation value” from context x under policy π. That cπ(x) ≤ γymax is due the basic fact
that Jπ(x(cid:48), y(cid:48)) ≤ ymax, i.e. no policy can accrue expected reward exceeding the maximum possible offer.
Similarly, the weighted policy iteration objective becomes,

B (θ | ηπ, Jπ) =

(cid:88)

x∈X

η(cid:48)
π(x)

=

(cid:88)

x∈X

η(cid:48)
π(x)

ymax(cid:90)

Qπ((x, y) , 1(y > θx))qx(y)dy

ymin
ymax(cid:90)

[y1(y > θx) + cπ(x)1(y < θx)] qx(y)dy.

(45)

ymin

Here we use that there is zero value-to-go from the terminal state τ to restrict the sum to continuation states.
We now proceed to verify all the conditions needed to apply our general results.

Condition 1: Closure under policy improvement.
It is easy to verify that the class of threshold policies is
closed under policy improvement. For any π ∈ ΠΘ, the policy iteration update for any state s = (x, y) ∈ SC
is given by

π+(x, y) = arg max
a∈{0,1}

Qπ((x, y), a) = 1(y > cπ(x)).

This result can be seen immediately from the formula for Qπ(·) given above. Clearly the policy iteration
update is another threshold policy. In particular, the policy iteration update to π is a new threshold policy πθ+
with parameters

θ+
x = max{ymin , cπ(x)}.

(46)

for each x ∈ X . This works because a policy with stopping threshold ymin accepts the next offer with
probability one when cπ(x) < ymin and otherwise and we are assured that cπ(x) ∈ [ymin, γymax] is a feasible
choice for θ+
x .

Condition 2.A: No suboptimal stationary points for the weighted PI objective. This result here is implied
by the gradient dominance result that follows, but it provides a warmup for that analysis.

We now show that θ (cid:55)→ B(θ|ηπ, Jπ) has no suboptimal stationary points for each π ∈ ΠΘ. As we

formulate the optimal stopping example as a maximization problem, any stationary point θ satisﬁes,

∂
∂θx

B(θ|η, Jπ) · (θ(cid:48)

x − θx) ≤ 0

∀θ(cid:48)

x ∈ Y,

for every x ∈ X . It is possible to separate the stationarity condition into a componentwise inequality in this
manner because the parameters space is the Cartesian product Θ = Y |X |.

From (45), we have the following formula for the derivative:

∂
∂θx

B(θ | ηπ, Jπ) = (cπ(x) − θx) η(cid:48)

π(x)qx(θx)

(47)

Therefore, θ is stationary point when (cπ(x) − θx) · (θ(cid:48)
x ∈ [ymin, ymax]. If cπ(x) ∈
(ymin, ymax) is in the interior of the feasible region, this implies θx = cπ(x). Otherwise (since it is impossible
to have cπ(x) ≥ ymax), we have cπ(x) ≤ ymin and a stationary point must satisfy θx = ymin. We have
found any stationary point θ satisﬁes, θx = max{ymin, cπ(x)}, which matches the formula (46) for the policy
iteration update that maximizes θ (cid:55)→ B(θ | ηπ, Jπ).

x − θx) ≤ 0 for all θ(cid:48)

50

Concentrability coefﬁcient for optimal stopping. We ﬁrst recall the claim.

Lemma 13. For the optimal stopping problem in Example 7, consider a policy πC that never stops, i.e.
πC(s) = 1 for all s ∈ SC. Let µ be a stationary distribution of the induced Markov process, meaning
µ(M) = (cid:82) P (M|s(cid:48), 1)µ(ds(cid:48)) for any M ⊂ S. Then, choosing ρ = µ implies κρ ≤ 1.
Proof. We show that the Bellman operator T is a contraction with modulus γ in (cid:107) · (cid:107)1,µ. The proof then
follows immediately using part (c) of Theorem 4.

For a policy that never stops, the stationary distribution over continuation states, (x, y) ∈ SC factorizes
as µ(x, y) = µ(cid:48)(x)qx(y) where µ(cid:48) is the marginal stationary distribution over context states X such that
µ(cid:48)(x(cid:48)) = (cid:80)

x∈X µ(cid:48)(x)p(x(cid:48)|x). Then, for any bounded cost-to-go functions J, J (cid:48) ∈ J ,

(cid:107)T J − T J (cid:48)(cid:107)1,µ =

µ(cid:48)(x)

(cid:88)

x∈X

(cid:90)

Y

|T J(x, y) − T J (cid:48)(x, y)| qx(y)dy.

By deﬁnition,

T J(x, y) = max{y, γ

(cid:88)

p(x(cid:48)|x)

(cid:90)

J(x(cid:48), y(cid:48))qx(cid:48)(y(cid:48))dy(cid:48)}.

x(cid:48)∈X
Note that for any scalars (x1, x2, y), we have | max{y, x1} − max{y, x2}| ≤ |x1 − x2|. Therefore,

Y

|T J(x, y) − T J (cid:48)(x, y)| ≤ γ

p(x(cid:48)|x)

(cid:88)

x(cid:48)∈X

(cid:90)

Y

|J(x(cid:48), y(cid:48)) − J (cid:48)(x(cid:48), y(cid:48))| qx(cid:48)(y(cid:48))dy(cid:48).

(48)

As the right hand side in (48) is independent of y, integrating (48) with respect to qx(·) gives

|T J(x, y) − T J (cid:48)(x, y)| qx(y)dy ≤ γ

p(x(cid:48)|x)

(cid:88)

x(cid:48)∈X

(cid:90)

Y

|J(x(cid:48), y(cid:48)) − J (cid:48)(x(cid:48), y(cid:48))| qx(cid:48)(y(cid:48))dy(cid:48).

(cid:90)

Y

Therefore,

(cid:107)T J − T J (cid:48)(cid:107)1,µ =

µ(cid:48)(x)

(cid:88)

x∈X

(cid:90)

Y

|T J(x, y) − T J (cid:48)(x, y)| qx(y)dy

≤ γ

(cid:88)

µ(cid:48)(x)

(cid:88)

p(x(cid:48)|x)

(cid:90)

Y

|J(x(cid:48), y(cid:48)) − J (cid:48)(x(cid:48), y(cid:48))| qx(cid:48)(y(cid:48))dy(cid:48)

|J(x(cid:48), y(cid:48)) − J (cid:48)(x(cid:48), y(cid:48))| qx(cid:48)(y(cid:48))dy(cid:48)

x∈X
(cid:88)

(a)
= γ

x(cid:48)∈X
(cid:90)

µ(cid:48)(x(cid:48))

x(cid:48)∈X
= γ(cid:107)J − J (cid:48)(cid:107)1,µ

Y

where (a) follows as µ(cid:48) is the stationary distribution over X . For ρ = µ, we have C, c = 1 in part (c) of
Theorem 4, implying that κρ ≤ 1.

Further results in the supplementary technical report. One expects that many smoothness results hold
for this problem (since with a continuous offer distribution, inﬁntesmal changes to a stopping threshold should
have an inﬁntesimal impact on performance), and we have already calculated one derivative in (47). For
completeness, we give a detailed veriﬁcation of the differentiability properties in Condition 0 in the Bhandari
and Russo [2021].

For a reader who is interested in this speciﬁc optimal stopping problem, we note that it possible to prove
stronger results that yield convergence rates rather than just convergence to an optimal policy. For example, the
electronic companion proves the following gradient dominance and smoothness results, which are sufﬁcient
to guarantee the convergence rates in Lemma 3. Notice that the gradient dominance constant depends on a
measure of the degree of uniformity in the offer distribution qx(·). We know β is ﬁnite because X is ﬁnite,
qx(y) > 0 for each y ∈ Y (by assumption), and Y is compact.

51

Lemma 23 (Gradient dominance for optimal stopping). Consider the optimal stopping problem formulated
in Example 7. For any π ∈ ΠΘ, the function θ (cid:55)→ B(θ|ηπ, Jπ) is (β, 0)–gradient-dominated where β =
maxx∈X ,y∈Y qx(y)/ minx∈X ,y∈Y qx(y).

Lemma 24. For the optimal stopping problem in Example 7, maxθ∈Θ (cid:107)∇2(cid:96)(θ)(cid:107) < ∞.

E.6 Finite horizon inventory control

Differentiability and derivative calculations.
In inventory control, an attractive approach for computing
derivatives with respect to policy parameters is described in detail in Glasserman and Tayur [1995]. One
can compute derivatives of expected costs with respect to the base-stock levels by calculating the derivative
for many simulated sample paths (i.e. realizations of the initial state and demands) and averaging the result.
Making such an argument rigorous requires justifying the exchange of an integral and derivative. This is
essentially treated in past work like Glasserman and Tayur [1995], but for reproducibility we carefully verify
the partial differentiability requirements in Condition 0 in the technical report [Bhandari and Russo, 2021].
The proof also shows that under any base-stock policy, the distribution of the inventory levels (x0, x1, · · · ) has
a density, which is a basic consequence of the assumption that the demands follow a continuous distribution.

Verifying Condition 4. The following lemma shows how Condition 4 holds for the ﬁnite horizon inventory
control problem.

Lemma 25. Consider the ﬁnite horizon inventory control problem in Example 8. Let J ∗ be the cost-to-go
function corresponding to the optimal policy. Then, for any π, πθ ∈ ΠΘ, the weighted policy iteration objective
B(θ|ηπ, J ∗) has no suboptimal stationary points.

Proof. Recall Q∗(s, a) = Qπ∗ (s, a) denotes the Q-function corresponding to an optimal policy. We follow a
classical approach to rewriting costs as a function of the target inventory level x + a rather than the state and
action. We ﬁnd

Q∗((x, h), a) = c · a + Ew [b max{x + a − w, 0} + p max{−x − a + w, 0} + J ∗((x + a − w, h + 1))]

= c · x + Gh(x + a)

where the expectation is taken over the demand distribution and Gh(y) := Ew[b max{y −w, 0}+p max{−y +
w, 0} + J ∗((y − w, h + 1))]. Here y is thought of as a target inventory level. The function Gh(·) is well
known to be convex [see e.g Bertsekas, 1995].

Recall that πθ((x, h)) = max{θh − x , 0}. We can then calculate the derivative of the weighted PI costs

in terms of Gh(·) as

∂
∂θh

B(θ|ηπ, J ∗) =

=

(a)
=

∂
∂θh
∂
∂θh
(cid:90)

(cid:90)

(cid:20)(cid:90)

x<θh
∂
∂θh

Q∗ ((x, h), πθ(x, h)) ηπ(dx, h)

Q∗((x, h), θh − x) ηπ(dx, h) +

(cid:90)

x>θh

Q∗((x, h), 0) ηπ(dx, h)

(cid:21)

Q∗((x, h), θh − x) ηπ(dx, h)

x<θh

(cid:90)

=

x<θh

= G(cid:48)

h(θh)

G(cid:48)

h(θh) ηπ(dx, h)

(cid:90)

x<θh

ηπ(dx, h)

where (a) uses the Leibniz rule. Since θh is constrained to lie above 0, and negative (i.e. back-ordered)
inventory levels are possible under the initial distribution, we know (cid:82)
ηπ(dx, h) > 0. Let θ∗ an optimal

x<θh

52

vector of base-stock levels, so θ∗
base-stock level), then by convexity G(cid:48)
B(·|ηπ, J ∗).

h ∈ arg miny Gh(y). If θh is not a minimizer of Gh(·) (so it is a suboptimal
h − θh) < 0, implying θh cannot be a stationary point of

h(θh) · (θ∗

E.7 Linear MDPs: proof of Lemma 12

The proof sketch in the body of the paper already established Condition 1 and that

d
dα

B(πθα | ηπ, Jπ) =

(cid:90) d
dα

Qπ(s, πθα (s))ηπ(ds) =

(cid:90) d
dα

Qθ+

(s, πθα (s))ηπ(ds).

Our remaining goal is to show that d
(s, ·). To
see this, note that one can rewrite the problem maxa∈A Qπ(s, a) as maxa1:k−1∈X f (a1:k−1, y) where f is as
in Lemma 26, a1:k−1 are components of the action except ak (which is redundant, as ak = 1−a1 −· · ·−ak−1)
and yj = e(cid:62)

k Φ(s)θ. Applying Lemma 26 then establishes the claim.

(s, πθα(s)) < 0 for any state at which Qθα

j Φ(s)θ − e(cid:62)

(s, ·) (cid:54)= Qθ+

dα Qθ+

Lemma 26. Deﬁne X = {x ∈ (0, 1)k−1 : (cid:80)k−1

i=1 xi < 1} and f : X × Rk−1 → R by

f (x, θ) = θ(cid:62)x + λH(x)

where H(x) = (cid:80)k

i=1

1
k log

(cid:17)

(cid:16) 1/k
xi

with xk ≡ 1 − (cid:80)k−1

i=1 xi. For ﬁxed θ0 ∈ Rk−1, deﬁne

h(θ) = f (x∗(θ), θ0) where x∗(θ) ≡ arg min

x∈X

f (x, θ).

Then h(·) is differentiable, θ0 is its unique minimizer, and (θ0 −θ)(cid:62)∇h(θ) = −(cid:107)θ0 −θ(cid:107)2

(∇2H(x∗(θ)))−1/λ < 0.

i (θ) = Ce−θi/λ where
Proof. The function H satisﬁes ∇2H(x) (cid:31) 0. It is well known that x∗(θ) satisﬁes x∗
C = 1 + (cid:80)k
j=1 e−θj /λ. Instead of using this formula, we use implicit differentiation to derive a formula for
∇x∗(θ). Since x∗(θ) is an interior solution (i.e. 0 < (cid:80)k
i=2 x∗(θ)i < 1), the ﬁrst order condition implies
θ + λ∇H(x∗(θ)) = 0. Differentiating this expression again and re-arranging terms yields the formula
∇x∗(θ) = − 1
λ

(cid:0)∇2H(x∗(θ))(cid:1)−1

∈ Rk×k. Then

∇h(θ) = (∇x∗(θ))θ0 + λ(∇x∗(θ))∇H(x∗(θ)) = (∇x∗(θ))(θ0 − θ) = −λ−1 (cid:0)∇2H(x∗(θ))(cid:1)−1

(θ0 − θ)

where the second equality uses the ﬁrst order optimality condition. Taking the dot product of both sides of the
equation above with θ0 − θ yields the result.

53

