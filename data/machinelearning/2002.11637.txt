Learning Navigation Costs from Demonstration in Partially Observable
Environments

Tianyu Wang

Vikas Dhiman

Nikolay Atanasov

0
2
0
2

b
e
F
6
2

]

G
L
.
s
c
[

1
v
7
3
6
1
1
.
2
0
0
2
:
v
i
X
r
a

Abstract— This paper focuses on inverse reinforcement learn-
ing (IRL) to enable safe and efﬁcient autonomous navigation
in unknown partially observable environments. The objective
is to infer a cost function that explains expert-demonstrated
navigation behavior while relying only on the observations and
state-control trajectory used by the expert. We develop a cost
function representation composed of two parts: a probabilistic
occupancy encoder, with recurrent dependence on the obser-
vation sequence, and a cost encoder, deﬁned over the occu-
pancy features. The representation parameters are optimized
by differentiating the error between demonstrated controls
and a control policy computed from the cost encoder. Such
differentiation is typically computed by dynamic programming
through the value function over the whole state space. We
observe that this is inefﬁcient in large partially observable
environments because most states are unexplored. Instead, we
rely on a closed-form subgradient of the cost-to-go obtained
only over a subset of promising states via an efﬁcient motion-
planning algorithm such as A* or RRT. Our experiments show
that our model exceeds the accuracy of baseline IRL algorithms
in robot navigation tasks, while substantially improving the
efﬁciency of training and test-time inference.

I. INTRODUCTION
Practical applications of autonomous robot systems in-
creasingly require operation in unstructured, partially ob-
served, unknown, and changing environments. Achieving
safe and robust navigation in such conditions is directly
coupled with the quality of the environment representation
and the cost function specifying desirable robot behavior.
Designing a cost function that accurately encodes safety,
liveness, and efﬁciency requirements of navigation tasks is a
major challenge. In contrast, it is signiﬁcantly easier to obtain
demonstrations of desirable behavior. The ﬁeld of inverse re-
inforcement learning [1]–[3] (IRL) provides numerous tools
for learning cost functions from expert demonstration.

We consider the problem of safe navigation from partial
observations in unknown environments. We assume that
demonstrations containing expert poses, controls, and sen-
sory observations over time are available. The objective is to
infer the cost function, which depends on the observation
sequence, and explains the demonstrated behavior. This
inference can only be done indirectly by comparing the
control inputs, that a robot may take based on its current
cost representation, to the expert’s actions in that situation.
Learning a cost function requires a differentiable control
policy with respect to the stage cost parameters. Computing
such derivatives has been addressed by several successful
approaches [4]–[7]. Ratliff et al. [4] developed algorithms

We gratefully acknowledge support from NSF CRII IIS-1755568, ARL

DCIST CRA W911NF-17-2-0181, and ONR SAI N00014-18-1-2828.

The authors are with the Department of Electrical and Computer Engi-
neering, University of California, San Diego, La Jolla, CA 92093, USA
{tiw161,vdhiman,natanasov}@eng.ucsd.edu

Fig. 1: Architecture for learning cost function representations from
demonstrations. Our main contribution lies in a non-stationary cost
representation, combining a probabilistic occupancy map encoder,
with recurrent dependence on observations z1:t, and a cost encoder,
deﬁned over the occupancy features. We also perform efﬁcient (in
the size of the state-space) forward non-stationary policy computa-
tion and efﬁcient (closed-form subgradient) backpropagation. The
bottom-right plot illustrates that motion planning may be used to
update and differentiate cost-to-go estimates only in promising areas
of the state space rather than using full Bellman backups, which is
particularly redundant in partially observable environments.

with regret bounds for computing subgradients of planning
algorithms (e.g., A* [8], RRT [9], [10], etc.) with respect
to the cost features. Ziebart et al. [5] developed a dynamic
programming algorithm for computing the expected state
visitation frequency of a policy extracted from expert demon-
strations to enforce that it earns the same reward as the
demonstrated policy. Tamar et al. [6] showed that the value
iteration algorithm could be approximated using a series
of convolution (computing value function expectation over
stochastic transitions) and maxpooling (choosing the best ac-
tion) allowing automatic differentiation. Okada et al. [7] pro-
posed path integral networks in which the control sequence
resulting from path integral control may be differentiated
with respect to the controller parameters. All of these works,
however, assume a known environment and only optimize the
parameters of a cost function deﬁned over it.

A series of recent works [11]–[16] address IRL under
partial observability. Wulfmeier et al. [13] train deep neu-
ral network representations for Ziebart et al.’s Maximum
Entropy IRL [5] method and consider streaming lidar scan
observations of the environment. Karkus et al. [16] formulate
the IRL problem as a POMDP [17], including the robot
pose and environment map in the state. Since a na¨ıve
representation of the occupancy distribution may require
exponential memory in the map size, the authors assume
partial knowledge of the environment (the structure of a
building is known but the furniture placement is not). A
control policy is obtained via the SARSOP [18] algorithm,

Explored areaGoalArea of Bellman updateUnexplored areaState spaceCurrent positionMotion planningMotionplanningPolicy evaluationStoch. Grad. Descent on Closed form gradientTraining phaseMap encoderCostencoderCost representationMotionplanningTesting phaseMap encoderCostencoderCost representation 
 
 
 
 
 
↓Reference \Feature → POE

PBB

DNN

CfG

VIN [6]
CMP [14]
MaxEntIRL [5]
MaxMarginIRL [4]
DAN [16]
Ours

(cid:55)
(cid:51)
(cid:55)
(cid:55)
(cid:51)†
(cid:51)

(cid:55)
(cid:55)
(cid:55)
(cid:51)
(cid:55)
(cid:51)

(cid:51)
(cid:51)
(cid:55)
(cid:55)
(cid:51)
(cid:51)

(cid:55)
(cid:55)
(cid:51)
(cid:51)
(cid:55)
(cid:51)

TABLE I: Comparison with closely related work based on the use of
Partially Observable Environments (POE), Partial Bellman Backups
(PBB), Deep Neural Network (DNN) representation, and Closed-
form Gradients (CfG). PBB refers to computing and differentiating
values over a subset of promising states as opposed to the whole
state space. † DAN [16] works with uncertainty only on the robot or
furniture location, while the main environment structure is known.

which approximates the cost-to-go function only over an
optimally reachable space. Gupta et al. [14] address visual
navigation in partially observed environments while using
hierarchical VIN as the planner. Khan et al. [19] introduce
a memory module to VIN to address partial observability.

Many IRL algorithms rely on dynamic programming,
including VIN and derivatives [14], [19], which requires
updating cost-to-go estimates over all possible states. Our
insight is that in partially observable environments, the cost-
to-go estimates need to be updated and differentiated only
over a subset of states. Inspired by [4], we obtain cost-to-go
estimates only over promising states using a motion planning
algorithm. This helps to obtain a closed-form subgradient of
the cost-to-go with respect to the learned cost function from
the optimal trajectory. While Ratiff et al. [4] exploit this
observation in fully observable environments, none of the
works focusing on partial observability take advantage of
this. Our work differs from closely related works in Table I.
In summary, we offer two contributions illustrated in Fig. 1:
Firstly, we develop a non-stationary cost function represen-
tation composed of a probabilistic occupancy map encoder,
with recurrent dependence on the observation sequence, and
a cost encoder, deﬁned over the occupancy features (Sec. III).
Secondly, we optimize the cost parameters using a closed-
form subgradient of the cost-to-go obtained only over a
subset of promising states (Sec. IV).

II. PROBLEM FORMULATION

Consider a robot navigating in an unknown environment
with the task of reaching a goal state xg ∈ X . Let xt ∈ X be
the robot state, capturing its pose, twist, etc., at discrete time
t. For a given control input ut ∈ U where U is assumed ﬁnite,
the robot state evolves according to known deterministic
dynamics: xt+1 = f (xt, ut). Let m∗ : X → {−1, 1} be a
function specifying the true occupancy of the environment by
labeling states as either feasible (−1) or infeasible (1) and let
M be the space of possible environment realizations m∗. Let
c∗ : X × U × M → R≥0 be a cost function specifying desir-
able robot behavior in a given environment, e.g., according to
an expert user or an optimal design. We assume that the robot
does not have access to either the true occupancy map m∗ or
the true cost function c∗. However, the robot is able to make
observations zt ∈ Z (e.g., using a lidar scanner or a depth
camera) of the environment in its vicinity, whose distribution
depends on the robot state xt and the environment m∗. Given
a training set D := (cid:8)(xt,n, u∗
of N

t,n, zt,n, xg,n)(cid:9)Tn,N

t=1,n=1

c1

CNN

c2

CNN

Cost encoder

ux

h1

BF

ux

h2

BF

Map encoder

h0

ct

CNN

ux
ht

BF

x1, z1

x2, z2
Fig. 2: Neural network model of a cost function representation. A
Bayes ﬁlter with likelihood function parameterized by ψ, takes in
sequential observations z1:t and outputs a latent map representation
ht. A convolutional neural network, parameterized by φ, extracts
features from the map state to specify the cost ct at a given robot
state-control pair (x, u). The learnable parameters are θ = {ψ, φ}.

xt, zt

expert trajectories with length Tn to demonstrate desirable
behavior, our goal is to

• learn a cost function estimate ct : X × U × Z t × Θ →
R≥0 that depends on an observation sequence z1:t from
the true latent environment and is parameterized by θ,
• derive a stochastic policy πt from ct such that the robot
behavior under πt matches the prior experience D.
To balance exploration in partially observable environments
with exploitation of promising controls, we specify πt as a
Boltzmann policy [3], [20] associated with the cost ct:

πt(ut|xt; z1:t, θ) =

exp(−Q∗
u∈U exp(−Q∗

(cid:80)

t (xt, ut; z1:t, θ))

t (xt, u; z1:t, θ))

,

(1)

where the optimal cost-to-go function Q∗

t is:

Q∗

t (xt, ut; z1:t, θ) := min

ut+1:T −1

T −1
(cid:88)

k=t

ct(xk, uk; z1:t, θ)

(2)

s.t. xk+1 = f (xk, uk), xT = xg.

Problem. Given demonstrations D, optimize the cost func-
tion parameters θ so that log-likelihood of the demonstrated
controls u∗

t,n is maximized under the robot policies πt,n:

min
θ

L(θ) := −

N
(cid:88)

Tn(cid:88)

n=1

t=1

log πt,n(u∗

t,n|xt,n; z1:t, θ).

(3)

The problem setup is illustrated in Fig. 1. While Eqn. (2)
is a standard deterministic shortest path (DSP) problem, the
challenge is to make it differentiable with respect to θ.
This is needed to propagate the loss in (3) back through
the DSP problem to update the cost parameters θ. Once
the parameters are optimized, the robot can generalize to
navigation tasks in new partially observable environments
by evaluating the cost ct based on the observations z1:t
iteratively and (re)computing the associated policy πt.

III. COST FUNCTION REPRESENTATION

We propose a cost function representation comprised of

two components: a map encoder and a cost encoder.

The map encoder incrementally updates a hidden state
ht using the most recent observation zt obtained from
robot state xt. For example, a Bayes ﬁlter with likelihood

function parameterized by ψ can convert the sequential input
(x1:t, z1:t) into a ﬁxed-sized hidden state ht+1:

ht+1 = BF(ht, xt, zt; ψ).

(4)

The cost encoder uses the latent environment map ht
to deﬁne the cost function estimate ct(x, u) at a given
state-control pair (x, u). A convolutional neural network
(CNN) [21] with parameters φ can extract cost features from
the environment map:

ct(x, u) = CNN(ht, x, u; φ).

(5)

This conceptual model, combining recurrent estimation of
a hidden environment state, followed by feature extraction
to deﬁne the cost at (x, u) is illustrated in Fig. 2. The
model is differentiable by design, allowing its parameters
θ = {ψ, φ} to be optimized. We propose an instantiation
of this general model, speciﬁc to modeling occupancy costs
from depth measurements in robot navigation tasks.

A. Map Encoder

We encode the occupancy probability of different environ-
ment areas into a hidden state ht. In detail, we discretize X
into N cells and let m∗ ∈ {−1, 1}N be the vector of true
occupancy values over the cells. Since m∗ is unknown to
the robot, we maintain the occupancy likelihood P(m∗ =
1|x1:t, z1:t) given the history of states x1:t and observations
z1:t. See Fig. 3 for an example of a depth measurement zt
and associated occupancy likelihood over the map m∗. The
representation complexity may be simpliﬁed signiﬁcantly if
one assumes independence among the map cells m∗
j :

P(m∗ = 1|x1:t, z1:t) =

N
(cid:89)

j=1

P(m∗

j = 1|x1:t, z1:t) .

(6)

We use inspiration from occupancy grid mapping [22], [23]
to design recurrent updates for the occupancy probability of
each cell m∗
j is binary, its likelihood update can
be simpliﬁed by deﬁning the log-odds ratio of occupancy:

j . Since m∗

ht,j := log

P(m∗
P(m∗

j = 1|x1:t, z1:t)
j = −1|x1:t, z1:t)

.

The recurrent Bayesian update of ht,j is:

ht+1,j = ht,j + log

p(zt+1|m∗
p(zt+1|m∗

j = 1, xt+1)
j = −1, xt+1)

,

(7)

(8)

where the increment is a log-odds observation model. The
occupancy posterior can be recovered from the occupancy
log-odds ratio ht,j via a sigmoid function:

Fig. 3: A robot (green) navigating in a 2-D grid environment. The
true environment (left) is a 16 × 16 grid where black regions are
obstacles and white regions are free. On the right, the noisy lidar
beams with endpoints in red have maximum range 2.5. The robot
estimates the occupancy probability through an inverse observation
model in Eqn (14) (darker means higher probability of occupancy).

where Bayes rule was used to represent it in terms of an
inverse observation model gj(x, z) and a prior occupancy
log-odds ratio h0,j, whose value may depend on the en-
vironment (e.g., h0,j = 0 speciﬁes a uniform prior over
occupied and free cells). Note that map cells m∗
j outside
of the sensor ﬁeld of view at time t are not affected by
zt, in which case gj(xt, zt) = h0,j. Now, consider a cell
m∗
j along the direction of the k-th sensor ray, whose depth
measurement is zt,k. Let d(xt, m∗
j ) be the distance between
the robot position and the center of mass of cell m∗
j . We
model the occupancy likelihood of m∗
j as a truncated sigmoid
around the true distance d(xt, m∗

j ):

P(m∗

j = 1|xt, zt,k) =

(cid:40)

σ(ψkδzt,k)
σ(h0,j)

if δzt,k ≤ (cid:15)
if δzt,k > (cid:15)

,

(12)

where ψk is a learnable parameter, δzt,k := d(xt, m∗
j )−zt,k,
and (cid:15) is a distance threshold on the inﬂuence of the k-th
sensor ray on the occupancy of cells around the point of
reﬂection. Using (10), this implies the following log-odds
inverse sensor model for cells m∗
j along the k-th sensor ray:

gj(xt, zt) =

(cid:40)

ψkδzt,k
h0,j

if δzt,k ≤ (cid:15)
otherwise

,

(13)

This model suggests that one may also use a more expressive
multi-layer neural network in place of the linear transforma-
tion ψkδzt,k of the distance differential along the k-th ray:

(cid:40)

gj(xt, zt) =

NN(zt,k, d(xt, m∗
h0,j

j ); ψk)

if δzt,k ≤ (cid:15)
otherwise

(14)

In summary, the map encoder starts with prior occupancy

P(m∗

j = 1|x1:t, z1:t) = σ (ht,j) .

(9)

log-odds h0, updates them recurrently via:

The sigmoid function satisﬁes the following properties:

σ(x) =

1

1 + e−x , 1 − σ(x) = σ(−x),

log

σ(x)
σ(−x)

= x (10)

To complete the recurrent occupancy model design, we

ht+1 = ht + g(xt, zt; ψ) − h0,

(15)

where the log-odds inverse sensor model gj(xt, zt; ψk) is
speciﬁed for the j-th cell along the k-th ray in (14), and
provides the cell occupancy likelihood in (9) as output.

parameterize the log-odds observation model:
j = 1|z, x)
j = −1|z, x)
(cid:125)
(cid:123)(cid:122)
gj (x,z)

j = 1, x)
j = −1, x)

p(z|m∗
p(z|m∗

P(m∗
P(m∗

= log

log

(cid:124)

B. Cost Encoder

−h0,j (11)

Given a state-control pair (x, u), the cost encoder uses the
output σ (ht) of the map encoder to obtain a cost function
estimate ct(x, u). A complex interacting structure over the

feature representation σ (ht) can be obtained via a deep
neural network with parameters φ. Wulfmeier et al. [13]
proposed several CNN architectures, including pooling and
residual connections of fully convolutional networks,
to
model ct(x, u) from σ (ht). While complex architecture may
provide better performance in practice, we also develop a
simpler model for comparison using the inductive bias of
obstacle avoidance in robot navigation.

Let s and l be a small and large positive parameter,
respectively. The cost of applying control u in robot state
x can be modeled as large when the transition f (x, u)
encounters an obstacle and as small otherwise:

c(x, u) :=

(cid:40)

s
l

if m∗(x) = −1 and m∗(f (x, u)) = −1
if m∗(x) = 1 or m∗(f (x, u)) = 1

Since m∗ is unknown, we use the estimated occupancy
probabilities σ (ht) to compute the expectation of c(x, u)
over m∗:

ct(x, u) := E[c(x, u)] = s σ (ht[x]) σ (ht[f (x, u)])

+ l (1 − σ (ht[x]) σ (ht[f (x, u)])) ,

(16)

where ht[x] is the entry of the map encoder state that corre-
sponds to the environment cell containing x. This simple cost
encoder has parameters φ := [s, l]T and its output ct(x, u)
is differentiable with respect to φ and ψ through σ (ht).

IV. COST LEARNING VIA DIFFERENTIABLE PLANNING

We focus on optimizing the parameters θ of the cost
representation ct(x, u; z1:t, θ) developed in Sec. III. Since
the true cost c∗ is not directly observable, we need to
differentiate the loss function L(θ) in (3), which, in turn,
requires differentiating through the DSP problem in (2) with
respect to the cost function estimate ct.

Value Iteration Networks (VIN) [6] shows that T iterations
of the value iteration algorithm can be approximated by a
neural network with T convolutional and minpooling layers.
This allows VIN to be differentiable with respect to the stage
cost. While VIN can be modiﬁed to operate with a ﬁnite
horizon and produce a non-stationary policy, it would still
be based on full Bellman backups (convolutions and min-
pooling) over the entire state space. As a result, VIN scales
poorly with the state-space size, while it might not even be
necessary to determine the optimal cost-to-go Q∗
t (x, u) at
every state x ∈ X and control u ∈ U in the case of partially
observable environments.

Instead of using dynamic programming to solve the
DSP (2), any motion planning algorithm (e.g., A* [8],
RRT [9], [10], etc.) that returns the optimal cost-to-go
Q∗
t (x, u) over a subset of the state-control space provides
an accurate enough solution. For example, a backwards A*
search applied to problem (2) with start state xg, goal state
x ∈ X , and predecessors expansions according to the motion
model f provides an upper bound to the optimal cost-to-go:
Q∗
Q∗

t (x, u) = ct(x, u) + g(f (x, u))
t (x, u) ≤ ct(x, u) + g(f (x, u))

∀f (x, u) ∈ CLOSED,
∀f (x, u) (cid:54)∈ CLOSED,

a Boltzmann policy πt(u | x) can be deﬁned using the g-
values returned by A* for all x ∈ CLOSED∪OPEN ⊆ X and
a uniform distribution over U for all other states x. A* would
signiﬁcantly improves the efﬁciency of VIN [6] or other full
backup Dynamic Programming variants by performing local
Bellman backups on promising states (the CLOSED list).

In addition to improving the efﬁciency of the forward
computation of Q∗
t (x, u), using a planning algorithm to
solve (2) is also more efﬁcient in back-propagating errors
with respect to θ. In detail, using the subgradient method [4],
[24] to optimize L(θ) leads to a closed-form (sub)gradient
of Q∗
t (xt, ut) with respect to ct(x, u), removing the need
for back-propagation through multiple convolutional or min-
pooling layers. We proceed by rewriting Q∗
t (xt, ut) in a
form that makes its subgradient with respect to ct(x, u)
obvious. Let T (xt, ut) be the set of feasible state-control
trajectories τ := xt, ut, xt+1, ut+1, . . . , xT −1, uT −1 start-
ing at xt, ut and satisfying xk+1 = f (xk, uk) for k =
t, . . . , T − 1 with xg = xT . Let τ ∗ ∈ T (xt, ut) be an
optimal trajectory corresponding to the optimal cost-to-go
function Q∗
t (xt, ut) of a deterministic shortest path problem,
the controls in τ ∗ satisfy the additional constraint
i.e.,
uk = arg minu∈U Q∗
t (xk, u) for k = t + 1, . . . , T − 1.
Let µτ (x, u) := (cid:80)T −1
1(xk,uk)=(x,u) be a state-control
k=t
visitation function indicating if (x, u) is visited by τ . With
these deﬁnitions, we can view the optimal cost-to-go function
Q∗
t (xt, ut) as minimum over T (xt, ut) of the inner product
of the cost function ct and the visitation function µτ :

Q∗

t (xt, ut) = min

τ ∈T (xt,ut)

(cid:88)

x∈X ,u∈U

ct(x, u)µτ (x, u)

(17)

where X can be assumed ﬁnite because both T and U are
ﬁnite. This form allows us to (sub)differentiate Q∗
t (xt, ut)
with respect to ct(x, u) for any x ∈ X , u ∈ U.

Lemma 1. Let f (x, y) be differentiable and convex in x.
Then, ∇xf (x, y∗), where y∗ := arg miny f (x, y),
is a
subgradient of the piecewise-differentiable convex function
g(x) := miny f (x, y).

Applying Lemma 1 to (17) leads to the following subgra-

dient of the optimal cost-to-go function:

t (xt, ut)

∂Q∗
∂ct(x, u)

= µτ ∗ (x, u),

(18)

which can be obtained from the optimal trajectory τ ∗ corre-
sponding to Q∗
t (xt, ut). This result and the chain rule allow
us to obtain the complete (sub)gradient of L(θ).

the loss function L(θ)
Proposition 1. A subgradient of
in (3) with respect to the cost function parameters θ can
be obtained as follows:

dL(θ)
dθ

= −

N
(cid:88)

Tn(cid:88)

n=1

t=1

d log πt,n(u∗
dθ

t,n | xt,n)

(19)

N
(cid:88)

Tn(cid:88)

(cid:88)

∂ log πt,n(u∗

t,n | xt,n)

dQ∗

t,n(xt,n, ut,n)

n=1

t=1

ut,n∈U

∂Q∗

t,n(xt,n, ut,n)

dθ

where g are the values computed by A* for expanded nodes
in the CLOSED list and visited nodes in the OPEN list. Thus,

= −

t,n, zt,n, xg,n)(cid:9)Tn,N

L(θ) ← 0
for n = 1, . . . , N and t = 1, . . . , Tn do

Algorithm 1 Training: learn cost function parameters θ
1: Input: Demonstrations D = (cid:8)(xt,n, u∗
2: while θ not converged do
3:
4:
5:
6:
7:
8:

Update ct,n based on xt,n and zt,n as in Sec. III
Obtain Q∗
Obtain πt,n(u|xt,n) from Q∗
L(θ) ← L(θ) − log πt,n(u∗
Update θ ← θ − α∇L(θ) via Prop. 1

t,n(x, u) from DSP (2) with stage cost ct,n
t,n(xt,n, u) via Eq. (1)
t,n|xt,n)

t=1,n=1

9:
10: Output: θ

Algorithm 2 Testing: compute control policy for learned θ

1: Input: Start state xs, goal state xg, optimized θ
2: Current state xt ← xs
3: while xt (cid:54)= xg do
4:
5:
6:
7:
8:
9: Output: Navigation succeeds or fails.

Make an observation zt
Update ct based on xt and zt as in Sec. III
Obtain Q∗
Obtain πt(u|xt) from Q∗
Update xt ← f (xt, ut) via ut := arg maxu πt(u|xt)

t (xt, u), u ∈ U from DSP (2) with stage cost ct
t (xt, u) via Eq. (1)

where the ﬁrst term has a closed-form, while the second term
is available from (18) and the cost representation in Sec. III:

∂ log πt,n(u∗

t,n | xt,n)

t,n(xt,n, ut,n)

∂Q∗
t,n(xt,n, ut,n)

dQ∗

dθ

(cid:16)

=

1{ut,n=u∗

t,n} − πt,n(ut,n|xt,n)

(cid:17)

∂Q∗

(cid:88)

=
(x,u)∈τ ∗

t,n(xt,n, ut,n)
∂ct(x, u)

∂ct(x, u)
∂θ

(20)

The computation graph structure implied by Prop. 1 is il-
lustrated in Fig. 1. The graph consists of a cost representation
layer and a differentiable planning layer, allowing end-to-end
minimization of L(θ) via stochastic (sub)gradient descent.
Full algorithms for the training and testing phases (Fig. 1)
are shown in Alg. 1 and Alg. 2.

Although the form of the gradient in Prop. 1 is similar to
that in Ziebart et al. [5], our contributions are orthogonal. Our
contribution is to obtain a non-stationary cost-to-go function
and its (sub)gradient for a ﬁnite-horizon problem using
forward and backward computations that scale efﬁciently
with the size of the state space. On the other hand, the
results of Ziebart et al. [5] provide a stationary cost-to-go
function and its gradient for an inﬁnite horizon problem. The
maximum entropy formulation of the stationary policy is a
well grounded property of using a soft version of the Bell-
man update, which can explicitly model the suboptimality
of expert trajectories. However, we can show the beneﬁts
of our approach without including the maximum entropy
formulation and will leave it as future work.

V. EXPERIMENTS
We evaluate our approach in 2D grid world navigation
tasks at two scales. Obstacle conﬁgurations are generated
randomly in maps m∗
n of sizes 16 × 16 or 100 × 100. We use
an 8-connected grid so that a control ut causes a transition
xt+1 = f (xt, ut) from xt to one of the eight neighbor cells
xt+1. At each step, the robot receives a 360◦ lidar scan zt
at 5◦ resolution, resulting in 72 beams zt,k in each scan (see

16 × 16

100 × 100

Dataset

#maps

#samples

#maps

#samples

Train
Validation
Test

7638
966
952

514k
66k
-

970
122
122

460k
58k
-

TABLE II: Dataset size. We sample 10 trajectories in each
map in training and validation, and each sample takes the form
(x1:t,n, u∗
t,n, z1:t,n, xg,n). In testing, the robot’s task is to navigate
from one randomly sampled start to goal location on each map.

Fig. 3). The lidar range readings are corrupted by an additive
zero mean Gaussian noise. The standard deviation of the
noise is 0.05 and 0.2 (grid cell = 1) and the lidar maximum
range is 2.5 and 10 in 16 × 16 and 100 × 100 domains,
respectively. Note that the lidar range is smaller than the map
size to demonstrate environment partial observability. During
test time, the domain size is the maximum size allowed
for the observed area along a trajectory. Demonstrations are
obtained by running an A* planning algorithm to solve the
deterministic shortest path problem on the true map m∗
n. The
number of maps and training samples generated are shown
in Table II.

A. Baseline and model variations

We use DeepMaxEnt [13] as a baseline and compare it to
three variants of our model. In all variants, we parameterize
an inverse observation model and use the log-odds update
rule in Eqn. (13) as the map encoder. This map encoder is
sufﬁciently expressive to model occupancy probability from
lidar observations in a 2D enviroment. The A* algorithm is
used to solve the DSP (2), providing the optimal cost-to-go
Q∗
t (x, u) for x ∈ CLOSED ∪ OPEN and a subgradient of
Q∗
t (xt, ut) according to (18). All the neural networks are
implemented in the PyTorch library [25] and trained with
the Adam optimizer [26] until convergence.

DeepMaxEnt uses a neural network to learn a cost
function directly from lidar observations without explicitly
having a map representation. The neural network in our
experiments is the “Standard FCN” in [13] in the 16 × 16
domain, and the encoder-decoder architecture in [27] in the
100×100 domain. Value iteration is approximated by a ﬁnite
number of Bellman backup iterations, equal to the map size.
The experiments in the original DeepMaxEnt paper [13] use
the mean and variance of the height of the 3D lidar points
in each cell, as well as a binary indicator of cell visibility,
as input features to the neural network. Since our synthetic
experiments are set up in 2D, the count of lidar beams in each
cell is used as replacement of the height mean and variance.
This is a fair adaptation because Wulfmeier et. al. [13] argued
that obstacles generally represent areas of larger height
variance which means more beam counts in our observations.
Ours-HCE stands for hard-coded cost encoder. This sim-
ple variant of our model uses Eqn. (16) with s = 1 and
l = 100 set explicitly as constants.

Ours-SCE stands for soft-coded cost encoder and has s

and l in Eqn (16) as learnable parameters.

Ours-CNN is our most generic variant using a convolu-
tional neural network as cost encoder. The network architec-
ture is the same as in DeepMaxEnt for fair comparison.

16 × 16

100 × 100

Model

Val. loss

Val. acc.
(%)

Test traj.
succ. rate (%)

Test
traj. diff.

Val. loss

Val. acc.
(%)

Test traj.
succ. rate (%)

Test
traj. diff.

DeepMaxEnt [13]
Ours-HCE
Ours-SCE
Ours-CNN

0.18
1.10
0.66
0.27

93.6
59.5
62.1
90.5

90.9
99.7
97.2
96.7

0.145
0.378
0.174
0.144

0.23
1.32
0.66
0.14

93.7
42.2
62.1
95.1

31.1
100.0
84.4
90.1

6.528
2.876
1.569
1.196

Fig. 4: Validation and test results for the 16 × 16 and 100 × 100 grid world domains. Cross entropy loss (3) and prediction accuracy
for the validation set are reported. Test trajectories are iteratively rolled out from the non-stationary policy πt. A trial is classiﬁed as
successful if the goal is reached without collisions within twice the number of steps of a shortest path in the groundtruth enviroment.
Ours-CNN is capable of matching the expert demonstrations while generalizing to new robot navigation tasks in test time. Right: Plot
showing the effect of noise on the accuracy of Ours-CNN model.

Fig. 5: Examples of occupancy estimation, A* motion planning and subgradient computation during a successful test trajectory. The ﬁrst
ﬁgure shows the true occupancy map m∗ with the robot start and goal locations in green and magenta, respectively. The second and
third ﬁgures show the current lidar observation zt in red and robot trajectory thus far x1:t in green. The map occupancy estimate σ(ht)
in grayscale is in the background. The optimal cost-to-go estimate g from A* is shown in a blue-yellow colormap in the foreground
(brighter means higher cost-to-go). The optimal trajectories τ ∗ in cyan corresponding to Q∗
t (xt, ut) are obtained during A* planning for
subgradient computation in Eqn. (18). The last ﬁgure shows the ﬁnal successful trajectory in green and an optimal shortest path in the
fully observable enviroment in blue.

B. Experiments and Results

a) Model generalization: Fig 4 shows the comparison
of multiple measures of accuracy for different algorithms.
Both Ours-HCE and Ours-SCE explicitly incorporate cell
traversability through the cost design in (16). Test results
show that this explicit cost encoder is successful at obstacle
avoidance, regardless of whether the parameters s, l are
constants in Ours-HCE or learnable in Ours-SCE. The
performance of Ours-HCE also shows that the map encoder
is learning a correct map representation from the noisy lidar
observations since the only trainable parameters are ψ in the
inverse observation model (13). However, both models fail
at matching demonstrations in validation because the cost
encoder (16) emphasizes obstacle avoidance explicitly, leav-
ing little capacity to learn from demonstrations. Ours-CNN
combines the strength of learning from demonstrations and
generalization to new navigation tasks while avoiding obsta-
cles. Its validation results are on par with DeepMaxEnt,
showing the validity of the closed-form subgradient in (18).
Ours-CNN signiﬁcantly outperforms DeepMaxEnt in new
tasks at test time. The performance gap of DeepMaxEnt
in the two domains shows that a general CNN architecture
applied directly to the lidar scan measurements is not as
effective as the map encoder in Ours-CNN at modeling
occupancy probability. Fig 5 shows the map occupancy
estimation, as well as the optimal trajectories necessary for
subgradient computation in Sec IV.

b) Robustness to noise: The robustness of Ours-CNN
to the observation noise is evaluated in the 16 × 16 domain.

Fig 4 shows that the performance degrades as the noise
increases but our inverse observation model (13) still gener-
alizes well considering that noise with standard deviation of
0.5 is signiﬁcant when the lidar range is only 2.5.

c) Computational efﬁciency: Finally, we compare the
efﬁciency of a forward pass through our A* planner and the
value iteration algorithm in DeepMaxEnt. The A* algo-
rithm in our models is implemented in C++ and evaluated on
a CPU. The VI algorithm is implemented using convolutional
and minpooling layers in Pytorch as described in [6] and is
evaluated on a GPU. We record the time that each models
takes to return a policy πt given a cost function ct. Our
A* algorithm takes only 0.02 ms as compared to VI’s 0.2
ms on the 16 × 16 map. In the 100 × 100 domain, our A*
algorithm takes 0.6 ms, compared to VI’s 14 ms, illustrating
the scalability of our model in the size of the state space.

VI. CONCLUSION
We proposed an inverse reinforcement learning approach
for infering navigation costs from demonstration in partially
observable enviroments. Our model introduces a new cost
representation composed of a probabilistic occupancy en-
coder and a cost encoder deﬁned over the occupancy features.
We showed that a motion planning algorithm can compute
optimal cost-to-go values over the cost representation, while
the cost-to-go (sub)gradient may be obtained in closed-form.
Our work offers a promising model for encoding occupancy
features in navigation tasks and may enable efﬁcient online
learning in challenging operational conditions.

00.10.20.30.40.580859095100GaussiannoiseSTDPercentage(%)Testtraj.succ.rateValidacc.020406080100120140160010203040506070[23] A. Hornung, K. M. Wurm, M. Bennewitz, C. Stachniss, and W. Bur-
gard, “OctoMap: An efﬁcient probabilistic 3D mapping framework
based on octrees,” Autonomous Robots, vol. 34, no. 3, pp. 189–206,
2013.

[24] N. Z. Shor, Minimization methods for non-differentiable functions.

Springer Science & Business Media, 2012, vol. 3.

[25] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito,
Z. Lin, A. Desmaison, L. Antiga, and A. Lerer, “Automatic differen-
tiation in pytorch,” in NIPS-W, 2017.

[26] D. P. Kingma and J. Ba, “ADAM: A method for stochastic optimiza-

tion,” in ICLR, 2014.

[27] V. Badrinarayanan, A. Kendall, and R. Cipolla, “SegNet: A Deep Con-
volutional Encoder-Decoder Architecture for Image Segmentation,”
PAMI, 2017.

REFERENCES

[1] A. Y. Ng and S. Russell, “Algorithms for inverse reinforcement
learning,” in Proceedings of the Seventeenth International Conference
on Machine Learning, 2000, pp. 663–670.

[2] B. D. Argall, S. Chernova, M. Veloso, and B. Browning, “A survey
learning from demonstration,” Robotics and Autonomous

of robot
Systems, vol. 57, no. 5, pp. 469 – 483, 2009.

[3] G. Neu and C. Szepesv´ari, “Apprenticeship learning using inverse
reinforcement learning and gradient methods,” in Proceedings of the
Twenty-Third Conference on Uncertainty in Artiﬁcial Intelligence.
AUAI Press, 2007, pp. 295–302.

[4] N. D. Ratliff, J. A. Bagnell, and M. A. Zinkevich, “Maximum margin
planning,” in Proceedings of the 23rd international conference on
Machine learning. ACM, 2006, pp. 729–736.

[5] B. D. Ziebart, A. Maas, J. Bagnell, and A. K. Dey, “Maximum entropy
inverse reinforcement learning,” in Proceedings of the Twenty-Third
AAAI Conference on Artiﬁcial Intelligence, 2008, pp. 1433–1438.
[6] A. Tamar, Y. WU, G. Thomas, S. Levine, and P. Abbeel,
“Value iteration networks,” in Advances in Neural
Information
Processing Systems 29, D. D. Lee, M. Sugiyama, U. V.
I. Guyon, and R. Garnett, Eds. Curran Associates,
Luxburg,
Inc., 2016, pp. 2154–2162. [Online]. Available: http://papers.nips.cc/
paper/6046-value-iteration-networks.pdf

[7] M. Okada, L. Rigazio, and T. Aoshima, “Path integral net-
works: End-to-end differentiable optimal control,” arXiv preprint
arXiv:1706.09597, 2017.

[8] M. Likhachev, G. Gordon, and S. Thrun, “ARA*: Anytime A*
with Provable Bounds on Sub-Optimality,” in in Advances in Neural
Information Processing Systems, 2004, p. 767774.

[9] S. M. LaValle, “Rapidly-exploring random trees: A new tool for path

planning,” 1998.

[10] S. Karaman and E. Frazzoli, “Sampling-based algorithms for optimal
journal of robotics research,

motion planning,” The international
vol. 30, no. 7, pp. 846–894, 2011.

[11] J. Choi and K.-E. Kim, “Inverse reinforcement learning in partially
observable environments,” Journal of Machine Learning Research,
vol. 12, no. Mar, pp. 691–730, 2011.

[12] T. Shankar, S. K. Dwivedy, and P. Guha, “Reinforcement learning via
recurrent convolutional neural networks,” in 2016 23rd International
Conference on Pattern Recognition (ICPR).
IEEE, 2016, pp. 2592–
2597.

[13] M. Wulfmeier, D. Z. Wang, and I. Posner, “Watch this: Scalable cost-
function learning for path planning in urban environments,” in 2016
IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS).

IEEE, 2016, pp. 2089–2095.

[14] S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Malik,
“Cognitive mapping and planning for visual navigation,” in The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July
2017.

[15] P. Karkus, D. Hsu,

and W. S. Lee,

“QMDP-Net: Deep
learning for planning under partial observability,” in Advances
in Neural Information Processing Systems 30, I. Guyon, U. V.
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and R. Garnett,
2017,
[Online]. Available: http://papers.nips.cc/paper/
pp. 4694–4704.
7055-qmdp-net-deep-learning-for-planning-under-partial-observability.
pdf

Curran Associates,

Inc.,

Eds.

[16] P. Karkus, X. Ma, D. Hsu, L. Kaelbling, W. S. Lee, and T. Lozano-
Perez, “Differentiable algorithm networks for composable robot learn-
ing,” in Proceedings of Robotics: Science and Systems, Freiburgim-
Breisgau, Germany, June 2019.

[17] K. strm, “Optimal control of markov processes with incomplete state
information,” Journal of Mathematical Analysis and Applications,
vol. 10, no. 1, pp. 174 – 205, 1965.
[Online]. Available:
http://www.sciencedirect.com/science/article/pii/0022247X6590154X
[18] W. S. L. Hanna Kurniawati, David Hsu, “SARSOP: Efﬁcient point-
based POMDP planning by approximating optimally reachable belief
spaces,” in Proceedings of Robotics: Science and Systems IV, Zurich,
Switzerland, June 2008.

[19] A. Khan, C. Zhang, N. Atanasov, K. Karydis, V. Kumar, and D. D. Lee,
“Memory augmented control networks,” in International Conference
on Learning Representations (ICLR), 2018.

[20] D. Ramachandran and E. Amir, “Bayesian inverse reinforcement

learning.” in IJCAI, vol. 7, 2007, pp. 2586–2591.

[21] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT

Press, 2016, http://www.deeplearningbook.org.

[22] S. Thrun, W. Burgard, and D. Fox, Probabilistic Robotics. The MIT

Press, 2005.

