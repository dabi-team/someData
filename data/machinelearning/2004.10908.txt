1
2
0
2

p
e
S
6

]

C
D
.
s
c
[

4
v
8
0
9
0
1
.
4
0
0
2
:
v
i
X
r
a

TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS

1

Taskﬂow: A Lightweight Parallel and
Heterogeneous Task Graph Computing System

Tsung-Wei Huang, Dian-Lun Lin, Chun-Xun Lin, and Yibo Lin

Abstract—Taskﬂow aims to streamline the building of parallel and heterogeneous applications using a lightweight task graph-based
approach. Taskﬂow introduces an expressive task graph programming model to assist developers in the implementation of parallel and
heterogeneous decomposition strategies on a heterogeneous computing platform. Our programming model distinguishes itself as a
very general class of task graph parallelism with in-graph control ﬂow to enable end-to-end parallel optimization. To support our model
with high performance, we design an efﬁcient system runtime that solves many of the new scheduling challenges arising out of our
models and optimizes the performance across latency, energy efﬁciency, and throughput. We have demonstrated the promising
performance of Taskﬂow in real-world applications. As an example, Taskﬂow solves a large-scale machine learning workload up to 29%
faster, 1.5× less memory, and 1.9× higher throughput than the industrial system, oneTBB, on a machine of 40 CPUs and 4 GPUs. We
have opened the source of Taskﬂow and deployed it to large numbers of users in the open-source community.

Index Terms—Parallel programming, task parallelism, high-performance computing, modern C++ programming

✦

1 INTRODUCTION

T ASK graph computing system (TGCS) plays an essential

role in advanced scientiﬁc computing. Unlike loop-
based models, TGCSs encapsulate function calls and their
dependencies in a top-down task graph to implement ir-
regular parallel decomposition strategies that scale to large
numbers of processors, including manycore central process-
ing units (CPUs) and graphics processing units (GPUs). As
a result, recent years have seen a great deal amount of
TGCS research, just name a few, oneTBB FlowGraph [2],
StarPU [17], TPL [39], Legion [18], Kokkos-DAG [24], PaR-
SEC [20], HPX [33], and Fastﬂow [15]. These systems have
enabled vast success in a variety of scientiﬁc computing
applications, such as machine learning, data analytics, and
simulation.

However, three key limitations prevent existing TGCSs
from exploring the full potential of task graph parallelism.
First, existing TGCSs closely rely on directed acyclic graph
(DAG) models to deﬁne tasks and dependencies. Users
implement control-ﬂow decisions outside the graph descrip-
tion, which typically results in rather complicated imple-
mentations that lack end-to-end parallelism. For instance,
when encountering an if-else block, users need to syn-
chronize the graph execution with a TGCS runtime, which
could otherwise be omitted if in-graph control-ﬂow tasks
are supported. Second, existing TGCSs do not align well
with modern hardware. In particular, new GPU task graph
parallelism, such as CUDA Graph, can bring signiﬁcant
yet largely untapped performance beneﬁts. Third, existing
TGCSs are good at either CPU- or GPU-focused workloads,

• Tsung-Wei Huang, and Dian-Lun Lin are with the Department of
Electrical and Computer Engineering, the University of Utah, Salt Lake
City, UT.

• Chun-Xun Lin is with MathWorks, USA.
• Yibo Lin is with the Department of Computer Science, Peking University,

Beijing, China.

but rarely both simultaneously. Consequently, we introduce
in this paper Taskﬂow, a lightweight TGCS to overcome
these limitations. We summarize three main contributions
of Taskﬂow as follows:

• Expressive programming model – We design an ex-
pressive task graph programming model by leveraging
modern C++ closure. Our model enables efﬁcient imple-
mentations of parallel and heterogeneous decomposition
strategies using the task graph model. The expressiveness
of our model lets developers perform rather a lot of work
with relative ease of programming. Our user experiences
lead us to believe that, although it requires some effort
to learn, a programmer can master our APIs needed for
many applications in just a few hours.

• In-graph control ﬂow – We design a new conditional
tasking model to support in-graph control ﬂow beyond
the capability of traditional DAG models that prevail in
existing TGCSs. Our condition tasks enable developers
to integrate control-ﬂow decisions, such as conditional
dependencies, cyclic execution, and non-deterministic
ﬂows into a task graph of end-to-end parallelism. In
case applications have frequent dynamic behavior, such
as optimization and branch and bound, programmers
can efﬁciently overlap tasks both inside and outside the
control ﬂow to hide expensive control-ﬂow costs.

• Heterogeneous work stealing – We design an efﬁcient
work-stealing algorithm to adapt the number of workers
to dynamically generated task parallelism at any time
during the graph execution. Our algorithm prevents the
graph execution from underutilized threads that is harm-
ful to performance, while avoiding excessive waste of
thread resources when available tasks are scarce. The
result largely improves the overall system performance,
including latency, energy usage, and throughput. We have
derived theory results to justify the efﬁciency of our work-
stealing algorithm.

 
 
 
 
 
 
TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS

2

We have evaluated Taskﬂow on real-world applications
to demonstrate its promising performance. As an example,
Taskﬂow solved a large-scale machine learning problem up
to 29% faster, 1.5× less memory, and 1.9× higher through-
put than the industrial system, oneTBB [2], on a machine
of 40 CPUs and 4 GPUs. We believe Taskﬂow stands out as
a unique system given the ensemble of software tradeoffs
and architecture decisions we have made. Taskﬂow is open-
source at GitHub under MIT license and is being used by
many academic and industrial projects [10].

2 MOTIVATIONS

Taskﬂow is motivated by our DARPA project to reduce
the long design times of modern circuits [1]. The main
research objective is to advance computer-aided design (CAD)
tools with heterogeneous parallelism to achieve transforma-
tional performance and productivity milestones. Unlike tra-
ditional loop-parallel scientiﬁc computing problems, many
CAD algorithms exhibit irregular computational patterns and
complex control ﬂow that require strategic task graph decom-
positions to beneﬁt from heterogeneous parallelism [28].
This type of complex parallel algorithm is difﬁcult to imple-
ment and execute efﬁciently using mainstream TGCS. We
highlight three reasons below, end-to-end tasking, GPU task
graph parallelism, and heterogeneous runtimes.

End-to-End Tasking – Optimization engines implement
various graph and combinatorial algorithms that frequently
call for iterations, conditionals, and dynamic control ﬂow.
Existing TGCSs [2], [7], [12], [17], [18], [20], [24], [33], [39],
closely rely on DAG models to deﬁne tasks and their depen-
dencies. Users implement control-ﬂow decisions outside the
graph description via either statically unrolling the graph
across ﬁxed-length iterations or dynamically executing an
“if statement” on the ﬂy to decide the next path and so
forth. These solutions often incur rather complicated imple-
mentations that lack end-to-end parallelism using just one
task graph entity. For instance, when describing an iterative
algorithm using a DAG model, we need to repetitively wait
for the task graph to complete at the end of each iteration.
This wait operation is not cheap because it involves syn-
chronization between the application code and the TGCS
runtime, which could otherwise be totally avoided by
supporting in-graph control-ﬂow tasks. More importantly,
developers can beneﬁt by making in-graph control-ﬂow
decisions to efﬁciently overlap tasks both inside and outside
control ﬂow, completely decided by a dynamic scheduler.

GPU Task Graph Parallelism – Emerging GPU task
graph acceleration, such as CUDA Graph [4], can offer
dramatic yet largely untapped performance advantages by
running a GPU task graph directly on a GPU. This type
of GPU task graph parallelism is particularly beneﬁcial for
many large-scale analysis and machine learning algorithms
that compose thousands of dependent GPU operations to
run on the same task graph using iterative methods. By
creating an executable image for a GPU task graph, we can
iteratively launch it with extremely low kernel overheads.
However, existing TGCSs are short of a generic model to ex-
press and ofﬂoad task graph parallelism directly on a GPU,
as opposed to a simple encapsulation of GPU operations
into CPU tasks.

Heterogeneous Runtimes – Many CAD algorithms
compute extremely large circuit graphs. Different quantities
are often dependent on each other, via either logical relation
or physical net order, and are expensive to compute. The
resulting task graph in terms of encapsulated function calls
and task dependencies is usually very large. For example,
the task graph representing a timing analysis on a million-
gate design can add up to billions of tasks that take several
hours to ﬁnish [32]. During the execution, tasks can run
on CPUs or GPUs, or more frequently a mix. Scheduling
these heterogeneously dependent tasks is a big challenge.
Existing runtimes are good at either CPU- or GPU-focused
work but rarely both simultaneously.

Therefore, we argue that there is a critical need for a new
heterogeneous task graph programming environment that
supports in-graph control ﬂow. The environment must han-
dle new scheduling challenges, such as conditional depen-
dencies and cyclic executions. To this end, Taskﬂow aims
to (1) introduce a new programming model that enables
end-to-end expressions of CPU-GPU dependent tasks along
with algorithmic control ﬂow and (2) establish an efﬁcient
system runtime to support our model with high perfor-
mance across latency, energy efﬁciency, and throughput.
Taskﬂow focuses on a single heterogeneous node of CPUs
and GPUs.

3 PRELIMINARY RESULTS
Taskﬂow is established atop our prior system, Cpp-
Taskﬂow [32] which targets CPU-only parallelism using
a DAG model, and extends its capability to heteroge-
neous computing using a new heterogeneous task dependency
graph (HTDG) programming model beyond DAG. Since we
opened the source of Cpp-Taskﬂow/Taskﬂow, it has been
successfully adopted by much software, including impor-
tant CAD projects [14], [30], [43], [54] under the DARPA ERI
IDEA/POSH program [1]. Because of the success, we are
recently invited to publish a 5-page TCAD brief to overview
how Taskﬂow address the parallelization challenges of CAD
workloads [31]. For the rest of the paper, we will provide
comprehensive details of the Taskﬂow system from the top-
level programming model to the system runtime, including
several new technical materials for control-ﬂow primitives,
capturer-based GPU task graph parallelism, work-stealing
algorithms and theory results, and experiments.

4 TASKFLOW PROGRAMMING MODEL
This section discusses ﬁve fundamental task types of Task-
ﬂow, static task, dynamic task, module task, condition task, and
cudaFlow task.

4.1 Static Tasking

Static tasking is the most basic task type in Taskﬂow. A
static task takes a callable of no arguments and runs it.
The callable can be a generic C++ lambda function object,
binding expression, or a functor. Listing 1 demonstrates a
simple Taskﬂow program of four static tasks, where A runs
before B and C, and D runs after B and C. The graph is run by
an executor which schedules dependent tasks across worker
threads. Overall, the code explains itself.

TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS

3

t f : : Taskflow t as k f low ;
t f : : E xe c u t or e xe c u t or ;
auto [A, B , C , D] = t as k f low . emplace (

[ ]
[ ]
[ ]
[ ]

( ) { s t d : : cout << ” Task A” ; } ,
( ) { s t d : : cout << ” Task B” ; } ,
( ) { s t d : : cout << ” Task C” ; } ,
( ) { s t d : : cout << ” Task D” ; }

) ;
A. precede ( B , C ) ;
D. succeed ( B , C ) ;
e xe c u t or . run ( t f ) . wait ( ) ;

// A runs b e f o r e B and C
B and C
// D runs a f t e r

Listing 1: A task graph of four static tasks.

4.2 Dynamic Tasking

Dynamic tasking refers to the creation of a task graph
during the execution of a task. Dynamic tasks are spawned
from a parent task and are grouped to form a hierarchy
called subﬂow. Figure 1 shows an example of dynamic
tasking. The graph has four static tasks, A, C, D, and B. The
precedence constraints force A to run before B and C, and
D to run after B and C. During the execution of task B, it
spawns another graph of three tasks, B1, B2, and B3, where
B1 and B2 run before B3. In this example, B1, B2, and B3
are grouped to a subﬂow parented at B.

(-

!"#$%&’ ()*(+,)(-,)(./

(+

2

(.

1

0

Fig. 1: A task graph that spawns another task graph (B1, B2,
and B3) during the execution of task B.

auto [A, C , D] = t as k f low . emplace (
( ) { s t d : : cout << ”A” ; } ,
( ) { s t d : : cout << ”C” ; } ,
( ) { s t d : : cout << ”D” ; }

[ ]
[ ]
[ ]

) ;
auto B = t f . emplace ( [ ]
s t d : : cout << ”B\n” ;
auto [ B1 , B2 , B3 ] = subflow . emplace (
( ) { s t d : : cout << ”B1” ; } ,
( ) { s t d : : cout << ”B2” ; } ,
( ) { s t d : : cout << ”B3” ; }

[ ]
[ ]
[ ]

( t f : : Subflow& subflow ) {

) ;
B3 . succeed ( B1 , B2 ) ;

} ) ;
A. precede ( B , C ) ;
D. succeed ( B , C ) ;

Listing 2: Taskﬂow code of Figure 1.

Listing 2 shows the Taskﬂow code in Figure 1. A dy-
namic task accepts a reference of type tf::Subflow that
is created by the executor during the execution of task
B. A subﬂow inherits all graph building blocks of static
tasking. By default, a spawned subﬂow joins its parent task
(B3 precedes its parent B implicitly), forcing a subﬂow to
follow the subsequent dependency constraints of its parent
task. Depending on applications, users can detach a subﬂow
from its parent task using the method detach, allowing its
execution to ﬂow independently. A detached subﬂow will
eventually join its parent taskﬂow.

4.3 Composable Tasking

Composable tasking enables developers to deﬁne task hier-
archies and compose large task graphs from modular and
reusable blocks that are easier to optimize. Figure 2 gives
an example of a Taskﬂow graph using composition. The
top-level taskﬂow deﬁnes one static task C that runs before
a dynamic task D that spawns two dependent tasks D1 and
D2. Task D precedes a module task E that composes a taskﬂow
of two dependent tasks A and B.

#

<*=4+(5>-$

$%

$&

./014+(5 %-60,7(8)-+,9,+:

!

"

’()*+,-./01-2-
3./014+(5-%-60,7(8)-+,9,+:;

./014+(5 &-6?(@-+,9,+:

Fig. 2: An example of taskﬂow composition.

// f i l e 1 d e f i n e s
t f : : Taskflow t as k f low 1 ;
auto [A, B ] = t as k f low 1 . emplace (

t as k f low 1

[ ]
[ ]

( ) { s t d : : cout << ”TaskA ” ; } ,
( ) { s t d : : cout << ”TaskB ” ; }

) ;
A. precede ( B ) ;
// f i l e 2 d e f i n e s
t f : : Taskflow t as k f low 2 ;
auto [ C , D] = t as k f low 2 . emplace (

t as k f low 2

[ ]
[ ]

( ) { s t d : : cout << ”TaskC ” ; } ,
( t f : : Subflow& s f ) {
s t d : : cout << ”TaskD ” ;
auto [ D1 , D2 ] = s f . emplace (

[ ]
[ ]

( ) { s t d : : cout << ”D1” ; } ,
( ) { s t d : : cout << ”D2” ; }

) ;
D1 . precede ( D2 ) ;

}

) ;
auto E = t as k f low 2 . composed of ( t as k f low 1 ) ; // module
D. precede ( E ) ;
C . precede (D ) ;

Listing 3: Taskﬂow code of Figure 2.

Listing 3 shows the Taskﬂow code of Figure 2. It declares
two taskﬂows, taskflow1 and taskflow2. taskflow2
forms a module task E by calling the method composed_of
from taskflow1, which is then preceded by task D. Unlike
a subﬂow task, a module task does not own the taskﬂow but
maintains a soft mapping to its composed taskﬂow. Users
can create multiple module tasks from the same taskﬂow
but they must not run concurrently; on the contrary, sub-
ﬂows are created dynamically and can run concurrently.
In practice, we use composable tasking to partition large
parallel programs into smaller or reusable taskﬂows in
separate ﬁles (e.g., taskflow1 in ﬁle 1 and taskflow2
in ﬁle 2) to improve program modularity and testability.
Subﬂows are instead used for enclosing a task graph that
needs stateful data referencing via lambda capture.

4.4 Conditional Tasking

We introduce a new conditional tasking model to overcome
the limitation of existing frameworks in expressing general
control ﬂow beyond DAG. A condition task is a callable that

TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS

4

returns an integer index indicating the next successor task to
execute. The index is deﬁned with respect to the order of the
successors preceded by the condition task. Figure 3 shows
an example of if-else control ﬂow, and Listing 4 gives its
implementation. The code is self-explanatory. The condition
task, cond, precedes two tasks, yes and no. With this order,
if cond returns 0, the execution moves on to yes, or no if
cond returns 1.

consists of two regular tasks, init and stop, and three
condition tasks, F1, F2, and F3. Each condition task forms a
dynamic control ﬂow to randomly go to either the next task
or loop back to F1 with a probability of 1/2. Starting from
init, the expected number of condition tasks to execute
before reaching stop is eight. Listing 6 implements Figure
5 in just 11 lines of code.

init

cond

0

1

yes

no

Fig. 3: A Taskﬂow graph of if-else control ﬂow using one
condition task (in diamond).

auto [ i n i t , cond , yes , no ] = t as k f low . emplace (

[ ]
[ ]
[ ]
[ ]

( ) { s t d : : cout << ” i n i t ” ; } ,
( ) { s t d : : cout << ”cond” ;
r e t u r n 0 ; } ,
( ) { s t d : : cout << ”cond r e t u r n s 0” ; } ,
( ) { s t d : : cout << ”cond r e t u r n s 1” ; }

) ;
cond . succeed ( i n i t )

. precede ( yes , no ) ;

Listing 4: Taskﬂow program of Figure 3.

Our condition task supports iterative control ﬂow by
introducing a cycle in the graph. Figure 4 shows a task
graph of do-while iterative control ﬂow, implemented in
Listing 5. The loop continuation condition is implemented
by a single condition task, cond, that precedes two tasks,
body and done. When cond returns 0, the execution loops
back to body. When cond returns 1, the execution moves
onto done and stops. In this example, we use only four
tasks even though the control ﬂow spans 100 iterations.
Our model is more efﬁcient and expressive than existing
frameworks that count on dynamic tasking or recursive
parallelism to execute condition on the ﬂy [18], [20].

init

body

0

cond

1

done

Fig. 4: A Taskﬂow graph of iterative control ﬂow using one
condition task.

i ;

i n t
auto [ i n i t , body , cond , done ] = t as k f low . emplace (

[ & ] ( ) { i =0; } ,
[ & ] ( ) { i ++; } ,
[ & ] ( ) { r e t u r n i <100 ? 0 : 1 ; } ,
[ & ] ( ) { s t d : : cout << ”done ” ; }

) ;
i n i t . precede ( body ) ;
body . precede ( cond ) ;
cond . precede ( body , done ) ;

Listing 5: Taskﬂow program of Figure 4.

Furthermore, our condition task can model non-
deterministic control ﬂow where many existing models do
not support. Figure 5 shows an example of nested non-
deterministic control ﬂow frequently used in stochastic op-
timization (e.g., VLSI ﬂoorplan annealing [53]). The graph

1

F1

0

1

init

F2

1

0

F3

0

stop

Fig. 5: A Taskﬂow graph of non-deterministic control ﬂow
using three condition tasks.

auto [ i n i t , F1 , F2 , F3 ,

s t op ] = t as k f low . emplace (

[ ]
[ ]
[ ]
[ ]
[ ]

( ) { s t d : : cout << ” i n i t ” ; } ,
( ) { r e t u r n rand ()%2 } ,
( ) { r e t u r n rand ()%2 } ,
( ) { r e t u r n rand ()%2 } ,
( ) { s t d : : cout << ” s t op ” ; }

) ;
i n i t . precede ( F1 ) ;
F1 . precede ( F2 , F1 ) ;
F2 . precede ( F3 , F1 ) ;
F3 . precede ( stop , F1 ) ;

Listing 6: Taskﬂow program of Figure 5.

The advantage of our conditional tasking is threefold.
First, it is simple and expressive. Developers beneﬁt from
the ability to make in-graph control-ﬂow decisions that are
integrated within task dependencies. This type of decision
making is different from dataﬂow [33] as we do not abstract
data but tasks, and is more general than the primitive-based
method [56] that is limited to domain applications. Second,
condition tasks can be associated with other tasks to inte-
grate control ﬂow into a uniﬁed graph entity. Users ought
not to partition the control ﬂow or unroll it to a ﬂat DAG,
but focus on expressing dependent tasks and control ﬂow.
The later section will explain our scheduling algorithms
for condition tasks. Third, our model enables developers
to efﬁciently overlap tasks both inside and outside control
ﬂow. For example, Figure 6 implements a task graph of
three control-ﬂow blocks, and cond_1 can run in parallel
with cond_2 and cond_3. This example requires only 30
lines of code.

A

B

F

C

0

cond_1

1

cond_2

0

1

G

H

E

0

cond_3

1

L

Fig. 6: A Taskﬂow graph of parallel control-ﬂow blocks using
three condition tasks.

4.5 Heterogeneous Tasking

We introduce a new heterogeneous task graph program-
ming model by leveraging C++ closure and emerging GPU

TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS

5

task graph acceleration, CUDA Graph [4]. Figure 7 and
Listing 7 show the canonical CPU-GPU saxpy (A•X plus
Y) workload and its implementation using our model. Our
model lets users describe a GPU workload in a task graph
called cudaFlow rather than aggregated GPU operations
using explicit CUDA streams and events. A cudaFlow lives
inside a closure and deﬁnes methods for constructing a
GPU task graph. In this example, we deﬁne two par-
allel CPU tasks (allocate_x, allocate_y) to allocate
uniﬁed shared memory (cudaMallocManaged) and one
cudaFlow task to spawn a GPU task graph consisting of
two host-to-device (H2D) transfer tasks (h2d_x, h2d_y),
one saxpy kernel task (kernel), and two device-to-host
(D2H) transfer tasks (d2h_x, d2h_y), in this order of task
dependencies. Task dependencies are established through
precede or succeed. Apparently, cudaFlow must run
after allocate_x and allocate_y. We emplace this cud-
aFlow on GPU 1 (emplace_on). When deﬁning cudaFlows
on speciﬁc GPUs, users are responsible for ensuring all
involved memory operations stay in valid GPU contexts.

+%%&.+/01,

+%%&.+/01-

2341,

2341-

!"#$%&’()*+,"-

saxpy kernel

4321,

4321-

Fig. 7: A saxpy (“single-precision A·X plus Y”) task graph using
two CPU tasks and one cudaFlow task.

g l o b a l

void saxpy ( i n t n , i n t a , f l o a t

* x , f l o a t

* y ) ;

c o n s t unsigned N = 1<<20;
s t d : : v e c t or <f l o a t > hx (N, 1 . 0 f ) , hy (N, 2 . 0 f ) ;
f l o a t

* dx{ n u l l p t r } ,

* dy{ n u l l p t r } ;

auto [ a l l o c a t e x , a l l o c a t e y ] = t as k f low . emplace (
[ & ] ( ) { cudaMallocManaged (&dx , N* s i z e o f ( f l o a t ) ) ; }
[ & ] ( ) { cudaMallocManaged (&dy , N* s i z e o f ( f l o a t ) ) ; }

) ;
auto cudaFlow = t as k f low . emplace on (

[ & ] ( t f : : cudaFlow& c f ) {

auto h2d x = c f . copy ( dx , hx . data ( ) , N) ;
auto h2d y = c f . copy ( dy , hy . data ( ) , N) ;
auto d2h x = c f . copy ( hx . data ( ) , dx , N) ;
auto d2h y = c f . copy ( hy . data ( ) , dy , N) ;
auto k e r n e l = c f . k e r n e l (

GRID , BLOCK, SHM, saxpy , N, 2 . 0 f , dx , dy

) ;
k e r n e l . succeed ( h2d x , h2d y )

. precede ( d2h x , d2h y ) ;

} , 1

) ;
cudaFlow . succeed ( a l l o c a t e x , a l l o c a t e y ) ;

Listing 7: Taskﬂow program of Figure 7.

Our cudaFlow has the three key motivations. First,
users focus on the graph-level expression of dependent
GPU operations without wrangling with low-level streams.
They can easily visualize the graph by Taskﬂow to reduce
turnaround time. Second, our closure forces users to express
their intention on what data storage mechanism should be
used for each captured variable. For example, Listing 7
captures all data (e.g., hx, dx) in reference to form a stateful
closure. When allocate_x and allocate_y ﬁnish, the
cudaFlow closure can access the correct state of dx and dy.
This property is very important for heterogeneous graph

parallelism because CPU and GPU tasks need to share
states of data to collaborate with each other. Our model
makes it easy and efﬁcient to capture data regardless of
its scope. Third, by abstracting GPU operations to a task
graph closure, we judiciously hide implementation details
for portable optimization. By default, a cudaFlow maps to a
CUDA graph that can be executed using a single CPU call.
On a platform that does not support CUDA Graph, we fall
back to a stream-based execution.

Taskﬂow does not dynamically choose whether to ex-
ecute tasks on CPU or GPU, and does not manage GPU
data with another abstraction. This is a software decision
we have made when designing cudaFlow based on our
experience in parallelizing CAD using existing TGCSs.
While it is always interesting to see what abstraction is
best suited for which application, in our ﬁeld, developing
high-performance CAD algorithms requires many custom
efforts on optimizing the memory and data layouts [28],
[26]. Developers tend to do this statically in their own
hands, such as direct control over raw pointers and ex-
plicit memory placement on a GPU, while leaving tedious
details of runtime load balancing to a dynamic scheduler.
After years of research, we have concluded to not abstract
memory or data because they are application-dependent.
This decision allows Taskﬂow to be framework-neutral while
enabling application code to take full advantage of native
or low-level GPU programming toolkits.

t as k f low . emplace on ( [ & ] ( t f : : cudaFlowCapturer& c f c ) {

auto h2d x = c f c . copy ( dx , hx . data ( ) , N) ;
auto h2d y = c f c . copy ( dy , hy . data ( ) , N) ;
auto d2h x = c f c . copy ( hx . data ( ) , dx , N) ;
auto d2h y = c f c . copy ( hy . data ( ) , dy , N) ;
auto k e r n e l = c f c . on ( [ & ] ( cudaStream t s ) {

in v ok e 3r d p ar t y s axp y k e r n e l ( s ) ;

} ) ;
k e r n e l . succeed ( h2d x , h2d y )

. precede ( d2h x , d2h y ) ;

} , 1 ) ;

Listing 8: Taskﬂow program of Figure 7 using a capturer.

Constructing a GPU task graph using cudaFlow requires
all kernel parameters are known in advance. However,
third-party applications, such as cuDNN and cuBLAS, do
not open these details but provide an API for users to
invoke hidden kernels through custom streams. The bur-
den is on users to decide a stream layout and witness its
concurrency across dependent GPU tasks. To deal with this
problem, we design a cudaFlow capturer to capture GPU
tasks from existing stream-based APIs. Listing 8 outlines an
implementation of the same saxpy task graph in Figure 7
using a cudaFlow capturer, assuming the saxpy kernel is
only invocable through a stream-based API.

Both cudaFlow and cudaFlow capturer can work seam-
lessly with condition tasks. Control-ﬂow decisions fre-
quently happen at the boundary between CPU and GPU
tasks. For example, a heterogeneous k-means algorithm
iteratively uses GPU to accelerate the ﬁnding of k centroids
and then uses CPU to check if the newly found centroids
converge to application rules. Taskﬂow enables an end-to-
end expression of such a workload in a single graph entity,
as shown in Figure 8 and Listing 9. This capability largely
improves the efﬁciency of modeling complex CPU-GPU

TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS

6

workloads, and our scheduler can dynamically overlap
CPU and GPU tasks across different control-ﬂow blocks.

h2d

update centroids

0

converged?

1

d2h

Fig. 8: A cyclic task graph using three cudaFlow tasks and one
condition task to model an iterative k-means algorithm.

auto [ h2d , update , cond , d2h ] = t as k f low . emplace (

[ & ] ( t f : : cudaFlow& c f ) { /* copy in p u t
[ & ] ( t f : : cudaFlow& c f ) { /* update k e r n e l */ } ,
[ & ] ( ) { r e t u r n converged ( ) ? 1 : 0 ; } ,
[ & ] ( t f : : cudaFlow& c f ) { /* copy r e s u l t

t o GPU */ } ,

t o CPU */ }

) ;
h2d . precede ( update ) ;
update . precede ( cond ) ;
cond . precede ( update , d2h ) ;

Listing 9: Taskﬂow program of Figure 8.

5 TASKFLOW SYSTEM RUNTIME

Taskﬂow enables users to express CPU-GPU dependent
tasks that integrate control ﬂow into an HTDG. To support
our model with high performance, we design the system
runtime at two scheduling levels, task level and worker
level. The goal of task-level scheduling is to (1) devise a
feasible, efﬁcient execution for in-graph control ﬂow and
(2) transform each GPU task into a runnable instance on
a GPU. The goal of worker-level scheduling is to optimize
the execution performance by dynamically balancing the
worker count with task parallelism.

5.1 Task-level Scheduling Algorithm

5.1.1 Scheduling Condition Tasks

Conditional tasking is powerful but challenging to sched-
ule. Speciﬁcally, we must deal with conditional depen-
dency and cyclic execution without encountering task race,
i.e., only one thread can touch a task at a time. More
importantly, we need to let users easily understand our
task scheduling ﬂow such that they can infer if a written
task graph is properly conditioned and schedulable. To
accommodate these challenges, we separate the execution
logic between condition tasks and other tasks using two
dependency notations, weak dependency (out of condition
tasks) and strong dependency (other else). For example, the
six dashed arrows in Figure 5 are weak dependencies and
the solid arrow init→F1 is a strong dependency. Based on
these notations, we design a simple and efﬁcient algorithm
for scheduling tasks, as depicted in Figure 9. When the
scheduler receives an HTDG, it (1) starts with tasks of
zero dependencies (both strong and weak) and continues
executing tasks whenever strong remaining dependencies
are met, or (2) skips this rule for weak dependency and
directly jumps to the task indexed by the return of that
condition task.

Taking Figure 5 for example, the scheduler starts with
init (zero weak and strong dependencies) and proceeds
to F1. Assuming F1 returns 0, the scheduler proceeds to
its ﬁrst successor, F2. Now, assuming F2 returns 1, the
scheduler proceeds to its second successor, F1, which forms

"

!

<$"$"%"=>’5;

!

!"#$"$"%&%’&()%!

9-+:*’*-+%’&();

"

" 8%*+,-)".!/

#$%&#&#’"!" ($00"((-3

()*!’+,"’!)-.-

*+,-)".!/

!"0%!"#$%&%&’ ()!*&’+
,"-"&,"&.%"( -1%!2(%
($00"((-3(%45%-+"

6+#$"$"%($00"((-3(%-1%
7"3-%!"#$%&%&’+()!*&’+
,"-"&,"&.%"(

Fig. 9: Flowchart of our task scheduling.

a cyclic execution and so forth. With this concept, the
scheduler will cease at stop when F1, F2, and F3 all return
0. Based on this scheduling algorithm, users can quickly
infer whether their task graph deﬁnes correct control ﬂow.
For instance, adding a strong dependency from init to F2
may cause task race on F2, due to two execution paths,
init→F2 and init→F1→F2.

Figure 10 shows two common pitfalls of conditional
tasking, based on our task-level scheduling logic. The ﬁrst
example has no source for the scheduler to start with. A
simple ﬁx is to add a task S of zero dependencies. The
second example may race on D, if C returns 0 at the same
time E ﬁnishes. A ﬁx is to partition the control ﬂow at C
and D with an auxiliary node X such that D is strongly
conditioned by E and X.

Error 1: no source

Fix 1: add a source

Error 2: race on D Fix2: add an auxiliary node

A

0

S

C

E

C

1

2

1

0

0

1

B

C

A

0

F

D

X

F

E

1

2

B

C

D

Fig. 10: Common pitfalls of conditional tasking.

5.1.2 Scheduling GPU Tasks

We leverage modern CUDA Graph [4] to schedule GPU
tasks. CUDA graph is a new asynchronous task graph pro-
gramming model introduced in CUDA 10 to enable more
efﬁcient launch and execution of GPU work than streams.
There are two types of GPU tasks, cudaFlow and cudaFlow
capturer. For each scheduled cudaFlow task, since we know
all the operation parameters, we construct a CUDA graph
that maps each task in the cudaFlow, such as copy and
kernel, and each dependency to a node and an edge in the
CUDA graph. Then, we submit it to the CUDA runtime
for execution. This organization is simple and efﬁcient,
especially under modern GPU architectures (e.g., Nvidia
Ampere) that support hardware-level acceleration for graph
parallelism.

TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS

On the other hand, for each scheduled cudaFlow cap-
turer task, our runtime transforms the captured GPU tasks
and dependencies into a CUDA graph using stream cap-
ture [4]. The objective is to decide a stream layout optimized
for kernel concurrency without breaking task dependencies.
We design a greedy round-robin algorithm to transform
a cudaFlow capturer to a CUDA graph, as shown in Al-
gorithm 1. Our algorithm starts by levelizing the capturer
graph into a two-level array of tasks in their topological
orders. Tasks at the same level can run simultaneously.
However, assigning each independent task here a unique
stream does not produce decent performance, because GPU
has a limit on the maximum kernel concurrency (e.g., 32
for RTX 2080). We give this constraint to users as a tunable
parameter, max streams. We assign each levelized task an
id equal to its index in the array at its level. Then, we
can quickly assign each task a stream using the round-
robin arithmetic (line 6). Since tasks at different levels have
dependencies, we need to record an event (lines 13:17)
and wait on the event (lines 7:11) from both sides of a
dependency, saved for those issued in the same stream (line
8 and line 14).

Algorithm 1: make graph(G)

Input: a cudaFlow capturer C
Output: a transformed CUDA graph G

1 S ← get capture mode streams(max streams);
2 L ← levelize(C);
3 l ← L.min level;
4 while l <= L.max level do
5

foreach t ∈ L.get tasks(l) do

6

7

8

9

10

11

12

13

14

15

16

17

s ← (t.id mod max streams);
foreach p ∈ t.predecessors do

if s 6= (p.id mod max streams) then
stream wait event(S[s], p.event);

end

end
stream capture(t, S[s]);
foreach n ∈ t.successors do

if s 6= (n.id mod max streams) then
stream record event(S[s], p.event);

end

end

end

18
19 end
20 G ← end capture mode streams(S);
21 return G;

Figure 11 gives an example of transforming a user-
given cudaFlow capturer graph into a native CUDA graph
using two streams (i.e., max stream = 2) for execution.
The algorithm ﬁrst levelizes the graph by performing a
topological traversal and assign each node an id equal to
its index at the level. For example, A and B are assigned 0
and 1, C, D, and E are assigned 0, 1, and 2, and so on. These
ids are used to quickly determine the mapping between a
stream and a node in our round-robin loop, because CUDA
stream only allows inserting events from the latest node
in the queue. For instance, when A and B are assigned to
stream 0 (upper row) and stream 1 (lower row) during

()

(*

(+

!

"

#

$

%

&

’

7

,-./0123/4/53/567
#.899:9-./0123/4/53/5672;/</5-=

!

"

#

$

%

&

’

!44BC60-C8526>30&B8A ’.04@

#04->./32#?$!2’.04@2;-A829-./019=

Fig. 11: Illustration of Algorithm 1 on transforming an applica-
tion cudaFlow capturer graph into a native CUDA graph using
two streams.

the level-by-level traversal (line 4 of Algorithm 1), we can
determine ahead of the stream numbers of their successors
and ﬁnd out the two cross-stream dependencies, A→D and
B→E, that need recording events. Similarly, we can wait on
recorded events by scanning the predecessors of each node
to ﬁnd out cross-stream event dependencies.

5.2 Worker-level Scheduling Algorithm

At the worker level, we leverage work stealing to execute
submitted tasks with dynamic load balancing. Work steal-
ing has been extensively studied in multicore program-
ming [2], [12], [13], [16], [23], [40], [39], [47], [52], but an
efﬁcient counterpart for hybrid CPU-GPU or more general
heterogeneous systems remains demanding. This is a chal-
lenging research topic, especially under Taskﬂow’s HTDG
model. When executing an HTDG, a CPU task can submit
both CPU and GPU tasks and vice versa whenever de-
pendencies are met. The available task parallelism changes
dynamically, and there are no ways to predict the next
coming tasks under dynamic control ﬂow. To achieve good
system performance, the scheduler must balance the num-
ber of worker threads with dynamically generated tasks to
control the number of wasteful steals because the wasted
resources should have been used by useful workers or other
concurrent programs [13], [23].

Keeping workers busy in awaiting tasks with a yield-
ing mechanism is a commonly used work-stealing frame-
work [16], [17], [25]. However, this approach is not cost-
efﬁcient, because it can easily over-subscribe resources
when tasks become scarce, especially around the decision-
making points of control ﬂow. The sleep-based mechanism
is another way to suspend the workers frequently failing
in steal attempts. A worker is put into sleep by waiting
for a condition variable to become true. When the worker
sleeps, OS can grant resources to other workers for running
useful jobs. Also, reducing wasteful steals can improve
both the inter-operability of a concurrent program and the
overall system performance, including latency, throughput,
and energy efﬁciency to a large extent [23]. Nevertheless,
deciding when and how to put workers to sleep, wake up workers
to run, and balance the numbers of workers with dynamic task
parallelism is notoriously challenging to design correctly and
implement efﬁciently.

Our previous work [42] has introduced an adaptive
work-stealing algorithm to address a similar line of the
challenge yet in a CPU-only environment by maintaining

TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS

8

a loop invariant between active and idle workers. How-
ever, extending this algorithm to a heterogeneous target is
not easy, because we need to consider the adaptiveness
in different heterogeneous domains and bound the total
number of wasteful steals across all domains at any time
of the execution. To overcome this challenge, we introduce
a new scheduler architecture and an adaptive worker man-
agement algorithm that are both generalizable to arbitrary
heterogeneous domains. We shall prove the proposed work-
stealing algorithm can deliver a strong upper bound on the
number of wasteful steals at any time during the execution.

5.2.1 Heterogeneous Work-stealing Architecture

At the architecture level, our scheduler maintains a set of
workers for each task domain (e.g., CPU, GPU). A worker
can only steal tasks of the same domain from others. Figure
12 shows the architecture of our work-stealing scheduler
on two domains, CPU and GPU. By default, the number
of domain workers equals the number of domain devices
(e.g., CPU cores, GPUs). We associate each worker with
two separate task queues, a CPU task queue (CTQ) and a
GPU task queue (GTQ), and declare a pair of CTQ and GTQ
shared by all workers. The shared CTQ and GTQ pertain to
the scheduler and are primarily used for external threads
to submit HTDGs. A CPU worker can push and pop a
new task into and from its local CTQ, and can steal tasks
from all the other CTQs; the structure is symmetric to GPU
workers. This separation allows a worker to quickly insert
dynamically generated tasks to their corresponding queues
without contending with other workers.

0"#/,-*1/5)’)’

!"#/,-*1/5)’)’

6,’-.

")*+7(%(/,-*1

()*’56$-%$’+"$%#&,

7)*’56$-%$’+"$%#&,

*,’-.

*,’-./0"#/,-*1*

()*’+#,-’
./%/%

()*+

(%(

()*+

$)2-3.%4

()*+

$%&’

$%&’

$%&’

$%&’

!"#

$%&’

!"#

then checks the predicate again and calls commit_wait
to wait, if the outcome remains false, or cancel_wait to
cancel the request. Reversely, the notifying worker changes
the predicate to true and call notify_one or notify_all
to wake up one or all waiting workers. Event notiﬁer is
particularly useful for our scheduler architecture because
we can keep notiﬁcation between workers non-blocking.
We develop one event notiﬁer for each domain, based on
Dekker’s algorithm by [11].

5.2.2 Heterogeneous Work-stealing Algorithm

Atop this architecture, we devise an efﬁcient algorithm to
adapt the number of active workers to dynamically gener-
ated tasks such that threads are not underutilized when
tasks are abundant nor overly subscribed when tasks are
scarce. Our adaptiveness is different from existing frame-
works, such as constant wake-ups [2], [23], data locality [21],
[49], and watchdogs [23]. Instead, we extend our previous
work [42] to keep a per-domain invariant to control the
numbers of thieves and, consequently, wasteful steals based
on the active worker count: When an active worker exists, we
keep at least one worker making steal attempts unless all workers
are active.

Unlike the CPU-only scheduling environment in [42],
the challenge to keep this invariant in a heterogeneous
target comes from the heterogeneously dependent tasks and
cross-domain worker notiﬁcations, as a CPU task can spawn
a GPU task and vice versa. Our scheduler architecture is
particularly designed to tackle this challenge by separating
decision controls to a per-domain basis. This design allows
us to realize the invariant via an adaptive strategy–the last
thief to become active will wake up a worker in the same domain
to take over its thief role, and so forth. External threads (non-
workers) submit tasks through the shared task queues and
wake up workers to run tasks.

()*+

()*+

()*+

()*+

(%(

Algorithm 2: worker loop(w)

*,’-./!"#/
,-*1*

7)*’+#,-’
./%/%

Input: w: a worker
Per-worker global: t: a task (initialized to NIL)

!"#$%&’()*’
+#,-’./%/%
0%1+%$2#3’+"$%#&,4

()*+

89:7

()*+

!"#$%&’7)*’+#,-’
./%/%
0%1+%$2#3’+"$%#&,4

1 while true do
2

3

4

break;

exploit task(w, t);
if wait for task(w, t) == false then

Fig. 12: Architecture of our work-stealing scheduler on two
domains, CPU and GPU.

end

5
6 end

We leverage two existing concurrent data structures,
work-stealing queue and event notiﬁer, to support our schedul-
ing architecture. We implemented the task queue based on
the lock-free algorithm proposed by [36]. Only the queue
owner can pop/push a task from/into one end of the queue,
while multiple threads can steal a task from the other end
at the same time. Event notiﬁer is a two-phase commit
protocol (2PC) that allows a worker to wait on a binary
predicate in a non-blocking fashion [11]. The idea is similar to
the 2PC in distributed systems and computer networking.
The waiting worker ﬁrst checks the predicate and calls
prepare_wait if it evaluates to false. The waiting worker

Our scheduling algorithm is symmetric by domain.
Upon spawned, each worker enters the loop in Algorithm 2.
Each worker has a per-worker global pointer t to a task that
is either stolen from others or popped out from the worker’s
local task queue after initialization; the notation will be used
in the rest of algorithms. The loop iterates two functions,
exploit_task and wait_for_task. Algorithm 3 imple-
ments the function exploit_task. We use two scheduler-
level arrays of atomic variables, actives and thieves,
to record for each domain the number of workers that are
actively running tasks and the number of workers that are

TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS

9

Algorithm 3: exploit task(w, t)

Input: w: a worker (domain dw)
Per-worker global: t: a task

1 if t 6= NIL then
2

if AtomInc(actives[dw ]) == 1 and thieves[dw] == 0
then

3

4

5

6

7

8

notif ier[dw].notify one();

end
do

execute task(w, t);
t ← w.task queue[dw].pop();

while t 6= NIL;
AtomDec(actives[dw]);

9
10 end

making steal attempts, respectively.1 Our algorithm relies
on these atomic variables to decide when to put a worker to
sleep for reducing resource waste and when to bring back
a worker for running new tasks. Lines 2:4 implement our
adaptive strategy using two lightweight atomic operations.
In our pseudocodes, the two atomic operations, AtomInc
and AtomDec, return the results after incrementing and
decrementing the values by one, respectively. Notice that
the order of these two comparisons matters (i.e., active
workers and then thieves), as they are used to synchronize
with other workers in the later algorithms. Lines 5:8 drain
out the local task queue and executes all the tasks using
execute_task in Algorithm 4. Before leaving the function,
the worker decrements actives by one (line 9).

Algorithm 4: execute task(w, t)

Input: w: a worker
Per-worker global: t: a task
1 r ← invoke task callable(t);
2 if r.has value() then
3

submit task(w, t.successors[r]);
return;

4
5 end
6 foreach s ∈ t.successors do
7

if AtomDec(s.strong dependents) == 0 then

8

submit task(w, s);

end

9
10 end

Algorithm 4 implements the function execute_task.
We invoke the callable of the task (line 1). If the task returns
a value (i.e., a condition task), we directly submit the task
of the indexed successor (lines 2:5). Otherwise, we remove
the task dependency from all immediate successors and
submit new tasks of zero remaining strong dependencies
(lines 6:10). The detail of submitting a task is shown in
Algorithm 5. The worker inserts the task into the queue
of the corresponding domain (line 1). If the task does not
belong to the worker’s domain (line 2), the worker wakes
up one worker from that domain if there are no active

1. While our pseudocodes use array notations of atomic variables for
the sake of brevity, the actual implementation considers padding to
avoid false-sharing effects.

Algorithm 5: submit task(w, t)

Input: w: a worker (domain dw)
Per-worker global: t: a task (domain dt)

1 w.task queue[dt].push(t);
2 if dw! = dt then
3

if actives[dt] == 0 and thieves[dt] == 0 then

4

notif ier[dt].notify one();

end

5
6 end

workers or thieves (lines 3:5). The function submit_task
is internal to the workers of a scheduler. External threads
never touch this call.

When a worker completes all tasks in its local queue,
it proceeds to wait_for_task (line 3 in Algorithm 2),
as shown in Algorithm 6. At ﬁrst, the worker enters
explore_task to make steal attempts (line 2). When the
worker steals a task and it is the last thief, it notiﬁes a
worker of the same domain to take over its thief role
and returns to an active worker (lines 3:8). Otherwise, the
worker becomes a sleep candidate. However, we must avoid
underutilized parallelism, since new tasks may come at the
time we put a worker to sleep. We use 2PC to adapt the
number of active workers to available task parallelism (lines
9:41). The predicate of our 2PC is at least one task queue, both
local and shared, in the worker’s domain is nonempty. At line
8, the worker has drained out its local queue and devoted
much effort to stealing tasks. Other task queues in the same
domain are most likely to be empty. We put this worker
to a sleep candidate by submitting a wait request (line 9).
From now on, all the notiﬁcations from other workers will be
visible to at least one worker, including this worker. That is,
if another worker call notify at this moment, the 2PC
guarantees one worker within the scope of lines 9:41 will
be notiﬁed (i.e., line 42). Then, we inspect our predicate by
examining the shared task queue again (lines 10:20), since
external threads might have inserted tasks at the same time
we call prepare_wait. If the shared queue is nonempty
(line 10), the worker cancels the wait request and makes
an immediate steal attempt at the queue (lines 11:12); if
the steal succeeds and it is the last thief, the worker goes
active and notiﬁes a worker (lines 13:18), or otherwise enters
the steal loop again (line 19). If the shared queue is empty
(line 20), the worker checks whether the scheduler received
a stop signal from the executor due to exception or task
cancellation, and notiﬁes all workers to leave (lines 21:28).
Now, the worker is almost ready to sleep except if it is the
last thief and: (1) an active worker in its domain exists
(lines 30:33) or (2) at least one task queue of the same
domain from other workers is nonempty (lines 34:39). The
two conditions may happen because a task can spawn tasks
of different domains and trigger the scheduler to notify the
corresponding domain workers. Our 2PC guarantees the
two conditions synchronize with lines 2:4 in Algorithm 3
and lines 3:5 in Algorithm 5, and vice versa, preventing
the problem of undetected task parallelism. Passing all
the above conditions, the worker commits to wait on our
predicate (line 41).

Algorithm 7 implements explore_task, which resem-

TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS

10

Algorithm 6: wait for task(w, t)
Input: w: a worker (domain dw)
Per-worker global: t: a task
Output: a boolean signal of stop

1 AtomInc(thieves[dw]);
2 explore task(w, t);
3 if t 6= NIL then
4

if AtomDec(thieves[dw]) == 0 then
notif ier[dw].notify one();

end
return true;

7
8 end
9 notif ier[dw].prepare wait(w);
10 if task queue[dw].empty() 6= true then
notif ier[dw].cancel wait(w);
11
t ← task queue[dw].steal();
if t 6= NIL then

12

13

if AtomDec(thieves[dw]) == 0 then
notif ier[dw].notify one();

end
return true;

end
goto Line 2;

19
20 end
21 if stop == true then
22

notif ier[dw].cancel wait(w);
foreach domain d ∈ D do

notif ier[d].notify all();

end
AtomDec(thieves[dw]);
return false;

27
28 end
29 if AtomDec(thieves[dw]) == 0 then
if actives[dw] > 0 then
30

5

6

14

15

16

17

18

23

24

25

26

31

32

33

34

35

36

37

38

notif ier[dw].cancel wait(w);
goto Line 1;

end
foreach worker x ∈ W do

if x.task queue[dw].empty() 6= true then

notif ier[dw].cancel wait(w);
goto Line 1;

end

end

39
40 end
41 notif ier[dw].commit wait(w);
42 return true;

bles the normal work-stealing loop [16]. At each iteration,
the worker (thief) tries to steal a task from a randomly
selected victim, including the shared task queue, in the
same domain. We use a parameter M AX ST EALS to
control the number of iterations. In our experiments, setting
M AX ST EAL to ten times the number of all workers is
sufﬁcient enough for most applications. Up to this time,
we have discussed the core work-stealing algorithm. To
submit an HTDG for execution, we call submit_graph,
shown in Algorithm 8. The caller thread inserts all tasks
of zero dependencies (both strong and weak dependencies)
to the shared task queues and notiﬁes a worker of the

Algorithm 7: explore task(w, t)

Input: w: a worker (a thief in domain dw)
Per-worker global: t: a task (initialized to NIL)

1 steals ← 0;
2 while t != NIL and ++steals ≤ M AX ST EAL do
3

yield();
t ← steal task from random victim(dw);

4
5 end

Algorithm 8: submit graph(g)
Input: g: an HTDG to execute

1 foreach t ∈ g.source tasks do
2

scoped lock lock(queue mutex);
dt ← t.domain;
task queue[dt].push(t);
notif ier[dt].notify one();

3

4

5
6 end

corresponding domain (lines 4:5). Shared task queues may
be accessed by multiple callers and are thus protected under
a lock pertaining to the scheduler. Our 2PC guarantees
lines 4:5 synchronizes with lines 10:20 of Algorithm 6 and
vice versa, preventing undetected parallelism in which all
workers are sleeping.

6 ANALYSIS

To justify the efﬁciency of our scheduling algorithm, we
draw the following theorems and give their proof sketches.

Lemma 1. For each domain, when an active worker (i.e., running
a task) exists, at least one another worker is making steal attempts
unless all workers are active.

Proof. We prove Lemma 1 by contradiction. Assuming there
are no workers making steal attempts when an active
worker exists, this means an active worker (line 2 in Algo-
rithm 3) fails to notify one worker if no thieves exist. There
are only two scenarios for this to happen: (1) all workers
are active; (2) a non-active worker misses the notiﬁcation
before entering the 2PC guard (line 9 in Algorithm 6). The
ﬁrst scenario is not possible as it has been excluded by
the lemma. If the second scenario is true, the non-active
worker must not be the last thief (contradiction) or it will
notify another worker through line 3 in Algorithm 6. The
proof holds for other domains as our scheduler design is
symmetric.

Theorem 1. Our work-stealing algorithm can correctly complete
the execution of an HTDG.

Proof. There are two places where a new task is submitted,
line 4 in Algorithm 8 and line 1 in Algorithm 5. In the ﬁrst
place, where a task is pushed to the shared task queue by
an external thread, the notiﬁcation (line 5 in Algorithm 8)
is visible to a worker in the same domain of the task for
two situations: (1) if a worker has prepared or committed
to wait (lines 9:41 in Algorithm 6), it will be notiﬁed; (2)
otherwise, at least one worker will eventually go through
lines 9:20 in Algorithm 6 to steal the task. In the second

TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS

11

place, where the task is pushed to the corresponding local
task queue of that worker, at least one worker will exe-
cute it in either situation: (1) if the task is in the same
domain of the worker, the work itself may execute the
task in the subsequent exploit_task, or a thief steals
the task through explore_task; (2) if the worker has a
different domain from the task (line 2 in Algorithm 5), the
correctness can be proved by contradiction. Assuming this
task is undetected, which means either the worker did not
notify a corresponding domain worker to run the task (false
at the condition of line 3 in Algorithm 5) or notiﬁed one
worker (line 4 in Algorithm 5) but none have come back.
In the former case, we know at least one worker is active
or stealing, which will eventually go through line 29:40 of
Algorithm 6 to steal this task. Similarly, the latter case is not
possible under our 2PC, as it contradicts the guarding scan
in lines 9:41 of Algorithm 6.

Theorem 2. Our work-stealing algorithm does not under-
subscribe thread resources during the execution of an HTDG.

Proof. Theorem 2 is a byproduct of Lemma 1 and Theorem
1. Theorem 1 proves that our scheduler never has task leak
(i.e., undetected task parallelism). During the execution of
an HTDG, whenever the number of tasks is larger than
the present number of workers, Lemma 1 guarantees one
worker is making steal attempts, unless all workers are
active. The 2PC guard (lines 34:39 in Algorithm 6) ensures
that worker will successfully steal a task and become an
active worker (unless no more tasks), which in turn wakes
up another worker if that worker is the last thief. As a
consequence, the number of workers will catch up on the
number of tasks one after one to avoid under-subscribed
thread resources.

Theorem 3. At any moment during the
execution of
an HTDG,
the number of wasteful steals is bounded by
O(M AX ST EALS × (|W | + |D| × (E/es))), where W is
the worker set, D is the domain set, E is the maximum execution
time of any task, and es is the execution time of Algorithm 7.

Proof. We give a direct proof for Theorem 3 using the
following notations: D denotes the domain set, d denotes
a domain (e.g., CPU, GPU), W denotes the entire worker
set, Wd denotes the worker set in domain d, wd denotes a
worker in domain d (i.e., wd ∈ Wd), es denotes the time to
complete one round of steal attempts (i.e., Algorithm 7), ed
denotes the maximum execution time of any task in domain
d, and E denotes the maximum execution time of any task
in the given HTDG.

At any time point, the worst case happens at the fol-
lowing scenario: for each domain d only one worker wd is
actively running one task while all the other workers are making
unsuccessful steal attempts. Due to Lemma 1 and lines 29:40 in
Algorithm 6, only one thief w′
d will eventually remain in the
loop, and the other |Wd| − 2 thieves will go sleep after one
round of unsuccessful steal attempts (line 2 in Algorithm 6)
which ends up with M AX ST EALS × (|Wd| − 2) wasteful
steals. For the only one thief w′
d, it keeps failing in steal
attempts until the task running by the only active worker
wd ﬁnishes, and then both go sleep. This results in another
M AX ST EALS × (ed/es) + M AX ST EALS wasteful
steals; the second terms comes from the active worker

because it needs another round of steal attempts (line 2
in Algorithm 6) before going to sleep. Consequently, the
number of wasteful steals across all domains is bounded as
follows:

M AX ST EALS × (|Wd| − 2 + (ed/es) + 1)

X
d∈D

≤ M AX ST EALS × X
d∈D

(|Wd| + ed/es)

(1)

≤ M AX ST EALS × X
d∈D
= O(M AX ST EALS × (|W | + |D| × (E/es)))

(|Wd| + E/es)

We do not derive the bound over the execution of an
HTDG but the worst-case number of wasteful steals at any
time point, because the presence of control ﬂow can lead
to non-deterministic execution time that requires a further
assumption of task distribution.

7 EXPERIMENTAL RESULTS
We evaluate the performance of Taskﬂow on two fronts:
micro-benchmarks and two realistic workloads, VLSI in-
cremental timing analysis and machine learning. We use
micro-benchmarks to analyze the tasking performance of
Taskﬂow without much bias of application algorithms.
We will show that the performance beneﬁts of Taskﬂow
observed in micro-benchmarks become signiﬁcant in real
workloads. We will study the performance across runtime,
energy efﬁciency, and throughput. All experiments ran on
a Ubuntu Linux 5.0.0-21-generic x86 64-bit machine with
40 Intel Xeon CPU cores at 2.00 GHz, 4 GeForce RTX 2080
GPUs, and 256 GB RAM. We compiled all programs using
Nvidia CUDA v11 on a host compiler of clang++ v10 with
C++17 standard -std=c++17 and optimization ﬂag -O2
enabled. We do not observe signiﬁcant difference between
-O2 and -O3 in our experiments. Each run of N CPU cores
and M GPUs corresponds to N CPU and M GPU worker
threads. All data is an average of 20 runs.

7.1 Baseline

Give a large number of TGCSs, it is impossible to com-
pare Taskﬂow with all of them. Each of the existing sys-
tems has its pros and cons and dominates certain appli-
cations. We consider oneTBB [2], StarPU [17], HPX [33],
and OpenMP [7] each representing a particular paradigm
that has gained some successful user experiences in CAD
due to performance [44]. oneTBB (2021.1 release) is an
industrial-strength parallel programming system under In-
tel oneAPI [2]. We consider its FlowGraph library and
encapsulate each GPU task in a CPU function. At the time
of this writing, FlowGraph does not have dedicated work
stealing for HTDGs. StarPU (version 1.3) is a CPU-GPU
task programming system widely used in the scientiﬁc
computing community [17]. It provides a C-based syntax
for writing HTDGs on top of a work-stealing runtime highly
optimized for CPUs and GPUs. HPX (version 1.4) is a C++
standard library for concurrency and parallelism [33]. It
supports implicit task graph programming through aggre-
gating future objects in a dataﬂow API. OpenMP (version

TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS

4.5 in clang toolchains) is a directive-based programming
framework for handling loop parallelism [7]. It supports
static graph encoding using task dependency clauses.

To measure the expressiveness and programmability of
Taskﬂow, we hire ﬁve PhD-level C++ programmers outside
our research group to implement our experiments. We edu-
cate them the essential knowledge about Taskﬂow and base-
line TGCSs and provide them all algorithm blocks such that
they can focus on programming HTDGs. For each imple-
mentation, we record the lines of code (LOC), the number
of tokens, cyclomatic complexity (measured by [8]), time
to ﬁnish, and the percentage of time spent on debugging.
We average these quantities over ﬁve programmers until
they obtain the correct result. This measurement may be
subjective but it highlights the programming productivity
and turnaround time of each TGCSs from a real user’s
perspective.

7.2 Micro-benchmarks

We randomly generate a set of DAGs (i.e., HTDGs) with
equal distribution of CPU and GPU tasks. Each task per-
forms a SAXPY operation over 1K elements. For fair pur-
pose, we implemented CUDA Graph [4] for all baselines;
each GPU task is a CUDA graph of three GPU operations,
H2D copy, kernel, and H2D copy, in this order of depen-
dencies. Table 1 summarizes the programming effort of
each method. Taskﬂow requires the least amount of lines of
code (LOC) and written tokens. The cyclomatic complexity
of Taskﬂow measured at a single function and across the
whole program is also the smallest. The development time
of Taskﬂow-based implementation is much more produc-
tive than the others. For this simple graph, Taskﬂow and
oneTBB are very easy for our programmers to implement,
whereas we found they spent a large amount of time on
debugging task graph parallelism with StarPU, HPX, and
OpenMP.

TABLE 1: Programming Effort on Micro-benchmark

LOC
69
182
253
255
182

Method
Taskﬂow
oneTBB
StarPU
HPX
OpenMP

CC WCC Dev
#Tokens
14
8
6
650
25
15
8
1854
47
21
8
2216
41
24
10
2264
57
19
13
1896
CC: maximum cyclomatic complexity in a single function
WCC: weighted cyclomatic complexity of the program
Dev: minutes to complete the implementation
Bug: time spent on debugging as opposed to coding task graphs

Bug
1%
6%
19%
33%
49%

Next, we study the overhead of task graph parallelism
among Taskﬂow, oneTBB, and StarPU. As shown in Table
2, the static size of a task, compiled on our platform, is
272, 136, and 1472 bytes for Taskﬂow, oneTBB, and StarPU,
respectively. We do not report the data of HPX and OpenMP
because they do not support explicit task graph construction
at the functional level. The time it takes for Taskﬂow to
create a task and add a dependency is also faster than
oneTBB and StarPU. We amortize the time across 1M opera-
tions because all systems support pooled memory to recycle
tasks. We found StarPU has signiﬁcant overhead in creating
HTDGs. The overhead always occupies 5-10% of the total
execution time regardless of the HTDG size.

TABLE 2: Overhead of Task Graph Creation

Ttask
61 ns
99 ns
259 ns

Method
Taskﬂow
oneTBB
StarPU

Stask
272
136
1472
Stask: static size per task in bytes
Ttask/Tedge: amortized time to create a task/dependency
ρv : graph size where its creation overhead is below v%

Tedge
14 ns
54 ns
384 ns

ρ<10
550
1225
7550

ρ<5
2550
2750
-

12

ρ<1
35050
40050
-

Figure 13 shows the overall performance comparison
between Taskﬂow and the baseline at different HTDG sizes.
In terms of runtime (top left of Figure 13), Taskﬂow out-
performs others across most data points. We complete the
largest HTDG by 1.37×, 1.44×, 1,53×, and 1.40× faster
than oneTBB, StarPU, HPX, and OpenMP, respectively. The
memory footprint (top right of Figure 13) of Taskﬂow is
close to oneTBB and OpenMP. HPX has higher memory
because it relies on aggregated futures to describe task
dependencies at the cost of shared states. Likewise, StarPU
does not offer a closure-based interface and thus requires
a ﬂat layout (i.e., codelet) to describe tasks. We use the
Linux perf tool to measure the power consumption of all
cores plus LLC [3]. The total joules (bottom left of Figure
13) consumed by Taskﬂow is consistently smaller than the
others, due to our adaptive worker management. In terms
of power (bottom right of Figure 13), Taskﬂow, oneTBB, and
OpenMP are more power-efﬁcient than HPX and StarPU.
The difference between Taskﬂow and StarPU continues to
increase as we enlarge the HTDG size.

Runtime

Memory

)
s

m

(

e
m

i
t
n
u
R

6,000

4,000

2,000

0

0

)
J
(

s
e
l
u
o
J

l
a
t
o
T

1,000

800

600

400

200

0

0

Taskﬂow
oneTBB
StarPU
HPX
OpenMP

0.4

0.6

0.2
0.8
Graph Size (|V | + |E|)
Energy

Taskﬂow
oneTBB
StarPU
HPX
OpenMP

1
·105

650

600

550

500

450

400

200

150

100

)
B
M

(

S
S
R
m
u
m
i
x
a
M

)

W

(

r
e
w
o
P
e
g
a
r
e
v
A

Taskﬂow
oneTBB
StarPU
HPX
OpenMP

0

0.6

0.4

0.2
0.8
Graph Size (|V | + |E|)
Power (all cores + LLC)

1
·105

Taskﬂow
oneTBB
StarPU
HPX
OpenMP

0.4

0.2
0.8
Graph Size (|V | + |E|)

0.6

50

0

1
·105

0.4

0.2
0.8
Graph Size (|V | + |E|)

0.6

1
·105

Fig. 13: Overall system performance at different problem sizes
using 40 CPUs and 4 GPUs.

Figure 14 displays the runtime distribution of each
method over a hundred runs of two HTDGs, 5K and 20K
tasks. The boxplot shows that the runtime of Taskﬂow is
more consistent than others and has the smallest variation.
We attribute this result to the design of our scheduler, which
effectively separates task execution into CPU and GPU
workers and dynamically balances cross-domain wasteful
steals with task parallelism.

TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS

13

)
s

m

(

e
m

i
t
n
u
R

Runtime (5K tasks)

Runtime (20K tasks)

1,200

1,000

800

6,000

5,500

5,000

4,500

)
s

m

(

e
m

i
t
n
u
R

w
o
ﬂ
k
s
a
T

B
B
T
e
n
o

U
P
r
a
t
S

X
P
H

P
M
n
e
p
O

w
o
ﬂ
k
s
a
T

B
B
T
e
n
o

U
P
r
a
t
S

X
P
H

P
M
n
e
p
O

Fig. 14: Runtime distribution of two task graphs.

Corun (20K tasks)

Consumed CPU Resource

Taskﬂow
oneTBB
StarPU
HPX
OpenMP

2.5

2

1.5

1

0.5

t
u
p
h
g
u
o
r
h
T

n
o
i
t
a
z
i
l
i
t

U

40

30

20

10

0

0

2

4

6

8

Number of Coruns

0

Taskﬂow
oneTBB
StarPU
HPX
OpenMP

init

inc_loop

0.4

0.2
0.8
Graph Size (|V | + |E|)

0.6

1
·105

get_cands

millions or even billions of times to incrementally analyze
the timing improvement of a design transform. We consider
the GPU-accelerated critical path analysis algorithm [26]
and run it across one thousand incremental iterations based
on the design transforms given by TAU 2015 Contest [29].
The data is generated by an industrial tool to evaluate
the performance of an incremental timing algorithm. Each
incremental iteration corresponds to at least one design
modiﬁer followed by a timing report operation to trigger
incremental timing update of the timer.

net_delay

flattern_net_2

flattern_net_1

h2d_at

h2d_slew

elmore_slew

elmore_delay_0

elmore_delay_1

elmore_delay_2

elmore_delay_3

d2h_slew

Fig. 15: Throughput of corunning task graphs and CPU utiliza-
tion at different problem sizes under 40 CPUs and 4 GPUs.

rc3

rc4

rc1

rc2

d2h_at

0

cpu_gpu

Finally, we compare the throughput of each method on
corunning HTDGs. This experiment emulates a server-like
environment where multiple programs run simultaneously
on the same machine to compete for the same resources.
The effect of worker management propagates to all parallel
processes. We consider up to nine corun processes each ex-
ecuting the same HTDG of 20K tasks. We use the weighted
speedup [23] to measure the system throughput. Figure 15
compares the throughput of each method and relates the
result to the CPU utilization. Both Taskﬂow and oneTBB
produce signiﬁcantly higher throughput than others. Our
throughput is slightly better than oneTBB by 1–15% except
for seven coruns. The result can be interpreted by the CPU
utilization plot, reported by perf stat. We can see both
Taskﬂow and oneTBB make effective use of CPU resources
to schedule tasks. However, StarPU keeps workers busy
most of the time and has no mechanism to dynamically
control thread resources with task parallelism.

Since both oneTBB and StarPU provides explicit task
graph programming models and work-stealing for dynamic
load balancing, we will focus on comparing Taskﬂow with
oneTBB and StarPU for the next two real workloads.

7.3 VLSI Incremental Timing Analysis

As part of our DARPA project, we applied Taskﬂow to solve
a VLSI incremental static timing analysis (STA) problem in
an optimization loop. The goal is to optimize the timing
landscape of a circuit design by iteratively applying design
transforms (e.g., gate sizing, buffer insertion) and evaluating
the timing improvement until all data paths are passing,
aka timing closure. Achieving timing closure is one of the
most time-consuming steps in the VLSI design closure ﬂow
process because optimization algorithms can call a timer

0

1

cpu_run

rc_update

0

gpu_run

1

merge

1

stop

Fig. 16: A partial HTDG of 1 cudaFlow task (purple box), 4
condition tasks (green diamond), and 8 static tasks (other else)
for one iteration of timing-driven optimization.

Figure 16 shows a partial Taskﬂow graph of our imple-
mentation. One condition task forms a loop to implement
iterative timing updates and the other three condition tasks
branch the execution to either CPU-based timing update
(over 10K tasks) or GPU-based timing update (cudaFlow
tasks). The motivation here is to adapt the timing update
to different incrementalities. For example, if a design trans-
form introduces only a few hundreds of nodes to update,
there is no need to ofﬂoad the computation to GPUs due
to insufﬁcient amount of data parallelism. The cudaFlow
task composes over 1K operations to compute large in-
terconnect delays, which often involves several gigabytes
of parasitic data. Since oneTBB FlowGraph and StarPU
do not support control ﬂow, we unroll their task graphs
across ﬁxed-length iterations found in hindsight to avoid
expensive synchronization at each iteration; the number of
concatenated graphs is equal to the number of iterations.

Table 3 compares the programming effort between Task-
ﬂow, oneTBB, and StarPU. In a rough view, the imple-
mentation complexity using Taskﬂow is much less than
that of oneTBB and StarPU. The amount of time spent on
implementing the algorithm is about 3.9 hours for Taskﬂow,
6.1 hours for oneTBB, and 4.3 hours for StarPU. It takes 3–

TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS

14

TABLE 3: Programming Effort on VLSI Timing Closure
Bug
Method
LOC
13%
Taskﬂow 3176
51%
4671
oneTBB
38%
5643
StarPU
CC: maximum cyclomatic complexity in a single function
WCC: weighted cyclomatic complexity of the program
Dev: hours to complete the implementation
Bug: time spent on the debugging versus coding task graphs

CC WCC Dev
3.9
67
30
6.1
92
41
4.3
98
46

#Tokens
5989
8713
13952

4× more time to debug oneTBB and StarPU than Taskﬂow,
mostly on control ﬂow. Interestingly, while StarPU involves
more LOC and higher cyclomatic complexity than oneTBB,
our programmers found StarPU easier to write due to its
C-styled interface. Although there is no standard way to
conclude the programmability of a library, we believe our
measurement highlights the expressiveness of Taskﬂow and
its ease of use from a real user’s perspective.

Runtime (40 CPUs 1 GPU)

Speed-up (1000 iterations)

billions of iterations that take several hours to ﬁnish [30]. We
observed similar results at other CPU numbers; in terms of
the runtime speed-up over 1 CPU (all ﬁnish in 113 minutes),
Taskﬂow is always faster than oneTBB and StarPU, regard-
less of the CPU count. Speed-up of Taskﬂow saturates at
about 16 CPUs (3×), primarily due to the inherent irregular-
ity of the algorithm (see Figure 16). The memory footprint
(middle of Figure 17) shows the beneﬁt of our conditional
tasking. By reusing condition tasks in the incremental tim-
ing loop, we do not suffer signiﬁcant memory growth as
oneTBB and StarPU. On a vertical scale, increasing the
number of CPUs bumps up the memory usage of both
methods, but Taskﬂow consumes much less because we use
only simple atomic operations to control wasteful steals. In
terms of energy efﬁciency (bottom of Figure 17, measured
on all cores plus LLC using power/energy-pkg [3]), our
scheduler is very power-efﬁcient in completing the tim-
ing analysis workload, regardless of iterations and CPU
numbers. Beyond 16 CPUs where performance saturates,
Taskﬂow does not suffer from increasing power as oneTBB
and StarPU, because our scheduler efﬁciently balances the
number of workers with dynamic task parallelism.

Corun (500 iterations)

Corun (1000 Iterations)

Taskﬂow
oneTBB
StarPU

0

10

20

30

40

CPU Count (under 1 GPU)
Memory (1000 iterations)

Taskﬂow
oneTBB
StarPU

t
u
p
h
g
u
o
r
h
T

5

4

3

2

1

0

Taskﬂow
oneTBB
StarPU

2

3

4

5

6

7

t
u
p
h
g
u
o
r
h
T

2.5

2

1.5

1

0.5

0

Taskﬂow
oneTBB
StarPU

2

3

4

5

6

7

Taskﬂow
oneTBB
StarPU

200

400

600

800

1,000

Iterations
Memory (40 CPUs 1 GPU)

Taskﬂow
oneTBB
StarPU

60

40

20

0

25

20

15

10

5

3

2.5

2

1.5

1

25

20

15

10

5

0

o
i
t
a
R

)
B
G

(

S
S
R
m
u
m
i
x
a
M

)

m

(

e
m

i
t
n
u
R

)
B
G

(

S
S
R
m
u
m
i
x
a
M

200

400

600

800

1,000

Iterations
Power (40 CPUs 1 GPU)

0

10

20

30

40

CPU Count (under 1 GPU)
Power (1000 iterations)

Taskﬂow
oneTBB
StarPU

)

W

(

r
e
w
o
P

150

100

50

0

Taskﬂow
oneTBB
StarPU

200

400

600

800

1,000

Iterations

0

10

20

30

40

CPU Count (under 1 GPU)

)

W

(

r
e
w
o
P

200

150

100

50

Fig. 17: Runtime, memory, and power data of 1000 incremental
timing iterations (up to 11K tasks and 17K dependencies per
iteration) on a large design of 1.6M gates.

The overall performance is shown in Figure 17. Using
40 CPUs and 1 GPU, Taskﬂow is consistently faster than
oneTBB and StarPU across all incremental timing iterations.
The gap continues to enlarge as increasing iteration num-
bers; at 100 and 1000 iterations, Taskﬂow reaches the goal in
3.45 and 39.11 minutes, whereas oneTBB requires 5.67 and
4.76 minutes and StarPU requires 48.51 and 55.43 minutes,
respectively. Note that the gain is signiﬁcant because a
typical timing closure algorithm can invoke millions to

Number of Coruns

Number of Coruns

Fig. 18: Throughput of corunning timing analysis workloads
on two iteration numbers using 40 CPUs and 1 GPU.

We next compare the throughput of each implementa-
tion by corunning the same program. Corunning programs
is a common strategy for optimization tools to search for the
best parameters. The effect of worker management propa-
gates to all simultaneous processes. Thus, the throughput
can be a good measurement for the inter-operability of a
scheduling algorithm. We corun the same timing analysis
program up to seven processes that compete for 40 CPUs
and 1 GPU. We use the weighted speedup to measure the
system throughput, which is the sum of the individual
speedup of each process over a baseline execution time [23].
A throughput of one implies that the corun’s throughput is
the same as if the processes were run consecutively. Figure
18 plots the throughput across nine coruns at two itera-
tion numbers. Both Taskﬂow and oneTBB achieve decent
throughput greater than one and are signiﬁcantly better
than StarPU. We found StarPU keep workers busy most of
the time and has no mechanism to balance the number of
workers with dynamically generated task parallelism. For
irregular HTDGs akin to Figure 16, worker management is
critical for corunning processes. When task parallelism be-
comes sparse, especially around the decision-making point
of an iterative control ﬂow, our scheduler can adaptively
reduce the wasteful steals based on the active worker count,
and we offer a stronger bound than oneTBB (Theorem

TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS

15

3). Saved wasteful resources can thus be used by other
concurrent programs to increase the throughput.

cudaFlow: GPU 0

h2dw

cudaFlow: GPU 1

cudaFlow: GPU 2

cudaFlow: GPU 3

h2dw

h2dw

h2dw

h2dw

SpMM

h2dw

SpMM

h2dw

SpMM

h2dw

SpMM

cudaFlow vs Stream (mem)

SpMM

h2dw

d2hr

SpMM

h2dw

d2hr

SpMM

h2dw

d2hr

SpMM

h2dw

d2hr

cudaFlow vs Stream (runtime)
140

cudaFlow
CUDA Stream 1
CUDA Stream 2

e
m

i
t
n
u
R

120

100

80

60

40

)
B
G

(

S
S
R
m
u
m
i
x
a
M

3.5

3

2.5

2

1.5

cudaFlow
CUDA Stream 1
CUDA Stream 2

0

10

20

30

40

0

10

20

30

40

CPU Count (under 1 GPU)

CPU Count (under 1 GPU)

Fig. 19: Comparison of runtime and memory between cud-
aFlow (CUDA Graph) and stream-based execution in the VLSI
incremental timing analysis workload.

Figure 19 shows the performance advantage of CUDA
Graph and its cost in handling this large GPU-accelerated
timing analysis workloads. The line cudaFlow represents our
default implementation using explicit CUDA graph con-
struction. The other two lines represent the implementation
of the same GPU task graph but using stream and event
insertions (i.e., non-CUDA Graph). As partially shown
in Figure 16, our cudaFlow composes over 1K depen-
dent GPU operations to compute the interconnect delays.
For large GPU workloads like this, the beneﬁt of CUDA
Graph is clear; we observed 9–17% runtime speed-up over
stream-based implementations. The performance improve-
ment mostly comes from reduced kernel call overheads and
graph-level scheduling optimizations by CUDA runtime.
Despite the improved performance, cudaFlow incurs higher
memory costs because CUDA Graph stores all kernel pa-
rameters in advance for optimization. For instance, creating
a node in CUDA Graph can take over 300 bytes of opaque
data structures.

7.4 Large Sparse Neural Network Inference

We applied Taskﬂow to solve the MIT/Amazon Large
Sparse Deep Neural Network (LSDNN) Inference Chal-
lenge, a recent effort aimed at new computing methods for
sparse AI analytics [35]. Each dataset comprises a sparse
matrix of the input data for the network, 1920 layers of
neurons stored in sparse matrices, truth categories, and the
bias values used for the inference. Preloading the network
to the GPU is impossible. Thus, we implement a model
decomposition-based kernel algorithm inspired by [19] and
construct an end-to-end HTDG for the entire inference
workload. Unlike VLSI incremental timing analysis, this
workload is both CPU- and GPU-heavy. Figure 20 illustrates
a partial HTDG. We create up to 4 cudaFlows on 4 GPUs.
Each cudaFlow contains more than 2K GPU operations to
run partitioned matrices in an iterative data dispatching
loop formed by a condition task. Other CPU tasks evaluate
the results with a golden reference. Since oneTBB Flow-
Graph and StarPU do not support in-graph control ﬂow, we
unroll their task graph across ﬁxed-length iterations found
ofﬂine.

Figure 21 compares the performance of solving a 1920-
layered LSDNN each of 4096 neurons under different CPU
and GPU numbers. Taskﬂow outperforms oneTBB and

h2dw

d2hr

SpMM

h2dw

d2hr

SpMM

h2dw

d2hr

SpMM

h2dw

d2hr

SpMM

d2hr

SpMM

d2hr

init0

init1

init2

init3

d2hr

SpMM

d2hr

d2hr

SpMM

d2hr

d2hr

SpMM

d2hr

start

f0

pf0

pf1

f1

pf2

pf3

f2

0

0

1

1

1

1

0

0

1

1

1

0

0

0

1

f3

0

GPU0

stop

GPU 1

GPU 2

GPU 3

Fig. 20: A partial HTDG of 4 cudaFlows (purple boxes), 8
conditioned cycles (green diamonds), and 6 static tasks (other
else) for the inference workload.

Runtime (40 CPUs)

Runtime (4 GPUs)

1,800

1,600

1,400

)
s

m

(

e
m

i
t
n
u
R

1,200

1,000

800

)
B
G

(

S
S
R
m
u
m
i
x
A
M

1.4

1.2

1

0.8

0.6

)
s

m

(

e
m

i
t
n
u
R

Taskﬂow
oneTBB
StarPU

Taskﬂow
oneTBB
StarPU

1,400

1,200

1,000

800

1

2

3

4

12 4

8

16

32

Number of GPUs
Memory (40 CPUs)

Number of CPUs
Memory (4 GPUs)

Taskﬂow
oneTBB
StarPU

1.4

1.2

1

)
B
G

(

S
S
R
m
u
m
i
x
A
M

Taskﬂow
oneTBB
StarPU

1

2

3

4

12 4

8

16

32

Number of GPUs

Number of CPUs

Fig. 21: Runtime and memory data of the LSDNN (1920 layers,
4096 neurons per layer) under different CPU and GPU numbers

StarPU in all aspects. Both our runtime and memory scale
better regardless of the CPU and GPU numbers. Using 4
GPUs, when performance saturates at 4 CPUs, we do not
suffer from further runtime growth as oneTBB and StarPU.
This is because our work-stealing algorithm more efﬁciently
control wasteful steals upon available task parallelism. On
the other hand, our memory usage is 1.5-1.7× less than
oneTBB and StarPU. This result highlights the beneﬁt of
our condition task, which integrates iterative control ﬂow
into a cyclic HTDG, rather than unrolling it statically across
iterations.

We next compare the throughput of each implementa-
tion by corunning the same inference program to study
the inter-operability of an implementation. We corun the
same inference program up to nine processes that compete
for 40 CPUs and 4 GPUs. We use weighted speedup to
measure the throughput. Figure 22 plots the throughput
of corunning inference programs on two different sparse
neural networks. Taskﬂow outperforms oneTBB and StarPU
across all coruns. oneTBB is slightly better than StarPU
because StarPU tends to keep all workers busy all the
time and results in large numbers of wasteful steals. The
largest difference is observed at ﬁve coruns of inferencing

TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS

16

Corun (1920×4096)

Corun (1920×65536)

Taskﬂow
oneTBB
StarPU

t
u
p
h
g
u
o
r
h
T

5

4

3

2

1

0

Taskﬂow
oneTBB
StarPU

2

4

6

8

Number of Coruns

t
u
p
h
g
u
o
r
h
T

5

4

3

2

1

0

cudaFlow vs Stream (runtime)
4,000

cudaFlow vs Stream (mem)

e
m

i
t
n
u
R

3,000

2,000

1,000

cudaFlow
CUDA Stream 1
CUDA Stream 2
CUDA Stream 4

)
B
G

(

S
S
R
m
u
m
i
x
a
M

9

8

7

6

cudaFlow
CUDA Stream 1
CUDA Stream 2
CUDA Stream 4

2

4

6

8

1

Number of Coruns

2
3
GPU Count

4

1

2
3
GPU Count

4

Fig. 22: Throughput of corunning inference workloads on two
1920-layered neural networks, one with 4096 neurons per layer
and another with 65536 neurons per layer.

Fig. 24: Comparison of runtime and memory between cud-
aFlow (CUDA Graph) and stream-based execution.

the 1920×4096 neural network, where our throughput is
1.9× higher than oneTBB and 2.1× higher than StarPU.
These CPU- and GPU-intensive workloads highlight the
effectiveness of our heterogeneous work stealing. By keep-
ing a per-domain invariant, we can control cross-domain
wasteful steals to a bounded value at any time during the
execution.

Capturer (1920×4096)

Capturer (1920×65536)

Capturer 1

Capturer 2

Capturer 4

Capturer 8

cudaFlow

Capturer 1

Capturer 2

Capturer 4

Capturer 8

cudaFlow

750

800

850

900

950

8,000

8,500

9,000

9,500

Runtime (ms)

Runtime (ms)

Fig. 23: Performance of our cudaFlow capturer using 1, 2, 4, and
8 streams to complete the inference of two neural networks.

We study the performance of our cudaFlow capturer
using different numbers of streams (i.e., max streams). For
complex GPU workloads like Figure 20, stream concurrency
is crucial to GPU performance. As shown in Figure 23,
explicit construction of a CUDA graph using cudaFlow
achieves the best performance, because the CUDA runtime
can dynamically decide the stream concurrency with inter-
nal optimization. For applications that must use existing
stream-based APIs, our cudaFlow capturer achieves com-
parable performance as cudaFlow by using two or four
streams. Taking the 1920×65536 neural network for exam-
ple, the difference between our capturer of four streams and
cudaFlow is only 10 ms. For this particular workload, we do
not observe any performance beneﬁt beyond four streams.
Application developers can ﬁne-tune this number.

We ﬁnally compare the performance of cudaFlow with
stream-based execution. As shown in Figure 24, the line
cudaFlow represents our default implementation using ex-
plicit CUDA graph construction, and the other lines rep-
resent stream-based implementations for the same task
graph using one, two, and four streams. The advantage of
CUDA Graph is clearly demonstrated in this large machine
learning workload of over 2K dependent GPU operations
per cudaFlow. Under four streams that deliver the best
performance for the baseline, cudaFlow is 1.5× (1451 vs
2172) faster at one GPU and is 1.9× (750 vs 1423) faster

at four GPUs. The cost of this performance improvement
is increased memory usage because CUDA Graph needs
to store all the operating parameters in the graph. For
instance, under four streams, cudaFlow has 4% and 6%
higher memory usage than stream-based execution at one
and four GPUs, respectively.

8 RELATED WORK
8.1 Heterogeneous Programming Systems

Heterogeneous programming systems are the main driving
force to advance scientiﬁc computing. Directive-based pro-
gramming models [5], [6], [7], [25], [38] allow users to aug-
ment program information of loop mapping onto CPUs/G-
PUs and data sharing rules to designated compilers for
automatic parallel code generation. These models are good
at loop-based parallelism but cannot handle irregular task
graph patterns efﬁciently [37]. Functional approaches [2],
[15], [17], [18], [20], [24], [32], [33], [34], [41] offer either im-
plicit or explicit task graph constructs that are more ﬂexible
in runtime control and on-demand tasking. Each of these
systems has its pros and cons. However, few of them enable
end-to-end expressions of heterogeneously dependent tasks
with general control ﬂow.

8.2 Heterogeneous Scheduling Algorithms

Among various heterogeneous runtimes, work stealing is
a popular strategy to reduce the complexity of load bal-
ancing [16], [41] and has inspired the designs of many
parallel runtimes [2], [12], [39], [40], [52]. A key challenge
in work-stealing designs is worker management. Instead
of keeping all workers busy most of the time [16], [17],
[32], both oneTBB [2] and BWS [23] have developed sleep-
based strategies. oneTBB employs a mixed strategy of ﬁxed-
number worker notiﬁcation, exponential backoff, and noop
assembly. BWS modiﬁes OS kernel to alter the yield be-
havior. [42] takes inspiration from BWS and oneTBB to de-
velop an adaptive work-stealing algorithm to minimize the
number of wasteful steals. Other approaches, such as [13]
that targets a space-sharing environment, [47] that tunes
hardware frequency scaling, [22], [48] that balance load on
distributed memory, [21], [27], [51], [57] that deal with data
locality, and [49] that focuses on memory-bound applica-
tions have improved work stealing in certain performance
aspects, but their results are limited to the CPU domain.
How to migrate the above approaches to a heterogeneous
target remains an open question.

TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS

17

In terms of GPU-based task schedulers, Whippletree [50]
design a ﬁne-grained resource scheduling algorithm for
sparse and scattered parallelism atop a custom program
model. [45] leverages reinforcement learning to place ma-
chine learning workloads onto GPUs. Hipacc [46] intro-
duces a pipeline-based optimization for CUDA graphs to
speed up image processing workloads. [55] develops a com-
piler to transforms OpenMP directives to a CUDA graph.
These works have primarily focused on scheduling GPU
tasks in various applications, which are orthogonal to our
generic heterogeneous scheduling approaches.

9 ACKNOWLEDGEMENTS

The project is supported by the DARPA contract FA 8650-
18-2-7843 and the NSF grant CCF-2126672. We appreciate
all Taskﬂow contributors and reviewers’ comments for im-
proving this paper.

10 CONCLUSION

In this paper, we have introduced Taskﬂow, a lightweight
task graph computing system to streamline the creation of
heterogeneous programs with control ﬂow. Taskﬂow has
introduced a new programming model that enables an end-
to-end expression of heterogeneously dependent tasks with
general control ﬂow. We have developed an efﬁcient work-
stealing runtime optimized for latency, energy efﬁciency,
and throughput, and derived theory results to justify its
efﬁciency. We have evaluated the performance of Taskﬂow
on both micro-benchmarks and real applications. As an
example, Taskﬂow solved a large-scale machine learning
problem up to 29% faster, 1.5× less memory, and 1.9×
higher throughput than the industrial system, oneTBB, on a
machine of 40 CPUs and 4 GPUs.

Taskﬂow is an on-going project under active develop-
ment. We are currently exploring three directions: First,
we are designing a distributed tasking model based on
partitioned taskﬂow containers with each container run-
ning on a remote machine. Second, we are extending our
model to incorporate SYCL [9] to provide a single-source
heterogeneous task graph programming environment. The
author Dr. Huang is a member of SYCL Advisory Panel and
is collaborating with the working group to design a new
SYCL Graph abstraction. Third, we are researching auto-
matic translation methods between different task graph pro-
gramming models using Taskﬂow as an intermediate repre-
sentation. Programming-model translation has emerged as
an important research area in today’s diverse computing
environments because no one programming model is opti-
mal across all applications. The recent 2021 DOE X-Stack
program directly calls for novel translation methods to
facilitate performance optimizations on different computing
environments.

One important future direction is to collaborate with
Nvidia CUDA teams to design a conditional tasking inter-
face within the CUDA Graph itself. This design will enable
efﬁcient control-ﬂow decisions to be made completely in
CUDA runtime, thereby largely reducing the control-ﬂow
cost between CPU and GPU.

REFERENCES

[1] DARPA Intelligent Design of Electronic Assets (IDEA) Program.

https://www.darpa.mil/program/intelligent-design-of-electronic-assets.
Intel oneTBB. https://github.com/oneapi-src/oneTBB.

[2]
[3] Linux Kernel Proﬁler. https://man7.org/linux/man-pages/man1/perf-stat.1.html.
[4] Nvidia CUDA Graph. https://devblogs.nvidia.com/cuda-10-features-revealed/.
[5] OmpSs. https://pm.bsc.es/ompss.
[6] OpenACC. http://www.openacc-standard.org.
[7] OpenMP. https://www.openmp.org/.
SLOCCount. https://dwheeler.com/sloccount/.
[8]
[9]
SYCL. https://www.khronos.org/sycl/.
[10] Taskﬂow github. https://taskﬂow.github.io/.
[11] Two-phase Commit Protocol. http://www.1024cores.net/home/lock-free-algorithms/eventcounts.
[12] K. Agrawal, C. E. Leiserson, and J. Sukha. Nabbit: Executing task
graphs using work-stealing. In IEEE IPDPS, pages 1–12, 2010.
[13] Kunal Agrawal, Yuxiong He, and Charles E. Leiserson. Adaptive
Work Stealing with Parallelism Feedback. In PPoPP, pages 112–
120, 2007.

[14] Tutu Ajayi, Vidya A. Chhabria, Mateus Fogac¸a, Soheil Hashemi,
Abdelrahman Hosny, Andrew B. Kahng, Minsoo Kim, Jeongsup
Lee, Uday Mallappa, Marina Neseem, Geraldo Pradipta, Sherief
Reda, Mehdi Saligane, Sachin S. Sapatnekar, Carl Sechen, Mo-
hamed Shalan, William Swartz, Lutong Wang, Zhehong Wang,
Mingyu Woo, and Bangqi Xu. Toward an Open-Source Digital
Flow: First Learnings from the OpenROAD Project. In ACM/IEEE
DAC, 2019.

[15] Marco Aldinucci, Marco Danelutto, Peter Kilpatrick, and Massimo
Torquati. Fastﬂow: High-Level and Efﬁcient Streaming on Multicore,
chapter 13, pages 261–280. John Wiley and Sons, Ltd, 2017.
[16] Nimar S. Arora, Robert D. Blumofe, and C. Greg Plaxton. Thread
In ACM

Scheduling for Multiprogrammed Multiprocessors.
SPAA, pages 119–129, 1998.

[17] C´edric Augonnet, Samuel Thibault, Raymond Namyst, and
Pierre-Andr´e Wacrenier. StarPU: A Uniﬁed Platform for Task
Scheduling on Heterogeneous Multicore Architectures. Concurr.
Comput. : Pract. Exper., 23(2):187–198, 2011.

[18] M. Bauer, S. Treichler, E. Slaughter, and A. Aiken.

Expressing locality and independence with logical regions.
IEEE/ACM SC, pages 1–11, 2012.

Legion:
In

[19] M. Bisson and M. Fatica. A GPU Implementation of the Sparse
In IEEE HPEC, pages

Deep Neural Network Graph Challenge.
1–8, 2019.

[20] George Bosilca, Aurelien Bouteiller, Anthony Danalis, Mathieu
Faverge, Thomas Herault, and Jack J. Dongarra. PaRSEC: Exploit-
ing Heterogeneity to Enhance Scalability. Computing in Science
Engineering, 15(6):36–45, 2013.

[21] Quan Chen, Minyi Guo, and Haibing Guan. LAWS: Locality-
Aware Work-Stealing for Multi-Socket Multi-Core Architectures.
In ACM ICS, page 3–12, 2014.

[22] James Dinan, D. Brian Larkins, P. Sadayappan, Sriram Krish-
namoorthy, and Jarek Nieplocha. Scalable Work Stealing. In ACM
SC, SC ’09, 2009.

[23] Xiaoning Ding, Kaibo Wang, Phillip B. Gibbons, and Xiaodong
Zhang. BWS: Balanced Work Stealing for Time-sharing Multi-
cores. In ACM EuroSys, pages 365–378, 2012.

[24] H. Carter Edwards, Christian R. Trott, and Daniel Sunderland.
Kokkos: Enabling manycore performance portability through
Journal of Parallel and
polymorphic memory access patterns.
Distributed Computing, 74(12):3202 – 3216, 2014.

[25] T. Gautier, J. V. F. Lima, N. Maillard, and B. Rafﬁn. XKaapi: A Run-
time System for Data-Flow Task Programming on Heterogeneous
Architectures. In IEEE IPDPS, pages 1299–1308, 2013.

[26] Guannan Guo, Tsung-Wei Huang, Yibo Lin, and Martin Wong.
In IEEE/ACM

GPU-accelerated Pash-based Timing Analysis.
DAC, 2021.

[27] Yi Guo. A Scalable Locality-Aware Adaptive Work-Stealing

Scheduler for Multi-Core Task Parallelism. 2010.

[28] Zizheng Guo, Tsung-Wei Huang, and Yibo Lin. GPU-accelerated
Static Timing Analysis. In IEEE/ACM ICCAD, pages 1–8, 2020.
[29] J. Hu, G. Schaeffer, and V. Garg. TAU 2015 Contest on Incremental
Timing Analysis. In IEEE/ACM ICCAD, pages 895–902, 2015.
[30] Tsung-Wei Huang, Guannan Guo, Chun-Xun Lin, and Martin
Wong. OpenTimer 2.0: A New Parallel Incremental Timing Anal-
ysis Engine. IEEE TCAD, (4):776–789, 2021.

[31] Tsung-Wei Huang, Dian-Lun Lin, Yibo Lin, and Chun-Xun Lin.
Taskﬂow: A General-purpose Parallel and Heterogeneous Task
Programming System. IEEE TCAD, 2021 (to appear).

TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS

18

[32] Tsung-Wei Huang, Yibo Lin, Chun-Xun Lin, Guannan Guo, and
Martin D. F. Wong. Cpp-taskﬂow: A general-purpose parallel task
programming system at scale. IEEE TCAD, 40(8):1687–1700, 2021.
[33] Hartmut Kaiser, Thomas Heller, Bryce Adelstein-Lelbach, Adrian
Serio, and Dietmar Fey. HPX: A Task Based Programming Model
in a Global Address Space. In PGAS, pages 6:1–6:11, 2014.

[34] Laxmikant V. Kale and Sanjeev Krishnan. Charm++: A Portable
In ACM

Concurrent Object Oriented System Based on C++.
ASPLOS, page 91–108, New York, NY, USA, 1993.

[35] Jeremy Kepner, Simon Alford, Vijay Gadepally, Michael Jones,
Lauren Milechin, Ryan Robinett, and Sid Samsi. Sparse deep
neural network graph challenge. IEEE HPEC, 2019.

[36] Nhat Minh Lˆe, Antoniu Pop, Albert Cohen, and Francesco
Zappa Nardelli. Correct and Efﬁcient Work-stealing for Weak
Memory Models. In ACM PPoPP, pages 69–80, 2013.

[37] S. Lee and J. S. Vetter. Early Evaluation of Directive-Based GPU
Programming Models for Productive Exascale Computing.
In
IEEE/ACM SC, pages 1–11, 2012.

[38] Seyong Lee and Rudolf Eigenmann. OpenMPC: Extended
OpenMP Programming and Tuning for GPUs. In IEEE/ACM SC,
pages 1–11, 2010.

[39] Daan Leijen, Wolfram Schulte, and Sebastian Burckhardt. The
Design of a Task Parallel Library. In ACM OOPSLA, pages 227–
241, 2009.

[40] Charles E. Leiserson. The Cilk++ concurrency platform. The

Journal of Supercomputing, 51(3):244–257, 2010.

[41] Jo˜ao V.F. Lima, Thierry Gautier, Vincent Danjean, Bruno Rafﬁn,
and Nicolas Ma illard. Design and Analysis of Scheduling Strate-
gies for multi-CPU and multi-GPU Architectures. Parallel Comput.,
44:37–52, 2015.

[42] Chun-Xun Lin, Tsung-Wei Huang, and Martin D. F. Wong. An
efﬁcient work-stealing scheduler for task dependency graph. In
2020 IEEE ICPADS, pages 64–71, 2020.

[43] Y. Lin, W. Li, J. Gu, H. Ren, B. Khailany, and D. Z. Pan. ABCD-
Place: Accelerated Batch-based Concurrent Detailed Placement on
Multi-threaded CPUs and GPUs. IEEE TCAD, 2020.

[44] Yi-Shan Lu and Keshav Pingali. Can Parallel Programming

Revolutionize EDA Tools? Advanced Logic Synthesis, 2018.

[45] Azalia Mirhoseini, Hieu Pham, Quoc V. Le, Benoit Steiner, Rasmus
Larsen, Yuefeng Zhou, Naveen Kumar, Mohammad Norouzi,
Samy Bengio, and Jeff Dean. Device placement optimization with
reinforcement learning. In ACM ICML, page 2430–2439, 2017.
[46] Bo Qiao, M. Akif ¨Ozkan, J ¨urgen Teich, and Frank Hannig. The best
of both worlds: Combining cuda graph with an image processing
dsl. In IEEE/ACM DAC, pages 1–6, 2020.

[47] Haris Ribic and Yu David Liu. Energy-efﬁcient Work-stealing
Language Runtimes. In ACM ASPLOS, pages 513–528, 2014.
[48] Vijay A. Saraswat, Prabhanjan Kambadur, Sreedhar Kodali, David
Grove, and Sriram Krishnamoorthy. Lifeline-Based Global Load
Balancing. In PPoPP, page 201–212, 2011.

[49] Shumpei Shiina and Kenjiro Taura. Almost Deterministic Work

Stealing. In ACM SC, 2019.

[50] Markus Steinberger, Michael Kenzel, Pedro Boechat, Bernhard
Kerbl, Mark Dokter, and Dieter Schmalstieg. Whippletree: Task-
based scheduling of dynamic workloads on the gpu. ACM Trans.
Graph., 33(6), November 2014.

[51] Warut Suksompong, Charles E. Leiserson, and Tao B. Schardl. On
Information Processing

the efﬁciency of localized work stealing.
Letters, 116(2):100–106, Feb 2016.

[52] Olivier Tardieu, Haichuan Wang, and Haibo Lin. A Work-stealing
Scheduler for X10’s Task Parallelism with Suspension. SIGPLAN,
47(8):267–276, 2012.

[53] D. F. Wong, H. W. Leong, and C. L. Liu. Simulated Annealing for

VLSI Design. Kluwer Academic Publishers, USA, 1988.

[54] B. Xu, K. Zhu, M. Liu, Y. Lin, S. Li, X. Tang, N. Sun, and D. Z.
Pan. MAGICAL: Toward Fully Automated Analog IC Layout
Leveraging Human and Machine Intelligence: Invited Paper. In
IEEE/ACM ICCAD, pages 1–8, 2019.

[55] Chenle Yu, Sara Royuela, and Eduardo Qui ˜nones. Openmp to
cuda graphs: A compiler-based transformation to enhance the
programmability of nvidia devices. In International Workshop on
Software and Compilers for Embedded Systems, page 42–47, 2020.
[56] Yuan Yu, Mart´ın Abadi, Paul Barham, Eugene Brevdo, Mike
Burrows, Andy Davis, Jeff Dean, Sanjay Ghemawat, Tim Harley,
Peter Hawkins, Michael Isard, Manjunath Kudlur, Rajat Monga,
Derek Murray, and Xiaoqiang Zheng. Dynamic Control Flow in
Large-Scale Machine Learning. In IEEE EuroSys, 2018.

[57] Han Zhao, Quan Chen, Yuxian Qiu, Ming Wu, Yao Shen, Jing-
wen Leng, Chao Li, and Minyi Guo. Bandwidth and Locality
Aware Task-Stealing for Manycore Architectures with Bandwidth-
Asymmetric Memory. ACM Trans. Archit. Code Optim., 15(4), 2018.

Tsung-Wei Huang received the B.S. and M.S.
degrees from the Department of Computer Sci-
ence, National Cheng Kung University (NCKU),
Tainan, Taiwan, in 2010 and 2011, respectively.
He obtained his Ph.D. degree in the Electrical
and Computer Engineering (ECE) Department
at the University of Illinois at Urbana-Champaign
(UIUC). He is currently an assistant professor in
the ECE department at the University of Utah.
Dr. Huang has been building software systems
for parallel computing and timing analysis. His
PhD thesis won the prestigious 2019 ACM SIGDA Outstanding PhD
Dissertation Award for his contributions to distributed and parallel VLSI
timing analysis.

Dian-Lun Lin received the B.S. degree from
the Department of Electrical Engineering at Tai-
wan’s Cheng Kung University and M.S. degree
from the Department of Computer Science at
National Taiwan University. He is current a Ph.D.
student at
the Department of Electrical and
Computer Engineering at the University of Utah.
His research interests are in parallel and het-
erogeneous computing with a speciﬁc focus on
CAD applications.

Chun-Xun Lin received the B.S. degree in Elec-
trical Engineering from the National Cheng Kung
University, Tainan, Taiwan, and the M.S. de-
gree in Electronics Engineering from the Gradu-
ate Institute of Electronics Engineering, National
Taiwan University, Taipei, Taiwan, in 2009 and
2011, respectively. He received his Ph.D. degree
from the department of Electrical and Computer
Engineering (ECE) at the University of Illinois
at Urbana-Champaign (UIUC) in 2020. His re-
search interest is in parallel processing.

Yibo Lin (S’16–M’19) received the B.S. degree
in microelectronics from Shanghai Jiaotong Uni-
versity in 2013, and his Ph.D. degree from the
Electrical and Computer Engineering Depart-
ment of
the University of Texas at Austin in
2018. He is current an assistant professor in
the Computer Science Department associated
with the Center for Energy-Efﬁcient Computing
and Applications at Peking University, China. His
research interests include physical design, ma-
chine learning applications, GPU acceleration,

and hardware security.

