JavaBERT: Training a transformer-based model for
the Java programming language

Nelson Tavares de Sousa, Wilhelm Hasselbring
Software Engineering Group, Kiel University
Kiel, Germany
{tavaresdesousa, hasselbring}@email.uni-kiel.de

1
2
0
2

t
c
O
0
2

]
E
S
.
s
c
[

1
v
4
0
4
0
1
.
0
1
1
2
:
v
i
X
r
a

Abstract—Code quality is and will be a crucial factor while
developing new software code, requiring appropriate tools to
ensure functional and reliable code. Machine learning techniques
are still rarely used for software engineering tools, missing out the
potential beneﬁts of its application. Natural language processing
has shown the potential to process text data regarding a variety of
tasks. We argue, that such models can also show similar beneﬁts
for software code processing. In this paper, we investigate how
models used for natural language processing can be trained upon
software code. We introduce a data retrieval pipeline for software
code and train a model upon Java software code. The resulting
model, JavaBERT, shows a high accuracy on the masked language
modeling task showing its potential for software engineering tools.

I. INTRODUCTION

Reliability is gaining importance as more and more systems
across all domains, such as Internet of Things devices and
banking systems, rely on software to function properly. Defec-
tive software has the potential to cause malfunctioning systems
which may impose a variety of risks and greater amount of
costs. Therefore, software quality is a signiﬁcant factor which
must be considered during software development, playing a
crucial role in software engineering (SE). Rule-based systems,
like static code analysis, allow to detect bugs, code smells,
and similar issues, improving code quality [10]. However, they
rarely are able to self-adapt to evolving environments, as this
requires a manual update of the rules. Machine learning (ML)
could close this gap by learning from such new environments,
but we still observe a lack of usage of ML technologies in
the domain of SE. SE just starts to make use of AI powered
tools, but still stays behind the advancements made in other
ML domains such as natural language processing (NLP).

NLP has seen drastic improvements over the past years
with the development of new ML models, especially with
transformer-based models. Such models show a high perfor-
mance, which is able to outperform a human baseline on
certain tasks [12, 15]. Therefore, the state-of-the-art of NLP
already proves to be reliable on performing tasks upon the
semantic content of natural language. First approaches to take
concepts from the NLP domain to the domain of program
code understanding emerge [4]. Yet current approaches handle
software code as raw text neglecting the syntactical features
given by programming languages. Additionally, most existing
approaches are tailored to a speciﬁc downstream task, narrow-
ing down their ﬁeld of application. In contrast to pre-trained

models, which are trained on generic tasks, potential further
research with such speciﬁc models is limited.

We argue that not only a well pre-trained model for software
code is required, but also a deﬁned processing pipeline to
improve reproducibility. Thanks to the application of transfer
learning, such models carry the potential to accelerate the
development of new approaches. Different downstream tasks
can be trained on the same baseline model [3], increasing the
performance and comparability of the resulting models. An
adaptable pipeline can also be used for different programming
languages, facilitating the incorporation of new languages.

In this paper, we present our approach to train transformer-
based models on source code to enable the use of ML for
software engineering. We contribute by proposing a data
retrieval pipeline for software code and a concept which
allows to take syntactical features explicitly into account
while training a model Additionally, we evaluate whether this
concept improves the ﬁnal performance. Lastly, we present
a state-of-the-art model for the Java programming language,
called JavaBERT.

II. RELATED WORK

Recent research in this domain shows an increasing interest
in the application of machine learning for programming tasks.
For instance, the approach presented by Tufano et al. [14]
gives an example on how ML can be used in the domain
of SE. To assist the development process, an ML model is
used to anticipate recommended changes in reviews. With a
multimodal approach, program code before and after those
changes, and review comments are used to train the modal.

Bielik et al. [2] show how machine learning can be used
for static code analysis. Here, a ML model tries to identify
patterns which are associated with malicious program behavior
or code smells This is achieved by introducing a language-
agnostic rule speciﬁcation upon which the model is trained to
identify code parts which violate these rules. While being able
to be applied to different languages, the rule set still needs to
be updated manually if changes regarding the detectable code
parts need to be included.

By combining a convolutional neural network (CNN) with a
long short-term memory (LSTM), Huo and Li [5] were able to
achieve state-of-the-art performance for bug detection. Their
contribution especially relies on the application of a LSTM to
exploit the sequential nature of program code. This approach

 
 
 
 
 
 
is then fed with bug reports such that the model is able to
be trained toward bug detection. In contrast to this approach,
we will employ models which are able to grasp context to
a greater extent, eliminating the requirement for a LSTM.
The combination of a CNN and LSTM by Huo and Li [5]
is tailored to the use case of bug detection, whereas we aim
to pretrain a model on a generic task making it suitable for
downstream tasks later on.

throughout

The Bidirectional Encoder Representations from Transform-
ers (BERT) model is able to capture the context of large
the
text documents and propagate the context
whole document to both sides [3]. Therefore, any token in
the document is able to attend the context to its left and
right side. This allows BERT to be trained on documents of
greater sizes with comparably small models. The architectures
of previous state-of-the-art models imposed a lower overall
limit to the document sizes. Furthermore, BERT also uses
masked language modeling (MLM) for its pretraining task.
With MLM, words in the input are masked and the model’s
task is to predict the original word. BERT demonstrated how
this task is able to pretrain a reliable model which can be
used to train for downstream tasks and show state-of-the-art
performance. We transfer BERT to Java code.

Feng et al. [4] used data sets to ﬁrst pretrain a BERT model
relying on a variety of programming languages and afterwards
trained it upon several downstream tasks. The resulting model
shows state-of-the-art performance on the given tasks, showing
how transformer-based models like BERT may also be appli-
cable to other tasks in the domain of software development. In
contrast to this work, we investigate how a model dedicated
to a speciﬁc language performs compared to such a broader
approach.

III. RESEARCH OBJECTIVES

Our goal is to research ways to pretrain a BERT-based
model for programming languages, in our case Java. Three
research questions (RQ) can be derived to give an answer to
this.

RQ1: How can program code be retrieved for training pur-
poses?

A proper amount of data is crucial to train any ML model.
Large amounts of corpora are made available for the domain of
NLP but such data sets still lack for programming languages.
Sources for a greater amount of programming code ﬁles are
available but still need to be collected and be bundled into
corpora. We will showcase how this can be done for the Java
language and collect data for our use case.

RQ2: How to train an NLP model for programming lan-
guages?

Data for training purposes undergo certain preprocessing
steps before being forwarded to an ML model. However, in our
use case, program code may beneﬁt from an adapted approach.
Syntactical features of programming languages may be used
to increase the efﬁciency of ML models for such domains.

We will introduce such an approach, evaluate it by comparing
the results with state-of-the-art approaches, and discuss the
advantages and disadvantages of it.

RQ3: How does such a programming language model per-
form?

To evaluate the merits of such an approach, we need to
measure the performance of our approach. Metrics will be
introduced which give a well-deﬁned measurement that can
be used to compare models. These metrics will be applied to
all trained models to evaluate and compare the performance
of our custom approach with state-of-the-art NLP models.

IV. ARCHITECTURE

A. Data Retrieval

The ﬁrst step consists of the selection of appropriate data
for our use case. First of all, we choose a speciﬁc language we
want to use. The sole requirement for this language is its broad
acceptance and usage. The PYPL ranking1 gives an insight into
the programming languages which have been most searched on
Google. For the year 2021, Python takes ﬁrst place, however
Java has constantly shown a higher ranking until recent years.
Similar information can be taken from the TIOBE index.2 We
chose Java as our target language, due to its high rankings
in the previous years, which will most likely yield in greater
amounts of available code in comparison to Python or other
languages.

After this preliminary decision, we start

As a next step, we decide upon a data source. We require
lots of publicly available data which can be reused for our
purposes. GitHub, as a platform providing a version control
service, provides a multitude of Open Source Software (OSS),
which complies to these requirements. An addition to that,
GitHub provides interfaces (APIs), which allow to investigate
each repository, without downloading all data and parsing it
beforehand. Therefore, we chose GitHub as our data source.
to retrieve the
data. Figure 1 gives an overview of the different steps of our
retrieval pipeline. Each square depicts the status of the data
throughout the pipeline. The different processing and ﬁltering
actions are depicted by arrows. The GH Archive project3
allows to download all API events which took place in GitHub
for speciﬁed time ranges. We download all data for the year
2020 and load it into an Elasticsearch index, which allows us
to perform various queries on the data without relying on third
party services.

We choose to ﬁlter for projects which show recent activities.
Pull requests give a simple metric for active projects. There-
fore, we will ﬁrst only take projects into account which have
yielded pull requests. We do so, by performing an aggregation
query on our Elasticsearch instance, which counts the events
triggered by the creation of comments within pull requests
for each project. Projects without any pull request will not

1https://pypl.github.io/PYPL.html
2https://www.tiobe.com/tiobe-index/
3https://www.gharchive.org/

Fig. 1. Data retrieval and ﬁltering pipeline

be returned by this query. This list of active projects is
further ﬁltered down by their used licenses and programming
languages. To do so, we use the provided API by GitHub,
which returns metadata regarding the project, such as the used
license and programming languages. We ﬁlter out projects
which do not make use of the MIT or Apache 2.0 license
and in which Java is not within top three used programming
languages. Choosing these licenses allows to avoid conﬂicts
regarding copyright issues and improves the availability of the
data for further research. In addition, we ﬁlter out all projects,
which have less than 10 comments within all pull requests.
Eventually, this yields in 26983 projects which are then cloned
to a local drive.

In a next step, we take a sample out of all Java ﬁles. This
includes a simple ﬁltering of smaller and bigger ﬁles. As
threshold we decide to ﬁlter out ﬁles with more than 3000
and less than 40 Java tokens. This eliminates ﬁles with almost
no content, such as empty classes, and also god classes with
greater amount of code content. As a result, we have a sample
of the size of 4283372 Java ﬁles. These are split into two
sets which represent the training and test sets. The training
set contains a random subset of around 70% (2,998,345) of
all Java ﬁles, whereas the test set contains the remaining
1,285,027 ﬁles of those ﬁles.

B. Tokenizer

Tokenization describes a preprocessing step, where text is
divided into tokens which then can be fed into ML models. In
NLP this means typically the separation by words or smaller
parts of text like subword units. The latter allows to deal
with the disadvantages of ﬁxed vocabularies [17]. Afterwards,
tokens are assigned a unique numeric identiﬁer such that they
can be processed by ML models.

Programming languages undergo a different segmentation
by using lexemes, which introduce semantics to sequences
of characters [1]. These lexemes are then analyzed and fur-
ther processed into a stream of tokens. For Java, the Java
Language Speciﬁcation (JLS) deﬁnes how such tokens may
be composed.4. Tokens can be further distinguished into ﬁve
types: Identiﬁer, Literal, Keyword, Separator, and Operator.
For our use case, we will divide these into two groups. Tokens
of the types Separator and Operator are used by Java to
identify structural or operational entities, such as {, }, ==,
etc.. Keyword tokens include a speciﬁc set of tokens, which
are reserved by Java itself, such as if, new, etc... These token
types can be considered ﬁxed, as their textual representations
always stay the same. For our approach, we add the literals

true, false, and null to this list, as these seem like keywords,
but technically are not considered as such. We further refer to
this token set as special tokens.

Tokens of the types Identiﬁer and Literal compose a second
group of non-special tokens. These are not limited to a speciﬁc
vocabulary and may reﬂect any combination of character
sequences. The challenge here is to provide a vocabulary
which can be used to tokenize these words. The number of
indexed words is a relevant factor as it imposes some trade-
offs. We may provide tokens for any provided word, however
the size of the vocabulary dominates the performance of the
tokenizer. This can be circumvented by further splitting words
into smaller segments, effectively reducing the required vo-
cabulary size. WordPiece [17] is an algorithms which supports
such splitting into smaller segments. It searches for common
segments within all words and splits any words in the text
by such segments. This effectively reduces the vocabulary
size, as such subword segments can be reused to compose
complete words instead of including all possible segment
combinations occurring in the text. This is especially the case
for programming languages, due to the common usage of
variable names such as isEnabled, isDisabled, etc..

All ﬁles of our sample are used to ﬁrst extract included
tokens. The Javalang python library5 allows us to do so
respecting the rules given by the JLS. Special tokens are
extracted from the JLS and saved in a separate list. To this
list, we add special tokens required by BERT, such as [MASK],
[UNK] or [PAD]. These tokens are used by the BERT model
to indicate masked tokens or tokens uncovered by the provided
vocabulary. This allows WordPiece to handle these tokens
separately and not to further split them into subwords. To
train the tokenizer, we use the Tokenize library provided by
Hugging Face [16]. To analyze the performance impact of
different vocabulary sizes, we train the tokenizer thrice with
different settings for the resulting vocabulary size. We chose to
train the tokenizer on sizes of 8000, 16000, and 32000, which
reﬂect the range most used in the NLP domain. Furthermore,
we want to analyze how the vocabulary sizes will impact the
performance of the trained models, as it was already shown to
have a greater inﬂuence on this aspect [13]. Figure 2 shows a
short example on how the trained tokenizer handles code. In
a pre-tokenization step, the string is separated into lexemes,
which then are partially further split into subwords. Eventually,
each token is encoded with a unique id. The special tokens
are highlighted in this example and remain untouched.

4https://docs.oracle.com/javase/specs/jls/se8/html/jls-3.html#jls-3.5

5https://github.com/c2nes/javalang

Fig. 2. Output of the trained tokenizer with a vocabulary size of 8000

C. Model

We choose BERT as model to train, as it is well-known and
also used for other research and optimizations [6, 7]. Similar
optimization approaches may be applicable on our ﬁnal model.
As we perform a MLM training task, we use an untrained
BERT model with a language modeling (LM) head. The model
will be trained on program code on which a certain amount of
tokens are masked. Its task is to predict the original tokens.

V. EXPERIMENT

Our experiment

is set up by a training process and a
subsequent evaluation. We will train three different instances
of our BERT model, each using one of the three vocabularies.
To examine the possible beneﬁt of the Java tokenizer, we train
further BERT instances using the pretrained vocabulary for
BERT Base Uncased and BERT Base Cased to compare our
approach with generic ones. Eventually, an evaluation step will
compare the performance of all models against each other. As
we aim to evaluate the performance of different vocabulary
sizes, all remaining parameters will remain untouched. We
use the Transformers (4.9.1), Tokenizers (0.10.3), and Datasets
(1.10.2) libraries by Hugging Face in combination with Py-
Torch (1.7.1). Training is performed on three Nvidia Titan X
GPUs with 12GB of memory each, running on CUDA 10.1.
Two Intel Xeon E5-2650 with a total of 128GB of memory are
used to run the WordPiece algorithm to create the vocabularies.
The conﬁguration for our BERT model relies mainly on the
default values given by the Transformers library (12-layer,
768-hidden, 12-heads). The vocabulary size will be set to one
of the values of either 8000 (BERT Java-8k), 16000 (BERT
Java-16k), 32000 (BERT Java-32k), or 30522 (BERT Base
Cased/Uncased) depending on which model we aim to train.
For the optimizer we rely on the default, which is the AdamW
optimizer [8] and use a learning rate of 5e − 5. A batch
size of effectively 30 and a single epoch is used. With these
parameters, training takes between 17 and 24 hours depending
on the vocabulary size, with smaller vocabularies resulting in
faster training processes. The loss is calculated by using a
cross-entropy criterion. Figure 3 shows an excerpt of the loss
progress throughout the training for all models. We omitted
the ﬁrst 4000 steps for the sake of a better differentiation
of the losses at later steps in our visualization. All models
perform similar in the ﬁrst 4000 steps, starting with losses
between 5.6 and 5.8. However, all models converge at slightly
different levels. Models trained with a Java speciﬁc tokenizer
converge at levels of 0.42 for BERT Java-8k, 0.46 for BERT
Java-16k, and 0.5 for BERT Java-32k. The BERT Base model
with an uncased tokenizer shows a better training outcome

Fig. 3. Excerpt of the calculated loss throughout the training process.

and converges at 0.36. With a ﬁnal loss at 0.29, the BERT
Base model with a case-sensitive tokenizer shows the most
promising results of all models.

VI. EVALUATION

For the evaluation of all models, we use the test split of
our data set, as benchmarking data sets for MLM on Java
are not available yet. To measure the prediction accuracy of
the models, we employ the Word Error Rate (WER) metric,
but adapt is to the MLM task. WER is mostly used on large
text parts such as in translation tasks and measures the rate
of correctly predicted words [9]. As only a certain amount of
tokens are masked in MLM, the remaining unmasked words
can be ignored in our calculation. Including these words in
the metric would always imply perfect matches and would
therefore skew the results. To circumvent this, we simplify the
WER calculation and only consider the masked words with:

R =

C
N

The value N depicts the number of masked tokens and C
is the number of correctly predicted tokens. R is the rate of
correctly predicted tokens. For C we follow two approaches
to count the correct number of tokens. With 1-Word-Match
we consider a prediction correct, if the prediction with the
highest score matches the original token. The 3-Words-Match
considers a prediction correct, if one of the ﬁrst three pre-
dictions sorted by highest scores matches the original token.
We ran the test data set on all models and aggregated the
results by calculating the mean values over all batch results.
Due to the similar architecture and MLM training task, we
included CodeBERT in the evaluation. Besides its architecture,
CodeBERT differs from our approaches, as it is mainly trained
upon multiple programming languages with a smaller set of
Java ﬁles compared to our data set [4].

Table I shows the results as percentage values, showing the
accuracy of the predictions. Regarding our approach, we see
that the BERT Java-8k, trained on the smallest vocabulary
outperforms the other two Java-speciﬁc models. This meets

Model
BERT Java-8k
BERT Java-16k
BERT Java-32k
BERT Base Uncased
BERT Base Cased
CodeBERT

1-Word-Match
91.8%
91.1%
90.9%
92.9%
94.4%
75.2%

3-Words-Match
95.0%
94.3%
94.0%
95.7%
96.6%
81.6%

TABLE I
SCORES FOR EACH ML MODEL

the observations made on the loss values while training. BERT
Java-8k is able to correctly predict 91.8% of the masked tokens
on ﬁrst try, whereas both bigger models perform slightly worse
with 91.1% and 90.9%. The same observation can be made
on the 3-Words-Match score. However, training BERT with
a vocabulary intended for natural language outperforms the
models with a Java-speciﬁc vocabulary on both scores with
up to 94.4% on the 1-Word-Match. A closer inspection shows
that WordPiece differentiates syntactical features in the same
way as our approach. However, its vocabulary contains several
variants of these tokens which lead to the assumptions that
certain sequences may return these non-exclusive variants.
As we were not able to observe this, we argue that
the
increased performance is mainly achieved by a more efﬁcient
tokenization of the literals and identiﬁers. We also argue, that
the naming convention within Java accounts for the increased
performance of the BERT Base Cased variant in comparison
with BERT Base Uncased, which allows the model to better
distinguish between different
types of entities. CodeBERT
shows a signiﬁcantly lower scoring performance in comparison
to all other models. This may be explained due to amount of
ﬁles upon CodeBERT was trained, as well due to its training
task upon multiple languages. As a result, CodeBERT is not
able to predict tokens in Java as reliable as dedicated models.

Model
BERT Java-8k
BERT Java-16k
BERT Java-32k
BERT Base Uncased
BERT Base Cased
CodeBERT

# Parameters
92.19 M
98.35 M
110.65 M
109.51 M
109.51 M
124.70 M

TABLE II
NUMBER OF PARAMETERS

We observed different memory consumption and execution
times between the models. Table II shows the number of
parameters for each model. The increased memory consump-
tion and execution times correlate with the parameter sizes,
therefore underlining the assumption of the model size being
the main contributor for this. However, due to the increased
memory footprint, we needed to change the test batch sizes
depending on the model, which may also show impact on
the execution times. These aspects even render the bigger
models using the Java-speciﬁc tokenizer more obsolete, as
the increased execution times and memory consumption do

not come with greater performance. Comparing BERT Java-
8k with BERT Base Uncased shows a different picture. Both
show similar performance, however they differ about 15.8%
in size. Therefore, although a Java-speciﬁc pre-tokenization
did not show a better performance,
it does come with a
reduced memory footprint. Memory sensitive application may
therefore proﬁt from such approach. Nonetheless, BERT Base
Cased shows a better performance with the same number of
parameters as BERT Base Uncased making it the obvious
choice when considering performance.

With these observations, we can answer our research ques-

tion stated in Section III.

RQ1: We have introduced an approach on data retrieval
for certain programming languages, which in our case is
Java. The resulting performance of our trained models in
comparison to a already pretrained model shows that our
retrieval pipeline is able to deliver data for a state-of-the-
art model. Furthermore, as the pipeline is language-agnostic,
we consider it also suitable for the code retrieval of other
programming languages.

RQ2: We have investigated two different approaches to
train a model upon software code. An approach leveraging
syntactical features of the Java language did not outperform
the model using a vocabulary intended for natural language.
Therefore, we argue a tokenizer for NL performs well enough
that a further optimization on this step may be neglected.
Comparing models trained solely for Java also shows a better
performance than a model trained on several programming
languages, showing the advantage of training solely on a single
language. Due to its performance, we choose the model using
the BERT Base Cased tokenizer as our ﬁnal JavaBERT model.
RQ3: We employed a adapted WER metric to measure
the performance of all trained models. Our evaluation shows
an accuracy of up to 94.4% for prediction of masked tokens
on the ﬁrst try. This accuracy value can be considered high
enough to further use this model on SE related tasks.

A. Threats to Validity

The observed outcome of our experiment depends heavily
on the used data and parameters. Deviating from the used data
may show different results in favor of other models. We tried
to circumvent this threat by using a broad set of data and by
applying the same data to all models. Also, we assume that
GitHub provides data suitable for training tasks as used in
our case. The used hyperparameters are also a factor which
impact the performance of each model. Setting different values
to these parameters may show different outcomes, which we
avoided by using the same settings for all models, considering
it to increase comparability. Furthermore, we just evaluated a
limited set of models, which may not be the best options for
our use case. Additionally, we assume the WER metric to be
suitable for our evaluation and its scores to be valid indicator
for the performance of each models.

VII. FUTURE WORK
Future work based on our results can be divided into two

categories.

First, our model may beneﬁt from optimization approaches
also used by the original BERT model. Lan et al. [6] optimized
the original BERT model by applying techniques which effec-
tively reduce the number of parameters within the model. This
reduces the memory footprint and accelerates the execution of
the model. Different model architectures may also be taken
into consideration. For instance, GPT2 and its causal language
modeling introduce a different architecture and also a different
generic training task, showing comparable performance[11]. In
addition, a hyperparameter search was not conducted in this
work, which may also optimize the performance of our model.
Second, we will focus on ﬁnetuning our model for more
complex downstream tasks. This may include the detection
of malicious code, duplicate code detection, or comment
generation. Such tasks will require multimodal data retrieval
approaches also showing the potential for future work upon the
retrieval pipeline. This will eventually allow us to introduce
this model to the domain of SE.

VIII. CONCLUSION

Pretrained models for software code understanding are still
scarce, although their uses in the domain of SE are mani-
fold. We therefore presented a concept on how to pretrain a
model for a single programming language. A data retrieval
pipeline was used to showcase an approach on which syn-
tactical features of programming languages are used within
the pretraining process of multiple BERT models. The same
pipeline allowed us to train distinct models not relying on this
technique and using a generic vocabulary instead. While out-
performing the language-speciﬁc models regarding accuracy
on predictions for an MLM task, the latter vocabulary shows
disadvantages in memory consumption and execution time.
We argue that the performance should be considered as the
more relevant topic, making the model using a case-sensitive
NL tokenizer the ﬁrst choice for any tasks performed on Java
software code. All models pretrained on our collected data set
outperform a model trained with a similar objective, showing
state-of-the-art performance. We therefore consider this model
suitable for further research toward ML-based tooling for SE.
To encourage this, we uploaded the JavaBERT model to the
Hugging Face Hub6 and the code for our retrieval pipeline to
GitHub7.

REFERENCES

[1] Alfred Aho. Compilers : principles,

techniques, & tools.

Pearson/Addison Wesley, Boston, 2007. ISBN 0321486811.
[2] Pavol Bielik, Veselin Raychev, and Martin Vechev. Learning
In International Conference on

a static analyzer from data.
Computer Aided Veriﬁcation, pages 233–253. Springer, 2017.

[3] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: pre-training of deep bidirectional transform-
ers for language understanding. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Association for
Computational Linguistics, pages 4171–4186. Association for
Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423.
URL https://doi.org/10.18653/v1/n19-1423.

6https://huggingface.co/CAUKiel/JavaBERT
7https://github.com/cau-se/gh-archive-code-retrieval

[4] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng
Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin
Jiang, and Ming Zhou. Codebert: A pre-trained model for pro-
gramming and natural languages. arXiv:2002.08155v4, 2020.
URL http://arxiv.org/abs/2002.08155v4.

[5] Xuan Huo and Ming Li. Enhancing the uniﬁed features to
locate buggy ﬁles by exploiting the sequential nature of source
In Proceedings of the Twenty-Sixth International Joint
code.
Conference on Artiﬁcial Intelligence, aug 2017. doi: 10.24963/
ijcai.2017/265. URL https://doi.org/10.24963/ijcai.2017/265.
[6] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin
Gimpel, Piyush Sharma, and Radu Soricut. ALBERT: A Lite
BERT for Self-supervised Learning of Language Representa-
tions. arXiv:1909.11942, Feb 2020. URL http://arxiv.org/abs/
1909.11942.

[7] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and
Veselin Stoyanov. RoBERTa: A Robustly Optimized BERT
arXiv:1907.11692, Jul 2019. URL
Pretraining Approach.
http://arxiv.org/abs/1907.11692.

[8] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv:1711.05101, Jan 2019. URL http://arxiv.
org/abs/1711.05101.

[9] Andrew Cameron Morris, Viktoria Maier, and Phil Green. From
WER and RIL to MER and WIL: improved evaluation measures
In Eighth International
for connected speech recognition.
Conference on Spoken Language Processing, 2004.

[10] Sebastian C. M¨uller and Thomas Fritz. Using (bio)metrics
In Proceedings of the 38th
to predict code quality online.
International Conference on Software Engineering - ICSE '16.
ACM Press, 2016. doi: 10.1145/2884781.2884803.

[11] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
Amodei, Ilya Sutskever, et al. Language models are unsuper-
vised multitask learners. OpenAI blog, 1(8):9, 2019.

[12] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy
Liang. Squad: 100,000+ questions for machine comprehension
of text. arXiv:1606.05250v3, 2016. URL http://arxiv.org/abs/
1606.05250v3.

[13] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neu-
ral machine translation of rare words with subword units.
arXiv:1508.07909, Jun 2016. URL http://arxiv.org/abs/1508.
07909.

[14] Rosalia Tufano, Luca Pascarella, Michele Tufano, Denys Poshy-
vanyk, and Gabriele Bavota. Towards automating code review
In 2021 IEEE/ACM 43rd International Conference
activities.
on Software Engineering (ICSE), pages 163–174. IEEE, May
2021. doi: 10.1109/icse43902.2021.00027. URL https://doi.
org/10.1109/icse43902.2021.00027.

[15] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer
Levy, and Samuel R. Bowman. Glue: A multi-task bench-
mark and analysis platform for natural language understand-
ing. arXiv:1804.07461v3, 2018. URL http://arxiv.org/abs/1804.
07461v3.

[16] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-
mond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim
Rault, R´emi Louf, Morgan Funtowicz, and et al. Hugging-
face’s transformers: State-of-the-art natural language process-
ing. arXiv:1910.03771, Jul 2020. URL http://arxiv.org/abs/
1910.03771.

[17] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mo-
hammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan
Cao, Qin Gao, Klaus Macherey, and et al. Google’s neural
machine translation system: Bridging the gap between human
and machine translation. arXiv:1609.08144, Oct 2016. URL
http://arxiv.org/abs/1609.08144.

