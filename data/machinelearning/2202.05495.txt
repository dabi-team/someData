2
2
0
2

b
e
F
1
1

]
E
M

.
t
a
t
s
[

1
v
5
9
4
5
0
.
2
0
2
2
:
v
i
X
r
a

INFERENCE FOR PROJECTION-BASED WASSERSTEIN DISTANCES
ON FINITE SPACES

RYO OKANO1 AND MASAAKI IMAIZUMI1,2

1The University of Tokyo / 2RIKEN Center for Advanced Intelligence Project

Abstract. The Wasserstein distance is a distance between two probability distributions
and has recently gained increasing popularity in statistics and machine learning, owing to
its attractive properties. One important approach to extending this distance is using low-
dimensional projections of distributions to avoid a high computational cost and the curse of
dimensionality in empirical estimation, such as the sliced Wasserstein or max-sliced Wasser-
stein distances. Despite their practical success in machine learning tasks, the availability of
statistical inferences for projection-based Wasserstein distances is limited owing to the lack
of distributional limit results. In this paper, we consider distances deﬁned by integrating
or maximizing Wasserstein distances between low-dimensional projections of two probabil-
ity distributions. Then we derive limit distributions regarding these distances when the
two distributions are supported on ﬁnite points. We also propose a bootstrap procedure
to estimate quantiles of limit distributions from data. This facilitates asymptotically exact
interval estimation and hypothesis testing for these distances. Our theoretical results are
based on the arguments of Sommerfeld and Munk (2018) for deriving distributional limits
regarding the original Wasserstein distance on ﬁnite spaces and the theory of sensitivity
analysis in nonlinear programming. Finally, we conduct numerical experiments to illustrate
the theoretical results and demonstrate the applicability of our inferential methods to real
data analysis.

1. Introduction

The Wasserstein distance is a distance between two probability distributions, having at-
tracted considerable interest in the statistics and machine learning literature [45, 30, 32].
This distance is based on the optimal transport problem and measures the amount of work
required to transform one distribution into another. Speciﬁcally, given two probability distri-
butions P and Q with ﬁnite p ≥ 1 moments and support in X ⊂ Rd, d ≥ 1, the p-Wasserstein
distance between P and Q is deﬁned as

(cid:18)

Wp(P, Q) =

inf
π∈Π(P,Q)

(cid:107)x − y(cid:107)pdπ(x, y)

(cid:19)1/p

(cid:90)

X ×X

,

(1)

where Π(P, Q) is the set of joint probability distributions whose respective marginals coincide
with P and Q, known as couplings. Compared to other measures of distribution closeness,
1

 
 
 
 
 
 
such as the Kullback–Leibler divergence or the total variation distance, the Wasserstein dis-
tance has two main advantages: (i) it is sensitive to the underlying geometry of distribution
support and (ii) it does not assume the absolute continuity of distributions with respect
to the other. Owing to these advantages, it has recently been used as an attractive data
analytical tool, particularly in computer vision [36, 40, 37] and natural language processing
[20, 47].

Recently, various extensions of the original Wasserstein distance have been proposed to
address its shortcomings, mainly the high computational cost and the curse of dimensionality
in empirical estimation [32, 46]. One important approach is using low-dimensional projec-
tions of distributions, that is, computing the Wasserstein distances between low-dimensional
projections of distributions P and Q instead of dealing with the original ones. The most
representative example of this approach is the sliced Wasserstein distance [33, 3], which
averages the Wasserstein distances between the random one-dimensional projections of dis-
tributions P and Q. As the Wasserstein distance between univariate distributions is known
to be easily computed, the sliced Wasserstein distance is an easily computable variant. An-
other example is the max-sliced Wasserstein distance [9], which maximizes the Wasserstein
distances between random one-dimensional projections and has a computational advantage.
By considering k-dimensional projections (1 ≤ k ≤ d), the max-sliced Wasserstein distance
is generalized to the projection robust Wasserstein (PRW) distance [31, 29]. The PRW dis-
tance eﬀectively captures the diﬀerence between two distributions if they only diﬀer in a
low-dimensional subspace and solves the curse of dimensionality in estimation [29, 23]. Sev-
eral recent studies have shown that these proposals are practical for many machine learning
tasks [22, 19, 18, 4, 24].

The development of inferential tools (e.g., interval estimation or hypothesis testing) for
the Wasserstein distance and its extensions has become an active research area in statistics.
Many studies have derived limit distributions for these distances as a basis for inferential
procedures. For example, the limit distributions of the empirical Wasserstein distance are
studied when distributions P and Q are supported in R [27, 12, 6, 34] and when they are
supported at ﬁnite or countable points [41, 42]. The limit distributions of the empirical regu-
larized optimal transport distance in ﬁnite spaces, which is an easily computable extension of
the Wasserstein distance, have been derived by [2, 16]. However, for projection-based exten-
sions of the Wasserstein distance, such distributional limit results are not well established,
which hinders their inference. For more details on related works, see Section 1.1.

In this study, we propose inferential procedures for projection-based Wasserstein distances
when distributions P and Q are supported on ﬁnite points. We consider two types of dis-
tances: (i) the integral projection robust Wasserstein (IPRW) distance, which is deﬁned
by integrating Wasserstein distances between k-dimensional projections of distributions P
and Q (1 ≤ k ≤ d) and includes the sliced Wasserstein distance as a special case, and (ii)

2

the PRW distance we introduced above. As a ﬁrst contribution, we derive limit distribu-
tions of the empirical IPRW and PRW distances with entropic regularization. Second, we
show the consistency of the rescaled bootstrap (or the m-out-n bootstrap), which enables
us to estimate quantiles of the limit distributions from data. Consequently, we construct
asymptotically exact conﬁdence intervals for these two distances, and obtain new statistics
for testing the equality of the distributions. Finally, we conduct numerical experiments to
verify our theoretical results and apply our inferential methods to real data analysis.

We derive our distributional limits by showing the directional Hadamard diﬀerentiability
of the IPRW and PRW distances and applying a reﬁned delta method. This strategy was
developed by [41] for the inference of the original Wasserstein distance in ﬁnite spaces. To
implement this strategy for the PRW distance, we use the following two key techniques. First,
we utilize sensitivity analysis in nonlinear programming, which investigates how the optimal
value of an optimization problem changes when the objective function and the constraints
are changed [11]. We regard the PRW distance between distributions P and Q as the optimal
value of a parametric optimization problem with parameters P and Q, and apply the result
of the sensitivity analysis to show its directional diﬀerentiability. Second, we introduce an
entropic regularization term in the PRW distance to avoid including a non-smooth objective
function in its deﬁnition, which helps show its directional diﬀerentiability. The idea of adding
an entropic regularization term to the PRW distance was proposed by [22], and we call this
quantity the regularized PRW distance.

We can summarize the contributions of this paper as follows:

• We derive limit distributions of the empirical versions of the IPRW and regularized

PRW distances when distributions P and Q are supported on ﬁnite points.

• We show the consistency of the rescaled bootstrap for the IPRW and PRW distances,
which enables us to estimate quantiles of the limit distributions from data. This
facilitates asymptotically exact interval estimations and hypothesis testing for these
distances.

• We conduct numerical experiments to illustrate our theoretical results, and show

the applicability of our inferential methods to real data analysis.

1.1. Related work. There are several extensions of the Wasserstein distance based on low-
dimensional projections, in addition to the distances we consider, such as the generalized
sliced [17], tree-sliced [21], and distributional sliced [28] Wasserstein distances. Beyond the
projection-based approach, [5] proposed the entropic regularization of optimal transport,
which can be eﬃciently computed through an iterative method, called the Sinkhorn algo-
rithm. Further, [13] proposed the smooth Wasserstein distance, which avoids the curse of
dimensionality in estimation by smoothing out local irregularities in distributions P and Q
via convolution with a Gaussian kernel.

3

Statistical inference for the Wasserstein distance and its extensions has been studied in
several settings, based on their limit distributions. When distributions P and Q are sup-
ported in R, the Wasserstein distance between them has a closed form and is described as
the Lp norm of the quantile functions of P and Q. Using this fact, [27, 12, 7, 34] derived the
limit distributions of the empirical Wasserstein distances in the univariate case and studied
the validity of the bootstrap. The inference for the Wasserstein distance over ﬁnite spaces
was studied by [41], and the result was extended to a case with countable spaces by [42].
[2, 16] considered inference for the entropic regularized optimal transport distance on ﬁ-
nite spaces.
In a general setting, [8] established central limit theorems for the empirical
Wasserstein distance and [26] established similar results for the entropic regularized optimal
transport distance; however, these results contain unknown centering constants that hinder
their use for statistical inference.

To the best of our knowledge, statistical inference for projection-based Wasserstein dis-
tances has only been considered in one study. Speciﬁcally, [25] proposed conﬁdence intervals
with ﬁnite-sample validity for the sliced Wasserstein distance and showed their minimax op-
timality in length. Owing to the closed-form expression of the one-dimensional Wasserstein
distance, their inference method is valid, without imposing strong assumptions on distri-
butions P and Q such as discreteness. However, their approach is not applicable when the
projection dimension is greater than one. By contrast, our work covers Wasserstein distances
based on projections to dimensions greater than one.

1.2. Notation. (cid:107) · (cid:107) and (cid:104)·(cid:105) denote the Euclidean norm and inner product, respectively.
R>0 is the positive real and R≥0 the non-negative real. ⊗ is the Kronecker product. For any
a, b ∈ R, a ∧ b denotes the minima of a and b. For 1 ≤ k ≤ d, the set of d × k matrices
with orthonormal columns is denoted as Sd,k = {E ∈ Rd×k : E(cid:62)E = Ik}. Note that, when
k = 1, Sd,k coincides with the d-dimensional unit ball, Sd−1 = {x ∈ Rd : (cid:107)x(cid:107) = 1}. Given
a map T : Rd → R and Borel probability measure P supported in Rd, T#P denotes the
pushforward of P under T , deﬁned by T#P (B) = P (T −1(B)) for all Borel sets B ⊂ Rd.
For any set A ⊂ Rd, its diameter is denoted by diam(A) = sup{(cid:107)x − y(cid:107) : x, y ∈ A}.
d→ denotes convergence in distribution of random
P(Rn) denotes the set of all subsets of Rn.
variables and d= denotes distributional equality of the random variables.

2. Background

Here, we provide background details on the Wasserstein distance and its projection-based

extensions in ﬁnite spaces.

2.1. Wasserstein distance and entropic regularization.

4

2.1.1. Wasserstein distance. In this study, we restrict support X = {x1, ..., xN } ⊂ Rd to
a ﬁnite set of size N ∈ N. Every probability measure on X is represented as an element
in an (N − 1)-dimensional sphere ∆N = {r ∈ RN
i=1 ri = 1}; hence, we do not
distinguish vector r ∈ ∆N and its corresponding probability distribution. Given support
X = {x1, ..., xN } and order p ≥ 1, we deﬁne cost vector cp(X ) ∈ RN 2 as cp(X )(i−1)N +j =
(cid:107)xi − xj(cid:107)p for 1 ≤ i, j ≤ N , representing the transport cost from xi to xj. The p-Wasserstein
distance between the two distributions r, s ∈ ∆N on X ⊂ Rd is given by

>0 : (cid:80)N

(cid:26)

(cid:27)1/p

Wp(r, s; X ) =

min
π∈Π(r,s)

(cid:104)cp(X ), π(cid:105)

,

(2)

where Π(r, s) is a set of vectors of length N 2 that represent the couplings of r and s. Formally,
Π(r, s) is deﬁned as

Π(r, s) =

π ∈ RN 2 : Aπ =

,

(3)

(cid:40)

(cid:32)

(cid:33)(cid:41)

r
s

where A is a coeﬃcient matrix:

(cid:32)

A =

IN ×N ⊗ 11×N
11×N ⊗ IN ×N

(cid:33)

∈ R2N ×N 2.

Constraint Aπ = (r, s)(cid:62) ensures that π satisﬁes the marginal constraints: a matrix (cid:101)π ∈
RN ×N , generated by π as (cid:101)πi,j = π(i−1)N +j, satisﬁes (cid:80)N
j=1 (cid:101)πi,j = ri for 1 ≤ i ≤ N , and
(cid:80)N

i=1 (cid:101)πi,j = sj for 1 ≤ j ≤ N .

2.1.2. Entropic regularization. The entropic regularization is a typical extension of the Wasser-
stein distance [5]. Given p ≥ 1, distributions r, s ∈ ∆N , and a regularization parameter
λ > 0, we consider an entropic regularized optimal transport problem as follows:

where ϕ : RN 2 → R is the negative Boltzmann-Shannon entropy, deﬁned as

min
π∈Π(r,s)

(cid:104)cp(X ), π(cid:105) + λϕ(π),

ϕ(π) =






(cid:80)N 2

i=1 πi log(πi) − πi + 1 if π ∈ RN 2
≥0 ,
otherwise.

+∞

(4)

(5)

Here, we set 0 log(0) = 0. Because problem (4) is a strictly convex optimization problem,
it has a unique optimal solution. We refer to the solution of (4) as the regularized opti-
mal transport plan πp,λ(r, s; X ). Using this notion, we can deﬁne the p-regularized optimal
transport distance (or the p-Sinkhorn divergence) between r, s ∈ ∆N as

Wp,λ(r, s; X ) = (cid:104)cp(X ), πp,λ(r, s; X )(cid:105)1/p.

(6)

Several computational advantages and statistical properties of the regularized optimal trans-
port distance have been studied (e.g., see [5, 32, 16, 2]).

5

2.2. Projection-based Wasserstein distances. We introduce extensions of the Wasser-
stein distance based on low-dimensional projections of the distributions. Fix k ≤ d and let
πE : x ∈ Rd (cid:55)→ E(cid:62)x for E ∈ Sd,k. For distribution P on Rd, the k-dimensional projection of
P in E ∈ Sd,k is deﬁned by PE = πE #P . That is, PE is the distribution of E(cid:62)X for X ∼ P .

2.2.1. Integral projection robust Wasserstein distance. We study k-dimensional projections
of distributions r, s ∈ ∆N on a ﬁnite X = {x1, ..., xN } ⊂ Rd. The Wasserstein distance
between the projections of the distributions r and s in direction E ∈ Sd,k is represented
by Wp(r, s; XE), where XE = {E(cid:62)x1, ..., E(cid:62)xN } ⊂ Rk. The p-integral projection robust
Wasserstein (IPRW) distance [23] is deﬁned as an integral of the Wasserstein distances over
the direction E, that is,

IWp(r, s) =

(cid:32)(cid:90)

Sd,k

(cid:33)1/p

W p

p (r, s; XE)dµ(E)

,

(7)

where µ is a given measure on Sd,k.
[23] shows that the IPRW distance with the uniform
measure on Sd,k solves the curse of dimensionality in estimation. When the projection
dimension is k = 1 and µ is a uniform measure of Sd,1, which coincides with the d-dimensional
unit ball Sd−1, the IPRW distance corresponds to the sliced Wasserstein distance [33, 3]. The
sliced Wasserstein distance has the advantage of being easy to calculate, due to the fact that
the Wasserstein distance between one-dimensional distributions is easy to compute.

2.2.2. Projection robust Wasserstein distance. The p-projection robust Wasserstein (PRW)
distance [31] is deﬁned as the maximum Wasserstein distance between k-dimensional projec-
tions of r, s ∈ ∆N over direction E ∈ Sd,k, namely

PWp(r, s) = max
E∈Sd,k

Wp(r, s; XE).

(8)

When k = 1, the PRW distance corresponds to the max-sliced Wasserstein distance [9]. The
PRW distance eﬀectively captures the diﬀerence between the two distributions r, s if they
diﬀer only in a low-dimensional subspace, and [29, 23] showed that it solves the curse of
dimensionality in estimation.

We further introduce the entropic regularization for the PRW distance. With a ﬁxed reg-
ularization parameter λ > 0 and projection direction E ∈ Sd,k, we represent the regularized
optimal transport distance between the projections of r and s as Wp,λ(r, s; XE). Then, the
p-regularized PRW distance is deﬁned by

PWp,λ(r, s) = max
E∈Sd,k

Wp,λ(r, s; XE).

(9)

This method with entropy regularization has the advantage of reducing the computational
cost, owing to smoothing out the non-smoothness due to maximization [22].

6

3. Distributional limits

We study distributional limits of the empirical version of the IPRW and regularized PRW
distances on a ﬁnite space. Speciﬁcally, we consider the following setting. For probability
distributions r, s ∈ ∆N on X = {x1, ..., xN } ⊂ Rd and sample sizes n and m, let X1, ..., Xn ∼
r, Y1, ..., Ym ∼ s be independent and identically distributed (i.i.d.) samples. Then, we deﬁne
their corresponding empirical distributions (cid:98)rn, (cid:98)sm ∈ ∆N , whose ith elements are given as

(cid:98)rn,i =

#{k : Xk = xi}
n

,

(cid:98)sm,i =

#{k : Yk = xi}
m

,

for 1 ≤ i ≤ N . Given order p ≥ 1 and regularization parameter λ > 0, we derive the
distributions to which

(cid:114) nm
n + m

{IWp((cid:98)rn, (cid:98)sm) − IWp(r, s)}

and

(cid:114) nm
n + m

{PWp,λ((cid:98)rn, (cid:98)sm) − PWp,λ(r, s)}

converge in law as n, m → ∞.

3.1. Outline and preparation. We derive distributional limits using the delta method,
which is based on the diﬀerentiability of the IPRW and regularized PRW distances. Specif-
ically, following that (cid:112) nm
n+m {((cid:98)rn, (cid:98)sm) − (r, s)} converges to a Gaussian random vector by
the central limit theorem, we can derive distributional limits by applying the delta method
with the maps (r, s) (cid:55)→ IWp(r, s) and (r, s) (cid:55)→ PWp,λ(r, s). To use the delta method in this
setting, we consider the directional Hadamard diﬀerentiability, which is deﬁned as follows.

Deﬁnition 1 (Directional Hadamard diﬀerentiability [35, 41]). A function f : Df ⊂ Rd → R
is directionally Hadamard diﬀerentiable at u ∈ Df tangentially to D0 ⊂ Rd, if there exists a
map f (cid:48)

u : D0 → R so that

lim
n→∞

f (u + tnhn) − f (u)
tn

= f (cid:48)

u(h),

(10)

for any h ∈ D0 and arbitrary sequences {tn} ⊂ R and {hn} ⊂ Rd so that tn (cid:38) 0, hn → h,
and u + tnhn ∈ Df for all large n ∈ N. We refer f (cid:48)
u to the directional Hadamard derivative.

In contrast to the usual (non-directional) Hadamard diﬀerentiability (e.g., [44]), directional
Hadamard diﬀerentiability does not require the derivative to be linear, but allows for the
Delta method.

Theorem 1 (Delta method with a directionally Hadamard diﬀerentiable map: Theorem 1 in
[35] and Theorem 3 in [41]). Let f : Df ⊂ Rd → R be directionally Hadamard diﬀerentiable
u : D0 → R. Let Tn be Rd-valued random
at u ∈ Df tangentially to D0 ⊂ Rd with derivative f (cid:48)
variables, so that ρn(Tn − u) d→ T for a sequence of numbers ρn → ∞, and a random variable
T taking its values in D0. Then, ρn(f (Tn) − f (u)) d→ f (cid:48)

u(T ).

7

Our approach using the directional Hadamard derivative is important when dealing with
the projection-based Wasserstein distances. These distances are not diﬀerentiable in the
sense of (non-directional) Hadamard diﬀerentiation, but will be shown to have a directional
Hadamard derivative, which makes it possible to apply the delta method.

3.2. Distributional limit for IPRW distance. As our ﬁrst main result, we derive a
distributional limit of the empirical IPRW distance, IWp((cid:98)rn, (cid:98)sm). To this end, we ﬁrst
show the directional Hadamard diﬀerentiability of the map (r, s) (cid:55)→ IWp
p(r, s) and derive its
derivative. In preparation, we deﬁne sets of dual solutions for the optimization problem in
(2). Following [41], given two distributions r, s ∈ ∆N and a ground space X = {x1, ..., xN },
we deﬁne

Φ∗

p(X ) = {u ∈ RN : ui − uj ≤ (cid:107)xi − xj(cid:107)p, 1 ≤ i, j ≤ N }

and

Φ∗

p(r, s; X ) = {(u, v) ∈ RN × RN : (cid:104)u, r(cid:105) + (cid:104)v, s(cid:105) = W p

p (r, s; X ),

ui + vj ≤ (cid:107)xi − xj(cid:107)p, 1 ≤ i, j ≤ N }.

(11)

(12)

These sets play a role in describing a limit distribution. Additionally, we deﬁne a set of
directions in which the limits are considered as ΩN = {h ∈ RN : (cid:80)N
i=1 hi = 0}. Then, we
achieve the following result on diﬀerentiability.

Proposition 1 (Directional Hadamard diﬀerentiability of IWp
∆N → R, (r, s) (cid:55)→ IWp
tangentially to ΩN × ΩN with derivative:

p : ∆N ×
p(r, s) is directional Hadamard diﬀerentiable at all (r, s) ∈ ∆N × ∆N

p). The map IWp

(h1, h2) (cid:55)→

(cid:90)

Sd,k

max

(cid:104)u, h1(cid:105) + (cid:104)v, h2(cid:105)dµ(E).

(u,v)∈Φ∗

p(r,s;XE )

(13)

Proof. Let {h1(cid:96)}, {h2(cid:96)} ⊂ ΩN be sequences satisfying h1(cid:96) → h1, h2(cid:96) → h2 and let t(cid:96) (cid:38) 0 as
(cid:96) → ∞. Following the deﬁnition of the directional Hadamard derivative, we consider the
following diﬀerence:

IWp

p(r + t(cid:96)h1(cid:96), s + t(cid:96)h2(cid:96)) − IWp
t(cid:96)

p(r, s)

.

(cid:90)

=

Sd,k

W p

p (r + t(cid:96)h1(cid:96), s + t(cid:96)h2(cid:96); XE) − W p

p (r, s; XE)

t(cid:96)

and consider its limit. For each E ∈ Sd,k, Theorem 4 in [41] implies

dµ(E),

(14)

W p

p (r + t(cid:96)h1(cid:96), s + t(cid:96)h2(cid:96); XE) − W p

p (r, s; XE)

t(cid:96)

8

→

max

(u,v)∈Φ∗

p(r,s;XE )

(cid:104)u, h1(cid:105) + (cid:104)v, h2(cid:105),

as (cid:96) → ∞. Furthermore, the Lipschitz continuity of the Wasserstein distance (Theorem 4 of
[41]) implies

(cid:12)
(cid:12)
(cid:12)
(cid:12)

W p

p (r + t(cid:96)h1(cid:96), s + t(cid:96)h2(cid:96); XE) − W p

p (r, s; XE)

t(cid:96)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

pdiam(XE)p(cid:107)t(cid:96)(h1(cid:96), h2(cid:96))(cid:107)
t(cid:96)
≤ pkpdiam(X )p(cid:107)(h1(cid:96), h2(cid:96))(cid:107).

The last inequality follows diam(XE) ≤ kdiam(X ), which follows

(cid:107)E(cid:62)x(cid:107) ≤ |E(cid:62)

1 x| + · · · + |E(cid:62)

k x| ≤ (cid:107)E(cid:62)

1 (cid:107)(cid:107)x(cid:107) + · · · + (cid:107)E(cid:62)

k (cid:107)(cid:107)x(cid:107) = k(cid:107)x(cid:107)

for E = (E1, ..., Ek) ∈ Sd,k and x ∈ Rd. Because X is ﬁnite and {h1(cid:96)} and {h2(cid:96)} are
convergent sequences, pkpdiam(X )p(cid:107)(h1(cid:96), h2(cid:96))(cid:107) is bounded by a constant not depending on
E and (cid:96). Therefore, by taking (cid:96) → ∞ in (14), we can apply the dominated convergence
(cid:3)
theorem, and the claim then holds.

We state our main result on a limit distribution of the empirical IPRW distance. This
derivation is based on the diﬀerentiability in Proposition 1 and the delta method in Theorem
1. For r ∈ ∆N , we deﬁne the covariance matrix as

Σ(r) =









r1(1 − r1)
−r2r1
...
−rN r1

−r1r2
r2(1 − r2)
...
−rN r2

· · ·
· · ·
. . .
· · ·

−r1rN
−r2rN
...
rN (1 − rN )









.

(15)

Then, we obtain the following result.

Theorem 2 (Distributional limits of IWp((cid:98)rn, (cid:98)sm)). Let r, s ∈ ∆N be two probability dis-
tributions supported on X ⊂ Rd, let X1, ..., Xn ∼ r, Y1, ..., Ym ∼ s be i.i.d. n and m sam-
ples, and let (cid:98)rn, (cid:98)sm be the corresponding empirical distributions. Let G ∼ N (0, Σ(r)) and
H ∼ N (0, Σ(s)) be independent Gaussian random vectors. Then, we have the followings:

(i) If r = s, and n ∧ m → ∞ and m/(n + m) → δ ∈ (0, 1), we have

(cid:19) 1

2p

(cid:18) nm
n + m

IWp((cid:98)rn, (cid:98)sm) d→

(cid:32)(cid:90)

Sd,k

(cid:33)1/p

max

(cid:104)G, u(cid:105)dµ(E)

u∈Φ∗

p(XE )

,

where Φ∗

p(XE) is given by (11).

(ii) If r (cid:54)= s, and n ∧ m → ∞ and m/(n + m) → δ ∈ (0, 1), we have

(cid:114) nm
n + m

{IWp((cid:98)rn, (cid:98)sm) − IWp(r, s)}

d→

1
p

IW1−p
p

(r, s)

(cid:90)

Sd,k

max

(u,v)∈Φ∗

p(r,s;XE )

√

δ(cid:104)G, u(cid:105) +

√

1 − δ(cid:104)H, v(cid:105)dµ(E),

where Φ∗

p(r, s; XE) is given by (12).

9

Proof. Our proof follows the same line as the proof of Theorem 1 of [41]. Under the assump-
tion of the theorem, the central limit theorem implies
(cid:114) nm
n + m

√
{((cid:98)rn, (cid:98)sm) − (r, s)} d→ (

1 − δH),

δG,

√

as n ∧ m → ∞.

About (i): An application of the delta method in Theorem 1 with the directional Hadamard

derivative of the map (r, s) (cid:55)→ IWp

p(r, s) given in Proposition 1 yields

(cid:114) nm
n + m

IWp

p((cid:98)rn, (cid:98)sm) d→

(cid:90)

Sd,k

max

(u,v)∈Φ∗

p(r,s;XE )

√

δG(cid:105) + (cid:104)v,

√

1 − δH(cid:105)dµ(E).

(16)

(cid:104)u,

Note that, under r = s, we have (u, v) ∈ Φ∗(r, s; XE) if and only if u ∈ Φ∗
Therefore, with G d= H, we have

p(XE) and v = −u.

max

(u,v)∈Φ∗

p(r,s;XE )

√

δG(cid:105) + (cid:104)v,

√

(cid:104)u,

1 − δH(cid:105) d= max
(u,v)∈Φ∗

p(XE )

√

δ(cid:104)G, u(cid:105) −

√

1 − δ(cid:104)H, u(cid:105)

d= max
(u,v)∈Φ∗

p(XE )

(cid:112)δ + (1 − δ)(cid:104)G, u(cid:105)

= max
(u,v)∈Φ∗

p(XE )

(cid:104)G, u(cid:105),

(17)

for each E ∈ Sd,k. (16), (17), and the application of the continuous mapping theorem with
a map t (cid:55)→ t1/p provide the conclusion.

About (ii): Consider a map (r, s) (cid:55)→ IWp(r, s) = (IWp

p(r, s))1/p. By Proposition 1 and
the chain rule for directional Hadamard derivatives (Proposition 3.6 of [39]), the directional
Hadamard derivative of this map at (r, s) is given by

(h1, h2) (cid:55)→

1
p

IW1−p
p

(r, s)

(cid:90)

Sd,k

max

(cid:104)u, h1(cid:105) + (cid:104)v, h2(cid:105)dµ(E).

(u,v)∈Φ∗

p(r,s;XE )

An application of the delta method in Theorem 1 yields the conclusion.

(cid:3)

The scaling rate in Theorem 2 is independent of the dimension of underlying space X ,
which is the same as the other extensions of the Wasserstein distance on ﬁnite spaces [41,
16, 2]. Moreover, for p ≥ 2, the scaling rate in the case of r = s (i.e., n−1/2p) is slower than
that in the case of r (cid:54)= s (i.e., n−1/2). This implies that IWp((cid:98)rn, (cid:98)sm) converges faster under
r = s for p ≥ 2.

3.3. Distributional limit for regularized PRW distance. As our second main result,
we derive a distributional limit of the empirical regularized PRW distance, PWp,λ((cid:98)rn, (cid:98)sm). To
study the PRW distance, we need to introduce the entropic regularization to add smoothness
to the Wasserstein distance. For the regularization of the Wasserstein distance in a ﬁnite
space, we refer to [16].

10

We derive a distributional limit by showing the directional Hadamard diﬀerentiability of
the regularized PRW distance and apply the delta method. Our proof relies on the results
of the sensitivity analysis in nonlinear programming [11], which is as follows.

Let us consider the following optimization problem containing a parameter u ∈ U in the

objective function:

max
x∈Rn

f (x, u)

subject to x ∈ S,

where f : Rn × U → R, and ∇uf is continuous on Rn × U . Moreover, feasible region
S ⊂ Rn is a nonempty closed set and parameter set U ⊂ Rp is open and bounded. We
deﬁne the optimal value function φ : U → R and the optimal set mapping Φ : U → P(Rn)
as: φ(u) = max{f (x, u) : x ∈ S} and Φ(u) = {x ∈ S : φ(u) = f (x, u)}. Then, we have the
following result.

Theorem 3 (Theorem 2.3.1 in [11]). For all u ∈ U and in any direction h ∈ Rp, the optimal
value function φ is directionally diﬀerentiable in the sense of Gˆateaux; that is, limit (10)
exists for a ﬁxed h and not a sequence hn → h. Additionally, the derivative is given by

h (cid:55)→ max
x∈Φ(u)

(cid:104)∇uf (x, u), h(cid:105).

We employ this result to demonstrate the directional Hadamard diﬀerentiability of the

regularized PRW distance.

For technical reasons, we reformulate regularized optimal transport problem (4). The
transport condition in (3) can be stated in terms of only the 2N − 1 equality constraints
instead of 2N , which allows for linearly independent constraints. Following [16], we denote
by A(cid:63) and s(cid:63) the deletion of the last row of matrix A in (3) and the last entry of vector
s ∈ ∆N , respectively. We denote the set of such s(cid:63) as (∆N )(cid:63). Using constraint (cid:80)N
i=1 si = 1,
we can identify vector s ∈ ∆N with s(cid:63) ∈ (∆N )(cid:63). To apply Theorem 3 to the regularized
PRW distance, we show the continuous diﬀerentiability of the regularized optimal transport
plan with projection in the following lemma.

Lemma 1. Let p ≥ 1 be even and λ > 0. The map (r, s(cid:63), E) (cid:55)→ πp,λ(r, s(cid:63); XE) is continuously
diﬀerentiable on ∆N ×(∆N )(cid:63)×Rd×k. In addition, the matrix of partial derivatives with respect
to (r, s(cid:63)) at (r0, (s0)(cid:63), E0) is given by

∇(r,s(cid:63))πp,λ(r0, s0(cid:63); XE0) = DA(cid:62)

(cid:63) (A(cid:63)DA(cid:62)

(cid:63) )−1 ∈ RN 2×(2N −1),

where D ∈ RN 2×N 2 is a diagonal matrix whose (j, j)-entry is a j-th element of πp,λ(r0, s0(cid:63); XE0).

Proof. Our proof is similar to the proof of Theorem 2.3 in [16], which shows the continu-
ous diﬀerentiability of a regularized optimal transport plan without projection. Note that
regularized optimal transport (4) with marginal r0 and s0 satisﬁes the Slater’s constraint
11

qualiﬁcation (Proposition 26.18 in [1]). Therefore, strong duality holds and the dual prob-
lem admits an optimal solution. In addition, we can characterize the regularized optimal
transport plan πp,λ and its corresponding optimal dual solution µp,λ ∈ R2N −1 by the necessary
and suﬃcient Karush-Kuhn-Tucker conditions:

cp(XE) + λ∇φ(πp,λ)(cid:62) − A(cid:62)

(cid:63) µp,λ = 0, A(cid:63)πp,λ − (r0, s0(cid:63))(cid:62) = 0.

We now obtain the statement by applying the implicit function theorem to this system of
equations. Let us deﬁne a function F : RN 2 × R2N −1 × R2N −1 × Rd×k → RN 2+2N −1 by

F (π, µ, (r, s(cid:63)), E) =

(cid:32)

cp(XE) + λ∇φ(π)(cid:62) − A(cid:62)
A(cid:63)π − (r, s(cid:63))(cid:62)

(cid:63) µ

(cid:33)

.

Because p is even, F is continuously diﬀerentiable in the neighborhood of the speciﬁc point
(πp,λ, µp,λ, (r0, s0(cid:63)), E0) with F (πp,λ, µp,λ, (r0, s0(cid:63)), E0) = 0. The matrix of the partial deriva-
tives of F with respect to π and µ is given by

∇(π,µ)F (πp,λ, µp,λ, (r0, s0(cid:63)), E0) =

(cid:32)

λ∇2φ(πp,λ) −A(cid:62)
(cid:63)

(cid:33)

A(cid:63)

0

∈ R(N 2+2N −1)×(N 2+2N −1).

This matrix is non-singular because λ > 0, the Hessian ∇φ(πp,λ) is positive deﬁnite (Section
2.1 in [16]]) and the matrix A(cid:62)
(cid:63) has full rank. As a result, the implicit function theorem
guarantees the existence of a continuously diﬀerentiable function that parameterizes the
regularized optimal transport plan with projection in the neighborhood of (r0, s0(cid:63), E0). The
computation of the partial derivative form is directly followed by Theorem 2.3 and Example
(cid:3)
2.6 in [16].

Now, we show the directionally Hadamard diﬀerentiability of the regularized PRW dis-
p(r, s(cid:63)) as the set of directions that maximizes

tance. Given (r, s(cid:63)) ∈ ∆N ×(∆N )(cid:63), we deﬁne Ψ∗
the regularized optimal transport distance between the projections of r and s, that is,

Ψ∗

p(r, s(cid:63)) = {E ∈ Sd,k : Wp,λ(r, s; XE) = PWp,λ(r, s)}.

We denote by h(cid:63) the deletion of the last entry of vector h ∈ ΩN and the set of such h(cid:63) as
(ΩN )(cid:63).

Proposition 2. Let p ≥ 1 be even and let λ > 0. The map (r, s(cid:63)) (cid:55)→ PWp,λ(r, s(cid:63)) is
directionally Hadamard diﬀerentiable at all (r, s(cid:63)) ∈ ∆N × (∆N )(cid:63) tangentially to ΩN × (ΩN )(cid:63)
with the following derivative:

(h1, h2(cid:63)) (cid:55)→ max

E∈Ψ∗

p(r,s(cid:63))

(cid:104)γ(cid:62)DA(cid:62)

(cid:63) (A(cid:63)DA(cid:62)

(cid:63) )−1, (h1, h2(cid:63))(cid:105),

where

γ =

1
p

(cid:104)cp(XE), πp,λ(r, s(cid:63), XE)(cid:105)

1

p −1cp(XE) ∈ RN 2,

12

(18)

(19)

and D ∈ RN 2×N 2 is a diagonal matrix whose (j, j)-entry is a j-th element of πp,λ(r, s(cid:63), XE)
for j = 1, ..., N 2.

Proof. Since the regularized optimal transport distance is deﬁned as

Wp,λ(r, s(cid:63); XE) = (cid:104)cp(XE), πp,λ(r, s(cid:63); XE)(cid:105)1/p,

it follows from Lemma 1 that the map (r, s(cid:63), E) (cid:55)→ Wp,λ(r, s(cid:63), XE) can be continuously
diﬀerentiated on ∆N × (∆N )(cid:63) × Rd×k. Moreover, the matrix of partial derivatives with
respect to (r, s∗) is given by

∇(r,s(cid:63))Wp,λ(r, s(cid:63); XE) = γ(cid:62)DA(cid:62)

(cid:63) (A(cid:63)DA(cid:62)

(cid:63) )−1,

where γ is the gradient of function π (cid:55)→ (cid:104)cp(XE), π(cid:105)1/p evaluated in the regularized transport
plan πp,λ(r, s(cid:63); XE), which is formally deﬁned by (19). Consequently, Theorem 3 implies
that the map (r, s(cid:63)) (cid:55)→ PWp,λ(r, s(cid:63)) is directionally diﬀerentiable with derivative (18) in the
sense of Gˆateaux. To see this is also a directionally derivative in the Hadamard sense, it is
suﬃcient to show the local Lipschitz continuity of this map (Proposition 3.5 of [39]). To this
end, we ﬁx a closed set S0 ⊂ ∆N × (∆N )(cid:63). For any (r, s(cid:63)), (r(cid:48), s(cid:48)

(cid:63)) ∈ S0, we have

|PWp,λ(r, s(cid:63)) − PWp,λ(r(cid:48), s(cid:48)

(cid:63))| ≤ max
E∈Sd,k

|Wp,λ(r, s(cid:63); XE) − Wp,λ(r(cid:48), s(cid:48)

(cid:63); XE)|.

(20)

Because the map (r, s(cid:63), E) (cid:55)→ Wp,λ(r, s(cid:63), XE) is continuously diﬀerentiable, there exists a
constant C > 0 that does not depend on (r, s(cid:63)), (r(cid:48), s(cid:48)

(cid:63)) or E so that

|Wp,λ(r, s(cid:63); XE) − Wp,λ(r(cid:48), s(cid:48)

(cid:63); XE)| ≤ C(cid:107)(r, s(cid:63)) − (r(cid:48), s(cid:48)

(cid:63))(cid:107).

(21)

A combination of equations (20) and (21) leads to local Lipschitz continuity of the map
(cid:3)
(r, s(cid:63)) (cid:55)→ PWp,λ(r, s(cid:63)). This completes the proof.

The next theorem states our main result regarding the limit distribution of the empirically

regularized PRW distance.

Theorem 4 (Distributional limit of PWp,λ((cid:98)rn, (cid:98)sm)). Let p ≥ 1 be even and λ > 0. Under
the assumptions of Theorem 2, as n ∧ m → ∞ and m/(n + m) → δ ∈ (0, 1), we have

(cid:114) nm
n + m
d→ max
E∈Ψ∗

p(r,s(cid:63))

{PWp,λ((cid:98)rn, (cid:98)sm) − PWp,λ(r, s)}

(cid:104)γ(cid:62)DA(cid:62)

(cid:63) (A(cid:63)DA(cid:62)

(cid:63) )−1, (

√

√

1 − δH(cid:63))(cid:105),

δG,

where γ ∈ RN 2 and D ∈ RN 2×N 2 are deﬁned in Proposition 2 and H(cid:63) denotes the deletion of
the last entry of random vector H ∼ N (0, Σ(s)).

Proof. The proof is a simple application of the delta method (Theorem 1), with the derivative
(cid:3)
of the regularized PRW distance (Proposition 2).

13

4. Bootstrap

We consider approximating the derived limit distributions by a bootstrap procedure. Let
r, s ∈ ∆N and X1, ..., Xn ∼ r, Y1, ..., Ym ∼ s be i.i.d. samples with empirical distributions (cid:98)rn
and (cid:98)sm. Furthermore, let (cid:98)r∗
(cid:96) be empirical bootstrap distributions deﬁned using i.i.d.
1 , ..., X ∗
bootstrap samples X ∗

(cid:96) and (cid:98)s∗
(cid:96) ∼ (cid:98)rn and Y ∗

(cid:96) ∼ (cid:98)sm.

1 , ..., Y ∗

The functionals IWp and PWp,λ are only directionally Hadamard diﬀerentiable, that is,
they have nonlinear derivatives with respect to (h1, h2). As mentioned by [10] and [41],
the naive n-out-n bootstrap is inconsistent for such functionals with a nonlinear Hadamard
derivative, but re-sampling fewer than n observations leads to a consistent bootstrap (the
rescaled or m-out-n bootstrap). From this fact, we obtain the following results regarding the
bootstrap for the IPRW and regularized PRW distances. In the following, BL1(R) denotes
the set of all bounded functions on R with a Lipschitz constant of at most one.

Proposition 3. Let p ≥ 1. We assume that (cid:96) → ∞, (cid:96)/n → ∞ and (cid:96)/m → ∞ as n, m → ∞.
Then, the plug-in bootstrap with (cid:98)r∗
(cid:96) for the integral projection robust Wasserstein
distance is consistent:

(cid:96) and (cid:98)s∗

(i) If r = s, and n ∧ m → ∞ and m/(n + m) → δ ∈ (0, 1), we have:

(cid:12)
(cid:12)
(cid:12)
sup
(cid:12)
h∈BL1(R)
(cid:12)

(cid:34)

E

h

IWp((cid:98)r∗

(cid:96) , (cid:98)s∗
(cid:96) )

X1, ..., Xn, Y1, ..., Ym

(cid:35)

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2p

(cid:19) 1

(cid:32)(cid:18) (cid:96)
2
(cid:32)(cid:18) nm
n + m

h

(cid:34)

(cid:19) 1

2p

IWp((cid:98)rn, (cid:98)sm)

(cid:33)(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

→ 0,

− E

in outer probability.

(ii) If r (cid:54)= s and n ∧ m → ∞ and m/(n + m) → δ ∈ (0, 1), we have:

(cid:12)
(cid:12)
(cid:12)
sup
(cid:12)
h∈BL1(R)
(cid:12)

E

(cid:34)

(cid:32)(cid:114)

h

(cid:96)
2

{IWp((cid:98)r∗

(cid:96) , (cid:98)s∗

(cid:96) ) − IWp((cid:98)rn, (cid:98)sm)}

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

X1, ..., Xn, Y1, ..., Ym

(cid:35)

− E

(cid:20)

h

(cid:18)(cid:114) nm
n + m

{IWp((cid:98)rn, (cid:98)sm) − IWp(r, s)}

(cid:19)(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

→ 0,

in outer probability.

Proof. As shown in Proposition 1, the map (r, s) (cid:55)→ IWp(r, s) is directionally Hadamard
diﬀerentiable. Then, the proof is a direct application of Proposition 2 in [10] with this
(cid:3)
map.

Proposition 4. Let p ≥ 1 be even and λ > 0. We assume that (cid:96) → ∞, (cid:96)/n → ∞ and
(cid:96)/m → ∞ as n, m → ∞. Then, the plug-in bootstrap with (cid:98)r∗
(cid:96) for the regularized
projection robust Wasserstein distance is consistent. That is, as n ∧ m → ∞ and m/(n +
14

(cid:96) and (cid:98)s∗

m) → δ ∈ (0, 1), we have
(cid:32)(cid:114)

(cid:34)

sup
h∈BL1(R)

E

h

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:96)
2

{PWp,λ((cid:98)r∗

(cid:96) , (cid:98)s∗

(cid:96) ) − PWp,λ((cid:98)rn, (cid:98)sm)}

X1, ..., Xn, Y1, ..., Ym

(cid:35)

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

− E

(cid:20)

h

(cid:18)(cid:114) nm
n + m

{PWp,λ((cid:98)rn, (cid:98)sm) − PWp,λ(r, s)}

(cid:19)(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

→0,

in outer probability.

Proof. As shown in Proposition 2, the map (r, s) (cid:55)→ PWp,λ(r, s) is directionally Hadamard
diﬀerentiable. Then, the proof is a direct application of Proposition 2 in [10] with this
(cid:3)
map.

In practice, the performance of our bootstrap procedure depends on the choice of replace-
ment number (cid:96). In Section 5, we investigate how the choice of (cid:96) aﬀects the ﬁnite-sample
performance of bootstrapping using simulation studies.

5. Simulation studies

We illustrate our distributional limit results in Monte Carlo simulations. Speciﬁcally, we
investigate the speed of convergence for the empirical IPRW distance (p = 1) and the empir-
ical regularized PRW distance (p = 2) to their limit distributions (Theorems 2 and 4). We
also illustrate the accuracy of the approximation using the rescaled bootstrap (Propositions
3 and 4). All simulations were performed using R ([43]). The Wasserstein distances were
calculated using the R package transport [38], and the regularized transport distances were
calculated using the R package Barycenter [15].

5.1. Speed of convergence to the distributional limit.

5.1.1. Integral projection robust Wasserstein distance. We consider ﬁnite ground space X to
be an equidistant two-dimensional L × L grid on [0, 1] × [0, 1], with size N = L2. We ﬁrst
set the grid size to L = 7 (i.e., N = 49).

For case r = s, we consider a probability distribution r on X as the realization of a Dirichlet
random variable Dir(1) with concentration parameter 1 = (1, ..., 1) ∈ RN and set s = r.
Given such distributions r, s ∈ ∆N , we sample observations X1, ..., Xn ∼ r and Y1, ..., Ym ∼
s i.i.d. with sample size n = m ∈ {25, 50, 100, 1000, 5000} and compute (cid:112) n
2 IW1((cid:98)rn, (cid:98)sn)
with one-dimensional projection and the uniform measure, which corresponds to the sliced
Wasserstein distance. This process is repeated 20,000 times. Similarly, we consider the same
setup for r (cid:54)= s, where we generate a second distribution, s ∼ Dir(1), independently. We then
compare the ﬁnite distributions with the theoretical limit distributions given by Theorem 2.
We demonstrate the results using kernel density estimators and the corresponding Q-Q
plots in Figure 1 (A) and (B). The limit distributions are good approximations of the ﬁnite
15

sample distributions for a large sample size (n = 1000) in both cases r = s and r (cid:54)= s. We
also observe that, under r = s, the limit law approximates the ﬁnite sample distribution quite
well, even for a small sample size (n = 50). In Figure 2, we also show the speed of convergence
with respect to the Kolmogorov–Smirnov distance (maximum absolute diﬀerence between
the distribution function of the ﬁnite sample law and that of the limit law) for grid sizes
L = 3, 5, 7. This shows that the Kolmogorov–Smirnov distances decrease as the sample size
increases, and the size of ground space N = L2 slows the speed of convergence marginally,
especially for r (cid:54)= s.

5.1.2. Regularized projection robust Wasserstein distance. We consider ground space X to
be of form {1/M, 2/M, ..., M/M } × {−0.001, 0.001} × {−0.001, 0.001} ⊂ R3 with grid size M
and total size N = 4M . This ground space X is set to have a low-dimensional structure: two
distributions on X diﬀer mostly in the ﬁrst coordinate, while the diﬀerences in the second and
third coordinates are regarded as noise. For M = 10 (i.e., N = 40), we generated probability
distributions r and s on X as realizations of independent Dirichlet random variables Dir(1).
Given distributions r (cid:54)= s, we consider the same sampling scenarios as in the case of the IPRW
distance and compute (cid:112) n
2 {PW2,λ((cid:98)rn, (cid:98)sn)−PW2,λ(r, s)} with one-dimensional projection and
regularization parameter λ = 1. We repeat this process 20,000 times and compare the ﬁnite
distribution to its theoretical limit distribution given by Theorem 4.

Figure 3 shows the results demonstrated by the kernel density estimators and correspond-
ing Q-Q plots. The limit distributions are good approximations of the ﬁnite sample distri-
butions for both small and large sample sizes. Figure 4 shows the speed of convergence with
respect to the Kolmogorov-Smirnov distance under grid sizes M = 3, 7, 10. We observe a
declining tendency of the Kolmogorov-Smirnov distances as the sample size increases.

5.2. Simulation of bootstrap.

5.2.1. Integral projection robust Wasserstein distance. We simulate the rescaled plug-in boot-
strap approximations from Section 4 for the IPRW distance. For a grid with L = 7, we
generate r ∼ Dir(1), set s = r, and sample n = 1000 observations according to proba-
In addition, for ﬁxed empirical distributions (cid:98)rn, (cid:98)sn, we generate
bility distributions r, s.
(cid:96) , (cid:98)s∗
(cid:96) ) by drawing independently with replace-
B = 500 bootstrap replications of
ment (cid:96) ∈ {n, n4/5, n2/3, n1/2} according to (cid:98)rn and (cid:98)sn. Similarly, we consider the same setup in
the case of r (cid:54)= s, where the second distribution, s, is generated independently from Dir(1).
In the r (cid:54)= s case, the form of bootstrap replications is
(cid:96) ) − IW1((cid:98)rn, (cid:98)sn)}.
The ﬁnite bootstrap sample distributions are then compared with their ﬁnite sample and
theoretical limit distributions.

2{IW1((cid:98)r∗

2IW1((cid:98)r∗

(cid:96) , (cid:98)s∗

(cid:113) (cid:96)

(cid:113) (cid:96)

The results are shown in Figure 5. We observe that, under r = s, ﬁnite bootstrap distri-
butions with fewer replacements ((cid:96) = n4/5, n2/3, n1/2) are better approximations of the ﬁnite
16

(a) r = s

(b) r (cid:54)= s

Figure 1.

(A) Comparison of the ﬁnite sample distributions and the
limit distribution of the empirical IPRW distance for the case r = s. First
row shows ﬁnite sample density (dashed line) of the empirical IPRW distance for
n = 50 on a regular grid of size L = 7 compared to its limit density (solid line).
The densities are estimated by kernel density estimators with Gaussian kernel and
Silverman’s rule is used to select bandwidth. The corresponding Q-Q plot is pre-
sented on the right, where the red solid line indicates perfect ﬁt. Second row is the
same setting as above, but n = 1000. (B) Comparison of the ﬁnite sample
distributions and the limit distribution of the empirical IPRW distance
17
for the case r (cid:54)= s. Same scenario as in (A), but here the sampling distributions r
and s are not equal.

(a) r = s

(b) r (cid:54)= s

Figure 2.

(A) Kolmogorov-Smirnov distances of the IPRW distance
for the case r = s. The Kolmogorov-Smirnov distance between the ﬁnite sample
distributions of the empirical IPRW distance and its theoretical limit distribution
for diﬀerent sample size n ∈ {25, 50, 100, 1000, 5000} and diﬀerent grid sizes L. The
axes are given on a logarithmic scale.
(B) Kolmogorov-Smirnov distances of
the IPRW distance for the case r (cid:54)= s. Same scenario as in (A), but here the
sampling distributions r and s are not equal.

sample distribution than the naive bootstrap ((cid:96) = n). This is consistent with the theoretical
result in Section 4, which claims the naive bootstrap does not have consistency for the IPRW
distance but resampling fewer observations leads to consistency. However, under r (cid:54)= s, the
bootstrap approximations with fewer replacements are not good, and the naive bootstrap
approximation is better. This good approximation by the naive bootstrap is possible due
to the fact that the map (r, s) (cid:55)→ IWp(r, s) is only directionally Hadamard diﬀerentiable in
general but (non-directionally) Hadamard diﬀerentiable at most points (r, s) with r (cid:54)= s. For
instance, for ground size N = 2 (i.e., X = {x1, x2}), the IPRW distance can be explicitly
written as IWp(r, s) = ((cid:82)
(cid:107)E(cid:62)(x1 − x2)(cid:107)pdµ(E))1/p|r1 − s1|. Therefore, in this case, the
map (r, s) (cid:55)→ IWp(r, s) is Hadamard diﬀerentiable if r (cid:54)= s.

Sd,k

5.2.2. Regularized projection robust Wasserstein distance. For grid size M = 10, we gen-
erate distributions r and s as realizations of independent random variables from Dir(1)
and sample n = 1000 observations according to probability distributions r, s. Addition-
ally, for ﬁxed empirical distributions (cid:98)rn, (cid:98)sn, we generate B = 500 bootstrap replications of
(cid:113) (cid:96)
(cid:96) ) − PW2,λ((cid:98)rn, (cid:98)sn)} with λ = 1 by drawing independently with replacement
(cid:96) ∈ {n, n4/5, n2/3, n1/2}, according to (cid:98)rn and (cid:98)sn. The ﬁnite bootstrap sample distributions
are then compared with their ﬁnite sample and theoretical limit distributions.

2{PW2,λ((cid:98)r∗

(cid:96) , (cid:98)s∗

The results are shown in Figure 6. The accuracy of the bootstrap approximation is not

aﬀected by replacement number (cid:96) in this case.
18

Figure 3. Comparison of the ﬁnite sample distributions and the limit
distribution of the empirical regularized PRW distance for the case r (cid:54)= s.
First row shows ﬁnite sample density (dashed line) of the empirical regularized PRW
distance for n = 50 on a ground space of grid size M = 10 compared to its limit
density (solid line). The densities are estimated in the same way as Figure 1. The
corresponding Q-Q plot is presented on the right, where the red solid line indicates
perfect ﬁt. Second row is the same setting as above, but n = 1000.

Figure 4. Kolmogorov-Smirnov distance of the regularized PRW for
r (cid:54)= s. The Kolmogorov-Smirnov distance between the ﬁnite sample distribution
of the empirical regularized PRW distance and its theoretical limit distribution for
diﬀerent sample size n ∈ {25, 50, 100, 1000, 5000} and diﬀerent grid sizes M . The
axes are given on a logarithmic scale.

19

(a) r = s

(b) r (cid:54)= s

Figure 5. (A) Bootstrap for the empirical IPRW distance under r = s.
Illustration of the rescaled plug-in bootstrap approximation (n = 1000) with re-
placement (cid:96) ∈ {n, n4/5, n2/3, n1/2} and grid size L = 7. Finite bootstrap densities
(dotted lines) are compared to their ﬁnite sample density (solid line) and limit den-
sity (dashed line). The densities are estimated in the same way with Figure 1. (B)
Bootstrap for the empirical IPRW distance under r (cid:54)= s. Same scenario as
in (A), but here the sampling distributions r and s are not equal.

20

Figure 6. Bootstrap for the regularized PRW distance under r (cid:54)= s. Il-
lustration of the rescaled plug-in bootstrap approximation (n = 1000) with the
replacement (cid:96) ∈ {n, n4/5, n2/3, n1/2} and grid size M = 10. Finite bootstrap den-
sities (dotted lines) are compared with their ﬁnite sample density (solid line) and
limit density (dashed line). The densities are estimated in the same way with Figure
1.

6. Applications

6.1. Two-sample testing with sliced Wasserstein distance. Let r, s ∈ ∆N and take
X1, ..., Xn ∼ r, Y1, ..., Ym ∼ s be i.i.d. samples. The nonparametric two-sample testing is a
problem of detecting whether sampling distributions r, s are equal, based on samples. This
is described as

H0 : r = s vs. H1 : r (cid:54)= s.

Based on the previous distributional results, we propose a test using the sliced Wasserstein
distance, that is, the IPRW distance with one-dimensional projection and uniform measure.
Speciﬁcally, we denote SWm,n = (cid:112) mn

m+nIWp((cid:98)rn, (cid:98)sm) and propose a test

SWm,n > cα ⇒ reject H0,

where cα is a critical value chosen according to the given level of α ∈ (0, 1). The two-sample
testing based on the Wasserstein distance was performed by [34]. They designed univariate
test statistics using the Wasserstein distance and analyzed their limit distribution. However,
their approach is only available for the d = 1 case, as it does not extend to higher dimensions.
Our proposed test is not restricted to a one-dimensional setting, being applicable to large-
scale datasets because of the low computational complexity of the sliced Wasserstein distance.
21

r = s r (cid:54)= s

(cid:96) = n4/5 0.001 1.000
(cid:96) = n2/3 0.016 1.000
(cid:96) = n1/2 0.037 1.000

Table 1. Rejection rates of the proposed test. The signiﬁcance level is 0.05.

1 , ..., Y ∗
(cid:113) (cid:96)
m,n =

(cid:96) be the empirical bootstrap distributions obtained from bootstrap samples X ∗

We use the bootstrap procedure to choose an appropriate critical value from data. Let (cid:98)r∗
(cid:96)
and (cid:98)s∗
(cid:96) ∼
(cid:98)rn and Y ∗
(cid:96) ∼ (cid:98)sm, respectively. We deﬁne the bootstrap version of the test statistics
as: SW∗
2IWp((cid:98)r∗
m,n. Note that (cid:98)cα
can be computed numerically. Then, the validity of the rescaled bootstrap for the IPRW
distance (Proposition 3) implies that, under (cid:96) → ∞, (cid:96)/n → 0, and (cid:96)/m → 0 as n, m → ∞,
the test

(cid:96) ) and denote by (cid:98)cα the (1 − α)quantile of SW∗

1 , ..., X ∗

(cid:96) , (cid:98)s∗

SWm,n > (cid:98)cα ⇒ reject H0

has asymptotic level α. Speciﬁcally, lim supm,n→∞ P (SWm,n > (cid:98)cα) ≤ α.

We here illustrate the ﬁnite sample performance of this test. We set the ﬁnite ground
space X to be an equidistant two-dimensional 7 × 7 grid on [0, 1] × [0, 1]. For the case r = s,
we generate a distribution r ∼ Dir(1) and set s = r, while for the case r (cid:54)= s, we generate
two distributions r, s ∼ Dir(1) independently. We set the sample size as n = m = 1000 and
vary the replacement number as (cid:96) ∈ {n4/5, n2/3, n1/2}. We set the signiﬁcance level to be
α = 0.05 and run 1000 Monte Carlo iterations in each case.

Table 1 shows the rejection rates of the proposed test in each case. For the case r = s,
the rejection rates should be under the signiﬁcance level α = 0.05, and this is true for all
(cid:96) ∈ {n4/5, n2/3, n1/2}. For the case r (cid:54)= s, the power of the test is 1.000, which is satisfactory.
We now apply the proposed test to testing the equality of color distributions in images.
Given two diﬀerent images, the aim is to investigate whether the images have signiﬁcantly
diﬀerent color distributions. Figure 7 shows the datasets of images used. Each image has
768 × 576 = 442368 pixels. We obtained these images from a publicly available dataset
http://tabby.vision.mcgill.ca/html/welcome.html. We transform each image into a
color histogram in the RGB color space with grid size 163 = 4086. In the dataset 1 (the ﬁrst
column in Figure 7), the two images are expected to have diﬀerent color distributions. In the
dataset 2 (the second column in Figure 7), the two images are expected to have diﬀerent but
similar color distributions. In the dataset 3 (the third row in Figure 7), one image is obtained
by turning the other image from side to side; thus, they have the same color histograms. In
each dataset, we randomly select n = 10, 000 pixels from each image and construct empirical
color distributions (cid:98)rn, (cid:98)sn. We then calculate the test statistics SWn,n and p-values based on
22

B = 500 bootstrap with replacement (cid:96) ∈ {n4/5, n2/3, n1/2}. The results are shown in Table
2.

Figure 7. Datasets of images. The ﬁrst, second and third columns show the
dataset 1, 2 and 3, respectively.

Dataset Statistic

p-value

(cid:96) = n4/5

(cid:96) = n2/3

(cid:96) = n1/2

1
2
3

15.55
9.07
0.25

<0.001 <0.001 < 0.001
< 0.001 < 0.001 < 0.001
0.372

0.352

0.446

Table 2. Two-sample testing for the color distributions of the images

We observe that, for the dataset 1, the proposed test with every replacement (cid:96) suggests
a strong rejection of the null hypothesis. For the dataset 2, we also see a strong rejection
of the null hypothesis, but the test statistics (9.07) is smaller than that for the dataset 1
(15.55). For the dataset 3, the proposed test with any replacement (cid:96) does not report a small
p-value, which means there is no strong evidence to reject the null hypothesis.

6.2. Interval estimation for regularized projection robust Wasserstein distance.
Given a level α ∈ (0, 1) and i.i.d. samples X1, ..., Xn ∼ r, Y1, ..., Ym ∼ s, we aim to construct
an asymptotic conﬁdence interval Cnm for the regularized PRW distance PWp,λ(r, s), so that

lim inf
n,m→∞

P (PWp,λ(r, s) ∈ Cmn) ≥ 1 − α.

23

The previous distributional results allow us to construct Cnm. Although we focus on the
regularized PRW distance, we can also construct such an interval for the IPRW distance
under r (cid:54)= s in the same manner.

Let (cid:98)r∗
X ∗
1 , ..., X ∗
of PWp,λ((cid:98)r∗

(cid:96) , (cid:98)s∗

(cid:96) and (cid:98)s∗
(cid:96) ∼ (cid:98)rn and Y ∗

(cid:96) be the empirical bootstrap distributions obtained from bootstrap samples
(cid:96) ∼ (cid:98)sm, respectively. We denote the α/2 and (1−α/2) quantiles

1 , ..., Y ∗

(cid:96) ) as qα/2 and q1−α/2, respectively, and deﬁne

(cid:34)

Cnm =

PWp,λ((cid:98)rn, (cid:98)sm) −

(cid:114) n + m
nm

q1−α/2, PWp,λ((cid:98)rn, (cid:98)sm) −

(cid:114)n + m
nm

(cid:35)

qα/2

.

Then, the validity of the rescaled bootstrap for the regularized PRW distance (Proposition
4) implies that, under (cid:96) → ∞, (cid:96)/n → 0, (cid:96)/m → 0 as n, m → ∞ and m/(n + m) → δ ∈ (0, 1),
Cnm is an asymptotic (1 − α) conﬁdence interval for PWp,λ(r, s).

We apply the proposed interval estimation method to handwritten letter images from
the Modiﬁed National Institute of Standards and Technology database (MNIST) dataset
(http://yann.lecun.com/exdb/mnist/). The dataset contains images with 576 pixels for
handwritten digits from 0 to 9. Because the distributions generating the images of each digit
are likely to have low-dimensional structures, the PRW distance is expected to capture the
diﬀerences between them eﬀectively. Based on the above result, we construct 0.95 conﬁdence
intervals for regularized PRW distances between pairs of digits. Speciﬁcally, we use n = m =
892 images of digits 0, 1, 4, 7 and 9, and extract 128-dimensional features of each image
using a convolution neural network (CCN), as outlined in [22]. Then, we estimate the global
intrinsic dimension of feature data using the maxLikLocalDimEst function in the R package
intrinsicDimension [14] and obtain an estimate of 6.77. Based on this estimate, we set the
projection dimension to 7 and the order to p = 2. We then construct the 0.95 conﬁdence
intervals using B = 1000 bootstrap with replacement n4/5 ≈ 230. The regularized PRW
distance are calculated by the Riemannian optimization method proposed by [22].

Figure 8 shows the results. The distances between digits 1 and 7 or digits 4 and 9 are
smaller than those between digits 0 and 1 or digits 0 and 4. Moreover, the distances between
the same digits are quite small. These results are consistent with our intuition.

Furthermore, we add Gaussian noise with a standard deviation of σ = 1, 5, 10 to the feature
data and again construct 0.95 conﬁdence intervals for the regularized PRW distances. For
comparison, we also construct 0.95 conﬁdence intervals for the original Wasserstein distances
[41]. The results are shown in Figure 9. The interval estimates of the regularized PRW
distance are less inﬂuenced by the increase of the variance of the Gaussian noise than those
of the Wasserstein distance. This result implies that, the PRW distance is more robust to the
noise than than the original Wasserstein distance, when the dataset has a low-dimensional
structure.

24

Figure 8. Display of 0.95 conﬁdence intervals for the regularized PRW distance
between hand-written digits. Intervals for the same digits are calculated by splitting
the dataset into two groups. Intervals are normalized by setting the lower bound
for 0&1 to be 1.

7. Discussion and conclusions

This study investigated statistical inference for the IPRW and regularized PRW distances.
Although these projection-based Wasserstein distances are practical for many machine learn-
ing tasks, their inferential tools have not been well established. We derived the limit distribu-
tions of the empirical versions of these distances on ﬁnite spaces by showing their directional
Hadamard diﬀerentiability. We also show that, while the naive bootstrap fails for these
distances, the rescaled bootstrap is consistent.

There are promising directions for future research. Our theoretical results are limited to
ﬁnitely supported measures and it is worthwhile to extend them to more general settings.
The appropriate choice of the replacement number of the rescaled bootstrap or projection
dimension of the PRW distance is important in practice. Developing data-driven methods
to choose their values is an interesting direction for further research.

References

[1] Heinz H Bauschke, Patrick L Combettes, et al. Convex analysis and monotone operator

theory in Hilbert spaces, volume 408. Springer, 2011.

[2] J´er´emie Bigot, Elsa Cazelles, and Nicolas Papadakis. Central limit theorems for entropy-
regularized optimal transport on ﬁnite spaces and statistical applications. Electronic
Journal of Statistics, 13(2):5120–5150, 2019.

25

Figure 9. Display of 0.95 conﬁdence intervals for the regularized PRW and
Wasserstein distance between hand-written digits with Gaussian noises. Intervals
for the same digits are calculated by splitting the dataset into two groups. For each
distance, intervals are normalized by setting the lower bound for 0&1 to be 1.

26

[3] Nicolas Bonneel, Julien Rabin, Gabriel Peyr´e, and Hanspeter Pﬁster. Sliced and radon
wasserstein barycenters of measures. Journal of Mathematical Imaging and Vision,
51(1):22–45, 2015.

[4] Mathieu Carriere, Marco Cuturi, and Steve Oudot. Sliced wasserstein kernel for per-
In International conference on machine learning, pages 664–673.

sistence diagrams.
PMLR, 2017.

[5] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Ad-

vances in neural information processing systems, 26:2292–2300, 2013.

[6] Eustasio Del Barrio, Juan A Cuesta-Albertos, Carlos Matr´an, and Jes´us M Rodr´ıguez-
Rodr´ıguez. Tests of goodness of ﬁt based on the l2-wasserstein distance. Annals of
Statistics, pages 1230–1239, 1999.

[7] Eustasio Del Barrio, Evarist Gin´e, and Frederic Utzet. Asymptotics for l2 functionals
of the empirical quantile process, with applications to tests of ﬁt based on weighted
wasserstein distances. Bernoulli, 11(1):131–189, 2005.

[8] Eustasio Del Barrio and Jean-Michel Loubes. Central limit theorems for empirical
transportation cost in general dimension. The Annals of Probability, 47(2):926–951,
2019.

[9] Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi
Koyejo, Zhizhen Zhao, David Forsyth, and Alexander G Schwing. Max-sliced wasser-
stein distance and its use for gans.
In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 10648–10656, 2019.

[10] Lutz D¨umbgen. On nondiﬀerentiable functions and the bootstrap. Probability Theory

and Related Fields, 95(1):125–140, 1993.

[11] Anthony V Fiacco et al. Introduction to sensitivity and stability analysis in nonlinear

programming, volume 165. Academic press, 1983.

[12] Gudrun Freitag and Axel Munk. On hadamard diﬀerentiability in k-sample semipara-
metric models—with applications to the assessment of structural relationships. Journal
of multivariate analysis, 94(1):123–158, 2005.

[13] Ziv Goldfeld and Kristjan Greenewald. Gaussian-smoothed optimal transport: Metric
structure and statistical eﬃciency. In International Conference on Artiﬁcial Intelligence
and Statistics, pages 3327–3337. PMLR, 2020.

[14] Kerstin Johnsson and Lund University. intrinsicDimension: Intrinsic Dimension Esti-

mation, 2019. R package version 1.2.0.

[15] Marcel Klatt. Barycenter: Regularized Wasserstein Distances and Barycenters, 2018.

R package version 1.3.1.

[16] Marcel Klatt, Carla Tameling, and Axel Munk. Empirical regularized optimal transport:
Statistical theory and applications. SIAM Journal on Mathematics of Data Science,
2(2):419–443, 2020.

27

[17] Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, and Gustavo K Rohde.
Generalized sliced wasserstein distances. arXiv preprint arXiv:1902.00434, 2019.
[18] Soheil Kolouri, Phillip E Pope, Charles E Martin, and Gustavo K Rohde. Sliced wasser-
stein auto-encoders. In International Conference on Learning Representations, 2018.
[19] Soheil Kolouri, Yang Zou, and Gustavo K Rohde. Sliced wasserstein kernels for prob-
ability distributions. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 5258–5267, 2016.

[20] Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings
to document distances. In International conference on machine learning, pages 957–966.
PMLR, 2015.

[21] Tam Le, Makoto Yamada, Kenji Fukumizu, and Marco Cuturi. Tree-sliced variants of

wasserstein distances. arXiv preprint arXiv:1902.00342, 2019.

[22] Tianyi Lin, Chenyou Fan, Nhat Ho, Marco Cuturi, and Michael I Jordan. Pro-
arXiv preprint

jection robust wasserstein distance and riemannian optimization.
arXiv:2006.07458, 2020.

[23] Tianyi Lin, Zeyu Zheng, Elynn Chen, Marco Cuturi, and Michael Jordan. On projection
robust optimal transport: Sample complexity and model misspeciﬁcation. In Interna-
tional Conference on Artiﬁcial Intelligence and Statistics, pages 262–270. PMLR, 2021.
[24] Antoine Liutkus, Umut Simsekli, Szymon Majewski, Alain Durmus, and Fabian-Robert
St¨oter. Sliced-wasserstein ﬂows: Nonparametric generative modeling via optimal trans-
port and diﬀusions. In International Conference on Machine Learning, pages 4104–4113.
PMLR, 2019.

[25] Tudor Manole, Sivaraman Balakrishnan, and Larry Wasserman. Minimax conﬁdence
intervals for the sliced wasserstein distance. arXiv preprint arXiv:1909.07862, 2019.
[26] Gonzalo Mena and Jonathan Weed. Statistical bounds for entropic optimal transport:
sample complexity and the central limit theorem. arXiv preprint arXiv:1905.11882,
2019.

[27] Axel Munk and Claudia Czado. Nonparametric validation of similar distributions and
assessment of goodness of ﬁt. Journal of the Royal Statistical Society: Series B (Statis-
tical Methodology), 60(1):223–241, 1998.

[28] Khai Nguyen, Nhat Ho, Tung Pham, and Hung Bui. Distributional sliced-wasserstein
and applications to generative modeling. arXiv preprint arXiv:2002.07367, 2020.
[29] Jonathan Niles-Weed and Philippe Rigollet. Estimation of wasserstein distances in the

spiked transport model. arXiv preprint arXiv:1909.07513, 2019.

[30] Victor M Panaretos and Yoav Zemel. Statistical aspects of wasserstein distances. Annual

review of statistics and its application, 6:405–431, 2019.

[31] Fran¸cois-Pierre Paty and Marco Cuturi. Subspace robust wasserstein distances.
International Conference on Machine Learning, pages 5072–5081. PMLR, 2019.

In

28

[32] Gabriel Peyr´e, Marco Cuturi, et al. Computational optimal transport: With applications
to data science. Foundations and Trends® in Machine Learning, 11(5-6):355–607, 2019.
[33] Julien Rabin, Gabriel Peyr´e, Julie Delon, and Marc Bernot. Wasserstein barycenter
and its application to texture mixing. In International Conference on Scale Space and
Variational Methods in Computer Vision, pages 435–446. Springer, 2011.

[34] Aaditya Ramdas, Nicol´as Garc´ıa Trillos, and Marco Cuturi. On wasserstein two-sample

testing and related families of nonparametric tests. Entropy, 19(2):47, 2017.

[35] Werner R¨omisch. Delta method, inﬁnite dimensional. 2004.
[36] Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover’s distance as a
metric for image retrieval. International journal of computer vision, 40(2):99–121, 2000.
[37] Roman Sandler and Michael Lindenbaum. Nonnegative matrix factorization with earth
mover’s distance metric for image analysis. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 33(8):1590–1602, 2011.

[38] Dominic Schuhmacher, Bj¨orn B¨ahre, Carsten Gottschlich, Valentin Hartmann, Florian
transport: Computation of Optimal Transport

Heinemann, and Bernhard Schmitzer.
Plans and Wasserstein Distances, 2020. R package version 0.12-2.

[39] Alexander Shapiro. On concepts of directional diﬀerentiability. Journal of optimization

theory and applications, 66(3):477–487, 1990.

[40] Justin Solomon, Fernando De Goes, Gabriel Peyr´e, Marco Cuturi, Adrian Butscher,
Andy Nguyen, Tao Du, and Leonidas Guibas. Convolutional wasserstein distances:
Eﬃcient optimal transportation on geometric domains. ACM Transactions on Graphics
(TOG), 34(4):1–11, 2015.

[41] Max Sommerfeld and Axel Munk. Inference for empirical wasserstein distances on ﬁnite
spaces. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
80(1):219–238, 2018.

[42] Carla Tameling, Max Sommerfeld, and Axel Munk. Empirical optimal transport on
countable metric spaces: Distributional limits and statistical applications. The Annals
of Applied Probability, 29(5):2744–2781, 2019.

[43] R Core Team et al. R: A language and environment for statistical computing. 2013.
[44] Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press,

2000.

[45] C´edric Villani. Optimal transport: old and new, volume 338. Springer, 2009.
[46] Jonathan Weed and Francis Bach. Sharp asymptotic and ﬁnite-sample rates of con-
vergence of empirical measures in wasserstein distance. Bernoulli, 25(4A):2620–2648,
2019.

[47] Meng Zhang, Yang Liu, Huanbo Luan, Maosong Sun, Tatsuya Izuha, and Jie Hao.
Building earth mover’s distance on bilingual word embeddings for machine translation.
In Thirtieth AAAI conference on artiﬁcial intelligence, 2016.

29

