CodeGenerationfromNaturalLanguagewithLessPriorandMoreMonolingualDataSajadNorouzi∗KeyiTangYanshuaiCaoBorealisAIsajadn@cs.toronto.edu,{keyi.tang,yanshuai.cao}@borealisai.comAbstractTrainingdatasetsforsemanticparsingaretypi-callysmallduetothehigherexpertiserequiredforannotationthanmostotherNLPtasks.Asaresult,modelsforthisapplicationusuallyneedadditionalpriorknowledgetobebuiltintothearchitectureoralgorithm.Theincreasedde-pendencyonhumanexpertshindersautoma-tionandraisesthedevelopmentandmainte-nancecostsinpractice.Thisworkinvestigateswhetheragenerictransformer-basedseq2seqmodelcanachievecompetitiveperformancewithminimalcode-generation-speciﬁcinduc-tivebiasdesign.Byexploitingarelativelysize-ablemonolingualcorpusofthetargetprogram-minglanguage,whichischeaptominefromtheweb,weachieved81.03%exactmatchac-curacyonDjangoand32.57BLEUscoreonCoNaLa.BothareSOTAtothebestofourknowledge.Thispositiveevidencehighlightsapotentiallyeasierpathtowardbuildingaccu-ratesemanticparsersinpractice.†1IntroductionForamachinetoactuponusers’naturallanguageinputs,amodelneedstoconvertthenaturallan-guageutterancestomachine-understandablemean-ingrepresentation,i.e.semanticparsing(SP).Theoutputmeaningrepresentationisbeyondshallowidentiﬁcationoftopic,intention,entityorrelation,butcomplexstructuredobjectsexpressedaslogi-calforms,querylanguageorgeneral-purposepro-grams.Therefore,annotatingparallelcorpusforsemanticparsingrequiresmorecostlyexpertise.SPsharessomeresemblancewithmachinetrans-lation(MT).However,SPdatasetsaretypicallysmaller,withonlyafewthousandtoatmosttensofthousandsofexamples,evensmallerthanmostlowresourceMTproblems.Simultaneously,because*WorkdoneduringinternshipatBorealisAI†Codeathttps://github.com/BorealisAI/code-gen-TAEFigure1:TAE:themonolingualcorpusisusedbothassourceandtarget.Theencoderisfrozeninthecompu-tationbranchonthemonolingualdata.thepredictedoutputsgenerallyneedtobeexactlycorrecttoexecuteandproducetherightanswer,theaccuracyrequirementisgenerallyhigherthanMT.Asaresult,inductivebiasdesigninarchitec-tureandalgorithmhasbeenprevalentintheSPliterature(DongandLapata,2016;YinandNeubig,2017,2018;DongandLapata,2018;Guoetal.,2019;Wangetal.,2019;YinandNeubig,2019).Whiletheirprogressisremarkable,excessivetask-speciﬁcexpertdesignmakesthemodelscom-plicated,hardtotransfertonewdomains,andchal-lengingtodeployinreal-worldapplications.Inthiswork,welookattheoppositeendofthespectrumandtrytoanswerthefollowingquestion:withlit-tleinductivebiasinthemodel,andnoadditionallabelleddata,isitstillpossibletoachievecompet-itiveperformance?Thisisanimportantquestion,astheanswercouldpointtoamuchshorterroadtopracticalSPwithoutbreakingthebank.Thispapershowsthattheanswerisencourag-inglyafﬁrmative.Byexploitingarelativelylargemonolingualcorpusoftheprogramminglanguage,atransformer-basedSeq2Seqmodel(Vaswanietal.,2017)withlittleSPspeciﬁcpriorcouldpotentiallyattainresultssuperiortoorcompetitivewiththearXiv:2101.00259v2  [cs.CL]  10 Jun 2021state-of-the-artmodelsspeciallydesignedforse-manticparsing.Ourcontributionsarethree-fold:•Weprovideevidencethattransformer-basedseq2seqmodelscanreachacompetitiveorsuperiorperformancewithmodelsspeciﬁcallydesignedforsemanticparsing.Thissuggestsanalternativerouteforfutureprogressotherthaninductivebiasdesign;•Wedoempiricalanalysisoverpreviouslypro-posedapproachesforincorporatingmonolin-gualdataandshowtheeffectivenessofourmodiﬁedtechniqueonarangeofdatasets;•Wesetthenewstate-of-the-artonDjango(Odaetal.,2015)reaching81.03%exactmatchaccuracyandonCoNaLa(Yinetal.,2018)withaBLEUscoreof32.57.2PreviousWorkonSemanticParsingDifferentsourcesofpriorknowledgeabouttheSPproblemstructurecouldbeexploited.Inputstructure:Wangetal.(2019)adaptsthetransformerrelativepositionencoding(Shawetal.,2018)toexpressrelationsamongthedatabaseschemaelementsaswellaswiththeinputtextspans.HerzigandBerant(2020)proposedaspan-basedneuralparserwithcompositionalinductivebiasbuilt-in.HerzigandBerant(2020)alsolever-agesaCKY-style(Cocke,1969;Kasami,1966;Younger,1967)inferencetolinkinputfeaturestooutputcodes.Outputstructure:Theimplicittreeorgraph-likestructuresintheprogramscanalsobeexploited.DongandLapata(2016)proposedparent-feedingLSTMfollowingthetreestructure.DongandLap-ata(2018)proposedacoarse-to-ﬁnedecodingap-proach.Guoetal.(2019)craftedanintermediatemeaningrepresentationtobridgethelargegapbe-tweeninpututteranceandtheoutputSQLqueries.YinandNeubig(2017,2018)proposedTranX,amoregeneral-purposetransition-basedsystem,toensuregrammaticalityofpredictions.UsingTranX,theneuralmodelpredictsthelinearsequenceofAST-treeconstructingactionsinsteadofthepro-gramtokens.However,ahumanexpertneedstocraftthegrammar,andthedesignqualityimpactsthelearningandgeneralizationfortheneuralnets.SequentialmodelswithlessSPspeciﬁcpriorshavebeeninvestigated(DongandLapata,2016;Lingetal.,2016b;Zengetal.,2020),However,theygenerallyfellshortinaccuracycomparingtothebestofstructure-exploitingoneslistedabove.ThemostcloselyrelatedtooursistheworkbyXuetal.(2020)forincorporatingexternalknowl-edgefromextradatasets,whichusedanoisyparal-leldatasetfromStackoverﬂowtopre-traintheSPandﬁne-tuneditontheprimarydataset.Theirap-proach’smainlimitationisstilltheneedfor(noisy)paralleldata,albeitcheaperthantheprimaryla-belledset.Nonetheless,asweshallseeintheex-perimentsectionlater,ourapproachachievesbetterresultswhenusingthesameamountofdataminedfromthesamesourcedespiteignoringthesourcesentence.3BackgroundandMethodologyBERT(Devlinetal.,2018)classofpre-trainedmodelscanmakeupforthelackofinductivebiasontheinputsidetosomedegree.Ontheoutputside,wehopetolearnthenecessarypriorknowl-edgeaboutthetargetmeaningrepresentationfromunlabelledmonolingualdata.Usingmonolingualdatatoimproveseq2seqmodelsisnotnewandhasbeenextensivelystud-iedinMTbefore.Notablemethodsincludefusion(Gulcehreetal.,2015;Ramachandranetal.,2016;Srirametal.,2018;Stahlbergetal.,2018),back-translation(BT)(Sennrichetal.,2015;Edunovetal.,2018;Hoangetal.,2018),(Curreyetal.,2017;BurlotandYvon,2018,2019),andBTwithcopiedmonolingualdata(Curreyetal.,2017;Bur-lotandYvon,2019).However,duetomorestruc-turedoutputs,lesstrainingdata,anddifferenteval-uationmetricsofexactmatchcorrectnessinsteadofBLEU,itisuncleariftheselessonstransferfromMTtoSP.SoSP-speciﬁcinvestigationisneeded.3.1TargetAutoencodingwithFrozenEncoderWeassumeaparallelcorpusofnaturallanguageutterancesandtheircorrespondingprograms,B={xxxi,yyyi}.Thegoalistotrainatranslatormodel(TM)tomaximizetheconditionallogprobabil-ityofyyyigivenxxxi,Tθθθ(yyyi|xxxi),overthetrainingset:Lsup=PBTθθθ(yyyi|xxxi)whereθθθisthevectorofTMmodelparameters.LetM={yyy0i}denotethemonolingualdatasetinthetargetlanguage.Curreyetal.(2017);BurlotandYvon(2019)demonstratedthatinlowresourceMT,auto-encodingthemonolingualdatabesidesthemainsupervisedtrainingishelpful.Followingthesamepath,weaddanauto-encodingobjectivetermonmonolingualdata:Lfull=Lsup+PMTθθθ(yyy0i|yyy0i).Figure2:Modeloverviewduringtraining:weuseastandardtransformer-basedencoder-decodermodelwherethepositionalandwordembeddingsaresharedbetweenencoderanddecoder.Themodulesrelatedtotheencoderarerepresentedinblueandthedecoderonesareinyellow.Standardteacherforcingandtransformermaskingisappliedduringtraining.Thetargetyyy0i’sarereconstructedusingthesharedencoder-decodermodel.Weconjecturethatmonolingualdataauto-encodingmainlyhelpsthedecoder,soweproposetofreezetheencoderparametersformonolingualdata.Writingtheencoderanddecoderparametersseparatelywithθθθ=[θθθe,θθθd],thenθθθeisupdatedusingthegradientofthesupervisedobjectiveLsup,whereasthedecodergradientcomesfromLfull.Weverifythishypothesisinsection4.1.Intermsofmodelarchitecture,ourTMisastan-dardtransformer-basedseq2seqmodelwithcopyattention(Guetal.,2016)(illustratedinFig.2ofC).Weﬁne-tuneBERTastheencoderandusea4-layertransformerdecoder.ThereislittleSP-speciﬁcinductivebiasinthearchitecture.Theonlyspecialstructureisthecopyattention,whichisnotastronginductivebiasdesignedforSPascopyattentioniswidelyusedinothertasksaswell.Werefertothemethodofusingcopiedmono-lingualdataandfreezingtheencoderoverthemastargetautoencoding(TAE).Unlessotherwisespec-iﬁedintheablationstudies,theencoderisalwaysfrozen.4ExperimentsForourprimaryexperimentsweconsideredtwopythondatasetsnamelyDjangoandCoNaLa.TheformerisbasedonDjangowebframeworkandthelatterisannotatedcodesnippetsfromstackover-ﬂowanswers.Additionally,weexperimentontheSQLversionofGeoQueryandATISfromFinegan-Dollaketal.(2018)(withquerysplit),WikiSQL(Zhongetal.,2017),andMagic(Java)(Lingetal.,2016b).PythonMonolingualCorpora:CoNaLacomeswith600KminedquestionsfromStack-overﬂow.Weignoredthenoisysourceintents/sen-tencesandjustusethepythonsnippets.TobecomparablewithXuetal.(2020),wealsoselectacorresponding100Ksubsetversionforcompari-son.SeeAppendixAfordetailsontheSQLandJavamonolingualcorpora.ExperimentalSetup:Inallexperiments,weuselabelsmoothingwithaparameterof0.1andPolyakaveraging(PolyakandJuditsky,1992)ofparameterswithamomentumof0.999exceptforGeoQuerywhichweuse0.995.WeuseAdam(KingmaandBa,2014)andearlystoppingbasedonthedatasetspeciﬁcevaluationmetricondevset.Thelearningratefortheencoderis1×10−5overalldatasets.Weusedthelearningrateof7.5×10−5onalldatasetsexceptGeoQueryandATISwhichweuse1×10−4.ThearchitectureoverviewisshowsinFig.2.Attheinferencetimeweusebeamsearchwithbeamsizeof10andalengthnormalizationbasedon(Wuetal.,2016).Weruneachexperimentwith5differentrandomseedsandreporttheaverageandstandarddeviation.WordPiecetokenizationisusedforbothnaturallanguageutterancesandprogrammingcode.4.1EmpiricalAnalysisFirst,weconsideredascenariowherethemono-lingualcorpuscomesfromthesamedistributionasthebitext.Wesimulatethissetupbyusing10%ofDjangotrainingdataaslabeleddatawhileusingallthepythonexamplesfromDjangoasthemono-Source:callthefunctionlazywith2arguments:stringconcatandsix.texttype,substitutetheresultforstringconcat.Gold&TAE:string_concat=lazy(_string_concat,six.text_type)Baseline:string_concat=lazy(_concat_concat,six.text_type)Note:copymistake:wrongvariableresultingfromfailedcopySource:deﬁnethefunctiontimesincewithd,nowdefaultingtonone,reverseddefaultingtofalseasarguments.Gold&TAE:deftimesince(d,now=none,reversed=false):passBaseline:deftimesince(d=none,reversed(d=false):passNote:unbalancedparanthesisandmultiplesemanticmistakes.Table1:ExamplemistakesbythebaselinethatareﬁxedbyTAE.MoreexamplesinAppendixE.Figure3:Analysisusingonly10%Djangotrainbitext.lingualdatasetof10timesbigger.Resultswith“AuthenticDataset”inFig.3showstheeffective-nessofTAEvsotherapproaches.Next,weusedthemonolingualdatasetpreparedforpython(StackOverﬂowCorpus)whichisfromadifferentdistribution.Fig.3showsevenmorecon-siderableimprovement,thankstothelargermono-lingualset.WeconsiderednoisyintentsprovidedinCoNaLamonolingualcorpusanddummysourcesentenceswhereeachmonolingualsampleispairedalongwitharandomlengtharraycontainingze-ros.Wealsocomparedagainstotherwell-knownapproacheslikefusionandback-translation,seeex-perimentsdetailsinAppendixD.TAEoutperformsallthoseapproachesbyalargemargin.Nowoneimportantquestionis,whatpartofthemodelbeneﬁtsfrommonolingualdatamost?InSec.3.1,weconjecturedthatauto-encodingofmonolingualdatashouldmostlyhelpthedecoder,nottheencoder.Toverifythis,weperformanablationbycomparingfreezingencoderparame-tersversusnotfreezingoverthemonolingualset.Fig.3showsthatwithoutfreezingtheencoder,performancedropsslightlyforTAEonauthenticDjangowhiledroppingsigniﬁcantlywhencopyingonStackoverﬂowdata.Thisconﬁrmsthattheper-formancegainisduetoitseffectonthedecoder,whilethecopiedmonolingualdatamightevenhurtstheencoder.4.2MainResultsonFullDataTable2-3showcaseourSOTAresultsonDjangoandCoNaLa.Whileoursimplebaseseq2seqmodeldoesnotoutperformpreviousworks,withTAEonthemonolingualdata,ourperformanceimprovesandoutperformsallthepreviousworks.ThemostdirectcomparisoniswithXuetal.(2020)thatalsoleveragethesameextradataminedfromStackOverﬂow(EKinTable3).AsmentionedinSec.2,theyusedthenoisyparallelcorpusforpre-training,whereasweonlyleveragethemono-lingualset.However,weobtainbothlargerrela-tiveimprovementsoverourbaseline(32.29from30.98)comparedtoXuetal.(2020)(28.14from27.20),aswellasbetterabsoluteresultsinthebestcase.Infact,withonlythe100KStackOverﬂowmonolingualdata,ourresultisonparwiththebestonefromXuetal.(2020)thatusestheadditionalpythonAPIbitextdata.Finally,notethatpartofoursuperiorperformanceisduetousingBERTasanencoder.Finally,TAEalsoyieldsimprovementsonotherprogramminglanguages,asshownforGeoQuery(SQL),ATIS(SQL)andMagic(Java)inTable4.WeobservenoimprovementonWikiSQL.Butitisnotsurprisinggivenitslargedatasetsizeandthesimplicityofitstargets.Asobservedbypreviousworks(Finegan-Dollaketal.,2018),morethanhalfofqueriesfollowsimplepatternof“SELECTcolFROMtableWHEREcol=value”.ThemainresultsintermsofimprovementoverpreviousbestmethodsarestatisticallysigniﬁcantinTable2-3.OnDjango,ourresultisbetterthanReranker(YinandNeubig,2019)(bestpreviousmethodinTable2)withaP-value<0.05,un-derone-tailedtwo-samplet-testformeanequality.SincethepreviousstateoftheartonCoNaLa(EK+100k+APIinTable3)didnotprovidethestan-darddeviation,wecannotconductatwo-samplet-testagainstit.Instead,weperformedaone-tailedtwo-samplet-testagainsttheTranX+BERTbase-lineandobservedthatourimprovementisstatisti-callysigniﬁcantwithP-value<0.05.InTable4,ModelDjangoYN17(YinandNeubig,2017)71.6TRANX(YinandNeubig,2018)73.7Coarse2Fine(DongandLapata,2018)74.1TRANX2(YinandNeubig,2019)77.3±0.4TRANX2+BERT79.7±0.42Reranker(YinandNeubig,2019)∗80.2±0.4Ourbaseline77.05±0.6Ourbaseline+TAE81.03±0.14Table2:ExactmatchaccuracyforDjangotestset.YinandNeubig(2019)∗trainedaseparatemodelontopofSPtorankbeamsearchoutputs.ModelCoNaLaReranker(YinandNeubig,2019)∗30.11TRANX(YinandNeubig,2019)+BERT30.47±0.7EK(baseline)(Xuetal.,2020)27.20EK+100k(Xuetal.,2020)28.14EK+100k+API(Xuetal.,2020)∗†32.26Ourbaseline30.98±0.1Ourbaseline+TAEon100k32.29±0.4Ourbaseline+TAEon600k32.57±0.3Table3:CoNaLatestBLEU.Methodswith∗trainedaseparatemodelontopofSPtorerankbeamsearchout-puts.Xuetal.(2020)†usedanadditionalbitextcorpusminedfrompythonAPIdocumentation.DatasetBaseline(%)Baseline+TAE(%)GeoQuery47.69±0.0551.87±0.02ATIS38.04±0.7740.56±0.57Magic41.61±2.0742.34±0.52WikiSQL85.36±0.0685.30±0.07Table4:Additionaldatasetresults:testsetexactmatchaccuracyonalldataset.improvementsonGeoQueryandATISarestatis-ticallysigniﬁcantwithP-value<0.05,whileitisnotthecaseforMagicandWikiSQL.4.3DiscussionThusfar,wehaveveriﬁedthatthedecoderbeneﬁtsfromTAEandtheencoderdoesnot.ForabetterunderstandingofwhatTAEimprovesinthedecoder,weproposetwometricsnamelycopyaccuracyandgenerationaccuracy.Copyaccuracyonlyconsiderstokensappearinginthesourcesentence.Ifthemodelproducesallofthetokensthatneedtobecopiedfromthesourcesentence,andintherightorder,thenthescoreisoneotherwisezerofortheexample.Generation-accuracyignorestokensappearinginthesourceintentandcomputestheexactmatchaccuracyoftheprediction.Weshowhowtocomputethesemetricsforthefollowingexample:Question:deﬁnethefunctiontimesincewithd,nowdefaultingtonone,reverseddefaultingtofalseasarguments.GroundTruth:“deftimesince(d,now=none,re-versed=false):pass”Weiterateoverthegroundtruthscripttokensonebyoneandremovethosethatcanbecopiedfromthesource,leadingtothiscode:GenerationGroundTruth:“def(=none=):pass”,andtheremovedto-kenswillbeconsideredforcopygroundtruth.CopyGroundTruth:“timesinced,now,reversedfalse”.Wewouldthenusethecopyandgenerationgroundtruthstringstocomputeeachmetric.Notethattheorderoftokensarestillimportantandexactequalityisrequired.AsshowninTable5bothmetricsareimproved.Table1illustratesoneexamplefromeachtypeandwithmoresamplesintheAppendixE.Copyaccu-racyisimportantforproducingtherightvariablenamesmentioned,anditisimprovedasexpected.Itisalsoencouragingtoseequantitativelyandqual-itativelythatgrammarmistakesarereduced,mean-ingthatthelackofpriorknowledgeoftargetlan-guagestructureiscompensatedbylearningfrommonolingualdata.ModelCopyGeneration10%basline34.1855.7310%baseline+TAE58.8966.31Fullbaseline80.1181.27Fullbaseline+TAE84.5982.65Table5:CopyandgenerationaccuraciesonDjangotestset5ConclusionThisworkhasshownthepossibilitytoachieveacompetitiveorevenSOTAperformanceonseman-ticparsingwithlittleornoinductivebiasdesign.Besidestheusuallarge-scalepre-trainedencoders,thekeyistoexploitrelativelylargemonolingualcorporaofthemeaningrepresentation.Themod-iﬁedcopiedmonolingualdataapproachfromma-chinetranslationliteratureworkswellinthisex-tremelylow-resourcesetting.Ourresultspointtoapromisingalternativedirectionforfutureprogress.AcknowledgementsWeappreciatetheACLanonymousreviewersandareachairfortheirvaluableinputs.WealsowouldliketothankanumberofBorealisAIcolleaguesforhelpfuldiscussions,includingWei(Victor)Yang,PengXu,DhruvKumar,andSimonJ.D.Princeforfeedbackonthewriting.ReferencesMiltiadisAllamanisandCharlesSutton.2013.MiningSourceCodeRepositoriesatMassiveScaleusingLanguageModeling.InThe10thWorkingConfer-enceonMiningSoftwareRepositories,pages207–216.IEEE.FranckBurlotandFranc¸oisYvon.2019.Usingmono-lingualdatainneuralmachinetranslation:asystem-aticstudy.arXivpreprintarXiv:1903.11437.FranckBurlotandFranc¸oisYvon.2018.Usingmono-lingualdatainneuralmachinetranslation:asystem-aticstudy.ArXiv,abs/1903.11437.JohnCocke.1969.Programminglanguagesandtheircompilers:Preliminarynotes.NewYorkUniver-sity.AnnaCurrey,AntonioValerioMiceli-Barone,andKen-nethHeaﬁeld.2017.Copiedmonolingualdataim-proveslow-resourceneuralmachinetranslation.InProceedingsoftheSecondConferenceonMachineTranslation,pages148–156.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.arXivpreprintarXiv:1810.04805.LiDongandMirellaLapata.2016.Languagetolog-icalformwithneuralattention.arXivpreprintarXiv:1601.01280.LiDongandMirellaLapata.2018.Coarse-to-ﬁnede-codingforneuralsemanticparsing.arXivpreprintarXiv:1805.04793.SergeyEdunov,MyleOtt,MichaelAuli,andDavidGrangier.2018.Understandingback-translationatscale.arXivpreprintarXiv:1808.09381.CatherineFinegan-Dollak,JonathanKKummerfeld,LiZhang,KarthikRamanathan,SeshSadasivam,RuiZhang,andDragomirRadev.2018.Improvingtext-to-sqlevaluationmethodology.arXivpreprintarXiv:1806.09029.JiataoGu,Z.Lu,HangLi,andV.Li.2016.Incorpo-ratingcopyingmechanisminsequence-to-sequencelearning.ArXiv,abs/1603.06393.CaglarGulcehre,OrhanFirat,KelvinXu,KyunghyunCho,LoicBarrault,Huei-ChiLin,FethiBougares,HolgerSchwenk,andYoshuaBengio.2015.Onus-ingmonolingualcorporainneuralmachinetransla-tion.arXivpreprintarXiv:1503.03535.JiaqiGuo,ZechengZhan,YanGao,YanXiao,Jian-GuangLou,TingLiu,andDongmeiZhang.2019.Towardscomplextext-to-sqlincross-domaindatabasewithintermediaterepresentation.InPro-ceedingsofthe57thAnnualMeetingoftheAsso-ciationforComputationalLinguistics,pages4524–4535.J.HerzigandJ.Berant.2020.Span-basedseman-ticparsingforcompositionalgeneralization.arXivpreprintarXiv:2009.06040.VuCongDuyHoang,PhilippKoehn,GholamrezaHaffari,andTrevorCohn.2018.Iterativeback-translationforneuralmachinetranslation.InPro-ceedingsofthe2ndWorkshoponNeuralMachineTranslationandGeneration,pages18–24.TadaoKasami.1966.Anefﬁcientrecognitionandsyntax-analysisalgorithmforcontext-freelan-guages.CoordinatedScienceLaboratoryReportno.R-257.DiederikPKingmaandJimmyBa.2014.Adam:Amethodforstochasticoptimization.arXivpreprintarXiv:1412.6980.WangLing,PhilBlunsom,EdwardGrefenstette,KarlMoritzHermann,Tom´aˇsKoˇcisk´y,FuminWang,andAndrewSenior.2016a.Latentpredictornetworksforcodegeneration.InProceedingsofthe54thAnnualMeetingoftheAssociationforCompu-tationalLinguistics(Volume1:LongPapers),pages599–609,Berlin,Germany.AssociationforCompu-tationalLinguistics.WangLing,EdwardGrefenstette,KarlMoritzHer-mann,Tom´aˇsKoˇcisk`y,AndrewSenior,FuminWang,andPhilBlunsom.2016b.Latentpredic-tornetworksforcodegeneration.arXivpreprintarXiv:1603.06744.YusukeOda,HiroyukiFudaba,GrahamNeubig,HideakiHata,SakrianiSakti,TomokiToda,andSatoshiNakamura.2015.Learningtogeneratepseudo-codefromsourcecodeusingstatisticalma-chinetranslation.InProceedingsofthe201530thIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering(ASE),ASE’15,pages574–584,Lincoln,Nebraska,USA.IEEEComputerSo-ciety.BorisTPolyakandAnatoliBJuditsky.1992.Ac-celerationofstochasticapproximationbyaverag-ing.SIAMjournaloncontrolandoptimization,30(4):838–855.PrajitRamachandran,PeterJLiu,andQuocVLe.2016.Unsupervisedpretrainingforsequencetose-quencelearning.arXivpreprintarXiv:1611.02683.RicoSennrich,BarryHaddow,andAlexandraBirch.2015.Improvingneuralmachinetranslationmodelswithmonolingualdata.arXivpreprintarXiv:1511.06709.PeterShaw,JakobUszkoreit,andAshishVaswani.2018.Self-attentionwithrelativepositionrepresen-tations.InProceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTech-nologies,Volume2(ShortPapers),pages464–468.AnuroopSriram,HeewooJun,S.Satheesh,andA.Coates.2018.Coldfusion:Trainingseq2seqmodelstogetherwithlanguagemodels.ArXiv,abs/1708.06426.FelixStahlberg,J.Cross,andVeselinStoyanov.2018.Simplefusion:Returnofthelanguagemodel.ArXiv,abs/1809.00125.AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin.2017.Attentionisallyouneed.InAdvancesinneuralinformationpro-cessingsystems,pages5998–6008.BailinWang,RichardShin,XiaodongLiu,Olek-sandrPolozov,andMatthewRichardson.2019.Rat-sql:Relation-awareschemaencodingandlinkingfortext-to-sqlparsers.arXivpreprintarXiv:1911.04942.Y.Wu,MikeSchuster,Z.Chen,QuocV.Le,Mo-hammadNorouzi,WolfgangMacherey,M.Krikun,YuanCao,Q.Gao,KlausMacherey,JeffKlingner,ApurvaShah,M.Johnson,X.Liu,L.Kaiser,S.Gouws,Y.Kato,TakuKudo,H.Kazawa,K.Stevens,G.Kurian,NishantPatil,W.Wang,C.Young,J.Smith,JasonRiesa,AlexRudnick,OriolVinyals,G.S.Corrado,MacduffHughes,andJ.Dean.2016.Google’sneuralmachinetranslationsystem:Bridgingthegapbetweenhumanandma-chinetranslation.ArXiv,abs/1609.08144.FrankFXu,ZhengbaoJiang,PengchengYin,BogdanVasilescu,andGrahamNeubig.2020.Incorporat-ingexternalknowledgethroughpre-trainingfornat-urallanguagetocodegeneration.arXivpreprintarXiv:2004.09015.ZiyuYao,DanielSWeld,Wei-PengChen,andHuanSun.2018.Staqc:Asystematicallyminedquestion-codedatasetfromstackoverﬂow.InProceedingsofthe2018WorldWideWebConferenceonWorldWideWeb,pages1693–1703.InternationalWorldWideWebConferencesSteeringCommittee.PengchengYin,BowenDeng,EdgarChen,BogdanVasilescu,andGrahamNeubig.2018.Learningtominealignedcodeandnaturallanguagepairsfromstackoverﬂow.InInternationalConferenceonMin-ingSoftwareRepositories,MSR,pages476–486.ACM.PengchengYinandGrahamNeubig.2017.Asyntacticneuralmodelforgeneral-purposecodegeneration.arXivpreprintarXiv:1704.01696.PengchengYinandGrahamNeubig.2018.Tranx:Atransition-basedneuralabstractsyntaxparserforse-manticparsingandcodegeneration.arXivpreprintarXiv:1810.02720.PengchengYinandGrahamNeubig.2019.Rerankingforneuralsemanticparsing.InProceedingsofthe57thAnnualMeetingoftheAssociationforCompu-tationalLinguistics,pages4553–4559.DanielHYounger.1967.Recognitionandparsingofcontext-freelanguagesintimen3.Informationandcontrol,10(2):189–208.JichuanZeng,XiVictoriaLin,S.Hoi,R.Socher,Caim-ingXiong,MichaelR.Lyu,andIrwinKing.2020.Photon:Arobustcross-domaintext-to-sqlsystem.ArXiv,abs/2007.15280.VictorZhong,CaimingXiong,andRichardSocher.2017.Seq2sql:Generatingstructuredqueriesfromnaturallanguageusingreinforcementlearning.arXivpreprintarXiv:1709.00103.ADatasetsWeused6datasetsintotal.DjangoincludesprogramsfromDjangowebframeworkandCoNaLacontainsdiversesetofintentsannotatedonpythonsnippetsgatheredfromStackoverﬂow.WikiSQL,GeoQuery,andATISincludenaturallanguagequestionsandtheircorrespondingSQLqueries.WikiSQLincludessingletablequerieswhileGeogQueryandATISrequiresqueriesonmorethanonetable.Finally,MagichasJavaclassimplementationofgamecardswithdifferentmethodsusedduringthegame.Table6summarisesalltheparalleldatasets.ForGoeQueryweusedquerysplitprovidedby(Finegan-Dollaketal.,2018).MonolingualCorpus:CoNaLacomeswith600KminedquestionsfromStackoverﬂow.Weignoredthenoisysourceintents/sentencesandjustusethepythonsnippets.TobecomparablewithXuetal.(2020),wealsoselectacorresponding100Ksubsetversionforcomparison.ForSQL,Yaoetal.(2018)automaticallyparsedStackOverﬂowquestionsrelatedtoSQLandprovidedasetcontaining120KSQLexamples.WeautomaticallyparsedtheSQLcodesandremovedsampleswithgrammaticalmistakes.WealsoﬁlteredsamplesnotstartingwithSELECTspecialtoken.AllamanisandSutton(2013)downloadedfullrepositoriesofindividualprojectsthatwereforkedatleastonce;duplicateprojectswereremoved.Werandomlysampled100KJavaexamplesfrommorethan14Kprojectsandusethatasmonolingualset.Table7summarisesallthemonolingualdatasets.ParallelCorpusLanguageTrainDevTestDjango(Odaetal.,2015)(link)Python1600010001805CoNaLa(Yinetal.,2018)(link)Python2,179200500WikiSQL(Zhongetal.,2017)(link)SQL56,355842115878ATIS(Finegan-Dollaketal.,2018)(link)SQL4812121347GeoQuery(Finegan-Dollaketal.,2018)(link)SQL536159182Magic(Lingetal.,2016a)(link)Java8,457446483Table6:Paralleldatasetsizes.WeﬁlteredoutMagicdatawithjavacodelongerthan350tokensinordertoﬁtinGPUmemory.MonolingualCorpusSourceSizePython(Yinetal.,2018)(link)Stackoverﬂow100KSQL(Yaoetal.,2018)(link)Stackoverﬂow52KJava(AllamanisandSutton,2013)(link)Github100kTable7:Monolingualdatasetsizes.BDevSetResultsDatasetBaseline(%)Baseline+TAE(%)CoNaLa32.43±0.2134.81±0.36ATIS5.79±0.297.23±0.45GeoQuery53.33±1.4752.58±0.70Django75.52±0.2178.56±0.39Magic42.26±1.4244.17±0.99WikiSQL85.92±0.0985.83±0.07Table8:DevsetexactmatchaccuracyonalldatasetsexceptCoNaLawhichusesBLEU.Wefollowed(YinandNeubig,2018)implementationofBLEUscorewhichcanbefoundhere.CArchitectureandExperimentDetailsWeselectedthedecoderlearningratebasedonlinearsearchover[1×10−3−2.5×10−5].Numberofdecoderlayershasbeendecidedbasedonsearchover{2,3,4,5,6}layersand4layerdecodershowssuperiorperformance(weusedasinglerunforhyperparameterselection).Eachmodelhas150MparametersoptimizedusingasingleGTX1080TiGPU.Withbatchsizeof16eachsteptakes1.7sonGeoQuerydataset(otherdatasetshaveverysimilarruntime).OnDjangoandCoNaLa,wefollowed(YinandNeubig,2018;Xuetal.,2020)onreplacingquotedvalueswitha“str#”where#isauniqueid.OnMagicdataset,wereplacedallnewline“\n”tokenswith“#”;following(Lingetal.,2016a),wesplittedCamel-Casewords(e.g.,classTirionFordring→classTirionFordring)andallpunctuationcharacters.WeﬁlteredoutMagicdatawithjavacodelongerthan350tokensinordertoﬁtinGPUmemory.DBack-TranslationandFusiondetailsForfusionwefollowequation1whereTMstandsfortranslationmodelandLMstandsforlanguagemodel.τlimitstheconﬁdenceofthelanguagemodelandλcontrolsthebalancebetweenTMandLM.ﬁgure4showstheperformanceofabaseTMtrainedon10%ofDjangotrainingdatawithtestexactmatchaccuracyof31.80overdifferentvaluesofλandτ.TheLMistrainedoverfullDjangotrainingset.logp(yti)=logpTM(yti)+λlogpLM(yti)=logpTM(yti)+λlogelti/τPielti/τ(1)Figure4:TestexactmatchaccuracyofTMleveragefusionwithdifferentparametersForback-translationweﬁrsttrainedthemodelusingthesamearchitectureexplainedaboveinthebackwarddirection.WeusedBLEUscoreasaevaluationmetricanduseearlystoppingbasedonthat.Usinggreedysearchwegeneratethecorrespondingsourceintentforeachcodesnippet.Intheend,thesyntheticdataismergedwiththebitextandtrainedaforwardmodel.EAdditionalQualitativeExamplesSource:callthefunctionlazywith2arguments:_string_concatandsix.text_type[six.text_type],substitutetheresultforstring_concat.Gold:string_concat=lazy(_string_concat,six.text_type)Baseline:string_concat=lazy(_concat_concat,six.text_type)TAE:string_concat=lazy(_string_concat,six.text_type)Note:wrongvarSource:gettranslation_functionattributeoftheobjectt,calltheresultwithanargumenteol_message,substitutetheresultforresult.Gold:result=getattr(t,translation_function)(eol_message)Baseline:result=getattr(t,translation_message)(eol_message)TAE:result=getattr(t,translation_function)(eol_message)Note:wrongvarSource:convertwhitespacecharactertounicodeandsubstitutetheresultforspace.Gold:space=unicode('')Baseline:space=unicode(character)TAE:space=unicode('')Note:wronglycopiedvariablenameSource:assigninteger2topartsifthirdelementofversionequalstozero,otherwiseassignitinteger3.Gold:parts=2ifversion[2]==0else3Baseline:parts[2]=2TAE:parts=2ifversion[2]==0else3Note:baselinefailedtocopyafewsourcetokens,andinsteadformedagrammati-callycorrectbutsemanticallyincorrectoutputCopymistakeexamplesSource:definethefunctiontimesincewithd,nowdefaultingtonone,reverseddefaultingtofalseasarguments.Gold:deftimesince(d,now=none,reversed=false):passBaseline:deftimesince(d=none,reversed(d=false):passTAE:deftimesince(d,now=none,reversed=false):passNote:unbalancedparanthesisandmultiplesemanticmistakes.Source:definethefunctionexecwith3arguments:_code_,_globs_settononeand_locs_settonone.Gold:defexec_(_code_,_globs_=none,_locs_=none):passBaseline:defexec(_code_,_globs=none,_locs_set()):passTAE:defexec(_code_,_globs_=none,_locs_=none):passNote:wrongvariablenameandgrammarmistakeSource:returnaninstanceofescapebytes,createdwithanargument,reusltofthecalltothefunctionbyteswithanarguments.Gold:returnescapebytes(bytes(s))Baseline:returnescapebytes(bytes(s).re(s)TAE:returnescapebytes(bytes(s))Note:extrasemanticallyincorrectpredictionsandunbalancedparathesesSource:callthefunctionblankoutwith2arguments:pandstr0,writetheresulttoout.Gold:out.write(blankout(p,'str0'))Baseline:out.write(blankout(p,'str0')TAE:out.write(blankout(p,'str0'))Note:unbalancedparathesesGrammarorsemanticmistakeexamplesTable9:Mistakeexamples1000001002003004005006007008009010011012013014015016017018019020021022023024025026027028029030031032033034035036037038039040041042043044045046047048049050051052053054055056057058059060061062063064065066067068069070071072073074075076077078079080081082083084085086087088089090091092093094095096097098099ACL-IJCNLP2021Submission***.ConﬁdentialReviewCopy.DONOTDISTRIBUTE.ADatasetsWeused6datasetsintotal.DjangoincludesprogramsfromDjangowebframeworkandCoNaLacontainsdiversesetofintentsannotatedonpythonsnippetsgatheredfromStackoverﬂow.WikiSQL,GeoQuery,andATISincludenaturallanguagequestionsandtheircorrespondingSQLqueries.WikiSQLincludessingletablequerieswhileGeogQueryandATISrequiresqueriesonmorethanonetable.Finally,MagichasJavaclassimplementationofgamecardswithdifferentmethodsusedduringthegame.Table1summarisesalltheparalleldatasets.ForGoeQueryweusedquerysplitprovidedby(?).MonolingualCorpus:CoNaLacomeswith600KminedquestionsfromStackoverﬂow.Weignoredthenoisysourceintents/sentencesandjustusethepythonsnippets.Tobecomparablewith?,wealsoselectacorresponding100Ksubsetversionforcomparison.ForSQL,?automaticallyparsedStackOverﬂowquestionsrelatedtoSQLandprovidedasetcontaining120KSQLexamples.WeautomaticallyparsedtheSQLcodesandremovedsampleswithgrammaticalmistakes.WealsoﬁlteredsamplesnotstartingwithSELECTspecialtoken.?downloadedfullrepositoriesofindividualprojectsthatwereforkedatleastonce;duplicateprojectswereremoved.Werandomlysampled100KJavaexamplesfrommorethan14Kprojectsandusethatasmonolingualset.Table2summarisesallthemonolingualdatasets.ParallelCorpusLanguageTrainDevTestDjango(?)(link)Python1600010001805CoNaLa(?)(link)Python2,179200500WikiSQL(?)(link)SQL56,355842115878ATIS(?)(link)SQL4812121347GeoQuery(?)(link)SQL536159182Magic(?)(link)Java8,457446483Table1:Paralleldatasetsizes.WeﬁlteredoutMagicdatawithjavacodelongerthan350tokensinordertoﬁtinGPUmemory.MonolingualCorpusSourceSizePython(?)(link)Stackoverﬂow100KSQL(?)(link)Stackoverﬂow52KJava(?)(link)Github100kTable2:Monolingualdatasetsizes.BDevSetResultsDatasetBaseline(%)Baseline+TAE(%)CoNaLa32.43±0.2134.81±0.36ATIS5.79±0.297.23±0.45GeoQuery53.33±1.4752.58±0.70Django75.52±0.2178.56±0.39Magic42.26±1.4244.17±0.99WikiSQL85.92±0.0985.83±0.07Table3:DevsetexactmatchaccuracyonalldatasetsexceptCoNaLawhichusesBLEU.Wefollowed(?)imple-mentationofBLEUscorewhichcanbefoundhere.CArchitectureandExperimentDetailsWeselectedthedecoderlearningratebasedonlinearsearchover[1×10−3−2.5×10−5].Numberofdecoderlayershasbeendecidedbasedonsearchover{2,3,4,5,6}layersand4layerdecoderarXiv:2101.00259v2  [cs.CL]  10 Jun 20212100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199ACL-IJCNLP2021Submission***.ConﬁdentialReviewCopy.DONOTDISTRIBUTE.showssuperiorperformance(weusedasinglerunforhyperparameterselection).Eachmodelhas150MparametersoptimizedusingasingleGTX1080TiGPU.Withbatchsizeof16eachsteptakes1.7sonGeoQuerydataset(otherdatasetshaveverysimilarruntime).OnDjangoandCoNaLa,wefollowed(??)onreplacingquotedvalueswitha“str#”where#isauniqueid.OnMagicdataset,wereplacedallnewline“\n”tokenswith“#”;following(?),wesplittedCamel-Casewords(e.g.,classTirionFordring→classTirionFordring)andallpunctuationcharacters.WeﬁlteredoutMagicdatawithjavacodelongerthan350tokensinordertoﬁtinGPUmemory.DBack-TranslationandFusiondetailsForfusionwefollowequation1whereTMstandsfortranslationmodelandLMstandsforlanguagemodel.τlimitstheconﬁdenceofthelanguagemodelandλcontrolsthebalancebetweenTMandLM.ﬁgure1showstheperformanceofabaseTMtrainedon10%ofDjangotrainingdatawithtestexactmatchaccuracyof31.80overdifferentvaluesofλandτ.TheLMistrainedoverfullDjangotrainingset.logp(yti)=logpTM(yti)+λlogpLM(yti)=logpTM(yti)+λlogelti/τPielti/τ(1)Figure1:TestexactmatchaccuracyofTMleveragefusionwithdifferentparametersForback-translationweﬁrsttrainedthemodelusingthesamearchitectureexplainedaboveinthebackwarddirection.WeusedBLEUscoreasaevaluationmetricanduseearlystoppingbasedonthat.Usinggreedysearchwegeneratethecorrespondingsourceintentforeachcodesnippet.Intheend,thesyntheticdataismergedwiththebitextandtrainedaforwardmodel.3200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299ACL-IJCNLP2021Submission***.ConﬁdentialReviewCopy.DONOTDISTRIBUTE.EAdditionalQualitativeExamplesSource:callthefunctionlazywith2arguments:_string_concatandsix.text_type[six.text_type],substitutetheresultforstring_concat.Gold:string_concat=lazy(_string_concat,six.text_type)Baseline:string_concat=lazy(_concat_concat,six.text_type)TAE:string_concat=lazy(_string_concat,six.text_type)Note:wrongvarSource:gettranslation_functionattributeoftheobjectt,calltheresultwithanargumenteol_message,substitutetheresultforresult.Gold:result=getattr(t,translation_function)(eol_message)Baseline:result=getattr(t,translation_message)(eol_message)TAE:result=getattr(t,translation_function)(eol_message)Note:wrongvarSource:convertwhitespacecharactertounicodeandsubstitutetheresultforspace.Gold:space=unicode('')Baseline:space=unicode(character)TAE:space=unicode('')Note:wronglycopiedvariablenameSource:assigninteger2topartsifthirdelementofversionequalstozero,otherwiseassignitinteger3.Gold:parts=2ifversion[2]==0else3Baseline:parts[2]=2TAE:parts=2ifversion[2]==0else3Note:baselinefailedtocopyafewsourcetokens,andinsteadformedagrammati-callycorrectbutsemanticallyincorrectoutputCopymistakeexamplesSource:definethefunctiontimesincewithd,nowdefaultingtonone,reverseddefaultingtofalseasarguments.Gold:deftimesince(d,now=none,reversed=false):passBaseline:deftimesince(d=none,reversed(d=false):passTAE:deftimesince(d,now=none,reversed=false):passNote:unbalancedparanthesisandmultiplesemanticmistakes.Source:definethefunctionexecwith3arguments:_code_,_globs_settononeand_locs_settonone.Gold:defexec_(_code_,_globs_=none,_locs_=none):passBaseline:defexec(_code_,_globs=none,_locs_set()):passTAE:defexec(_code_,_globs_=none,_locs_=none):passNote:wrongvariablenameandgrammarmistakeSource:returnaninstanceofescapebytes,createdwithanargument,reusltofthecalltothefunctionbyteswithanarguments.Gold:returnescapebytes(bytes(s))Baseline:returnescapebytes(bytes(s).re(s)TAE:returnescapebytes(bytes(s))Note:extrasemanticallyincorrectpredictionsandunbalancedparathesesSource:callthefunctionblankoutwith2arguments:pandstr0,writetheresulttoout.Gold:out.write(blankout(p,'str0'))Baseline:out.write(blankout(p,'str0')TAE:out.write(blankout(p,'str0'))Note:unbalancedparathesesGrammarorsemanticmistakeexamplesTable4:Mistakeexamples