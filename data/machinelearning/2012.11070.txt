2
2
0
2

y
a
M
2
2

]
I

N
.
s
c
[

2
v
0
7
0
1
1
.
2
1
0
2
:
v
i
X
r
a

1

Energy Efﬁcient Federated Learning over
Heterogeneous Mobile Devices via Joint Design
of Weight Quantization and Wireless
Transmission

Rui Chen, Student Member, IEEE Liang Li, Member, IEEE Kaiping Xue, Senior Member, IEEE Chi
Zhang, Member, IEEE Miao Pan, Senior Member, IEEE and Yuguang Fang, Fellow, IEEE

Abstract—Federated learning (FL) is a popular collaborative distributed machine learning paradigm across mobile devices. However,
practical FL over resource constrained mobile devices confronts multiple challenges, e.g., the local on-device training and model
updates in FL are power hungry and radio resource intensive for mobile devices. To address these challenges, in this paper, we
attempt to take FL into the design of future wireless networks and develop a novel joint design of wireless transmission and weight
quantization for energy efﬁcient FL over mobile devices. Speciﬁcally, we develop ﬂexible weight quantization schemes to facilitate
on-device local training over heterogeneous mobile devices. Based on the observation that the energy consumption of local computing
is comparable to that of model updates, we formulate the energy efﬁcient FL problem into a mixed-integer programming problem where
the quantization and spectrum resource allocation strategies are jointly determined for heterogeneous mobile devices to minimize the
overall FL energy consumption (computation + transmissions) while guaranteeing model performance and training latency. Since the
optimization variables of the problem are strongly coupled, an efﬁcient iterative algorithm is proposed, where the bandwidth allocation
and weight quantization levels are derived. Extensive simulations are conducted to verify the effectiveness of the proposed scheme.

Index Terms—Federated learning over mobile devices, Weight quantization, Device heterogeneity

(cid:70)

1 INTRODUCTION

D UE to the incredible surge of mobile data and the growing

computing capabilities of mobile devices, it becomes a trend
to apply deep learning (DL) on these devices to support fast
responsive and customized intelligent applications. Recently, fed-
erated learning (FL) has been regarded as a promising DL solution
to providing an efﬁcient, ﬂexible, and privacy-preserving learning
framework over a large number of mobile devices. Under the FL
framework [1], each mobile device executes model training locally
and then transmits the model updates, instead of raw data, to an
FL server. The server will then aggregate the intermediate results
and broadcast the updated model to the participating devices. Its
potential has prompted wide applications in various domains such
as keyboard predictions [2], physical hazards detection in smart
home [3], health event detection [4], etc. Unfortunately, it also
faces many signiﬁcant challenges when deploying FL over mobile
devices in practice. First, although mobile devices are gradually
equipped with artiﬁcial intelligence (AI) computing capabilities,

• R. Chen and M. Pan are with the Department of Electrical and Com-
puter Engineering, University of Houston, Houston, TX, 77204 (e-mail:
rchen19@uh.edu, mpan2@uh.edu).
L. Li is with the School of Computer Science, Beijing University of
Posts and Telecommunications, Beijing 100876, P. R. China (e-mail:
liliang1127@bupt.edu.cn).

•

• K. Xue is with the School of Cyber Security, and C. Zhang is with the
School of Information Science and Technology, University of Science
and Technology of China, China, Hefei, 230027, P. R. China (e-mail:
kpxue@ustc.edu.cn, chizhang@ustc.edu.cn).
Y. Fang is with the Department of Electrical and Computer Engi-
neering, University of Florida, Gainesville, Florida 32611 (e-mail:
fang@ece.uﬂ.edu).

•

the limited resources (e.g., battery power, computing and stor-
age capacity) restrain them from training deep and complicated
learning models at scale. Second, it is unclear how to establish an
effective wireless network architecture to support FL over mobile
devices. Finally, the power-hungry local computing and wireless
communications during iterations in FL may be too much for the
power-constrained mobile devices to afford.

The mismatch between the computing and storage require-
ments of DL models and the limited resources of mobile devices
becomes even more challenging due to the increasing complexity
of the state-of-art DL models. To address this issue, one of the
most popular solutions is to compress a trained network [5]–
[7]. Han et al. [6] successfully applied multiple compression
methods, e.g., pruning and quantization, to several large-scale
neural networks (e.g., AlexNet and VGG-16). These compression
techniques help reduce model complexity by multiple orders of
magnitude and speed up model inference on mobile devices. How-
ever, on-device training is less explored and more complicated
than its inference counterpart. Some pioneering works [8], [9]
have made efforts on quantizing the model parameters to make
it possible to conduct computationally efﬁcient on-device training.
Nevertheless, most existing compressed on-device learning frame-
works and the associated convergence analysis for the potential
on-device training only consider the case of a single mobile
device. A few works, such as [10], have considered quantized
on-device training in distributed learning settings. However, they
assign the same quantization strategy for different mobile devices.
In practice, FL may encompass massively distributed mobile
devices that are highly heterogeneous in computing capability and

 
 
 
 
 
 
communication conditions. Thus, it is in dire need to develop a
ﬂexible quantization scheme catering to the heterogeneous devices
and investigate the impacts of such heterogeneity on learning
performance.

Besides the on-device training for local computing, the energy
consumption for FL over mobile devices also includes the wireless
communications for the intermediate model update exchanges.
Particularly, with the advance of computing hardware and future
wireless communication techniques, like 5G and beyond (5G+)
[11], we have observed that the energy consumption for local com-
puting in FL is comparable to that for the wireless transmissions
on mobile devices. For instance, the energy consumption of local
computing (e.g., 42.75J for one Tesla P100 GPU of one training
iteration for Alexnet with batch size of 128) is comparable to that
of today’s wireless communications (e.g., 38.4J for transmitting
240MB Alexnet model parameters at 100 Mbps data rate [12]).
Thus, a viable design of the energy efﬁcient FL over mobile
devices has to consider the energy consumption of both “working”
(i.e., local computing) and “talking” (i.e., wireless communica-
tions). However, most existing works in wireless communities
have mainly conducted the radio resource allocation under the
FL convergence constraints [13]–[15], while neglecting the energy
consumption in learning. Moreover, among the previous works,
the targeted learning models are either relatively simple (i.e., with
convex loss functions) or shallow networks [13]–[16], which is
inconsistent with the current trend of the overparameterized DL
models. On the other hand, most efforts in the machine learning
communities have focused on communication efﬁcient FL algo-
rithmic designs, such as compressing the size of the model updates
or reducing the update frequency during the training phase. The
basic assumption is that the wireless transmission data rate is slow,
which results in the bottleneck to support complicated learning
models over mobile devices. Therefore, the goal of such designs is
to reduce the number of communications in model updates without
considering the advance of wireless transmissions.

Fortunately, the future wireless transmissions (e.g., 5G/6G
cellular, WiFi-6 or future version of WiFi), featured by very high
data rate (1 Gbps or more [11]) with ultra low latency of 1 ms or
less for massive number of devices, can be leveraged to relieve the
communication bottleneck with proper design. Furthermore, the
multi-access edge computing in the future networks enhances the
computing capabilities at the edge networks, and hence provides
an ideal architecture to support viable FL.

Motivated by the aforementioned challenges (i.e., inefﬁcient
on-device training and large overall energy consumption in FL
training), in this paper, we develop a wireless transmission and
on-device weight quantization co-design for energy efﬁcient FL
over heterogeneous mobile devices. We aim to 1) facilitate ef-
ﬁcient on-device training on heterogeneous local devices via a
ﬂexible quantization scheme, and 2) minimize the overall energy
consumption during the FL learning process by considering the
learning performance and training latency. Based on the derived
convergence analysis, we formulate the energy minimization prob-
lem to determine the optimal quantization strategy and bandwidth
allocations. Our major contributions are summarized as follows.

• We propose a novel efﬁcient FL scheme over mobile
devices to reduce the overall energy consumption in com-
munication and computing. Brieﬂy, subject to their current
computing capacities, the participating mobile devices are
allowed to compress the model and compute the gradients

2

of the compressed version of the models. Meanwhile,
for a given training time threshold, the network resource
allocation is to minimize the total computing and commu-
nication energy cost in FL training.

• To facilitate on-device training for FL over heterogeneous
mobile devices, weight quantization is employed to best
utilize the limited computing capacities by representing
model parameters with different bit-widths. We further
provide the theoretical analysis of the convergence rate of
FL with quantization and obtain a closed-form expression
for the novel convergence bound in order to explore the
relationship between the weight quantization error, and the
performance of the FL algorithm.

• Based on the obtained theoretical convergence bound, the
energy minimization problem in FL training is formulated
as a mixed-integer nonlinear problem to balance the com-
puting and communication costs by jointly determining
the bandwidth allocation and weight quantization levels
for each mobile device. An efﬁcient iterative algorithm is
proposed with low complexity, in which we derive new
closed-form solutions for the bandwidth allocation and
weight quantization levels.

• We evaluate the performance of our proposed solution
via extensive simulations using various open datasets and
models to verify the effectiveness of our proposed scheme.
Compared with different schemes, our proposed method
shows signiﬁcant superiority in terms of energy efﬁciency
for FL over heterogeneous devices.

The rest of this paper is organized as follows. The related
work is discussed in Section 2. In Section 3, a detailed description
of the system model is presented and the convergence analysis
of the proposed FL with weight quantization is also discussed.
The energy minimization problem and joint quantization selection
and bandwidth allocation algorithm are presented in Section 4.
In Section 5, the feasible solutions from the real datasets are
analyzed. The paper is concluded in Section 6.

2 RELATED WORK
2.1 Cost-efﬁcient design for FL over wireless networks

Recognizing that training large-scale FL models over mobile
devices can be both time and energy consuming tasks, several
research efforts have been made on decreasing these costs via
device scheduling [17], network optimization [16] and resource
utilization optimization [14], [18]–[23]. In particular, the resource
allocation for optimizing overall FL energy efﬁciency was studied
in [20]–[23]. Mo et al. in [22] have designed the computing
and communication resources allocation to minimize the energy
consumption while only considering the CPU models for mobile
devices. Zeng et al. [20] proposed to partition the computing
workload between CPU-GPU to improve the computing energy
efﬁciency. However, their resource allocation strategies are for
particular (non-optimal) model parameters (i.e., weight quantiza-
tion levels in this paper). Thus, they overlook the opportunities
to ﬁrst reduce the costs in learning (i.e., model quantization in
this paper) before utilizing the available resources. Close to our
work, Li et al. [23] considered to sparsify the model size before
transmission to improve communication efﬁciency and determine
heterogeneity-aware gradient sparsiﬁcation strategies. However,
they neglect the mismatch between the computing/storage require-
ment for on-device training and the limited computing resources

3

To train the FL model in low precision, we deﬁne a quantiza-
tion function Q(·) to convert a real number w into a quantized ver-
sion ˆw = Q(w). We use the same notation for quantizing vectors
since Q acts on each dimension of the vector independently in the
same manner. Moreover, we employ stochastic rounding (SR) [7]
in our proposed model and analyze its convergence properties.
SR, also known as unbiased rounding, possesses the important
property: E[Q(w)] = w. This property avoids the negative effect
of quantization noise, which is useful for the theory of non-convex
setting [26]. For each component wn of a vector w, the function
Q(·) converts the data type from 32-bit into q-bit, deﬁned as:

Q(wn) = s · sgn(wn) ·

(cid:40)Ia+1, w.p.
w.p.

Ia,

|wn|
s∆q
Ia+1
∆q

− Ia
∆q
− |wn|
s∆q

,

(1)

where sgn(·) represents the sign function, s = (cid:107)w(cid:107)∞ denotes the
scaling factor, the index k satisﬁes Ia ≤ |wn|
s ≤ Ia+1, quantiza-
tion set Sw = {−IA, · · · , I0, · · · , IA} with A = 2q−1 − 1, 0 =
I0 ≤ I1 ≤ · · · ≤ IA are uniformly spaced, and ∆q denotes
the quantization resolution as ∆q = Ia+1 − Ia = 1/(2q − 1).
Smaller resolution leads to a smaller gap and keeps as much
information as the original weight, while it has higher memory
requirements. In practice, the bit-width for the weight quantization
can be extremely small, like 2 or 3 bits without notable perfor-
mance degradation. Other parameters, such as the weight gradient
calculations and updates, are applied to capture accumulated
small changes in stochastic gradient descent (SGD). In contrast,
quantization makes them insensitive to such information and may
impede convergence performance during training. Therefore, we
keep a higher precision for the gradients than the weights and
inputs so that the edge server aggregates the local gradients and
updates the global model in full precision.

3.2 FL with ﬂexible weight quantization

We consider a mobile edge network consisting of one edge
server and a set N = {1, 2, · · · , N } of distributed mobile
devices, collaboratively training a DNN model through FL frame-
work, which is depicted in Fig. 1. Each mobile device i is
equipped with a single antenna and has its own dataset Di
with data size |Di|. The data is collected locally by the mobile
device i itself. Generally, each learning model has a particular
loss function fj(w) with the parameter vector w for each data
sample j. The loss function represents the difference of the
model prediction and groundtruth of the training data. Thus, the
loss function on the local data of mobile devices i is given as
Fi(w) := 1
j=1 fj(w). The training objective of the shared
|Di|
model is to collaboratively learn from all the participating mobile
devices, formulated as follows:

(cid:80)|Di|

min
w∈Rd

F (w) =

N
(cid:88)

i=1

πiFi(w),

(2)

i=1 |Di| and (cid:80)N

where d denotes the total number of the DNN model param-
eters and πi is the weight of the n-th device such that πi =
|Di|/ (cid:80)N
i=1 πi = 1. Given the sensitive nature of
the users’ data, each mobile device keeps its data locally instead
of uploading its data to the edge server. An FL framework [1]
is adopted to solve problem (2), named FedAvg, that allows
the users to update the model to the edge server periodically.
Let r be the r-th training iteration in FL. In FedAvg, the edge
server ﬁrst broadcasts the latest model ¯wr to all the devices.

Fig. 1. Federated learning framework with weight quantization.

on mobile devices. Based on the example illustrated in Section 1,
on-device computing consumes more energy than model update
transmission. Hence, different from [23], this paper leverages the
quantization method for on-device training instead of wireless
transmission only.

2.2 On-device training with low precision

Various works have been developed for on-device learning to
reduce the model complexities via low precision operation and
storage requirements [24]. In the extreme case, the weights and
activations are represented in one bit, called Binary Neural Net-
works (BNN) [25], while the performance degrades signiﬁcantly
in large DNNs. For weight quantization, the prior work such as
“LQ-Net” in Zhang et al. [8] quantized weights and activations
such that the inner products can be computed efﬁciently with
bit-wise operations, performing in the case of single machine
computation. Similar to our work, Fu et al. [9] considered the
weight quantization for local devices in the distributed learning
setting and proposed to quantize activations via estimating Weibull
distributions. However, they did not consider optimization for
energy efﬁciency in FL training. Besides, they assigned the same
quantization level across different participating devices, which
limited the performance when facing the challenges of device
heterogeneity. It left the impact of ﬂexible quantization schemes
on the learning model accuracy as an open problem, which
will be addressed in this work. Unlike these existing works, a
mobile-compatible FL algorithm with ﬂexible weight quantization
is introduced in our proposed model. By jointly considering
the heterogeneous computing and communication conditions, we
formulate the overall FL energy (computing + transmissions)
minimization problem to seek for the optimal weight quantization
levels and bandwidth allocation across multiple mobile devices.

3 FL WITH FLEXIBLE WEIGHT QUANTIZATION

3.1 Preliminary of Weight Quantization

In this subsection, we introduce the related concepts about
weight quantization for on-device training. Quantization is an
attractive solution to implementing FL models on mobile devices
efﬁciently. It represents model parameters, including the weights,
feature maps, and even gradients, with low-precision arithmetic
(e.g., 8-bit ﬁxed-point numbers). When the model parameters are
stored and computed with low-bitwidth, the computational units
and memory storage to perform the operations during on-device
training are much smaller than the full-precision counterparts,
resulting in energy reduction during on-device training.

Algorithm 1 Flexible Weight Quantizated FL (FWQ-FL)
Input: η = learning rate; Q(·) = quantization function; initial
¯w0; a mini-batch size M ; a number of local SGD iterations
H; a number of training iterations R

Output: ¯wR
1: for r = 0, · · · , R − 1 do
2:

Edge server sends ¯wr to the set of participating mobile
devices N
for each mobile device i ∈ N in parallel do
Sample mini-batch data set {(cid:101)xm, (cid:101)ym}M
m=1 from Di
Compute the mini-batch stochastic gradient (cid:79) (cid:101)fi(wr
i )
Update the model parameters
(cid:16)
wr+1
wr
the low precision}
if ((r + 1) mod H) = 0 then
to the FL server.

(cid:17)
i − ηi(cid:79) (cid:101)fi (wr
i )

{store the weight in

Send wr+1

i ← Q

i

end if
end for
Edge server updates the global model ¯wr+1 as follows
¯wr+1 ← (cid:80)N
precision}

i=1 πiwr+1

{update the weight in the high

i

3:
4:

5:
6:

7:
8:
9:
10:
11:
12:

13: end for

i

i

Second, every device i ∈ N performs H mini-batch SGD steps in
parallel, obtains and transmits its intermediate local model wr+H
to the edge server. After that, the edge server will update the
model based on aggregated results from the mobile devices, i.e.,
¯wr+H = (cid:80) πiwr+H
. This procedure repeats until FL converges.
Targeting at the energy-efﬁcient FL training over mobile de-
vices, we propose a ﬂexible weight quantization (FWQ) scheme
for heterogeneous mobile devices. After mobile devices receive
the shard model from the edge server, they ﬁrst quantize and store
the model to satisfy their current storage budget. Unlike the prior
works that maintain the same quantization strategy across all the
participating devices, FWQ considers device heterogeneity and
allows the mobile devices to perform weight quantization with
different bit-widths of qi during on-device training and transmit
the model updates in more bits. Note that the weights and gradients
at the server side remain in full precision operations to avoid
further model performance degradation. A pseudo-code of our
FWQ algorithm is presented in Alg. 1.

3.3 Convergence Analysis of FL with FWQ

Before we discuss the convergence of Alg. 1, we make the
following assumptions on the loss function, which are com-
monly used for the analysis of SGD approach under the dis-
tributed/federated learning settings [27], [28].

4

Assumption 1 indicates that the local loss functions Fi and the
aggregated loss function F are also L-smooth. The unbiasedness
and bounded variance of stochastic gradients in Assumption 2 are
customary for non-convex analysis of SGD.
(cid:113) M

Theorem 1. Let the learning rate η be
R . If Assumptions 1-2
hold, the average-squared gradient after R iterations is bounded
as follow,

1
R

≤

R−1
(cid:88)

E (cid:107)(cid:79)F ( ¯wr)(cid:107)2
2

t=0
4(E (cid:2)F ( ¯w0)(cid:3) − F (cid:63))
M R

√

+

6HLτ
√
M R

√

+

dLG

N
(cid:88)

i=1

π2
i δi,

≤ O(

H + 1
√
M R

√

) + O(

d

N
(cid:88)

i=1

π2
i δi),

(3)

(4)

where δi = s∆bi is the quantization noise. τ = (cid:80)N
F (cid:63) is the global minimum of F.

i=1 π2

i τ 2

i , and

Proof. Please refer to the detailed proof in Appendix A in the
separate supplemental ﬁle.

Here, the average expected squared gradient norm charac-
terizes the convergence rate due to the non-convex objective in
modern learning models [28]–[30]. From Theorem 1, we can
observe that the proposed model admits the same convergence rate
as parallel SGD in the sense that both of them attain the asymp-
). Weight quantization makes FL
totic convergence rate O(
converge to the neighborhood of the optimal solution without
affecting the convergence rate. The limit point of the iterates is
related to the quantization noise δi. If the quantization becomes
more ﬁne-grained (i.e., by increasing the number of bits), the
model performance will approach the model with full precision
ﬂoating point.

1√

M R

4 OPTIMIZATION FOR ENERGY EFFICIENT FWQ

Motivated by the above discussion, the quantization levels
{qi}N
i=1 and the numbers of local SGD iterations, H, act as
critical parameters of FL training performance (i.e., model con-
vergence rate). Besides, these strategies also greatly impact the
energy consumption of mobile devices since they affect the total
communication rounds and computing workload per round. In this
section, we formulate the energy efﬁcient FWQ problem (EE-
FWQ) under model convergence and training delay guarantee.
We develop ﬂexible weight quantization and bandwidth allocation
to balance the trade-off between computing and communication
energy of mobile devices in FL training. We start with discussion
on the computing and communication energy model, followed by
problem formulation and solution.

Assumption 1. All the loss functions fj are differentiable and
their gradients are L-Lipschitz continuous in the sense of l2-norm:
for any x and y ∈ Rd, (cid:107)(cid:79)fj(x) − (cid:79)fj(y)(cid:107)2 ≤ L (cid:107)x − y(cid:107)2.

4.1 Energy Model

4.1.1 Computing Model

Assumption 2. Assume that (cid:101)fi is randomly sampled from the
i-th mobile device local loss functions. For local device i, its
stochastic gradient is an unbiased estimator and its variance:
E||(cid:79) (cid:101)fi(wr) − (cid:79)Fi(wr)||2
i . Thus, the a mini-batch size
M of gradient variance is given as τ 2
i /M and its second moment
is E

≤ G2, for any i = 1, · · · , N .

2 ≤ τ 2

(cid:13)
(cid:13)
2
(cid:79) (cid:101)fi(wr)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

Here, we consider the GPU computing model

instead of
the CPU model, for two reasons. First, CPUs cannot support
relatively large and complicated model training tasks. Second,
GPUs are more energy efﬁcient than CPUs for on-device training
and are increasingly integrated into today’s mobile devices (e.g.,
Google Pixel). The GPU based training makes computing energy
consumption comparable to that of communications in FL. Noted

that the local computing of mobile device i involves the data
fetching in GPU memory modules and the arithmetic in GPU
core modules, where the voltage and frequency of each module
are independent and conﬁgurable:

1) GPU runtime power model of mobile device i is modeled

as a function of the core/memory voltage/frequency [31],
)2f core
i

i + ζ mem
i

pcp
i = pG0

+ ζ core
i

(V core
i

f mem
i

,

(5)

i

where pG0
is the summation of the power consumption unrelated
to the GPU voltage/frequency scaling; V core
denote
the GPU core voltage, GPU core frequency, and GPU memory
frequency, respectively; ζ mem
are the constant coefﬁ-
cients that depend on the hardware and arithmetic for one training
iteration, respectively.

and ζ core
i

, f mem
i

, f core
i

i

i

2) GPU execution time model of mobile device i with quanti-

zation level qi is formulated as

T comp
i

(qi) = t0

i +

c1(qi)θmem
i
f mem
i

+

c2(qi)θcore
i
f core
i

,

(6)

where t0
i represents the other component unrelated to training task;
and θcore
θmem
denote the number of cycles to access data from
i
i
the memory and to compute one mini-batch size of data samples,
respectively, which are measured on a platform-based experiment
in this paper. Due to the weight quantization, the number of
cycles for data fetching and computing are reduced with scaling
c1(qi) and c2(qi), respectively. For simplicity, we assume that the
number of cycles for data fetching and computing scales, c1(qi)
and c2(qi), are linear functions of data bit-width qi, respectively.
This is reasonable since the quantization reduces the bit-widths,
and the data size scales linearly to the bit representation [32].
the
With the above GPU power and performance model,
local energy consumed to pass a single mini-batch SGD with
quantization strategy qi of the i-th mobile device is the product
of the runtime power and the execution time, i.e.,

Ecomp
i

(qi, H) = H · pcp
i

· T cp

i (qi).

(7)

4.1.2 Communication Model

We consider orthogonal frequency-division multiple access
(OFDMA) protocol for devices to upload their local results to
the edge server. The total channel bandwidth is bounded by
Bmax and Bi is denoted as the bandwidth allocated to device i
where Bi satisﬁes (cid:80)N
i=1 Bi ≤ Bmax. As a result, the achievable
transmission rate (bit/s) of mobile device i can be calculated as
(cid:18)

(cid:19)

γi = Bi ln

1 +

hipcm
i
N0

,

(8)

i

where N0 represents the noise power, and pcm
is the transmission
power. Here, hi denotes the average channel gain of the mobile
device i during the training task of FWQ-FL. The dimension of the
gradient vector gi is ﬁxed for a given model so that the overall data
size to transmit the gradient vector is the same for all the mobile
devices, which is denoted by Dg. Then, the communication time
to transmit Dg for mobile device i is

T cm
i

(Bi) =

Dg
γi

=

(cid:16)

Dg
1 + hipcm
i
N0

(cid:17) .

Bi ln

(9)

Thus, the communication energy consumption of mobile device i
can be derived as

4.2 Problem Formulation

Considering the computing capabilities of different mobile
devices vary, we formulate the optimization problem to minimize
the total energy consumption during the training process as

5

min
H,K,(cid:15)q,
q,B

N
(cid:88)

i=1

K (Ecomm
i

(Bi) + Ecomp

i

(qi, H))

s.t.

c3(qi)Ui ≤ Ci, ∀i ∈ N ,

N
(cid:88)

A3

π2
i δi ≤ (cid:15)q,

(11a)

(11b)

(11c)

(11d)

i=1
A1H + A2
√
M HK

N
(cid:88)

+ A3

π2
i δi ≤ (cid:15),

K (HT cp

i=1
i + T comm
i

max
i
N
(cid:88)

Bi ≤ Bmax,

i=1
Bi > 0, qi ∈ Q, ∀i ∈ N ,
H ∈ Z+, 0 ≤ (cid:15)q ≤ (cid:15),

) ≤ Tmax,

(11e)

(11f)

(11g)

(11h)

where K represents the total number of communication rounds,
Ui, and Ci represent the learning model size (MB) stored in
full precision and the memory capacity in mobile device i,
respectively. c3(qi) is the ratio of the bit-width to full precision.
q = [q1, · · · , qN ] and B = [B1, · · · , BN ] are the quantization
and bandwidth allocation strategies of mobile devices, respec-
tively. Constraint (11b) states the model size stored on mobile
device i does not exceed its storage capacity. The constraint (11c)
controls the average quantization error over participating devices
as small as possible. The constraints in (11e) ensures the entire
training time can be completed within predeﬁned deadline Tmax.
In constraint (11f), the bandwidth allocation to the mobile devices
must not exceed the channel bandwidth available to the edge
server. Constraints (11g) and (11h) indicate that variables take the
values from a set of non-negative numbers. Bit representation set
Q is deﬁned as a power of 2, ranging from 8 to 32 bits, which
is a standard-setting and hardware friendly [33]. The number
of communication rounds K is determined by the FL model
convergence. Based on the results in Theorem 1, we set upper
bound to satisfy the convergence constraint as in (11d), where A1,
A2 and A3 are coefﬁcients1 used to approximate the big-O in
Eqn. (3). Furthermore, given the constraint (11c), we can rewrite
the (11d) as

A1H + A2
√
M HK

+ (cid:15)q ≤ (cid:15).

(12)

For the relaxed problem, if any feasible solution H, (cid:15)q, and K
satisﬁes constraint (12) with inequality, we note that the objective
function is a decreasing function of K. Thus, for optimal K, the
constraint (12) is always satisﬁed with equality, and we can derive
K from this equality as

K(H, (cid:15)q) =

(A1H + A2)2
M H((cid:15) − (cid:15)q)2 ,

(13)

From (13), we observe that K(H, (cid:15)q) is a function of H that ﬁrst
decreases and then increases, which implies that too small and too
large H all lead to high communication cost and that there exists

Ecomm

i

(Bi) =

Dgpcm
i
(cid:16)
1 + hipcm
i
N0

(cid:17) .

Bi ln

(10)

1. These coefﬁcients can be estimated by using a small sampling set of

training experimental results.

an optimal H. Besides, a large (cid:15)q, which results from aggressive
quantization levels (small bit-widths), also hinders the learning
efﬁciency since it requires more communication rounds to recover
the learning accuracy. In light of this, local update and weight
quantization levels should be carefully determined to minimize
the overall energy consumption for FWQ-FL.

For the ease of analysis, we simplify the description of the
i qi +

GPU time model as a linear function of qi, i.e., T cp
c1
i . By substituting (13) into its expression, we obtain

i (qi) = c2

Proof. Please refer to the detailed proof in Appendix B in the
separate supplemental ﬁle.

Noted that it can be veriﬁed that the objective function Ψ(H)
in (17) is convex. The optimal H (cid:63) can be obtain by setting the
following ﬁrst-order derivative into zero,

6

dΨ(H)
dH

= 2A2

0HEcp(q) + A2
1Ecm(B)
A2
H 2

.

−

0Ecm(B) + 2A0A1Ecp(q)

(18)

+ Hpcp

i (c2

i qi + c1
i )

(cid:19)

It is a cubic equation of H and can be solved analytically via
Cardano formula [34]. Therefore, for the ﬁxed values of q and B,
we have a unique real solution of H in closed form as follows

N
(cid:88)

i=1

(A0H + A1)2
M H((cid:15) − (cid:15)q)2

(cid:18) pcm
i Dg
γi

min
H,(cid:15)q,
q,B

s.t.

(11b) − (11h).

(14)
The relaxed problem above is a mixed-integer non-linear program-
ming. It is intractable to deal with due to the multiplicative form of
the integer variables (H and q) and continuous variables ((cid:15)q and
B) in both the objective function and constraints. In the following,
we develop an iterative algorithm with low complexity to seek
feasible solutions.

4.3 Iterative algorithm for EE-FWQ

The proposed iterative algorithm divides the original problem
(14) into two sub-problems: 1) Local update and quantization error
optimization (for H and (cid:15)q); 2) Joint weight quantization selection
and bandwidth allocation (for q and B), which can be solved in an
iterative manner. For the two sub-problems, we are able to derive
the closed-form solutions for local updates, bandwidth allocation
and weight quantization levels. The details are presented in the
following subsections.

4.3.1 Local update and quantization error optimization

To obtain the optimal strategies for FWQ, we ﬁrst relax H as a
continuous variable for theoretical analysis, which is later rounded
back to the nearest integer. Given B and q, problem (14) is written
as follows

min
H,(cid:15)q

s.t.

(A0H + A1)2
M H((cid:15) − (cid:15)q)2 (Ecm(B) + HEcp(q))
(A0H + A1)2
M H((cid:15) − (cid:15)q)2 ≤

Tmax
(Bi) + HT cp

T cm
i

i (qi)

(cid:15)q ≥ (cid:15)min
,
0 ≤ (cid:15)q ≤ (cid:15), H ≥ 0,

q

(15a)

, ∀i ∈ N ,

A3π2
2qi −1 , Ecm(B) = (cid:80)N
i s

i=1 Ecm

i

q = (cid:80)N
i=1 Ecp

where (cid:15)min
i=1
Ecp(q) = (cid:80)N
i (qi).
Theorem 2. The optimal (cid:15)(cid:63)

q in problem (14) satisﬁes
q = (cid:15)min
(cid:15)(cid:63)
q

,

and the optimal H (cid:63) is given by

min
H

Ψ(H) (cid:44)

(A0H + A1)2(Ecm(B) + HEcp(q))
M H((cid:15) − (cid:15)min

)2

q

s.t. Hmin ≤ H ≤ Hmax,

H =

(cid:115)(cid:114)

3

+

α
3

,

α3β
27

+

β2
4

−

α3
27

−

β
2

+

(cid:114)

(cid:115)
3

−

α3β
27

+

β2
4

−

α3
27

−

β
2

(19)

with α = A0Ecm(B)+2A1Ecp(q)

2A0Ecp(q)

, and β = − A2
2A2

1Ecm(B)
0Ecp(q) .

4.3.2 Joint weight quantization selection and bandwidth
allocation

Given the updated H, (cid:15)q, the optimal quantization levels q(cid:63)
and the bandwidth allocation B(cid:63) can be obtained by solving the
following problem,

min
q,B

s.t.

K(H, (cid:15)q)

N
(cid:88)

i=1

pcm
i α1
i
Bi

+ Hpcp
i

· (c2

i qi + c1
i )

(11b), (11c), (11f ), (11g),
α1
i
Bi

i qi + c1

+ H(c2

i ) ≤

Tmax
K(H, (cid:15)q)

, ∀i ∈ N .

(20a)

(20b)

(20c)

Based on the observation of problem (20), it is clear that
problem (20) is a mixed-integer non-linear problem. Besides, the
integer variable qi and a fractional form of continuous variable
Bi are linearly coupled in constraint (20c), which makes the
optimization problem difﬁcult to tackle. To address the above
issues, we ﬁrst introduce a new variable (cid:101)q = log2(q) and its
ﬁnite set can be deﬁned as (cid:101)Q = {1, 2, 3, 4, 5}. We then relax (cid:101)qi
to be continuous and then round the solution. Since (cid:101)q = log2(q) is
monotonously increasing function, we can transform an equivalent
formulation as follows

(15b)

(15c)

(15d)

N
(cid:88)

i=1

pcm
i α1
i
Bi

min
(cid:101)q,B

K(H, (cid:15)q)

s.t.

(11f ),

+ Hpcp

i (c2

i 2(cid:101)qi + c1
i )

(Bi), and

φ((cid:101)q1, · · · , (cid:101)qN ) (cid:44)

N
(cid:88)

i=1

A3π2
i s
22 (cid:101)qi − 1

≤ (cid:15)q,

+ H(c2

i 2(cid:101)qi + c1

c3(2(cid:101)qi)Ui ≤ Ci, ∀i ∈ N ,
α1
Tmax
i
Bi
K(H, (cid:15)q)
Bi > 0, qmin ≤ (cid:101)qi ≤ qmax, ∀i ∈ N .
and pcp

i ) ≤

i

, ∀i ∈ N ,

For objective function in (21), K(H,(cid:15)q)α1
i 2(cid:101)qi are convex
functions in Bi and (cid:101)qi, respectively. The afﬁne combination of
convex functions preserves convexity. Similarly, we can easily
verify the convexity of the constraints.

i c2

Bi

(16)

(17a)

(17b)

(21a)

(21b)

(21c)

(21d)

(21e)

(21f)

where ρ(Hmin) = ρ(Hmax) = M N ((cid:15) − (cid:15)min
is deﬁned in (51b).

q

)2Tmax and ρ(H)

Next, we propose an efﬁcient iterative algorithm to reduce
the computational complexity. The main idea of the proposed

Algorithm 2 The proposed iterative algorithm for (20)
1: Input: Given H, (cid:15)q, two small constants, ι1, ι2, and a large

positive number ˆµ.
2: Output: Optimal 2(cid:101)q(cid:63)
3: Initialization: µ1
4: Set µ1 = (µ1
5: repeat
6:

U B + µ1

i , B(cid:63)
i
LB = ωLB = 0; µ1

U B = ˆµ; ωU B = ˆω;
LB)/2 and ω = (ωU B + ωLB)/2

Set ω = (ωU B + ωLB)/2
Calculate B(z)
repeat

via (25)

i

via (24)

i

Calculate (cid:101)q(z)
if φ((cid:101)q(z)
Set µ1

1 , · · · , (cid:101)q(z)
U B = µ1

else

Set µ1

LB = µ1

N ) > (cid:15)q then

7:
8:

9:

10:
11:
12:
13:
14:
15:

end if
until µ1
if (cid:80) B(z)

U B − µ1
LB ≤ ι1
i > B then

else

Set ωU B = ω

16:
17:
18:
19:
20:
21: until ωU B − ωLB ≤ ι2

Set ωLB = ω

end if

iterative algorithm as follows. In the (z)-th iteration, we ﬁrst ﬁx
the bandwidth in the (z − 1)-th iteration, denoted as B(z−1) to
solve problem (21) to obtain quantization strategy (cid:101)q; then, with
the updated (cid:101)q(z), we can get the optimal B(z). In the intermediate
steps, we attempt to derive some analytical solutions to reduce the
computation load.

In the (z)-th iteration, we can decompose problem (21) into

two convex subproblems as

min
(cid:101)q(z)
s.t.

R

N
(cid:88)

i=1

pcp
i (c2

i 2(cid:101)q(z)

i + c1
i )

(21c), (21d), (21f ),
α1
i 2(cid:101)q(z)
i
HB(z−1)
i

+ (c2

i + c1

i ) ≤

Tmax
R

, ∀i ∈ N ,

where R = HK(H, (cid:15)q) and

min
B(z)

K(H, (cid:15)q)

s.t.

(11f ), (21f ),

N
(cid:88)

i=1

i α1
pcm
i
B(z)
i

K(H, (cid:15)q)α1
i
B(z)
i

≤ Tmax − RT cp

i (2(cid:101)q(z)

i ), ∀i ∈ N .

(22a)

(22b)

(22c)

(23a)

(23b)

(23c)

Theorem 3. The optimal quantization levels (cid:101)q(cid:63)
allocation B(cid:63)

i for the i-th device are given by

i and bandwidth

(cid:101)q(z)(cid:63)
i = min{(cid:101)qmax

i

, (cid:101)qi(µ1(cid:63))},

(24)

and

B(z)(cid:63)

i = max{B(z)

i,min((cid:101)q(z)(cid:63)

i

), B(z)
i

(ω(cid:63))},

(25)

7

Algorithm 3 Joint design of ﬂexible weight quantization and
bandwidth allocation for EE-FWQ
1: Input: Initialize H(0), (cid:15)q(0), qi(0), Bi(0) of problem (14)

and set l = 0.
2: Output: H (cid:63), (cid:15)(cid:63)
3: repeat
4: With given q(l), B(l), compute (cid:15)q(l + 1) and H(l + 1)

q, q(cid:63), B(cid:63)

via (16) and (19), respectively

5: With given (cid:15)q(l + 1) and H(l + 1), compute qi(l + 1) and

bi(l + 1) by Alg. 2

6: until objective value (14) converges
7: Rounding ˆqi = (cid:98)(cid:101)q(cid:63)

i (cid:101) and (cid:98)H (cid:63)(cid:101) and obtain the quantization

strategy q(cid:63)

i = 2ˆqi .

where

(cid:101)qi = log2

(cid:18)

log2(λi +

(cid:113)

(cid:19)

λ2
i + 4) − 1

,

λi =

ln(2)µ1(cid:63)A3π2
R(pcp
i + µ2
i,min((cid:101)q(z)(cid:63)
B(z)

i s2
i (µ1(cid:63)))

) =

i

,

B(z)
i

(ω(cid:63)) =

K(H, (cid:15)q)

i (2(cid:101)q(z)(cid:63)

i

Tmax − RT cp
i (A0H + A1)
M H((cid:15) − (cid:15)q)

i α1
pcm
√
ω(cid:63)

,

,

)

(26)

(27)

(28)

(29)

µ1(cid:63) and ω(cid:63) are the optimal Lagrange multipliers to satisfy the
quantization error constraint φ((cid:101)q(z)(cid:63)
N ) = (cid:15)q and band-
width capacity constraint (cid:80)N
i=1 B(z)(cid:63)
Proof. Please refer to the detailed proof in Appendix C in the
separate supplemental ﬁle.

i = Bmax, respectively.

, · · · , (cid:101)q(z)(cid:63)

1

Theorem 3 suggests that (cid:101)q(cid:63)

i is determined by local computing
capabilities. Speciﬁcally, small quantization levels can be allo-
cated to devices with weaker computing capabilities for the beneﬁt
of sum computing energy reduction. Given the overall quantization
error constraint, the devices with higher computing capabilities
may use a higher quantization level to maintain the model ac-
curacy. It also indicates that the optimal bandwidth allocation B
depends not only on the channel conditions (hi) but also on the
quantization levels (cid:101)q(cid:63)
i . Concisely, assigned bandwidth increases
with the poor channel condition to avoid the straggler issues. In
addition, when the devices use a higher quantization level for local
training (higher computing energy), the device should be assigned
more bandwidth to reduce total energy consumption.

The algorithm that solves problem (14) is summarized in
Alg 3. Since the optimal solution of problem (15) or (20) can
be obtained in each loop, the objective value of the problem
(14) keeps decreasing in the loop until
the solutions of the
two subproblems converge. Next, we analyze the computational
complexity of Alg. 3. To solve the EE-FWQ problem by using
Alg. 3, two subproblems (15) and (20) need to be solved. For the
subproblem (15), we can obtain a unique real solution of H from
(18) in closed form, which does not resort to any iterative solver.
For the subproblem (20), we use Alg. 2, where the complexity
is O(N log2(1/ι1) log2(1/ι2)) with accuracy ι1. As a result,
the total complexity of Alg. 3 is O(N L log2(1/ι1) log2(1/ι2))
where L is the number of iterations required in Alg 3. The
complexity of Alg. 3 is low since O(N L log2(1/ι1) log2(1/ι2))
grows linearly with the total number of participating devices.

It should be noted that the FL server is in charge of solving
the optimization in (11). It is piratical because the FL protocol in
[35] requires mobile devices to check in with the FL server ﬁrst
before the FL training begins. Hence, the FL server can collect the
information (c1(qi), c2(qi), Ci, pcm
and hi) from mobile devices,
i
determine the optimal strategies (qi, Bi) of each device via Alg. 3,
and inform the strategies to the participating devices. It only needs
to be solved once if the network information remains unchanged.
That is absolutely affordable for the FL server.

5 PERFORMANCE EVALUATION
5.1 Data and settings

1) Learning Model and Dataset: To test the model perfor-
mance, we choose two commonly-used deep learning models:
ResNet-34 and Mobilenet. The well-known datasets, CIFAR-10
and CIFAR-100, are used to train FL models for image classiﬁca-
tion tasks. The CIFAR-10 dataset consists of 60000 32x32 color
images in 10 classes with 5000 training images per class. The
CIFAR-100 dataset has 100 classes and each class has 500 32x32
training images and 100 testing images. For the CIFAR-10 dataset,
each device contains a total number of 20000/N training samples
with only 4 classes. For the CIFAR-100, each device contains a
total number of 20000/N training samples with only 40 classes.
2) Communication and Computing Models: For the communi-
cation model, we assume the noise power is N0 = −174 dBm.
The transmitting power of each device is uniformly selected from
{19, 20, 21, 22, 23} dBm. Unless speciﬁed otherwise, we set the
bandwidth Bmax = 100MHz and the channel gains hi are mod-
eled as i.i.d. Rayleigh fading with average path loss set to 10−3.
Furthermore, we assume that model parameter is quantized into
16 bits before transmission. For the GPU computing model, the
scaling factors of quantization are measured by Nvidia proﬁling
tools on Jetson Xavier NX. We use ResNet-34 model with CIFAR-
10 multiple times and obtain the simulated function c1(q) =
7.12 × 10−3q + 0.274 and c2(q) = 4.24 × 10−4q + 1.035.
The GPU core frequency f core
, ∀i is uniformly selected from
{1050, 1100, 1150, 1200}MHz and memory frequency f mem
, ∀i
is uniformly selected from {1450, 1500, 1550, 1600}MHz.

i

i

3) Peer Schemes for Comparison: We compare our proposed
FWQ scheme with the following two different energy efﬁcient FL
schemes:

• FL FDMA [21]: All mobile devices train their local mod-
els with full precision, i.e., without quantization. Their
scheme optimizes the computing and communication re-
sources (i.e., CPU frequency and wireless bandwidth) to
minimize the energy consumption in FL training. For a
fair comparison, we change the CPU model in FL FDMA
to GPU model and set q = 32.

• FlexibleSpar [23]: All mobile devices train their local
models with the full precision and sparsity of their model
updates before transmitting to the FL server. Their scheme
optimizes the frequency of model updates and gradient
sparsity ratio to minimize the energy consumption in FL
training. Here, we set q = 32 in the GPU model.

Beside, we also consider two different quantization levels for our
evaluation:

• Rand Q: All mobile devices choose a quantization level
randomly without considering the learning performance.

The resource allocation strategies for Uniﬁed Q and Rand Q are
optimized by solving a simpliﬁed version of the problem (14).

8

5.2 Convergence analysis

First, we conduct convergence analysis. We implement the
above learning models and choose a uniﬁed quantization strategy
q1 = · · · = qN = 16 in the “Uniﬁed Q” scheme. Fig. 2(a) and
Fig. 2(c) show the model performance in terms of testing accuracy.
Here the testing accuracy of the model trained by “FL FDMA” is
regarded as a baseline because “FL FDMA” trains the local models
in full precision and transmits full gradients to the server. From
Fig. 2(a) and Fig. 2(c), the weight quantization used in “FWQ”
and “Uniﬁed Q” exhibits slight accuracy degradation. That is
consistent with our convergence analysis that the discretization
error induced by the quantization is unavoidable. This error is ac-
cumulated by all the participating mobile devices, which indicates
some mobile devices take aggressive quantization levels (e.g., 8
bit) due to their resource limitation. As for our proposed FWQ
scheme, since it considers this error in the quantization selection,
the degradation is well controlled and relatively small. The cor-
responding energy consumption in the FL training is presented
in Fig. 2(b) and Fig. 2(d). It demonstrates the effectiveness of
our proposed scheme. The FWQ scheme is superior to the other
three schemes in terms of the trade-off between the overall energy
efﬁciency for FL training and training accuracy. It consumes x2 -
x100 less energy than the other three schemes in the FL training
process with only a 1.1% accuracy loss.

5.3 Impact of number of users

We now evaluate how the number of users affects the total
energy consumption for FL training. Fig. 3 shows that the average
energy consumption decreases as the number of mobile devices
participating in FL increases. The average energy consumption
per device does not experience too much change even after more
devices participate in FL training under all the schemes. The rea-
son is that bringing more devices to train the FL model helps speed
up the model convergence and thus reduce energy consumption,
which is consistent with the sub-linear speedup in Theorem 1.
However, as N continues increasing, the marginal reduction of the
total number of training iterations becomes smaller and smaller.
Besides, our proposed FWQ scheme outperforms “FL FDMA”
and “FlexibleSpar”. For example, the proposed FWQ scheme
saves the energy of “FL FDMA” by 56% and of “FlexibleSpar” by
35%, respectively. The reason is that the proposed FWQ leverages
weight quantization to reduce the workload for on-device training
and optimize the weight quantization levels for heterogeneous
devices, while the computing workload is not optimized and
ﬁxed for all the devices in both “FL FDMA” and “FlexibleSpar”
scheme. Moreover, the proposed FWQ scheme reduces the energy
in the “Uniﬁed Q” strategy by 20% and the “Rand Q” strategy
by 38.7%, respectively, when the number of users is equal to 10.
These results demonstrate the effectiveness of our proposed weight
quantization scheme.

• Uniﬁed Q: All the devices are set to use the same quanti-
zation strategy regardless of resource budgets for different
mobile devices.

5.4 Impact of computing capacities

We evaluate the impact of device heterogeneity concerning
computing capability. Here, we keep the number of mobile devices

9

(a) Testing accuracy vs epochs.

(b) Training error vs energy cost.

(c) Testing accuracy vs epochs.

(d) Training error vs energy cost.

Fig. 2. Convergence Analysis. (a)-(b): ResNet-34 on CIFAR-10 with the estimated parameters A1 = 13.765, A2 = 1.023, A3 = 0.0435 (c)-(d):
MobileNet on CIFAR-100 with the estimated parameters A1 = 16.655, A2 = 1.013, A3 = 0.0795

Fig. 3. Energy vs the numbers of devices.

Fig. 4. Energy vs device heterogeneity.

Fig. 5. Quantization vs bandwidth.

as ten and divide them into four groups. Fixing the minimum
capacity as 1800MB, we set different capacities into 4 groups:
CMB, (C + 50L)MB, (C + 150L)MB, and (C + 200L)MB,
respectively. The values of L range from 0 to 10. A larger
value of L means mobile devices have more diverse computing
conditions, implying that the optimized quantization strategy has
more diverse values. From Fig. 4, we observe that the total energy
consumption grows as the value of L increases. It indicates that
device heterogeneity does impact the energy efﬁciency in FL
training. Since the proposed FWQ scheme jointly optimizes the
quantization levels and bandwidth allocation for heterogeneous
devices, the FWQ scheme is superior to all other schemes in terms
of high levels of computing heterogeneity across participating
devices.

5.5 Impact of communication capacities

Figure 5 shows the impacts of the wireless conditions on
the optimal quantization selection. We vary the total available
bandwidth from 80 MHz to 98 MHz and divide the mobile devices
into 4 different groups, denoted as {g1, g2, g3, g4}, where the
channel gain h(g1) ≤ h(g2) ≤ h(gd3) ≤ h(g4). From Fig. 5,
we observe that, as the overall bandwidth becomes small, the
ratio of the communication energy consumption to the overall
energy consumption grows, which means wireless communica-
tions have a larger impact on the total energy consumption than
local computing. As a result, the mobile devices in group 1,
with small channel gain, become the stragglers in FL training
and could slow down the gradient update time for one iteration.
To avoid the update delay for the next iteration and reduce the
overall energy consumption, they have to take aggressive actions
to compress their local models into the smallest number of bits.
However this results in large discretization noise and degrades

the performance, as stated in Theorem 1. To compensate for that,
those who have better channel gain need to “work” more by
using a higher precision model to perform local training. Similarly,
when the available bandwidth increases, the computing contributes
more to the overall energy consumption. Those mobile devices
with smaller local computing capacities choose to compress their
models more to save computing energy.

6 CONCLUSION

In this paper, we have studied the energy efﬁciency of FL
training via joint design of wireless transmission and weight
quantization. We have jointly exploited the ﬂexible weight quanti-
zation selection and the bandwidth allocation to develop an energy
efﬁcient FL training algorithm over heterogeneous mobile devices,
constrained by the training delay and learning performance. The
weight quantization approach has been leveraged to deal with the
mismatch between high model computing complexity and limited
computing capacities of mobile devices. The convergence rate of
FL with local quantization has been analyzed. Guided by the
derived theoretical convergence bound, we have formulated the
energy efﬁcient FL training problem as a mixed-integer nonlinear
programming. Since the optimization variables of the problem
are strongly coupled, we have proposed an efﬁcient
iterative
algorithm, where the closed-form solution of the bandwidth
allocation and weight quantization levels are derived in each
iteration. By comparing with different quantization levels through
extensive simulations, we have demonstrated the effectiveness
of our proposed scheme in handling device heterogeneity and
reducing overall energy consumption in FL over heterogeneous
mobile devices.

10

[22] X. Mo and J. Xu, “Energy-efﬁcient federated edge learning with joint
communication and computation design,” Journal of Communications
and Information Networks, vol. 6, no. 2, pp. 110–124, 2021.

[23] L. Li, D. Shi, R. Hou, H. Li, M. Pan, and Z. Han, “To talk or to work:
Flexible communication compression for energy efﬁcient federated learn-
ing over heterogeneous mobile edge devices,” in Proc. IEEE Conference
on Computer Communications (INFOCOM), Virtual Conference, May
2021.

[24] C. De Sa, M. Leszczynski, J. Zhang, A. Marzoev, C. R. Aberger,
K. Olukotun, and C. R´e, “High-accuracy low-precision training,” arXiv
preprint arXiv:1803.03383, 2018.

[25] M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv, and Y. Bengio,
“Binarized neural networks: Training deep neural networks with weights
and activations constrained to+ 1 or-1,” arXiv preprint arXiv:1602.02830,
2016.

[26] Z. Li and C. M. De Sa, “Dimension-free bounds for low-precision
training,” in Advances in Neural Information Processing Systems (NIPS),
Vancouver, BC, Canada, December 2019.

[27] X. Li, K. Huang, W. Yang, S. Wang, and Z. Zhang, “On the convergence
of fedavg on non-iid data,” in International Conference on Learning
Representations (ICLR), New Orleans, Louisiana, May 2019.

[28] H. Yu, S. Yang, and S. Zhu, “Parallel restarted sgd with faster con-
vergence and less communication: Demystifying why model averaging
works for deep learning,” in Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, Honolulu, USA, January 2019.

[29] S. Ghadimi and G. Lan, “Stochastic ﬁrst-and zeroth-order methods for
nonconvex stochastic programming,” SIAM Journal on Optimization,
vol. 23, no. 4, pp. 2341–2368, 2013.

[30] A. Reisizadeh, A. Mokhtari, H. Hassani, A. Jadbabaie, and R. Pedarsani,
“Fedpaq: A communication-efﬁcient federated learning method with
periodic averaging and quantization,” in International Conference on
Artiﬁcial Intelligence and Statistics. PMLR, 2020, pp. 2021–2031.
[31] X. Mei, X. Chu, H. Liu, Y.-W. Leung, and Z. Li, “Energy efﬁcient real-
time task scheduling on cpu-gpu hybrid clusters,” in IEEE Conference on
Computer Communications (INFOCOM), Atlanta, GA, USA, May 2017.
[32] T.-J. Yang, Y.-H. Chen, J. Emer, and V. Sze, “A method to estimate the
energy consumption of deep neural networks,” in 2017 51st asilomar
conference on signals, systems, and computers.
Paciﬁc Grove, CA:
IEEE, November 2017.

[33] E. Torti, A. Fontanella, M. Musci, N. Blago, D. Pau, F. Leporati, and
M. Piastra, “Embedded real-time fall detection with deep learning on
wearable devices,” in 2018 21st euromicro conference on digital system
design (DSD). Prague, Czech Republic: IEEE, August 2018, pp. 405–
412.

[34] K.-H. Schlote, “Bl van der waerden, moderne algebra, (1930–1931),” in
Landmark Writings in Western Mathematics 1640-1940. Elsevier, 2005,
pp. 901–916.

[35] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman,
V. Ivanov, C. Kiddon, J. Koneˇcn`y, S. Mazzocchi, H. B. McMahan et al.,
“Towards federated learning at scale: System design,” arXiv preprint
arXiv:1902.01046, 2019.

[36] T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, “Federated learning:
Challenges, methods, and future directions,” IEEE Signal Processing
Magazine, vol. 37, no. 3, pp. 50–60, 2020.

[37] H. Li, S. De, Z. Xu, C. Studer, H. Samet, and T. Goldstein, “Training
quantized nets: A deeper understanding,” in Advances in Neural Infor-
mation Processing Systems, 2017, pp. 5811–5821.

[38] P. Jiang and G. Agrawal, “A linear speedup analysis of distributed deep
learning with sparse and quantized communication,” in Advances in
Neural Information Processing Systems (NIPS’18), Montreal, Canada,
December 2018, pp. 2525–2536.

REFERENCES

[1] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efﬁcient learning of deep networks from decentralized
data,” in Artiﬁcial Intelligence and Statistics (AISTATS), Ft. Lauderdale,
FL, April 2017.

[2] A. Hard, K. Rao, R. Mathews, S. Ramaswamy, F. Beaufays, S. Augen-
stein, H. Eichner, C. Kiddon, and D. Ramage, “Federated learning for
mobile keyboard prediction,” arXiv preprint arXiv:1811.03604, 2018.

[3] T. Yu, T. Li, Y. Sun, S. Nanda, V. Smith, V. Sekar, and S. Seshan,
“Learning context-aware policies from multiple smart homes via fed-
erated multi-task learning,” in Proc. of the IEEE/ACM Fifth International
Conference on Internet-of-Things Design and Implementation (IoTDI),
Sydney, Australia, May 2020.

[4] T. S. Brisimi, R. Chen, T. Mela, A. Olshevsky, I. C. Paschalidis,
and W. Shi, “Federated learning of predictive models from federated
electronic health records,” International journal of medical informatics,
vol. 112, pp. 59–67, 2018.

[5] Y. Li, X. Dong, and W. Wang, “Additive powers-of-two quantization: An
efﬁcient non-uniform discretization for neural networks,” in International
Conference on Learning Representations (ICLR), Virtual Conference,
September 2020.

[6] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing deep
neural networks with pruning, trained quantization and huffman coding,”
arXiv preprint arXiv:1510.00149, 2015.

[7] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan, “Deep
learning with limited numerical precision,” in International Conference
on Machine Learning (ICML), Lille, France, July 2015.

[8] D. Zhang, J. Yang, D. Ye, and G. Hua, “Lq-nets: Learned quantization for
highly accurate and compact deep neural networks,” in Proceedings of
the European conference on computer vision (ECCV), Munich, Germany,
September 2018.

[9] F. Fu, Y. Hu, Y. He, J. Jiang, Y. Shao, C. Zhang, and B. Cui, “Don’t waste
your bits! squeeze activations and gradients for deep neural networks via
tinyscript,” in International Conference on Machine Learning.
virtual:
PMLR, July 2020, pp. 3304–3314.

[10] L. Hou, R. Zhang, and J. T. Kwok, “Analysis of quantized models,”
in International Conference on Learning Representations, Vancouver,
Canada, May 2018.

[11] NSF,

“Resilient & intelligent

nextG systems

(RINGS),”

https://www.nsf.gov/pubs/2021/nsf21581/nsf21581.htm?WT.mc id=
USNSF 25&WT.mc ev=click#toc, Last Accessed May 4, 2021.

[12] 3GPP,

Speciﬁcation Group

15 Description; Summary
,” 3rd Generation Partnership Project

System
“Technical
of Rel-15 Work
Aspects; Release
(3GPP), Technical
Items
Speciﬁcation
[On-
2.0.0.
09
line]. Available: https://portal.3gpp.org/desktopmodules/Speciﬁcations/
SpeciﬁcationDetails.aspx?speciﬁcationId=3389

Services

21.915,

version

2019,

(TS)

and

[13] N. H. Tran, W. Bao, A. Zomaya, N. M. NH, and C. S. Hong, “Federated
learning over wireless networks: Optimization model design and anal-
ysis,” in IEEE Conference on Computer Communications (INFOCOM),
Paris, France, April 2019.

[14] T. T. Vu, D. T. Ngo, N. H. Tran, H. Q. Ngo, M. N. Dao, and R. H.
Middleton, “Cell-free massive mimo for wireless federated learning,”
IEEE Transactions on Wireless Communications, vol. 19, no. 10, pp.
6377 – 6392, 2020.

[15] M. Chen, Z. Yang, W. Saad, C. Yin, H. V. Poor, and S. Cui, “A joint
learning and communications framework for federated learning over
wireless networks,” arXiv preprint arXiv:1909.07972, 2019.

[16] K. Yang, T. Jiang, Y. Shi, and Z. Ding, “Federated learning via over-
the-air computation,” IEEE Transactions on Wireless Communications,
vol. 19, no. 3, pp. 2022–2035, 2020.

[17] T. Nishio and R. Yonetani, “Client selection for federated learning
with heterogeneous resources in mobile edge,” in IEEE International
Conference on Communications (ICC), Shanghai, China, May 2019.
[18] D. Shi, L. Li, R. Chen, P. Prakash, M. Pan, and Y. Fang, “Towards energy
efﬁcient federated learning over 5G+ mobile devices,” arXiv preprint
arXiv:2101.04866, 2021.

[19] W. Shi, S. Zhou, Z. Niu, M. Jiang, and L. Geng, “Joint device scheduling
and resource allocation for latency constrained wireless federated learn-
ing,” IEEE Transactions on Wireless Communications, 2020.

[20] Q. Zeng, Y. Du, K. Huang, and K. K. Leung, “Energy-efﬁcient resource
management for federated edge learning with cpu-gpu heterogeneous
computing,” IEEE Transactions on Wireless Communications, 2021.
[21] Z. Yang, M. Chen, W. Saad, C. S. Hong, and M. Shikh-Bahaei, “Energy
efﬁcient federated learning over wireless communication networks,”
IEEE Transactions on Wireless Communications (Early Access), 2020.

APPENDIX A
PROOF OF THEOREM 1

A.1 Additional Notation

(cid:44) Qi

For the simplicity of notations, we denote the error of weight
(cid:1) − wr+1
quantization er
, and the local “gradients”
i
i
(cid:44) (cid:79) (cid:101)fi(wr
with weight quantization as (cid:101)gr
i /η. Since
i
the quantization scheme Qi(·) we used is an unbiased scheme,
EQ [er

i ) − er

(cid:0)wr+1
i

i ] = 0.

Inspired by the iterate analysis framework in [36], we deﬁne

the following virtual sequences:

ur+1
i = wr

i − η(cid:101)gr
i ,
N
(cid:88)

wr+1

i = κr

πiur+1

i + (1 − κr)ur+1

i

(30)

(31)

,

i=1

where κr = 1 if (r + 1) mod H = 0 and κr = 0 otherwise.
The following short-hand will be found useful in the convergence
analysis of the proposed FWQ framework

¯ur =

N
(cid:88)

i=1

πiur
i ,

¯wr =

N
(cid:88)

i=1

πiwr
i ,

(cid:101)gr =

N
(cid:88)

i=1

πi (cid:101)gr
i .

(32)

Thus, ¯ur+1 = ¯wr − η(cid:101)gr. Note that we can only obtain ¯wr+1
when (r + 1) mod H = 0.

A.2 Key Lemmas

r(cid:48) ≤ r ≤ r(cid:48) + H, we have

A1r :=

E

π2
i

(cid:104)

(cid:104)

EQ

N
(cid:88)

i=1

(cid:107) ¯wr − wr

i (cid:107)2

2

11

(cid:105)(cid:105)

N
(cid:88)

=

E

π2
i

(cid:104)

(cid:104)

EQ

(cid:107)(wr

i − ¯wr(cid:48)

) − ( ¯wr − ¯wr(cid:48)

)(cid:107)2
2

(cid:105)(cid:105)

i=1
N
(cid:88)

(a)
≤

E

π2
i

(cid:104)

(cid:104)

EQ

(cid:107)wr

i − wr(cid:48)

i (cid:107)2
2

(cid:105)(cid:105)

i=1
N
(cid:88)

π2
i

=

=

i=1

N
(cid:88)

i=1

r
(cid:88)

E[(cid:107)

j=r(cid:48)

−η(cid:79) (cid:101)fi(wj

i ) + er

i (cid:107)2
2]

r
(cid:88)

π2
i

E[(cid:107)

j=r(cid:48)

η(cid:79) (cid:101)fi(wj

i )(cid:107)2

2 +

j
(cid:88)

j=r(cid:48)

EQ[(cid:107)er

i (cid:107)2
2]]

(b)
≤ η2

N
(cid:88)

i=1

(cid:124)

r(cid:48)
H(cid:88)

π2
i

E[(cid:107)

i=r(cid:48)
(cid:123)(cid:122)
A2

(cid:79) (cid:101)fi(wj

i )(cid:107)2
2]

+η

√

dHG

N
(cid:88)

i=1

(cid:125)

π2
i δi,

(35)

H = r(cid:48) + H − 1, (a) results from (cid:80)n
, wr(cid:48)

i − wr(cid:48)
where r(cid:48)
i ) =
¯wr − ¯wr(cid:48)
, and E(cid:107)x − E[x](cid:107)2
2. (b) follows
Lemma 1. We generalize the result from [38] to upper-bound the
second term A2 in RHS of (35):

i=1 πi(wr
2 ≤ E(cid:107)x(cid:107)2

i = ¯wr(cid:48)

A2 := η2

N
(cid:88)

i=1

r(cid:48)
H(cid:88)

π2
i

E[(cid:107)

(cid:79) (cid:101)fi(wj

i ) − (cid:79)Fi(wj

i ) + (cid:79)Fi(wj

i )(cid:107)2
2]

j=r(cid:48)
r(cid:48)
H(cid:88)

(cid:18)

≤ 3η2H

N
(cid:88)

π2
i

i=1
(cid:80)N

j=r(cid:48)
i=1 π2
i τ 2
i
M

+ η2H

L2 (cid:13)
(cid:13) ¯wj − wj
(cid:13)

i

(cid:13)
2
(cid:13)
(cid:13)
2

+ (cid:13)

(cid:13)(cid:79)F ( ¯wj)(cid:13)
2
(cid:13)
2

(cid:19)

.

(36)

Now, we give four important lemmas to offer our proof.

It follows that

Lemma 1 (Weight quantization error [37]). If Assumption 2 holds,
the stochastic rounding error on each iteration can be bounded,
in expectation

(cid:104)

EQ

(cid:107)er

i (cid:107)2

2

√

(cid:105)

≤ η

dGδi.

(33)

Lemma 2 (Bounding the divergence). Suppose 1 − 3η2L2H 2 >
0, we have,

R−1
(cid:88)

N
(cid:88)

r=0

i=1

E

π2
i

(cid:104)

(cid:104)

EQ

(cid:107) ¯wr − wr

(cid:105)(cid:105)

2

i (cid:107)2
√

≤

η

+

η2RHτ
M (1 − 3η2L2H 2)
3η2H 2π
1 − 3η2L2H 2

+

R−1
(cid:88)

r=0

dRHG (cid:80)N
1 − 3η2L2H 2

i=1 π2

i δi

(cid:107)(cid:79)F ( ¯wr)(cid:107)2
2 ,

(34)

where τ = (cid:80)N

i=1 π2

i τ 2

i and π = (cid:80)N

i=1 π2
i .

R−1
(cid:88)

r=0

A1r ≤ η2RH

τ
M

+ 3η2H 2

R−1
(cid:88)

(L2A1r + π (cid:107)(cid:79)F ( ¯wr)(cid:107)2
2)

r=0

√

+ η

dHG

N
(cid:88)

i=1

π2
i δi,

(37)

where τ = (cid:80)N

i τ 2
i=1 π2
i .
Suppose 1 − 3η2L2H 2 > 0, we have

i and π = (cid:80)N

i=1 π2

R−1
(cid:88)

r=0

A1r ≤

η2RHτ
M (1 − 3η2L2H 2)

η

+

√

dHG (cid:80)N
i=1 π2
1 − 3η2L2H 2

i δi

+

3η2H 2π
1 − 3η2L2H 2

R−1
(cid:88)

r=0

(cid:107)(cid:79)F ( ¯wr)(cid:107)2
2 ,

(38)

and the proof is completed.

Lemma 3. According to the proposed algorithm the expected
inner product can be bounded with:
E (cid:2)EQ
(cid:2)(cid:104)(cid:79)F ( ¯wr), ¯ur+1 − ¯wr(cid:105)(cid:3)(cid:3)
N
η
(cid:88)
2

(cid:107)(cid:79)F ( ¯wr)(cid:107)2
2

(cid:107) ¯wr − wr

ηL2
2

i (cid:107)2

≤ −

EQ

π2
i

(cid:105)(cid:105)

+

E

E

(cid:104)

(cid:104)

(cid:105)

(cid:104)

2

i=1

Proof. Recalling that at the synchronization step r(cid:48) ∈ UH , wr(cid:48)
¯wr(cid:48)

i =
for all i ∈ N . Therefore, for any r ≥ 0, such that for

−

E

η
2





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

N
(cid:88)

i=1

2
(cid:13)
(cid:13)
πi(cid:79)Fi(wr
(cid:13)
i )
(cid:13)
(cid:13)
2


 .

(39)

Proof.
E (cid:2)(cid:104)(cid:79)F ( ¯wr), ¯ur+1 − ¯wr(cid:105)(cid:3)
= E [(cid:104)(cid:79)F ( ¯wr), −η(cid:101)gr(cid:105)]
(cid:34) N
(cid:88)

(cid:42)

(cid:79)F ( ¯wr), E

= −η

(cid:35)(cid:43)

πi(cid:79)Fi(wr
i )

i=1

(a)
= −

(cid:104)

E

η
2

(cid:107)(cid:79)F ( ¯wr)(cid:107)2
2

(cid:105)

−

E

η
2





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

N
(cid:88)

i=1

2
(cid:13)
(cid:13)
πi(cid:79)Fi(wr
(cid:13)
i )
(cid:13)
(cid:13)
2





+

E

η
2





(cid:13)
(cid:13)
(cid:79)F ( ¯wr) −
(cid:13)
(cid:13)
(cid:13)

N
(cid:88)

i=1

2
(cid:13)
(cid:13)
πi(cid:79)Fi(wr
(cid:13)
i )
(cid:13)
(cid:13)
2





(b)
≤ −

(cid:104)

E

η
2

(cid:107)(cid:79)F ( ¯wr)(cid:107)2
2

(cid:105)

−

E

η
2





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

N
(cid:88)

i=1

+

ηL2
2

N
(cid:88)

i=1

E

π2
i

(cid:104)

(cid:104)

EQ

(cid:107) ¯wr − wr

i (cid:107)2

2

πi(cid:79)Fi(wr
i )





2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

(cid:105)(cid:105)

,

(40)

where (a) is due to 2(cid:104)a, b(cid:105) = ||a||2 + ||b||2 − ||a − b||2 and (b)
follows from L-smoothness assumption.

Since κr = 0 when (r + 1) mod H (cid:54)= 0, and we have the
second term in (42) equals to zero. When (r + 1) mod H = 0,

12

L
2

N
(cid:88)

i=1

E

π2
i

(cid:104)(cid:13)
(cid:0)∆r+1
(cid:13)

i − ¯∆r+1(cid:1)(cid:13)
2
(cid:13)
2

(cid:105)

L
2

L
2

N
(cid:88)

i=1
N
(cid:88)

i=1

≤

=

≤

E

π2
i

(cid:20)(cid:13)
(cid:13)wr(cid:48)
(cid:13)

i − ur+1

i

(cid:21)

(cid:13)
2
(cid:13)
(cid:13)
2

r
(cid:88)

π2
i

E[(cid:107)

η(cid:79) (cid:101)fi(wr

i ) − er

i (cid:107)2
2]

j=r(cid:48)
√

LA2
2

+

ηLH
2

dG

N
(cid:88)

i=1

π2
i δi

(a)
≤

3η2LHπ
2

r(cid:48)
H(cid:88)

j=r(cid:48)

(cid:13)(cid:79)F ( ¯wj)(cid:13)
(cid:13)
2
2 +
(cid:13)

√

dG

ηLH
2

N
(cid:88)

i=1

π2
i δi

+

η2LHτ
2M

+

3η2L3H
2

r(cid:48)
H(cid:88)

j=r(cid:48)

A1j,

(44)

Lemma 4. If Assumptions 1 and 2 hold, then for sequences ¯w
deﬁned in (31)-(32), we have

where (a) follows the results in (36).

E

(cid:104)(cid:13)
(cid:13) ¯wr+1 − ¯wr(cid:13)
2
(cid:13)
2

(cid:105)

L
2

≤

η2L
2

+

L
2

N
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
N
(cid:88)

i=1

2
(cid:13)
(cid:13)
πi(cid:79)Fi(wr
(cid:13)
i )
(cid:13)
(cid:13)
2

+

η2Lτ
2M

+

ηL
2

√

dG

N
(cid:88)

i=1

π2
i δi

A.3 Main Results

Under the Lipschitzan gradient assumption on F , we have,

E

(cid:104)(cid:13)
(cid:13)κrπi

(cid:0)∆r+1

i − ¯∆r+1(cid:1)(cid:13)
2
(cid:13)
2

(cid:105)

,

(41)

where ∆r+1

i = wr(cid:48)

i − ur+1

i

and ¯∆r = (cid:80)N

i=1 πi∆r+1

i

.

Proof. According to the updating rules deﬁned in (31), we have,

E

(cid:104)(cid:13)
(cid:13) ¯wr+1 − ¯wr(cid:13)
2
(cid:13)
2

(cid:105)

L
2

=

L
2

E[(cid:107)

N
(cid:88)

i=1

πiur+1

i − wr

i + κr (cid:16)

wr(cid:48)

i − ¯∆r − ur+1

i

(a)
= E

(cid:17)

(cid:107)2
2]

E (cid:2)F ( ¯wr+1)(cid:3) − E [F ( ¯wr)]
≤ E (cid:2)(cid:104)(cid:79)F ( ¯wr), ¯wr+1 − ¯wr(cid:105)(cid:3) +
= E (cid:2)(cid:104)(cid:79)F ( ¯wr), ¯wr+1 − ¯ur+1 + ¯ur+1 − ¯wr(cid:105)(cid:3)
(cid:104)(cid:13)
(cid:13) ¯wr+1 − ¯wr(cid:13)
2
(cid:13)
2

L
2
(cid:20)
(cid:104)(cid:79)F ( ¯wr), ¯ur+1 − ¯wr(cid:105) +

L
2

+

E

E

(cid:105)

(cid:104)(cid:13)
(cid:13) ¯wr+1 − ¯wr(cid:13)
2
(cid:13)
2

(cid:105)

(cid:13) ¯wr+1 − ¯wr(cid:13)
(cid:13)
2
(cid:13)
2

(cid:21)

,

(45)

L
2

(cid:34)

=

E

L
2

η2 (cid:107)ˆgr(cid:107)2

2 +

N
(cid:88)

i=1

π2
i

(cid:13)
(cid:13)κr (cid:0)∆r+1

i − ¯∆r+1(cid:1)(cid:13)
2
(cid:13)
2

(cid:35)

,

(42)

where ∆r+1
ﬁrst term in (42) can be bounded by,

i − ur+1

i = wr(cid:48)

i

and ¯∆r+1 = (cid:80)N

i=1 πi∆r+1

i

where (a) is E[EQ[ ¯wr+1]] = E[ ¯ur+1]. We use Lemma 1-3,4 to
upper bound two terms in the RHS of (45) and obtain,

E (cid:2)F ( ¯wr+1)(cid:3) − E [F ( ¯wr)]

. The

≤ −

(cid:104)

E

η
2

(cid:107)(cid:79)F ( ¯wr)(cid:107)2
2

(cid:105)

+

(cid:18)

−

η
2

+

η2L
2

(cid:19) (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

N
(cid:88)

i=1

2
(cid:13)
(cid:13)
πi(cid:79)Fi(wr
(cid:13)
i )
(cid:13)
(cid:13)
2

πi(cid:79) (cid:101)fi(wr

i ) − er

i /η





2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2





η2L
2

E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

η2L
2

E

N
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)



N
(cid:88)

i=1

πi((cid:79) (cid:101)fi(wr

i ) − (cid:79)Fi(wr

i ) + (cid:79)Fi(wr

2
(cid:13)
(cid:13)
(cid:13)
i ))
(cid:13)
(cid:13)
2





+

+

ηL2
2

A1r +

η2Lτ
2M

+

ηL
2

√

dG

N
(cid:88)

i=1

π2
i δi

L
2

N
(cid:88)

i=1

E

(cid:104)(cid:13)
(cid:13)κrπi

(cid:0)∆r+1

i − ¯∆r+1(cid:1)(cid:13)
2
(cid:13)
2

(cid:105)

+

L
2

N
(cid:88)

i=1

(cid:104)

E

π2
i

(cid:105)

(cid:107)er

i (cid:107)2

2

≤

ηL
2

(

ητ
M

N
(cid:88)

+ η||

i=1

πi(cid:79)Fi(wr

i )||2

2 +

(a)
≤ −

(cid:104)

E

η
2

(cid:107)(cid:79)F ( ¯wr)(cid:107)2
2

(cid:105)

+

ηL2
2

A1r +

√

ηL
2

dG

N
(cid:88)

i=1

π2
i δi

√

dG

N
(cid:88)

i=1

π2
i δi).

(43)

+

η2Lτ
2M

+

L
2

N
(cid:88)

i=1

E

(cid:104)(cid:13)
(cid:13)κr (cid:0)∆r+1

i − ¯∆r+1(cid:1)(cid:13)
2
(cid:13)
2

(cid:105)

.

(46)

Here, (a) holds if the learning rate ηL ≤ 1. Summing up for all R
communication rounds, we have:

(cid:104)

(cid:105)
F ( ¯wR)

E

− E (cid:2)F ( ¯w0)(cid:3)

≤

η
2

+

(3ηLH − 1)

R−1
(cid:88)

r=0

E (cid:107)(cid:79)F ( ¯wr)(cid:107)2

2 + ηLR

√

dG

N
(cid:88)

i=1

π2
i δi

η2LRτ
M

+

ηL2
2

(1 + 3ηLH)

R−1
(cid:88)

r=0

A1r.

(47)

where ρ(H) = (A0H +A1)2 (cid:16) T cm
. It is easy to
verify that ρ(H) is a convex function w.r.t H. Due to the convexity
of ρ(H), constraints (51b) can be equivalently transformed to

(cid:17)
i (qi)

+ T cp

(Bi)
H

i

13

Hmin ≤ H ≤ Hmax,

(52)

where ρ(Hmin) = ρ(Hmax) = M N ((cid:15) − (cid:15)min
)2Tmax. By
substituting (52) into problem (15), we can simplify problem (51)
to (17).

q

Plugging (38) into (47), we can obtain

(cid:104)

E

F ( ¯wR)

(cid:105)

− E (cid:2)F ( ¯w0)(cid:3)

APPENDIX C
PROOF OF THEOREM 3

≤ −

η
2

(1 − 3ηLH −

3η2LHC1
2

)

+ R

η2Lτ
M

(1 +

C1
2

) +

√

ηL

dRG
2

R−1
(cid:88)

(cid:104)

E

r=0

(cid:107)(cid:79)F ( ¯wr)(cid:107)2
2

(cid:105)

Proof. For problem (22), we observe that constraint (21d) is
served as the upper bound of (cid:101)q(z)
. Hence, we can substitute (21d)
and (21f) constraints with

i

(

1
2

+ C1)

N
(cid:88)

i=1

π2
i δi,

(48)

where (cid:101)qmax

i

1 ≤ (cid:101)q(z)

i ≤ (cid:101)qmax
= Ci/Ui. The Lagrangian duality L of (22) is

,

i

(53)

where C1 = ηLH+3η2L2H 2π

1−3η2L2H 2

and rearranging the terms gives

η
2

C (cid:48)
1

R−1
(cid:88)

r=0

(cid:107)(cid:79)F ( ¯wr)(cid:107)2
2

≤ E

(cid:104)

(cid:105)
F ( ¯w0) − F ( ¯wR)

+

√

ηL

dRG
2

(1 + 2C1)

N
(cid:88)

i=1

π2
i δi

+ R

(2 + C1)τ,

η2L
2M
1 = 1 − 3ηLH − 3η2LHC1
η

set

where C (cid:48)

If we

.
= (cid:112)M/R and

2

3η2LH(ηLH+3η2L2H 2π)
2(1−3η2L2H 2)

, we can get the 1/C (cid:48)

3ηLH
1 ≤ 2. Thus,

(49)

≥

1
R

≤

R−1
(cid:88)

(cid:107)(cid:79)F ( ¯wr)(cid:107)2
2

(E (cid:2)F ( ¯w0)(cid:3) − F (cid:63)) +

r=0
2
ηRC (cid:48)
1
√
dLG(1 + 2C1) (cid:80)N

i=1 π2

i δi

+

2C (cid:48)
1

ηL(2 + C1)τ
1M

C (cid:48)

≤

√

4Γ
M R

+

6HLτ
√
M R

√

+

dLG

N
(cid:88)

i=1

π2
i δi,

where Γ = E (cid:2)F ( ¯w0)(cid:3) − F (cid:63) and the proof is completed.

APPENDIX B
PROOF OF THEOREM 2

Proof. It is obvious that variable (cid:15)q is monotonously increasing
with the function in (15a) with its feasible set, i.e., the optimal
quantization error is (cid:15)q = (cid:15)min
into
problem (15) yields

. Substituting (cid:15)(cid:63)

q = (cid:15)min

q

q

(A0H + A1)2
M H((cid:15) − (cid:15)min

min
H
s.t. ρ(H) ≤ M N ((cid:15) − (cid:15)min

)2 (Ecm(B) + HEcp(q))
)2Tmax, ∀i,

q

q

H ≥ 0,

(51a)

(51b)

(51c)

L((cid:101)q(z), µ1, µ2)
N
(cid:88)

= R

pcp
i (c2

i 2(cid:101)q(z)

i + c1

i ) + µ1

i=1

N
(cid:88)

i=1

µ2
i

+

(cid:32)

R(c2

i 2(cid:101)q(z)

i + c1

i ) +

22 (cid:101)q
i=1
K(H, (cid:15)q)α1
i
B(z−1)
i

(cid:32) N
(cid:88)

A3πis

(z)
i − 1

(cid:33)

− (cid:15)q

(cid:33)

− Tmax

,

(54)

where µ1 and µ2 = {µ2
i } are the non-negative Lagrange multi-
pliers corresponding to the related constraints. Since the objective
function (22a) is a monotonically decreasing function w.r.t (cid:101)qi,
constraint (21c) is always satisﬁed with equality. The Karush-
Kuhn-Tucker (KKT) conditions can be used to obtain the optimal
solution, given by,






∂L
∂ (cid:101)q(z)
i
(cid:18)

µ2
i

R(c2

(cid:80)N

i=1

= 2(cid:101)q(z)

i

ln 2(R(pcp

i + µ2

i ) + K(H,(cid:15)q)α1
B(z−1)

i

R(c2
i 2(cid:101)q(z)

i 2(cid:101)q(z)
i + c1
A3π2
i s
(z)
22 (cid:101)q
i −1

i + c1
i ) + K(H,(cid:15)q)α1
B(z−1)
= (cid:15)q, µ1 ≥ 0.

i

i

i

(z)
i ) − ln 2 22 (cid:101)q
i A3π2
(z)
(22 (cid:101)q
i −1)2
(cid:19)

i sµ1

− Tmax

= 0,

− Tmax ≤ 0, µ2

i ≥ 0,

) = 0,

(55)

(cid:101)qi = log2

(cid:18)

(cid:113)

log2(λi +

(cid:19)

λ2
i + 4) − 1

,

(56)

where

λi =

ln(2)µ1A3π2
c2
i (A0H+A1)2
M N ((cid:15)−(cid:15)q)2 (pcp

i s2
i + µ2
i )

.

(57)

Here, (cid:101)qi is a function of µ1 and µ2
i . We adopt the bisection search
methods to ﬁnd the optimal µ1(cid:63) that satisﬁes constraint (21c).
Given the KKT conditions, we then substitute the second equation
in (55) with (56) and µ2(cid:63)

i and calculate the optimal µ2(cid:63)
i
For problem (23), constraint (21f) is equivalent to

.

B(z)

i ≥ B(z)

i,min

(cid:44)

K(H, (cid:15)q)
Tmax − RT cp

i (2(cid:101)q(z)
i )

.

(58)

(50)

We can ﬁnd (cid:101)qi in closed-form as

Substituting (58) into problem (23), we can obtain

14

K(H, (cid:15)q)

N
(cid:88)

i=1

i α1
pcm

i /B(z)

i

min
B(z)

s.t.

(11f ),
B(z)

i ≥ B(z)
and its Lagrangian is given as

i,min,

(59a)

(59b)

(59c)

L(B(z), ω, ω2)

=

N
(cid:88)

i=1

K(H, (cid:15)q)

pcm
i α1
i
B(z)
i

+ ω

(cid:32) N
(cid:88)

i=1

B(z)

i − Bmax

(cid:33)

,

(60)

where ω ≥ 0 is the Lagrange multiplier associated with con-
straint (11f). The KKT condition is then cast as

= −pcmK(H, (cid:15)q)α1

i /(B(z)

i

)2 + ω = 0,






∂L
∂B(z)
i
(cid:80)N

i=1 B(z)

i = Bmax.

Therefore, we obtain the optimal B(cid:63)
bisection search algorithm to efﬁciently ﬁnd the optimal ω(cid:63).

i as in (25). We then adopt the

