0
2
0
2

t
c
O
5
1

]

C
O
.
h
t
a
m

[

4
v
4
9
9
5
0
.
6
0
9
1
:
v
i
X
r
a

Benders Cut Classiﬁcation via Support Vector Machines for Solving
Two-stage Stochastic Programs

Huiwen Jia∗ Siqian Shen†

Abstract

We consider Benders decomposition for solving two-stage stochastic programs with complete
recourse based on ﬁnite samples of the uncertain parameters. We deﬁne the Benders cuts binding
at the ﬁnal optimal solution or the ones signiﬁcantly improving bounds over iterations as valuable
cuts. We propose a learning-enhanced Benders decomposition (LearnBD) algorithm, which adds
a cut classiﬁcation step in each iteration to selectively generate cuts that are more likely to be
valuable cuts. The LearnBD algorithm includes two phases: (i) sampling cuts and collecting
information from training problems and (ii) solving testing problems with a support vector
machines (SVM) cut classiﬁer. We run the LearnBD algorithm on instances of capacitated
facility location and multi-commodity network design under uncertain demand. Our results
show that SVM cut classiﬁer works eﬀectively for identifying valuable cuts, and the LearnBD
algorithm reduces the total solving time of all instances for diﬀerent problems with various sizes
and complexities.

Keywords: Benders decomposition, two-stage stochastic (integer) programming, support vec-

tor machine (SVM), cut classiﬁcation

1 Introduction

In this paper, we focus on the Benders decomposition (Benders, 1962) and its implementation for
solving a broad class of two-stage stochastic programming models. In the ﬁrst stage, the value of
decision variable x ∈ Rn1 or x ∈ Zn1 is chosen from a feasible region X before the realization of
the uncertainty given the cost vector c ∈ Rn1. In the second stage, decision variable y ∈ Rn2 is
a continuous recourse decision. The matrix W ∈ Rm2×n2, vector h ∈ Rm2, matrix T ∈ Rm2×n1
and cost vector q ∈ Rn2 are subject to uncertainty. We denote the overall uncertain parameter as
ξ = [W, h, T, q]. A two-stage stochastic programming model is given by

min
x∈X

cT x + Eξ[Q(x, ξ)]

(1)

∗Department of Industrial and Operations Engineering, University of Michigan at Ann Arbor, USA. Email:

hwjia@umich.edu;

†Corresponding author; Department of Industrial and Operations Engineering, University of Michigan at Ann

Arbor, USA. Email: siqian@umich.edu.

1

 
 
 
 
 
 
where Eξ[·] takes expectation of · based on the probability distribution of ξ and

Q(x, ξ) def= min
y

qT y

s.t. W y = h − T x.

(2)

We consider a ﬁnite number of realizations of the uncertain parameter ξ, called “scenarios” or
“samples” in the stochastic programming literature (see, e.g., Birge and Louveaux, 2011). Let Ω
be the sample space that contains all the scenarios and each scenario ω ∈ Ω is associated with
a speciﬁc realization ξω = [Wω, hω, Tω, qω] of the uncertain parameter ξ. Denote the occurrence
probability of scenario ω as pω, and thus (cid:80)

ω∈Ω pω = 1. Model (1) can be reformulated as

where

min
x∈X

cT x +

(cid:88)

ω∈Ω

pωQω(x),

Qω(x) def= Q(x, ξω) = min
y

qT
ω y

s.t. Wωy = hω − Tωx.

(3)

(4)

Note that the assumption of having ﬁnite scenarios is made without loss of generality. If the un-
certain parameter ξ = [W, h, T, q] follows a continuous distribution, one can apply the Monte Carlo
sampling approach to generate Ns i.i.d. samples {ω1, . . . , ωNs} of the uncertain parameters. The
second-stage objective function Eξ[Q(x, ξ)] can be replaced by the sample average approximation
(SAA) 1
Ns

i=1 Qωi(x) (Kleywegt et al., 2002).

(cid:80)Ns

1.1 An Overview of Benders Decomposition

With large |Ω|, Model (3) is in general computationally intractable when it involves integer variable
x. The Benders decomposition, which takes advantage of the decomposable structure of two-stage
stochastic programs, is applied widely to optimize variants of Model (3) formulated for a wide range
of applications (see, e.g., Magnanti and Wong, 1981). Creating new variables θω ∈ R, ∀ω ∈ Ω in the
ﬁrst-stage problem, one can formulate a relaxation of the original problem, called relaxed master
problem (RMP), which has an initial form:

(RMP0) min
x∈X ,θ

cT x +

(cid:88)

ω∈Ω

pωθω.

(5)

Subproblems (SPs) are deﬁned as the linear programming dual of the second-stage problems (4)
with dual variable πω ∈ Rm2, ∀ω ∈ Ω. We refer to SPω as the SP for scenario ω, formulated as

(SPω) QD

(hω − Tωx)T πω

ω (x) = max
πω
s.t. W T

ω πω ≤ qω.

(6)

2

Let V ω,t be the set of identiﬁed extreme points of the feasible region of SPω in iteration t, and we
have V ω,0 = ∅. Similarly, let Rω,t be the set of identiﬁed extreme rays of the feasible region of SPω
in iteration t, and Rω,0 = ∅. The two sets are respectively associated with Benders optimality cuts
and feasibility cuts generated during iterations 1, . . . , t − 1, and we will explain the cuts in detail
later. In iteration t, the corresponding RMP is given by:

(RMPt) min
x∈X , θ

cT x +

(cid:88)

pωθω

ω∈Ω
θω ≥ (hω − Tωx)T νω
(hω − Tωx)T ρω ≤ 0

s.t.

ω ∈ Ω, νω ∈ V ω,t;
ω ∈ Ω, ρω ∈ Rω,t.

(7)

After solving RMPt, we obtain an optimal solution (ˆxt, ˆθt). Then for each scenario ω ∈ Ω and its
subproblem SPω, we ﬁrst check whether the current RMPt solution leads to a feasible second-stage
problem by solving a corresponding subproblem with decision variable σω ∈ Rm2, modeled as

(SPω-F) max
σω

(hω − Tω ˆxt)T σω

s.t.

W T

ω σω ≤ 0;
(cid:107)σω(cid:107) ≤ 1.

(8)

If SPω-F has a positive optimal objective value with an optimal solution ¯σω, then from any feasible
solution to SPω, we can move along the direction ¯σω to stay feasible (due to the ﬁrst constraint of
model (8)) but increase the objective value of SPω. This implies that SPω is unbounded and the
second-stage problem is infeasible for given ˆxt. To cut oﬀ the infeasible ﬁrst-stage solution ˆxt, we
generate a Benders feasibility cut:

(hω − Tωx)T ¯σω ≤ 0

(9)

to RMPt+1, which is equivalent to letting Rω,t+1 = Rω,t ∪ {¯σω}. In this paper, we only focus on the
case having complete recourse, under which any feasible ﬁrst-stage solution will result in a feasible
second-stage problem. Therefore, our RMPt (i.e., Model (7)) for each iteration t only contains the
ﬁrst set of optimality cuts, whose derivation is given as follows. If the subproblem is feasible, then
in iteration t we check the optimality of (ˆxt, ˆθt). We solve SPω with x = ˆxt to obtain an optimal
solution ¯πω and the optimal objective value QD
ω (hω − Tω ˆxt). By strong duality, for any
ω (x) = Qω(x). The solution to RMPt will reach the same objective value as the
value of x, QD
original problem when ˆθt
ω (x) indicates that the current
solution (ˆxt, ˆθt) is not optimal for the original problem. Thus, we add a Benders optimality cut

ω ≥ Qω(ˆxt), ∀ω ∈ Ω. Therefore, ˆθt

ω (x) = ¯πT

ω < QD

θω ≥ (¯πω)T (hω − Tωx)

(10)

to RMPt+1, which is equivalent to letting V ω,t+1 = V ω,t ∪ {¯πω}. We refer to the cuts being added
and the corresponding dual extreme points being identiﬁed interchangeably in this paper.

In iteration t, the objective value of RMPt provides a valid lower bound to the origin problem

3

because it is a relaxation. If all SPs have ﬁnite optimal objective values, ˆxt and all recourse solutions
together form a feasible solution to the original two-stage problem, and thus

cT ˆxt +

(cid:88)

pωQω(ˆxt)

ω∈Ω

(11)

provides a valid upper bound. The algorithm terminates when the upper and lower bounds are
equal or their gap is within a pre-speciﬁed tolerance δ. In this paper, we deﬁne the gap as

optimality gap =

upper bound − lower bound
lower bound

× 100%.

(12)

The Benders decomposition converges in a ﬁnite number of iterations due to the ﬁnite number of
dual extreme rays and extreme points of the ﬁnitely many SPs.

1.2 Challenges and Research Overview

While the Benders decomposition method helps to solve two-stage stochastic programs eﬃciently,
it could suﬀer from slow convergence. One reason is that the size of RMPs becomes too large due
to the quickly increased number of newly added cuts over iterations. Geoﬀrion and Graves (1974)
are among the ﬁrst to notice and emphasize on the computational diﬃculty of solving RMPs for
stochastic binary integer programs. Magnanti and Wong (1981) report that over 90% of the total
time of implementing the Benders decomposition is spent on solving RMPs. Minoux (1986) points
out that not all extreme points of the feasible region of SPs equally contribute to restricting the
optimal solution to RMPs. Therefore, a larger number of Benders cuts are not tight at the ﬁnal
optimal solution, but can increase the size of RMPs, which are then extremely hard to solve as
large-scale integer programs.

We propose a two-phase learning-enhanced Benders decomposition (LearnBD) algorithm to
solve two-stage stochastic integer programs with ﬁnite samples of the uncertain parameter and
complete recourse, where the second-stage subproblems are linear programs (LPs). We deﬁne cuts
as valuable cuts when they can either cut the feasible region in the current iteration signiﬁcantly,
or be tight at the ﬁnal optimal solution (see Holmberg, 1990, for a similar deﬁnition in the latter
case). Our goal is to only add valuable cuts to the corresponding RMP in each iteration. Up
to date, there is no practical and systematic way to perform cut classiﬁcation and to accelerate
the iterative process for Benders decomposition for large-scale optimization problems, according to
Rahmaniani et al. (2017). We propose to integrate machine learning techniques into the traditional
Benders decomposition framework to learn cut characteristics and selectively generate subsets of
Benders cuts iteratively.

1.3 Contributions of the Paper

We summarize the main contributions of this paper as follows.

• Firstly, we identify a set of characteristics and quantify performance measures of Benders cuts.

4

We construct a cut classiﬁer using support vector machines (SVM), a widely used supervised
machine learning method that takes history observations and their labels as input, to identify
valuable cuts in each iteration.

• Secondly, we develop the LearnBD algorithm with SVM cut classiﬁer, to limit the size of RMPs
and reduce total solving time. We also provide guidelines for choosing hyperparameters for
enhancing the eﬀectiveness of the LearnBD algorithm.

• Thirdly, we test instances of capacitated facility location and multi-commodity network de-
sign under uncertain demand, to demonstrate the computational advantages of LearnBD in
diﬀerent problem settings. Our results show that the LearnBD algorithm leads to smaller
sizes of RMPs with fewer accumulated cuts, and therefore shorter time for solving RMPs.

1.4 Structure of the Paper

The remainder of this paper is organized as follows.
In Section 2, we review the literature on
the eﬀort of improving Benders decomposition for solving large-scale optimization problems. In
Section 3, we develop the LearnBD algorithm and use SVM for constructing the cut classiﬁer. In
Section 4, we present the computational results of the LearnBD algorithm benchmarked with the
traditional Benders approach. In Section 5, we conclude the paper and describe future research.

2 Literature Review

The Benders decomposition was initially proposed by Benders (1962) and was then widely used for
solving problems of scheduling and planning (Cordeau et al., 2001; Hooker, 2007), network opti-
mization and transportation (Laporte et al., 1994; Costa, 2005; Binato et al., 2001), and inventory
control and management (Federgruen and Zipkin, 1984; Cai et al., 2001). Magnanti and Wong
(1981) and Naoum-Sawaya and Elhedhli (2013) note that directly applying the traditional Benders
decomposition may require excessive computational eﬀort. It is mainly due to the poor convergence
of RMPs that has been computationally demonstrated in Orchard-Hays et al. (1968) and Wolfe
(1970). Several researchers have proposed enhancement strategies depending on diﬀerent problem
structures to accelerate the algorithm accordingly, of which we describe the details below.

In the traditional Benders approach detailed in Section 1.1, RMPs and SPs are solved iteratively,
and thus the ﬁrst stream of studies concentrates on problem-solving techniques, and particularly
techniques for eﬃciently computing RMPs or SPs. Geoﬀrion and Graves (1974) propose to only
sub-optimally solve RMPs in each iteration to enable cut generation, without seeking tight cuts at
the beginning of the Benders approach. Similarly, Raidl (2015) solves RMPs using heuristics to
save computational time. Zakeri et al. (2000) show that sub-optimal solutions to the SPs can still
generate valid cuts in RMPs, and thus eﬀective heuristic approaches are designed for solving the
SPs approximately.

The second stream of studies focuses on decomposition strategies, to guide the process of par-
titioning variables to remain in RMPs or in SPs. Crainic et al. (2014) propose a partial Benders

5

decomposition algorithm to reduce the number of feasibility and optimality cuts, while adding infor-
mation of SPs into RMPs by retaining or creating scenarios. They develop diﬀerent decomposition
strategies for choosing the retaining scenarios when solving two-stage mixed-integer programming
(MIP) models with continuous recourse. In addition, Gendron et al. (2016) propose a non-standard
decomposition strategy, which retains the second-stage variables in RMPs, and the authors test the
results using instances of network design problems.

Machine learning techniques have been applied to general-purpose optimization algorithms for
urging quick convergence to optimal or sub-optimal solutions (see, e.g., He et al., 2014; Khalil et al.,
2017). Khalil et al. (2016) introduce a novel data-driven framework for variable selection to solve
MIP models via branch-and-bound algorithm eﬃciently. Kruber et al. (2017) develop a supervised
learning approach to distinguish a stronger reformulation of a given MIP model and to determine
which decomposition to implement in order to improve the speed of MIP solvers. Misra et al. (2018)
directly construct a model for seeking the optimal solution as a function of the input parameter, by
learning relevant sets of active constraints given computationally expensive large-scale parametric
models.

Recently, several papers apply machine learning to improve algorithmic eﬃciency of decom-
position approaches, especially focusing on cut classiﬁcation. Among them, Tang et al. (2019)
model the cut selection in integer programming as a reinforcement learning (RL) problem. They
deﬁne the corresponding concepts in RL and implement an oﬄine training phase. Baltean-Lugojan
et al. (2019) develop linear outer-approximations of semi-deﬁnite constraints that can be eﬀec-
tively integrated into global solvers. They construct a neural network for predicting the objective
improvement of each cut, which is similar to the performance measure proposed in our paper.

3 Learning-enhanced Benders Decomposition

We develop a Learning-enhanced Benders Decomposition (LearnBD) for solving two-stage stochas-
tic programs with complete and continuous recourse in the second stage. The algorithm aims to
solve a set of two-stage problems that share similar problem structures (i.e., dimensions of decision
variables, constraint matrices, and cost parameters in the objective function) but could have dif-
ferent realizations of the uncertain parameter. As a result, LearnBD constructs a training problem
that shares the same problem structures as the original problem(s), while the distributions of the
uncertain parameter can be diﬀerent. LearnBD samples cut and collects information from the
training problem. Then, it uses the collected cut information to train an SVM cut classiﬁer and
then optimizes the original problem(s).

To ﬁnd potential applications of LearnBD, consider some industries where we need to periodi-
cally solve similar optimization problems with the same system structures and decision frameworks,
but with diﬀerent input data representing the current environment and status of the system. For
example, the unit commitment problem in the power system is solved every hour to determine
the operational schedule of the generating units under random renewable generation and electric-
ity loads (see, e.g., Saravanan et al., 2013; Dashti et al., 2016). A grid operator needs to solve a

6

stochastic program in the form of (3) every hour with diﬀerent ξω-values. If one can solve these
similar problems in an eﬃcient way, it can signiﬁcantly improve the operational eﬃciency of power
grids. The proposed LearnBD can be applied in this case, where one can sample cuts from training
problems using previous days’ data , and re-use the cut classiﬁer for the stochastic programs to be
solved in future hours. Moreover, even for the problems that we only solve once, LearnBD could
be useful. For instance, consider solving stochastic programming models using SAA, where we can
solve a number of SAA-based reformulations with diﬀerent i.i.d. samples repeatedly, by using cut
information collected from solving one such reformulation as a training problem.

LearnBD includes two phases: Oﬄine cut sampling (Phase 1) and solving a given problem using
cut classiﬁcation (Phase 2). The collected cut information can be used to solve any testing problem
of the same variable-and-constraint size. Therefore, the time spent on Phase 1 does not aﬀect the
total solving time. In Phase 1, we solve training problems under the estimation of the uncertainty
and collect training data for cut classiﬁcation. In Phase 2, for a given testing problem, which is
viewed as an unseen testing instance, we train cut classiﬁers using the training data from Phase 1
and apply cut classiﬁcation steps throughout the Benders iterations.

We provide an overview of LearnBD in Figure 1, in which related to Phase 1, K is the number
of sampling paths, N is the length of each sampling path, and RMPn
k , n = 1, . . . , N, k = 1, . . . , K is
the RMP of the training problem corresponding to iteration n in sampling path k. Related to Phase
2, RMPt, t = 0, . . . , T is the RMP of the testing problem corresponding to iteration t and t = T
denotes the last iteration. In Phase 1, we perform cut sampling to generate training data, including
cut characteristics and performance measures (see Section 3.1). In Phase 2, we utilize the classiﬁer
to distinguish valuable cuts from all generated cuts in each iteration and solve RMPs iteratively
by only adding valuable cuts (see Section 3.3). As a sub-procedure in Phase 2, we train an SVM
classiﬁer with the training data generated in Phase 1, which takes cut characteristics as input and
{1, −1} valued label as output to classify whether or not a cut is valuable (see Section 3.2).

3.1 Phase 1: Cut Sampling

In Phase 1, we conduct cut sampling from some training problem to collect the information of
valuable cuts, which will then be used to train the classiﬁer in Phase 2. The training data set D
can be viewed as a Drow × Dcol matrix, where each row is the information of a speciﬁc sample cut,
the ﬁrst Dcol − 1 columns are cut characteristics, and the last column is a {−1, 1} valued label.

Characteristics. Cut characteristics are features of a cut that can help us predict the perfor-
mance of the cut in future iterations if it is added to the current RMP. We consider the following
two characteristics. The ﬁrst is cut violation at the current solution (ˆxt, ˆθt) of RMPt, denoted by
VL and it can be computed as πT
ω, according to (10). This characteristic reﬂects
how large the feasible region of RMPt can be cut oﬀ if adding the cut. The second characteristic
is related to the scenario where a cut is generated from. We denote the number of cuts gener-
ated by the same scenario in previous iterations as NC. This characteristic reﬂects the trade-oﬀ
between exploration and exploitation, two typical learning strategies. A preference to a cut whose

ω (hω − Tω ˆxt) − ˆθt

7

Figure 1: An overview of LearnBD procedures. In Phase 1, we collect cuts from training problems
and label them as valuable cuts (green points) and non-valuable cuts (blue points). Then, we train
an SVM classiﬁer and use it to classify the new cuts we meet in Phase 2 when solving the testing
problem. In Phase 2, we only include the valuable cuts identiﬁed by the SVM classiﬁer in each
iteration.

8

associated scenario generates more cuts in previous iterations, links to an exploitation strategy,
while the opposite preference leads to an exploration strategy. On the one hand, a large number of
cuts generated from the same scenario shows that this scenario is crucial for identifying an optimal
solution. However, it could be the case where the majority of valuable cuts in this scenario have
already been generated. Thus, the change of the objective value of RMP brought by a new cut from
the same scenario can be small. Therefore, the relationship between NC and future performance of
a cut is highly possible to be nonlinear. A collection of characteristics of one speciﬁc cut is referred
to as an observation o, with o = (VL, NC).

In the training data set, each observation also needs to be assigned a
Performance index.
label l, where 1 is assigned to valuable cuts and −1 is assigned to non-valuable cuts. Therefore,
we deﬁne a performance index of each cut and then transform it into {−1, 1} valued label. We
choose the change amount of the objective value of RMPt before and after adding a cut as the
performance index of the cut, denoted by PI. We add exactly one cut to RMPt each time to
recognize the change of objective value brought by the cut. In practice, users can customize the
characteristics and performance index according to speciﬁc applications. The rule for transforming
the performance index will be discussed after we introduce sampling paths next.

k, which is initialized by RMP0.

Sampling path. We construct sampling paths to guide the cut sampling process to record the
cut characteristics and performance index. The number of sampling paths K and the length of
sampling path N are pre-determined hyperparameters. In each sampling path k, k = 1, . . . , K,
we start with RMP0
In iteration n of a sampling path k, for
n = 0, . . . , N − 1, k = 1, . . . , K, we solve RMPn
k ). Then
we follow the Monte Carlo sampling approach to randomly sample one scenario ω ∈ Ω and solve
the corresponding SPω by plugging in (ˆxn
k ): (i) if no optimality cut is generated, then we
continue sampling another scenario and solving the corresponding SP; (ii) if an optimality cut is
generated, we record the two characteristics, instantly add the cut to RMPn+1
, and then record
the performance index. Similar to Section 1.1, we use V ω,n
to denote the set of identiﬁed extreme
points of the feasible region of SPω in iteration n of sampling path k, and RMPn
k is deﬁned in the
following form:

k and obtain an optimal solution (ˆxn

k , ˆθn

k , ˆθn

k

k

(RMPn

k ) min
x∈X , θ

cT x +

(cid:88)

pωθω

ω∈Ω
θω ≥ (hω − Tωx)T νω

s.t.

ω ∈ Ω, νω ∈ V ω,n

k

.

(13)

Once a new cut is generated, we move one step forward in one sampling path and therefore the
iterative process stops after reaching RMPN

k in sampling path k, k = 1, . . . , K.

Through cut sampling, we collect Γ = N × K number of training data. A larger set of training
data in general leads to a more precise classiﬁer. Larger N means that we can collect information
of more representative cuts because we solve RMPs in a wider range of problem sizes. Diﬀerent
independent sampling paths can be conducted in parallel, and therefore, larger K will not signiﬁ-

9

cantly increase the time of Phase 1. However, the cuts generated by RMPs with similar sizes can
share similar characteristics. These similar inputs can also lead to over-ﬁtting and can eventually
weaken the power of the classiﬁer. In our later computational studies, we choose N = 2 × |Ω| and
K = 2.

Remark 1. The cut sampling process is independent across all sample paths, and thus the cut
information collected in diﬀerent sampling paths is independent of one another.

Remark 2. These sampled cut information can be re-used in Phase 2 when solving diﬀerent testing
problems and thus the time of Phase 1 does not aﬀect the total solving time of testing problems.

Algorithm 1 Phase 1 of the LearnBD algorithm.

1: Input: a two-stage stochastic program with a set Ω of scenarios; values of N , K, ∆.
2: Initialize: RMP0 and SPω, ∀ω ∈ Ω of the training problem, D ← ∅.
3: for k = 1, . . . , K do
4:

k ← ∅, ∀ω ∈ Ω, NCω = 0, ∀ω ∈ Ω, RMP0

k ← RMP0 , Dtemp ← ∅.
k} with optimal objective value ˆz0
k.

k, ˆθ0

k, obtain an optimal solution {ˆx0

Initialize: V ω,0
Solve RMP0
for n = 0, . . . , N − 1 do

Randomly select ω(cid:48) ∈ Ω, solve SPω(cid:48), obtain an optimal solution πω(cid:48) and its objective value
ζω(cid:48).
if (ˆθn

k ∪ {πω(cid:48)} , NCω(cid:48) ← NCω(cid:48) + 1, VL ← ζω(cid:48) − (ˆθn

k )ω;

k )ω(cid:48) < ζω(cid:48) then
← V ω,n

V ω,n+1
k
else

Go to Step 7.

end if
, obtain an optimal solution {ˆxn+1
Solve RMPn+1
k
PI ← |ˆzn+1
k − ˆzn
k |,
(Dtemp)n+1,. ← (VL, NCω(cid:48), PI) .

k

, ˆθn+1
k

} with optimal objective value ˆzn+1

k

,

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

18:

19:

20:

21:

22:

end for

15:
16: D ← D ∪ ((Dtemp)N,1, (Dtemp)N,2, 1)
17:

for n = N − 1, . . . , 1 do

if

(Dtemp)n,3
(Dtemp)n+1,3
D ← D ∪ ((Dtemp)n,1, (Dtemp)n,2, −1)

< ∆ then

else

D ← D ∪ ((Dtemp)n,1, (Dtemp)n,2, 1)

end if
end for

23:
24: end for
25: return D

3.2 Subroutine in Phase 2: Classiﬁer Construction

We introduce a subroutine in Phase 2, i.e., constructing SVM classiﬁers with training data D, to
predict the potential performance of cuts and identify valuable cuts. As mentioned in Section 3.1, D
can be presented as a collection of observations and labels of sample cuts, where D = {(od, ld), d =

10

1, . . . , Γ} and more speciﬁcally each cut observation o = (VL, NC). In Section 3.2.1, we show how
SVM works and how to estimate the parameters with training data. In Section 3.2.2, we discuss
the advantages of SVM as a cut classiﬁer.

3.2.1 Building Cut Classiﬁer Using Support Vector Machines (SVM).

SVM is a well-known supervised machine learning approach (see Cortes and Vapnik, 1995; Vapnik,
1998, 1999, 2013) and has been used for analyzing data in many applications. Given a training
data set D = {(od, ld), d = 1, . . . , Γ} from Phase 1, where od ∈ RΣ is an observation (in our
problem Σ = 2), a subset of training data are identiﬁed as support vectors after the training
process. Parameterized by a coeﬃcient vector a ∈ RΓ, and an intercept b ∈ R, the SVM classiﬁer
fSV M (·) : RΣ → {−1, 1} for a new observation o(cid:48) ∈ RΣ (i.e., the collected information of a speciﬁc
cut in our problem), is given by

fSV M (o(cid:48)) = sign

(cid:20) Γ
(cid:88)

(cid:21)
ldadK(o(cid:48), od) + b

,

d=1

(14)

where K(·, ·) : RΣ × RΣ → R is a predetermined kernel function. Here we use one of the most
popular kernel functions, the Radial Basis Function (RBF), in which K(o1, o2) = exp(−γ(cid:107)o1 ·o2(cid:107)2).

Label transformation. We deﬁne a label transformation function to transform continuous per-
formance index into {−1, 1} label, where 1 indicates a valuable cut. The nature of the convergence
of Benders decomposition is accompanied by the fact that the change of the objective value of
RMPs is decreasing over iterations. Therefore, we treat the cuts that can bring a large enough
proportion of PI of the next cut in the same sampling path as valuable cuts. We directly assign
label 1 to the last cut of each cut sampling path. For other cuts, we calculate the ratio of its PI and
the PI of the next cut in the same sampling path and then compare the ratio with a pre-determined
threshold ∆ ∈ [0, 2]. The label transformation function is deﬁned as:

ln
k =




− 1,



1,

< ∆

if

PIn
k
PIn+1
k
otherwise,

n = 0, . . . , N − 1, k = 1, . . . , K

(15)

Larger ∆ shows a more strict rule for recognizing a cut as a valuable cut. With this label transfor-
mation function, one can calculate all labels using current performance indices and use these labels
to train an SVM classiﬁer. We present the algorithmic details in Algorithm 1.

Remark 3. Label transformation function eliminates a degree of dependency across cuts generated
in the same sampling path. Together with Remark 1, all training data are independent with each
other.

The label prediction function fSV M (·) can be interpreted as follows. We can treat the coeﬃcient
ad as a signiﬁcance-magnitude of the corresponding data point d = 1, . . . , Γ, because the label ld is

11

always shown in ad × ld in the predicting process and ad × ld as a whole indicates the power of data
point d for classifying new cuts. The kernel function K(o(cid:48), od) presents the similarity between the
characteristics of a new cut o(cid:48) and cut od. Then the label of o(cid:48) is the sign of a sum of magnitude-
adjusted label of all training data points plus an intercept b. Then by eliminating the training
data with zero estimated coeﬃcients ad, the remaining training data form the support vector set
S, which is a subset of D, and function (14) can be simpliﬁed as

fSV M (o(cid:48)) = sign

(cid:20) (cid:88)

(cid:21)
lsasK(o(cid:48), os) + b
.

s∈S

(16)

The parameters {S, a, b} can be trained by minimizing the prediction loss function among
training data as well as maximizing the ﬂatness of the boundary between valuable and non-valuable
cuts. The prediction loss is computed by the hinge loss function to improve the model sparsity.
Given the estimated result u = (cid:80)Γ
d=1 ldadK(o(cid:48), od) + b from Equation (14) and the ground truth
label l, the loss is calculated by:

Loss(u, l) = max(0, 1 − l · u).

(17)

It can be seen that when l and u have the same sign and |u| ≥ 1, the loss = 0; otherwise the loss
= |u − l|. For brevity, we elaborate on the training process of SVM in Appendix A.

Remark 4. The penalty hyperparameter C balances the explanatory and predictive power of the
classiﬁer. In general, a larger C shows a smaller tolerance of prediction error within the training
dataset and hence results in a classiﬁer with higher explanatory power while too large C will destroy
the predictive power. The discount rate γ in RBF kernel determines the magnitude of similarity
between observations, which is related to the model sensitivity and convergence property. Proper
(γ, C) will generate a relatively small number of support vectors with an accepted classiﬁcation
accuracy. The classiﬁcation accuracy is deﬁned as the percentage of the given cuts whose predicted
labels are the same as the input labels.
In our later computational studies, the classiﬁcation
accuracy of classiﬁers on the training data is almost 100%. Those two hyperparameters are generally
selected together via cross-validation and grid search to reach the best empirical performance.
Typically, the larger C we use, the more support vectors can be identiﬁed by the classiﬁer, and
thus the classiﬁcation eﬀort will increase. The classiﬁcation accuracy on the training data can be
improved, while the accuracy on unseen testing data can be impaired.

3.2.2 Reasons for Choosing SVM.

The advantage of using support vector type of methods for cut classiﬁcation is threefold. Firstly,
with the help of the hinge loss function, SVM only selects representative observations from the
training data. Those observations are referred to as support vectors and are stored for future
classiﬁcation. This sparse nature increases the computational speed for evaluating new cuts. Sec-
ondly, the mechanism of SVM can be explained by using the similarity between a new cut and

12

all support vectors to predict future performance, which is consistent with our assumptions and
motivation that valuable cuts share similarities. Furthermore, the kernel-based method can ﬂexibly
help capture the nonlinear relationship between cut performance and characteristics. Thirdly, the
solving process of SVM is a convex optimization problem (see model (SVM-P) in (A-2)) which is
computationally tractable.

Another support vector type of learning method is support vector regression (SVR) (see, e.g.,
Smola, 2004). The main idea of those two approaches is similar. SVM classiﬁes cuts by {1, −1}
labels and works as a classiﬁer. SVR evaluates continuous scores of cuts and works as a regressor,
which is more informative than a classiﬁer because it can distinguish more rank levels and also allows
any fractional rank between levels. We choose SVM over SVR following concerns listed as follows.
Indeed, cuts have diﬀerent levels of eﬀectiveness for improving RMP solutions. However, we choose
not to spend time and eﬀort to fully distinguish between those levels. Recall that the geometric
explanation of Benders decomposition is to cut oﬀ the feasible region of RMPt in each iteration t.
The cuts generated in one iteration can be linearly independent with each other, and thus they cut
the feasible region from diﬀerent directions. Therefore, it is better to include several valuable cuts
rather than only one cut in each iteration. On the other hand, since we do not select the cuts with
relatively low eﬀectiveness, we do not even need to distinguish among those non-valuable cuts. A
similar reason is also mentioned in the learning approach used for a branch-and-bound algorithm
by Khalil et al. (2016).

Tang et al. (2019) employ a neural network for selecting cuts for solving integer programming
models. One advantage of utilizing a neural network is the complex and high-dimensional data it
can handle and process. The information we use contains coeﬃcients of the current cut and those
of all added cuts. If we use a neutral network, it can evaluate each cut adaptively to the solving
process. In LearnBD, we achieve this iteration-adaptive property by allowing retraining of the cut
classiﬁer (see Remark 5). The training time of SVM classiﬁer is much shorter than that of neural
networks, and the classiﬁer can be trained before solving the testing problems (see Remark 2 and
Remark 7).

3.3 Phase 2: Cut Classiﬁcation

Iteration rule.
In Phase 2, we solve a given two-stage model in the form of (3), which is also
referred to as the testing problem. In iteration t, we solve RMPt of the testing problem and obtain
an optimal solution (ˆxt, ˆθt); by plugging in (ˆxt, ˆθt), we solve all SPω, ∀ω ∈ Ω of the testing problem
and record the two characteristics of each generated cut. Using the characteristic information,
the SVM classiﬁer assigns label 1 to valuable cuts and −1 to non-valuable cuts (see Section 3.2).
Then we add all valuable cuts with label = 1 to RMPt+1. We use V
to denote the identiﬁed
extreme points of the feasible region of SPω in iteration t in Phase 2. We repeat adding cuts until
the optimality gap between upper bound and lower bound, which is deﬁned in (12), is less than
a pre-speciﬁed tolerance δ. If no cut is labeled 1 by SVM classiﬁer, but we have not reached the
optimal tolerance, then we retrain the cut classiﬁer with a smaller ∆ and continue iterating. We
deﬁne a decreasing list L∆ as potential values of ∆, and retrain the SVM classiﬁer by plugging in

ω,t

13

∆ = L∆(l) in the lth retraining. The algorithmic details of Phase 2 are presented in Algorithm 2.

Remark 5. As mentioned in Section 3.2.2, we allow retraining to achieve the iteration-adaptive
property of Benders decomposition. If we deﬁne the state of the two-stage optimization problem
as the set of added cuts to RMP, then the number of possible states is extremely large because the
new cuts are also directly aﬀected by the previous solving trajectory. Thus, it is highly likely that
the state we see during the solving process may not have been encountered during the training data
collection process and consequently, the relationship between the cut features and valuable labels
may not reﬂect the true relationship. In the context of machine learning, this problem, induced
by encountering unseen data points, is also deﬁned as distribution shift. When distribution shift
happens, the previous classiﬁer does not work anymore for predicting the labels of cuts generated
in an unseen state. Actually, distribution shift may happen in several machine learning algorithms
while solving sequential decision-making problems, such as algorithms based on behavioral cloning
in Imitation Learning (see, e.g., Pomerleau, 1991), and thus several studies focus on remedying
distribution shift (see, e.g., Ross et al., 2011; Reddy et al., 2019). In this paper, we propose to
mitigate distribution shift by retraining the SVM classiﬁer with decreasing ∆ in (15), or equivalently,
we enforce a less strict standard for valuable cuts in later iterations.

Remark 6. In algorithmic steps, the SVM classiﬁer is retrained several times in Phase 2.
In
practical implementation, one can train classiﬁers with diﬀerent values of ∆ before starting Phase
2 and call classiﬁers with the speciﬁc ∆-value when solving a problem. Thus, these classiﬁers can be
re-used and the training time of classiﬁers does not aﬀect the total solving time of testing problems.
We summarize the numerical performance of retraining in Remark 7 in Section 4.3.2.

4 Numerical Studies

We evaluate LearnBD on two classes of stochastic programs: (i) Capacitated facility location
problem (CFLP) and (ii) ﬁxed charge multi-commodity network design problem (CMND). CFLP
contains binary ﬁrst-stage variables and continuous second-stage variables with complete recourse.
CMND contains binary ﬁrst-stage variables and continuous second-stage variables. The traditional
formulation of CMND (see, e.g., Crainic et al., 2014) also involves feasibility cuts in the Benders
decomposition. We introduce auxiliary variables in the formulation so that the complete recourse
assumption holds. These problems naturally appear in many applications (see, e.g., Melkote and
Daskin, 2001; Klibi et al., 2010; Klibi and Martel, 2012), and they are notoriously hard to solve
(see, e.g., Geoﬀrion and Graves, 1974; Birge and Louveaux, 2011; Crainic et al., 2001, 2011).

Section 4.1 describes our experimental design and computational settings. Section 4.2 shows
prediction accuracy of the SVM Classiﬁer over diﬀerent validation sets. Section 4.3 presents the
overall results of diverse-sized instances and Section 4.4 presents detailed computational results
over iterations. Section 4.5 presents the time spent on Phase 1 and on training SVM classiﬁers.
Section 4.6 provides results of LearnBD using a classiﬁer trained with cut information collected

14

Algorithm 2 Phase 2 of the LearnBD algorithm.
1: Input: RMP0 and SPω, ∀ω = 1, . . . , Ω of the testing problem, value of δ.
2: Initialize: set of generated cuts V

ω,0

← ∅ and number of cuts NCω ← ∅, ∀ω ∈ Ω, list L∆,

l = 1, train an SVM classiﬁer with ∆ = L∆(l).

3: Solve RMP0, obtain an optimal solution {ˆx0, ˆθ0} and optimal objective ˆz0, t ← 0, UB ← +∞,

LB← ˆz0.

LB > δ do

4: while UB−LB
ncut ← 0.
5:
for ω ∈ Ω do

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

Solve SPω, obtain an optimal solution πω and its optimal objective value ζω.
if (ˆθt)ω < ζω then
VL ← ζω − (ˆθt)ω.
Input {VL, NCω} into SVM classiﬁer.
if Predicted label is 1 then
ω,t

ω,t+1

← V

∪ {πω}; ncut ← ncut + 1; NCω ← NCω + 1.

V
end if

end if
end for
if ncut = 0 then

l = l + 1, train an SVM classiﬁer with ∆ = L∆(l); Continue.

end if

18:
19: UB ← min{ˆzt + (cid:80)
20:

ω∈Ω ζω}.

t ← t + 1, re-solve RMPt, obtain an optimal solution {ˆxt, ˆθt}, optimal objective ˆzt.
LB ← max{LB, ˆzt}.

21:
22: end while
23: return Optimal solution {ˆxt, ˆθt}, optimal objective value ˆzt.

15

from another instance. All the tests are performed on a computer with an Intel Core E5-2630 v4
CPU 2.20 GHz and 128 GB of RAM.

4.1 Experimental Setup and Test Instances

4.1.1 Capacitated Facility Location Problem (CFLP).

Consider a set W of production plants (facilities) and a set F of factories which have uncertain
demand ˜d. The setup cost of facility i, ∀i ∈ W is ki and the production capacity limit is ui. The
demand of factory j, ∀j ∈ F is uncertain and can be satisﬁed by products produced in facility
i, ∀i ∈ W if it is open with a unit transportation cost cij, and the unmet demand will generate
lost-sale with a unit penalty cost ρj. One needs to decide a subset of facilities to open before the
realization of the demand to minimize the expected total cost. We provide the details about RMP
and SP formulations in Appendix B.1.

For our studies, we use problem sets IV and VI in Beasley (1988), which are originally from
Akinc and Khumawala (1977) and Christoﬁdes and Beasley (1983). Each problem set uses the
same network and identical capacity among facilities. Table 1 summarizes the attributes of the
instances, which are originally proposed for the deterministic capacitated facility location problem,
and we apply the techniques in Song et al. (2014) for sampling scenarios. The demand ˜dj of factory
j in each scenario ω follows a Normal distribution with mean equal to the demand used in the
original deterministic instances and standard deviation equal to 0.1 − 0.2 times the mean.

Table 1: Instance Attributes of CFLP

Problem Set

|W |

|F | Capacity u

Setup Cost k

IV
VI

16
16

50
50

5000
15000

7.5 / 12.5 / 17.5 / 25
12.5

4.1.2 Multi-commodity Network Design Problem (CMND).

Consider a directed network with node set N , arc set A, and commodity set K. An uncertain ˜vk
amount of commodity k, ∀k ∈ K must be routed from an origin node, ok ∈ N , to a destination node,
dk ∈ N . The installation cost and arc capacity of arc (i, j), ∀(i, j) ∈ A are fij and uij, respectively.
The cost for transporting one unit of commodity k, ∀k ∈ K on installed arc (i, j), ∀(i, j) ∈ A is
ck
ij. One needs to decide a subset of arcs to install before the realization of the demand to minimize
the expected total cost. We provide details about the RMP and SP formulations in Appendix B.2.
We use the problem sets in Crainic et al. (2014), i.e., ﬁve problem sets (IV–VIII) from the set
of R instances in Crainic et al. (2011). Each problem set uses the same network, with parameters
of each network shown in Table 2. The instances were originally proposed for the deterministic
ﬁxed charge multi-commodity network design problem (Crainic et al., 2001). We apply techniques
in Song et al. (2014) to generate random samples. The demand ˜vk of commodity k in each scenario

16

ω follows a Normal distribution with mean equal to the demand in the deterministic instances and
standard deviation equal to 0.1–0.2 times the mean.

Table 2: Instance Attributes of CMND

Problem Set

|N |

|A|

|K|

IV
V
VI
VII
VIII

10
10
10
10
10

60
60
60
82
83

10
25
50
10
25

4.2 Performance of the SVM Classiﬁer

We ﬁrst take Instance cap41 of CFLP and Instance r082 of CMND as examples to demonstrate the
performance of the SVM classiﬁer. We present the prediction accuracy for cuts in each data set in
Table 3. For each instance, we present the accuracy of ﬁve sets. The ﬁrst set is the training data
set, which contains the cuts sampled in Phase 1. We create four validation data sets consisting
of unseen cuts. To compute the prediction accuracy, we need to compute the “true labels” of
cuts in the validation set by the label transformation function (15). Therefore, all the cuts in
validation sets are collected in the same way as that of Phase 1 (see Algorithm 1), but using
diﬀerent parameters (as shown in Table 3). The validation data set 1 shares the same parameter as
those of the training data set. The validation data set 2 uses the twice standard deviation as that
of the training data set to generate realizations of the uncertain parameters for the optimization
model in diﬀerent scenarios. The validation data set 3 doubles the number of sampling paths, which
can be equivalently viewed as a set sharing the same property with validation data set 1 but with
a twice larger size. The validation data set 4 uses a larger length of the sampling path, which can
be viewed as a set containing more types of cuts, i.e., the validation data set 4 also includes cuts
generated in later iterations in addition to the cuts in the training data set.

Table 3: Prediction Accuracy of the SVM Classiﬁer

Prediction Accuracy (%)

Inst.

cap41
r082

Training Data Validation Data 1 Validation Data 2 Validation Data 3 Validation Data 4

Std = 0.1

Std = 0.1

Std = 0.2

Std = 0.1

Std = 0.1

K = 2 K = 2 K = 2

K = 2

K = 2

K = 2

K = 4 N = 200 K = 2 N = 300

99.78
98.15

78.25
82.95

67.50
74.90

75.00
83.57

78.33
82.10

The results in Table 3 show that the in-sample prediction accuracy is higher than 98% while the
out-of-sample prediction accuracy of the validation data sets is relatively lower. The out-of-sample
prediction accuracy of validation data sets 1, 2, and 4 is at similar levels higher than 75% for both
instances, and the prediction accuracy of Instance r082 is higher. The out-of-sample prediction

17

accuracy of validation data set 2 is the lowest among the validation sets. This is because the
uncertain model parameters across scenarios of the validation data set 2 are diﬀerent from that
of the training data set, and thus the generalization power is weaker than other validation sets.
Please note that the “true labels” of the validation data sets are computed by (15), which are our
belief but not the exact classiﬁcation of valuable or non-valuable cuts.

4.3 Results of Comparing LearnBD with Benders Decomposition

As mentioned in Section 1.2, the main obstacle of the traditional Benders is the large size of RMPs
and, consequently, the long CPU time spent on solving RMPs in each iteration. SPs have smaller
sizes and linear programming structures, and they can be eﬃciently solved in parallel. Thus, the
total time for solving a testing problem with Bender’s approach is almost the cumulative time for
solving RMPs. Therefore, we refer to algorithm eﬃciency as the RMP solving time in the rest of
the paper. In this section, we present the computational results for solving the CFLP and CMND
instances with traditional Benders decomposition (BD) and LearnBD. Speciﬁcally, we show the
number of iterations, optimality gap, number of cuts, and cumulative time of solving RMPs (i.e.,
the last four columns in Table 4, Table 6, and Table 7). For LearnBD, we create a training problem
for collecting cut information and we re-use this information when solving the testing problems.
For all instances, we set the initial value of ∆ as 1.2 and decrease it by 0.01 for each retraining
conducted, i.e., L∆ = [1.2 − 0.01i] for i = 0, . . . , 50.

4.3.1 Results of CFLP.

We compare the results of solving CFLP instances using LearnBD and BD in Table 4. Based on the
problem size and the computational diﬃculty, we set the precision parameter δ = 0.01% and the
time limit as one hour. The solving process terminates when UB-LB
LB < δ (see Step 4 in Algorithm 2)
or the cumulative solving time of the RMP reaches the time limit.

In Table 4, (i) all instances in problem set IV can be optimized within the one-hour time limit.
For these instances, the cuts generated by LearnBD are fewer than those of BD; the time of solving
RMPs in LearnBD is shorter, and the total time reduction ranges from 6% (for cap43-0.2) to 47.5%
(for cap41-0.1). (ii) Instance cap62 is much more diﬃcult to solve compared with other instances in
problem set IV. Both BD and LearnBD exceed the one-hour time limit and LearnBD has a smaller
optimality gap within one-hour time limit. The number of iterations and cuts of LearnBD are more
than those of BD. It happens that LearnBD executes more iterations because it adds fewer cuts
than BD during each iteration. For the second testing problem of the Instance cap62, LearnBD
can solve RMPs with more cuts, which implies that the RMPs of LearnBD is easier to solve than
the ones in BD. In Table 5, we present the time and the number of cuts needed by LearnBD
to reach the same or smaller optimality gap of BD shown in Table 4 for solving Instance cap62.
Demonstrated in Table 5, LearnBD adds fewer cuts and takes much shorter time to achieve similar
optimality gap as BD for the hard instance. (iii) For each instance, by comparing testing problems
with two diﬀerent standard deviations used for generating demand scenarios, we conclude that

18

Table 4: Results of CFLP Instances Solved by LearnBD and BD

Problem Instance Std Training

Set

(× mean)

|Ω| Std Testing Method Number Opt Gap Number Total Time
of RMPs (s)

(× mean)

of Cuts

of Iter.

(%)

cap41

0.1

100

cap42

0.1

200

IV

cap43

0.1

400

cap44

0.1

400

VI

cap62

0.1

50

0.1

0.2

0.1

0.2

0.1

0.2

0.1

0.2

0.1

0.2

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

40
38

47
45

30
32

30
29

27
26

26
26

24
25

22
22

258
215

206
216

–
–

–
–

–
–

–
–

–
–

–
–

–
–

–
–

3.26
2.54

3.06
2.53

4000
3790

4700
4467

6000
4304

6000
5796

10800
10378

10400
10369

9600
9938

8800
8764

12900
10701

10300
10786

183.48
96.31

187.05
141.30

111.96
84.28

93.93
86.72

243.57
220.18

228.66
215.07

235.34
156.52

172.99
134.11

LIMIT
LIMIT

LIMIT
LIMIT

the improvement of LearnBD is more signiﬁcant for testing problems having similar uncertainty
distribution as the training problem. One reason is that, for testing problems with larger standard
deviation, the collected cuts in the training data in Phase 1 can be viewed as a subset of the total
cut population that LearnBD encounters in Phase 2.

Table 5: Comparing LearnBD with BD for Solving Instance cap62 to Similar Accuracy

Problem Instance Std Training

Set

(× mean)

|Ω| Std Testing Method Number Opt Gap Number Total Time
of RMPs (s)

(× mean)

of Cuts

of Iter.

(%)

VI

cap62

0.1

50

0.1

0.2

BD
LearnBD

BD
LearnBD

258
168

206
194

3.26
3.12

3.06
3.05

12900
8357

10300
9686

>3600
1851.10

>3600
2674.99

4.3.2 Results of CMND.

We present the results of LearnBD and BD for solving CMND instances in Table 6, where the
optimality tolerance δ = 1% and the time limit is set as two hours.

In Table 6, most CMND instances take longer time than CFLP instances. We have similar
observations as in Table 4: (i) Instances r046, r054, and r076 can be optimized within the two-hour
time limit and the cumulative solving time of RMPs of LearnBD is signiﬁcantly less than that

19

Table 6: Results of CMND Instances Solved by LearnBD and BD

Problem Instance Std Training

Set

(× mean)

|Ω| Std Testing Method Number Opt Gap Number Total Time
of RMPs (s)

(× mean)

of Cuts

of Iter.

(%)

IV

V

r041

0.1

80

r046

0.1

80

r051

0.1

100

r054

0.1

100

VI

r061

0.1

100

r071

0.1

80

VII

r075

0.1

80

r076

0.1

80

VIII

r082

0.1

80

269
248

275
244

32
28

28
35

185
155

193
190

64
56

85
78

121
125

137
135

159
170

154
167

121
150

119
126

38
52

38
48

106
113

104
106

42.19
22.88

20.44
18.18

–
–

–
–

9.79
3.66

10.37
8.69

–
–

–
–

10.54
9.01

10.28
9.69

958.69
884.19

967.37
885.94

10.89
9.24

11.23
10.07

–
–

–
–

9.77
9.27

9.91
7.13

21520
19761

22000
19283

2560
1708

2240
2094

18500
15210

19300
18803

6400
5398

8500
7041

12100
12288

13700
13602

12720
13194

12320
13215

9680
9621

9520
9350

3040
3026

3040
3080

8480
8873

8320
8156

LIMIT
LIMIT

LIMIT
LIMIT

62.30
41.70

111.53
80.81

LIMIT
LIMIT

LIMIT
LIMIT

585.944
385.89

1172.40
984.71

LIMIT
LIMIT

LIMIT
LIMIT

LIMIT
LIMIT

LIMIT
LIMIT

LIMIT
LIMIT

LIMIT
LIMIT

360.93
276.91

374.81
305.71

LIMIT
LIMIT

LIMIT
LIMIT

0.1

0.2

0.1

0.2

0.1

0.2

0.1

0.2

0.1

0.2

0.1

0.2

0.1

0.2

0.1

0.2

0.1

0.2

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

20

of BD. The cuts in LearnBD are fewer than cuts generated by BD for Instances r046 and r076.
(ii) The rest of instances are hard to solve and both LearnBD and BD reach the two-hour time
limit before converging. Instance r071 is extremely hard to solve and the optimality gaps of the
two testing problems are greater than 100% when reaching time limit. For all of these instances,
LearnBD obtains a smaller optimality gap. In Table 7, we further show the computational time
and number of cuts needed by LearnBD to achieve similar optimality gap as BD for the instances
that were not solved to optimality either by BD or LearnBD in Table 6.

Table 7: Comparing LearnBD with BD for Solving CMND instances to Similar Accuracy

Problem Instance Std Training

Set

(× mean)

|Ω| Std Testing Method Number Opt Gap Number Total Time
of RMPs (s)

(× mean)

of Cuts

of Iter.

(%)

IV

r041

0.1

80

V

r051

0.1

100

VI

r061

0.1

100

r071

0.1

80

VII

r075

0.1

80

VIII

r082

0.1

80

0.1

0.2

0.1

0.2

0.1

0.2

0.1

0.2

0.1

0.2

0.1

0.2

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

269
204

275
158

185
63

193
119

121
71

137
70

159
143

154
139

121
115

119
103

106
55

104
54

42.19
27.41

20.44
20.39

9.79
7.67

10.37
10.36

10.54
10.45

10.28
10.26

958.69
937.63

967.37
966.31

10.89
10.86

11.23
11.19

9.77
9.75

9.91
8.72

21520
13241

22000
12403

18500
6010

19300
11764

12100
6902

13700
6835

12720
11034

12320
10975

9680
6821

9520
7510

8480
4233

8320
4158

> 7200
4140.49

> 7200
2505.91

> 7200
249.52

> 7200
2197.31

> 7200
1129.37

> 7200
1030.30

> 7200
4868.40

> 7200
4816.33

> 7200
2416.36

> 7200
3641.27

> 7200
590.09

> 7200
658.68

In Table 7, LearnBD uses much shorter time to obtain a similar (slightly tighter) gap as the gap
that BD achieves under the two-hour time limit for all instances. And similarly, LearnBD performs
better in testing problems which have similar uncertainty distribution as the training problems.
For instances r051 and r082, LearnBD achieves a similar optimality gap within 10% of the time
spent by BD. Comparing the optimality gap results and the solving time of LearnBD versus BD
in Tables 6 and 7, we notice that BD takes longer to attain solutions with similar optimality gaps
as the ones of LearnBD for the very hard instances that cannot be optimized within the 2-hour
time limit by either method in Table 6. Therefore, we conclude that for instances which are hard

21

to solve: (i) when we allow a relatively large optimality gap, for example, 10% for Instance r051,
LearnBD can achieve this gap in signiﬁcantly shorter time compared to BD; and (ii) when we
allow a relatively small gap, both LearnBD and BD need several iterations and thus long solving
time. Note that LearnBD only takes 249 seconds while BD takes more than 7200 seconds to solve
Instance r051 to a less than 10% gap.

Remark 7. Based on our computational results, retraining almost happens in every instance and
∆ may decrease consecutively over iterations before the classiﬁer can identify valuable cuts, i.e.,
we may consecutively perform retraining. As mentioned in Remark 6, the retraining can be done
before the solving process and thus does not aﬀect the total solving time. The label predicting
process of cuts is extremely fast and can be implemented in parallel. Therefore, the time consumed
by retraining, or consecutive retraining, is also negligible when considering the total solving time.
In our computational tests, LearnBD decreases the values of ∆ relatively more frequently in the
ﬁrst several iterations and then keeps using a ﬁxed ∆ until the termination. For readers’ interest,
we present the values of ∆ during iterations of ﬁve CMND instances in Table 8.

Table 8: ∆-values Throughout Computational Iterations of Five CMND Instances

r041

r051

r071

r075

r076

Iteration

1–3

3–248

∆

1.20

1.13

Iteration

1–2

3

4–155

∆

1.07

1.06

1.01

Iteration

1–5

6–170

∆

1.20

1.12

Iteration

1

2

3

4–5

6–7

8–15

16–150

∆

1.20

1.19

1.18

1.13

1.07

1.06

1.00

Iteration

1

2

3

4

5

6–7

8–10

11–15

16–52

∆

1.20

1.11

1.06

1.05

1.02

1.01

1.00

0.99

0.98

4.3.3 Results of Replicates for CFLP Instance.

In this section, we provide additional computational results for ﬁve replicates of each instance in
the CFLP problem set IV to show the performance consistency of comparing LearnBD and BD
across randomly sampled scenarios. The demand scenarios of ﬁve replicates of each instance are
generated independently with the same standard deviation as the training problem. We present
the minimum, maximum, mean, and median values of (i) the number of generated cuts and (i) the
total solution time of RMPs in Table 9. The observations are consistent with those in Section 4.3.1
and Section 4.3.2 that LearnBD can save more time than BD for optimizing the testing problems.

22

Table 9: Results of Multiple Runs for CFLP Instance from Problem Set IV

Instance Method

Min Max Mean Median Min

Max Mean Median

Number of Cuts

Total Time of RMPs (s)

cap41

cap42

cap43

cap44

BD

3000

4000

3740

LearnBD 2499

3790

3366.2

BD

5800

6000

5920

LearnBD 4304

5798

5123.2

3900

3446

6000

5177

124.38

209.68

161.89

154.11

81.08

173.07

106.55

96.31

71.70

124.95

99.95

104.79

63.10

100.56

78.61

81.39

BD

10400

10800

10640

10800

147.02

263.42

211.64

221.44

LearnBD 10254

10486

10374

10378

145.02

246.86

200.44

214.07

BD

8000

9600

8720

LearnBD 8374

9938

8927.6

8800

8774

171.10

249.28

213.06

217.24

135.97

174.92

159.06

156.52

4.4 Performance Comparison Over Iterations

To track the performance of BD and LearnBD over iterations to show their convergence, we depict
and analyze the results of two speciﬁc instances in Section 4.4.1 and Section 4.4.2. For each instance,
we solve it with both BD and LearnBD separately to the same optimality gap. For each approach,
we record the following values after each iteration:

• the optimality gap after the current iteration, which helps track the algorithm convergence;

• the cumulative time for solving RMPs;

• the total number of cuts added to RMPs in the previous iterations, which reﬂects the size of

RMPs and power of the classiﬁer;

• the cumulative time for solving SPs, which can be performed in parallel in each iteration, and

therefore this value will not aﬀect the total solving time of the algorithm.

4.4.1 CFLP: Problem IV, cap41.

The results over the iterations are shown in Figure 2. The instance has 100 scenarios and the ter-
mination criterion is reaching δ = 0.01% optimality gap. BD requires 40 iterations while LearnBD
takes 38 iterations. LearnBD adds fewer cuts but achieves a similar optimality gap, which indicates
the power of our SVM cuts classiﬁer. Due to the smaller sizes of RMPs, the cumulative time for
solving RMPs in LearnBD is signiﬁcantly shorter than that in BD.

4.4.2 CMND: Problem Set VII, r075.

The results over the iterations for both BD and LearnBD are shown in Figure 3. The instance has
80 scenarios and the termination criterion is reaching δ = 10.89% optimality gap (the result that

23

(a) Optimality gap.

(b) Cumulative time for solving RMPs.

(c) Number of cuts added to RMPs.

(d) Cumulative time for solving SPs.

Figure 2: CFLP: Problem set IV, cap41 instance solved by BD and LearnBD. The horizontal axis
is the iteration number.

24

0510152025303540Iteration0100200300400500600Relative Difference (%)LearnBD, 38, 0.0%BD, 40, 0.01%BDLearnBD0510152025303540Iteration0255075100125150175Cumulative time of solving RMPs (s)BD, 40, 183.48LearnBD, 38, 96.31BDLearnBD0510152025303540Iteration05001000150020002500300035004000# of CutsLearnBD, 38, 3790BD, 40, 4000BDLearnBD0510152025303540Iteration0510152025303540Cumulative time of solving SPs (s)BD, 40, 39.76LearnBD, 38, 21.4BDLearnBDBD achieves within the two-hour time limit). We observe similar performance of LearnBD as in
the case of solving the CFLP instance. In Figure 3a, in the ﬁrst 40 iterations, the gap of LearnBD
convergences slower than BD because it adds fewer cuts. After 100 iterations, both algorithms
achieve similar gaps. In Figure 3c, in the later iterations, the number of added cuts per iteration is
quite similar to that added by BD. Thus, we can conclude that the ﬁrst few iterations are important
for reducing the total solving time.

(a) Optimality gap.

(b) Cumulative time for solving RMPs.

(c) Number of cuts added to RMPs.

(d) Cumulative time for solving SPs.

Figure 3: CMND: Problem set VII, r075 instance solved by BD and LearnBD. The horizontal axis
is the iteration number.

4.5 Sampling and Training Time

As we mentioned in Remark 2 and Remark 6, the cut sampling and classiﬁer training processes can
be done before starting solving the testing problems. The outcomes of these two processes can be
repeatedly used for solving diﬀerent testing problems. Therefore, the time of these two processes
does not aﬀect the total solving time of LearnBD. Here, we record and present the time of Phase 1
and the time of classiﬁer training in Phase 2 in Table 10. Column K shows the number of sampling
paths and Column N shows the length of the sampling paths, i.e., the number of iterations in each
sampling path, in Phase 1. We include the following four columns showing the results of solving

25

020406080100120Iteration05001000150020002500300035004000Relative Difference (%)LearnBD, 115, 10.87%BD, 121, 10.89%BDLearnBD020406080100120Iteration01000200030004000500060007000Cumulative time of solving RMPs (s)BD, 121, 7260.87LearnBD, 115, 2416.36BDLearnBD020406080100120Iteration0200040006000800010000# of CutsLearnBD, 115, 6821BD, 121, 9680BDLearnBD020406080100120Iteration0510152025Cumulative time of solving SPs (s)BD, 121, 26.13LearnBD, 115, 27.89BDLearnBDone testing problem of each instance: Method, Opt Gap, Number of Cuts, and Testing-Time. Note
that the testing problems are diﬀerent from the training problem and for each instance, we only
include results of one testing problem with the same standard deviation as the training problem.
The last two columns in Table 10 are: “Phase-1-Time” showing the total time of cut sampling and
collecting information from training problems in Phase 1 and “SVM-Training-Time” showing the
total time used for training and retraining SVM classiﬁers.

Table 10: Sampling and Training Time

Instance K N Method Opt Gap Number Testing- Phase-1- SVM-Training-
Time (s)

of Cuts Time (s) Time (s)

(%)

r041

r046

r051

r054

r061

r071

r075

r076

r082

2

2

2

2

2

2

2

2

2

80

80

100

100

100

80

80

80

80

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

BD
LearnBD

42.19
22.88

1
1

9.79
3.66

1
1

10.54
9.01

958.69
884.19

10.89
9.24

1
1

9.77
9.27

21520
19761

2560
1708

18500
15210

6400
5398

12100
12288

12720
13194

9680
9621

3040
3026

8480
8873

> 7200
> 7200

62.30
41.70

> 7200
> 7200

585.94
385.89

> 7200
> 7200

> 7200
> 7200

> 7200
> 7200

360.93
276.91

> 7200
> 7200

-
37.63

-
15.29

-
22.39

-
24.41

-
28.59

-
186.77

-
58.94

-
51.93

-
45.26

-
0.01

-
0.01

-
0.01

-
0.38

-
0.40

-
0.30

-
0.22

-
0.21

-
0.30

From Table 10, the time of Phase 1 is highly dependent on N , while the time of each sampling
path is the cumulative time for solving N number of RMPs with n cuts in iteration n for n =
1, . . . , N . In our computational studies, we use N = 2 × |Ω|, i.e., proportional to |Ω|, and thus the
time in Phase 1 is highly related to |Ω|. In most of the instances, the time in Phase 1 and the
training time of SVM classiﬁers are relatively short as compared to the total solving time. Together
with the results in Table 7, for Instances r051, r061, and r082, LearnBD reduces the total solving
time by 80% to reach a 10% optimality gap even if LearnBD conducts Phase 1 for solving only one
testing problem (if we also consider the time in sampling cuts and training classiﬁers).

26

4.6 Classiﬁer Transfer Between Instances

Previously, we propose to solve a given two-stage stochastic optimization problem with cuts sam-
pled from a similar training problem, where the only diﬀerence between the training problem and
the original problem is the underlying distribution of the uncertain parameter. Intuitively, if the
problem can be solved with training data collected from another training problem in a diﬀerent
size, e.g., the diﬀerent number of variables and constraints, then the algorithmic eﬃciency can
be further improved because we can use the same training data, or equivalently speaking, we can
transfer the classiﬁers to solve other problems.

In this section, we present the results where the training problem is diﬀerent from the original
problem as an extension. We consider two instances cap42 and cap62 of CFLP. We normalize
the two cut characteristics, cut violation and number of cuts generated by the same scenario, to
eliminate the incompatible eﬀects of the diﬀerence between the training and testing problems.
, where
Instead of using the absolute value of cut violation, we scale the cut violation by
i∈W,j∈F cij
¯dj is the nominal value of the uncertain demand of factory j, ¯c =
|W |·|F |
transportation cost, and ¯k = (cid:80)
can be reviewed as the relative total transportation cost, which reﬂects the magnitude of the optimal
objective function value of the CFLP problem. The second cut characteristic, the number of cuts
generated by the same scenario, is related to the required number of iterations if using the traditional
Benders. This characteristic is hard to estimate before solving the problem. Thus, we propose to
use the size of the transportation network, i.e., |W | · |F |, to scale it.

i∈W ki/|W | is the average warehouse setup cost. The scalar

is the average
(cid:80)
¯dj ¯c

j∈F
¯k

j∈F
¯k

¯dj ¯c

(cid:80)

(cid:80)

The results are presented in Table 11, for which we set the precision δ as 0.01% and the time limit
as one hour. In the ﬁrst two rows, we solve cap42 with traditional BD and the proposed LearnBD
where the training data is collected from a training problem with the same model parameters (the
results are the same as that of cap42 in Table 4). In the third row, we construct a training problem
from cap62 and then use the training data to train an SVM classiﬁer and solve cap42.

Table 11: Results with Transferred Classiﬁer

Inst.

|Ω|

Std.Testing Method Training Std. Training Number Opt Gap Number Total Time
of RMPS (s)
(× mean)

(× mean)

Instance

of Cuts

of Iter.

(%)

cap42

100

0.1

BD

-

LearnBD

cap42

LearnBD

cap62

-

0.1

0.1

30

32

29

0.01

0.01

0.01

6000

4304

5794

111.96

84.28

97.96

In Table 11, LearnBD with a transferred SVM classiﬁer, i.e., the third row, also reduces the
cumulative time of RMPs as compared to BD. Via comparing the second and the third rows,
LearnBD trained with the same instance adds fewer cuts and takes short time to solve RMPs.
Therefore, we can conclude that with a proper scaling rule of the two cut features, the training
data can also be re-used for solving other instances to improve the solving eﬃciency.

27

5 Conclusions

In this paper, we developed a learning-enhanced Benders decomposition algorithm to accelerate the
solving process of Benders decomposition, one of the most useful algorithms for solving two-stage
stochastic programs. The bottleneck for traditional Benders decomposition is the increasing sizes
and the long solving time of RMPs. We restricted RMP sizes over iterations by distinguishing valu-
able cuts. The computational studies based on capacitated facility location and multi-commodity
network design instances demonstrated the power of SVM cut classiﬁer. With a proper selection of
hyperparameters, the LearnBD algorithm worked eﬃciently with smaller sizes and shorter solving
time of RMPs compared to traditional Benders decomposition.

Our numerical results of diverse instances showed that LearBD can achieve better computa-
tional performance than BD for solving diﬀerent types of benchmark two-stage stochastic programs
considered in the literature. We consider the following future research directions to improve our al-
gorithm. First, we can extend the characteristics and performance indices for the current LearnBD
algorithm to capture multiple types of information of cuts. The second direction is to explore the
possibility of constructing an online learning algorithm using reinforcement learning, which requires
decomposing the eﬀects of multiple cuts added simultaneously into the same RMP. We are also
interested in improving LearnBD for solving a broader range of large-scale problems with special
structural properties.

Acknowledgements

The authors thank the Associate Editor and two reviewers for their constructive feedback and sug-
gestions. The authors gratefully acknowledge the support from the U.S. Department of Engineering
(DoE) grant # DE-SC0018018.

References

U. Akinc and B. M. Khumawala. An eﬃcient branch and bound algorithm for the capacitated

warehouse location problem. Management Science, 23(6):585–594, 1977.

R. Baltean-Lugojan, P. Bonami, R. Misener, and A. Tramontani. Scoring positive semideﬁnite

cutting planes for quadratic optimization via trained neural networks. 2019.

J. E. Beasley. An algorithm for solving large capacitated warehouse location problems. European

Journal of Operational Research, 33(3):314–325, 1988.

J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Nu-

merische Mathematik, 4(1):238–252, 1962.

S. Binato, M. V. F. Pereira, and S. Granville. A new Benders decomposition approach to solve

28

power transmission network design problems.
235–240, 2001.

IEEE Transactions on Power Systems, 16(2):

J. R. Birge and F. Louveaux. Introduction to Stochastic Programming. Springer, 2011.

X. Cai, D. C. McKinney, L. S. Lasdon, and D. W. Watkins Jr. Solving large nonconvex water
resources management models using generalized Benders decomposition. Operations Research,
49(2):235–245, 2001.

C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions

on Intelligent Systems and Technology (TIST), 2(3):1–27, 2011.

N. Christoﬁdes and J. E. Beasley. Extensions to a lagrangean relaxation approach for the ca-
pacitated warehouse location problem. European Journal of Operational Research, 12(1):19–28,
1983.

J.-F. Cordeau, G. Stojkovi´c, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous

aircraft routing and crew scheduling. Transportation Science, 35(4):375–388, 2001.

C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20(3):273–297, 1995.

A. M. Costa. A survey on Benders decomposition applied to ﬁxed-charge network design problems.

Computers & Operations Research, 32(6):1429–1450, 2005.

T. G. Crainic, A. Frangioni, and B. Gendron. Bundle-based relaxation methods for multicommodity
capacitated ﬁxed charge network design. Discrete Applied Mathematics, 112(1-3):73–99, 2001.

T. G. Crainic, X. Fu, M. Gendreau, W. Rei, and S. W. Wallace. Progressive hedging-based meta-

heuristics for stochastic network design. Networks, 58(2):114–124, 2011.

T. G. Crainic, M. Hewitt, and W. Rei. Partial decomposition strategies for two-stage stochastic

integer programs. CIRRELT, 2014.

H. Dashti, A. J. Conejo, R. Jiang, and J. Wang. Weekly two-stage robust generation scheduling
for hydrothermal power systems. IEEE Transactions on Power Systems, 31(6):4554–4564, 2016.

A. Federgruen and P. Zipkin. A combined vehicle routing and inventory allocation problem. Op-

erations Research, 32(5):1019–1037, 1984.

B. Gendron, M. G. Scutell`a, R. G. Garroppo, G. Nencioni, and L. Tavanti. A branch-and-Benders-
cut method for nonlinear power design in green wireless local area networks. European Journal
of Operational Research, 255(1):151–162, 2016.

A. M. Geoﬀrion and G. W. Graves. Multicommodity distribution system design by Benders de-

composition. Management Science, 20(5):822–844, 1974.

29

H. He, H. Daume III, and J. M. Eisner. Learning to search in branch and bound algorithms. In

Advances in Neural Information Processing Systems (NIPS), pages 3293–3301, 2014.

K. Holmberg. On the convergence of cross decomposition. Mathematical Programming, 47(1-3):

269–296, 1990.

J. N. Hooker. Planning and scheduling by logic-based Benders decomposition. Operations Research,

55(3):588–602, 2007.

E. Khalil, H. Dai, Y. Zhang, B. Dilkina, and L. Song. Learning combinatorial optimization algo-
rithms over graphs. In Advances in Neural Information Processing Systems, pages 6348–6358,
2017.

E. B. Khalil, P. Le Bodic, L. Song, G. L. Nemhauser, and B. N. Dilkina. Learning to branch
in mixed integer programming. In Proceedings of the thirtieth AAAI Conference on Artiﬁcial
Intelligence (AAAI-16), pages 724–731, 2016.

A. J. Kleywegt, A. Shapiro, and T. Homem-de Mello. The sample average approximation method

for stochastic discrete optimization. SIAM Journal on Optimization, 12(2):479–502, 2002.

W. Klibi and A. Martel. Scenario-based supply chain network risk modeling. European Journal of

Operational Research, 223(3):644–658, 2012.

W. Klibi, A. Martel, and A. Guitouni. The design of robust value-creating supply chain networks:

a critical review. European Journal of Operational Research, 203(2):283–293, 2010.

M. Kruber, M. E. L¨ubbecke, and A. Parmentier. Learning when to use a decomposition. In In-
ternational Conference on AI and OR Techniques in Constraint Programming for Combinatorial
Optimization Problems, pages 202–210. Springer, 2017.

G. Laporte, F. V. Louveaux, and H. Mercure. A priori optimization of the probabilistic traveling

salesman problem. Operations Research, 42(3):543–549, 1994.

T. L. Magnanti and R. T. Wong. Accelerating Benders decomposition: Algorithmic enhancement

and model selection criteria. Operations Research, 29(3):464–484, 1981.

S. Melkote and M. S. Daskin. Capacitated facility location/network design problems. European

journal of operational research, 129(3):481–495, 2001.

M. Minoux. Mathematical Programming: Theory and Algorithms. John Wiley & Sons, 1986.

S. Misra, L. Roald, and Y. Ng. Learning for constrained optimization: Identifying optimal active

constraint sets. arXiv preprint arXiv:1802.09639, 2018.

J. Naoum-Sawaya and S. Elhedhli. An interior-point Benders based branch-and-cut algorithm for

mixed integer programs. Annals of Operations Research, 210(1):33–55, 2013.

30

W. Orchard-Hays et al. Advanced Linear-programming Computing Techniques. McGraw-Hill, 1968.

D. A. Pomerleau. Eﬃcient training of artiﬁcial neural networks for autonomous navigation. Neural

Computation, 3(1):88–97, 1991.

R. Rahmaniani, T. G. Crainic, M. Gendreau, and W. Rei. The Benders decomposition algorithm:

A literature review. European Journal of Operational Research, 259(3):801–817, 2017.

G. R. Raidl. Decomposition based hybrid metaheuristics. European Journal of Operational Re-

search, 244(1):66–76, 2015.

S. Reddy, A. D. Dragan, and S. Levine. Sqil: Imitation learning via regularized behavioral cloning.

arXiv preprint arXiv:1905.11108, 2019.

S. Ross, G. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction to
no-regret online learning. In Proceedings of the fourteenth international conference on artiﬁcial
intelligence and statistics, pages 627–635, 2011.

B. Saravanan, S. Das, S. Sikri, and D. Kothari. A solution to the unit commitment problem—a

review. Frontiers in Energy, 7(2):223–236, 2013.

B. Sch¨olkopf, A. J. Smola, F. Bach, et al. Learning with Kernels: Support Vector Machines,

Regularization, Optimization, and Beyond. MIT Press, 2002.

A. J. Smola. A Tutorial on Support Vector Regression. Kluwer Academic Publishers, 2004.

Y. Song, J. R. Luedtke, and S. K¨u¸c¨ukyavuz. Chance-constrained binary packing problems. IN-

FORMS Journal on Computing, 26(4):735–747, 2014.

Y. Tang, S. Agrawal, and Y. Faenza. Reinforcement learning for integer programming: Learning

to cut. arXiv preprint arXiv:1906.04859, 2019.

V. Vapnik. Statistical Learning Theory. 1998, volume 3. Wiley, New York, 1998.

V. Vapnik. The Nature of Statistical Learning Theory. Springer, 2013.

V. N. Vapnik. An overview of statistical learning theory. IEEE Transactions on Neural Networks,

10(5):988–999, 1999.

P. Wolfe. Convergence theory in nonlinear programming. Integer and Nonlinear Programming,

pages 1–36, 1970.

G. Zakeri, A. B. Philpott, and D. M. Ryan. Inexact cuts in Benders decomposition. SIAM Journal

on Optimization, 10(3):643–657, 2000.

31

APPENDIX

A Preliminaries of SVM

We start with

(cid:20)
fSV M (o(cid:48)) = sign

(cid:21)
wT φ(o(cid:48)) + b

(A-1)

and u = wT φ(o(cid:48))+b, where φ(·) is a unique mapping function such that K(o1, o2) = (cid:104)φ(o1), φ(o2)(cid:105).
By the kernel trick (see, e.g., Sch¨olkopf et al., 2002), we do not have to know the exact form of φ(·)
and we can employ the SVM model only with kernel function K(·, ·). In Proposition 4, we show this
formulation is equivalent to (16). With a penalty hyperparameter C ≥ 0 assigned to the prediction
error, the objective function for solving the parameters is deﬁned as 1
d=1 ξd, where
the ﬁrst term representing the ﬂatness and ξd, d = 1, . . . , Γ is an auxiliary variable for representing
loss amount of training data (od, ld). The parameters can be solved by an optimization problem:

2 wT w + C (cid:80)Γ

(SVM-P) min
w,ξ,b

s.t.

Γ
(cid:88)

ξd

wT w + C

1
2
ld · (cid:0)wT φ(od) + b(cid:1) ≥ 1 − ξd
ξd ≥ 0

d=1

d = 1, . . . , Γ,

d = 1, . . . , Γ;

(A-2a)

(A-2b)

(A-2c)

where (A-2b) are used to calculate the hinge loss and (A-2c) are sign restrictions of ξ. (SVM-P) is a
convex optimization problem with convex inequality constraints and a quadratic objective function,
and thus it is easy to solve by taking Lagrangian Dual and applying Krash-Kuhn-Tucker (KKT)
conditions (see, e.g., Chang and Lin, 2011).

Proposition 1. The optimal objective value of

min
w,ξ,b

1
2

wT w + C

Γ
(cid:88)

d=1

ξd −

Γ
(cid:88)

d=1

(cid:8)ld · (cid:0)wT φ(od) + b(cid:1) − 1 + ξd

(cid:9) −

ad

Γ
(cid:88)

d=1

vdξd

(A-3)

with any a, v ≥ 0 is a valid lower bound of (SVM-P).

Proof. Assume that (w1, ξ1, b1) is an optimal solution to (SVM-P), and therefore it is feasible to
(cid:1) − 1 + ξ1d ≥ 0
the relaxation (A-3). Given the constraints in (SVM-P), we have ld · (cid:0)wT
and ξ1d ≥ 0 for all d. Therefore, for any a, v ≥ 0, the objective value of (A-3) based on solution
(w1, ξ1, b1) is no larger than 1
d=1 ξ1d. Moreover, as the optimal objective value of
(A-3) is smaller than or equal to the objective value of any feasible solution, we can conclude that
the objective value of (A-3) evaluated at the feasible solution (w1, ξ1, b1) is always smaller than or
equal to 1
d=1 ξ1d, which is the optimal objective value of (SVM-P) and thus provides
a valid lower bound of (SVM-P). This completes the proof.

1 w1 + C (cid:80)Γ

1 w1 + C (cid:80)Γ

1 φ(od) + b1

2 wT

2 wT

By associating dual variables a ≥ 0 with inequality constraints (A-2b) and dual variables v ≥ 0
with inequality constraints (A-2c), we can relax those two sets of constraints and then obtain the

32

corresponding Lagrangian function for any feasible solution (w, ξ, b) as

L(w, ξ, b; a, v) =

wT w + C

1
2

Γ
(cid:88)

d=1

ξd −

Γ
(cid:88)

d=1

(cid:8)ld · (cid:0)wT φ(od) + b(cid:1) − 1 + ξd

(cid:9) −

ad

Γ
(cid:88)

d=1

vdξd.

By weak duality, the Lagrangian problem

min
w,ξ,b

L(w, ξ, b; a, v)

yields a valid lower bound of (SVM-P). Moreover,

max
a≥0,v≥0

min
w,ξ,b

L(w, ξ, b; a, v)

is the dual problem that seeks the best lower bound.

Deﬁnition 1. Krash-Kuhn-Tucker (KKT) is a set of conditions including: Primal feasibility, dual
feasibility, complementary slackness, and the ﬁrst derivative of Lagrangian function L(·) being zero.
If the primal problem is

min
x

f0(x)

subject to fi(x) ≤ 0 ∀i ∈ I
hi(x) = 0 ∀i ∈ I (cid:48)

and the associated dual multipliers are λ ≥ 0 and µ, then the KKT conditions are:

• fi(x∗) ≤ 0 ∀i ∈ I and hi(x∗) = 0 ∀i ∈ I (cid:48) (primal feasibility),

• λ∗ ≥ 0 (dual feasibility),

• fi(x∗)λ∗

i = 0 (complementary slackness),
i ∇fi(x∗) + (cid:80)

i∈I (cid:48) µ∗

i∈I λ∗

• ∇f0(x∗) + (cid:80)

i ∇hi(x∗) = 0 (ﬁrst derivative of L(·) is zero).

Proposition 2. The strong duality holds for (SVM-P) and KKT conditions are satisﬁed at the
optimal primal and dual solution pair.

Proof. (SVM-P) has a quadratic objective and aﬃne inequality constraints, and therefore by Slater’s
condition strong duality holds. Because (SVM-P) is diﬀerentiable, KKT conditions hold at the
global optimum. This completes the proof.

Theorem A.1. The optimal objective value of the optimization problem

max
a≥0,v≥0

min
w,ξ,b

wT w + C

1
2

Γ
(cid:88)

d=1

ξd −

Γ
(cid:88)

d=1

(cid:8)ld · (cid:0)wT φ(od) + b(cid:1) − 1 + ξd

(cid:9) −

ad

Γ
(cid:88)

d=1

vdξd

(A-4)

equals to the optimal objective value of (SVM-P).

33

Proof. Recall the Lagrangian dual problem

max
a≥0,v≥0

min
w,ξ,b

L(w, ξ, b; a, v).

By Proposition 2, strong duality holds and thus the optimal objective value of the dual problem
and primal problem are equal.

Proposition 3. The Lagrangian dual function (A-3) in Proposition 1 can be reformulated as

Γ
(cid:88)

Γ
(cid:88)

ldld(cid:48)adad(cid:48)K(od, od(cid:48)) +

1
2

d(cid:48)=1

d=1
Γ
(cid:88)

with

adld = 0;

d=1

C − ad − vd = 0 d = 1, . . . , Γ.

Γ
(cid:88)

d=1

ad −

Γ
(cid:88)

d=1

vdξd

(A-5a)

(A-5b)

(A-5c)

Proof. The Lagrangian dual function (A-3) is diﬀerentiable, and therefore the derivatives associated
with (w, ξ, b) at the minimum are equal to zero, i.e.,

= 0 → w =

Γ
(cid:88)

d=1

adldφd

= 0 →

Γ
(cid:88)

d=1

adld = 0

(A-6a)

(A-6b)

= 0 → C − ad − vd = 0, d = 1, . . . , Γ → ad ≤ c, d = 1, . . . , Γ.

(A-6c)

∂L
w

∂L
b

∂L
ξ

Plugging in the results in (A-6), we can obtain the reformulation of (A-3) in (A-5). This completes
our proof.

Theorem A.2. The Lagrangian dual problem (A-4) is equivalent to solving a convex quadratic
program:

Γ
(cid:88)

Γ
(cid:88)

ldld(cid:48)adad(cid:48)K(od, od(cid:48)) +

1
2

max
a

s.t.

d(cid:48)=1

d=1
Γ
(cid:88)

adld = 0;

Γ
(cid:88)

d=1

ad

d=1

0 ≤ ad ≤ C d = 1, . . . , Γ.

Proof. By Proposition 3, we obtain an equivalent formulation of (A-4) as follows.

max
a,v≥0

1
2

Γ
(cid:88)

Γ
(cid:88)

d=1

d(cid:48)=1

ldld(cid:48)adad(cid:48)K(od, od(cid:48)) +

Γ
(cid:88)

d=1

ad −

Γ
(cid:88)

d=1

vdξd

34

(A-7a)

(A-7b)

(A-7c)

(A-8a)

with

Γ
(cid:88)

d=1

adld = 0;

C − ad − vd = 0 d = 1, . . . , Γ;

(A-8b)

(A-8c)

In the third term in the objective function (A-8a), all vdξd, ∀d = 1, . . . , Γ are zero at the
optimum because of the complementary slackness by Proposition 2. Therefore, we can discard the
third term without loss of optimality. Moreover, because vd ≥ 0, ∀d = 1, . . . , Γ, we can combine
(A-8c) with v ≥ 0 and derive valid constraints ad ≤ C, ∀d = 1, . . . , Γ, which helps to eliminate
variables vd, ∀d = 1, . . . , Γ. Finally, we can rewrite model (A-8) as shown in (A-7) (see, e.g., Chang
and Lin, 2011). This completes our proof.

d(cid:48)=1 ld(cid:48)(a∗

Proposition 4. The parameter of the classiﬁer in (A-1) are w∗ = (cid:80)Γ
(cid:80)Γ
d(cid:48)K(od, od(cid:48))) for any d = 1, . . . , Γ associated with a∗

dldφd and b∗ = 1 −
d ∈ (0, C). The three prediction
functions (14), (16) and (A-1) are equivalent to each other, where the support vector set S in (16)
contains all (od, ld), d = 1, . . . , Γ such that a∗
d > 0.

d=1 a∗

Proof. The value of w∗ is obtained by (A-6a) and it shows the equivalence between (14) and
(A-1). Assume that we solve and obtain an optimal solution a∗ to (A-7). Then following the
complementary slackness:

(cid:2)ld · (cid:0)wT φ(od) + b(cid:1) − 1 + ξd

(cid:3) = 0, vdξd = 0, ∀d = 1, . . . , Γ,

ad

we have:

d = C > 0, then ld · (cid:0)w∗T φ(od) + b(cid:1) = 1 − ξ∗

• If a∗
ξ∗
d ≥ 0. The observation d is called non-margin support vector.
d < C, then ld · (cid:0)w∗T φ(od) + b(cid:1) = 1 − ξ∗

• If 0 < a∗

d. By a∗

d > 0 and thus
ξ∗
d = 0. The observation d is called margin support vector. Therefore, we can compute
b∗ = 1 − (cid:80)Γ
d(cid:48)K(od, od(cid:48))) with any d = 1, . . . , Γ associated with a∗

d. Similarly, we have v∗

d(cid:48)=1 ld(cid:48)(a∗

d ∈ (0, C),

d = C − v∗

d we have v∗

d = 0 and thus

• If a∗

d = 0, then this type of observation d does not aﬀect the value of the second prediction
d (cid:54)= 0

function. Therefore, we can build a support vector set S of (od, ld), d = 1, . . . , Γ with a∗
and thus simplify (14) as (16).

B Detailed Formulations of Problems for Computational Studies

B.1 CFLP

Consider a set W of production plants (facilities) and a set F of factories which have uncertain
demand ˜d. The setup cost of facility i, ∀i ∈ W is ki and the production capacity limit is ui. The

35

demand of factory j, ∀j ∈ F is uncertain and can be satisﬁed by products produced in facility
i, ∀i ∈ W if it is open with a unit transportation cost cij, and the unmet demand will generate
lost-sale with a unit penalty cost ρj. One needs to decide a subset of facilities to open before the
realization of the demand to minimize the expected total cost.

The two-stage stochastic programming model consists of two types of decisions. We deﬁne
ﬁrst-stage binary decision variables xi, ∀i ∈ W such that xi = 1 if we open facility i and xi = 0
otherwise. In the second stage, we obtain the demand value from each factory and deﬁne continuous
decision variables yij ≥ 0, ∀i ∈ W, j ∈ F , which represent transportation units from facility i to
factory j. The model aims to ﬁnd the best decisions to minimize the facility setup cost, expected
transportation cost, and expected lost-sale cost. The ﬁrst-stage formulation is:

(CFLP) min

x

(cid:88)

i∈W

kixi +

(cid:88)

ω∈Ω

pωQω(x)

s.t. xi ∈ {0, 1}

i ∈ W.

(B-9)

The second-stage problem for each scenario ω is deﬁned using variables yij, i ∈ W, j ∈ F and
auxiliary variables αj, j ∈ F that denote the amount of unmet demand. We have

Qω(x) = min
y,α

s.t.

(cid:88)

(cid:88)

cijyij +

(cid:88)

j∈F

ρjαj

j∈F

i∈W
(cid:88)

yij ≤ uixi

i ∈ W ;

j∈F
˜dω,j −

(cid:88)

i∈W

yij ≥ 0

αj ≥ 0

yij ≤ αj

j ∈ F ;

(B-10)

i ∈ W, j ∈ F ;

j ∈ F.

By allowing unmet demand, the problem always has a feasible solution and Benders decomposition
only generates optimality cuts. Then, we derive the dual of second-stage problems and formulate
SPs as shown in Section 1.1. By deﬁning dual variables hi, ∀i ∈ W and πj, ∀j ∈ F , respectively
associated with the ﬁrst and second constraints in model (B-10), we formulate the subproblem in
scenario ω as

(SPω)

max
h,π

−

(cid:88)

uixihi +

(cid:88)

˜dω,jπj

i∈W

j∈F

s.t. − hi − πj ≤ cij

i ∈ W, j ∈ F ;

(B-11)

0 ≤ πj ≤ ρ

hi ≥ 0

j ∈ F ;

i ∈ W.

Letting V ω,t be a collection of extreme points of SPω that have been identiﬁed when reaching

36

iteration t, we formulate

(RMPt) min
x,θ

(cid:88)

i∈W

kixi +

s.t. θω ≥ −

(cid:88)

i∈W

xi ∈ {0, 1}

(cid:88)

pωθω

ω∈Ω
uixi ˆhi +

(cid:88)

j∈F

˜dω,j ˆπj

(ˆhi, ˆπj) ∈ V ω,t, ω ∈ Ω;

(B-12)

i ∈ W.

B.2 CMND

Consider a directed network with node set N , arc set A, and commodity set K. An uncertain ˜vk
amount of commodity k, ∀k ∈ K must be routed from an origin node, ok ∈ N , to a destination node,
dk ∈ N . The installation cost and arc capacity of arc (i, j), ∀(i, j) ∈ A are fij and uij, respectively.
The cost for transporting one unit of commodity k, ∀k ∈ K on installed arc (i, j), ∀(i, j) ∈ A is ck
ij.
One needs to decide a subset of arcs to install before the realization of the demand to minimize the
expected total cost. In the ﬁrst stage, we make binary decisions xij, ∀(i, j) ∈ A such that xij = 1 if
we install arc (i, j). In the second stage, we obtain the demand of each commodity and then solve
non-negative continuous decisions yk
ij, ∀(i, j) ∈ A, k ∈ K, which represents transportation units of
commodity k on arc (i, j).

The ﬁrst-stage formulation is:

(CMND) min

x

(cid:88)

(i,j)∈A

fijxij +

(cid:88)

ω∈Ω

pωQω(x)

s.t. xij ∈ {0, 1}

(i, j) ∈ A.

(B-13)

The second-stage problem for each scenario ω is deﬁned with decision variables yk
K and auxiliary variables αk

i , ∀i ∈ N, k ∈ K for denoting unmet demand:

ij, ∀(i, j) ∈ A, k ∈

Qω(x) = min
y,α

s.t.

(cid:34)

(cid:88)

(cid:88)

(cid:35)

ijyk
ck

ij + Bαk
i

(i,j)∈A
(cid:88)

k∈K
yk
ji −

(cid:88)

ij ≤ ˜dk
yk

i + αk
i

j:(i,j)∈A

j:(j,i)∈A
(cid:88)
yk
ij ≤ uijxij

k∈K
yk
ij ≥ 0
αk
i ≥ 0

i ∈ N, k ∈ K;

(i, j) ∈ A;

(B-14)

(i, j) ∈ A, k ∈ K;

i ∈ N, k ∈ K.

Deﬁne an auxiliary demand unmet cost B. The parameter ˜dk
the commodity k, −˜vk is node i is the destination of the commodity k, or 0 otherwise.

i is set to ˜vk if node i is the origin of

By allowing unmet demand, the problem always has a feasible solution and Benders decompo-
sition only generates optimality cuts. We derive the dual of second-stage problems and formulate
SPs as shown in Section 1.1. By deﬁning dual variables hk
i , ∀i ∈ N, k ∈ K and πij, ∀(i, j) ∈ A,

37

respectively associated with the ﬁrst and second constraints in model (B-14), we formulate the
subproblem in scenario ω as

(SPω)

max
h,π

(cid:88)

− ˜dk

i hk

i −

(cid:88)

(i,j)∈A

uijxijπij

s.t. hk

j − πij ≤ ck
ij

i∈N,k∈K
i − hk
0 ≤ πij
0 ≤ hk

i ≤ B

(i, j) ∈ A, k ∈ K;

(B-15)

(i, j) ∈ A.

i ∈ N, k ∈ K.

Letting V ω,t be a collection of extreme points of SPω that have been identiﬁed when reaching
iteration t, we formulate

(RMPt) min
x,θ

(cid:88)

fijxij +

(cid:88)

pωθω

(i,j)∈A

s.t. θω ≥

(cid:88)

ω∈Ω
ˆhk
i −

˜dk
i

(cid:88)

uijxij ˆπij

(ˆhi, ˆπij) ∈ V ω,t, ω ∈ Ω;

(B-16)

i∈N,k∈K

(i,j)∈A

xij ∈ {0, 1}

(i, j) ∈ A.

38

