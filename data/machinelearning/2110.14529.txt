1
2
0
2

t
c
O
5
2

]
T
G
.
s
c
[

1
v
9
2
5
4
1
.
0
1
1
2
:
v
i
X
r
a

HSVI FOR ZS-POSGS USING
CONCAVITY, CONVEXITY AND LIPSCHITZ PROPERTIES

A PREPRINT

Aurélien Delage
Univ. Lyon, INSA Lyon, INRIA, CITI,
F-69621 Villeurbanne, France
firstname.lastname@inria.fr

Olivier Buffet
Université de Lorraine, INRIA, CNRS, LORIA,
F-54000 Nancy, France
firstname.lastname@inria.fr

Jilles Dibangoye
Univ. Lyon, INSA Lyon, INRIA, CITI,
F-69621 Villeurbanne, France
firstname.lastname@inria.fr

October 28, 2021

ABSTRACT

Dynamic programming and heuristic search are at the core of state-of-the-art solvers for sequential
decision-making problems. In partially observable or collaborative settings (e.g., POMDPs and
Dec-POMDPs), this requires introducing an appropriate statistic that induces a fully observable
problem as well as bounding (convex) approximators of the optimal value function. This approach
has succeeded in some subclasses of 2-player zero-sum partially observable stochastic games (zs-
POSGs) as well, but failed in the general case despite known concavity and convexity properties,
which only led to heuristic algorithms with poor convergence guarantees. We overcome this issue,
leveraging on these properties to derive bounding approximators and efﬁcient update and selection
operators, before deriving a prototypical solver inspired by HSVI that provably converges to an
(cid:15)-optimal solution in ﬁnite time, and which we empirically evaluate. This opens the door to a novel
family of promising approaches complementing those relying on linear programming or iterative
methods.

1

Introduction

Solving imperfect information sequential games is a challenging ﬁeld with many applications from playing Poker
[17] to security games [1]. We focus on ﬁnite-horizon 2-player 0-sum partially observable stochastic games ((2p)
zs-POSGs) an important class of games coming with compact problem representations that allow for exploiting
structure (e.g., to derive relaxations). From the viewpoint of (maximizing) Player 1, we aim at ﬁnding a strategy with
a worst-case expected return (i.e., whatever Player 2’s strategy) within (cid:15) of the problem’s Nash equilibrium value.
A ﬁrst approach to solving a zs-POSG is to turn it into a 0-sum extensive-form game (zs-EFG) [20]1 addressed
as a sequence form linear program [14, 26, 3], giving rise to an exact algorithm. A second approach is to use an
iterative game solver, i.e., either a counterfactual-regret-based method (CFR) [30, 4], or a ﬁrst-order method [16],
both coming with asymptotic convergence properties. CFR-based approaches now incorporate deep reinforcement
learning and search, some of them winning against top human players at heads-up no limit hold’em poker [18, 4, 5].

In contrast, dynamic programming and heuristic search have not been applied to general zs-POSGs, while often at the
core of state-of-the-art solvers in other problem classes that involve Markovian dynamics, partial observability and
multiple agents (POMDP [21, 22], Dec-POMDP [24, 8], or subclasses of zs-POSGs with simplifying observability
assumptions [9, 6, 2, 12, 7, 11]). They all rely on some statistic that induces a fully observable problem whose value
function (V ∗) exhibits continuity properties that allow deriving bounding approximations. Wiggers et al. [29][28]
contributed two continuity properties, namely V ∗’s concavity and convexity in two different spaces, but which only
led to heuristic algorithms with poor convergence guarantees [27].

1Note: POSGs are equivalent to the large class of “well-behaved” EFGs as deﬁned by Kovaˇrík et al. [15].

 
 
 
 
 
 
HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

We here follow up on this work, successfully achieving the same 3-step process as aforementioned approaches. First,
we obtain a fully observable game for which Bellman’s principle of optimality—the problem being made of nested
subproblems—directly applies (Section 2) by reasoning on the occupancy state [8] (also known for example as
the public (belief) state [18, 5]), i.e., the probability distribution over the players’ past action-observation histories.
Second, we exhibit novel continuity properties of optimal value functions (not limited to V ∗), i.e., we extend
Wiggers et al.’s [28, 29] continuity properties (see also [5]), and introduce complementary Lipschitz-continuity
properties. They allow proposing point-based upper and lower bound approximations, and efﬁcient update and
selection operators based on linear programming (Sec. 3). Third, we adapt Smith and Simmons’ [23] HSVI’s
algorithmic scheme to (cid:15)-optimally solve the problem in ﬁnitely many iterations (Sec. 4). In particular, we adopt the
same changes to the algorithms, and thus to the theoretical analysis of the ﬁnite-time convergence, as in the work of
Horák et al.[12, 11]. These changes are required because, in both cases, the induced tree of possible futures has an
inﬁnite branching factor. Sec. 5 empirically validates the contributed algorithm.

2 Background

Note: We may replace:

• subscript “τ : H − 1” with “τ :”,
• any function f (x) linear in vector x with either f (·) · x or x(cid:62) · f (·), and
• a full tuple with its few elements of interest.

We ﬁrst deﬁne zs-POSGs before recasting them into a new, fully-observable, game (Sec. 2.1). Then, concavity and
convexity properties of this game’s optimal value function V ∗ are presented (Sec. 2.2), before introducing a local
game and our ﬁrst contributed results, which will allow exploiting the nesting of subproblems (Sec. 2.3).

For the sake of clarity, the concepts and results of the EFG literature used in this work are recast in the POSG
setting. We will employ the terminology of behavioral strategies and strategy proﬁles—more convenient in our
non-collaborative setting—instead of deterministic or stochastic policies (private or joint ones)—common in the
collaborative setting of Dec-POMDPs.

A (2-player)
(cid:104)S, A1, A2, Z 1, Z 2, P, r, H, γ, b0(cid:105), where

zero-sum partially observable

stochastic game

(zs-POSG)

is deﬁned by a

tuple

• S is a ﬁnite set of states;
• Ai is (player) i’s ﬁnite set of actions;
• Z i is i’s ﬁnite set of observations;
• P z1,z2

a1,a2 (s(cid:48)|s) is the probability to transition to state s(cid:48) and receive observations z1 and z2 when actions a1
and a2 are performed in state s;

• r(s, a1, a2) is a (scalar) reward function;
• H ∈ N is a (ﬁnite) temporal horizon;
• γ ∈ [0, 1] is a discount factor; and
• b0 is the initial belief state.

Player 1 wants to maximize the expected return, deﬁned as the discounted sum of future rewards, while 2 wants to
minimize it, what we formalize next.

Due to the symmetric setting, many deﬁnitions and results are given from a single player’s viewpoint when only
obvious changes are needed for the other.

From the Dec-POMDP, POSG and EFG literature, we use the following concepts and deﬁnitions, where i ∈ {1, 2}:

−i
θi
τ
θτ
[βi

is i’s opponent. Thus: −1 = 2, and −2 = 1.
= (ai
= (θ1

τ , zi
1, . . . , ai
τ ) (∈ Θ = ∪H−1

τ ) (∈ Θi = ∪H−1

1, zi
τ , θ2

t=0 Θt) is a joint AOH at τ .

t=0 Θi

t) is a length-τ action-observation history (AOH) for i.

τ ] A (behavioral) decision rule (DR) at time τ for i is a mapping βi

τ from private AOHs in Θi

τ to distributions

τ (θi

τ , ai) the probability to pick action ai when facing history θi
τ .

over private actions. We note βi
τ (cid:105) (∈ B = ∪H−1
= (cid:104)β1
βτ
βi
τ :τ (cid:48) = (βi
βτ :τ (cid:48) = (cid:104)β1

τ , β2
τ , . . . , βi
τ :τ (cid:48), β2

t=0 Bt) is a decision rule proﬁle.

τ (cid:48)) is a behavioral strategy (aka policy) for i from time step τ to τ (cid:48) (included).

τ :τ (cid:48)(cid:105) is a behavioral strategy proﬁle (aka joint policy).

2

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

[V0(β0:H−1)] The value of a strategy proﬁle β0:H−1 is

V0(β0:H−1) = E[

H−1
(cid:88)

t=0

γtRt | β0:H−1],

where Rt is the random variable associated to the instant reward at t.

The objective is here to ﬁnd a Nash equilibrium strategy (NES), i.e., a strategy proﬁle β∗
player has an incentive to deviate, which can be written:

0: = (cid:104)β1∗

0: , β2∗

0: (cid:105) such that no

∀β1, V0(β1∗

0: , β2∗

0: ) ≥ V0(β1

0:, β2∗

0: ),

and ∀β2, V0(β1∗

0: , β2∗

0: ) ≤ V0(β1∗

0: , β2

0:).

In such a game, all NESs have the same Nash-equilibrium value (NEV) V ∗
0

def= V0(β1∗

0: , β2∗

0: ).

As explained in the introduction, we aim at deriving an algorithm based on dynamic programming or heuristic
search, as in other sequential decision-making problems. Yet, Bellman’s principle of optimality cannot be directly
applied in a game where players do not share their individual histories, and thus do not have the same information
about the current situation (except at τ = 0). To address this issue, we follow the same idea as for Dec-POMDPs or
some subclasses of zs-POSGs such as One-Sided POSGs [12] to consider a different game where each (new) player
controls an avatar, that interacts with the environment for her, by publicly providing decision rules to be executed,
but not knowing which AOHs are experienced. This is what we do in the following section, demonstrating later that
one can retrieve solution strategies for the original game which are robust to deviations.

2.1 Re-casting POSGs as Occupancy Markov Games

Here, a different, fully observable, zero-sum game is derived from the zs-POSG. To that end, let us deﬁne the
occupancy state σβ0:τ −1 (∈ Oσ
τ ) as the probability distribution over joint AOHs θτ given partial strategy proﬁle
β0:τ −1. This statistic exhibits the following properties (cf. also [8, Thm. 1]).
Proposition 1 (Markov dynamics and rewards – Proof in App. B.1). σβ0:τ −1, together with βτ , is a sufﬁcient
(cid:3), where ⊕
statistics to compute (i) the next OS, σβ0:τ , and (ii) the expected reward at τ : E (cid:2)Rτ | β0:τ −1 ⊕ βτ
denotes a concatenation.

These Markov properties allow introducing an equivalent game (implicitely used by Wiggers et al. [28]), called a
zero-sum occupancy Markov game (zs-OMG),2 and deﬁned by the tuple (cid:104)Oσ, B, T, r, H, γ(cid:105), where:

t=0 Oσ

t ) is the set of OSs induced by the zs-POSG;

• Oσ(= ∪H−1
• B is the set of DR proﬁles of the zs-POSG;
• T is a deterministic transition function that maps each pair (στ , βτ ) to the (only) possible next OS στ +1;
τ , a1, z1, θ2

formally (see proof of Prop. 1 in App. B.1), ∀θ1

τ , a2, z2,

T (στ , βτ )((θ1

τ , a1, z1), (θ2

τ , a2, z2)) def= P r((θ1
τ (θ1

= β1

τ , a1, z1), (θ2
τ (θ2
τ , a1)β2

τ , a2, z2)|στ , βτ )
a (s(cid:48)|s)b(s|θτ ),
P z

τ , a2)στ (θτ )

(cid:88)

(1)

where b(s|θτ ) is a belief state obtained by HMM ﬁltering;

• r is a reward function induced from the zs-POSG as the expected reward for the current OS and DR proﬁle:

s,s(cid:48)

r(στ , βτ ) def= E[r(S, A1, A2)|στ , βτ ] =

(cid:88)

s,θτ ,a

στ (θτ )b(s|θτ )β1

τ (a1|θ1)β2

τ (a2|θ2)r(s, a);

(2)

we use the same notation r for zs-POSGs as the context will indicate which one is discussed;

• H and γ are as in the zs-POSG (b0 is not in the tuple but serves to deﬁne T and r).

The value of a strategy proﬁle β0: will be the same for both games, so that they share the same NEV and NESs. We
will see that, by computing the zs-OMG’s NEV, we can obtain an (cid:15)-optimal zs-POSG solution strategy for 1 or 2 as a
by-product.

A zs-OMG is no standard ﬁnite zs Markov game since (i) it is non-stationary, with different (continuous and of
increasingly dimensionality) state and action spaces at each time step; (ii) at each time step, there are inﬁnitely many
actions, and a mixture of such pure actions is equivalent to a pre-existing pure action; and (iii) the dynamics are

2We use (i) “Markov game” instead of “stochastic game” because the dynamics are not stochastic, and (ii) “partially observable

stochastic game” to stick with the literature.

3

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

deterministic (even for “mixed” actions). But an important beneﬁt of working with a zs-OMG is that both players
know the current OS, στ , thus always share the same information about the game. This will allow ﬁnding an (cid:15)-Nash
equilibrium solution of that game by exploring the tree of partial strategy proﬁles. This tree has an inﬁnite branching
factor due to the continuous action (DR) and state (OS) spaces.

We aim at using HSVI’s algorithmic scheme, which relies on bounding approximations of the optimal value function
(updating them along generated trajectories until (cid:15)-convergence in the initial point). We thus now deﬁne such value
functions and look at their known structural properties, before building on them in Sec. 3 to obtain the required
bounding approximations.

2.2 Concavity and Convexity (CC) Properties of V ∗

A zs-OMG’s subgame at στ is its restriction starting from time step τ under this particular OS (thus looking for
strategies β1
τ :). στ tells which AOHs each player could be facing with non-zero probability, and are thus
relevant for planning. We can then deﬁne the (optimal) value function in any OS στ as follows:

τ : and β2

Vτ (στ , βτ :) def= E[

∞
(cid:88)

t=τ

γt−τ r(St, At)|στ , βτ :],

and V ∗

τ (στ ) def= max
β1
τ :

min
β2
τ :

Vτ (στ , β1

τ :, β2

τ :).

(3)

τ

(θ1

τ , θ2

στ (θ1

τ ) def= (cid:80)

τ . In addition, T 1

τ ), and a conditional one, σc,1

To study V ∗, Wiggers et al. [28] (whose results we extend here from γ = 1 to γ ≤ 1) decompose στ , from 1’s
viewpoint, as a marginal term, σm,1
, so that
θ2
τ
στ = σm,1
m(στ , βτ ) and T 1
τ σc,1
c (στ , βτ ) here denote 1’s marginal and conditional terms associated
to T (στ , βτ ).
τ :, 1 faces a POMDP, and the optimal (POMDP) value in any AOH θ1
Given σc,1
t=τ γt−τ r(St, A1
by ν2
following concavity and convexity properties of V ∗.
Theorem 1 (Concavity and convexity (CC) of V ∗
σm,1
τ

τ is given
. Wiggers et al. then demonstrate the

and a ﬁxed strategy β2
(θ1

τ , and (ii) convex w.r.t. σm,2

τ [28, Thm. 2]). For any τ ∈ {0 . . H − 1}, V ∗
for a ﬁxed σc,2
τ . More precisely,
(cid:104)

τ ) def= στ (θ1
σm,1
τ

τ is (i) concave w.r.t.

τ ) def= maxβ1

for a ﬁxed σc,1

(cid:110)(cid:80)H−1

τ :, σc,1
τ

τ ,θ2
τ )
(θ1
τ )

t ) | θ1

τ (θ2

τ :, β2

t , A2

τ , β1

τ |θ1

τ ,β2

[σc,1

τ :]

(cid:111)

E

(cid:105)

(cid:105)

τ :

τ

τ

V ∗
τ (στ ) = min
β2
τ :

(cid:104)
σm,1
τ

· ν2

[σc,1

τ ,β2

τ :]

= max
β1
τ :

σm,2
τ

· ν1

[σc,2

τ ,β1

τ :]

.

However, this property alone allows approximating V ∗
ﬁnitely many conditional terms σc,i

τ , not for the whole occupancy space.

τ only with a ﬁnite set of vectors ν−i
[σc,i

τ ,β−i
τ : ]

, thus only for

2.3

Introducing Local Games

Subgames (Eq. (3)) involve sufﬁx strategies (over τ : H − 1), while we aim at dealing with DRs for one τ at a
time. Let us then introduce the local game at στ , whose payoff function is the optimal action-value function (which
assumes a known V ∗

τ +1(·)) :

Q∗

τ (στ , βτ ) = r(στ , βτ ) + γV ∗

τ +1(T (στ , βτ )).

(4)

τ (στ , ·, ·) may not be bilinear (cf. Prop. 5, App. 5), so that local games are not amenable to linear programming.

Q∗
But Theorem 1 (and T ’s linearity in β1 and β2) leads to the following result.
Lemma 1 (New result – Proof in App. B.2). Q∗

τ (στ , βτ ) is concave in β1

τ and convex in β2
τ .

von Neumann’s minimax theorem [25] thus applies, and solution strategies can be obtained by respectively maximiz-
ing and minimizing the following two intermediate value functions:

W 1,∗
τ

(στ , β1

τ ) def= min
β2
τ

Q∗

τ (στ , β1

τ , β2

τ ),

and

W 2,∗
τ

(στ , β2

τ ) def= max
β1
τ

Q∗

τ (στ , β1

τ , β2

τ ).

This, plus the inherent nesting of local games, opens the way to applying Bellman’s principle of optimality, building
a solution of the zs-OMG by concatening solutions of subsequent local games.

3 Properties and Approximations of Optimal Value Functions

V ∗’s known structural properties have not been sufﬁcient to derive appropriate approximations for an HSVI-like
algorithm. We solve this issue by proving V ∗’s Lipschitz continuity (LC) (Sec. 3.1) before introducing approximations
of V ∗, W 1,∗, and W 2,∗ (Sec. 3.2), and their related operators (Sec. 3.3).

4

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

3.1 Lipschitz Continuity of V ∗

Establishing V ∗’s Lipschitz continuity starts with properties of T .
Lemma 2 (Proof in App. C.1.1). At depth τ , T (στ , βτ ) is linear in β1
precisely 1-Lipschitz-continuous (1-LC) in στ (in 1-norm), i.e., for any στ , σ(cid:48)
τ :

τ , β2

τ , and στ , where βτ = (cid:104)β1

τ , β2

τ (cid:105). It is more

(cid:107)T (σ(cid:48)

τ , βτ ) − T (στ , βτ )(cid:107)1 ≤ 1 · (cid:107)σ(cid:48)

τ − στ (cid:107)1.

Also, the expected instant reward at any τ is linear in στ (cf. proof of Proposition 1, App. B.1), and thus so is the
expected value of a ﬁnite-horizon strategy proﬁle from τ onwards (Lemma 3). This leads to V ∗
τ being LC in στ
(Theorem 2).
Lemma 3 (Proof in App. C.1.2). At depth τ , Vτ (στ , βτ :) is linear w.r.t. στ .

(or hτ
Theorem 2 (Proof in App. C.1.2). Let hτ
continuous in στ at any depth τ ∈ {0 . . H − 1}, where λτ = 1

def= 1−γH−τ
1−γ

def= H − τ if γ = 1). Then V ∗
2 hτ (rmax − rmin).

τ (στ ) is λτ -Lipschitz

3.2 Bounding Approximations of V ∗, W 1,∗ and W 2,∗

We now derive bounding approximations of (i) V ∗ to compute the gap V τ (στ ) − V τ (στ ), and (ii) W 1,∗ and W 2,∗
to efﬁciently solve the upper- and lower-bounding local games at στ obtained by replacing V ∗
τ +1 by V τ +1 or V τ +1
in Eq. (4) when τ < H − 1.3

Bounding V ∗ – Using both V ∗
τ ’s concavity property (Theorem 1) and its Lipschitz continuity (Lemma 2) allows
deriving the following upper bound approximation (details in App. C.2.1, including its symmetric V τ (στ )) as the
lower-envelope of several upper bounds:

V τ (στ ) =

min
τ (cid:105)∈bagV τ

(cid:104)˜σc,1

τ ,ν2

(cid:2)σm,1

τ

· ν2

τ + λτ (cid:107)στ − σm,1

τ

˜σc,1
τ (cid:107)1

(cid:3) ,

where bagV τ is a set of data points wherein, for each ˜σc,1
(Sec. 2.2), so that (ii) the scalar product gives an upper-bounding hyperplane under ﬁxed ˜σc,1
term allows generalizing to any σc,1
τ .

τ , (i) the vector ν2

τ upper bounds ν2

[˜σc,1

for some β2
τ :
τ , and (iii) the Lipschitz

τ ,β2

τ :]

Upper Bounding W 1,∗ – V ∗
properties of T 1
mation W
τ , β2
(cid:104)˜σc,1

1
τ of W 1,∗
τ +1(cid:105) stored in a set bagW

m(στ , βτ ) and T 1

τ , ν2

τ

τ +1 being Lipschitz in στ +1 (Theorem 2), and exploiting linearity and independence
c (στ , βτ ) (Lemmas 4+5, App. C.2.2), we can derive an upper bound approxi-
) by using ﬁnitely many tuples

τ of W 2,∗

τ

(and conversely a lower bound approximation W 2
1
τ (cf. App. C.2.2):

W

1
τ (στ , β1

τ ) =

(cid:104)˜σc,1

τ ,β2

τ ,ν2

min
τ +1(cid:105)∈bagW 1

τ

(cid:62)

β1
τ

·

(cid:104)
r(στ , ·, β2

τ ) + γT 1

m(στ , ·, β2

τ ) · ν2

τ +1

(5)

+γλτ +1 · (cid:107)T (στ , ·, β2

τ ) − T 1

m(στ , ·, β2

τ )T 1

c (˜σc,1

τ , β2

τ )(cid:107)1

(cid:105)

.

For τ = H − 1, only the reward term is preserved.

We now look at the operators used to manipulate the approximations.

3.3 Related Operators

Selection Operator As detailed in App. C.3.1, given a distribution δ2
now upper bound the value of “proﬁle” (cid:104)β1
matrix (with null columns for improbable histories θ1
as solving a zero-sum game where pure strategies are: for 1, the choice of |Θ1
1
τ . The corresponding linear program, LPW
element of bagW

(cid:62) · M στ · δ2
τ (cid:105) when in στ as β1
τ
τ under σ1
τ ). Solving maxβ1

1
τ (στ ), is:

τ , δ2

τ

τ over tuples (cid:104)˜σc,1

τ , β2

τ , ν2
τ , with M στ an |Θ1
1
τ (στ , β1

1
τ +1(cid:105) in W
τ , we can
1
τ × A1| × |bagW
τ |
τ ) can then be written
τ | actions and, for 2, the choice of 1

W

v

max
β1
τ ,v

s.t. (i) ∀w ∈ bagW

1
τ ,

(ii) ∀θ1

τ ∈ Θ1
τ ,

(cid:62) · M στ

(·,w)

v ≤ β1
τ
(cid:88)

β1
τ (a1|θ1

τ ) = 1,

(6)

3At τ = H − 1, the game maxβ1

H−1

minβ2

H−1

r(σH−1, β1

H−1, β2

H−1) is solved as an LP [28, Sec. 3]).

a1

5

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

and its dual, DLPW

1
τ (στ ), is

v

min
δ2
τ ,v

s.t. (i) ∀(θ1

τ , a1) ∈ Θ1

τ × A1,

(ii)

Strategy Extraction Any tuple wτ ∈ bagW

1
τ contains

• a default strategy for 2 if this is an initial wτ , and

v ≥ M στ
((θ1
(cid:88)

τ

τ ,a1),·) · δ2
δ2
τ (w) = 1.

w∈bagW 1
τ

• both (i) a decision rule β2

τ [wτ ], and (ii) a probability distribution δ2

τ +1 over tuples wτ +1 ∈ bagW

(unless τ = H − 1) otherwise.

(7)

1
τ +1

As a consequence, each such tuple wτ induces a recursively-deﬁned strategy for 2.4

1
δ2
τ needs to be stored as this strategy will play a key role in the following, hence the new deﬁnitions of V τ and W
τ −1
(which rely essentially on the same information (when τ > 1) and will be discussed together): bagV τ contains
tuples (cid:104)σc,1

τ −1 (for τ ≥ 1) related tuples (cid:104)σc,1
Note: For convenience, we explain how to derive an equivalent behavioral strategy in Appendix C.3.6.

τ (cid:105)(cid:105), and bagW

τ −1, (cid:104)δ2

τ −1, β2

τ , (cid:104)δ2

τ , ν2

τ , ν2

τ (cid:105)(cid:105).

1

Initializations One can look for an upper bound of the optimal value function V ∗, i.e., an optimistic bound (an
admissible heuristic) for (maximizing) player 1, by relaxing the problem she faces. To that end, we here solve the
POMDP obtained when 2 is assigned a uniformly random β2,(cid:11)
. At
depth τ , (cid:104)β1,⊗
τ (against β2,(cid:11)
0:
, ν2
τ , (cid:104)δ2,(cid:11)
and under σc,1
τ (cid:105)(cid:105)}
1
(resp. {(cid:104)σc,1
τ .

τ ) is the value of β1,⊗
τ (θ1
in θ1
0:
1
τ −1) is initialized as {(cid:104)σc,1
τ ). Given these strategies, each bagV τ (respectively bagW

τ
is a degenerate distribution over the only element in bagW

0: (cid:105) induces (i) an OS στ and (ii) a vector ν2

, the resulting best response being noted β1,⊗

τ (cid:105)(cid:105)}), where δ2,(cid:11)

0:
τ , where ν2

τ −1, β2,(cid:11)

τ −1, (cid:104)δ2,(cid:11)

, β2,(cid:11)

, ν2

0:

0:

τ

τ

1
τ −1 As depicted in Algorithm 1, lines 7 to 10, V τ and W

Updating V τ and W
Given a tuple (cid:104)στ , σc,1
both δ2
τ + 1 on is given by V τ +1 (= 0 if τ + 1 = H), the value of 1’s best action a1 at θ1

1
τ −1 are updated simultaneously.
τ −1(cid:105) (partly undeﬁned if τ = 0), solving the dual LP (7), which relies on bagV τ +1, gives
τ and (ii) the expected return from

τ . Indeed, assuming that (i) 2 follows strategy δ2

τ and, as a by-product, ν2

τ is upper bounded by:

τ −1, β2

τ (θ1
ν2

τ ) =

1
τ,m(θ1
σ1
τ )

max
a1∈A1

M στ
((θ1

τ ,a1),.) · δ2

τ

(cf. Prop. 8, App. C.3.3).

(8)

One then needs to add (cid:104)σc,1

τ , (cid:104)δ2

τ , ν2

τ (cid:105)(cid:105) to bagV τ , and (if τ ≥ 1) (cid:104)σc,1

τ −1, β2

τ −1, (cid:104)δ2

τ , ν2

τ (cid:105)(cid:105) to bagW

1
τ −1.

Pruning Because they have different forms, bagV τ and bagW
preserve the recursively deﬁned strategies δ2

τ , pruned tuples should be kept in memory.

1
τ −1 have to be pruned independently. Yet, to

V τ relies on a “min-surfaces” (rather than “min-planes”) representation, where each surface is linear in σm,1
and
exploits the Lipschitz-continuity. This allows exploiting (inverted) POMDP max-planes pruning methods so that, as
explained in Theorem 4 (App. C.3.4), whether a test may induce false positives (pruning non-dominated elements) or
false negatives (not pruning dominated elements) carries on from the min-planes setting to our min-surfaces setting.

τ

1
τ involving (i) a reward term that is bilinear (linear in both στ and β1

For its part, W
τ ), and (ii) a possibly non-
continuous term, deriving pruning techniques is not as straightforward. While solving local games may signiﬁcantly
beneﬁt from pruning W

1
τ , we leave this issue for future work.

About Improbable Histories To save on time and memory, we do not store DRs and components of vectors ν2
τ
for 0-probability AOHs ˜θ1
, which carry little relevant information. This leads to replacing, when
computing M στ , undeﬁned components of vectors ν2
τ by a heuristic overestimate such as (cf. App. C.3.5): νinit (not
admissible), and νbMDP (admissible).

τ in current σm,1

τ

4This new space of recursive strategies trivially contains the space of behavioral strategies, which correspond to recursive

strategies whose intermediate distributions are degenerate.

6

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

4 HSVI for zs-POSGs

4.1 Algorithm

τ = ∆(Θτ ) and “beliefs” b(s|θτ ).

HSVI for zs-OMGs, which seeks (cid:15)-optima, is described in Algorithm 1. As vanilla HSVI, it relies on (i) generating
1
trajectories while acting optimistically (lines 14+15), i.e., player 1 (resp. 2) acting “greedily” w.r.t. W
τ (resp.
W 2
τ ), and (ii) locally updating the upper and lower bound approximations (lines 21+22). Both phases rely on
solving the same normal-form games described by LP (6). At τ = H − 1, line 18 selects DRs by solving the exact
game (Sec. 3.2), and line 8 returns a distribution reduced to the single element added in line 19. Note that the
implementation maintains full occupancy states oτ ∈ ∆(S × Θτ ), which allow easily retrieving both “simple” OSs
στ ∈ Oσ
A key difference with Smith and Simmons’ HSVI algorithm [23] lies in the criterion for stopping trajectories. In
vanilla HSVI (for POMDPs), the ﬁnite branching factor allows looking at the convergence of V and V at each point
reachable under an optimal strategy. To ensure (cid:15)-convergence at σ0, trajectories just need to be interrupted when the
current width at στ ( def= V τ (στ ) − V τ (στ ) ) is smaller than a threshold γ−τ (cid:15). (This happens even if γ = 1 due to
the approximation’s width falling to 0 beyond H.) Here, dealing with an inﬁnite branching factor, one may converge
towards an optimal solution while always visiting new points of the occupancy space. To counter this, we bound the
width within balls around visited points by exploiting V ∗’s Lipschitz continuity. This is achieved by adding a term
− (cid:80)τ
i=1 2ρλτ −iγ−i [12] (even if γ = 1) to ensure that the width is below γ τ (cid:15) within a ball of radius ρ around the
current point (here στ ), hence the threshold

thr(τ ) def= γ−τ (cid:15) −

τ
(cid:88)

i=1

2ρλτ −iγ−i.

Algorithm 1: zs-OMG-HSVI(b0, [(cid:15), ρ]) [here returning solution strategy δ1

0 for Player 1]

1 Fct zs-OMG-HSVI(b0 (cid:39) σ0)

∀τ ∈ 0 . . H − 1, initialize V τ , V τ , W
while (cid:2)V 0(σ0) − V 0(σ0) > thr(0)(cid:3) do

1
τ , & W 2
τ

Explore(σ0, 0, −, −)
(cid:16)

arg max
0 ,ν1

0(cid:105)(cid:105)∈bagV

0 ,(cid:104)δ1

0

δ1
0 ←

(cid:104)σc,1
return δ1
0

σm,2
0

· ν1

0 + λ0 · 0

(cid:17)

2

3

4

5

6

8

7 Fct Update(V τ , W
τ , ν2

1
τ −1, (cid:104)στ , σc,1
τ −1, β2
1
τ (στ , δ2
(cid:104)δ2
τ )
τ (cid:105) ← DLPW
bagV τ ← bagV τ ∪ {(cid:104)σc,1
τ , (cid:104)δ2
τ , ν2
τ (cid:105)(cid:105)}
1
τ −1 ∪ {(cid:104)σc,1
τ −1, β2
τ −1 ← bagW
bagW

τ −1

1

9

10

(cid:105))

τ −1

, (cid:104)δ2

τ , ν2

τ (cid:105)(cid:105)}

13

14

15

16

17

18

19

20

21

22

11 Fct Explore(στ , τ, στ −1, βτ −1)
12

if (cid:2)V τ (σ) − V τ (σ) > thr(τ )(cid:3) then

if τ < H − 1 then
1
τ ← LPW
← LPW 2

1
τ (σ, β1
τ )
τ (σ, β2
τ )
1
τ , β2
Explore(T (στ , β

β
β2
τ

), τ + 1, στ , (cid:104)β

1

τ , β2

τ

τ

1

(β

else (τ = H − 1)
τ , β2
bagW
bagW 2
τ

Update(V τ , W
Update(V τ , W 2

1

τ )(cid:1)
τ , β2
τ ∪ {(cid:104)σc,1
τ , β2
τ
1
∪ {(cid:104)σc,2
τ , −(cid:105)}
τ , β

, −(cid:105)}

) ← NES (cid:0)r(σ, β1
τ
1
τ ← bagW
← bagW 2
τ
1
τ −1, (cid:104)στ , σc,1
τ −1, (cid:104)στ , σc,2

(cid:105))

τ −1, β2
τ −1, β

τ −1
1
τ −1(cid:105))

(9)

(cid:105))

Setting ρ As can be observed, this threshold function should always return positive values, which requires a small
enough (but > 0) ρ. For a given problem (cf. Prop. 9, App. D.1.1), the maximum possible value ρmax depends on the
Lipschitz constants at each time step, which themselves depend on the initial upper and lower bounds of the optimal
value function. But what is the effect of setting ρ ∈ (0, ρmax) to small or large values?

• The smaller ρ, the larger thr(τ ), the shorter the trajectories, but the smaller the balls and the higher the
required density of points around the optimal trajectory, thus the more trajectories needed to converge.

• The larger ρ, the smaller thr(τ ), the longer the trajectories, but the larger the balls and the lower the required

density of points around the optimal trajectory, thus the less trajectories needed to converge.

So, setting ρ means making a trade-off between the number of generated trajectories and their length.

4.2 Finite-Time Convergence

Theorem 3 (Proof in App. D.2.1). zs-OMG-HSVI (Algorithm 1) terminates in ﬁnite time with an (cid:15)-approximation of
V ∗
0 (σ0).

7

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

Proof. (sketch adapted from Horák and Bošanský [11]) Assume for the sake of contradiction that the algorithm
does not terminate and generates an inﬁnite number of explore trials. Then, the number of trials of length T (for
some 0 ≤ T ≤ H) must be inﬁnite. It is impossible to ﬁt an inﬁnite number of occupancy points σT satisfying
(cid:107)σT − σ(cid:48)
T . There must thus be two trials of length T , {στ,1}T
τ =0, such that
(cid:107)σT −1,1 − σT −1,2(cid:107)1 ≤ ρ, and one can show (as detailed in Appendix D.2.1) that the second trial should not have
happened.

T (cid:107)1 > ρ within Oσ

τ =0 and {στ,2}T

The ﬁnite time complexity suffers from the same combinatorial explosion as for Dec-POMDPs, and is even worse
as we have to handle "inﬁnitely branching" trees of possible futures. More precisely, the bound on the number of
iterations depends on the number of balls of radius ρ required to cover occupancy simplexes at each depth.

Also, the following proposition allows solving inﬁnite horizon problems as well (when γ < 1) by bounding the
length of HSVI’s trajectories using the boundedness of V − V and the exponential growth of thr(τ ).
Proposition 2 (Proof in App. D.2.2). When γ < 1, using the depth-independent Lipschitz constant λ∞, and with
(0)
W def= (cid:107)V
− V (0)(cid:107)∞ the maximum width between initializations, the length of trajectories is upper bounded by
(cid:24)
def=

1−γ

(cid:25)

.

Tmax

logγ

(cid:15)− 2ρλ∞
W − 2ρλ∞

1−γ

As in the Dec-POMDP case, the length of trajectories required to approximate a discounted criterion is non-
exponential.

4.3 Execution

·ν2

As can be noted, any strategy δ2
τ in a tuple w guarantees at most (i.e., at worst from 2’s viewpoint) expected return
σm,1
τ if in the associated στ , whatever 1’s strategy. This holds in particular at τ = 0, where σ0 always corresponds
τ
to the initial OS. Thus, if 2 executes a strategy δ2,(cid:63)
· ν0, then her expected return is at
0 ,(cid:104)ν2
most V 0(σ0) (≤ V ∗
0 (σ0) + (cid:15)) (whatever 1’s strategy). Solving the derived zs-OMG therefore provides a solution
strategy for each player in the original zs-POSG, and each player can derive her strategy on her own (no need for a
coordinating central planner as for Dec-POMDPs). For instance, Alg. 1 returns a solution strategy only for 1.

0 ∈ arg max(cid:104)σc,1

0 (cid:105)(cid:105) σm,1

0,δ2

0

5 Experiments

The experiments aim at validating the proposed approach. Additional results appear in Appendix E.

5.1 Setup

Benchmark Problems Four benchmark problems were used. Mabc and Recycling Robot are well-known Dec-
POMDP benchmark problems (cf. http://masplan.org) and were adapted to our competitive setting by making
Player 2 minimize (rather than maximize) the objective function. Adversarial Tiger and Competitive Tiger were
introduced by Wiggers [27]. We only consider ﬁnite horizons and γ = 1.

Algorithms Algorithm 1 is denoted OMGHSVILC
CC, while OMGHSVILCdenotes a variant relying only on the Lipschitz
continuity (cf. Appendix F), and used to highlight the importance of exploiting the concavity and convexity properties.
They are compared against Sequence Form LP [14], and Wiggers’ two heuristic algorithms, Informed and Random
[27], which rely on the concavity and convexity.5

OMGHSVILC ran with an error (cid:15) speciﬁed in Table 1, and λτ = H · (rmax − rmin). OMGHSVILC
CC ran with an error
(cid:15) = 0.01, λτ = H · (rmax − rmin), ρ the middle of its feasible interval, and the heuristic estimate for missing
components of ν2
τ indicated in Table 1.6 We also use FB-HSVI’s LPE lossless compression of probabilistically
equivalent AOHs in OSs, so as to reduce their dimensionality [8].

Experiments ran on an Ubuntu machine with i7-10810U 1.10 GHz Intel processor and 16 GB available RAM. We
intend to make the code available within coming months under MIT license.

5.2 Results

A ﬁrst observation is that both OMGHSVILC
value at σ0, and reduce the gap progressively (cf. ﬁgures in App. E). Table 1 shows that (i) OMGHSVILC

CC and OMGHSVILC maintain valid lower and upper bounds of the optimal
CC is always

5We use Wiggers’ own (unreleased) Sequence Form LP solver, but could only copy the results for the two heuristic algorithms

(based on a single run despite their randomization) from [27].

6The inadmissible heuristic νinit failed only on Adversarial Tiger.

8

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

Table 1: Experiments comparing 4 solvers on various benchmark problems. Reported values are the running times, or
[gap] values [V 0(σ0) − V 0(σ0)] if the 24 h timeout limit is reached. “(ni)” indicates no improvement over the initialization
after 24 h. “xx” indicates an out-of-memory error. “n/a” indicates an unavailable result.

Adversarial Tiger

H=2

H=3

H=4

Wiggers Random
Wiggers Informed
OMGHSVILC(0.1)
OMGHSVILC
CC(νbMDP)
Sequence Form LP

[0.04]
[0.59]
22 min
1 s
0.02 s

[0.38]
[1.32]
(ni)
44 s
0.17 s

[0.92]
[1.79]
(ni)
[1.79]
3 s

Competitive Tiger

H=2

H=3

H=4

Wiggers Random
Wiggers Informed
OMGHSVILC(1.0)
OMGHSVILC
CC(νinit)
Sequence Form LP

Mabc

OMGHSVILC(0.05)
OMGHSVILC
CC(νinit)
Sequence Form LP

Recycling Robot

[0.56]
[2.07]
[2.8]
6 s
0.14 s

H=2

2 s
1 s
0.1 s

H=3

OMGHSVILC(0.2)
OMGHSVILC
CC(νinit)
Sequence Form LP

(ni)
3 min
1 s

[2.67]
[2.33]
(ni)
[0.04]
48 s

[5.81]
[3.61]
(ni)
[2.30]
14 min

H=3

(ni)
34 s
1 s

H=4

(ni)
12 h
10 s

H=4

(ni)
[0.05]
3 s

H=5

(ni)
[0.77]
1.5 h

H=5

[2.07]
[3.34]
(ni)
[2.27]
107 s

H=5

[6.97]
xx
(ni)
[4.92]
2.5 h

H=5

(ni)
[0.44]
181 s

H=6

(ni)
[2.65]
xx

better than OMGHSVILC and Wiggers’ [27] algorithms, and (ii) unless running out of memory, Sequence Form LP
always outperforms OMGHSVILC
CC to exploit some games’
structure and thus generate trajectories even for large horizons (e.g., Recycling Robot for H = 6, cf. App. E). More
CC in 24 h is highly correlated to the
generally, we observe that the number of iterations performed by OMGHSVILC
quality of the LPE compression (Recycling Robot compresses the most and Adversarial Tiger the least). As expected,
OMGHSVILC turns out to be very slow, not terminating even its ﬁrst iteration in most cases.

CC. However, the LPE compression allows OMGHSVILC

6 Discussion

Inspired by state-of-the-art solution techniques for POMDPs, Dec-POMDPs, and subclasses of zs-POSGs, we solve
here zs-POSGs by turning them into zero-sum occupancy Markov games, i.e., a fully-observable game that allows
exploiting Bellman’s principle of optimality. We expand the concavity-convexity and Lipschitz-continuity properties
of V ∗ and Q∗, and build on them to propose point-based bounding value function approximations, along with
efﬁcient selection and update operators based on linear programming. This allows deriving a variant of HSVI that
provably converges in ﬁnite time to an (cid:15)-optimal solution, providing (safe) solution strategies in a recursive form as a
by-product of the solving process. Experiments conﬁrm the feasibility of this approach and show improved results
compared to related heuristics (also exploiting the concavity and convexity).

This approach paves the way for a large family of solvers as many variants could be envisioned, e.g., using different
algorithmic schemes, approximations, selection and update operators, or pruning techniques. For instance, we also
evaluated a variant relying only on V ∗’s Lipschitz-continuity.

Future work includes: looking for better initializations, e.g., with more advanced POMDP initializations or building
on One-Sided zsPOSGs, and better Lipschitz constants, possibly through an incremental search; proposing a pruning
method for W
τ ; exploiting oracle methods or other heuristics to solve local games faster; exploiting TPE
rather than LPE compression; and branching on public observations (or even public information revealed by the
occupancy state’s structure).

1
τ and W 2

Acknowledgements Let us thank Abdallah Safﬁdine, Vincent Thomas, and anonymous reviewers for fruitful
discussions and comments that helped improve this work.

This work was supported by the French National Research Agency through the “Planning and Learning to Act in
Systems of Multiple Agents” Project under Grant 19-CE23-0018-01. [http://perso.citi-lab.fr/jdibangoy/
#/plasma]

9

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

Bibliography

[1] Nicola Basilico, Giuseppe De Nittis, and Nicola Gatti. A security game combining patrolling and
alarm–triggered responses under spatial and detection uncertainties. In Proceedings of the Thirtieth AAAI
Conference on Artiﬁcial Intelligence, 2016.

[2] Arnab Basu and Lukasz Stettner. Finite- and inﬁnite-horizon Shapley games with nonsymmetric partial

observation. SIAM Journal on Control and Optimization, 53(6):3584–3619, 2015.

[3] Branislav Bošanský, Christopher Kiekintveld, Viliam Lisý, and Michal Pˇechouˇcek. An exact double-oracle
algorithm for zero-sum extensive-form games with imperfect information. Journal of Artiﬁcial Intelligence
Research, 51:829–866, 2014. doi: 10.1613/jair.4477.

[4] Noam Brown and Tuomas Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats top

professionals. Science, 359(6374):418–424, 2018.

[5] Noam Brown, Anton Bakhtin, Adam Lerer, and Qucheng Gong. Combining deep reinforcement learning
and search for imperfect-information games. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 17057–17069. Curran
Associates, Inc., 2020.

[6] Krishnendu Chatterjee and Laurent Doyen. Partial-observation stochastic games: How to win when belief fails.

ACM Transactions on Computational Logic, 15(2):16, 2014.

[7] Harold L. Cole and Narayama Kocherlakota. Dynamic games with hidden actions and hidden states. Journal of

Economic Theory, 98(1):114–126, 2001.

[8] Jilles Dibangoye, Chris Amato, Olivier Buffet, and François Charpillet. Optimally solving Dec-POMDPs as

continuous-state MDPs. Journal of Artiﬁcial Intelligence Research, 55:443–497, 2016.

[9] Mrinal K. Ghosh, David R. McDonald, and Sagnik Sinha. Zero-sum stochastic games with partial information.

Journal of Optimization Theory and Applications, 121(1):99–118, April 2004.

[10] Karel Horák. Scalable Algorithms for Solving Stochastic Games with Limited Partial Observability. PhD thesis,

Czech Technical University in Prague, Faculty of Electrical Engineering, 2019.

[11] Karel Horák and Branislav Bošanský. Solving partially observable stochastic games with public observations.
In Proceedings of the Thirty-Third AAAI Conference on Artiﬁcial Intelligence, pages 2029–2036, 2019.

[12] Karel Horák, Branislav Bošanský, and Michal Pˇechouˇcek. Heuristic search value iteration for one-sided partially
observable stochastic games. In Proceedings of the Thirty-First AAAI Conference on Artiﬁcial Intelligence,
pages 558–564, 2017.

[13] D. Koller, N. Megiddo, and B. von Stengel. Fast algorithms for ﬁnding randomized strategies in game trees. In
Proceedings of the 26th ACM Symposium on the Theory of Computing (STOC’94), pages 750–759, 1994.

[14] Daphne Koller, Nimrod Megiddo, and Bernhard von Stengel. Efﬁcient computation of equilibria for extensive

two-person games. Games and Economic Behavior, 14(51):220–246, 1996.

[15] Vojtˇech Kovaˇrík, Martin Schmid, Neil Burch, Michael Bowling, and Viliam Lisý. Rethinking formal models of

partially observable multiagent decision making. CoRR, abs/1906.11110, 2019.

[16] Christian Kroer, Kevin Waugh, Fatma Kılınç-Karzan, and Tuomas Sandholm. Faster algorithms for extensive-
form game solving via improved smoothing functions. Mathematical Programming, 179:385–417, 2020. doi:
10.1007/s10107-018-1336-7.

[17] Harold W. Kuhn. Simpliﬁed two-person Poker. In H. W. Kuhn and A. W. Tucker, editors, Contributions to the

Theory of Games, volume 1. Princeton University Press, 1950.

[18] Matej Moravˇcík, Martin Schmid, Neil Burch, Viliam Lisý, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin
Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artiﬁcial intelligence in heads-up
no-limit poker. Science, 356(6337):508–513, 2017.

[19] Rémi Munos. From bandits to Monte-Carlo Tree Search: The optimistic principle applied to optimization and

planning. Foundations and Trends in Machine Learning, 7(1):1–130, 2014.

[20] Frans Oliehoek and Nikos Vlassis. Dec-POMDPs and extensive form games: equivalence of models and
algorithms. Technical Report IAS-UVA-06-02, Intelligent Systems Laboratory Amsterdam, University of
Amsterdam, 2006.

[21] Karl Åström. Optimal control of Markov processes with incomplete state information. Journal of Mathematical

Analysis and Applications, 10(1):174 – 205, 1965. ISSN 0022-247X.

[22] Trey Smith. Probabilistic Planning for Robotic Exploration. PhD thesis, The Robotics Institute, Carnegie

Mellon University, 2007.

10

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

[23] Trey Smith and R.G. Simmons. Point-based POMDP algorithms: Improved analysis and implementation. In

Proceedings of the Twenty-First Conference on Uncertainty in Artiﬁcial Intelligence, pages 542–549, 2005.

[24] Daniel Szer, François Charpillet, and Shlomo Zilberstein. MAA*: A heuristic search algorithm for solving
decentralized POMDPs. In Proceedings of the Twenty-First Conference on Uncertainty in Artiﬁcial Intelligence,
pages 576–583, 2005.

[25] John von Neumann. Zur Theorie der Gesellschaftsspiele. Mathematische Annalen, 100, 1928.
[26] Bernhard von Stengel. Efﬁcient computation of behavior strategies. Games and Economic Behavior, 14(50):

220–246, 1996.

[27] Auke Wiggers. Structure in the value function of two-player zero-sum games of incomplete information.

Master’s thesis, 2015.

[28] Auke Wiggers, Frans Oliehoek, and Diederik Roijers. Structure in the value function of two-player zero-sum

games of incomplete information. Computing Research Repository, abs/1606.06888, 2016.

[29] Auke Wiggers, Frans Oliehoek, and Diederik Roijers. Structure in the value function of two-player zero-sum
games of incomplete information. In Proceedings of the Twenty-Second European Conference on Artiﬁcial
Intelligence, pages 1628–1629, 2016. doi: 10.3233/978-1-61499-672-9-1628.

[30] Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization in games

with incomplete information. In Advances in Neural Information Processing Systems 20, 2007.

11

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

Table 2: Known properties of various functions appearing in this work

στ
T (στ , βτ )
Lin (prop. 1, p. 3)
T m,i(στ , βτ ) Lin (lem. 4, p. 14)
T c,i(στ , βτ )
V ∗
τ (στ )
Q∗
τ (στ , βτ )

LC (thm. 2, p. 5)
LC (from V ∗

-

τ +1 LC)

W i,∗

τ (στ , βi
τ )

LC (from Q∗

τ +1 LC)

ν2
[σc,1

τ ,β2

τ :]

N/A

σm,1
τ
-
-

σc,1
τ
-
-

⊥⊥ (lem. 5, p. 14) ¬LC (prop. 4, p. 15)

P W LCv (thm. 1, p. 4)
-

-

N/A

-
-

-

LC (lem. 6, p. 18)

βi
τ

Lin (prop. 1, p. 3)
Lin (lem. 4, p. 14)
⊥⊥ (lem. 5, p. 14)
N/A
¬Lin (prop. 5, p. 16)
Cc (lem. 1, p. 4)
¬Lin (from Q∗ ¬Lin)
Cc (prop. 6, p. 19)
N/A

β−i
τ
Lin (prop. 1, p. 3)
Lin (lem. 4, p. 14)
¬LC (prop. 3, p. 15)
N/A
¬Lin (prop. 5, p. 16)
Cv (lem. 1, p. 4)
N/A

-

A Synthetic Tables

For convenience, we provide two synthetic tables: Table 2 to sum up various theoretical properties that are stated in
this paper (assuming a ﬁnite temporal horizon), and Table 3 to sum up the notations used in this paper.

More precisely, Table 2 indicates, for various functions f and variables x, properties that f is known to exhibit with
respect to x. We denote by

-

N/A

Lin

LC

Cv

a function with no known (or used) property (see also comment below);

a non-applicable case;

a linear function;

a Lipschitz-continuous function;

(resp. Cc) a convex (resp. concave) function;

P W LCv (resp. P W LCc) a piecewise linear and convex (resp. concave) function;

⊥⊥

¬P

the function being independent of the variable;

the negation of some property P (i.e., P is known not to hold).

Note also that, as στ = σc,1
both σc,1
and σm,1
τ
we just indicate results that cannot be derived from one of the two other columns.

, the linearity or Lipschitz-continuity properties of any function w.r.t. στ extends to
to στ . In these three columns,

. Reciprocally, related negative results extend from σc,1

τ or σm,1

τ σm,1
τ

τ

τ

B Background

B.1 Re-casting POSGs as Occupancy Markov Games

The following result shows that the occupancy state is (i) Markovian, i.e., its value at τ only depends on its previous
value στ −1, the system dynamics P z1,z2
τ −1, and (ii)
sufﬁcient to estimate the expected reward. Note that it holds for general-sum POSGs with any number of agents, and
as many reward functions; similar results have already been established, e.g., for Dec-POMDPs (cf. [8, Theorem 1]).
Proposition 1. (originally stated on page 3) σβ0:τ −1, together with βτ , is a sufﬁcient statistics to compute (i) the
next OS, σβ0:τ , and (ii) the expected reward at τ : E (cid:2)Rτ | β0:τ −1 ⊕ βτ

a1,a2 (s(cid:48)|s), and the last behavioral decision rules β1

(cid:3), where ⊕ denotes a concatenation.

τ −1 and β2

Proof. Let us ﬁrst derive a recursive way of computing σβ0:τ (θτ , aτ , zτ +1):

σβ0:τ (θτ , aτ , zτ +1) def= P r(θτ , aτ , zτ +1 | β0:τ )
=

P r(θτ , aτ , zτ +1, sτ , sτ +1 | β0:τ )

(cid:88)

=

=

sτ ,sτ +1
(cid:88)

sτ ,sτ +1
(cid:88)

sτ ,sτ +1

P r(zτ +1, sτ +1 | θτ , aτ , sτ , β0:τ )P r(aτ | θτ , sτ , β0:τ )P r(sτ | θτ , β0:τ )P r(θτ | β0:τ )

P r(zτ +1, sτ +1 | aτ , sτ )
(cid:124)
(cid:123)(cid:122)
(cid:125)
(sτ +1|sτ )

zτ +1
aτ

=P

P r(aτ | θτ , βτ )
(cid:123)(cid:122)
(cid:125)
(cid:124)
=β(θτ ,aτ )

P r(sτ | θτ , β0:τ )
(cid:123)(cid:122)
(cid:125)
(cid:124)
=b(sτ |θτ )

,
P r(θτ | β0:τ −1)
(cid:125)
(cid:123)(cid:122)
(cid:124)
=σβ0:τ −1 (θτ )

12

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

Table 3: Various notations used in this work

−i def= i’s opponent. Thus: −1 = 2, and −2 = 1.

Histories and occupancy states

t) is a length-τ action-observation history (AOH) for i.

θi
τ
θτ

def= (ai
def= (θ1

1, . . . , ai
τ , zi
τ ) (∈ Θ = ∪H−1
στ (θτ ) def= Occupancy state (OS) στ (∈ Oσ = ∪H−1

τ ) (∈ Θi = ∪H−1

1, zi
τ , θ2

t=0 Θi

t=0 Θt) is a joint AOH at τ .

t , where Oσ
τ
over joint AOHs θτ (typically for some applied β0:τ −1).
τ ) def= Marginal term of στ from player i’s point of view (σm,i
τ ∈ ∆(Θi
τ ) def= Conditional term of στ from i’s point of view (σc,i
: Θi

(θi
τ |θi
b(s|θτ ) def= Belief state, i.e., probability distribution over states given a joint AOH (b(s|θτ ) : S × Θτ (cid:55)→ R).

def= ∆(Θτ )), i.e., probability distribution

τ )).
τ (cid:55)→ ∆(Θ−i
τ )).

t=0 Oσ

τ

σm,i
τ
τ (θ−i
σc,i

oτ

βi
τ

βτ

βi
τ :τ (cid:48)
βτ :τ (cid:48)

Can be computed by an HMM ﬁltering process.

def= Full occupancy state oτ (∈ ∆(S × Θτ )), i.e., P r(s, θτ ) for the current β0:τ −1, and thus veriﬁes
s∈S oτ (s, θτ ). Is used in the implementation to simplify computations (e.g., of rt

στ (θτ ) = (cid:80)
and στ +1 through b).

Decision rules and strategies

def= A (behavioral) decision rule (DR) at time τ for i is a mapping βi

distributions over private actions. We note βi

τ to
τ , ai) the probability to pick ai when facing θi
τ .

τ from private AOHs in Θi

τ (θi

τ (cid:105) (∈ B = ∪H−1

t=0 Bt) is a decision rule proﬁle.

τ (cid:48)) is a behavioral strategy for i from time step τ to τ (cid:48) (included).

def= (cid:104)β1
def= (βi
def= (cid:104)β1

τ , β2
τ , . . . , βi
τ :τ (cid:48), β2

τ :τ (cid:48)(cid:105) is a behavioral strategy proﬁle.

Rewards and value functions

rmax
rmin

def= maxs,a r(s, a)
def= mins,a r(s, a)

Maximum possible reward.
Minimum possible reward.
Value of βτ :H−1 in OS στ .

Vτ (στ , βτ :) def= E[(cid:80)H−1

t=τ γtRt | στ , βτ :],

where Rt is the random var. for the reward at t.

τ (στ ) def= maxβ1
V ∗

τ :

minβ2

τ :

τ (στ , βτ ) def= r(στ , βτ ) + γV ∗
Q∗
τ ) def= optβ−i
τ (στ , βi
W i,∗

τ (στ , βτ ),

Q∗

τ

Vτ (στ , βτ :)
τ +1(T (στ , βτ )) Opt. (joint) action-value fct.

Optimal value function

Opt. (individual) action-value fct.

ν2
[σc,1

τ ,β2

τ :]

where opt = max if i = 1, min otherwise.
def= Vector of values (one component per AOH θ1
solution of a POMDP allows computing V ∗

τ ) for 1’s best response to β2

τ : assuming σc,1

τ . This

τ (see Theorem 1).

Approximations

V τ (στ ) def= Upper bound approximation of V ∗
V τ (στ ) def= Lower bound approximation of V ∗
1
τ ) def= Upper bound approximation of W ∗,1
τ (στ , β1
τ ) def= Lower bound approximation of W ∗,2
τ (στ , β2
ν2
τ

def= Vector (with one component per AOH θ1

τ

τ

W
W 2

τ (στ ); relies on data set bagV τ .
τ (στ ); relies on data set bagV
.

τ

(στ , β1
(στ , β2

1
τ ); relies on data set bagW
τ .
τ ); relies on data set bagW 2
.
τ

τ ) used in V τ and W

1
τ −1 (if τ ≥ 1).

Miscellaneous

wτ
δ2
τ

def= Denotes a triplet (cid:104)σc,1
def= Distribution over triplets wτ +1 ∈ bagW

τ −1, (cid:104)ν2

τ −1, β1

τ , δ2

τ (cid:105)(cid:105) ∈ bagW

1
τ (or a triplet in bagW 2
τ

).

1
τ +1 (inducing a recursively deﬁned strategy from τ to

H − 1). Often denotes the strategy it induces.
x(cid:62) def= The transpose of a (usually column) vector x of Rn.
c[y] def= Denotes ﬁeld c of object/tuple y.

Supp(d) def= Support of distribution d, i.e., set of its non-zero probability elements.

13

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

(where b(s | θτ ) is the belief over states obtained by a usual HMM ﬁltering process)

(cid:88)

=

P zτ +1
aτ

sτ ,sτ +1

(sτ +1|sτ )β(θτ , aτ )b(sτ | θτ )σβ0:τ −1(θτ ).

σβ0:τ can thus be computed from σβ0:τ −1 and βτ without explicitly using β0:τ −1 or earlier occupancy states.
Then, let us compute the expected reward at τ given β0:τ :

E[r(Sτ , A1

τ , A2

τ ) | β0:τ ] =

(cid:88)

r(sτ , aτ )P r(sτ , aτ | β0:τ )

=

=

=

=

sτ ,aτ
(cid:88)

(cid:88)

sτ ,aτ
(cid:88)

θτ
(cid:88)

sτ ,aτ
(cid:88)

θτ
(cid:88)

sτ ,aτ

θτ

(cid:88)

(cid:88)

sτ ,aτ

θτ

r(sτ , aτ )P r(sτ , aτ , θτ | β0:τ )

r(sτ , aτ )P r(sτ , aτ | θτ , β0:τ )P r(θτ | β0:τ )

r(sτ , aτ ) P r(aτ | θτ , β0:τ )
(cid:125)

(cid:124)

(cid:123)(cid:122)
βτ (θτ ,aτ )

P r(sτ | θτ , β0:τ )
(cid:124)
(cid:123)(cid:122)
(cid:125)
b(sτ |θτ )

P r(θτ | β0:τ )
(cid:124)
(cid:125)
(cid:123)(cid:122)
σβ0:τ −1 (θτ )

r(sτ , aτ )βτ (θτ , aτ )b(sτ | θτ )σβ0:τ −1(θτ ).

The expected reward at τ can thus be computed from σβ0:τ −1 and βτ without explicitly using β0:τ −1 or earlier
occupancy states.

B.2 Introducing Local Games

The ﬁrst two lemmas below present properties of T 1
convexity properties of Q∗.
Lemma 4. T 1

m(στ , βτ ) is linear in στ , β1

τ , and β2
τ .

m and T 1

c that will be useful to demonstrate concavity and

Proof.

m(στ , βτ )(θ1
T 1

τ , a1, z1) =

(cid:88)

τ ,a2,z2
θ2
(cid:88)

=

T (στ , βτ )((θ1

τ , a1, z1), (θ2

τ , a2, z2))

τ (θ1
β1

τ , a1)β2

τ (θ2

τ , a2)

(cid:88)

P z1,z2
a1,a2 (s(cid:48)|s)b(s|θ1

τ , θ2

τ )στ (θ1

τ , θ2
τ )

s(cid:48),θ2

τ ,a2,z2
τ , a1)

τ (θ1

= β1

(cid:88)

τ ,a2
θ2

β2
τ (θ2

τ , a2)

(cid:88)

s,s(cid:48),z2

s
P z1,z2
a1,a2 (s(cid:48)|s)b(s|θ1

τ , θ2

τ )στ (θ1

τ , θ2

τ ).

(from Eq. (1))

(10)

Lemma 5. T 1

c (στ , βτ ) is independent of β1

τ and σm,1

τ

.

Proof.

c (στ , βτ )((θ2
T 1

τ , a2, z2)|(θ1

τ , a1, z1)) =

τ , a1, z1), (θ2

τ , a2, z2))

τ , a1, z1), (θ2
a1,a2 (s(cid:48)|s)b(s|θ1

τ , θ2

τ , a2, z2))

τ , a1)β2

(cid:80)

T (στ , βτ )((θ1
τ ,a2,z2 T (στ , βτ )((θ1
θ2
s,s(cid:48) P z1,z2
τ , a2) (cid:80)
τ (θ2
τ (θ2
θ2,a2 β2
s,s(cid:48) P z1,z2

τ , a2) (cid:80)
a1,a2 (s(cid:48)|s)b(s|θ1

β1
τ (θ1

τ (θ1
β1
τ , a1) (cid:80)
τ (θ2
β2
θ2,a2 β2

(cid:80)

(cid:80)

τ (θ2
β2
θ2,a2 β2
(cid:16)

τ , a2) (cid:80)
τ (θ2

τ , a2) (cid:80)

τ , a2) (cid:80)
τ (θ2

τ , a2) (cid:80)

τ )στ (θ1
τ , θ2

τ , θ2
τ )
τ )στ (θ1

τ , θ2
τ )

s,s(cid:48),z2 P z1,z2

s,s(cid:48) P z1,z2

a1,a2 (s(cid:48)|s)b(s|θ1

s,s(cid:48),z2 P z1,z2

s,s(cid:48),z2 P z1,z2
a1,a2 (s(cid:48)|s)b(s|θ1
τ , θ2
τ )στ (θ1
τ , θ2
τ )
τ , θ2
τ )στ (θ1
τ , θ2
a1,a2 (s(cid:48)|s)b(s|θ1
τ )
(cid:123)
(cid:125)(cid:124)
(cid:122)
(θ1
τ )σm,1
τ |θ1
τ (θ2
σc,1
τ )
τ
τ |θ1
τ ) σc,1
τ )σm,1
τ (θ2
τ
(cid:123)(cid:122)
(cid:124)
(cid:17)
σm,1
τ (θ2
τ |θ1
τ )
τ
(cid:17)
τ )σc,1

τ , θ2
τ )
a1,a2 (s(cid:48)|s)b(s|θ1

a1,a2 (s(cid:48)|s)b(s|θ1

τ )σc,1

τ |θ1
τ )

τ (θ2

τ , θ2

τ , θ2

τ , θ2

(θ1
τ )
(cid:125)

(θ1
τ )

σm,1
τ

(θ1
τ )

τ (θ2
β2

τ , a2) (cid:80)

s,s(cid:48) P z1,z2

a1,a2 (s(cid:48)|s)b(s|θ1

(cid:16)(cid:80)

θ2,a2 β2

τ (θ2

τ , a2) (cid:80)

s,s(cid:48),z2 P z1,z2

=

=

=

=

14

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

=

(cid:80)

τ (θ2
β2
θ2,a2 β2

τ , a2) (cid:80)
τ (θ2

τ , a2) (cid:80)

s,s(cid:48) P z1,z2

a1,a2 (s(cid:48)|s)b(s|θ1

τ , θ2

s,s(cid:48),z2 P z1,z2

a1,a2 (s(cid:48)|s)b(s|θ1

τ )σc,1
τ , θ2

τ |θ1
τ (θ2
τ )
τ )σc,1
τ (θ2

.

τ |θ1
τ )

Addendum: The following complementary properties explain why seeking for better approximations is
difﬁcult.
Proposition 3. T i

c (στ , βτ ) may be non continuous (thus non Lipschitz-continuous) w.r.t. β¬i
τ .

Proof. First, let us deﬁne

f : S3(1) → R
(x, y, z) (cid:55)→ αx

αx+βy ,

where (α, β) ∈ (R+,∗)2 and Sk(1) is the k-dimensional probability simplex.
One can show that f is not Lipschitz-Continuous. Indeed, the sequences
(cid:19)(cid:19)(cid:19)

(cid:18)

(un)n =

(vn)n =

f

f

(cid:18)

,

(cid:18) 1
n
(cid:18) 1
n2 ,

1
n2 , 1 −
1
n

, 1 −

(cid:18) 1
n
(cid:18) 1
n

+

+

1
n2
1
n2

and

(cid:19)(cid:19)(cid:19)

n

n

(11)

(12)

(13)

converge towards different values (respectively 1 and 0). f is thus not continuous around (0, 0, 1), and
therefore not Lipschitz continuous.
This property extends to functions of the form f (x, y1, . . . , yI , z1, . . . , zJ ) =

with

αx
αx+(cid:80)I
i=1 βiyi

• I, J ∈ N∗,
• (x, y1, . . . , yI , z1, . . . , zJ ) ∈ S1+I+J (1),
• positive scalars α and βi (i ∈ {1, . . . , I}).

Note: In the following, we make plausible assumptions without providing a detailed example. Let us now
consider Eq. (11) for two tuples (cid:104)θ1

τ , a2, z2(cid:105) such that στ (θ1

τ , a1, z1(cid:105) and (cid:104)θ2

τ ) (cid:54)= 0:

τ , θ2

τ , a2, z2|θ1
c (στ , βτ )(θ2
T 1
τ , a2)
τ (θ2
β2
τ (ˆθ2

ˆθ2,ˆa2 β2

(cid:80)

=

τ , ˆa2)

τ , a1, z1)
(cid:104)(cid:80)

s,s(cid:48) P z1,z2
(cid:104)(cid:80)

a1,a2 (s(cid:48)|s)b(s|θ1

s,s(cid:48),ˆz2 P z1,ˆz2

a1,ˆa2 (s(cid:48)|s)b(s|θ1

(cid:105)
τ , θ2
τ )
τ , ˆθ2
τ )

στ (θ1
(cid:105)

τ , θ2
τ )
τ , ˆθ2
τ )

στ (θ1

and assuming a simple case where σc,1(θ2

=

τ (θ2
β2

τ , a2)

(cid:80)

ˆa2 β2

τ (θ2

τ , ˆa2)

τ |θ1
(cid:104)(cid:80)

(cid:104)(cid:80)

τ ) = 1 (i.e., all other AOHs for 2 being impossible):
s,s(cid:48) P z1,z2
s,s(cid:48),ˆz2 P z1,ˆz2

στ (θ1
(cid:105)
τ , θ2
τ )

a1,a2 (s(cid:48)|s)b(s|θ1

a1,ˆa2 (s(cid:48)|s)b(s|θ1

(cid:105)
τ , θ2
τ )

τ , θ2
τ )

τ , θ2
τ )

στ (θ1

.

(14)

(15)

(16)

Then, in cases where
s,s(cid:48) P z1,z2
• (cid:80)
s,s(cid:48),ˆz2 P z1,ˆz2
• (cid:80)

a1,˜a2 (s(cid:48)|s)b(s|θ1

τ , θ2
a1,˜a2 (s(cid:48)|s)b(s|θ1

τ ) > 0 for action a2, and
τ , θ2

and ˜a2 are incompatible),

τ ) > 0 for some, but not all, other actions ˜a2 (= 0 typically when z1

we recognize the above function f (x, y1, . . . , yI , z1, . . . , zJ ), which is not continuous.
Such situations where T 1

c is not continuous thus may indeed happen.

c (στ , βτ ) may be non continuous (thus non Lipschitz-continuous) w.r.t. σc,1
Proposition 4. T i
τ .
A similar proof as for Proposition 3 applies, the variables corresponding to parameters σc,1
τ (θ2
ﬁxed θ1
τ .

τ |θ1

τ ) under

This leads us to our main result here regarding Q∗.
Lemma 1. (originally stated on page 4) Q∗

τ (στ , βτ ) is concave in β1

τ and convex in β2
τ .

15

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

Proof. Let us rewrite Q∗

τ (στ , βτ ) to look at its properties with respect to β1
τ :

Q∗

τ (στ , βτ ) = r(στ , βτ ) + γV ∗

= r(στ , βτ ) + γ min

β2

τ +1:

τ +1(T (στ , βτ ))
(cid:104)
T 1
m(στ , βτ ) · ν2

(cid:105)

[T 1

c (στ ,βτ ),β2

τ +1:]

linear in β1
τ
(proof of Proposition 1)
(cid:122)
(cid:125)(cid:124)
r(στ , β1

(cid:123)
τ , β2
τ ) +γ min

τ +1:

β2
(cid:124)

=

linear in β1
τ
(Lemma 4)
(cid:125)(cid:124)
(cid:122)
m(στ , β1
T 1

(cid:123)
τ , β2
τ ) ·

(cid:104)

independent of β1
τ
(Lemma 5)
(cid:125)(cid:124)
c (στ ,β2

τ ),β2

(cid:123)
τ +1:]

(cid:122)
ν2
[T 1

(cid:123)(cid:122)
concave in β1
τ
(as a concave combination of linear functions)

(17)

(18)

(19)

(cid:105)

.

(cid:125)

Combining a reward that is linear in β1
τ (στ , βτ ) is concave in β1
functions), Q∗

τ and a term that is concave in β1
τ and (symmetrically) convex in β2
τ .

τ (as a concave combination of linear

Addendum: The following complementary property highlights the difference in nature between usual ﬁnite
normal-form games and the local games encountered in each occupancy state.
t and β2
t , β2
Proposition 5. Local game Q∗
t ).

t ) may not be bi-linear (in β1

t (σt, β1

Proof. Let us consider a sequential version of matching pennies:

• S = {si, sh, st}, the initial, head, and tail states;
• Ai = {ah, at} for any player i, the head and tail actions;
• Z i = {z∅} for any player i, both being blind;
• P z1,z2

a1,a2 (s(cid:48)|s) = 1s(cid:48)=a1, the next state being the head (resp. tail) state if 1’s action is the head (resp.
tail) action;

• r(s, a1, a2) = 1s=si · (1 − 2 · 1s(cid:48)=a2), so that player 1 gets: (i) 0 at t = 0, and (ii) +1 (resp. −1) if

player 2 has not guessed at t her previous action (at t − 1) at any other t;

• γ = 1; h = 2.

Player 1 thus has to pick head or tail ﬁrst (at t = 0) and 2 second (at t = 1), trying to guess 1’s pick.
Let us then parameterize i’s strategy by her probability pi ∈ [0, 1] of picking action ah at i’s only actual
decision point (t = 0 for 1, and t = 1 for 2). Player 2’s best response to some p1 is for example to set

p2 =

(cid:26)0
1

if p1 ≤ 0.5, and
if p1 > 0.5.

The value of the local game at t = 0 can thus be written as the following function of p1:

Q∗

0(b0, β1

0 , β2

0 ) = Q∗

0(b0, p1) = r(b0, p1)
(cid:124) (cid:123)(cid:122) (cid:125)
=0

+ γ

(cid:124)(cid:123)(cid:122)(cid:125)
=1

· V ∗
(cid:124)

1 (T (b0, p1))
(cid:125)
(cid:123)(cid:122)
=2·|p1−0.5|

= 2 · |p1 − 0.5|.

Q∗

0(b0, β1

0 , β2

0 ) is thus not linear in β1

0 , which concludes the proof.

C Properties and Approximation of Optimal Value Functions

C.1 Properties of V ∗

C.1.1 Linearity and Lipschitz-continuity of T (στ , β1

τ , β2
τ )

Lemma 2. (originally stated on page 5) At depth τ , T (στ , βτ ) is linear in β1
is more precisely 1-Lipschitz-continuous (1-LC) in στ (in 1-norm), i.e., for any στ , σ(cid:48)
τ :

τ , β2

τ , and στ , where βτ = (cid:104)β1

τ , β2

τ (cid:105). It

(cid:107)T (σ(cid:48)

τ , βτ ) − T (στ , βτ )(cid:107)1 ≤ 1 · (cid:107)σ(cid:48)

τ − στ (cid:107)1.

Proof. Let σ be an occupancy state at time τ and βτ be a decision rule. Then, as seen in the proof of Proposition 1,
the next occupancy state σ(cid:48) = T (σ, βτ ) satisﬁes, for any s(cid:48) and (θ, a, z):

σ(cid:48)(θ, a, z) def= P r(θ, a, z|σ, β1

τ , β2
τ )

16

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

= β1

τ (θ1, a1)β2

τ (θ2, a2)





(cid:88)


a (s(cid:48)|s)b(s|θ)
P z

 σ(θ).

s(cid:48),s∈S

b(s|θ) depending only on the model (transition function and initial belief), the next occupancy state σ(cid:48) thus evolves
linearly w.r.t. (i) private decision rules β1
The 1-Lipschitz-continuity holds because each component of vector στ is distributed over multiple components of σ(cid:48).
Indeed, let us view two occupancy states as vectors x, y ∈ Rn, and their corresponding next states under βτ as M x
and M y, where M ∈ Rm×n is the corresponding transition matrix (i.e., which turns σ into σ(cid:48) def= T (στ , βτ )). Then,

τ , and (ii) the occupancy state σ.

τ and β2

(cid:107)M x − M y(cid:107)1

m
(cid:88)

n
(cid:88)
|

def=

Mi,j(xi − yi)|

j=1
m
(cid:88)

i=1
n
(cid:88)

|Mi,j(xi − yi)|

(convexity of |·|)

j=1
m
(cid:88)

i=1
n
(cid:88)

j=1
n
(cid:88)

i=1
m
(cid:88)

Mi,j|xi − yi|

(∀i, j, Mi,j ≥ 0)

Mi,j

|xi − yi|

(M is a transition matrix)

≤

=

=

i=1

j=1
(cid:124) (cid:123)(cid:122) (cid:125)
=1
def= (cid:107)x − y(cid:107)1.

C.1.2 Lipschitz-Continuity of V ∗

The next two results demonstrate that, in the ﬁnite horizon setting, V ∗ is Lipschitz-continuous (LC) in occupancy
space, which allows deﬁning LC upper and lower bound approximations.
Lemma 3. (originally stated on page 5) At depth τ , Vτ (στ , βτ :) is linear w.r.t. στ .

Note: This result in fact applies to any reward function of a general-sum POSG with any number of agents
(here N ), e.g., to a Dec-POMDP. The following proof handles the general case (with βτ
τ (cid:105), and
βτ (a|θ) = (cid:81)N

τ (ai, θ1)).

τ , . . . , βN

def= (cid:104)β1

i=1 βi

Proof. This property trivially holds for τ = H − 1 because

VH−1(σH−1, βH−1:) = r(σH−1, βH−1)

(cid:32)

(cid:88)

(cid:88)

=

s,a

θ

(cid:32)

(cid:88)

(cid:88)

=

s,a

θ

(cid:33)

P r(s, a|θ)σH−1(θ)

r(s, a)

b(s|θ)βτ (a|θ)σH−1(θ)

r(s, a)

(cid:33)

(cid:88)

=

s,θ

b(s|θ)σH−1(θ)

(cid:33)

βτ (a|θ)r(s, a)

.

(cid:32)

(cid:88)

a

Now, let us assume that the property holds for τ + 1 ∈ {1 . . H − 1}. Then,
(cid:16) (cid:88)

(cid:88)

(cid:17)

Vτ (στ , βτ :) =

b(s|θ)βτ (a|θ)στ (θ)

r(s, a) + γVτ +1

(cid:0)T (στ , βτ ), βτ +1:

(cid:1)

s,a
(cid:88)

s,θ

=

θ

b(s|θ)στ (θ)

(cid:16) (cid:88)

βτ (a|θ)r(s, a)

(cid:17)

+ γVτ +1

(cid:0)T (στ , βτ ), βτ +1:

(cid:1) .

a

As

• T (στ , βτ ) is linear in στ (Lemma 2) and

• Vτ +1(στ +1, βτ +1:) is linear in στ +1 (induction hypothesis),

their composition, Vτ +1(T (στ , βτ ), βτ +1:), is also linear in στ , and so is Vτ (στ , βτ :).

17

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

Theorem 2. (originally stated on page 5) Let hτ
continuous in στ at any depth τ ∈ {0 . . H − 1}, where λτ = 1

(or hτ

def= 1−γH−τ
1−γ

2 hτ (rmax − rmin).

def= H − τ if γ = 1). Then V ∗

τ (στ ) is λτ -Lipschitz

Proof. At depth τ , the value of any behavioral strategy βτ : is bounded, independently of στ , by

V max
τ

def= hτ rmax, where rmax

V min
τ

def= hτ rmin, where rmin

def= max
s,a
def= min
s,a

r(s, a).

r(s, a), and

Thus, Vβτ : being a linear function deﬁned over a probability simplex (Oσ
[V min
τ

], we can apply Horák’s [10] Lemma 3.5 (p. 33) to establish that it is also λτ -LC, i.e.,

, V max
τ

τ ) (cf. Lemma 3) and bounded by

|Vβτ : (σ) − Vβτ : (σ(cid:48))| ≤ λτ (cid:107)σ − σ(cid:48)(cid:107)1
− V min
τ
2

with λτ =

V max
τ

(∀σ, σ(cid:48)),

.

Considering now optimal solutions, this means that, at depth τ and for any (σ, σ(cid:48)) ∈ Oσ
τ :

τ (σ) − V ∗
V ∗

τ (σ(cid:48)) = max
β1
τ :
≤ max
β1
τ :

min
β2
τ :
min
β2
τ :
= λτ (cid:107)σ − σ(cid:48)(cid:107)1.

Vτ (σ, β1

τ :, β2

(cid:2)Vτ (σ(cid:48), β1

τ :, β2

Vτ (σ(cid:48), β(cid:48)1

τ :) − max
β(cid:48)1
τ :

min
β(cid:48)2
τ :
τ :) + λτ (cid:107)σ − σ(cid:48)(cid:107)1

τ :, β(cid:48)2
τ :)
(cid:3) − max
β(cid:48)1
τ :

min
β(cid:48)2
τ :

Vτ (σ(cid:48), β(cid:48)1

τ :, β(cid:48)2
τ :)

Symmetrically, V ∗

τ (σ) − V ∗

τ (σ(cid:48)) ≥ −λτ (cid:107)σ − σ(cid:48)(cid:107)1, hence the expected result:

|V ∗

τ (σ) − V ∗

τ (σ(cid:48))| ≤ λτ (cid:107)σ − σ(cid:48)(cid:107)1.

As it will be used later, let us also present the following lemma.
Lemma 6. Let us consider τ ∈ {0 . . H − 1}, θ1

τ , and δ2

τ . Then ν2

[σc,1

τ ,δ2
τ ]

(θ1

τ ) is λτ -LC in σc,1

τ (·|θ1

τ ).

Equivalently, we will also write that ν2

is λτ -LC in σc,1

τ

in vector-wise 1-norm, i.e.:

τ ]|1 (cid:126)≤ λτ
where (i) the absolute value of a vector is obtained by taking the absolute value of each component; and (ii) the
vector-wise 1-norm of a matrix is a vector made of the 1-norm of each of its component vectors.

τ (cid:107)1,

[˜σc,1

[σc,1

τ ,δ2
τ ]
#                                               »
τ ] − ν2
|ν2

τ ,δ2

τ ,δ2

[σc,1

#                            »
τ − ˜σc,1
(cid:107)σc,1

Proof. For any θ1
corresponds to a pair (cid:104)s, θ2

τ , σc,1
τ

and δ2

bθ1

t

So,

τ induce a POMDP for Player 1 from τ on, where (i) the state at any t ∈ {τ . . H − 1}

t (cid:105), and (ii) the initial belief is derived from σc,1

(s, θ2

t ) def= P r(s, θ2

t |θ1

t ) = P r(s|θ2
(cid:124)
(cid:123)(cid:122)
bHMM
θ2
t ,θ1
t

t , θ1
t )
(cid:125)
(s)

τ (·|θ1
t ). The belief state at t thus gives:
t |θ1
· P r(θ2
t )
(cid:125)
(cid:123)(cid:122)
(cid:124)
σc,1
t |θ1
(θ2
t )
t

.

• the value function of any behavioral strategy β1

τ : is linear at t in bθ1

t

, thus (in particular) in σc,1

t (·|θ1

t ); and

• the optimal value function is LC at t also in bθ1

t

constant λt as in Theorem 2),7 thus (in particular) in σc,1

t (·|θ1

t ).

(with the same depth-dependent upper-bounding Lipschitz

Using t = τ , the optimal value function is ν2

[σc,1

τ ,δ2
τ ]

(θ1

τ ), which is thus λτ -LC in σc,1

τ (·|θ1

τ ).

C.2 Bounding Approximations of V ∗, W 1,∗ and W 2,∗

C.2.1 V τ and V τ
To ﬁnd a form that could be appropriate for an upper bound approximation of V ∗
single tuple (cid:104)˜στ , ν2

(cid:105), and deﬁne ζτ

˜σc,1
τ . Then,

def= σm,1
τ

[˜σc,1

τ ,β2

τ :]

τ , let us consider an OS στ and a

V ∗(στ ) ≤ V ∗(ζτ ) + λτ (cid:107)στ − ζτ (cid:107)1

(LC, cf. Theorem 2)

7The proof process is similar. The only difference lies in the space at hand, but without any impact on the resulting formulas.

18

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

= V ∗(σm,1
τ
· ν2
≤ σm,1
τ

˜σc,1
τ ) + λτ (cid:107)στ − ζτ (cid:107)1

[˜σc,1

τ ,β2

τ :] + λτ (cid:107)στ − σm,1

τ

˜σc,1
τ (cid:107)1.

(Cvx, cf. Theorem 1)

Notes:

does not appear in the resulting upper bound, thus will not need to be speciﬁed.

is a simple function of r, ˜σc,1

τ , β2

τ :, and the dynamics of the system, as described

• ˜σm,1
τ
• For τ = H − 1, ν2

in Eq. (9) of Wiggers et al. [28].

[˜σc,1

τ ,β2

τ :]

From this, we can deduce the following appropriate forms of upper and (symmetrically) lower bound function
approximations for V ∗
τ :

V τ (στ ) =

V τ (στ ) =

min
τ (cid:105)∈bagV

(cid:104)˜σc,1

τ ,ν2

max

(cid:104)˜σc,2

τ ,ν1

τ (cid:105)∈bagV

(cid:2)σm,1

τ

(cid:2)σm,2

τ

· ν2

τ + λτ (cid:107)στ − σm,1

τ

· ν1

τ − λτ (cid:107)στ − σm,2

τ

˜σc,1
τ (cid:107)1

(cid:3) , and

˜σc,2
τ (cid:107)1

(cid:3) ,

which are respectively concave in σm,1

τ

and convex in σm,2

τ

, and which both exploit the Lipschitz continuity.

C.2.2 W

1
τ and W 2
τ

Note: We discuss all depths from 0 to H − 1, even though we do not need these approximations at τ = H − 1.
Let us ﬁrst see how concavity-convexity properties affect W ∗,1
Lemma 7. Considering that vectors ν2

are null vectors, we have, for all τ ∈ {0 . . H − 1}:

τ

.

[σc,1

H ,β2

H:]

W 1,∗
τ

(στ , β1

τ ) =

τ ,(cid:104)β2
β2

τ +1:,ν2

min
c (στ ,β2
[T 1

τ ),β2

τ +1:

β1
τ ·

(cid:105)

]

(cid:104)
r(στ , ·, β2

τ ) + γT 1

m(στ , ·, β2

τ ) · ν2

[T 1

c (στ ,β2

τ ),β2

τ +1:]

(cid:105)

.

Proof. Considering that vectors ν2

are null vectors, we have, for all τ ∈ {0 . . H − 1}:

W 1,∗
τ

(στ , β1

τ ) = min
β2
τ

[σc,1

H ,β2
τ (στ , β1

H:]
τ , β2

Q∗

τ ) = min
β2
τ

(cid:2)r(στ , βτ ) + γV ∗

τ +1(T (στ , βτ ))(cid:3)

(Line below exploits Theorem 1 (p. 4) and T 1

c ’s independence from β1

τ (Lemma 5).)



= min
β2
τ

r(στ , βτ ) + γ

(cid:104)β2

τ +1:,ν2

[T 1

min
c (στ ,β2

τ ),β2

τ +1:

(cid:104)

(cid:105)

]

m(στ , βτ ) · ν2
T 1

[T 1

c (στ ,β2

τ ),β2

τ +1:]


(cid:105)


=

τ ,(cid:104)β2
β2

τ +1:,ν2

min
c (στ ,β2
[T 1

τ ),β2

τ +1:

r(στ , βτ ) + γT 1

m(στ , βτ ) · ν2

[T 1

c (στ ,β2

τ ),β2

τ +1:]

(cid:105)

(cid:104)

(cid:105)

]

(Line below exploits r and T 1

m’s linearity in β1

=

τ ,(cid:104)β2
β2

τ +1:,ν2

min
c (στ ,β2
[T 1

τ ),β2

τ +1:

(cid:105)

]

τ (Lemma 4).)
(cid:62)

(cid:104)
r(στ , ·, β2

·

β1
τ

τ ) + γT 1

m(στ , ·, β2

τ ) · ν2

[T 1

c (στ ,β2

τ ),β2

τ +1:]

(cid:105)

.

Note that, since V ∗

H = 0, τ = H − 1 is a particular case which can be simply re-written:

W 1,∗
τ

(στ , β1

τ ) = min
β2
τ

(cid:62)

β1
τ

· r(στ , ·, β2

τ ).

Addendum: The following complementary property is not directly used in the present work, but makes for a
more complete table of properties (Table 2).
Proposition 6. W 1,∗

τ ) is concave in β1
τ .

(στ , β1

τ

Proof. Let X and Y be two convex domains, f : X × Y (cid:55)→ R be a concave-convex function, and
g(x) def= miny∈Y f (x, y). Then, for any x1, x2 ∈ X, and any α ∈ [0, 1],
g(αx1 + (1 − α)x2) = miny f (αx1 + (1 − α)x2, y)
(cid:124)
(cid:125)
(cid:123)(cid:122)
≥αf (x1,y)+(1−α)f (x2,y)

(concavity in x)

(20)

19

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

≥ miny [αf (x1, y) + (1 − α)f (x2, y)]
≥ minyαf (x1, y) + miny(1 − α)f (x2, y)
= αg(x1) + (1 − α)g(x2).

(21)
(22)
(23)

(α ≥ 0)

g is thus concave in x.
This result directly applies to the function at hand, proving its concavity in β1
τ .

To ﬁnd a form that could be appropriate for an upper bound approximation of W ∗,1
and a single tuple (cid:104)˜στ , ˜β2

τ

τ , ν2

, let us now consider an OS στ

W 1,∗
τ

(στ , β1

[T 1

c (˜στ , ˜β2

τ ), ˜β2
τ +1:]
(cid:2)r(στ , βτ ) + γV ∗

(cid:105). Then,
τ +1(T (στ , βτ ))(cid:3)

(where V BR,1

τ +1 (T (στ , β1

τ ) = min
β2
τ
≤ r(στ , β1
τ )| ˜β2
τ , ˜β2
= r(στ , β1

τ ) + γV BR,1

τ , ˜β2
τ +1 (T (στ , β1
τ +1:) is the value of 1’s best response to ˜β2
τ , ˜β2

τ ) + γT 1

m(στ , β1

τ )| ˜β2

τ , ˜β2

τ , ˜β2

τ +1:)

τ ) · ν2
(cid:124)
(cid:16)

[T 1

c (σc,1

τ ), ˜β2

τ , ˜β2
(cid:123)(cid:122)

τ +1:]
(cid:125)

(Use ˜β2

τ & ˜β2
τ , ˜β2

τ ))

τ +1: if in T (στ , β1

τ +1: instead of mins)

≤ r(στ , β1

τ , ˜β2

τ ) + γT 1

τ ) ·

c (σc,1

τ , ˜β2

m(στ , β1

τ , ˜β2
τ , ˜β2
#                                                                     »
τ ) − T 1
(cid:107)T 1
τ )(cid:107)1

ν2
c (˜σc,1
[T 1
τ , ˜β2
c (˜σc,1
(cid:16)

m(στ , ·, ˜β2
τ ) + γT 1
#                                                                     »
τ ) − T 1
(cid:107)T 1
τ )(cid:107)1

ν2
[T 1
τ , ˜β2

τ , ˜β2

c (˜σc,1

c (σc,1

c (˜σc,1

τ ) ·

τ ) + γT 1

m(στ , ·, ˜β2

τ ) · ν2

+λτ +1 ·
(cid:104)
r(στ , ·, ˜β2

+λτ +1 ·
(cid:104)
r(στ , ·, ˜β2

= β1
τ

(cid:62)

·

= β1
τ

(cid:62)

·

τ +1:]

τ ), ˜β2
(cid:17)

τ ), ˜β2

τ +1:]

τ , ˜β2
(cid:17)(cid:105)

+γλτ +1 · (cid:107)T (στ , ·, ˜β2

τ ) − T 1

c (˜σc,1
[T 1
m(στ , ·, ˜β2

τ , ˜β2
τ )T 1

τ ), ˜β2
τ +1:]
τ , ˜β2
c (˜σc,1

(cid:105)

τ )(cid:107)1

(Lem. 3 of Wiggers et al. [28])

(Lem. 6: λτ +1-LC

(24)

of ν2

[T 1

c (σc,1

τ , ˜β2

τ ), ˜β2

τ +1:])

(Linearity in β1
τ )

(25)

(Alternative writing)

(26)

From this, we can deduce the following appropriate forms of (i) upper bounding approximation for W 1,∗
(symmetrically) of lower bound approximation for W 2,∗
τ
(cid:62)
β1
τ

(cid:104)
r(στ , ·, β2

m(στ , ·, β2

1
τ (στ , β1

τ ) + γT 1

τ ) · ν2

τ ) =

τ +1

W

:

·

τ

(cid:104)˜σc,1

τ ,β2

τ ,ν2

min
τ +1(cid:105)∈bagW 1

τ

and (ii)

+ γλτ +1 · (cid:107)T (στ , ·, β2

τ ) − T 1

m(στ , ·, β2

τ )T 1

c (˜σc,1

τ , β2

τ )(cid:107)1

(cid:105)

, and

W 2

τ (στ , β2

τ ) =

(cid:104)˜σc,2

τ ,β1

τ ,ν1

max
τ +1(cid:105)∈bagW 2
τ

(cid:62)

β2
τ

·

(cid:104)
r(στ , β1

τ , ·) + γT 2

m(στ , β1

τ , ·) · ν1

τ +1

− γλτ +1 · (cid:107)T (στ , β1

τ , ·) − T 2

m(στ , β1

τ , ·)T 2

c (˜σc,2

τ , β1

τ )(cid:107)1

(cid:105)

,

where ν2
τ +1 and ν1
strategies (resp. of 2 and 1).

τ +1 respectively upper and lower bound the actual vectors associated to the players’ future

Again, τ = H − 1 is a particular case where only the reward term is preserved.

C.3 Related Operators

C.3.1 Selection Operator: Solving for β1

τ as an LP

Proposition 7. Using now a distribution δ2
upper-bounding value for “proﬁle” (cid:104)β1

τ , δ2

τ over tuples w = (cid:104)˜σc,1

τ , β2

τ , ν2

τ +1(cid:105) ∈ bagW

τ (cid:105) when in στ can be written as an expectancy:

1
τ , the corresponding

where M στ is an |Θ1

τ × A1| × |bagW

1
τ | matrix.

(cid:62)

β1
τ

· M στ · δ2
τ ,

Proof. From the right-hand side term in (25), the upper-bounding value associated to στ , β1
(cid:104)˜σc,1

τ , β2

τ , ν2

τ and a tuple

τ +1(cid:105) ∈ bagW
(cid:104)

(cid:62)

β1
τ

·

r(στ , ·, β2

1
τ can be written:
τ ) + γT 1

m(στ , ·, β2

τ ) ·

#                                                                     »
τ ) − T 1
(cid:107)T 1
τ )(cid:107)1

τ , ˜β2

τ , ˜β2

c (˜σc,1

c (σc,1

(cid:17)(cid:105)

.

(cid:16)

ν2
τ +1+λτ +1 ·

20

def= r(στ , ·, β2
(cid:88)

=

s,θ2

τ ,a2

+ γ

(cid:88)

z1

+ λτ +1 ·

(cid:88)

=

στ (θτ )

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

Using now a distribution δ2
for “proﬁle” (cid:104)β1

τ , δ2

τ (cid:105) when in στ can be written as an expectancy:

τ over tuples w = (cid:104)˜σc,1

τ , β2

τ , ν2

τ +1(cid:105) ∈ bagW

1
τ , the corresponding upper-bounding value

(cid:104)
r(στ , ·, β2

τ [w]) + γT 1

m(στ , ·, β2

τ [w]) ·

(cid:16)

ν2
τ +1[w] + λτ +1 ·

#                                                                                             »
(cid:107)T 1
τ [w])(cid:107)1

τ [w]) − T 1

τ [w], β2

c (˜σc,1

c (σc,1

τ , β2

(cid:17)(cid:105)

· δ2

τ (w)

(cid:88)

(cid:62)

β1
τ

·

w∈W 1
τ

(where x[w] denotes the ﬁeld x of tuple w)

(cid:62)

= β1
τ

· M στ · δ2
τ ,

where M στ is an |Θ1

τ × A1| × |bagW

1
τ | matrix.

For implementation purposes, using Eqs. (2) and (10) (to develop respectively r(·, ·, ·) and T 1
the expression of a component, i.e., the upper-bounding value if a1 is applied in θ1

τ while w is chosen:

m(·, ·, ·)), we can derive

M στ
((cid:104)θ1

τ ,a1(cid:105),w)

τ [w]) + γT 1

m(στ , ·, β2

τ [w]) ·

(cid:16)

ν2
τ +1[w] + λτ +1 ·

#                                                                                             »
(cid:107)T 1
τ [w])(cid:107)1

τ [w]) − T 1

τ [w], β2

c (˜σc,1

c (σc,1

τ , β2

(cid:17)

στ (θτ )b(s|θτ )β2

τ [w](a2|θ2)r(s, a)





(cid:88)

τ [w](a2|θ2
β2
τ )

(cid:88)

a (s(cid:48)|s)b(s|θτ )στ (θτ )
P z

 ·



(cid:16)

τ +1[w](θ1
ν2

τ , a1, z1)

s,s(cid:48),z2

τ ,a2
θ2
#                                                                                             »
c (σc,1
(cid:107)T 1
(cid:88)

τ [w]) − T 1

τ [w], β2

c (˜σc,1

τ , β2

τ [w])(cid:107)1(θ1

τ [w](a2|θ2
β2
τ )

(cid:17)
τ , a1, z1)

θ2
τ

a2

(cid:32)

(cid:88)

·

s

b(s|θτ )r(s, a) + γ

(cid:88)





(cid:88)


a (s(cid:48)|s)b(s|θτ )
P z

 ·

(cid:16)

z1

s,s(cid:48),z2

τ +1[w](θ1
ν2

τ , a1, z1)

+ λτ +1 ·

#                                                                                             »
(cid:107)T 1

τ [w]) − T 1

τ [w], β2

c (˜σc,1

c (σc,1

τ , β2

τ [w])(cid:107)1(θ1

(cid:17)
τ , a1, z1)

(cid:33)
.

Then, solving maxβ1

τ

W

1
τ (στ , β1

τ ) can be rewritten as solving a zero-sum game where pure strategies are:

• for Player 1, the choice of not 1, but |Θ1

τ | actions (among |A1|) and,

• for Player 2, the choice of 1 element of bagW

1
τ .

One can view it as a Bayesian game with one type per history θ1

τ for 1, and a single type for 2.

With our upper bound approximation, maxβ1

τ

W

1
τ (στ , β1

τ ) can thus be solved as the linear program:

v

max
β1
τ ,v

s.t. (i) ∀w ∈ bagW

1
τ ,

(ii) ∀θ1

τ ∈ Θ1
τ ,

(cid:62) · M στ

(·,w)

v ≤ β1
τ
(cid:88)

τ (a1|θ1
β1

τ ) = 1,

whose dual LP is given by

v

min
δ2
τ ,v

a1

s.t. (i) ∀(θ1

τ , a1) ∈ Θ1

τ × A1,

(ii)

v ≥ M στ
((θ1
(cid:88)

τ

τ ,a1),·) · δ2
δ2
τ (w) = 1.

As can be noted, M στ ’s columns corresponding to 0-probability histories θ1
are empty, so that the
corresponding decision rules (for these histories) are not relevant and can be set arbitrarily. The actual implementation
thus ignores these histories, whose corresponding decision rules also do not need to be stored.

τ in σm,1

τ

w∈bagW 1
τ

21

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

(cid:104)σc,1
τ −1, β2
τ , ν2
(cid:104)δ2

τ −1,
τ (cid:105) (cid:105)1
1

(cid:104)σc,1
τ −1, β2
τ , ν2
(cid:104)δ2

τ −1,
τ (cid:105) (cid:105)2
1

(cid:104)σc,1
τ −1, β2
τ , ν2
(cid:104)δ2

τ −1,
τ (cid:105) (cid:105)1
2

(cid:104)σc,1
τ −1, β2
τ , ν2
(cid:104)δ2

τ −1,
τ (cid:105) (cid:105)2
2

...

(cid:104)σc,1
τ −1, β2
τ , ν2
(cid:104)δ2

τ −1,
τ (cid:105) (cid:105)1

H−2

(cid:104)σc,1
τ −1, β2
τ , ν2
(cid:104)δ2

τ −1,
τ (cid:105) (cid:105)2

H−2

(cid:104)σc,1

τ −1, β2
−

τ −1,
(cid:105)1
H−1

(cid:104)σc,1

τ −1, β2
−

τ −1,
(cid:105)2
H−1

(cid:104)σc,1

τ , δ2

τ , ν2

τ (cid:105)0

· · ·

· · ·

...

· · ·

· · ·

(cid:104)σc,1
τ −1, β2
τ , ν2
(cid:104)δ2

τ −1,
τ (cid:105) (cid:105)n1−1
1

(cid:104)σc,1
τ −1, β2
τ , ν2
(cid:104)δ2

τ −1,
τ (cid:105) (cid:105)n1
1

(cid:104)σc,1
τ −1, β2
τ , ν2
(cid:104)δ2

τ −1,
τ (cid:105) (cid:105)n2−1
2

(cid:104)σc,1
τ −1, β2
τ , ν2
(cid:104)δ2

τ −1,
τ (cid:105) (cid:105)n2
2

...

(cid:104)σc,1
τ −1, β2
τ , ν2
(cid:104)δ2

τ −1,
τ (cid:105) (cid:105)nH−2−1

H−1

(cid:104)σc,1
τ −1, β2
τ , ν2
(cid:104)δ2

τ −1,
τ (cid:105) (cid:105)nH−2

H−2

(cid:104)σc,1

τ −1, β2
−

τ −1,

(cid:105)nH−1−1
H−1

(cid:104)σc,1

τ −1, β2
−

τ −1,

(cid:105)nH−1
H−1

τ = 0

τ = 1

τ = 2

...

τ = H − 2

τ = H − 1

Figure 1: DAG structure of the recursively deﬁned strategy induced by δ2
distribution over elements wτ +1, thus inducing 2 other probability distributions: (i) one over decision rules β2
(ii) the other over probability distributions δ2

τ (part of a tuple wτ ) is a probability
τ , and

0. Each δ2

τ +1.

C.3.2 Strategy Induced by δ2
τ

δ2
τ , as a distribution over tuples in bagW
DRs at τ , and (right) a mixture of other mixture strategies for τ + 1 on:

1
τ , induces a recursively-deﬁned strategy for 2 as (left) a mixture of behavioral

τ [δ2
β2

τ ] def=

(cid:88)

τ ( ˜β2
δ2

τ ) · ˜β2
τ ,

and

τ +1[δ2
δ2

τ ] def=

(cid:88)

τ ∈bagW 1
˜β2

τ

τ +1∈bagW 1
˜δ2

τ

τ (˜δ2
δ2

τ +1) · ˜δ2

τ +1,

until reaching the horizon. δ2

τ needs to be stored as this strategy will play a key role in the following.

For τ ≥ 1, both V τ and W
will discuss them together. bagV τ contains tuples (cid:104)σc,1
(cid:104)σc,1

1
τ −1 rely essentially on the same information and are strongly related, so that we
1
τ −1 (for τ ≥ 1) related tuples

τ (cid:105)(cid:105), and bagW

τ , (cid:104)δ2

τ , ν2

τ −1, β2

τ −1, (cid:104)δ2

τ , ν2

τ (cid:105)(cid:105).

Figure 1 represents (in rectangular nodes) the elements of bagW
reachable from a given element of bagV 0
(the ellipsoid root node). The children of any internal node at level/depth τ (including the root) are the nodes
corresponding to the elements wτ +1 in the support of δ2
τ ) of elements with non-zero probability
in distribution δ2
As can be observed, it is directed and acyclic. On can thus extract a behavioral strategy from some δ2
τ through a
recursive process or, better, dynamic programming (to avoid repeating the same computations when the same internal
node is reached through various branches).

τ (i.e., the set Supp(δ2
τ ). Level τ = H − 1 corresponds to the leaves of this graph.

1

C.3.3 Upper Bounding ν2

[σc,1

τ ,δ2
τ ]

Adding a new complete tuple to bagW
to the strategy induced by δ2
Proposition 8. For each δ2
is upper bounded by a value ν2

1
τ requires a new vector ν2

τ that upper bounds the vector ν2

[σc,1

τ ,δ2
τ ]

associated

τ . We can obtain one in a recursive manner (not solving the induced POMDP).

τ obtained as the solution of the aforementioned (dual) LP in στ , and each θ1

τ (θ1

τ ) that depends on vectors ν2

τ +1 in the support of δ2

τ , ν2
τ ,δ2
τ ]
τ . In particular, if θ1

[σc,1

(θ1
τ )
τ ∈

22

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

Supp(σm,1

τ

), we have:

τ (θ1
ν2

τ ) def=

1
τ,m(θ1
σ1
τ )

max
a1∈A1

M στ
((θ1

τ ,a1),.) · δ2
τ .

Proof. For a newly derived δ2
σc,1
τ

and (ii) 2 plays δ2

τ , we have:

τ , as ν2

[σc,1

τ ,δ2
τ ]

(θ1

τ ) is the value of 1’s best action (∈ A1) if 1 (i) observes θ1

τ while in

ν2
[σc,1

τ ,δ2

τ ](θ1

τ ) def= V (cid:63)

τ ](θ1
τ )

τ ,δ2

[σc,1
(cid:34) H
(cid:88)

(optimal POMDP value function)

(cid:35)

γt−τ Rt | β1

τ :, θ1

τ , σc,1

τ , δ2
τ

= max
β1
τ :

E

E

E

(cid:104)

= max

a1

= max

a1

= max

a1

t=τ

(cid:34)

Rτ + γ max

β1

τ +1:

(cid:34) H
(cid:88)

E

t=τ +1

γt−(τ +1)Rt | β1

τ +1:, (cid:104)θ1

τ , a1, Z 1(cid:105), σc,1

τ +1, δ2

τ +1

(cid:35)

(cid:35)

| a1, θ1

τ , σc,1

τ , δ2
τ

τ , a1, Z 1) | a1, θ1

τ , σc,1

τ , δ2
τ

(cid:105)

Rτ + γV (cid:63)
(cid:88)

[σc,1

τ +1,δ2
P r(w, θ2
(cid:124)

w,θ2

τ ,a2,z1

(cid:16)
r(θτ , aτ ) + γν2

·

[σc,1

τ +1,δ2

= max

a1

(cid:16)

·

= max

a1

(cid:88)

P r(w|δ2
τ )
(cid:125)
(cid:123)(cid:122)
(cid:124)

w,θ2

τ ,a2,z1
r(θτ , a) + γν2
(cid:88)
(cid:88)

δ2
τ (w)

[σc,1

τ +1,δ2
τ (θ2
σc,1

τ , σc,1

τ , δ2
τ )
(cid:125)

(cid:17)
τ , a1, z1)

τ +1](θ1
τ , z1, a2 | a1, θ1
(cid:123)(cid:122)
τ +1[w]](θ1
τ , σc,1
τ |θ1
· P r(θ2
τ )
(cid:124)
(cid:125)
(cid:123)(cid:122)
τ +1[w]](θ1
(cid:88)
τ |θ1
τ )

τ [w](a2|θ2
β2
τ )

· P r(a2|β2
τ [w], θ2
τ )
(cid:124)
(cid:125)
(cid:123)(cid:122)
(cid:17)
τ , a1, z1)

w

θ2
τ

a2

(cid:32)

·

r(θτ , a) + γ

(cid:88)

z1

P r(z1|θτ , a) ν2
(cid:124)

[σc,1

τ +1,δ2

τ +1[w]](θ1
(cid:123)(cid:122)

τ , a1, z1)
(cid:125)

· P r(z1|θτ , aτ )
(cid:125)
(cid:123)(cid:122)
(cid:124)

(cid:33)

(where σc,1

τ +1 = T 1

c (σc,1

τ , β2

τ [w]) (lem. 5, p. 14))

then, as ν2

[σc,1

τ +1,δ2

τ +1[w]]

is λτ +1-LC in (any) σc,1

τ +1 (Lemma 6),

≤ max

a1

(cid:88)

w

δ2
τ (w)

(cid:88)

θ2
τ

τ (θ2
σc,1

τ |θ1
τ )

(cid:88)

a2

(cid:32)

τ [w](a2|θ2
β2

τ ) ·

r(θτ , a)

+ γ

(cid:88)

z1

P r(z1|θτ , a)

(cid:34)(cid:122)
ν2
[˜σc,1
(cid:124)

τ +1[w],δ2

≤ max

a1

(cid:88)

w

δ2
τ (w)

(cid:88)

θ2
τ

τ (θ2
σc,1

τ |θ1
τ )

a2

τ , a1, z1)
(cid:125)

τ +1[w]](θ1
(cid:123)(cid:122)
(cid:88)

τ [w](a2|θ2
β2

τ ) ·

r(θτ , a)
(cid:124) (cid:123)(cid:122) (cid:125)

#                                         »
(cid:107)σc,1
τ +1 − ˜σc,1

τ +1[w](cid:107)1(θ1

(cid:35) (cid:33)
(cid:123)
τ , a1, z1)

(cid:125)(cid:124)
+λτ +1

(cid:32)

+ γ

(cid:88)

z1

P r(z1|θτ , a)
(cid:124)
(cid:125)
(cid:123)(cid:122)

(cid:20)(cid:122)
(cid:125)(cid:124)
τ +1[w](θ1
ν2

(cid:123)
τ , a1, z1) +λτ +1

#                                         »
τ +1 − ˜σc,1
(cid:107)σc,1

τ +1[w](cid:107)1(θ1

τ , a1, z1)

(cid:21) (cid:33)

= max

a1

(cid:88)

w

δ2
τ (w)

(cid:88)

θ2
τ

τ (θ2
σc,1

τ |θ1
τ )

(cid:88)

a2

τ [w](a2|θ2
β2

τ ) ·

(cid:32) (cid:122)

(cid:88)

s

(cid:125)(cid:124)

(cid:123)
b(s|θτ )r(s, a)

+ γ

(cid:32)(cid:122)

(cid:88)

(cid:88)

z1

s

(cid:125)(cid:124)

(cid:123)
b(s|θτ ) P r(z1|s, a)
(cid:124)
(cid:125)

(cid:123)(cid:122)

(cid:33)

(cid:104)

·

τ +1[w](θ1
ν2

τ , a1, z1)

+ λτ +1

#                                         »
(cid:107)σc,1
τ +1 − ˜σc,1
(cid:124)
(cid:88)

τ +1[w](cid:107)1(θ1
(cid:123)(cid:122)
τ (θ2
σc,1

δ2
τ (w)

τ |θ1
τ )

(cid:88)

τ , a1, z1)
(cid:125)
τ [w](a2|θ2
β2
τ )

(cid:88)

(cid:33)

(cid:105)

= max

a1

w

θ2
τ

a2

23

(27)

(28)

(29)

(30)

(31)

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

(42)

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

(cid:32)

·

(cid:88)

s

+ λτ +1

b(s|θτ )r(s, a) + γ

(cid:88)


(cid:122)
(cid:88)


(cid:125)(cid:124)
b(s|θτ )P z


(cid:123)
a (s(cid:48)|s)
 ·

(cid:104)

z1

s,s(cid:48),z2

τ +1[w](θ1
ν2

τ , a1, z1)

#                                                                                             »
(cid:122)
(cid:107)T 1

(cid:125)(cid:124)
τ [w], β2
c (˜σc,1

τ [w]) − T 1

c (σc,1

τ , β2

τ [w])(cid:107)1(θ1

(cid:123)
τ , a1, z1)

(cid:33)

(cid:105)

=

1
τ,m(θ1
σ1
τ )

max
a1∈A1

M στ
((θ1

τ ,a1),.) · δ2
τ .

C.3.4 Pruning V τ

The following key theorem allows reusing usual POMDP max-planes pruning techniques in our setting (reverting
them to handle min-planes upper bound approximations).
Theorem 4. (originally stated on page 24) Let P be a min-planes pruning operator (inverse of max-planes pruning
for POMDPs), and (cid:104)σc,1
τ as non-dominated (or resp. dominated) under
τ , ν2
τ , then (cid:104)σc,1
ﬁxed σc,1

τ , ν2
τ (cid:105) ∈ bagV τ . If P correctly identiﬁes ν2
τ (cid:105) is non-dominated (or resp. dominated) in Oσ
τ .

Proof. We will demonstrate that:

• if P shows that a vector ν2

τ (associated to στ ) is dominated under ﬁxed σc,1

τ by a min-planes upper bound

relying only on other vectors ˜ν2

τ is useful at least around ξτ = (ξm,1

τ , then this vector is dominated in the whole space Oσ;
, σc,1

τ ), where ξm,1

τ

τ

is the domination point returned

• else, the vector ν2

by P .

Note: The following is simply showing that, if the linear part is dominated by a min-planes approximation for a
given conditional term σc,1
τ , then the Lipschitz generalization in the space of conditional terms is also dominated
since λ is constant.

#       »
(cid:107)M (cid:107)1 denote the column vector whose ith component is (cid:107)mi,·(cid:107)1. Here, such
τ is

τ (cid:107)1 denoting the vector whose component for AOH θ1

#                            »
τ − ˜σc,1
(cid:107)σc,1

τ ) may also be denoted σc,1

τ (θ1

τ ) is dominated under σc,1

τ ) for brevity).
τ , i.e., ∀ξm,1

τ

,

Given a matrix M = (mi,j), let
matrices will correspond to conditional terms,
(cid:107)σc,1
Let us assume that the vector ν2

τ )(cid:107)1 (where σc,1

τ ) − ˜σc,1

τ (·|θ1

τ (·|θ1

τ (·|θ1
τ (associated to σc,1
0
#                            »
(cid:125)(cid:124)
(cid:123)
(cid:122)
τ − σc,1
(cid:107)σc,1

(ξm,1
τ

)(cid:62) · (ν2

τ + λτ

τ (cid:107)1) ≥ min
τ ,˜σc,1
˜ν2

τ

We will show that, ∀ξτ = (ξm,1

τ

, ξc,1

(ξm,1
τ

)(cid:62) · (ν2

τ + λτ

τ ),
#                           »
τ − σc,1
(cid:107)ξc,1

τ (cid:107)1) ≥ min
τ ,˜σc,1
˜ν2

τ

(cid:104)
(ξm,1
τ

)(cid:62) · (˜ν2

τ + λτ

#                            »
(cid:107)σc,1
τ − ˜σc,1

(cid:105)
τ (cid:107)1)

.

(cid:104)
(ξm,1
τ

)(cid:62) · (˜ν2

τ + λτ

#                           »
τ − ˜σc,1
(cid:107)ξc,1

τ (cid:107)1)

(cid:105)

.

Let ξτ be an occupancy state. First, remark that ∃(cid:104)˜ν2

τ , ˜σc,1

τ (cid:105) such that

(ξm,1
τ

)(cid:62) · (ν2

τ + λτ

0
#                            »
(cid:125)(cid:124)
(cid:123)
(cid:122)
τ − σc,1
(cid:107)σc,1

τ (cid:107)1) ≥ (ξm,1

τ

)(cid:62) · (˜ν2

τ + λτ

#                            »
τ − ˜σc,1
(cid:107)σc,1

τ (cid:107)1).

For the sake of clarity, let us introduce the following functions (where x, y, and z will denote conditional terms for
player 1):

g(x) def=

(cid:88)

ξm,1
τ

θ1
τ

(θ1

τ ) · (ν2

y (θ1

τ ) + λτ (cid:107)y(θ1

τ ) − x(θ1

τ )(cid:107)1)

= g(y) + λτ (ξm,1

τ

)(cid:62) ·

#              »
(cid:107)y − x(cid:107)1,

(43)

and

h(x) def=

(cid:88)

ξm,1
τ

θ1
τ

(θ1

τ ) · (ν2

z (θ1

τ ) + λτ (cid:107)z(θ1

τ ) − x(θ1

τ )(cid:107)1)

= h(z) + λτ (ξm,1

τ

)(cid:62) ·

#              »
(cid:107)z − x(cid:107)1.

24

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

Let us assume that g(y) ≥ h(y), and show that g ≥ h. First,
#              »
(cid:107)x − y(cid:107)1
#              »
(cid:107)x − y(cid:107)1
#              »
(cid:107)y − z(cid:107)1 +
(cid:16) #              »
(cid:107)y − z(cid:107)1 + |

g(x) = g(y) + λτ (ξm,1
≥ h(y) + λτ (ξm,1
= h(z) + λτ (ξm,1
≥ h(z) + λτ (ξm,1

)(cid:62) ·
)(cid:62) ·
)(cid:62) · (

)(cid:62) ·

τ

τ

τ

τ

#              »
(cid:107)x − y(cid:107)1)
#              »
(cid:107)x − z(cid:107)1 −

#              »
(cid:107)z − y(cid:107)1|

(cid:17)

.

Now, ∀θ1

τ , if (cid:107)x(θ1

τ ) − z(θ1

τ )(cid:107)1 ≥ 0, then

τ )(cid:107)1 − (cid:107)z(θ1

τ ) − y(θ1
τ )(cid:107)1 + (cid:12)
(cid:12)(cid:107)x(θ1
(cid:107)y(θ1
τ ) − z(θ1
= (cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)
τ )(cid:107)1 + (cid:107)x(θ1
τ ) − z(θ1
(cid:107)y(θ1
τ ) − z(θ1
= (cid:107)x(θ1
τ )(cid:107)1,

τ ) − z(θ1

τ ) − z(θ1

(cid:12)
τ )(cid:107)1 − (cid:107)z(θ1
τ ) − y(θ1
(cid:12)
τ )(cid:107)1 − (cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)
(cid:107)z(θ1
τ )(cid:107)1

τ )(cid:107)1
τ ) − y(θ1

else,

τ ) − z(θ1

τ )(cid:107)1 + (cid:12)

(cid:107)y(θ1
= 2(cid:107)y(θ1
≥ (cid:107)x(θ1

(cid:12)(cid:107)x(θ1
τ )(cid:107)1 − (cid:107)x(θ1
τ )(cid:107)1.

τ ) − z(θ1
τ ) − z(θ1

τ ) − z(θ1

τ )(cid:107)1 − (cid:107)z(θ1
τ )(cid:107)1

τ ) − z(θ1

τ ) − y(θ1

τ )(cid:107)1

(cid:12)
(cid:12)

Finally, coming back to (44):

g(x) ≥ h(z) + λτ (ξm,1

τ

)(cid:62) ·

(cid:16) #              »
(cid:107)y − z(cid:107)1 + |

#              »
(cid:107)x − z(cid:107)1 −

#              »
(cid:107)z − y(cid:107)1|

(cid:17)

≥ h(z) + λτ (ξm,1
≥ h(x).

τ

)(cid:62) · (cid:107)x(θ1

τ ) − z(θ1

τ )(cid:107)1

(from (45+47))

With x = ξc,1

τ , y = σc,1

and z = ˜σc,1

τ
g(ξc,1

τ ) =

τ , this gives:
τ )(ν2
(θ1
ξm,1
τ

(cid:88)

τ (θ1

τ ) + λτ (cid:107)σc,1

τ (θ1

τ ) − ξc,1

τ (θ1

τ )(cid:107)1)

≥ h(ξc,1

τ ) =

θ1
τ
(cid:88)

θ1
τ

ξm,1
τ

(θ1

τ )(˜ν2

τ (θ1

τ ) + λτ (cid:107)˜σc,1

τ (θ1

τ ) − ξc,1

τ (θ1

τ )(cid:107)1).

(44)

(45)

(46)

(47)

(48)

(49)

This shows that ν2
prune a vector ν2

τ is dominated for every (ξm,1
τ using P applied in the space where σc,1

τ ), where both ξm,1
is ﬁxed.

, ξc,1

τ

τ

τ

and ξc,1

τ

are arbitrary. Therefore, one can

As a consequence, some properties of P are preserved in its extension to zsPOSGs:

• If P correctly identiﬁes ν2

τ as non-dominated at σc,1

τ , then (cid:104)ν2

τ , σc,1

τ (cid:105) is non-dominated in Oσ
τ .

That is, if P does not induce false negatives, neither does its extension to zsPOSGs.

• If P correctly identiﬁes ν2

τ (cid:105) is dominated in Oσ
τ .
That is, if P does not induce false positives, neither does its extension to zsPOSGs.

τ as dominated at σc,1

τ , then (cid:104)ν2

τ , σc,1

C.3.5 About Improbable Histories

τ ∈ Supp(σm,1
When solving the LP for β1
τ in some OS στ , the resulting DR is optimized for the AOHs θ1
) only
(and otherwise random). Also, as mentioned in Proposition 8, the corresponding vector ν2
τ can be obtained as a
by-product of the LP if restricted to the same AOHs, and has not very relevant values for other AOHs because of the
non-optimized decisions.
As a consequence, and in a view to save on time and memory, we prefer not computing and storing DRs β1
vectors ν2
required when computing new LPs, but can then be replaced by

. The missing values for some AOHs ˜θ1

τ outside the support of the σm,1

τ outside Supp(σm,1

τ and
) may be

τ

τ

τ

• any probability distribution over actions for β1

τ (·|˜θ1

τ ) (because solving the LP would have led to a random

choice anyway), and

• a generic upper bound such as V max

τ

(cf. Proof of Theorem 2).

is a gross (but conservative) overestimation, thus far from informative, which impedes the convergence of the

V max
τ
algorithm. We now present the two approaches we considered as a replacement.

25

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

1
Initialization-based upper bound [νinit]
τ by
solving the POMDP relaxation of the zs-POSG obtained by making the opponent always select actions uniformly
at random. This is in fact not a valid upper bound for a given δ2
τ , because this opponent’s strategy used for the
initialization may, at least considering some AOHs θ1
τ , be better than the current strategy deﬁned by δ2
τ . Yet, this
upper bound turns out to give satisfying results in most of our experiments. We denote this heuristic νinit.

This ﬁrst approach uses the vectors computed when initializing W

bMDP upper bound [νbMDP]
ing

This second approach is a “belief MDP” heuristic approximation based on comput-

1. the optimal value function V ∗

MDP for the (ﬁnite-horizon) MDP relaxation of the POMDP obtained for δ2
τ ,

then, for a given AOH θ1

τ , a1, z1(cid:105), β2
, the probability distribution over states given θ1

τ and σc,1
τ ,

τ +1

def= (cid:104)θ1

2. b|θ1
3. the weighted sum (cid:80)

τ +1

s b|θ1

τ +1

(s) · V ∗

MDP(s).

τ +1, β2

τ and σc,1

τ , and

The above-mentioned belief is obtained with:

P r(sτ +1 | σc,1

τ , (cid:104)θ1
=

τ , a1, z1(cid:105), β2
τ )
(cid:88)
P r(sτ +1, sτ , (cid:104)θ2

(cid:88)

τ , a2, z2(cid:105) | σc,1

τ , (cid:104)θ1

τ , a1, z1(cid:105), β2
τ )

τ ,a2,z2
θ2

sτ

(by Law of total probability)

(cid:88)

(cid:88)

=

τ ,a2,z2
θ2

sτ

(by Bayes’ Theorem), where:

(cid:122)
P r(sτ +1, sτ , (cid:104)θ2

X
(cid:125)(cid:124)
τ , a2, z2(cid:105), z1 | σc,1

τ , (cid:104)θ1
τ , a2, z2(cid:105), z1 | σc,1

(cid:123)
τ , a1(cid:105), β2
τ )
τ , a1(cid:105), β2
τ , (cid:104)θ1
τ )

(cid:80)

sτ +1,sτ ,(cid:104)θ2

τ ,a2,z2(cid:105) P r(sτ +1, sτ , (cid:104)θ2

X = P r(sτ +1, sτ , (cid:104)θ2

τ , a2, z2(cid:105), z1 | σc,1

τ , (cid:104)θ1

= P r(sτ +1, z1, z2 | sτ , a1, a2) · P r(sτ , (cid:104)θ2
= P r(sτ +1, z1, z2 | sτ , a1, a2) · P r(a2 | θ2
= P r(sτ +1, z1, z2 | sτ , a1, a2) · P r(a2 | θ2
· P r(a2 | θ2
= P r(sτ +1, z1, z2 | sτ , a1, a2)
(cid:124)
(cid:125)
(cid:123)(cid:122)
P z1,z2
a1 ,a2 (sτ +1|sτ )
P z1,z2
a1,a2 (sτ +1|sτ )
(cid:125)
(cid:123)(cid:122)
(cid:124)
O(z1,z2|sτ +1,a1,a2)·T (sτ +1|sτ ,a1,a2)

·β2

=

β2

(cid:124)

τ , a1(cid:105), β2
τ )
τ , (cid:104)θ1
τ , a2(cid:105) | σc,1
τ ) · P r(sτ , (cid:104)θ2
τ , β2
τ ) · P r(sτ | θ1
τ , β2
τ , β2
· P r(sτ | θ1
τ )
(cid:123)(cid:122)
(cid:125)
(cid:124)
τ (a2|θ2
τ )

(cid:123)(cid:122)
P r(sτ |θ1

τ , a1(cid:105), β2
τ )
τ , (cid:104)θ1
τ (cid:105) | σc,1
τ ) · P r(θ2
τ , θ2
· P r(θ2
τ , θ2
τ )
(cid:125)
(cid:124)
τ ,θ2
τ )

τ , a1(cid:105))
τ | σc,1
τ | σc,1
(cid:123)(cid:122)
σc,1
τ (θ2

τ |θ1
τ )

τ , (cid:104)θ1
τ , θ1
τ )
(cid:125)

τ , a1(cid:105))

τ (a2 | θ2

τ ) · P r(sτ | θ1

τ , θ2

τ ) · σc,1

τ (θ2

τ | θ1

τ ).

We denote this heuristic νbMDP.
As a consequence, for some σc,1
even if the strategy extracted from δ2
computed upper bound at τ = 0 remains valid.

and δ2

τ

τ , the computed ν2

,
τ ,δ2
τ ]
τ replaces unspeciﬁed DRs by any probability distribution. In particular, the

τ remains a valid upper bound of the true vector ν2

[σc,1

C.3.6 Strategy Conversion

As discussed in Section 3.3, no effort is required to extract a solution strategy for a player from the lower bound (for
1) or the upper bound (for 2), but that strategy is in an unusual recursive form. We will here see (in the ﬁnite horizon
setting) how to derive a (unique) equivalent behavioral strategy βi
0: using realization weights [13] in intermediate
steps. To that end, we ﬁrst deﬁne these realization weights in the case of a behavioral strategy (rather than for a
mixed strategy as done by Koller et al.) and present some useful properties.

About Realization Weights Let us denote rwi(ai
τ under strategy βi
0, zi
ai

1, . . . , ai

1, ai

0:, deﬁned as
τ
(cid:89)

rwi(ai

0, zi

1, ai

1, . . . , ai

τ ) def=

0:(ai
βi

t|ai

0, zi

1, ai

1, . . . , zi
t)

0, zi

1, ai

1, . . . , ai

τ ) the realization weight (RW) of sequence

t=0
= rwi(ai

0, zi

1, ai

1, . . . , ai

τ −1) · βi

0:(ai

0, zi

τ | ai
(cid:124)

1, . . . , zi
1, ai
).
τ
(cid:125)
(cid:123)(cid:122)
θi
τ

26

(50)

(51)

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

This deﬁnition already leads to useful results such as:

0:(ai
βi

τ |θi

τ ) =

rwi(θi

τ −1, ai

rwi(θi

τ , ai
τ )

τ −1, zi
τ −1)

τ −1, ai

,

and

∀zi
τ ,

rwi(θi

τ −1, ai

τ −1) = rwi(θi

τ −1, ai

τ −1) ·

(cid:88)

ai
τ
(cid:124)

β(ai

τ |θi

τ −1, ai

τ −1, zi
τ )

rwi(θi

τ −1, ai

τ −1) · β(ai

τ |θi

τ −1, zi
τ )

(cid:125)

(cid:123)(cid:122)
=1
τ −1, ai

=

=

(cid:88)

ai
τ
(cid:88)

ai
τ

rwi(θi

τ −1, ai

τ −1, zi

τ , ai

τ ).

(52)

(53)

(54)

(55)

We now extend Koller et al.’s deﬁnition by introducing conditional realization weights, where the realization weight
of a sufﬁx sequence is “conditioned” on a preﬁx sequence:

rwi(ai
(cid:124)

τ , . . . , ai
τ (cid:48)
(cid:123)(cid:122)
(cid:125)
sufﬁx seq.

| ai
(cid:124)

) def=
0, . . . , zi
τ
(cid:125)
(cid:123)(cid:122)
preﬁx seq.

τ (cid:48)
(cid:89)

t=τ

0:(ai
βi

t|ai

0, . . . , zi

τ , ai

τ , . . . , zi
t)

= βi

0:(ai

τ |ai

0, . . . , zi

τ ) · rwi(ai

τ +1, . . . , ai

τ (cid:48)|ai

0, . . . , zi

τ +1).

(56)

(57)

As can be noted, this deﬁnition only requires the knowledge of a partial strategy βi
βi
0:.

τ : rather than a complete strategy

Mixing Realization Weights Let τ (cid:48) ≥ τ + 1, and rwi[w] denote the realization weights of some element w at
τ + 1. Then, for some δi

τ , we have

rw[δi

τ ](ai

τ +1, . . . , ai

τ (cid:48)|ai

0, . . . , zi

τ +1) =

(cid:88)

w

τ (w) · rw[w](ai
δi

τ +1, . . . , ai

τ (cid:48)|ai

0, . . . , zi

τ +1).

(58)

From wi
equivalent to the recursive strategy induced by some tuple wi

0: Using the above results, function Extract in Algorithm 2 derives a behavioral strategy βi
0:

0 to βi

0 in 3 steps as follows:

1. From wi

0:H−1, ai

0 to rw(θi

H−1) (∀(θi

H−1)) — These (classical) realization weights are obtained
by recursively going through the directed acyclic graph describing the recursive strategy, computing full
length (conditional) realization weights rw(θi
When in a leaf node, at depth H − 1, the initialization is given by Equation (56) when τ = τ (cid:48) = H − 1:

0:t) (for t = H − 1 down to 0).

t:H−1, ai

H−1|θi

0:H−1, ai

rwi(ai

H−1|ai

0, . . . , zi

H−1) def=

H−1
(cid:89)

βi(ai

t|ai

0, . . . , zi
t)

t=H−1
= βi(ai

H−1|ai

0, . . . , zi

H−1).

Then, in the backward phase, we can compute full length realization weights rw(θi
0:t) with
increasingly longer sufﬁxes (thus shorter preﬁxes) using (i) Equation (58) (in function RecGetRWMix,
line 24) to “mix” several strategies using the distribution δi
t attached to the current w, and (ii) Equation (57),
with τ (cid:48) = H − 1, (in function RecGetRWCat, line 15) to concatenate the behavioral decision rule βi
t
attached to the current w in front of the strategy induced by the distribution δi
t also attached to w. Note:
Memoization can here be used to avoid repeating the same computations.

t+1:H−1, ai

H−1|θi

2. Retrieving (classical) realization weights rw(θi

0:t, ai
t|−) for all t’s using Equation (55) (line 6).

0:t, ai

rw(θi

t|−) (∀t) — We can now compute realization weights

3. Retrieving behavioral decision rules βi

t — Applying Equation (52) (line 9) then provides the expected

behavioral decision rules.

In practice, lossless compressions are used to reduce the dimensionality of the occupancy state (cf. Section 5.1), which
are currently lost in the current implementation of the conversion. Ideally, one would like to preserve compressions
whenever possible or at least retrieve them afterwards, and possibly identify further compressions in the solution
strategy.

27

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

Algorithm 2: Extracting βi

0: from wi
0

*/

*/

*/

1 Fct Extract(wi

0)

/* Step 1., keeping only rw(θi
0:H−1)(cid:1)
(cid:0)rw(θi
/* Step 2.
for t = H − 2, . . . , 0 do

θi
0:H−1

← RecGetRWMix(0, wi

0:H−1) for all θi
0)

0:H−1

0:t, ai

forall θi
t do
t+1 ← zi s.t. βt(·|θi
zi
t) ← (cid:80)
rw(θ1
ai

0:t, ai

t+1

0:t, ai

t, zi) is deﬁned

rw(θi

0:t, ai

t, zi

t+1, ai

t+1|−)

/* Step 3.
for t = H − 1, . . . , 0 do

forall θi
t(ai
βi

0:t, ai
t|θi

t do
0:t) ←

rwi(θi

0:t−1,ai

rwi(θi

0:t−1,ai

t,ai
t)

t−1,zi
t−1)

return βi
10
0:
11 Fct RecGetRWMix(t, w = (cid:104)βi
12

for w(cid:48) s.t. δi

t(w(cid:48)) > 0 do

t, δi

t(cid:105))

13

rwCat[w(cid:48)] ← RecGetRWCat(t, w(cid:48))

14

15

forall (ai

H−1) do
t, . . . , ai

0, . . . , ai
rwM ix[w](ai
return rwM ix[w]
16
17 Fct RecGetRWCat(t, w = (cid:104)βi
if t = H − 1 then
18
forall (ai

19

0, . . . , ai
rwCat[w](ai

H−1) do
H−1|ai

t, δi

t(cid:105))

2

3

4

5

6

7

8

9

20

21

22

23

24

25

H−1|ai

0, . . . , zi

t) ← (cid:80)

w(cid:48) δi

t(w(cid:48)) · rwCat[w(cid:48)](ai

t+1, . . . , ai

H−1|ai

0, . . . , zi

t+1)

0, . . . , zi

H−1) ← βi

t(ai

H−1|ai

0, . . . , zi

H−1)

else

rwM ix[w] ← RecGetRWMix(t, w)
forall (ai

0, . . . , ai
rwCat[w](ai
t|ai
t(ai
βi

0, . . . , zi

H−1) do
t, . . . , ai

H−1|ai
0, . . . , zi
t) · rwM ix[w](ai

t) ←
t+1, . . . , ai

H−1|ai

0, . . . , zi

t+1)

return rwCat[w]

D HSVI for zs-POSGs

This section presents results that help (i) tune zs-OMG-HSVI’s radius parameter ρ, ensuring that trajectories will
always stop, and (ii) then demonstrate the ﬁnite time convergence of this algorithm.

D.1 Algorithm

D.1.1 Setting ρ

Proposition 9. (originally stated on page 28) Bounding λτ by λ∞ = 1
2
that

1
1−γ [rmax − rmin] when γ < 1, and noting

thr(τ ) = γ−τ (cid:15) − 2ρλ∞ γ−τ − 1
1 − γ

( or (cid:15) − ρ(rmax − rmin)(2H + 1 − τ )τ

if γ = 1 ),

(59)

one can ensure positivity of the threshold at any τ ∈ 1 . . H − 1 by enforcing 0 < ρ < 1−γ
(rmax−rmin)(H+1)H if γ = 1).

(cid:15)

2λ∞ (cid:15) (or 0 < ρ <

Proof. Let us ﬁrst consider the case γ < 1.
We have (for τ ∈ {1 . . H − 1}):

thr(τ ) = γ−τ (cid:15) −

τ
(cid:88)

i=1

2ρλ∞γ−i

28

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

= γ−τ (cid:15) − 2ρλ∞

τ
(cid:88)

i=1

γ−i

= γ−τ (cid:15) − 2ρλ∞ (cid:0)γ−1 + γ−2 + · · · + γ−τ (cid:1)
= γ−τ (cid:15) − 2ρλ∞γ−1 (cid:16)
= γ−τ (cid:15) − 2ρλ∞γ−1 γ−τ − 1
γ−1 − 1

γ0 + γ−1 + · · · + γ−(τ −1)(cid:17)

= γ−τ (cid:15) − 2ρλ∞ γ−τ − 1
1 − γ

.

Then, let us derive the following equivalent inequalities:

0 < thr(τ )

2ρλ∞ γ−τ − 1
1 − γ

< γ−τ (cid:15)

ρ <

ρ <

1
2λ∞
1
2λ∞

γ−τ (cid:15)

1 − γ
γ−τ − 1
1 − γ
1 − γτ (cid:15).

To ensure positivity of the threshold for any τ ≥ 1, one thus just needs to set ρ as a positive value smaller than 1−γ

2λ∞ (cid:15).

Let us now consider the case γ = 1.
We have (for τ ∈ {1, . . . , H − 1}):

thr(τ ) def= (cid:15) −

= (cid:15) −

τ
(cid:88)

i=1
τ
(cid:88)

i=1

2ρλτ −i

2ρ(H − (τ − i)) · (rmax − rmin)

= (cid:15) − 2ρ(rmax − rmin)

τ (H − τ ) +

(cid:34)

τ
(cid:88)

(cid:35)
i

= (cid:15) − 2ρ(rmax − rmin)

(cid:20)
τ H − τ 2 +

i=1
1
2

τ (τ + 1)

(cid:21)

(cid:20)

1
= (cid:15) − 2ρ(rmax − rmin)
2
= (cid:15) − ρ(rmax − rmin) (cid:2)(2H + 1)τ − τ 2(cid:3)
= (cid:15) − ρ(rmax − rmin) [(2H + 1 − τ )τ ] .

(H +

)τ −

1
2

(cid:21)

τ 2

Then, let us derive the following equivalent inequalities:

0 < thr(τ )

ρ(rmax − rmin)(2H + 1 − τ )τ < (cid:15)

(holds when τ = 0 and τ = H + 1)

ρ <

(cid:15)
(rmax − rmin)(2H + 1 − τ )τ

(when τ ∈ {0 . . H + 1}).

The function f : τ (cid:55)→
2 . To ensure
positivity of the threshold for any τ ∈ {1 . . H − 1}, one thus just needs to set ρ as a positive value smaller than
(cid:15)
(rmax−rmin)(H+1)H .

(rmax−rmin)(2H+1−τ )τ reaches its minimum (for τ ∈ (0, H + 1)) when τ = H + 1

(cid:15)

D.2 Finite-Time Convergence

D.2.1 Convergence Proof

Proving the ﬁnite-time convergence of zs-OMG-HSVI to an error-bounded solution requires some preliminary
lemmas.
Lemma 8. Let (σ0, . . . , στ +1) be a full trajectory generated by zs-OMG-HSVI and βτ the joint behavioral DR that
induced the last transition, i.e., στ +1 = T (στ , βτ ). Then, after updating W
τ ) −
W 2

τ , we have that W

1
τ and W 2

1
τ (στ , β1

τ ) ≤ γthr(τ + 1).

τ (στ , β2

29

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

Proof. By deﬁnition,

W

1
τ (στ , β1

τ ) =

(cid:104)˜σc,1

min
τ , ˜β2
τ ,ν2
∈bagW 1
τ

τ +1(cid:105)

(cid:16)

β1
τ ·

r(στ , ·, ˜β2

τ ) + γT 1

m(στ , ·, ˜β2

τ ) ·

(cid:104)

ν2
τ +1 + λτ +1

#                                                                     »
τ ) − T 1
(cid:107)T 1
τ )(cid:107)1

c (σc,1

c (˜σc,1

τ , β2

τ , β2

(cid:105)(cid:17)
.

Therefore, after the update (β2
ν2
τ +1 and ν1
τ +1),

τ and β1

τ being added to their respective bags (bagW

1
τ and bagW 2
τ

) along with vectors

W
W 2

1
τ (στ , β1
τ (στ , β2

τ ) ≤ β1
τ ) ≥ β2

τ · (cid:2)r(στ , ·, β2
τ · (cid:2)r(στ , β1

τ ) + γT 1
τ , ·) + γT 2

m(στ , ·, β2
m(στ , β1

τ ) · ν2
τ , ·) · ν1

τ +1

τ +1

(cid:3) , and
(cid:3) .

Then,

W

1
τ (στ , β1

τ ) − W 2

τ (στ , β2

τ ) ≤

(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)
(cid:104)
τ , β2
r(στ , β1

m(στ , βτ ) · ν2
τ ) + γT 1
= γ (cid:2)V (T (στ , βτ )) − V (T (στ , βτ ))(cid:3)
≤ γthr(τ + 1)

(Holds at the end of any trajectory.)

(cid:105)

−

(cid:104)

(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)
τ , β2
r(στ , β1

τ ) + γT 2

m(στ , βτ ) · ν1

τ +1

(cid:105)

τ +1

Lemma 9 (Monotonic evolution of W
at στ with behavioral DR (cid:104)β
K (n+1)W 2

τ be the approximations after an update
1
τ +1 and ν1
τ and
τ be the same approximations after n other updates (in various OSs). Then,
1
τ )

1
τ and KW 2
τ ). Let KW
(cid:105) (respectively associated to vectors ν2

τ +1). Let also K (n+1)W

1
τ and W 2

1
τ (στ , β1

1
τ (στ , β1

K (n+1)W

1
τ (στ , β

τ ) ≤ W

1
τ , β2
τ

KW

and

max
β1
τ

τ ) ≤ max
β1
τ

min
β2
τ

K (n+1)W 2

τ (στ , β2

τ ) ≥ min
β2
τ

KW 2

τ (στ , β2

τ ) ≥ W 2

τ (στ , β2
τ

).

Proof. Starting from the deﬁnition,

KW

1
τ (στ , β1
τ )

max
β1
τ

= max
β1
τ
bagW 1

min
τ ,ν2
(cid:104)˜σc,1
τ ,β2
τ ∪{(cid:104)σc,1
τ ,β2

τ +1(cid:105)∈
,ν2

τ

τ +1(cid:105)}

(cid:20)
r(στ , ·, β2

τ ) + γT 1

m(στ , ·, β2

τ ) ·

(cid:16)

ν2
τ +1 + λτ +1

#                                                                     »
τ ) − T 1
(cid:107)T 1
τ )(cid:107)1

c (˜σc,1

c (σc,1

τ , β2

τ , β2

(cid:17)(cid:21)

β1
τ ·

(cid:20)
r(στ , ·, β2

τ ) + γT 1

m(στ , ·, β2

τ ) ·

(cid:16)

ν2
τ +1 + λτ +1

#                                                                     »
(cid:107)T 1
τ ) − T 1
τ )(cid:107)1

c (˜σc,1

c (σc,1

τ , β2

τ , β2

(cid:17)(cid:21)

β1
τ ·

min
τ +1(cid:105)∈bagW 1

τ

≤ max
β1
τ

(cid:104)˜σc,1

τ ,β2

τ ,ν2
1
τ (στ , β1
τ )

W

= max
β1
τ
1
τ (στ , β

= W

1
τ ).

Then, this upper bound approximation can only be reﬁned, so that, for any n ∈ N,

∀β1

τ , K (n+1)W
K (n+1)W

thus, min
β1
τ

1
τ (στ , β1
1
τ (στ , β1

τ ) ≤ KW

τ ) ≤ min
β1
τ

1
τ (στ , β1
τ ),
1
τ (στ , β1
KW

τ ).

The expected result thus holds for W

1
τ , and symmetrically for W 2
τ .

Lemma 10. After updating, in order, W

After updating, in order, W 2

1
τ and V τ , we have KV τ (στ ) ≤ maxβ1
τ (στ , β2

KW 2

τ

τ and V τ , we have KV τ (στ ) ≥ minβ2

τ ).

τ

KW

1
τ (στ , β1

τ ).

Proof. After updating bagW
τ ) and the associated vector ν2
σ1

τ , so that

1
τ , the algorithm computes (Algorithm 1, Line 8) a new solution δ

2
τ of the dual LP (at

KW

max
β1
τ

1
τ (σ1

τ , β1

τ ) = σm,1

τ

· ν2
τ .

This vector will feed bagV τ along with σ1

τ , so that

KV τ (στ ) ≤ σm,1

τ

· ν2
τ .

30

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

As a consequence,

KV τ (στ ) ≤ max

β1
τ

KW

1
τ (στ , β1

τ ).

The symmetric property holds for KV τ and KW 2
Theorem 3. (originally stated on page 7) zs-OMG-HSVI (Algorithm 1) terminates in ﬁnite time with an (cid:15)-
approximation of V ∗

τ , which concludes the proof.

0 (σ0).

Proof. We will prove by induction from τ = H to 0, that the algorithm stops expanding OSs at depth τ after ﬁnitely
many iterations (/trajectories).

First, by deﬁnition of horizon H, no OS σH is ever expanded. The property thus holds at τ = H.

Let us now assume that the property holds at depth τ + 1 after Nτ +1 iterations. By contradiction, let us assume
that the algorithm generates inﬁnitely many trajectories of length τ + 1. Then, because Oσ
τ × Bτ is compact, after
some time the algorithm will have visited (cid:104)στ , βτ (cid:105), then, some iterations later, (cid:104)σ
τ (cid:107)1 ≤ ρ.
Let us also note the corresponding terminal OSs (because trajectories beyond iteration Nτ +1 do not go further)
στ +1 = T (στ , βτ ) and σ

τ (cid:105), such that (cid:107)στ − σ

τ , β

τ +1 = T (σ

τ , β

τ ).

(cid:48)

(cid:48)

(cid:48)

(cid:48)

(cid:48)

(cid:48)

Now, we show that the second trajectory should not have happened, i.e., V (σ

(cid:48)

τ ) − V (σ

(cid:48)

τ ) ≤ thr(τ ).

Combining the previous lemmas,

V (σ

Symmetrically, we also have

Hence,

V (σ

V (σ

(cid:48)

τ ) − V (σ

(cid:48)

τ ) ≤ V (στ ) + λτ (cid:107)στ − σ
1

(cid:48)

τ (cid:107)1

τ (στ , ˜β1

τ ) + λτ (cid:107)στ − σ

(By Lipschitz-Continuity)

(cid:48)

τ (cid:107)1

(Lemma 10)

τ ) + λτ (cid:107)στ − σ

(cid:48)

τ (cid:107)1

(Lemma 9)

W

≤ max
˜β1
τ
1
τ (στ , β1
1
τ (στ , β1

= W

≤ W

τ ) + λτ ρ.

(cid:48)

τ ) ≥ W 2

τ (στ , β2

τ ) − λτ ρ.

(cid:16)

(cid:48)

τ ) ≤

(cid:16)

=

W

W

τ ) + λτ ρ

1
τ (στ , β1
1
τ ) − W 2
τ (στ , β1
≤ γthr(τ + 1) + 2λτ ρ
τ +1
(cid:88)

(cid:32)

γ−(τ +1)(cid:15) −

= γ

(cid:17)

− (cid:0)W 2
τ (στ , β2
(cid:17)

τ (στ , β2
τ )

+ 2λτ ρ

τ ) − λτ ρ(cid:1)

(Lemma 8)

(cid:33)

2ρλτ +1−iγ−i

+ 2λτ ρ

i=1

2ρλτ +1−iγ−i+1 + 2λτ ρ

2ρλτ −jγ−j + 2λτ ρ

= γ−τ (cid:15) −

= γ−τ (cid:15) −

τ +1
(cid:88)

i=1
τ
(cid:88)

j=0

= γ−τ (cid:15) − (cid:24)(cid:24)(cid:24)(cid:24)(cid:24)
2ρλτ −0γ−0 −

τ
(cid:88)

j=1

2ρλτ −jγ−j + (cid:8)(cid:8)(cid:8)

2λτ ρ = thr(τ ).

(cid:48)

Therefore, σ
trajectories of length τ .

τ should not have been expanded. This shows that the algorithm will generate only a ﬁnite number of

D.2.2 Handling Inﬁnite Horizons

Proposition 2. (originally stated on page 8) When γ < 1, using the depth-independent Lipschitz constant λ∞, and
with W def= (cid:107)V
− V (0)(cid:107)∞ the maximum width between initializations, the length of trajectories is upper bounded
(cid:24)
(cid:15)− 2ρλ∞
W − 2ρλ∞

by Tmax

logγ

def=

(0)

1−γ

(cid:25)

.

1−γ

31

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

Table 4: Number of states/actions/observations for each benchmark problem

S A1 A2 O1 O2

Competitive Tiger
Adversarial Tiger
Recycling Robot
Mabc

2
2
4
4

4 4
3 2
3 3
2 2

3
2
2
2

3
2
2
2

Proof. (detailed version) Since W is the largest possible width, any trajectory stops in the worst case at depth τ
such that

thr(τ ) < W

γ−τ (cid:15) − 2ρλ∞ γ−τ
1 − γ

γ−τ (cid:15) − 2ρλ∞ γ−τ − 1
1 − γ
− 2ρλ∞ −1
1 − γ
(cid:19)
(cid:18)

γ−τ

(cid:15) −

(cid:124)

>0

2ρλ∞
1 − γ
(cid:123)(cid:122)
(Prop. 9)

(cid:125)

(from Eq. (59))

< W

< W

< W −

2ρλ∞
1 − γ

γ−τ <

W − 2ρλ∞
1−γ
(cid:15) − 2ρλ∞
1−γ
(cid:32)

exp(−τ ln(γ)) < exp

ln

(cid:33)(cid:33)

(cid:32) W − 2ρλ∞
1−γ
(cid:15) − 2ρλ∞
1−γ
(cid:33)

(cid:33)

(cid:32) W − 2ρλ∞
1−γ
(cid:15) − 2ρλ∞
1−γ
(cid:32) (cid:15) − 2ρλ∞
1−γ
W − 2ρλ∞
1−γ
(cid:32) (cid:15) − 2ρλ∞
1−γ
W − 2ρλ∞
1−γ

(cid:33)

.

−τ ln(γ) < ln

τ ln(γ) > ln

τ < logγ

Even if the problem horizon is inﬁnite, trajectories will thus have bounded length. Then, everything beyond this
effective horizon will rely on the upper- and lower-bound initializations and the corresponding strategies.

E Experiments

This section provides (i) information regarding the benchmark problems at hand in Table 4 and (ii) supplemental
experimental results.

Graphs in Figure 2 show how the upper- and lower-bounding values at σ0 (i.e., V 0(σ0) (red curve) and V 0(σ0) (blue
curve)) evolve as a function of the number of iterations, here considering the same benchmarks and time horizons as
in Section 5 (except for H = 2).

As expected, these bounds monotonically converge toward the optimal value (here provided by Sequence Form in
all cases but Recycling Robot for H = 6). This convergence would be symmetric in Competitive Tiger, the only
symmetric game, if it were not for the algorithm breaking ties in a biased manner when multiple equivalent solutions
are possible.

The dotted red (respectively blue) curve is obtained by simply removing from bagV 0 and (resp. bagV
) its initial
element. This curve somehow allows better observing the actual (hidden) progress of the convergence at the beginning
since the resulting value is always obtained (even in ﬁrst iterations) from updates. A solid curve is thus constant until
joined by the corresponding dotted curve. In the case of Adversarial Tiger, the “merge” takes place earlier for the
upper bound than for the lower bound for H = 5, while the opposite is observed for H = 4. Such a difference might
be due to the optimal strategies being possibly very different when the horizon is extended (e.g., as also observed in
the Tiger POMDP, or the Multi-Agent Tiger Dec-POMDP), but needs to be further investigated.

0

32

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

Note also that iteration durations are expected to increase with the time horizon, so that it is surprising to observe
similar numbers of iterations in 24 hours in both Competitive Tiger (H = 4 vs 5) and Recycling Robot (H = 5 vs 6).
This phenomenon is currently under investigation, but probably linked to the very efﬁcient compression in these two
problems.

F Relying on Lipschitz-Continuity Alone

This appendix demonstrates that one can derive an other version of HSVI for solving zs-POSGs using only V ∗
Lipschitz-continuity (Theorem 2, p. 5). To that end, we

1. ﬁrst describe appropriate LC upper and lower bound approximations;

2. then discuss the various operators (initialization, update and pruning) required by these approximations; and

3. ﬁnally explain how the local game faced in each visited στ is solved as bi-level optimization problem with
an error-bounded algorithm based on Munos’ Deterministic Optimistic Optimization (DOO) algorithm
(2014).

We focus here on upper bounding V ∗, as lower bounding is a symmetrical problem.

F.1 LC-only Approximation of V ∗

V ∗
τ being LC, we deﬁne an upper bound approximation V τ at depth τ as a lower envelope of (i) an initial upper
(0)
τ (στ ) and (ii) downward-pointing L1-cones, where an upper-bounding cone c = (cid:104)στ , vτ (cid:105)—located at στ ,
bound V
τ (στ ) def= vτ + λ(H−τ )(cid:107)στ − στ (cid:107)1. Formally,
with “summit” value vτ , and slope λ(H−τ )—induces a function V
the set of cones being denoted bagC τ ,

(c)

V τ (στ ) = min{V

(0)
τ (στ ), min

c∈bagCτ

V

(c)
τ (στ )}.

F.2 Related Operators

Initialization With such an approximation, the same relaxations proposed in Sec. 3.3 can be used to initialize V .
Note that that the initialization is required to

• be a LC function (because the LC property is needed to solve local games); and

• come with a default decision rule at each time step for 2 (that shall be used as a default (safe) strategy to

execute).

τ , β2

τ ) def= r(στ , β1

Update Then, updating this approximation when in στ requires solving the (inﬁnite) local game deﬁned by
τ , β2
Qτ (στ , β1
Lemma 11. In any στ , the induced local game Qτ (στ , β1
τ and β2
continuous in both β1
τ .

τ )), what is enabled by the following property.
τ )+γV τ +1(T (στ , β1
τ , β2

τ ) + γV τ (στ , β1

τ ) def= r(στ , β1

τ )) is Lipschitz

τ , β2

τ , β2

τ , β2

Proof. In this local game’s deﬁnition,

Qτ (στ , β1

τ , β2

τ ) def= r(στ , β1

τ , β2

τ ) + γV τ +1(T (στ , β1

τ , β2

τ )),

• the ﬁrst term (reward-based) is λr-LC in each βi

τ (Proposition 1, p. 3), with λr = rmax−rmin

, and

2

• the second term is (γ · λτ +1 · 1)-LC in each βi

T being linear in βi

τ (Proposition 1, p. 3).

τ , due to (i) V τ +1 being λτ +1-LC (Theorem 2, p. 5), and (ii)

Qτ is thus λQ

τ -LC with λQ

τ = λr + γ · λτ +1.

We provide an algorithm for solving such a bi-level optimization problem given the known Lipschitz constant in
Appendix F.3. It returns β

1
τ to guide the trajectory, and the associated value v to create a new cone at στ .

33

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (a) Adversarial Tiger . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (b) Competitive Tiger . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (c) Mabc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (d) Recycling Robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Figure 2: Evolution of the upper- and lower-bound value V 0(σ0) (in red) and V 0(σ0) (in blue) of OMGHSVILC
CC for
the various benchmark problems as a function of the number of iterations (generated trajectories). Dotted curves:
same bounding approximations when removing each bag’s (bagV 0 and bagV
) initial elements. In green: reference
optimal value found by Sequence Form LP (when available).

0

34

020406080100number of trajectories151050510valueAdversarial Tiger, LC+CC, Horizon 3upper boundlower boundoptimal value from Seq. Form02004006008001000number of trajectories20151050510valueAdversarial Tiger, LC+CC, Horizon 4upper boundlower boundoptimal value from Seq. Form0100200300400500number of trajectories252015105051015valueAdversarial Tiger, LC+CC, Horizon 5upper boundlower boundoptimal value from Seq. Form0250500750100012501500number of trajectories42024valueCompetitiveTiger, LC+CC, Horizon 3upper boundlower boundoptimal value from Seq. Form050100150200250300350400number of trajectories6420246valueCompetitiveTiger, LC+CC, Horizon 4upper boundlower boundoptimal value from Seq. Form050100150200number of trajectories10.07.55.02.50.02.55.07.510.0valueCompetitiveTiger, LC+CC, Horizon 5upper boundlower boundoptimal value from Seq. Form051015202530number of trajectories0.00.20.40.60.81.01.2valueMABC, LC+CC, Horizon 3upper boundlower boundoptimal value from Seq. Form0100200300400500600700800number of trajectories0.00.20.40.60.81.01.21.41.6valueMABC, LC+CC, Horizon 4upper boundlower boundoptimal value from Seq. Form050100150200250300350400number of trajectories0.000.250.500.751.001.251.501.752.00valueMABC, LC+CC, Horizon 5upper boundlower boundoptimal value from Seq. Form050100150200250300number of trajectories420246valueRecycling Robot, LC+CC, Horizon 3upper boundlower boundoptimal value from Seq. Form05001000150020002500number of trajectories64202468valueRecycling Robot, LC+CC, Horizon 4upper boundlower boundoptimal value from Seq. Form050010001500200025003000number of trajectories7.55.02.50.02.55.07.510.0valueRecycling Robot, LC+CC, Horizon 5upper boundlower boundoptimal value from Seq. Form050010001500200025003000number of trajectories1050510valueRecycling Robot, LC+CC, Horizon 6upper boundlower boundHSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

2
Execution But let us point out that v is a worst expected value for 2 if (i) applying β
τ , solution of the dual bi-level
2
1
optimization problem, and (ii) then acting so as to obtain at most V τ +1(T (στ , β
τ )). We see here appearing
τ , β
again a recursive deﬁnition of strategies for 2 with guaranteed worst-case value. This leads to storing along with each
2
upper-bounding cone both (i) β
τ and (ii) a pointer to the (single) cone at τ + 1 (or the initial value approximation)
involved in computing v, which will allow deducing decision rules to apply from τ + 1 on.

Then, once the algorithm has converged, 2 (resp. 1) can retrieve a solution strategy to perform using the (recursive)
strategy attached to V 0(σ0) (resp. V 0(σ0)).

Pruning In this setting where all cones have the same slope, a cone c = (cid:104)στ , vτ (cid:105) of V τ can be pruned if and only
if it is dominated by another cone at στ .

F.3 Bi-level Optimization with DOO

The discussion below explains how to compute an optimal strategy proﬁle (cid:104)β1
τ (cid:105) at any time step τ for any
occupancy state στ for the local game Qτ (στ , β1
τ )). Unfortunately,
Qτ (στ , β1
τ ) is not convex or concave, so that a method relying on differentiability could return a local optimum
instead of the optimal value. Moreover, we require the algorithm to ﬁnd an (cid:15)-optimal solution in ﬁnite time,
asymptotic convergence guarantees being insufﬁcient.

τ ) + γV τ +1(T (στ , β1

τ ) def= r(στ , β1

τ , β2

τ , β2

τ , β2

τ , β2

τ , β2

Lipschitz Optimization Munos’ DOO (Deterministic Optimistic Optimization) algorithm (2014) is a ﬁnite-time
(cid:15)-optimal algorithm that computes maxx f (x) for a λ-Lipschitz function f (see Algorithm 3, top). It iteratively
covers up a compact search space by other compact sets (the current cover being here denoted (Ri)i∈I) on which the
optimal value function can be upper bounded thanks to its Lipschitz continuity. Each set Ri is attached a reference
point xi, its value f (xi), and its radius ri,8 so that the value of f on Ri is upper bounded by f (xi) + λri. With this,
the algorithm starts with a few compact sets and iteratively subdivides the one whose upper bound is the largest.
Repeating this, the algorithm converges towards an (cid:15)-optimal solution in ﬁnite time. Figure 3 [19] represents a
subdivision tree for the optimisation of f (x) = [sin(13x)sin(27x) + 1]/2. Note that, except for the subdividing
process that will be discussed later, this algorithm is generic.

Algorithm 3: DOO & BiDOO

(cid:104)
1 Fct DOO(

(cid:105)
D → R ; x (cid:55)→ f (x)
:f : R → R is a λ-Lipschitz function

, (cid:15), λ)

input
Initialize I and (Ri)i∈I s.t. D ⊆ ∪i∈IRi
while maxi∈I (f (xi) + λri) − maxi∈I f (xi) > (cid:15) do

2

3

4

5

6

7

8

i∗ ← arg maxi∈I f (xi) + λri
Subdivide Ri∗ into ∪j∈I∗ Rj
∀j ∈ I ∗, xj ← Center(Rj)
I ← [I \ i∗] ∪ I ∗
return (cid:104)arg & maxxi f (xi)(cid:105)

(⊇ Ri∗ )

// Theorem 5 allows building I∗ in the case of simplexes.

(cid:104)
Dx × Dy → R ; x, y (cid:55)→ f (x, y)

(cid:105)
, (cid:15)1, (cid:15)2, λ))

: f : Dx × Dy → R a λ-Lipschitz function (in both x and y)

9 Fct BiDOO(
input
(cid:104)xmax, vmax(cid:105) ← DOO(

10

(cid:104)
Dx → R ; x (cid:55)→ −getM ax
(cid:15)1, λ)

(cid:16)

(cid:104)
DOO(

(cid:105)
Dy → R ; y (cid:55)→ −f (x, y)

, (cid:15)2, λ)

(cid:17)(cid:105)
,

/* getM ax(·, ·) here returns its second argument, i.e., the maximum of the inner DOO computation.

*/

11 return (cid:104)xmax, vmax(cid:105)

Lipschitz bi-level Optimization We solve our bi-level optimization problem (ﬁnding maxx miny f (x, y) for a
λ-Lipschitz function f ) by using two nested DOO processes, i.e.,

• an outer (cid:15)1-optimal DOO maximizing the function x (cid:55)→ miny f (x, y), using the solution of
• an inner (cid:15)2-optimal DOO minimizing the function y (cid:55)→ f (x, y) for ﬁxed x

(see Algorithm 3, bottom). Munos’ proof straightforwardly adapts to this case. The ﬁnal error is then (cid:15) = (cid:15)1 + (cid:15)2.

8Ri is contained in the ball of center xi and radius ri.

35

HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

Figure 3: Example tree obtained while maximizing f (x) = [sin(13x)sin(27x) + 1]/2 (Fig. 3.7 from Munos [19])

Figure 4: Example of iterative subdivision of the 2D simplex using squares. The optimized function is (x, y) →
x + 1.01y2. The blue diagonal represents valid probability distributions (i.e., x + y = 1). Ignored hypercubes are
colored in brown and retained ones in green.

36

0.00.20.40.60.81.0x0.00.20.40.60.81.0yHSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties

PREPRINT - OCTOBER 28, 2021

In our setting, we need to subdivide an n-dimensional probability simplex (in
Subdividing Probability Simplexes
fact, one |A|-dimensional simplex per individual action-observation history). To that end, we propose starting with
the n-dimensional hypercube (i.e. a closed ball B(xi, ri) = {x ∈ Rn | (cid:107)x − xi(cid:107)∞ ≤ ri} of radius ri and whose
center is xi) that contains this simplex (with lowest corner (0, . . . , 0) and highest corner (1, . . . , 1)) and subdividing
it in 2n smaller hypercubes, but only keeping the ones that intersect with the simplex. Then, for a given hypercube,
one will evaluate the function to optimize at the center of the intersection between the hypercube and the simplex.
The following theorem shows how to determine whether an hypercube intersects the simplex or not, and how to
compute its reference point.
Theorem 5 (Intersection bewteen the n-dimensional unit simplex and an n-dimensional hypercube). Let n ∈ N∗\{1}.
Let H be an n-dimensional cube (i.e. a closed ball for (cid:107)·(cid:107)∞) of radius η whose center is called m, and let S(1) be
the unit simplex in dimension n. Then, H and S(1) intersect (i.e., (H ∩ S(1)) (cid:54)= ∅) if and only if ∃(xi, xj) such that
(cid:80)n
j ≥ 1, and the unique intersection point can be computed.

i ≤ 1 and (cid:80)n

k=1 xk

k=1 xk

Proof. Let us consider the diagonal from the lowest point (xinf = (m1 − η, . . . , mn − η)) to the highest point
(xsup = (m1 + η, . . . , mn + η)), and use the Intermediate Value Theorem on it. There is no intersection point if
(cid:80)

sup < 1. Else, the intersection point is x = xinf +t(xsup −xinf ), where t =

inf > 1 or (cid:80)

k=1 xk

1−(cid:80)n

k xk

inf

.

n·(cid:107)xinf −xsup(cid:107)∞

k xk
Indeed,

n
(cid:88)

k=1

xk = 1 ⇔

n
(cid:88)

k=1

⇔ t =

⇔ t =

(cid:2)xk

inf + t(xk

sup − xk

inf )(cid:3) = 1

1 − (cid:80)n
(cid:80)
k xk
1 − (cid:80)n

k=1 xk
inf
sup − xk
inf
k=1 xk

inf

n · (cid:107)xsup − xinf (cid:107)∞

.

(60)

(61)

(62)

Figure 4 illustrates the iterative subdivision of the 2-dimensional unit simplex by 2-dimensional hypercubes (i.e.,
squares). Let us point out that the subdivision operation is here concentrated around the optimum: (0, 1).

37

