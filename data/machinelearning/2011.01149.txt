Collective Knowledge: organizing research projects as a database of
reusable components and portable workﬂows with common APIs

Grigori Fursin

cTuning foundation and cKnowledge SAS

github.com/ctuning/ck

cKnowledge.io

Accepted for Philosophical Transactions of the Royal Society A

(Submitted: 12 June 2020)

Abstract

This article provides the motivation and overview of the Collective Knowledge framework (CK or cKnowledge).
The CK concept is to decompose research projects into reusable components that encapsulate research artifacts
and provide uniﬁed application programming interfaces (APIs), command-line interfaces (CLIs), meta descriptions
and common automation actions for related artifacts. The CK framework is used to organize and manage research
projects as a database of such components.
Inspired by the USB ”plug and play” approach for hardware, CK also helps to assemble portable workﬂows that
can automatically plug in compatible components from diﬀerent users and vendors (models, datasets, frameworks,
compilers, tools). Such workﬂows can build and run algorithms on diﬀerent platforms and environments in a uniﬁed
way using the customizable CK program pipeline with software detection plugins and the automatic installation of
missing packages.
This article presents a number of industrial projects in which the modular CK approach was successfully validated in
order to automate benchmarking, auto-tuning and co-design of eﬃcient software and hardware for machine learning
(ML) and artiﬁcial intelligence (AI) in terms of speed, accuracy, energy, size and various costs. The CK framework
also helped to automate the artifact evaluation process at several computer science conferences as well as to make it
easier to reproduce, compare and reuse research techniques from published papers, deploy them in production, and
automatically adapt them to continuously changing datasets, models and systems.
The long-term goal is to accelerate innovation by connecting researchers and practitioners to share and reuse all their
knowledge, best practices, artifacts, workﬂows and experimental results in a common, portable and reproducible
format at cKnowledge.io.

Keywords:
research automation, reproducibility, reusability, portability, DevOps, AIOps, MLOps, FAIR principles,
knowledge management, best practices, collaboration, optimization, portable workﬂow, microservices, adaptive container,
machine learning, artiﬁcial
intelligence, edge devices, MLPerf, API, crowd-benchmarking, crowd-tuning, auto-tuning,
software/hardware co-design, eﬃcient systems, Pareto eﬃciency, optimization repository

1 Motivation

provided by volunteers.

1
2
0
2

n
a
J

0
3

]

G
L
.
s
c
[

2
v
9
4
1
1
0
.
1
1
0
2
:
v
i
X
r
a

Ten years ago I developed the cTuning.org platform
and released all my research code and data into the
public domain in order to crowdsource the training
of our machine learning-based compiler (MILEPOST
GCC)
to accelerate the
very time-consuming auto-tuning process and help our
compiler to learn the most eﬃcient optimizations across
real programs, datasets, platforms and environments

My intention was

[41].

We had a positive response from the community and,
in just a few days, I collected as many optimization
results as I had during the entire MILEPOST project.
However, the initial excitement quickly faded when I
struggled to reproduce most of the performance numbers
and ML model predictions because even a tiny change in
software, hardware, environment or the run-time state of
the system could inﬂuence performance while I did not
have a mechanism to detect such changes [51, 43].

1

 
 
 
 
 
 
Figure 1: Reproducing research papers and adopting novel techniques in production is a tedious, repetitive and
time-consuming process because of continuously changing software, hardware, models and datasets, and a lack of
common formats and APIs for shared artifacts (code, data, models, experimental results, scripts and so on).

Worse still,

I could not compare these empirical
results with other published techniques because they
rarely included the full experiment speciﬁcation with
the shared research code and all the necessary artifacts
needed to be able to reproduce results. Furthermore, it
was always very diﬃcult to add new tools, benchmarks
and datasets to any research code because it required
numerous changes in diﬀerent ad-hoc scripts, repetitive
recompilation of the whole project when new software was
released, complex updates of database tables with results
and so on [42].

These problems motivated me

to establish the
non-proﬁt cTuning foundation and continue working
with the community on a common methodology and
open-source tools to enable collaborative, reproducible,
reusable and trustable R&D. We helped to initiate
reproducibility initiatives and support artifact evaluation
at several computer science conferences in collaboration
with the ACM [5, 40]. We also promoted sharing of code,
artifacts and results in a uniﬁed way along with research
papers [4].

This community service gave me a unique chance
to participate in reproducibility studies of more than
100 research papers at the International Conference on
Architectural Support for Programming Languages and
Operating Systems (ASPLOS), International Symposium
on Code Generation and Optimization (CGO), ACM
SIGPLAN Annual Symposium on Principles and Practice
of Parallel Programming (PPoPP), Supercomputing,
International Conference on Machine Learning and

Systems (MLSys) and other computer science conferences
over the past ﬁve years [27]. I also started deploying some
of these techniques in production in collaboration with my
industrial partners in order to better understand all the
problems associated with building trustable, reproducible
and production-ready computational systems.

is

threefold.

This practical experience conﬁrmed my previous
ﬁndings: while sharing ad-hoc research code, artifacts,
trained models, and Docker images along with research
it is only the tip of
papers is a great step forward,
the reproducibility iceberg [42]. The major challenge
afterwards
is necessary to ﬁgure
out how to: use research techniques outside original
containers with other data, code and models; run them
in a reliable and eﬃcient way across rapidly evolving
software, heterogeneous hardware and legacy platforms
with continuously changing interfaces and data formats;
and balance multiple characteristics including speed,
latency, accuracy, memory size, power consumption,
reliability and costs (Figure 1).

It

2 Collective Knowledge concept

When helping to organize the artifact evaluation process
at systems and machine learning conferences, I decided
to introduce an Artifact Appendix and a reproducibility
checklist [4, 27]. My goal was to help researchers
to describe how to conﬁgure, build, run, validate and
compare research techniques in a uniﬁed way across
It also led me to
diﬀerent conferences and journals.

2

Develop new softwarePublish new algorithms and modelsUse standard data setPublish new algorithms and modelsReported accuracy and other resultsDesign new hardwareResearchersEngineersSomehow builda workingsystem based on published resultsand a new SW/HW stackStable containerReal dataAttempt to reproduce resultsReal accuracyReal speedSizeCostsIncrementContinue rebuilding, optimizing, reproducingStruggle to reproduce results and design an efficient systemPractitionerscan take many months to optimize & deployVery painful and time-consuming to build and optimize algorithms with continuously changing software, hardware, models & datafrom different vendors Timelinenotice that most research projects use some ad-hoc
scripts, often with hardwired paths, to perform the
same repetitive tasks including downloading models
and datasets, detecting platform properties,
installing
software dependencies, building research code, running
experiments, validating outputs,
reproducing results,
plotting graphs and generating papers [42].

This experience motivated me to search for a solution
for automating such common tasks and making them
reusable and customizable across diﬀerent
research
projects, platforms and environments. First, I started
looking at related tools that were introduced to automate
experiments and make research more reproducible:

• Workﬂow frameworks

such as MLFlow [57],
Kedro [52], Amazon SageMaker [3], Kubeﬂow [34],
Apache Taverna [56], popper [48], CommonWL [36]
and many others help to abstract and automate
data science operations.
They are very useful
for data scientists but do not yet provide a
universal mechanism to automatically build and run
algorithms across diﬀerent platforms, environments,
libraries, tools, models and datasets. Researchers
this
and engineers often have
functionality for each new project from scratch,
which can become very complicated particularly
when targeting new hardware, embedded devices,
TinyML and IoT.

to implement

• Machine learning benchmarking initiatives such as
MLPerf [53], MLModelScope [39] and Deep500 [37]
attempt to standardise machine learning model
benchmarking and make it more reproducible.
However, production deployment, integration with
complex systems and adaptation to continuously
changing user environments, platforms, tools, and
data are currently out of their scope.

• Package managers

[46]

such as Spack

and
EasyBuild [47] are very useful for rebuilding and
ﬁxing the whole software environment. However,
the integration with workﬂow frameworks and
automatic adaptation to existing environments,
native cross-compilation particularly for embedded
devices, and support for non-software packages
(models, datasets, scripts) are still in progress.

• Container technology such as Docker [50] is very
useful
for preparing and sharing stable software
releases. However, it hides the software chaos rather
than solving it,
lacks common APIs for research
projects, requires enormous amount of space, has
very poor support for embedded devices, and does
not yet help integrate models with existing projects,
legacy systems and user data.

• PapersWithCode platform [24] helps to ﬁnd relevant
research code for machine learning papers and
keeps track of state-of-the-art ML research using

3

public scoreboards with non-validated experimental
results from papers.
However, my experience
reproducing research papers suggests that sharing
ad-hoc research code is not enough to make research
techniques reproducible, customizable, portable and
trustable [42].

While testing all these useful tools and analyzing
the Jupyter notebooks, Docker images and GitHub
repositories shared alongside research papers, I started
thinking that it would be possible to reorganize them
as some sort of database of reusable components with
a common API, command line, web interface and meta
description. We could then reuse artifacts and some
common automation actions across diﬀerent projects
while applying DevOps principles to research projects.

Furthermore, we could also gradually abstract and
interconnect all existing tools rather than rewriting or
in turn, helped to create
substituting them. This,
”plug and play” workﬂows that could automatically
connect compatible components from diﬀerent users and
vendors (models, datasets, frameworks, compilers, tools,
platforms) while minimising manual
interventions and
providing a common interface for all shared research
projects.

I called my project Collective Knowledge (CK)
because my goal was
researchers and
practitioners in order to share their knowledge, best
practices and research results in the form of reusable
components, portable workﬂows, and automation actions
with common APIs and meta descriptions.

to connect

3 CK framework and an open CK

format

I have developed the CK framework as a small Python
library with minimal dependencies to be very portable
while oﬀering the possibility of being re-implemented
in other languages such as C, C++, Java and Go.
The CK framework has a uniﬁed CLI, a Python
API and a JSON-based web service to manage CK
repositories and add, ﬁnd, update, delete, rename and
move CK components (sometimes called CK entries or
CK data) [9].

CK repositories are human-readable databases of
reusable CK components that can be created in any local
directory and inside containers, pulled from GitHub and
similar services, and shared as standard archive ﬁles [7].
CK components simply wrap (encapsulate) user artifacts
and provide an extensible JSON meta description with
common automation actions [6] for related artifacts.

Automation actions are

implemented using CK
modules, which are Python modules with functions
exposed in a uniﬁed way via CK API and CLI and which
use dictionaries for input and output (extensible and
uniﬁed CK input/output (I/O)). The use of dictionaries

Figure 2: Organizing software projects as a human-readable database of reusable components that wrap artifacts
and provide uniﬁed APIs, CLI, meta descriptions and common automation actions for related artifacts. The CK
APIs and control ﬁles are highlighted in blue.

makes it easier to support continuous integration tools
and web services and to extend the functionality while
keeping backward compatibility. The uniﬁed I/O also
makes it possible to reuse such actions across projects
and to chain them together into uniﬁed CK pipelines and
workﬂows.

As I wanted CK to be non-intrusive and technology
neutral,
I decided to use a simple 2-level directory
structure to wrap user artifacts into CK components
as shown in Figure 2.
The root directory of the
CK repository contains the .ckr.json ﬁle to describe
the repository and specify dependencies on other CK
repositories to explicitly reuse their components and
automation actions.

user-friendly names have changed over time (CK alias).
CK modules are always stored in module / <CK module
name > directories in the CK repository. CK components
are stored in <CK module name >/ <CK data name
> directories. Each CK component has a .cm directory
with the meta.json ﬁle describing a given artifact and
info.json ﬁle to keep the provenance of a given artifact
including copyrights, licenses, creation date, names of all
contributors, and so on.

The CK framework has an internal default CK
repository with stable CK modules and the most
commonly used automation actions across many research
projects. When the CK framework is used for the ﬁrst
time, it also creates a local CK repository in the user
space to be used as a scratch pad.

CK uses .cm directories (Collective Meta) similar to
.git to store meta information of all components as well
as Unique IDs to be able to ﬁnd components even if their

After discussing the CK CLI with colleagues, I decided
to implement it in a similar way to natural language to
make it easier for users to remember the commands:

ck <a c t i o n > <CK module name> ( f l a g s )
ck <a c t i o n > <CK module name>:<CK data name> ( f l a g s )
ck <a c t i o n > <CK r e p o s i t o r y name>:<CK module name>:<CK data name>

( @input . j s o n o r @input . yaml )

The next example demonstrates how to compile and
run the shared automotive benchmark on any platform,
and then create a copy of the CK program component:

4

1) GitHub repo or archivefile.ckr.json/.cm/alias-a-dataset/.cm/alias-a-module/.cm/alias-a-program/.cm/alias-a-paper/dataset/.cm/alias-a-images/dataset/images/1.png/.cm/meta.json/.cm/info.json/program/.cm/alias-a-detect-edges/program/detect-edges/program.cppMakefilerun.shcheck-output.shautotune.sh/.cm/meta.json/.cm/info.json/paper/.cm/alias-a-report/paper/report/pldi.tex/.cm/meta.json/.cm/info.json/module/.cm/alias-a-program/module/program/module.py/.cm/meta.json/.cm/info.json2) User home directory$HOME/project/2000-forgot-everything/.ckr.json/.cm/alias-a-dataset/.cm/alias-a-program/.cm/alias-a-experiment/.cm/alias-a-paper/dataset/images-2000/2.png/.cm/meta.json/program/crazy-algorithm/source.cppbuild.bat/.cm/meta.json/experiment/autotuning/many.logs/lots-of-stats.sql/.cm/meta.json/paper/asplos/source.tex/.cm/meta.json      3) Jupyter/colabnotebookimport matplotlib.pyplotas pltimport pandasimport numpy...import ck.kernel as ck# We can now access all our software projects as a databaser=ck.access({‘action’:’search’, ‘module_uoa’:’dataset’,‘add_meta’:’yes’})if r[‘return’]>0: ck.err(r)list_of_all_ck_entries=r[‘lst’]for ck_entry in list_of_ck_entries:# CK will find all dataset entries in all CK-compatible projects,#  even old ones –you don’t need to remember #  the project structure. Furthermore, you can continue#reusing project even if students or engineers leave!image=ck_entry[‘path’]+ck_entry[‘meta’][‘image_filename’]…# Call reusable CK automation action to extract featuresfeatures=ck.access({‘action’:’get_features’,‘module_uoa’:’dataset’,‘image’:image})if features[‘return’]>0: ck.err(features)4) Docker imageInstall stable OS and  packagesSet environmentUse familiar CK API/CLIto run experiments inside or outside your VMMove data outside VM in the CK format to continue processing it via CK!$ckpull repo:ck-crowdtuning$ ckadd repo:2000-forgot-everything$ cklsdataset:image*dataset:imagesdataset:images-2000$ ckls programprogram:detect-edgesprogram:object-classification$ ckcompile program:detect-edges --speedDetecting compilers on your system…1) LLVM 10.0.12) GCC 8.13) GCC 9.34) ICC 19.1$ ckrun program:detect-edgesSearching for datasets ...Select dataset:1)images2)images-2000$ ckbenchmark program:detect-edges--record --ecord_uoa=autotuning...$ ckreproduce experiment:autotuning...p i p i n s t a l l ck

ck p u l l

r e p o −−u r l=h t t p s : / / g i t h u b . com/ c t u n i n g / ck−c r o w d t u n i n g

ck s e a r c h d a t a s e t −−t a g s=j p e g

ck s e a r c h program : cbench−automotive −∗

ck f i n d program : cbench−automotive−s u s a n

ck l o a d program : cbench−automotive−s u s a n

ck h e l p program

ck c o m p i l e program : cbench−automotive−s u s a n −−s p e e d
ck run program : cbench−automotive−s u s a n −−env .OMP NUM THREADS=4

ck run program −−h e l p

ck cp program : cbench−automotive−s u s a n l o c a l : program : new−program−w o r k fl o w
ck f i n d program : new−program−w o r k fl o w

ck benchmark program : new−program−w o r k f l o w −−r e c o r d −−r e c o r d u o a=my−t e s t
ck r e p l a y e x p e r i m e n t : my−t e s t

The CK program module describes dependencies on
software detection plugins and meta packages using

simple tags and version ranges that the community has
to agree on:

{

” c o m p i l e r ” : {

”name ” : ”C++ c o m p i l e r ” ,
” s o r t ” : 1 0 ,
” t a g s ” : ” c o m p i l e r , lan g −cpp ”

} ,
” l i b r a r y ” : {

”name ” : ” TensorFlow C++ API ” ,
” s o r t ” : 2 0 ,
” v e r s i o n f r o m ” :
” v e r s i o n t o ” :
” n o t a g s ” : ” t e n s o r f l o w − l i t e ” ,
” t a g s ” : ” l i b , t e n s o r f l o w , v s t a t i c ”

[ 1 , 1 3 , 1 ] ,

[ 2 , 0 , 0 ] ,

}

}

I also implemented a simple access function in the CK
Python API to access all the CK functionality in a very

simple and uniﬁed way:

im po rt ck . k e r n e l a s ck

# E q u i v a l e n t o f ” ck c o m p i l e program : cbench−automotive−s u s a n −−s p e e d ”
r=ck . a c c e s s ( { ’ a c t i o n ’ : ’ c o m p i l e ’ ,

’ module uoa ’ : ’ program ’ ,
’ s p e e d ’ : ’ y e s ’ } )

’ d a t a u o a ’ : ’ cbench−automotive−s u s a n ’ ,

i f

r [ ’ r e t u r n ’ ] > 0:

r e t u r n r # u n i f i e d e r r o r h a n d l i n g

p r i n t

( r )

# E q u i v a l e n t o f ” ck run program : cbench−automotive−s u s a n −−env .OMP NUM THREADS=4
r=ck . a c c e s s ( { ’ a c t i o n ’ : ’ run ’ ,

’ module uoa ’ : ’ program ’ ,
’ env ’ : { ’OMP NUM THREADS ’ : 4 } } )

’ d a t a u o a ’ : ’ cbench−automotive−s u s a n ’ ,

i f

r [ ’ r e t u r n ’ ] > 0:

r e t u r n r # u n i f i e d e r r o r h a n d l i n g

p r i n t

( r )

5

researchers and engineers

Such an approach allowed me to connect colleagues,
students,
from diﬀerent
workgroups to implement, share and reuse automation
actions and CK components rather than reimplementing
the Collective
them from scratch.
Knowledge
so-called FAIR
concept
principles to make data ﬁndable, accessible, interoperable
and reusable [55] while also extending it to code and other
research artifacts.

Furthermore,
supported the

4 CK components and workﬂows
to automate machine learning
and systems research

One of the biggest challenges I have faced throughout
my research career automating the co-design process of
eﬃcient and self-optimizing computing systems has been
how to deal with rapidly evolving hardware, models,
datasets, compilers and research techniques.
It is for
this reason that my ﬁrst use case for the CK framework
was to work with colleagues to collaboratively solve these
problems and thus enable trustable, reliable and eﬃcient
computational systems that can be easily deployed and
used in the real world.

We started using CK as a ﬂexible playground to
decompose complex computational systems into reusable,
customizable and non-virtualized CK components while
agreeing on their APIs and meta descriptions. As the ﬁrst
step, I implemented basic actions that could automate
the most common R&D tasks that I encountered during
artifact evaluation and in my own research on systems
and machine learning [42]. I then shared the automation
actions to analyze platforms and user environments
in a uniﬁed way, detect already installed code, data
and ML models (CK software detection plugins [32]),
and automatically download,
install and cross-compile
missing packages (CK meta packages [31]). At the same
time, I provided initial support for diﬀerent compilers,
operating systems (Linux, Windows, MacOS, Android)
and hardware from diﬀerent vendors including Intel,
Nvidia, Arm, Xilinx, AMD, Qualcomm, Apple, Samsung
and Google.

Such an approach allowed my collaborators [12] to
create, share and reuse diﬀerent CK components with
uniﬁed APIs to detect, install and use diﬀerent AI and
ML frameworks, libraries, tools, compilers, models and
datasets. The uniﬁed automation actions, APIs and
JSON meta descriptions of all CK components also helped
us to connect them into platform-agnostic, portable
and customizable program pipelines (workﬂows) with
a common interface across all research projects while
applying the DevOps methodology.

Such ”plug&play” workﬂows [45] can automatically
adapt to evolving environments, models, datasets and
non-virtualized platforms by automatically detecting the

6

properties of a target platform, ﬁnding all required
dependencies and artifacts (code, data and models)
on a user platform with the help of CK software
detection plugins [32],
installing missing dependencies
using portable CK meta packages [31], building and
running code, and unifying and testing outputs [25].
Moreover, rather than substituting or competing with
existing tools, the CK approach helped to abstract and
interconnect them in a relatively simple and non-intrusive
way.

CK also helps to protect user workﬂows whenever
external ﬁles or packages are broken, disappear or move
to another URL because it is possible to ﬁx such issues
in a shared CK meta package without changing existing
workﬂows. For example, our users have already taken
advantage of this functionality when the Eigen library
moved from BitBucket to GitLab and when the old
ImageNet dataset was no longer supported but could
still be downloaded via BitTorrent and other peer-to-peer
services.

Modular CK workﬂows can help to keep track of the
information ﬂow within such workﬂows, gradually expose
conﬁguration and optimization parameters as vectors via
dictionary-based I/O, and combine public and private
code and data. They also help to monitor, model
and auto-tune system behaviour, retarget research code
and machine learning models to diﬀerent platforms from
data centers to edge devices, integrate them with legacy
systems, and reproduce results.

Furthermore, we can use CK workﬂows inside standard
containers such as Docker while providing a uniﬁed
CK API to customize,
rebuild and adapt them to
any platform (Adaptive CK container ) [2] thus making
research techniques truly portable,
reproducible and
reusable. I envision that such adaptive CK containers and
portable workﬂows can complement existing marketplaces
to deliver portable, customizable, trustable and eﬃcient
AI and ML solutions that can be continuously optimized
libraries,
across diverse models, datasets,
compilers and run-time systems.

frameworks,

systems

In spite of some initial doubts, my collaborative CK
approach has worked well to decompose complex research
projects and computational
into reusable
components while agreeing on common APIs and meta
descriptions: the CK functionality evolved from just a
few core CK modules and abstractions to hundreds of CK
modules [30] and thousands of reusable CK components
and workﬂows [33] to automate the most repetitive
research tasks particularly for AI, ML and systems R&D
as shown in Figure 3.

Figure 3: Collective Knowledge framework provided a ﬂexible playground for researchers and practitioners to
decompose complex computational systems into reusable components while agreeing on APIs and meta descriptions.
Over the past 5 years, the Collective Knowledge project has grown to thousands of reusable CK components and
automation actions.

5 CK use cases

5.1 Unifying benchmarking, auto-tuning

and machine learning

The MILEPOST and cTuning projects were like the
Apollo mission:
on the one hand, we managed to
demonstrate that it was indeed possible to crowdsource
learning across multiple
auto-tuning and machine
users to automatically co-design eﬃcient software and
hardware [41, 44]. On the other hand, we exposed so
many issues when using machine learning for system
optimization in the real world that I had to stop this
research and have since focused on solving a large number
of related engineering problems.

For this reason, my ﬁrst step was to test the CK concept
by converting all artifacts and automation actions from
all my past research projects related to self-optimizing
computer systems into reusable CK components and
workﬂows.
I shared them with the community in
the CK-compatible Git repositories [7] and started
reproducing experiments from my own or related research
projects [20].

I then implemented a customizable and portable
program pipeline as a CK module to unify benchmarking
and auto-tuning while supporting all research techniques
and experiments
from my PhD research and the
MILEPOST project [10]. Such a pipeline could perform
compiler auto-tuning and software/hardware co-design
combined with machine learning in a uniﬁed way across
diﬀerent programs, datasets, frameworks, compilers, ML
models and platforms as shown in Figure 4.

The CK program pipeline helps users to gradually
and optimization
expose diﬀerent design choices
(models,
parameters
frameworks, compilers,
run-time systems, hardware)
via uniﬁed CK APIs and meta descriptions and thus
It also
enable the whole ML and system auto-tuning.

from all CK components

helps users to keep track of all
information passed
between components in complex computational systems
to ensure the reproducibility of results while ﬁnding the
most eﬃcient conﬁguration on a Pareto frontier in terms
of speed, accuracy, energy and other characteristics
also exposed via uniﬁed CK APIs. More importantly,
it can be now reused and extended in other real-world
projects [12].

5.2 Bridging the growing gap between
education, research and practice

During the MILEPOST project I noticed how diﬃcult it
is to start using research techniques in the real world.
New software, hardware, datasets and models are usually
available at the end of such research projects making
it very challenging, time-consuming and costly to make
research software work with the latest systems or legacy
technology.

That prompted us to organize a proof-of-concept
foundation to check
project with the Raspberry Pi
if it was possible to use portable CK workﬂows and
components to enable sustainable research software that
can automatically adapt to rapidly evolving systems.
for
We also wanted to provide an open-source tool
students and researchers to help them share their code,
data and models as reusable, portable and customizable
workﬂows and artifacts - something now known as FAIR
principles [55].

For

this purpose, we decided to reuse the CK
program workﬂow to demonstrate that it was possible to
crowdsource compiler auto-tuning across any Raspberry
Pi device, with any environment and any version of
any compiler (GCC or LLVM) to automatically improve
the performance and code size of the most popular
RPi applications. CK helped to automate experiments,
collect performance numbers on live CK scoreboards
and automatically plug in CK components with various

7

20152020Figure 4: Portable, customizable and reusable program pipeline (workﬂow) assembled from the CK components
to unify benchmarking, auto-tuning and machine learning. It is possible to gradually expose design search space,
optimizations, features and the run-time state from all CK components to make it easier to unify performance
modelling and auto-tuning.

machine learning and predictive analytics techniques
including decision trees, nearest neighbour classiﬁers,
support vector machines (SVM) and deep learning, to
automatically learn the most eﬃcient optimizations [45].
With this project, we demonstrated that it was
possible to reuse portable CK workﬂows and let users
participate in collaborative auto-tuning (crowd-tuning)
on new systems while sharing best optimizations and
unexpected behaviour on public CK scoreboards, even
after the project.

5.3 Co-designing eﬃcient software and
hardware for AI, ML and other
emerging workloads

While helping companies to assemble eﬃcient software
and hardware for image classiﬁcation, object detection
and other emerging AI and ML workloads, I noticed that
it can easily take several months to build an eﬃcient and
reliable system before moving it to production.

This process is so long and tedious because one
has to navigate a multitude of design decisions when
selecting components from diﬀerent vendors for diﬀerent
applications
object detection,
natural language processing (NLP), speech recognition
and many others) while trading oﬀ speed,
latency,
accuracy,
what network
architecture to deploy and how to customize it (ResNet,

energy and other costs:

classiﬁcation,

(image

MobileNet, GoogleNet, SqueezeNet, SSD, GNMT); what
framework to use (PyTorch vs. MXNet vs. TensorFlow
vs. TF Lite vs. Caﬀe vs. CNTK); what compilers to
use (XLA vs. nGraph vs. Glow vs. TVM); what libraries
and which optimizations to employ (ArmNN vs. MKL vs.
OpenBLAS vs. cuDNN). This is generally a consequence
of the target hardware platform (CPU vs. GPU vs. DSP
vs. FPGA vs. TPU vs. Edge TPU vs. numerous
accelerators). Worse still, this semi-manual process is
usually repeated from scratch for each new version of
hardware, models, frameworks, libraries and datasets.

My modular CK program pipeline shown in Figure 5
helped to automate this process. We extended it
slightly to plug in diﬀerent AI and ML algorithms,
datasets, models, frameworks and libraries for diﬀerent
hardware such as CPU, GPU, DSP and TPU and
diﬀerent target platforms from servers to Android devices
and IoT [12]. We also customized this ML workﬂow
with the new CK plugins that performed pre- and
post-processing of diﬀerent models and datasets to make
them compatible with diﬀerent frameworks, backends
and hardware while unifying benchmarking results such
as throughput, latency, mAP (mean Average Precision),
recall and other characteristics. We also exposed diﬀerent
design and optimization parameters including model
topology, batch sizes, hardware frequency, compiler ﬂags
and so on.

Eventually, CK allowed us

to

automate

and

8

Choose optimization strategyPerform SW/HW DSE (skeletontransforms, hardware config, compiler flags, model topology and params…)Perform stat. analysisDetect (Pareto)frontierModel behavior, predict optimizationsReduce complexityCK program module with pipeline functionCompile  programRun codeCK module (code, data, and model wrappers) with a unified I/OCK input (JSON/dict)CK output (JSON/dict)Unified inputBehaviorChoicesFeaturesStateActionUnified outputBehaviorChoicesFeaturesStateb       = B( c      , f       , s       )  …………Model behavior of a computational system as a function of all choices (ML/SW/HW), features,and run-time state:Flatteninput into vectorsto unify auto-tuning, statistical analysis, and machine learningFrameworks, tools, datasets, modelsGenerated filesSet  environment for a given versionAutomationaction(Python function)Figure 5: The use of the CK framework to automate benchmarking, optimization and co-design of eﬃcient software
and hardware for machine learning and artiﬁcial intelligence. The goal is to make it easier to reproduce, reuse, adopt
and build upon ML and systems research.

space

design

results

exploration

(DSE)
stacks and distribute

of
systematise
AI/ML/software/hardware
it
across diverse platforms and environments [8]. This
is possible because CK automatically detects all
necessary dependencies on any platform, installs and/or
rebuilds the prerequisites, runs experiments, and records
all
together with the complete experiment
conﬁguration (resolved dependencies and their versions,
environment variables, optimization parameters and so
on) in a uniﬁed JSON format inside CK repositories.
CK also ensured the reproducibility of results while
making it easier to analyze and visualize results locally
using Jupyter notebooks and standard toolsets or
within workgroups using universal CK dashboards also
implemented as CK modules [20].

Note that it is also possible to share the entire
experimental setup in the CK format inside Docker
containers, thus automating all the DSE steps using the
uniﬁed CK API instead of trying to ﬁgure them out
from the ReadMe ﬁles. This method enables CK-powered
adaptive containers that help users to start using and
customizing research techniques across diverse software
and hardware from servers to mobile devices in just
a few simple steps while sharing experimental results
within workgroups or along research papers in the CK
format, reproducing and comparing experiments, and
even automatically reporting unexpected behaviour such
as bugs and mispredictions [35].

Eventually,

I managed to substitute my original
cTuning framework completely with the modular,

9

portable, customizable and reproducible experimental
framework while addressing most of the engineering and
reproducibility issues exposed by the MILEPOST and
cTuning projects [41]. It also helped me to return to my
original research on lifelong benchmarking, optimization
and co-design of eﬃcient software and hardware for
emerging workloads,
including machine learning and
artiﬁcial intelligence.

5.4 Automating MLPerf and enabling

portable MLOps

The modularity of my portable CK program workﬂow
helped to enable portable MLOps when combined with
AI and ML components, FAIR principles and the
DevOps methodology. For example, my CK workﬂows
and components were reused and extended by General
Motors and dividiti to collaboratively benchmark and
optimize deep learning implementations across diverse
devices from servers to the edge [11]. They were also
used by Amazon to enable scaling of deep learning
on AWS using C5 instances with MXNet, TensorFlow
Finally, the CK framework made
and BigDL [49].
it easier to prepare,
submit and reproduce MLPerf
inference benchmark results (”fair and useful benchmark
for measuring training and inference performance of
ML hardware, software, and services”) [53, 23]. These
results are aggregated in the public optimization
repository [20] to help users ﬁnd and compare diﬀerent
AI/ML/software/hardware stacks in terms of speed,

Research papersAutomation actions (tasks)Unified CK APICLI ;Python/C/C++/Java ; REST…CK module(Python implementation of a given action)JSON inputEnvironmentDependenciesParametersJSON outputResultsCharacteristicsFeaturesJSON describing code,data and dependenciescompatible with this actionExtensible metadata…Reusable CK componentsCK dependencies describe compatible versions of code, data and modelsfor a given algorithm and a given target platform. CK actions can then automatically detect or download & install all the necessary dependenciesfor a given target.ckpull repo:ck-mlperfCK componentUnified API and JSON IOckdetect soft --tags=lib,tensorflowckinstall package –tags=imagenetckrun program:object-detection-onnx-pyckautotuneprogramckreproduceexperiment…Portablefile-basedCK solutions&workflowsaccuracy, power consumption, costs and other collected
metrics.

• Models: MobileNets, ResNet-18, ResNet-50,

Inception-v3, VGG16, AlexNet, SSD.

5.5 Enabling reproducible papers with
and reusable

portable workﬂows
artifacts

Ever since my very ﬁrst research project, I wanted to be
able to easily ﬁnd all artifacts (code, data, models) from
research papers, reproduce and compare results in just a
few clicks, and immediately test research techniques in
the real world with diﬀerent platforms, environments and
data.
It is for that reason that one of my main goals
when designing the CK framework was to use portable
CK workﬂows for these purposes.

I had the opportunity to test my CK approach
when co-organizing the ﬁrst reproducible tournament
at the ACM ASPLOS’18 conference (the International
Conference on Architectural Support for Programming
Languages and Operating Systems). The tournament
required participants to co-design Pareto-eﬃcient systems
for deep learning in terms of speed, accuracy, energy,
costs and other metrics [29]. We wanted to extend
existing optimization competitions,
tournaments and
hackathons including Kaggle [19], ImageNet [18], the
Low-Power Image Recognition Challenge (LPIRC) [21],
DAWNBench (an end-to-end deep learning benchmark
and competition) [17], and MLPerf
[53, 22] with a
customizable experimental framework for collaborative
and reproducible optimization of Pareto-eﬃcient software
and hardware stack for deep learning and other emerging
workloads.

This tournament helped to validate my CK approach
for reproducible papers. The community submitted
ﬁve complete implementations (code, data, scripts, etc.)
for the popular ImageNet object classiﬁcation challenge.
We then collaborated with the authors to convert their
artifacts into the CK format, evaluate the converted
artifacts on the original or similar platforms, and
reproduce the results based on the rigorous artifact
evaluation methodology [5]. The evaluation metrics
included accuracy on the ImageNet validation set (50,000
images), latency (seconds per image), throughput (images
per second), platform price (dollars) and peak power
consumption (Watts).
Since collapsing all metrics
into one to select a single winner often results in
over-engineered solutions, we decided to aggregate all
reproduced results on a universal CK scoreboard shown
in Figure 6 and let users select multiple implementations
from a Pareto frontier, based on their requirements and
constraints.

We then published all ﬁve papers with our uniﬁed
artifact appendix [4] and a set of ACM reproducibility
badges in the ACM Digital Library [38], accompanied
by adaptive CK containers
(CK-powered Docker)
and portable CK workﬂows covering a very diverse
model/software/hardware stack:

• Data types:

8-bit integer, 16-bit ﬂoating-point

(half), 32-bit ﬂoating-point (ﬂoat).

• AI

frameworks

MXNet,
TensorFlow, Caﬀe, Keras, Arm Compute Library,
cuDNN, TVM, NNVM.

libraries:

and

• Platforms: Xilinx Pynq-Z1 FPGA, Arm Cortex
CPUs and Arm Mali GPGPUs (Linaro HiKey960
and T-Fireﬂy RK3399), a farm of Raspberry Pi
devices, NVIDIA Jetson TX1 and TX2, and Intel
Xeon servers in Amazon Web Services, Google Cloud
and Microsoft Azure.

The reproduced results also exhibited great diversity:

• Latency: 4 .. 500 milliseconds per image

• Throughput: 2 .. 465 images per second

• Top 1 accuracy: 41 .. 75 percent

• Top 5 accuracy: 65 .. 93 percent

• Model size (pre-trained weights): 2 ..

130

megabytes

• Peak power consumption: 2.5 .. 180 Watts

• Device frequency: 100 .. 2600 megahertz

• Device cost: 40 .. 1200 dollars

• Cloud usage cost: 2.6E-6 .. 9.5E-6 dollars per

inference

The

community can now access all

the above
CK workﬂows under permissive licenses and continue
collaborating on them via dedicated GitHub projects with
CK repositories. These workﬂows can be automatically
adapted to new platforms and environments by either
detecting already installed dependencies (frameworks,
libraries, datasets) or rebuilding dependencies using CK
meta packages supporting Linux, Windows, MacOS and
Android. They can be also extended to expose new
design and optimization choices such as quantization,
as well as evaluation metrics such as power or memory
consumption. We also used these CK workﬂows to
crowdsource the design space exploration across devices
provided by volunteers such as mobile phones, laptops
and servers with the best solutions aggregated on live
CK scoreboards [20].

After validating that my portable CK program
workﬂow can support reproducible papers for deep
learning systems, I decided to conduct one more test
to check if CK could also support quantum computing
R&D. Quantum computers have the potential to solve
certain problems dramatically faster than conventional
computers, with applications in areas such as machine

10

Figure 6: The CK dashboard with reproduced results from the ACM ASPLOS-REQUEST’18 tournament to co-design
Pareto-eﬃcient image classiﬁcation in terms of speed, accuracy, energy and other costs. Each paper submission was
accompanied by the portable CK workﬂow to be able to reproduce and compare results in a uniﬁed way.

learning, drug discovery, materials, optimization, ﬁnance
and cryptography. However it is not yet known when
the ﬁrst demonstration of quantum advantage will be
achieved, or what shape it will take.

the participants were able to start running quantum
experiments with a standard CK command:

r e p o −−u r l=h t t p s : / / g i t h u b . com/ c t u n i n g / ck−q i s k i t
ck p u l l
ck run program : q i s k i t −demo −−cmd key=q u a n t u m c o i n f l i p

the

about

state-of-the-art

This led me to co-organize several Quantum hackathons
similar to the REQUEST tournament [26] with IBM,
Rigetti, Riverlane and dividiti. My main goal was to
check if we could aggregate and share multidisciplinary
knowledge
in quantum
computing using portable CK workﬂows that can run
on classical hardware and quantum platforms from IBM,
Rigetti and other companies, can be connected to a public
dashboard to simplify reproducibility and comparison of
diﬀerent algorithms across diﬀerent platforms, and can be
extended by the community even after hackathons.
from one of

such
Quantum hackathons where over 80 participants, from
undergraduate and graduate students to startup founders
and experienced professionals from IBM and CERN,
worked together to solve a quantum machine learning
problem designed by Riverlane. All participants were
given some labelled quantum data and asked to develop
algorithms for solving a classiﬁcation problem.

Figure 7 shows

results

the

in a

reproducible

We also taught participants how to perform these
experiments
and
collaborative,
automated way using the CK framework so that the
results could be transfered to industry. For example,
we introduced the CK repository with workﬂows and
components for the Quantum Information Science Kit
(QISKit) - an open source software development kit
(SDK) for working IBM Q quantum processors [13].
Using the CK program workﬂow from this repository,

Whenever ready, the participants could submit their
solutions to the public CK dashboards to let other users
validate and reuse their results [20, 26].

the

Following

successful

validation of portable
CK workﬂows for reproducible papers,
I continued
collaborating with ACM [1] and ML and systems
conferences to automate the tedious artifact evaluation
process [5, 42]. For example, we developed several CK
workﬂows to support the Student Cluster Competition
Reproducibility Challenge (SCC) at the Supercomputing
conference [15]. We demonstrated that it was possible
to reuse the CK program workﬂow to automate the
installation, execution, customization and validation of
the SeisSol application (Extreme Scale Multi-Physics
Sumatra
the Tsunamigenic
of
Simulations
2004
Megathrust Earthquake) [54]
from the SC18 Student
Cluster Competition Reproducibility Challenge across
several supercomputers and HPC clusters [14]. We
also showed that it was possible to abstract HPC job
managers including Slurm and Flux and connect them
with our portable CK workﬂows.

Some authors already started using CK to share their
research research artifacts and workﬂows at diﬀerent ML
and systems conferences during artifact evaluation [28].
My current goal
is to make the CK onboarding as
simple as possible and help researchers to automatically
convert their ad-hoc artifacts and scripts into CK
workﬂows, reusable artifacts, adaptive containers and live

11

CK workflow1with validated resultsAWS with c5.18xlarge instance; Intel® Xeon® Platinum 8124MFrom the authors:“The 8-bit optimized model is automatically generated with a calibration process from FP32 model without the need of fine-tuning or retraining. We show that the inference throughput and latency with ResNet-50, Inception-v3 and SSD are improved by 1.38X-2.9X and 1.35X-3X respectively with negligible accuracy loss from IntelCaffeFP32 baseline and by 56X-75X and 26X-37X from BVLC Caffe.”https://github.com/ctuning/ck-request-asplos18-caffe-intelFigure 7: The CK dashboard connected with portable CK workﬂows to visualize and compare public results from
reproducible Quantum Hackathons. Over 80 participants worked together to solve a quantum machine learning
problem and minimise time to solution.

dashboards.

5.6 Connecting

and
practitioners to co-design eﬃcient
computational systems

researchers

explain what was happening inside complex and ”black
box” computational systems, integrate them with legacy
systems, use them inside ”adaptive” Docker, and share
them along with published papers while applying the
DevOps methodology and agile principles in scientiﬁc
research.

CK use cases demonstrated that it was possible to develop
and use a common research infrastructure with diﬀerent
levels of abstraction to bridge the gap between researchers
and practitioners and help them to collaboratively
co-design eﬃcient computational systems.
Scientists
could then work with a higher-level abstraction while
allowing engineers to continue improving the lower-level
abstractions
for continuously evolving software and
hardware in deploying new techniques in production
without waiting for each other, as shown in Figure 8.
Furthermore, the uniﬁed interfaces and meta descriptions
of all CK components and workﬂows made it possible to

6 CK platform

The practical use of CK as a portable and customizable
workﬂow framework in multiple academic and industrial
projects exposed several limitations:

• The distributed nature of the CK technology, the
lack of a centralized place to keep all CK components,
automation actions and workﬂows, and the lack
of a convenient GUI made it very challenging to
keep track of all contributions from the community.

12

The most efficient designFigure 8: The CK concept helps to connect researchers and practitioners to co-design complex computational systems
using DevOps principles while automatically adapting to continuously evolving software, hardware, models and
datasets. The CK framework and public dashboards also help to unify, automate and crowdsource the benchmarking
and auto-tuning process across diverse components from diﬀerent vendors to automatically ﬁnd the most eﬃcient
systems on the Pareto frontier.

As a result,
it is not easy to discuss and test
APIs, add new components and assemble workﬂows,
automatically validate them across diverse platforms
and environments, and connect them with legacy
systems.

• The concept of backward compatibility of CK
APIs and the lack of versioning similar to Java
made it challenging to keep stable and bug-free
workﬂows in the real world: any bug in a reusable
CK component from one GitHub project could
easily break dependent workﬂows in another GitHub
project.

• The CK command-line interface with the access to all
automation actions with numerous parameters was
too low-level for researchers. This is similar to the
situation with Git, a powerful but quite complex and
CLI-based tool that requires extra web services such
as GitHub and GitLab to make it more user-friendly.

This feedback from CK users motivated me to start
developing cKnowledge.io (Figure 9), an open web-based
platform with a GUI to aggregate, version and test all
CK components and portable workﬂows. I also wanted
to replace aging optimization repository at cTuning.org
with an extensible and modular platform to crowdsource
and reproduce tedious experiments such as benchmarking
and co-design of eﬃcient systems for AI and ML across
diverse platforms and data provided by volunteers.

The CK platform is inspired by GitHub and PyPI:
I see it as a collaborative platform to share reusable
automation actions for repetitive research tasks and
assemble portable workﬂows.
It also includes the
open-source CK client [16] that provides a common
API to initialise, build,
run and validate diﬀerent
research projects based on a simple JSON or YAML
manifest. This client is connected with live scoreboards
on the CK platform to collaboratively reproduce and
compare the state-of-the-art
research results during
artifact evaluation that we helped to organize at ML and
Systems conferences [20].

existing

My intention is to use the CK platform to complement
the ACM Digital Library,
and enhance MLPerf,
PapersWithCode.com,
reproducibility
and
initiatives and artifact evaluation at ACM, IEEE and
NeurIPS conferences with the help of CK-powered
adaptive
reusable
components, ”live” papers and reproducible results
validated by the community using realistic data across
diverse models, software and hardware.

portable workﬂows,

containers,

7 CK
and
benchmarking

demo:
customizing

automating
AI/ML

I prepared a live and interactive demonstration of
the CK solution that automates the MLPerf inference

13

Develop new softwarePublish new algorithms and modelsPublish new algorithms and modelsReported accuracy and other resultsDesign new hardwareResearchers using CKEngineers using CKAdaptive CK containerReal dataAttempt to reproduce resultsIncrementPractitioners using CKShare portable Collective Knowledge solutionto automatically build a system using the latest or stable software and hardwareUse standard data setAutotune to balance all characteristics depending on use case (cloud vs edge)Real accuracyReal speedSizeCostsAutomatically rebuild systemTimelineFigure 9: cKnowledge.io: an open platform to organize scientiﬁc knowledge in the form of portable workﬂows, reusable
components and reproducible research results. It already contains many automation actions and components needed
to co-design eﬃcient and self-optimizing computational systems, enable reproducible and live papers validated by
the community, and keep track of the state-of-the-art research techniques that can be deployed in production.

connects

it with the

benchmark [53],
live CK
dashboard (public optimization repository) [20], and
helps volunteers to crowdsource benchmarking and design
space exploration across diverse platforms similar to the
Collective Tuning Initiative [41] and the SETI@home
project: cKnowledge.io/test.

This demonstration shows how to use a uniﬁed CK API
to automatically build, run and validate object detection
based on SSD-Mobilenet, TensorFlow and COCO dataset
across Raspberry Pi computers, Android phones, laptops,
desktops and data centers. This solution is based on a
simple JSON ﬁle describing the following tasks and their
dependencies on CK components:

• prepare a Python virtual environment (can be

skipped for the native installation)

• download and install the Coco dataset (50 or 5000

images)

• detect C++ compilers or Python interpreters needed

for object detection

• install Tensorﬂow framework with a speciﬁed version

for a given target machine

• download and install the SSD-MobileNet model

compatible with selected Tensorﬂow

• manage installation of all other dependencies and

libraries

• compile object detection for a given machine and

prepare pre/post-processing scripts

This solution was published on the cKnowledge.io
platform using the open-source CK client [16] to help
users participate in crowd-benchmarking on their own
machines as follows:

# I n s t a l l
p i p i n s t a l l cbench

t h e CK c l i e n t

from PyPi u s i n g :

# Download and b u i l d t h e s o l u t i o n on a g i v e n machine ( example f o r Linux ) :
cb i n i t demo−obj−d e t e c t i o n −coco−t f −cpu−benchmark−l i n u x −p o r t a b l e −w o r k f l o w s

# Run t h e s o l u t i o n on a g i v e n machine :
cb benchmark demo−obj−d e t e c t i o n −coco−t f −cpu−benchmark−l i n u x −p o r t a b l e −w o r k f l o w s

14

CK JSON APIObjectdetectionObject classificationSpeechrecognition…AlgorithmsTraining/inference…CK JSON APIMobileNetsResNetVGG…SqueezeDet…ModelsCK JSON APITensorFlowCaffeMXNet…PyTorch/Caffe2…SoftwareCK JSON APIImageNetKITTIVOC…Real data sets…Data setsCK JSON APICPUGPUTPU…NN accelerators…HardwareCK solutionsand adaptive containerswith reusable components, portable workflows, common APIs, and unified I/OAlgorithmsModelsDatasetsSoftwareHardwareCKhelps to run solutions on any tech. stackInitializeBuildRunValidateCK enables “live” research papers that can be validated by the community across diverse platforms, models and datasets: cKnowledge.io/reproduced-resultsUnified inputUnified outputML/AI paperscKnowledge.io platformSystems papersCK solutionsand adaptive containerscan be shared along with research papers to make it easier to reproduce the results and try the algorithms with the latest or different components: cKnowledge.io/solutionscKnowledge.io/c/dockerResults shared by volunteersOriginal results from the authorslatency,

accuracy

The users can then see the benchmarking results
exposed
and
(speed,
through the CK workﬂow) on the
characteristics
solution
live CK dashboard associated with this
and compare
oﬃcial MLPerf
results or with the results shared by other users:
cKnowledge.io/demo-result.

them against

other

the

After validating this solution on a given platform, the
users can also clone it and update the JSON description
to retarget this benchmark to other devices and operating

systems such as MacOS, Windows, Android phones,
servers with CUDA-enabled GPUs and so on.

The users have the possibility to integrate such ML
solutions with production systems with the help of uniﬁed
CK APIs as demonstrated by connecting the above CK
solution for object detection with the webcam in any
browser: cKnowledge.io/demo-solution.

Finally,

it is possible to use containers with CK

repositories, workﬂows and common APIs as follows:

run c t u n i n g / c b r a i n −obj−d e t e c t i o n −coco−t f −cpu−benchmark−l i n u x −p o r t a b l e −w o r k f l o w s \
d o c k e r
/ b i n / bash −c ” cb benchmark demo−obj−d e t e c t i o n −coco−t f −cpu−benchmark−l i n u x −p o r t a b l e −w o r k f l o w s

d o c k e r

run c t u n i n g / cbench−m l p e r f −i n f e r e n c e −v0 .5 − d e t e c t i o n −openvino−ssd−m o b i l e n e t −coco −500− l i n u x \
/ b i n / bash −c ” cb benchmark m l p e r f −i n f e r e n c e −v0 .5 − d e t e c t i o n −openvino−ssd−m o b i l e n e t −coco −500− l i n u x

Combining Docker and portable CK workﬂows enables
”adaptive” CK containers for MLPerf that can be easily
customized, rebuilt with diﬀerent ML models, datasets,
compilers, frameworks and tools encapsulated inside CK
components, and deployed in production [2].

8 Conclusions and future work

My very ﬁrst research project to prototype an analog
neural network stalled in the late 1990s because it took me
far too long to build from scratch all the infrastructure
to model and train Hopﬁeld neural networks, generate
diverse datasets, co-design and optimize software and
hardware, run and reproduce all experiments, compare
them with other techniques from published papers, and
use this technology in practice in a completely diﬀerent
environment.

In this article,

I explain why I have developed
the Collective Knowledge framework and how it can
help to address the issues above by organizing all
research projects as a database of reusable components,
portable workﬂows and reproducible experiments based
on FAIR principles (ﬁndable, accessible, interoperable,
and reusable).
I also describe how the CK framework
attempts to bring DevOps and ”Software 2.0” principles
to scientiﬁc research and to help users share and
reuse best practices, automation actions and research
artifacts in a uniﬁed way alongside reproducible papers.
I demonstrate how the CK concept helps
Finally,
to complement, unify and interconnect existing tools,
platforms, and reproducibility initiatives with common
APIs and extensible meta descriptions rather than
rewriting them or competing with them.

I present several use cases of how CK helps to connect
researchers and practitioners to collaboratively design
more reliable, reproducible and eﬃcient computational
systems for machine learning, artiﬁcial intelligence and
other emerging workloads that can automatically adapt

to continuously evolving software, hardware, models and
datasets.
I also describe the https://cKnowledge.io
platform that I have developed to organize knowledge
particularly about AI, ML, systems and other innovative
technology in the form of portable CK workﬂows,
automation actions, reusable artifacts and reproducible
results from research papers. My goal is to help the
community to ﬁnd useful methods from research papers,
quickly build them on any tech stack, integrate them with
new or legacy systems, start using them in the real world
with real data, and combine Collective Knowledge and AI
to build better AI systems.

Finally, I demonstrate the concept of ”live” research
papers connected with portable CK workﬂows and online
CK dashboards to let the community automatically
validate and update experimental results even after the
project, detect and share unexpected behaviour, and
ﬁx problems collaboratively [45].
I believe that such a
collaborative approach can make computational research
more reproducible, portable, sustainable, explainable and
trustable.

However, CK is still a proof-of-concept and there
remains a lot to simplify and improve. Future work
will
intend to make CK more user-friendly and to
simplify the onboarding process, as well as to standardise
all APIs and JSON meta descriptions.
It will also
focus on development of a simple GUI to create and
share automation actions and CK components, assemble
portable workﬂows, run experiments, compare research
techniques, generate adaptive containers, and participate
in lifelong AI, ML and systems optimization.

is

My long-term goal

to use CK to develop a
virtual playground, an optimization repository and a
marketplace where researchers and practitioners assemble
AI, ML and other novel applications similar to live
species that continue to evolve, self-optimize and compete
with each other across diverse tech stacks from diﬀerent
vendors and users. The winning solutions with the best

15

trade-oﬀs between speed, latency, accuracy, energy, size,
costs and other metrics can then be selected at any time
from the Pareto frontier based on user requirements and
constraints. Such solutions can be immediately deployed
in production on any platform from data centers to the
edge in the most eﬃcient way thus accelerating AI, ML
and systems innovation and digital transformation.

Acknowledgements

I thank Sam Ainsworth, Erik Altman, Lorena Barba,
Victor Bittorf, Unmesh D. Bordoloi, Steve Brierley,
Luis Ceze, Milind Chabbi, Bruce Childers, Nikolay
Chunosov, Marco Cianfriglia, Albert Cohen, Cody
Coleman, Chris Cummins, Jack Davidson, Alastair
Donaldson, Achi Dosanjh, Thibaut Dumontet, Debojyoti
Dutta, Daniil Efremov, Nicolas Essayan, Todd Gamblin,
Leo Gordon, Wayne Graves, Christophe Guillon, Herve
Guillou, Stephen Herbein, Michael Heroux, Patrick
Hesse, James Hetherignton, Kenneth Hoste, Robert
Hundt, Ivo Jimenez, Tom St. John, Timothy M. Jones,
David Kanter, Yuriy Kashnikov, Gaurav Kaul, Sergey
Kolesnikov, Shriram Krishnamurthi, Dan Laney, Andrei
Lascu, Hugh Leather, Wei Li, Anton Lokhmotov, Peter
Mattson, Thierry Moreau, Dewey Murdick, Luigi Nardi,
Cedric Nugteren, Michael O’Boyle, Ivan Ospiov, Bhavesh
Patel, Gennady Pekhimenko, Massimiliano Picone, Ed
Plowman, Ramesh Radhakrishnan, Ilya Rahkovsky, Vijay
Janapa Reddi, Vincent Rehm, Alka Roy, Shubhadeep
Roychowdhury, Dmitry Savenko, Aaron Smith, Jim
Spohrer, Michel Steuwer, Victoria Stodden, Robert
Stojnic, Michela Taufer, Stuart Taylor, Olivier Temam,
Eben Upton, Nicolas Vasilache, Flavio Vella, Davide Del
Vento, Boris Veytsman, Alex Wade, Pete Warden, Dave
Wilkinson, Matei Zaharia, Alex Zhigarev and other great
colleagues for interesting discussions, practical use cases
and useful feedback.

References

[1] ACM Pilot demo ”Collective Knowledge: packaging
and sharing”. https://youtu.be/DIkZxraTmGM.

[2] Adaptive CK containers with a uniﬁed CK API to
customize, rebuild, and adapt them to any platform
and environment as well as to integrate them with
external tools and data. https://cKnowledge.io/
c/docker.

[3] Amazon SageMaker: fully managed service to build,
train, and deploy machine learning models quickly.
https://aws.amazon.com/sagemaker.

[5] Artifact Evaluation:

from
published papers at machine learning and systems
conferences. https://cTuning.org/ae.

reproducing results

[6] Automation actions with a uniﬁed API and
JSON IO to share best practices and introduce
DevOps principles to scientiﬁc research.
https:
//cKnowledge.io/actions.

[7] CK-compatible GitHub, GitLab, and BitBucket
repositories with reusable components, automation
actions, and workﬂows. https://cKnowledge.io/
repos.

[8] CK dashboard with results

and multi-objective

from CK-powered,
space
automated
exploration of image classiﬁcation across diﬀerent
models,
https:
//cknowledge.io/ml-optimization-repository.

hardware.

software

design

and

[9] CK GitHub:

an open-source and cross-platform
framework to organize software projects as a
database of reusable components with common
automation actions, uniﬁed APIs, and extensible
meta descriptions based on FAIR principles
(ﬁndability,
and
reusability). https://github.com/ctuning/ck.

interoperability,

accessibility,

[10] CK repository
to
components
project.
reproduce-milepost-project.

with
and
the MILEPOST
reproduce
https://github.com/ctuning/

CK workﬂows

[11] CK use case from General Motors: Collaboratively
Benchmarking and Optimizing Deep Learning
Implementations.
https://youtu.be/
1ldgVZ64hEI.

[12] Collective Knowledge real-world use cases. https:

//cKnowledge.org/partners.

[13] Collective Knowledge Repository for the Quantum
https://

Information Software Kit (QISKit).
github.com/ctuning/ck-qiskit.

installation,

[14] Collective Knowledge Repository to automate
customization and
execution,
the
the SeisSol application from the
validation of
SC18 Student Cluster Competition Reproducibility
Challenge across diﬀerent platforms, environments
and datasets.
https://github.com/ctuning/
ck-scc18/wiki.

[15] Collective Knowledge workﬂow to prepare digital
artifacts
the Student Cluster Competition
Reproducibility Challenge. https://github.com/
ctuning/ck-scc.

for

[4] Artifact Appendix and reproducibility checklist to
unify and automate the validation of results at
systems and machine learning conferences. https:
//cTuning.org/ae/checklist.html.

[16] Cross-platform CK client

to unify preparation,
execution, and validation of research techniques
shared along with research papers.
https://
github.com/ctuning/cbench.

16

[17] DAWNBench:

an

end-to-end

benchmark and competition.
cs.stanford.edu/benchmark/.

deep
learning
https://dawn.

[32] Shared CK plugins to detect software (models,
frameworks, data sets, scripts) on a user machine.
https://cKnowledge.io/soft.

[18] Imagenet challenge (ILSVRC): Imagenet large scale
visual recognition challenge where software programs
compete to correctly classify and detect objects and
scenes. http://www.image-net.org.

[19] Kaggle:

platform for predictive modelling and
analytics competitions. https://www.kaggle.com.

[20] Live scoreboards with reproduced results
learning

at machine

from
and
https://cKnowledge.io/

papers

research
systems conferences.
reproduced-results.

[21] LPIRC:

low-power

image recognition challenge.

https://rebootingcomputing.ieee.org/lpirc.

[22] MLPerf: a broad ML benchmark suite for measuring
performance of ML software
frameworks, ML
hardware accelerators, and ML cloud platforms.
https://mlperf.org.

[23] MLPerf Inference Benchmark results.
mlperf.org/inference-results.

https://

[24] PapersWithCode.com provides a free and open
resource with Machine Learning papers, code and
evaluation tables. https://paperswithcode.com.

[25] Portable and reusable CK workﬂows.

https://

cKnowledge.io/programs.

[26] Quantum Collective Knowledge and reproducible
the
using
live CK dashboards, and
https://cknowledge.io/c/

quantum hackathons:
state-of-the-art
portable CK workﬂows,
reproducible results.
event/reproducible-quantum-hackathons/.

quantum computing

track of

keeping

in

[27] Reproduced papers with ACM badges based on
https://

the cTuning evaluation methodology.
cKnowledge.io/reproduced-papers.

[28] Reproduced papers with CK workﬂows during
Systems
https://cknowledge.io/?q=

artifact
conferences.
"reproduced-papers"AND"portable-workflow-ck".

evaluation

at ML

and

[29] ReQuEST: open tournaments on collaborative,
reproducible and Pareto-eﬃcient software/hardware
co-design of deep learning and other emerging
workloads.
https://cknowledge.io/c/event/
request-reproducible-benchmarking-tournament/.

[30] Shared and versioned CK modules.

https://

cKnowledge.io/modules.

[31] Shared CK meta packages (code, data, models)
with automated installation across diverse platforms.
https://cKnowledge.io/packages.

[33] The Collective Knowledge platform:

computational

organizing
intelligence, machine
knowledge about artiﬁcial
other
systems,
learning,
innovative technology in the form of portable
and reusable
workﬂows,
artifacts
https:
//cKnowledge.io.

automation actions,
from reproduced papers.

and

[34] The Machine Learning Toolkit
https://www.kubeflow.org.

for Kubernetes.

[35] The public database of

collaborative

image misclassiﬁcations
during
of ML
models across Android devices using the CK
framework.
http://cknowledge.org/repo/
web.php?wcid=model.image.classification:
collective_training_set.

benchmarking

[36] Peter Amstutz, Michael R. Crusoe, Nebojˇsa
Tijani´c, Brad Chapman, John Chilton, Michael
Heuer, Andrey Kartashov, Dan Leehr, Herv´e
M´enager, Maya Nedeljkovich, Matt Scales, Stian
Soiland-Reyes, and Luka Stojanovic.
Common
Workﬂow Language, v1.0. https://doi.org/10.
6084/m9.figshare.3115156.v2, 7 2016.

[37] T. Ben-Nun, M. Besta,

S. Huber, A. N.
Ziogas, D. Peter, and T. Hoeﬂer. A Modular
Benchmarking Infrastructure for High-Performance
and Reproducible Deep Learning. IEEE, May 2019.
The 33rd IEEE International Parallel & Distributed
Processing Symposium (IPDPS’19).

[38] Luis Ceze Ceze, Natalie Enright Jerger, Babak
Falsaﬁ, Grigori Fursin, Anton Lokhmotov, Thierry
Moreau, Adrian Sampson, and Phillip Stanley
Marbell.
ACM ReQuEST’18: Proceedings of
the 1st on Reproducible Quality-Eﬃcient Systems
Tournament on Co-Designing Pareto-Eﬃcient Deep
Learning. 2018.

[39] Cheng Li and Abdul Dakkak and Jinjun Xiong
and Wei Wei and Lingjie Xu and Wen-mei
Hwu. Across-Stack Proﬁling and Characterization
of Machine Learning Models on GPUs.
https:
//arxiv.org/abs/1908.06869, 2019.

[40] Bruce R. Childers, Grigori Fursin,

Shriram
Krishnamurthi, and Andreas Zeller.
Artifact
Evaluation for Publications (Dagstuhl Perspectives
Workshop 15452). Dagstuhl Reports, 5(11):29–35,
2016.

[41] Grigori Fursin.

Collective Tuning Initiative:
automating and accelerating development and
optimization of computing systems. https://hal.
inria.fr/inria-00436029v2, June 2009.

17

[42] Grigori Fursin.

Enabling reproducible ML and
Systems research: the good, the bad, and the ugly.
https://doi.org/10.5281/zenodo.4005773, 2020.

[43] Grigori

and

Fursin

Christophe

Dubach.
Community-driven reviewing and validation of
the 1st ACM
In Proceedings of
publications.
SIGPLAN Workshop on Reproducible Research
Methodologies and New Publication Models
in
Computer Engineering, TRUST ’14, New York, NY,
USA, 2014. Association for Computing Machinery.

[44] Grigori Fursin, Yuriy Kashnikov, Abdul Wahid
Memon, Zbigniew Chamski, Olivier Temam, Mircea
Namolaru, Elad Yom-Tov, Bilha Mendelson,
Ayal Zaks, Eric Courtois, Francois Bodin, Phil
Barnard, Elton Ashton, Edwin Bonilla, John
Thomson, Christopher Williams,
and Michael
F. P. O’Boyle. Milepost gcc: Machine learning
International
enabled
Journal of Parallel Programming,
39:296–327,
2011. 10.1007/s10766-010-0161-2.

self-tuning

compiler.

[45] Grigori Fursin, Anton

and Eben Upton.

Savenko,
Knowledge workﬂow for
into multi-objective
learning techniques.
report/rpi3-crowd-tuning-2017-interactive,
January 2018.

Lokhmotov, Dmitry
A Collective
research
and machine
https://cknowledge.io/

collaborative

autotuning

[46] T. Gamblin, M. LeGendre, M. R. Collette, G. L.
Lee, A. Moody, B. R. de Supinski, and S. Futral.
bringing order to
The spack package manager:
hpc software chaos.
In SC ’15: Proceedings of
the International Conference for High Performance
Computing, Networking, Storage and Analysis, pages
1–12, Nov 2015.

[47] Kenneth Hoste, Jens Timmerman, Andy Georges,
and Stijn Weirdt. Easybuild: Building software with
ease. pages 572–582, 11 2012.

[48] I. Jimenez, M. Sevilla, N. Watkins, C. Maltzahn,
J. Lofstead, K. Mohror, A. Arpaci-Dusseau, and
R. Arpaci-Dusseau. The popper convention: Making
In
reproducible systems evaluation practical.
2017 IEEE International Parallel and Distributed
Processing Symposium Workshops (IPDPSW), pages
1561–1570, 2017.

[49] Gaurav Kaul, Suneel Marthi, and Grigori Fursin.
Scaling deep learning on AWS using C5 instances
with MXNet, TensorFlow, and BigDL: From the
edge to the cloud. https://conferences.oreilly.
com/artificial-intelligence/ai-eu-2018/
public/schedule/detail/71549, 2018.

[50] Dirk Merkel. Docker: Lightweight linux containers
for consistent development and deployment. Linux
J., 2014(239), March 2014.

[51] Todd Mytkowicz, Amer Diwan, Matthias Hauswirth,
and Peter F. Sweeney.
Producing wrong data
In
without doing anything obviously wrong!
Proceedings of the 14th International Conference on
Architectural Support for Programming Languages
and Operating Systems, pages 265–276, 2009.

[52] QuantumBlack. Kedro: the open source library for
production-ready machine learning code. https://
github.com/quantumblacklabs/kedro, 2019.

[53] Vijay Janapa Reddi, Christine Cheng, David Kanter,
Peter Mattson, Guenther Schmuelling, Carole-Jean
Wu, Brian Anderson, Maximilien Breughe, Mark
Charlebois, William Chou, Ramesh Chukka, Cody
Coleman, Sam Davis, Pan Deng, Greg Diamos,
Jared Duke, Dave Fick, J. Scott Gardner,
Itay
Hubara, Sachin Idgunji, Thomas B. Jablin, Jeﬀ
Jiao, Tom St. John, Pankaj Kanwar, David Lee,
Jeﬀery Liao, Anton Lokhmotov, Francisco Massa,
Peng Meng, Paulius Micikevicius, Colin Osborne,
Gennady Pekhimenko, Arun Tejusve Raghunath
Rajan, Dilip Sequeira, Ashish Sirasao, Fei Sun,
Hanlin Tang, Michael Thomson, Frank Wei, Ephrem
Wu, Lingjie Xu, Koichi Yamada, Bing Yu, George
Yuan, Aaron Zhong, Peizhao Zhang, and Yuchen
Zhou. MLPerf Inference Benchmark.
https://
arxiv.org/abs/1911.02549, 2019.

[54] Carsten Uphoﬀ, Sebastian Rettenberger, Michael
Bader, Elizabeth H. Madden, Thomas Ulrich,
and Alice-Agnes Gabriel.
Stephanie Wollherr,
Extreme scale multi-physics
the
tsunamigenic 2004 sumatra megathrust earthquake.
In Proceedings of the International Conference for
High Performance Computing, Networking, Storage
and Analysis, SC ’17, New York, NY, USA, 2017.
Association for Computing Machinery.

simulations of

[55] Mark D Wilkinson, Michel Dumontier, IJsbrand Jan
Aalbersberg, Gabrielle Appleton, Myles Axton,
Arie Baak, Niklas Blomberg, Jan-Willem Boiten,
Luiz Bonino da Silva Santos, Philip E Bourne,
et al. The fair guiding principles for scientiﬁc data
management and stewardship. Scientiﬁc data, 3,
2016.

[56] Katherine Wolstencroft, Robert Haines, Donal
Fellows, Alan Williams, David Withers, Stuart
Owen, Stian Soiland-Reyes, Ian Dunlop, Aleksandra
Nenadic, Paul Fisher,
Jiten Bhagat, Khalid
Belhajjame, Finn Bacall, Alex Hardisty, Abraham
Nieva de la Hidalga, Maria P. Balcazar Vargas,
Shoaib Suﬁ, and Carole Goble.
The Taverna
workﬂow suite: designing and executing workﬂows
of Web Services on the desktop, web or in the
cloud. Nucleic Acids Research, 41(W1):W557–W561,
05 2013.

18

[57] Matei Zaharia, Andrew Chen, Aaron Davidson, Ali
Ghodsi, Sue Ann Hong, Andy Konwinski, Siddharth
Murching, Tomas Nykodym, Paul Ogilvie, Mani
Parkhe, Fen Xie, and Corey Zumar. Accelerating
the Machine Learning Lifecycle with MLﬂow. IEEE
Data Eng. Bull., 41:39–45, 2018.

19

