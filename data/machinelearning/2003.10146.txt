0
2
0
2

y
a
M
7
1

]

G
L
.
s
c
[

2
v
6
4
1
0
1
.
3
0
0
2
:
v
i
X
r
a

Julia Language in Machine Learning: Algorithms, Applications, and Open Issues

Kaifeng Gaoa, Gang Meia,∗, Francesco Picciallib,∗, Salvatore Cuomob,∗, Jingzhi Tua, Zenan Huoa

aSchool of Engineering and Technology, China University of Geosciences (Beijing), 100083, Beijing, China
bDepartment of Mathematics and Applications R. Caccioppoli, University of Naples Federico II, Naples, Italy

Abstract

Machine learning is driving development across many ﬁelds in science and engineering. A simple and efﬁ-

cient programming language could accelerate applications of machine learning in various ﬁelds. Currently,

the programming languages most commonly used to develop machine learning algorithms include Python,

MATLAB, and C/C ++. However, none of these languages well balance both efﬁciency and simplicity. The

Julia language is a fast, easy-to-use, and open-source programming language that was originally designed

for high-performance computing, which can well balance the efﬁciency and simplicity. This paper sum-

marizes the related research work and developments in the applications of the Julia language in machine

learning. It ﬁrst surveys the popular machine learning algorithms that are developed in the Julia language.

Then, it investigates applications of the machine learning algorithms implemented with the Julia language.

Finally, it discusses the open issues and the potential future directions that arise in the use of the Julia

language in machine learning.

Keywords: Julia language, Machine learning, Supervised learning, Unsupervised learning, Deep learning,

Artiﬁcial neural networks

Contents

1

Introduction

2 A Brief Introduction to the Julia Language

3

Julia in Machine Learning: Algorithms

3.1 Overview .

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4

6

7

7

∗Corresponding author
Email addresses: gang.mei@cugb.edu.cn (Gang Mei), francesco.piccialli@unina.it (Francesco Piccialli),

salvatore.cuomo@unina.it (Salvatore Cuomo)

Preprint submitted to Computer Science Review

May 19, 2020

 
 
 
 
 
 
3.2 Supervised Learning Algorithms Developed in Julia . . . . . . . . . . . . . . . . . . . . .

3.3 Unsupervised Learning Algorithms Developed in Julia . . . . . . . . . . . . . . . . . . .

3.4 Other Main Algorithms .

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.5 List of Commonly Used Julia Packages

. . . . . . . . . . . . . . . . . . . . . . . . . . .

4

Julia in Machine Learning: Applications

4.1 Overview .

.

.

.

.

.

.

4.2 Analysis of IoT Data .

4.3 Computer Vision .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.4 Natural Language Processing (NLP) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.5 Autonomous Driving .

4.6 Graph Analytics .

.

4.7 Signal Processing .

.

.

4.8 Pattern Recognition .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5

Julia in Machine Learning: Open Issues

5.1 Overview .

.

.

.

.

.

.

.

5.2 A Developing Language

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.3 Lack of Stable Development Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.4

Interfacing with Other Languages

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.5 Limited Number of Third-party Packages

. . . . . . . . . . . . . . . . . . . . . . . . . .

6 Conclusions

8

12

16

19

19

19

19

20

21

21

22

22

23

23

23

23

24

25

25

26

2

List of Abbreviations

AD

APIs

CNN

Algorithmic Differentiation

Application Programming Interfaces

Convolutional Neural Network

DPMM Dirichlet Process Mixture Model

ELM

FFGs

Extreme Learning Machine

Forney-style Factor Graphs

GBDT

Gradient-Boosting Decision Tree

GMMs Gaussian Mixture Models

GPU

ICA

IoT

JIT

Graphics Processing Unit

Independent Component Analysis

Internet of Things

Just-In-Time

kNN

k-Nearest Neighbors

LLVM Low-Level Virtual Machine

NLP

Natural Language Processing

ODPS

Open Data Processing Service

PCA

RNN

SVD

Principal Component Analysis

Recurrent Neural Network

Singular Value Decomposition

SVM

Support Vector Machine

3

1. Introduction

Machine learning is currently one of the most rapidly growing technical ﬁelds, lying at the intersection

of computer science and statistics and at the core of artiﬁcial intelligence and data science [1, 2, 3, 4].

Machine learning technology powers many aspects of modern society, from web searches to content ﬁltering

on social networks to recommendations on electronic commerce websites. Recent advances in machine

learning methods promise powerful new tools for practicing scientists. Modern machine learning methods

are closely related to scientiﬁc application [5, 6]; see Figure 1.

Figure 1: Main applications of machine learning.

4

MachineLearningPython, MATLAB, Go, R, and C/C++ are widely used programming languages in machine learning.

Python has proven to be a very effective programming language and is used in many scientiﬁc computing

applications [7]. MATLAB combines the functions of numerical analysis, matrix calculation, and scientiﬁc

data visualization in an easy-to-use manner. Both Python and MATLAB are ”Plug-and-Play” programming

languages; the algorithms are prepackaged and mostly do not require learning processes, but they are used

to solve large-scale tasks at a slow speed and have very strict requirements for memory and computing

power [8]. In addition, MATLAB is commercial.

Go is an open-source programming language that makes it easy to build simple, reliable, and efﬁcient

software. Go is syntactically similar to C, but with memory safety, garbage collection, and structural typing.

Rather than call out to libraries written in other languages, developers can work with machine learning

libraries written directly in Go. However, the current machine learning libraries written in Go are not

extensive. R is a language and environment for statistical computing and graphics. R provides a wide

variety of statistical and graphical techniques, and is highly extensible. One of the advantages of R is that

it can easily produce high-quality drawings. However, R stores data in system memory (RAM), which is a

constraint when analyzing big data.

C/C++ is one of the main programming languages in machine learning. It is of high efﬁciency and strong

portability. However, the development and implementation of machine learning algorithms with C/C++ is

not easy due to the difﬁculties in learning and using C/C++. In machine learning, the availability of large

data sets is increasing, and the demand for general large-scale parallel analysis tools is also increasing [9].

Therefore, it is necessary to choose a programming language with both simplicity and good performance.

Julia is a simple, fast, and open-source language [10]. The efﬁciency of Julia is almost comparable to

that of static programming languages such as C/C++ and Fortran [11]. Julia is rapidly becoming a highly

competitive language in data science and general scientiﬁc computing. Julia is as easy to use as R, Python,

and MATLAB.

Julia was originally designed for high-performance scientiﬁc computing and data analysis. Julia can call

many other mature high-performance basic codes, such as linear algebra and fast Fourier transforms. Simi-

larly, Julia can call C++ language functions directly without packaging or special application programming

interfaces (APIs). In addition, Julia has special designs for parallel computing and distributed computing.

In high-dimensional computing, Julia has more advantages than C++ [9]. In the ﬁeld of machine learning,

Julia has developed many third-party libraries, including some for machine learning.

5

In this paper, we systematically review and summarize the development of the Julia programming lan-

guage in the ﬁeld of machine learning by focusing on the following three aspects:

(1) Machine learning algorithms developed in the Julia language.

(2) Applications of the machine learning algorithms implemented with the Julia language.

(3) Open issues that arise in the use of the Julia language in machine learning.

The rest of the paper is organized as follows. Section 2 gives a brief introduction to the Julia language.

Section 3 summarizes the machine learning algorithms developed in Julia language. Section 4 introduces

applications of the machine learning algorithms implemented with Julia language. Section 5 presents open

issues occurring in the use of Julia language in machine learning. Finally, Section 6 concludes this survey.

2. A Brief Introduction to the Julia Language

Julia is a modern, expressive, and high-performance programming language for scientiﬁc computing

and data processing. Its development started in 2009, and the current stable release as of April 2020 is

v1.4.0. Although this low version number indicates that the language is still developing rapidly, it is stable

enough to enable the development of research code. Julia’s grammar is as readable as that of MATLAB

or Python, and it can approach the C/C++ language in performance by compiling in real time. In addition,

Julia is a free, open-source language that runs on all popular operation systems.

With the low-level virtual machine (LLVM)-based just-in-time (JIT) compiler, Julia provides powerful

computing performance. [12, 13]; see Figure 2. Julia also incorporates some important features from the

beginning of its design, such as excellent support for parallelism [14] and a practical functional program-

ming orientation, which were not fully implemented in the development of scientiﬁc computing languages

decades ago. Julia can also be embedded in other programming languages. These advantages make Julia a

universal language.

Julia successfully combines the high performance of a static programming language with the ﬂexibility

of a dynamic programming language [13]. It provides built-in primitives for parallel computing at every

level: instruction level parallelism, multi-threading and distributed computing. The Julia modules allow

users to suspend and resume computations with full control of communication without having to manually

interface with the operating system’s scheduler. Besides, Julia provides a multiprocessing environment

based on message passing to allow programs to run on multiple processes in separate memory domains

at once [15]. Moreover, the use of the high-level Julia programming language enables new and dynamic

6

approaches for graphics processing unit (GPU) programming, and Julia GPU code can be highly generic

and ﬂexible, without sacriﬁcing performance [14, 16]. However, parallel computing has not yet reached the

required level of richness and interactivity [10]. The Julia language could further improve the efﬁciency of

data division and combination, and optimize the parallel algorithms.

Figure 2: Julia benchmarks (the benchmark data shown above were computed with Julia v1.0.0, Go 1.9, Javascript V8 6.2.414.54,

MATLAB R2018a, Anaconda Python 3.6.3, and R 3.5.0. C and Fortran are compiled with gcc 7.3.1, taking the best timing from

all optimization levels. C performance = 1.0, smaller is better [17].)

3. Julia in Machine Learning: Algorithms

3.1. Overview

This section describes machine learning algorithm packages and toolkits written either in or for Julia.

Most applications of machine learning algorithms in Julia can be divided into supervised learning and un-

supervised learning algorithms. However, more complex algorithms, such as deep learning, artiﬁcial neural

7

FortranGoJavaScriptJuliaMathematicaMATLABPythonR100101102100101102 userfunc_mandelbrot recursion_quicksort recursion_fibonacci print_to_file parse_integers matrix_statistics matrix_multiply iteration_pi_sumnetworks, and extreme learning machines, include both supervised learning and unsupervised learning, and

these require separate classiﬁcation; see Figure 3.

Figure 3: Main machine learning algorithms

Supervised learning learns the training samples with class labels and then predicts the classes of data

outside the training samples. All the markers in supervised learning are known; therefore, the training

samples have low ambiguity. Unsupervised learning learns the training samples without class labels to

discover the structural knowledge in the training sample set. All categories in unsupervised learning are

unknown; thus, the training samples are highly ambiguous.

3.2. Supervised Learning Algorithms Developed in Julia

Supervised learning infers a model from labeled training data. Supervised learning algorithms devel-

oped in Julia mainly include classiﬁcation and regression algorithms; see Figure 4.

8

Machine LearningUnsupervised LearningSupervised LearningClusteringDimension ReductionRegressionClassicationk-meansHierarchicalClusteringBi-ClusteringGaussian MixtureModelBayesian ModelRandom ForestSupport Vector MachineRegression AnalysisRegression TreeDecisionTreek-Nearest NeighborsPrincipal Component AnalysisIndependent Component AnalysisFactor AnalysisOther Main AlgorithmsDeep LearningArtificial Neural NetworkExtreme Learning MachineFigure 4: Main supervised learning algorithms developed in Julia

Bayesian Model

There are two key points in the deﬁnition of Bayesian model:

independence between features and

the Bayesian theorem. One of the most important research areas of Bayesian model is Bayesian linear

regression. Bayesian linear regression solves the problem of overﬁtting in maximum likelihood estimation.

Moreover, it makes full use of data samples and is suitable for modeling complex data [18, 19]. In addition

to regression, Bayesian reasoning can also be applied in other ﬁelds. Some researchers have conducted

research on naive Bayes in image recognition and text classiﬁcation.

There are some Bayesian model packages and algorithms developed in mature languages. Strickland

et al. [20] developed the Python package Pyssm, which was developed for time series analysis using a

linear Gaussian state-space model. Mertens et al. [21] developed a user-friendly Python package Abrox

for approximate Bayesian computation with a focus on model comparison. There are also Python pack-

ages BAMSE [22], BayesPy [23], PyMC [24] and so on. Moreover, Vanhatalo et al. [25] developed the

MATLAB toolbox GPstuff for Bayesian modeling with Gaussian processes, and Zhang et al. [26] devel-

oped the MATLAB toolbox BSmac, which implements a Bayesian spatial model for brain activation and

connectivity.

The Julia language is also used to develop packages for the Bayesian model. Gen [27] is a proba-

bilistic programming language proposed by Cusumano and Mansinghka that can be embedded in Julia.

This language provides a structure for the optimization of the automatic generation of custom reasoning

9

Supervised LearningClassification RegressionClusteringSupport Vector MachineRandom Forestk-Nearest NeighborBayesian ModelRegression AnalysisRegression TreeDecision Treestrategies for static analysis based on an objective probability model. They described Gen’s language de-

sign informally and used an example Bayesian statistical model for robust regression to show that Gen is

more expressive than Stan, a widely used language for hierarchical Bayesian modeling. Cox et al. [28]

explored a speciﬁc probabilistic programming paradigm, namely, message passing in Forney-style factor

graphs (FFGs), in the context of the automated design of efﬁcient Bayesian signal processing algorithms.

Moreover, they developed ForneyLab.jl as a Julia Toolbox for message passing-based inference in FFGs.

Due to the increasing availability of large data sets, the need for a general-purpose massively parallel

analysis tool is becoming ever greater. Bayesian nonparametric mixture models, exempliﬁed by the Dirich-

let process mixture model (DPMM), provide a principled Bayesian approach to adapt model complexity to

the data. Dinari et al. [9] used Julia to implement efﬁcient and easily modiﬁable distributed inference in

DPMMs.

k-Nearest Neighbors (kNN)

The kNN algorithm has been widely used in data mining and machine learning due to its simple imple-

mentation and distinguished performance. A training data set with a known label category is used, and for

a new data set, the k instances closest to the new data are found in the feature space of the training data set.

If most of the instances belong to a category, the new data set belongs to this category.

At present, there are many packages developed for the kNN algorithm in the Python language. Among

these, scikit-learn and Pypl are the most commonly used packages. It should be noted that scikit-learn

and Pypl are not specially developed for the kNN algorithm; they contain many other machine learning

algorithms. In addition, Bergstra et al. [29] developed Hyperopt to deﬁne a search space that encompasses

many standard components and common patterns of composing them.

Julia is also used to develop packages for the kNN algorithm. NearestNeighbors.jl [30] is a package

written in Julia to perform high-performance nearest neighbor searches in arbitrarily high dimensions. This

package can realize kNN searches and range searches.

Decision Tree, Regression Tree, and Random Forest

Mathematically, a decision tree is a graph that evaluates a limited number of probabilities to determine a

reliable classiﬁcation for each data point. A regression tree is the opposite of a decision tree and is suitable

for solving regression problems. It does not predict labels but predicts a continuous change value. Random

forests are a set of decision trees or regression trees that work together [31]. The set of decision trees (or

10

continuous y regression trees) is constructed by performing bootstrapping on the data sets and averaging or

acquiring pattern prediction (called ”bagging”) from the trees. Subsampling of features is used to reduce

generalization errors [32]. An ancillary result of the bootstrapping procedure is that the data not sampled in

each bootstrap (called ”out-of-bag” data) can be used to estimate the generalization error as an alternative

to cross-validation [33].

Many packages have been developed for decision trees, regression trees, and random forests. For exam-

ple, the above three algorithms are implemented in Spark2 ML and scikit-learn using Python. In addition,

Upadhyay et al. [34] proposed land-use and land-cover classiﬁcation technology based on decision trees

and k-nearest neighbors, and the proposed techniques are implemented using the scikit-learn data mining

package for python. Keck [35] proposed a speed-optimized and cache-friendly implementation for multi-

variate classiﬁcation called FastBDT, which provides interfaces to C/C++, Python, and TMVA. Yang et al.

[36] used the open data processing service (ODPS) and Python to implement the gradient-boosting decision

tree (GBDT) model.

DecisionTree.jl [37], written in the Julia language, is a powerful package that can realize decision tree,

regression tree, and random forest algorithms very well. The package has two functions, and the ingenious

use of these functions can help us realize these three algorithms.

Support Vector Machine (SVM)

In SVM, the objective is to ﬁnd a hyperplane in high-dimensional space, which represents the maximum

margin between any two instances of two types of training data points (support vectors) or maximizes the

correlation function when it cannot be separated. The so-called kernel similarity function is used to design

the non-linear SVM [38].

Currently, there are textbook style implementations of two popular linear SVM algorithms: Pegasos

[39], Dual Coordinate Descent. LIBSVM developed by the Information Engineering Institute of Taiwan

University is the most widely used SVM tool [40]. LIBSVM includes standard SVM algorithm, probability

output, support vector regression, multi-classiﬁcation SVM and other functions. Its source code is originally

written by C. It provides Java, Python, R, MATLAB, and other language invocation interfaces.

SVM.jl [41], MLJ.jl [42], and LIBSVM.jl [43] are native Julia implementations of SVM algorithm.

However, LIBSVM.jl is more comprehensive than SVM.jl. LIBSVM.jl supports all libsvm models: clas-

siﬁcation c-svc, nu-svc, regression: epsilon-svr, nu-svr and distribution estimation: a class of support vector

machines and ScikitLearn.jl [44] API. In addition, the model object is represented by a support vector

11

machine of Julia type. The SVM can easily access the model features and can be saved as a JLD ﬁle.

Regression Analysis

Regression analysis is an important supervised learning algorithm in machine learning. It is a predictive

modeling technique, which constructs the optimal solution to estimate unknown data through the sample

and weight calculation. Regression analysis is widely used in the ﬁelds of the stock market and medical

data analysis.

Python has been widely used to develop a variety of third-party packages for regression analysis, in-

cluding scikit-learn and orange. The scikit-learn package is a powerful Python module, which supports

mainstream machine learning algorithms such as regression, clustering, classiﬁcation and neural network

[45, 46, 47]. The orange package is a component-based data mining software, which can be used as a mod-

ule of Python programming language, especially suitable for classiﬁcation, clustering, regression and other

work [48, 49]. MATLAB also supports the regression algorithm. By invoking commands such as regress

and stepwise in the statistical toolbox of MATLAB, regression operation can be performed conveniently on

the computer.

The Julia language is also used to develop a package, Regression.jl [50], to perform the regression

analysis. The Regression.jl package seeks to minimize empirical risk based on EmpiricalRisk.jl [51] and

provides a set of algorithms for performing regression analysis. It supports multiple linear regression, non-

linear regression, and other regression algorithms. In addition, the Regression.jl package also provides a

variety of solvers such as analytical solution (for linear and ridge regression) and gradient descent.

3.3. Unsupervised Learning Algorithms Developed in Julia

Unsupervised learning is a type of self-organized learning that can help ﬁnd previously unknown pat-

terns in a dataset without the need for pre-existing labels. Two of the main methods used in unsupervised

learning are dimensionality reduction and cluster analysis; see Figure 5.

Gaussian Mixture Models (GMMs)

GMMs are probabilistic models for representing normally distributed subpopulations within an overall

population. GMMs use Gaussian distribution as the basic parameter model, accurately characterizes the data

distribution by combining multiple Gaussian distributions, and use the expectation-maximization algorithm

for training. Compared with the Gaussian models, the GMMs provide greater ﬂexibility and precision in

12

Figure 5: Main unsupervised learning algorithms developed in Julia

modeling the underlying statistics of sample data [52]. Generally, the GMMs are used to solve problems

such as image segmentation and dynamic target detection [53].

Currently, there are many libraries that can implement Gaussian mixture models; these include packages

developed with Python, such as PyBGMM and numpy-ml, and packages developed with C++, such as

Armadillo. There are also some GMM packages for specialized ﬁelds. Bruneau et al. [54] proposed a

new Python package for nucleotide sequence clustering, which implements a Gaussian mixture model for

DNA clustering. Holoien et al. [55] developed a new open-source tool, EmpiriciSN, written in Python, for

performing extreme deconvolution Gaussian mixture modeling.

To the best of the authors’ knowledge, there is no mature Julia package for the GMMs. GmmFlow.jl

[56] is a Julia library that can implement some simple functions of the GMMs, including model generation

and cluster mapping. But the algorithm optimization could be improved. GaussianMixtures.jl [57] is

another Julia package for the GMMs. This package has implemented both diagonal covariance and full

covariance GMMs, and full covariance variational Bayes GMMs. However, the package is slightly strict

with data types. ScikitLearn.jl implements the popular scikit-learn interface and algorithms in Julia, and it

can access approximately 150 Julia and Python models, including the Gaussian mixture model. Moreover,

Srajer et al. [58] used algorithmic differentiation (AD) tools in a GMM ﬁtting algorithm.

13

Unsupervised Learning Cluster Analysis  Dimensionality Reduce Clustering k-means Density-Based Clustering Gaussian Mixture Model  Hierarchical Clustering  Principal Components Analysis Independent Components AnalysisFactor Analysisk-means

The k-means clustering algorithm is an iterative clustering algorithm. It ﬁrst randomly selects k objects

as the initial clustering center, then calculates the distance between each object and each seed clustering

center, and assigns each object to the nearest clustering center. Cluster centers and the objects assigned to

them represent a cluster. As an unsupervised clustering algorithm, k-means is widely used because of its

simplicity and effectiveness.

The k-means algorithm is a classic clustering method, and many programming languages have de-

veloped packages related to it. The third-party package scikit-learn in Python implements the k-means

algorithm [47, 59]. The Kmeans function in MATLAB can also implement a k-means algorithm [60]. In

addition, many researchers have implemented the k-means algorithm in the C/C++ programming language.

Julia has also been used to develop a speciﬁc package, Clustering.jl [61], for clustering. Clustering.jl

provides several functions for data clustering and clustering quality evaluation. Because Clustering.jl has

comprehensive and powerful functions, this package is a good choice for k-means.

Hierarchical Clustering

Hierarchical clustering is a kind of clustering algorithm that performs clustering by calculating the

similarity between data points of different categories [62, 63, 64]. The strategy of cohesive hierarchical

clustering is to ﬁrst treat each object as a cluster and then merge these clusters into larger and larger clusters

until all objects are in one cluster or some termination condition is satisﬁed.

The commonly used Python packages for hierarchical clustering are scikit-learn and scipy. Hierarchi-

cal clustering within the scikit-learn package is implemented in the sklearn.cluster method, which includes

three important parameters: the number of clusters, the connection method, and connection measurement

options [47]. scipy implements hierarchical clustering with the scipy.cluster method [65].

In addition,

programming languages such as MATLAB and C/C++ can also perform hierarchical clustering [66].

The package QuickShiftClustering.jl [67], written using Julia, can realize hierarchical clustering al-

gorithms. This package is quite easy to use. It provides three functions: clustering matrix data, clustering

labels, and creating hierarchical links to achieve hierarchical clustering [68].

Bi-Clustering

Bi-Clustering algorithm is based on traditional clustering. Its basic idea is to cluster rows and columns

of matrices through traditional clustering, and then merge the clustering results. Bi-Clustering algorithm

14

solves the bottleneck problem of traditional clustering in high-dimensional data. Data sets in reality are

mostly high-dimensional and inherently sparse. Traditional clustering algorithms often fail to detect mean-

ingful clustering in high-dimensional data sets. However, Bi-Clustering can detect clusters of any shape

and position in space, and it is an effective method to solve the problem of subspace clustering in high-

dimensional data sets [69, 70]. To search for local information better in the data matrix, researchers put

forward the concept of bi-clustering.

The package scikit-learn can implement bi-clustering, and the implementation module is sklearn.cluster.bicluster.

At present, bi-clustering is mainly applied to highthroughput detection technologies such as gene chips and

DNA microarrays.

The Julia language is also used to develop packages that implement bi-clustering. For example, Kpax3

[71] is a Bayesian method for multi-cluster multi-sequence alignment. Bezanson et al. [10] used a Bayesian

dual clustering model, which extended and improved the model originally introduced by Pessia et al. [71].

They wrote the kpax3.jl library package in Julia and the output contains multiple text ﬁles containing a

cluster of rows and columns of the input dataset.

Principal Component Analysis (PCA)

PCA is a method of statistical analysis and a simpliﬁed data set. It uses an orthogonal transformation to

linearly transform observations of a series of possibly related variables and then project them into a series of

linearly uncorrelated variables. These uncorrelated variables are called principal components. PCA is often

used to reduce the dimensionality of a data set while maintaining the features that have the largest variance

contribution in the data set.

Python is the most frequently used language for developing PCA algorithms. The scikit-learn package

provides a class, sklearn.decomposition.PCA [72], to implement PCA algorithms in the sklearn.decomposition

module. Generally, the PCA class does not need to adjust parameters very much but needs to specify the

target dimension or the variance of the principal components after dimensionality reduction. In addition,

many researchers have developed related application packages using the C++ programming language. These

include the ALGLIB [73] package and the class cv :: PCA [74] in OpenCV.

To the best of the authors’ knowledge, there is no mature Julia package speciﬁcally for PCA. However,

MultivariateStats.jl [75] is a Julia package for multivariate statistics and data analysis. This package

deﬁnes a PCA type to represent a PCA model and provides a set of methods to access properties.

15

Independent Component Analysis (ICA)

ICA is a new signal processing technology developed in recent years. The ICA method is based on mu-

tual statistical independence between sources. In the practical applications of signal processing, especially

in communication and biomedicine, it is important to eliminate noise data. Traditional signal processing

techniques for noise cancellation include band-pass ﬁltering, fast Fourier transform, autocorrelation, autore-

gressive modeling, adaptive ﬁltering, Kalman ﬁltering and singular value decomposition. The traditional

ﬁltering technology is based on the assumption that noise is the only additive, which is not suitable for

multi-sensor observation of mixed signals [76]. However, the ICA algorithm has strong robustness to ad-

ditive noise, and it is one of the most promising methods to solve the problem of blind noise suppression

[77, 78]. Moreover, in contrast to traditional signal separation methods based on feature analysis, such as

singular value decomposition (SVD) and PCA, ICA is an analysis method based on higher-order statistical

characteristics. In many applications, the analysis of higher-order statistical characteristics is more practical.

Python is the most frequently used language in developing ICA algorithms. The scikit-learn package

has developed a class, FastICA [79], to implement ICA algorithms in the sklearn.decomposition module.

In addition, Brian Moore [80] developed a PCA and ICA Package using the MATLAB programming

language. The PCA and ICA algorithms are implemented as functions in this package, and it includes

multiple examples to demonstrate their usage.

To the best of the authors’ knowledge, ICA does not have a mature software package developed in

the Julia language. However, MultivariateStats.jl [75], like a Julia package for multivariate statistics and

data analysis, deﬁnes an ICA type representing the ICA model and provides a set of methods to access the

attributes.

3.4. Other Main Algorithms

In addition to supervised learning algorithms and unsupervised learning algorithms, machine learning

algorithms include a class of algorithms that are more complex and cannot be categorized into a speciﬁc

category. For example, artiﬁcial neural networks can implement supervised learning, unsupervised learning,

reinforcement learning, and self-learning. Deep learning algorithms are based on artiﬁcial neural network

algorithms and can perform supervised learning, unsupervised learning, and semisupervised learning. Ex-

treme learning machines were proposed for supervised learning algorithms but were extended to unsuper-

vised learning in subsequent developments.

16

Deep Learning

Deep learning allows computational models that are composed of multiple processing layers to learn

representations of data with multiple levels of abstraction [5]. Several deep learning frameworks, such as

the depth neural network, the convolutional neural network, the depth conﬁdence network and the recursive

neural network, have been applied to computer vision, speech recognition, natural language processing,

image recognition, and bioinformatics and have achieved excellent results.

It has been several years since the birth of deep learning algorithms. Many researchers have improved

and developed deep learning algorithms. Python is the most frequently used language in developing deep

learning algorithms. For example, PyTorch [81, 82] and ALiPy [83] are Python packages with many deep

learning algorithms. Moreover, Tang et al. developed GCNv2 [84] using C++ and Python, Huang et al.

wrote Mask Scoring R-CNN [85] using Python, Hanson and Frazier-Logue compared the Dropout [86]

algorithm with the SDR [87] algorithm, and Luo et al. [88] proposed and used Python to write AdaBound

(a new adaptive optimization algorithm).

Julia has also been used to develop various deep learning algorithms. For example, AD allows the exact

computation of derivatives given only an implementation of an objective function, and Srajer et al. [58]

wrote an AD tool and used it in a hand-tracking algorithm.

Augmentor is a software package available in both Python and Julia that provides a high-level API

for the expansion of image data using a stochastic, pipeline-based approach that effectively allows images

to be sampled from a distribution of augmented images at runtime [89]. To demonstrate the API and to

highlight the effectiveness of augmentation on a well-known dataset, a short experiment was performed. In

the experiment, the package is used on a convolutional neural network (CNN) [90].

MXNet.jl [91], Knet.jl [92], Flux.jl [93], and TensorFlow.jl [94] are deep learning frameworks with

both efﬁciency and ﬂexibility. At its core, MXNet.jl contains a dynamic dependency scheduler that au-

tomatically parallelizes both symbolic and imperative operations on the ﬂy. MXNet.jl is portable and

lightweight, scaling effectively to multiple GPUs and multiple machines.

Artiﬁcial Neural Networks

A neural network is a feedforward network consisting of nodes (”neurons”), each side of which has

weights. These allow the network to form a mapping between the input and output [95]. Each neuron that

receives input from a previous neuron consists of the following components: the activation, a threshold, the

time at which the newly activated activation function is calculated and the output function of the activation

17

output.

At present, the framework of a neural network model is usually developed in C++ or Python. DLL

is a machine learning framework written in C++ [96]. It supports a variety of neural network layers and

standard backpropagation algorithms. It can train artiﬁcial neural networks and CNNs and support basic

learning options such as momentum and weight attenuation. scikit-learn, a machine learning library based

on Python, also supports neural network models [47].

Employing the Julia language, Difﬁqﬂux.jl [97] is a package that integrates neural networks and differ-

ential equations. Rackauckas et al. [97] described differential equations from the perspective of data science

and discuss the complementarity between machine learning models and differential equations. These au-

thors demonstrated the ability to combine DifferentialEquations.jl [98] deﬁned differential equations into

Flux-deﬁned neural networks. Backpropneuralnet.jl [58] is an easy-to-use neural network package.

Extreme Learning Machine (ELM)

ELM [99] is a variant of Single Hidden Layer Feedforward Networks (SLFNs). Because its weight

is not adjusted iteratively, it deviates greatly. This signiﬁcantly improves the efﬁciency when training the

neural networks.

The basic algorithm of ELM and Multi-Layer [100]/Hierarchical [101] ELM have been implemented

in HP-ELM. Meanwhile, C/C++, MATLAB, Python and JAVA versions are provided. HP-ELM includes

GPU acceleration and memory optimization, which is suitable for large data processing. HP-ELM supports

LOO (Leave One Out) and k-fold cross-validation to dynamically select the number of hidden layer nodes.

The available feature maps include linear function, Sigmoid function, hyperbolic sinusoidal function, and

three radial basis functions.

According to ELM, parameters of hidden nodes or neurons are not only independent of training data

but also independent of each other. Standard feedforward neural networks with hidden nodes have universal

approximation and separation capabilities. These hidden nodes and their related maps are terminologically

ELM random nodes, ELM random neurons or ELM random features. Unlike traditional learning methods,

which need to see training data before generating hidden nodes or neuron parameters, ELM could generate

hidden nodes or neuron parameters randomly before seeing training data. Elm.jl [102] is an easy-to-use

extreme learning machine package.

18

3.5. List of Commonly Used Julia Packages

We summarize the commonly used Julia language packages and the machine learning algorithms that

these packages primarily support; see the investigation in Table 1.

4. Julia in Machine Learning: Applications

4.1. Overview

Machine learning is one of the fastest-growing technical ﬁelds nowadays. It is a cross-cutting ﬁeld

of statistics and computer science [1, 2, 3]. Machine learning specializes in how computers simulate or

implement human learning behaviors. By acquiring new knowledge and skills, the existing knowledge

structure is reorganized to improve its performance.

Julia, as a programming language with the rise of machine learning, has corresponding algorithmic

library packages in most machine learning applications. In the following, we summarize the applications of

Julia in machine learning. As shown in Figure 6, the current applications of Julia programming language

in machine learning mainly focus on the Internet of Things (IoT), computer vision, autonomous driving,

pattern recognition, etc.

Figure 6: Major applications of machine learning using Julia language

4.2. Analysis of IoT Data

The IoT, also called the Internet of Everything or the Industrial Internet, is a new technology paradigm

envisioned as a global network of machines and devices capable of interacting with each other [103]. The

19

Pattern RecognitionSignal ProcessingNatural Language ProcessingComputer VisionIoT DataAnalysisAutonomous DrivingGraph Analyticsapplication of the IoT in industry, agriculture, the environment, transportation, logistics, security, and other

infrastructure ﬁelds effectively promotes the intelligent development of these areas and more rationally uses

and allocates limited resources, thus improving the efﬁciency of these ﬁelds [104, 105, 106]. Machine learn-

ing has brought enormous development opportunities for the IoT and has a signiﬁcant impact on existing

industries [107, 108].

Invenia Technical Computing used the Julia language to expand its energy intelligence system [109].

They optimized the entire North American grid and used the energy intelligent system (EIS) and various

signals to directly improve the day-ahead planning process. They used the latest research in machine learn-

ing, complex systems, risk analysis, and energy systems.

In addition, Julia provided Invenia Technical

Computing with versatility in terms of programming style, parallelism, and language interoperability [109].

Fugro Roames engineers [110] used the Julia language to implement machine learning algorithms to

identify network faults and potential faults, achieving a 100-fold increase in speed. Protecting the grid

means ensuring that all power lines, poles, and wires are in good repair, which used to be a laborious

manual task that required thousands of hours to travel along the power line. Fugro Roames engineers have

developed a more effective way to identify threats to wires, poles, and conductors. Using a combination

of LiDAR and high-resolution aerial photography, they created a detailed three-dimensional map of the

physical conditions of the grid and possible intrusions. Then, they used machine learning algorithms to

identify points on the network that have failed or are at risk of failure [110].

4.3. Computer Vision

Computer vision is a simulation of biological vision using computers and related equipment. Its main

task is to obtain the three-dimensional information of the corresponding scene by processing collected pic-

tures or videos. Computer vision includes image processing and pattern recognition. In addition, it also

includes geometric modeling and recognition processes. The realization of image understanding is the

ultimate goal of computer vision. Machine learning is developing, and computer vision research has grad-

ually shifted from traditional models to deep learning models represented by CNNs and deep Boltzmann

machines.

Computer vision is currently widely applied in the ﬁelds of biological and medical image analysis [111],

urban streetscapes [37, 112], rock type identiﬁcation [113], automated pavement distress detection and clas-

siﬁcation [114], structural damage detection in buildings [115], and other ﬁelds. The development language

used in current research is usually Python or another mature language. In contrast, when dealing with large-

20

scale data sets, the Julia language has inherent advantages in high-performance processing. Therefore,

many scholars and engineers use Julia to develop packages for the applications of computer vision. The

Metalhead.jl [116] package provides computer vision models that run on top of the Flux machine learning

library. The package ImageProjectiveGeometry.jl [117] is intended as a starting point for the development

of a library of projective geometry functions for computer vision in Julia. Currently, the package consists

of a number of components that could ultimately be separated into individual packages or added to other

existing packages.

4.4. Natural Language Processing (NLP)

NLP employs computational techniques for the purpose of learning, understanding, and producing hu-

man language content [118]. It is an important research direction in the ﬁeld of computer science and arti-

ﬁcial intelligence. Modern NLP algorithms are based on machine learning algorithms, especially statistical

machine learning algorithms. Many different machine learning algorithms have been applied to NLP tasks,

the most representative of which are deep learning algorithms exempliﬁed by CNN [119, 120, 121, 122].

Currently, one of the main research tasks of NLP is to investigate the characteristics of human language

and establish the cognitive mechanism of understanding and generating language. In addition, new practical

applications for processing human language through computer intelligence have been developed. Many

researchers and engineers have developed practical application tools or software packages using the Julia

language. For example, LightNLP.jl [123] is a lightweight NLP toolkit for the Julia language. However,

to the best of the authors’ knowledge, there are currently no stable library packages developed in the Julia

language speciﬁcally for NLP.

4.5. Autonomous Driving

Machine learning is widely used in autonomous driving, and it mainly focuses on the environmental

perception and behavioral decision-making of autonomous vehicles. The application of machine learning

in environmental perception belongs to the category of supervised learning. When performing object recog-

nition on images obtained from the surrounding environment of a vehicle, a large number of images with

solid objects are required as training data, and then deep learning methods can identify objects from the

new images [41, 42, 102, 124]. The application of machine learning in behavioral decision-making gen-

erally involves reinforcement learning. Autonomous vehicles need to interact with the environment, and

reinforcement learning learns the mapping relationship between the environment and behavior that interacts

21

with the environment from a large amount of sample data. Thus, whenever an autonomous vehicle perceives

the environment, it can act intelligently [125, 126].

To the best of the authors’ knowledge, there are no software packages or solutions specially developed

in Julia for autonomous driving. However, the machine learning algorithms used in autonomous driving are

currently implemented by researchers in the Julia language. The amount of data obtained by autonomous

vehicles is huge, and the processing is complex, but autonomous vehicles have strict requirements for data

processing time. High-level languages such as Python and MATLAB are not as efﬁcient in computing as the

Julia language, which was speciﬁcally developed for high-performance computing. Therefore, we believe

that Julia has strong competitiveness as a programming language for autonomous vehicle platforms.

4.6. Graph Analytics

Graph analytics is a rapidly developing research ﬁeld.

It combines graph-theoretic, statistics and

database technology to model, store, retrieve and analyze graph-structured data. Samsi [127] used sub-

graph isomorphism to solve the previous scalability difﬁculties in machine learning, high-performance

computing, and visual analysis. The serial implementations of C++, Python, and Pandas and MATLAB

are implemented, and their single-thread performance is measured.

LightGraphs.jl is currently the most comprehensive library developed in Julia for graph analysis [128].

LightGraphs.jl provides a set of simple, concrete graphical implementations (including undirected and di-

rected graphs) and APIs for developing more complex graphical implementations under the AbstractGraph

type.

4.7. Signal Processing

The signal processing in communications is the cornerstones of electrical engineering research and

other related ﬁelds [129, 130]. Python has natural advantages in analyzing complex signal data due to

its numerous packages. In addition, the actually collected signals need to be processed before they can

be used for analysis. MATLAB provides many signal processing toolboxes, such as spectrum analysis

toolbox, waveform viewer, ﬁlter design toolbox. Therefore, MATLAB is also a practical tool for signal data

processing.

Current and emerging means of communication increasingly rely on the ability to extract patterns from

large data sets to support reasoning and decision-making using machine learning algorithms. This calls

the use of the Julia language. For example, Srivastava Prakalp et al. [131] designed an end-to-end pro-

grammable hybrid signal accelerator, PROMISE, for machine learning algorithms. PROMISE can use

22

machine learning algorithms described by Julia and generate PROMISE code. PROMISE can combine

multiple signals and accelerate machine learning algorithms.

4.8. Pattern Recognition

Pattern recognition is the automatic processing and interpretation of patterns by means of a computer

using mathematical technology [132]. With the development of computer technology, it is possible for hu-

mans to study the complex process of information-processing, an important form of which is the recognition

of the environment and objects by living organisms. The main research directions of pattern recognition

are image processing and computer vision, speech information processing, medical diagnosis and biometric

authentication technology [133].

Pattern recognition is generally categorized according to the type of learning procedure used to generate

the output value. Medical diagnosis is a typical ﬁeld of pattern recognition applications. Rajsavi et al. [134]

used the Julia libraries packages such as GLM.jl [135] to predict the mortality rate of diabetic ICU patients

through severity indicators. The application case of this pattern recognition was completely written by Julia

language. Other typical applications of pattern recognition techniques are automatic speech recognition, text

classiﬁcation , face recognition. Languages.jl [136] is a Julia package for working with human languages.

Script detection model works by checking the Unicode character ranges present within the input text. But

the package was supported only for English and German currently.

5. Julia in Machine Learning: Open Issues

5.1. Overview

Since its release, the advantages of Julia language, such as simplicity and efﬁciency, have been recog-

nized by developers in various ﬁelds. However, with the promotion of the Julia language and the steady

increase in the number of users, it also faces several open issues; see Figure 7.

5.2. A Developing Language

Although Julia has developed rapidly, its inﬂuence is far less than that of other popular programming

languages. After several versions of updates, Julia has become relatively stable, but there are still several

problems to be solved. Julia’s grammar has changed considerably, and although these changes are for the

sake of performance or ease of expression, these differences also make it difﬁcult for different programs to

work together. One of Julia’s obvious advantages is its satisfactory efﬁciency; but to write efﬁcient code,

23

Figure 7: Open issues of Julia language

one needs to transform the method of thinking in programming and not just copy code into Julia. For people

who have just come into contact with Julia, the ease of use can also cause them to ignore this problem and

ultimately lead to unsatisfactory code efﬁciency.

5.3. Lack of Stable Development Tools

Currently, the commonly used editors and IDEs for the Julia language include (1) Juno (Atom Plugin),

(2) Visual Studio Code (VS Code Extension), (3) Jupyter (Jupyter kernel), and (4) Jet Brains (IntelliJ IDEA

Plugin). According to [137], Juno is currently the most popular editor. These editors and IDEs are exten-

sions based on third-party platforms, which can quickly build development environments for Julia in its

early stages of development, but in the long run, this is not a wise approach. Users need to conﬁgure Julia

initially, but the ﬁnal experience is not satisfactory. Programming languages such as MATLAB, Python and

C/C++ each have their own IDE, which integrates the functions of code writing, analysis, compilation and

debugging. Although editors and IDEs have achieved many excellent functions, it is very important to have

a complete and Julia-speciﬁc IDE.

24

Open IssuesLimited Number of Third-party PackagesInterfaceswith Other  LanguagesA Developing LanguageLack of StableDevelopment  Tools5.4. Interfacing with Other Languages

In the process of using Julia for development, although most of the code can be written in Julia, many

high-quality, mature numerical computing libraries have been written in C and Fortran. To facilitate the use

of existing code, Julia should also make it easy and effective to call C/C++ and Fortran functions. In the

ﬁeld of machine learning, Python has been used to write a large quantity of excellent code. If one desires

to transplant code into Julia in a short time for maintenance, simple calls are necessary, which can greatly

reduce the learning curve.

Currently, PyCall and Call are used in Julia to invoke these existing libraries, but Julia still needs a more

concise and general invocation method. More important is ﬁnding a method to ensure that the parts of these

calls can maintain the original execution efﬁciency or the execution efﬁciency of the native Julia language.

At the same time, it is also very important to embed Julia’s code in other languages, which would not only

popularize the use of Julia more quickly but also combine the characteristics of Julia to enable researchers

to accomplish tasks more quickly.

5.5. Limited Number of Third-party Packages

For good programming languages, the quantity and quality of third-party libraries are very important.

For Python, there are 194,934 projects registered in PyPI [11], while the number of Julia third-party pack-

ages registered in Julia Observer is approximately 3,000. The number of third-party libraries in Julia is

increasing, but there are still relatively few compared with other programming languages, and there may

not be suitable libraries available in some unpopular areas.

Because Julia is still in the early stage of development, version updates are faster, and the interface and

grammar of the program are greatly changed in each version upgrade. After the release of Julia 1.0, the Julia

language has become more mature and stable than in the past. However, many excellent third-party machine

learning libraries were written before the release of Julia 1.0 and failed to update to the new version in time.

Users need to carefully evaluate whether a third-party library has been updated to the latest version of Julia

to ensure its normal use. In addition, Julia is designed for parallel programming, but there are not many

third-party libraries for parallel programming. Currently, the more commonly used third-party packages of

Julia are CUDAnative.jl, CuArrays.jl, and juliaDB.jl. However, many functions in these packages are

still in the testing stage.

Although Julia libraries are not as rich as those of Python, the prospects for development are optimistic.

Ofﬁcials have provided statistical trends in the number of repositories. Many scholars and technicians are

25

committed to improving the Julia libraries. Rong Hongbo et al. [138] used Julia, Intel MKL and the SPMP

library to implement Sparso, which is a sparse linear algebra context-driven optimization tool that can

accelerate machine learning algorithms. Plumb Gregory et al. [139] compiled a library package for fast

Fourier analysis using Julia, which made it easier for fast Fourier analysis to be employed in statistical

machine learning algorithms.

6. Conclusions

This paper has systematically investigated the development status of the Julia language in the ﬁeld of

machine learning, including machine learning algorithms written in Julia, the application of the Julia lan-

guage in machine learning, and the potential challenges faced by Julia. We ﬁnd that: (1) Machine learning

algorithms written in Julia are mainly supervised learning algorithms, and there are fewer algorithms for

unsupervised learning. (2) The Julia language is widely used in seven popular machine learning research

topics: pattern recognition, NLP, IoT data analysis, computer vision, autonomous driving, graph analytics,

and signal processing. (3) There are far fewer available application packages than there are for other high-

level languages, such as Python, which is Julia’s greatest challenge. This survey provides a comprehensive

investigation of the applications of Julia in machine learning. We believe that with the gradual maturing of

the Julia language and the development of related third-party packages, the Julia language will be a highly

competitive programming language for machine learning.

Declaration of competing interest

The authors declare that they have no known competing interests.

Acknowledgments

This research was jointly supported by the National Natural Science Foundation of China (11602235),

the Fundamental Research Funds for China Central Universities (2652018091), and the Major Project for

Science and Technology (2020AA002). The authors would like to thank the editor and the reviewers for

their valuable comments.

26

Reference

References

[1] M. I. Jordan, T. M. Mitchell, Machine learning: Trends, perspectives, and prospects, Science 349 (6245) (2015) 255–260.

doi:10.1126/science.aaa8415.

[2] R. C. Deo, Machine Learning in Medicine, Circulation 132 (20)

(2015) 1920–1930.

doi:10.1161/

circulationaha.115.001593.

[3] P. Domingos, A Few Useful Things to Know About Machine Learning, Communications of the ACM 55 (10) (2012) 78–87.

doi:10.1145/2347736.2347755.

[4] P. Riley, Three pitfalls to avoid in machine learning, Nature 572 (7767)

(2019) 27–29.

doi:10.1038/

d41586-019-02307-y.

[5] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (7553) (2015) 436–444. doi:10.1038/nature14539.

[6] E. Mjolsness, D. DeCoste, Machine learning for science: State of the art and future prospects, Science 293 (5537) (2001)

2051–+. doi:10.1126/science.293.5537.2051.

[7] E. Serrano, J. Garcia Blas, J. Carretero, M. Abella, M. Desco, Medical Imaging Processing on a Big Data platform us-

ing Python: Experiences with Heterogeneous and Homogeneous Architectures, IEEE-ACM International Symposium on

Cluster Cloud and Grid Computing, 2017, pp. 830–837. doi:10.1109/ccgrid.2017.56.

[8] Z. Voulgaris, Julia for Data Science, Technics Publications, LLC; First edition, July 30, 2016.

[9] O. Dinari, A. Yu, O. Freifeld, J. Fisher, Distributed mcmc inference in dirichlet process mixture models using Julia, in:

2019 19th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID), 2019, pp. 518–525.

doi:10.1109/CCGRID.2019.00066.

[10] J. Bezanson, A. Edelman, S. Karpinski, V. B. Shah, Julia: A Fresh Approach to Numerical Computing, Siam Review 59 (1)

(2017) 65–98. doi:10.1137/141000671.

[11] J. M. Perkel, Julia: Come for the syntax, stay for the speed, Nature 572 (7767) (2019) 141–142. doi:10.1038/

d41586-019-02310-3.

[12] C. Lattner, V. Adve, LLVM: A compilation framework for lifelong program analysis and transformation (2004) 75–86doi:

10.1109/cgo.2004.1281665.

[13] Z. Huo, G. Mei, G. Casolla, F. Giampaolo, Designing an efﬁcient parallel spectral clustering algorithm on multi-core

processors in Julia, Journal of Parallel and Distributed Computing 138 (2020) 211–221. doi:https://doi.org/10.

1016/j.jpdc.2020.01.003.

[14] T. Besard, C. Foket, B. De Sutter, Effective extensible programming: unleashing Julia on GPUs, IEEE Transactions on

Parallel and Distributed Systems 30 (4) (2018) 827–841. doi:10.1109/TPDS.2018.2872064.

[15] R. Huang, W. Xu, Y. Wang, S. Liverani, A. E. Stapleton, Performance Comparison of Julia Distributed Implementations of

Dirichlet Process Mixture Models, in: Proceedings - 2019 IEEE International Conference on Big Data, Big Data 2019, pp.

3350–3354. doi:10.1109/BigData47090.2019.9005453.

[16] L. Ruthotto, E. Treister, E. Haber, jlnv-A ﬂexible Julia package for PDE parameter estimation, Siam Journal on Scientiﬁc

Computing 39 (5) (2017) S702–S722. doi:10.1137/16m1081063.

27

[17] [link].

URL https://julialang.org/benchmarks/

[18] W. D. Penny, J. Kilner, F. Blankenburg, Robust Bayesian general linear models, Neuroimage 36 (3) (2007) 661–671. doi:

10.1016/j.neuroimage.2007.01.058.

[19] R. Frigola, Bayesian time series learning with Gaussian processes, University of Cambridge, 2015. doi:10.17863/

CAM.46480.

[20] C. Strickland, R. Burdett, K. Mengersen, R. Denham, PySSM: A Python Module for Bayesian Inference of Linear Gaussian

State Space Models, Journal of Statistical Software, Articles 57 (6) (2014) 1–37. doi:10.18637/jss.v057.i06.

[21] U. K. Mertens, A. Voss, S. Radev, ABrox A-user-friendly Python module for approximate Bayesian computation with a

focus on model comparison, Plos One 13 (3). doi:10.1371/journal.pone.0193981.

[22] H. Toosi, A. Moeini, I. Hajirasouliha, BAMSE: Bayesian model selection for tumor phylogeny inference among multiple

samples, BMC Bioinformatics 20. doi:10.1186/s12859-019-2824-3.

[23] J. Luttinen, BayesPy: Variational Bayesian inference in Python, Journal of Machine Learning Research 17 (2016) 1–6.

doi:https://dl.acm.org/doi/10.5555/2946645.2946686.

[24] A. Patil, D. Huard, C. J. Fonnesbeck, PyMC: Bayesian Stochastic Modelling in Python, Journal of Statistical Software

35 (4) (2010) 1–81. doi:10.18637/jss.v035.i04.

[25] V. Jarno, R. Jaakko, H. Jouni, J. Pasi, T. Ville, V. Aki, Bayesian Modeling with Gaussian Processes using the MATLAB

Toolbox GPstuff (v3.3), STATISTICS.

[26] L. Zhang, S. Agravat, G. Derado, S. Chen, B. J. McIntosh, F. D. Bowman, BSMac: A MATLAB toolbox implementing

a Bayesian spatial model for brain activation and connectivity, Journal of Neuroscience Methods 204 (1) (2012) 133–143.

doi:10.1016/j.jneumeth.2011.10.025.

[27] M. M. Cusumano-Towner, V. K. K. V. Mansinghka, A design proposal for Gen: Probabilistic programming with fast custom

inference via code generation, in: Proceedings - The 2nd ACM SIGPLAN International Workshop, 2018, p. 57. doi:

10.1145/3211346.3211350.

[28] M. Cox, T. van de Laar, B. de Vries, A factor graph approach to automated design of Bayesian signal processing algorithms,

International Journal of Approximate Reasoning 104 (2019) 185–204. doi:10.1016/j.ijar.2018.11.002.

[29] J. Bergstra, B. Komer, C. Eliasmith, D. Yamins, D. D. Cox, Hyperopt: A Python library for model selection and hyperpa-

rameter optimization, Computational Science and Discovery 8 (2015) 1. doi:10.1088/1749-4699/8/1/014008.

[30] [link].

URL https://github.com/KristofferC/NearestNeighbors.jl

[31] L. Breiman, Random forests machine learning, Machine Learning 45 (2001) 532. doi:10.1023/A:1010933404324.

[32] T. K. Ho, Random decision forests, in: Proceedings of 3rd International Conference on Document Analysis and Recognition,

Vol. 1, 1995, pp. 278–282 vol.1. doi:10.1109/ICDAR.1995.598994.

[33] Y. Zhou, P. Gallins, A Review and Tutorial of Machine Learning Methods for Microbiome Host Trait Prediction, Frontiers

in Genetics 10 (2019) 579. doi:10.3389/fgene.2019.00579.

[34] A. Upadhyay, A. Shetty, S. Kumar Singh, Z. Siddiqui, Land use and land cover classiﬁcation of LISS-III satellite image

using KNN and decision tree, in: 2016 3rd International Conference on Computing for Sustainable Global Development

(INDIACom), 2016, pp. 1277–1280. doi:https://ieeexplore.ieee.org/document/7724471.

28

[35] T. Keck, FastBDT: A speed-optimized and cache-friendly implementation of stochastic gradient-boosted decision trees for

multivariate classiﬁcation (2016). arXiv:1609.06119.

[36] F. Yang, X. Han, J. Lang, W. Lu, L. Liu, L. Zhang, J. Pan, Acm, Commodity Recommendation for Users Based on E-

commerce Data, Proceedings of the 2018 2nd International Conference on Big Data Research, 2018. doi:10.1145/

3291801.3291803.

[37] I. Seiferling, N. Naik, C. Ratti, R. Proulx, Green streets - Quantifying and mapping urban trees with street-level imagery

and computer vision, Landscape and Urban Planning 165 (2017) 93–101. doi:10.1016/j.landurbplan.2017.

05.010.

[38] V. Vapnik, The nature of statistical learning theory, Springer science and business media, 2013.

[39] S. Shalev-Shwartz, Y. Singer, N. Srebro, A. Cotter, Pegasos: primal estimated sub-gradient solver for SVM, Mathematical

Programming 127 (1) (2011) 3–30. doi:10.1007/s10107-010-0420-4.

[40] C. Chang, C. Lin, LIBSVM: A library for support vector machines, ACM transactions on intelligent systems and technology

(TIST) 2 (3) (2011) 27. doi:10.1145/1961189.1961199.

[41] P. M. Kebria, A. Khosravi, S. M. Salaken, S. Nahavandi, Deep imitation learning for autonomous vehicles based on con-

volutional neural networks, IEEE-CAA Journal of Automatica Sinica 7 (1) (2020) 82–95. doi:10.1109/jas.2019.

1911825.

[42] Y. Parmar, S. Natarajan, G. Sobha, DeepRange: deep-learning-based object detection and ranging in autonomous driving,

IET Intelligent Transport Systems 13 (8) (2019) 1256–1264. doi:10.1049/iet-its.2018.5144.

[43] [link].

URL https://github.com/mpastell/LIBSVM.jl

[44] J. Gwak, J. Jung, R. Oh, M. Park, M. A. K. Rakhimov, J. Ahn, A Review of Intelligent Self-Driving Vehicle Software

Research, Ksii Transactions on Internet and Information Systems 13 (11) (2019) 5299–5320. doi:10.3837/tiis.

2019.11.002.

[45] A. Abraham, F. Pedregosa, M. Eickenberg, P. Gervais, A. Mueller, J. Kossaiﬁ, A. Gramfort, B. Thirion, G. Varoquaux,

Machine learning for neuroirnaging with scikit-learn, Frontiers in Neuroinformatics 8 (2014) 14. doi:10.3389/fninf.

2014.00014.

[46] A. Jovic, K. Brkic, N. Bogunovic, An overview of free software tools for general data mining, in: 2014 37th International

Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), 2014, pp. 1112–

1117. doi:10.1109/MIPRO.2014.6859735.

[47] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,

V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, E. Duchesnay, Scikit-learn: Machine Learning

in Python, Journal of Machine Learning Research 12 (2011) 2825–2830.

[48] J. Demsar, T. Curk, A. Erjavec, C. Gorup, T. Hocevar, M. Milutinovic, M. Mozina, M. Polajnar, M. Toplak, A. Staric,

M. Stajdohar, L. Umek, L. Zagar, J. Zbontar, M. Zitnik, B. Zupan, Orange: Data Mining Toolbox in Python, Journal of

Machine Learning Research 14 (2013) 2349–2353.

[49] J. Demsar, B. Zupan, G. Leban, T. Curk, Orange: From experimental machine learning to interactive data mining, Vol. 3202

of Lecture Notes in Artiﬁcial Intelligence, 2004, pp. 537–539.

[50] Y. H. Shan, W. F. Lu, C. M. Chew, Pixel and feature level based domain adaptation for object detection in autonomous

29

driving, Neurocomputing 367 (2019) 31–38. doi:10.1016/j.neucom.2019.08.022.

[51] E. Arnold, O. Y. Al-Jarrah, M. Dianati, S. Fallah, D. Oxtoby, A. Mouzakitis, A Survey on 3D Object Detection Methods for

Autonomous Driving Applications, IEEE Transactions on Intelligent Transportation Systems 20 (10) (2019) 3782–3795.

doi:10.1109/tits.2019.2892405.

[52] C. Bishop, Neural Networks for Pattern Recognition, Oxford University Press, 1995.

[53] Y. Raja, S. McKenna, S. Gong, Segmentation and tracking using colour mixture models, Lecture Notes in Computer Science

1351 (1998) 607–614. doi:10.1007/3-540-63930-6_173.

[54] M. Bruneau, T. Mottet, S. Moulin, M. Kerbiriou, F. Chouly, S. Chretien, C. Guyeux, A clustering package for nucleotide

sequences using Laplacian Eigenmaps and Gaussian Mixture Model, Computers in Biology and Medicine 93 (2017) 66–74.

doi:10.1016/j.compbiomed.2017.12.003.

[55] T. W. S. Holoien, P. J. Marshall, R. H. Wechsler, EmpiriciSN: Re-sampling Observed Supernova/Host Galaxy Populations

Using an XD Gaussian Mixture Model, Astronomical Journal 153 (6). doi:10.3847/1538-3881/aa68a1.

[56] [link].

URL https://github.com/AmebaBrain/GmmFlow.jl

[57] [link].

URL https://github.com/davidavdav/GaussianMixtures.jl

[58] F. Srajer, Z. Kukelova, A. Fitzgibbon, A benchmark of selected algorithmic differentiation tools on some problems in

computer vision and machine learning, Optimization Methods and Software 33 (4-6) (2018) 889–906. doi:10.1080/

10556788.2018.1435651.

[59] G. Douzas, F. Bacao, F. Last, Improving imbalanced learning through a heuristic oversampling method based on k-means

and SMOTE, Information Sciences 465 (2018) 1–20. doi:10.1016/j.ins.2018.06.056.

[60] S. Yu, L. C. Tranchevent, X. H. Liu, W. Glanzel, J. A. K. Suykens, B. De Moor, Y. Moreau, Optimized Data Fusion for

Kernel k-Means Clustering, Ieee Transactions on Pattern Analysis and Machine Intelligence 34 (5) (2012) 1031–1039.

doi:10.1109/tpami.2011.255.

[61] K. Zhang, Y. X. Zhu, S. P. Leng, Y. J. He, S. Maharjan, Y. Zhang, Deep Learning Empowered Task Ofﬂoading for Mobile

Edge Computing in Urban Informatics, IEEE Internet of Things Journal 6 (5) (2019) 7635–7647. doi:10.1109/jiot.

2019.2903191.

[62] F. Corpet, Multiple sequence alignment with hierarchical-clustering, Nucleic Acids Research 16 (22) (1988) 10881–10890.

doi:10.1093/nar/16.22.10881.

[63] S. C. Johnson, Hierarchical clustering schemes, Psychometrika 32 (3) (1967) 241–54. doi:10.1007/bf02289588.

[64] G. Karypis, E. H. Han, V. Kumar, Chameleon: Hierarchical clustering using dynamic modeling, Computer 32 (8) (1999)

68–+. doi:10.1109/2.781637.

[65] D. Jaeger, J. Barth, A. Niehues, C. Fufezan, pyGCluster, a novel hierarchical clustering approach, Bioinformatics 30 (6)

(2014) 896–898. doi:10.1093/bioinformatics/btt626.

[66] D. Muellner, fastcluster: Fast Hierarchical, Agglomerative Clustering Routines for R and Python, Journal of Statistical

Software 53 (9) (2013) 1–18. doi:10.18637/jss.v053.i09.

[67] J. Kabzan, L. Hewing, A. Liniger, M. N. Zeilinger, Learning-Based Model Predictive Control for Autonomous Racing, Ieee

Robotics and Automation Letters 4 (4) (2019) 3363–3370. doi:10.1109/lra.2019.2926677.

30

[68] S. Datta, Hierarchical stellar clusters in molecular clouds, IAU Symposium Proceedings Series, 2010, pp. 377–379. doi:

10.1017/s1743921309991396.

[69] G. Kerr, H. J. Ruskin, M. Crane, P. Doolan, Techniques for clustering gene expression data, Computers in Biology and

Medicine 38 (3) (2008) 283–293. doi:10.1016/j.compbiomed.2007.11.001.

[70] J. Jacques, C. Biernacki, Model-based co-clustering for ordinal data, Computational Statistics and Data Analysis 123 (2018)

101–115. doi:10.1016/j.csda.2018.01.014.

[71] A. Pessia, J. Corander, Kpax3: Bayesian bi-clustering of large sequence datasets, Bioinformatics 34 (12) (2018) 2132–2133.

doi:10.1093/bioinformatics/bty056.

https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.

[72] [link].

URL

html

[73] [link].

URL https://www.alglib.net/dataanalysis/principalcomponentsanalysis.php

[74] [link].

URL https://docs.opencv.org/master/d1/dee/tutorial_introduction_to_pca.html

[75] [link].

URL https://github.com/JuliaStats/MultivariateStats.jl

[76] S. Vorobyov, A. Cichocki, Blind noise reduction for multisensory signals using ICA and subspace ﬁltering, with application

to EEG analysis, Biological Cybernetics 86 (4) (2002) 293–303. doi:10.1007/s00422-001-0298-6.

[77] X. Ren, X. Hu, Z. Wang, Z. Yan, MUAP extraction and classiﬁcation based on wavelet transform and ICA for

EMG decomposition, Medical and Biological Engineering and Computing 44 (5) (2006) 371–382. doi:10.1007/

s11517-006-0051-3.

[78] E. B. Assi, S. Rihana, M. Sawan, Kmeans-ICA based automatic method for ocular artifacts removal in a motorimagery

classiﬁcation, in: 2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,

2014, pp. 6655–6658. doi:10.1109/EMBC.2014.6945154.

[79] [link].

URL

https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.

FastICA.htm

[80] [link].

URL https://www.mathworks.com/matlabcentral/fileexchange/38300-pca-and-ica-package?

s_tid=prof_contriblnk

[81] M. Fey, J. E. Lenssen, Fast Graph Representation Learning with PyTorch Geometric (2019). arXiv:1903.02428.

[82] J. Shen, P. Nguyen, Y. Wu, Z. Chen, M. X. Chen..., Lingvo: a Modular and Scalable Framework for Sequence-to-Sequence

Modeling (2019). arXiv:1902.08295.

[83] Y.-P. Tang, G.-X. Li, S.-J. Huang, ALiPy: Active Learning in Python (2019). arXiv:1901.03802.

[84] J. Tang, L. Ericson, J. Folkesson, P. Jensfelt, GCNv2: Efﬁcient Correspondence Prediction for Real-Time SLAM, IEEE

Robotics and Automation Letters 4 (4) (2019) 3505–3512. doi:10.1109/LRA.2019.2927954.

[85] Z. Huang, L. Huang, Y. Gong, C. Huang, X. Wang, Mask Scoring R-CNN (2019). arXiv:1903.00241.

31

[86] N. Frazier-Logue, S. J. Hanson, Dropout is a special case of the stochastic delta rule: faster and more accurate deep learning

(2018). arXiv:1808.03578.

[87] S. J. Hanson, A stochastic version of the delta rule, Physica. Section D: Nonlinear Phenomena 42 (1990) 265–272. doi:

10.1016/0167-2789(90)90081-Y.

[88] L. Luo, Y. Xiong, Y. Liu, X. Sun, Adaptive Gradient Methods with Dynamic Bound of Learning Rate (2019). arXiv:

1902.09843.

[89] Marcus D. Bloice and Christof Stocker and Andreas Holzinger, Augmentor: An image augmentation library for machine

learning (2017). arXiv:1708.04680.

[90] A. Krizhevsky, I. Sutskever, G. E. Hinton, ImageNet Classiﬁcation with Deep Convolutional Neural Networks, Communi-

cations of the Acm 60 (6) (2017) 84–90. doi:10.1145/3065386.

[91] Y.-R. Liu, Y.-Q. Hu, H. Qian, Y. Yu, C. Qian, ZOOpt: Toolbox for Derivative-Free Optimization (2017). arXiv:1801.

00329.

[92] [link].

URL https://github.com/denizyuret/Knet.jl

[93] [link].

URL https://github.com/FluxML/Flux.jl

[94] [link].

URL https://github.com/malmaud/TensorFlow.jl

[95] G. Ditzler, J. C. Morrison, Y. Lan, G. L. Rosen, Fizzy: Feature subset selection for metagenomics 16 (2015) 358. doi:

10.1186/s12859-015-0793-8.

[96] B. Wicht, A. Fischer, J. Hennebert, DLL: A Fast Deep Neural Network Library, Vol. 11081 of Lecture Notes in Artiﬁcial

Intelligence, 2018, pp. 54–65. doi:10.1007/978-3-319-99978-4_4.

[97] C. Rackauckas, M. Innes, Y. Ma, J. Bettencourt, L. White, V. Dixit, DiffEqFlux.jl - A Julia Library for Neural Differential

Equations (2019). arXiv:1902.02376.

[98] [link].

URL https://github.com/JuliaDiffEq/DifferentialEquations.jl

[99] G. B. Huang, Q. Y. Zhu, C. K. Siew, Extreme learning machine: A new learning scheme of feedforward neural networks,

IEEE International Joint Conference on Neural Networks (IJCNN), 2004, pp. 985–990. doi:10.1109/IJCNN.2004.

1380068.

[100] L. L. C. Kasun, H. Zhou, G.-B. Huang, C. M. Vong, Representational Learning with ELMs for Big Data, IEEE Intelligent

Systems 28 (6) (2013) 31–34.

[101] J. Tang, C. Deng, G.-B. Huang, Extreme learning machine for multilayer perceptron, IEEE Transactions on Neural Networks

and Learning Systems 27 (4) (2015) 809–821. doi:10.1109/TNNLS.2015.2424995.

[102] Z. C. Ouyang, J. W. Niu, Y. Liu, M. Guizani, Deep CNN-Based Real-Time Trafﬁc Light Detector for Self-Driving Vehicles,

IEEE Transactions on Mobile Computing 19 (2) (2020) 300–313. doi:10.1109/tmc.2019.2892451.

[103] I. Lee, K. Lee, The Internet of Things (IoT): Applications, investments, and challenges for enterprises, Business Horizons

58 (4) (2015) 431–440. doi:10.1016/j.bushor.2015.03.008.

[104] J. Gubbi, R. Buyya, S. Marusic, M. Palaniswami, Internet of Things (IoT): A vision, architectural elements, and future

32

directions, Future Generation Computer Systems-the International Journal of Escience 29 (7) (2013) 1645–1660. doi:

10.1016/j.future.2013.01.010.

[105] L. Atzori, A. Iera, G. Morabito, The Internet of Things: A survey, Computer Networks 54 (15) (2010) 2787–2805. doi:

10.1016/j.comnet.2010.05.010.

[106] G. Mei, N. Xu, J. Qin, B. Wang, P. Qi, A Survey of Internet of Things (IoT) for Geo-hazards Prevention: Applications,

Technologies, and Challenges, IEEE Internet of Things Journal (2019) 1–1doi:10.1109/JIOT.2019.2952593.

[107] M. Mohammadi, A. Al-Fuqaha, S. Sorour, M. Guizani, Deep Learning for IoT Big Data and Streaming Analytics: A Survey,

IEEE Communications Surveys And Tutorials 20 (4) (2018) 2923–2960. doi:10.1109/comst.2018.2844341.

[108] M. S. Mahdavinejad, M. Rezvan, M. Barekatain, P. Adibi, P. Barnaghi, A. P. Sheth, Machine learning for Internet of Things

data analysis: A survey, Digital Communications and Networks 4 (3) (2018) 161–175. doi:10.1016/j.dcan.2017.

10.002.

[109] [link].

URL https://invenia.github.io/blog/

[110] [link].

URL https://juliacomputing.com/case-studies/fugro-roames-ml.html

[111] B. T. Grys, D. S. Lo, N. Sahin, O. Z. Kraus, Q. Morris, C. Boone, B. J. Andrews, Machine learning and computer vision

approaches for phenotypic proﬁling, Journal of Cell Biology 216 (1) (2017) 65–71. doi:10.1083/jcb.201610026.

[112] N. Naik, S. D. Kominers, R. Raskar, E. L. Glaeser, C. A. Hidalgo, Computer vision uncovers predictors of physical urban

change, Proceedings of the National Academy of Sciences of the United States of America 114 (29) (2017) 7571–7576.

doi:10.1073/pnas.1619003114.

[113] A. K. Patel, S. Chatterjee, Computer vision-based limestone rock-type classiﬁcation using probabilistic neural network,

Geoscience Frontiers 7 (1) (2016) 53–60. doi:10.1016/j.gsf.2014.10.005.

[114] K. Gopalakrishnan, S. K. Khaitan, A. Choudhary, A. Agrawal, Deep Convolutional Neural Networks with transfer learning

for computer vision-based data-driven pavement distress detection, Construction and Building Materials 157 (2017) 322–

330. doi:10.1016/j.conbuildmat.2017.09.110.

[115] Y. J. Cha, J. G. Chen, O. Buyukozturk, Output-only computer vision based damage detection using phase-based optical ﬂow

and unscented Kalman ﬁlters, Engineering Structures 132 (2017) 300–313. doi:10.1016/j.engstruct.2016.11.

038.

[116] [link].

URL https://github.com/FluxML/Metalhead.jl

[117] [link].

URL https://github.com/peterkovesi/ImageProjectiveGeometry.jl

[118] J. Hirschberg, C. D. Manning, Advances in natural language processing, Science 349 (6245) (2015) 261–266. doi:

10.1126/science.aaa8685.

[119] S. Poria, E. Cambria, A. Gelbukh, Aspect extraction for opinion mining with a deep convolutional neural network,

Knowledge-Based Systems 108 (2016) 42–49. doi:10.1016/j.knosys.2016.06.009.

[120] T. Young, D. Hazarika, S. Poria, E. Cambria, Recent Trends in Deep Learning Based Natural Language Processing, IEEE

Computational Intelligence Magazine 13 (3) (2018) 55–75. doi:10.1109/mci.2018.2840738.

33

[121] M. Gimenez, J. Palanca, V. Botti, Semantic-based padding in convolutional neural networks for improving the performance

in natural language processing. A case of study in sentiment analysis, Neurocomputing 378 (2020) 315–323. doi:10.

1016/j.neucom.2019.08.096.

[122] W. B. Liu, Z. D. Wang, X. H. Liu, N. Y. Zengb, Y. R. Liu, F. E. Alsaadi, A survey of deep neural network architectures and

their applications, Neurocomputing 234 (2017) 11–26. doi:10.1016/j.neucom.2016.12.038.

[123] [link].

URL https://github.com/hshindo/LightNLP.jl

[124] H. F. Liu, X. F. Han, X. R. Li, Y. Z. Yao, P. Huang, Z. M. Tang, Deep representation learning for road detection using Siamese

network, Multimedia Tools and Applications 78 (17) (2019) 24269–24283. doi:10.1007/s11042-018-6986-1.

[125] L. G. Cuenca, E. Puertas, J. F. Andres, N. Aliane, Autonomous Driving in Roundabout Maneuvers Using Reinforcement

Learning with Q-Learning, Electronics 8 (12) (2019) 13. doi:10.3390/electronics8121536.

[126] C. Desjardins, B. Chaib-draa, Cooperative Adaptive Cruise Control: A Reinforcement Learning Approach, IEEE Transac-

tions on Intelligent Transportation Systems 12 (4) (2011) 1248–1260. doi:10.1109/tits.2011.2157145.

[127] S. Samsi, V. Gadepally, M. Hurley, M. Jones, E. Kao, S. Mohindra, P. Monticciolo, A. Reuther, S. Smith, W. Song,

D. Staheli, J. Kepner, Static graph challenge: Subgraph isomorphism, in: 2017 IEEE High Performance Extreme Computing

Conference (HPEC), 2017, pp. 1–6. doi:10.1109/HPEC.2017.8091039.

[128] [link].

URL https://github.com/JuliaGraphs/LightGraphs.jl

[129] B. Uengtrakul, D. Bunnjaweht, Ieee, A Cost Efﬁcient Software Deﬁned Radio Receiver for Demonstrating Concepts in

Communication and Signal Processing using Python and RTL-SDR, International Conference on Digital Information and

Communication Technology and it’s Applications, 2014, pp. 394–399.

[130] K. Gideon, C. Nyirenda, C. Temaneh-Nyah, Echo state network-based radio signal strength prediction for wireless commu-

nication in Northern Namibia, Iet Communications 11 (12) (2017) 1920–1926. doi:10.1049/iet-com.2016.1290.

[131] P. Srivastava, M. Kang, S. K. Gonugondla, S. Lim, J. Choi, V. Adve, N. S. Kim, N. Shanbhag, PROMISE: An End-to-End

Design of a Programmable Mixed-Signal Accelerator for Machine-Learning Algorithms, Conference Proceedings Annual

International Symposium on Computer Architecture, 2018, pp. 43–56. doi:10.1109/isca.2018.00015.

[132] C. M. Bishop, Pattern recognition and machine learning, Springer, 2006.

[133] R. Milewski, V. Govindaraju, Binarization and cleanup of handwritten text from carbon copy medical form images, Pattern

Recognition 41 (4) (2008) 1308–1315. doi:10.1016/j.patcog.2007.08.018.

[134] R. S. Anand, P. Stey, S. Jain, D. R. Biron, H. Bhatt, K. Monteiro, E. Feller, M. L. Ranney, I. N. Sarkar, E. S. Chen, Predicting

Mortality in Diabetic ICU Patients Using Machine Learning and Severity Indices, AMIA Jt Summits Transl Sci Proc 2017

(2018) 310–319.

[135] [link].

URL https://github.com/JuliaStats/GLM.jl

[136] [link].

URL https://github.com/JuliaText/Languages.jl

[137] [link].

URL https://julialang.org/blog/2019/08/2019-julia-survey

34

[138] H. Rong, J. Park, L. Xiang, T. A. Anderson, M. Smelyanskiy, Sparso: Context-driven Optimizations of Sparse Linear

Algebra, 2016 International Conference on Parallel Architecture and Compilation Techniques, 2016. doi:10.1145/

2967938.2967943.

[139] G. Plumb, D. Pachauri, R. Kondor, V. Singh, SnFFT: A Julia Toolkit for Fourier Analysis of Functions over Permutations,

Journal of Machine Learning Research 16 (2015) 3469–3473.

35

r
e
h
t
O

M
L
E

N
N
A

p
e
e
D

g
n
i
n
r
a
e
L

A
C

I

A
C
P

g
n
i
r
e
t
s
u
l
C

g
n
i
r
e
t
s
u
l
C

-
i

B

l
a
c
i
h
c
r
a
r
e
i
H

i

g
n
n
r
a
e
L
d
e
s
i
v
r
e
p
u
s
n
U

s
n
a
e
m
-
k

M
M
G

n
o
i
s
s
e
r
g
e
R

s
i
s
y
l
a
n
A

M
V
S

m
o
d
n
a
R

t
s
e
r
o
F

g
n
i
n
r
a
e
L
d
e
s
i
v
r
e
p
u
S

s
e
g
a
k
c
a
p

e
g
a
u
g
n
a
l

a
i
l
u
J

d
e
s
u

y
l
n
o
m
m
o
C

:
1

e
l
b
a
T

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

N
N
k

(cid:88)

n
a
i
s
e
y
a
B

l
e
d
o
M

(cid:88)

(cid:88)

.
f
e
R

s
e
g
a
k
c
a
P
a
i
l
u
J

]
8
2
[

]
0
3
[

]
7
3
[

]
1
4
[

]
2
4
[

]
3
4
[

]
4
4
[

]
0
5
[

]
1
5
[

]
1
6
[

]
7
6
[

]
8
[

]
5
7
[

]
1
9
[

]
2
9
[

]
3
9
[

]
4
9
[

]
7
9
[

]
8
9
[

]
8
5
[

]
2
0
1
[

]
6
5
[

]
7
5
[

l
j
.
b
a
L
y
e
n
r
o
F

l
j
.
s
r
o
b
h
g
i
e
N
t
s
e
r
a
e
N

l
j
.
e
e
r
T
n
o
i
s
i
c
e
D

l
j
.

M
V
S

l
j
.
J
L
M

l
j
.

M
V
S
B
I
L

l
j
.
n
r
a
e
L

t
i
k
i
c
S

l
j
.
n
o
i
s
s
e
r
g
e
R

l
j
.
k
s
i
R
l
a
c
i
r
i
p
m
E

l
j
.
g
n
i
r
e
t
s
u
l
C

l
j
.
t
e
N
X
M

l
j
.
t
e
n
K

l
j
.
x
u
l
F

l
j
.

w
o
l
F
r
o
s
n
e
T

l
j
.
x
u
ﬂ
q
ﬁ
f
i

D

l
j
.
s
n
o
i
t
a
u
q
E
l
a
i
t
n
e
r
e
f
f
i

D

l
j
.
t
e
n
l
a
r
u
e
n
p
o
r
p
k
c
a
B

l
j
.
g
n
i
r
e
t
s
u
l
C

t
f
i
h
S
k
c
i
u
Q

l
j
.
s
t
a
t
S
e
t
a
i
r
a
v
i
t
l
u
M

l
j
.
3
x
a
p
k

36

l
j
.

m
E

l

l
j
.

w
o
l
F
m
m
G

l
j
.
s
e
r
u
t
x
i
M
n
a
i
s
s
u
a
G

