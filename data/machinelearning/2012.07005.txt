Predicting Software Effort from Use Case Points: A Systematic Review 

Mohammad Azzeh 
Department of Software 
Engineering 
Applied Science University 
Amman, Jordan POBOX 166 
m.y.azzeh@asu.edu.jo 

Ali Bou Nassif 
Department of Computer 
Engineering 
University of Sharjah 
Sharjah, UAE 
anassif@sharjah.ac.ae 

Imtinan Basem Attili  
Department of Electrical 
Engineering 
University of Sharjah 
Sharjah, UAE 
iattili@sharjah.ac.ae 

Abstract.  
Context: Predicting software project effort from Use Case  Points (UCP) method is increasingly used among 
researchers and practitioners. However, unlike other effort estimation domains, this area of interest has not 
been systematically reviewed. 
Aims: There is a need for a systemic literature review to provide directions and supports for this research area 
of effort estimation. Specifically, the objective of this study is twofold: 1) to classify UCP effort estimation papers 
based on four criteria: contribution type, research approach, dataset type and techniques used with UCP; and 
2) to analyze these papers from different views: estimation accuracy, favorable estimation context and impact 
of combined techniques on the accuracy of UCP. 
Method: We used the systematic literature review methodology proposed by Kitchenham and Charters. This 
includes searching for the most relevant papers, selecting quality papers, extracting data and drawing results. 
Result:  The  authors  of  UCP  research  paper,  are  generally  not  aware  of  previous  published  results  and 
conclusions in the field of UCP effort estimation. There is a lack of UCP related publications in the top software 
engineering journals. This makes a conclusion that such papers are not useful for the community. Furthermore, 
most articles used small numbers of projects which cannot support generalizing the conclusion in most cases. 
Conclusions: There are multiple research directions for UCP method that have not been examined so far such 
as validating the algebraic construction of UCP based on industrial data. Also,  there is a need for standard 
automated tools that govern the process of translating use case diagram into its corresponding UCP metrics. 
Although there is an increase interest among researchers to collect industrial data and build effort prediction 
models based on machine learning methods, the quality of data is still subject to debate.  

Keywords: Systematic Literature Review, Use Case Points, Effort Estimation. 

Introduction 

1. 
Use  Case  Points  (UCP)  has  been  studied  intensively  in  the  past  two  decades  as  an  alternative  method  to 
Function Points, with a goal to predicting software effort at early software development phases [1][2][3]. In 
spite  of  the  widespread  of  use  case  techniques  in  software  industry,  the  UCP  method  still  confronts  some 
challenges such as: (1) lack of sufficient public industrial data [4], (2) different human interpretation [5], (3) the 
debate about algebraic construction of UCP method [1], (4) the way that the use case complexity is measured, 
and most importantly (5) there is no stable technique in literature that enables practitioners to produce most 
likely effort from UCP size metrics. remarkably, not all published research studies on UCP provide significant 

 
 
 
 
 
 
contributions to body of knowledge of the research field. In fact, a good number of studies attempt to only run 
examples on UCP method without supporting the community with useful conclusions. Therefore, there is a need 
for a Systematic Literature Review (SLR) that can help in classifying and analyzing the published studies with 
a goal to structure this field of interest and provide directions and support for future research [6][7]. Despite 
many systematic literature reviews on effort estimation, the field of effort estimation based on UCP is still not 
reviewed  carefully.  It  is  well  known  that  the  goal  of  SLRs  is  not  only  aggregate  all  existing  evidence  on 
research  questions,  but  also  to  support  the  development  of  evidence-based  guidelines  for  practitioners 
[6][8][9].  

This paper introduces a systematic literature review for UCP method. This allows us to: 1) build a classification 
scheme and structuring the field of interest, and 2) provide recommendations based on the strength of the 
evidence of UCP effort estimation performance in the current research. In fact, we reviewed over 120 published 
research studies on effort estimation based on UCP. These studies have been identified from different research 
venues in the period from 1993 to February 2020. We chose the year of 1993 because the original model of UCP 
effort estimation was proposed in that year by Karner [10]. In order to ensure the quality of these studies and 
their  benefit  for  our  systematic  mapping  and  literature  review,  we  followed  the  procedure  proposed  by 
Kitchenham and Charters [11] based on evidence-based software engineering. Basically, we identified some 
mapping and review questions in order to classify the published UCP effort estimation studies based on various 
criteria. To facilitate the searching process, we constructed a comprehensive search query to find candidate 
papers. Then, inclusion and exclusion criteria are used to filter the most relevant papers, in addition, the quality 
assessment is run on the selected papers in order to filter the most quality papers that help us to draw better 
conclusion.  lastly,  the  data  extraction,  analysis  and  synthesis  are  performed  to  extract  knowledge  and 
conclusions. At the end of these steps we ended up with 75 quality papers that help us to address mapping and 
review questions. Our paper is different from previous SLRs in the following points: 

1.  Different focus: The focus of this study is on a narrower field of effort estimation which is based on 
UCP size metric only. The UCP method itself has not be reviewed thoroughly in previous SLRs as a 
mean of effort estimation. 

2.  Structuring the field of interest:  this  study  aims  to  build  classification  scheme  based  on  effort 
estimation and UCP. The classification scheme includes classifying and studying UCP papers based on 
their contribution, favorable prediction techniques, research approaches and nature of datasets.  

The rest of the paper is organized as follows: Section 2 presents related work on previous SLR studies in the 
field of software effort estimation. Section 3 introduces the method of SLR that we used in this paper. Sections 
4 and 5 present results of mapping and review questions. Sections 6 and 7 present discussion on the findings 
and conclusions.  

2.  Related Work 
When researchers encounter software engineering literature, they found multiple systematic review studies 
on effort estimation, but just one of them was designed for UCP effort estimation. The reason behind that might 
be  because  they  involve  UCP  effort  estimation  methods  with  other  effort  estimation  methods.  A  recent 

 
 
 
 
 
systematic literature review study was conducted by Mahmood et al. [12] based on UCP and experts judgement 
software effort estimation methods. They identified only 34 papers within the period 2000 to 2019. The main 
theme of this study is to examine the published paper on UCP and expert judgement in terms of several point 
of views including research contribution, dataset usage, accuracy metrics, and findings of the selected studies. 
They  found  that  expert  judgment  estimation  technique  is  the  most  frequently  used  in  effort  estimation.  In 
addition to that, the industrial datasets are the most used type of data in UCP method.  

The main difference between our SLR study and Mahmood et al. [12] study are: 

1.  Different  theme,  they  focus  on  two  estimation  approaches  which  are  Use  Case  Points  and  Expert 
Judgement. Among 34 studies, only 25 studies are related to UCP. Therefore, great number of valuable 
publications in the domain has been ignored.  

2.  Our SLR is a mapping and review studies, while their study is just review. 
3.  Search string, inclusion and exclusion criteria and research questions are quietly different. 
4.  Mahmood et al. [12] focused on public datasets that are not related to UCP, whereas we focused our 

study on UCP public and private datasets. 

On the other side, we present a literature for common SLR studies in software effort estimation. Jørgensen 
and Shepperd [9] conducted a systematic literature review on software development cost estimation studies. 
The  purpose  of  this  SLR  was  to  increase  awareness  of  researchers  with  quality  and  reliability  of  previous 
published studies. They identified 304 relevant conference and journal papers published in different venues. 
The outcomes of this SLR provided recommendations about identifying relevant research papers and selecting 
appropriate research venues. They also recommended using industrial software effort estimation methods and 
datasets.     

Idri et al. [7] conducted a systematic mapping and literature review on analogy-based effort estimation. They 
used a comprehensive SLR process to cover all aspects of analogy-based research area including estimation 
accuracy and context. The results of this SLR focused the light on the current research directions on this kind 
of  estimation  and  open  new  research  questions  on  how  to  improve  the  visibility  of  analogy-based  effort 
estimation. 

Usman  et  al.  [13]  conducted  a  systematic  literature  review  on  software  effort  estimation  in  agile  software 
development.  They  selected  32  quality  papers  out  of  443  candidate  papers.  Among  various  estimation 
techniques,  they  found  expert  judgment,  planning  poker,  and  use  case  points  method  are  the  most  used 
estimation techniques. However, none of them achieved good accuracy. Furthermore, there was a debate about 
which reliable cost drivers that should be applied for this kind of estimation. The story points and use case 
points were the most used size metric in agile software development.  

Kitchenham  et  al.  [6]  investigated  and  evaluated  the  previous  SLR  studies  in  software  engineering.  They 
conducted a systematic review and showed that eight out of twenty relevant studies addressed research trends 
rather than technique evaluation, and seven SLRs addressed cost estimation. The study suggests the following: 
1) mainstream software engineering topics are not well represented. 2) the Simula Research Laboratory in 

 
 
 
 
 
Norway is the leading software engineering institution in terms of undertaking SLRs. 3) the current output of 
Evidence Based Software Engineering (EBSE) articles is strongly supported by European researchers. 

In the same direction MacDonell et al. [14], investigated the reliability of previous systematic reviews in the 
context  of  empirical  software  engineering.  They  conducted  two  parallel  reviews  of  the  same  research 
questions based on an agreed metaprotocol with a goal to measure the outcome stability of systematic reviews. 
Both teams worked separately in parallel. The results found that there is no significant difference between two 
outcomes,  which  showed  that  the  SLR  proved  to  be  robust  to  differences  in  process  and  produced  stable 
outcomes. 

   Method 

3. 
The procedure of SLR that we used in this study includes five main steps, as shown in Figure 1. In the first step, 
we identified main mapping and review questions. The goal of mapping review is to enable us classifying the 
published UCP effort estimation studies based on various criteria. In the second step, we construct the search 
query that will be used to find candidate papers on the common digital libraries such as IEEE Xplore, ACM, 
Science Direct, Wiley and Springer Link. In the third step, the inclusion and exclusion criteria are applied on 
all candidate papers to filter the most relevant papers. During this step, the duplicate papers are also removed. 
Later, the quality assessment is run on the selected papers from the previous step in order to filter the most 
quality papers that help us to draw better conclusion and extract required knowledge. In the last steps, the 
data  acquisition,  analysis  and  synthesis  are  performed  in  order  to  extract  knowledge  and  findings.  In  the 
upcoming subsections the complete description of each step is discussed in more details. 

analyze and 
synthesize data
•Data 

synthesize

Extract data
•Data 

extraction

Searching for 
candidate articles
•Search Query
•Search Process

Filtering the 
relevant papers
•Inclusion 
Criteria
•Exclusion 
Criteria 

Running quality 
assessment on the 
selected papers
•Quality 

Questions

Fig. 1 Mapping and Review Process 

Identifying 
Research 
Questions
•Mapping 
Questions 

•Review 

Questions

3.1  Research Questions 
Nine research questions (5 mapping questions and 4 review questions) were proposed to achieve the goals of 
this SLR study. The obtained review results enable researchers to understand the main types of contributions 
and challenges in this field of interest. The synthesized data can usually serve as basis for recommendations 
of the best practices. The Nine research questions have been proposed and reviewed by three authors in order 
to avoid inconsistency among them. At glance, these questions are classified into two categories: mapping and 
review questions. The mapping questions (MQ1 to MQ5) aim at building a classification scheme and structuring 

 
 
 
 
 
 
the field of interest. The classification criteria that we used are described in Table 1. The review questions (RQ1 
to RQ4) aim at providing recommendations based on the strength of the evidence of UCP effort estimation 
performance in current research. The mapping and review questions are summarized as follows: 
-  Mapping Questions: 

o  MQ1. Which and how many sources include papers on effort estimation based on UCP? 
o  MQ2. What are the main contributions of UCP research papers? 
o  MQ3. What are the main types of research approaches applied to effort estimation based on UCP? 
o  MQ4. What are the most favorable techniques that are used to produce effort estimates from UCP, 

and how has this changed over time? 

o  MQ5. What are the types of datasets that are used to run experiments for UCP effort estimation? 

-  Review Questions: 

o  RQ1. How easy is to collect UCP data from software industry and what is the implication on effort 

estimation? 

o  RQ2. What are the impacts of combining other techniques with an UCP method technique on its 

estimation accuracy?  

o  RQ3. What is the most favorable context for UCP effort estimation method? 
o  RQ4. To what extent are researchers on UCP methods aware of the breadth of potential estimation 

study sources? 

Property 
Contribution Type 
Research 
Approaches 
Techniques used 
in combination 
with UCP 

Dataset Type 

Table 1. Classification Criteria 

Categories 

technique, tool, comparison, validation, Metric and model 
Proposal, Evaluation, Review, Case Study, Survey, Theory, 
Comparative studies 

Fuzzy Logic Neural Networks, Multiple Linear Regression, 
Support Vector Regression, Stepwise Regression 

Industrial, Educational, Mix of Both Industrial and Educational, 
and Case studies. 

The  purpose  of  MQ1  is  to  support  researchers  with  the  most  relevant  studies  in  UCP  effort  estimation.  To 
address  MQ1,  we  have  reviewed  the  common  venues  of  effort  estimation  publications  such  as  “IEEE 
Transactions  on  Software  Engineering,  ACM  Transactions  on  Software  Engineering  and  Methodology, 
Information  and  Software  Technology,  Journal  of  Systems  and  Software,  Empirical  Software  Engineering.” 
Also, some papers published in different journals or conference proceedings have been reviewed. MQ2 aims 
to list various types of contributions in UCP. We identified the possible main contributions in the field of interest. 
These contributions are classified to 1) technique, 2) tool, 3) comparison, 4) validation, 5) metric and 6) model 
as  shown  and  described  in  Table  2.  MQ3  attempts  to  identify  the  potential  categories  of  applied  research 
approaches for UCP effort estimation studies. We have identified seven research categories that have been 
applied in this field of interest as shown and described in Table 3. 

 
 
 
 
 
Contribution Type 
Technique 
Tool 

Comparison 

Validation 

Enhancement 

Model 

Research Approach 
Proposal 
Evaluation 
Review 

Case Study 

Survey 

Theory 

Comparative studies 

Table 2. Types of contributions 

Description 
A new UCP technique is proposed, or existing one is improved.  
Proposing a new tool for UCP effort estimation 
A comparison among different configurations of UCP, or comparison 
with other techniques.  
An evaluation of the performance of existing UCP techniques using 
some historical datasets. 
An enhancement made to existing or original UCP model. 
A new UCP effort estimation model is developed using machine 
learning or data mining methods. 

Table 3. Types of research approaches 
Explanation 
A new model or improved model for UCP. 
Evaluating existing UCP or effort estimation models based UCP. 
Studies that reviewed UCP or Effort estimation based on UCP. 
Empirical evaluation of UCP and effort estimation based on case 
studies.  
Studies that provide comprehensive survey for UCP. 
Studies that evaluate UCP structure or part of its properties 
theoretically, not empirically. 
Studies that provide a comparison among different effort estimation 
models based on UCP. 

To address MQ4, we identify the most prediction techniques that are used to generate effort estimates from 
UCP. We have identified all possible methods that were used to build effort estimation model based on UCP 
such as: Karner (i.e. original model of UCP) [10], Fuzzy Logic [3], [15], Neural Networks [17][18], Multiple Linear 
Regression [18]. Finally, MQ5 is designed to investigate the type of data the was used to validate and check 
accuracy of various UCP effort estimation models. The datasets types were classified into four categories: 1) 
Industrial, 2) Educational, 3) Mix of Both Industrial and Educational, and finally 4) Case studies.  

With respect to RQ1, we discuss the impact of used data and validation technique on the accuracy of UCP effort 
estimation. We discuss the accuracy for different types of used datasets. For RQ2, we identify the impact of 
using other machine learning and data mining techniques on the accuracy of UCP method. RQ3 aims to identify 
the different UCP research contexts that are most favorable among researchers. we identify the challenges 
that appear when using student projects and case studies for building effort estimation based UCP models. we 
consider the assessment of the vulnerability of UCP computing procedure (using transaction points or steps) 
and weights on effort estimation. Finally, RQ4 intends to shade the light on the possible shortcomings when 
searching for relevant work. 

 
 
 
 
 
3.2  Search process 
To address the proposed research questions, we made an automated search query, using predefined searching 
terms, in the common research digital libraries and electronic databases such as “IEEE Digital Library, ACM 
Digital  Library,  Science  Direct,  Wiley,  Springer”.  These  databases  were  selected  because,  from  experience, 
most of the journals and proceeding of the selected papers were indexed by these databases. Furthermore, 
Google Scholar was also used to search for papers that were not indexed in the above libraries because it 
explores other digital libraries on the Internet.      

With respect to query string formulation, we followed the SLR guidelines mentioned in [11]. The query string is 
constructed using all important terms then these terms are linked using AND/OR operators. OR operator is 
used to link all synonyms and variation of each incorporated terms, then the AND operator is used to connect 
the  main  terms.  All  authors  have  participated  in  the  query  construction  and  revision  to  ensure  its 
comprehensive  and  correctness.  The  formulated  query  string  has  been  tested  in  some  digital  libraries  to 
ensure its quality. The initial query string did not include terms such as Lifecyle and Simplifying which they 
have been added after testing on some digital libraries such as IEEE digital library and Science Direct.  The 
query string has been executed in February 2020. Keywords from other known studies (e.g. title, abstract and 
keywords  from  well-known  studies)  have  been  added  if  they  were  not  part  of  the  initial  query  string.  The 
complete proposed query string is: 

(“Use Case Points” OR “Use Cases” OR UCP) AND (Project OR Software OR System OR Application OR 
Product OR Development OR Lifecycle) AND (Effort OR Cost) AND (Prediction OR Predicting OR Estimation 
OR Estimating OR Evaluating OR Simplifying)  

In the initial search stage, we used the proposed query string to search for candidate papers in the mentioned 
digital libraries. In the second search stage, we have reviewed the reference lists of the filtered studies that 
met inclusion and exclusion criteria (see section 3.3) to identify papers related to UCP based on their title. The 
papers that were found highly relevant are then added to the list of relevant papers. Furthermore, some papers 
that we already aware of have been used to control the quality of the search as shown in Table 4. We recorded 
for each control paper the digital library from which it was retrieved before and after the search. Only four 
cases present mismatch result because of the sequence of digital library search (IEEE, ACM, Springer, Science 
Direct, Wiley and Google Scholar). This enables us to assess whether or not the initial search stage had missed 
any  highly  relevant  papers  and to  ensure  that  the  search covered  the  maximum  number  of  available  UCP 
studies. 

According  to  Kitchenham  et  al.  [11],  we  also  formulated  the  query  string  elements  using  Population, 
Intervention, Comparison, Outcome, Context) PCIOC approach: 

Population: software project.  
Intervention: Use Case Points effort estimation model.  
Comparison: Variants of Use Case Points effort estimation model  
Outcomes: Prediction or estimate accuracy. 

 
 
 
 
 
 
 
Table 4. List of known existing papers used to validate the search string. 

Paper Id 
S5 
S6 
S8 
S9 
S20 
S24 
S25 
S35 
S36 
S41 
S46 
S56 
S62 
S63 
S68 
S69 
S70 
S73 

Database before search  Data based after search 

IEEE 
IEEE 
ACM 
ACM 
Google Scholar 
Google Scholar 
Google Scholar 
Science Direct 
Google Scholar 
Science Direct 
ACM 
Google Scholar 
Wiley 
Science Direct 
IET Software 
Wiley 
Science Direct 
Google Scholar 

IEEE 
IEEE 
ACM 
ACM 
IEEE 
Google Scholar 
Google Scholar 
Science Direct 
IEEE 
Science Direct 
ACM 
Science Direct 
Wiley 
Science Direct 
IET Software 
Wiley 
Science Direct 
IEEE 

3.3  Inclusion and Exclusion Criteria  
This process is conducted to filter the selected papers from previous step by running inclusion and exclusion 
criteria. This step is important as it enables us to identify the relevant  and related papers that can help in 
answering research questions. First, all authors discussed the criteria of inclusion and exclusion that should 
be used. After reaching an agreement on the inclusion and exclusion criteria, each paper (mainly title, abstract 
and  keywords)  was  evaluated  by  first  and  second  authors  based  on  the  recommended  inclusion/exclusion 
criteria to determine whether it should be retained or rejected. The paper is considered “include” if at least 
meets  one  of  inclusion  criteria  and  none  of  exclusion  criteria.  On  the  other  hand,  the  paper  is  considered 
“exclude” if at least one of the exclusion criteria and none of inclusion criteria is met.  Finally, the paper is 
considered “uncertain” if it meets some inclusion and exclusion criteria. In this case if all authors agreed that 
the paper is “include” then the paper is retained, otherwise the paper is considered “exclude”, thus it is rejected. 
If the decision could not be made, all authors discussed the paper again by reading the full text until they reach 
agreement. The high number of agreements on the decision confirm the relevance of the proposed inclusion 
and exclusion criteria. The following points explain the inclusion and exclusion criteria in which OR Boolean 
operator is used to link between points in each type. 

 
      
 
 
 
 
Inclusion criteria: 

-  Using the UCP to predict effort estimation, OR 
-  Comparison between different effort estimation based on UCP, OR 
- 

improving UCP sizing method. 

Exclusion criteria:   

-  Using UCP for late software project development such as testing and maintenance, OR 
-  Duplicate studies (only complete version is selected), OR 
-  UCP description and explanation papers, OR 
-  Comparing between UCP and other software size metrics (e.g. Function Points, Object Points), OR 
-  White papers and Newsletters about UCP. 

3.4  Quality assessment 
Quality assessment is an important step in SLR to ensure the quality of the relevant studies. Also, to limit bias 
in  conducting  SLR.  This  process  comes  after  inclusion  and  exclusion  step.  The  questions  that  we  used  for 
quality assessment are:   

QA1. Are the objectives of the study clear? 
QA2. Does the study add value to the existing literature? 
QA3. Is the methodology of the experiments clearly described? 
QA4. Is the used dataset suitable for this type of studies? 
QA5. Are the findings supported by performance and statistical results? 
QA6. Are the threats to validity mentioned?  
QA7. Was the study published in high-quality journal or conference proceedings? 

The third author ran quality assessment on the selected papers independently. She reviewed each paper and 
gave score for each question based on the criteria discussed below. When the decision could not be reached, 
the third author communicated with other authors to give their opinion and to reach an agreement. The total 
score for each paper is then computed by summing the questions scores. If the total score is less than or equal 
to 4, the paper is excluded at this stage, otherwise it is retained. The questions were scored as follows: 
QA1: score (+1) is given if the objectives are stated clearly. Score (+0.5) is given if the objectives are not well 
defined. Score (0) is given if the objectives are not defined at all. 
QA2: score (+1) is given if the study contributed noticeably to the existing literature. (+0.5) is given if the study 
contributed partially to the literature. (0) is given if no contributions were added to the literature. 
QA3: score (+1) is given if the methodology of research is clearly described including descriptions of validation 
strategy, dataset and performance measures. (+0.5) is given if the methodology described part of the previous 
components. (0) is given if no methodology description is given in the paper.        
QA4: score (+1) is given if the size of the dataset is sufficient enough to support the findings and quality of 
projects is accepted. (+0.5) is given if the dataset size is not large or quality of data is partially met. (0) is given 
if there is no dataset to support the findings. 

 
 
 
 
QA5: score (+1) is given if there are results and statistical tests to support the findings of the study. (+0.5) is 
given if there are results but there is no statistical test to support the findings. (0) is given if the results are 
weak, and no statistical tests were conducted.   
QA6: score (+1) is given if the paper reports the threats to validity in a correct way. (+0.5) is given if the threats 
to validity are mentioned but not related to methodology. (0) is given if no threats to validity was given. 
QA7:  to  answer  this  question  we  use  on  the  Journal  Citation  Report  (JCR)  2019  or  Science  Citation  Index 
Expanded (SCIE) to score the journal of the paper, and the Computer Science Conference Ranking (CORE) to 
score conference/workshop/symposium of the paper. Score (+1) is given if the journal is ranked Q1 or Q2 in 
JCR or indexed in SCIE, or if the conference/workshop/symposium is ranked A in CORE. (+0.5) is given if the 
journal is ranked Q3 in JCR, or if the conference/workshop/symposium is ranked B in CORE. (0) is given if the 
journal is ranked Q4 in JCR, or if the conference/workshop/symposium is ranked C in CORE. 

3.5  Data extraction and synthesize 
As recommended in the SLR guideline [11], The data extraction was carried out by the first two authors of this 
paper,  where  both  authors  extracted  and  checked  the  data.  However,  the  first  author  coordinated  the 
extraction and checking data procedure. Each paper was read by the three authors and the necessary data 
were collected with goals to serve objectives of our SLR. Any disagreement among the authors regarding a 
particular paper was resolved by contacting the authors of that paper to answer some necessary questions 
about  their  paper.  The  list  below  contains  data  extraction  form  which  describes  the  data  that  have  been 
extracted from candidate papers. These data are then summarized and tabulated in a way that can serve the 
objectives of our SLR. The data that we have extracted are: 

-  Article Identifier. 
-  Article Title. 
The articles source. 
- 
-  Publication Venue. 
-  Publication year 
-  Authors  
-  Objectives. 
-  Quality evaluation. 
- 
-  Research type. 
-  Contribution type. 
-  Datasets used. 
-  Keywords. 

Techniques that are used to construct the effort estimation. 

After that, the summarized data are synthesized in order to answer the proposed research questions. Since 
the data contain  quantitative and qualitative data, different kinds of synthesis  approaches were used  such 
narrative synthesis and quantitative data synthesis to show the synthesized data in different forms. However, 
approaches like meta-analysis is not possible because the studies included in our SLR are not similar due to 
different experiments configuration and different datasets.  

 
 
 
 
3.6  Threats to Validity       
The main threat to validity of our review is the exclusion of relevant articles. Finding the most relevant and 
quality papers is the main task of an SLR study because it has significant impact on the findings and the drawn 
conclusions. To reduce this threat, all authors participated in designing the search string. The proposed search 
string was evaluated on the electronic digital libraries in order to examine its accuracy in retrieving related 
papers. The first two authors checked manually the references of the selected papers separately, in parallel, 
in order to identify all missing articles that were not returned by the search string. Furthermore, two authors 
separately conducted applying inclusion/exclusion criteria and quality assessment, if there was any doubt, the 
full  study  was  read  again.  All  disagreements  among  authors  were  discussed  until  a  final  consensus  was 
reached. For quality assessment, the minimum threshold was set by all authors to be 4 out of 7 which enables 
us to select the most relevant papers. The second threat is publication bias. Basically, we have identified five 
main electronic digital libraries that are commonly have strong publications in the field. Google scholar was 
also used to apply the search string because it explores other digital libraries on the internet. Google scholar 
allows  us  to  not  avoid  publications  venues  that  are  not  commonly  used  among  software  engineering 
researchers. The third threat is data extraction bias. Two authors conducted exhaustive search and read each 
paper independently to extract important data from the selected papers. We have identified the primary data 
that helped us to address the research questions as explained in section 3.5. However, data extraction bias 
may occur, especially when collecting data from papers that present case studies or that used unclear dataset. 
Therefore, the data extracted for each paper were compared and all disagreements were discussed by the 
researchers.  

4.  Results of Mapping Questions 

This section describes the results of mapping questions (MQs). Firstly, we made search for candidate papers 
using the  proposed search  string on the common digital libraries (IEEE Xplore, ACM,  Science  Direct, Wiley, 
Springer Link and IET). We also applied the search query on google scholar because it is a common indexing 
venue for other digital libraries. This process has resulted with total of 127 related papers as shown in Figure 
2, most of them were collected from IEEE Xplore digital library. After that, the inclusion and exclusion criteria 
have been applied to filter out all the relevant papers. This step has resulted 91 (72%) selected papers. Finally, 
the  quality  assessment  has  been  applied  on  the  selected  papers  from  previous  step  by  assessing  and 
aggregating quality scores for each paper as shown in Appendix B. This step has resulted 75 (59%) acceptable 
quality papers where 33.3% of them (i.e. 25 out of 75) were high and very high quality as shown in Table 5.  

IEEE Digital 
Library  

ACM Digital 
Library  

Science 
Direct  

Springer 
Link  

IET  

Wiley  

Google 
Scholar  

(54) 

(12) 

(10) 

(12) 

 (2) 

 (2) 

(35) 

Step 1 

Step 3 

Step 2 

75 Papers 

91 Papers 

127 Papers 

Fig. 2 Results of searching process 

The complete lists of the selected studies with their citations and classification are presented in Appendixes A 
and C. Figure 3 shows the number of papers in each searching phase classified as conference paper, journal 
article, magazine, newsletter and white paper. It is clearly noted that most of the papers have been published 
in conferences and few of them were published in journals. This may explain why this type of effort estimation 
is not yet mature enough like other estimation disciplines. We kept only one white paper, which is the Karner 
paper [10], because it  describes the original UCP model,  which is the basis of effort estimation based UCP. 
Figure  4  shows  the  publications  over  the  years,  colored  with  type  of  publications.  Clearly,  there  was  an 
increasing interest in effort estimation based on UCP among researchers in the beginning of 2000s and reached 
peak in 2005, then it began to decline with some attempts to stabilize the publications. Notably, the journal 
articles started to appear after 2005 which demonstrates that this area of research become mature after 2005. 
Number of publications for this kind of estimation is still small in comparison to other type of effort estimation. 
Figure 5 shows that the majority of selected papers have been found on IEEE Xplore digital library because the 
common conferences that authors usually published were sponsored or technically co-sponsored by IEEE. This 
is because we executed the query string first on the main research digital libraries such as IEEE, ACM, Science 
Direct, Springer Link and the papers found in these libraries are counted for them. Then the remaining papers 
that have been found in Google scholar are counted for Google scholar. this explain why IEEE digital library 
has larger number of publications than Google scholar.   

 
 
 
 
 
 
78

63

29

26

12

4

4

80

70

60

50

40

30

20

10

0

53

20

1

1

0

1

1

0

satge 1

Stage 2

Stage 3

Conference

Journal

Magazine

White Paper

NewsLetter

Fig3. Number of papers after each stage of searching and filtering process 

Table 5. Quality levels of the selected studies. 

Quality Level 

Very high (6<quality score ≤ 7) 
High (5<quality score ≤6) 
Average (4<quality score ≤5) 
Low (3<quality score ≤4) 
Very Low (0<quality score ≤3) 

No. of Studies 
11 
14 
50 
9 
7 
91 

%proportion 
0.121 
0.154 
0.549 
0.099 
0.077 
1.0 

Total 

Fig 4. Publications over the years, based on the article type. 

 
 
 
Fig 5. Number of papers in every digital library 

4.1  Venues of UCP Effort Estimation Publications (MQ1) 

As mentioned earlier we identified 75 relevant papers after two stages of filtering. Most of these papers were 
published in conferences and just few of them were published in journals as shown in Figure 1.  The top journals 
that have papers on UCP effort estimation are summarized in Table 6. We can notice that there are no common 
venues among publications. Also, not all listed journals are classified as software engineering research focus. 
Only 7 out of 14 venues can be considered software engineering journals, which have 13 out of 20 publications. 
The  remaining  seven  publications  were  published  in  none-focused  software  engineering  journals  which 
dramatically affect the visibility and breadth of the quality of forthcoming papers. This raises a big concern 
about the reputation of the published work in this field of interest. Therefore, the outcome of those papers that 
were  published  in  none-focused  software  engineering  journals  would  be  biased  toward  the  theme  of  that 
journal and their findings would seem unuseful for software engineering community. Unsurprisingly, the top 
five journals in the table that have two publications or more are classified as software engineering journals, 
in addition to the journal of International Journal of Software Engineering and Knowledge Engineering and The 
Journal  of  Defense  Software  Engineering  that  have  one  publication.    Indeed,  reading  only  the  top  5  most 
relevant journals means that important research results may be missed. The other top ranked journal in the 
field  of  software  engineering  such  as  IEEE  transactions  on  Software  Engineering,  Empirical  software 
engineering, ACM transactions on software engineering and methodology are not yet having papers on UCP 
effort estimation.  

 
 
 
 
 
 
 
 
 
Table 6. Most Important UCP Effort Estimation Journals 

“Journal Name 

# Publications 

Reference 

Information Software and Technology 
Journal of Systems and Software 
Innovations in Systems and Software Engineering 

IET Software 

Journal of Software: Evolution and Process 
Applied Soft Computing 

International Journal of Information Processing 

International Journal of Computer Applications 
International Journal of Intelligent Systems and Applications 
International Journal of Software Engineering and Knowledge Engineering 

Foundations of Computing and Decision Sciences 
Journal of Global Research in Computer Science 

INFOCOMP Journal of Computer Science 

The Journal of Defense Software Engineering 

3 
2 

2 

2 

2 

1 

1 
1 
1 
1 

1 

1 

1 
1 

[S13] [S35][S70]  
[S41][S63] 

[S15][S64] 

[S57][S68] 

[S62][S69] 

[S56] 

[S51] 
[S47] 
[S40] 
[S34] 

[S25] 

[S24] 

[S16] 
[S11]” 

4.2  Main contributions (MQ2) 
We have been interested in getting to know what are the main contribution types in this field of interest. We 
have identified and described six possible research contributions as shown in Table  2. The classification of 
papers based on contribution types is presented in Table 7. After studying and analyzing the selected papers 
by three authors, we found that that Enhancement contribution type is the most dominant one, followed by 
validation contribution type with very slight difference as shown in Figure 6. This is an indication that most 
authors favor either continue developing new models on the original UCP sizing method or validating existing 
one on industrial and new datasets from different domains. The third top ranked contribution type is developing 
techniques to estimate effort from UCP size metrics. Building tools to measure UCP from use case diagram and 
requirements  are  very  rare  and  have  little  of  interest  among  researchers.  From  these  results  we  can 
understand that researchers do not interest in developing effort estimation models based on UCP as much as 
their interest in enhancing the sizing mechanism of UCP method.  

Figure 6. Bar plot for main contributions types 

 
 
 
Table 7. Studies classification based on contribution type. 

“Contribution Type 

Technique 

Tool 
Comparison 

Validation 

Enhancement 

Model 

Studies 
[S1][S12][S15][S16][S21][S24][S25][S26][S29][S30][S32][S34][S45][S49][
S54][S63][S71][S73] 
[S6] 
[S3][S10][S22][S27][S44][S58][S68] 
[S2][S7][S9][S11][S23][S33][S35][S43][S59][S62][S64][S65][S66][S67][S6
9][S70][S74][S75] 
[S4][S5][S8][S14][S17][S19][S28][S31][S37][S42][46][S47][S48][S50] 
[S52][S53][S55][S60][S61][S72] 
[S13][S18][S20][S36][S38][S39][S40][S41][S51][S56][S57] 

Proportion 

24% 

1.3% 
9.3% 

24% 

26.7% 

14.7%” 

4.3  Main research approaches (MQ3)  
The third proposed research questions stated that:” What are the main types of research approaches applied 
to effort estimation based on UCP?”. Indeed, we are interested in getting know which are the main research 
methodologies  that  were  used  by  researchers  when  studying  this  field  of  UCP  effort  estimation.  The 
classification of papers based on research approach is shown in Table 8 and Figure 7. We can notice that the 
most studies focus on evaluating UCP effort estimation as potential approach for early effort estimation. The 
second approach is the case study. This approach was used by around 20 studies to examine the usefulness of 
UCP  effort  estimation  based  on  specific  case  studies.  The  third  approach is  based  on  comparison  between 
different effort estimation models based on UCP such as comparing between Fuzzy Logic and Neural network 
over UCP historical projects.      

Fig7. Bar plot for main research approaches. 

 
 
 
 
 
 
 
Table 8. Studies classification based on research approach. 

“Research approach 
Proposal 

Evaluation 

Review 

Case Study 

Survey 
Theory 

Comparative studies 

Explanation 
[S1][S37][S48][S49][S58][S59][S72] 
[S5][S12][S14][S16][S19][S21][S23][S24][S26][S29][S30][S32][S33][S35][S
36][S38][S39][S40][S41][S45][S46][S47][S54][S55][S57][S60][S61][S63][
S64][S65][S67][S69][S70] 
[S34][S43][S66][S74] 
[S1][S2][S4][S6][S7][S8][S9][S10][S11][S15][S18][S19][S20] 
[S25][S49][S53][S62][S65][S71][S72][S75] 
[S34] 
[S17][S28][S31][S42] 
[S3][S12][S13][S22][S27][S44][S46][S50][S51][S52][S54][S56] 
[S57][S68][S73] 

Proportion 
9.3% 

44% 

5.3% 

28% 

1.3% 
5.3% 

20%” 

4.4  MQ4. What are the most favorable techniques that are used to produce effort estimates from UCP, and how 

has this changed over time? 

The principal problem in UCP method is how to convert the likely UCP size metric into its corresponding project 
effort. The early version of UCP method that is proposed by Karner [10] suggested using productivity as second 
cost driver bedside UCP, where a fixed Productivity or a very limited productivity ratios were used [3], [19], [20]. 
These productivity values were figured out based on a very limited number of case studies. This approach was 
the dominate approach when estimating effort from UCP in most previous papers. The main reason behind 
using this approach was due to the absence of large historical industrial data, so authors used the original 
model to validate their hypothesis. In addition to that basic approach, authors attempt to build machine learning 
or statistical models to estimate effort from UCP based on small number of projects. Figure 8 depicts the most 
common techniques that were frequently used in combination of UCP in order to improve its accuracy. Amongst 
them, fuzzy logic, multiple linear regression and neural network have been the dominant methods used to 
estimate effort from UCP size metrics. The main problem with machine learning is that they need sufficient 
data  in  order  to  build  a  reliable  prediction  model.  We  can  also  notice  that  the  most  techniques  that  were 
published in journals are Karner and multiple linear regression. 

 
 
Fig8. Bar plot for main effort prediction techniques based on UCP. 

4.5  MQ5. What are type of datasets that are used to run experiments for UCP effort estimation? 
Until recently, most studies were using data collected from case studies or small number of business projects. 
This was clearly appearant in 90’s and 2000’s because most software companies do not document use case 
diagrams or sometimes, they do not follow a particular guideline when writing use case description. The case 
studies usually proposed by the authors of the papers based on information collected from some available 
project  (either  educational  or  commercial).  Note  that  industrial  datasets  are  those  that  collected  within 
company from real business projects. While educational projects are those that collected by students.  Some 
authors prefer collecting data from students because their graduation projects were most likely available and 
easy to collect after course of explanation and rehearsal.  

From the analysis we found 62 (82.7%) of the selected papers used data in their experiments. The rest of studies 
they just propose ideas or discuss some issues related to UCP model. The list of all studies that used data to 
validate their hypothesis or models are listed in Table 9. The table illustrates the type of dataset that was used 
in the selected studies. From these studies we found that both industrial datasets and case studies were the 
dominant type of datasets, where 41.9% of the studies used case studies to validate the hypothesis, and 35.5.3% 
of the studies used industrial datasets. 19.4% of the papers used both industrial and educational studies. The 
most interesting observation is that most studies during the period 1993 until 2005 used few case studies or 
small dataset to validate their work. The first reason for that is the UCP method was not of greater research 

 
 
 
interest among researchers. The second thing is that the UCP itself was not widely spread among practitioners 
because it needs a lot of details regarding  translation of use case diagram to metric values, in addition to 
assessment of environmental and technical variables that are subject to some degree of uncertainty.  

Table 9. Studies classification based on type of datasets. 

“Dataset Type 

Industrial dataset 

Educational Dataset 
Both Industrial and 
Educational 

Case Studies 

Studies 
[S11][S14][S19][S23][S24][S26][S29][S32][S33][S34][S37][S40][S44][S46][S47][S52]
[S53][S57][S63][S64][S70][S72] 
[S31][S34] 

[S35][S36][S38][S39][S41][S51][S54][S56][S62][S68][S69][S73] 

[S1][S2][S4][S5][S6][S7][S8][S10][S12][S15][S16][S21][S27][S28][S30][S42][S43][S45
][S58][S59][S60][S61][S65][S66][S71][S75] 

Proportion 

35.5% 

3.2% 

19.4% 

41.9%” 

Figure 9. shows Bar plot of the number of projects in each case study. It is clear that most of the case studies 
build their knowledge on data of one project which is empirically not acceptable to generalize their findings. 
This can be also confirmed in Table 10 which shows the studies and percentages for each used number of 
projects in their case studies.  

Fig. 9 Bar plot of number of case studies and number of its projects 

Table 10. Case Studies classification based on number of projects. 

# Projects 
1 Project 
2 Projects 
3 Projects 
4 Projects 
5 Projects 
6 Projects 

Studies 
[S10][S16][S21][S28][S30][S42][S43][S45][S58][S59][S60][S65][S66][S71][S75] 
[S15] [S61] 
[S1] [S2][S4] [S8] 
[S27] 
[S5][S6] [S12] 
[S7] 

Proportion 
57.6% 
7.7% 
15.4% 
3.8% 
11.5% 
3.8% 

   
 
 
 
 
 
 
 
5.  Results of Review Questions 

This section presents and comments on the findings obtained from mapping questions. Specifically, we present 
answers to our proposed review question with the aim to synthesis the findings from our analysis in section 4.  

5.1  RQ1.  How  easy  is  to  collect  UCP  data  from  software  industry  and  what  is  the  implication  on  effort 

estimation? 

The reported accuracies of the selected studies depend on two parameters: (1) quality of dataset including the 
number of employed projects; and (2) the used evaluation method (leave-one-out cross validation, holdout, n-
fold cross validation, evaluation criteria, etc.). With respect to first point, we have seen from the results of MQ5, 
the evaluation of UCP effort estimation is primarily performed on case studies with a percentage of 41.9% or 
industrial UCP datasets with percentage of 35.5% of the selected papers. Also, most of case studies used a few 
numbers of projects to validate their hypothesis as mentioned in section 4.5. A few of them reported accuracies 
in  terms  of  the  well-known  accuracy  measures.  Table  11  shows  the  most  frequently  datasets  and  their 
availability.  It  is  important  to  note  that  multiple  datasets  were  used  in  some  studies.  Interestingly,  Nassif 
dataset is the most frequently used (63%), followed by the Ochodek dataset (45.5%). Note that the review takes 
into account both industrial and educational datasets. The most remarkable part of this analysis is that not all 
datasets are available for community to replicate experiments and generalize extracted knowledge. Unlike 
other effort estimation research areas, this is a big challenge that face this area of interest. 
Table 11. Summary of main data sources that were used in previous studies 

Main Author 

Studies 

Prop% 

# Projects 

Source 

Ochodek 

Nassif 

Silhavy 

[S35][S44][S52][S53][S56][S62]
[S63][S68][S69][S70] 

[S24][S26][S29][S36][S38][S39]
[S41][S51][S54][S56][S57][S62][
S68][S69] [S73] 
[S63][S69][S70] 

Frohnhoff 

[S14][S33] 

Robiolo 

[S19] 

Lavazza 

[S23] 

ARUMUGAM 

[S34] 

ALWIDIAN 

 Iraji 

Alves 

Badri 

[S37] 

[S40] 

[S46] 

[S64] 

45.5% 

14 

[S35] 

63% 

13.6% 

9.1% 

4.5% 

4.5% 

4.5% 

4.5% 

4.5% 

4.5% 

4.5% 

110 

73 

15 

13 

17 

15 

14 

5600 

7 

5 

[S41] 

[S63] 

[S14] 

[S19] 

[S23] 

[S34] 

[S37] 

[S40] 

[S46] 

[S64] 

Note 
It contains 7 student 
projects and 7 
industrial projects. 
It contains 65 student 
projects and 45 
industrial projects. 

The 15 projects were 
developed by the IT-
company. 

17 project were 
developed 
in three different 
environments. 

15 object oriented 
projects developed in 
SE laboratory 
14 Industrial projects 
Derived from ISBSG 
projects 

5 open java source 
project 

Availability 

√ 

× 

√ 

± 

± 

± 

√ 

√ 

× 

± 

× 

√: public dataset and available for community, ×: private dataset, and may available on request. ±: partial available  

Regarding evaluation techniques that have been followed in the selected studies, not all selected studies used 
the common machine learning validation techniques (e.g. Leave one-out cross validation, Hold-out and K-Fold 

 
 
 
 
Cross validation) because they mainly use a few number of projects as case studies and they did not apply 
either machine learning or data mining techniques. However, the studies that used validation strategies are 
listed in Table 12 and categorized into three types of validations. The most popular of these were Hold-out and 
K-fold cross validation (k > 1) with percentage of 35.7% each.  

Table 12. Summary of validation techniques. 

Validation Technique 
Leave-One-Out Cross Validation (LOOCV) 
Hold-out 
k-Fold Cross Validation 

Studies 
[S64][S66][S70][S71] 
[S26][S28][S31][S34][S72] 
[S37][S40][S41][S43][65] 

Proportion 
28.6% 
35.7% 
35.7% 

Regarding the accuracy measures that were used to validate UCP effort estimation models and their variants, 
there was a discrepancy in the type of evaluation measures. Table 13 describes the most common accuracy 
measures the are frequently used to validate UCP effort estimation models. The most noticeable observation 
is that only 44% of the papers recorded accuracies in terms of the well-known accuracy measures as shown 
in Table 14. Particularly, MAE was used in 9 of the studies (12%), MMRE was used in 19 of the studies (25.3%), 
MMER was used in 4 of the studies (5.3%), Pred(25) was used in 13 of the studies (17.3%), MBRE and MIBRE were 
used in 6 of the studies each (8%) and MSE was used in 7 of the studies (9.3%).  

Table 13. Description of the frequent evaluation measures. 

“Name 

Equation 

Description 

Mean Absolute Error 

𝑀𝐴𝐸 =

𝑛
𝑖=1

∑ |𝑒𝑖 − 𝑒̂𝑖|
𝑛

To measure the average of errors. 

Mean Magnitude Relative 
Error 

𝑀𝑀𝑅𝐸 =

Magnitude of relative error 
relative to the estimate 

𝑀𝑀𝐸𝑅 =

1
𝑛

1
𝑛

𝑛

∑

𝑖=1

|𝑒𝑖 − 𝑒̂𝑖|
𝑒𝑖

𝑛

∑

𝑖=1

|𝑒𝑖 − 𝑒̂𝑖|
𝑒̂𝑖

To measure the average of relative errors to actual projects 
effort. 

To measure the average of relative errors to predicted 
projects effort. 

Mean Balanced Relative 
Error 

𝑀𝐵𝑅𝐸 =

1
𝑛

𝑛

∑
𝑖=1

|𝑒𝑖 − 𝑒̂𝑖|
𝑚𝑖𝑛⁡(𝑒𝑖, 𝑒̂𝑖)

To measure the average of relative errors to minimum 
between actual and predicted effort. 

Mean Inverse Balanced 
Relative Error (MIBRE) 

𝑀𝐼𝐵𝑅𝐸 =

1
𝑛

𝑛

∑
𝑖=1

|𝑒𝑖 − 𝑒̂𝑖|
𝑚𝑎𝑥⁡(𝑒𝑖, 𝑒̂𝑖)

To measure the average of relative errors to maximum 
between actual and predicted effort. 

Standardized Accuracy 

Performance 

𝑆𝐴 = 1 −

𝑀𝐴𝐸
𝑀𝐴𝐸̅̅̅̅̅̅̅𝑝𝑜

𝑃𝑅𝐸𝐷(ℓ) =

𝜆
𝑛

× 100 

Root Mean Squared Error 

𝑅𝑀𝑆𝐸 = √|𝑒𝑖 − 𝑒̂𝑖|2 

To test whether the prediction model really surpasses a 
baseline of random guessing and produces meaningful 
predictions. Where 𝑀𝐴𝐸̅̅̅̅̅̅̅𝑝𝑜 the mean absolute errors of 
random guessing. 
To count the percentage of relative errors that fall within less 
than or equal toℓ of the actual values. where 𝜆 is the count of 
project that have magnitude relative error less than ℓ. 
To measure Mean Squared Error.” 

Where 𝑒𝑖 and 𝑒̂𝑖 are the actual and predicted effort respectively. 

 
 
 
 
 
 
 
Table 14 summarizes the frequent accuracy measures for all studies that recorded their evaluation results. We 
have noticed that not all studies share the same accuracy measures during their validation, taking in account 
that these studies use different versions of the dataset. For studies with different model configurations we 
used optimal configuration if possible, otherwise we use the average of accuracy values if there were different 
dataset  samplings.  To  further  analyze  the  distribution  of  the  accuracy  measures  of  UCP  effort  estimation 
techniques,  we  drew  interval  plots  corresponding  to  each  of  these  criteria  using  the  estimation  accuracy 
values of each selected study. As can be seen in Figure 10, the mean of the accuracy values of UCP effort 
estimation techniques are around 31.5% for MMRE, 32.1% for MMER, 80.6% for Pred (25), 27.0 for MBRE, 15.7 for 
MIBRE,  102.7  for  RMSE,  2093  for  MAE  and  72.7%  for  SA.  However,  it  can  be  noticed  that,  according  to  the 
distribution  analysis  of  MMRE,  MBRE,  MIBRE  and  RMSE  criteria,  the  UCP  effort  estimation  techniques  are 
positively skewed, While the distribution of MMER and SA are symmetrically distributed around the median. 
Whereas the Pred is negatively skewed. In addition, there is high variability in the distribution of Pred and 
MMER. The results obtained by these studies indicate that UCP methods tend to produce acceptable estimates, 
especially if we consider this type of estimate is done at early stage of software development.     

Table 14. Summary of reported accuracy measures. 

P
R
E
D

M
B
R
E

I

M
B
R
E

“

S
t
u
d
y

I

D

S3 

S15 

S19 

S20 

S21 

S23 

S24 

S26 

S29 

S35 

S36 

S38 

S39 

S40 

S41 

S44 

S46 

S51 

S52 

S54 

S56 

S57 

S61 

S62 

M
M
R
E

21% 

30.5% 

- 

- 

- 

25.6% 

45% 

24% 

23% 

40% 

29% 

49% 

- 

- 

- 

9% 

66.3% 

38.6% 

44.5% 

- 

- 

34% 

6.5% 

- 

M
M
E
R

- 

- 

- 

- 

- 

- 

62.5% 

- 

- 

- 

55.2% 

27.6% 

- 

- 

- 

- 

- 

- 

54% 

7% 

40% 

- 

- 

- 

- 

- 

- 

- 

- 

95.8% 

95.8% 

50% 

64% 

86.11% 

89% 

- 

92% 

- 

- 

98% 

- 

- 

- 

97.3% 

- 

- 

R
M
S
E

- 

- 

129.92 

- 

329 

- 

- 

- 

- 

- 

- 

- 

- 

- 

M
A
E

- 

- 

- 

4.7 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

24 

8940 

- 

- 

- 

2231.8 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

15.9% 

17.5% 

12.5% 

13.2% 

- 

- 

- 

- 

16.8% 

13.1% 

S
A

M
o
d
e
l

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

Algorithmic 

Algorithmic 

Algorithmic 

ANN 

Algorithmic 

Algorithmic 

FL +  ANN 

FL 

FL 

Regression 
Treeboost + 
Regression 
ANN 

ANN 

FL 

Regression 

Algorithmic 

Algorithmic 

SVM 

FL 

FL 

i

n
s
t
a
n
c
e
s

#

1 

2 

13 

75 

1 

17 

20 

24 

24 

14 

59 

D
a
t
a
T
y
p
e

CS 

CS 

Ind 

Ind 

CS 

Ind 

Ind 

Ind 

Ind 

Ind+ Edu 

Ind+ Edu 

240 

Ind+ Edu 

72 

7 

110 

14 

7 

84 

14 

14 

195 

149 

10 

2  

Ind+ Edu 

Ind 

Ind+ Edu 

Ind 

Ind 

Both 

Ind+ Edu 

Ind 

Ind+ Edu 

Ind+ Edu 

Ind 

CS 

1598 

84.3% 

Algorithmic 

- 

- 

- 

- 

Algorithmic 

Algorithmic 

3182 

62% 

Algorithmic 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S63 

S64 

S68 

S69 

S70 

S71 

S72 

S73 

- 

- 

- 

- 

- 

18.4% 

30.4% 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

82% 

- 

- 

- 

- 

- 

18.74 

- 

58.71 

33.7 

- 

- 

65.9% 

24.1% 

28% 

18% 

- 

- 

- 

- 

- 

- 

17.7% 

13% 

- 

- 

55.9 

- 

- 

- 

1761.4 

73.15% 

820.4 

71.3% 

- 

- 

- 

262.8 

- 

- 

- 

- 

Algorithmic 
Stepwise 
multiple linear 
regression  
simple linear 
regression 
Algorithmic 

Algorithmic 

Algorithmic 

Regression  

Algorithmic 

98 

Ind+ Edu 

5 

Ind 

195 

195 

98 

1  

22 

110 

Ind+ Edu 

Ind+ Edu 

Ind+ Edu 

CS 

Ind+ Edu 

Ind+ Edu” 

Figure 10. Interval Plot of accuracy measures for all studies in Table 12 

To further analyze the estimation accuracy of  UCP methods, Table 15 provides the detailed statistics of the 
most accuracy measures MMRE, Pred(25), MBRE and MIBRE for each of the most frequently used datasets 
(Nassif and Ochodek). Corresponding to Nassif dataset, the mean of the prediction accuracy values varies from 
23% to 49% for MMRE, from 64% to 98% for Pred(25), from 15.9% to 65.9% for MBRE and from 12.5 to 24.1 for 
MIBRE. For Ochodek dataset, the mean of the prediction accuracy values varies from 9% to 44.5% for MMRE, 
from 16.8% to 65.9% for MBRE and from 13.1 to 24.1 for MIBRE. This indicates that UCP methods tend to yield 
acceptable estimates. 

Table 15. Summary of reported accuracy measures for top two frequent datasets. 

“Dataset 

MMRE 

Pred 

MBRE 

MIBRE 

i

M
n
%

M
e
a
n
%

M
e
d
a
n
%

i

M
a
x
%

i

M
n
%

M
e
a
n
%

M
e
d
a
n
%

i

M
a
x
%

i

M
n
%

M
e
a
n
%

M
e
d
a
n
%

i

M
a
x
%

i

M
n
%

M
e
a
n
%

M
a
x
%

M
e
d
a
n
%

i

Nassif 
Ochodek 

23% 

9% 

35% 

31% 

34% 

40% 

49% 

45% 

64% 

50% 

90% 

50% 

94% 

50% 

98% 

50% 

16% 

17% 

27% 

32% 

18% 

23% 

66% 

66% 

13% 

13% 

16% 

17% 

13% 

16% 

24% 

24%” 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
5.2  RQ2. What are the impacts of combining other techniques with an UCP method technique on its estimation 

accuracy? 

Since the introduction of the original UCP model in 1993 by Kerner, various of machine learning and data mining 
technique  have  been  adopted  and  combined  with  UCP  in  order  to  improve  its  accuracy.  The  uses  of  such 
techniques were utilized for two goals: the first one is to improve the UCP sizing procedure, while the second 
goal  is  to  build  effort  estimation  model  based  on  UCP  variable.  However,  since  the  use  of  such  technique 
enhance UCP sizing approach, it will have direct impact on the accuracy of UCP effort estimation. From Table 
14  we  summarize  the  results  of  accuracies  for  all  techniques  that  were  used  either  to  enhance  UCP  size 
measure or to improve accuracy of effort estimation. The studies that followed Kerner approach in measuring 
UCP  and  estimating  effort  are  called  algorithmic  in  Table  16.  From  the  table,  we  can  notice  that  all  used 
techniques produce relatively acceptable accuracy. If we consider the median statistic method as reference 
for comparison because it is less bias than other statistical measures, we notice that algorithmic model is the 
superior method in terms of MMRE and RMSE, whereas Fuzzy Logic is the superior models in terms of Pred, 
MBRE and MIBRE. ANN is best technique in terms of MAE. Note that each model may use different dataset and 
different  validation  technique.  However,  this  can  confirm  that  there  is  instability  ranking  among  these 
techniques. Also, the use of machine learning methods such as Neural networks and Regression do not always 
produce  superior  predictions  in  comparison  with  Algorithmic  techniques.  This  can  open  a  new  research 
direction to assess the usefulness of machine learning for predicting UCP effort estimation especially when 
historical datasets are become available.  

The  benefits  of  combining  UCP  with  other  techniques,  especially  machine  learning,  can  still  be  useful  for 
improving  productivity  prediction  as  it  is  an  important  key  driver  in  translating  UCP  into  the  most  likely 
software  effort.  In  addition,  measuring  use  case  complexity  can  also  benefit  from  combination  with  other 
techniques such as Neural Networks and Fuzzy Logic.  

Table 16. Summary of accuracy measures for the most used estimation techniques 

Accuracy 
Measure 

MMRE% 

Pred% 

MBRE% 

MIBRE% 

Statistic 
method 
Min 
Mean 
Median 
Max 
Min 
Mean 
Median 
Max 
Min 
Mean 
Median 
Max 
Min 
Mean 

Algorithmic 

7.0 
26.5 
23.5 
66.0 
55.0 
71.7 
63.0 
97.0 
17.0 
20.3 
18.0 
28.0 
13.0 
14.3 

Fuzzy 
Logic 
23.0 
34.25 
34.5 
45.0 
96.0 
96.0 
96.0 
96.0 
16.0 
16.0 
16.0 
16.0 
13.0 
13.0 

Neural 
Networks 
49.0 
49.0 
49.0 
49.0 
86.0 
875 
87.5 
89.0 
- 
- 
- 
- 
- 
- 

Regression 

30.0 
35.0 
35.0 
40.0 
50.0 
74.7 
82.0 
92.0 
66.0 
66.0 
66.0 
66.0 
24.0 
24.0 

 
 
Median 
Max 
Min 
Mean 
Median 
Max 
Min 
Mean 
Median 
Max 
Min 
Mean 
Median 
Max 

13.0 
18.0 
262.8 
1465.8 
1209.2 
3182 
18.74 
133.4 
92.9 
329 
62.0 
72.3 
71.0 
84..0 

13.0 
13.0 
2231.8 
2231.8 
2231.8 
2231.8 
- 
- 
- 
- 
- 
- 
- 
- 

- 
- 
4.7 
4.7 
4.7 
4.7 
- 
- 
- 
- 
- 
- 
- 
- 

24.0 
24.0 
33.7 
3578.4 
1761.4 
8940 
24.0 
41.4 
41.4 
58.7 
73.0 
73.0 
73.0 
73.0 

MAE 

RMSE 

SA% 

5.3  RQ3. What is the most favorable context for UCP effort estimation method? 
Authors in the area of UCP effort estimation work on different contexts to improve performance and reliability 
of UCP method. Therefore, it is important to identify the favorable contexts to direct researchers on the most 
interesting research parts of such method. Ochodek et al. [1] and Nassif et al. [3] identified several factors that 
affect  the  UCP  method  such  as  complexity  measure  of  use  cases,  assessment  of  UCP  adjustment  factors, 
impact of learning productivity. Based on their findings, we attempt to identify the favorable UCP contexts that 
received greater attention than others. To accomplish that we extracted and investigated the strengths and 
weaknesses reported in the selected UCP studies as shown in Tables 15 and 16.  

We  found  that  the  information  reported  is  mainly  related  to  UCP  sizing  technique  and  productivity  factor 
prediction, which seem to have a significant impact on the prediction accuracy of UCP method. Although the 
philosophy behind construction of UCP method was inspired by Function Points method, the construction of 
UCP was broadly discussed and questioned because of the algebraic construction of all metrics and the weights 
that are used in the calculation. For instance, researchers turn attention to the limitation on the construction 
of UCP such as technical complexity factors assessment, environment factors assessment and involvement of 
calculations that are based on algebraically inadmissible scale-type transformations. Yet, there is no study 
attempts to validate the weights that are used in computing UAW, UUCW, EF and TCF metrics [21] and [22]. 
Authors of [23], [24], [25], [26], [4] and [1] went further to extend the UCP model by providing new complexity 
weights or by modifying the method used to predict effort. Other researchers work on the experience needed 
to translate use case descriptions into suitable metric (i.e. UUCW). Few authors discusses different approaches 
to measure the complexity of use cases based on transaction, paths and TTPoints [1], [23]–[25], [31] and [31]. 
Transaction is a set of trigger-response activities that is extracted from textual description of use cases. Paths 
is another measure based on cyclomatic complexity metric which is also derived from textual description of 
the use cases. In this regard, Robiolo et al., [27][29] proposed an improvement to transactions by calculating 
paths  which  are  computed  form  the  cyclomatic  complexity  of  the  use  case  scenario.  They  introduced  the 
concept of stimuli which is a system entry point that generates response (transaction) of an actor action in a 
use case. Ochodek et al. [1] discusses a reliability of transaction identification process in line of other use case 

 
 
complexity measures such as TTPoints. The main conclusion drawn from these studies is that both paths and 
transaction are useful for UCP calculation even that there is a slight difference in the accuracy. However, there 
is no automated tool that can help in this issue.  

Recently, authors of [3], [19], [20], [32] studies shade the light into the need to learn the productivity factor from 
environmental  factors  of  historical  projects.  They  discussed  the  importance  of  using  dynamic  productivity 
ration  instead  of  using  fixed  productivity  ratio  that  was  suggested  by  Kerner.  Other  approaches  suggest 
learning productivity from UCP size metrics which could be as alternative solutions to improve accuracy of 
translating UCP into most likely software effort. 

There are characteristics other than UCP construction parameters to be considered when using a UCP method. 
We summarize these in Tables 17 and 18. For example, an UCP technique is well defined procedure because it 
depends  on  robust  modelling  technique  which  is  UML.  This  provided  UCP  the  support  needed  to  be  spread 
across  industry  because  use  case  diagram  is  a  de-facto  analysis  technique  for  object-oriented  software 
projects. In addition, the use case specification always receives support from software engineering community 
which helps to keep UCP model more stable. Moreover, the UCP has proven to be more accurate than expert 
judgement especially at early stages of software development. This is very intuitive as the UCP depends heavily 
on the use case diagram that has been extracted from software requirements. On the other hand, UCP is still 
not  widely  used  within  agile  project  development  because  of  the  nature  of  agile  projects  that  depend  on 
dividing project into small tasks with small number of requirements to be built within a period called sprint. 
This approach hinders applying use case diagram on a very small set of the project requirements while ignoring 
other important requirements.  

To summarize the findings, the UCP is promising approach for early effort estimation and has many benefits 
for software industry. But still some  improvements are required in some sides in order to cover all issues 
mentioned in Table 18. As mentioned in the previous section combining UCP with machine learning techniques 
could be a solution, for some limitations mentioned in Table 18, when we carefully consider the characteristics 
of the UCP dataset.   

Table 17. List of main advantages of applying UCP with supporting studies 

“Advantage 

1-Improving early effort estimation 

2-UCP is well defined procedure 
3-UCP performs better than algorithmic models at early stage 
of software development 
4-UCP performs better than expert judgement 
5-UCP effort estimation method does not always need 
historical data to produce effort estimate 

Supporting studies  
[S4][S7][S8][S9][S10][S11][S22] 
[S23][S24][S26][S29][S33][SS39][S40] 
[S42][S46][S51][S56][S57][S58][S60] 
[S62][S63][S64][S68][S69][S70][S73] 
[S31][S33][S48][S56][S57][S62] 

[S38][S44][S61] 

[S2][S3][42][S61] 

[S13][S16][S43][S47][S62][S68][S69]” 

 
 
 
 
 
Table 18. List of main limitations mentioned in the published UCP with supporting studies 

“Limitation 
1-There is no standard for use case specification 
2-Assessment of Technical Factors and Environmental Factors are 
subject to expert some degree of uncertainty  
3-Algebraic constructions of UCP methods and arbitrary factors weight 
are not clear. 

4-There is not stable technique to translate UCP into corresponding effort 

5-Measuring Use Case complexity using transaction is not always 
accurate in comparison to paths and TTPoints 
6- UCP cannot be easily adopted in Agile Development 

Supporting studies  
[S27][S33][S43][S60] 

[S35][S61][S62][S68][S69] 

[S35] 

[S29][S33][S36][S38][S39] 
[S41][S56][S62] 

[S19][S23][S35] 

[S10][S42][S45][S58]” 

5.4  RQ4. To what extent are researchers on UCP methods aware of the breadth of potential estimation study 

sources. 

Our main goal in this question is to make sure that researchers are aware of the variety of possible venues to 
publish studies related to the UCP method. An indication of this awareness was derived through a random 
selection of 5 UCP effort estimation journal papers and 10 conference papers (about 20% of the total). These 15 
papers are marked with (A) in Appendix A. We noticed that: 1) Researchers usually do not refer to previously 
published, papers on the same research topic. This can be clearly seen in their related work or comparison 
with previous published results. Some researchers are not aware with state-of art results or models published 
with  area  of  interest.  2)  Conference  papers  are  cited  more  than  journal  papers.  This  explain  why  the 
contributions of the new studies do not really handle the major challenges in this area of interest. For example, 
we can see too many repetitions in the experiment without drawing useful conclusion of clear contribution. We 
found that 40 percent of papers made references to at least one of the top five journals mentioned in Table 5. 
There were, relative to the number of papers available, few references to papers in IET software (20 percent), 
Innovations  in  Systems  and  Software  Engineering,  or  Journal  of  Software:  Evolution  and  Process.  Most  of 
citation were made to both Elsevier Journals: Information and Software Technology and Journal of Systems 
and Software. 3) Few papers referred to estimation results outside the UCP research community, e.g., to studies 
in soft computing, and artificial intelligent. The main portion of references to sources outside the software 
community seems to be to literature on fuzzy logic. 

6.  Discussion and Research Implication 

This section presents summary of results and recommendations for future research. With respect to main type 
of contributions in the field of UCP effort estimation, our review revealed that the enhancement and validation 
on UCP model are the most frequent contribution types. Enhancement aims to improve the structure of UCP 
model to produce more accurate estimates, while validations aims at evaluating of the performance of existing 
UCP techniques using some historical datasets. Our review revealed that no one of the enhanced UCP models 
was adopted later as de-facto model for further investigation. This raise a concern regarding the quality and 
performance of such models. Therefore, it is recommended that future studies replicate the previous enhanced 
UCP  models  on  various  software  development  contexts  and  using  new  industrial  datasets.  Moreover,  the 
review has found that there is a lack of in-depth studies on how to evaluate UCP models in real-life contexts. 

 
In addition, most of the cases studies and datasets that were used to evaluate the UCP model are too obsolete 
to be representative. We suggest that researchers should put more effort on collecting reliable and quality 
UCP datasets which can help in evaluating and enhancing UCP models. In this regard, the UCP method can 
benefit from other related research fields that aims to improve UML modelling and specification. In fact, the 
use case specification has great impact on how the use case complexity are calculated which therefore affect 
final estimate accuracy. On the other hand, few tools implementing UCP method have been developed. It is 
perhaps not surprising that the use of UCP method within software industry is so limited. To solve this issue, 
we  recommend  that  researchers  implement  automated  tools  to  translate  use  case  diagram  and  its 
specification to size metrics and provide guidelines on how to use these tools in industry. 

Regarding the main techniques that have been used to produce effort estimates from UCP. This review revealed 
that  fuzzy  logic,  multiple  linear  regression,  and  neural  network  are  the  frequent  employed  technique. 
Researchers put further focus on fuzzy logic because of imprecision nature of data collected at early stage of 
software development. However, we recommend that other techniques can be used in combination of UCP to 
enhance its accuracy, for example optimization algorithms can be used to search for best environmental and 
technical  factors that  improve UCP size  metric. Some other techniques, such as genetic programming and 
Bayesian networks were not used in combination with UCP method. Therefore, researchers are encouraged to 
investigate the impact of these techniques when combined with UCP. 

With respect the estimation accuracy, this review revealed that most UCP models tend to produce acceptable 
accuracy  level.  However,  these  results  are  based  on  case  studies  with  few  examples  or  on  confidential 
datasets. Therefore, it is recommended to run more experiments on large industrial datasets. Also, there were 
no common accuracy metrics among researchers. Researchers are encouraged to use the common accuracy 
measures that proved to avoid bias and can generalize findings in the future such as Standardized accuracy 
(SA), MdMRE, MBRE and MIBRE. Researchers are also encouraged to discard biased measures such as MMRE 
and MMER. 

Regarding datasets, our review showed that the cases studies and small datasets were the most common 
source  of  UCP  evaluation.  This  shortage  in  reliable  datasets,  especially  industrial  one  (i.e.  which  collected 
within  software  companies)  hinder  the  generalization  of  UCP  among  practitioners  and  researchers.  We 
recommend  that  researcher  should  work  in  line  with  software  industry  to  build  a  comprehensive  data 
collection tools that attempt to analyze use case diagram and translate its specification into appropriate UCP 
metrics values. 
Regarding the estimation contexts of UCP method, researchers should be aware of the impact various technical 
and  environmental  factors  assessment  on  the  constructing  and  evaluating  UCP  models.  In  addition, 
researchers should be aware of the best way to compute the complexity of use cases. Although the common 
technique to measure the use case complexity is based on transactions, our review revealed that there are 
different versions of measuring use case complexity such as paths and TTPoints that proven to produce better 
results than transactions in some studies. In this regard, we suggest that researchers should put more effort 
on improving the construction of UCP such as technical complexity factors assessment, environment factors 
assessment  and  involvement  of  calculations  that  are  based  on  algebraically  inadmissible  scale-type 
transformations.  few  research  works  have  studied  the  limitations  of  applying  UCP  in  agile  software 

 
 
 
development. It would be beneficial for the research community to address this limitation since the existing 
structure of UCP method cannot be easily adopted with agile development.  
Finally, the set of studies are dominated by 12 researchers who have appeared in at least three studies each 
as shown in Table 19. In particular, we found three main research groups which contributed good number of 
publications. The first group involve researchers from University of Western Ontario in Canada which has been 
involved in 10 of the studies: Ali Bou Nassif (16), Luiz Capretz (10) and Danny Ho (10). These papers are published 
in  different  venues,  mainly  on  IEEE.  Mohammad  Azzeh  and  Ali  Bou  Nassif  worked  together  in  8  studies  (4 
conference papers and 4 journal papers published in Science Direct, IET and Wiley). Bente Anda contributed 
with 5 studies in this research field. Petr Silhavy and Radek Silhavy were also another team who contributed 
with  three  publications,  mainly  on  Science  Direct.  Furthermore,  Miroslaw  Ochodek  at  Poznan  University  of 
Technology has a substantial contribution in software requirements that are related to the applications of use 
cases in general. However, he has limited studies that address the UCP method in particular.  

Table 19. Summary of accuracy measures for the most used estimation techniques, where C: Conference, J: 
Journal 
Google 
Scholar 
J 
C 

Science 
Direct 
J 
C 

Springer  Wiley 

Author name 

Total 

ACM 

IEEE 

IET  

C 

C 

C 

J 

J 

J 

J 

J 

Ali Bou Nassif 

Bente Anda 

Danny Ho 

Ford Gaol 

Gabriela Robiolo 

Harco Warnars 

Jerzy Nawrocki 

Luiz Capretz 

Mohammad Azzeh 

Petr Silhavy 

Radek Silhavy 

Shadi Banitaan 

1 

1 

1 

0 

2 

0 

0 

1 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

2 

0 

1 

0 

0 

0 

2 

1 

1 

2 

2 

0 

2 

0 

2 

0 

0 

0 

0 

2 

0 

0 

0 

0 

1 

0 

1 

0 

0 

0 

1 

1 

0 

0 

0 

0 

7 

1 

5 

3 

0 

3 

0 

5 

4 

0 

0 

2 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

0 

1 

0 

0 

0 

0 

0 

0 

0 

1 

0 

0 

1 

0 

3 

0 

0 

0 

0 

0 

0 

0 

1 

1 

0 

0 

0 

0 

0 

1 

0 

0 

0 

0 

0 

0 

0 

2 

0 

0 

0 

0 

0 

0 

0 

2 

0 

0 

0 

16 

5 

10 

3 

3 

3 

3 

10 

8 

3 

3 

3 

7.  Conclusions 

Predicting software effort from UCP size metric has witnessed increase interest among researchers in the 
past  two  decades.  This  kind  of  estimation  is  designed  mainly  for  early  software  estimation.  Although  this 
approach has promises for better early effort estimation, there are many challenges that hinder the spread of 
this method in the industry. This systematic literature reviews most relevant published studies in this area of 
interest with attempts to focus the light on the main challenges and potential research directions. Among 127 
searched papers, only 75 relevant papers with good quality have been selected for investigation. Twenty out 
of 75 selected papers were published in journals which depicts that the quality of work published in the field 
of interest is  still low. The main reasons are that: 1) absence of reliable public industrial datasets, 2) most 
authors mainly focus on building effort estimation from UCP, but few studies were designed to improve process 
of  UCP,  3)  The  algebraic  construction  of  UCP  method  has  not  been  toughly  validated,  mainly  the  arbitrary 

 
 
numbers used to adjust technical and environmental factors, and finally 4) most developers do not follow a 
proper guide when translating use case diagram into size metrics, which is easily influenced by uncertainty in 
understanding the use cases description. Another interesting observation is that half of the journal articles 
were published in nonspecialized software engineering journals. This may affect the visibility and quality of 
research outcomes of these studies. Even that the readers of these papers might not find them useful since 
they were affected with the theme of the journal that they published in.  

Most articles used small numbers of projects because of the difficulty in obtaining reliable datasets that are 
commonly used in software cost estimation domains. This is raised from the fact that UCP data are different in 
structure from other effort estimation dataset. Furthermore, the process of measure the required information 
takes long time comparing with other effort estimation datasets. The good thing that we observed in this SLR 
is that large number of studies used industrial projects rather than case studies and educational projects. 

The original UCP effort estimation that was proposed by Karner was the main the commonly used model in 
estimating effort from UCP, especially in the first decade after publishing UCP. This is because of the absent of 
historical datasets. Later, fuzzy logic and linear regression were among published effort estimation models.      

Finally, Enhancement and building techniques were the dominant approaches for the UCP studies. There was 
little  interest  in  developing  estimation  tools  approach,  especially  intelligent  tools  that  translate  use  case 
description to UCP metric. Also, we have investigated some journals to examine awareness of published results 
and breadth of potential estimation study sources. We found that authors of some journal articles, are not 
aware  of  previous  published  results  and  conclusions  in  the  field  of  UCP  effort  estimation.  Furthermore, 
Researchers usually do not refer to previously published, seemingly relevant, papers on the same research 
topic. Interestingly, few papers referred to estimation results outside the software community.  

ACKNOWLEDGMENTS 

“Mohammad  Azzeh  is  grateful  to  the  Applied  Science  Private  University,  Amman,  Jordan,  for  the  financial 
support granted to cover the publication fee of this research article. 

Ali Bou Nassif and Imtinan Attili would like to thank the University of Sharjah for supporting this research.” 

References 

[1] 

[2] 

[3] 

[4] 

M. Ochodek, J. Nawrocki, and K. Kwarciak, “Simplifying effort estimation based on Use Case Points,” 
Inf. Softw. Technol., vol. 53, no. 3, pp. 200–213, 2011. 

M. Azzeh and A. B. Nassif, “A hybrid model for estimating software project effort from Use Case 
Points,” Appl. Soft Comput. J., vol. 49, 2016. 

A. B. Nassif, D. Ho, and L. F. Capretz, “Towards an early software estimation using log-linear 
regression and a multilayer perceptron model,” J. Syst. Softw., vol. 86, no. 1, 2013. 

P. Mohagheghi, B. Anda, and R. Conradi, “Effort estimation of use cases for incremental large-scale 
software development,” in Proceedings of the 27th international conference on Software engineering, 
2005, vol. St. Louis, pp. 303–311. 

 
 
[5] 

[6] 

[7] 

[8] 

[9] 

A. B. Nassif, L. F. Capretz, D. Ho, and M. Azzeh, “A treeboost model for software effort estimation 
based on use case points,” in Proceedings - 2012 11th International Conference on Machine Learning 
and Applications, ICMLA 2012, 2012, vol. 2, pp. 314–319. 

B. Kitchenham, O. Pearl Brereton, D. Budgen, M. Turner, J. Bailey, and S. Linkman, “Systematic 
literature reviews in software engineering - A systematic literature review,” Inf. Softw. Technol., vol. 
51, no. 1, 2009. 

A. Idri, F. A. Amazal, and A. Abran, “Analogy-based software development effort estimation: A 
systematic mapping and review,” Information and Software Technology, vol. 58. 2015. 

J. Wen, S. Li, Z. Lin, Y. Hu, and C. Huang, “Systematic literature review of machine learning based 
software development effort estimation models,” Inf. Softw. Technol., vol. 54, no. 1, pp. 41–59, 2012. 

M. Jorgensen and M. Shepperd, “A systematic review of software development cost estimation 
studies,” IEEE Trans. Softw. Eng., vol. 33, no. 1, 2007. 

[10] 

G. Karner, “Resource Estimation for Objectory Projects,” 1993. 

[11] 

[12] 

[13] 

[14] 

[15] 

[16] 

[17] 

[18] 

[19] 

B. Kitchenham and S. Charters, “Guidelines for performing Systematic Literature Reviews in Software 
Engineering.” 2007. 

Y. Mahmood, N. Kama, and A. Azmi, “A systematic review of studies on use case points and expert-
based estimation of software development effort,” J. Softw. Evol. Process, vol. pre-print, Jan. 2020. 

M. Usman, E. Mendes, F. Weidt, and R. Britto, “Effort estimation in Agile Software Development: A 
systematic literature review,” in ACM International Conference Proceeding Series, 2014. 

S. MacDonell, M. Shepperd, B. Kitchenham, and E. Mendes, “How reliable are systematic reviews in 
empirical software engineering?,” IEEE Trans. Softw. Eng., vol. 36, no. 5, 2010. 

A. B. Nassif, L. F. Capretz, and D. Ho, “Enhancing Use Case Points Estimation Method using Soft 
Computing Techniques,” J. Glob. Res. Comput. Sci., vol. 1, no. 4, pp. 12–21, 2010. 

M. S. Iraji and H. Motameni, “Object Oriented Software Effort Estimate with Adaptive Neuro Fuzzy use 
Case Size Point (ANFUSP),” Int. J. Intell. Syst. Appl., vol. 4, no. 6, pp. 14–24, 2012. 

M. Azzeh, A. B. Nassif, and S. Banitaan, “Comparative analysis of soft computing techniques for 
predicting software effort based use case points,” IET Software, vol. 12, no. 1. pp. 19–29, 2018. 

R. Silhavy, P. Silhavy, and Z. Prokopova, “Evaluating subset selection methods for use case points 
estimation,” Inf. Softw. Technol., vol. 97, pp. 1–9, May 2018. 

M. Azzeh and A. B. Nassif, “Analyzing the relationship between project productivity and environment 
factors in the use case points method,” J. Softw. Evol. Process, vol. 29, no. 9, 2017. 

[20]  M. Azzeh and A. B. Nassif, “Project productivity evaluation in early software effort estimation,” J. 

Softw. Evol. Process, vol. 30, no. 12, p. e2110, 2018. 

[21] 

[22] 

S. Diev, “Use cases modeling and software estimation: applying use case points,” SIGSOFT 
Softw.Eng.Notes, vol. 31, no. 6, pp. 1–4, 2006. 

B. Anda, H. Dreiem, D. I. K. Sjøberg, and M. Jørgensen, “Estimating software development effort based 
on use cases – experiences from industry,” in Lecture Notes in Computer Science (including subseries 
Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 2001. 

[23] 

[24] 

K. Periyasamy and A. Ghode, “Cost Estimation Using Extended Use Case Point (e-UCP) Model,” in 
International Conference on Computational Intelligence and Software Engineering, 2009. 

F. Wang, X. Yang, X. Zhu, and L. Chen, “Extended Use Case Points Method for Software Cost 
Estimation,” in International Conference on Computational Intelligence and Software Engineering, 
2009. 

[25]  M. R. Braz and S. R. Vergilio, “Software Effort Estimation Based on Use Cases,” 30th Annual 
International Computer Software and Applications Conference COMPSAC06. 2006. 

[26] 

[27] 

[28] 

[29] 

A. B. Nassif, L. Fernando Capretz, and D. Ho, “Estimating Software Effort Based on Use Case Point 
Model Using Sugeno Fuzzy Inference System,” 2011 IEEE 23rd Int. Conf. Tools with Artif. Intell., 2011. 

G. Robiolo, C. Badano, and R. Orosco, “Transactions and paths: Two use case based metrics which 
improve the early effort estimation,” in International Symposium on Empirical Software Engineering 
and Measurement, 2009, vol. 0, pp. 422–425. 

G. Robiolo and R. Orosco, “Employing use cases to early estimate effort with simpler metrics,” Innov. 
Syst. Softw. Eng., vol. 4, no. 1, pp. 31–43, 2008. 

L. Lavazza and G. Robiolo, “The role of the measure of functional complexity in effort estimation,” in 
ACM International Conference Proceeding Series, 2010. 

[30]  M. Ochodek, J. N.-F. of C. and Decision, and U. 2010, “Enhancing use-case-based effort estimation with 

transaction types,” Found. Comput. Decis. Sci., vol. 35, no. 2, 2010. 

[31] 

Sarwosri, M. J. Al Haiyan, M. Husein, and A. Putra Ferza, “The development of method of the 
enhancement of Technical Factor (TF) and Environmental Factor (EF) to the Use Case Point (UCP) to 
calculate the estimation of software’s effort,” in Proceedings of 2016 International Conference on 
Information and Communication Technology and Systems, ICTS 2016, 2017. 

[32]  M. Azzeh, A. Bou Nassif, S. Banitaan, and C. Lopez-Martin, “Ensemble of Learning Project Productivity 
in Software Effort Based on Use Case Points,” in Proceedings - 17th IEEE International Conference on 
Machine Learning and Applications, ICMLA 2018, 2019. 

[33] 

[34] 

B. Anda, “Comparing Effort Estimates Based on Use Case Points with Expert Estimates,” in Empirical 
Assessment in Software Engineering, 2002, pp. 8–10. 

B. Anda, E. Angelvik, and K. Ribu, “Improving estimation practices by applying use case models,” in 
Product Focused Software Process Improvement, Springer, 2002, pp. 383–397. 

[35]  M. R. Braz and S. R. Vergilio, “Using fuzzy theory for effort estimation of object-oriented software,” in 

Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI, 2004. 

[36] 

[37] 

S. Kusumoto, F. Matukawa, K. Inoue, S. Hanabusa, and Y. Maegawa, “Estimating effort by use case 
points: Method, tool and case study,” in Proceedings - International Software Metrics Symposium, 
2004. 

B. Anda, H. C. Benestad, and S. E. Hove, “A multiple-case study of software effort estimation based on 
use case points,” in 2005 International Symposium on Empirical Software Engineering, ISESE 2005, 
2005. 

[38] 

E. R. Carroll, “Estimating software based on use case points,” in Companion to the 20th annual ACM 
SIGPLAN conference on Object-oriented programming, systems, languages, and applications, 2005, 

pp. 257–265. 

[39] 

C. Gencel, L. Buglione, O. Demirors, and P. Efe, “A Case Study on the Evaluation of COSMIC-FFP and 
Use Case Points,” in Software Measurement European Forum, 2006, vol. Italy, pp. 121–140. 

[40] 

R. K. Clemmons, “Project estimation with Use Case Points,” J. Def. Softw. Eng., vol. 19, no. 2, 2006. 

[41] 

[42] 

[43] 

[44] 

[45] 

[46] 

[47] 

[48] 

[49] 

[50] 

[51] 

[52] 

[53] 

M. Ochodek, B. Alchimowicz, … J. J.-I. and S., and U. 2011, “Improving the reliability of transaction 
identification in use cases,” Inf. Softw. Technol., vol. 53, no. 8, pp. 885–897, 2011. 

S. Frohnhoff and G. Engels, “Revised Use Case Point Method-Effort Estimation in Development 
Projects for Business Applications,” in 11th International Conference on Quality Engineering in 
Software Technology (CONQUEST 2008), 2008. 

R. Palucci Pantoni, E. A. Mossin, and D. Brandão, “Task Effort Fuzzy Estimator for Software 
Development,” INFOCOMP, vol. 7, no. 2, 2008. 

S. Ajitha, T. V. S. Kumar, D. E. Geetha, and K. R. Kanth, “Neural network model for software size 
estimation using use case point approach,” in International Conference on Industrial and Information 
Systems (ICIIS), 2010, vol. Mangalore, pp. 372–376. 

G. B. Ibarra and P. Vilain, “Software estimation based on use case size,” in Proceedings - 24th 
Brazilian Symposium on Software Engineering, SBES 2010, 2010. 

A. B. Nassif, L. Capretz, and D. H. Emerging, “Software estimation in the early stages of the software 
life cycle,” in International conference on emerging trends in computer science, communication and 
information technology, 2010, pp. 5–13. 

A. B. Nassif, L. F. Capretz, and D. Ho, “A Regression Model with Mamdani Fuzzy Inference System for 
Early Software Effort Estimation Based on Use Case Diagrams,” in Third International Conference on 
Intelligent Computing and Intelligent Systems, 2011, vol. Guangzhou, pp. 615–620. 

Q. Yu, C. Liu, N. Li, and N. Ji, “Application of estimating based on use cases in Software Industry,” in 7th 
International Conference on Wireless Communications, Networking and Mobile Computing, WiCOM 
2011, 2011. 

S. Dash and A. A. Acharya, “Cost estimation for distributed systems using synthesized use case point 
model,” in Communications in Computer and Information Science, 2011. 

J. Lee, W. T. Lee, and J. Y. Kuo, “Fuzzy logic as a basic for use case point estimation,” in IEEE 
International Conference on Fuzzy Systems, 2011. 

N. Nunes, L. Constantine, and R. Kazman, “IUCP: Estimating interactive-software project size with 
enhanced use-case points,” IEEE Softw., 2011. 

A. B. Nassif, D. Ho, and L. F. Capretz, “Regression Model for Software Effort Estimation Based on the 
Use Case Point Method,” in 2011 International Conference on Computer and Software Modeling, 2011, 
vol. Singapore, pp. 117–121. 

Y. Yavari, M. Afsharchi, and M. Karami, “Software complexity level determination using software effort 
estimation use case points metrics,” in 2011 5th Malaysian Conference in Software Engineering, 
MySEC 2011, 2011. 

[54] 

C. Arumugam and C. Babu, “Developmental Size Estimation for Object-Oriented Software Based on 
Analysis Model,” Int. J. Softw. Eng. Knowl. Eng., vol. 23, no. 3, pp. 289–308, 2013. 

[55] 

[56] 

[57] 

[58] 

[59] 

[60] 

[61] 

[62] 

J. Alwidian and W. Hadi, “Enhancing the results of UCP in cost estimation using new external 
environmental factors,” 2012 Int. Conf. Inf. Technol. e-Services, ICITeS 2012, 2012. 

A. B. Nassif, L. F. Capretz, and D. Ho, “Estimating software effort using an ANN model based on use 
case points,” in Proceedings - 2012 11th International Conference on Machine Learning and 
Applications, ICMLA 2012, 2012, vol. 2, pp. 42–47. 

A. B. Nassif, L. F. Capretz, and D. Ho, “Software effort estimation in the early stages of the software 
life cycle using a cascade correlation neural network model,” in Proceedings - 13th ACIS International 
Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed 
Computing, SNPD 2012, 2012. 

Z. C. Ani and S. Basri, “A CASE STUDY OF EFFORT ESTIMATION IN AGILE SOFTWARE DEVELOPMENT 
USING USE CASE POINTS,” Malaysia. Sci.Int.(Lahore), vol. 25, no. 4, pp. 1111–1126, 2013. 

L. M. Alves, A. Sousa, P. Ribeiro, and R. J. Machado, “An empirical study on the estimation of software 
development effort with use case points,” in Proceedings - Frontiers in Education Conference, FIE, 
2013. 

T. E. Ayyıldız, “Comparison of Three Software Effort Estimation Methodologies with Case Study,” 
AWERProcedia Inf. Technol. Comput. Sci., vol. 04, pp. 257–262, 2013. 

A. W. M. M. Parvez, “Efficiency factor and risk factor based user case point test effort estimation 
model compatible with agile software development,” in Proceedings - 2013 International Conference 
on Information Technology and Electrical Engineering: “Intelligent and Green Technologies for 
Sustainable Development”, ICITEE 2013, 2013. 

R. Alves, P. Valente, and N. J. Nunes, “Improving software effort estimation with human-centric 
models: a comparison of UCP and iUCP accuracy,” in Proceedings of the 5th ACM SIGCHI symposium 
on Engineering interactive computing systems, 2013, pp. 287–296. 

[63] 

C. Nagar and A. Dixit, “Efforts Estimation by Use Case Point using Experience Data,” Int. J. Comput. 
Appl., vol. 61, no. 17, pp. 975–8887, 2013. 

[64]  M. Azzeh, “Software cost estimation based on use case points for global software development,” in 

2013 5th International Conference on Computer Science and Information Technology, CSIT 2013 - 
Proceedings, 2013. 

[65] 

[66] 

[67] 

N. A. Ahmed and A. H. Ahmed, “Enabling complexity use case function point on service-oriented 
architecture,” in Proceedings - 2013 International Conference on Computer, Electrical and Electronics 
Engineering: “Research Makes a Difference”, ICCEEE 2013, 2013. 

A. B. Nassif, L. F. Capretz, and D. Ho, “Calibrating use case points,” in 36th International Conference on 
Software Engineering, ICSE Companion 2014 - Proceedings, 2014. 

S. M. Satapathy and S. K. Rath, “Use Case Point Approach Based Software Effort Estimation using 
Various Support Vector Regression Kernel Methods,” arXiv Prepr. arXiv1401.3069, vol. 1, no. 1, pp. 1–13, 
2014. 

[68] 

P. Jovan, P. Sofija, B. M.-… F. T. (TELFOR), and U. 2015, “Enhancing use case point estimation method 
using fuzzy algorithms,” in 23rd Telecommunications Forum Telfor (TELFOR), 2015. 

[69]  M. Saroha, S. S.-I. C. on Computing, and U. 2015, “Software effort estimation using enhanced use case 
point model,” in International Conference on Computing, Communication & Automation, 2015. 

[70]  M. Azzeh, A. B. Nassif, and S. Banitaan, “An application of classification and class decomposition to 

use case point estimation method,” in Proceedings - 2015 IEEE 14th International Conference on 
Machine Learning and Applications, ICMLA 2015, 2016, pp. 1268–1271. 

[71] 

[72] 

[73] 

[74] 

[75] 

Y. Xie, J. Guo, and A. Shen, “Use case points method of software size measurement based on fuzzy 
inference,” in Lecture Notes in Electrical Engineering, 2015. 

S. K. Rath, B. P. Acharya, and S. M. Satapathy, “Early stage software effort estimation using random 
forest technique based on use case points,” IET Softw., vol. 10, no. 1, 2016. 

S. Khatri, S. Malhotra, P. J.-2016 5th International, and U. 2016, “Use case point estimation technique in 
software development,” in 2016 5th international conference on reliability, infocom technologies and 
optimization (trends and future directions)(ICRITO, 2016. 

K. Iskandar, F. L. Gaol, B. Soewito, H. L. H. S. Warnars, and R. Kosala, “Software size measurement of 
knowledge management portal with use case point,” in Proceeding - 2016 International Conference on 
Computer, Control, Informatics and its Applications: Recent Progress in Computer, Control, and 
Informatics for Data Science, IC3INA 2016, 2017. 

B. K. Park, S. Y. Moon, and R. Y. C. Kim, “Improving Use Case Point (UCP) Based on Function Point (FP) 
Mechanism,” in 2016 International Conference on Platform Technology and Service, PlatCon 2016 - 
Proceedings, 2016. 

[76] 

R. Silhavy, P. Silhavy, and Z. Prokopova, “Analysis and selection of a regression model for the Use 
Case Points method using a stepwise approach,” J. Syst. Softw., vol. 125, 2017. 

[77]  M. Badri, L. Badri, W. Flageol, and F. Toure, “Source code size prediction using use case metrics: an 
empirical comparison with use case points,” Innov. Syst. Softw. Eng., vol. 13, no. 2–3, 2017. 

[78] 

[79] 

[80] 

[81] 

[82] 

[83] 

[84] 

H. Leslie Hendric Spits Warnars, E. Abdurachman, and F. Lumban Gaol, “Use case point as software 
size measurement with study case of Academic Information System.” 

D. Kurniadi, S. Sasmoko, H. L. H. S. Warnars, and F. L. Gaol, “Software size measurement of student 
information terminal with use case point,” in 2017 IEEE International Conference on Cybernetics and 
Computational Intelligence, CyberneticsCOM 2017 - Proceedings, 2018. 

Z. Prokopova, R. Silhavy, and P. Silhavy, “The effects of clustering to software size estimation for the 
use case points methods,” in Advances in Intelligent Systems and Computing, 2017, pp. 479–490. 

S. Bagheri and A. Shameli-Sendi, “Software Project Estimation Using Improved Use Case Point,” in 
2018 IEEE 16th International Conference on Software Engineering Research, Management and 
Applications (SERA), 2018. 

K. Qi, B. B.-P. of the 10th I. W. On, and U. 2018, “Detailed use case points (DUCPs): a size metric 
automatically countable from sequence and class diagrams,” in 2018 IEEE/ACM 10th International 
Workshop on Modelling in Software Engineering (MiSE), 2018. 

H. L. T. K. Nhung, H. T. Hoc, and V. Van Hai, “A Review of Use Case-Based Development Effort 
Estimation Methods in the System Development Context,” in Proceedings of the Computational 
Methods in Systems and Software., 2019, pp. 484–499. 

A. Effendi, R. Setiawan, Z. R.-P. C. Science, and U. 2019, “Adjustment Factor for Use Case Point 
Software Effort Estimation (Study Case: Student Desk Portal),” in Procedia Computer Science, 2019, 
pp. 691–698. 

Appendix A. Selected papers 

ID 
S1 
S2 
S3 
S4 
S5 
S6 
S7 
S8 
S9 
S10 
S11 
S12 
S13 
S14 

S15 
S16 
S17 
S18 
S19 

S20 
S21 
S22 
S23 
S24 
S25 
S26 

S27 
S28 
S29 

S30 
S31 
S32 
S33 

S34 
S35 
S36 
S37 

S38 
S39 

A1 

A2 

A3 

A4 

A5 

A6 

S40  A7 

Title 

Resource Estimation for Objectory Projects 
Estimating Software Development Effort Based on Use Cases –Experiences from Industry 
Comparing Effort Estimates Based on Use Case Points with Expert Estimates  
Improving Estimation Practices by Applying Use Case Models 
Using Fuzzy Theory for Effort Estimation of Object-Oriented Software 
Estimating Effort by Use Case Points: Method, Tool and Case Study 
A Multiple Case Study of Effort Estimation Based on Use Case Points 
Effort Estimation of Use Cases for Incremental Large-Scale Software Development 
Estimating Software Based on Use Case Points 
A Case Study on The Evaluation Of COSMIC-FFP And Use Case Points 
Project Estimation with Use Case Points 
Software Effort Estimation Based on Use Cases 
Improving the reliability of transaction identification in use cases 
Revised Use Case Point Method - Effort Estimation in Development Projects for Business 
Applications 
Employing Use Cases to Early Estimate Effort with Simpler Metrics 
Task Effort Fuzzy Estimator for Software Development 
Cost Estimation Using Extended Use Case Point (E-UCP) Model 
Extended Use Case Point Method for Software Cost Estimation 
Transactions and Paths: Two Use Case Based Metrics Which Improve the Early Effort 
Estimation  
Neural Network Model for Software Size Estimation Using Use Case Point Approach 
Software Estimation Based on Use Case Size 
Software Estimation in The Early Stages of The Software Life Cycle 
The Role of The Measure of Functional Complexity in Effort Estimation 
Enhancing Use Case Points Estimation Method Using Soft Computing Techniques 
Enhancing Use-Case-Based Effort Estimation with Transaction Types 
A Regression Model with Mamdani Fuzzy Inference System for Early Software Effort 
Estimation Based on Use Case Diagrams 
Application of Estimating Based on Use Cases in Software Industry  
Cost Estimation for Distributed Systems Using Synthesized Use Case Point Model 
Estimating Software Effort Based on Use Case Point Model Using Sugeno Fuzzy Inference 
System. 
Fuzzy Logic as A Basic for Use Case Point Estimation 
iUCP: Estimating Interactive-Software Project Size with Enhanced Use-Case Points 
Regression Model for Software Effort Estimation Based on The Use Case Point Method  
Software Complexity Level Determination Using Software Effort Estimation Use Case 
Points Metrics  
Developmental Size Estimation for Object-Oriented Software Based on Analysis Model 
Simplifying Effort Estimation Based on Use Case Points 
A Treeboost Model for Software Effort Estimation Based on Use Case Points 
Enhancing the Results of UCP In Cost Estimation Using New External Environmental 
Factors 
Estimating Software Effort Using an ANN Model Based on Use Case Points 
Software Effort Estimation in The Early Stages of The Software Life Cycle Using A Cascade 
Correlation Neural Network Model  
Object Oriented Software Effort Estimate with Adaptive Neuro Fuzzy Use Case Size Point 
(ANFUSP) 

Ref 
[10] 
[22] 
[33] 
[34] 
[35] 
[36] 
[37] 
[4] 
[38] 
[39] 
[40] 
[25] 
[41] 

[42] 

[28] 
[43] 
[23] 
[24] 

[27] 

[44] 
[45] 
[46] 
[29] 
[15] 
[30] 

[47] 

[48] 
[49] 

[26] 

[50] 
[51] 
[52] 

[53] 

[54] 
[1] 
[5] 

[55] 

[56] 

[57] 

[16] 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S41 

S42 
S43 

S44  A8 
S45 

A9 

S46 

S47 
S48 
S49 
S50 
S51 

Towards an Early Software Estimation Using Log-Linear Regression and A Multilayer 
Perceptron Model 
A Case Study of Effort Estimation In a Agile Software Development Using Use Case Points 
An Empirical Study on The Estimation of Software Development Effort with Use Case 
Points 
Comparison of Three Software Effort Estimation Methodologies with Case Study 
Efficiency Factor and Risk Factor Based User Case Point Test Effort Estimation Model 
Compatible with Agile Software Development 
Improving Software Effort Estimation with Human-Centric Models: A Comparison of UCP 
and iUCP Accuracy  
Efforts Estimation by Use Case Point Using Experience Data 
Software Cost Estimation Based on Use Case Points for Global Software Development 
Enabling Complexity Use Case Function Point on Service-Oriented Architecture 
Calibrating Use Case Points 
Use Case Point Approach Based Software Eﬀort Estimation Using Various Support Vector 
Regression Kernel Methods 
Enhancing Use Case Point Estimation Method Using Fuzzy Algorithms 
Software Effort Estimation Using Enhanced Use Case Point Model 

S52 
S53 
S54  A10  An Application of Classification and Class Decomposition to Use Case Point Estimation 

S55 
S56 
S57 

S58 
S59 
S60 
S61 

S62 

Method 
Use Case Points Method of Software Size Measurement Based on Fuzzy Inference 

A11  A Hybrid Model for Estimating Software Project Effort from Use Case Points 

A12 

Early Stage Software Effort Estimation Using Random Forest Technique Based on Use 
Case Points 
Use Case Point Estimation Technique in Software Development 
Software Size Measurement of Knowledge Management Portal with Use Case Point 
Improving Use Case Point (UCP) Based on Function Point (FP) Mechanism 
The Development of Method of The Enhancement of Technical Factor (TF) And 
Environmental Factor (EF) to the Use Case Point (UCP) To Calculate The Estimation of 
Software's Effort 
Analyzing the Relationship Between Project Productivity and Environment Factors in The 
Use Case Points Method 

S63 

A13  Analysis and Selection of a Regression Model for The Use Case Points Method Using A 

S64 

S65 

S66 
S67 
S68 

S69 
S70 
S71 
S72 

S73 
S74 

Stepwise Approach 
Source Code Size Prediction Using Use Case Metrics: An Empirical Comparison with Use 
Case Points. 
Use Case Point as Software Size Measurement with Study Case of Academic Information 
System, 
Software Size Measurement of Student Information Terminal with Use Case Point 
The Effects of Clustering to Software Size Estimation for The Use Case Points Methods. 
Comparative Analysis of Soft Computing Techniques for Predicting Software Effort Based 
Use Case Points 
Project Productivity Evaluation in Early Software Effort Estimation 
Evaluating Subset Selection Methods for Use Case Points Estimation 
Software Project Estimation Using Improved Use Case Point 

A14  Detailed Use Case Points (Ducps): A Size Metric Automatically Countable from Sequence 

and Class Diagrams 
Ensemble of Learning Project Productivity in Software Effort Based on Use Case Points 
A Review of Use Case-Based Development Effort Estimation Methods in The System 
Development Context 

S75 

A15  Adjustment Factor for Use Case Point Software Effort Estimation (Study Case: Student 

Desk Portal) 

[3] 

[58] 

[59] 

[60] 

[61] 

[62] 

[63] 
[64] 
[65] 
[66] 

[67] 

[68] 
[69] 

[70] 

[71] 
[2] 

[72] 

[73] 
[74] 
[75] 

[31] 

[19] 

[76] 

[77] 

[78] 

[79] 
[80] 

[17] 

[20] 
[18] 
[81] 

[82] 

[32] 

[83] 

[84] 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Appendix B. Quality assessment results 

ID 
S1 
S2 
S3 
S4 
S5 
S6 
S7 
S8 
S9 
S10 
S11 
S12 
S13 
S14 
S15 
S16 
S17 
S18 
S19 
S20 
S21 
S22 
S23 
S24 
S25 
S26 
S27 
S28 
S29 
S30 
S31 
S32 
S33 
S34 
S35 
S36 
S37 
S38 

QA1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
0.5 
1 
1 
1 
1 
1 
1 
1 

QA2 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
0.5 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 

QA3 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 

QA4 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 

QA5 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
1 
1 
0 
1 

QA6 
0 
1 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
1 
1 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
1 
0 
1 

QA7 
0.5 
0.5 
1 
0.5 
1 
0.5 
1 
1 
0.5 
0.5 
0.5 
1 
1 
0.5 
0.5 
0.5 
0.5 
0.5 
0.5 
0.5 
0.5 
0.5 
0.5 
0 
0.5 
0.5 
0.5 
0.5 
1 
0.5 
1 
0.5 
0.5 
0.5 
1 
0.5 
0.5 
0.5 

ID 
S39 
S40 
S41 
S42 
S43 
S44 
S45 
S46 
S47 
S48 
S49 
S50 
S51 
S52 
S53 
S54 
S55 
S56 
S57 
S58 
S59 
S60 
S61 
S62 
S63 
S64 
S65 
S66 
S67 
S68 
S69 
S70 
S71 
S72 
S73 
S74 
S75 

QA1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 

Total 
4.5 
5.5 
5 
4.5 
5 
5.5 
5 
5 
5.5 
4.5 
4.5 
4.5 
5 
4.5 
5.5 
4.5 
4.5 
4.5 
4.5 
4.5 
4.5 
4.5 
5.5 
5 
5.5 
4.5 
4.5 
4.5 
5 
4.5 
4.5 
4.5 
4.5 
5.5 
7 
6.5 
4.5 
6.5 

QA2 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 

QA3 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 

QA4 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
0.5 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 

QA5 
1 
0 
1 
0 
0 
0.5 
0.5 
1 
0 
0 
0 
0 
0 
0 
0 
1 
0 
1 
0 
0 
0 
0 
0 
1 
1 
1 
0 
0 
0 
1 
0 
1 
0 
0 
0 
0 
0 

QA6 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
1 
1 
1 
0 
0 
0 
1 
0 
1 
0 
1 
0 
1 
1 

QA7 
0.5 
0.5 
1 
0.5 
0.5 
0 
0 
0.5 
0.5 
0.5 
0.5 
1 
0.5 
0.5 
0.5 
0.5 
0.5 
1 
1 
0.5 
0.5 
0.5 
0.5 
0.5 
1 
1 
0.5 
0.5 
0.5 
0.5 
0.5 
1 
0.5 
0.5 
0.5 
0.5 
0.5 

Total 
5.5 
4.5 
7 
4.5 
4.5 
4.5 
4.5 
5.5 
4.5 
4.5 
4.5 
4.5 
4.5 
4.5 
4.5 
5.5 
4.5 
7 
5 
4.5 
4.5 
4.5 
4.5 
6.5 
7 
7 
4.5 
4.5 
4.5 
6.5 
4.5 
7 
4.5 
5.5 
4.5 
5.5 
5.5 

 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
Appendix C. Classification of the selected studies. 

ID 
S1 
S2 
S3 
S4 
S5 
S6 
S7 
S8 
S9 
S10 
S11 
S12 
S13 
S14 
S15 
S16 
S17 
S18 
S19 
S20 
S21 
S22 
S23 
S24 
S25 
S26 
S27 
S28 
S29 
S30 
S31 
S32 
S33 
S34 
S35 
S36 
S37 
S38 
S39 
S40 
S41 
S42 
S43 
S44 
S45 
S46 
S47 
S48 
S49 
S50 
S51 
S52 
S53 
S54 
S55 
S56 
S57 
S58 
S59 
S60 

Source Type 
white paper 
Conference 
Conference 
Conference 
Conference 
Conference 
Conference 
Conference 
Conference 
Conference 
Journal 
Conference 
Journal 
Conference 
Journal 
Journal 
Conference 
Conference 
Conference 
Conference 
Conference 
Conference 
Conference 
Journal 
Journal 
Conference 
Conference 
Conference 
Conference 
Conference 
Magazine 
Conference 
Conference 
Journal 
Journal 
Conference 
Conference 
Conference 
Conference 
Journal 
Journal 
Conference 
Conference 
Conference 
Conference 
Conference 
Journal 
Conference 
Conference 
Conference 
Journal 
Conference 
Conference 
Conference 
Conference 
Journal 
Journal 
Conference 
Conference 
Conference  

Year 
1993 
2001 
2002 
2002 
2004 
2004 
2005 
2005 
2005 
2006 
2006 
2006 
2011 
2008 
2008 
2008 
2009 
2009 
2009 
2010 
2010 
2010 
2010 
2010 
2010 
2011 
2011 
2011 
2011 
2011 
2011 
2011 
2011 
2011 
2011 
2012 
2012 
2012 
2012 
2012 
2012 
2013 
2013 
2013 
2013 
2013 
2013 
2013 
2013 
2014 
2014 
2015 
2015 
2015 
2015 
2016 
2016 
2016 
2016 
2016 

Contribution Type 
Technique 
Validation 
Comparison 
Enhancement 
Enhancement 
Tool 
Validation 
Enhancement 
Validation 
Comparison 
Validation 
Technique 
Model 
Enhancement 
Technique 
Technique 
Enhancement 
Model 
Enhancement 
Model 
Technique 
Comparison 
Validation 
Technique 
Technique 
Technique 
Comparison 
Enhancement 
Technique 
Technique 
Enhancement 
Technique 
Validation 
Technique 
Validation 
Model 
Enhancement 
Model 
Model 
Model 
Model 
Enhancement 
Validation 
Comparison 
Technique 
Validation + Model 
Enhancement 
Enhancement 
Technique 
Enhancement 
Model 
Enhancement 
Enhancement 
Technique 
Enhancement 
Model 
Model 
Comparison 
Validation 
Enhancement 

Research Approach 
Proposal + Case Study 
Case Study 
Comparative studies 
Case Study 
Evaluation 
Case Study 
Case Study 
Case Study 
Case Study 
Case Study 
Case Study 
Evaluation + Comparative studies 
Comparative studies 
Evaluation 
Case Study 
Evaluation 
Theory 
Case Study 
Evaluation + Case Study 
Case Study 
Evaluation 
Comparative studies 
Evaluation 
Evaluation 
Case Study 
Evaluation 
Comparative studies 
Theory 
Evaluation 
Evaluation 
Theory 
Evaluation 
Evaluation 
Review + Survey 
Evaluation 
Evaluation 
Proposal 
Evaluation 
Evaluation 
Evaluation 
Evaluation 
Theory 
Review 
Comparative studies 
Evaluation 
Evaluation + Comparative studies 
Evaluation 
Proposal 
Proposal + Case Study 
Comparative studies 
Comparative studies 
Comparative studies 
Case Study 
Evaluation + Comparative studies 
Evaluation 
Comparative studies 
Evaluation + Comparative studies 
Proposal 
Proposal 
Evaluation 

Techniques 
Kerner 
Kerner 
Kerner 
Kerner 
Fuzzy Logic 
Kerner 
Kerner 
Kerner 
Kerner 
Kerner 
Kerner 
Kerner 
NN+SR 
Kerner 
Kerner 
Fuzzy Logic 
Kerner 
Fuzzy Logic + BBN 
Kerner 
NN 
Kerner 
Fuzzy Logic + Kerner 
Fuzzy Logic + Kerner 
Fuzzy Logic + NN 
Kerner 
Fuzzy Logic 
Kerner 
Kerner 
Fuzzy Logic 
Fuzzy Logic 
Kerner 
Regression 
Kerner 
New Model 
Karner + Regression 
Treeboost + Regression 
Kerner 
ANN 
ANN 
ANFIS 
Regression 
N/A 
Kerner 
Kerner 
N/A 
Kerner 
Kerner 
Kerner 
Kerner 
Fuzzy Logic + NN 
SVM 
Fuzzy Logic 
Kerner 
Kerner 
Kerner 
Kerner 
Kerner 
N/A 
N/A 
N/A 

Digital Library 
Google Scholar 
Springer 
Springer 
Springer 
IEEE 
IEEE 
IEEE 
ACM 
ACM 
Google Scholar 
Google Scholar 
IEEE 
Science Direct 
Google Scholar 
Springer 
Google Scholar 
IEEE 
IEEE 
ACM 
IEEE 
IEEE 
Google Scholar 
ACM 
Google Scholar 
Google Scholar 
IEEE 
IEEE 
Springer 
IEEE 
IEEE 
IEEE 
Google Scholar 
IEEE 
Google Scholar 
Science Direct 
IEEE 
IEEE 
IEEE 
IEEE 
Google Scholar 
Science Direct 
Google Scholar 
IEEE 
Google Scholar 
IEEE 
ACM 
Google Scholar 
IEEE 
IEEE 
ACM 
Google Scholar 
IEEE 
IEEE 
IEEE 
Springer 
Science Direct 
IET Software 
IEEE 
IEEE 
IEEE 

S61 
S62 
S63 
S64 
S65 
S66 
S67 
S68 
S69 
S70 
S71 
S72 
S73 
S74 
S75 

Conference 
Journal 
Journal 
Journal 
Conference 
Conference 
Conference 
Journal 
Journal 
Journal 
Conference 
Conference 
Conference 
Conference 
Conference 

2016 
2017 
2017 
2017 
2017 
2017 
2017 
2018 
2018 
2018 
2018 
2018 
2018 
2019 
2019 

Enhancement 
Validation 
Technique 
Validation 
Validation 
Validation 
Validation 
Comparison 
Validation 
Validation 
Technique + Enhancement 
Enhancement 
Technique 
Validation 
Validation 

Evaluation 
Case Study 
Evaluation 
Evaluation 
Case Study + Evaluation 
Review 
Evaluation 
Comparative studies 
Evaluation 
Evaluation 
Case Study 
Proposal + Case Study 
Comparative studies 
Review 
Case Study 

Kerner 
Kerner 
Stepwise multiple linear regression 
simple linear regression 
Kerner 
N/A 
Kerner 
Kerner 
Kerner 
regression + subset selection 
Kerner 
Kerner 
Kerner 
N/A 
Kerner 

IEEE 
Wiley 
Science Direct 
Springer 
IEEE 
IEEE 
Springer 
IET Software 
Wiley 
Science Direct 
IEEE 
IEEE 
IEEE 
Springer 
Science Direct 

 
