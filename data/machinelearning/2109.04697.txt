Unfolding Projection-free SDP Relaxation of Binary Graph Classiﬁer via
GDPA Linearization

Cheng Yang †, Gene Cheung ‡, Wai-tian Tan §, Guangtao Zhai †
† Shanghai Jiaotong University, Shanghai, China ‡ York University, Toronto, Canada § Cisco Systems, San José, CA

1
2
0
2

p
e
S
0
1

]

G
L
.
s
c
[

1
v
7
9
6
4
0
.
9
0
1
2
:
v
i
X
r
a

Abstract

Algorithm unfolding creates an interpretable and parsi-
monious neural network architecture by implementing
each iteration of a model-based algorithm as a neural
layer. However, unfolding a proximal splitting algorithm
with a positive semi-deﬁnite (PSD) cone projection op-
erator per iteration is expensive, due to the required full
matrix eigen-decomposition. In this paper, leveraging a
recent linear algebraic theorem called Gershgorin disc
perfect alignment (GDPA), we unroll a projection-free al-
gorithm for semi-deﬁnite programming relaxation (SDR)
of a binary graph classiﬁer, where the PSD cone con-
straint is replaced by a set of “tightest possible” linear
constraints per iteration. As a result, each iteration only
requires computing a linear program (LP) and one ex-
treme eigenvector. Inside the unrolled network, we opti-
mize parameters via stochastic gradient descent (SGD)
that determine graph edge weights in two ways: i) a
metric matrix that computes feature distances, and ii) a
sparse weight matrix computed via local linear embed-
ding (LLE). Experimental results show that our unrolled
network outperformed pure model-based graph classi-
ﬁers, and achieved comparable performance to pure data-
driven networks but using far fewer parameters.

INTRODUCTION
While generic and powerful deep neural networks (DNN)
(LeCun, Bengio, and Hinton 2015) can achieve state-of-the-
art performance using large labelled datasets for many data-
ﬁtting problems such as image restoration and classiﬁcation
(Zhang et al. 2017; Krizhevsky, Sutskever, and Hinton 2012),
they operate as “black boxes” that are difﬁcult to explain. To
build an interpretable system targeting a speciﬁc problem
instead, algorithm unfolding (Monga, Li, and Eldar 2021)
takes a model-based iterative algorithm, implements (unrolls)
each iteration as a neural layer, and stacks them in sequence
to compose a network architecture. As a pioneering example,
LISTA (Gregor and LeCun 2010) implemented each iteration
of a sparse coding algorithm called ISTA (Beck and Teboulle
2009)—composed of a gradient descent step and a soft thresh-
olding step—as linear and ReLU operators in a neural layer.
By optimizing two matrix parameters in the linear operator
per layer end-to-end via stochastic gradient descent (SGD)

Preprint. Under review.

Figure 1: Overview of our SDP relaxation (SDR) network.
Upper: SDR network composed of stacked SDR layers.
Lower: The architecture of a single SDR layer. The lower
triangluar matrix Q and local linear embedding (LLE) weight
matrix C are used to deﬁne the conically combined Lapla-
cian matrix L, which is then used together with the SDP dual
variables y and z to deﬁne the matrix ¯H. Next, ¯H is passed
to differentiable LOBPCG solver and LP solver to update y
and z. The network is end-to-end trained with an MSE loss
of the predicted labels.

(Bottou 1998), LISTA converged faster and had better perfor-
mance. This means that the required iteration / neural layer
count was comparatively small, resulting in a parsimonious
architecture with few learned network parameters.

However, algorithm unfolding is difﬁcult if the iterative
algorithm performs proximal splitting (Boyd et al. 2011)
with a positive semi-deﬁnite (PSD) cone projection operator
per iteration; PSD cone projection is common in algorithms
solving a semi-deﬁnite programming (SDP) problem with a
PSD cone constraint (Gartner and Matousek 2012). A PSD
cone projection for a matrix variable H requires full matrix
eigen-decomposition on H with complexity O(N 3). Not
only is the computation cost of the projection in a neural
layer expensive, optimizing network parameters through the
projection operator via SGD is difﬁcult.

In this paper, using binary graph classiﬁer (Zhou et al.
2003; Belkin, Matveeva, and Niyogi 2004; Guillory and
Bilmes 2009; Luo et al. 2010) as an illustrative application,
we demonstrate how PSD cone projection can be entirely
circumvented for an SDP problem, facilitating algorithm
unfolding and end-to-end optimization of network parame-
ters without sacriﬁcing performance. Speciﬁcally, we ﬁrst
replace the PSD cone constraint in the original semi-deﬁnite
programming relaxation (SDR) (Li, Liu, and Tang 2008) of

LOBPCGSDRlayer 1…predictedgroundtruthLPLOBPCGinitial sets of scalars SDRlayer 2SDRlayer 
 
 
 
 
 
the NP-hard graph classiﬁer problem with “tightest possible”
linear constraints per iteration, thanks to a recent linear al-
gebraic theorem called Gershgorin disc perfect alignment
(GDPA) (Yang, Cheung, and Hu 2021). Together with the
linear objective, each iteration computes only a linear pro-
gram (LP) (Vanderbei 2021) and one extreme eigenvector
(computable in O(N ) using LOBPCG (Knyazev 2001)).

We next unroll the now projection-free iterative algorithm
into an interpretable network, and optimize parameters that
determine graph edge weights per neural layer via SGD in
two ways. First, assuming edge weight wi,j is inversely pro-
portional to feature distance di,j between nodes i and j en-
dowed with feature vectors fi and fj respectively, we optimize
a PSD metric matrix M via Cholesky factorization (Golub
and Van Loan 1996) M = QQ(cid:62) that computes Mahalanobis
distance (Mahalanobis 1936) as di,j = (fi − fj)(cid:62)M(fi − fj).
Second, we initialize a non-negative symmetric weight matrix
via local linear embedding (LLE) (Roweis and Saul 2000;
Ghojogh et al. 2020) given feature vectors fi’s, which we
subsequently ﬁne-tune per layer in a semi-supervised man-
ner. We employ a conic combination of the two resulting
graph Laplacian matrices for classiﬁcation in each layer. An
illustration of the unrolled network is shown in Fig. 1.

We believe this methodology of replacing the PSD cone
constraint by linear constraints per iteration—leading to an
iterative algorithm amenable to algorithm unfolding—can
be more generally applied to a broad class of SDP problems
with PSD cone constraints (Gartner and Matousek 2012). For
binary graph classiﬁers, experimental results show that our
interpretable unrolled network substantially outperformed
pure model-based classiﬁers (Yang et al. 2021), and achieved
comparable performance as pure data-driven networks (Le-
Cun, Bengio, and Hinton 2015) but using noticeably fewer
parameters.

RELATED WORK
Algorithm unfolding is one of many classes of approaches in
model-based deep learning (Shlezinger et al. 2021), and has
been shown effective in creating interpretable network archi-
tectures for a range of data-ﬁtting problems (Monga, Li, and
Eldar 2021). We focus on unfolding of iterative algorithms
involving PSD cone projection (O’Donoghue et al. 2016) that
are common when addressing SDR of NP-hard quadratically
constrained quadratic programming (QCQP) problems (Luo
et al. 2010), of which binary graph classiﬁer is a special case.
Graph-based classiﬁcation was ﬁrst studied two decades
ago (Zhou et al. 2003; Belkin, Matveeva, and Niyogi 2004;
Guillory and Bilmes 2009). An interior point method tai-
lored for the slightly more general binary quadratic problem1
(BQP) has complexity O(N 3.5 log(1/(cid:15))), where (cid:15) is the tol-
erable error (Helmberg et al. 1996). Replacing PSD cone
constraint M (cid:23) 0 with a factorization M = XX(cid:62) was
proposed (Shah et al. 2016), but it resulted in a non-convex
optimization for X that was solved locally via alternating
minimization, where in each iteration a matrix inverse of
worst-case complexity O(N 3) was required. More recent

1BQP objective takes a quadratic form x(cid:62)Qx, but Q is not

required to be a Laplacian matrix to a similarity graph.

ﬁrst-order methods such as (O’Donoghue et al. 2016) used
ADMM (Boyd et al. 2011), but still requires expensive PSD
cone projection per iteration. In contrast, leveraging GDPA
theory (Yang, Cheung, and Hu 2021), our algorithm is en-
tirely projection-free.

GDPA theory was developed for metric learning (Moutaﬁs,
Leng, and Kakadiaris 2017) to optimize a PD metric matrix
M, given a convex and differentiable objective Q(M), in
a Frank-Wolfe optimization framework (Jaggi 2013). This
paper leverages GDPA (Yang, Cheung, and Hu 2021) in an
entirely different direction for unfolding of a projection-free
graph classiﬁer learning algorithm.

PRELIMINARIES

Graph Deﬁnitions
A graph is deﬁned as G(V, E, W), with node set V =
{1 . . . , N }, and edge set E = {(i, j)}, where (i, j) means
nodes i and j are connected with weight wi,j ∈ R. A node i
may have a self-loop of weights ui ∈ R. Denote by W the
adjacency matrix, where Wi,j = wi,j and Wi,i = ui. We as-
sume that edges are undirected, and W is symmetric. Deﬁne
next the diagonal degree matrix D, where Di,i = (cid:80)
j Wi,j.
The combinatorial graph Laplacian matrix (Ortega et al.
2018) is then deﬁned as L (cid:44) D − W. To account for self-
loops, the generalized graph Laplacian matrix is deﬁned as
L (cid:44) D−W+diag(W). Note that any real symmetric matrix
can be interpreted as a generalized graph Laplacian matrix.
The graph Laplacian regularizer (GLR) (Pang and Cheung
2017) that quantiﬁes smoothness of signal x ∈ RN w.r.t.
graph speciﬁed by L is

x(cid:62)Lx =

(cid:88)

wi,j(xi − xj)2 +

(i,j)∈E

uix2
i .

(1)

(cid:88)

i∈V

GLR is also the objective of our graph-based classiﬁcation
problem.

GDPA Linearization
To ensure matrix variable M is PSD without eigen-
decomposition, we leverage GDPA (Yang, Cheung, and Hu
2021). Given a real symmetric matrix, we deﬁne a Ger-
shgorin disc Ψi corresponding to row i of M with center
ci(M) (cid:44) Mi,i and radius ri(M) (cid:44) (cid:80)
j(cid:54)=i |Mi,j|. By Gersh-
gorin Circle Theorem (GCT) (Varga 2004), the smallest real
eigenvalue λmin(M) of M is lower-bounded by the smallest
disc left-end λ−

min(M), i.e.,

min(M) (cid:44) min
λ−

i

ci(M) − ri(M) ≤ λmin(M).

(2)

Thus, to ensure M (cid:23) 0, one can impose the sufﬁcient condi-
tion λ−

min(M) ≥ 0, or equivalently

ci(M) − ri(M) ≥ 0, ∀i.

(3)

However, GCT lower bound λ−
min(M) tends to be loose. As
an example, consider the positive deﬁnite (PD) matrix M
in Fig. 2 with λmin(M) = 0.1078. The ﬁrst disc left-end is
c1(M) − r1(M) = 2 − 3 = −1, and λ−

min(M) < 0.

M =

SMS−1 =









2 −2 −1
5 −2
−2
4
−1 −2





2
−3.0746
−1.6915 −2.2007

−1.301 −0.5912
−1.8176
4

5





Figure 2: Example of a PD matrix M and its similarity transform
˜M = SMS−1, and their respective Gershgorin discs Ψi. Gersh-
gorin disc left-ends of ˜M are aligned at λmin(M) = 0.1078.

1 , . . . , v−1

GDPA provides a theoretical foundation to tighten the
GCT lower bound. Speciﬁcally, GDPA states that given a
generalized graph Laplacian matrix M corresponding to a
“balanced” signed graph2 G (Cartwright and Harary 1956),
one can perform a similarity transform3, ˜M = SMS−1,
where S = diag(v−1
N ) and v is the ﬁrst eigenvector
of M, such that all the disc left-ends of ˜M are exactly aligned
at λmin(M) = λmin( ˜M). This means that transformed ˜M
min( ˜M) = λmin( ˜M); i.e., the GCT lower bound is
satisﬁes λ−
the tightest possible after an appropriate similarity transform.
Continuing our example, similarity transform ˜M = SMS−1
of M has all its disc left-ends exactly aligned at λmin(M) =
λmin( ˜M) = 0.1078.

1 , . . . , v−1

min(StMS−1

Leveraging GDPA, (Yang, Cheung, and Hu 2021) de-
veloped a fast metric learning algorithm, in which the
PSD cone constraint M (cid:23) 0 is replaced by linear con-
straints λ−
t ) ≥ 0 per iteration, where St =
diag(v−1
N ) and v is the ﬁrst eigenvector of previ-
ous solution Mt−1. Assuming that the algorithm always
seeks solutions M in the space of graph Laplacian ma-
trices of balanced graphs, this means previous PSD solu-
tion Mt−1 remains feasible at iteration t, since by GDPA
λ−
min(StMt−1S−1
t ) = λmin(Mt−1) ≥ 0. Together with a
convex and differentiable objective, the optimization can thus
be solved efﬁciently in each iteration using the projection-
free Frank-Wolfe procedure (Jaggi 2013). This process of
computing the ﬁrst eigenvector v of a previous PSD solution
Mt−1 to establish linear constraints λ−
t ) ≥ 0 in
the next iteration, replacing the PSD cone constraint M (cid:23) 0,
is called GDPA linearization.

min(StMS−1

2A balanced graph has no cycles of odd number of negative
edges. By the Cartwright-Harary Theorem, a graph is balanced iff
nodes can be colored into red/blue, so that each positive/negative
edge connects nodes of the same/different colors.

3A similarity transform B = SAS−1 and the original matrix A

share the same set of eigenvalues (Varga 2004).

Figure 3: (a) An example 3-node line graph. (b) Unbalanced graph
corresponding to solution H to SDR dual (8) interpreted as graph
Laplacian matrix. (c) Balanced graph corresponding to solution
¯H to modiﬁed SDR dual (10) interpreted as Laplacian. Positive /
negative edges are colored in blue / red. Self-loop weight u4 in (b)
for node 4 is u4 = y4 + z1 + z2.

GRAPH CLASSIFIER LEARNING
We ﬁrst formulate the binary graph classiﬁer learning prob-
lem and relax it to an SDP problem. We then present its SDP
dual with dual variable matrix H. Finally, we augment the
SDP dual with variable ¯H, which is a graph Laplcian to a
balanced graph, amenable to GDPA linearization.

SDP Primal
Given a PSD graph Laplacian matrix L ∈ RN ×N of a pos-
itive similarity graph Go (i.e., wi,j ≥ 0, ∀(i, j) ∈ E), we
formulate a graph-based binary classiﬁcation problem as

x(cid:62)Lx,

s.t.

min
x

(cid:26) x2

i = 1, ∀i ∈ {1, . . . , N }
xi = ˆxi, ∀i ∈ {1, . . . , M }

.

(4)

where {ˆxi}M
i=1 are the M known binary labels. The quadratic
objective in (4) is a GLR (1), promoting a label solution x that
is smooth w.r.t. graph Go speciﬁed by L. The ﬁrst constraint
ensures xi is binary, i.e., xi ∈ {−1, 1}. The second constraint
ensures that entries in x agree with known labels {ˆxi}M

As an example, consider a 3-node line graph shown in
Fig. 3(a), where edges (1, 2) and (2, 3) have weights w1,2
and w2,3, respectively. The corresponding adjacency and
graph Laplacian matrices, W and L, are:

i=1.

W =






0
w1,2
0

w1,2
0
w2,3






0
w2,3
0

, L =






d1
−w1,2
0

−w1,2
d2
−w2,3






0
−w2,3
d3

where di = (cid:80)
labels are ˆx1 = 1 and ˆx2 = −1.

j wi,j is the degree of node i. Suppose known

(4) is NP-hard due to the binary constraint. One can deﬁne
a corresponding SDR problem as follows. Deﬁne ﬁrst matrix
X = xx(cid:62), then M = [X x; x(cid:62) 1]. M is PSD because: i)
block [1] is PSD, and ii) the Schur complement of block [1] of
M is X−xx(cid:62) = 0, which is also PSD. Thus, X = xx(cid:62) (i.e.,
rank(X) = 1) implies M (cid:23) 0. X = xx(cid:62) and Xii = 1, ∀i
together imply x2
i = 1, ∀i. To convexify the problem, we
drop the non-convex rank constraint and write the SDR as

min
x,X

Tr(LX) s.t.






Xii = 1, i ∈ {1, . . . , N }
(cid:20) X x
M (cid:44)
x(cid:62) 1
xi = ˆxi, i ∈ {1, . . . , M }

(cid:23) 0

(cid:21)

(5)

where Tr(x(cid:62)Lx) = Tr(Lxx(cid:62)) = Tr(LX). Because (5) has
linear objective and constraints with an additional PSD cone

λmin(M)Σj|j≠i|Mi,j|Mi,iΨ1Ψ2Ψ3GDPA-135λmin(MM)Σj|j≠i|MMi,j|Mi,iΨ1Ψ2Ψ3179-135179(7)

(8)

constraint, M (cid:23) 0, it is an SDP problem. We call (5) the
SDR primal.

Unfortunately, the solution M to (5) is not a graph Lapla-
cian matrix to a balanced graph, and hence GDPA lineariza-
tion cannot be applied. Thus, we next investigate its SDP
dual instead.

SDP Dual with Balanced Graph Laplacian
Following standard SDP duality theory (Gartner and Ma-
tousek 2012), we write the corresponding dual problem as
follows. We ﬁrst deﬁne

Ai = diag(eN +1(i)), Bi =

(cid:20) 0N ×N eN (i)

(cid:21)

e(cid:62)
N (i)

0

(6)

where eN (i) ∈ {0, 1}N is a length-N binary canonical vec-
tor with a single non-zero entry equals to 1 at the i-th entry,
0N ×N is a N -by-N matrix of zeros, and diag(v) is a diago-
nal matrix with diagonal entries equal to v.

Next, we put M known binary labels {ˆxi}M
b ∈ RM of length M ; speciﬁcally, we deﬁne
bi = 2ˆxi, ∀i ∈ {1, . . . , M }.

i=1 into a vector

We are now ready to write the SDR dual of (5) as

min
y,z

N +1y + b(cid:62)z,
1(cid:62)

s.t. H (cid:44)

N +1
(cid:88)

i=1

yiAi +

M
(cid:88)

i=1

ziBi + LN +1 (cid:23) 0

where 1N is an all-one vector of length N , and LN +1 (cid:44)
[L 0N ×1; 01×N 0]. Variables to the dual (8) are y ∈ RN +1
and z ∈ RM .

Given the minimization objective, when bi < 0, the cor-
responding zi must be ≥ 0, since zi < 0 would make H
harder to be PSD (a larger Gershgorin disc radius) while
worsening the objective. Similarly, for bi > 0, zi ≤ 0.
Thus, the signs of zi’s are known beforehand. Without loss
of generality, we assume zi ≤ 0, ∀i ∈ {1, . . . , M1} and
zi ≥ 0, ∀i ∈ {M1 + 1, . . . , M } in the sequel.

Continuing our earlier 3-node graph example, solution H

to the SDP dual (8) is





0




(9)


 .

H =

z1
z2
0
y4

y1 + d1 −w1,2
−w1,2
0
z1

y2 + d2 −w2,3
y3 + d3
−w2,3
0
z2
The signed graph G corresponding to H—interpreted as a
generalized graph Laplacian matrix—is shown in Fig. 3(b).
We see that the ﬁrst three nodes correspond to the three nodes
in Laplacian L with added self-loops of weights yi’s. The
last node has M = 2 edges with weights −z1 and −z2 to the
ﬁrst two nodes. Because of the edges from the last node have
different signs to the ﬁrst N nodes, G is not balanced.

Reformulating the SDP Dual
We construct a balanced graph ¯G as an approximation to the
imbalanced G. This is done by splitting node N + 1 in G into
two in ¯G, dividing positive and negative edges between them,
as shown in Fig. 3. This results in N + 2 nodes for ¯G. The
speciﬁc graph construction for ¯G procedure is:

1. Construct ﬁrst N nodes with the same edges as G.
2. Construct node N + 1 with positive edges {−zi}M1

node N + 2 with negative edges {−zi}M
N nodes in G.

i=1 and
i=M1+1 to the ﬁrst

3. Add self-loops for node N + 1 and N + 2 with respective
weights ¯uN +1 = uN +1/2 − (cid:15) and ¯uN +2 = uN +1/2 + (cid:15),
where (cid:15) ∈ R is a parameter.
Denote by ¯H ∈ R(N +2)×(N +2) the generalized graph
Laplacian matrix to augmented graph ¯G. Continuing our ex-
ample, Fig. 3(c) shows graph ¯G. Corresponding ¯H is








¯H =

0

y1 + d1 −w1,2
−w1,2
0
z1
0

y2 + d2 −w2,3
y3 + d3
−w2,3
0
0
0
z2

z1
0
0
¯u4 + z1
0








.

0
z2
0
0
¯u5 + z2

where ¯u4 = u4/2 − (cid:15), ¯u5 = u4/2 + (cid:15), and u4 = y4 + z1 + z2.
Spectrally, ¯H and H are related; λmin( ¯H) ≤ λmin(H). See
(Yang et al. 2021) for a proof.

We reformulate the SDP dual (8) by keeping the same
objective but imposing PSD cone constraint on ¯H instead,
which implies a PSD H. Deﬁne A(cid:48)
i similarly
to (6) but for a larger (N + 2)-by-(N + 2) matrix; i.e.,
A(cid:48)
N +1 0], and
B(cid:48)(cid:48)
N +1(i) 0]. The reformu-
lated SDR dual is

i = diag(eN +2(i)), B(cid:48)
i = [0(N +1)×(N +1) eN +1(i); e(cid:62)

i = [Bi 0N +1; 0(cid:62)

i and B(cid:48)(cid:48)

i, B(cid:48)

min
y,z

N +1y + b(cid:62)z,
1(cid:62)

(10)

s.t. ¯H (cid:44)

N
(cid:88)

i=1

yiA(cid:48)

i + κN +1A(cid:48)

N +1 + κN +2A(cid:48)

N +2

M
(cid:88)

ziB(cid:48)

i +

ziB(cid:48)(cid:48)

i − L (cid:23) 0

+

M1(cid:88)

i=1

i=M1+1
i=1 zi − (cid:15) and κN +2 = uN +1

2 −

2 − (cid:80)M1

where κN +1 = uN +1
(cid:80)M
i=M1+1 zi + (cid:15).
Given ¯H is now a Laplacian to a balanced graph, GDPA
linearization can be applied to solve (10) efﬁciently. Speciﬁ-
cally, in each iteration t, the ﬁrst eigenvector v of previous
solution ¯Ht−1 is computed using LOBPCG to deﬁne matrix
St = diag(v−1
1 , . . .). St is then used to deﬁne linear con-
t ) ≥ 0, replacing ¯H (cid:23) 0 in (10). This
straints λ−
results in a LP, efﬁciently solvable using a state-of-the-art LP
solver such as Simplex or interior point (Vanderbei 2021).
The algorithm is run iteratively until convergence.

min(St ¯HS−1

OPTIMIZING GRAPH PARAMETERS
After unrolling the iterative algorithm described above to
solve (10) into a neural network architecture as shown in
Fig. 1, we discuss next how to optimize parameters in each
layer end-to-end via SGD for optimal performance. Speciﬁ-
cally, we consider two methods—Mahalanobis distance learn-
ing and local linear embedding—to optimize graph edge
weights, so that the most appropriate graph can be employed
for classiﬁcation in each layer.

Table 1: Trainable parameters for a P -layer network. M
denotes the number of neurons in a dense layer.

method

type

trainable parameters
count

MLP/CNN/GCN

weights/bias M [M (P − 1) + P + K + 2] + 2

SDR

Q
Q, λ, µ, αi

≤ P K(K + 1)/2
≤ P (4 + K(K + 1)/2)

Figure 4: Trainable parameters when the number of layers
P = 2 and number of neurons M = 32 in a dense layer of a
black-box network. Our SDR can have trainable parameters
close to O(K) (i.e., a line that leans towards the black line)
with a sparsity pattern constraint.

Mahalanobis Distance Learning
We assume that edge weight wi,j between nodes i and j
is inversely proportional to feature distance di,j, computed
using a Gaussian kernel, i.e.,

(cid:18)

wi,j = exp

−

(cid:19)

.

di,j
σ2
d

(11)

Using an exponential kernel for di,j ∈ [0, ∞) means wi,j ∈
(0, 1], which ensures a positive graph as required in (4).

We optimize feature distance di,j in each neural layer as
follows. Assuming each node i is endowed with a feature
vector fi ∈ RK of dimension K, di,j can be computed as
the Mahalanobis distance (Mahalanobis 1936) using a PSD
metric matrix M (cid:23) 0:

di,j = (fi − fj)(cid:62)M(fi − fj).
(12)
M can be decomposed into M = QQ(cid:62) via Cholesky fac-
torization (Golub and Van Loan 1996), where Q is a lower
triangular matrix. In each neural layer, we ﬁrst initialize an
empirical covariance matrix E using available feature vectors
{fi}. We then apply Cholesky factorization to E−1 = QQ(cid:62).
Next, we designate a sparsity pattern in Q by setting to zero
entries in Q whose amplitudes are small than factor ζ > 0
times the average of the diagonals in Q. Table 1 and Fig. 4
show the number of trainable parameters for a P -layer un-
rolled network. With a sparsity pattern set by a carefully
chosen ζ, the number of trainable parameters in Q in our
network is O(K).

Using computed edge weights in (11), one can compute a
graph Laplacian matrix L1 = diag(W1) − W, where 1 is
the all-one vector.

Local Linear Embedding
We compute a second graph Laplacian matrix L2 via local
linear embedding (LLE) (Roweis and Saul 2000; Ghojogh

et al. 2020). Speciﬁcally, we compute a sparse coefﬁcient
matrix C ∈ RN ×N , so that each feature vector fi ∈ RK
can be represented as a sparse linear combination of other
feature vectors fj, ∀j | j (cid:54)= i. We ﬁrst deﬁne matrix F (cid:44)
[f1; . . . , ; fK] ∈ RN ×K that contains feature vector fi as row
i. We then formulate the following group sparsity problem:

min
C∈S+

(cid:107)F − CF(cid:107)2

2 + η(cid:107)C(cid:107)1,1

(13)

where S + is the set of symmetric matrices with zero diag-
onal terms and non-negative off-diagonal terms, and η > 0
is a parameter that induces sparsity in C. Matrix symme-
try and non-negativity are enforced, so that bi-directional
positive edge weights can be easily deduced from C. Using
non-negative weights for LLE is called non-negative kernel
regression (NNK) in (Shekkizhar and Ortega 2020).

The objective in (13) contains two convex terms, where
only the ﬁrst term is differentiable. Thus, we optimize (13)
iteratively using proximal gradient (PG) (Parikh and Boyd
2013) given an initial matrix C that corresponds to the adja-
cency matrix of a k-nearest neighbor graph as input. Specif-
ically, at each iteration t, we ﬁrst optimize the ﬁrst term
(cid:107)F − CF(cid:107)2
2 via gradient descent with step size δ. We then
optimize the second term via soft-thresholding T (ci,j):
(cid:26) ci,j − η
0

if ci,j ≥ η
o.w.

T (ci,j) =

(14)

.

T (·) combines the proximal operator for the (cid:96)1-norm and the
projection operator onto S +.

As done in (He et al. 2019; Ghojogh et al. 2020), the
weights of the optimized C can be further adjusted using
known labels in a semi-supervised manner: the weights for
the same-label (different-label) sample pairs are increased by
parameter γ > 0 (decreased by parameter µ > 0).

After C is obtained, we interpret it as an adjacency matrix
and compute its corresponding graph Laplacian matrix L2 =
diag(C1)−C. Finally, we compute a new graph Laplacian L
as a conic combination of L1 computed via feature distance
speciﬁed by metric matrix M and L2 computed via LLE
speciﬁed by coefﬁcient matrix C, i.e.,

L = α1L1 + α2L2,

αi ≥ 0.

(15)

αi ≥ 0 ensures that the conically combined L is a Laplacian
for a positive graph. Trainable parameters for L consist of
the two LLE adjustment parameters (γ and µ) and the two
Laplacian weight parameters (α1 and α2).

Loss Function and Inference
As shown in Fig. 1, we train parameters Q, γ, µ, α1 and α2 in
each SDR layer in an end-to-end fashion via backpropagation
(Rumelhart, Hinton, and Williams 1986). During training, a
mean-squared-error (MSE) loss function is deﬁned as

L =

(cid:13)
(cid:13)
(cid:13)
(cid:13)

g

(cid:16)

(cid:104)

fP

fP −1

(cid:0) · · · f1(Q, αi, γ, µ, y, z)(cid:1)(cid:17)(cid:105)

− ˆx{M +1,...,N }

(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

,

2
(16)

where g[·] (cid:44) sign (cid:8)ˆx1v1v{M +1,...,N }
(cid:9) is the label predic-
tion equation, v is the ﬁrst eigenvector of H computed by

LOBPCG, and fP (fP −1(· · · )) are nested differentiable func-
tions corresponding to the P -layers in our unrolled network.
(16) is essentially the MSE loss of entries M + 1 to N of x
(unknown labels) compared to the ground-truth labels. We op-
timize the parameters using an off-the-shelf SGD optimizer.
During inference, test data is passed through the unrolled
network, where the optimized Q, γ, µ, α1 and α2 are ﬁxed.
Q is used to deﬁne the metric matrix M to construct L1. γ
and µ are used to construct L2 together with the LLE weight
matrix C learned from the test data via (13). α1 and α2 are
used to deﬁne L. Finally, unknown labels are predicted.

EXPERIMENTS

Experimental Setup
We implemented our unrolled network in PyTorch4, and eval-
uated it in terms of average classiﬁcation error rate and in-
ference runtime. We compared our algorithm against the
following six model-based schemes: i) a primal-dual interior-
point solver that solves the SDP primal in Eq. (5), MOSEK,
available in CVX with a Professional license (CVX 2020);
ii) a biconvex relaxation solver BCR (Shah et al. 2016;
BCR 2020); iii) a spectrahedron-based relaxation solver SD-
Cut (Wang, Shen, and van den Hengel 2013; Wang et al. 2017;
SDcut 2013) that involves L-BFGS-B (Zhu et al. 1997); iv)
an ADMM ﬁrst-order operator-splitting solver CDCS (Zheng,
Fantuzzi, and Papachristodoulou 2019; Zheng et al. 2020)
with an LGPL-3.0 License (CDCS 2016) that solves the
modiﬁed SDP dual in Eq. (10); v) a graph Laplacian regu-
larizer GLR (Pang and Cheung 2017) with a box constraint
xi ∈ [−1, 1] for predicted labels; and vi) baseline model-
based version of our SDR network proposed in (Yang et al.
2021) based on GDPA (Yang, Cheung, and Hu 2021).

In addition, we compared our network against four neural
network schemes: vii) an unrolled 1-layer SDP classiﬁer net-
work that solves (10) using a differentiable SDP solver in a
Cvxpylayer library (Agrawal et al. 2019; Agrawal and Boyd
2020); viii) a multi-layer perceptron (MLP) consisted of two
dense layers; ix) a convolutional neural network (CNN) con-
sisted of two 1-D convolutional layers (each with a kernel size
1 and stride 2); and x) a graph convolutional network (GCN)
(Kipf and Welling 2017) consisted of two graph convolu-
tional layers. For GCN, the adjacency matrix is computed in
the same way as the one used to compute the graph Laplacian
in (1) and is ﬁxed throughout the network training procedure.
For MLP, CNN and GCN, each (graph) convolutional layer
is consisted of 32 neurons and is followed by group normal-
ization (Wu and He 2018), rectiﬁed linear units (Fukushima
1969) and dropout (Srivastava et al. 2014) with a rate 0.2. A
1-D max-pooling operation with a kernel size 1 and stride 2
is placed before the dropout for CNN. A cross entropy loss
based on the log-softmax (de Brébisson and Vincent 2016) is
adopted for MLP, CNN and GCN.

We set the sparsity factor as ζ = 0.9 to initial lower trian-
gular Q, the sparsity weight parameter as η = 0.01 in (14),
the Laplacian weight parameter in (15) as α1 = α2 = 1,
and the LLE weight adjustment parameters as γ = 1 and

4results reproducible via code in https://anonymous.

4open.science/r/SDP_RUN-4D07/.

Figure 5: Inference time of SDR v.s. naive SDP unrolling.
Our speedup is over 1400× when the sample size is 200.

µ = 1. We set the convergence threshold of i) LOBPCG
to 10−4 with 200 maximum iterations, ii) the differentiable
LP solver to 10−6 with 1000 maximum iterations. We set
the learning rate for the SGD optmizer used in all methods
to 10−2. The maximum iterations for the optimization of Q
and C was set to 1000 for the pure model-based methods
i, iii, iv, v and vi that involve graph construction. For fast
convergence, we set the convergence thresholds of CDCS and
SDCut to 10−3, the maximum ADMM iterations in CDCS
to 1000, the maximum iterations for L-BFGS-B in SDCut
and the main loop in BCR to 100, and the Frobenius norm
weight in SDCut to 100. The number of epochs for the three
data-driven networks, viii, ix and x, was set to 1000. For the
SDP unrolled network vii and our unrolled network it was 20.
All computations were carried out on a Ubuntu 20.04.2 LTS
PC with AMD RyzenThreadripper 3960X 24-core processor
3.80 GHz and 128GB of RAM.

We employed 17 binary datasets freely available from UCI
(UCI 2021) and LibSVM (LibSVM 2021). For efﬁciency, we
ﬁrst performed a K-fold (K ≤ 9) split for each dataset with
random seed 0, and then created 5 instances of 80% training-
20% test split for each fold, with random seeds 1-5 (Russell
and Norvig 2009). For the six model-based approach and
the three data-driven networks, the ground-truth labels for
the above 80% training data were used for semi-supervised
graph classiﬁer learning (Yang et al. 2021) and supervised
network training. For our SDR unrolled network, we further
created a 75% unroll-training-25% unroll-test split for the
80% training data, where, ﬁrst, the ground-truth labels for
the unroll-training data were used for the semi-supervised
SDR network training together with the unroll-test data, and
second, the learned parameters were used for label inference
of the remaining 20% test data. The above setup resulted in
sample sizes from 62 to 292. We applied a standardization
data normalization scheme in (Dong et al. 2020) that ﬁrst
subtracts the mean and divides by the feature-wise standard
deviation, and then normalizes to unit length sample-wise.
We added 10−12 noise to the dataset to avoid NaN’s due to
data normalization on small samples.

Experimental Results
We ﬁrst show in Fig. 5 the inference runtime of our SDR
network compared to a SDP unrolled network that naïvely
unrolls the PSD cone projection using the same Cvxpylayer
python library described earlier. It is clear that our SDR
network is substantially faster in inference than the naïve
SDP unrolled network, with a speedup that is over 1400×
when the sample size is 200.

We next show in Table 2 the classiﬁcation error rates of

Table 2: Classiﬁcation error rates (%). K denotes feature count.

dataset

K

model-based

MOSEK

BCR

SDcut CDCS

GLR

GDPA MLP

CNN

australian
breast-cancer
diabetes
fourclass
german
haberman
heart
ILPD
liver-disorders
monk1
pima
planning
voting
WDBC
sonar
madelon
colon-cancer
avg.

14
10
8
2
24
3
13
10
5
6
8
12
16
30
60
500
2000
−

20.14
3.85
35.16
28.30
26.90
23.61
20.37
28.10
30.00
29.82
35.16
25.00
11.40
7.54
31.90
49.75
38.33
26.20

15.65
3.41
32.94
23.98
26.90
23.61
18.89
28.10
27.86
26.25
32.68
25.00
10.70
7.72
23.33
44.44
36.67
24.01

15.65
3.56
31.76
23.51
26.90
23.61
18.89
28.10
30.71
26.07
31.90
25.00
10.70
7.54
21.90
48.94
38.33
24.30

15.65
3.41
31.63
23.51
27.00
23.61
18.89
28.10
30.00
27.86
32.03
25.00
12.09
8.07
21.90
48.84
38.33
24.47

16.67
4.30
33.59
25.38
26.90
23.61
18.52
28.10
29.29
26.43
33.59
25.00
11.40
7.37
23.33
48.79
38.33
24.74

15.51
3.56
35.03
25.03
26.90
23.61
18.89
31.21
30.71
26.07
36.47
25.00
10.70
7.54
21.90
48.59
38.33
25.00

17.39
5.19
32.31
26.08
31.60
27.10
24.81
26.78
37.86
6.43
33.08
39.44
3.95
4.64
17.62
46.82
28.33
24.08

17.83
4.89
36.15
25.15
28.80
29.68
24.07
27.97
39.29
5.71
32.69
40.56
2.79
4.46
17.14
47.78
26.67
24.21

GCN

neural nets
SDR
T
18.70
3.48
30.98
29.77
25.60
23.55
17.41
29.31
41.33
32.73
31.37
25.41
10.93
9.47
14.63
41.92
32.31
24.64

19.57
12.59
33.08
25.15
24.40
28.71
23.70
30.00
44.29
12.86
35.00
33.89
10.47
22.86
40.95
46.11
38.33
28.35

SDR
Q
15.65
3.48
30.00
27.93
24.40
22.58
18.89
28.62
36.00
26.18
28.08
24.86
3.72
7.14
20.00
43.59
28.33
22.91

SDR
Q+LLE
16.95
5.33
29.62
27.12
23.20
22.90
21.11
25.34
34.67
27.64
29.62
23.78
4.19
6.79
19.05
40.76
23.08
22.42

the six model-based schemes, namely MOSEK, BCR, SD-
cut, CDCS, GLR and GDPA, the three data-driven networks,
namely MLP, CNN and GCN, and the three variants of our
SDR network where a single SDR layer optimizes i) a K × J
matrix T for M = TT(cid:62) where J = 2 is the pre-deﬁned
rank, ii) our proposed lower-triangular matrix Q sparsiﬁed
by ζ, and iii) Q plus LLE weight adjustment parameters γ
and µ and Laplacian weighting parameters α1 and α2.

We ﬁrst observe that, in general, with an appropriate choice
of the trainable parameters, SDR Q and SDR Q+LLE outper-
formed on average all model-based schemes and were com-
petitive with data-driven schemes. SDR Q+LLE on average
performed better than SDR Q thanks to the four additional
trainable parameters γ, µ, α1 and α2.

We observe also that the three data-driven schemes, MLP,
CNN and GCN, performed on average slightly worse than
SDR Q and SDR Q+LLE. This can be explained by the rela-
tively large number of trainable parameters that may cause
overﬁtting. For example, MLP is consisted of 1602 trainable
parameters with 32 neurons in each of the two dense layers
during training on the dataset australian, while 1-layer
SDR Q and SDR Q+LLE have at most 105 and 109 trainable
parameters, respectively. We see also that SDR T, SDR Q
and SDR Q+LLE learned faster than the three data-driven
schemes with only 20 epochs in training stage compared to
1000 epochs for MLP, CNN and GCN. We note further that
our unrolled network is by design more interpretable than the
three generic black box data-driven implementations, where
each neural layer is an iteration of an iterative algorithm.

We observe that SDR Q and SDR Q+LLE outperformed
SDR T, demonstrating that our proposed parameterization of
graph edge weights at each neural layer is better than simple
low-rank factorization M = TT(cid:62). For SDR T, SDR Q
and SDR Q+LLE, the noticeably worse performance on the

Table 3: Classiﬁcation error rates (%) on the dataset sonar
using our SDR Q+LLE with P layers.
2
16.59

P
error rate (%)

3
16.10

1
19.05

dataset liver-disorders compared to the model-based
schemes may be explained by the fact that the optimizer was
stuck at a bad local minimum.

We show in Table 3 the classiﬁcation error rate of our SDR
Q+LLE with 1, 2 and 3 SDR layers on the dataset sonar.
We see that as the number of layers P increases, the clas-
siﬁcation error rates are reduced at the cost of introducing
more network parameters. This indicates that our SDR net-
work is resilient to overﬁtting when the number of trainable
parameters increases by a factor of P .

CONCLUSION
To facilitate algorithm unfolding of a proximal splitting algo-
rithm that requires PSD cone projection, using binary graph
classiﬁer as an illustrative example, we propose an unrolling
strategy via GDPA linearization. Speciﬁcally, we replace the
PSD cone constraint in the semi-deﬁnite programming re-
laxation (SDR) of the classiﬁer problem by “tight possible”
linear constraints per iteration, so that each iteration requires
only computing a linear program (LP) and the ﬁrst eigenvec-
tor of the previous matrix solution. After unrolling iterations
of the projection-free algorithm into neural layers, we opti-
mize parameters that determine graph edge weights in each
layer via stochastic gradient descent (SGD). Experiments
show that our unrolled network outperformed pure model-
based classiﬁers, and had comparable performance as pure
data-driven schemes while employing far fewer parameters.

2020.

BCR implementa-
https://github.com/Axeldnahcram/

References
[Agrawal and Boyd 2020] Agrawal, A., and Boyd, S. 2020.
Differentiating through log-log convex programs. arXiv.
[Agrawal et al. 2019] Agrawal, A.; Amos, B.; Barratt, S.;
Boyd, S.; Diamond, S.; and Kolter, Z. 2019. Differentiable
convex optimization layers. In Advances in Neural Informa-
tion Processing Systems.
[BCR 2020] BCR.
tion.
biconvex_relaxation. Accessed: 2021-9-6.
[Beck and Teboulle 2009] Beck, A., and Teboulle, M. 2009.
A fast iterative shrinkage-thresholding algorithm for linear
inverse problems. SIAM J. Imaging Sci. 2:183–202.
[Belkin, Matveeva, and Niyogi 2004] Belkin, M.; Matveeva,
I.; and Niyogi, P. 2004. Regularization and semisupervised
learning on large graphs. In Shawe-Taylor J., Singer Y. (eds)
Learning Theory, COLT 2004, Lecture Notes in Computer
Science, volume 3120, 624–638.
1998. Online algorithms and
[Bottou 1998] Bottou, L.
stochastic approximations. In Saad, D., ed., Online Learning
and Neural Networks. Cambridge, UK: Cambridge Univer-
sity Press. revised, oct 2012.
[Boyd et al. 2011] Boyd, S.; Parikh, N.; Chu, E.; Peleato, B.;
and Eckstein, J. 2011. Distributed optimization and statistical
learning via the alternating direction method of multipliers.
In Foundations and Trends in Optimization, volume 3, no.1,
1–122.
[Cartwright and Harary 1956] Cartwright, D., and Harary, F.
1956. Structural balance: a generalization of Heider’s theory.
In Psychological Review, volume 63, no.5, 277–293.
[CDCS 2016] CDCS. 2016. CDCS implementation. https:
//github.com/oxfordcontrol/CDCS. Accessed:
2021-9-6.
[CVX 2020] CVX. 2020. CVX Research. http://cvxr.
com/cvx/. Accessed: 2021-9-6.
[de Brébisson and Vincent 2016] de Brébisson, A., and Vin-
cent, P. 2016. An exploration of softmax alternatives belong-
ing to the spherical loss family. In International Conference
on Learning Representations.
[Dong et al. 2020] Dong, M.; Wang, Y.; Yang, X.; and Xue,
J. 2020. Learning local metrics and inﬂuential regions for
classiﬁcation. IEEE TPAMI 42(6):1522–1529.
[Fukushima 1969] Fukushima, K. 1969. Visual feature ex-
traction by a multilayered network of analog threshold ele-
ments. IEEE Transactions on Systems Science and Cybernet-
ics 5(4):322–333.
[Gartner and Matousek 2012] Gartner, B., and Matousek, J.
2012. Approximation Algorithms and Semideﬁnite Program-
ming. Springer.
[Ghojogh et al. 2020] Ghojogh, B.; Ghodsi, A.; Karray, F.;
and Crowley, M. 2020. Locally linear embedding and its
variants: Tutorial and survey.
[Golub and Van Loan 1996] Golub, G. H., and Van Loan,
C. F. 1996. Matrix Computations. The Johns Hopkins
University Press, third edition.

[Gregor and LeCun 2010] Gregor, K., and LeCun, Y. 2010.
Learning fast approximations of sparse coding. In Interna-
tional Conference on Machine Learning, ICML’10, 399–406.
[Guillory and Bilmes 2009] Guillory, A., and Bilmes, J. 2009.
Label selection on graphs. In Twenty-Third Annual Confer-
ence on Neural Information Processing Systems.
[He et al. 2019] He, P.; Jing, T.; Xu, X.; Zhang, L.; Liao, Z.;
and Fan, B. 2019. Nonlinear manifold classiﬁcation based on
lle. In Bhatia, S. K.; Tiwari, S.; Mishra, K. K.; and Trivedi,
M. C., eds., Advances in Computer Communication and Com-
putational Sciences, 227–234. Singapore: Springer Singa-
pore.
[Helmberg et al. 1996] Helmberg, C.; Rendl, F.; Vanderbei,
R.; and Wolkowicz, H. 1996. An interior-point method for
semideﬁnite programming. In SAIM J. Optim., volume 6,
no.2, 342–361.
[Jaggi 2013] Jaggi, M.
2013. Revisiting Frank-Wolfe:
Projection-free sparse convex optimization. In International
Conference on Machine Learning, 427–435.
[Kipf and Welling 2017] Kipf, T. N., and Welling, M. 2017.
Semi-Supervised Classiﬁcation with Graph Convolutional
Networks. In International Conference on Learning Repre-
sentations.
[Knyazev 2001] Knyazev, A. V. 2001. Toward the optimal
preconditioned eigensolver: Locally optimal block precondi-
tioned conjugate gradient method. SIAM Journal on Scientiﬁc
Computing 23(2):517–541.
[Krizhevsky, Sutskever, and Hinton 2012] Krizhevsky, A.;
Sutskever, I.; and Hinton, G. E. 2012. Imagenet classiﬁcation
with deep convolutional neural networks. In Advances in
Neural Information Processing Systems, volume 25.
[LeCun, Bengio, and Hinton 2015] LeCun, Y.; Bengio, Y.;
and Hinton, G. 2015. Deep learning. Nature 521(7553):436–
444.
[Li, Liu, and Tang 2008] Li, Z.; Liu, J.; and Tang, X. 2008.
Pairwise constraint propagation by semideﬁnite program-
ming for semi-supervised classiﬁcation. In ACM Interna-
tional Conferene on Machine Learning.
[LibSVM 2021] LibSVM. 2021. LibSVM Data: Classiﬁ-
cation (Binary Class). https://www.csie.ntu.edu.
tw/~cjlin/libsvmtools/datasets/binary.
html. Accessed: 2021-9-6.
[Luo et al. 2010] Luo, Z.; Ma, W.; So, A. M.; Ye, Y.; and
Zhang, S. 2010. Semideﬁnite relaxation of quadratic op-
IEEE Signal Processing Magazine
timization problems.
27(3):20–34.
[Mahalanobis 1936] Mahalanobis, P. C. 1936. On the gen-
eralized distance in statistics. Proceedings of the National
Institute of Sciences of India 2(1):49–55.
[Monga, Li, and Eldar 2021] Monga, V.; Li, Y.; and Eldar,
Y. C. 2021. Algorithm unrolling: Interpretable, efﬁcient
deep learning for signal and image processing. IEEE Signal
Processing Magazine 38(2):18–44.
[Moutaﬁs, Leng, and Kakadiaris 2017] Moutaﬁs, P.; Leng,
M.; and Kakadiaris, I. A. 2017. An overview and empiri-

semideﬁnite relaxation and applications. IEEE Transactions
on Pattern Analysis and Machine Intelligence 39(3):470–485.
[Wang, Shen, and van den Hengel 2013] Wang, P.; Shen, C.;
and van den Hengel, A. 2013. A fast semideﬁnite approach
to solving binary quadratic problems. In IEEE International
Conference on Computer Vision and Pattern Recognition.
[Wu and He 2018] Wu, Y., and He, K. 2018. Group normal-
ization. In ECCV.
[Yang et al. 2021] Yang, C.; Cheung, G.; tian Tan, W.; and
Zhai, G. 2021. Projection-free graph-based classiﬁer learning
using Gershgorin disc perfect alignment. arXiv.
[Yang, Cheung, and Hu 2021] Yang, C.; Cheung, G.; and Hu,
W. 2021. Signed graph metric learning via Gershgorin disc
perfect alignment. arXiv.
[Zhang et al. 2017] Zhang, K.; Zuo, W.; Gu, S.; and Zhang, L.
2017. Learning deep cnn denoiser prior for image restoration.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR).
[Zheng et al. 2020] Zheng,
Y.;
Pa-
2020.
pachristodoulou, A.; Goulart, P.; and Wynn, A.
Chordal decomposition in operator-splitting methods for
sparse semideﬁnite programs. Mathematical Programming
180:489—-532.
[Zheng, Fantuzzi, and Papachristodoulou 2019] Zheng, Y.;
Fantuzzi, G.; and Papachristodoulou, A. 2019. Fast ADMM
for sum-of-squares programs using partial orthogonality.
IEEE Transactions on Automatic Control 64(9):3869–3876.
[Zhou et al. 2003] Zhou, D.; Bousquet, O.; Lal, T. N.; Weston,
J.; and Scholkopf, B. 2003. Learning with local and global
In 16th International Conference on Neural
consistency.
Information Processing (NIPS).
[Zhu et al. 1997] Zhu, C.; Byrd, R.; Lu, P.; and Nocedal, J.
1997. Algorithm 778: L-BFGS-B: Fortran subroutines for
large-scale bound-constrained optimization. ACM Trans.
Math. Softw. 23(4):550–560.

Fantuzzi,

G.;

cal comparison of distance metric learning methods. IEEE
Transactions on Cybernetics 47(3):612–625.
[O’Donoghue et al. 2016] O’Donoghue, B.; Chu, E.; Parikh,
N.; and Boyd, S. 2016. Conic optimization via operator
splitting and homogeneous self-dual embedding. In Journal
of Optimization Theory and Applications, volume 169, no.3,
1042–1068.
[Ortega et al. 2018] Ortega, A.; Frossard, P.; Kovacevic, J.;
Moura, J. M. F.; and Vandergheynst, P. 2018. Graph sig-
nal processing: Overview, challenges, and applications. In
Proceedings of the IEEE, volume 106, no.5, 808–828.
[Pang and Cheung 2017] Pang, J., and Cheung, G. 2017.
Graph Laplacian regularization for inverse imaging: Analysis
in the continuous domain. In IEEE Transactions on Image
Processing, volume 26, no.4, 1770–1785.
[Parikh and Boyd 2013] Parikh, N., and Boyd, S. 2013. Prox-
imal algorithms. In Foundations and Trends in Optimization,
volume 1, no.3, 123–231.
[Roweis and Saul 2000] Roweis, S., and Saul, L. 2000. Non-
linear dimensionality reduction by locally linear embedding.
Science 290 5500:2323–6.
[Rumelhart, Hinton, and Williams 1986] Rumelhart, D. E.;
Hinton, G. E.; and Williams, R. J. 1986. Learning Represen-
tations by Back-propagating Errors. Nature 323(6088):533–
536.
[Russell and Norvig 2009] Russell, S., and Norvig, P. 2009.
Artiﬁcial Intelligence: A Modern Approach. USA: Prentice
Hall Press, 3rd edition.
[SDcut 2013] SDcut. 2013. SDcut implementation. https:
//github.com/chhshen/SDCut. Accessed: 2021-9-
6.
[Shah et al. 2016] Shah, S.; Kumar, A.; Castillo, C.; Jacobs,
D.; Studer, C.; and Goldstein, T. 2016. Biconvex relax-
ation for semideﬁnite programming in computer vision. In
European Conference on Computer Vision.
[Shekkizhar and Ortega 2020] Shekkizhar, S., and Ortega, A.
2020. Graph construction from data using non negative kernel
regression (NNK graphs). In IEEE International Conference
on Acoustics, Speech and Signal Processing.
[Shlezinger et al. 2021] Shlezinger, N.; Whang, J.; Eldar,
Y. C.; and Dimakis, A. G. 2021. Model-based deep learning.
[Srivastava et al. 2014] Srivastava,
G.;
Krizhevsky, A.; Sutskever,
I.; and Salakhutdinov, R.
2014. Dropout: A simple way to prevent neural networks
from overﬁtting. J. Mach. Learn. Res. 15(1):1929–1958.
[UCI 2021] UCI.
UCI machine learning
repository. https://archive.ics.uci.edu/ml/
datasets.php. Accessed: 2021-9-6.
[Vanderbei 2021] Vanderbei, R. 2021. Linear Programming:
Foundations and Extensions (5th Edition). Springer Nature.
[Varga 2004] Varga, R. S. 2004. Gershgorin and his circles.
Springer.
[Wang et al. 2017] Wang, P.; Shen, C.; Hengel, A.; and Torr,
P. 2017. Large-scale binary quadratic optimization using

Hinton,

2021.

N.;

