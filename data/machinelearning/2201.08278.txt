2
2
0
2

n
a
J

0
2

]
I

A
.
s
c
[

1
v
8
7
2
8
0
.
1
0
2
2
:
v
i
X
r
a

Lifelong Learning Metrics

Alexander New∗, Megan Baker, Eric Nguyen, Gautam Vallabha
Johns Hopkins University Applied Physics Laboratory
Laurel, MD 20723
alex.new@jhuapl.edu, megan.baker@jhuapl.edu,
eric.nguyen@jhuapl.edu, gautam.vallabha@jhuapl.edu
∗corresponding author

January 21, 2022

Contents

1 Introduction

1.1 How this document was developed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Concepts for Measuring Lifelong Learning

2.1 Conditions of Lifelong Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Components of lifelong learning scenarios
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Performance Measures and Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Learning and Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5 Application-Speciﬁc Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5.1 Example of a single-task lifelong learning scenario . . . . . . . . . . . . . . . . . . . .
2.5.2 Example of a multi-task lifelong learning scenario . . . . . . . . . . . . . . . . . . . . .
2.5.3 Preprocessing application-speciﬁc metrics . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.6 Generating Lifelong Learning Metrics

3 Metrics for Lifelong Learning

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1 Proposed Lifelong Learning Metrics
3.2 Single-Task Metrics, Cross-Task Metrics, and Aggregation Across a Lifetime . . . . . . . . . .
3.3 Contrasts vs. Ratios for Calculating Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4 Proposed Lifelong Learning Metrics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4.1 Performance Maintenance (PM)
3.4.2 Forward Transfer (FT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4.3 Backward Transfer (BT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4.4 Performance Relative to a Single-Task Expert (RP)
. . . . . . . . . . . . . . . . . . .
Sample Eﬃciency (SE) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4.5

A Preprocessing Application-Speciﬁc Metrics

B Supplemental Metrics

B.1 Performance Recovery (PR) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.2 Cumulative Gain of a Lifelong Learner (CG)
. . . . . . . . . . . . . . . . . . . . . . . . . . .
B.3 Learn Burn (LB) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

C Glossary of Terms

2
2

2
3
4
4
5
6
6
9
9
11

11
11
11
12
12
12
15
15
16
17

20

20
20
21
22

22

1

 
 
 
 
 
 
1

Introduction

The DARPA Lifelong Learning Machines (L2M) program seeks to yield advances in artiﬁcial intelligence
(AI) systems so that they are capable of learning (and improving) continuously, leveraging data on one
task to improve performance on another, and doing so in a computationally sustainable way. Performers on
this program developed systems capable of performing a diverse range of functions, including autonomous
driving, real-time strategy, and drone simulation. These systems featured a diverse range of characteristics
(e.g., task structure, lifetime duration), and an immediate challenge faced by the program’s testing and
evaluation team was measuring system performance across these diﬀerent settings. This document, developed
in close collaboration with DARPA and the program performers, outlines a formalism for constructing and
characterizing the performance of agents performing lifelong learning scenarios.

In Section 2, we introduce the general form of a lifelong learning scenario. This requires specifying
diﬀerent types of experiences agents may be exposed to, and what metrics should be generated from these
experiences. In Section 2.1, we brieﬂy outline the criteria for an agent to demonstrate lifelong learning. In
Section 3, we deﬁne a set of metrics that characterize to what extent an agent demonstrates lifelong learning
in a given scenario. Discussions in sources such as [4, 7, 24] are also useful for grounding the ideas behind
lifelong learning.

Our framework and metrics are meant to be as agnostic as possible to the agent conﬁguration (e.g.,
progressive network [21] or elastic weight consolidation [11]), domain (e.g., autonomous navigation, robotics,
strategy, classiﬁcation), and environment (e.g., StarCraft [25], AirSim [23], CARLA [2], Habitat [14], Ar-
cade [1], SplitMNIST [7], or Core50 [12]). It may also be used in combination with platforms for lifelong
learning, such as Avalanche [13] or CORA [19].

Agents, domains, environments, and other terms are deﬁned in more detail in Appendix C. We rec-
ommend [18] as an overview of recent approaches and advances in the general ﬁeld of lifelong learning.
Historically, there has been wide variation in how systems and metrics for lifelong learning are deﬁned and
assessed; diﬀerent papers may focus on diﬀerent metrics. Beyond those papers cited here, many others
motivate their system designs with arguments to concepts like positive transfer. This document provides a
consistently-deﬁned suite of metrics applicable for general lifelong learning problems. In particular, although
much early work in the ﬁeld of lifelong learning focused on the problem of mitigating catastrophic forget-
ting [5], [15] – an agent losing the previously-acquired ability to perform tasks as it encounters new tasks
– our metrics here endeavor to capture both catastrophic forgetting and other characterizations of lifelong
learning, such as transfer and comparison to agents exposed to only a single task.

A Python library, l2metrics, containing implementations of these metrics is in development and will be

publicly available shortly. This document will be updated when it is.

1.1 How this document was developed

Over the course of the DARPA L2M program, performers, the testing and evaluation team, and DARPA
Systems Engineering and Technical Advisors (SETAs) formed several Working Groups that met regularly to
discuss the concepts behind and metrics for characterizing Lifelong Learning. This document captures the
consensuses arrived at after these discussions, and its content could only have been developed in the close
collaboration this process entailed. In particular, the Deﬁnitions and Scenarios Working Group developed
concepts like criteria of lifelong learning (Section 2.1), and tasks and environments (Section 2), the Metrics
Working Group formulated the metrics (Section 3), and, during and after program evaluations, the performers
gave feedback on the deﬁnitions and metrics based on their experiences using them in their systems.

2 Concepts for Measuring Lifelong Learning

In this document, we deﬁne a Lifelong Learning Agent (L2 Agent) as an agent that continues learning
once it is deployed. Here, agent refers to a learner that is interacting with an environment, where each
action (output) of the agent shapes the agent’s future observations. The agent is able to adapt eﬃciently
and ﬂexibly to new tasks and new variations of known tasks; becomes a better learner over time (learn new
tasks better and faster, perform better on old tasks); and is able to do these in a scalable and sustained

DISTRIBUTION STATEMENT A - APPROVED FOR PUBLIC RELEASE, DISTRIBUTION
UNLIMITED

2

Figure 1: Notional overview of Lifelong Learning performance for an agent, showing performance on two tasks
– tennis and badminton. Each time the L2M agent switches to a new task, its performance (y-axis) drops,
but then it recovers. Its ability to leverage transfer between the tasks enables it to surpass the performance
of an agent exposed only to one skill (dashed line). Here, Rxi denotes the length (in “experiences”) of the
ith occurrence of tennis in the agent’s lifetime (its ith Learning Block), and STE refers to a Single Task
Expert exposed only to badminton.

manner. Feedback to the agent in response to its actions is given via occasional scalar “rewards”; however,
the concepts discussed in this document are readily applicable to classiﬁcation learners as well.

Fig. 1 notionally describes the L2 concept, where an agent receives some initial training and then learns
a sequence of tasks, gradually improving its performance over time. In principle, a lifelong learner should
be able to learn a pre-speciﬁed task sequence (ﬁxed curriculum), be able to select the tasks to learn and
their order (based on criteria like novelty), handle overlapping or concurrent tasks, and so on. For current
purposes, we focus on assessing the ability of a L2 agent to learn a pre-speciﬁed task sequence, with a mixture
of old and new tasks. Although it is beyond the scope of this document, many of the concepts and measures
introduced here may be extended to situations where multiple tasks are running simultaneously.

2.1 Conditions of Lifelong Learning

The following conditions were identiﬁed as being required for an agent to demonstrate lifelong learning. Each
of the metrics in Section 3 was developed to characterize an agent’s ability to meet one of these criteria.

1. Continual Learning: The L2M learns a nonstationary stream of tasks (both novel and recurring)
without distinct training and testing phases, and continually consolidates new information to improve
performance while coping with irrelevance and noise.

2. Transfer and Adaptation: As learning progresses, the L2M performs better on average on the next
task it experiences, for both novel and known tasks (forward and reverse transfer), and maintaining
performance during rapid changes in the ongoing task (adaptation).

3. Sustainability: The L2M continues learning for an arbitrarily long lifetime using limited resources

(e.g., memory, time) in a scalable way.

DISTRIBUTION STATEMENT A - APPROVED FOR PUBLIC RELEASE, DISTRIBUTION
UNLIMITED

3

2.2 Components of lifelong learning scenarios

An L2 agent learns in a speciﬁc environment, which deﬁnes the set of Tasks and Actions it may be exposed
to. By Task, we mean some non-trivial capability the agent must learn and may be quantitatively evaluated
on. In Fig. 1, tennis and badminton are both Tasks. Tasks may also be subdivided into Task Variants;
two Variants of the same Task retain similarity but are diﬀerent enough to still pose diﬃculty in obtaining
mastery of both. For example, “Tennis on a clay court at night” and “Tennis on a grass court during the
day” might be two variants of the Tennis Task. For the purposes of this document, the delineation between
tasks, and between variants of tasks is meant to be heuristic.
The Lifetime of an agent consists of the following stages:

1. Pre-deployment stage: This is akin to genetic (or factory-installed) knowledge; the agent is endowed
with pre-built knowledge for the given environment. The actual mechanism for the “endowment” may
include meta-learning, genetic programming, pre-trained models based on statistically representative
data, or other techniques for selecting a suitable “starting point” for the agent.

2. Deployment stage: The agent is released into the environment and starts learning (see vertical dotted
line in Fig. 1). The deployment stage consists of a sequence of Learning Experiences (LXs; more on
this below) optionally interspersed with Evaluation Experiences (EXs). Each Learning or Evaluation
Experience is drawn from an environment-speciﬁc Task. A Lifelong Learning Scenario (LLS), is a
template for a particular sequence of learning experiences, intended to exercise one or more conditions
of lifelong learning.

Lifetimes are broken into smaller basic units we call Experiences. They come in two types: A Learning
Experience (LX) is a minimum amount of experience with a Task that enables some learning activity on
part of the agent; it is environment and task-speciﬁc. For example, in Arcade, a playthrough of a Breakout
game would be a single Learning Experience. An Evaluation Experience (EX) is similar to a Learning
it is an experience within a task used to evaluate an agent’s capabilities. Colloquially, the
Experience:
agent’s ability to learn is “turned oﬀ” during an EX.

Each Learning Experience will generate one or more environment and task-speciﬁc performance measures
(e.g., in single playout of Arcade Breakout, these might be the total number of bricks destroyed and the
total amount of time the ball is kept in motion). Over a lifetime, an agent may encounter tens of thousands
to millions of LXs.

It is useful to estimate how much beneﬁt a Lifelong Learning Agent has gained from being exposed to
diverse Tasks during a Lifetime. To this end, we also consider Single-Task Experts (STEs), which are
L2 agents only ever exposed to a single Task after the pre-deployment training.

We assume that, in almost all cases of interest, an agent’s learning will be stochastic, from some combina-
tion of its algorithmic core and its environmental exposures. Thus, a single lifetime will not not suﬃcient to
characterize an agent’s lifelong learning capability. Reliably assessing the variability in the responses of the
agent, assuming the inherent variability of its inputs (which could be expressed as statistical distributions,)
requires assessing performance of the agent over multiple lifetimes.

Below is an illustrative approach:

1. Exposure a blank state agent to a ﬁxed Pre-Deployment process.

2. Copy the agent’s state after this Pre-Deployment, and independently exercise each copy on the same

Lifelong Learning Scenario.

3. Aggregate the metrics across the clones in a statistically meaningful manner.

2.3 Performance Measures and Metrics

An agent’s Learning and Evaluation Experiences are expected to generate a variety of domain, environment,
and task-speciﬁc performance measures, only some of which may be relevant for lifelong learning. Data
for performance measures are collected in the context of a learning scenario, a high-level description of what
the agent will learn and the tasks it will experience. A Lifelong Learning Scenario is the instantiation of a
learning scenario in a speciﬁc environment.

DISTRIBUTION STATEMENT A - APPROVED FOR PUBLIC RELEASE, DISTRIBUTION
UNLIMITED

4

Figure 2: Environments such as AirSim and L2StarCraft deﬁne performance measures. Some subset of these
are treated as Application-Speciﬁc Metrics (Section 2.3), which feed into the calculation of Lifelong Learning
Metrics (Section 3).

The Lifelong Learning metrics are scenario, domain, environment and task-agnostic numbers that char-
acterize one or more lifelong learning capabilities across the lifetime of the agent. Each Learning Expe-
rience (LX) is also assumed to generate one or more scenario, domain, environment and application-speciﬁc
performance metrics.

Fig. 2 summarizes the relationship between performance measures, application-speciﬁc metrics, and

lifelong-learning metrics. The following sections illustrate this analysis pipeline.

2.4 Learning and Evaluation

Fig. 3 illustrates some basic terms in setting up a Lifelong Learning Scenario and in the resulting metrics
calculation. It assumes that the agent is deployed and has to learn two tasks, and that we are interested in
how performance on the ﬁrst task (Task-1) is aﬀected by intervening experience with the second task (Task-
2). The expected appearance and structure of these performance curves motivate the later development of
our lifelong learning metrics.

One set of terms in Fig. 3 relates to how the learning scenario is sequenced. As noted earlier, the deploy-
ment stage consists of a sequence of Learning Experiences (LXs) optionally interspersed with Evaluation
Experiences (EXs). Each LX is for a Task, instantiated with some speciﬁc parameters.

Our assumption is that a single Learning Experience’s granularity is too ﬁne to enable reliable estimates
of trends in agent learning capability. Thus, we adopt several less-granular categories. A set of contiguous
LXs with the same Task and parameters is a Learning Regime. A sequence of Learning Experiences
(potentially containing multiple Regimes) is a Learning Block, and a sequence of Evaluation Experiences
is an Evaluation Block. Thus, a Lifelong Learning Scenario speciﬁes a sequence of blocks.

There are three important points to keep in mind:

• Performance measures need to be collected from Learning Blocks as well as Evaluation Blocks. This
allows us to characterize how performance changed (i.e., recovered, dropped) within a Learning Block
relative to prior Learning Blocks.

• Lifelong Learning Scenarios do not need to have Evaluation Blocks. For example, say we want to only
assess the agent’s ability to learn continuously and adapt to changes in the environment. This might be
done with a scenario that consists of a sequence of Learning Blocks (each Block boundary corresponds
to a change in the environment), and we assess how well the agent recovers after each change.

DISTRIBUTION STATEMENT A - APPROVED FOR PUBLIC RELEASE, DISTRIBUTION
UNLIMITED

5

Figure 3: Illustration of terms related to metrics calculation in a single agent lifetime. A learning block
may have thousands to millions of LXs. Note that Droop is relative to the most recent Terminal Learning
Performance of the same task.

• The notion of Learning Regimes and Blocks, Evaluation Blocks, and Tasks is agnostic to speciﬁc
details of the agent and its environment. For example, the notion of a Learning Regime enables a
task’s deﬁnition to shift over the course of a lifetime, and our Lifelong Learning Metrics (Section 3)
do not require that an agent be aware (or unaware) of the Task it is currently Experiencing. (A Task
Identiﬁcation score could be an Application-Speciﬁc Metric, in fact.) Thus, our framework includes
existing Lifelong Learning scenarios like task-, domain-, and class-incremental learning [24].

The other set of terms in Fig. 3 identify characteristics of the performance curves. The Terminal
Learning Performance (TLP) is the performance at the end of a Learning Block; the Evaluation
Performance is the performance on a task measured within an Evaluation Block, the Droop is the drop
in performance relative to the most recent TLP for the same task; the Recovery Time is the number of
LXs needed to get back to the most recent TLP (for the same task); and the Recovery Rate is a measure
of how quickly that recovery happens.

By default, the TLP is calculated by averaging performance over the last 10% of the LXs in a Learn-
ing Block. When an Evaluation Block contains more than one EX (because of, e.g., stochasticity in the
evaluation), the Evaluation Performance is the mean of all performance values in that EB.

2.5 Application-Speciﬁc Metrics

2.5.1 Example of a single-task lifelong learning scenario

Table 1 illustrates an example Learning Scenario for assessing performance of agents within the CARLA
(Fig. 4) and SplitMNIST environments.

There are three points to note in the above table:

1. The above Lifelong Learning Scenarios start with the Deployment Stage

2. The speciﬁc number of LXs may be diﬀerent for each environment (e.g., 10000 LXs for Task-1 for
CARLA; 1000 LXs for Task-1 for SplitMNIST). This is due to the intrinsic learning diﬃculty of each
environment and the informativeness of each Learning Experience (i.e., how much it can contribute
toward the agent’s learning).

DISTRIBUTION STATEMENT A - APPROVED FOR PUBLIC RELEASE, DISTRIBUTION
UNLIMITED

6

Task

Lifelong Learning
Scenario

Application-
speciﬁc metrics
Result of running
Lifelong Learning
Scenario

CARLA
Task-1: Navigate from Point A to B in
minimum time.

SplitMNIST
Task-1: Classify images as either 0 or 1

• One LX = minibatch of sixteen

• One LX = one end-to-end naviga-

randomly sampled images

tion.

• Parametric variation = weather,

time of day, etc.

• Regime 1:

noonday lighting,

foggy

• Parametric variation = random
brightness and contrast shifts

• Regime 1: All
than normal

images brighter

• Regime 2: All images darker than

• Regime 2:

dusk/evening light,

normal

rainy

• Regime 3: stochastic time of day,

sunny weather

• Regime 3:

Images have the
chance to have very high contrast

• Learning Block 1:

10000 LXs

• Learning Block 1: 1000 LXs with

with Task-1 (regime 1)

Task-1 (regime 1)

• Learning Block 2: 5000 LXs with

• Learning Block 2: 500 LXs with

Task-1 (regime 2)

Task-1 (regime 2)

• Learning Block 3:

10000 LXs

• Learning Block 3: 1000 LXs with

with Task-1 (regime 3)

Task-1 (regime 3)

• (No Evaluation Blocks by design)

• (No Evaluation Blocks by design)

Task-1. Weighted combination of time
to destination and safe speed.

Task-1: Classiﬁcation Accuracy

• Learning Block 1: 10000 Task-1

• Learning Block 1: 1000 Task-1

reward values

metrics

• Learning Block 2: 5000 Task-1 re-

• Learning Block 2:

500 Task-1

ward values

metrics

• Learning Block 3: 10000 Task-1

• Learning Block 3: 1000 Task-1

reward values

metrics

Table 1: Sample collection of application-speciﬁc metrics for a single-task LLS

DISTRIBUTION STATEMENT A - APPROVED FOR PUBLIC RELEASE, DISTRIBUTION
UNLIMITED

7

Figure 4: An example street view from the CARLA simulator. Image taken from [2].

3. Each LX generates an application-speciﬁc metric, and these metrics are environment and task-speciﬁc.
For CARLA Task-1, the application-speciﬁc metric is a task-speciﬁc reward value, whereas for SplitM-
NIST Task-2, the application-speciﬁc metric is classiﬁcation accuracy. These metrics may have very
diﬀerent magnitudes and ranges. (The task-speciﬁcity of the metrics is further illustrated in the next
section, see Table 2).

At the end of the above LLS for a learning agent, the result would be a time series (with 25000 points
for CARLA, and 2500 points for SplitMNIST) that notionally looks like Fig. 5. We might then calculate the
droop, recovery time, etc. for each Learning Block.

Figure 5: Notional performance time series for learning scenario in Table 1, for a single application-speciﬁc
metric for a single environment. All three Learning Blocks use the same Task but with diﬀerent parameter
values (regimes). Note that there are no Evaluation Blocks by design.

DISTRIBUTION STATEMENT A - APPROVED FOR PUBLIC RELEASE, DISTRIBUTION
UNLIMITED

8

2.5.2 Example of a multi-task lifelong learning scenario

Table 2 illustrates an example Learning Scenario for assessing Transfer between two tasks by training agents
within the CARLA and SplitMNIST environments. For a single LLS, the Learning and Evaluation Blocks
are assumed to adhere to a ﬁxed parameter regime.

Note that an LX may generate multiple application-speciﬁc metrics. Furthermore, the range of values
for the application-speciﬁc metrics will vary across Tasks (across tasks within the same environment, and
a fortiori, across environments). For example, in StarCraft, “total resources collected” may range from 0
to 200; “# of enemy units defeated” may range from 0 to 10. Hence, the values for application-speciﬁc
metrics will not be directly comparable across Tasks without a normalizing function that can account for
the diﬀerence in task diﬃculty.

As output from the LLS for a learning agent, the result would be a time series that notionally looks like
Fig. 6. In this illustrative case, we might note that Task-2 performance increases between Eval Blocks 1 and
2 (indicating strong forward transfer), and Task-1 performance increases slightly between Eval Blocks 2 and
3 (indicating weak reverse transfer).

2.5.3 Preprocessing application-speciﬁc metrics

In order to be valid across variations in domain, scenario, and environment, the Lifelong Learning Metrics
require Application-Speciﬁc Metrics to be preprocessed in certain ways. The primary requirement is that task
performances be normalized to a ﬁxed range. This accounts for the fact that diﬀerent application-speciﬁc
metrics might have naturally diﬀerent ranges of variation. E.g., a classiﬁcation task might have performance
values based on a minibatch accuracy rate (ranging from 0 to 1), and a reinforcement learning task might
have performance values based on a reward function that cluster in the range -100 to 1,000.

We have also found it useful to smooth performance curves (thereby mitigating some of the stochasticity of
the learning process), and to clamp outlier performance values. In Appendix A, we give a speciﬁc instantiation
of this procedure, but the validity of the metrics does not rely on a particular preprocessing strategy.

Figure 6: Notional performance time series for learning scenario in Table 2, for a single environment and a
single application-speciﬁc metric per task within that environment.

DISTRIBUTION STATEMENT A - APPROVED FOR PUBLIC RELEASE, DISTRIBUTION
UNLIMITED

9

Task

CARLA
Task-1: Navigate from Point A to B in
minimum time

SplitMNIST
Task-1: Classify images as either 0 or 1

• One LX = minibatch of sixteen

• One LX = one end-to-end naviga-

randomly sampled images

tion process

• Parametric variation = weather,

time of day, etc.

Task-2: Follow a designated vehicle

• One LX = following designated
vehicle for a ﬁxed amount of time.

• Parametric variation = weather,
vehicle to be followed, distance to
vehicle, etc

• Parametric variation = random
brightness and contrast shifts

Task-2: Classify images as either 2 or 3

• One LX = minibatch of sixteen

randomly sampled images

• Parametric variation = random
brightness and contrast shifts

• Evaluation Block 1: 100 EXs with

• Evaluation Block 1: 10 EXs with

Task-1, 100 with Task-2

Task-1, 10 with Task-2

• Learning Block 1: 5000 LXs with

• Learning Block 1: 1000 LXs with

Task-1

Task-1

• Evaluation Block 2: 100 EXs with

• Evaluation Block 2: 10 EXs with

Task-1, 100 with Task-2

Task-1, 10 with Task-2

• Learning Block 2:

10000 LXs

• Learning Block 2: 2000 LXs with

with Task-2

Task-2

• Evaluation Block 3: 100 EXs with

• Evaluation Block 3: 10 EXs with

Task-1, 100 with Task-2

Task-1, 10 with Task-2

• Task-1: Weighted combination of
time to destination, safe speed,
etc.

• Task-2: Weighted combination of
safe

distance to other vehicle,
speed, etc.

• Task-1: Classiﬁcation accuracy

• Task-2: Classiﬁcation accuracy

Lifelong Learning
Scenario

Application-
speciﬁc metrics

Result of running
Lifelong Learning
Scenario

• Evaluation Block 1: 100 Task-1 &

• Evaluation Block 1: 10 Task-1 &

100 Task-2 reward values

10 Task-2 metrics

• Learning Block 1: 5000 Task-1 re-

ward values

• Evaluation Block 2: 100 Task-1 &

100 Task-2 reward values

• Learning Block 2: 10000 Task-2

reward values

• Evaluation Block 3: 100 Task-1 &

100 Task-2 reward values

• Learning Block 1: 1000 Task-1
metrics (Application-speciﬁc met-
rics per LX)

• Evaluation Block 2: 10 Task-1 &

10 Task-2 metrics

• Learning Block 2: 2000 Task-2
metrics (Application-speciﬁc met-
rics per LX)

• Evaluation Block 3: 10 Task-1 &

10 Task-2 metrics

Table 2: Sample collection of application-speciﬁc metrics for a multi-task LLS

DISTRIBUTION STATEMENT A - APPROVED FOR PUBLIC RELEASE, DISTRIBUTION
UNLIMITED

10

Beyond any speciﬁc preprocessing scheme, we still recommend that metrics values be presented in the
context of the scenarios in which they were generated.
Improvements in values can be confounded by
underlying diﬃculty of scenario. Such contextual descriptions are essential to prevent gaming or unfair
comparisons of the metrics.

2.6 Generating Lifelong Learning Metrics

The lifelong learning metrics are domain, scenario, environment and task-agnostic numbers that characterize
the one or more lifelong learning capabilities across the lifetime of the agent. In other words, each
Lifelong Learning metric has a single value per lifetime of an agent. Some lifelong learning metrics
will ﬁrst produce a set of task-speciﬁc values; task-speciﬁc metrics are aggregated into a single value via the
mean operation at the end of the agent’s lifetime.

Notionally, each agent lifetime generates a sequence of application-speciﬁc metrics; these metrics feed

into the computation of the lifelong learning metric, which generates a single environment-agnostic value.

3 Metrics for Lifelong Learning

3.1 Proposed Lifelong Learning Metrics

Table 3.1 presents each metric in a jargon-free manner. The L2 Conditions are deﬁned in Section 2.1. Several
additional metrics are presented in Appendix B.

Metrics Assesses System’s Ability
To:
Be robust to catastrophic forgetting de-
spite the introduction of new parame-
ters and/or tasks
Make use of knowledge acquired from
one task to catalyze learning a new task
Make use of knowledge acquired from a
new task to improve performance on a
previously learned task
Approach or exceed the performance of
a Single Task Expert (STE)
Make use of
learn tasks quickly and eﬃciently

incoming knowledge to

L2 Condition

Metric Name:

Continual Learning

Performance Maintenance

Transfer and Adapation

Forward Transfer

Transfer and Adaptation

Backward Transfer

Transfer and Adaptation

Performance Relative to STE

Sustainability

Sample Eﬃciency

Table 3: Lifelong Learning Metrics

3.2 Single-Task Metrics, Cross-Task Metrics, and Aggregation Across a Life-

time

Of the proposed Lifelong Learning Metrics, some are calculated using performance measures for a single Task,
and others are calculated between pairs of Tasks. Furthermore, most metrics utilize quantities calculated
multiple times during a lifetime: Performance Maintenance, for example, uses Maintenance Values calculated
after each Learning Block.

To report on a system’s lifetime performance, each metric calls for its values to be aggregated into a single
quantity. We recommend performing this aggregation with the mean operator, although other operators are
possible.1 It can also be informative to examine how these sub-metrics change over time; this can inform
understanding of how an agent’s learning dynamics vary.

1Due to computational constraints, lifelong learning scenarios may feature fairly short lifetimes (e.g., three repetitions of

four core tasks). In this low-data setting, distribution means are often easier to estimate than distribution medians.

DISTRIBUTION STATEMENT A - APPROVED FOR PUBLIC RELEASE, DISTRIBUTION
UNLIMITED

11

3.3 Contrasts vs. Ratios for Calculating Transfer

Our formulation of forward and backward transfer uses the contrast function:

Contrast(a, b) =

a − b
a + b

The contrast function is qualitatively similar to the ratio function: Ratio(a, b) =

. However, contrasts
are deﬁned when b = 0, unlike ratios. This makes them more robust in our use case, when application-
speciﬁc metrics could be zero. Interpreting a ratio is easier than interpreting a contrast, so ratios may also
be calculated if desired.

a
b

3.4 Proposed Lifelong Learning Metrics

An important note about these metric scores is that they must be presented in the context of the scenarios
in which they were generated. For example, an agent might exhibit a reduced positive PR score if the
input conditions are such that the agent has to “work harder” to achieve improved performance over the
Learning Experiences. Such contextual descriptions are essential to prevent gaming or unfair comparisons
of the metrics.

For each metric, we describe how to qualitatively interpret the values of the metric: does the metric
suggest a system is demonstrating lifelong learning? To ensure the meaningfulness of these interpretations,
they should be considered in the context of the learning scenario that generated the metrics. A poorly-
designed scenario can yield nonsensical metric values.

For each metric, we explain how to calculate it, and give an interpretation of its values (i.e., does a given
observed value indicate that a system is demonstrating lifelong learning). Then we also note references from
the literature that use this metric or something similar to it. For example, transfer has been formalized
in several diﬀerent ways. These references were sourced from papers published at NeurIPS 2020 and 2019,
ICML 2020 and 2019, and ICLR 2020 and 2019.

In Figs. 7, 8, 9, and 10, we show performance curves for two representative scenarios, generated by an
idealized learning system. Figs. 7 and 9 show the performance of the L2 agent in Learning and Evaluation
Blocks, and Figs. 8 and 10 compare the Learning Block performance of the L2 agents to STEs.

3.4.1 Performance Maintenance (PM)

Does an L2 agent catastrophically forget a previously learned task? Metrics similar to PM are used in [16, 9, 8]

• Computed by:

– Select an application-speciﬁc metric to monitor for the given environment (e.g., total reward).

– Set up a learning scenario with a sequence of Learning Blocks alternating with Evaluation Blocks.

Each Evaluation Block exercises all the previously learned tasks.

– For a given Task:

∗ Calculate the Maintenance Values, deﬁned as the diﬀerences between each of the Task’s Eval-
uation Performances (excluding Evaluation Blocks immediately following the Task’s Learning
Blocks) and the Evaluation Performance after the Task’s most recent Learning Block.

– Performance Maintenance (PM) for a lifetime = mean Maintenance Value across the lifetime.

• Interpretation of the metric:

– PM = 0: No forgetting; no additional learning.

– PM > 0: (Demonstrates LL) that performance on task is getting better over lifetime; May be an

indication of transfer.

– PM < 0: (Does not demonstrate LL) Indicates forgetting.

• Notes:

DISTRIBUTION STATEMENT A - APPROVED FOR PUBLIC RELEASE, DISTRIBUTION
UNLIMITED

12

Figure 7: Performance curves for an L2 agent in a scenario with two tasks. White regions indicate Learning
Blocks, and shaded regions indicate Evaluation Blocks. Bi and Gi refer to performance in the ith Evaluation
Block for the blue and green tasks, respectively.

Figure 8: Single-task expert (orange) and L2 agent (blue and green) curves for the scenario shown in Fig. 7.
Dashed lines indicate the boundaries between Learning Blocks.

DISTRIBUTION STATEMENT A - APPROVED FOR PUBLIC RELEASE, DISTRIBUTION
UNLIMITED

13

Figure 9: Performance curves for an L2 agent in a scenario with two tasks. White regions indicate Learning
Blocks, and shaded regions indicate Evaluation Blocks. Bi and Gi refer to performance in the ith Evaluation
Block for the blue and green tasks, respectively.

Figure 10: Single-task expert (orange) and L2 agent (blue and green) curves for the scenario shown in Fig. 9.
Dashed lines indicate the boundaries between Learning Blocks.

DISTRIBUTION STATEMENT A - APPROVED FOR PUBLIC RELEASE, DISTRIBUTION
UNLIMITED

14

– For tasks with signiﬁcant overlap, Performance Maintenance may act as an additional measure of
transfer; however, even learning disjoint tasks should result in a minimum score of zero for PM.

– Performance Maintenance is confounded in lengthy scenarios where the system quickly saturates
in task performance. As the scenario length increases, PM will trend toward 0, as no further
improvement is possible.

– Fig. 7 shows positive Performance Maintenance for both the blue and green tasks.

– Fig. 9 shows negative Performance Maintenance for both the blue and green tasks.

3.4.2 Forward Transfer (FT)

Does an L2 agent improve learning on a new task by leveraging data from previous tasks? Existing literature
often calls this a “jumpstart” formulation of the concept of forward transfer, which measures the boost in
performance one task gives to another when the second task is ﬁrst seen. Metrics similar to FT are used
in [16, 17, 10, 20]

• Computed by:

– Select an application-speciﬁc metric to monitor for the given environment (e.g. total reward).

– Set up a learning scenario beginning with initial Evaluation Blocks for all tasks, followed by a

sequence of Learning Blocks (for diﬀerent tasks) alternating with Evaluation Blocks.

– Assuming a block sequence like: Eval Block 1, Learning Block 1 (Task-1), Eval Block 2, then Task-
2’s Forward Transfer (from Task-1) is computed as the contrast of the Evaluation Performances
of Task-2 in Eval Block 2 to Eval Block 1.

– The Forward Transfer for a lifetime is the mean of each task pair’s ﬁrst Forward Transfers.

• Interpretation of metric:

– FT = 0: No transfer or forgetting.

– FT > 0: (Demonstrates LL) Indicates positive forward transfer.

– FT < 0: (Does not demonstrate LL) Indicates interference.

• Notes:

– FT is computed only once per task pair, and can only be computed before that task has been

learned, since a task is only “new” before it has been seen.

– Order of the tasks is important. Forward Transfer may be present from Task 1 → 2, but not 2

→ 1 or vice versa.

– The usefulness of a task’s FT score depends on the structure of the learning scenario. If, in Fig.
9’s example below, Task-1’s ﬁrst learning block were very short in length, then the calculated
transfer from Task-1 to Task-2 would not be an informative metric, as there would not been
suﬃcient experience for an agent to learn a shared task representation.

– As a “jumpstart” metric, FT is highly dependent on a task’s initial performance.

– In Figs. 7 and 9, the forward transfer from the blue task (Task B) to the green task (Task G) is

given by Contrast(G1, G0) =

G1 − G0
G1 + G0

.

3.4.3 Backward Transfer (BT)

Does an L2 agent improve performance on a previously learned task by leveraging data from new tasks? This
is a “jumpstart” formulation of the concept of backward transfer, which measures the boost in performance
one task gives to another when the second task is ﬁrst seen. Metrics similar to BT are used in [17, 6, 10, 3]

• Computed by:

DISTRIBUTION STATEMENT A - APPROVED FOR PUBLIC RELEASE, DISTRIBUTION
UNLIMITED

15

– Select an application-speciﬁc metric to monitor for the given environment (e.g., total reward).

– Set up a learning scenario with a sequence of Learning Blocks. Between each Learning Block,

there are Evaluation Blocks for each of the other tasks.

– For each task:

∗ Compute the Backward Transfer Contrast, deﬁned as the contrast of the average performance

within the most recent Evaluation Block to the second-most recent Evaluation Block.

∗ Backward Transfer for task T = the average of the Backward Transfer Contrasts.

– The Backward Transfer for a scenario is the mean of each task pair’s ﬁrst calculated Backward

Transfer value.

• Interpretation of metric:

– BT = 0: No interference, but no transfer, either.

– BT > 0: (Demonstrates LL) Indicates positive backward transfer.

– BT < 0: (Does not demonstrate LL) Indicates interference.

• Notes:

– The scenario’s Backward Transfer value takes as input only each task’s ﬁrst calculated BT value.
However, unlike Forward Transfer, Backward Transfer can be computed several times for each
task in a lifetime.

– These additional BT values may still be calculated and track to provide additional mechanisms
for assessing performance. Note that, if a lifetime lasts suﬃciently long that agents saturate in
all their tasks, BT values may trend toward 0.

– While there is no need to start the scenario with a “pop quiz” evaluation, both LX and EX blocks

of the same task are required to compute BT.

– The usefulness of a task’s BT score depends on the structure of the learning scenario. If, in Fig.
7’s example above, Task-2’s learning block were very short in length, then the calculated transfer
from Task-2 to Task-1 would not be an informative metric, as there would not been suﬃcient
experience for an agent to learn a shared task representation.

– As a “jumpstart” metric, BT is highly dependent on a task’s initial performance.

– In Figs. 7 and 9, the backward transfer from the green task (Task G) to the blue task (Task B)

is given by Contrast(B2, B1) =

B2 − B1
B2 + B1

.

3.4.4 Performance Relative to a Single-Task Expert (RP)

How does the performance of an L2 system compare to a non-lifelong learner with comparable learning
experience? Metrics similar to RP are used in [8].

• Computed by:

– Select an application-speciﬁc metric to monitor for the given environment (e.g. total reward) that

has also been logged for a Single Task Expert (e.g. a non-lifelong learner).

– Set up a learning scenario with some sequence of Learning Blocks for some number of tasks.

– For a given task T :

∗ Extract the Learning Blocks for Task T , in order of appearance. This is illustrated in Figs. 8

and 10.

∗ Compare the “area under the curve” for the lifelong learner experiencing Task T with the

area under the curve for the equivalent Single Task Expert.

DISTRIBUTION STATEMENT A - APPROVED FOR PUBLIC RELEASE, DISTRIBUTION
UNLIMITED

16

∗ Formally, Compute the Single Task Expert Ratio, deﬁned as the ratio of the sum of the
application-speciﬁc metric over all of the Learning Experiences in the lifetime to the sum of
the same application-speciﬁc metric over the same amount of learning experiences for the
Single Task Expert.

∗ Relative Performance for Task T = the Single Task Expert Ratio.

– The RP for a lifetime is the mean of each task’s RP score.

• Interpretation of metric:

– RP = 1 : L2 agent demonstrates the same overall performance compared to the Single Task

Expert’s performance.

– RP > 1: (Demonstrates LL) L2 agent demonstrates better performance given the same amount

of Learning Experiences than the Single Task Expert.

– RP < 1: (Does not demonstrate LL) L2 agent demonstrates worse performance given the same

amount of Learning Experiences than the Single Task Expert.

• Notes:

– For each task, Single Task Expert “learning curves” are required.

– If the STE and L2 Learner learning curves contain diﬀering numbers of LXs, RP is calculated by

using the minimal number of LXs.

– No Evaluation Blocks are necessary for this computation.

– In Fig. 8, RP is positive, and, in Fig. 10, RP is generally negative.

3.4.5 Sample Eﬃciency (SE)

Does the system learn quickly and eﬃciently over the course of its lifetime? Metrics similar to SE are used
in [20].

• Computed by:

– Select an application-speciﬁc metric to monitor for the given environment (e.g. total reward) that

has also been logged for a Single Task Expert (e.g. a non-lifelong learner).

– Set up a learning scenario with some sequence of Learning Blocks for some number of tasks.

– Intuitively, compare the saturation value of the Single Task Expert with that of the lifelong

learner.

– For each task T :

∗ Extract the Learning Blocks for Task T , in order of appearance. This is illustrated in Figs. 8

and 10.

∗ Compute Saturation Value (the max of the rolling average of the application-speciﬁc metric)
and the Experience to Saturation (the number of Learning Experiences it takes to achieve
the Saturation Value) for both the L2 agent and the Single Task Expert (STE) system.

∗ Compute the ratio of the Saturation Values of the L2 agent and the STE.
∗ Compute the ratio of the Experience to Saturation (ETS) for the STE and the L2 Agent.
∗ Sample Eﬃciency = Saturation Value Ratio × Experience to Saturation Ratio.
∗ In Fig. 8, SE is above 1, and, in Fig. 10, SE is below 1.

– The Sample Eﬃciency for a lifetime is the mean of each task’s SE score.

• Interpretation of metric:

– SE = 1 : L2 agent demonstrates the same overall performance compared to the Single Task

Expert’s performance.

DISTRIBUTION STATEMENT A - APPROVED FOR PUBLIC RELEASE, DISTRIBUTION
UNLIMITED

17

– SE > 1: (Demonstrates LL) L2 agent demonstrates better performance than the Single Task
Expert; either it learns faster or achieves a higher overall value compared to the Single Task
Expert.

– SE < 1: (Does not demonstrate LL) L2 agent demonstrates worse performance than the Single
Task Expert; either it learns slower or achieves a lower overall value compared to the Single Task
Expert.

• Notes:

– Single Task Expert “learning curves” are required for this computation

– Only tasks which demonstrate a steady-state/saturated performance are eligible for this metric.

Acknowledgements

Primary development of this document was funded by the DARPA Lifelong Learning Machines (L2M)
Program. The authors would like to thank the performers in this program, particularly the members of the
Deﬁnitions and Metrics Working Groups, who helped to develop and reﬁne these metrics. Thanks also goes
to the DARPA SETAs for this program – Conrad Bell, Robert McFarland, Rebecca McFarland, and Ben
Epstein.

Disclaimer

The views, opinions, and/or ﬁndings expressed are those of the author(s) and should not be interpreted as
representing the oﬃcial views or policies of the Department of Defense or the U.S. Government.

References

[1] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evalu-
ation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, jun 2013.

[2] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA: An
open urban driving simulator. In Proceedings of the 1st Annual Conference on Robot Learning, pages
1–16, 2017.

[3] Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-guided con-
tinual learning with bayesian neural networks. In International Conference on Learning Representations,
2020.

[4] Sebastian Farquhar and Yarin Gal. Towards robust evaluations of continual learning, 2019.

[5] Robert M. French. Catastrophic forgetting in connectionist networks. Trends in Cognitive Sciences,

3(4):128–135, 1999.

[6] Gunshi Gupta, Karmesh Yadav, and Liam Paull. La-maml: Look-ahead meta learning for continual

learning. CoRR, abs/2007.13904, 2020.

[7] Yen-Chang Hsu, Yen-Cheng Liu, and Zsolt Kira. Re-evaluating continual learning scenarios: A catego-

rization and case for strong baselines. CoRR, abs/1810.12488, 2018.

[8] K J Joseph and Vineeth N Balasubramanian. Meta-consolidation for continual learning, 2020.

[9] Sangwon Jung, Hongjoon Ahn, Sungmin Cha, and Taesup Moon. Adaptive group sparse regularization

for continual learning. CoRR, abs/2003.13726, 2020.

DISTRIBUTION STATEMENT A - APPROVED FOR PUBLIC RELEASE, DISTRIBUTION
UNLIMITED

18

[10] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence of similar and
dissimilar tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances
in Neural Information Processing Systems, volume 33, pages 18493–18504. Curran Associates, Inc.,
2020.

[11] James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A.
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Clau-
dia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural
networks. CoRR, abs/1612.00796, 2016.

[12] Vincenzo Lomonaco and Davide Maltoni. Core50: a new dataset and benchmark for continuous object

recognition. CoRR, abs/1705.03550, 2017.

[13] Vincenzo Lomonaco, Lorenzo Pellegrini, Andrea Cossu, Antonio Carta, Gabriele Graﬃeti, Tyler L.
Hayes, Matthias De Lange, Marc Masana, Jary Pomponi, Gido M. van de Ven, Martin Mundt, Qi She,
Keiland Cooper, Jeremy Forest, Eden Belouadah, Simone Calderara, German Ignacio Parisi, Fabio Cuz-
zolin, Andreas S. Tolias, Simone Scardapane, Luca Antiga, Subutai Amhad, Adrian Popescu, Christo-
pher Kanan, Joost van de Weijer, Tinne Tuytelaars, Davide Bacciu, and Davide Maltoni. Avalanche:
an end-to-end library for continual learning. CoRR, abs/2104.00405, 2021.

[14] Manolis Savva*, Abhishek Kadian*, Oleksandr Maksymets*, Yili Zhao, Erik Wijmans, Bhavana Jain,
Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A
Platform for Embodied AI Research. In Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV), 2019.

[15] Michael McCloskey and Neal J. Cohen. Catastrophic interference in connectionist networks: The sequen-
tial learning problem. volume 24 of Psychology of Learning and Motivation, pages 109–165. Academic
Press, 1989.

[16] Jorge A. Mendez, Boyu Wang, and Eric Eaton. Lifelong policy gradient learning of factored policies for

faster training without forgetting. CoRR, abs/2007.07011, 2020.

[17] Pingbo Pan, Siddharth Swaroop, Alexander Immer, Runa Eschenhagen, Richard E. Turner, and Mo-
hammad Emtiyaz Khan. Continual deep learning by functional regularisation of memorable past, 2021.

[18] German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter. Continual

lifelong learning with neural networks: A review. Neural Networks, 113:54–71, 2019.

[19] Sam Powers, Eliot Xing, Eric Kolve, Roozbeh Mottaghi, and Abhinav Gupta. Cora: Benchmarks,

baselines, and metrics as a platform for continual reinforcement learning agents, 2021.

[20] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience
replay for continual learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and
R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates,
Inc., 2019.

[21] Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray
Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. CoRR, abs/1606.04671,
2016.

[22] Pranab Kumar Sen. Estimates of the regression coeﬃcient based on kendall’s tau. Journal of the

American Statistical Association, 63(324):1379–1389, 1968.

[23] Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor. Airsim: High-ﬁdelity visual and

physical simulation for autonomous vehicles. In Field and Service Robotics, 2017.

[24] Gido M. van de Ven and Andreas S. Tolias. Three scenarios for continual

learning. CoRR,

abs/1904.07734, 2019.

DISTRIBUTION STATEMENT A - APPROVED FOR PUBLIC RELEASE, DISTRIBUTION
UNLIMITED

19

[25] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle
Yeo, Alireza Makhzani, Heinrich K¨uttler, John P. Agapiou, Julian Schrittwieser, John Quan, Stephen
Gaﬀney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado van Hasselt, David Silver, Timothy P. Lil-
licrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence, Anders Ekermo, Jacob Repp,
and Rodney Tsing. Starcraft II: A new challenge for reinforcement learning. CoRR, abs/1708.04782,
2017.

A Preprocessing Application-Speciﬁc Metrics

Here, we give an example metrics preprocessing strategy. Note both the ordering of operations, and the
speciﬁc values used for clamping and normalization. This strategy is intended for the case that there are
multiple available performance curves for both L2 and STE agents, but it still works when there is only one
curve of each agent type.

1. For each Learning Block (across all available performance curves), smooth the performance curves with
a ﬂat window of length L, where L is the minimum of 20% of the LB’s length (in LXs) and 100 LXs.
Pad the smoothed curve so that it is the same length as the raw curve.

2. For each Task Variant in the scenario, calculate the 10% and above the 90% percentiles of the distribu-
tion of that Task Variant’s Application-Speciﬁc Metric Values, over the Lifelong Learning Agent and
the STE.

3. Reduce the impact of outliers by clamping values below the 10% and 90% percentiles to those values,
and then scale all performance values to a ﬁxed range. (We recommend 1 to 101, as having a minimum
performance of 0 can cause jumpstart metrics to take uninformatively large values.)

4. If a Task Variant has multiple known Single Task Experts, aggregate relevant Lifelong Learning Metrics
by computing those metrics with respect to each Single Task Expert, then averaging the values at the
end.

B Supplemental Metrics

The following metrics have also been proposed, but they are not as ﬁne-tuned in deﬁnition.

B.1 Performance Recovery (PR)

How eﬀectively can an L2 agent’s performance “bounce back” after a change is introduced to its environment?
Performance Recovery is a Single-Task metric.

• Computed by:

– For each Task:

∗ Select an application-speciﬁc metric to monitor for the given environment (e.g. total reward).
∗ Set up a learning scenario with a sequence of LX blocks. Each LX block introduces a para-

metric change to an already-learned task.

∗ From the second Learning Block onwards, calculate the Recovery Time relative to the most
recent Terminal Learning Performance. The “Recovery Time” is the number of LXs for
performance to return to the previous Terminal Learning Performance.

∗ Task-Speciﬁc Performance Recovery = negative slope of the line of (Learning Block index,

recovery time) values.

∗ Report Lifetime PR as the mean of all Task-Speciﬁc PRs.

• Interpretation:

DISTRIBUTION STATEMENT A - APPROVED FOR PUBLIC RELEASE, DISTRIBUTION
UNLIMITED

20

– PR = 0: system is not exhibiting learning (the Recovery Times are not getting better or worse

over the lifetime of the agent).

– PR > 0: (Demonstrates LL) Recovery Times are decreasing over the lifetime, i.e., the system gets

faster at recovering from environmental changes.

– PR < 0: (Does not demonstrate LL) Recovery Times are increasing over the lifetime, i.e., the

system gets slower at recovering from environmental changes.

• Notes:

– Performance Recovery can only be assessed while the system is learning, as a frozen system will

not recover.

– If, during a Learning Block, the system never recovers to its most recent Terminal Learning

Performance, the recovery time is (# LXs in the learning block) + 1.

– If, due to transfer, a system’s performance starts above the previous Terminal Learning Perfor-

mance, the recovery time is 0.

– To handle potentially inﬁnite recovery times, we recommend the Theil-Sen estimator of slope [22].

– For long lifetimes in which agent performance saturates, PR may trend to 0, as all of a system’s

recovery times trend to 0.

– The PR slope is negative so that all L2M metrics have the same monotonic interpretation (i.e.,

bigger is better).

– This metric has been applied in evaluations, unlike the other supplemental metrics. However,
experience in these suggested that it has high variability and limited utility and informativeness.

B.2 Cumulative Gain of a Lifelong Learner (CG)

Does an L2 system continue to learn as new tasks are introduced over the course of its lifetime?

• Computed by:

– Select an application-speciﬁc metric to monitor for the given environment (e.g. total reward).

– Set up a learning scenario with some sequence of LX blocks for some number of tasks.

– For each LX block, compute a trend line through the application-speciﬁc metric to determine

gain:

∗ If trendline has positive slope, gain = 1.
∗ If trendline has zero slope, gain = 0.
∗ If trendline has negative slope, gain = -1.

– CG = average of all gains.

• Interpretation of metric:

– CG ≥ 0: (Demonstrates LL) L2 agent is not losing the ability to learn.

– CG < 0: (Does not demonstrate LL) L2 agent’s ability to learn is deteriorating over time.

• Notes:

– No EX blocks are required for this metric

DISTRIBUTION STATEMENT A - APPROVED FOR PUBLIC RELEASE, DISTRIBUTION
UNLIMITED

21

B.3 Learn Burn (LB)

Does an L2 system eﬃciently make use of learning opportunities when changes are introduced?

• Computed by:

– Select an application-speciﬁc metric to monitor for the given environment (e.g. total reward).

– Set up a learning scenario with some sequence of LX blocks for some number of tasks and/or

parametric variations.

– After each change is introduced:

∗ Compute Burn Rate for change i, (BRi), deﬁned as the slope of the learning curve in an LX

block following the change.

– Compute Average Learn Rate across the whole lifetime, deﬁned as the slope of the trendline across

all performances.

– Learn Burn LB = average Burn Rate / Average Learn Rate.

• Interpretation of metric:

– LB > 1: (Does not demonstrate LL) L2 agent, on average, learns slowly while adapting to a

change.

– LB < 1: (Demonstrates LL) L2 agent, on average, learns quickly while adapting to a change.

• Notes:

– No EX blocks are required for this metric.

C Glossary of Terms

Table C deﬁnes the terms used in this document.

DISTRIBUTION STATEMENT A - APPROVED FOR PUBLIC RELEASE, DISTRIBUTION
UNLIMITED

22

Learning Machine
Lifelong
(equivalently, L2 System or L2
Agent)
Condition of Lifelong Learning

Domain

Environment

Task

Task Instance

Learning Experience

Learning Scenario

Lifelong Learning Scenario

Pre-Deployment Stage

Deployment Stage

Agent Lifetime

The system that is capable of demonstrating lifelong learning in a speciﬁc
domain and environment.

A necessary characteristic of lifelong learning; something that a L2 agent
exhibits. The set of L2 conditions comprises a necessary and suﬃcient
deﬁnition of L2.
The kind of challenge being addressed by the lifelong learning agent.
Examples: autonomous navigation; robotics; embodied agent
The computational setting for the domain; the “world” in which the
L2 agent acts. A given environment can support diﬀerent domains (de-
pending on how Tasks are formulated). Examples: StarCraft [25], Air-
Sim [23], CARLA [2], Habitat [14], Arcade [1]
Tasks and Learning Scenarios
A single environmentally-relevant capability that a L2 Agent must learn;
equivalently, a capability that is organized to achieve a single speciﬁc
goal.
A Task is uniquely speciﬁed by an environment; observation space; action
space; a reward function; and a range of parametric variations (that can
be either goal-relevant or irrelevant).
A Task may have a well-deﬁned end state (e.g., agent “dies”; goal accom-
plished) or it may continue indeﬁnitely (e.g, open-world exploration).
Example: In Arcade, Pong is a Task with a potential range of variation
(paddle width, ball speed, background color, etc.).
A instance of a Task; a speciﬁc environmental context that the L2 agent
must act in to accomplish a goal.
Example: In Arcade, A Task Instance of Pong is an actual game that
a L2 agent must play (with speciﬁc instantiated values of paddle width,
ball speed etc.)
An interaction with the environment that gives the L2 agent an oppor-
tunity to do task-relevant learning.
A template for a sequence of Learning Experiences that exercises a L2
agent and allows it to exhibit one or more L2 conditions. A Learning
Scenario is agnostic to speciﬁc Environments and Tasks.
A Learning Scenario consists of an informal description of:
1. The Task sequence encountered by the agent (e.g., how long the L2
agent experiences each Task; when new Tasks or new Task Instances are
introduced).
2. When and how the L2 agent is evaluated during that sequence
3. What constitutes successful lifelong learning within that scenario
A Learning Scenario supports one or more Metrics (i.e., a Learning Sce-
nario allows for calculation of the metrics, and thereby assess the L2
capability of the agent).
An instantiation of a Learning Scenario in a speciﬁc environment. Con-
sists of a sequence of Task Instances drawn from one or more Tasks.

L2 Agent ”Lifetime”

An optional stage where agents are endowed with pre-built knowledge
or capabilities for a speciﬁc environment. Learning during this phase is
not considered part of lifelong learning as such.
Once the agent is released into the ﬁeld, this is considered the start of
the agent’s “lifetime”, and the start of its lifelong learning. Consists of
Learning and Evaluation Experiences.
A sequence of one or more Lifelong Learning Scenarios.

Table 4: Glossary of Terms

DISTRIBUTION STATEMENT A - APPROVED FOR PUBLIC RELEASE, DISTRIBUTION
UNLIMITED

23

