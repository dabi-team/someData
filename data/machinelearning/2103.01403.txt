2
2
0
2

p
e
S
0
2

]

G
L
.
s
c
[

2
v
3
0
4
1
0
.
3
0
1
2
:
v
i
X
r
a

A Minimalist Dataset for Systematic Generalization
of Perception, Syntax, and Semantics

Qing Li1
liqing@ucla.edu

Siyuan Huang2
syhuang@bigai.ai

Yining Hong1
yininghong@cs.ucla.edu

Yixin Zhu3,4
yixin.zhu@pku.edu.cn

Ying Nian Wu1
ywu@stat.ucla.edu

Song-Chun Zhu1,2,3,4,5
sczhu@bigai.ai

1 Center for Vision, Cognition, Learning, and Autonomy (VCLA), UCLA
2 Beijing Institute for General Artiﬁcial Intelligence (BIGAI)
3 Institute for Artiﬁcial Intelligence, Peking University

4 School of Artiﬁcial Intelligence, Peking University

5 Department of Automation, Tsinghua University

Abstract

Inspired by humans’ remarkable ability to master arithmetic and generalize to
unseen problems, we present a new dataset, Handwritten arithmetic with INTegers
(HINT), to study machines’ capability of learning generalizable concepts at three
levels: perception, syntax, and semantics. Learning agents are tasked to reckon
how concepts are perceived from raw signals such as images (i.e., perception), how
multiple concepts are structurally combined to form a valid expression (i.e., syntax),
and how concepts are realized to afford various reasoning tasks (i.e., semantics),
all in a weakly supervised manner. With a focus on systematic generalization,
we carefully design a ﬁve-fold test set to evaluate both the interpolation and the
extrapolation of learned concepts w.r.t. the three levels. We further design a
few-shot learning split to test whether models could quickly learn new concepts
and generalize them to more complex scenarios. To understand existing models’
limitations, we conduct extensive experiments with various sequence-to-sequence
models, including RNNs, Transformers, and GPT-3 (with the chain of thought
prompting). The results suggest that current models still struggle in extrapolation
to long-range syntactic dependency and semantics. Models show a signiﬁcant gap
toward human-level generalization when tested with new concepts in a few-shot
setting. Moreover, we ﬁnd that it is infeasible to solve HINT by simply scaling
up the dataset and the model size; this strategy barely helps the extrapolation
over syntax and semantics. Finally, in zero-shot GPT-3 experiments, the chain
of thought prompting shows impressive results and signiﬁcantly boosts the test
accuracy. We believe the proposed dataset together with the experimental ﬁndings
is of great interest to the community on systematic generalization.1

1

Introduction

Humans possess a versatile mechanism for learning concepts [29]. Take the arithmetic examples
in Fig. 1: When we master concepts like digits and operators, we not only know how to recognize,
write, and pronounce them—what these concepts mean at the perceptual level, but also know how to
compose them into valid expressions—at the syntactic level, and how to calculate the results—at the
semantic level. Learning concepts heavily rely on these three-level interweaving meanings. Such an
observation also conforms with the classic view of human cognition, which postulates at least three
distinct levels of organizations in computation systems [75, 31].

1We release our dataset, code, and experiments at our project website https://liqing-ustc.github.io/HINT/.

Preprint. Under review.

 
 
 
 
 
 
Another appealing property of human concept
learning is its systematic compositionality [12,
69]: the algebraic capacity to understand and
produce a potentially inﬁnite number of novel
combinations from a ﬁnite set of known com-
ponents, i.e., “inﬁnite use of ﬁnite means” [13].
This type of compositionality is crucial to the
human ability to make strong generalizations
from limited data [59]. Human demonstrate sys-
tematic compositionality in many domains. E.g.,
once we master the syntax of arithmetic using
short expressions, we can parse novel, long ex-
pressions; once we master operators’ semantics
using small numbers, we can apply them over
novel, large numbers.

Figure 1: Concept learning and generalization at
three levels. A learning agent needs to simultaneously
master (i) perception, how concepts are perceived from
raw signals such as images, (ii) syntax, how multiple
concepts are structurally combined to form a valid ex-
pression, and (iii) semantics, how concepts are realized
to afford various reasoning tasks.

The emerging community on learning models
capturing human-like systematic composition-
ality has introduced various benchmarks [57,
43, 50, 4, 81, 51] and methods [18, 56, 82, 2,
33, 15, 7, 1]. However, as collecting real data
with systematic compositionality is challenging,
prior benchmarks are from artiﬁcial domains
with synthetic data and tasks and only cover a
partial spectrum of concept learning; see Tab. 1
for an in-depth comparison. Critically, prior datasets often mix syntax and semantics when evaluating
systematic compositionality.2 For example, the SCAN dataset [57] is a semantic parsing task from
natural language commands to action sequences; when a model fails on a longer command than the
ones in the training set, the root cause could come from understanding the complex syntactic relations
in a long input sequence (command) or its deﬁciency of generating a long output sequence (actions)
(e.g., due to the EOS decision problem [71]). Furthermore, prior benchmarks often involve simple
semantics (e.g., a simple mapping or repeating), leading to an undesirable bias towards syntactic
generalization.

To extend the systematic compositionality to a full-spectrum systematic generalization w.r.t. percep-
tion, syntax, and semantics, we take inspiration from arithmetic and introduce a new benchmark
HINT, Handwritten arithmetic with INTegers. The task of HINT is intuitive: Machines take as
input images of handwritten expressions and predict the ﬁnal results of expressions, restricted in
the integers. This design is minimal yet complete on systematic generation with real images. HINT
differs from prior datasets as: (i) The images are realistic handwriting with considerable visual
variance. (ii) The syntactic relations between the tokens in the expressions are more complex with
long-range dependency. (iii) The semantics are the functional meanings of these arithmetic concept,
which are more complicated than the simple mappings in previous datasets. In HINT, models are
tasked to learn the three-level meanings simultaneously to predict the correct results. Since there is
no intermediate supervision, the three-level meanings are presumably intertwined during learning.

The three meanings in HINT have clear deﬁnition and boundary, thus affords to design tests focusing
on a certain type of generalization. For example, to evaluate if a model can extrapolate over syntax, we
can build a test set composed of longer expressions with the same range of results as the training set.
To provide a holistic and rigorous test on how models generalize the learned concepts, we introduce
a carefully designed evaluation scheme with ﬁve subsets, focusing on generalization patterns (i.e.,
interpolation and extrapolation) at different levels (i.e., perception, syntax, and semantics).

To facilitate future research along this direction, we conduct extensive experiments of various
sequence-to-sequence models, including Recurrent Neural Networks [40, 14], Transformers [88],
and GPT-3 [8] (with the chain of thought prompting). Our experiments show that there is still large
room for improvement on HINT: Even the state-of-the-art model, Universal Transformer [17] with
relative positional encoding [85, 16], can only achieve an accuracy of 54% on HINT, whereas the

2The concept of “syntax” and “semantics” have various meanings in different communities. In this work, we
adopt the original and speciﬁc deﬁnition, wherein “syntax” is the underlying compositional structure of input
sequences, and “semantics” represents what concepts mean in reasoning tasks.

2

syntaxperceptionsemanticsLearnLearnGeneralizeGeneralize+3×92+8-61)(÷×-97)(451+1=22×3=67–2=56×9=5431–7=2417+23=40very same model obtains nearly perfect accuracy on previous datasets like SCAN [15]. An in-depth
analysis of the results on each test subset reveals that current models still struggle on extrapolation
to long-range syntactic dependency and semantics. In the GPT-3 experiments, the chain of thought
prompting boost the zero-shot test accuracy by a large margin from 8.6% to 27.6%.

By inspecting the scaling trends of the test accuracy w.r.t. the size of the model and the dataset, we
ﬁnd that it is impractical to solve HINT by scaling up the size of dataset or the model as done in NLP
tasks [49, 38]; more data and parameters barely help the extrapolation over syntax and semantics.

Although the top performing models exhibit decent capabilities of learning new concepts, the few-
shot learning experiments show that they are still far from the human-level generalization that only
requires the learning examples of a new concept in a primitive form and readily generalizes to more
complex compositions of the learned concept.

Taking together, we present the HINT dataset for systematic generalization of perception, syntax, and
semantics. We benchmark various seq2seq models and identify the key weaknesses. We hope our
dataset and the experimental ﬁndings inspire new advancements in systematic generalization.

2 Related Work

Three Levels of Concept Learning Although the recent surge of deep learning [60] has signif-
icantly advanced the perception accuracy given raw signals across multiple modalities, including
images [37, 54] and audios [73, 39, 34], learning models fail to disentangle the perceptual generaliza-
tions from the other two levels of generalizations. Syntax analysis is to understand the compositional
and recursive structures in various tasks, such as natural language parsing [9, 52, 46], image and
video parsing [87, 99, 98, 35, 77, 76, 44], scene understanding [42, 41, 78, 45, 11, 93, 36], task
planning [92, 64, 21, 63, 97], and abstract reasoning [94, 95, 96, 23, 24, 22]. Semantics of concepts
essentially describe its causal effect. Two primary semantic representations prevail in symbolic rea-
soning: (i) Logic [66, 68] regards the semantic learning as inductive logic programming [70, 28]—a
general framework to induce ﬁrst-order logic theory from examples. (ii) Program treats the semantic
learning as inductive program synthesis [55, 58, 6, 20, 25, 27, 26]. Compared to the aforementioned
literature, ours is the ﬁrst that takes a more holistic stand that simultaneously addresses all three levels
of concept learning, essentially taking one step closer to realize a versatile mechanism of concept
learning under weak supervision.

Table 1: Dataset categorization and comparison. SP: semantic parsing, IC: image classiﬁcation, QA: question
answering, NV: navigation, i&t: image & text. Perception/Syntax/Semantics denote whether the task requires
models to learning perception/syntax/semantics. Generalization denotes the type of generalization required to
handle the test examples. *Images in the dataset are generated with small variance. +Datasets require learning
semantics but do not evaluate systematic generalization w.r.t. semantics.
Syntax
(cid:88)
(cid:88)
(cid:88)
(cid:88)

Semantics Generalization

Perception

(cid:88)*

Dataset
SCAN [57]
gSCAN [81]
PCFG [43]
CFQ [50]
CURI [89]
COGS [51]
SQOOP [5]
Mathematics [83]
HALMA [91]
CLOSURE [4]
CLEVR [47]
HWF [61]
MNIST-Add [68]
HINT

Domain
synthetic
synthetic
synthetic
realistic
synthetic
realistic
synthetic
realistic
synthetic
synthetic
synthetic
realistic
realistic
realistic

Task Modality
SP
SP
SP
SP
IC
SP
QA
QA
NV
QA
QA
IC
IC
QA

text
i&t
text
text
image
text
i&t
text
image
i&t
i&t
image
image
image

(cid:88)

(cid:88)*

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)+

(cid:88)

systematic
systematic
systematic
systematic
systematic
systematic
systematic
systematic
systematic
systematic
i.i.d
i.i.d
i.i.d
systematic

Size
100K
300K
100K
239K
15K
30K
1M
2M
-
7K
865K
12K
-
1M

Benchmarks on Systematic Generalization Despite that various benchmarks [57, 43, 50, 4, 81,
51, 50] have boost the progresses on systematic generalization (see Tab. 1), most are built from
artiﬁcial domains with synthetic tasks and data, involving only one or two aspects of concept
learning and often mixing the generalization over syntax and semantics. In contrast, the proposed
HINT benchmark originates from a more realistic domain of arithmetic reasoning and requires a
joint learning of perception, syntax, and semantics. The clear deﬁnitions and boundaries of these

3

Figure 2: The data generation pipeline and operator semantics. We deﬁne −(a, b) as max(0, a − b) to avoid
negative results and ÷(a, b) as ceil(a ÷ b) to avoid too many zero results in the generated expressions.

meanings in HINT affords building test splits to evaluate the speciﬁc generalizations. Of note,
HINT possesses more complicated and holistic semantics, which avoids the undesirable bias towards
syntactic generalization in previous datasets. The task of the HINT benchmark is inspired by the HWF
dataset [61], but requires a full-spectrum learning of perception, syntax, and semantics. By going
beyond an i.i.d train/test split in Li et al. [61], HINT focuses on evaluating systematic generalization
across the different aspects of concepts.

Methods on Systematic Generalization New training regimes [56, 2, 1] and model architec-
tures [18, 82, 15, 33, 7] have been devised to capture systematic generalization. For example, Russin
et al. [82] extend a seq2seq model by separating semantic and syntactic information. Csordas et
al. [15] explore various conﬁgurations for Transformer to improve its systematic compositionality.
Andreas et al. [2] and Akyurek et al. [1] explore the data augmentation for compositional general-
ization. Additionally, several neural-symbolic methods with domain-speciﬁc designs [10, 72, 65]
achieve near perfect accuracy on prior systematic generalization datasets like SCAN [57]. However,
these neural-symbolic methods introduce some non-trivial domain-speciﬁc symbolic components
into the system and thus are not straightforward to transfer to other domains; the ﬂexibility and
transferability are questionable [32, 84]. In this work, we benchmark on HINT prevailing seq2seq
frameworks, including RNNs, Transformers, and GPT-3, which require minimal domain-speciﬁc
design and might be of broad interest to the community at present. We leave the exploration of more
sophisticated methods like data augmentation and neural-symbolic approaches for future work.

3 The HINT Dataset

In this section, we describe the details about the HINT benchmark, designed to study models’ capa-
bility of learning generalizable concepts at three different levels: perception, syntax, and semantics.

The Deﬁnitions of Perception, Syntax, and Semantics We ﬁrst deﬁne the perception, syntax,
and semantics in the domain of HINT. Perception denotes the mapping from image pixels to
meaningful patterns, e.g., mapping a handwritten expression to a symbolic sequence. Syntax denotes
the mechanism of how the concepts in one sample are combined with each other in a structured way,
e.g., parsing the symbolic sequence into a tree. The syntax can be represented by a phrase-structure
grammar in Tab. A2. Semantics denotes the functional meanings of these arithmetic concepts, e.g.,
what value ‘5’ represents and what value ‘+’ outputs given two arguments 1 and 1; see Fig. 2.

Of note, although these three levels have a clear boundary in their deﬁnition, a model does not
necessarily represent them by separate and individual modules. For example, an end-to-end neural
network trained on this domain will probably have mixed neurons and parameters for the three levels.
The notion of perception, syntax, and semantics only requires the models to capture these meanings
during evaluation, regardless of how the models ﬁnish the tasks, implicitly or explicitly.

Task The task of HINT is intuitive: predict the ﬁnal results of handwritten arithmetic expressions
in a weakly-supervised manner. I.e., only the ﬁnal results are given as supervision; all the symbolic
expressions, parse trees, and intermediate values are latent. Clearly, models must simultaneously
master perception, syntax, and semantics to successfully achieve this task.

Data Generation The data generation process follows three steps; see Fig. 2. First, we extract
handwritten images for each concept from CROHME [67], including digits 0 ∼ 9, operators
+, −, ×, ÷, and parentheses (, ). Second, we randomly sample preﬁx expressions and convert them

4

PrefixInfixHWResults×+328(3+2)×840−−53×525−3−5×2012÷(5×4)÷2×54operator semantics    +(a, b): a + b    −(a, b): max(0, a - b)    ×(a, b): a × b    ÷(a, b): ceil(a ÷ b)to inﬁx expressions with necessary parentheses based on the operator precedence; only single-digit
numbers are allowed. The symbolic expressions are fed into a solver to calculate the ﬁnal results.
Third, we randomly sample handwritten images for symbols in an expression and concatenate them
to construct the ﬁnal handwritten expression. We only keep the handwritten expressions as input and
the corresponding ﬁnal results as supervision; all intermediate results are discarded.

Full-Spectrum Systematic Generalization To rigorously evaluate systematical generalization of
the learned concepts, we replace the typical i.i.d. split with a carefully designed evaluation scheme.
We randomly split all handwritten images into train (75%), validation (5%), and test (20%).

First, for the training set, we limit the maximum number of operators to 10 and the maximum
intermediate values to 100:

Dtrain ⊂ Ttrain = {(x, y) : |x| ≤ 10, max(v) ≤ 100},
(1)
where x is the handwritten expression, |x| its number of operators, y the ﬁnal result, and v all the
intermediate values and the ﬁnal results. To ensure the diversity in the training set, we sample at most
100,000 unique expressions which have the same number of operators. To avoid the bias in the ﬁnal
results, we limit the maximum percentage of a certain result under 5%.

Next, we carefully curate the test set to evaluate different generalization capabilities (i.e., interpo-
lation and extrapolation) on different levels of meanings (i.e., perception, syntax, and semantics).
Speciﬁcally, the test set is composed of ﬁve subsets, formally deﬁned as:
Dtest = DI ∪ DSS ∪ DLS ∪ DSL ∪ DLL, where

(2)

DI ⊂ Dtrain,
DSS ⊂ Ttrain \ Dtrain,
DLS ⊂ {(x, y) : |x| > 10, max(v) ≤ 100},
DSL ⊂ {(x, y) : |x| ≤ 10, max(v) > 100},
DLL ⊂ {(x, y) : |x| > 10, max(v) > 100}.

generalization on perception only

interpolation on both syntax and semantics

extrapolation on syntax and interpolation on semantics

interpolation on syntax and extrapolation on semantics

extrapolation on both syntax and semantics

All subsets in the test set require generalization on perception since all images in the test set are
unseen in training. For the test set, we sample at most 1,000 unique expressions with the same
number of operators, and the ﬁnal results are also balanced. The maximum number of operators is set
up to 20, and the maximum intermediate value to 10,000. We also build a small validation set for
hyperparameter tuning during model development. See Appx. A for a detailed analysis.

Few-shot Learning and Generalization To further test whether models can learn new concepts
as quickly as humans, we build a few-shot learning split to learn six new concepts. This few-shot
learning split is used to test if the models pre-trained on the training set can quickly learn a new
concept by ﬁne-tuning on only a handful of examples involving the new concept. Here, “few-shot”
implies that the examples used to learn a new concept are much fewer than those of the training set,
but they are still more than how many humans require to learn a new concept. These six concepts
and y
have different means in terms of perception, syntax, and semantics: two new numbers (x
, representing
max and min), and two operators of precedence 2 (c
, representing arithmetic mean
and harmonic mean). The train, validation, and test splits are built by the same strategy as in the
full-spectrum generalization. Expressions are sampled to ensure that the corresponding new concept
appears at least once in the expression.

, representing 11 and 12, respectively), two operators of precedence 1 (a

and d

and b

4 Deep Sequence-to-Sequence Baselines

We benchmark deep sequence-to-sequence (seq2seq) frameworks [86] on HINT, since the task of
HINT can be naturally formulated as a seq2seq problem: The input is a handwritten expression,
segmented into a sequence of images by a sliding window, and the output is an integer, converted into
a sequence of digits. Fig. A3 illustrates applying the seq2seq framework to a HINT example.

4.1

Image Tokenizing and Embedding

The current seq2seq frameworks usually take as input a sequence of tokens. To tokenize a handwritten
expression, we ﬁrst resize it by making its height 32 and apply a sliding window of size 32 along

5

the horizontal axis to render a sequence of images. Next, each image in the sequence is encoded by
ResNet-18 [37], which has enough capacity for handling the visual variance in handwriting.

4.2 Encoder-Decoder Architectures

RNNs Recurrent neural networks (RNNs) have long been a dominant choice for sequence modeling
tasks, thus we test two popular RNNs in the literature: long short-term memory (LSTM) [40] and
gated recurrent units (GRU) [14]. Attention has often been used in RNNs; we evaluate each network
both with and without attention [3].

Transformers Since invented [88] for machine translation, Transformers have gradually replaced
recurrent or convolutional neural networks as the de facto choice for many sequence modeling
tasks [19, 79, 8]. However, prior work [17, 43, 51] suggests that the vanilla Transformer [88] fails
dramatically in many tasks requiring systematic generalization when the sequence lengths exceed
those observed during training. Recently, several simple tricks [15] have been demonstrated to
improve the generalization capability of Transformers; two of them work particularly well: (i) using
relative positional encoding [85, 16], and (ii) sharing weights across the blocks in the Transformer,
a.k.a.., Universal Transformer [17]. Therefore, we benchmark three variants of Transformer: the
vanilla Transformer, Transformer with relative positional encoding, and Universal Transformer with
relative positional encoding.

GPT-3 Since GPT-3 [8] was released, there have been heated discussions and different opinions
about the mathematical reasoning capability of pre-trained large language models.3 To systematically
and comprehensively evaluate GPT-3’s capability of arithmetic reasoning, we test GPT-3 on the
proposed HINT benchmark using symbolic expressions as input. Since all tokens of HINT are in the
vocabulary of GPT-3, we directly evaluate GPT-3 via zero-shot prompting using the OpenAI API
4. We construct the prompt in the following form: “Q: What is <Expression>? A: The answer is,”
similar to the practice in Brown et al. [8] but with more complex expressions.

Very recently, chain of thought prompting (CoT) [90] is extended to the zero-shot setting [53] by
adding a simple prompt, “Let’s think step by step,” to facilitate step-by-step thinking before answering
each question. Zero-shot CoT amazingly outperforms the standard zero-shot prompting by a large
margin in a variety of reasoning tasks. Therefore, we also apply zero-shot CoT prompting to evaluate
GPT-3 on HINT; see Appx. B.2 for the details of zero-shot CoT.

4.3 Training and Evaluation

Training All models are trained by the Adam optimizer; the gradients larger than 5.0 are clipped.
Dropout is applied to each recurrent layer of RNNs and each sub-layer of Transformers: both the
multi-head attention layers and the feedforward layers. For zero-shot experiments on GPT-3, no
training is required; instead, we randomly select 100 samples for each test subset and feed them to
GPT-3 by zero-shot or zero-shot-CoT prompting.

Hyperparameter Tuning To make the results reliable, we conduct a thorough hyperparameter
tuning w.r.t. the number of layers in the encoder and the decoder, the dimension of the token
embedding, the number of hidden units per layer, the number of attention heads in Transformers, the
dropout ratio, and the learning rate. Please refer to Tab. A3 for details.

Evaluation Metric We report the accuracy of ﬁnal results. A predicted result is considered correct
when it exactly equals to the ground-truth answer.

6

Table 2: The accuracy on the test set using image inputs. All models are jointly trained with a randomly
initialized ResNet-18. Reported accuracy (%) is the median and standard deviation of 5 runs. “rel.” denotes
Transformer with relative positional encoding, and “uni.” denotes Universal Transformer.

Model

GRU

LSTM

Transformer

Variant
w/o att
w/ att
w/o att
w/ att
vanilla
rel.
rel. uni.

I
61.3±1.4
66.7±2.0
80.0±5.7
83.9±0.9
20.9±0.4
86.2±0.9
88.4±1.3

SS
53.3±1.7
58.7±2.2
76.2±7.4
79.7±0.8
9.3±0.2
83.1±1.3
86.0±1.3

LS
30.5±1.2
33.1±2.7
55.7±8.2
62.0±2.5
5.7±0.3
60.1±2.3
62.5±4.1

SL
9.2±0.2
9.4±0.3
10.9±0.6
11.2±0.1
1.5±0.3
10.9±0.2
10.9±0.2

LL
11.9±0.5
12.8±1.0
19.8±2.6
21.0±0.8
2.9±0.5
19.4±0.5
19.0±1.0

Avg.
33.2±0.9
35.9±1.6
48.6±4.9
51.5±1.0
8.3±0.3
51.7±1.0
53.1±1.6

Table 3: The accuracy on the test set using symbol inputs.

Model

GRU

LSTM

Transformer

GPT-3

Variant
w/o att
w/ att
w/o att
w/ att
vanilla
rel.
rel. uni.
0-shot
0-shot-CoT

I
74.9±1.6
76.2±0.6
84.3±5.2
92.9±1.4
93.9±0.3
96.6±0.3
98.0±0.3
19.0
42.0

SS
68.1±0.5
69.5±0.6
79.6±6.0
90.9±1.1
91.0±0.5
95.1±0.4
96.8±0.6
9.0
36.0

LS
42.1±1.9
42.8±1.5
63.7±6.1
74.9±1.5
33.2±1.2
72.1±1.5
78.2±2.9
3.0
5.0

SL
10.5±0.2
10.5±0.2
11.7±0.3
12.1±0.2
11.5±0.1
11.8±0.2
11.7±0.3
10.0
49.0

LL
14.0±0.8
15.1±1.2
22.1±1.4
24.3±0.3
11.5±0.7
22.3±0.6
22.4±1.1
2.0
6.0

Avg.
41.3±0.6
42.5±0.7
52.3±3.8
58.9±0.7
47.4±0.4
59.4±0.5
61.5±0.9
8.6
27.6

5 Experimental Results

5.1

Joint Learning of Perception, Syntax, and Semantics

Tabs. 2 and 3 summarize the results of all models on HINT using image inputs and symbol inputs,
respectively. Among all models, Universal Transformer with relative positional encoding (“Trans-
former rel. uni.”) achieves the best average accuracy on the test set. Taking a closer look at the results,
we draw the following observations and insights:

• Models achieve high accuracy on the subset I. Particularly, Transformer rel. uni. using image
inputs obtains an accuracy of 88.4%. The test subset I shares the symbolic expressions with training
and has different handwritten images for symbols. This indicates that Transformers and RNNs,
jointly trained with ResNet-18, have strong generalization over perception. As shown in Fig. A4,
the model forms meaningful clusters for each concept and captures syntactic roles to some extent
although no direct supervision on perception.

• Transformers achieve high accuracy on the subset SS. The expressions in SS come from the
same length and value distribution as training. This result indicates that Transformers demonstrate
strong interpolation over syntax and semantics.

• The accuracy of Transformer rel. uni. on LS is signiﬁcantly lower than its accuracy on SS
or I (see Tab. 3). The very same model produces perfect accuracy on the length cutoff splits of
SCAN [15]. This result may be explained by the difference between the syntax of HINT and SCAN,
shown in Tab. A2: The expressions in HINT can have longer-range dependency and larger tree
depth than the commands in SCAN. This observation indicates that current Transformers, which
have ﬁnite depth, cannot fully capture the syntax with long dependency and large depth.

• Transformer with relative positional encoding achieves similar performance on I and SS as
the vanilla Transformer with absolute positional encoding, yet relative positional encoding
boosts Transformer’s accuracy on LS over twice (see Tab. 3). This contradiction indicates that
relative positional encoding is critical for Transformer to generalize to long expressions. Sharing
weights between the layers using the Universal Transformer can further improve the performance.
• Models behave clumsily on the subsets SL and LL. The accuracy on SL and LL is much lower
than that on I and SS. All models have nearly zero accuracy on samples whose answers are over
100 (the maximum ﬁnal result in the training set). This result indicates that both RNNs and
Transformers have little capability to extrapolate to larger numbers than those in the training set.

3Can GPT-3 do math? https://www.youtube.com/watch?v=TMxAbNAVrzI
4https://openai.com/api/

7

Figure 3: Scaling trends of the accuracy on the test
subset LL w.r.t. model size and dataset size when
training Transformer rel. uni. with symbol inputs.

Figure 4: The performance of Transformer rel. uni.
when using varied maximum operators in the few-
shot learning.

• While GPT-3 with zero-shot prompting has poor performance, chain of thought (CoT)
prompting signiﬁcantly improves the accuracy. Notably, GPT-3 with zero-shot CoT obtains an
accuracy of 49.0% on SL, better than other ﬁne-tuned models. We believe the reason is that GPT-3
is pre-trained on data involving larger numbers, and CoT enhances the reasoning process. However,
GPT-3 performs poorly on long expressions in LS and LL even with CoT prompting.

Summary Taking together, we observe a large space for improvement on HINT. Even the best
model, Universal Transformer with relative positional encoding, can only achieve an accuracy of
54.3% on HINT, while the same model obtains nearly perfect accuracy on previous datasets of
systematic generalization like SCAN [15]. The challenges of HINT lie in that it requires a joint
learning and generalization of perception, syntax, and semantics: The perception has large variance
from realistic handwriting, the syntax supports long dependency between symbols, and the semantic
complexity is well beyond the capability of the state-of-the-art models.

Scaling Laws on HINT Since HINT can produce inﬁnite data for training, one might wonder if
simply scaling up the dataset and the model size can solve the problem, as done in NLP tasks [49, 38].
Empirically, Fig. 3 plots the scaling trend of the test accuracy w.r.t. the model size and the number
of training samples. Models of various sizes are constructed by varying the hidden dimension, the
embedding dimension, and the number of the attention heads. Training sets of various sizes are
constructed by randomly sampling from the original training set. Assuming a log-linear scaling trend,
we need to train a model of 1033 parameters on 1015 examples to achieve 90% accuracy on the test
subset LL, which is impractical. Therefore, efﬁcient architecture and training algorithm are required
to improve the model’s capability of extrapolating over syntax and semantics.

5.2 Few-shot Learning and Generalization

In this section, we ﬁne-tune the top two models on six new concepts; Tab. 4 tabulates the results.
Transformer rel. uni. outperforms LSTM w/ attn across all concepts by a large margin, which is
over six times their performance gap in Tab. 2. This discrepancy indicates that with limited data,
Transformer can learn the new concepts much better than LSTM.

Table 4: The few-shot learning performance of the top two models: LSTM w/ attn (left) and Transformer
rel. uni. (right). In “Syntax,” “Number,” “Op1,” and “Op2” denote that the concept is appended to the
corresponding production rule of the grammar in Tab. A2. In “Semantics,” i and j are the inputs to the operator.
Reported results are the median of 5 runs.

Concept Perception

x
y
a
b
c
d
Overall

-

Syntax
Number
Number
Op1
Op1
Op2
Op2
-

Semantics
11
12
max(i, j)
min(i, j)
(i + j)/2
2ij/(i + j)
-

I
87.8/89.2
64.5/83.8
71.8/84.4
73.4/77.1
61.5/59.2
59.2/62.8
69.7/76.1

SS
47.3/80.2
39.1/74.8
44.2/72.0
29.9/59.1
19.6/34.0
22.7/39.0
33.8/59.9

LS
42.8/58.6
38.5/54.0
29.7/48.9
27.4/39.4
15.2/24.4
20.2/27.0
29.0/42.0

SL
10.8/12.2
11.6/13.8
7.9/8.4
7.4/16.8
4.5/6.1
7.2/9.2
8.2/11.1

LL
16.4/19.3
18.9/22.4
11.1/12.3
12.7/17.1
6.5/9.4
8.9/10.7
12.4/15.2

Avg.
42.8/52.8
35.4/50.7
33.8/46.4
31.1/42.6
21.9/27.3
24.7/30.4
31.6/41.7

8

0.0M0.2M0.4M0.6M0.8M1.0MData Size5101520Test Accuracy (LL)0M5M10M15M20M25M30M35MModel SizeY = 7.42 * log D - 20.91Y = 2.64 * log M + 2.860246810Operators20253035404550Test Accuracy (Avg.)(0, 26.5)(2, 24.1)xyabcdFig. 4 shows the test accuracy of Transformer rel. uni. while using varied maximum operators for
training. Generally, the more data and the longer expressions used for training, the better performance
the model can achieve. One test case for learning new numbers (“xy”) is (0, 26.5), where the model
is exposed to the primitive concept only in the training phrase and required to generalize to complex
compositions in test. According to the classic thought experiments [30], this is simple for humans: If
you know the meaning of “1,” “1 + 1,” and “x,” you should also understand what “1 + x” means.
A similar test case for learning new operators (“abcd”) is (2, 24.1), since expressions containing at
least two operators are necessary for capturing the syntax of a new operator. Transformer performs
poorly in these two types of tests, indicating that it is still far from the human-level generalization.

6 Discussions: Conclusions and Limitations

In this paper, we take inspiration from arithmetic and present a new challenge for the machine
learning community, HINT, which serves as a minimal yet complete benchmark towards studying the
full-spectrum systematic generalization of concept learning w.r.t. perception, syntax, and semantics.
HINT is inherently more challenging than previous datasets due to its large perceptual variance in
realistic handwriting, complex syntax, and sophisticated semantics. We benchmark on HINT the
state-of-the-art seq2seq models, including RNNs, Transformers, and GPT-3, and the results point out
their ineptitude of extrapolation over syntax and semantics. The scaling trends of test accuracy w.r.t.
dataset size and model size indicate that it is impractical to solve HINT by simply scaling up model
and dataset. We believe that the HINT dataset together with our experimental ﬁndings might inspire
new advancement for systematic generalization, especially extrapolation over syntax and semantics.

Limitations and Future Work Despite a large visual variance, the handwritten expressions are
relatively simple in terms of spatial locations. It would be more interesting if we can further increase
the perceptual complexity w.r.t. spatial relations like natural images [62]. Although syntax and
semantics in HINT are already more complicated than previous datasets, they are still context-free.
Extending our ﬁndings to context-dependent syntax and semantics would be of practical beneﬁts
since they are very common in natural languages, e.g., a word might have different syntactic roles or
semantic meanings in different contexts.

In terms of model development on HINT, our results indicate that current seq2seq models, including
Transformers, still lack the ability to extract the systematic rules for both syntax and semantics
from the training data. How to improve the systematic generalization of Transformers, especially
extrapolation over semantics, is an important future direction. We also plan to explore more sophis-
ticated methods, such as meta-learning [56], data augmentation [2, 1], Edge Transformer [7], and
Neural-Symbolic Stack Machines [10]. Furthermore, it would be helpful to understand the systematic
generalization of large language models by evaluating them in few-shot or ﬁne-tuning settings.

9

References

[1] Ekin Akyürek, Afra Feyza Akyürek, and Jacob Andreas. Learning to recombine and resample
data for compositional generalization. In International Conference on Learning Representations
(ICLR), 2020. 2, 4, 9

[2] Jacob Andreas. Good-enough compositional data augmentation. In Annual Meeting of the

Association for Computational Linguistics (ACL), 2020. 2, 4, 9

[3] Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. Neural machine translation by
jointly learning to align and translate. In International Conference on Learning Representations
(ICLR), 2015. 6, A4

[4] Dzmitry Bahdanau, Harm de Vries, Timothy J O’Donnell, Shikhar Murty, Philippe Beaudoin,
Yoshua Bengio, and Aaron Courville. Closure: Assessing systematic generalization of clevr
models. arXiv preprint arXiv:1912.05783, 2019. 2, 3

[5] Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen, Harm de Vries,
and Aaron Courville. Systematic generalization: what is required and can it be learned? In
International Conference on Learning Representations (ICLR), 2018. 3

[6] Matej Balog, Alexander L Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow.
Deepcoder: Learning to write programs. In International Conference on Learning Representa-
tions (ICLR), 2017. 3

[7] Leon Bergen, Timothy O’Donnell, and Dzmitry Bahdanau. Systematic generalization with edge
transformers. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 2, 4, 9
[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS), 2020. 2,
6, A4, A5

[9] Danqi Chen and Christopher D Manning. A fast and accurate dependency parser using neural
In Annual Conference on Empirical Methods in Natural Language Processing

networks.
(EMNLP), 2014. 3

[10] Xinyun Chen, Chen Liang, Adams Wei Yu, Dawn Song, and Denny Zhou. Compositional gen-
eralization via neural-symbolic stack machines. In Advances in Neural Information Processing
Systems (NeurIPS), 2020. 4, 9

[11] Yixin Chen, Siyuan Huang, Tao Yuan, Siyuan Qi, Yixin Zhu, and Song-Chun Zhu. Holistic++
scene understanding: Single-view 3d holistic scene parsing and human pose estimation with
human-object interaction and physical commonsense. In International Conference on Computer
Vision (ICCV), 2019. 3

[12] Noam Chomsky. Syntactic structures. In Syntactic Structures. De Gruyter Mouton, 1957. 2
[13] Noam Chomsky. Aspects of the Theory of Syntax. MIT press, 1965. 2
[14] Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation
of gated recurrent neural networks on sequence modeling. In NIPS Workshop on Deep Learning,
2014. 2, 6, A4

[15] Róbert Csordás, Kazuki Irie, and Juergen Schmidhuber. The devil is in the detail: Simple
tricks improve systematic generalization of transformers. In Annual Conference on Empirical
Methods in Natural Language Processing (EMNLP), 2021. 2, 3, 4, 6, 7, 8, A1, A4

[16] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov.
Transformer-xl: Attentive language models beyond a ﬁxed-length context. In Annual Meeting
of the Association for Computational Linguistics (ACL), 2019. 2, 6

[17] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Uni-
versal transformers. In International Conference on Learning Representations (ICLR), 2018. 2,
6

[18] Roberto Dessì and Marco Baroni. Cnns found to jump around more skillfully than rnns:
Compositional generalization in seq2seq convolutional networks. In Annual Meeting of the
Association for Computational Linguistics (ACL), 2019. 2, 4

[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. In North American Chapter of
the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),
2019. 6

10

[20] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed,
and Pushmeet Kohli. Robustﬁll: Neural program learning under noisy i/o. In International
Conference on Machine Learning (ICML), 2017. 3

[21] Mark Edmonds, Feng Gao, Hangxin Liu, Xu Xie, Siyuan Qi, Brandon Rothrock, Yixin Zhu,
Ying Nian Wu, Hongjing Lu, and Song-Chun Zhu. A tale of two explanations: Enhancing
human trust by explaining robot behavior. Science Robotics, 4(37), 2019. 3

[22] Mark Edmonds, Feng Kubricht, James, Colin Summers, Yixin Zhu, Brandon Rothrock, Song-
Chun Zhu, and Hongjing Lu. Human causal transfer: Challenges for deep reinforcement
learning. In Annual Meeting of the Cognitive Science Society (CogSci), 2018. 3

[23] Mark Edmonds, Xiaojian Ma, Siyuan Qi, Yixin Zhu, Hongjing Lu, and Song-Chun Zhu. Theory-
based causal transfer: Integrating instance-level induction and abstract-level structure learning.
In AAAI Conference on Artiﬁcial Intelligence (AAAI), 2020. 3

[24] Mark Edmonds, Siyuan Qi, Yixin Zhu, James Kubricht, Song-Chun Zhu, and Hongjing Lu.
Decomposing human causal learning: Bottom-up associative learning and top-down schema
reasoning. In Annual Meeting of the Cognitive Science Society (CogSci), 2019. 3

[25] Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, and Joshua B Tenenbaum. Learning
In Advances in Neural Information

to infer graphics programs from hand-drawn images.
Processing Systems (NeurIPS), 2018. 3

[26] Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lucas Morales,
Luke Hewitt, Armando Solar-Lezama, and Joshua B Tenenbaum. Dreamcoder: Growing
generalizable, interpretable knowledge with wake-sleep bayesian program learning. arXiv
preprint arXiv:2006.08381, 2020. 3

[27] Kevin M Ellis, Lucas E Morales, Mathias Sablé-Meyer, Armando Solar Lezama, and Joshua B
Tenenbaum. Library learning for neurally-guided bayesian program induction. In Advances in
Neural Information Processing Systems (NeurIPS), 2018. 3

[28] Richard Evans and Edward Grefenstette. Learning explanatory rules from noisy data. Journal

of Artiﬁcial Intelligence Research, 61:1–64, 2018. 3

[29] Chaz Firestone and Brian J Scholl. Cognition does not affect perception: Evaluating the

evidence for “top-down” effects. Behavioral and Brain Sciences, 39, 2016. 1

[30] Jerry A Fodor. The language of thought. Harvard university press, 1975. 9
[31] Jerry A Fodor, Zenon W Pylyshyn, et al. Connectionism and cognitive architecture: A critical

analysis. Cognition, 28(1-2):3–71, 1988. 1

[32] Daniel Furrer, Marc van Zee, Nathan Scales, and Nathanael Schärli. Compositional gen-
eralization in semantic parsing: Pre-training vs. specialized architectures. arXiv preprint
arXiv:2007.08970, 2020. 4

[33] Jonathan Gordon, David Lopez-Paz, Marco Baroni, and Diane Bouchacourt. Permutation
equivariant models for compositional generalization in language. In International Conference
on Learning Representations (ICLR), 2019. 2, 4

[34] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep
In International Conference on Acoustics, Speech and Signal

recurrent neural networks.
Processing (ICASSP), 2013. 3

[35] Abhinav Gupta, Praveen Srinivasan, Jianbo Shi, and Larry S Davis. Understanding videos,
constructing plots learning a visually grounded storyline model from annotated videos. In
Conference on Computer Vision and Pattern Recognition (CVPR), 2009. 3

[36] Muzhi Han, Zeyu Zhang, Ziyuan Jiao, Xu Xie, Yixin Zhu, Song-Chun Zhu, and Hangxin
Liu. Reconstructing interactive 3d scenes by panoptic mapping and cad model alignments. In
International Conference on Robotics and Automation (ICRA), 2021. 3

[37] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 3, 6,
A4

[38] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson,
Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive
generative modeling. arXiv preprint arXiv:2010.14701, 2020. 3, 8

[39] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep
Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep
neural networks for acoustic modeling in speech recognition. Signal Processing Magazine,
29(6):82–97, 2012. 3

11

[40] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation,

9(8):1735–1780, 1997. 2, 6, A4

[41] Siyuan Huang, Siyuan Qi, Yinxue Xiao, Yixin Zhu, Ying Nian Wu, and Song-Chun Zhu. Coop-
erative holistic scene understanding: Unifying 3d object, layout, and camera pose estimation.
In Advances in Neural Information Processing Systems (NeurIPS), 2018. 3

[42] Siyuan Huang, Siyuan Qi, Yixin Zhu, Yinxue Xiao, Yuanlu Xu, and Song-Chun Zhu. Holistic
3d scene parsing and reconstruction from a single rgb image. In European Conference on
Computer Vision (ECCV), 2018. 3

[43] Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. Compositionality decomposed:
how do neural networks generalise? Journal of Artiﬁcial Intelligence Research, 67:757–795,
2020. 2, 3, 6

[44] baoxiong Jia, Yixin Chen, Siyuan Huang, Yixin Zhu, and Song-Chun Zhu. Lemma: A multi-
view dataset for learning multi-agent multi-task activities. In European Conference on Computer
Vision (ECCV), 2020. 3

[45] Chenfanfu Jiang, Siyuan Qi, Yixin Zhu, Siyuan Huang, Jenny Lin, Lap-Fai Yu, Demetri
Terzopoulos, and Song-Chun Zhu. Conﬁgurable 3d scene synthesis and 2d image rendering
with per-pixel ground truth using stochastic grammars. International Journal of Computer
Vision (IJCV), 126(9):920–941, 2018. 3

[46] Yong Jiang, Wenjuan Han, and Kewei Tu. Unsupervised neural dependency parsing. In Annual

Meeting of the Association for Computational Linguistics (ACL), 2016. 3

[47] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick,
and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary
visual reasoning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 3

[48] Daniel Kahneman. Thinking, fast and slow. Macmillan, 2011. A5
[49] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models. arXiv preprint arXiv:2001.08361, 2020. 3, 8

[50] Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashu-
bin, Nikola Momchev, Danila Sinopalnikov, Lukasz Staﬁniak, Tibor Tihon, et al. Measuring
compositional generalization: A comprehensive method on realistic data. In International
Conference on Learning Representations (ICLR), 2020. 2, 3

[51] Najoung Kim and Tal Linzen. Cogs: A compositional generalization challenge based on
semantic interpretation. In Annual Conference on Empirical Methods in Natural Language
Processing (EMNLP), 2020. 2, 3, 6

[52] Nikita Kitaev and Dan Klein. Constituency parsing with a self-attentive encoder. In Annual

Meeting of the Association for Computational Linguistics (ACL), 2018. 3

[53] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large
language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022. 6, A5
[54] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep con-
volutional neural networks. In Advances in Neural Information Processing Systems (NeurIPS),
2012. 3

[55] Tejas D Kulkarni, Pushmeet Kohli, Joshua B Tenenbaum, and Vikash Mansinghka. Picture: A
probabilistic programming language for scene perception. In Conference on Computer Vision
and Pattern Recognition (CVPR), 2015. 3

[56] Brenden M Lake. Compositional generalization through meta sequence-to-sequence learning.

In Advances in Neural Information Processing Systems (NeurIPS), 2019. 2, 4, 9

[57] Brenden M. Lake and Marco Baroni. Generalization without systematicity: On the compo-
sitional skills of sequence-to-sequence recurrent networks. In International Conference on
Machine Learning (ICML), 2018. 2, 3, 4

[58] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept
learning through probabilistic program induction. Science, 350(6266):1332–1338, 2015. 3
[59] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building
machines that learn and think like people. Behavioral and Brain Sciences, 40, 2017. 2
[60] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444,

2015. 3

[61] Qing Li, Siyuan Huang, Yining Hong, Yixin Chen, Ying Nian Wu, and Song-Chun Zhu. Closed
loop neural-symbolic learning via integrating neural perception, grammar parsing, and symbolic
reasoning. In International Conference on Machine Learning (ICML), 2020. 3, 4

12

[62] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European
Conference on Computer Vision (ECCV), 2014. 9

[63] Hangxin Liu, Chi Zhang, Yixin Zhu, Chenfanfu Jiang, and Song-Chun Zhu. Mirroring without
overimitation: Learning functionally equivalent manipulation actions. In AAAI Conference on
Artiﬁcial Intelligence (AAAI), 2019. 3

[64] Hangxin Liu, Yaofang Zhang, Wenwen Si, Xu Xie, Yixin Zhu, and Song-Chun Zhu. Interactive
robot knowledge patching using augmented reality. In International Conference on Robotics
and Automation (ICRA), 2018. 3

[65] Qian Liu, Shengnan An, Jian-Guang Lou, Bei Chen, Zeqi Lin, Yan Gao, Bin Zhou, Nanning
Zheng, and Dongmei Zhang. Compositional generalization by learning analytical expressions.
In Advances in Neural Information Processing Systems (NeurIPS), 2020. 4

[66] John W Lloyd. Foundations of logic programming. Springer Science &amp; Business Media,

2012. 3

[67] Mahshad Mahdavi, Richard Zanibbi, Harold Mouchere, Christian Viard-Gaudin, and Utpal
Garain. Icdar 2019 crohme+ tfd: Competition on recognition of handwritten mathematical
expressions and typeset formula detection. In International Conference on Document Analysis
and Recognition (ICDAR), 2019. 4, A1

[68] Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc
De Raedt. Deepproblog: Neural probabilistic logic programming. In Advances in Neural
Information Processing Systems (NeurIPS), 2018. 3

[69] Richard Montague. Universal grammar. Theoria, 36(3):373–398, 1970. 2
[70] Stephen Muggleton and Luc De Raedt. Inductive logic programming: Theory and methods.

The Journal of Logic Programming, 19:629–679, 1994. 3

[71] Benjamin Newman, John Hewitt, Percy Liang, and Christopher D Manning. The eos decision
and length extrapolation. In BlackboxNLP Workshop on Analyzing and Interpreting Neural
Networks for NLP, 2020. 2

[72] Maxwell Nye, Armando Solar-Lezama, Josh Tenenbaum, and Brenden M Lake. Learning
compositional rules via neural program synthesis. In Advances in Neural Information Processing
Systems (NeurIPS), 2020. 4

[73] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk,
and Quoc V Le. Specaugment: A simple data augmentation method for automatic speech
recognition. In Interspeech, 2019. 3

[74] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
style, high-performance deep learning library. In Advances in Neural Information Processing
Systems (NeurIPS), 2019. A4

[75] Zenon W Pylyshyn. Computation and cognition: Towards a foundation for cognitive science.

Cambridge, Ma: MIT Press, 1984. 1

[76] Siyuan Qi, Baoxiong Jia, Siyuan Huang, Ping Wei, and Song-Chun Zhu. A generalized earley
parser for human activity parsing and prediction. Transactions on Pattern Analysis and Machine
Intelligence (TPAMI), 43(8):2538–2554, 2020. 3

[77] Siyuan Qi, Baoxiong Jia, and Song-Chun Zhu. Generalized earley parser: Bridging symbolic
grammars and sequence data for future prediction. In International Conference on Machine
Learning (ICML), 2018. 3

[78] Siyuan Qi, Yixin Zhu, Siyuan Huang, Chenfanfu Jiang, and Song-Chun Zhu. Human-centric
indoor scene synthesis using stochastic grammar. In Conference on Computer Vision and
Pattern Recognition (CVPR), 2018. 3

[79] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.

Language models are unsupervised multitask learners. OpenAI Blog, 2019. 6

[80] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song,
John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language
models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446,
2021. A5

[81] Laura Ruis, Jacob Andreas, Marco Baroni, Diane Bouchacourt, and Brenden M Lake. A
benchmark for systematic generalization in grounded language understanding. Advances in
Neural Information Processing Systems (NeurIPS), 2020. 2, 3

13

[82] Jake Russin, Jason Jo, Randall C O’Reilly, and Yoshua Bengio. Compositional generalization
in a deep seq2seq model by separating syntax and semantics. arXiv preprint arXiv:1904.09708,
2019. 2, 4

[83] David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical
reasoning abilities of neural models. In International Conference on Learning Representations
(ICLR), 2018. 3

[84] Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. Compositional
generalization and natural language variation: Can a semantic parsing approach handle both?
In Annual Meeting of the Association for Computational Linguistics (ACL), 2021. 4

[85] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position rep-
resentations. In North American Chapter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT), 2018. 2, 6

[86] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural
networks. In Advances in Neural Information Processing Systems (NeurIPS), 2014. 5, A4
[87] Zhuowen Tu, Xiangrong Chen, Alan L Yuille, and Song-Chun Zhu. Image parsing: Unifying
segmentation, detection, and recognition. International Journal of Computer Vision (IJCV),
63(2):113–140, 2005. 3

[88] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information
Processing Systems (NeurIPS), 2017. 2, 6

[89] Ramakrishna Vedantam, Arthur Szlam, Maximillian Nickel, Ari Morcos, and Brenden M
Lake. Curi: A benchmark for productive concept learning under uncertainty. In International
Conference on Machine Learning (ICML), 2021. 3

[90] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny
Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint
arXiv:2201.11903, 2022. 6, A5

[91] Sirui Xie, Xiaojian Ma, Peiyu Yu, Yixin Zhu, Ying Nian Wu, and Song-Chun Zhu. Halma:
Humanlike abstraction learning meets affordance in rapid problem solving. arXiv preprint
arXiv:2102.11344, 2021. 3

[92] Xu Xie, Hangxin Liu, Mark Edmonds, Feng Gao, Siyuan Qi, Yixin Zhu, Brandon Rothrock,
and Song-Chun Zhu. Unsupervised learning of hierarchical models for hand-object interactions.
In International Conference on Robotics and Automation (ICRA), 2018. 3

[93] Tao Yuan, Hangxin Liu, Lifeng Fan, Zilong Zheng, Tao Gao, Yixin Zhu, and Song-Chun
Zhu. Joint inference of states, robot knowledge, and human (false-)beliefs. In International
Conference on Robotics and Automation (ICRA), 2020. 3

[94] Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Raven: A dataset for
relational and analogical visual reasoning. In Conference on Computer Vision and Pattern
Recognition (CVPR), 2019. 3

[95] Chi Zhang, Baoxiong Jia, Feng Gao, Yixin Zhu, Hongjing Lu, and Song-Chun Zhu. Learning
perceptual inference by contrasting. In Advances in Neural Information Processing Systems
(NeurIPS), 2019. 3

[96] Wenhe Zhang, Chi Zhang, Yixin Zhu, and Song-Chun Zhu. Machine number sense: A dataset
of visual arithmetic problems for abstract and relational reasoning. In AAAI Conference on
Artiﬁcial Intelligence (AAAI), 2020. 3

[97] Zhenliang Zhang, Yixin Zhu, and Song-Chun Zhu. Graph-based hierarchical knowledge
representation for robot task transfer from virtual to physical world. In International Conference
on Intelligent Robots and Systems (IROS), 2020. 3

[98] Yibiao Zhao and Song-Chun Zhu. Image parsing with stochastic scene grammar. In Advances

in Neural Information Processing Systems (NeurIPS), 2011. 3

[99] Song-Chun Zhu, David Mumford, et al. A stochastic grammar of images. Foundations and

Trends® in Computer Graphics and Vision, 2(4):259–362, 2007. 3

14

A Dataset Statistics

The handwritten images for each arithmetic concept originate from the handwritten math symbols
dataset1 hosted on Kaggle under the “CC0: Public Domain” license, parsed and extracted from the
Competition on Recognition of Online Handwritten Mathematical Expressions (CROHME) [67]2.
We further clean the dataset by removing duplicate images, resulting in statistics shown in Fig. A1.

To demonstrate the validity of the HINT dataset as a benchmark for systematic generalization, we
conduct a detailed analysis of the collected data. Tab. A1 shows the size of each split in HINT, and
Tab. A2 shows a comparison between the grammars of HINT and SCAN.

Table A1: Dataset size. The ﬁrst row is the main split of HINT and the rest are the few-shot learning split. As
advocated by Csordas et al. [15], the validation set also contains ﬁve generalization subsets for model selection.

Split

main
x
y
a
b
c
d

Train

Validation

998000
1100
1100
1000
1000
1000
1000

4698
491
493
470
470
470
470

Total
46620
4900
4900
4700
4700
4700
4700

I
9980
1100
1100
1000
1000
1000
1000

Test

SS
8000
900
900
900
900
900
900

LS
10000
1000
1000
1000
1000
1000
1000

SL
8640
900
900
800
800
800
800

LL
10000
1000
1000
1000
1000
1000
1000

Table A2: The phrase-structure grammars for HINT and SCAN. While the grammars of both HINT and
SCAN can generate inﬁnite examples, HINT produces examples with larger depth and longer dependency due to
the parentheses; the expression inside parentheses can be arbitrarily long. Speciﬁcally, the maximum depth and
dependency range in SCAN are 6 and 4, respectively; the maximum length generated by the non-terminal “S” in
the grammar of SCAN is 4.

HINT

SCAN

T = {Expression, Term, Factor, Number}
Start symbol: Expression
Σ = {+, −, ×, ÷, 0, 1, ..., 9, (, )}
R = {

Expression → Term
Expression → Expression Op1 Term
Op1 → + | −
Term → Factor
Term → Term Op2 Factor
Op2 → × | ÷
Factor → Number
Factor → ( Expression )
Number → 0|1|2|3...|9 }

T= {C, S, V, D, U}
Start symbol: C
Σ = {walk, look, run, jump, turn, left, right,
around, opposite, and after, twice, thrice}
R = {

C → S | S and S | S after S
S → V | V twice | V thrice
V → D[1] opposite D[2]
V → D[1] around D[2]
V → D | U
D → U left | U right
D → turn left | turn right
U → walk | look | run | jump }

For each split, we plot the frequency distributions of various aspects, including symbol, number of
operators, expression length, tree depth, maximum dependency range, and result, as shown in Fig. A2.
The symbol distributions are similar across different splits, and the Kullback–Leibler divergence
between train and test is low (0.0055). The digits and operators are approximately equally distributed,
except for the test-SL split. The test-SL split has a relatively higher portion of multiplication (‘*’)
since generating large numbers generally requires more multiplication for short expressions.

The test set’s result distributions differ from the train set. All results in train set are smaller than 100
as desired; about half are in [0, 10). In comparison, 29% of the results in test set are larger than 100.

Several properties of an input expression, including length, number of operators, tree depth, and
maximum dependency range, are indicators of the difﬁculty of calculating the expression. We plot
the frequency distributions w.r.t. these input properties in Fig. A2. These distributions demonstrate
signiﬁcant differences between train and test.

1https://www.kaggle.com/datasets/xainano/handwrittenmathsymbols
2https://www.cs.rit.edu/~crohme2019/

A1

Figure A1: The number of handwritten images for each symbol. There are 82 arithmetic symbols (top 50 are
shown here) and 83,501 images in total. We use the handwritten images for digits 0 ∼ 9, operators +, −, ×, ÷,
and parentheses (, ) in this work; others are for potential future use.

A2

010002000300040005000600012-+x()=a3Nyb0sqrt4czdi5f6Tsin798pRksumeinttimes,McosthetapiGvinftyalphaHS|uqj632762106022544353333986397836402724246924031895183018101649164111941075106210271008848812795783753742731676674666642616606600596576574543498409391386383369355348321308303Figure A2: The frequency distributions w.r.t. various aspects, including symbol, number of operators,
expression length, tree depth, maximum dependency range, and result.

A3

0123456789+-*/()Symbol0.000.020.040.060.080.100.120.140.16Frequencytraintesttest-Itest-SStest-LStest-SLtest-LL01234567891011121314151617181920Number of Operators0.000.020.040.060.080.100.12Frequency13579111315171921232527293133353739414345474951535557596163656769Expression Length0.000.020.040.060.08Frequency123456789101112Tree Depth0.000.050.100.150.200.250.30Frequency01234567891011121314151617181920212223242526272829Maximum Dependency0.000.020.040.060.080.100.120.14Frequency0102030405060708090100200300400500600700800900100010000Result0.00.10.20.30.40.5FrequencyDataset License The HINT dataset is licensed under CC BY-NC 4.03 and hosted on the project
website: https://liqing-ustc.github.io/HINT. The authors bear all responsibility in
case of violation of rights, etc.

B Baseline Details

We benchmark deep sequence-to-sequence (seq2seq) frameworks [86] on HINT. Fig. A3 illustrates a
HINT example applied by the seq2seq framework. All models are implemented in PyTorch [74].

Figure A3: The seq2seq framework applied to an example in HINT. The symbols <SOS> and <EOS>
represent the start-of-sentence and the end-of-sentence, respectively. The handwritten expression is segmented
by a sliding window into a sequence of images, which are then individually encoded by the ResNet-18. <SOS>
and <EOS> are appended to the start and the end of the image sequence. The result is converted to a sequence of
digits in the reverse order.

B.1 Image Tokenizer and Embedding

To tokenize a handwritten expression, we ﬁrst resize it by making its height 32 and apply a sliding
window of size 32 along the horizontal axis to render a sequence of images. Next, each image in the
sequence is encoded by the ResNet-18 [37]. We found in preliminary experiments that pre-training
on the ImageNet does not help, likely due to the domain gap between ImageNet and HINT. Therefore,
we use a random initialization for ResNet-18 in our experiments.

B.2 Encoder-Decoder Architectures

We consider the following three choices for the encoder-decoder architecture in a seq2seq framework:
Recurrent Neural Networks (RNNs), Transformers, and GPT-3.

RNNs We test two popular RNNs: long short-term memory (LSTM) [40] and gated recurrent units
(GRU) [14]. Both networks are evaluated with and without attention [3]. Our implementations of
RNNs are adapted from a seq2seq tutorial.4

Transformers We benchmark three variants of Transformer: the vanilla Transformer, Transformer
with relative positional encoding, and Universal Transformer with relative positional encoding. The
implementations of these Transformers are adapted from Csordas et al. [15].5

GPT-3 To test GPT-3’s ability to perform simple arithmetic operations without task-speciﬁc training,
Brown et al. [8] developed a small battery of 10 tests that involve asking GPT-3 a simple arithmetic
problem in natural language; see Section 3.9.1 and Table 3.9 in Brown et al. [8] for the results.
In these tests, GPT-3 displays reasonable proﬁciency at simple arithmetic in the few-shot setting.
However, they do not evaluate the multi-hop reasoning capability required by complex arithmetic
expressions, which usually involve more operators and larger numbers.

To systematically and comprehensively evaluate GPT-3’s capability of arithmetic reasoning, we test
GPT-3 on the proposed HINT benchmark using symbolic expressions as input. Since all tokens of

3https://creativecommons.org/licenses/by-nc/4.0/
4https://github.com/bentrevett/pytorch-seq2seq
5https://github.com/RobertCsordas/transformer_generalization

A4

HINT are in the vocabulary of GPT-3, we directly evaluate GPT-3 via zero-shot prompting using the
OpenAI API 6. We construct the prompt in the following form: “Q: What is <Expression>? A: The
answer is,” similar to the practice in Brown et al. [8] but with more complex expressions.

Via task-speciﬁc zero-shot or few-shot prompting, pre-trained large language models achieve amazing
performance in intuitive and single-step System 1 tasks [48]. However, LLMs struggled on System
2 tasks that require slow thinking and multi-hop reasoning [80], even at the scale of over 100B
parameters like GPT-3. To address this shortcoming, chain of thought prompting (CoT) [90], which
feeds LLMs with the intermediate step-by-step reasoning to augment the ﬁnal answer in a few-shot
setting, has been proposed to elicit the multi-hop reasoning in LLMs.

Very recently, chain of thought prompting is extended to the zero-shot setting [53] by adding a simple
prompt, “Let’s think step by step”, to facilitate step-by-step thinking before answering each question.
Zero-shot CoT amazingly outperforms the standard zero-shot prompting by a large margin in a variety
of reasoning tasks. Therefore, we also apply zero-shot CoT prompting to evaluate GPT-3 on HINT.
More concretely, it follows a two-stage prompting strategy similar to [53]:
1st prompt “Q: What is <Expression>? A: Let’s think step-by-step.” This prompt extracts the
step-by-step reasoning process in the form of natural language from GPT-3, which is denoted by <Z>.
2st prompt “Q: What is <Expression>? A: Let’s think step-by-step. <Z> Therefore, the answer
(arabic numerals) is” In the second stage, the response <Z> generated in the ﬁrst step is appended to
the initial prompt along with an answer trigger sentence. This second prompt is then fed into GPT-3
to predict the ﬁnal answer.

In our experiments, we use the ‘text-davinci-002’ engine in the OpenAI API, the most capable GPT-3
model at the time of writing with approximately 175 billion parameters7.

B.3 Training

Tab. A3 shows the tuned hyperparameters for the baselines. Our choices for each model are underlined,
and the performance is reported under these settings unless explicitly stated otherwise. When
generating the output, we use greedy decoding in all models for simplicity.

Table A3: Hyperparameter tuning. Our choices are underlined.

Model

RNN

Transformer

Variant
LSTM (+ att)
GRU (+ att)
vanilla
relative
relative universal

Encoder Decoder Embedding

Hidden

Heads Dropout Batch Steps Learning Rate

1,3,6,9

1,3,6,9

128, 256, 512 128, 256, 512

-

0, 0.1, 0.5

128

100K 10−3, 10−4, 10−5

1,3,6,9

1,3,6,9

128, 256, 512 128, 256, 512 4,8,12 0, 0.1, 0.5

128

100K 10−3, 10−4, 10−5

For the few-shot learning experiments, models are ﬁrst pre-trained on the main training set and then
ﬁne-tuned on the training set of each new concept individually. Models are ﬁne-tuned for 1000
iterations using a batch size of 128 with half examples from the main training set to prevent forgetting.
The learning rates are 10−5 and 10−3 for Transformers and RNNs, respectively.

All models reported in our paper can be trained on a single NVIDIA TITAN V GPU with 12G
memory. It takes at most eight hours to train a model.

C Experimental Results

Fig. A4 shows the t-SNE visualization for the embeddings of handwritten images in the test set using
the Transformer rel. univ. model. Fig. A5 shows the test accuracy as a function of several sample
properties. Fig. A6 shows the importance of these properties.

6https://openai.com/api/
7OpenAI API GPT-3 model sizes: https://blog.eleuther.ai/gpt3-model-sizes

A5

Figure A4: The t-SNE visualization for the embeddings (the outputs of ResNet-18) of handwritten images
in the test set using the Transformer rel. univ. model. The image embeddings form clear clusters for each
concept based on visual appearance. Furthermore, these clusters reﬂect the concepts’ syntactic roles: Most digits
are near the bottom part, the operators are near the middle part, and the parentheses appear near the top part.

Figure A5: Test accuracy (avg.) of Transformer rel. uni. using symbol inputs as a function of several
properties of samples: the expression’s length, the depth of the expression’s parse tree, the expression’s
maximum dependency range, the number of operators in the expression, the ﬁnal result.

Figure A6: The importance of sample properties w.r.t. the test accuracy of Transformer rel. uni. using
symbol inputs. Normalized permutation feature importance is reported here using a k-nearest neighbors
classiﬁer (k=3) to predict if the model can generate correct results.

A6

7106439258*/-+()0204060length0.00.20.40.60.81.0Accuracy51015depth0204060dependency01020operators101103resultlengthdepthdependencyoperatorsresultImportanceISSLSSLLL