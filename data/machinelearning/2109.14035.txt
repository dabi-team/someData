2
2
0
2

b
e
F
4
2

]

G
L
.
s
c
[

3
v
5
3
0
4
1
.
9
0
1
2
:
v
i
X
r
a

Formalizing the Generalization-Forgetting Trade-Off
in Continual Learning

R. Krishnan1 and Prasanna Balaprakash1,2
1Mathematics and Computer Science Division
2Leadership Computing Facility
Argonne National Laboratory
kraghavan,pbalapra@anl.gov

Abstract

We formulate the continual learning problem via dynamic programming and model
the trade-off between catastrophic forgetting and generalization as a two-player
sequential game. In this approach, player 1 maximizes the cost due to lack of
generalization whereas player 2 minimizes the cost due to increased catastrophic
forgetting. We show theoretically and experimentally that a balance point between
the two players exists for each task and that this point is stable (once the balance is
achieved, the two players stay at the balance point). Next, we introduce balanced
continual learning (BCL), which is designed to attain balance between generaliza-
tion and forgetting, and we empirically demonstrate that BCL is comparable to or
better than the state of the art.

1

Introduction

In continual learning (CL), we incrementally adapt a model to learn tasks (deﬁned according to
the problem at hand) observed sequentially. CL has two main objectives: maintain long-term
memory (remember previous tasks) and navigate new experiences continually (quickly adapt to
new tasks). An important characterization of these objectives is provided by the stability-plasticity
dilemma [7], where the primary challenge is to balance network stability (preserve past knowledge;
minimize catastrophic forgetting) and plasticity (rapidly learn from new experiences; generalize
quickly). This balance provides a natural objective for CL: balance forgetting and generalization.

Traditional CL methods either minimize catastrophic forgetting or improve quick generalization but
do not model both. For example, common solutions to the catastrophic forgetting issue include (1)
representation-driven approaches [47, 23], (2) regularization approaches [25, 2, 32, 14, 46, 46, 22, 35,
8, 41], and (3) memory/experience replay [29, 30, 9, 10, 15]. Solutions to the generalization problem
include representation-learning approaches (matching nets [43], prototypical networks [40], and
metalearning approaches [16, 17, 6, 45]). More recently, several approaches [33, 14, 44, 21, 46, 13]
have been introduced that combine methods designed for quick generalization with frameworks
designed to minimize forgetting.

The aforementioned CL approaches naively minimize a loss function (combination of forgetting and
generalization loss) but do not explicitly account for the trade-off in their optimization setup. The
ﬁrst work to formalize this trade-off was presented in meta-experience replay (MER) [36], where
the forgetting-generalization trade-off was posed as a gradient alignment problem. Although MER
provides a promising methodology for CL, the balance between forgetting and generalization is
enforced with several hyperparameters. Therefore, two key challenges arise: (1) lack of theoretical
tools that study the existence (under what conditions does a balance point between generalization
and forgetting exists?) and stability (can this balance be realistically achieved?) of a balance point

35th Conference on Neural Information Processing Systems (NeurIPS 2021).

 
 
 
 
 
 
and (2) lack of a systematic approach to achieve the balance point. We address these challenges in
this paper.

We describe a framework where we ﬁrst formulate CL as a sequential decision-making problem and
seek to minimize a cost function summed over the complete lifetime of the model. At any time k,
given that the future tasks are not available, the calculation of the cost function becomes intractable.
To circumvent this issue, we use Bellman’s principle of optimality [4] and recast the CL problem
to model the catastrophic forgetting cost on the previous tasks and generalization cost on the new
task. We show that equivalent performance on an inﬁnite number of tasks is not practical (Lemma 1
and Corollary 1) and that tasks must be prioritized. To achieve a balance between forgetting and
generalization, we pose the trade-off as a saddle point problem where we designate one player for
maximizing the generalization cost (player 1) and another for minimizing the forgetting cost (player
2). We prove mathematically that there exists at least one saddle point between generalization and
forgetting for each new task (Theorem 1). Furthermore, we show that this saddle point can be attained
asymptotically (Theorem 2) when player strategies are chosen as gradient ascent-descent. We then
introduce balanced continual learning (BCL), a new algorithm to achieve this saddle point. In our
algorithm (see Fig. 1 for a description of BCL), the generalization cost is computed by training
and evaluating the model on given new task data. The catastrophic forgetting cost is computed by
evaluating the model on the task memory (previous tasks). We ﬁrst maximize the generalization
cost and then minimize the catastrophic forgetting cost to achieve the balance. We compare our
approach with other methods such as elastic weight consolidation (EWC) [25], online EWC [38],
and MER [36] on continual learning benchmark data sets [19] to show that BCL is better than or
comparable to the state-of-the-art methods. Moreover, we also show in simulation that our theoretical
framework is appropriate for understanding the continual learning problem. The contributions of this
paper are (1) a theoretical framework to study the CL problem, (2) BCL, a method to attain balance
between forgetting and generalization, and (3) advancement of the state of the art in CL.

2 Problem Formulation

We use R to denote the set of real numbers and N to denote the set of natural numbers. We use (cid:107).(cid:107)
to denote the Euclidean norm for vectors and the Frobenius norm for matrices, while using bold
symbols to illustrate matrices and vectors. We deﬁne an interval [0, K), K ∈ N and let p(Q) be the
distribution over all the tasks observed in this interval. For any k ∈ [0, K), we deﬁne a parametric
model g(.) with yk = g(xk; θk), where θk is a vector comprising all parameters of the model with
xk ∈ X k. Let n be the number of samples and m be the number of dimensions. Suppose a task
at k|k ∈ [0, K) is observed and denoted as Qk : Qk ∼ p(Q), where Qk = {X k, (cid:96)k} is a tuple
with X k ∈ Rn×m being the input data and (cid:96)k quantiﬁes the loss incurred by X k using the model g
for the task at k. We denote a sequence of θk as uk:K = {θτ ∈ Ωθ, k ≤ τ ≤ K}, with Ωθ being
the compact (feasible) set for the parameters. We denote the optimal value with a superscript (∗);
for instance, we use θ(∗)
to denote the optimal value of θk at task k. In this paper we use balance
k
point, equilibrium point, and saddle point to refer to the point of balance between generalization and
forgetting. We interchange between these terms whenever convenient for the discussion. We will use
∇(j)i to denote the gradient of i with respect to j and ∆i to denote the ﬁrst difference in discrete
time.

An exemplary CL problem is described in Fig. 1 where we address a total of K = 3 tasks. To
particularize the idea in Fig. 1, we deﬁne the cost (combination of catastrophic cost and generalization
cost) at any instant k as

Jk(θk) = γk(cid:96)k +

k−1
(cid:88)

τ =0

γτ (cid:96)τ ,

where (cid:96)τ is computed on task Qτ with γτ describing the contribution of Qτ to this sum.

To solve the problem at k, we seek θk to minimize Jk(θk). Similarly, to solve the problem in the
complete interval [0, K], we seek a θk to minimize Jk(θk) for each k ∈ [0, K]. In other words we
seek to obtain θk for each task such that the cost Jk(θk) is minimized. Therefore, the optimization
problem for the overall CL problem (overarching goal of CL) is provided as the minimization of the

2

Figure 1: (left) Exemplary CL problem: the lifetime of the model can be split into three intervals. At k = 1 we
seek to recognize lions; at k = 2 we seek to recognize both lions and cats; and at k = 3 we seek to recognize
cats, lions, and dogs. (right) Illustration of the proposed method: our methodology comprises an interplay
between two players. The ﬁrst player maximizes generalization by simulating maximum discrepancy between
two tasks. The second player minimizes forgetting by adapting to maximum discrepancy.

cumulative cost

such that V (∗)

k

, is given as

Vk(uk:K) =

K
(cid:88)

τ =k

βτ Jτ (θτ )

V (∗)
k = minuk:K Vk(uk:K),
with 0 ≤ βτ ≤ 1 being the contribution of Jτ and uk:K being a weight sequence of length K − k.

(1)

3

Within this formulation, two parameters determine the contributions of tasks: γτ , the contribution
of each task in the past, and βτ , the contribution of tasks in the future. To successfully solve the
optimization problem, Vk(uk:K) must be bounded and differentiable, typically ensured by the choice
of γτ , βτ . Lemma 1 (full statement and proof in Appendix A) states that equivalent performance
cannot be guaranteed for an inﬁnite number of tasks. Furthermore, Corollary 1 (full statement and
proof in Appendix A) demonstrates that if the task contributions are prioritized, the differentiability
and boundedness of Jτ (θτ ) can be ensured. A similar result was proved in [26], where a CL problem
with inﬁnite memory was shown to be NP-hard from a set theoretic perspective. These results (both
ours and in [26]) demonstrate that a CL methodology cannot provide perfect performance on a large
number of tasks and that tasks must be prioritized.

Despite these invaluable insights, the data corresponding to future tasks (interval [k, K]) is not
available, and therefore Vk(uk:K) cannot be evaluated. The optimization problem in Eq. (1) naively
minimizes the cost (due to both previous tasks and new tasks) and does not provide any explicit
modeling of the trade-off between forgetting and generalization. Furthermore, uk:K, the solution
to Eq. (1) is a sequence of parameters, and it is not feasible to maintain uk:K for a large number of
tasks. Because of these three issues, the problem is theoretically intractable in its current form.

We will ﬁrst recast the problem using tools from dynamic programming [27], speciﬁcally Bellman’s
principle of optimality, and derive a difference equation that summarizes the complete dynamics for
the CL problem. Then, we will formulate a two-player differential game where we seek a saddle
point solution to balance generalization and forgetting.

3 Dynamics of Continual Learning

Let

V (∗)
k = minuk:K

K
(cid:88)

τ =k

βτ Jτ (θτ );

the dynamics of CL (the behavior of optimal cost with respect to k) is provided as

∆V (∗)

k = −minθk∈Ωθ

(cid:2)βkJk(θk) + (cid:0)(cid:104)∇θk V (∗)

k

, ∆θk(cid:105) + (cid:104)∇xk V (∗)

k

, ∆xk(cid:105)(cid:1)(cid:3).

(2)

k

k

The derivation is presented in Appendix A (refer to Proposition 1). Note that V (∗)
is the minima for
k
the overarching CL problem in Eq. (2)and ∆V (∗)
represents the change in V (∗)
upon introduction
of a new task (we hitherto refer to this as perturbations). Zero perturbations (∆V (∗)
k = 0) implies
that the introduction of a new task does not impact our current solution; that is, the optimal solution
on all previous tasks is optimal on the new task as well. Therefore, the smaller the perturbations,
the better the performance of a model on all tasks, thus providing our main objective: minimize the
perturbations (∆V (∗)
is quantiﬁed by three terms: the cost contribution from
all the previous tasks and the new task Jk(θk); the change in the optimal cost due to the change in
the parameters (cid:104)∇θk V (∗)
, ∆θk(cid:105); and the change in the optimal cost due to the change in the input
(introduction of new task) (cid:104)∇xk V (∗)
The ﬁrst issue with the cumulative CL problem (Eq. (1)) can be attributed to the need for information
from the future. In Eq. (2), all information from the future is approximated by using the data from
the new and the previous tasks. Therefore, the solution of the CL problem can directly be obtained by
solving Eq. (2) using all the available data. Thus,

). In Eq. 2, ∆V (∗)

, ∆xk(cid:105).

k

k

k

k

minθk∈Ω

(cid:2)H(∆xk, θk)(cid:3) yields ∆V (∗)

k ≈ 0

for β > 0, with

H(∆xk, θk) = βkJk(θk) + (cid:104)∇θk V (∗)

k

, ∆θk(cid:105) + (cid:104)∇xk V (∗)

k

, ∆xk(cid:105).

Essentially, minimizing H(∆xk, θk) would minimize the perturbations introduced by any new task
k.

In Eq. (2), the ﬁrst and the third term quantify generalization and the second term quantiﬁes forgetting.
A model exhibits generalization when it successfully adapts to a new task (minimizes the ﬁrst and

4

the third term in Eq. (2)). The degree of generalization depends on the discrepancy between the
previous tasks and the new task (numerical value of the third term in Eq. (2)) and the worst-case
discrepancy prompts maximum generalization. Quantiﬁcation of generalization is provided by ∆xk
that summarizes the discrepancy between subsequent tasks. However, ∆xk = xk+1 − xk, and xk+1
is unknown at k. Therefore, we simulate worst-case discrepancy by iteratively updating ∆xk through
gradient ascent in order to maximize H(∆xk, θk); thus maximizing generalization. However, large
discrepancy increases forgetting, and worst-case discrepancy yields maximum forgetting. Therefore,
once maximum generalization is simulated, minimizing forgetting (update θk by gradient descent)
under maximum generalization provides the balance.
To formalize our idea, let us indicate the iteration index at k by i and write ∆xk as ∆x(i)
θ(i)
k with H(∆xk, θk) as H(∆x(i)
as H whenever convenient). Next, we write

k ) (for simplicity of notation, we will denote H(∆x(i)

k and θk as
k , θ(i)
k )

k , θ(i)

(cid:2)βkJk(θ(i)

k ) + (cid:104)∇θ(i)

k

V (∗)
k

, ∆θ(i)

k (cid:105) + (cid:104)∇x(i)

k

V (∗)
k

, ∆x(i)

k (cid:105)(cid:3)

(cid:20)
H(∆x(i)

k , θ(i)
k )

(cid:21)

min
k ∈Ωθ

θ(i)

= min
θ(i)
k ∈Ωθ
V (∗)
k

≤ min
θ(i)
k ∈Ωθ

(cid:2)βkJk(θ(i)

k ) + (cid:104)∇θ(i)

k

, ∆θ(i)

k (cid:105) + max

∆x(i)

k ∼p(Q)

(cid:104)∇x(i)

k

V (∗)
k

, ∆x(i)

k (cid:105)(cid:3)

≤ min
θ(i)
k ∈Ωθ

max
k ∼p(Q)

∆x(i)

(cid:2)H(∆x(i)

k , θ(i)

k )(cid:3).

(3), we seek the solution pair (∆x(∗)

In Eq.
H (maximizing player, player 1) while θ(∗)
) are the feasible sets for ∆x(i)
(Ωθ, Ω∆x(∗)
k , θ(∗)
(∆x(∗)

(3)
k maximizes
k minimizes H (minimizing player, player 2) where
k and θ(i)
respectively. The solution is attained, and
k ) is said to be the equilibrium point when it satisﬁes the following condition:

k ) ∈ (Ωθ, Ω∆x(∗)

), where ∆x(∗)

k , θ(∗)

k

k

k

H(∆x(∗)

k , θ(i)

k ) ≥ H(∆x(∗)

k , θ(∗)

k ) ≥ H(∆x(i)

k , θ(∗)
k ).

(4)

3.1 Theoretical Analysis

With our formulation, two key questions arise:
Does our problem setup have an equilibrium
point satisfying Eq. (4)? and how can one at-
tain this equilibrium point? We answer these
questions with Theorems 1 and 2, respectively.
Full statements and proofs are provided in Ap-
pendix A.

the

theory, we

k (red circle)}
k , ∆x(i)

Figure 2: Illustration of the proofs. ∆x (player 1) is the
horizontal axis, and the vertical axis indicates
θ (player 2) where the curve indicates H. If we start from
the red circle for player 1 (player 2 is ﬁxed at the blue
circle), H is increasing (goes from a grey circle to a red
asterisk) with player 1 reaching the red asterisk. Next,
start from the blue circle (θ is at the red asterisk), the
cost decreases.

to
To illustrate
refer
Fig. 4, where the initial values for
the
two players are characterized by the pair
{θ(i)
k (blue circle), ∆x(i)
and
the cost value at {θ(i)
k } is indicated
by H(∆x(i)
k ) (the grey circle on the
cost curve (the dark blue curve)).
Our
prooﬁng strategy is as follows. First, we
ﬁx θ(.)
k ∈ Ωθ and construct a neighborhood
Mk = {Ωx, θ(.)
k }. Within this neighborhood
we prove in Lemmas 2 and 4 that if we search for ∆x(i)
k through gradient ascent, we can converge to
a local maximizer, and H is maximizing with respect to ∆x(i)
k ∈ Ωx be ﬁxed,
and we search for θ(i)
k through gradient descent. Under this condition, we demonstrate two ideas in
Lemmas 3 and 5: (1) we show that H is minimizing in the neighborhood Nk : Nk = {Ωθ, ∆x(.)
k };
and (2) we converge to the local minimizer in the neighborhood Nk. Third, in the union of the two
neighborhoods Mk ∪ Nk, (proven to be nonempty according to Lemma 6), we show that there exists
at least one local equilibrium point (Theorem 1); that is, there is at least one balance point.

k . Second, we let ∆x(.)

k , θ(i)

5

Theorem 1 (Existence of an Equilibrium Point). For any k ∈ [0, K], let θ(∗)
of H according to Lemma 5 and deﬁne M(∗)
maximizer of H according to Lemma 4 and deﬁne N (∗)
be nonempty according to Lemma. 6, then (∆x(∗)
k , θ(∗)

k = {∆x(∗)
k ) ∈ M(∗)

k = {Ωx, θ(∗)

k ∪ N (∗)

k ∈ Ωθ, be the minimizer
k ∈ Ωx, be the
k ∪ N (∗)
is a local equilibrium point.

k , Ωθ}. Further, let M(∗)

k }. Similarly, let ∆x(∗)

k

k

(5)

(6)

(7)

Proof. By Lemma 5 we have at (∆x(∗)

k , θ(∗)
k , θ(∗)
Similarly, according to Lemma 4, at (∆x(∗)

H(∆x(∗)

k ∪ N (∗)

k

that

k , θ(i)

k ), (∆x(∗)
k ) ≤ H(∆x(∗)
k , θ(∗)
k ), (∆x(i)
k ) ≥ H(∆x(i)

k ) ∈ M(∗)
k , θ(i)
k ).
k ) ∈ M(∗)

k , θ(∗)

k , θ(∗)
k ).

k ∪ N (∗)

k we have

H(∆x(∗)

k , θ(∗)

Putting these inequalities together, we get

H(∆x(∗)

k , θ(i)

k ) ≥ H(∆x(∗)

k , θ(∗)

k , θ(∗)
k ),

k ) ≥ H(∆x(i)
k , θ(∗)

k ) is a local equilibrium point in

which is the saddle point condition, and therefore (∆x(∗)
k ∪ N (∗)
M(∗)

k . since M(∗)

k ∪ N (∗)

be nonempty according to Lemma. 6.

k

We next show that this equilibrium point is stable (Theorem 2) under a sequential play. Speciﬁcally,
we show that when player 1 plays ﬁrst and player 2 plays second, we asymptotically reach a saddle
point pair (∆x(∗)
k ) for H. At this saddle point, both players have no incentive to move, and the
game converges.

k , θ(∗)

∈ Ωθ be the initial values for ∆x(i)
k

Theorem 2 (Stability of
Ωx and θ(i)
k
Mk = {Ωx, Ωθ} with H(∆x(i)
α(i)
k × (∇∆x(i)
H(∆x(.)
∇θ(i)
a consequence of Lemmas 2 and 3, (∆x(∗)

the Equilibrium Point). For any k ∈ [0, K], ∆x(i)
∈
k
Deﬁne
k =
k ×
k ). Let the existence of an equilibrium point be given by Theorem 1, then, as
k ) ∈ Mk is a stable equilibrium point for H given .

and θ(i)
k
Let ∆x(i+1)
k
− θ(i)

k ) given by Proposition 2.
k , θ(.)

− ∆x(i)
k = −α(i)

k , θ(i)
k ))/(cid:107)∇∆x(i)

k )(cid:107)2) and θ(i+1)

respectively.

k , θ(∗)

H(∆x(i)

H(∆x(i)

k , θ(i)

k , θ(.)

k

k

k

k

k ) ∈ Mk will reach (∆x(∗)

Proof. Consider now the order of plays by the two players. By Lemma 2, a game starting at
(∆x(i)
k ) which is a maximizer for H. Now, deﬁne Nk =
(∆x(∗)
k ) ∈ Nk will converge to (∆x(∗)
k ) ∈ Nk
according to Lemma 3. Since, Nk ⊂ Mk, our result follows.

k , θ(i)
k , Ωθ) ⊂ Mk then a game starting at (∆x(∗)

k , θ(∗)

k , θ(i)

k , θ(i)

In this game, the interplay between these two opposing players (representative of generalization
and forgetting, respectively) introduces the dynamics required to play the game. Furthermore, the
results presented in this section are local to the task. In other words, we prove that we can achieve a
balance between generalization and forgetting for each task k (neighborhoods are task dependent,
and we achieve a local solution given a task k). Furthermore, our game is sequential; that is, there is
k ) plays ﬁrst, and the follower (θ(i)
a leader (player 1) and a follower (player 2). The leader (∆x(i)
k )
plays second with complete knowledge of the leader’s play. The game is directed by ∆x(i)
k , and
any changes in the task (reﬂected in ∆x(i)
k ) will shift the input and thus the equilibrium point.
Consequently, the equilibrium point varies with respect to a task, and one will need to attain a new
equilibrium point for each shift in a task. Without complete knowledge of the tasks (not available in a
CL scenario), only a local result is possible. This highlights one of the key limitations of this work.
Ideally, we would like a balance between forgetting and generalization that is independent of tasks.
However, this would require learning a trajectory of the equilibrium point (How does the equilibrium
point change with the change in the tasks?) and is beyond the scope of this paper. One work that
attempts to do this is [37], where the authors learn a parameter per task. For a large number of tasks,
however, such an approach is computationally prohibitive.

These results are valid only under certain assumptions: (1) the Frobenius norm of the gradient is
bounded, always positive; (2) the cost function is bounded and differentiable; and (3) the learning

6

rate goes to zero as i tends to inﬁnity. The ﬁrst assumption is reasonable in practice, and gradient
clipping or perturbation strategies can be used to ensure it. The boundedness of the cost (second
assumption) can be ensured by prioritizing the contributions of the task (Lemma 1 and Corollary 1).
The third assumption assumes a decaying learning rate. Learning rate decay is a common strategy
and is employed widely. Therefore, all assumptions are practical and reasonable.

3.2 Balanced Continual Learning

Equipped with the theory, we develop a new CL
method to achieve a balance between forgetting and
generalization. By Proposition 2, the cost function can
be upper bounded as H(∆x(i)
k ) +
(Jk(θ(i+ζ)
) − Jk(θ(i)
k )),
k
where Jk+ζ indicates ζ updates on player 1 and θ(i+ζ)
indicates ζ updates on player 2.

k ) ≤ βkJk(θ(i)
k ) − Jk(θ(i)

k )) + (Jk+ζ(θ(i)

k , θ(i)

k

k ∇∆xk E[H(∆x(i)
α(i)
k , θ(i)
(cid:107)∇∆xk H(∆x(i)

k , θ(i)
k )]
k )(cid:107)2

,

(cid:124)

−α(i)
(cid:124)

k × ∇θk E[H(∆x(∗)

k , θ(i)

(cid:123)(cid:122)
Player 1

(cid:123)(cid:122)
Player 2

(cid:125)

,
k ))]
(cid:125)

(8)

Algorithm 1: BCL
Initialize θ, DP , DN
while k = 1, 2, 3, ...K do

j = 0
while j < ρ do

Get bN ∈ DN
k
Get bP ∈ DP
k
Get bP N = bP ∪ bN
Copy bP N into xP N
i = 0 while i + 1 <= ζ do

k

Update xP N
using gradient ascent

k with Jk(θk)

i = i+1

k ) − Jk(θ(i)
k )

Calculate Jk+ζ(θ(i)
Copy θ(i)
k into θB
i = 0 while i + 1 <= ζ do
k with Jk(θB
k )

k

Update θB
i = i+1
Calculate (Jk(θB
Calculate H(∆x(i)
Update θ(i)
descent

k ))

j= j+1

Update DP with DN

k ) − Jk(θ(i)
k , θ(i)
k )
k using gradient

The strategies for the two players ∆xk, θk are cho-
sen in Eq. (8) with E being the expected value op-
erator. We can approximate the required terms in
our update rule (player strategies) using data samples
(batches). Note that the approximation is performed
largely through one-sided ﬁnite difference, which may
introduce an error and is another potential drawback.
The pseudo code of the BCL is shown in Algorithm 1.
We deﬁne a new task array DN (k) and a task memory
array DP (k) ⊂ ∪k−1
τ =0Qτ (samples from all previous tasks). For each batch bN ∈ DN (k), we
sample bP from DP (k), combine to create bP N (k) = bP (k) ∪ bN (k), and perform a sequential play.
k = bP N (k) and performs ζ updates on xP N
Speciﬁcally, for each task the ﬁrst player initializes xP N
through gradient ascent. The second player, with complete knowledge of the ﬁrst player’s strategy,
chooses the best play to reduce H(∆x(i)
k ). To estimate player 2’s play, we must estimate differ-
ent terms in H(∆x(i)
k , θ(i)
k ). This procedure involves three steps. First, we use the ﬁrst player’s play
k ) − Jk(θ(i)
and approximate (Jk+ζ(θ(i)
k )) : (a), we
copy ˆθ into ˆθB (a temporary network) and perform ζ updates on ˆθB; and (b) we compute Jk(θ(i+ζ)
)
using ˆθB(k + ζ) and evaluate (Jk(θ(i+ζ)
k )). Third, equipped with these approximations,
we compute H(∆x(i)
k ) and obtain the play for the second player. Both these players perform
the steps repetitively for each piece of information (batch of data). Once all the data from the new
task is exhausted, we move to the next task.

k )). Second, to approximate (Jk(θ(i+ζ)

) − Jk(θ(i)

) − Jk(θ(i)

k , θ(i)

k , θ(i)

k

k

k

k

3.3 Related Work

Traditional solutions to the CL focus on either the forgetting issue [37, 47, 45, 5, 25, 48, 2, 29, 30, 9]
or the generalization issue [43, 40, 16, 17, 6]. Common solutions to the forgetting problem involve
dynamic architectures and ﬂexible knowledge representation such as [37, 47, 45, 5], regularization
approaches including [25, 48, 2] and memory/experience replay [29, 30, 9]. Similarly, quick gen-
eralization to a new task has been addressed through few-shot and one-shot learning approaches
such as matching nets [43] and prototypical network [40]. More recently, the ﬁeld of metalearn-
ing has approached the generalization problem by designing a metalearner that can perform quick
generalization from very little data [16, 17, 6].

7

In the past few years, metalearners for quick generalization have been combined with methodologies
speciﬁcally designed for reduced forgetting [20, 3]. For instance, the approaches in [20, 3] adapt the
model-agnostic metalearning (MAML) framework in [16] with robust representation to minimize
forgetting and provide impressive results on CL. However, both these approaches require a pretraining
phase for learning representation. Simultaneously, Gupta et al. [18] introduced LA-MAML—a
metalearning approach where the impact of learning rates on the CL problem is reduced through
the use of per-parameter learning rates. LA-MAML [18] also introduced episodic memory to
address the forgetting issue. Other approaches also have attempted to model both generalization and
forgetting. In [14], the gradients from new tasks are projected onto a subspace that is orthogonal to
the older tasks, and forgetting is minimized. Similarly, Joseph and Balasubramanian [21] utilized a
Bayesian framework to consolidate learning across previous and current tasks, and Yin et al. [46]
provided a framework for approximating loss function to summarize the forgetting in the CL setting.
Furthermore, Abolfathi et al. [1] focused on sampling episodes in the reinforcement learning setting,
and Elrahimi et al. [13] introduced a generative adversarial network-type structure to progressively
learn shared features assisting reduced forgetting and improved generalization. Despite signiﬁcant
progress, however, these methods [20, 3, 18, 14, 21, 46, 1, 13] are still inherently tilted toward
maximizing generalization or minimizing forgetting because they naively minimize the loss function.
Therefore, the contribution of different terms in the loss function becomes important. For instance, if
the generalization cost is given more weight, a method would generalize better. Similarly, if forgetting
cost is given more weight, a method would forget less. Therefore, the resolution of the trade-off
inherently depends on an hyperparameter.

The ﬁrst work to formalize the trade-off in CL was MER, where the trade-off was formalized as a
gradient alignment problem. Similar to MER, Doan et al. ([11]) studied forgetting as an alignment
problem. In MER, the angle between the gradients was approximated by using Reptile [34], which
promotes gradient alignment by reducing weight changes. On the other hand, [11] formalized the
alignment as an eigenvalue problem and introduced a PCA-driven method to ensure alignment. Our
approach models this balance as a saddle point problem achieved through stochastic gradient such
that the saddle point (balance point or equilibrium point) resolves the trade-off.

Our approach is the ﬁrst in the CL literature to prove the existence of the saddle point (the balance
point) between generalization and forgetting given a task in a CL problem. Furthermore, we are the
ﬁrst to theoretically demonstrate that the saddle point can be achieved reasonably under a gradient
ascent-descent game. The work closest to ours is [13], where an adversarial framework is described
to minimize forgetting in CL by generating task-invariant representation. However, [13] is not model
agnostic (the architecture of the network is important) and requires a considerable amount of data at
the start of the learning procedure. Because of these issues, [13] is not suitable for learning in the
sequential scenario.

4 Experiments

We use the CL benchmark [19] for our experiments and retain the experimental settings (hyperparame-
ters) from [19, 42]. For comparison, we use the split-MNIST, permuted-MNIST, and split-CiFAR100
data sets while considering three scenarios: incremental domain learning (IDL), incremental task
learning (ITL), and incremental class learning (ICL). The splitting and permutation strategies when
applied to the MNIST or CiFAR100 data set can generate task sequences for all three scenarios (il-
lustrated in Figure 1 and Appendix: Figure 2 of [19]). For comparing our approach, we use three
baseline strategies—standard neural network with Adam [24], Adagrad [12], and SG—and use
L2-regularization and naive rehearsal (which is similar to experience replay). For CL approaches, we
use EWC [25], online EWC [38], SI [48], LwF [28], DGR [39], RtF [42], MAS [2], MER [36], and
GEM [31]. We utilize data preprocessing as provided by [19]. Additional details on experiments can
be found in Appendix B and [19, 42]. All experiments are conducted in Python 3.4 using the pytorch
1.7.1 library with the NVIDIA-A100 GPU for our simulations.

Comparison with the state of the art: The results for our method are summarized in Table 1 and 2.
The efﬁciency for any method is calculated by observing the average accuracy (retained accuracy
(RA) [36]) at the end of each repetition and then evaluating the mean and standard deviation of RA
across different repetitions. For each method, we report the mean and standard deviation of RA
over ﬁve repetitions of each experiment. In each column, we indicate the best-performing method
in bold. For the split-MNIST data set, we obtain 99.52 ± 0.07 for ITL, 98.71 ± 0.06 for IDL, and

8

Table 1: Performance of our approach compared with other methods in the literature. We record the mean and
standard deviation of the retained accuracy for the different methods. The best scores are in bold.

Method

Adam
SGD
Adagrad
L2
Naive rehearsal
Naive rehearsal-C

EWC
Online EWC
SI
MAS
GEM
DGR
RtF
MER
BCL (With Game)
BCL (Without Game)

Incremental
task learning
[ITL]
95.52 ± 2.14
97.65 ± 0.28
98.37 ± 0.29
97.62 ± 0.69
99.32 ± 0.10
99.41 ± 0.04

96.59 ± 0.99
99.01 ± 0.12
99.10 ± 0.16
98.88 ± 0.14
98.32 ± 0.08
99.47 ± 0.03
99.66 ± 0.03
97.12 ± 0.10
99.52 ± 0.07
97.73 ± 0.03

split-MNIST
Incremental
domain learning
[IDL]
54.75 ± 2.06
62.80 ± 0.34
57.59 ± 2.54
66.84 ± 3.91.
94.85 ± 0.80
97.13 ± 0.37

57.31 ± 1.07
58.25 ± 1.23
64.63 ± 1.67
61.98 ± 7.17
97.37 ± 0.22
95.74 ± 0.23
97.31 ± 0.11
92.16 ± 0.35
98.71 ± 0.06
96.43 ± 0.29

Incremental
class learning
[ICL]
19.72 ± 0.03
19.36 ± 0.02
19.59 ± 0.17
22.92 ± 1.90
90.88 ± 0.70
94.92 ± 0.63

19.70 ± 0.14
19.68 ± 0.05
19.67 ± 0.25
19.70 ± 0.34
93.04 ± 0.05
91.24 ± 0.33
92.56 ± 0.21
93.20 ± 0.12
97.32 ± 0.17
91.88 ± 0.55

Incremental
task learning
[ITL]
93.42 ± 0.56
90.95 ± 0.20
92.45 ± 0.16
94.87 ± 0.38
96.23 ± 0.04
97.13 ± 0.03

95.38 ± 0.33
95.15 ± 0.49
94.35 ± 0.51
94.74 ± 0.52
95.44 ± 0.96
92.52 ± 0.08
97.31 ± 0.01
97.15 ± 0.08
97.41 ± 0.01
96.16 ± 0.03

permuted-MNIST
Incremental
domain learning
[IDL]
77.87 ± 1.27
78.17 ± 1.16
91.59 ± 0.46
92.81 ± 0.32
95.84 ± 0.06
96.75 ± 0.03

89.54 ± 0.52
93.47 ± 0.01
91.12 ± 0.93
93.22 ± 0.80
96.86 ± 0.02
95.09 ± 0.04
97.06 ± 0.02
96.11 ± 0.31
97.51 ± 0.05
96.08 ± 0.06

Incremental
class learning
[ICL]
14.02 ± 1.25
12.82 ± 0.95
29.09 ± 1.48
13.92 ± 1.79
96.25 ± 0.10
97.24 ± 0.05

26.32 ± 4.32
42.58 ± 6.50
58.52 ± 4.20
50.81 ± 2.92
96.72 ± 0.03
92.19 ± 0.09
96.23 ± 0.04
91.71 ± 0.03
97.61 ± 0.01
95.96 ± 0.06

97.32 ± 0.17 for ICL. Similarly, with the permuted-MNIST data set, we obtain 97.41 ± 0.01 for
ITL, 97.51 ± 0.05 for IDL, and 97.61 ± 0.01 for ICL. Furthermore, with the split-CiFAR100 data
set, we obtain 81.82 ± 0.17 for ITL, 62.11 ± 0.00 for IDL, and 69.27 ± 0.03 for ICL. BCL is the
best-performing methodology for all cases (across both data sets) except RtF for ITL (0.14% drop)
with the split-MNIST data set.

Method

Table 2: Performance of BCL for the split-CiFAR100 data set. We
record the retained accuracy for the different methods. We obtained
RA scores for all methods except BCL from [19].

Generally, ITL is the easiest learn-
ing scenario [19], and all methods
therefore perform well on ITL (the
performance is close). For the ITL
scenario with the split-MNIST data
set, BCL is better than most methods;
but several methods, such as naive re-
hearsal, naive rehearsal-C, RtF, and
DGR, attain close RA values (less
than 1% from BCL). Note that both
DGR and RtF involve a generative
model pretrained with data from all
the tasks.
In a sequential learning
scenario, one cannot efﬁciently train
generative models because data cor-
responding to all the tasks is not
available beforehand. Although RtF provides improved performance for split-MNIST (ITL), the
improvement is less than 1% (not signiﬁcant). In fact, RtF performance is poorer for BCL in ICL by
4.76% (a signiﬁcant drop in performance) and in IDL by 1.4%.

split-CiFAR100
Incremental
domain learning
19.65 ± 0.14
19.17 ± 0.12
19.06 ± 0.14
19.96 ± 0.15
35.94 ± 0.39
51.81 ± 0.18

Incremental
task learning
30.53 ± 0.58
43.77 ± 1.15
36.27 ± 0.43
51.73 ± 1.30
70.20 ± 0.17
78.41 ± 0.37

Incremental
class learning
17.20 ± 0.06
17.18 ± 0.12
15.83 ± 0.20
17.12 ± 0.04
34.33 ± 0.19
51.28 ± 0.17

EWC
Online EWC
SI
MAS
BCL(With Game)
BCL(Without Game)

Adam
SGD
Adagrad
L2
Naive rehearsal
Naive rehearsal-C

61.11 ± 1.43
63.22 ± 0.97
64.81 ± 1.00
64.77 ± 0.78
81.82 ± 0.17
69.17 ± 0.12

19.70 ± 0.14
17.16 ± 0.09
17.26 ± 0.11
17.07 ± 0.12
69.27 ± 0.03
52.82 ± 0.01

19.76 ± 0.12
20.03 ± 0.10
20.26 ± 0.09
19.99 ± 0.16
62.11 ± 0.00
51.82 ± 0.19

Two additional observations can be made about the split-MNIST data set. First, Adagrad, SGD,
and L2 achieve better performance than does Adam. Therefore, in our analysis Adagrad appears
more appropriate although Adam is popularly used for this task. Second, naive rehearsal (both naive
rehearsal and naive rehearsal-C approaches) achieves performance equivalent to the state-of-the-art
methods with similar memory overhead. Furthermore, naive rehearsal performs much better than
online EWC and SI, especially in the ICL scenario. These limitations indicate that regularization-
driven approaches are not much better than baseline models and in fact perform poorer than methods
involving memory (naive rehearsals, MER, BCL, etc.). In [26], it was shown theoretically that
memory-based approaches typically do better than regularization-driven approaches, as is empirically
observed in this paper, too. Another interesting observation is that EWC and online EWC require
signiﬁcant hyperparameter tuning, which would be difﬁcult to do in real-world scenarios. Other
regularization-based methods, such as SI and MAS, also suffer from the same issue.

The observations from the split-MNIST carry forward to the permuted-MNIST data set. Moreover,
RA values for the permutation MNIST data set are better for the split-MNIST data set across the
board, indicating that the permutation MNIST data set presents an easier learning problem. Similar
to the observations made with the split-MNIST data set, BCL is better than all methods for the
permuted-MNIST dataset, with naive rehearsal and RtF providing RA values that are close (less than

9

1%). The only methodology in the literature that attempts to model the balance between forgetting and
generalization is MER, an extension of GEM. From our results, we observe that BCL is better than
MER in all cases (Split-MNIST–2.4% improvement for ITL, 6.55% improvement for IDL, 4.12%
improvement for ICL and Permuted-MNIST–0.26% improvement for ITL, 1.4% improvement for
IDL and 5.9% improvement for ICL).

The substantial improvements obtained by BCL are more evident in Table 2 where the results on
the split-CiFAR100 data set are summarized. BCL is clearly the best-performing method. The next
best-performing method is naive rehearsal-C, where BCL improves performance by 3.41% for ITL,
10.3% for IDL, and 17.99% for ICL. Other observations about the regularization methods and the
rest of the baseline methods carry forward from Table 1. However, one key difference is that while
AdaGrad is observed to be better than Adam for the MNIST data set, Adam is comparable to SGD
and Adagrad for split-CiFAR100. In summary, BCL is comparable to or better than the state of the
art in the literature for both the MNIST and CiFAR100 data sets.

(a)

Do we need a game to achieve this perfor-
mance? To provide additional insights into the
beneﬁts of the game, we compare RA values
with and without the game. In our setup, ∆x(i)
k
aims at increasing the cost, and θ(i)
k aims at re-
ducing the cost. If we hold the play for ∆x(i)
k ,
then the dynamics required to play the game
do not exist, thus providing a method that can
perform CL without the game. Therefore, we
induce the absence of a game by ﬁxing ∆x(i)
k
and perform each of the nine experiments for
ﬁve repetitions (ICL, IDL, and ITL for split-
MNIST, permuted-MNIST, and split-CiFAR100).
We summarize these results in the last two rows
of Tables 1 and 2. Consequently, we make two
observations. First: even without the game, we
achieve RA values comparable to the state of
the art. This is true for both the MNIST and Ci-
FAR100 data sets. Second, with the introduction
of the game, we observe improved RA values
across the board (at least by 1% with MNIST).
The difference is clearer with the CiFAR100 data
set where we observe a substantial improvement
in RA values (at least 10%).

(b)

k

Figure 3: (Top) Progression of different terms in Eq.
(2) with respect to update iterations, where the index
on the x axis is calculated as k × 300. The tasks
boundaries (at what i the tasks are introduced) are
illustrated through shades of grey. (down) Illustration
of the cost at k = 3.

Does the theory appropriately model the con-
tinual learning problem? In Fig. 3a we plot the
progression of Jk(θk) + (cid:104)∇xk V (∗)
, ∆xk(cid:105) (blue
k
curve), (cid:104)∇θk V (∗)
, ∆θk(cid:105) (red curve), and the
sum, namely, ∆V (∗)
(green curve). A total of
six tasks, sampled from the permutation MNIST
data set within the ICL setting, are illustrated in Fig. 3a. These tasks are introduced every 300 update
steps. From Fig. 3a we make two important observations. First, as soon as tasks 1, 3, 4, and 6 are
introduced, all three curves (red, blue, and green) indicate a bump. Second, when tasks 2 and 5
are introduced, there is no change. On a closer look at task 3 in Fig. 3b, we observe that when the
task 3 is introduced, the blue curve exhibits a large positive bump (the introduction of the new tasks
increases the ﬁrst and the third terms in Eq. (2)). The increase implies that task 3 forced the model to
generalize and increased forgetting on tasks 1 and 2 (observed by the increase in the green curve). To
compensate for this increase, we require the model θk to behave adversarially and introduce a large
enough negative value in (cid:104)∇θk V (∗)
, ∆θk(cid:105) (red curve) to cancel out the increase in the blue curve. In
Fig. 3b the red curve demonstrates a large negative value (expected behavior) and eventually (as i
increases) forces the blue curve (by consequence, green: the sum of red and blue) to move toward
zero (the model compensates for the increase in forgetting). As observed, the blue and the red curves

k

k

10

02004006008001000120014001600i (update iteration at each task k)10505101520Task 1Task 2Task 3Task 4Task 5Task 6›∇θkV(∗)k,∆θkﬁ Jk(θk)+›∇xkV(∗)k,∆xkﬁ∇V∗k050100150200250300i (update iteration at task k=3)5051015behave opposite to each other and introduce a push-pull behavior that stops only when the two cancel
each other and the sum (green) is zero. Once the sum has reached zero, there is no incentive for the
red and green to be nonzero, and therefore they remain at zero; thus, all three curves (green, red,
and blue) remain at zero once converged until a task 4 is introduced (when there is another bump,
as seen in Fig. 3a). However, this increase in the blue curve is not observed when tasks 2 and 5
are introduced. When new tasks are similar to the older tasks, it is expected that ∆V (∗)
k = 0, as is
observed in Fig. 3a.

k

k

, ∆θk(cid:105) and (cid:104)∇xk V (∗)

All of these observations are fully explained by Eq. 2, which illustrates that the solution to the CL
problem is obtained optimally only when ∆V (∗)
k = 0 (observed in Figs. 3a and 3b). The term
∆V (∗)
is quantiﬁed by Jk(θk), (cid:104)∇θk V (∗)
, ∆xk(cid:105). Our theory suggests that
k
there exists an inherent trade-off between different terms in Eq. 2. Therefore every time a new task is
observed, it is expected that Jk(θk) + (cid:104)∇xk V (∗)
, ∆xk(cid:105) increases (increase in blue curve when tasks
k
1, 3, 4, and 6 are introduced) and (cid:104)∇θk V (∗)
, ∆θk(cid:105) compensates to cancel this increase (red curve
exhibits a negative jump). In Theorems 1 and 2 we demonstrate the existence of this balance point (for
each task, as i increases, ∆V (∗)
remains zero (the
balance point is stable, proved in Theorem 2) until a new task increases forgetting. Furthermore,
our theory claims the existence of a solution with respect to each task. This is also observed in
Fig. 3a as, for each task, there is an increase in cost, and BCL quickly facilitates convergence. These
observations indicate that our dynamical system in Eq. 2 accurately describes the dynamics of the
continual learning problem. Furthermore, the assumptions under which the theory is developed are
practical and are satisﬁed by performing continual learning on the permuted MNIST problem.

tends to zero, as observed in Fig. 3b) and ∆V (∗)

k

k

k

5 Conclusion

We developed a dynamic programming-based framework to enable the methodical study of key
challenges in CL. We show that an inherent trade-off between generalization and forgetting exists
and must be modeled for optimal performance. To this end, we introduce a two-player sequential
game that models the trade-off. We show in theory and simulation that there is an equilibrium
point that resolves this trade-off (Theorem 1) and that this saddle point can be attained (Theorem 2).
However, we observe that any change in the task modiﬁes the equilibrium point. Therefore, a global
equilibrium point between generalization and forgetting is not possible, and our results are valid
only in a neighborhood (deﬁned given a task). To attain this equilibrium point, we develop BCL and
demonstrate state-of-the-art performance on a CL benchmark [19]. In the future, we will extend our
framework for nonEuclidean tasks.

6 Broader Impact

Positive Impacts: CL has a wide range of applicability. It helps avoids retraining, and it improves
the learning efﬁciency of learning methods. Therefore, in science applications where the data is
generated sequentially but the data distribution varies with time, our theoretically grounded method
provides the potential for improved performance. Negative Impacts: Our theoretical framework does
not have direct adverse impacts. However, the potential advantages of our approach can improve the
efﬁciency of adverse ML systems such as fake news, surveillance, and cybersecurity attacks.

Acknowledgments and Disclosure of Funding

This work was supported by the U.S. Department of Energy, Ofﬁce of Science, Advanced Scientiﬁc
Computing Research, under Contract DE-AC02-06CH11357 and by a DOE Early Career Research
Program award. We are grateful for the computing resources from the Joint Laboratory for System
Evaluation and Leadership Computing Facility at Argonne. We also are grateful to Dr. Vignesh
Narayanan, assistant professor, University of South Carolina, and Dr. Marieme Ngom, Dr. Sami
Khairy –postdoctoral appointees, Argonne National Laboratory, for their insights.

11

References

[1] Elmira Amirloo Abolfathi, Jun Luo, Peyman Yadmellat, and Kasra Rezaee. CoachNet: An

adversarial sampling approach for reinforcement learning, 2021.

[2] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuyte-
laars. Memory aware synapses: Learning what (not) to forget. In Proceedings of the European
Conference on Computer Vision (ECCV), pages 139–154, 2018.

[3] Shawn Beaulieu, Lapo Frati, Thomas Miconi, Joel Lehman, Kenneth O Stanley, Jeff Clune, and

Nick Cheney. Learning to continually learn. arXiv preprint arXiv:2002.09571, 2020.

[4] Richard E Bellman. Adaptive control processes: a guided tour. Princeton university press,

2015.

[5] Massimo Caccia, Pau Rodriguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin, Lucas
Caccia, Issam Laradji, Irina Rish, Alexandre Lacoste, David Vazquez, and Laurent Charlin.
Online fast adaptation and knebowledge accumulation: a new approach to continual learning,
2021.

[6] Massimo Caccia, Pau Rodríguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin, Lucas
Caccia, Issam H. Laradji, Irina Rish, Alexande Lacoste, David Vázquez, and Laurent Charlin.
Online fast adaptation and knowledge accumulation: A new approach to continual learning.
ArXiv, abs/2003.05856, 2020.

[7] Gail A Carpenter and Stephen Grossberg. A massively parallel architecture for a self-organizing
neural pattern recognition machine. Computer vision, graphics, and image processing, 37(1):54–
115, 1987.

[8] Arslan Chaudhry, Naeemullah Khan, Puneet K Dokania, and Philip HS Torr. Continual learning

in low-rank orthogonal subspaces. arXiv preprint arXiv:2010.11635, 2020.

[9] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K
Dokania, Philip HS Torr, and Marc’Aurelio Ranzato. Continual learning with tiny episodic
memories. arXiv preprint arXiv:1902.10486, 2019.

[10] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K
Dokania, Philip HS Torr, and Marc’Aurelio Ranzato. On tiny episodic memories in continual
learning. arXiv preprint arXiv:1902.10486, 2019.

[11] Thang Doan, Mehdi Abbana Bennani, Bogdan Mazoure, Guillaume Rabusseau, and Pierre
Alquier. A theoretical analysis of catastrophic forgetting through the NTK overlap matrix. In
International Conference on Artiﬁcial Intelligence and Statistics, pages 1072–1080. PMLR,
2021.

[12] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning

and stochastic optimization. Journal of Machine Learning Research, 12(7), 2011.

[13] Sayna Ebrahimi, Franziska Meier, Roberto Calandra, Trevor Darrell, and Marcus Rohrbach.

Adversarial continual learning, 2020.

[14] Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li. Orthogonal gradient descent for

continual learning, 2019.

[15] Enrico Fini, Stéphane Lathuilière, Enver Sangineto, Moin Nabi, and Elisa Ricci. Online
continual learning under extreme memory constraints. In European Conference on Computer
Vision, pages 720–735. Springer, 2020.

[16] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-
tation of deep networks. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pages 1126–1135. JMLR. org, 2017.

[17] Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning.

arXiv preprint arXiv:1902.08438, 2019.

[18] Gunshi Gupta, Karmesh Yadav, and Liam Paull. La-MAML: Look-ahead meta learning for

continual learning, 2020.

[19] Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating continual
learning scenarios: A categorization and case for strong baselines. In NeurIPS Continual
learning Workshop, 2018.

12

[20] Khurram Javed and Martha White. Meta-learning representations for continual learning. In

Advances in Neural Information Processing Systems, pages 1818–1828, 2019.

[21] K J Joseph and Vineeth N Balasubramanian. Meta-consolidation for continual learning, 2020.

[22] Sangwon Jung, Hongjoon Ahn, Sungmin Cha, and Taesup Moon. Continual learning with
node-importance based adaptive group sparse regularization. arXiv e-prints, pages arXiv–2003,
2020.

[23] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence of similar

and dissimilar tasks. Advances in Neural Information Processing Systems, 33, 2020.

[24] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

[25] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,
Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.
Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy
of Sciences, 114(13):3521–3526, 2017.

[26] Jeremias Knoblauch, Hisham Husain, and Tom Diethe. Optimal continual learning has perfect
memory and is NP-hard. In International Conference on Machine Learning, pages 5327–5337.
PMLR, 2020.

[27] Frank L Lewis, Draguna Vrabie, and Vassilis L Syrmos. Optimal control. John Wiley & Sons,

2012.

[28] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern

Analysis and Machine Intelligence, 40(12):2935–2947, 2017.

[29] Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and

teaching. Machine Learning, 8(3-4):293–321, 1992.

[30] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning.

In Advances in neural information processing systems, pages 6467–6476, 2017.

[31] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning.

arXiv preprint arXiv:1706.08840, 2017.

[32] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Razvan Pascanu, and Hassan Ghasemzadeh. Under-
standing the role of training regimes in continual learning. arXiv preprint arXiv:2006.06958,
2020.

[33] Anusha Nagabandi, Chelsea Finn, and Sergey Levine. Deep online learning via meta-learning:

Continual adaptation for model-based RL, 2019.

[34] Alex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms,

2018.

[35] Pingbo Pan, Siddharth Swaroop, Alexander Immer, Runa Eschenhagen, Richard E Turner, and
Mohammad Emtiyaz Khan. Continual deep learning by functional regularisation of memorable
past. arXiv preprint arXiv:2004.14070, 2020.

[36] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Ger-
ald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing
interference. arXiv preprint arXiv:1810.11910, 2018.

[37] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick,
Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv
preprint arXiv:1606.04671, 2016.

[38] Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska,
Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework
for continual learning. In International Conference on Machine Learning, pages 4528–4537.
PMLR, 2018.

[39] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep

generative replay. arXiv preprint arXiv:1705.08690, 2017.

[40] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In

Advances in neural information processing systems, pages 4077–4087, 2017.

13

[41] Michalis K Titsias, Jonathan Schwarz, Alexander G de G Matthews, Razvan Pascanu, and
Yee Whye Teh. Functional regularisation for continual learning with Gaussian processes. arXiv
preprint arXiv:1901.11356, 2019.

[42] Gido M. van de Ven and Andreas S. Tolias. Generative replay with feedback connections as a

general strategy for continual learning, 2019.

[43] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for
one shot learning. Advances in Neural Information Processing Systems, 29:3630–3638, 2016.
[44] Risto Vuorio, Dong-Yeon Cho, Daejoong Kim, and Jiwon Kim. Meta continual learning. CoRR,

abs/1806.06928, 2018.

[45] Huaxiu Yao, Longkai Huang, Ying Wei, Li Tian, Junzhou Huang, and Zhenhui Li. Don’t
overlook the support set: Towards improving generalization in meta-learning. arXiv preprint
arXiv:2007.13040, 2020.

[46] Dong Yin, Mehrdad Farajtabar, Ang Li, Nir Levine, and Alex Mott. Optimization and general-
ization of regularization-based continual learning: a loss approximation viewpoint, 2020.
[47] Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with

dynamically expandable networks. arXiv preprint arXiv:1708.01547, 2017.

[48] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic
intelligence. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pages 3987–3995. JMLR. org, 2017.

Checklist

1. For all authors...

(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s

contributions and scope? [Yes] See Fig 1 (right)

(b) Did you describe the limitations of your work? [Yes] Refer Section 3.1 and Section 3.2
(c) Did you discuss any potential negative societal impacts of your work? [Yes] Refer

Section Broader Impact

(d) Have you read the ethics review guidelines and ensured that your paper conforms to

them? [Yes] Yes

2. If you are including theoretical results...

(a) Did you state the full set of assumptions of all theoretical results? [Yes] Refer to the

theorem statements in section. 3.1

(b) Did you include complete proofs of all theoretical results? [Yes] Refer to proofs in

Appendix A
3. If you ran experiments...

(a) Did you include the code, data, and instructions needed to reproduce the main ex-
perimental results (either in the supplemental material or as a URL)? [Yes] Refer to
supplementary materials

(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they

were chosen)? [Yes] Refer to Appendix C in supplementary materials and [19]

(c) Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [Yes] Five repetitions and mean and standard deviations are
reported. Refer to Table. 1

(d) Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [Yes] Refer to the Results section
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators? [Yes] We used the continual

learning benchmark by [19]; we cite their paper.

(b) Did you mention the license of the assets? [Yes] No licence information was provided

by [19].

(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]

14

(d) Did you discuss whether and how consent was obtained from people whose data you

are using/curating? [N/A]

(e) Did you discuss whether the data you are using/curating contains personally identiﬁable

information or offensive content? [N/A]

5. If you used crowdsourcing or conducted research with human subjects...

(a) Did you include the full text of instructions given to participants and screenshots, if

applicable? [N/A]

(b) Did you describe any potential participant risks, with links to Institutional Review

Board (IRB) approvals, if applicable? [N/A]

(c) Did you include the estimated hourly wage paid to participants and the total amount

spent on participant compensation? [N/A]

Supplementary Information

We use R to denote the set of real numbers and N to denote the set of natural numbers. We use (cid:107).(cid:107)
to denote the Euclidean norm for vectors and the Frobenius norm for matrices, while using bold
symbols to illustrate matrices and vectors. We deﬁne an interval [0, K), K ∈ N and let p(Q) be the
distribution over all the tasks observed in this interval. For any k ∈ [0, K), we deﬁne a parametric
model g(.) with yk = g(xk; θk), where θk is a vector comprising all parameters of the model with
xk ∈ X k. Let n be the number of samples and m be the number of dimensions. Suppose a task
at k|k ∈ [0, K) is observed and denoted as Qk : Qk ∼ p(Q), where Qk = {X k, (cid:96)k} is a tuple
with X k ∈ Rnm being the input data and (cid:96)k quantiﬁes the loss incurred by X k using the model g
for the task at k. We denote a sequence of θk as uk:K = {θτ ∈ Ωθ, k ≤ τ ≤ K}, with Ωθ being
the compact (feasible ) set for the parameters. We denote the optimal value with a superscript (∗);
for instance, we use θ(∗)
to denote the optimal value of θk at task k. In this paper we use balance
k
point, equilibrium point, and saddle point to refer to the point of balance between generalization and
forgetting. We interchange between these terms whenever convenient for the discussion. We will use
∇(j)i to denote the gradient of i with respect to j and ∆i to denote the ﬁrst difference in discrete
time.

7 Additional Results

We deﬁne the cost (combination of catastrophic cost and generalization cost ) at any instant k as
Jk(θk) = γk(cid:96)k + (cid:80)k−1
τ =0 γτ (cid:96)τ , where (cid:96)τ is computed on task Qτ with γτ describing the contribution
of Qτ to this sum. We will show that for any ﬁxed k, the catastrophic forgetting cost Jk(θk) is
divergent in the limit k → ∞ if equal contribution from each task is expected.
Lemma 1. For any k ∈ N, deﬁne Jk(θk) = (cid:80)k
L ≥ (cid:96)τ ≥ (cid:15), ∀τ, (cid:15) > 0 and let γτ = 1. Then Jk(θk) is divergent as k → ∞.

τ =0 γτ (cid:96)τ . For all τ, assume (cid:96)τ to be continuous with

Proof of Lemma 1. With Jk(θk)
limk→∞
divergent.

(cid:80)k

τ =0 γτ (cid:15), where γτ = 1 which provides limk→∞

= (cid:80)k

τ =0 γτ (cid:96)τ we write

limk→∞

≥
τ =0 (cid:15) = ∞. Therefore, Jk(θk) is

τ =0 γτ (cid:96)τ

(cid:80)k

(cid:80)k

When (cid:96)τ ≥ (cid:15) with (cid:15) > 0 implies that each task incurs a nonzero cost. Furthermore, γτ = 1, it
implies that each task provides equal contribution to the catastrophic forgetting cost and contributed
nonzero value to Jk(θk). The aforementioned lemma demonstrates that equivalent performance (no
forgetting on all tasks ) cannot be guaranteed for an inﬁnite number of tasks when each task provides
a nonzero cost to the sum (you have to learn for all the tasks ). However, if the task contributions are
prioritized based on knowledge about the task distribution, the sum can be ensured to be convergent
as shown in the next corollary.
Corollary 1. For any k ∈ N, deﬁne Jk(θk) = (cid:80)k
that (cid:15) ≤ (cid:96)τ ≤ L, ∀(cid:15) > 0. Deﬁne N = 1
when there are inﬁnite number of tasks, limN→∞
is convergent.

τ =0 γτ (cid:96)τ where (cid:96)τ is continuous and bounded such
k and choose γN such that γN → 0, N → ∞ and assume
N γN ≤ M. Under these assumptions, Jk(θk)

(cid:80)

15

Proof of Corollary 1. Since (cid:96)τ ≤ L, Jk(θk) = limk→∞
Llimk→∞

(cid:80)k

τ =0 γτ .
(cid:80)k

Since limk→∞
Jk(θk) is upper bounded by LM . As a result, Jk(θk) is convergent since Jk(θk) is a monotone.

k , therefore limk→∞

τ =0 γτ = limN→∞

τ =0 γτ ≤ M and

N γN as N = 1

(cid:80)k

(cid:80)

(cid:80)k

τ =0 γτ (cid:96)τ ≤ limk→∞

(cid:80)k

τ =0 γτ L ≤

To solve the problem at k, we seek θk to minimize Jk(θk). Similarly, to solve the problem in the
complete interval [0, K), we seek a θk to minimize Jk(θk) for each k ∈ [0, K). In other words we
seek to obtain θk for each task such that the cost Jk(θk) is minimized. The optimization problem
for the overall CL problem (overarching goal of CL ) is then provided as the minimization of the
cumulative cost Vk(uk:K) = (cid:80)K

τ =k βτ Jτ (θτ ) such that V (∗)

, is given as

k

V (∗)
k = minuk:K Vk(uk:K),
(9)
with 0 ≤ βτ ≤ 1 being the contribution of Jτ and uk:K being a weight sequence of length K − k.
We will now derive the difference equation for our cost formulation.
Proposition 1. For any k ∈ [0, K), deﬁne Vk = (cid:80)K
τ =k βτ Jτ (θτ ) with θτ ∈ Ω. Deﬁne uk:K =
{θτ ∈ Ω, k ≤ τ ≤ K},, with Ω being the compact (feasible ) set as a sequence of parameters with
length K − k and V (∗)
∆V (∗)

k = −minθk∈Ω
represents the ﬁrst difference due to the introduction of a task, ∆θk due to parameters

where ∆V (∗)
and ∇xk due to the task data with βk ∈ R ∪ [0, 1], ∀k and Jk(θk) = γk(cid:96)k + (cid:80)k−1

τ =k βτ Jτ (θτ ). Then, the following is true

(cid:2)βkJk(θk) + (cid:0)(cid:104)∇θk V (∗)

, ∆θk(cid:105) + (cid:104)∇xk V (∗)

k = minuk:K

, ∆xk(cid:105)(cid:1)(cid:3),

(cid:80)K

(10)

τ =0 γτ (cid:96)τ .

k

k

k

Proof. Given V (∗)
[k + 1, K) to write

k = minuk:K

(cid:80)K

τ =k βτ Jτ (θτ ), we split the interval [k, K) as [k, k + 1) and

V (∗)
k = minθτ ∈Ω

(cid:2)βkJk(θk)(cid:3) + minuk+1:K

(cid:2)

K
(cid:88)

τ =k+1

βτ Jτ (θτ )(cid:3).

Vk
=
minuk+1:K

(cid:80)K

τ =k βτ Jτ (θτ )
τ =k+1 βτ Jτ (θτ )(cid:3) is V (∗)

provides (cid:80)K
k+1. We then achieve

(cid:2) (cid:80)K

τ =k+1 βτ Jτ (θτ )

is

Vk+1

therefore

V (∗)
k = minθk∈Ω

(cid:2)βkJk(θk) + V (∗)

k+1

(cid:3).

Since the minimization is with respect to k now, the terms in k + 1 can be pulled into of the bracket
without any change to the minimization problem. We then approximate V (∗)
k+1 using the information
provided at k. Since V (∗)
k+1 is a function of yk, which is then a function of (k, xk, θk), and all changes
in yk can be summarized through (k, xk, θk). Therefore, a Taylor series of V (∗)
k+1 around (k, xk, θk)
provides

k+1 = V (∗)
V (∗)
+ (cid:104)∇xk V (∗)

k + (cid:104)∇θk V (∗)

, ∆θk(cid:105)
k
, ∆xk(cid:105) + (cid:104)∇k(V (∗)

k
where · · · summarizes all the higher order terms. As k ∈ N and (cid:104)∇k(V (∗)
difference in V (∗)

. We therefore achieve

k

k

k

hitherto denoted by ∆V (∗)
k+1 = V (∗)
V (∗)

k

k + (cid:104)∇θk V (∗)

k

+ (cid:104)∇xk V (∗)

k

, ∆ θk(cid:105)
, ∆xk(cid:105) + ∆V (∗)

k + · · · ,

), ∆k(cid:105) + · · · ,

(11)

), ∆k(cid:105) represents the ﬁrst

Substitute into the original equation to get
V (∗)
k = minθk∈Ω
+ (cid:104)∇xk V (∗)

(cid:2)βkJk(θk)(cid:3) + (cid:0)V (∗)
(cid:1) + · · · ,

, ∆xk(cid:105) + ∆V (∗)

k + (cid:104)∇θk V (∗)

k

, ∆θk(cid:105)

k
Cancel common terms and assume that the higher order terms (· · · ) are negligible to obtain
(cid:2)βkJk(θk) + (cid:104)∇θk V (∗)

, ∆θk(cid:105) + (cid:104)∇xk V (∗)

k = −minθk∈Ω

, ∆xk(cid:105)(cid:3).

∆V (∗)

k

k

k

which is a difference equation in V (∗)

k

.

16

(12)

(13)

(14)

k

is the minima for the overarching CL problem and ∆V (∗)

Note that V (∗)
V (∗)
k
(∆V (∗)
the optimal solution on all previous tasks is optimal on the new task as well.

represents the change in
upon introduction of a task (we hitherto refer to this as perturbations ). Zero perturbations
k = 0) implies that the introduction of a new task does not impact our current solution; that is,

k

The solution of the CL problem can directly be obtained by solving Eq. (14) using all the available data.
Thus, minθk∈Ω
k ≈ 0 for β > 0, with H(∆xk, θk) = βkJk(θk) +
(cid:104)∇θk V (∗)
, ∆θk(cid:105) + (cid:104)∇xk V (∗)
, ∆xk(cid:105). Essentially, minimizing H(∆xk, θk) would minimize the
perturbations introduced by any new task.

(cid:2)H(∆xk, θk)(cid:3) yields ∆V (∗)

k

k

We simulate worst-case discrepancy by iteratively updating ∆xk through gradient ascent, thus maxi-
mizing generalization. Next, we minimize forgetting under maximum generalization by iteratively
updating θk through gradient descent. To formalize our idea, let us indicate the iteration index at k
k and θk as θ(i)
by i and write ∆xk as ∆x(i)
k ) (for simplicity of
notation, we will denote H(∆x(i)
k , θ(i)
k ) as H whenever convenient ). Towards these updates, we
will ﬁrst get an upper bound on H(∆x(i)
k , θ(i)
Proposition 2. Let k ∈ [0, K) and deﬁne H(∆x(i)
k (cid:105) assume that ∇θk V (∗)
(cid:104)∇x(i)

k ) and solve the upper bounding problem.
k ) = βkJk(θ(i)

k (cid:105) +
k ). Then the following approximation is true:

k with H(∆xk, θk) as H(∆x(i)

k ≤ ∇θk Jk(θ(i)

k ) + (cid:104)∇θk V (∗)

k , θ(i)

k , θ(i)

, ∆x(i)

, ∆θ(i)

V (∗)
k

k

k

k , θ(i)

H(∆x(i)

k ) ≤ βkJk(θ(i)

k )) + (Jk+ζ(θ(i)
k ) + (Jk(θ(i+ζ)
where βk ∈ R ∪ [0, 1], ∀k and ζ ∈ N and Jk+ζ indicates ζ updates on ∆x(i)
updates on θ(i)
k .

) − Jk(θ(i)

k

k ) − Jk(θ(i)
k )),
k and θ(i+ζ)

k

(15)

indicates ζ

Proof. Consider H(∆x(i)
k ≤ ∇θk Jk(θ(i)
∇θk V (∗)

k ) = βkJk(θ(i)

k , θ(i)
k )+(cid:104)∇θk V (∗)
k ) we may write through ﬁnite difference approximation as

k (cid:105)+(cid:104)∇x(i)

, ∆θ(i)

V (∗)
k

k

k

, ∆x(i)

k (cid:105). Assuming

(cid:104)∇θ(i)

k

V (∗)
k

, ∆θ(i)

k (cid:105) ≤ (cid:104)∇θ(i)

k

Jk(θ(i)

k ), ∆θ(i)
k (cid:105),
) − Jk(θ(i)

k ))

≤ (Jk(θ(i+ζ)

k

Similarly, we may write

(cid:104)∇xk V (∗)

k

, ∆xk(cid:105) ≤

(cid:104)∇xk Jk(θ(i)

k ), ∆xk(cid:105),
k )).

k ) − Jk(θ(i)

≤ (Jk+ζ(θ(i)

Upon substitution, we have our result:

H(∆x(i)

k , θ(i)

k ) ≤ βkJk(θ(i)

k ) + (Jk(θ(i+ζ)

k

) − Jk(θ(i)

k )) + (Jk+ζ(θ(i)

k ) − Jk(θ(i)

k )).

(16)

(17)

(18)

Our cost to be analyzed will be given as
k ) = βkJk(θ(i)
k , θ(i)

and use this deﬁnition of H(∆x(i)

H(∆x(i)

k , θ(i)

k ) from here on.

k ) + (cid:104)∇θ(i)

k

V (∗)
k

, ∆θ(i)

k (cid:105) + (cid:104)∇x(i)

k

V (∗)
k

, ∆x(i)

k (cid:105).

(19)

8 Main results

k , θ(i)

We will deﬁne two compact sets Ωθ, Ωx and seek to show existence and stability of a saddle
point (∆x(i)
k ) for a ﬁxed k. To illustrate the theory, we refer to Fig. 4, for each k, the initial
values for the two players are characterized by the pair {θ(i)
k (red circle )}, and
H(∆x(i)
k , θ(i)
k ) is indicated by the grey circle on the cost curve (the dark blue curve ). Our prooﬁng
strategy is as follows.
First, we ﬁx θ(.)
to ∆x(i)
k .

k , Ωx}, to prove that H is maximizing with respect

k ∈ Ωθ and construct Mk = {θ(.)

k (blue circle ), ∆x(i)

17

Figure 4: Illustration of proofs. ∆x (player 1 ) is the horizontal axis and the vertical axis indicates θ (player 2 )
where the curve indicates H. If we start from red circle for player 1 (player 2 is ﬁxed at the blue circle ) H is
increasing (goes from a grey circle to a red asterisk ) with player 1 reaching the red star. Next, start from the
blue circle (θ is at the red star ), the cost decreases.

Lemma 2. For each k ∈ [0, K), ﬁx θ(.)
being the sets of all feasible θ(.)
k and x(i)
(∆x(i)
k ) ∈ Mk and consider

k , θ(.)

k ∈ Ωθ and construct Mk = {Ωx, θ(.)
k respectively. Deﬁne H(∆x(i)

k } with Ωθ, Ωx
k ) as in Eq. (19) for

k , θ(.)

∆x(i+1)
k

− ∆x(i)

H(∆x(i)

k , θ(.)

k ))/(cid:107)∇∆x(i)

k ∇∆x(i)

k = α(i)
V (∗)
k ≤ ∇x(i)

k

Consider the assumptions ∇x(i)
k , θ(.)
∞. It follows that H(∆x(i)

Jk and (cid:104)∇x(i)
k ) converges asymptotically to a maximizer.

Jk, ∇x(i)

k

k

k

k

k

k )(cid:107)2.

H(∆x(i)

k , θ(.)
Jk(cid:105) > 0, and let α(i)

k → 0, i →

Proof. Fix θ(.)
hood. Therefore, for (∆x(i+1)
k
expansion of H(∆x(i+1)
, θ(.)
k ) = H(∆x(i)
H(∆x(i+1)

k ∈ Ωθ and construct Mk such that Mk = {θ(.)
k , θ(.)
k ), (∆x(i)
k ) around H(∆x(i)
k , θ(.)

k , Ωx} which we call a neighbor-
k ) ∈ Mk we may write a ﬁrst-order Taylor series
k , θ(.)
k ) + (cid:104)∇∆x(i)
H(∆x(i)

k ), ∆x(i+1)

H(∆x(i)

k
, θ(.)

k , θ(.)

− ∆x(i)

k ) as

, θ(.)

k (cid:105).

(20)

k

k

k

We substitute the update as α(i)
k

∇

∆x

(i)
k

(cid:107)∇

∆x

(i)
k

k ,θ(.)
k )
k )(cid:107)2 to get
k ,θ(.)

H(∆x(i)

∇∆x(i)
(cid:107)∇∆x(i)

k

k

H(∆x(i)

H(∆x(i)

k , θ(.)
k )
k , θ(.)

k )(cid:107)2

(cid:105).

H(∆x(i+1)

k

, θ(.)

k ) − H(∆x(i)

k , θ(.)

k ) = (cid:104)∇∆x(i)

k

H(∆x(i)

k , θ(.)

k ), α(i)

k

The derivative ∇∆x(i)

k

H(∆x(i)

k , θ(.)

k ) can be written as

∇∆x(i)

k

H(∆x(i)

k )) ≤ ∇∆x(i)

k

k , θ(.)
(cid:105)
, ∆x(i)
k (cid:105)

V (∗)
k

= ∇x(i)

k

V (∗)
k ≤ ∇x(i)

k

Jk.

(cid:104)

βkJk(θ(.)

k ) + (cid:104)∇θ(.)

k

V (∗)
k

, ∆θ(.)
k (cid:105)

+ (cid:104)∇x(i)

k

Substitution reveals

H(∆x(i+1)

k

, θ(.)

k ) − H(∆x(i)

k , θ(.)

k ) = α(i)

k

18

Jk(cid:105)

(cid:104)∇x(i)

k

Jk, ∇x(i)
Jk(cid:107)2

k

(cid:107)∇x(i)

k

(21)

(22)

(23)

for α(i)

k > 0; and under the assumption that (cid:104)∇x(i)

Jk, ∇x(i)

k

Jk(cid:105) > 0 we obtain

k

H(∆x(i+1)

k

, θ(.)

k ) − H(∆x(i)

k , θ(.)

k ) = α(i)

k

(cid:104)∇x(i)

k

Jk, ∇x(i)
Jk(cid:107)2

k

(cid:107)∇x(i)

k

Jk(cid:105)

≥ 0.

(24)

Let Bx = α(i)
k

(cid:104)∇xk Jk,∇xk Jk(cid:105)
(i)
k

(cid:107)∇

x

Jk(cid:107)2 ≤ α(i)
, θ(.)

k and therefore 0 ≤ H(∆x(i+1)
k , θ(.)
k ) − H(∆x(i)

k

k ) ≥ 0 and H(∆x(i)

k ) − H(∆x(i)
k , θ(.)

, θ(.)

k . Furthermore, under the assumption that α(i)

k ) is maximizing
k → 0, k → ∞, we have

k , θ(.)

k ) ≤ α(i)
k .

We therefore have H(∆x(i+1)
with respect to ∆x(i)
H(∆x(i+1)

k ) − H(∆x(i)

, θ(.)

k

k

k , θ(.)

k ) → 0 asymptotically and we have our result.

k ∈ Ωx and construct Nk = {Ωθ, ∆x(.)

k }, to prove that H is minimizing with

Similarly, we ﬁx ∆x(.)
respect to θ(i)
k .
Lemma 3. For each k ∈ [0, K), ﬁx ∆x(.)
any (∆x(i)
and let θ(i+1)

k ) ∈ Nk deﬁne H(∆x(i)
k = −α(i)
− θ(i)
k ∇θ(i)
k )(cid:107) ≤ L2 and let α(i)

k
Jk+ζ(θ(i)

k , θ(.)

k

(cid:107)∇θ(i)

k

k ∈ Ωx and construct Nk = {∆x(.)

k , Ωθ}. Then for
(19) with Proposition. 2 being true
Jk(θ(i+ζ)
)(cid:107) ≤ L1 and
k

k , θ(.)
H(∆x(.)

k ) as in Eq.
k , θ(i)
k → 0, i → ∞. Then θ(i)

k )). Assume that (cid:107)∇θ(i)

k

k converges to a local minimizer.

Proof. First, we ﬁx ∆x(.)
k }. For any
k
(∆x(.)
k ), (∆x(.)
) ∈ Nk we write a ﬁrst-order Taylor series expansion of
H(∆x(.)

and construct Nk = {Ωθ, ∆x(.)

k , θ(i)

∈ Ωx

k

k , θ(i+1)

k
H(∆x(.)

k , θ(i+1)
) around H(∆x(.)
k , θ(i+1)

k , θ(i)
) = H(∆x(.)

k

k ) to write
k , θ(i)

H(∆x(.)

k , θ(i)

k ), θ(i+1)

k

− θ(i)

k (cid:105).

(25)

We then substitute θ(i+1)

k

− θ(i)

H(∆x(.)

k , θ(i+1)

k

) − H(∆x(.)

k = −α(i)
k , θ(i)

k ∇θ(i)
k ) = −α(i)

k

k (cid:104)∇θ(i)

k

k , θ(i)
H(∆x(.)

k ) to get
k , θ(i)

k ) + (cid:104)∇θ(i)
H(∆x(.)

k

k ), ∇θ(i)

k

H(∆x(.)

k , θ(i)

k )(cid:105).

(26)

Following Proposition 2, the derivative ∇θ(i)

k

H(∆x(.)

k , θ(i)

k ) can be written as

∇θ(i)

k

H(∆x(.)

k , θ(i)

k ) ≤ ∇θk [βkJk(θ(i)

k ) + (Jk(θ(i+ζ)

k

) − Jk(θ(i)

k )) + (Jk+ζ(θ(i)

k ) − Jk(θ(i)

k ))]

Simpliﬁcation reveals

∇θ(i)

k

H(∆x(.)

k , θ(i)

k )) ≤ ∇θ(i)

k

(βk − 2)Jk(θ(i)

k ) + ∇θ(i)

k

Jk(θ(i+ζ)
k

) + ∇θk Jk+ζ(θ(i)
k ).

Substitution therefore provides
k , θ(i+1)
k
k (cid:104)∇θ(i)
(βk − 2)Jk(θ(i)

H(∆x(.)
≤ −α(i)

k

∇θ(i)

k

) − H(∆x(.)
(βk − 2)Jk(θ(i)

k , θ(i)
k )
k ) + ∇θ(i)
Jk(θ(i+ζ)
k

k

k ) + ∇θ(i)

k

Opening the square with Cauchy’s inequality provides

Jk(θ(i+ζ)
) + ∇θ(i)
k
) + ∇θk Jk+ζ(θ(i)

k

k )(cid:105).

Jk+ζ(θ(i)

k ),

(27)

(28)

(29)

H(∆x(.)

k , θ(i+1)

k

) − H(∆x(.)

k , θ(i)

k ) ≤

+

+

+

(cid:107)∇θ(i)

k

2(cid:107)∇θ(i)

k

(βk − 2)Jk(θk)(cid:107)2

k

−α(i)
k

Jk(θ(i+ζ)
k

(cid:104)
(cid:107)∇θ(i)
)(cid:107)2 + (cid:107)∇θk Jk+ζ(θ(i)
Jk(θ(i+ζ)
k
)(cid:107)(cid:107)∇θk Jk+ζ(θ(i)

(βk − 2)Jk(θk)(cid:107)(cid:107)∇θ(i)
Jk(θ(i+ζ)
k

2(cid:107)∇θ(i)
(βk − 2)Jk(θk)(cid:107)(cid:107)∇θk Jk+ζ(θ(i)

k

k

k )(cid:107)
(cid:105)
k )(cid:107)

.

k )(cid:107)2

)(cid:107)

(30)

+ 2(cid:107)∇θ(i)

k

19

(31)

(32)

(33)

We simplify with Young’s inequality to achieve

H(∆x(.)

k , θ(i+1)

k

) − H(∆x(.)

k , θ(i)

k ) ≤ −α(i)

k

(cid:104)
(cid:107)∇θ(i)

k

(βk − 2)Jk(θk)(cid:107)2

+

+

+

+

+

+

+

+

Further simpliﬁcation results in

H(∆x(.)

k , θ(i+1)

k

) − H(∆x(.)

k , θ(i)

k ) ≤

+ 3(cid:107)∇θ(i)

k

k

)(cid:107)2

Jk(θ(i+ζ)
(cid:107)∇θ(i)
k
(cid:107)∇θk Jk+ζ(θ(i)
k )(cid:107)2
(βk − 2)Jk(θk)(cid:107)2

(cid:107)∇θ(i)

k

k

k

)(cid:107)2

)(cid:107)2

(cid:107)∇θ(i)

Jk(θ(i+ζ)
k
Jk(θ(i+ζ)
(cid:107)∇θ(i)
k
(cid:107)∇θk Jk+ζ(θ(i)
k )(cid:107)2
(βk − 2)Jk(θk)(cid:107)2
k )(cid:107)2(cid:105)
.

(cid:107)∇θk Jk+ζ(θ(i)

k

(cid:107)∇θ(i)

−α(i)
k
Jk(θ(i+ζ)
k

(cid:104)

3(cid:107)∇θ(i)

k

(βk − 2)Jk(θk)(cid:107)2
k )(cid:107)2(cid:105)
.

)(cid:107)2 + 3(cid:107)∇θk Jk+ζ(θ(i)

With the assumption that (cid:107)∇θ(i)

k

Jk(θ(i+ζ)
k

)(cid:107) ≤ L1 and (cid:107)∇θk Jk+ζ(θ(i)

k )(cid:107) ≤ L2,, we may write

(cid:34)

where Bθ =

((

√

√

3βk − 2

H(∆x(.)

k , θ(i+1)

k

) − H(∆x(.)
(cid:35)

k , θ(i)

k ) ≤ −α(i)

k Bθ,

3)2 + 3)L2

1 + 3L2
2

. Assuming that α(i)

k is chosen such that α(i)

k → 0, we

obtain H(∆x(.)
0. Therefore H converges to a local minimizer.

)−H(∆x(.)

k , θ(i+1)

k , θ(i)

k

k ) → 0 as i → ∞ and H(∆x(.)

k , θ(i+1)

k

)−H(∆x(.)

k , θ(i)

k ) <

From here on, we will deﬁne our cost function as H whereever convinient for simplicity of notations.
Since, for any k, there exists a local maximizer ∆x(∗)
k , Ωθ}
where the set Ωx is comprised of a local maximizer ∆x(∗)
k
Lemma 4. For any k ∈ [0, K), let θ(∗)
k = {Ωx, θ(∗)
and deﬁne M(∗)
k ), where ∆x(∗)
H(∆x(i)

k ∈ Ωθ, be the minimizer of H according to Lemma 3
k ) ≥

k ∈ Ωx, we may deﬁne N (∗)

is a maximizer for H according to Lemma. 2.

k }. Then for (∆x(∗)

k = {∆x(∗)

k , H(∆x(∗)

k ) ∈ M(∗)

k ), (∆x(i)

k , θ(∗)

k , θ(∗)

k , θ(∗)

k , θ(∗)

k

k = {Ωx, θ(∗)
, θ(∗)

Proof. By Lemma 3, for each k ∈ [0, K), there exists a minimizer θ(∗)
k
M(∗)
k }. Therefore by Lemma 2, H(∆x(i+1)
k , θ(∗)
(∆x(i+1)
k
Lemma 2. Then, for (∆x(∗)
by Lemma 2 which provides the result.

∈ Ωθ such that
k ) − H(∆x(i)
k ) ≥ 0 for
k ∈ Ωx be the converging point according to
k ) ≥ 0.

k . Let ∆x(∗)
k , θ(∗)

k ) ∈ M(∗)
k , θ(∗)

k ) − H(∆x(i)

k ) ∈ M(∗)

k ), (∆x(i)

k ), (∆x(i)

a H(∆x(∗)

k , θ(∗)

k , θ(∗)

k , θ(∗)

, θ(∗)

k

k

Lemma 5. For any k ∈ [0, K), let ∆x(∗)
k = {∆x(∗)
and deﬁne N (∗)
k ), where θ(∗)
k , θ(i)
H(∆x(∗)

k , Ωθ}. Then for (∆x(∗)

k , θ(∗)
is a minimizer for H according to Lemma. 2.

k , θ(i)

k ∈ Ωx, be the maximizer of H according to Lemma 2
k ) ≤

k , H(∆x(∗)

k ), (∆x(∗)

k ) ∈ N (∗)

k , θ(∗)

k

Proof. By Lemma 2, for each k ∈ [0, K), there exists a maximizer ∆x(∗)
N (∗)
(∆x(∗)

k , Ωθ}. Therefore by Lemma 3, H(∆x(∗)
), (∆x(∗)

k ∈ Ωx, such that
k ) ≤ 0 for
k ∈ Ωθ be the converging point according to Lemma 3.

k = {∆x(∗)
k , θ(i+1)

k . Let θ(∗)

) − H(∆x(∗)

k ) ∈ N (∗)

k , θ(i+1)

k , θ(i)

k , θ(i)

k

k

20

Then, for (∆x(∗)
which provides the result.

k , θ(∗)

k ), (∆x(∗)

k , θ(i)

k ) ∈ M(∗)

k , H(∆x(∗)

k , θ(∗)

k ) − H(∆x(∗)

k , θ(i)

k ) ≤ 0 by Lemma 3

Next, we prove that the union of the two neighborhoods for each k M(∗)
Lemma 6. For any k ∈ [0, K), let θ(∗)
deﬁne M(∗)
and deﬁne N (∗)

k ∈ Ωθ, be the minimizer of H according to Lemma 3 and
k ∈ Ωx, be the maximizer of H according to Lemma 2
k ∪ N (∗)

k }. Similarly, let ∆x(∗)
k , Ωθ}. Then, M(∗)

k = {Ωx, θ(∗)

k , is non-empty.

k = {∆x(∗)

k ∪ N (∗)

is nonempty.

k

k

Proof. Let M(∗)
H(∆x(i+1)
, θ(.)
5. Similarly, H(∆x(.)
also contradicts Lemma 4. Therefore, by contradiction, Mk ∪ Nk cannot be empty.

k ∪ N (∗)
, θ(.)
k ,
k ) is undeﬁned because the union is empty. This contradicts Lemma
k ∪ N (∗)

be empty. Then, for any (∆x(i+1)
k , θ(.)

k ∪ N (∗)
k )−H(∆x(i)
k , θ(i+1)

k ) for (∆x(.)

k ) ∈ M(∗)

k ) ∈ M(∗)

k ), (∆x(i)

) − H(∆x(.)

k , θ(i+1)

), (∆x(.)

k , θ(i)

k , θ(i)

k , θ(.)

k

k

k

k

k

8.1 Final Results

We are now ready to present the main results. We show that there exists an equilibrium point (Theo-
rem 1 ) and that the equilibrium point is stable (Theorem 2 ).
Theorem 3 (Existence of an Equilibrium Point). For any k ∈ [0, K), let θ(∗)
of H according to Lemma 5 and deﬁne M(∗)
maximizer of H according to Lemma 4 and deﬁne N (∗)
k , θ(∗)
be nonempty according to Lemma. 6, then (∆x(∗)

k ∈ Ωθ, be the minimizer
k ∈ Ωx, be the
k ∪ N (∗)
is a local equilibrium point.

k = {∆x(∗)
k ) ∈ M(∗)

k , Ωθ}. Further, let M(∗)

k }. Similarly, let ∆x(∗)

k = {Ωx, θ(∗)

k ∪ N (∗)

k

k

(34)

(35)

(36)

Proof. By Lemma 5 we have at (∆x(∗)

k , θ(∗)
k , θ(∗)
Similarly, according to Lemma 4, at (∆x(∗)

H(∆x(∗)

k ∪ N (∗)

k

that

k ) ∈ M(∗)
k , θ(i)
k ).
k ) ∈ M(∗)

k , θ(∗)

k , θ(i)

k ), (∆x(∗)
k ) ≤ H(∆x(∗)
k ), (∆x(i)
k , θ(∗)
k ) ≥ H(∆x(i)

k , θ(∗)
k ).

k ∪ N (∗)

k we have

H(∆x(∗)

k , θ(∗)

Putting these inequalities together, we get

H(∆x(∗)

k , θ(i)

k ) ≥ H(∆x(∗)

k , θ(∗)

k , θ(∗)
k ),

k ) ≥ H(∆x(i)
k , θ(∗)

k ) is a local equilibrium point in

which is the saddle point condition, and therefore (∆x(∗)
M(∗)

k ∪ N (∗)
k .

According to the preceeding theorem, there is at least one equillibrium point for the game summarized
by H.

∈ Ωθ be the initial values for ∆x(i)
k

Theorem 4 (Stability of
Ωx and θ(i)
k
Mk = {Ωx, Ωθ} with H(∆x(i)
α(i)
k × (∇∆x(i)
H(∆x(.)
∇θ(i)
a consequence of Lemma 2 and 3 (∆x(∗)

the Equilibrium Point). For any k ∈ [0, K), ∆x(i)
∈
k
Deﬁne
k =
k ×
k ). Let the existence of an equilibrium point be given by Theorem 1, then as

and θ(i)
k
Let ∆x(i+1)
k
− θ(i)

k ) given by Proposition 2.
k , θ(.)

− ∆x(i)
k = −α(i)

k , θ(i)
k ))/(cid:107)∇∆x(i)

k ) ∈ Mk is a stable equilibrium point for H.

k )(cid:107)2) and θ(i+1)

respectively.

k , θ(∗)

H(∆x(i)

H(∆x(i)

k , θ(i)

k , θ(.)

k

k

k

k

k ) ∈ Mk will reach (∆x(∗)

Proof. Consider now the order of plays by the two players. By Lemma 2, a game starting at
(∆x(i)
k ) which is a maximizer for H. Now, deﬁne Nk =
(∆x(∗)
k ) ∈ Nk will converge to (∆x(∗)
k ) ∈ Nk
according to Lemma 3. Since, Nk ⊂ Mk, our result follows.

k , θ(i)
k , Ωθ) ⊂ Mk then a game starting at (∆x(∗)

k , θ(∗)

k , θ(i)

k , θ(i)

21

9 Experimental Details

Much of this information is a repetition of details provided in [19, 42].

1. Incremental Domain Learning (IDL ): Incremental domain refers to the scenario when each
new task introduces changes in the marginal distribution of the inputs. This scenario has
been extensively discussed in the domain adaptation literature, where this shift in domain is
typically referred to as “non-stationary data distribution" or domain shift. Overall, we aim
to transfer knowledge from the old task to a new task where each task can be different in the
sense of their marginal distribution.

2. Incremental Class Learning (ICL ): In this scenario, each task contains an exclusive subset
of classes. The number of output nodes in a model equals the number of total classes in the
task sequence. For instance, tasks could be constructed by using exactly one class from the
MNIST data set where we aim to transfer knowledge from one class to another.

3. Incremental Task Learning (ITL ): In this setup, the output spaces are disjoint between
tasks[ for example, the previous task can be a classiﬁcation problem of ﬁve classes, while
the new task can be a regression. This scenario is the most generic and allows for the tasks
to be deﬁned arbitrarily. For each tasks, a model requires task-speciﬁc identiﬁert.

Split-MNIST For split-MNIST, the original MNIST-data set is split into ﬁve partitions where each
partition is a two-way classiﬁcation. We pick 60000 images for training (6000 per digit ) and 10000
images for test, i.e. (1000 per digit ). For the incremental task learning in the split-MNIST experiment,
the ten digits are split into ﬁve two-class classiﬁcation tasks (the model has ﬁve output heads, one for
each task ) and the task identity (1 to 5 ) is given for test. For the incremental class learning setup, we
require the model to make a prediction over all classes (digits 0 to 9 ). For the incremental domain
learning, the model always predicts over two classes.

Permuted-MNIST For permuted-MNIST, we permute the pixels in the MNIST data to create tasks
where each task is a ten-way classiﬁcation. The three CL scenarios that are generated for the permuted-
MNIST are similar to the Split-MNIST data set except for the idea that the different tasks are now
generated by applying random pixel permutations to the images. For incremental task learning, we
use a multi-output strategy, and each task is attached to a task identiﬁer. For incremental domain and
class, we use a single output strategy and each task as one where one of the 10 digits are predicted. In
incremental class learning, for each new task 10 new classes are generated by permuting the MNIST
data set. For incremental task and domain, we use a total of 10 tasks whereas for incremental classes,
we generate a total of 100 tasks.

Network Architecture We keep our architecture identical to what is provided in [19, 42]. The loss
function is categorical cross-entropy for classiﬁcation. All models were trained for 2 epochs per task
with a minibatch size of 128 using the Adam optimizer (β1 = 0.9, β2 = 0.999, learning rate= 0.001
) as the default. For BCL, the size of the buffer (i.e., a new task array ) DN (k) and a task memory
array (samples from all the previous tasks ) DP (k)) is kept equivalent to naive rehearsal and other
memory-driven approaches such as GEM and MER (16, 000 samples ).

Comparison Methods – Baseline Strategies Additional details can be found from [19, 42]

1. A sequentially-trained neural network with different optimizers such as SGD, Adam [24],

and Adagrad [12].

2. A standard L2−regularized neural network where each task is shown sequentially.
3. Naive rehearsal strategy (experience replay ) where a replay buffer is initialized and data
corresponding to all the previous tasks are stored. The buffer size is chosen to match the
space overhead of online EWC and SI.

Comparison Method-CL We compared the following CL methods:

1. EWC [25] / Online EWC [38] / SI [48]: For these methods, a regularization term is added
to the loss, with a hyperparameter used to control the regularization strength such that:
L(total) = L(current) + λL(regularization). λ is set through a hyperparameter.

2. LwF [28] / DGR [39] Here, we set the loss to be L(total) = αL(current) + (1 −
α)L(replay) where hyperparameter α is chosen according to how many tasks have been
seen by the model.

22

3. For RtF [42], MAS [2], GEM[30] and MER[36], we refer to the respective publication for

details.

Additional details about the experiments can be found in [19] as our paper retains their hyper-
parameters and the experimental settings.

References

[1] Elmira Amirloo Abolfathi, Jun Luo, Peyman Yadmellat, and Kasra Rezaee. CoachNet: An

adversarial sampling approach for reinforcement learning, 2021.

[2] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuyte-
laars. Memory aware synapses: Learning what (not) to forget. In Proceedings of the European
Conference on Computer Vision (ECCV), pages 139–154, 2018.

[3] Shawn Beaulieu, Lapo Frati, Thomas Miconi, Joel Lehman, Kenneth O Stanley, Jeff Clune, and

Nick Cheney. Learning to continually learn. arXiv preprint arXiv:2002.09571, 2020.

[4] Richard E Bellman. Adaptive control processes: a guided tour. Princeton university press,

2015.

[5] Massimo Caccia, Pau Rodriguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin, Lucas
Caccia, Issam Laradji, Irina Rish, Alexandre Lacoste, David Vazquez, and Laurent Charlin.
Online fast adaptation and knebowledge accumulation: a new approach to continual learning,
2021.

[6] Massimo Caccia, Pau Rodríguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin, Lucas
Caccia, Issam H. Laradji, Irina Rish, Alexande Lacoste, David Vázquez, and Laurent Charlin.
Online fast adaptation and knowledge accumulation: A new approach to continual learning.
ArXiv, abs/2003.05856, 2020.

[7] Gail A Carpenter and Stephen Grossberg. A massively parallel architecture for a self-organizing
neural pattern recognition machine. Computer vision, graphics, and image processing, 37(1):54–
115, 1987.

[8] Arslan Chaudhry, Naeemullah Khan, Puneet K Dokania, and Philip HS Torr. Continual learning

in low-rank orthogonal subspaces. arXiv preprint arXiv:2010.11635, 2020.

[9] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K
Dokania, Philip HS Torr, and Marc’Aurelio Ranzato. Continual learning with tiny episodic
memories. arXiv preprint arXiv:1902.10486, 2019.

[10] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K
Dokania, Philip HS Torr, and Marc’Aurelio Ranzato. On tiny episodic memories in continual
learning. arXiv preprint arXiv:1902.10486, 2019.

[11] Thang Doan, Mehdi Abbana Bennani, Bogdan Mazoure, Guillaume Rabusseau, and Pierre
Alquier. A theoretical analysis of catastrophic forgetting through the NTK overlap matrix. In
International Conference on Artiﬁcial Intelligence and Statistics, pages 1072–1080. PMLR,
2021.

[12] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning

and stochastic optimization. Journal of Machine Learning Research, 12(7), 2011.

[13] Sayna Ebrahimi, Franziska Meier, Roberto Calandra, Trevor Darrell, and Marcus Rohrbach.

Adversarial continual learning, 2020.

[14] Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li. Orthogonal gradient descent for

continual learning, 2019.

[15] Enrico Fini, Stéphane Lathuilière, Enver Sangineto, Moin Nabi, and Elisa Ricci. Online
continual learning under extreme memory constraints. In European Conference on Computer
Vision, pages 720–735. Springer, 2020.

[16] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-
tation of deep networks. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pages 1126–1135. JMLR. org, 2017.

[17] Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning.

arXiv preprint arXiv:1902.08438, 2019.

23

[18] Gunshi Gupta, Karmesh Yadav, and Liam Paull. La-MAML: Look-ahead meta learning for

continual learning, 2020.

[19] Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating continual
learning scenarios: A categorization and case for strong baselines. In NeurIPS Continual
learning Workshop, 2018.

[20] Khurram Javed and Martha White. Meta-learning representations for continual learning. In

Advances in Neural Information Processing Systems, pages 1818–1828, 2019.

[21] K J Joseph and Vineeth N Balasubramanian. Meta-consolidation for continual learning, 2020.

[22] Sangwon Jung, Hongjoon Ahn, Sungmin Cha, and Taesup Moon. Continual learning with
node-importance based adaptive group sparse regularization. arXiv e-prints, pages arXiv–2003,
2020.

[23] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence of similar

and dissimilar tasks. Advances in Neural Information Processing Systems, 33, 2020.

[24] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

[25] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,
Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.
Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy
of Sciences, 114(13):3521–3526, 2017.

[26] Jeremias Knoblauch, Hisham Husain, and Tom Diethe. Optimal continual learning has perfect
memory and is NP-hard. In International Conference on Machine Learning, pages 5327–5337.
PMLR, 2020.

[27] Frank L Lewis, Draguna Vrabie, and Vassilis L Syrmos. Optimal control. John Wiley & Sons,

2012.

[28] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern

Analysis and Machine Intelligence, 40(12):2935–2947, 2017.

[29] Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and

teaching. Machine Learning, 8(3-4):293–321, 1992.

[30] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning.

In Advances in neural information processing systems, pages 6467–6476, 2017.

[31] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning.

arXiv preprint arXiv:1706.08840, 2017.

[32] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Razvan Pascanu, and Hassan Ghasemzadeh. Under-
standing the role of training regimes in continual learning. arXiv preprint arXiv:2006.06958,
2020.

[33] Anusha Nagabandi, Chelsea Finn, and Sergey Levine. Deep online learning via meta-learning:

Continual adaptation for model-based RL, 2019.

[34] Alex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms,

2018.

[35] Pingbo Pan, Siddharth Swaroop, Alexander Immer, Runa Eschenhagen, Richard E Turner, and
Mohammad Emtiyaz Khan. Continual deep learning by functional regularisation of memorable
past. arXiv preprint arXiv:2004.14070, 2020.

[36] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Ger-
ald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing
interference. arXiv preprint arXiv:1810.11910, 2018.

[37] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick,
Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv
preprint arXiv:1606.04671, 2016.

[38] Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska,
Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework
for continual learning. In International Conference on Machine Learning, pages 4528–4537.
PMLR, 2018.

24

[39] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep

generative replay. arXiv preprint arXiv:1705.08690, 2017.

[40] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In

Advances in neural information processing systems, pages 4077–4087, 2017.

[41] Michalis K Titsias, Jonathan Schwarz, Alexander G de G Matthews, Razvan Pascanu, and
Yee Whye Teh. Functional regularisation for continual learning with Gaussian processes. arXiv
preprint arXiv:1901.11356, 2019.

[42] Gido M. van de Ven and Andreas S. Tolias. Generative replay with feedback connections as a

general strategy for continual learning, 2019.

[43] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for
one shot learning. Advances in Neural Information Processing Systems, 29:3630–3638, 2016.
[44] Risto Vuorio, Dong-Yeon Cho, Daejoong Kim, and Jiwon Kim. Meta continual learning. CoRR,

abs/1806.06928, 2018.

[45] Huaxiu Yao, Longkai Huang, Ying Wei, Li Tian, Junzhou Huang, and Zhenhui Li. Don’t
overlook the support set: Towards improving generalization in meta-learning. arXiv preprint
arXiv:2007.13040, 2020.

[46] Dong Yin, Mehrdad Farajtabar, Ang Li, Nir Levine, and Alex Mott. Optimization and general-
ization of regularization-based continual learning: a loss approximation viewpoint, 2020.
[47] Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with

dynamically expandable networks. arXiv preprint arXiv:1708.01547, 2017.

[48] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic
intelligence. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pages 3987–3995. JMLR. org, 2017.

The submitted manuscript has been created by UChicago Argonne, LLC, Operator of
Argonne National Laboratory (“Argonne”). Argonne, a U.S. Department of Energy Of-
ﬁce of Science laboratory, is operated under Contract No. DE-AC02-06CH11357. The
U.S. Government retains for itself, and others acting on its behalf, a paid-up nonexclu-
sive, irrevocable worldwide license in said article to reproduce, prepare derivative works,
distribute copies to the public, and perform publicly and display publicly, by or on be-
half of the Government. The Department of Energy will provide public access to these
results of federally sponsored research in accordance with the DOE Public Access Plan.
http://energy.gov/downloads/doe-public-access-plan

25

