An Adaptive Synaptic Array using Fowler-Nordheim Dynamic 
Analog Memory 
Darshit Mehta1, Kenji Aono2 and Shantanu Chakrabartty1,2,* 

Abstract 
In this paper we present a synaptic array that uses dynamical states to implement an analog memory for 
energy-efficient  training  of  machine  learning  (ML)  systems.  Each  of  the  analog  memory  elements  is  a 
micro-dynamical  system  that  is  driven  by  the  physics  of  Fowler-Nordheim  (FN)  quantum  tunneling, 
whereas  the  system  level  learning  modulates  the  state  trajectory  of  the  memory  ensembles  towards  the 
optimal solution. We show that the extrinsic energy required for modulation can be matched to the dynamics 
of learning and weight decay leading to a significant reduction in the energy-dissipated during ML training. 
With the energy-dissipation as low as 5 fJ per memory update and a programming resolution up to 14 bits, 
the proposed synapse array could be used to address the energy-efficiency imbalance between the training 
and the inference phases observed in artificial intelligence (AI) systems. 

Introduction 

Implementation of reliable and scalable synaptic weights or memory remains an unresolved challenge 
in  the  design  of  energy-efficient  machine  learning  (ML)  and  neuromorphic  processors  [1].  Ideally,  the 
synaptic  weights  should  be  “analog”  and  should  be  implemented  on  a  non-volatile,  easily  modifiable 
storage device [2]. Furthermore, if these memory elements are integrated in proximity with the computing 
circuits or processing elements, then the resulting compute-in-memory (CIM) architecture [3, 4] has the 
potential to mitigate the “memory wall” [5, 6, 7] which refers to the energy-efficiency bottleneck in ML 
processors that arises due to repeated memory access. In most practical and scalable implementations, the 
processing  elements  are  implemented  using  CMOS  circuits;  as  a  result,  it  is  desirable  that  the  analog 
synaptic weights be implemented using a CMOS-compatible technology. In literature, several multi-level 
non-volatile memory devices have been proposed for implementing analog synapses. These include the 
cross-bar  memristor  based  resistive  random-access  memories  (RRAM)  [8],  magnetic  random-access 
memories (MRAM) [9], Phase Change Memory (PCM) [10], Spin Torque Transfer RAM (STTRAM) [11], 
Conductive  Bridge  RAM  [12]  or  the  three  terminal  devices  like  the  floating-gate  transistors  [13], 
ferroelectric  field-effect  transistor-based  RAM  (FeRAM)  [14],  Charge  Trap  Memory  [15]  and 
Electrochemical RAMs (ECRAM) [16]. In all these devices the analog memory states are static in nature, 
where each state needs to be separated from others by an energy barrier ΔE. In non-volatile storage, it is 
critical that this energy-barrier is chosen to be large enough to prevent memory leakage due to thermal-
fluctuations  or  other  environmental  disturbances.  For  example,  in  memristive  devices  the  state  of  the 
conductive filament between two electrodes determines the stored analog value, whereas in charge-based 
devices like floating-gates or FeFET, the state of polarization determines the analog value. At a fundamental 
level, the energy dissipated to transition between different analog states is determined by the energy-barrier 
ΔE. For example, switching the RRAM memory state requires 100 fJ per bit [17], whereas STT_MRAM 
requires about 4.5pJ per bit [18]. A learning/training algorithm that adapts its weights in quantized steps 

1Department of Biomedical Engineering, Washington University in St. Louis 
2Department of Electrical and Systems Engineering, Washington University in St. Louis 
*Corresponding author: shantanu@wustl.edu 

(…,Wn-1, Wn, Wn+1, …) towards a target solution (or local extrema), must dissipate energy (…,ΔEn-1, ΔEn, 
ΔEn+1, …)  for memory updates, as shown in Fig. 1(a).  

Figure  1:  Motivation  and  principle  of  operation  for  the  proposed  synaptic  memory  device:  (a)  conventional  non-volatile 
analog memory where transition between analog static states dissipates energy; (b) Dynamic analog memory where an external 
energy  is  used  to  modulate  the  trajectory  of  the  memory  states  towards  the  optimal  solution;  (c)  desired  analog  synapse 
characteristic where the memory retention rate is traded-off with the write energy; reducing the energy dissipation per weight 
update  in  training  phase  by  matching  the  dynamics  of  the  dynamic  analog  memory  to  the  weight  decay;  (d)  micrograph  of  a 
fabricated DAM array along with (e) its equivalent circuit where the leakage current IFN is implemented by (f) the electron transport 
across a Fowler-Nordheim (FN) tunneling barrier; (g) Implementation of the FN tunneling based DAM where dynamic states g1-
g3 determines the energy dissipated per memory update and memory retention rate.  

In this paper we present a synaptic memory device that uses dynamical states (instead of static states) 
to implement analog memory in an effort to improve the energy-efficiency of ML training. The core of the 
proposed  device  is  itself  a  micro-dynamical  system  and  the  system-level  learning/training  process 
modulates the dynamical state (or state trajectory) of the memory ensembles. The concept is illustrated in 
Fig.  1(b),  which  shows  a  reference  ensemble  trajectory  that  continuously  decays  towards  a  zero  vector 

2 

t2t3ECEFtimeelectron gasFN Barriern-siliconFloating-gatestate0W*ΔEnΔE1ΔE2ΔEnL(W)Wn-1WnW020μm(a)(b)(d)Sense Buffers(g)FN Synaptic ArraytimeΔE1t1t2t3SETRESETΔE2ΔE3t1(f)(g1)(g2)(g3)(e)SETRESETIFN(WS)IFN(WR)CFGCFGCCCCREFWSWRVSVRtimevalueTrainingWeight Update RateEnergy dissipation per weight updateWeight RetentionInference(c) 
 
without the presence of any external modulation. However, during the process of learning, the trajectory of 
the memory ensemble is pushed towards an optimal solution W*. The main premise of this paper is that the 
extrinsic energy (… ,ΔEn-1, ΔEn, ΔEn+1, …) required for modulation, if matched to the dynamics of learning, 
could reduce the energy-budget for ML training. This is illustrated in Fig. 1(c) which shows a convergence 
plot corresponding to a typical ML system as it transitions from a training phase to an inference phase. 
During the training phase, the synaptic weights are adapted based on some learning criterion whereas in the 
inference phase the synaptic weights remain fixed or are adapted intermittently to account for changes in 
the operating conditions. Generally, during the training phase the amount of weight updates is significantly 
higher than in the inference phase, as a result, memory update operations require a significant amount of 
energy.  Take  for  example  support-vector machine  (SVM)  training, the  number  of  weight  updates  scale 
quadratically with the number of support vectors and the size of the training data, whereas adapting the 
SVM during inference only scales linearly with the number of support-vectors [19]. Thus, for a constant 
energy dissipation per update, the total energy-dissipated due to weight updates is significantly higher in 
training than during inference. However, if the energy-budget per weight updates could follow a temporal 
profile as shown in Fig.1c, wherein the energy dissipation is no longer constant, but inversely proportional 
to the expected weight update rate, then the total energy dissipated during training could be significantly 
reduced. One way to reduce the weight update or memory write energy budget is to trade-off the weight’s 
retention rate according to the profile shown in Fig. 1c. During the training phase, the synaptic element can 
tolerate lower retention rates or parameter leakage because this physical process could be matched to the 
process of weight decay or regularization, techniques commonly used in ML algorithms to achieve better 
generalization performance [20]. As shown in Fig. 1c, the memory’s retention rate should increase as the 
training progresses such that at convergence or  in the inference phase the  weights are stored on a  non-
volatile memory. 

In this paper we describe a dynamic analog memory (DAM) that can exhibit a temporal profile similar 
to that of Fig. 1c. Furthermore, the memory is implemented on a standard CMOS process without the need 
for any additional processing layers. Fig. 1e shows a micrograph of a DAM array and in the Supplementary 
Section I we describe the circuit implementation details. The proposed DAM requires a Fowler-Nordheim 
(FN) quantum-tunneling barrier which can be created by injecting sufficient electrons onto a polysilicon 
island  (floating-gate)  that  is  electrically  isolated  by  thin  silicon-di-oxide  barriers  [21].  As  the  electron 
tunnels through the triangular barrier, as shown in Fig. 1f, the barrier profile changes which further inhibits 
the tunneling of electrons. We have previously shown that the dynamics of this simple system is robust 
enough to implement time-keeping devices [22] and self-powered sensors [23]. In this paper, we use a pair 
of  synchronized  FN-dynamical  systems  to  implement  a  DAM  suitable  for  implementing  ML 
training/inference engines. Figure 1(f) shows the dynamics of two FN-dynamical systems, labeled as SET 
and  RESET,  whose  analog  states  continuously  and  synchronously  decay  with  respect  to  time.  In  our 
previous  work  [23],  we  have  shown  the  dynamics  across  different  FN-dynamical  systems  can  be 
synchronized with respect to each other with an accuracy greater than 99.9%. However, when an external 
voltage pulse modulates the SET system, as shown in Fig. 1f, the dynamics of the SET system becomes 
desynchronized with respect to the RESET system. The degree of desynchronization is a function of the 
state  of  the  memory  at  different  time  instances  (Fig.  1g,  insets  g1-g3)  which  determines  the  memory’s 
retention rate. For instance, at time-instant 𝑡1, a small magnitude pulse would produce the same degree of 
desynchronization  as  a  large  magnitude  pulse  at  the  time-instant  𝑡3.  However,  at  𝑡1  the  pair  of 

3 

 
desynchronized  systems  (SET  and  RESET)  would  resynchronize  more  rapidly  as  compared  to 
desynchronized  systems  at  time-instants  𝑡2  or  𝑡3.  This  resynchronization  effect  results  in  shorter  data 
retention; however, this feature could be leveraged to implement weight-decay in ML training. At time-
instant t3, the resynchronization effect is weak enough that the FN-dynamical system acts as a persistent 
non-volatile  memory  with  high  data-retention  time.  In  Methods  section,  we  derive  the  FN-dynamical 
system mathematical model and compare it to ML training formulation. We show that the energy required 
for updating the memory and its data retention capacity can be annealed according to the profile shown in 
Fig. 1c.  

Results 
Dynamic analog memory with an asymptotic non-volatile storage 

Figure 2: a) WS (solid line) and WR (dashed line) response at 3 different operating conditions (zoomed insets: a1, a2, a3). b-

d) DAM response calculated from the WS and WR voltage values.  

The dynamics of the FN-tunneling based DAM (or FN-DAM) were verified using prototypes fabricated 
in a standard CMOS process (micrograph shown in Fig. 1e.). The FN-DAM devices were programed and 
initialized through a combination of FN tunneling and hot electron injection. Detailed description of the 
general  programming  process  can  be  found  in  [23]  with  implementation  specific  notes  in  the  Methods 
section. The tunneling nodes (WS and WR in Fig. 1e) were initialized to around 8 V and decoupled from the 
readout node by a decoupling capacitor to the sense buffers (shown in supplementary Fig. 1). The readout 
nodes were biased at a lower voltage (~3 V) to prevent hot electron injection [24] onto the floating gate 
during  readout  operation.  Fig.  2  shows  the  measured  dynamics  of  the  FN-DAM  device  in  different 
initialization regimes used in ML training, as described in Fig. 1c. The different regimes were obtained by 

4 

010020030040050060070080090010002.82.933.1Time (s)Time (s)Time (s)y (mV)y (mV)y (mV)05000.5105000.5105000.51Out (V)Time (s)(a)(a1)(a2)(a3)(b)(c)(d) 
 
initializing the tunneling nodes (WS and WR) to different voltages (see Methods section), whilst ensuring 
that the tunneling rates on the WS and WR nodes were equal. Initially (during the training phase), tunneling-
node voltages were biased high (readout node voltage of 3.1 V), leading to faster FN tunneling (Fig. 2, inset 
a).  A  square  input  pulse  of  100  mV  magnitude  and  500  ms  duration  (5  fJ  of  energy)  was  found  to  be 
sufficient  to  desynchronize  the  SET  node  by  1  mV.  However,  as  shown  in  Fig.  2(b),  the  rate  of 
resynchronization in this regime is high leading to a decay in the stored weight down to 30% in 40 s. At t 
= 90 s, the voltage at node WS has reduced (readout node voltage of 2.9 V), and a larger voltage amplitude 
(500  mV)  is  required  to  achieve  the  same  desynchronization  magnitude  of  1  mV,  corresponding  to  an 
energy expenditure of 125 fJ. However, as shown in Fig. 2(c), the rate of resynchronization is low in this 
regime, leading to a decay in the stored weight down to 70% its value in 40 s. Similarly, at a later time 
instant t = 540 s, a 1 V signal desynchronizes the recorder by 1 mV, however as shown in Fig. 1(d), in this 
regime 95% of the stored weight value is retained after 40 s. This mode of operation is suitable during the 
inference phase of machine learning when the weights have already been trained, but the models need to 
be  sporadically  adapted  to  account  for  statistical  drifts.  Modeling  studies  described  in  Supplementary 
Section II shows that the write energy per update starts from as low as 5 fJ and increases to 2.5 pJ over a 
period of a period of 12 days. Supplementary Fig. 3 indicates that at lower WS/WR operating voltage (~ 6V) 
or  at  greater  instants  of  time  the  retention  time  of  FN-DAM  converges  to  that  of  other  FLASH  based 
memory. 

Figure 3: (a-b) FN-DAM response to SET pulses of varying frequency. c) Change in WS and WR potentials due to SET and 
RESET pulses. g) DAM response calculated as difference between WS and WR voltages. Error bars indicate standard deviation 
estimated across 12 devices.   

Each DAM in the FN-DAM device was programmed by independently modulating the SET and RESET 
junctions shown in Fig. 1(e). The corresponding WS and WR nodes were initially synchronized with respect 
to each other. After a programming pulse was applied to the SET or RESET control gate, the difference 
between the voltages at the WS and WR nodes were measured using an array of sense buffers. In results 
shown in Fig. 3a-d, a sequence of 100 ms SET and RESET pulses were applied. The measured difference 
between the voltages at the WS and WR nodes indicates the state of the memory. Each SET pulse increases 
the state while a RESET pulse decreases the state. In this way, the FN-device can implement a DAM that 

5 

SET InputRESET InputSETRESETy (mV)ΔV (mV)Time (s)y (mV)SET InputTime (s)(c)(b)(d)(a)(f)(e) 
 
is bidirectionally programmable with unipolar pulses. Fig. 3d also shows the cumulative nature of the FN-
DAM updates which implies that the device can work as an incremental/decremental counter. 

Fig.  3e-f  show  measurement  results  which  demonstrate  the  resolution  at  which  a  FN-DAM  can  be 
programmed as an analog memory. The analog state can be updated by applying digital pulses of varying 
frequency and variable number of pulses. In Fig. 3e, four cases of applying a 3 V SET signal for a total of 
100 ms are shown: a single 100 ms pulse; two 50 ms pulses; four 25 ms pulses; and eight 12.5 ms pulses. 
The results show the net change in the stored weight was consistent across the 4 cases. A higher frequency 
leads to a finer control of the analog memory updates. Note that any variations across the devices can be 
calibrated  or  mitigated  by  using  an  appropriate  learning  algorithm  [25].  The  variations  could  also  be 
reduced by using careful layout techniques and precise timing of the control signals.  

Characterization of FN-DAM 

Figure  4:  Device  characterization:  (a)  Change  in  DAM  response  with  each  pulse  of  same  magnitude  and  duration.  b)  DAM 
response to varying number of pulses. c) DAM response to pulses of different magnitude. d) device state drift due to temperature 
variations after 1,2 and 3 hours.  

The FN-DAM device can be programmed by changing the magnitude of the SET/RESET pulse or its 
duration (equivalently number of pulses of fixed duration). Fig. 4a shows response when the magnitude of 
the SET and RESET input signals varies from 4.1 V to 4.5 V. The measured response shown in Fig. 4a 
shows an exponential relationship with the amplitude of the signal. When short-duration pulses are used 

6 

Δy (mV)Cycle NumberSETRESETNumber of pulsesSETRESETPulse magnitude (V)SETRESETTemperature (oC)Δy (mV)Δy (mV)Δy (mV)(c)(b)(a)(d)1 hour2 hours3 hours(d) 
 
for programming, the stored value varies linearly with the number of pulses, as shown in Fig. 4b. However, 
repeated  application  of  pulses  with  constant  magnitude  produces  successively  smaller  change  in 
programmed  value  due  to  the  dynamics  of  the  DAM  device  (Fig.  4a).  One  way  to  achieve  a  constant 
response is to pre-compensate the SET/RESET control voltages such that a target voltage difference y = 
(WS  –  WR)  can  be  realized.  The  differential  architecture  increases  the  device  state  robustness  against 
disruptions from thermal fluctuations (Fig. 4d). The stored value on DAM devices will leak due to thermal-
induced processes or due to trap-assisted tunneling. However, in DAM, the weight is stored as difference 
in the voltages corresponding to WS and WR tunneling junctions which are similarly affected by temperature 
fluctuations. To verify this, we exposed the FN-DAM device to temperature ranging from 5 – 40 oC. Fig. 
4d shows that the DAM response is robust to temperature variation and the amount of desynchronization 
for a single recorder never exceeds 20 mV. When responses from multiple FN-DAM devices are pooled 
together, the variation due to temperature further reduces. 

FN-DAM based Classifier 

Figure 5: Synaptic memory for neuromorphic applications a) Test data set with randomly initialized decision boundary b) Decision 
boundary after training. c) Evolution of weights after 5 epochs. d) Input voltage required for initiating a unit change in weight. e) 
Energy expended in updating the weights. f) Average magnitude of weight update and average energy required for each epoch. 

In  this  section  we  experimentally  demonstrate the benefits  of  exploiting the  dynamics  of  FN-DAM 
weights when training a simple linear classifier. For this results, two FN-DAM devices were independently 
programmed according to the perceptron training rule [26]. We trained the weights of a perceptron model 
to classify a linearly separable dataset comprises 50 instances of two-dimensional vectors, shown in Fig. 
5a. During each epoch, the network loss function and gradients were evaluated for every training point in 

7 

Time (s)Time (s)Time (s)Epoch NumberVtrain(V)x1x1x2x2Energy (J/F)Weightsa.u.EnergyWt. updatew0w1wR,0wS,1wS,0wR,1E(w0)E(w1) 
 
a randomized order, with time interval between successive training points being two seconds. Fig. 5b shows 
that after training for 5 epochs, the learned boundary can correctly classify the given data. Fig. 5c shows 
the evolution of weights as a function of time. As can be noted in the figure, initially the magnitude of 
weight updates (negative of the cost function gradient) was high for the first 50 seconds, after which the 
weights stabilized and required smaller updates. The energy consumption of the training algorithm can be 
estimated based on the magnitude and number of the SET/RESET pulses required to carry out the required 
update for each misclassified point. As the SET/RESET nodes evolve in time, they require larger voltages 
for carrying out updates, shown in Fig. 5d. The gradient magnitude was mapped onto an equivalent number 
of 1 kHz pulses, rounding to the nearest integer. Fig. 5e shows the energy (per unit capacitance) required 
to carry out the weight update whenever a point was misclassified. Though the total magnitude of weight 
update decreased with each epoch, the energy required to carry out the updates had lower variation (Fig. 
5f). The relatively larger energy required for smaller weight updates at later epochs led to longer retention 
times of the weights (Supplementary Fig. 3).  

Discussions 

In this paper we reported a Fowler-Nordheim quantum tunneling based dynamic analog memory (FN-
DAM)  whose  physical  dynamics  can  be  matched  to  the  dynamics  of  weight  updates  used  in  machine 
learning (ML) or neural network training. During the training phase, the weights stored on FN-DAM are 
plastic in nature and decay according to a learning-rate evolution that is necessary for the convergence of 
gradient-descent training [27]. As the training phase transitions to an inference phase, the FN-DAM acts as 
a  non-volatile  memory.  As  a  result,  the  trained  weights  are  persistently  stored  without  requiring  any 
additional refresh steps (used in volatile embedded DRAM architectures [28]). The plasticity of FN-DAM 
during the training phase can be traded off with the energy-required to update the weights. This is important 
because the number of weight updates during training scale quadratically with the number of parameters, 
hence the energy-budget during training is significantly higher than the energy-budget for inference. The 
dynamics of FN-DAM bears similarity to the process of annealing used in neural network training and other 
stochastic optimization engines to overcome local minima artifacts [29]. Thus, it is possible that FN-DAM 
implementations or ML processors can naturally implement annealing without dissipating any additional 
energy.  If  such  dynamics  were  to  be  emulated  on  other  analog  memories,  it  would  require  additional 
hardware and control circuitry. In the Supplementary Section IV, we show that an FN-DAM based deep 
neural network (DNN) can achieve similar classification accuracy as a conventional DNN while dissipating 
significantly less energy during training. Note that for this demonstration, only the fully connected layers 
were trained while the feature layers were kept static. This mode of training is common for many practical 
DNN implementations on edge computing platforms where the goal is not only to improve the energy-
efficiency of inference but also for training [30]. 

Several challenges exist in scaling the FN-DAM to large neural-networks. Training a large-scale neural 
network could take days to months [31] depending on the complexity of the problem, complexity of the 
network, and the size of the training data. This implies that the FN-DAM dynamics need to match the long 
training durations as well. Fortunately, the 1/log characteristics of FN devices ensures that the dynamics 
could last for durations greater than a year [32] The other challenge that might limit the scaling of FN-DAM 
to large neural network is the measurement precision. The resolution of the measurement and the read-out 
circuits  limit  the  energy-dissipated  during  memory  access  and  how  fast  the  gradients  can  be  computed 

8 

 
(Supplementary  Fig.  5).  For  instance,  a  1  pF  floating-gate  capacitance  can  be  initialized  to  store  107 
electrons. Even if one were able to measure the change in synaptic weights for every electron tunneling 
event, the read-out circuits would need to discriminate 100 nV changes. A more realistic scenario would be 
measuring the change in voltage after 1000 electron tunneling events which would imply measuring 100 
µV  changes.  However,  this  will  reduce  the  resolution  of  the  stored  weights/updates  to  14  bits.  This 
resolution  might  be  sufficient  for  training  a  medium  sized  neural  network;  however,  it  is  still  an  open 
question if this resolution would be sufficient for training large-scale networks [33, 34]. A mechanism to 
improve the dynamic range and the measurement resolution is to use a current-mode readout integrated 
with current-mode neural network architecture. If the read-out transistor is biased in weak-inversion, 120 
dB of dynamic range could be potentially achieved. However, note that even in this operating mode, the 
resolution  of  the  weight  would  still  be  limited  by  the  number  of  electrons  and  the  quantization  due  to 
electron transport. Addressing this limitation would be a part of future research.  

Another  limitation  that  arises  due  to  finite  number  of  electrons  stored  on  the  floating-gate  and 
transported  across  the  tunneling  barrier  during  SET  and  RESET,  is  the  speed  of  programming.  Shorter 
duration programming pulses would reduce the change in stored voltage (weight) which could be beneficial 
if precision in updates is desired. In contrast, by increasing the magnitude of the programming pulses, as 
shown in Fig. 4(a), the change in stored voltage can be coarsely adjusted. However, this would limit the 
number of updates before the weights saturate. Note that due to device mismatch the programmed values 
would be different on different FN-DAM devices. 

In  terms  of  endurance,  after  a  single initialization  the  FN-DAM  can  support  103–104  update  cycles 
before the weight saturates. However, at the core FN-DAM is a FLASH technology and could potentially 
be reinitialized again. Given that the endurance of FLASH memory is 103 [35], it is anticipated that FN-
DAM to have an endurance of 106–107 cycles. In terms of other memory performance metrics, the ION/IOFF 
ratio for the FN-DAM is determined by the operating regime and the read-out mechanism. Supplementary 
Fig. 6 shows the expected ratio estimated using the FN-DAM model. Also, FN-DAM when biased as a 
non-volatile memory requires on-chip charge-pumps only to generate high-voltage programming pulses for 
infrequent global erase; thus, compared to FLASH memory, FN-DAM should have fewer failure modes 
[36]. 

The main advantage of FN-DAM compared to other emerging memory technologies is its scalability 
and compatibility with CMOS. At its core, FN-DAM is based on floating-gate memories which have been 
extensively  studied  in  context  of  machine  learning  architectures  [13].  Furthermore,  from  an  equivalent 
circuit point of view, FN-DAM could be viewed as a capacitor whose charge can be precisely programmed 
using CMOS processing elements. FN-DAM also provides a balance between weight-updates that are not 
too small so that learning never occurs versus weight-updates being too large such that the learning becomes 
unstable. The physics of FN-DAM ensures that weight decay (in the absence of any updates) towards a zero 
vector (due to resynchronization) which is important for neural network generalization [37].  

Like other analog non-volatile memories, FN-DAM could be used in any previously proposed compute-
in-memory (CIM) architectures. However, in conventional CIM implementations the weights are trained 
offline and then downloaded on chip without retraining the processor [38]. This makes the architecture 
prone to analog artifacts like offsets, mismatch and non-linearities. On-chip learning and training mitigates 

9 

 
this  problem  whereby  the  weights  self-calibrate  for  the  artifacts  to  produce  the  desired  output  [39]. 
However, to support on-chip training/learning, weights need to be updated at a precision greater than 12 
bits [34]. In this regard FN-DAM exhibit a significant advantage compared to other analog memories. Even 
though in this proof-of-concept work,  we have a used a  hybrid chip-in-the-loop training paradigm, it is 
anticipated that in the future the training circuits and FN-DAM modules could be integrated together on-
chip. 

Methods 
Initialization of the FN-DAM array 

For each node of each recorder, the readout voltage was programmed to around 3 V while the tunneling 
node  was  operating  in  the  tunneling  regime  (Supplementary  Fig.  1).  This  was  achieved  through  a 
combination of tunneling and injection. Specifically, VDD was set to 7 V, input to 5 V, and the program 
tunneling pin was gradually increased to 23 V. Around 12–13V the tunneling node’s potential would start 
increasing. The coupled readout node’s potential would also increase. When the readout potential went over 
4.5 V, electrons would start injecting into the readout floating gate, thus ensuring its potential was clamped 
below  5  V.  After  this  initial  programming,  VDD  was  set  to  6  V  for  the  rest  of  the  experiments.  See 
Supplementary  Section  I  for  further  details.  After  one-time  programming,  input  was  set  to  0  V,  input 
tunneling  voltage  was  set  to  21.5  V  for  1  minute  and  then  the  floating  gate  was  allowed  to  discharge 
naturally. Readout voltages for the SET and RESET nodes were measured every 500 milliseconds. The rate 
of discharge for each node was calculated; and a state where the tunneling rates would be equal was chosen 
as the initial synchronization point for the remainder of the experiments. 

FN Tunneling dynamics 

V(t) is the floating gate voltage given by [22, 21] 

𝑉(𝑡) =

𝑘2
log(𝑘1𝑡 + 𝑘0)

Where 𝑘1 and 𝑘2 are device specific parameters and 𝑘0 depends on initial condition as: 

𝑘0 = exp (−

𝑘2
𝑉0

) 

Using the dynamic given in Eqn.1, the Fowler-Nordheim tunneling current can be calculated as: 

𝐼𝐹𝑁(𝑉(𝑡))
𝐶𝑇

=

𝑑(𝑉(𝑡))
𝑑𝑡

=   (

𝑘1
𝑘2

) 𝑉2 exp (−

𝑘2
𝑉

) 

(1) 

(2) 

Weight decay model and FN-DAM dynamics 

Many neural network training algorithms are based on solving an optimization problem of the form [26]: 

min
𝑤̅

𝐻(𝑤) =

𝛼
2

‖𝑤̅‖ + ℒ(𝑤̅) 

(3) 

10 

 
 
 
 
 
 
where 𝑤̅ denotes the network synaptic weights, ℒ(∙) is a loss-function based on the training set and 𝛼 is a 
hyper-parameter that controls the effect of the ℒ2 regularization. Applying gradient descent updates on each 
element 𝑤𝑖 of the weight vector 𝑤̅ as: 

𝑤𝑖,𝑛+1 − 𝑤𝑖,𝑛 = −𝛼𝜂𝑛𝑤𝑖,𝑛 − 𝜂𝑛

𝛿ℒ(𝑤̅)
𝛿𝑤𝑖,𝑛

(4) 

Where the learning rate 𝜂𝑛 is chosen to vary according to 𝜂𝑛~ 𝑂(1/𝑛) to ensure convergence to a local 
minimum [27]: 

The  naturally  implemented  weight  decay  dynamics  in  FN-DAM  devices  can  be  modeled  by  applying 
Kirchhoff’s Current Law at the SET and RESET floating gate nodes (see Fig. 1e). 

(5) 

(6) 

(8) 

(9) 

𝐶𝑇

(𝑊𝑆) + 𝐼𝐹𝑁(𝑊𝑆) = 𝐶𝐶

(𝑉𝑆𝐸𝑇) 

𝐶𝑇

(𝑊𝑅) + 𝐼𝐹𝑁(𝑊𝑅) = 𝐶𝐶

(𝑉𝑅𝐸𝑆𝐸𝑇) 

𝑑
𝑑𝑡
𝑑
𝑑𝑡

𝑑
𝑑𝑡
𝑑
𝑑𝑡

Where 𝐶𝐹𝐺 + 𝐶𝐶 = 𝐶𝑇 is the total capacitance at the floating gate. Taking the difference between the above 
two equations, we get:  

𝐶𝑇

𝑑
𝑑𝑡

(𝑊𝑆 − 𝑊𝑅) + 𝐼𝐹𝑁(𝑊𝑆) − 𝐼𝐹𝑁(𝑊𝑅) = 𝐶𝐶

𝑑
𝑑𝑡

(𝑉𝑆𝐸𝑇 − 𝑉𝑅𝐸𝑆𝐸𝑇)  

(7) 

For the differential architecture, 𝑤 = 𝑊𝑆 − 𝑊𝑅. Let 𝑉𝑡𝑟𝑎𝑖𝑛 = 𝑉𝑆𝐸𝑇 − 𝑉𝑅𝐸𝑆𝐸𝑇, the training voltage calculated 
by the training algorithm. In addition, 𝐼𝐹𝑁 is substituted from Eqn. 2. Let 𝐶𝐶/𝐶𝑇 =   𝐶𝑅, the input coupling 
ratio: 

𝑑𝑤
𝑑𝑡

= −

(𝐼𝐹𝑁(𝑊𝑆) − 𝐼𝐹𝑁(𝑊𝑅))
𝐶𝑇

+ 𝐶𝑅

(𝑉𝑡𝑟𝑎𝑖𝑛)  

𝑑
𝑑𝑡
𝑘2
𝑊𝑆

− (

𝑘1
𝑘2

) 𝑊𝑅

2 exp (−

𝑑𝑤
𝑑𝑡

=

𝑘1
𝑘2

) + (

𝑘2
𝑊𝑅
𝑊𝑅 − 𝑊𝑆

Discretizing the update for a small time-interval Δ𝑡 

) 𝑊𝑆

2 exp (−

)

𝑤 + 𝐶𝑅

𝑑
𝑑𝑡

(𝑉𝑡𝑟𝑎𝑖𝑛) 

 𝑤𝑛+1 = 𝑤𝑛 +  

− (

𝑘1
𝑘2

) 𝑊𝑅

2 exp (−

Let 𝜇 = 𝑊𝑅/𝑊𝑆 

𝑘1
𝑘2

) + (

𝑘2
𝑊𝑅
𝑊𝑅 − 𝑊𝑆

) 𝑊𝑆

2 exp (−

𝑘2
𝑊𝑆

)

𝑤𝑛Δ𝑡 + 𝐶𝑅Δ𝑉𝑡𝑟𝑎𝑖𝑛,𝑛 

(10) 

𝑤𝑛+1 = 𝑤𝑛 − (

) 𝑊𝑆 exp (−

𝑘1
𝑘2
+ 𝐶𝑅Δ𝑉𝑡𝑟𝑎𝑖𝑛,𝑛 

𝜇2 exp (−

1
𝜇)) − 1

(1 −

𝑘2
𝑊𝑆
𝜇 − 1

𝑤𝑛Δ𝑡

𝑘2
𝑊𝑆

) 

(11) 

Assuming that the stored weight (measured in mV) is much smaller than node potential (> 6V) i.e., 𝑤 ≪
𝑊𝑅 (and 𝑊𝑅 ≈ 𝑊𝑆) and taking the limit (𝜇 → 1) using L'Hôpital's rule: 

11 

 
 
 
 
 
 
 
 
 
𝑤𝑛+1 = (1 − (

𝑘1
𝑘2

) (2𝑊𝑆 + 𝑘2) exp (−

𝑘2
𝑊𝑆

) Δ𝑡) 𝑤𝑛 + 𝐶𝑅Δ𝑉𝑡𝑟𝑎𝑖𝑛,𝑛 

(12) 

𝑊𝑆 follows the temporal dynamics given in Eqn. 1, 

𝑤𝑛+1 = −𝑘1 (

2
log(𝑘1𝑛Δ𝑡 + 𝑘0)

+ 1) (

1
𝑘1𝑛Δ𝑡 + 𝑘0

) 𝑤𝑛Δ𝑡 + 𝐶𝑅Δ𝑉𝑡𝑟𝑎𝑖𝑛,𝑛 

(13) 

Comparing above equation to Eqn. 4, the weight decay factor for FN-DAM system is given as: 

𝛼𝜂𝑛 = 𝑘1 (

2
log(𝑘1𝑛Δ𝑡 + 𝑘0)

+ 1) (

1
𝑘1𝑛Δ𝑡 + 𝑘0

) → 𝑂 (

1
𝑛

) 

(14) 

Chip-in-the-loop linear classifier training 

A hybrid hardware-software system was implemented to carry out an online machine learning task. The 
physical weights (𝑤̅ = [𝑤1, 𝑤2]) stored in two FN-DAM devices were measured and used to classify points 
from a labelled test data set in software. We sought to train a linear decision boundary of the form: 

𝑓(x̅, 𝑤̅) = 𝑥2 + 𝑤1𝑥1 + w0  

(15) 

x̅ = [𝑥1, 𝑥2]  are  the  features  of  the  training  set.  For  each  point  that  was  misclassified,  the  error  in  the 
classification was calculated and a gradient of the loss function with respect to the weights was calculated. 
Based on the gradient information, the weights were updated in hardware by application of SET and RESET 
pulses via a function generator.  

The states of the SET and RESET nodes were measured every 2 seconds and the weight of each memory 

cell, 𝑖, was calculated as:  

𝑤𝑖 = 1000 ∗ (𝑊𝑅,𝑖 − 𝑊𝑆,𝑖) 

(16) 

The  factor  of  1000  indicates  that  the  weight  is  stored  as  the  potential  difference  between  the  SET  and 
RESET nodes as measured in mV. We followed a stochastic gradient descent method. We defined loss 
function as:  

The gradient of the loss function was calculated as: 

ℒ𝑛(𝑤̅) = 𝑅𝑒𝐿𝑈(1 − 𝑦𝑛𝑓(xn̅̅̅, 𝑤̅)) 

The weights needed to be updated as 

Gn(𝑤̅) =

𝜕ℒ𝑛(𝑤̅)
𝜕𝑤̅

𝑤𝑛+1 = 𝑤𝑛 − 𝜆𝑛𝐺𝑛(𝑤̅) 

(17) 

(18) 

(19) 

Here 𝜆𝑛 is the learning rate as set by the learning algorithm. The gradient information is used to update FN-
DAM by applying control pulses to SET/RESET nodes via a suitable mapping function 𝑇: 

𝑉𝑡𝑟𝑎𝑖𝑛,𝑛 = 𝑇(𝜆𝑛𝐺𝑛(𝑤̅)) 

(20) 

12 

 
 
 
 
 
 
 
 
 
  
 
 
Positive weight updates were carried out by application of SET pulses and negative updates via RESET 
pulses. The magnitude of the update was implemented by modulating the number of input pulses. 

References 
 [1] 
G. W.  Burr,  R. M.  Shelby,  A. Sebastian,  S. Kim,  S. Kim,  S. Sidler,  K. Virwani,  M. Ishii,  P. Narayanan, 
A. Fumarola, et al., “Neuromorphic computing using non-volatile memory,” Advances in Physics: X, vol. 2, no. 1, 
pp. 89–124, 2017. 

T. P. Xiao, C. H. Bennett, B. Feinberg, S. Agarwal, and M. J. Marinella, “Analog architectures for neural 

[2] 
network acceleration based on non-volatile memory,” Applied Physics Reviews, vol. 7, no. 3, p. 031301, 2020. 

A. Sebastian, M. Le Gallo, R. Khaddam-Aljameh, and E. Eleftheriou, “Memory devices and applications for 

[3] 
in-memory computing,” Nature nanotechnology, vol. 15, no. 7, pp. 529–544, 2020. 

W. A.  Wulf  and  S. A.  McKee,  “Hitting  the  memory  wall:  Implications  of  the  obvious,”  ACM  SIGARCH 

[4] 
computer architecture news, vol. 23, no. 1, pp. 20–24, 1995. 

[5] 
A. Nowatzyk,  F. Pong,  and  A. Saulsbury,  “Missing  the  memory  wall:  The  case  for  processor/memory 
integration,” in 23rd Annual International Symposium on Computer Architecture (ISCA’96), pp. 90–90, IEEE, 1996. 

D. Ielmini and H.-S. P. Wong, “In-memory computing with resistive switching devices,” Nature Electronics, 

[6] 
vol. 1, no. 6, pp. 333–343, 2018. 

M. Horowitz, “1.1 computingâ€™s energy problem (and what we can do about it). in 2014 ieee international 

[7] 
solid-state circuits conference digest of technical papers (isscc),” in IEEE, feb, 2014. 

H. Akinaga and H. Shima, “Resistive random access memory (reram) based on metal oxides,” Proceedings 

[8] 
of the IEEE, vol. 98, no. 12, pp. 2237–2251, 2010. 

G. Srinivasan, A. Sengupta, and K. Roy, “Magnetic tunnel junction based long-term short-term stochastic 

[9] 
synapse for a spiking neural network with on-chip stdp learning,” Scientific reports, vol. 6, no. 1, pp. 1–13, 2016. 

[10] 
G. W. Burr, M. J. Brightsky, A. Sebastian, H.-Y. Cheng, J.-Y. Wu, S. Kim, N. E. Sosa, N. Papandreou, H.-
L. Lung, H. Pozidis, et al., “Recent progress in phase-change memory technology,” IEEE Journal on Emerging and 
Selected Topics in Circuits and Systems, vol. 6, no. 2, pp. 146–162, 2016. 

[11] 
A. Khvalkovskiy,  D. Apalkov,  S. Watts,  R. Chepulskii,  R. Beach,  A. Ong,  X. Tang,  A. Driskill-Smith, 
W. Butler, P. Visscher, et al., “Basic principles of stt-mram cell operation in memory arrays,” Journal of Physics D: 
Applied Physics, vol. 46, no. 7, p. 074001, 2013. 

[12] 
J. R.  Jameson,  N. Gilbert,  F. Koushan,  J. Saenz,  J. Wang,  S. Hollmer,  M. Kozicki,  and  N. Derhacobian, 
“Quantized conductance in ag/ges2/w conductive-bridge memory cells,” IEEE electron device letters, vol. 33, no. 2, 
pp. 257–259, 2012. 

[13] 
F. Merrikh-Bayat, X. Guo, M. Klachko, M. Prezioso, K. K. Likharev, and D. B. Strukov, “High-performance 
mixed-signal  neurocomputing  with  nanoscale  floating-gate  memory  cell  arrays,”  IEEE  transactions  on  neural 
networks and learning systems, vol. 29, no. 10, pp. 4782–4790, 2017. 

[14] 
S. Dünkel,  M. Trentzsch,  R. Richter,  P. Moll,  C. Fuchs,  O. Gehring,  M. Majer,  S. Wittek,  B. Müller, 
T. Melde, et al., “A fefet based super-low-power ultra-fast embedded nvm technology for 22nm fdsoi and beyond,” 
in 2017 IEEE International Electron Devices Meeting (IEDM), pp. 19–7, IEEE, 2017. 

X. Gu, Z. Wan, and S. S. Iyer, “Charge-trap transistors for cmos-only analog memory,” IEEE Transactions 

[15] 
on Electron Devices, vol. 66, no. 10, pp. 4183–4187, 2019. 

13 

 
J. Tang, D. Bishop, S. Kim, M. Copel, T. Gokmen, T. Todorov, S. Shin, K.-T. Lee, P. Solomon, K. Chan, 
[16] 
et al.,  “Ecram  as  scalable  synaptic  cell  for  high-speed,  low-power  neuromorphic  computing,”  in  2018  IEEE 
International Electron Devices Meeting (IEDM), pp. 13–1, IEEE, 2018. 

C.-X. Xue, T.-Y. Huang, J.-S. Liu, T.-W. Chang, H.-Y. Kao, J.-H. Wang, T.-W. Liu, S.-Y. Wei, S.-P. Huang, 
[17] 
W.-C. Wei, et al., “15.4 a 22nm 2mb reram compute-in-memory macro with 121-28tops/w for multibit mac computing 
for tiny ai edge devices,” in 2020 IEEE International Solid-State Circuits Conference-(ISSCC), pp. 244–246, IEEE, 
2020. 

[18] 
Q. Dong,  Z. Wang,  J. Lim,  Y. Zhang,  M. E.  Sinangil,  Y.-C.  Shih,  Y.-D.  Chih,  J. Chang,  D. Blaauw,  and 
D. Sylvester, “A 1-mb 28-nm 1t1mtj stt-mram with single-cap offset-cancelled sense amplifier and in situ self-write-
termination,” IEEE Journal of Solid-State Circuits, vol. 54, no. 1, pp. 231–239, 2018. 

R. Genov, S. Chakrabartty, and G. Cauwenberghs, “Silicon support vector machine with on-line learning,” 

[19] 
International Journal of Pattern Recognition and Artificial Intelligence, vol. 17, no. 03, pp. 385–404, 2003. 

I. Loshchilov  and  F. Hutter,  “Decoupled  weight  decay  regularization,”  arXiv  preprint  arXiv:1711.05101, 

[20] 
2017. 

[21]  M. Lenzlinger  and  E. Snow, “Fowler-nordheim  tunneling  into  thermally grown  sio2,”  Journal of  Applied 
physics, vol. 40, no. 1, pp. 278–283, 1969. 

L. Zhou  and  S. Chakrabartty,  “Self-powered  timekeeping  and  synchronization  using  fowler–nordheim 
[22] 
tunneling-based  floating-gate integrators,”  IEEE  Transactions  on  Electron  Devices,  vol. 64,  no. 3,  pp. 1254–1260, 
2017. 

D. Mehta,  K. Aono,  and  S. Chakrabartty,  “A  self-powered  analog  sensor-data-logging  device  based  on 

[23] 
fowler-nordheim dynamical systems,” Nature communications, vol. 11, no. 1, pp. 1–9, 2020. 

E. Takeda and N. Suzuki, “An empirical model for device degradation due to hot-carrier injection,” IEEE 

[24] 
electron device letters, vol. 4, no. 4, pp. 111–113, 1983. 

G. Cauwenberghs and M. Bayoumi, Learning on silicon: Adaptive VLSI neural systems, vol. 512. Springer 

[25] 
Science & Business Media, 1999. 

[26] 

C. M. Bishop, Pattern recognition and machine learning. springer, 2006. 

[27] 

J. Nocedal and S. Wright, Numerical optimization. Springer Science & Business Media, 2006. 

[28] 
F. Tu,  W. Wu,  S. Yin,  L. Liu,  and  S. Wei,  “Rana:  Towards  efficient  neural  acceleration  with  refresh-
optimized embedded dram,” in  2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture 
(ISCA), pp. 340–352, IEEE, 2018. 

L. Chen  and  K. Aihara,  “Chaotic  simulated  annealing  by  a  neural  network  model  with  transient  chaos,” 

[29] 
Neural networks, vol. 8, no. 6, pp. 915–930, 1995. 

[30] 
T. Semwal,  P. Yenigalla,  G. Mathur,  and  S. B.  Nair,  “A  practitioners’  guide  to  transfer  learning  for  text 
classification using convolutional neural networks,” in Proceedings of the 2018 SIAM International Conference on 
Data Mining, pp. 513–521, SIAM, 2018. 

[31] 
D. Silver,  A. Huang,  C. J.  Maddison,  A. Guez,  L. Sifre,  G. Van  Den Driessche,  J. Schrittwieser, 
I. Antonoglou, V. Panneershelvam, M. Lanctot, et al., “Mastering the game of go with deep neural networks and tree 
search,” nature, vol. 529, no. 7587, pp. 484–489, 2016. 

L. Zhou, S. H. Kondapalli, K. Aono, and S. Chakrabartty, “Desynchronization of self-powered fn tunneling 

[32] 
timers for trust verification of iot supply-chain,” IEEE Internet of Things Journal, 2019. 

14 

 
Y. Chen,  T. Luo,  S. Liu,  S. Zhang,  L. He,  J. Wang,  L. Li,  T. Chen,  Z. Xu,  N. Sun,  et al.,  “Dadiannao:  A 
[33] 
machine-learning supercomputer,” in 2014 47th Annual IEEE/ACM International Symposium on Microarchitecture, 
pp. 609–622, IEEE, 2014. 

S. Gupta,  A. Agrawal,  K. Gopalakrishnan,  and  P. Narayanan,  “Deep  learning  with  limited  numerical 

[34] 
precision,” in International conference on machine learning, pp. 1737–1746, PMLR, 2015. 

L. M.  Grupp,  J. D.  Davis,  and  S. Swanson,  “The  bleak  future  of  nand  flash  memory.,”  in  FAST,  vol. 7, 

[35] 
pp. 10–2, 2012. 

P. Cappelletti,  R. Bez,  D. Cantarelli,  and  L. Fratin,  “Failure  mechanisms  of  flash  cell  in  program/erase 

[36] 
cycling,” in Proceedings of 1994 IEEE International Electron Devices Meeting, pp. 291–294, IEEE, 1994. 

C. Wei, J. Lee, Q. Liu, and T. Ma, “Regularization matters: Generalization and optimization of neural nets 

[37] 
vs their induced kernel,” 2019. 

S. Chakrabartty  and  G. Cauwenberghs,  “Sub-microwatt  analog  vlsi  trainable  pattern  classifier,”  IEEE 

[38] 
Journal of Solid-State Circuits, vol. 42, no. 5, pp. 1169–1179, 2007. 

[39] 
S. Ambrogio,  P. Narayanan,  H. Tsai,  R. M.  Shelby,  I. Boybat,  C. Di Nolfo,  S. Sidler,  M. Giordano, 
M. Bodini, N. C. Farinha, et al., “Equivalent-accuracy accelerated neural-network training using analogue memory,” 
Nature, vol. 558, no. 7708, pp. 60–67, 2018. 

15 

 
 
Supplementary Information 

I. 

Initial Programming 

Figure 1: Architecture of one half of the FN-DAM, which can be configured as a SET or a RESET node. 

The  FN-DAM consists of two  identical  nodes: SET and  RESET node. Each node contains two floating gates 
decoupled via a capacitor (SI Figure 1) – tunneling gate (W) and readout gate (Wread). The charge on each gate of the 
system can be individually programmed using a combination of tunneling (to increases charge, coarse tuning) and hot 
electron injection (to decreases charge, fine tuning). The tunneling gate, which stores the dynamic analog memory, is 
biased in the FN tunneling regime. By setting 𝑉𝑝𝑟𝑜𝑔 to a high potential of 22 V, the tunneling node is set to ~8 V which 
is sufficient to initiate observable FN tunneling for oxide thickness of around 13 nm. Wread is capacitively decoupled 
from  the  tunneling  node  to  avert  readout  disturbances.  The  readout  node  is  biased  at  a  lower  voltage  to  prevent 
injection into the readout node during operation. The potential of the readout node is lowered through hot electron 
injection. Injection is initiated by setting 𝑉𝐷𝐷= 7 V and the input pin to a value such that 𝑉𝐷𝑆 across the PMOS is above 
4.2 V. Switch 𝑆𝑗 allows for individual control of each FN-DAM block for reading and programming. 

16 

 
 
 
 
 
II.  Write Energy Dissipation 

Figure 2: a) Target voltage, floating gate voltage and training voltage as a function of time. b) Energy required to charge 

unit capacitance as a function of time. 

The magnitude of input pulse required, 𝑉𝑡𝑟𝑎𝑖𝑛(𝑡) (Fig. 2a) so that the floating gate node at current potential 𝑉𝐹𝐺(𝑡) 

shifts to a target voltage 𝑉𝑇 is given by: 

𝑉𝑡𝑟𝑎𝑖𝑛(𝑡) =

𝑉𝑇 − 𝑉𝐹𝐺(𝑡)
𝐶𝑅

Where 𝐶𝑅 is the input capacitive coupling ratio 𝐶𝑅 =

𝐶𝐶
𝐶𝐶+𝐶𝐹𝐺

 . The floating gate voltage 𝑉𝐹𝐺(𝑡) is approximated 

by the following dynamic [1]: 

𝑉𝐹𝐺(𝑡) =

𝑘2
log(𝑘1𝑡 + 𝑘0)

(1) 

The energy required to charge the input capacitor is given as 

𝐸(𝑡) =

1
2

𝐶𝑖𝑛(𝑉𝑖𝑛(𝑡))

2

Figure 2b shows instantaneous energy required to charge unit capacitance when 𝑉𝑇 = 7.6𝑉 and 𝑉𝐹𝐺(0) = 7.5𝑉. 
The input capacitance of our device was 1 pF, and the instantaneous write energy per update increased from 5 fJ to 
2.5pJ over 12 days. 

17 

Time (s)Energy (J/F)Time (s)Potential(V)(a)(b)VfgVtrainVT 
 
 
 
 
 
 
 
 
III.  Memory Retention 

Figure 3: a) Retention time as a function of floating gate voltage for a range of input pulse magnitudes. b-c) Retention time 

of weight updates as a function of time elapsed after initialization to 7.5V (b) and 6V (c). 

The method for calculating retention time of dynamical systems was described in [2]. In brief, the point at which 
the analog memory  – due to resynchronization  – falls below the noise floor is the retention time. The  noise floor 
consists of a constant noise introduced by the readout noise and an operational noise that increases with time, due to 
thermally induced random desynchronization. 

Total noise: 𝜎𝑇(𝑡) = 𝜎0 + 𝜎(𝑡) 

In  this  case,  we  assumed  𝜎0 = 100 𝜇𝑉  and  estimated  𝜎𝑡 = 1.4𝑡0.5 𝜇𝑉  from  experiments  without  any  external 

pulse. 

At 𝑇𝑅𝑒𝑡, synaptic memory’s state goes below the noise floor and hence the following condition is satisfied: 

𝑤(𝑇𝑅𝑒𝑡) = 𝜎𝑇(𝑇𝑅𝑒𝑡) 

When the FN-DAM is biased at around 6 V, its retention time is similar to FLASH/EEPROM memory. However, 

energy consumption is around 150 𝑓𝐽 (for a 100 𝑓𝐹 input capacitance). 

18 

Time from initializationTime from initialization       Retention time(a)(b)(c)V0= 7.5VV0= 6V100 mV     100 mV2.5 V2.5 V1.7 V 
 
 
 
IV.  Deep neural network simulations 

Figure 4: a) Network loss for 3 types of network models. Inset shows same data with X axis in log scale. b) Energy spent in 

updating the network weights. Inset shows same data with X axis in log scale. 

The performance of FG-DAM model was compared to that of a standard network model. A 15-layer convolutional 
neural network was trained on the MNIST dataset using the MATLAB Deep Learning Toolbox. For each learnable 
parameter in the CNN, a software FN-DAM instance corresponding to that parameter was created. In each iteration, 
the loss of the network function and gradients were calculated. The gradients were used to update the weights via 
Stochastic Gradient Descent with Momentum (SGDM) algorithm. The updated weights were mapped onto the FN-
DAM array. The weights in the FN-DAM array were decayed according to Eqn. 14. These weights were then mapped 
back into the CNN. This learning process was carried on for 9 epochs. In the 10th epoch, no gradient updates were 
performed. However, the weights were allowed to decay for the last epoch (note that in the standard CNN case, the 
memory was static). A special case with a 0.1% randomly assigned mismatch in the floating gate parameters (𝑘1 and 
𝑘2) was also implemented.  

Table 1: Comparison metrics 

Accuracy (%) 
Standard CNN 
FN-DAM CNN 
FN-DAM CNN with mismatch 

After 9 epochs 
98.6 
99 
97.4 

After 10 epochs 
98.6 
99.2 
96.3 

19 

Energy (a.u.)LossIterationIteration(a)(b) 
 
 
 
V.  Read Energy Dissipation 

Figure 5: Minimum power required to read floating gate voltage as a function of required readout speed. Noise floors shown 

in legend. 

The readout power is dependent on the readout accuracy required and the speed at which it operates. 

For a PMOS in a source follower configuration, the readout noise is given by: 

For subthreshold operation, 

2 =

𝑉𝑛

4𝑘𝑇
𝑔𝑚

Δ𝑓 =

4𝑘𝑇
𝑞

∗

𝑞
𝑔𝑚

Δ𝑓 =

4𝑈𝑇𝑞
𝑔𝑚

Δ𝑓 

𝑔𝑚 =

𝜅𝐼𝑑
𝑈𝑇

∴ 𝑉𝑛

2 =

2𝑞
4𝑈𝑇
𝜅𝐼𝑑

Δ𝑓 =

2𝑞𝑉𝐷𝐷
4𝑈𝑇
𝜅𝑃𝑟𝑒𝑎𝑑

Δ𝑓 

Above equation is plotted in SI  Figure 5 for different noise floors and readout frequency for  𝑉𝑑𝑑 = 5𝑉, 𝑈𝑇 =

26 𝑚𝑉 and 𝜅 = 0.7 

20 

 
 
 
 
 
VI.  Programming dynamics 

Figure 6: Programming ratio for different k1 parameter which can be controlled by changing the size of tunneling junction. 

The FN-DAM is programmed by applying a pulse of magnitude 𝑉𝑡𝑟𝑎𝑖𝑛(𝑡) so that the node reaches a potential of 

𝑉𝑇 through the input coupling capacitor, as derived in the previous section. The programming ratio is given by: 

Iprog
Iprog
̅̅̅̅̅̅̅

=

𝐼𝐹𝑁(𝑉𝑇)
𝐼𝐹𝑁(𝑉𝐹𝐺(𝑡))

Dynamics of FN tunneling current are given by [1]: 

𝐼𝐹𝑁(𝑉(𝑡))
𝐶𝑇

=

𝑑(𝑉(𝑡))
𝑑𝑡

=   (

𝑘1
𝑘2

) 𝑉2 exp (−

𝑘2
𝑉

) 

Iprog
Iprog
̅̅̅̅̅̅̅

= (

𝑉𝑇
𝑉𝐹𝐺(𝑡)

2
)

exp (

𝑘2
𝑉𝐹𝐺(𝑡)

−

𝑘2
𝑉𝑇

) 

Above equation is plotted for 3 values of k1 which affect the dynamics of 𝑉𝐹𝐺(𝑡). The parameter k1 can be altered 

during the design phase by changing the area and capacitance of the floating gate node. 

21 

        0.1 k1k110k1 
 
 
 
 
 
Supplementary References 

1.  L. Zhou and S. Chakrabartty, “Self-powered timekeeping and synchronization using fowler–nordheim 
tunneling-based floating-gate integrators,” IEEE Transactions on Electron Devices, vol. 64, no. 3, 
pp. 1254–1260, 2017. 

2.  D. Mehta, K. Aono, and S. Chakrabartty, “A self-powered analog sensor-data-logging device based 
on fowler-nordheim dynamical systems,” Nature communications, vol. 11, no. 1, pp. 1–9, 2020. 

22 

 
 
