Adaptive Metamorphic Testing with Contextual Bandits

Helge Spiekera,∗, Arnaud Gotlieba

aSimula Research Laboratory, P.O. Box 134, 1325 Lysaker, Norway

0
2
0
2

r
a

M
3
1

]
E
S
.
s
c
[

3
v
2
6
2
0
0
.
0
1
9
1
:
v
i
X
r
a

Abstract

Metamorphic Testing is a software testing paradigm which aims at using necessary properties of a system

under test, called metamorphic relations, to either check its expected outputs, or to generate new test

cases. Metamorphic Testing has been successful to test programs for which a full oracle is not available or

to test programs for which there are uncertainties on expected outputs such as learning systems. In this

article, we propose Adaptive Metamorphic Testing as a generalization of a simple yet powerful reinforcement

learning technique, namely contextual bandits, to select one of the multiple metamorphic relations available

for a program. By using contextual bandits, Adaptive Metamorphic Testing learns which metamorphic

relations are likely to transform a source test case, such that it has higher chance to discover faults. We

present experimental results over two major case studies in machine learning, namely image classiﬁcation

and object detection, and identify weaknesses and robustness boundaries. Adaptive Metamorphic Testing

eﬃciently identiﬁes weaknesses of the tested systems in context of the source test case.

Keywords: Software Testing, Metamorphic Testing, Contextual Bandits, Machine Learning

1. Introduction

Metamorphic Testing (MT) is a software testing paradigm that aims at using necessary properties of

a software under test to either check its expected outputs or to generate new test cases [1, 2]. More

precisely, MT tackles the so-called oracle problem which occurs whenever predicting the expected outputs

of a system is just too diﬃcult or even impossible. Typical examples include machine learning models used

for classiﬁcation tasks, for which only stochastic behaviors can be speciﬁed [3]. Indeed, these models are

often initially trained with existing datasets and then exploited to classify new data samples. However, the

expected class of any new data sample is unknown and thus, these samples cannot be used for testing the

trained models. Fortunately, transformations over the data samples which do not change their (unknown)

class, are usually available. By applying these transformations, called Metamorphic Relations (MRs) in MT,

it becomes possible to eﬀectively test trained machine learning models [4, 5, 6].

∗Corresponding author
Email addresses: helge@simula.no (Helge Spieker), arnaud@simula.no (Arnaud Gotlieb)

Preprint submitted to Journal of Systems and Software

March 16, 2020

 
 
 
 
 
 
MT has been very successful to address testing issues in various application domains, e.g., driverless cars

[7], search engines [8], or bioinformatics [9] just to name a few (see Section 2.3 for more references). However,

generally speaking, applying MT in practice requires to address two issues: the MR identiﬁcation and the

MR selection problems [2]. The former occurs when trying to identify MRs for a speciﬁc system, i.e., to

formalize input transformation properties which lead to a known transformation of the outputs. Finding

such relations may be diﬃcult when there is no obvious symmetries in the input data, or obvious system

invariant, or else when the functional behavior of the system is unknown. The second occurs when several

MRs have been identiﬁed, but determining which ones are best suited to discover faults in the system under

test is hard. It is important to select appropriate MRs for testing the system to avoid redundancies in test

cases and thus to avoid slack in the test execution process. This problem is especially critical when testing

is part of a continuous integration process where there is usually a limit on the time allocated for testing in

an integration cycle.

This paper addresses exclusively the latter problem, i.e., MRs selection, by formulating the eﬀective

selection of MRs as a reinforcement learning problem, based on contextual bandits. Our method, called

Adaptive Metamorphic Testing (AMT), deﬁnes a test transformation bandit which sequentially selects a MR

that is expected to provide the highest payoﬀ, i.e., that is most likely to reveal faults. Which MRs are likely

to reveal faults is learned from successive exploration trials. The bandit explores the diﬀerent available MRs

and evaluates the fault landscape of the system under test, thereby providing valuable information to the

tester.

Learning the selection of MRs can be useful when testing under resource-constraints, for example in cases

where the system under test changes are frequently integrated and tested, but also for infrequent testing

when the number of MRs is large or their checking is costly. We also discuss a second application which

is to identify robust boundaries of the MR parameters. Robust boundaries describe related scenarios but

focus on a single MR that can be controlled via parameters. The interest is to ﬁnd parameters that produce

fault-revealing test cases with minimal changes only.

In this paper, we evaluate Adaptive Metamorphic Testing on two case study applications for image

analysis, namely, image classiﬁcation and object detection. As implementations of these case studies, we

test freely available and pre-trained deep learning systems that can be used as black-box components in other

software systems. For each system, we explore both the general fault-revealing capabilities of metamorphic

relations and the discovery of robustness boundaries.

The main contributions of this paper are three-fold. First, we introduce Adaptive Metamorphic Testing

as a general adaptive selection method for metamorphic relations and test transformations. The method

is based on reinforcement learning with contextual bandits and learns to identify those relations which are

likely to reveal faults in the system under test. This method is useful in a context where a source test case

can be modiﬁed by several diﬀerent metamorphic relations and the system is to be repeatedly tested. To the

2

best of our knowledge, it is the ﬁrst time that reinforcement learning is applied to select MRs and embedded

into a general methodology for MT.

Second, we provide an implementation of Adaptive Metamorphic Testing in a tool called Tetraband,

dedicated to testing machine learning models for image analysis. Our tool facilitates metamorphic relations

based on image augmentation functions and provides dedicated environments for adaptive metamorphic

testing that can be integrated with other implementations.

Third, we explore the beneﬁts of Tetraband on two case studies coming from image analysis, namely image

classiﬁcation and object recognition. Both of these case studies are relevant subsystems in a wide number

of applications, such as autonomous cars, robot navigation, or industrial automation [10, 11]. For these

applications, high-quality standards are essential and rigorous testing is a requirement. Our experiments

show that Tetraband is highly beneﬁcial to optimize the testing process towards fault-revealing MRs.

The remainder of the paper is structured as follows. We review the background on metamorphic testing

and contextual bandits and related work in the area in Section 2. Section 3 introduces Adaptive Metamorphic

Testing and its components. We discuss general application scenarios in Section 4 before introducing the

experimental setup, consisting of our implementation of AMT, Tetraband, and the two case studies in

Section 5. The results are presented in Section 6 and Section 7 ﬁnally concludes the paper.

2. Background

2.1. Metamorphic Testing

Metamorphic Testing (MT) aims at using necessary properties of a software under test to either check

its expected outputs or to generate new test cases [1, 2]. Central to MT is the concept of Metamorphic

Relations (MRs) which are high-level observable properties that must hold over inputs and outputs of the

system under test.

In the following, we formalize the deﬁnition of a metamorphic relation, the transformation from source

test cases to follow-up test cases, and metamorphic testing. Our deﬁnitions follow the formalization by

Chen et al.

[2], except for the transformation from a source to a follow-up test case. We interpret the

transformation function to apply to the whole test case, which includes the test input, and formalize it

accordingly in this more general, but compatible, way:

Deﬁnition 1 (Metamorphic Relation (MR)). Let f be a target function or algorithm. A metamorphic

relation (MR) is a necessary property over a sequence of multiple inputs (cid:104)x1, x2, . . . , xn(cid:105) (n ≥ 2) and their
corresponding outputs (cid:104)f (x1), f (x2), . . . , f (xn)(cid:105). It can be expressed as a relation R ⊆ X n × Y n, with ⊆
being the subset relation, and X n and Y n being the Cartesian products of n input and output spaces.

Deﬁnition 2 (Transformation from Source Test Cases to Follow-up Test Cases). Consider a MR

R(x1, x2, . . . , xn, f (x1), f (x2), f (x3)). The sequence of inputs and their corresponding outputs deﬁnes the

3

set of source test cases. For each source test case S(x) ∈ R, a follow-up test case F is derived by applying a

possibly non-deterministic transformation function T to the input of the source test case x: F(x) = f (T (x)).

The transformation function T is constructed such that the follow-up test case F fulﬁlls the necessary prop-

erty of R.

Deﬁnition 3 (Metamorphic Testing (MT)). Let P be an implementation of a target algorithm f . For

an MR R, suppose that we have R(x1, x2, . . . , xn, f (x1), f (x2), f (x3)). Metamorphic testing (MT) based on

this MR for P involves the following steps:

1. Deﬁne R(cid:48) by replacing f by P in R.

2. Given a sequence of source test cases (cid:104)x1, x2, . . . , xk(cid:105), execute them to obtain their respective outputs

(cid:104)P (x1), P (x2), . . . , P (xk)(cid:105). Construct and execute a sequence of follow-up test cases (cid:104)xk+1, xk+2, . . . , xn(cid:105)
according to R(cid:48) and obtain their respective outputs (cid:104)P (xk+1), P (xk+2), . . . , P (xn)(cid:105).

3. Examine the results with reference to R(cid:48). If R(cid:48) is not satisﬁed, then this MR has revealed that P is

faulty.

For the remainder of this paper, we refer in general to a metamorphic relation R as the tuple of (cid:104)R, T (cid:105),

the combination of necessary properties over the outputs for a speciﬁc MR and the transformation function

T to generate follow-up test cases from source test cases. Therefore, a source test case S produces output

P (x) for the system P with test input x. Using a metamorphic relation R, the follow-up test case F with

test input T (x), where T is a transformation over the input x, can be generated. Due to the metamorphic

relation over R (including T ), S, and F, the result P (T (x)) can be veriﬁed. Note that MRs are only partial

properties, which means that only test cases that violate them indicate the presence of faults in the system

under test. Showing that the system satisﬁes a MR on any input or a test suite does not guarantee the

absence of faults but increases our conﬁdence in the system correctness. However, this issue concerns any

software testing method, not only MT.

The transformation function T of a metamorphic relation R does not have to be a deterministic function,

but is usually parameterized and can result in several diﬀerent follow-up test cases from one source test case,

depending on a parameter φ which speciﬁes the exact transformation to be applied. In many cases and the

simplest implementation of metamorphic testing, φ is chosen from a random distribution and T produces a

random follow-up test case. We use Rφ to denote a metamorphic relation conﬁgured by φ, which makes it

deterministic, and R for the general metamorphic relation, which might be non-stochastic if no additional

conﬁguration is possible.

2.2. Contextual Bandits

The selection of a test transformation to apply on a source test case is formalized as a multi-armed bandit

problem with context information, also known as a contextual bandit [12, 13]. A contextual bandit acts in

4

discrete iterations, where each iteration corresponds to the generation and execution of one follow-up test

case. A bandit B has k arms, where every arm corresponds to the selection of one possible MR to generate

the follow-up test case. In every iteration i, the bandit receives the context vector ci which in our case

describes the source test case.

The bandit then acts according to one or multiple policies π which formalize the action selection, i.e. the

decision making strategy. These policies are trained from external feedback about the success of previously

made decisions. A policy is often realized by function approximation techniques, for example, multiple linear

regression or neural networks. Additionally, an exploration strategy is used to try previously unexplored

actions instead of following the policy only. The bandit chooses an arm (ai = π(ci)) and receives a reward,

also called payoﬀ, ri which is the external feedback for this decision. As only one arm can be selected, there

is also only feedback for the eﬀect of this single arm ai. Afterwards, the bandit updates its policy from the

observation (ci, ai, ri). Updating the bandit works by adjusting a set of weights, such that the new set of

weights better ﬁts the previously made experiences and minimizes regret for historical decisions. The actual

implementation of the weight update is dependent on the speciﬁc contextual bandits algorithm [14] and the

learner used to approximate the policy, e.g. whether it is a form of linear regression or a non-linear neural

network.

Our goal for the contextual bandit is to maximize the total payoﬀ, i.e., the cumulative undiscounted

reward over all iterations (cid:80)T

i=1 ri. To achieve this goal and to identify highly rewarded actions, contextual
bandit algorithms are designed to also minimize the regret. The regret of a bandit is the gap between the

expected reward over a number of iterations when following one policy and the cumulative reward the agent

actually receives over the same number of iterations [15]. As a smaller regret means to choose actions more

closely to the highest possible payoﬀ, minimizing the regret implies the maximization of the total payoﬀ,

but by maintaining the concept of regret also badly rewarded actions contribute to the improvement of the

policy.

A challenge in the design of contextual bandits is to ﬁnd a balance between exploration, i.e., evaluating the

eﬀect of rarely used actions, and exploitation, i.e., repeating those actions that showed to be eﬀective before.

This process is called the exploration-exploitation trade-oﬀ. To this extent, several exploration techniques

have been developed as part of bandit algorithms. We introduce here two techniques that are useful in

Adaptive Metamorphic Testing. Epsilon-greedy [16] decides to explore a random action with probability (cid:15)

and with probability 1 − (cid:15) the current learned policy is used to select an action. The parameter (cid:15) ∈ [0, 1) is

chosen by the user. A more advanced exploration strategy is online cover [14]. Instead of training a single

policy, m diﬀerent polices are trained to produce diverse behaviors. The exploration algorithm then chooses

from those actions which have not been learned to perform bad, i.e., having high regret, in the current

context. Again, m is a parameter to be chosen by the user.

Contextual bandits are related to Reinforcement Learning (RL) [16]. The main distinction between

5

bandits and RL agents is that bandits perceive each iteration as independent of the previous one [15], i.e.,

the selected action does not aﬀect the next context that is observed. In our scenario, the next test case is

independent of the test transformation chosen for the previous test case. RL agents, however, are designed

to operate over multiple subsequent iterations, where a chosen action inﬂuences the context in the next

iteration. General RL agents could be applied by reducing the length of each scenario to one step, but our

early experiments found contextual bandits to be more eﬃcient.

Bandit algorithms have been successfully applied in a variety of domains, such as news article recom-

mendation [17], advertisement selection [18, 19], statistical software testing [20], constraint optimization

[21, 22], or real-time strategy games [23]. In this work, we apply contextual bandits to the selection and

conﬁguration of metamorphic relations in software testing.

2.3. Related Work

Metamorphic Testing (MT) has been applied to a variety of domains and applications, see [24, 2] for an

in-depth overview. Successful application domains include testing of driverless cars [7], search engines [8],

machine translation systems [25], performance testing [26, 27], constraint solvers [28] or bioinformatics [9].

Previous works already focused on its automation, for example by exploring algorithms to speciﬁcally identify

fault-revealing inputs [29] or performing an empirical study on selecting good Metamorphic Relations (MRs)

[30]. Other works predict the applicability of a MR for a system [31, 32]. Based on source code traces, a

classiﬁcation model predicts which MRs of a given set can be applied.

Due to the emergent success and usage of machine learning in diﬀerent application areas, the veriﬁcation

and validation of these systems has received increasing attention. Several works approach testing machine

learning systems based on software testing techniques, such as diﬀerential, multi-implementation [33] or

mutation testing [34].

Because testing machine learning systems, due to their stochastic nature, is aﬀected by the oracle problem

[3], there has been work to especially apply MT for this purpose. Murphy et al. identify a set of general

MRs, that hold for a variety of machine learning algorithms [4], and are shown to be eﬀective [35]. Chan et

al. further use MT to identify violations of MRs from passed test cases of classiﬁcation models [36]. It has

also been shown that MT can be used for deep learning-based applications, e.g., to test the classiﬁcation of

biological cells [5]. Dwarakanath et al. identify implementation faults in image classiﬁers [6]. They introduce

MRs that aﬀect the training and test data used during model training and demonstrate how these MRs can

be applied to ﬁnd implementation errors in training procedures and model architecture. Yang et al. propose

to test unsupervised clustering methods [37] and Mekala et al. explore the application of MRs to detect

adversarial examples for deep learning models [38]. However, Saha and Kanewala [39] recently evaluated

the eﬀectiveness of MRs for testing supervised classiﬁers based on mutations of the system under test. They

found that the detection rates for the used MRs of previous studies are limited when generating a large set

6

Algorithm 1 Adaptive Metamorphic Testing with Contextual Bandits

Input: M : set of MRs; SUT : system under test P ; T S: set of test cases; Iter: Number of iterations

Output: B: trained bandit

1: i ← 0

2: B ← Load existing or initialize new Bandit

3: while i < Iter do

4:

5:

6:

7:

8:

9:

Select S ∈ T S

(cid:46) Draw source test case from test suite

c ← B.ExtractContextFeatures(S)

(cid:46) Generate feature vector for source test case S

Rφ ← B.SelectBanditArm(c, M )

(cid:46) Select one MR Rφ using the bandit

v ← Apply(SUT , Rφ(F))

(cid:46) Execute SUT with transformed test F, get a verdict v

B ← B.UpdateBandit(Rφ, c, v)

(cid:46) Train the bandit with the feedback

i ← i + 1

10: end while

11: return B

of mutants.

(cid:46) Return updated bandit for future test cycle

Previous work also considered the adaptive control of software testing through feedback while testing

[40, 41]. Similar to our method, these works exploit the behavior of the system during the test execution and

adjust the testing strategy when the understanding of the software changes. In these works, the adjustment

of the testing strategy focuses on test case prioritization and selection, whereas our method focuses on the

generation of follow-up test cases using metamorphic testing and under consideration of many repeated

testing cycles.

3. Adaptive Metamorphic Testing

3.1. Overview

In this section, we introduce Adaptive Metamorphic Testing (AMT) with contextual bandits. Our

method is based on a test transformation bandit that learns to select follow-up test cases from a set of

applicable metamorphic test cases. Algorithm 1 shows an overview of the main steps of AMT. At the core

of AMT, a contextual bandit receives a description of the source test case. Based on this context vector,

the bandit selects an action, which resembles a MR, and the conﬁguration of this transformation. Both are

applied to generate a follow-up test case. After generating the follow-up test case, the system under test

is executed and the test result evaluated according to the MR acceptance criterion. The method can be

directly deployed without any pre-training step. However, during the ﬁrst iterations, MR selection is partly

random to gather initial experiences about the diﬀerent MRs eﬀectiveness and their potential payoﬀs, when

7

applied to the available source test cases. After several iterations have been performed, the bandit learns to

focus on MRs which are most likely to reveal faults. Nevertheless, the bandit continues to explore among

the MRs, i.e., it sometimes chooses MRs which do not promise the highest payoﬀ. This is important to

adjust to changes in the system under test as well as to gather additional information about the eﬀect of

MRs in diﬀerent contexts.

Deﬁnition 4 (Adaptive Metamorphic Testing). Let P be an implementation of a target algorithm f

with T S being its test suite; let M be a set of metamorphic relations applicable on T S and let B be a

contextual bandit. Adaptive Metamorphic Testing (AMT) is an iterative variant of metamorphic testing

and involves the following steps at each iteration:

1. A test case S is (randomly) selected from the test suite T S and executed to obtain its output P (S).

2. The bandit B selects a MR R based on the context features of S.

3. Construct one or more follow-up test cases (cid:104)F1, F2, . . . , Fk(cid:105) according to R and obtain their respective

outputs (cid:104)P (F1), P (F2), . . . , P (Fk)).

4. Examine the results with reference to R. If R is not satisﬁed, then this MR has revealed that P is

faulty.

5. Report the results of the execution back to B for adaptation of the learning algorithm.

The arm selection of the bandit, i.e., the selection of a metamorphic relation R with its parameters is

handled by a hierarchy of contextual bandits using the context features c at each iteration of the algorithm.

On the highest level, the main contextual bandit B selects one MR R from the set of supported MRs.

Afterwards another action-speciﬁc contextual bandit BR is queried, using the same context information as

the main bandit, for the conﬁguration parameter φ: BR(ci) → φ. If the metamorphic relation does not

require additional conﬁguration, the second step is skipped. The MR Rφ can then be used to generate a

follow-up test case for the current iteration. After test execution, both bandits B and BR are trained from

the received feedback. Tetraband consists of one main contextual bandit plus one additional contextual

bandit for each conﬁgurable MR.

Adaptive Metamorphic Testing is independent of the application domain or the implementation or else

the speciﬁc MRs that can be applied. It only takes as inputs a set of MRs, the system under test, a set of

test cases, and a user-deﬁned parameter corresponding to the maximum number of iterations to run. As

output, the method returns a trained bandit B which has learned to select the MRs which have the greatest

chances to detect faults in the system under test.

3.2. Components

Adaptive Metamorphic Testing requires only a few system-speciﬁc components.

In the following, we

discuss each of these components.

8

3.2.1. Extract Context Features

In order to select an appropriate MR which is likely to reveal a fault, for a source test case, it is mandatory

to feed the contextual bandit with relevant context information about the source test case. This context is

captured with a feature vector, which is a real-valued vector of ﬁxed size n. The function receives the source

test case as input and returns the feature vector: ExtractContextF eatures : S → Rn.

By using representative features, the contextual bandit can learn a mapping from the source test cases,

which are described through the features, to the MRs. For that, it has to include test characteristics that

can be aﬀected by the metamorphic relations. Therefore, the features need to capture the necessary details

distinctive about the individual test case, especially those that relate to the eﬀect of the MR.

How the feature vector is formed, is a domain-speciﬁc problem and requires some degree of domain

knowledge. For example, when testing scientiﬁc software for matrix calculations [30], the features should

describe the characteristics of the original input matrix in order to select which transformation is applied. In

our experiments, which are based on testing computer vision problems, we rely on common feature modeling

used in machine learning. For instance, using a pre-trained neural network to extract image features is

common in computer vision. A similar approach could be used for text processing, where textual features,

e.g., used vocabulary, text sentiment, or sentence structure, can be derived using pre-trained networks.

3.2.2. Metamorphic Relations

The most relevant component to acquire for applying Adaptive Metamorphic Testing is the set of MRs,

which is highly domain-dependent. The automatic and systematic identiﬁcation of MRs for a system is an

ongoing research topic [2], but in many cases, MRs can be extracted by using domain knowledge about the

system or reviewing the existing literature from the Software Testing community.

In the special case of

testing machine learning systems, a starting point to uncover MRs is to exploit data augmentation methods

that are commonly used. As shown in our experiments, these augmentations can serve as a basis for MRs

and help to identify weaknesses in ML systems.

3.2.3. Select Bandit Arm

The selection of an appropriate MR is mostly handled by the internal contextual bandit algorithm

and does not have to be individually implemented for a new system under test. The main bandit selects

the MR and, if necessary, the action-speciﬁc bandit for this MR selects the parameter to conﬁgure the MR.

Nevertheless, the conﬁguration of the contextual bandit inﬂuences the performance of the system and should

be adjusted, depending on the number of available MRs and the robustness of the SUT. This allows us to

focus on exploiting MR that reveal faults in the system or to broadly explore the eﬀects of many diﬀerent

MRs.

The most important conﬁguration parameter to adjust is the exploration rate, i.e., how often does the

9

bandit choose a diﬀerent action than the most promising one. Using a high exploration rate allows us to

examine diﬀerent combinations of test cases and systems, which is relevant to detect new faults in the system

and extend the coverage of diﬀerent tests. Conversely, a lower exploration rate exploits combinations of tests

and MRs that have often fail previously. Traditionally, Metamorphic Testing often creates follow-up test

cases at random, which corresponds to a maximal exploration rate here.

Exploitation is relevant when repeatedly testing the system, e.g., in continuous integration settings, or

when trying to understand the weaknesses of the system for a certain group of MRs. Still, it is not only

desired to exploit known weaknesses, but broad coverage of the system behavior is desired for higher test

conﬁdence. If the behavior of the system changes, because it is becoming more robust to previously eﬀective

MRs, the bandit learns this and can adjust its selection for future iterations. Conclusively, compared to

other applications of contextual bandits, where especially the exploitation of known good actions is in focus,

exploration is more prominent in Adaptive Metamorphic Testing to broadly test the system behavior.

3.2.4. Transform, Execute and Evaluate

The selected MR and its conﬁguration transform the source test case into a follow-up test case. That test

case is then executed and the test verdict is evaluated according to the MR. These core steps of Adaptive

Metamorphic Testing are similar to those required in traditional MT.

3.2.5. Update Bandit

After the follow-up test case has been executed and the results have been evaluated, the bandit’s policy

is updated. This requires information about the initial context feature vector, the chosen MR and its

conﬁguration, and the test verdict. The update routine updates the expected reward for this MR. The

exact update routine is speciﬁc to the contextual bandit algorithm and its conﬁguration and we refer to the

corresponding literature for its description [12, 15].

Nevertheless, choosing the appropriate reward for a failed test case has to be done while adjusting the

bandit for a new system to test. In most scenarios, where the goal is to ﬁnd the most fault-revealing MRs,

as described below in Section 4.1, the reward is the same for every failing test case. However, if the bandit

has the goal to identify certain properties of the SUT, it can be necessary to propose a diﬀerent reward

structure that depends on the selected MR. This second scenario is further described in Section 5.3.

4. Application Scenarios of Adaptive Metamorphic Testing

Contextual bandits are powerful to explore the eﬀects of the MRs in diﬀerent contexts, but can also

exploit the gathered experiences to subsequently focus on those relations that are most likely to reveal

faults. From these properties, we identify two application scenarios of Adaptive Metamorphic Testing that

we discuss further and evaluate as part of the case studies.

10

4.1. Fault-Revealing MR Selection

The ﬁrst application of Adaptive Metamorphic Testing is the selection of metamorphic relations which

are prone to reveal faults in the system under test. In cases where the MR has parameters, an additional

contextual bandit is responsible to select these parameters, as described before. This application, which we

refer to as fault-revealing MR selection, steers MT towards greater eﬀectiveness when there are many MRs

available and not suﬃcient resources to apply them all. In this application, all MRs are considered distinct

from each other. Accordingly, the achievable reward received for revealing a fault is identical for all MRs.

4.2. Robustness Boundaries

The second application uses the exploration/exploitation trade-oﬀ of contextual bandits to identify ro-

bustness boundaries of the software under test. With robust boundaries, we focus on MRs whose eﬀect can

be adjusted by user-deﬁned parameters, especially those with continuous or a range of discrete values that

control the distance between source and follow-up test cases. As an example taken from the case studies,

while testing an image analysis system, one possible transformation is to rotate the image, where the degree

of rotation is a user-deﬁned parameter. If the system is susceptible to treat wrongly rotated images, it is

likely that large rotations, e.g. by 90 degrees, are more likely to cause mistakes than smaller rotations.

By identifying the robust boundaries, information can be inferred about the trade-oﬀ between acceptable

transformations and exceedingly strong manipulations. This result yields both a robustness characteristic

and a starting point for a more curated set of requirements on the system under test.

5. Experimental Evaluation

5.1. Case Studies

We consider two case studies to evaluate our tool Tetraband. Both case studies come from the ﬁeld of dig-

ital image processing, where deep learning methods commonly represent the state-of-the-art approaches [11]:

image classiﬁcation and object detection. For each of the case studies, we consider both previously intro-

duced application scenarios (see Section 4) and identify fault-revealing MRs as well as robustness boundaries

against conﬁgurable image transformations. We have formulated the following three research questions as a

guideline for our experiments:

RQ1 Does Adaptive Metamorphic Testing, implemented as Tetraband, learn to select MRs whose follow-up

test cases reveal faults?

RQ2 Is AMT eﬀective to approximate the distribution of faults in the system under test?

RQ3 Is AMT computation- and data-eﬃcient compared to random sampling of MRs and exhaustive search

of all follow-up test cases?

11

(a) Image Classiﬁcation Example (from ImageNet [45]).

(b) Object Detection Example (from [46]). The goal is

The goal is to assign a single class label to the image, e.g.

to identify and categorize objects by drawing a bounding

toucan here.

box and assigning a class label.

Figure 1: Image classiﬁcation and object detection examples.

Previous work for testing image processing applications has considered random and metamorphic testing

[42, 43], but focused on the evaluation of handcrafted image processing applications, whereas we focus in

our experiments especially on machine learning-based computer vision systems. The existing studies focus

on testing the basic functionality of the system by generating random images and transforming them using

a set of transformations, diﬀerent from our approach where we base on an existing dataset of images from

the domain that the ML model has been trained on. We furthermore especially consider the selection of

good MRs. Xu et al. recently presented another use case for metamorphic relations in image classiﬁcation

applications beyond testing [44]. Their work uses metamorphic relations, based on separation and occlusion,

to augment the training data and ﬁne-tune the model.

In the following, we present the two case studies, image classiﬁcation and object detection, with their

setup and the considered MRs. We further discuss the conﬁguration of Tetraband, and ﬁnally, present the

experimental results and our ﬁndings.

5.2. Image Processing Applications

We describe two case studies where testing image processing systems is necessary (see Figure 1).

In

the ﬁrst case study, an image classiﬁcation system is tested. The second case study focuses on an object

detection application.

5.2.1. Image Classiﬁcation

An image classiﬁcation task, also image recognition, has the goal to identify the object shown in an

image, e.g., assign the image to one of a ﬁxed number of classes. Since 2012, the state-of-the-art method

for image classiﬁcation, among other image analysis tasks, are deep neural networks, such as residual neural

networks (ResNets) [47] or SqueezeNet [48].

12

person: 0.98cow: 1.00cow: 0.94cow: 0.90In our case study, we test a SqueezeNet model that has been initially trained on the ImageNet dataset [45]

and then ﬁne-tuned for the 10 classes of the CIFAR-10 dataset [49]. For testing the model, we use the

CIFAR-10 test set, consisting of 10, 000 labeled images.

Following the metamorphic relation between source and follow-up test cases, we consider a test as failed,

if the transformed image leads to a diﬀerent classiﬁcation result than the original image. The correctness

of the original class prediction does not inﬂuence the test result, because the bandit does not know the

initial model performance. Instead, the bandit aims to select transformations that aﬀect the outcome in a

fault-revealing manner compared to the original output. Testing the diﬀerence in outputs between source

and follow-up test cases removes the dependency from having to use labeled data, i.e. data where the

ground-truth class is known and allows the integration of other data sources. Nevertheless, for testing the

system, we monitor also the accuracy of the system for correctly classifying the images, but we do not use

this information as feedback for the test transformation bandit, although that would also be a viable setup.

5.2.2. Object Detection

Object detection is a generalization of the image classiﬁcation task in the sense that there can be

multiple objects on a single image. Besides assigning classes to these objects, it is also necessary to provide

bounding boxes around the location of each object. This means that the output of an object detection

model consists of a class label and four coordinates for the bounding box for each detected object. Object

detection systems employ deep neural networks of a similar, but extended, architecture compared to image

classiﬁcation systems.

The system to test in this case study is a pre-trained object detection model, based on the open-source

TensorFlow Object Detection API1 [50]. In particular, we test an implementation of a single shot multibox

detector (SSD) [46] with a feature pyramid network (FPN) [51], based on a ResNet-50 network [47]. We

refer the reader to the given references for an in-depth overview of the models. We see the system to test

here as a black box. However, brieﬂy said, the model detects objects in images with a single neural network

by assigning one of multiple predeﬁned box sizes, their size adjustment and classiﬁcation scores at the same

time. By reducing the complexity to a single neural network, it is a fast model for real-time object detection

that achieves state-of-the-art performance. The used model was trained on Microsoft COCO dataset [52]

and is available within the Object Detection API.2

For testing, we use 5, 000 images from the validation set of the MS COCO challenge 2017 as source test

cases. We apply the same input transformations on the images as in the image classiﬁcation case study

and as described in Section 5.3. Because each image annotation consists of an additional bounding box per

object in the image, the metamorphic relations are extended to transformation also on the bounding boxes.

1TensorFlow Object Detection API: github.com/tensorflow/models/tree/master/research/object_detection
2The exact name in the object detection model zoo is ssd resnet50 v1 fpn shared box predictor 640x640 coco14 sync 2018 07 03

13

For example, rotating the image rotates the bounding box of the object to match the rotated object, and

ﬂipping the image from left to right also ﬂips the positions of the bounding boxes in the image (see the next

Section 5.3 for an introduction of the applied transformations).

Furthermore, we consider the diﬀerent evaluation metrics for object detection tasks. In image classiﬁ-

cation, the result is easily veriﬁed by comparing the estimated class with the ground truth class. In object

detection, it is necessary to evaluate the overlapping regions between the estimated bounding boxes and the

ground truth, which is called the intersection-over-union (IoU), in addition to the class label of each box:

IoU (A, B) =

A ∩ B
A ∪ B

where A are the proposed pixels from the object detection model and B is the ground-truth from the

dataset.

If the IoU value exceeds a certain threshold, the object is counted as correctly detected. We

follow the evaluation guidelines from the MS COCO challenge and compare the results for the mean average

precision (mAP) which is calculated over all objects in an image and the average of diﬀerent IoU thresholds

0.5, 0.55, 0.6, . . . , 0.95, for both the original and the transformed image.

If the mAP of the transformed

image is below the original mAP minus a performance reduction of 0.05 which is close to 10% of the average

model performance, we interpret the test case to be violating the MR and therefore as failed.

5.3. Metamorphic Relations and Rewards

In the case studies of this paper, we use tests where the input is an image and the output is a classiﬁcation

of this image to recognize certain objects. The considered MRs are all related to image transformations,

e.g. mirroring or rotating, under the property that the results of image classiﬁcation and object detection

do not change, i.e., the MR deﬁnes equality of the outputs, while the input is transformed.

In most cases, these transformations must only not modify the class of these images. However, in object

detection, some transformations of images that impact object location markers entail similar transformation

over the outputs. Here, the MR deﬁnes a relation between the outputs that is similar to the transformation

of the input.

We select seven common image transformations among all possible transformations as MRs, some of

which have been used in previous work on metamorphic testing for image analysis methods [42, 38]. Among

these MRs, two are conﬁgurable by an additional parameter φ. These transformations are the following:

1. Blur the image by the average value of neighbor pixels (Blur)

2. Flip the image from the left to the right (Flip L/R)

3. Flip the image upside down (Flip U/D)

4. Convert a colored image to grayscale (Grayscale)

5. Invert the colors of the image (Invert)

14

(a) Original Image

(b) Blur

(c) Flip left ↔ right

(d) Flip up ↔ down (e)

Convert

to

grayscale

(f) Invert the image

(g) Rotate -30 degrees (h) Rotate +30 de-

(i) Shear -20 degrees

(j) Shear +20 degrees

grees

Figure 2: Metamorphic relations, i.e. image transformations, and their eﬀects. Image taken from the ImageNet dataset [45].

6. Rotate the image by x degrees (Rotation)

7. Shear the image by x degrees (Shear)

The eﬀects of these transformations are shown in Figure 2. The MRs Rotation and Shear expect a

parameter to deﬁne the transformation eﬀect. For Rotation, we consider 36 distinct values in steps of 5

degrees in the degree range [−90; 90], excluding rotations of 0 degrees. For shear, we include 18 values in

the range [−45; 45] in steps of 5 degrees, again excluding 0 degrees.

The reward for the main MRs is set up such that a failed test case is rewarded with 1 and a passed

test case with 0, independent of which MR was selected. For the action-speciﬁc bandits, which select the

MR parameters for Rotation and Shear, the reward structure is designed to encourage the selection of the

smallest failing parameters. Therefore, the smallest parameter of −5 respectively 5 degrees receives a reward

of 10000, if revealing a fault. For each additional step, the reward is divided by two. Thereby, choosing a

smaller parameter value has always higher payoﬀ than a larger rotation, if successful.

5.4. Implementation

We implemented Adaptive Metamorphic Testing in a tool called Tetraband. Our implementation package

is available at http://github.com/helges/tetraband. The software is implemented in Python 2.7 and it

is structured into two main components, as shown in Figure 3.

One component provides the SUTs used in our case studies, i.e. the image classiﬁcation and object

detection systems. These SUTs are encapsulated via the OpenAI Gym3 interface [53]. Having separate,

3OpenAI Gym: https://gym.openai.com/

15

Figure 3: Overview of Tetraband, our implementation of Adaptive Metamorphic Testing. It is a light-weight implementation

that allows extension and adaptation to other environments and settings.

standardized environments allows easier reproduction of the experiments and their usage in other work.

Their functionality includes the feature extraction for each SUT, as well as the application of the available

MRs. The metamorphic relations for image manipulation are realized with the imgaug library (version 0.2.6)

for image augmentation.4 Further details for the setup of the SUTs are given in the description of the case

studies in Section 5.2.

The second component is Tetraband itself, our implementation of AMT. It is mostly an adaptation of a

contextual bandit as the main actor, using the machine learning library Vowpal Wabbit 8.6.1.5 Additionally,

for comparison, we include a random agent that uniformly picks an arbitrary MR and conﬁguration.

The contextual bandits use the doubly robust policy evaluation algorithm for learning and action selection

[54]. Exploration is performed through a combination of epsilon-greedy exploration, where it chooses a

random action in 10 % of the iterations, and online cover exploration [14] with three policies. The policy

itself is approximated by a feed-forward artiﬁcial neural network with a single hidden layer with 16 neurons.

We choose a moderately high exploration rate, because we not only want the bandits to converge on few

single actions that repeatedly provide high payoﬀ, but we also want to learn about the eﬀectiveness of other

actions. An important aspect of contextual bandits is that the exploration is not reduced or disabled after

training, but stays active although self-adjusted to a lesser extent than in the initial iterations. This helps

to re-use the bandit to test the SUT repeatedly, e.g.

in Continuous Integration, because it can adapt to

changing behaviors in the SUT.

4imgaug: https://imgaug.readthedocs.io/
5Vowpal Wabbit: github.com/VowpalWabbit/vowpal_wabbit

16

Environments: System-under-TestImage ClassiﬁcationObject DetectionMetamorphicRelationsDatasetNeural NetworkMetamorphicRelationsDatasetNeural NetworkTetraband: Test Transformation BanditMain Control LoopRandom SelectionContextual BanditAgents5.5. Experimental Setup

At each iteration, the source test case is formed by an input image and its annotations from the data

set. The context feature vector is extracted through an additional neural network [55] which processes the

image and returns a feature vector of 512 ﬂoating-point numbers. The network is a pre-trained ResNet-18

network [47] from the PyTorch Torchvision model zoo.6 The ﬁnal layer, that usually outputs the identiﬁed

image class, has been removed and the output of the previous layer is used as the feature vector. We have

also experimented with perceptual image hashing for feature extraction but did not ﬁnd the hash value

expressive enough.

5.6. Fault-Revealing Metamorphic Relations

The ﬁrst experiment focuses on identifying general weaknesses in the system, i.e.

identifying which

metamorphic relations are fault-revealing for the system under test.

The bandit can freely choose from the seven transformations described in Section 5.3 and their reward

structure. A follow-up test case fails, i.e. it violates the MR, if the transformed image is classiﬁed diﬀerently

than the source image: SU T (S) (cid:54)= SU T (F). We deﬁne the MR to produce equal outputs for the source and

follow-up test cases, but we could instead also consider the annotated ground-truth labels for comparison.

Using the diﬀerence in outputs between source and follow-up test cases reduces the dependency on this

labeling, and is thereby applicable to unlabeled datasets.

5.7. Robustness Boundaries

The second experiment takes two speciﬁc transformations and learns the robustness boundaries of the

SUT for these transformations. As the robustness boundary, we describe the parameterization of the trans-

formation which changes the source test case as little as possible but is most likely to reveal a fault.

For evaluation, we use the two parameterized MRs that are already used in the other experiments, i.e.

Rotation and Shear transformation. For both MRs the same parameter space as described in Section 5.3 is

kept. The main diﬀerence in this experiment is that the focus lies on a single MR and its parameterization.

This allows us to speciﬁcally examine the weaknesses of the SUT towards one transformation and learn

about its robustness.

6. Experimental Results

For each case study, we have performed two experiments: identifying fault-revealing MRs and robustness

boundaries. Each run consists of one pass over the full training set, i.e. 10,000 iterations for image classi-

ﬁcation and 5,000 iterations for object detection. For each of the experiments, we show the mean result of

6PyTorch: https://pytorch.org / https://github.com/pytorch/vision

17

10 runs with diﬀerent random seeds. Our ﬁndings underline the eﬀectiveness of Tetraband for controlling

the metamorphic testing process in software testing, especially for testing machine learning systems.

6.1. Eﬀectiveness of Metamorphic Relations on Image Classiﬁcation

Before the evaluation of Tetraband, we ﬁrst analyze the eﬀectiveness of the selected MRs for our ex-

periments on the image classiﬁcation dataset, i.e. CIFAR-10. We aim to understand whether there are

diﬀerent eﬀects of diﬀerent MRs and how they aﬀect certain classes of images. To this end, all MRs were

applied to all images of the dataset, which we refer to in the other experiments as the baseline reference

and the changes in the predicted classes are observed. The baseline results reported for image classiﬁcation

correspond thereby to the average violation rate over all classes as shown in the rightmost column of the

table.

Table 1 shows the percentage of images in a class that is aﬀected by a MR, such that they are wrongly

classiﬁed afterward. CIFAR-10 consists of ten classes of images, printed as column names. The eﬀectiveness

of the image transformations varies between both MRs and image classes. The MR Flip U/D is an example

of a particularly eﬀective transformation that aﬀects over 50% of the images in the test set. However, when

noting the diﬀerent classes in the dataset, all images have a clear vertical orientation and the ﬂipped image

of the objects is unlikely to occur in the dataset, for example for automobiles where 74.6% of the images are

misclassiﬁed. Images of frogs or airplanes, where the perspectives of the images vary more naturally are less

aﬀected, but still to a moderately high degree compared to other MRs. This is diﬀerent for the MR Flip

L/R where the horizontal orientation is reversed, which corresponds to a variation that is already included

in the training data and therefore is the least eﬀective MR. Other MRs identify stronger diﬀerences between

classes. Converting the image to grayscale has little eﬀect on airplanes or ships, which commonly have

few distinct features related to color, but large eﬀects for birds or dogs, where colors are more distinctive

characteristics.

Conclusively, from the experiment using the image classiﬁcation dataset, we see diﬀerent eﬀects per MR

and image, which underlines the motivation to learn which MRs are eﬀective for a particular image.

6.2. Fault-Revealing Metamorphic Relations

6.2.1. Case Study 1: Image Classiﬁcation

In our ﬁrst case study, which is an image classiﬁcation ML model, the goal of the bandit is to select a

MR which leads to a diﬀerent classiﬁcation of the transformed image compared to the original image of the

source test case. While we looked at the true classes of the source images in the previous initial experiment,

we only consider the change of the predicted class from source to a follow-up test case in this experiment.

This approach focuses on the consistency of outputs for diﬀerent variants of the same input image, i.e.,

the follow-up test cases. The actual true label is not as relevant in this case, as it is unlikely to ﬁnd one

18

Airplane Automobile

Bird

Cat

Deer

Dog

Frog Horse

Ship Truck

Avg.

Blur

Flip L/R

Flip U/D

Grayscale

Invert

Rotation

Shear

Avg.

10.60

2.90

14.90

4.70

16.50

25.49

11.22

12.33

11.40

13.10

1.00

4.10

9.81

6.71

7.30

13.50

17.70

2.20

6.80

1.30

9.00

2.40

6.00

0.90

6.20

10.46

2.40

3.07

74.60

37.80

33.13

59.10

53.90

29.30

92.40

72.20

43.30

51.06

5.40

28.10

7.91

18.10

26.00

14.30

6.70

4.80

5.30

12.13

29.40

29.50

33.13

41.40

70.30

41.80

38.30

27.30

35.70

36.33

37.09

35.43

17.70

69.00

46.10

20.63

60.44

42.44

50.01

40.43

4.99

26.69

35.79

45.45

51.97

15.63

40.24

19.78

55.24

30.70

23.41

24.96

20.60

34.65

38.37

20.10

35.64

24.77

28.31

26.31

Table 1: CIFAR-10 dataset: Eﬀects of MRs by the true class of the image. Each cell value shows the percentage of images

in the class, which are wrongly classiﬁed after applying the MR. Every class contains 1000 images. Rotation and Shear are

parameterized by 30 degrees. The values are determined using the baseline method, i.e. exhaustive search over all MRs and all

images. The rightmost column (Avg.) corresponds to the baseline results in the image classiﬁcation case study (see Figure 4a).

MR which transforms the image consistently in a way that the correct label is predicted. Additionally, the

accuracy, i.e., the correctness of the outputs is usually already tested during or after the training of the

model. Focusing on the diﬀerence between outputs for source and follow-up test cases furthermore allows

to extend the set of source test cases from unlabelled datasets, making it easier to enlarge the system’s test

suite.

For the seven main transformations, Figure 4a shows the distribution of the violation rate for each follow-

up test case generated by a selected MR. We compare this violation rate of MRs selected by Tetraband to

the true violation rate for the set of source test cases, which corresponds to the rightmost column (Avg.)

in Table 1. These ground-truth results form our baseline and are determined by applying all available MRs

to all source test cases in an exhaustive search. While this exhaustive search covers all possible follow-

up test cases, it is time- and resource-intense compared to adaptive metamorphic testing and not ideally

suited for repeated testing. We have therefore considered the random selection of a MR, such as it is

common in traditional metamorphic testing, as the comparison method for computational cost and resource

consumption. Conclusively, to discuss the quality of the selected MRs with Tetraband, we use exhaustive

search as the baseline. The evaluation of computational cost, including a comparison to the common random

selection, is discussed separately in Section 6.2.3.

The violation rate estimates how often the image classiﬁer changes its prediction after the MR was

applied to the source image. This can be diﬀerent from the failure rate in cases where the prediction on

the original image was wrong, but the transformation made the model predict the correct output. However,

as we do not expect the test data to be labeled, we do not require and consider this information for our

experiments and instead focus on the robustness and consistency of the model for the predictions. Therefore,

19

)

%
n
i
(

e
t
a
R
n
o
i
t
a
l
o
i
V

)

%
n
i
(

e
t
a
R
n
o
i
t
a
l
o
i
V

50

40

30

20

10

0

80

60

40

20

0

Tetraband
Baseline

Flip L/R

Blur

Grayscale

Shear

Invert

Rotation

Flip U/D

Metamorphic Relation

(a) Violation rate of MRs selected by Tetraband and baseline results for exhaustive search
80

Tetraband
Baseline

60

40

20

0

−80 −60 −40 −20

0

20

40

60

80

−40 −30 −20 −10

0

10

20

30

40

(b) Conﬁguration of Rotation transformation (in degrees)

(c) Conﬁguration of Shear transformation (in degrees)

Figure 4: Fault-Revealing MRs for image classiﬁcation: Violation rate and conﬁguration of parameterized MR transformations.

Tetraband approximates the true error distribution and select fault-revealing MRs and their parameters.

we mostly report the violation rate. Ideally, a perfectly-trained image classiﬁcation model should not show

any change and the violation rates should be zero independently of the selected MR.

Generally speaking, the violation rate distribution shows a diﬀerent impact for diﬀerent MRs. The least

fault-revealing MR is the one that mirrors images over the middle vertical axis, i.e., ﬂip left and right.

This transformation is most likely to be found in the training datasets for the image classiﬁer. Often the

image is either symmetric by itself, like a human face or body, or included together with another image

of the same object but taken from a diﬀerent angle, showing a symmetric proﬁle. Other MRs are more

eﬀective to reveal faults in the image classiﬁer, which is related to the disturbance impact they have on

the original image. They often represent transformations that could be expected in real-world applications

and should be covered by a robust image classiﬁcation system. As seen from the exhaustive search baseline

experiment, the most fault-revealing MR identiﬁed by Tetraband showed to be ﬂipping the image upside-

down, i.e., mirroring on the middle of the horizontal axis. This is within expectations, as it results in an

image, which is unlikely to be represented in the distribution of training images, e.g., an image of a car

is likely to be shown with its wheels on the ground. Nevertheless, high violation rates are also observed

for less invasive MRs, such as rotating the image or inverting it, and these MRs are either likely to be

encountered in practical applications or preserve many of the distinctive features. Having this statistic not

only allows us to identify the weaknesses of the classiﬁer (model testing), but it can also be used as a basis

20

to conﬁgure image augmentation techniques to train a new version of the image classiﬁcation model (model

training). With image augmentation, the training set of images is extended by including modiﬁed versions

of the original image, through small perturbations or aﬃnity scaling, while preserving the original label.

By knowing the MRs that fail the old classiﬁer, the necessary transformations to include in future image

augmentation are known and can help to improve the performance of new models. However, not all MRs

are necessarily suitable image augmentations at training time, as they might produce images that are not

within the distribution of inputs, i.e. images, for which the model is trained.

The performance of Tetraband closely approximates the violation distribution for the baseline results

and exceeds them for all MRs, except Rotation where the violation rate is close to the baseline violation

rate. The reason for the lower violation rate is related to the necessary exploration to select an appropriate

parameter to rotate the source image. Due to the large number of parameters from −90 to 90 the bandit

exploration had to take ineﬀective actions to learn, which leads to an initially lower violation rate.

However, we also observe that longer runtime and more iterations over the dataset further increase the

averaged violation rate as the bandit algorithm can focus more on exploitation than exploration. This can be

explained as initial iterations do not have knowledge about the MRs eﬀectiveness and the selection is more

random, which includes selecting ineﬀective MRs. While these actions are valuable for exploration, they do

not contribute to the violation rate. Later, when the eﬀect of MRs has been suﬃciently explored, the focus

changes on also exploiting the MRs and selecting fault-revealing MRs. These actions then contribute to

increasing the violation rate. Accordingly, if the system under test does not rapidly change, more iterations

will increase the amount of exploitation with high violation rates and the impact of the initial exploration

on the violation rate decreases.

The violation distribution for the diﬀerent parameters of the MRs Rotation and Shear are shown in

Figure 4b and Figure 4c. The bandit eﬀectively picks the appropriate degree of rotation to closely resemble

the true error distribution. The only exceptions are the largest degrees of rotation, where the selection

does not completely approximate the true distribution. However, due to our reward structure, the agent is

encouraged to focus on minimal rotations in the images that lead to some misclassiﬁcation. Especially in

the range between −45 and 45 degrees the parameter selection is appropriate, which indicates the successful

convergence towards the most revealing parameter for image rotation.

In the Shear transformation, Tetraband was less eﬀective to smoothly approximate the true error distri-

bution but broadly follows its shape. Here, we observe that due to the lower general violation rate of the

MR, there are fewer chances for successful exploration of the parameter space than for the Rotation MR.

Accordingly, the approximation of the parameter distribution for Shear is not as close as for Rotation, but

still follows the general distribution.

21

)

%
n
i
(

e
t
a
R
n
o
i
t
a
l
o
i
V

)

%
n
i
(

e
t
a
R
n
o
i
t
a
l
o
i
V

80

60

40

20

0

100

90

80

70

60

50

Tetraband
Baseline

Flip L/R

Blur

Grayscale

Invert

Shear

Flip U/D

Rotation

Metamorphic Relation

(a) Violation rate of MRs selected by Tetraband and baseline results for exhaustive search

Tetraband
Baseline

100

90

80

70

60

50

−80 −60 −40 −20

0

20

40

60

80

−40 −30 −20 −10

0

10

20

30

40

(b) Conﬁguration of Rotation transformation (in degrees)

(c) Conﬁguration of Shear transformation (in degrees)

Figure 5: Fault-Revealing MRs for object detection: Violation rate and conﬁguration of parameterized MR transformations.

Tetraband approximates the true error distribution and select fault-revealing MRs and their parameters.

6.2.2. Case Study 2: Object Detection

The second case study application is the object detection neural network. Similar to the presentation of

the ﬁrst case study, Figure 5 shows the results of the object detection case study. As a ﬁrst main diﬀerence,

the results show a higher violation rate for object detection, with over 70% violation rate for four of the

seven MRs. The ranking of MRs is similar but applying the Shear MR with parameter selection is more

eﬃcient here than in the image classiﬁcation case study, where the Invert MR showed a higher violation rate,

and Rotation is the most eﬀective MR. The MRs Flip L/R, Blur and Grayscale have the lowest violation

rate, i.e., the model is most robust to these changes. However, while the total violation rate is higher, the

case study itself is more diﬃcult as the dataset on which we apply Tetraband is smaller and only consists

of 5000 images, which means the overall process takes half of the iterations of the image classiﬁcation case

study. This diﬀerence explains the approximation diﬀerence for some of the main MRs in comparison to the

baseline violation rates.

For the two MRs Rotation and Shear, the approximation quality for the additional parameter is similar

to the image classiﬁcation results. For Rotation, the distribution is close to the true distribution of the

exhaustive search baseline results, related to the high overall violation rate for this MR and its correspond-

ing higher selection and thereby better exploration opportunities. The Shear parameters match the true

distribution less closely in this case study, which is also related to the higher general eﬀectiveness of this

22

Runtime (in s)

Accuracy (in %)

Case Study

Tetraband Random Unmodiﬁed Tetraband Random

Image Classiﬁcation

Object Detection

0.1

0.63

0.06

0.56

96.6

54.3

54.0

23.1

72.9

36.3

Table 2: Computational cost and accuracy of Tetraband and random selection for selecting fault-revealing MRs. The average

runtime per image shows that the overhead introduced by the ML model is relatively small, especially for object detection with

a more costly test execution. Tetraband selects MRs more eﬀectively and the accuracy is smaller than with random selection.

MR for the object detection dataset.

In general, the results of the object detection case study conﬁrm the results of the image classiﬁcation

case study while at the same time respecting the higher diﬃculty of fewer iterations, due to which the

violation rate of the main MRs is close to the true violation rate, but does not exceed it after the given

number of iterations.

6.2.3. Computational Cost and Random Selection

From the previous experiments, we have evaluated the eﬀectiveness of Tetraband for selecting fault-

revealing MRs in image classiﬁcation and object detection systems. However, we did not consider the

computational cost of introducing machine learning in the MT process or compared Tetraband to the

commonly used approach of randomly selecting MRs and their parameters. In this experiment, to answer

RQ3, we analyze common characteristics for both case studies, which consider the computational cost of

introduced contextual bandits and the additional learning step in the MR process. We further brieﬂy discuss

another comparison method that more closely resembles the state-of-practice in MT, which is to randomly

select MRs and their parameters. A summary of the results is given in Table 2.

Running Tetraband is computationally cheaper and more sample-eﬃcient than the exhaustive search

that we consider as a baseline. While the exhaustive search considers all MRs with all diﬀerent parameters

for Rotation and Shear, in total 59 diﬀerent transformations, Tetraband selects one MR and one parameter

per iteration. In addition to the application of the MR and the execution of the SUT, Tetraband has a small

computational overhead for selecting the MR and its parameter and learning from the received feedback.

The average duration per iteration in the image classiﬁcation case study is 0.1s using Tetraband and 0.06s

when using a random MR and not learning from feedback. While this overhead increases the execution

time, it is still faster and more eﬃcient than running all possible transformations, as we will see below. For

object detection, the average duration is 0.63s with learning and 0.56s without, here the main computation

lies in the object detection neural network. The overhead for training the contextual bandit could be further

reduced by moving the learning step outside the main processing loop; however, we argue that in a practical

23

)

%
n
i
(

e
t
a
R
n
o
i
t
a
l
o
i
V

80

60

40

20

0

Tetraband
Baseline

80

60

40

20

0

−80 −60 −40 −20

0

20

40

60

80

−40 −30 −20 −10

0

10

20

30

40

(a) Conﬁguration of Rotation transformation (in degrees)

(b) Conﬁguration of Shear transformation (in degrees)

Figure 6: Image classiﬁcation: Average violation rate per degree step. The approximated violation rate closely approximates

the true violation distribution of the ground truth baseline, i.e. exhaustive search.

application the overhead is negligible due to the lower number of executions and the availability of highly

optimized contextual bandit implementations.

We also considered a random selection of MRs and their parameters. At each iteration, a random MR

is sampled uniformly instead of using the bandit selection. This selection is less eﬃcient than Tetraband.

Using Tetraband, the accuracy of the image classiﬁer is reduced from 96.6% for the unmodiﬁed images to

54.0% for the images modiﬁed by the selected MRs. With random selection, the accuracy for the modiﬁed

images remains at 72.9%, which is a substantial reduction, but not as high as Tetraband. In object detection,

the precision is reduced from 54.3% to 23.1% with Tetraband and to 36.3% with random selection. Due to

the lower general eﬀectiveness of random selection, we do not further discuss its results in detail.

6.3. Robustness Boundaries

As a second experiment and application of Tetraband to learning MR selection, we aim to ﬁnd the

robustness boundaries of the case study systems against diﬀerent degrees of image rotations and shearing.

We show the results for both case study applications in Figure 6 for image classiﬁcation and Figure 7

for object detection. The experimental results mostly conﬁrm the inherent hypothesis, also following the

previous results from the ﬁrst experiments that larger modiﬁcations of the source test case lead to a higher

violation rate of the follow-up test case. This is true for both case study applications, but the extent to

which the eﬀect applies varies with the object detection system being much more susceptible to images

rotated even only by small degrees. At the same time, we conﬁrm that a larger number of iterations allows

better exploration and approximation of the true error distribution. Where the parameter distribution in

the previous experiments showed divergence, mostly for larger parameter values, the focus on the speciﬁc

MRs in this experiment allows suﬃcient exploration and good approximation.

The results for image classiﬁcation clearly show the eﬀect that larger rotations of the original image

are more likely to cause a diﬀerent classiﬁcation (see Figure 6a). Here, the bandit does not only learn to

24

)

%
n
i
(

e
t
a
R
n
o
i
t
a
l
o
i
V

100

90

80

70

60

50

Tetraband
Baseline

100

90

80

70

60

50

−80 −60 −40 −20

0

20

40

60

80

−40 −30 −20 −10

0

10

20

30

40

(a) Conﬁguration of Rotation transformation (in degrees)

(b) Conﬁguration of Shear transformation (in degrees)

Figure 7: Object detection: Average violation rate per degree step. The approximated violation rate closely approximates the

true violation distribution of the ground truth baseline, i.e. exhaustive search.

select the largest rotation but does accurately approximate the distribution of true faults as given by the

exhaustive search baseline. Our results show that for more than 10% of the source test cases a rotation of

at least 10 degrees leads to a diﬀerent image classiﬁcation. When considering real-world scenarios for the

application of image classiﬁcation systems, a rotation of 10 degrees can likely occur due to tilt or shifts in

either the camera or due to external inﬂuences on the actual object.

For object detection, the interpretation of the results needs to consider two aspects, which lead to the

conclusion that the results can not be directly compared to the image classiﬁcation case study. First, the

original SUT already has lower performance for the original data set than the image classiﬁcation SUT.

When considering the system to be more imprecise for unmodiﬁed data, then it is also likely to be more

fragile for modiﬁed data. Second, the evaluation metric used, mean average precision, is more fragile than

the metric used for image classiﬁcation. The results show the fragility of the object detection system, as well

as the capability to learn to approximate this error distribution over transforming each of the 5000 source

test case images only once.

6.4. Discussion

For both case studies in our experiments, our results showed the eﬀectiveness of contextual bandits, as

part of Tetraband, to adapt to a prior unknown error distribution in two diﬀerent case study applications,

based on two diﬀerent neural network architectures and tasks.

From the results, we draw two major conclusions. First, we see a conﬁrmation for the applicability

of Tetraband for selecting metamorphic relations using contextual bandits (i.e., Adaptive Metamorphic

Testing), as is shown by the close approximation of the true error distribution with limited iterations. Second,

our tests reveal robustness weaknesses in the two systems-under-test. Weaknesses in neural networks have

been addressed before and are an active research area [56, 57, 58]. The research under the area of adversarial

examples focuses on ﬁnding input perturbations that lead to misbehavior of the model with only minimal or

25

hard-to-detect changes in the input. This approach is diﬀerent from the setting of our experiment. We select

distinct and known image transformations to modify the image without the goal to hide the transformation,

which often is the intent of an adversarial example.

7. Conclusion

This paper introduces Adaptive Metamorphic Testing (AMT), a method to control metamorphic testing

using contextual bandits. AMT receives a feature vector representing the source test case and selects a MR

to generate a follow-up test case. From the result of evaluating the follow-up test case, whether it reveals a

fault, the bandit learns which MRs can be used to exploit weaknesses in the system. At the same time, using

state-of-the-art algorithms for contextual bandits, AMT explores non-optimal actions to identify previously

unknown weaknesses and to adapt to changing behavior in repeated testing of the same system.

We have evaluated the applicability of AMT using our implementation Tetraband on two image analysis

case studies in two distinct tasks. For both case studies, our results showed that Tetraband approximates

the true distribution of faults in the SUT with fewer iterations and executions of the SUT than an exhaustive

search and more eﬃciently than random sampling of MRs and their parameters, which is the common best

practice. Tetraband learns to select fault-revealing MRs in relation to the source test case while ignoring non-

relevant MRs. Furthermore, in the second experiment, Tetraband proved to be eﬀective for the identiﬁcation

of robustness boundaries, that is, exploring the parameterization of individual MRs, which have diﬀerent

impacts on the SUT. Our experiment explored how diﬀerent degrees of rotation and shear aﬀected the

classiﬁcation result of the transformed image.

In conclusion, AMT is eﬀective for selecting MRs and is more time-eﬃcient than exhaustive testing

and more eﬀective than the standard approach of pure random sampling. We see the method to be useful

in repeated testing scenarios, such as continuous integration, where regression of the SUT can be tested

from an initial knowledge about previous fault characteristics. For future work, we plan to investigate the

combination of multiple MRs to create follow-up test cases instead of selecting only one MR per iteration.

Acknowledgements

This work is supported by the Research Council of Norway (RCN) through the research-based innovation

center Certus, under the SFI program. The experiments were performed on the Abel Cluster, owned by the

University of Oslo and Uninett/Sigma2, and operated by the Department for Research Computing at USIT,

the University of Oslo IT-department.

Declarations of Interest

None of the authors declares a conﬂict of interest.

26

References

[1] T. Chen, S. Cheung, S. Yiu, Metamorphic Testing: A New Approach for Generating Next Test Cases, Technical Report

HKUST-CS98-01, Department of Computer Science, Hong Kong University of Science and Technology, Hong Kong (1998).

[2] T. Y. Chen, F.-C. Kuo, H. Liu, P.-L. Poon, D. Towey, T. H. Tse, Z. Q. Zhou, Metamorphic Testing: A Review of

Challenges and Opportunities, ACM Computing Surveys 51 (1) (2018). doi:10.1145/3143561.

[3] E. T. Barr, M. Harman, P. McMinn, M. Shahbaz, S. Yoo, The Oracle Problem in Software Testing: A Survey, IEEE

Transactions on Software Engineering 41 (5) (2015) 507–525. doi:10.1109/TSE.2014.2372785.

[4] C. Murphy, G. Kaiser, L. Hu, L. Wu, Properties of Machine Learning Applications for Use in Metamorphic Testing,

Proceedings of the 20th International Conference on Software Engineering and Knowledge Engineering (SEKE) (2008)

867–872doi:10.7916/D8XK8PFD.

[5] J. Ding, X.-H. Hu, V. Gudivada, A Machine Learning Based Framework for Veriﬁcation and Validation of Massive Scale

Image Data, IEEE Transactions on Big Data 26 (3) (2017) 1–1. doi:10.1109/TBDATA.2017.2680460.

[6] A. Dwarakanath, M. Ahuja, S. Sikand, R. M. Rao, R. P. J. C. Bose, N. Dubash, S. Podder, Identifying implementation

bugs in machine learning based image classiﬁers using metamorphic testing, in: Proceedings of the 27th ACM SIGSOFT

International Symposium on Software Testing and Analysis (ISSTA), 2018, pp. 118–128. doi:10.1145/3213846.3213858.

[7] Z. Q. Zhou, L. Sun, Metamorphic Testing of Driverless Cars, Communications of the ACM 62 (3) (2019) 61–67. doi:

10.1145/3241979.

[8] Z. Q. Zhou, S. Xiang, T. Y. Chen, Metamorphic Testing for Software Quality Assessment: A Study of Search Engines,

IEEE Transactions on Software Engineering 42 (3) (2016) 264–284. doi:10.1109/TSE.2015.2478001.

[9] M. P. Shahri, M. Srinivasan, G. Reynolds, D. Bimczok, I. Kahanda, U. Kanewala, Metamorphic Testing for Quality

Assurance of Protein Function Prediction Tools, in: 2019 IEEE International Conference On Artiﬁcial Intelligence Testing

(AITest), 2019, pp. 140–148. doi:10.1109/AITest.2019.00017.

[10] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (7553) (2015) 436–444. doi:10.1038/nature14539.

[11] S. Pouyanfar, S. Sadiq, Y. Yan, H. Tian, Y. Tao, M. P. Reyes, M.-L. Shyu, S.-C. Chen, S. S. Iyengar, A Survey on Deep

Learning: Algorithms, Techniques, and Applications, ACM Computing Surveys 51 (5) (2018) 1–36. doi:10.1145/3234150.

[12] J. Langford, T. Zhang, The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information, in: Advances in

Neural Information Processing Systems 20 (NeurIPS 2007), 2007, pp. 817–824.

[13] L. Zhou, A Survey on Contextual Multi-armed Bandits, arXiv preprint arXiv:1508.03326 (2016).

[14] A. Agarwal, D. Hsu, S. Kale, J. Langford, L. Li, L. G. Oct, Taming the Monster: A Fast and Simple Algorithm for

Contextual Bandits, in: International Conference on Machine Learning, 2014, pp. 1638–1646.

[15] T. Lattimore, C. Szepesvari, Bandit Algorithms, Vol. Revision:

8b22b8b6131c37e388d5e3b2eecf0b4ﬀ5d7db92,

https://banditalgs.com/, 2019.

[16] R. S. Sutton, A. G. Barto, Reinforcement Learning: An Introduction, 2nd Edition, MIT Press, 2018.

[17] L. Li, W. Chu, J. Langford, R. E. Schapire, A contextual-bandit approach to personalized news article recommendation,

in: International Conference on World Wide Web (WWW), 2010, pp. 661–670. doi:10.1145/1772690.1772758.

[18] T. Lu, D. Pal, M. Pal, Contextual Multi-Armed Bandits, in: Proceedings of the 13th International Conferenceon Artiﬁcial

Intelligence and Statistics (AISTATS), 2010, pp. 485–492.

[19] L. Tang, R. Rosales, A. Singh, D. Agarwal, Automatic ad format selection via contextual bandits, in: Proceedings of

the 22nd ACM International Conference on Conference on Information & Knowledge Management, 2013, pp. 1587–1594.

doi:10.1145/2505515.2514700.

[20] N. Baskiotis, M. Sebag, M.-C. Gaudel, S.-D. Gouraud, EXIST: Exploitation/Exploration Inference for Statistical Software

Testing, in: On-Line Trading of Exploration and Exploitation, NeurIPS Workshop, 2006.

27

[21] M. Loth, M. Sebag, Y. Hamadi, M. Schoenauer, Bandit-based Search for Constraint Programming, in: International

Conference on Principles and Practice of Constraint Programming, 2013, pp. 464–480.

[22] A. Balafrej, C. Bessiere, A. Paparrizou, Multi-armed bandits for adaptive constraint propagation, International Joint

Conference on Artiﬁcial Intelligence (2015) 290–296.

[23] S. Onta˜n´on, Combinatorial Multi-armed Bandits for Real-Time Strategy Games, Journal of Artiﬁcial Intelligence Research

58 (1) (2017) 665–702. doi:10.1613/jair.5398.

[24] S. Segura, G. Fraser, A. B. Sanchez, A. Ruiz-Cortes, A Survey on Metamorphic Testing, IEEE Transactions on Software

Engineering 42 (9) (2016) 805–824. doi:10.1109/TSE.2016.2532875.

[25] L. Sun, Z. Q. Zhou, Metamorphic Testing for Machine Translations: MT4MT, in: 2018 25th Australasian Software

Engineering Conference (ASWEC), 2018, pp. 96–100. doi:10.1109/ASWEC.2018.00021.

[26] S. Segura, J. Troya, A. Dur´an, A. Ruiz-Cort´es, Performance metamorphic testing: A Proof of concept, Information and

Software Technology 98 (2018) 1–4. doi:10.1016/j.infsof.2018.01.013.

[27] O. Johnston, D. Jarman, J. Berry, Z. Q. Zhou, T. Y. Chen, Metamorphic Relations for Detection of Performance Anomalies,

in: 2019 IEEE/ACM 4th International Workshop on Metamorphic Testing (MET), 2019, pp. 63–69. doi:10.1109/MET.

2019.00017.

[28] ¨O. Akg¨un, I. P. Gent, C. Jeﬀerson, I. Miguel, P. Nightingale, Metamorphic Testing of Constraint Solvers, in: J. Hooker

(Ed.), Principles and Practice of Constraint Programming, Vol. 11008 of LNCS, 2018, pp. 727–736. doi:10.1007/

978-3-319-98334-9\_46.

[29] A. Gotlieb, B. Botella, Automated metamorphic testing, Proceedings 27th Annual International Computer Software and

Applications Conference (2003) 34–40doi:10.1109/CMPSAC.2003.1245319.

[30] J. Mayer, R. Guderlei, An Empirical Study on the Selection of Good Metamorphic Relations, in: 30th Annual International

Computer Software and Applications Conference, 2006, pp. 475–484. doi:10.1109/COMPSAC.2006.24.

[31] U. Kanewala, J. M. Bieman, Using machine learning techniques to detect metamorphic relations for programs without

test oracles, 2013 IEEE 24th International Symposium on Software Reliability Engineering (ISSRE) (2013). doi:10.1109/

ISSRE.2013.6698899.

[32] U. Kanewala, J. M. Bieman, A. Ben-Hur, Predicting metamorphic relations for testing scientiﬁc software: A machine

learning approach using graph kernels, Software Testing, Veriﬁcation and Reliability 26 (3) (2016) 245–269. doi:10.1002/

stvr.1594.

[33] K. Pei, Y. Cao, J. Yang, S. Jana, DeepXplore: Automated Whitebox Testing of Deep Learning Systems, in: Proceedings

of the 26th Symposium on Operating Systems Principles, ACM Press, 2017, pp. 1–18. doi:10.1145/3132747.3132785.

[34] L. Ma, Y. Liu, J. Zhao, Y. Wang, F. Juefei-Xu, F. Zhang, J. Sun, M. Xue, B. Li, C. Chen, T. Su, L. Li, DeepGauge: Multi-

granularity testing criteria for deep learning systems, in: Proceedings of the 33rd ACM/IEEE International Conference

on Automated Software Engineering - ASE 2018, 2018, pp. 120–131. doi:10.1145/3238147.3238202.

[35] X. Xie, J. W. Ho, C. Murphy, G. Kaiser, B. Xu, T. Y. Chen, Testing and validating machine learning classiﬁers by

metamorphic testing, Journal of Systems and Software 84 (4) (2011) 544–558. doi:10.1016/j.jss.2010.11.920.

[36] W. K. Chan, J. C. F. Ho, T. H. Tse, Finding failures from passed test cases:

Improving the pattern classiﬁcation

approach to the testing of mesh simpliﬁcation programs, Software Testing, Veriﬁcation and Reliability 20 (2) (2010)

89–120. doi:10.1002/stvr.408.

[37] S. Yang, D. Towey, Z. Q. Zhou, Metamorphic Exploration of an Unsupervised Clustering Program, in: Proceedings of the

4th International Workshop on Metamorphic Testing, 2019, pp. 48–54. doi:10.1109/MET.2019.00015.

[38] R. R. Mekala, G. E. Magnusson, A. Porter, M. Lindvall, M. Diep, Metamorphic Detection of Adversarial Examples in

Deep Learning Models with Aﬃne Transformations, in: Proceedings of the 4th International Workshop on Metamorphic

Testing, 2019, pp. 55–62. doi:10.1109/MET.2019.00016.

28

[39] P. Saha, U. Kanewala, Fault Detection Eﬀectiveness of Metamorphic Relations Developed for Testing Supervised

Classiﬁers,

in: 2019 IEEE International Conference On Artiﬁcial Intelligence Testing (AITest), 2019, pp. 157–164.

doi:10.1109/AITest.2019.00019.

[40] K.-Y. Cai, B. Gu, H. Hu, Y.-C. Li, Adaptive software testing with ﬁxed-memory feedback, Journal of Systems and Software

80 (8) (2007) 1328–1348. doi:10.1016/j.jss.2006.11.008.

[41] Z. Q. Zhou, A. Sinaga, W. Susilo, L. Zhao, K.-Y. Cai, A cost-eﬀective software testing strategy employing online feedback

information, Information Sciences 422 (2018) 318–335. doi:10.1016/j.ins.2017.08.088.

[42] J. Mayer, R. Guderlei, On Random Testing of Image Processing Applications, in: 2006 Sixth International Conference on

Quality Software (QSIC’06), 2006, pp. 85–92. doi:10.1109/QSIC.2006.45.

[43] R. Guderlei, J. Mayer, Towards automatic testing of imaging software by means of random and metamorphic test-

ing, International Journal of Software Engineering and Knowledge Engineering 17 (06) (2007) 757–781. doi:10.1142/

S0218194007003471.

[44] L. Xu, D. Towey, A. P. French, S. Benford, Z. Q. Zhou, T. Y. Chen, Enhancing supervised classiﬁcations with metamorphic

relations, in: Proceedings of the 3rd International Workshop on Metamorphic Testing - MET ’18, 2018, pp. 46–53.

doi:10.1145/3193977.3193978.

[45] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C.

Berg, L. Fei-Fei, ImageNet Large Scale Visual Recognition Challenge, International Journal of Computer Vision 115 (3)

(2015) 211–252. doi:10.1007/s11263-015-0816-y.

[46] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-y. Fu, A. C. Berg, SSD: Single Shot MultiBox Detector, in:

European Conference on Computer Vision, Vol. 9905 of LNCS, 2016, pp. 21–37. doi:10.1007/978-3-319-46448-0\_2.

[47] K. He, X. Zhang, S. Ren, J. Sun, Identity mappings in deep residual networks, in: European Conference on Computer

Vision, Vol. 9908 of LNCS, 2016, pp. 630–645. doi:10.1007/978-3-319-46493-0\_38.

[48] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, K. Keutzer, SqueezeNet: AlexNet-level accuracy with

50x fewer parameters and <0.5MB model size, arXiv preprint arXiv:1602.07360 (2016).

[49] A. Krizhevsky, V. Nair, G. Hinton, The CIFAR-10 dataset, online: http://www. cs. toronto. edu/kriz/cifar. html (2014).

[50] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, K. Murphy,

Speed/Accuracy Trade-Oﬀs for Modern Convolutional Object Detectors, in: IEEE Conference on Computer Vision and

Pattern Recognition (CVPR), 2017, pp. 3296–3297. doi:10.1109/CVPR.2017.351.

[51] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, S. Belongie, Feature Pyramid Networks for Object Detection,

in: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 2017, pp. 936–944. doi:

10.1109/CVPR.2017.106.

[52] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll´ar, C. L. Zitnick, Microsoft COCO: Common

Objects in Context, in: European Conference on Computer Vision, Vol. 8693 of LNCS, 2014, pp. 740–755. doi:10.1007/

978-3-319-10602-1\_48.

[53] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, W. Zaremba, OpenAI Gym, arXiv:1606.01540

[cs] (Jun. 2016).

[54] M. Dud, J. Langford, Doubly Robust Policy Evaluation and Learning, in: Proceedings of the 28th International Conference

on Machine Learning, 2011, pp. 1097–1104.

[55] A. Sharif Razavian, H. Azizpour, J. Sullivan, S. Carlsson, CNN Features Oﬀ-the-Shelf: An Astounding Baseline for

Recognition, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2014, pp.

806–813.

[56] I. Goodfellow, H. Lee, Q. V. Le, A. Saxe, A. Y. Ng, Measuring Invariances in Deep Networks, in: Advances in Neural

Information Processing Systems, Vol. 22, 2009, pp. 646–654.

29

[57] N. Carlini, D. Wagner, Towards Evaluating the Robustness of Neural Networks, in: IEEE Symposium on Security and

Privacy, 2017, pp. 39–57. doi:10.1109/SP.2017.49.

[58] B. Biggio, F. Roli, Wild patterns: Ten years after the rise of adversarial machine learning, Pattern Recognition 84 (2018)

317–331. doi:10.1016/j.patcog.2018.07.023.

30

