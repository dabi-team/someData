1
2
0
2

p
e
S
3
1

]

G
L
.
s
c
[

1
v
6
6
8
5
0
.
9
0
1
2
:
v
i
X
r
a

On Solving a Stochastic Shortest-Path Markov
Decision Process as Probabilistic Inference

Mohamed Baioumy, Bruno Lacerda, Paul Duckworth, and Nick Hawes

Oxford Robotics Institute, University of Oxford
{mohamed, bruno, pduckworth, nickh}@robots.ox.ac.uk

Abstract. Previous work on planning as active inference addresses ﬁnite
horizon problems and solutions valid for online planning. We propose
solving the general Stochastic Shortest-Path Markov Decision Process
(SSP MDP) as probabilistic inference. Furthermore, we discuss online
and oﬄine methods for planning under uncertainty. In an SSP MDP,
the horizon is indeﬁnite and unknown a priori. SSP MDPs generalize
ﬁnite and inﬁnite horizon MDPs and are widely used in the artiﬁcial
intelligence community. Additionally, we highlight some of the diﬀerences
between solving an MDP using dynamic programming approaches widely
used in the artiﬁcial intelligence community and approaches used in the
active inference community. F

1

Introduction

A core problem in the ﬁeld of artiﬁcial intelligence (AI) is building agents capable
of automated planning under uncertainty. Problems involving planning under
uncertainty are typically formulated as an instance of a Markov Decision Process
(MDP). At a high level, an MDP comprises 1) a set of world states, 2) a set
of actions, 3) a transition model describing the probability of transitioning to
a new state when taking an action in the current state, and 4) an objective
function (e.g. minimizing costs over a sequence of time steps). An MDP solution
determines the agent’s actions at each decision point. An optimal MDP solution
is one that optimizes the objective function. These are typically obtained using
dynamic programming algorithms 1 [17,26].

Recent work based on the active inference framework [13] poses the planning
problem as a probabilistic inference problem. Several papers have been pub-
lished showing connections between active inference and dynamic programming
to solve an MDP [15,8,7]. However, the planning problem being solved in the
two communities is not equivalent.

First, dynamic programming approaches used to solve an MDP, such as policy
iteration, are valid for ﬁnite, inﬁnite and indeﬁnite horizons. Indeﬁnite horizons
are ﬁnite but of which the length is unknown a priori. For instance, consider

1 Linear programming approaches are also popular methods for solving MDPs
[2,12,22,9]. Additionally, other methods exist in the reinforcement learning com-
munity such as policy gradient methods [28,14,27].

 
 
 
 
 
 
2

M. Baioumy et al.

an agent navigating from a starting state to a goal state in a grid world where
the outcome is uncertain (e.g. the 4 × 4 grid world in Figure 1 shown in the
appendix). Before starting to act in the environment, there is no way for the
agent to know how many time steps it will take to reach the goal. Algorithms
based on dynamic programming, such as policy iteration, are valid for such
settings. They can solve the Stochastic Shortest-Path Markov decision process
(SSP MDP)[17,2]. However, work from active inference is only formulated for
ﬁnite horizons [8].

Second, the optimal solution to an SSP MDP is a stationary deterministic
policy [17]. This refers to a mapping from states to actions independent of time.
Computing this optimal policy can be done oﬄine (without interaction with
the environment) or online (while interacting). In the active inference literature
however, solving the planning problem is performed by computing a stochastic
plan (a sequence of actions given the current state). This is only valid during
online planning. Additionally, the solution is only optimal given a certain hori-
zon, which is speciﬁed a priori. If the horizon chosen is too short, the agent will
not ﬁnd a solution to reach the target. If it is too long, the solution will be
sub-optimal.

The main contribution of this paper is presenting a novel algorithm to solve a
Stochastic Shortest-Path Markov Decision Process using probabilistic inference.
This is an MDP with an indeﬁnite horizon. Additionally, highlighting the several
gaps between solving an MDP in the AI community and the active inference
community.

Section 2 discusses the SSP MDP and section 3 presets an approach for
solving an SSP MDP as probabilistic inference. The equivalence between the
two methods is shown in Section 4.1. Furthermore, the diﬀerence between world
states and temporal state is highlighted in Section 4.2. Policies, plans and prob-
abilistic plans are discussed in Section 4.3. Finally, a discussion on online vs
oﬄine planning can be found in Section 5.

2 Stochastic Shortest Path MDP

An SSP MDP is deﬁned as a tuple M = (S, A, C, T, G). S is the set of states, A is
the set of actions, C : S×A×S → R is the cost function, and T : S×A×S → [0, 1]
is the transition function. G ⊂ S is the set of goal states. Each goal state sg ∈ G
is absorbing and incurs zero cost. The expected cost of applying action a in state
s is ¯C(s, a) = (cid:80)
s(cid:48)∈S T (s, a, s(cid:48)) · C (s, a, s(cid:48)) . The minimum expected cost at state
s is ¯C ∗(s) = mina∈A ¯C(s, a). A policy maps a state to a distribution over action
choices. A policy is deterministic if it chooses a single action at each step. A
policy π is proper if it reaches sg ∈ G starting from any s with probability 1. In
an SSP MDP, the following assumptions are made [17]: a) there exists a proper
policy, and b) every improper policy incurs inﬁnite cost at all states where it is
improper.

The goal is to ﬁnd the an optimal policy π∗ with the minimum expected cost

¯C ∗(s) and can be computed as

SSP MDP as Probabilistic Inference

3

π∗(s) = argmina∈A

(cid:34)

(cid:88)

(cid:35)
T (s, a, s(cid:48)) [C (s, a, s(cid:48)) + V ∗ (s(cid:48))]

.

s(cid:48)∈S

V ∗(s) is referred to as the optimal value for a state s and is deﬁned as:

V ∗(s) = min
a∈A

(cid:34)

(cid:88)

(cid:35)
T (s, a, s(cid:48)) [C (s, a, s(cid:48)) + V ∗ (s(cid:48))]

.

s(cid:48)∈S

Crucially, the optimal policy π∗ corresponding to the optimal value func-
tion is Markovian (only dependant on the current state) and deterministic [17].
Solving an SSP MDP means ﬁnding a policy that minimizes expected cost, as
opposed to one that maximizes reward. This diﬀerence is purely semantic as the
problems are dual. We can deﬁne a reward function R = −C and move to a re-
ward maximization formulation. A more fundamental distinction is the presence
of a special set of (terminal) goal states, in which staying forever incurs no cost.
Solving an SSP MDP can be done using standard dynamic programming
algorithms such as policy iteration. Policy iteration can be divided in two steps,
policy evaluation and improvement. In policy evaluation, for a policy π, the value
function Vπ(s) is recursively evaluated until convergence as

Vπ(s) ←

(cid:88)

s(cid:48)∈S

T (s, π(s), s(cid:48)) [C (s, π(s), s(cid:48)) + Vπ (s(cid:48))] .

In the policy improvement step, the state-action value function Q(s, a) is

computed as:

Q(s, a) =

(cid:88)

s(cid:48)∈S

T (s, a, s(cid:48))[C(s, a, s(cid:48)) + V (s(cid:48))].

Then we compute a new policy as π(cid:48) = argmina∈A Q(s, a) for every state
in S. Iterating between these two steps guarantees convergence to an optimal
policy.

Properties of an SSP MDP. An SSP MDP can be shown to generalize ﬁnite,
inﬁnite and indeﬁnite horizon MDPs [3,17]. Thus algorithms valid for an SSP
MDP are also valid for the ﬁnite and inﬁnite horizon MDPs. Additionally, it can
be proven that each SSP MDP has an optimal deterministic policy independent
of time. Therefore, the claims made in [8] about active inference being more
general since it computes stochastic policies are unjustiﬁed when solving an
MDP. However, these results do not hold in partially observable cases or in
the presence of uncertain models. But in the SSP MDP deﬁned above (which
is commonly used in the AI community), there always exists a deterministic
optimal policy. Note that there is an inﬁnite number of stochastic policies but
only a ﬁnite number of deterministic policies (|S||A|). This greatly speeds up the
algorithms while still guaranteeing optimality.

4

M. Baioumy et al.

3 Solving an SSP MDP as probabilistic inference

In this section we discuss a novel approach for solving an SSP MDP as prob-
abilistic inference. We use an inference algorithm that exactly solves an SSP
MDP as deﬁned in the previous section. This approach is inspired by work from
[32,31] which solves an MDP with an indeﬁnite horizon. This approach has been
successfully applied to solve problems of planning under uncertainty, e.g. [18,30].

3.1 Deﬁnitions

The deﬁnition of an SSP MDP includes a set of world states S and actions A. In
probabilistic inference we instead reason about temporal states and actions. A
temporal state st is a random variable deﬁned over all world states. Conceptually
it represents the state that the agent will visit at the time-step t. For the grid
world in Figure 1, there are 16 world states but the number of temporal states
is unknown a priori since the horizon is unknown.

The transition probability is deﬁned as a probability distribution over tem-
poral states and actions as P (st+1|at, st). If the random variables are ﬁxed
to speciﬁc world states i and j and an action a, the transition probability
P (st+1 = j|at = a, st = i) would be equivalent to the transition function T
deﬁned for an SSP MDP. The probability of taking a certain action in a state is
parameterized by a policy π as P (at = a|st = i; π) = πai. This policy is deﬁned
exactly the same as in the case of an SSP MDP.

The cost function P (ct|st, at) is deﬁned diﬀerently. The temporal cost vari-
ables ct are deﬁned as binary random variables ct ∈ {0, 1}. Translating an arbi-
trary cost function to temporal costs can be done by scaling the cost function
C(s, a) (as deﬁned in the previous section) between the minimum cost (min(C))
and maximum cost (max(C)) as:

P (ct = 1 | at = a, st = s) =

C(a, s) − min(C)
max(C) − min(C)

.

Any expression with P (ct = 1) can be though of as ‘the probability of a cost
being maximal’. Thus, the probability of a cost being maximal given a state and
an action is P (ct = 1 | at = a, st = s). Now we can reason about the highest
possible cost for a state s and action a as one where P (ct = 1 | at = a, st = s) = 1
and the lowest possible cost as P (ct = 1 | at = a, st = s) = 0. Any other cost
will have a probability in-between, according to its magnitude.

Finally, we model the horizon as a random variable. The temporal states and
actions are considered up to the end of the horizon T . However, the horizon is
generally unknown. We thus model T itself as a random variable. Combining all
this information we can deﬁne the SSP MDP using a probabilistic model.

3.2 Mixture of ﬁnite MDPs

In this section we deﬁne the SSP MDP in terms of a mixture of ﬁnite MDPs
with only a ﬁnal cost variable. Given every horizon (for instance T = 1) the

SSP MDP as Probabilistic Inference

5

ﬁnite MDP can be given as P (c, s0:T , a0:T | T ; π). Note that we dropped the
time-index for ct since there is only one cost variable now. This model can be
factorized as

P (c, s0:T , a0:T | T ; π) = P (c | aT , sT ) P (a0 | s0; π)
t=1 P (at | st; π) P (st | at−1, st−1)

P (s0) · (cid:81)T

To reason about the full MDP, we consider the mixture model of the joint

given by the joint probability distribution

P (c, s0:T , a0:T , T ; π) = P (c, s0:T , a0:T | T ; π) P (T )

where P (T ) is a prior over the total time, which we choose to be a ﬂat prior
(uniform distribution).

3.3 Computing an optimal policy

Our objective is to ﬁnd a policy that minimizes the expected cost. Similarly
to policy iteration, we do not assume any knowledge about the initial state.
Expectation-Maximization2 can be used to ﬁnd the optimal parameters of our
model: the policy π. The E-step will, for a given π, compute a posterior over state-
action sequences. The M-step then adapts the model parameters π to optimize
the expected likelihood with respect to the quantities calculated in the E-step.

E-step: a backwards pass in all ﬁnite MDPs. We use the simpler notation
p(j | a, i) ≡ P (st+1 = j | at = a, st = i) and p(j | i; π) ≡ P (st+1 = j | st = i; π) =
(cid:80)

a p(j | a, i)πai. Further, as a ‘base’ for backward propagation, we deﬁne

β0(i) = P (c = 1 | xT = i; π) =

(cid:88)

a

P (c = 1 | aT = a, xT = i) πai.

This is the immediate cost when following a policy π. It is the expected cost if
there is only one time-step remaining. Then, we can recursively compute all the
other backward messages. We use the index τ to indicate a backwards counter.
This means that τ + t = T , where T is total (unknown) horizon length. This is
computed as

βτ (i) = P (c = 1 | xT −τ = i; π) =

(cid:88)

j

p(j | i; π)βτ −1(j).

Intuitively, the backward messages are the expected cost if one incurs a cost at
the last time step only. So, β2, is the expected cost if the agent follows the policy
π for two time-steps and only incurs a cost after that. Using these messages, we
can compute a value function dependent on time, actions and states given as:

2 An expectation-maximization algorithm can be viewed as performing free-energy
minimization [21,16]. In the E-step, the free-energy is computed and the M-step
updates the parameters to minimize the free-energy.

6

M. Baioumy et al.

qτ (a, i) = P (c = 1 | at = a, st = i, T = t + τ ; π)
j p(j | i, a)βτ −1(j)

τ > 1
P (c = 1 | aT = a, sT = i) τ = 0.

(cid:26) (cid:80)

=

Marginalizing out time, we get the state-action value-function

P (c = 1 | at = a, st = i; π) =

1
C

(cid:88)

τ

P (T = t + τ )qτ (a, i)

where C is a normalization constant. This quantity is the probability of getting
a maximum cost given a state and action. It is similar to the Q(s, a) function
computed in policy iteration.

M-step: the policy improvement step. The standard M-step in an EM-
algorithm maximizes the expected complete log-likelihood with respect to the
new parameters π(cid:48). Given that the optimal policy for an MDP is deterministic,
a greedy M-step can be used. However, our goal is to minimize the log-likelihood
in this case as it refers to a the probability of receiving a maximal cost. This can
done as

π(cid:48) = argmin

(P (c = 1 | at = a, st = i; π))

a

(1)

This update converges much faster than in a standard M-step. Here an M-
step can be used to to obtain a stochastic policy. However, this is unnecessary
since the optimal policy is deterministic. Note that there is an inﬁnite number
of stochastic policies but a ﬁnite number of deterministic ones. In conclusion, a
greedy M-step is faster to converge but still guarantees an optimal policy.

4 Connections between the two views

4.1 Exact relationship between policy iteration and planning as

probabilistic inference

The messages β computed during backward propagation are exactly equal to the
value functions for a single MDP of ﬁnite time. The full value function is can
therefore be written as the sum of the βs,

Vπ(i) =

(cid:88)

T

βT (i)

since the prior over time P (T ) is a uniform prior. If P (T ) is not a uniform
distribution, this would result in a mixture rather than a sum. The same applies
to the relationship between the Q-value function:

Qπ(a, i) =

(cid:88)

T

qT (a, i).

SSP MDP as Probabilistic Inference

7

Hence, the E-step essentially performs a policy evaluation which yields the
classical value function. Given this relation to policy evaluation, the M-step
performs an operation exactly equivalent to standard policy improvement. Thus,
the EM-algorithm using exact inference is equivalent to Policy Iteration but
computes the necessary quantities diﬀerently.

One unanswered question is when to stop computing the backward messages.
In [32] messages are computed up to a number Tmax. From this perspective, the
planning as inference algorithm presented is equivalent to the so-called truncated
policy iteration algorithm as opposed to the more common (cid:15)-greedy version.

In the policy evaluation step, one iterates through the state space to update
the value vπ(s) for every state until a termination criterion is met. An (cid:15)-greedy
criterion means that we stop iterating though the state space once the maxi-
mum diﬀerence in Vπ(s) for any s is smaller than a positive small number (cid:15).
In truncated policy iteration, however, we iterate through the state space Tmax
times and then perform the policy improvement step. The probabilistic inference
algorithm presented in this paper is equivalent to truncated policy iteration if
we restrict the maximum number of β messages to be computed.

4.2 World states vs temporal states

In dynamic programming, one reasons over the world states. In the grid world
example in Figure 1, this refers to a grid cell. This grid world has 16 world states.
In probabilistic inference, one reasons about a temporal state. This is a random
variable over all world states. The number of temporal states is dependent on
how many time-steps the agents acts in the environment (which is often unknown
beforehand). An illustration of the diﬀerence is given in Fig. 2.

4.3 Policies, plans and probabilistic plans

A classical planning algorithm computes a plan: a sequence of actions. An algo-
rithm like A* or Dijkstra’s algorithm [5] can be used to ﬁnd the optimal path
from a stating state to a goal state, given a deterministic world. Crucially, this
solution can be computed oﬄine (without interactions with the environment). In
a stochastic world, this does not work since the agent can not predict in which
states it will end up. However, one can use deterministic planning algorithms
for stochastic environments if the path is re-planned online at every time-step.
Determinization-based methods have found success in solving planning under
uncertainity problems such as the famous FF-replan algorithm [34]

Active inference approaches computes a probabilistic plan. The active infer-
ence literature calls this a policy; however, we use a diﬀerent term to avoid
confusion.3 In active inference, the agents computes a ﬁnite plan while interact-
ing with the environment. However, rather than assuming a deterministic world

3 The distinction between a plan and a policy when using active inference has been
brieﬂy discussed in [20]. Additionally, other methods computing plans as probabilis-
tic inference have been proposed before active inference in [1,33]

8

M. Baioumy et al.

(like FF-replan [34]), the probabilities are taken into account. This can be shown
to compute the optimal solution to an MDP (when planning online). We thus
refer to it as a probabilistic plan, a plan that was computed while taking the
transition probabilities into account.

Finally, a policy is a mapping from states to actions, i.e. the agent has a pre-
ferred action to take for every state. Policies can be stochastic or time-dependent;
however, for an SSP MDP the optimal policy is deterministic and independent
of time. An agent can compute a policy oﬄine and use it online without need-
ing any additional computation while interacting with the environment. The
diﬀerence between a plan and policy is illustrated in Fig. 1.

To summarize, a plan or a probabilistic plan can only be used for online
planning. Since the outcome of an action is inherently uncertain. Probabilistic
plans (as used in active inference) ﬁnd an optimal solution when used to plan
online. A policy also provides an optimal solution and can be computed oﬄine
or online.

5 Discussion

In this paper we present a novel approach to solve a stochastic shortest path
Markov decision process (SSP MDP) as probabilistic inference. The SSP MDP
generalizes many models, including ﬁnite and inﬁnite MDPs. Crucially, the dy-
namic programming algorithms (such as policy iteration) classically used to solve
an SSP MDP are valid for indeﬁnite horizons (ﬁnite but of unknown length);
this is not the case for active inference approaches.

The exact connections between solving an MDP using policy iteration and
the presented algorithm are discussed. Afterwards, we discussed the gap be-
tween solving an MDP in active inference and the approaches in the artiﬁcial
intelligence community. This included the diﬀerence between world states and
temporal states, the diﬀerence between plans, probabilistic plans and policies.
An interesting question now is, which approach is more appropriate? This de-
pends on the problem at hand and whether it can be solved online or oﬄine.

Online and oﬄine planning. As discussed in Section 4.3, a policy is mapping
from states to actions and can be used for oﬄine and online planning. Computing
a policy is somewhat computationally expensive; however, a look-up is very
cheap. Thus if one operates in an environment where the transition and cost
function do not change, it is best to compute an optimal policy oﬄine then
use it online (while interacting with the environment). This is the case for many
planning and scheduling problems, such as a set of elevators operating in sync [6],
task-level planning in robotics [19], multi-objective planning [23,11] and playing
games [4,25]. The challenges in these problems are often that the state-space is
incredibly large and thus approximations are needed. However, the problem is
fully observable and the cost and transition models are static; the rules of chess
do not change half way, for instance.

SSP MDP as Probabilistic Inference

9

If the transition or cost functions vary while interacting with the environment
(e.g. [24,10]), an oﬄine solution is not optimal. In this case, the agent can plan
online by re-evaluating a policy or computing probabilistic plans (as done in
active inference). Computing the latter is cheaper and requires less memory.
This is because a probabilistic plan is a distribution over actions p(at) up to a
time horizon T while a (ﬁnite policy) is a conditional distribution p(at|st) over
all world states in S. For any time-step, the posterior over the action is related
to the policy such that p(at) = (cid:80)

s p(at|st)p(st).
Consider the work in [29,24]. In both cases a robot operates in an environment
susceptible to changes. If the environment changes, the agent can easily construct
a new model by varying the cost or transition function but needs to recompute a
solution. In [29] the authors recompute a policy at every time-step while in [24]
a probabilistic plans is computed using active inference. Since in both cases the
solution is recomputed at every time-step, active inference is preferred since it
requires less memory and can be computationally cheaper. On the other hand,
if the environment only changes occasionally, computing a policy might remain
preferable.

To conclude, if the transition and cost functions (T and C) are static, it is
preferable to compute a policy oﬄine. If T and C change occasionally, one may
still compute an oﬄine policy and recompute a policy only when a change occurs.
However, if the environment is dynamic, computing a probabilistic plan (using
active inference) is preferable to recomputing a policy at every time-step.

References

1. Attias, H.: Planning by probabilistic inference. In: AISTATS (2003)
2. Bertsekas, D.P., Tsitsiklis, J.N.: An analysis of stochastic shortest path problems.

Mathematics of Operations Research 16(3), 580–595 (1991)

3. Bertsekas, D.P., Tsitsiklis, J.N.: Neuro-dynamic programming: an overview. In:
Proceedings of 1995 34th IEEE conference on decision and control. vol. 1, pp.
560–564. IEEE (1995)

4. Campbell, M., Hoane, A.J., Hsu, F.: Deep blue. Artif. Intell. 134, 57–83 (2002)
5. Cormen, T.H., Leiserson, C.E., Rivest, R.L., Stein, C.: Introduction to algorithms.

MIT press (2009)

6. Crites, R.H., Barto, A.G., et al.: Improving elevator performance using reinforce-
ment learning. Advances in neural information processing systems pp. 1017–1023
(1996)

7. Da Costa, L., Parr, T., Sajid, N., Veselic, S., Neacsu, V., Friston, K.: Active infer-
ence on discrete state-spaces: a synthesis. arXiv preprint arXiv:2001.07203 (2020)
8. Da Costa, L., Sajid, N., Parr, T., Friston, K., Smith, R.: The relationship between
dynamic programming and active inference: The discrete, ﬁnite-horizon case. arXiv
preprint arXiv:2009.08111 (2020)

9. d’Epenoux, F.: A probabilistic production and inventory problem. Management

Science 10(1), 98–108 (1963)

10. Duckworth, P., Lacerda, B., Hawes, N.: Time-bounded mission planning in time-

varying domains with semi-mdps and gaussian processes (2021)

10

M. Baioumy et al.

11. Etessami, K., Kwiatkowska, M., Vardi, M.Y., Yannakakis, M.: Multi-objective
model checking of markov decision processes. In: International Conference on Tools
and Algorithms for the Construction and Analysis of Systems. pp. 50–65. Springer
(2007)

12. Forejt, V., Kwiatkowska, M., Norman, G., Parker, D.: Automated veriﬁcation tech-
niques for probabilistic systems. In: International school on formal methods for the
design of computer, communication and software systems. pp. 53–113. Springer
(2011)

13. Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., Pezzulo, G.: Active in-

ference: a process theory. Neural computation 29(1), 1–49 (2017)

14. Grondman, I., Busoniu, L., Lopes, G.A., Babuska, R.: A survey of actor-critic
reinforcement learning: Standard and natural policy gradients. IEEE Transactions
on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 42(6),
1291–1307 (2012)

15. Kaplan, R., Friston, K.J.: Planning and navigation as active inference. Biological

cybernetics 112(4), 323–343 (2018)

16. Koller, D., Friedman, N.: Probabilistic graphical models: principles and techniques.

MIT press (2009)

17. Kolobov, A.: Planning with Markov Decision Processes: An AI Perspective, vol. 6.

Morgan & Claypool Publishers (2012)

18. Kumar, A., Zilberstein, S., Toussaint, M.: Probabilistic inference techniques for
scalable multiagent decision making. Journal of Artiﬁcial Intelligence Research 53,
223–270 (2015)

19. Lacerda, B., Faruq, F., Parker, D., Hawes, N.: Probabilistic planning with formal
performance guarantees for mobile service robots. The International Journal of
Robotics Research 38(9), 1098–1123 (2019)

20. Millidge, B., Tschantz, A., Seth, A.K., Buckley, C.L.: On the relationship between
active inference and control as inference. In: International Workshop on Active
Inference. pp. 3–11. Springer (2020)

21. Murphy, K.P.: Machine learning: a probabilistic perspective. MIT press (2012)
22. Nazareth, J.L., Kulkarni, R.B.: Linear programming formulations of markov deci-

sion processes. Operations research letters 5(1), 13–16 (1986)

23. Painter, M., Lacerda, B., Hawes, N.: Convex hull monte-carlo tree-search. In: Pro-
ceedings of the International Conference on Automated Planning and Scheduling.
vol. 30, pp. 217–225 (2020)

24. Pezzato, C., Hernandez, C., Wisse, M.: Active inference and behavior trees for
reactive action planning and execution in robotics. arXiv preprint arXiv:2011.09756
(2020)

25. Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanc-
tot, M., Sifre, L., Kumaran, D., Graepel, T., et al.: Mastering chess and shogi
by self-play with a general reinforcement learning algorithm. arXiv preprint
arXiv:1712.01815 (2017)

26. Sutton, R.S., Barto, A.G., et al.: Introduction to reinforcement learning, vol. 135.

MIT press Cambridge (1998)

27. Sutton, R.S., McAllester, D.A., Singh, S.P., Mansour, Y.: Policy gradient methods
for reinforcement learning with function approximation. In: Advances in neural
information processing systems. pp. 1057–1063 (2000)

28. Thomas, P.S., Brunskill, E.: Policy gradient methods for reinforcement learn-
ing with function approximation and action-dependent baselines. arXiv preprint
arXiv:1706.06643 (2017)

SSP MDP as Probabilistic Inference

11

29. Tomy, M., Lacerda, B., Hawes, N., Wyatt, J.L.: Battery charge scheduling in long-
life autonomous mobile robots via multi-objective decision making under uncer-
tainty. Robotics and Autonomous Systems 133, 103629 (2020)

30. Toussaint, M., Charlin, L., Poupart, P.: Hierarchical pomdp controller optimization

by likelihood maximization. In: UAI. vol. 24, pp. 562–570 (2008)

31. Toussaint, M., Harmeling, S., Storkey, A.: Probabilistic inference for solving (po)
mdps. University of Edinburgh, School of Informatics Research Report EDI-INF-
RR-0934 (2006)

32. Toussaint, M., Storkey, A.: Probabilistic inference for solving discrete and contin-
uous state markov decision processes. In: Proceedings of the 23rd international
conference on Machine learning. pp. 945–952. ACM (2006)

33. Verma, D., Rao, R.P.: Goal-based imitation as probabilistic inference over graphical
models. In: Advances in neural information processing systems. pp. 1393–1400
(2006)

34. Yoon, S.W., Fern, A., Givan, R.: Ff-replan: A baseline for probabilistic planning.

In: ICAPS. vol. 7, pp. 352–359 (2007)

A Appendix: Illustrations

Fig. 1. An illustration of a 4 × 4 grid world (right). The initial state is blue and goal
state is green. An illustration for a policy (left) and a plan (middle).

Fig. 2. Annotated world states (left) and a posterior over a temporal state (right).

