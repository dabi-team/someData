9
1
0
2

t
c
O
3

]
h
p
-
t
n
a
u
q
[

3
v
4
6
2
5
0
.
5
0
9
1
:
v
i
X
r
a

Programming multi-level quantum gates in disordered computing reservoirs via
machine learning and TensorFlow

Giulia Marcucci
Department of Physics, University Sapienza, Piazzale Aldo Moro 2, 00185 Rome (IT) and
Institute for Complex Systems, National Research Council (ISC-CNR), Via dei Taurini 19, 00185 Rome (IT)

Davide Pierangeli
Department of Physics, University Sapienza, Piazzale Aldo Moro 2, 00185 Rome (IT) and
Institute for Complex Systems, National Research Council (ISC-CNR), Via dei Taurini 19, 00185 Rome (IT)

Pepijn Pinkse
Complex Photonic Systems (COPS), MESA+ Institute for Nanotechnology,
University of Twente, P.O. Box 217, 7500 AE Enschede, The Netherlands

Mehul Malik
Institute of Photonics and Quantum Sciences (IPAQS),
Heriot-Watt University, Edinburgh, EH144AS UK

Claudio Conti
Department of Physics, University Sapienza, Piazzale Aldo Moro 2, 00185 Rome (IT) and
∗
Institute for Complex Systems, National Research Council (ISC-CNR), Via dei Taurini 19, 00185 Rome (IT)
(Dated: October 4, 2019)

Novel machine learning computational tools open new perspectives for quantum information sys-
tems. Here we adopt the open-source programming library TensorFlow to design multi-level quan-
tum gates including a computing reservoir represented by a random unitary matrix. In optics, the
reservoir is a disordered medium or a multi-modal ﬁber. We show that trainable operators at the
input and the readout enable one to realize multi-level gates. We study various qudit gates, includ-
ing the scaling properties of the algorithms with the size of the reservoir. Despite an initial low
slop learning stage, TensorFlow turns out to be an extremely versatile resource for designing gates
with complex media, including diﬀerent models that use spatial light modulators with quantized
modulation levels.

I.

INTRODUCTION

The development of multi-level quantum information
processing systems has steadily grown over the past few
years, with experimental realizations of multi-level, or
qudit logic gates for several widely used photonic degrees
of freedom, such as orbital-angular-momentum and path
encoding [1–4]. However, eﬀorts are still needed for in-
creasing the complexity of such systems while still be-
ing practical, with the ultimate goal of realizing complex
large-scale computing devices that operate in a techno-
logically eﬃcient manner.

A key challenge is the development of design techniques
that are scalable and versatile. Recent work outlined
the relevance of a large class of devices, commonly de-
noted as “complex” or “multi-mode” [5, 6]. In these sys-
tems, many modes or channels are mixed and controlled
at input and readout to realize a target input-output
operation. This follows the ﬁrst experimental demon-
strations of assisted light transmission through random
media [7–10], which demonstrated many applications in-

∗ claudio.conti@uniroma1.it

cluding arbitrary linear gates [5], mode conversion, and
sorting [11, 12].

The use of complex mode-mixing devices is surpris-
ingly connected to leading paradigms in modern machine
learning (ML), as the “reservoir computing” (RC) [13],
and the “extreme learning machine” (ELM) [13, 14]. In
standard ML, one trains the parameters (weights) of an
artiﬁcial neural network (ANN) to ﬁt a given function,
In RC, due to the in-
which links input and output.
creasing computational eﬀort to train a large number of
weights, one internal part of the network is left untrained
(“the reservoir”) and the weights are optimized only at
input and readout.

ML concepts, such as photonic neuromorphic and
reservoir computing [15, 16], are ﬁnding many applica-
tions in telecommunications [17, 18], multiple scattering
[19], image classiﬁcation [20], metasurfaces [21, 22], bio-
photonics [10], Ising machines [23], integrated and ﬁber
optics [24, 25], and topological photonics [26]. Various
authors have reported the use of ML for augmenting and
assisting quantum experiments [27–31]. The ﬁeld of ma-
chine learning is in turn inﬂuenced by quantum physics,
for example in the orthogonal units [32, 33].

Here we adopt RC-ML to design complex multi-level
gates [2, 3, 34, 35], which form a building block for

 
 
 
 
 
 
high-dimensional quantum information processing sys-
tems. While low-dimensional examples of such gates have
been implemented using bulk and integrated optics, ef-
ﬁciently scaling them up to high dimensions remains a
challenge.

In quantum key distribution (QKD), one uses at least
two orthogonal bases to encode information. High-
dimensional QKD oﬀers an increased information capac-
ity as well as an increased robustness to noise over qubit-
based protocols [36, 37]. Such protocols may be realized
by using the photonic spatial degrees of freedom as the
encoding (computational) basis, and suitable unitary op-
erators to switch between bases mutually unbiased with
respect to the computational basis. However, the security
of the QKD protocol may be compromised by the ﬁdelity
of such basis transformations, leading to errors in the
key rate. An additional consideration is the experimen-
tal complexity of such transformations, which can scale
rather poorly using established techniques based on bulk
optical systems. By using a random medium and I/O
readout operators, one can realize such high-dimensional
operations in a controllable and scalable manner, relying
only on the existing complexity of the disordered medium
and a control operation at the input. Here, we explore
methodologies to train a disordered medium to function
as a multi-level logic gate by using diﬀerent implementa-
tions of ML concepts.

Figure 1 shows the schematic of a device including the
complex medium, represented by the unitary operator ˆU ,
and two trainable input ˆSin and readout ˆSout operators.
|h(1,2)i are hidden states. The use of an optical gate in
this manner is related to the use of a disordered medium
as a physically unclonable function (PUF) [38–40].

In our general framework, we have a random system
modeled by a unitary random matrix. We want to use the
random medium to perform a computation in a Hilbert
space containing many qudits. The random medium is
not necessarily a disordered system (for example, a di-
electric assembly of scattering particles), but may also
be a multimode ﬁber, or an array of waveguides. The
input/output relation is represented by a linear unitary
matrix operator UM and only forward modes are consid-
ered. The UM matrix has dimensions M × M , with M
the dimension of the embedding space.

The “reduced” state vector at input has dimensions
N × 1, with N ≤ M . This models the case in which
we use a subset of all the available modes. The input to
the reservoir is a “rigged” state vector x with dimension
M , where the missing complementing C components are
replaced by C = M − N ancillas. Our goal is to use the
random medium to perform a given operation denoted
by a gate unitary matrix

TM = Sout

M · UM · Sin
M .

(1)

Sin
M and Sout
M are two “training” operators that are ap-
plied at input and output (see Fig. 1) and whose elements
can be adjusted. We ﬁrst consider the presence of the in-
put operator Sin
M = 1M , which can be

M = SM , and Sout

2

implemented by spatial-light modulators (we denote as
1M the identity matrix with dimension M ).

We identify two cases: either (i) we know the matrix
UM , or (ii) we have to infer UM from the input/output
relation. We show in the following the way these
two problems can be solved by ANNs, where we denote
the two families as non-inferencing and inferencing gates.

II. NON-INFERENCING GATES

We consider a target gate with complex-valued input
state with dimension N , and components x1, x2, ..., xN .
We embed the input vector in a rigged Hilbert space
with dimension M ≥ N , so that the overall input vec-
tor is x = {x1, x2, ..., xN , xN +1, ..., xM }. We have a lin-
ear propagation through a medium with unitary com-
plex transfer matrix UM . The overall transmission ma-
trix is TM = UM · SM , such that the output vector is
y = TM · x = UM · SM · x. The observed output vector
is written as P · y, where P is a N −projector operator
with dimensions N × M such that P = [1N |0], with 1N
the identity matrix with size N × N , and 0 a null matrix
with dimension N × C. The goal is ﬁnding the matrix
SM such that

P · UM · SM = [XN 0]

(2)

where XN is the N × N target gate and 0 is the null
complement N × C at dimension M . Eq. (2) is a ma-
trix equation, which guarantees that the overall system
behaves as a XN gate on the reduced input.

Solving the matrix Eq. (2) may be demanding and non-
trivial when the number of dimensions grows. In the fol-
lowing, we discuss the use of ML techniques.

The transmission matrix TM in the rigged space from

x to y can be written as blocks

TM =

XN
0
(cid:20)

0
OC (cid:21)

(3)

where OC is a unitary matrix with dimensions C × C to
be determined. If UM and SM are unitary, the resulting
transmission matrix TM is also unitary. However, if one
uses Eq. (2), the problem may also have a nonunitary
solution (“projected case”) as some channels are dropped
at the output.
In other words, solving Eq. (3) is not
equivalent to solving Eq. (2), and we adopt two diﬀerent
methodologies: one can look for unitary or nonunitary
solutions by ANN.

By following previous work developed for real-valued
matrices [41], we map the complex-valued matrix equa-
tion (2) into a recurrent neural network (RNN). In the
“non-inferencing” case, the matrix UM is known, and the
solution is found by the RNN in Fig. 2. The RNN solves
an unconstrained optimization problem, by ﬁnding the
minimum of the sum of the elements eij > 0 of an er-
ror matrix E. The error depends on a “state matrix”

Input layer

Random or multimodal system

Readout

3

 ˆU

  x

inˆS

1h

2h

  out
ˆS

  y

Figure 1. A general optical gate based on a complex random medium; the input state x is processed to the input layer with
operator ˆSin, the system is modeled by the unitary operator ˆU , and the output is further elaborated by ˆSout.

WM , and one trains the elements wij of WM to ﬁnd the
minimum

min
WM

E[G(WM )] = min

WM Xi,j

eij[G(WM )].

(4)

In the adopted approach, the sum of the elements eij is
minimal when the hidden layer elements gij of the matrix
G(W ) are zero. E and G have to be suitably chosen to
solve the considered problem. We found two possible G
matrices: (i) the “projected”

GP = P · UM · WM − XN 0,

(5)

with XN 0 = [XN 0] as in Eq. (2) and, (ii) the “unitary”
[see Eq. (3)]

GU = UM · WM − TM .

(6)

These two cases are discussed below.

To ﬁnd the unknown training matrix SM , one starts
from an initial guess matrix WM (0). The guess is then
recurrently updated, as in Fig. 2, until a stationary state
WM (∞) is reached. Once this optimization converges,
the solution is given by SM = WM (∞). The update
equation is determined by a proper choice of the error
matrix E as follows.

As the matrices are complex valued, eij is a function of
gij and g∗
ij. We set eij = eij(|gij |2). The corresponding
dynamic RNN equation, which for large time gives the
solution to the optimization problem, is

dWM
dt

= −µU †

M · F [G(WM )]

(7)

where µ is the “learning rate”, an optimization coeﬃ-
cient (hyperparameter), which is set to speed-up the con-
vergence. The elements fij of the matrix F are fij = deij
.
dg∗
ij
Letting eij = |gij|2, one has fij = gij.

Eq. (7) implies that the RNN is composed of two bidi-
rectionally connected layers of neurons, the output layer
with state matrix W , and the hidden layer with state ma-
trix G. The training corresponds to sequential updates of
F and W when solving the ordinary equations [(7)]. As
shown in [41], this RNN is asymptotically stable and its
steady state matrix represents the solution (an example
of training dynamics is in Fig. 2b).

We code the RNN by TensorFlowTM and use the ordi-
nary diﬀerential equations (ODEs) integrator odeint. In
the case N = M , as XN = XM is a unitary operator, the
solution of the recurrent network furnishes a unitary SM
matrix, which solves the problem. For M > N the RNN
furnishes a unitary solution SM , and a unitary transfer
function TM , only if we embed the target gate XN in a
unitary operator as in (3) with OC a randomly generated
unitary matrix.

A. Single non-inferencing qutrit gate X

For the training of a gate X3 deﬁned by [2, 42]

X3 =

d−1

Xl=0

|l ⊕ 1ihl| = 



0 1 0
0 0 1
1 0 0





(8)

The gate X3 is obtained by an embedding dimension
M = 5 and unitary transfer function U5 as in Fig. 2.

For G = GP , the number of ODEs for the training of
the network is minimal (N = 3). However, the solution
is not unitary, as some channels are dropped out by the
N −projector. The overall M × M transmission matrix
TM , after the training, is such that T †
M · TM 6= I because
the solution SM is not unitary. However, the system
always reaches a stationary case.

 
 
 
4

(a)

1Mw

(b)

MMw

11w

M1w

recurrent
training

1Mg

status layer

ijw t

MMg

0.6

0.4

0.2

0.0

-0.2

-0.4

hidden layer

M1g

0.0

0.2

0.4

0.6

0.8

1.0

time t

11g

(c)

Target matrix 

X =

0 1 0

0 0 1

1 0 0

Transfer matrix of the random system 

i
0.5

0.4

0.6 0.2

i

0.4

i

U

=

0.2
i
0.4 0.3
i
0.2 0.4

i
0.5 0.3

i
0.5 0.3
0.4
i
0.3 0.1

i

i
0.1 0.1
i
0.4 0.3
i
0.4 0.1

i

0.4 0.4
i
0.3 0.5
i
0.1 0.3

0.5

0.3

0.3 0.2

i

0.3 0.2

i

PG

UG

Training

0.1 0.1

i

i
0.3 0.4
i
0.1 0.2
i
0.3 0.6

Transmission matrix after training

 X

T

=

0

1

0
1
i
0.1 0.3

0
0
0.3 0.4

i

0

1
0
0.3

0

0
0
0.3

0

0
0
0.7

i

i
0.4

i
0.5 0.1

0.3

i

0.5 0.2

i

0.4 0.2

i

non unitary solution

X

0 1 0

0 0 1

1 0 0
0 0 0
0 0 0

T

=

0

0

0

0

0
0.6 0.6
0.5 0.2

i
i

0
0.4 0.4
i
0.3 0.7
i

O

unitary solution

Figure 2. (a) Recurrent neural network for the matrix equation (7). The status nodes are denoted by the elements of the
matrix W , and the hidden state of the system is in the nodes of the matrix F ; (b) training dynamics for the case N = M = 3
with XT corresponding to a single-qutrit X-gate (µ = 100); (c) resulting transfer function for the case N = 3 and M = 5
in the unitary and non-unitary case. In the latter case, the excess channels are ignored during the training. The resulting
transmission channels TM are displayed, O2 is the unitary complements for C = M − N = 2 in the unitary case.

A unitary solution is found by letting G = GU and in-
volving the maximum number of ODEs in (7) with a uni-
tary embedding of XN as in (3), i.e., adopting a further -
randomly generated - unitary matrix OC . The key point
is that the system ﬁnds a solution for any random uni-
tary rigging of the matrix XN , that is, for any randomly
assigned matrix OC . This implies that we can train all
these systems to realize diﬀerent multi-level gates.

III.

INFERENCING GATES

use an ANN to determine the training operators with-
out measuring the transfer matrix. Figure 3 shows the
scheme of the ANN, where the unitary matrix UM is
represented by its elements uij, and the wij are the ad-
justable weights. After training, the resulting wij are the
elements of the solution matrix SM . For the sake of sim-
plicity, we consider Sout = 1M , as above. For a target
XN we build the TM as in (3) by randomly generating the
unitary complement OC . As TM and UM are unitary, the
resulting SM is also unitary. One can use a non unitary
TM by choosing, for example, OC = 0. Correspondingly
- after the training - SM is not unitary.

In the case that we do not know the transfer matrix
of the system, we can still train the overall transmission
matrix by using a neural network and infer UM . Here we

We randomly generate a set of input states xi, with
i = 1, ..., ntrain. Each input state is “labelled” with the
target output yi = TM · xi. We remark that xi and yi

 
 
 
 
 
 
 
 
 
 
 
 
 
 
are vector with size M . A further set of nvalid validation
rigged vectors is used to validate the training.

For any input xi in the training set, we adjust the

weights to minimize the error function

ei =

1
N

XN

|yi − UM · WM · xi|2

(9)

with yi = TM · xi. After this training, we test the ac-
curacy on the validation set. Each cycle of training and
validation is denoted as “epoch”.

Figure 3 shows the ANN for N = 3, and M = 5. In
our model, we build a matrix WM of unknown weights.
As we deal with complex quantities, WM is written as
WM = W ′
M real-valued matri-
ces, whose elements form the weights of the ANN. Using
random matrices as initial states, we end the iteration
when the validation cost is below a threshold εvalid.

M with W ′

M and W ′′

M + ıW ′′

A. Single-qutrit inference X-gate

Figure 3 shows the training of a single qutrit gate X3 in
(8). Similar results are obtained with other single qudit
gates as X 2 and Z and for higher dimensions. Training
typically needs tens of iterations and scales well with the
number of dimensions. Figure 3 shows an example with
N = 3 and M = 5. Figure 3c shows that the number of
training epochs nepochs scales linearly with the embed-
ding space dimension M .

IV. SPATIAL LIGHT MODULATOR
IMPLEMENTATION

In the general case, one needs a unitary gate to train
the complex medium, and a modulator to test diﬀerent
inputs signals. In practical and simpliﬁed implementa-
tions, the training gate and the input modulator can be
made with a single device. It is possible to realize the
ML design with a single spatial light modulator (SLM),
as sketched in the inset of Fig. 4a. Ref. [1] already gave
a recipe for implementing a unitary in a lossy way with a
single SLM and a complex medium. However, here we fol-
low the more recent but also lossy technique introduced
in Ref. [5]. We consider an input plane wave represented
by a constant vector eN = 1, 1, ..., 1 with dimension N ,
where N is the number of pixels in the amplitude and
phase SLM.

Assuming that we want to design a gate with input
x and output y, we generate the input x by an opera-
tor Diag(x), which has the ﬁrst N elements of x on the
diagonal.

Assuming that the ML algorithm has produced an op-
erator SM , the actual operator to be implemented on the
SLM is ˜SM = SM · Diag(x). Note that ˜SM encodes the
input and hence changes for diﬀerent inputs [5].

In other words, with a single SLM, after optimization
for a given output y, the training realizes ˜SM for a ﬁxed

plane wave input eM = 1, 1, ..., 1, 0, ...0 with N ones and
M − N zeros.

5

A. Phase-only modulators

A pure phase modulator is implemented by the ele-
ments writing the model matrix for ˜SM as cos(φij ) +
ı sin(φij ), with φij the phase of the i, j segment of the
SLM. In Fig. 4a, we show the performance of the train-
ing process, focusing on a single qutrit X−gate (N = 3),
and varying the size of the reservoir M . If the reservoir is
about one order of magnitude larger than the dimension
of the gate, the algorithm converges in less than 1000
epochs, and the error decreases with M .

B. Sign modulators and quantized amplitude

A pure amplitude modulator is modeled by a real ma-
trix ˜SM . A combination of an amplitude modulator, such
as a digital micromirror device (DMD), along with spatial
ﬁltering, enables one to realize positive and negative val-
ues for ˜SM [5]. The elements of the real ˜SM are trained
to provide the target output with the ﬁxed plane wave
at the input. Using typical functions in application pro-
gram interfaces, such as tensorﬂow.clip by value, one can
clip the values of the amplitude modulation (we use the
range [−1.0, 1.0]). In contrast with the phase-modulator
case, the performance in the amplitude modulation case
is reduced. Our numerical experiments show that con-
vergence (corresponding to a cost-function smaller than
10−4) is not reached. On the contrary, the error reaches
a stable minimal value after about 1000 epochs. The
minimal error decreases with the size of the reservoir
(Fig. 4b). In Fig. 4b, we also account for the fact that
modulator devices have limited resolution, and accessible
modulation levels are quantized with a given number of
bits. We can implement the level quantization in Tensor-
Flow by using tensorﬂow.quantize and dequantize, after
each iteration. In Fig. 4, we show results for phase-only
modulation for the 1-bit case, corresponding to modula-
tion levels −1, 0, 1, as well as for the 8 and 64-bit cases.

V. CONCLUSIONS

We have investigated the use of machine learning
paradigms for designing linear multi-level quantum gates
by using a complex transmitting multi-modal system.
The developed algorithms are versatile and scalable when
the unitary operator for the random system is either
known or unknown. We show that generalized single-
qudit gates can be designed. The overall methodology is
easily implemented by the TensorFlow application pro-
gram interface and can be directly adapted to experimen-
tally retrieved data. The method can be generalized to

(a)

(b)

x1

x2

xN

xM

w11
w12

wMM
in
MS

Transmission matrix before training 

T =

i
0.4 0.7
i
0.6 0.3
i
1.1 0.1
i
0.9 0.1
i
0.4 0.2

i
0.3 0.6
i
0.6 0.1
i
0.8 0.2
i
0.1 0.3
i
0.8 0.4

i
0.6 0.3
1.0 0.9
0.9
0.4 0.3
0.4 0.1

i

i
i

i

0.6
0
0.7 0.3
0.9
0.1 0.3

i

i

i
0.4 0.7
i
1.0 0.8
i
1.2 0.4
0.3 0.4
0.4

i

After training

T =

0 1 0
0 0 1
1 0 0
0 0 0
0 0 0

0
0
0
i
0.2 0.1
i
0.3 0.9

0
0
0
i
0.1 1.0
i
0.2 0.1

u11
u12

uMM

MU

(c)

s
h
c
o
p
e

16

14

12

10

8

6

4

2

6

y1

y2

yN

yM

out

MS

0

20

40

60

80

100

reservoir size M

Figure 3. Example of inference training of a random system (M = 5) to act as X3 gate. (a) Neural network model (in our
example Sout
M is not used); (b) numerical examples for the trasmission matrix TM = UM · Sin
M before and after training; (c)
scaling properties in terms of training epochs. Parameters: ntrain = 100, nvalid = 50, evalid = 10

−3, nepoch = 6

n
o
i
t
c
n
u
f

t
s
o
c

10-3

10-4

10-5

10-6

10-7

.

(a)

0

50

100 150 200 250
reservoir size M

300

350 400

n
o
i
t
c
n
u
f

t
s
o
c

0.35

0.30

0.25

0.20

0.15

0.10

0.05

0.00

64 bit
20

(b)

1 bit

8 bit

60

40
reservoir size M

80

100

(a) Error after 1000 training epochs versus the size of the reservoir M ; the inset shows a sketch of the experimental
Figure 4.
implementation with a single spatial light modulator (SLM). (b) Error versus reservoir size M after 1000 epochs with a single
amplitude modulator (with sign) with quantized levels (diﬀerent bit numbers are indicated).

 
 
 
 
 
 
 
more complex information protocols, and embedded in
real-world multi-modal systems.

Acknowledgments – We acknowledge support from
the Sapienza Ateneo, PRIN2015 NEMO project
(2015KEZNYM), the H2020 QuantERA project QUOM-
PLEX (grant number 731473), and the PRIN 2017
PELM project (20177PSCKT), the H2020 grant number
820392.

7

[1] S. R. Huisman, T. J. Huisman, T. A. W. Wolterink,
A. P. Mosk, and P. W. H. Pinkse, Programmable mul-
tiport optical circuits in opaque scattering materials,
Opt. Express 23, 3102 (2015).

[2] A. Babazadeh, M. Erhard, F. Wang, M. Malik,
R. Nouroozi, M. Krenn, and A. Zeilinger, High-
Dimensional Single-Photon Quantum Gates: Concepts
and Experiments, Phys. Rev. Lett. 119 (2017).

[3] M. Malik, M. Erhard, M. Huber, M. Krenn, R. Fickler,
and A. Zeilinger, Multi-photon entanglement in high di-
mensions, Nat. Photonics 10, 248 (2016).

[4] C. Taballione, T. A. W. Wolterink, J. Lugani, A. Eck-
stein, B. A. Bell, R. Grootjans, I. Visscher, D. Geskus,
C. G. H. Roeloﬀzen, J. J. Renema, I. A. Walmsley,
P. W. H. Pinkse, and K.-J. Boller, 8&#x00d7;8 recon-
ﬁgurable quantum photonic processor based on silicon
nitride waveguides, Opt. Express 27, 26842 (2019).
[5] M. W. Matth`es, P. del Hougne, J. de Rosny, G. Lerosey,
and S. M. Popoﬀ, Turning Optical Complex Media into
Universal Reconﬁgurable Linear Operators by Wavefront
Shaping, arXiv:1810.05688 (2018).

[6] P. Zhao, S. Li, X. Feng, S. M. Barnett, W. Zhang,
K. Cui, F. Liu, and Y. Huang, Universal linear opti-
cal operations on discrete phase-coherent spatial modes,
arXiv:1801.05092 [physics.optics] (2018).

[7] I. M. Vellekoop and A. P. Mosk, Focusing coher-
ent light through opaque strongly scattering media,
Opt. Lett. 32, 2309 (2007).

[8] I. M. Vellekoop, Feedback-based wavefront shaping,

Opt. Express 23, 12189 (2015).

[9] R. Horstmeyer, H. Ruan, and C. Yang, Guidestar-
assisted wavefront-shaping methods for focusing light
into biological tissue, Nat. Photonics 9, 563 (2015).
[10] D. Pierangeli, V. Palmieri, G. Marcucci, C. Moriconi,
G. Perini, M. De Spirito, M. Papi, and C. Conti, Deep
optical neural network by living tumour brain cells,
arXiv:1812.09311 (2018).

[11] D. Fu, Y. Zhou, R. Qi, S. Oliver, Y. Wang, S. M. H.
Rafsanjani, J. Zhao, M. Mirhosseini, Z. Shi, P. Zhang,
and R. W. Boyd, Realization of a scalable laguerre-
gaussian mode sorter based on a robust radial mode
sorter, Opt. Express 26, 33057 (2018).

[12] N. K. Fontaine, R. Ryf, H. Chen, D. T. Neilson,
K. Kim, and J. Carpenter, Laguerre-Gaussian mode
sorter, arXiv:1803.04126 [physics.optics] (2018).

[13] D. Verstraeten, B. Schrauwen, M. DHaene, and
D. Stroobandt, An experimental uniﬁcation of reservoir
computing methods, Neural Networks 20, 391 (2007).

[14] G.-B. Huang, Q.-Y. Zhu, and C.-K. Siew, Ex-
treme learning machine: Theory and applications,
Neurocomputing 70, 489 (2006).

[15] F. Duport, B. Schneider, A. Smerieri, M. Haelter-
man, and S. Massar, All-optical reservoir computing,
Opt. Express 20, 22783 (2012).
[16] d. S. G. Van, D. Brunner,

ano, Advances
Nanophotonics 6, 561 (2017).

in photonic

[17] N. Borhani, E. Kakkava, C. Moser, and D. Psaltis,
ﬁbers,

through multimode

Learning
see
Optica 5, 960 (2018).

to

8

[18] S. Lohani, E. M. Knutson, M. O’Donnell, S. D. Huver,
and R. T. Glasser, On the use of deep neural networks
in optical communications, Appl. Opt. 57, 4180 (2018).
[19] Y. Sun, Z. Xia, and U. S. Kamilov, Eﬃcient and accu-
rate inversion of multiple scattering with deep learning,
Opt. Express 26, 14678 (2018).

[20] X. Lin, Y. Rivenson, N. T. Yardimci, M. Veli,
Y. Luo, M. Jarrahi, and A. Ozcan, All-optical ma-
chine learning using diﬀractive deep neural networks,
Science 361, 1004 (2018).

[21] G. Favraud, J. S. T. Gongora, and A. Fratalocchi, Evolu-
tionary photonics: Evolutionary photonics for renewable
energy, nanomedicine, and advanced material engineer-
ing, Laser & Photonics Reviews 12, 1870047 (2018).
[22] N. Mohammadi Estakhri, B. Edwards, and N. Engheta,
Inverse-designed metastructures that solve equations,
Science 363, 1333 (2019).

[23] D. Pierangeli, G. Marcucci, and C. Conti, Large-scale
light modulation,

photonic ising machine by spatial
Phys. Rev. Lett. 122, 213902 (2019).

[24] K. Wu,

J. Garca

P. Ping Shum, and N.
ﬁber
Light: Science & Applications 3, e147 (2014).

np-complete

network

oracle

for

Abajo,

de
Soci,
I. Zheludev, An optical
problems,

C.

[25] D. Englund, H. Larochelle, M. Soljaˇci´c, M. Hochberg,
M. Prabhu, N. C. Harris, S. Skirlo, S. Zhao, T. Baehr-
Jones, X. Sun, and Y. Shen, Deep learning with coherent
nanophotonic circuits, Nat. Photonics 11, 441 (2017).
[26] L. Pilozzi, F. A. Farrelly, G. Marcucci, and C. Conti, Ma-
chine learning inverse problem for topological photonics,
Communications Physics 1, 57 (2018).

[27] M. Krenn, M. Malik, R. Fickler, R. Lapkiewicz, and
A. Zeilinger, Automated search for new quantum exper-
iments, Phys. Rev. Lett. 116, 090405 (2016).

[28] T. F¨osel, P. Tighineanu, T. Weiss, and F. Marquardt, Re-
inforcement learning with neural networks for quantum
feedback, Phys. Rev. X 8, 031084 (2018).

[29] A. Lumino, E. Polino, A. S. Rab, G. Milani,
N. Spagnolo, N. Wiebe, and F. Sciarrino, Experimen-
tal phase estimation enhanced by machine learning,
Phys. Rev. Applied 10, 044033 (2018).

[30] Z. A. Kudyshev, S. Bogdanov, T. Isacsson, A. V. Kild-
ishev, A. Boltasseva, and V. M. Shalaev, Rapid classiﬁ-
cation of quantum sources enabled by machine learning,
arXiv:1908.08577 [physics.optics] (2019).

[31] S. Leedumrongwatthanakun, L. Innocenti, H. Deﬁenne,
T. Juﬀmann, A. Ferraro, M. Paternostro, and S. Gi-
gan, Programming linear quantum networks with a mul-
timode ﬁber, arXiv:1902.10678 [quant-ph] (2019).
[32] L. Jing, C. Gulcehre, J. Peurifoy, Y. Shen, M. Tegmark,
M. Soljaˇci´c, and Y. Bengio, Gated Orthogonal Recurrent
Units: On Learning to Forget, arXiv:1706.02761 (2017),
arXiv: 1706.02761.

[33] R. Dangovski, L. Jing, and M. Soljacic, Rotational Unit

valued
Phys. Rev. A 62, 052309 (2000).

gates

logic

for

Stroud, Multi-
quantum computation,

[35] Y.-M. Di and H.-R. Wei, Elementary gates for ternary

quantum logic circuit, arXiv:1105.5485 (2011).

and M. C. Sori-
computing,

reservoir

of Memory, arXiv:1710.09537 (2017).
and C. R.

[34] A. Muthukrishnan

[36] M. Mirhosseini, O. S. Maga˜na-Loaiza, M. N. O’Sullivan,
B. Rodenburg, M. Malik, M. P. J. Lavery, M. J.
Padgett, D. J. Gauthier, and R. W. Boyd, High-
dimensional quantum cryptography with twisted light,
New J. Phys. 17, 033033 (2015).

[37] S. Ecker, F. Bouchard, L. Bulla, F. Brandt, O. Kohout,
F. Steinlechner, R. Fickler, M. Malik, Y. Guryanova,
R. Ursin, and M. Huber, Entanglement distribution be-
yond qubits or: How I stopped worrying and learned to
love the noise, arXiv:1904.01552 (2019).

[38] S. A. Goorden, M. Horstmann, A. P. Mosk, B. ˇSkori´c,
and P. W. H. Pinkse, Quantum-secure authentication of
a physical unclonable key, Optica 1, 421 (2014).

[39] T. B. H. Tentrup, W. M. Luiten, R. van der Meer,
P. Hooijschuur, and P. W. H. Pinkse, Spatially en-

9

coded light for Large-alphabet Quantum Key Distribu-
tion, arXiv:1808.02823 (2018).

[40] M. Leonetti, S. Karbasi, A. Maﬁ, E. DelRe, and C. Conti,
Secure information transport by transverse localization of
light, Sci. Rep. 6, 29918 (2016).

Wang,

[41] J.
for
Computers & Mathematics with Applications 26, 23 (1993).

networks
equations,

Recurrent
linear

solving

matrix

neural

[42] X. Gao, M. Krenn, J. Kysela, and A. Zeilinger, Ar-
bitrary d-dimensional pauli x gates of a ﬂying qudit,
Phys. Rev. A 99, 023825 (2019).

