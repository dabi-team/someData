1
2
0
2

r
p
A
5

]
I

A
.
s
c
[

1
v
0
8
9
1
0
.
4
0
1
2
:
v
i
X
r
a

Probabilistic Programming Bots in Intuitive
Physics Game Play

Fahad Alhasoun1*, Sarah Alnegheimish1, Joshua Tenenbaum1

1Massachusetts Institute of Technology

April 6, 2021

Abstract

Recent ﬁndings suggest that humans deploy cognitive mechanism of physics
simulation engines to simulate the physics of objects. We propose a framework for
bots to deploy probabilistic programming tools for interacting with intuitive physics
environments. The framework employs a physics simulation in a probabilistic way
to infer about moves performed by an agent in a setting governed by Newtonian
laws of motion. However, methods of probabilistic programs can be slow in such
setting due to their need to generate many samples. We complement the model with
a model-free approach to aid the sampling procedures in becoming more efﬁcient
through learning from experience during game playing. We present an approach
where combining model-free approaches (a convolutional neural network in our
model) and model-based approaches (probabilistic physics simulation) is able to
achieve what neither could alone. This way the model outperforms an all model-free
or all model-based approach. We discuss a case study showing empirical results of
the performance of the model on the game of Flappy Bird.

Introduction

The last few years have been marked with exceptional progress in the ﬁeld of Artiﬁcial
Intelligence (AI). Much of the progress has come from recent advances in deep learning.
Models employing deep neural networks achieved remarkable performance in many
areas including speech recognition, object recognition and reinforcement learning
[LBH15; Mni+16; Mni+15; Gu+16]. In reinforcement learning, Mnih et al. proposed
a deep learning approach to estimate Q-learning function of state and action tuples in
Atari games to achieve human-level performance [Mni+15; Mni+13; Guo+14]. Silver et
al. applied a similar approach of deep learning to learn policy and value functions of
states and action in the complex game of AlphaGo [Sil+16; Sil+17].

Another front where AI is progressing exceptionally is approaching AI through draw-
ing inspiration from human cognitive processes [Lak+16; KBS17; Bat+16; Ngu+20;

*fha@mit.edu

1

 
 
 
 
 
 
Rem+20; LGF16; Bar+19]. Lake et al. developed a model for human-level concept
learning through probabilistic induction [LST15]. The approach deploys methods of
probabilistic programming [Gha15] to construct computational frameworks that cap-
ture human learning abilities in forming concepts. The construction of computational
frameworks was used to draw insights on human cognition in other areas, Battaglia et
al. proposed a model based on an intuitive physics engine as a cognitive mechanism
humans use to make robust inferences in complex natural scenes [BHT13; Bat+16].

In human development, infants have primitive concepts of how objects move in
their environments, this is observed through their ability to track moving objects around
them. It is through these primitive concepts infants grow to learn faster and make more
accurate predictions [MWF83; Lak+16]. Experiments on humans cognitive processes
of intuitive physics inference show that as tasks of inference are harder, the response
time of humans increases [Ham+15]. This is due to a trade off between response time
and number of physics simulations performed, and the harder the task gets, the more
simulation runs humans seem to perform.

Several attempts proposed models for agents to develop a sense of intuitive physics
that humans possess [Wat+17; Agr+16; Wu+16; Wu+15; Bak+19]. Agrawal et al.
approached the problem by reverse engineering intuitive physics [Agr+16]. Using robot
arms, the agent performed enormous number of actions (i.e. pokes) on objects placed
on a table to understand the process by how objects move. The approach is inspired by
how infants develop their physics intuition. The model is then tested by requiring an
agent to move objects on a table to match a given ﬁnal state of object positions on the
table. Wu et al. proposed a model capable of predicting the movement of objects placed
on an inclined surface given image pixel data [Wu+15]. The model incorporates deep
learning methods to learn physical features of objects and a 3D physics simulation to
predict their trajectories and where an object will most likely stop, the approach was
capable of achieving an accuracy comparable to human subjects.

In this paper, we propose a framework for bots to deploy tools for interacting with
the physics of their environments. The bots employ a coupling of a probabilistic program
with a physics simulation engine to do inference of moving objects in a setting governed
by Newtonian laws of motion. However, methods of probabilistic programs can be slow
in such setting due to their need to generate many samples. Hence we complement our
approach with a model free component to aid the sampling procedures in becoming
more efﬁcient through learning from experience during game playing . We present a
case where a combination of model-free approaches (CNN in our model) and model-
based approaches (probabilistic programming and physics simulation) is able to achieve
what neither could alone [Kan+17; Sch+15; Hen+17]. Existing research proposed such
ideas of combining model-free and model-based approaches in other contexts [Che+17;
Nag+18]. The performance of the model outperforms an all model-free or model-based
approach [EG18]. It has been evident that such approaches combine the best of both
worlds [Bat+18]. The case study shows empirical results of the performance of the
model on the game of Flappy Bird, a game of a bird in free fall and required to avoid
obstacles by jumping through openings. Our model exhibits similar patterns in behavior
of humans when it comes to the trade off between sampling time versus accuracy of
decisions [Ham+15] and the ability to learn through experience of game play.

In sections 2, 3 and 4 of the paper, we propose the framework and discuss the process

2

Figure 1: The decision making pipeline, it starts with the CNN having the objective
of improving the sampling process through estimating α, then a probabilistic program
together with an intuitive physics engine samples decisions according probabilities
estimated by CNN and simulates samples in a physics simulation. The physics simula-
tions returns the state of the agent after a period of time ch (ch = False if unwanted
collision was not observed). Flappy bird is used as an example here to demonstrate the
structure of the model, the results and discussion section include an implementation of
the model.

of inference and learning of parameters. Sections 5 include empirical results of the
model performance while playing Flappy Bird and discussion.

Methods

Given the state of an agent in an environment governed by Newtonian laws of physics,
the goal is to have an agent that predicts the desired behavior given a state. It does that
by probabilistically sampling actions most suitable to achieve the desired behavior. To
illustrate, given a bot in a state very close to hit the ﬂoor, the agent ﬁrst should be able
to infer that it needs to increase altitude and then probabilistically bias the sampling
process to actions that are in line with increasing altitude to avoid collision.

The decision making pipeline of the agent includes two main subparts; the ﬁrst is a
convolutional neural network (CNN) and the second is a probabilistic framework for
sampling actions in an intuitive physics setting. The general architecture of the pipeline
is included in ﬁgure 1. The CNN takes pixel data as inputs and produces the parameter
α corresponding to prior probabilities for the probabilistic program. The probabilistic
program is parametrized by α the prior probabilities for each action, θ the probabilities
of each action, a the sampled action, γa the velocity of the action and ch the collision
state at time h.

Convolutional Neural Network (CNN)

The objective of the CNN is to make the sampling procedure of probabilistic programs
more efﬁcient, this is done through estimate parameters of the prior probability distribu-

3

✓⇠Dirichlet(↵)a⇠Categorical(✓)↵tt+2t+4 at at+2 at+4 at+1 at+3 a⇠Gaussian(µa, a) a,haddatomchifch=FalsethenUpdatesamples,mActionssamplingProbabilisticIntuitivePhysicsEngineCNNPhysicssimulationtion of actions to be taken given a state of the agent. This helps the probabilistic model
in sampling more effectively through skewing a Dirichlet distribution in a manner where
actions sampled are more likely to match a desired behavior. CNN has a similar archi-
tecture to that developed by Mnih et al. [Mni+15].The CNN takes the last 4 frames as
inputs and outputs α parameterizing the prior for the distribution of actions probabilities.
The input of the neural network consists of an 80
4 frames of pixel data for the
past 4 time steps after preprocessing. Preprocessing denoted φ includes transforming
the raw pixel data to grayscale then rescaling frame size to a resolution of 80
80. The
8 with stride 4 on the input frames and
ﬁrst hidden layer convolves 32 ﬁlter of size 8
applies rectiﬁer nonlinearity. The second hidden layer convolves 64 ﬁlters with sizes of
4 with stride 2 then applies rectiﬁer nonlinearity. The third hidden layer convolves
4
64 ﬁlters with sizes of 2
2 with stride 1 then applies rectiﬁer nonlinearity. The fourth
layer is a fully connected 512 nodes with rectiﬁer nonlinearity. Then the output layer is
fully connected and has as many nodes as there are actions in the game. The output of
the CNN parametrizes a Dirichlet distribution in the probabilistic framework.

80

×

×

×

×

×

×

Learning

The model learns from experience through the data generated while game playing.
Sources of the data include positions information of objects in the environment, pixel
data of the state, actions taken by the agent and the rewards received at every time step.
The CNN learns from the pixel data of past states and actions; the inputs to the
CNN are the pixel data and the outputs are the frequencies of actions in subsequent time
steps of a predeﬁned interval. αi is the frequency of making decision i in the future ∆
time steps after state s. The training sample of the model has (s, α) tuples after which
the model was negatively rewarded are not included in the training of the CNN since
the goal is to learn about values of parameter α resulting in the bot being positively
rewarded. The CNN is ﬁt through applying stochastic gradient descent on the following
cost function:

Where the parameters for the neural network are denoted by κ .

L(α, φ, s, κ) = (cid:0)α

(φ(s), κ)(cid:1)2

− M

Inference

We use probabilistic programming to perform the tasks of inference in a fashion similar
to the discussion by Gharmani et al. in [Gha15]. This section discusses the process
by which the data for the world of the agent and its actions are generated. We employ
control ﬂow to sample actions resulting in no collisions.

At every iteration, the agent will use a probabilistic program coupled with the
physics engine algorithm illustrated in Algorithm 1 to do inference about the physics
of its future. The Algorithm takes as inputs the state of the world deﬁned by the past 4
frames of pixel data and past positions data of objects in the environment. The CNN
estimates the direction the agent should be moving towards through estimating Dirichlet

4

parameters α. An alternative approach is to is to provide a α for a uniform Dirichlet
on probabilities of actions which can be computationally cumbersome under tight time
constraints to make a decision.

Algorithm 1 Probabilistic Intuitive Physics Process

procedure GETACTION(s, t, m)

Inputs are the state s, current time t and previously sampled actions m
Initialize the simulation horizon h to constant c
α
while elapsed time < (cid:15) do

(cid:46) estimate the starting α from the CNN
(cid:46) keep sampling for a duration of (cid:15) seconds

(φ(s))

SAMPLEACTIONS(s, t, α, h)

αi +

1aj =i

h(cid:80)
j=1

← M
a, ch ←
if ch is False then
add a to m
αi ←
h
end if
end while
ch = False)
˜p(at = x
|
samples in m
˜at ←
return ˜at
end procedure
procedure SAMPLEACTIONS(s, t, α, h)

arg maxat(˜p(at|

h + δ

(cid:80)|

←

←

ch = False))

(cid:46) observe when unwanted collision didn’t occur
(cid:46) store samples resulting in no unwanted collisions

(cid:46) update α parameters

(cid:46) expand the horizon of the simulation

m
i=1 1mi,t=x
|

(cid:46) estimating the conditional from

Dirichlet(α)
(cid:46) sample θ, the probability of actions
1)
1) ←
Gaussian(µai, σai)
(cid:46) sample impact on the velocity
PHYSICSSIMULATION(s, [γat, ..., γat+(h−1) ], h) (cid:46) simulate sampled plan

Categorical(θ) (cid:46) sample actions for timesteps t to t + (h

−

−

θ
←
at, ..., at+(h
γai ←
ch ←
for next h steps
return a, ch
end procedure

The agent proceeds with an iterative process between a probabilistic program and a
physics engine simulation. The process starts with sampling probabilities for each action
denoted θ from a Dirichlet distribution. The agent samples h actions from a categorical
distribution parametrized with θ. To account for the possible stochasticity of actions, a
Gaussian distribution is ﬁtted overs the observed change in velocities for every action
type, the model learns the distributions through the history of its position data and the
actions taken in the past. For every move the agent sampled, γai is sampled from a
Gaussian corresponding to action type ai.

After an action plan is sampled from the probabilistic program, the simulation engine
will simulate the plan. The physics simulation returns the status of the agent after h time
steps. The process continues to sample then simulate in an iterative manner for an (cid:15)
milliseconds. The physics engine estimates physics characteristics of the environment
(for example the gravitational acceleration) through the application of Newtonian laws

5

Figure 2: This ﬁgure demonstrates how CNN helps making the process of sampling
more efﬁcient. Frames from the game are inputs to the CNN which outputs α parameters
for the Dirichlet distribution illustrated in the ﬁgure for the case of Flappy Bird. The
dashed line of θ∗f lap = 0.072 corresponds to the probability of ﬂap where bird maintains
the same height. In (a) bird is descending and is faced with a gap that is higher than its
current altitude, a trained CNN outputs α = (3.6, 16.8) biasing the sampling procedure
to generate actions that will more likely result in its ascending towards the opening.
In (b) bird is descending where a ﬂap will result in its collision, the CNN outputs
α = (0.7, 19.7) corresponding to values of θ that will more likely result in its descending
towards the opening.

of motion to the historical positions data it observed.

Upon the end of the iterative sampling and simulation process, the bot is to estimate
ch = False) through the set of simulations of actions called m.

the distribution p(at|
Given the samples it generated in m, the conditional density is given by:

p(at = x

|

ch = False)

˜p(at = x

ch = False)
|

∝

=

m
|(cid:88)
|

i=1

1(mi,t=x)

The decision on which action to take at the current time step denoted ˜at is then given
by:

˜at ∼

argmaxat ˜p(at|

ch = False)

Before starting a new iteration of sampling new actions then simulating, the parame-
1) if a simulation results in no collision
ters α are updated with the actions at, ..., at+(h
(i.e. ch = F alse). If so, the process increments the simulation horizon h with δ time
steps, the strategy is to continuously expand the simulation horizon as no collisions
are observed for the bot to detect potential further away obstacles in its direction. The
process of sampling and simulation loops until (cid:15) milliseconds pass. The Algorithm
returns the action providing the highest probability for the bot to survive unwanted
collisions in h time steps.

−

6

0.00.20.40.60.81.001234567Dirichlet(↵)(a)(b)(a)(b)✓⇤flap=0.072Dirichlet(↵)Experiments and Results

The approach was tested on the game of Flappy Bird, a game of inference about the
physics of the bird and its environment towards avoiding collision with obstacles.
Several replica implementations of the game is available on github along with an
implementation of DQN and A3C proposed by Mnih et al. [Mni+15; Mni+16]. The
game is available for play at http://ﬂappybird.io. During the game, bird is required to
chose from ﬂapping or doing nothing to avoid collisions and pass through the openings.
Figure 2 demonstrates how CNNs aid probabilistic programs in sample more efﬁ-
ciently, the task for the agent is to infer about actions with highest probability in terms
of passing through openings facing Flappy Bird. The agent is to sample action θ from
a prior parametrized by α, the α is given by the CNN to help reduce the number of
samples needed to pass through the opening. Given actions of ﬂap and do nothing, with
θ∗f lap = 0.072 the agent is more likely to sample ﬂaps in its plan as to maintain its
current height moving towards an opening. Figure 2(a) illustrates the frames of pixel
data used as inputs to estimate the Dirichlet (i.e. Beta since two actions are offered
for bird to pick from), the probability of θ > θ∗f lap is larger resulting in simulating
more samples with ascending altitude. On the contrary, in (b) the sampling distribution
is skewed towards values less than θ∗ resulting in the sampling process halting from
ﬂapping in most of its samples. Hence, the CNN skews the Dirichlet distribution in a
manner where actions sampled match the desired change in altitude.

Figure 3 demonstrates the performance of the model against alternative approaches
and state of the art techniques. Average scores are calculated after running each trained
model for 10 times and observing the ﬁnal score. PB-CNN is the proposed methodology
and PB-Uniform is an alternative approach where a CNN is not present, instead the
Dirichlet distribution is parametrized with all one parameter resulting in a uniform
distribution over the parameter θ. A3C is an implementation of Mnih et al. [Mni+16] on
Flappy Bird. Human data are gathered through players on the web page in flappy.io.
Figure 3 (a) shows the average accuracy of PB-CNN and PB-Uniform for different
(cid:15) milliseconds of time allowed for iterative sampling and simulation process. The
advantage CNN brings to the model is signiﬁcant, this is because CNN narrows down the
sampling space signiﬁcantly allowing the model to explore the conditional distribution
of samples given no collisions much faster (i.e. higher frequency of samples resulting
in ch = F alse). In ﬁgure 3 (b) we show the average score per training epoch of the
A3C approach. In ﬁgure 3 (c), the average score of humans play is close to PB-Uniform
with (cid:15) = 30ms and (cid:15) = 40ms. After training the CNN part of PB-CNN of the model
on 10000 frames of game play, the model’s performance improves signiﬁcantly. The
performance of PB-CNN with (cid:15) = 1.5ms is similar to A3C in average score. The
average score is calculated for 10 times of game play.

The average score for humans was 11.27 in 47 million games played, 95% of
them had a score of 6 points or lower according to flappybird.io. One possible
reason why humans under-perform could be explained by their signiﬁcantly large delay
in response time compared to methods discussed in the paper here. Under a task of
inference about the physics of the world, research suggested that humans response rate
was in between 500 and 2000 milliseconds depending on the hardness of the inference
task at hand [Ham+15]. The urgency of making a decision leaves no ample time for

7

Figure 3: Results of models trained to play Flappy Bird. PB-CNN is the proposed
model, PB-Uniform is composed of the model proposed without the CNN where the
probabilistic program starts with an all-ones α parameter. A3C is the model proposed by
[Mni+16], human data for game play are reported by the web page flappybird.io

sampling and simulating the physics of the game that could be enough for humans to
perform as well as bots.

Conclusion and Future Work

We propose a framework for bots to maneuver games with intuitive physics inspired by
cognitive processes of humans. The approach draws inspiration from recent approaches
of modeling concept learning where the framework includes a coupling of a probabilistic
program with a physics simulation engine. The model was tested on the game of Flappy
Bird and compared to state of the art techniques of model-free approaches. Advantages
of our model over model-free approaches namely A3C is the ability to learn from very
few examples relative to the number of examples A3C requires. Advantages of our
model over model-based approaches is its ability to learn from experience.

Potential future work include investigating approaches to learn about rewards struc-
tures in games of physics intuition. This will enable bots to perform more complex
moves beyond simpler tasks such as the ones in the illustrated game of Flappy Bird
where the objective is to avoid unwanted collision. Other games such as Space Invaders
involve learning strategies of shooting and hiding that are beyond the capabilities of
the model in its existing state. Another potential future direction is to deploy neural
network to detect objects in the frames of the game rather than explicitly having access
to position data of the objects in the game. This would potentially help the framework
better generalizes over other games.

References

[MWF83] Michael McCloskey, Allyson Washburn, and Linda Felch. “Intuitive physics:
The straight-down belief and its origin.” In: Journal of Experimental Psy-
chology: Learning, Memory, and Cognition 9.4 (1983), p. 636.

8

Human(cid:31)=30ms(cid:31)=40ms(cid:31)=1.5ms0102030405060708090averagescores11.278.9018.5050.03HumanPB-UniformPB-CNN020406080100(cid:31)(ms)020040060080010001200140016001800averagescorePB-CNNPB-Uniform010002000300040005000600070008000Epoch02505007501000125015001750averagescoreA3C(cid:31)(cid:30)(cid:29)(cid:31)(cid:28)(cid:29)(cid:31)(cid:27)(cid:29)[BHT13]

Peter W Battaglia, Jessica B Hamrick, and Joshua B Tenenbaum. “Simula-
tion as an engine of physical scene understanding”. In: Proceedings of the
National Academy of Sciences 110.45 (2013), pp. 18327–18332.

[Mni+13] Volodymyr Mnih et al. “Playing atari with deep reinforcement learning”.

In: NIPS Deep Learning Workshop (2013).

[Guo+14] Xiaoxiao Guo et al. “Deep learning for real-time Atari game play using
ofﬂine Monte-Carlo tree search planning”. In: Advances in neural informa-
tion processing systems. 2014, pp. 3338–3346.

[Gha15]

Zoubin Ghahramani. “Probabilistic machine learning and artiﬁcial intelli-
gence”. In: Nature 521.7553 (2015), pp. 452–459.

[Ham+15]

Jessica B Hamrick et al. “Think again? The amount of mental simulation
tracks uncertainty in the outcome.” In: CogSci. Citeseer. 2015.

[LST15]

Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. “Human-
level concept learning through probabilistic program induction”. In: Sci-
ence 350.6266 (2015), pp. 1332–1338.

[LBH15]

Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. “Deep learning”. In:
Nature 521.7553 (2015), pp. 436–444.

[Mni+15] Volodymyr Mnih et al. “Human-level control through deep reinforcement

learning”. In: Nature 518.7540 (2015), pp. 529–533.

[Sch+15]

[Wu+15]

[Agr+16]

[Bat+16]

[Gu+16]

[Lak+16]

[LGF16]

John Schulman et al. “Trust region policy optimization”. In: International
conference on machine learning. 2015, pp. 1889–1897.

Jiajun Wu et al. “Galileo: Perceiving physical object properties by in-
tegrating a physics engine with deep learning”. In: Advances in neural
information processing systems. 2015, pp. 127–135.

Pulkit Agrawal et al. “Learning to Poke by Poking: Experiential Learning of
Intuitive Physics”. In: Advances in Neural Information Processing Systems
29. Ed. by D. D. Lee et al. Curran Associates, Inc., 2016, pp. 5074–5082.
URL: http://papers.nips.cc/paper/6113- learning- to- poke- by- poking-
experiential-learning-of-intuitive-physics.pdf.

Peter Battaglia et al. “Interaction networks for learning about objects,
relations and physics”. In: Advances in Neural Information Processing
Systems. 2016, pp. 4502–4510.

Shixiang Gu et al. “Continuous deep q-learning with model-based acceler-
ation”. In: International Conference on Machine Learning. PMLR. 2016,
pp. 2829–2838.

Brenden M Lake et al. “Building machines that learn and think like people”.
In: arXiv preprint arXiv:1604.00289 (2016).

Adam Lerer, Sam Gross, and Rob Fergus. “Learning physical intuition
of block towers by example”. In: International conference on machine
learning. PMLR. 2016, pp. 430–438.

9

[Mni+16] Volodymyr Mnih et al. “Asynchronous methods for deep reinforcement

learning”. In: International Conference on Machine Learning. 2016, pp. 1928–
1937.

[Sil+16]

[Wu+16]

David Silver et al. “Mastering the game of Go with deep neural networks
and tree search”. In: Nature 529.7587 (2016), pp. 484–489.

Jiajun Wu et al. “Physics 101: Learning Physical Object Properties from
Unlabeled Videos.” In: BMVC. Vol. 2. 6. 2016, p. 7.

[Che+17] Yevgen Chebotar et al. “Combining model-based and model-free up-
dates for trajectory-centric reinforcement learning”. In: arXiv preprint
arXiv:1703.03078 (2017).

[Hen+17]

Peter Henderson et al. “Deep reinforcement learning that matters”. In:
arXiv preprint arXiv:1709.06560 (2017).

[Kan+17] Ken Kansky et al. “Schema networks: Zero-shot transfer with a generative
causal model of intuitive physics”. In: arXiv preprint arXiv:1706.04317
(2017).

[KBS17]

[Sil+17]

Joseph Kim, Christopher Banks, and Julie Shah. “Collaborative planning
with encoding of users’ high-level strategies”. In: Proceedings of the AAAI
Conference on Artiﬁcial Intelligence. Vol. 31. 1. 2017.

David Silver et al. “Mastering chess and shogi by self-play with a general
reinforcement learning algorithm”. In: arXiv preprint arXiv:1712.01815
(2017).

[Wat+17] Nicholas Watters et al. “Visual interaction networks: Learning a physics
simulator from video”. In: Advances in neural information processing
systems. 2017, pp. 4539–4547.

[Bat+18]

[EG18]

Peter W Battaglia et al. “Relational inductive biases, deep learning, and
graph networks”. In: arXiv preprint arXiv:1806.01261 (2018).

Richard Evans and Edward Grefenstette. “Learning explanatory rules from
noisy data”. In: Journal of Artiﬁcial Intelligence Research 61 (2018), pp. 1–
64.

[Nag+18] Anusha Nagabandi et al. “Neural network dynamics for model-based
deep reinforcement learning with model-free ﬁne-tuning”. In: 2018 IEEE
International Conference on Robotics and Automation (ICRA). IEEE. 2018,
pp. 7559–7566.

[Bak+19] Anton Bakhtin et al. “Phyre: A new benchmark for physical reasoning”.

In: arXiv preprint arXiv:1908.05656 (2019).

[Bar+19]

Fabien Baradel et al. “Cophy: Counterfactual learning of physical dynam-
ics”. In: arXiv preprint arXiv:1909.12000 (2019).

[Ngu+20] Hung Nguyen et al. “Learning Intuitive Physics by Explaining Surprise”.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops. 2020, pp. 374–375.

10

[Rem+20] Davis Rempe et al. “Predicting the physical dynamics of unseen 3d objects”.
In: Proceedings of the IEEE/CVF Winter Conference on Applications of
Computer Vision. 2020, pp. 2834–2843.

11

