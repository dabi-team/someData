9
1
0
2

c
e
D
7
1

]

C
O
.
h
t
a
m

[

1
v
2
1
1
8
0
.
2
1
9
1
:
v
i
X
r
a

A learning-based algorithm to quickly compute
good primal solutions for Stochastic Integer
Programs

Yoshua Bengio2,3, Emma Frejinger2,3, Andrea Lodi1,3, Rahul Patel1,3, and
Sriram Sankaranarayanan1

1 Canada Excellence Research Chair, Polytechnique Montreal
2 Department of Computer Science and Operations Research, University of Montreal
3 Mila - Quebec Artiﬁcial Intelligence Institute
andrea.lodi@polymtl.ca

Abstract. We propose a novel approach using supervised learning to
obtain near-optimal primal solutions for two-stage stochastic integer
programming (2SIP) problems with constraints in the ﬁrst and second
stages. The goal of the algorithm is to predict a representative scenario
(RS) for the problem such that, deterministically solving the 2SIP with
the random realization equal to the RS, gives a near-optimal solution
to the original 2SIP. Predicting an RS, instead of directly predicting a
solution ensures ﬁrst-stage feasibility of the solution. If the problem is
known to have complete recourse, second-stage feasibility is also guaran-
teed. For computational testing, we learn to ﬁnd an RS for a two-stage
stochastic facility location problem with integer variables and linear con-
straints in both stages and consistently provide near-optimal solutions.
Our computing times are very competitive with those of general-purpose
integer programming solvers to achieve a similar solution quality.

Keywords: Stochastic Integer Programming · Machine Learning · Heuris-
tics.

1

Introduction

Two-stage stochastic integer programming (2SIP) is a standard framework to
model decision making under uncertainty. In this framework, ﬁrst the so-called
ﬁrst-stage decisions are made. Then, the values of some uncertain parameters in
the problem are determined, as if sampled from a known distribution. Finally,
the second set of decisions are made depending upon the realized value of the
uncertain parameters, the so-called second-stage or recourse of the problem. The
decision maker, in this setting, minimizes the sum of (i) a linear function of
the ﬁrst-stage decision variables and (ii) the expected value of the second-stage
optimization problem.

2SIP is studied extensively in the literature [4, 7, 11, 13–15, 17–20, 24] ow-
ing to its applicability in various decision making situations with uncertainty,
like the stochastic unit-commitment problems for electricity generation [18, 19],

 
 
 
 
 
 
2

Y. Bengio et al.

stochastic facility location problems [14], stochastic supply chain network design
[21], among others. With the overwhelming importance of 2SIP a wide range of
solution algorithms have been proposed, for example, [1, 2, 6, 15, 22, 23].

In this paper we are interested in using machine learning (ML) to obtain good
primal solutions to 2SIP. Along this line, Nair et al. [16] proposed a reinforcement
learning-based heuristic solver to quickly ﬁnd solutions to 2SIP. Given that the
agent can be trained oﬄine, the algorithm provided solutions much faster for
some classes of problems compared to an open-source general-purpose mixed-
integer programming (MIP) solver, in their case, SCIP [9, 10]. However, their
method is based on the following restrictive assumptions:

a. All ﬁrst-stage variables are required to be binary. General integer variables

or continuous variables in the ﬁrst stage cannot be handled.

b. Every binary vector is required to be feasible for the ﬁrst stage of the problem,

i.e., no constraints are allowed in the ﬁrst stage.

Assumption a above is intrinsic to the method in [16], as both the initialization
policy and the local move policy of the method involves ﬂipping the bits of the
ﬁrst-stage decision vector. Hence, one cannot easily have general integer vari-
ables or continuous variables. Assumption b is again crucial to the algorithm in
[16], as ﬂipping a bit in the ﬁrst stage could potentially make the new decision
infeasible and it might require a more complicated feedback mechanism to check
and discard infeasible solutions. In fact, if there are constraints, it is N P -hard
to decide if there exists a ﬂip that keeps the decision feasible. Alternatively, one
could empirically penalize the infeasible solutions, but tuning the penalty might
be a hard problem in itself.

In contrast, our method does not require either of these two restrictive as-
sumptions. We allow binary, general integer as well as continuous variables in
both ﬁrst and second stage of the problem. We also allow constraints in both
stages of the problem. Furthermore, we have a simple and direct approach to
handle the ﬁrst-stage constraints, without requiring any empirical penalties.

We make the following common assumption to exclude pathological cases,
where an uncertain realization can turn a feasible ﬁrst-stage decision infeasible.

Assumption 1. The 2SIP has complete recourse, i.e., if a ﬁrst-stage decision
is feasible given the ﬁrst-stage constraints, then it is feasible for all the second
stage problems as well.

We make another assumption of uncertainty with ﬁnite support, so we can
have a proper benchmark to compare our solution against. However, this as-
sumption can be readily removed, without aﬀecting the proposed algorithm.

Assumption 2. The uncertainty distribution in the 2SIP has a ﬁnite support.

2 Problem deﬁnition

We formally deﬁne a 2SIP as follows:

min
x∈Rn1

cT x + Eξ [Q(x, ξ)]

(1a)

Learning based SIP

3

subject to Ax ≤ b
xi ∈ Z,

∀ i ∈ I1

(1b)

(1c)

where,

Q(x, ξ) = min
y∈Rn2

(cid:8)qT

ξ yξ : W yξ ≤ hξ − Tξx, yξ ≥ 0; yi ∈ Z ∀ i ∈ I2

(cid:9)

where x ∈ Rn1 and y ∈ Rn2 are the ﬁrst and second-stage decisions respectively,
c ∈ Rn1 , A ∈ Rm1×n1, b ∈ Rm1, yξ ∈ Rn2 , qξ ∈ Rn2, W ∈ Rm2×n2, Tξ ∈ Rm2×n1 ,
hξ ∈ Rm2, I1 ⊆ {1, . . . , n1}, I2 ⊆ {1, . . . , n2}.

When Assumption 2 holds, the 2SIP described above can also be expressed

as a single deterministic MIP as follows:

min
x,y

cT x +

(cid:88)

∀ξ∈Ξ

pξqT

ξ yξ

subject to Ax ≤ b

∀ξ ∈ Ξ

W yξ ≤ hξ − Tξx
xi ∈ Z,
∀ i ∈ I1
yξi ∈ Z,
∀ξ ∈ Ξ, ∀ i ∈ I2.

(2a)

(2b)

(2c)

(2d)

(2e)

When Assumption 2 does not hold, the formulation (2) could be a ﬁnite-
sample approximation of (1), which is extensively studied in the stochastic pro-
gramming literature. Imitating [16], we compare our algorithm against solving
(2) with a general-purpose MIP solver.

3 Methodology

In this section, we discuss the algorithmic contribution of the paper.

3.1 Surrogate formulation

We ﬁrst deﬁne the objective value function (OVF) Φ : Rn1 → R, mapping
x (cid:55)→ cT x + Eξ [Q(x, ξ)] - the function we are trying to optimize over the mixed-
integer set deﬁned in (1).

Given (2), we deﬁne the surrogate problem associated with ¯ξ = (¯q, ¯h, ¯T ), as

follows:

min
x,y

cT x + ¯qT y

subject to Ax ≤ b

W y ≤ ¯h − ¯T x
xi, yj ∈ Z,

∀ i ∈ I1; j ∈ I2

(3a)

(3b)

(3c)

(3d)

4

Y. Bengio et al.

In other words, should the value that the uncertain parameters are going
to take is deterministically known to be ¯ξ, then the decision maker can solve
the surrogate problem associated with ¯ξ. Now, the idea behind the algorithm
proposed in this paper is captured by Conjecture 1.

Conjecture 1. Let (2) (and hence (1)) have an optimal objective value of f ∗.
There exists q(cid:63), h(cid:63), T (cid:63) in Rn2, Rm2 and Rm2×n1 such that if (x†, y†) solves the
(much smaller) surrogate problem deﬁned by (q(cid:63), h(cid:63), T (cid:63)), then, f ∗ = Φ(x†).

Observe that by construction, x† is feasible to the original problem in (1).
Also, Conjecture 1 asserts that, there exists a realization of the uncertainty
(ξ(cid:63) = (q(cid:63), h(cid:63), T (cid:63))) such that if one deterministically optimizes for that realization
ξ(cid:63), then its solutions are optimal for the original 2SIP. Each such ξ(cid:63) is called a
representative scenario (RS) for the given 2SIP.

Now, given adequate computing resources, one can solve the following bilevel

program to obtain an RS.

min
U,v,w
x,y

cT x +

(cid:88)

∀ξ∈Ξ

pξqT

ξ yξ

subject to (x, w) ∈ arg min
x,w






W yξ ≤ hξ − Tξx
yξi ∈ Z,

∀ξ ∈ Ξ, ∀ i ∈ I2

Ax ≤ b;
W w ≤ v − U x;
xi ∈ Z
wi ∈ Z

∀ i ∈ I1
∀ i ∈ I2






cT x + vT w :

∀ξ ∈ Ξ

(4a)

(4b)

(4c)

(4d)

If the optimal value of this problem matches the optimal value of the original
2SIP, then the corresponding values for (U, v, w) form the RS. Note that if Tξ =
T, ∀ξ ∈ Ξ, then (4) is a mixed-integer bilevel linear program (MIBLP) and can
hopefully be solved faster than the general case.

3.2 Learning algorithm

The goal of ML algorithms is to predict an optimal (U, v, w) to (4), given the
data for the 2SIP. On the one hand, we are expecting the ML algorithms to
predict the solutions of a seemingly much harder optimization problem than
the original 2SIP. On the other hand, this is easier for ML since there are no
constraints on the predicted variables – U, v, w. Supervised learning is a natural
tool to achieve this goal.

Supervised learning can be used if there is a training dataset of problem
instances and their corresponding RS. The task of predicting RS can be formu-
lated as a regression task as RS is real valued. The algorithm tries to minimize
the mean squared error (MSE) between the true and predicted RS. The predic-
tion can also be evaluated on the merits of optimization metrics, comparing the
solution and objective value of true and predicted RS.

Learning based SIP

5

4 Computational study

This section discusses the computation study performed to support Conjecture 1.

4.1 Problem deﬁnition

In this work, we consider a version of two-stage stochastic capacitated facility
location (S-CFLP) for computational analysis. The problem is enhanced such
that both the ﬁrst and the second stage of the problem have integer as well as
continuous variables. More precisely, the ﬁrst stage consists of deciding (i) the
locations where a facility has to be opened (binary decisions), and (ii) if a facil-
ity is open, then the maximum demand that the facility can serve (continuous
decisions). There are also constraints which dictate bounds on the total number
of facilities that can be opened. The uncertainty in the problem pertains to the
demand values at various locations in the second stage of the problem, which
are sampled from a ﬁnite distribution. Once the demand is realized, the second
stage consists in deciding (i) if a given open facility should serve the demand in a
location (binary decisions) (ii) if a facility serves the demand in a location, then
what quantity of demand is to be served (continuous decisions). These decisions
have to ensure that the demand and supply constraints are met. The problem
formulation is presented formally in Appendix A.1.

4.2 Data generation

Generate instances. We generate 50K instances of S-CFLP, with 10 facilities, 10
clients and 50 scenarios. We provide details on how the data for these instances
are generated in Appendix A.2. The generated instances are solved using Gurobi,
running 2 threads, to at most 2% gap or 10 min time limit.

Next, we compute an RS for each of the 50K instances. As stated earlier, one
could solve mixed-integer bilevel programs (4) to obtain the RSs. However, due
to the computational burden, we use heuristics that work using the knowledge of
the (nearly) optimal solutions to the 2SIP already obtained from Gurobi. These
heuristics are detailed in Appendix A.3. Out of 50K instances, they ﬁnd an RS
for 49,290 instances. We believe that a more thorough search will enable us to
ﬁnd the RS for all the problems.

4.3 Learning algorithm

We formulate the task of predicting the RS as a regression task. The size of the
dataset, which comprises of instances and their corresponding RS, is 49,290. The
dataset is split into training and test sets of size 45K and 4,290, respectively.
Further, a validation set of 5K is carved out from the training set. We use
linear regression (LR) and artiﬁcial neural network (ANN) to minimize the MSE
between the true and predicted RS.

6

Y. Bengio et al.

Feature engineering. It is well known that features describing the connection
between variables, constraints and other interaction help ML to perform well
rather than just providing plain data matrices [3, 5, 8, 12]. In this spirit, along
with the ﬁxed and variable costs to open facilities at diﬀerent locations, we
also provide aggregated features on the set of scenarios. These features give
information about each of the potential locations for facilities in S-CFLP as well
as the way diﬀerent locations interact through the demands in adjacent nodes.
A detailed set of the features is given in Appendix A.4.

4.4 Comparison

In order to evaluate the ML-based prediction of ξ(cid:63), which we refer to as ˆξ(cid:63), we
compare the solution obtained by solving the surrogate problem associated with
ˆξ(cid:63) against solutions obtained by various algorithms.

We use LR and ANN to predict ξ(cid:63). We compare these predictions against (i)
Solution obtained using Gurobi by solving (2) (GRB) (ii) a solution obtained by
solving the surrogate associated with the average scenario, namely (cid:80)N
i=1 Ξi/N
(AVG) (iii) a solution obtained by solving the surrogate associated with a ran-
domly chosen scenario from the N choices (RND) (iv) a solution obtained by
solving the surrogate associated with a randomly chosen scenario from the distri-
bution of the scenario predicted by LR (DIST). Note that GRB produces better
solutions (in most cases) than the ML methods, however, taking a signiﬁcantly
longer time. We therefore assess the time it takes GRB to get a solution of
comparable quality to LR and ANN. We refer to these as GRB-L and GRB-A,
respectively.

5 Results

Table 1 reports the objective value diﬀerence ratio deﬁned as ((Obj val by a
method−GRB obj val)/GRB obj val) for each method and Table 2 statistics
on computing times. Before analyzing the results in more detail, we note a key
ﬁnding that emerges. Namely, LR and ANN perform almost as good as GRB
(in terms of quality of the objective value) in a fraction of the time taken by
GRB. Figure 1 captures the trade oﬀ between the quality of solutions obtained
by diﬀerent methods as well as the time taken to obtain these solutions.

We observe from Table 1 that LR and ANN produce decisions that are as
good as GRB ones on an average (and by the median value), and in some cases
the ML-based methods even beat GRB, i.e., produce solutions whose objective
value is better than that of GRB. This is possible because GRB does not neces-
sarily solve the problem to optimality, but only up to a gap of 2%. Further, even
in the worst of the 4,290 test cases, LR is at most 2.64% away from GRB. To
show that this is not easily achieved, we also compare GRB against AVG, RND
and DIST. We observe that these methods perform much poorer than GRB,
unlike LR and ANN.

Learning based SIP

7

Fig. 1: Objective value diﬀerence ratio vs. avg time in seconds

Table 1: Objective value diﬀerence ratio, GRB vs. the ﬁve other methods (in %)

GRB vs. AVG RND DIST
-0.17
49.87
5.41
3.54
5.39

Min
Max
Avg.
Median
Std. dev.

-0.33
94.42
12.10
8.24
12.11

3.36
14.23
8.10
8.08
1.59

LR ANN
-0.45
7.85
1.02
0.90
0.70

-0.62
2.64
0.64
0.60
0.41

Table 2: Statistics on computing times of the diﬀerent methods

Method GRB AVG RND DIST GRB-L GRB-A LR

Min
Max
Avg.
Median
Std. dev.

0.4354
559.32
8.7713
2.2621
17.919

0.0041
0.0077
0.0043
0.0043
0.0001

0.0041
0.0081
0.0045
0.0044
0.0003

0.0040
0.0065
0.0043
0.0042
0.0001

0.4049
140.43
4.2650
1.4613
8.5576

0.4268
140.43
3.0398
1.2898
6.8362

0.0194
0.0454
0.0206
0.0200
0.0019

ANN
0.0197
0.0457
0.0209
0.0204
0.0019

Analyzing the time improvement in using LR and ANN, we observe from
Table 2 that these methods solve the S-CFLP orders of magnitude faster than
GRB. Indeed, GRB takes over 8 seconds on an average to solve these problems,
while the maximum time is 0.046 seconds using LR and ANN. We emphasize
that the time taken to solve using the ML methods includes the time elapsed
in computing the values of the features used in ML and the time elapsed in
solving the surrogate associated with the corresponding ˆξ(cid:63). Recall that GRB-L

8

Y. Bengio et al.

and GRB-A denote the time it takes GRB to produce a solution of comparable
quality to LR and ANN. The results show that GRB cannot produce a solution
of the same quality as LR and ANN in a comparable time. In fact, LR and ANN
are still orders of magnitude faster than GRB.

6 Discussion

In this paper, we present an algorithm to solve 2SIP using ML-based meth-
ods. The method hinges on the existence of the RS conjectured in Section 3.1.
Computationally, we see that the methods proposed in this paper consistently
provide good quality solutions to S-CFLP orders of magnitude faster.

An important observation we had while training the models is that we were
never able to get the training loss close to zero. Naturally, the predicted RS in
the test dataset were quite diﬀerent from the RS estimated using our heuristics.
The diﬀerences in the predicted values of the components of RS and those ob-
tained using the heuristics are documented by Figure 2 in the Appendix. Despite
this, the solution value to the 2SIP as determined by our algorithm were near
optimal as shown in the results and signiﬁcantly better than those obtained with
other methods. The mismatch between the ML metrics and those characterizing
discrete optimization problems is a known issue [3] requiring extensive research
and, in our context, we believe that exploring this avenue might produce better
solutions.

Another interesting observation is that LR beats ANN in this task. We sus-
pect that this is partly caused by the parsimony oﬀered by LR. However, this
is also encouraging news that the sample complexity of the learning task might
be relatively small in general. We believe that a natural extension to this work
is to provide these analyses more formally.

Further, we believe that computational tests assessing the performance of the
algorithms in diﬀerent datasets of 2SIP is crucial to show how much and where
our method generalizes. This might involve learning solutions to other forms of
2SIP like the stochastic unit-commitment problem, the stochastic supply chain-
network design problem etc. These are cases where we believe that Conjecture 1
still holds, but we do not have computational validation.

Finally, we would also be interested in extending the theory side when Con-
jecture 1 is not expected to hold at all or holds only with weaker guarantees;
for example, in the case where both the stages are mixed-integer nonlinear pro-
gramming (MINLP) problems. In such cases, it will be useful to understand the
reach of ML-based solution techniques as opposed to traditional MINLP solvers.

Bibliography

[1] Shabbir Ahmed. A scenario decomposition algorithm for 0–1 stochastic

programs. Operations Research Letters, 41(6):565–569, 2013.

[2] Shabbir Ahmed, Mohit Tawarmalani, and Nikolaos V Sahinidis. A ﬁ-
nite branch-and-bound algorithm for two-stage stochastic integer programs.
Mathematical Programming, 100(2):355–377, 2004.

[3] Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for
combinatorial optimization: a methodological tour d’horizon. arXiv preprint
arXiv:1811.06128, 2018.

[4] John R Birge and Francois Louveaux. Introduction to stochastic program-

ming. Springer Science & Business Media, 2011.

[5] Pierre Bonami, Andrea Lodi, and Giulia Zarpellon. Learning a classiﬁcation
of mixed-integer quadratic programming problems. In International Confer-
ence on the Integration of Constraint Programming, Artiﬁcial Intelligence,
and Operations Research, pages 595–604. Springer, 2018.

[6] Claus C Carøe and Jørgen Tind. L-shaped decomposition of two-stage
stochastic programs with integer recourse. Mathematical Programming, 83
(1-3):451–464, 1998.

[7] Jitka Dupaˇcov´a, Nicole Gr¨owe-Kuska, and Werner R¨omisch. Scenario reduc-
tion in stochastic programming. Mathematical programming, 95(3):493–511,
2003.

[8] Maxime Gasse, Didier Ch´etelat, Nicola Ferroni, Laurent Charlin, and An-
drea Lodi. Exact combinatorial optimization with graph convolutional neu-
ral networks. arXiv preprint arXiv:1906.01629, 2019.

[9] Ambros Gleixner, Michael Bastubbe, Leon Eiﬂer, Tristan Gally, Ger-
ald Gamrath, Robert Lion Gottwald, Gregor Hendel, Christopher Hojny,
Thorsten Koch, Marco E. L¨ubbecke, Stephen J. Maher, Matthias Mil-
tenberger, Benjamin M¨uller, Marc E. Pfetsch, Christian Puchert, Daniel
Rehfeldt, Franziska Schl¨osser, Christoph Schubert, Felipe Serrano, Yuji
Shinano, Jan Merlin Viernickel, Matthias Walter, Fabian Wegscheider,
Jonas T. Witt, and Jakob Witzig. The SCIP Optimization Suite 6.0.
Technical report, Optimization Online, July 2018. URL http://www.
optimization-online.org/DB_HTML/2018/07/6692.html.

[10] Ambros Gleixner, Michael Bastubbe, Leon Eiﬂer, Tristan Gally, Ger-
ald Gamrath, Robert Lion Gottwald, Gregor Hendel, Christopher Hojny,
Thorsten Koch, Marco E. L¨ubbecke, Stephen J. Maher, Matthias Mil-
tenberger, Benjamin M¨uller, Marc E. Pfetsch, Christian Puchert, Daniel
Rehfeldt, Franziska Schl¨osser, Christoph Schubert, Felipe Serrano, Yuji Shi-
nano, Jan Merlin Viernickel, Matthias Walter, Fabian Wegscheider, Jonas T.
Witt, and Jakob Witzig. The SCIP Optimization Suite 6.0. ZIB-Report 18-
26, Zuse Institute Berlin, July 2018. URL http://nbn-resolving.de/urn:
nbn:de:0297-zib-69361.

10

Y. Bengio et al.

[11] Peter Kall, Stein W Wallace, and Peter Kall. Stochastic programming.

Springer, 1994.

[12] Elias Boutros Khalil, Pierre Le Bodic, Le Song, George Nemhauser, and
Bistra Dilkina. Learning to branch in mixed integer programming.
In
Thirtieth AAAI Conference on Artiﬁcial Intelligence, 2016.

[13] Jeﬀ Linderoth, Alexander Shapiro, and Stephen Wright. The empirical
behavior of sampling methods for stochastic programming. Annals of Op-
erations Research, 142(1):215–241, 2006.

[14] Fran¸cois V Louveaux and Dominique Peeters. A dual-based procedure for
stochastic facility location. Operations research, 40(3):564–573, 1992.

[15] Guglielmo Lulli and Suvrajeet Sen. A branch-and-price algorithm for multi-
stage stochastic integer programming with application to stochastic batch-
sizing problems. Management Science, 50(6):786–796, 2004.

[16] Vinod Nair, Dj Dvijotham, Iain Dunning, and Oriol Vinyals. Learning fast
optimizers for contextual stochastic integer programs. In UAI, pages 591–
600, 2018.

[17] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander
Shapiro. Robust stochastic approximation approach to stochastic program-
ming. SIAM Journal on optimization, 19(4):1574–1609, 2009.

[18] Warren B Powell and Stephan Meisel. Tutorial on stochastic optimization in
energypart i: Modeling and policies. IEEE Transactions on Power Systems,
31(2):1459–1467, 2015.

[19] Warren B Powell and Stephan Meisel. Tutorial on stochastic optimization in
energypart ii: An energy storage illustration. IEEE Transactions on Power
Systems, 31(2):1468–1475, 2015.

[20] Andr´as Pr´ekopa. Stochastic programming, volume 324. Springer Science &

Business Media, 2013.

[21] Tjendera Santoso, Shabbir Ahmed, Marc Goetschalckx, and Alexander
Shapiro. A stochastic programming approach for supply chain network
design under uncertainty. European Journal of Operational Research, 167
(1):96–115, 2005.

[22] Suvrajeet Sen. Stochastic mixed-integer programming algorithms: Beyond
benders’ decomposition. Wiley Encyclopedia of Operations Research and
Management Science, 2010.

[23] Suvrajeet Sen and Julia L Higle. The c 3 theorem and a d 2 algorithm
for large scale stochastic mixed-integer programming: set convexiﬁcation.
Mathematical Programming, 104(1):1–20, 2005.

[24] Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczy´nski. Lectures

on stochastic programming: modeling and theory. SIAM, 2009.

Learning based SIP

11

A Computational test details

A.1 Problem Formulation

We provide below the problem considered in this work for computational study.

min
b∈{0,1}n,v∈Rn
+

n
(cid:88)

(cid:16)

cf
i bi + cv

i vi

(cid:17)

+ Eξ(Q(x, ξ))

i=1

n
10

≤

n
(cid:88)

i=1

bi ≤

3n
4

vi ≤ M bi

where Q(x, ξ) is

min
u∈{0,1}n×n,y∈Rn×n

+

n
(cid:88)

n
(cid:88)

ctv
ij yij +

n
(cid:88)

n
(cid:88)

ctf
ij uij

i=1
n
(cid:88)

j=1
n
(cid:88)

j=1

i=1

j=1

yij ≤ vi

yij = dj(ξ)

i=1
yij ≤ uijM

(5a)

(5b)

(5c)

(5d)

(5e)

(5f)

(5g)

In this problem, we minimize the ﬁxed and variable costs of opening a facility
along with the ﬁxed and variable costs of transportation between the facilities
and the demand nodes. There are n potential locations where a facility could
be opened. A ﬁxed cost of cf
is incurred, if a facility is opened in location i,
i
and a variable cost of cv
i is incurred per-unit capacity of the facility opened in
location i. The binary variable, bi tracks if a facility is opened in location i and
the continuous variable vi indicates the size of the facility at location i. The
constraint in (5c), along with the binary constraint on b ensures that the costs
are incurred in the right way. Finally, (5b) is a complicating constraint, which
says that at least a tenth of the locations must have a facility open and not more
than three-quarters of the locations must have a facility open.

In the second stage, ctf

ij is the ﬁxed cost incurred in transporting from location
i to j; ctv
ij is the per-unit variable cost incurred in transporting from i to j. The
binary variable uij denotes if any transportation happens from i to j and the
continuous variable yij denotes the actual quantity transported from i to j.
The second-stage objective in (5d) minimizes the transportation cost incurred
under a random demand scenario parameterized by ξ. Then, (5e) ensures that
the total quantity transported out of a facility is not greater than the capacity
of the facility, while (5f) ensure that the total quantity supplied to a location j
equals the (random) demand at j. Finally, constraints (5g) link u and y variables
appropriately.

12

Y. Bengio et al.

Algorithm 1: GenerateXiHat

Data: 2SIP P with objective value oe and ﬁrst-stage solution xe, max iterations

iter

Result: ξ(cid:63) or NULL
(cid:80)|Ξ|

1 ¯ξ ← 1
|Ξ|
2 while iter do

i=1 ξi;

3

4

5

6

7

8

Formulate surrogate problem P
Solve P
o ¯ξ = Φ( ¯ξ) if o ¯ξ ≤ c ∗ oe then

(cid:48)

and extract ﬁrst-stage solution x ¯ξ;

(cid:48)

using ¯ξ;

return ¯ξ;

else

Perturb ¯ξ using heuristics based on xe and x ¯ξ

9 return NULL

A.2 Data generation

Instance generation. We generate 50K instances of S-CFLP, with 10 facilities, 10
clients and 50 scenarios. The random parameters cf , cv and Ξ = [ξ1, . . . , ξ50] vary
across instances, where as ctf and ctv are ﬁxed across all instances. Moreover,
cf and cv are sampled from a discrete uniform distribution [15, 20) and [5,
10), respectively. The demand matrix Ξ is generated by ﬁrst evaluating λ =
(cid:4)(cf + 10 ∗ cv)/
n(cid:5). The ith demand scenario (1 ≤ i ≤ 50) is generated by
sampling from a Poisson distribution with the mean equal to λ.

√

The generated instances are solved using Gurobi, running 2 threads, to opti-
mality (less than 2% gap) or 10 min time limit. We store the objective value, gap
closed and master solution x∗ = (b∗, v∗). We were able to solve all the instances
up to the speciﬁed gap, within the speciﬁed time limit.

We follow Algorithm 1 for generating ξ(cid:63). Then, |Ξ| refers to the cardinality
of Ξ and c = 1.01 in step 6. The heuristics for updating the RS, based on xe
and x ¯ξ, are described in Appendix A.3.

A.3 Heuristics
Let x∗ = (b∗, v∗) and x ¯ξ = (b ¯ξ, v ¯ξ) be the ﬁrst-stage optimal and surrogate solu-
tion associated with the scenario ¯ξ, respectively. There are three heuristics that
we use in tandem to generate ξ(cid:63). The ﬁrst heuristic is based on the comparison
of facilities being open or close in the optimal and surrogate solution. The de-
mand in the ¯ξ is zeroed out at clients for which the b∗ is close and b ¯ξ is open, as
suggested by

¯ξ
i = 1 =⇒ ¯ξi = 0 i = 1, . . . , n.
b∗
i = 0 ∧ b

(6)

The remaining two heuristics are based on the comparison of capacities installed
in facilities in the optimal and the surrogate solution. First, the client with max-
imum absolute diﬀerence between capacities installed in optimal and surrogate

Learning based SIP

13

Fig. 2: Demand histogram for diﬀerent methods.

solution is identiﬁed i.e., argmaxi |v∗
is updated either by a ﬁxed percentage p of the current demand

i − v

¯ξ
i |. The demand at this client in the ¯ξ

¯ξi = ¯ξi +

v∗
i − v
|v∗
i − v

¯ξ
i
¯ξ
i |

× p ¯ξi,

(7)

or by a fraction f of the diﬀerence between the capacities installed in optimal
and surrogate solutions

¯ξi = ¯ξi + (v∗

i − v

¯ξ
i )f ¯ξi.

(8)

A.4 Feature engineering

The inputs of the models are cf , cv and Ξ. We do not provide ctf and ctv in
the input as they are ﬁxed across all instances. We do feature engineering on
Ξ, instead of providing it as a raw input, to extract information which can be
useful in predicting ξ(cid:63). Let Ξ be an m × n matrix, where m is the number
of scenarios and n is the number of clients. We calculate minimum, maximum,
average, standard deviation, median, 75th quantile, and 25th quantile of Ξ[:,i]
(ith column of Ξ) for i = 1, . . . , n.

We also ﬁnd the percentage of scenarios in which some fraction of demand

for a client is greater than and less than the demand on all the other nodes

c ∗ Ξ[:,i] ≥ Ξ[:,(cid:54)=i]
m

and

c ∗ Ξ[:,i] ≤ Ξ[:,(cid:54)=i]
m

.

We set c to diﬀerent values (0.9, 1, 1.1, 1.2 and 1.5) and we thus end up with an
input vector of size 190, combining cf , cv and features extracted from Ξ. The
feature extraction performs an aggregation over the number of scenarios.

