0
2
0
2

p
e
S
1

]

G
L
.
s
c
[

1
v
0
9
6
0
0
.
9
0
0
2
:
v
i
X
r
a

Improved Bilevel Model: Fast and Optimal
Algorithm with Theoretical Guarantee

Junyi Li
University of Pittsburgh
junyili.ai@gmail.com

Bin Gu
Nanjing University of Information Science & Technology
jsgubin@gmail.com

Heng Huang
University of Pittsburgh & JD Finance America Corporation
henghuanghh@gmail.com

Abstract

Due to the hierarchical structure of many machine learning problems, bilevel
programming is becoming more and more important recently, however, the com-
plicated correlation between the inner and outer problem makes it extremely
challenging to solve. Although several intuitive algorithms based on the automatic
differentiation have been proposed and obtained success in some applications, not
much attention has been paid to ﬁnding the optimal formulation of the bilevel
model. Whether there exists a better formulation is still an open problem. In
this paper, we propose an improved bilevel model which converges faster and
better compared to the current formulation. We provide theoretical guarantee and
evaluation results over two tasks: Data Hyper-Cleaning and Hyper Representation
Learning. The empirical results show that our model outperforms the current
bilevel model with a great margin. This is a concurrent work with Liu et al. [20]
and we submitted to ICML 2020. Now we put it on the arxiv for record.

1

Introduction

Bilevel programming [34] deﬁnes a type of optimization where one problem is nested into an-
other problem. Many machine learning problems can be united under the framework of bilevel
programming, for example, hyper-parameter optimization [27, 3, 17, 22, 2], few-shot learn-
ing [8, 14, 33, 30, 24] and the generative adversarial network [12, 1, 13, 4, 28]. As a result, the bilevel
programming and its applications in machine learning [23, 22, 9, 2, 21] have drawn more and more
attention recently, however, bilevel programming is much more challenging than classical optimiza-
tion problems due to the complicated correlation between the outer problem and the inner problem,
whose difﬁculty is compounded in the high-dimensional domain. Since the explicit optimal solution
is usually not available, some intuitive approximate gradient-based methods have been proposed,
such as reversible learning [23], the forward-HG and reverse-HG [9], truncated back-propagation
[31]. These methods acquire impressive results when applied in the hyper-parameter optimization
[27, 9, 31] and few shot learning [10]. Despite the advance in algorithms and applications, not much
attention has been paid to the formulation of the bilevel model, and whether there exists a better
formulation that can lead to better results and faster convergence is still an open problem.

In this paper, we focus on improving the current formulation of the bilevel model. Note in the
process of solving a bilevel programming problem (the exact form will be introduced shortly), the
ﬁrst step is to solve the inner problem, however, the optimal way to solve it is still an open problem.
In the current formulation, it simply states to choose the minimizer of the inner problem. In fact,
this is both sub-optimal and unrealistic. On the one hand, the outer problem is the optimization

Preprint. Under review.

 
 
 
 
 
 
objective, therefore, optimizing the inner problem solely for its own sake is sub-optimal from the
outer problem’s perspective, on the other hand, it is impossible to get the "optimum" if the inner
problem has multiple equally good local minimizers. So we should come up with a better way to
solve the inner problem, and a natural choice is to regularize the inner problem with the outer problem,
more speciﬁcally, we want the inner problem to converge to a point that is not only optimal for the
inner problem itself but also is good for the outer problem. Intuitively, this can ﬁlter out the "bad"
values of the inner variable and therefore accelerate the convergence of the outer problem. We will
elaborate more of the idea in the subsequent sections.

The main contributions of this paper are summarized as follows:

(i) We propose an improved bilevel model with better and faster convergence and an efﬁcient

algorithm to solve this new model;

(ii) We prove the convergence of the proposed algorithm and also provide a lower bound result

showing the superior performance of our model;

(iii) We conduct extensive experiments and ablation studies over different tasks and data-sets to

demonstrate the effectiveness of our improved bilevel model.

Organization. The remaining part of this paper is organized as follows: in Section 2, we introduce
the related work including the current bilevel model and commonly used algorithms solving it; in
Section 3, we formally deﬁne our improved bilevel model and also propose an efﬁcient algorithm
to solve this model; in Section 4, we present the theoretical analysis of the proposed model and the
algorithm; in Section 5, we show experiments and ablation studies and make discussions; in Section 6,
we summarize the paper and discuss future directions.

In this paper, we denote model parameters with Greek lowercase letters (such as λ),
Notations.
vector space with Greek uppercase letters (such as Λ) and real valued functions with Latin lowercase
letters (such as f and g). Moreover, we use ∇ to denote the gradient of a function, while for partial
derivatives, we either use ∇k to represent the partial derivative with respect to the kth variable, or
∇λ to represent the partial derivative with respect to the variable λ. Higher order derivatives follow
similar rules. Finally, we use [N ] to denote the sequence of integers from 1 to N, and || • || the
euclidean norm for vectors or (cid:96)2-norm for matrices.

2 Related Work

In the mathematics and optimization literature, bilevel programming dates back to at least the 1960s
when Willoughby [34] proposes a regularization method to solve the following problem:

min
ω∈Ω∗

g(ω) s.t. Ω∗ = {ω|ω ∈ arg min

h(ω)}

ω∈Ω

(1)

This problem is called "bilevel" as the constraint itself is an optimization problem. It has been
extensively investigated since then [38, 7, 32, 29, 36]. Particularly, Sabach and Shtern [29] proposes
a ﬁrst order method called BiG-SAM to solve Eq. (1), which allows the bilevel programming to be
solved as like a "single level" optimization problem. The algorithmic details of the BiG-SAM and
other methods can be found in Appendix A.

Recently, there is a rise of interest in the bilevel programming in the machine learning ﬁeld due
to its potential application in many problems. In machine learning, we study a more complicated
formulation involved with two sets of variables [5, 11]:

min
λ∈Λ,ω∈Ω

g(ω, λ) s.t. ω = arg min

h(ω, λ)

ω∈Ω

(2)

For the ease of reference, we will refer to ω as the inner variable and h(ω, λ) as the inner problem,
similarly refer to λ as the outer variable and g(ω, λ) as the outer problem. What’s more, we will
refer to Eq. (2) as the basic bilevel model in contrast to our model in the subsequent discussions.
The best-known methods for solving the basic bilevel model are gradient-based methods [9, 31, 10],
which gain popularity because of its simplicity and scalability. These methods typically apply back-
propagation through time and automatic differentiation to compute the gradients. Based on how

2

the gradients are stored and calculated, they can be divided into the forward-mode methods or the
reverse-mode methods [9]. Roughly speaking, the reverse-mode runs faster, while the forward-mode
consumes less storage. Based on if the back-propagation path are fully included, they can be divided
into truncated or full methods [31]. More introduction of these methods can be found in Appendix A.

3

Improved Bilevel Model

3.1 New Bilevel Programming Formulation

Most previous research work focuses on proposing a better algorithm to solve the basic bilevel model
Eq. (2), however, we instead focus on ﬁnding a new formulation of bilevel model which has better
and faster convergence than Eq. (2), though we do introduce an efﬁcient algorithm for our improved
formulation in Section 3.2. To the best of our knowledge, we are the ﬁrst to explicitly address this
improved formulation, instead of the original one. More precisely, we propose to solve the following
model:

min
λ∈Λ,ω∈Ω

g(ω, λ) s.t. ω = arg min

ω∈Ω∗
λ

g(ω, λ) where Ω∗

λ = {ω, ω ∈ arg min

ω∈Ω

h(ω, λ)}

(3)

Compared to the formulation in Eq. (2), the outer problem g(ω, λ) shows twice in our model. One is
in the objective which is the same as in Eq. (2), while the other is shown in the constraints, which
differs from the original formulation and is the key innovation of our model. We select ω in the set
Ω∗
λ that achieves the lowest outer objective value, in other words, we encourage the model to explore
the "good" ω and avoid the "bad" ones.

The intuition behind this design is as follows: With the regularization, the outer problem narrows
down its search space to where the outer objective value is small. Naturally, this smaller and better
search space will accelerate the convergence. From another perspective, this leads to more efﬁcient
use of the outer problem’s information. In the original basic formulation, the signal of the outer
problem is back-propagated only if the inner problem is (approximately) solved. In contrast, the outer
problem passes information frequently during the inner problem solving process in our model. Not
surprisingly, our model will converge faster and better compared to the original one. The theoretical
analysis and empirical results will be presented in the subsequent sections.

3.2 Proposed Algorithms

In this section, we present an efﬁcient gradient-based algorithm to solve Eq. (3). As shown in
Algorithm 2, we apply the gradient descent step (Line 10) to update the outer variable λ within each
iteration, where G is an estimation of the hyper-gradient with respect to λ. There are two phases to
calculate G. Firstly, we solve the inner problem (Line 4), then we apply automatic differentiation to
compute the gradient (Line 5 - 9). In the ﬁrst phase, we solve the following inner problem:

arg min
ω∈Ω∗
λ

g(ω, λ) where Ω∗

λ = {ω, ω ∈ arg min

ω∈Ω

h(ω, λ)}

(4)

. In fact, Eq. (4) degenerates to Eq. (1) when λ is ﬁxed, so it can be solved efﬁciently with a ﬁrst
order method such as the BiG-SAM, we adopt BiG-SAM [29] as it is both simple to implement and
achieves SOTA convergence rate. The procedure to solve the inner problem is in Algorithm 1. Note
that we evaluate the gradient with respect to the outer function (Line 6 of Algorithm 1) within each
BiG-SAM step to regularize the inner problem, what’s more, an interesting point deserves to mention
is that Algorithm 1 degenerates to a gradient descent optimizer when we set αk as 1 and we can use
it to solve the inner problem of the basic bilevel model. We will perform an ablation study later to
study this phenomenon.

3

Algorithm 1 Optimization of inner problem
1: Input: Initial inner variable ω0; current outer
variable λ; number of iterations K; hyper-
parameters t, s and α;

η

2: Output: Optimal inner variable ˆωλ;
3: for k = 0 → K do
(BiG-SAM step)
4:
θk+1 = ωk − t∇1h(ωk, λ)
5:
φk+1 = ωk − s∇1g(ωk, λ)
6:
ωk+1 = αk+1θk+1 + (1 − αk+1)φk+1
7:
8: end for
9: Return ˆωλ = ωK

Algorithm 2 Optimization of outer problem (main
algorithm)
1: Input: Initial outer variable λ0, learning rate

2: Output: Optimal outer variable λT ;
3: for t = 1 to T - 1 do
4:

Initialize inner variable ω0 and invoke Al-
gorithm 1 to get ˆωλ (solve inner problem);
Let α = ∇1g(ˆωλ, λt), G = ∇2g(ˆωλ, λt)
for k = K - 1 → 1 do

5:
6:
7:
8:
9:
10:
11: end for
12: Return λT

G = G + α∇2Φ(ωk+1, λt)
α = α∇1Φ(ωk+1, λt)

end for
λt+1 = λt − ηG

In the next phase, we evaluate hyper-gradient with automatic differentiation. Suppose we solve the
inner problem with K steps, then its dynamics is formally written as:

ˆωλ = ωK, ωk+1 = Φ(ωk, λ), k ∈ [K]

(5)

where Φ(ωk, λ) denotes the dynamics of BiG-SAM step. A procedure to calculate the hyper-gradient
based on the reverse mode is shown in Line 5 - 9 of Algorithm 2 (the derivation of this can be found in
Appendix B.1). The convergence property of Algorithm 2 will be shown in Theorem 1 of Section 4.

Complexity Analysis. Now we analyze the time complexity of our algorithm. For simplicity, we
analyze the time complexity of calculating hyper-gradient in terms of the inner iterations K (the
number of iterations in Algorithm 1). In our algorithm, calculation of hyper-gradient is divided into
two phases. In phase 1, we calculate ˆωλ in Algorithm 1, which has O(K) complexity. In phase 2, we
apply automatic differentiation. It is well-known that the complexity of calculating derivatives with
automatic differentiation has the same time complexity as of the function evaluation, which can also
be veriﬁed from the step 5 - 9 in Algorithm 2. So the overall time complexity is O(K). As a result,
we maintain the same O(K) time complexity as those gradient-based methods (see Appendix A for a
reference of these methods) used for solving the basic bilevel model, though we may have a larger
const factor, as we need to extraly evaluate ∇1g(ωk, λ) in every BiG-SAM step, however, we believe
this is acceptable compared to the merits of our model. In fact, we can avoid calculating ∇1g(ωk, λ)
to save computation by setting αk as 1. We will elaborate more about this in the ablation study.

4 Theoretical Analysis

In this section, we introduce two important properties of our model. In Theorem 1, we study the
convergence property of Algorithm 2. Next in Theorem 2, we show that the minimum of our model
is guaranteed to be less than that of the basic model, this result directly shows the advantage of our
model in optimizing the outer objective. The necessary assumptions made in the proof is summarized
as follows:

(i) Λ ⊂ Rm is a compact set;
(ii) Ω ⊂ Rn is a compact set;
(iii) g(ω, λ) is Lipschitz continuous with constant Lg, Lipschitz differentiable with constant

L∇g, and g(•, λ) is uniformly strongly convex with parameter σg;

(iv) h(ω, λ) is Lipschitz continuous with constant Lh, Lipschitz differentiable with constant
L∇h ; h(•, λ) is convex and ∇2h(•, λ) exists and is bounded below with constant S∇2h when
∇h(•, λ) (cid:54)= 0;

Now we discuss the convergence property of Algorithm 2. For the simplicity of notation, we use f (λ)
to denote g(ωλ, λ) where ωλ denotes the exact solution of the inner prblem. Similarly, we use fK(λ)
to denote g(ˆωλ, λ) where ˆωλ is the output of Algorithm 1. Before getting into the details of the main

4

theorem, we ﬁrstly show the existence of minimizer of f (λ). In fact, due to additional regularization
in the constraint of Eq. (3), f (λ) may be discontinuous, and therefore dose not attain the minimum.
However, Proposition 1 and Lemma 1 show that the continuity property holds under our assumption.
Firstly, Proposition 1 shows a result about the convergence of {Ω∗
λn

}n∈N , more precisely :

Proposition 1. let (λn)n∈N be a sequence converges to λ, with the above assumptions (i) - (iv) hold,
Ω∗

λ is the Kuratowski limit [16] of {Ω∗
λn

}n∈N , i.e.:

Li
n→∞

Ω∗
λn

Ls
n→∞

Ω∗
λn

where:

= {ω ∈ Ω∗

= {ω ∈ Ω∗

λ|lim sup
n→∞
λ|lim inf
n→∞

d(ω, Ω∗
λn

) = 0} = Ω∗
λ

d(ω, Ω∗
λn

) = 0} = Ω∗
λ

Ω∗

λ = {ω, ω ∈ arg min h(ω, λ)} , Ω∗
λn

= {ω, ω ∈ arg min h(ω, λn)}

(6)

(7)

Proof Sketch. Follow the deﬁnition of Kuratowski convergence, we need to prove the Kuratowski
limits inferior and superior of the sequence agree for every point ω in Ω∗
λ, which we show by proving
lim
n→∞

) exists and equals 0, the detailed proof is shown in Appendix D.

d(ω, Ω∗
λn

With Proposition 1, we prove the the existence of minimizer of f (λ) in the following lemma:
Lemma 1. With the above assumptions (i) - (iv) hold, f (λ) is continuous in Λ and admits a minimizer.

Proof Sketch. To prove the continuity of f (λ), for any λ ∈ Λ, and a sequence (λn)n∈N → λ, we
need to prove f (λn)n∈N → f (λ), which relies on the convergence result in Proposition 1. As for the
existence of the minimizer of f (λ), since Λ is compact and f (λ) is continuous, we can easily get f (λ)
attains a minimum by the extreme value theorem. The detailed proof is shown in the Appendix D.

Now that f (λ) is continuous and attains a minimizer, we can prove the convergence property of
Algorithm 2, where we show by the convergence of fK(λ) to f (λ):
Theorem 1. With the assumptions (i)-(iv) above hold, we have as K → ∞ for all λ ∈ Λ:

(i) min fK(λ) → min f (λ);

(ii) arg min fK(λ) → arg min f (λ);

Proof Sketch. if we can prove fK(λ) → f (λ) uniformly as K → ∞, then the two property is a
direct consequence of the stability of minimum and minimizers in optimization [6, 10]. To prove
the above uniform convergence, we make use of the Lipschitz continuity of g(ω, λ) and the uniform
convergence of ωK → ωλ, A more precise and rigorous proof is shown in Appendix D.

Theorem 1 states that the output of Algorithm 2 will converge to the minimizer of f (λ) in the limit,
which shows the effectiveness of the algorithm. Next we present Theorem 2, which compares the
minimum of our model and that of the basic model:
Theorem 2. Let λc and λh denote the minimizer of Eq. (2) and Eq. (3) respectively. Then the
minimum of Eq. (3) is less than that of Eq. (2), i.e.:

where ωh,λ denotes the solution of inner problem for a ﬁxed λ in Eq. (3) and ωc,λ denotes the solution
of inner problem for a ﬁxed λ in Eq. (2).

g(ωh,λh , λh) ≤ g(ωc,λc, λc)

(8)

Proof. Based on the formulation in Eq. (4), we have:

ωh,λ = arg min

ω∈Ω∗
λ

g(ω, λ)

5

(9)

for each ﬁxed λ and since ωc,λ ∈ Ω∗

λ so we have:

for all λ, naturally we have:

g(ωh,λ, λ) ≤ g(ωc,λ, λ)

g(ωh,λh , λh) = min
λ∈Λ

g(ωh,λ, λ) ≤ min
λ∈Λ

g(ωc,λ, λ) = g(ωc,λc , λc)

which proves the desired result.

(10)

(11)

Note that we use the claim ωc,λ ∈ Ω∗
λ in the above proof. This is based on the fact that algorithms
used to solve Eq. (2) usually ﬁnd a random local minimizer of the inner problem. In contrast, we
choose the "best" minimizer of the inner problem. This is the key that leads to the result stated in
Theorem 2. In fact, there is a great gap between the minimums found by the two models in practice,
which we will show in the experiments shortly.

5 Experimental Results

In this section, we present the experimental results and compare the improved bilevel model with
the basic bilevel model. We test over two tasks, i.e. Data Hyper-Cleaning and Hyper-Representation
Learning. For the basic bilevel model, we apply a gradient-based algorithm similar to our Algorithm 2
where we replace Algorithm 1 with a gradient descent optimizer, the detailed description of the
algorithm used for the basic bilevel model is shown in Appendix B.2. The hyper-parameters of the
experiments for both models are kept the same for fair comparison (hyper-parameter settings for each
experiment are shown in Appendix C.1 and C.2). The code is written with Pytorch [26] and run on a
server with Tesla P40 GPU.

5.1 Data Hyper-Cleaning

The data hyper-cleaning task aims to clean a noisy data-set where the labels of some samples are
corrupted. One way to tackle this problem is to learn a "weight" for each sample and treat samples
with negative "weight" as corrupted samples. To get these weights, we ﬁt a weighted model on the
noisy data-set and tune the weights over a clean validation data-set. This is a bilevel problem as
the weighted model needs to be learned before evaluated over the validation data-set. The precise
mathematical formulation of this method is shown in Appendix C.1. We run experiments over ﬁve
datasets, including MNIST [19], Fashion-MNIST [35], QMNIST [37], CIFAR10 [15] and SVHN [25].
We handcraft the noisy data-set by randomly perturbing the labels of a given percentage of samples
(we denote this percentage as ρ, e.g. ρ = 0.8 represents the label of 80% samples in the training set
are corrupted). The details of data-set construction can also be found in Appendix C.1.

Table 1: Comparison of F1 score between the improved bilevel model and the basic model

F1-score

ρ = 0.9

ρ = 0.8

ρ = 0.6

Improved

Basic

Improved

Basic

Improved

Basic

MNIST
Fashion-MNIST
Q-MNIST
SVHN
CIFAR10

0.618±0.008
0.569±0.005
0.615±0.007
0.404±0.025
0.321±0.014

0.527±0.008
0.519±0.012
0.531±0.010
0.279±0.009
0.289±0.007

0.781±0.014
0.736±0.008
0.780±0.005
0.536±0.010
0.499±0.008

0.721±0.011
0.697±0.005
0.721±0.004
0.443±0.015
0.456±0.011

0.898±0.004
0.851±0.004
0.900±0.006
0.646±0.013
0.649±0.007

0.877±0.005
0.839±0.003
0.873±0.004
0.614±0.010
0.621±0.010

Now we present some experimental results. We use F1-score as the metric of the data cleaning
algorithm. From Table 1, we can see that the F1-scores of our model surpass the basic model over
different data-sets and under different hardness levels with a great margin (We report mean and
variance over ﬁve runs for each experiment). Moreover, the performance gap between our model and
the basic model becomes larger as the task becomes harder (larger ρ). For harder tasks, it is more
important to make use of the clean validation set’s information, therefore, this experiment veriﬁes
furthermore that our model is much better at exploiting the information of the outer problem, which
is an important reason of the superior performance of our model. What’s more, we plot the F1 score
curve of MNIST data-set (ﬁrst row in Table 1) in Fig. 1 as an example, from which we can see our
model converges much faster than the basic model.

6

(a) ρ = 0.6

(b) ρ = 0.8

(c) ρ = 0.9

Figure 1: F1 score curve of MNIST data-set under different noise levels (ρ), larger ρ means harder
data-set.

(a) MNIST

(b) SVHN

Figure 2: Applying Big-SAM step with different frequencies, "improved-k" means applying BiG-
SAM step every k iterations and using gradient descent step otherwise.

Ablation Study. Although our model are mathematically distinct from the basic model, the gradient-
based algorithms used to solve these two models share similarities in many aspects. As mentioned in
Section 3.2, if we set αk to be 1, Algorithm 1 becomes a gradient descent optimizer and could be
used to solve the basic model. We thus design the following experiments: we replace the BiG-SAM
step with ordinary gradient descent step by setting αk as 1 in Algorithm 1 and see how the F1 score
curve changes. Not surprisingly, the performance of our model degrades when the BiG-SAM step is
used less often as shown in Fig. 2, however, our model still outperforms the basic model, e.g., when
our model performs the BiG-SAM step every 20 steps (only ﬁve times if the total steps are 100), our
model still outperforms the basic model slightly. This experiment is a great demonstration of the
effectiveness of our model, it directly shows how the modiﬁed inner problem in our model improves
the model performance. Besides, this can also be used as the trade-off between model performance
and computation overhead. The more frequent we perform BiG-SAM steps, the better the model
performance is, but with more computation overhead, and vice versa.

Finally, we study the relationship between number of inner iterations and model performance. As
shown in Fig. 3, when we train 200 iterations (note that the inner problem is not overﬁtting in this
case), the F1 curve increases much slower and even performs slightly worse compared to that of
using less inner iterations. This could be explained by the long back propagation path and the sparse
use of outer problem’s information, moreover, this also demonstrates that optimizing inner problem
solely is not optimal for the whole bilevel problem.

5.2 Hyper Representation Learning

We now present the experimental results of the hyper representation learning task. Representation
learning aims to learn a universal representational mapping that is useful for a bunch of tasks. One
way to tackle this problem is to phrase it as bilevel programming. i.e. learning the representational
mapping is the outer problem, while ﬁtting a simple (linear) model on top of this mapping in a speciﬁc
task is the inner problem. The precise mathematical formulation can be found in Appendix C.2. For
this task, we benchmark over two data-sets, i.e. Omniglot [18] and MiniImageNet [33]. The detailed
construction of training and validation set is also shown in Appendix C.2.

7

(a) MNIST

(b) SVHN

Figure 3: F1 score curve under different number of inner iterations. Note the unit of X-axis is
log(Time)

(a) 5-way 1-shot

(b) 5-way 5-shot

(c) 20-way 1-shot

(d) 20-way 5-shot

Figure 4: Accuracy curve for the Omniglot Data-set. "basic-k" means basic bielvel model with k
inner iterations, plots of the improved model follow similar rules.

(a) 5 way

(b) 20 way

Figure 5: Accuracy curve for the MiniImageNet Data-set. "classical-k-n" means basic bilevel model
with n-way k-shot, plots of improved model follow similar rules.

In Fig. 4 and Fig. 5, we present the experimental results for Omniglot and MiniImageNet. We test
under various settings such as number of classes (way), number of examples per class (shot) and
number of inner iterations. As shown in ﬁgures, our model achieves better accuracy and converges
faster than the basic bilevel model under all settings. For example, for the 20-way 1-shot case of the
MiniImageNet, our model reaches the accuracy of 30%, but the algorithm based on the basic bilevel

8

model gets only about 26%. What’s more, similar to the cleaning task, our model has the greatest
performance gain under the hardest setting, i.e. the 1-shot case for both data-sets. This phenomenon
demonstrates oncemore the effectiveness of our model.

6 Conclusion

In this paper, we propose an improved bilevel model in place of the current bilevel formulation,
and show both theoretically and empirically the superior performance of our model. We show that
our model converges to a lower optimal value and makes more efﬁcient use of the outer problem’s
information. Moreover, we empirically demonstrate the superior performance of our model over
the data hyper-cleaning task and the hyper representation learning task. As many machine learning
problems have a bilevel structure, our new model is in great potential to be applied to solve many
related applications.

Broader Impact

Bilevel programming could be applied to a wide range of machine learning applications, such as
hyper-parameter optimization, few-shot learning and generative adversarial learning. Our research
focuses on improving the current bilevel formulation. Generally speaking, our new formulation can
lead to a better and faster convergence compared to the current bilevel formulation. As a result, the
applications that apply bilevel programming will beneﬁt from our method. Note that our method is
not based on any bias in the data-set and has not direct ethical consequences, however, our method is
quite general and can be applied to many potential applications, so we encourage researchers to apply
our new formulation to many applications and investigate the various ethical effects of our model.

References

[1] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875,

2017.

[2] A. G. Baydin, R. Cornish, D. M. Rubio, M. Schmidt, and F. Wood. Online learning rate

adaptation with hypergradient descent. arXiv preprint arXiv:1703.04782, 2017.

[3] J. S. Bergstra, R. Bardenet, Y. Bengio, and B. Kégl. Algorithms for hyper-parameter optimiza-

tion. In Advances in neural information processing systems, pages 2546–2554, 2011.

[4] A. Brock, J. Donahue, and K. Simonyan. Large scale gan training for high ﬁdelity natural image

synthesis. arXiv preprint arXiv:1809.11096, 2018.

[5] N. Couellan and W. Wang. On the convergence of stochastic bi-level gradient methods. Opti-

mization, 2016.

[6] A. L. Dontchev and T. Zolezzi. Well-posed optimization problems. Springer, 2006.

[7] M. C. Ferris and O. L. Mangasarian. Finite perturbation of convex programs. Applied Mathe-

matics and Optimization, 23(1):263–273, 1991.

[8] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep
networks. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pages 1126–1135. JMLR. org, 2017.

[9] L. Franceschi, M. Donini, P. Frasconi, and M. Pontil. Forward and reverse gradient-based
hyperparameter optimization. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pages 1165–1173. JMLR. org, 2017.

[10] L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil. Bilevel programming for
hyperparameter optimization and meta-learning. arXiv preprint arXiv:1806.04910, 2018.

[11] S. Ghadimi and M. Wang. Approximation methods for bilevel programming. arXiv preprint

arXiv:1802.02246, 2018.

9

[12] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems,
pages 2672–2680, 2014.

[13] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of
wasserstein gans. In Advances in neural information processing systems, pages 5767–5777,
2017.

[14] G. Koch, R. Zemel, and R. Salakhutdinov. Siamese neural networks for one-shot image

recognition. In ICML deep learning workshop, volume 2. Lille, 2015.

[15] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.

[16] K. Kuratowski. Topology, volume 1. Elsevier, 2014.

[17] A. Lacoste, H. Larochelle, F. Laviolette, and M. Marchand. Sequential model-based ensemble

optimization. arXiv preprint arXiv:1402.0796, 2014.

[18] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning through

probabilistic program induction. Science, 350(6266):1332–1338, 2015.

[19] Y. LeCun, C. Cortes, and C. Burges. Mnist handwritten digit database. ATT Labs [Online].

Available: http://yann. lecun. com/exdb/mnist, 2, 2010.

[20] R. Liu, P. Mu, X. Yuan, S. Zeng, and J. Zhang. A generic ﬁrst-order algorithmic framework for
bi-level programming beyond lower-level singleton. arXiv preprint arXiv:2006.04045, 2020.

[21] J. Lorraine and D. Duvenaud. Stochastic hyperparameter optimization through hypernetworks.

arXiv preprint arXiv:1802.09419, 2018.

[22] J. Luketina, M. Berglund, K. Greff, and T. Raiko. Scalable gradient-based tuning of continuous
regularization hyperparameters. In International conference on machine learning, pages 2952–
2960, 2016.

[23] D. Maclaurin, D. Duvenaud, and R. Adams. Gradient-based hyperparameter optimization
through reversible learning. In International Conference on Machine Learning, pages 2113–
2122, 2015.

[24] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel. A simple neural attentive meta-learner.

arXiv preprint arXiv:1707.03141, 2017.

[25] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural

images with unsupervised feature learning. 2011.

[26] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning
library. In Advances in Neural Information Processing Systems, pages 8024–8035, 2019.

[27] F. Pedregosa. Hyperparameter optimization with approximate gradient. arXiv preprint

arXiv:1602.02355, 2016.

[28] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolu-

tional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.

[29] S. Sabach and S. Shtern. A ﬁrst order method for solving convex bilevel optimization problems.

SIAM Journal on Optimization, 27(2):640–660, 2017.

[30] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap. Meta-learning with
memory-augmented neural networks. In International conference on machine learning, pages
1842–1850, 2016.

[31] A. Shaban, C.-A. Cheng, N. Hatch, and B. Boots. Truncated back-propagation for bilevel

optimization. arXiv preprint arXiv:1810.10667, 2018.

10

[32] M. Solodov. An explicit descent method for bilevel convex optimization. Journal of Convex

Analysis, 14(2):227, 2007.

[33] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al. Matching networks for one shot learning.

In Advances in neural information processing systems, pages 3630–3638, 2016.

[34] R. A. Willoughby. Solutions of ill-posed problems (an tikhonov and vy arsenin). SIAM Review,

21(2):266, 1979.

[35] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking

machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.

[36] H.-K. Xu. Viscosity approximation methods for nonexpansive mappings. Journal of Mathemat-

ical Analysis and Applications, 298(1):279–291, 2004.

[37] C. Yadav and L. Bottou. Cold case: The lost mnist digits. In Advances in Neural Information

Processing Systems, pages 13443–13452, 2019.

[38] I. Yamada, M. Yukawa, and M. Yamagishi. Minimizing the moreau envelope of nonsmooth
convex functions over the ﬁxed point set of certain quasi-nonexpansive mappings. In Fixed-Point
Algorithms for Inverse Problems in Science and Engineering, pages 345–390. Springer, 2011.

A More Related Work

Eq. (1) has been extensively investigated since it is ﬁrstly proposed by Willoughby [34]. More
speciﬁcally, Willoughby [34] proposes to solve the following problem as a proxy of Eq. (1):

min
ω∈Ω

h(ω) + λg(ω)

(12)

where λ is a regularization hyper-parameter; Ferris and Mangasarian [7], Solodov [32] focus on the
regularity conditions of the involved functions and investigate the scenario when h(ω) is the sum
of a smooth function and an indicator function; Yamada et al. [38] focuses on ﬁnding a reasonable
regularization parameter λ and proposes sufﬁcient conditions of λ s.t.
the solution of Eq. (12)
converges to that of Eq. (1); Sabach and Shtern [29] proposes an algorithm to solve Eq. (1) directly
named BiG-SAM with guaranteed convergence rate of O( 1
k ). More speciﬁcally, BiG-SAM is a ﬁrst
order method making use of the Sequential Averaging Method [36] which is originally proposed for
ﬁxed point problems. Its procedure is shown in Algorithm 3.

Algorithm 3 Bilevel Gradient Sequential Averaging Method (BiG-SAM)
1: Input: Initial inner variable ω0; number of inner iterations K; hyper-parameters t, s and α;
2: Output: Optimal solution ω;
3: for k = 0 → K do
4:
5:
6:
7: end for
8: Return ω = ωK

θk+1 = ωk − t∇1h(ωk)
φk+1 = ωk − s∇1g(ωk)
ωk+1 = αk+1θk+1 + (1 − αk+1)φk+1

In machine learning, we study Eq. (2) as introduced in Section 2, for which the best-known methods
are gradient-based methods [9, 31, 10]. Here we present an example of the reverse-mode fully
back-propagated method in Algorithm 4.

11

Algorithm 4 Reverse mode fully back-propagated method
1: Input: Initial outer variable λ0; number of iterations K; hyper-parameters ηo, ηi
2: Output: Optimal outer variable λT ;
3: for m = 1 → T - 1 do
for k = 0 → K do
4:
5:

Update ωk using gradient descent with learning rate ηi (whose dynamics is denoted as
Φ(ωk, λm))

end for
Let ˆωλm = ωK, α = ∇1g(ˆωλm , λm), G = ∇2g(ˆωλm, λm)
for k = K - 1 → 1 do

G = G + α∇2Φ(ωk+1, λm), α = α∇1Φ(ωk+1, λm)

6:
7:
8:
9:
10:
11:
12: end for
13: Return λT

end for
λm+1 = λm − ηG

B Details of Algorithms

In this section, we include more details about the proposed algorithms in Section 3, moreover, we
also present the algorithm we use to solve the basic bilevel model in Section 5, for the purpose of
illustrating the differences between our model and the basic model from an algorithmic perspective.

B.1 Algorithm of the Improved Bilevel Model

As mentioned in Section 3.2, there are two phases within each hyper-iteration. Firstly, we solve the
inner problem using Algorithm 1, next we apply automatic differentiation to compute the hyper-
gradient. For concision, we combine Algorithm 1 and Algorithm 2 together here in Algorithm 5.

Now we derive the procedure to compute hyper-gradient based on automatic-differentiation. Firstly,
we give the precise form of dynamics Φ(ω, λ) and its partial derivatives, more speciﬁcally, the
BiG-SAM step update is:

ωk+1 = ωk − tαk+1∇1h(ωk, λ) − s(1 − αk+1)∇1g(ωk, λ)

(13)

Naturally, its derivatives are deﬁned as follows:

∇1Φ(ωk, λ) = I − tαk+1∇11h(ωk, λ) − s(1 − αk+1)∇11g(ωk, λ)
∇2Φ(ωk, λ) = −tαk+1∇12h(ωk, λ) − s(1 − αk+1)∇12g(ωk, λ)

(14)

where I denotes the unit matrix. Finally, the hyper-gradient can be calculated by combining the above
results and the chain-rule as:

∇g(ˆωλ, λ) = ∇2g(ˆωλ, λ)+




∇1g(ˆωλ, λ) ×





K−2
(cid:88)





K−1
(cid:89)







∇1Φ(ωj, λ)

 ∇2Φ(ωk, λ)

 + ∇2Φ(ωK−1, λ)



k=1

j=k+1

(15)

which corresponds to Line 10 - 13 in Algorithm 5 or Line 5 - 9 in Algorithm 2.

12

Initialize: Inner variable ω0;
for k = 0 → K do

(BiG-SAM step)
θk+1 = ωk − t∇1h(ωk, λm), φk+1 = ωk − s∇1g(ωk, λm)
ωk+1 = αk+1θk+1 + (1 − αk+1)φk+1

Algorithm 5 Algorithm of the improved bilevel model
1: Input: Initial outer variable λ0, number of iterations K, hyper-parameter η, t, s and α;
2: Output: Optimal outer variable λT ;
3: for m = 1 to T - 1 do
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15: end for
16: Return λT

end for
Let ˆωλm = ωK, α = ∇1g(ˆωλm, λm), G = ∇2g(ˆωλm, λm)
for k = K - 1 → 1 do

G = G + α∇2Φ(ωk+1, λm), α = α∇1Φ(ωk+1, λm)

end for
λm+1 = λm − ηG

B.2 Algorithm of the Basic Bilevel Model

Now we introduce the algorithm we use to solve the basic bilevel model. Since the the inner problem
of a basic bilevel model is:

arg min
ω∈Ω

h(ω, λ)

(16)

We solve Eq. (16) with a gradient descent optimizer, while for the calculation of hyper-gradient, we
also apply the automatic differentiation. We summarize the algorithm in Algorithm 6. The dynamics
Φ(ω, λ) of the gradient descent used for solving the inner problem Eq. (16) is:

ωk+1 = ωk − t∇1h(ωk, λ)

(17)

and its derivatives are deﬁned as follows:

∇1Φ(ωk, λ) = I − t∇11h(ωk, λ), ∇2Φ(ωk, λ) = −t∇12h(ωk, λ)

(18)

We can calculate the hyper-gradient of the basic bilevel model easily by substituting Eq. (17) and
Eq. (18) into Eq. (15).

ωk+1 = ωk − t∇1h(ωk, λm)

Initialize: Inner variable ω0;
for k = 0 → K do

Algorithm 6 Algorithm of the basic bilevel model
1: Input: Initial outer variable λ0, number of iterations K, hyper-parameter η, t;
2: Output: Optimal outer variable λT ;
3: for m = 1 to T - 1 do
4:
5:
6:
7:
8:
9:
10:
11:
12:
13: end for
14: Return λT

end for
Let ˆωλm = ωK, α = ∇1g(ˆωλm, λm), G = ∇2g(ˆωλm, λm)
for k = K - 1 → 1 do

G = G + α∇2Φ(ωk+1, λm), α = α∇1Φ(ωk+1, λm)

end for
λm+1 = λm − ηG

13

C Experimental Details

C.1 Data Hyper-Cleaning

In the data hyper-cleaning task, the inner and outer problem are deﬁned as follows:

g(ω, λ) =

Nval(cid:88)

i=1

l(yi,val, xi,val × ω) , h(ω, λ) =

Ntr(cid:88)

i=1

σ(λi) × l(yi,tr, xi,tr × ω) ,

(19)

where {(xi,tr, yi,tr)i∈[Ntr]} denotes the training set and {(xi,val, yi,val)i∈[Nval]} denotes the valida-
tion set. The outer variable {λi}i∈[Ntr] is the un-normalized weights for training samples and σ(•)
represents the sigmoid function, while the inner variable ω is weights of the machine learning model
and l is the loss function, e.g. cross entropy loss. Note that the number of hyper-parameters in this
task is of order O(Ntr), which can be very large depending on the scale of the data-set to be cleaned.

Data construction. We randomly select 5000 samples to construct the training set and corrupt
the label of ρ percentage samples, then we randomly select another 5000 samples to construct the
validation set.

Hyper-parameters. The model we use for this task is based on the ResNet-18 architecture. For
the experiments in Table 1 and Fig. 1, the inner iterations are set 100 and the outer problem is run
until converge. In the improved bilevel model, we set both t and s as 0.001, α as k−0.25, and η as 1;
while in the basic model, we set t as 0.001 and η as 1. The learning rates of both models are kept
the same on propose for fair comparison. In Fig. 2, the hyper-parameters are kept the same other
than setting αk as 1 occasionally. In Fig. 3, we vary the number of inner iterations but keep the other
hyper-parameters the same.

C.2 Hyper Representation Learning

In the hyper representation learning, the inner and outer problem are deﬁned as follows:

g(ω, λ) =

Nt(cid:88)

i=1

l(m(Di,val; λ); ωi) , h(ω, λ) =

Nt(cid:88)

i=1

l(m(Di,tr; λ); ωi) .

(20)

where {Di,tr, Dt,val}i∈[Nt] denotes the tasks used for training. The outer variable λ is the parameters
of the representation mapping m, while the inner variable ωi is the weight of linear model for each
task. Note that λ is shared between tasks and ωi is learned independently for each task.

Data construction. We resize the images (28 by 28 for Omniglot and 64 by 64 for MiniImageNet)
and also create more classes by rotating the images (90◦, 180◦ and 270◦). For each training task Di,
we include 5 or 20 classes with 1 or 5 training samples and 10 validation samples for each class,
which correspond to Di,tr and Di,val respectively. The validation tasks are constructed similarly for
ﬁnal evaluation.

Hyper-parameters. For the mapping m, we apply four convolutional layer with 64 ﬁlters each
for Omniglot related experiments and ResNet-18 for MiniImageNet related experiments, while l is
simply a linear mapping. In Fig. 4, for the improved bilevel model, we set both t and s as 0.01, α as
k−0.25, and η as 0.001; while for the basic model, we set t as 0.01 and η as 0.001. In Fig. 5, for the
improved bilevel model, we set both t and s as 0.001, α as k−0.25, and η as 0.0001; while for the
basic model, we set t as 0.001 and η as 0.0001. The learning rates of both models are kept the same
on propose for fair comparison.

D Proof of Theorems

Proposition 1. let (λn)n∈N be a sequence converges to λ, with the above assumptions (i) - (iv) hold,
Ω∗

λ is the Kuratowski limit [16] of {Ω∗
λn

}n∈N , i.e.:

Li
n→∞

Ω∗
λn

Ls
n→∞

Ω∗
λn

= {ω ∈ Ω∗

= {ω ∈ Ω∗

λ|lim sup
n→∞
λ|lim inf
n→∞

d(ω, Ω∗
λn

) = 0} = Ω∗
λ

d(ω, Ω∗
λn

) = 0} = Ω∗
λ

(21)

14

where:

Ω∗

λ = {ω, ω ∈ arg min h(ω, λ)} , Ω∗
λn

= {ω, ω ∈ arg min h(ω, λn)}

(22)

Proof. Since (λn)n∈N → λ, then for any (cid:15) > 0, there exists N(cid:15) s.t. for any n > N(cid:15):

Then for any point ω ∈ Ω∗

λ, since h is Lipschitz differentiable, we have:

||λn − λ|| <

S∇2h(cid:15)
L∇h

||∇1h(ω, λ) − ∇1h(ω, λn)|| ≤ ||∇h(ω, λ) − ∇h(ω, λn)|| ≤ L∇h × ||(ω, λ) − (ω, λn)||

=L∇h × ||λ − λn|| < L∇h ×

since ω ∈ Ω∗

λ, we have:

S∇2h(cid:15)
L∇h

= S∇2h(cid:15)

combining (24) and (25) we got:

Next denote:

∇1h(ω, λ) = 0

||∇1h(ω, λn)|| < S∇2h(cid:15)

(23)

(24)

(25)

(26)

d(ω, Ω∗
λn

) = inf{d(ω, a), a ∈ Ω∗
}
λn
) < (cid:15) (we can assume d(ω, Ω∗
λn

(27)
) > 0 otherwise, we have

Now we want to prove d(ω, Ω∗
λn
d(ω, Ω∗
λn
Suppose that the above inﬁmum is acquired at ω∗, since that Ω is compact and h(ω, λ) is continuous,
so Ω∗

) = 0 < (cid:15) as needed).

λ is compact, we have ω∗ ∈ Ω∗
λn

, i.e.:

What’s more, by the mean value theorem, there exists ω0 /∈ Ω∗
λn

:

∇1h(ω∗, λn) = 0

(28)

∇1h(ω, λn) = ∇1h(ω, λn) − ∇1h(ω∗, λn) = ∇11h(ω0, λn) × (ω − ω∗)

(29)

, we have ∇1h(ω0, λ) (cid:54)= 0, with assumption iv, ∇11h(ω0, λn) is invertible, so we

since ω0 /∈ Ω∗
λn
have:

then we have:

ω − ω∗ = ∇11h(ω0, λn)−1∇1h(ω, λn)

d(ω, Ω∗
λn

) = ||ω − ω∗|| = ||∇11h(ω0, λn)−1∇1h(ω, λn)||

≤||∇11h(ω0, λn)−1|| × ||∇1h(ω, λn)|| <

1
S∇2h

× S∇2h(cid:15) = (cid:15)

The last inequality is a result of Assumption (iv) and (26), now we get the conclusion that:

therefore, we have the Kuritowski limit inferior:

lim
n→∞

d(ω, Ω∗
λn

) = 0

Li
n→∞

Kn = {ω ∈ Ω∗

d(ω, Ω∗
λn

) = 0} = {ω ∈ Ω∗

λ| lim
n→∞

d(ω, Ω∗
λn

) = 0} = Ω∗
λ

(33)

Similarly, we can get Ls
n→∞

λ, therefore we have:

λ|lim sup
n→∞
Kn = Ω∗

Ω∗
λn

→ Ω∗
λ

15

(34)

(30)

(31)

(32)

Lemma 1. With the above assumptions (i) - (iv) hold, f (λ) is continuous in Λ and admits a minimizer.

Proof. For any λ ∈ Λ, and a sequence (λn)n∈N → λ, since Ω is compact, there exists a subsequence
(kn)n∈N s.t.
→ ˆω, now if we can prove ˆω = ωλ, then by the continuity of g(ω, λ), we
have:

lim
n→∞

ωλkn

lim
n→∞

f (λkn ) = lim
n→∞

g(ωλkn

, λkn ) = g(ωλ, λ) = f (λ)

Now we prove: ˆω = ωλ, for any w ∈ Ω, ﬁrstly we have:

h(ˆω, λ) = lim
n→∞

h(ωλkn

, λkn ) ≤ lim
n→∞

h(ω, λkn ) = h(ω, λ)

(35)

(36)

The ﬁrst equality and last equality is by the continuity of h(ω, λ), and the inequality is because of
ωλkn

, now we have proved ˆω ∈ Ω∗

λ. Furthermore, since:

∈ Ω∗

λkn

ωλkn

= arg min
ω∈Ω∗
λkn

g(ω, λkn )

(37)

and denote IΩ∗
any w ∈ Ω, we have:

λkn

as the indicator function where IΩ∗

= 1 if ω ∈ Ω∗

λkn

and 0 otherwise, then for

λkn

g(ˆω, λ) = lim
n→∞
≤ lim
n→∞

, λkn ) ≤ lim
n→∞

g(ωλkn
g(ω, λkn ) + I lim

g(ω, λkn ) + IΩ∗

λkn

(ω)

Ω∗

λkn

(ω) = g(ω, λ) + IΩ∗

λ

(ω)

n→∞

(38)

The ﬁrst inequality is because of (37), the second inequality is by the deﬁnition of indicator function
and the last equality is because of the convergence result in Proposition 1. with (36) and (38) we
have:

ˆω = arg min

ω∈Ω∗
λ

g(ω, λ) = ωλ

(39)

then by (35) we prove that f (λ) is continuous, and since Λ is compact and f (λ) is continuous, we
can easily get f (λ) attains its minimum by the extreme value theorem, which proves the desired
result.

16

Theorem 1. With the assumptions (i) - (iv) above hold, we have as K → ∞:

1. min fK(λ) → min f (λ);
2. arg min fK(λ) → arg min f (λ);

To prove Theorem 1. we need to cite 2 results (list below as Lemma 2 and Lemma 3).
Lemma 2. (Adapted from proposition 5. in Sabach and Shtern [29] and and Theorem 3.2 from Xu
[36])
Let {θk}, {φk} and {ωk} be sequences generated by BiG-SAM:

(i) the sequence {ωk}k∈N converges to ˆω which satisﬁes:

(cid:104)∇1g(ˆω, λ), ω − ˆω(cid:105) ≥ 0 , ∀ω ∈ Ω∗
λ

(40)

thus ˆω = ωλ

(ii) and the sequence {ωk}k∈N also satisﬁes the following inequality, which leads to the above

result :

||ωk+1 − ωλ|| ≤ (1 − ¯α)||ωk − ωλ|| + ¯α ¯β

(41)

Where ¯α = ¯α(L∇g, σg) and ¯β = ¯β(ωk; L∇g, σg) for sufﬁciently large k;

Lemma 3. (Theorem A.1 in Franceschi et al. [10]) Let φK and φ be lower semi-continuous functions
deﬁned on a compact set Λ. Suppose that φK converges uniformly to φ on Λ as K → ∞. Then:

(i) inf φK → inf φ

(ii) arg min φK → arg min φ

Proof. With the two lemmas above, we come back to the proof of Theorem 1. From Lemma 2.ii, we
know that the convergence of the sequence {ωk}k∈N depends on L∇g, σg, which is independent of
λ, so we have that {ωk}k∈N converges to ωλ uniformly; then since g(ω ,λ) is Lipschitz continuous,
we have:

||fK(λ) − f (λ)|| = ||g(ωK, λ) − g(ωλ, λ)|| ≤ Lg||ωK − ωλ||
(42)
So we have fK(λ) → f (λ) uniformly, what’s more, fK(λ) is continuous based on its deﬁnition and
f (λ) is continuous and attains a minimizer because of Lemma 1, so we satisfy the conditions in
Lemma 3, then we can prove the desired result in Theorem 1 using Lemma 3.

17

