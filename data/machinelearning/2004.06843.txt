rspa.royalsocietypublishing.org

Research

Article submitted to journal

Bayesian differential
programming for robust
systems identiﬁcation under
uncertainty

Yibo Yang1, Mohamed Aziz Bhouri1 and
Paris Perdikaris1

Subject Areas:

Scientiﬁc machine learning, Applied

1 Department of Mechanical Engineering
and Applied Mechanics

Mathematics, Bayesian statistics,

University of Pennsylvania

Data-driven modeling

Philadelphia, PA 19104

Keywords:

Machine learning, Dynamical

systems, Uncertainty quantiﬁcation,

Model discovery

Author for correspondence:

Paris Perdikaris

e-mail: pgp@seas.upenn.edu

This paper presents a machine learning framework
for Bayesian systems identiﬁcation from noisy, sparse
and irregular observations of nonlinear dynamical
systems. The proposed method takes advantage of
recent developments in differentiable programming
to propagate gradient information through ordinary
differential equation solvers and perform Bayesian
inference with respect to unknown model parameters
using Hamiltonian Monte Carlo sampling. This allows
an efﬁcient inference of the posterior distributions
over plausible models with quantiﬁed uncertainty,
while the use of sparsity-promoting priors enables
the discovery of
interpretable and parsimonious
representations for the underlying latent dynamics.
is presented to
A series of numerical
demonstrate the effectiveness of the proposed methods
including nonlinear oscillators, predator-prey systems
and examples from systems biology. Taken all
forth a ﬂexible and
together, our ﬁndings put
robust workﬂow for data-driven model discovery
under uncertainty. All codes and data accompanying
this manuscript are available at https://bit.ly/
34FOJMj.

studies

1. Introduction

In the era of big data, dynamical systems discovery has
received a lot of attention, primarily due to the signiﬁcant
growth of accessible data across different scientiﬁc
disciples,
including systems biology [1], bio-medical
imaging [2], ﬂuid dynamics [3], climate modeling [4], and
physical chemistry [5]. Extracting a set of features that are

c(cid:13) The Authors. Published by the Royal Society under the terms of the

Creative Commons Attribution License http://creativecommons.org/licenses/

by/4.0/, which permits unrestricted use, provided the original author and

source are credited.

0
2
0
2

r
p
A
8
1

]

G
L
.
s
c
[

2
v
3
4
8
6
0
.
4
0
0
2
:
v
i
X
r
a

 
 
 
 
 
 
2

l

r
s
p
a
.
r
o
y
a
s
o
c
e
t
y
p
u
b

i

l
i

i

s
h
n
g
.
o
r
g

P
r
o
c
R
S
o
c
A
0
0
0
0
0
0
0

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

interpretable and predictive is crucial for developing physical insight and gaining a better
understanding of the natural phenomena under study [6]. Perhaps more importantly, this can
enable the reliable forecasting of future states and subsequently lead to effective intervention
strategies for design and control of complex systems [7–9].

Machine learning methods and data-driven modeling techniques have already proven their
utility in solving high-dimensional problems in computer vision [10], natural language processing
[11], etc. Due to their capability of extracting features from high dimensional and multi-
ﬁdelity noisy data [12,13], these methods are also gaining attraction in modeling and simulating
physical and biological systems. The evolution of such systems can be typically characterized
by differential equations, and several techniques have been developed to construct predictive
algorithms that can synergistically combine data and mechanistic prior knowledge. Such scientiﬁc
machine learning approaches are currently employed to distill dynamics from time-series data
[14–21], infer the solution of differential equations [22–27], infer parameters, latent variables and
unknown constitutive laws [3,28–31], as well as tackle forward and inverse problems in complex
application domains including cardiovascular ﬂow dynamics, [32], metamaterials [33], cardiac
electrophysiology [34], etc.

Speciﬁc to systems identiﬁcation, most recent data-driven approaches [14–17,21] heavily
rely on the quality of the observations and are not designed to return predictions with
quantiﬁed uncertainty. For instance, the sparse regression methods put forth in [17,18] can
exhibit unstable behavior if the data is highly noisy and are not able to directly digest time-
series data with irregular sampling frequency or missing values. On the other hand, recent
approaches leveraging differentiable programming [14–16] can support irregularly sampled
data, but are only designed to provide point-estimates for the discovered dynamics with no
characterization of predictive uncertainty. Such lack of robustness and missing capabilities may
limit the use of existing techniques to idealized settings and pose the need for a more ﬂexible
framework that can effectively accommodate noisy, sparse and irregularly sampled data to infer
posterior distributions over plausible models, and subsequently yield robust future forecasts with
quantiﬁed uncertainty.

In this work, we aim to address the aforementioned capability gaps by formulating a fully
Bayesian framework for robust systems identiﬁcation from imperfect time-series data. Our
speciﬁc contributions can be summarized in the following points:

• We leverage recent developments in differentiable programming [14–16] to enable gradient
back-propagation through numerical ODE solvers, and utilize this information to construct
accelerated Hamiltonian Monte Carlo schemes for Bayesian inference.

• The proposed workﬂow is computationally efﬁcient, end-to-end differentiable, and can directly

accommodate sparse, noisy and irregularly sampled time-series data.

• Equipped with sparsity-promoting priors, we can recover interpretable and parsimonious
representations for the latent dynamics, while entire posterior distributions over plausible
models are obtained.

• This probabilistic formulation is key for safe-guarding against erroneous data, incomplete
model parametrizations, as well as for producing reliable future forecasts with quantiﬁed
uncertainty.

• We demonstrate enhanced capabilities and robustness against state-of-the-art methods for

systems identiﬁcation [17,18] across a range of benchmark problems.

Taken all together, our ﬁndings put forth a novel, ﬂexible and robust workﬂow for data-driven
model discovery under uncertainty that can potentially lead to improved algorithms for robust
forecasting, control and model-based reinforcement learning of complex systems.

The rest of this paper is organized as follows. Section 2 presents the proposed method
and its corresponding technical ingredients. In section (b), a review of Bayesian inference and
Hamiltonian Monte Carlo sampling is given. In section (a), we provide details on how the
proposed method can effectively propagate gradient information through ODE solvers leveraging

differential programming. In section (e), we discuss a simple but essential step for pre-processing
the observed data in order to obtain robust training behavior. In section 3 the effectiveness of
the proposed method is tested on a series of numerical studies, including nonlinear oscillators,
predator-prey systems and a realistic example in systems biology. Finally, in section 4 we
summarize our key ﬁndings, discuss the limitations of the proposed approach, and carve out
directions for future investigation.

2. Methods

This section provides a comprehensive overview of the key ingredients that deﬁne our work,
namely differential programming via Neural ordinary differential equations (NeuralODEs) [16]
and Bayesian inference with Hamiltonian Monte Carlo sampling [35]. Our presentation is focused
on describing how these techniques are be interfaced to obtain a novel, efﬁcient, and robust
workﬂow for parsimonious model discovery from imperfect time-series data.

(a) Differentiable programming

In their original work, Chen et. al. [16] introduced a general framework for propagating gradient
information through classical numerical solvers for ordinary differential equations (ODEs) that
blends classical adjoint methods [36] with modern developments in automatic differentiation [37].
To illustrate the main concepts, consider a general dynamical system of the form

3

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

l

r
s
p
a
.
r
o
y
a
s
o
c
e
t
y
p
u
b

i

l
i

i

s
h
n
g
.
o
r
g

.

.

.

.

.

.

.

.

.

.

.

.

P
r
o
c
R
S
o
c
A
0
0
0
0
0
0
0

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

dx
dt

= f (x, t; θ),

.

(2.1)

where x ∈ RD denotes the state space of the D-dimensional dynamical system, and θ is a
vector unknown parameters that parametrizes the latent dynamics f : RD → RD. A systems
identiﬁcation task is now summarized as follows. Given some observations xi, i = 1, ..., n
evaluated at time instances ti, i = 1, ..., n, one would like to learn the θ that best parametrizes the
underlying dynamics. A typical approach for identifying these optimal parameters is to deﬁne a
loss function g that measures the discrepancy between the observed data xi+1 and the model’s
predictions ˆxi+1 for a given θ, i.e.,

J (θ) =

n−1
(cid:88)

i=1

g(xi+1, hθ(x(ti))),

(2.2)

where hθ(x(ti)) = ˆxi+1 is the predicted value under a given set of estimated model parameters θ
obtained by integrating the dynamical system with some ODE solver. g(·) could be any metric to
evaluate the distance / evaluating the discrepancy between the true value and the predicted one
(e.g., L1 loss, L2 loss [38] or KL-divergence [39], Wasserstein distance [40], etc). A sufﬁcient way
to minimize the loss function is through gradient descent [41,42], however appropriate methods
need to be employed for effectively computing the gradient of the loss function with respect
to the parameters, namely ∂L
∂θ . This is done by deﬁning the adjoint of the dynamical system as
a(t) = ∂L
∂x(t) . Then, the dynamical system describing the evolution of the adjoint can be derived
as [16,36]

da(t)
dt

= −a(t)T ∂f (x(t), t, θ)

∂x

.

(2.3)

Note that the adjoint a(t) can be computed by an additional call to the chosen ODE solver, and

the target derivative ∂L

∂θ can be then computed as

∂L
∂θ

= −

(cid:90)

t0

t1

a(t)T ∂f (x(t), t, θ)

∂θ

dt,

(2.4)

∂f (x(t),t,θ)
∂θ

where
this approach can be summarized in the following points:

can be evaluated via automatic differentiation [16,37]. The main advantages of

4

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

l

r
s
p
a
.
r
o
y
a
s
o
c
e
t
y
p
u
b

i

l
i

i

s
h
n
g
.
o
r
g

.

.

.

.

.

.

.

.

P
r
o
c
R
S
o
c
A
0
0
0
0
0
0
0

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

• The data does not need to be collected on a regular time grid.
• The time-step ∆ti between an observed data pair {x(ti), x(ti + ∆ti)} can be relatively
large. Within each ∆ti, a classical numerical scheme can be used to integrate equation
2.4, where ∆ti = Nidt is discretized in Ni equal spaced steps, with the step size dt being
typically chosen according to the stability properties of the underlying ODE solver.

• As this setup only assumes dependency between individual

input-output pairs
{x(ti), x(ti + ∆ti)}, the observed time-series data does not need to be continuous and
could be selected from different time intervals.

The speciﬁc choices of ∆ti, dt and Ni will be discussed for each of the different examples given in
this work. In general, the choice of N is made based on the following trade-off between accuracy
and computational complexity. To this end, small N may lead to a less accurate model, while large
N would lead to a massive computational graph that can signiﬁcantly slow down model training.
The unknown model parameters θ can be estimated by minimizing appropriate loss function.

Throughout this work, the following L2 loss is employed

J (θ) =

n
(cid:88)

i=1

(cid:107)x(ti + ∆ti) − hθ(x(ti))(cid:107)2,

(2.5)

where hθ(x(ti)) denotes the output of a numerical ODE solver. Throughout this work,
fourth order Runge-Kutta method [43], although more general
we use the classical
choices can be employed [14]. The training data-set consists of pairs {(x(t1), x(t1 +
∆t1)), (x(t2), x(t2 + ∆t2)), ..., (x(tn), x(tn + ∆tn))}. To simplify notation, let X(t) be deﬁned
as X(t) = {x(t1), x(t2), ..., x(tn)}, such that X(t + ∆t) = {x(t1 + ∆t1), x(t2 + ∆t2), ..., x(tn +
∆tn)}, then the training data-set is given by: D = {X(t), X(t + ∆t)}.

Notice that the aforementioned workﬂow is only capable of producing deterministic point
estimates for the unknown parameters θ, typically corresponding to a local minimum of equation
2.5. In many practical cases it is desirable to obtain a distribution over plausible parameter
conﬁgurations that can effectively characterize the uncertainty in the estimates due to noise and
sparsity in the observed data, as well as potential misspeciﬁcations in the model parametrization.
A framework for accounting for such uncertainties can be constructed by adopting a Bayesian
approach via the use of effective sampling algorithms for approximating a posterior distribution
over the unknown model parameters p(θ|D), as discussed in the next section.

(b) Bayesian inference with Hamiltonian Monte Carlo

The Bayesian formalism provides a natural way to account for uncertainty, while also enabling
the injection of prior information for the unknown model parameters θ (e.g., sparsity), as well as
for modeling sparse and noisy observations in the training data-set. Perhaps more importantly,
it enables the complete statistical characterization for all inferred parameters in the model. The
latter, is encapsulated in the posterior distribution which can be factorized as

p(γ, λ, θ|X(t + ∆t), X(t)) ∝ p(X(t + ∆t)|X(t), θ, γ)p(θ|λ)p(γ)p(λ),

(2.6)

where p(X(t + ∆t)|X(t), θ, γ) is a likelihood function that measures the discrepancy between
the observed data and the model’s predictions for a given set of parameters θ, p(θ|λ) is a
prior distribution parametrized by a set of parameters λ that can help encode any domain
knowledge about the unknown model parameters θ, and γ contains a set of parameters that aim to
characterize the noise process that may be corrupting the observations. In this work, a hierarchical
Bayesian approach corresponding to the following likelihood and priors is employed:

5

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

l

r
s
p
a
.
r
o
y
a
s
o
c
e
t
y
p
u
b

i

l
i

i

s
h
n
g
.
o
r
g

.

.

.

.

.

P
r
o
c
R
S
o
c
A
0
0
0
0
0
0
0

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

p(X(t + ∆t)|X(t), θ, γ) =

N
(cid:89)

i=1

N (x(ti + ∆ti)|f (x(ti), ti, θ), γ−1),

p(θ|λ) = Laplace(θ|0, λ−1),

p(log λ) = Gam(log λ|α1, β1),

p(log γ) = Gam(log γ|α2, β2).

(2.7)

The use of a Gaussian likelihood stems from assuming a simple isotropic Gaussian noise model
with zero mean and precision γ. The Laplace prior over the unknown model parameters θ [44]
can promote sparsity in the inferred model representations and enable an effective reduction
of the inﬂuence of any irrelevant parameters – a key feature for recovering interpretable and
parsimonious representations [17]. The Gamma distribution is a common choice for the prior
distributions over the unknown precision parameters λ and γ. For additional motivation and
alternative choices, the interested reader is referred to [45–49]. Finally, the logarithm of the
precision variables λ and γ is used to ensure that their estimated values remain positive during
model training.

The posterior distribution deﬁned in equation 2.7 is not analytically tractable in general due
to the modeling assumptions on the likelihood and priors, as well as due to the presence of
non-linearity in the latent dynamics of equation 2.1. Typically, sampling from this unnormalized
distribution is difﬁcult and computationally expensive, especially when the dimension of θ is
large. Hamiltonian Monte Carlo (HMC) [35] is a powerful tool to handle Bayesian inference
tasks in high dimensions by utilizing gradient information to effectively generate approximate
posterior samples. To illustrate the key ideas behind HMC sampling, let us denote Θ =
[θ, log(λ), log(γ)] as the vector including all the unknown parameters that need to be inferred
from data. The starting point for building an HMC sampler is to deﬁne a Hamiltonian function
H = U (Θ) + V (v), where U (Θ) is the potential energy of the original system usually taken as the
logarithm of the unnormalized distribution in equation 2.7, and V (v) is the kinetic energy of the
system introduced by the auxiliary velocity variables v := dΘ
dt . The evolution of Θ and v can be
expressed by taking gradients of the Hamiltonian as

dv
dt

= −

∂H
∂Θ

,

dΘ
dt

=

∂H
∂v

.

(2.8)

A Markov chain that convergences to the stationary distribution of equation 2.6 can be
simulated by integrating this dynamical system using an energy preserving leapfrog scheme [35].
∂H(t)
Note here that computing the gradient
∂Θ is not trivial as the evaluation of the likelihood
function involves the call to an ODE solver. This difﬁculty can be directly addressed via the
differentiable programming approach discussed in section (a) to effectively enable gradient back-
propagation through the ODE solver. Note that all parameters in Θ can be either updated
simultaneously, or separately for θ and {λ, γ} using a Metropolis-within-Gibbs scheme [50,51].

(c) Learning dynamics with Bayesian differential programming

Here we distinguish between three different problem settings that cover a broad range of practical
applications. The ﬁrst class consists of problems in which the model form of the underlying latent
dynamics is completely unknown. In this case, one can parametrize the unknown dynamical
system using black-box function approximators such as deep neural networks [15,16,20], or aim
to distill a more parsimonious and interpretable representation by constructing a comprehensive
dictionary over all possible interactions and try to infer a predictive, yet minimal model form
[17,18,21]. The second class of problems contains cases where a model form for the underlying
dynamics is prescribed by domain knowledge, but a number of unknown parameters needs to
be calibrated in order to accurately explain the observed data [31,52]. Finally, a third class of
problems arises as a hybrid of the aforementioned settings, in which some parts of the model form

6

l

r
s
p
a
.
r
o
y
a
s
o
c
e
t
y
p
u
b

i

l
i

i

s
h
n
g
.
o
r
g

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

P
r
o
c
R
S
o
c
A
0
0
0
0
0
0
0

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

may be known from domain knowledge, but there exists additional functional terms that need to
be inferred [14]. As detailed in the following sections, the proposed workﬂow can seamlessly
accommodate all of the aforementioned cases in a uniﬁed fashion, while remaining robust with
respect to incomplete model parametrizations, as well as imperfections in the observed data. To
this end, a general framework can be constructed by parametrizing the unknown dynamics as

dx
dt

= Aϕ(x)
(cid:124) (cid:123)(cid:122) (cid:125)
dictionary

,

+ fw(x)
(cid:124) (cid:123)(cid:122) (cid:125)
black-box

(2.9)

where A ∈ RD×K represents a matrix of D × K unknown coefﬁcients, with K being the length
of a dictionary ϕ(x) ∈ RK , that may or may not be constructed using domain knowledge.
Speciﬁcally, ϕ(x) represents the possible terms that may appear in the right hand side of the
ordinary differential equation, which could encapsulate a known model form or, more generally,
a prescribed dictionary of features (e.g., polynomials, Fourier modes, etc., and combinations
thereof) [17,18,53]. On the other hand, fw(x) denotes a black-box function approximator (e.g.,
a neural network) parametrized by w that aims to account for any missing interactions that are
not explicitly captured by ϕ(x) [14].

The domain knowledge can play a crucial role in enhancing the efﬁciency of the resulting
inference scheme as it can effectively reduce the size of the dictionary, and, consequently, the
number of data required to train the model [23,24]. Such knowledge is also critical to constrain
the space of admissible solutions such that key physical principles are faithfully captured by
the inferred model (e.g., convergence to equilibrium limit cycles in chemical systems [54],
conservation of mass and momentum in ﬂuid dynamics [30], etc).

Although the use of dictionary learning (with or without speciﬁc domain knowledge) offers
a ﬂexible paradigm for recovering interpretable dynamic representations, it may not always be
sufﬁcient to explain the observed data, as important terms may be missing from the model
parametrization. To address this shortcoming one can try to capture these missing interactions
via the use of closure terms that often lack physical intuition, and hence can be represented by a
black-box function approximator fw(x) with parameters w, such as a deep neural network [14].
Under this setup, the algorithmic framework outlined in sections (a) and (b) is employed to jointly
perform probabilistic inference over plausible sets of model parameters θ := {A, w} that yield
interpretable, parsimonious, and predictive representations, as well as the precision parameters
λ and γ of the Bayesian hierarchical model in equation 2.7.

(d) Generating forecasts with quantiﬁed uncertainty

The goal of the HMC algorithm described in section (b) is to produce a faithful set of
samples that concentrate in regions of high-probability in the posterior distribution p(θ, λ, γ|D).
Approximating this distribution is central to the proposed workﬂow as it enables the generation
of future forecasts x∗(t) with quantiﬁed uncertainty via computing the predictive posterior
distribution

(cid:90)

p(x∗(t)|D, x0, t) =

p(x∗(t)|θ, x0, t)p(θ|D)dθ.

(2.10)

This predictive distribution provides a complete statistical characterization for the forecasted
states by encapsulating epistemic uncertainty in the inferred dynamics, as well as accounting for
the fact that the model was trained on a ﬁnite set of noisy observations. This allows the generation
of plausible realizations of x∗(t) by sampling from the predictive posterior distribution as

x∗(t) = hθ(x0, t) + (cid:15),

(cid:15) ∼ N (0, γ−1),

θ, λ, γ ∼ p(θ, λ, γ|D)

(2.11)

where, θ, λ and γ are approximate samples from p(θ, λ, γ|D) computed during model training
via HMC sampling, hθ(x0, t) denotes any numerical integrator that takes some initial condition
x0 and predicts the system’s state at any time t, and (cid:15) accounts for the noise corrupting the

observations used during model training. Moreover, the maximum a-posteriori (MAP) estimate
of the model parameters is given as follows:

7

l

r
s
p
a
.
r
o
y
a
s
o
c
e
t
y
p
u
b

i

l
i

i

s
h
n
g
.
o
r
g

P
r
o
c
R
S
o
c
A
0
0
0
0
0
0
0

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

θMAP, λMAP, γMAP = arg max
θ,λ,γ

p(θ, λ, γ|D),

(2.12)

and it is used to obtain a point estimate prediction of the predicted states ˆxMAP(t) deﬁned as
following:

x∗

MAP(t) = hθMAP (x0, t).

(2.13)

Finally, it is straightforward to utilize the posterior samples of θ ∼ p(θ|D) to approximate the ﬁrst-
and second-order statistics of the predicted states x∗(t) for any given initial condition x0 as
Ns(cid:88)

(cid:90)

ˆµx∗ (x0, t) =

hθ(x0, t)p(θ|D)dθ ≈

hθi (x0, t),

(2.14)

1
Ns

i=1

(cid:90)

ˆσ2
x∗ (x0, t) =

[hθ(x0, t) − ˆµx∗ (x0, t)]2p(θ|D)dθ ≈

1
Ns

Ns(cid:88)

[hθi (x0, t) − ˆµx∗ (x0, t)]2,

(2.15)

i=1

where Ns denotes the number of samples drawn from the Hamiltonian Markov Chain used to
simulate the posterior, i.e., θi ∼ p(θ|D), i = 1, . . . , Ns. Note that higher-order moments are also
readily computable in a similar manner.

(e) Model initialization and data pre-processing

To promote robustness and stability in the training of the proposed machine learning pipeline,
users should be cognizant of several important aspects. First, although the proposed Bayesian
approach can naturally safe-guard against over-ﬁtting, it is important that a reasonable amount
of training data is provided – relative to the complexity of the system – in order to mitigate any
effects of prior misspeciﬁcation. Second, the training data should be appropriately normalized in
order to prevent gradient pathologies during back-propagation [55]. The speciﬁc utility of this
step will be demonstrated in the numerical examples presented in this work, and is carried out
using a standard normalization of the form

˜x =

x
σx

(2.16)

where σx is the dimension-wise standard deviation of the training data and the division is
an element-wise operation. Notice that this modiﬁcation directly implies that the assumed
parametrization of the underlying dynamical system also needs to be normalized accordingly
(see section 3 for a more detailed discussion). A third important remark here, is that the noise
precision γ obtained from the model aims to reﬂect the noise level in the observed data. However,
it may not be the true noise level because the initial condition of the ODE can also be noisy. This
point will be further discussed in section 3.

Another important point relates to the initialization of the Hamiltonian Monte Carlo Markov
Chain sampler. To this end, in order to mitigate poor mixing and convergence to local minima, a
preconditioning step is considered to ﬁnd a reasonable initial guess for the unknown variables θ
that parametrize the underlying latent dynamics. This step is typically carried out by minimizing
the reconstruction loss of the training data using an L1 regularization,

L(θ) =

1
n

n
(cid:88)

i=1

||xi − ˆxi(cid:107)2 + β(cid:107)θ(cid:107),

(2.17)

using a small number (O(103)) of stochastic gradient descent iterations. Notice that this
essentially aims at obtaining a rough point estimate for θ, where the use of L1 regularization
stems from employing a sparsity-promoting Laplace prior. This preconditioning step is closely
related to the SINDy algorithm of Brunton et. al. [17], however without the limitations of requiring
training data with small noise amplitude and sampled on regular time grid with small a time step.

8

l

r
s
p
a
.
r
o
y
a
s
o
c
e
t
y
p
u
b

i

l
i

i

s
h
n
g
.
o
r
g

P
r
o
c
R
S
o
c
A
0
0
0
0
0
0
0

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

Moreover, the numerical experiments carried out indicate that tuning the hyper-parameter β has
almost no effect on the obtained results for all the problems considered in this work, as this simply
serves as an initialization step for the HMC sampler. Hence, in all examples considered, β is taken
equal to 1. The minimization of equation 2.17 is carried out using stochastic Adam updates [56]
with a learning rate of O(10−2).

Note that this preconditioning is not precisely equivalent to the MAP estimation of the
posterior distribution over all model parameters Θ, since the initialization of the precision
parameters λ and γ follows a different treatment. Speciﬁcally, for all the problems considered
in section 3, the parameters αi’s and βi’s of the prior Gamma distributions are chosen to be
1. Moreover, the precision of the Gaussian noise distribution γ is initialized as follows. If the
preconditioning precision is larger than exp(6), which means the training data appears to be
nearly noise-free, the initial guess for γ is set to exp(6) to avoid numerical stagnancy of the
HMC sampler. Otherwise, if the preconditioning precision is less than exp(6), then it is used as
the initial guess for γ. This empirical initialization strategy has a positive effect in accelerating
the convergence of the HMC sampler for all the examples considered in section 3. In all of
the numerical examples, the Hamiltonian Monte Carlo step-size is taken as (cid:15) = 10−4, while the
number number of leapfrog steps to integrate the Hamiltonian dynamics in equation 2.8 is ﬁxed to
L = 10. While this choice of (cid:15) is the safest choice, one can increase its value as long as not too many
samples are rejected during model training. Alternatively, more sophisticated HMC samplers that
allow for adaptively tuning the step-size can be employed [57]. Finally, in all examples the Markov
Chains are simulated for 5, 000 steps, while the last Ns = 2, 000 samples produced by HMC are
used to compute the response statistics (see equation 2.14).

3. Results

In this section, a comprehensive collection of numerical studies that aim to illustrate the key
contributions of this work is presented and placed in context of the existing SINDy framework
of Brunton et. al. [17], which is currently considered as a state-of-the-art method for dictionary
learning of dynamical systems. Speciﬁcally, we expand on four benchmark problems that cover
all possible cases discussed in section (c) in terms of parametrizing the latent dynamics using
a dictionary, domain knowledge, black-box approximations, or a combination thereof. The
algorithmic settings used across all cases follow the discussion provided in (e), unless otherwise
noticed. All code and data presented in this section will be made publicly available at https://
github.com/PredictiveIntelligenceLab/BayesianDifferentiableProgramming.

(a) Dictionary learning for a two-dimensional nonlinear oscillator

Let us start with a pedagogical example on dictionary learning for inferring the dynamics of
a two-dimensional damped oscillator from scattered time-series data [20]. The exact system
dynamics are given by

dx1
dt
dx2
dt

= αx3

1 + βx3
2,

= γx3

1 + δx3
2.

(3.1)

The goal here is to recover this dynamical system directly from data using a dictionary
parametrization containing polynomial terms with up to 3rd order interactions, taking the form

(cid:35)

(cid:34) dx1
dt
dx2
dt

= Aϕ(x) =

(cid:34)

a11
a21

a12
a22

a13
a23

a14
a24

a15
a25

a16
a26

a17
a27

a18
a28

a19
a29

(cid:35)

a110
a210

ϕ(x)

(3.2)

where ϕ(x) = [1, x1, x2, x2
2]T and the aij ’s are unknown scalar
1x2, x1x2
coefﬁcients that will be estimated using the proposed Bayesian differential programming method.
The model’s active non-zero parameters are highlighted with red color for clarity. The goal

1, x1x2, x2

2, x3

1, x2

2, x3

9

l

r
s
p
a
.
r
o
y
a
s
o
c
e
t
y
p
u
b

i

l
i

i

s
h
n
g
.
o
r
g

P
r
o
c
R
S
o
c
A
0
0
0
0
0
0
0

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

is to infer a posterior distribution for the parameters θ := {A}, while it is obvious that the
coefﬁcient matrix A and dictionary ϕ(x) will increase in size with the order of the polynomial
features used to parametrize the system. In all presented experiments, a set of training data is
generated by simulating the exact dynamics of equation 3.1 in the time interval t ∈ [0, 20] with
the initial condition (x1, x2) = (2, 0) and with α = −0.1, β = 2.0, γ = −2.0, and δ = −0.1. In what
follows, the performance of the proposed algorithms is investigated with respect to the temporal
resolution and the noise level in the observed data. Moreover, the analysis is provided with a
comprehensive comparison with the SINDy algorithm of Brunton et. al. [17].

(i) Effects of sparsity in the training data

To examine the performance of the proposed methods with respect to sparsity in the training
data, a data-set is generated by integrating the exact dynamics with a relatively large dt = 0.0677,
such that there are only n = 300 training data pairs. In order to establish a comparison against the
SINDy algorithm [17], the training data is assumed noise-free and sampled on a regular temporal
grid, as required by the SINDy setup [17].

As seen in ﬁgures 1 and 2, the SINDy algorithm fails to identify the underlying system, while
the proposed approach remains robust thanks to the Bayesian formulation outlined in section (b)
and which enables a faithful statistical characterization of the latent system dynamics even under
sparse observations. Indeed, the proposed method does not only have the capability of accurately
identifying the parameters even for relatively large dt (see table 1), but also provides reasonable
uncertainty estimates for the extrapolated long-term forecasts. In contrast, the SINDy algorithm
gives inaccurate estimations for the dictionary parameters (see table 2), consequently leading to
large errors in the forecasted system states. As shown in ﬁgures 1(b),(d), the estimated trajectories
clearly deviate from the exact solution since its oscillatory frequency is considerably higher than
the exact one, and it is not capturing the decay of the oscillations’ peak. Moreover, SINDy’s
results highly depend on the choice of the sequential least-squares threshold parameter value,
and slightly different values of this hyper-parameter may give signiﬁcantly different results.

Table 1: Dictionary learning for a two-dimensional nonlinear oscillator: MAP estimation of the inferred
model parameters using low-resolution training data (dt = 0.0677).

a11
-0.0052
a21
0.0048

a12
0.0057
a22
0.0195

a13
-0.0095
a23
-0.0072

a14
0.0094
a24
-0.021

a15
0.0116
a25
0.0024

a16
0.0057
a26
-0.0075 −2.08

a17
−0.097
a27

a18
-0.0093
a28
0.0280

a110
2.092
a210

a19
-0.054
a29
-0.014 −0.099

Table 2: Dictionary learning for a two-dimensional nonlinear oscillator: Point estimates for the
dictionary coefﬁcients obtained by the SINDy algorithm [17] using low-resolution training data
(dt = 0.0677).

a11
0
a21
0

a12
0
a22
0

a13
0
a23
0

a14
0
a24
0

a15
0
a25
0

a16
0
a26
0

a17
0
a27
-1.87

a18
0
a28
0

a19
0
a29
0

a110
2.10
a210
0

10

l

r
s
p
a
.
r
o
y
a
s
o
c
e
t
y
p
u
b

i

l
i

i

s
h
n
g
.
o
r
g

P
r
o
c
R
S
o
c
A
0
0
0
0
0
0
0

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

Figure 1: Two-dimensional damped oscillator with low-resolution training data: (a) Learned dynamics
versus the true dynamics and the training data for x1(t). (b) SINDy’s prediction for x1(t) versus
the true dynamics and the training data. (c) Learned dynamics versus the true dynamics and the
training data for x2(t). (d) SINDy’s prediction for x2(t) versus the true dynamics and the training
data.

Figure 2: Two-dimensional damped oscillator with low-resolution training data: (a) Phase plot of the
training data and the true trajectory. (b) Posterior distribution of the inferred active model
parameters.

(ii) Effects of noise in the training data

11

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

l

r
s
p
a
.
r
o
y
a
s
o
c
e
t
y
p
u
b

i

l
i

i

s
h
n
g
.
o
r
g

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

P
r
o
c
R
S
o
c
A
0
0
0
0
0
0
0

In this section, the sensitivity of the proposed methods with respect to the presence of noise in
the training data is investigated. To this end, a training data-set is generated with n = 1, 000 equi-
spaced data-pairs in t ∈ [0, 20] by simulating the exact dynamics of equation 3.1 with dt = 0.02,
and the observations are deliberately corrupted with uncorrelated Gaussian noise of the form
N (0, 0.022) (see ﬁgure 4(a)).

Figure 3 summarizes the predictions of the proposed Bayesian framework in comparison to the
SINDy algorithm of Brunton et. al. [17]. Moreover, the inferred dictionary parameters are provided
for both methods in tables 3 and 4, respectively. Notice that, although the predicted trajectories of
SINDy are quite close to the true trajectories, the identiﬁed parameters are quite different from the
true model form. Even though SINDy employs a total variation diminishing (TVD) regularization
to safe-guard against small noise corruptions in the training data [17], it is still prone to providing
inaccurate results in cases where the training data is imperfect. This limitation is addressed in the
proposed framework by explicitly accounting for the effects of noise in the training data using
the hierarchical Bayesian model in equation 2.6. This source of uncertainty is also effectively
propagated through the system’s dynamics to yield a sensible characterization of uncertainty
in the predicted forecasts (see ﬁgure 3(a)), via the inferred posterior distribution over the model
parameters (see ﬁgure 4(b)). Moreover, the resulting MAP estimates for the model parameters
exhibit excellent agreement with the ground truth, as reported in table 3. This result illustrates
the robust performance of the proposed framework in identifying interpretable and parsimonious
system representations, even in the presence of noise in the training data.

Table 3: Dictionary learning for a two-dimensional nonlinear oscillator: MAP estimation of the inferred
model parameters using noisy training data.

a11
0.0042
a21
-0.0029

a12
-0.0059
a22
-0.010

a13
-0.0045
a23
-0.0073

a14
-0.0072
a24
0.0017

a15
-0.0078
a25
0.017

a16
-0.0071
a26
0.0064

a17
-0.10
a27
-2.0

a18
0.047
a28
0.015

a110
2.0
a210

a19
0.014
a29
-0.049 −0.108

Table 4: Dictionary learning for a two-dimensional nonlinear oscillator: estimation of the SINDy’s
parameters using noisy training data.

a11
0
a21
0

a12
0
a22
0

a13
0
a23
0

a14
0
a24
0

a15
0
a25
0

a16
0
a26
0

a17
-0.139
a27
-1.97

a18
0
a28
0

a19
0
a29
-0.188

a110
2.0
a210
0

(b) Parameter

inference in a predator-prey system with irregularly

sampled observations

This case study is designed to illustrate the capability of the proposed framework to accommodate
noisy and irregularly sampled time-series data; a common practical setting that cannot be
effectively addressed by SINDy and other popular data-driven systems identiﬁcation methods
[17,18,20,21,58]. To this end, a classical prey-predator system described by the Lotkaâ ˘A¸SVolterra

12

l

r
s
p
a
.
r
o
y
a
s
o
c
e
t
y
p
u
b

i

l
i

i

s
h
n
g
.
o
r
g

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

P
r
o
c
R
S
o
c
A
0
0
0
0
0
0
0

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

Figure 3: Two-dimensional damped oscillator with noisy training data: (a) Learned dynamics versus
the true dynamics and the training data for x1(t). (b) SINDy’s prediction for x1(t) versus the true
dynamics and the training data. (c) Learned dynamics versus the true dynamics and the training
data for x2(t). (d) SINDy’s prediction for x2(t) versus the true dynamics and the training data.

Figure 4: Two-dimensional damped oscillator with noisy training data: (a) Phase plot of the training
data and the true trajectory. (b) Posterior distribution of the inferred active model parameters.

equations is considered

dx1
dt
dx2
dt

= αx1 − βx1x2 ,

= δx1x2 − γx2 ,

(3.3)

13

l

r
s
p
a
.
r
o
y
a
s
o
c
e
t
y
p
u
b

i

l
i

i

s
h
n
g
.
o
r
g

P
r
o
c
R
S
o
c
A
0
0
0
0
0
0
0

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

Figure 5: Parameter inference in a predator-prey system from irregularly sampled, noisy data: (a) Learned
dynamics versus the true dynamics and the training data for x1. (b) Learned dynamics versus the
true dynamics and the training data for x2.

which is known to exhibit a stable limit cycle behavior for α = 1.0, β = −0.1, γ = −1.5, and
δ = 0.75. Without loss of generality, a dictionary that precisely contains the active terms of the
system is considered. However, the n = 1000 training data-pairs will be irregularly sampled in
the interval t ∈ [0, 25] by randomly sub-sampling an exact trajectory of system 3.3 starting from
an initial condition set to (x1, x2) = (5, 5). Moreover, the training data is perturbed by 3% white
noise proportional to its standard deviation. Given this irregular and noisy training data, the
goal is to demonstrate the performance of the proposed Bayesian framework in identifying the
unknown model parameters θ := {α, β, γ, δ} with quantiﬁed uncertainty, as well as in producing
sensible forecasts of extrapolated future states.

The results of this experiment are summarized in ﬁgures 5 and 6. It is evident that the proposed
Bayesian differential programming approach (i) is able to provide an accurate estimation for the
unknown model parameters, (ii) yields a MAP estimator with a predicted trajectory that closely
matches the exact system’s dynamics, (iii) returns a posterior distribution over plausible models
that captures both the epistemic uncertainty of the assumed parametrization and the uncertainty
induced by training on a ﬁnite amount of noisy training data, and (iv) propagates this uncertainty
through the system’s dynamics to characterize variability in the predicted future states.

Another interesting observation here is that the Hamiltonian Monte Carlo sampler is very
efﬁcient in identifying the importance/sensitivity of each inferred parameter in the model. For
instance, less important parameters have the highest uncertainty, as observed in the posterior
density plots shown in ﬁgures 6(b). Speciﬁcally, notice how the posterior distribution of α and
γ has a considerably larger standard deviation than the other parameters, implying that the
evolution of this dynamical system is less sensitive with respect to these parameters.

(c) Safe-guarding against model inadequacy: a damped pendulum case

study

The purpose of this example is to demonstrate the effects of a misspeciﬁed model parametrization,
and to highlight how the proposed Bayesian framework can help detect such problematic cases
and safe-guard against them. Such cases may arise when domain knowledge is insufﬁcient to
guide the selection of a parsimonious model form, as well as when important interaction terms
may be missing from a dictionary representation. To illustrate the main ideas, a simple damped
pendulum system described by the following equation is considered:

dx1
dt
dx2
dt

= γx2,

= −αx2 − β sin(x1),

(3.4)

14

l

r
s
p
a
.
r
o
y
a
s
o
c
e
t
y
p
u
b

i

l
i

i

s
h
n
g
.
o
r
g

P
r
o
c
R
S
o
c
A
0
0
0
0
0
0
0

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

Figure 6: Parameter inference in a predator-prey system from irregularly sampled, noisy data: (a) Phase
plot of the training data and the true trajectory. (b) Posterior distribution of the inferred model
parameters.

with γ = 1, α = 0.2 and β = 8.91. A set of sparse and irregularly sampled training data-pairs can
be generated by randomly sub-sampling a simulated trajectory of the exact dynamics in t ∈ [0, 20],
starting from an initial condition (x1, x2) = (−1.193, −3.876). This imperfect data-set can be the
used to recover an interpretable model representation via dictionary learning, albeit here we
deliberately choose to use an incomplete dictionary containing polynomial terms only up to 1st
order, i.e.,

(cid:35)

(cid:34) dx1
dt
dx2
dt

= Aϕ(x) =

(cid:34)

a11
a21

a12
a22

(cid:35)

a13
a23








1
x1
x2


 ,

(3.5)

where the unknown model parameters are θ := {a11, a12, a13, a21, a22, a23}, with the active
terms being marked with red color for clarity. Moreover, a blue marker is used to highlight a
mismatched term in the incomplete dictionary, hinting its erroneous capacity to approximate the
true sin(x1) term using just x1 as a feature. It is evident that such a dictionary choice cannot
faithfully capture the exact physics of the problem, and is hence destined to yield inaccurate
predictions. Nevertheless, here we argue that this discrepancy between the true form and the
assumed incomplete parametrization of the dynamical system can be effectively detected by
the proposed Bayesian workﬂow via inspecting the inferred posterior distribution over all
parameters p(θ, λ, γ|D), which is expected to exhibit high entropy in presence of a misspeciﬁed
model parametrization and imperfect training data.

The results of this experiment are summarized in ﬁgures 7 and 8. In particular, ﬁgure 7 shows
the scattered training data, the exact simulated trajectory, the predicted trajectory corresponding
to the MAP estimate of the inferred model parameters, as well as a two standard deviations
band around a set of plausible forecasted states. As expected, the use of a misspeciﬁed dictionary
leads to an inferred model that has difﬁculty in ﬁtting the observed data and yields predictions
with high variability. This model inadequacy is also clearly captured in the posterior distribution
over the inferred parameters shown in ﬁgure 8. Indeed, the inferred density for the β parameter
exhibits very high variance, as the β coefﬁcient crucially corresponds to the misspeciﬁed term
in the dictionary. This is a direct indication that the inferred model is inadequate to capture the
observed reality, leading to forecasts with inevitably large error-bars. This innate ability of the
proposed framework to detect model misspeciﬁcation via a rigorous quantiﬁcation of predictive

Figure 7: Damped pendulum with irregularly sampled data: (a) Learned dynamics versus the true
dynamics and the training data for x1(t). (b) Learned dynamics versus the true dynamics and the
training data for x2(t).

15

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

l

r
s
p
a
.
r
o
y
a
s
o
c
e
t
y
p
u
b

i

l
i

i

s
h
n
g
.
o
r
g

.

.

.

.

.

.

.

.

.

.

.

.

P
r
o
c
R
S
o
c
A
0
0
0
0
0
0
0

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

Figure 8: Damped pendulum with irregularly sampled data: (a) Phase plot of the training data and the
true trajectory. (b) Posterior distribution of the model’s parameters.

uncertainty can prove crucial in risk-sensitive applications, and provides an important capability
that is currently missing from the recently active literature on data-driven model discovery
[14,17,18,20,21,53].

A natural question to now ask is: can we still recover an accurate predictive model even if
the true dynamics can only be partially captured by the assumed dictionary representation? To
tackle this question we turn our attention to the hybrid learning setting discussed in section
(c), in which some parts of the model can be captured by sparsely selecting interpretable terms
from a dictionary, while other missing parts or closure terms can be accounted for via a black-
box function approximator. To this end, the damped pendulum case study is revisited, and
the dictionary learning parametrization is endowed with the ability to approximate the missing
sin(x1) term via a deep neural network as

(cid:35)

(cid:34) dx1
dt
dx2
dt

= Aϕ(x) + fw(x) =

(cid:34)

a11 + a12x1 + a13x2 + a14x2

1 + a15x1x2 + a16x2
2

a21x2 + a22x1x2 + a23x2

2 + fw(x1),

(cid:35)

(3.6)

16

l

r
s
p
a
.
r
o
y
a
s
o
c
e
t
y
p
u
b

i

l
i

i

s
h
n
g
.
o
r
g

P
r
o
c
R
S
o
c
A
0
0
0
0
0
0
0

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

Figure 9: Safe-guarding against model inadequacy in damped pendulum representation: (a) Learned
dynamics versus the true dynamics and the training data for x1(t). (b) Learned dynamics versus
the true dynamics and the training data for x2(t). A deep neural network is used to approximate
any interactions missing from an incomplete dictionary representation.

where A is a matrix of unknown coefﬁcients corresponding to a dictionary ϕ(x) containing
polynomial interactions up to 2nd order, and fw(x1) is a fully-connected deep neural network
with 2 hidden layers of dimension 20, a hyperbolic tangent activation, and a set of unknown
weight and bias parameters denoted by w. Notice that the neural network admits only x1
as an input to make sure that the resulting parametrized dynamical system has a unique
solution. Under this setup, the proposed Bayesian framework can be employed to jointly infer the
dictionary and the neural network parameters that deﬁne the model representation, namely θ :=
{a11, a12, a13, a14, a15, a16, a21, a22, a23, w}. This deﬁnes a 490-dimensional inference problem.

Figure 9 shows the scattered training data, the exact simulated trajectory, the predicted
trajectory corresponding to the MAP estimate of the inferred model parameters, as well as a two
standard deviations band around a set of plausible forecasted states. It is evident that the revised
hybrid model formulation can now correctly identify the true underlying dynamics, leading to
an accurate predicted MAP trajectory, while the predicted uncertainty effectively diminishes and
concentrates around the ground truth. Moreover, the inferred coefﬁcients of the irrelevant terms
are very close to zero, while the active terms are all properly identiﬁed. This is also true for the
neural network approximation of the missing closure term −β sin(x1), as depicted in ﬁgure 10
which also includes uncertainty estimates over the neural network outputs. This simple example
illustrates the great ﬂexibility of the proposed Bayesian framework in seamlessly distilling
parsimonious and interpretable models from imperfect data via physics-informed dictionary
learning, as well harnessing the power of black-box representations for approximating missing
closure terms.

(d) Bayesian calibration of a Yeast Glycolysis model

In this ﬁnal example, the performance of the proposed algorithms applied to a realistic problem in
systems biology is investigated. To this end, a yeast glycolysis process is considered and described

17

l

r
s
p
a
.
r
o
y
a
s
o
c
e
t
y
p
u
b

i

l
i

i

s
h
n
g
.
o
r
g

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

P
r
o
c
R
S
o
c
A
0
0
0
0
0
0
0

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

Figure 10: Safe-guarding against model inadequacy in damped pendulum representation: (a) Deep neural
network approximation to the missing dictionary term −β sin(x1). The shade region indicated the
range of data seen during model training. (b) Posterior distribution of the inferred active model
parameters.

by a 7-dimensional dynamical system [1,52] as:

dS1
dt
dS2
dt
dS3
dt
dS4
dt
dN2
dt
dA3
dt
dSex
4
dt

= J0 − k1S1A3[1 + (

A3
KI

)q]−1,

= 2k1S1A3[1 + (

A3
KI

)q]−1 − k2S2N1 − k6S2N2,

= k2S2N1 − k3S3A2,

= k3S3A2 − k4S4N2, −κ(S4 − Sex

4 ),

(3.7)

= k2S2N1 − k4S4N2, −k6S2N2,

= −2k1S1A3[1 + (

A3
KI

)q]−1 + 2k3S3A2 − k5A3,

= φκ(S4 − Sex

4 ) − kSex
4 ,

where N1 + N2 = N and A2 + A3 = A. This model form is assumed to be known from existing
domain knowledge, and the goal is (i) to compute the posterior distribution over the unknown
model parameters θ := {J0, k1, k2, k3, k4, k5, k6, k, κ, q, KI , φ, N, A} from a small set of noisy and
irregularly sampled observations, and (ii) to test the ability of the inferred model to accurately
generalize from different initial conditions that were not observed during model training.

A training data-set is generated by randomly sub-sampling n = 1, 000 irregularly distributed
observations from a single simulated trajectory of the system from an initial condition:
(S1, S2, S3, S4, N2, A3, Sex
4 ) = (0.5, 1.9, 0.18, 0.15, 0.16, 0.1, 0.064) in the time interval t ∈ [0, 5],
assuming a ground truth set of parameters obtained from the experimental data provided
in [1]: J0 = 2.5mM/min, k1 = 100.0mM/min, k2 = 6.0mM/min, k3 = 16.0mM/min, k4 =
100.0mM/min, k5 = 1.28/min, k6 = 12.0mM/min, k = 1.8/min, κ = 13.0/min, q = 4.0, KI =
0.52mM, N = 1.0mM, A = 4.0mM and φ = 0.1. Moreover, the training data is perturbed by a 2%
white noise proportional to its standard deviation.

Table 5 summarizes the inferred MAP estimators for the unknown model parameters. Based on
those results, all inferred parameters closely agree with the ground truth values used to generate
the training data as reported in [1]. Uncertainty estimates for the inferred parameters can also be
deducted from the computed posterior distribution p(θ|D), as presented in the box plots of ﬁgure
11 where the minimum, maximum, median, ﬁrst quantile and third quantile obtained from the

18

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

l

r
s
p
a
.
r
o
y
a
s
o
c
e
t
y
p
u
b

i

l
i

i

s
h
n
g
.
o
r
g

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

P
r
o
c
R
S
o
c
A
0
0
0
0
0
0
0

Figure 11: Yeast Glycolysis dynamics: Uncertainty estimation of the inferred model parameters
obtained using the proposed Bayesian differential programming method. Estimates for the
minimum, maximum, median, ﬁrst quantile and third quantile are provided, while the true
parameter values are highlighted in blue.

.

HMC simulations for each parameter are presented. Finally, notice that all true values fall between
the predicted quantiles, while the MAP estimators of all the parameters have considerably small
relative errors compared with the true parameter values, as shown in table 5.

Table 5: Yeast Glycolysis dynamics: MAP estimation of the inferred model parameters using noisy
training data.

J0
2.52

k1
98.92

k2
6.43

k3
16.12

k4
101.24

k5
1.28

k6
12.13

k
1.82

κ
13.12

q
4.01

KI
0.52

φ
0.01

N
0.95

A
4.01

Finally, to illustrate the generalization capability of the inferred model with respect to different
initial conditions than those used during training, the quality of the predicted states is assessed
considering a random initial condition of [0.428, 1.42, 0.11, 0.296, 0.252, 0.830, 0.064], that has not
been observed during model training. The close agreement with the reference solution indicates
that the inferred model is well capable of generalizing both in terms of handling different initial
conditions, as well as extrapolating to reliable future forecasts with quantiﬁed uncertainty.

4. Conclusions

We have presented a novel machine learning framework for robust systems identiﬁcation
under uncertainty. The proposed framework leverages state-of-the-art differential programming
techniques in combination with gradient-enhanced sampling schemes for scalable Bayesian
inference of high-dimensional posterior distributions, to infer interpretable and parsimonious

19

l

r
s
p
a
.
r
o
y
a
s
o
c
e
t
y
p
u
b

i

l
i

i

s
h
n
g
.
o
r
g

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

P
r
o
c
R
S
o
c
A
0
0
0
0
0
0
0

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

Figure 12: Learning Yeast Glycolysis dynamics from noisy data: Future forecasts with quantiﬁed
uncertainty from a previously unseen initial condition (i.e. an initial condition that was not used
during model training).

representations of complex dynamical systems from imperfect (e.g. sparse, noisy, irregularly
sampled) data. The developed methods are general as they can seamlessly combine dictionary
learning, domain knowledge and black-box approximations, all in a computationally efﬁcient
workﬂow with end-to-end uncertainty quantiﬁcation. The effectiveness of
the proposed
techniques has been systematically investigated and compared to state-of-the-art approaches
across a collection of prototype problems.

Although the proposed Bayesian differential programming framework provides great
ﬂexibility to infer a distribution over plausible parsimonious representations of a dynamical
system, a number of technical issues need to be further investigated. The ﬁrst relates to devising
more effective initialization procedures for Markov Chain Monte Carlo sampling. Here we have

20

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

l

r
s
p
a
.
r
o
y
a
s
o
c
e
t
y
p
u
b

i

l
i

i

s
h
n
g
.
o
r
g

.

.

.

.

.

.

.

.

.

.

.

.

P
r
o
c
R
S
o
c
A
0
0
0
0
0
0
0

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

partially addressed this via the MAP preconditioning algorithm discussed in section (e), however
during the preconditioning process, since the form of the dynamical system is unknown, the
intermediate estimations of the parameters may cause the system to become stiff. Moreover,
for cases where only very sparse observations are available, the model needs to be integrated
with a large time-step dt and stiffness of the system can lead to numerical instabilities during
model training. A possible enhancement in this direction is to use more general stifﬂy stable ODE
solvers as discussed in [14,15] or more sophisticated time-step annealing strategies [27]. Another
potential future work could be identifying the uncertainty in model’s parameters with partial
observations, such that some variables of the system are not accessible. Such task would involve
physics-informed regularization on the unknown latent dynamics of the system. Approaches
used in [52] could be helpful for solving this problem. A third open question is how to adapt the
proposed method to stochastic dynamical systems where the dynamics itself may be driving by a
stochastic process. Approaches mentioned in [59] could be useful. Finally, the proposed Bayesian
differential programming framework can be extended to parameter identiﬁcation for partial
differential equations (PDEs). The latter generally translates into a high dimensional dynamical
systems after discretization. The learning task in this context could be carried out not only for the
PDEs’ parameters, but also for the discretization scheme [14].

Data Accessibility. All code and data accompanying this manuscript is available at https://github.
com/PredictiveIntelligenceLab/BayesianDifferentiableProgramming.

Authors’ Contributions. P.P. and Y.Y. conceived the methods, Y.Y. implemented the methods, Y.Y. and
M.A.B. performed the simulations. Y.Y., M.A.B. and P.P. wrote the manuscript.

Competing Interests. The authors have no competing interests to declare.

Funding. This work received support from the US Department of Energy under the Advanced Scientiﬁc
Computing Research program (grant DE-SC0019116) and the Defense Advanced Research Projects Agency
under the Physics of Artiﬁcial Intelligence program (grant HR00111890034).

References

1. Ruoff P, Christensen MK, Wolf J, Heinrich R. 2003 Temperature dependency and temperature
compensation in a model of yeast glycolytic oscillations. Biophysical chemistry 106, 179–192.
2. Kak AC, Slaney M, Wang G. 2002 Principles of computerized tomographic imaging. Medical

Physics 29, 107–107.

3. Raissi M, Yazdani A, Karniadakis GE. 2020 Hidden ﬂuid mechanics: Learning velocity and

pressure ﬁelds from ﬂow visualizations. Science 367, 1026–1030.

4. Palmer TN. 1999 A nonlinear dynamical perspective on climate prediction. Journal of Climate

12, 575–591.

5. Feinberg M, Horn FJ. 1974 Dynamics of open chemical systems and the algebraic structure of

the underlying reaction network. Chemical Engineering Science 29, 775–787.

6. Haller G. 2002 Lagrangian coherent structures from approximate velocity data. Physics of ﬂuids

14, 1851–1861.

7. Tantet A, Lucarini V, Lunkeit F, Dijkstra HA. 2018 Crisis of the chaotic attractor of a climate

model: a transfer operator approach. Nonlinearity 31, 2221.

8. Bemporad A, Morari M. 1999 Control of systems integrating logic, dynamics, and constraints.

Automatica 35, 407–427.

9. Lu F, Zhong M, Tang S, Maggioni M. 2019 Nonparametric inference of interaction laws in
systems of agents from trajectory data. Proceedings of the National Academy of Sciences 116,
14424–14433.

10. Krizhevsky A, Sutskever I, Hinton GE. 2012 Imagenet classiﬁcation with deep convolutional

neural networks. In Advances in neural information processing systems pp. 1097–1105.

11. Bahdanau D, Cho K, Bengio Y. 2014 Neural machine translation by jointly learning to align

and translate. arXiv preprint arXiv:1409.0473.

12. Yang Y, Perdikaris P. 2019 Conditional deep surrogate models for stochastic, high-

dimensional, and multi-ﬁdelity systems. Computational Mechanics 64, 417–434.

13. Han J, Jentzen A, Weinan E. 2018 Solving high-dimensional partial differential equations using

deep learning. Proceedings of the National Academy of Sciences 115, 8505–8510.

21

l

r
s
p
a
.
r
o
y
a
s
o
c
e
t
y
p
u
b

i

l
i

i

s
h
n
g
.
o
r
g

P
r
o
c
R
S
o
c
A
0
0
0
0
0
0
0

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

14. Rackauckas C, Ma Y, Martensen J, Warner C, Zubov K, Supekar R, Skinner D, Ramadhan
A. 2020 Universal Differential Equations for Scientiﬁc Machine Learning. arXiv preprint
arXiv:2001.04385.

15. Gholami A, Keutzer K, Biros G. 2019 Anode: Unconditionally accurate memory-efﬁcient

gradients for neural odes. arXiv preprint arXiv:1902.10298.

16. Chen TQ, Rubanova Y, Bettencourt J, Duvenaud DK. 2018 Neural ordinary differential

equations. In Advances in neural information processing systems pp. 6571–6583.

17. Brunton SL, Proctor JL, Kutz JN. 2016 Discovering governing equations from data by sparse
identiﬁcation of nonlinear dynamical systems. Proceedings of the National Academy of Sciences
113, 3932–3937.

18. Rudy SH, Brunton SL, Proctor JL, Kutz JN. 2017 Data-driven discovery of partial differential

equations. Science Advances 3, e1602614.

19. Brennan C, Venturi D. 2018 Data-driven closures for stochastic dynamical systems. Journal of

Computational Physics 372, 281–298.

20. Raissi M, Perdikaris P, Karniadakis GE. 2018 Multistep neural networks for data-driven

discovery of nonlinear dynamical systems. arXiv preprint arXiv:1801.01236.

21. Qin T, Wu K, Xiu D. 2019 Data driven governing equations approximation using deep neural

networks. Journal of Computational Physics 395, 620–635.

22. Raissi M, Perdikaris P, Karniadakis GE. 2017 Inferring solutions of differential equations using

noisy multi-ﬁdelity data. Journal of Computational Physics 335, 736–746.

23. Raissi M, Perdikaris P, Karniadakis GE. 2019 Physics-informed neural networks: A deep
learning framework for solving forward and inverse problems involving nonlinear partial
differential equations. Journal of Computational Physics 378, 686–707.

24. Zhu Y, Zabaras N, Koutsourelakis PS, Perdikaris P. 2019 Physics-constrained deep learning for
high-dimensional surrogate modeling and uncertainty quantiﬁcation without labeled data.
Journal of Computational Physics 394, 56–81.

25. Yang Y, Perdikaris P. 2019 Adversarial uncertainty quantiﬁcation in physics-informed neural

networks. Journal of Computational Physics 394, 136–152.

26. Yang L, Zhang D, Karniadakis GE. 2020 Physics-Informed Generative Adversarial Networks
for Stochastic Differential Equations. SIAM Journal on Scientiﬁc Computing 42, A292–A317.
27. Wang S, Teng Y, Perdikaris P. 2020 Understanding and mitigating gradient pathologies in

physics-informed neural networks. arXiv preprint arXiv:2001.04536.

28. Wang Q, Hesthaven JS, Ray D. 2019 Non-intrusive reduced order modeling of unsteady
ﬂows using artiﬁcial neural networks with application to a combustion problem. Journal of
computational physics 384, 289–307.

29. Deveney T, Mueller E, Shardlow T. 2019 A deep surrogate approach to efﬁcient Bayesian

inversion in PDE and integral equation models. arXiv preprint arXiv:1910.01547.

30. Raissi M, Karniadakis GE. 2018 Hidden physics models: Machine learning of nonlinear partial

differential equations. Journal of Computational Physics 357, 125–141.

31. Tartakovsky AM, Marrero CO, Perdikaris P, Tartakovsky GD, Barajas-Solano D. 2018 Learning
parameters and constitutive relationships with physics informed deep neural networks. arXiv
preprint arXiv:1808.03398.

32. Kissas G, Yang Y, Hwuang E, Witschey WR, Detre JA, Perdikaris P. 2020 Machine learning in
cardiovascular ﬂows modeling: Predicting arterial blood pressure from non-invasive 4D ﬂow
MRI data using physics-informed neural networks. Computer Methods in Applied Mechanics and
Engineering 358, 112623.

33. Chen Y, Lu L, Karniadakis GE, Negro LD. 2019 Physics-informed neural networks for inverse

problems in nano-optics and metamaterials. arXiv preprint arXiv:1912.01085.

34. Sahli Costabal F, Yang Y, Perdikaris P, Hurtado DE, Kuhl E. 2020 Physics-informed neural

networks for cardiac activation mapping. Frontiers in Physics 8, 42.

35. Neal RM et al.. 2011 MCMC using Hamiltonian dynamics. Handbook of markov chain monte carlo

2, 2.

36. Pontryagin LS. 2018 Mathematical theory of optimal processes. Routledge.
37. van Merrienboer B, Breuleux O, Bergeron A, Lamblin P. 2018 Automatic differentiation in
ML: Where we are and where we should be going. In Advances in neural information processing
systems pp. 8757–8767.

38. Hastie T, Tibshirani R, Friedman J. 2009 The elements of statistical learning: data mining, inference,

and prediction. Springer Science & Business Media.

22

l

r
s
p
a
.
r
o
y
a
s
o
c
e
t
y
p
u
b

i

l
i

i

s
h
n
g
.
o
r
g

P
r
o
c
R
S
o
c
A
0
0
0
0
0
0
0

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

39. Ghahramani Z. 2015 Probabilistic machine learning and artiﬁcial intelligence. Nature 521, 452–

459.

40. Gulrajani I, Ahmed F, Arjovsky M, Dumoulin V, Courville AC. 2017 Improved training of

wasserstein gans. In Advances in neural information processing systems pp. 5767–5777.

41. Su W, Boyd S, Candes E. 2014 A differential equation for modeling Nesterovâ ˘A ´Zs accelerated
gradient method: Theory and insights. In Advances in Neural Information Processing Systems pp.
2510–2518.

42. Bottou L. 2010 Large-scale machine learning with stochastic gradient descent. In Proceedings

of COMPSTAT’2010 pp. 177–186. Springer.

43. Iserles A. 2008 A First Course in the Numerical Analysis of Differential Equations. Cambridge Texts

in Applied Mathematics. Cambridge University Press 2 edition.

44. Williams PM. 1995 Bayesian regularization and pruning using a Laplace prior. Neural

computation 7, 117–143.

45. Geweke J. 1993 Bayesian treatment of the independent Student-t linear model. Journal of

applied econometrics 8, S19–S40.

46. Gelman A, Carlin JB, Stern HS, Dunson DB, Vehtari A, Rubin DB. 2013 Bayesian data analysis.

CRC press.

47. Winkler RL. 1967 The assessment of prior distributions in Bayesian analysis. Journal of the

American Statistical association 62, 776–800.

48. Bernardo JM. 1979 Reference posterior distributions for Bayesian inference. Journal of the Royal

Statistical Society: Series B (Methodological) 41, 113–128.

49. Berger JO. 1990 Robust Bayesian analysis: sensitivity to the prior. Journal of statistical planning

and inference 25, 303–328.

50. Gilks WR, Best NG, Tan K. 1995 Adaptive rejection Metropolis sampling within Gibbs

sampling. Journal of the Royal Statistical Society: Series C (Applied Statistics) 44, 455–472.

51. Millar RB, Meyer R. 2000 Non-linear state space modelling of ﬁsheries biomass dynamics by
using Metropolis-Hastings within-Gibbs sampling. Journal of the Royal Statistical Society: Series
C (Applied Statistics) 49, 327–342.

52. Yazdani A, Raissi M, Karniadakis GE. 2019 Systems biology informed deep learning for

inferring parameters and hidden dynamics. bioRxiv p. 865063.

53. Champion K, Lusch B, Kutz JN, Brunton SL. 2019 Data-driven discovery of coordinates and

governing equations. Proceedings of the National Academy of Sciences 116, 22445–22451.

54. Schnakenberg J. 1979 Simple chemical reaction systems with limit cycle behaviour. Journal of

theoretical biology 81, 389–400.

55. Glorot X, Bengio Y. 2010 Understanding the difﬁculty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence and
statistics pp. 249–256.

56. Kingma DP, Ba J. 2014 Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980.

57. Hoffman MD, Gelman A. 2014 The No-U-Turn sampler: adaptively setting path lengths in

Hamiltonian Monte Carlo.. Journal of Machine Learning Research 15, 1593–1623.

58. Lusch B, Kutz JN, Brunton SL. 2018 Deep learning for universal linear embeddings of

nonlinear dynamics. Nature communications 9, 4950.

59. Li X, Wong TKL, Chen RT, Duvenaud D. 2020 Scalable Gradients for Stochastic Differential

Equations. arXiv preprint arXiv:2001.01328.

