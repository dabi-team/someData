1
2
0
2

r
a

M
1
1

]

G
L
.
s
c
[

1
v
7
9
7
6
0
.
3
0
1
2
:
v
i
X
r
a

BODAME: Bilevel Optimization for Defense Against Model
Extraction

Yuto Mori1†, Atsushi Nitanda1,2,3‡, Akiko Takeda1,2(cid:63)

1Graduate School of Information Science and Technology, The University of Tokyo, Japan
2Center for Advanced Intelligence Project, RIKEN, Japan
3PRESTO, Japan Science and Technology Agency, Japan
Email: †yuto mori@mist.i.u-tokyo.ac.jp, ‡nitanda@mist.i.u-tokyo.ac.jp, (cid:63)takeda@mist.i.u-tokyo.ac.jp

Abstract

Model extraction attacks have become serious issues for service providers using machine learn-
ing. We consider an adversarial setting to prevent model extraction under the assumption that
attackers will make their best guess on the service provider’s model using query accesses, and
propose to build a surrogate model that signiﬁcantly keeps away the predictions of the attacker’s
model from those of the true model. We formulate the problem as a non–convex constrained
bilevel optimization problem and show that for kernel models, it can be transformed into a non–
convex 1–quadratically constrained quadratic program with a polynomial–time algorithm to ﬁnd
the global optimum. Moreover, we give a tractable transformation and an algorithm for more
complicated models that are learned by using stochastic gradient descent–based algorithms. Nu-
merical experiments show that the surrogate model performs well compared with existing defense
models when the diﬀerence between the attacker’s and service provider’s distributions is large. We
also empirically conﬁrm the generalization ability of the surrogate model.

1 Introduction

Model extraction attacks (Lowd and Meek, 2005; Tram`er et al., 2016) have become serious problems as
more and more services based on machine learning are released as application programming interfaces
(APIs). A service provider who uses such an API is at risk of having its service imitated by model
extraction attacks in addition to other types of attacks using adversarial examples (Szegedy et al.,
2014; Goodfellow et al., 2015) or model inversion (Fredrikson et al., 2015), that become easier to
execute because of model extraction attacks.
If an attacker is able to steal the internal model, it
might become public and people might stop using the service because they can use a copy in their
local environments. Furthermore, there is even a risk that a similar service will be developed by an
attacker. Model extraction attacks thus pose huge risks for service providers who need to defend their
machine learning models against attackers.

Although model extraction attacks have been recently studied from various viewpoints, focusing on
the targeted model classes (Tram`er et al., 2016; Bastani et al., 2017; Oh et al., 2018; Milli et al., 2019;
Rolnick and Kording, 2020), the method to choose queries (Orekondy et al., 2019; Chandrasekaran
et al., 2020), the attacker’s objectives (Jagielski et al., 2020), and the attacker’s knowledge about the
defender (Batina et al., 2019; Pal et al., 2020; Krishna et al., 2020), there is not much research aimed
at defending against such attacks, except those summarized in Table 1. These defense methods in
Table 1 consider preserving the quality of the output or the surrogate function not to be far from
the defender’s true model, which is characteristic of the model extraction problems. For example,
rounding (Tram`er et al., 2016), reverse sigmoid (Lee et al., 2019), boundary diﬀerentially private layer

1

 
 
 
 
 
 
Figure 1: Overview of our attacker–defender framework

(BDPL) (Zheng et al., 2019), and maximum angular deviation (MAD) (Orekondy et al., 2020) are the
defending methods that change an output of the defender’s true model by, e.g. adding a noise. There
is another type of a defense against model extraction; (Alabdulmohsin et al., 2014) builds a surrogate
function that is robust against exploratory attacks, and their approach is theoretically analyzed by
using active learning in terms of randomization (Chandrasekaran et al., 2020).

Table 1: Existing work on defenses against model extraction. Objective means the goal
for the defender. Disagreement means to make an attacker’s model apart from the defender’s true
model. Detection means to detect whether model extraction can be performed. S/P means surro-
gate/perturbation:
it shows whether a method uses a surrogate function or a perturbation as the
defense. OptDef means whether an optimal defense is taken to prevent the model extraction by an
attacker. Knowledge is the defender’s knowledge about an attacker that is needed for the procedure
to make a defense. B/O means batch/online: it shows whether a defender can get attacker’s queries
in batch or online. C/R means classiﬁcation/regression: it shows whether a method can be applied to
a classiﬁcation or regression task. Model is the defender’s model that is the target of each method.
Linear–SVM means support vector machine for linear models. One–hot Classiﬁer means a model of
which output is one–hot vector. KR means kernel regression.

Objective

S/P

OptDef

Knowledge

B/O

C/R

Model

(Alabdulmohsin et al., 2014)
Rounding (Tram`er et al., 2016)
(Kesarwani et al., 2018)
Reverse Sigmoid (Lee et al., 2019)
PRADA (Juuti et al., 2019)
BDPL (Zheng et al., 2019)
MAD (Orekondy et al., 2020)

Disagreement
Disagreement
Detection
Disagreement
Detection
Disagreement
Disagreement

BODAME–KRR/KR (Ours.)
BODAME–SGD/SGA (Ours.)

Disagreement
Disagreement

S
P
-
P
-
P
P

S
S

No
No
No
No
No
No
Yes

Yes
Yes

-
Query
Query, Model
Query
Query
Query
Query, Gradient

Query, Model
Query, Model

-
O
B
O
B
O
O

B
B

C
C/R
C
C
C/R
C
C

R
C/R

Linear–SVM
-
Decision Tree
One–hot Classiﬁer
-
-
One–hot Classiﬁer

KR
Diﬀerentiable

In this paper, we consider an adversarial setting to prevent model extraction by attackers, unlike
the above–mentioned defense methods, under the assumption that attackers will make their best guess
on the defender’s model by query accesses. The purpose of our model is to build surrogate models
for defenders in such a way that the predictions between the defender’s true model and the attackers’
models are signiﬁcantly diﬀerent under the defender’s distribution. By utilizing the surrogate models
instead of their true models for API services, service providers will not be damaged even if their
surrogate models are imitated from the outputs of their surrogate models (see Fig 1 for the structure

2

Defender(Service Provider)fgθh˜w(θ)AttackerXgθ(X)X∼QX∼PTrue ModelSurrogate ModelQueryOutputDefenseExtractAttacker’s Modelof our model). We formulate the problem as a bilevel optimization problem to construct a surrogate
model including the attacker’s problem that imitates the defender’s model as a constraint. Another
constraint requires the surrogate model to be close to the true defender’s model.

The resulting constrained bilevel optimization problem is a non–convex problem. We show that the
problem can be transformed into a non–convex quadratic optimization problem having one quadratic
constraint (1–QCQP) with a polynomial–time algorithm to ﬁnd the global optimum in the setting that
the attacker uses kernel ridge regression and the defender uses a kernel regression model. We also devise
an algorithm to ﬁnd solutions to more general machine learning models under the assumption that
attackers use a stochastic gradient descent (SGD)–based algorithm to imitate the defender’s models.
We performed numerical experiments conﬁrming that our algorithms succeed in the defending the
model when the distributions between the attackers and defenders are substantially diﬀerent. We also
show that as long as the attacker’s queries follow the same distribution of queries used for defense, the
defenders can use the same surrogate models without additional updates.

2 Bilevel Optimization for Defense Against Model Extraction

In this section, we formulate Bilevel Optimization for Defense Against Model Extraction (BODAME),
theoretically analyze two extreme cases, and show the BODAME formulation in an empirical setting
on the attacker’s samples and defender’s samples.

2.1 Attacker–Defender setting

In this paper, we will consider the following setting.

For the attacker, we assume ﬁdelity extraction (Jagielski et al., 2020); the objective of the attacker
is to ﬁnd a parameter of the attacker model that minimizes an expected loss function on the attacker’s
input distribution by using the outputs of the defender’s surrogate model as supervised data. Here, we
incorporate the attacker’s optimization problem as a constraint, which leads to a bilevel formulation.
The optimization problem in the ﬁdelity extraction has the same objective function as supervised
learning models; therefore, we can apply the standard algorithms developed for supervised learning to
the attacker’s optimization problem.

The defender is assumed to know some information about attackers’ models, in particular, the
loss functions, hyperparameters, and solution methods (or update formula) of the attacker’s models
including some of the initial parameter values. Although these settings are stronger than those of prior
work shown as Table 1, they seem inevitable in the adversarial setting. In particular, examining this
setting will shed light on the limits of the defense against model extraction.

The goal of the defender is to build a surrogate model that maximizes the diﬀerences between
predictions given the defender’s distribution between the defender’s true model and the attacker’s
model. At the same time, the defender wants to keep almost the same prediction quality between the
surrogate model and the true defender’s model. These two objectives lead us to formulate a constrained
version of bilevel optimization.

2.2 Expected BODAME

We will denote the input space as X , the output space as Y, and the defender and attacker distribution
as Q and P , respectively, on X . We set function spaces F , G, and H as model classes. In particular,
G and H are parameterized function spaces with parameter spaces Θ and W , respectively, that is,
G = {gθ : X → Y | θ ∈ Θ} and H = {hw : X → Y | w ∈ W }. The defender has the true model
f ∈ F : X → Y. Because a naive deployment of the true model has some risk of the true model being
stolen, the defender prepares a parameterized surrogate model gθ ∈ G so as to be ε–approximation
(ε ≥ 0) to the true model f on a loss function l(c) : Y × Y → R on the defender’s distribution Q. The
attacker builds a parameterized copy–model hw ∈ H of the true model gθ as a result of minimizing

3

the attacker’s loss function l(a) : Y × Y → R on the attacker’s input distribution P as (2). The
defender wants to prevent the attacker from re–training by maximizing its objective loss function
l(o) : Y × Y → R between f and hw on Q as (1). Accordingly, we obtain the following formulation:

max
θ∈Θ

s.t.

E
X∼Q

[l(o)(f (X), h ˜w(θ)(X))],

˜w(θ) = arg min

E
X∼P
[l(c)(f (X), gθ(X))] ≤ ε.

w∈W

E
X∼Q

[l(a)(gθ(X), hw(X))],

(1)

(2)

(3)

Condition (3) originates from the motivation that the defender does not want to make the surrogate
model worse in order to guarantee the quality of the service for users who possess benign data following
the true distribution Q. We call this formulation the expected Bilevel Optimization for Defense Against
Model Extraction (BODAME) framework. The resulting attacker model is denoted as h ˜w(θ) and the
surrogate model is denoted as gθ, where θ is a solution of the problem (1)–(3).

2.3 Two extreme cases for expected BODAME

We are now interested in this question: “What are the circumstances in which the defender succeeds
or fails in defending against model extraction?”, which can be rephrased as “How large or small can
the optimal value of expected BODAME be?” The functions g and h need to be parameterized as in
(1), (2), and (3), but we will omit the notation in this subsection in order to focus on properties of
the model as functions. Here, we call l(a) a p-distance function in this subsection if it satisﬁes partial
conditions of distance functions that wherein for all y, y(cid:48) ∈ Y, l(a)(y, y(cid:48)) ≥ 0 and y = y(cid:48) if and only if
l(a)(y, y(cid:48)) = 0. First, we show a suﬃcient condition for a defender not to make an adequate defense.

Theorem 1 (Suﬃcient condition for almost 0 optimal value). Let l(a) be a p-distance function and
l(o) = l(c), and also suppose that supp(Q) = supp(P ) for distributions and G = H for function spaces.
Then, the optimal value (1) is upper bounded by ε for any f ∈ F .

We give a proof of Theorem 1 in the supplementary material. Theorem 1 implies that since the
attacker can imitate the defender’s model almost surely on P by using a p-distance loss function such
as a squared loss function and the attacker has the distribution in the same region with defender’s one
(i.e., supp(Q) = supp(P )), the defender fails to defend against attacks because of the constraint (3).
Next, we show an example where a defender can defend against an attacker in a regression setting.

Theorem 2 (Suﬃcient condition for inﬁnitely large optimal value). Let l(a) be a p-distance function
and suppose that X = Y = R, l(o)(y, y(cid:48)) = l(c)(y, y(cid:48)) = (y − y(cid:48))2, Q and P are absolutely continuous
measures on R, supp(Q) = [a, b], supp(P ) = [c, d], (a < b ≤ c < d), F and H are L-Lipschitz function
spaces and G = {g : R → R}. Then, the optimal value of (1) becomes ∞.

We give a proof of Theorem 2 in the supplementary material. Theorem 2 implies that the defender
can mount a defense by making a surrogate model which is largely diﬀerent from the true model only
in P ; therefore, the diﬀerence between the supports of P and Q is a crucial condition. We can easily
extend Theorem 2 to the multi–dimensional input space.

2.4 Empirical batch BODAME

For practical purposes, we will consider an empirical setting. Suppose that we have i.i.d. samples
of the attacker {x(a)
i=1 (x(a)
i ∼ P , n ∈ N), i.i.d. samples of the defender used in the constraint
j(cid:48)=1 (x(c)
{x(c)
j(cid:48) ∼ Q, m(cid:48) ∈ N), and i.i.d.
samples of the defender used in the objective function
{x(o)
j=1 (x(o)
j ∼ Q, m ∈ N) instead of the attacker’s and defender’s distributions. The corresponding

j(cid:48) }m(cid:48)
j }m

i }n

4

empirical batch BODAME is

max
θ∈Θ

1
m

m
(cid:88)

j=1

l(o)(f (x(o)

j ), h ˜w(θ)(x(o)

j )),

s.t. ˜w(θ) = arg min

w∈W

1
n

n
(cid:88)

i=1

l(a)(gθ(x(a)

i

), hw(x(a)

i

)),

1
m(cid:48)

m(cid:48)
(cid:88)

j(cid:48)=1

l(c)(f (x(c)

j(cid:48) ), gθ(x(c)

j(cid:48) )) ≤ ε.

(4)

(5)

(6)

The resulting optimization problem is diﬃcult to solve; it is a non–convex problem due to (4) and an
optimization problem (5) is included as a constraint. In the next two sections, we provide methods for
solving the problem.

3 BODAME–KRR/KR

We consider empirical batch BODAME, where we assume that the defender uses a kernel regression
model and the attacker uses a kernel ridge regression model. Although the optimization problem is
still non–convex, we propose a polynomial–time algorithm with an easy implementation to ﬁnd the
global optimum for the non–convex problem.

3.1 1–QCQP formulation

s

s }M

s=1 θ∗

s k∗(x, x(tr)

On the basis of the representer theorem (see e.g. Theorem 4.2. in (Sch¨olkopf et al., 2002)), we assume
that the true model f can be expressed as f (x) = (cid:80)M
) where θ∗ ∈ RM , k∗ : X × X →
R is a positive–deﬁnite kernel, M ∈ N, {x(tr)
s=1 are training samples which are identically and
independently distributed in Q. Moreover, we assume a surrogate model gθ(x) = (cid:80)M
s=1 θsk∗(x, x(tr)
)
(θ ∈ RM ), which means that the surrogate model uses the same training data and kernel as the true
model. Further, we assume that the attacker’s model is a kernel ridge regression model, but the
attacker does not precisely know the defender’s kernel in general. Therefore, the attacker’s model hw
can be expressed as (cid:80)n
), where w ∈ Rn and kh is also a positive–deﬁnite kernel diﬀerent
in general from the defender’s kernel in general. The attacker’s loss function is l(a)(gθ(x), hw(x)) =
(gθ(x) − hw(x))2 + λ(cid:107)hw(cid:107)2
H where λ > 0 and (cid:107) · (cid:107)H denotes the norm induced from the reproducing
kernel Hilbert space H corresponding to kh. Here, we assume that the defender knows some of the
attacker’s information, as follows:

i=1 wikh(x, x(a)

s

i

Requirement 1 (Defender’s knowledge about the attacker’s kernel model). The defender knows that
the deﬁnitions of the functions l(a) and hw, including the hyperparameter value λ and the attacker’s
samples {x(a)

i }n

i=1.

Based on Requirement 1, lower–level optimization (5) is tractable with an analytic solution:

˜w(θ) = (K1 + λIn)−1K2θ,

(7)

, x(a)

where K1 = (kh(x(a)
i(cid:48) ))n,n
also note that K3 = (kh(x(o)
(f (x(o)

, x(a)
i
j=1 ∈ Rm and f2 = (f (x(c)

i=1,i(cid:48)=1 ∈ Rn×n and K2 = (k∗(x(a)

, x(tr)
s
j=1,i=1 ∈ Rm×n, K4 = (k∗(x(c)
j(cid:48)=1 ∈ Rm(cid:48)

))m,n
j(cid:48) ))m(cid:48)

j ))m

j

i

i

))n,M
j(cid:48) , x(tr)

i=1,s=1 ∈ Rn×M . We should
j(cid:48)=1,s=1 ∈ Rm(cid:48)×M , f1 =

))m(cid:48),M

s

. We use the following matrix and vector notations:

˜A = K3(K1 + λIn)−1K2, A = (1/m) ˜A(cid:62) ˜A,
a = (1/m) ˜A(cid:62)f1, γa = (1/m)f (cid:62)
b = (1/m(cid:48))K (cid:62)

4 f2, γb = (1/m(cid:48))f (cid:62)

2 f2.

1 f1, B = (1/m(cid:48))K (cid:62)

4 K4,

5

After putting (7) into (4) and assuming that l(o) and l(c) are both squared loss functions, we can
rewrite empirical batch BODAME (4)–(6) as follows:

max
θ∈RM

θ(cid:62)Aθ − 2a(cid:62)θ + γa,

s.t.

θ(cid:62)Bθ − 2b(cid:62)θ + γb ≤ ε.

(8)

(9)

We can easily check that A and B are positive semi–deﬁnite and thereby that this problem is a
non–convex 1–quadratically constrained quadratic program (1–QCQP). We know a feasible solution
exists because the parameter of the true model θ∗ always satisﬁes (9). We call this optimization
problem BODAME–KRR/KR since the attacker uses a kernel ridge regression (KRR) model and
the defender uses a kernel regression (KR) model.

3.2 Algorithm for BODAME–KRR/KR

We use an eﬃcient algorithm (Adachi et al., 2017) with an easy implementation based on solving a
generalized eigenvalue problem. To make the algorithm applicable to problem (8) and (9), we use the
following assumption:

Assumption 1. B is positive–deﬁnite.

Under Assumption 1, we obtain the following formulation by changing a parameter from θ to

ˆθ = θ − B−1b:

min
(cid:107)ˆθ(cid:107)B ≤ˆε

1
2

ˆθ(cid:62) ˆAˆθ + ˆa(cid:62) ˆθ,

(10)

where (cid:107)ˆθ(cid:107)B =

(cid:112)ˆθ(cid:62)B ˆθ, ˆA = −2A, ˆa = 2a − 2AB−1b, and ˆε = (cid:112)ε + b(cid:62)B−1b − γb.

Since the problem (10) is a concave function minimization on a nonempty compact convex set,
it is ensured that the optimal solution exists at the boundary of the inequality constraint (see, e.g.,
Theorem 1.1, (Horst and Tuy, 1996)). We can reduce some of the procedures in the original algorithm
by exploiting the concave property to obtain the following Theorem 3. Suppose that Z is the eigenspace
of A and −B corresponding to the rightmost generalized eigenvalue.

Theorem 3 (Global optimization for BODAME–KRR/KR where ˆa (cid:54)⊥ Z). On the basis of Require-
ment 1, if Assumption 1 and ˆa (cid:54)⊥ Z hold, then the defender can ﬁnd the global optimum of the problem
(8) and (9), which is equivalent to (10) by applying Algorithm 1.

We give the proof of Theorem 3 in the supplementary material. We assume ˆa (cid:54)⊥ Z in Theorem 3
which leads to a simpliﬁed algorithm, but this assumption is not necessary if we use Algorithm 2 in
(Adachi et al., 2017) (See the supplementary material).

For the inner computation of (11), we can use some algorithms such as the Arnoldi methods (see
e.g. (Lehoucq et al., 1998)) to ﬁnd the eigenvectors of smallest generalized eigenvalue problems. The
computational complexity of Algorithm 1 is O(M 3), where M denotes the dimension of θopt.

4 BODAME–SGD/SGA

While an attacker can ﬁnd an analytic solution for the lower level optimization (5) in BODAME in the
previous setting, it may be diﬃcult to do so in more general cases. In an application, an attacker often
learns a defender’s model by using a few empirical samples by stochastic gradient descent (SGD) that
stops in a few steps. In this section, we assume that we can explicitly obtain the attacker’s gradient.
We give an algorithm based on gradient ascent using such knowledge on the SGD attacker. Compared
with BODAME–KRR/KR, the algorithm can be applied to a wider range of defender models since it
requires only diﬀerentiability of the models of the attacker and defender.

6

Algorithm 1 Algorithm for BODAME–KRR/KR by solving generalized eigenvalue problems
Input: A, a, B, ε, b, γb
Output: Surrogate parameter θopt
1: Compute B−1b, ˆA, ˆa, ˆε
2: Compute the smallest α∗ ∈ R and (divided) eigenvectors z1, z2 ∈ RM of following problem:
(cid:33) (cid:18)z1
z2

(cid:18)O B
B O

(cid:19) (cid:18)z1
z2

= α∗

ˆA

(cid:32)

(cid:19)

(cid:19)

−B
ˆA − ˆaˆa(cid:62)
ˆε2

(11)

3: ˆθopt = −sgn(ˆa(cid:62)z2)ˆε z1
4: θopt = ˆθopt + B−1b
5: return θopt

(cid:107)z1(cid:107)B

4.1 Formulation assuming SGD attacker

Recent machine learning models are trained with (mini–batch) SGD–based algorithms (with momen-
tum) (Robbins and Monro, 1951; Duchi et al., 2011; Kingma and Ba, 2015). Suppose that w(T )(θ) is
a parameter learned by an SGD–based algorithm in T ∈ N steps. More precisely, for t = 0, . . . , T − 1,
w(t+1) is recurrently deﬁned as

w(t+1)(θ) = w(t)(θ) − ηt+1∇w

1
|D(a)
t

|

(cid:88)

i∈D(a)
t

l(a)(gθ(x(a)

i

), hw(t) (x(a)

i

)),

(12)

where ηt+1 > 0 is the learning rate, D(a)
the number of elements of D(a)
the same notation as in section 2.4. We summarize these update formulae below:

| means
, and w0 ∈ W , w(0)(θ) = w0 is given as an initial value. Here we use

t means mini–batch data indices of {x(a)

i }, and |D(a)

t

t

w(t+1) = Φt(w(t), θ),

(13)

where Φt : W × Θ → W . We assume that an attacker generates sequences of w by using update
formulae Φt from some initial point since the attacker does not have analytic solutions for the lower–
level optimization problem (2). Furthermore, we assume that the defender knows the attacker’s settings
as follows:

Requirement 2 (Defender’s knowledge about a SGD attacker). A defender knows Φt (t = 0, . . . , T −
1), w0 and attacker’s samples {x(a)

i }n

i=1.

Requirement 2 means that the defender knows w0, D(a)

, ηt+1 (t = 0, . . . , T − 1), l(a), hw and W , as
well as the SGD–update formula in the SGD setting. Under Requirement 2, we transform the original
expected BODAME (1) (2) (3) as follows:

t

[l(o)(f (X), hw(T )(θ)(X))],

E
max
X∼Q
θ∈Θ
s.t. w(0) = w0,

w(t+1) = Φt(w(t), θ)

(t = 0 . . . , T − 1),

E
X∼Q

[l(c)(f (X), gθ(X))] ≤ ε.

(14)

(15)

(16)

(17)

We can easily conﬁrm that this formulation is equivalent to the original expected BODAME (1)–(3)
if limt→∞ w(t) = arg minw∈W

EX∼P [l(a)(gθ(X), hw(X))].

7

4.2 Algorithm for BODAME–SGD/SGA

We empirically optimize this problem by using an algorithm based on mini–batch Stochastic Gradient
Ascent (SGA) w.r.t. θ. We assume that θ ∈ Rd and w ∈ Rd(cid:48)
, where d, d(cid:48) ∈ N. We introduce an
objective loss function in each step,

L(o)

u (θ) =

1
|D(o)
u |

(cid:88)

j∈D(o)

u

l(o)(f (x(o)

j ), hw(T )(θ)(x(o)

j )),

(18)

where max iteration U ∈ N and steps u = 0, . . . , U − 1, D(o)
j=1. We
need to use the gradient of this loss function to apply a gradient ascent method. Here, we assume that
l(o), l(c), g, h and Φt
u (θ) is
directly related to meta–learning or hyperparameter optimization, where gradients can be computed
by using Hypergradient (Maclaurin et al., 2015; Franceschi et al., 2017, 2018). We will use forward
computation on t to calculate Hypergradient. We describe the batch constraint function corresponding
to (17) as L(c)(θ) ≤ ε, where

(t = 0, . . . , T − 1) are diﬀerentiable. In fact, the computation of ∇θL(o)

u are indices of a subset of {x(o)

j }m

L(c)(θ) =

1
m(cid:48)

m(cid:48)
(cid:88)

j(cid:48)=1

l(c)(f (x(c)

j(cid:48) ), gθ(x(c)

j(cid:48) )).

(19)

To remove (17), we use the log–barrier method (see e.g. (Forsgren et al., 2002)), adding a barrier
function λ(c) log(ε − L(c)(θ)) with the barrier parameter λ(c) to the objective function. In so doing,
we expect that, without having L(c)(θ) ≤ ε as a constraint, this constraint must hold by having
Algorithm 2 start from a strictly feasible point θ0 as a following assumption:

Assumption 2. The initial point θ0 is strictly feasible (i.e. L(c)(θ0) < ε).

It is enough to set θ0 = θ∗, where θ∗ denotes the true parameter of f to satisfy Assumption 2
when F = G. The reason for using full–batch constraint instead of a mini–batch one is that we
need to guarantee that all θ(u) (a parameter learned in step u) are strictly feasible on each u without
randomness. The optimization problem (14)–(17) based on a diﬀerentiable assumption of each model
is called BODAME–SGD/SGA, as the attacker uses a model which can be optimized by SGD and
the defender uses a model which can be optimized by SGA. We ﬁnally obtain Algorithm 2.

5 Experiments on BODAME–KRR/KR

We performed experiments on BODAME–KRR/KR by changing the diﬀerence between the attacker’s
and defender’s distribution to see if a large diﬀerence would provide enough room for an eﬀective
defense in our BODAME framework. We also evaluated the generalization ability of our surrogate
model against new queries from an attacker.

In what follow, Let fθ∗ denotes the true model and gθopt denotes the optimized surrogate model.
Furthermore, h ˜w(θopt) denotes the attacker’s model learned by using the outputs of the surrogate model
gθopt as supervised data and h ˜w(θ∗) denotes the one learned by using the outputs of the true model
fθ∗ . That is, h ˜w(θopt) corresponds to the attacker’s model built in our BODAME framework and h ˜w(θ∗)
corresponds to undefended naive deployment. These settings are illustrated in Fig 2.

We used white–wine data in the UCI wine dataset (Dua and Graﬀ, 2017). The sample size was
4899, and the number of features was 11, excluding quality of wine. We constructed models to predict
the quality (3 – 9) of wine in regression settings. The experiments were conducted on an Intel Core i9
at 2.4 GHz.

8

Algorithm 2 Algorithm for BODAME–SGD/SGA with log–barrier
Input: Initial values θ0 and w0, max iteration U ∈ N, learning rates βu > 0 (u = 1, . . . , U ), mini–batch

indices D(o)
u

(u = 1, . . . , U ), a barrier parameter λ(c) and an approximation parameter ε.

u (θ(u)) using Hypergradient

u + ∂Φt(w(t),θ(u))
Z (t)

∂θ

u (θ(u))

Output: Surrogate parameter θU
1: # Initialization
2: θ(0) = θ0
3: w(0) = w0
4: for u = 0 to U − 1 do
# Calculate ∇θL(o)
5:
Z (0)
u = Od(cid:48)d
for t = 0 to T − 1 do

6:
7:

8:

= ∂Φt(w(t),θ(u))
Z (t+1)
u
∂w
w(t+1) = Φt(w(t), θ(u))

u )(cid:62)∇wL(o)

u (θ(u)) = (Z (T )

9:
end for
10:
11: ∇θL(o)
12:
13:
14: while L(c)(θ(u+1)) ≥ ε do
15:
16:
17:
18:
19: end for
20: return θU

end while
(optional: λ(c) → 0)

θ(u+1) = θ(u) + βu+1(∇θL(o)
βu+1 = βu+1/2

# Guarantee the next point to be strictly feasible using backtracking
L(c)(θ(u+1)) = ∞

u (θ(u)) + λ(c)∇θ log(ε − L(c)(θ(u))))

5.1 Eﬀect of diﬀerence between distributions

We divided the wine dataset into defender’s data, attacker’s data, and test data. To make the diﬀerence
of two distributions, we shifted attacker’s samples by adding noises that are normally distributed in
N (dµ111, 0.22I11). We set the parameters to M = 350, n = 300, m = 1000, m(cid:48) = 1500, ε = 0.1, and
λ = 1.0, and the test sample size is 500. Both the defender and the attacker had RBF kernels, expressed
2} for x, x(cid:48) ∈ X . We compared our method with rounding a 16-decimal-place
as exp{−0.005(cid:107)x − x(cid:48)(cid:107)2
number to an integer in terms of disagreement and regression shown as Table 1. Here, g (Rounding)
denotes the surrogate model using rounding of the output of the true model, and h (Rounding) denotes
the attacker’s model using g (Rounding) as supervised data. We evaluated the mean squared error
(MSE) of the defender’s and attacker’s models in test data to examine the eﬀect of the diﬀerence
between the distributions Q and P , since Theorem 1 and Theorem 2 suggest the importance of the
diﬀerence of two supports.

The results are in Fig 3. We performed 50 experiments on each dµ by shuﬄing the data to preserve
the size of each sample and aggregating the results to calculate the median. The large value of
h ˜w(θopt) indicates that our defense method performed well especially in the case of a large dµ when
the distributional gap between the attacker and defender was large. Whereas undefended deployment,
expressed as h ˜w(θ∗), was susceptible to the attacker’s imitation even for large dµ, the surrogate model
made an adequate defense for large dµ. We also found that the prior defense by rounding had almost
the same eﬀect as undefended models. We also observed that the MSE of gθopt stayed near to the MSE
of the true model fθ∗ , which implies that the constraint of BODAME–KRR/KR performed well.

9

Figure 2: Attacker–defender framework in experiments.

Figure 3: Relation between each model’s MSE and the distance of the attacker’s data from the de-
fender’s data in the UCI wine dataset. Dotted lines are for models in relation to the rounding defense.

5.2 Generalization ability of surrogate model against noisy attacker’s queries

We tested the generalization ability of our surrogate model against attacker’s queries in the BODAME–
KRR/KR. We optimized BODAME–KRR/KR and obtained a surrogate model gθopt with an MSE of
1.477 for the true model fθ∗ with an MSE of 1.377 on 500 test samples when the attacker’s distribution
was shifted by dµ = 0.5. Next, we trained the attacker’s model using the same hyperparameters,
but used new 300 queries that were generated by adding new noise that followed the same normal
distribution for which dµ was 0.5 to attacker’s data, by using supervised data by the ﬁxed surrogate
model gθopt . We performed 50 experiments in which we changed the normally distributed noise and
computed the mean and the standard deviation of MSEs.

The results in Fig 4 imply that our surrogate models have generalization ability against the at-
tacker’s queries, since the attacker’s models that were trained on new queries had almost the same
MSEs as the attacker’s model trained with the original queries. This experiment suggests that the de-
fender can prevent an attacker whose queries follow the same distribution used in BODAME–KRR/KR
from extracting the defender’s model solving BODAME–KRR/KR only once when the defender knows
the attacker’s kernel model as Requirement 1 except for the attacker’s samples. The generalization
ability of the surrogate model against attacker’s queries is signiﬁcantly diﬀerent from rounding because

10

fθ*gθopth˜w(θopt)DefenderAttackerX∼QX∼Ph˜w(θ*){x(a)i}ni=1{fθ*(xi)}ni=1{x(a)i}ni=1{gθopt(xi)}ni=1{x(tr)s}Ms=1{x(o)j}mj=1{x(c)j′ }m′ j′ =1it requires all queries on the attacker’s original data due to the online property of each attack.

Figure 4: Results of experiments testing the generalization ability of surrogate models against at-
tacker’s queries in BODAME–KRR/KR. New models mean the attacker’s models learned by using
new attacker’s queries. The error bar indicates the standard deviation.

6 Experiments on BODAME–SGD/SGA

We performed similar experiments on BODAME–SGD/SGA to those on BODAME–KRR/KR, but
using a diﬀerent dataset. Here, we used the MNIST dataset (Lecun et al., 1998), which is a dataset
of hand–written digits from 0 to 9 in 28 × 28 pixels. The training sample size was 60000 and the test
sample size was 10000. The experiments were conducted on one GPU of an NVIDIA Tesla T4. We
use the same notation as in section 5.

6.1 Eﬀect of diﬀerence between distributions

We ﬁxed defender’s labels {0, 1, 2} and changed the attacker’s labels into {0, 1, 2}, {1, 2, 3}, {7, 8, 9} to
make the two distributions diﬀerent. Fig 5 is a 2D visualization of Q and P made by t–SNE (Maaten
and Hinton, 2008). It shows that the restriction of labels makes a diﬀerence between Q and P . We
used the same CNN consisting of two convolutional layers, a maxpooling layer, and two fully–connected
layers as the defender’s model and attacker’s model. We trained the defender’s true model with 29399
samples which had full labels in 30 epochs using mini–batch SGD without momentum to make fθ∗ . We
also pre–trained the attacker’s model with 12600 samples, which had full labels, in 5 epochs by using
mini–batch SGD without momentum. To make the surrogate model gθopt, we used transfer learning
in Algorithm 2 from the initial point θ∗. Only the parameters in the ﬁnal layers are learned in the
attacker’s model and surrogate model. A squared loss function was commonly used for l(o), l(c), and
l(a). The same {x(c)
j(cid:48)=1, where m(cid:48) = 1957, was used in all experiments . We set one epoch of the
attacker and 15 steps for a mini-batch with a size of 64, and set one epoch of the defender and 15 steps
for a mini-batch with a size of 64. The parameters were βu = 0.3 (for all u = 1, . . . , 15), ηt = 0.01 (for
all t = 1, . . . , 15), λ(c) = 0.1 and ε = 1.0.

j(cid:48) }m(cid:48)

We evaluated the models using 3147 test samples that were not used for pre–training or transfer
learning. Fig 6 shows that our model defends well when the defender’s distribution is diﬀerent from the
attacker’s distribution, since the diﬀerence in accuracy between h ˜w(θ∗) and h ˜w(θopt) was about 20 % in
the setting in which the attacker’s labels were {7, 8, 9}. These results also suggest that the diﬀerence
of distributions is an important factor in a defense against model extraction in BODAME–SGD/SGA.
We also observed that the accuracy of gθopt stayed near to the accuracy of the true model fθ∗ , which
implies that the constraint of BODAME–SGD/SGA performed well.

11

Figure 5: 2D visualization of attacker’s data and defender’s data made by t–SNE conditioned on partial
labels. We selected 500 samples from each Q and P in each setting of partial labels.

Figure 6: Relation between each model’s accuracy and the diﬀerence of the attacker’s data from
defender’s data in the MNIST dataset.

6.2 Generalization ability of surrogate model against noisy attacker’s queries

We tested the generalization ability of a surrogate model against attacker’s queries as section 5.2 when
Q and P were respectively conditioned on labels {0, 1, 2} and {7, 8, 9} so that the diﬀerence between
the distributions would be large. We obtained a surrogate model gθopt with an accuracy of 98.47
for the true model fθ∗ with an accuracy of 99.05 by solving BODAME–SGD/SGA once. Next, we
trained the attacker’s model by using new 768 queries with the same attacker’s hyperparameters. We
performed ten experiments in which we randomly chose mini–batch data and computed the mean and
the standard deviation of accuracy.

The results are in Fig 7. The surrogate model showed generalization ability against attacker’s
queries since the attacker’s models that were trained on the new queries had almost the same accuracy
as the original model. This experiment also implies that the defender can prevent an attacker who has
queries following the same distribution from extracting the defender’s model by solving BODAME–
SGD/SGA only once, which it is signiﬁcantly diﬀerent from the previous online defenses.

7 Conclusion

We formulated and theoretically analyzed a defense against model extraction framed as a bilevel op-
timization problem. In addition, under the assumption of kernel models or SGD attacker, we showed
respectively the optimization problem and the algorithms to ﬁnd the solutions. In numerical experi-
ments, we showed that the proposed defense better mitigates the attacker’s model extraction attack
compared with the case without defense and with a prior defense. We also conﬁrmed the general-

12

Figure 7: Results of experiments to conﬁrm generalization ability of surrogate models on attacker’s
queries. New models mean the attacker models learned by using new attacker’s queries. The error bar
indicates the standard deviation.

ization ability of the defender’s surrogate model against attacker’s queries in numerical experiments.
The results suggests that the proposed defense has an advantage with the prior online defense against
individual queries in that the same model can be used for the defense once our optimization problem
is solved.

Acknowledgments

AN was partially supported by JSPS Kakenhi (19K20337) and JST-PRESTO.

References

Adachi, S., Iwata, S., Nakatsukasa, Y., and Takeda, A. (2017). Solving the trust-region subproblem

by a generalized eigenvalue problem. SIAM Journal on Optimization, 27(1):269–291.

Alabdulmohsin, I. M., Gao, X., and Zhang, X. (2014). Adding robustness to support vector machines
against adversarial reverse engineering. In 23rd ACM International Conference on Conference on
Information and Knowledge Management, pages 231–240.

Bastani, O., Kim, C., and Bastani, H. (2017). Interpretability via model extraction. In Workshop on

Fairness, Accountability, and Transparency.

Batina, L., Bhasin, S., Jap, D., and Picek, S. (2019). CSI NN: Reverse engineering of neural network
architectures through electromagnetic side channel. In 28th USENIX Security Symposium, pages
515–532.

Chandrasekaran, V., Chaudhuri, K., Giacomelli, I., Jha, S., and Yan, S. (2020). Exploring connections
between active learning and model extraction. In 29th USENIX Security Symposium, pages 1309–
1326.

Dua, D. and Graﬀ, C. (2017). UCI machine learning repository.

Duchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research, 12(7).

Forsgren, A., Gill, P. E., and Wright, M. H. (2002). Interior methods for nonlinear optimization. SIAM

review, 44(4):525–597.

13

Franceschi, L., Donini, M., Frasconi, P., and Pontil, M. (2017). Forward and reverse gradient-based
hyperparameter optimization. In 34th International Conference on Machine Learning, pages 1165–
1173.

Franceschi, L., Frasconi, P., Salzo, S., Grazzi, R., and Pontil, M. (2018). Bilevel programming for
In 35th International Conference on Machine

hyperparameter optimization and meta-learning.
Learning, pages 1568–1577.

Fredrikson, M., Jha, S., and Ristenpart, T. (2015). Model inversion attacks that exploit conﬁdence
information and basic countermeasures. In 22nd ACM SIGSAC Conference on Computer and Com-
munications Security, pages 1322–1333.

Goodfellow, I., Shlens, J., and Szegedy, C. (2015). Explaining and harnessing adversarial examples.

In 3rd International Conference on Learning Representations.

Horst, R. and Tuy, H. (1996). Global optimization: Deterministic approaches. Springer Science &

Business Media, third edition.

Jagielski, M., Carlini, N., Berthelot, D., Kurakin, A., and Papernot, N. (2020). High accuracy and
high ﬁdelity extraction of neural networks. In 29th USENIX Security Symposium, pages 1345–1362.

Juuti, M., Szyller, S., Marchal, S., and Asokan, N. (2019). PRADA: protecting against DNN model

stealing attacks. In 4th IEEE European Symposium on Security and Privacy, pages 512–527.

Kesarwani, M., Mukhoty, B., Arya, V., and Mehta, S. (2018). Model extraction warning in mlaas

paradigm. In 34th Annual Computer Security Applications Conference, pages 371–380.

Kingma, D. P. and Ba, J. (2015). Adam: A method for stochastic optimization. In 3rd International

Conference on Learning Representations.

Krishna, K., Tomar, G. S., Parikh, A., Papernot, N., and Iyyer, M. (2020). Thieves of sesame street:
Model extraction on BERT-based APIs. In 8th International Conference on Learning Representa-
tions.

Lecun, Y., Bottou, L., Bengio, Y., and Haﬀner, P. (1998). Gradient-based learning applied to document

recognition. In IEEE, pages 2278–2324.

Lee, T., Edwards, B., Molloy, I., and Su, D. (2019). Defending against neural network model stealing
attacks using deceptive perturbations. In IEEE Security and Privacy Workshops, pages 43–49.

Lehoucq, R. B., Sorensen, D. C., and Yang, C. (1998). ARPACK users’ guide: solution of large-scale

eigenvalue problems with implicitly restarted Arnoldi methods. SIAM.

Lowd, D. and Meek, C. (2005). Adversarial learning. In 11th ACM SIGKDD International Conference

on Knowledge Discovery in Data Mining, pages 641–647.

Maaten, L. v. d. and Hinton, G. (2008). Visualizing data using t-SNE. Journal of Machine Learning

Research, 9(Nov):2579–2605.

Maclaurin, D., Duvenaud, D., and Adams, R. (2015). Gradient-based hyperparameter optimization
through reversible learning. In 32nd International Conference on Machine Learning, pages 2113–
2122.

Milli, S., Schmidt, L., Dragan, A. D., and Hardt, M. (2019). Model reconstruction from model

explanations. In 3rd Conference on Fairness, Accountability, and Transparency, pages 1–9.

Oh, S. J., Augustin, M., Fritz, M., and Schiele, B. (2018). Towards reverse-engineering black-box

neural networks. In 6th International Conference on Learning Representations.

14

Orekondy, T., Schiele, B., and Fritz, M. (2019). Knockoﬀ nets: Stealing functionality of black-box
models. In 32nd IEEE Conference on Computer Vision and Pattern Recognition, pages 4954–4963.

Orekondy, T., Schiele, B., and Fritz, M. (2020). Prediction poisoning: Towards defenses against DNN

model stealing attacks. In 8th International Conference on Learning Representations.

Pal, S., Gupta, Y., Shukla, A., Kanade, A., Shevade, S., and Ganapathy, V. (2020). Activethief:
Model extraction using active learning and unannotated public data. In 34th AAAI Conference on
Artiﬁcial Intelligence, pages 865–872.

Robbins, H. and Monro, S. (1951). A stochastic approximation method. The annals of mathematical

statistics, pages 400–407.

Rolnick, D. and Kording, K. (2020). Reverse-engineering deep relu networks. In 37th International

Conference on Machine Learning, pages 8178–8187.

Sch¨olkopf, B., Smola, A. J., Bach, F., et al. (2002). Learning with kernels: support vector machines,

regularization, optimization, and beyond. MIT press.

Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R. (2014).
Intriguing properties of neural networks. In 2nd International Conference on Learning Representa-
tions.

Tram`er, F., Zhang, F., Juels, A., Reiter, M. K., and Ristenpart, T. (2016). Stealing machine learning

models via prediction APIs. In 25th USENIX Security Symposium, pages 601–618.

Zheng, H., Ye, Q., Hu, H., Fang, C., and Shi, J. (2019). BDPL: A boundary diﬀerentially private layer
against machine learning model extraction attacks. In 24th European Symposium on Research in
Computer Security, pages 66–83.

15

Appendix

1 Notation

Models. A defender has a true model as f ∈ F : X → Y, where F is a function space, X is an input
space and Y is an output space. A defender builds a parameterized surrogate model gθ ∈ G : X → Y,
where G is a function space, θ ∈ Θ, and Θ is a parameter space. If f is parameterized by θ∗ ∈ Θ, it
is denoted by fθ∗ . An attacker builds a parameterized model hw ∈ H by model extraction where H
is a function space, w ∈ W , and W is a parameter space. gθ is simply written by g and hw is also
simply written by h when we discuss functionally properties. If Θ is a Euclidean space, Θ indicates
Rd, where d ∈ N. If W is a Euclidean space, W indicates Rd(cid:48)
, where d(cid:48) ∈ N. When we assumue that
a true model uses a positive deﬁnite kernel, the kernel is denoted by k∗. When we assume that an
attacker’s model uses a positive deﬁnite kernel, the kernel is denoted by kh.
Distribution and samples. A defender has input distribution Q on X . An attacker has input
distribution P on X . In empirical settings, A defender has objective samples {x(o)
j ∼ Q i.i.d.,
m ∈ N) and constraint samples {x(c)
j(cid:48) ∼ Q i.i.d., m(cid:48) ∈ N). In empirical settings, an attacker
has samples {x(a)
i ∼ P , i.i.d., n ∈ N). Training samples to learn a true model f are denoted
i }n
s=1 (x(tr)
by {x(tr)
s ∼ Q, i.i.d., M ∈ N).
s }M
Loss functions. A defender has an objective loss function l(o) : Y × Y → R and constraint loss
function l(c) : Y × Y → R. An attacker has a loss function l(a) : Y × Y → R.
Surrogate quality. ε > 0 is a parameter to control quality of a surrogate model g to approximate f .
Hyperparameters. λ > 0 is a coeﬃcient of regularization in l(a). η is a learning rate of an attacker
that uses SGD. β is a learning rate used in defender’s optimization for defense. λ(c) is a barrier
parameter used in Algorithm 2.
Symbols. Support of distribution is denoted by supp(·). Indicator function is denoted by 1. Kernel
of linear map A is denoted by Ker(A). Sign function is denoted by sgn : R → {−1, 1},

j(cid:48)=1 (x(c)

j=1 (x(o)

i=1 (x(a)

j(cid:48) }m(cid:48)

j }m

sgn(x) =

(cid:40)
1
−1

(x > 0)
(x ≤ 0)

.

2 Proofs

2.1 Proofs of Theorem 1 and Theorem 2

We will denote the expected BODAME as a function approximation as follows:

max
g∈G

E
X∼Q

[l(o)(f (X), hg(X))],

s.t. hg = arg min

[l(a)(g(X), h(X))],

E
X∼P
[l(c)(f (X), g(X))] ≤ ε.

h∈H

E
X∼Q

(20)

(21)

(22)

(23)

We give a proof of Theorem 1.

Proof of Theorem 1. Minimum of EX∼P [l(a)(g(X), h(X))] is obviously 0 since l(a) ≥ 0, G = H and we
can set h(x) = g(x) for all x ∈ X . Letting 1 be an indicator function, we have

E
X∼P

[l(a)(g(X), h(X))] = E
X∼P

[1{g(X)(cid:54)=h(X)}l(a)(g(X), h(X))] + E
X∼P

[1{g(X)=h(X)}l(a)(g(X), h(X))]

= E
X∼P

[1{g(X)(cid:54)=h(X)}l(a)(g(X), h(X))].

16

(24)

(25)

Since l(a)(y, y(cid:48)) > 0 if y (cid:54)= y(cid:48) and minimum of EX∼P [l(a)(g(X), h(X))] is 0, it is necessary that all hg
satisﬁes hg(X) = g(X) a.s. (X ∼ P ). This condition is equivalent to hg(X) = g(X) a.s. (X ∼ Q) from
the assumption supp(Q) = supp(P ). Therefore, EX∼Q[l(o)(f (X), hg(X))] = EX∼Q[l(c)(f (X), g(X))].
As a result, EX∼Q[l(o)(f (X), hg(X))] can increase to ε from the constraint EX∼Q[l(c)(f (X), g(X))] ≤
ε.

We also give a proof of Theorem 2.

Proof of Theorem 2. For C > 0, suppose that

g(x) =

(cid:40)

f (x) + C (x ∈ supp(P ))
f (x)

(otherwise).

(26)

This g satisﬁes EX∼Q[l(c)(f (X), g(X))] = 0. hg(X) = g(X) a.s. (X ∼ P ) holds from the assumption
of l(a) as shown in the proof of Theorem 1. It should be noted that when we take f ∗ ∈ F for f and
h∗
g ∈ H for hg which satisfy for x ∈ [b, b + C/2L],

and f ∗(X) = h∗

g(X) a.s. (X ∼ Q) on supp(Q) \ [b, b + C/2L], the following inequality holds:

f ∗(x) = Lx − bL + f (b),
h∗
g(x) = −Lx + bL + f (b) + C,

E
X∼Q

[l(o)(f (X), hg(X))] ≥ E

[l(o)(f ∗(X), h∗

g(X))]

X∼Q

(27)

(28)

(29)

from the assumption that F and H are L-Lipschitz function spaces. For the right–hand side in (29),
we have the following,

E
X∼Q

[l(o)(f ∗(X), h∗

g(X))] =

=

(cid:90) d

c
(cid:90) d

c

(f ∗(x) − h∗

g(x))2dQ(x)

(C + 2bL − 2Lx)2dQ(x)

and therefore, the claim of Theorem 2 holds.

2.2 Proof of Theorem 3

→ ∞ (C → ∞),

(30)

(31)

(32)

We use the following Lemma (see e.g. Theorem 1.1, (Horst and Tuy, 1996)) mentioned in our paper:
Lemma 1 (Concave function minimization on a nonempty compact convex set). Let S ⊂ RM be
nonempty, compact and convex. Let L : S → R be concave. Then the global minimum of L on S is
attained at an extreme point of S.

We give a proof of Theorem 3.

Proof of Theorem 3. Since A is positive semi–deﬁnite, ˆA (cid:22) O holds, and therefore, the objective of
(10) is concave. B is positive deﬁnite from the Assumption 1, {ˆθ|(cid:107)ˆθ(cid:107)B ≤ ˆε} is closed and bounded.
Furthermore, {ˆθ|(cid:107)ˆθ(cid:107)B ≤ ˆε} contains a feasible solution θ∗−B−1b, that implies that the set is nonempty.
Using Lemma 1, we can guarantee that the problem (10) attains its minimum on {ˆθ|(cid:107)ˆθ(cid:107)B = ˆε}. From
Theorem 3.2 and Theorem 3.3 in (Adachi et al., 2017), we obtain the global optimum of (10) as
ˆθopt = −sgn(ˆa(cid:62)z2)ˆεz1/(cid:107)z1(cid:107)B in {ˆθ|(cid:107)ˆθ(cid:107)B = ˆε} where z1, z2 satisﬁes for the smallest eigenvalue α∗,
(cid:19)

(cid:32)

(cid:19)

ˆA

−B
ˆA − ˆaˆa(cid:62)
ˆε2

(cid:33) (cid:18)z1
z2

= α∗

(cid:18)O B
B O

(cid:19) (cid:18)z1
z2

(33)

on the assumption ˆa (cid:54)⊥ Z. By changing θ to θ − B−1b, we obtain the global optimum of the problem
(8) and (9).

17

3 Global optimization for BODAME–KRR/KR

In this section, we show an algorithm for the hard case of BODAME–KRR/KR that ˆa ⊥ Z holds. We
use the same notation in section 3.

Theorem 4 (Global optimization for BODAME–KRR/KR). Algorithm 3 for the case of ˆa ⊥ Z and
Algorithm 1 for the other cases output a a global optimum solution of BODAME–KRR/KR (8) and
(9).

Proof of Theorem 4. If ˆa (cid:54)⊥ Z holds, we can get global optimum of (8) and (9) using Theorem 3. If
ˆa ⊥ Z holds, we can get a global optimum of (8) and (9) using Theorem 4.3 in (Adachi et al., 2017). We
can distinguish the hard case that ˆa ⊥ Z holds by checking whether the B–norm of eigenvalues (cid:107)z1(cid:107)B
equals to 0. Therefore, we ﬁrst check the condition that (cid:107)z1(cid:107)B equals to 0 by following Theorem 3
and then, if necessary, run Algorithm 3 to ﬁnd the global optimum of BODAME–KRR/KR (8) and
(9).

Algorithm 3 Algorithm for BODAME–KRR/KR where ˆa ⊥ Z holds
Input: A, a, B, ε, b, α∗, γb
Output: Surrogate parameter θopt
1: Compute B−1b, ˆA, ˆa, ˆε
2: Compute the B–orthogonal null vectors of ˆA + α∗B, v1, . . . , vd, where d = dim(Ker( ˆA + α∗B))
3: Fix α > 0
4: Compute H = ˆA + α∗B + α (cid:80)d
5: Solve Hq + ˆa = 0 for q by the Conjugate Gradient Method
6: Take an eigenvector v ∈ Ker( ˆA + α∗B) and ﬁnd β ∈ R such that (cid:107)q + βv(cid:107)B = ˆε
7: θopt = q + βv + B−1b
8: return θopt

i=1 Bviv(cid:62)

i B

In practice, we may use adding a perturbation such as normal noise to ˆa to avoid the hard case using
Algorithm 3 that ˆa ⊥ Z holds; we only use the Algorithm 1 by the perturbation to solve BODAME–
KRR/KR (8) and (9) although slight change of ˆa may not lead to a global optimum of the original
problem.

18

