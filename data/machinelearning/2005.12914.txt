0
2
0
2

y
a
M
6
2

]
L
M

.
t
a
t
s
[

1
v
4
1
9
2
1
.
5
0
0
2
:
v
i
X
r
a

Class-Weighted Classification: Trade-offs and Robust Approaches

Ziyu Xu*

Chen Dan*

Justin Khim*

Pradeep Ravikumar*

May 28, 2020

Abstract

We address imbalanced classification, the problem in which a label may have low marginal
probability relative to other labels, by weighting losses according to the correct class. First, we
examine the convergence rates of the expected excess weighted risk of plug-in classifiers where
the weighting for the plug-in classifier and the risk may be different. This leads to irreducible
errors that do not converge to the weighted Bayes risk, which motivates our consideration of
robust risks. We define a robust risk that minimizes risk over a set of weightings and show excess
risk bounds for this problem. Finally, we show that particular choices of the weighting set leads
to a special instance of conditional value at risk (CVaR) from stochastic programming, which we
call label conditional value at risk (LCVaR). Additionally, we generalize this weighting to derive
a new robust risk problem that we call label heterogeneous conditional value at risk (LHCVaR).
Finally, we empirically demonstrate the efficacy of LCVaR and LHCVaR on improving class
conditional risks.

1

Introduction

Classification is a fundamental problem in statistics and machine learning, including scientific
problems such as cancer diagnosis and satellite image processing as well as engineering applications
such as credit card fraud detection, handwritten digit recognition, and text processing (Khan et al.,
2001; Lee et al., 2004), but modern applications have brought new challenges. In online retailing,
websites such as Amazon have hundreds of thousands or millions of products to taxonomize (Lin
et al., 2018). In text data, the distribution of words in documents has been observed to follow a
power law in that there are many labels with few instances (Zipf, 1936; Feldman, 2019). Similarly,
image data also a long tail of many classes with few examples (Salakhutdinov et al., 2011; Zhu et al.,
2014). In such settings, the classes with smaller probabilities are generally classified incorrectly
more often, and this is undesirable when the smaller classes are important, such as rare forms
of cancer, fraudulent credit card transactions, and expensive online purchases. Thus, we need
modern classification methods that work well when there are a large number of classes and when
the class-wise probabilities are imbalanced.

When faced with such class imbalance a popular approach in practice is to choose a metric
other than zero-one accuracy, such as precision, recall, ùêπùõΩ-measure (Van Rijsbergen, 1974, 1979),
which explicitly take class conditional risks into account, and train classifiers to optimize this metric.
A difficulty with this approach however is that the right metric for imbalanced classification is
often not clear. A related class of approaches keep the zero-one accuracy metric but modifies the
samples instead. The popular algorithm SMOTE (Chawla et al., 2002) performs a type of data
augmentation for a minority class, i.e., a class with lower probability, and sub-samples the large

*Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213.

1

 
 
 
 
 
 
classes. This has led to variants with different forms of data augmentation (Zhou and Liu, 2006;
Mariani et al., 2018), but from a theoretical perspective, these methods remain poorly understood.
A much simpler approach, which is also related to the approaches above, is class-weighting,
in which different costs are incurred for mis-classifying samples of different labels. Practically,
this is a natural approach because it is often possible to assign different costs to different classes.
For example, the average fraudulent credit card transaction may cost hundreds of dollars, or in
online retailing, failing to show a customer the correct item causes the company to lose out on the
profit of selling that item. Thus, a good classifier should be fairly sensitive to possibly fraudulent
transactions, and online retailers should prioritize displaying high-profit products. As a result,
class-weighting has been studied in a variety of settings, including modifying black-box classifiers,
SVMs, and neural networks (Domingos, 1999; Lin et al., 2002; Scott, 2012; Zhou and Liu, 2006).
Additionally, class-weighting has been observed to be useful for estimating class probabilities, since
class-weighting amounts to adjusting decision thresholds (Wang et al., 2008; Wu et al., 2010; Wang
et al., 2019).

A crucial caveat with cost-weighting however is the right choice of costs is often not clear, and
with any one choice of costs, the performance of the corresponding classifier might suffer for some
other, perhaps more suitable, choices of costs.

In this paper, we use cost-weighting for imbalanced classification in three ways. We start by
examining a weighted sum of class-conditional risks, i.e., the risks conditional on the class ùëå taking
some specific value ùëñ. This allows us to upweight a minority class to achieve better performance on
the minority examples. We then provide an illuminating analysis of the fundamental tradeoffs that
occur with any single choice of costs.

Since we may not understand precisely which weighting ùëû to pick, we examine a robust risk that
is a supremum of the weighted risks over an uncertainty set ùëÑ of possible weights. This objective
can be interpreted as a class-wise distributionally robust optimization problem where we ask for
robustness over the marginal distribution of ùëå . This leads to a minimax problem, for which we
provide generalization guarantees. We also note that a standard gradient descent-ascent algorithm
may solve the optimization problem when the risk is convex in the classifier parameters.

Finally, we show that for a natural class of uncertainty sets, the robust risk reduces to what
call label conditional value at risk (LCVaR). We highlight a connection to conditional value at risk
(CVaR), which is a well-studied quantity in portfolio optimization and stochastic programming
parametrized by an ùõº in (0, 1) (Rockafellar et al., 2000; Shapiro et al., 2009). Further, we propose a
generalization that we call label heterogeneous conditional value at risk (LHCVaR) that allows for
different parameters ùõºùëñ for each class ùëñ. To the best of our knowledge, this has not been examined
previously, and it could possibly be used more broadly. To give an example in portfolio optimization,
we may wish to treat risks arising from different types of assets, e.g., large-cap stocks versus small-cap
stocks or domestic debt versus international debt, differently. Next, we show that the dual form for
LHCVaR is similar to that for LCVaR as long as the heterogeneity is finite-dimensional, and this
leads to an unconstrained optimization problem. Finally, we examine the efficacy of LCVaR, and
LHCVaR on real and synthetic data.

The rest of the paper is outlined as follows. In Section 2, we discuss our problem setup. In
Section 3, we examine weighting in plug-in classification. In particular, we elucidate the fundamental
trade-off in weighted classification and its methodological implications. In Section 4, we examine a
robust version of the weighted risk problem, including generalization guarantees and connections
to stochastic programming. In Section 5, we provide numerical results, and we conclude with
a discussion in Section 6. Additional proofs and results in related settings are deferred to the
appendices.

2

1.1 Further Related Work

We briefly review other research related to imbalanced classification, but for a far more exhaustive
treatment, see a survey of the area (He and Garcia, 2009; Fern√°ndez et al., 2018). First, two other
methods may be employed to solve imbalanced classification problems. The first is class-based
margin adjustment (Lin et al., 2002; Scott, 2012; Cao et al., 2019), in which the margin parameter
for the margin loss function may vary by class. Broadly, margin adjustment and weighting may both
be considered loss modification procedures. The second method is Neyman-Pearson classification, in
which one attempts to minimize the error on one class given a constraint on the worst permissible
error on the other class (Rigollet and Tong, 2011; Tong, 2013; Tong et al., 2016).

An important topic related to our paper but that has not been well-connected to imbalanced
classification is robust optimization. Robust optimization is a well-studied topic (Ben-Tal and
Nemirovski, 1999, 2003; Ben-Tal et al., 2004, 2009). A variant that has gained traction more recently
is distributionally robust optimization (Ben-Tal et al., 2013; Bertsimas et al., 2014; Namkoong and
Duchi, 2017). Unsurprisingly, CVaR, as a coherent risk measure, has been previously connected
to distributionally robust optimization (Goh and Sim, 2010). Distributionally robust optimization
generally and CVaR specifically have also previously been used in machine learning to deal with
imbalance (Duchi et al., 2018; Duchi and Namkoong, 2018), but in these works, the imbalance was
considered to exist in the covariates, whether known to the algorithm or not. These are motivated
by the recent push toward fairness in machine learning, in particular so that ethnic minorities do
not suffer discrimination in high-stakes situations such as loan applications, medical diagnoses, or
parole decisions, due to biases in the data.

2 Preliminaries

2.1 Classification with Imbalanced Classes

In this section, we briefly go over the problem setup. First, we draw samples from the space ùíµ = ùí≥ √óùí¥.
For our purposes, we are interested in ùí¥ = {0, 1} or ùí¥ = {1, . . . , ùëò}. Note there are two slightly
different mechanisms for the data-generating process that are considered in imbalanced classification
and Neyman-Pearson classification. In the first, we are given ùëõ i.i.d. samples (ùëã1, ùëå1), . . . , (ùëãùëõ, ùëåùëõ)
from a distribution ùëÉùëã,ùëå . Here, we let ùëùùëñ = P (ùëå = ùëñ) be the probability of class ùëñ. Additionally, we
sometimes refer to the vector of class probabilities as ùëù. This is our framework of interest, since
it corresponds to standard assumptions in nonparametric statistics and learning theory. In the
alternative framework, we are given ùëõùëñ samples (ùëã1, ùëñ), . . . , (ùëãùëõùëñ, ùëñ) from each marginal distribution
ùëÉùëã|ùëå =ùëñ. The probability of class ùëñ in this case is then known: ùëùùëñ = ÃÇÔ∏Äùëùùëñ = ùëõùëñ/ùëõ. For the most
part, these two mechanisms yield similar results, but the analyses differ slightly. To streamline the
presentation, we only consider the first case in the main paper, although we give a result for the
alternative framework in the appendix that illustrates the difference.

2.2 Class Conditioned Risk

We are interested in finding a good classifier ùëì : ùí≥ ‚Üí ùíü ‚äá ùí¥ in some function space ‚Ñ±, such as
linear classifiers or neural networks. In this section, we establish our risk measures of interest. In
general, we want to minimize the expectation of some loss function ‚Ñì : ‚Ñ± √ó ùíµ ‚Üí [0, 1], which we call
risk and denote ùëÖ(ùëì ) = E[‚Ñì(ùëì, ùëç)]. Analogously, we define the class-conditioned risk for class ùëñ to be

ùëÖ‚Ñì,ùëñ(ùëì ) = E [‚Ñì(ùëì, ùëç)|ùëå = ùëñ] .

3

At this point, we make some observations for plug-in classification and empirical risk minimization.
In the plug-in classification results, we consider the zero-one loss ‚Ñì01(ùëì, ùëß) = 1{ùëì (ùë•) Ã∏= ùë¶}, and for
our results on empirical risk minimization, we are primarily interested in convex surrogate losses.
For simplicity, when ‚Ñì is clear from context, or a statement is made for a generic ‚Ñì, we will denote
this as ùëÖùëñ.

Now, we can work toward defining weighted risks. We defined Observe that we can relate the
ùëñ‚ààùí¥ ùëùùëñùëÖùëñ(ùëì ). An important part of our

risk to the class-conditioned risk by ùëÖ(ùëì ) = E [ùëÖùëå (ùëì )] = ‚àëÔ∏Ä
paper is an examination of class-weighted risk.
Definition 1. Let ùëû = (ùëû1, . . . , ùëû|ùí¥|) be a vector such that ùëûùëñ ‚â• 0 for all ùëñ and E[ùëûùëå ] = ‚àëÔ∏Ä
Then, the ùëû-weighted risk is

ùëñ‚ààùí¥ ùëûùëñùëùùëñ = 1.

ùëÖùëû(ùëì ) = E [ùëûùëå ùëÖùëå (ùëì )] = ‚àëÔ∏Å

ùëûùëñùëùùëñùëÖùëñ(ùëì ).

ùëñ‚ààùí¥

Note that the usual risk is recovered by setting ùëû = (1, . . . , 1).

2.3 Plug-in Classification

In this section, we discuss weighted plug-in classification. For plug-in, we restrict our attention
to the binary classification case of ùí¥ = {0, 1}, and the primary quantity of interest is usually the
one-zero risk ùëÖ01(ùëì ) i.e the risk under ‚Ñì0,1. In general, the risk for the best classifier is nonzero
because for a given ùë• in ùí≥ , there is some probability it may take the value 0 or 1.

As a result, we need a way to discuss the convergence of our estimator to the best possible
estimator. We define the regression function ùúÇ by ùúÇ(ùë•) = P (ùëå = 1|ùëã = ùë•) . Now, the Bayes optimal
classifier is the classifier that minimizes the risk, and it is defined by ùëì *(ùë•) = 1 {ùúÇ(ùë•) > 1/2} . The
minimum possible risk is called the Bayes risk and denoted by ùëÖ* = ùëÖ(ùëì *), and generally we focus
on minimizing the excess risk ‚Ñ∞(ùëì ) = ùëÖ(ùëì ) ‚àí ùëÖ*.

Following the form of the Bayes classifier, a plug-in estimator ÃÇÔ∏Äùëì attempts to estimate the
regression function ùúÇ by some ÃÇÔ∏ÄùúÇ and then ‚Äúplugs in‚Äù the result to a threshold function. Thus, ÃÇÔ∏Äùëì
has the form ÃÇÔ∏Äùëì (ùë•) = 1 {ÃÇÔ∏ÄùúÇ(ùë•) > 1/2} , which is analogous to the form of the Bayes classifier. For
additional background on plug-in estimation, see, e.g., Devroye et al. (1996).

At this point, we wish to define the weighted versions of Bayes classifier, Bayes risk, plug-in
classifier, and excess risk. For brevity, define the threshold ùë°ùëû = ùëû0/(ùëû0 + ùëû1). First, we consider the
Bayes classifier.

Lemma 1. Let ùëû = (ùëû0, ùëû1) be a weighting. The Bayes optimal classifier for ùëû-weighted risk is
ùëì *
ùëû (ùë•) = 1 {ùúÇ(ùë•) > ùë°ùëû} .

The proof, along with proofs of other subsequent results on plug-in classification, appears in
the appendix. In this case, we denote the Bayes risk by ùëÖ*
ùëû ). Lemma 1 reveals that the
Bayes classifier is a plug-in rule, and analogously, we see that a plug-in estimator in the weighted
case takes the form ÃÇÔ∏Äùëìùëû(ùë•) = 1 {ÃÇÔ∏ÄùúÇ(ùë•) > ùë°ùëû} . Consequently, we define excess ùëû-risk for an empirical
classifier ÃÇÔ∏Äùëì . The excess ùëû-risk for an empirical classifier is ‚Ñ∞ùëû( ÃÇÔ∏Äùëì ) = ùëÖùëû( ÃÇÔ∏Äùëì ) ‚àí ùëÖ*
ùëû, and note that we are
interested in bounding the expected excess ùëû-risk for plug-in estimators.

ùëû = ùëÖùëû(ùëì *

2.4 Empirical Risk Minimization

In this section, we define empirical quantities that we need for empirical risk minimization, par-
ticularly the weighted and robust risks. We consider ùí¥ = {1, . . . , ùëò}. We define the empirical

4

class-conditioned risk by ÃÇÔ∏ÄùëÖùëñ = (1/ùëÅùëñ) ‚àëÔ∏Äùëõ
ùëó=1 1 {ùë¶ùëó = ùëñ}. Let
ÃÇÔ∏Äùëùùëñ = ùëÅùëñ/ùëõ denote the empirical proportion of observations of class ùëñ, and let ùëû be a weight vector.
The empirical ùëû-weighted risk is

ùëó=1 ‚Ñì(ùëì, ùëßùëó)1 {ùë¶ùëñ = ùëñ} where ùëÅùëñ = ‚àëÔ∏Äùëõ

ÃÇÔ∏ÄùëÖùëû(ùëì ) =

ùëò
‚àëÔ∏Å

ùëñ=1

ùëûùëñ ÃÇÔ∏Äùëùùëñ ÃÇÔ∏ÄùëÖùëñ(ùëì ).

The empirical ùëÑ-weighted risk is defined analogously by ÃÇÔ∏ÄùëÖùëÑ = supùëû‚ààùëÑ ÃÇÔ∏ÄùëÖùëû(ùëì ). This problem is convex
in ùëì when the loss ‚Ñì is convex and concave in ùëû due to linearity; so one may solve the resulting
saddle-point problem with standard techniques such as gradient descent-ascent, which we give in
the appendix.

Often in empirical risk minimization, generalization bounds are provided, i.e., a bound on the
true risk of a classifier ùëì in ‚Ñ± in terms of its empirical risk and a variance term. To bring our
results closer to those of plug-in estimation, we also consider a form of excess risk. To distinguish
the two, define the excess (‚Ñ±, ùëÑ)-weighted risk to be ‚Ñ∞ùëÑ(‚Ñ±) = ùëÖùëÑ( ÃÇÔ∏Äùëì ) ‚àí ùëÖùëÑ(ùëì *
ùëÑ) where here ÃÇÔ∏Äùëì is the
ùëÑ-weighted empirical risk minimizer in ‚Ñ± and ùëì *
ùëÑ is the population ùëÑ-weighted risk minimizer in
‚Ñ±. Beyond the robust formulation, the key difference between excess ùëû-weighted risk and excess
(‚Ñ±, ùëÑ)-weighted risk is that in the former we compete with the true regression function, and in the
latter we compete with the best classifier in ‚Ñ±.

One additional tool we need for empirical risk minimization is a measure of function class
complexity, and a typical measure of the expressiveness of a function class is Rademacher complexity.
The empirical Rademacher complexity given a sample (ùëã1, ùëå1), . . . , (ùëãùëõ, ùëåùëõ) is

ÃÇÔ∏ÄRùëõ(‚Ñ±) = Eùúé sup
ùëì ‚àà‚Ñ±

ùëõ
‚àëÔ∏Å

ùëñ=1

ùúéùëñùëì (ùëãùëñ),

where the expectation is taken with respect to the ùúéùëñ, which are Rademacher random variables.
The Rademacher complexity is Rùëõ(‚Ñ±) = E ÃÇÔ∏ÄRùëõ(‚Ñ±), where the expectation is with respect to the ùëãùëñ
random variables.

Finally, we make one note about the loss for our empirical risk minimization results. For binary
classification, one can obtain bounds for any bounded loss function that is Lipschitz continuous in
ùëì (ùë•). Since we present multiclass results, we use the multiclass margin loss, which is a bounded
version of the multiclass hinge loss (Mohri et al., 2012). Here, it is assumed that for each ùëñ in ùí¥, the
function ùëì outputs a score ùëìùëñ(ùë•), and the chosen class is argmaxùëñ‚ààùí¥ ùëìùëñ(ùë•). The multiclass margin loss
is defined as ‚Ñìmar(ùëì, ùëß) = Œ¶ (Ô∏Äùëìùë¶(ùë•) ‚àí maxùë¶‚Ä≤Ã∏=ùë¶ ùëìùë¶‚Ä≤(ùë•))Ô∏Ä where Œ¶(ùëé) = 1 {ùëé ‚â§ 0} + (1 ‚àí ùëé)1 {0 < ùëé ‚â§ 1}.
For simplicity, we ignore the margin parameter, usually denoted by ùúå, and treat it as 1 in our results.
Finally, we define the projection set Œ†1(‚Ñ±) = {ùë• ‚Ü¶‚Üí ùëìùë¶(ùë•) : ùë¶ ‚àà ùí¥, ùëì ‚àà ‚Ñ±} .

3 Tradeoffs with Class Weighted Risk

In this section, we examine weighted plug-in classification, and we have two main results. First,
we show that weighted plug-in classification enjoys essentially the same rate of convergence as
unweighted plug-in classification, although there is dependence on the chosen weights. Second,
there is a fundamental trade-off in that optimizing for one set of weights ùëû may lead to suboptimal
performance for another set of weights ùëû‚Ä≤.

3.1 Excess Risk Bounds

We start with the excess risk bound for plug-in estimators when the weighting is well-specified.

5

Figure 1. The irreducible error (IE) and estimation error (EE). The irreducible error is the measure of the
set of ùë• where ùúÇ(ùë•) is between thresholds of ùëû‚Ä≤ and ùëû, which does not depend on ÃÇÔ∏ÄùúÇ. The estimation error is
the measure of the ùë• for which ÃÇÔ∏ÄùúÇ(ùë•) and ùúÇ(ùë•) lead to different plug-in estimates.

Proposition 1. Suppose the regression function ùúÇ is ùõΩ-H√∂lder. Then, the ùëû-weighted excess risk of
ÃÇÔ∏Äùëìùëû satisfies

E‚Ñ∞ùëû( ÃÇÔ∏Äùëìùëû) ‚â§ ùëÇ

(Ô∏Ç

(ùëû0 + ùëû1)ùëõ‚àí ùõΩ

2ùõΩ+ùëë

)Ô∏Ç

.

Here, we see that the upper bound depends linearly on ùëû0 and ùëû1. This implies that when we
increase the weight for a class with few examples, then our bound on the excess risk increases. While
previous cost weighting setups have normalized the sum of weights Scott (2012), our normalization
scheme is computed with respect to prior probabilities on each class as well, and consequently we
explicitly include ùëû0, ùëû1 in our bound. Our choice of domain for weights is defined in Section 4.

Now, we turn to our second task: examining the weighted excess risk of the ÃÇÔ∏Äùëìùëû under a different

weighting ùëû‚Ä≤. Observe that we can decompose the excess risk as
+ ùëÖùëû‚Ä≤(ùëì *

E‚Ñ∞ùëû‚Ä≤( ÃÇÔ∏Äùëìùëû) = EùëÖùëû‚Ä≤( ÃÇÔ∏Äùëìùëû) ‚àí ùëÖùëû‚Ä≤(ùëì *
ùëû )

‚èü

 ‚èû
estimation error

‚èü

=: (EE) + (IE).

ùëû ) ‚àí ùëÖùëû‚Ä≤(ùëì *
ùëû‚Ä≤)
 ‚èû
irreducible error

(1)

Unsurprisingly, we see that an error term that is constant, or ‚Äùirreducible‚Äù appears in equation (1).
Then, we see the irreducible error is given by the measure of the subset of ùí≥ where ùúÇ(ùë•) lies between
ùë°ùëû and ùë°ùëû‚Ä≤. Given that we know the Bayes optimal classifier for any weighting, we observe that the
irreducible error can be upper bounded by a term proportional to the the product of the measure
of ùëÉùëã in the region between ùë°ùëû and ùë°ùëû‚Ä≤, and the difference between the thresholds themselves. We
state this formally in the following proposition.

Proposition 2. Let ùë°ùëû,ùëû‚Ä≤ = min{ùë°ùëû, ùë°ùëû‚Ä≤} and ùë°ùëû,ùëû‚Ä≤ = max{ùë°ùëû, ùë°ùëû‚Ä≤}. The irreducible error satisfies the
bound

(IE) ‚â§ (ùëû‚Ä≤

0 + ùëû‚Ä≤

1) ‚Éí

‚Éíùë°ùëû ‚àí ùë°ùëû‚Ä≤

‚Éí P (Ô∏Å
‚Éí

ùë°ùëû,ùëû‚Ä≤ ‚â§ ùúÇ(ùëã) ‚â§ ùë°ùëû,ùëû‚Ä≤

)Ô∏Å

6

 
 
A visualization is given in Figure 1. Now, we turn to analyze the estimation error. The result is
in many ways similar to Proposition 1, but an additional term appears due to the decision threshold
ùë°ùëû for ÃÇÔ∏ÄùúÇ differing from that of the risk measurement ùë°ùëû‚Ä≤.
Proposition 3. For any density estimator ÃÇÔ∏ÄùúÇ, the estimation error satisfies
(Ô∏Ç
ùëõ‚àí ùõΩ

(EE) ‚â§ (ùëû‚Ä≤

)Ô∏Å]Ô∏Å)Ô∏Ç

+ ‚Éí

ÃÇÔ∏Äùëìùëû(ùë•) Ã∏= ùëì *

‚Éí E [Ô∏ÅP (Ô∏Å
‚Éí

‚Éíùë°ùëû‚Ä≤ ‚àí ùë°ùëû

2ùõΩ+ùëë

ùëÇ

0 + ùëû‚Ä≤
1)

ùëû (ùë•)

(Ô∏Ç

)Ô∏Ç

Corollary 1. When ùúÇ is ùõΩ-H√∂lder, using local polynomial estimator Yang (1999) for ÃÇÔ∏ÄùúÇ gives

(EE) ‚â§(ùëû‚Ä≤

0 + ùëû‚Ä≤

1)ùëÇ

(Ô∏Ç
ùëõ‚àí ùõΩ

2ùõΩ+ùëë

)Ô∏Ç

+ (ùëû‚Ä≤

0 + ùëû‚Ä≤

1) ‚Éí

‚Éíùë°ùëû‚Ä≤ ‚àí ùë°ùëû

‚Éí E [Ô∏ÅP (Ô∏Å
‚Éí

ÃÇÔ∏Äùëìùëû(ùë•) Ã∏= ùëì *

ùëû (ùë•)

)Ô∏Å]Ô∏Å

Consequently, we can upper bound the expected excess ùëû‚Ä≤-risk. The probability in the bound
of the estimation error has been considered in the context of nearest neighbors (Chaudhuri and
Dasgupta, 2014), but in general, additional assumptions are required to provide an explicit rate.
We consider one such assumption in the appendix.

4 Robust Class Weighted Risk

Based the results in the previous section, we know that the performance degradation need not be
graceful when we don‚Äôt know how to choose the weights. This motivates us to study a more robust
version of class weighted risk.

Definition 2. Let ùëÑ ‚äÜ R|ùí¥| be a compact convex set such that ùëûùëñ ‚â• 0 for each ùëñ and E[ùëûùëå ] = 1 for
each ùëû in ùëÑ. Then, the ùëÑ-weighted risk is

ùëÖùëÑ(ùëì ) = sup
ùëû‚ààùëÑ

E [ùëûùëå ùëÖùëå (ùëì )] = sup
ùëû‚ààùëÑ

ùëûùëñùëùùëñùëÖùëñ(ùëì ).

‚àëÔ∏Å

ùëñ‚ààùí¥

Additionally, we refer to the set ùëÑ as the uncertainty set.

In this section, we have two goals: (1) to provide excess ‚Ñ±-risk bounds and generalization
bounds for robust weighted risk via uniform convergence and (2) to make connections to stochastic
optimization via special choices of uncertainty set. We start with generalization; the proofs are
given in the appendix.
Theorem 1. Let ‚Ñì = ‚Ñìmar be the multiclass margin loss. Recall that ùëÅùëñ = ‚àëÔ∏Äùëõ
probability at least 1 ‚àí ùõø, we have the generalization bound

ùëó=1 1 {ùë¶ùëó = ùëñ}. With

ùëÖùëÑ(ùëì ) ‚â§ sup
ùëû‚ààùëÑ

‚éß
‚é™‚é®

‚é™‚é©

ÃÇÔ∏ÄùëÖùëû(ùëì ) +

ùëò
‚àëÔ∏Å

ùëñ=1

ùëûùëñùëùùëñ √ó

‚éõ
‚éù4ùëòE
‚éú

[Ô∏Ç ùëÅùëñ
ùëùùëñùëõ

]Ô∏Ç
ÃÇÔ∏ÄRùëÅùëñ(Œ†1(‚Ñ±))

+

‚éØ
‚é∏
‚é∏
‚é∑

log ùëò
ùõø
2ùëù2
ùëñ ùëõ

‚éû

‚éü
‚é†

‚é´
‚é™‚é¨

‚é™‚é≠

for every ùëì in ‚Ñ± and the excess risk bound

‚Ñ∞ùëÑ(‚Ñ±) ‚â§ 2 sup
ùëû‚ààùëÑ

ùëò
‚àëÔ∏Å

ùëñ=1

ùëûùëñùëùùëñ √ó

‚éõ
‚éù8ùëòE
‚éú

[Ô∏Ç ùëÅùëñ
ùëùùëñùëõ

]Ô∏Ç
ÃÇÔ∏ÄRùëÅùëñ(Œ†1(‚Ñ±))

+

‚éØ
‚é∏
‚é∏
‚é∑

log ùëò
ùõø
2ùëù2
ùëñ ùëõ

‚éû

‚éü
‚é† .

7

A few remarks are in order. First, note that we only use the multiclass margin loss because it
leads to simple multiclass bounds. In a binary classification setting, standard results would imply
generalization for other Lipschitz losses. Second, in many cases, we can simplify the Rademacher
complexity term. The following result applies to commonly-used function classes such as linear
functions and neural networks (Bartlett et al., 2017; Golowich et al., 2018; Mohri et al., 2012).

Corollary 2. Let ‚Ñì = ‚Ñìmar be the multiclass margin loss. Let ‚Ñ± be a function class satisfying
ÃÇÔ∏ÄRùëõ(Œ†1(‚Ñ±)) ‚â§ ùê∂(‚Ñ±)ùëõ‚àí1/2 for some constant ùê∂(‚Ñ±) that does not depend on ùëõ. Then with probability
at least 1 ‚àí ùõø, we have the generalization bound

ùëÖùëÑ(ùëì ) ‚â§ sup
ùëû‚ààùëÑ

and the excess (‚Ñ±, ùëû)-risk bound

‚éß
‚é™‚é®

‚é™‚é©

ÃÇÔ∏ÄùëÖùëû(ùëì ) +

ùëò
‚àëÔ∏Å

ùëñ=1

ùëûùëñùëùùëñ √ó

‚éõ

‚éú
‚éù

4ùëòùê∂(‚Ñ±)
ùëùùëñùëõ

‚àö

‚éØ
‚é∏
‚é∏
‚é∑

+

log ùëò
ùõø
2ùëù2
ùëñ ùëõ

‚éû

‚éü
‚é†

‚é´
‚é™‚é¨

‚é™‚é≠

‚Ñ∞ùëÑ(‚Ñ±) ‚â§ 2 sup
ùëû‚ààùëÑ

‚éõ

‚éú
‚éù

ùëò
‚àëÔ∏Å

ùëñ=1

ùëûùëñùëùùëñ

8ùëòùê∂(‚Ñ±)
ùëùùëñùëõ

‚àö

‚éØ
‚é∏
‚é∏
‚é∑

+

‚éû

‚éü
‚é† .

log ùëò
ùõø
2ùëù2
ùëñ ùëõ

4.1 Connections to Stochastic Programming

In this section, we make concrete connections to stochastic programming (Shapiro et al., 2009).
First, we introduce label conditional value at risk, and then we describe the generalization, label
heterogeneous conditional value at risk.

4.1.1 Label CVaR

We start with the definition.
Definition 3. Let ùõº in (0, 1) be given. Define the set ùëÑùõº = {Ô∏Äùëû : E[ùëûùëå ] = 1, ùëûùëñ ‚àà [Ô∏Ä0, ùõº‚àí1]Ô∏Ä for ùëñ ‚àà 1, . . . , ùëò}Ô∏Ä .
The label conditional value at risk (LCVaR) is LCVaRùõº(ùëì ) = ùëÖùëÑùõº(ùëì ).

ùõº

ùõº

EùëÑ[ùëç] = supùëÑ‚ààùëÑ*

E[(ùëëùëÑ/ùëëùëÉ )ùëç], where ùëÑ*

Now, we describe the connection to CVaR. Letting ùëç be a random variable, the CVaR of ùëç
at level ùõº is CVaRùõº(ùëç) = supùëÑ‚ààùëÑ*
ùõº is the set of all
probability measures that are absolutely continuous with respect to the underlying measure ùëÉ such
that ùëëùëÑ/ùëëùëÉ ‚â§ ùõº‚àí1. If ùëç takes values on a finite discrete probability space with probability mass
function ùëù, then the CVaR may be written as CVaRùõº(ùëç) = supùëû‚ààùëÑùõº
ùëñ=1 ùëûùëñùëùùëñùëç. Thus, LCVaR is
a specialization of CVaR to the variables ùëÖùëå (ùëì ), which take values on the finite discrete space ùí¥.
Notably, this is in contrast to other uses of CVaR in machine learning where, as noted previously,
CVaR is used with respect to samples directly, in order to provide robustness or fairness. As with
CVaR, LCVaR is a straightforward way to provide robustness. Intuitively, it moves weight to the
worst losses, where all weightings are bounded by the same constant ùõº‚àí1. Now, we consider the
dual form.

‚àëÔ∏Äùëò

Proposition 4 (LCVaR dual form). LCVaR permits the dual formulation

LCVaRùõº(ùëì ) = inf
ùúÜ‚ààR

E[(ùëÖùëå (ùëì ) ‚àí ùúÜ)+] + ùúÜ

}Ô∏Ç

.

{Ô∏Ç 1
ùõº

Moreover, if ‚Ñ± is compact in the supremum norm on ùí≥ and ‚Ñì is continuous, then the dual form
holds for all ùëì in ‚Ñ±.

8

The proof is mostly standard and therefore deferred to the appendix. The only trick compared
with CVaR is showing that we may restrict the domain of ùúÜ to a compact set; which essentially
requires showing that the process {ùëÖùëå (ùëì ) : ùëì ‚àà ‚Ñ±} is sufficiently well-behaved. It would also
suffice to assume that ‚Ñì is bounded, as with most theoretical results in learing theory. Note that to
minimize LCVaR, we can solve this convex program in ùúÜ and ùëì .

4.1.2 Label Heterogeneous CVaR

While the LCVaR approach of the previous section is useful for providing some robustness in a
computationally tractable manner, it may not be best suited for imbalanced classification because
it treats all classes identically in that each ùëûùëñ must lie in the interval [0, ùõº‚àí1]. Since imbalanced
classification is inherently a problem of heterogeneity, we may wish to allow ùëûùëñ to be in some interval
[0, ùõº‚àí1
ùëñ

] instead. We can formalize this problem as follows.

Definition 4. Define the uncertainty set ùëÑùêª,ùõº =
. We
call the resulting optimization problem label heterogeneous conditional value at risk (LHCVaR), and
we write

] for ùëñ = 1, . . . , ùëò

ùëñ

{Ô∏Å
ùëû : E[ùëûùëå ] = 1, ùëûùëñ ‚àà [0, ùõº‚àí1

}Ô∏Å

LHCVaRùõº(ùëì ) = sup

ùëû‚ààùëÑùêª,ùõº

E [ùëûùëå ùëÖùëå (ùëì )] .

Similar to LCVaR, this has a dual form.

Proposition 5. A dual form for LHCVaR is given by

LHCVaRùõº(ùëì ) = inf
ùúÜ‚ààR

E [Ô∏Å

ùõº‚àí1

ùëå (ùëÖùëå (ùëì ) ‚àí ùúÜ)+

]Ô∏Å

+ ùúÜ.

Moreover, if ‚Ñ± is compact in the supremum norm on ùí≥ and ‚Ñì is continuous, then the dual form
holds for all ùëì in ‚Ñ±.

Again, we note that an alternative sufficient condition for the dual to hold for all ùëì in ‚Ñ± is that
‚Ñì be bounded. Importantly, the label heterogeneous CVaR dual form is convex in ùëì and ùúÜ. As a
result, we can still optimize efficiently, in principle.

We also note that the finite dimension ùëò is crucial for label heterogeneous CVaR. This is due to
our use of the minimax theorem, which requires compactness in various places; so in general this
result cannot be extended to the infinite-dimensional case.

5 Numerical Results

5.1 Methods

We examine the empirical performance of LCVaR and LHCVaR risks, and compare them against
the standard risk and a balanced risk as baselines. Let ÃÇÔ∏Äùëùùëñ be the empirical proportion of the ùëñth
label and ÃÇÔ∏ÄùëÖùëñ be the empirical class conditional risk.

Balanced risk Here, we consider the specific weighting where each class is equally weighted:

i.e., we fix ùëûùëñ = 1/(ùëò ÃÇÔ∏Äùëùùëñ).

ÃÇÔ∏ÄùëÖ1/(ùëòÃÇÔ∏Äùëù)(ùëì ) =

1
ùëò

ùëò
‚àëÔ∏Å

ùëñ=1

ÃÇÔ∏ÄùëÖùëñ(ùëì )

9

(a) Class 0 risk

(b) Class 1 risk

(c) Worst class risk

Figure 2. Plots of class 0, class 1, and worst class risk on the test dataset under different choices of 1 ‚àí ùëù
in the synthetic experiment. The worst test class risk is the maximum of the risks of the two classes for
each choice of the probability of class 0. LCVaR and LHCVaR performs better in worst class risk than both
standard and balanced risks as class imbalance increases.

LCVaR The empirical formulation optimizes the dual formulation, in which ùõº is a hyperparameter:

(cid:92)LCVaRùõº(ùëì ) = min
ùúÜ‚ààR

{Ô∏É

1
ùõº

ùëò
‚àëÔ∏Å

ùëñ=1

ÃÇÔ∏Äùëùùëñ( ÃÇÔ∏ÄùëÖùëñ(ùëì ) ‚àí ùúÜ)+ + ùúÜ

.

}Ô∏É

(2)

LHCVaR We similarly optimize a dual form in the empirical LHCVaR risk. To reduce the number
of hyperparameters to only ùëê ‚àà (0, 1] and ùúÖ ‚àà (0, ‚àû), we calculate ùõºùëñ as follows:

(Ô∏É

ùõº(ùúÖ,ùëê)
ùëñ

= ùëê

1/ùúÖ

ÃÇÔ∏Äùëùùëñ
ùëó=1 ÃÇÔ∏Äùëùùëó

‚àëÔ∏Äùëò

)Ô∏É

.

1/ùúÖ

(3)

ùúÖ behaves as a temperature parameter (similar to Jang et al. 2016; Wang et al. 2020) and causes
ùõº to become a smoother distribution of weights when ùúÖ > 1 and converge to uniform weights as
ùúÖ ‚Üí ‚àû. Conversely, when ùúÖ < 1, the alpha distribution becomes sharper and heavily weights the
classes with lowest ÃÇÔ∏Äùëùùëñ as ùúÖ ‚Üí 0. We simply choose a ùúÖ of 1 unless otherwise stated. ùëê consequently
characterizes the total magnitude of the weights. Ultimately, we formulate the empirical risk as:

(cid:92)LHCVaRùúÖ,ùëê(ùëì ) = inf
ùúÜ‚ààR

{Ô∏É ùëò

‚àëÔ∏Å

ùëñ=1

ÃÇÔ∏Äùëùùëñ
ùõº(ùúÖ,ùëê)
ùëñ

( ÃÇÔ∏ÄùëÖùëñ(ùëì ) ‚àí ùúÜ)+ + ùúÜ

}Ô∏É

We train a logistic regression model with gradient descent on a cross entropy loss, which acts as

a convex surrogate loss for zero-one risk.

5.2 Datasets

We evaluate our methods on both synthetic and real datasets.

Synthetic Datasets The data in our synthetic experiment is constructed for ùí≥ = [0, 1] and
ùí¥ = {0, 1}. For a given ùëù = ùëÉ (ùëå = 0), we generated a dataset by uniformly randomly sampling an
ùëã in [0, 1] and sampling a ùëå with the following distribution:

ùëÉ (ùëå = 1 | ùëã = ùë•) = ùë•

ùëù
1‚àíùëù

ùëÉ (ùëå = 0 | ùëã = ùë•) = 1 ‚àí ùë•

ùëù
1‚àíùëù .

10

(a) Varying ùõº for LCVaR

(b) Varying ùúÖ for LHCVaR with ùõº =
0.01

Figure 3. Worst class risk of different ùõº values for LCVaR and ùúÖ values for LHCVaR in the synthetic setting.
Across different levels of class imbalance, ùõº and ùúÖ do not have a significant impact on worst class risk of
LCVaR and LHCVaR.

In these synthetic datasets, we note that the Bayes optimal classifier and class risks are:

{Ô∏É

ùëì *(ùë•) = 1

ùë• >

)Ô∏Ç 1‚àíùëù
ùëù

}Ô∏É

(Ô∏Ç 1
2

)Ô∏Ç 1
ùëù

(Ô∏Ç 1
2

ùëÖ0(ùëì *) = 1 ‚àí (1 + ùëù)

ùëÖ1(ùëì *) =

)Ô∏Ç 1
ùëù

.

(Ô∏Ç 1
2

When ùëù is high, ùëÖ0(ùëì *) < ùëÖ1(ùëì *), which leads to a classifier that has vastly worse performance
on class 1 compared to class 0. This discrepancy in class risk is a common issue in classification
problems where there is a significant class imbalance.

We randomly generated 100,000 data points for both train and test sets. We generated datasets

for each value of ùëù from 0.80 to 0.98, inclusive, in steps of 0.02.

Real World Datasets We also experiment on the Covertype dataset taken from the UCI dataset
repository Dua and Graff (2017). This dataset is 53-dimensional with 7 classes and has 2%-98%
(11340-565892 examples) train-test split.

(a) Standard

(b) Balanced

(c) LCVaR

(d) LHCVaR

Figure 4. Histogram of class risks for each method on the Covertype dataset. The red line marks the largest
risk for each method. The distribution of class risks for standard and balanced methods are more spread out,
while the class risks for LCVaR and LHCVaR are more concentrated near the max class risk. The max class
risks are slightly lower for LCVaR and LHCVaR compared to the other two methods.

11

5.3 Results

Synthetic In Fig. 2, we can observe that the the worst case class risk of LCVaR and LHCVaR
across multiple values of ùëù is better than both the standard and balanced classifier. The classwise
risks of LCVaR and LHCVaR are relatively close across different values of ùëù, while there is a large
discrepancy between classwise risks of the classifier trained under the standard or balanced risks.
Note that the more significant the imbalance, i.e., the smaller the ùëù, the better LCVaR and LHCVaR
perform compared to balanced risk on class 0, while paying a progressively smaller price on the class
1 risk. The same is also true between both LCVaR and LHCVaR and the standard risk, although
with the classes swapped. We note that while the worst class risk of LCVaR and LHCVaR seem
to decrease with greater imbalance, this may not be a general property of these methods. Rather,
this is more likely an artifact of the synthetic setup having more probability mass further from the
decision boundary as the imbalance increases. The main observation is simply that LCVaR and
LHCVaR have lower worst class risk in comparison to the baseline methods. Thus, this empirically
demonstrates that both LCVaR and LHCVaR can significantly improve the highest class risks while
losing little in performance on classes with lower risks.

In addition to comparing against baselines, we also examine the effect of different choices of ùõº
and ùúÖ on LCVaR and LHCVaR, respectively. The results of this comparison are in Fig. 3. In both
methods, varying the hyperparameters does not have a dramatic impact on the behavior of the
worst class risk for both these methods across different values of class imbalance.

Table 1. Standard risk and risk of the worst class for each method on the Covertype dataset. LCVaR and
LHCVaR improve on the worst class risk.

Method Standard Risk Worst Class Risk
LHCVaR
LCVaR
Standard
Balanced

0.3979
0.3384
0.3275
0.3765

0.4907
0.5037
0.5111
0.5333

Table 2. Performance of LCVaR across different ùõº values, and LHCVaR across different ùúÖ values. The
performance each method is relatively agnostic to choices of ùõº and ùúÖ, although the smallest choices of ùõº and
ùúÖ for each method have the largest changes in worst class risk, respectively.

Method

ùõº

ùúÖ

Standard Risk Worst Class Risk

LCVaR

LHCVaR

0.01 N/A
0.05 N/A
0.1 N/A
0.8
0.05
1
0.05
1.2
0.05

0.4266
0.3993
0.4060
0.4308
0.3979
0.4171

0.5474
0.4932
0.5037
0.5408
0.4907
0.5050

In Table 1, we observe that LCVaR and LHCVaR have better worst class risks than the
Real
standard and class weighted baselines. However, improving worst class risk comes at a cost to to the
standard risk in the case of both LCVaR and LHCVaR. This tradeoff is reflected in the histograms of
class risk shown in Fig. 4, where the class risks under the standard and balanced classifiers are more
spread out and have classes with much lower risks. On the other hand, LCVaR and LHCVaR have
class risk distributions that are more concentrated towards the worst class risk value. Consequently,

12

LCVaR and LHCVaR achieve a lower worst class risk, which is consistent with our theory.

We also compare the effect of choosing different ùõº and ùúÖ on LCVaR and LHCVaR, respectively,
in Table 2. We see that the worst class risk still performs well under different choices of ùõº and
ùúÖ, although there is some degradation when the ùõº is smaller than optimal choice, in the case of
LCVaR, and when ùúÖ is smaller and produces a sharper distribution, in the case of LHCVaR.

6 Discussion

In this work, we have studied the effect of optimizing classifiers with respect to different weightings
and developed robust risk measures that minimizes worst case weighted risk across a set of weightings.
We subsequently show that optimizing with respect to LCVaR and LHCVaR empirically improves the
worst class risk, at a reasonable cost to accuracy. One future direction for research is to understand
the Bayes optimal classifier under LCVaR and LHCVaR. Another more applied direction could
be to consider domain shift. If we formalize each prior over the classes as a weighting, optimizing
LCVaR or LHCVaR may improve performance when the test class priors are different from the
training class priors.

References

J.-Y. Audibert and A. B. Tsybakov. Fast learning rates for plug-in classifiers. The Annals of

Statistics, 35(2):608‚Äì633, 2007.

P. L. Bartlett, D. J. Foster, and M. J. Telgarsky. Spectrally-normalized margin bounds for neural

networks. In Advances in Neural Information Processing Systems, pages 6240‚Äì6249, 2017.

A. Ben-Tal and A. Nemirovski. Robust solutions of uncertain linear programs. Operations research

letters, 25(1):1‚Äì13, 1999.

A. Ben-Tal and A. Nemirovski. Robust solutions of linear programming problems contaminated

with uncertain data. Mathematical programming, 88(3):411‚Äì424, 2003.

A. Ben-Tal, A. Goryashko, E. Guslitzer, and A. Nemirovski. Adjustable robust solutions of uncertain

linear programs. Mathematical Programming, 99(2):351‚Äì376, 2004.

A. Ben-Tal, L. El Ghaoui, and A. Nemirovski. Robust Optimization. Princeton University Press,

2009.

A. Ben-Tal, D. Den Hertog, A. De Waegenaere, B. Melenberg, and G. Rennen. Robust solutions of
optimization problems affected by uncertain probabilities. Management Science, 59(2):341‚Äì357,
2013.

D. Bertsimas, V. Gupta, and N. Kallus. Robust sample average approximation. Mathematical

Programming, pages 1‚Äì66, 2014.

K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma. Learning imbalanced datasets with label-

distribution-aware margin loss. arXiv preprint arXiv:1906.07413, 2019.

K. Chaudhuri and S. Dasgupta. Rates of convergence for nearest neighbor classification. In Advances

in Neural Information Processing Systems, pages 3437‚Äì3445, 2014.

13

N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. SMOTE: synthetic minority

over-sampling technique. Journal of Artificial Intelligence Research, 16:321‚Äì357, 2002.

L. Devroye, L. Gy√∂rfi, and G. Lugosi. A probabilistic theory of pattern recognition. Springer Science

& Business Media, 1996.

P. Domingos. Metacost: A general method for making classifiers cost-sensitive. In KDD, volume 99,

pages 155‚Äì164, 1999.

D. Dua and C. Graff. Uci machine learning repository, 2017.

J. Duchi and H. Namkoong. Learning models with uniform performance via distributionally robust

optimization. arXiv preprint arXiv:1810.08750, 2018.

J. C. Duchi, T. Hashimoto, and H. Namkoong. Distributionally robust losses against mixture

covariate shifts. Arxiv, 2018.

V. Feldman. Does learning require memorization? a short tale about a long tail. arXiv preprint

arXiv:1906.05271, 2019.

A. Fern√°ndez, S. Garc√≠a, M. Galar, R. C. Prati, B. Krawczyk, and F. Herrera. Learning from

imbalanced data sets. Springer, 2018.

J. Goh and M. Sim. Distributionally robust optimization and its tractable approximations. Operations

research, 58(4-part-1):902‚Äì917, 2010.

N. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks.

In Conference On Learning Theory, pages 297‚Äì299, 2018.

L. Gy√∂rfi. The Rate of Convergence of kn-NN Regression Estimates and Classification Rule. IEEE

Transactions on Information Theory, 27(3):357‚Äì362, 1981. ISSN 0018-9448.

E. Hazan. Introduction to online convex optimization. Foundations and Trends R‚óã in Optimization,

2(3-4):157‚Äì325, 2016.

H. He and E. A. Garcia. Learning from imbalanced data. IEEE Transactions on knowledge and

data engineering, 21(9):1263‚Äì1284, 2009.

E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint

arXiv:1611.01144, 2016.

J. Khan, J. S. Wei, M. Ringner, L. H. Saal, M. Ladanyi, F. Westermann, F. Berthold, M. Schwab,
C. R. Antonescu, C. Peterson, et al. Classification and diagnostic prediction of cancers using gene
expression profiling and artificial neural networks. Nature medicine, 7(6):673, 2001.

O. O. Koyejo, N. Natarajan, P. K. Ravikumar, and I. S. Dhillon. Consistent Binary Classification
with Generalized Performance Metrics. In Advances in Neural Information Processing Systems
27, pages 2744‚Äì2752. Curran Associates, Inc., 2014.

A. Krzyzak and M. Pawlak. The pointwise rate of convergence of the kernel regression estimate.

Journal of Statistical Planning and Inference, 16:159‚Äì166, 1987.

V. Kuznetsov, M. Mohri, and U. Syed. Rademacher complexity margin bounds for learning with a
large number of classes. In ICML Workshop on Extreme Classification: Learning with a Very
Large Number of Labels, 2015.

14

Y. Lee, G. Wahba, and S. A. Ackerman. Cloud classification of satellite radiance data by multicategory
support vector machines. Journal of Atmospheric and Oceanic Technology, 21(2):159‚Äì169, 2004.

D. D. Lewis. Evaluating and optimizing autonomous text classification systems. In SIGIR, volume 95,

pages 246‚Äì254. Citeseer, 1995.

Y. Lin, Y. Lee, and G. Wahba. Support vector machines for classification in nonstandard situations.

Machine learning, 46(1-3):191‚Äì202, 2002.

Y.-C. Lin, P. Das, and A. Datta. Overview of the SIGIR 2018 eCom Rakuten Data Challenge. In

eCOM@ SIGIR, 2018.

G. Mariani, F. Scheidegger, R. Istrate, C. Bekas, and C. Malossi. Bagan: Data augmentation with

balancing gan. arXiv preprint arXiv:1803.09655, 2018.

A. Menon, H. Narasimhan, S. Agarwal, and S. Chawla. On the statistical consistency of algorithms
for binary classification under class imbalance. In International Conference on Machine Learning,
pages 603‚Äì611, 2013.

M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of Machine Learning. MIT Press, 2012.

H. Namkoong and J. C. Duchi. Variance-based regularization with convex objectives. In Advances

in Neural Information Processing Systems, pages 2971‚Äì2980, 2017.

H. Narasimhan, R. Vaish, and S. Agarwal. On the statistical consistency of plug-in classifiers for
non-decomposable performance measures. In Advances in Neural Information Processing Systems,
pages 1493‚Äì1501, 2014.

P. Rigollet and X. Tong. Neyman-pearson classification, convexity and stochastic constraints.

Journal of Machine Learning Research, 12(Oct):2831‚Äì2855, 2011.

R. T. Rockafellar, S. Uryasev, et al. Optimization of conditional value-at-risk. Journal of risk, 2:

21‚Äì42, 2000.

R. Salakhutdinov, A. Torralba, and J. Tenenbaum. Learning to share visual appearance for multiclass

object detection. In CVPR 2011, pages 1481‚Äì1488. IEEE, 2011.

C. Scott. Calibrated asymmetric surrogate losses. Electronic Journal of Statistics, 6:958‚Äì992, 2012.

A. Shapiro, D. Dentcheva, and A. Ruszczy≈Ñski. Lectures on stochastic programming: modeling and

theory. SIAM, 2009.

C. J. Stone. Optimal Global Rates of Convergence for Nonparametric Regression. The Annals of

Statistics, 10(4):1040‚Äì1053, 1982.

X. Tong. A plug-in approach to neyman-pearson classification. The Journal of Machine Learning

Research, 14(1):3011‚Äì3040, 2013.

X. Tong, Y. Feng, and A. Zhao. A survey on neyman-pearson classification and suggestions for
future research. Wiley Interdisciplinary Reviews: Computational Statistics, 8(2):64‚Äì81, 2016.

C. J. Van Rijsbergen. Foundation of evaluation. Journal of Documentation, 30(4):365‚Äì373, 1974.

C. J. Van Rijsbergen. Information Retrieval. Butterworth-Heinemann, London, 2nd edition, 1979.

15

J. Wang, X. Shen, and Y. Liu. Probability estimation for large-margin classifiers. Biometrika, 95

(1):149‚Äì167, 2008.

X. Wang, H. Helen Zhang, and Y. Wu. Multiclass probability estimation with support vector

machines. Journal of Computational and Graphical Statistics, pages 1‚Äì18, 2019.

X. Wang, Y. Tsvetkov, and G. Neubig. Balancing training for multilingual neural machine translation.

arXiv preprint arXiv:2004.06748, 2020.

Y. Wu, H. H. Zhang, and Y. Liu. Robust model-free multiclass probability estimation. Journal of

the American Statistical Association, 105(489):424‚Äì436, 2010.

Y. Yang. Minimax nonparametric classification. i. rates of convergence. IEEE Transactions on

Information Theory, 45(7):2271‚Äì2284, 1999.

Z.-H. Zhou and X.-Y. Liu. Training cost-sensitive neural networks with methods addressing the
class imbalance problem. IEEE Transactions on Knowledge and Data Engineering, 18(1):63‚Äì77,
2006.

X. Zhu, D. Anguelov, and D. Ramanan. Capturing long-tail distributions of object subcategories. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 915‚Äì922,
2014.

G. K. Zipf. The Psycho-Biology of Language: an Introduction to Dynamic Philology. George

Routledge & Sons, Ltd., 1936.

16

A Organization

Our appendices contain proofs, all of which are omitted from the main text, and additional details
on the weighting approach to imbalanced classification. In Appendix B, we prove our results for
plug-in classification. Additionally, we show that a threshold-shifted version of Tsybakov‚Äôs noise
condition implies precise rates for the convergence of expected excess risk. Finally, we briefly discuss
the universality of weighting, i.e., the fact that choosing the correct weighting is often the means to
optimizing other classification metrics, for a class of classification metrics.

In Appendix C, we show a result analogous to Proposition 3 for empirical risk minimization.
ùëû‚Ä≤ for weights

However, the result is less illuminating, since it depends on the optimal classifiers ùëì *
ùëû and ùëû‚Ä≤ within the class ‚Ñ±, which is difficult to analyze more precisely in any generality.

ùëû ùëì *

In Appendix D, we prove our results for robust weighting. This includes both the convergence
and duality results. In Appendix E, we prove the analog of Theorem 1 for the conditional sampling
model. The only difference to observe is that the bounded differences inequality is used with respect
to a different number of variables, which leads to a slightly stronger bound.

In Appendix F, we discuss gradient descent-ascent, which is a standard algorithm for solving
robust optimization problems. This may be used in cases where the uncertainty set ùëÑ does not
lead to LCVaR or LHCVaR. In Appendix G and Appendix H, we provide technical and standard
lemmas respectively.

Finally, we include additional experiment details, and an algorithm for analytically deriving

dual variables in the empirical LCVaR and LHCVaR formulations in Appendix I.

B Plug-in Classification Details

In this appendix, we provide additional details surrounding plug-in classification. We first start
with the proofs of results from the main text, and then we provide more concrete results based on
an additional assumption of that gives us faster rates of convergence. Finally, we provide details on
the universality of weighting.

For simplicity, we assume that our density estimator ÃÇÔ∏ÄùúÇ is a local polynomial estimator (Stone,
1982), but the properties that the estimator must have for the following proofs to succeed can also
be satisfied by other nonparametric estimators such as kernelized regression (Krzyzak and Pawlak,
1987), and nearest-neighbors regression (Gy√∂rfi, 1981).

B.1 Proofs

Proof of Lemma 1. By the definition of the ùëû-weighted risk and the tower property, we have

ùëÖ01,ùëû(ùëì ) = E[ùëûùëå ùëÖùëå (ùëì )]

= E [ùëû0(1 ‚àí ùúÇ(ùëã))E[1 {ùëì (ùëã) = 1} |ùëå = 0] + ùëû1ùúÇ(ùëã)E[1 {ùëì (ùëã) = 0} |ùëå = 1]]

=E [ùëû0(1 ‚àí ùúÇ(ùëã))1 {ùëì (ùëã) = 1} + ùëû1ùúÇ(ùëã)1 {ùëì (ùëã) = 0}] .
By inspection, we observe that the ùëì * minimizing the ùëû-risk satisfies

ùëì *(ùë•) =

{Ô∏É1
0

ùëû0(1 ‚àí ùúÇ(ùë•)) < ùëû1ùúÇ(ùë•)
ùëû0(1 ‚àí ùúÇ(ùë•)) > ùëû1ùúÇ(ùë•).

When ùëû0(1 ‚àí ùúÇ(ùë•)) = ùëû1ùúÇ(ùë•), we note that the decision may be arbitrary because it does not affect
the risk. So, by simple algebraic manipulation, we have

ùëì *(ùë•) = 1

{Ô∏Ç

ùúÇ(ùë•) ‚â•

ùëû0
ùëû0 + ùëû1

}Ô∏Ç

,

17

which completes the proof.

Now, we turn to Proposition 1, Proposition 2, and Proposition 3. Our proofs rely on the following
lemma of Yang (1999). First, we introduce a few additional definitions. Denote the ùúÄ-entropy of Œ£
with respect to the ùêøùëù norm for 1 ‚â§ ùëù ‚â§ ‚àû by ‚Ñã(ùúÄ, Œ£, ùêøùëù). We define the norm

‚ÄñÃÇÔ∏ÄùúÇ ‚àí ùúÇ‚Äñùêø1(ùëÉùëã ) =

‚à´Ô∏Å

|ùúÇ(ùë•) ‚àí ÃÇÔ∏ÄùúÇ(ùë•)| ùëëùëÉùëã

.

Lemma 2 (Theorem 1 of Yang 1999). Let ùúÇ be an element of Œ£ where Œ£ is a class of functions
from Rùëë to [0, 1]. Suppose the ùúÄ-entropy satisfies

‚Ñã(ùúÄ, Œ£, ùêøùëù) ‚â§ ùê∂ùúÄ‚àíùúå,

where ùê∂ > 0, ùúå > 0. Then the minimax upper bound on the mean convergence rate of any regression
estimator ÃÇÔ∏ÄùúÇ is

E [Ô∏Å

min
ÃÇÔ∏ÄùúÇ

max
ùúÇ‚ààŒ£

‚ÄñùúÇ ‚àí ÃÇÔ∏ÄùúÇ‚Äñùêø1(ùëÉùëã )

]Ô∏Å

‚â§ ùëÇ

(Ô∏Å

ùëõ‚àí 1

2+ùúå

)Ô∏Å

,

where the expectation is taken over the samples for estimating ÃÇÔ∏ÄùúÇ.

The upper bound converges at a rate of ùëÇ

where ùúå is a smoothness parameter for ùúÇ,
with standard assumptions on the function class of ùúÇ. For the class of ùõΩ-H√∂lder functions, ùúå = ùõΩ/ùëë,
which is our setting of interest.

(Ô∏Å

ùëõ‚àí1/(2+ùúå))Ô∏Å

Proof of Proposition 1. We start by bounding the excess ùëû-risk for a classifier ùëì by

‚Ñ∞ùëû(ùëì ) = ùëÖùëû(ùëì ) ‚àí ùëÖùëû(ùëì *
ùëû )

= (ùëû0 + ùëû1)

‚à´Ô∏Å ‚Éí
‚Éí
‚Éí
‚Éí

ùúÇ(ùë•) ‚àí

ùëû0
ùëû0 + ùëû1

‚Éí
‚Éí
‚Éí
‚Éí

1

{Ô∏Å

ùëì (ùë•) Ã∏= ùëì *

ùëû (ùë•)

ùëëùëÉùëã

}Ô∏Å

‚à´Ô∏Å

‚â§ (ùëû0 + ùëû1)

|ùúÇ(ùë•) ‚àí ÃÇÔ∏ÄùúÇ(ùë•)| ùëëùëÉùëã ,

where the upper bound follows when |ùúÇ(ùë•) ‚àí ùëû0/(ùëû0 + ùëû1)| ‚â§ |ùúÇ(ùë•) ‚àí ÃÇÔ∏ÄùúÇ(ùë•)| when ùëì (ùë•) Ã∏= ùëì *
applying Lemma 2 for ùõΩ-H√∂lder functions as noted above completes the proof.

ùëû (ùë•). Finally,

Proof of Proposition 2. The proposition follows from basic algebraic manipulations and one common
observation in nonparametric classification. We have

]Ô∏Å

ùëû (ùëã))

(IE) = E [Ô∏Å
‚à´Ô∏Å

=

ùëÖùëû‚Ä≤(ùëì *

ùëû‚Ä≤(ùëã)) ‚àí ùëÖùëû‚Ä≤(ùëì *
1ùúÇ(ùë•)‚Éí
{Ô∏Å

0(1 ‚àí ùúÇ(ùë•)) + ùëû‚Ä≤

‚Éí
‚Éíùëû‚Ä≤

‚à´Ô∏Å

0 + ùëû‚Ä≤
1)
1) ‚Éí

0 + ùëû‚Ä≤

‚Éí
‚ÉíùúÇ(ùë•) ‚àí ùë°ùëû‚Ä≤
‚Éí P (Ô∏Å
‚Éí

‚Éíùë°ùëû ‚àí ùë°ùëû‚Ä≤

‚Éí
‚Éí 1

= (ùëû‚Ä≤

‚â§ (ùëû‚Ä≤

ùëû‚Ä≤(ùë•) Ã∏= ùëì *
ùëì *
ùëû (ùë•)
)Ô∏Å

ùëû‚Ä≤(ùëã) Ã∏= ùëì *
ùëì *

ùëû (ùëã)

,

{Ô∏Å

‚Éí ùëëùëÉùëã 1

ùëû‚Ä≤(ùë•) Ã∏= ùëì *
ùëì *

ùëû (ùë•)

}Ô∏Å

}Ô∏Å

ùëëùëÉùëã

where in the inequality we use the fact that if ùëì *
‚Éí ‚â§ |ùë°ùëû,ùëû‚Ä≤ ‚àí ùë°ùëû,ùëû‚Ä≤| = ‚Éí
‚Éí
Thus, we have ‚Éí
‚Éíùë°ùëû ‚àí ùë°ùëû‚Ä≤

‚ÉíùúÇ(ùë•) ‚àí ùë°ùëû‚Ä≤

‚Éí
‚Éí .

ùëû‚Ä≤(ùëã) Ã∏= ùëì *

ùëû (ùëã) then ùúÇ(ùëã) must be in [ùë°ùëû,ùëû‚Ä≤, ùë°ùëû,ùëû‚Ä≤].

18

Proof of Proposition 3. Recall that the expected estimation error is

(EE) = E [Ô∏Å

ùëÖùëû‚Ä≤( ÃÇÔ∏Äùëìùëû) ‚àí ùëÖùëû‚Ä≤(ùëì *
ùëû )

]Ô∏Å

We can upper bound the term inside the expectation by

ùëÖ‚Ä≤

ùëû( ÃÇÔ∏Äùëìùëû) ‚àí ùëÖùëû‚Ä≤(ùëì *

ùëû ) =

‚à´Ô∏Å

‚à´Ô∏Å

=

0(1 ‚àí ùúÇ(ùë•))1{ ÃÇÔ∏Äùëìùëû(ùë•) = 1} + ùëû‚Ä≤
ùëû‚Ä≤
‚à´Ô∏Å

1ùúÇ(ùë•)1{ ÃÇÔ∏Äùëìùëû(ùë•) = 0}ùëëùëÉùëã

‚àí

0(1 ‚àí ùúÇ(ùë•))1{ùëì *
ùëû‚Ä≤

ùëû (ùë•) = 1} + ùëû‚Ä≤

1ùúÇ(ùë•)1{ùëì *

ùëû (ùë•) = 0}ùëëùëÉùëã

(ùëû‚Ä≤

0(1 ‚àí ùúÇ(ùë•)) ‚àí ùëû‚Ä≤

1ùúÇ(ùë•))1{ ÃÇÔ∏Äùëìùëû(ùë•) = 1, ùëì *
‚à´Ô∏Å

ùëû (ùë•) = 0}ùëëùëÉùëã

1{ ÃÇÔ∏Äùëìùëû(ùë•) = 0, ùëì *

ùëû (ùë•) = 1}ùëëùëÉùëã

+ (ùëû‚Ä≤

0(1 ‚àí ùúÇ(ùë•)))
‚Éí
‚Éí
‚Éí
‚Éí

1ùúÇ(ùë•) ‚àí ùëû‚Ä≤
‚à´Ô∏Å ‚Éí
ùëû‚Ä≤
‚Éí
0
‚Éí
0 + ùëû‚Ä≤
ùëû‚Ä≤
‚Éí
1
(Ô∏Ä|ùúÇ(ùë•) ‚àí ùë°ùëû| + ‚Éí

ùúÇ(ùë•) ‚àí

‚à´Ô∏Å

= (ùëû‚Ä≤

0 + ùëû‚Ä≤
1)

‚â§ (ùëû‚Ä≤

0 + ùëû‚Ä≤
1)

1{ ÃÇÔ∏Äùëìùëû(ùë•) Ã∏= ùëì *

ùëû (ùë•)}ùëëùëÉùëã

‚Éíùë°ùëû‚Ä≤ ‚àí ùë°ùëû

‚Éí
)Ô∏Ä 1{ ÃÇÔ∏Äùëìùëû(ùë•) Ã∏= ùëì *
‚Éí

ùëû (ùë•)}ùëëùëÉùëã ,

where we use the triangle inequality in the final line. Next, using the fact that |ùúÇ(ùë•)‚àíùë°ùëû| ‚â§ |ùúÇ(ùë•)‚àíÃÇÔ∏ÄùúÇ(ùë•)|
when ùëì (ùë•) Ã∏= ùëì *

ùëû (ùë•), we have

ùëÖ‚Ä≤

ùëû( ÃÇÔ∏Äùëìùëû) ‚àí ùëÖùëû‚Ä≤(ùëì *

ùëû ) ‚â§ (ùëû‚Ä≤

0 + ùëû‚Ä≤
1)

(Ô∏Ç‚à´Ô∏Å

|ùúÇ(ùë•) ‚àí ùë°ùëû| 1

+ ‚Éí

‚Éíùë°ùëû‚Ä≤ ‚àí ùë°ùëû

‚Éí P (Ô∏Å
‚Éí

ÃÇÔ∏Äùëìùëû(ùë•) Ã∏= ùëì *

ùëû (ùë•)

}Ô∏Å

ùëû (ùë•)

ùëëùëÉùëã

{Ô∏Å
ÃÇÔ∏Äùëìùëû(ùë•) Ã∏= ùëì *
)Ô∏Å )Ô∏Ç

(Ô∏Ç‚à´Ô∏Å

‚â§ (ùëû‚Ä≤

0 + ùëû‚Ä≤
1)

|ùúÇ(ùë•) ‚àí ÃÇÔ∏ÄùúÇ(ùë•)| ùëëùëÉùëã + ‚Éí

‚Éíùë°ùëû‚Ä≤ ‚àí ùë°ùëû

‚Éí P (Ô∏Å
‚Éí

ÃÇÔ∏Äùëìùëû(ùë•) Ã∏= ùëì *

ùëû (ùë•)

)Ô∏Å)Ô∏Ç

Thus, we obtain the upper bound

(EE) ‚â§ (ùëû‚Ä≤

0 + ùëû‚Ä≤
1)

[Ô∏Ç‚à´Ô∏Å

(Ô∏Ç

E

|ùúÇ(ùë•) ‚àí ÃÇÔ∏ÄùúÇ(ùë•)| ùëëùëÉùëã

]Ô∏Ç

+ ‚Éí

‚Éíùë°ùëû‚Ä≤ ‚àí ùë°ùëû

‚Éí E [Ô∏ÅP (Ô∏Å
‚Éí

ÃÇÔ∏Äùëìùëû(ùë•) Ã∏= ùëì *

ùëû (ùë•)

)Ô∏Å]Ô∏Å)Ô∏Ç

Therefore we have completed the proof. Applying Lemma 2 to the first term also proves Corollary
1.

B.2 Shifted Margin Assumption

An important tool in nonparametric classification is the Tsybakov margin condition.

Definition 5. A distribution ùëÉùëã,ùëå satisfies the (ùõº, ùê∂)-margin condition if for all ùë° > 0, we have

(Ô∏Ç

P

0 ‚â§

‚Éí
‚Éí
‚Éí
‚Éí

ùúÇ(ùëã) ‚àí

)Ô∏Ç

‚â§ ùë°

‚Éí
‚Éí
‚Éí
‚Éí

1
2

‚â§ ùê∂ùë°ùõº.

Subsequent works (Audibert and Tsybakov, 2007; Chaudhuri and Dasgupta, 2014) leverage this
assumption to provide fast, explicit rates of convergence for expected risk. The margin condition is
naturally suited to standard plug-in classification because the decision threshold is 1/2; for weighted
plug-in classification, we need a shifted margin condition.

19

Definition 6. A distribution ùëÉùëã,ùëå satisfies the (ùëû, ùõº, ùê∂)-margin condition if for all ùë° > 0, we have

P (0 ‚â§ |ùúÇ(ùë•) ‚àí ùë°ùëû| ‚â§ ùë°) ‚â§ ùê∂ùë°ùõº.

Using the shifted margin condition, we can obtain better results than we presented in the main
paper. However, the shifted margin condition may be be less interpretable than the original margin
condition. Intuitively, the original margin condition says that there is very little probability mass
where distinguishing between ùëå = 0 and ùëå = 1 is difficult, i.e., near ùúÇ(ùëã) = 1/2. For other ùë°ùëû, the
decision may not be difficult in that ùë°ùëû may be far from 1/2, but we would still require little mass
near this point.

Proposition 6. Suppose the distribution ùëÉùëã,ùëå satisfies the (ùëû, ùõº, ùê∂)-margin condition and ùëã has a
density that is lower bounded by some constant ùúámin on its support. Additionally, suppose that ùúÇ is
ùõΩ-H√∂lder. Then, the excess expected ùëû‚Ä≤-risk of ÃÇÔ∏Äùëìùëû satisfies the bound

E‚Ñ∞ùëû‚Ä≤( ÃÇÔ∏Äùëìùëû) ‚â§ (ùëû‚Ä≤

0 + ùëû‚Ä≤
1)

‚éõ
‚éùùëÇ

(Ô∏Ç log ùëõ
ùëõ

)Ô∏Ç ùõΩ

2ùõΩ+ùëë

+ ‚Éí

‚Éíùë°ùëû‚Ä≤ ‚àí ùë°ùëû

‚Éí
‚Éí ùëÇ

)Ô∏Ç ùõºùõΩ

2ùõΩ+ùëë

‚éû
‚é† + (IE)

(Ô∏Ç log ùëõ
ùëõ

Before proving this proposition, we prove a helpful lemma that leverages the shifted margin

condition, similar to one from Audibert and Tsybakov (2007).

Lemma 3. For a fixed density estimate ÃÇÔ∏ÄùúÇ, if ùëÉùëã,ùëå satisfies the (ùëû, ùõº, ùê∂)-margin condition, then
following upper bound is always true:

P (Ô∏Å

ÃÇÔ∏Äùëìùëû(ùë•) Ã∏= ùëì *

ùëû (ùë•), ùúÇ(ùë•) Ã∏= ùë°ùëû

)Ô∏Å

‚â§ ùê∂ ‚ÄñùúÇ ‚àí ÃÇÔ∏ÄùúÇ‚Äñùõº
‚àû .

Proof. We use a simple upper bound on the error probability event and apply the margin condition
to obtain

P (Ô∏Å

ÃÇÔ∏Äùëìùëû(ùë•) Ã∏= ùëì *

ùëû (ùë•), ùúÇ(ùë•) Ã∏= ùë°ùëû)

)Ô∏Å

‚â§ P (0 ‚â§ |ùúÇ(ùë•) ‚àí ùë°ùëû| ‚â§ |ùúÇ(ùë•) ‚àí ÃÇÔ∏ÄùúÇ(ùë•)|)
‚â§ P (0 ‚â§ |ùúÇ(ùë•) ‚àí ùë°ùëû| ‚â§ ‚ÄñùúÇ ‚àí ÃÇÔ∏ÄùúÇ‚Äñ‚àû)
‚â§ ùê∂0 ‚ÄñùúÇ ‚àí ÃÇÔ∏ÄùúÇ‚Äñùõº
‚àû .

This completes the proof.

Since, by Lemma 3, we have proved an upper bound in terms of ‚ÄñùúÇ ‚àí ÃÇÔ∏ÄùúÇ‚Äñùõº

‚àû, we now cite an upper

bound on that quantity that is a property of regression estimator.

Lemma 4 (Theorem 1 of Stone 1982). Let ÃÇÔ∏ÄùúÇ be a local polynomial regression estimator, and suppose
ùëã has a density that is lower bounded by some constant ùúámin > 0 on its support. Then, we have the
following upper bound:

E [‚ÄñùúÇ ‚àí ÃÇÔ∏ÄùúÇ‚Äñùõº

‚àû] ‚â§ ùê∂

(Ô∏Ç log ùëõ
ùëõ

)Ô∏Ç ùõºùõΩ

2ùõΩ+ùëë

.

(4)

20

The above bound is the optimal rate of uniform convergence for nonparametric estimators under
the regularity conditions shown here, and local polynomial regression achieves this optimal rate
(Stone, 1982).

Proof of Proposition 6. It suffices to prove an upper bound on the estimation error. We have

(EE) ‚â§ (ùëû‚Ä≤

0 + ùëû‚Ä≤
1)

[Ô∏Ç‚à´Ô∏Å

(Ô∏Ç

E

|ùúÇ(ùë•) ‚àí ÃÇÔ∏ÄùúÇ(ùë•)| ùëëùëÉùëã

]Ô∏Ç

+ ‚Éí

‚Éíùë°ùëû‚Ä≤ ‚àí ùë°ùëû

‚Éí E [Ô∏ÅP( ÃÇÔ∏Äùëìùëû(ùë•) Ã∏= ùëì *
‚Éí

ùëû (ùë•))

]Ô∏Å)Ô∏Ç

by the final equation of the proof of Proposition 3. Next, we use the fact that for all ùë• in ùí≥ we
have ùúÇ(ùë•) ‚àí ÃÇÔ∏ÄùúÇ(ùë•) ‚â§ ‚ÄñùúÇ ‚àí ÃÇÔ∏ÄùúÇ‚Äñ‚àû and Lemma 3 to obtain
1) (Ô∏ÄE [‚ÄñùúÇ ‚àí ÃÇÔ∏ÄùúÇ‚Äñ‚àû] + ‚Éí

‚Éí
‚Éí ùê∂0E [‚ÄñùúÇ ‚àí ÃÇÔ∏ÄùúÇ‚Äñùõº

(EE) ‚â§ (ùëû‚Ä≤

‚Éíùë°ùëû‚Ä≤ ‚àí ùë°ùëû

0 + ùëû‚Ä≤

‚àû])Ô∏Ä

Finally, we apply Lemma 4 to obtain

(EE) ‚â§ (ùëû‚Ä≤

0 + ùëû‚Ä≤
1)

‚éõ
‚éùùê∂

(Ô∏Ç log ùëõ
ùëõ

)Ô∏Ç ùõΩ

2ùõΩ+ùëë

+ ‚Éí

‚Éíùë°ùëû‚Ä≤ ‚àí ùë°ùëû

‚Éí
‚Éí ùê∂0ùê∂

)Ô∏Ç ùõºùõΩ

2ùõΩ+ùëë

‚éû
‚é† ,

(Ô∏Ç log ùëõ
ùëõ

which completes the proof.

B.3 Universality of Weighting

Since we may be interested in performance in error metrics other than risk, we discuss other
classification metrics here. In particular, we simply show that weighting is ‚Äúuniversal‚Äù in that it
can be used to optimize these other classification metrics. The reason for this is that, in plug-in
classification, optimizing many classification metrics is equivalent to altering the threshold for the
classification, and this has been observed to lead to the optimal decision rule in many cases (Lewis,
1995; Menon et al., 2013; Narasimhan et al., 2014; Koyejo et al., 2014). We examine the specific
case of metrics considered in Koyejo et al. (2014).

Definition 7. Let ùëì be a classifier over ùí≥ . Define the true positive, false negative, false positive,
and true negative proportions to be

TP = P(ùëå = 1, ùëì (ùëã) = 1)
FN = P(ùëå = 1, ùëì (ùëã) = 0)

FP = P(ùëå = 0, ùëì (ùëã) = 1)
TN = P(ùëå = 0, ùëì (ùëã) = 0).

A linear-fractional metric is defined as

‚Ñí(ùëì, ùëÉùëã , ùúÇ) =

ùëé0 + ùëé11TP + ùëé10FP + ùëé01FN + ùëé00TN
ùëè0 + ùëè11TP + ùëè10FP + ùëè01FN + ùëè00TN

for constants ùëé0, ùëé11, ùëé10, ùëé01, ùëé00, ùëè0, ùëè11, ùëè10, ùëè01, ùëè00.

Koyejo et al. (2014) showed that the optimal classifier for any linear-fractional metric is simply

a threshold classifier. Specifically, the following theorem is true.

Theorem 2 (Koyejo et al. 2014). Let ‚Ñí be a linear-fractional metric, and let ùëÉùëã be absolutely
continuous with respect to the dominating measure ùúà on ùí≥ . Define

‚Ñí* = max

ùëì

‚Ñí(ùëì, ùëÉùëã , ùúÇ)

21

and

ùõø* =

(ùëè10 ‚àí ùëè00)‚Ñí* ‚àí ùëé10 + ùëé00
ùëé11 ‚àí ùëé10 ‚àí ùëé01 + ùëé00 ‚àí (ùëè11 ‚àí ùëè10 ‚àí ùëè01 + ùëè00)‚Ñí* .

Then, the optimal classifier for ‚Ñí is ùëì *

‚Ñí(ùë•) = 1 {ùúÇ(ùë•) > ùõø*} if

ùëé11 ‚àí ùëé10 ‚àí ùëé01 + ùëé00 ‚àí (ùëè11 ‚àí ùëè10 ‚àí ùëè01 + ùëè00)‚Ñí* > 0

and ùëì *

‚Ñí(ùë•) = 1 {ùúÇ(ùë•) < ùõø*} otherwise.

Corollary 3. We note by Proposition 1 that for an metric ‚Ñí where

ùëé11 ‚àí ùëé10 ‚àí ùëé01 + ùëé00 ‚àí (ùëè11 ‚àí ùëè10 ‚àí ùëè01 + ùëè00)‚Ñí* > 0,

if we set define ùëû to be

then ùëì *

ùëû = ùëì *
‚Ñí.

ùëû0 = (ùëè10 ‚àí ùëè00)‚Ñí* ‚àí ùëé10 + ùëé00
ùëû1 = (ùëè01 ‚àí ùëè11)‚Ñí* ‚àí ùëé01 + ùëé11,

Performance metrics that are used in evaluating classifiers such as F1 and arithmetic mean
satisfy the the conditions of Corollary 3. Thus, we can reformulate optimization of a classifier in
these error metrics as a specific weighting the risk.

C The Fundamental Trade-off in Empirical Risk Minimization

Part of our motivation for the robust weighted problem is the fundamental trade-off under different
weightings ùëû and ùëû‚Ä≤. We demonstrated this for plug-in classification in the main text because it
elucidates the nature of the problem naturally via thresholds, but we should also convince ourselves
that this is not simply a quirk of plug-in classification. To this end, we provide a brief analysis for
empirical risk minimization.

Let ÃÇÔ∏Äùëìùëû and ùëì *

ùëû denote the empirical risk minimizer and risk minimizer within ‚Ñ±. Define the excess
ùëû ). Suppose that we have a uniform convergence

risk to be the difference between ùëÖ( ÃÇÔ∏Äùëìùëû) and ùëÖùëû(ùëì *
guarantee

ùëÖùëû(ùëì ) ‚àí ÃÇÔ∏ÄùëÖùëû(ùëì ) ‚â§ ùëÇ

(Ô∏Å
ùëõ‚àí 1

2

)Ô∏Å

for all ùëì in ‚Ñ±. Then, a standard chaining argument reveals that the excess risk decay rate satisfies

‚Ñ∞ùëû( ÃÇÔ∏Äùëìùëû) = ùëÖùëû( ÃÇÔ∏Äùëìùëû) ‚àí ùëÖ(ùëì *
ùëû )

= ùëÖùëû( ÃÇÔ∏Äùëìùëû) ‚àí ÃÇÔ∏ÄùëÖùëû( ÃÇÔ∏Äùëìùëû) + ÃÇÔ∏ÄùëÖùëû( ÃÇÔ∏Äùëìùëû) ‚àí ÃÇÔ∏ÄùëÖùëû(ùëì *

ùëû ) + ÃÇÔ∏ÄùëÖùëû(ùëì *

ùëû ) ‚àí ùëÖ(ùëì *
ùëû )

‚â§ ùëÇ

= ùëÇ

(Ô∏Å

2

ùëõ‚àí 1
(Ô∏Å
ùëõ‚àí 1

2

)Ô∏Å

)Ô∏Å

+ 0 + ùëÇ

(Ô∏Å

ùëõ‚àí 1

2

)Ô∏Å

,

where in the inequality we used our uniform convergence guarantee twice and the fact that ÃÇÔ∏Äùëìùëû is the
empirical ùëû-risk minimizer. This mirrors the case of ùëû-weighted plug-in estimation in that the excess
ùëû-risk still converges to 0 at the standard rate.

22

On the other hand, we obtain a constant term when performing a similar analysis for ‚Ñ∞ùëû‚Ä≤( ÃÇÔ∏Äùëìùëû).

Specifically, we get

‚Ñ∞ùëû‚Ä≤( ÃÇÔ∏Äùëìùëû) = ùëÖùëû‚Ä≤( ÃÇÔ∏Äùëìùëû) ‚àí ùëÖùëû‚Ä≤(ùëì *
ùëû )
= ùëÖùëû( ÃÇÔ∏Äùëìùëû) ‚àí ùëÖùëû(ùëì *
ùëû ) + ùëÖùëû‚Ä≤( ÃÇÔ∏Äùëìùëû) ‚àí ùëÖùëû( ÃÇÔ∏Äùëìùëû) + ùëÖùëû(ùëì *
ùëõ‚àí 1

+ ùëÖùëû‚Ä≤( ÃÇÔ∏Äùëìùëû) ‚àí ùëÖùëû( ÃÇÔ∏Äùëìùëû) + ùëÖùëû(ùëì *

‚â§ ùëÇ

)Ô∏Å

(Ô∏Å

2

ùëû ) ‚àí ùëÖùëû‚Ä≤(ùëì *

ùëû ) ‚àí ùëÖùëû‚Ä≤(ùëì *
ùëû‚Ä≤)
ùëû‚Ä≤).

Now, using the prior convergence result for the empirical risk minimizers, we obtain

‚Ñ∞ùëû‚Ä≤( ÃÇÔ∏Äùëìùëû) ‚â§ ùëÖùëû‚Ä≤( ÃÇÔ∏Äùëìùëû) ‚àí ùëÖùëû( ÃÇÔ∏Äùëìùëû) + ùëÖùëû(ùëì *

ùëû ) ‚àí ùëÖùëû‚Ä≤(ùëì *
ùëû ) ‚àí ùëÖùëû‚Ä≤(ùëì *
ùëû ) + ùëÖùëû(ùëì *
ùëõ‚àí 1

+ùëÇ

)Ô∏Å

(Ô∏Å

.

2

ùëû‚Ä≤) + ùëÇ

ùëû‚Ä≤) + ùëÇ

)Ô∏Å

(Ô∏Å

2

ùëõ‚àí 1
(Ô∏Å
ùëõ‚àí 1

2

)Ô∏Å

‚â§ ùëÖùëû‚Ä≤(ùëì *

= ùëÖùëû‚Ä≤(ùëì *

ùëû ) ‚àí ùëÖùëû(ùëì *
ùëû ) ‚àí ùëÖùëû‚Ä≤(ùëì *
ùëû‚Ä≤)

‚èü

 ‚èû
ùê¥

ùëû‚Ä≤ minimizes ùëÖùëû‚Ä≤ and ùëì *

Since ùëì *
ùëû minimizes ùëÖùëû, we see that ùê¥ ‚â• 0. Thus, even though there is not a
clear threshold interpretation, we do see that there is irreducible error that arises in the empirical
risk minimization setting as well.

D Robust Weighting Proofs

In this section, we prove our results for robust weighting. We start with our generalization and
excess risk bounds.

Proof of Theorem 1. Define the risk ùëÖùëñ,1 as

ÃÇÔ∏ÄùëÖùëñ,1(ùëì ) = ÃÇÔ∏Äùëùùëñ ÃÇÔ∏ÄùëÖùëñ(ùëì ) =

1
ùëõ

ùëõ
‚àëÔ∏Å

ùëó=1

‚Ñìmar(ùëì, ùëßùëó)1 {ùë¶ùëó = ùëñ} .

Let ùëÖùëñ,1(ùëì ) denote E ÃÇÔ∏ÄùëÖùëñ,1(ùëì ). Note that we have

ùëÖùëñ,1(ùëì ) =

1
ùëõ

ùëõ
‚àëÔ∏Å

ùëó=1

E [‚Ñìmar(ùëì, ùëßùëó)1 {ùë¶ùëó = ùëñ}] =

1
ùëõ

ùëõ
‚àëÔ∏Å

ùëó=1

ùëùùëñE [‚Ñìmar(ùëì, ùëßùëó)|ùë¶ùëó = ùëñ] = ùëùùëñùëÖùëñ(ùëì ).

By definition, we have

ùëÖùëÑ(ùëì ) = sup
ùëû‚ààùëÑ

ùëò
‚àëÔ∏Å

ùëñ=1

ùëûùëñùëùùëñùëÖùëñ(ùëì ) = sup
ùëû‚ààùëÑ

ùëò
‚àëÔ∏Å

ùëñ=1

ùëûùëñùëÖùëñ,1(ùëì ),

and so for our purposes, it suffices to analyze ÃÇÔ∏ÄùëÖùëñ,1. Define the class

‚Ñ±ùëñ,1 = {‚Ñìmar(ùëì, ¬∑)1 {ùë¶ùëó = ùëñ} : ùëì ‚àà ‚Ñ±} .

By Lemma 9, we have with probability at least 1 ‚àí ùõø/ùëò that

ùëÖùëñ,1(ùëì ) ‚â§ ÃÇÔ∏ÄùëÖùëñ,1(ùëì ) + 2Rùëõ(‚Ñìmar ‚àò ‚Ñ±ùëñ,1) +

‚àöÔ∏É

log ùëò
ùõø
2ùëõ

23

 
for each ùëì in ‚Ñ±. So, it suffices to analyze the Rademacher complexity term. Let ùúéùëó be iid Rademacher
random variables. We condition on the value of ùë¶1, . . . , ùë¶ùëõ. Let ‚Ñãùëå be the sigma-field ùúé(ùë¶1, . . . , ùë¶ùëõ).
Suppose without loss of generality that under the conditioning, we have ùë¶1 = ¬∑ ¬∑ ¬∑ = ùë¶ùëÅùëñ = ùëñ and
ùë¶ùëó Ã∏= ùëñ for all ùëó > ùëÅùëñ. Then, we have

Rùëõ(‚Ñ±ùëñ,1) =

=

EE

EE

1
ùëõ

1
ùëõ

[Ô∏É

sup
ùëì ‚àà‚Ñ±

‚é°
‚é£sup
ùëì ‚àà‚Ñ±

ùëõ
‚àëÔ∏Å

ùëñ=1
ùëÅùëñ‚àëÔ∏Å

ùëó=1

‚Éí
‚Éí
ùúéùëó‚Ñìmar(ùëì, ùëßùëó)1{ùë¶ùëó = ùëñ}
‚Éí
‚Éí

‚Ñãùëå

]Ô∏É

ùúéùëó‚Ñìmar(ùëì, ùëßùëó)

‚é§

‚Ñãùëå

‚é¶

‚Éí
‚Éí
‚Éí
‚Éí

= E

[Ô∏Ç ùëÅùëñ
ùëõ

ÃÇÔ∏ÄRùëÅùëñ(‚Ñìmar ‚àò ‚Ñ±)

]Ô∏Ç

.

By the proof of Lemma 11, we have

ÃÇÔ∏ÄRùëÅùëñ(‚Ñìmar ‚àò ‚Ñ±) ‚â§ 2ùëò ÃÇÔ∏ÄRùëÅùëñ(Œ†1(‚Ñ±)).

Putting everything together completes the proof of the generalization bound; now we turn to the
excess (‚Ñ±, ùëû)-risk bound.

Recall that ÃÇÔ∏ÄùëìùëÑ is the empirical ùëÑ-risk minimizer and ùëì *
Lemma 10, we have with probability at least 1 ‚àí ùõø/ùëò that

ùëÑ is the population ùëÑ-risk minimizer. By

ùëÖùëñ,1( ÃÇÔ∏ÄùëìùëÑ) ‚â§ ÃÇÔ∏ÄùëÖùëñ,1( ÃÇÔ∏ÄùëìùëÑ) + 4Rùëõ(‚Ñìmar ‚àò ‚Ñ±ùëñ,1) +

‚àöÔ∏É

log ùëò
ùõø
2ùëõ

.

Summing, we have

ùëÖùëû( ÃÇÔ∏ÄùëìùëÑ) =

ùëò
‚àëÔ∏Å

ùëñ=1

ùëûùëñùëùùëñùëÖùëñ( ÃÇÔ∏ÄùëìùëÑ) =

‚â§

ùëò
‚àëÔ∏Å

ùëñ=1

ùëò
‚àëÔ∏Å

ùëñ=1

ùëûùëñ(ùëÖùëñ,1 ÃÇÔ∏ÄùëìùëÑ)

‚éõ
‚éù ÃÇÔ∏ÄùëÖùëñ,1( ÃÇÔ∏ÄùëìùëÑ) + 4Rùëõ(‚Ñìmar ‚àò ‚Ñ±ùëñ,1) +

ùëûùëñ

‚àöÔ∏É

‚éû

‚é†

log ùëò
ùõø
2ùëõ

‚â§ ÃÇÔ∏ÄùëÖùëû( ÃÇÔ∏ÄùëìùëÑ) +

ùëò
‚àëÔ∏Å

ùëñ=1

ùëûùëñùëùùëñ

‚éõ

‚éú
‚éù

4
ùëùùëñ

Rùëõ(‚Ñìmar ‚àò ‚Ñ±ùëñ,1) +

‚éØ
‚é∏
‚é∏
‚é∑

log ùëò
ùõø
2ùëù2
ùëñ ùëõ

‚éû

‚éü
‚é† .

Using the proof of Lemma 11 as before, we then obtain

ùëÖùëû( ÃÇÔ∏ÄùëìùëÑ) ‚â§ ÃÇÔ∏ÄùëÖùëû( ÃÇÔ∏ÄùëìùëÑ) +

ùëò
‚àëÔ∏Å

ùëñ=1

ùëûùëñùëùùëñ

‚éõ
‚éù8ùëòE
‚éú

[Ô∏Ç ùëÅùëñ
ùëùùëñùëõ

]Ô∏Ç
ÃÇÔ∏ÄRùëÅùëñ(Œ†1(‚Ñ±))

+

‚éØ
‚é∏
‚é∏
‚é∑

log ùëò
ùõø
2ùëù2
ùëñ ùëõ

‚éû

‚éü
‚é† .

Thus, by taking supremums, we observe that

ùëÖùëÑ( ÃÇÔ∏ÄùëìùëÑ) ‚â§ ÃÇÔ∏ÄùëÖùëÑ( ÃÇÔ∏ÄùëìùëÑ) + sup
ùëû‚ààùëÑ

ùëò
‚àëÔ∏Å

ùëñ=1

ùëûùëñùëùùëñ

‚éõ
‚éù8ùëòE
‚éú

[Ô∏Ç ùëÅùëñ
ùëùùëñùëõ

]Ô∏Ç
ÃÇÔ∏ÄRùëÅùëñ(Œ†1(‚Ñ±))

+

‚éØ
‚é∏
‚é∏
‚é∑

log ùëò
ùõø
2ùëù2
ùëñ ùëõ

‚éû

‚éü
‚é† .

(5)

Similarly, by Lemma 10, we have

‚àíùëÖùëñ,1(ùëì *

ùëÑ) ‚â§ ‚àí ÃÇÔ∏ÄùëÖùëñ,1(ùëì *

ùëÑ) + 4Rùëõ(‚Ñìmar ‚àò ‚Ñ±ùëñ,1) +

‚àöÔ∏É

log ùëò
ùõø
2ùëõ

.

24

Summing as before and using the proof of Lemma 11, we have

‚àíùëÖùëû(ùëì *

ùëÑ) ‚â§ ‚àí ÃÇÔ∏ÄùëÖùëû(ùëì *

ùëÑ) +

ùëò
‚àëÔ∏Å

ùëñ=1

ùëûùëñùëùùëñ

‚éõ
‚éù8ùëòE
‚éú

[Ô∏Ç ùëÅùëñ
ùëùùëñùëõ

]Ô∏Ç
ÃÇÔ∏ÄRùëÅùëñ(Œ†1(‚Ñ±))

+

‚éØ
‚é∏
‚é∏
‚é∑

log ùëò
ùõø
2ùëù2
ùëñ ùëõ

‚éû

‚éü
‚é† .

Taking the infimum and using Lemma 8, we have

‚àíùëÖùëÑ(ùëì *

ùëÑ) ‚â§ ‚àí ÃÇÔ∏ÄùëÖùëÑ(ùëì *

ùëÑ) + sup
ùëû‚ààùëÑ

ùëò
‚àëÔ∏Å

ùëñ=1

ùëûùëñùëùùëñ

‚éõ
‚éù8ùëòE
‚éú

[Ô∏Ç ùëÅùëñ
ùëùùëñùëõ

]Ô∏Ç
ÃÇÔ∏ÄRùëÅùëñ(Œ†1(‚Ñ±))

+

‚éØ
‚é∏
‚é∏
‚é∑

log ùëò
ùõø
2ùëù2
ùëñ ùëõ

‚éû

‚éü
‚é† .

(6)

Summing equation (5) and equation (6) and noting that ÃÇÔ∏ÄùëìùëÑ minimizes the empirical robust risk, we
have

‚Ñ∞ùëÑ(‚Ñ±) = ùëÖùëÑ( ÃÇÔ∏ÄùëìùëÑ) ‚àí ùëÖùëÑ(ùëì *

ùëÑ) ‚â§ 2 sup
ùëû‚ààùëÑ

ùëò
‚àëÔ∏Å

ùëñ=1

ùëûùëñùëùùëñ

‚éõ
‚éù8ùëòE
‚éú

[Ô∏Ç ùëÅùëñ
ùëùùëñùëõ

]Ô∏Ç
ÃÇÔ∏ÄRùëÅùëñ(Œ†1(‚Ñ±))

+

‚éØ
‚é∏
‚é∏
‚é∑

log ùëò
ùõø
2ùëù2
ùëñ ùëõ

‚éû

‚éü
‚é† ,

and this completes the proof.

Proof of Corollary 2. The only thing we need to do here is calculate the Rademacher complexity
term of Theorem 1. Using our assumption and Jensen‚Äôs inequality, we have

E

[Ô∏Ç ùëÅùëñ
ùëõ

ÃÇÔ∏ÄRùëÅùëñ(Œ†1(‚Ñ±))

]Ô∏Ç

‚â§

ùê∂(‚Ñ±)
ùëõ

E [Ô∏Å‚àöÔ∏ÄùëÅùëñ

]Ô∏Å

‚â§

ùê∂(‚Ñ±)
ùëõ

E[ùëÅùëñ]1/2 = ùê∂(‚Ñ±)

‚àöÔ∏Ç ùëùùëñ
ùëõ

.

This completes the proof of the corollary.

Next, we prove our duality results. We start with LCVaR.

Proof of Proposition 4. The Lagrangian of LCVaR is

ùêø(ùëû, ùúÜ) = E[ùëûùëå ùëÖùëå (ùëì )] + ùúÜ(1 ‚àí E[ùëûùëå ]) = E [ùëûùëå (ùëÖùëå (ùëì ) ‚àí ùúÜ)] + ùúÜ.

Our goal is to use the minimax theorem, which we state as Theorem 4, to switch the infimum

over ùúÜ and the supremum over ùëû. First, we do not need the minimax theorem to obtain

inf
ùúÜ‚ààR

ùêø(ùëû, ùúÜ) ‚â§ inf
ùúÜ‚ààR

sup
ùëû:ùëû(¬∑)‚àà[0,ùõº‚àí1]

E [ùëûùëå (ùëÖùëå (ùëì ) ‚àí ùúÜ)] + ùúÜ = inf
ùúÜ‚ààR

{Ô∏ÅE [Ô∏Å

ùõº‚àí1E(ùëÖùëå (ùëì ) ‚àí ùúÜ)+ + ùúÜ

]Ô∏Å}Ô∏Å

,

(7)

since the inequality follows the trivial direction of the minimax theorem and we can solve the inner
maximization problem by setting

ùëûùëñ =

{Ô∏É0

ùëÖùëñ(ùëì ) ‚àí ùúÜ < 0
ùõº‚àí1 ùëÖùëñ(ùëì ) ‚àí ùúÜ ‚â• 0.

Our present goal is to verify the conditions of the minimax theorem. First, we note that
ùúÜ ‚Ü¶‚Üí ùêø(ùëû, ùúÜ) is linear and therefore convex for any ùëû, and similarly, ùëû ‚Ü¶‚Üí ùêø(ùëû, ùúÜ) is linear and
therefore concave for any ùëû. Additionally, the domain of ùëû, in this case [0, ùõº‚àí1]ùëò, is compact and
convex by definition; so we only need to prove that it suffices to consider ùúÜ on a compact, convex
domain.

25

Denote the right hand side of equation (7) by inf ùúÜ‚ààR ùê∑(ùúÜ). Let ùêπùëì (ùúÜ) denote the cumulative

distribution function of ùëÖùëå at ùúÜ. By Lemma 6, the derivative of ùê∑(ùúÜ) is given by

ùê∑‚Ä≤(ùúÜ) = 1 + ùõº‚àí1(ùêπùëì (ùúÜ) ‚àí 1),

when ùêπùëì is continuous at ùúÜ. If it is not, then the same result holds for the left and right limits.
Thus by considering signs of the derivative, we see that ùúÜ achieves minimizes ùê∑(ùúÜ) for a value in
the interval [ùúÜ*(ùëì ), ùúÜ*(ùëì )] where

ùúÜ*(ùëì ) = inf{ùë° : ùêπùëì (ùë°) ‚â• 1 ‚àí ùõº} and ùúÜ*(ùëì ) = sup{ùë° : ùêπùëì (ùë°) ‚â§ 1 ‚àí ùõº}.

Note further that when ‚Ñ± is compact in, say, sup norm, then we also have finite ùúÜ* = inf ùëì ‚àà‚Ñ± ùë°*(ùúÜ)
and ùúÜ* = inf ùëì ‚àà‚Ñ± ùë°*(ùúÜ). In any case, we see that it suffices to define ùúÜ on a compact set Œõ = [ùúÜ*, ùúÜ*],
and so we may assume without loss of generality that the domain of ùúÜ is compact.

This verifies the conditions of the minimax theorem, and so we have

LCVaRùõº(ùëì ) = inf
ùúÜ‚ààR

sup
ùëû:ùëû(¬∑)‚àà[0,ùõº‚àí1]

E [ùëûùëå (ùëÖùëå (ùëì ) ‚àí ùúÜ)] + ùúÜ = inf
ùúÜ‚ààR

{Ô∏ÅE [Ô∏Å

ùõº‚àí1E(ùëÖùëå (ùëì ) ‚àí ùúÜ)+ + ùúÜ

]Ô∏Å}Ô∏Å

,

which completes the proof.

Next, we consider LHCVaR.

Proof of Proposition 5. The proof is similar to that of Proposition 4. The Lagrangian of LHCVaR
is

ùêø(ùëû, ùúÜ) = E[ùëûùëå ùëÖùëå (ùëì )] + ùúÜ (1 ‚àí E[ùëûùëå ]) = E [ùëûùëå (ùëÖùëå (ùëì ) ‚àí ùúÜ)] + ùúÜ.

Next, by the trivial direction of the minimax theorem, we have

LHCVaRùõº(ùëì ) ‚â§ inf
ùúÜ‚ààR

sup
ùëû:ùëûùëå ‚àà[0,ùõº‚àí1
ùëå ]

ùêø(ùëû, ùúÜ) = inf
ùúÜ‚ààR

E [Ô∏Å

ùõº‚àí1

ùëå (ùëÖùëå (ùëì ) ‚àí ùúÜ)+

]Ô∏Å

+ ùúÜ.

(8)

So, now our goal is to verify the conditions of the minimax theorem. As with LCVaR, the Lagrangian
ùêø is linear and therefore concave in ùëû; is linear and therefore convex in ùúÜ; and is defined over a
compact domain of values of ùëû given by [0, ùõº‚àí1]ùëò. Thus, the only difficulty, as with LCVaR, is
showing that it suffices to define ùúÜ over a compact interval. To this end, define the right hand side
of equation (8) to be inf ùúÜ‚ààR ùêª(ùúÜ). It suffices to show that ùê∑(ùúÜ) achieves its infimum on a closed
interval, in which case we can restrict the domain of ùúÜ to this compact, convex set.

To prove such an interval exists, we wish to show that there exist constants ùúÜ* and ùúÜ* such that
ùêª is decreasing for all ùúÜ < ùúÜ* and increasing for all ùúÜ > ùúÜ*. By Lemma 7, we see that the derivative
of ùêª is

ùêª ‚Ä≤(ùúÜ) = 1 ‚àí E[ùõº‚àí1

ùëå 1 {ùëÖùëå (ùëì ) > ùúÜ}] = 1 ‚àí

ùõº‚àí1

ùëñ ùëùùëñ1 {ùëÖùëñ(ùëì ) > ùúÜ}

ùëò
‚àëÔ∏Å

when ùêª ‚Ä≤ exists; otherwise the result holds for the left and right derivatives. Let ùúÜ*(ùëì ) =
minùëñ=1,...,ùëò ùëÖùëñ(ùëì ). Then, for ùúÜ ‚â§ ùúÜ*(ùëì ), we have

ùëñ=1

ùêª ‚Ä≤(ùúÜ) = 1 ‚àí

ùëò
‚àëÔ∏Å

ùëñ=1

ùõº‚àí1

ùëñ ùëùùëñ ‚â§ 0.

26

Next, pick ùúÜ*(ùëì ) = maxùëñ=1,...,ùëò ùëÖùëñ(ùëì ) + 1. Then, for all ùúÜ ‚â• ùúÜ*(ùëì ), we have

ùêª ‚Ä≤(ùúÜ) = 1 ‚â• 0.

If ‚Ñì is continuous, then each ùëÖùëñ(ùëì ) is continuous in ùëì . Moreover, when ‚Ñ± is compact on ùí≥ in the
supremum norm, then we can define finite constants ùúÜ* = inf ùëì ‚àà‚Ñ± ùúÜ*(ùëì ) and ùúÜ*(ùëì ) = supùëì ‚àà‚Ñ± ùúÜ*(ùëì ).
Thus, we may restrict the domain of ùúÜ to [ùúÜ*, ùúÜ*] without loss of generality. The minimax

theorem now implies that equation (8) holds with equality, which completes the proof.

E Results for the Conditional Sampling Model

Now, we present the alternative result for the conditional sampling model. Recall that ùëõùëñ is the
number of samples of class ùëñ, which is assumed to be fixed.

Theorem 3. Let ‚Ñì be the multiclass margin loss. With probability at least 1 ‚àí ùõø, for every ùëì in ‚Ñ±
we have

ùëÖùëÑ ‚â§ max
ùëû‚ààùëÑ

‚éß
‚é®

‚é©

ÃÇÔ∏ÄùëÖùëû(ùëì ) +

ùëò
‚àëÔ∏Å

ùëñ=1

ùëûùëñ ÃÇÔ∏Äùëùùëñ

‚éõ
‚éù2ùëòRùëõùëñ(‚Ñ±) +

‚àöÔ∏É

log ùëò
ùõø
2ùëõùëñ

‚éû

‚é†

‚é´
‚é¨

‚é≠

.

Proof. The proof is similar to that of Cao et al. (2019). We apply Lemma 9 and Lemma 11 to
obtain

ùëÖùëñ(ùëì ) ‚â§ ÃÇÔ∏ÄùëÖùëñ(ùëì ) + 2ùëòRùëõùëñ(‚Ñ±) +

‚àöÔ∏É

log ùëò
ùõø
2ùëõùëñ

.

Multiplying by ùëûùëñ ÃÇÔ∏Äùëùùëñ, summing over ùëñ, and taking a supremum over ùëÑ completes the proof.

F Gradient Descent-Ascent

In general, the robust classification problem is a saddle-point problem. For our purposes, define a
saddle-point problem to be an optimization problem of the form

inf
ùëé‚ààùíú

sup
ùëè‚àà‚Ñ¨

ùëì (ùëé, ùëè).

(9)

One of the seminal results in game theory is that the minimax problem is equivalent to the

maximin problem.
Theorem 4 (minimax theorem). Let ùíú and ‚Ñ¨ be compact convex sets. Let ùëì : ùíú √ó ‚Ñ¨ ‚Üí R be a
function such that ùëé ‚Ü¶‚Üí ùëì (ùëé, ùëè) is convex and ùëè ‚Ü¶‚Üí ùëì (ùëé, ùëè) is concave. Then, we have

inf
ùëé‚ààùíú

sup
ùëè‚àà‚Ñ¨

ùëì (ùëé, ùëè) = sup
ùëè‚àà‚Ñ¨

inf
ùëé‚ààùíú

ùëì (ùëé, ùëè).

Lemma 5 (Theorem 3.1 of Hazan 2016). Let ùëì1, . . . , ùëìùëá : ùíú ‚Üí R be a sequence of ùêø-Lipschitz
convex functions. If the step size for online gradient descent is chosen to be

then we have

ùúÇùë° =

ùê∑
‚àö

ùêø

,

ùë°

ùëá
‚àëÔ∏Å

ùë°=1

ùëìùë°(ùëéùë°) ‚àí min
ùëé*‚ààùíú

ùëá
‚àëÔ∏Å

ùë°=1

ùëìùë°(ùëé*) ‚â§

‚àö

ùê∑ùêø

ùëá .

3
2

27

Algorithm 1: Online Gradient Descent

Input
for ùë° = 1, . . . , ùëá do

: Convex domain ùíú, ùëé1 ‚àà ùíú, step sizes ùúÇùë°, number of rounds ùëá

Play ùëéùë° and observe cost ùëìùë°(ùëéùë°).
Update and project

ùë•ùë°+1 = ùëéùë° ‚àí ùúÇùë°‚àáùëìùë°(ùëéùë°)
ùëéùë°+1 = Œ†ùíú(ùë•ùë°+1).

end
Output : The average iterate ¬Øùëéùëá = 1
ùëá

‚àëÔ∏Äùëá

ùë°=1 ùëéùë°.

Now we return to the saddle-point problem. We give the gradient descent-ascent algorithm in

Algorithm 2 and the convergence result in Proposition 7.

Algorithm 2: Gradient Descent-Ascent

Input
for ùë° = 1, . . . , ùëá do

: Convex-concave function ùëì , step sizes ùúÇùëé,ùë° and ùúÇùëè,ùë°, number of rounds ùëá

Play (ùëéùë°, ùëèùë°) and observe cost ùëì (ùëéùë°, ùëèùë°).
Update and project

Update and project

ùë•ùë°+1 = ùëéùë° ‚àí ùúÇùë°‚àáùëéùëì (ùëéùë°, ùëèùë°)
ùëéùë°+1 = Œ†ùíú(ùë•ùë°+1).

ùë¶ùë°+1 = ùëèùë° + ùúÇùë°‚àáùëèùëì (ùëéùë°, ùëèùë°)
ùëèùë°+1 = Œ†ùíú(ùë¶ùë°+1).

end
Output : The average iterates ¬Øùëéùëá = 1
ùëá

‚àëÔ∏Äùëá

ùë°=1 ùëéùë° and ¬Øùëèùëá = 1

ùëá

‚àëÔ∏Äùëá

ùë°=1 ùëèùë°.

Proposition 7. Let ùíú and ‚Ñ¨ be convex, compact sets. Suppose that ùíú has diameter ùê∑ùëé and ‚Ñ¨
has diameter ùê∑ùëè. Let ùëì : ùíú √ó ‚Ñ¨ ‚Üí R be convex-concave, ùêøùëé-Lipschitz in its first argument, and
ùêøùëè-Lipschitz in its second argument. Let (ùëé*, ùëè*) denote the solution to the saddle-point problem of
equation (9). If (¬Øùëéùëá , ¬Øùëèùëá ) is the output of Algorithm 2, then we have

ùëì (ùëé*, ùëè*) ‚àí

3(ùêøùëéùê∑ùëé + ùêøùëèùê∑ùëè)
‚àö
2

ùëá

‚â§ ùëì (¬Øùëéùëá , ¬Øùëèùëá ) ‚â§ ùëì (ùëé*, ùëè*) +

3(ùêøùëéùê∑ùëé + ùêøùëèùê∑ùëè)
‚àö

.

2

ùëá

First, we want to use a lemma from online convex optimization. For this, we also state the
standard online gradient descent algorithm. Here, we use Œ†ùíú to denote projection onto the set ùíú.

Proof. The proof is fairly straightforward from pre-existing results on online gradient descent; so we

28

state it here. We start first with the upper bound. Define the ‚Äúregret‚Äù to be

ùëÖùëá =

ùëá
‚àëÔ∏Å

ùë°=1

[ùëì (ùëéùë°, ùëèùë°) ‚àí ùëì (ùëé*, ùëè*)]

where (ùëé*, ùëè*) is a solution to the saddle-point problem. Then, we have the decomposition

ùëÖùëá =

ùëá
‚àëÔ∏Å

ùë°=1

[ùëì (ùëéùë°, ùëèùë°) ‚àí ùëì (ùëé*, ùëèùë°)] +

ùëá
‚àëÔ∏Å

ùë°=1

[ùëì (ùëé*, ùëèùë°) ‚àí ùëì (ùëé*, ùëè*)] ‚â§

‚àö

ùêøùëéùê∑ùëé

3
2

ùëá + 0,

(10)

where the inequality follows from applying Lemma 5 and noting that the second summand is
nonpositive by the definition of ùëè*. Similarly, we have

‚àíùëÖùëá =

ùëá
‚àëÔ∏Å

ùë°=1

[ùëì (ùëé*, ùëè*) ‚àí ùëì (ùëéùë°, ùëèùë°)] +

ùëá
‚àëÔ∏Å

ùë°=1

[ùëì (ùëéùë°, ùëèùë°) ‚àí ùëì (ùëéùë°, ùëèùë°)] ‚â§ 0 +

‚àö

ùëá .

ùêøùëèùê∑ùëè

3
2

(11)

So, now we consider the averaged iterates. We have

ùëì (¬Øùëéùëá , ¬Øùëèùëá ) ‚â§ max
ùëè‚àà‚Ñ¨

ùëì (¬Øùëéùëá , ùëè)

‚â§

1
ùëá

max
ùëè‚àà‚Ñ¨

ùëá
‚àëÔ∏Å

ùë°=1

ùëì (ùëéùë°, ùëè)

= ùëì (ùëé*, ùëè*) +

1
ùëá

max
ùëè‚àà‚Ñ¨

ùëá
‚àëÔ∏Å

[ùëì (ùëéùë°, ùëè) ‚àí ùëì (ùëéùë°, ùëèùë°)] +

ùë°=1

1
ùëá

ùëá
‚àëÔ∏Å

[ùëì (ùëéùë°, ùëèùë°) ‚àí ùëì (ùëé*, ùëè*)]

ùë°=1

‚â§ ùëì (ùëé*, ùëè*) +

3ùêøùëèùê∑ùëè
‚àö
ùëá
2

+

3ùêøùëéùê∑ùëé
‚àö
ùëá
2

.

Note that the second inequality is due to convexity, and the third is due to Lemma 5 and equation (10).

Similarly, we have

ùëì ( ¬Øùëéùëá , ¬Øùëèùëá ) ‚â• min
ùëé‚ààùíú

ùëì (ùëé, ¬Øùëèùëá )

‚â•

1
ùëá

min
ùëé‚ààùíú

ùëá
‚àëÔ∏Å

ùë°=1

ùëì (ùëé, ùëèùë°)

= ùëì (ùëé*, ùëè*) +

1
ùëá

min
ùëé‚ààùíú

ùëá
‚àëÔ∏Å

ùë°=1

[ùëì (ùëé, ùëèùë°) ‚àí ùëì (ùëéùë°, ùëèùë°)] +

1
ùëá

ùëá
‚àëÔ∏Å

ùë°=1

[ùëì (ùëéùë°, ùëèùë°) ‚àí ùëì (ùëé*, ùëè*)]

‚â• ùëì (ùëé*, ùëè*) ‚àí

3ùêøùëéùê∑ùëé
‚àö
ùëá
2

‚àí

3ùêøùëèùê∑ùëè
‚àö
ùëá
2

.

The second inequality follows from concavity, and the final inequality is a result of Lemma 5 applied
to the sequence ùëéùë° and equation (11). This completes the proof.

G Additional Lemmas

Lemma 6. Define ùê∑(ùúÜ) = ùõº‚àí1E(ùëÖùëå (ùëì ) ‚àí ùúÜ)+ + ùúÜ, and let ùêπùëì denote the cumulative distribution
function of ùëÖùëå (ùëì ). Then, we have

ùê∑‚Ä≤(ùúÜ) = 1 + ùõº‚àí1(ùêπùëì (ùúÜ) ‚àí 1).

29

Proof. We compute the derivative directly. We obtain

ùê∑‚Ä≤(ùúÜ) = 1 + ùõº‚àí1 lim
ùúÄ‚Üí0

{E [(ùëÖùëå (ùëì ) ‚àí ùúÜ ‚àí ùúÄ)+ ‚àí (ùëÖùëå (ùëì ) ‚àí ùúÜ)+]}

1
ùúÄ
1
ùúÄ

{E [‚àíùúÄ1 {ùëÖùëå (ùëì ) ‚àí ùúÜ > 0}]}

= 1 + ùõº‚àí1 lim
ùúÄ‚Üí0
= 1 ‚àí ùõº‚àí1E1 {ùëÖùëå (ùëì ) > ùúÜ}
= 1 + ùõº‚àí1(ùêπùëì (ùúÜ) ‚àí 1).

This completes the proof.
Lemma 7. Define ùêª(ùúÜ) = E [Ô∏Äùõº‚àí1(ùëÖùëå ‚àí ùúÜ)+
ùêª ‚Ä≤(ùúÜ) = 1 ‚àí E [Ô∏Å

]Ô∏Ä + ùúÜ. Then, the derivative of ùêª(ùúÜ) is
]Ô∏Å

ùõº‚àí1

ùëå 1 {ùëÖùëå (ùëì ) > ùúÜ}

.

Proof. We again compute directly, obtaining

ùëå (ùëÖùëå (ùëì ) ‚àí ùúÜ ‚àí ùúÄ)+ ‚àí ùõº‚àí1
ùõº‚àí1

ùëå (ùëÖùëå (ùëì ) ‚àí ùúÜ)+

]Ô∏Å

ùêª ‚Ä≤(ùúÜ) = 1 + lim
ùúÄ‚Üí0

E [Ô∏Å

1
ùúÄ
1
ùúÄ
ùõº‚àí1
ùëå 1 {ùëÖùëå (ùëì ) > ùúÜ}

ùõº‚àí1

E [Ô∏Å

ùëå (‚àíùúÄ)1 {ùëÖùëå (ùëì ) > ùúÜ}
]Ô∏Å

,

]Ô∏Å

= 1 + lim
ùúÄ‚Üí0
= 1 ‚àí E [Ô∏Å

as desired.

Lemma 8. We have the inequality

inf
ùëû‚ààùëÑ

{ùê¥(ùëû) + ùêµ(ùëû)} ‚â§ inf
ùëû‚ààùëÑ

ùê¥(ùëû) + sup
ùëû‚ààùëÑ

ùêµ(ùëû).

Proof. We have the inequality ùê¥(ùëû) + ùêµ(ùëû) ‚â§ ùê¥(ùëû) + supùëû‚Ä≤‚ààùëÑ ùêµ(ùëû‚Ä≤), and taking infimums completes
the proof.

H Standard Lemmas

Lemma 9 (Theorem 3.1 of Mohri et al. 2012). Let ùê∫ be a family of functions mapping from R to
[0, 1]. Then for ùõø > 0 and all ùëî in ùê∫, with probability at least 1 ‚àí ùõø, we have

Eùëî(ùëç) ‚â§

1
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

ùëî(ùëçùëñ) + 2Rùëõ(ùê∫) +

‚àöÔ∏É

log 1
ùõø
2ùëõ

.

For our excess (‚Ñ±, ùëû)-risk bounds, we also use a slight variant, the proof of which is nearly

identical to that of Lemma 9.

Lemma 10. Let ùê∫ be a family of functions mapping from R to [0, 1]. Then for ùõø > 0 and all ùëî in
ùê∫, with probability at least 1 ‚àí ùõø, we have

‚Éí
‚Éí
‚Éí
‚Éí
‚Éí

Eùëî(ùëç) ‚àí

1
ùëõ

ùëõ
‚àëÔ∏Å

ùëñ=1

‚Éí
‚Éí
‚Éí
ùëî(ùëçùëñ)
‚Éí
‚Éí

‚â§ 4Rùëõ(ùê∫) +

‚àöÔ∏É

log 1
ùõø
2ùëõ

.

30

The following learning bound handles the multi-class margin loss more effectively in the number

of classes (Kuznetsov et al., 2015).

Lemma 11. Let ‚Ñ± be a set of ùëì : ùí≥ √ó ùí¥ ‚Üí R. Recall that

Then, under the margin loss, we have the bound

Œ†1(‚Ñ±) = {ùë• ‚Ü¶‚Üí ùëìùë¶(ùë•) : ùë¶ ‚àà ùí¥, ùëì ‚àà ‚Ñ±} .

ùëÖ(ùëì ) ‚â§ ÃÇÔ∏ÄùëÖ(ùëì ) + 4ùëòRùëõ(Œ†1(‚Ñ±)) +

‚àöÔ∏É

log 1
ùõø
2ùëõ

for all ùëì in ‚Ñ± with probability at least 1 ‚àí ùõø.

I Additional Experiment Details

For all methods and datasets, we optimized a logistic regression model with gradient descent over
the entire data.

For all datasets, we chose a learning rate of 0.01 that was linearly annealed to 0.0001 over 2000

epochs.

I.1 Optimizing LCVaR/LHCVaR formulation

Note that in the formulation for LHCVaR described in Eq. (3), despite its convexity, the optimization
is over a non-smooth loss. Thus, ùúÜ can be explicitly calculated given the classes of each risk. Let
ùëÖ(ùëñ) be the ùëñth largest class risk.

ùúÜ = min

‚éõ

‚éù

‚éß
‚é®

‚é©

ùëÖ(ùëñ) : ùëñ ‚àà [ùëò],

ùëñ
‚àëÔ∏Å

ùëó=1

ÃÇÔ∏Äùëùùëñùõº‚àí1

ùëñ ‚â§ 1

‚é´
‚é¨

‚é≠

‚éû

‚à™ {0}

‚é†

An algorithm for computing this can be akin to water filling in order from largest to smallest
class risk. When optimizing by some form of gradient descent the parameters of the classifier, this
analytic form of the LHCVaR formulation can be quickly computed and avoid gradient computations
on ùúÜ itself. Empirically, we used this formulation to speed up our experiments and leads to faster
convergence than performing gradient descent on ùúÜ in addition to the model parameters. This
algorithm is also applicable when optimizing LCVaR as well.

31

