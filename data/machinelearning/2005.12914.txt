0
2
0
2

y
a
M
6
2

]
L
M

.
t
a
t
s
[

1
v
4
1
9
2
1
.
5
0
0
2
:
v
i
X
r
a

Class-Weighted Classification: Trade-offs and Robust Approaches

Ziyu Xu*

Chen Dan*

Justin Khim*

Pradeep Ravikumar*

May 28, 2020

Abstract

We address imbalanced classification, the problem in which a label may have low marginal
probability relative to other labels, by weighting losses according to the correct class. First, we
examine the convergence rates of the expected excess weighted risk of plug-in classifiers where
the weighting for the plug-in classifier and the risk may be different. This leads to irreducible
errors that do not converge to the weighted Bayes risk, which motivates our consideration of
robust risks. We define a robust risk that minimizes risk over a set of weightings and show excess
risk bounds for this problem. Finally, we show that particular choices of the weighting set leads
to a special instance of conditional value at risk (CVaR) from stochastic programming, which we
call label conditional value at risk (LCVaR). Additionally, we generalize this weighting to derive
a new robust risk problem that we call label heterogeneous conditional value at risk (LHCVaR).
Finally, we empirically demonstrate the efficacy of LCVaR and LHCVaR on improving class
conditional risks.

1

Introduction

Classification is a fundamental problem in statistics and machine learning, including scientific
problems such as cancer diagnosis and satellite image processing as well as engineering applications
such as credit card fraud detection, handwritten digit recognition, and text processing (Khan et al.,
2001; Lee et al., 2004), but modern applications have brought new challenges. In online retailing,
websites such as Amazon have hundreds of thousands or millions of products to taxonomize (Lin
et al., 2018). In text data, the distribution of words in documents has been observed to follow a
power law in that there are many labels with few instances (Zipf, 1936; Feldman, 2019). Similarly,
image data also a long tail of many classes with few examples (Salakhutdinov et al., 2011; Zhu et al.,
2014). In such settings, the classes with smaller probabilities are generally classified incorrectly
more often, and this is undesirable when the smaller classes are important, such as rare forms
of cancer, fraudulent credit card transactions, and expensive online purchases. Thus, we need
modern classification methods that work well when there are a large number of classes and when
the class-wise probabilities are imbalanced.

When faced with such class imbalance a popular approach in practice is to choose a metric
other than zero-one accuracy, such as precision, recall, ğ¹ğ›½-measure (Van Rijsbergen, 1974, 1979),
which explicitly take class conditional risks into account, and train classifiers to optimize this metric.
A difficulty with this approach however is that the right metric for imbalanced classification is
often not clear. A related class of approaches keep the zero-one accuracy metric but modifies the
samples instead. The popular algorithm SMOTE (Chawla et al., 2002) performs a type of data
augmentation for a minority class, i.e., a class with lower probability, and sub-samples the large

*Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213.

1

 
 
 
 
 
 
classes. This has led to variants with different forms of data augmentation (Zhou and Liu, 2006;
Mariani et al., 2018), but from a theoretical perspective, these methods remain poorly understood.
A much simpler approach, which is also related to the approaches above, is class-weighting,
in which different costs are incurred for mis-classifying samples of different labels. Practically,
this is a natural approach because it is often possible to assign different costs to different classes.
For example, the average fraudulent credit card transaction may cost hundreds of dollars, or in
online retailing, failing to show a customer the correct item causes the company to lose out on the
profit of selling that item. Thus, a good classifier should be fairly sensitive to possibly fraudulent
transactions, and online retailers should prioritize displaying high-profit products. As a result,
class-weighting has been studied in a variety of settings, including modifying black-box classifiers,
SVMs, and neural networks (Domingos, 1999; Lin et al., 2002; Scott, 2012; Zhou and Liu, 2006).
Additionally, class-weighting has been observed to be useful for estimating class probabilities, since
class-weighting amounts to adjusting decision thresholds (Wang et al., 2008; Wu et al., 2010; Wang
et al., 2019).

A crucial caveat with cost-weighting however is the right choice of costs is often not clear, and
with any one choice of costs, the performance of the corresponding classifier might suffer for some
other, perhaps more suitable, choices of costs.

In this paper, we use cost-weighting for imbalanced classification in three ways. We start by
examining a weighted sum of class-conditional risks, i.e., the risks conditional on the class ğ‘Œ taking
some specific value ğ‘–. This allows us to upweight a minority class to achieve better performance on
the minority examples. We then provide an illuminating analysis of the fundamental tradeoffs that
occur with any single choice of costs.

Since we may not understand precisely which weighting ğ‘ to pick, we examine a robust risk that
is a supremum of the weighted risks over an uncertainty set ğ‘„ of possible weights. This objective
can be interpreted as a class-wise distributionally robust optimization problem where we ask for
robustness over the marginal distribution of ğ‘Œ . This leads to a minimax problem, for which we
provide generalization guarantees. We also note that a standard gradient descent-ascent algorithm
may solve the optimization problem when the risk is convex in the classifier parameters.

Finally, we show that for a natural class of uncertainty sets, the robust risk reduces to what
call label conditional value at risk (LCVaR). We highlight a connection to conditional value at risk
(CVaR), which is a well-studied quantity in portfolio optimization and stochastic programming
parametrized by an ğ›¼ in (0, 1) (Rockafellar et al., 2000; Shapiro et al., 2009). Further, we propose a
generalization that we call label heterogeneous conditional value at risk (LHCVaR) that allows for
different parameters ğ›¼ğ‘– for each class ğ‘–. To the best of our knowledge, this has not been examined
previously, and it could possibly be used more broadly. To give an example in portfolio optimization,
we may wish to treat risks arising from different types of assets, e.g., large-cap stocks versus small-cap
stocks or domestic debt versus international debt, differently. Next, we show that the dual form for
LHCVaR is similar to that for LCVaR as long as the heterogeneity is finite-dimensional, and this
leads to an unconstrained optimization problem. Finally, we examine the efficacy of LCVaR, and
LHCVaR on real and synthetic data.

The rest of the paper is outlined as follows. In Section 2, we discuss our problem setup. In
Section 3, we examine weighting in plug-in classification. In particular, we elucidate the fundamental
trade-off in weighted classification and its methodological implications. In Section 4, we examine a
robust version of the weighted risk problem, including generalization guarantees and connections
to stochastic programming. In Section 5, we provide numerical results, and we conclude with
a discussion in Section 6. Additional proofs and results in related settings are deferred to the
appendices.

2

1.1 Further Related Work

We briefly review other research related to imbalanced classification, but for a far more exhaustive
treatment, see a survey of the area (He and Garcia, 2009; FernÃ¡ndez et al., 2018). First, two other
methods may be employed to solve imbalanced classification problems. The first is class-based
margin adjustment (Lin et al., 2002; Scott, 2012; Cao et al., 2019), in which the margin parameter
for the margin loss function may vary by class. Broadly, margin adjustment and weighting may both
be considered loss modification procedures. The second method is Neyman-Pearson classification, in
which one attempts to minimize the error on one class given a constraint on the worst permissible
error on the other class (Rigollet and Tong, 2011; Tong, 2013; Tong et al., 2016).

An important topic related to our paper but that has not been well-connected to imbalanced
classification is robust optimization. Robust optimization is a well-studied topic (Ben-Tal and
Nemirovski, 1999, 2003; Ben-Tal et al., 2004, 2009). A variant that has gained traction more recently
is distributionally robust optimization (Ben-Tal et al., 2013; Bertsimas et al., 2014; Namkoong and
Duchi, 2017). Unsurprisingly, CVaR, as a coherent risk measure, has been previously connected
to distributionally robust optimization (Goh and Sim, 2010). Distributionally robust optimization
generally and CVaR specifically have also previously been used in machine learning to deal with
imbalance (Duchi et al., 2018; Duchi and Namkoong, 2018), but in these works, the imbalance was
considered to exist in the covariates, whether known to the algorithm or not. These are motivated
by the recent push toward fairness in machine learning, in particular so that ethnic minorities do
not suffer discrimination in high-stakes situations such as loan applications, medical diagnoses, or
parole decisions, due to biases in the data.

2 Preliminaries

2.1 Classification with Imbalanced Classes

In this section, we briefly go over the problem setup. First, we draw samples from the space ğ’µ = ğ’³ Ã—ğ’´.
For our purposes, we are interested in ğ’´ = {0, 1} or ğ’´ = {1, . . . , ğ‘˜}. Note there are two slightly
different mechanisms for the data-generating process that are considered in imbalanced classification
and Neyman-Pearson classification. In the first, we are given ğ‘› i.i.d. samples (ğ‘‹1, ğ‘Œ1), . . . , (ğ‘‹ğ‘›, ğ‘Œğ‘›)
from a distribution ğ‘ƒğ‘‹,ğ‘Œ . Here, we let ğ‘ğ‘– = P (ğ‘Œ = ğ‘–) be the probability of class ğ‘–. Additionally, we
sometimes refer to the vector of class probabilities as ğ‘. This is our framework of interest, since
it corresponds to standard assumptions in nonparametric statistics and learning theory. In the
alternative framework, we are given ğ‘›ğ‘– samples (ğ‘‹1, ğ‘–), . . . , (ğ‘‹ğ‘›ğ‘–, ğ‘–) from each marginal distribution
ğ‘ƒğ‘‹|ğ‘Œ =ğ‘–. The probability of class ğ‘– in this case is then known: ğ‘ğ‘– = Ì‚ï¸€ğ‘ğ‘– = ğ‘›ğ‘–/ğ‘›. For the most
part, these two mechanisms yield similar results, but the analyses differ slightly. To streamline the
presentation, we only consider the first case in the main paper, although we give a result for the
alternative framework in the appendix that illustrates the difference.

2.2 Class Conditioned Risk

We are interested in finding a good classifier ğ‘“ : ğ’³ â†’ ğ’Ÿ âŠ‡ ğ’´ in some function space â„±, such as
linear classifiers or neural networks. In this section, we establish our risk measures of interest. In
general, we want to minimize the expectation of some loss function â„“ : â„± Ã— ğ’µ â†’ [0, 1], which we call
risk and denote ğ‘…(ğ‘“ ) = E[â„“(ğ‘“, ğ‘)]. Analogously, we define the class-conditioned risk for class ğ‘– to be

ğ‘…â„“,ğ‘–(ğ‘“ ) = E [â„“(ğ‘“, ğ‘)|ğ‘Œ = ğ‘–] .

3

At this point, we make some observations for plug-in classification and empirical risk minimization.
In the plug-in classification results, we consider the zero-one loss â„“01(ğ‘“, ğ‘§) = 1{ğ‘“ (ğ‘¥) Ì¸= ğ‘¦}, and for
our results on empirical risk minimization, we are primarily interested in convex surrogate losses.
For simplicity, when â„“ is clear from context, or a statement is made for a generic â„“, we will denote
this as ğ‘…ğ‘–.

Now, we can work toward defining weighted risks. We defined Observe that we can relate the
ğ‘–âˆˆğ’´ ğ‘ğ‘–ğ‘…ğ‘–(ğ‘“ ). An important part of our

risk to the class-conditioned risk by ğ‘…(ğ‘“ ) = E [ğ‘…ğ‘Œ (ğ‘“ )] = âˆ‘ï¸€
paper is an examination of class-weighted risk.
Definition 1. Let ğ‘ = (ğ‘1, . . . , ğ‘|ğ’´|) be a vector such that ğ‘ğ‘– â‰¥ 0 for all ğ‘– and E[ğ‘ğ‘Œ ] = âˆ‘ï¸€
Then, the ğ‘-weighted risk is

ğ‘–âˆˆğ’´ ğ‘ğ‘–ğ‘ğ‘– = 1.

ğ‘…ğ‘(ğ‘“ ) = E [ğ‘ğ‘Œ ğ‘…ğ‘Œ (ğ‘“ )] = âˆ‘ï¸

ğ‘ğ‘–ğ‘ğ‘–ğ‘…ğ‘–(ğ‘“ ).

ğ‘–âˆˆğ’´

Note that the usual risk is recovered by setting ğ‘ = (1, . . . , 1).

2.3 Plug-in Classification

In this section, we discuss weighted plug-in classification. For plug-in, we restrict our attention
to the binary classification case of ğ’´ = {0, 1}, and the primary quantity of interest is usually the
one-zero risk ğ‘…01(ğ‘“ ) i.e the risk under â„“0,1. In general, the risk for the best classifier is nonzero
because for a given ğ‘¥ in ğ’³ , there is some probability it may take the value 0 or 1.

As a result, we need a way to discuss the convergence of our estimator to the best possible
estimator. We define the regression function ğœ‚ by ğœ‚(ğ‘¥) = P (ğ‘Œ = 1|ğ‘‹ = ğ‘¥) . Now, the Bayes optimal
classifier is the classifier that minimizes the risk, and it is defined by ğ‘“ *(ğ‘¥) = 1 {ğœ‚(ğ‘¥) > 1/2} . The
minimum possible risk is called the Bayes risk and denoted by ğ‘…* = ğ‘…(ğ‘“ *), and generally we focus
on minimizing the excess risk â„°(ğ‘“ ) = ğ‘…(ğ‘“ ) âˆ’ ğ‘…*.

Following the form of the Bayes classifier, a plug-in estimator Ì‚ï¸€ğ‘“ attempts to estimate the
regression function ğœ‚ by some Ì‚ï¸€ğœ‚ and then â€œplugs inâ€ the result to a threshold function. Thus, Ì‚ï¸€ğ‘“
has the form Ì‚ï¸€ğ‘“ (ğ‘¥) = 1 {Ì‚ï¸€ğœ‚(ğ‘¥) > 1/2} , which is analogous to the form of the Bayes classifier. For
additional background on plug-in estimation, see, e.g., Devroye et al. (1996).

At this point, we wish to define the weighted versions of Bayes classifier, Bayes risk, plug-in
classifier, and excess risk. For brevity, define the threshold ğ‘¡ğ‘ = ğ‘0/(ğ‘0 + ğ‘1). First, we consider the
Bayes classifier.

Lemma 1. Let ğ‘ = (ğ‘0, ğ‘1) be a weighting. The Bayes optimal classifier for ğ‘-weighted risk is
ğ‘“ *
ğ‘ (ğ‘¥) = 1 {ğœ‚(ğ‘¥) > ğ‘¡ğ‘} .

The proof, along with proofs of other subsequent results on plug-in classification, appears in
the appendix. In this case, we denote the Bayes risk by ğ‘…*
ğ‘ ). Lemma 1 reveals that the
Bayes classifier is a plug-in rule, and analogously, we see that a plug-in estimator in the weighted
case takes the form Ì‚ï¸€ğ‘“ğ‘(ğ‘¥) = 1 {Ì‚ï¸€ğœ‚(ğ‘¥) > ğ‘¡ğ‘} . Consequently, we define excess ğ‘-risk for an empirical
classifier Ì‚ï¸€ğ‘“ . The excess ğ‘-risk for an empirical classifier is â„°ğ‘( Ì‚ï¸€ğ‘“ ) = ğ‘…ğ‘( Ì‚ï¸€ğ‘“ ) âˆ’ ğ‘…*
ğ‘, and note that we are
interested in bounding the expected excess ğ‘-risk for plug-in estimators.

ğ‘ = ğ‘…ğ‘(ğ‘“ *

2.4 Empirical Risk Minimization

In this section, we define empirical quantities that we need for empirical risk minimization, par-
ticularly the weighted and robust risks. We consider ğ’´ = {1, . . . , ğ‘˜}. We define the empirical

4

class-conditioned risk by Ì‚ï¸€ğ‘…ğ‘– = (1/ğ‘ğ‘–) âˆ‘ï¸€ğ‘›
ğ‘—=1 1 {ğ‘¦ğ‘— = ğ‘–}. Let
Ì‚ï¸€ğ‘ğ‘– = ğ‘ğ‘–/ğ‘› denote the empirical proportion of observations of class ğ‘–, and let ğ‘ be a weight vector.
The empirical ğ‘-weighted risk is

ğ‘—=1 â„“(ğ‘“, ğ‘§ğ‘—)1 {ğ‘¦ğ‘– = ğ‘–} where ğ‘ğ‘– = âˆ‘ï¸€ğ‘›

Ì‚ï¸€ğ‘…ğ‘(ğ‘“ ) =

ğ‘˜
âˆ‘ï¸

ğ‘–=1

ğ‘ğ‘– Ì‚ï¸€ğ‘ğ‘– Ì‚ï¸€ğ‘…ğ‘–(ğ‘“ ).

The empirical ğ‘„-weighted risk is defined analogously by Ì‚ï¸€ğ‘…ğ‘„ = supğ‘âˆˆğ‘„ Ì‚ï¸€ğ‘…ğ‘(ğ‘“ ). This problem is convex
in ğ‘“ when the loss â„“ is convex and concave in ğ‘ due to linearity; so one may solve the resulting
saddle-point problem with standard techniques such as gradient descent-ascent, which we give in
the appendix.

Often in empirical risk minimization, generalization bounds are provided, i.e., a bound on the
true risk of a classifier ğ‘“ in â„± in terms of its empirical risk and a variance term. To bring our
results closer to those of plug-in estimation, we also consider a form of excess risk. To distinguish
the two, define the excess (â„±, ğ‘„)-weighted risk to be â„°ğ‘„(â„±) = ğ‘…ğ‘„( Ì‚ï¸€ğ‘“ ) âˆ’ ğ‘…ğ‘„(ğ‘“ *
ğ‘„) where here Ì‚ï¸€ğ‘“ is the
ğ‘„-weighted empirical risk minimizer in â„± and ğ‘“ *
ğ‘„ is the population ğ‘„-weighted risk minimizer in
â„±. Beyond the robust formulation, the key difference between excess ğ‘-weighted risk and excess
(â„±, ğ‘„)-weighted risk is that in the former we compete with the true regression function, and in the
latter we compete with the best classifier in â„±.

One additional tool we need for empirical risk minimization is a measure of function class
complexity, and a typical measure of the expressiveness of a function class is Rademacher complexity.
The empirical Rademacher complexity given a sample (ğ‘‹1, ğ‘Œ1), . . . , (ğ‘‹ğ‘›, ğ‘Œğ‘›) is

Ì‚ï¸€Rğ‘›(â„±) = Eğœ sup
ğ‘“ âˆˆâ„±

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğœğ‘–ğ‘“ (ğ‘‹ğ‘–),

where the expectation is taken with respect to the ğœğ‘–, which are Rademacher random variables.
The Rademacher complexity is Rğ‘›(â„±) = E Ì‚ï¸€Rğ‘›(â„±), where the expectation is with respect to the ğ‘‹ğ‘–
random variables.

Finally, we make one note about the loss for our empirical risk minimization results. For binary
classification, one can obtain bounds for any bounded loss function that is Lipschitz continuous in
ğ‘“ (ğ‘¥). Since we present multiclass results, we use the multiclass margin loss, which is a bounded
version of the multiclass hinge loss (Mohri et al., 2012). Here, it is assumed that for each ğ‘– in ğ’´, the
function ğ‘“ outputs a score ğ‘“ğ‘–(ğ‘¥), and the chosen class is argmaxğ‘–âˆˆğ’´ ğ‘“ğ‘–(ğ‘¥). The multiclass margin loss
is defined as â„“mar(ğ‘“, ğ‘§) = Î¦ (ï¸€ğ‘“ğ‘¦(ğ‘¥) âˆ’ maxğ‘¦â€²Ì¸=ğ‘¦ ğ‘“ğ‘¦â€²(ğ‘¥))ï¸€ where Î¦(ğ‘) = 1 {ğ‘ â‰¤ 0} + (1 âˆ’ ğ‘)1 {0 < ğ‘ â‰¤ 1}.
For simplicity, we ignore the margin parameter, usually denoted by ğœŒ, and treat it as 1 in our results.
Finally, we define the projection set Î 1(â„±) = {ğ‘¥ â†¦â†’ ğ‘“ğ‘¦(ğ‘¥) : ğ‘¦ âˆˆ ğ’´, ğ‘“ âˆˆ â„±} .

3 Tradeoffs with Class Weighted Risk

In this section, we examine weighted plug-in classification, and we have two main results. First,
we show that weighted plug-in classification enjoys essentially the same rate of convergence as
unweighted plug-in classification, although there is dependence on the chosen weights. Second,
there is a fundamental trade-off in that optimizing for one set of weights ğ‘ may lead to suboptimal
performance for another set of weights ğ‘â€².

3.1 Excess Risk Bounds

We start with the excess risk bound for plug-in estimators when the weighting is well-specified.

5

Figure 1. The irreducible error (IE) and estimation error (EE). The irreducible error is the measure of the
set of ğ‘¥ where ğœ‚(ğ‘¥) is between thresholds of ğ‘â€² and ğ‘, which does not depend on Ì‚ï¸€ğœ‚. The estimation error is
the measure of the ğ‘¥ for which Ì‚ï¸€ğœ‚(ğ‘¥) and ğœ‚(ğ‘¥) lead to different plug-in estimates.

Proposition 1. Suppose the regression function ğœ‚ is ğ›½-HÃ¶lder. Then, the ğ‘-weighted excess risk of
Ì‚ï¸€ğ‘“ğ‘ satisfies

Eâ„°ğ‘( Ì‚ï¸€ğ‘“ğ‘) â‰¤ ğ‘‚

(ï¸‚

(ğ‘0 + ğ‘1)ğ‘›âˆ’ ğ›½

2ğ›½+ğ‘‘

)ï¸‚

.

Here, we see that the upper bound depends linearly on ğ‘0 and ğ‘1. This implies that when we
increase the weight for a class with few examples, then our bound on the excess risk increases. While
previous cost weighting setups have normalized the sum of weights Scott (2012), our normalization
scheme is computed with respect to prior probabilities on each class as well, and consequently we
explicitly include ğ‘0, ğ‘1 in our bound. Our choice of domain for weights is defined in Section 4.

Now, we turn to our second task: examining the weighted excess risk of the Ì‚ï¸€ğ‘“ğ‘ under a different

weighting ğ‘â€². Observe that we can decompose the excess risk as
+ ğ‘…ğ‘â€²(ğ‘“ *

Eâ„°ğ‘â€²( Ì‚ï¸€ğ‘“ğ‘) = Eğ‘…ğ‘â€²( Ì‚ï¸€ğ‘“ğ‘) âˆ’ ğ‘…ğ‘â€²(ğ‘“ *
ğ‘ )

âŸ

 â
estimation error

âŸ

=: (EE) + (IE).

ğ‘ ) âˆ’ ğ‘…ğ‘â€²(ğ‘“ *
ğ‘â€²)
 â
irreducible error

(1)

Unsurprisingly, we see that an error term that is constant, or â€irreducibleâ€ appears in equation (1).
Then, we see the irreducible error is given by the measure of the subset of ğ’³ where ğœ‚(ğ‘¥) lies between
ğ‘¡ğ‘ and ğ‘¡ğ‘â€². Given that we know the Bayes optimal classifier for any weighting, we observe that the
irreducible error can be upper bounded by a term proportional to the the product of the measure
of ğ‘ƒğ‘‹ in the region between ğ‘¡ğ‘ and ğ‘¡ğ‘â€², and the difference between the thresholds themselves. We
state this formally in the following proposition.

Proposition 2. Let ğ‘¡ğ‘,ğ‘â€² = min{ğ‘¡ğ‘, ğ‘¡ğ‘â€²} and ğ‘¡ğ‘,ğ‘â€² = max{ğ‘¡ğ‘, ğ‘¡ğ‘â€²}. The irreducible error satisfies the
bound

(IE) â‰¤ (ğ‘â€²

0 + ğ‘â€²

1) âƒ’

âƒ’ğ‘¡ğ‘ âˆ’ ğ‘¡ğ‘â€²

âƒ’ P (ï¸
âƒ’

ğ‘¡ğ‘,ğ‘â€² â‰¤ ğœ‚(ğ‘‹) â‰¤ ğ‘¡ğ‘,ğ‘â€²

)ï¸

6

 
 
A visualization is given in Figure 1. Now, we turn to analyze the estimation error. The result is
in many ways similar to Proposition 1, but an additional term appears due to the decision threshold
ğ‘¡ğ‘ for Ì‚ï¸€ğœ‚ differing from that of the risk measurement ğ‘¡ğ‘â€².
Proposition 3. For any density estimator Ì‚ï¸€ğœ‚, the estimation error satisfies
(ï¸‚
ğ‘›âˆ’ ğ›½

(EE) â‰¤ (ğ‘â€²

)ï¸]ï¸)ï¸‚

+ âƒ’

Ì‚ï¸€ğ‘“ğ‘(ğ‘¥) Ì¸= ğ‘“ *

âƒ’ E [ï¸P (ï¸
âƒ’

âƒ’ğ‘¡ğ‘â€² âˆ’ ğ‘¡ğ‘

2ğ›½+ğ‘‘

ğ‘‚

0 + ğ‘â€²
1)

ğ‘ (ğ‘¥)

(ï¸‚

)ï¸‚

Corollary 1. When ğœ‚ is ğ›½-HÃ¶lder, using local polynomial estimator Yang (1999) for Ì‚ï¸€ğœ‚ gives

(EE) â‰¤(ğ‘â€²

0 + ğ‘â€²

1)ğ‘‚

(ï¸‚
ğ‘›âˆ’ ğ›½

2ğ›½+ğ‘‘

)ï¸‚

+ (ğ‘â€²

0 + ğ‘â€²

1) âƒ’

âƒ’ğ‘¡ğ‘â€² âˆ’ ğ‘¡ğ‘

âƒ’ E [ï¸P (ï¸
âƒ’

Ì‚ï¸€ğ‘“ğ‘(ğ‘¥) Ì¸= ğ‘“ *

ğ‘ (ğ‘¥)

)ï¸]ï¸

Consequently, we can upper bound the expected excess ğ‘â€²-risk. The probability in the bound
of the estimation error has been considered in the context of nearest neighbors (Chaudhuri and
Dasgupta, 2014), but in general, additional assumptions are required to provide an explicit rate.
We consider one such assumption in the appendix.

4 Robust Class Weighted Risk

Based the results in the previous section, we know that the performance degradation need not be
graceful when we donâ€™t know how to choose the weights. This motivates us to study a more robust
version of class weighted risk.

Definition 2. Let ğ‘„ âŠ† R|ğ’´| be a compact convex set such that ğ‘ğ‘– â‰¥ 0 for each ğ‘– and E[ğ‘ğ‘Œ ] = 1 for
each ğ‘ in ğ‘„. Then, the ğ‘„-weighted risk is

ğ‘…ğ‘„(ğ‘“ ) = sup
ğ‘âˆˆğ‘„

E [ğ‘ğ‘Œ ğ‘…ğ‘Œ (ğ‘“ )] = sup
ğ‘âˆˆğ‘„

ğ‘ğ‘–ğ‘ğ‘–ğ‘…ğ‘–(ğ‘“ ).

âˆ‘ï¸

ğ‘–âˆˆğ’´

Additionally, we refer to the set ğ‘„ as the uncertainty set.

In this section, we have two goals: (1) to provide excess â„±-risk bounds and generalization
bounds for robust weighted risk via uniform convergence and (2) to make connections to stochastic
optimization via special choices of uncertainty set. We start with generalization; the proofs are
given in the appendix.
Theorem 1. Let â„“ = â„“mar be the multiclass margin loss. Recall that ğ‘ğ‘– = âˆ‘ï¸€ğ‘›
probability at least 1 âˆ’ ğ›¿, we have the generalization bound

ğ‘—=1 1 {ğ‘¦ğ‘— = ğ‘–}. With

ğ‘…ğ‘„(ğ‘“ ) â‰¤ sup
ğ‘âˆˆğ‘„

â§
âªâ¨

âªâ©

Ì‚ï¸€ğ‘…ğ‘(ğ‘“ ) +

ğ‘˜
âˆ‘ï¸

ğ‘–=1

ğ‘ğ‘–ğ‘ğ‘– Ã—

â›
â4ğ‘˜E
âœ

[ï¸‚ ğ‘ğ‘–
ğ‘ğ‘–ğ‘›

]ï¸‚
Ì‚ï¸€Rğ‘ğ‘–(Î 1(â„±))

+

â¯
â¸
â¸
â·

log ğ‘˜
ğ›¿
2ğ‘2
ğ‘– ğ‘›

â

âŸ
â 

â«
âªâ¬

âªâ­

for every ğ‘“ in â„± and the excess risk bound

â„°ğ‘„(â„±) â‰¤ 2 sup
ğ‘âˆˆğ‘„

ğ‘˜
âˆ‘ï¸

ğ‘–=1

ğ‘ğ‘–ğ‘ğ‘– Ã—

â›
â8ğ‘˜E
âœ

[ï¸‚ ğ‘ğ‘–
ğ‘ğ‘–ğ‘›

]ï¸‚
Ì‚ï¸€Rğ‘ğ‘–(Î 1(â„±))

+

â¯
â¸
â¸
â·

log ğ‘˜
ğ›¿
2ğ‘2
ğ‘– ğ‘›

â

âŸ
â  .

7

A few remarks are in order. First, note that we only use the multiclass margin loss because it
leads to simple multiclass bounds. In a binary classification setting, standard results would imply
generalization for other Lipschitz losses. Second, in many cases, we can simplify the Rademacher
complexity term. The following result applies to commonly-used function classes such as linear
functions and neural networks (Bartlett et al., 2017; Golowich et al., 2018; Mohri et al., 2012).

Corollary 2. Let â„“ = â„“mar be the multiclass margin loss. Let â„± be a function class satisfying
Ì‚ï¸€Rğ‘›(Î 1(â„±)) â‰¤ ğ¶(â„±)ğ‘›âˆ’1/2 for some constant ğ¶(â„±) that does not depend on ğ‘›. Then with probability
at least 1 âˆ’ ğ›¿, we have the generalization bound

ğ‘…ğ‘„(ğ‘“ ) â‰¤ sup
ğ‘âˆˆğ‘„

and the excess (â„±, ğ‘)-risk bound

â§
âªâ¨

âªâ©

Ì‚ï¸€ğ‘…ğ‘(ğ‘“ ) +

ğ‘˜
âˆ‘ï¸

ğ‘–=1

ğ‘ğ‘–ğ‘ğ‘– Ã—

â›

âœ
â

4ğ‘˜ğ¶(â„±)
ğ‘ğ‘–ğ‘›

âˆš

â¯
â¸
â¸
â·

+

log ğ‘˜
ğ›¿
2ğ‘2
ğ‘– ğ‘›

â

âŸ
â 

â«
âªâ¬

âªâ­

â„°ğ‘„(â„±) â‰¤ 2 sup
ğ‘âˆˆğ‘„

â›

âœ
â

ğ‘˜
âˆ‘ï¸

ğ‘–=1

ğ‘ğ‘–ğ‘ğ‘–

8ğ‘˜ğ¶(â„±)
ğ‘ğ‘–ğ‘›

âˆš

â¯
â¸
â¸
â·

+

â

âŸ
â  .

log ğ‘˜
ğ›¿
2ğ‘2
ğ‘– ğ‘›

4.1 Connections to Stochastic Programming

In this section, we make concrete connections to stochastic programming (Shapiro et al., 2009).
First, we introduce label conditional value at risk, and then we describe the generalization, label
heterogeneous conditional value at risk.

4.1.1 Label CVaR

We start with the definition.
Definition 3. Let ğ›¼ in (0, 1) be given. Define the set ğ‘„ğ›¼ = {ï¸€ğ‘ : E[ğ‘ğ‘Œ ] = 1, ğ‘ğ‘– âˆˆ [ï¸€0, ğ›¼âˆ’1]ï¸€ for ğ‘– âˆˆ 1, . . . , ğ‘˜}ï¸€ .
The label conditional value at risk (LCVaR) is LCVaRğ›¼(ğ‘“ ) = ğ‘…ğ‘„ğ›¼(ğ‘“ ).

ğ›¼

ğ›¼

Eğ‘„[ğ‘] = supğ‘„âˆˆğ‘„*

E[(ğ‘‘ğ‘„/ğ‘‘ğ‘ƒ )ğ‘], where ğ‘„*

Now, we describe the connection to CVaR. Letting ğ‘ be a random variable, the CVaR of ğ‘
at level ğ›¼ is CVaRğ›¼(ğ‘) = supğ‘„âˆˆğ‘„*
ğ›¼ is the set of all
probability measures that are absolutely continuous with respect to the underlying measure ğ‘ƒ such
that ğ‘‘ğ‘„/ğ‘‘ğ‘ƒ â‰¤ ğ›¼âˆ’1. If ğ‘ takes values on a finite discrete probability space with probability mass
function ğ‘, then the CVaR may be written as CVaRğ›¼(ğ‘) = supğ‘âˆˆğ‘„ğ›¼
ğ‘–=1 ğ‘ğ‘–ğ‘ğ‘–ğ‘. Thus, LCVaR is
a specialization of CVaR to the variables ğ‘…ğ‘Œ (ğ‘“ ), which take values on the finite discrete space ğ’´.
Notably, this is in contrast to other uses of CVaR in machine learning where, as noted previously,
CVaR is used with respect to samples directly, in order to provide robustness or fairness. As with
CVaR, LCVaR is a straightforward way to provide robustness. Intuitively, it moves weight to the
worst losses, where all weightings are bounded by the same constant ğ›¼âˆ’1. Now, we consider the
dual form.

âˆ‘ï¸€ğ‘˜

Proposition 4 (LCVaR dual form). LCVaR permits the dual formulation

LCVaRğ›¼(ğ‘“ ) = inf
ğœ†âˆˆR

E[(ğ‘…ğ‘Œ (ğ‘“ ) âˆ’ ğœ†)+] + ğœ†

}ï¸‚

.

{ï¸‚ 1
ğ›¼

Moreover, if â„± is compact in the supremum norm on ğ’³ and â„“ is continuous, then the dual form
holds for all ğ‘“ in â„±.

8

The proof is mostly standard and therefore deferred to the appendix. The only trick compared
with CVaR is showing that we may restrict the domain of ğœ† to a compact set; which essentially
requires showing that the process {ğ‘…ğ‘Œ (ğ‘“ ) : ğ‘“ âˆˆ â„±} is sufficiently well-behaved. It would also
suffice to assume that â„“ is bounded, as with most theoretical results in learing theory. Note that to
minimize LCVaR, we can solve this convex program in ğœ† and ğ‘“ .

4.1.2 Label Heterogeneous CVaR

While the LCVaR approach of the previous section is useful for providing some robustness in a
computationally tractable manner, it may not be best suited for imbalanced classification because
it treats all classes identically in that each ğ‘ğ‘– must lie in the interval [0, ğ›¼âˆ’1]. Since imbalanced
classification is inherently a problem of heterogeneity, we may wish to allow ğ‘ğ‘– to be in some interval
[0, ğ›¼âˆ’1
ğ‘–

] instead. We can formalize this problem as follows.

Definition 4. Define the uncertainty set ğ‘„ğ»,ğ›¼ =
. We
call the resulting optimization problem label heterogeneous conditional value at risk (LHCVaR), and
we write

] for ğ‘– = 1, . . . , ğ‘˜

ğ‘–

{ï¸
ğ‘ : E[ğ‘ğ‘Œ ] = 1, ğ‘ğ‘– âˆˆ [0, ğ›¼âˆ’1

}ï¸

LHCVaRğ›¼(ğ‘“ ) = sup

ğ‘âˆˆğ‘„ğ»,ğ›¼

E [ğ‘ğ‘Œ ğ‘…ğ‘Œ (ğ‘“ )] .

Similar to LCVaR, this has a dual form.

Proposition 5. A dual form for LHCVaR is given by

LHCVaRğ›¼(ğ‘“ ) = inf
ğœ†âˆˆR

E [ï¸

ğ›¼âˆ’1

ğ‘Œ (ğ‘…ğ‘Œ (ğ‘“ ) âˆ’ ğœ†)+

]ï¸

+ ğœ†.

Moreover, if â„± is compact in the supremum norm on ğ’³ and â„“ is continuous, then the dual form
holds for all ğ‘“ in â„±.

Again, we note that an alternative sufficient condition for the dual to hold for all ğ‘“ in â„± is that
â„“ be bounded. Importantly, the label heterogeneous CVaR dual form is convex in ğ‘“ and ğœ†. As a
result, we can still optimize efficiently, in principle.

We also note that the finite dimension ğ‘˜ is crucial for label heterogeneous CVaR. This is due to
our use of the minimax theorem, which requires compactness in various places; so in general this
result cannot be extended to the infinite-dimensional case.

5 Numerical Results

5.1 Methods

We examine the empirical performance of LCVaR and LHCVaR risks, and compare them against
the standard risk and a balanced risk as baselines. Let Ì‚ï¸€ğ‘ğ‘– be the empirical proportion of the ğ‘–th
label and Ì‚ï¸€ğ‘…ğ‘– be the empirical class conditional risk.

Balanced risk Here, we consider the specific weighting where each class is equally weighted:

i.e., we fix ğ‘ğ‘– = 1/(ğ‘˜ Ì‚ï¸€ğ‘ğ‘–).

Ì‚ï¸€ğ‘…1/(ğ‘˜Ì‚ï¸€ğ‘)(ğ‘“ ) =

1
ğ‘˜

ğ‘˜
âˆ‘ï¸

ğ‘–=1

Ì‚ï¸€ğ‘…ğ‘–(ğ‘“ )

9

(a) Class 0 risk

(b) Class 1 risk

(c) Worst class risk

Figure 2. Plots of class 0, class 1, and worst class risk on the test dataset under different choices of 1 âˆ’ ğ‘
in the synthetic experiment. The worst test class risk is the maximum of the risks of the two classes for
each choice of the probability of class 0. LCVaR and LHCVaR performs better in worst class risk than both
standard and balanced risks as class imbalance increases.

LCVaR The empirical formulation optimizes the dual formulation, in which ğ›¼ is a hyperparameter:

(cid:92)LCVaRğ›¼(ğ‘“ ) = min
ğœ†âˆˆR

{ï¸ƒ

1
ğ›¼

ğ‘˜
âˆ‘ï¸

ğ‘–=1

Ì‚ï¸€ğ‘ğ‘–( Ì‚ï¸€ğ‘…ğ‘–(ğ‘“ ) âˆ’ ğœ†)+ + ğœ†

.

}ï¸ƒ

(2)

LHCVaR We similarly optimize a dual form in the empirical LHCVaR risk. To reduce the number
of hyperparameters to only ğ‘ âˆˆ (0, 1] and ğœ… âˆˆ (0, âˆ), we calculate ğ›¼ğ‘– as follows:

(ï¸ƒ

ğ›¼(ğœ…,ğ‘)
ğ‘–

= ğ‘

1/ğœ…

Ì‚ï¸€ğ‘ğ‘–
ğ‘—=1 Ì‚ï¸€ğ‘ğ‘—

âˆ‘ï¸€ğ‘˜

)ï¸ƒ

.

1/ğœ…

(3)

ğœ… behaves as a temperature parameter (similar to Jang et al. 2016; Wang et al. 2020) and causes
ğ›¼ to become a smoother distribution of weights when ğœ… > 1 and converge to uniform weights as
ğœ… â†’ âˆ. Conversely, when ğœ… < 1, the alpha distribution becomes sharper and heavily weights the
classes with lowest Ì‚ï¸€ğ‘ğ‘– as ğœ… â†’ 0. We simply choose a ğœ… of 1 unless otherwise stated. ğ‘ consequently
characterizes the total magnitude of the weights. Ultimately, we formulate the empirical risk as:

(cid:92)LHCVaRğœ…,ğ‘(ğ‘“ ) = inf
ğœ†âˆˆR

{ï¸ƒ ğ‘˜

âˆ‘ï¸

ğ‘–=1

Ì‚ï¸€ğ‘ğ‘–
ğ›¼(ğœ…,ğ‘)
ğ‘–

( Ì‚ï¸€ğ‘…ğ‘–(ğ‘“ ) âˆ’ ğœ†)+ + ğœ†

}ï¸ƒ

We train a logistic regression model with gradient descent on a cross entropy loss, which acts as

a convex surrogate loss for zero-one risk.

5.2 Datasets

We evaluate our methods on both synthetic and real datasets.

Synthetic Datasets The data in our synthetic experiment is constructed for ğ’³ = [0, 1] and
ğ’´ = {0, 1}. For a given ğ‘ = ğ‘ƒ (ğ‘Œ = 0), we generated a dataset by uniformly randomly sampling an
ğ‘‹ in [0, 1] and sampling a ğ‘Œ with the following distribution:

ğ‘ƒ (ğ‘Œ = 1 | ğ‘‹ = ğ‘¥) = ğ‘¥

ğ‘
1âˆ’ğ‘

ğ‘ƒ (ğ‘Œ = 0 | ğ‘‹ = ğ‘¥) = 1 âˆ’ ğ‘¥

ğ‘
1âˆ’ğ‘ .

10

(a) Varying ğ›¼ for LCVaR

(b) Varying ğœ… for LHCVaR with ğ›¼ =
0.01

Figure 3. Worst class risk of different ğ›¼ values for LCVaR and ğœ… values for LHCVaR in the synthetic setting.
Across different levels of class imbalance, ğ›¼ and ğœ… do not have a significant impact on worst class risk of
LCVaR and LHCVaR.

In these synthetic datasets, we note that the Bayes optimal classifier and class risks are:

{ï¸ƒ

ğ‘“ *(ğ‘¥) = 1

ğ‘¥ >

)ï¸‚ 1âˆ’ğ‘
ğ‘

}ï¸ƒ

(ï¸‚ 1
2

)ï¸‚ 1
ğ‘

(ï¸‚ 1
2

ğ‘…0(ğ‘“ *) = 1 âˆ’ (1 + ğ‘)

ğ‘…1(ğ‘“ *) =

)ï¸‚ 1
ğ‘

.

(ï¸‚ 1
2

When ğ‘ is high, ğ‘…0(ğ‘“ *) < ğ‘…1(ğ‘“ *), which leads to a classifier that has vastly worse performance
on class 1 compared to class 0. This discrepancy in class risk is a common issue in classification
problems where there is a significant class imbalance.

We randomly generated 100,000 data points for both train and test sets. We generated datasets

for each value of ğ‘ from 0.80 to 0.98, inclusive, in steps of 0.02.

Real World Datasets We also experiment on the Covertype dataset taken from the UCI dataset
repository Dua and Graff (2017). This dataset is 53-dimensional with 7 classes and has 2%-98%
(11340-565892 examples) train-test split.

(a) Standard

(b) Balanced

(c) LCVaR

(d) LHCVaR

Figure 4. Histogram of class risks for each method on the Covertype dataset. The red line marks the largest
risk for each method. The distribution of class risks for standard and balanced methods are more spread out,
while the class risks for LCVaR and LHCVaR are more concentrated near the max class risk. The max class
risks are slightly lower for LCVaR and LHCVaR compared to the other two methods.

11

5.3 Results

Synthetic In Fig. 2, we can observe that the the worst case class risk of LCVaR and LHCVaR
across multiple values of ğ‘ is better than both the standard and balanced classifier. The classwise
risks of LCVaR and LHCVaR are relatively close across different values of ğ‘, while there is a large
discrepancy between classwise risks of the classifier trained under the standard or balanced risks.
Note that the more significant the imbalance, i.e., the smaller the ğ‘, the better LCVaR and LHCVaR
perform compared to balanced risk on class 0, while paying a progressively smaller price on the class
1 risk. The same is also true between both LCVaR and LHCVaR and the standard risk, although
with the classes swapped. We note that while the worst class risk of LCVaR and LHCVaR seem
to decrease with greater imbalance, this may not be a general property of these methods. Rather,
this is more likely an artifact of the synthetic setup having more probability mass further from the
decision boundary as the imbalance increases. The main observation is simply that LCVaR and
LHCVaR have lower worst class risk in comparison to the baseline methods. Thus, this empirically
demonstrates that both LCVaR and LHCVaR can significantly improve the highest class risks while
losing little in performance on classes with lower risks.

In addition to comparing against baselines, we also examine the effect of different choices of ğ›¼
and ğœ… on LCVaR and LHCVaR, respectively. The results of this comparison are in Fig. 3. In both
methods, varying the hyperparameters does not have a dramatic impact on the behavior of the
worst class risk for both these methods across different values of class imbalance.

Table 1. Standard risk and risk of the worst class for each method on the Covertype dataset. LCVaR and
LHCVaR improve on the worst class risk.

Method Standard Risk Worst Class Risk
LHCVaR
LCVaR
Standard
Balanced

0.3979
0.3384
0.3275
0.3765

0.4907
0.5037
0.5111
0.5333

Table 2. Performance of LCVaR across different ğ›¼ values, and LHCVaR across different ğœ… values. The
performance each method is relatively agnostic to choices of ğ›¼ and ğœ…, although the smallest choices of ğ›¼ and
ğœ… for each method have the largest changes in worst class risk, respectively.

Method

ğ›¼

ğœ…

Standard Risk Worst Class Risk

LCVaR

LHCVaR

0.01 N/A
0.05 N/A
0.1 N/A
0.8
0.05
1
0.05
1.2
0.05

0.4266
0.3993
0.4060
0.4308
0.3979
0.4171

0.5474
0.4932
0.5037
0.5408
0.4907
0.5050

In Table 1, we observe that LCVaR and LHCVaR have better worst class risks than the
Real
standard and class weighted baselines. However, improving worst class risk comes at a cost to to the
standard risk in the case of both LCVaR and LHCVaR. This tradeoff is reflected in the histograms of
class risk shown in Fig. 4, where the class risks under the standard and balanced classifiers are more
spread out and have classes with much lower risks. On the other hand, LCVaR and LHCVaR have
class risk distributions that are more concentrated towards the worst class risk value. Consequently,

12

LCVaR and LHCVaR achieve a lower worst class risk, which is consistent with our theory.

We also compare the effect of choosing different ğ›¼ and ğœ… on LCVaR and LHCVaR, respectively,
in Table 2. We see that the worst class risk still performs well under different choices of ğ›¼ and
ğœ…, although there is some degradation when the ğ›¼ is smaller than optimal choice, in the case of
LCVaR, and when ğœ… is smaller and produces a sharper distribution, in the case of LHCVaR.

6 Discussion

In this work, we have studied the effect of optimizing classifiers with respect to different weightings
and developed robust risk measures that minimizes worst case weighted risk across a set of weightings.
We subsequently show that optimizing with respect to LCVaR and LHCVaR empirically improves the
worst class risk, at a reasonable cost to accuracy. One future direction for research is to understand
the Bayes optimal classifier under LCVaR and LHCVaR. Another more applied direction could
be to consider domain shift. If we formalize each prior over the classes as a weighting, optimizing
LCVaR or LHCVaR may improve performance when the test class priors are different from the
training class priors.

References

J.-Y. Audibert and A. B. Tsybakov. Fast learning rates for plug-in classifiers. The Annals of

Statistics, 35(2):608â€“633, 2007.

P. L. Bartlett, D. J. Foster, and M. J. Telgarsky. Spectrally-normalized margin bounds for neural

networks. In Advances in Neural Information Processing Systems, pages 6240â€“6249, 2017.

A. Ben-Tal and A. Nemirovski. Robust solutions of uncertain linear programs. Operations research

letters, 25(1):1â€“13, 1999.

A. Ben-Tal and A. Nemirovski. Robust solutions of linear programming problems contaminated

with uncertain data. Mathematical programming, 88(3):411â€“424, 2003.

A. Ben-Tal, A. Goryashko, E. Guslitzer, and A. Nemirovski. Adjustable robust solutions of uncertain

linear programs. Mathematical Programming, 99(2):351â€“376, 2004.

A. Ben-Tal, L. El Ghaoui, and A. Nemirovski. Robust Optimization. Princeton University Press,

2009.

A. Ben-Tal, D. Den Hertog, A. De Waegenaere, B. Melenberg, and G. Rennen. Robust solutions of
optimization problems affected by uncertain probabilities. Management Science, 59(2):341â€“357,
2013.

D. Bertsimas, V. Gupta, and N. Kallus. Robust sample average approximation. Mathematical

Programming, pages 1â€“66, 2014.

K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma. Learning imbalanced datasets with label-

distribution-aware margin loss. arXiv preprint arXiv:1906.07413, 2019.

K. Chaudhuri and S. Dasgupta. Rates of convergence for nearest neighbor classification. In Advances

in Neural Information Processing Systems, pages 3437â€“3445, 2014.

13

N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. SMOTE: synthetic minority

over-sampling technique. Journal of Artificial Intelligence Research, 16:321â€“357, 2002.

L. Devroye, L. GyÃ¶rfi, and G. Lugosi. A probabilistic theory of pattern recognition. Springer Science

& Business Media, 1996.

P. Domingos. Metacost: A general method for making classifiers cost-sensitive. In KDD, volume 99,

pages 155â€“164, 1999.

D. Dua and C. Graff. Uci machine learning repository, 2017.

J. Duchi and H. Namkoong. Learning models with uniform performance via distributionally robust

optimization. arXiv preprint arXiv:1810.08750, 2018.

J. C. Duchi, T. Hashimoto, and H. Namkoong. Distributionally robust losses against mixture

covariate shifts. Arxiv, 2018.

V. Feldman. Does learning require memorization? a short tale about a long tail. arXiv preprint

arXiv:1906.05271, 2019.

A. FernÃ¡ndez, S. GarcÃ­a, M. Galar, R. C. Prati, B. Krawczyk, and F. Herrera. Learning from

imbalanced data sets. Springer, 2018.

J. Goh and M. Sim. Distributionally robust optimization and its tractable approximations. Operations

research, 58(4-part-1):902â€“917, 2010.

N. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks.

In Conference On Learning Theory, pages 297â€“299, 2018.

L. GyÃ¶rfi. The Rate of Convergence of kn-NN Regression Estimates and Classification Rule. IEEE

Transactions on Information Theory, 27(3):357â€“362, 1981. ISSN 0018-9448.

E. Hazan. Introduction to online convex optimization. Foundations and Trends Râ—‹ in Optimization,

2(3-4):157â€“325, 2016.

H. He and E. A. Garcia. Learning from imbalanced data. IEEE Transactions on knowledge and

data engineering, 21(9):1263â€“1284, 2009.

E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint

arXiv:1611.01144, 2016.

J. Khan, J. S. Wei, M. Ringner, L. H. Saal, M. Ladanyi, F. Westermann, F. Berthold, M. Schwab,
C. R. Antonescu, C. Peterson, et al. Classification and diagnostic prediction of cancers using gene
expression profiling and artificial neural networks. Nature medicine, 7(6):673, 2001.

O. O. Koyejo, N. Natarajan, P. K. Ravikumar, and I. S. Dhillon. Consistent Binary Classification
with Generalized Performance Metrics. In Advances in Neural Information Processing Systems
27, pages 2744â€“2752. Curran Associates, Inc., 2014.

A. Krzyzak and M. Pawlak. The pointwise rate of convergence of the kernel regression estimate.

Journal of Statistical Planning and Inference, 16:159â€“166, 1987.

V. Kuznetsov, M. Mohri, and U. Syed. Rademacher complexity margin bounds for learning with a
large number of classes. In ICML Workshop on Extreme Classification: Learning with a Very
Large Number of Labels, 2015.

14

Y. Lee, G. Wahba, and S. A. Ackerman. Cloud classification of satellite radiance data by multicategory
support vector machines. Journal of Atmospheric and Oceanic Technology, 21(2):159â€“169, 2004.

D. D. Lewis. Evaluating and optimizing autonomous text classification systems. In SIGIR, volume 95,

pages 246â€“254. Citeseer, 1995.

Y. Lin, Y. Lee, and G. Wahba. Support vector machines for classification in nonstandard situations.

Machine learning, 46(1-3):191â€“202, 2002.

Y.-C. Lin, P. Das, and A. Datta. Overview of the SIGIR 2018 eCom Rakuten Data Challenge. In

eCOM@ SIGIR, 2018.

G. Mariani, F. Scheidegger, R. Istrate, C. Bekas, and C. Malossi. Bagan: Data augmentation with

balancing gan. arXiv preprint arXiv:1803.09655, 2018.

A. Menon, H. Narasimhan, S. Agarwal, and S. Chawla. On the statistical consistency of algorithms
for binary classification under class imbalance. In International Conference on Machine Learning,
pages 603â€“611, 2013.

M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of Machine Learning. MIT Press, 2012.

H. Namkoong and J. C. Duchi. Variance-based regularization with convex objectives. In Advances

in Neural Information Processing Systems, pages 2971â€“2980, 2017.

H. Narasimhan, R. Vaish, and S. Agarwal. On the statistical consistency of plug-in classifiers for
non-decomposable performance measures. In Advances in Neural Information Processing Systems,
pages 1493â€“1501, 2014.

P. Rigollet and X. Tong. Neyman-pearson classification, convexity and stochastic constraints.

Journal of Machine Learning Research, 12(Oct):2831â€“2855, 2011.

R. T. Rockafellar, S. Uryasev, et al. Optimization of conditional value-at-risk. Journal of risk, 2:

21â€“42, 2000.

R. Salakhutdinov, A. Torralba, and J. Tenenbaum. Learning to share visual appearance for multiclass

object detection. In CVPR 2011, pages 1481â€“1488. IEEE, 2011.

C. Scott. Calibrated asymmetric surrogate losses. Electronic Journal of Statistics, 6:958â€“992, 2012.

A. Shapiro, D. Dentcheva, and A. RuszczyÅ„ski. Lectures on stochastic programming: modeling and

theory. SIAM, 2009.

C. J. Stone. Optimal Global Rates of Convergence for Nonparametric Regression. The Annals of

Statistics, 10(4):1040â€“1053, 1982.

X. Tong. A plug-in approach to neyman-pearson classification. The Journal of Machine Learning

Research, 14(1):3011â€“3040, 2013.

X. Tong, Y. Feng, and A. Zhao. A survey on neyman-pearson classification and suggestions for
future research. Wiley Interdisciplinary Reviews: Computational Statistics, 8(2):64â€“81, 2016.

C. J. Van Rijsbergen. Foundation of evaluation. Journal of Documentation, 30(4):365â€“373, 1974.

C. J. Van Rijsbergen. Information Retrieval. Butterworth-Heinemann, London, 2nd edition, 1979.

15

J. Wang, X. Shen, and Y. Liu. Probability estimation for large-margin classifiers. Biometrika, 95

(1):149â€“167, 2008.

X. Wang, H. Helen Zhang, and Y. Wu. Multiclass probability estimation with support vector

machines. Journal of Computational and Graphical Statistics, pages 1â€“18, 2019.

X. Wang, Y. Tsvetkov, and G. Neubig. Balancing training for multilingual neural machine translation.

arXiv preprint arXiv:2004.06748, 2020.

Y. Wu, H. H. Zhang, and Y. Liu. Robust model-free multiclass probability estimation. Journal of

the American Statistical Association, 105(489):424â€“436, 2010.

Y. Yang. Minimax nonparametric classification. i. rates of convergence. IEEE Transactions on

Information Theory, 45(7):2271â€“2284, 1999.

Z.-H. Zhou and X.-Y. Liu. Training cost-sensitive neural networks with methods addressing the
class imbalance problem. IEEE Transactions on Knowledge and Data Engineering, 18(1):63â€“77,
2006.

X. Zhu, D. Anguelov, and D. Ramanan. Capturing long-tail distributions of object subcategories. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 915â€“922,
2014.

G. K. Zipf. The Psycho-Biology of Language: an Introduction to Dynamic Philology. George

Routledge & Sons, Ltd., 1936.

16

A Organization

Our appendices contain proofs, all of which are omitted from the main text, and additional details
on the weighting approach to imbalanced classification. In Appendix B, we prove our results for
plug-in classification. Additionally, we show that a threshold-shifted version of Tsybakovâ€™s noise
condition implies precise rates for the convergence of expected excess risk. Finally, we briefly discuss
the universality of weighting, i.e., the fact that choosing the correct weighting is often the means to
optimizing other classification metrics, for a class of classification metrics.

In Appendix C, we show a result analogous to Proposition 3 for empirical risk minimization.
ğ‘â€² for weights

However, the result is less illuminating, since it depends on the optimal classifiers ğ‘“ *
ğ‘ and ğ‘â€² within the class â„±, which is difficult to analyze more precisely in any generality.

ğ‘ ğ‘“ *

In Appendix D, we prove our results for robust weighting. This includes both the convergence
and duality results. In Appendix E, we prove the analog of Theorem 1 for the conditional sampling
model. The only difference to observe is that the bounded differences inequality is used with respect
to a different number of variables, which leads to a slightly stronger bound.

In Appendix F, we discuss gradient descent-ascent, which is a standard algorithm for solving
robust optimization problems. This may be used in cases where the uncertainty set ğ‘„ does not
lead to LCVaR or LHCVaR. In Appendix G and Appendix H, we provide technical and standard
lemmas respectively.

Finally, we include additional experiment details, and an algorithm for analytically deriving

dual variables in the empirical LCVaR and LHCVaR formulations in Appendix I.

B Plug-in Classification Details

In this appendix, we provide additional details surrounding plug-in classification. We first start
with the proofs of results from the main text, and then we provide more concrete results based on
an additional assumption of that gives us faster rates of convergence. Finally, we provide details on
the universality of weighting.

For simplicity, we assume that our density estimator Ì‚ï¸€ğœ‚ is a local polynomial estimator (Stone,
1982), but the properties that the estimator must have for the following proofs to succeed can also
be satisfied by other nonparametric estimators such as kernelized regression (Krzyzak and Pawlak,
1987), and nearest-neighbors regression (GyÃ¶rfi, 1981).

B.1 Proofs

Proof of Lemma 1. By the definition of the ğ‘-weighted risk and the tower property, we have

ğ‘…01,ğ‘(ğ‘“ ) = E[ğ‘ğ‘Œ ğ‘…ğ‘Œ (ğ‘“ )]

= E [ğ‘0(1 âˆ’ ğœ‚(ğ‘‹))E[1 {ğ‘“ (ğ‘‹) = 1} |ğ‘Œ = 0] + ğ‘1ğœ‚(ğ‘‹)E[1 {ğ‘“ (ğ‘‹) = 0} |ğ‘Œ = 1]]

=E [ğ‘0(1 âˆ’ ğœ‚(ğ‘‹))1 {ğ‘“ (ğ‘‹) = 1} + ğ‘1ğœ‚(ğ‘‹)1 {ğ‘“ (ğ‘‹) = 0}] .
By inspection, we observe that the ğ‘“ * minimizing the ğ‘-risk satisfies

ğ‘“ *(ğ‘¥) =

{ï¸ƒ1
0

ğ‘0(1 âˆ’ ğœ‚(ğ‘¥)) < ğ‘1ğœ‚(ğ‘¥)
ğ‘0(1 âˆ’ ğœ‚(ğ‘¥)) > ğ‘1ğœ‚(ğ‘¥).

When ğ‘0(1 âˆ’ ğœ‚(ğ‘¥)) = ğ‘1ğœ‚(ğ‘¥), we note that the decision may be arbitrary because it does not affect
the risk. So, by simple algebraic manipulation, we have

ğ‘“ *(ğ‘¥) = 1

{ï¸‚

ğœ‚(ğ‘¥) â‰¥

ğ‘0
ğ‘0 + ğ‘1

}ï¸‚

,

17

which completes the proof.

Now, we turn to Proposition 1, Proposition 2, and Proposition 3. Our proofs rely on the following
lemma of Yang (1999). First, we introduce a few additional definitions. Denote the ğœ€-entropy of Î£
with respect to the ğ¿ğ‘ norm for 1 â‰¤ ğ‘ â‰¤ âˆ by â„‹(ğœ€, Î£, ğ¿ğ‘). We define the norm

â€–Ì‚ï¸€ğœ‚ âˆ’ ğœ‚â€–ğ¿1(ğ‘ƒğ‘‹ ) =

âˆ«ï¸

|ğœ‚(ğ‘¥) âˆ’ Ì‚ï¸€ğœ‚(ğ‘¥)| ğ‘‘ğ‘ƒğ‘‹

.

Lemma 2 (Theorem 1 of Yang 1999). Let ğœ‚ be an element of Î£ where Î£ is a class of functions
from Rğ‘‘ to [0, 1]. Suppose the ğœ€-entropy satisfies

â„‹(ğœ€, Î£, ğ¿ğ‘) â‰¤ ğ¶ğœ€âˆ’ğœŒ,

where ğ¶ > 0, ğœŒ > 0. Then the minimax upper bound on the mean convergence rate of any regression
estimator Ì‚ï¸€ğœ‚ is

E [ï¸

min
Ì‚ï¸€ğœ‚

max
ğœ‚âˆˆÎ£

â€–ğœ‚ âˆ’ Ì‚ï¸€ğœ‚â€–ğ¿1(ğ‘ƒğ‘‹ )

]ï¸

â‰¤ ğ‘‚

(ï¸

ğ‘›âˆ’ 1

2+ğœŒ

)ï¸

,

where the expectation is taken over the samples for estimating Ì‚ï¸€ğœ‚.

The upper bound converges at a rate of ğ‘‚

where ğœŒ is a smoothness parameter for ğœ‚,
with standard assumptions on the function class of ğœ‚. For the class of ğ›½-HÃ¶lder functions, ğœŒ = ğ›½/ğ‘‘,
which is our setting of interest.

(ï¸

ğ‘›âˆ’1/(2+ğœŒ))ï¸

Proof of Proposition 1. We start by bounding the excess ğ‘-risk for a classifier ğ‘“ by

â„°ğ‘(ğ‘“ ) = ğ‘…ğ‘(ğ‘“ ) âˆ’ ğ‘…ğ‘(ğ‘“ *
ğ‘ )

= (ğ‘0 + ğ‘1)

âˆ«ï¸ âƒ’
âƒ’
âƒ’
âƒ’

ğœ‚(ğ‘¥) âˆ’

ğ‘0
ğ‘0 + ğ‘1

âƒ’
âƒ’
âƒ’
âƒ’

1

{ï¸

ğ‘“ (ğ‘¥) Ì¸= ğ‘“ *

ğ‘ (ğ‘¥)

ğ‘‘ğ‘ƒğ‘‹

}ï¸

âˆ«ï¸

â‰¤ (ğ‘0 + ğ‘1)

|ğœ‚(ğ‘¥) âˆ’ Ì‚ï¸€ğœ‚(ğ‘¥)| ğ‘‘ğ‘ƒğ‘‹ ,

where the upper bound follows when |ğœ‚(ğ‘¥) âˆ’ ğ‘0/(ğ‘0 + ğ‘1)| â‰¤ |ğœ‚(ğ‘¥) âˆ’ Ì‚ï¸€ğœ‚(ğ‘¥)| when ğ‘“ (ğ‘¥) Ì¸= ğ‘“ *
applying Lemma 2 for ğ›½-HÃ¶lder functions as noted above completes the proof.

ğ‘ (ğ‘¥). Finally,

Proof of Proposition 2. The proposition follows from basic algebraic manipulations and one common
observation in nonparametric classification. We have

]ï¸

ğ‘ (ğ‘‹))

(IE) = E [ï¸
âˆ«ï¸

=

ğ‘…ğ‘â€²(ğ‘“ *

ğ‘â€²(ğ‘‹)) âˆ’ ğ‘…ğ‘â€²(ğ‘“ *
1ğœ‚(ğ‘¥)âƒ’
{ï¸

0(1 âˆ’ ğœ‚(ğ‘¥)) + ğ‘â€²

âƒ’
âƒ’ğ‘â€²

âˆ«ï¸

0 + ğ‘â€²
1)
1) âƒ’

0 + ğ‘â€²

âƒ’
âƒ’ğœ‚(ğ‘¥) âˆ’ ğ‘¡ğ‘â€²
âƒ’ P (ï¸
âƒ’

âƒ’ğ‘¡ğ‘ âˆ’ ğ‘¡ğ‘â€²

âƒ’
âƒ’ 1

= (ğ‘â€²

â‰¤ (ğ‘â€²

ğ‘â€²(ğ‘¥) Ì¸= ğ‘“ *
ğ‘“ *
ğ‘ (ğ‘¥)
)ï¸

ğ‘â€²(ğ‘‹) Ì¸= ğ‘“ *
ğ‘“ *

ğ‘ (ğ‘‹)

,

{ï¸

âƒ’ ğ‘‘ğ‘ƒğ‘‹ 1

ğ‘â€²(ğ‘¥) Ì¸= ğ‘“ *
ğ‘“ *

ğ‘ (ğ‘¥)

}ï¸

}ï¸

ğ‘‘ğ‘ƒğ‘‹

where in the inequality we use the fact that if ğ‘“ *
âƒ’ â‰¤ |ğ‘¡ğ‘,ğ‘â€² âˆ’ ğ‘¡ğ‘,ğ‘â€²| = âƒ’
âƒ’
Thus, we have âƒ’
âƒ’ğ‘¡ğ‘ âˆ’ ğ‘¡ğ‘â€²

âƒ’ğœ‚(ğ‘¥) âˆ’ ğ‘¡ğ‘â€²

âƒ’
âƒ’ .

ğ‘â€²(ğ‘‹) Ì¸= ğ‘“ *

ğ‘ (ğ‘‹) then ğœ‚(ğ‘‹) must be in [ğ‘¡ğ‘,ğ‘â€², ğ‘¡ğ‘,ğ‘â€²].

18

Proof of Proposition 3. Recall that the expected estimation error is

(EE) = E [ï¸

ğ‘…ğ‘â€²( Ì‚ï¸€ğ‘“ğ‘) âˆ’ ğ‘…ğ‘â€²(ğ‘“ *
ğ‘ )

]ï¸

We can upper bound the term inside the expectation by

ğ‘…â€²

ğ‘( Ì‚ï¸€ğ‘“ğ‘) âˆ’ ğ‘…ğ‘â€²(ğ‘“ *

ğ‘ ) =

âˆ«ï¸

âˆ«ï¸

=

0(1 âˆ’ ğœ‚(ğ‘¥))1{ Ì‚ï¸€ğ‘“ğ‘(ğ‘¥) = 1} + ğ‘â€²
ğ‘â€²
âˆ«ï¸

1ğœ‚(ğ‘¥)1{ Ì‚ï¸€ğ‘“ğ‘(ğ‘¥) = 0}ğ‘‘ğ‘ƒğ‘‹

âˆ’

0(1 âˆ’ ğœ‚(ğ‘¥))1{ğ‘“ *
ğ‘â€²

ğ‘ (ğ‘¥) = 1} + ğ‘â€²

1ğœ‚(ğ‘¥)1{ğ‘“ *

ğ‘ (ğ‘¥) = 0}ğ‘‘ğ‘ƒğ‘‹

(ğ‘â€²

0(1 âˆ’ ğœ‚(ğ‘¥)) âˆ’ ğ‘â€²

1ğœ‚(ğ‘¥))1{ Ì‚ï¸€ğ‘“ğ‘(ğ‘¥) = 1, ğ‘“ *
âˆ«ï¸

ğ‘ (ğ‘¥) = 0}ğ‘‘ğ‘ƒğ‘‹

1{ Ì‚ï¸€ğ‘“ğ‘(ğ‘¥) = 0, ğ‘“ *

ğ‘ (ğ‘¥) = 1}ğ‘‘ğ‘ƒğ‘‹

+ (ğ‘â€²

0(1 âˆ’ ğœ‚(ğ‘¥)))
âƒ’
âƒ’
âƒ’
âƒ’

1ğœ‚(ğ‘¥) âˆ’ ğ‘â€²
âˆ«ï¸ âƒ’
ğ‘â€²
âƒ’
0
âƒ’
0 + ğ‘â€²
ğ‘â€²
âƒ’
1
(ï¸€|ğœ‚(ğ‘¥) âˆ’ ğ‘¡ğ‘| + âƒ’

ğœ‚(ğ‘¥) âˆ’

âˆ«ï¸

= (ğ‘â€²

0 + ğ‘â€²
1)

â‰¤ (ğ‘â€²

0 + ğ‘â€²
1)

1{ Ì‚ï¸€ğ‘“ğ‘(ğ‘¥) Ì¸= ğ‘“ *

ğ‘ (ğ‘¥)}ğ‘‘ğ‘ƒğ‘‹

âƒ’ğ‘¡ğ‘â€² âˆ’ ğ‘¡ğ‘

âƒ’
)ï¸€ 1{ Ì‚ï¸€ğ‘“ğ‘(ğ‘¥) Ì¸= ğ‘“ *
âƒ’

ğ‘ (ğ‘¥)}ğ‘‘ğ‘ƒğ‘‹ ,

where we use the triangle inequality in the final line. Next, using the fact that |ğœ‚(ğ‘¥)âˆ’ğ‘¡ğ‘| â‰¤ |ğœ‚(ğ‘¥)âˆ’Ì‚ï¸€ğœ‚(ğ‘¥)|
when ğ‘“ (ğ‘¥) Ì¸= ğ‘“ *

ğ‘ (ğ‘¥), we have

ğ‘…â€²

ğ‘( Ì‚ï¸€ğ‘“ğ‘) âˆ’ ğ‘…ğ‘â€²(ğ‘“ *

ğ‘ ) â‰¤ (ğ‘â€²

0 + ğ‘â€²
1)

(ï¸‚âˆ«ï¸

|ğœ‚(ğ‘¥) âˆ’ ğ‘¡ğ‘| 1

+ âƒ’

âƒ’ğ‘¡ğ‘â€² âˆ’ ğ‘¡ğ‘

âƒ’ P (ï¸
âƒ’

Ì‚ï¸€ğ‘“ğ‘(ğ‘¥) Ì¸= ğ‘“ *

ğ‘ (ğ‘¥)

}ï¸

ğ‘ (ğ‘¥)

ğ‘‘ğ‘ƒğ‘‹

{ï¸
Ì‚ï¸€ğ‘“ğ‘(ğ‘¥) Ì¸= ğ‘“ *
)ï¸ )ï¸‚

(ï¸‚âˆ«ï¸

â‰¤ (ğ‘â€²

0 + ğ‘â€²
1)

|ğœ‚(ğ‘¥) âˆ’ Ì‚ï¸€ğœ‚(ğ‘¥)| ğ‘‘ğ‘ƒğ‘‹ + âƒ’

âƒ’ğ‘¡ğ‘â€² âˆ’ ğ‘¡ğ‘

âƒ’ P (ï¸
âƒ’

Ì‚ï¸€ğ‘“ğ‘(ğ‘¥) Ì¸= ğ‘“ *

ğ‘ (ğ‘¥)

)ï¸)ï¸‚

Thus, we obtain the upper bound

(EE) â‰¤ (ğ‘â€²

0 + ğ‘â€²
1)

[ï¸‚âˆ«ï¸

(ï¸‚

E

|ğœ‚(ğ‘¥) âˆ’ Ì‚ï¸€ğœ‚(ğ‘¥)| ğ‘‘ğ‘ƒğ‘‹

]ï¸‚

+ âƒ’

âƒ’ğ‘¡ğ‘â€² âˆ’ ğ‘¡ğ‘

âƒ’ E [ï¸P (ï¸
âƒ’

Ì‚ï¸€ğ‘“ğ‘(ğ‘¥) Ì¸= ğ‘“ *

ğ‘ (ğ‘¥)

)ï¸]ï¸)ï¸‚

Therefore we have completed the proof. Applying Lemma 2 to the first term also proves Corollary
1.

B.2 Shifted Margin Assumption

An important tool in nonparametric classification is the Tsybakov margin condition.

Definition 5. A distribution ğ‘ƒğ‘‹,ğ‘Œ satisfies the (ğ›¼, ğ¶)-margin condition if for all ğ‘¡ > 0, we have

(ï¸‚

P

0 â‰¤

âƒ’
âƒ’
âƒ’
âƒ’

ğœ‚(ğ‘‹) âˆ’

)ï¸‚

â‰¤ ğ‘¡

âƒ’
âƒ’
âƒ’
âƒ’

1
2

â‰¤ ğ¶ğ‘¡ğ›¼.

Subsequent works (Audibert and Tsybakov, 2007; Chaudhuri and Dasgupta, 2014) leverage this
assumption to provide fast, explicit rates of convergence for expected risk. The margin condition is
naturally suited to standard plug-in classification because the decision threshold is 1/2; for weighted
plug-in classification, we need a shifted margin condition.

19

Definition 6. A distribution ğ‘ƒğ‘‹,ğ‘Œ satisfies the (ğ‘, ğ›¼, ğ¶)-margin condition if for all ğ‘¡ > 0, we have

P (0 â‰¤ |ğœ‚(ğ‘¥) âˆ’ ğ‘¡ğ‘| â‰¤ ğ‘¡) â‰¤ ğ¶ğ‘¡ğ›¼.

Using the shifted margin condition, we can obtain better results than we presented in the main
paper. However, the shifted margin condition may be be less interpretable than the original margin
condition. Intuitively, the original margin condition says that there is very little probability mass
where distinguishing between ğ‘Œ = 0 and ğ‘Œ = 1 is difficult, i.e., near ğœ‚(ğ‘‹) = 1/2. For other ğ‘¡ğ‘, the
decision may not be difficult in that ğ‘¡ğ‘ may be far from 1/2, but we would still require little mass
near this point.

Proposition 6. Suppose the distribution ğ‘ƒğ‘‹,ğ‘Œ satisfies the (ğ‘, ğ›¼, ğ¶)-margin condition and ğ‘‹ has a
density that is lower bounded by some constant ğœ‡min on its support. Additionally, suppose that ğœ‚ is
ğ›½-HÃ¶lder. Then, the excess expected ğ‘â€²-risk of Ì‚ï¸€ğ‘“ğ‘ satisfies the bound

Eâ„°ğ‘â€²( Ì‚ï¸€ğ‘“ğ‘) â‰¤ (ğ‘â€²

0 + ğ‘â€²
1)

â›
âğ‘‚

(ï¸‚ log ğ‘›
ğ‘›

)ï¸‚ ğ›½

2ğ›½+ğ‘‘

+ âƒ’

âƒ’ğ‘¡ğ‘â€² âˆ’ ğ‘¡ğ‘

âƒ’
âƒ’ ğ‘‚

)ï¸‚ ğ›¼ğ›½

2ğ›½+ğ‘‘

â
â  + (IE)

(ï¸‚ log ğ‘›
ğ‘›

Before proving this proposition, we prove a helpful lemma that leverages the shifted margin

condition, similar to one from Audibert and Tsybakov (2007).

Lemma 3. For a fixed density estimate Ì‚ï¸€ğœ‚, if ğ‘ƒğ‘‹,ğ‘Œ satisfies the (ğ‘, ğ›¼, ğ¶)-margin condition, then
following upper bound is always true:

P (ï¸

Ì‚ï¸€ğ‘“ğ‘(ğ‘¥) Ì¸= ğ‘“ *

ğ‘ (ğ‘¥), ğœ‚(ğ‘¥) Ì¸= ğ‘¡ğ‘

)ï¸

â‰¤ ğ¶ â€–ğœ‚ âˆ’ Ì‚ï¸€ğœ‚â€–ğ›¼
âˆ .

Proof. We use a simple upper bound on the error probability event and apply the margin condition
to obtain

P (ï¸

Ì‚ï¸€ğ‘“ğ‘(ğ‘¥) Ì¸= ğ‘“ *

ğ‘ (ğ‘¥), ğœ‚(ğ‘¥) Ì¸= ğ‘¡ğ‘)

)ï¸

â‰¤ P (0 â‰¤ |ğœ‚(ğ‘¥) âˆ’ ğ‘¡ğ‘| â‰¤ |ğœ‚(ğ‘¥) âˆ’ Ì‚ï¸€ğœ‚(ğ‘¥)|)
â‰¤ P (0 â‰¤ |ğœ‚(ğ‘¥) âˆ’ ğ‘¡ğ‘| â‰¤ â€–ğœ‚ âˆ’ Ì‚ï¸€ğœ‚â€–âˆ)
â‰¤ ğ¶0 â€–ğœ‚ âˆ’ Ì‚ï¸€ğœ‚â€–ğ›¼
âˆ .

This completes the proof.

Since, by Lemma 3, we have proved an upper bound in terms of â€–ğœ‚ âˆ’ Ì‚ï¸€ğœ‚â€–ğ›¼

âˆ, we now cite an upper

bound on that quantity that is a property of regression estimator.

Lemma 4 (Theorem 1 of Stone 1982). Let Ì‚ï¸€ğœ‚ be a local polynomial regression estimator, and suppose
ğ‘‹ has a density that is lower bounded by some constant ğœ‡min > 0 on its support. Then, we have the
following upper bound:

E [â€–ğœ‚ âˆ’ Ì‚ï¸€ğœ‚â€–ğ›¼

âˆ] â‰¤ ğ¶

(ï¸‚ log ğ‘›
ğ‘›

)ï¸‚ ğ›¼ğ›½

2ğ›½+ğ‘‘

.

(4)

20

The above bound is the optimal rate of uniform convergence for nonparametric estimators under
the regularity conditions shown here, and local polynomial regression achieves this optimal rate
(Stone, 1982).

Proof of Proposition 6. It suffices to prove an upper bound on the estimation error. We have

(EE) â‰¤ (ğ‘â€²

0 + ğ‘â€²
1)

[ï¸‚âˆ«ï¸

(ï¸‚

E

|ğœ‚(ğ‘¥) âˆ’ Ì‚ï¸€ğœ‚(ğ‘¥)| ğ‘‘ğ‘ƒğ‘‹

]ï¸‚

+ âƒ’

âƒ’ğ‘¡ğ‘â€² âˆ’ ğ‘¡ğ‘

âƒ’ E [ï¸P( Ì‚ï¸€ğ‘“ğ‘(ğ‘¥) Ì¸= ğ‘“ *
âƒ’

ğ‘ (ğ‘¥))

]ï¸)ï¸‚

by the final equation of the proof of Proposition 3. Next, we use the fact that for all ğ‘¥ in ğ’³ we
have ğœ‚(ğ‘¥) âˆ’ Ì‚ï¸€ğœ‚(ğ‘¥) â‰¤ â€–ğœ‚ âˆ’ Ì‚ï¸€ğœ‚â€–âˆ and Lemma 3 to obtain
1) (ï¸€E [â€–ğœ‚ âˆ’ Ì‚ï¸€ğœ‚â€–âˆ] + âƒ’

âƒ’
âƒ’ ğ¶0E [â€–ğœ‚ âˆ’ Ì‚ï¸€ğœ‚â€–ğ›¼

(EE) â‰¤ (ğ‘â€²

âƒ’ğ‘¡ğ‘â€² âˆ’ ğ‘¡ğ‘

0 + ğ‘â€²

âˆ])ï¸€

Finally, we apply Lemma 4 to obtain

(EE) â‰¤ (ğ‘â€²

0 + ğ‘â€²
1)

â›
âğ¶

(ï¸‚ log ğ‘›
ğ‘›

)ï¸‚ ğ›½

2ğ›½+ğ‘‘

+ âƒ’

âƒ’ğ‘¡ğ‘â€² âˆ’ ğ‘¡ğ‘

âƒ’
âƒ’ ğ¶0ğ¶

)ï¸‚ ğ›¼ğ›½

2ğ›½+ğ‘‘

â
â  ,

(ï¸‚ log ğ‘›
ğ‘›

which completes the proof.

B.3 Universality of Weighting

Since we may be interested in performance in error metrics other than risk, we discuss other
classification metrics here. In particular, we simply show that weighting is â€œuniversalâ€ in that it
can be used to optimize these other classification metrics. The reason for this is that, in plug-in
classification, optimizing many classification metrics is equivalent to altering the threshold for the
classification, and this has been observed to lead to the optimal decision rule in many cases (Lewis,
1995; Menon et al., 2013; Narasimhan et al., 2014; Koyejo et al., 2014). We examine the specific
case of metrics considered in Koyejo et al. (2014).

Definition 7. Let ğ‘“ be a classifier over ğ’³ . Define the true positive, false negative, false positive,
and true negative proportions to be

TP = P(ğ‘Œ = 1, ğ‘“ (ğ‘‹) = 1)
FN = P(ğ‘Œ = 1, ğ‘“ (ğ‘‹) = 0)

FP = P(ğ‘Œ = 0, ğ‘“ (ğ‘‹) = 1)
TN = P(ğ‘Œ = 0, ğ‘“ (ğ‘‹) = 0).

A linear-fractional metric is defined as

â„’(ğ‘“, ğ‘ƒğ‘‹ , ğœ‚) =

ğ‘0 + ğ‘11TP + ğ‘10FP + ğ‘01FN + ğ‘00TN
ğ‘0 + ğ‘11TP + ğ‘10FP + ğ‘01FN + ğ‘00TN

for constants ğ‘0, ğ‘11, ğ‘10, ğ‘01, ğ‘00, ğ‘0, ğ‘11, ğ‘10, ğ‘01, ğ‘00.

Koyejo et al. (2014) showed that the optimal classifier for any linear-fractional metric is simply

a threshold classifier. Specifically, the following theorem is true.

Theorem 2 (Koyejo et al. 2014). Let â„’ be a linear-fractional metric, and let ğ‘ƒğ‘‹ be absolutely
continuous with respect to the dominating measure ğœˆ on ğ’³ . Define

â„’* = max

ğ‘“

â„’(ğ‘“, ğ‘ƒğ‘‹ , ğœ‚)

21

and

ğ›¿* =

(ğ‘10 âˆ’ ğ‘00)â„’* âˆ’ ğ‘10 + ğ‘00
ğ‘11 âˆ’ ğ‘10 âˆ’ ğ‘01 + ğ‘00 âˆ’ (ğ‘11 âˆ’ ğ‘10 âˆ’ ğ‘01 + ğ‘00)â„’* .

Then, the optimal classifier for â„’ is ğ‘“ *

â„’(ğ‘¥) = 1 {ğœ‚(ğ‘¥) > ğ›¿*} if

ğ‘11 âˆ’ ğ‘10 âˆ’ ğ‘01 + ğ‘00 âˆ’ (ğ‘11 âˆ’ ğ‘10 âˆ’ ğ‘01 + ğ‘00)â„’* > 0

and ğ‘“ *

â„’(ğ‘¥) = 1 {ğœ‚(ğ‘¥) < ğ›¿*} otherwise.

Corollary 3. We note by Proposition 1 that for an metric â„’ where

ğ‘11 âˆ’ ğ‘10 âˆ’ ğ‘01 + ğ‘00 âˆ’ (ğ‘11 âˆ’ ğ‘10 âˆ’ ğ‘01 + ğ‘00)â„’* > 0,

if we set define ğ‘ to be

then ğ‘“ *

ğ‘ = ğ‘“ *
â„’.

ğ‘0 = (ğ‘10 âˆ’ ğ‘00)â„’* âˆ’ ğ‘10 + ğ‘00
ğ‘1 = (ğ‘01 âˆ’ ğ‘11)â„’* âˆ’ ğ‘01 + ğ‘11,

Performance metrics that are used in evaluating classifiers such as F1 and arithmetic mean
satisfy the the conditions of Corollary 3. Thus, we can reformulate optimization of a classifier in
these error metrics as a specific weighting the risk.

C The Fundamental Trade-off in Empirical Risk Minimization

Part of our motivation for the robust weighted problem is the fundamental trade-off under different
weightings ğ‘ and ğ‘â€². We demonstrated this for plug-in classification in the main text because it
elucidates the nature of the problem naturally via thresholds, but we should also convince ourselves
that this is not simply a quirk of plug-in classification. To this end, we provide a brief analysis for
empirical risk minimization.

Let Ì‚ï¸€ğ‘“ğ‘ and ğ‘“ *

ğ‘ denote the empirical risk minimizer and risk minimizer within â„±. Define the excess
ğ‘ ). Suppose that we have a uniform convergence

risk to be the difference between ğ‘…( Ì‚ï¸€ğ‘“ğ‘) and ğ‘…ğ‘(ğ‘“ *
guarantee

ğ‘…ğ‘(ğ‘“ ) âˆ’ Ì‚ï¸€ğ‘…ğ‘(ğ‘“ ) â‰¤ ğ‘‚

(ï¸
ğ‘›âˆ’ 1

2

)ï¸

for all ğ‘“ in â„±. Then, a standard chaining argument reveals that the excess risk decay rate satisfies

â„°ğ‘( Ì‚ï¸€ğ‘“ğ‘) = ğ‘…ğ‘( Ì‚ï¸€ğ‘“ğ‘) âˆ’ ğ‘…(ğ‘“ *
ğ‘ )

= ğ‘…ğ‘( Ì‚ï¸€ğ‘“ğ‘) âˆ’ Ì‚ï¸€ğ‘…ğ‘( Ì‚ï¸€ğ‘“ğ‘) + Ì‚ï¸€ğ‘…ğ‘( Ì‚ï¸€ğ‘“ğ‘) âˆ’ Ì‚ï¸€ğ‘…ğ‘(ğ‘“ *

ğ‘ ) + Ì‚ï¸€ğ‘…ğ‘(ğ‘“ *

ğ‘ ) âˆ’ ğ‘…(ğ‘“ *
ğ‘ )

â‰¤ ğ‘‚

= ğ‘‚

(ï¸

2

ğ‘›âˆ’ 1
(ï¸
ğ‘›âˆ’ 1

2

)ï¸

)ï¸

+ 0 + ğ‘‚

(ï¸

ğ‘›âˆ’ 1

2

)ï¸

,

where in the inequality we used our uniform convergence guarantee twice and the fact that Ì‚ï¸€ğ‘“ğ‘ is the
empirical ğ‘-risk minimizer. This mirrors the case of ğ‘-weighted plug-in estimation in that the excess
ğ‘-risk still converges to 0 at the standard rate.

22

On the other hand, we obtain a constant term when performing a similar analysis for â„°ğ‘â€²( Ì‚ï¸€ğ‘“ğ‘).

Specifically, we get

â„°ğ‘â€²( Ì‚ï¸€ğ‘“ğ‘) = ğ‘…ğ‘â€²( Ì‚ï¸€ğ‘“ğ‘) âˆ’ ğ‘…ğ‘â€²(ğ‘“ *
ğ‘ )
= ğ‘…ğ‘( Ì‚ï¸€ğ‘“ğ‘) âˆ’ ğ‘…ğ‘(ğ‘“ *
ğ‘ ) + ğ‘…ğ‘â€²( Ì‚ï¸€ğ‘“ğ‘) âˆ’ ğ‘…ğ‘( Ì‚ï¸€ğ‘“ğ‘) + ğ‘…ğ‘(ğ‘“ *
ğ‘›âˆ’ 1

+ ğ‘…ğ‘â€²( Ì‚ï¸€ğ‘“ğ‘) âˆ’ ğ‘…ğ‘( Ì‚ï¸€ğ‘“ğ‘) + ğ‘…ğ‘(ğ‘“ *

â‰¤ ğ‘‚

)ï¸

(ï¸

2

ğ‘ ) âˆ’ ğ‘…ğ‘â€²(ğ‘“ *

ğ‘ ) âˆ’ ğ‘…ğ‘â€²(ğ‘“ *
ğ‘â€²)
ğ‘â€²).

Now, using the prior convergence result for the empirical risk minimizers, we obtain

â„°ğ‘â€²( Ì‚ï¸€ğ‘“ğ‘) â‰¤ ğ‘…ğ‘â€²( Ì‚ï¸€ğ‘“ğ‘) âˆ’ ğ‘…ğ‘( Ì‚ï¸€ğ‘“ğ‘) + ğ‘…ğ‘(ğ‘“ *

ğ‘ ) âˆ’ ğ‘…ğ‘â€²(ğ‘“ *
ğ‘ ) âˆ’ ğ‘…ğ‘â€²(ğ‘“ *
ğ‘ ) + ğ‘…ğ‘(ğ‘“ *
ğ‘›âˆ’ 1

+ğ‘‚

)ï¸

(ï¸

.

2

ğ‘â€²) + ğ‘‚

ğ‘â€²) + ğ‘‚

)ï¸

(ï¸

2

ğ‘›âˆ’ 1
(ï¸
ğ‘›âˆ’ 1

2

)ï¸

â‰¤ ğ‘…ğ‘â€²(ğ‘“ *

= ğ‘…ğ‘â€²(ğ‘“ *

ğ‘ ) âˆ’ ğ‘…ğ‘(ğ‘“ *
ğ‘ ) âˆ’ ğ‘…ğ‘â€²(ğ‘“ *
ğ‘â€²)

âŸ

 â
ğ´

ğ‘â€² minimizes ğ‘…ğ‘â€² and ğ‘“ *

Since ğ‘“ *
ğ‘ minimizes ğ‘…ğ‘, we see that ğ´ â‰¥ 0. Thus, even though there is not a
clear threshold interpretation, we do see that there is irreducible error that arises in the empirical
risk minimization setting as well.

D Robust Weighting Proofs

In this section, we prove our results for robust weighting. We start with our generalization and
excess risk bounds.

Proof of Theorem 1. Define the risk ğ‘…ğ‘–,1 as

Ì‚ï¸€ğ‘…ğ‘–,1(ğ‘“ ) = Ì‚ï¸€ğ‘ğ‘– Ì‚ï¸€ğ‘…ğ‘–(ğ‘“ ) =

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘—=1

â„“mar(ğ‘“, ğ‘§ğ‘—)1 {ğ‘¦ğ‘— = ğ‘–} .

Let ğ‘…ğ‘–,1(ğ‘“ ) denote E Ì‚ï¸€ğ‘…ğ‘–,1(ğ‘“ ). Note that we have

ğ‘…ğ‘–,1(ğ‘“ ) =

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘—=1

E [â„“mar(ğ‘“, ğ‘§ğ‘—)1 {ğ‘¦ğ‘— = ğ‘–}] =

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘—=1

ğ‘ğ‘–E [â„“mar(ğ‘“, ğ‘§ğ‘—)|ğ‘¦ğ‘— = ğ‘–] = ğ‘ğ‘–ğ‘…ğ‘–(ğ‘“ ).

By definition, we have

ğ‘…ğ‘„(ğ‘“ ) = sup
ğ‘âˆˆğ‘„

ğ‘˜
âˆ‘ï¸

ğ‘–=1

ğ‘ğ‘–ğ‘ğ‘–ğ‘…ğ‘–(ğ‘“ ) = sup
ğ‘âˆˆğ‘„

ğ‘˜
âˆ‘ï¸

ğ‘–=1

ğ‘ğ‘–ğ‘…ğ‘–,1(ğ‘“ ),

and so for our purposes, it suffices to analyze Ì‚ï¸€ğ‘…ğ‘–,1. Define the class

â„±ğ‘–,1 = {â„“mar(ğ‘“, Â·)1 {ğ‘¦ğ‘— = ğ‘–} : ğ‘“ âˆˆ â„±} .

By Lemma 9, we have with probability at least 1 âˆ’ ğ›¿/ğ‘˜ that

ğ‘…ğ‘–,1(ğ‘“ ) â‰¤ Ì‚ï¸€ğ‘…ğ‘–,1(ğ‘“ ) + 2Rğ‘›(â„“mar âˆ˜ â„±ğ‘–,1) +

âˆšï¸ƒ

log ğ‘˜
ğ›¿
2ğ‘›

23

 
for each ğ‘“ in â„±. So, it suffices to analyze the Rademacher complexity term. Let ğœğ‘— be iid Rademacher
random variables. We condition on the value of ğ‘¦1, . . . , ğ‘¦ğ‘›. Let â„‹ğ‘Œ be the sigma-field ğœ(ğ‘¦1, . . . , ğ‘¦ğ‘›).
Suppose without loss of generality that under the conditioning, we have ğ‘¦1 = Â· Â· Â· = ğ‘¦ğ‘ğ‘– = ğ‘– and
ğ‘¦ğ‘— Ì¸= ğ‘– for all ğ‘— > ğ‘ğ‘–. Then, we have

Rğ‘›(â„±ğ‘–,1) =

=

EE

EE

1
ğ‘›

1
ğ‘›

[ï¸ƒ

sup
ğ‘“ âˆˆâ„±

â¡
â£sup
ğ‘“ âˆˆâ„±

ğ‘›
âˆ‘ï¸

ğ‘–=1
ğ‘ğ‘–âˆ‘ï¸

ğ‘—=1

âƒ’
âƒ’
ğœğ‘—â„“mar(ğ‘“, ğ‘§ğ‘—)1{ğ‘¦ğ‘— = ğ‘–}
âƒ’
âƒ’

â„‹ğ‘Œ

]ï¸ƒ

ğœğ‘—â„“mar(ğ‘“, ğ‘§ğ‘—)

â¤

â„‹ğ‘Œ

â¦

âƒ’
âƒ’
âƒ’
âƒ’

= E

[ï¸‚ ğ‘ğ‘–
ğ‘›

Ì‚ï¸€Rğ‘ğ‘–(â„“mar âˆ˜ â„±)

]ï¸‚

.

By the proof of Lemma 11, we have

Ì‚ï¸€Rğ‘ğ‘–(â„“mar âˆ˜ â„±) â‰¤ 2ğ‘˜ Ì‚ï¸€Rğ‘ğ‘–(Î 1(â„±)).

Putting everything together completes the proof of the generalization bound; now we turn to the
excess (â„±, ğ‘)-risk bound.

Recall that Ì‚ï¸€ğ‘“ğ‘„ is the empirical ğ‘„-risk minimizer and ğ‘“ *
Lemma 10, we have with probability at least 1 âˆ’ ğ›¿/ğ‘˜ that

ğ‘„ is the population ğ‘„-risk minimizer. By

ğ‘…ğ‘–,1( Ì‚ï¸€ğ‘“ğ‘„) â‰¤ Ì‚ï¸€ğ‘…ğ‘–,1( Ì‚ï¸€ğ‘“ğ‘„) + 4Rğ‘›(â„“mar âˆ˜ â„±ğ‘–,1) +

âˆšï¸ƒ

log ğ‘˜
ğ›¿
2ğ‘›

.

Summing, we have

ğ‘…ğ‘( Ì‚ï¸€ğ‘“ğ‘„) =

ğ‘˜
âˆ‘ï¸

ğ‘–=1

ğ‘ğ‘–ğ‘ğ‘–ğ‘…ğ‘–( Ì‚ï¸€ğ‘“ğ‘„) =

â‰¤

ğ‘˜
âˆ‘ï¸

ğ‘–=1

ğ‘˜
âˆ‘ï¸

ğ‘–=1

ğ‘ğ‘–(ğ‘…ğ‘–,1 Ì‚ï¸€ğ‘“ğ‘„)

â›
â Ì‚ï¸€ğ‘…ğ‘–,1( Ì‚ï¸€ğ‘“ğ‘„) + 4Rğ‘›(â„“mar âˆ˜ â„±ğ‘–,1) +

ğ‘ğ‘–

âˆšï¸ƒ

â

â 

log ğ‘˜
ğ›¿
2ğ‘›

â‰¤ Ì‚ï¸€ğ‘…ğ‘( Ì‚ï¸€ğ‘“ğ‘„) +

ğ‘˜
âˆ‘ï¸

ğ‘–=1

ğ‘ğ‘–ğ‘ğ‘–

â›

âœ
â

4
ğ‘ğ‘–

Rğ‘›(â„“mar âˆ˜ â„±ğ‘–,1) +

â¯
â¸
â¸
â·

log ğ‘˜
ğ›¿
2ğ‘2
ğ‘– ğ‘›

â

âŸ
â  .

Using the proof of Lemma 11 as before, we then obtain

ğ‘…ğ‘( Ì‚ï¸€ğ‘“ğ‘„) â‰¤ Ì‚ï¸€ğ‘…ğ‘( Ì‚ï¸€ğ‘“ğ‘„) +

ğ‘˜
âˆ‘ï¸

ğ‘–=1

ğ‘ğ‘–ğ‘ğ‘–

â›
â8ğ‘˜E
âœ

[ï¸‚ ğ‘ğ‘–
ğ‘ğ‘–ğ‘›

]ï¸‚
Ì‚ï¸€Rğ‘ğ‘–(Î 1(â„±))

+

â¯
â¸
â¸
â·

log ğ‘˜
ğ›¿
2ğ‘2
ğ‘– ğ‘›

â

âŸ
â  .

Thus, by taking supremums, we observe that

ğ‘…ğ‘„( Ì‚ï¸€ğ‘“ğ‘„) â‰¤ Ì‚ï¸€ğ‘…ğ‘„( Ì‚ï¸€ğ‘“ğ‘„) + sup
ğ‘âˆˆğ‘„

ğ‘˜
âˆ‘ï¸

ğ‘–=1

ğ‘ğ‘–ğ‘ğ‘–

â›
â8ğ‘˜E
âœ

[ï¸‚ ğ‘ğ‘–
ğ‘ğ‘–ğ‘›

]ï¸‚
Ì‚ï¸€Rğ‘ğ‘–(Î 1(â„±))

+

â¯
â¸
â¸
â·

log ğ‘˜
ğ›¿
2ğ‘2
ğ‘– ğ‘›

â

âŸ
â  .

(5)

Similarly, by Lemma 10, we have

âˆ’ğ‘…ğ‘–,1(ğ‘“ *

ğ‘„) â‰¤ âˆ’ Ì‚ï¸€ğ‘…ğ‘–,1(ğ‘“ *

ğ‘„) + 4Rğ‘›(â„“mar âˆ˜ â„±ğ‘–,1) +

âˆšï¸ƒ

log ğ‘˜
ğ›¿
2ğ‘›

.

24

Summing as before and using the proof of Lemma 11, we have

âˆ’ğ‘…ğ‘(ğ‘“ *

ğ‘„) â‰¤ âˆ’ Ì‚ï¸€ğ‘…ğ‘(ğ‘“ *

ğ‘„) +

ğ‘˜
âˆ‘ï¸

ğ‘–=1

ğ‘ğ‘–ğ‘ğ‘–

â›
â8ğ‘˜E
âœ

[ï¸‚ ğ‘ğ‘–
ğ‘ğ‘–ğ‘›

]ï¸‚
Ì‚ï¸€Rğ‘ğ‘–(Î 1(â„±))

+

â¯
â¸
â¸
â·

log ğ‘˜
ğ›¿
2ğ‘2
ğ‘– ğ‘›

â

âŸ
â  .

Taking the infimum and using Lemma 8, we have

âˆ’ğ‘…ğ‘„(ğ‘“ *

ğ‘„) â‰¤ âˆ’ Ì‚ï¸€ğ‘…ğ‘„(ğ‘“ *

ğ‘„) + sup
ğ‘âˆˆğ‘„

ğ‘˜
âˆ‘ï¸

ğ‘–=1

ğ‘ğ‘–ğ‘ğ‘–

â›
â8ğ‘˜E
âœ

[ï¸‚ ğ‘ğ‘–
ğ‘ğ‘–ğ‘›

]ï¸‚
Ì‚ï¸€Rğ‘ğ‘–(Î 1(â„±))

+

â¯
â¸
â¸
â·

log ğ‘˜
ğ›¿
2ğ‘2
ğ‘– ğ‘›

â

âŸ
â  .

(6)

Summing equation (5) and equation (6) and noting that Ì‚ï¸€ğ‘“ğ‘„ minimizes the empirical robust risk, we
have

â„°ğ‘„(â„±) = ğ‘…ğ‘„( Ì‚ï¸€ğ‘“ğ‘„) âˆ’ ğ‘…ğ‘„(ğ‘“ *

ğ‘„) â‰¤ 2 sup
ğ‘âˆˆğ‘„

ğ‘˜
âˆ‘ï¸

ğ‘–=1

ğ‘ğ‘–ğ‘ğ‘–

â›
â8ğ‘˜E
âœ

[ï¸‚ ğ‘ğ‘–
ğ‘ğ‘–ğ‘›

]ï¸‚
Ì‚ï¸€Rğ‘ğ‘–(Î 1(â„±))

+

â¯
â¸
â¸
â·

log ğ‘˜
ğ›¿
2ğ‘2
ğ‘– ğ‘›

â

âŸ
â  ,

and this completes the proof.

Proof of Corollary 2. The only thing we need to do here is calculate the Rademacher complexity
term of Theorem 1. Using our assumption and Jensenâ€™s inequality, we have

E

[ï¸‚ ğ‘ğ‘–
ğ‘›

Ì‚ï¸€Rğ‘ğ‘–(Î 1(â„±))

]ï¸‚

â‰¤

ğ¶(â„±)
ğ‘›

E [ï¸âˆšï¸€ğ‘ğ‘–

]ï¸

â‰¤

ğ¶(â„±)
ğ‘›

E[ğ‘ğ‘–]1/2 = ğ¶(â„±)

âˆšï¸‚ ğ‘ğ‘–
ğ‘›

.

This completes the proof of the corollary.

Next, we prove our duality results. We start with LCVaR.

Proof of Proposition 4. The Lagrangian of LCVaR is

ğ¿(ğ‘, ğœ†) = E[ğ‘ğ‘Œ ğ‘…ğ‘Œ (ğ‘“ )] + ğœ†(1 âˆ’ E[ğ‘ğ‘Œ ]) = E [ğ‘ğ‘Œ (ğ‘…ğ‘Œ (ğ‘“ ) âˆ’ ğœ†)] + ğœ†.

Our goal is to use the minimax theorem, which we state as Theorem 4, to switch the infimum

over ğœ† and the supremum over ğ‘. First, we do not need the minimax theorem to obtain

inf
ğœ†âˆˆR

ğ¿(ğ‘, ğœ†) â‰¤ inf
ğœ†âˆˆR

sup
ğ‘:ğ‘(Â·)âˆˆ[0,ğ›¼âˆ’1]

E [ğ‘ğ‘Œ (ğ‘…ğ‘Œ (ğ‘“ ) âˆ’ ğœ†)] + ğœ† = inf
ğœ†âˆˆR

{ï¸E [ï¸

ğ›¼âˆ’1E(ğ‘…ğ‘Œ (ğ‘“ ) âˆ’ ğœ†)+ + ğœ†

]ï¸}ï¸

,

(7)

since the inequality follows the trivial direction of the minimax theorem and we can solve the inner
maximization problem by setting

ğ‘ğ‘– =

{ï¸ƒ0

ğ‘…ğ‘–(ğ‘“ ) âˆ’ ğœ† < 0
ğ›¼âˆ’1 ğ‘…ğ‘–(ğ‘“ ) âˆ’ ğœ† â‰¥ 0.

Our present goal is to verify the conditions of the minimax theorem. First, we note that
ğœ† â†¦â†’ ğ¿(ğ‘, ğœ†) is linear and therefore convex for any ğ‘, and similarly, ğ‘ â†¦â†’ ğ¿(ğ‘, ğœ†) is linear and
therefore concave for any ğ‘. Additionally, the domain of ğ‘, in this case [0, ğ›¼âˆ’1]ğ‘˜, is compact and
convex by definition; so we only need to prove that it suffices to consider ğœ† on a compact, convex
domain.

25

Denote the right hand side of equation (7) by inf ğœ†âˆˆR ğ·(ğœ†). Let ğ¹ğ‘“ (ğœ†) denote the cumulative

distribution function of ğ‘…ğ‘Œ at ğœ†. By Lemma 6, the derivative of ğ·(ğœ†) is given by

ğ·â€²(ğœ†) = 1 + ğ›¼âˆ’1(ğ¹ğ‘“ (ğœ†) âˆ’ 1),

when ğ¹ğ‘“ is continuous at ğœ†. If it is not, then the same result holds for the left and right limits.
Thus by considering signs of the derivative, we see that ğœ† achieves minimizes ğ·(ğœ†) for a value in
the interval [ğœ†*(ğ‘“ ), ğœ†*(ğ‘“ )] where

ğœ†*(ğ‘“ ) = inf{ğ‘¡ : ğ¹ğ‘“ (ğ‘¡) â‰¥ 1 âˆ’ ğ›¼} and ğœ†*(ğ‘“ ) = sup{ğ‘¡ : ğ¹ğ‘“ (ğ‘¡) â‰¤ 1 âˆ’ ğ›¼}.

Note further that when â„± is compact in, say, sup norm, then we also have finite ğœ†* = inf ğ‘“ âˆˆâ„± ğ‘¡*(ğœ†)
and ğœ†* = inf ğ‘“ âˆˆâ„± ğ‘¡*(ğœ†). In any case, we see that it suffices to define ğœ† on a compact set Î› = [ğœ†*, ğœ†*],
and so we may assume without loss of generality that the domain of ğœ† is compact.

This verifies the conditions of the minimax theorem, and so we have

LCVaRğ›¼(ğ‘“ ) = inf
ğœ†âˆˆR

sup
ğ‘:ğ‘(Â·)âˆˆ[0,ğ›¼âˆ’1]

E [ğ‘ğ‘Œ (ğ‘…ğ‘Œ (ğ‘“ ) âˆ’ ğœ†)] + ğœ† = inf
ğœ†âˆˆR

{ï¸E [ï¸

ğ›¼âˆ’1E(ğ‘…ğ‘Œ (ğ‘“ ) âˆ’ ğœ†)+ + ğœ†

]ï¸}ï¸

,

which completes the proof.

Next, we consider LHCVaR.

Proof of Proposition 5. The proof is similar to that of Proposition 4. The Lagrangian of LHCVaR
is

ğ¿(ğ‘, ğœ†) = E[ğ‘ğ‘Œ ğ‘…ğ‘Œ (ğ‘“ )] + ğœ† (1 âˆ’ E[ğ‘ğ‘Œ ]) = E [ğ‘ğ‘Œ (ğ‘…ğ‘Œ (ğ‘“ ) âˆ’ ğœ†)] + ğœ†.

Next, by the trivial direction of the minimax theorem, we have

LHCVaRğ›¼(ğ‘“ ) â‰¤ inf
ğœ†âˆˆR

sup
ğ‘:ğ‘ğ‘Œ âˆˆ[0,ğ›¼âˆ’1
ğ‘Œ ]

ğ¿(ğ‘, ğœ†) = inf
ğœ†âˆˆR

E [ï¸

ğ›¼âˆ’1

ğ‘Œ (ğ‘…ğ‘Œ (ğ‘“ ) âˆ’ ğœ†)+

]ï¸

+ ğœ†.

(8)

So, now our goal is to verify the conditions of the minimax theorem. As with LCVaR, the Lagrangian
ğ¿ is linear and therefore concave in ğ‘; is linear and therefore convex in ğœ†; and is defined over a
compact domain of values of ğ‘ given by [0, ğ›¼âˆ’1]ğ‘˜. Thus, the only difficulty, as with LCVaR, is
showing that it suffices to define ğœ† over a compact interval. To this end, define the right hand side
of equation (8) to be inf ğœ†âˆˆR ğ»(ğœ†). It suffices to show that ğ·(ğœ†) achieves its infimum on a closed
interval, in which case we can restrict the domain of ğœ† to this compact, convex set.

To prove such an interval exists, we wish to show that there exist constants ğœ†* and ğœ†* such that
ğ» is decreasing for all ğœ† < ğœ†* and increasing for all ğœ† > ğœ†*. By Lemma 7, we see that the derivative
of ğ» is

ğ» â€²(ğœ†) = 1 âˆ’ E[ğ›¼âˆ’1

ğ‘Œ 1 {ğ‘…ğ‘Œ (ğ‘“ ) > ğœ†}] = 1 âˆ’

ğ›¼âˆ’1

ğ‘– ğ‘ğ‘–1 {ğ‘…ğ‘–(ğ‘“ ) > ğœ†}

ğ‘˜
âˆ‘ï¸

when ğ» â€² exists; otherwise the result holds for the left and right derivatives. Let ğœ†*(ğ‘“ ) =
minğ‘–=1,...,ğ‘˜ ğ‘…ğ‘–(ğ‘“ ). Then, for ğœ† â‰¤ ğœ†*(ğ‘“ ), we have

ğ‘–=1

ğ» â€²(ğœ†) = 1 âˆ’

ğ‘˜
âˆ‘ï¸

ğ‘–=1

ğ›¼âˆ’1

ğ‘– ğ‘ğ‘– â‰¤ 0.

26

Next, pick ğœ†*(ğ‘“ ) = maxğ‘–=1,...,ğ‘˜ ğ‘…ğ‘–(ğ‘“ ) + 1. Then, for all ğœ† â‰¥ ğœ†*(ğ‘“ ), we have

ğ» â€²(ğœ†) = 1 â‰¥ 0.

If â„“ is continuous, then each ğ‘…ğ‘–(ğ‘“ ) is continuous in ğ‘“ . Moreover, when â„± is compact on ğ’³ in the
supremum norm, then we can define finite constants ğœ†* = inf ğ‘“ âˆˆâ„± ğœ†*(ğ‘“ ) and ğœ†*(ğ‘“ ) = supğ‘“ âˆˆâ„± ğœ†*(ğ‘“ ).
Thus, we may restrict the domain of ğœ† to [ğœ†*, ğœ†*] without loss of generality. The minimax

theorem now implies that equation (8) holds with equality, which completes the proof.

E Results for the Conditional Sampling Model

Now, we present the alternative result for the conditional sampling model. Recall that ğ‘›ğ‘– is the
number of samples of class ğ‘–, which is assumed to be fixed.

Theorem 3. Let â„“ be the multiclass margin loss. With probability at least 1 âˆ’ ğ›¿, for every ğ‘“ in â„±
we have

ğ‘…ğ‘„ â‰¤ max
ğ‘âˆˆğ‘„

â§
â¨

â©

Ì‚ï¸€ğ‘…ğ‘(ğ‘“ ) +

ğ‘˜
âˆ‘ï¸

ğ‘–=1

ğ‘ğ‘– Ì‚ï¸€ğ‘ğ‘–

â›
â2ğ‘˜Rğ‘›ğ‘–(â„±) +

âˆšï¸ƒ

log ğ‘˜
ğ›¿
2ğ‘›ğ‘–

â

â 

â«
â¬

â­

.

Proof. The proof is similar to that of Cao et al. (2019). We apply Lemma 9 and Lemma 11 to
obtain

ğ‘…ğ‘–(ğ‘“ ) â‰¤ Ì‚ï¸€ğ‘…ğ‘–(ğ‘“ ) + 2ğ‘˜Rğ‘›ğ‘–(â„±) +

âˆšï¸ƒ

log ğ‘˜
ğ›¿
2ğ‘›ğ‘–

.

Multiplying by ğ‘ğ‘– Ì‚ï¸€ğ‘ğ‘–, summing over ğ‘–, and taking a supremum over ğ‘„ completes the proof.

F Gradient Descent-Ascent

In general, the robust classification problem is a saddle-point problem. For our purposes, define a
saddle-point problem to be an optimization problem of the form

inf
ğ‘âˆˆğ’œ

sup
ğ‘âˆˆâ„¬

ğ‘“ (ğ‘, ğ‘).

(9)

One of the seminal results in game theory is that the minimax problem is equivalent to the

maximin problem.
Theorem 4 (minimax theorem). Let ğ’œ and â„¬ be compact convex sets. Let ğ‘“ : ğ’œ Ã— â„¬ â†’ R be a
function such that ğ‘ â†¦â†’ ğ‘“ (ğ‘, ğ‘) is convex and ğ‘ â†¦â†’ ğ‘“ (ğ‘, ğ‘) is concave. Then, we have

inf
ğ‘âˆˆğ’œ

sup
ğ‘âˆˆâ„¬

ğ‘“ (ğ‘, ğ‘) = sup
ğ‘âˆˆâ„¬

inf
ğ‘âˆˆğ’œ

ğ‘“ (ğ‘, ğ‘).

Lemma 5 (Theorem 3.1 of Hazan 2016). Let ğ‘“1, . . . , ğ‘“ğ‘‡ : ğ’œ â†’ R be a sequence of ğ¿-Lipschitz
convex functions. If the step size for online gradient descent is chosen to be

then we have

ğœ‚ğ‘¡ =

ğ·
âˆš

ğ¿

,

ğ‘¡

ğ‘‡
âˆ‘ï¸

ğ‘¡=1

ğ‘“ğ‘¡(ğ‘ğ‘¡) âˆ’ min
ğ‘*âˆˆğ’œ

ğ‘‡
âˆ‘ï¸

ğ‘¡=1

ğ‘“ğ‘¡(ğ‘*) â‰¤

âˆš

ğ·ğ¿

ğ‘‡ .

3
2

27

Algorithm 1: Online Gradient Descent

Input
for ğ‘¡ = 1, . . . , ğ‘‡ do

: Convex domain ğ’œ, ğ‘1 âˆˆ ğ’œ, step sizes ğœ‚ğ‘¡, number of rounds ğ‘‡

Play ğ‘ğ‘¡ and observe cost ğ‘“ğ‘¡(ğ‘ğ‘¡).
Update and project

ğ‘¥ğ‘¡+1 = ğ‘ğ‘¡ âˆ’ ğœ‚ğ‘¡âˆ‡ğ‘“ğ‘¡(ğ‘ğ‘¡)
ğ‘ğ‘¡+1 = Î ğ’œ(ğ‘¥ğ‘¡+1).

end
Output : The average iterate Â¯ğ‘ğ‘‡ = 1
ğ‘‡

âˆ‘ï¸€ğ‘‡

ğ‘¡=1 ğ‘ğ‘¡.

Now we return to the saddle-point problem. We give the gradient descent-ascent algorithm in

Algorithm 2 and the convergence result in Proposition 7.

Algorithm 2: Gradient Descent-Ascent

Input
for ğ‘¡ = 1, . . . , ğ‘‡ do

: Convex-concave function ğ‘“ , step sizes ğœ‚ğ‘,ğ‘¡ and ğœ‚ğ‘,ğ‘¡, number of rounds ğ‘‡

Play (ğ‘ğ‘¡, ğ‘ğ‘¡) and observe cost ğ‘“ (ğ‘ğ‘¡, ğ‘ğ‘¡).
Update and project

Update and project

ğ‘¥ğ‘¡+1 = ğ‘ğ‘¡ âˆ’ ğœ‚ğ‘¡âˆ‡ğ‘ğ‘“ (ğ‘ğ‘¡, ğ‘ğ‘¡)
ğ‘ğ‘¡+1 = Î ğ’œ(ğ‘¥ğ‘¡+1).

ğ‘¦ğ‘¡+1 = ğ‘ğ‘¡ + ğœ‚ğ‘¡âˆ‡ğ‘ğ‘“ (ğ‘ğ‘¡, ğ‘ğ‘¡)
ğ‘ğ‘¡+1 = Î ğ’œ(ğ‘¦ğ‘¡+1).

end
Output : The average iterates Â¯ğ‘ğ‘‡ = 1
ğ‘‡

âˆ‘ï¸€ğ‘‡

ğ‘¡=1 ğ‘ğ‘¡ and Â¯ğ‘ğ‘‡ = 1

ğ‘‡

âˆ‘ï¸€ğ‘‡

ğ‘¡=1 ğ‘ğ‘¡.

Proposition 7. Let ğ’œ and â„¬ be convex, compact sets. Suppose that ğ’œ has diameter ğ·ğ‘ and â„¬
has diameter ğ·ğ‘. Let ğ‘“ : ğ’œ Ã— â„¬ â†’ R be convex-concave, ğ¿ğ‘-Lipschitz in its first argument, and
ğ¿ğ‘-Lipschitz in its second argument. Let (ğ‘*, ğ‘*) denote the solution to the saddle-point problem of
equation (9). If (Â¯ğ‘ğ‘‡ , Â¯ğ‘ğ‘‡ ) is the output of Algorithm 2, then we have

ğ‘“ (ğ‘*, ğ‘*) âˆ’

3(ğ¿ğ‘ğ·ğ‘ + ğ¿ğ‘ğ·ğ‘)
âˆš
2

ğ‘‡

â‰¤ ğ‘“ (Â¯ğ‘ğ‘‡ , Â¯ğ‘ğ‘‡ ) â‰¤ ğ‘“ (ğ‘*, ğ‘*) +

3(ğ¿ğ‘ğ·ğ‘ + ğ¿ğ‘ğ·ğ‘)
âˆš

.

2

ğ‘‡

First, we want to use a lemma from online convex optimization. For this, we also state the
standard online gradient descent algorithm. Here, we use Î ğ’œ to denote projection onto the set ğ’œ.

Proof. The proof is fairly straightforward from pre-existing results on online gradient descent; so we

28

state it here. We start first with the upper bound. Define the â€œregretâ€ to be

ğ‘…ğ‘‡ =

ğ‘‡
âˆ‘ï¸

ğ‘¡=1

[ğ‘“ (ğ‘ğ‘¡, ğ‘ğ‘¡) âˆ’ ğ‘“ (ğ‘*, ğ‘*)]

where (ğ‘*, ğ‘*) is a solution to the saddle-point problem. Then, we have the decomposition

ğ‘…ğ‘‡ =

ğ‘‡
âˆ‘ï¸

ğ‘¡=1

[ğ‘“ (ğ‘ğ‘¡, ğ‘ğ‘¡) âˆ’ ğ‘“ (ğ‘*, ğ‘ğ‘¡)] +

ğ‘‡
âˆ‘ï¸

ğ‘¡=1

[ğ‘“ (ğ‘*, ğ‘ğ‘¡) âˆ’ ğ‘“ (ğ‘*, ğ‘*)] â‰¤

âˆš

ğ¿ğ‘ğ·ğ‘

3
2

ğ‘‡ + 0,

(10)

where the inequality follows from applying Lemma 5 and noting that the second summand is
nonpositive by the definition of ğ‘*. Similarly, we have

âˆ’ğ‘…ğ‘‡ =

ğ‘‡
âˆ‘ï¸

ğ‘¡=1

[ğ‘“ (ğ‘*, ğ‘*) âˆ’ ğ‘“ (ğ‘ğ‘¡, ğ‘ğ‘¡)] +

ğ‘‡
âˆ‘ï¸

ğ‘¡=1

[ğ‘“ (ğ‘ğ‘¡, ğ‘ğ‘¡) âˆ’ ğ‘“ (ğ‘ğ‘¡, ğ‘ğ‘¡)] â‰¤ 0 +

âˆš

ğ‘‡ .

ğ¿ğ‘ğ·ğ‘

3
2

(11)

So, now we consider the averaged iterates. We have

ğ‘“ (Â¯ğ‘ğ‘‡ , Â¯ğ‘ğ‘‡ ) â‰¤ max
ğ‘âˆˆâ„¬

ğ‘“ (Â¯ğ‘ğ‘‡ , ğ‘)

â‰¤

1
ğ‘‡

max
ğ‘âˆˆâ„¬

ğ‘‡
âˆ‘ï¸

ğ‘¡=1

ğ‘“ (ğ‘ğ‘¡, ğ‘)

= ğ‘“ (ğ‘*, ğ‘*) +

1
ğ‘‡

max
ğ‘âˆˆâ„¬

ğ‘‡
âˆ‘ï¸

[ğ‘“ (ğ‘ğ‘¡, ğ‘) âˆ’ ğ‘“ (ğ‘ğ‘¡, ğ‘ğ‘¡)] +

ğ‘¡=1

1
ğ‘‡

ğ‘‡
âˆ‘ï¸

[ğ‘“ (ğ‘ğ‘¡, ğ‘ğ‘¡) âˆ’ ğ‘“ (ğ‘*, ğ‘*)]

ğ‘¡=1

â‰¤ ğ‘“ (ğ‘*, ğ‘*) +

3ğ¿ğ‘ğ·ğ‘
âˆš
ğ‘‡
2

+

3ğ¿ğ‘ğ·ğ‘
âˆš
ğ‘‡
2

.

Note that the second inequality is due to convexity, and the third is due to Lemma 5 and equation (10).

Similarly, we have

ğ‘“ ( Â¯ğ‘ğ‘‡ , Â¯ğ‘ğ‘‡ ) â‰¥ min
ğ‘âˆˆğ’œ

ğ‘“ (ğ‘, Â¯ğ‘ğ‘‡ )

â‰¥

1
ğ‘‡

min
ğ‘âˆˆğ’œ

ğ‘‡
âˆ‘ï¸

ğ‘¡=1

ğ‘“ (ğ‘, ğ‘ğ‘¡)

= ğ‘“ (ğ‘*, ğ‘*) +

1
ğ‘‡

min
ğ‘âˆˆğ’œ

ğ‘‡
âˆ‘ï¸

ğ‘¡=1

[ğ‘“ (ğ‘, ğ‘ğ‘¡) âˆ’ ğ‘“ (ğ‘ğ‘¡, ğ‘ğ‘¡)] +

1
ğ‘‡

ğ‘‡
âˆ‘ï¸

ğ‘¡=1

[ğ‘“ (ğ‘ğ‘¡, ğ‘ğ‘¡) âˆ’ ğ‘“ (ğ‘*, ğ‘*)]

â‰¥ ğ‘“ (ğ‘*, ğ‘*) âˆ’

3ğ¿ğ‘ğ·ğ‘
âˆš
ğ‘‡
2

âˆ’

3ğ¿ğ‘ğ·ğ‘
âˆš
ğ‘‡
2

.

The second inequality follows from concavity, and the final inequality is a result of Lemma 5 applied
to the sequence ğ‘ğ‘¡ and equation (11). This completes the proof.

G Additional Lemmas

Lemma 6. Define ğ·(ğœ†) = ğ›¼âˆ’1E(ğ‘…ğ‘Œ (ğ‘“ ) âˆ’ ğœ†)+ + ğœ†, and let ğ¹ğ‘“ denote the cumulative distribution
function of ğ‘…ğ‘Œ (ğ‘“ ). Then, we have

ğ·â€²(ğœ†) = 1 + ğ›¼âˆ’1(ğ¹ğ‘“ (ğœ†) âˆ’ 1).

29

Proof. We compute the derivative directly. We obtain

ğ·â€²(ğœ†) = 1 + ğ›¼âˆ’1 lim
ğœ€â†’0

{E [(ğ‘…ğ‘Œ (ğ‘“ ) âˆ’ ğœ† âˆ’ ğœ€)+ âˆ’ (ğ‘…ğ‘Œ (ğ‘“ ) âˆ’ ğœ†)+]}

1
ğœ€
1
ğœ€

{E [âˆ’ğœ€1 {ğ‘…ğ‘Œ (ğ‘“ ) âˆ’ ğœ† > 0}]}

= 1 + ğ›¼âˆ’1 lim
ğœ€â†’0
= 1 âˆ’ ğ›¼âˆ’1E1 {ğ‘…ğ‘Œ (ğ‘“ ) > ğœ†}
= 1 + ğ›¼âˆ’1(ğ¹ğ‘“ (ğœ†) âˆ’ 1).

This completes the proof.
Lemma 7. Define ğ»(ğœ†) = E [ï¸€ğ›¼âˆ’1(ğ‘…ğ‘Œ âˆ’ ğœ†)+
ğ» â€²(ğœ†) = 1 âˆ’ E [ï¸

]ï¸€ + ğœ†. Then, the derivative of ğ»(ğœ†) is
]ï¸

ğ›¼âˆ’1

ğ‘Œ 1 {ğ‘…ğ‘Œ (ğ‘“ ) > ğœ†}

.

Proof. We again compute directly, obtaining

ğ‘Œ (ğ‘…ğ‘Œ (ğ‘“ ) âˆ’ ğœ† âˆ’ ğœ€)+ âˆ’ ğ›¼âˆ’1
ğ›¼âˆ’1

ğ‘Œ (ğ‘…ğ‘Œ (ğ‘“ ) âˆ’ ğœ†)+

]ï¸

ğ» â€²(ğœ†) = 1 + lim
ğœ€â†’0

E [ï¸

1
ğœ€
1
ğœ€
ğ›¼âˆ’1
ğ‘Œ 1 {ğ‘…ğ‘Œ (ğ‘“ ) > ğœ†}

ğ›¼âˆ’1

E [ï¸

ğ‘Œ (âˆ’ğœ€)1 {ğ‘…ğ‘Œ (ğ‘“ ) > ğœ†}
]ï¸

,

]ï¸

= 1 + lim
ğœ€â†’0
= 1 âˆ’ E [ï¸

as desired.

Lemma 8. We have the inequality

inf
ğ‘âˆˆğ‘„

{ğ´(ğ‘) + ğµ(ğ‘)} â‰¤ inf
ğ‘âˆˆğ‘„

ğ´(ğ‘) + sup
ğ‘âˆˆğ‘„

ğµ(ğ‘).

Proof. We have the inequality ğ´(ğ‘) + ğµ(ğ‘) â‰¤ ğ´(ğ‘) + supğ‘â€²âˆˆğ‘„ ğµ(ğ‘â€²), and taking infimums completes
the proof.

H Standard Lemmas

Lemma 9 (Theorem 3.1 of Mohri et al. 2012). Let ğº be a family of functions mapping from R to
[0, 1]. Then for ğ›¿ > 0 and all ğ‘” in ğº, with probability at least 1 âˆ’ ğ›¿, we have

Eğ‘”(ğ‘) â‰¤

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

ğ‘”(ğ‘ğ‘–) + 2Rğ‘›(ğº) +

âˆšï¸ƒ

log 1
ğ›¿
2ğ‘›

.

For our excess (â„±, ğ‘)-risk bounds, we also use a slight variant, the proof of which is nearly

identical to that of Lemma 9.

Lemma 10. Let ğº be a family of functions mapping from R to [0, 1]. Then for ğ›¿ > 0 and all ğ‘” in
ğº, with probability at least 1 âˆ’ ğ›¿, we have

âƒ’
âƒ’
âƒ’
âƒ’
âƒ’

Eğ‘”(ğ‘) âˆ’

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

âƒ’
âƒ’
âƒ’
ğ‘”(ğ‘ğ‘–)
âƒ’
âƒ’

â‰¤ 4Rğ‘›(ğº) +

âˆšï¸ƒ

log 1
ğ›¿
2ğ‘›

.

30

The following learning bound handles the multi-class margin loss more effectively in the number

of classes (Kuznetsov et al., 2015).

Lemma 11. Let â„± be a set of ğ‘“ : ğ’³ Ã— ğ’´ â†’ R. Recall that

Then, under the margin loss, we have the bound

Î 1(â„±) = {ğ‘¥ â†¦â†’ ğ‘“ğ‘¦(ğ‘¥) : ğ‘¦ âˆˆ ğ’´, ğ‘“ âˆˆ â„±} .

ğ‘…(ğ‘“ ) â‰¤ Ì‚ï¸€ğ‘…(ğ‘“ ) + 4ğ‘˜Rğ‘›(Î 1(â„±)) +

âˆšï¸ƒ

log 1
ğ›¿
2ğ‘›

for all ğ‘“ in â„± with probability at least 1 âˆ’ ğ›¿.

I Additional Experiment Details

For all methods and datasets, we optimized a logistic regression model with gradient descent over
the entire data.

For all datasets, we chose a learning rate of 0.01 that was linearly annealed to 0.0001 over 2000

epochs.

I.1 Optimizing LCVaR/LHCVaR formulation

Note that in the formulation for LHCVaR described in Eq. (3), despite its convexity, the optimization
is over a non-smooth loss. Thus, ğœ† can be explicitly calculated given the classes of each risk. Let
ğ‘…(ğ‘–) be the ğ‘–th largest class risk.

ğœ† = min

â›

â

â§
â¨

â©

ğ‘…(ğ‘–) : ğ‘– âˆˆ [ğ‘˜],

ğ‘–
âˆ‘ï¸

ğ‘—=1

Ì‚ï¸€ğ‘ğ‘–ğ›¼âˆ’1

ğ‘– â‰¤ 1

â«
â¬

â­

â

âˆª {0}

â 

An algorithm for computing this can be akin to water filling in order from largest to smallest
class risk. When optimizing by some form of gradient descent the parameters of the classifier, this
analytic form of the LHCVaR formulation can be quickly computed and avoid gradient computations
on ğœ† itself. Empirically, we used this formulation to speed up our experiments and leads to faster
convergence than performing gradient descent on ğœ† in addition to the model parameters. This
algorithm is also applicable when optimizing LCVaR as well.

31

