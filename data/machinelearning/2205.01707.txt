MemSE: Fast MSE Prediction for Noisy
Memristor-Based DNN Accelerators

Jonathan Kern1,2, S´ebastien Henwood1, Gonc¸alo Mordido1,3, Elsa Dupraz2,
Abdeldjalil A¨ıssa-El-Bey2, Yvon Savaria1, and Franc¸ois Leduc-Primeau1
1Department of Electrical Engineering, Polytechnique Montreal, Montreal, QC, Canada
2IMT Atlantique, CNRS UMR 6285, Lab-STICC, Brest, France
3Mila - Quebec AI Institute, Montreal, QC, Canada

2
2
0
2

y
a
M
3

]

G
L
.
s
c
[

1
v
7
0
7
1
0
.
5
0
2
2
:
v
i
X
r
a

Abstract—Memristors enable the computation of matrix-vector
multiplications (MVM) in memory and, therefore, show great
potential in highly increasing the energy efﬁciency of deep neural
network (DNN) inference accelerators. However, computations in
memristors suffer from hardware non-idealities and are subject
to different sources of noise that may negatively impact system
performance. In this work, we theoretically analyze the mean
squared error of DNNs that use memristor crossbars to compute
MVM. We take into account both the quantization noise, due
to the necessity of reducing the DNN model size, and the
programming noise, stemming from the variability during the
programming of the memristance value. Simulations on pre-
trained DNN models showcase the accuracy of the analytical
prediction. Furthermore the proposed method is almost two order
of magnitude faster than Monte-Carlo simulation, thus making
it possible to optimize the implementation parameters to achieve
minimal error for a given power constraint.

I. INTRODUCTION

Energy consumption represents one of the most important
design objectives for deep neural network (DNN) accelera-
tors, in particular, because it enables low-latency on-device
processing. The main source of energy consumption in most
accelerators is due to data movement [1], speciﬁcally retrieving
data from memory and delivering it
to processing units.
in-memory computing techniques
To bypass this problem,
show great promise in terms of energy efﬁciency by di-
rectly processing the data in memory. Particularly, memristors
are an emerging technology well-suited for neural networks,
which allow performing computations, such as dot products,
in memory [2]. Despite the energy beneﬁts, programming
the values of the conductance in memristor crossbars is an
inexact process subject to noise [3]. For instance, existing
hardware implementations report precisions from 2 bits [4] to
7.5 bits [5] per memristor. While additional techniques such
as bit-slicing [6] may be leveraged to increase precision, this
comes at the cost of increased area and energy usage.

DNNs have been shown to be robust to noise affecting
the weights, although the amount of noise must be designed
carefully to satisfy accuracy constraints [7]–[9]. Over the past
few years, implementing neural networks using memristors
has attracted a lot of attention [5], [10]. However, recent
works focus mostly on the hardware architecture design and

This work was supported by an IVADO grant (PRF-2019-4784991664) and

by the Samuel-de-Champlain program.

Fig. 1. Memristor crossbar architecture for matrix-vector multiplication.

experimental results, neglecting theoretical analyses. One ex-
ception is [11], which presented a theoretical framework for
DNN inference on memristors based on tracking second-order
statistical moments. However, they used a crossbar model
based on passive summing circuits, rather than active ones
as in this paper, and did not consider quantization in the
conductance values. Furthermore, the accuracy of the method
was only veriﬁed on very small DNNs.

In this work, we analytically study neural network in-
ference on memristor crossbars. To this end, we provide
theoretical computations, which take into account practical
implementation non-idealities, such as programming noise and
quantization errors. Using these computations, we predict the
mean squared error (MSE) at the output of the ﬁnal layer
of a neural network, depending on the network’s parameters
and scaling factors. Theoretical formulas are also provided
to compute the power usage of the memristors crossbars
depending on the scaling factors. Finally, we combine these
analyses to formulate an optimization problem to minimize
the MSE for a desired power usage. Lastly, simulations are
performed to verify the accuracy of the theoretical analysis.

II. MODELS

A. Memristor crossbar model

Figure 1 illustrates the architecture of the considered mem-
ristor crossbar. In accordance with Ohm’s Law and Kirchoff’s
Law, the conductance at each node is multiplied with the
input voltage of the row and these products are then summed

G1,1X1rG2,1GL,1X2XLZ1rZ2rZ3rZMG1,2G1,3G1,MG2,2GL,2G2,3GL,3G2,MGL,M............. . .. . .. . . 
 
 
 
 
 
along the column. Finally, a transimpedance ampliﬁer (TIA)
converts the current into a voltage at end of each column. In
the ideal case, the output of the j-th column is thus given by
zj = r (cid:80)L
i=1 gi,jxi, where xi is the voltage at the input of
row i, gi,j is the conductance of the memristor at row i and
column j, and r is the feedback resistance of the TIA.

However, several practical

issues may cause the actual
computation to differ from the aforementioned ideal case.
Speciﬁcally, values may be affected by fabrication variations
and noise during programming [12]–[14] as well as quanti-
zation errors. With these practical constraints in mind, we
consider that the memristors have a conductance ranging from
0 to Gmax, and divide this range into N possible values. Gmax
is chosen depending on the desired trade-off between accuracy
and power consumption and only needs to be inferior to the
maximum physical conductance value. We denote the resulting
resolution as ∆ = Gmax
N . The programmed conductance values
can then be represented as random variables Gi,j, which may
be decomposed as

(1)

i,j ,

i,j + (cid:15)v

Gi,j = gi,j + δq
where gi,j is the desired value, δq
i,j is the quantization error,
and (cid:15)v
i,j is the noise due to variability in conductance program-
v the variance of (cid:15)v. In practice, there
ming. We denote by σ2
can be different σv for each possible memristor value, but
here, to simplify the notations and computations, we consider
that σv is constant for all N possible conductance values. The
analysis proposed in this paper remains valid in the case where
σv is allowed to depend on the conductance value.

Since memristors can only store positive values, each weight
wi,j is decomposed as wi,j = w+
i,j and w−
i,j
store the positive and negative value of wi,j, respectively. Then
w+ and w− are converted to the conductance g+
i,j. The
matrix-vector multiplication can then be realized as

i,j, where w+

i,j and g−

i,j − w−

+

= (cid:80)L

For a given linear layer, the conductance values gi,j are
computed following (3) and uniformly quantized over the
conductance range [0, Gmax]. Then, the memristors products
−
i,jxi and ˜Zj
˜Zj
i=1 rG+
i,jxi are com-
puted. The difference ˜Zj = ˜Zj
− ˜Zj
, as well as its rescaling,
˜Zj
Zj =
c , is performed outside of the memristors crossbars.
The non-linear activation function f is applied: f (Zj). Finally,
an average pooling is applied as Ai,j = Avg(f (Z))i,j =
1
l=j f (Zk,l) where s is the kernel size.
s2

= (cid:80)L
−

i=1 rG−

(cid:80)i+s
k=i

(cid:80)j+s

+

III. THEORETICAL ANALYSIS

A. MSE prediction

We now derive a theoretical analysis of the performance of a
memristor-based implementation of neural network inference.
As a proxy of performance, our goal is to predict the MSE
between the noisy neural network outputs (computed using
memristors) and the full precision (noiseless) outputs. We use
the following notations throughout our analysis: Var[Gi,j] =
σ2, E[Xi] = xi, Var[Xi] = γ2

i , Cov[Xi, Xj] = γi,j.

The computation at the linear layer followed by the rescal-

ing can be written as

Zj =

r
c

L
(cid:88)

i=1

Gi,jXi ,

(4)

and we can formulate the ﬁrst and second moments of Zj as

µj = E[Zj] = r

L
(cid:88)

(wi,j + δq

i,j)xi ,

i=1

σ2x2
i
c2 + γ2

i (wi,j + δq

i,j)2 +

(5)

i σ2
γ2
c2
(cid:33)

ρ2
j = Var[Zj] = r2

L
(cid:88)

+

(cid:32) L
(cid:88)

i=1
L
(cid:88)

(wi,j + δq

i,j)(wi(cid:48)j + δq

i(cid:48),j)γi,i(cid:48)

,

(6)

Zj =

L
(cid:88)

i=1

rG+

i,jXi −

L
(cid:88)

i=1

rG−

i,jXi ,

(2)

i=1

i(cid:48)=1,i(cid:48)(cid:54)=i

where Zj, G+

i,j, G−

i,j, and Xi are random variables.

B. Computation model

For our theoretical analysis, we consider a neural network
composed of convolutional, average pooling, and linear layers
as well as differentiable activation functions. For simplicity, we
consider that all convolutional layers are converted to linear
layers. Moreover, batch normalization is not considered but
could be easily incorporated into our analysis.

Because of the range of conductance possible, the matrix

weights wi,j is scaled by a factor c = Gmax
Wmax

, such that

ρj,j(cid:48) = Cov[Zj, Zj(cid:48)] = r2

L
(cid:88)

L
(cid:88)

(wi,j+δq

i,j)(wi(cid:48)j(cid:48)+δq

i(cid:48),j(cid:48))γi,i(cid:48) .

i=1

i(cid:48)=1

(7)
Then, an approximation of the moments after the non-linear
activation function f is possible via Taylor expansions [11]:

f (cid:48)(cid:48) (µj) ρ2
j ,

E[f (Zj)] ≈ f (µj) +

1
2
g(cid:48)(cid:48) (µj) ρ2
Var[f (Zj)] ≈
Cov[f (Zj), f (Zj(cid:48))] ≈ f (cid:48) (µj) f (cid:48) (µj(cid:48)) ρj,j(cid:48) ,

j − f (µj) f (cid:48)(cid:48) (µj) ρ2
j ,

1
2

(8)

(9)

(10)

gi,j = cwi,j.

(3)

where g = f 2. From these moments, the MSE of f (Zj) is

We then divide the result of the memristor computations by
this same factor c. Denoting by ¯gi,j = cwi,j + δq(cwi,j) the
quantized version of g, where δq(cwi,j) is the (determinis-
tic) quantization error, it should be noted that δq(cwi,j) =
cδq(wi,j). Therefore

¯gi,j
c = wi,j + δq(wi,j).

MSE[f (Zj)] = Var[f (Zj)] + (E[f (Zj)] − f (zj))2 .

(11)

The MSE can also be expressed as a function of c as

MSE[f (Zj)] =

F1,j
c4 +

F2,j
c2 + F3,j

(12)

The expressions of F1,j, F2,j, and F3,j can be computed by
substituting (5) and (6) in (8) and (9). Note that if c → ∞, then
MSE[f (Zj)] → F3,j. Hence, F3,j gives us a lower bound on
the MSE. Since this bound does not depend on σ, it is possible
to ﬁnd values of c for any σ that minimize the MSE to F3,j.

For average pooling (Section II-B), the moments are

E[Ai,j] =

1
s2

i+s
(cid:88)

j+s
(cid:88)

k=i

l=j

µk,l ,

Var[Ai,j] =

1
s4

i+s
(cid:88)

j+s
(cid:88)

i+s
(cid:88)

j+s
(cid:88)

k=i

l=j

m=i

n=j

γk,l,m,n ,

(13)

(14)

Cov[Ai,j, Ai(cid:48),j(cid:48)] =

B. Power consumption

1
s4

i+s
(cid:88)

j+s
(cid:88)

i(cid:48)+s
(cid:88)

j(cid:48)+s
(cid:88)

k=i

l=j

m=i(cid:48)

n=j(cid:48)

γk,l,m,n .

(15)

We now derive an estimation of the power consumption of
the memristor computations. The power consumption of each
memristor can be written as P (mem)

i,j = |Gi,j| X 2
(cid:12)
(cid:12)wi,j + δq
(cid:12) (γ2
i,j

i , with
i + x2

i ) .

(16)

E[P (mem)
i,j

] = E[Gi,jX 2

i ] = c (cid:12)

Moreover, the power consumption of each transimpedance
ampliﬁer (TIA) is

+

P (TIA)
j

=

−

P (TIA)
j

=

((cid:80)L

i=1 G+
r

i,jXi)2

((cid:80)L

i=1 G−
r

i,jXi)2

˜Z +2
j
r2

=

˜Z −2
j
r2 ,

=

(17)

(18)

] = c2 ρ+2

i + µ+2
r2

i

, E[P (TIA)

j

−

] = c2 ρ−2

i + µ−2
r2

i

.
(19)

and

with

+

E[P (TIA)
j

Hence, the power consumption of each layer is

E[Ptot] =

L
(cid:88)

(cid:32) L
(cid:88)

j=1

i=1

E[P (mem)
i,j

] + E[P (TIA)

j

+

] + E[P (TIA)

j

−

(cid:33)
]

.

(20)

As a function of c, the power of each layer’s column is
E[Ptotj] = c2H1,j + cH2,j + H3,j ,

(21)

where the expressions of H1,j, H2,j, and H3,j can be com-
puted by developing the terms of equation (20) from their
deﬁnitions and equations (5) and (6).

IV. OPTIMIZATION
The parameter Gmax may be chosen with different gran-
ularity to balance design complexity and energy efﬁciency.
For instance, one may apply the same Gmax to all memristor
crossbars, associate a speciﬁc Gmax to each layer of the neural
network, or even use a different Gmax per crossbar column.
We denote Gmax as the set of Gmax variables that can be
modiﬁed to optimize our computations. Depending if we have
only one Gmax for the whole network or one Gmax for each
layer, the size of Gmax is 1 or P , respectively.

Fig. 2. MSE at the output of the ﬁnal layers of the smaller and larger network
averaged over input examples, in terms of the standard deviation σ of the
conductance values.

To minimize the MSE for a speciﬁc power constraint, the

global optimization problem can be formulated as

max MSE[f (Z P )] ,

min
Gmax

(22)

(p) > 0. This corresponds to
subject to E[Ptot] ≤ V and Gmax
ﬁnding the best set of scaling constants c that minimizes MSE
for a desired total power usage. The problem may be solved
approximately using a heuristic optimizing search.

V. SIMULATIONS

We trained two convolutional neural networks on CIFAR-10
composed of ﬁve pairs of convolutional and average pooling
layers and a ﬁnal linear layer. Each subsequent convolutional
layer in the smaller model has 2, 4, 8, 16, and 16 ﬁlters, as
opposed to the 16, 32, 64, 128, and 128 ﬁlters of the larger
model. We used a kernel size of 3 and a unit stride for all
convolutional layers. Both models used the Softplus activation
function and were trained for 164 epochs using stochastic
gradient descent (SGD) with momentum, weight decay, and
an initial learning rate of 0.1 (decayed by 10 at epochs 81 and
122). The number of quantized values N is set to 128 and r
is set to 1.

A. Accuracy of the theoretical analysis

Figure 2 shows the mean of the MSE on the ﬁnal layer
outputs of the smaller and larger models computed over 100
different inputs. These values are plotted both based on simula-
tions and on the analytical formula presented in Section III. We
observe a close match between the theoretical and simulated
MSE, especially in the high accuracy regime. Moreover, we
see an inverse correlation between MSE and accuracy, which
conﬁrms predicting MSE to be a good proxy for estimating
performance degradation. Moreover, as σ decreases, the MSE
converges to a value dependent on the quantization error.

With a Tesla P100 GPU, the mean runtime for the MSE
computation of the small network on a batch of 64 inputs
with σ = 0.01 using our method is 27 ms. Under the same

103102101101102Mean of MSEMSE - sim - Larger networkMSE - th - Larger networkMSE - sim - Smaller networkMSE - th - Smaller network10203040506070Accuracy (%)Accuracy - Larger networkAccuracy - Smaller networkFig. 3. Mean of the maximum of the MSE of the output of the smaller
network’s ﬁnal layer depending on Gmax and σ values.

conditions, using 200 Monte-Carlo trials takes on average 2.3
seconds to reach a MSE within 2% of the true MSE 98% of
the time. This 85× speedup showcases the usefulness of our
method in practice.

Figure 3 shows the mean of the maximum MSE on the
output of the smaller model depending on the value of
Gmax, for different values of σ. Once again, we observe that
the theoretical computations accurately predict the simulation
results. Moreover, we see the predicted convergence to the
theoretical bound. Such bound is reached faster as σ decreases.
In Figures 2 and 3, we notice that for a high ratio of noise
to Gmax there is a gap between theoretical and simulation
results. This is likely due to the Taylor expansions used for
approximating the moments after the activation function.

B. Numerical optimization

Figure 4 shows the results of optimizing Gmax following
(22) for the smaller network. For each power constraint, a ge-
netic optimizer was run for 100 generations with a population
size of 50 and a sample of 100 inputs for computing the mean
of the theoretical MSE and power consumption of the network.
The proposed approach allows to efﬁciently ﬁnd the value(s)
of Gmax that minimize MSE (maximize accuracy) for a given
power constraint. As expected, adding degrees of freedom by
allowing a different Gmax for each layer leads to improved
performance, although the beneﬁt is marginal in this case.

VI. CONCLUSION

In this work, we studied the implementation of DNN mod-
els using memristors crossbars. Using second-degree Taylor
expansions, we proposed approximate theoretical formulas of
the MSE at the output of the network, as well as theoretical
computations of the power usage of the memristors. We
then considered an optimization problem for maximizing task
performance under a power usage constraint. The theoretical
analysis makes it feasible to solve this optimization problem
numerically since its computing time is faster than using
simulations by almost two orders of magnitude.

Fig. 4. Maximization of the smaller network accuracy using a genetic algo-
rithm minimizing the maximum of the MSE for different power constraints
with σ = 0.01

REFERENCES

[1] A. Pedram, S. Richardson, M. Horowitz, S. Galal, and S. Kvatinsky,
“Dark memory and accelerator-rich system optimization in the dark
silicon era,” IEEE Design Test, 2017.

[2] A. Sebastian, M. Le Gallo, R. Khaddam-Aljameh, and E. Eleftheriou,
“Memory devices and applications for in-memory computing,” Nature
Nanotechnology, 2020.

[3] A. Chen and M.-R. Lin, “Variability of resistive switching memories
and its impact on crossbar array performance,” in Int. Reliability Physics
Symp., 2011.

[4] E. P´erez, C. Zambelli, M. K. Mahadevaiah, P. Olivo, and C. Wenger,
“Toward reliable multi-level operation in RRAM arrays: Improving
post-algorithm stability and assessing endurance/data retention,” IEEE
Journal of the Electron Devices Society, 2019.

[5] M. Hu, C. E. Graves, C. Li, Y. Li, N. Ge, E. Montgomery, N. Davila,
H. Jiang, R. S. Williams, J. J. Yang, Q. Xia, and J. P. Strachan,
“Memristor-based analog computation and neural network classiﬁcation
with a dot product engine,” Advanced Materials, 2018.

[6] S. Diware, A. Gebregiorgis, R. V. Joshi, S. Hamdioui, and R. Bishnoi,
“Unbalanced bit-slicing scheme for accurate memristor-based neural
network architecture,” in IEEE Int. Conf. on Artiﬁcial Intelligence
Circuits and Systems, 2021.

[7] S. Henwood, F. Leduc-Primeau, and Y. Savaria, “Layerwise noise
maximisation to train low-energy deep neural networks,” in IEEE Int.
Conf. on Artiﬁcial Intelligence Circuits and Systems, 2020.

[8] G. B. Hacene, F. Leduc-Primeau, A. B. Soussia, V. Gripon, and
F. Gagnon, “Training modern deep neural networks for memory-fault
robustness,” in IEEE Int. Symp. on Circuits and Systems, 2019.

[9] T. Hirtzlin, M. Bocquet, J.-O. Klein, E. Nowak, E. Vianello, J.-M.
Portal, and D. Querlioz, “Outstanding bit error tolerance of Resistive
RAM-based binarized neural networks,” in IEEE Int. Conf. on Artiﬁcial
Intelligence Circuits and Systems, 2019.

[10] C. Li, D. Belkin, Y. Li, P. Yan, M. Hu, N. Ge, H. Jiang, E. Montgomery,
P. Lin, Z. Wang, W. Song, J. P. Strachan, M. Barnell, Q. Wu, R. S.
Williams, J. J. Yang, and Q. Xia, “Efﬁcient and self-adaptive in-situ
learning in multilayer memristor neural networks,” Nature Communica-
tions, 2018.

[11] E. Dupraz, L. R. Varshney, and F. Leduc-Primeau, “Power-efﬁcient
deep neural networks with noisy memristor implementation,” in IEEE
Information Theory Workshop, 2021.

[12] S. Liu, Y. Wang, M. Fardad, and P. K. Varshney, “A memristor-based
optimization framework for artiﬁcial intelligence applications,” IEEE
Circuits and Systems Magazine, 2018.

[13] A. J. P´erez- ´Avila, G. Gonz´alez-Cordero, E. P´erez, E. P.-B. Que-
sada, M. Kalishettyhalli Mahadevaiah, C. Wenger, J. B. Rold´an, and
F. Jim´enez-Molinos, “Behavioral modeling of multilevel HfO2-based
memristors for neuromorphic circuit simulation,” in Conf. on Design of
Circuits and Integrated Systems, 2020.

[14] V. Milo, C. Zambelli, P. Olivo, E. P´erez, M. K. Mahadevaiah, O. G. Os-
sorio, C. Wenger, and D. Ielmini, “Multilevel HfO2-based RRAM
devices for low-power neuromorphic networks,” APL Materials, 2019.

100101Gmax101102Mean of max of MSESimulation - =0.01Theory - =0.01Simulation - =0.003Theory - =0.003Simulation - =0.001Theory - =0.001105106107108Power20304050Accuracy (%)One Gmax per layerOne Gmax for the whole network