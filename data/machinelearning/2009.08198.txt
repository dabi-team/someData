Multi-Objective Dynamic Programming with 
Limited Precision1 

L. Mandow , J.L. PÃ©rez de la Cruz , and N. Pozas 

Universidad de MÃ¡laga, AndalucÃ­a Tech, Departamento de 
Lenguajes y Ciencias de la ComputaciÃ³n, MÃ¡laga, EspaÃ±a. 
lawrence@lcc.uma.es, perez@lcc.uma.es 

September 15, 2020 

Abstract 
This paper addresses the problem of approximating the set of all solutions 
for  Multi-objective  Markov  Decision  Processes.  We  show  that  in  the  vast 
majority of interesting cases, the number of solutions is exponential or even 
infinite. In order to overcome this difficulty we propose to approximate the 
set of all solutions by means of a limited precision approach based on Whiteâ€™s 
multi-objective  value-iteration  dynamic  programming  algorithm.  We  prove 
that the number of calculated solutions is tractable and show experimentally 
that the solutions obtained are a good approximation of the true Pareto front. 
Keywords  â€”  Reinforcement  learning,  Multi-objective  Markov  decision 

processes, Multi-objective dynamic programming 

Introduction 

1 
Markov  decision  processes  (MDP)  are  a  well-known  conceptual  tool  useful  for 
modelling sequential decision processes and have been widely used in real-world 
applications such as adaptive production control (see e. g. Kuhnle et al. (2020)), 
equipment  maintenance  (see  Barde  et  al.  (2019),  Liu  et  al.  (2019))  or  robot 
planning (see Veeramani et al. (2020)), to name a few. 

Usual  optimization  procedures  take  into  account  just  a  scalar  value  to  be 
maximized.  However,  in  many  cases  the  objective  function  is  more  accurately 
described by a vector (see e. g. Gen  and Lin (2014), Zhang and Xu (2017)) and 
multi-objective optimization must be applied. 

By merging both concepts (MDP and multi-objective optimization) we are led 
to  consider  Multi-objective  Markov  decision  processes  (MOMDPs),  that  are 
Markov decision processes in which rewards given to the agent consist of two or 

1 Funded by the Spanish Government, Agencia Estatal de InvestigaciÃ³n (AEI), and European Union, 

Fondo Europeo de Desarrollo Regional (FEDER), grant TIN2016-80774-R (AEI/FEDER, UE). 

1 

                                                                   
more  independent  values,  i.  e.,  rewards  are  numerical  vectors.  In  recent  years 
there has been a growing interest in considering theory and applications of multi-
objective  Markov  decision  processes  and  multi-objective  Reinforcement 
Learning, e.g. see Roijers and Whiteson (2017), and Drugan et al. (2017). 

As in any multi-objective problem, the solution to a MOMDP is not a singleton 
but in general a set with many nondominated values that is called the Pareto front. 
Two  approaches  can  be  followed  in  order  to  find  the  solutions  of  a  MOMDP 
(Roijers et al. (2013)). The single-policy approach (Perny and Weng (2010), Wray 
et al. (2015)) computes a single optimal policy according to some scalarization of 
user preferences; of course, the process can be repeated as many times as desired 
in order to find different solutions. On the other hand, the multi-policy approach 
tries to compute simultaneously all the values in the Pareto front (Van Moffaert 
and NowÃ© (2014), Ruiz-Montiel et al. (2017)). 

In any case, the computation of the Pareto front must face a difficulty: since 
the number of values in the front is usually huge (in fact, it can be infinite), the 
computation  can  be infeasible. Therefore, recent multi-objective reinforcement 
learning techniques avoid approximating the full Pareto front, or are tested on 
limited problem instances, like deterministic domains (see Drugan et al. (2017)). 
So it is desirable to have a procedure that, while being feasible, provides a â€œgoodâ€ 
approximation of the front. That is the goal of the proposal presented here. 

The work of Drugan (2019) acknowledges that multi-objective reinforcement 
learning  methods  have  had  â€slow  development  due  to  severe  computational 
problemsâ€.  Incorporating  elements  already  widely  used  in  multi-objective 
evolutionary  computation 
like 
scalarization,  and  dimensionality  reduction  through  principal  component 
analysis,  is  a  promising  avenue  of  research  to  overcome  these  difficulties  (see 
Giuliani et al. (2014)). 

learning  methods, 

reinforcement 

into 

On the other hand, the application of multi-objective evolutionary methods in 
a  multi-policy  setting  (i.e.  to  directly  approximate  the  Pareto  front  of  MOMDP 
solutions)  seems  to  remain  largely  unexplored.  Although  computationally 
efficient in many domains, multi-objective evolutionary techniques generally lack 
guarantees  on  the  optimality  or  precision  of  the  solutions  found.  Zitzler  et  al. 
(2003) analyzed different metrics that can be used to assess the performance of 
multi-objective  algorithms  against  a  reference  Pareto  set.  Regrettably,  these 
benchmark  references  are  generally  not  available  even  for  simple  stochastic 
MOMDPs due to their inherent complexity. 

A  different  way  to  deal  with  the  exponential  nature  of  MOMDPs,  is  to  use 
mixture policies as proposed by Vamplew et al. (2009): given a set of deterministic 
policies, one of them is chosen  probabilistically at  the start  of the process, and 
followed  onwards.  The  initial  set  of  policies  could  be  the  set  of  supported 
solutions,  calculated  by  linear  scalarized  versions  of  reinforcement  learning 
algorithms. The mixture policies form their convex hull, and would be therefore 
nondominated. However, there may be situations where mixture policies are not 
acceptable, e.g. for ethical reasons (Lizotte et  al.  (2010)).  These considerations 

2 

lead us to propose in this paper a new approach that approximates deterministic 
policies without relying on mixtures. 

The structure of this paper is as follows: first we define formally the concepts 
related to MOMDPs and prove that under a very restrictive set of assumtions the 
number of solutions of a MOMDP is tractable (Proposition 1); but we show that if 
any of these assumptions does not hold, then that number becomes intractable or 
even  infinite.  In  the  following  section  we  present  the  basic  valueiteration 
algorithm  for  solving  a  MOMDP  (Algorithm  1)  and  the  modification  that  we 
propose  (Algorithm  WLP,  given  by  equation  1),  and  prove  that  algorithm  WLP 
computes  a  tractable  number  of  approximate  solutions.  Algorithm  WLP is  then 
applied to benchmark problems to check if it can provide a â€œgoodâ€ approximation 
of the true Pareto front. Finally some conclusions are drawn. 

2  Multi-objective Markov Decision Processes 
This section  defines multi-objective Markov decision  processes (MOMDPs) and 
their  solutions.  When  possible,  notation  is  consistent  with  that  of  Sutton  and 
Barto (2018). 

A MOMDP is defined by at least the elements in the tuple (S,A,p,ğ‘Ÿâƒ—,Î³), where: S 
is a finite set of states; A(s) is the finite set of actions available at state s âˆˆ S; p is a 
probability distribution  such that  p(s,a,sâ€™) is the probability of reaching state  sâ€™ 
immediately after taking action a in state s; ğ‘Ÿâƒ— is a function such that ğ‘Ÿâƒ—(s,a,sâ€™) âˆˆ IRq 
is  the  reward  obtained  after  taking  action  a  in  state  s  and  immediately 
transitioning to state sâ€™; and Î³ âˆˆ (0,1] denotes the current value of future rewards. 
The  only  apparent  difference  with  scalar  finite  Markov  decision  processes 
(MDPs) (e.g., see Sutton and Barto (2018)) is the use of a vector reward function. 
A MOMDP can be episodic, if the process always starts at a start state so âˆˆ S, 
terminates  when  reaching  any  of  a  set  of  terminal  states  Î“  âŠ†  S,  and  before 
termination there is always a non-zero probability of eventually reaching some 
terminal state. Otherwise, the process is  continuing. A MOMDP can  be of finite-
horizon, if the process terminates after at most a given finite number of actions n. 
We say that such process is n-step bounded. Otherwise, it is of infinite-horizon. 

MDPs  are  frequently  used  to  model  the  interaction  of  an  agent  with  an 
environment  at  discrete  time  steps.  We  define  the  goal  of  the  agent  as  the 
maximization  of  the  expected  accumulated  (additive)  discounted  reward  over 
time. 

Let us now define the concepts related to solving a MOMDP. A decision rule Î´ 
is  a  function  that  associates  each  state  to  an  available  action.  A  policy  Ï€  = 
(Î´1,Î´2,...Î´i,...) is a sequence of decision rules, such that Î´i(s) determines the action 
to take if the process is in state s at time step i. If there is some decision rule Î´ such 
that for all i, Î´i = Î´, then the policy is stationary. Otherwise, it is non-stationary. We 
denote by Ï€n = (Î´1,...,Î´n), n â‰¥ 1 the finite n-step subsequence of policy Ï€. 

3 

Let  St  and  Rt  be  two  random  variables  denoting  the  state,  and  the  vector 
reward received at time step t respectively. The value ğ‘£âƒ—Ï€ is defined as the expected 
accumulated discounted reward obtained starting at state s and applying policy Ï€ 
(analogously to the scalar case, see Sutton and Barto (2018)), 

For any given policy Ï€, we denote the values of its n-step subpolicies Ï€n as ğ‘£âƒ—ğœ‹

ğ‘›. 

Vector  values  define  only  a  partial  order  dominance  relation.  Given  two 
vectors ğ‘¢âƒ—âƒ—= (u1,...uq), ğ‘£âƒ— = (v1,...,vq), we define the following relations: (a)Dominates 
or equals, 
 and ğ‘¢âƒ—âƒ— â‰  ğ‘£âƒ—; (c) 
Indifference, 

; (b) Dominates, 

 and 

. 

Given  a  set  of  vectors  X  âŠ‚  IRq,  we  define  the  subset  of  nondominated,  or 

Pareto-optimal, vectors as ğ‘ğ·(ğ‘‹)

. 

We denote by Vn(s) and V(s) the set of nondominated values of all possible n-
step  policies,  and  of  all  possible  policies  at  state  s  respectively.  For  an  n-step 
bounded process, Vn(s) = V(s) for all s âˆˆ S. The solution to a MOMDP is given by 
the V(s) sets of all states. 
2.1  Combinatorial explosion 

In  this  section  we  analyze  some  computational  difficulties  related  to  solving 
MOMDPs. 

Proposition 1 Let us consider an episodic q-objective MDP with initial state s0 
satisfying the following assumptions: 

1.  The length of every episode is at most d. 

2.  Immediate rewards ğ‘Ÿâƒ—(s,a,sâ€™) are integer in every component and every 

component is bounded by rmin, rmax. 

3.  The MPD is deterministic. 

4.  Discount rate is Î³ = 1. 

Then |V(s0)| â‰¤ (R Ã— d + 1)qâˆ’1, where R = rmax âˆ’ rmin. 

Proof: given assumptions 2, 3 and 4 the value of a policy is a vector with integer 
components. Given assumptions 1 and 2 these components lie in the interval [d Ã— 
rmin,d Ã— rmax]. The interval can contain at most R Ã— d + 1 integers. So there can be 
at most (RÃ—d+1)q different vectors for the values; however, not all of them can be 
nondominated. Consider the q âˆ’1 first components 1,...,q âˆ’1 of the vector. There 
are  at  most  (RÃ—d+1)qâˆ’1  different  possibilities  for  them.  For  each  one,  just  one 
vector  is  nondominated:  the  one  having  the  greatest  value  for  the  q-th 

4 

 
component.  Hence  there  are  at  most  (RÃ—d+1)qâˆ’1  nondominated  policy  values, 
q.e.d. 

Notice that nothing has been assumed about the number of  policies (except 
that it is finite, by assumption 1). So in general there are many more policies (an 
exponential number of them) than values. 

Let  us  consider  the  graph  of  figure  1  (adapted  from  Hansen  (1980))  with 
rewards ğ‘Ÿâƒ— (si,a1,si+1) = (0,1) and ğ‘Ÿâƒ— (si,a2,si+1) = (1,0). It represents an instance (for 
d = 3) of a family of deterministic MDPs. Assume Î³ = 1. All episodes are of length 
d and all the assumptions of Proposition 1 hold. And V(s0) = ND(V(s0)) = {(d,0),(d 
âˆ’ 1,1),...,(0,d)} and |V(s0)| = d + 1. Notice that the number of policies is 2d. 

Figure 1: Hansenâ€™s graph 

We  will  show  now  that  if  any  of  the  assumptions  is  not  satisfied,  then  the 

number of different nondominated values |V(s0)| can be also exponential in d. 

Nonbounded or noninteger immediate rewards Let us consider a case where 
rewards are not bounded and assumption 2 does not hold. Consider for example 
for all i, ğ‘Ÿâƒ—(si,a1,si+1) = (0,2i) and ğ‘Ÿâƒ—(si,a2,si+1) =  (2i ,0). Then, for depth d, for all subset 
I âŠ† [1,d] and all vector  ğ‘£âƒ— =   (âˆ‘ 2ğ‘–, âˆ‘ 2ğ‘– 
)  there exists a policy with value ğ‘£âƒ—  
ğ‘–âˆ‰ğ¼
for s0, namely the policy that for all state si selects a1 if i âˆ‰ I and a2 if i âˆˆ I. Notice 
that all these values ğ‘£âƒ— are nondominated. So the number of nondominated values 
grows exponentially with d. 

ğ‘–âˆˆğ¼

Let us consider now a case with noninteger rewards, for example for all i, 
ğ‘Ÿâƒ—(si,a1,si+1) = (0,1/2i) and ğ‘Ÿâƒ—(si,a2,si+1) =  (1/2i ,0). Let b be a bit (0 or 1) and ğ‘Ì… its 
negation (1 or 0). Consider numbers in binary notation. Then, for depth d and for 
all ğ‘£âƒ— = (0.b1b2 ...bd,0.ğ‘Ì…1ğ‘Ì…2 ... ğ‘Ì…d) there exists a policy with value ğ‘£âƒ— for s0, namely the 
policy that  for all state  si selects  a1 if bi = 0 and  a2 if  bi = 1. Notice that  all these 
values  ğ‘£âƒ—  are  nondominated.  So  the  number  of  nondominated  values  grows 
exponentially with d. 
Discount rate  Assume rewards are for all i ğ‘Ÿâƒ—(si,a1,si+1) = (0,1) and ğ‘Ÿâƒ—(si,a2,si+1) = 
(1,0), so integer and bounded. But consider a discount rate Î³ < 1, for example Î³ = 
1/2. Then, for depth d and for all ğ‘£âƒ— = (0.b1b2 ...bd,0.ğ‘Ì…1ğ‘Ì…2 ... ğ‘Ì…d) there exists again a 
policy  (the  same  defined  for  noninteger  rewards)  with  value  ğ‘£âƒ—  for  s0.  So  the 
number of nondominated values grows exponentially with d. 

5 

 
 
Nondeterministic actions Assume rewards are ğ‘Ÿâƒ— (si,a1,si+1) = (0,1) and ğ‘Ÿâƒ—(si,a2,si+1) 
= (1,0), so integer and bounded. Assume also  Î³ = 1. It is well known that every 
discounted  MDP  can  be  transformed  into  an  equivalent  nondiscounted 
probabilistic MDP (see for instance Puterman (1994), section 5.3). Let us add a 
final state g to the previous model, and for all i let p(si,a1,si+1) = p(si,a2,si+1) = 1/2, 
p(si,a1,g) =  p(si,a2,g) = 1/2. Then  again for depth  d, for all ğ‘£âƒ— =  (0.b1b2 ...bd,0.ğ‘Ì…1ğ‘Ì…2 
... ğ‘Ì…d) there exists a policy (again the same defined for noninteger rewards) with 
value ğ‘£âƒ— for s0, and the number of nondominated values grows exponentially with 
d. 

Figure 2: Unbounded episodes 

Cyclical  graphs  Let  us  now  withhold  assumption  1  (i.  e.,  let  us  assume  that 
episodes are finite but can be of unbounded length). To avoid the divergence of ğ‘£âƒ—, 
we must consider that the model is probabilistic, i. e., also abandon assumption 3. 
Then the situation is even worse: there are MDPs with a finite number of states 
and an infinite number of nondominated values. This is due to the existence of 
nonstationary  nondominated  policies,  i.  e.,  policies  that  take  a  different  action 
every time a state is reached. For example, let us consider the graph in figure 2. 
Let ğ‘Ÿâƒ—(s0,a1,s0) = (0,1), ğ‘Ÿâƒ—(s0,a2,s0) = (1,0), ğ‘Ÿâƒ—(s0,a1,s1) = ğ‘Ÿâƒ—(s0,a2,s1) = (0,0). Let p(s0,a1,s0) 
= p(s0,a1,s1) = p(s0,a2,s0) = p(s0,a2,s1) = 1/2. Then, for all ğ‘£âƒ— =(0.b1b2 ...biâ€¦, 0.ğ‘Ì…1ğ‘Ì…2 ...ğ‘Ì…i 
â€¦) there exists a nonstationary policy with value ğ‘£âƒ— for s0. That policy selects for 
the i-th visit to s0 a1 if bi = 0 and a2 if bi = 1. So the number of nondominated values 
is infinite. 

If  we  consider  continuing  MDPs  -even  deterministic-  with  Î³  <  1  the  same 
situation appears: there are nonstationary nondominated policies and an infinite 
number of values can be obtained. For example, let us consider the graph of figure 

6 

 
3, ğ‘Ÿâƒ—(s0,a1,s0) = (0,1), ğ‘Ÿâƒ—(s0,a2,s0) = (1,0), Î³ = 1/2. Again for all ğ‘£âƒ— = (0.b1b2 ...biâ€¦, 0.ğ‘Ì…1ğ‘Ì…2 
...ğ‘Ì…i â€¦)  the same policy above defined yields the value ğ‘£âƒ—. 

Figure 3: Continuing task 

3  Algorithms 
This  section  describes  two  previous  multi-objective  dynamic  programming 
algorithms, and proposes a tractable modification. 

3.1  Recursive backwards algorithm 

An  early  extension  of  dynamic  programming  to  the  multi-objective  case  was 
described by Daellenbach and De Kluyver (1980). The authors showed that the 
standard backwards recursive dynamic programming procedure can be applied 
straightforwardly  to  problems  with  directed  acyclic  networks  and  additive 
positive  vector  costs.  The  idea  easily  extends  to  other  decision  problems, 
including  stochastic  networks.  We  denote  this  first  basic  multi-objective 
extension of dynamic programming as algorithm B. 

3.2  Vector value iteration algorithm 

The work of White (1982) considered the general case, applicable to stochastic 
cyclic networks, and extended the scalar value-iteration method to multiobjective 
problems. 

Algorithm 1 displays a reformulation of the algorithm described by White. The 
original  formulation  considers  a  single  reward  associated  to  a  transition  (s,a), 
while we consider that  rewards  ğ‘Ÿâƒ—(s,a,si) may depend on  the reached state  si as 
well. Otherwise, both procedures are equivalent. 

The procedure is as follows. Initially, the set of nondominated values V0(s) for 
each state s is {0âƒ—âƒ—}. Line 9 calculates a vector ğ‘ âƒ—2 = (s1,s2,...si,...sm) with all reachable 
states for the state-action pair (s,a). Let s2(l) = sl be the l-th element in that vector. 
Then, lines 11 to 13 calculate a list lv with all the sets Viâˆ’1(sl) for each state in ğ‘ âƒ—2, 
i.e. lv = [Viâˆ’1(s1),...Viâˆ’1(sl),...Viâˆ’1(sm)]. Function len(s2) returns the length of vector 

7 

 
ğ‘ âƒ—2. Next, the cartesian product P of all sets in lv is calculated. Each element in P is 
of the form ğ‘âƒ— = (ğ‘£âƒ—1,... ğ‘£âƒ—l,... ğ‘£âƒ—m) | âˆ€l ğ‘£âƒ—l âˆˆ Viâˆ’1(sl). 

Algorithm 1 Algorithm W (adapted from White (1982)). 

1: Input: size of the state space (N); a discount rate (Î³); number of iterations to 

run (n). 

2: Output: Vn, a vector such that Vn(s) = Vn(s) 
3: V0 â† vector(size: N, defaultValue: {0âƒ—âƒ—} ) 
4: for i âˆˆ n do 

   5: 

   6: 

   7: 

   8: 

   9: 

10: 

11: 

12: 

13: 

14: 

Vi â† vector(size: N) 
for s âˆˆ S do 

T â† vector(size: |A(s)|, defaultValue: âˆ…) 
for a âˆˆ A(s) do 

ğ‘ âƒ—2 â† reachableStates(s,a) 
lv â† emptyList() 
for j âˆˆ len(ğ‘ âƒ—2) do 

lv â† lv.append(Viâˆ’1(ğ‘ âƒ—2(j))) 

end for 
P â† cartesianProduct(lv) 
for ğ‘âƒ— âˆˆ P do 

15:                    ğ‘‡(ğ‘) â† ğ‘‡(ğ‘) âˆª {âˆ‘

ğ‘˜âˆˆğ‘™ğ‘’ğ‘›(ğ‘ âƒ—2)

(ğ‘(ğ‘ , ğ‘, ğ‘ 2(ğ‘˜))  Ã—   [ğ‘Ÿâƒ—(ğ‘ , ğ‘, ğ‘ 2(ğ‘˜)) + ğ›¾ğ‘âƒ—(ğ‘˜)]

}  

end for 

end for 
Vi(s) â† ND( âˆªa T(a)) 

16: 

17: 

18: 

19: 

end for 
delete(Viâˆ’1) 

20: 
21: end for 
22: return(Vn) 

Line 15 calculates a temporal set of updated vector value estimates  T(a) for 
action a. Each new estimate is calculated according to the dynamic programming 
update rule. The calculation takes one vector estimate for each reachable state, 
and adds the immediate reward and discounted value for each reachable state, 
weighted by the probability of transition to each such state. All possible estimates 
are stored in set T(a). Finally, in line 18, the estimates calculated for each action 
are joined and the non-dominated set is used to update the new value for Vi(s). 
Maximum memory requirements are determined by the size of Vi, Viâˆ’1 and T. 

8 

 
 
 
 
As  n â†’  âˆ, the values returned by algorithm W converge to  V(s) (see White 

(1982), theorem 1). 

Some new difficulties arise in algorithm W, when compared to standard scalar 

value iteration White (1982): 

â€¢  After n iterations, algorithm W provides Vn(s), i.e. the sets of nondominated 
values for the set of n-step policies (see White (1982), theorem 2). However, 
for infinite-horizon problems these are only approximations of the V(s) sets. 
Let  us  consider  two  infinite-horizon  policies  Ï€1,  and  Ï€2  with  values 
ğ‘£âƒ—Ï€1(s), ğ‘£âƒ—Ï€2(s) respectively at a given state, and all their n-step sub-policies 
ğ‘› (ğ‘ ). It is possible to have ğ‘£âƒ—Ï€1(s) â‰º ğ‘£âƒ—Ï€2(s), and 
and 
ğ‘› (ğ‘ )  for all n. In other words, given two infinite-
at the same time ğ‘£âƒ—ğœ‹1
horizon policies that dominate each other, their n-step approximations may 
not dominate each other for any finite value of n. 

ğ‘› (ğ‘ ), ğ‘£âƒ—ğœ‹2
ğ‘› (ğ‘ ) ~ ğ‘£âƒ—ğœ‹2

 with values ğ‘£âƒ—ğœ‹1

â€¢  Nonstationary  policies  may  be  nondominated.  This  is  also  an  important 
departure from the scalar case, where under reasonable assumptions there 
is always a  stationary optimal policy. Whiteâ€™s  algorithm converges to the 
values of nondominated policies, either stationary or nonstationary. 

â€¢  Finally, if policies with probability mixtures over actions are allowed, these 
policies may also be nondominated. Therefore, if allowed, these must also 
be covered in the calculations of the W algorithm. 

Despite  of  its  theoretical  importance,  we  are  not  aware  of  practical 
applications  of  Whiteâ€™s  algorithm  to  general  MOMDPs.  This  is  not  surprising, 
given the result presented in proposition 1. 

3.3  Vector value iteration with limited precision 

In order to overcome the computational difficulties imposed by proposition 1, we 
propose  a  simple  modification  of  algorithm 1.  This  involves  the  use  of  limited 
precision  in  the  values  of  vector  components  calculated  by  the  algorithm.  We 
consider a maximum precision factor. Then, vector components are rounded to 
the allowable precision in step 18. More precisely, the vectors in the T(a) sets are 
rounded  before  being  joined.  Therefore,  all  vectors  in  Vi  will  be  of  limited 
precision. 

We  denote  the  W  algorithm  limited  to  precision  Îµ  by  WLP(Îµ).  The  new 

algorithm has a new parameter Îµ, and line 18 is replaced by, 

ğ‘‰ğ‘–(ğ‘ ) â† ğ‘ğ·(âˆªğ‘  ğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘(ğ‘‡(ğ‘), ğœ–)) 

                                                    (1) 

where ğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘(ğ‘‹, ğœ–)  returns a set with all vectors in set X rounded to Îµ  precision. 

9 

 
 
Proposition  2  Let  us  consider  an  episodic  q-objective  MDP  with  initial  state  s0 
satisfying the following assumption: 

1. Immediate rewards ğ‘Ÿâƒ—(s,a,sâ€™) are bounded by rmin, rmax. 

Then the size of the Vn(s) sets for all states s calculated by WLP(Îµ)  after n iterations, 
is bounded by [(R Ã—n+1)/ Îµ]q-1, where R = rmax âˆ’ rmin. 

The proof is straightforward from the considerations in the proof of proposition 
1,  since  regardless  of  the  number  of  possible  policies,  that  is  the  maximum 
number of different limited precision vectors that can be possibly generated by 
the algorithm in n iterations. 

4  Experimental evaluation 
This section analyzes the performance of WLP(Îµ) on two different MOMDPs: the 
Stochastic Deep Sea Treasure environment; and the N-pyramid environment. 

4.1  Stochastic Deep Sea Treasure 

We first analyze the performance of WLP(Îµ) on a modified version of the Deep Sea 
Treasure  (DST)  environment  from  Vamplew  et  al.  (2008).  The  DST  is  a 
deterministic MOMDP defined over a 11 Ã— 10 grid environment, as depicted in 
figure 4. The true Pareto front for this problem is straightforward. In any case, the 
environment  satisfies  all  the  simplifying  assumptions  of  proposition  1,  and 
therefore  can  easily  be  solved  with  reasonable  resources  by  multi-policy 
methods. 

We introduce the Stochastic DST environment with right-down moves (SDST-
RD),  as  a  modified  version  of  the  DST  task.  Our  aim  is  to  provide  a  test 
environment where the approximations of the Pareto front calculated by multi-
policy approaches like WLP(Îµ) can be evaluated over a truly stochastic MOMDP. 
We consider the same grid environment depicted in figure 4. The agent controls 
a submarine that is placed in the top left cell at the beginning of each episode. The 
agent can move to adjacent neighboring cells situated down or to its right. Moves 
outside the grid are not  allowed (i.e. the state  space has no cycles). When  two 
moves are allowed, the agent actually moves in the specified direction 80% of the 
time, and in the other allowed direction the remaining 20%. Episodes terminate 
when the submarine reaches one of the bottom cells. Each bottom cell contains a 
treasure,  whose  value  is  indicated  in  the  cell.  The  agent  has  two  objectives: 
maximizing  a  penalty  score  (a  reward  of  -1  is  received  after  each  move),  and 
maximizing  treasure  value  (the  reward  value  of  the  treasure  is  received  after 
reaching a treasure cell). 

The state space of the SDST-RD is a directed acyclic graph. Therefore, the exact 
Pareto front of the start  state can  be potentially calculated using the recursive 
backwards algorithm discussed above (we denoted this as algorithm B). 

10 

 
This provides a reference for the approximations calculated by WLP(Îµ). We are 
interested  in  analyzing  the  performance  of  the  algorithms  as  a  function  of 
problem size. Therefore, we actually consider a set of subproblems obtained form 
the SDST-RD. The state space of subproblem i comprises all 11 rows, but only the 
i leftmost columns of the grid shown in figure 4. 

The  subproblems  were  solved  with  algorithm  B,  and  with  WLP(Îµ)  with 
precision Îµ âˆˆ {0.1, 0.05, 0.02, 0.01, 0.001}. Smaller values provide higher precision 
and better approximations. The discount rate was set  to  Î³ = 1.0. With  WLP, the 
number  of  iterations  for  each  subproblem  was  set  to  the  distance  of  the  most 
distant treasure. 

Figure 5 displays the exact Pareto front for subproblems 1 to 6 as calculated 
by algorithm B. Larger subproblems exceeded a 96 hour time limit. In general, the 
Pareto fronts present a non-convex nature, like in the standard deterministic DST. 
However, the  number of nondominated vectors in  V  (s0) is much larger (in  the 
standard DST, subproblem i has just i nondominated solution vectors). 

Figure 4: Deep Sea Treasure environment from Vamplew et al. (2008). Start cell 
is denoted by â€™Sâ€™. 

11 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Algorithm 

B 

WLP 

Subpr. 

0.001  0.01  0.02  0.05  0.1 

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 

1 
2 
6 
56 
3542 
34243 
- 
- 
- 
- 

1 
2 
6 
56 

1 
1 
2 
2 
6 
6 
34 
45 
1152  182  107 
1923  238  143 

1 
1 
2 
2 
6 
5 
24 
15 
49 
29 
58 
36 
679  344  137 
69 
602  316  137 
72 
94 
423  181 
491  208  108 

- 
- 
- 
- 

- 
- 

Table 1: Cardinality of V (s0) for the SDST-RD subproblems. 

Only precisions of Îµ â‰¥ 0. 02 were able to solve all 10 subproblems within the time 
limit.  Figure  6  shows  the  best  Pareto  frontier  approximation  obtained  for 
subproblems 7 to 10 by WLP within the time limit. 

The cardinality of V (s0) for each subproblem is shown in table 1. These figures 
illustrate  the  combinatorial  explosion  frequently  experienced  in  stochastic 
MOMDP even for acyclic state spaces. 

Figure 7 shows the maximum number of vectors stored for each subproblem 
by  each  algorithm  (in  logarithmic  scale).  As  expected,  the  requirements  of  the 
exact algorithm grow much faster than any of the approximations. WLP achieves 
better  approximations,  and  in  consequence  larger  vector  sets,  with  increasing 
precision (smaller values of Îµ). 

Finally, we briefly examine the  quality of the approximations.  Figure 8 
shows the exact Pareto front and the approximations obtained for subproblem 6. 
In  general,  all  approximations  lie  very  close,  and  are  difficult  to  distinguish 
visually  from  each  other  in  objective  space.  Results  for  other  subproblems 
(omitted  for  brevity)  show  a  similar  pattern.  Figure  9  zooms  into  a  particular 
region of the Pareto front, to display values obtained by different precisions. In 
general, the approximations are evenly distributed across the true Pareto front, 
and  reasonably  close  according  to  the  precision  values.  Table  2  shows  the 
hypervolume of the V (s0) sets returned by each algorithm for each subproblem. 
These  are  calculated  relative  to  the  standard  reference  point  (âˆ’25,0)  used  for 
DST. Differences with the exact Pareto front are almost negligible for Îµ â‰¤ 0.02. 

12 

 
 
 
 
 
 
 
 
 
Figure 5: Exact Pareto front for subproblems 1 to 6 of the SDST-RD task calculated 
with algorithm B. 

Figure 6: Best approximation of the Pareto frontier obtained for subproblems 7 
to 10 of the SDST-RD task with algorithm WLP. The exact frontier of subproblem 
6 is included as reference. 

13 

 
 
Figure 7: Memory requirements (# of vectors) of each algorithm for the different 
subproblems of the SDST-RD task (log scale). 

4.2  N-Pyramid 

The N-pyramid environment presented here is inspired in the pyramid MOMDP 
described by Van Moffaert and NowÃ© (2014). More precisely, we define a family 
of problems of variable size over N Ã—N grid environments. Each cell in the grid is 
a  different  state.  Each  episode  starts  always  at  the  start  node,  situated  in  the 
bottom left corner (with coordinates (x,y) = (1,1)). The agent can move in any of 
the four cardinal directions to an adjacent cell, except for moves that would lead 
the agent outside of the grid, which are not allowed. Transitions are stochastic, 
with  the  agent  moving  in  the  selected  direction  with  probability  0.95.  With 
probability  0.05  the  agent  actually  moves  in  a  random  direction  from  those 
available at the state. The episode terminates when the agent reaches one of the 
cells  in  the  diagonal  facing  the  start  state.  Figure  10  displays  a  sample  grid 
environment  for  N  = 5.  The  agent  wants  to maximize  two objectives.  For  each 
transition to a non-terminal state, the agent receives a vector reward of (âˆ’1,âˆ’1). 
When  a  transition  reaches  a  terminal  cell  with  coordinates  (x,y),  the  agent 
receives  a  vector  reward  of  (10x,10y).  This  environment  defines  a  cyclic  state 
space. Therefore, its solution cannot be calculated using the B algorithm. 

14 

 
 
Figure  8:  Pareto  fronts  obtained  for  SDST-RD  subproblem  6.  The  area  in  the 
rectangle is shown magnified in figure 9. 

Algorithm 

B 

WLP 

Pr. 

0.001 

0.01 

0.02 

0.05 

0.1 

24.0 
24.0 
24.0 
1 
41.8 
41.8 
41.8 
2 
57.9 
57.9 
57.9 
3 
88.9 
88.9 
88.9 
4 
134.5  134.5  134.4 
5 
252.6  252.6  252.6 
6 
349.8 
7 
687.7 
8 
- 
9 
10 
- 
Table 2: Hypervolumes of V (s0) for the SDST-RD task. 

24.0 
24.0 
24.0 
41.8 
41.8 
41.8 
58.6 
57.5 
57.7 
89.4 
89.3 
88.9 
135.7 
134.7 
134.5 
253.0 
252.7 
252.6 
350.6 
350.3 
349.8 
689.7 
688.4 
687.6 
956.1 
953.0 
951.1 
1513.9  1517.9  1522.2 

- 
- 
- 
- 

- 
- 
- 
- 

15 

 
 
 
 
 
 
 
 
 
 
 
Figure 9: Enlarged portion of objective space from figure 8 (SDST-RD subproblem 
6). 

Figure 10: An N-pyramid environment for N = 5. The start state is denoted by â€™Sâ€™. 
Terminal states are indicated in light grey. 

WLP 

N  0.005  0.01  0.05  0.1  1.0 
2 
2 
3 
3 
8 
4 
19 
5 

2 
84 
- 
- 
Table 3: Cardinality of V (s0) for the N-pyramid instances. 

2 
137 
- 
- 

2 
20 
74 
- 

2 
30 
- 
- 

16 

 
 
 
 
 
 
The  N-pyramid  environment  was  solved with WLP(Îµ) using Îµ âˆˆ{1.0, 0.1, 
0.05  ,0.01  ,0.005}.  The  discount  rate  was  set  to  Î³  =  1.0,  and  the  number  of 
iterations to 3 Ã— N. A twelve hour runtime limit was imposed for all experiments. 
The largest subproblem solved was N = 5, for Îµ = 1.0. 

Figure 11 displays the best frontier approximation for each problem instance, 
and  the  corresponding  Îµ  value.  Smaller  Îµ  values  could  not  solve  the  problem 
within the time limit. 

Figure 12 displays the frontiers calculated by the different precision  values 
for the 3-pyramid. This was the largest problem instance solved by all precision 
values.  Figure  13  displays  an  enlargement  of  a  portion  of  this  frontier.  This 
illustrates  the  size  and  shape  of  the  different  approximations.  As  expected, 
smaller values of Îµ  produce more densely populated policy values. 

Finally, table 3 summarizes the cardinality of the V (s0) sets calculated for each 
problem instance by the different precision values, and table 4 the corresponding 
hypervolumes using the original reference point (âˆ’20,âˆ’20). 

Figure 11: Best approximations obtained for the Pareto fronts for the N-pyramid 
problem instances. 

17 

 
 
WLP 

0.1 
764.00 

0.05 
766.25 

0.005 
766.25 

0.01 
766.25 

1.0 
N 
755.00 
2 
3  1201.16  1200.58  1199.40  1201.51  1180.00 
-  1686.61  1679.00 
4 
5 
-  2255.00 
- 
Table 4: Hypervolumes of V (s0) for the N-pyramid task. 

- 
- 

- 
- 

Figure 12: Approximated Pareto fronts for the 3-pyramid instance. An enlarged 
portion is displayed in figure 13. 

18 

 
 
 
 
 
 
Figure  13:  Enlarged  portion  of  objective  space  from  figure  12  (3-pyramid 
instance). 

5  Conclusions and future work 
This  paper  analyzes  some  practical  difficulties  that  arise  in  the  solution  of 
MOMDPs. We show that the number of nondominated policy values is tractable 
(for a fixed number of objectives q) only under a number of limiting assumptions. 
If any of these assumptions is violated, then the number of nondominated values 
becomes intractable or even infinite with problem size in the worst case. 

Multi-policy  methods  for  MOMDPs  try  to  approximate  the  set  of  all 
nondominated  policy  values  simultaneously.  Previous  works  have  addressed 
mostly  deterministic  tasks  satisfying  the  limiting  assumptions  that  guarantee 
tractability. 

We show that, if policy values are restricted to vectors with limited precision, 
the number of such values becomes tractable. This idea is applied to a variant of 
the  algorithm  proposed  by  White  (1982).  This  new  variant  has  been  analyzed 
over  a  set  of  stochastic  benchmark  problems.  Results  show  that  good 
approximations of the Pareto front can be obtained. To our knowledge, the results 
reported deal with the hardest stochastic problems solved to date by multi-policy 
multi-objective dynamic programming algorithms. 

19 

 
 
Future  work  includes  the  application  of  the  limited  precision  idea  to  more 
general value iteration algorithms, and the generation of additional benchmark 
problems  for  stochastic  MOMDPs.  This  should  support  the  exploration  and 
evaluation  of  other  approximate  solution  strategies, 
like  multi-policy 
multiobjective evolutionary techniques. 

References 

Barde SRA, Yacout S, Shin H (2019) Optimal preventive maintenance policy based 
on reinforcement learning. Journal of Intelligent Manufacturing 30(1):147â€“161 

Daellenbach  H,  De  Kluyver  C  (1980)  Note  on  multiple  objective  dynamic 

programming. J Opl Res Soc 31:591â€“594 

Drugan  M,  Wiering  M,  Vamplew  P,  Chetty  M  (2017)  Special  issue  on 

multiobjective reinforcement learning. Neurocomputing 263:1 â€“ 2 

Drugan MM (2019) Reinforcement learning versus evolutionary computation: A 

survey on hybrid algorithms. Swarm Evol Comput 44:228â€“246 

Gen  M,  Lin  L  (2014)  Multiobjective  evolutionary  algorithm  for  manufacturing 
Intelligent 

scheduling  problems:  state-of-the-art  survey. 
Manufacturing 25(5):849â€“866 

Journal  of 

Giuliani M, Galelli S, Soncini-Sessa R (2014) A dimensionality reduction approach 
for  many-objective  markov  decision  processes:  Application  to  a  water 
reservoir operation problem. Environ Model Softw 57:101â€“114 

Hansen P (1980) Bicriterion path problems. In: Lecture Notes in Economics and 

Mathematical Systems 177, Springer, pp 109â€“127 

Kuhnle A, Kaiser JP, Theiss F, Stricker N, Lanza G (2020) Designing an adaptive 
production control system using reinforcement learning. Journal of Intelligent 
Manufacturing DOI https://doi.org/10.1007/s10845-020-01612-y 

Liu Q, Dong M, Lv W, Ye C (2019) Manufacturing system maintenance based on 
dynamic  programming  model  with  prognostics  information.  Journal  of 
Intelligent Manufacturing 30(3):1155â€“1173 

Lizotte DJ, Bowling M, Murphy SA (2010) Efficient reinforcement learning with 
multiple  reward  functions  for  randomized  controlled  trial  analysis.  In: 
Proceedings  of  the  27th  International  Conference  on  Machine  Learning,  pp 
695â€“ 702 

Perny  P,  Weng  P  (2010)  On  finding  compromise  solutions  in  multiobjective 

markov decision processes. In: ECAI 2010, pp 969â€“970 

20 

Puterman  ML  (1994)  Markov  Decision  Processes.  Discrete  Stochastic  Dynamic 

Programming. Wiley 

Roijers  DM,  Whiteson  S  (2017)  Multi-Objective  Decision  Making.  Synthesis 
Lectures on Artificial Intelligence and Machine Learning, Morgan & Claypool 

Roijers DM, Vamplew P, Whiteson S, Dazeley R (2013) A survey of multiobjective 
sequential  decision-making.  Journal  of  Artificial  Intelligence  Research  (JAIR) 
48:67â€“113 

Ruiz-Montiel  M,  Mandow  L,  PÃ©rez-de-la-Cruz  J  (2017)  A  temporal  difference 
method for multi-objective reinforcement learning. Neurocomputing 263:15â€“ 
25 

Sutton R, Barto A (2018) Reinforcement learning: an introduction, 2nd edn. The 

MIT Press 

Vamplew  P,  Yearwood  J,  Dazeley  R,  Berry  A  (2008)  On  the  limitations  of 
scalarisation  for  multi-objective  reinforcement  learning  of  pareto  fronts.  In: 
Wobcke W, Zhang M (eds) AI 2008: Advances in Artificial Intelligence, Springer 
Berlin Heidelberg, Berlin, Heidelberg, pp 372â€“378 

Vamplew  P,  Dazeley  R,  Barker  E,  Kelarev  A  (2009)  Constructing  stochastic 
mixture policies for episodic multiobjective reinforcement learning tasks. In: 
AI 2009: Proceedings of the Twenty-Second Australasian Joint Conference on 
Artificial Intelligence, pp 340â€“349 

Van  Moffaert  K,  NowÃ©  A  (2014)  Multi-Objective  Reinforcement  Learning  using 
sets  of  Pareto  dominating  policies.  Journal  of  Machine  Learning  Research 
15:3663â€“3692 

Veeramani  S,  Muthuswamy  S,  Sagar  K,  Zoppi  M  (2020)  Artificial  intelligence 
planners  for  multi-head  path  planning  of  swarmitfix  agents.  Journal  of 
Intelligent Manufacturing 31(4):815â€“832 

White  DJ  (1982)  Multi-objective  infinite-horizon  discounted  Markov  decision 
processes. Journal of Mathematical Analysis and Applications 89:639â€“647 

Wray KH, Zilberstein S, Mouaddib A (2015) Multi-objective mdps with 

conditional lexicographic reward preferences. In: Twenty-Ninth AAAI 
Conference on Artificial Intelligence 

Zhang W, Xu W (2017) An effective hybrid evolutionary algorithm for stochastic 
multiobjective  assembly  line  balancing  problem.  Journal  of  Intelligent 
Manufacturing 28(3):783â€“790 

Zitzler E, Thiele L, Laumanns M, Fonseca CM, da Fonseca VG (2003) Performance 

assessment of multiobjective optimizers: an analysis and review. IEEE 
Trans Evol Comput 7(2):117â€“132 

21 

