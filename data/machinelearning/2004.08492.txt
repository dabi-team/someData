Orbit: Probabilistic Forecast with Exponential Smoothing

Edwin Ng 1 Zhishi Wang 1 Huigang Chen 1 Steve Yang 1 Slawek Smyl 1

{edwinng, zhishiw, huigang, steve.yang, slawek}@uber.com

1
2
0
2

n
a
J

2
2

]

O
C

.
t
a
t
s
[

4
v
2
9
4
8
0
.
4
0
0
2
:
v
i
X
r
a

Abstract
Time series forecasting is an active research topic
in academia as well as industry. Although we
see an increasing amount of adoptions of ma-
chine learning methods in solving some of those
forecasting challenges, statistical methods remain
powerful while dealing with low granularity data.
This paper introduces a package Orbit where it
reﬁned Bayesian exponential smoothing model
with the help of probabilistic programming pack-
age such as Stan and Pyro. Our model reﬁnements
include additional global trend, transformation for
multiplicative form, noise distribution and choice
of priors. A benchmark study is conducted on a
rich set of time-series data sets for our models
along with other well-known time series models.

1. Introduction

Time series forecasting is one of the most popular and yet
the most challenging tasks, faced by researchers and prac-
titioners. Its industrial applications have a wide range of
areas such as social science, bioinformatics, ﬁnance, busi-
ness operations and revenue forecast. At Uber, time series
forecasting has its applications from demand/supply predic-
tion in the marketplace to the Ads budget optimization in
the marketing data science.

In recent years, machine learning and deep learning methods
(Gers et al., 1999; Huang et al., 2015; Selvin et al., 2017)
have gained increasing attention in the time series forecast-
ing, due to their great ability in capturing the non-linear
trend and complex interactions within multivariate time se-
ries. However, the theories for deep learning are still in
the active research and development progress and not well
established yet in the forecasting literature, especially in the
case of univariate time series. At the same time, the black-
box nature of machine learning/deep learning models causes
difﬁculties in interpretability and explainability (Gunning,
2017; Gilpin et al., 2018). In a benchmark study with differ-

1Uber Technologies, Inc..

Preliminary work. To be submitted to StanCon 2020.

ent time series data (Hewamalage et al., 2019), the authors
show mixed performances of various deep learning models
when compared against traditional statistical models.

On the other hand, traditional statistical parametric models
have a well-established theoretical foundation, such as the
popular autoregressive integrated moving average (ARIMA)
(Box & Jenkins, 1968) and exponential smoothing (Gard-
ner Jr, 1985). Recently, researchers from Facebook devel-
oped Prophet (Taylor & Letham, 2018), which is based
on an additive model where non-linear trends are ﬁt with
seasonality and holidays.

In this paper we propose a family of reﬁned Bayesian ex-
ponential smoothing models, with great ﬂexibility on the
choice of priors, model type speciﬁcations, as well as noise
distribution. Our model introduces a novel global trend
term, which works well for short term time series. Most
importantly, our models come with a well crafted compute
software/package in Python, called Orbit (Object-oriented
Bayesian Time Series). Our package leverages the prob-
abilistic programming languages, Stan (Carpenter et al.,
2017) and Pryo (Bingham et al., 2019), for the underlying
MCMC sampling process and optimization. Pyro, devel-
oped by researchers at Uber, is a universal probabilistic pro-
gramming language (PPL) written in Python and supported
by PyTorch and JAX on the backend. Orbit currently has
a subset of the available prediction and sampling methods
available for estimation using Pyro.

The remainder of this paper is organized as follows. In
section 2, a review is given on some popular statistical
parametric time series models. In section 3, we give our
reﬁned model equations, and Orbit package design review.
In section 4, an extensive benchmark study is conducted to
evaluate the proposed models’ performance and compare
with other time series models discussed in the paper. sec-
tion 5 is about the conclusion and future work. We focus on
the univariate time series in this work.

 
 
 
 
 
 
Orbit: Probabilistic Forecast with Exponential Smoothing

2. Review

2.1. Problem Deﬁnition

Let {y1, · · · , yt} be a sequence of observations at time t.
Then a point forecast at time T denotes the process of esti-
mating the set of future values of {ˆyT +1, · · · , ˆyT +h} given
information at t ∀t ≤ T where h denotes the forecasting
horizon.

The ARMA(p,q) model is deﬁned for stationary data. How-
ever, many realistic time series in practice exhibit a non-
stationary structure, e.g., time series with trend and sea-
sonality. The ARIMA(p,d,q) (or SARIMA) overcomes this
limitation by including an integration parameter of order
d. In principle, ARIMA works by applying d differencing
transformations to the time series until it becomes stationary
before applying ARMA(p,q).

2.2. The State Space Models

2.2.2. EXPONENTIAL SMOOTHING

State space models is a family of models which can be
written in a general form

Another popular approach is done through exponential
smoothing in a reduced form

yt = Z T

t αt + (cid:15)t

αt+1 = Ttαt + Rtηt

One big advantage is that they are modular, in the sense
that independent state components can be combined by
concatenating their observation vectors Zt and arranging
the other model matrices as elements in a block diagonal
matrix. This provides considerable ﬂexibility for modeling
trend, seasonality, regressors, and potentially other state
components that may be necessary in practice.

However, such form is not unique since the same model can
be expressed in multiple ways. In practice, there are various
reduced forms introduced by researchers. We will go over
some of those widely used in the industry.

2.2.1. ARIMA

ARMA model is one of the most commonly used methods
to model univariate time series. ARMA(p,q) combines two
components: AR(p), and MA(q). In AR(p) model, the value
of a given time series, yt, can be estimated using a linear
combination of the p past observations, together with an
error term (cid:15)t and a constant term c:

yt = c + Σp

i=1ψiyt−i + (cid:15)t

where ψi, ∀i ∈ {1, · · · , p} denote the model parameters,
and p represents the order of the model. Similarly, the
MA(q) model uses past errors as explanatory variables:

yt = µ + Σq

i=1θi(cid:15)t−i + (cid:15)t

where µ denotes the mean of the observations, θi, ∀i ∈
{1, · · · , q} represents the parameters of the models and q
denotes the order of the model. Essentially, the method
MA(q) models the time series according to the random
errors that occurred in the past q lags.

The model ARMA(p,q) can be constructed by combining
the AR(p) model with the MA(q) model, i.e.,

yt = c + Σp

i=1ψiyt−i + Σq

i=1θi(cid:15)t−i + (cid:15)t

ˆyt|t−1 = Z T αt−1

(cid:15)t = yt − ˆyt|t−1

αt = T αt−1 + k(cid:15)

This was ﬁrst described by Box and Jekins (Box, 1998).
Note that ˆyt|t−1 can be computed recursively. Hyndman
et al. 2008 provide a complete review on such form. In the
literature, it introduces an ETS form with notation “A” and
“M” which stands for additive and multiplicative respectively.
Well known methods such as Holt-Winter’s model can be
viewed as ETS(A, A, A) or ETS(A, M, A). For notation
such as Ad, the subscript d denotes a damped factor is intro-
duced. Many of those are implemented under the forecast
package written in the statistical programming language R.

2.2.3. BAYESIAN STRUCTURAL TIME SERIES (BSTS)

Scott and Varian (Scott & Varian, 2014) provide a bayesian
approach on a reduced form as such

yt = µt + τt + βT xt(cid:15)t

µt+1 = µt + δt + η0t

δt+1 = δt + η1t

τt+1 = −

S−1
(cid:88)

s=1

τt + η2t

This model is named as local linear trend model and is avail-
able in the bsts package written in R. It allows additional
regression estimation with coefﬁcients β.

2.3. Prophet

Researchers at Facebook (Taylor & Letham, 2018) use a
generalized additive model (GAM) with three main com-
trend, seasonality, and holidays. They can be
ponents:
combined in the following equation:

yt = gt + st + ht + (cid:15)t

Orbit: Probabilistic Forecast with Exponential Smoothing

where gt is the piecewise linear or logistic growth curve
to model the non-periodic changes in the time series, st
is the seasonality term, ht is the holiday effect with irreg-
ular schedules, and (cid:15)t is the error term. On a high level,
Prophet is framing the forecasting problem as a curve-ﬁtting
exercise rather than looking explicitly at the time based
dependence of each observation within a time series. As
a computational tool/software, moreover, Prophet allows
users to manually supply change points in ﬁtting the trend
term and set the boundaries for saturation growth, which
gives great ﬂexibility in business applications.

3. The Orbit Package

3.1. Reﬁned Model I - LGT

the computation cost by vectorizing the noise generation
process.

One limitation in this model is that it assumes lt > 0, ∀t. To
ensure such condition is satisﬁed during the training period,
it imposes a requirement such that yt > 0, ∀t.

3.2. Reﬁned Model II - DLT

For use cases with yt ∈ R, we provide an alternative -
Damped Local Trend (DLT) model.

Such model is an extension of ETS(A, Ad, A) in Hyndman
et al. 2008, where θ is known as the damped factor. In the
forecast process, we have

Our proposed Local and Global Trend (LGT) model is an
additive version based on the multiplicative model proposed
in Rlgt (Smyl et al., 2019). In the forecast process, we have

yt = µt + st + rt + (cid:15)t
µt = D(t) + lt−1 + θbt−1

yt = µt + st + (cid:15)t
µt = lt−1 + ξ1bt−1 + ξ2lλ
(cid:15)t ∼ Student(ν, 0, σ)
σ ∼ HalfCauchy(0, γ0)

t−1

where lt−1, ξ1bt−1, ξ2lλ
t−1, st and (cid:15)t can be viewed as level,
local trend, global trend, seasonality and error term, respec-
tively.

In the update process, it is similar to the triple exponential
smoothing form

lt = ρl(yt − st) + (1 − ρl)lt−1
bt = ρb(lt − lt−1) + (1 − ρb)bt−1

st+m = ρs(yt − lt) + (1 − ρs)st

where ρl, ρb and ρs are the smoothing parameters. Finally,
we ﬁt ρl, ρb, ρs, θ, ξ, λ, ν in our training process while γ0
is a data-driven scaler.

This model structure is similar to the ETS family established
by Rob Hynd (De Livera et al., 2011; Hyndman & Athana-
sopoulos, 2018) except that the form we proposed is a hybrid
model generalizing both ETS(A, A, A) (when ξ = 0) and
autoregressive model (when ξ > 0). A similar approach is
found earlier in (Smyl, Zhang et al., 2015). The advantages
over Rlgt with such form are two-fold. First, the computa-
tion is more effective on the additive form, where we can
apply simple log transformation to maintain multiplicative
properties. Second, σ in Rlgt is parameterized with yt which
introduces dependency in noise generation, while σ is re-
ﬁned as an independent noise in LGT. This change reduces

with the update process as such

gt = D(t)
lt = ρl(yt − gt − st − rt) + (1 − ρl)(lt−1 + bt−1)
bt = ρb(lt − lt−1) + (1 − ρb)θbt−1
st+m = ρs(yt − lt − rt) + (1 − ρs)st

rt = Σjβjxjt

There are a few choices of D(t) as a deterministic global
trend. Options such as linear, log-linear and logistic are
included in the package. Another feature of DLT is the
introduction of regression component rt. This serves the
purpose of nowcasting or forecasting when exogenous re-
gressors are known such as events and holidays. Without
loss of generality, assume

βj ∼ Normal(µj, σj)

where µj = 0 and σj = 1 by default as a non-informative
prior. There are more choices of priors for the regression
component in the package.

3.3. Multiplicative Form

In previous section, we derive a general additive form for
LGT and DLT. It is trivial that by using transformation
y(cid:48)
t = ln(yt), our forecast process yields a multiplicative
form as such

ˆyt = µ(cid:48)

t · s(cid:48)

t · r(cid:48)
t

In the later benchmarking process, we used models ﬁtted in
such multiplicative form to report performance metrics.

Orbit: Probabilistic Forecast with Exponential Smoothing

3.4. Package Design

Orbit is our Python package to implement the reﬁned models
discussed above. This package is written and designed from
a strict object oriented perspective with the goals of re-
usability, ease of maintenance, and high efﬁciency.

The base Estimator class contains generic logic to handle
interaction with the underlying inference engine (e.g PyStan,
Pyro) along with utilities to load and save Orbit models, and
the speciﬁcs of the model are implemented in each model
class. Figure 1 is the overall workﬂow of Orbit package.

Figure 1. Overall Design of Orbit Package.

4. Model Benchmark

4.1. Data

We performed a comprehensive benchmark study on ﬁve
datasets:

• US and Canada rider ﬁrst-trips with Uber (20 weekly

series by city)

• US and Canada driver weekly ﬁrst-trips with Uber (20

weekly series by city)

• Worldwide ﬁrst-orders with Uber Eats(15 daily series

by country)

• M3 series (1428 monthly series)

• M4 series (359 weekly series)

where M3/M4 time series are well-known in the forecast
community (Makridakis et al., 2018).

4.2. Performance Metric

We use symmetric mean absolute percentage error
(SMAPE), a widely adopted forecast metric, as our per-
formance benchmark metric

SMAPE =

h
(cid:88)

t=1

|Ft − At|
(|Ft| + |At|)/2

where Xt represents the value measured at time t and h is
the forecast horizon.

Here h is the forecast horizon which can also be considered
as the “holdouts” in a backtest process. Following what
competitions suggested, we use 13 forecast horizon and
18 forecast horizon, respectively, for M4 weekly and M3
monthly series with 1 split; for Uber datasets, we use h =
13, 3 splits with 26 incremental steps for weekly series and
h = 28, 4 splits and 14 incremental steps for daily series.
With multiple splits, we expect more robust result. The
calculation is done with the help of our backtest utilities
built in the Orbit package.

4.3. Results

We compared our proposed models, LGT and DLT, to other
popular time series models such as SARIMA (Seabold &
Perktold, 2010) and Facebook Prophet (Taylor & Letham,
2018). Both Prophet and Orbit models use Maximum A
Posterior (MAP) estimates and they are conﬁgured as similar
as possible in terms of optimization and seasonality settings.
For SARIMA, we ﬁt the (1, 1, 1) × (1, 0, 0, )S structure by
maximum likelihood estimation (MLE) where S represents
the choice of seasonality.

Orbit: Probabilistic Forecast with Exponential Smoothing

Table 1. Model Average SMAPE Comparison

MODEL
RIDER-WEEKLY
DRIVER-WEEKLY
EATER-DAILY
M4-WEEKLY
M3-MONTHLY

LGT
0.108 (0.030)
0.205 (0.098)
0.178 (0.036)
0.077 (0.073)
0.145 (0.155)

DLT
0.106 (0.033)
0.206 (0.095)
0.182 (0.043)
0.081 (0.080)
0.148 (0.158)

PROPHET
0.121 (0.035)
0.274 (0.162)
0.309 (0.055)
0.192 (0.321)
0.193 (0.234)

SARIMA
0.110 (0.041)
0.207 (0.123)
0.221 (0.040)
0.076 (0.131)
0.150 (0.171)

Within a dataset, the models in consideration were run on
each time series separately, and then we report the aggre-
gated metrics. Table 1 gives the average and the standard
deviation (within parentheses) of SMAPE across different
models and datasets.

It shows that our models consistently deliver better accuracy
than other candidate time series models in terms of SMAPE.
Orbit is also computationally efﬁcient. For example, the
average compute time per series with full MCMC sampling
and prediction from a subset of M4 weekly data is about
2.5 minutes and 16 ms. The run time for the same series
in Prophet is about 10 minutes for sampling and 2.4 s for
prediction. That’s a 4x speed up in training, and orders of
magnitude difference in prediction.

Code and M3/M4 data used in this benchmark study are
available upon request.

5. Conclusion

We have shown that our proposed models outperform the
baseline time series models consistently in terms of SMAPE
metrics. Furthermore, we also identiﬁed compute cost im-
provements when using the Orbit package. For our future
work, we will continue to actively maintain the package, in-
corporate new models, and provide new features or enhance-
ments (to support dual seasonality, fully Pyro integration,
etc).

References

Bingham, E., Chen, J. P., Jankowiak, M., Obermeyer, F.,
Pradhan, N., Karaletsos, T., Singh, R., Szerlip, P., Hors-
fall, P., and Goodman, N. D. Pyro: Deep universal proba-
bilistic programming. The Journal of Machine Learning
Research, 20(1):973–978, 2019.

Box, G. P, Jenkins, GM and Reinsel, GC. Time series
analysis: forecasting and control, volume 54, pp. 176–
180. 1998.

Box, G. E. and Jenkins, G. M. Some recent advances in
forecasting and control. Journal of the Royal Statistical
Society. Series C (Applied Statistics), 17(2):91–109, 1968.

Carpenter, B., Gelman, A., Hoffman, M. D., Lee, D.,
Goodrich, B., Betancourt, M., Brubaker, M., Guo, J.,

Li, P., and Riddell, A. Stan: A probabilistic programming
language. Journal of statistical software, 76(1), 2017.

De Livera, A. M., Hyndman, R. J., and Snyder, R. D. Fore-
casting time series with complex seasonal patterns using
exponential smoothing. Journal of the American statisti-
cal association, 106(496):1513–1527, 2011.

Gardner Jr, E. S. Exponential smoothing: The state of the

art. Journal of forecasting, 4(1):1–28, 1985.

Gers, F. A., Schmidhuber, J., and Cummins, F. Learning to

forget: Continual prediction with lstm. 1999.

Gilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M.,
and Kagal, L. Explaining explanations: An overview of
interpretability of machine learning. In 2018 IEEE 5th
International Conference on data science and advanced
analytics (DSAA), pp. 80–89. IEEE, 2018.

Gunning, D. Explainable artiﬁcial intelligence (xai). De-
fense Advanced Research Projects Agency (DARPA), nd
Web, 2, 2017.

Hewamalage, H., Bergmeir, C., and Bandara, K. Recurrent
neural networks for time series forecasting: Current status
and future directions. arXiv preprint arXiv:1909.00590,
2019.

Huang, Z., Xu, W., and Yu, K. Bidirectional lstm-crf models
for sequence tagging. arXiv preprint arXiv:1508.01991,
2015.

Hyndman, R., Koehler, A. B., Ord, J. K., and Snyder, R. D.
Forecasting with exponential smoothing: the state space
approach. Springer Science & Business Media, 2008.

Hyndman, R. J. and Athanasopoulos, G. Forecasting: prin-

ciples and practice. OTexts, 2018.

Hyndman, R. J., Athanasopoulos, G., Bergmeir, C., Cac-
eres, G., Chhay, L., O’Hara-Wild, M., Petropoulos, F.,
Razbash, S., Wang, E., and Yasmeen, F. forecast: Fore-
casting functions for time series and linear models. 2018.

Makridakis, S., Spiliotis, E., and Assimakopoulos, V. The
m4 competition: Results, ﬁndings, conclusion and way
International Journal of Forecasting, 34(4):
forward.
802–808, 2018.

Scott, S. L. and Varian, H. R. Predicting the present with
bayesian structural time series. International Journal of
Mathematical Modelling and Numerical Optimisation, 5
(1-2):4–23, 2014.

Seabold, S. and Perktold, J. statsmodels: Econometric and
statistical modeling with python. In 9th Python in Science
Conference, 2010. Package version 0.11.1.

Orbit: Probabilistic Forecast with Exponential Smoothing

Selvin, S., Vinayakumar, R., Gopalakrishnan, E., Menon,
V. K., and Soman, K. Stock price prediction using lstm,
rnn and cnn-sliding window model. In 2017 international
conference on advances in computing, communications
and informatics (icacci), pp. 1643–1647. IEEE, 2017.

Smyl, S., Bergmeir, C., Wibowo, E., and Ng, T. W. Bayesian
exponential smoothing models with trend modiﬁcations.
https://cran.r-project.org/web/packages/Rlgt/index.html,
2019.

Taylor, S. J. and Letham, B. Forecasting at scale. The Amer-
ican Statistician, 72(1):37–45, 2018. Package version
0.7.1.

