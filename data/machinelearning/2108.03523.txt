KLIFF: A framework to develop physics-based and machine learning interatomic
potentials

Mingjian Wena,1, Yaser Afshara, Ryan S. Elliotta, Ellad B. Tadmora,

∗

aDepartment of Aerospace Engineering and Mechanics, University of Minnesota, Minneapolis, MN 55455, USA

1
2
0
2

t
c
O
1

]
i
c
s
-
l
r
t

m

.
t
a
m
-
d
n
o
c
[

3
v
3
2
5
3
0
.
8
0
1
2
:
v
i
X
r
a

Abstract

Interatomic potentials (IPs) are reduced-order models for calculating the potential energy of a system of atoms given their posi-
tions in space and species. IPs treat atoms as classical particles without explicitly modeling electrons and thus are computationally
far less expensive than ﬁrst-principles methods, enabling molecular simulations of signiﬁcantly larger systems over longer times.
Developing an IP is a complex iterative process involving multiple steps: assembling a training set, designing a functional form,
optimizing the function parameters, testing model quality, and deployment to molecular simulation packages. This paper intro-
duces the KIM-based learning-integrated ﬁtting framework (KLIFF), a package that facilitates the entire IP development process.
KLIFF supports both physics-based and machine learning IPs. It adopts a modular approach whereby various components in the
ﬁtting process, such as atomic environment descriptors, functional forms, loss functions, optimizers, quality analyzers, and so on,
work seamlessly with each other. This provides a ﬂexible framework for the rapid design of new IP forms. Trained IPs are com-
patible with the Knowledgebase of Interatomic Models (KIM) application programming interface (API) and can be readily used
in major materials simulation packages compatible with KIM, including ASE, DL POLY, GULP, LAMMPS, and QC. KLIFF is
written in Python with computationally intensive components implemented in C++. It is parallelized over data and supports both
shared-memory multicore desktop machines and high-performance distributed memory computing clusters. We demonstrate the
use of KLIFF by ﬁtting a physics-based Stillinger–Weber potential and a machine learning neural network potential for silicon. The
KLIFF package, together with its documentation, is publicly available at: https://github.com/openkim/kliff.

Keywords: interatomic potentials, machine learning, uncertainty, OpenKIM

1. Introduction

Molecular simulations are a powerful computational tech-
nique for exploring material behavior and properties based on
an understanding of the physics of bonding at the atomic scale
[1]. This approach is used across the sciences with examples
such as phase transition in crystals [2], protein folding [3], and
thermal expansion and conductivity of layered 2D materials
[4, 5] to name just a few. At the core of any molecular sim-
ulation lies a description of the interactions between atoms that
produces the forces governing atomic motion. First-principles
approaches (e.g. density functional theory (DFT)) that involve
solving the Schr¨odinger equation of quantum mechanics are
most accurate, but due to hardware and algorithmic limitations,
these approaches are limited to extremely small system sizes
and time scales precluding the study of most systems of tech-
nological interest. For example, the supercell required to sim-
ulate a graphene bilayer with a 1.1◦ twist angle has more than
10,000 atoms, which is well beyond the capabilities of current
ﬁrst-principles approaches [5].

Interatomic potentials (IPs, also known as force ﬁelds) pro-
vide a classical alternative based on the Born–Oppenheimer ap-

∗Corresponding author. E-mail address: tadmor@umn.edu
1Current address: Energy Technologies Area, Lawrence Berkeley National

Laboratory, Berkeley, CA 94720, United States.

proximation (BOA) [6]. Due to the large mass diﬀerence be-
tween nuclei and electrons, the BOA assumes that electrons
instantaneously adapt to changes in nuclei positions adopting
their ground state conﬁguration — eﬀectively decoupling nu-
clei and electron physics. This approximation is reasonable for
many problems of interest in materials science and condensed-
matter physics [1]. Consistent with the BOA, IPs treat atoms
as classical particles without explicitly modeling the electrons,
but strive to capture their inﬂuence on atomic nuclei in an ef-
fective manner. As such, IPs are computationally far less ex-
pensive than ﬁrst-principles methods and can therefore be used
to compute static and dynamic properties that are inaccessi-
ble to ﬁrst-principles calculations [7–9]. In essence, an IP is
a reduced-order model for the quantum-mechanical interaction
of electrons and nuclei in a material through a parameterized
functional form that depends only on the positions of the atomic
nuclei (atoms hereafter).

Development of an IP is a complex iterative process involv-
ing multiple steps as shown in Fig. 1. (Refer back to this ﬁgure
as your read the remainder of this section.) First, a dataset of ex-
perimental and/or ﬁrst principles reference data must be assem-
bled to which the IP will be ﬁtted. When developing machine
learning potentials, it is common practice to split the dataset
into three parts: (1) a training set that is used to optimize the
model parameters, (2) a validation set for ﬁtting hyperparam-

 
 
 
 
 
 
can be resolved because as many training data as needed can be
readily generated.

Construction of a good reference dataset is critical for suc-
cess. The ﬁdelity of the IP for a given application hinges on
including the appropriate physics in the dataset. It is also im-
portant to not swamp out rare conﬁgurations (such as transition
states) that can have a disproportionate eﬀect on material be-
havior. Dataset curation remains a diﬃcult open problem and
an area of active research [11].

Next an appropriate functional form has to be selected. Tra-
ditionally, the functional form of an IP was devised to represent
the physics underlying the material system. One of the earliest
examples is the pair potential developed by Lennard-Jones (LJ)
in the 1920s to model van der Waals interactions in noble gases
6 term (where r is the
[12–14]. The LJ potential includes an r−
distance between atoms) that is based on a theoretical model for
12 term meant to model repulsion
London dispersion, and an r−
due to Pauli exclusion. In the past century, a large number of
physics-based potentials have been developed for a variety of
ionic, metallic, and covalent systems [1]. A physics-based po-
tential typically adopts a closed-form functional expression that
is based on known physical or geometric aspects of bonding in
the material. The functional forms of these IPs have become
increasingly complex with an ever growing number of parame-
ters.3

Devising the appropriate functional form to correctly cap-
ture the physics underlying the material system is arguably the
most diﬃcult task in developing a physics-based potential. It
involves a mix of art and science as pointed out by Brenner
[16]. This is largely alleviated by machine learning potentials
[17–21], which have emerged in recent years and been shown
to be highly eﬀective for a spectrum of material systems rang-
ing from organic molecules [19] to alloys [22]. Diﬀerent from
physics-based potentials, machine learning potentials are con-
structed by ﬁrst transforming the atomic environment informa-
tion in a large training set of ﬁrst-principles results into vector
representations (descriptors) and then training general-purpose
regression algorithms on the atomic environment descriptors.
In a machine learning potential, the regression algorithm con-
tains no physics, but instead it attempt to “learn” the quantum
mechanical Schr¨odinger equation directly from the training set
of reference data. Properly tuned with a suﬃciently dense train-
ing set, machine learning potentials have the advantage that, in
principle, they can describe arbitrary bonding states and thus
can achieve extremely high accuracy.

After the functional form has been selected (either physics-
based or machine learning), the next step is to determine the
values of the function parameters. This is typically formulated
as a least-squares minimization problem by ﬁrst constructing a
loss function that quantiﬁes the diﬀerence between the IP pre-
dictions and the reference values in the training set and then
adjusting the parameters to reduce the loss function as much
as possible. This can be challenging because IPs are nonlinear

Figure 1: Flowchart of the IP development process. Developing an IP involves
four major steps: (1) assemble a set of reference data and design an IP func-
tional form; (2) optimize IP parameters, typically carried out by minimizing a
weighted least-squares loss function of the model predictions and the reference
data; (3) assess the quality of the optimized model via veriﬁcation and valida-
tion tests; and (4) deploy the model to molecular simulation packages. These
steps can be iterative. When a model fails a veriﬁcation test (e.g. by not satisfy-
ing a universal requirement, such as translational and rotational invariance, or
by having a low goodness of ﬁt on a test set) or fails a validation test (e.g. being
unable to reproduce experimental material properties), it is necessary to return
to earlier steps, make adjustments, and redo the ﬁtting.

eters and monitor overﬁtting, and (3) a test set to assess the
goodness of the ﬁt.

Traditionally, the reference dataset contains material prop-
erties considered important for a given application, such as the
cohesive energy, equilibrium lattice constant, and elastic mod-
uli of given crystal phases to name a few. In recent years, many
IPs adopt a force-matching scheme [10], in which the training
set is augmented with the forces on atoms obtained by ﬁrst-
principles calculations for a large set of atomic conﬁgurations.2
An advantage of this approach is that the issue of insuﬃcient
training data (particularly true for machine learning potentials)

2These can be conﬁgurations associated with important structures or snap-
shots of the crystal as the atoms oscillate at ﬁnite temperature or through ran-
dom perturbations.

3For example, there are only two parameters in the LJ potential [14],
whereas the ReaxFF [15] model developed for more complex systems has hun-
dreds of adjustable parameters.

2

StartAssemble datasetOptimize IP parameters:weighted least-squaresDesign IP functional formVeriﬁcation testspassed?Validation testspassed?Failingreason?Fail onuniversalrequirements,parameterdeterminationFail onseparatereference  dataDeploy to molecularsimulation packagesLow  goodness   of ﬁt noyesyesnofunctions that are often “sloppy” in the sense that their predic-
tions are insensitive to certain parameters or certain combina-
tions of parameters [23, 24]. These soft modes in parameter
space can cause the minimization algorithms to fail to con-
verge [9]. A solution is to use a minimization algorithm that
moves along ﬂat regions in parameter space more quickly (e.g.
the geodesic Levenberg–Marquardt algorithm [25–27]), or bet-
ter yet, to identify soft modes using a sensitivity analysis (e.g. a
Fisher information based method [4]) and then apply a suitable
model reduction.

Once an IP is trained, its quality must be assessed. This can
be approached from a veriﬁcation & validation (V&V) perspec-
tive. These terms are deﬁned as [28]:

• Veriﬁcation: The process of determining that a computa-
tional model accurately represents the underlying mathe-
matical model and its solution.

• Validation:4 The process of determining the degree to
which a model is an accurate representation of the real
world from the perspective of the intended uses of the
model.

Veriﬁcation for an IP includes satisfaction of universal require-
ments such as translational and rotational invariance (objectiv-
ity), permutation symmetry, forces returned by the IP corre-
spond to the negative gradient of the energy, and so on. These
are referred to as “veriﬁcation checks” within the Knowledge-
base of Interatomic Models (KIM) framework [29–32]. In ad-
dition, veriﬁcation includes tests that assess the quality of the
model in terms of the uncertainty in parameter determination,
and the goodness of the ﬁt using a test set as mentioned above.
The V&V notion of validation can be understood within the
context of transferability, i.e. the ability of the IP to predict phe-
nomena that it was not ﬁt to reproduce. This includes prediction
of material properties, computed by “KIM Tests” within the
KIM framework [31], and predictions obtained through large-
scale molecular simulations of real-world behavior. For exam-
ple, the ability of IPs for carbon to reproduce the experimental
structure of amorphous carbon [33].

As a general rule, physics-based potentials are better placed
to exhibit transferability than machine learning potentials as
long as the functional forms capture the requisite physics. For
example an LJ potential ﬁtted to the properties of an ideal gas
provides a good approximation (within 10%) for the ground
state crystal structure obtained by cooling the gas down to 0 K
[1]. This is an impressive demonstration of transferability. In
contrast, machine learning potentials have no physics beyond
that in the training set (and possibly the descriptors). This means
that a machine learning potential can only “transfer” to conﬁg-
urations that are close to what already exists in its training set.

Transferability can be included in the IP ﬁtting process through

a comparison of IP predictions with separate reference data.
In cases where this fails, either the functional form needs to

4Note that the term validation is used diﬀerently in the V&V context than

the validation set in machine learning mentioned above.

be extended for a physics-based potential, and/or the training
set needs to be expanded for both physics-based and machine
learning potentials. The training must then be redone.

Finally, once the IP ﬁtting process is complete, the IP must
be deployed to one or more molecular simulation packages of
choice. Traditionally this is done on a code-by-code basis, which
can be a time consuming and error prone process. If the IP class
is already implemented in the code, then simply providing pa-
rameters may be enough — although even there things can go
wrong. For example, for the REBO potential [34] implemented
in LAMMPS [35], some of the parameters were not the ones
presented in the original paper by Brenner et al. [34], but rather
from the closely related AIREBO potential [36].5 In situations
where an IP class is not available in a simulation code, the work
involved in implementing it may be prohibitive. For example
in the amorphous carbon study mentioned above [33] only IPs
implemented in LAMMPS were tested, leaving out more than
half of the possible IPs identiﬁed by the authors. The KIM ap-
plication programming interface (API) [37] was designed to ad-
dress this by creating a standard that allows a conforming IP to
work seamlessly with any simulation code that supports it. The
KIM API is supported by major materials simulation platforms
including ASE [38, 39], DL POLY [40, 41], GULP [42, 43],
LAMMPS [35, 44, 45], and QC [46, 47].

This paper introduces the KIM-based learning-integrated
ﬁtting framework (KLIFF), a package that facilitates the en-
tire IP development process described above. KLIFF provides
a uniﬁed Python interface to train both physics-based and ma-
chine learning potentials, and is constructed in modular fash-
ion, making it easy to use and extend. It integrates closely with
the KIM ecosystem for accessing IPs to train, testing trained
IPs, and deploying trained IPs. The paper is structured as fol-
lows. Section 2 introduces two example IPs (one physics-based
and the other machine learning) that will be trained later, and
discusses the least-squares approach used to parameterize IPs.
Section 3 presents KLIFF features and capabilities. Implemen-
tation details of the code are outlined in Section 4. Section 5
presents a demonstration of using KLIFF to ﬁt the two IPs in-
troduced in Section 2. The paper concludes in Section 6 with a
summary.

2. Interatomic potentials

An IP is typically formulated as a parametric model that
takes the positions of the atoms as its arguments and returns the
potential energy,6

=

V

V

(r1, r2, . . . , rN; θ),

(1)

where r1, r2, . . . , rN are the positions of a system of N atoms,
and θ denotes a set of ﬁtting parameters associated with the IP

5This has been corrected in more recent implementations.
6In general, IPs also depend on the species of the atoms. For notational sim-
plicity, we limit our discussion to systems of a single atomic species. However,
KLIFF supports systems with arbitrary species.

3

The cutoﬀ distance in the SW potential is implicitly deﬁned
as rcut = aσ. This is not ideal from a potential ﬁtting perspec-
tive. When ﬁtting an IP, it is typical to ﬁx the cutoﬀ distance,
and then adjust other parameters to minimize a loss function
(discussed later in Section 2.3). For the standard form of SW,
both a and σ must be ﬁxed to set the cutoﬀ. However, this
adds an unnecessary constraint since two parameters are ﬁxed
instead of just the cutoﬀ. If instead only a or σ are ﬁxed (or nei-
ther), then the cutoﬀ will vary during the ﬁtting process. This
can lead to failure of the optimization due to discontinuity in the
loss function when neighbors enter or leave the cutoﬀ sphere of
an atom. In addition to the cutoﬀ problem, another issue with
the SW form is that (cid:15) is a redundant parameter that only scales
the energy.

To avoid these pitfalls, Eqs. (3) and (4) are recast in a form
in which all parameters are independent and the dependence on
the cutoﬀ radius is made explicit [4]. Let A (cid:66) (cid:15) ˆA, λ (cid:66) (cid:15) ˆλ,
γ (cid:66) σˆγ, and rcut (cid:66) aσ, we have

φ2(ri j) = A

(cid:19)
−

p

(cid:20)

B

(cid:18) ri j
σ

(cid:18) ri j
σ

−

q(cid:21)
(cid:19)
−

exp

×

(cid:33)

,

(cid:32)

σ
ri j −

rcut

(5a)

φ3(ri j, rik, β jik) =λ

cos β0(cid:105)2
(cid:104)
cos β jik −
(cid:32)
γ
exp
ri j −
The new parameters are A, B, p, q, σ, λ, γ along with the cutoﬀ
radius rcut and the equilibrium bond angles β0. The SW model
implemented in KIM [49] takes the form of Eqs. (5a) and (5b)
instead of Eqs. (3) and (4).

γ
rik −

(5b)

rcut

rcut

+

×

(cid:33)

.

2.2. Machine learning potentials

In contrast to physics-based potentials whose functional forms

aim to capture the physics underlying the material system, ma-
chine learning potentials employ general-purpose regression mod-
els that interpolate across a dense training set of ﬁrst principles
energies and forces. Similar to a physics-based potential, a ma-
chine learning model returns the energy of an atom based on
a ﬁnite neighborhood of atoms in its vicinity. Directly using
the positions of an atom and its neighbors as input to the ma-
chine learning potential is ill-advised since this would require
the model to learn the physical invariances of the IP [1, 50], sig-
niﬁcantly increasing the complexity of the model and required
training data. Instead, the atomic environment in terms of po-
sitions is transformed to a suitable “descriptor” vector repre-
sentation that identically satisﬁes all invariances. For example
two atomic environments that diﬀer only by a rigid-body rota-
tion would yield the same descriptor vector. Various descriptors
have been developed to represent atomic environments, includ-
ing the Coulomb matrix [19], symmetry functions [17, 51], bis-
pectrum [18, 20, 50], many-body tensor [52], and others [53].
As an example, we brieﬂy review the symmetry functions ap-
proach, which is one of the earliest and most intuitive repre-
sentations. For a more detailed discussion, see for example
Ref. [54].

4

Figure 2: Bulk silicon crystallizes in a diamond cubic crystal structure in which
each atom has four nearest neighbors forming the sp3 hybridized tetrahedral
structure.

functional form. An IP must be invariant with respect to rigid-
body translation and rotation, inversion of space, and permuta-
tion of chemically equivalent species according to the laws of
physics [1]. These symmetry requirements are typically intrin-
sic to the functional form of the IP. For example, if an IP is
expressed in terms of distances between atoms, it automatically
satisﬁes the requirements of translation, rotation and inversion
invariance.

2.1. Physics-based potentials

The functional form of a physics-based potential is care-
fully devised to model the physics underlying the material sys-
tem. For example, as discussed above, the LJ potential [12–14]
provides a good model for van der Waals interactions in the no-
ble gases, whereas for covalent systems more complex forms
are required, such as bond-order potentials [15, 34]. Here, we
brieﬂy review the three-body Stillinger–Weber (SW) potential
for silicon [48] as an example.
The SW potential energy

of a system consisting of N

atoms has the form,

V

=

V

N(cid:88)

N(cid:88)

i=1

j>i

φ2(ri j) +

N(cid:88)

N(cid:88)

N(cid:88)

i=1

j(cid:44)i

k> j
k(cid:44)i

φ3(ri j, rik, β jik),

(2)

where the two-body interaction takes the form

φ2(ri j) = (cid:15) ˆA

(cid:19)

p

−

(cid:20)

B

(cid:18) ri j
σ

(cid:18) ri j
σ

−

(cid:19)

q(cid:21)
−

exp

×

(cid:32)

1
ri j/σ

(cid:33)

,

a

−

and the three-body term is

φ3(ri j, rik, β jik) =(cid:15) ˆλ

×

(cid:104)
cos β jik −
(cid:32)
ˆγ
exp
ri j/σ

cos β0(cid:105)2

+

ˆγ
rik/σ

(cid:33)

,

a

−

a

−

(3)

(4)

(cid:107)

r j(cid:107)

ri −

in which ri j =
is the bond length between atoms i and j,
β jik is the bond angle formed by bonds i– j and i–k with the ver-
tex at atom i. The parameters are (cid:15), ˆA, B, p, q, σ, a, ˆλ, ˆγ, and β0.
The functional form is based on the lattice structure of bulk sil-
icon shown in Fig. 2. The two-body term (Eq. (3)) models bond
stretching and compression, and the three-body term (Eq. (4))
penalizes conﬁgurations away from the tetrahedral ground state
structure of silicon.

The symmetry functions [17, 51] are comprised of a set of
two-body radial functions and a set of three-body angular func-
tions. Speciﬁcally, the environment of atom i is characterized
by three types of radial functions:

G1

i =

G2

i =

G3

i =

(cid:88)

j(cid:44)i
(cid:88)

j(cid:44)i
(cid:88)

j(cid:44)i

fc(ri j),

e−

α(ri j−

Rs)2

fc(ri j),

cos(κri j) fc(ri j),

(6)

(7)

(8)

and two types of angular functions:

G4

i = 21

−

G5

i = 21

−

ζ (cid:88)

(cid:88)

j(cid:44)i

k> j
k(cid:44)i

ζ (cid:88)

(cid:88)

j(cid:44)i

k> j
k(cid:44)i

(1 + λ cos β jik)ζe−

η(r2

i j+r2

ik+r2

jk) fc(ri j) fc(rik) fc(r jk),

(1 + λ cos β jik)ζe−

η(r2

i j+r2

ik) fc(ri j) fc(rik),

(9)

(10)

where ri j and β jik are distance and angle as deﬁned in Sec-
tion 2.1, and α, Rs, κ, ζ, λ, and η are hyperparameters. The cut-
oﬀ function fc is given by

fc(r) =



1
2

0

(cid:104)

cos

(cid:16) πr
rcut

(cid:17)

(cid:105)
+ 1

rcut
for r
≤
for r > rcut

,

(11)

where rcut is the cutoﬀ distance beyond which atoms do not
contribute to the local environment.

The symmetry functions depend on both distances and an-
gles, however since angles can be expressed in terms of dis-
tances through the law of cosines, the symmetry functions de-
pend entirely on distances and are therefore invariant with re-
spect to translation, rotation, and inversion of space [1]. The
symmetry functions also satisfy the permutation symmetry re-
quirement, because they are constructed by summation over
all bond lengths and bond angles within the cutoﬀ sphere and
changing the summation order does not aﬀect the results. One
can select all the symmetry functions G1
i to describe the
atomic environment or a subset. As an example, we select one
radial function and one angular function, G2
i . The de-
scriptor vector is comprised of distinct G2
i values ob-
α, Rs}
tained for diﬀerent choices of the hyperparameter sets
and
, respectively. The length of the descriptor vec-
tor is then equal to the total number of hyperparameter sets,
NG2
. (See the supplementary material [55] for the hyper-
parameter sets for G2

i and G4
i and G4

λ, ζ, η
}

i used in Section 5.1.)

i and G4

i . . . G5

+ NG4

Many machine learning regression methods are suitable for
constructing IPs including parametric linear regression and neu-
ral network (NN) models, nonparametric kernel ridge regres-
sion and Gaussian process models, and others [53]. Here, we
discuss the NN model. In an NN potential, the total potential
energy of a conﬁguration consisting of N atoms is decomposed
into the contributions of individual atoms

{

{

i

i

=

V

N(cid:88)

i=1

Ei,

(12)

5

Figure 3: Schematic representation of an NN potential to compute the atomic
energy Ei. The NN consists of an input layer, two hidden layers and an output
layer. The local atomic neighborhood information of atom i (all atoms within
a sphere of radius rcut around atom i) is transformed to descriptor vector with
components y j
0 ( j = 1, 2, . . . ) that serves as the input to the NN. Each arrow
connecting two nodes between adjacent NN layers represents a weight. The
fully-connected NN becomes a dropout NN when some connections are cut
(e.g. removing the dashed arrows). Biases and activation function are not shown
in this plot. See text for explanation of the variables.

0, y2

where Ei is the energy of atom i, represented by an NN as shown
in Fig. 3. The NN returns the energy Ei based on the positions
of atom i and its neighbors up to a cutoﬀ distance rcut. The
values y1
0, . . . in the input layer are the components of the
descriptor. Between the input layer and the energy output layer
are so-called “hidden” layers that add complexity to the NN. In
a fully-connected NN, each node in a hidden layer is connected
to all the nodes in the previous layer and in the following layer.
The value of node n in layer m is7

yn
m = h


(cid:88)



n(cid:48)

1wn(cid:48),n

m + bn
m

yn(cid:48)
m
−



 ,

(13)

−

m is the weight that connects node n(cid:48) in layer m

where wn(cid:48),n
1
and node n in layer m, bn
m is the bias applied to node n of layer
m, and h is an activation function (e.g. hyperbolic tangent) that
introduces nonlinearity into the NN. In a more compact way,
Eq. (13) can be written as ym = h(ym
1Wm + bm), where ym is a
−
row vector of the node values in layer m, Wm is a weight matrix,
and bm is a row vector of the biases. For example, y1 and b1 are
row vectors each with 4 elements and W1 is a 5
4 matrix for
the NN shown in Fig. 3. Consequently, the atomic energy Ei
represented in Fig. 3 can be expressed as

×

Ei = h[h[y0W1 + b1]W2 + b2]W3 + b3.

(14)

The weights and biases are the ﬁtting parameters in an NN po-
W1, W2, . . . , WL, b1, b2, . . . , bL}
tential: θ =
, where L is the
{
number of layers (hidden and output).

2.3. Parameterization

Once an IP functional form is selected, the parameters must
be determined. This is typically framed as a least-squares min-
imization problem where the IP parameters are adjusted to best

7The input layer and the output layer are indexed as the zeroth layer and

third layer, respectively.

7Input  layerHidden  layer 1Hidden  layer 2Output  layerLocal atomic neighborhoody11y22y10y20y30y40y50y21y31y41y42y32y12w5,41w4,42Ei<latexit sha1_base64="4nYcpO389AuWIMlKSzee+RNysa8=">AAACB3icbVDLSgMxFM3UV62vqks3wVJwVWaqYJcFEVxWtA9oh5LJZNrQPIYkI5ShH+DCrX6GO3HrZ/gV/oKZdha29UDgcM69nJsTxIxq47rfTmFjc2t7p7hb2ts/ODwqH590tEwUJm0smVS9AGnCqCBtQw0jvVgRxANGusHkJvO7T0RpKsWjmcbE52gkaEQxMlZ6uB3SYbni1tw54DrxclIBOVrD8s8glDjhRBjMkNZ9z42NnyJlKGZkVhokmsQIT9CI9C0ViBPtp/NTZ7BqlRBGUtknDJyrfzdSxLWe8sBOcmTGetXLxP+8fmKihp9SESeGCLwIihIGjYTZv2FIFcGGTS1BWFF7K8RjpBA2tp2llIDPIKzCQLIQZjkl25C32sc66dRr3mWtfn9VaTbyrorgDJyDC+CBa9AEd6AF2gCDEXgBr+DNeXbenQ/nczFacPKdU7AE5+sXFGCYgw==</latexit>i<latexit sha1_base64="kVYamhsggBIxwqlFv9ODNCys43c=">AAACBnicbVDLSgMxFL1TX7W+qi7dBEvBVZmpgl0W3LisYh/QDiWTZtrQJDMkGaEM3btwq5/hTtz6G36Fv2CmnYW2HggczrmXc3OCmDNtXPfLKWxsbm3vFHdLe/sHh0fl45OOjhJFaJtEPFK9AGvKmaRtwwynvVhRLAJOu8H0JvO7j1RpFskHM4upL/BYspARbKx0z0rDcsWtuQugdeLlpAI5WsPy92AUkURQaQjHWvc9NzZ+ipVhhNN5aZBoGmMyxWPat1RiQbWfLi6do6pVRiiMlH3SoIX6eyPFQuuZCOykwGaiV71M/M/rJyZs+CmTcWKoJMugMOHIRCj7NhoxRYnhM0swUczeisgEK0yMLedPSiDmCFVREPERynKyhrzVPtZJp17zLmv1u6tKs5F3VYQzOIcL8OAamnALLWgDgRCe4QVenSfnzXl3PpajBSffOYU/cD5/APPll98=</latexit>rcut<latexit sha1_base64="oPlakOaa8agB76Uk+hUvk5DOH2w=">AAACEHicbVDLSsNAFJ3UV62vqks3g6XgqiRVsMuCG5cV7AOaWCaTSTt08mDmRiwhP+HCrX6GO3HrH/gV/oKTNgvbeuDC4Zx7OZfjxoIrMM1vo7SxubW9U96t7O0fHB5Vj096KkokZV0aiUgOXKKY4CHrAgfBBrFkJHAF67vTm9zvPzKpeBTewyxmTkDGIfc5JaAlWz7YwJ4gpQlko2rNbJhz4HViFaSGCnRG1R/bi2gSsBCoIEoNLTMGJyUSOBUsq9iJYjGhUzJmQ01DEjDlpPOfM1zXiof9SOoJAc/VvxcpCZSaBa7eDAhM1KqXi/95wwT8lpPyME6AhXQR5CcCQ4TzArDHJaMgZpoQKrn+FdMJkYSCrmkpxQ0yjOvYjYSH85yKbsha7WOd9JoN67LRvLuqtVtFV2V0hs7RBbLQNWqjW9RBXURRjF7QK3ozno1348P4XKyWjOLmFC3B+PoFQkSdBQ==</latexit>match a training set of reference data obtained from experi-
ments and/or ﬁrst-principles calculations. For a training set of
M conﬁgurations, the diﬀerence between the predictions of the
IP and the reference data is quantiﬁed by a loss function deﬁned
as

(cid:105)2

M(cid:88)

1
2

M(cid:88)

(cid:104)

1
2

∈

−

−

−

+

L

m=1

m=1

wf

m(cid:107)

ˆEm

we
m

(θ) =

|Rm ∈

f (Rm; θ)

(∂E/∂R)

E(Rm; θ)

m and wf

R and f (Rm; θ) =

ˆfm(cid:107)
2,
(15)
R3Nm
where E(Rm; θ)
are the energy and forces in conﬁguration m obtained from
an IP, ˆEm and ˆfm are the corresponding reference energy and
R3Nm
forces for conﬁguration m in the training set, with Rm ∈
the concatenated coordinates of all atoms in conﬁguration m
and Nm the number of atoms in conﬁguration m. The weights
we
m are typically chosen to be inversely proportional to
(Nm)2, so that each conﬁguration has an equal contribution to
(θ). This prevents conﬁgurations with more
the loss function
atoms from dominating the optimization. For energy in units
of eV and forces in units of eV/Å, these weights have units of
2, respectively. Here, we only use energy and
eV−
forces to construct the loss function, but in principle one can ﬁt
any physical property, such as the equilibrium lattice constants
and elastic moduli of a ground state crystal structure. The ob-
jective then is to minimize the loss function in Eq. (15) with
respect to θ to obtain the optimal set of IP parameters.

2 and (eV/Å)−

L

Simply minimizing Eq. (15) can lead to overﬁtting and thus
low transferability of an IP. This is especially true for machine
learning IPs due to the lack of physics in their functional forms
and the large parameter space. Various techniques have been
proposed to overcome this problem. One approach is to add
regularization terms to the loss function to prevent overly com-
2 can be
θ
plex results, for example an L2 term of the form λ
(cid:107)
(cid:107)
added, where λ is a hyperparameter that determines the regular-
ization weight. Another widely used approach is early stopping
[56], where model performance is monitored on a validation
set and ﬁtting is terminated when accuracy begins to degrade.
There are also regularization techniques that are speciﬁc to cer-
tain types of models. For example, the dropout method [57, 58]
can be applied to NN potentials (see Section 3.2 for more on
dropout).

3. Features and capabilities of KLIFF

A variety of software packages have been developed to de-
velop IPs, including potﬁt [59, 60], ænet [61], Amp [62], aPIP
[63], atomicrex [64], DeePMD-kit [65], GAP [18, 66], MAISE
[67], MLIP [68], PACE [69], PANNA [70], PyXtal FF [71],
RuNNer [17, 72], SIMPLE-NN [73], among others. KLIFF
shares many features with these packages, but is also distin-
guished by some capabilities described in this section that ad-
dress the problems discussed in Section 1.

3.1. Integration with KIM

As indicated by the name, KLIFF is deeply integrated with
the KIM ecosystem. (We note that the Potﬁt IP ﬁtting frame-
work is also compatible with KIM [9].)

6

First, KLIFF supports the training of IPs archived within the
OpenKIM repository. An IP is called a model in KIM nomen-
clature, and a KIM portable model is an independent computer
implementation of an IP that conforms to the KIM API portable
model interface (PMI) standard.8 In practice portable models
consist of a “model driver,” which implements an IP class (e.g.
the embedded atom method (EAM) form) and a parameter set
for a speciﬁc set of species. All content in the OpenKIM reposi-
tory is archived subject to strict versioning and provenance con-
trol with digital object identiﬁers (DOIs) assigned. This makes
it possible to access the exact IP used in a publication at a later
date to reproduce the calculations or to conduct further ﬁtting.
A large number of physics-based and machine learning IPs are
implemented as portable models and archived in the OpenKIM
repository. These models are subjected to an editorial review
process by the KIM Editor on acceptance to ensure quality con-
trol. Users of KLIFF can employ these models directly without
having to implement them with signiﬁcant savings in time and
potential errors.

Second, IPs trained with KLIFF can be easily tested via
OpenKIM. KLIFF can automatically generate models that are
compatible with the KIM API, thus allowing a trained IP to run
against KIM veriﬁcation checks (VCs) and KIM tests [31]. As
noted in Section 1, KIM VCs are programs that explore the in-
tegrity of an IP implementation. They check for programming
errors (e.g. memory leak [74]), failures to satisfy required be-
haviors (e.g. inversion [75] and permutation [76] symmetries),
and determine general characteristics of the IP functional form
(e.g. are the forces returned by the model consistent with those
obtained through numerical diﬀerentiation of the energy [77]).
As opposed to KIM VCs, KIM tests check the accuracy of an
IP by computing a variety of physical properties of interest to
researchers, such as the stacking fault energy [78], elastic mod-
uli [79], and linear thermal expansion coeﬃcient [80] to name a
few. The information provided by KIM VCs and KIM tests can
save researchers a great deal of time by identifying limitations
of an IP that can lead to subtle problems in simulations (e.g.
poor convergence during energy minimization due to incorrect
or discontinuous forces), and assisting in the selection of IPs by
considering its predictions for relevant physical properties.

Third, IPs trained with KLIFF can be deployed via KIM.
Traditionally, most IP development papers only report the func-
tional form of the IPs and the associated parameters, without
mentioning or providing a computer implementation. Recently
developed machine learning potentials typically do provide com-
puter implementations, but these are often standalone codes that
cannot be used in major molecular simulation packages. This
creates a signiﬁcant barrier for the universal usability of IPs.
By providing portable implementations, KIM addresses this is-
sue, as well as enabling reproducibility.9 As mentioned above,

8KIM also supports a second type of model called a simulator model. While
a portable model will work seamlessly with any simulation package that sup-
ports the KIM API/PMI standard, a simulator model only speciﬁes how to setup
and run a model that is implemented as an integrated part of a speciﬁc simula-
tion package. KLIFF supports the ﬁtting of portable models.

9In some cases the same parameter ﬁle can lead to diﬀerent results when

KLIFF can automatically create IP models that are compati-
ble with the KIM API, which enables the IP to work seam-
lessly with any KIM-compliant simulation package including
ASE [38, 39], DL POLY [40, 41], GULP [42, 43], LAMMPS
[35, 44, 45], and QC [46, 47]. The ﬁnal production IP can also
be contributed to the OpenKIM repository for deployment as
source and binary packages for major Mac, Linux and Windows
platforms.

3.2. Uncertainty analysis

Historically, molecular simulation with IPs has been pri-
marily viewed as a tool limited to providing qualitative insight.
A key reason is that such simulations include multiple sources
of uncertainty that are diﬃcult to quantify, with the result that
predictions obtained from the simulation typically lack robust
conﬁdence intervals [81]. A major source of uncertainty orig-
inates from the IPs themselves, since these are empirical func-
tional forms ﬁtted to experimental results and/or ﬁrst-principles
calculations. To make molecular simulation with IPs more reli-
able, it is imperative to quantify the intrinsic uncertainty of the
IP and propagate it to the simulation results. This is an area that
has not received much attention in the past. To address this lim-
itation, KLIFF provides functionality that enables uncertainty
analysis of IPs.

As mentioned in Section 1, IPs are often “sloppy” [4, 23,
24] in that their predictions are insensitive to certain combina-
tions of the parameters. This behavior can be quantiﬁed us-
ing the notion of a Fisher information matrix (FIM). The FIM
provides a measure for the information in the training set on
the parameters, which leads to an estimate for the precision
with which the parameters can be determined [4]. For exam-
ple, for the case where the loss function contains only forces
(i.e. we
m = 0 in Eq. (15)), the FIM can be written as [4, 24]:

F(θ)

(cid:32)

M(cid:88)

∝

m=1

∂ fm
∂θ

(cid:33)T (cid:32)

(cid:33)

,

∂ fm
∂θ

(16)

R3Nm are the forces on atoms of conﬁguration m
where fm ∈
(Nm is the number of atoms in conﬁguration m), M is the num-
ber of conﬁgurations in the training set, and the superscript T
denotes matrix transpose. The diagonal elements of the inverse
FIM provide lower bounds on the variance of the parameters,
known as the Cram´er–Rao bound [82],
1(cid:17)

(cid:16)

(17)

Var[θi]

F−

.

ii

≥

The larger a diagonal element of the inverse FIM, the larger
the lower bound on the variance for the corresponding param-
eter, indicating that the parameter is less well determined. As
an illustrative example, we plot in Fig. 4 a schematic represen-
tation of the contours of the cost function in Eq. (15) for an IP
with two parameters. Here, the two diagonal components of the

Figure 4: Schematic representation of the cost contours in the vicinity of the
optimal parameters θ∗ of an IP with two parameters θ1 and θ2. The aspect
ratio of the contours is determined by the eigenvalues λ1 and λ2 of the FIM.
1)22 provide a lower
The diagonal elements of the inverse FIM (F−
bound on the variance of the parameters θ1 and θ2, respectively.

1)11 and (F−

1)11 and (F−

1)22 are nearly of the same magni-
inverse FIM (F−
tude, indicating that the two parameters θ1 and θ2 are equally
determined in the ﬁtting. If this were not the case and a diag-
onal component of the inverse FIM was much larger (an order
of magnitude or more) than the others, then the IP parameter
associated with this component is poorly determined. To ad-
dress this, one could attempt to modify the IP functional form
as discussed in Section 1 and shown in Fig. 1. The FIM also
provides an upper bound on the uncertainty in a physical quan-
tity of interest (QOI) due to small variations in IP parameters.
A detailed discussion of such an analysis for the thickness of
monolayer MoS2 can be found in Ref. [4]. The FIM in Eq. (16)
is implemented in KLIFF as an Analyzer (discussed in Sec-
tion 4) using numerical diﬀerentiation.

The FIM analysis is well suited for physics-based poten-
tials, which have dozens of parameters and each parameter plays
a vital role. However, machine learning potentials are typically
over-parameterized and the inﬂuence of a single parameter on
the model performance is not large. Instead of parameter un-
certainty, it is more important and useful to analyze the predic-
tion uncertainty of a QOI (e.g. elastic moduli). A simple yet
powerful approach to obtaining the QOI uncertainty is to con-
struct an ensemble of IPs instead of a single best ﬁt model. This
can be done by either training diﬀerent IPs using diﬀerent ini-
tial guesses for the parameters or using diﬀerent subsets of the
training data. At the prediction stage, each individual model in
the ensemble is applied to compute the QOI P. The average

¯P =

1
NP

NP(cid:88)

i=1

Pi

(18)

used with diﬀerent implementations of an IP, either in newer versions of the
same code, or in diﬀerent simulation packages. For example, see Ref. [8] for a
discussion of this eﬀect for tabulated EAM potentials.

is then used as the predictive mean for the QOI, and the standard

7

deviation

(cid:118)(cid:117)(cid:116)

NP(cid:88)

1

i=1

¯P)2

(19)

(Pi −

Std[P] =

1
NP −
as the uncertainty. The ensemble approach can be applied to
any type of model, either physics-based [83, 84] or machine
learning potentials [85–88]. Although straightforward to train,
it is computationally expensive since multiple models have to
be trained to form the ensemble. For NN potentials, there is
an alternative that is computationally less costly and performs
equally well to the ensemble approach [89]. By removing some
connections between layers (e.g. removing the dashed arrows
for the NN shown in Fig. 3), a fully-connected NN is changed
into a dropout NN [57, 58]. It has been shown that training an
NN with dropout (i.e. dropping diﬀerent connections at each
training step) approximates a Bayesian NN [90, 91]. Conse-
quently, a dropout NN possesses all the properties of a prob-
abilistic Bayesian model, from which uncertainty information
can be extracted. For dropout NN potentials [89], only one
model needs to be trained at the training stage. At the predic-
tion stage, it is essentially an ensemble model and can be used
in a similar fashion: conduct multiple stochastic forward passes
through the dropout NN (each time drop diﬀerent connections)
to obtain multiple samples of the QOI and then compute the av-
erage and standard deviation. KLIFF supports the training of
both ensemble and dropout NN potentials. The associated KIM
DUNN model driver [92] allows molecular simulation codes
to work with individual members in the ensemble and perform
uncertainty quantiﬁcation.

3.3. A wide range of support

By conforming to the KIM API, KLIFF supports a wide
range of IPs available through OpenKIM. At the time of this
writing, the OpenKIM repository contains 35 model drivers, in-
cluding widely used physics-based potentials such as Stillinger–
Weber (SW) [48, 49], Tersoﬀ [93–96], EDIP [97–100], and
EAM [101–103] potentials among others. For machine learn-
ing potentials, KLIFF currently supports the symmetry func-
tions [17, 51] and bispectrum [18, 50] atomic environment de-
scriptors.
Interfacing with other descriptor libraries, such as
DScribe [104], is being explored. For machine learning regres-
sion algorithms, KLIFF has its own implementation of simple
algorithms (e.g. linear regression) and takes advantage of Py-
Torch [105] to build and train NN potentials. The NN model
in KLIFF wraps PyTorch so that the user interface appears the
same as other models in KLIFF, but still retains the ﬂexibility
of PyTorch to create customizable NN structures and train with
state-of-the-art deep learning techniques available through this
package.

KLIFF provides an interface to many widely-used mini-
mization algorithms for model training. As discussed in Sec-
tion 2.3, the IP parameters are obtained by minimizing a loss
function that quantiﬁes the diﬀerence between IP predictions
and the training set. The optimizer directly determines the val-
ues of the parameters and thus the quality of the IP. It is im-
possible to make a general statement about which optimizer is

8

best, since this is problem-dependent, but some optimizers (e.g.
the L-BFGS-B algorithm [106]) tend to work well for a wide
range of problems. KLIFF supports the optimization algorithms
in SciPy [107] and PyTorch [105]. The minimize module of
scipy.optimize provides a large number of general-purpose
minimization algorithms, and the least_squares module of
scipy.optimize provides algorithms speciﬁc for nonlinear
least-squares minimization problems. The optimizers in Py-
Torch are targeted for training NN models, including the stochas-
tic gradient descent (SGD) method [108, 109] and its variants
such as the Adam method [110]. In addition, KLIFF also sup-
ports the geodesic Levenberg–Marquardt (LM) algorithm [25–
27], which has been shown to work well for “sloppy” IPs whose
predictions are insensitive to certain parameters or certain com-
binations of parameters.

3.4. Uniformity, modularity, and extensibility

KLIFF is designed to be as uniform, modular, and exten-
sible as possible.
It is implemented using an object-oriented
programming (OOP) paradigm and provides a pure Python user
interface. All the atomic environment descriptors, models, cal-
culators, analyzers, etc. are subclassed from individual super-
classes. A subclass only provides or modiﬁes speciﬁc imple-
mentations of superclass methods when necessary, guarantee-
ing a uniform interface across subclasses. As mentioned in Sec-
tion 3.3, KLIFF takes advantage of the optimization algorithms
in SciPy [107] and PyTorch [105], as well as the geodesic LM
algorithm to train models when minimization of a loss function
is needed. Although vanilla SciPy, PyTorch, and LM have dif-
ferent APIs to call the optimization algorithms, KLIFF provides
a uniﬁed interface that wraps them under the hood.

Extending KLIFF is straightforward. New descriptors, mod-
els, calculators, loss functions, optimization algorithms, analyz-
ers, etc. can be seamlessly added to existing modules in KLIFF.
For example, a new physics-based potential can be easily im-
plemented by subclassing the KLIFF “Model” class, specifying
the IP parameters, and then using Python to code the functional
form of the IP. As a concrete example, we provide a Python
code demonstrating how to implement the Lennard-Jones po-
tential in the supplementary material [55]. Other parts such as
periodic boundary conditions handling are dealt with by KLIFF.
The newly created model can then be used for training with any
loss function and optimization algorithms that are available in
KLIFF. To gain the beneﬁts of KIM models discussed in Sec-
tion 3.1, it is currently necessary to implement the IP as a sepa-
rate code conforming to the KIM API. Future plans include the
development of a general KIM model driver that will directly
work with KLIFF IPs stored in a portable format.

3.5. Data parallelization

Computationally intensive KLIFF components, such as neigh-

bor list generation and descriptor calculation, are internally im-
plemented in C++. Even with this, the computational require-
ments can become quite demanding as the size of the train-
ing set increases. Fortunately, evaluation of the loss function
Eq. (15) can be easily divided into independent sub-problems

Figure 5: Data parallelization scheme used by KLIFF. S is the number of con-
ﬁgurations assigned to each process, and M is the total number of conﬁgura-
tions.

allowing for easy parallelization. KLIFF adopts the paralleliza-
tion over data scheme illustrated in Fig. 5. Atomic conﬁgura-
tions in the dataset are distributed to diﬀerent processes. Each
process computes the sub-loss according to Eq. (15) for the con-
ﬁgurations assigned to it, and the total loss is then obtained as
the sum of the sub-losses from all the processes. KLIFF sup-
ports both OpenMP-style parallelism for shared-memory archi-
tectures, and MPI-style parallelism typical of high-performance
computing clusters composed of multiple standalone machines
connected by a network.

4. Implementation details: the KLIFF code

KLIFF is written primarily in Python with several compu-
tationally intensive components implemented in C++ accessi-
ble via Python bindings. As such, users interact with KLIFF
through a pure Python interface. KLIFF is built in a mod-
ular fashion, as discussed in Section 3.4, with key modules
Dataset, Model, Calculator, Loss, Optimizer, and Analyzer.
A ﬂowchart showing the interaction and information transfer
between these modules for IP training is displayed in Fig. 6.
The modules are described below.

4.1. Dataset module

A dataset is comprised of a set of atomic conﬁgurations,
which provide the training data to optimize IP parameters or
provide the test data to test the quality of an IP. An atomic con-
ﬁguration includes three vectors deﬁning the simulation cell,
ﬂags to indicate whether periodic boundary conditions (PBCs)
are applied along the cell vectors, the species and coordinates
of all atoms in the conﬁguration, and reference outputs. KLIFF
reads atomic conﬁgurations from extended XYZ ﬁles, with each
conﬁguration stored in a separate ﬁle. The reference outputs
(energy, forces, and stress) associated with an atomic conﬁgu-
ration are also read in from the extended XYZ ﬁle. The standard
XYZ format only stores the number of atoms in a conﬁguration
and the species and coordinates of the atoms. The extended
XYZ format allows for additional information to be stored, ei-
ther in the second line via a series of key=value pairs (e.g.

9

Figure 6: Flowchart of the procedures of using KLIFF to train an IP.

PBC="T,T,T" and energy=1.2) or in the body section by ap-
pending values (e.g. forces) to the coordinates. Internally, each
atomic conﬁguration and the reference outputs are associated
with a Configuration object and a Dataset is essentially a
set of Conﬁguration objects.

4.2. Model module

The ﬁtting process begins with the instantiation of a model
(IP). Depending on the nature of the model, diﬀerent operations
can be applied. For KIM models and physics-based KLIFF po-
tentials, KLIFF can provide information on what parameters
are available for ﬁtting, together with a description of each pa-
rameter and the data structure and data type of each parameter.
Based on this information, a user can select the parameters to
ﬁt and specify initial values or use defaults. Lower and upper
bounds on parameter values can also be provided to restrict it
to a range. For an NN model, the descriptor representation of
an atomic environment, which serves as the input to the NN
model, must be deﬁned. Then the NN can be constructed using
an arbitrary number of layers, nodes per layer, and activation
functions. Unlike physics-based models, KLIFF automatically
initializes the parameters in the network. For example, the He
initializer [111] is used to initialize the weights and biases in
Eq. (13). Other default choices are made by KLIFF based on
the authors’ physical understanding and experience to make it
easier for users to develop machine learning potentials with-
out having to master subtle aspects of machine learning train-
ing. For example, in a standard dropout approach, diﬀerent NN
connections would be removed for each atom in a conﬁguration
(see Section 3.2). However, KLIFF defaults to a native dropout
scheme that removes the same NN connections for all atoms

LossDataset…Process 1: conﬁgurations 1, 2, …, SProcess 2: conﬁgurationsS+1, S+2, …, 2SProcess P-1: conﬁgurations(P-2)S+1, (P-2)S+2, …, (P-1)SProcess P: conﬁgurations (P-1)S+1, (P-1)S+2, …, Mupdated parametersenergy, forces, stress, etc.initial parameterswrite model with optimized parameterslattice vectors, atomic species, coordinates, etc. reference energy, forces, stress, etc.StartOptimizerModelLossCalculatorDatasetAnalyzernoyesconverged?in a conﬁguration. This ensures that atoms with identical en-
vironments (e.g. all atoms in an ideal silicon crystal) will have
the same atomic energy, forces, and other properties. Users can
overwrite default choices, for example, by selecting the native
PyTorch dropout instead of KLIFF’s native implementation.

4.3. Calculator module

The created model is attached to a calculator that computes
the predictions corresponding to the reference outputs for the
atomic conﬁgurations in the training set. The native KLIFF cal-
culator supports the evaluation of energy, forces, and stress. If a
property other than these is to be ﬁtted, a new calculator needs
to be implemented. A new calculator can wrap any KIM com-
pliant molecular simulation package to compute the property
with the given model in a similar fashion to ASE calculators
[38, 39].

4.4. Loss module

The predictions computed by the calculator and the corre-
sponding reference output values stored in the training set are
then used to construct a loss function (e.g. Eq. (15)) that quanti-
ﬁes the diﬀerence between the model predictions and the refer-
ences. A weight can be assigned to each conﬁguration, so that
“important” conﬁgurations are emphasized more during opti-
mization. If the available loss functions in KLIFF do not satisfy
a speciﬁc need, a user-deﬁned loss function can be added.

4.5. Optimizer module

The optimization process involves minimization of the loss
function with respect to the IP parameters until speciﬁed stop-
ping criteria are satisﬁed, such as reducing the loss function
value below a tolerance or reaching a maximum allowed num-
ber of minimization steps.

The optimizers supported by KLIFF can be broadly catego-
rized in two classes: batch optimizers and mini-batch optimiz-
ers. The former (e.g. the L-BFGS-B and geodesic LM meth-
ods) typically require the evaluation of the entire training set at
each minimization step, whereas the latter (e.g. the SGD and
Adam methods) only use a subset of the training set at a time.
Batch optimizers guarantee a monotonic decrease of the loss
throughout the minimization process and typically yield smaller
ﬁnal loss values compared with mini-batch optimizers. Mini-
batch optimizers become advantageous for very large training
sets (typical of machine learning potentials) where evaluation
of the entire training set becomes prohibitive due to memory
and/or computing constraints. For NN models that contain a
large number of parameters, SGD-based optimizers can typi-
cally ﬁnd a reasonable solution in parameter space that mini-
mizes the loss to a certain level. By default, KLIFF uses an
L-BFGS-B optimizer for physics-based potentials, which typ-
ically have relatively small numbers of parameters and small
training sets, and an SGD-based Adam optimizer for NN po-
tentials, which have many parameters and very large training
sets. The user can overwrite this default and select a preferred
optimizer.

Once the optimization is completed, the ﬁtted IP can be
written out as a KIM model that conforms to the KIM API,
which can then be run against KIM VCs and KIM tests or be
used with any KIM-compliant simulations codes as discussed
in Section 3.1. Generated KIM models, can be uploaded to
https://openkim.org to receive a DOI and make the model
available to the broader research community. Also, the model
can be attached to an Analyzer to carry out post-processing
analysis, such as computing the FIM discussed in Section 3.2
and computing the root mean square errors of energy and forces
for a test set.

4.6. Command line tool

KLIFF provides a command line tool called kliff that fa-
cilitates the execution of many common tasks. For example,
query a physics-based potential for available parameters that
can be optimized and their associated metadata, print a synopsis
of the atomic conﬁgurations in the dataset, or split a dataset into
multiple subsets. Once installed, executing “kliff --help”
in the terminal will list the commands, their arguments, and
help information.

5. Demonstration

KLIFF has been extensively tested through the development
of multiple IPs, including an SW potential for two-dimensional
molybdenum disulﬁde [4], an interlayer potential for multilayer
graphene [112], a hybrid NN potential for multilayer graphene
[5], and a dropout uncertainty NN potential (DUNN) to quan-
tify uncertainty in molecular simulations [89]. In this section
we present examples demonstrating the use of KLIFF in train-
ing an SW potential and an NN potential for silicon. The func-
tional forms of the two IPs are described in Section 2.

5.1. Parameterization

The training set is comprised of the energies and forces
for 2513 conﬁgurations of silicon in the diamond cubic crys-
tal structure. This includes conﬁgurations with compressed and
stretched cells and random perturbations of atoms, as well as
conﬁgurations drawn from a molecular dynamics trajectory at
a temperature of 300 K. Since this is only a demonstration, in-
stead of using ﬁrst-principles calculation or experimental data,
the conﬁgurations were generated using the EDIP model [97–
100]. The dataset is provided in the supplementary material
[55].

The SW potential has seven parameters, A, B, p, q, σ, λ, γ,
along with the cutoﬀ radius rcut and the equilibrium angle β0.
The cutoﬀ radius is set to rcut = 3.77118 Å, as used by Stillinger
and Weber [48], and the equilibrium angle is set to the tetrahe-
dral angle of the ideal cubic diamond structure, β0 = 109.47◦.
Following most SW parameterizations [4, 48, 113], the param-
eters p and q are set to 4 and 0, respectively. The values of
the remaining parameters are obtained by minimizing the loss
function in Eq. (15) using the geodesic LM algorithm [25–27].
m = 1/(Nm)2 and
The energy and force weights are set to we

10

Table 1: Summary of SW parameters obtained by minimizing the loss function
and the preset parameters.

Table 2: Summary of parameters in the NN potential and hyperparameters that
deﬁne the NN structure.

number of hidden layers
number of nodes in hidden layers
cutoﬀ rcut
activation function h
descriptor hyperparameters

3
10
3.5 Å
tanh
see [55]

Parameter
A
p
σ
γ
β0

Value
15.46588611 eV
4
2.05971554 Å
2.71009995 Å
109.47◦

Parameter
B
q
λ
rcut

Value
0.61032816
0
65.46736831 eV
3.77118 Å

wf
m = 10/(Nm)2. A larger force weight is used to better re-
produce the phonon dispersions discussed in Section 5.2. One
m = 10/(Nm)2
exception is that the energy weight is set to we
for conﬁgurations that have an ideal cubic diamond structure at
diﬀerent lattice parameters. The increased weight ensures that
these conﬁgurations are not underrepresented in the ﬁtting since
their force terms in Eq. (15) are identically zero (regardless of
the IP parameters) due to the symmetry of the underlying struc-
ture. The optimal parameter set identiﬁed by this process and
the preset parameters are listed in Table 1.

i and G5

For the NN potential, we employ the G2

i symme-
try functions (Eqs. (7) and (10)) as the descriptors for char-
acterizing atomic environments. The hyperparameters α and
Rs in Eq. (7) and ζ, λ, and η in Eq. (10) are provided in the
supplementary material [55]. The cutoﬀ in Eq. (11) is set to
rcut = 3.5 Å to include only nearest-neighbor interactions. A
challenging aspect of training an NN, which is also a source
of the power and ﬂexibility of the method, is that it is up to
the developer to select the number of descriptor terms to re-
tain, the number of hidden layers, the number of nodes within
each hidden layer (which need not be the same), and the activa-
tion function. It is also possible to create diﬀerent connectivity
scenarios between layers. Here we have opted for simplicity
and adopted a fully-connected network with the same number
of nodes in each hidden layer. The number of hidden layers
and the number of nodes in each hidden layers are determined
through a grid search and are listed in Table 2. The activation
function h is taken to be the commonly used hyperbolic tangent
function, tanh(x) = (ex

x)/(ex + e−
The NN potential parameters are obtained by minimizing
the loss function Eq. (15). The energy weight we
m and forces
weight wf
m are the same as those used for the SW potential. The
minimization is carried out using the Adam optimizer [110]
with a learning rate of 0.001. As discussed in Section 4, to
accelerate the training process a mini-batch technique [114] is
employed with a batch size of 100 conﬁgurations at each mini-
mization step for a total of 2000 epochs.10

x).

e−

−

Figure 7: Total potential energy of silicon as a function of the lattice parameter
predicted by the trained SW and NN potentials along with the EDIP reference
data.

mond cubic crystal structure. As discussed in Section 3.1, IPs
trained by KLIFF can be exported in a form compatible with
the KIM API, which allows them to be used directly with a vari-
ety of major molecular simulation packages, such as LAMMPS
[35, 44, 45]. The tests described in this section were carried out
using LAMMPS.

First, we investigate the cohesive energy versus lattice pa-
rameter for ideal cubic diamond silicon (see structure in Fig. 2).
The ﬁtted SW and NN potentials are compared with the EDIP
reference data in Fig. 7. Both potentials reproduce the equilib-
rium state well as seen in Table 3, however the NN potential
with its ﬂexible functional form is able to follow the reference
data more closely across most of the range except for lattice pa-
rameters smaller than 5 Å and larger than 5.9 Å. The training
10%
set contains conﬁgurations with lattice parameters up to
from the equilibrium value (i.e. 4.89
5.97 Å). Thus con-
ﬁgurations with lattice parameters smaller than 5 Å and larger
than 5.9 Å are at the “edge” of the training data where accu-
racy of the NN potential is clearly reduced. This is consistent

∼

±

The scripts used to train the SW and NN potentials are pro-

vided in the supplementary material [55].

5.2. Testing the trained potentials

To test the ﬁtted SW and NN potentials, we applied them to
study energetic and vibrational properties of silicon in the dia-

10An epoch is one complete pass over the dataset. For example, if a dataset
includes 50 conﬁgurations and a mini-batch size of 10 conﬁguration is used,
then one epoch consists of 5 minimization steps.

11

Table 3: Cohesive energy (absolute value of the minimum of the energy versus
lattice parameter curve) and equilibrium lattice constant for the diamond struc-
ture computed using the EDIP potential (taken as the reference) and the SW
and NN potentials (with errors relative to EDIP given in parentheses).
Ecoh [eV/atom]
4.650
4.647 (0.06%)
4.645 (0.1%)

a0 [Å]
5.43
5.39 (0.74%)
5.42 (0.18%)

Potential
EDIP
SW
NN

4.85.05.25.45.65.86.0Latticeparametera(˚A)−4.7−4.6−4.5−4.4−4.3−4.2−4.1−4.0EnergyE(eV/atom)ReferenceSWNNFigure 8: Energy of silicon as a function of the lattice parameter predicted by
the DUNN potential.
(a) Predictive mean and uncertainty of the energy by
DUNN, where the uncertainty band is twice the width of the standard deviation
in the energy. Also plotted are the reference EDIP energies. (b) DUNN uncer-
tainty. The uncertainty band is the same as that in panel (a) except that here it
is centered around 0 instead of the prediction mean in panel (a).

with the discussion in Section 1. While highly accurate within
the training set, the NN potential has low transferability and
thus its ability to extrapolate beyond its training set is limited.
This is particularly clear on the compressive end of the response
(lattice constant smaller than 5.0 Å). In contrast, the SW poten-
tial has a lower accuracy overall since it is constrained by its
physical functional form, but this leads to a more correct trend
outside the training set.

It is important to quantify the uncertainty in the predictions
of machine learning potentials given their low transferability.
As discussed in Section 3.2, KLIFF supports the training of
DUNN potentials [89] that are based on dropout uncertainty
estimation. To demonstrate this, we train a DUNN potential
for the silicon dataset and apply it to investigate the same en-
ergy versus lattice parameter problem discussed above. Since
the emphasis is on the uncertainty in energy, forces are not used
in the training. (Details of the parameterization procedure are
provided in the supplementary material [55].) When a DUNN
model is used it provides a mean value, which is the average
over the dropout ensemble, and an associated uncertainty esti-
mate. The results for the cohesive energy versus lattice param-
eter are compared with the EDIP reference data in Fig. 8(a)).
The mean DUNN values are in excellent agreement with the
reference data.11 More importantly, the band around the mean

11The agreement is better than the NN potential in Fig. 2 since only ener-
gies are used in training the DUNN potential allowing it to obtain a better ﬁt,
whereas the NN potential is ﬁt using energies and forces.

12

Figure 9: Phonon dispersions of diamond cubic silicon along high symmetry
points in the ﬁrst Brillouin zone predicted by the trained SW and NN potentials
along with the reference data by EDIP.

values shows that the DUNN uncertainty estimate increases as
the silicon crystal is strained away from its equilibrium state
(a = 5.43 Å and that the increase accelerates towards the edges
of the training set (see Fig. 8(b)). Such uncertainty informa-
tion can help to determine whether a molecular simulation is
reliable or not.

As a second example, we consider phonon dispersion. This
set of curves provides a comprehensive view of the elastic vi-
brational properties of a material, which play a key role in many
dynamical properties including thermal transport and stress wave
propagation. It is therefore important for IPs to predict phonon
dispersion correctly. Fig. 9 presents the phonon dispersion curves
of silicon along high-symmetry points in the ﬁrst Brillouin zone
obtained using the phonopy package [115]. The SW poten-
tial is in better agreement with the reference data for branches
with larger phonon frequencies, but is less accurate for the two
lowest-frequency branches, which can be seen at the W, K, and
U points. Despite these small diﬀerences, the predictions by
both the SW and NN potentials are in good agreement with
the reference data. The training set does not explicitly contain
phonon frequency data, so the fact that both the SW and NN
potentials are able to correctly reproduce EDIP’s phonon dis-
persion curves indicates that they provide a good representation
for the EDIP functional form near the equilibrium state.

4.85.05.25.45.65.86.0Latticeparametera(˚A)°4.7°4.6°4.5°4.4°4.3°4.2°4.1°4.0EnergyE(eV/atom)ReferenceDUNNmeanDUNNuncertainty4.85.05.25.45.65.86.0Latticeparametera(˚A)°0.04°0.020.000.020.04Uncertainty(eV/atom)ab05101520ReferenceSW05101520Phononfrequency(THz)ΓXWKΓLUWLKNN6. Summary and outlook

In this paper, we introduce the KIM-based learning-integrated

ﬁtting framework (KLIFF) for developing IPs. KLIFF provides
a uniform Python user interface to train both physics-based and
machine learning potentials. It is ﬂexible and easily extended to
support new atomic environment descriptors, models, loss func-
tions, minimizers, and analyzers. KLIFF integrates closely with
the KIM framework. An IP trained using KLIFF can be read-
ily deployed in a format consistent with the KIM API, which
enables it to be used directly in major simulation codes such
as LAMMPS [35, 44, 45], ASE [38, 39], DL POLY [40, 41],
GULP [42, 43] and ASAP [116] among others. The pack-
age is distributed under an open-source license and is available
at https://github.com/openkim/kliff along with a com-
prehensive user manual with several tutorials.

KLIFF (version 0.3.0) is fully functional as demonstrated
in this paper by training the SW, NN, and DUNN potentials
for silicon. Development continues with an emphasis on incor-
porating new features, including (1) supporting more machine
learning models and descriptors; (2) integration with KIM tests
to train on material properties beyond energy, forces, and stress;
and (3) creation of tools for automatic selection of hyperparam-
eters for machine learning potentials (e.g. optimal number of
terms to retain for a descriptor and optimal number of layers
and nodes in each layer for an NN potential). We encourage
other researchers to contribute to the development, and pro-
vide full and detailed documentation of the KLIFF API (see
the Package Reference section in the documentation https:
//github.com/openkim/kliff).

Acknowledgements

This research was partly supported by the Army Research
Oﬃce (W911NF-14-1-0247) under the MURI program, the Na-
tional Science Foundation (NSF) under grants DMR-1834251,
DMR-1834332 and OAC-2039575, and through the University
of Minnesota MRSEC under Award Number DMR-1420013.
The authors wish to acknowledge the Minnesota Supercomput-
ing Institute (MSI) at the University of Minnesota for providing
resources that contributed to the results reported in this paper.
MW thanks the University of Minnesota Doctoral Dissertation
Fellowship for supporting his research.

References

[1] E. B. Tadmor, R. E. Miller, Modeling materials: continuum, atomistic

and multiscale techniques, Cambridge University Press, 2011.

[2] R. Z. Khaliullin, H. Eshet, T. D. K¨uhne, J. Behler, M. Parrinello, Nu-
cleation mechanism for the direct graphite-to-diamond phase transition,
Nat. Mater. 10 (9) (2011) 693. doi:10.1038/nmat3078.

[3] S. Piana, K. Lindorﬀ-Larsen, D. E. Shaw, Protein folding kinetics
and thermodynamics from atomistic simulation, Proc. Natl. Acad. Sci.
109 (44) (2012) 17845–17850. doi:10.1073/pnas.1201811109.
[4] M. Wen, S. N. Shirodkar, P. Plech´aˇc, E. Kaxiras, R. S. Elliott, E. B.
Tadmor, A force-matching stillinger-weber potential for MoS2: Param-
eterization and ﬁsher information theory based sensitivity analysis, J.
Appl. Phys. 122 (24) (2017) 244301. doi:10.1063/1.5007842.

[5] M. Wen, E. B. Tadmor, Hybrid neural network potential for multi-
layer graphene, Phys. Rev. B 100 (19) (2019) 195419. doi:10.1103/
physrevb.100.195419.

[6] M. Born, R. Oppenheimer, Zur quantentheorie der molekeln, An-
doi:10.1002/andp.

nalen der physik 389 (20) (1927) 457–484.
19273892002.

[7] Y. Mishin, D. Farkas, M. J. Mehl, D. A. Papaconstantopoulos, Inter-
atomic potentials for monoatomic metals from experimental data and ab
initio calculations, Phys. Rev. B 59 (1999) 3393–3407. doi:10.1103/
physrevb.59.3393.

[8] M. Wen, S. M. Whalen, R. S. Elliott, E. B. Tadmor, Interpolation ef-
fects in tabulated interatomic potentials, Modell. Simul. Mater. Sci. Eng.
23 (7) (2015) 074008. doi:10.1088/0965-0393/23/7/074008.
[9] M. Wen, J. Li, P. Brommer, R. S. Elliott, J. P. Sethna, E. B. Tadmor, A
KIM-compliant Potﬁt for ﬁtting sloppy interatomic potentials: Applica-
tion to the edip model for silicon, Modell. Simul. Mater. Sci. Eng. 25 (1)
(2017) 014001. doi:10.1088/0965-0393/25/1/014001.

[10] F. Ercolessi, J. B. Adams, Interatomic potentials from ﬁrst-principles
calculations: the force-matching method, Europhys. Lett. 26 (8) (1994)
583–588. doi:10.1209/0295-5075/26/8/005.

[11] P. Zhang, D. R. Trinkle, Database optimization for empirical interatomic
potential models, Model. Simul. Mater. Sci. Eng. 23 (2015) 065011.
doi:10.1088/0965-0393/23/6/065011.

[12] J. E. Jones, On the determination of molecular ﬁelds. I. from the vari-
ation of the viscosity of a gas with temperature, Proc. Roy. Soc. A
106 (738) (1924) 441–462. doi:10.1098/rspa.1924.0081.

[13] J. E. Jones, On the determination of molecular ﬁelds. II. from the equa-
tion of state of a gas, Proc. Roy. Soc. A 106 (738) (1924) 463–477.
doi:10.1098/rspa.1924.0082.

[14] J. E. Lennard-Jones, Cohesion, Proc. Phys. Soc. 43 (5) (1931) 461–482.

doi:10.1088/0959-5309/43/5/301.

[15] A. C. T. van Duin, S. Dasgupta, F. Lorant, W. A. Goddard, ReaxFF: a
reactive force ﬁeld for hydrocarbons, J. Phys. Chem. A 105 (41) (2001)
9396–9409. doi:10.1021/jp004368u.

[16] D. W. Brenner, The art and science of an analytic potential, Phys. Stat.

Sol. (b) 217 (2000) 23–40. doi:10.1002/3527603107.ch2.

[17] J. Behler, M. Parrinello, Generalized neural-network representation of
high-dimensional potential-energy surfaces, Phys. Rev. Lett. 98 (14)
(2007) 146401. doi:10.1103/physrevlett.98.146401.

[18] A. P. Bart´ok, M. C. Payne, R. Kondor, G. Cs´anyi, Gaussian Approx-
imation Potentials: The accuracy of quantum mechanics, without the
electrons, Phys. Rev. Lett. 104 (13) (2010) 136403. doi:10.1103/
physrevlett.104.136403.

[19] M. Rupp, A. Tkatchenko, K.-R. M¨uller, O. A. Von Lilienfeld, Fast
and accurate modeling of molecular atomization energies with machine
learning, Phys. Rev. Lett. 108 (5) (2012) 058301. doi:10.1103/
physrevlett.108.058301.

[20] A. P. Thompson, L. P. Swiler, C. R. Trott, S. M. Foiles, G. J.
Tucker, Spectral neighbor analysis method for automated generation of
quantum-accurate interatomic potentials, J. Comput. Phys. 285 (2015)
316–330. doi:10.1016/j.jcp.2014.12.018.

[21] A. V. Shapeev, Moment tensor potentials: A class of systematically im-
provable interatomic potentials, Multiscale Model. Simul. 14 (3) (2016)
1153–1173. doi:10.1137/15m1054183.

[22] S. Hajinazar, J. Shao, A. N. Kolmogorov, Stratiﬁed construction of neu-
ral network based interatomic models for multicomponent materials,
Phys. Rev. B 95 (1) (2017) 014114. doi:10.1103/physrevb.95.
014114.

[23] J. J. Waterfall, F. P. Casey, R. N. Gutenkunst, K. S. Brown, C. R. Myers,
P. W. Brouwer, V. Elser, J. P. Sethna, Sloppy model universality class
and the Vandermonde matrix, Phys. Rev. Lett. 97 (2006) 150601. doi:
10.1103/PhysRevLett.97.150601.

[24] Y. Kurniawan, C. L. Petrie, K. J. Williams, M. K. Transtrum,
E. B.Tadmor, R. S. Elliott, D. S. Karls, M. Wen, Bayesian, frequentist,
and information geometry approaches to parametric uncertainty quan-
tiﬁcation of classical empirical potentials, submitted (2021).

[25] M. K. Transtrum, B. B. Machta, J. P. Sethna, Geometry of nonlinear
least squares with applications to sloppy models and optimization, Phys.
Rev. E 83 (3) (2011) 036701. doi:10.1103/PhysRevE.83.036701.
[26] M. K. Transtrum, J. P. Sethna, Geodesic acceleration and the small-
curvature approximation for nonlinear least squares, arXiv preprint

13

arXiv:1207.4999 (2012).

[27] M. K. Transtrum, J. P. Sethna,

Improvements to the Levenberg–
Marquardt algorithm for nonlinear least-squares minimization, arXiv
preprint arXiv:1201.5885 (2012).

[28] T. M. M. . M. S. (TMS), Veriﬁcation & Validation of Computational
Models Associated with the Mechanics of Materials, TMS, Pittsburgh,
PA, 2019. doi:10.7449/VandV_1.

[29] E. B. Tadmor, R. S. Elliott, J. P. Sethna, R. E. Miller, C. A.
Becker, The potential of atomistic simulations and the knowledgebase
of interatomic models, JOM 63 (7) (2011) 17–17. doi:10.1007/
s11837-011-0102-6.

[30] E. B. Tadmor, R. S. Elliott, S. R. Phillpot, S. B. Sinnott, Nsf cyber-
infrastructures: A new paradigm for advancing materials simulation,
Curr. Opin. Solid State Mater. Sci. 17 (6) (2013) 298–304. doi:http:
//dx.doi.org/10.1016/j.cossms.2013.10.004.

[31] D. S. Karls, M. Bierbaum, A. A. Alemi, R. S. Elliott, J. P. Sethna,
E. B. Tadmor, The OpenKIM Processing Pipeline: A cloud-based auto-
matic materials property computation engine, J. Chem. Phys. 153 (2020)
064104. doi:10.1063/5.0014267.

[32] Open knowledgebase of interatomic models (OpenKIM), https://

openkim.org, accessed: 2021-05-30 (2021).

[33] C. de Tomas, I. Suarez-Martinez, N. A. Marks, Graphitization of amor-
phous carbons: A comparative study of interatomic potentials, Carbon
109 (2016) 681–693. doi:10.1016/j.carbon.2016.08.024.
[34] D. W. Brenner, O. A. Shenderova, J. A. Harrison, S. J. Stuart, B. Ni,
S. B. Sinnott, A second-generation reactive empirical bond order (rebo)
potential energy expression for hydrocarbons, J. Phys.: Condens. Matter
14 (4) (2002) 783–802. doi:10.1088/0953-8984/14/4/312.
[35] Large-scale atomic/molecular massively parallel simulator (LAMMPS),

https://www.lammps.org, accessed: 2021-05-30 (2021).

[36] S. J. Stuart, A. B. Tutein, J. A. Harrison, A reactive potential for hydro-
carbons with intermolecular interactions, J. Chem. Phys. 112 (14) (2000)
6472–6486. doi:10.1063/1.481208.

[37] R. S. Elliott, E. B. Tadmor, Knowledgebase of Interatomic Models
doi:10.

(KIM) application programming interface (API) (2011).
25950/ff8f563a.

[38] A. H. Larsen, J. J. Mortensen, J. Blomqvist, I. E. Castelli, R. Chris-
tensen, M. Dułak, J. Friis, M. N. Groves, B. Hammer, C. Hargus,
E. D. Hermes, P. C. Jennings, P. B. Jensen, J. Kermode, J. R. Kitchin,
E. L. Kolsbjerg, J. Kubal, K. Kaasbjerg, S. Lysgaard, J. B. Maron-
sson, T. Maxson, T. Olsen, L. Pastewka, A. Peterson, C. Rostgaard,
J. Schiøtz, O. Sch¨utt, M. Strange, K. S. Thygesen, T. Vegge, L. Vilhelm-
sen, M. Walter, Z. Zeng, K. W. Jacobsen, The atomic simulation envi-
ronment—a python library for working with atoms, J. Phys.: Condens.
Matter 29 (27) (2017) 273002. doi:10.1088/1361-648x/aa680e.

[39] ASE: The atomic simulation environment—a python library for working
with atoms, https://wiki.fysik.dtu.dk/ase/, accessed: 2021-
05-30 (2021).

[40] W. Smith, T. R. Forester, DL POLY 2.0: A general-purpose molecular
dynamics simulation package, J. Mol. Graph. 14 (1996) 136–141. doi:
10.1016/S0263-7855(96)00043-4.

[41] DL POLY classic molecular simulation package, https://www.scd.

stfc.ac.uk/Pages/DL_POLY.aspx, accessed: 2021-05-30 (2021).

[42] J. D. Gale, GULP: A computer program for the symmetry-adapted sim-
ulation of solids, J. Chem. Soc.-Farad. Trans. 93 (4) (1997) 629–637.
doi:10.1039/a606455h.

[43] General utility lattice program (GULP), https://gulp.curtin.edu.

au/gulp, accessed: 2021-05-30 (2021).

[44] S. Plimpton, Fast parallel algorithms for short-range molecular dynam-
ics, J. Comput. Phys. 117 (1) (1995) 1–19. doi:10.1006/jcph.1995.
1039.

[45] A. P. Thompson, H. M. Aktulga, R. Berger, D. S. Bolintineanu,
W. Michael Brown, P. S. Crozier, P. J. in ’t Veld, A. Kohlmeyer, S. G.
Moore, T. D. Nguyen, R. Shan, M. Stevens, J. Tranchida, C. Trott, S. J.
Plimpton, Lammps - a ﬂexible simulation tool for particle-based ma-
terials modeling at the atomic, meso, and continuum scales, Comput.
Phys. Commun. (2021) 108171doi:https://doi.org/10.1016/j.
cpc.2021.108171.

[46] E. B. Tadmor, M. Ortiz, R. Phillips, Quasicontinuum analysis of de-
fects in solids, Phil. Mag. A 73 (6) (1996) 1529–1563. doi:10.1080/
01418619608243000.

[47] Quasicontinuum method website, https://openkim.org (2009).
[48] F. H. Stillinger, T. A. Weber, Computer simulation of local order in
condensed phases of silicon, Phys. Rev. B 31 (8) (1985) 5262. doi:
10.1103/physrevb.31.5262.

[49] M. Wen, Stillinger-Weber (SW) Model Driver v005, Online; accessed:

2021-05-30 (2018). doi:10.25950/934dca3e.

[50] A. P. Bart´ok, R. Kondor, G. Cs´anyi, On representing chemical environ-
ments, Phys. Rev. B 87 (18) (2013) 184115. doi:10.1103/physrevb.
87.184115.

[51] J. Behler, Atom-centered symmetry functions for constructing high-
dimensional neural network potentials, J. Chem. Phys. 134 (7) (2011)
074106. doi:10.1063/1.3553717.

[52] H. Huo, M. Rupp, Uniﬁed representation of molecules and crystals for

machine learning, arXiv preprint arXiv:1704.06439 (2017).

[53] M. F. Langer, A. Goeßmann, M. Rupp, Representations of molecules
and materials for interpolation of quantum-mechanical simulations via
machine learning, arXiv preprint arXiv:2003.12081 (2020).

[54] M. Wen, Development of interatomic potentials with uncertainty quan-
tiﬁcation: applications to two-dimensional materials, Ph.D. thesis, Uni-
versity of Minnesota (2019).
URL https://hdl.handle.net/11299/206694

[55] See Supplementary Material at [URL will be inserted by publisher] for
the dataset, the symmetry functions used as the descriptors for the neural
network, and the code script to train the potentials.

[56] L. Prechelt, Automatic early stopping using cross validation: quan-
tifying the criteria, Neural Networks 11 (4) (1998) 761–767. doi:
10.1016/S0893-6080(98)00010-0.

[57] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, R. R. Salakhut-
dinov, Improving neural networks by preventing co-adaptation of feature
detectors, arXiv preprint arXiv:1207.0580 (2012).

[58] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,
Dropout: a simple way to prevent neural networks from overﬁtting, J
Mach. Learn. Res. 15 (1) (2014) 1929–1958.

[59] P. Brommer, F. G¨ahler, Potﬁt: eﬀective potentials from ab initio data,
Modell. Simul. Mater. Sci. Eng. 15 (3) (2007) 295. doi:10.1088/
0965-0393/15/3/008.

[60] P. Brommer, A. Kiselev, D. Schopf, P. Beck, J. Roth, H.-R. Trebin, Clas-
sical interaction potentials for diverse materials from ab initio data: a
review of potﬁt, Modell. Simul. Mater. Sci. Eng. 23 (7) (2015) 074002.
doi:10.1088/0965-0393/23/7/074002.

[61] N. Artrith, A. Urban, An implementation of artiﬁcial neural-network po-
tentials for atomistic materials simulations: Performance for tio2, Com-
put. Mater. Sci. 114 (2016) 135–150. doi:10.1016/j.commatsci.
2015.11.047.

[62] A. Khorshidi, A. A. Peterson, Amp: A modular approach to machine
learning in atomistic simulations, Comput. Phys. Commun. 207 (2016)
310–324. doi:10.1016/j.cpc.2016.05.010.

[63] A. E. Allen, G. Dusson, C. Ortner, G. Cs´anyi, Atomic permutation-
ally invariant polynomials for ﬁtting molecular force ﬁelds, Mach.
Learn.: Sci. Technol. 2 (2) (2021) 025017. doi:10.1088/2632-2153/
abd51e.

[64] A. Stukowski, E. Fransson, M. Mock, P. Erhart, Atomicrex—a general
purpose tool for the construction of atomic interaction models, Mod-
ell. Simul. Mater. Sci. Eng. 25 (5) (2017) 055003. doi:10.1088/
1361-651x/aa6ecf.

[65] H. Wang, L. Zhang, J. Han, E. Weinan, Deepmd-kit: A deep learn-
ing package for many-body potential energy representation and molec-
ular dynamics, Comput. Phys. Commun. 228 (2018) 178–184. doi:
10.1016/j.cpc.2018.03.016.

[66] GAP and SOAP documentation, https://libatoms.github.io/

GAP/, accessed: 2021-09-16 (2021).

[67] S. Hajinazar, A. Thorn, E. D. Sandoval, S. Kharabadze, A. N. Kol-
mogorov, Maise: Construction of neural network interatomic models
and evolutionary structure optimization, Comput. Phys. Commun. 259
(2021) 107679. doi:10.1016/j.cpc.2020.107679.

[68] I. S. Novikov, K. Gubaev, E. V. Podryabinkin, A. V. Shapeev, The mlip
package: Moment tensor potentials with mpi and active learning, Mach.
Learn.: Sci. Technol. 2 (2) (2020) 025002. doi:10.1088/2632-2153/
abc9fe.

[69] Y. Lysogorskiy, C. van der Oord, A. Bochkarev, S. Menon, M. Rinaldi,
T. Hammerschmidt, M. Mrovec, A. Thompson, G. Cs´anyi, C. Ortner,

14

et al., Performant implementation of the atomic cluster expansion (pace)
and application to copper and silicon, npj Comput. Mater. 7 (97) (2021).
doi:10.5281/zenodo.4734036.

[70] R. Lot, F. Pellegrini, Y. Shaidu, E. K¨uc¸ ¨ukbenli, Panna: Properties from
artiﬁcial neural network architectures, Comput. Phys. Commun. 256
(2020) 107402. doi:10.1016/j.cpc.2020.107402.

[71] H. Yanxon, D. Zagaceta, B. Tang, D. S. Matteson, Q. Zhu, PyXtal FF: a
python library for automated force ﬁeld generation, Mach. Learn.: Sci.
Technol. 2 (2) (2020) 027001. doi:10.1088/2632-2153/abc940.

[72] The RuNNer

code,

https://www.uni-goettingen.de/de/

560580.html, accessed: 2021-09-16 (2021).

[73] K. Lee, D. Yoo, W. Jeong, S. Han, Simple-nn: An eﬃcient package for
training and executing neural-network interatomic potentials, Comput.
Phys. Commun. 242 (2019) 95–103. doi:10.1016/j.cpc.2019.04.
014.

[74] E. Tadmor, Veriﬁcation Check for Memory Leaks using Valgrind v001,
Online; accessed: 2021-05-30 (2018). doi:10.25950/ba474f45.
[75] E. Tadmor, Veriﬁcation Check of Invariance with respect to the Inversion
Operation (Inversion Symmetry) v001, Online; accessed: 2021-05-30
(2018). doi:10.25950/63a96579.

[76] E. Tadmor, Veriﬁcation Check of Invariance with respect to Atom Per-
mutations (Permutation Symmetry) v001, Online; accessed: 2021-05-30
(2018). doi:10.25950/dfbf8222.

[77] E. Tadmor, Veriﬁcation Check of Forces via Numerical Diﬀerentiation
(Richardson Extrapolation Technique) v002, Online; accessed: 2021-
05-30 (2018). doi:10.25950/9be59b8d.

[78] S. Pattamatta, Stacking and twinning fault energies of an fcc lattice
at zero temperature and pressure v001, Online; accessed: 2021-05-30
(2018). doi:10.25950/d6ffade7.

[79] J. Li, E. Tadmor, Elastic constants for cubic crystals at zero tempera-
ture and pressure v005, Online; accessed: 2021-05-30 (2019). doi:
10.25950/49c5c255.

[80] M. Wen, Linear thermal expansion coeﬃcient of a cubic crystal structure
at a given temperature and pressure v001, Online; accessed: 2021-05-30
(2016). doi:10.25950/fc69d82d.

[81] R. A. Messerly, T. A. Knotts, W. V. Wilding, Uncertainty quantiﬁcation
and propagation of errors of the lennard-jones 12-6 parameters for n-
alkanes, J. Chem. Phys. 146 (19) (2017) 194110. doi:10.1063/1.
4983406.

[82] H. Cram´er, Mathematical Methods of Statistics, Princeton University

Press, Princeton, 1999.

[83] S. L. Frederiksen, K. W. Jacobsen, K. S. Brown, J. P. Sethna,
Bayesian ensemble approach to error estimation of interatomic poten-
tials, Physical review letters 93 (16) (2004) 165501. doi:10.1103/
PhysRevLett.93.165501.

[84] S. Longbottom, P. Brommer, Uncertainty quantiﬁcation for classical ef-
fective potentials: an extension to potﬁt, Modelling Simul. Mater. Sci.
Eng. 27 (4) (2019) 044001. doi:10.1088/1361-651x/ab0d75.
[85] N. Artrith, J. Behler, High-dimensional neural network potentials for
metal surfaces: A prototype study for copper, Phys. Rev. B 85 (4) (2012)
045439. doi:10.1103/PhysRevB.85.045439.

[86] A. A. Peterson, R. Christensen, A. Khorshidi, Addressing uncertainty in
atomistic machine learning, Phys. Chem. Chem. Phys. 19 (18) (2017)
10978–10985. doi:10.1039/C7CP00375G.

[87] L. Zhang, D.-Y. Lin, H. Wang, R. Car, E. Weinan, Active learn-
ing of uniformly accurate interatomic potentials for materials simu-
doi:10.1103/
lation, Phys. Rev. Materials 3 (2) (2019) 023804.
PhysRevMaterials.3.023804.

[88] W. Jeong, D. Yoo, K. Lee, J. Jung, S. Han, Eﬃcient atomic-resolution
uncertainty estimation for neural network potentials using a replica en-
semble, J. Phys. Chemistry Lett. 11 (15) (2020) 6090–6096. doi:
10.1021/acs.jpclett.0c01614.

[89] M. Wen, E. B. Tadmor, Uncertainty quantiﬁcation in molecular simula-
tions with dropout neural network potentials, npj Comput. Mater. 6 (1)
(2020) 124. doi:10.1038/s41524-020-00390-8.

[90] Y. Gal, Z. Ghahramani, Dropout as a Bayesian approximation: Rep-
resenting model uncertainty in deep learning, in: M. F. Balcan, K. Q.
Weinberger (Eds.), Proceedings of The 33rd International Conference
on Machine Learning, Vol. 48 of Proceedings of Machine Learning Re-
search, PMLR, New York, New York, USA, 2016, pp. 1050–1059.
URL https://proceedings.mlr.press/v48/gal16.html

[91] Y. Gal, Uncertainty in deep learning, Ph.D. thesis, University of Cam-

bridge (2016).

[92] M. Wen, A dropout uncertainty neural network (DUNN) model driver
v000, OpenKIM, https://doi.org/10.25950/9573ca43 (2019).
doi:10.25950/9573ca43.

[93] T. Brink, Model driver

LAMMPS v003, Online; accessed: 2021-05-30 (2019).
25950/55b7b34e.

for Tersoﬀ-style potentials ported from
doi:10.

[94] J. Tersoﬀ, New empirical approach for the structure and energy of cova-
lent systems, Phys. Rev. B 37 (12) (1988) 6991–7000. doi:10.1103/
PhysRevB.37.6991.

[95] J. Tersoﬀ, Modeling solid-state chemistry: Interatomic potentials for
multicomponent systems, Phys. Rev. B 39 (1989) 5566–5568. doi:
10.1103/PhysRevB.39.5566.

[96] J. Nord, K. Albe, P. Erhart, K. Nordlund, Modelling of compound
semiconductors: analytical bond-order potential for gallium, nitrogen
and gallium nitride, J. Phys.: Condens. Matter 15 (2003) 5649. doi:
10.1088/0953-8984/15/32/324.

[97] D. S. Karls, Environment-Dependent Interatomic Potential (EDIP)
model driver v002, Online; accessed: 2021-05-30 (2018). doi:10.
25950/75c4686e.

[98] M. Z. Bazant, E. Kaxiras, Modeling of covalent bonding in solids by
inversion of cohesive energy curves, Phys. Rev. Lett. 77 (1996) 4370–
4373. doi:10.1103/PhysRevLett.77.4370.

[99] M. Z. Bazant, E. Kaxiras, J. F. Justo, Environment-dependent inter-
atomic potential for bulk silicon, Phys. Rev. B 56 (1997) 8542–8552.
doi:10.1103/PhysRevB.56.8542.

[100] J. a. F. Justo, M. Z. Bazant, E. Kaxiras, V. V. Bulatov, S. Yip, Inter-
atomic potential for silicon defects and disordered phases, Phys. Rev. B
58 (1998) 2539–2550. doi:10.1103/PhysRevB.58.2539.

[101] R. S. Elliott, EAM Model Driver for tabulated potentials with cubic Her-
mite spline interpolation as used in LAMMPS v005, Online; accessed:
2021-05-30 (2018). doi:10.25950/68defa36.

[102] M. S. Daw, M. I. Baskes, Embedded-atom method: Derivation and ap-
plication to impurities, surfaces, and other defects in metals, Phys. Rev.
B 29 (12) (1984) 6443. doi:10.1103/physrevb.29.6443.

[103] M. S. Daw, S. M. Foiles, M. I. Baskes, The embedded-atom method: a
review of theory and applications, Mater. Sci. Rep. 9 (7) (1993) 251–
310. doi:10.1016/0920-2307(93)90001-u.

[104] L. Himanen, M. O. J¨ager, E. V. Morooka, F. F. Canova, Y. S. Ranawat,
D. Z. Gao, P. Rinke, A. S. Foster, Dscribe: Library of descriptors for ma-
chine learning in materials science, Comput. Phys. Commun. 247 (2020)
106949. doi:10.1016/j.cpc.2019.106949.

[105] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf,
E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,
L. Fang, J. Bai, S. Chintala, Pytorch: An imperative style, high-
performance deep learning library, in: Advances in Neural Information
Processing Systems 32, Curran Associates, Inc., 2019, pp. 8024–8035.
[106] C. Zhu, R. H. Byrd, P. Lu, J. Nocedal, Algorithm 778: L-BFGS-b: For-
tran subroutines for large-scale bound-constrained optimization, ACM
Trans. Math. Software 23 (4) (1997) 550–560. doi:10.1145/279232.
279236.

[107] SciPy: a Python-based ecosystem of open-source software for mathe-
matics, science, and engineering, https://www.scipy.org, accessed:
2021-05-30 (2021).

[108] H. Robbins, S. Monro, A stochastic approximation method, Ann. Math.
Stat. (1951) 400–407doi:10.1007/978-1-4612-5110-1_9.
[109] J. Kiefer, J. Wolfowitz, et al., Stochastic estimation of the maximum of
a regression function, Ann. Math. Stat. 23 (3) (1952) 462–466. doi:
10.1007/978-1-4613-8505-9_4.

[110] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, arXiv

preprint arXiv:1412.6980 (2014).

[111] K. He, X. Zhang, S. Ren, J. Sun, Delving deep into rectiﬁers: Sur-
passing human-level performance on imagenet classiﬁcation, 2015 IEEE
International Conference on Computer Vision (ICCV) (2015). doi:
10.1109/iccv.2015.123.

[112] M. Wen, S. Carr, S. Fang, E. Kaxiras, E. B. Tadmor, Dihedral-angle-
corrected registry-dependent interlayer potential for multilayer graphene
doi:10.1103/
structures, Phys. Rev. B 98 (23) (2018) 235404.
physrevb.98.235404.

15

[113] X. Zhou, D. Ward, J. Martin, F. Van Swol, J. Cruz-Campa, D. Zubia,
Stillinger-weber potential for the ii-vi elements zn-cd-hg-s-se-te, Phys.
Rev. B 88 (8) (2013) 085309. doi:10.1103/physrevb.88.085309.
[114] M. Li, T. Zhang, Y. Chen, A. J. Smola, Eﬃcient mini-batch training for
stochastic optimization, in: Proceedings of the 20th ACM SIGKDD in-
ternational conference on Knowledge discovery and data mining, ACM,
2014, pp. 661–670. doi:10.1145/2623330.2623612.

[115] A. Togo, I. Tanaka, First principles phonon calculations in materials
science, Scr. Mater. 108 (2015) 1–5. doi:10.1016/j.scriptamat.
2015.07.021.

[116] Asap: a calculator for doing large-scale classical molecular dynam-
ics, https://wiki.fysik.dtu.dk/asap/, accessed: 2021-05-30
(2021).

16

Supplementarymaterialfor“KLIFF:Aframeworktodevelopphysics-basedandmachinelearninginteratomicpotentials”MingjianWena,YaserAfshara,RyanS.Elliotta,ElladB.Tadmora,∗aDepartmentofAerospaceEngineeringandMechanics,UniversityofMinnesota,Minneapolis,MN55455,USA1.ParameterizationofaDUNNpotentialThearchitectureofthedropoutuncertaintyneuralnetwork(DUNN)potentialisthesameastheneuralnetwork(NN)potentialexceptthatdropoutlayersareaddedaftertheactivationlayers.Weuseadropoutratioof0.1and64nodesineachlayers.Asbrieﬂydiscussedinthemaintext,insteadofbothenergiesandforces,weonlyincludeenergiesfortrainingtheDUNNpotentialtofocusoninvestigatingtheuncertaintyintheenergypredictions.Otherparameterizationchoices(e.g.atomicenvironmentdescriptorandcutoﬀdistance)areexactlythesameasthosefortheNNpotentialdiscussedinthemaintext.AnexampletrainingscriptisgiveninSection2.3.2.DatasetandcodeexamplesWeprovidethescriptsusedfortrainingtheStillinger-Weber(SW),NN,andDUNNpotentials.ThesescriptsarecompatiblewithKLIFFv.0.3.0andmayrequirechangesinfuturereleasesofKLIFF.Forup-to-dateexamples,seetheKLIFFdocumentationathttps://github.com/openkim/kliff.Thedatasetusedtoﬁtthepotentialsisprovidedalongwiththissupplementarymaterial.2.1.KLIFFscriptfortraininganSWpotentialfromkliff.calculatorsimportCalculatorfromkliff.datasetimportDatasetfromkliff.lossimportLossfromkliff.modelsimportKIMModel#CreateamodelfortheSWpotentialandsetparameterstooptimizemodel=KIMModel(model_name="SW_StillingerWeber_1985_Si__MO_405512056662_006")model.set_opt_params(A=[["default"]],B=[["default"]],sigma=[["default"]],gamma=[["default"]])model.set_one_opt_param("lambda",[["default"]])#Loadtrainingsetdataset_name="Si_dataset_EDIP"tset=Dataset(dataset_name)configs=tset.get_configs()#Setweightforconfigurationswithoutperturbationto10(defaultis1)forconfinconfigs:if"perturb0"inconf.identifier:conf.weight=10∗Correspondingauthor.E-mailaddress:tadmor@umn.edu1#CreatecalculatorfortheSWmodelcalc=Calculator(model)calc.create(configs)#MinimizeusingthegeodesicLevenberg-Marquardtoptimizeron4processesloss=Loss(calc,residual_data={"energy_weight":1,"forces_weight":10},nprocs=4)loss.minimize(method="geodesiclm",maxiter=1000)#SavemodelforfuturereloadandwriteaKIMmodelmodel.save("kliff_model.yaml")model.write_kim_model()2.2.KLIFFscriptfortraininganNNpotentialfromkliffimportnnfromkliff.calculatorsimportCalculatorTorchfromkliff.datasetimportDatasetfromkliff.descriptorsimportSymmetryFunctionfromkliff.lossimportLossfromkliff.modelsimportNeuralNetwork#InitializetheNNmodelusingthesymmetryfunctionsdescriptordescriptor=SymmetryFunction(cut_name="cos",cut_dists={"Si-Si":3.5},hyperparams="set51",normalize=True)model=NeuralNetwork(descriptor)#CreateanNNmodelof3hiddenlayerswith10nodesineachN=10model.add_layers(#firsthiddenlayernn.Linear(descriptor.get_size(),N),nn.Tanh(),#secondhiddenlayernn.Linear(N,N),nn.Tanh(),#thirdhiddenlayernn.Linear(N,N),nn.Tanh(),#outputlayernn.Linear(N,1),)#Loadtrainingsetdataset_name="Si_dataset_EDIP"tset=Dataset(dataset_name)configs=tset.get_configs()#Setweightforconfigurationswithoutperturbationto10(defaultis1)forconfinconfigs:if"perturb0"inconf.identifier:conf.weight=10#CreatecalculatorfortheNNmodelcalc=CalculatorTorch(model)calc.create(configs)#Minimizingforamaximumof2000epochsloss=Loss(calc,residual_data={"energy_weight":1,"forces_weight":10})loss.minimize(method="Adam",num_epochs=2000,batch_size=100,lr=0.001)#SavemodelforfuturereloadandwriteaKIMmodelmodel.save("kliff_model.pkl")loss.save_optimizer_state("optimizer_stat.pkl")2model.write_kim_model()2.3.KLIFFscriptfortrainingaDUNNpotentialfromkliffimportnnfromkliff.calculatorsimportCalculatorTorchfromkliff.datasetimportDatasetfromkliff.descriptorsimportSymmetryFunctionfromkliff.lossimportLossfromkliff.modelsimportNeuralNetwork#InitializetheNNmodelusingthesymmetryfunctionsdescriptordescriptor=SymmetryFunction(cut_name="cos",cut_dists={"Si-Si":3.5},hyperparams="set51",normalize=True)model=NeuralNetwork(descriptor)#CreateanNNmodelof3hiddenlayerswith10nodesineachN=64model.add_layers(#firsthiddenlayernn.Linear(descriptor.get_size(),N),nn.Tanh(),nn.Dropout(p=0.1),#secondhiddenlayernn.Linear(N,N),nn.Tanh(),#nn.Dropout(p=0.1),#thirdhiddenlayernn.Linear(N,N),nn.Tanh(),#nn.Dropout(p=0.1),#outputlayernn.Linear(N,1),)#Loadtrainingsetdataset_name="../Si_dataset_EDIP_child-1"tset=Dataset(dataset_name)configs=tset.get_configs()#Setweightforconfigurationswithoutperturbationto10(defaultis1)forconfinconfigs:if"perturb0"inconf.identifier:conf.weight=10#CreatecalculatorfortheDUNNmodelandturnoffthecalculationofforcescalc=CalculatorTorch(model)calc.create(configs,use_forces=False)#Minimizingforamaximumof2000epochsloss=Loss(calc,residual_data={"energy_weight":1,"forces_weight":0})loss.minimize(method="Adam",num_epochs=2000,batch_size=100,lr=0.001)#SavemodelforfuturereloadandwriteaKIMmodelmodel.save("kliff_model.pkl")loss.save_optimizer_state("optimizer_stat.pkl")model.write_kim_model(dropout_ensemble_size=100)3.SymmetryfunctionhyperparametersTheatomicenvironmentdescriptorfortheNNandDUNNpotentialsisobtainedusingthesymmetryfunctionsG2iandG4i.ThehyperparametersusedinG2iarelistedinTable1andinG4iarelistedinTable2.3Table1:HyperparametersusedintheradicaldescriptorG2i.No.α(Bohr−2)Rs(Bohr)10.001020.01030.02040.035050.06060.1070.2080.40Table2:HyperparametersusedintheangulardescriptorG4i.No.ζλη(Bohr−2)No.ζλη(Bohr−2)11−10.0001232−10.0252110.000124210.02532−10.0001254−10.0254210.000126410.02551−10.0032716−10.0256110.003281610.02572−10.003291−10.0458210.00330110.04591−10.008312−10.04510110.00832210.045112−10.008334−10.04512210.00834410.045131−10.0153516−10.04514110.015361610.045152−10.015371−10.0816210.01538110.08174−10.015392−10.0818410.01540210.081916−10.015414−10.08201610.01542410.08211−10.025431610.0822110.0254.ImplementationofanewpotentialinKLIFFKLIFFisdesignedtobeextendable.ThisexampledemonstrateshowtoaddanewinteratomicpotentialoftheLennard-Jonesform.4fromtypingimportDictimportnumpyasnpfromkliff.dataset.configurationimportConfigurationfromkliff.models.modelimportComputeArguments,Modelfromkliff.models.parameterimportParameterfromkliff.neighborimportNeighborList,assemble_forcesclassLennardJones(Model):"""Lennard-Jones6-12potentialmodel."""def__init__(self,model_name:str="LJ6-12"):super(LennardJones,self).__init__(model_name)definit_model_params(self):model_params={"epsilon":Parameter([15.3]),"sigma":Parameter([0.01025]),"cutoff":Parameter([3.5]),}returnmodel_paramsdefinit_influence_distance(self):returnself.model_params["cutoff"][0]definit_supported_species(self):return{"Ar":1}defget_compute_argument_class(self):returnLJComputeArgumentsclassLJComputeArguments(ComputeArguments):"""Lennard-Jones6-12potentialcomputationroutine."""implemented_property=["energy","forces"]def__init__(self,conf:Configuration,supported_species:Dict[str,int],influence_distance:float,):super(LJComputeArguments,self).__init__(conf,supported_species,influence_distance)self.neigh=NeighborList(self.conf,influence_distance,padding_need_neigh=False)defcompute(self,params:Dict[str,Parameter]):epsilon=params["epsilon"][0]sigma=params["sigma"][0]rcut=params["cutoff"][0]coords=self.conf.coordscoords_including_padding=self.neigh.coords5forces_including_padding=np.zeros_like(coords_including_padding)energy=0fori,xyz_iinenumerate(coords):neighlist,_,_=self.neigh.get_neigh(i)forjinneighlist:xyz_j=coords_including_padding[j]rij=xyz_j-xyz_ir=np.linalg.norm(rij)phi,dphi=self.calc_phi_dphi(epsilon,sigma,r,rcut)energy+=0.5*phipair=0.5*dphi/r*rijforces_including_padding[i]=forces_including_padding[i]+pairforces_including_padding[j]=forces_including_padding[j]-pairself.results["energy"]=energyforces=assemble_forces(forces_including_padding,len(coords),self.neigh.padding_image)self.results["forces"]=forces@staticmethoddefcalc_phi_dphi(epsilon,sigma,r,rcut):ifr>rcut:phi=0.0dphi=0.0else:sor=sigma/rsor6=sor*sor*sorsor6=sor6*sor6sor12=sor6*sor6phi=4*epsilon*(sor12-sor6)dphi=24*epsilon*(-2*sor12+sor6)/rreturnphi,dphi6