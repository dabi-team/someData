1
2
0
2

p
e
S
3
2

]

G
L
.
s
c
[

2
v
0
2
7
0
1
.
6
0
0
2
:
v
i
X
r
a

IReEn: Reverse-Engineering of Black-Box
Functions via Iterative Neural Program Synthesis

Hossein Hajipour1, Mateusz Malinowski2, and Mario Fritz1

1 CISPA Helmholtz Center for Information Security
2 DeepMind

Abstract. In this work, we investigate the problem of revealing the
functionality of a black-box agent. Notably, we are interested in the
interpretable and formal description of the behavior of such an agent.
Ideally, this description would take the form of a program written in a
high-level language. This task is also known as reverse engineering and
plays a pivotal role in software engineering, computer security, but also
most recently in interpretability. In contrast to prior work, we do not rely
on privileged information on the black box, but rather investigate the
problem under a weaker assumption of having only access to inputs and
outputs of the program. We approach this problem by iteratively reﬁning
a candidate set using a generative neural program synthesis approach until
we arrive at a functionally equivalent program. We assess the performance
of our approach on the Karel dataset. Our results show that the proposed
approach outperforms the state-of-the-art on this challenge by ﬁnding
an approximately functional equivalent program in 78% of cases – even
exceeding prior work that had privileged information on the black-box.

Keywords: Reverse-engineering · Program synthesis · Neural program
synthesis · Iterative program synthesis

1

Introduction

Reverse-engineering (RE) is about
gaining insights into the inner work-
ings of a mechanism, which often re-
sults in the capability of reproducing
the associated functionality. In our
work, we consider a program to be a
black-box function that we have no in-
sights into its internal mechanism, and
we can only interface with it through
inputs and the program generated out-
puts. This is a desired scenario in soft-
ware engineering [15,9], or security, where we reverse-engineer, e.g., binary ex-
ecutables for analysis and for ﬁnding potential vulnerabilities [14,25]. Similar
principles have been applied in the program synthesis domain to understand the

Fig. 1: An example of revealing the func-
tionality of a black-box function using
only input-output interactions.

 
 
 
 
 
 
2

H. Hajipour et al.

functionality of the given program [11,22]. Furthermore, a similar paradigm is
used to reverse-engineer the brain to advance knowledge in various brain-related
disciplines [17] or seeking interpretation for reinforcement learning agents [23].

Despite all of the progress in reverse-engineering the software and machine
learning models, there are typical limitations in the proposed works. E.g., in the
decompilation task, one of the assumptions is to have access to the assembly code
of the black-box programs or other privileged information, which is a signiﬁcant
information leak about the black-box function. In program synthesis, a common
issue is that the problem is considered in a relaxed setting, where they only
synthesize loop-free or conditional-free programming languages [11]. Furthermore,
in deep learning, a common issue is that the reverse-engineered models usually
are not represented in an interpretable and human-readable form.

In recent work, neural program synthesizers are employed to recover a func-
tional and interpretable form of a black-box program that is generated based
only on I/Os examples [4]. On close inspection, however, it turns out, that these
approaches also leverage privileged information, by relying on a biased sampling
strategy of I/Os that was obtained under the knowledge of the black-box function.

In contrast to prior work, we propose an iterative neural program synthesis
scheme which is the ﬁrst to tackle the task of reveres-engineering in a black-
box setting without any access to privileged information. Despite the weaker
assumptions and hence the possibility to use our method broadly in other ﬁelds,
we show that in many cases it is possible to reverse engineer approximately
functionally equivalent programs on the Karel dataset benchmark. We even
achieve better results than prior work that has access to privileged information.

We achieve this by an iterative reverse-engineering approach. We query a
black-box function using random inputs to obtain a set of I/Os, and reﬁne
the candidate set by an iterative neural program synthesis scheme. This neural
program synthesis is trained with pairs of I/Os and target programs. To adapt
our program synthesize to the domain of random I/Os we ﬁne-tune our neural
program synthesize using random I/Os and the corresponding target program.

To summarize the contributions of this work are as follow:

1. We propose an iterative neural program synthesizer scheme to reverse-engineer
a functionally equivalent form of the black-box program. To the best of our
knowledge, this is the ﬁrst approach that operates in a black-box setting
without privileged information.

2. We proposed functional equivalence metric in order to quantify progress on

this challenging task.

3. We evaluate our approach on Karel dataset, where our approach successfully
revealed the underlying programs of 78% of the black-box programs. Our
approach outperforms prior work despite having access to less information
due to weaker assumptions.

IReEn

3

2 Related Work

Reverse-Engineering of programs. Decompilation is the task of translating a
low-level program into a human-readable high-level language. Phoenix [3] and Hex-
Rays [1] are conventional decompilers. These decompilers are relying on pattern
matching and hand-crafted rules, and often fail to decompile non-trivial codes
with conditional structures. Fu et al. [9] proposed a deep-learning-based approach
to decompile the low-level codes in an end-to-end fashion. In the decompilation
task, the main assumption is to have access to a low-level code of the program.
However, in our approach, our goal is to represent a black-box function in a
high-level program language only by relying on input-output interactions.

Reverse-Engineering of neural networks. Reverse-engineering neural network
recently has gained popularity. Oh et al. [18] proposed a meta-model to predict
the attributes of the black-box neural network models, such as architecture and
optimization process. Orekondy et al. [19] investigate how to steal the functionality
of the black-box model only based on image query interactions. While these works
try to duplicate the functionality of a black-box function, in this work our goal
is to represent the functionality of the black-box function in a human-readable
program language.

Reverse-Engineering for interpretability. In another line of work, Verma et al.
[23] and Bastani et al. [2] proposed diﬀerent approaches to have interpretable
and veriﬁable reinforcement learning. Verma et al. [23] designed a reinforcement
learning framework to represent the policy network using human-readable domain-
speciﬁc language, and Bastani et al. [2] represent policy network by a training
decision tree. Both of these works are designed for a small set of RL problems with
a simple program structure. However, in our work, we consider reverse-engineer
a wide range of programs with complex structures.

Program synthesis. Program synthesis is a classic task which has been studied
since the early days of Artiﬁcial Intelligence [24,16]. Recently there has been a lot
of recent progress in employing the neural-networks-based approaches to do the
task of program synthesis. One type of these approaches called neural program
induction involves learning a machine learning model to mimic the behavior
of the target program [10,12,6]. Another type of approach is neural program
synthesis, where the goal is to learn to generate an explicit discrete program in a
domain-speciﬁc program language. Devlin et al. [7] proposed an encoder-decoder
neural network style to learn to synthesize programs from input-output examples.
Bunel et al. [4] synthesizing Karel programs from examples, where they learn to
generate program using a deep-learning-based model by leveraging the syntax
constraints and reinforcement learning. Shin et al. [21] and Chen et al. [5] leverage
the semantic information of execution trace of the programs to generate more
accurate programs. These works assume that they have access to the crafted I/O
examples. However, in this work, we proposed an iterative program synthesis
scheme to deal with the task of black-box program synthesis, where we only have
access to the random I/O examples.

4

H. Hajipour et al.

3 Problem Overview

In this section, we formulate the problem description and our method. We base
our notation on [4,21,5].

Program synthesis. Program synthesis deals with the problem of deriving a
program in a speciﬁed programming language that satisﬁes the given speciﬁcation.
We treat input-output pairs I/O = (cid:8)(I k, Ok)(cid:9)K
k=1 as a form of specifying the
functionality of the program. This problem can be formalized as ﬁnding a solution
to the following optimization problem:

arg min
p∈P

Ω(p)

s.t. p(I k) = Ok ∀k ∈ {1, . . . , K}

(1)

(2)

where P is the space of all possible programs written in the given language, and
Ω is some measure of the program. For instance, Ω can be a cost function that
chooses the shortest program.

The situation is illustrated in Figure 2. For many applications – also the
one we are interested in – there is a true underlying black-box program that
satisﬁes all the input-output pairs. As most practical languages do not have
a unique representation for certain functionality or behavior, a certain set of
functionally equivalent programs will remain indistinguishable even given an
arbitrary large number of input-output observations and respective constraints
in our optimization problem. Naturally, by adding more constraints, we obtain
a nested constraint set that converges towards the feasible set of functionally
equivalent programs.

Program synthesis with privileged
information. Recent works implic-
itly or explicitly incorporates in-
sider information on the function
to reverse-engineer. This can come
in the form of a binary of the com-
piled code or an informed sam-
pling strategy of the input-output
pairs. It turns out that the ma-
jority of recent research implic-
itly uses privileged information via
biased sampling scheme in terms
of crafted speciﬁcations [4,5,20,21].
Note that in order to arrive at these speciﬁcations, one has to have access to the
program P under the question as they are designed to capture e.g. all branches of
the program. We call these crafted speciﬁcations crafted I/Os and will investigate
later in detail how much information they leak about the black-box program.

Fig. 2: Illustration of the optimization prob-
lem, functional equivalence, and feasible sets
w.r.t. nested constraint sets.

IReEn

5

Black-Box program synthesis. In our work, we focus on a black-box setting, where
no such side or privileged information is available. Hence, we will have to defer
initially to randomly generate K inputs {I k}K
k=1 and next query the program p
to obtain the corresponding outputs {Ok}K
k=1. Such generated input-output pairs
become our speciﬁcation that we use to synthesize programs. Note that, unlike
the previous setting, here we take advantage of querying the black-box program
p in an active way, even though the whole procedure remains automatic. To
generate random inputs we follow the procedure proposed by Bunel et al. [4]. We
call the obtained I/Os in the black-box setting random I/Os. It turns out (as we
will also show in our experiments), that indeed such random, uninformed input
queries yield signiﬁcantly less information than the crafted I/Os used in prior
work. Hence, to arrive at an eﬀective and black-box approach, in the following
we propose an iterative reverse-engineering scheme, that gradually queries more
relevant inputs.

4

IReEn: Iterative Reverse-Engineering of Black-Box
Functions

Reverse-engineering a black-box function and representing it in a high-level
language is a challenging task. The main reason is that we can only interact with
the black-box function using input-output examples. In addition, solving the
above constraint optimization problem is intractable. Therefore, in the following,
we relax the optimization problem to a Bayesian inference problem and show how
to iteratively incorporate additional constraints in order to arrive at a functional
equivalent program with respect to the black-box function.

Figure 3 provides an overview of our iterative neural program synthesis
scheme to reverse-engineer the given black-box function. In the ﬁrst step, we
obtain the I/Os by querying the black-box function using random inputs drawn
from a distribution of inputs. We condition the neural program synthesizer on
the obtained I/Os. Neural program synthesizer outputs the potential program
candidate(s), and then we use a scoring system to score the generated candidates.
For example, in this ﬁgure "program candidate 1" satisﬁed two out of four sample
I/Os, so its score will be 2. If the best candidate does not cover all of the I/Os,
we select a subset of I/Os which were not covered by the best candidate program
to condition them on the program synthesizer for the next iteration.

4.1 Finding Programs given Input-Output Constraints

Even for a small set of input-output constraints, ﬁnding the feasible set of
programs that satisﬁes these I/Os is not tractable due to the discrete and
compositional nature of programs. We approach this challenging problem by
relaxing the constraint optimization problem to a Bayesian Inference problem.
In this way, samples of the model are solutions to the constraint optimization
problem. In order to train such a generative model, we directly optimize the
Neural Program Synthesis approach based on Bunel et al. [4]. This is a conditional

6

H. Hajipour et al.

Fig. 3: Overview of the proposed iterative neural program synthesis approach.

generative model that samples candidate programs by conditioning on the input-
output information.

ˆP ∼ Ψ (I/O).

(3)

Where ˆP is a set of sampled solutions that are program candidate(s) {ˆp1, ..., ˆpC} ∈
ˆP and C ≥ 1.

i )}K

i ) = Ok
i

In detail, we train a recurrent encoder and decoder for program synthesis on a
set of ground-truth programs {pi}i and speciﬁcations {I/Oi}i. Each speciﬁcation
i , Ok
is a set of K pairs I/Oi = {(I k
k=1 where the program needs to be consistent
with, that is, pi(I k
for all k ∈ {1, . . . , K}. In our work, we pre-train
the program synthesis proposed by Bunel et al. [4], where they use encoder-
decoder neural networks to generate the desired program given input-output
speciﬁcations. Note that the synthesizer is dependent on the input speciﬁcation,
that is, diﬀerent I/Oa and I/Ob may produce diﬀerent programs through the
synthesis, i.e., Ψ (I/Oa) = pa and Ψ (I/Ob) = pb. For a detailed discussion, e.g. of
the I/O encodings, we refer to Bunel et al. [4].

4.2 Sample Rejection Strategy

Naturally, we expect approximation errors of the optimization problem by the
generative model. Two main sources of error are (1) challenges to approximate
the discontinuous target distribution (2) only a limited number of constraints
can be incorporated in the conditional generative model. In order to correct for
these errors, we follow up with a sample rejection stage based on a scoring of the
generated program candidates. We use random I/Os obtained from interacting
with black-box function to evaluate the generated programs, and score them
based on the number of the I/Os which were covered by the programs (See
Figure 3). While in principle, any failed I/O should lead to rejecting a candidate,
empirically, we ﬁnd that keeping the highest scoring samples turns out to be
advantageous and prevents situations where no candidates would remain.

IReEn

7

4.3

Iterative Reﬁnement

We are still facing two major issues: (1) As we have motivated before and also
our experiments will show, querying for certain I/O pairs is more informative
than others. Hence, we seek an iterative approach that yields more informative
queries to the black box. (2) Due to the computational bottleneck, the conditional
generative model only takes a small number of constraints, while it is unclear
which constraints to use in order to arrive at the “functional equivalent” feasible
set.

Similar problems have been encountered in constraint optimization, where
column generation algorithms / delayed constraint generation techniques have
been employed to deal with large number of constraints [8]. Motivated by these
ideas, we propose an iterative strategy, where we condition in each step on a set
of violated constraints that we ﬁnd.

In detail, we present the algorithm of the proposed method in Algorithm
1. Iterative synthesis function takes synthesizer Ψ and a set of I/Os (line 1).
In line 2 we initial the sbest to zero. Note that we use pbest to store the best
candidate, and sbest to store the score of the best candidate. In the iterative
loop, we ﬁrst condition the program synthesizer on the given I/O set to get
the program candidates ˆP (line 5). Then we call Scoring function to score the
program candidates in line 6. The scoring function returns the best program
candidate, the score of that candidate, and the new set of I/O where the new
I/Os are the one which were not satisﬁed by ˆpbest. Note that ˆpbest, and ˆsbest store
the best candidate and the score of it for the current iteration. Then at line 7,
we check if ˆsbest for the current iteration is larger than the global score sbest, if
the condition satisﬁes we update the global pbest, and sbest (line 8-9). In line 12
we return the best candidate pbest after searching for it for n iterations.

4.4 Fine-tuning

The goal of synthesizer Ψ is to generate a program for the given I/Os, so it is
not desirable to generate a program that contains not-used statements (e.g. a
while statement which never hit by the given I/Os). However, in the black-box
setting, we only have access to the random I/Os, and there is no guarantee if
these I/Os represent all details of the black-box program. So the synthesizer
might need to generate a statement in the program which was not represented
in the given I/Os. The question is how we can have a synthesizer that makes a
balance between these two contradictory situations. To address this issue, we ﬁrst
train synthesizer Ψ on the crafted I/Os and then ﬁne-tune it on the random I/Os.
Please note that we only use the crafted I/Os during training. We get the data
for ﬁne-tuning by pairing random I/Os with the target programs. We empirically
ﬁnd that ﬁne-tuning the synthesizer can lead to better performance than training
it using only random I/Os.

5 Experiments

8

H. Hajipour et al.

In this section, we show the
eﬀectiveness of our proposed
approach for the task of black-
box program synthesis. We
consider Karel dataset [6,4]
in a strict black-box setting,
where we can only have ac-
cess to I/Os by querying the
black-box functions without
any privileged information or
informed sampling scheme.

5.1 The Karel Task and
Dataset

Algorithm 1: Iterative Algorithms

1 Function IterativeSynthesis(Ψ ,

I/O):

2

3

4

5

6

7

8

9

sbest = 0 // To keep the best

score.

n = constant // e.g., n=10
for i ← 1 to n do
ˆP = Ψ (I/O)
ˆpbest, ˆsbest, I/O = Scoring( ˆP )
if sbest < ˆsbest then
pbest = ˆpbest
sbest = ˆsbest

10

end

11

end
return pbest
12
13 End Function

To evaluate our proposed ap-
proach, we consider Karel pro-
gramming language. Karel featured a robot agent in a grid world, where this
robot can move inside the grid world and modify the state of the world using a set
of predeﬁned functions and control ﬂow structures. Recently it has been used as
a benchmark in several neural program synthesis works [4,21,5]. Figure 4 shows
the grammar speciﬁcation of this programming language [4]. Using control ﬂow
structures such as condition and loop in the grammar of Karel makes this DSL a
challenging language for the task of program synthesis. Figure 5 demonstrates
an example of the Karel task with two I/O examples and the corresponding
program.

Bunel et al. [4] deﬁned a dataset to train and evaluate neural program synthesis
approaches by randomly sampling programs from the Karel’s DSL. In this dataset,
for each program, there is 5 I/Os as speciﬁcation, and one is the held-out test
sample. In this work, we consider the Karel’s programs as black-box agent’s task,
and our goal is to reveal the underlying functionality of this black-box function
by solely using input-output interactions. This dataset contains 1,116,854 pairs of
I/Os and programs, 2,500 for validations, and 2,500 for testing the models. Note
that, to ﬁne-tune the synthesizer to the domain of random I/Os, we used 100,000
pairs of random I/Os and target program for training and 2,500 for validation.

5.2 Training and Inference

We train the neural program synthesizer using the Karel Dataset. To train this
synthesizer we employ the neural networks architecture proposed by Bunel et al.
[4], and use that in our iterative reﬁnement approach as the synthesizer. Note
that, to ﬁne-tune the synthesizer model on random I/Os we use Adam optimizer
[13] and the learning rate 10−5. We ﬁne-tune the synthesizer model for 10 epochs.
During inference, we use beam search algorithm with beam width 64 and select
top-k program candidates.

IReEn

9

Prog p := def run() : s
Stmt s := while(b) : s | repeat(r) : s | s1; s2 | a

| if(b) : s | ifelse(b) : s1 else : s2

Cond b := frontIsClear() | leftIsClear() | rightIsClear()

| markersPresent() | noMarkersPresent() | not b

Action a := move() | turnRight() | turnLeft()

| pickMarker() | putMarker()

Cste r := 0 | 1 | · · · | 19

Fig. 4: The grammar for the Karel programming language.

Fig. 5: Example of two I/Os of a Karel task with the corresponding underlying
program. The robot is Karel, the brick walls represent obstacles, and markers
are represented with circles.

5.3 Functional Equivalence Metric

In [4,21], two metrics have been used to evaluate the trained neural program
synthesizer. 1. Exact Match: A predicted program is an exact match of the target
if it is the same as the target program in terms of tokens. 2. Generalization:
A predicted program is a generalization of the target if it satisﬁes the I/Os of
the speciﬁcation set and the held-out example. Both of these metrics have some
drawbacks. A predicted program might be functionally equivalent to the target
program but not be the exact match. On the other side, a program can be a
generalization of the target program by satisfying a small set of I/Os (in Bunel et
al. [4] 5 I/Os has been used as speciﬁcation and 1 I/O is considered as held-out).
However, it might not cover a larger set of I/Os for that target program. To
overcome this issue, in this work we proposed the Functional Equivalence metric,
where we consider a predicted program as an approximately functional equivalent
to the target program if it covers a large set of I/Os which were not been used as
the speciﬁcation in the synthesizing time. To get the set of I/Os, we generate the
inputs randomly and query the program to get the outputs. We check if these

10

H. Hajipour et al.

inputs hit all of the branches of the target program. In our experiments, we found
that with using more I/Os the more predicted programs we discover to not be
functionally equivalent to the target programs. We found 100 number of I/Os as
a point where the number of approximately functional equivalent programs stay
stable in the evaluations.

5.4 Evaluation

Models

Generalization

Functional

Exact Match

top-1

top-50

top-1

top-50

top-1

top-50

Random I/Os
Random I/Os + FT
Random I/Os + IReEn
Random I/Os + FT + IReEn 78.96% 88.39% 65.55% 78.08% 44.51% 48.11%

57.12% 71.48% 49.36% 63.72% 34.96% 40.92%
64.72% 77.64% 55.64% 70.12% 39.44% 45.4%
76.20% 85.28% 61.64% 73.24% 40.95% 44.99%

Crafted I/Os ([4])

73.12% 86.28% 55.04% 68.72% 40.08% 43.08%

Table 1: Top: Results of performance comparison of our approach in diﬀerent
settings using random I/Os for black-box program synthesis. Random I/Os mean
that we use randomly obtained I/Os in the black-box setting, FT refers to ﬁne-
tuned model, and IReEn denote to our iterative approach. Bottom: Results of
Bunel et al. [4] when we use crafted I/Os. top-1 denote the results for the most
likely candidate, and top-50 denote the results for 50 most likely candidates.

We investigate the performance of our approach in diﬀerent settings to do the
task of black-box program synthesis. To evaluate our approach we query each
black-box program in the test set with 50 valid inputs to get the corresponding
outputs. Using the obtained 50 I/Os we synthesize the target program, where we
use 5 out of 50 I/Os to conditions on the synthesizer and use 50 I/Os to score
the generated candidate and ﬁnd the best one based on sample rejection strategy.
In our iterative approach in each iteration, using sample rejection strategy we
ﬁnd a new 5 I/Os among the 50 I/Os to condition on the synthesizer for the next
iteration. To evaluate the generated programs, in addition to generalization and
exact match accuracy, we also consider our proposed metric called Functional
Equivalence (subsection 5.3). To compute the functional equivalency we use 100
I/Os which were not seen by the model. If the generated program satisﬁes all of
100 I/Os we consider it as a program which approximately functional equivalent
to the target program. In all of the results top-k means that we use the given
I/Os to ﬁnd the best candidate among the "k" top candidates. For computing
the results for all of the metrics we evaluate the best candidate among top-k.

IReEn

11

Models

Action

Repeat

While

If

Mix

top-1

top-50 top-1

top-50

top-1

top-50

top-1

top-50

top-1

top-50

95.59% 99.69% 85.52% 91.44% 26.98% 61.58% 48.88% 72.69% 10.69% 27.12%
Random I/Os
99.39% 99.76% 90.72% 96.38% 56.50% 82.22% 52.06% 77.46% 14.33% 32.19%
Random I/Os + FT
Random I/Os + IReEn
99.84% 99.84% 96.38% 97.36% 60.95% 84.76% 81.26% 89.84% 27.67% 49.94%
Random I/Os + FT + IReEn 99.84% 100% 95.39% 99.64% 81.58% 93.33% 81.52% 92.06% 32.08% 56.22%

Crafted I/Os

99.08% 100.0% 91.11% 96.71% 54.28% 84.12% 49.20% 79.68% 14.88% 33.84%

Table 2: Top: Functional equivalence results of our approaches in synthesizing
black-box programs with diﬀerent complexity. Random I/Os means that we use
randomly obtained I/Os in the black-box setting, FT refers to ﬁne-tuned model,
and IReEn denotes to our iterative approach. Bottom: Results of Bunel et al. [4]
when we use crafted I/Os. Action refers to programs that only contain action
functions, Repeat denotes programs with action functions and only a repeat
structure, While denotes programs with action functions and only a while control
ﬂow, If refers to the programs with action functions and only an if control ﬂow,
and Mix denotes programs with more than one control ﬂow structures and action
functions. top-1 denotes the results for the most likely candidate, and top-50
denotes the results for 50 most likely candidates.

Comparison to baseline and ablation. Table 1 shows the performance of our
approach in diﬀerent settings in the top, and the results of the neural program
synthesizer proposed by Bunel et al. [4] in the bottom. These results show that
when we only use random I/Os (ﬁrst row), there is a huge drop in the accuracy
in all of the metrics in comparison to the results of crafted I/Os. However, when
we ﬁne-tune the synthesizer the results improve in all of the metrics, especially
for the top-1 and top-50 functional equivalence accuracy. Furthermore, when we
use our iterative approach for 10 iterations with the ﬁne-tuned model (fourth
row), we can see that our approach outperforms even the crafted I/Os in all of
the metrics. For example, it outperforms crafted I/Os in functional equivalence
and exact match metric by a large margin, 9%, and 5% respectively for top-50
results.

Importance of the crafted I/Os. In Table 1 in the top ﬁrst row (Random I/Os)
we use random I/Os to condition on the synthesizer, and in the bottom (Crafted
I/Os) we use crafted I/Os to condition on the same synthesizer. These results
show that using random I/Os on the same synthesizer leads to 15%, and 5%
drops in the results for top-50 generalization and functional accuracy respectively.
Based on these results we can see that random I/Os contain signiﬁcantly less
information about the target program in comparison to the crafted I/Os.

To further investigate the importance of the crafted I/Os we provide the results
of synthesizing programs with diﬀerent levels of complexity in Table 2. In this table
(Table 2) we show the functional equivalency results of simple programs including
programs that only contain action functions or Repeat structure with action

12

H. Hajipour et al.

functions, and also complex programs that contain one or multiple conditional
control ﬂows.
In Table 2 we can see that for simple programs that only contains action functions
or action functions with Repeat structure (Note that Repeat is like for-loop
structure, so any valid input can hit a Repeat structure) we have low performance
drops in functional equivalence accuracy for Random I/Os in comparison to
Crafted I/Os. For example, in the Action column (Table 2) for top-50 accuracy,
there is less than 1% points drop for Random I/Os in comparison to the Crafted
I/Os results. This is because any I/O examples can represent the functionality of
these simple programs. In other words, any I/Os hits all parts of these simple
programs. Furthermore, Table 2 shows that for more complex programs we have
large drop in functional equivalence accuracy for Random I/Os in comparison
to Crafted I/Os. As an example, we can see that for programs with While
structure in top-1 column, there is more than 27% points drop for Random I/Os
in comparison to the Crafted I/Os results. These results indicate that Crafted
I/Os contain more informative details about the complex programs in comparison
to random I/Os. This is because Crafted I/Os are designed to hit all branches of
the programs. However, there is no guarantee if the given Random I/Os hit all
of the branches of the complex programs.

Table 2 also provides the results of our iterative approach with and without
ﬁne-tuning the model. In this table (Table 2) we can see that for the program with
complex control ﬂows our approaches have higher performance gain in comparison
to the results for the simple program. This indicates that our iterative reﬁnement
approach is capable of generating more accurate programs by iteratively condition
the model on informative I/Os. As an example, for the program with multiple
control ﬂows (Mix) in the top-1 column, we have around 17% points improvement
for Random I/Os + IReEn in comparison to Random I/Os.

Eﬀectiveness of iterative reﬁnement. Figure 6a, Figure 6b, and Figure 6c show the
eﬀectiveness of our proposed iterative approach in 10 iterations. In these ﬁgures,
x and y axis refer to the number of iterations and the accuracy respectively.
Figure 6a shows the generalization accuracy for top-50, in Figure 6b we can see
the results of functional equivalence metric for top-50, and Figure 6c demonstrates
the exact match accuracy for top-50. In these ﬁgures, we provide results with
and without ﬁne-tuning the synthesizer. Here we can see the improvement of
the generalization, functional equivalence, and exact match accuracy over the
iterations. We have a margin of 7% improvement in the functional equivalence
accuracy for "Random I/Os + FT + IReEn" setting after 10 iterations. In other
words, these results show that we can search for better random I/Os and program
candidates by iteratively incorporate additional constraints.

Eﬀectiveness of number of I/Os for sample rejection strategy. In our approach in
order to choose one candidate among all of the generated program candidates,
we consider a sample rejection strategy. To do that, we use the random I/Os
to assign a score to the generated candidates based on the number of satisﬁed
random I/Os. Finally, we consider the candidate with the higher score as the

IReEn

13

best candidate and reject the rest. Figure 7a, Figure 7b, and Figure 7c show the
eﬀect of using the diﬀerent numbers of random I/Os on scoring the candidates
and ﬁnding the best program candidate. x and y axis in these ﬁgures refer to
the number of random I/Os, and accuracy of our approaches with and without
ﬁne-tuning. In Figure 7a we show the generalization accuracy, in Figure 7b we
provide functional equivalence results, and Figure 7c shows the results for exact
match accuracy. These ﬁgures show that by using more random I/Os in the
sample rejection strategy we can ﬁnd more accurate programs that result to gain
better performance in terms of generalization, functional equivalence, and exact
match accuracy. In other words, with using more random I/Os for scoring the
candidates we can capture more details of the black-box function, and ﬁnd the
best potential candidate among the generated one.

(a)

(b)

(c)
Fig. 6: (a) Generalization accuracy after each iteration for "Random I/Os +
IReEn" and "Random I/Os + FT + IReEn". (b) Functional equivalence accuracy
after each iteration for "Random I/Os+ IReEn" and "Random I/Os + FT +
IReEn". (c) Exact match accuracy after each iteration for "Random I/Os +
IReEn" and "Random I/Os + FT + IReEn". Note that, Random I/Os means
that we use randomly obtained I/Os, FT denotes to the ﬁne-tuned model, and
IReEn refers to our iterative approach.

14

H. Hajipour et al.

(a)

(b)

(c)
Fig. 7: (a) Generalization accuracy after using the diﬀerent numbers of random
I/Os in scoring strategy for "Random I/Os" and "Random I/Os + FT". (b)
Functional equivalence accuracy after using the diﬀerent numbers of random I/Os
in scoring strategy for "Random I/Os" and "Random I/Os + FT". (c) Exact
match accuracy after using diﬀerent numbers of random I/Os in scoring strategy
for "Random I/Os" and "Random I/Os + FT". Note that, Random I/Os means
that we use randomly obtained I/Os, FT denotes to the ﬁne-tuned model, and
IReEn refers to our iterative approach.

6 Conclusion

In this work, we propose an iterative neural program synthesis scheme to reverse-
engineer the black-box functions and represent them in a high-level program. In
contrast to previous works, where they have access to privileged information, in
our problem setting, we only rely on the input-output interactions. To tackle
the problem of reverse-engineering the black-box function in this challenging
setting, we employ a neural program synthesizer in an iterative scheme. Using
this iterative approach we search for the best program candidate in each iteration
by conditioning the synthesizer on a set of violated constraints. Our evaluation on
the Karel dataset demonstrates the eﬀectiveness of our proposed approach in the
reverse-engineering functional equivalent form of the black-box programs. Besides
this, the provided results show that our proposed approach even outperforming the
previous work that uses privileged information to sample input-output examples.

IReEn

15

References

1. Hex-rays (1998), https://www.hex-rays.com/products/ida/
2. Bastani, O., Pu, Y., Solar-Lezama, A.: Veriﬁable reinforcement learning via policy

extraction. In: NeurIPS (2018)

3. Brumley, D., Lee, J., Schwartz, E.J., Woo, M.: Native x86 decompilation using
semantics-preserving structural analysis and iterative control-ﬂow structuring. In:
USENIX (2013)

4. Bunel, R., Hausknecht, M., Devlin, J., Singh, R., Kohli, P.: Leveraging grammar

and reinforcement learning for neural program synthesis. In: ICLR (2018)

5. Chen, X., Liu, C., Song, D.: Execution-guided neural program synthesis. In: ICLR

(2019)

6. Devlin, J., Bunel, R.R., Singh, R., Hausknecht, M., Kohli, P.: Neural program

meta-induction. In: NIPS (2017)

7. Devlin, J., Uesato, J., Bhupatiraju, S., Singh, R., Mohamed, A.r., Kohli, P.: Ro-

bustﬁll: Neural program learning under noisy i/o. In: ICML (2017)

8. Ford Jr, L.R., Fulkerson, D.R.: A suggested computation for maximal multi-

commodity network ﬂows. Management Science (1958)

9. Fu, C., Chen, H., Liu, H., Chen, X., Tian, Y., Koushanfar, F., Zhao, J.: Coda: An

end-to-end neural program decompiler. In: NeurIPS (2019)

10. Graves, A., Wayne, G., Danihelka, I.: Neural turing machines. arXiv (2014)
11. Jha, S., Gulwani, S., Seshia, S.A., Tiwari, A.: Oracle-guided component-based

program synthesis. In: ICSE (2010)

12. Johnson, J., Hariharan, B., Van Der Maaten, L., Hoﬀman, J., Fei-Fei, L.,
Lawrence Zitnick, C., Girshick, R.: Inferring and executing programs for visual
reasoning. In: CVPR (2017)

13. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR (2015)
14. Kolbitsch, C., Comparetti, P.M., Kruegel, C., Kirda, E., Zhou, X.y., Wang, X.:
Eﬀective and eﬃcient malware detection at the end host. In: USENIX (2009)
15. Lee, J., Avgerinos, T., Brumley, D.: Tie: Principled reverse engineering of types in

binary programs (2011)

16. Manna, Z., Waldinger, R.: Knowledge and reasoning in program synthesis. Artiﬁcial

intelligence (1975)

17. Markram, H.: The human brain project. Scientiﬁc American (2012)
18. Oh, S.J., Schiele, B., Fritz, M.: Towards reverse-engineering black-box neural

networks. In: ICLR (2018)

19. Orekondy, T., Schiele, B., Fritz, M.: Knockoﬀ nets: Stealing functionality of black-

box models. In: CVPR (2019)

20. Pattis, R.E.: Karel the robot: a gentle introduction to the art of programming.

John Wiley & Sons, Inc. (1981)

21. Shin, E.C., Polosukhin, I., Song, D.: Improving neural program synthesis with

inferred execution traces. In: NeurIPS (2018)

22. Solar-Lezama, A., Tancau, L., Bodik, R., Seshia, S., Saraswat, V.: Combinatorial

sketching for ﬁnite programs. In: ASPLOS (2006)

23. Verma, A., Murali, V., Singh, R., Kohli, P., Chaudhuri, S.: Programmatically

interpretable reinforcement learning. In: ICML (2018)

24. Waldinger, R.J., Lee, R.C.: Prow: A step toward automatic program writing. In:

IJCAI (1969)

25. Yakdan, K., Dechand, S., Gerhards-Padilla, E., Smith, M.: Helping johnny to
analyze malware: A usability-optimized decompiler and malware analysis user study.
In: Security and Privacy (SP) (2016)

