An Efﬁcient Adversarial Attack for Tree Ensembles

Chong Zhang

Huan Zhang
Department of Computer Science, UCLA
chongz@cs.ucla.edu, huan@huan-zhang.com, chohsieh@cs.ucla.edu

Cho-Jui Hsieh

0
2
0
2

t
c
O
2
2

]

G
L
.
s
c
[

1
v
8
9
5
1
1
.
0
1
0
2
:
v
i
X
r
a

Abstract

We study the problem of efﬁcient adversarial attacks on tree based ensembles such
as gradient boosting decision trees (GBDTs) and random forests (RFs). Since these
models are non-continuous step functions and gradient does not exist, most existing
efﬁcient adversarial attacks are not applicable. Although decision-based black-box
attacks can be applied, they cannot utilize the special structure of trees. In our
work, we transform the attack problem into a discrete search problem specially
designed for tree ensembles, where the goal is to ﬁnd a valid “leaf tuple” that
leads to mis-classiﬁcation while having the shortest distance to the original input.
With this formulation, we show that a simple yet effective greedy algorithm can
be applied to iteratively optimize the adversarial example by moving the leaf
tuple to its neighborhood within hamming distance 1. Experimental results on
several large GBDT and RF models with up to hundreds of trees demonstrate
that our method can be thousands of times faster than the previous mixed-integer
linear programming (MILP) based approach, while also providing smaller (better)
adversarial examples than decision-based black-box attacks on general (cid:96)p (p =
1, 2, ∞) norm perturbations. Our code is available at https://github.com/
chong-z/tree-ensemble-attack.

1

Introduction

It has been widely studied that machine learning models are vulnerable to adversarial examples
(Szegedy et al., 2013; Goodfellow et al., 2015; Athalye et al., 2018), where a small imperceptible
perturbation on the input can easily alter the prediction of a model. A series of adversarial attack
methods have been proposed on continuous models such as neural networks, which can be generally
split into two types. The gradient based methods formulate the attack into an optimization problem
on a specially designed loss function for attacks, where the gradient can be acquired through either
back-propagation in the white-box setting (Carlini, Wagner, 2017; Madry et al., 2018), or numerical
estimation in the soft-label black-box setting (Chen et al., 2017; Tu et al., 2018; Ilyas et al., 2018).
The decision based (or hard-label black-box) methods only have access to the output label, which
usually starts with an initial adversarial example and minimizes the perturbation along the decision
boundary (Brendel et al., 2018; Brunner et al., 2018; Cheng et al., 2019, 2020; Chen et al., 2019c).

In this paper we study the problem of efﬁcient adversarial attack on tree based ensembles such as
gradient boosting decision trees (GBDT) and random forests (RFs), which have been widely used
in practice (Chen, Guestrin, 2016; Ke et al., 2017; Zhang et al., 2017; Prokhorenkova et al., 2018).
We minimize the perturbation to ﬁnd the smallest possible attack, to uncover the true weakness of
a model. Different from neural networks, tree based ensembles are non-continuous step functions
and existing gradient based methods are not applicable. Decision based methods can be applied but
they usually require a large number of queries and may easily fall into local optimum due to rugged
decision boundary. In general, ﬁnding the exact minimal adversarial perturbation for tree ensembles
is NP-complete (Kantchelian et al., 2015), and a feasible approximation solution is necessary to
evaluate the robustness of large ensembles.

34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

 
 
 
 
 
 
The major difﬁculty of attacking tree ensembles is that the prediction remains unchanged within
regions on the input space, where the region could be large and makes continuous updates inef-
ﬁcient. To overcome this difﬁculty, we transform the continuous Rd input space into a discrete
{1, 2, . . . , N }K “leaf tuple” space, where N is the number of leaves per tree and K is the number of
trees. On the leaf tuple space we deﬁne the distance between two input examples to be the number of
trees that have different prediction leaves (i.e., hamming distance), and deﬁne the neighborhood of a
tuple to be all valid tuples within a small hamming distance. In practice, we propose the attack that
iteratively optimizes the adversarial leaf tuple by moving it to the best adversarial tuple within the
neighborhood of distance 1. Intuitively we could reach a far away adversarial tuple through a series
of smaller updates, based on the fact that each tree makes prediction independently.

In experiments, we compare (cid:96)1,2,∞ norm perturbation metrics across 10 datasets, and show that
our method is thousands of times faster than MILP (Kantchelian et al., 2015) on most of the large
ensembles, and 3∼72x faster than decision based and empirical attacks on all datasets while achieving
a smaller distortion. For instance, with the standard (natural) GBDT on the MNIST dataset with
10 classes and 200 trees per class, our method ﬁnds the adversarial example with only 2.07 times
larger (cid:96)∞ perturbation than the optimal solution produced by MILP and only uses 0.237 seconds
per test example, whereas MILP requires 375 seconds. As for other approximate attacks, SignOPT
(Cheng et al., 2020) ﬁnds a 13.93 times larger (cid:96)∞ perturbation (compared to MILP) using 3.7 seconds,
HSJA (Chen et al., 2019c) achieves a 8.36 times larger (cid:96)∞ perturbation using 1.8 seconds, and
Cube (Andriushchenko, Hein, 2019) achieves a 4 times larger (cid:96)∞ perturbation using 4.42 seconds.
Additionally, although (cid:96)p distance is widely used in previous attacks and a small (cid:96)p perturbation is
usually invisible, our method is general and can also be adapted to other distance metrics.

2 Background and Related Work

Problem Setting While the main idea can be applied to multi-class classiﬁcation models and
targeted attacks, for simplicity we consider a binary classiﬁcation model f : Rd → {−1, 1} consisting
of K decision trees. Each tree t is a weak learner ft : Rd → R of N leaves, and the ensemble returns
the sign f (x) = sign((cid:80)K
t=1 ft(x)). Given a victim input example x0 with y0 = f (x0), we want to
ﬁnd the minimal adversarial perturbation r∗
p, determining the adversarial robustness under (cid:96)p
norm:

r∗
p = min

δ

(cid:107)δ(cid:107)p

s.t.

f (x0 + δ) (cid:54)= y0.

(1)

Exact Solutions
In general computing the exact (optimal) solution for Eq. (1) requires exponential
time: Kantchelian et al. (2015) showed that the problem is NP-complete for general ensembles and
proposed a MILP based method; On the other hand, faster algorithms exist for models of special
form: Zhang et al. (2020) restricted both the input and prediction of every tree t to binary values
ft : {−1, 1}d → {−1, 1} and provided an integer linear program (ILP) based formulation about
4 times faster than Kantchelian et al. (2015); Andriushchenko, Hein (2019) showed that the exact
robustness of boosted decision stumps (i.e., depth = 1) can be solved in polynomial time; Chen et al.
(2019b) proposed a polynomial time algorithm to solve a single decision tree.

Approximate Solutions
To get a feasible solution for general models a series of methods have been
proposed to compute the lower bound (robustness veriﬁcation) and the upper bound (adversarial
attacks) of r∗
p. Chen et al. (2019b) formulated the robustness veriﬁcation problem into a max-clique
problem on a multi-partite graph and produced the lower bound on (cid:96)∞; Wang et al. (2020) extended
the veriﬁcation method and produced a lower bound on general (cid:96)p norms; Lee et al. (2019) veriﬁed
the (cid:96)0 robustness on the randomly smoothed ensembles which is not directly related to our work.
On the other hand, decision based attacks (Brendel et al., 2018; Brunner et al., 2018; Cheng et al.,
2019, 2020; Chen et al., 2019c) can be applied here to produce an upper bound since they don’t
have architecture or smoothness assumptions on f (·), however they are usually ineffective due to
the discrete nature of tree models; Andriushchenko, Hein (2019) proposed the Cube attack for tree
ensembles that does stochastic updates along the (cid:96)∞ boundary, which typically achieves better results
than decision based attacks; Yang et al. (2019) focused on the theoretical analysis of search space
decomposition, and proposed RBA-Appr to search over a subset of the N K convex polyhedrons
containing training examples; Zhang et al. (2020) restricted both the input and prediction of all trees to
binary values and provided a heuristic attack on (cid:96)0 by assigning empirical weights to each feature, in
both the white-box setting and a special “black-box” setting via training substitute models. In contrast,

2

Table 1: Key differences to prior adversarial attacks that are applicable to general tree ensembles.

SignOPT

HSJA

Cube

RBA-Appr

Ours

Access Level
Search Space
Step Size
Model Queries / iteration

black-box
input space
small steps in continuous space

200

100∼632

black-box
input space
(cid:96)0 boundary
100

white-box + data
training data
N/A
N/A

white-box
leaf tuple space
one leaf node
∼1 (see §3.4.1)

our method works on ensemble of general trees ft : Rd → R, and utilizes the special properties of
tree models to produce a tighter upper bound on general (cid:96)p (p = 1, 2, ∞) norms efﬁciently. Table 1
highlights our key differences to prior adversarial attacks that are applicable to general tree ensembles.

Robust Training
To overcome the vulnerability, Kantchelian et al. (2015) proposed adversarial
boosting by appending adversarial examples to the training dataset; Chen et al. (2019a) optimized
the worst case perturbation through a max-min saddle point problem and effectively increased the
minimal adversarial perturbation; Andriushchenko, Hein (2019) upper bounded the robust test error
by the sum of the max loss of each tree, and proposed a training scheme by minimizing this upper
bound; Recently Chen et al. (2019d) approximated the saddle point objective with a greedy heuristic
algorithm and further increased the robustness. We consider both the standard (natural) models and
the robustly trained models (Chen et al., 2019a) to demonstrate our performance on different settings.

3 Proposed Algorithm

We propose an iterative approach where the algorithm starts with an initial adversarial example x(cid:48) s.t.
f (x(cid:48)) (cid:54)= y0 and greedily moves closer to x0. At each iteration we choose a new adversarial example
x(cid:48)
new within a small neighborhood around x(cid:48) that has the minimum (cid:96)p distance to x0. Formally we
deﬁne the update rule below, and the algorithm stops if x(cid:48)
new does not give smaller perturbation than
x(cid:48). The key problem is to deﬁne the neighborhood so that Eq. (2) can be efﬁciently solved, and we
provide an efﬁcient formulation in following sections. We defer all the proofs to Appendix C.

x(cid:48)
new = argmin

x

(cid:107)x − x0(cid:107)p

s.t.

x ∈ Neighbor(x(cid:48)), f (x) (cid:54)= y0.

(2)

3.1 Transform Continuous Input Space into Discrete Leaf Tuples

In most of the existing attacks, Eq. (2) is solved by a continuous optimization algorithm where the
Neighbor(x(cid:48)) (the region where we ﬁnd an improved solution) is a small (cid:96)p ball around the current
solution x(cid:48). The major difﬁculty of attacking tree ensemble is that the model prediction will remain
unchanged within a region (which may not be small) containing x(cid:48), so traditional continuous distance
measurements are not suitable here. And if we deﬁne the neighborhood as a large (cid:96)p ball, due to
the non-continuity of trees, f (x) becomes intractable to enumerate. To handle these difﬁculties we
introduce the concept of leaf tuple and rewrite Eq. (2) into a discrete form. Given an input example
x = [x1, . . . , xd] the traverse starts from the root node of each tree. Each internal node of index i has
two children and a feature split threshold (ji, ηi), and x will be passed to the left child if xji ≤ ηi
and to the right child otherwise. The leaf node has a prediction label vi, and will output vi when x
reaches here. We use C(x) = (i(1)(x), . . . , i(K)(x)) to denote the index tuple of K prediction leaves
from input x. In general we use the subscript ·j to denote the jth dimension, and the superscripts ·i or
·(t) to denote the ith node and tth tree respectively.
1] × · · · × (li
Deﬁnition 1. (Bounding Box) Bi = (li
d] denotes the bounding box of node i,
which is the region that x will fall into this node following the feature split thresholds along the
traverse path, and each l and r is either ±∞ or equals to one ηik along the traverse path. We use
B(C(x)) = (cid:84)
d] to denote the Cartesian product
of the intersection of K bounding boxes on the ensemble.
Deﬁnition 2. (Valid Tuple) C = {C(x) | ∀x ∈ Rd} denotes the set of all possible tuples that
correspond to at least one point in the input space, and C = (i(1), . . . , i(K)) is a valid tuple iff. C ∈ C.

i∈C(x) Bi = (cid:84)

1] × · · · × (cid:84)

i∈C(x)(li

i∈C(x)(li

d, ri

d, ri

1, ri

1, ri

3

x1 ≤ 3

x2 ≤ 2

x2 ≤ 5

1

-20

2

5

3

5

4

-5

x2 ≤ 10

x2 ≤ 20

x1 ≤ 15

x1 ≤ 5

x1 ≤ 10

x1 ≤ 20

5

-1

6

10

7

1

8

3

9

3

10

10

11

1

12

10

2
x

25
23

20

15

10

5

2
0

d

x0

y0 = +1

our path

c

b

a

y(cid:48) = −1

x(cid:48)

decision based path

0

3

5

10

15

20

23

25

x1

Figure 1: An ensemble deﬁned on [0, 25]2 and its corresponding decision boundaries on the input
space. Numbers inside circles are indexes of leaves, and the number below each leaf is the correspond-
ing prediction label vi. For clarity we mark boundaries belong to tree 1, 2, 3 with red, green, and
blue respectively, and ﬁll +1 area with gray. x0 is the victim example and x(cid:48) is an initial adversarial
example. Assume we are optimizing (cid:96)1 perturbation, our method can reach d by changing one leaf of
the tuple at a time (black arrows). On the other hand, decision based attacks update the solution along
the decision boundary, and easily fall into local minimums such as x(cid:48) and a (brown arrows) since
they look only at the continuous neighborhood. To move from a to b, the path on decision boundary
is a → (5, 10) → b, but since a → (5, 10) increases the distortion they won’t ﬁnd this path.

Theorem 1. (Chen et al., 2019b) The intersection B(C) can also be written as the Cartesian product
of d intervals (similar to Bi), and C ∈ C ⇐⇒ B(C) (cid:54)= ∅ ⇐⇒ ∀i, j ∈ C, Bi ∩ Bj (cid:54)= ∅. (These
concepts were used in Chen et al. 2019b for veriﬁcation instead of attack.)
Corollary 1. (Tuple to Example Distance) The shortest distance between a valid leaf tuple and an
example, deﬁned as distp(C, x0) = minx∈B(C) (cid:107)x − x0(cid:107)p, can be solved in O(d) time.

Observe C(x) = C(x(cid:48)), ∀x ∈ B(C(x(cid:48))) and we can transform the intractable number of x into
tractable number of leaf tuples C. We abuse the notation f (C) to denote the model prediction
sign((cid:80)
i∈C vi), which is a constant within B(C). Combined with Corollary 1 we can rewrite Eq. (2)
into the discrete form below. Neighbor(C(cid:48)) denotes the neighborhood space around C(cid:48), which is a
set of leaf tuples that close to C(cid:48) in certain distance measurements. Fig. 1 presents an example to
demonstrate that it’s less likely to fall into local optimum on our newly deﬁned neighborhood space.

C(cid:48)
new = argmin

C

distp(C, x0)

s.t.

C ∈ Neighbor(C(cid:48)) ∩ C, f (C) (cid:54)= y0.

(3)

3.2 Limitations of Naive Neighborhood Space Deﬁnitions

Now we discuss how to deﬁne Neighbor(C(cid:48)) to facilitate our attack. Intuitively the space should be
efﬁcient to compute, and has a reasonable coverage to avoid falling into local minimums too easily. In
this section we discuss two naive approaches that fail on these two properties, and provide empirical
results in Table 2.
Enumerating all leaves is not efﬁcient (NaiveLeaf ): Given current adversarial example x(cid:48) and
its corresponding leaf tuple C(cid:48) = (i(1), . . . , i(K)), an intuitive approach is to change a single i(t) to
a different leaf i(t)
new. However the resulting tuple may not be valid, and we will have to query the
ensemble to get a valid tuple in C. We provide a possible implementation in Appendix D.2 where we
move x(cid:48) to the closest point within Bi(t)
new . NaiveLeaf requires multiple full model queries and takes
O(K · 2l · Kl) time per iteration for K trees of depth l, which is too time consuming (see Table 2).

Mutating one feature at a time has poor coverage (NaiveFeature): Given current adversarial
example x(cid:48) and its corresponding leaf tuple C(cid:48), another intuitive approach is to move x(cid:48) outside of
B(C(cid:48)) on each feature dimension. This approach is efﬁcient since there are at most 2d neighborhood,
and each neighborhood is only different by one tree (assuming unique split thresholds). However the

4

Table 2: Average (cid:96)2 perturbation over 500 test examples on the standard (natural) GBDT models.
("*"): For a fair comparison we disabled the random noise optimization discussed in §3.5. Our
LT-Attack searches in a subspace of NaiveLeaf so ¯rour is slightly larger, but it is signiﬁcantly faster.

Standard GBDT

NaiveLeaf

NaiveFeature LT-Attack (Ours)* Ours vs. NaiveLeaf

(cid:96)2 Perturbation

MNIST
F-MNIST
HIGGS

¯rleaf
.081
.080
.008

time

2.37s
3.93s
3.17s

¯r

.229
.181
.011

time

.069s
.061s
.023s

¯rour
.108
.096
.009

time

.105s
.224s
.031s

¯rour/¯rleaf
1.33
1.20
1.13

Speedup

22.6X
17.5X
102.3X

method easily falls into local minimums due to the fact that each leaf is bounded by up to l features
jointly, thus it’s unlikely to reach certain leaves by only changing one feature at a time. Taking Fig. 1
as an example and assume we are at x(cid:48), notice that B(C(x(cid:48))) = [0, 3] × [0, 2] and the algorithm stops
here since both neighborhood {(3 + (cid:15), 2), (3, 2 + (cid:15))} are not adversarial examples.

3.3 Deﬁne Neighborhood Space by Hamming Distance

In this section we introduce a neighborhood space through discrete hamming distance, and show that
it’s fast to compute and has good coverage. We deﬁne the distance D(C, C(cid:48)) between two tuples as
the number of different leaves, and the neighborhood of C(cid:48) with distance h by

Neighborh(C(cid:48)) = {C | ∀C ∈ C, D(C, C(cid:48)) = h}.
The intuition is that each tree can be queried independently, and we want to utilize such property by
limiting the number of affected trees at each iteration. Neighborh(·) has a nice property where we
can increase h for larger search scope, or decrease h to improve speed. Observe that Neighbor1(C(cid:48))
is a subset of NaiveLeaf (minus invalid leaf tuples that requires an expensive model query), and a
superset of NaiveFeature (plus leaf tuples that may affect multiple features). In experiments we are
able to achieve good results with Neighbor1(·), and we provide an empirical greedy algorithm in
Appendix D.3 to estimate the minimal h required to reach the exact solution.

(4)

3.4 An Efﬁcient Algorithm for Solving Neighbor1(·)

We propose Leaf Tuple attack (LT-Attack) in Algorithm 1 that efﬁciently solves Eq. (3) through two
additional concepts TBound(·) and Neighbor(t)
1 (·) as deﬁned below. Let C(cid:48) be any valid adversarial
tuple, and assume unique feature split thresholds. By deﬁnition tuples in Neighbor1(C(cid:48)) are only
different from C(cid:48) by one leaf, and we use Neighbor(t)
1 (C(cid:48)) to denote the neighborhood that has
different prediction leaf on tree t. Formally

Neighbor(t)

1 (C(cid:48)) = {C | C(t) (cid:54)= C(cid:48)(t), C ∈ Neighbor1(C(cid:48))}.

(5)
Deﬁnition 3. (Bound Trees) Let x(cid:48) ∈ B(C(cid:48)) be the example that minimizes distp(C(cid:48), x0), we denote
the indexes of trees that bounds x(cid:48) by TBound(C(cid:48)) = {t | OnEdge(x(cid:48), BC(cid:48)(t)
), ∀t ∈ {1, . . . , K}}.
Here OnEdge(x, B) is true iff. x equals to the left or the right bound of B on at least one dimension.
Deﬁnition 4. (Advanced Neighborhood) We denote the set of neighborhood with smaller (advanced)
perturbation than C(cid:48) by Neighbor+
1 (C(cid:48)) = {C | C ∈ Neighbor1(C(cid:48)), distp(C, x0) < distp(C(cid:48), x0)}.
Theorem 2. (Bound Neighborhood) Let NeighborBound(C(cid:48)) = (cid:83)
1 (C(cid:48)), then
Neighbor+

t∈TBound(C(cid:48)) Neighbor(t)

1 (C(cid:48)) ⊆ NeighborBound(C(cid:48)).

Theorem 2 suggests that we can solve Eq. (3) by searching over NeighborBound(C(cid:48)) since it is a
superset of the advanced neighborhood which leads to smaller perturbation. In general the algorithm
consists of an outer loop and an inner NeighborBound(·) function. The outer loop iterates until no
better adversarial example can be found, while the inner function generates bound neighborhood with
distance 1. The inner function computes TBound and runs the top-down traverse for each t ∈ TBound
with the intersection of other K − 1 bounding boxes, denoted by B(−t). According to Theorem 1, a
leaf node of t is guaranteed to form a valid tuple if it has non-empty intersection with B(−t).

To efﬁciently obtain B(−t) we cache K bounding boxes in B(cid:48), and for each feature dimension we
maintain the sorted list of left and right bounds from K boxes respectively. Note that Bi of leaf node i

5

Algorithm 1: Our proposed LT-Attack for constructing adversarial examples.
Data: White-box model f , victim example x0, y0, initial adversarial example x(cid:48).

1 begin
2

3

4

5

6

7

8

9

10

11

12

13

14

15
16 end

17 Function NeighborBound(C(cid:48), B(cid:48), f ):

r(cid:48), C(cid:48) ← distp(C(x(cid:48)), x0), C(x(cid:48));
B(cid:48) ← BuildSortedBoxes(C(cid:48), f );
(cid:46) O(Kl log K) - B(cid:48) maintains K sorted bounding

18

19

N1 ← ∅;
T ← TBound(C(cid:48), B(cid:48));
(cid:46) O(d log K) - Need O(log K) to get the first

boxes on d feature dimensions.

has_better_neighbor ← True;
while has_better_neighbor do

N1 ← NeighborBound(C(cid:48), B(cid:48), f );
(cid:46) See complexity in §3.4.1
N (cid:48)
(cid:46) O(|N1|) - f (C) can be calculated from
f (C(cid:48)) in O(1) using the diff leaf.

1 ← {C | C ∈ N1, f (C) (cid:54)= y0};

r∗, C∗ ← argmin

{distp(C, x0) | C ∈ N (cid:48)

1};

r,C

(cid:46) O(l|N (cid:48)

1|) - distp(C, x0) can be calculated

from r(cid:48) in O(l) using the diff leaf.

has_better_neighbor ← r∗ < r(cid:48);
if has_better_neighbor then
r(cid:48), C(cid:48) ← r∗, C∗;
B(cid:48) ← B(cid:48).ReplaceBox(C∗);
(cid:46) O(l log K) - Remove and add one box.

end

end
return r(cid:48), C(cid:48)

(tightest) tree on each dimension, and assume
C(cid:48) caches the closet x(cid:48) to x0. We give a
closer complexity analysis for |TBound(·)| in
the following section.

for t ∈ T do

B(−t) ← B(cid:48).RemoveBox(t);
(cid:46) O(l log K) - Remove the bounding box of

tree t from the sorted list (lazily), the
box has at most l non-infinite dimensions.
I ← {i | Bi ∩ B(−t) (cid:54)= ∅, i ∈ S(t) \ C(cid:48)(t)};
(cid:46) O(2l) - Traverse tree t top-down and

return leaves for Neighbor(t)
denotes the set of leaves of tree t.

1 (C(cid:48)). S(t)

N1 ← N1 ∪ {C(cid:48) | C(cid:48)(t) ← i, i ∈ I};
(cid:46) O(|I|) - Construct the neighborhood tuple
by replacing the tth leaf. In practice,
we only need to return the diff (t, i).

20

21

22

23

end
return N1

24

25
26 end

comes from feature split thresholds along the top-down traverse path, thus it has at most l non-inﬁnite
dimensions, where l is the depth of the tree. In conclusion we can add/remove a bounding box Bi
to/from B(cid:48) in O(l log K) time. We provide time complexity for most operations in Algorithm 1 inline,
and give a detailed analysis for the size of NeighborBound(C(cid:48)) in the next section. See Appendix D.1
for the algorithm generating random initial adversarial examples.

3.4.1 Size of the Bound Neighborhood

Our LT-Attack enumerates all leaf tuples in the bound neighborhood at each iteration, thus the
complexity of each iteration largely depends on the size of NeighborBound(C(cid:48)). In this section we
analyze the size | NeighborBound(C(cid:48))| and show it will not be too large on real datasets.
Corollary 2. (Size of Neighbor(t)
j
feature split thresholds inside B(−t), we have | Neighbor(t)
(cid:84)

, (j, η) ∈ H(t)}| be the number of
1 (C(cid:48))| ≤ 2min(k(t),l) − 1. Here B(−t) =
i∈C(cid:48),i(cid:54)=C(cid:48)(t) Bi and H(t) denotes the set of feature split thresholds on all internal nodes of tree t.

1 (C(cid:48))) Let k(t) = |{η ∈ B(−t)

In practice, k(t) (cid:28) l since B(−t) is the intersection of K − 1 bounding boxes and only covers a small
region of the input space Rd. |TBound(C(cid:48))| ≤ d and is also usually small in real datasets, which can be
explained by the intuition that some features are less important and could reach the same value as x0
easily. Both |TBound(C(cid:48))| and | Neighbor(t)
1 (C(cid:48))| characterize the complexity of | NeighborBound(C(cid:48))|.
We provide empirical statistics in Appendix A, which suggests that | NeighborBound(C(cid:48))| has the
similar complexity as a single full model query. For instance, on the MNIST dataset with 784 features
and 400 trees we have mean | NeighborBound(C(cid:48))| ≈ 367.9, and the algorithm stops in ∼159.4
iterations when it cannot ﬁnd a better neighborhood. As a comparison, decision based methods
usually require hundreds of full model queries per iteration to estimate the update direction.

3.4.2 Convergence Guarantee with Neighbor1(·)
In this section ¯C denotes the converged tuple when the outer loop stops, and we discuss the property
of the converged solution. Trivially ¯C has the minimal adversarial perturbation within Neighbor1( ¯C),
and we can show that the guarantee is actually stronger.

6

Table 3: Average (cid:96)∞ and (cid:96)2 perturbation of 500 test examples (or the entire test set when its size
is less than 500) on standard (natural) GBDT models. Datasets are ordered by training data size.
Bold and blue highlight the best and the second best entries respectively (not including MILP).
("*"): Average of 50 examples due to long running time. ("(cid:63)"): HSJA has ﬂuctuating running time.

Standard GBDT

SignOPT

HSJA

RBA-Appr

Cube

LT-Attack (Ours)

MILP

Ours vs. MILP

(cid:96)∞ Perturbation

¯r

breast-cancer
diabetes
MNIST2-6
ijcnn
MNIST
F-MNIST
webspam
covtype
HIGGS

.258
.083
.480
.043
.195
.155
.013
.047
.009

time

.308s
.343s
2.73s
.313s
3.70s
4.38s
1.01s
.508s
.465s

¯r

time

¯r

time

.256
.078
.277
.043
.117
.065
.023
.074
.012

.070s
.066s
1.23s
.096s
28.7s(cid:63)
1.81s
.445s
.209s
.157s

.247
.113
.963
.074
.983
.607
.051
.086
.099

.0008s
.0009s
.155s
.020s
4.11s
5.55s
.720s
3.05s
55.3s*

¯r

.530
.080
.143
.035
.056
.038
.003
.036
.005

time

.230s
.240s
2.43s
.334s
4.42s
5.45s
.866s
.958s
.862s

¯rour
.235
.059
.097
.033
.029
.028
.001
.032
.004

time

.001s
.002s
.222s
.007s
.237s
.370s
.051s
.038s
.036s

r∗

.222
.056
.065
.031
.014
.013
.0008
.028
.004

time

.013s
.084s
28.7s
6.60s
375s*
15min*
27.5s
10min*
52min*

¯rour/r∗
1.06
1.05
1.49
1.06
2.07
2.15
1.25
1.14
1.00

Speedup

13X
42X
129.3X
942.9X
1582.3X
2473X
539.2X
15736.8X
87166.7X

Standard GBDT

SignOPT

HSJA

RBA-Appr

Cube

LT-Attack (Ours)

MILP

Ours vs. MILP

(cid:96)2 Perturbation

¯r

breast-cancer
diabetes
MNIST2-6
ijcnn
MNIST
F-MNIST
webspam
covtype
HIGGS

.310
.106
2.18
.051
1.20
.870
.023
.061
.015

time

.811s
.650s
7.29s
.544s
8.71s
9.57s
3.26s
.976s
1.02s

¯r

time

¯r

time

.370
.123
2.45
.052
1.45
.581
.112
.123
.015

.072s
.064s
1.54s
.094s
23.7s(cid:63)
2.03s
.529s
.217s
.154s

.352
.158
3.98
.112
5.38
3.85
.128
.129
.196

.0008s
.001s
.155s
.020s
4.11s
5.48s
.721s
3.03s
55.5s*

¯r

.678
.136
.801
.067
.310
.225
.009
.070
.013

time

.248s
.269s
3.73s
.355s
6.67s
9.20s
1.04s
1.06s
.905s

¯rour
.283
.077
.245
.044
.072
.073
.002
.045
.008

time

.001s
.003s
.235s
.010s
.243s
.400s
.053s
.039s
.037s

r∗

.280
.073
.183
.043
.043
.049
.002
.042
.007

time

.011s
.055s
2.52s
2.18s
32.5s
49.3s
3.17s
237s
13min*

¯rour/r∗
1.01
1.05
1.34
1.02
1.67
1.49
1.00
1.07
1.14

Speedup

11X
18.3X
10.7X
218X
133.7X
123.3X
59.8X
6076.9X
20621.6X

Theorem 3. (Convergence Guarantee) Let V + = {i | i ∈ C, C ∈ Neighbor+
leaves appeared in the advanced neighborhood Neighbor+

1 ( ¯C) (Deﬁnition 4), then

1 ( ¯C)} be the union of

¯C is the optimum adversarial tuple within valid combinations of V +.

Note that V + is the union of leaves, and leaves from multiple tuples of distance 1 could form a
new valid tuple of larger distance to ¯C (by combining the different leaves together). In other words
Theorem 3 suggests that our solution is not only a local optimal in Neighbor1( ¯C), but also better than
certain tuples in Neighborh( ¯C) with h > 1. In our illustrated example Fig. 1, assume the algorithm
converged at ¯C = C(a) = (4, 5, 9) on (cid:96)∞ norm, here Neighbor+
1 (C(a)) = {(4, 8, 9), (4, 5, 10)}.
Theorem 3 claims that there is no better adversarial tuple from any valid combinations within
V + = {4, 5, 8, 9, 10} such as (4, 8, 10), even though it is from Neighbor2( ¯C).

3.5

Implementation Details

For the initial point, we draw 20 random initial adversarial examples from a Gaussian distribution,
and optimize with a ﬁne-grained binary search before feeding to the proposed algorithm. We return
the best adversarial example found among them (see Appendix D.1 for details). The ensemble is
likely to contain duplicate feature split thresholds even though it’s deﬁned on Rd, for example it may
come from the image space [255]d and scaled to Rd. Duplicate split thresholds are problematic since
we cannot move across the threshold without affecting multiple trees, and to overcome the issue we
use a relaxed version of Neighbor1(·) to allow changing multiple trees at one iteration, as long as
it’s caused by the same split threshold. When searching for the best neighborhood it’s likely to have
perturbation ties in (cid:96)∞ and (cid:96)1 norm, in this case we use a secondary (cid:96)2 norm to break the tie. Eq. (3)
looks for the best tuple across all neighborhood which may be unnecessary at early stage of iterations.
To improve the efﬁciency we sort feature dimensions by abs(x(cid:48) − x0) (large ﬁrst), and terminate the
search earlier if a better tuple was found in the top 1 feature. To escape converged local minimums
we change each coordinate to a nearby value from Gaussian distribution with 0.1 probability, and
continue the iteration if a better adversarial example was found within 300 trials.

4 Experimental Results

We evaluate the proposed algorithm on 9 public datasets (Smith et al., 1988; Lecun et al., 1998;
Chang, Lin, 2011; Wang et al., 2012; Baldi et al., 2014; Xiao et al., 2017; Dua, Graff, 2017) with both
the standard (natural) GBDT and RF models, and on an additional 10th dataset (Bosch, 2016) with

7

Table 4: Average (cid:96)∞ and (cid:96)2 perturbation of 5000 test examples (or the entire test set when its size is
less than 5000) on robustly trained GBDT models. Datasets are ordered by training data size. Bold
and blue highlight the best and the second best entries respectively (not including MILP).
("*" / "(cid:63)"): Average of 1000 / 500 examples due to long running time.

Robust GBDT

SignOPT

HSJA

(cid:96)∞ Perturbation

¯r

breast-cancer
diabetes
MNIST2-6
ijcnn
MNIST
F-MNIST
webspam
covtype
bosch
HIGGS

.403
.119
.588
.032
.513
.254
.047
.064
.343
.015

time

.371s
.364s
3.06s
.353s
3.93s
4.31s
1.00s
.540s
3.28s
.466s

¯r

.405
.123
.470
.030
.389
.154
.043
.080
.337
.016

time

.073s
.068s
1.30s
.105s
1.68s
1.79s
.414s
.186s
1.42s
.134s

Robust GBDT

SignOPT

HSJA

(cid:96)2 Perturbation

¯r

breast-cancer
diabetes
MNIST2-6
ijcnn
MNIST
F-MNIST
webspam
covtype
bosch
HIGGS

.437
.142
2.97
.033
3.08
1.67
.097
.076
.750
.020

time

.711s
.591s
7.37s
.572s
9.14s
9.27s
3.24s
1.11s
9.62s
.879s

¯r

.449
.150
3.32
.035
3.04
1.34
.100
.104
2.33
.020

time

.069s
.061s
1.28s
.096s
1.61s
1.64s
.431s
.196s
1.54s
.128s

RBA-Appr

¯r

time

.405
.138
.671
.032
.690
.596
.061
.093
.533
.048

.002s
.001s
.137s
.018s
6.42s
7.83s
.641s
3.61s
1.22s
72.4s*

RBA-Appr

¯r

time

.436
.161
2.95
.040
4.07
3.72
.148
.137
1.45
.085

.002s
.003s
.156s
.014s
5.11s
7.01s
.589s
3.26s
1.21s
66.5s*

Cube

LT-Attack (Ours)

MILP

Ours vs. MILP

¯r

.888
.230
.337
.027
.296
.101
.020
.055
.158
.012

time

.238s
.239s
2.15s
.313s
3.95s
4.45s
.756s
.720s
2.49s
.644s

¯rour
.404
.113
.333
.025
.290
.095
.017
.047
.143
.01

time

.002s
.003s
.275s
.006s
.234s
.412s
.031s
.047s
.213s
.050s

r∗

.401
.112
.313
.022
.270
.076
.015
.045
.100
.009

time

.010s*
.039s*
177s*
4.24s*
20min*
74min*
129s*
14min*
237s*
73min(cid:63)

¯rour/r∗
1.01
1.01
1.06
1.14
1.07
1.25
1.13
1.04
1.43
1.11

Speedup

5.6X
14.4X
641.6X
759.6X
5067.5X
10778.5X
4129.4X
17164.9X
1112X
87149.2X

Cube

LT-Attack (Ours)

MILP

Ours vs. MILP

¯r

.940
.274
1.31
.042
1.33
.500
.068
.096
.480
.023

time

.239s
.240s
3.19s
.307s
6.26s
7.01s
.869s
.726s
3.84s
.580s

¯rour
.434
.133
.971
.030
.932
.310
.041
.062
.258
.016

time

.002s
.005s
.438s
.006s
.291s
.385s
.034s
.047s
.232s
.045s

r∗

.431
.132
.762
.025
.670
.233
.035
.058
.214
.014

time

.011s*
.025s*
25.0s*
.853s*
7min*
231s*
28.3s*
9min*
28.0s*
24min(cid:63)

¯rour/r∗
1.01
1.01
1.27
1.20
1.39
1.33
1.17
1.07
1.21
1.14

Speedup

5.2X
4.8X
57.1X
140.3X
1523.6X
600.8X
840.6X
11183.1X
120.7X
31715.5X

the robustly trained GBDT. Datasets have a mix of small/large scale and binary/multi classiﬁcation
(statistics in Appendix A), and are normalized to [0, 1] to make results comparable across datasets. We
order datasets by training data size in all of our tables, where HIGGS is the largest dataset with 10.5
million training examples. All GBDTs were trained using the XGBoost framework (Chen, Guestrin,
2016) and we use the models provided by Chen et al. (2019b) as target models (except bosch). We
compare with the following existing adversarial attacks that are applicable to tree ensembles:

• SignOPT (Cheng et al., 2020): The decision based attack that constructs adversarial examples
based on hard-label black-box queries. We report the average distortion, denoted as ¯r in the results
(since the norm of adversarial example is an upper bound of minimal adversarial perturbation r∗ ).
• HSJA (Chen et al., 2019c): Another decision based attack for constructing adversarial examples.

• RBA-Appr (Yang et al., 2019): An approximate attack for tree ensembles that constructs adversarial
examples by searching over training examples of the opposite class.

• Cube (Andriushchenko, Hein, 2019): An empirical attack for tree ensembles that constructs
adversarial examples by stochastically changing a few coordinates to the (cid:96)∞ boundary, and accepts
the change if it decreases the functional margin. The method provides good experimental results in
general, but lacks theoretical guarantee and could be unreliable on certain datasets such as breast-
cancer. Cube doesn’t support (cid:96)2 objective by default and we report the (cid:96)2 perturbation of the
constructed adversarial examples from (cid:96)∞ objective attacks.
• LT-Attack (Ours): Our proposed attack that constructs adversarial examples for tree ensembles. We
report the average distortion of the adversarial examples, denoted as ¯rour in the results.
• MILP (Kantchelian et al., 2015): The mixed-integer linear programming based method provides
the exact minimal adversarial perturbation r∗ but could be very slow on large models.

We run our experiments with 20 threads per task. Conventionally black-box attacks measure efﬁciency
by the number of queries, here we compare running time since it’s difﬁcult to quantify queries for
white-box attacks. To minimize the efﬁciency variance between programming languages we feed
an XGBoost model (Chen, Guestrin, 2016) to SignOPT, HSJA, and Cube, which has an efﬁcient
C++ implementation and supports multi-threading batch query. MILP uses a thin wrapper around the
Gurobi Solver (Gurobi Optimization, 2020). Baseline methods spend majority of time on XGBoost
model inference rather than Python code. For instance, on Fashion-MNIST, SignOPT, HSJA, Cube
spent 72.8%, 57.3%, 73.4% of runtime in XGBoost library (C++) calls, respectively. HSJA and

8

Table 5: Average (cid:96)2 perturbation over 100 test examples on the standard (natural) random forests
(RF) models. Datasets are ordered by training data size. Bold and blue highlight the best and the
second best entries respectively (not including MILP).

Standard RF

Cube

LT-Attack (Ours)

MILP

Ours vs. MILP

(cid:96)2 Perturbation

¯r

breast-cancer
diabetes
MNIST2-6
ijcnn
MNIST
F-MNIST
webspam
covtype
HIGGS

1.03
.260
.439
.046
.057
.141
.005
.087
.015

time

.224s
.285s
2.13s
.336s
2.88s
3.51s
.704s
.700s
.423s

¯rour
.413
.151
.207
.028
.018
.066
.003
.055
.009

time

.001s
.003s
.045s
.003s
.057s
.080s
.033s
.040s
.013s

r∗

.402
.146
.194
.028
.018
.066
.003
.055
.009

time

.008s
.042s
.071s
.185s
4.56s
7.44s
.664s
30.1s
6.66s

¯rour/r∗
1.03
1.03
1.07
1.00
1.00
1.00
1.00
1.00
1.00

Speedup

8X
14X
1.6X
61.7X
80X
93X
20.1X
752.5X
512.3X

SignOPT start with 1 initial adversarial example and run 100∼632 and 200 queries per iteration
respectively to approximate the gradient, and Cube uses 20 initial examples which utilizes batch
query. In Table 3 and Table 4 we show the empirical comparisons on (cid:96)∞ and (cid:96)2, and provide (cid:96)1
results as well as the attack success rate in Appendix B due to the space limit. We can see that our
method provides a tight upper bound ¯rour compared to the exact r∗ from MILP, which means that
the adversarial examples found are very close to the one with minimal adversarial perturbation, and
our method achieved 1,000∼80,000x speedup on some large models such as HIGGS. Our bound
is tight especially in the (cid:96)2 case. For instance, on Fashion-MNIST our ¯r/r∗ ratio is 1.49x and
1.33x for standard and robustly trained models respectively, while the respective Cube ratio is 4.59x
and 2.15x using ∼21x time, and the respective HSJA ratio is 11.86x and 5.75x using ∼4.5x time.
For completeness we also include veriﬁcation results (Chen et al., 2019b; Wang et al., 2020) in
Appendix B since they are not directly comparable to adversarial attacks (they output lower bounds
of the minimal adversarial perturbation while attacks aim to output an upper bound). To demonstrate
the capability on general tree based ensembles we also conduct experiments on the standard (natural)
random forests (RFs). We present the average (cid:96)2 perturbation in Table 5, where the ¯rour/r∗ ratio is
close to 1 across all datasets. Additional experimental results on (cid:96)∞ perturbation can be found in
Appendix B.1, and we include model parameters as well as statistics in Appendix A.

To study the impact of using different number of initial examples, we conduct the experiments with
{1, 2, 4, 6, 10, 20, 40} initial examples on SignOPT, HSJA, Cube, and LT-Attack, allocating 2 threads
per task. We report the smallest (best) adversarial perturbation among those initial examples. Using
more initial examples could lead to smaller (better) adversarial perturbation, but requires linearly
increasing computational cost. Fig. 2 presents the (cid:96)2 perturbation vs. runtime per test example in log
scale, where our method is able to construct small (good) adversarial examples (y-axis) with a few
initial examples, and can be orders of magnitude faster than other methods in the meantime (x-axis).

Standard GBDT (covtype)

Standard GBDT (F-MNIST)

Standard GBDT (HIGGS)

Standard GBDT (webspam)

0.12

0.1

0.08

0.06

0.04

n
o
i
t
a
b
r
u
t
r
e
p

2
(cid:96)

n
o
i
t
a
b
r
u
t
r
e
p

2
(cid:96)

0.12

0.1

0.08

0.06

0.8

0.6

0.4

0.2

0.02

0.015

0.01

0.04

0.02

0

10−1 100 101 102
time/s

0
10−1

100

102

101
time/s

10−210−1 100 101 102 103
time/s

10−1

100
time/s

101

102

Robust GBDT (covtype)

Robust GBDT (F-MNIST)

Robust GBDT (HIGGS)

Robust GBDT (webspam)

1.5

1

0.5

0.03

0.025

0.02

0.015

0.1

0.08

0.06

0.04

10−1 100

101
time/s

102

100

101
time/s

102

103

10−2 10−1 100 101 102 103
time/s

10−1 100

101

102

time/s

MILP

Ours

RBA-Appr

Cube

HSJA

SignOPT

Figure 2: Average (cid:96)2 perturbation of 50 test examples vs. runtime per test example in log scale.
Methods on the bottom-left corner are better.

9

Broader Impact

To the best of our knowledge, this is the ﬁrst practical attack algorithm (in terms of both computational
time and solution quality) that can be used to evaluate the robustness of tree ensembles. The study of
robustness training algorithms for tree ensemble models have been difﬁcult due to the lack of attack
tools to evaluate their robustness, and our method can serve as the benchmark tool for robustness
evaluation (similar to FGSM, PGD and C&W attacks for neural networks) (Goodfellow et al.,
2015; Madry et al., 2018; Carlini, Wagner, 2017) to stimulate the research in the robustness of tree
ensembles.

Acknowledgments and Disclosure of Funding

We acknowledge the support by NSF IIS-1901527, IIS-2008173, ARL-0011469453, Google Cloud
and Facebook.

References

Andriushchenko Maksym, Hein Matthias. Provably robust boosted decision stumps and trees against
adversarial attacks // Advances in Neural Information Processing Systems 32. 2019. 13017–13028.

Athalye Anish, Carlini Nicholas, Wagner David A. Obfuscated Gradients Give a False Sense of

Security: Circumventing Defenses to Adversarial Examples // ICML. 2018.

Baldi Pierre, Sadowski Peter, Whiteson D. O. Searching for exotic particles in high-energy physics

with deep learning. // Nature communications. 2014. 5. 4308.

Bosch .

Bosch Production Line Performance. 2016.

https://www.kaggle.com/c/

bosch-production-line-performance/data.

Brendel Wieland, Rauber Jonas, Bethge Matthias. Decision-Based Adversarial Attacks: Reliable
Attacks Against Black-Box Machine Learning Models // International Conference on Learning
Representations. 2018.

Brunner Thomas, Diehl Frederik, Truong-Le Michael, Knoll Alois. Guessing Smart: Biased Sampling
for Efﬁcient Black-Box Adversarial Attacks // 2019 IEEE/CVF International Conference on
Computer Vision (ICCV). 2018. 4957–4965.

Carlini Nicholas, Wagner David A. Towards Evaluating the Robustness of Neural Networks // 2017

IEEE Symposium on Security and Privacy (SP). 2017. 39–57.

Chang Chih-Chung, Lin Chih-Jen. LIBSVM: A library for support vector machines // ACM
Transactions on Intelligent Systems and Technology. 2011. 2. 27:1–27:27. Software available at
http://www.csie.ntu.edu.tw/~cjlin/libsvm.

Chen Hongge, Zhang Huan, Boning Duane S., Hsieh Cho-Jui. Robust Decision Trees Against

Adversarial Examples // ICML. 2019a. 1122–1131.

Chen Hongge, Zhang Huan, Si Si, Li Yang, Boning Duane, Hsieh Cho-Jui. Robustness Veriﬁcation
of Tree-based Models // Advances in Neural Information Processing Systems 32. 2019b. 12317–
12328.

Chen Jianbo, Jordan Michael I., Wainwright Martin J. HopSkipJumpAttack: A Query-Efﬁcient

Decision-Based Adversarial Attack // arXiv preprint arXiv:1904.02144. 2019c.

Chen Pin-Yu, Zhang Huan, Sharma Yash, Yi Jinfeng, Hsieh Cho-Jui. ZOO: Zeroth Order Optimiza-
tion Based Black-box Attacks to Deep Neural Networks without Training Substitute Models //
Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and Security. 2017.

Chen Tianqi, Guestrin Carlos. XGBoost: A Scalable Tree Boosting System // Proceedings of the
22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. New
York, NY, USA: Association for Computing Machinery, 2016. 785–794. (KDD ’16).

10

Chen Yizheng, Wang Shiqi, Jiang Weifan, Cidon Asaf, Jana Suman. Training Robust Tree Ensembles

for Security. 2019d.

Cheng Minhao, Le Thong, Chen Pin-Yu, Zhang Huan, Yi JinFeng, Hsieh Cho-Jui. Query-Efﬁcient
Hard-label Black-box Attack: An Optimization-based Approach // International Conference on
Learning Representations. 2019.

Cheng Minhao, Singh Simranjit, Chen Patrick H., Chen Pin-Yu, Liu Sijia, Hsieh Cho-Jui. Sign-
OPT: A Query-Efﬁcient Hard-label Adversarial Attack // International Conference on Learning
Representations. 2020.

Dua Dheeru, Graff Casey. UCI Machine Learning Repository. 2017.

Goodfellow Ian J., Shlens Jonathon, Szegedy Christian. Explaining and Harnessing Adversarial

Examples // CoRR. 2015. abs/1412.6572.

Gurobi Optimization LLC. Gurobi Optimizer Reference Manual. 2020.

Ilyas Andrew, Engstrom Logan, Athalye Anish, Lin Jessy. Black-box Adversarial Attacks with Limited

Queries and Information // ICML. 2018.

Kantchelian Alex, Tygar J. Doug, Joseph Anthony D. Evasion and Hardening of Tree Ensemble

Classiﬁers // ICML. 2015.

Ke Guolin, Meng Qi, Finley Thomas, Wang Taifeng, Chen Wei, Ma Weidong, Ye Qiwei, Liu Tie-Yan.
LightGBM: A Highly Efﬁcient Gradient Boosting Decision Tree // Advances in Neural Information
Processing Systems 30. 2017. 3146–3154.

Lecun Y., Bottou L., Bengio Y., Haffner P. Gradient-based learning applied to document recognition //

Proceedings of the IEEE. 1998. 86, 11. 2278–2324.

Lee Guang-He, Yuan Yang, Chang Shiyu, Jaakkola Tommi. Tight Certiﬁcates of Adversarial Robust-
ness for Randomly Smoothed Classiﬁers // Advances in Neural Information Processing Systems
32. 2019. 4910–4921.

Madry Aleksander, Makelov Aleksandar, Schmidt Ludwig, Tsipras Dimitris, Vladu Adrian. Towards
Deep Learning Models Resistant to Adversarial Attacks // International Conference on Learning
Representations. 2018.

Prokhorenkova Liudmila, Gusev Gleb, Vorobev Aleksandr, Dorogush Anna Veronika, Gulin Andrey.
CatBoost: unbiased boosting with categorical features // Advances in neural information processing
systems. 2018. 6638–6648.

Smith J. Walter, Everhart James E., Dickson William C., Knowler William C, Johannes Richard S.
Using the ADAP Learning Algorithm to Forecast the Onset of Diabetes Mellitus // Proceedings of
the Annual Symposium on Computer Application in Medical Care. 1988.

Szegedy Christian, Zaremba Wojciech, Sutskever Ilya, Bruna Joan, Erhan Dumitru, Goodfellow Ian J.,

Fergus Rob. Intriguing properties of neural networks // CoRR. 2013. abs/1312.6199.

Tu Chun-Chen, Ting Pai-Shun, Chen Pin-Yu, Liu Sijia, Zhang Huan, Yi Jinfeng, Hsieh Cho-Jui, Cheng
Shin-Ming. AutoZOOM: Autoencoder-based Zeroth Order Optimization Method for Attacking
Black-box Neural Networks // AAAI. 2018.

Wang De, Irani Danesh, Pu Calton. Evolutionary study of web spam: Webb Spam Corpus 2011
versus Webb Spam Corpus 2006 // 8th International Conference on Collaborative Computing:
Networking, Applications and Worksharing (CollaborateCom). 2012. 40–49.

Wang Yihan, Zhang Huan, Chen Hongge, Boning Duane, Hsieh Cho-Jui. On Lp-norm Robustness of

Ensemble Decision Stumps and Trees // ICML. 2020.

Xiao Han, Rasul Kashif, Vollgraf Roland. Fashion-MNIST: a Novel Image Dataset for Benchmarking

Machine Learning Algorithms. 2017.

11

Yang Yao-Yuan, Rashtchian Cyrus, Wang Yizhen, Chaudhuri Kamalika. Robustness for Non-

Parametric Classiﬁcation: A Generic Attack and Defense. 2019.

Zhang Fuyong, Wang Yi, Liu Shigang, Wang Hua. Decision-based evasion attacks on tree ensemble

classiﬁers // World Wide Web. 04 2020.

Zhang Huan, Si Si, Hsieh Cho-Jui. GPU-acceleration for Large-scale Tree Boosting // arXiv preprint

arXiv:1706.08359. 2017.

12

A Dataset and Model Statistics

We use 9 datasets and pre-trained models provided in Chen et al. (2019b), which can be downloaded
from https://github.com/chenhongge/RobustTrees. Table 6 summarized the statistics of
the datasets as well as the standard (natural) GBDT models, and we report the average complexity
statistics for | NeighborBound(·)| from 500 test examples. For multi-class datasets we count trees
either belong to the victim class or the class of the initial adversarial example. Datasets may contain
duplicate feature split thresholds and the extra complexity is covered in the statistics. We disabled the
random noise optimization discussed in §3.5 to provide a cleaner picture of Algorithm 1. We train
standard (natural) RF models using XGBoost’s native RF APIs1, and provide the statistics in Table 7.

Table 6: The average complexity statistics for | NeighborBound(·)| from 500 test examples.

Dataset

features

classes

trees

depth l

iterations

|TBound(·)|

| Neighbor(t)

1 (·)|

breast-cancer
diabetes
MNIST2-6
ijcnn
MNIST
F-MNIST
webspam
covtype
HIGGS

10
8
784
22
784
784
254
54
28

2
2
2
2
10
10
2
7
2

4
20
1,000
60
400
400
100
160
300

6
5
4
8
8
8
8
8
8

2.1
6.3
121.7
26.5
159.4
236.8
100.7
36.7
107.1

3.2
6.1
374.2
7.4
124.7
149.1
37.0
30.8
13.5

5.2
3.4
14.9
3.3
5.0
6.5
3.8
10.6
2.1

| NeighborBound(·)|
9.2
10.6
256.5
17.8
367.9
717.4
129.7
39.2
24.0

Table 7: Parameters and statistics for datasets and the standard (natural) RFs.

Dataset

train size

test size

trees

depth l

subsampling

test acc.

breast-cancer
diabetes
MNIST2-6
ijcnn
MNIST
F-MNIST
webspam
covtype
HIGGS

546
614
11,876
49,990
60,000
60,000
300,000
400,000
10,500,000

137
154
1,990
91,701
10,000
10,000
50,000
180,000
500,000

4
25
1000
100
400
400
100
160
300

6
8
4
8
8
8
8
8
8

.8
.8
.8
.8
.8
.8
.8
.8
.8

.974
.775
.963
.919
.907
.823
.948
.745
.702

B Supplementary Experiments

B.1 Additional Experimental Results on Random Forests

Table 8: Average (cid:96)∞ perturbation over 100 test examples on the standard (natural) random forests
(RF) models. Datasets are ordered by training data size. Bold and blue highlight the best and the
second best entries respectively (not including MILP).

Standard RF

Cube

LT-Attack (Ours)

MILP

Ours vs. MILP

(cid:96)∞ Perturbation

¯r

breast-cancer
diabetes
MNIST2-6
ijcnn
MNIST
F-MNIST
webspam
covtype
HIGGS

.797
.159
.135
.032
.017
.036
.004
.050
.008

time

.208s
.271s
1.85s
.340s
1.98s
2.57s
.652s
.684s
.389s

¯rour
.340
.111
.130
.026
.010
.036
.002
.048
.007

time

.001s
.002s
.041s
.003s
.056s
.084s
.023s
.037s
.011s

r∗

.332
.103
.121
.026
.009
.032
.002
.048
.006

time

.008s
.054s
.335s
.338s
21.4s
34.1s
2.63s
72.2s
20.9s

¯rour/r∗
1.02
1.08
1.07
1.00
1.11
1.13
1.00
1.00
1.17

Speedup

8X
27X
8.2X
112.7X
382.1X
406X
114.3X
1951.4X
1900X

B.2 Attack Success Rate

We present attack success rate in Fig. 3, which is calculated as the ratio of constructed adversarial
examples that have smaller perturbation than the thresholds. We use 50 test examples for MILP due
to long running time, and 500 test examples for other methods.

1https://xgboost.readthedocs.io/en/release_1.0.0/tutorials/rf.html

13

Standard GBDT (covtype)

Robust GBDT (covtype)

Standard GBDT (covtype)

Robust GBDT (covtype)

1

0.8

0.6

0.4

0.2

e
t
a
r

s
s
e
c
c
u
s

0

0

0.02 0.04 0.06 0.08 0.1

(cid:96)∞ threshold

1

0.8

0.6

0.4

0.2

0

0

0.02 0.04 0.06 0.08 0.1

(cid:96)∞ threshold

1

0.8

0.6

0.4

0.2

0

0

0.05

0.15

0.1
(cid:96)2 threshold

1

0.8

0.6

0.4

0.2

0.2

0

0

0.05

0.1
(cid:96)2 threshold

0.15

0.2

Standard GBDT (F-MNIST)

Robust GBDT (F-MNIST)

Standard GBDT (F-MNIST)

Robust GBDT (F-MNIST)

1

0.8

0.6

0.4

0.2

e
t
a
r

s
s
e
c
c
u
s

0

0

1

0.8

0.6

0.4

0.2

0.2

0

0

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0.2

0

0

0.2
0.4
(cid:96)2 threshold

0.6

0

0

0.2
0.4
(cid:96)2 threshold

0.6

0.05

0.1
(cid:96)∞ threshold

0.15

0.05

0.1
(cid:96)∞ threshold

0.15

MILP

Ours

RBA-Appr

Cube

HSJA

SignOPT

Figure 3: Attack success rate vs. perturbation thresholds.

B.3

(cid:96)∞ Perturbation Using Different Number of Initial Examples

Fig. 4 presents the average (cid:96)∞ perturbation of 50 test examples vs. runtime per test example in log
scale. We plot the results for SignOPT, HSJA, Cube, and LT-Attack on {1, 2, 4, 6, 10, 20, 40} initial
examples, using 2 threads per task. Initial examples are not applicable to RBA-Appr and MILP thus
we only plot a single point for each method.

Standard GBDT (covtype)

Standard GBDT (F-MNIST)

Standard GBDT (HIGGS)

Standard GBDT (webspam)

0.08

0.06

0.04

n
o
i
t
a
b
r
u
t
r
e
p
∞
(cid:96)

0.15

0.1

0.05

10−2 10−1 100 101 102 103
time/s

0
10−1 100

101
time/s

0.012

0.01

0.008

0.006

0.004

0.02

0.01

0

102

103

10−2

100

102

104

time/s

10−1

100
time/s

101

Robust GBDT (covtype)

Robust GBDT (F-MNIST)

Robust GBDT (HIGGS)

Robust GBDT (webspam)

n
o
i
t
a
b
r
u
t
r
e
p
∞
(cid:96)

0.08

0.06

0.04

0.25

0.2

0.15

0.1

0.015

0.01

0.04

0.03

0.02

10−1 100 101 102 103

10−1 100 101 102 103

time/s

time/s

10−1 100 101 102 103
time/s

10−1

100
time/s

101

102

MILP

Ours

RBA-Appr

Cube

HSJA

SignOPT

Figure 4: Average (cid:96)∞ perturbation of 50 test examples vs. runtime per test example in log scale.
Methods on the bottom-left corner are better.

B.4 Experimental Results for (cid:96)1 Norm and Veriﬁcation

Table 9 presents the experimental results on (cid:96)1 norm perturbation. Cube doesn’t support (cid:96)1 objective
by default and we report the (cid:96)1 perturbation of the constructed adversarial examples from (cid:96)∞ objective
attacks. For completeness we include veriﬁcation results (Chen et al., 2019b; Wang et al., 2020) in
Table 9 and Table 10, which output lower bounds of the minimal adversarial perturbation denoted as
r (in contrast to adversarial attacks that aim to output an upper bound ¯r).

14

Table 9: Average (cid:96)1 perturbation over 50 test examples on the standard (natural) GBDT models
and robustly trained GBDT models. Datasets are ordered by training data size. Bold and blue
highlight the best and the second best entries respectively (not including MILP and Veriﬁcation).

Standard GBDT

SignOPT

RBA-Appr

Cube

LT-Attack (Ours)

MILP

Veriﬁcation

Ours vs. MILP

(cid:96)1 Perturbation

¯r

breast-cancer
diabetes
MNIST2-6
ijcnn
MNIST
F-MNIST
webspam
covtype
HIGGS

.413
.183
35.4
.075
22.8
13.8
.287
.074
.050

time

.686s
1.04s
6.81s
.889s
8.77s
9.55s
3.43s
1.74s
1.20s

¯r

time

.535
.294
25.9
.286
46.5
42.7
.614
.223
.611

.0002s
.0005s
.109s
.016s
3.13s
4.57s
.630s
2.29s
44.7s

¯r

1.02
.290
3.73
.247
2.16
1.20
.036
.132
.039

time

.234s
.238s
3.47s
.340s
7.11s
9.04s
1.01s
1.01s
.854s

¯rour
.372
.131
.783
.064
.341
.260
.006
.057
.016

time

.002s
.003s
.230s
.008s
.267s
.479s
.062s
.039s
.049s

r∗

.372
.126
.568
.060
.207
.181
.004
.052
.013

time

.012s
.080s
2.51s
3.28s
56.8s
70.3s
4.17s
314s
25min

r

.367
.095
.057
.044
.013
.013
.0003
.024
.002

time

.003s
.133s
1.17s
1.68s
5.40s
7.19s
6.40s
2.26s
7.36s

¯rour/r∗
1.00
1.04
1.38
1.07
1.65
1.44
1.50
1.10
1.23

Speedup

6.X
26.7X
10.9X
410.X
212.7X
146.8X
67.3X
8051.3X
30183.7X

Robust GBDT

SignOPT

RBA-Appr

Cube

LT-Attack (Ours)

MILP

Veriﬁcation

Ours vs. MILP

(cid:96)1 Perturbation

¯r

breast-cancer
diabetes
MNIST2-6
ijcnn
MNIST
F-MNIST
webspam
covtype
HIGGS

.654
.201
23.8
.076
57.4
28.6
.186
.097
.033

time

.869s
.667s
6.51s
.693s
7.93s
10.4s
3.65s
1.65s
1.12s

¯r

time

.598
.228
17.8
.205
32.7
41.3
.540
.217
.226

.0008s
.001s
.106s
.015s
3.70s
4.96s
.522s
2.43s
43.6s

¯r

1.23
.514
6.69
.233
9.76
3.70
.309
.241
.071

time

.210s
.235s
3.47s
.337s
8.35s
9.96s
1.00s
1.02s
.839s

¯rour
.574
.189
2.76
.067
4.00
1.20
.119
.080
.028

time

.002s
.002s
.523s
.007s
.455s
.477s
.037s
.075s
.052s

r∗

.574
.189
1.78
.065
1.72
.720
.073
.071
.021

time

r

.008s
.028s
23.2s
1.02s
13min
244s
67.7s
437s
40min

.506
.166
.381
.043
.270
.077
.014
.033
.006

time

.001s
.007s
2.91s
.279s
8.61s
13.6s
2.17s
3.12s
5.55s

¯rour/r∗
1.00
1.00
1.55
1.03
2.33
1.67
1.63
1.13
1.33

Speedup

4.X
14.X
44.4X
145.7X
1652.7X
511.5X
1829.7X
5826.7X
46576.9X

Table 10: Average (cid:96)∞ and (cid:96)2 perturbation over 500 test examples on the standard (natural) GBDT
models and robustly trained GBDT models. ("*"): Average of 50 examples due to long running time.

Standard GBDT LT-Attack (Ours)

MILP

Veriﬁcation

Robust GBDT

LT-Attack (Ours)

MILP

Veriﬁcation

(cid:96)∞ Perturbation

breast-cancer
diabetes
MNIST2-6
ijcnn
MNIST
F-MNIST
webspam
covtype
HIGGS

¯rour
.235
.059
.097
.033
.029
.028
.001
.032
.004

time

.001s
.002s
.222s
.007s
.237s
.370s
.051s
.038s
.036s

r∗

.222
.056
.065
.031
.014
.013
.0008
.028
.004

time

r

.013s
.084s
28.7s
6.60s
375s*
15min*
27.5s
10min*
52min*

.220
.047
.053
.027
.011
.012
.0002
.021
.002

time

.002s
.910s
1.27s
6.25s
9.38s
6.96s
9.79s
4.21s
13.2s

(cid:96)∞ Perturbation

breast-cancer
diabetes
MNIST2-6
ijcnn
MNIST
F-MNIST
webspam
covtype
HIGGS

¯rour
.415
.122
.331
.038
.298
.098
.016
.047
.01

time

.001s
.002s
.302s
.006s
.315s
.403s
.038s
.053s
.054s

r∗

.415
.121
.317
.036
.278
.078
.014
.044
.009

time

.008s
.036s
98.7s
3.60s
13min*
29min*
51.2s
518s*
45min*

r

.414
.119
.311
.032
.255
.075
.011
.031
.005

time

.001s
.011s
29.4s
.799s
7.47s
13.3s
5.85s
3.24s
8.42s

Standard GBDT LT-Attack (Ours)

MILP

Veriﬁcation

Robust GBDT

LT-Attack (Ours)

MILP

Veriﬁcation

(cid:96)2 Perturbation

breast-cancer
diabetes
MNIST2-6
ijcnn
MNIST
F-MNIST
webspam
covtype
HIGGS

¯rour
.283
.077
.245
.044
.072
.073
.002
.045
.008

time

.001s
.003s
.235s
.010s
.243s
.400s
.053s
.039s
.037s

r∗

.280
.073
.183
.043
.043
.049
.002
.042
.007

time

r

.011s
.055s
2.52s
2.18s
32.5s
49.3s
3.17s
237s
13min*

.277
.058
.058
.030
.013
.013
.0002
.023
.002

time

.002s
.458s
1.06s
4.62s
6.81s
6.72s
8.95s
2.96s
10.3s

(cid:96)2 Perturbation

breast-cancer
diabetes
MNIST2-6
ijcnn
MNIST
F-MNIST
webspam
covtype
HIGGS

¯rour
.452
.144
.968
.048
.996
.326
.039
.063
.016

time

.001s
.002s
.401s
.007s
.395s
.468s
.036s
.054s
.054s

r∗

.452
.143
.803
.046
.701
.251
.033
.059
.015

time

.007s
.024s
17.3s
.728s
200s
99.3s
12.0s
280s
15min*

r

.450
.130
.358
.035
.273
.079
.012
.033
.006

time

.001s
.009s
4.42s
.575s
10.1s
13.5s
4.72s
3.10s
7.16s

C Proofs

C.1 Proof of Theorem 2

Proof. By contradiction. Given adversarial tuple C(cid:48) and victim example x0, assume
1 (C(cid:48))

C1 /∈ NeighborBound(C(cid:48)).

∃C1 ∈ Neighbor+

s.t.

Assume p ∈ {1, 2, ∞}. Let x1 = argminx∈B(C1) (cid:107)x − x0(cid:107)p, x(cid:48) = argminx∈B(C(cid:48)) (cid:107)x − x0(cid:107)p, and
let J be the set of dimensions that x1 is closer to x0:

J = {j | |x1,j − x0,j| < |x(cid:48)

j − x0,j|}.

According to the deﬁnition of Neighbor+
1 (C(cid:48)) we have distp(C1, x0) < distp(C(cid:48), x0), and conse-
quently J (cid:54)= ∅. We choose any j(cid:48) ∈ J and for cleanness we use (l(cid:48), r(cid:48)] to denote the interval from
B(C(cid:48)) on j(cid:48)

th dimension, and let d0 = x0,j(cid:48), d1 = x1,j(cid:48), d(cid:48) = x(cid:48)

j(cid:48).

15

Observe d0 /∈ (l(cid:48), r(cid:48)], otherwise we have |d(cid:48) − d0| = 0 and |d1 − d0| cannot be smaller. W.l.o.g.
assume d0 is on the right side of the interval, i.e., r(cid:48) < d0, then according to the argmin property of
x(cid:48) we have d(cid:48) = r(cid:48).
Recall that B(C(cid:48)) is the intersection of K bounding boxes from the ensemble, then

∃t(cid:48) ∈ [K]

s.t. BC(cid:48)(t)

j(cid:48)

.r = r(cid:48).

Observe r(cid:48) = d(cid:48) < d1 since d0 is on the right side of d(cid:48) and d1 has smaller distance to d0, which means
C1 has different leaf than C(cid:48) on tree t. Also according to the above equation t(cid:48) ∈ TBound(C(cid:48)), and C1 ∈
Neighbor(t(cid:48))

1 (C(cid:48)) = NeighborBound(C(cid:48)), contradiction.

t∈TBound(C(cid:48)) Neighbor(t)

(C(cid:48)), thus C1 ∈ (cid:83)

1

C.2 Proof of Corollary 2

Proof. According to Theorem 1 B(−t) is the intersection of K − 1 bounding boxes and can be written
as the Cartesian product of d intervals, we call it a box. Each tree t of depth l splits the Rd space into up
to 2l non-overlapping axis-aligned boxes, and | Neighbor(t)
1 (C(cid:48))| + 1 (plus current box) corresponds
to the number of boxes that has non-empty intersection with B(−t), thus | Neighbor(t)
1 (C(cid:48))| ≤ 2l − 1.

Observe that k(t) axis-aligned feature split thresholds can split Rd into at most 2k(t)
non-overlapping
boxes, assuming d ≥ k(t), and the maximum can be reached by having at most 1 split threshold on
each dimension. In conclusion there are at most 2min(k(t),l) boxes that has non-empty intersection
with B(−t), thus | Neighbor(t)

1 (C(cid:48))| ≤ 2min(k(t),l) − 1 (minus the current box).

C.3 Proof of Theorem 3

Proof. By contradiction. Let x0, y0 be the victim example and assume

∃C∗ ∈ (V +)K ∩ C s.t.
Assume p ∈ {1, 2, ∞}. Recall f (C) = sign((cid:80)
ence

f (C∗) (cid:54)= y0 ∧ distp(C∗, x0) < distp(C(cid:48), x0).

i∈C vi), we compute the tree-wise prediction differ-

diff = vC∗(t)
v(t)

− vC(cid:48)(t)

, t ∈ [K].

Let tmin be the tree with the smallest functional margin difference

tmin = argmin
t∈[K]

y0 · v(t)
diff.

We construct a tuple C1 which is the same as C(cid:48) except on tmin, where C(tmin)
we have

1

C1 ∈ C ∧ distp(C1, x0) < distp(C(cid:48), x0).

= C∗(tmin), consequently

Now we show f (C1) (cid:54)= y0, or y0

(cid:80)

i∈C1

vi < 0:

i. Case y0 · v(tmin)

diff ≤ 0. Then
(cid:88)
y0

vi = y0

i∈C1

ii. Case y0 · v(tmin)

diff > 0. Then
(cid:88)

y0

vi = y0

≤ y0

i∈C1
(cid:88)

i∈C(cid:48)

vi + y0

(cid:88)

i∈C(cid:48)
(cid:88)

t∈[K]

vi + y0 · v(tmin)

diff ≤ y0

(cid:88)

i∈C(cid:48)

vi < 0.

(cid:88)

i∈C(cid:48)

vi + y0 · v(tmin)

diff < y0

vi + Ky0 · v(tmin)

diff

(cid:88)

i∈C(cid:48)

v(t)
diff = y0

(cid:88)

i∈C∗

vi < 0.

In conclusion C1 is a valid adversarial tuple within Neighbor1(C(cid:48)) and has smaller perturbation than
C(cid:48), thus the algorithm won’t stop.

16

D Supplementary Algorithms

D.1 Generating Initial Adversarial Examples for LT-Attack

Algorithm 2: Generating Initial Adversarial Examples for LT-Attack
Data: Target white-box model f , victim example x0.

y0 ← f (x0);
r(cid:48), C(cid:48) ← MAX, None;
num_attack ← 20;
for i ← 1, . . . , num_attack do

do

x(cid:48) ← x0 + Normal(0, 1)d;

while f (x(cid:48)) = y0;
x(cid:48) ← BinarySearch(x(cid:48), x0, f );
(cid:46) Do a fine-grained binary search between x0 and x(cid:48) to optimize the initial perturbation of x(cid:48).

Similar to g(θ) proposed by Cheng et al. (2019).

r∗, C∗ ← LT-Attack(f, x0, x(cid:48));
if r∗ < r(cid:48) then

r(cid:48), C(cid:48) ← r∗, C∗;

end

end
return r(cid:48), C(cid:48)

1 begin
2

3

4

5

6

7

8

9

10

11

12

13

14

15
16 end

D.2 Algorithm for NaiveLeaf

Algorithm 3: Compute NaiveLeaf
Data: Target white-box model f , current adversarial example x(cid:48), victim example x0.
Result: The NaiveLeaf neighborhood of C(x(cid:48))

1 begin

(i(1), . . . , i(K)) ← C(x(cid:48));
N ← ∅;
for t ← 1 . . . K do

for i ∈ S(t), i (cid:54)= i(t) do

(cid:46) S(t) denotes the leaves of tree t.
xnew ← x(cid:48);
for j, (l, r] ∈ Bi do

(cid:46) The jth dimension of the bounding box Bi, can be acquired from f .
if xnew,j /∈ (l, r] then

xnew,j ← min(r, max(l + (cid:15), x0,j));

end

end
N ← N ∪ {C(xnew)};

end

end
return N

2

3

4

5

6

7

8

9

10

11

12

13

14

15
16 end

D.3 A Greedy Algorithm to Estimate the Minimum Neighborhood Distance

To understand the quality of constructed adversarial examples we use an empirical greedy algorithm to
estimate the minimum neighborhood distance h such that Neighborh(·) can reach the exact solution.
Assume our method converged at C(cid:48) and the optimum solution is C∗, let Tdiff = {t | C(cid:48)(t) (cid:54)= C∗(t)}
be the set of trees with different prediction, then the Hamming distance ¯h = |Tdiff| is a trivial upper
bound where Neighbor¯h(C(cid:48)) can reach C∗ with a single addition iteration. To estimate a realistic h∼
we want to ﬁnd the disjoint split Tdiff = ∪i∈[k]Ti such that we can mutate C(cid:48) into C∗ by changing
trees in Ti to match C∗ at ith iteration. We make sure all intermediate tuples are valid and has strictly

17

decreasing perturbation as required by Eq. (3), and report h∼ = maxi∈[k] |Ti|. h∼ is an estimation
of the minimum h because we cannot guarantee the argmin constrain due to the large complexity.
As shown in Table 11 we have median(¯h) = 23 and median(h∼) = 8 on ensemble with 300 trees
(HIGGS), which suggests that our method is likely to reach the exact optimum on half of the test
examples through ∼3 additional iterations on Neighbor8(·). In this experiment we disabled the
random noise optimization discussed in §3.5 to provide a cleaner picture of Algorithm 1.

Algorithm 4: A greedy algorithm to estimate the minimum neighborhood distance h∼.
Data: The model f , our adversarial point xour, exact MILP solution x∗.
Result: An estimation of neighborhood distance h∼.

1 begin
2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23
24 end

Cour, C∗ ← C(xour), C(x∗);
r∗ ← distp(C∗, x0);
y∗ ← f (x∗);
hmin ← D(Cour, C∗) ;
Idiff ← {(vC∗(t)
− vC
(cid:46) Idiff is the list of tuples in the form of (label diff, our leaf, MILP leaf).
num_trial ← 200;
for i ← 1, . . . , num_trial do

our , C∗(t)) | C(t)
our

(cid:54)= C∗(t), t ∈ [K]};

our , C(t)

(t)

(cid:46) Hamming distance is the upper bound.

h ← 0;
I ← shufﬂe(Idiff);
Ctmp ← Cour;
while I (cid:54)= ∅ do

rlast ← distp(Ctmp, x0);
Ctmp ← pop the ﬁrst tuple from I with positive label diff and replace with MILP leaf;
h ← h + 1;
while Ctmp /∈ C or distp(Ctmp, x0)p /∈ [r∗, rlast) or f (Ctmp) (cid:54)= y∗ do

(cid:46) Making sure Ctmp satisfies Equation 3 except the argmin. We cannot guarantee

argmin due to the high complexity. The while loop is guaranteed to exit since we
can pop all tuples in I to become C∗.

Ctmp ← pop the ﬁrst tuple from I and replace with MILP leaf;
h ← h + 1;

end

end
hmin ← min(hmin, h);

end
return hmin

Table 11: Convergence statistics for the standard (natural) GBDT models between our solution and
the optimum MILP solution. We collect the data after the ﬁne-grained binary search but before
applying LT-Attack (Initial), and the data after LT-Attack (Converged). We disabled the random noise
optimization discussed in §3.5.

Dataset

Model

HammingDist ¯h

NeighborDist h∼

Initial

Converged

Initial

Converged

# of trees max median max median max median max median

breast-cancer
diabetes
MNIST2-6
ijcnn
MNIST
F-MNIST
webspam
covtype
HIGGS

4
20
1000
60
400
400
100
160
300

2
10
676
27
266
237
88
84
190

0
3
490
10
115
174
56
23
62

0
6
347
16
96
100
36
67
125

0
0
172
3
33
56
16
9
23

2
5
172
18
173
82
75
82
181

0
1
46
2
8
12
7
7
12

0
4
64
8
12
18
9
65
121

0
0
36
2
7
10
4
6
8

18

