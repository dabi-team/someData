2
2
0
2

t
c
O
2
1

]
L
M

.
t
a
t
s
[

2
v
5
7
9
6
0
.
9
0
2
2
:
v
i
X
r
a

Wasserstein K-means for clustering
probability distributions

Yubo Zhuang, Xiaohui Chen, Yun Yang
Department of Statistics
University of Illinois at Urbana-Champaign
{yubo2,xhchen,yy84}@illinois.edu

Abstract

Clustering is an important exploratory data analysis technique to group objects
based on their similarity. The widely used K-means clustering method relies
on some notion of distance to partition data into a fewer number of groups. In
the Euclidean space, centroid-based and distance-based formulations of the K-
means are equivalent. In modern machine learning applications, data often arise
as probability distributions and a natural generalization to handle measure-valued
data is to use the optimal transport metric. Due to non-negative Alexandrov
curvature of the Wasserstein space, barycenters suffer from regularity and non-
robustness issues. The peculiar behaviors of Wasserstein barycenters may make the
centroid-based formulation fail to represent the within-cluster data points, while the
more direct distance-based K-means approach and its semideﬁnite program (SDP)
relaxation are capable of recovering the true cluster labels. In the special case
of clustering Gaussian distributions, we show that the SDP relaxed Wasserstein
K-means can achieve exact recovery given the clusters are well-separated under
the 2-Wasserstein metric. Our simulation and real data examples also demonstrate
that distance-based K-means can achieve better classiﬁcation performance over
the standard centroid-based K-means for clustering probability distributions and
images.

1

Introduction

Clustering is a major tool for unsupervised machine learning problems and exploratory data analysis
in statistics. Suppose we observe a sample of data points X1, . . . , Xn taking values in a metric
space (X , (cid:107) · (cid:107)). Suppose there exists a clustering structure G∗
K such that each data point
Xi belongs to exactly one of the unknown cluster G∗
k. The goal of clustering analysis is to recover
K given the input data X1, . . . , Xn. In the Euclidean space X = Rp,
the true clusters G∗
the K-means clustering is a widely used method that achieves the empirical success in many
applications [MacQueen, 1967].
In modern machine learning and data science problems such
as computer graphics [Solomon et al., 2015], data exhibits complex geometric features and traditional
clustering methods developed for Euclidean data may not be well suited to analyze such data.

1, . . . , G∗

1, . . . , G∗

In this paper, we consider the clustering problem of probability measures µ1, . . . , µn into K groups.
As a motivating example, the MNIST dataset contains images of handwritten digits 0-9. Normalizing
the greyscale images into histograms as probability measures, a common task is to cluster the images.
One can certainly apply the Euclidean K-means to the vectorized images. However, this would
lose important geometric information of the two-dimensional data. On the other hand, theory of
optimal transport [Villani, 2003] provides an appealing framework to model measure-valued data as
probabilities in many statistical tasks [Domazakis et al., 2019, Chen et al., 2021, Bigot et al., 2017,
Seguy and Cuturi, 2015, Rigollet and Weed, 2019, Hütter and Rigollet, 2019, Cazelles et al., 2018].

36th Conference on Neural Information Processing Systems (NeurIPS 2022).

 
 
 
 
 
 
Background on K-means clustering. Algorithmically, the K-means clustering have two equivalent
formulations in the Euclidean space – centroid-based and distance-based – in the sense that they
both yield the same partition estimate for the true clustering structure. Given the Euclidean data
X1, . . . , Xn ∈ Rp, the centroid-based formulation of standard K-means can be expressed as

min
β1,...,βK ∈Rd

n
(cid:88)

i=1

min
k∈[K]

(cid:107)Xi − βk(cid:107)2

2 = min

G1,...,GK

(cid:110) K
(cid:88)

(cid:88)

k=1

i∈Gk

(cid:107)Xi − ¯Xk(cid:107)2
2 :

K
(cid:71)

k=1

Gk = [n]

(cid:111)
,

(1)

i∈Gk

k=1 are determined by the Voronoi diagram from {βk}K

k=1, ¯Xk =
where clusters {Gk}K
Xi denotes the centroid of cluster Gk, (cid:70) denotes the disjoint union and [n] =
|Gk|−1 (cid:80)
{1, . . . , n}. Heuristic algorithm for solving (1) includes Lloyd’s algorithm [Lloyd, 1982], which is
an iterative procedure alternating the partition and centroid estimation steps. Speciﬁcally, given an
initial centroid estimate β(1)
K , one ﬁrst assigns each data point to its nearest centroid at the
t-th iteration according to the Voronoi diagram, i.e.,
(cid:110)
i ∈ [n] : (cid:107)Xi − β(t)

1 , . . . , β(1)

k (cid:107)2 (cid:54) (cid:107)Xi − β(t)

j (cid:107)2, ∀j ∈ [K]

k =

G(t)

(2)

(cid:111)

,

and then update the centroid for each cluster

β(t+1)
k

=

1
|G(t)
k |

(cid:88)

Xi,

i∈G(t)
k

(3)

k | denotes the cardinality of G(t)

where |G(t)
The distance-based (sometimes also referred as partition-based) formulation directly solves the
following constrained optimization problem without referring to the estimated centroids:

k . Step (2) and step (3) alternate until convergence.

min
G1,...,GK

(cid:110) K
(cid:88)

k=1

1
|Gk|

(cid:88)

i,j∈Gk

(cid:107)Xi − Xj(cid:107)2
2 :

Gk = [n]

(cid:111)
.

K
(cid:71)

k=1

(4)

Observe that (1) with nearest centroid assignment and (4) are equivalent for the clustering purpose
due to the following identity, which extends the parallelogram law from two points to n points,

n
(cid:88)

i,j=1

(cid:107)Xi − Xj(cid:107)2

2 = 2n

n
(cid:88)

i=1

(cid:107)Xi − ¯X(cid:107)2

2, with

¯X =

1
n

n
(cid:88)

i=1

Xi

and Xi ∈ Rp.

(5)

Consequently, the two criteria yield the same partition estimate for G∗
K. The key identity (5)
establishing the equivalence relies on two facts of the Euclidean space: (i) it is a vector space (i.e.,
vectors can be averaged in the linear sense); (ii) it is ﬂat (i.e., zero curvature), both of which are
unfortunately not true for the Wasserstein space (P2(Rp), W2) that endows the space P2(Rp) of all
probability distributions with ﬁnite second moments with the 2-Wasserstein metric W2 [Ambrosio
et al., 2005]. In particular, the 2-Wasserstein distance between two distributions µ and ν in P2(Rp) is
deﬁned as

1, . . . , G∗

W 2

2 (µ, ν) := min

γ

(cid:110) (cid:90)

(cid:107)x − y(cid:107)2

2 dγ(x, y)

(cid:111)
,

(6)

Rp×Rp
where minimization over γ runs over all possible couplings with marginals µ and ν. It is well-known
that the Wasserstein space is a metric space (in fact a geodesic space) with non-negative curvature in
the Alexandrov sense [Lott, 2008].

Our contributions. We summarize our main contributions as followings: (i) we provide evidence for
pitfalls (irregularity and non-robustness) of barycenter-based Wasserstein K-means, both theoretically
and empirically, and (ii) we generalize the distance-based formulation of K-means to the Wasserstein
space and establish the exact recovery property of its SDP relaxation for clustering Gaussian measures
under a separateness lower bound in the 2-Wasserstein distance.

Existing work. Since the K-means clustering is a worst-case NP-hard problem [Aloise et al.,
2009], approximation algorithms have been extensively studied in literature including: Llyod’s
algorithm [Lloyd, 1982], spectral methods [von Luxburg, 2007, Meila and Shi, 2001, Ng et al.,
2001], semideﬁnite programming (SDP) relaxations [Peng and Wei, 2007], non-convex methods via

2

low-rank matrix factorization [Burer and Monteiro, 2003]. Theoretic guarantees of those methods are
established for statistical models on Euclidean data [Lu and Zhou, 2016, von Luxburg et al., 2008,
Vempala and Wang, 2004, Fei and Chen, 2018, Giraud and Verzelen, 2018, Chen and Yang, 2021,
Zhuang et al., 2022].

The concept of clustering general measure-valued data is introduced by Domazakis et al. [2019], where
the authors proposed the centroid-based Wasserstein K-means algorithm. It replaced the Euclidean
norm and sample means by the Wasserstein distance and barycenters respectively. Verdinelli and
Wasserman [2019] proposed a modiﬁed Wasserstein distance for distribution clustering. And after
that, Chazal et al. [2021] proposed a method in Clustering of measures via mean measure quantization
by ﬁrst vectorizing the measures in a ﬁnite Euclidean space followed by an efﬁcient clustering
algorithm such as single-linkage clustering with L∞ distance. The vectorization methods could
improve the computational efﬁciency but might not capture the properties of probability measures
well compared to clustering algorithms based directly on Wasserstein space.

2 Wasserstein K-means clustering methods

In this section, we generalize the Euclidean K-means to the Wasserstein space. Our starting point is
to mimic the standard K-means methods for Euclidean data. Thus we may deﬁne two versions of the
Wasserstein K-means clustering formulations: centroid-based and distance-based. As we mentioned
in Section 1, when working with Wasserstein space (P2(Rp), W2), the corresponding centroid-based
criterion (1) and the distance-based criterion (4), where the Euclidean metric (cid:107) · (cid:107)2 is replaced with
the 2-Wasserstein metric W2, may lead to radically different clustering schemes. To begin with, we
would like to argue that due to the irregularity and non-robustness of barycenters in the Wasserstein
space, the centroid-based criterion may lead to unreasonable clustering schemes that lack physical
interpretations and are sensitive to small data perturbations.

2.1 Clustering based on barycenters

The centroid-based Wasserstein K-means for extending the Lloyd’s algorithm into the Wasserstein
space has been recently considered by Domazakis et al. [2019]. Speciﬁcally, it is an iterative
algorithm proceeds as following. Given an initial centroid estimate ν(1)
K , one ﬁrst assigns
each probability measure µ1, . . . , µn to its nearest centroid in the Wasserstein geometry at the t-th
iteration according to the Voronoi diagram:

1 , . . . , ν(1)

G(t)

k =

(cid:110)
i ∈ [n] : W2(µi, ν(t)

k ) (cid:54) W2(µi, ν(t)
j ),

∀j ∈ [K]

(cid:111)

,

and then update the centroid for each cluster

ν(t+1)
k

= arg min

ν∈P2(Rd)

1
|G(t)
k |

(cid:88)

i∈G(t)
k

W 2

2 (µi, ν).

(7)

(8)

k

Note that ν(t+1)
in (8) is referred as barycenter of probability measures µi, i ∈ G(t)
k , a Wasserstein
analog of the Euclidean average or mean [Agueh and Carlier, 2011]. We will also ex-changeably
use barycenter-based K-means to mean the centroid-based K-means in the Wasserstein space. Even
though the Wasserstein barycenter is a natural notion of averaging probability measures, it may
exhibit peculiar behaviours and fail to represent the within-cluster data points, partly due to the
violation of the generalized parallelogram law (5) (for non-ﬂat spaces) if the Euclidean metric (cid:107) · (cid:107)2
is replaced with the 2-Wasserstein metric W2.

Example 1 (Irregularity of Wasserstein barycenters). Wasserstein barycenter has much less regu-
larity than the sample mean in the Euclidean space [Kim and Pass, 2017]. In particular, Santambrogio
and Wang [2016] constructed a simple example of two probability measures that are supported on line
segments in R2, whereas the support of their barycenter obtained as the displacement interpolation
the two endpoint probability measures is not convex (cf. left plot in Figure 1). In this example, the
probability density µ0 and µ1 are supported on the line segments L0 = {(s, as) : s ∈ [−1, 1]} and
L1 = {(s, −as) : s ∈ [−1, 1]} respectively. We choose a ∈ (0, 1) to identify the orientation of L0
and L1 based on the x-axis. Moreover, we consider the linear density functions µ0(s) = (1 − s)/2

3

and µ1(s) = (1 + s)/2 for s ∈ [−1, 1] supported on L0 and L1 respectively. Then the optimal
transport map T := Tµ0→µ1 from µ0 to µ1 is given by

T (x, ax) = (cid:0) − 1 + (cid:112)4 − (1 − x)2, −a · (−1 + (cid:112)4 − (1 − x)2)(cid:1),

(9)

and the barycenter corresponds to the displacement interpolation µt = [(1 − t)id + tT ](cid:93)µ0 at
t = 0.5 [McCann, 1997]. For self-contained purpose, we give the proof of (9) in Appendix D.1.
Fig. 1 on the left shows the support of barycenter µ0.5 is not convex (in fact part of an ellipse
boundary) even though the supports of µ0 and µ1 are convex. This example shows that the barycenter
functional is not geodesically convex in the Wasserstein space. As barycenters turn out to be essential
in centroid-based Wasserstein K-means and irregularity of the barycenter may fail to represent the
cluster (see more details in Example 3 and Remark 9 below), this counter-example is our motivation
(cid:4)
to seek alternative formulation.

Figure 1: Left: support of the Wasserstein barycenter as the displacement interpolation between µ0
and µ1 at t = 0.5 in Example 1. Right: non-robustness of the optimal transport map (arrow lines) and
Wasserstein barycenter w.r.t. small perturbation around a = 1 for the target measure in Example 2.

Example 2 (Non-robustness of Wasserstein barycenters). Another unappealing feature of the
Wasserstein barycenter is its sensitivity to data perturbation: a small (local) change in one contributing
probability measure may lead to large (global) changes in the resulting barycenter. See Fig. 1 on
the right for such an example. In this example, we take the source measure as µ0 = 0.5 δ(−1,1) +
0.5 δ(1,−1) and the target measure as µ1 = 0.5 δ(−1,−a) + 0.5 δ(1,a) for some a > 0. It is easy to see
that the optimal transport map T := Tµ0→µ1 has a dichotomy behavior:

T (−1, 1) =

(cid:26)(−1, −a)
(1, a)

if 0 < a < 1
if a > 1

and T (1, −1) =

(cid:26) (1, a)

(−1, −a)

if 0 < a < 1
if a > 1

.

(10)

Thus the Wasserstein barycenter determined by the displacement interpolation µt = [(1−t)id+tT ](cid:93)µ0
is a discontinuous function at a = 1. This non-robustness can be attribute to the discontinuity of
the Wasserstein barycenter as a function of its input probability measures; in contrast, the Euclidean
(cid:4)
mean is a globally Lipchitz continuous function of its input points.

Because of these pitfalls of the Wasserstein barycenter shown in Examples 1 and 2, the centroid-
based Wasserstein K-means approach described at the beginning of this subsection may lead to
unreasonable and unstable clustering schemes. In addition, an ill-conditioned conﬁguration may
signiﬁcantly slow down the convergence of commonly used barycenter approximating algorithms
such as iterative Bregman projections [Benamou et al., 2015]. Below, we give a concrete example of
such phenomenon in the clustering context.

Example 3 (Failure of centroid-based Wasserstein K-means). In a nutshell, the failure in this
example is due to the counter-intuitive phenomenon illustrated in the right panel of Fig. 2, where
some distribution µ3 in the Wasserstein space may have larger W2 distance to Wasserstein barycenter
µ∗
1 than every distribution µi (i = 1, 2) that together forms it. As a result of this strange conﬁguration,
even though µ3 is closer to µ1 and µ2 from the ﬁrst cluster with barycenter µ∗
1 than µ4 coming from
a second cluster with barycenter µ∗
2, it will be incorrectly assigned to the second cluster using the
2) > max (cid:8)W2(µ3, µ1), W2(µ3, µ2)(cid:9).
centroid-based criterion (7), since W2(µ3, µ∗
In contrast, for Euclidean spaces due to the following equivalent formulation of the generalized

1) > W2(µ3, µ∗

4

−1.0−0.50.00.51.0−0.4−0.20.00.20.4−1.0−0.50.00.51.0−0.4−0.20.00.20.4−1.0−0.50.00.51.0−0.4−0.20.00.20.4m0m0.5m1−1.0−0.50.00.51.01.52.0−2−1012m0m1, a=0.5m1, a=2parallelogram law (5),

n
(cid:88)

i=1

(cid:107)X − Xi(cid:107)2

2 = n(cid:107)X − ¯X(cid:107)2

2 +

n
(cid:88)

i=1

(cid:107)Xi − ¯X(cid:107)2
2

(cid:62) n(cid:107)X − ¯X(cid:107)2
2,

for any X ∈ Rp,

there is always some point Xi† satisfying (cid:107)X − Xi†(cid:107)2 (cid:62) (cid:107)X − ¯X(cid:107)2, that is, further away from X
than the mean ¯X; thereby excluding counter-intuitive phenomena as the one shown in Fig. 2.

Figure 2: Left: visualization of Example 3 in R2 and Wasserstein space. Right: the black curve
connecting µ1 and µ2 depicts the geodesic between them.
Concretely, the ﬁrst cluster G∗
1 is shown in the left panel of Fig. 2 highlighted by a red circle,
consisting of m copies of (µ1, µ2) pairs and one copy of µ3; the second cluster G∗
2 containing copies
of µ4 is highlighted by a blue circle. Each distribution assigns equal probability mass to two points,
where the two supporting points are connected by a dashed line for easy illustration. More speciﬁcally,
we set

µ1 = 0.5 δ(x,y) + 0.5 δ(−x,−y), µ2 = 0.5 δ(x,−y) + 0.5 δ(−x,y),
µ3 = 0.5 δ(x+(cid:15)1,y) + 0.5 δ(x+(cid:15)1,−y),

and µ4 = 0.5 δ(x+(cid:15)1+(cid:15)2,y) + 0.5 δ(x+(cid:15)1+(cid:15)2,−y),

where δ(x,y) denotes the point mass measure at point (x, y), and (x, y, (cid:15)1, (cid:15)2) are positive constants.
The property of this conﬁguration can be summarized by the following lemma.

Lemma 4 (Conﬁguration characterization). If (x, y, (cid:15)1, (cid:15)2) satisﬁes

y2 < min{x2, 0.25 ∆(cid:15)1,x}

and ∆(cid:15)1,x < (cid:15)2

2 < ∆(cid:15)1,x + y2,

where ∆(cid:15)1,x := (cid:15)2
W2(µ3, µ∗

2) < W2(µ3, µ∗
1)

and

1 + 2x2 + 2x(cid:15)1, then for all sufﬁciently large m (number of copies of µ1 and µ2),

max
i,j∈Gk

W2(µi, µj)

<

max
k=1,2
(cid:124)

W2(µi, µj),

min
i∈G1,j∈G2
(cid:124)

(cid:123)(cid:122)
largest within-cluster distance

(cid:125)

(cid:123)(cid:122)
least between-cluster distance

(cid:125)

where µ∗

k denotes the Wasserstein barycenter of cluster Gk for k = 1, 2.

2 of cluster G∗

Note that the condition of Lemma 4 implies y < x. Therefore, the barycenter between µ1 andµ2
1 := 0.5 δ(x,0) + 0.5 δ(−x,0) lying on the horizontal axis. By increasing m, the barycenter µ∗
is ˜µ∗
1
of cluster G∗
1 can be made arbitrarily close to ˜µ∗. The second inequality in Lemma 4 shows that
all within-cluster distances are strictly less than the between-cluster distances; therefore, clustering
based on pairwise distances is able to correctly recover the cluster label of µ3. However, since µ3
is closer to the barycenter µ∗
2 according to the ﬁrst inequality in Lemma 4, it will
be mis-classiﬁed into G∗
2 using the centroid-based criterion. We emphasize that cluster positions
in this example are generic and do exist in real data; see Remark 9 and Section 4.3 for further
discussions on our experiment results on MNIST data. Moreover, similar to Example 2, a small
change in the orientation of distribution µ1 may completely alter the clustering membership of µ3
based on the centroid criterion. Speciﬁcally, if we slightly increase x to make it exceed y, then
the barycenter between µ1 and µ2 becomes ¯µ∗
1 := 0.5 δ(0,y) + 0.5 δ(0,−y) that lies on the vertical
axis. Correspondingly, if based on centroids, then µ3 should be clustered into G∗
1 as it is closer to
the barycenter µ∗
2 of G∗
2. Therefore, the centroid-based criterion can
be unstable against data perturbations. In comparison, a pairwise distances based criterion always
(cid:4)
assigns µ3 into cluster G∗

1 than the barycenter µ∗

2 no matter x < y or x > y.

1 of G∗

5

2.2 Clustering based on pairwise distances

Due to the irregularity and non-robustness of centroid-based Wasserstein K-means, we instead
propose and advocate the use of distance-based Wasserstein K-means below, which extends the
Euclidean distance-based K-means formulation (4) into the Wasserstein space,

min
G1,...,GK

(cid:110) K
(cid:88)

k=1

1
|Gk|

(cid:88)

i,j∈Gk

W 2

2 (µi, µj) :

Gk = [n]

(cid:111)
.

K
(cid:71)

k=1

(11)

Correspondingly, we can analogously design a greedy algorithm resembling the Wasserstein Lloyd’s
algorithm described in Section 2.1 that solves the centroid-based Wasserstein K-means. Speciﬁcally,
the greedy algorithm proceeds in an iterative manner as following. Given an initial cluster membership
estimate G(1)
K , one assigns each probability measure µ1, . . . , µn based on minimizing the
averaged squared W2 distances to all current members in every cluster, leading to an updated cluster
membership estimate

1 , . . . , G(1)

G(t+1)
k

=

(cid:26)

i ∈ [n] :

1
|G(t)
k |

(cid:88)

s∈G(t)
k

W 2

2 (µi, µs) (cid:54) 1
|G(t)
j |

(cid:88)

s∈G(t)

j

W 2

2 (µi, µs),

(cid:27)

∀j ∈ [K]

.

(12)

We arbitrarily select among the least W2 distance clusters in the case of a tie. We highlight that
the center-based and distance-based Wasserstein K-means formulations may not necessarily be
equivalent to yield the same cluster labels (cf. Example 3). Below, we shall give some example
illustrating connections to the standard K-means clustering in the Euclidean space.

Example 5 (Degenerate probability measures). If the probability measures are Dirac at point
Xi ∈ Rp, i.e., µi = δXi, then the Wasserstein K-means is the same as the standard K-means since
(cid:4)
W2(µi, µj) = (cid:107)Xi − Xj(cid:107)2.

Example 6 (Gaussian measures). If µi = N (mi, Vi) with positive-deﬁnite covariance matrices
Σi (cid:31) 0, then the squared 2-Wasserstein distance can be expressed as the sum of the squared Euclidean
distance on the mean vector and

d2(Vi, Vj) = Tr

(cid:20)
Vi + Vj − 2

(cid:16)

V 1/2
i

VjV 1/2
i

(cid:17)1/2(cid:21)

,

(13)

the squared Bures distance on the covariance matrix [Bhatia et al., 2019]. Here, we use V 1/2 to
denote the unique symmetric square root matrix of V (cid:31) 0. That is,

W 2

2 (µi, µj) = (cid:107)mi − mj(cid:107)2

2 + d2(Vi, Vj).

(14)

Then the Wasserstein K-means, formulated either in (7) or (11), can be viewed as a covariance-
adjusted Euclidean K-means by taking account into the shape or orientation information in the
(cid:4)
(non-degenerate) Gaussian inputs.
Example 7 (One-dimensional probability measures). If µi are probability measures on R with
cumulative distribution function (cdf) Fi, then the Wasserstein distance can be written in terms of the
quantile transform

W 2

2 (µi, µj) =

[F −

i (u) − F −

j (u)]2 du,

(15)

(cid:90) 1

0

where F − is the generalized inverse of the cdf F on [0, 1] deﬁned as F −(u) = inf{x ∈ R : F (x) >
u} (cf. Theorem 2.18 [Villani, 2003]). Thus the one-dimensional probability measures in Wasserstein
space can be isometrically embedded in a ﬂat L2 space, and we can bring back the equivalence of the
(cid:4)
Wasserstein and Euclidean K-means clustering methods.

3 SDP relaxation and its theoretic guarantee

Note that Wasserstein Lloyd’s algorithm requires to use and compute the barycenter in (7) and (8)
at each iteration, which can be computationally expensive when the domain dimension d is large
or the conﬁguration is ill-conditioned (cf. Example 2). On the other hand, it is known that solving
the distance-based K-means (4) is worst-case NP-hard for Euclidean data. Thus we expect solving

6

the distance-based Wasserstein K-means (11) is also computationally hard. A common way is
to consider convex relaxations to approximate the solution of (11). It is known that certain SDP
relaxation is information-theoretically tight for (4) when the data X1, . . . , Xn ∈ Rp are generated
from a Gaussian mixture model with isotropic known variance [Chen and Yang, 2021]. In this paper,
we extend the idea into Wasserstein setting for solving (11).

A typical SDP relaxation for Euclidean data uses pairwise inner products to construct an afﬁnity matrix
for clustering [Peng and Wei, 2007]; unfortunately, due to the non-ﬂatness nature, a globally well-
deﬁned inner product does not exist for Wasserstein spaces with dimension higher than one. Therefore,
we will derive a Wasserstein SDP relaxation to the combinatorial optimization problem (4) using
the squared distance matrix An×n = {aij} with aij = W 2
2 (µi, µj). Concretely, we can one-to-one
reparameterize any partition (G1, . . . , GK) as a binary assignment matrix H = {hik} ∈ {0, 1}n×K
such that hik = 1 if i ∈ Gk and hik = 0 otherwise. Then (11) can be expressed as a nonlinear 0-1
integer program,

(cid:110)

min

(cid:104)A, HBH (cid:62)(cid:105) : H ∈ {0, 1}n×K, H1K = 1n

(cid:111)
,

(16)

where 1n is the n × 1 vector of all ones and B = diag(|G1|−1, . . . , |GK|−1). Changing of variable
to the membership matrix Z = HBH (cid:62), we note that Zn×n is a symmetric positive semideﬁnite
(psd) matrix Z (cid:23) 0 such that Tr(Z) = K, Z1n = 1n, and Z (cid:62) 0 entrywise. Thus we obtain the
SDP relaxation of (11) by only preserving these convex constraints:

(cid:110)

min
Z∈Rn×n

(cid:104)A, Z(cid:105) : Z (cid:62) = Z, Z (cid:23) 0, Tr(Z) = K, Z1n = 1n, Z (cid:62) 0

(17)

(cid:111)
.

To theoretically justify the SDP formulation (17) of Wasserstein K-means, we consider the scenario
of clustering Gaussian distributions in Example 6, where the Wasserstein distance (14) contains
two separate components:
the Euclidean distance on mean vector and the Bures distance (13)
on covariance matrix. Without loss of generality, we focus on mean-zero Gaussian distributions
since optimal separation conditions for exact recovery based on the Euclidean mean component
have been established in [Chen and Yang, 2021]. Suppose we observe Gaussian distributions
νi ∼ N (0, Vi), i ∈ [n] from K groups G∗
K, where cluster G∗
k contains nk members, and
the covariance matrices have the following clustering structure: if i ∈ G∗
k, then
i.i.d.∼ SymN (0, 1),

Vi = (I + tXi)V (k)(I + tXi) with X1, . . . , Xn

1, · · · , G∗

(18)

where the psd matrix V (k) is the center of the k-th cluster, SymN (0, 1) denotes the symmetric random
matrix with i.i.d. standard normal entries, and t is a small perturbation parameter such that (I +tXi) is
psd with high probability. For zero-mean Gaussian distributions, we have W2(N (0, V ), N (0, U )) =
d(V, U ) according to (14). Note that on the Riemannian manifold of psd matrices, the geodesic
emanating from V (k) in the direction X as a symmetric matrix can be linearized by V = (I +
tX)V (k)(I + tX) in a small neighborhood of t, thus motivating the parameterization of our statistical
model in (18). The next theorem gives a separation lower bound to ensure exact recovery of the
clustering labels for Gaussian distributions.
Theorem 8 (Exact recovery for clustering Gaussians). Let ∆2 := mink(cid:54)=l d2(V (k), V (l)) denote
the minimal pairwise separation among clusters, ¯n := maxk∈[K] nk (and n := mink∈[K] nk) the
maximum (minimum) cluster size, and m := mink(cid:54)=l
the minimal pairwise harmonic mean
of cluster sizes. Suppose the covariance matrix Vi of Gaussian distribution νi = N (0, Vi) is
independently drawn from model (18) for i = 1, 2, . . . , n. Let β ∈ (0, 1). If the separation ∆2
satisﬁes

2nknl
nk+nl

∆2 > ¯∆2 : =

C1t2
min{(1 − β)2, β2}

V p2 log n,

(19)

then the SDP (17) achieves exact recovery with probability at least 1 − C2n−1, provided that

(cid:112)log n/(cid:2)(p + log ¯n)V 1/2T 1/2

v

(cid:3), n/m ≤ C5 log n,

n ≥ C3 log2 n, t ≤ C4
(cid:13)V (k)(cid:13)
(cid:13)

(cid:13)op, Tv = maxk Tr(cid:2)(cid:0)V (k)(cid:1)−1(cid:3), and Ci, i = 1, 2, 3, 4, 5 are constants.

where V = maxk
Remark 9 (Further insight on pitfalls of barycenter-based Wasserstein K-means). Theorem 8
suggests that different from Euclidean data, distributions after centering can be clustered if scales and
rotation angles vary (i.e., covariance-adjusted). We further illustrate the rotation and scale effects on

7

Figure 3: Visualization of misclassiﬁcation for the barycenter-based Wasserstein K-means (B-WKM)
on a randomly sampled subset from MNIST (200 digit "0" and 100 digit "5"). The plot at the bottom
is a example of misclassiﬁed image. The right plot is the abstraction of the images in the Wasserstein
space. The color depth indicates the frequency of the distributions. Red and blue colors stand for
distributions belong to true clusters "0" and "5".

1, µ∗
2), we see that the image µ0 (containing digit "0") is closer to µ∗

the MNIST data that may mislead the centroid-based Wasserstein K-means, thus providing a real
data support for Example 3. Here we randomly sample two unbalanced clusters with 200 numbers of
"0" and 100 numbers of "5". Fig. 3 shows the clustering results for the centroid-based Wasserstein
K-means and its oracle version where we replace the estimated barycenters µ1, µ2 with the true
barycenters µ∗
2 computed on the true labels. Comparing the Wasserstein distances W2(µ0, µ∗
1)
and W2(µ0, µ∗
2 (true barycenter of
digit "5") and thus it cannot be classiﬁed correctly based on the nearest true barycenter (cf. Fig. 3 on
the left). Moreover, Wasserstein K-means based on estimated barycenters µ1, µ2 yields two clusters
of mixed "0" and "5". In both cases, the misclassiﬁcation error is characterized by grouping similar
degrees of angle and/or stretch. Since there are two highly unbalanced clusters of distributions,
Wasserstein K-means is likely to enforce larger cluster to separate into two clusters and absorb those
around centers (cf. Fig. 3 on the right), leading to larger classiﬁcation errors. We shall see that
in Section 4.3 the distance-based Wasserstein K-means and its SDP relaxation have much smaller
classiﬁcation error rate on MNIST for the reason that we explained in Example 3 (cf. Lemma 4). (cid:4)

4 Experiments

4.1 Counter-example in Example 3 revisited

Our ﬁrst experiment is to back up the claim about the failure of centroid-based Wasserstein K-
means in Example 3 through simulations. Instead of using point mass measures that may results
in instability for computing the barycenters, we use Gaussian distributions with small variance
as a smoothed version. We consider K = 2, where cluster G∗
1 consists of m1 many copies of
(µ1, µ2) pairs and m2 many µ3, and cluster G∗
2 consists of m3 many copies of µ4. We choose
µi as the following two-dimensional mixture of Gaussian distributions µi = 0.5 N (ai,1, Σi,1) +
0.5 N (ai,2, Σi,2) for i = 1, 2, 3, 4. Due to the space limit, detailed simulation setups and parameters
are given in Appendix B. From Table 1, we can observe that Wasserstein SDP has achieved exact
recovery for all cases while barycenter-based Wasserstein K-means has only around 40% exact
recovery rate among all repetitions. In addition, Wasserstein SDP is more stable than distance-
based Wasserstein K-means. Denote ∆k := W 2(µ3, µ∗
k) as the squared distance between µ3 and
µ∗
k for k = 1, 2, where µ∗
k. Let ∆∗ := maxk=1,2 maxi,j∈Gk W2(µi, µj)
and ∆∗ := mini∈G1,j∈G2 W 2(µi, µj) be the maximum within-cluster distance and the minimum
between-cluster distance respectively. From Table 7 in the Appendix, we can observe that ∆∗ < ∆∗,
from which we can expect Wasserstein SDP to correctly cluster all data points in the Wassertein
space. Moreover, Table 1 shows that about 25% times that the distributions (as µ3) in G∗
1 satisfy
∆1 > ∆2, implying those µ3 to be likely assigned to the wrong cluster, which is consistent with

k is the barycenter of G∗

8

Example 3. The experiment results also show that any copy of µ3 is misclassiﬁed whenever exact
recovery fails for B-WKM, which means the misclassiﬁed rate for µ3 equals to (1 − γ), where γ is
the exact recovery rate for B-WKM shown in Table 1. Table 6 in the appendix further reports the
run time comparison, from which we see that distance-based approaches are more computationally
efﬁcient than the barycenter-based one in our settings.

Table 1: Exact recovery rates and frequency of ∆1 > ∆2 for B-WKM among total 50 repetitions in
the counter example. W-SDP: Wasserstein SDP, D-WKM: Distance-based Wasserstein K-means,
B-WKM: Barycenter-based Wasserstein K-means. n: total number of distributions.

n W-SDP D-WKM B-WKM Frequency of ∆1 > ∆2
0.40
101
0.34
202
0.46
303

1.00
1.00
1.00

0.82
0.84
0.72

0.32
0.26
0.20

4.2 Gaussian distributions

Next, we simulate random Gaussian measures from model (18) with K = 4 and all cluster size
equal. We set the centers of each cluster of Gaussians such that all pairwise distances among the
barycenters are all equal, i.e., W 2
2 (N (0, V (k1)), N (0, V (k2))) ≡ D for all k1, k2 ∈ {1, 2, 3, 4}
with V = maxk (cid:107)V (k)(cid:107)op ∈ [4.5, 5.5]. We ﬁx the dimension p = 10 and vary the sample size
n = 200, 400, 600. And we set the perturbation parameter t = 10−3 on the covariance matrix. The
simulation results are reported over 100 times in each setting. Fig. 4 shows the misclassiﬁcation
rate (log-scale) versus the squared distance D between centers. We observe that when the distance
between centers of clusters are larger than certain threshold (squared distance D > 10−3 in this case),
then Wasserstein SDP can achieve exact recovery for different n, while the misclassiﬁcation rate for
the two Wasserstein K-means are stably around 10%. And when the distance between centers of
clusters are relatively small, the two Wasserstein K-means and SDP behave similarly.

Figure 4: Mis-classiﬁcation error versus squared distance D from Wasserstein SDP (W-SDP) and
barycenter/distance-based Wasserstein K-means (B-WKM and D-WKM) for clustering Gaussians
under n ∈ {200, 400, 600}. Due to the log-scale, 10−6 corresponds to exact recovery.

4.3 Real-data applications

We consider three benchmark image datasets: MNIST, Fashion-MNIST and USPS handwrit-
ing digits. Due to complexity issues, we consider subsets of the whole datasets and randomly
choose ﬁxed number of images from each clusters based on 10 replicates for each cases. Here
we used Sinkhorn divergence to calculate the Wasserstein distance and the Bregman projection
with 100 iterations for computing the barycenters, which is efﬁcient and stable for non-degenerate
case in practice. For both Wasserstein K-means methods, we use the initialization method in
analogue to the K-means++ for Euclidean data, i.e., the ﬁrst cluster barycenter is chosen uni-
formly at random as one of the distributions, after which each subsequent cluster barycenter is

9

chosen from the remaining distributions with probability proportional to its squared Sinkhorn di-
vergence from the distribution’s closest existing cluster barycenter. Codes using MATLAB and
Python implementing W-SDP and D-WKM are available at: https://github.com/Yubo02/
Wasserstein-K-means-for-clustering-probability-distributions.
For MNIST dataset, we choose and ﬁx two clusters: G∗
2 containing
the number "5" for two cases. (1) In Case 1, we randomly draw 200 number "0" and 100 number of
"5" for each repetition. (2) In Case 2, we double the number and randomly draw 400 number "0" and
200 number of "5" instead. For Fashion-MNIST and USPS handwriting digits dataset, we consider
K = 3: G∗
2 containing the
"Trouser" (or the number "5" for USPS handwriting) and G∗
3 containing the "Dress" (or the number
"7" for USPS handwriting). The cluster sizes are unbalanced where we randomly choose 200, 100
and 100 number from G∗
3 respectively. The error rates are shown in Table 2. Detailed
setups and more results about F1 scores (analogous to error rates) as well as time costs are placed at
Appendix A due to space limit.

1 containing the "T-shirt/top" (or the number "0" for USPS handwriting), G∗

1 containing the number "0" and G∗

2 and G∗

1, G∗

From Table 2, we can observe that the performances for Wasserstein SDP (W-SDP) and distance-based
Wasserstein K-means (D-WKM) are better compared with barycenter-based Wasserstein K-means
(B-WKM) for all cases. And the results from Table 3 in Appendix A by using F1 score are consistent
with the results from Table 2. The original K-means method behaves similar as barycenter-based
Wasserstein K-means in some cases and behaves less preferable for cases such as our experiment on
USPS handwriting digits. In particular, the visualization of the clustering results for case 1 has been
shown in Fig. 3. From this ﬁgure we can ﬁnd that the classiﬁcation criterion for B-WKM will end up
with the closeness to certain shape of "0", which is characterized by certain angle or the degree of
stretch. And this will lead to the high misclassiﬁcation error for barycenter-based or centroid-based
Wasserstein K-means.

Table 2: Error rate (SD) for clustering three benchmark datasets: MNIST, Fashion-MNIST and USPS
handwriting digits. MNIST1 (MNIST2) refers to the results of Case 1 (Case 2) for MNIST dataset.

MNIST1
MNIST2
Fashion-MNIST
USPS handwriting

W-SDP
0.235 (0.045)
0.279 (0.050)
0.082 (0.020)
0.206 (0.020)

D-WKM
0.156 (0.057)
0.185 (0.097)
0.056 (0.014)
0.159 (0.061)

B-WKM
0.310 (0.069)
0.324 (0.032)
0.141 (0.059)
0.240 (0.045)

KM
0.295 (0.066)
0.362 (0.033)
0.138 (0.099)
0.284 (0.025)

5 Discussion

In this paper, we observed and analyzed the peculiar behaviors of Wasserstein barycenters and
their results in clustering probability distributions. After that, we proposed the distance-based K-
means approach (D-WKW) and its semideﬁnite program relaxation (W-SDP) by showing the exact
recovery results for Gaussians theoretically and numerically. For several real benchmark datasets, we
showed results where D-WKM and W-SDP could outperform barycenter-based K-means approach
(B-WKM). And we focused on one unbalanced case of two clusters from MNIST and analyzed its
behavior through visualization of misclassiﬁcation for the barycenter-based Wasserstein K-means.
The corresponding time costs for B-WKM, D-WKM and W-SDP for benchmark datasets suggest that
the scalability for them and especially our approaches could be serious when sample size is large.
One of our future goals would be addressing the corresponding computational complexity issues for
real data. Another goal is to ﬁnd out more conclusive results where our approaches are preferable
within or out of the realm of unbalanced clusters for real datasets.

Acknowledgments and Disclosure of Funding

Xiaohui Chen was partially supported by NSF CAREER grant DMS-1752614. Yun Yang was partially
supported by NSF grant DMS-2210717.

10

References

Radoslaw Adamczak. A tail inequality for suprema of unbounded empirical processes with appli-
cations to Markov chains. Electronic Journal of Probability, 13(none):1000 – 1034, 2008. doi:
10.1214/EJP.v13-521. URL https://doi.org/10.1214/EJP.v13-521.

Martial Agueh and Guillaume Carlier. Barycenters in the wasserstein space. SIAM J. Math. Anal., 43

(2):904–924, 2011.

Daniel Aloise, Amit Deshpande, Pierre Hansen, and Preyas Popat. Np-hardness of euclidean sum-of-

squares clustering. Machine learning, 75(2):245–248, 2009.

Luigi Ambrosio, Nicola Gigli, and Giuseppe Savaré. Gradient ﬂows: in metric spaces and in the

space of probability measures. Springer Science & Business Media, 2005.

Jean-David Benamou, Guillaume Carlier, Marco Cuturi, Luca Nenna, and Gabriel Peyré. Itera-
tive bregman projections for regularized transportation problems. SIAM Journal on Scientiﬁc
Computing, 37(2):A1111–A1138, 2015.

Rajendra Bhatia, Tanvi Jain, and Yongdo Lim. On the bures–wasserstein distance between positive
deﬁnite matrices. Expositiones Mathematicae, 37(2):165–191, 2019. ISSN 0723-0869. doi: https:
//doi.org/10.1016/j.exmath.2018.01.002. URL https://www.sciencedirect.com/science/
article/pii/S0723086918300021.

Jérémie Bigot, Raúl Gouet, Thierry Klein, and Alfredo López. Geodesic PCA in the Wasserstein
space by convex PCA. Annales de l’Institut Henri Poincaré, Probabilités et Statistiques, 53(1):1 –
26, 2017. doi: 10.1214/15-AIHP706. URL https://doi.org/10.1214/15-AIHP706.

Yann Brenier. Polar factorization and monotone rearrangement of vector-valued functions. Com-
munications on Pure and Applied Mathematics, 44(4):375–417, 1991. doi: https://doi.org/10.
1002/cpa.3160440402. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.
3160440402.

Samuel Burer and Renato D. C. Monteiro. A nonlinear programming algorithm for solving semideﬁ-
nite programs via low-rank factorization. Mathematical Programming, 95(2):329–357, 2003. doi:
10.1007/s10107-002-0352-8. URL https://doi.org/10.1007/s10107-002-0352-8.

Elsa Cazelles, Vivien Seguy, Jérémie Bigot, Marco Cuturi, and Nicolas Papadakis. Geodesic pca
versus log-pca of histograms in the wasserstein space. SIAM Journal on Scientiﬁc Comput-
ing, 40(2):B429–B456, 2018. doi: 10.1137/17M1143459. URL https://doi.org/10.1137/
17M1143459.

Frédéric Chazal, Clément Levrard, and Martin Royer. Clustering of measures via mean measure
quantization. Electronic Journal of Statistics, 15(1):2060 – 2104, 2021. doi: 10.1214/21-EJS1834.
URL https://doi.org/10.1214/21-EJS1834.

Xiaohui Chen and Yun Yang. Cutoff for exact recovery of gaussian mixture models. IEEE Transac-

tions on Information Theory, 67(6):4223–4238, 2021.

Yaqing Chen, Zhenhua Lin, and Hans-Georg Müller. Wasserstein regression. Journal of the
American Statistical Association, 0(0):1–14, 2021. doi: 10.1080/01621459.2021.1956937. URL
https://doi.org/10.1080/01621459.2021.1956937.

Pierre Del Moral and Angele Niclas. A taylor expansion of the square root matrix functional, 2017.

URL https://arxiv.org/abs/1705.08561.

G. Domazakis, Dimosthenis Drivaliaris, Sotirios Koukoulas, G. I. Papayiannis, Andrianos E.
Tsekrekos, and Athanasios N. Yannacopoulos. Clustering measure-valued data with wasserstein
barycenters. arXiv: Machine Learning, 2019.

Darina Dvinskikh and Daniil Tiapkin.

Improved complexity bounds in wasserstein barycenter

problem, 2020. URL https://arxiv.org/abs/2010.04677.

Yingjie Fei and Yudong Chen. Hidden integrality of sdp relaxation for sub-gaussian mixture models.

arXiv:1803.06510, 2018.

11

Aude Genevay, Gabriel Peyre, and Marco Cuturi. Learning generative models with sinkhorn di-
In Amos Storkey and Fernando Perez-Cruz, editors, Proceedings of the Twenty-
vergences.
First International Conference on Artiﬁcial Intelligence and Statistics, volume 84 of Pro-
ceedings of Machine Learning Research, pages 1608–1617. PMLR, 09–11 Apr 2018. URL
https://proceedings.mlr.press/v84/genevay18a.html.

Christophe Giraud and Nicolas Verzelen. Partial recovery bounds for clustering with the relaxed

kmeans. arXiv:1807.07547, 2018.

Jan-Christian Hütter and Philippe Rigollet. Minimax estimation of smooth optimal transport maps,

2019. URL https://arxiv.org/abs/1905.05828.

Hicham Janati, Marco Cuturi, and Alexandre Gramfort. Debiased Sinkhorn barycenters.

In
Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on
Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 4692–4701.
PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/janati20a.html.

Young-Heon Kim and Brendan Pass. Wasserstein barycenters over riemannian manifolds. Ad-
vances in Mathematics, 307:640–683, 2017.
doi: https://doi.org/10.
1016/j.aim.2016.11.026. URL https://www.sciencedirect.com/science/article/pii/
S0001870815304643.

ISSN 0001-8708.

Khang Le, Huy Nguyen, Quang M Nguyen, Tung Pham, Hung Bui, and Nhat Ho. On ro-
bust optimal transport: Computational complexity and barycenter computation.
In M. Ran-
zato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Ad-
vances in Neural Information Processing Systems, volume 34, pages 21947–21959. Cur-
ran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/
b80ba73857eed2a36dc7640e2310055a-Paper.pdf.

Stuart Lloyd. Least squares quantization in pcm. IEEE Transactions on Information Theory, 28:

129–137, 1982.

John Lott. Some geometric calculations on wasserstein space. Communications in Mathematical
Physics, 277(2):423–437, 2008. doi: 10.1007/s00220-007-0367-3. URL https://doi.org/10.
1007/s00220-007-0367-3.

Yu Lu and Harrison Zhou. Statistical and computational guarantees of lloyd’s algorithm and its

variants. arXiv:1612.02099, 2016.

J.B. MacQueen. Some methods for classiﬁcation and analysis of multivariate observations. Proc.

Fifth Berkeley Sympos. Math. Statist. and Probability, pages 281–297, 1967.

Robert J. McCann. A convexity principle for interacting gases. Advances in Mathematics, 128(1):
153–179, 1997. ISSN 0001-8708. doi: https://doi.org/10.1006/aima.1997.1634. URL https:
//www.sciencedirect.com/science/article/pii/S0001870897916340.

Marina Meila and Jianbo Shi. Learning segmentation by random walks. In In Advances in Neural

Information Processing Systems, pages 873–879. MIT Press, 2001.

Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm.

In Advances in Neural Information Processing Systems, pages 849–856. MIT Press, 2001.

Jiming Peng and Yu Wei. Approximating k-means-type clustering via semideﬁnite programming.

SIAM J. OPTIM, 18(1):186–205, 2007.

Philippe Rigollet and Jonathan Weed. Uncoupled isotonic regression via minimum Wasserstein
deconvolution. Information and Inference: A Journal of the IMA, 8(4):691–717, 04 2019. ISSN
2049-8772. doi: 10.1093/imaiai/iaz006. URL https://doi.org/10.1093/imaiai/iaz006.

Filippo Santambrogio and Xu-Jia Wang. Convexity of the support of the displacement inter-
polation: Counterexamples. Applied Mathematics Letters, 58:152–158, 2016.
ISSN 0893-
9659. doi: https://doi.org/10.1016/j.aml.2016.02.016. URL https://www.sciencedirect.
com/science/article/pii/S0893965916300726.

12

Vivien Seguy and Marco Cuturi.

Principal geodesic analysis for probability measures un-
der the optimal transport metric.
In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and
R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Cur-
ran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/
f26dab9bf6a137c3b6782e562794c2f2-Paper.pdf.

Justin Solomon, Fernando de Goes, Gabriel Peyré, Marco Cuturi, Adrian Butscher, Andy Nguyen, Tao
Du, and Leonidas Guibas. Convolutional wasserstein distances: Efﬁcient optimal transportation on
geometric domains. ACM Trans. Graph., 34(4), jul 2015. ISSN 0730-0301. doi: 10.1145/2766963.
URL https://doi.org/10.1145/2766963.

Aad W. van der Vaart and Jon A. Wellner. Maximal Inequalities and Covering Numbers, pages
95–106. Springer New York, New York, NY, 1996. ISBN 978-1-4757-2545-2. doi: 10.1007/
978-1-4757-2545-2_14. URL https://doi.org/10.1007/978-1-4757-2545-2_14.

Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. J. Comput.

Syst. Sci, 68:2004, 2004.

Isabella Verdinelli and Larry Wasserman. Hybrid Wasserstein distance and fast distribution clustering.
Electronic Journal of Statistics, 13(2):5088 – 5119, 2019. doi: 10.1214/19-EJS1639. URL
https://doi.org/10.1214/19-EJS1639.

Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science.
Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.
doi: 10.1017/9781108231596.

Cédric Villani. Topics in optimal transportation. Graduate studies in mathematics. American

mathematical society, 2003.

Ulrike von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):395–416,

2007.

Ulrike von Luxburg, Mikhail Belkin, and Olivier Bousquet. Consistency of spectral clustering.

Annals of Statistics, 36(2):555–586, 2008.

Yubo Zhuang, Xiaohui Chen, and Yun Yang. Sketch-and-lift: scalable subsampled semideﬁnite
program for k-means clustering. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera,
editors, Proceedings of The 25th International Conference on Artiﬁcial Intelligence and Statistics,
volume 151 of Proceedings of Machine Learning Research, pages 9214–9246. PMLR, 28–30 Mar
2022. URL https://proceedings.mlr.press/v151/zhuang22a.html.

13

A Additional details on application for real datasets in Section 4.3

In this section, we provide more details of setups and results for real applications in Section 4.3. The
results of error rates, F1 scores and time costs are shown in Table 2, Table 3 and Table 4 respectively,
which are based on 10 replicates.1

Table 3: F1 score (SD) for clustering three benchmark datasets: MNIST, Fashion-MNIST and USPS
handwriting digits. MNIST1 (MNIST2) refers to the results of Case 1 (Case 2) for MNIST dataset.

MNIST1
MNIST2
Fashion-MNIST
USPS handwriting

W-SDP
0.771 (0.044)
0.729 (0.049)
0.919 (0.018)
0.799 (0.019)

D-WKM
0.842 (0.056)
0.814 (0.093)
0.934 (0.036)
0.835 (0.081)

B-WKM
0.698 (0.067)
0.685 (0.031)
0.817 (0.117)
0.761 (0.060)

KM
0.708 (0.063)
0.647 (0.032)
0.791(0.168)
0.689 (0.093)

Table 4: Time cost (SD) for clustering three benchmark datasets: MNIST, Fashion-MNIST and USPS
handwriting digits. MNIST1 (MNIST2) refers to the results of Case 1 (Case 2) for MNIST dataset.

MNIST1
MNIST2
Fashion-MNIST
USPS handwriting

W-SDP
525.13 (4.70)
2187.66 (74.67)
849.24 (7.09)
1100.87 (19.13)

D-WKM
524.80 (4.92)
2160.91 (7.26)
852.49 (8.60)
1098.05 (16.41)

B-WKM
388.87 (647.15)
693.67 (142.57)
463.28 (176.32)
317.12 (113.94)

KM
0.01 (0.01)
0.02 (0.00)
0.01 (0.00)
0.02 (0.01)

First we run our Wasserstein SDP algorithm against Wasserstein K-means on the MNIST dataset
for two cases. (1) For the ﬁrst case, we choose two clusters: G∗
1 containing the number "0" and G∗
2
containing the number "5", so that the number of clusters is K = 2 in the algorithms. The cluster
sizes are unbalanced with |G∗
2| = 2, where we randomly choose 200 number "0" and 100
number of "5" for each repetition. (2) For the second case, we follow the same settings as case 1
except that randomly choose 400 number "0" and 200 number of "5" for each repetition.

1|/|G∗

Next we considered benchmark dataset Fashion-MNIST 28×28 containing 10 clusters of 28×28
greyscale images of clothes. Here we choose three clusters: G1 containing the "T-shirt/top", G2
containing the "Trouser" and G3 containing the "Dress", so that the number of clusters is K = 3
in the algorithms. The cluster sizes are unbalanced where we randomly choose 200, 100 and 100
number from G1, G2 and G3 respectively for each repetition.

Finally, we consider the USPS handwriting dataset, analogous to MNIST, which contains digits
automatically scanned from envelopes by the U.S. Postal Service containing a total of 9,298 16×16
pixel grayscale samples. We choose three clusters: G1 containing the number "0", G2 containing
the number "5" and G3 containing the number "7", so that the number of clusters is K = 3 in the
algorithms. The cluster sizes are unbalanced where we randomly choose 200, 100 and 100 number
from G1, G2 and G3 respectively for each repetition.

Now if we look at the time cost for two cases on MNIST (MNIST1 and MNIST2) in Table 4, we can
see that Wasserstein SDP (W-SDP), distance-based Wasserstein K-means (D-WKM) and barycenter-
based Wasserstein K-means (B-WKM) all have time complexity issues when we enlarge n. The large
variance for B-WKM for MNIST1 is due to the convergence of the algorithm. The total iterations for
B-WKM in case 1 achieves maximum iteration 100 for 1 replicate out of 10 total replicates. More
arguments for time complexity can be found in Appendix B.

B Additional details on simulation studies in Section 4.1

In this section, we provide more details of our simulation setups and results for Gaussian mixtures in
Section 4.1.

1We run all the simulations and experiments except for USPS datasets on the machine with Intel Core
i7-10700K 3.80 GHz 64 bit 8-core 16 Tread Processor and 16 GB DDR4 Memory; run experiments on USPS
datasets with 1.6 GHz Dual-Core Intel Core i5 and 8 GB 2133 MHz LPDDR3 Memory.

14

We set m1 = 40·r, m2 = r, m3 = 20·r which means that there are total 81·r number of distributions
in G1, 20 · r distributions in G2. The r is set to be 1, 2, 3, where we have n = 101, 202, 303
respectively. The mean for the Gaussian distributions are shown in the table below. The entries of
covariance matrices for the Gaussian distributions are chosen to be O(10−3) for µ1, µ2 and they are
chosen to be O(10−6) for µ3 and µ4. Then we scale down the distribution with scaling parameter
equals 0.5. This ensures that with high probability, all the distributions will fall into the bounded
range [0, 1] × [0, 1].

The algorithm we use to get the barycenter is Frank-Wolfe algorithm with 200 iterations. And we use
Sinkhorn divergence to calculate the Wasserstein distance. The regularization parameters for both
algorithms are chosen to be 10−3. To approximate the true distribution, ﬁrst we divide [0, 1] × [0, 1]
range into 80 × 80 grids, then we randomly sample 600 samples each time and count the number of
times it falls into certain grid to approximate the distribution. The results show us that for each n and
each iterations among 50 repetitions, all the distributions in G∗
2 will be assigned to same cluster, so it
will be reasonable to deﬁne that µ3 is misclassiﬁed if any copy of them are in the same cluster of an
arbitrarily chosen µ from G∗
2.

The arrangement of mean for Gaussian mixture models shown in Table 5 indicates that the distribu-
tions are set based on Example 3. Recall that in Section 4.1, ∆∗ := maxk=1,2 maxi,j∈Gk W2(µi, µj)
and ∆∗ := mini∈G1,j∈G2 W 2(µi, µj) are the maximum within-cluster distance and the minimum
between-cluster distance respectively. Table 7 shows that ∆∗ < ∆∗ on average and ∆∗ < ∆∗ for
around 80% among 50 repetitions. So we can expect Wasserstein SDP to correctly cluster all data
points in the Wassertein space. From Table 6 we can observe that in our settings the time cost for
Wasserstein SDP and distance-based Wasserstein K-means is relatively lower than the time cost
for barycenter-based Wasserstein K-means. But we can see that as n increases, the time cost for
B-WKM grows almost linearly w.r.t. n while almost quadratically for W-SDP and D-WKM. Thus we
should expect relatively higher time cost for W-SDP and D-WKM when n is sufﬁciently large, where
we can consider several methods to bring down the time cost (e.g., subsampling-based method for
SDP from Zhuang et al. [2022]).

Computationally speaking, the calculations of Wasserstein distances and barycenters are usually
based on one-step discretization and one-step application of entropic regularization methods such as
Sinkhorn (Genevay et al. [2018], Janati et al. [2020]). Dvinskikh and Tiapkin [2020] shows that the
complexity of calculating barycenters should be of the order O(nd2/(cid:15)2)) or O(ng4/(cid:15)2)), where n is
the total number of distributions, d = g2 is the discretization size, e.g. g = 28 for MNIST datasets
and (cid:15) is the numerical accuracy; while Le et al. [2021] gives a O(d2/(cid:15))) or O(g4/(cid:15))) complexity
algorithm for calculating the Wasserstein distance on robust optimal transport.

Table 5: Positions (x, y) ∈ R2 of means for two-dimensional mixture of Gaussian distributions for
the counter example in Section 4.1.

a1,1
0.75
1.15

a1,2
0.25
0.85

a2,1
0.75
0.85

a2,2
0.25
1.15

a3,1
0.9
0.85

a3,2
0.9
1.15

a4,1
1.3
0.75

a4,2
1.3
1.25

x
y

Table 6: The time cost with standard deviation shown in parentheses for the counter example. TC:
Time cost, W-SDP: Wasserstein SDP, D-WKM: Distance-based Wasserstein K-means, B-WKM:
Barycenter-based Wasserstein K-means.

n
101
202
303

TC for W-SDP (SD) TC for D-WKM TC for B-WKM (SD)
14.15 (0.5132)
54.98 (1.516)
123.9 (3.606)

14.50 (0.5873)
56.94 (1.490)
128.4 (3.640)

181.1 (372.4)
341.0 (136.2)
549.2 (200.2)

C Background on optimal transport

The optimal transport (OT) problem (a.k.a. the Monge problem) is to ﬁnd an optimal map T ∗ : Rp →
Rp for transporting a source distribution µ0 to a target distribution µ1 that minimizes some cost

15

Table 7: Estimated Wasserstein distances with standard deviation shown in parentheses and frequency
of ∆∗ > ∆∗ for the counter example.

n
101
202
303

∆∗
0.1978 (0.0055)
0.1990 (0.0058)
0.1996 (0.0067)

∆∗
0.2046 (0.0050)
0.2050 (0.0051)
0.2052 (0.0050)

Frequency of ∆∗ < ∆∗
0.8200
0.8200
0.7600

function c : Rp × Rp → R:

min
T :Rp→Rp

(cid:26)(cid:90)

Rp

c(x, T (x))dµ0(x) : T(cid:93)µ0 = µ1

,

(20)

(cid:27)

(21)

where T(cid:93)µ denotes the pushforward measure deﬁned by (T(cid:93)µ)(B) = µ(T −1(B)) for measurable
subset B ⊂ Rp. A standard example of the cost function is the quadratic cost c(x, y) = (cid:107)x−y(cid:107)2
2. The
Monge problem (20) with the quadratic cost induces a metric, known as the 2-Wasserstein distance,
on the space P2(Rp) of probability measures on Rp with ﬁnite second moments. In particular, the
2-Wasserstein distance can be expressed in the relaxed Kantorovich form:
(cid:27)

(cid:26)(cid:90)

W 2

2 (µ0, µ1) := min

γ

(cid:107)x − y(cid:107)2

2dγ(x, y)

,

Rp×Rp
where minimization over γ runs over all possible couplings with marginals µ0 and µ1 [Villani, 2003].
It is well-known from Brenier’s theorem [Brenier, 1991] that if the source measure µ0 does not
charge on small subsets of Rp (i.e., subsets of Hausdorff dimension at most p − 1), then there exists a
unique µ0-almost everywhere OT map T ∗ solving (20). That is, T ∗

(cid:93) µ0 = µ1 and

W 2

2 (µ0, µ1) =

(cid:90)

Rp

(cid:107)x − T ∗(x)(cid:107)2

2dµ0(x).

Let (µt)1
and t ∈ [0, 1], we have

t=0 be the constant-speed geodesic connecting µ0, µ1 ∈ P2(Rp). Then for any ν ∈ P2(Rp)

W 2

2 (µt, ν) (cid:62) (1 − t)W 2

(22)
The above semiconcavity inequality (22) can be interpreted as that the Wasserstein space P2(Rp) is a
positive curved metric space (PC-space) in the sense of Alexandrov (cf. Section 7.3 and Section 12.3
in Ambrosio et al. [2005]).

2 (µ1, ν) − t(1 − t)W 2

2 (µ0, ν) + tW 2

2 (µ0, µ1).

D Additional proofs

In this section, we will give detailed proofs for Example 1, Lemma 4 and Theorem 8. For the proof
of Theorem 8, we will ﬁrst introduce the main part and put the rest proofs of corresponding lemmas
at the end of this section to make it clear.

D.1 Proof of Example 1

Recall that µ0(s) = (1 − s)/2 and µ1(s) = (1 + s)/2 are probability densities supported on
the line segments L0 = {(s, as) : s ∈ [−1, 1]} and L1 = {(s, −as) : s ∈ [−1, 1]} for some
a ∈ (0, 1), respectively. To derive the optimal transport (OT) map T from µ0 to µ1, it sufﬁces to
consider the one-dimensional OT problem by parameterization of T : [−1, 1] → [−1, 1] identiﬁed
via (s, as) (cid:55)→ (T (s), −aT (s)). Then our goal is to ﬁnd the solution to the following optimization
problem

min
T :T(cid:93)µ0=µ1

= min

T :T(cid:93)µ0=µ1

(cid:90) 1

−1
(cid:90) 1

−1

(cid:107)(s, −as) − (T (s), −aT (s))(cid:107)2

2dµ0(s)

[(s − T (s))2 + a2(s + T (s))2]dµ0(s)

= (1 − a2) × min

T :T(cid:93)µ0=µ1

(cid:90) 1

−1

(cid:34)(cid:114) 1 + a2

1 − a2 T (s) − s

(cid:35)2

dµ0(s) + constant,

16

where the constant does not depend on T . Now rescale the distribution density µ1 to
(cid:33)
(cid:32)(cid:114) 1 − a2
(cid:114) 1 + a2
1 + a2 s
1 − a2

(cid:114) 1 + a2
1 − a2 ,

1 + a2 µ1

(cid:114) 1 − a2

˜µ1(s) =

for s ∈

−

(cid:34)

(cid:35)

,

and deﬁne the transport map ˜T =
it sufﬁces to ﬁnd the OT map ˜T such that ˜T(cid:93)µ0 = ˜µ1, i.e.,

(cid:113) 1+a2

1−a2 T on [−1, 1]. To ﬁnd the OT map T such that T(cid:93)µ0 = µ1,

min
˜T : ˜T(cid:93)µ0=˜µ1

(cid:90) 1

−1

[ ˜T (s) − s]2dµ0(s),

whose solution is known as the quantile transform for one-dimensional distributions. Speciﬁcally, let

F0(s) =

(cid:90) s

−1

µ0(t)dt =

(cid:18)

1
2

s −

1
2

s2 +

(cid:19)

3
2

for s ∈ [−1, 1],

be the cumulative distribution function (cdf) of the density µ0 and

˜F1(s) =

(cid:90) s

(cid:114)

−

1+a2
1−a2

˜µ1(t)dt =

(cid:32)(cid:114) 1 − a2

1 + a2 s +

1
2

1
2

·

1 − a2
1 + a2 s2 +

1
2

(cid:33)

(cid:34)

for s ∈

−

(cid:114) 1 + a2
1 − a2 ,

(cid:114) 1 + a2
1 − a2

(cid:35)

,

be the cdf of the density ˜µ1. It is easy to ﬁnd that

˜F −1
1

(y) =

(cid:114) 1 + a2

1 − a2 ((cid:112)4y − 1)

for y ∈ [0, 1].

Then the OT map ˜T from µ0 to ˜µ1 is given by
(cid:114) 1 + a2

˜T (s) = ˜F −1

1

◦ F0(s) =

1 − a2 [−1 + (cid:112)4 − (1 − s)2],

s ∈ [−1, 1].

This gives the OT map T from µ0 to µ1 (in the one-dimensional parameterization form) as

T (s) = −1 + (cid:112)4 − (1 − s)2.

(23)

Thus, the OT map T from µ0 to µ1 as (degenerate) probability distribution in R2 is given by

T (s, as) = (cid:0) − 1 + (cid:112)4 − (1 − s)2, −a · (−1 + (cid:112)4 − (1 − s)2)(cid:1).

D.2 Proof of Lemma 4 in Section 2.1

Recall the settings as following

µ1 = 0.5 δ(x,y) + 0.5 δ(−x,−y), µ2 = 0.5 δ(x,−y) + 0.5 δ(−x,y),
µ3 = 0.5 δ(x+(cid:15)1,y) + 0.5 δ(x+(cid:15)1,−y),

and µ4 = 0.5 δ(x+(cid:15)1+(cid:15)2,y) + 0.5 δ(x+(cid:15)1+(cid:15)2,−y),

where δ(x,y) denotes the point mass measure at point (x, y) ∈ R2, and (x, y, (cid:15)1, (cid:15)2) are positive
constants.

Lemma 4 (Conﬁguration characterization). If (x, y, (cid:15)1, (cid:15)2) satisﬁes

y2 < min{x2, 0.25 ∆(cid:15)1,x}
1 + 2x2 + 2x(cid:15)1, then for all sufﬁciently large m (number of copies of µ1 and µ2),

and ∆(cid:15)1,x < (cid:15)2

2 < ∆(cid:15)1,x + y2,

where ∆(cid:15)1,x := (cid:15)2
W2(µ3, µ∗

2) < W2(µ3, µ∗
1)

and

max
i,j∈Gk

W2(µi, µj)

<

max
k=1,2
(cid:124)

W2(µi, µj),

min
i∈G1,j∈G2
(cid:124)

(cid:123)(cid:122)
largest within-cluster distance

(cid:125)

(cid:123)(cid:122)
least between-cluster distance

(cid:125)

k denotes the Wasserstein barycenter of cluster Gk for k = 1, 2.

where µ∗
Proof. For any wi ∈ R2, i = 1, 2, 3, 4, let µ = 0.5 δw1 + 0.5 δw2, ν = 0.5 δw3 + 0.5 δw4. By
deﬁnition of Wasserstein distance we can show that

W 2

2 (µ, ν) = 0.5 min{(cid:107)w1 − w3(cid:107)2 + (cid:107)w2 − w4(cid:107)2, (cid:107)w1 − w4(cid:107)2 + (cid:107)w2 − w3(cid:107)2}.

17

Let µ0 = 0.5 δ(x,0) + 0.5 δ(−x,0), by algebraic calculation it is direct to check

W2(µ3, µ∗

2) < W2(µ3, µ0)

and

max
i,j∈Gk

W2(µi, µj)

<

max
k=1,2
(cid:124)

W2(µi, µj),

min
i∈G1,j∈G2
(cid:124)

(cid:123)(cid:122)
largest within-cluster distance

(cid:125)

(cid:123)(cid:122)
least between-cluster distance

(cid:125)

2 (µ3, µ∗

once plugging in the assumptions. So we only need to show that ∀ε, ∃M , s.t. when m > M we
have W 2
2 (µ3, µ0) − ε. For notation simplicity, let vx = (x, 0), v−x = (−x, 0), v1 =
(x, y), v2 = (−x, −y), v3 = (x, −y), v4 = (x, −y). By deﬁnition we know there exist measures
ξi, i = 1, 2, 3, 4, s.t.

1) ≥ W 2

W 2

2 (µ∗

1, µ1) =

W 2

2 (µ∗

1, µ2) =

(cid:90)

(cid:90)

(cid:107)v − v1(cid:107)2dξ1(v) +

(cid:107)v − v3(cid:107)2dξ3(v) +

(cid:90)

(cid:90)

(cid:107)v − v2(cid:107)2dξ2(v),

(cid:107)v − v4(cid:107)2dξ4(v),

where µ∗
{1, 2}, j ∈ {3, 4}, then ξi = ξi,3 + ξi,4, ξj = ξ1,j + ξ2,j, i ∈ {1, 2}, j ∈ {3, 4}. Thus

1 = ξ1 + ξ2 = ξ3 + ξ4 with ξi(R2) = 0.5, ∀i. Furthermore, if we deﬁne ξi,j = ξi · ξj/µ∗

1, i ∈

W 2

2 (µ∗

1, µ1) + W 2

2 (µ∗

1, µ2) =

4
(cid:88)

(cid:90)

i=1

(cid:107)v − vi(cid:107)2dξi(v)

(cid:88)

=

i∈{1,2},j∈{3,4}

(cid:90)

(cid:107)v − vi(cid:107)2 + (cid:107)v − vj(cid:107)2dξi,j(v).

Now suppose t = (cid:107)v − vx(cid:107), by algebraic calculation we can get

Choose T > 0 s.t. T 2 < min{2x2 − 2y2, y2}, then we have

(cid:107)v − v1(cid:107)2 + (cid:107)v − v3(cid:107)2 = t2 + 2y2.

W 2

2 (µ∗

1, µ1) + W 2

2 (µ∗

1, µ2) =

(cid:88)

(cid:90)

i∈{1,2},j∈{3,4}

(cid:107)v − vi(cid:107)2 + (cid:107)v − vj(cid:107)2dξi,j(v)

(cid:90)

≤

BT (vx)

(cid:107)v − v1(cid:107)2 + (cid:107)v − v3(cid:107)2dξ1,3(v) +

(cid:90)

BT (v−x)

(cid:107)v − v2(cid:107)2 + (cid:107)v − v4(cid:107)2dξ2,4(v)

+ (T 2 + 2y2)(1 − ξ1,3(BT (vx)) − ξ2,4(BT (v−x)))

(cid:90)

=

t1(v)2dξ1,3(v) +

(cid:90)

BT (vx)

BT (v−x)

t2(v)2dξ2,4(v) + 2y2 + T 2(1 − ξ1,3(BT (vx)) − ξ2,4(BT (v−x))),

where Bt(v) stands for the ball with radius t centered at v, t1(v) := (cid:107)v − vx(cid:107), t2(v) := (cid:107)v − v−x(cid:107).
On the other hand, by deﬁnition we know that

1, µ1) + m · W 2
2 (µ∗
2 (µ0, µ1) + m · W 2

m · W 2
2 (µ∗
≤ m · W 2
= m · (2y2) + C,

1, µ2) + W 2
2 (µ∗
2 (µ0, µ2) + W 2

1, µ3)
2 (µ0, µ3)

2 (µ0, µ3). So we have W 2

2 (µ∗

1, µ1) + W 2

2 (µ∗

1, µ2) ≤ 2y2 + C/m. i.e.,

t1(v)2dξ1,3(v)+

(cid:90)

BT (v−x)

t2(v)2dξ2,4(v)+T 2(1−ξ1,3(BT (vx))−ξ2,4(BT (v−x))) ≤

C
m

.

where C := W 2
(cid:90)

BT (vx)

So we have

(cid:90)

BT (vx)

t1(v)2dξ1,3(v) ≤

C
m

,

(cid:90)

BT (v−x)

t2(v)2dξ2,4(v) ≤

C
m

,

0.5 − ξ1,3(BT (vx)) ≤

C
T 2m

, 0.5 − ξ2,4(BT (v−x)) ≤

C
T 2m

.

18

1 + y2 and
2 (µ3, µ0) = 0.5(cid:107)vx − v(cid:15)1(cid:107)2 + 0.5(cid:107)v−x − v−(cid:15)1(cid:107)2. By deﬁnition of Wasserstein distance and

Now suppose v(cid:15)1 := (x + (cid:15)1, y), v−(cid:15)1 := (x + (cid:15)1, −y), note that T 2 < y2 < (cid:15)2
W 2
symmetry we have

W 2

2 (µ3, µ∗

1) ≥

(cid:90)

BT (vx)

((cid:107)vx − v(cid:15)1 (cid:107) − t1(v))2dξ1,3(v) +

(cid:90)

BT (v−x)

((cid:107)v−x − v(cid:15)1 (cid:107) − t2(v))2dξ2,4(v)

≥ (cid:107)vx − v(cid:15)1(cid:107)2ξ1,3(BT (vx)) + (cid:107)v−x − v(cid:15)1(cid:107)2ξ2,4(BT (v−x))

− 2(cid:107)vx − v(cid:15)1 (cid:107)

t1(v)dξ1,3(v) − 2(cid:107)vx − v−(cid:15)1 (cid:107)

t2(v)dξ2,4(v)

(cid:90)

BT (vx)

(cid:90)

BT (v−x)

≥ W 2

2 (µ3, µ0) − C 2
1 ·

C
T 2m

− C 2
2 ·

(cid:90)

− 2C1

t1(v)dξ2,4(v) − 2C2

C
T 2m
(cid:90)

t2(v)dξ2,4(v),

BT (vx)

BT (v−x)

where C1 = (cid:107)vx − v(cid:15)1 (cid:107), C2 = (cid:107)v−x − v(cid:15)1 (cid:107). Set ∀ε > 0. Finally, by Hölder’s inequality we have

W 2

2 (µ3, µ∗

1) ≥ W 2

2 (µ3, µ0) − C 2
1 ·
(cid:115)(cid:90)

C
T 2m

− C 2
2 ·

C
T 2m
(cid:115)(cid:90)

− 2C1

t2
1(v)dξ2,4(v) − 2C2

t2
2(v)dξ2,4(v)

BT (vx)

BT (v−x)

≥ W 2

2 (µ3, µ0) − C 2
1 ·

≥ W 2

2 (µ3, µ0) − ε,

C
T 2m

− C 2
2 ·

C
T 2m

− 2C1

(cid:114)

C
m

− 2C2

(cid:114)

C
m

for large m, as desired.

D.3 Proof of Theorem 8 in Section 3

(cid:4)

Theorem 8 (Exact recovery for clustering Gaussians). Let ∆2 := mink(cid:54)=l d2(V (k), V (l)) denote
the minimal pairwise separation among clusters, ¯n := maxk∈[K] nk (and n := mink∈[K] nk) the
maximum (minimum) cluster size, and m := mink(cid:54)=l
the minimal pairwise harmonic mean
of cluster sizes. Suppose the covariance matrix Vi of Gaussian distribution νi = N (0, Vi) is
independently drawn from model (18) for i = 1, 2, . . . , n. Let β ∈ (0, 1). If the separation ∆2
satisﬁes

2nknl
nk+nl

∆2 > ¯∆2 : =

C1t2
min{(1 − β)2, β2}

V p2 log n,

then the SDP (17) achieves exact recovery with probability at least 1 − C2n−1, provided that

(cid:112)log n/(cid:2)(p + log ¯n)V 1/2T 1/2

v

(cid:3), n/m ≤ C5 log n,

n ≥ C3 log2 n, t ≤ C4
(cid:13)V (k)(cid:13)
(cid:13)

where V = maxk
Lemma 10 (Dual argument for SDP (Section B in Chen and Yang [2021])). The sufﬁcient condition
for Z ∗ = (cid:80)
to be the unique solution of the SDP problem is to ﬁnd (λ, α, B) s.t.

(cid:13)op, Tv = maxk Tr(cid:2)(cid:0)V (k)(cid:1)−1(cid:3), and Ci, i = 1, 2, 3, 4, 5 are constants.

k∈[K]

1
nk

1Gk 1T
Gk

(C1) B ≥ 0 (BGkGk = 0, BGkGl > 0, ∀k (cid:54)= l),

1
(C2) Wn := λId +
2
(C3) Tr(WnZ ∗) = 0,
(C4) Tr(BZ ∗) = 0,

(1αT + α1T ) − A − B (cid:23) 0,

which implies that

αGk =

2
nk

AGkGk 1nk −

λ
nk

1nk −

1
n2
k

(1T
nk

AGkGk 1nk ).

19

[BGlGk 1nk ]j = −

nl + nk
2nl

λ +





1
n2
l

nk
2

(cid:88)

s,r∈Gl

d2(Vs, Vr) −



d2(Vs, Vr)



1
n2
k

(cid:88)

s,r∈Gk

+ nk

(cid:34)

1
nk

(cid:88)

r∈Gk

d2(Vj, Vr) −

(cid:35)

d2(Vj, Vr)

,

1
nl

(cid:88)

r∈Gl

for k (cid:54)= l, j ∈ Gl.

Remarks. It can be justiﬁed that if we can ﬁnd (λ, B) satisfying above equations, then (C3), (C4)
will hold automatically. Details can be found in Section B in Chen and Yang [2021].

Now we will proof the main theorem by two steps. First we will provide a lower bound for
[BGlGk 1nk ]j. Similar to the argument from Chen and Yang [2021], we want to set λ properly such
that (C1) can hold. In the next step we will try to verify that the choice of (λ, α, B) and the conditions
on the signals could actually imply (C2). And since number of clusters K is treated as ﬁxed for most
practical settings, we will not emphasize K = O(1).

D.3.1 Proof of main result.

Step 1 (Construct (λ, B)). Recall [BGlGk 1nk ]j = − nl+nk
2nl

λ + nkL, where L equals





1
2

1
n2
l

(cid:88)

s,r∈Gl

d2(Vs, Vr) −

1
n2
k

(cid:88)

s,r∈Gk



(cid:34)

d2(Vs, Vr)

 +

1
nk

(cid:88)

r∈Gk

d2(Vj, Vr) −

(cid:35)

d2(Vj, Vr)

.

1
nl

(cid:88)

r∈Gl

For L deﬁned above, by Lemma 14, we have

L ≥ d2(V (l), V (k)) − d(V (l), V (k))K1 − K2,

w.p. at least (1 − c/n2), where

K1 = C(cid:112)log ntV 1/2 + Ct2(p + log ¯n)VT 1/2
K2 = Ct2p2 log nV,

v

,

for some constant C, c. Now we chose β ∈ (0, 1) and let m := mink(cid:54)=l

2nknl
nk+nl

. If we suppose

∆ ≥ Ctp(cid:112)log nV 1/2/(1 − β), t ≤ C (cid:48)(cid:112)log n/(cid:2)(p + log ¯n)V 1/2T 1/2

v

(cid:3),

for some constant C, C (cid:48), then we have

(1 − β)d2(V (l), V (k)) − d(V (l), V (k))K1 − K2 ≥ 0, ∀k (cid:54)= l,

which implies that

Deﬁne for k (cid:54)= l,

L ≥ βd2(V (l), V (k)).

c(k,l)
j
r(k,l)
i

:= [BGlGk 1nk ]j, j ∈ Gl,

BGlGk ]i, i ∈ Gk,

:= [1T
nl
t(k,l) := 1T
nl
)ij := r(k,l)

(B#

GlGk

BGlGk 1nk ,
c(k,l)
j

i

/t(k,l).

And deﬁne (B#

GlGl

)ij := 0, ∀l. By setting λ = β

4 m∆2, further we have

c(k,l)
j

≥

β
2

nkd2(V (l), V (k)), r(k,l)

i

≥

β
2

nld2(V (l), V (k)), t(k,l) ≥

β
2

nlnkd2(V (l), V (k)),

)ij > 0, ∀i ∈ Gk, j ∈ Gl. And [BGlGk 1nk ]j = [B#

which implies that (B#
1nk ]j, which
means we can construct B# based on [BGlGk 1nk ]j with [BGlGk 1nk ]j = [B#
1nk ]j. So essen-
tially, they are the same in the sense that we only care about they quantity through [BGlGk 1nk ]j. And
thus for notation simplicity, we will use the symbol B instead of B#.

GlGk

GlGk

GlGk

20

Step 2 (Verify the condition for Wn in (C2)). Next we would like to ﬁnd sufﬁcient condition for
(C2), i.e.,

vT Wnv ≥ 0, ∀v ∈ ΓK := span{1Gk : k ∈ [K]}⊥, (cid:107)v(cid:107) = 1.
Note that vT Wnv = λ−vT Av −vT Bv ≥ λ−vT Bv. And by deﬁnition as well as simple calculation
we have

vT Bv =

K
(cid:88)

(cid:88)

k=1

l(cid:54)=k

1
t(k,l)

(cid:32)

vir(k,l)
i

(cid:88)

i∈Gk

(cid:33) 




(cid:88)

vjc(k,l)
j

 ,

(cid:32)

(cid:88)

j∈Gl

1
nk

(cid:88)

r∈Gk

d2(Vj, Vr) −

j∈Gl

1
nl

(cid:88)

r∈Gl

(cid:33)

d2(Vj, Vr)

vj.

(cid:88)

vjc(k,l)
j

= nk

j∈Gl
Further note that
1
nk

d2(Vj, Vr) −

(cid:88)

r∈Gk

1
nl

(cid:88)

r∈Gl

d2(Vj, Vr) = d2(V (l), V (k)) + E(k,l)

j

,

where

E(k,l)
j

=

(cid:34)

1
nk

(cid:88)

r∈Gk

d2(Vj, Vr) − d2(Vj, V (k))

(cid:35)

(cid:104)
d2(Vj, V (k)) − d2(V (l), V (k))

(cid:105)

+

−

1
nl

(cid:88)

r∈Gl

d2(Vj, Vr).

Then by triangle inequality and throwing away the last term of E(k,l)
j
(E(k,l)

(cid:88)

(cid:88)

(cid:88)

vj ≤ nk

= nk

vjc(k,l)
j

E(k,l)
j

1,j + E(k,l)

2,j )|vj|,

, we have

j∈Gl

j∈Gl

j∈Gl

where

E(k,l)

1,j =

(cid:34)

d2(V (k), Vr) +

(cid:88)

d(V (k), Vr)d(Vj, V (k))

,

(cid:35)

2
nk

r∈Gk
2,j = d2(V (l), Vj) + 2d(V (l), Vj)d(V (l), V (k)).

r∈Gk

(cid:88)

1
nk
E(k,l)

If we set ˜E(k,l)

h,j = E(k,l)
h,j /d(V (l), V (k)), h = 1, 2, then the inequality can be written as
(cid:88)

(cid:88)

vjc(k,l)
j

≤ nkd(V (l), V (k))

( ˜E(k,l)

1,j + ˜E(k,l)

2,j )|vj|.

j∈Gl
By Lemma 15 we know

j∈Gl

˜E(k,l)
1,j

|vj| ≤ CV 1/2pt

(cid:88)

j∈Gl

√



nl



(cid:88)



1/2

v2
j



,

j∈Gl

w.p. ≥ 1 − cn−2. And by Lemma 16 we have

˜E(k,l)
2,j

|vj| ≤ CtV 1/2p(

√

nl + log2(n))

(cid:88)

j∈Gl





(cid:88)



1/2

v2
j



,

j∈Gl

w.p. ≥ 1 − cn−1, for some constants C, c. Now if we assume mink nk ≥ C log2 n and notice that
t(k,l) ≥ β

2 nlnkd2(V (l), V (k)), then further we can get
1/2 (cid:32)





vT Bv ≤

(cid:88)

nknl
t(k,l)

√

√

nl

nk



(cid:88)

v2
j



(cid:88)

(cid:33)1/2

v2
i

Ct2Vp2





(cid:88)

(cid:88)

v2
j



j∈Gl
1/2 (cid:32)



i∈Gk

(cid:33)1/2 (cid:32)

(cid:88)

nl

(cid:88)

v2
i

(cid:33)1/2 (cid:32)

(cid:88)

(cid:33)1/2

nk

Vp2

(cid:88)

k,l

Ct2
β

Ct2
β

≤

=

l

j∈Gl

l

k

i∈Gk

k

p2nV,

21

where the second inequality comes from Cauchy-Schwarz inequality. So by assuming

∆2 ≥

Ct2
β2 V · p2n/m,

for some constant C, we have

vT Wnv ≥ λ − vT Bv ≥

β
4

m∆2 −

Ct2
β

p2nV > 0.

Or it is sufﬁcient to assume

∆2 ≥

Ct2
β2 V · p2 log n,

if n/m = O(log n). To sum up, if we assume

∆2 ≥

Ct2

min{1 − β, β}2 V · p2 log n,

then w.p. ≥ 1 − c/n, we have (C1) − (C4) hold by the construction of (λ, B) for some constants
C, c. Finally by Lemma 10 we know the solution of SDP Z ∗ exists uniquely, which is

Z ∗ =

(cid:88)

k∈[K]

1
nk

1Gk 1T
Gk

(cid:4)
as desired.
Remarks. In our theorem, we focus on the relation between minimum cluster distance ¯∆ with number
of distributions n, which should be tight enough in the sense that ¯∆ (cid:16)
log n. This is the same order
for the cut-off of exact recovery of SDP for Euclidean case from Chen and Yang [2021].

√

On the other hand, one sufﬁcient condition for Vi, i = 1, . . . , n to be psd is 1 − t maxi (cid:107)Xi(cid:107)op > 0,
which will hold w.p. ≥ 1 − c/n2 if t ≤ C/[
log n] for some constant C, c. Recall from our
assumption,

p +

√

√

t ≤ c(cid:112)log n/[(p + log ¯n)V 1/2T 1/2

] ≤ c(cid:112)log n/(p + log ¯n),

v

for some constant, since Tv = maxk Tr((V (k))−1) ≥ p/ mink (cid:107)V (k)(cid:107)op. This indicates that our
bound for t guarantees Vi to be psd w.p. ≥ 1 − c/n2 as n (cid:16) ¯n. One may apply triangle inequality
directly to Lemma 14 to get the upper bound of t with less order in p, which is of less concern in our
theorem, where we put more emphasis on the order in n.

D.3.2 Proofs of lemmas.

Before proving Lemma 14, let us ﬁrst look at the Taylor expansion for psd matrix.
Lemma 11 (Taylor expansion for psd matrix (Theorem 1.1 in Del Moral and Niclas [2017])). The
square root function ϕ : Q ∈ S +
r with the ﬁrst
order derivative given for any (A, H) ∈ S +

r (cid:55)→ Q1/2 is Fréchet differentiable at any order on S +

r × Sr by the formula

∇ϕ(A) · H =

(cid:90) ∞

0

e−tϕ(A)He−tϕ(A)dt,

r , Sr are the positive semi-deﬁnite matrix and symmetric matrix respectively. The higher

where S +
order derivatives are deﬁned inductively for any n ≥ 2 by


∇nϕ(A) · H = −∇ϕ(A) ·



(cid:88)

p+q=n−2&p,q≥0

n!
(p + 1)!(q + 1)!

[∇p+1ϕ(A) · H][∇q+1ϕ(A) · H]



 .

Again from the same paper, we have the Taylor expansion for ϕ(A):

with

ϕ(A + H) = ϕ(A) +

(cid:88)

1≤k≤n

1
k!

∇kϕ(A) · H + ¯∇n+1ϕ[A, H],

¯∇n+1ϕ[A, H] :=

1
n!

(cid:90) 1

0

(1 − (cid:15))n∇n+1ϕ(A + (cid:15)H) · Hd(cid:15).

22

Corollary 12 (Decomposition of Wasserstein distance for Gaussians). If we choose n = 1 in
Lemma 11, we have for k (cid:54)= l, j ∈ G∗
l , and under the assumptions in the Theorem, the following
expansion holds.

d2(Vj, V (k)) − d2(Vj, V (l))

=d2(V (l), V (k)) +

(cid:68)
A(V (l), V (k)), t(XjV (l) + V (l)Xj) + t2XjV (l)Xj

(cid:69)

−d2(Vj, V (l)) − ∆0,

d2(Vj, Vr) − d2(Vj, V (k))

(cid:88)

r∈Gk

1
nk
(cid:42)

=

A(Vj, V (k)),

t(XrV (k) + V (k)Xr) + t2XrV (k)Xr

− ∆1,

(cid:43)

1
nk

(cid:88)

r∈Gk

where A(U, V ) := Id − U 1/2(U 1/2V U 1/2)−1/2U 1/2, for U, V : psd. And ∆0 ≤ 0, ∆1 ≤ 0, which
are extra terms (high order terms in Lemma 11).

Proof. By deﬁnition we know d2(V, U ) = W 2

2 (ν, µ), where ν ∼ N (0, V ), µ ∼ N (0, U ). Thus

d2(V, U ) = Tr(V ) + Tr(U ) − 2Tr[

(cid:112)

V 1/2U V 1/2].

So we have

d2(Vj, V (k)) − d2(V (k), V (l))

= Tr[Vj − V (l)] − 2Tr

(cid:20)(cid:113)

(V (k))1/2Vj(V (k))1/2 −

(cid:113)

(V (k))1/2V (l)(V (k))1/2

(cid:21)

.

On the other hand, by deﬁnition we know Vj = (I + tXj)V (l)(I + tXj) = V (l) + t(XjV (l) +
V (l)Xj) + t2XjV (l)Xj. Then by Lemma 11 and note the second order remainder term is always
(cid:4)
negative semi-deﬁnite, we can directly get the results by ﬁrst order Taylor expansion.

Lemma 13 (Norm for operator A). We conclude that for any U, V : psd, we have

(cid:107)A(U, V ) · V 1/2(cid:107)2

F = (cid:107)V 1/2 · A(U, V )(cid:107)2

F = d2(U, V ).

Proof. Suppose we have the SVD

U 1/2V 1/2 = QT

1 ΣQ2,

then we have

which implies that

A(U, V ) · V 1/2 = (I − U 1/2(U 1/2V U 1/2)−1/2)V 1/2
= V 1/2 − U 1/2QT

1 Q2,

(cid:107)A(U, V ) · V 1/2(cid:107)2

F = Tr(V ) + Tr(U ) − 2Tr(V 1/2U 1/2QT

1 Q2)

= Tr(V ) + Tr(U ) − 2Tr(QT
(cid:112)

= Tr(V ) + Tr(U ) − 2Tr(

2 ΣQ2)

U 1/2V U 1/2).

Lemma 14 (Lower bound for L). Recall that L equals





1
2

1
n2
l

(cid:88)

s,r∈Gl

d2(Vs, Vr) −

1
n2
k

(cid:88)

s,r∈Gk



(cid:34)

d2(Vs, Vr)

 +

1
nk

(cid:88)

r∈Gk

d2(Vj, Vr) −

1
nl

(cid:88)

r∈Gl

(cid:4)

(cid:35)

d2(Vj, Vr)

,

23

we have

w.p. at least (1 − c/n2), where

L ≥ d2(V (l), V (k)) − d(V (l), V (k))K1 − K2,

K1 = C(cid:112)log ntV 1/2 + Ct2(p + log ¯n)VT v1/2,
K2 = Ct2p2 log nV,

for some constant C, c.

Proof. First note that we can decompose the term into three terms:

1
nk

(cid:88)

r∈Gk

d2(Vj, Vr) −

1
nl

(cid:88)

r∈Gl

d2(Vj, Vr) = U1 − U2 + U3,

where

U1 :=

U2 :=

1
nk

1
nl

(cid:88)

d2(Vj, Vr) − d2(Vj, V (k)),

r∈Gk
(cid:88)

d2(Vj, Vr) − d2(Vj, V (l))

r∈Gl

U3 := d2(Vj, V (k)) − d2(Vj, V (l)).

If we further deﬁne U0 := 1
2

(cid:104) 1
n2
l

(cid:80)

d2(Vs, Vr) − 1
n2
k

(cid:80)

s,r∈Gl
L = U0 + U1 − U2 + U3.
From Corollary 12 we know U1 and U2 can be lower bounded by throwing out the remainders ∆1, ∆2,
i.e.,

s,r∈Gk

(cid:105)
d2(Vs, Vr)

, then we have

d2(Vj, Vr) − d2(Vj, V (k))

U1 =

(cid:88)

r∈Gk

1
nk
(cid:42)

≥

A(Vj, V (k)),

t(XrV (k) + V (k)Xr) + t2XrV (k)Xr

,

(cid:43)

1
nk

(cid:88)

r∈Gk

U3 =d2(Vj, V (k)) − d2(Vj, V (l))

≥d2(V (l), V (k)) +

(cid:68)

−d2(Vj, V (l)).

A(V (l), V (k)), t(XjV (l) + V (l)Xj) + t2XjV (l)Xj

(cid:69)

As for the U0 and U3, we choose to use triangle inequality to get a rough bound, i.e., by noting
d(Vj, Vr) ≤ d(Vj, V (l)) + d(Vr, V (l)), we have

And

U2 =

≤

1
nl

1
nl

(cid:88)

r∈Gl
(cid:88)

r∈Gl

d2(Vj, Vr) − d2(Vj, V (l))

d2(V (l), Vr) +

2
nl

d(V (l), Vr)

(cid:88)

r∈Gl

d(Vj, V (l)).

U0 =





1
2

1
n2
l

(cid:88)

s,r∈Gl

d2(Vs, Vr) −

d2(Vs, Vr)





1
n2
k

(cid:88)

s,r∈Gk

≥ −

1
2

1
n2
l

≥ −

2
nl

(cid:88)

(d(Vs, V (k)) + d(Vr, V (k)))2

s,r∈Gk
(cid:88)

d2(V (k), Vr).

r∈Gk

24

For the RHS of the inequality for U1, it can be divided into two parts.

and

(cid:42)

Z 1

1 :=

A(Vj, V (k)),

1
nk

(cid:88)

r∈Gk

t(XrV (k) + V (k)Xr)

(cid:43)

(cid:42)

Z 1

2 :=

A(Vj, V (k)), t2 1
nk

(cid:88)

r∈Gk

(cid:43)

XrV (k)Xr

.

is

part

the ﬁrst
c1t2(cid:107)A(Vj, V (k))V (k)(cid:107)2
u) ≤ e−u2/2, ∀u > 0 and Lemma 13, we have

by
a Gaussian
F /nk, for some constant c1. By Gaussian tail bound P (|N (0, 1)| >

distribution whose

bounded

variance

can

be

|Z 1

1 | ≤ c2t(cid:112)log n(cid:107)V (k)(cid:107)1/2/

√

nk · d(Vj, V (k))

≤ c2t(cid:112)log nV 1/2 · d(Vj, V (k)),
w.p. ≥ 1 − c3/n2, for some constant c2, c3. On the other hand,

|Z 1

2 | = t2

A(Vj, V (k))(V (k))1/2,

(cid:88)

XrV (k)Xr(V (k))−1/2

(cid:42)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
≤ t2 1
nk

≤ t2

1
nk

(cid:88)

r∈Gk
(cid:88)

r∈Gk

1
nk

r∈Gk
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:13)(V (k))−1/2(cid:13)
(cid:13)
(cid:13)F

(cid:13)
(cid:13)

XrV (k)Xr(V (k))−1/2

· d(Vj, V (k))

(cid:107)Xr(cid:107)2 (cid:107)V (k)(cid:107)

· d(Vj, V (k))

(cid:43)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ t2 max
r∈Gk

(cid:107)Xr(cid:107)2 VT v1/2 · d(Vj, V (k))

≤ c4t2(p + log n)VT v1/2 · d(Vj, V (k)),

w.p. ≥ 1 − c5/n2, for some constant c4, c5. The last inequality can be implied from union bound and
Corollary 4.4.8 in Vershynin [2018]:

(cid:107)Xr(cid:107) ≤ C(

√

p + u), w.p. ≥ 1 − 4e−u2

.

Now by combining Z 1

1 , Z 1

2 we have

1 + Z 1
U1 ≥ Z 1
2
c2t(cid:112)log nV 1/2 + c4t2(p + log n)VT v1/2(cid:105)
(cid:104)

≥ −

· d(Vj, V (k)),

w.p. ≥ 1 − (c3 + c5)/n2.

For U0, we have

U0 ≥ −

= −

2
nk

2t2
nk

(cid:88)

r∈Gk

(cid:88)

r∈Gk
1
nk

d2(V (k), Vr)

Tr(XrV (k)Xr)

(cid:88)

Tr(X 2
r )

r∈Gk

≥ −2t2V

≥ −c6t2Vp2,

w.p. ≥ 1 − c7/n2 for some constant c6, c7. The equation is a direct result by deﬁnition of Wasserstein
distance for Gaussians:

d2(V (k), Vr) = Tr(V (k)) + Tr(Vr) − 2Tr(

(cid:113)

(V (k))1/2Vr(V (k))1/2).

25

Note here

(cid:113)

(V (k))1/2Vr(V (k))1/2 =

(cid:113)

(V (k))1/2(I + tXr)V (k)(I + tXr)(V (k))1/2

= (V (k))1/2(I + tXr)(V (k))1/2.

The last inequality can be derived through Bernstein’s inequality (Theorem 2.8.2 in Vershynin [2018])
by noting that Tr(X 2
r )) = p2. Similar to the argument for
U0, U1, after we apply high-dimensional bound for sub-Gaussian or sub-exponential distributions we
can get bound for U2, U3:

r ) is sub-exponential with mean E(Tr(X 2

U2 ≤

1
nl

(cid:88)

d2(V (l), Vr) +

r∈Gl
≤ c8t2Vp2 log n,

2
nl

(cid:88)

r∈Gl

d(Vr, V (l))d(V (l), Vj)

w.p. ≥ 1 − c9/n2, for some constant c8, c9.

U3 ≥ d2(V (l), V (k)) +

(cid:68)

A(V (l), V (k)), t(XjV (l) + V (l)Xj) + t2XjV (l)Xj

(cid:69)

− d2(Vj, V (l))
≥ d2(V (l), V (k)) −

(cid:104)

c2t(cid:112)log nV 1/2 + c4t2(p + log ¯n)VT v1/2(cid:105)

· d(V (l), V (k))

− c10t2(p + log n)pV,

w.p. ≥ 1 − c11/n2., for some constant c10, c11. Lastly, by noting d(Vj, V (k)) ≤ d(V (l), V (k)) +
d(Vj, V (l)) in U1, and combine them together we have

L = U0 + U1 − U2 + U3

≥ d2(V (l), V (k)) − d(V (l), V (k))K1 − K2,

w.p. at least (1 − c/n2), where

K1 = C(cid:112)log ntV 1/2 + Ct2(p + log ¯n)VT v1/2,
K2 = Ct2p2 log nV,

for some constant C, c.
Lemma 15 ( ˜E(k,l)

1,j upper bound). Suppose v ∈ ΓK := span{1Gk : k ∈ [K]}⊥, (cid:107)v(cid:107) = 1. Let

(cid:4)

E(k,l)

1,j =

1
nk

(cid:88)

r∈Gk

d2(V (k), Vr) +

(cid:34)

2
nk

(cid:88)

r∈Gk

d(V (k), Vr)d(Vj, V (k))

,

(cid:35)

and ˜E(k,l)

1,j = E(k,l)

1,j /d(V (l), V (k)). Then w.p. ≥ 1 − n−2, we have

˜E(k,l)
1,j

|vj| ≤ CV 1/2pt

(cid:88)

j∈Gl

√



nl



(cid:88)



1/2

v2
j



,

j∈Gl

r )) = p2, E((cid:112)Tr(X 2

Proof. Note E(Tr(X 2
r )) = p by Jensen’s inequality. From
high-dimension bound for sub-exponential and sub-Gaussian (Hoeffding’s inequality and Bernstein’s
inequality) we have that w.p. ≥ 1 − c/n2,
1
nk

r V (k)) ≤ CVp2t2,

d2(V (k), Vr) =

Tr(X 2

1
nk

r )) ≤ (cid:112)E(Tr(X 2

(cid:88)

(cid:88)

r∈Gk

(cid:88)

1
nk

r∈Gk
(cid:113)

(cid:88)

1
nk

d(V (k), Vr) =
for some constants C, c. Suppose that d(V (l), V (k)) ≥ C0tV 1/2√
C0. Then we have w.p. ≥ 1 − c/n2
d(Vj, V (k)) ≤ d(Vj, V (l)) + d(V (l), V (k)) ≤ Ctp(cid:112)log nV 1/2 + d(V (l), V (k)) ≤ Cd(V (l), V (k)),

log np, for some ﬁxed constant

r V (k)) ≤ CV 1/2pt,

Tr(X 2

r∈Gk

r∈Gk

26

for some large constant C. So we have w.p. ≥ 1 − c/n2

˜E(k,l)
1,j

|vj| ≤ CV 1/2pt

(cid:88)

j∈Gl

|vj| ≤ CV 1/2pt

(cid:88)

j∈Gl

√



nl



(cid:88)



1/2

v2
j



,

j∈Gl

where V = maxk (cid:107)V (k)(cid:107), for some large constant C.
Lemma 16 ( ˜E(k,l)

2,j upper bound). Suppose v ∈ ΓK := span{1Gk : k ∈ [K]}⊥, (cid:107)v(cid:107) = 1. Let

(cid:4)

E(k,l)

2,j = d2(V (l), Vj) + 2d(V (l), Vj)d(V (l), V (k)).

and ˜E(k,l)

2,j = E(k,l)

2,j /d(V (l), V (k)). Then w.p. ≥ 1 − n−1, we have

˜E(k,l)
2,j

(cid:88)

j∈Gl

|vj| ≤ CtV 1/2p(

√

nl + log2(n))





(cid:88)



1/2

v2
j



,

j∈Gl

Proof. First we make the following claim:
Claim 17. Following the above setting, w.p. ≥ 1 − cn−1, we have

d2(V (l), Vj)|vj| ≤ Ct2Vp2(

√

nl + log(n)2)

(cid:88)

j∈Gl





(cid:88)



1/2

v2
j



,

j∈Gl

d(V (l), Vj)|vj| ≤ CtV 1/2p

√

(cid:88)

j∈Gl



nl



(cid:88)



1/2

v2
j



,

j∈Gl

(24)

(25)

for some large constant C.

If the claim holds, by plugging in the lower bound for ∆ in the assumption, we have

˜E(k,l)
2,j

|vj| ≤

(cid:88)

j∈Gl

Ct2Vp2(

√

nl + log(n)2)
log n

√

C0tV 1/2p

≤ CtV 1/2p(

√

nl + log2(n))

+ CtV 1/2p

√



nl





1/2

(cid:88)

j∈Gl

v2
j









1/2

(cid:88)

v2
j



j∈Gl







1/2

(cid:88)

v2
j



j∈Gl

Proof of the claim. First we look at (25):

(cid:88)

d(V (l), Vj)|vj| ≤ tV 1/2 (cid:88)

(cid:113)

Tr(X 2

j )|vj|.

j∈Gl

j∈Gl

By Theorem 2.6.3 (General Hoeffding’s inequality) in Vershynin [2018] we have w.p.≥ 1 − c/n2,

(cid:88)

(cid:113)

j∈Gl

Tr(X 2

j )|vj| ≤ p

(cid:88)

j∈Gl

√

|vj| + Cp



nl



(cid:88)



1/2

v2
j



,

j∈Gl

for some constant C. i.e., w.p.≥ 1 − c/n2,

d(V (l), Vj)|vj| ≤ CtV 1/2p

√

(cid:88)

j∈Gl



nl



(cid:88)



1/2

v2
j



,

j∈Gl

27

for some constant C. Next we will show (24). First note that d2(V (l), Vj) ≤ t2VTr(X 2

j ), let

G1(v) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

j∈Gl

[Tr(X 2

j ) − ETr(X 2

j )]|vj|

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

then

(cid:88)

d2(V (l), Vj)|vj| ≤ t2VG1(v) + t2Vp2√

j∈Gl



nl



(cid:88)



1/2

v2
j



.

j∈Gl

W.O.L.G., we may assume v ∈ V := {v ∈ ΓK : (cid:107)v(cid:107) = 1}, (cid:107)G1(cid:107)V := supv∈V |G1(v)|. Then by
Theorem 4 in Adamczak [2008] we know

P((cid:107)G1(cid:107)V ≥ 2E(cid:107)G1(cid:107)V + s) ≤ exp

(cid:19)

(cid:18)

−

s2
3τ 2
1

(cid:18)

+ 3 exp

−

s
3(cid:107)M1(cid:107)ψ1

(cid:19)

,

where

τ 2
1 = sup
v∈V

(cid:88)

v2
j

E[Tr(X 2

j ) − ETr(X 2

j )]2 ≤ E[Tr(X 2

j )]2 ≤ p4,

M1 = max

j∈Gl,v∈V

j∈Gl
(cid:12)
(cid:12)vj[Tr(X 2

j ) − ETr(X 2

j )](cid:12)

(cid:12) ≤ max
j∈Gl

(cid:12)
(cid:12)[Tr(X 2

j ) − ETr(X 2

j )](cid:12)
(cid:12) .

By maximal inequality (Lemma 2.2.2 in van der Vaart and Wellner [1996]) we have
(cid:13)
(cid:13)[Tr(X 2
≤ C log(nl)p2.

j ) − ETr(X 2

j )](cid:13)

(cid:107)M1(cid:107)ψ1 ≤ C log(nl) max
j∈Gl

(cid:13)ψ1

So by choosing s = C log2(n)p2, we have w.p. ≥ 1 − c/n,

G1(v) ≤ 2E(cid:107)G1(cid:107)V + C log2(n)p2,

for some C, c. On the hand,

E(cid:107)G1(cid:107)V = E

[Tr(X 2

j ) − ETr(X 2

j )]|vj|

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:88)

(cid:88)

j∈Gl

≤

E|Tr(X 2

j ) − ETr(X 2

j )||vj|

j∈Gl

≤ 2E|Tr(X 2

1 )|

√



nl





1/2

(cid:88)

v2
j



= 2p2√



nl



(cid:88)

j∈Gl

j∈Gl


1/2

v2
j



.

So w.p. ≥ 1 − cn−1, we have

d2(V (l), Vj)|vj| ≤ Ct2Vp2(

√

nl + log(n)2)

(cid:88)

j∈Gl





(cid:88)



1/2

v2
j



.

j∈Gl

(cid:4)

28

