2
2
0
2

n
u
J

1
2

]

G
L
.
s
c
[

1
v
0
2
6
0
1
.
6
0
2
2
:
v
i
X
r
a

CoCoPIE XGen: A Full-Stack AI-Oriented Optimizing Framework

Xiaofeng Li, Bin Ren, Xipeng Shen, Yanzhi Wang
Contact: info@cocopie.ai

1

Introduction

Recent years have witnessed a rapid rise of Deep Learning. This Deep Neural Networks (DNN)
based technology has given drastically improved results over traditional machine learning on a
wide range of Artiﬁcial Intelligence (AI) tasks. It has quickly become the new norm of AI. Its
recent development has shown two important trends. First, the size of DNNs and their demands
for computing power have grown in a much higher rate than that of the underlying computing
hardware, as shown in Figure 1. Second, there is a growing demand for shifting the delivery of AI
capability from data centers on the cloud to edge or end devices, exempliﬁed by the fast emerg-
ing real-time AI-based apps running on smartphones, AR/VR devices, autonomous vehicles, and
various IoT devices. The shift has however been seriously hampered by the even larger gap be-
tween DNN computing demands and the computing power on edge or end devices. We call the
gap DNN-hardware speed gap.

To ﬁll the DNN-hardware speed gap, software solutions are essential. Even though many soft-
ware tools (e.g., PyTorch [2], TensorFlow [3], MNN [4], TVM [5], TensorRT [6]) have been created
in the recent years to tackle the issue, their effectiveness has been seriously limited by a princi-
pled shortcoming in their designs, namely siloed optimization. To bridge the huge gap illustrated in
Figure 1, optimizations have to happen at every layer in the software stack, from the DNN model
to its computation ﬂow, code generation, deployment and execution. More importantly, as the
results of these optimizations affect each other, their full potential cannot be unlocked unless they
are designed together in a hand-in-hand manner. None of the existing frameworks are doing that.
Some of them may cover optimizations on more than one layer of the software stack, but they are
designed separately, or, at the very best, in an only loosely related manner.

XGen from CoCoPIE is created to ﬁll the void. XGen is an optimizing framework for DNN. It
takes cross-cutting co-design as its ﬁrst-order consideration. Its full-stack AI-oriented optimizations
consist of a number of innovative optimizations at every layer of the DNN software stack, all
designed in a cooperative manner. The unique technology makes XGen able to optimize various
DNNs, including those with an extreme depth (e.g., BERT, GPT, other transformers), and generate
code that runs several times faster than those from existing DNN frameworks, while delivering the
same level of accuracy. This article provides an overview of the core technology inside XGen, its
product form, the comparisons with other DNN optimizing frameworks, and the demonstrations
of its use in several real-world applications. (XGen is potentially useful for accelerating both the
training and inferences of DNNs, although currently its main focus is DNN inference.)

2 Technology inside XGen

Figure 2 provides an overview of the key components of XGen and their roles in optimizing DNN.
As a full-stack optimizing framework for DNN, it optimizes a given DNN model at all layers of
the stack.

• For a given DNN model written in some common DNN APIs (e.g., PyTorch, TensorFlow),
XGen ﬁrst compresses the model through CoCo model optimizer, which reduces the size

1

 
 
 
 
 
 
Figure 1: Growing gap between the computing demands of DNNs and the computing power
offered by modern hardware including AI accelerators. [1]

and complexity of the DNN model in a manner friendly to the later optimizations via pattern-
based pruning and block-based pruning, along with the compatible model compression tech-
niques, such as quantization and knowledge distillation.

• The optimized DNN model then goes through CoCo DNN compiler, which consists of two-
level code optimizations. The high-level optimization streamlines the DNN computations at
the level of DNN graphs through DNN graph rewriting and universal DNN fusion. The low-
level optimization ensures efﬁcient (parallel) code being generated through pattern-conscious
code generation [7] and deep reuse. The output of the compiler is executable code that imple-
ments the DNN model for inference.

• The deployment of the generated DNN code and its execution on devices can be (optionally)
further optimized by CoCo DNN runtime, a lightweight runtime system that coordinates
DNN model deployment and optimizes resource utilization in the presence of multiple DNN
tasks. The runtime does it effectively through plastic DNN IR, synergistic adaptation, and DNN
co-scheduling.

Compression-compilation co-design ties the ﬁrst three components in the ﬂow together.

It re-
duces the DNN model size while preserving regular patterns and/or blocks in DNN kernels. It
enables co-optimizations of the model architecture and pruning with code generation through
an innovative compiler-aware neural architecture & pruning co-search (CAPS). As the patterns and
blocks are designed hand-in-hand with the compiler, the optimized DNN model exhibits a form
best ﬁt for the compiler to generate efﬁcient code for the target device. Both the high-level and
low-level optimizations in the compiler take advantage of the regular patterns, removing as much
redundant computation as possible while emitting code that fully taps into the underlying paral-
lel computing units and memory hierarchy. The last component, CoCo runtime, is designed to be
conscious of the effects of DNN model and code optimizations. It creates the optimal schedules
for a set of DNN models and other modules with complicated dependence, such that these tasks
can progress smoothly in a resource-constrained environment while meeting the expected quality
of service (QoS). It supports devices equipped with heterogeneous computing units (e.g., CPU,

2

Figure 2: High-level view of the core technology of XGen.

Figure 3: (a) Non-structured weight pruning and (b) two types of structured weight pruning.

GPU, DSP, DLA). It achieves that by distinctively co-optimizing the DNN models and schedules,
and combining ofﬂine AI model analysis and just-in-time priority adjustment.

XGen builds on a set of proprietary technology, invented in the 20+ combined years of re-
search by the three world-class research groups afﬁliated with CoCoPIE (Northeastern University,
The College of William & Mary, and North Carolina State University), and protected by over ten
patents. The principle of co-design threads the designs of all the components in XGen. We next
explain each of the components.

2.1 Model Optimization

Its main goal is to reduce the size and
The ﬁrst component of XGen optimizes DNN models.
complexities of the input DNN model.
It does it through unique DNN weight pruning tech-
niques named pattern-based pruning and block-based pruning, and uses a composability-driven method
to minimize the pruning time even in the presence of an enormous pruning space. XGen is also
compatible with orthogonal DNN compression techniques such as quantization and knowledge
distillation.

3

DNN models
& dataModel 
compressionHigh-level
optimizationLow-level
optimizationOptimized
DNN codeDeployment and executionPattern-based pruning
Block-based
pruningDNN graph rewriting
Universal DNN fusionPattern-conscious code generation
Deep reusePlastic DNN IR
Synergistic adaptation
DNN co-schedulingCompression-Compilation Co-designFull-Stack AI-Oriented OptimizationCoCo DNN CompilerCoCo Model OptimizerCoCo DNN RuntimeCompiler-aware neural architecture & pruning co-searchDNN weight pruning is one of the most effective ways to reduce the size of a DNN and its

computations. Prior work on DNN pruning falls into three categories.

1) Non-Structured Pruning. Figure 3(a) illustrates this method, where arbitrary weights can
It can give a high pruning rate (i.e., reduction in the number of weights) without
be pruned.
degrading the accuracy. However, for compiler and code optimization, non-structured pruning
incurs several challenges due to the irregularity in computation and memory access. Similarly,
for hardware acceleration, since the pruned models are stored in some sparse matrix format with
indices, they often lead to performance degradation in GPU and CPU implementations [8, 9, 10].
2) Structured Pruning. This method can produce smaller regular weight matrices. Figure 3 (b)
illustrates the typical structured pruning schemes: ﬁlter pruning and channel pruning [8]. Filter
and channel pruning can be considered as equivalent in that pruning a ﬁlter in the k-th layer is
equivalent to pruning the corresponding channel in the (k + 1)-th layer. Filter/channel pruning
is compatible with Winograd algorithm [11, 12] that has been used to accelerate computation of
the original DNNs. Due to the regular structure, the GPU/CPU implementations typically lead
to more signiﬁcant acceleration [8, 9]. However, the structured pruning suffers from notable accu-
racy loss [8, 9].

Other Types of Weight Pruning. There are a few other types of DNN pruning schemes (e.g.,
vector-based [10] and those supported by PyTorch and TensorRT) that produce sparsity between
the above deﬁnitions of non-structured and structured pruning. These pruning schemes aim to
ﬁnd a balance between the sparsity degree, the computation load, and the achieved accuracy.
These techniques essentially can be categorized into structured and non-structured pruning. They
either are not well compatible with the underlying vector machine of general-purpose computing
devices, or cannot well preserve the original model accuracy.

To address the limitations of the prior methods, XGen develops two hardware-aware compiler-
friendly DNN pruning methods, namely pattern-based pruning and block-based pruning. The former
gives the best known results on a class of DNNs. The latter is a generalization of the former; it
keeps most of the beneﬁts of the former, and at the same time, extends the applicability to all kinds
of DNNs.

2.1.1 Pattern-Based Pruning

Pattern-based pruning achieves the high accuracy of non-structured pruning and the hardware
friendliness of structured ones. It does that by creating ﬁne-grained pruning patterns inside the coarse-
grained structures.

Figure 4 illustrates the basic idea of pattern-based pruning. For each kernel (in a CONV ﬁlter), a
ﬁxed number of weights are pruned, and the remaining weights (white cells) form speciﬁc “pat-
terns”. We deﬁne the example in Figure 4 as 4-entry pattern pruning, since every kernel reserves
4 non-zero weights out of the original 3 × 3 kernel (the most commonly used kernel). It can gen-
eralize to other kernel sizes and fully connected layers. Each kernel has the ﬂexibility in choosing
among a number of pre-deﬁned patterns.

At theory and algorithm levels, such patterns exhibit similarities to the connection structure
in human visual systems [13, 14, 15]. At compiler level, the known patterns allow a compiler
to re-order and generate codes at ﬁlter and kernel level such that kernels with the same pattern
can be grouped together for consecutive executions, thereby maximizing instruction-level and
thread-level parallelism. At hardware level, 4-entry patterns perfectly ﬁt the SIMD architecture in
embedded processors, for both CPUs and GPUs.

The selection of appropriate patterns for a kernel can be achieved via search through an ex-

tended ADMM-based framework [13].

4

Figure 4: Illustration of (a) kernel pattern pruning on CONV kernels, and (b) connectivity pruning
by removing kernels.

Figure 5: Illustration of block-based pruning that applies to all of CNN, RNN, transformers and
to different layer types.

The method can be used together with connectivity pruning, which cuts the connections between

certain input and output channels, to achieve even higher weight pruning/acceleration rates.

Although pattern-based pruning gives the best results on a class of DNNs where the DNN kernel
size is among a set (3 × 3, 5 × 5, 7 × 7), the number of pattern candidates grows rapidly as the
kernel size increases, which exacerbates the computation irregularity and degrades the execution
performance. The limitation prompts the development of block-based pruning.

2.1.2 Block-Based Pruning and Generalization to 3D Convolutions

As an effective complement to the above pattern-based pruning, we develop block-based prun-
ing [16, 17] that is a general pruning scheme that applies to all of CNNs, RNNs, transformers and
all types of network layers. Speciﬁcally, for any weight matrices in CNNs and RNNs, we ﬁrst
partition it into a number of weight blocks and then apply independent column pruning and row
pruning to each block, as shown in Figure 5. Please note that the operations in CONV layers
can be transformed into the general matrix multiplication (GEMM) routine [18] and therefore we
can obtain the corresponding matrix format for ﬁlters in a CONV layer. We have extended the
ADMM-based pruning algorithm to automatically determine the block-based sparsity for each
block and each network layer, as well as a algorithm-compiler co-design to determine the appro-

5

FiltersChannels...............................................................................................................xshyxst-1hxsthxst+1ht-1tt+1t-1tt+1ytyt+1yt-1Input OutputActivationState transitionDNN Layer StructureBlockBlock-based Column-Row PruningPruned Row Pruned ColumnFeature MapFeature MapWeight MatrixCNN：RNN：............................................................Figure 6: Accuracy vs. Latency (on a mobile phone) with different block sizes on ImageNet using
ResNet-50 under uniform 6× pruning rate.

Figure 7: The generalized block-based pruning that applies to 3D convolutions.

priate, layerwise block size. Our block-based pruning enjoys simultaneously the high accuracy as
the non-structured sparsity and the regularity as the course-grained structured sparsity, achieving
all their advantages while overcoming their shortcomings.

Figure 6 shows example results of the accuracy vs. latency when applying block-based prun-
ing on ResNet-50 (ImageNet dataset) with different block sizes. A uniform pruning rate (i.e., 6×)
and block size are adopted through all layers. Under the same pruning rate, non-structured prun-
ing preserves the highest accuracy but has the worst performance in latency. On the contrary,
coarse-grained structured pruning (i.e., the whole weight matrix as a block) achieves the lowest
latency but with a severe accuracy degradation. The results of block-based pruning show high
accuracy and high inference speed (low latency) simultaneously. The reason is that the maximum
hardware parallelism is limited by computation resources. Thus, even when dividing weights into
blocks, each block’s remaining weights are still sufﬁcient to fulﬁll on-device hardware parallelism,
especially on resource-limited mobile devices.

The proposed block-based pruning results in effective acceleration while maintaining high ac-
curacy on camera and LiDAR-based object detection [17, 19] as well as RNN, transformer-based
NLP applications [20, 21]. Moreover, we have generalized block-based pruning to 3D convolu-
tions [22] (Figure 7), which apply to activity detection, 3D LiDAR-based detection, 3D reconstruc-
tion, and other sophisticated computer vision tasks. This is an important advantage of XGen. As
an example of activity detection (details in [22]), we achieve over 20X speedup while maintaining
accuracy compared with competing frameworks.

6

71727374757677101520253035Top-1 Accuracy (%)Latency (ms)unstructured8x48x816x432x4Figure 8: Overview of CoCoPIE’s model and code optimization framework for DNN (using
pattern-based pruning as an example).

2.2 High-Level Optimization

As Figure 2 shows, after XGen model optimizer reduces the size and complexity of the DNN
model, there are a set of high-level optimizations, trying to optimize the computational graph
structure, thus reducing the computation and intermediate result access and improving the com-
putation parallelism.

2.2.1 Computational Graph Opt I: Graph Rewriting

The ﬁrst high-level optimization of XGen is computational graph rewriting. It employs a novel
mathematical-property based graph rewriting pass to optimize the computational graph. With
this pass, XGen is able to 1) remove unnecessary operations, 2) eliminate redundant intermediate
data copies, and 3) replace costly (combination of) operators with more efﬁcient ones. This graph
rewriting carried out here is in the spirit of the classical compiler optimization of strength reduc-
tion [23]; however, here it is performed on complicated operators on matrices or tensors rather
than on scalar expressions. Moreover, the rules we present are more complex and involved, and
are based on operations that are common in DNNs. More importantly, compared to existing ef-
forts on computational graph substitution (e.g., TASO [24]), our graph rewriting is designed to
work in conjunction with the subsequent high-level optimization (operator fusion) and identiﬁes
a set of operators and rules for that speciﬁc purpose. Our evaluation results show that with graph
rewriting, there are 18% fewer fused layers left after fusion on GPT-2. Figure 9 shows speciﬁc
examples of leveraged mathematical properties (distributive, communicative, and associative).

2.2.2 Computational Graph Opt II: Universal DNN Operator Fusion

The second high-level optimization (on computational graphs) fuses DNN operators in a creative
way. To achieve high accuracy, DNN models have become increasingly deep with hundreds or
even thousands of operator layers (e.g., various transformers and the cutting-edge vision trans-
formers). This trend causes two consequences: First, models with more layers usually generate
more intermediate results, thus increasing the memory/cache pressure. Second, deep models
usually have an insufﬁcient amount of computations in each layer, thus degrading the processor’s
utilization, particularly for GPUs. Operator fusion (or kernel/layer fusion) can be an effective
technique to reduce memory requirements and improve efﬁciency, and is a key optimization in

7

Graph-optimizationExecution code generationPruned modelExecution graph with weightsname: vgg16 device: [CPU] layers:   - name: conv_op1     storage:tight     pattern:       type: [1, 2]     layout: FKW       …     tuning:       unroll: [4, 2, 8, 1]       tile: [16, 32, 8]       permute: cohwci_b     …     info:       strides: [1, 1]       dilations: [1, 1]     …   LRFilter kernel reorderLoad redundant elimination12345678901234561xx2xxxxx``5678```Parameter tuningOpt-code for CPU/GPUCompact modelLRExecutorCGExplorerfor ...:   for ...:     for ...:       for ...:       for ...:       for ...: Computation graphGraph optimizationepoch 1epoch 20epoch 40epoch 100ADMMregularizationCNN weight matrixCONV kernelPre-designedpattern poolGuideADMM training processfine-tuneepochsPrune regularized weights and fine-tuneFine-tunePattern-based pruningFigure 9: Examples of graph rewriting with mathematical properties. Associative property ex-
plores the optimal execution order of operators and replaces the expensive combination of oper-
ators with a cheaper one. Distributive property explores the common combination of operators
and simpliﬁes the computation structure. Commutative property switches the execution order of
operators to reduce the overall computation. Note: the letter below each operator (e.g., B below
Conv in (a)) or the letter in rectangle (e.g., C in (b)) denotes that this input is from model weights
rather than an intermediate result. The letter in diamond (e.g., A) means that this is the input of
this operator block, which could be the input of the model or intermediate result from a prior
block. The intermediate results within this block are omitted for readability.

many state-of-the-art DNN execution frameworks, such as TensorFlow, TVM, and MNN. How-
ever, these frameworks usually adopt fusion approaches based on certain patterns that are too
restrictive to cover the diversity of operators and layer connections. Polyhedral-based loop fusion
techniques, on the other hand, work on a low-level view of the computation without operator-
level information, and can also miss potential fusion opportunities.

To address this challenge, the solution from CoCoPIE proposes a rigorous and extensive loop
fusion framework called DNNFusion that can exploit the operator view of computations in DNNs,
and yet enable a set of advanced transformations. The core idea is to classify operators into dif-
ferent types, and develop rules for different combinations of the types, as opposed to looking
for patterns with speciﬁc combination of operations. Particularly, DNNFusion ﬁrst classiﬁes the
existing operations in a DNN into several groups based on the mapping relation between their
input and output (such as One-to-One, One-to-Many, and others). Then, DNNFusion leverages
a mapping type analysis to infer the proﬁtability of different fusing combinations of these types
of operators, binning the combination into three groups: likely proﬁtable (and legal), likely not
proﬁtable, and ones where proﬁtability may need to be determined through proﬁle information.
Table 1 shows the details of this analysis. The rest of DNNFusion framework comprises algo-
rithms for determining fusion of speciﬁc operations (based on certain heuristics) and generating
optimized fused code.

DNNFusion has been extensively evaluated on a number of DNN models with varied types
of tasks, model sizes, and layer counts. The results show that DNNFusion ﬁnds up to 8.8x
higher fusion opportunities, outperforms four state-of-the-art DNN execution frameworks with
9.3x speedup. The memory requirement reduction and execution speedups enable many models
on edge and mobile devices. Moreover, DNNFusion is especially effective in supporting extremely
deep, next-generation NLP transformers (e.g., GPT) and vision transformers.

8

Graph rewritingABConvCMulMulRecipRecip("⊛$)!"⨀(("⊛$)⨀')!"("⊛$)!#⨀'ABConvCMulRecipSquareABCMulDMulAdd"·$⨀'+("·$)⨀*"·$⨀('+*)AMulDAddCBGEMMGEMMComputation based rewriting(a) Data aggregation(b) Data transportation(c) Data splittingAInputBAddCMulAABMulCAddBMulCAddDSubESubLoop jointTransposeRecipRecipSliceSliceData based rewritingABMulCAddIndex transformABMulCAddDSubESubLoop splittingAInputBAddCMulConcatRecipRecipJoint axisSplit axis$+,-ℎ+/,(012341-35("))After: A012341-35($+,-ℎ+/,("))ABefore:BitShiftReduceSumBitShiftReduceSum(a) Associative property(b) Distributive property(c) Commutative propertyTable 1: Mapping type analysis. The ﬁrst column and the ﬁrst row (both without color) show
the mapping types of ﬁrst and second operators, respectively, before fusion, and the colored cells
show the mapping type of the operator after fusion. Green implies that these fusion combinations
can be fused directly (i.e., they are proﬁtable). Red implies that these fusions are unproﬁtable.
Yellow implies that further proﬁling is required to determine proﬁtability.

2.3 Low-Level Optimization

After the high-level optimization streamlines the computations of the DNN, the low-level opti-
mization ensures that efﬁcient (parallel) code be generated for each layer in the DNN. The code
should be able to fully capitalize the important hardware features and minimize the unneces-
sary computations for efﬁciency. Our optimizations at this level harness the opportunities from
the regularity of the DNN sparsity created by the pattern- and block-based pruning at the model
level.

2.3.1 Pattern-Conscious Code Generation

Another key low-level optimization is to support pattern-conscious code generation through a set
of compiler and parallel computing techniques (as shown on the right half of Figure 8). This code
generation is based on a high-level ﬁne-grained Layerwise Representation (LR) that captures the
DNN sparsity information. This LR also includes intensive DNN layer speciﬁc information to
enable aggressive layerwise optimizations. In particular, it includes detailed kernel pattern and
connectivity-related information (e.g., the pattern types presented in this layer, the pattern order in
each ﬁlter, the connection between kernels and input/output channels, etc.); and tuning-decided
parameters (e.g., the input and output tile sizes, unrolling factors, the loop permutation of this
layer, etc.). We particularly emphasize two critical optimizations in this code generation:
Filter Kernel Reorder and Compact Filter-Kernel-Weight (FKW) Storage: It proposes ﬁlter ker-
nel reordering to address two key challenges: heavy control-ﬂow instructions, and thread diver-
gence and load imbalance. The insight is that for a speciﬁc DNN layer, the patterns of all kernels
are already known after model training, so the inference computation pattern is also known be-
fore model deployment. Kernel reordering leverages this knowledge to organize the ﬁlters with
similar kernels together to improve inter-thread parallelization and order the same kernels in a
ﬁlter together to improve intra-thread parallelization. Figure 10 illustrates the basic idea of ker-
nel reordering. After kernel reordering, our LR stores the DNN’s weights in a novel compact
Filter-Kernel-Weight format (called FKW [7]). Compared with existing compact data formats (like
CSR), FKW is higher-level and results in much less extra structure overhead (i.e., the total size of
all index arrays that are used for weights data access). In addition, FKW leverages the pattern
information, and stores the kernels with the kernel reordering information that will support later
branch-less DNN execution. Other compact data format cannot support this.
Load Redundancy Elimination: We propose two techniques to improve the memory performance
and eliminate data load redundancy caused by irregular memory access (in the form of array in-

9

Mapping type comboOne-to-OneOne-to-ManyMany-to-ManyReorganizeShuffleOne-to-OneOne-to-OneOne-to-ManyMany-to-ManyReorganizeShuffleOne-to-ManyOne-to-ManyOne-to-ManyOne-to-ManyOne-to-ManyMany-to-ManyMany-to-ManyMany-to-ManyMany-to-ManyMany-to-ManyReorganizeReorganizeOne-to-ManyMany-to-ManyReorganizeReorganizeShuffleShuffleOne-to-ManyMany-to-ManyReorganizeShuffleMapping relation: One-to-one < (Reorganize, Shuffle) < (One-to-many, many-to-one)First opSecond opFigure 10: An example of ﬁlter kernel reorder. Each cell is a CONV kernel and the number on
it speciﬁes its pruning pattern style. After ﬁlter kernel reorder, each group will be executed con-
currently by multiple threads with each thread processing a ﬁlter. This design improves critical
performance concerns like heavy control-ﬂow instructions, thread divergence and load imbalance.

direction): an effective input tiling to improve the cache performance, and the optimized code
generation that eliminates redundant memory loads with the help of the pre-deﬁned pattern in-
formation. The second one is particularly interesting. Our key insight for this load redundancy
elimination is: in DNN execution, such as a convolution operation, the data access pattern of
the input and output is decided by the (none-zero elements) patterns of kernels that are already
known after training. Therefore, it is possible to generate the optimized data access code with this
information for each pattern of kernels and call them dynamically during the DNN execution.
The generated codes consist of all statically determined data access instructions for the kernel-
level computation with a careful instruction reorganization to 1) eliminate all indirect memory
accesses; and 2) eliminate all redundant register load operations.

2.3.2 Deep Reuse

Besides the low-level code generation, another technique deep reuse offers a method from the di-
mension of activation maps to further reduce the amount of unnecessary computations. It speeds
up DNN training and inference through discovering and exploiting deep reusable computations
on the ﬂy. It is effective, halving the inference time of CNNs implemented on state-of-the-art high
performance libraries and compression techniques, while causing virtually no (<0.0005) accuracy
loss. It is meanwhile easy to use, requiring no special hardware support or CNN model changes,
ready to be applied on today’s systems.

Deep reuse centers around similarities among neuron vectors. A neuron vector is made up of
values carried by some consecutive neurons at a CNN layer. As Figure 11 illustrates, if the layer
is an input image layer, a neuron vector contains the values of a segment of input image pixels; if
the layer is a hidden layer, it contains a segment in its activation map.

The basic idea of deep reuse is to leverage similarities among neuron vectors, such that com-
putation results attained on one neuron vector can be effectively reused for some other neuron
vectors in CNN inferences. Figure 12 illustrates the basic form of such reuses. The eight 3-neuron
vectors, represented by (cid:126)xij, form four groups. Neuron vectors in a group are similar to each other.
In this example, when the dot product of one of them is reused for all others in the group (e.g.,
(cid:126)x11 · (cid:126)w11 for (cid:126)x31 · (cid:126)w11 and (cid:126)x41 · (cid:126)w11), half of the computations in X × W could be saved.

Deep reuse can be implemented through Locality Sensitive Hashing (LSH), an online data
clustering method. The computation reuse can have multiple levels, within an input item, within
a batch of inputs, or across batches. By effectively exploiting the redundancy in inputs and ac-
tivation maps, it reduces DNN computations signiﬁcantly and brings substantial performance
beneﬁts. More details are given in our papers [25, 26].

10

Reorder211222221211211212212112212122211212121212211212221122DNN layerGroup 0FiltersKernelsGroup 1Group 2Figure 11: Illustration of a simple 1-D CNN. The input for convolutional layer 1 is called the input
layer while the input for convolutional layer i with i (cid:54)= 1 is called the activation map. Neurons
in the same block form a neuron-vector. Block colors indicate the similarity of the neuron-vector
values.

Figure 12: An example of the basic form of computation reuse across neuron vectors in convolu-
tion X × W . Instead of calculating 16 dot products, we only need to compute 8 of them: (cid:126)x11 · (cid:126)w11,
(cid:126)x11 · (cid:126)w12, (cid:126)x21 · (cid:126)w11, (cid:126)x21 · (cid:126)w12, (cid:126)x12 · (cid:126)w21, (cid:126)x12 · (cid:126)w22, (cid:126)x32 · (cid:126)w21and (cid:126)x32 · (cid:126)w22.

11

Input Layera neuron-vector in input layera neuron-vector in activation mapActivation MapConvolutionalLayer 1ConvolutionalLayer ix11 x21 x31 x41 x12 x22 x32 x43 w12 w11 w22 w21 xWgroup 1: x11, x31, x41     group 3: x12, x22 group 2: x21                        group 4: x32, x43 x21w11+x22w21x31w11+x32w21x41w11+x42w21x11w11+x12w21x11w12+x12w22x21w12+x22w22x31w12+x32w22x41w12+x42w22Figure 13: Overview of the proposed CAPS framework, which simultaneously optimizes neural
network architecture and pruning schemes.

2.4 Compiler-Aware Neural Architecture & Pruning Co-Search (CAPS)

The three main components of XGen is tied together with CAPS, which iteratively identiﬁes the
best model and code optimization parameters as well as model architectures.

While our compiler optimizations provide notable mobile acceleration and support various

sparsity schemes, it introduces a much larger model optimization space.

An active research area is the Neural Architecture Search (NAS), which designs more efﬁcient
DNN architectures using automatic searching algorithms. Some recent work acknowledge the im-
portance of hardware-software co-design and incorporate the inference latency into NAS, which
can achieve better result than the intuitive volume estimation using the number of weight parame-
ters or computations. However, none of these hardware-targeting work fully exploit the potential
of compiler optimizations or satisfy an overall latency requirement. Moreover, the steps of model
compression, compiler optimization and network architecture search (NAS) are largely performed
independently in prior work.

It is desirable to perform a joint network pruning and architecture search with compiler-based code
optimizations included in the loop, determining the best ﬁlter type and size, as well as pruning
scheme and rate, for each individual layer. CAPS is designed to that end. It can be conﬁgured
to meet various objectives, such as to maximize accuracy while at the same time satisfying the
DNN latency constraint on the target mobile device.

The overall workﬂow of CAPS [27] is shown on the left side in Figure 13. It ﬁrst prepares the
input DNN model by replacing some mobile-unfriendly operations with friendly ones. It then
enters the main loop. At the outermost level is the trials of different pruning algorithms, which
determine what search algorithms to use for the co-search of the neural architecture and pruning.
For a given pruning algorithm, NAPS effectively explores the neural architecture and pruning

12

Pretrained DNN ModelMobile-unfriendly Operation ReplacementMobile-friendlyDNN ModelPruning Algorithm SearchCompiler-Aware Neural Architecture & Pruning Co-Search (CAPS) Final DNN ModelCompiler-Generated CodeCAPSAgentFigure 14: Top-1 Accuracy vs. Latency comparison on mobile GPU (on Samsung Galaxy S10
mobile phone) on ImageNet dataset.

space in a hand-in-hand manner to ﬁnd the best pruned model. NAPS includes code-generation
and performance assessment in the loop to ensure the speed of the generated DNN model. As
the process exhibits a larger search space than prior NAS work, to perform efﬁcient search, NAPS
employs a meta-modeling procedure based on reinforcement learning (RL) with fast evaluation
and Bayesian optimization. It makes the total number of training epochs comparable with that in
the state-of-the-art NAS frameworks.

To further reduce the search in the huge pruning space, XGen explores composability, a property
(fundamental in software engineering) that we discovered in the training of a collection of pruned
CNN models. The basic observation is that two candidate CNN networks in the pruning space
often differ in only some layers, and the training results of the common layers can be reused across
networks to save some training time. More generally, our solution views the networks to search as
compositions of a set of building blocks (a block is a sequence of CNN layers). It pre-trains (some
of) these building blocks and then assembles them into the to-be-explored networks.

To identify the best set of building blocks to pre-train and maximize the beneﬁts, it uses a novel
algorithm, which represents all layers of all to-be-explored networks as a sequence of symbols, and
uses a hierarchical compression algorithm Sequitur [28] to produce a context free grammar (CFG)
and uses it to quickly ﬁnd out the most reusable building blocks [29].

The NPAS framework in XGen achieves by far the best mobile acceleration results (shown in
Figure 14). For example, the inference times with ImageNet are 6.7ms, 5.9ms, and 3.9ms with
78.2%, 75%, and 71% Top-1 accuracy, respectively, on an off-the-shelf mobile phone. The unique
features of XGen include the following:

2.5 XGen Runtime

The ﬁrst three components of XGen output highly optimized DNN code. XGen runtime helps
ensure that the DNN code can actually achieve a high speed in its executions, regardless of the
hardware differences or interferences from other applications on the same device. XGen runtime
is not mandatory for the optimization result of XGen to be used, but could make large differences

13

Top-1 Accuracy (%)Mobile GPU Latency (ms)3     4     5    6     7     8     9    10   11   12   13   14  15   16   80797877767574737271706968MobileNet-V3: MNN  TF-Lite Our CompilerEfficientNet-B0 : MNN      TF-Lite  Our CompilerEfficientNet-B0 (x0.7) : MNN   TF-Lite Our CompilerEfficientNet-B0 (x0.5) : MNN TF-Lite  Our Compiler NPAS (Ours)(7.4, 71.5)(12.1, 71.5)(7.9, 75.2)(16.0, 75.2)(11.0, 77.1)(13.7, 77.1) (6.7, 78.2)(MACs=385M)(5.9, 75.0)(MACs=201M)(MACs=147M)(MACs=99M)(9.6, 68.1)(11.2, 68.1)(6.1, 68.1)(3.9, 70.9)(3.3, 68.3 )(15.1, 75.2)(12.9, 71.5)17   18   19 (15.0, 77.1)Figure 15: The adaptations of DNN executions on a device by XEngine are through knobs injected
into DNNs by XGen as well as the scheduling by XEngine runtime.

in an environment that requires dynamic adaptations, due to either hardware diversity or serious
resource contention.

In our design, XGen runtime has two main functionalities: (i) helps the DNN code from XGen
adapt its internal paths or components (functions, instructions, data layouts) to ﬁt the underlying
hardware, especially when the target hardware is diverse (e.g., different kind of smartphones of
the users of a popular app); (ii) coordinates the usage of computing resources (e.g., computing
units, memory) among co-running DNNs.

For the ﬁrst functionality, XGen has a design of plastic intermediate representation (IR) for rep-
resenting DNN code, which can be leveraged by XGen runtime to adapt DNN code on the ﬂy to
best ﬁt the underlying hardware.

For the second functionality, XGen has a design of AI-conscious DNN co-scheduling, which takes
DNN properties and (ofﬂine and online) DNN optimizations into consideration when coordinat-
ing resource usage. The environments can be closed where the set of co-running tasks are pre-
deﬁned, or open where the set of co-running tasks dynamically change and are hard to predict.
XGen runtime has designs for both closed and open environments.

These two functionalities are coupled, served together by XGen runtime. As Figure 15 shows,
the adaptations are enabled via knobs injected into DNNs by XGen as well as the scheduling
by XEngine. We call it synergistic adaptation. An example is the use of GPU on a smartphone.
The knobs inside a DNN—such as at which layer to exit on a multi-exits DNN—may allow the
adjustment of the amount of computations the DNNs may impose on a GPU, while the scheduling
by XEngine may allow the DNN to time-share the GPU with other DNNs on the device in a
desirable manner.

We next draw on autonomous vehicle as an example closed environment to brieﬂy showcase
the beneﬁts from XEngine runtime. An autonomous driving application consists of a predeﬁned
set of task modules that run on the system concurrently. These task modules involve complicated
dependences, with DNN playing an important role in the modules as illustrated in Figure 16.

Even though there are runtime systems dedicated to autonomous driving, they work poorly
in resource-constrained scenarios. The realtime runtime in the industry, ROSCH, for instance fails
delivering real-time responses when it runs L4 autonomous driving applications on an NVIDIA
Xavier Jetson card (Section 3.2).

14

XEngineMobileHardwareDNN invocation queueAdapt Adapt Adapt Adapt DNN1DNN2DNN3Knob put into DNNs by XGenFigure 16: High-level workﬂow of a level-4 autonomous vehicle. The input of each camera and
lidar device is fed into a DNN-based 2D or 3D perception module.

XGen runtime features DNN-oriented heterogeneous scheduling. The scheduling addresses limi-
tations of existing scheduling algorithms used in both real-time systems and conventional operat-
ing systems, such that when multiple DNNs execute on a device at the same time, they can all run
efﬁciently by taking a full advantage of the processing units and memory on the device.

More speciﬁcally, the design addresses three major limitations of existing runtime systems.

• Limitation I: Starvation happens when prior scheduling schemes are applied to applications

with multiple DNNs deployed on a single resource-constrained device.

Our solution features just-in-time priority adjustment, which resolves the starvation by adjust-
ing the afﬁnity and priorities of tasks in a just-in-time manner.

• Limitation II: Some types of accelerators are left substantially under-utilized due to the

hardware-oblivious model designs and implementations.

Our solution employs model-schedule co-optimization, an approach that customizes the opti-
mization of DNN models based on the constraints of the underlying hardware to maximize
the hardware utilization. It further integrates DNN scheduling into the model optimiza-
tion process to iteratively determine the best model optimizations and the corresponding
schedules in a hand-in-hand manner to achieve the desirable accuracy-speed tradeoff.

• Limitation III: Current scheduling algorithms cannot deal with hybrid workloads that can

employ multiple types of accelerators.

Our solution offers DAG-instantiating scheduling, an approach that extends the current
scheduling algorithms to better manage the heterogeneous computing resources. To ac-
commodate the different performance of a DNN model on different types of computing
units, it creates an algorithm to efﬁciently enumerate the combinations of DNN models on
all types of computing units according to their dependence, based on which, it produces the
schedules that best tap into the full potential of all types of computing units for the inter-
dependent DNN tasks.

The proposed runtime techniques prove effective. When being applied to autonomous driving
workloads, it makes complex workloads able to achieve realtime performance on a low-end device

15

Table 2: Qualitative Comparison between XGen and Competitors

Competitors

Differences

PyTorch Mobile [31], TF-Lite
[32], MNN [4], SNPE [33]
OctoML [34]

Deci [35], Neural Magic [36],
DeepLite [37]

Siloed design in compression and/or compilation; no run-
time scheduling; partial stack
Compilation only; no compression or runtime scheduling;
no co-design
Compression plus existing compilers, designed and devel-
oped separately; no runtime scheduling; partial stack

(Xavier Jetson), which costs 10X less than the devices that the industry currently requires for those
workloads as Section 3.2 shows. For details, please refer to our paper [30].

These four core technologies respectively focus on four different levels of the DNN software
stack: Model, computation graph, code generation, and runtime. But at the same time, they form
a synergy: The model optimizations and the graph and code optimizations are designed hand
in hand, and they all help reduce the computing and memory demands of DNNs, paving the
path for the runtime techniques to exert its full power. Such a full-stack coordinated optimization
distinguishes CoCoPIE solutions from any other existing solutions. It is the reason that CoCoPIE
can achieve multiple times of better results than other solutions.

3 Comparisons

There are many other DNN optimizing frameworks and techniques developed in the recent sev-
eral years. This section ﬁrst lists the principle features of XGen and its qualitative comparison
with several representative frameworks, and then provides quantitative comparisons with the
state-of-the-art frameworks on the market on some common DNN models.

3.1 Qualitative Comparisons

The principle features that distinguish XGen from other frameworks are three:

• Compression-compilation co-design: This feature produces the unique ﬁne-grained pruning pat-
terns inside the coarse-grained structures, and the correspondingly tailored optimizations in
the compiler.

• AI-aware co-optimizing runtime: This feature in the XGen runtime produces the unique model-

schedule co-optimization and the other just-in-time optimizations.

• Full-stack synergy: XGen is the only DNN framework that includes optimizations from DNN
models to code and runtime schedules, all designed in a hand-in-hand manner to form a
coherent synergy.

Table 2 summarizes the qualitative differences between XGen and some best-known competi-
tors. Fundamentally, none of the previous frameworks fully share the three key features of XGen.
Also OctoML (TVM) is less optimized for edge devices compared with servers. Furthermore,
XGen supports next-generation DNN models such as extremely deep transformers and vision
transformers.

16

Table 3: Mobile comparison results on Samsung Galaxy S10 phone over different DNNs/applica-
tions under the same accuracy.

3.2 Quantitative Comparisons

3.2.1 Comparison Results on Off-the-Shelf Mobile Phone

Beneﬁted from Compression-Compilation Co-Design, we evaluate XGen on a Samsung Galaxy
S10 cell phone with the latest Qualcomm Snapdragon 855 mobile platform that consists of a
Qualcomm Kryo 485 Octa-core CPU and a Qualcomm Adreno 640 GPU. We perform a com-
prehensive evaluation on four categories of applications, image classiﬁcation, object detection,
semantic/instance segmenation, and transformer-based NLP applications, with representative,
state-of-the-art DNN models. We compare with four state-of-the-art software acceleration frame-
works TFLite [32], TVM [5], MNN [4], and PyTorch Mobile [31], under the same testing accu-
racy for these models. Table 3 shows the detailed comparison results on mobile CPU and GPU,
while Figure 17 provides the summary. XGen outperforms all other frameworks for all cases,
and largely satisﬁes the overall real-time requirement on off-the-shelf mobile device. On average,
XGen achieves 6.8×, 8.2×, 6.4×, and 16.5× speedups on TFLite, TVM, MNN, and PyTorch Mobile,
respectively, illustrating the effectiveness of compression-compilation co-design. Moreover, one
signiﬁcant advantage of XGen is on NLP and the recent advances of vision transformers, which are
lack of support by other frameworks. Please see video demos at the CoCoPIE YouTube channel1
and Bilibili channel2.

When using compiler only (i.e., comparing on the same model without compression or NAS),
XGen also consistently outperforms the other software acceleration frameworks, TFLite, TVM,
MNN, and PyTorch Mobile by at least 2.5X speedup on average. This illustrates the signiﬁcant
advantages of our high-level and low-level compiler code generation optimizations. Please refer
to [38] for more details.

Energy Efﬁciency Comparison: In terms of energy consumption, XGen is 8.0× less than TVM.
The power consumption rate of the entire mobile device is about the same as that of TVM execu-

1http://www.youtube.com/channel/UCCKVDtg2eheRTEuqIJ5cD8A/
2https://space.bilibili.com/573588276?from=search&seid=11881710196887435131

17

Figure 17: Average speedup summary on Samsung Galaxy S10 phone compared with the other
frameworks under the same accuracy.

Figure 18: Energy efﬁciency comparison of CoCoPIE solution with ASIC Google’s cloud TPU-V2.

tions, 3.8W (tested by Qualcomm Trepn power proﬁler). But its 8.2× less execution time leads to
the large savings in energy.

The results also consistently outperform a number of ASIC and FPGA solutions in both perfor-
mance and energy efﬁciency. Figure 18 demonstrates the comparison results on performance and
energy efﬁciency of the CoCoPIE solution on off-the-shelf mobile device with special ASIC hard-
ware including Google’s cloud TPU-V2 [39], on the same network models. Similar advantages
of the CoCoPIE software solution can be found comparing with edge TPU [39], NVIDIA Jetson
AGX Xavier, Cambricon MLU-100, Eyeriss [40], etc., and a series of FPGA solutions in terms of
accuracy, performance, and energy efﬁciency. Please refer to [41, 42] for more details.

The better results of XGen come from three reasons: (i) the compression-compilation co-design
more effectively matches models with hardware; (ii) smartphone chips are built with the most
advanced technology (e.g., 7nm, 11nm technology), while FPGA/ASIC solutions are based on
older and less energy-efﬁcient 28nm or 40nm technologies. (iii) current ASIC/FPGA solutions are

18

Table 4: Overall Performance Comparison among TFLite, SNPE, and XGen on Mobile DSP. “-
” means this model is not supported by the framework yet. OverT and OverS are the speedup
of XGen over TFLite, and SNPE, respectively. XGen’s overall compilation time for these models
ranges from 5 minutes (WDSR-b) to 25 minutes (EfﬁcientDet-d0).

Model
MobileNet-V3
EfﬁcientNet-b0
ResNet-50
FST
CycleGAN
WDSR-b
EfﬁcientDet-d0
PixOr
TinyBERT
Conformer

Type

Task

2D CNN Classiﬁcation
2D CNN Classiﬁcation
2D CNN Classiﬁcation
2D CNN Style transfer
Image trans.
SR

GAN
2D CNN
2D CNN 2D obj. detect
2D CNN 3D obj. detect

Transformer
Transformer Speech recog.

NLP

#MACS #Params #Operators TFLite (ms) SNPE (ms) XGen (ms) OverT OverS
0.22G
0.40G
4.1G
161G
186G
11.5G
2.6G
8.8G
1.4G
5.6G

5.5M
4M
25.5M
1.7M
11M
22.2K
4.3M
2.1M
4.7M
1.2M

6.2
9.2
11.6
870
366
137
-
26.4
-
-

7.5
9.1
13.9
935
450
400
62.8
43
-
-

4.0
6.0
7.1
211
181
66.7
26
11.7
12.2
65

193
254
140
64
84
32
822
150
211
675

1.9
1.5
2.0
4.4
2.5
6.0
2.4
3.7
-
-
2.8

1.6
1.5
1.6
4.1
2.0
2.1
-
2.3
-
-
2.1

Speedup (geometric mean)

often optimized for a speciﬁc DNN type/size (e.g., edge TPU for small-scale DNNs, Cambricon
MLU-100 for large-scale DNNs), while XGen, as a software method, can better adapt to all kinds
of networks.

Comparison with NeuroMagic: NeuroMagic focuses on generating sparsity (non-structured
sparsity) on sample neural networks (ResNet-50 and YOLO-v3), and has a sparsity-aware infer-
ence engine on desktop CPU. Our XGen outperforms NeuroMagic in (1) advanced sparsity and
NAS schemes instead of the most inefﬁcient non-structured sparsity, (2) advanced compiler-level
code generation techniques, and (3) compression-compilation co-design. For a comparison un-
der the same accuracy, NeuroMagic achieves 27ms inference time on MobileNet-V2 on an Intel
4-core CPU (power consumption >30W), while we achieve 3.3ms inference time on a 3.8W mobile
platform. We achieve an energy efﬁciency gain of 64.6×. For YOLO-based object detection, Neuro-
Magic achieves 36.2ms inference time on a 24-core Intel CPU (power consumption >100W), while
we achieve 55ms inference time with the same accuracy on a 3.8W mobile platform. We achieve
an energy efﬁciency gain of 17.3×. The NeuroMagic results are from their webpage. Needless
to say, our CoCoPIE solution also has much broader applications (including the most advanced,
state-of-the-art DNN models), supporting platforms, and degree of automation.

3.2.2 Comparison Results on Mobile DSP and MCU

Our XGen is a general framework that applies not only to mobile CPU and GPU, but also applies
to dedicated NPU/DSP devices and micro-controllers (MCUs) and achieves signiﬁcant speedup.
This part ﬁrst evaluates the latency of XGen by comparing it against two state-of-the-art frame-
works, TFLite [32] and SNPE [33] on mobile DSP. Table 4 shows the comparison for 10 cutting-
edge models on a Samsung Galaxy S20 (with Snapdragon 865 SoC [43]) with Hexagon 698 DSP
(with Vector eXtensions support). TFLite and SNPE do not support Transformer-based models.
For the other 8 models, XGen achieves 1.5× to 6.0×, and 1.5× to 4.1× speedup over TFLite and
SNPE, respectively. Table 4 shows that XGen achieves the most speedup (6.0× over TFLite) on
WDSR-b because the feature map shapes in WDSR vary signiﬁcantly among different operators.
This enables more beneﬁts from our compiler optimizations (e.g., instruction selection and layout
transformation optimizations). Particularly, XGen for the ﬁrst time supports TinyBERT and Con-
former executed on mobile DSP because it supports more operators than TFLite and SNPE, e.g.,
more variants of MatMul, and Pow. It also the ﬁrst time achieves real-time execution on mobile
DSP for EfﬁcientDet-d0. We also compared several individual convolutional computation kernels

19

with Halide [44] and TVM [5], as the end-to-end inference is not yet supported from these frame-
works on the DSP chip. Speciﬁcally, ﬁrst 8 unique Conv2D operators in ResNet-50 are used. XGen
achieves 3 − 5× speedup over Halide and TVM.

Figure 19: Latency comparison between TFLM (TensorFlow Lite Micro) and XGen with optimiza-
tions (Unrolling and Optimized Quantization), respectively.

This part next compares XGen with a state-of-the-art inference framework, TensorFlow Lite
Micro (TFLM) [45] on a popular Micro-controller Unit (MCU), STM32F469NI. TFLM leverages the
high-performance ARM CMSIS-NN libraries [46] for common DNN operations (e.g., convolution)
to deliver optimized performance. Figure 19 compares the inference latency of XGen and TFLM
on an optimized MobileNet-V2. With compiler optimizations (e.g., loop unrolling that reduces
the register spilling), XGen can achieve 1.2× speedup over TFLM. With our further optimized
quantization, XGen can achieve 1.8× speedup over TFLM.

3.2.3 Beneﬁts from AI-Aware Runtime

The beneﬁts from the AI-aware runtime of CoCoPIE are demonstrated in the deployment of a
complicated Level-4 autonomous driving (shown in Figure 16) on a low-end single-board device,
NVIDIA Xavier Jetson. The current industry has regarded Level-4 autonomous driving possible
only on high-end devices, such as NVIDIA Petegus (worth over $10000). Jetson, worth $700, has
only a small fraction of the computing power of Petegus. Our experiments show that if we directly
deploy Level-4 autonomous driving applications on Jetson, the application makes no progress at
all, as Segment 1 in Table 5 shows. The reason is that the contentions for the limited computing re-
sources by the many AI models in the application cause a deadlock. After we port the application
to use the default Linux time-sharing scheduler, the deadlock is resolved but the 2D perception
modules are about twice as slow as its required speed, as Segment 2 in Table 5 shows. The reason
is that the many AI modules cause the 2D perception module difﬁcult to get enough computing
resource. Some computing units (DLA) are still underutilized but cannot be fully leveraged by the
AI modules due to some mismatches between the model structures and the hardware constraints,
as well as the limitations of the runtime scheduling. After equipping the device with CoCoPIE
runtime, the application runs smoothly, meeting the real-time requirements completely, as Seg-
ment 5 in Table 5 shows. Segments 3 and 4 in Table 5 give the ablation studies on the beneﬁts from
each of the optimizations of the CoCoPIE runtime.

4 XGen as a Product

This section describes the forms of XGen as a product and its usage. In our design, XGen has two
forms. One is the form of Software-As-A-Service (SAAS) on the cloud, the other is a standalone
software package installable on a single machine or a cluster. The ﬁrst form makes it easy to use
by customers, who can start using XGen immediately without installation while paying only for
use. It also makes it possible for XGen to serve many customers at the same time in a scalable

20

TFLM+Unrolling+Quanti.050100150Latency (ms.)139108(1.2x)77(1.8x)Table 5: Execution time (mean± std) of each module in the ADApp applications on Jetson AGX
Xavier and the miss rates. The ∞ represents timeout. The miss rate of a module is how often the mod-
ule misses its expected latency (shown in the parentheses in the table header)—up to 10% over is allowed to
tolerate system noises. The column Miss Rate shows the miss rates of the most sluggish modules (whose
times are preﬁxed with an ∗), that is, the modules with the largest miss rate in the application.

Application

Running Time of Each Module (ms) [expected latency in brakets]

Miss Rate

Sensing

[100ms]

3D Percept

2D Percept

Localization

Tracking

Prediction

Planning

[100ms]

[100ms]

[100ms]

[100ms]

[100ms]

[10ms]

1. Default ROSCH

ADy288

ADy416

ADy608

ADs288

ADs416

ADs608

8.8 ± 1.0

8.5 ± 0.7

8.5 ± 0.8

9.0 ± 0.8

8.4 ± 1.0

8.5 ± 0.9

2. Default Linux Time Sharing

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

* ∞

1.1 ± 0.8

1.3 ± 0.9

1.1 ± 0.7

1.2 ± 1.0

1.3 ± 0.6

1.5 ± 0.8

ADy288

ADy416

ADy08

ADs288

ADs416

ADs608

14.3 ± 5.2

94.7 ± 12.8

* 193.3 ± 17.5

89.5 ± 30.5

0.9 ± 0.8

0.4 ± 1.0

1.0 ± 0.1

15.3 ± 5.1

90.2 ± 12.0

* 167.6 ± 12.7

89.1 ± 29.1

0.9 ± 0.7

0.5 ± 0.9

1.1 ± 0.2

14.8 ± 4.8

89.0 ± 18.7

* 192.8 ± 16.2

91.5 ± 31.2

1.1 ± 0.7

0.4 ± 0.9

1.1 ± 0.2

14.3 ± 5.0

95.6 ± 13.7

* 195.2 ± 18.2

88.7 ± 28.9

1.0 ± 0.8

0.4 ± 1.0

1.1 ± 0.1

14.8 ± 4.8

91.3 ± 13.4

* 168.8 ± 13.3

90.1 ± 30.2

1.1 ± 0.9

0.5 ± 1.0

1.1 ± 0.0

14.7 ± 4.9

90.6 ± 19.2

* 194.2 ± 17.7

91.2 ± 29.3

0.9 ± 0.6

0.4 ± 1.1

1.1 ± 0.1

3. Just-In-Time (JIT) Priority Adjustment

ADy288

ADy416

ADy608

ADs288

ADs416

ADs608

8.5 ± 0.9

94.6 ± 13.4

* 194.7 ± 16.3

43.5 ± 10.2

1.0 ± 0.7

0.6 ± 1.1

1.2 ± 0.4

8.4 ± 1.0

91.7 ± 11.2

* 166.8 ± 11.4

45.3 ± 11.3

0.8 ± 1.0

0.6 ± 1.1

1.0 ± 0.4

8.7 ± 0.7

88.9 ± 17.6

* 190.9 ± 17.9

47.2 ± 9.9

1.1 ± 0.6

0.5 ± 0.9

1.0 ± 0.5

8.9 ± 0.9

96.1 ± 12.7

* 195.9 ± 17.3

46.9 ± 12.1

0.9 ± 0.9

0.6 ± 1.0

1.0 ± 0.4

9.0 ± 0.7

92.8 ± 11.4

* 169.7 ± 13.3

48.1 ± 11.9

1.0 ± 0.5

0.5 ± 1.1

1.3 ± 0.4

8.7 ± 0.9

91.2 ± 20.3

* 194.8 ± 16.9

44.7 ± 10.9

0.8 ± 1.0

0.4 ± 1.2

1.2 ± 0.2

4. JIT Adjustment + Migration to Accelerators

ADy288

ADy416

ADy608

ADs288

ADs416

ADs608

8.7 ± 0.8

123.8 ± 18.5

* 225.6 ± 5.0

43.0 ± 9.9

0.9 ± 1.0

0.5 ± 0.9

1.0 ± 0.6

8.8 ± 1.1

128.7 ± 12.1

* 177.6 ± 3.3

46.7 ± 10.5

1.0 ± 0.8

0.5 ± 1.0

1.2 ± 0.3

9.1 ± 0.9

144.3 ± 8.1

* 171.8 ± 3.0

48.5 ± 9.8

1.0 ± 0.8

0.6 ± 1.1

1.2 ± 0.3

9.0 ± 0.8

125.6 ± 17.1

* 225.6 ± 6.3

47.3 ± 11.3

1.1 ± 0.7

0.4 ± 1.1

1.3 ± 0.2

8.8 ± 1.1

130.5 ± 13.2

* 180.1 ± 4.7

47.6 ± 9.8

0.9 ± 0.9

0.7 ± 1.2

1.5 ± 0.2

8.6 ± 0.9

147.2 ± 9.2

* 174.3 ± 4.5

47.6 ± 10.4

0.9 ± 0.9

0.6 ± 1.2

1.2 ± 0.5

5. JIT Adjustment + Migration to Accelerators + Model-Schedule Co-optimization

ADy288

ADy416

ADy608

ADs288

ADs416

ADs608

8.4 ± 1.2

* 89.0 ± 15.3

95.6 ± 5.1

46.3 ± 9.8

0.9 ± 0.9

0.7 ± 0.9

1.0 ± 0.4

9.0 ± 0.9

72.0 ± 9.0

88.1 ± 4.3

44.9 ± 10.7

1.0 ± 0.8

0.6 ± 0.9

1.3 ± 0.2

8.8 ± 1.2

80.8 ± 10.6

* 98.1 ± 5.0

46.4 ± 11.0

1.0 ± 0.7

0.4 ± 1.1

1.1 ± 0.3

9.0 ± 1.1

* 92.0 ± 14.3

96.4 ± 5.4

45.8 ± 10.1

1.1 ± 0.7

0.4 ± 1.1

1.2 ± 0.6

8.9 ± 0.8

74.2 ± 9.3

90.0 ± 4.2

47.2 ± 10.0

1.0 ± 0.8

0.5 ± 0.9

1.2 ± 0.2

8.8 ± 1.0

83.7 ± 9.7

* 100.1 ± 4.4

46.8 ± 9.9

0.9 ± 0.7

0.7 ± 0.8

1.3 ± 0.8

100%

100%

100%

100%

100%

100%

100%

100%

100%

100%

100%

100%

100%

100%

100%

100%

100%

100%

100%

100%

100%

100%

100%

100%

0%

0%

0%

0%

0%

0%

21

Figure 20: The service modes and paths of XGen. The paths of three colors (green, red, yellow)
indicate the three service paths offered by XGen. Broken lines indicate optional components.

manner. The second form helps meet the needs of customers who have difﬁculties or concerns
in uploading their data or DNN models to a third-party cloud. By installing XGen in their own
cluster or datacenter, they can use it inside their organization.

In either form, XGen can be used in several ways to meet the needs in various scenarios, as

shown in Figure 20.

• Scenario I: A customer needs a common AI capability with a certain requirement (speed on
some common devices, accuracy, size, etc.) that is met by some of the models XGen has
produced before. The customer has no particular requirements on what DNN models or
datasets to use.
Usage I: The customer provides her requirements through the XGen interface, and XGen
immediately returns the AI capability it has already stored in its repository that meets the
requirements. The top green path in Figure 20 shows this service path.

• Scenario II: Similar to Scenario I except that the requirement (speed, accuracy, size, etc.) from

the customer is not met by any of the models XGen has produced before.
Usage II: The customer provides her requirements through the XGen interface, and XGen
identiﬁes promising base DNN models and then conducts its optimizations to generate ef-
ﬁcient code that meets the user’s requirements. The solid red path in Figure 20 shows this
service path.

• Scenario III: Similar to Scenario II except that the customer has her own dataset and/or DNN
model. This scenario arises typically when the customer needs a special AI capability that
requires custom model training and optimization.
Usage III: The customer provides her requirements as well as her dataset and/or DNN model
and the model’s training script that have been prepared based on XGen guidelines. XGen
can seamlessly integrate the customer’s training script into its workﬂow, optimizes the DNN
model in a way similar to Usage II, and produces the code that meets the customer’s required
accuracy on her dataset and her other speciﬁed requirements. The solid plus broken red lines
in Figure 20 show this service path.

22

Requirements(task, speed, accuracy, size, etc.)DataModel & training scriptSpecialhardwareOptimized AI RepositoryCustomExtensionCodeLib…     …XGenRuntimeXGenModel-Code OptimizerModeloptHigh-leveloptLow-leveloptFigure 21: Three use cases and the comparisons with other DNN frameworks.

• Scenario IV: The customer wants to use XGen to optimize DNNs for a special hardware (e.g.,

a new AI accelerator).
Usage IV: XGen is designed to ﬁt the features of common processors (e.g., GPU, CPU, DSP).
It is at the same time extensible. Custom extensions can be easily built into XGen to support
other special processors. XGen guarantees this extensibility by designing a general Inter-
mediate Representation (IR) that captures the key DNN operator features and modern AI
accelerator design trends, such as increasingly powerful SIMD and the integration of SIMD
and VLIW features. This IR enables fast/low-cost new back-end ISA support.

In any of the usage scenario, the execution of the optimized DNN can beneﬁt from XGen
runtime if it is installed on the target device. The use of XGen runtime is optional. Without it,
the optimized DNN can already run efﬁciently; with it, the efﬁciency can be better assured in a
multi-tenant resource constrained environment.

5 Example Use Cases

This section draws on three example use cases to explain the practical impact of XGen. The three
use cases are car classiﬁcation, home safety monitor, and super resolution. Figure 21 shows each
of them and the performance measurements on Samsugn Galaxy S10 and S20 cellphones. We next
provide a brief explanation of each of them.

• Use case I: Car classiﬁcation. Company A needs to develop a smartphone app for its cus-
tomers with which the customers can recognize the models of vehicles in real-time. It is a
typical image classiﬁcation problem, one of the most common uses of DNN. For its popu-

23

Image classification (EfficientNet-B0)              Time per image (ms)2x--3.33x speedup0510152025XGenMNNTF-LitePyTorch6.713.715.022.3Samsung Galaxy S10Home safety monitor (S3D)                                          Time per frame (ms)22.6x speedup010020030040050018.31413.56XGenPyTrochSamsung Galaxy S20SuperResolution(WDSR)                                          Time per frame (ms)7.2x speedup540p1080p05010015020027.8198.2XGenTF-LiteSamsung Galaxy S20larity, many efforts have been spent by the industry to optimize the speed. But even with
that, XGen still brings 2×-3.33× speedups over the results from the mainstream DNN frame-
works, PyTorch, TF-Lite, MNN, while keeping the classiﬁcation accuracy unchanged. The
speedups come from the signiﬁcant reduction of computations by the model pruning and
the enhanced parallelism on GPU brought by the pattern-aware data layout and other opti-
mizations.

• Use case II: Home safety monitor. Company B wants to develop a smart app which can rec-
ognize the activity of babies or elders at real-time and send out alarms when there is safety
risks. The core of the app is a DNN for activity recognition. Compared to the DNN models
used in image classiﬁcation, this task uses a more complex DNN model, S3D, which consid-
ers both spatial and temporal dimensions of the inputs. In our experimented existing DNN
frameworks, only PyTorch was able to produce code that successfully ran on the device.
XGen shows a 22.6× speedup over PyTorch, while giving the same accuracy. The larger size
and complexity of the DNN model provide even more room for the full-stack cooperative
optimizations of XGen to take effect. The dramatic acceleration by XGen is a game changer:
With a speed of 18.31ms per frame, it for the ﬁst time enables real-time activity recognition
on a smartphone, which makes the real-time safety monitoring possible.

• Use case III: Super resolution. Company C is a video content provider. It would like to develop
a video player that can automatically upscale an image or a video to a higher resolution in
real time. One of the purposes is to improve their user experience when users watch a
streaming video in a unstable network condition. The other purpose is to save the network
bandwidth consumption and hence cost: If the upscaling can take place on the user’s device,
a lower resolution stream can be transferred to the user without compromising her experi-
ence. The used DNN model is WDSR. TF-Lite is the only existing DNN framework working
on this task in our experiments. Without model pruning, XGen can already outperform
TF-Lite by 1.9×, thanks to its advanced optimizations (e.g., operator replacement, SIMD op-
timizations, etc.). When combined with pattern-based pruning, XGen produces 3.7× extra
speedups, yielding overall 7.2× speedups over TF-Lite. The numbers of frames per second
increase from 5 to 36, for the ﬁrst time making real-time super resolution on mobile devices
a reality.

Please see video demos of more use cases at the CoCoPIE YouTube channel3 and Bilibili chan-

nel4.

3http://www.youtube.com/channel/UCCKVDtg2eheRTEuqIJ5cD8A/
4https://space.bilibili.com/573588276?from=search&seid=11881710196887435131

24

References Cited

[1] Neil C. Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F. Manso. The computa-
tional limits of deep learning. MIT INITIATIVE ON THE DIGITAL ECONOMY RESEARCH
BRIEF 2020, Vol. 4, 2020.

[2] Pytorch. https://pytorch.org/, 2020.

[3] Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean,
Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: a sys-
tem for large-scale machine learning. In OSDI, volume 16, pages 265–283, 2016.

[4] Alibaba. Mobile Neural Network. https://github.com/alibaba/MNN, 2019.

[5] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen,
Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. TVM: An automated end-to-
end optimizing compiler for deep learning. In 13th USENIX Symposium on Operating Systems
Design and Implementation (OSDI 18), pages 578–594, 2018.

[6] NVIDIA. TensorRT. https://developer.nvidia.com/tensorrt, 2020.

[7] Wei Niu, Xiaolong Ma, Sheng Lin, Shihao Wang, Xuehai Qian, Xue Lin, Yanzhi Wang, and
Bin Ren. Patdnn: Achieving real-time dnn execution on mobile devices with pattern-based
weight pruning. In Proceedings of the Twenty-Fifth International Conference on Architectural Sup-
port for Programming Languages and Operating Systems, pages 907–922, 2020.

[8] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity
in deep neural networks. In Advances in neural information processing systems, pages 2074–2082,
2016.

[9] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural
networks. In Computer Vision (ICCV), 2017 IEEE International Conference on, pages 1398–1406.
IEEE, 2017.

[10] Huizi Mao, Song Han, Jeff Pool, Wenshuo Li, Xingyu Liu, Yu Wang, and William J Dally.
Exploring the regularity of sparse structure in convolutional neural networks. arXiv preprint
arXiv:1705.08922, 2017.

[11] Shmuel Winograd. Arithmetic complexity of computations, volume 33. Siam, 1980.

[12] Andrew Lavin and Scott Gray. Fast algorithms for convolutional neural networks. In Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4013–4021,
2016.

[13] Xiaolong Ma, Fu-Ming Guo, Wei Niu, Xue Lin, Jian Tang, Kaisheng Ma, Bin Ren, and Yanzhi
Wang. Pconv: The missing but desirable sparsity in dnn weight pruning for real-time execu-
tion on mobile devices. AAAI, 2020.

[14] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning ﬁlters
for efﬁcient convnets. In International Conference on Learning Representations (ICLR), 2017.

25

[15] Vadim Lebedev and Victor Lempitsky. Fast convnets using group-wise brain damage.

In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2554–2564,
2016.

[16] Wei Niu, Zhengang Li, Xiaolong Ma, Peiyan Dong, Gang Zhou, Xuehai Qian, Xue Lin, Yanzhi
Wang, and Bin Ren. Grim: A general, real-time deep learning inference framework for mo-
bile devices based on ﬁne-grained structured weight sparsity. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2021.

[17] Yuxuan Cai, Hongjia Li, Geng Yuan, Wei Niu, Yanyu Li, Xulong Tang, Bin Ren, and Yanzhi
Wang. YOLObile: Real-time object detection on mobile devices via compression-compilation
co-design. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35:2, pages
955–963, 2021.

[18] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan
Catanzaro, and Evan Shelhamer. cudnn: Efﬁcient primitives for deep learning. arXiv preprint
arXiv:1410.0759, 2014.

[19] Pu Zhao, Geng Yuan, Yuxuan Cai, Wei Niu, Qi Liu, Wujie Wen, Bin Ren, Yanzhi Wang, and
Xue Lin. Neural pruning search for real-time object detection of autonomous vehicles. In
2021 58th ACM/IEEE Design Automation Conference (DAC), pages 835–840. IEEE, 2021.

[20] Bingbing Li, Zhenglun Kong, Tianyun Zhang, Ji Li, Zhengang Li, Hang Liu, and Caiwen
Ding. Efﬁcient transformer-based large scale language representations using hardware-
friendly block structured pruning. arXiv preprint arXiv:2009.08065, 2020.

[21] Peiyan Dong, Siyue Wang, Wei Niu, Chengming Zhang, Sheng Lin, Zhengang Li, Yifan Gong,
Bin Ren, Xue Lin, and Dingwen Tao. RTmobile: Beyond real-time mobile acceleration of rnns
for speech recognition. In 2020 57th ACM/IEEE Design Automation Conference (DAC), pages
1–6. IEEE, 2020.

[22] Wei Niu, Mengshu Sun, Zhengang Li, Jou-An Chen, Jiexiong Guan, Xipeng Shen, Yanzhi
Wang, Sijia Liu, Xue Lin, and Bin Ren. RT3D: Achieving real-time execution of 3d convolu-
tional neural networks on mobile devices. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 35:10, pages 9179–9187, 2021.

[23] Keith D Cooper, L Taylor Simpson, and Christopher A Vick. Operator strength reduction.
ACM Transactions on Programming Languages and Systems (TOPLAS), 23(5):603–625, 2001.

[24] Zhihao Jia, Oded Padon, James Thomas, Todd Warszawski, Matei Zaharia, and Alex Aiken.
Taso: optimizing deep learning computation with automatic generation of graph substitu-
tions. In SOSP 2019, pages 47–62, 2019.

[25] Lin Ning, Hui Guan, and Xipeng Shen. Adaptive deep reuse: Accelerating cnn training on

the ﬂy. In International Conference on Data Engineering, 2019.

[26] Lin Ning and Xipeng Shen. Deep reuse: Streamline cnn inference on the ﬂy via coarse-grained

computation reuse. In International Conference on Supercomputing, 2019.

[27] Zhengang Li, Geng Yuan, Wei Niu, Pu Zhao, Yanyu Li, Yuxuan Cai, Xuan Shen, Zheng Zhan,
Zhenglun Kong, Qing Jin, et al. NPAS: A compiler-aware framework of uniﬁed network
pruning and architecture search for beyond real-time mobile acceleration. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14255–14266, 2021.

26

[28] Craig G. Nevill-Manning and Ian H. Witten. Identifying hierarchical structure in sequences:

A linear-time algorithm. J. Artif. Intell. Res.(JAIR), 7:67–82, 1997.

[29] Hui Guan, Xipeng Shen, and Seung-Hwan Lim. Wootz: A compiler-based framework for
In Proceedings of the Programming Language Design and

fast cnn pruning via composability.
Implementation (PLDI), 2019.

[30] Hsin-Hsuan Sung, Yuanchao Xu, Jiexiong Guan, Wei Niu, Shaoshan Liu, Bin Ren, Yanzhi
Wang, and Xipeng Shen. Enabling level-4 autonomous driving on a single $1k off-the-shelf
card, 2021.

[31] Pytorch mobile. https://pytorch.org/mobile/home/, 2020.

[32] Google. Tensorﬂow Lite. https://www.tensorflow.org/mobile/tflite/, 2017.

[33] Qualcomm. SNPE. https://developer.qualcomm.com/software/qualcomm-neu

ral-processing-sdk, 2017.

[34] OctoML. https://octoml.ai/, 2021.

[35] Deci.ai. https://deci.ai/, 2021.

[36] Neural Magic. https://neuralmagic.com/, 2021.

[37] DeepLite.ai. https://www.deeplite.ai/, 2021.

[38] Wei Niu, Jiexiong Guan, Yanzhi Wang, Gagan Agrawal, and Bin Ren. Dnnfusion: Accel-
erating deep neural networks execution with advanced operator fusion. In the 42nd ACM
SIGPLAN Conference on Programming Language Design and Implementation, 2021.

[39] Google. Google Cloud TPU. https://cloud.google.com/tpu/, 2017.

[40] Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne. Eyeriss: An Energy-
Efﬁcient Reconﬁgurable Accelerator for Deep Convolutional Neural Networks. In IEEE In-
ternational Solid-State Circuits Conference, ISSCC 2016, Digest of Technical Papers, pages 262–263,
2016.

[41] Hui Guan, Shaoshan Liu, Xiaolong Ma, Wei Niu, Bin Ren, Xipeng Shen, Yanzhi Wang, and
Pu Zhao. CoCoPIE: Enabling real-time ai on off-the-shelf mobile devices via compression-
compilation co-design. Communications of the ACM, (to appear), 2020.

[42] Geng Yuan, Peiyan Dong, Mengshu Sun, Wei Niu, Zhengang Li, Yuxuan Cai, Jun Liu, Wei-
wen Jiang, Xue Lin, Bin Ren, et al. Work in progress: Mobile or fpga? a comprehen-
sive evaluation on energy efﬁciency and a uniﬁed optimization framework. arXiv preprint
arXiv:2106.09166, 2021.

[43] Qualcomm. Snapdragon 865, 2019.

[44] Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Fr´edo Durand, and
Saman Amarasinghe. Halide: A language and compiler for optimizing parallelism, locality,
and recomputation in image processing pipelines. In PLDI 2013, page 519–530, New York,
NY, USA, 2013. Association for Computing Machinery.

27

[45] Robert David, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger,
Ian Nappier, Meghna Natraj, Tiezhen Wang, Pete Warden, and Rocky Rhodes. Tensorﬂow
lite micro: Embedded machine learning for tinyml systems. In A. Smola, A. Dimakis, and
I. Stoica, editors, Proceedings of Machine Learning and Systems (MLSys), volume 3, pages 800–
811, 2021.

[46] Liangzhen Lai, Naveen Suda, and Vikas Chandra. Cmsis-nn: Efﬁcient neural network kernels

for arm cortex-m cpus. arXiv preprint arXiv:1801.06601, 2018.

28

