1
2
0
2

y
a
M
7
2

]
E
C
.
s
c
[

2
v
7
0
3
2
1
.
5
0
1
2
:
v
i
X
r
a

Optimal Transport Based Reﬁnement of
Physics-Informed Neural Networks

Vaishnav Tadiparthi
Department of Aerospace Engineering
Texas A&M University
College Station, TX 77843
vaishnavtv@tamu.edu

Raktim Bhattacharya
Department of Aerospace Engineering
Texas A&M University
College Station, TX 77843
raktim@tamu.edu

Abstract

In this paper, we propose a reﬁnement strategy to the well-known Physics-Informed
Neural Networks (PINNs) for solving partial differential equations (PDEs) based
on the concept of Optimal Transport (OT). Conventional black-box PINNs solvers
have been found to suffer from a host of issues: spectral bias in fully-connected
architectures, unstable gradient pathologies, as well as difﬁculties with convergence
and accuracy. Current network training strategies are agnostic to dimension sizes
and rely on the availability of powerful computing resources to optimize through a
large number of collocation points. This is particularly challenging when study-
ing stochastic dynamical systems with the Fokker-Planck-Kolmogorov Equation
(FPKE), a second-order PDE which is typically solved in high-dimensional state
space. While we focus exclusively on the stationary form of the FPKE, positivity
and normalization constraints on its solution make it all the more unfavorable to
solve directly using standard PINNs approaches. To mitigate the above challenges,
we present a novel training strategy for solving the FPKE using OT-based sam-
pling to supplement the existing PINNs framework. It is an iterative approach
that induces a network trained on a small dataset to add samples to its training
dataset from regions where it nominally makes the most error. The new samples
are found by solving a linear programming problem at every iteration. The paper is
complemented by an experimental evaluation of the proposed method showing its
applicability on a variety of stochastic systems with nonlinear dynamics.

1

Introduction

The problem of uncertainty propagation through nonlinear dynamical systems with stochastic excita-
tions or uncertain parameters arises in a variety of ﬁelds, ranging from statistical physics to economy
and ﬁnance [1, 2]. The exact description of the time evolution of the probability density function (pdf)
ρ(t, x) corresponding to the state x, can be analysed by studying the Fokker-Planck-Kolmogorov
equation (FPKE) [3–5]. It is a partial differential equation (PDE) whose analytical solutions for
different dynamical systems are difﬁcult to obtain except under very special circumstances, for eg.,
systems with linear dynamics and a small class of nonlinear low-dimensional systems [6, 7]. In this
paper, we are concerned with solving the steady-state form of the FPKE for any nonlinear system.

There are several numerical techniques to solve the FPKE like the ﬁnite element method [8–10],
ﬁnite difference method [11, 12], path integral methods [13, 14], that have emerged from the literature
on solving PDEs. While these methods perform reliably for systems with low dimensions , they do
not scale well and are known to become computationally intractable for high-dimensional systems.
Reﬁnement techniques [15, 16] to combat the curse of dimensionality have been introduced in the
past. The orthodox way to study uncertainty propagation is by using the Monte Carlo method [17,18]:

Preprint. Under review.

 
 
 
 
 
 
to generate a large number of sample paths from a family of test points, propagate the nonlinear
system forward exactly, and study the statistics at the time-steps of choice. While this certainly
provides a reliable solution to the governing PDE, it too depends heavily on the amount of computing
resources available to generate large amounts of data, plagued again by the curse of dimensionality.
Alternative methods to analyse uncertainty propagation involve linearization of the nonlinear system
in consideration [19], and the choice to ignore the higher-order terms can lead to signiﬁcant errors.

This is where the universal approximation power of neural networks is coming to the fore. With novel
developments in architecture design and training strategies, they have been tremendously successful
in the data-rich domains of computer vision, speech processing and the like, revolutionizing the
way we build predictive models of complex systems today. In the function approximation world of
physics-informed scientiﬁc computing, there has been remarkable progress in the way these black-box
models are being harnessed to solve solve problems by endowing them with prior domain knowledge
and appropriate inductive biases [20–22].

Of late, the dominant school of thought to train neural networks to solve differential equations is by
introducing penalty terms into the loss functions that constrain the obtained solutions to satisfy some
sets of governing conditions [23]. Speciﬁcally, physics-informed neural networks (PINNs) solve
PDEs by enforcing a composition of constraints coming from the PDE and associated initial and
boundary conditions.

Unfortunately however, PINNs have been found to suffer from a host of issues: spectral bias from
fully-connected architectures, instabilities during training, convergence and accuracy issues with
"stiff" PDEs [24–26]. Numerous modiﬁcations to the baseline PINNs algorithm have been proposed
and most of them revolve around adaptation mechanisms to strategically weight the different loss
functions in the composite loss to account for differing gradients used during training.

Using the baseline PINNs for solving the FPKE is also not as straightforward. The FPKE is a
formidable challenge to solve using a generic neural network because the resulting pdf has to
satisfy two unique properties: positivity over the entire domain and a normalization condition.While
positivity can be ensured through an appropriate choice of architecture, enforcing normalization
through a penalty function [27] introduces another gradient pathology that can be hard to resolve.

To overcome these issues, we transform the stationary FPKE to an equivalent PDE that allows us to use
the conventional PINNs framework. A state-independent diffusion vector allows us to generalize the
solution to the true FPKE as the exponential of a potential function [4], i.e., ρ(x) = N0exp(−η(x)),
where η(x) is the potential and N0 is a normalizing coefﬁcient. This transformation implicitly
preserves positivity, and normalization can be performed using numerical integration over the entire
domain after the PDE in η(x) has been solved.

Further, to overcome any instability issues that may arise during training, we propose a reﬁnement
strategy based on the theory of optimal transport (OT) [28] to actively choose collocation points that
improve the training data regime over which the solution is learnt. The OT framework allows the
synthesis of optimal mappings between functions on some measure spaces with respect to a variety
of cost functions, depending on the nature of the problem. The theory links very well with stochastic
dynamical systems, and has been used with great success in the past for solving Bayesian ﬁltering
problems [29].

Typically, collocation points are chosen through sampling from a pre-determined distribution or
placed uniformly across the grid in consideration. Uniform grid distributions do not scale well with
increasing dimensionality, nor do they account for the efﬁcacy of the obtained solution over the entire
domain. Alternatively, we seek to generate these points by examining the underlying distribution of
the error regime from a nominally trained network. We solve a linear programming problem based on
the theory of optimal transport to obtain these new points at the regions where the network fails the
most, and iteratively train the network to a reasonable degree of accuracy. This can work in tandem
with the adaptive weight tuning strategies proposed in the last few years, remarkably improving the
utility of a small training set and reducing the computational overhead of training a highly complex
network over each and every point in a very ﬁne mesh.

The primary contributions of this paper are:

2

• A novel iterative training scheme based on the theory of Optimal Transport to generate
collocation points for PINNs in regions of high error, thereby improving the sample efﬁciency
of a conventional PINNs solver

• A PINNs framework to solve the stationary Fokker-Planck equation for any dynamical
system through a convenient transformation, thus ensuring the requisite conditions of the
obtained probability density function

The paper is organized as follows: in section 2, we brieﬂy describe the original PINNs formulation
and the Fokker-Planck-Kolmogorov equation for any stochastic dynamical system. Section 3.1
details the reduced formulation for the FPKE that allows the conventional PINNs framework to yield
satisfactory results. In section 3.2, we elaborate upon the proposed sampling strategy using Optimal
Transport. This is succeeded by numerical simulations on a few systems, demonstrating the prowess
of the formulations proposed. A helpful algorithm has been provided for reference. The paper ends
with a summary of the ﬁndings and some questions posed for further research.

2 Background

2.1 Physics Informed Neural Networks

Consider any nonlinear PDE of the form:

Nt,x[u(t, x)] = 0, x ∈ Ω, t ∈ [0, T ],
u(t, x) = g(t, x), x ∈ ∂Ω, t ∈ [0, T ],
u(0, x) = h(x), x ∈ Ω.

(1a)
(1b)
(1c)

where Nt,x is a spatio-temporal differential operator. x ∈ Ω is a n-dimensional spatial vector-
variable operating in a domain Ω ∈ Rn and t is time. ∂Ω refers to the boundary of the domain
Ω. Following the work of [21], the objective is to approximate u(t, x) by a fully-connected neural
network uθ(t, x) with inputs t and x, parameterized by θ. Deﬁne an additional residual network
rθ(t, x) with the same weights θ such that:

rθ(t, x) = Nt,x[uθ(t, x)].

The optimal set of weights is obtained by training the neural network on a composite loss function of
the form:

where

L(θ) = Lr(θ) + Lb(θ) + Li(θ),

Lr(θ) =

Lb(θ) =

Li(θ) =

1
Nr

1
Nb

1
Nr

Nr(cid:88)

j=1

Nb(cid:88)

j=1

Nr(cid:88)

j=1

|rθ(t, xj

r)|2,

|uθ(t, xj

b) − g(t, xj

b)|2,

|uθ(xi

i) − h(xj

i )|2.

(2)

(3a)

(3b)

(3c)

Here, Lr(θ) corresponds to the loss function on the residual from the original PDE, and Lb(θ) and
Li(θ) relate to losses on the boundary and initial conditions respectively.

2.2 Fokker-Planck Equation

The probability density function of stochastic differential equations is governed by the Fokker-Planck
equation. The stationary response of nonlinear dynamical systems can be obtained by solving the
steady-state Fokker-Planck equation. It is a linear partial differential equation (PDE) whose analytical
solutions have been derived for a restricted class of systems.

3

Assuming an n-dimensional stochastic differential equation (SDE) of the form

˙x = F (x) + Λ(x, t)Γ,
with x ∈ Rn, Λ is an n × m matrix and Γ is a vector of uncorrelated standard Gaussian white-noise
processes. We are interested in the evolution of the state PDF (probability density function) ρ(t, x)
given by the Fokker-Planck equation,

∂ρ(t, x)
∂t

+ ∇. (F (x)ρ(t, x)) −

∂ρ(t, x)
∂t

+

n
(cid:88)

i=1

∂
∂xi

(F (x)ρ(t, x)) −

n
(cid:88)

n
(cid:88)

i=1
n
(cid:88)

j=1
n
(cid:88)

i=1

j=1

∂2
∂xi∂xj

∂2
∂xi∂xj

(Dij(x, t)ρ(t, x)) = 0.

(Dij(t, x)ρ(t, x)) = 0.

(4)

where D = 1

2 ΛΛT .

The probability density function ρ(t, x) gives the probability of being in a differential element
(x, x + dx) of the phase plane at time t. It is subject to the normalization condition:
(cid:90)

ρ(t, x)dx = 1.

(5)

x

The boundary conditions are given by ρ → 0 as |xi| → ∞ ∀i = 1, 2, . . . , n. This is inferred from
the normalization condition, but alternative conditions like zero ﬂux at inﬁnity can be imposed as
well [30].

Further, assuming a constant Λ reduces (4) to:

∂ρ(t, x)
∂t

+

n
(cid:88)

i=1

∂
∂xi

(F (x)ρ(t, x)) −

n
(cid:88)

n
(cid:88)

i=1

j=1

Dij(t, x)

∂2ρ(t, x)
∂xi∂xj

= 0.

In the steady state, the equation is:

n
(cid:88)

i=1

∂
∂xi

(F (x)ρ(x)) −

n
(cid:88)

n
(cid:88)

i=1

j=1

Dij(x)

∂2ρ(x)
∂xi∂xj

= 0.

We seek to ﬁnd the solution ρθ(x) to (7) using PINNs. Alternatively, (7) can be expressed as

Nx[ρθ(x)] = 0.

(6)

(7)

(8)

It is a linear second-order parabolic PDE for which, a typical PINNs solver will return a trivial
zero solution for ρ(x) during training. We circumvent this problem by transforming the PDE to an
equivalent PDE after a change of variables. The resulting PDE can be solved in the conventional
PINNs framework. Since we are solving the steady-state FPKE, the composite loss function is simply,

L(θ) = Lr(θ) + Lb(θ).

3 Proposed Formulation

3.1 Steady-state FPKE in η

From [4], the stationary solution to the standard FPKE can be expressed as:

lim
t→∞

ρ(x) = N0 exp(−η(x)),

(9)

(10)

where η(x) is the potential function associated with the stationary solution to the Fokker-Planck
equation. N0 is a normalizing constant that ensures: limt→∞

(cid:82)

x ρ(x)dx = 1.

This implies,

∂ρ
∂xi

= −exp(−η(x))

∂η(x)
∂xi

, and

∂2ρ
∂xi∂xj

= −exp(−η(x))

∂2η(x)
∂xi∂xj

+ exp(−η(x))

∂η(x)
∂xi

∂η(x)
∂xj

.

4

Consequently, (7) transforms to

n
(cid:88)

i=1

(cid:18) ∂Fi(x)
∂xi

− Fi(x)

∂η(x)
∂xi

(cid:19)

−

n
(cid:88)

n
(cid:88)

i=1

j=1

(cid:20)
Dij

(cid:18) ∂2η(x)
∂xi∂xj

+

∂η(x)
∂xi

∂η(x)
∂xj

(cid:19)(cid:21)

= 0.

(11)

The solution to this PDE can now be obtained by using conventional PINNs methods.

3.2 Obtaining Collocation Points using Optimal Transport

The baseline PINNs solver has several modes of failure: instability during training [25, 26], stiffness
in gradient ﬂow dynamics arising from imbalance in loss gradient magnitudes [24]. Most recent work
in the realm of PINNs has been to mitigate issues of this sort by introducing modiﬁable weights to
these loss functions, thus altering the loss function during training to arrive at a reasonably good
approximation.

Typically, collocation points used to train the network are generated either on a uniform grid or
sampled from a chosen distribution. The distributions are pre-determined and do not account for the
efﬁcacy of the obtained solution across the entire domain. We propose an iterative formulation in
which we use the theory of Optimal Transport (OT) to add points to the training dataset in regions
where the nominal PINNs solution fails the most.

All OT problems involve two probability measures µ and ν deﬁned on some space Ω and a non-
negative cost function c(.) on Ω × Ω. The choices of Ω and c(.) subjectively depend on the problem
being considered. Let Dµ denote the support of the measure µ. The objective is to ﬁnd a map
φ : Dµ → Dν such that the total cost transporting a single element x to φ(x) is minimized. This cost
(cid:82) c(x, φ(x))µ(dx). In the context of our proposed formulation, µ is a uniform
is given by: inf φ(.)
distribution over the domain, whereas ν denotes the equation error distribution computed from the
nominal solution at each step.

This transport map can be approximated by a linear map Φ, represented as a matrix Φ := [φij] such
that:

j,k = E
x+

(cid:33)

x−
i,k

Iij

=

(cid:32) N
(cid:88)

i=1

N
(cid:88)

i=1

x−

i,kφij,

(12)

where φij = p(Iij), i.e., the probability that x−
if x−
indicates the ith sample at iteration k and x+

i,k is mapped to x+

j,k and where the indicator Iij = 1
j,k and 0 otherwise. N denotes the number of samples being transformed. x−

i,k is mapped to x+

i,k denotes the transformed sample. Compactly,

i,k

X +

k = X −

k Φ = X −

k N T .

1,k, x−

k ∈ [x−

X −
N,k] is the equally weighted ensemble from uniform distribution µ at
iteration k. Similarly, the equally weighted transformed ensemble from the error distribution at
iteration k is: X +

2,k, . . . , x−

1,k, x+

2,k, . . . , x+

k ∈ [x+

N,k].

To ensure that the matrix T := [tij] is measure-preserving, we need to enforce the following
constraints:

N
(cid:88)

i=1

tij = 1/N,

N
(cid:88)

j=1

tij = wi, and tij ≥ 0,

(13)

where wi is the equation error squared at each point x−
i,k.

The optimization problem can be rewritten as a linear programming problem of size N 2,

T ∗ = arg min

N
(cid:88)

N
(cid:88)

tijD(x−

i,k, ˆx+

j,k) subject to :

N
(cid:88)

tij = 1/N,

T

j=1

where D(x−
have weights equal to wi. The step, X +

i=1
i=1
j,k) is the Euclidean distance between x−
k Φ = X −

k = X −

i,k, ˆx+

N
(cid:88)

j=1

tij = wi, tij ≥ 0.

(14)

i,k and x+
j,k. The transformed samples x+
k N T can be interpreted as a resampling.

j,k

5

Our algorithm detailed in 1 in section 4.2 makes use of this optimization to transform a uniformly
weighted distribution to one that corresponds to the distribution of the square of equation error. We
then add a chosen number of samples from the transformed distribution to the training dataset and
re-train our PINNs network to ﬁnd the optimal solution.

4 Numerical Simulations

We ﬁrst demonstrate the PINNs formulation to solve the steady-state FPKE with the PDE in η(x)
as the solution variable. In the succeeding subsection, we apply the OT-based sampling strategy to
iteratively train our neural network with far fewer points than the chosen baseline and arrive at a
similar degree of accuracy.

We use the same architecture for all the examples that follow: [d, 48, 1] where d is the size of the
state-space of the system. A hyperbolic tangent activation function (tanh) is used between the
input layer and the hidden layer, where as commonly observed in regression problems, a linear
layer is used between the hidden layer and the ﬁnal output layer. We used Julia [31–35] to run our
simulations on the High Performance Research Computing facilities at Texas A&M University. To
obtain the normalization constant N0, we rely on numerical integration schemes. We used the BFGS
optimizer [36, 37] with maximum number of training iterations set to 10000 for all cases.

There are two ways to measure the accuracy of the returned solution. For examples in which the
analytical solution is available, the mean squared solution error evaluated over a ﬁnely discretized
evaluation grid may be a fair criterion for judging correctness. We shall refer to this as (cid:15)ρ. While
this remains suitable for most purposes, the true test of accuracy comes from measuring the residual
equation error (cid:15)pde over the interior of the testing grid. Assuming NU is the number of points in the
interior of the testing grid U,
NU(cid:88)

NU(cid:88)

|ρθ(xi) − ρtrue(xi)|2, and (cid:15)pde =

Nx[ρθ(xi)].

(15)

(cid:15)ρ =

1
NU

i=1

1
NU

i=1

For systems without analytical solutions, we use (cid:15)pde to gauge the accuracy of the solution. As will
be seen in the results, the stationary FPKE boundary conditions are easily satisﬁed.

4.1 Steady-state FPKE using Conventional PINNs

Consider a 2-dimensional Van der Pol - Rayleigh oscillator with dynamics given by:

¨x + (−1 + x2 + ˙x2) ˙x + x = σΓ.

(16)

This can be rewritten as a system of differential equations in x = [x1, x2] as:

˙x1 = x2,

˙x2 = (1 − x2

1 − x2

2)x2 − x1 + σΓ.

where Γ is standard Gaussian white noise. The FPKE in η(x) can be obtained from equation 11. The
analytical solution to the stationary FPKE is available [6]:

ρss(x) = N0exp

(cid:18)

(cid:20) 1
σ2

(x2

1 + x2

2) −

1
2

(x2

1 + x2

2)2

(cid:19)(cid:21)

.

(17)

We consider the domain: x ∈ [−2, 2] × [−2, 2]. We discretize the region with dx = [0.05, 0.05].
This corresponds to a training dataset of 6241 points inside the boundary of the training dataset. After
running the neural network training to get the optimal sets of weights and biases minimising the
composite loss function, we obtain a solution (ﬁgure 1) with (cid:15)pde = 1.92e−4 and (cid:15)ρ = 3.5e−5.

4.2 OT-Based Sampling for Reﬁnement

In this section of the paper, we demonstrate the effectiveness of the proposed OT-based reﬁnement
strategy on two examples. The ﬁrst is the two-dimensional Van der Pol - Rayleigh oscillator, described
above in section 4.1 whose solution η(x) is a polynomial in state space. The second is the classic
Van der Pol oscillator, for which no analytical solution exists.

In the following algorithm, nOT refers to the number of OT iterations we wish to carry out with M
points discovered in each iteration. f (x) is the dynamics of the nonlinear system in consideration
without the diffusion term. S and U denote the interiors of the training and testing set respectively.

6

Figure 1: Van der Pol – Rayleigh oscillator: ρ for Conventional PINNs with 6241 points.

Algorithm 1: OT-PINNs algorithm for solving stationary FPKE
Data: nOT, f (x), S, U, M , hyperparameters for NN (Θ)
pde ← Generate FPKE using (11) for system f (x) ;
NN ← Create a PINNs using Θ for pde ;
θ0 ← Train NN with S;
i = 0;
while i <= nOT do

(cid:15)pde ← Evaluate error using equation (11) on U;
Uerr ← Query top M points corresponding to highest (cid:15)pde;
Transform Uerr to SOT using equation (14) ;
S ← S ∪ SOT ;
i = i + 1 ;
θi ← Train NN with S and θi−1 as the initial guess ;

end

4.2.1 Van der Pol - Rayleigh oscillator

We shall assume the solution obtained above using a uniform grid of dx = [0.05, 0.05] as the baseline.
Our objective is to train a neural network with fewer points and obtain similar levels of accuracy
without compromising on computational time.

For our nominal solution, we set up our network with the aforementioned architecture with dx = 0.25,
which corresponds to 225 training points, a measly 3.6% of the baseline training dataset.

As expected, the solution obtained after training performs poorly when its accuracy is measured w.r.t
the equation error on the ﬁnely discretized testing grid.

We then solve the Optimal Transport linear programming problem to obtain 200 new data points from
regions in which the solution performs the worst. We add these points to our training dataset and
subsequently train the neural network again. Within 2 iterations of generating 200 new data points
and retraining, (cid:15)pde goes down to O(10−2), a remarkable increase in accuracy with just 625 data
points. Within just 5 iterations, (cid:15)pde goes down to O(1e−4) with just 1225 points, i.e., less than 20%
of the training dataset used for the baseline.

The ﬁnal solution obtained after 10 iterations of OT-based sampling can be observed in ﬁgure 2.
The plot of the resulting (cid:15)pde after each iteration can be seen in ﬁgure 5a. The equation error for the
nominal solution trained using 225 points is understandably large at O(1e2). After a single iteration,
(cid:15)pde falls to O(1e−1), a sharp decrease in magnitude from adding only 200 points.

4.2.2 Van der Pol Oscillator

Consider a 2-dimensional Van der Pol oscillator with dynamics given by:

¨x + (−1 + x2) ˙x + x = σΓ.

(18)

This can be rewritten as a system of differential equations in x = [x1, x2] as:

˙x1 = x2,

˙x2 = (1 − x2

1)x2 − x1 + σΓ.

7

Figure 2: Van der Pol – Rayleigh oscillator: ρ for OT-PINNs with 2225 points.

We shall solve for ρ(x) with σ2 = 0.1. This is a more challenging example than the system described
in section 4.1. A network trained with dx = [0.05, 0.05] uses 25281 training points inside the
boundary and returns a solution with (cid:15)pde = 1.27e−2, as can be seen in ﬁgure 3.

Figure 3: Van der Pol oscillator: ρ for Conventional PINNs with 25281 points.

We train our nominal network with dx = [0.1, 0.1]. This corresponds to using 6241 points in a
uniformly spaced grid across [−4, 4] × [−4, 4]. The initial (cid:15)pde is tremendously high, at O(1e5).
Within 10 iterations of ﬁnding 200 new points in each step through the OT-based sampling strategy,
this falls down to O(1e−3). The proposed method has outperformed the baseline training strategy by
using a fraction of the dataset and still returned a more accurate solution.

In ﬁgure 4 on the right, we can see the locations of the 200 points (colored white) found by solving
the OT problem after 10 iterations of applying the OT technique. The ﬁnal solution can be seen on the
left in ﬁgure 4, with the limit cycle clearly visible. The equation errors with increasing OT iterations
in ﬁgure 5b follows a marginally different path than the plot for the Van der Pol - Rayleigh oscillator
in ﬁgure 5a, but the trend observed is the same.

Table 1 summarizes the key beneﬁts of the proposed method in terms of sample efﬁciency. Clearly,
the OT-PINNs algorithm is signiﬁcantly more data-efﬁcient than the baseline PINNs framework and
produces comparable results.

5 Conclusion

In this paper, we solved the stationary Fokker-Planck-Kolmogorov equation using Physics-Informed
Neural Networks. We proposed and demonstrated an iterative strategy based on the concept of
Optimal Transport to generate training datasets. This dramatically reduced the size of dataset required

8

Figure 4: Van der Pol oscillator: ρ for OT-PINNs with 8241 points.

(a) Van der Pol – Rayleigh oscillator

(b) Van der Pol oscillator

Figure 5: Decline of equation error with OT iterations.

Table 1: Comparison of Training Strategies. OT-PINNs has signiﬁcantly high sample-efﬁciency.

System

Training Method NS

Baseline PINNs

6241

(cid:15)pde
1.92e−4

(cid:15)ρ
3.5e−5

Van der Pol -
Rayleigh

Van der Pol

OT-PINNs

Baseline PINNs
OT-PINNs

2225

25281
8241

1.05e−4
1.27e−2
1.26e−3

6.01e−5

-
-

to produce a solution with high accuracy. This will pay off when we seek to study uncertainty
propagation for high-dimensional systems. Moreover, we bypassed the typical difﬁculties of solving
the FPKE by solving an equivalent PDE in terms of the associated potential function of the probability
density current for any given system.

However, there are still a few questions that remain unanswered in the proposed framework. Is there
a requisite degree of accuracy that the nominal network must achieve for the OT-based strategy to
succeed? How many points should the user choose to transform and how many OT iterations would
it take to return a meaningfully accurate solution? Would increasing the number of hidden layers
in the nominal network worsen the reﬁnement strategy? We shall investigate these questions in our
future works.

9

1234567891011121314151617181920OT iterations105104103102101100101102pde1234567891011121314151617181920OT iterations102100102104106pdeReferences

[1] M. Escobedo, M. A. Herrero, J. Velazquez, Radiation dynamics in homogeneous plasma,

Physica D: Nonlinear Phenomena 126 (3-4) (1999) 236–260.

[2] G. Furioli, A. Pulvirenti, E. Terraneo, G. Toscani, Fokker–planck equations in the modeling of
socio-economic phenomena, Mathematical Models and Methods in Applied Sciences 27 (01)
(2017) 115–158.

[3] A. Fuller, Analysis of nonlinear stochastic systems by means of the fokker–planck equation,

International Journal of Control 9 (6) (1969) 603–655.

[4] H. Risken, Fokker-planck equation, in: The Fokker-Planck Equation, Springer, 1996, pp. 63–95.

[5] J. Elgin, The fokker-planck equation: Methods of solution and applications, Optica Acta:

International Journal of Optics 31 (11) (1984) 1206–1207.

[6] T. Caughey, F. Ma, The steady-state response of a class of dynamical systems to stochastic

excitation, Journal of Applied Mechanics 104 (3) (1982) 629–632.

[7] T. Caughey, H. Payne, On the response of a class of self-excited oscillators to stochastic

excitation, International Journal of Non-Linear Mechanics 2 (2) (1967) 125–151.

[8] B. Spencer, L. Bergman, On the numerical solution of the fokker-planck equation for nonlinear

stochastic systems, Nonlinear Dynamics 4 (4) (1993) 357–372.

[9] J. Náprstek, R. Král, Finite element method analysis of fokker–plank equation in stationary and

evolutionary versions, Advances in Engineering Software 72 (2014) 28–38.

[10] R. F. Galán, G. B. Ermentrout, N. N. Urban, Stochastic dynamics of uncoupled neural oscillators:
Fokker-planck studies with the ﬁnite element method, Physical Review E 76 (5) (2007) 056110.

[11] Y. Jiang, A new analysis of stability and convergence for ﬁnite difference schemes solving
the time fractional fokker–planck equation, Applied Mathematical Modelling 39 (3-4) (2015)
1163–1171.

[12] B. Sepehrian, M. K. Radpoor, Numerical solution of non-linear fokker–planck equation using
ﬁnite differences method and the cubic spline functions, Applied mathematics and computation
262 (2015) 187–190.

[13] A. Drozdov, J. Brey, Accurate path integral representations of the fokker-planck equation with a
linear reference system: Comparative study of current theories, Physical Review E 57 (1) (1998)
146.

[14] Y. Xu, W. Zan, W. Jia, J. Kurths, Path integral solutions of the governing equation of sdes

excited by lévy white noise, Journal of Computational Physics 394 (2019) 41–55.

[15] M. Kumar, S. Chakravorty, P. Singla, J. L. Junkins, The partition of unity ﬁnite element approach
with hp-reﬁnement for the stationary fokker–planck equation, Journal of Sound and Vibration
327 (1-2) (2009) 144–162.

[16] M. Kumar, P. Singla, S. Chakravorty, J. L. Junkins, A multi-resolution approach for steady
state uncertainty determination in nonlinear dynamical systems, in: 2006 Proceeding of the
Thirty-Eighth Southeastern Symposium on System Theory, IEEE, 2006, pp. 344–348.

[17] E. Johnson, S. Wojtkiewicz, L. Bergman, B. Spencer Jr, Observations with regard to massively
parallel computation for monte carlo simulation of stochastic dynamical systems, International
journal of non-linear mechanics 32 (4) (1997) 721–734.

[18] N. Harnpornchai, H. Pradlwarter, G. Schnëller, Stochastic analysis of dynamical systems by
phase-space-controlled monte carlo simulation, Computer methods in applied mechanics and
engineering 168 (1-4) (1999) 273–283.

[19] H. Pradlwarter, Non-linear stochastic response distributions by local statistical linearization,

International Journal of Non-linear mechanics 36 (7) (2001) 1135–1151.

[20] L. Sun, H. Gao, S. Pan, J.-X. Wang, Surrogate modeling for ﬂuid ﬂows based on physics-
constrained deep learning without simulation data, Computer Methods in Applied Mechanics
and Engineering 361 (2020) 112732.

[21] M. Raissi, Z. Wang, M. S. Triantafyllou, G. E. Karniadakis, Deep learning of vortex-induced

vibrations, Journal of Fluid Mechanics 861 (2019) 119–137.

10

[22] Z. Fang, J. Zhan, Deep physical informed neural networks for metamaterial design, IEEE

Access 8 (2019) 24506–24513.

[23] M. Raissi, Deep hidden physics models: Deep learning of nonlinear partial differential equations,

The Journal of Machine Learning Research 19 (1) (2018) 932–955.

[24] S. Wang, Y. Teng, P. Perdikaris, Understanding and mitigating gradient pathologies in physics-

informed neural networks, arXiv preprint arXiv:2001.04536.

[25] L. McClenny, U. Braga-Neto, Self-adaptive physics-informed neural networks using a soft

attention mechanism, arXiv preprint arXiv:2009.04544.

[26] S. Wang, X. Yu, P. Perdikaris, When and why pinns fail to train: A neural tangent kernel

perspective, arXiv preprint arXiv:2007.14527.

[27] Y. Xu, H. Zhang, Y. Li, K. Zhou, Q. Liu, J. Kurths, Solving fokker-planck equation using deep
learning, Chaos: An Interdisciplinary Journal of Nonlinear Science 30 (1) (2020) 013133.
[28] C. Villani, Optimal transport: old and new, Vol. 338, Springer Science & Business Media, 2008.
[29] N. Das, R. Bhattacharya, Optimal transport based ﬁltering with nonlinear state equality con-

straints, IFAC-PapersOnLine 53 (2) (2020) 2373–2378.

[30] H. Langtangen, A general numerical solution method for fokker-planck equations with applica-

tions to structural reliability, Probabilistic Engineering Mechanics 6 (1) (1991) 33–48.
[31] J. Bezanson, A. Edelman, S. Karpinski, V. B. Shah, Julia: A fresh approach to numerical

computing, SIAM review 59 (1) (2017) 65–98.

[32] C. Rackauckas, Q. Nie, Differentialequations. jl–a performant and feature-rich ecosystem for

solving differential equations in julia, Journal of Open Research Software 5 (1).

[33] M. Innes, E. Saba, K. Fischer, D. Gandhi, M. C. Rudilosso, N. M. Joy, T. Karmali, A. Pal,
V. Shah, Fashionable modelling with ﬂux, CoRR abs/1811.01457. arXiv:1811.01457.
URL https://arxiv.org/abs/1811.01457

[34] M. Innes, Flux: Elegant machine learning with julia, Journal of Open Source Softwaredoi:

10.21105/joss.00602.

[35] Y. Ma, S. Gowda, R. Anantharaman, C. Laughman, V. Shah, C. Rackauckas, Modelingtoolkit:
A composable graph transformation system for equation-based modeling (2021). arXiv:
2103.05244.

[36] J. Nocedal, S. Wright, Numerical optimization, Springer Science & Business Media, 2006.
[37] P. K. Mogensen, A. N. Riseth, Optim: A mathematical optimization package for julia, Journal

of Open Source Software 3 (24).

11

