JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

Data Poisoning Attacks on Federated Machine
Learning

Gan Sun, Yang Cong, Senior Member, IEEE, Jiahua Dong, Qiang Wang and Ji Liu

0
2
0
2

r
p
A
9
1

]

R
C
.
s
c
[

1
v
0
2
0
0
1
.
4
0
0
2
:
v
i
X
r
a

Abstract—Federated machine learning which enables resource-
constrained node devices (e.g., mobile phones and IoT devices) to
learn a shared model while keeping the training data local, can
provide privacy, security and economic beneﬁts by designing an
effective communication protocol. However, the communication
protocol amongst different nodes could be exploited by attackers
to launch data poisoning attacks, which has been demonstrated
as a big threat to most machine learning models. In this paper,
we attempt to explore the vulnerability of federated machine
learning. More speciﬁcally, we focus on attacking a federated
multi-task learning framework, which is a federated learning
framework via adopting a general multi-task learning framework
to handle statistical challenges. We formulate the problem of
computing optimal poisoning attacks on federated multi-task
learning as a bilevel program that is adaptive to arbitrary choice
of target nodes and source attacking nodes. Then we propose a
novel systems-aware optimization method, ATTack on Federated
Learning (AT2FL), which is efﬁciency to derive the implicit
gradients for poisoned data, and further compute optimal attack
strategies in the federated machine learning. Our work is an
earlier study that considers issues of data poisoning attack for
federated learning. To the end, experimental results on real-world
datasets show that federated multi-task learning model is very
sensitive to poisoning attacks, when the attackers either directly
poison the target nodes or indirectly poison the related nodes by
exploiting the communication protocol.

Index Terms—Federated Machine Learning, Data Poisoning.

I. INTRODUCTION

M Achine learning has been widely-applied into a broad

array of applications, e.g., spam ﬁltering [20] and
natural gas price prediction [1]. Among these applications,
the reliability or security of the machine learning system has
been a great concern, including adversaries [10], [19]. For
example, for product recommendation system [16], researchers
can either rely on public crowd-sourcing platform, e.g., Amazon
Mechanical Turk or Taobao, or private teams to collect training
datasets. However, both of these above methods have the
opportunity of being injected corrupted or poisoned data by
attackers. To improve the robustness of real-world machine
learning systems, it is critical to study how well machine
learning performs under the poisoning attacks.

For the attack strategy on machine learning methods, it
can be divided into two categories: causative attacks and

G. Sun, Y. Cong, J. Dong and Q. Wang are with State Key Labo-
ratory of Robotics, Shenyang Institute of Automation, Chinese Academy
of Sciences, Shenyang, 110016, China, Email: sungan1412@gmail.com,
congyang81@gmail.com, dongjiahua@sia.cn, wangqiang@sia.cn.

J. Dong is also with University of Chinese Academy of Sciences, Beijing.
J. Liu is with the Department of Computer Science, University of Rochester,

Rochester, NY 14627 USA (e-mail: jliu@cs.rochester.edu).

Manuscript received April 19, 2005; revised August 26, 2015.

Fig. 1. The demonstration of our data poisoning attack model on federated
machine learning, where different colors denote different nodes, and there
are n nodes in this federated learning system. Some nodes are injected by
corrupted/poisoned data, and some nodes are only with clean data.

exploratory attacks [3], where exploratory attacks inﬂuence
learning via controlling over training data, and exploratory
attacks can take use of misclassiﬁcations without affecting
training. However, previous researches on poisoning attacks
focus on the scenarios that training samples are collected in
a centralized location, or the training samples are sent to a
centralized location via a distributed network, e.g., support
vector machines [5], autoregressive models [1] and collaborative
ﬁltering [13]. However, none of the current works study
poisoning attacks on federated machine learning [11], [17],
[18], where the training data are distributed across multiple
devices (e.g., users’ mobile devices: phones/tablets), and may
be privacy sensitive. To further improve its robustness, in this
paper, our work explores how to attack the federated machine
learning.

For the federated machine learning, its main idea is to build
machine learning models based on data sets that are distributed
across multiple devices while preventing data leakage. Although
recent improvements have been focusing on overcoming the
statistical challenges (i.e., the data collected across the network
are in a non-IID manner, where the data on each node are
generated by a distinct distribution) or improving privacy-
preserving, the attempt that makes federated learning more
reliability under poisoning attacks, is still scarce. For example,
consider several different e-commerce companies in a same
region, and the target is to establish a prediction model for
product purchase based on user and product information, e.g.,
user’s browsing and purchasing history. The attackers can
control a prescribed number of user accounts and inject the
poisoning data in a direct manner. Furthermore, due to the

iPhone10:20 AMInjected DataClean DataNode 1iPhone10:20 AMClean DataNode 2iPhone10:20 AMInjected DataClean DataNode n…DownloadUploadDownloadUploadDownloadUploadServer 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

2

communication protocol existing amongst different companies,
this protocol also opens a door for the attacker to indirectly
affect the unaccessible target nodes, which is also not addressed
by existing poisoning methods whose training data are collected
in a centralized location.

Motivated by the aforementioned analysis, we attempt
to analyze optimal poisoning attacks on federated machine
learning. More speciﬁcally, as shown in Figure 1, we focus
on the recently-proposed federated multi-task learning frame-
work [15], a federated learning framework which captures node
relationships among multiple nodes to tackle the statistical
challenges in the federated setting. Our work is to formulate
the optimal poisoning attack strategy on federated multi-task
learning model as a general bilevel optimization problem,
which can be adaptive to any choice of target nodes and
source attacking nodes. However, current optimization methods
for this bilevel problem are not suited to tackle the systems
challenges (e.g., high communication cost, stragglers) that
exist in federated learning. As a key component of this work,
we thus design a novel optimization method, ATTack on
Federated Learning (AT2FL), to derive the implicit gradients
for computing poisoned data in the source attacking nodes.
Furthermore, the obtained gradient can be effectively used to
compute the optimal attack strategies. Finally, we empirically
evaluate the effectiveness of the proposed optimal attack
strategy against random baselines on several real-world datasets.
The experiment results strongly support our proposed model
when attacking federated machine learning based on the
communication protocol.

The novelty of our proposed model is threefold:
• We propose a bilevel optimization framework to compute
optimal poisoning attacks on federated machine learning.
To our best knowledge, this is an earlier attempt to explore
the vulnerability of federated machine learning from the
perspective of data poisoning.

• We derive an effective optimization method, i.e., ATTack
on Federated Learning (AT2FL), to solve the optimal
attack problem, which can address systems challenges
associated with federated machine learning.

• We demonstrate the empirical performance of our optimal
attack strategy, and our proposed AT2FL algorithm with
several real-world datasets. The experiment results indicate
that the communication protocol among multiple nodes
opens a door for attacker to attack federated machine
learning.

II. RELATED WORK

Our work mainly draws from data poisoning attacks and
federated machine learning, so we give a brief review on these
two topics.

For the data poisoning attacks, it has become an urgent
research ﬁeld in the adversarial machine learning, in which
the target is against machine learning algorithms [4], [10].
The earlier attempt that investigates the poisoning attacks
on support vector machines (SVM) [5], where the adopted
attack uses a gradient ascent strategy in which the gradient is
obtained based on properties of the SVM’s optimal solution.

Furthermore, poisoning attack is investigated on many machine
learning models, including autoregressive models [1], matrix
factorization based collaborative ﬁltering [13] and neural
networks for graph data [22]. In addition to single task learning
models, perhaps [21] is the most relevant work to ours in the
context of data poisoning attacks, which provides the ﬁrst study
on one much challenging problem, i.e., the vulnerability of
multi-task learning. However, the motivations for [21] and our
work are signiﬁcantly different as follows:

• The data sample in [21] are put together, which is different
from the scenario in federated machine learning, i.e.,
machine learning models are built based on datasets
that are distributed across multiple nodes/devices while
preventing data leakage.

• The proposed algorithm in [21] is based on optimization
method of current multi-task learning methods, which is
not suited to handle the systems challenges in federated
learning, including high communication cost, etc. Han-
dling these challenges in the setting of data poisoning
attacks is a key component of this work.

For the federated machine learning, its main purpose is
to update classiﬁer fast for modern massive datasets, and the
training data it can handle are with the following properties [12]:
1) Non-IID: data on each node/device may be drawn from a
different distribution; 2) Unbalanced: the number of training
samples for different nodes/devices may vary by orders of
magnitude. Based on the distribution characteristics of the data,
federated learning [18] can be categorized into: 1) horizontal
(sample-based) federated learning, i.e., datasets share the same
feature space but different in samples. The representative work
is a multi-task style federated learning system [15], which
is proposed to allow multiple nodes to complete separate
tasks while preserving security and sharing knowledge; 2)
vertical (feature-based) federated learning, i.e., two datasets
share the same sample ID space but differ in feature space.
Several privacy-preserving machine learning methods have
been presented for vertically partitioned data, e.g., secure
linear regression [8], gradient descent methods [9]; 3) federated
transfer learning, i.e., two datasets differ not only in samples but
also in feature space. For this case, traditional transfer learning
techniques can be adopted into provide solutions for the entire
sample and feature space with the federated setting. As an ﬁrst
attempt, [2] develops a new model-replacement methodolody
that exploits these vulnerabilities and demonstrated its efﬁcacy
on federated learning tasks. However, its objective is to retain
high accuracy on the backdoor subtask after attacking. On
the contrast, in this work, we try to ﬁll data poisoning gap
by investigating poisoning attack against horizontal federated
machine learning.

III. THE PROPOSED FORMULATION
In this section, we introduce the details of our proposed
poisoning attacks strategies on federated machine learning.
Therefore, we ﬁrstly present some preliminaries about federated
multi-task learning, which is a general multi-task learning
formulation in federated setting. Then we provide the mathe-
matical form of our poisoning attack formulation, followed by
how to optimize our proposed model.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

3

TABLE I
NOTATIONS FOR ALL THE USED VARIABLES.

Variables
D(cid:96)
Dt
ˆD(cid:96)
ˆDs
Ntar
Nsou
H
W
Ω

Interpretation
Clean data for the (cid:96)-th node
Clean data for the t-th target node
Injected data for the (cid:96)-th node
Injected data for the s-th source attacking node
A set of target nodes
A set of source attacking nodes
Upper level function in Eq. (3)
Weight matrix
Model relationship among nodes

A. Federated Machine Learning

(cid:96)=1

i=1

In the setting of federated machine learning, the target is
to learn a model over data that resides on, and has been
generated by, m distributed nodes. Each node/device, (cid:96) ∈ [m],
may generate local data via a distinct distribution, and so it is
natural to ﬁt separate models, {w1, . . . , wm}, to the distributed
data-one for each local dataset. In this paper, we focus on an
effective horizontal (sample-based) federated learning model,
i.e., federated multi-task learning [15], which is proposed via
incorporating with a general multi-task learning framework.
More speciﬁcally, the federated multi-task learning model can
be formulated as:
m
(cid:88)

n(cid:96)(cid:88)

(cid:96)) + λ1tr(W ΩW (cid:62)) + λ2 (cid:107)W (cid:107)2
F ,

L(cid:96)(w(cid:62)

(cid:96), yi

(cid:96) xi

min
W,Ω

(cid:96), yi

(1)
where (xi
(cid:96)) is the i-th sample for the (cid:96)-th node, n(cid:96)
is the number of clean samples for the (cid:96)-th node, W =
[w1, . . . , wm] ∈ Rd×m is a matrix whose (cid:96)-th column is
the weight vector for the (cid:96)-th node and matrix Ω ∈ Rm×m
models relationships among nodes. λ1 > 0 and λ2 > 0 are the
parameters to control regularization terms.

From the equation above, we can notice that solving
for matrix Ω is not dependent on the data and therefore
can be computed centrally; one major contribution of [15]
is an efﬁcient distributed optimization method for the W
step. Furthermore, the update of W can be achieved by
extending works on distributed primal-dual optimization [14].
Let n := (cid:80)m
(cid:96)=1 n(cid:96) and X := Diag(X1, . . . , Xm) ∈ Rmd×n.
With the ﬁxed variable Ω, the dual formulation of optimization
problem in Eq. (1), deﬁned with respect to dual variables
α(cid:96) ∈ Rn(cid:96) , is given by:

min
α,W,Ω

m
(cid:88)

n(cid:96)(cid:88)

(cid:96)=1

i=1

L∗

(cid:96) (−αi

(cid:96)) + λ1R∗(Xα),

(2)

(cid:96) and R∗ are the conjugate dual functions of L(cid:96) and
where L∗
tr(W ΩW (cid:62))+λ2/λ1 (cid:107)W (cid:107)2
(cid:96) in α ∈ Rn is
the dual variable of the i-th data point (xi
(cid:96)) for the (cid:96)-th node.
Meanwhile, we denote D(cid:96) = {(X(cid:96), α(cid:96), y(cid:96))|X(cid:96) ∈ Rd×n(cid:96), α(cid:96) ∈
Rn(cid:96), y(cid:96) ∈ Rn(cid:96)} as the clean data in the node (cid:96) in this work.

F , respectively, and αi

(cid:96), yi

B. Poisoning Attacks on Federated Multi-task Learning

In this section, we ﬁrstly introduce the problem setting of the
data poisoning attack on federated machine learning. Then three

kinds of attacks based on real-world scenarios are provided,
followed by a bilevel formulation to compute optimal attacks.
Suppose that the attacker aims to degrade the performance of
a set of target nodes Ntar ⊂ m by injecting corrupted/poisoned
data to a set of source attacking nodes Nsou ⊂ m. Based on the
dual problem in Eq. (2), we denote ˆD(cid:96) = {( ˆX(cid:96), ˆα(cid:96), ˆy(cid:96))| ˆX(cid:96) ∈
Rd×ˆn(cid:96), ˆα(cid:96) ∈ Rˆn(cid:96), ˆy(cid:96) ∈ Rˆn(cid:96)} as the set of malicious data
injected to the node (cid:96), where ˆn(cid:96) denotes the number of injected
samples for the node (cid:96). More speciﬁcally, ˆD(cid:96) will be ∅, i.e.,
ˆn(cid:96) = 0, if (cid:96) /∈ Nsou. We also deﬁne the following three kinds
of attacks based on real-world federated learning scenarios.

• Direct attack: Ntar = Nsou. Attacker can directly inject
data to all the target nodes since a door will be opened
when collecting data. For example, consider learning or
recognizing the activities of mobile phone users in a cell
network based on their individual sensor, text or image
data. Attackers can directly attack the target mobile phones
(nodes) by providing counterfeit sensor data into the target
phones (nodes).

• Indirect attack: Ntar∩Nsou = ∅. Attacker cannot directly
inject data to any of the target nodes. However, due to
the communication protocol existing amongst multiple
mobile phones, the attacker can inject poisoned data to
other mobile phones and affect the target nodes in an
indirect way.

• Hybrid attack: An attack style which combines direct
attack and indirect attack, i.e., the attacker can inject
poisoned data samples into target nodes and source
attacking nodes simultaneously.

To maximally degrade the performance of target nodes, we
then formulate the optimal attack problem as the following
bilevel optimization problem by following [5]:

max
{ ˆDs|s∈Nsou}

(cid:88)

Lt(Dt, wt),

{t|t∈Ntar}
m
(cid:88)

1
n(cid:96) + ˆn(cid:96)

(cid:96)=1

s.t., min
α,W,Ω

(cid:96) (D(cid:96) ∩ ˆD(cid:96)) + λ1R∗(Xα).
L∗

(3)

where ˆDs denotes the injected data for the s-th source attacking
node. Intuitively, the variables in the upper level problem are
the data points ˆDs, and we denote the upper level problem
as a H in this paper. The lower level problem in the Eq. (3)
is a federated multi-task learning problem with training set
consisting of both clean and injected data points. Therefore,
the lower level problem can be considered as the constraint of
the upper level problem.

IV. ATTACK ON FEDERATED LEARNING (AT2FL)
This section proposes an effective algorithm for computing
optimal attack strategies, i.e., AT2FL. More speciﬁcally, we
follow the setting of most data poisoning attack strategies
(e.g., [5], [21]), and design a projected stochastic gradient
ascent based algorithm that efﬁciently maximally increases
the empirical loss of target nodes, and further damages their
classiﬁcation/regression performance. Notice that there is no
closed-form relation between the empirical loss and the injected
data, so we intend to compute the gradients by exploiting the
optimality conditions of the subproblem.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4

A. Attacking Alternating Minimization

Bilevel problems are usually hard to optimize due to their
non-convexity property. In our bilevel formulation Eq. (3),
although the upper level problem is a simple primal problem,
the lower level problem is highly non-linear and non-convex.
To effectively solve this problem, the idea is to iteratively
update the injected data in the direction of maximizing the
function H of target nodes. Therefore, in order to reduce the
complexity of the optimal attack problem, we need to optimize
over the features of injected data (ˆxi
s) by ﬁxing the labels
of injected data. Then the update rules for injected data ˆxi
s can
be written as:

s, ˆαi

(ˆxi

s)k ← ProjX

(cid:0)(ˆxi

s)k−1 + η1∇(ˆxi

s)k−1H(cid:1),

(4)

where η1 > 0 is the step size, k denotes the k-th iteration, and
X represents the feasible region of the injected data, which
is speciﬁed by the ﬁrst constraint in the upper level problem
H. More speciﬁcally, ProjX(x) is x if (cid:107)x(cid:107)2 ≤ r; xr/ (cid:107)x(cid:107)2,
otherwise, i.e., X can be considered as an (cid:96)2-norm ball by
following [7]. Accordingly, the corresponding dual variable ˆαi
s
can be updated gradually as the ˆxi

s comes as following:

(ˆαi

s)k ← (ˆαi

s)k−1 + ∆(ˆαi

s).

(5)

B. Gradients Computation

To compute the gradient of the t-th target node in Eq. (4),
Lt(Dt, wt), we adopt the chain rule and then obtain

i.e., ∇ˆxi
the following equation:

s

∇ˆxi

s

Lt(Dt, wt) = ∇wtLt(Dt, wt) · ∇ˆxi

s

wt.

(6)

From the equation above, we can notice that the ﬁrst term on
the right side can be easily computed since it depends only
on the loss function Lt(·). However, the second term on the
right side depends on the optimality conditions of lower level
problem of Eq. (3). In the following, we show how to compute
wt with respect to two commonly-adopted loss
the gradient ∇ˆxi
functions: least-square loss (regression problems) and hinge
loss (classiﬁcation problems). Please check the deﬁnitions in
the Appendix A.

s

Based on the loss functions above, we ﬁrst ﬁx variable Ω
to eliminate the constraints of the lower level problem, and
obtain the following dual subproblem:

min
α,W

m
(cid:88)

(cid:96)=1

1
n(cid:96) + ˆn(cid:96)

(cid:96) (D(cid:96) ∪ ˆD(cid:96)) + λ1R∗(Xα).
L∗

(8)

Since the optimality of the lower level problem can be
considered as a constraint to the upper level problem, we
can treat Ω in problem of Eq. (8) as a constant-value matrix
when computing the gradients. Additionally, since R∗(Xα) is
continuous differentiable, W and α could be connected via
deﬁning the next formulation by following [14]:

w(cid:96)(α) = ∇R∗(Xα).

(9)

Therefore, the key component on the rest is how to update
the dual variable α and further compute the corresponding
gradients in Eq. (6).

Algorithm 1 ATTack on Federated Learning (AT2FL)
Input: Nodes Ntar, Nsou, attacker budget ˆns;

1: Randomly initialize ∀(cid:96) ∈ Nsou,
(cid:96) = {( ˆX 0
ˆD0
(cid:96) , ˆy0

(cid:96) ∈ Rd×ˆn(cid:96), ˆα0

(cid:96) )| ˆX 0

(cid:96) , ˆα0

(cid:96) ∈ Rˆn(cid:96), ˆy0

(cid:96) ∈ Rˆn(cid:96)} (7)

(cid:96) , ∀(cid:96) ∈ Nsou and matrix Ω0;

2: Initialize ˆD(cid:96) = ˆD0
3: for k = 0, 1, . . . do
4:
5:
6:

Set subproblem learning rate η;
for all nodes (cid:96) = 1, 2, . . . , m in parallel do

Compute the approximate solution ∆α(cid:96) via Eq. (13)
or Eq. (14);
Update local variables α(cid:96) ← α(cid:96) + ∆α(cid:96);
if (cid:96) ∈ Nsou then

Compute the approximate solution ∆ˆα(cid:96) via Eq. (13)
or Eq. (14);
Update local variables ˆα(cid:96) ← ˆα(cid:96) + ∆ˆα(cid:96);

end if
end for
Update W k and Ωk based on the latest α;
for all source nodes s = 1, 2, . . . , Nsou in parallel do

Update ˆxi

s based on the Eq. (4);

7:

8:
9:

10:
11:
12:

13:
14:
15:
16:
17:
18: end for

end for
ˆD(cid:96) = ˆDk

(cid:96) , ∀(cid:96) ∈ Nsou;

Update Dual Variable α: Consider the maximal increase of
the dual objective with a least-square loss function or a hinge
loss function, where we only allow to change each element
of α. We can reformulate Eq. (8) as the following constrained
optimization problem for the (cid:96)-th node:

min
α

m
(cid:88)

(cid:96)=1

1
n(cid:96) + ˆn(cid:96)

(cid:0)L∗

(cid:96) (−αi

(cid:96)) + L∗

(cid:96) (−ˆαi(cid:48)

(cid:96) )(cid:1) + λ1R∗(X[α(cid:96); ˆα(cid:96)]).

(10)
To solve Eq. (10) across distributed nodes, we can deﬁne
the following data-local subproblems, which are formulated
via a careful quadratic approximation of this dual problem to
separate computation across the nodes. Moreover, at every step
k, two samples (i.e., i in {1, . . . , n(cid:96)} and i(cid:48) in {1, . . . , ˆn(cid:96)})
are chosen uniformly at random from original clean data and
(cid:96) and ˆαi(cid:48)
injected data, respectively, and the updates of both αi
in node (cid:96) can be computed as:
(cid:96))k = (αi
(cid:96) )k = (ˆαi(cid:48)

(cid:96))k−1 + ∆αi
(cid:96),
(cid:96) )k−1 + ∆ˆαi(cid:48)
(cid:96) ,

(11)

(cid:96)

where both ∆αi
(cid:96) are the stepsizes chosen to achieve
maximal ascent of the dual objective in Eq. (10) when all
variables are ﬁxed. To achieve maximal dual ascent, one has
to optimise:

L∗

∆αi

∆ˆαi(cid:48)

(cid:96) (−(αi

(cid:96) = arg min
a∈R

(cid:96) + a))+a(cid:104)w(cid:96)(α(cid:96)), xi

(cid:96)a(cid:13)
(cid:13)
2
(cid:13)xi
(cid:13)
M(cid:96)
(cid:13)
(cid:13)
2
(cid:13)xi(cid:48)
(cid:13)
(cid:13)
(cid:96) ˆa
(cid:13)
M(cid:96)
(12)
where M(cid:96) ∈ Rd×d is the (cid:96)-th diagonal block of a sym-
metric positive deﬁnite matrix M . Furthermore, M −1 =

(cid:96) + ˆa))+ˆa(cid:104)w(cid:96)(ˆα(cid:96)), xi(cid:48)

(cid:96) = arg min
ˆa∈R

(cid:96) (−(ˆαi(cid:48)
L∗

(cid:96) (cid:105)+

(cid:96)(cid:105)+

λ1
2
λ1
2

(αi
(ˆαi(cid:48)
(cid:96) and ∆ˆαi(cid:48)

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

5

¯Ω + λ2/λ1Imd×md, where ¯Ω := Ω ⊗ Id×d ∈ Rmd×md.
Therefore, ∆αi
(cid:96) can be computed in a closed-form for the
least-square loss function, i.e.,

∆αi

(cid:96) =

(cid:96) − (xi
yi

(cid:96))(cid:62)xi
(cid:96)αi
0.5 + λ1

(cid:96) − 0.5(αi
(cid:13)
(cid:13)
2
(cid:13)xi
(cid:13)
(cid:96)
M(cid:96)

(cid:96))k−1

.

(13)

Meanwhile, ∆ˆαi(cid:48)
(cid:96) can be computed in a same manner. Fur-
thermore, we substitute the hinge loss into the optimization
problem in Eq. (8), and can obtain:

∆αi

(cid:96) =yi

(cid:96)max

(cid:16)

0, min(cid:0)1,

1 − (xi

(cid:96))k−1yi
(cid:96)

+(αi

(cid:96))k−1yi
(cid:96)

(cid:1)(cid:17)

(cid:96))(cid:62)xi
(cid:13)
(cid:13)xi
λ1
(cid:96)

(cid:96)(αi
(cid:13)
2
(cid:13)
M(cid:96)

− (αi

(cid:96)).

(14)
Update Gradient: Given Eq. (13), Eq. (14) with Eq. (9),
we can compute the gradient in Eq. (4). For the least-square
loss, we can the gradient of each injected data ˆxi
s with its
associated target node t:

∇(ˆxi

s)Lt((wt)(cid:62)xj
t )xj
t − yj
t )xj
t − yj
where j denotes the j-th sample for t-th target node. Similarly,
for the hinge loss, we can have:

t , yj
t )
t · ∇ˆxi
t · ∆ˆαi

=2((wt)(cid:62)xj
=2((wt)(cid:62)xj

wt
sΩ(t, s),

(15)

s

t , yj
t )

∇(ˆxi
t xj
t xj

=yj
=yj

s)Lt((wt)(cid:62)xj
wt
t · ∇ˆxi
t · ∆αi
sΩ(t, s).

s

(16)

Finally, the whole optimization procedure of attack on

federated learning can be summarized in Algorithm 1.

V. EXPERIMENTS

In this section, we evaluate the performance of the proposed
poisoning attack strategies, and its convergence. More speciﬁ-
cally, we ﬁrstly introduce several adopted datasets, followed
by the experimental results. Then some analyses about our
proposed model are reported.

A. Real Datasets

We adopt the following benchmark datasets for our ex-
periments, containing three classiﬁcation datasets and one
regression dataset:

EndAD (Endoscopic image abnormality detection): this
dataset is collected from 544 healthy volunteers and 519
volunteers with various lesions, including gastritis, cancer,
bleeding and ulcer. With the help of these volunteers, we
obtain 9769 lesion images and 9768 healthy images with the
resolution 489 × 409. Some examples are shown in Fig. 2. In
the implementation, we extract a 128-dimensional deep feature
via the VGG model trained in the [6]. To simulate the federated
learning procedure, we split the lesion images on a per-disease
basis, while splitting the healthy images randomly. Then we
obtain a federated learning dataset with 6 clinics, where each
clinic can be regarded as one node, i.e., 6 nodes.

Fig. 2. Sample images from EndAD dataset, where the ﬁrst and the second
rows are healthy images and lesion images, respectively.

Human Activity Recognition1: this classiﬁcation dataset
consists of mobile phone accelerometer and gyroscope data,
which are collected from 30 individuals, performing one of
six activities: {walking, walking-upstairs, walking-downstairs,
sitting, standing, lying down}. The provided feature vectors
of time is 561, whose variables are generated from frequency
domain. In this experiment, we model each individual as a
separate task and aim to predict between sitting and other
activities (e.g., lying down or walking). Therefore, we have 30
nodes in total, where each node corresponds to an individual.
Landmine: this dataset intends to detect whether or not
a land mine is presented in an area based on radar images.
Moreover, it is modeled as a binary classiﬁcation problem. Each
object in a given dataset is represented by a 9-dimensional
feature vector (four-moment based features, three correlation-
based features, one energy-ratio feature, one spatial variance
feature) and the corresponding binary label (1 for landmine
and -1 for clutter). This dataset consists of a total of 14,820
samples divided into 29 different geographical regions, i.e., the
total node number is 29.

Parkinson Data2 is used to predict Parkinson’s disease
symptom score for patients based on 16 biomedical features.
The Parkinson dataset contains 5,875 observations for 42
patients, and predicting the symptom score for each patient
is treated as a regression task, leading to 42 regression tasks
with the number of instances for each task ranging from 101
to 168. Additionally, the dataset’s output is a score consisting
of Total and Motor, we thus have two regression datasets:
Parkinson-Total and Parkinson-Motor in this experiment.

For the evaluation, we adopt the classiﬁcation Error to
evaluate the classiﬁcation performance. For the regression
problems, RMSE (root mean squared error) is a reasonable
evaluation metric. The smaller the Error and RMSE value is,
the better the performance of model will be, i.e., the weaker
the attack strategy will be.

B. Evaluating Attack Strategies

In our

evaluate

experiments, we

the performance
of our proposed poisoning attack strategies with ran-
dom/direct/inditect/hybrid attacks on different datasets. For
each dataset, we randomly choose half of nodes as Ntar, while
selecting the rest of nodes as free nodes. For example, the
number of Ntar for Human Activity dataset is 15, and the

1https://archive.ics.uci.edu/ml/datasets/Human+Activity+

Recognition+Using+Smartphones

2https://archive.ics.uci.edu/ml/datasets/parkinsons+telemonitoring

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

TABLE II
COMPARISONS BETWEEN OUR MODEL AND STATE-OF-THE-ARTS IN TERMS OF CLASSIFICATION ERROR AND RMSE ON FIVE DATASETS: MEAN AND
STANDARD ERRORS OVER 10 RANDOM RUNS. MODELS WITH THE BEST PERFORMANCE ARE BOLDED.

EndAD
Human Activity
Landmine

Parkinson-Total
Parkinson-Moter

Metrics
Error(%)
Error(%)
Error(%)
Avg. Error (%)
RMSE
RMSE
Avg. RMSE(%)

Non attacks
6.881±0.52
2.586±0.84
5.682±0.28
5.049±0.55
6.302±0.45
4.125±0.50
5.213±0.48

Random direct attacks
7.659±1.14
3.275±0.71
5.975±0.36
5.636±0.74
13.651±2.10
11.472±2.51
12.562±2.31

Random indirect attacks
6.888±0.45
2.894±0.83
5.735±0.36
5.172±0.55
6.633±0.75
5.046±1.14
5.839±0.95

Random hybird attacks
7.154±0.16
3.172±0.69
5.819±0.22
5.382±0.36
11.145±1.83
9.422±1.81
10.284±1.82

Direct attacks
28.588±3.74
29.422±2.96
13.648±0.54
23.886±2.41
44.939±3.21
32.992±3.78
38.966±3.49

Indirect attacks
7.324±0.62
3.438±0.34
7.428±0.39
6.069±0.45
7.763±0.82
6.866±1.21
7.314±1.02

Hybird attacks
16.190±2.26
17.829±2.75
9.579±0.27
14.533±1.76
21.990±3.17
16.956±3.78
19.473±3.48

(a) EndAD

(b) Human Activity

(c) Landmine

(d) Parkinson

Fig. 3. The correlation matrices Ω of different datasets: End_AD dataset (a),
Human Activity dataset (b), Landmine dataset (c) and Parkinson dataset (d),
where the ﬁrst half of nodes and the rest of nodes denote the Ntar and Nsou
for the indirect attack scenario, respectively. The darker color indicates the
higher correlation for each dataset, and vice versa.

number of Ntar for EndAD dataset is 3, respectively. Moreover,
the details of evaluated attack strategies are:

• Direct attacks: all the source attacking nodes Nsou are set

as the same as the target nodes, i.e., Nsou = Ntar.

• Indirect attacks: all the source attacking nodes Nsou are
from the rest of nodes, where the number of Nsou is same
as that of Ntar.

• Hybrid attacks: all the source attacking nodes Nsou are
randomly selected from all the nodes, where the number
of Nsou is same as that of Ntar.

• Random Direct/Indirect/Hybrid attacks: the attack strat-
egy are same as that of Direct/Indirect/Hybrid attacks.
However, the injected data samples are randomly chosen.
For the used parameters in our model, the step size η1 in
Eq. (4) is set as 100, both λ1 and λ2 are set as 0.001 among all
the experiments for a fair comparison. Furthermore, the number
of injected data samples for each source attacking node is set
as 20% of the clean data. The experimental results averaged
over ten random repetitions are presented in Table. II. From
the presented results, we have the following observations:

• All the attack strategies (e.g., direct attacks, random
attacks, etc) have the impact on all the datasets, which
veriﬁes the vulnerability of federated machine learning.
Among all the attack strategies, notice that the direct
attacks can signiﬁcantly damage the classiﬁcation or
regression on all the datasets. For example, the perfor-

mances of EndAD and Human Activity datasets can
lead to 21.707% and 26.836% deterioration in terms of
classiﬁcation Error, respectively. This observation indicates
direct attacks are the big threats to the federated machine
learning system.

• When comparing with the random attack strategies, our
proposed attack strategies can obtain better deterioration
performance among most cases for the federated machine
learning problems, which justiﬁes that the learning attack
strategies can work better than just launching random
attack strategies.

• For the indirect attack strategy, we can notice its per-
formances are not good as direct and hybrid attack
strategies, e.g., Human Activity Recognition dataset. This
is because that the indirect attack can be successfully
launched via effectively using the communication protocol
among different nodes, where the communication protocol
is bridged by the model relationship matrix Ω in this
paper. To verify this observation, we also present the
corresponding correlation matrix of each used dataset in
Figure 3. As illustrated in Figure 3, the nodes from Ntar
and Nsou have highly correlated in Landmine dataset,
i.e., Figure 3(c). However, the node correlations in the
Parkinson dataset (i.e., Figure 3(d)) are not close. This
observation indicates the reason why indirect attack
strategy performs better on the Landmine dataset when
comparing with other used datasets.

C. Sensitivity Study and Convergence Analysis

This subsection ﬁrst conducts sensitivity studies on how the
ratio of injected data and step size η affect different attack
strategies. Then we also provide the convergence analysis about
our proposed AT2FL.

1) Effect of Ratio of Injected Data: To investigate the effect
of ratio of injected poisoned data, we use all the datasets in
this subsection. For each dataset, we take the same attack
setting as that in Table II, e.g., for the direct attack, the
Ntar and Nsou are selected by randomly selecting half of
nodes as Ntar, and the rest of nodes to form Nsou. For the
injected data in each source attacking node, we tune it in
a range {0, 5%, 10%, 20%, 30%, 40%} of the clean data, and
present the results in Figure 4. From the provided results in
Figure 4, we can ﬁnd that: 1) For all the used datasets, the
performances under different attack strategies are decreased
with the increasing of the injected data. This observation
demonstrates the effectiveness of attack strategies computed by
our proposed AT2FL; 2) Obviously, the performances of direct

Node 1Node 2Node 3Node 4Node 5Node 6Node 1Node 2Node 3Node 4Node 5Node 60.120.130.140.150.160.17Node 5Node 10Node 15Node 20Node 25Node 30Node 5Node 10Node 15Node 20Node 25Node 3000.010.020.030.04Node 5Node 10Node 15Node 20Node 25Node 5Node 10Node 15Node 20Node 2500.010.020.030.040.050.06Node 5Node 10Node 15Node 20Node 25Node 30Node 35Node 40Node 5Node 10Node 15Node 20Node 25Node 30Node 35Node 400.10.20.30.40.50.6JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

7

(a) EndAD

(b) Human Activity

(a) EndAD

(b) Human Activity

(c) Landmine

(d) Parkinson-Total

Fig. 4. The effect of ratio of injected data on EndAD (a), Human Activity (b),
Landmine (c) and Parkinson-Total (d) datasets, where different lines denote
different types of attacking strategies.

(a) Human Activity

(b) Landmine

Fig. 5. The effect of step size η of Eq. (4) on Human Activity (a) and Landmine
(b) datasets, where different lines denote different attacking strategies.

attack are better than that of indirect attack by varying the ratio
of injected data. This because that it can directly involve the
data of Ntar; 3) Indirect attack almost has no effect on EndAD
dataset since the values of correlation matrix Ω for this dataset
are relatively low, and each node can learn good classiﬁer
without the help of other nodes; 4) Different from other datasets,
the classiﬁcation Errors slightly decrease after the ratio of
injected data is 10%. It is because the classiﬁcation Error is
computed on the test data while the formulation in Eq (3)
aims to maximize the loss on the training data. Meanwhile, the
classiﬁcation Error on the test data obtains the upper bound
when the ratio of injected data is 10%.

2) Effect of Step Size η: In order to study how the value
of step size η affects the performance of our proposed attack
strategies, we adopt Human Activity Recognition and Landmine
datasets in this subsection. By ﬁxing other parameters (e.g., λ1
and λ2) and varying the value of step size η of Eq. (4) in range
{0.01, 0.1, 1, 10, 100, 1000}, we present the corresponding 20-
th iteration performances of different attack strategies in
Figure 5. Notice that the performances of different attack
strategies are improved with the increasing of the step size η.
This is because the gradient in Eq. (4) with small step size
will be treated as noise information, which will just have little
impact on the classiﬁcation performance. However, these attack
strategies also outperforms the non-attack strategy. Furthermore,

(c) EndAD

(d) Human Activity

Fig. 6. The convergence curves of our proposed AT2FL algorithm, where the
classiﬁcation Errors are in (a) EndAD and (b) Human Activity datasets, and
the primal sub-optimality performances of the lower level problem for Eq. (3)
are in (c) EndAD and (d) Human Activity datasets.

the performance of different attack strategies tend to a ﬁxed
point with the increasing of step size η. This observation
indicates that our AT2FL can effectively converge to a local
optimum when the step size is enough.

3) Convergence of Proposed AT2FL: To study the conver-
gence of our proposed AT2FL algorithm, we adopt the EndAD
and Human Activity Recognition datasets in this subsection.
Speciﬁcally, under different attack strategies, we present the
primal sub-optimality of the lower level problem for Eq. (3)
and classiﬁcation Error (%) of the nodes in Ntar in Figure 6.
From the presented curves in Figure 6, we can notice that
the original values of primal sub-optimality and classiﬁcation
error are higher. This is because that the weight matrix W in
Eq. (3) is just initialized with the injected data points. Then our
proposed AT2FL algorithm can converge to a local optima after
a few iterations for all the three kinds of attacks on EndAD
and Human Activity Recognition datasets. This observation
indicates the effectiveness of our proposed AT2FL algorithm.

VI. CONCLUSION

In this paper, we take an earlier attempt on how to effectively
launch data poisoning attacks on federated machine learning.
Beneﬁtting from the communication protocol, we propose a
bilevel data poisoning attacks formulation by following general
data poisoning attacks framework, where it can include three
different kinds of attacks. As a key contribution of this work, we
design a ATTack on Federated Learning (AT2FL) to address the
system challenges (e.g., high communication cost) existing in
federated setting, and further compute optimal attack strategies.
Extensive experiments demonstrate that the attack strategies
computed by AT2FL can signiﬁcantly damage performances of
real-world applications. From the study in this paper, we ﬁnd
that the communication protocol in federated learning can be
used to effectively launch indirect attacks, e.g., when two nodes
have strong correlation. Except for the horizontal federated

05%10%20%30%40%Ratio of Injected Data5101520253035Classification Error (%)Direct AttackIndirect AttackHybrid AttackNon Attack05%10%20%30%40%Ratio of Injected Data05101520253035Classification Error (%)Direct AttackIndirect AttackHybrid AttackNon Attack05%10%20%30%40%Ratio of Injected Data4681012141618Classification Error (%)Direct AttackIndirect AttackHybrid AttackNon Attack05%10%20%30%40%Ratio of Injected Data51015202530354045RMSEDirect AttackIndirect AttackHybrid AttackNon Attack0.010.11101001000Value of Step Size  051015202530Classification Error (%)Direct AttackIndirect AttackHybrid AttackNon Attack0.010.11101001000Value of Step Size  6789101112Classification Error (%)Direct AttackIndirect AttackHybrid AttackNon Attack05101520Number of Iteration020406080100Classification Error (%)EndAD datasetDirect AttackIndirect AttackHybrid Attack05101520Number of Iteration0102030405060Classification Error (%)Human Activity Recognition datasetDirect AttackIndirect AttackHybrid Attack05101520Number of Iteration23456Primal Sub-OptimalityEndAD datasetDirect AttackIndirect AttackHybrid Attack05101520Number of Iteration051015202530Primal Sub-OptimalityHuman Activity Recognition datasetDirect AttackIndirect AttackHybrid AttackJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

8

[17] Guowen Xu, Hongwei Li, Sen Liu, Kan Yang, and Xiaodong Lin.
Verifynet: Secure and veriﬁable federated learning. IEEE Transactions
on Information Forensics and Security, 15:911–926, 2019.

[18] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated
machine learning: Concept and applications. ACM Transactions on
Intelligent Systems and Technology (TIST), 10(2):12, 2019.

[19] Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. Adversarial examples:
Attacks and defenses for deep learning. IEEE transactions on neural
networks and learning systems, 2019.

[20] Mengchen Zhao, Bo An, and Christopher Kiekintveld. Optimizing
personalized email ﬁltering thresholds to mitigate sequential spear
phishing attacks. In Thirtieth AAAI Conference on Artiﬁcial Intelligence,
2016.

[21] Mengchen Zhao, Bo An, Yaodong Yu, Sulin Liu, and Sinno Jialin Pan.
Data poisoning attacks on multi-task relationship learning. In Thirty-
Second AAAI Conference on Artiﬁcial Intelligence, 2018.

[22] Daniel Zügner, Amir Akbarnejad, and Stephan Günnemann. Adversarial
attacks on neural networks for graph data. In Proceedings of the 24th
ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining, pages 2847–2856. ACM, 2018.

learning in this work, we will consider the data poisoning
attacks study on vertical (feature-based) federated learning and
federated transfer learning in the future.

APPENDIX A
DEFINITION OF LEAST-SQUARE AND HINGE LOSSES

For the regression problem in this paper, we adopt least-
(cid:96))2, and dual formulation

(cid:96)) = (w(cid:62)

(cid:96) − yi

(cid:96) xi

(cid:96) xi

square loss: L(cid:96)(w(cid:62)
is:

L∗

(cid:96) (−αi

(cid:96)) = −yi

(cid:96)αi
the classiﬁcation problems, we adopt hinge loss:
(cid:96) xi
(cid:96)), and the dual problem is:

(cid:96) + (αi

(cid:96))2/4,

(17)

(cid:96)yi

For
L(cid:96)(w(cid:62)

(cid:96)) = max(0, 1 − w(cid:62)
(cid:96) xi
(cid:96)αi
(cid:96),

(cid:40)

L∗

(cid:96) (−α(cid:96)) =

−yi
∞

0 ≤ yi

(cid:96)αi
(cid:96) ≤ 1,
otherwise,

(18)

where αi
in the regression or classiﬁcation problems.

(cid:96) is the corresponding dual variable for the (cid:96)-th node

REFERENCES

[1] Scott Alfeld, Xiaojin Zhu, and Paul Barford. Data poisoning attacks
against autoregressive models. In Thirtieth AAAI Conference on Artiﬁcial
Intelligence, 2016.

[2] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and
Vitaly Shmatikov. How to backdoor federated learning. arXiv preprint
arXiv:1807.00459, 2018.

[3] Marco Barreno, Blaine Nelson, Anthony D Joseph, and J Doug Tygar.
The security of machine learning. Machine Learning, 81(2):121–148,
2010.

[4] Marco Barreno, Blaine Nelson, Russell Sears, Anthony D Joseph, and
J Doug Tygar. Can machine learning be secure? In Proceedings of the
2006 ACM Symposium on Information, computer and communications
security, pages 16–25. ACM, 2006.

[5] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks
against support vector machines. arXiv preprint arXiv:1206.6389, 2012.
[6] Ken Chatﬁeld, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.
Return of the devil in the details: Delving deep into convolutional nets.
arXiv preprint arXiv:1405.3531, 2014.

[7] Ingrid Daubechies, Massimo Fornasier, and Ignace Loris. Accelerated
projected gradient method for linear inverse problems with sparsity
constraints. journal of fourier analysis and applications, 14(5-6):764–
792, 2008.

[8] Adrià Gascón, Phillipp Schoppmann, and Mariana Raykova. Secure

linear regression on vertically partitioned datasets.

[9] Shuguo Han, Wee Keong Ng, Li Wan, and Vincent CS Lee. Privacy-
preserving gradient-descent methods. IEEE Transactions on Knowledge
and Data Engineering, 22(6):884–899, 2009.

[10] Ling Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein,
In Proceedings of the
and JD Tygar. Adversarial machine learning.
4th ACM workshop on Security and artiﬁcial intelligence, pages 43–58.
ACM, 2011.

[11] Jakub Koneˇcn`y, Brendan McMahan, and Daniel Ramage. Federated
optimization: Distributed optimization beyond the datacenter. arXiv
preprint arXiv:1511.03575, 2015.

[12] Jakub Koneˇcn`y, H Brendan McMahan, Daniel Ramage, and Peter
Richtárik. Federated optimization: Distributed machine learning for
on-device intelligence. arXiv preprint arXiv:1610.02527, 2016.
[13] Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. Data
poisoning attacks on factorization-based collaborative ﬁltering.
In
Advances in neural information processing systems, pages 1885–1893,
2016.

[14] Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic
dual coordinate ascent for regularized loss minimization. In International
conference on machine learning, pages 64–72, 2014.

[15] Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar.
In Advances in Neural Information

Federated multi-task learning.
Processing Systems, pages 4424–4434, 2017.

[16] Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, and Xiaokui Xiao.
Coupled multi-layer attentions for co-extraction of aspect and opinion
terms. In Thirty-First AAAI Conference on Artiﬁcial Intelligence, 2017.

