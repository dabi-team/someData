Journal of Machine Learning Research 1 (2000) 1-48

Submitted 4/00; Published 10/00

CodeTrans: Towards Cracking the Language of Silicon’s
Code Through Self-Supervised Deep Learning and High
Performance Computing

Ahmed Elnaggar∗
Wei Ding∗
Department of Informatics
TUM (Technical University of Munich)
Boltzmannstrasse 3, 85748 Garching, Germany

Llion Jones
Google AI
Google
1600 Amphitheatre Parkway, Mountain View, CA 94043, USA

ahmed.elnaggar@tum.de
wei.ding@tum.de

llions@google.com

Tom Gibbs
Tamas Feher
Christoph Angerer
Nvidia
2788 San Tomas Expy, Santa Clara, CA 95051, Vereinigte Staaten, USA.

tgibbs@nvidia.com
tfeher@nvidia.com
cangerer@nvidia.com

Silvia Severini
Center for Information and Language Processing
Ludwig-Maximilians-Universit¨at M¨unchen
Oettingenstraße 67, D-80538 M¨unchen, Germany.

Florian Matthes
Burkhard Rost
Department of Informatics
TUM (Technical University of Munich)
Boltzmannstrasse 3, 85748 Garching, Germany

Editor: Leslie Pack Kaelbling

silvia@cis.uni-muenchen.de

matthes@tum.de
assistant@rostlab.org

Abstract
Currently, a growing number of mature natural language processing applications make peo-
ple’s life more convenient. Such applications are built by source code — the language in
software engineering. However, the applications for understanding source code language
to ease the software engineering process are under-researched. Simultaneously, the trans-
former model, especially its combination with transfer learning, has been proven to be a
powerful technique for natural language processing tasks. These breakthroughs point out
a promising direction for process source code and crack software engineering tasks. This
paper describes CodeTrans — an encoder-decoder transformer model for tasks in the soft-
ware engineering domain, that explores the eﬀectiveness of encoder-decoder transformer
models for six software engineering tasks, including thirteen sub-tasks. Moreover, we have
investigated the eﬀect of diﬀerent training strategies, including single-task learning, transfer

∗. Equal contribution. Correspondence to ahmed.elnaggar@tum.de.

©2000 Elnaggar, Ding, Matthes, Rost, Jones, Gibbs, Feher, Angerer, and Severini.

Elnaggar, Ding, Matthes, Rost, Jones, Gibbs, Feher, Angerer, and Severini

learning, multi-task learning, and multi-task learning with ﬁne-tuning. CodeTrans outper-
forms the state-of-the-art models on all the tasks. To expedite future works in the software
engineering domain, we have published our pre-trained models of CodeTrans.1

Keywords:
Code Summarization, Commit Message Generation, API Generation, Program Synthesis

Software Engineering, Natural Language Processing, Transformer, Source

1. Introduction

Software engineering can be considered a process of designing, implementing, testing, and
maintaining information systems such as applications, frameworks, or other software com-
ponents (Royce, 1987) — this is a highly complex undertaking. So experienced specialists
invent and use diﬀerent tools and methods (for example, design patterns, code documen-
tation, unit tests, version control tools, etc.) to control and improve the software quality
and make the software engineering process more eﬀective and convenient. In software engi-
neering, works are done by using diﬀerent programming languages. Programming language
can be considered as a type of language to communicate with the computer systems to
build the applications and achieve the requirements. Therefore, natural language process-
ing techniques could also be applied to solve the programming languages’ tasks to assist
the software engineering process.

Recently, using models like BERT (Devlin et al., 2018), XL-NET (Yang et al., 2019),
ALBERT (Lan et al., 2019), RoBERTa (Liu et al., 2019b), GPT-3 (Brown et al., 2020),
and T5 (Raﬀel et al., 2020) have become a trend in natural language processing. All
these models have the Transformer architecture of Vaswani et al. (2017) with attention
mechanisms. Furthermore, their large pre-trained checkpoints are very suitable to be ﬁne-
tuned on downstream supervised tasks. Such a transfer learning technique helps transfer the
knowledge gained from diﬀerent large datasets to a small speciﬁc dataset, avoids overﬁtting,
and saves computational cost. Among them, Raﬀel et al. (2020) carried out a large number
of experiments to explore transfer learning performance and proved its eﬀectiveness in
natural language processing tasks. In addition, the Multi-Task Deep Neural Network (MT-
DNN) proposed by Liu et al. (2019a) based on Bert also obtained excellent results on
Natural Language Understanding tasks. MT-DNN involves the pre-training stage of Bert
together with a multi-task learning approach and shows the latter’s excellent performance.
In the software engineering domain, transformer models with transfer learning and multi-
task learning are under-explored. According to the recent research by Watson (2020),
the Recurrent Neural Network (Kombrink et al., 2011) (including long short-term memory
(LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al.,
2014)) were the most popular model for software engineering tasks from 2000 to 2019. These
kinds of models can understand a sequence of texts. Iyer et al. (2016) proposed CODE-
NN with the architecture consisting of LSTM guided by a global attention model (Luong
et al., 2015) to summarize SQL and CSharp code snippets. Hu et al. (2018) published
the DeepCom composed by encoder-decoder LSTMs to generate code comments for Java
code functions automatically. Jiang et al. (2017) developed the Neural Machine Translation
(NMT) with encoder-decoder RNNs to generate commit messages from git change diﬀs. The
model DeepAPI from Gu et al. (2016) to generate API usage sequences for a given natural

1. https://github.com/agemagician/CodeTrans

2

CodeTrans: Transformer for Source Code

language query is composed of attention-based encoder-decoder GRUs. Polosukhin and
Skidanov (2018) also used GRU and RNN cells to build a model synthesizing LISP-inspired
domain-speciﬁc language (DSL) code.

Nevertheless, the transformer models have also lately gained attention for software engi-
neering tasks. Feng et al. (2020) proposed a transformer model called CodeBERT to support
natural language and programming language tasks like natural language code search, code
documentation generation, etc. Moreover, Lachaux et al. (2020) published a Transcoder
model for translating functions between C++, Java, and Python based on the transformer
architecture. These models all achieved state-of-the-art results for software engineering
tasks.

This paper proposes our CodeTrans models for software engineering tasks based on
the encoder-decoder transformer architecture. We applied CodeTrans to six diﬀerent kinds
of tasks, including Code Documentation Generation, Source Code Summarization, Code
Comment Generation, Git Commit Message Generation, API Sequence Recommendation,
and Program Synthesis. Furthermore, we trained CodeTrans using single-task learning,
transfer learning, and multi-task learning on one NVIDIA GPU and Google Cloud TPUs.
Also, we used both supervised tasks and self-supervised tasks for building a language model
in the software engineering domain. Our models achieve state-of-the-art performance on all
the tasks. We have also contributed our pre-trained checkpoints and published the models
for each task in Hugging Face model Hub.2

The structure of this paper is as follows: In Section 2, we introduce the tasks of our
experiments, the datasets, and our preprocessing methods. We explain the model architec-
ture and the vocabulary representation in Section 3. We list our experiment details during
the training in Section 4. The models’ performances are compared in Section 5. In Section
6 we oﬀer our reﬂection and conclusion. Future work necessary for further research is also
introduced in this section.

2. Tasks and Datasets

This section describes the supervised tasks and the corresponding datasets, and the unla-
beled datasets for self-supervised tasks. Furthermore, we explain the data preprocessing
methods for diﬀerent languages.

2.1 Supervised Tasks and Datasets

In this work, we trained six supervised tasks in the software engineering domain as follows.
Code Documentation Generation: For the Code Documentation Generation su-
pervised tasks, a code function was an input to the model, and the model generated the
documentation for this function. CodeSearchNet Corpus Collection3 (Husain et al., 2019)
It contains six programming languages’ func-
was selected as the dataset for this task.
tions/methods, including Python, Java, Go, Php, Ruby, and Javascript. These functions
with their documentation were downloaded from the Github4 repository. We used the pre-

2. https://huggingface.co/models?search=code_trans
3. https://github.com/github/CodeSearchNet
4. https://github.com/

3

Elnaggar, Ding, Matthes, Rost, Jones, Gibbs, Feher, Angerer, and Severini

processed version by CodeBERT5 (Feng et al., 2020), which already provides a parsed and
tokenized dataset.

Source Code Summarization: Given a short code snippet, the Source Code Sum-
marization task generates a summary for this code. This task involves Python, SQL, and
CSharp languages. We used the dataset6 generated by Iyer et al. (2016). This dataset is
extracted from StackOverﬂow.7 The code snippets are from accepted answers that contain
exactly one code snippet, and the summarization is the corresponding title of the ques-
tion.
Iyer et al. (2016) asked human annotators to provide two additional titles for 200
randomly chosen code snippets from the validation and test set for SQL and CSharp code.
We followed their preprocessing methods and evaluation using the test dataset annotated
by human annotators.

Code Comment Generation: Like Code Documentation Generation, this task fo-
cuses on generating the JavaDoc for the Java functions. We used the corpus8 from Hu
et al. (2018) for this task. They focused on Javadoc comments from 9,714 Java open source
projects from Github. The ﬁrst sentence in the Javadoc description is extracted as the
expected comment.

Git Commit Message Generation: The task Git Commit Message Generation aims
to generate a commit message describing the git commit changes. The development-assisted
tool Git is a version-control system for tracking changes in ﬁles and codes during software
engineering. A well-structured code commit helps to overview the project development and
control the code changes and the development quality. We used the dataset9 from Jiang
et al. (2017) based on 1,000 Java repositories having the most Github stars.

API Sequence Recommendation: We aimed to generate the API usage sequence,
including the class and function names, by giving the model a natural language description
as an input. It would be beneﬁcial to suggest to developers the API sequence when searching
and asking about the corresponding usages. The dataset10 was extracted by Gu et al. (2016)
from Java projects with at least one star from Github.

Program Synthesis: Program synthesis is the task of synthesizing or generating pro-
gramming codes based on natural language description. We used the AlgoLisp dataset11
(Polosukhin and Skidanov, 2018) for the Program Synthesis task. This dataset was ex-
tracted from homework assignments for introductory computer science courses, so each
example in this dataset consisted of a question and an answer. We input the question into
our model and expect the model to output the correct LISP-inspired DSL code answer.

Table 1 compares the number of samples in training, validation, and testing datasets
for each supervised learning task. We observe that the API Sequence Recommendation has
the most signiﬁcant number of samples. The second-largest dataset is the Code Comment
Generation dataset. Four out of six tasks have the datasets extracted from GitHub. Fur-
thermore, three out of six tasks used the function-level as an input rather than a complete
program-level input.

5. https://github.com/microsoft/CodeBERT
6. https://github.com/sriniiyer/codenn
7. https://stackoverflow.com/
8. https://github.com/xing-hu/DeepCom
9. https://sjiang1.github.io/commitgen/
10. https://github.com/guxd/deepAPI
11. https://github.com/nearai/program_synthesis/tree/master/program_synthesis/algolisp

4

CodeTrans: Transformer for Source Code

Task

Language

Train

Valid

Test

Data Source

Data Level

Code Documentation Generation

Source Code Summarization

Python
Java
Go
Php
Ruby
Javascript

Python
Csharp
SQL

251,820
164,923
167,288
241,241
24,927
58,023

12,004
52,943
25,671

13,914
5,183
7,325
12,982
1,400
3,885

2,792
6,601
3,326

14,918
10,955
8,122
14,014
1,261
3,291

2,783
108
100

Code Comment Generation

Java

470,451

58,811

58,638

Git Commit Message Generation

Java

26,208

3,000

3,000

GitHub

Function

StackOverﬂow Code Snippet

GitHub

GitHub

GitHub

Function

Commit

API

API Sequence Recommendation

Program Synthesis

Java

DSL

7,475,850

-

10,000

79,214

10,819

9,967

Homework

Function

Table 1: The summarization of supervised datasets. It includes the number of samples in
training, validation and testing data-sets, data source, and the programming data level for
each supervised dataset. Each sample could be a code function, a code snippet, a commit
diﬀ, or a natural language sentence, depending on the data level.

2.2 Unlabeled Datasets

Unlabeled datasets are used in the transfer learning pre-training stage and the multi-task
learning. They are helpful when building a language model for tasks in the software en-
gineering domain and make the ﬁnal model more generalized against overﬁtting (Ruder,
2017). We involved six diﬀerent corpora for unlabeled datasets, covering nine programming
languages and English.

CodeSearchNet Corpus Collection: The CodeSearchNet Corpus Collection (Husain
et al., 2019) was used as one of the supervised tasks and as one of the self-supervised tasks.
This dataset is divided into two parts — functions with the function documentation and
functions without documentation. In the self-supervised training, we have used both of these
parts together. For functions without documentation, we directly considered each function
as a separate sentence sequence. For functions with documentation, we concatenated each
pair of tokenized-function and its tokenized-documentation as one input sentence sequence.
All the code samples in this corpus are function-level samples.

Public Git Archive: We used CSharp and an additional Java datasets from the Public
Git Archive dataset12 (Markovtsev and Long, 2018). The Public Git Archive has code in
the ﬁle-level containing the import statements, multiple functions, and comments. Such
ﬁle-level data could help the models to understand more information like API usage and
the relationship between diﬀerent functions.

150k Python Dataset: We included the 150k Python Dataset13 (Raychev et al.,
2016) from the SRILAB at ETH Zurich. The Python programs in this data-set are collected
from GitHub repositories with permissive and non-viral licenses by removing duplicate ﬁles,

12. https://github.com/src-d/datasets/tree/master/PublicGitArchive
13. https://www.sri.inf.ethz.ch/py150

5

Elnaggar, Ding, Matthes, Rost, Jones, Gibbs, Feher, Angerer, and Severini

Language Without Doc With Doc

CodeSearchNet

150k Python
Dataset

The Public
Git Archive

StaQC

LISP

One Billion
Word Corpus

Total

Python

657,030

375,210

149,114

Java

Go

Php

Ruby

1,070,271

373,412

379,103

300,882

398,058

369,923

110,551

43,803

99,646

Javascript

1,717,933

CSharp

SQL

LISP

English

Total

720,124

469,038

133,191

122,602

1,181,354

2,163,807

679,985

767,981

154,354

1,817,579

469,038

133,191

122,602

5,895,822

149,114

1,189,162

133,191

122,602

30,913,716

38,403,607

30,913,716

30,913,716

Table 2: The number of samples of each unlabeled data-set for diﬀerent programming
languages and the English natural language. The ﬁrst column lists the languages. For
programming languages, each sample can be considered as one function or a programming
ﬁle, or part of the code, depending on the code level of that dataset. For the English
language, one sample means one sentence.

forked projects, and obfuscated ﬁles. The Python code in this corpus is also a ﬁle-level code
like Public Git Archive datasets.

StaQC: For the SQL unlabeled dataset, we chose StaQC14 (Yao et al., 2018). This
dataset was extracted from StackOverﬂow. StaQC contained SQL question-code pairs of
questions tagged by “SQL,” “database,” or “oracle” from StackOverﬂow. The SQL code in
StaQC is code-snippet level and not the entire ﬁle-level code.

LISP: We created a new LISP dataset, with which we extracted 20 GitHub reposito-
ries15 having the most stars from the Lisp Topic by applying the GitHub Rest API16 and
parsed the ﬁles into function-level LISP code.

One Billion Word Language Model Benchmark Corpus: Despite the program-
ming languages, we used one Billion Word Language Model Benchmark corpus (Chelba
et al., 2013) as our self-supervised English data-set. Text data in one Billion Word Lan-
guage Model Benchmark corpus is obtained from the WMT11 website.17 Normalization
and tokenization were applied to the data, and duplicated sentences were removed. The
vocabulary was constructed by discarding all words with a count below three, and sentence
order was randomized. Finally, the corpus contains almost one billion words in the training
data.

Table 2 shows the number of samples each dataset used in self-supervised learning.
In total, we have around 40 million samples for this self-supervised training. One Billion
Word Language Model Benchmark corpus has more than 30 million data samples and is

14. https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset
15. https://github.com/topics/lisp?o=desc&s=stars
16. https://docs.github.com/en/rest/reference/repos#contents
17. http://statmt.org/wmt11/training-monolingual.tgz

6

CodeTrans: Transformer for Source Code

the corpus with the most number of samples. Among the programming language datasets,
CodeSearchNet Corpus is the most extensive corpus. When comparing only the program-
ming languages, the Java language has the most self-supervised samples with more than
two million inputs. Following them, Javascript and Python, where each have more than
one million samples. Ruby, SQL, and LISP have the least number of self-supervised inputs.
They only have around 150,000 samples or fewer, each.

2.3 Dataset Preprocessing

We mainly followed the instructions to preprocess the datasets if these guides existed with
their original publications. We used the same parsers and tokenizers as the original reposi-
tories. We ﬁrst removed the programs’ comments for those without instructions or guidance
because they are not part of the code. We applied diﬀerent parsers to parse the code and
get the code structure and element for programming language samples. Also, we substi-
tuted characters, strings, and numbers (integer, real, hexadecimal) with speciﬁc tokens in
the code. Furthermore, we replaced the newline characters (\n, \r, \r\n) as < newline >.
We also tokenized each sample and concatenated tokens in the sample to a sentence by
inserting a space character between every two tokens to make all the samples from diﬀerent
datasets consistent at the input-level. Lastly, as an important point, we added a unique
preﬁx for each task to allow models, speciﬁcally multi-task models, to distinguish between
training samples from diﬀerent tasks. For example, in code documentation generation for
Javascript, we added “function documentation generation javascript: ” as a preﬁx for all
samples on this task.

• English: We used the tokenize18 package from the NLTK Natural Language Toolkit
(Bird, 2006). Most of the tasks’ datasets contain at least one part English sentence,
so this tokenizer is used in preprocessing these English texts. However, we did not
apply it to the English corpus because it was already tokenized. We have used the
English tokenizer for commit messages and API sequences because we do not have a
speciﬁc tokenizer for them.

• Python: For the Python functions from CodeSearchNet Corpus Collection,19 tree-
sitter library for Python20 was applied in order to have fair comparison between our
results and theirs. For other Python codes, we used the Python tokenize21 library that
contains functions to separate the tokens of a string and returns the token’s value and
the type (number, string, etc.). For the rest of the tasks, we used it because it is the
oﬃcial library from Python, which is well maintained and updated regulary.

• Java: For the Java functions from CodeSearchNet Corpus Collection, tree-sitter li-
brary to parse Java22 functions was applied. In addition, for other tasks involving the

18. https://www.nltk.org/api/nltk.tokenize.html
19. https://github.com/github/CodeSearchNet
20. https://github.com/tree-sitter/tree-sitter-python
21. https://docs.python.org/3.7/library/tokenize.html
22. https://github.com/tree-sitter/tree-sitter-java

7

Elnaggar, Ding, Matthes, Rost, Jones, Gibbs, Feher, Angerer, and Severini

Java language, we used the Python library called javalang23 that provides a lexer and
a parser targeting Java.

• Php, Go, Javascript, Ruby: These languages are all involved in the CodeSearchNet
Corpus Collection. They were parsed by Husain et al. (2019) using modiﬁed parsers
based on the tree-sitter library for Php,24 Go,25 and Javascript.26 We followed their
methods and didn’t make any changes.

• SQL and CSharp: The Python library sqlparse27 was used for SQL tasks. This
library provides support for parsing, splitting, and formatting SQL statements. For
CSharp code, ANTLR28 (ANother Tool for Language Recognition) parser from Parr
(2013) was applied.

3. CodeTrans

We explain our models, the vocabulary generation steps, and the hardware we applied in
this section.

3.1 Model
We adapted the encoder-decoder model proposed by Vaswani et al. (2017) and the T529
framework implemented by Raﬀel et al. (2020) to our tasks. The original T5 publication
proposed ﬁve diﬀerent model sizes — Small, Base, Large, 3B, and 11B. We used the small
(60 million parameters), base (220 million parameters), and large model (770 million pa-
rameters) in this research. More details for the diﬀerent models’ parameters are shown in
Table 3.

We set all the models’ input and output length as 512 because most of the samples’
length have less than 512 tokens. For the self-supervised objective, we applied the span-
corruption strategy with a corruption rate of 15%. We considered a span of an average of
three corrupted tokens as an entirety and used a unique mask token to replace it. For this
model, the input consisted of the original input, but with some 3-gram words replaced by a
unique mask token, while the target is the original corrupted 3-gram words surrounded by
unique mask tokens for the uncorrupted spans. Diﬀerent from the T5 models, we disabled
the method of reduce concat tokens. By disabling this method, every sample will only have
a single training example rather than concatenating diﬀerent training examples up to the
maximum training sequence length.

The T5 framework is very suitable for transfer learning, multi-task learning, and ﬁne-
tune the models. It has the concept of TaskRegistry and MixtureRegistry. Each task can
be built as one TaskRegistry, and one or more TaskRegistries can build one MixtureReg-
istry. We built 13 TaskRegistries. Each programming language from each task has one

23. https://github.com/c2nes/javalang
24. https://github.com/tree-sitter/tree-sitter-php
25. https://github.com/tree-sitter/tree-sitter-go
26. https://github.com/tree-sitter/tree-sitter-javascript
27. https://github.com/andialbrecht/sqlparse
28. https://www.antlr.org/
29. https://github.com/google-research/text-to-text-transfer-transformer

8

CodeTrans: Transformer for Source Code

Number of Blocks Each

Dense Layer Output Dimension

Attention Layer Key Value Dimension

Number of Attention Heads

Sub-Layers and Embeddings Dimension

Model Parameter (in Million)

Training Steps

Final Loss

Time Cost

Small

Base

Large

6

12

24

2048

3072

4096

64

8

512

60

64

12

768

220

64

16

1024

770

Transfer Learning
Multi-Task Learning

500,000
500,000

500,000
500,000

240,000
260,000

Transfer Learning
Multi-Task Learning

0.926
0.887

0.586
0.590

0.476
0.471

Transfer Learning
Multi-Task Learning

17 days
17 days

53 days
53 days

82 days
87 days

Table 3: Model Parameters for diﬀerent size of models, as well as the time cost, and the
ﬁnal loss of transfer learning and multi-task learning pre-training stage with a batch size of
4,096.

TaskRegistry. We also built one MixtureRegistry for self-supervised learning and another
MixtureRegistry for multi-task learning.

3.2 Vocabulary

Vocabulary is an essential aspect of natural language processing. Vocabulary itself contains
much information about the corpus, like the corpus domain, formality, tone, and target
audience. Vocabulary is helpful when processing the text corpus. We need to tokenize the
input into diﬀerent ids mapping to the components in the vocabulary before putting the
text into the model. It is also the storage to construct the output of the model. The choice
of vocabulary has a critical impact on model performance and output quality. Furthermore,
the token frequency in the vocabulary also indicates the diﬀerent importance of the text
information.

We used the SentencePiece model (Kudo, 2018) to construct the vocabulary for this
research, as well as to decode and encode the input/output. SentencePiece provides diﬀer-
ent tokenization methods, including the sub-word level tokenization. It extracts sub-words
containing the semantic meanings and overcomes the drawback of the character level tok-
enization. The vocabulary generated could cover almost all the texts in the datasets. This
is better than the word level tokenization, which requires an enormous vocabulary to cover
most words in the datasets. We trained the SentencePiece on all the labeled and unlabeled
datasets used in our experiment with the unigram language model algorithm. We set the
id for padding token as 0, end of statement (EOS) token as 1, Unknown token as 2, and
beginning of statement (BOS) token as 3. We set the size of the vocabulary to 32,000. The
whole datasets have more than 46 million lines (each line could be considered one model
input example and one SentencePiece input sentence). It is tremendous when using the
unigram language model algorithm and would cause the training crash for training on the
whole sentences. Therefore, we limited the “input sentence size” to 40 million, shuﬄed

9

Elnaggar, Ding, Matthes, Rost, Jones, Gibbs, Feher, Angerer, and Severini

the input sentences to get random sentence inputs, and enabled the setting for training
a huge corpus. We set the character coverage as 0.9999 because the corpus may contain
non-English characters or meaningless symbols. In this way, we could exclude these noises
from the vocabulary.

We noticed a signiﬁcant amount of tokens of tokens indicating the programming lan-
guages and processes from the generated vocabulary, including “function,” “String,” “var,”
“import,” etc. Furthermore, it covered most of the English vocabulary. This means our
generated vocabulary is suitable for both natural language processing tasks and software
engineering tasks.

3.3 Hardware

We utilized both Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs)
for training and evaluating the models. We used one NVIDIA GPU Quadro RTX 8000,30
which has 576 NVIDIA Tensor Cores, 72 NVIDIA RT Cores, and 48 GB GDDR6 with ECC
GPU memory. We used this GPU for all the single-task learning for small models and for
part of the base models. We had two types of Google TPUs, v2-8, and v3-8. We obtained
access to two TPUs v2-8 through the Google Colab notebooks31 and multiple TPUs v3-8
using Google Cloud console. TPUs v3-8 are mainly used for multi-task learning, transfer
learning pre-training, and ﬁne-tuning models for large datasets. Moreover, TPUs v2-8 are
applied for single-task training for the base model and ﬁne-tuning the pre-trained models
on relatively small datasets.

4. Experiments

We clarify our experiment details in this section, including single-task learning, transfer
learning, multi-task learning, and multi-task learning with ﬁne-tuning.

4.1 Single-Task Learning

For single-task learning, we trained the six tasks (13 sub-tasks in total) separately using the
T5 framework. We trained both small and base size models, which generated two models
for each task and, in total, 26 models. We tuned the batch size using the grid search inside
the range of 25 and 210. We determined the training steps using early stopping concerning
the models’ performance on the validation sets based on the T5 built-in BLEU (Post, 2018)
and ROUGE (Lin, 2004) scores. The optimal training steps with the corresponding batch
size are listed in Table 4.

We noticed the following points during the single-task training:

• The number of samples in a data-set has an essential impact on the model size
and the training steps. Task API Sequence Recommendation and Code Comment
Generation have the two largest datasets. The small models for these two tasks require
almost seven times more training steps than the base models until they could converge.

30. https://www.nvidia.com/en-us/design-visualization/quadro/rtx-8000/
31. https://colab.research.google.com/notebooks/intro.ipynb#recent=true

10

CodeTrans: Transformer for Source Code

• Corpus for Source Code Summarization converges extremely fast. For SQL and
CSharp data-sets in this corpus, the base model converges in 500 training steps even
if the batch size is only 32, and the model has not seen the complete dataset yet.
The scores on this task’s validation set become worse if we train the model with more
steps. So it is very easy to overﬁt the models for the Source Code Summarization
task, mainly because of its small dataset size.

• Half of the models achieve the best performance with a batch size of 256. However,
it varies slightly among diﬀerent tasks, like the Source Code Summarization task
requiring small batch sizes. Nevertheless, large batch sizes do not result in better
performance, no matter the number of samples in the dataset.

4.2 Transfer Learning

Transfer Learning has two steps, pre-training and ﬁne-tuning. The ﬁrst uses the self-
supervised method on unlabeled data, and the latter ﬁne-tunes the models on supervised
tasks using labeled datasets. We trained the small, base, and large T5 models for transfer
learning.

All the self-supervised tasks were used and combined in the pre-training step. We
set the T5 model to mask the spans of input data by enabling, which makes the model
predict the masked content and builds an initial language model in this way. Since our pre-
trained models used the datasets containing nine programming languages and one human
language, these models are suitable to be ﬁne-tuned on other downstream tasks in the
software development domain and natural language processing domain. This can be seen as
an expansion to current multi-language language models, which focuses on having a single
model that supports only multiple human languages Xue et al. (2020). For per-training,
we chose the batch size of 4096. We pre-trained the small and base model for 500,000 steps
and the large model for 240,000 steps. Furthermore, we mainly used single TPU v3-8 in the
pre-training. Table 3 shows the ﬁnal loss and training time for diﬀerent sizes of CodeTrans
models during the pre-training.

After obtaining the pre-training model on the 500,000 training steps for the small and
base models and 240,000 steps for the large model, we ﬁne-tuned the models for the 13
supervised sub-tasks. We have noticed that half of the single-task learning models reach
their best performance with a batch size of 256. So we chose 256 as the batch size for
ﬁne-tuning the downstream tasks. We applied early stopping to determine the ﬁne-tuning
steps based on the models’ performance on the validation sets using the T5 built-in BLEU
and ROUGE scores.

4.3 Multi-Task Learning

Multi-task learning trains a single model on a mixture of tasks, allowing sharing all model
parameters across diﬀerent tasks. This training strategy improves the data augmentation,
focuses attention, and shares information for eavesdropping, learning the representation,
and regularizes the weights (Ruder, 2017). Furthermore, it allows a single model to perform
more than one task using the same weights. We trained 13 supervised sub-tasks together
with all the self-supervised tasks. The self-supervised tasks are desired to help the model

11

Elnaggar, Ding, Matthes, Rost, Jones, Gibbs, Feher, Angerer, and Severini

Task

Language

Sample Size Model Size

Batch Size

Training Steps

ST TF-FT MT-FT

ST

TF-FT MT-FT

Code Documentation Generation

Python

251,820

Java

164,923

Go

167,288

Php

241,241

Ruby

24,927

Javascript

58,023

Python

12,004

Source Code Summarization

Csharp

52,943

SQL

25,671

Code Comment Generation

Java

470,451

Git Commit Message Generation

Java

26,208

API Sequence Recommendation

Java

7,475,850

Program Synthesis

DSL

79,214

Small
Base
Large
Small
Base
Large
Small
Base
Large
Small
Base
Large
Small
Base
Large
Small
Base
Large

Small
Base
Large
Small
Base
Large
Small
Base
Large

Small
Base
Large

Small
Base
Large

Small
Base
Large

Small
Base
Large

256
384
-
256
256
-
256
256
-
256
1024
-
128
128
-
256
256
-

233
32
-
128
32
-
128
32
-

256
256
-

128
512
-

256
256
-

512
256
-

256
256
256
256
256
256
256
256
256
256
256
256
256
256
256
256
256
256

256
256
256
256
256
256
256
256
256

256
256
256

256
256
256

256
256
256

256
256
256

256
256
256
256
256
256
256
256
256
256
256
256
256
256
256
256
256
256

256
256
256
256
256
256
256
256
256

256
256
256

256
256
256

256
256
256

256
256
256

20,000
90,000
-
60,000
80,000
-
5,000
80,000
-
200,000
30,000
-
10,000
8,000
-
16,000
18,000
-

5,000
1,000
-
2,000
500
-
500
500
-

520,000
80,000
-

15,000
4,000
-

5,000
2,000
500
10,000
5,000
500
10,000
5,000
1,000
10,000
65,000
18,000
5,000
5,000
1,000
40,000
35,000
4,000

5,000
1,000
100
2,000
500
200
1,000
500
200

4,000
4,000
500
2,000
2,000
500
2,000
2,000
4,500
2,000
5,000
8,000
2,000
12,000
2,000
32,000
10,000
2,500

600
1,000
100
1,200
500
100
1,200
500
100

750,000
80,000
60,000

5,000
2,000
4,500

750,000
60,000
25,000

8,000
16,000
3,000

840,000
145,000
-

1,400,000
340,000
180,000

1,150,000
320,000
130,000

6,000
10,000
-

5,000
45,000
3,500

16,000
30,000
2000

Table 4: The single-task learning (ST) training steps, transfer learning ﬁne-tuning (TF-
FT) steps, and multi-task learning ﬁne-tuning (MT-FT) steps for the small, base, and large
models. We also listed the batch size of each task in this table. The batch sizes of single-task
learning are diﬀerent for diﬀerent tasks. However, the batch size of transfer learning and
multi-task learning are both 256.

12

CodeTrans: Transformer for Source Code

gain information about the language attributes and build a language model in both the
software development domain and human language. This allows the model to understand
both human written languages, in our case the English language, and the computer code
languages. Simultaneously, the supervised tasks help each other make the model more
generalized for all the tasks and avoid overﬁtting on each speciﬁc task.

We used examples-proportional mixing to select samples in proportion to the size of
each task’s dataset and concatenated them. This ensures that the model will see samples
from small datasets as it will see samples from large datasets on every batch. We recorded
the model checkpoint every 20,000 training steps, using a batch size of 4,096. Usually, all
the tasks should share one same best performance checkpoint. However, Raﬀel et al. (2020)
proposed a way to relax this goal and select a diﬀerent checkpoint for each task. We also
evaluated the model on the validation set and selected the best checkpoint for each task.
Therefore, each task could have a diﬀerent checkpoint from the same model. We trained T5
small, base, and large models using only single TPU v3-8. Table 3 illustrate the training
steps, ﬁnal loss, and the time cost for the multi-task learning models.

4.4 Multi-Task Learning with Fine-Tuning

We further ﬁne-tuned the multi-task learning ﬁnal checkpoint, 500k steps for the small and
base model, and 260k steps for the large model, for each supervised task separately. Like
the transfer learning ﬁne-tuning, we chose the batch size of 256 and applied early stopping
to determine the ﬁne-tuning steps based on the models’ performance on the validation data-
sets. Table 4 shows all the related hyper-parameters for the multi-task learning ﬁne-tuning
step.

5. Result

We ran the ﬁnal evaluation on the test dataset, and we compared the performance of
CodeTrans models with diﬀerent state-of-the-art models for each task. Therefore, we applied
the same metrics as other SOT models for calculating the evaluation results, as shown in
Table 5.

5.1 Code Documentation Generation

We evaluated the Code Documentation Generation tasks using smoothed BLEU-4 score
(Lin and Och, 2004) on the CodeSearchNet test dataset. The evaluation results are shown
in Table 5, and were compared with CodeBert (Feng et al., 2020). Overall, we outperformed
CodeBert on all the programming languages in this task. Multi-task learning has, in general,
the best performance and achieves the best result for three programming languages. The
possible reason for this is that the CodeSearchNet dataset is involved in two self-supervised
tasks during the multi-task training, and it has seen many training data related to this task.
Transfer learning and multi-task learning ﬁne-tuning CodeTrans models also have relatively
good performance and are much better than single-task learning.

13

Elnaggar, Ding, Matthes, Rost, Jones, Gibbs, Feher, Angerer, and Severini

5.2 Source Code Summarization

We also applied smoothed BLEU-4 to evaluate the Source Code Summarization task. We
compared CodeTrans performance with Code-NN (Iyer et al., 2016). Unfortunately, Code-
NN did not provide their result for the Python code; however, we did evaluate it for future
comparisons. By evaluating the SQL and CSharp code, Code-NN selected 100 samples with
two additional human annotations and calculated the smoothed BLEU-4 on these samples.
We followed their instruction and evaluated in the same way to have a fair comparison.
CodeTrans outperformed Code-NN on the existing scores as shown in Table 5. Among
diﬀerent CodeTrans models, multi-task learning has the best performance on two of the
programming languages for this task.

5.3 Code Comment Generation

We compared CodeTrans’ performance with DeepCom using smoothed BLEU-4 for the
task Code Comment generation, with the results shown in Table 5. CodeTrans transfer
learning large model has the best performance, and its smoothed BLEU score is higher
than DeepCom by more than one percent. CodeTrans models with multi-task learning
have the worst performance. However, the score increases with the increase of model size.
The reason is the Code Comment Generation dataset has the second-largest sample size
with 470,451 samples, and we need bigger models to exploit them all.

14

CodeTrans: Transformer for Source Code

n
e
G

-
I
P
A

n
e
G

-
t
i

G

n
o
i
t
a
z
i
r
a
m
m
u
S

e
d
o
C
e
c
r
u
o
S

n
o
i
t
a
r
e
n
e
G
n
o
i
t
a
t
n
e
m
u
c
o
D
e
d
o
C

S
P

L
S
D

3
4
.
9
8

5
6
.
9
8

0
3
.
0
9

4
2
.
0
9

1
2
.
0
9

8
8
.
2
8

9
9
.
6
8

7
2
.
0
9

1
3
.
0
9

0
3
.
0
9

7
1
.
0
9

-

-

-

-

-

0
8
.
5
8

a
v
a
J

1
7
.
8
6

5
4
.
0
7

0
9
.
8
6

1
1
.
2
7

6
2
.
3
7

3
4
.
8
5

7
9
.
7
6

9
2
.
2
7

9
2
.
9
6

9
8
.
2
7

9
3
.
3
7

-

-

-

-

2
4
.
4
5

-

a
v
a
J

1
6
.
9
3

7
6
.
8
3

2
2
.
4
4

7
1
.
4
4

G
C
C

a
v
a
J

8
9
.
7
3

7
0
.
8
3

6
5
.
8
3

6
0
.
9
3

1
4
.
4
4

0
5
.
9
3

7
1
.
6
3

5
2
.
9
3

8
1
.
1
4

6
9
.
3
4

9
1
.
4
4

4
3
.
4
4

-

-

-

1
8
.
2
3

-

-

5
1
.
0
2

4
4
.
7
2

9
6
.
4
3

7
3
.
8
3

0
9
.
8
3

5
2
.
9
3

-

-

7
1
.
8
3

-

-

-

n
o
h
t
y
P

t
p
i
r
c
s
a
v
a
J

y
b
u
R

p
r
a
h
S
C

4
7
.
9
1

5
6
.
8
1

0
4
.
0
2

2
1
.
1
2

3
4
.
1
2

9
3
.
2
2

0
2
.
3
2

7
5
.
3
2

3
0
.
2
2

0
4
.
1
2

0
1
.
1
2

-

L
Q
S

5
5
.
7
1

0
0
.
5
1

1
7
.
7
1

6
6
.
7
1

0
4
.
8
1

5
1
.
9
1

4
2
.
9
1

9
4
.
9
1

5
2
.
8
1

1
9
.
6
1

8
9
.
9
1

-

0
5
.
0
2

0
4
.
8
1

-

-

-

-

-

-

-

-

5
4
.
8

2
1
.
9

6
0
.
0
1

4
9
.
0
1

1
4
.
2
1

1
1
.
3
1

7
3
.
3
1

4
2
.
3
1

0
1
.
2
1

4
6
.
0
1

4
1
.
2
1

-

-

-

-

-

-

0
7
.
3
1

7
1
.
3
1

3
2
.
7
1

5
2
.
8
1

8
9
.
8
1

6
2
.
5
1

1
1
.
6
1

3
2
.
6
1

4
2
.
7
1

2
6
.
8
1

3
8
.
8
1

0
9
.
4
1

-

-

-

-

-

9
1
.
9

3
2
.
8

5
1
.
3
1

7
0
.
4
1

4
9
.
4
1

1
9
.
4
1

p
h
P

5
0
.
3
2

8
9
.
2
2

5
3
.
5
2

4
8
.
5
2

8
1
.
6
2

8
6
.
4
2

6
2
.
5
1

0
0
.
5
1

3
2
.
6
2

8
0
.
6
2

0
7
.
3
1

4
2
.
4
1

9
1
.
4
1

6
1
.
2
1

5
5
.
5
2

9
7
.
5
2

0
2
.
6
2

6
1
.
5
2

-

-

-

-

-

-

-

-

-

-

o
G

9
8
.
6
1

6
1
.
7
1

8
8
.
8
1

0
5
.
9
1

4
5
.
9
1

5
1
.
9
1

3
4
.
9
1

8
3
.
9
1

6
3
.
9
1

6
8
.
8
1

7
7
.
8
1

7
0
.
8
1

-

-

-

-

-

a
v
a
J

5
6
.
6
1

7
1
.
7
1

8
4
.
9
1

9
1
.
0
2

6
0
.
0
2

0
0
.
9
1

2
2
.
1
2

7
8
.
1
2

4
0
.
0
2

2
1
.
1
2

2
4
.
1
2

5
6
.
7
1

-

-

-

-

-

n
o
h
t
y
P

1
3
.
7
1

6
8
.
6
1

3
9
.
9
1

6
2
.
0
2

5
3
.
0
2

4
6
.
9
1

9
3
.
0
2

8
1
.
0
2

7
7
.
9
1

7
7
.
9
1

4
9
.
8
1

6
0
.
9
1

-

-

-

-

-

l
l
a
m
S
-
T
S
-
s
n
a
r
T
e
d
o
C

-

e
s
a
B
T
S
-
s
n
a
r
T
e
d
o
C

l
l
a
m
S
-
F
T
-
s
n
a
r
T
e
d
o
C

e
s
a
B
-
F
T
-
s
n
a
r
T
e
d
o
C

e
g
r
a
L
-
F
T
-
s
n
a
r
T
e
d
o
C

l
l
a
m
S
-
T
M

-
s
n
a
r
T
e
d
o
C

e
s
a
B
T
M

-

-
s
n
a
r
T
e
d
o
C

e
g
r
a
L
-
T
M

-
s
n
a
r
T
e
d
o
C

l
l
a
m
S
-
F
T
T
M

-

-
s
n
a
r
T
e
d
o
C

e
s
a
B
-
F
T
T
M

-

-
s
n
a
r
T
e
d
o
C

e
g
r
a
L
-
F
T
T
M

-

-
s
n
a
r
T
e
d
o
C

)
0
2
0
2

,
.
l
a

t
e

g
n
e
F
(
t
r
e
B
e
d
o
C

)
6
1
0
2

,
.
l
a

t
e

r
e
y
I
(
N
N
E
D
O
C

-

)
8
1
0
2

,
.
l
a

t
e

u
H
(
m
o
C
p
e
e
D

)
6
1
0
2

,
.
l
a

t
e

u
G

(
I
P
A
p
e
e
D

)
7
1
0
2

,
.
l
a

t
e

g
n
a
i
J
(
T
M
N

)
8
1
0
2

,
v
o
n
a
d
i
k
S

d
n
a

n
i
h
k
u
s
o
l
o
P
(
e
e
r
T
2
q
e
S

h
t
i
w
g
n
i
n
r
a
e
l

k
s
a
t
-
i
t
l
u
m

s
i

T
F
-
T
M
d
n
a

,
g
n
i
n
r
a
e
l

k
s
a
t
-
i
t
l
u
m

s
n
a
e
m
T
M

,
g
n
i
n
r
a
e
l

r
e
f
s
n
a
r
t

s
i

F
T

,
g
n
i
n
r
a
e
l

k
s
a
t
-
e
l
g
n
i
s

r
o
f

s
d
n
a
t
s
T
S

t
r
a
-
e
h
t
-
f
o
-
e
t
a
t
s

h
t
i
w

s
n
a
r
T
e
d
o
C

f
o

e
c
n
a
m
r
o
f
r
e
p

e
h
t

e
r
a
p
m
o
c

d
n
a

t
e
s
a
t
a
d

t
s
e
t

e
h
t

n
o

s
n
a
r
T
e
d
o
C

e
h
t

e
t
a
u
l
a
v
e

e

W

.
g
n
i

n
u
t
-
e
n
ﬁ

t
n
e
m
m
o
C
e
d
o
C
d
n
a

,
n
o
i
t
a
z
i
r
a
m
m
u
S

e
d
o
C
e
c
r
u
o
S

,
n
o
i
t
a
r
e
n
e
G
n
o
i
t
a
t
n
e
m
u
c
o
D
e
d
o
C
r
o
f

4
-
U
E
L
B
d
e
h
t
o
o
m

s

d
e
i
l
p
p
a

e

W

.
s
l
e
d
o
m

m
a
r
g
o
r
P
r
o
f
y
c
a
r
u
c
c
a
d
n
a

,
n
o
i
t
a
d
n
e
m
m
o
c
e
R
e
c
n
e
u
q
e
S
I
P
A
d
n
a
n
o
i
t
a
r
e
n
e
G
e
g
a
s
s
e

M

t
i

m
m
o
C
t
i

G

r
o
f

4
-
U
E
L
B
e
s
u
e

W

.
n
o
i
t
a
r
e
n
e
G

t
i

G

s
i

n
e
G

-
t
i

G

.
n
o
i
t
a
r
e
n
e
G

t
n
e
m
m
o
C

e
d
o
C

k
s
a
t

e
h
t

s
n
a
e
m

G
C
C

.
r
e
p
a
p

s
i
h
t

n
i

s
k
s
a
t

e
h
t

l
l
a

f
o

s
t
l
u
s
e
r

n
o
i
t
a
u
l
a
v
E

:
5

e
l

b
a
T

.
s
i
s
e
h
t
n
y
S
m
a
r
g
o
r
P

k
s
a
t

e
h
t

s
i

S
P

.
n
o
i
t
a
d
n
e
m
m
o
c
e
R
e
c
n
e
u
q
e
S

I
P
A
k
s
a
t

e
h
t

r
o
f

s
d
n
a
t
s

n
e
G

-
I
P
A

.
n
o
i
t
a
r
e
n
e
G
e
g
a
s
s
e

M

t
i

m
m
o
C

15

.
s
i
s
e
h
t
n
y
S

Elnaggar, Ding, Matthes, Rost, Jones, Gibbs, Feher, Angerer, and Severini

5.4 Git Commit Message Generation

The task Git Commit Message Generation’s evaluation result is listed in Table 5. We
applied the BLEU-4 (Post, 2018) for the evaluation to compare it with previous research
results. All the CodeTrans models, including single-task training, outperform the NMT
model. Among them, the CodeTrans transfer learning large model has the best BLEU-4
score. The performance of the CodeTrans multi-task learning large model is very close to
the transfer learning large model.

5.5 API Sequence Recommendation

Table 5 also contains the task API Sequence Recommendation evaluation result. We com-
pared the CodeTrans models with the DeepAPI model. We applied the same BLEU-4
metric script as the DeepAPI used. All the CodeTrans models, including single-task train-
ing, outperform the DeepAPI model. Among the CodeTrans models, those trained using
only multi-task learning performed the most poorly mainly because of this task’s large
data-set. However, with increasing multi-task learning model size, it started to reach a
close/better performance than single-task learning and transfer learning. The CodeTrans
large model with multi-task learning ﬁne-tuning has the highest scores across all the models.
The CodeTrans transfer learning large model also has a similarly good performance.

5.6 Program Synthesis

We used accuracy when evaluating the Program Synthesis task. This accuracy calculates
whether our model output is precisely the same as the golden reference. Seq2Tree used
the code accuracy to count how many of the model’s outputs can pass the code tests. If
the model output is identical to the reference, this output could pass the code tests. As
shown in Table 5, nine out of ten CodeTrans models outperform the Seq2Tree model. The
CodeTrans multi-task learning ﬁne-tuning small model achieves the best score on accuracy.
For multi-task learning, the performance increases along with the model size. However, for
transfer learning and multi-task ﬁne-tuning, smaller models perform better. Generally, this
task’s scores are very high, which means that this is an easy task with similar validation
and test sets, and bigger models may be easy to be overﬁtted.

6. Reﬂection

We discuss our results and provides a conclusion in this section. We also list our future
work here.

6.1 Discussion

Our CodeTrans models with the transformer architecture with both encoder and decoder
outperform the thirteen sub-tasks’ baseline models. This proves the eﬀectiveness of the
transformer encoder-decoder architecture for these tasks in the software engineering do-
main, mainly when we utilize transfer learning and multi-task learning methods with self-
supervised learning. Nevertheless, the models’ performance varies a bit when using diﬀerent
training strategies for diﬀerent sizes of models on diﬀerent datasets.

16

CodeTrans: Transformer for Source Code

(a) The Code Comment Generation task’s
training dataset has 470,486 samples.

(b) The Source Code Summarization - SQL
task’s training dataset has 22,492 samples.

Figure 1: The evaluation of multi-task learning checkpoints on the validation set for two
tasks. The x-axis lists the training steps. The y-axis is the T5 built-in BLEU score. Diﬀerent
colors indicate diﬀerent sizes of models.

We noticed that the model size plays an essential role in the model’s performance.
For single-task learning, the larger the dataset is, the fewer training steps a bigger model
requires. A bigger model reaches a lower loss under the same batch size and the same eval-
uation steps when applying the multi-task learning or transfer learning strategy. Although
the pre-training may cost more time for bigger models, they need fewer iteration steps dur-
ing ﬁne-tuning for each task than the small models. As a result, for most of the tasks, the
bigger the model is, the better the evaluation scores the model could achieve with even less
ﬁne-tuning time.

The evaluation results also prove that transfer learning and multi-task learning
with ﬁne-tuning strategies outperform the models that only used single-task learning
on all the tasks. The performance of models using transfer learning is very similar to
those using multi-task learning ﬁne-tuning. It is hard to say which one is better. However,
transfer learning does not require the task dataset to be involved in the pre-training steps.
For a new task, the dataset only needs to be trained for relatively few ﬁne-tuning steps,
while multi-task learning with ﬁne-tuning needs the new task dataset during pre-training.
We can clearly say that transfer learning would save time and provide better results for a
new task when ﬁne-tuned on a pre-trained model checkpoint.

The performance of multi-task learning depends heavily on the data size and at-
tributes of the task itself. For large datasets like the datasets for the task Code Comment
Generation and API Sequence Recommendation, multi-task learning models are even worse
than the models that only applied single-task learning as shown in Table 5. Figure 1a shows
that the model’s performance improves signiﬁcantly when we increase the model size for
the Code Comment Generation task with a large dataset. Half a million multi-task training
steps are not enough for this task, even using the large model. When the dataset is tiny
and easy to be overﬁtted, small multi-task models could achieve the best result while
bigger setups do not always lead to better performance. This is due to the higher proba-
bility of overﬁtting the small dataset when more parameters are used, even when we used
regularization methods like a dropout (10% in all our models). Figure 1b shows that the

17

Elnaggar, Ding, Matthes, Rost, Jones, Gibbs, Feher, Angerer, and Severini

Model

Size

Model Output

CodeTrans
Single-Task Learning

Small mysql : how to get the diﬀerence of a column in a table ?
Base

how do i get the average of a date range in sql server 2005 ?

CodeTrans
Transfer Learning

CodeTrans
Multi-Task Learning

how to get the time in milliseconds since the start time of the transaction was taken ?
how to get current date time in sql server ?

Small
Base
Large mysql time ( ) function

how to get the time in mysql ?
Small
Base
how can i get the time of a date in mysql ?
Large how to convert datetime to time in mysql ?

CodeTrans
Multi-Task Learning Fine-tuning

Small
Base
Large

how to get the correct time from mysql database ?
how to convert date to time in mysql ?
select time from mysql table

Code Snippet as Input

select time ( col0 ) from tab0

Golden Reference

datetime implementation in php mysql

Table 6: The models’ output for an example of the task Source Code Summarization. We
compared diﬀerent CodeTrans model outputs and the golden reference for the input SQL
code “select time ( col0 ) from tab0”. The golden reference is the one extracted from the
StackOverﬂow.

base model performs better overall than the small models for the source code summariza-
tion - SQL task, but the large model has several overlaps with the base model. The large
model has a sign of overﬁtting after 120,000 training steps, and the model’s performance
decreases after that point.

Table 6 lists each CodeTrans model’s outputs compared with the golden reference ex-
tracted from the StackOverﬂow. The input for the models is “select time ( col0 ) from tab0”.
We can observe that all the models’ outputs are readable sentences. The majority of them
have a question format. Because the dataset contains questions and answers from Stack-
Overﬂow, the models have learned how to ask questions. Outputs from single-task learning
models do not make much sense. The transfer learning and multi-task learning outputs
all notice that this code is about time. All the multi-task learning models also speciﬁed
the mysql database system. The CodeTrans multi-task learning large model mentions the
keyword datetime, which also appears in the golden reference. Besides, the transfer learn-
ing and multi-task learning ﬁne-tuning base models have reasonable outputs as well. The
CodeTrans transfer learning and multi-task learning ﬁne-tuning models focus more on the
code function and structure to summarize this code snippet. In total, our judgement for the
models’ performances matches the ranking of our evaluation metrics. For more examples,
we chosen one example from each task and list all the models’ output for this example in
the Appendix.

Moreover, most of the Code Documentation Generation tasks achieved the best evalu-
ation performance when using the multi-task learning strategy. It is possible that we have
two more self-supervised tasks from the same CodeSearchNet corpus during the multi-task
learning. These give more similar samples for the supervised Code Documentation Genera-
tion tasks so that the model would focus on performing better for these tasks. Furthermore,
using diﬀerent types of tasks during multi-task learning eﬃciently avoids overﬁtting.

18

CodeTrans: Transformer for Source Code

6.2 Conclusion

This paper explores the CodeTrans models with transformer encoder-decoder architecture
on six main tasks and, in total, thirteen sub-tasks in the software engineering domain
covering nine programming languages. We have carried out experiments with diﬀerent
training strategies, including single-task learning, transfer learning, multi-task learning, and
multi-task learning with ﬁne-tuning. We applied diﬀerent models’ sizes based on the Text-
To-Text Transfer Transformer framework by utilizing NVIDIA GPUs and Google Cloud
TPUs.

Our CodeTrans models outperform all the baseline models and achieve the state of the
art over all the tasks. Our experiments on various tasks provide us with many insights about
training a neural network model on software engineering-relevant tasks. First, we ﬁnd that
larger models can bring a better model performance. Second, models with transfer learning
perform as well as models with multi-task learning ﬁne-tuning, and the pre-training models
can be ﬁne-tuned on the new downstream tasks eﬃciently while saving a signiﬁcant amount
of training time. Moreover, multi-task learning is very beneﬁcial for the small dataset on
which the model will overﬁt easily.
It is very promising that these experiences can be
generalized for training natural language processing tasks on diﬀerent domains.

In addition to these ﬁndings, we published our models on the Hugging Face Hub so
that everyone can access our models and use them for their purposes. We also provided
online the pre-trained checkpoints generated from our CodeTrans during transfer learning
pre-training and multi-task learning. These checkpoints are suitable for ﬁne-tuning other
software engineering tasks if the task’s programming language is covered in this paper.

6.3 Future Work

When working on the Code Documentation Generation tasks, we have noticed that a pro-
gramming language function has two aspects inﬂuencing the model performance: the func-
tion names/parameter names and the code structure. A well-named function would lower
the diﬃculty for the model to generate the documentation. Further investigation about
functions with disguised parameter names or function names would be valuable. In this
work, we considered a function as a sentence; from this aspect, we do not fully make use of
the code structure, so how to present the code is also a good research point. Experiments
about ﬁnding the best way to present the features of code structure can be carried out.

We preprocessed the datasets by parsing and tokenizing the programming codes using
diﬀerent Python libraries for each programming language. So when using our models,
applying the same preprocessing way would draw the best results. Nevertheless, not every
user is a programming expert, and the preprocessing increases the complexity for users to get
the best model performance. It would be meaningful to examine the eﬀect of preprocessing
for the software engineering tasks and train models with good performance, but without
preprocessing like parsing and tokenizing.

Moreover, more tasks can be explored using transformer encoder-decoder architecture.
It would be interesting to examine our models’ performance on the unseen programming
languages as Feng et al. (2020) did in their experiments. Furthermore, testing the pre-
trained language models on human language tasks like SQuAD dataset Rajpurkar et al.

19

Elnaggar, Ding, Matthes, Rost, Jones, Gibbs, Feher, Angerer, and Severini

(2016) could be of interest. Evaluation could be run directly on similar tasks with an
unseen programming language using the multi-task learning CodeTrans models.

Acknowledgments

The authors primarily thank Tim Karl (TUM) and Jian Kong (TUM) for their invaluable
help with hardware and software; Inga Weise and Aline Schmidt (both TUM) for support
with many other aspects of this work. Thanks for invaluable support and feedback from
NVIDIA, in particular to Ulrich Michaelis, Ada Sedova, Geetika Gupta, Axel Koehler,
Frederic Pariente, Jonathan Lefman, and Thomas Bradley. From Google, we would like to
deeply thank Jamie Kinney, Alex Schroeder, Nicole DeSantis, Andrew Stein, Vishal Mishra,
Eleazar Ortiz, Nora Limbourg, Cristian Mezzanotte, and all TFRC Team for their invaluable
support to set up our projects on Google Cloud and solve all the related Google TPU and
servers issues. All CodeTrans models could not be easily publicly accessible without the
amazing support from the Hugging Face team; that is why we are very grateful to Patrick
von Platen, Julien Chaumond, and Cl´ement Delangue from Hugging Face. Last, but not
least, thanks to all those who deposit their experimental data in public databases, and to
those who maintain these databases.

This work was supported by a grant from Software Campus through the German Min-
istry for Research and Education (BMBF: Bundesministerium fuer Bildung und Forschung).
We gratefully acknowledge the support of NVIDIA Corporation with the donation of two
Titan GPU used for this research development phase. We also want to thank LRZ (Leibniz
Rechenzentrum) for providing us access to DGX-1(V100) for the testing phase.

Finally and most importantly, this research used resources of TPUs under TensorFlow
Research Cloud grant. Furthermore, Rostlab acknowledge support from Google Cloud and
Google Cloud Research Credits program to fund this project under Covid19 grants.

20

CodeTrans: Transformer for Source Code

Appendix Model Output Examples for each Task.

For each sub-task we list one example containing all the models’ output, the input, and the
standard reference in this Appendix. These examples are all randomly selected from the
supervised tasks test datasets.

Appendix A. Code Documentation Generation

A.1 Code Documentation Generation — Python

Model

Size

Model Output

CodeTrans
Single-Task Learning

Small Prints an error and exits with an optional exit code .
Base

Prints the e .

CodeTrans
Transfer Learning

CodeTrans
Multi-Task Learning

Small Prints a message and exits with the speciﬁed exit code .
Base
Large Print an error message and exit .

Prints an error message and exits .

Small Prints a message and exits if exit code is not None .
Base
Large Print an error message and exit .

Prints an error message and exits with the given exit code .

CodeTrans
Multi-Task Learning Fine-Tuning

Small Print a message and exit .
Base
Large Print error message and exit .

Prints a message to the log and exits with an optional exit code .

Code Snippet as Input

def e ( message , exit code = None ) : print log ( message , YELLOW , BOLD )
if exit code is not None : sys . exit ( exit code )

Golden Reference

Print an error log message .

A.2 Code Documentation Generation — Java

Model

Size

Model Output

CodeTrans
Single-Task Learning

CodeTrans
Transfer Learning

CodeTrans
Multi-Task Learning

Small Casts a Cast to another type
Base

Casts a value into a function that returns the same type as the supplied parameter .

Small Casts the given class to a function .
Casts a function to a class
Base
Large Cast a value to the given target type .

Small Casts the given class to the given target class .
Base
Large Creates a function that performs a cast to the given class .

Returns a function that casts the given value to the given type .

CodeTrans
Multi-Task Learning Fine-Tuning

Small Creates a function that casts the given object to the target type .
Base
Returns a function that casts the given object to the given class .
Large Returns a function that casts its input type to the target type .

Code Snippet as Input

Golden Reference

public static ¡ T , U ¿ Function ¡ T , U ¿ castFunction ( Class ¡ U ¿ target )
{ return new CastToClass ¡ T , U ¿ ( target ) ; }
Returns a function that cast the incoming values via a Class object .

21

Elnaggar, Ding, Matthes, Rost, Jones, Gibbs, Feher, Angerer, and Severini

A.3 Code Documentation Generation — Go

Model

Size

Model Output

CodeTrans
Single-Task Learning

Small
Base

needSnapshotAbort returns true if we need to roll a snapshot .
needSnapshotAbort returns true if we need to call snapshot and false otherwise .

CodeTrans
Transfer Learning

CodeTrans
Multi-Task Learning

needSnapshotAbort returns true if the Progress . Match ﬁeld of ProgressState == ProgressStateSnapshot and pr . Match ¡ pr . PendingSnapshot .
needSnapshotAbort returns true if the progress is in a snapshot and the match exceeds the pending snapshot .

Small
Base
Large needsnapshotabort returns true if we need to abort a snapshot .

needSnapshotAbort returns true if the current state is a snapshot .
needSnapshotAbort returns true if we need to abort the snapshot .

Small
Base
Large needsnapshotabort returns true if we need to abort a snapshot .

CodeTrans
Multi-Task Learning Fine-Tuning

needSnapshotAbort returns true if the progress needs to be aborted .
Small
Base
needSnapshotAbort returns true if we need to abort the snapshot
Large needsnapshotabort returns true if we need to abort the snapshot .

Code Snippet as Input

Golden Reference

func ( pr * Progress ) needSnapshotAbort ( ) bool { return pr . State == ProgressStateSnapshot && pr . Match ¿= pr . PendingSnapshot }
needSnapshotAbort returns true if snapshot progress s Match is equal or higher than the pendingSnapshot .

A.4 Code Documentation Generation — Php

Model

Size

Model Output

CodeTrans
Single-Task Learning

Small Update table with speciﬁc data
Call when table is constructed
Base

CodeTrans
Transfer Learning

CodeTrans
Multi-Task Learning

CodeTrans
Multi-Task Learning Fine-Tuning

Code Snippet as Input

Golden Reference

Small Updates a table .
Base
Large Updates a table .

Calls the updateTable method to update a table .

Small Update a table .
Base
Updates a table .
Large updates a database table .

Small Update a table .
Base
Updates a table .
Large updates a database table .

public static function update ( $ table ) { if ( ! is array ( $ table ) )
{ $ table = json decode ( $ table , true ) ; } if ( ! SchemaManager :: tableExists ( $ table [ ’oldName’ ] ) ) { throw SchemaException ::
tableDoesNotExist ( $ table [ ’oldName’ ] ) ; } $ updater = new self ( $ table ) ; $ updater -¿ updateTable ( ) ; } }
Update the table .

A.5 Code Documentation Generation — Ruby

Model

Size

Model Output

CodeTrans
Single-Task Learning

Small Log a message at the given level if the logger is present
Base Writes a log message if the current log level is at or below the supplied severity .

CodeTrans
Transfer Learning

CodeTrans
Multi-Task Learning

Small Create an IO object at the given level with the given block . This method will not be called directly if the IO object at that level .
Base Writes a message of the given severity to the log .
Large Writes a message to the log device .

Small Add a message to the log .
Base
Large Add a message to the log .

Add a message to the logger .

CodeTrans
Multi-Task Learning Fine-Tuning

Small Writes a message to the log with a given severity .
Base We need to overload the add method . Basibally it is the same as the original one but we add our own log format to it .
Large Writes a message if the severity is high enough . This method is executed asynchronously .

Code Snippet as Input

Golden Reference

def add ( severity , progname , & block ) return true if io . nil? —— severity ¡ level message =
format message ( severity , progname , yield ) MUTEX . synchronize { io . write ( message ) } true end
Add a message to the logger with the given severity and progname .

22

CodeTrans: Transformer for Source Code

A.6 Code Documentation Generation — Javascript

Model

Size

Model Output

CodeTrans
Single-Task Learning

Small Returns true if the browser is a native element .
Base

Returns whether the givenEnv should be focused .

CodeTrans
Transfer Learning

CodeTrans
Multi-Task Learning

Small Checks if the current browser is on a standard browser environment .
Base
Large Check if the environment is standard browser .

Check if browser environment is a standard browser environment

Small Returns true if the browser environment is a standard browser environment .
Base
Large Determines if the current environment is a standard browser environment

Checks if the current browser environment is a standard browser environment .

CodeTrans
Multi-Task Learning Fine-Tuning

Standard browser environment has a notion of what React Native does not support it .
Check if the browserEnv is standard .

Small
Base
Large Checks if the browser is in a standard environment .

Code Snippet as Input

Golden Reference

function isStandardBrowserEnv ( ) { if ( typeof navigator !== ’undeﬁned’ && ( navigator . product === ’ReactNative’ —— navigator . product
=== ’NativeScript’ —— navigator . product === ’NS’ ) ) { return false ; } return ( typeof window !== ’undeﬁned’ && typeof document !== ’undeﬁned’ ) ; }
Determine if we are running in a standard browser environment

Appendix B. Source Code Summarization

B.1 Source Code Summarization — Python

Model

Size

Model Output

CodeTrans
Single-Task Learning

Small Python : Compare ﬁles containing a folder
Base

How to copy a text ﬁle into a variable ?

CodeTrans
Transfer Learning

CodeTrans
Multi-Task Learning

includes within a line ’ with python 2.7
Python : Include speciﬁc lines in a ﬁle

Small
Base
Large Changing a line in a text ﬁle

Small How to write a string to a ﬁle in Python ?
Base
Large How to remove a line from a ﬁle in python ?

How to write a text ﬁle to a text ﬁle in Python ?

CodeTrans
Multi-Task Learning Fine-Tuning

Small How to include text in a ﬁle using Python ?
Base
Python : ’Include this text in a ﬁle ’
Large How to remove empty string in python

Code Snippet as Input

with open ( CODE STRING , CODE STRING ) as in ﬁle : buf = in ﬁle . readlines ( ) with open ( CODE STRING , CODE STRING )
as out ﬁle : for line in buf : if line == ” ; Include this text ” : line = line + ” Include below ” out ﬁle . write ( line ) ’

Golden Reference

Adding lines after speciﬁc line

B.2 Source Code Summarization — CSharp

Model

Size

Model Output

CodeTrans
Single-Task Learning

Small How to convert C # DateTime to C #
Base

How to convert DateTime to DateTime format in C # ?

CodeTrans
Transfer Learning

CodeTrans
Multi-Task Learning

Small How to convert double to DateTime in C # ?
Base
Convert double to DateTime
Large How to convert double to datetime ?

Small How to convert unix timestamp to local time in C # ?
Base
Large How to convert double to datetime in C # ?

How to convert a double to a DateTime in C # ?

CodeTrans
Multi-Task Learning Fine-Tuning

Small How to convert double to DateTime in C # ?
Base
How to convert double to DateTime in C # ?
Large Convert double to datetime in C #

Code Snippet as Input

Golden Reference

public static DateTime ParseUnixDateTime ( double unixTime ) { var dt = new DateTime ( CODE INTEGER
CODE INTEGER , CODE INTEGER , CODE INTEGER , CODE INTEGER , CODE INTEGER , CODE INTEGER ,
System . DateTimeKind . Utc ) ; dt = dt . AddSeconds ( unixTimeStamp ) . ToLocalTime ( ) ; return dt ; }
C # Display compare timestamp to current month

23

Elnaggar, Ding, Matthes, Rost, Jones, Gibbs, Feher, Angerer, and Severini

B.3 Source Code Summarization — SQL

Model

Size

Model Output

CodeTrans
Single-Task Learning

Small mysql : how to get the diﬀerence of a column in a table ?
Base

how do i get the average of a date range in sql server 2005 ?

CodeTrans
Transfer Learning

CodeTrans
Multi-Task Learning

how to get the time in milliseconds since the start time of the transaction was taken ?
how to get current date time in sql server ?

Small
Base
Large mysql time ( ) function

how to get the time in mysql ?
Small
Base
how can i get the time of a date in mysql ?
Large how to convert datetime to time in mysql ?

CodeTrans
Multi-Task Learning Fine-Tuning

Small
Base
Large

how to get the correct time from mysql database ?
how to convert date to time in mysql ?
select time from mysql table

Code Snippet as Input

select time ( col0 ) from tab0

Golden Reference

datetime implementation in php mysql

Appendix C. Code Comment Generation

Model

Size

Model Output

CodeTrans
Single-Task Learning

Small Renders the URI into its textual representation .
Base

Renders the URI .

CodeTrans
Transfer Learning

CodeTrans
Multi-Task Learning

Small Renders the given URI into a string . Override this if you want to do something special to convert the URI into ASCII .
Base
Large Renders the given URI using the encoding scheme in ISO-8859-1 .

Render a URI

Small Renders a URI .
Base
Renders the given URI into a string .
Large Renders the URI as an ASCII string .

CodeTrans
Multi-Task Learning Fine-Tuning

Small Render the URI as ASCII string .
Base
Large Render the given URI .

Renders a URI as a string , escaping special characters .

Code Snippet as Input

protected String renderUri ( URI uri ) return uri . toASCIIString ( ) ;

Golden Reference

Render the URI as a string

Appendix D. Git Commit Message Generation

Model

Size

Model Output

CodeTrans
Single-Task Learning

Small Added missing Jungtaeks to log4j . jar
Base

Added missing jar ﬁle

CodeTrans
Transfer Learning

CodeTrans
Multi-Task Learning

Small Adding missing joscar jar
Base
Large

Added joscar . jar to the lib directory of the Gateway project
added missing joscar jar

add joscar jar
Add joscar jar

Small
Base
Large Add joscar

CodeTrans
Multi-Task Learning Fine-Tuning

Small Adding missing jar ﬁle
Base
Added joscar jar
Large Started work on the icq / aim protocol provider implementation

Code Snippet as Input

new ﬁle mode 100644 index 000000000 . . 892fda21b Binary ﬁles / dev / null and b / src / plugins / gateway / lib / joscar . jar diﬀer

Golden Reference

Added joscar JAR

24

CodeTrans: Transformer for Source Code

Appendix E. API Sequence Generation

Model

Size

Model Output

CodeTrans
Single-Task Learning

Small Color . getRed Color . getGreen Color . getBlue
Color . getRed Color . getGreen Color . getBlue
Base

CodeTrans
Transfer Learning

CodeTrans
Multi-Task Learning

Small Color . getRed Color . getGreen Color . getBlue
Base Math . max Math . min
Large Math . max Math . min

Small Math . min Math . max
Base
Large Renders Math . round Math . max Math . round Math . max Math . round

Color . getRed Color . getGreen Color . getBlue

CodeTrans
Multi-Task Learning Fine-Tuning

Small Color . getRed Color . getGreen Color . getBlue
Base
Color . getRed Color . getGreen Color . getBlue
Large Color . getRed Color . getGreen Color . getBlue

Code Snippet as Input

Convert from normal rgb to java hsb

Golden Reference

Color.RGBtoHSB Color.getHSBColor

Appendix F. Program Synthesis

All the CodeTrans models for this task receive more than 90% accuracy, because all the
samples in the tasks have very similar questions and answers in the train, validation, and
test dataset. Therefore, the models could give exactly the same outputs as the reference.

Model

Size

Model Output

CodeTrans
Single-Task Learning

CodeTrans
Transfer Learning

CodeTrans
Multi-Task Learning

CodeTrans
Multi-Task Learning Fine-Tuning

Small
Base

Small
Base
Large

Small
Base
Large

Small
Base
Large

[ map a [ partial1 b - ] ]
[ map a [ partial1 b - ] ]

[ map a [ partial1 b - ] ]
[ map a [ partial1 b - ] ]
[ map a [ partial1 b - ] ]

[ map a [ partial1 b - ] ]
[ map a [ partial1 b - ] ]
[ map a [ partial1 b - ] ]

[ map a [ partial1 b - ] ]
[ map a [ partial1 b - ] ]
[ map a [ partial1 b - ] ]

Code Snippet as Input

you are given an array of numbers a and a number b , compute the diﬀerence of elements in a and b

Golden Reference

[ map a [ partial1 b - ] ]

References

Steven Bird. Nltk: the natural language toolkit. In Proceedings of the COLING/ACL 2006

Interactive Presentation Sessions, pages 69–72, 2006.

Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.

25

Elnaggar, Ding, Matthes, Rost, Jones, Gibbs, Feher, Angerer, and Severini

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn,
and Tony Robinson. One billion word benchmark for measuring progress in statistical
language modeling. arXiv preprint arXiv:1312.3005, 2013.

Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi
Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using
rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078,
2014.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-
training of deep bidirectional transformers for language understanding. arXiv preprint
arXiv:1810.04805, 2018.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun
Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for pro-
gramming and natural languages. arXiv preprint arXiv:2002.08155, 2020.

Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim. Deep api learning. In
Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations
of Software Engineering, pages 631–642, 2016.

Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation,

9(8):1735–1780, 1997.

Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. Deep code comment generation. In 2018
IEEE/ACM 26th International Conference on Program Comprehension (ICPC), pages
200–20010. IEEE, 2018.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt.
Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint
arXiv:1909.09436, 2019.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Summarizing source
code using a neural attention model. In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pages 2073–2083,
2016.

Siyuan Jiang, Ameer Armaly, and Collin McMillan. Automatically generating commit mes-
sages from diﬀs using neural machine translation. In 2017 32nd IEEE/ACM International
Conference on Automated Software Engineering (ASE), pages 135–146. IEEE, 2017.

Stefan Kombrink, Tom´aˇs Mikolov, Martin Karaﬁ´at, and Luk´aˇs Burget. Recurrent neural
network based language modeling in meeting recognition. In Twelfth annual conference
of the international speech communication association, 2011.

Taku Kudo. Subword regularization: Improving neural network translation models with

multiple subword candidates. arXiv preprint arXiv:1804.10959, 2018.

Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, and Guillaume Lample. Unsu-
pervised translation of programming languages. arXiv preprint arXiv:2006.03511, 2020.

26

CodeTrans: Transformer for Source Code

Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
Radu Soricut. Albert: A lite bert for self-supervised learning of language representations.
arXiv preprint arXiv:1909.11942, 2019.

Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summa-

rization branches out, pages 74–81, 2004.

Chin-Yew Lin and Franz Josef Och. Orange: a method for evaluating automatic evaluation
metrics for machine translation. In COLING 2004: Proceedings of the 20th International
Conference on Computational Linguistics, pages 501–507, 2004.

Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural
networks for natural language understanding. arXiv preprint arXiv:1901.11504, 2019a.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized
bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019b.

Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Eﬀective approaches to
attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.

Vadim Markovtsev and Waren Long. Public git archive: A big code dataset for all.

In
Proceedings of the 15th International Conference on Mining Software Repositories, pages
34–37, 2018.

Terence Parr. The deﬁnitive ANTLR 4 reference. Pragmatic Bookshelf, 2013.

Illia Polosukhin and Alexander Skidanov. Neural program search: Solving programming

tasks from description and examples. arXiv preprint arXiv:1802.04335, 2018.

Matt Post. A call for clarity in reporting bleu scores. arXiv preprint arXiv:1804.08771,

2018.

Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning
with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21(140):
1–67, 2020. URL http://jmlr.org/papers/v21/20-074.html.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+
questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.

Veselin Raychev, Pavol Bielik, and Martin Vechev. Probabilistic model for code with deci-

sion trees. ACM SIGPLAN Notices, 51(10):731–747, 2016.

Winston W Royce. Managing the development of large software systems: concepts and
techniques. In Proceedings of the 9th international conference on Software Engineering,
pages 328–338, 1987.

Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint

arXiv:1706.05098, 2017.

27

Elnaggar, Ding, Matthes, Rost, Jones, Gibbs, Feher, Angerer, and Severini

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
In Advances

Gomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.
in neural information processing systems, pages 5998–6008, 2017.

Cody Allen Watson. Deep Learning in Software Engineering. PhD thesis, The College of

William and Mary, 2020.

Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant,
Aditya Barua, and Colin Raﬀel. mt5: A massively multilingual pre-trained text-to-text
transformer. arXiv preprint arXiv:2010.11934, 2020.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and
Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding.
In Advances in neural information processing systems, pages 5753–5763, 2019.

Ziyu Yao, Daniel S Weld, Wei-Peng Chen, and Huan Sun. Staqc: A systematically mined
question-code dataset from stack overﬂow. In Proceedings of the 2018 World Wide Web
Conference on World Wide Web, pages 1693–1703. International World Wide Web Con-
ferences Steering Committee, 2018.

28

