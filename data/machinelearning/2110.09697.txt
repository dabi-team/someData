2
2
0
2

n
u
J

7
1

]
L
M

.
t
a
t
s
[

2
v
7
9
6
9
0
.
0
1
1
2
:
v
i
X
r
a

Abess‚ÄîFast Best-Subset Selection

abess: A Fast Best-Subset Selection Library in Python and R

Jin Zhu1
Xueqin Wang2
Liyuan Hu1
Junhao Huang1
Kangkang Jiang1
Yanhang Zhang3
Shiyun Lin4
Junxian Zhu5
1 Department of Statistical Science, Sun Yat-Sen University, Guangzhou, GD, China
2 Department of Statistics and Finance/International Institute of Finance, School of Management,
University of Science and Technology of China, Hefei, Anhui, China
3 School of Statistics, Renmin University of China, Beijing, China
4 Center for Statistical Science, Peking University, Beijing, China
5 Saw Swee Hock School of Public Health, National University of Singapore, Singapore

zhuj37@mail2.sysu.edu.cn
wangxq20@ustc.edu.cn
huly5@mail2.sysu.edu.cn
huangjh256@mail2.sysu.edu.cn
jiangkk3@mail2.sysu.edu.cn
zhangyh98@ruc.edu.cn
shiyunlin@stu.pku.edu.cn
junxian@nus.edu.sg

Editor:

Abstract
We introduce a new library named abess that implements a uniÔ¨Åed framework of best-subset
selection for solving diverse machine learning problems, e.g., linear regression, classiÔ¨Åca-
tion, and principal component analysis. Particularly, abess certiÔ¨Åably gets the optimal
solution within polynomial time with high probability under the linear model. Our eÔ¨É-
cient implementation allows abess to attain the solution of best-subset selection problems
as fast as or even 20x faster than existing competing variable (model) selection toolboxes.
Furthermore, it supports common variants like best subset of groups selection and (cid:96)2 reg-
ularized best-subset selection. The core of the library is programmed in C++. For ease of
use, a Python library is designed for convenient integration with scikit-learn, and it can be
installed from the Python Package Index (PyPI). In addition, a user-friendly R library is
available at the Comprehensive R Archive Network (CRAN). The source code is available
at: https://github.com/abess-team/abess.
Keywords: best-subset selection, high-dimensional data, splicing technique

1. Introduction

Best-subset selection (BSS) is imperative in machine learning and statistics. It aims to Ô¨Ånd
a minimally adequate subset of variables to accurately Ô¨Åt the data, naturally reÔ¨Çecting the
Occam‚Äôs razor principle of simplicity. Nowadays, the BSS also has far-reaching applica-
tions in every facet of research like medicine and biology because of the surge of large-scale
datasets across a variety of Ô¨Åelds. As a benchmark optimization problem in machine learn-
ing and statistics, the BSS is also well-known as an NP-hard problem (Natarajan, 1995).
However, recent progress shows that the BSS can be eÔ¨Éciently solved (Huang et al., 2018;
Zhu et al., 2020; G¬¥omez and Prokopyev, 2021). Especially, the ABESS algorithm using a

*. Jin Zhu and Liyuan Hu contributed equally. Xueqin Wang is the corresponding author.

1

 
 
 
 
 
 
Zhu, Wang, Hu, Huang, Jiang, Zhang, Lin and Zhu

splicing technique Ô¨Ånds the best subset under the classical linear model in polynomial time
with high probability (Zhu et al., 2020), making itself even more attractive to practitioners.
We present a new library named abess that implements a uniÔ¨Åed toolkit based on the
splicing technique proposed by Zhu et al. (2020). The supported solvers in abess are sum-
marized in Table 1. Furthermore, our implementation improves computational eÔ¨Éciency
by warm-start initialization, sparse matrix support, and OpenMP parallelism. abess can
run on most Linux distributions, Windows 32 or 64-bit, and macOS with Python (version
‚â• 3.6) or R (version ‚â• 3.1.0), and can be easily installed from PyPI1 and CRAN2. abess
provides complete documentation3, where the API reference presents the syntax and the
tutorial presents comprehensible examples for new users. It relies on Github Actions4 for
continuous integration. The PEP8/tidyverse style guide keeps the source Python/R code
clean. Code quality is assessed by standard code coverage metrics (Myers et al., 2011).
The coverages for the Python and the R packages at the time of writing are 97% and 96%,
respectively. The source code is distributed under the GPL-3 license.

Learning

Target

Solver (Reference)

Supervised
(model the
variation of
response y)

y ‚àà R
y ‚àà {0, 1}
y ‚àà {0, 1, 2, . . .}
y ‚àà (0, ‚àû)

LinearRegression (Zhu et al., 2020)
LogisticRegression (Hosmer et al., 1989)
PoissonRegression (Vincent and Claire, 2010)
GammaRegression (Vincent and Claire, 2010)

y ‚àà {type1, type2, . . .} MultinomialRegression (Krishnapuram et al., 2005)
y ‚àà {level1, level2, . . .}
y ‚àà R √ó {0, 1}
y ‚àà Rd

OrdinalRegression (Wurm et al., 2021)
CoxPHSurvivalAnalysis (P¬®olsterl, 2020)
MultiTaskRegression (Zhang and Yang, 2017)

Unsupervised

Dimension reduction
Matrix decomposition

SparsePCA (d‚ÄôAspremont et al., 2008)
RobustPCA (Cai et al., 2019)

Table 1: The supported best-subset selection solvers. PCA: principal component analysis.

2. Architecture

Figure 1 shows the architecture of abess, and each building block will be described as fol-
lows. The Data class accepts the (sparse) tabular data from Python and R interfaces,
and returns an object containing the predictors that are (optionally) screened (Fan and Lv,
2008) or normalized. The Algorithm class implements the generic splicing technique for
the BSS with the additional support for group-structure predictors (Zhang et al., 2021),
(cid:96)2-regularization for parameters (Bertsimas and Parys, 2020), and nuisance selection (Sun
and Zhang, 2021). The concrete algorithms are programmed as subclasses of Algorithm
by rewriting the virtual function interfaces of class Algorithm. Seven implemented BSS
tasks are presented in Figure 1. Beyond that, the modularized design facilitates users to
extend the library to various machine learning tasks by writing a subclass of Algorithm.
The Metric class assesses the estimation returned by the Algorithm class by the cross
validation or information criteria like the Akaike information criterion and the high dimen-
sional Bayesian information criterion (Akaike, 1998; Wang et al., 2013). Python and R
interfaces collect and process the results of the Algorithm and Metric classes. The abess
Python library is compatible with scikit-learn (Pedregosa et al., 2011). For each solver (e.g.,

1. https://pypi.org/project/abess/
2. https://cran.r-project.org/web/packages/abess
3. https://abess.readthedocs.io and https://abess-team.github.io/abess/
4. https://github.com/abess-team/abess/actions

2

Abess‚ÄîFast Best-Subset Selection

Figure 1: abess software architecture.

LinearRegression) in abess, Python users can not only use a familiar scikit-learn API to train
the model but also easily create a scikit-learn pipeline including the model. In the R library,
S3 methods are programmed such that generic functions (like print, coef, and plot) can be
directly used to attain the BSS results and visualize solution paths or tuning value curves.

3. Usage Examples

Figure 2 shows that the abess R library exactly selects the eÔ¨Äective variables and accurately
estimates the coeÔ¨Écients. Figure 3 illustrates the integration of the abess Python interface
with scikit-learn‚Äôs modules to build a non-linear model for diagnosing malignant tumors.
The output of the code reports the information of the polynomial features for the selected
model among candidates, and its corresponding area under the curve (AUC), which is 0.966,
indicating the selected model would have an admirable contribution in practice.

1 library (abess)
2 dat <‚àí g enerate.data(n = 300, p = 1000, beta = c(3, ‚àí2, 0, 0, 2, rep(0, 995)))
3 best est <‚àí ex tract(abess(dat $x , dat $ y, family = "g aussian "))
4 cat("S elected subset: ", best est $ supp ort.vars,
5
6 ## S elected subset: x1 x2 x5 and coefficient estimation: 2.96 ‚àí2.05 1.9

"and coefficient estimation: ", round(best est $ supp ort.beta, digits = 2))

Figure 2: Using the abess R library on a synthetic data set to demonstrate its optimality.
The data set comes from a linear model with the true sparse coeÔ¨Écients given by beta.

4. Performance

We compare abess with popular variable selection libraries in Python and R through re-
gression, classiÔ¨Åcation, and PCA. The libraries include: scikit-learn (a benchmark Python
library for machine learning), celer (a fast Python solver for (cid:96)1-regularization optimization,
Massias et al. (2018, 2020)), and elasticnet (a elastic-net R solver for sparse PCA, Zou et al.
(2006)). All computations are conducted on a Ubuntu platform with Intel(R) Core(TM)
i9-9940X CPU @ 3.30GHz and 48 RAM. Python version is 3.9.1 and R version is 3.6.3.

3

ùëåùëã1‚ãØùëãùíëAlgorithmR & Python InterfacesConcrete AlgorithmsLinearLogisticPoissonCoxMulti-taskMulti-classificationWarm-startinitializationDataNormalizeMetricCrossValidationÔºàOpenMPÔºâInformationcriterionPrincipalComponentAnalysisSupport set: ùëã1,ùëã3Estimation:  1.6, 3.8Support size: 2‚Ä¶‚Ä¶Optimal estimatorScreening‚Ñì2regularizationGroup selection(sparse) Inputscikit-learn compatible API GenericsplicingNuisance selectionSolution pathsTuning curveC++ Implementation‚Ä¶‚Ä¶Zhu, Wang, Hu, Huang, Jiang, Zhang, Lin and Zhu

(‚Äôlogreg ‚Äô, LogisticRegression ())])

1 from abess . linear import LogisticRegression
2 from sklearn . datasets import load breast cancer
3 from sklearn . pipeline import Pipeline
4 from sklearn . metrics import make scorer , roc auc score
5 from sklearn . preprocessing import PolynomialFeatures
6 from sklearn . model selection import GridSearchCV
7 # combine feature transform and model:
8 pipe = Pipeline ([( ‚Äôpoly ‚Äô, PolynomialFeatures ( include bias = False)),
9
10 param grid = { ‚Äô poly interaction only ‚Äô:[True , False], ‚Äô poly degree ‚Äô:[1, 2, 3]}
11 # Use cross validation to tune parameters :
12 scorer = make scorer ( roc auc score , greater is better =True)
13 grid search = GridSearchCV (pipe , param grid , scoring =scorer , cv =5)
14 # load and fitting example data set:
15 X, y = load breast cancer ( return X y =True)
16 grid search .fit(X, y)
17 # print the best tuning parameter and associated AUC score:
18 print ([ grid search . best params , grid search . best score ])
19 # >>> [{ ‚Äô poly degree ‚Äô: 2, ‚Äô poly interaction only ‚Äô: True } , 0.9663829492654472]

Figure 3: Example of using the abess Python library with scikit-learn.

Table 2 displays the regression and classiÔ¨Åcation analyse results, suggesting abess derives
parsimonious models that achieve competitive performance in few minutes. Particularly, for
the cancer data set, it is more than 20x faster than scikit-learn ((cid:96)1). The results of the sparse
PCA (SPCA) are demonstrated in Table 3. Compared with elasticnet, abess consumes less
than a tenth of its runtime but explains more variance under the same sparsity level.

Library

Version

Superconductivity
(3895 √ó 85400)

Cancer
(118 √ó 22215)

Musk
(7074 √ó 166)

MSE

NNZ

Runtime AUC

NNZ

Runtime AUC

NNZ

Runtime

scikit-learn ((cid:96)1)
celer
scikit-learn ((cid:96)0)
abess

1.0.0
0.6.1
1.0.0
0.4.5

33.56
88.58
(cid:19)
41.72

1126.70
30.00
(cid:19)
81.50

1043.96
173.25
(cid:19)
110.41

0.92
0.91
(cid:55)
0.96

366.55
20.65
(cid:55)
1.00

62.16
26.24
(cid:55)
2.91

0.97
0.97
(cid:55)
0.97

165.40
162.15
(cid:55)
155.25

53.63
2.00
(cid:55)
140.68

Table 2: Average performance on the superconductivity data set (for regression), the can-
cer and the musk data sets (for classiÔ¨Åcation) (Chin et al., 2006; Dua and GraÔ¨Ä, 2017;
Hamidieh, 2018) based on 20 randomly drawn test sets. NNZ: the number of non-zero
elements. Runtime is measured in seconds. scikit-learn ((cid:96)1): LassoCV (for regression) and
LogisticRegressionCV (for classiÔ¨Åcation). celer: LassoCV (for regression) and LogisticRegres-
sion (for classiÔ¨Åcation). scikit-learn ((cid:96)0): OrthogonalMatchingPursuit (for regression). (cid:55): not
available. (cid:19): memory overÔ¨Çow.

Sparsity
Library

5
elasticnet

Explained variance
Runtime (seconds)

1.37
15.87

abess

2.28
1.06

10
elasticnet

1.61
15.77

abess

3.03
1.07

20
elasticnet

2.00
85.54

abess

3.88
1.05

Table 3: Performance of the SPCA when 5, 10, 20 elements in the loading vector of the
Ô¨Årst principal component are non-zero. The data set has 217 observations, where each
observation has 1,413 genetic factors (Christensen et al., 2009). elasticnet: version 1.3.0.

5. Conclusion
abess is a fast and comprehensive library for solving various BSS problems with statistical
guarantees. It oÔ¨Äers user-friendly interfaces for both Python and R users, and seamlessly in-
tegrates with existing ecosystems. Therefore, the abess library is a potentially indispensable
toolbox for machine learning and related applications. Future versions of abess intend to
support other important machine learning tasks, and adapt to advanced machine learning
pipelines in Python and R (Lang et al., 2019; Feurer et al., 2021; Binder et al., 2021).

4

Abess‚ÄîFast Best-Subset Selection

Acknowledgments

We would like to thank three reviewers for their constructive suggestions and valuable
comments, which have substantially improved this article and the abess library. Wang‚Äôs
research is partially supported by NSFC (72171216, 71921001, 71991474), The Key Re-
search and Development Program of Guangdong, China (2019B020228001), and Science
and Technology Program of Guangzhou, China (202002030129). Zhu‚Äôs research is partially
supported by the Outstanding Graduate Student Innovation and Development Program of
Sun Yat-Sen University (19lgyjs64). Zhang‚Äôs research is supported by the Fundamental
Research Funds for the Central Universities, and the Research Funds of Renmin University
of China (22XNH161). We are grateful to UCI Machine Learning Repository for sharing
the superconductivity and musk data sets.

References

Hirotogu Akaike. Information theory and an extension of the maximum likelihood principle.

In Selected Papers of Hirotugu Akaike, pages 199‚Äì213. Springer, 1998.

Dimitris Bertsimas and Bart Van Parys. Sparse high-dimensional regression: Exact scalable
algorithms and phase transitions. The Annals of Statistics, 48(1):300 ‚Äì 323, 2020. doi:
10.1214/18-AOS1804. URL https://doi.org/10.1214/18-AOS1804.

Martin Binder, Florian PÔ¨Åsterer, Michel Lang, Lennart Schneider, Lars KotthoÔ¨Ä, and Bernd
Bischl. mlr3pipelines ‚Äì Ô¨Çexible machine learning pipelines in r. Journal of Machine
Learning Research, 22(184):1‚Äì7, 2021. URL http://jmlr.org/papers/v22/21-0281.html.

Hanqin Cai, Jianfeng Cai, and Ke Wei. Accelerated alternating projections for robust
principal component analysis. Journal of Machine Learning Research, 20(1):685‚Äì717,
2019.

Koei Chin, Sandy DeVries, Jane Fridlyand, Paul T Spellman, Ritu Roydasgupta, Wen-Lin
Kuo, Anna Lapuk, Richard M Neve, Zuwei Qian, Tom Ryder, Fanqing Chen, Heidi Feiler,
Taku Tokuyasu, Chris Kingsley, Shanaz Dairkee, Zhenhang Meng, Karen Chew, Daniel
Pinkel, Ajay Jain, Britt Marie Ljung, Laura Esserman, Donna G Albertson, Frederic M
Waldman, and Joe W Gray. Genomic and transcriptional aberrations linked to breast
cancer pathophysiologies. Cancer Cell, 10(6):529‚Äì541, December 2006.

Brock C Christensen, E Andres Houseman, Carmen J Marsit, Shichun Zheng, Margaret R
Wrensch, Joseph L Wiemels, Heather H Nelson, Margaret R Karagas, James F Padbury,
Raphael Bueno, David J Sugarbaker, Ru-Fang Yeh, John K Wiencke, and Karl T Kelsey.
Aging and Environmental Exposures Alter Tissue-SpeciÔ¨Åc DNA Methylation Dependent
upon CpG Island Context. PLOS Genetics, 5(8):e1000602, August 2009.

Alexandre d‚ÄôAspremont, Francis Bach, and Laurent El Ghaoui. Optimal solutions for sparse

principal component analysis. Journal of Machine Learning Research, 9(7), 2008.

Dheeru Dua and Casey GraÔ¨Ä. UCI machine learning repository, 2017. URL http://archive.

ics.uci.edu/ml.

5

Zhu, Wang, Hu, Huang, Jiang, Zhang, Lin and Zhu

Jianqing Fan and Jinchi Lv. Sure independence screening for ultrahigh dimensional feature
space. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(5):
849‚Äì911, 2008. doi: https://doi.org/10.1111/j.1467-9868.2008.00674.x.

Matthias Feurer, Jan N. van Rijn, Arlind Kadra, Pieter Gijsbers, Neeratyoy Mallik, Sahithya
Ravi, Andreas M¬®uller, Joaquin Vanschoren, and Frank Hutter. Openml-python: an
extensible python api for openml. Journal of Machine Learning Research, 22(100):1‚Äì5,
2021. URL http://jmlr.org/papers/v22/19-920.html.

Andr¬¥es G¬¥omez and Oleg A. Prokopyev. A mixed-integer fractional optimization approach
to best subset selection. INFORMS Journal on Computing, 33(2):551‚Äì565, 2021. doi:
10.1287/ijoc.2020.1031.

Kam Hamidieh. A data-driven statistical model for predicting the critical temperature of

a superconductor. Computational Materials Science, 154:346‚Äì354, 2018.

David W. Hosmer, Borko Jovanovic, and Stanley Lemeshow. Best subsets logistic regression.

Biometrics, 45(4):1265‚Äì1270, 1989. URL http://www.jstor.org/stable/2531779.

Jian Huang, Yuling Jiao, Yanyan Liu, and Xiliang Lu. A constructive approach to l0

penalized regression. Journal of Machine Learning Research, 19(1):403‚Äì439, 2018.

Balaji Krishnapuram, Lawrence Carin, Mario A. T. Figueiredo, and Alexander J.
fast algorithms and generalization
Hartemink. Sparse multinomial logistic regression:
bounds. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(6):957‚Äì
968, 2005. doi: 10.1109/TPAMI.2005.127.

Michel Lang, Martin Binder, Jakob Richter, Patrick Schratz, Florian PÔ¨Åsterer, Stefan Coors,
Quay Au, Giuseppe Casalicchio, Lars KotthoÔ¨Ä, and Bernd Bischl. mlr3: A modern object-
oriented machine learning framework in R. Journal of Open Source Software, dec 2019.
doi: 10.21105/joss.01903.

Mathurin Massias, Alexandre Gramfort, and Joseph Salmon. Celer: a fast solver for the
In International Conference on Machine Learning, vol-

lasso with dual extrapolation.
ume 80, pages 3321‚Äì3330, 2018.

Mathurin Massias, Samuel Vaiter, Alexandre Gramfort, and Joseph Salmon. Dual extrapo-
lation for sparse glms. Journal of Machine Learning Research, 21(234):1‚Äì33, 2020. URL
http://jmlr.org/papers/v21/19-587.html.

Glenford J Myers, Corey Sandler, and Tom Badgett. The Art of Software Testing. John

Wiley & Sons, 2011.

Balas Kausik Natarajan. Sparse approximate solutions to linear systems. SIAM Journal

on Computing, 24(2):227‚Äì234, 1995. doi: 10.1137/S0097539792240406.

Fabian Pedregosa, Ga¬®el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand
Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent
Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher,

6

Abess‚ÄîFast Best-Subset Selection

Matthieu Perrot, and ¬¥Edouard Duchesnay. Scikit-learn: Machine learning in python.
Journal of Machine Learning Research, 12(85):2825‚Äì2830, 2011. URL http://jmlr.org/
papers/v12/pedregosa11a.html.

Sebastian P¬®olsterl. scikit-survival: A library for time-to-event analysis built on top of scikit-
learn. Journal of Machine Learning Research, 21(212):1‚Äì6, 2020. URL http://jmlr.org/
papers/v21/20-729.html.

Qiang Sun and Heping Zhang. Targeted inference involving high-dimensional data using
nuisance penalized regression. Journal of the American Statistical Association, 116(535):
1472‚Äì1486, 2021. doi: 10.1080/01621459.2020.1737079.

Calcagno Vincent and de Mazancourt Claire. glmulti: An r package for easy automated
model selection with (generalized) linear models. Journal of Statistical Software, 34(12):
1‚Äì29, 2010. doi: 10.18637/jss.v034.i12.

Lan Wang, Yongdai Kim, and Runze Li. Calibrating nonconvex penalized regression in
ultra-high dimension. The Annals of Statistics, 41(5):2505 ‚Äì 2536, 2013. doi: 10.1214/
13-AOS1159.

Michael J. Wurm, Paul J. Rathouz, and Bret M. Hanlon. Regularized ordinal regression
and the ordinalnet r package. Journal of Statistical Software, 99(6):1‚Äì42, 2021. doi:
10.18637/jss.v099.i06.

Yanhang Zhang, Junxian Zhu, Jin Zhu, and Xueqin Wang. A splicing approach to best

subset of groups selection. arXiv preprint arXiv:2104.12576, 2021.

Yu Zhang and Qiang Yang. An overview of multi-task learning. National Science Review,

5(1):30‚Äì43, 09 2017. ISSN 2095-5138. doi: 10.1093/nsr/nwx105.

Junxian Zhu, Canhong Wen, Jin Zhu, Heping Zhang, and Xueqin Wang. A polynomial
algorithm for best-subset selection problem. Proceedings of the National Academy of
Sciences, 2020. doi: 10.1073/pnas.2014241117. URL https://www.pnas.org/doi/abs/10.
1073/pnas.2014241117.

Hui Zou, Trevor Hastie, and Robert Tibshirani.

Journal of Computational and Graphical Statistics, 15(2):265‚Äì286, 2006.

sis.
10.1198/106186006X113430. URL https://doi.org/10.1198/106186006X113430.

Sparse principal component analy-
doi:

7

