1
2
0
2

n
u
J

7
2

]

G
L
.
s
c
[

1
v
2
3
2
4
1
.
6
0
1
2
:
v
i
X
r
a

Preprint

DGL-LIFESCI: AN OPEN-SOURCE TOOLKIT FOR
DEEP LEARNING ON GRAPHS IN LIFE SCIENCE

Mufei Li 1, Jinjing Zhou 1, Jiajing Hu 2, Wenxuan Fan 3,
Yangkang Zhang 4, Yaxin Gu 3, George Karypis 5
1 AWS Shanghai AI Lab, 2 King’s College London,
3 East China University of Science and Technology,
4 Zhejiang University, 5 AWS AI

ABSTRACT

Graph neural networks (GNNs) constitute a class of deep learning methods for
graph data. They have wide applications in chemistry and biology, such as molec-
ular property prediction, reaction prediction and drug-target interaction prediction.
Despite the interest, GNN-based modeling is challenging as it requires graph data
pre-processing and modeling in addition to programming and deep learning. Here
we present DGL-LifeSci, an open-source package for deep learning on graphs
in life science. DGL-LifeSci is a python toolkit based on RDKit, PyTorch and
Deep Graph Library (DGL). DGL-LifeSci allows GNN-based modeling on cus-
tom datasets for molecular property prediction, reaction prediction and molecule
generation. With its command-line interfaces, users can perform modeling with-
out any background in programming and deep learning. We test the command-line
interfaces using standard benchmarks MoleculeNet, USPTO, and ZINC. Com-
pared with previous implementations, DGL-LifeSci achieves a speed up by up
to 6x. For modeling ﬂexibility, DGL-LifeSci provides well-optimized modules
for various stages of the modeling pipeline. In addition, DGL-LifeSci provides
pre-trained models for reproducing the test experiment results and applying mod-
els without training. The code is distributed under an Apache-2.0 License and is
freely accessible at https://github.com/awslabs/dgl-lifesci.

1

INTRODUCTION

A large amount of the chemical and biological data corresponds to attributed graphs, e.g., molecular
graphs, interaction networks and biological pathways. Many of the machine learning (ML) tasks
that arise in this domain can be formulated as learning tasks on graphs. For example, molecular
property prediction can be formulated as learning a mapping from molecular graphs to real numbers
(regression) or discrete values (classiﬁcation) [8]; molecule generation can be formulated as learn-
ing a distribution over molecular graphs [13]; reaction prediction can be formulated as learning a
mapping from one set of graphs (reactants) to another set of graphs (products) [6]. A representation
is a vector of a user-deﬁned dimensionality. Graph neural networks (GNNs) combine graph struc-
tures and features in representation learning and they have been one of the most popular approaches
for learning on graphs [31, 33]. GNNs have also attracted considerable attention in life science and
researchers have applied them to many different tasks [26, 34, 13, 6, 25, 8, 10, 22, 9, 12].

Despite signiﬁcant research, it is often challenging for experts in life science to use GNN-based
approaches. To unlock the power of GNNs requires clean interfaces for custom datasets and robust
and efﬁcient pipelines. This is because developing GNN pipelines by oneself requires a combined
skill set of programming, machine learning, and GNN modeling, which is time-consuming to obtain.
This calls for a set of ready-to-run programs, which should make little assumption about users’
background.

Prior efforts have greatly lowered the bar for GNN-based modeling in life science, but none of
them fully addresses the problem. DeepChem [18] is a package for deep learning in drug discovery,
materials science, quantum chemistry, and biology. While it implements several GNN models, it
still requires users to program. Chainer Chemistry [1] is a package for deep learning in biology

1

 
 
 
 
 
 
Preprint

Figure 1: Illustration of the message passing phase (left) and readout phase (right).

and chemistry, based on Chainer [27]. It only provides a command-line interface for GNN-based
regression on molecules and requires users to write code for other tasks. PiNN [21] implements a
GNN variant for predicting potential energy surfaces and physicochemical properties of molecules
and materials. It also requires users to program themselves.

Here we present a python toolkit named DGL-LifeSci. It provides high-quality and robust imple-
mentations of seven models for molecular property prediction, one model for molecule generation,
and one model for chemical reaction prediction. For all these models and tasks, there is an as-
sociated command-line script for predictions on custom datasets without writing a single line of
code. DGL-LifeSci also provides pre-trained models for all experiments. Compared with previous
implementations, it achieves a speedup by up to 6x.

In the following, we ﬁrst provide a high-level overview of how graph neural networks over molecules
work. Then, we discuss the implementation and package features of DGL-LifeSci. After that, we
present the results of evaluating DGL-LifeSci interms of robustness and efﬁciency. Finally, we
conclude with a discussion on future work.

2 GRAPH NEURAL NETWORKS OVER MOLECULES

GNNs perform graph-based representation learning by combining information from the topology of
a graph and the features associated with its nodes and edges. They iteratively update the representa-
tion of a node by aggregating representations of its neighbors. As the number of iterations increases,
the nodes gain information from an increasingly larger local subgraph.

When applying GNNs to molecules as in molecular property prediction, there are two phases – a
message passing phase and a readout phase. Figure 1 is an illustration of them.

Message Passing Phase. The message passing phase updates node representations simultaneously
across the entire graph and consists of multiple rounds of message passing. In a round of message
passing, the representation of a node is updated by applying learnable functions to its original repre-
sentation, the representations of its adjacent nodes and the representations of its incident edges. The
operation is similar to gathering messages from adjacent nodes. By performing k rounds of message
passing, we can aggregate information from all the nodes/edges that are within k hops from each
node.

Readout Phase. The readout phase computes a representation for the entire graph. This representa-
tion is computed by applying a potentially learnable function to the representations of all the nodes
in the graph, e.g., summation over them. Once we obtain graph representations, we can pass them
to a multilayer perceptron (MLP) for ﬁnal prediction.

3 PACKAGE FEATURES

DGL-LifeSci contains four components: (i) a set of ready-to-run scripts for training and inference;
(ii) programming APIs for allowing researchers to develop their own custom pipelines and models;
(iii) a set of pre-trained models that can either be ﬁne-tuned or directly used to perform inference;
(iv) a set of built-in datasets for quick experimentation.

2

Preprint

Figure 2: An overview of modules in DGL-LifeSci and their usage.

It provides models that can be used to solve three tasks. The ﬁrst task is molecular property predic-
tion or quantitative structure activity relationship (QSAR) prediction. This can be formulated as a
regression or classiﬁcation task for single molecules. The second task is molecule generation. The
third task is chemical reaction prediction.

Usage. DGL-LifeSci provides command-line scripts for each task. They are responsible for in-
voking the modeling pipeline, which handles model training and model evaluation. Users need to
prepare their data in a standard format. They can then use the command-line interface by speci-
fying the path to the data ﬁle along with some additional arguments. For example, below is the
command-line interface for regression and classiﬁcation problems in molecular property prediction.
Users need to prepare molecules in the form of SMILES strings with their properties to predict in a
CSV ﬁle.

p y t h o n r e g r e s s i o n t r a i n . py −c
p y t h o n c l a s s i f i c a t i o n t r a i n . py −c

f i l e − s c h e a d e r −mo model

f i l e − s c h e a d e r −mo model

DGL-LifeSci also provides an optional support for hyperparameter search other than using the de-
fault ones. It uses Bayesian optimization based on hyperopt [4] for hyperparameter search.

4

IMPLEMENTATION

Dependencies. DGL-LifeSci is developed using PyTorch [16] and Deep Graph Library (DGL) [29].
PyTorch is a general-purpose deep learning framework and DGL is a high-performant GNN library.
In addition, it uses RDKit [19] for utilities related to cheminformatics.

Modeling Pipeline and Modules. A general GNN-based modeling pipeline consists of three stages:
dataset preparation, model initialization and model training. The dataset preparation stage involves
data loading, graph construction, representation initialization for nodes and edges (graph featuriza-
tion) and dataset interface construction. The model training stage involves model update, metric
computation and early stopping. As presented in ﬁgure 2, DGL-LifeSci is modularized for these
various stages and stage components so as to cater to the need of different uses. While DGL-LifeSci
allows users to perform GNN-based modeling without programming, advanced users can also adapt
these modules for their own development.

Dataset Preparation. DGL-LifeSci provides dataset interfaces for supporting both built-in datasets
and custom datasets. The interfaces are responsible for loading raw data ﬁles and invoking graph
construction and featurization.

Graph construction and featurization are two important steps for GNN-speciﬁc data preparation.
DGL-LifeSci provides built-in support for constructing three kinds of graphs for molecules – molec-
ular graphs, distance-based graphs, and complete graphs. In all these graphs, each node corresponds
to an atom in a molecule. In a molecular graph, the edges correspond to chemical bonds in the

3

Preprint

Table 1: Descriptors for feature initialization.

Descriptors

Atom type
Atom degree excluding hydrogen atoms
Atom degree including hydrogen atoms
Atom explicit valence
Atom implicit valence
Atom hybridization
Total number of hydrogen atoms attached
Atom formal charge
Number of radical electrons of an atom
Whether an atom is aromatic
Whether an atom is in a ring
Atom chiral tag
Atom chirality type
Atom mass
Whether an atom is chiral center

Possible values

C, N, O, etc
Non-negative integers
Non-negative integers
Non-negative integers
Non-negative integers
S, SP, SP2, SP3, SP3D, SP3D2
Non-negative integers
Integers
Non-negative integers
1 (True), 0 (False)
1 (True), 0 (False)
CW, CCW, unspeciﬁed, other
R, S
Non-negative real numbers
1 (True), 0 (False)

Bond type
Whether a bond is conjugated
Whether a bond is in a ring
Stereo conﬁguration of a bond
Direction of a bond

single, double, triple, aromatic
1 (True), 0 (False)
1 (True), 0 (False)
none, any, OZ, OE, CIS, TRANS
none, end-up-right, end-down-right

For non-numeric discrete-valued descriptors, one-hot encoding is used in featuriza-
tion. For numeric discrete-valued descriptors, either raw number or one-hot encod-
ing can be used in featurization.

molecule. The construction of a distance-based graph requires a molecule conformation and there
is an edge between a pair of atoms if the distance between them is within a cutoff distance. In a
complete graph, every pair of atoms is connected. For graph featurization, DGL-LifeSci allows ini-
tializing various node and edge features from atom and bond descriptors. Table 1 gives an overview
of them.

Users can split the dataset into training/validation/test subsets or do so for k-fold cross validation.
DGL-LifeSci provides built-in support for random split, scaffold split, weight split and stratiﬁed
split [30]. The random split performs a pure random split of a dataset. The scaffold split separates
structurally different molecules into different subsets based on their Bemis-Murcko scaffolds [3].
The weight split sorts molecules based on their weight and then splits them in order. The stratiﬁed
split sorts molecules based on their label and ensures that each subset contains nearly the full range
of provided labels.

Models Included. Table 2 lists the models implemented. GCN and GAT are two popular GNNs
initially developed for node classiﬁcation. We extend them for graph regression/classiﬁcation with
a readout function and an MLP. NF and Weave are among the earliest models that extend rule-based
molecular ﬁngerprints with graph neural networks. MPNN uniﬁes multiple GNNs for quantum
chemistry. AttentiveFP extends GAT with gated recurrent units [5].

One difﬁculty in developing learning-based approaches for molecular property prediction is the gap
between an extremely large chemical space and extremely limited labels for molecular properties.
It is estimated that the number of drug-like molecules is between 1023 and 1060 while most datasets
have less than tens of thousands of molecules in MoleculeNet [17, 24, 20, 7, 30]. Hu et al. [11]
propose to approach this problem by utilizing millions of unlabeled molecules in pre-training the
weights of a GIN model for general molecule representations. One can then ﬁne-tune the model
weights for predicting particular properties. We include four pre-trained models from their work
in DGL-LifeSci. The models were pre-trained with a same strategy for supervised learning and a
different strategy for self-supervised learning. We distinguish the models by the associated strategy
for self-supervised learning, which are context prediction, deep graph infomax, edge prediction and
attribute masking.

4

Preprint

Task

Table 2: Models Implemented.

Model

Molecular property prediction GCN [15], GAT [28], NF [8], Weave [14], MPNN [? ], AttentiveFP [32]

Molecule generation
Reaction prediction

GIN + context prediction/deep graph infomax/
edge prediction/attribute masking [11]
JTVAE [13]
WLN [6]

Table 3: Descriptors Considered in DeepChem Featurization.

Descriptors

Atom type (one-hot encoding)

Atom degree excluding hydrogen atoms (one-hot encoding)
Atom implicit valence (one-hot encoding)
Atom formal charge
Number of radical electrons of an atom
Whether an atom is aromatic
Atom hybridization (one-hot encoding)
Total number of hydrogen atoms attached (one-hot encoding)

Bond type (one-hot encoding)
Whether a bond is conjugated
Whether a bond is in a ring
Stereo conﬁguration of a bond (one-hot encoding)

Possible values

C, N, O, S, F, Si, P, Cl, Br, Mg, Na, Ca,
Fe, As, Al, I, B, V K, Tl, Yb, Sb, Sn,
Ag, Pd, Co, Se, Ti, Zn, H, Li, Ge, Cu,
Au, Ni, Cd, In, Mn, Zr, Cr, Pt, Hg, Pb
0 - 10
0 - 6
Integers
Non-negative integers
1 (True), 0 (False)
SP, SP2, SP3, SP3D, SP3D2
0 - 4

single, double, triple, aromatic
1 (True), 0 (False)
1 (True), 0 (False)
none, any, OZ, OE, CIS, TRANS

JTVAE is an autoencoder that utilizes both a junction tree and a molecular graph for the intermediate
representation of a molecule. WLN is a two-stage model for chemical reaction prediction. It ﬁrst
identiﬁes potential bond changes and then enumerates and ranks candidate products.

5 MODELING PERFORMANCE

Molecular Property Prediction. We test against six binary classiﬁcation datasets in MoleculeNet
and evaluate the model performance by ROC-AUC averaged over all tasks [30]. To evaluate the
model performance on unseen structures, we employ the scaffold split and use respectively 80%,
10% and 10% of the dataset for training, validation, and test. We train six models (NF, GCN, GAT,
Weave, MPNN, AttentiveFP) from scratch using the featurization proposed in DeepChem, which is
described in table 3. GCN and GAT take initial node features only and they do not take initial edge
features. We also ﬁne-tuned the four pre-trained GIN models. For a non-GNN baseline model, we
train an MLP taking Extended-Connectivity Fingerprints (ECFPs).

For all the settings, we perform a hyperparameter search for 32 trials. Within each trial, we train a
randomly initialized model and perform an early stopping if the validation performance no longer
improves for 30 epochs. Finally, we evaluate the model achieving the best validation performance
across all epochs and trials on the test set. Table 4 presents the summary of the test performance.
For a reference, we also include the ﬁne-tuning performance reported previously[11].

Reaction Prediction. We test WLN against USPTO [2] dataset following the setting in the original
work [6]. WLN is a two-stage model for reaction prediction. The ﬁrst stage identiﬁes candidate
reaction centers, i.e. pairs of atoms that lose or form a bond in the reaction. The second stage
enumerates candidate products from the candidate reaction centers and ranks them. We achieve
comparable performance for both stages as in table 5.

Molecule Generation. We test JTVAE against a ZINC [23] subset for reconstructing input
molecules [13]. We achieve an accuracy of 76.4% while the authors’ released code achieve an
accuracy of 74.4%.

5

Preprint

Model

Table 4: Test ROC-AUC on 6 Datasets from MoleculeNet.
Tox21

ToxCast

BBBP

SIDER HIV BACE

GCN
GAT
NF
Weave
MPNN
AttentiveFP

Models trained from scratch

0.63
0.68
0.66
0.67
0.65
0.71

0.77
0.71
0.75
0.56
0.70
0.70

0.62
0.64
0.60
0.62
0.59
0.57

Non-GNN baseline

0.58
0.52
0.53
0.58
0.54
0.53

0.76
0.76
0.74
0.73
0.74
0.75

0.84
0.84
0.80
0.79
0.85
0.73

MLP + ECFP

0.67

0.70

0.58

0.63

0.76

0.80

Pre-trained models ﬁne-tuned

GIN + context prediction
GIN + deep graph infomax
GIN + edge prediction
GIN + attribute masking

0.63
0.72
0.70
0.72

0.75
0.78
0.80
0.75

0.64
0.59
0.59
0.58

Previously reported results

GIN + context prediction
GIN + deep graph infomax
GIN + edge prediction
GIN + attribute masking

0.69
0.68
0.67
0.67

0.78
0.78
0.78
0.78

0.66
0.65
0.67
0.65

0.61
0.63
0.66
0.58

0.63
0.61
0.63
0.64

0.77
0.76
0.72
0.75

0.80
0.78
0.78
0.77

0.86
0.71
0.86
0.74

0.85
0.80
0.79
0.80

Table 5: Test Top-k Accuracy (%) of WLN on USPTO.

Implementations

Reaction center prediction

Candidate ranking

Top 6

Top 8

Top 10

Top 1

Top 2

Top 3

Top 5

Original
DGL-LifeSci

89.8
91.2

92.0
93.8

93.3
95.0

85.6
85.6

90.5
90.0

92.8
91.7

93.4
92.9

6 TRAINING SPEED

We compare the modeling efﬁciency of DGL-LifeSci against previous implementations, including
original implementations and DeepChem. All experiments record the averaged training time of one
epoch. The testbed is one AWS EC2 p3.2xlarge instance (one NVidia V100 GPU with 16GB GPU
RAM and 8 VCPUs).

7 CONCLUSIONS

Here, we present DGL-LifeSci, an open-source Python toolkit for deep learning on graphs in life
science. In the current version of DGL-LifeSci, we support GNN-based modeling for molecular
property prediction, reaction prediction and molecule generation.

With command-line interfaces, users can perform efﬁcient modeling on custom datasets without
programming a single line of code. Advanced users can also adapt highly modularized building
blocks for their own development.

In the current implementations of DGL-LifeSci, the primary focus is on small molecules. In the
future, we aim to extend the support to other graphs in life science like proteins and biological
networks. This will open up a much richer set of tasks in life science.

6

Preprint

Experiment

NF
AttentiveFP

Table 6: Epoch Training Time in Seconds.

Dataset

Previous implementation DGL-LifeSci

Speedup

Molecular property prediction

HIV
Aromaticity [32]

5.8 (DeepChem 2.3.0)
6.0

2.5
1.0

2.3x
6.0x

Reaction prediction

WLN for reaction center prediction

USPTO

JTVAE

ZINC subset

Molecule generation

11657

44666

2315

5.0x

44843

1.0x

8 DATA AND SOFTWARE AVAILABILITY

The datasets and models are publicly available at https://github.com/awslabs/dgl-lifesci. The scripts
for reproducing the experiments are available in the following examples.

• Molecular property prediction: examples/property prediction/moleculenet
• Reaction prediction: examples/reaction prediction/rexgen direct
• Molecule generation: examples/generative models/jtvae

REFERENCES

[1] Chainer chemistry: A library for deep learning in biology and chemistry. https://

github.com/chainer/chainer-chemistry. [Online; accessed 29-October-2020].

[2] Patent

reaction extraction:

downloads.

https://bitbucket.org/dan2097/

patent-reaction-extraction/downloads, 2014.

[3] G. W. Bemis and M. A. Murcko. The properties of known drugs. 1. molecular frameworks.

Journal of Medicinal Chemistry, 1996.

[4] J. Bergstra, D. Yamins, and D. D. Cox. Making a science of model search: Hyperparameter
optimization in hundreds of dimensions for vision architectures. In Proceedings of the 30th
International Conference on Machine Learning, pages I–115–I–123, 2012.

[5] K. Cho, B. van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Ben-
gio. Learning phrase representations using RNN encoder–decoder for statistical machine trans-
lation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1724–1734, 2014.

[6] C. Coley, W. Jin, L. Rogers, T. F. Jamison, T. S. Jaakkola, W. H. Green, R. Barzilay, and K. F.
Jensen. A graph-convolutional neural network model for the prediction of chemical reactivity.
Chem. Sci., 10:370–377, 2019.

[7] C. M. Dobson. Chemical space and biology. Nature, 432:824–828, 2004.
[8] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel, A. Aspuru-Guzik,
and R. P. Adams. Convolutional networks on graphs for learning molecular ﬁngerprints. In
Advances in Neural Information Processing Systems 28, pages 2224–2232. 2015.

[9] E. N. Feinberg, D. Sur, Z. Wu, B. E. Husic, H. Mai, Y. Li, S. Sun, J. Yang, B. Ramsundar,
and V. S. Pande. Potentialnet for molecular property prediction. ACS Central Science, 4(11):
1520–1530, 2018.

[10] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for
quantum chemistry. volume 70 of Proceedings of Machine Learning Research, pages 1263–
1272. PMLR, 2017.

[11] W. Hu, B. Liu, J. Gomes, M. Zitnik, P. Liang, V. Pande, and J. Leskovec. Strategies for pre-
In International Conference on Learning Representations,

training graph neural networks.
2020.

7

Preprint

[12] J. Ingraham, V. Garg, R. Barzilay, and T. Jaakkola. Generative models for graph-based protein
design. In Advances in Neural Information Processing Systems 32, pages 15820–15831. 2019.

[13] W. Jin, R. Barzilay, and T. Jaakkola. Junction tree variational autoencoder for molecular graph
generation. volume 80 of Proceedings of Machine Learning Research, pages 2323–2332, 2018.

[14] S. Kearnes, K. McCloskey, M. Berndl, V. Pande, and P. Riley. Molecular graph convolu-
tions: moving beyond ﬁngerprints. Journal of Computer-Aided Molecular Design, 30:595–
608, 2016.

[15] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks.

In International Conference on Learning Representations, 2017.

[16] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Te-
jani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative
style, high-performance deep learning library. In Advances in Neural Information Processing
Systems 32, pages 8026–8037. 2019.

[17] P. G. Polishchuk, T. I. Madzhidov, and A. Varnek. Estimation of the size of drug-like chemical
space based on gdb-17 data. Journal of Computer-Aided Molecular Design, 27:675–679, 2013.
[18] B. Ramsundar, P. Eastman, P. Walters, and V. Pande. Deep Learning for the Life Sciences.

O’Reilly Media, Inc., 2019.

[19] RDKit, online. RDKit: Open-source cheminformatics. http://www.rdkit.org. [On-

line; accessed 30-October-2020].

[20] J.-L. Reymond. The chemical space project. Accounts of Chemical Research, 48(3):722–730,

2015.

[21] Y. Shao, M. Hellstr¨om, P. D. Mitev, L. Knijff, and C. Zhang. Pinn: A python library for
building atomic neural networks of molecules and materials. Journal of Chemical Information
and Modeling, 60(3):1184–1193, 2020.

[22] Z. Shui and G. Karypis. Heterogeneous molecular graph neural networks for predicting
molecule properties. In 2020 IEEE International Conference on Data Mining (ICDM), pages
492–500. 2020.

[23] T. Sterling and J. J. Irwin. Zinc 15 – ligand discovery for everyone. Journal of Chemical

Information and Modeling, 55(11):2324–2337, 2015.

[24] D. B. K. Steve O’Hagan. Analysing and navigating natural products space for generating small,

diverse, but representative chemical libraries. Biotechnology Journal, 13, 2018.

[25] J. M. Stokes, K. Yang, K. Swanson, W. Jin, A. Cubillos-Ruiz, N. M. Donghia, C. R. MacNair,
S. French, L. A. Carfrae, Z. Bloom-Ackermann, V. M. Tran, A. Chiappino-Pepe, A. H. Badran,
I. W. Andrews, E. J. Chory, G. M. Church, E. D. Brown, T. S. Jaakkola, R. Barzilay, and J. J.
Collins. A deep learning approach to antibiotic discovery. Cell, 180(4):688 – 702.e13, 2020.

[26] M. Sun, S. Zhao, C. Gilvary, O. Elemento, J. Zhou, and F. Wang. Graph convolutional networks
for computational drug development and discovery. Brieﬁngs in Bioinformatics, 21(3):919–
935, 06 2019.

[27] S. Tokui, K. Oono, S. Hido, and J. Clayton. Chainer: a next-generation open source framework

for deep learning. In Workshop on Systems for ML at NeurIPS, 2015.

[28] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Li`o, and Y. Bengio. Graph attention

networks. In International Conference on Learning Representations, 2018.

[29] M. Wang, D. Zheng, Z. Ye, Q. Gan, M. Li, X. Song, J. Zhou, C. Ma, L. Yu, Y. Gai, T. Xiao,
T. He, G. Karypis, J. Li, and Z. Zhang. Deep graph library: A graph-centric, highly-performant
package for graph neural networks. arXiv preprint arXiv:1909.01315, 2020.

[30] Z. Wu, B. Ramsundar, E. N. Feinberg, J. Gomes, C. Geniesse, A. S. Pappu, K. Leswing, and
V. Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):
513–530, 2018.

[31] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu. A comprehensive survey on graph
neural networks. IEEE Transactions on Neural Networks and Learning Systems, pages 1–21,
mar 2020.

8

Preprint

[32] Z. Xiong, D. Wang, X. Liu, F. Zhong, X. Wan, X. Li, Z. Li, X. Luo, K. Chen, H. Jiang, and
M. Zheng. Pushing the boundaries of molecular representation for drug discovery with the
graph attention mechanism. Journal of Medical Chemistry, 2019.

[33] J. Zhou, G. Cui, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun. Graph neural

networks: A review of methods and applications, 2019.

[34] M. Zitnik, M. Agrawal, and J. Leskovec. Modeling polypharmacy side effects with graph

convolutional networks. Bioinformatics, 34(13):i457–i466, 06 2018.

9

