9
1
0
2

g
u
A
8
2

]

O
L
.
s
c
[

5
v
5
2
6
1
1
.
5
0
9
1
:
v
i
X
r
a

NIL: Learning Nonlinear Interpolants(cid:63)

Mingshuai Chen1,2((cid:66)), Jian Wang1,2, Jie An3, Bohua Zhan1,2((cid:66)),
Deepak Kapur4, and Naijun Zhan1,2((cid:66))

1 State Key Lab. of Computer Science, Institute of Software, CAS, Beijing, China
2 University of Chinese Academy of Sciences, Beijing, China
{chenms,bzhan,znj}@ios.ac.cn
3 School of Software Engineering, Tongji University, Shanghai, China
4 Department of Computer Science, University of New Mexico, Albuquerque, USA

Abstract. Nonlinear interpolants have been shown useful for the veriﬁcation of
programs and hybrid systems in contexts of theorem proving, model checking,
abstract interpretation, etc. The underlying synthesis problem, however, is chal-
lenging and existing methods have limitations on the form of formulae to be
interpolated. We leverage classiﬁcation techniques with space transformations
and kernel tricks as established in the realm of machine learning, and present
a counterexample-guided method named NIL for synthesizing polynomial inter-
polants, thereby yielding a uniﬁed framework tackling the interpolation problem
for the general quantiﬁer-free theory of nonlinear arithmetic, possibly involv-
ing transcendental functions. We prove the soundness of NIL and propose suf-
ﬁcient conditions under which NIL is guaranteed to converge, i.e., the derived
sequence of candidate interpolants converges to an actual interpolant, and is com-
plete, namely the algorithm terminates by producing an interpolant if there exists
one. The applicability and effectiveness of our technique are demonstrated exper-
imentally on a collection of representative benchmarks from the literature, where
in particular, our method sufﬁces to address more interpolation tasks, including
those with perturbations in parameters, and in many cases synthesizes simpler
interpolants compared with existing approaches.

Keywords: Nonlinear Craig interpolant · Counterexample-guided learning · Pro-
gram veriﬁcation · Support vector machines (SVMs)

1

Introduction

Interpolation-based technique provides a powerful mechanism for local and modular
reasoning, thereby improving scalability of various veriﬁcation techniques, e.g., theo-
rem proving, model checking and abstract interpretation, to name just a few. The study
of interpolation was pioneered by Kraj´ıˇcek [26] and Pudl´ak [34] in connection with
theorem proving, by McMillan [29] in the context of model checking, by Graf and
Sa¨ıdi [16], McMillan [30] and Henzinger et al. [19] pertaining to abstraction like CE-
GAR [7], and by Wang et al. [23] in the context of learning-based invariant generation.

(cid:63) This work has been supported through grants by NSFC under grant No. 61625206 and
61732001, by the CAS Pioneer Hundred Talents Program under grant No. Y9RC585036, and
by the National Science Foundation Award DMS-1217054.

 
 
 
 
 
 
2

M. Chen et al.

Developing efﬁcient algorithms for generating interpolants for various theories and their
combination has become an active research area, see e.g., [6, 24, 25, 30, 31, 35, 45].

Though established methods addressing interpolant generation for Presburger arith-
metic, decidable fragments of ﬁrst-order logic, theory of equality over uninterpreted
functions (EUFs) as well as their combination have been extensively studied in the lit-
erature, there appears to be little work on synthesizing nonlinear interpolants. Dai et
al. proposed an algorithm in [10] for generating interpolants for nonlinear polynomial
inequalities based on the existence of a witness guaranteed by Stengle’s Positivstellen-
satz [15] that can be computed using semi-deﬁnite programming (SDP). A major limi-
tation of this method is that the two mutually contradictory formulas to be interpolated
must share the same set of variables. Okudono et al. extended [10] in [32] to cater for the
so-called sharper and simpler interpolants by developing a continuous fraction-based
algorithm that rounds off numerical solutions. In [13], Gan et al. considered the inter-
polation for inequalities combined with EUFs by employing the hierarchical calculus
framework proposed in [37] (and its extension [38]), while the inequalities are limited
to be of the concave quadratic form. In [14], Gao and Zufferey transformed proof traces
from δ-complete decision procedures into interpolants, composed of Boolean combina-
tions of linear constraints, which can deal with certain transcendental functions beyond
polynomials. The techniques of encoding interpolants as logical combinations of linear
constraints, including [14], [27] and [36], however, yield potentially large interpolants
(requiring even an inﬁnite length in the worst case) and their usage thus becomes difﬁ-
cult in practical applications (cf. Example 1).

Interpolants can be viewed as classiﬁers that distinguish, in the context of program
veriﬁcation for instance, positive program states from negative ones (unreachable/error
states) and consequently the state-of-the-art classiﬁcation algorithms can be leveraged
for synthesizing interpolants. The universal applicability of classiﬁcation techniques
substantially extends the scope of theories admitting interpolant generation. This idea
was ﬁrst employed by Sharma et al. in [36], which infers linear interpolants through
hyperplane-classiﬁers generated by support vector machines (SVMs) [3,44] whilst han-
dles superﬁcial nonlinearities by assembling interpolants in the form purely of conjunc-
tions (or dually, disjunctions) of linear half-spaces, which addresses only a limited cat-
egory of formulae featuring nonlinearities. The learning-based paradigm has also been
exploited in the context of nonlinear constraint solving, see e.g., [11].

In this paper, we present a classiﬁcation-based learning method for the synthe-
sis of polynomial interpolants for the quantiﬁer-free theory of nonlinear arithmetic.
Our approach is based on techniques of space transformations and kernel tricks per-
tinent to SVMs that have been well-developed in the realm of machine learning. Our
method is described by an algorithm called NIL (and its several variants) that adopts the
counterexample-guided inductive synthesis framework [21, 39]. We prove the sound-
ness of NIL and propose sufﬁcient conditions under which NIL is guaranteed to con-
verge, that is, the derived sequence of classiﬁers (candidate interpolants) converges to
an actual interpolant, and is complete, i.e., if an interpolant exists, the method termi-
nates with an actual interpolant. In contrast to related work on generation of nonlin-
ear interpolants, which restrict the input formulae, our technique provides a uniform
framework, tackling the interpolation problem for the general quantiﬁer-free theory of

NIL: Learning Nonlinear Interpolants

3

nonlinear arithmetic, possibly involving transcendental functions. The applicability and
effectiveness of NIL are demonstrated experimentally on a collection of representative
benchmarks from the literature; as is evident from experimental results, our method is
able to address more demands on the nature of interpolants, including those with per-
turbations in parameters (due to the robustness inherited from SVMs); in many cases,
it synthesizes simpler interpolants compared with other approaches, as shown by the
following example.

Example 1 ( [14]). Consider two mutually contradictory inequalities φ (cid:98)= y ≥ x2 and
ψ (cid:98)= y ≤ − cos(x) + 0.8. Our NIL algorithm constructs a single polynomial inequality
I (cid:98)= 15x2 < 4 + 20y as the interpolant, namely, φ |= I and I ∧ ψ is unsatisﬁable; while
the interpolant generated by the approach in [14], only when provided with sufﬁciently
large ﬁnite domains, e.g., x ∈ [−π, π] and y ∈ [−0.2, π2], is y > 1.8 ∨ (0.59 ≤ y ≤
1.8 ∧ −1.35 ≤ x ≤ 1.35) ∨ (0.09 ≤ y < 0.59 ∧ −0.77 ≤ x ≤ 0.77) ∨ (y ≥ 0 ∧ −0.3 ≤
x ≤ 0.3). As will be discussed later, we do not need to provide a priori information to
our algorithm such as bounds on variables.

The rest of the paper is organized as follows. Sect. 2 introduces some preliminaries
on Craig interpolants and SVMs. In Sect. 3, we present the NIL algorithm dedicated to
synthesizing nonlinear interpolants, followed by the analysis of its soundness, condi-
tional completeness and convergence in Sect. 4. Sect. 5 reports several implementation
issues and experimental results on a collection of benchmarks (with the robustness dis-
cussed in Sect. 6). The paper is then concluded in Sect. 7.

2 Preliminaries

Let N, Q and R be the set of natural, rational and real numbers, respectively. We denote
by R[x] the polynomial ring over R with variables x = (x1, . . . , xn), and (cid:107)x(cid:107) denotes
the (cid:96)2-norm [4]. For a set X ⊆ Rn, its convex hull is denoted by conv(X). For x, x(cid:48) ∈
X, dist(x, x(cid:48)) = (cid:107)x − x(cid:48)(cid:107) denotes the Euclidean distance between two points, which
generalizes to dist(x, X (cid:48)) = minx(cid:48)∈X (cid:48) dist(x, x(cid:48)). Given δ ≥ 0, deﬁne B(x, δ) =
{x(cid:48) ∈ Rn|(cid:107)x(cid:48) − x(cid:107) ≤ δ} as the closed ball of radius δ centered at x. Consider the
quantiﬁer-free fragment of a ﬁrst-order theory of polynomials over the reals, denoted
by TP , in which a formula ϕ is of the form

ϕ (cid:98)= p(x) (cid:5) 0 | ϕ ∧ ϕ | ϕ ∨ ϕ | ¬ϕ
where p(x) ∈ R[x] and (cid:5) ∈ {<, >, ≤, ≥, =}. A natural extension of our method to
cater for more general nonlinearities involving transcendental functions will be demon-
strated in subsequent sections. In the sequel, we use ⊥ to stand for false and (cid:62) for true.
Let R[x]m consist of all polynomials p(x) of degree ≤ m ∈ N. We abuse the notation
ϕ ∈ R[x]m to abbreviate ϕ (cid:98)= p(x) (cid:5) 0 and p(x) ∈ R[x]m if no ambiguity arises.

Given formulas φ and ψ in a theory T , φ is valid w.r.t. T , written as |=T φ, iff φ is
true in all models of T ; φ entails ψ w.r.t. T , written as φ |=T ψ, iff every model of T
that makes φ true makes ψ also true; φ is satisﬁable w.r.t. T , iff there is a model of T in
which φ is true; otherwise unsatisﬁable. It follows that φ is unsatisﬁable iff φ |=T ⊥.
The set of all the models that make φ true is denoted by

φ

T .

(cid:74)

(cid:75)

4

M. Chen et al.

2.1 Craig Interpolant

Craig showed in [9] that given two formulas φ and ψ in a ﬁrst-order logic T s.t. φ |=T
ψ, there always exists an interpolant I over the common symbols of φ and ψ s.t. φ |=T I
and I |=T ψ. In the veriﬁcation literature, this terminology has been abused by [30],
which deﬁned an interpolant over the common symbols of φ and ψ as

Deﬁnition 1 (Interpolant). Given φ and ψ in a theory T s.t. φ ∧ ψ |=T ⊥, a formula
I is a (reverse) interpolant of φ and ψ if (i) φ |=T I; (ii) I ∧ ψ |=T ⊥; and (iii) I
contains only common symbols shared by φ and ψ.

It is immediately obvious that φ |=T ψ iff φ∧¬ψ |=T ⊥, namely, I is an interpolant
of φ and ψ iff I is a reverse interpolant in McMillan’s sense of φ and ¬ψ. We follow
McMillan in continuing to abuse the terminology.

2.2 Support Vector Machines

In machine learning, support vector machines [3,44] are supervised learning models for
effective classiﬁcation based on convex optimization. In a binary setting, we are given a
training dataset X = X + (cid:93) X − of n sample points {(x1, y1), (x2, y2), . . . , (xn, yn)},
where xi ∈ Rd, and yi is either 1, indicating a positive sample xi ∈ X +, or -1, indi-
cating a negative one in X −. The goal of classiﬁcation here is to ﬁnd a potential hyper-
plane (a.k.a. linear classiﬁer) to separate the positive samples from the negative ones.
There however might be various or even inﬁnite number of separating hyperplanes,
and an SVM aims to construct a separating hyperplane that yields the largest distance
(so-called functional margin) to the nearest positive and negative samples. Such a clas-
siﬁcation hyperplane is called the optimal-margin classiﬁer while the samples closest
to it are called the support vectors.
Linear SVMs. Assume that X + and X − are linearly separable, meaning that there
exists a linear separating hyperplane wTx + b = 0 such that yi(wTxi + b) > 0, for
all (xi, yi) ∈ X. Then the functional margin can be formulated as

γ (cid:98)= 2 min1≤i≤n 1/(cid:107)w(cid:107)|wTxi + b|.
Linear SVMs are committed to ﬁnding appropriate parameters (w, b) that maximize
the functional margin while adhering to the constraints of separability, which reduces
equivalently to the following convex quadratic optimization problem [2] that can be
efﬁciently solved by off-the-shelf packages for quadratic programming:

minimize
w,b

1
2

wTw subject to yi(wTxi + b) ≥ 1, i = 1, 2, . . . , n.

(1)

Lemma 1 (Correctness of SVMs [36]). Given positive samples X + which are linearly
separable from negative samples X −, SVMs produce, under computations of inﬁnite
precision, a half-space h s.t. ∀x ∈ X +. h(x) > 0 and ∀x ∈ X −. h(x) < 0.

Corollary 1 (Separation of Convex Hulls [1]). The half-space h in Lemma 1 satisﬁes
that ∀x ∈ conv(X +). h(x) > 0 and ∀x ∈ conv(X −). h(x) < 0.

NIL: Learning Nonlinear Interpolants

5

Φ

Fig. 1: Mapping from a two-dimensional input space into a three-dimensional feature space with linear separation thereof.

Nonlinear SVMs. When φ and ψ are formulas over nonlinear arithmetic, often after
sampling X, it is not possible to ﬁnd a linearly separable hyperplane in the common
variables. However, a nonlinear surface that can be described as a linear hyperplane
in the space of monomials of bounded degree may separate X + and X −. The above
construction is generalized by introducing a transformation from Rd to R ˜d, the vec-
tor space of monomials in the common variables up to some bounded degree, with
yi(wTxi + b) ≥ 1 in (1) replaced by yi(wTΦ(xi) + b) ≥ 1, where Φ is a linear expres-
sion in monomials in the common variables up to a bounded degree. Here, the vectors
Φ(x) span the feature space.

Consider the Lagrangian dual [3] of the modiﬁed optimization problem:

minimize
α

subject to

1
2
(cid:88)n

i=1

(cid:88)n

(cid:88)n

i=1

j=1

αiαjyiyjΦ(xi)TΦ(xj) −

(cid:88)n

i=1

αi

αiyi = 0, and αi ≥ 0 for i = 1, 2, . . . , n.

A kernel function κ : Rd × Rd (cid:55)→ R is deﬁned as κ(x, x(cid:48)) (cid:98)= Φ(x)TΦ(x(cid:48)). The intro-
duction of the dual problem and the kernel function [3] reduces the computational com-
plexity essentially from O( ˜d) down to O(d). For the sake of post-verifying a candidate
interpolant given by SVMs, we adopt an inhomogeneous polynomial kernel function of
the form

κ(x, x(cid:48)) (cid:98)= (βxTx(cid:48) + θ)m,
where m is the polynomial degree describing complexity of the feature space, θ ≥ 0
is a parameter trading off the inﬂuence of higher-order versus lower-order terms in the
polynomial, and β is a scalar parameter. Henceforth, the optimal-margin classiﬁer (if
there exists one) can be derived as wTΦ(x) = (cid:80)n
i=1 αiκ(xi, x) = 0, with xi being a
support vector iff αi > 0. In practice, usually a large amount of αis turn out to be zero
and this leads to a simple representation of a classiﬁer. Fig. 1 illustrates the intuitive idea
of the transformation from the original input space to the feature space. We will show
in the sequel that the resulting classiﬁer can be viewed as a candidate interpolant, while
its optimal-margin feature contributes to a certain “medium” logical strength of the
interpolant, which is thus robust to perturbations (in the feature space) in the formulae
to be interpolated.

3 Learning Interpolants

In this section, we present the NIL algorithm for synthesizing nontrivial (reverse) Craig
interpolants for the quantiﬁer-free theory of nonlinear arithmetic. It takes as input a

6

M. Chen et al.

pair (cid:104)φ, ψ(cid:105) of formulas in TP as well as a positive integer m, and aims to generate an
interpolant I of maximum degree m, i.e., I ∈ R[x]m, if it exists, such that φ |=TP I
and I ∧ ψ |=TP ⊥. Here, (cid:104)φ, ψ(cid:105) can be decorated as (cid:104)φ(x, y), ψ(x, z)(cid:105) with variables
involved in the predicates, and thus x denotes variables that are common to φ and ψ.
In the sequel, we drop the subscript TP in |=TP and
TP wherever the context is
unambiguous.

·
(cid:74)

(cid:75)

Due to the decidability of the ﬁrst-order theory of real-closed ﬁelds established by
Tarski [43], TP admits quantiﬁer elimination (QE). This means that the satisﬁability of
any formula in TP can be decided (in doubly exponential time in the number of vari-
ables for the worst case). If the formula is satisﬁable, models satisfying the formula can
also be constructed algorithmically (following the same time complexity). Though the
introduction of general forms of transcendental functions renders the underlying theory
undecidable, there does exist certain extension of TP with transcendental functions (in-
volving exponential functions, logarithms and trigonometric functions), e.g. that identi-
ﬁed by Strzebo´nski in [42] and references therein, which still admits QE. This allows a
straightforward extension of NIL to such a decidable fragment involving transcenden-
tal functions. Speciﬁcally, the decidability remains when the transcendental functions
involved are real univariate exp-log functions [40] or tame elementary functions [41]
which admit a real root isolation algorithm.

3.1 The Core Algorithm

The basic idea of NIL is to view interpolants as classiﬁers and use SVMs with the
kernel trick to perform effective classiﬁcation. The algorithm is based on the sampling-
guessing-reﬁning technique: in each iteration, it is fed with a classiﬁer (candidate in-
terpolant) for a ﬁnite set of sample points from
(line 5), and verify the
candidate (line 10) by checking the entailment problem that deﬁnes an interpolant (as
in Def. 1). If the veriﬁcation succeeds, the interpolant is returned as the ﬁnal result.
Otherwise, a set of counterexamples is obtained (line 13 and 14) as new sample points
to further reﬁne the classiﬁer. In what follows, we explain the steps of the interpolation
procedure in more detail.

ψ
(cid:74)

and

φ

(cid:74)

(cid:75)

(cid:75)

Initial sampling. The algorithm begins by checking the satisﬁability of φ ∧ ψ. If the
formula is satisﬁable, it is then impossible to ﬁnd an interpolant, and the algorithm stops
declaring no interpolant exists.

Next, the algorithm attempts to sample points from both

. This initial
(cid:75)
sampling stage can usually be done efﬁciently using the Monte Carlo method, e.g. by
(uniformly) scattering a number of random points over certain bounded range and then
selecting those fall in
respectively. However, this method fails when one
or both of the predicates is very unlikely to be satisﬁed. One common example is when
the predicate involves equalities. For such situations, solving the satisﬁability problem
using QE is guaranted to succeed in producing the sample points.

and

and

ψ

φ

(cid:74)

(cid:75)

(cid:74)

(cid:74)

(cid:75)

(cid:74)

(cid:75)

ψ

φ

To meet the condition that the generated interpolant can only involve symbols that
are common to φ and ψ, we can project the points sampled from
) to the
space of x by simply dropping the components that pertain to y (resp. z) and thereby
obtain sample points in X + (resp. X −).

(resp.

ψ

φ

(cid:75)

(cid:74)

(cid:75)

(cid:74)

NIL: Learning Nonlinear Interpolants

7

Algorithm NIL: Learning nonlinear interpolant

input : φ and ψ in TP over common variables x;

m, degree of the polynomial kernel, and hence

maximum degree of the interpolant.

/* checking unsatisfiability */
if φ ∧ ψ (cid:54)|= ⊥ then

/* no interpolant exists */
abort;

1

2

/* generating initial sample points */

3 (cid:104)X +, X −(cid:105) ← Sampling(φ, ψ);

/* counterexample-guided learning */

4 while (cid:62) do

/* generating a classifier by SVMs */
C ← SVM(X +, X −, m);
/* checking classification result */
if C = Failed then

/* no interpolant exists in R[x]m */
abort;

/* classifier as candidate interpolant */
else

I ← C;

/* valid interpolant found */
if φ |= I and I ∧ ψ |= ⊥ then

return I;

/* adding counterexamples */
else

X + ← X + (cid:93) FindInstance(φ ∧ ¬I);
X − ← X − (cid:93) FindInstance(I ∧ ψ);

5

6

7

8

9

10

11

12

13

14

φ

(cid:75)

(cid:74)

ψ

(cid:74)

(cid:75)

Fig. 2: In NIL, a candidate interpolant
(black line as its boundary) is reﬁned to
an actual one (red line as its boundary)
by adding a counterexample (red dot).

φ

(cid:75)

(cid:74)

δ

ψ

(cid:74)

(cid:75)

Fig. 3: In NILδ, a counterexample (red
dot) stays at least a distance of δ away
from the candidate interpolant (black line
as its boundary) to be reﬁned, leading to
an interpolant (red line as its boundary)
with tolerance δ.

(cid:75)

φ

Entailment checking. The correctness of SVM given in Lemma 1 only guarantees
that the candidate interpolant separates the ﬁnite set of points sampled from
and
ψ
, not necessarily the entirety of the two sets. Hence, post-veriﬁcation by checking
(cid:74)
the entailment problem (line 10) is needed for the candidate to be claimed as an in-
terpolant of φ and ψ. This can be achieved by solving the equivalent QE problems
∀x. φ(x, y)|x =⇒ I(x) and ∀x. I(x) ∧ ψ(x, z)|x =⇒ ⊥, where ·|x is the projection
to the common space over x. The candidate will be returned as an actual interpolant if
both formulae reduce to (cid:62) after eliminating the universal quantiﬁers. The satisﬁability
checking at line 1 can be solved analogously. Granted, the entailment checking can also
be encoded in SMT techniques by asking the satisﬁability of the negation of the uni-
versally quantiﬁed predicates, however, limitations of current SMT solvers in nonlinear
arithmetic hinders them from being practically used in our framework, as demonstrated
later in Sect. 5.

(cid:75)

(cid:74)

Counterexample generation. If a candidate interpolant cannot be veriﬁed as an actual
one, then at least one witness can be found as a counterexample to that candidate, which
can be added to the set of sample points in the next iteration to reﬁne further candidates
(cf. Fig. 2). Multiple counterexamples can be obtained at a time thereby effectively
reducing the number of future iterations.

In general, we have little control over which counterexample will be returned by QE.
In the worst case, the counterexample can lie almost exactly on the hyperplane found
by SVM. This poses issues for the termination of the algorithm. We will address this
theoretical issue by slightly modifying the algorithm, as explained in Sect. 3.3 and 4.

8

M. Chen et al.

3.2 Comparison with the Na¨ıve QE-Based Method

Simply performing QE on ∃y. φ(x, y) yields already an interpolant for mutually contra-
dictory φ and ψ. Such an interpolant is actually the strongest in the sense of [12], which
presents an ordered family of interpolation systems due to the logical strength of the
synthesized interpolants. Dually, the negation of the result when performing QE over
∃z. ψ(x, z) is the weakest interpolant. However, as argued by D’Silva et al. in [12],
a good interpolant (approximation of φ or ψ) –when computing invariants of transi-
tion systems using interpolation-based model checking– should be coarse enough to
enable rapid convergence but strong enough to be contained within the weakest induc-
tive invariant. In contrast, the advantages of NIL are two-fold: ﬁrst, it produces better
interpolants (in the above sense) featuring “medium” strength (due to the way optimal-
margin classiﬁer is deﬁned) which are thus more effective in practical use and further-
more resilient to perturbations in φ and ψ (i.e., the robustness shown later in Sect. 6);
second, NIL always returns a single polynomial inequality as the interpolant which is
often simpler than that derived from the na¨ıve QE-based method, where the direct pro-
jection of φ(x, y) onto the common space over x can be as complex as the original
φ.

These issues can be avoided by combining this method with a template-based ap-
proach, which in turn introduces fresh quantiﬁers over unknown parameters to be elim-
inated. Note that in NIL the candidate interpolants I ∈ R[x]m under veriﬁcation are
polynomials without unknown parameters, and therefore, in contrast to performing QE
over an assumed template, the learning-based technique can practically generate poly-
nomial interpolants of higher degrees (with acceptable rounds of iterations). For exam-
ple, NIL is able to synthesize an interpolant of degree 7 over 2 variables (depicted later
in Fig. 4(b)), which would require a polynomial template with (cid:0)7+2
(cid:1) = 36 unknown
parameters that goes far beyond the capability of QE procedures.

2

On the other hand, performing QE within every iteration of the learning process,
for entailment checking and generating counterexamples, limits the efﬁciency of the
proposed method, thereby conﬁning NIL currently to applications only of small scales.
Potential solutions to the efﬁciency bottleneck will be discussed in Sect. 5.

3.3 Variants of NIL

While the above basic algorithm is already effective in practice (as demonstrated in
Sect. 5), it is guaranteed to terminate only when there is an interpolant with positive
. In this section, we present two variants of the
functional margin between
(cid:74)
algorithm that have nicer theoretical properties in cases where the two sets are only
ψ
separated by an interpolant with zero functional margin, e.g., cases where
share adjacent or even coincident boundaries.

and

and

ψ

φ

φ

(cid:75)

(cid:74)

(cid:74)

(cid:75)

(cid:75)

(cid:75)

(cid:74)

Entailment checking with tolerance δ. When performing entailment checking for a
candidate interpolant I, instead of using, e.g., the formula p(x) ≥ 0 for I, we can
introduce a tolerance of δ. That is, we check the satisﬁability of φ ∧ (p(x) < −δ) and
(p(x) ≥ δ) ∧ ψ instead of the original φ ∧ (p(x) < 0) and (p(x) ≥ 0) ∧ ψ. This
means that a candidate that is an interpolant “up to a tolerance of δ” will be returned
as a true interpolant, which may be acceptable in some applications. If the candidate
interpolant is still not veriﬁed, the counterexample is guaranteed to be at least a distance

NIL: Learning Nonlinear Interpolants

9

of δ away from the separating hyperplane. Note the distance δ is taken in the feature
space R ˜d, not in the original space. We let NILδ(φ,ψ,m) denote the version of NIL with
this modiﬁcation (cf. Fig. 3). In the next section, we show NILδ(φ,ψ,m) terminates as
ψ
long as
are bounded, including the case where they are separated only by
(cid:75)
interpolants of functional margin zero.

and

φ

(cid:75)

(cid:74)

(cid:74)

Varying tolerance during the execution. A further reﬁnement of the algorithm can be
made by varying the tolerance δ during the execution. We also introduce a bounding box
B of the varying size to handle unbounded cases. Deﬁne algorithm NIL∗
δ,B(φ, ψ, m)
as follows. Let δ1 = δ and B1 = B. For each iteration i, execute the core algorithm,
except that the counterexample must be a distance of at least δi away from the separating
boundary, and have absolute value in each dimension at most B (both in R ˜d). After the
termination of iteration i, begin iteration i + 1 with δi+1 = δi/2 and Bi+1 = 2Bi. This
continues until an interpolant is found or until a pre-speciﬁed cutoff. For any
and
(without the boundedness condition), this variant of the algorithm converges to an
ψ
(cid:74)
interpolant in the limit, which will be made precise in the next section.

φ

(cid:74)

(cid:75)

(cid:75)

4 Soundness, Completeness and Convergence

In this section, we present theoretical results obtained for the basic NIL algorithm and
its variants. Proofs are included in Appx. A due to the lack of space.
First, the basic algorithm is sound, as captured by Theorem 1.

Theorem 1 (Soundness of NIL). If NIL(φ,ψ,m) terminates and returns I, then I is an
interpolant in R[x]m of φ and ψ.

Under certain conditions, the algorithm is also terminating (and hence complete).
We prove two such situations below. In both cases, we require boundedness of the two
sets that we want to separate. In the ﬁrst case, there exists an interpolant with positive
functional margin between the two sets.

Theorem 2 (Conditional Completeness of NIL). If
are bounded and
there exists an interpolant in R[x]m of φ and ψ with positive functional margin γ when
mapped to R ˜d, then NIL(φ,ψ,m) terminates and returns an interpolant I of φ and ψ.

ψ
(cid:74)

and

φ

(cid:75)

(cid:75)

(cid:74)

The standard algorithm is not guaranteed to terminate when

are only
separated by interpolants of functional margin zero. However, the modiﬁed algorithm
NILδ(φ,ψ,m) does terminate (with the cost that the resulting answer is an interpolant
with tolerance δ).

and

(cid:74)

(cid:75)

(cid:74)

(cid:75)

ψ

φ

Theorem 3 (Completeness of NILδ with zero margin). If
are bounded,
and δ > 0, then NILδ(φ,ψ,m) terminates. It returns an interpolant I of φ and ψ with
tolerance δ whenever such an interpolant exists.

and

ψ

φ

(cid:74)

(cid:75)

(cid:74)

(cid:75)

By iteratively decreasing δ during the execution of the algorithm, as well as intro-
ducing an iteratively increasing bounding box, as in NIL∗
δ,B(φ, ψ, m), we can obtain
more and more accurate candidate interpolants. We now show that this algorithm con-
verges to an interpolant without restrictions on φ and ψ. We ﬁrst make this convergence
property precise in the following deﬁnition.

10

M. Chen et al.

φ

and

Deﬁnition 2 (Convergence of a sequence of equations to an interpolant). Given
that we want to separate, and an inﬁnite sequence of equations
two sets
(cid:75)
I1, I2, . . . , we say the sequence In converges to an interpolant of φ and ψ if, for each
point p in the interior of
, there exists some integer Kp such that Ik classiﬁes
φ
(cid:74)
p correctly for all k ≥ Kp.

ψ
(cid:74)

or

ψ

(cid:75)

(cid:75)

(cid:74)

(cid:75)

(cid:74)

Theorem 4 (Convergence of NIL∗
. Suppose there
exists an interpolant of φ and ψ, then the inﬁnite sequence of candidates produced by
NIL∗

(cid:75)
δ,B(φ, ψ, m) converges to an interpolant in the sense of Deﬁnition 2.

δ,B). Given two regions

and

ψ

φ

(cid:75)

(cid:74)

(cid:74)

5

Implementation and Experiments

5.1

Implementation Issues

(cid:75)

(cid:75)

(cid:75)

(cid:74)

(cid:74)

φ

ψ

or

and

φ
(cid:74)

ψ
(cid:74)

We have implemented the core algorithm NIL as a prototype1 in Wolfram Mathemat-
ica with LIBSVM [5] being integrated as an engine to perform SVM classiﬁcations.
Despite featuring no completeness for adjacent
nor convergence for un-
bounded
, the standard NIL algorithm yields already promising results as
(cid:75)
shown later in the experiments. Key Mathematica functions that are utilized include
REDUCE, for entailment checking, e.g., the unsatisﬁability checking of φ ∧ ψ and the
post-veriﬁcation of a candidate interpolant, and FINDINSTANCE, for generating coun-
terexamples and sampling initial points (when the random sampling strategy fails). The
REDUCE command implements a decision procedure for TP and its appropriate exten-
sion to catering for transcendental functions (cf. [42]) based on cylindrical algebraic
decomposition (CAD), due to Collins [8]. The underlying quantiﬁer-elimination proce-
dure, albeit inducing rather high computation complexity, cannot in practice be replaced
by SMT-solving techniques (by checking the negation of a universally quantiﬁed pred-
icate) as in the linear arithmetic. For instance, the off-the-shelf SMT solver Z3 fails
to accomplish our tasks particularly when the coefﬁcients occurring in the entailment
problem to be checked get larger2.

Numerical errors and rounding. LIBSVM conducts ﬂoating-point computations for
solving the optimization problems induced by SVMs and consequently yields numeri-
cal errors occurring in the candidate interpolants. Such numerical errors may block an
otherwise valid interpolant from being veriﬁed as an actual one and additionally bring
down the simplicity and thereby the effectiveness of the synthesized interpolant, thus
not very often proving humans with clear-cut understanding. This is a common issue for
approaches that reduce the interpolation problem to numerical solving techniques, e.g.
SDP solvers exploited in [10, 13, 32], while an established method to tackle it is known
as rational recovery [28, 46], which retrieves the nearest rational number from the con-
tinued fraction representation of its ﬂoating-point approximation at any given accuracy
(see e.g. [46] for theoretical guarantees and [32] for applications in interpolation). The
algorithm implementing rational recovery has been integrated in our implementation
and the consequent beneﬁts are two-fold: (i) NIL can now cope with interpolation tasks

1 Available at http://lcs.ios.ac.cn/˜chenms/tools/NIL.tar.bz2
2 As can be also observed at https://github.com/Z3Prover/z3/issues/1765

NIL: Learning Nonlinear Interpolants

11

where only exact coefﬁcients sufﬁce to constitute an actual interpolant while any numer-
share
ical error therein will render the interpolant invalid, e.g., cases where
parallel, adjacent, or even coincident boundaries, as demonstrated later by examples
with ID 10–17 in Table 1; (ii) rationalizing coefﬁcients moreover facilitates simpliﬁ-
cations over all of the candidate interpolants and therefore practically accelerating the
entailment checking and counterexample generation processes, which in return yields
simpler interpolants, as shown in Table 2 in the following section.

ψ
(cid:74)

and

φ

(cid:75)

(cid:75)

(cid:74)

5.2 Benchmark and Experimental Results

Table 1 collects a group of benchmark examples from the literature on synthesizing non-
linear interpolants as well as some geometrically contrived ones. All of the experiments
have been evaluated on a 3.6GHz Intel Core-i7 processor with 8GB RAM running 64-
bit Ubuntu 16.04.

In Table 1, we group the set of examples into four categories comprising 20 cases
in total. For each example, ID numbers the case, φ, ψ and I represent the two formulas
to be interpolated and the synthesized interpolant by our method respectively, while
Time/s indicates the total time in seconds for interpolation. The categories are described
as follows, and the visualization of a selected set of typical examples thereof is further
depicted in Fig. 4.

(cid:75)

(cid:75)

(cid:75)

and

φ
(cid:74)

ψ
(cid:74)

ψ
(cid:74)

Cat. I: with/without rounding. This category includes 9 cases, for which our method
generates the polynomial interpolants correctly with or without the rounding operation.
Cat. II: with rounding. For cases 10 to 17 in this category, where
share
parallel, adjacent, or even coincident boundaries, our method produces interpolants suc-
cessfully with the rouding process based on rational recovery.
Cat. III: beyond polynomials. This category encloses two cases beyond the theory
TP of polynomials: for case 18, a veriﬁed polynomial interpolant is obtained in spite of
the transcendental term in ψ; while for case 19, the SVM classiﬁcation fails since
φ
(cid:75)
(cid:74)
and
are not linearly separable in any ﬁnite-dimensional feature space and hence no
polynomial interpolant exists for this example. Note that our counterexample-guided
learning framework admits a straightforward extension to a decidable fragment of more
general nonlinear theories involving transcendental functions, as investigated in [42].
Cat. IV: unbalanced. The case 20, called Unbalanced, instantiates a particular sce-
nario where φ and ψ have extraordinary “unbalanced” number of models that make
them true respectively. For this example, there are an inﬁnite number of models satis-
fying φ yet one single model (i.e., x = 0) satisfying ψ. The training process in SVMs
may fail when encountering extremely unbalanced number of positive/negative sam-
ples. This is solved by specifying a weight factor for the positive set of samples as the
number of negative ones, and dually for the other way around, to balance biased num-
ber of training samples before triggering the classiﬁcation. Such a balancing trick is
supported in LIBSVM.

Remark that examples named CAV13-1/3/4 are taken from [10] (and the latter two orig-
inally from [27] and [17] respectively), where interpolation is applied to discovering
inductive invariants in the veriﬁcation of programs and hybrid systems. For instance,
CAV13-3 is a program fragment describing an accelerating car and the synthesized in-
terpolant by NIL sufﬁces to prove the safety property of the car concerning its velocity.

12

M. Chen et al.

u
n
b
a
l
a
n
c
e
d

2
0

U
n
b
a
l
a
n
c
e
d

x
>
0
∨
x
<
0

x
=
0

b
e
y
o
n
d

p
o
l
y
n
o
m
i
a
l
s

1
9

1
8

T
r
a
n
s
c
e
n
d
e
n
t
a
l

T
A
C
A
S
1
6

[
1
4
]

s
i
n
x
≥
0
.
6

y
−
x

2
≥
0

s
i
n
x
≤
0
.
4

y
+
c
o
s
x
−
0
.
8
≤
0

y
=
y
1
+
x
∧
x
a
=
x
−
2
y
∧
y
a
=
2
x
+
y

1
7

C
A
V
1
3
-
4
[
1
0
]

−
2
x
a
1
+
y
a
1
−
y
1
=
0
∧
x
−
x
1
−
1
=
0
∧

x
a
+
2
y
a
<
0

x

2
>
0

S
V
M

f
a
i
l
e
d

1
5
x

2
<
4
+
2
0
y

2
x
a
+
4
y
a
>
5

w

i
t
h

r
o
u
n
d
i
n
g

1
6

I
J
C
A
R
1
6
-
2

[
1
3
]

−
y
21
−
x
21
+
2
x
1
y
1
−
2
y
1
+
2
x
1
≥
0
∧

−
z
21
−
4
x
22
+
4
x
2
z
1
+
3
z
1
−
6
x
2
−
2
≥
0
∧

x
1
<
x
2

−
y
1
+
x
1
−
2
≥
0
∧
2
x
2
−
x
1
−
1
>
0
∧

−
z
1
+
2
x
2
+
1
≥
0
∧
2
x
1
−
x
2
−
1
>
0
∧

−
y
22
−
y
21
−
x
22
−
4
y
1
+
2
x
2
−
4
≥
0

x
a
1
+
2
y
a
1
≥
0
∧
x
a
1
+
2
y
a
1
−
x
1
=
0
∧

−
z
22
−
x
21
−
x
22
+
2
x
1
+
z
1
−
2
x
2
−
1
≥
0

1
5

1
4

1
3

1
2

1
1

1
0

S
h
a
r
p
e
r
-
1

[
3
2
]

y
+
1
<
0

P
a
r
a
l
l
e
l
h
a
l
f
p
l
a
n
e

y
−
x
−
1
≥
0

P
a
r
a
l
l
e
l
p
a
r
a
b
o
l
a

y
−
x

2
−
1
≥
0

v
c
1
=
v
c
+
a
c

A
d
j
a
c
e
n
t

C
o
i
n
c
i
d
e
n
t

y
−
x

2
>
0

x
+
y
>
0
∨
x
+
y
<
0

S
h
a
r
p
e
r
-
2

[
3
2
]

y
−
x
>
0
∧
x
+
y
>
0

x
+
y
=
0

y
−
x

2
<
=
0

y
−
x

2
<
0

y
+
x

2
<
=
0

y
−
x
+
1
<
0

x

2
+
y

2
−
1
≤
0

x

2
<
y

2
+
y
<
y

2

y
>
0

(
x
+
y
)

2
>
0

x
<
y

12

+
x

2
<
y

v
c
<
4
9
.
6
1
∧
f
a
=
0
.
5
4
1
8
v
c

2

∧

9

C
A
V
1
3
-
3
[
1
0
]

f
r
=
1
0
0
0
−
f
a
∧
a
c
=
0
.
0
0
0
5
f
r
∧

v
c
1
≥
4
9
.
6
1

−
1
+

9
9

2
v
c
1

<
0

r
o
u
n
d
i
n
g

w

i
t
h
/
w

i
t
h
o
u
t

5

U

l
t
i

m
a
t
e

4

T
w
i
s
t
e
d

7

C
A
V
1
3
-
1
[
1
0
]

6

I
J
C
A
R
1
6
-
1

[
1
3
]

8

C
A
V
1
3
-
2
[
1
0
]

1
.
2
x

2
+
y

2
+
x
z
=
0

x

2
+
y

2
+
z

2
−
2
≥
0
∧

b
+
b
x
+
1
−
y
=
0

1
−
a

2
−
b

2
>
0
∧
a

−
x
1
−
x
2
+
3
−
y

2
>
0

−
x
21
+
4
x
1
+
x
2
−
4
≥
0
∧

(
x
+
1
)

2
+
y

2
−

12
5

≤
0

(
x
−
1
)

2
+
y

2
−
0
.
0
9
>
0
∧

(
x
+
1
)

2
+
y

2
−
1
.
1
0
2
5
≥
0
∨

(
x
−
1
)

2
+
y

2
−
0
.
9
0
2
5
≤
0
)
∧

C
a
t
e
g
o
r
y

I
D

N
a
m
e

φ

2

1

N
e
c
k
l
a
c
e

D
u
m
m
y

x
<
−
1

y
−
x

2
−
1
=
0

3

F
a
c
e

(
x
−
4
)

2
+
y

2
−
1
≤
0

(
x
+
4
)

2
+
y

2
−
1
≤
0
∨

−
y
z
+
z

2
−
1
≥
0
∧

x

2
−
2
x
y

2
+
3
x
z
−
y

2

2
+
b
−
1
−
x
=
0
∧

x

2
−
2
y

2
−
4
>
0

−
1
+

2

x

2

−

y3

+

3

x
y

−

y
24

<
0

−
3
x
21
−
x
22
+
1
≥
0
∧
x
2
−
z

2
≥
0

1
−

4

3
x
1

−

2

x
2

<
0

x

2
+
y

2
−
z
−
1
=
0

1
0
y

2

(
z
+
2
)
−
3
y
(
4
z

2
−
5
z
+
4
)
−
2
0
z
(
z

2
+
2
)
)

1
0
)
−
3
5
(
3
z

4
+
8
z

2
+
4
z
−
9
)
)
<
1
4
x
(
2
0
x

2

(
z
+
1
)
+

2
0
−
3
x

2
−
4
y

3
−
1
0
z

2
≥
0
∧

2
(
7
0
y

3

z
+
5
y

2

(
1
2
z

2
+
2
1
z
+
2
8
)
−
1
4
y
(
6
z

3
+
5
z

2
+

1
0
5
x

4
+
x

2

(
1
4
0
y

2
+
2
4
y
(
5
z
+
7
)
+
3
5
z
(
3
z
+
8
)
)
+

y

2

z

2
−
y

2
−
4
≤
0

(
x

2
+
y

2
−
3
.
8
0
2
5
≤
0
∧
y
≥
0
∨

(
−
3
.
8
0
2
5
+
x

2
+
y

2
≤
0
∧
−
y
≥
0
∨

x

2
+

1
2
0

16

(cid:16)

x

4
+
2
x

2

y

2
+
y

4
(cid:17)

+

1

(cid:16)

−
x

6
−
y

6
(cid:17)

+
x

2

z

2
−

w
2
+
4
(
x
−
y
)

4
+
(
x
+
y
)

2
−
8
0
≤
0
∧

−
w
2

(
x
−
y
)

4
+
1
0
0
(
x
+
y
)

2
−
3
0
0
0
≥
0

−

12
5

+
(
1
−
x
)

2
+
y

2
≤
0

−
1
.
1
0
2
5
+
(
1
−
x
)

2
+
y

2
≥
0
∨

−
0
.
0
9
+
(
−
1
−
x
)

2
+
y

2
>
0
∧

−
0
.
9
0
2
5
+
(
−
1
−
x
)

2
+
y

2
≤
0
)
∧

4
8

y

6

−

x

y

6

7
1

y
57

+

x

2

(
−

2
5

y

5

9

x

4

(
−

2
y

3

+

y
46

−

y
32

−

y
26

−

y5
9

1
1

2
y

4

−

1
8

y

4

+

y3

−

−

+

2
5

y

3

y
33

13
1

)
+
x

3

(

−

1
1

y

4

9

−
y

2
−

y4
5

−

+

1
0

y

2

13
2

)
+

2
7

x

7

+
x

6

(
−
y5

−

19
6

)
+
x

5

(

2
y

2

+

38


+

18
5

<
0

+

−

1
0

y

3

y3
2

−

−

1
3

1
0
y

2

12
)
+

+

y1
8

+

1
6

1
5

)
+

−

1
6
0

x

4

+
x

3

(cid:18)

y

−

1

(cid:19)

1
7
0

1
1
3

+
x

2

−

2
2
5

y

2

+

y7
6

+

22
7


+

x

y

3

2
5
9

+

6
3

y

2

+

5
1

5
y

−

3
1
6

1


−

1
8
3

y

4

−

9
4

y

3

+

1
4

y

2

+

2
5
5

y

−
1
<
0

1
4
0
.
6
2

T
a
b

l
e

1
:

B
e
n
c
k
m
a
r
k

e
x
a
m
p
l
e
s

f
o
r

s
y
n
t
h
e
s
i
z
i
n
g

n
o
n
l
i
n
e
a
r

i
n
t
e
r
p
o
l
a
n
t
s
.

ψ

x
≥
1

y
+
x

2
+
1
=
0

x

2
+
y

2
−
6
4
≤
0
∧

(
x
−
4
)

2
+
y

2
−
9
≥
0

(
x
+
4
)

2
+
y

2
−
9
≥
0
∧

9
5

y

3

+

3
7

y

2

+

3
6
6

y

+
1
<
0

x
(

8
9

y

3

+

6
8

y

2

−

y7
4

I

x
<
0

−
y
<
0

2
2
3

3
5
6

x

4

−

x

3

y

+
x

2

(

−

4
5

y

2

15
5

)
+

1
4
6

y

4

+

−

1
7
0

y

−

29
)
+

0
.
3
3

0
.
2
1

0
.
1
1

i

T
m
e
/
s

0
.
1
1

–

1
2
.
7
1

3
.
1
0

1
2
.
3
3

0
.
2
5

0
.
1
8

2
.
3
8

2
.
1
9

2
.
4
6

4
.
5
0

4
0
.
6
3

3
8
5
7
.
8
9

3
.
2
5

0
.
1
6

4
8
.
8
2

NIL: Learning Nonlinear Interpolants

13

(a) Adjacent

(b) Ultimate

(c) Twisted

(d) Parallel parabola

(e) Coincident

(f) Sharper-2

(g) Face

(h) CAV13-2

(i) TACAS16

Fig. 4: Visualization in NIL on a selected set of examples. Legends: gray region:
with
a valid interpolant I, red dots: X +, blue dots: X −, circled dots: support vectors. Sample points are hidden in 3D-graphics
for a clear presentation.

, pink region:
(cid:75)

, blue region:
(cid:75)

φ
(cid:74)

I
(cid:74)

ψ

(cid:74)

(cid:75)

Applicability and comparison with existing approaches. As shown in Table 1, our
learning-based technique succeeds in all of the benchmark examples that admit poly-
nomial interpolants. Due to theoretical limitations of existing approaches as elaborated
in Sect. 1, none of the aforementioned methods can cope with as many cases in Table 1
as NIL can. For instances, the Twisted example as depicted in Fig. 4(c) falls beyond
the scope of concave quadratic formulas and thus cannot be addressed by the approach
in [13], while the Parallel parabola example as shown in Fig. 4(d) needs an inﬁnite com-
bination of linear constraints as an interpolant when performing the technique in [14]
and hence not of practical use, to name just a few. Moreover, we list in Table 2 a com-
parison of the synthesized interpolants against works where the benchmark examples
are collected from. As being immediately obvious from Table 2, our technique often

-3-2-10123-3-2-10123x1x2-3-2-10123-3-2-10123xy-10-50510-10-50510xy-4-2024-4-2024xy-4-2024-4-2024xy-4-2024-4-2024xy-505-505xy-4-2024-4-2024xy14

M. Chen et al.

Table 2: Comparison of the synthesized interpolants.

Name

Interpolants by NIL

IJCAR16-1 [13] 1 −

3x1
4

−

x2
2

< 0

CAV13-1 [10] −1 + x2
2

−

y
3

+

xy
3

−

y2
4

< 0

Interpolants from the sources

−3 + 2x1 + x2

1 + 1
2

x2

2 > 0

436.45(x2 − 2y2 − 4) + 1
2

≤ 0

− 14629.26 + 2983.44x3 + 10972.97x

2
3+

105x

4

2

+ x

(140y

2

+ 24y(5z + 7) + 35z(3z + 8))+

2745.14x

297.62x2 + 297.64x2x3 + 0.02x2x

1161.80x

2
2
2
3 + 811.93x
2 x
2x3 + 0.01x
4
2 − 10648.11x1 + 3101.42x1x3+

2
3 + 9625.61x
3
2+

2
2−

CAV13-2 [10]

2(70y

3

z + 5y

2

2

(12z

+ 21z + 28) − 14y(6z

3

+ 5z

2

+

10) − 35(3z

4

2

+ 8z

+ 4z − 9)) < 14x(20x

2

(z + 1)+

2

10y

(z + 2) − 3y(4z

2

− 5z + 4) − 20z(z

2

+ 2))

2
3 + 511.84x1x2 − 1034x1x2 x3+
8646.17x1x
2
2
3 + 9233.66x1x
0.02x1x2x
2 + 1342.55x1x
2
2
3
1 x3+
1 − 3737.70x
2 + 11476.61x
2
2
1x2x3+
3 − 2153.00x12x2 + 373.14x
3
2
1 + 1937.92x
2 + 8950.77x
4
1 > 0

138.70x1x
2
1x
2
1x

3
1x2 + 4827.25x

3
1x3−

2
2x3−

4071.65x

7616.18x

64.07x

CAV13-3 [10] −1 +

2vc1
99

< 0

Sharper-1 [32]

2 + y < y2

Sharper-2 [32]

y > 0

IJCAR16-2 [13] x1 < x2

CAV13-4 [10]

2xa + 4ya > 5

TACAS16 [14]

15x2 < 4 + 20y

−1.3983vc1 + 69.358 > 0

34y2 − 68y − 102 ≥ 0

8y + 4x2 > 0

−x1 + x2 > 0

716.77 + 1326.74(ya) + 1.33(ya)

2

+ 433.90(ya)

3

+

668.16(xa) − 155.86(xa)(ya) + 317.29(xa)(ya)

2

+

222.00(xa)

2

+ 592.39(xa)

2

(ya) + 271.11(xa)

3

> 0

y > 1.8 ∨ (0.59 ≤ y ≤ 1.8 ∧ −1.35 ≤ x ≤ 1.35)∨

(0.09 ≤ y < 0.59 ∧ −0.77 ≤ x ≤ 0.77)∨

(y ≥ 0 ∧ −0.3 ≤ x ≤ 0.3)

produces interpolants of simpler forms, particularly for examples CAV13-2, CAV13-4
and TACAS16. Such a simplicity beneﬁts from both the rounding effect and the form
of interpolant (i.e., a single polynomial inequality) that we tend to construct.

Bottleneck of efﬁciency and potential solutions. The current implementation of NIL
works promisingly for small examples; it does not scale to interpolation problems with
large numbers of common variables, as reported in Table 1. The bottleneck stems from
quantiﬁer eliminations performed within every iteration of the learning process, for
entailment checking and generating counterexamples. We pose here several potential
solutions that are expected to signiﬁcantly reduce computational efforts: (i) substitute
general purpose QE procedure that perform CAD by the so-called variant quantiﬁer-
elimination (VQE) algorithm [20], which features singly-exponential complexity in the
number of variables. This however requires a careful inspection of whether our problem
meets the geometric conditions imposed by VQE; (ii) incorporate relaxation schemes,
e.g., Lagrangian relaxation and sum-of-squares decompositions [33], and complement
with QE only when the relaxation fails to produce desired results.

6 Taming Perturbations in Parameters.
An interpolant synthesized by the SVM-based technique features inherent robustness
due to the way optimal-margin classiﬁer is deﬁned (Sect. 2). That is, the validity of such
an interpolant is not easily perturbed by changes (in the feature space) in the formulae

NIL: Learning Nonlinear Interpolants

15

(a) (cid:15)-perturbations in the radii

(b) Interpolant resilient to perturbations

Fig. 5: (cid:15)-Face: introducing perturbations (with (cid:15) up to 0.5) in the Face example. The synthesized interpolant is resilient to
any (cid:15)-perturbation in the radii satisfying −0.5 ≤ (cid:15) ≤ 0.5.

to be interpolated. It is straightforward in NIL to deal with interpolation problems under
explicitly speciﬁed perturbations, which are treated as constraints over fresh variables.
An example named (cid:15)-Face is depicted in Fig. 5, which perturbs (cid:104)φ, ψ(cid:105) in the Face
example as φ (cid:98)= − 0.5 ≤ (cid:15)1 ≤ 0.5 ∧ ((x + 4)2 + y2 − (1 + (cid:15)1)2 ≤ 0 ∨ (x − 4)2 + y2 −
(1+(cid:15)1)2 ≤ 0) and ψ (cid:98)= −0.5 ≤ (cid:15)2 ≤ 0.5∧x2+y2−64 ≤ 0∧(x+4)2+y2−(3+(cid:15)2)2 ≥
0 ∧ (x − 4)2 + y2 − (3 + (cid:15)2)2 ≥ 0. The synthesized interpolant over common variables
of φ and ψ is x4
182 +
+ x
2y2
19 − y
218 + 1 < 0 which is hence resilient to any (cid:15)-perturbation in the radii satisfying
−0.5 ≤ (cid:15) ≤ 0.5, as illustrated in Fig. 5(b).

268 + x2 (cid:16) y2

139 + x3y

52 − y2

25 − y3

157 − y

39 − 11

52 − 1

+ y4

− y3

116

(cid:17)

(cid:16)

(cid:17)

36

7 Conclusions

We have presented a uniﬁed, counterexample-guided method named NIL for generating
polynomial interpolants over the general quantiﬁer-free theory of nonlinear arithmetic.
Our method is based on classiﬁcation techniques with space transformations and kernel
tricks as established in the community of machine-learning. We proved the soundness
of NIL and proposed sufﬁcient conditions for its completeness and convergence. The
applicability and effectiveness of our technique are demonstrated experimentally on a
collection of representative benchmarks from the literature, including those extracted
from program veriﬁcation. Experimental results indicated that our method sufﬁces to
address more interpolation tasks, including those with perturbations in parameters, and
in many cases synthesizes simpler interpolants compared with existing approaches.

For future work, we would like to improve the efﬁciency of NIL by substituting the
general purpose quantiﬁer-elimination procedure with alternative methods previously
discussed in Sect. 5. An extension of our approach to cater for the combination of non-
linear arithmetic with EUFs, by resorting to predicate-abstraction techniques [22], will
be of particular interest. Additionally, we plan to investigate the performance of NIL
over different classiﬁcation techniques, e.g., the widespread regression-based meth-
ods [18], though SVMs are expected to be more competent concerning the robustness
and predictability, as also observed in [36].

-505-505xy(cid:15)(cid:15)(cid:15)(cid:15)(cid:15)(cid:15)(cid:15)(cid:15)-505-505xy16

M. Chen et al.

References

1. K. P. Bennett and E. J. Bredensteiner. Duality and geometry in SVM classiﬁers. In ICML’00,

pages 57–64, 2000.

2. C. M. Bishop. Pattern recognition and machine learning, pages 326–328. Springer, 2006.
3. B. E. Boser, I. Guyon, and V. Vapnik. A training algorithm for optimal margin classiﬁers. In

COLT’92, pages 144–152, 1992.

4. N. Bourbaki. Topological Vector Spaces. Elements of Mathematics. Springer-Verlag, 1987.
5. C. Chang and C. Lin. LIBSVM: A library for support vector machines. ACM TIST,

2(3):27:1–27:27, 2011.

6. A. Cimatti, A. Griggio, and R. Sebastiani. Efﬁcient interpolant generation in satisﬁability

modulo theories. In TACAS’08, pages 397–412, 2008.

7. E. M. Clarke, O. Grumberg, S. Jha, Y. Lu, and H. Veith. Counterexample-guided abstraction

reﬁnement. In CAV’00, pages 154–169, 2000.

8. G. E. Collins. Quantiﬁer elimination for real closed ﬁelds by cylindrical algebraic decom-
postion. In H. Brakhage, editor, Automata Theory and Formal Languages 2nd GI Conference
Kaiserslautern, pages 134–183, Berlin, Heidelberg, 1975. Springer Berlin Heidelberg.
9. W. Craig. Linear reasoning. A new form of the herbrand-gentzen theorem. J. Symb. Log.,

22(3):250–268, 1957.

10. L. Dai, B. Xia, and N. Zhan. Generating non-linear interpolants by semideﬁnite program-

ming. In CAV’13, pages 364–380, 2013.

11. S. Dathathri, N. Arechiga, S. Gao, and R. M. Murray. Learning-based abstractions for non-

linear constraint solving. In IJCAI’17, pages 592–599, 2017.
12. V. D’Silva, D. Kroening, M. Purandare, and G. Weissenbacher.

Interpolant strength.

In

VMCAI’10, pages 129–145, 2010.

13. T. Gan, L. Dai, B. Xia, N. Zhan, D. Kapur, and M. Chen. Interpolant synthesis for quadratic
polynomial inequalities and combination with EUF. In IJCAR’16, pages 195–212, 2016.
14. S. Gao and D. Zufferey. Interpolants in nonlinear theories over the reals. In TACAS’16, pages

625–641, 2016.

15. S. Gilbert. A nullstellensatz and a positivstellensatz in semialgebraic geometry. Mathema-

tische Annalen, 207(2):87–97, 1974.

16. S. Graf and H. Sa¨ıdi. Construction of abstract state graphs with PVS. In CAV’97, pages

72–83, 1997.

17. B. S. Gulavani, S. Chakraborty, A. V. Nori, and S. K. Rajamani. Automatically reﬁning

abstract interpretations. In TACAS’08, pages 443–458, 2008.

18. T. Hastie, R. Tibshirani, and J. H. Friedman. The elements of statistical learning: data
mining, inference, and prediction, 2nd Edition. Springer series in statistics. Springer, 2009.
19. T. A. Henzinger, R. Jhala, R. Majumdar, and K. L. McMillan. Abstractions from proofs. In

POPL’04, pages 232–244, 2004.

20. H. Hong and M. S. E. Din. Variant quantiﬁer elimination. J. Symb. Comput., 47(7):883–901,

2012.

21. S. Jha, S. Gulwani, S. A. Seshia, and A. Tiwari. Oracle-guided component-based program

synthesis. In ICSE’10, pages 215–224, 2010.

22. R. Jhala, A. Podelski, and A. Rybalchenko. Predicate abstraction for program veriﬁcation.

In Handbook of Model Checking., pages 447–491. 2018.

23. Y. Jung, W. Lee, B. Wang, and K. Yi. Predicate generation for learning-based quantiﬁer-free

loop invariant inference. In TACAS’11, pages 205–219, 2011.

24. D. Kapur, R. Majumdar, and C. G. Zarba. Interpolation for data structures. In FSE’06, pages

105–116, 2006.

25. L. Kov´acs and A. Voronkov.

Interpolation and symbol elimination.

In CADE’09, pages

199–213, 2009.

NIL: Learning Nonlinear Interpolants

17

26. J. Kraj´ıˇcek. Interpolation theorems, lower bounds for proof systems, and independence re-

sults for bounded arithmetic. J. Symb. Log., 62(2):457–486, 1997.

27. S. Kupferschmid and B. Becker. Craig interpolation in the presence of non-linear constraints.

In FORMATS’11, pages 240–255, 2011.

28. S. Lang.

Introduction to Diophantine Approximations: New Expanded Edition. Springer

New York, 2012.

29. K. L. McMillan. Interpolation and sat-based model checking. In CAV’03, pages 1–13, 2003.
30. K. L. McMillan. An interpolating theorem prover. In TACAS’04, pages 16–30, 2004.
31. K. L. McMillan. Quantiﬁed invariant generation using an interpolating saturation prover. In

TACAS’08, pages 413–427, 2008.

32. T. Okudono, Y. Nishida, K. Kojima, K. Suenaga, K. Kido, and I. Hasuo. Sharper and simpler

nonlinear interpolants for program veriﬁcation. In APLAS’17, pages 491–513, 2017.

33. P. A. Parrilo. Semideﬁnite programming relaxations for semialgebraic problems. Math.

Program., 96(2):293–320, 2003.

34. P. Pudl´ak. Lower bounds for resolution and cutting plane proofs and monotone computations.

J. Symb. Log., 62(3):981–998, 1997.

35. A. Rybalchenko and V. Sofronie-Stokkermans. Constraint solving for interpolation. In VM-

CAI’07, pages 346–362, 2007.

36. R. Sharma, A. V. Nori, and A. Aiken. Interpolants as classiﬁers. In CAV’12, pages 71–87,

2012.

37. V. Sofronie-Stokkermans. Interpolation in local theory extensions. In IJCAR’06, pages 235–

250, 2006.

38. V. Sofronie-Stokkermans. On interpolation and symbol elimination in theory extensions. In

IJCAR’16, pages 273–289, 2016.

39. A. Solar-Lezama, R. M. Rabbah, R. Bod´ık, and K. Ebcioglu. Programming by sketching for

bit-streaming programs. In PLDI’05, pages 281–294, 2005.

40. A. W. Strzebo´nski. Real root isolation for exp-log functions. In ISSAC’08, pages 303–314,

2008.

41. A. W. Strzebo´nski. Real root isolation for tame elementary functions. In ISSAC’09, pages

341–350, 2009.

42. A. W. Strzebo´nski. Cylindrical decomposition for systems transcendental in the ﬁrst variable.

J. Symb. Comput., 46(11):1284–1290, 2011.

43. A. Tarski. A Decision Method for Elementary Algebra and Geometry. University of Califor-

nia Press, Berkeley, 1951.

44. V. Vladimir. Pattern recognition using generalized portrait method. Automation and remote

control, 24:774–780, 1963.

45. G. Yorsh and M. Musuvathi. A combination method for generating interpolants.

In

CADE’05, pages 353–368, 2005.

46. J. Zhang and Y. Feng. Obtaining exact value by approximate computations. Science in China

Series A: Mathematics, 50(9):1361, 2007.

18

M. Chen et al.

Appendix A Proofs of Theorems

Proof (of Theorem 1). The argument follows immediately from the checking, done in
line 10, of the deﬁnition of interpolant (Def. 1), and the fact that any candidate inter-
(cid:117)(cid:116)
polant is of maximum degree m.

Proof (of Theorem 2). Suppose the algorithm does not terminate. Then there must be
an inﬁnite sequence of counterexamples. In particular, the sequence of counterexamples
added to either X + or X − must be inﬁnite. Without loss of generality, we assume an
inﬁnite sequence of counterexamples x1, x2, . . . are added to X +, whose initial set of
points is X +
0 .

Let γi be the functional margin in R ˜d for the separating hyperplane Pi before the
counterexample xi is found. We claim that γi ≥ γ. This is because the convex hull of
X + and X − are subsets of
, respectively. Since there exists a hyperplane
with functional margin γ, the same hyperplane also separates
φ
separating
conv(X +) and conv(X −). Since SVM maximizes the functional margin, the actual
hyperplane found by SVM must have functional margin at least γ.

and

and

ψ

ψ

φ

(cid:74)

(cid:75)

(cid:75)

(cid:74)

(cid:75)

(cid:74)

(cid:74)

(cid:75)

Next, we claim dist(xi, xj) ≥ γ/2 for any i < j. Since xj is a counterexample
found when X + contains x1, . . . , xj−1, it lies on the other side of the hyperplane Pi
from x1, . . . , xj−1. Since the distance between any point on Pi and the convex hull of
X +
0 ∪ {x1, . . . , xj−1} is at least γi/2 ≥ γ/2, the same holds for xj. In particular, the
distance between xj and each of the points x1, . . . , xj−1 is at least γ/2.

In other words, the balls B(xi, γ/4) are disjoint from each other. Their total volume
is at
(cid:117)(cid:116)

is inﬁnite. However, they are contained in the set of points whose distance to
most γ/4. This contradicts with the assumption that

is bounded.

φ

φ

(cid:75)

(cid:74)

φ
(cid:74)

(cid:75)

Proof (of Theorem 3). The proof is analogous to that of Theorem 2. Again, we assume
an inﬁnite sequence of counterexamples x1, x2, . . . added to X +. Since the entailment
checking is performed with tolerance δ, each counterexample must have a distance of
at least δ away from the separating hyperplane. This means that the distance between a
counterexample xn and each existing one x1, . . . , xn−1 is at least δ. Hence, the balls
B(xi, δ/2) are disjoint from each other and their total volume is inﬁnite. However, they
is at most δ/2, which contradicts
are contained in the set of points whose distance to
(cid:117)(cid:116)
the fact that

is bounded.

φ

(cid:74)

(cid:75)

(cid:75)

(cid:74)

φ

ψ

(cid:74)
ψ
(cid:74)

Proof (of Theorem 4). Each iteration of NIL∗
δ,B(φ, ψ, m) can be considered as a run
∩
of algorithm NILδ with tolerance δ = δi, where the two regions are replaced by
∩[−B, B] ˜d. The two regions are clearly bounded, hence by Theorem
[−B, B] ˜d and
(cid:75)
3, each iteration of NIL∗
δ,B(φ, ψ, m) terminates. Given any point p in the interior of
, we need to show that there exists some integer Kp such that Ik
either
or
classiﬁes p correctly for all k ≥ Kp. Since each iteration of the algorithm terminates,
it sufﬁces to show the existence of an integer i such that p is classiﬁed correctly after
the i-th iteration. We take i sufﬁciently large so that the open ball B(p, δi) lies entirely
in the region containing p (which is possible since p is in the interior and δi converges
to 0), and the absolute value of p in each dimension is at most Bi (which is possible
since Bi tends to inﬁnity). It is then clear that p will be correctly classiﬁed after the i-th
(cid:117)(cid:116)
iteration.

φ
(cid:74)

(cid:75)

(cid:75)

(cid:74)

(cid:75)

