AI4D - African Language Program

Kathleen Siminyu1, Godson Kalipe2, Davor Orlic3, Jade Abbott4, Vukosi Marivate5,
Sackey Freshia6, Prateek Sibal7, Bhanu Neupane7, David I. Adelani8, Amelia Taylor9,
Jamiil Toure ALI2, Kevin Degila2, Momboladji Balogoun2, Thierno Ibrahima DIOP10,
Davis David11, Chayma Fourati12, Hatem Haddad12, Malek Naski12
1AI4D Africa, 2Takwimu Lab, 3Knowledge for All Foundation, 4Retro Rabbit,
5University of Pretoria, 6Jomo Kenyatta University of Agriculture and Technology,
7UNESCO, 8Saarland University, 9The University of Malawi,
10Baamtu Datamation, 11TAVODET Youth Development, 12iCompass,

Abstract

Advances in speech and language technolo-
gies enable tools such as voice-search, text-to-
speech, speech recognition and machine trans-
lation. These are however only available for
high resource languages like English, French
or Chinese. Without foundational digital re-
sources for African languages, which are con-
sidered low-resource in the digital context,
these advanced tools remain out of reach. This
work details the AI4D - African Language
Program, a 3-part project that 1) incentivised
the crowd-sourcing, collection and curation of
language datasets through an online quantita-
tive and qualitative challenge, 2) supported re-
search fellows for a period of 3-4 months to
create datasets annotated for NLP tasks, and
3) hosted competitive Machine Learning chal-
lenges on the basis of these datasets. Key
outcomes of the work so far include 1) the
creation of 9+ open source, African language
datasets annotated for a variety of ML tasks,
and 2) the creation of baseline models for these
datasets through hosting of competitive ML
challenges.

1 Motivation

Languages, with their complex implications for
identity, cultural diversity, spirituality, communica-
tion, social integration, education and development,
are of crucial importance for people, prosperity and
the planet. People not only embed in languages
their history, traditions, memory, traditional knowl-
edge, unique modes of thinking, meaning and ex-
pression, but more importantly they also construct
their future through them.

In this context, Language Technologies (LT),
greatly contribute to the promotion of linguistic
diversity and multilingualism. These technologies
are moving outside research laboratories into nu-
merous applications in many different areas. UN-
ESCO’s International Conference Language Tech-

nologies for All (LT4All): Enabling Linguistic Di-
versity and Multilingualism Worldwide1, organized
in December 2019, underlined spelling/grammar
checkers up to speech and speaker recognition, ma-
chine translation for text and audio, speech synthe-
sis, and spoken dialogue among others as important
areas for enabling linguistic diversity and multilin-
gualism. In addition, the Los Pinos Declaration on
the Decade of Indigenous Languages (2022-2032)2
calls for the design and access to sustainable, acces-
sible, workable and affordable language technolo-
gies and places indigenous peoples at the centre
of its recommendations under the slogan “Nothing
for us without us.”

1.1 The increasing technological gap

Apart from applications in automatic translation
which have revolutionized communication between
people around the world, NLP has been driving
signiﬁcant changes that African languages speakers
are often left out of, because their languages are not
yet taken into account in mainstream NLP research.
Some of those include the use of chatbots for better
customer service and improved user experience and
other tools like sentiment analysis solutions that
help companies better understand their market and
efﬁciently process huge amounts of feedback about
their services.

The lack of African languages’ datasets is dis-
couraging many NLP practitioners to start from
scratch and the task will have to be taken up by
African researchers because of the low economic
interest that our languages represent for top compa-
nies driving changes in NLP. In fact, Quartz Africa
explains that those giants are directly concerned
with Return On Investment while investing in new

1International Conference Language Technologies for All

(LT4All)

2Upcoming Decade of Indigenous Languages (2022 –

2032) to focus on Indigenous language users’ human rights

1
2
0
2

r
p
A
6

]
L
C
.
s
c
[

1
v
6
1
5
2
0
.
4
0
1
2
:
v
i
X
r
a

 
 
 
 
 
 
languages. It has appeared that the top 100 lan-
guages in NLP cover 96 percent of the GDP of the
world3 although they equated to less than 60 per-
cent of the population.4 Without action, the digital
gap between Africa and the rest of the world will
keep increasing as far as NLP solutions which are
driving changes in healthcare, ﬁnance or education,
are concerned.

1.2 NLP solutions in African languages as a

motor of socio economic change

UNESCO’s report ‘Steering AI and Advanced ICTs
for Knowledge Societies’ notes that access to data
is an important determinant for the development
of AI. For instance, access to data is essential for
training algorithms and for their usefulness in large
scale application. It notes that “in the absence of
access to data, new ﬁrms face potential entry bar-
riers in challenging the entrenched market actors”.
Further, even academic institutions face barriers in
accessing data, which becomes a barrier to research
and innovation (Hu et al., 2019).

In response to UNESCO and AI4D’s Artiﬁcial
Intelligence Needs Assessment Survey in Africa
(Figure 1), nine out of thirty countries in Africa
have underlined the availability of datasets to train
AI systems but a lack of human resources for devel-
oping datasets.6 Another 16 do not have datasets to
train AI systems nor the capacities to develop new
datasets.7

Initiatives have been spurring around the conti-
nent to experiment the use of indigenous languages
as a means of education in the past years. Studies
led by the Unesco Institute for Lifelong Learning
have shown that multilingual education in African
countries actually yield better results than its mono-
lingual alternative and is more beneﬁtial to socio-
economic development (Ouane and Glanz, 2010).
But, new technological tools are needed to support
initiatives such as this. In a wider sense, many of
the initiatives for revitalizing African languages
have been suffering from lack of funding, docu-
mentation and innovative approaches even when

3Language GDP coverage was calculated multiplying the

number of speakers of a language by the GDP per capita

4African languages are being left behind when it comes to

voice recognition innovation

6Botswana, Cabo Verde, Cameroon, Cˆote d’Ivoire, Equa-

torial Guinea, Madagascar, Senegal, Seychelles, Togo

7Benin, Chad, Comoros, Democratic Republic of the
Congo, Eswatini, Gambia, Guinea, Lesotho, Malawi, Nige-
ria, Sao Tome and Principe, Sierra Leone, Somalia, Uganda,
Zambia, Zimbabwe

national policies are in favor of their use (Sands
and Sands, 2018). Technological solutions based
on NLP have the ability to reduce program costs for
language revitalization programs by helping them
reach more ambitious goals, faster and with less
human resources.

In general, NLP in Africa holds a huge poten-
tial, as advances in NLP for African languages
will constitute a win not only for the speakers of
those languages but also for companies offering
services on the continent. Governments are another
potential great beneﬁciary of breakthroughs in the
domain as they would be able to leverage them to
bring information and modern nation-wide solu-
tions closer to the inhabitants of the continent who
have little or no proﬁciency in foreign languages.

2 Literature Review

Few endeavors in the ﬁeld of Artiﬁcial Intelligence
have been working towards facilitating data collec-
tion and facilitating NLP research efforts on the
African continent.

Masakhane (Orife et al., 2020b), is one of the
most prominent recent efforts towards machine
translation research in Africa and offers greater
visibility of the community on global platforms.
They publicly offer easy to grasp hands-on tools to
on-board newcomers to the ﬁeld and allow them to
train baseline models for a wide range of African
(Agic and
languages using the JW300 dataset
Vulic, 2020). They have helped build baseline mod-
els for more than 35 African languages to date.
Despite the tremendous work that Masakhane has
been doing, there is still a need for more diverse
data as JW300 data is heavily biased with religious
content. It is one of the motivations of the African
Language program. Another concern is the lack
of written documentation for most widely spoken
African languages. Initiatives such as the BULB
Project (Adda et al., 2016) aim at facilitating di-
rect audio data collection for African languages
and their annotation at phoneme and word level.

Much earlier efforts included the Language Re-
sources and Evaluations Journal Special Issue on
African Language Technology (De Pauw et al.,
2011) which presented a cross-section of the state-
of-the-art in the ﬁeld as well as an overview of the
most prominent research efforts in African Lan-
In 2016, Interspeech had a
guage Technology.
special session titled ”Sub-Saharan African lan-
guages: from speech fundamentals to applications”

Figure 1: Responses of African member states as to their capacities for data development. These were collected as
part of an AI Needs Assessment Survey facilitated by UNESCO and AI4D5

in recognition of the fact that Sub-Saharan African
languages tend to remain under-resourced, under-
documented and often also un-written.8

3 Program Description

AI4D - Artiﬁcial Intelligence for Development, is
an initiative to improve the quality of life for all
in Africa and beyond by partnering with Africa’s
science and policy communities to leverage AI
through high-quality research, responsible innova-
tion, and strengthening talent. The AI4D - African
Language Program was conceptualized as part of
a roadmap to work towards better integration of
African languages on digital platforms, in aid of
lowering the barrier of entry for African participa-
tion in the digital economy. It was organised in 3
key phases;

3.1 Language Dataset Challenges
The AI4D Language Dataset Challenges9 were
framed to focus on data collection, in response
to the challenges of low availability of input data
for African languages and the poor discoverability
of resources that do exist, thus hindering the ability
of researchers to do machine translation (Marti-
nus et al., 2020), and other NLP tasks. We put

8Interspeech 2016 Special Session
9AI4D Africa Language Challenge

together a panel of judges and performed both
qualitative and quantitative evaluations, based on
datasheets (Gebru et al., 2018) submitted with each
datasets and the datasets themselves, to identify
outstanding submissions. The two rounds of this
challenge yielded 52 dataset submissions accross
15 African languages with 13 winning prizes.

Some of the key observations identiﬁed from
this phase of the project include (Siminyu et al.,
2020):
• Teams composed of individuals from relevant
multi-disciplinary backgrounds, including com-
puter scientists, professional translators and lin-
guists, were able to create and annotate datasets
that captured fundamental lexical and semantic
nuances of languages.

• The challenge framing allowed for anyone to par-
ticipate. While useful as an exercise in evaluating
the interest in such a challenge, high quality sub-
missions came from teams who had been exposed
to NLP research work.

• Since the challenge was evaluated monthly, we
often received disparate submissions from the
same teams. Instead, one large dataset built over
a couple of months would have been the ideal
outcome.
These observations motivated the design of a

subsequent phase of the project, the Fellowship.

3.2 Language Dataset Fellowships

From the top teams that participated in the chal-
lenges, we invited nine to take part in a subsequent
phase of the program, a 3-4 month Fellowship Pro-
gram10. This provided research grants for teams
to invest in resources, gave enough time for col-
laborative consultations to determine the sizes of
expected datasets, the downstream NLP tasks they
would be annotated for as well as mentorship and
advisory requirements.

The datasets developed through this process
cover a variety of languages and NLP tasks; Ma-
chine Translation datasets (Ewe, Fonge, Yor`ub´a,
Luganda, Twi and the 11 ofﬁcial languages of
South Africa), a Text-to-Speech dataset (Wolof),
(Tunisian Ara-
a Sentiment Analysis dataset
bizi), a Keyword Spotting dataset (Luganda) and
Document Classiﬁcation datasets (Chichewa and
Kiswahili).

The Fellowship also presented a platform to
tackle some opportunities identiﬁed to support fu-
ture work in African, and low resource, language
dataset creation, including research and analysis of
the legal implications of obtaining textual, visual
and audio data from a variety of online sources, and
the development of copyright, intellectual property
and data protection guidelines for NLP researchers.
These guidelines will be published in addition
to research papers from the individual fellows on
their particular dataset development work.

3.2.1 Data sources

The teams used different methods to collect data.
Some of the main ones were :

Data scraping from online sources
Although, it is a general trend that African lan-
guages are getting less and less written, there
still exist online media content in local languages.
Some international media powerhouses such as
BBC and Voice Of America have versions of their
sites targeted at an African audience with the con-
tent exclusively in African languages. In addition,
social media remains a place where data collection
can be leveraged provided proper measures (au-
tomatic or manual) for checking correctness can
be put into place. Other interesting online sources
that were leveraged include TED talks/movies tran-
scripts, radio transcripts and software localization
texts (Adelani et al., 2021).

Translators
The most common way of generating data during
the ﬁrst phase of the program has been using the
help of local translators who have had extensive
experience translating documents encompassing a
wide range of themes and topics. This approach
was mostly followed by teams creating datasets
for neural machine translation. Data was ﬁrst gen-
erated (sentences created from scratch based on
given themes) or collected from online media out-
lets and blogs 11 and then shared with translators
for translation.

Audio recordings
Teams building text-to-speech datasets recruited
actors to record sentences from the cleaned text
datasets using Common Voice 12.

3.2.2 Challenges in data collection
There are challenges that are speciﬁc to building
dataset for low-resourced languages and that have
to be expected while attempting to build benchmark
datasets. Some of those the fellows encountered
had to do with :

The lack of standard in written scripts of the
languages
For a lot of African languages, there are no stan-
dardized grammar or orthographic rules. Worse,
with these languages being written less and
less (Sands, 2017), new orthographies have been
given to words depending on the country or on
the need of the moment. The challenge here is
that many resources are encountered each having
different orthographies for the same words and
different translations for the same types of sen-
tences. To tackle this, researchers reached out to
and worked in collaboration with national univer-
sities’ languages departments and national organi-
zations such as the Ewe Academy in Togo in order
make the corpora uniform.

The scarcity of existing resources
It is the essence of these languages being classiﬁed
as low-resource languages. Although there is a lot
of work being done notably by translators, it is not
saved and there is no history of the work that can
be accessed which makes years of work volatile.

Intellectual property issues
Beyond the legal and commercial aspects of intel-

11Consent to used the concerned articles was always sought

and obtained before proceeding to data collection

10Cracking the Language Barrier for a Multilingual Africa

12Common Voice

Challenge

Participants Leader Board

Submissions Best Score

Agricultural Keyword Spotter
for Luganda
Social Media Sentiment Analysis
for Tunisian Arabizi

Chichewa News Classiﬁcation
French to Fongbe and Ewe
Machine Translation

Yor`ub´a Machine Translation
Automatic Speech Recognition
in Wolof

713

733

567

240

426

204

255

312

208

44

59

19

3385

0.650 (Log Loss)

6674

0.95 (Accuracy)

2869

0.684 (Accuracy)

406

0.363 (BLEU)

443

0.468 (BLEU)

76

0.110 (WER)

Table 1: List of challenges and participation metrics where some data is already available. The grey cells indicate
where the metrics listed are ﬁnal as the challenge has been completed.

lectual property as it can exist related to any type
of work, what can make it particularly hard in an
African context is the fact that most authors or orga-
nizations that publish content in African languages
have little if any online presence allowing one to
reach them and request rights to use their work. In
instances where adequate contact information was
available, it was difﬁcult to obtain the permission
necessary. We therefore needed to either give up
on using these data sources at worst or incurring
delays in the work due to formalities necessary to
get access to the data.

Lack of commitment from partners
A major challenge concerning building Text-To-
Speech (TTS) datasets consisted in the difﬁculty
to ﬁnd committed actors to record the text datasets
items. The nature of TTS requires the same actor
to do all the recordings. Therefore at times, the
recordings had to be started over because of the
unavailability of the initial actors who started. The
feedback has indicated that this had a lot to do with
our inability to provide satisfying remuneration
rates for the amount of work that was required to
generate datasets large enough.

The scarcity of expert language professionals
As most African languages are less and less written
(Sands, 2017), there is a general loss of academic
interest from language students. It makes it par-
ticularly hard to encounter linguists, translators,
language teachers or other language professionals
that are up to date with the latest development of
the language. Many attempts might be necessary
in order to ﬁnd suitable language professionals to
work with. We have also noticed it is good practice
to work with a couple of different professionals in
order to cross-validate their work before accepting
it.

The reluctance of data holders to give it up
Data is the new gold and even for those who might
not have previously understood this, the sudden
interest shown in collecting data from them can
raise suspiscions as the concepts of open research
are not always clear to some of those data holders
who are not familiar to the academic world.

3.2.3 Data quality

As far as data quality is concerned, going the extra
mile to ensure diacritics were correctly written was
a major challenge as it required specialised exper-
tise, was time consuming and therefore demanded
that more funds were put into remunerating those
working with us or development of automatic di-
acritics application models (Orife, 2018) (Orife
et al., 2020a). In addition, most African languages
are now commonly spoken with the intervention
of many borrowed words from the respective coun-
tries’ ofﬁcial languages (French, English). Clean-
ing was therefore necessary to keep the text as pure
as possible. This was done using language detec-
tors and regular expressions before ﬁnalizing with
manual cleaning.

Another major quality check concerned the bal-
ance of the data. Some datasets contained a huge
amount of data of one type or source making it im-
balanced and likely to introduce bias in the models
built using it. This is a consideration that is not
speciﬁc to African language datasets and that dif-

ferent fellows have tried to remain aware of while
choosing their data sources in order to maintain a
proper balance in each dataset.

Also, in order to ensure proper classiﬁcation
categories, language professionals were involved
even in data annotation for classiﬁcation datasets.
This was in instances where only translations of
titles of news articles were available but did not
provide enough information to accurately associate
a category with the text.

3.3 Machine Learning Competitions

This ﬁnal phase involved design and split of the
datasets into train, development and test sets; the
preparation of datasheets to document the motiva-
tion, composition, collection process and recom-
mended uses of the datasets and hosting ML com-
petitions on Zindi, an African data science com-
petition platform, so as to engage the wider NLP
community.

Six competitions and one hackathon were hosted

on Zindi as follows:
• AI4D iCompass Social Media Sentiment Analy-

sis for Tunisian Arabizi13

• AI4D Yor`ub´a Machine Translation Challenge14
• AI4D Takwimu Lab - Machine Translation Chal-

lenge: French to Fongbe and Ewe15

• AI4D Chichewa News Classiﬁcation Challenge16
• AI4D Baamtu Datamation - Automatic Speech

Recognition in WOLOF17

• AI4D Swahili News Classiﬁcation Challenge 18
• GIZ NLP Agricultural Keyword Spotter for Lu-

ganda19
More challenges are to come as the data prepara-
tion gets completed. The objective of these compe-
titions is to :

- Generate benchmark models that can be built
upon by other researchers and that can assist local
languages professionals in their tasks.

- help create tools that can be used in insurance
companies, banking or social media mining to bet-
ter understand users’ feelings.

- Assist material creation for education in
African languages, bring services such as bank-
ing or healthcare closer to local languages speakers

13Social Media Sentiment Analysis for Tunisian Arabizi
14Yor`ub´a Machine Translation Challenge
15French to Fongbe and Ewe Machine Translation
16Chichewa News Classiﬁcation
17Automatic Speech Recognition in WOLOF
18Swahili News Classiﬁcation Challenge
19Agricultural Keyword Spotter for Luganda

among many other possible use cases.

4 Future Scope of the project

4.1 Achievements of the program

The program has been successful in creating a grass
roots movement of interdisciplinary researchers
and professionals across Africa to collaborate on
addressing the challenge of lack of access to data
in African languages that may inhibit the partici-
pation of people speaking these languages in the
digital economy. The program has been highlighted
at the launch of the Open for Good Alliance, an
initiative to support the development of localized
training data for AI-driven innovation20. The Al-
liance was launched in November 2020 by the In-
ternational Development Research Centre, FAIR
Forward / GIZ, Mozilla Foundation, Radiant Earth
Foundation, <A+>Alliance for Inclusive Algo-
rithms, African Institute for Mathematical Sciences,
Kwame Nkrumah University of Science and Tech-
nology, Makerere University and UNESCO with
three objectives:
• Making training datasets openly available, help-
ing existing open training datasets to be found
and supporting their maintenance.

• Facilitating the coordination and exchange of
good practices and ideas through discussions
with community members around the collection
of datasets and applications of AI, supporting
the development of standards where needed, and
share successful examples.

• Increasing the public awareness for the beneﬁts
of openly available, unbiased and localized train-
ing data.

This program will also be used as a model case
to inform evidence-based policy making concern-
ing Artiﬁcial Intelligence and will be included in
UNESCO’s AI Decision maker’s Essential to in-
form policy makers. The generated datasets have
been hosted on Zenodo in a community created
for African NLP21 where we hope to encourage
other enthusiasts to use them for Machine Trans-
lation, Text Classiﬁcation, Named Entity Recogni-
tion, overloaded terms or loan words from English
detection and also comparative vocabulary analy-
sis.

This fellowship has been an opportunity for the

20UNESCO organises workshop on strengthening multilin-
gualism through datasets in low resourced languages at IGF
2020

21African Natural Language Processing (AfricaNLP)

fellows to build networks of language profession-
als and institutions that collaboration will go on
with in order to produce better datasets in terms
of size, quality, annotations, cleanliness and diver-
sity among other criteria. More funding would be
required to take the work further as professional
expertise in African languages is becoming a rare
skill. In addition, due to the time consuming na-
ture of tasks such as translation, which can get
even more complex when dealing with languages
with diacritics, fuzzy grammar and orthographic
standards, working with African languages is ex-
pensive.

4.2 Future challenges

One major challenge moving forward will be the
creation of datasets and solutions that capture the
intrinsic multilingual nature of modern African
communication and the fact that code-switching
is very common in daily communication. Efﬁcient
NLP for African languages could look like; mixed
language processing tools that can catch up with
the democratization of English and Swahili coming
together in Kenya’s Sheng, the mixing of French
with Arabic or Berber in Algeria, or even the Ro-
manization of Amharic.22

Another challenge remains in the form of ways
in which the momentum generated by the project(s)
can be leveraged in scaling up the work, networks
and impact of the program. Possible pathways
to achieve scale require organising the network
around frameworks that can guide the community
in developing and sustaining data commons.

4.2.1 Data Commons Frameworks

In this respect, a Data Commons Framework would
be one relevant model to organize future work
around. There have been various frameworks pro-
posed, such as this Data Commons Version 1.0
Framework23 proposed by the Berkman Klein Cen-
tre which is intended to build towards AI for Good.
Expanding access to data is crucial for the devel-
opment of machine learning and AI. Yet even as
data commons develop, other concerns related to
representativeness of the data, discrimination and
openness need to be addressed.

22African languages are being left behind when it comes to

voice recognition innovation

23Data Commons Version 1.0: A Framework to Build To-

ward AI for Good

4.2.2 Framework for participatory

development of language datasets

In order to successfully scale up, the programme
must expand its reach among different actors en-
gaged in language development to tap into inter-
disciplinary knowledge and resources. As a tool
for participatory language technology development,
the Masakhane research community in (Nekoto
et al., 2020) details a framework it has developed
with entry points for content creators, translators,
curators, language technologists and evaluators to
work together in addressing some of the challenges
of language development. Such a framework would
enable the program to focus on efforts in expand-
ing the scope of its research in terms of quality,
quantity and impact as well as involve other stake-
holders in the language ecosystem to take part in
the development stages of AI language tools.

4.2.3 Near term needs of the programme

Near term requirements over a period of the next 1
to 2 years include:
• Funding support to sustain the fellowships and
expand the work on datasets so as to enable the
development of minimum viable products based
on these datasets

• Funding support to develop technical infrastruc-
ture for hosting the data commons, including
to ensure that the data is Findable, Accessible,
Interoperable and Reusable in line with the FAIR
principles24

• Research on legal and policy issues concerning
data protection, data sharing and transfer within
different jurisdictions to strengthen the collabo-
rative work through the program

• Enlarging the network of stakeholders in-
volved in the project to facilitate greater multi-
stakeholder cooperation

• Mentoring and engaging young researchers
through continent wide hackathons, workshops,
trainings and conferences

• Advocacy towards governments and policymak-
ers to garner support for the programme at the
national, regional and global level.

Acknowledgements

This work has been sponsored through a partner-
ship between several organisations, listed below in
alphabetical order;
• The AI4D Africa Initiative

24FAIR Principles

• The Centre for Intellectual Property and Informa-
tion Technology(CIPIT), Strathmore University
• The Data Science for Social Impact Research

Group, University of Pretoria

• Deutsche Gesellschaft f¨ur Internationale Zusam-

Osei, Freshia Sackey, et al. 2020.
Participatory
research for low-resourced machine translation: A
In Proceedings
case study in african languages.
of the 2020 Conference on Empirical Methods in
Natural Language Processing: Findings, pages
2144–2160.

Iroro Orife, David I. Adelani, Timi Fasubaa, Victor
Williamson, Wuraola Fisayo Oyewusi, Olamilekan
Wahab, and Kola Tubosun. 2020a.
Improving
Yor`ub´a Diacritic Restoration.

Iroro Orife, Julia Kreutzer, Blessing Sibanda, Daniel
Whitenack, Kathleen Siminyu, Laura Martinus,
Jamiil Toure Ali, Jade Abbott, Vukosi Marivate,
Salomon Kabongo, et al. 2020b. Masakhane–
arXiv preprint
machine translation for africa.
arXiv:2003.11529.

Iroro Fred `O. n`o. m`e. Orife. 2018. Sequence-to-Sequence
Learning for Automatic Yor`ub´a Diacritic Restora-
tion. In Proceedings of the Interspeech, pages 27–
35.

Adama Ouane and Christine Glanz. 2010. How and
why Africa should invest in African languages and
multilingual education: an evidence-and practice-
based policy advocacy brief. UNESCO Institute for
Lifelong Learning.

Bonny Sands. 2017. The challenge of documenting
africa’s least known languages. Africa’s endan-
gered languages: Documentary and theoretical ap-
proaches, pages 11–38.

Bonny Sands and B Sands. 2018. Language revitaliza-
tion in africa. The Oxford Handbook of Endangered
Languages, pages 613–636.

Kathleen Siminyu, Sackey Freshia,
and Vukosi Marivate. 2020.
language dataset challenge.
arXiv:2007.11865.

Jade Abbott,
Ai4d–african
arXiv preprint

menarbeit(GIZ)

• The International Development Research Cen-

tre(IDRC)

• The Knowledge 4 All Foundation(K4All)
• Masakhane
• The Swedish International Development Cooper-

ation Agency(Sida)

• The United Nations Educational, Scientiﬁc and

Cultural Organization(UNESCO)

• Zindi Africa

References

Gilles Adda, Sebastian St¨uker, Martine Adda-Decker,
Odette Ambouroue, Laurent Besacier, David Bla-
chon, H´elene Bonneau-Maynard, Pierre Godard, Fa-
tima Hamlaoui, Dmitry Idiatov, et al. 2016. Break-
ing the unwritten language barrier: The bulb project.
Procedia Computer Science, 81:8–14.

David I. Adelani, Dana Ruiter,

Jesujoba O. Al-
abi, Damilola Adebonojo, Adesina Ayeni, Mofe
Adeyemi, Ayodele Awokoya, and Cristina Espa˜na-
Bonet. 2021. MENYO-20k: A Multi-domain
English-Yor`ub´a Corpus for Machine Translation and
Domain Adaptation.

ˇZeljko Agic and Ivan Vulic. 2020.

Jw300: A wide-
coverage parallel corpus for low-resource languages.

Guy De Pauw, Gilles-Maurice De Schryver, Laurette
Pretorius, and Lori Levin. 2011. Introduction to the
special issue on african language technology. Lan-
guage Resources and Evaluation, 45(3):263–269.

Timnit Gebru, Jamie Morgenstern, Briana Vecchione,
Jennifer Wortman Vaughan, Hanna Wallach, Hal
Daum´e III, and Kate Crawford. 2018. Datasheets
for datasets. arXiv preprint arXiv:1803.09010.

Xianhong Hu, Bhanu Neupane, Lucia Flores Echaiz,
Prateek Sibal, and Macarena Rivera Lam. 2019.
Steering AI and advanced ICTs for knowledge so-
cieties: a Rights, Openness, Access, and Multi-
stakeholder Perspective. UNESCO Publishing.

Laura Martinus, Jason Webster, Joanne Moonsamy,
Moses Shaba Jnr, Ridha Moosa, and Robert
Neural machine translation for
Fairon. 2020.
arXiv preprint
south africa’s ofﬁcial languages.
arXiv:2005.06609.

Wilhelmina Nekoto, Vukosi Marivate, Tshinondiwa
Matsila, Timi Fasubaa, Taiwo Fagbohungbe,
Solomon Oluwole Akinola, Shamsuddeen Muham-
mad, Salomon Kabongo Kabenamualu, Salomey

