1
2
0
2

n
a
J

1

]

C
O
.
h
t
a
m

[

1
v
6
3
2
0
0
.
1
0
1
2
:
v
i
X
r
a

SVRG for SDP

On Stochastic Variance Reduced Gradient Method for
Semideﬁnite Optimization

jinshanzeng@jxnu.edu.cn
Jinshan Zeng
School of Computer and Information Engineering and Institute of Artiﬁcial Intelligence, Jiangxi
Normal University, Nanchang, China

Yixuan Zha
School of Computer and Information Engineering, Jiangxi Normal University, Nanchang, China
Ke Ma∗
make@iie.ac.cn
Institute of Computing, Chinese Academy of Sciences, and University of Chinese Academy of Sci-
ences, Beijing, China

zyx@jxnu.edu.cn

Yuan Yao∗
Department of Mathematics, Chemical and Biomolecular Engineering, Hong Kong University of
Science and Technology, Hong Kong.

yuany@ust.hk

Editor: xxx

Abstract

The low-rank stochastic semideﬁnite optimization has attracted rising attention due to its
wide range of applications. An eﬀective way to solve this problem is based on the low-rank
factorization, which signiﬁcantly improves the computational eﬃciency but brings some
great challenge to the analysis due to the recast nonconvex reformulation. Among existing
methods based on the nonconvex reformulation, the stochastic variance reduced gradient
(SVRG) method has been regarded as one of the most eﬀective methods. SVRG in general
consists of two loops, where a reference full gradient is ﬁrstly evaluated in the outer loop
and then used to yield a variance reduced estimate of the current gradient in the inner loop.
Two options have been suggested to yield the output of the inner loop, where Option I sets
the output as its last iterate, and Option II yields the output via selecting randomly from
all the iterates in the inner loop. However, there is a signiﬁcant gap between the theory and
practice of SVRG when adapted to the stochastic semideﬁnite programming (SDP). SVRG
practically works better with Option I, while most of existing theoretical results focus on
Option II. In this paper, we ﬁll this gap via exploiting a new semi-stochastic variant of the
inner loop in the original SVRG, adapted to the semideﬁnite optimization. Equipped with
this, we establish the global linear submanifold convergence (i.e., converging exponentially
fast to a submanifold of a global minimum under the orthogonal group action) of the
proposed SVRG method with Option I, given a provable initialization scheme and under
certain smoothness and restricted strongly convex assumptions. Our analysis includes
the eﬀects of the mini-batch size and update frequency in the inner loop as well as two
practical step size strategies, the ﬁxed step size and stabilized Barzilai-Borwein step sizes.
Some numerical results in matrix sensing demonstrate that the proposed SVRG method
is similar to the original SVRG with Option I, and outperforms their counterparts with
Option II as well as stochastic gradient descent and factored gradient descent methods.
The eﬀects of algorithmic parameters are also studied numerically. Not only this paper

∗. Corresponding author

1

 
 
 
 
 
 
Zeng, Zha, Ma, and Yao

establishes the convergence of SVRG with Option I for semi-deﬁnite optimization, but also
the new technical development in the convergence analysis is of value to general low-rank
matrix learning problems.
Keywords: SVRG, semideﬁnite programming, variance reduction, low-rank factorization,
linear convergence

1. Introduction

There are many applications in scientiﬁc research and engineering involving the following
low-rank stochastic convex semideﬁnite optimization problem:

min
X∈Sp

f (X) =

1
n

n
(cid:88)

i=1

fi(X)

s.t. X (cid:23) 0,

(1)

where fi(X) is some convex, smooth cost function associated with the i-th sample, X (cid:23) 0
is the positive semideﬁnite (PSD) constraint, and Sp represents the set of real symmetric
matrices of size p × p. In many related applications, certain low-rank assumption is usu-
ally imposed on X. Typical applications include the non-metric multidimensional scaling
(Agarwal et al., 2007; Borg and Groenen, 2005), matrix sensing (Jain et al., 2013; Tu et al.,
2016), phase retrieval (Candes et al., 2015a), synchronization (Bandeira et al., 2016), and
community detection (Montanari and Sen, 2016).

The classical algorithms for solving problem (1) mainly include the ﬁrst-order methods
such as the well-known projected gradient descent method (Nestrov and Nemirovski, 1989),
interior point method (Alizadeh, 1995), and more specialized path-following interior point
methods (for more detail, see the nice survey paper (Monteiro, 2003) and references therein).
However, most of these methods are not well-scalable due to the PSD constraint, i.e., X (cid:23) 0.
To break the hurdle of PSD constraint, the idea of low-rank factorization was adopted in
the literature (Burer and Monteiro, 2003, 2005) and became very popular in the past ﬁve
years due to its empirical success (Bhojanapalli et al., 2016; Jin et al., 2016; Ma et al., 2018,
2019; Wang et al., 2017c; Zeng et al., 2018, 2019).

Mathematically, let X = U U T for some rectangular matrix U ∈ Rp×r, X ∗ be a global
optimum of problem (1) with rank r (where r < p), and g(U ) := f (U U T ), then problem
(1) can be recast as

min
U ∈Rp×r

g(U ) := f (U U T ).

(2)

Since the PSD constraint has been eliminated, the recast problem (2) has a signiﬁcant
advantage over (1), but this beneﬁt has a corresponding cost: the objective function is
no longer convex but instead nonconvex in general. This brings a great challenge to the
analysis. Even for the simple ﬁrst-order methods like the factored gradient descent (FGD)
and gradient descent (GD), its local linear convergence remains unspeciﬁed until the recent
work in (Bhojanapalli et al., 2016; Wang et al., 2017b), respectively. Moreover, facing
the challenge in large scale applications with a big n, stochastic algorithms (Robbins and
Monro, 1951) have been widely adopted nowadays, that is, at each iteration, we only use the
gradient information of one or a small batch of the whole sample instead of the full gradient
over n samples. However, due to the existence of variance of such stochastic gradients, the
stochastic gradient descent (SGD) method either converges to an O(η) neighborhood of a

2

SVRG for SDP

global optimum at a linear rate when adopting a ﬁxed step size η, or has a local sublinear
convergence rate when adopting a diminishing step size, in the restricted strongly convex
(RSC) case (Zeng et al., 2019).

To accelerate SGD, various variance reduction techniques have been proposed in the
literature, e.g. the stochastic variance reduced gradient (SVRG) method in (Johnson and
Zhang, 2013) and stochastic average gradient (SAG) method in (Schmidt et al., 2017),
which resume the linear convergence for strongly convex problems in Euclidean space. One
distinction of the SVRG proposed by Johnson and Zhang (2013) lies in that it does not
need to memorize the gradients over training sample set;
instead it involves two loops
of iterations for updating full-sample gradients and stochastic variations. Its algorithmic
implementations give rise to two choices: Option I, where one naturally sets the output
of the inner loop as its last iterate; Option II, where one selects the output of the inner
loop from all the iterates in the inner loop in a uniformly random way, more expensive in
memory cost on recording inner loop iterates. Although the paper (Johnson and Zhang,
2013) established Option II’s linear convergence guarantee, it leaves open the convergence
of Option I, in spite of a more natural and eﬃcient scheme in practice. This gap was later
ﬁlled by (Tan et al., 2016) with an introduction of Barzilai-Borwein (BB) step size (Barzilai
and Borwein, 1988).

When adapted to the the nonconvex matrix problem, linear convergence results are only
known for Option II. Zhang et al. (2017) studied the SVRG with Option II for the matrix
sensing problem with the square loss and established its linear submanifold convergence,
(i.e., converges exponentially fast to a submanifold as the orbit of a global optimum under
the orthogonal group action) under certain restricted isometry property (RIP). This work
was generalized by Wang et al. (2017c) with a variant of SVRG with Option II called
Stochastic Variance-Reduced Gradient for Low-Rank Matrix Recovery (dubbed SVRG-LR
for short) for the low-rank matrix recovery problem with general loss and the Restricted
Strongly Convex (RSC) condition.

Yet, for Option I scheme in nonconvex low-rank matrix recovery, convergence theories
In fact, Option I is more natural and usually enjoys a faster em-
remain largely open.
pirical loss reduction than Option II used in SVRG (see Johnson and Zhang (2013); Tan
et al. (2016); Sebbouh et al. (2019) for example in Euclidean space and also Figure 1 when
adapted to the matrix sensing case considered in our later numerical experiments in Section
5). Moreover, the memory storage required in the implementation of SVRG with Option
I (called SVRG-I henceforth) is generally much less than that of SVRG with Option II
(called SVRG-II henceforth), since it only needs to store one iterate of the inner loop when
implementing SVRG-I, instead of all iterates of the inner loop required in the implementa-
tion of SVRG-II. Therefore, it is desired to establish the convergence of Option I and its
variants due to these advantages.

Particularly when the matrix variable X is positive semi-deﬁnite, several recent studies
provided partial answers to the convergence problem of Option I. First, Ma et al. (2018,
2019) adapted the original SVRG with Option I to the application of ordinal embedding
(Agarwal et al., 2007) based on the low-rank factorization reformulation. In these works,
a generalization of the well-known Barzilai-Borwein (BB) step size (Barzilai and Borwein,
1988) called the stabilized Barzilai-Borwein (SBB) step size was also introduced for SVRG to
alleviate the challenge of tuning the step size parameter, where the BB step size is a special

3

Zeng, Zha, Ma, and Yao

Figure 1: Comparison on the performance of SVRG methods with two options for the low-
rank matrix sensing problem, where SVRG-I and SVRG-II are respectively the
original SVRG with Option I and II, and SVRG-LR and SVRG-SDP are respec-
tively the modiﬁed SVRG using a semi-stochastic gradient with Option II and I.
The speciﬁc settings can be found in Section 5. It can be observed that SVRG
methods equipped with Option I is superior to their Option II counterparts.

case of SBB step size. Under certain smoothness assumption, the convergence to a critical
point of the SVRG with Option I and SBB step size was ﬁrstly established in Ma et al.
(2018, 2019); then its local linear point convergence (i.e., exponentially fast convergence
to a global minimum in Euclidean distance starting from a neighborhood of this global
minimum) was further established in Zeng et al. (2018) under the RSC condition. However,
the submanifold convergence of SVRG with Option I is still left open, an important problem
as a global optimum is invariant under the orthogonal group action.

In this paper, we ﬁll this gap by providing a global linear submanifold convergence theory
of Stochastic Variance-Reduced Gradient method for Semi-deﬁnite optimization (2), using a
variant of Option I, under general loss and RSC with the ﬁxed step size and general stabilized
Barzilai-Borwein step size. To achieve this, motivated by Wang et al. (2017a), we adopted
the following new semi-stochastic gradient (i.e., ∇fit(X t)U t − (∇fit( ˜X k) − ∇f ( ˜X k))U t)
in place of the original one in the inner loop of SVRG (i.e., ∇fit(X t)U t − (∇fit( ˜X k) −
∇f ( ˜X k)) ˜U k, called SVRG-I in this paper), see Algorithm 1 for detail. Thus, our algorithm
is called Stochastic Variance-Reduced Gradient for SDP (dubbed SVRG-SDP for short).
Distinguished to SVRG-LR, we adopt Option I instead of Option II by selecting the output
of inner loop as the last iterate ( ˜U k+1 = U m), as well as the mini-batch strategy. Under the
regular Lipschitz gradient and restricted strongly convex assumptions, we at ﬁrst establish a
local linear submanifold convergence of the proposed SVRG method (see Theorem 1), where
the radius of the initial ball permitted to guarantee the linear submanifold convergence is
larger than existing results established in the literature (Bhojanapalli et al., 2016; Zhang
et al., 2017; Wang et al., 2017a; Zeng et al., 2018, 2019), as listed in Table 1. Then, we boost

4

050100150200250300350400450500Epoch10-3010-2510-2010-1510-1010-5100Relative recovery errorSVRG-IISVRG-ISVRG-LRSVRG-SDPSVRG for SDP

such local linear submanifold convergence to the global linear submanifold convergence1
(see Theorem 5) with an appropriate initial scheme based on the projected gradient descent
method (see Algorithm 2). Moreover, we establish the same global linear submanifold
convergence results for the proposed SVRG methods equipped with two practical step size
schemes, i.e., the ﬁxed step size and certain stabilized Barzilai-Borwein step size introduced
in Ma et al. (2018, 2019). A series of experiments in matrix sensing are conducted to show
the eﬀectiveness of the proposed method. The numerical results show that the proposed
method performs similarly to original SVRG with Option I yet with provable convergence
guarantee, and works remarkably better than their counterparts with Option II, i.e., SVRG-
LR and SVRG-II, as well as the factored gradient descent and stochastic gradient descent
methods. The eﬀects of the mini-batch size, update frequency of the inner loop, and step
size schemes are also discussed and studied in both theory and experiment.

In contrast to the existing analysis for SVRG with Option II, the main diﬃculty in
dealing with Option I is to yield the contraction between two successive iterates in the
outer loop, since the iterates in the inner loop may change largely during the update of
Option I. Such possibly dramatic changes among iterates in the inner loop can be alleviated
in Option II where the output of the inner loop is a uniformly random sample of iterates,
by taking expectation or average over the iterates of the inner loop in convergence analysis.
Hence it is relatively easy to tackle the case of Option II in analysis. However when we
switch to Option I where the output of the inner loop is set as the last iterate, one has to
carefully develop some new technique to handle the change in the inner loop to avoid being
out of control.

Our key development in the convergence analysis is a novel second-order descent lemma
(see Lemma 6) for SVRG-SDP about the progress made by a single iterate of the inner loop,
F , where U ∗
characterized by the submanifold metric (i.e., E(U, U ∗
r
is a global minimum with rank r, O is the set of orthogonal matrices of size r × r). This
improves the second-order descent lemma in (Zeng et al., 2018, Lemma 1) with Euclidean
metric (i.e., ˜E(U, U ∗
F ) and leads to the desired linear submanifold convergence
of SVRG-SDP.

r ) := minR∈O (cid:107)U − U ∗

r ) := (cid:107)U −U ∗

r R(cid:107)2

r (cid:107)2

The rest of this paper is organized as follows. In Section 2, we introduce the proposed
SVRG method, together with an initialization scheme and some diﬀerent step size strategies.
In Section 3, we establish the linear submanifold convergence of the proposed method. In
Section 4, we present a key lemma as well as the proof for our main theorem. A series
of experiments are provided in Section 5 to demonstrate the eﬀectiveness of the proposed
method as well as the eﬀects of algorithmic parameters. We conclude this paper in Section
6. Part of proofs are presented in Appendix.

For any two matrices X, Y ∈ Rp×p, their inner product is deﬁned as (cid:104)X, Y (cid:105) = tr(X T Y ).
For any matrix X ∈ Rp×p, (cid:107)X(cid:107)F and (cid:107)X(cid:107)2 denote its Frobenius and spectral norms,
respectively, and σmin(X) and σmax(X) denote the smallest and largest strictly positive
singular values of X, denote τ (X) := σmax(X)
σmin(X) , with a slight abuse of notation, we also use
σ1(X) ≡ σmax(X) ≡ (cid:107)X(cid:107)2. Ip denotes the identity matrix with the size p × p. We will omit
the subscript p of Ip if there is no confusion in the context.

1. that is, exponentially fast convergence to a submanifold of the orthogonal group action orbit of a global
minimum starting from an initial point that is not necessarily close to the global minimum submanifold.

5

Zeng, Zha, Ma, and Yao

Table 1: Comparisons on linear convergence guarantees between this paper and existing literature,
where κ is the “condition number” of the objective function f (to be speciﬁed in (7)),
σr(X ∗) and τ (X ∗) are respectively the r-th largest singular value and the condition number
of the optimum X ∗, and r = rank(X ∗). Particularly, κ = 1 in the square loss case as
considered in Zhang et al. (2017). The larger bound on E( ˜U 0, U ∗
r ) implies a larger initial
ball permitted to guarantee the linear convergence. † The linear convergence of SVRG-I
established in Zeng et al. (2018) is in the sense of point convergence. From this table,
the initial ball permitted to guarantee the linear submanifold convergence of SVRG-SDP
established in this paper is slightly larger than existing ones.

Algorithm

loss

assumption

FGD (Bhojanapalli et al. (2016))

general

RSC

SGD (Zeng et al. (2019))

general

SVRG-II (Zhang et al. (2017))

square

SVRG-I (Zeng et al. (2018))†

general

SVRG-LR (Wang et al. (2017a))

general

SVRG-SDP (this paper)

general

RSC

RIP

RSC

RSC

RSC

E( ˜U 0, U ∗
r )
σr(X ∗)
10000κ2τ 2(X ∗)
√
(

2−1)σr(X ∗)

√

(2+

3)·(

√

2κ
σr(X ∗)
16
2−1)σr(X ∗)
6κ
2σr(X ∗)
15κ

√

8(

2−1)σr(X ∗)

9κ

2. A Stochastic Variance Reduced Gradient Scheme for SDP

The SVRG method was ﬁrstly proposed by Johnson and Zhang (2013) for minimizing a
ﬁnite sum of convex functions with a vector argument. The main idea of SVRG is adopting
the variance reduction technique to accelerate SGD and achieves a faster convergence rate.
SVRG is an inner-outer loop based method. The main purpose of inner loop is to reduce
the variance introduced by the stochastic sampling, and thus accelerate the convergence of
the outer loop iterates. In this section, we propose a new version of SVRG with Option I
adapted to Semideﬁnie Programming which enjoys provable convergence guarantees.

2.1 SVRG-SDP with Option I: Algorithm 1

Inspired by Ma et al. (2018, 2019) and Wang et al. (2017c), we propose a new variant
of SVRG method with Option I to solve the stochastic SDP problem (1) based on its
nonconvex reformulation (2), as described in Algorithm 1. Compared to SVRG-I suggested
in (Ma et al., 2018, 2019) for problem (2), a new semi-stochastic gradient

∇fit(X t)U t − (∇fit( ˜X k) − ∇f ( ˜X k))U t

(3)

is exploited in the inner loop to replace the original ∇fit(X t)U t − (∇fit( ˜X k) − ∇f ( ˜X k)) ˜U k
for the variance reduced estimate of the current gradient, where ˜X k is the estimate at the
k-th outer loop, ˜U k is the associated factorization of ˜X k (i.e., ˜X k = ˜U k( ˜U k)T ), and U t
Intuitively, due to the use of the latest iterate U t
is the t-th iterate in the inner loop.
in the inner loop, the new semi-stochastic gradient should be more accurate than the old

6

SVRG for SDP

Algorithm 1 Stochastic Variance-Reduced Gradient for SDP problem (SVRG-SDP )

Parameters: an update frequency m ≥ 1 of the inner loop, a mini-batch size b ≥ 1, a sequence
of positive step sizes {ηk}, an initial point ˜U 0 ∈ Rp×r.
for k = 0, 1, . . . do
T
˜X k := ˜U k( ˜U k)
U 0 = ˜U k
for t = 0, . . . , m − 1 do

X t = U tU tT
Pick It ⊂ {1, . . . , n} with |It| = b via a uniformly random way
(∇fit(X t) − ∇fit( ˜X k)) + ∇f ( ˜X k)]U t
U t+1 = U t − ηk[ 1
b

(cid:80)

it∈It

end for
˜U k+1 = U m

end for

one that mixed U t and ˜U k, resulting in a possible better performance than SVRG-I as
demonstrated by our later numerical experiments. More importantly, the use of such a new
semi-stochastic gradient is essential for deriving the linear convergence in the submanifold
metric E(U, U ∗
F , as shown in the following convergence analysis.
Besides the Option I such that ˜U k+1 = U m, we also adopt the mini-batch strategy to SVRG-
SDP, which (though obtainable) is missing in (Wang et al., 2017a; Zeng et al., 2018).

r ) := minR∈O (cid:107)U − U ∗

r R(cid:107)2

When m = 1 or b = n, SVRG-SDP reduces to the known factored gradient descent
(FGD) method studied in Bhojanapalli et al. (2016). This shows that our proposed algo-
rithmic framework provides more ﬂexible choices for the users.

2.2 Initialization Procedure: Algorithm 2

Note that the recast problem (2) is no longer convex, thus the choice of initialization is very
important in the implementations of these algorithms for the low-rank matrix estimation
as demonstrated in (Bhojanapalli et al., 2016; Wang et al., 2017a,b; Zhang et al., 2017;
Ma et al., 2018, 2019; Zeng et al., 2018, 2019). One of commonly used strategies is to
construct the initialization directly from the observed data like in the applications of matrix
sensing, matrix completion and phase retrieval (say, Candes et al. (2015b); Jain et al. (2013);
Netrapalli et al. (2013); Zheng and Laﬀerty (2015)). Such a strategy is generally eﬀective for
the case that the objective function has a small “condition number”, while for the general
objective functions as considered in this paper, another common strategy is to use one
of the standard convex algorithms (say, projected gradient descent (ProjGD) (Nestrov and
Nemirovski, 1989)). Some speciﬁc implementations of this idea have been used in (Tu et al.,
2016; Bhojanapalli et al., 2016; Wang et al., 2017a,b; Zeng et al., 2019).

Motivated by the existing literature, this paper also suggests using the ProjGD method
to generate the initialization ˜U 0 for the general nonlinear loss function as described in
+, the set of symmetric positive
Algorithm 2, where ProjSp
+
semideﬁnite matrices of size p × p.

represents the projection onto Sp

7

Zeng, Zha, Ma, and Yao

Algorithm 2 Initialization for SVRG-SDP

Let X 0 = 0, T be the number of iterations and L be a Lipschit constant of ∇f .
for t = 1, 2, . . . , T do

X t = ProjSp

(X t−1 − 1

L ∇f (X t−1))

+

end for
Let ˜X 0 = X T , and ˜U 0 be a rank-r factorization of ˜X 0.

2.3 Fixed and Stabilized Barzilai-Borwein Step Sizes

Besides the issue of the initial choice, another important implementation issue of SVRG is
the tuning of the step size. There are mainly two classes of step sizes: deterministic or data
adaptive. Here we discuss three particular choices.

(a) Fixed step size (Johnson and Zhang, 2013):

ηk ≡ η,

for some η > 0.

(4)

(b) Barzilai-Borwein (BB) step size (Barzilai and Borwein, 1988; Tan et al., 2016): given

an initial η0 > 0 and for k ≥ 1, let ˜gk := ∇f ( ˜X k),

ηk =

1
m

·

(cid:107) ˜X k − ˜X k−1(cid:107)2
F
|(cid:104) ˜X k − ˜X k−1, ˜gk − ˜gk−1(cid:105)|

.

(5)

Note that such a BB step size is originally studied for strongly convex objective
functions (Tan et al., 2016), and it may be breakout if there is no guarantee of the
curvature of f like in nonconvex cases (Ma et al., 2018). In order to avoid such possible
instability of (5) in our studies, a variant of BB step size, called the stabilized BB step
size, is suggested by Ma et al. (2018, 2019) shown as follows.

(c) Stabilized BB (SBB) step size (Ma et al., 2018, 2019): given an initial η0 > 0 and an

(cid:15) ≥ 0, for k ≥ 1,

ηk =

1
m

·

(cid:107) ˜X k − ˜X k−1(cid:107)2
F
|(cid:104) ˜X k − ˜X k−1, ˜gk − ˜gk−1(cid:105)| + (cid:15)(cid:107) ˜X k − ˜X k−1(cid:107)2
F

.

(6)

Note that the BB step size is a special case of SBB step size with (cid:15) = 0, thus we call
the BB step size as SBB0.

Besides the above three step sizes, there are some other schemes like the diminishing
step size and the use of smoothing technique in BB step size as discussed in Tan et al.
(2016). However, we mainly focus on the listed two step sizes (as BB is a special case
SBB0) in this paper due to their established eﬀectiveness in wide applications.

3. Convergence Theory of SVRG-SDP

In this section, we present the linear submanifold convergence of the proposed SVRG-SDP.

8

SVRG for SDP

3.1 Assumptions and Convergence Metric

To present our main convergence results, we need the following assumptions.

Assumption 1 Suppose that

(a) each fi (i = 1, . . . , n) is L-Lipschitz diﬀerentiable for some constant L > 0, i.e., fi is

smooth and ∇fi is Lipschitz continuous satisfying

(cid:107)∇fi(X) − ∇fi(Y )(cid:107)F ≤ L(cid:107)X − Y (cid:107)F , ∀X, Y ∈ Sp
+.

(b) f is (µ, r)-restricted strongly convex (RSC) for some constants µ > 0 and r ≤ p, i.e.,

for any X, Y ∈ Sp

+ with rank r,

f (Y ) ≥ f (X) + (cid:104)∇f (X), Y − X(cid:105) +

µ
2

(cid:107)Y − X(cid:107)2
F .

The above assumptions are regular and commonly used in the literature (see, Bhojana-
palli et al. (2016); Wang et al. (2017a,c); Zeng et al. (2019)). Assumption 1(a) implies that
f is also L-Lipschitz diﬀerentiable. For any L-Lipschitz diﬀerentiable and (µ, r)-restricted
strongly convex function h, the following hold (Nestrov (2004)):

h(Y ) ≤ h(X) + (cid:104)∇h(X), Y − X(cid:105) +

L
2

(cid:107)Y − X(cid:107)2
F ,

µ(cid:107)X − Y (cid:107)2

F ≤ (cid:104)∇h(X) − ∇h(Y ), X − Y (cid:105) ≤ L(cid:107)X − Y (cid:107)2
F ,

where the ﬁrst inequality holds for any X, Y ∈ Sp
+, and the second inequality holds for any
X, Y ∈ Sp
+ with rank r, the ﬁrst inequality and the right-hand side of the second inequality
hold for the Lipschitz continuity of ∇h, and the left-hand side of the second inequality is
due to the (µ, r)-restricted strong convexity of h. Under Assumption 1, let

κ :=

L
µ

,

√

γ0 := (

2 − 1)κ−1,

(7)

where κ ≥ 1 is generally called the condition number of the objective function.

In order to characterize the submanifold convergence of SVRG-SDP, we use the following

orthogonally invariant metric to measure the gap between U and V ,

E(U, V ) := min
R∈O

(cid:107)U − V R(cid:107)2
F ,

∀ U, V ∈ Rp×r,

where O is the set of orthogonal matrices of size r × r. Such metric has been widely used in
the convergence analysis of low-rank factorization based algorithms in the literature (e.g.,
Bhojanapalli et al. (2016); Tu et al. (2016); Wang et al. (2017a,c); Zeng et al. (2019)). Com-
pared to the Euclidean metric ˜E(U t, U ∗
r ) := (cid:107)U t − U ∗
F used in the convergence analysis of
SVRG-I in Zeng et al. (2018), such an orthogonally invariant metric E(U, V ) is more desired
since a global minimum of the low-rank stochastic semideﬁnite programming problem (1)
is naturally invariant in the loss after an orthogonal transform on its factorizations.

r (cid:107)2

9

Zeng, Zha, Ma, and Yao

3.2 Local Linear Submanifold Convergence

Let X ∗ be a global optimum of problem (1) with rank r = rank(X ∗), and U ∗
rank-r decomposition of X ∗ (i.e., X ∗ = U ∗
and we deﬁne the following constants:

r )T ). Let Nγ0 := {U : E(U, U ∗

r (U ∗

r ∈ Rp×r be a
r ) ≤ γ0σr(X ∗)},

B := max
U ∈Nγ0

(cid:107)U U T (cid:107)F ,

ηmax :=

54(2 +

γ0σr(X ∗)
√

γ0)2L(cid:107)X ∗(cid:107)2B

.

(8)

(9)

Let {ηk}k∈N be a sequence satisfying ηk ∈ (0, bηmax] for some mini-batch size b satisfying

1 ≤ b ≤ min

n, 54(

(cid:110)

√

2 + 1)κτ (X ∗)

(cid:111)

,

where τ (X ∗) = (cid:107)X ∗(cid:107)2

σr(X ∗) . Given a positive integer m, deﬁne

ρk :=

1
2

(cid:20)

(cid:18)

1 +

1 −

2
27

(cid:19)m(cid:21)

ηkLγ0σr(X ∗)

.

By (9) and (10), there holds

and thus,

ηk ≤ bηmax ≤

1
4BL

,

2
27

ηkLγ0σr(X ∗) ≤

√

2 − 1
54κτ (X ∗)

< 1,

(10)

(11)

(12)

(13)

where the second inequality holds for the deﬁnition of B (implying B ≥ (cid:107)X ∗(cid:107)2). The above
inequality shows that ρk ∈ (0, 1). Based on the above deﬁned constants, we present our
main theorem on the local linear submanifold convergence of the proposed SVRG-SDP as
follows, where its proof is postponed in Section 4.

Theorem 1 (Local linear submanifold convergence) Let { ˜U k} be a sequence gener-
ated by Algorithm 1. Suppose that Assumption 1 holds with 1 < κ ≤ 64(
2 − 1). Let
If the initialization ˜U 0 satisﬁes
m ≥ 1, b satisfy (10), and ηk ∈ (0, bηmax] for k ∈ N.
0 < E( ˜U 0, U ∗

, then for any positive integer k, there holds

2−1)σr(X ∗)

√

√

r ) < 8(

9κ

E[E( ˜U k, U ∗

r )] ≤





k−1
(cid:89)


 · E( ˜U 0, U ∗
r ).

ρj

(14)

j=0

Particularly, if a ﬁxed step size η ∈ (0, bηmax] is used, then the above inequality implies the
following linear convergence,

E[E( ˜U k, U ∗

r )] ≤

(cid:20)

(cid:18) 1
2

(cid:18)

1 +

1 −

2ηLγ0σr(X ∗)
27

(cid:19)m(cid:21)(cid:19)k

E( ˜U 0, U ∗

r ).

(15)

10

SVRG for SDP

√

9κ

Theorem 1 establishes the local linear submanifold convergence of the proposed SVRG
method for problem (2) under the smoothness and RSC assumptions. According to Theorem
2−1)σr(X ∗)
1, the radius of the initial ball permitted to guarantee the linear convergence is 8(
,
which is slightly better than the existing results as presented in Table 1 and in some sense
tight by (Zeng et al., 2019, Proposition 2), where a counter example is provided such that
FGD cannot converge to the global optimum once the initialization radius is not smaller
than σr(X ∗

r ). In the following, we provide some detailed comparisons between them.
Under the same smoothness and RSC assumptions, similar local linear submanifold
convergence of SVRG-LR was established in (Wang et al., 2017a, Theorem 4.7) in the
framework of both statistical and optimization frameworks. We provide some remarks on
comparisons between these two results in the following. At ﬁrst, according to (Wang et al.,
2017a, Theorem 4.7) and the discussion in (Wang et al., 2017a, Remark 4.8), the provable
radius of the initialization ball is 2σr(X ∗)
for SVRG-LR, which is smaller than that required
for the proposed SVRG-SDP in this paper. In this sense, the convergence conditions in this
paper are weaker than those used in Wang et al. (2017a). Secondly, from the discussion in
(Wang et al., 2017a, Remark 4.8), the contraction parameter ρ associated with SVRG-LR
lies in the interval [ 2
6 ] when m = O(τ 2(X ∗)), while by (11), the contraction parameter ρk
associated with our proposed SVRG-SDP (approaching to 1/2) shall be smaller than 2
3 if
a moderately large m is adopted. This implies in some sense that the proposed version of
SVRG as an Option I generally converges faster than SVRG-LR, the Option II counterpart
for problem (2). Moreover, note that for any positive integer m, ρk deﬁned in (11) is always
less than 1. This means we have no requirement on m to guarantee the linear convergence
of SVRG-SDP, while a suﬃciently large m (at least in the order of O(τ 2(X ∗))) is required
to guarantee the linear convergence of SVRG-LR studied in Wang et al. (2017a). Also, the
convergence analysis of mini-batch version of SVRG-LR (though obtainable) is missing in
the literature (Wang et al., 2017a).

3 , 5

15κ

In Algorithm 1, when m = 1, SVRG-SDP reduces to FGD studied in Bhojanapalli et al.
(2016). Under the similar smoothness and RSC assumptions, the local linear convergence
of FGD was ﬁrstly established in Bhojanapalli et al. (2016) if the initialization lies in the
ball E( ˜U 0, U ∗
10000κ2τ 2(X ∗) , and later the radius of the initialization ball was improved
to (
in Zeng et al. (2019). Notice that Theorem 1 above also holds for the case
of m = 1. This shows that the provable radius of initialization ball for FGD is further
improved to 8(

2−1)σr(X ∗)

2−1)σr(X ∗)

r ) ≤

σr(X ∗)

2κ

√

√

.

9κ

Compared to the stochastic version of FGD, i.e., stochastic gradient descent (SGD)
method studied in Zeng et al. (2019), the convergence rate of SVRG-SDP is linear while
that of SGD is sublinear when adopting a diminishing step size. As shown in Table 1, the
initialization ball for SVRG-SDP is also slightly larger than that of SGD with a ﬁxed step
size in Zeng et al. (2019), while in this case, SGD only converges to a O(η)-neighborhood
of a global minimum, where η is the used ﬁxed step size.

Compared to SVRG-I studied in Zeng et al. (2018), we establish the linear submanifold
convergence, a more precise characterization in the low-rank matrix factorization setting,
instead of point convergence for SVRG-SDP. The convergence conditions used in this paper
are also weaker than those in Zeng et al. (2018) in the following two aspects. The ﬁrst one
is the weaker assumption on the objective function f . From (Zeng et al., 2018, Assumption

11

Zeng, Zha, Ma, and Yao

1(b)), each fi is required to be (µ, r)-restricted strongly convex; while in this paper, we
only require the (µ, r)-restricted strong convexity of the average function f = 1
i=1 fi
n
as shown in Assumption 1(b), which is a more realistic condition used in the literature
(Bhojanapalli et al., 2016; Wang et al., 2017a,b,c; Zeng et al., 2019). The second one is
that the initialization radius is improved from (2+
8(

in this paper, as shown in Table 1. Moreover, there are lack of theoretical
guarantees on the initialization schemes (though obtainable) in Zeng et al. (2018), while
in this paper, we ﬁll this gap and provide the theoretical guarantees for the suggested
initialization scheme as shown in the following Proposition 4.

in Zeng et al. (2018) to

2−1)σr(X ∗)
6κ

2−1)σr(X ∗
r )

(cid:80)n

3)·(

9κ

√

√

√

Remark 2 (Inﬂuence of batch size and update frequency) Note that the range of
2 + 1)κτ (X ∗), where
the mini-batch size b is very ﬂexible, since its upper bound is 54(
κ and τ (X ∗) are generally far more than 1. If the particular ﬁxed step size ηk ≡ bηmax is
adopted in Algorithm 1, then (15) implies

√

E[E( ˜U k, U ∗

r )] ≤

(cid:20)

(cid:18) 1
2

(cid:18)

1 +

1 −

2ηmaxLγ0σr(X ∗)
27

(cid:19)m(cid:21)(cid:19)k
b

E( ˜U 0, U ∗

r ).

(16)

Regardless of the memory storage, the above inequality shows that a larger mini-batch size b
adopted implies a faster convergence speed of the iterates yielded in the outer loop of SVRG-
SDP, as long as b is smaller than the upper bound speciﬁed in (10). Similar claim also holds
for the choice of update frequency m in the inner loop by the above inequality since the base
1 − 2ηmaxLγ0σr(X ∗)
b is positive and less than 1 under the choice of b in (10). In order to yield
a fast convergence speed, a moderately large m is usually required in the implementation of
SVRG-SDP, as also implied by (16). They are reasonable since more gradient information
is exploited when a large mini-batch size or update frequency is adopted in the inner loop
with other ﬁxed parameters.

27

Besides the concerned linear convergence of the iterates yielded in the outer loop, it is
also important to estimate the computational complexity in terms of the amount of gradient
information used to achieve a prescribed precision. Speciﬁcally, given a precision (cid:15) > 0,
the above inequality gives an estimate of the computational complexity of SVRG-SDP as
follows:

C((cid:15), b, m) ≥

bm
C(b, m)

· log

(cid:15)
E( ˜U 0, U ∗
r )

,

(cid:17)m(cid:105)
b

(cid:104)
1 +

(cid:16)

27

1 − 2ηmaxLγ0σr(X ∗)

where C(b, m) := log 1
2

. On one hand, for some moderately
log 2 log E( ˜U 0,U ∗
r )
large m, the above computational complexity C((cid:15), b, m) approximates to bm
,
which shows that a smaller mini-batch size generally implies a lower computational com-
plexity of SVRG-SDP to achieve a prescribed precision. On the other hand, for some ﬁxed
b and considering some moderately large m, the above estimate of computational complexity
also implies that a smaller m leads to a lower computational complexity. These are also
veriﬁed by our later numerical experiments in Section 5.

(cid:15)

In the next, we give a corollary to show the linear convergence of SVRG-SDP when
adopting the considered SBB step size (6), where BB step size is its special case with (cid:15) = 0.

12

SVRG for SDP

Corollary 3 (Convergence of SVRG-SDP with SBB step size) Suppose that the as-
sumptions of Theorem 1 hold and that m ≥
for any (cid:15) ≥ 0. Then the convergence
claim in Theorem 1 also holds for SVRG-SDP equipped with the SBB step size in (6) .

1
(µ+(cid:15))bηmax

By the deﬁnition of SBB step size (6) and Assumption 1, we have

1
m(L + (cid:15))

≤ ηk ≤

1
m(µ + (cid:15))

.

Thus, if m ≥
in Theorem 1 holds for SVRG-SDP equipped with SBB step size.

, then ηk ≤ bηmax for any k ∈ N. This implies the convergence claim

1
(µ+(cid:15))bηmax

3.3 Global Linear Convergence with Provable Initial Scheme

By Theorem 1, the radius of the initialization ball plays a central role in the establishment
of local linear convergence of SVRG-SDP. Thus, it is crucial to show whether such proper
initial point can be easily achieved in practice. In the next, we show that such a desired
initial point can be indeed achieved by the suggested initialization scheme (see Algorithm
2) with the logarithmic computational complexity.

Proposition 4 Let Assumption 1 hold with 1 < κ ≤ 64(
Algorithm 2. If

√

2 − 1). Let ˜U 0 be yielded by

T ≥ log1−κ−1

16(

then E( ˜U 0, U ∗

r ) ≤ 8(

√

2−1)σr(X ∗)

9κ

.

√

r (X ∗)

2 − 1)2σ2
9κ(cid:107)X ∗(cid:107)2
F

,

(17)

Similar results of Proposition 4 have been shown in Wang et al. (2017b) for the gradient
descent method and in Zeng et al. (2019) for the stochastic gradient descent method. Ac-
cording to (Wang et al., 2017b, Theorem 5.7), the condition on κ is κ ∈ (1, 4/3), while the
requirement in Proposition 4 is κ ∈ (1, 64(
2 − 1)], which signiﬁcantly relaxes the condition
in Wang et al. (2017b). According to (Zeng et al., 2019, Proposition 1), the requirements on
κ of both papers are the same. However, the requirement on the radius of initialization ball
in this paper is slightly weaker that of Zeng et al. (2019), where the radius is improved from
√
2−1)σr(X ∗)
(

2−1)σr(X ∗)

√

√

in this paper. The proof of Proposition

in Zeng et al. (2019) to 8(

2κ

4 is provided in Appendix C.

9κ

Based on Proposition 4, we can boost the local linear convergence of SVRG-SDP shown

in Theorem 1 to the following global linear convergence.

√

Theorem 5 (Global linear convergence) Suppose that Assumption 1 holds with 1 <
2 − 1). Let { ˜U k} be a sequence generated by Algorithm 1 with ˜U 0 via the initial
κ ≤ 64(
scheme in Algorithm 2 (where T satisﬁes (17)), b satisfying (10), m ≥ 1, and ηk ∈ (0, bηmax]
for k ∈ N. Then { ˜U k} converges to a global minimum exponentially fast.

The proof of this theorem is summarized as follows. By Theorem 1, we show that
the proposed SVRG method converges to a global minimum exponentially fast starting

13

Zeng, Zha, Ma, and Yao

from an initial guess close to this global minimum, and then according to Proposition 4,
we show that the suggested initialization algorithm (see Algorithm 2) can ﬁnd the desired
initial guess permitted to the linear convergence with an order of logarithmic computational
complexity, starting from the trivial origin point. In other words, the convergence speed
of the suggested initial scheme in Algorithm 2 is also linear to reach the desired initial
precision. Therefore, combining Proposition 4 with the local linear convergence in Theorem
1, the whole convergence speed of SVRG-SDP equipped with such initial scheme is linear
starting from the origin point. This implies the global linear convergence of SVRG-SDP in
Theorem 5.

4. Second-Order Descent Lemma and Proof of Theorem 1

In this section, we present the key proofs of our main theorem (i.e., Theorem 1). The proof
idea is motivated by Zeng et al. (2018) with an extension. Speciﬁcally, to prove Theorem
1, we need the following second-order descent lemma which estimates the progress made by
a single iterate of the inner loop and is characterized in the manifold metric instead of the
Euclidean metric as in (Zeng et al., 2018, Lemma 1).

Lemma 6 (Second-order descent lemma) Let {U t}m
ner loop. Let Assumptions 1 hold with 1 < κ ≤ 64(
ηk ∈ (0, bηmax) for any k ∈ N, where ηmax is speciﬁed in (9). If 0 < E( ˜U k, U ∗
r ) < γ0σr(X ∗) with γ0 speciﬁed in (7), then
and 0 < E(U t, U ∗

t=0 be the sequence at the k-th in-
2 − 1). Let b satisfy (10), and let
r ) < γ0σr(X ∗)

√

EIt[E(U t+1, U ∗

r )] ≤ (cid:0)1 − ηkLγ0σr(X ∗) + η2

+ ηkLE 2(U t, U ∗

r ) + η2

(cid:1) · E(U t, U ∗
kB1
r )
kB1E( ˜U k, U ∗
r ),

(18)

where B1 = 2

b (2 +

√

γ0)2L2(cid:107)X ∗(cid:107)2B.

We call this lemma as Second-order descent lemma since both linear and quadratic
terms of E(U t, U ∗
r ) are involved in the upper bound in the right-hand side of (18). This is
in general diﬀerent from the literature (say, Johnson and Zhang (2013); Tan et al. (2016))
to yield the linear convergence of SVRG methods.

Proof [Proof of Lemma 6] Let vt
F , and V ∗
arg minR∈O (cid:107)U t − U ∗

r R(cid:107)2

(cid:80)
k := 1
b
r R∗
U t := U ∗

it∈It
U t. Note that

(∇fit(X t) − ∇fit( ˜X k)) + ∇f ( ˜X k), R∗

U t =

r R(cid:107)2
F

E(U t+1, U ∗

(cid:107)U t+1 − U ∗

r ) = min
R∈O
U t(cid:107)2
F
F + 2(cid:104)U t+1 − U t, U t − V ∗
U t(cid:105) + η2
kU t, U t − V ∗

≤ (cid:107)U t+1 − V ∗
U t(cid:107)2
= (cid:107)U t − V ∗
r ) − 2ηk(cid:104)vt
= E(U t, U ∗

U t(cid:105) + (cid:107)U t+1 − U t(cid:107)2
F
kU t(cid:107)2
k(cid:107)vt
F ,

which implies

EIt[E(U t+1, U ∗

r )] ≤ E(U t, U ∗

r ) − 2ηkEIt[(cid:104)vt

kU t, U t − V ∗

U t(cid:105)] + η2
k

14

EIt[(cid:107)vt

kU t(cid:107)2

F ].

(19)

SVRG for SDP

The bounds of both 2EIt[(cid:104)vt

F ] can be estimated respec-
tively by Lemma 11 and Lema 12 whose proofs are given in Appendix B. Combining these
bounds ((21) and (25) in Appendix B) for (19) yields

U t(cid:105)] and EIt[(cid:107)vt

kU t, U t − V ∗

kU t(cid:107)2

EIt[E(U t+1, U ∗

r )]

√

r ) − ηk(

≤ E(U t, U ∗

r ) + ηkLE 2(U t, U ∗
√

γ0)2L2(cid:107)X ∗(cid:107)2BE(U t, U ∗

(2 +

+ η2
k

2
b
≤ E(U t, U ∗
(
= (cid:0)1 − ηkLγ0σr(X ∗
r ) + η2
√

r ) − ηk

√

(cid:16)

r ) + η2
k

2
b
2 − 1)µσr(X ∗) − ηkB1
(cid:1) · E(U t, U ∗

kB1

where B1 := 2
and the ﬁnal equality holds for γ0 = (

γ0)2L2(cid:107)X ∗

b (2 +

√

− ηkB)(cid:107)PU t∇f (X t)(cid:107)2
F

2 − 1)µσr(X ∗)E(U t, U ∗
√

γ0)2L2(cid:107)X ∗

r ) − ηk(

1
4L
r (cid:107)2BE( ˜U k, U ∗
r )

(2 +
(cid:17)

E(U t, U ∗

r ) + η2

kB1E( ˜U k, U ∗

r ) + ηkLE 2(U t, U ∗
r )

r ) + ηkLE 2(U t, U ∗

r ) + η2

kB1E( ˜U k, U ∗
r ),

r (cid:107)2B, the second inequality holds for ηk ≤ 1

4BL by (12), and

2−1)µ
L

. Thus, the above inequality yields (18).

Equipped with Lemma 6, we are able to present the proof of Theorem 1 as follows.

Proof [Proof of Theorem 1] We prove this theorem by induction. We ﬁrstly develop the
contraction between two iterates of the outer loop based on Lemma 6, and then establish
the locally linear convergence recursively.

√

We assume that at the k-th inner loop, E( ˜U k, U ∗
2−1)σr(X ∗
8(
r )
8
9 γ0σr(X ∗) < γ0σr(X ∗

r ). This implies

9κ

for k ≥ 1 and 1 ≤ t ≤ m, then (18) still holds due to 8(

√

r ) < 8(

2−1)σr(X ∗
r )

9κ

and E(U t, U ∗
r ) <
2−1)σr(X ∗
r )
=

√

9κ

EIt[E(U t+1, U ∗

r )]

(cid:18)

≤

1 −

(cid:18)

≤

1 −

1
9
1
9

ηkLγ0σr(X ∗) + η2

kB1

ηkLγ0σr(X ∗) + η2

kB1

(cid:19)

(cid:19)

· E(U t, U ∗

r ) + η2

kB1E( ˜U k, U ∗
r )
(cid:18) 1
9

1
2

· E(U t, U ∗

r ) +

ηkLγ0σr(X ∗) − η2

kB1

(cid:19)

E( ˜U k, U ∗
r )

where the ﬁrst inequality holds for E(U t, U ∗
bγ0σr(X ∗
r )
kB1 ≤ 1
γ0)2(cid:107)X ∗(cid:107)2LB (implying η2
holds for ηk ≤
√
2 E( ˜U k, U ∗
− 1
r ) to both sides of the above inequality yields

r ) < 8

54(2+

9 γ0σr(X ∗

r ), and the second inequality
27 ηkLγ0σr(X ∗)). Adding the term

EIt[E(U t+1, U ∗

(cid:18)

≤

1 −

(cid:18)

≤

1 −

1
9
2
27

E( ˜U k, U ∗
r )

r )] −

1
2
ηkLγ0σr(X ∗) + η2
(cid:19) (cid:18)

kB1

ηkLγ0σr(X ∗)

(cid:19) (cid:18)

E(U t, U ∗

r ) −

(cid:19)

E( ˜U k, U ∗
r )

1
2

E(U t, U ∗

r ) −

(cid:19)

E( ˜U k, U ∗
r )

,

1
2

(20)

where the second inequality holds by η2
k ∈ N, 1 − 2

27 ηkLγ0σr(X ∗) again. Note that for any
27 ηkLγ0σr(X ∗) ∈ (0, 1) by (13) and U 0 = ˜U k at the k-th inner loop, then based

kB1 ≤ 1

15

Zeng, Zha, Ma, and Yao

on (20), we have

E[E( ˜U k+1, U ∗

r )] ≤

(cid:18)

(cid:18)

1 +

1 −

1
2

2
27

(cid:19)m(cid:19)

ηkLγ0σr(X ∗)

E( ˜U k, U ∗

r ),

which implies for any positive integer k

E[E( ˜U k, U ∗

r )] ≤





k−1
(cid:89)


 · E( ˜U 0, U ∗
r ),

ρj

where ρj := 1
2

(cid:0)1 + (cid:0)1 − 2

27 ηjLγ0σr(X ∗)(cid:1)m(cid:1). This ﬁnishes the proof of the theorem.

j=0

5. Numerical Experiments

In this section, we conduct a series of experiments in matrix sensing to show the eﬀectiveness
of the proposed method as well as the eﬀects of the associated parametric parameters
including the choices of step size, mini-batch size and update frequency involved in the
inner loop.

5.1 Experimental settings

The following matrix sensing problem

min
X(cid:23)0

f (X) :=

1
2n

n
(cid:88)

i=1

(yi − (cid:104)Ai, X(cid:105))2,

is considered in this paper, where X ∈ Sp is a low-rank matrix with rank r < p, Ai ∈ Rp×p
is a sub-Gaussian independent measurement matrix of the i-th sample, yi ∈ R, and n ∈ N
is the sample size.

Speciﬁcally, in these experiments, we let p = 100, the optimal matrix X ∗ = U ∗(U ∗)T be a
low-rank matrix with rank 5 and the sample size n = 10p. In order to demonstrate the eﬀec-
tiveness of the proposed method, we consider the following batch and stochastic methods as
competitors: factored gradient descent (FGD) method (Jain et al., 2013), stochastic gradient
descent (SGD) methods (Zeng et al., 2019) with a ﬁxed step size (SGD-ﬁx) or diminishing
step sizes (SGD-diminish), SVRG with with Option I (SVRG-I, where the update in the in-
ner loop of Algorithm 1 is the traditional one, ∇fit(X t)U t − (∇fit( ˜X k) − ∇f ( ˜X k)) ˜U k), and
SVRG-LR (Wang et al., 2017a) which is of Option II. For all algorithms, we take r = r∗ and
construct the initialization empirically via 10 iterations of the projected gradient descent
as similar to (Zeng et al., 2019, Section V.A). For FGD, we use a step size η =
,
where L is the Lipschitz constant of ∇f (X). For SGD, we set a more consecutive ﬁxed step
size ¯η =
k+1 . For SVRG type of methods, we set
b = 1, m = n and η =
. All these parametric settings are set either theoretically or
empirically optimal.

and a diminishing step size ηk = ¯η

1
4L(cid:107)X ∗(cid:107)F

1
8L(cid:107)X ∗(cid:107)F

1
4L(cid:107)X ∗(cid:107)F

There are four experiments conducted in this section. The ﬁrst three experiments are
conducted to show the eﬀects of algorithmic parameters including the choice of mini-batch

16

SVRG for SDP

(a) Convergence speed

(b) Computational complexity

Figure 2: Eﬀect of mini-batch size for SVRG-SDP. (a) Convergence speed of iterates yielded
in the outer loop of SVRG-SDP, (b) Recovery error with respect to running time.

size, update frequency and step size strategy involved in the inner loop, while the last
experiment is implemented to demonstrate the eﬀectiveness of the proposed method via
comparing with these batch and stochastic methods presented in the previous.
In all experiments, we use the relative recovery error deﬁned by (cid:107) ˜X k−X ∗(cid:107)2

as the evalua-
tion metric, where ˜X k is the estimate yielded at the k-th iteration of outer loop as presented
in Algorithm 1.

(cid:107)X ∗(cid:107)2
F

F

5.2 Eﬀect of mini-batch size

In this experiment, we study the eﬀect of mini-batch size. Under the same settings as
i.e.,
described in Section 5.1, we particularly consider ﬁve choices of mini-batch sizes,
{1, 2, 5, 8, 10, 20} for the proposed method. We set the update frequency m = n for all
ﬁve choices of mini-batch size. The experiment results are shown in Figure 2. From Figure
2(a), the proposed method exhibits the linear convergence for all the choices of mini-batch
size, which veriﬁes the established theoretical results in Theorem 1 and Theorem 5. Partic-
ularly, concerning the convergence speed of iterates yielded in the outer loop of SVRG-SDP,
a larger mini-batch size generally implies a faster convergence speed. This veriﬁes our the-
oretical discussion in Remark 2. Moreover, we also show the eﬀect of the mini-batch size
to the computational complexity of the proposed method in terms of running time. As
demonstrated by Figure 2(b), a smaller mini-batch size leads to lower computational com-
plexity to achieve the same recovery precision. This also coincides with our computational
complexity analysis as presented in Remark 2. Motivated by this experiment, we set the
mini-batch size of SVRG-SDP to be 1 in the following experiments.

5.3 Eﬀect of update frequency

We conduct this experiment to study the eﬀect of update frequency under the same settings
described in Section 5.1. Speciﬁcally, we consider ﬁve choices of update frequency, i.e.,

17

020406080100120140160180200Number of outer loop iterations10-3010-2510-2010-1510-1010-5100Relative recovery errorb=1b=5b=8b=10b=200100200300400500600700Running time (second)10-3010-2510-2010-1510-1010-5100Relative recovery errorb=1b=5b=8b=10b=20Zeng, Zha, Ma, and Yao

(a) Convergence speed

(b) Computational complexity

Figure 3: Eﬀect of update frequency of the inner loop for SVRG-SDP. (a) Convergence
speed of iterates yielded in the outer loop of SVRG-SDP, (b) Recovery error with
respect to running time.

4 , n

8 , n

m ∈ { n
2 , n, 2n}, where n is the sample size. The eﬀects of the choice of update
frequency to the convergence speed of iterates yielded in the outer loop of SVRG-SDP and
computational complexity are presented in Figure 3. From Figure 3(a), a larger m implies
a faster convergence speed of the iterates yielded in the outer loop of SVRG-SDP, which
coincides with the analysis in Remark 2. From Figure 3(b), the performance of SVRG-SDP
with m = n is the best among all these ﬁve choices in the sense that the computational
complexity of SVRG-SDP with m = n is the lowest to achieve a given recovery precision,
where the performance of SVRG-SDP with m = n/2 is very close to that of SVRG-SDP
with m = n. This also coincides with our analysis in Remark 2 and the empirical study for
SVRG in the literature Ma et al. (2019). Thus, in the later experiments, we ﬁx m = n in
the implementation of SVRG-SDP.

5.4 Performance of diﬀerent step sizes

1
4L(cid:107)X ∗(cid:107)F

We compare the performance of SVRG-SDP with two diﬀerent types of step sizes, i.e.,
ﬁxed and SBB step sizes under the same experimental settings as described in Section
5.1. Speciﬁcally, we consider a ﬁxed step size η =
and three SBB step sizes with
(cid:15) = 0, 10−10, 10−5, where SBB with (cid:15) = 0 is exactly the BB step size (Barzilai and Borwein,
1988). For SVRG-SDP, we set m = n and b = 1 for all schemes of step sizes, motivated
by the previous experiments. The experiment result is shown in Figure 4, where the epoch
number in the horizontal axis is deﬁned as the number of rounds that the total n gradients
are used. Since m = n and b = 1 in this experiment, an epoch of SVRG-SDP only includes
one iteration of the outer loop. By Figure 4, the performance of SVRG-SDP equipped with
the ﬁxed step size is slightly better than concerned SVRG-SDP with both SBB and BB step
sizes. When concerning the performance of SBB step size with a small (cid:15), the parameter (cid:15)
has little eﬀect on the performance of the proposed method.

18

050100150200250300350400450500Number of outer loop iterations10-3010-2510-2010-1510-1010-5100Relative recovery errorn/8n/4n/2n2n050100150200250300350400450500Running time (second)10-3010-2510-2010-1510-1010-5100Relative recovery errorn/8n/4n/2n2nSVRG for SDP

Figure 4: Performance of SVRG-SDP with diﬀerent step sizes.

5.5 Comparison with other algorithms

In this experiment, we demonstrate the eﬀectiveness of SVRG-SDP via comparing to the
other bath and stochastic methods including FGD (Jain et al., 2013), SGD-ﬁx (Zeng et al.,
2019), SGD-diminish, SVRG-I (Ma et al., 2018, 2019) and SVRG-LR (Wang et al., 2017a).
The experiment settings are presented in Section 5.1. The performance of these algorithms
is shown in Figure 5. From Figure 5, all these SVRG type methods have the linear conver-
gence and signiﬁcantly outperform FGD and SGD-diminish. This justiﬁes the established
theoretical results for SVRG methods in the literature and this paper. In particular, the
proposed SVRG-SDP outperforms the existing batch and stochastic methods, where the
performance of SVRG-I is very close or slightly inferior to that of SVRG-SDP. However,
the convergence of SVRG-I is not known guaranteed. Moreover, when compared to SVRG-
LR, the Option II counterpart of SVRG-SDP proposed in Wang et al. (2017a), our proposed
SVRG-SDP works much better than SVRG-LR in terms of the convergence speed. These
show the advantage of Option I scheme and particularly the eﬀectiveness of our proposed
method.

6. Conclusion

In this paper, we propose an eﬃcient SVRG method with Option I (i.e., the output of
inner loop is chosen as the last iterate) called SVRG-SDP for the stochastic semideﬁnite
optimization problem involved in many applications, via exploiting a new semi-stochastic
gradient in the update of inner loop. The local linear submanifold convergence of the
proposed version of SVRG method is established under the regular smoothness and RSC
assumptions of the objective function at a proper initial choice. The provable radius of
the initialization ball yielded in this paper is larger than those in the existing literature.
The local linear convergence of the proposed method can be boosted to the global linear
convergence when adopting an eﬃcient and provable initialization scheme. Moreover, several
step size schemes including the ﬁxed, BB and SBB step sizes are included in the proposed

19

020406080100120140160180200Epoch10-3010-2510-2010-1510-1010-5100Relative recovery errorfix ( = 6.25e-4)sbb ( =0)sbb (=1e-10)sbb (=1e-5)Zeng, Zha, Ma, and Yao

Figure 5: Comparison on the performance of diﬀerent algorithms.

method with provable guarantees. To demonstrate the eﬀectiveness of the proposed method,
a series of experiments in matrix sensing are conducted to compare both batch and stochastic
algorithms, where the eﬀects of algorithmic parameters include the mini-batch size, update
frequency and step size scheme studied theoretically and numerically. The proposed SVRG
method and its associated theoretical results established in this paper ﬁll in an existing gap
between the theory and practice of SVRG methods.

Appendix A. Preliminary Lemmas

We provide some preliminary lemmas, which are frequently used in the later proofs. All
these lemmas are either directly taken or slightly adapted from the literature.

Lemma 7 (Lemma 5.4 in Tu et al. (2016)) For any U, V ∈ Rp×r, then we have

(cid:107)U U T − V V T (cid:107)2

F ≥ 2(

√

2 − 1)σ2

r (V ) · E(U, V ).

Lemma 8 (Lemma 3 in Zeng et al. (2019)) For any U ∈ Rp×r, let X = U U T .
E(U, U ∗

r ) ≤ γσr(X ∗) for some constant 0 < γ < 1, then

σr(X) ≥ (1 −

√

γ)2σr(X ∗).

Lemma 9 (Lemma 4 in Zeng et al. (2019)) For any U ∈ Rp×r, let X = U U T .
E(U, U ∗

r ) ≤ γσr(X ∗) for some constant 0 < γ < 1, then
√

√

(cid:107)X − X ∗(cid:107)F ≤ (2 +

γ)(cid:107)U ∗

r (cid:107)2E 1/2(U, U ∗

r ) ≤ (2

γ + γ) · τ (U ∗

r ) · σr(X ∗),

If

If

where τ (U ∗

r ) := σ1(U ∗
r )
r ) .
σr(U ∗

For any matrix U ∈ Rp×r, let QU be a basis of the column space of U . Denote PU :=
U . Then PU U = U . For any matrix Y ∈ Rp×p, PU Y is a projection of Y onto the same

QU QT

20

050100150200250300350400450500Epoch10-3010-2510-2010-1510-1010-5100Relative recovery errorFGDSGD-diminishSGD-fixSVRG-ISVRG-LRSVRG-SDPSVRG for SDP

subspace spanned by X := U U T . The following lemma is a special case of (Zeng et al.,
2019, Lemma 5) with the exact rank r of the global optimum X ∗ (implying ∇f (X ∗) = 0)
2 − 1) (where the proof is shown in (Zeng et al., 2019,
and ¯η = 1 implied by 1 < κ ≤ 64(
Appendix E)).

√

Lemma 10 Let Assumption 1 hold with 1 < κ ≤ 64(
r ) < γ0σr(X ∗
X = U U T . If E(U, U ∗

r ), where γ0 = (

√

√

2 − 1). For any U ∈ Rp×r, let

2 − 1)κ−1, then the following hold:

(a) (cid:107)∇f (X)(cid:107)F ≤ (2

γ0 + γ0)Lτ (U ∗

r )σr(X ∗),

√

(b) ¯X := X − 1

L PU ∇f (X)PU is symmetric and positive semideﬁnite with rank r,

(c) (I − PU )X ∗ = 0.

Appendix B. Some Key Lemmas

Here, we develop two lemmas to bound these two terms, i.e., 2Eit[(cid:104)vt
Eit[(cid:107)vt

F ] in (19), respectively.

kU t(cid:107)2

kU t, U t − V ∗

U t(cid:105)] and

Appendix B.1. Bound 2Eit[(cid:104)vt

kU t, U t − V ∗

U t(cid:105)]

Lemma 11 Let Assumption 1 hold with 1 < κ ≤ 64(
U U T , QU be a basis of the column space of U , and PU := QU QT
with γ0 = (

2 − 1)κ−1, there holds

√

2 − 1). For any U ∈ Rp×r, let X =
r ) < γ0σr(X ∗)

U . If E(U t, U ∗

√

2EIt[(cid:104)vt
√

kU t, U t − V ∗
2 − 1)µσr(X ∗)E(U t, U ∗

U t(cid:105)]

≥ (

r ) − LE 2(U t, U ∗

r ) +

1
4L

(cid:107)PU t∇f (X t)(cid:107)2
F .

Proof Note that

kU t, U t − V ∗

U t(U t)T (cid:105)
2EIt[(cid:104)vt
= (cid:104)∇f (X t), X t − X ∗(cid:105) + (cid:104)∇f (X t), X t + X ∗ − 2V ∗

U t(cid:105)] = 2(cid:104)∇f (X t), X t − V ∗

U t(U t)T (cid:105).

(21)

(22)

In the following, we respectively establish the lower bounds of these two terms in the right-
hand side of the above equality.

(a) Lower bound of (cid:104)∇f (X t), X t − X ∗

r (cid:105), we utilize the
following two inequalities mainly by the L-smoothness and (µ, r)-restricted strong convexity
of f , that is,

r (cid:105). To bound (cid:104)∇f (X t), X t − X ∗

(i) f (X ∗) ≥ f (X t) + (cid:104)∇f (X t), X ∗ − X t(cid:105) +

(ii) f (X t) ≥ f (X ∗) +

1
2L

· (cid:107)PU t∇f (X t)(cid:107)2
F ,

µ
2

(cid:107)X ∗ − X t(cid:107)2
F ,

21

Zeng, Zha, Ma, and Yao

where (i) holds for the (µ, r)-restricted strong convexity of f , (ii) holds for the following
inequality induced by the L-smoothness of f , i.e.,

f (X t) ≥ f ( ¯X t) + (cid:104)∇f (X t), X t − ¯X t(cid:105) −

L
2

(cid:107)X t − ¯X t(cid:107)2
F

(where ¯X t := X t −

1
L

PU t∇f (X t)PU t)

1
2L

= f ( ¯X t) +

(cid:107)PU t∇f (X t)(cid:107)2
F
(∵ (cid:104)∇f (X t), PU t∇f (X t)PU t(cid:105) = (cid:107)PU t∇f (X t)(cid:107)2

F ),

and f ( ¯X t) ≥ f (X ∗) since X ∗ is an optimum and ¯X t is a feasible point by Lemma 10(b).
Summing the inequalities (i)-(ii) yields

(cid:104)∇f (X t), X t − X ∗(cid:105) ≥

µ
2

(cid:107)X t − X ∗(cid:107)2

F +

1
2L

(cid:107)PU t∇f (X t)(cid:107)2
F

√

≥ (

2 − 1)µσr(X ∗)E(U t, U ∗

r ) +

1
2L

(cid:107)PU t∇f (X t)(cid:107)2
F ,

(23)

where the second inequality is due to Lemma 7, i.e., (cid:107)X t−X ∗(cid:107)2

F ≥ 2(

2−1)σr(X ∗)E(U t, U ∗

r ).

(b) Lower bound of (cid:104)∇f (X t), X t + X ∗ − 2V ∗

U t(U t)T (cid:105). Note that

√

U t(U t)T (cid:105)

(cid:104)∇f (X t), X t + X ∗ − 2V ∗
= (cid:104)PU t∇f (X t) + (I − PU t)∇f (X t), X t + X ∗ − 2V ∗
= (cid:104)PU t∇f (X t), X t + X ∗ − 2V ∗
= (cid:104)PU t∇f (X t), (U t − V ∗

U t(U t)T (cid:105)
U t)T (cid:105)

U t(U t)T (cid:105)

≥ −

1
4L

(cid:107)PU t∇f (X t)(cid:107)2

U t)(U t − V ∗
F − LE 2(U t, U ∗

r ),

(24)

U t(U t)T (cid:105) =
where the second equality is due to (cid:104)(I−PU t)∇f (X t), X t(cid:105) = 0, (cid:104)(I−PU t)∇f (X t), V ∗
0 and (cid:104)(I − PU t)∇f (X t), X ∗(cid:105) = 0 by (I − PU t)U t = 0 and Lemma 10(c), and the third
T , and the inequality holds
equality holds for X ∗ = U ∗
for the basic inequality: (cid:104)Y, Z(cid:105) ≥ − δ

r R∗
2 (cid:107)Y (cid:107)2
Thus, substituting (24) and (23) into (22) yields the claim of this lemma.

F for any Y, Z ∈ Rp×p and δ = 1
2L .

r R∗
2δ (cid:107)Z(cid:107)2

U t)(U ∗
F − 1

U t)T = V ∗

T = (U ∗

U tV ∗
U t

r U ∗
r

B.2. Bound EIt[(cid:107)vt
Lemma 12 Let Assumption 1 hold. Assume that E(U t, U ∗
γ0σr(X ∗), then

kU t(cid:107)2
F ]

r ) < γ0σr(X ∗) and E( ˜U k, U ∗

r ) <

EIt[(cid:107)vt

kU t(cid:107)2

F ] ≤

√

(2 +

2
b

γ0)2L2(cid:107)X ∗(cid:107)2B(E(U t, U ∗

r ) + E( ˜U k, U ∗

r )) + (cid:107)PU t∇f (X t)(cid:107)2

F B, (25)

where γ0 = (

√

2 − 1)κ−1, and B is speciﬁed in (8).

22

SVRG for SDP

(cid:16)

(cid:17)
∇fit(X t) − ∇fit( ˜X k) + ∇f ( ˜X k)

U t, then {ξit}it∈It are independent
Proof Let ξit :=
and identically distributed (i.i.d) with Eit[ξit] = ∇f (X t)U t. Note that the deﬁnition of vt
k,
we have

EIt[(cid:107)vt

kU t(cid:107)2

F ] =

1
b2

EIt[(cid:107)

(cid:88)

it∈It

ξit(cid:107)2
F ]

(cid:32)

1
b2

=

=

EIt[(cid:107)

(cid:88)

it∈It

ξit − b∇f (X t)U t(cid:107)2

F ] + b2(cid:107)∇f (X t)U t(cid:107)2
F

(cid:33)

1
b

Eit[(cid:107)ξit − ∇f (X t)U t(cid:107)2

F ] + (cid:107)∇f (X t)U t(cid:107)2
F .

(26)

where the second equality holds for E[(cid:107)ξ(cid:107)2] = E[(cid:107)ξ−Eξ(cid:107)2]+(cid:107)Eξ(cid:107)2 for some random variable
ξ, and the ﬁnal equality holds for E[(cid:107) (cid:80)b
i=1(ξi − Eξi)(cid:107)2] = bE[(cid:107)ξi − Eξi(cid:107)2] for b i.i.d. random
variables. Note that

Eit[(cid:107)ξit − ∇f (X t)U t(cid:107)2
F ]
= Eit[(cid:107)(∇fit(X t) − ∇fit( ˜X k))U t − (∇f (X t) − ∇f ( ˜X k))U t(cid:107)2
F ]
≤ Eit[(cid:107)(∇fit(X t) − ∇fit( ˜X k))U t(cid:107)2
F ]
≤ L2(cid:107)X t − ˜X k(cid:107)2
≤ 2L2((cid:107)X t − X ∗(cid:107)2

F + (cid:107) ˜X k − X ∗(cid:107)2

F (cid:107)X t(cid:107)2

F )B,

(27)

where the ﬁrst inequality holds for E[(cid:107)ξ − Eξ(cid:107)2] ≤ E[(cid:107)ξ(cid:107)2] for any random variable ξ, the
second inequality holds for Assumption 1 and the norm inequality (cid:107)ABT (cid:107)F ≤ (cid:107)A(cid:107)2(cid:107)B(cid:107)F
for any two matrices A, B of the same sizes with A being full-column rank, and the third
inequality holds for the basic inequality (a − c)2 ≤ 2(a − d)2 + 2(c − d)2 and the deﬁnition
of B. Plugging (27) into (26) and noting that (cid:107)∇f (X t)U t(cid:107)2
F (cid:107)X t(cid:107)2 ≤
(cid:107)PU t∇f (X t)(cid:107)2

F (cid:107) ≤ (cid:107)PU t∇f (X t)(cid:107)2

F B yields

EIt[(cid:107)vt

kU t(cid:107)2

F ] ≤

≤

2
b
2
b

L2((cid:107)X t − X ∗(cid:107)2

F + (cid:107) ˜X k − X ∗(cid:107)2

F )B + (cid:107)PU t∇f (X t)(cid:107)2

F B

√

(2 +

γ0)2L2(cid:107)X ∗(cid:107)2B(E(U t, U ∗

r ) + E( ˜U k, U ∗

r )) + (cid:107)PU t∇f (X t)(cid:107)2

F B,

where the second inequality holds for Lemma 9. This ﬁnishes the proof of this lemma.

As shown by the proof of this lemma, the introduced semi-stochastic gradient is very crucial
to the establishment of the key inequality (27) for bounding the term EIt[(cid:107)vt

kU t(cid:107)2

F ].

Appendix C. Proof of Proposition 4

The proof of Proposition 4 is similar to that of (Zeng et al., 2019, Proposition 1). We
provide its proof here mainly for the completeness.
Proof From (Bubeck, 2015, Theorem 3.6), we have that for consecutive updates ˜X t−1,
˜X t, and the optimum X ∗, projected gradient descent satisﬁes:

(cid:107) ˜X t − X ∗(cid:107)2

F ≤ (1 − κ−1) · (cid:107) ˜X t−1 − X ∗(cid:107)2
F ,

23

Zeng, Zha, Ma, and Yao

which implies for any t ∈ N,

(cid:107) ˜X t − X ∗(cid:107)2

F ≤ (1 − κ−1)t · (cid:107) ˜X 0 − X ∗(cid:107)2

F = (1 − κ−1)t · (cid:107)X ∗(cid:107)2
F .

Thus, by the hypothesis of T ,

(cid:107)X 0 − X ∗(cid:107)2

F = (cid:107) ˜X T − X ∗(cid:107)2

F ≤

By Lemma 7, there holds

E( ˜U 0, U ∗) ≤

Therefore, we end the proof.

Acknowledgment

(cid:107)X 0 − X ∗(cid:107)2
F
√

2(

2 − 1)σr(X ∗)

√

16(

2 − 1)2σ2
9κ

r (X ∗)

.

√

8(

≤

2 − 1)σr(X ∗)

9κ

.

The work of Jinshan Zeng is supported in part by the National Natural Science Founda-
tion (NNSF) of China (No. 61977038), and Thousand Talents Plan of Jiangxi Province
(No.
jxsq2019201124). The work of Ke Ma is supported in part by the NNSF of China
(No. 62006217), and China Post-doctoral Science Foundation (2020M680651), as well as
supported by the Fundamental Research Funds for Central Universities. The research of
Yuan Yao is supported in part by HKRGC 16303817, ITF UIM/390, as well as awards from
Tencent AI Lab and Si Family Foundation. Part of Jinshan Zeng’s work was done when he
visited at Liu Bie Ju Centre for Mathematical Sciences, City University of Hong Kong.

References

S. Agarwal, J. Wills, L. Cayton, G. Lanckriet, D. Kriegman, and S. Belongie. Generalized
non-metric multidimensional scaling.
In Proc. The 11th International Conference on
Artiﬁcial Intelligence and Statistics (AISTATS), pages 11–18, San Juan, Puerto Rico,
2007.

F. Alizadeh.

Interior point methods for semideﬁnite programming with applications to

combinatorial optimization. SIAM Journal on Optimization, 5(1):13–51, 1995.

A.S. Bandeira, N. Boumal, and V. Voroninski. On the low-rank approach for semideﬁnite
programs arising in synchronization and community detection. In Proc. The 29th Annual
Conference on Learning Theory (COLT), volume 49, pages 361–382, New-York City,
USA, 2016.

J. Barzilai and J. M. Borwein. Two-point step size gradient methods. IMA Journal of

Numerical Analysis, 8(1):141–148, 1988.

S. Bhojanapalli, A. Kyrillidis, and S. Sanghavi. Dropping convexity for faster semi-deﬁnite
optimization. In Proc. The 29th Annual Conference on Learning Theory (COLT), vol-
ume 49, pages 1–53, New-York City, USA, 2016.

24

SVRG for SDP

I. Borg and P.J. Groenen. Modern Multidimensional Scaling: Theory and Applications.

Springer-Verlag, New York, 2005.

Sebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations (cid:114) Trends

in Machine Learning, 8(3-4):231–357, 2015.

S. Burer and R. D. Monteiro. A nonlinear programming algorithm for solving semideﬁnite
programs via low-rank factorization. Mathematical Programming, 95(2):329–357, 2003.

S. Burer and R. D. Monteiro. Local minima and convergence in low-rank semideﬁnite

programming. Mathematical Programming, 103(3):427–444, 2005.

E.J. Candes, X. Li, and M. Soltanolkotabi. Phase retrieval from coded diﬀraction patterns.

Applied and Computational Harmonic Analysis, 39:227–299, 2015a.

E.J. Candes, X. Li, and M. Soltanolkotabi. Phase retrieval via wirtinger ﬂow: theory and

algorithms. IEEE Transactions on Information Theory, 61(4):1985–2007, 2015b.

P. Jain, P. Netrapalli, and S. Sanghavi. Low-rank matrix completion using alternating
minimization. In Proc. The 45th annual ACM symposium on Symposium on theory of
computing, pages 665–674, Palo Alto, CA, 2013.

C. Jin, S. M. Kakade, and P. Netrapalli. Provable eﬃcient online matrix completion via non-
convex stochastic gradient descent. In Proc. The 30th Conference on Neural Information
Processing Systems (NeurIPS), Barcelona, Spain, December 2016.

R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance
In Proc. The 27th Conference on Neural Information Processing Systems

reduction.
(NeurIPS), Lake Tahoe, Nevada, December 2013.

Ke Ma, Jinshan Zeng, Jiechao Xiong, Qianqian Xu, Xiaochun Cao, Wei Liu, and Yuan
Yao. Stochastic non-convex ordinal embedding with stabilized barzilai-borwein step size.
In Proc. The 32nd AAAI Conference on Artiﬁcial Intelligence (AAAI), New Orleans,
Louisiana, February 2018.

Ke Ma, Jinshan Zeng, Jiechao Xiong, Qianqian Xu, Xiaochun Cao, Wei Liu, and Yuan
Yao. Fast stochastic ordinal embedding with variance reduction and adaptive step size.
IEEE Transactions on Knowledge and Data Engineering, 2019. doi: 10.1109/TKDE.
2019.2956700.

A. Montanari and S. Sen. Semideﬁnite programs on sparse random graphs and their appli-
cation to community detection. In Proc. The 48th annual ACM symposium on Theory of
Computing, Cambridge, MA, 2016.

R. D. Monteiro. First- and second-order methods for semideﬁnite programming. Mathe-

matical Programming, 91:209–244, 2003.

Y. Nestrov. Introductory Lectures on Convex Optimization: A Basic Course, volume 87.

Springer Science & Business Media, New York, USA, 2004.

25

Zeng, Zha, Ma, and Yao

Y. Nestrov and A. Nemirovski. Self-concordant Functions and Polynomial-time Methods
in Convex Programming. USSR Academy of Sciences, Central Economic & Mathematic
Institute, 1989.

P. Netrapalli, P. Jain, and S. Sanghavi. Phase retrieval using alternating minimization. In
Proc. the 32th Conference on Neural Information Processing Systems (NeurIPS), pages
2796–2804, Lake Tahoe, Nevada, 2013.

H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical

Statistics, 22(3):400–407, 1951.

M. Schmidt, N. L. Roux, and F. Bach. Minimizing ﬁnite sums with the stochastic average

gradient. Mathematical Programming, Ser. A, 162(1-2):83–112, 2017.

Othmane Sebbouh, Nidham Gazagnadou, Samy Jelassi, Francis Bach, and Robert M.
Gower. Towards closing the gap between the theory and practice of svrg. In Proc. the
33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver,
Canada, 2019.

C. Tan, S. Ma, Y.H. Dai, and Y. Qian. Barzilai-borwein step size for stochastic gradi-
ent descent. In Proc. The 30th Conference on Neural Information Processing Systems
(NeurIPS), Barcelona, Spain, December 2016.

S. Tu, R. Boczar, M. Simchowitz, M. Soltanokotabi, and B. Recht. Low-rank solutions of
linear matrix equations via procrustes ﬂow. In Proc. the 33rd International Conference
on Machine Learning (ICML), New York, USA, 2016.

L. Wang, X. Zhang, and Q. Gu. A universal variance reduction-based catalyst for nonconvex

low-rank matrix recovery. ArXiv:1701.02301, 2017a.

Lingxiao Wang, Xiao Zhang, and Quanquan Gu. A uniﬁed computational and stastical
In Proc. the 20th International
framework for nonconvex low-rank matrix estimation.
Conference on Artiﬁcial Intelligence and Stastics (AISTATS), Ft. Lauderdale, FL, 2017b.

Lingxiao Wang, Xiao Zhang, and Quanquan Gu. A uniﬁed variance reduction-based frame-
work for nonconvex low-rank matrix recovery. In Proc. the 34th International Conference
on Machine Learning (ICML), Sydney, Australia, 2017c.

Jinshan Zeng, Ke Ma, and Yuan Yao. Finding global optima in nonconvex stochastic
semideﬁnite optimization with variance reduction. In Proc. the 21st International Con-
ference on Artiﬁcial Intelligence and Stastics (AISTATS), Lanzarote, Canary Islands,
2018.

Jinshan Zeng, Ke Ma, and Yuan Yao. On global linear convergence in stochastic nonconvex
optimization for semideﬁnite programming. IEEE Transactions on Signal Processing, 67
(16):4261–4275, 2019.

Xiao Zhang, Lingxiao Wang, and Quanquan Gu. Stochastic variance-reduced gradient
descent for low-rank matrix recovery from linear measurments. ArXiv:1701.00481, 2017.

26

SVRG for SDP

Q. Zheng and J. Laﬀerty. A convergent gradient descent algorithm for rank minimization
and semideﬁnite programming from random linear measurements.
In Proc. the 28th
International Conference on Neural Information Processing Systems (NIPS), pages 109–
117, Montreal, Canada, 2015.

27

