1
2
0
2

b
e
F
7
1

]
E
S
.
s
c
[

2
v
7
7
8
7
0
.
2
0
1
2
:
v
i
X
r
a

Investigating and Recommending Co-Changed Entities
for JavaScript Programs

Zijian Jianga, Hao Zhongb, Na Menga,∗

aVirginia Polytechnic Institute and State University, Blacksburg VA 24060, USA
bShanghai Jiao Tong University, Shanghai 200240, China

Abstract

JavaScript (JS) is one of the most popular programming languages due to its
ﬂexibility and versatility, but maintaining JS code is tedious and error-prone. In
our research, we conducted an empirical study to characterize the relationship
between co-changed software entities (e.g., functions and variables), and built a
machine learning (ML)-based approach to recommend additional entity to edit
given developers’ code changes. Speciﬁcally, we ﬁrst crawled 14,747 commits in
10 open-source projects; for each commit, we created one or more change depen-
dency graphs (CDGs) to model the referencer-referencee relationship between
co-changed entities. Next, we extracted the common subgraphs between CDGs
to locate recurring co-change patterns between entities. Finally, based on those
patterns, we extracted code features from co-changed entities and trained an
ML model that recommends entities-to-change given a program commit.

According to our empirical investigation, (1) three recurring patterns com-
monly exist in all projects; (2) 80%–90% of co-changed function pairs either
invoke the same function(s), access the same variable(s), or contain similar
statement(s); (3) our ML-based approach CoRec recommended entity changes
with high accuracy (73%–78%). CoRec complements prior work because it sug-
gests changes based on program syntax, textual similarity, as well as software
history; it achieved higher accuracy than two existing tools in our evaluation.

Keywords: Multi-entity edit, change suggestion, machine learning, JavaScript

1. Introduction

JavaScript (JS) has become one of the most popular programming languages
because it is lightweight, ﬂexible, and powerful [1]. Developers use JS to build
web pages and games. JS has many new traits (1) it is dynamic and weakly

∗Corresponding author
Email addresses: wz649588@vt.edu (Zijian Jiang), zhonghao@sjtu.edu.cn (Hao Zhong),

nm8247@vt.edu (Na Meng)

 
 
 
 
 
 
typed; (2) it has ﬁrst-class functions; (3) it is a class-free, object-oriented pro-
gramming language that uses prototypal inheritance instead of classical inheri-
tance; and (4) objects in JS inherit properties from other objects directly and
all these inherited properties can be changed at runtime. All above-mentioned
traits make JS unique and powerful; they also make JS programs very challeng-
ing to maintain and reason about [2, 3, 4].

To reduce the cost of maintaining software, researchers proposed approaches
that recommend code co-changes [5, 6, 7, 8]. For instance, Zimmermann et
al. [5] and Rolfsnes et al. [6] mined co-change patterns of program entities from
software version history and suggested co-changes accordingly. Wang et al. [7, 8]
studied the co-change patterns of Java program entities and built CMSuggester
to suggest changes accordingly for any given program commit. However, existing
tools do not characterize any co-change patterns between JS software entities,
neither do they recommend changes by considering the unique language features
of JS or the mined co-changed patterns from JS programs (see Section 8.3) for
detailed discussions).

To overcome the limitations of the prior approaches, in this paper, we ﬁrst
conducted a study on 14,747 program commits from 10 open-source JS projects
to investigate (1) what software entities are usually edited together, and (2) how
those simultaneously edited entities are related. Based on this characterization
study for co-change patterns, we further developed a learning-based approach
CoRec to recommend changes given a program commit.

Speciﬁcally in our study, for any program commit, we constructed and com-
pared Abstract Syntax Trees (ASTs) for each edited JS ﬁle to identify all edited
entities (e.g., Deleted Classes (DC), Changed Functions (CF), and Added Vari-
ables (AV)). Next, we created change dependency graphs (CDGs) for each com-
mit by treating edited entities as nodes and linking entities that have referencer-
referencee relations. Afterwards, we extracted common subgraphs between
CDGs and regarded those common subgraphs as recurring change patterns.
In our study, we explored the following research question:

RQ1: What are the frequent co-change patterns in JS programs?

We automatically analyzed thousands of program commits from ten JS projects
and revealed the recurring co-change patterns in each project. By manually
inspecting 20 commits sampled for each of the 3 most popular patterns, we
observed that 80%–90% of co-changed function pairs either invoke the same
function(s), access the same variable(s), contain similar statement(s), or get
frequently co-changed in version history.

Besides the above ﬁndings, our study reveals three most popular change pat-
terns: (i) one or more caller functions are changed together with one changed
callee function that they commonly invoke; (ii) one or more functions are
changed together to commonly invoke an added function; (iii) one or more
functions are changed together to commonly access an added variable. The
co-changed callers in each pattern may share commonality in terms of variable
accesses, function invocations, code similarity, or evolution history.

2

Based on the above-mentioned observations, we built a machine learning
(ML)-based approach—CoRec—to recommend functions for co-change. Given
the commits that contain matches for any of the above-mentioned co-change
patterns, CoRec extracts 10 program features to characterize the co-changed
function pairs, and uses those features to train an ML model. Afterwards, given
a new program commit, the model predicts whether any unchanged function
should be changed as well and recommends changes whenever possible. With
CoRec, we investigated the following research question:

RQ2: How does CoRec perform when suggesting co-changes based

on the observed three most popular patterns?

We applied CoRec and two existing techniques (i.e., ROSE [5] and Transitive
Associate Rules (TAR) [9]) to the same evaluation datasets, and observed CoRec
to outperform both techniques by correctly suggesting many more changes.
CoRec’s eﬀectiveness varies signiﬁcantly with the ML algorithm it adopts. CoRec
works better when it trains three separate ML models corresponding to the three
patterns than training a uniﬁed ML model for all patterns. Our results show
that CoRec can recommend co-change functions with 73–78% accuracy; it sig-
niﬁcantly outperforms two baseline techniques that suggest co-changes purely
based on software evolution.

We envision CoRec to be used in the integrated development environments
(IDE) for JS, code review systems, and version control systems. In this way,
after developers make code changes or before they commit edits to software
repositories, CoRec can help detect and ﬁx incorrectly applied multi-entity ed-
In the sections below, we will ﬁrst describe a motivating example (Sec-
its.
tion 2), and then introduce the concepts used in our research (Section 3). Next,
we will present the empirical study to characterize co-changes in JS programs
(Section 4). Afterwards, we will explain our change recommendation approach
CoRec (Section 5) and expound on the evaluation results (Section 6).

2. A Motivating Example

The prior work [10, 11, 12, 13] shows that developers may commit errors
of omission (i.e., forgetting to apply edits completely) when they have to edit
multiple program locations simultaneously in one maintenance task (i.e., bug
ﬁxing, code improvement, or feature addition). For instance, Fry et al. [10]
reported that developers are over ﬁve times more precise at locating errors
of commission than errors of omission. Yin et al. [12] and Park et al. [13]
separately showed that developers introduced new bugs when applying patches
to ﬁx existing bugs. In particular, Park et al. inspected the supplementary bug
ﬁxes following the initial bug-ﬁxing trials, and summarized nine major reasons
to explain why the initial ﬁxes were incorrect. Two of the nine reasons were
about the incomplete program edits applied by developers.

To help developers apply JS edits completely and avoid errors of omission, we
designed and implemented a novel change recommendation approach—CoRec.

3

This section overviews our approach with a running example, which is extracted
from a program commit to Node.js—an open-source server-side JS runtime envi-
ronment [14]. Figure 1 shows a simpliﬁed version of the exemplar program com-
mit [15]. In this revision, developers added a function maybeCallback(...) to check
whether the pass-in parameter cb is a function, and modiﬁed seven functions in
distinct ways to invoke the added function(e.g., changing fs.write(...) on line
10 and line 14). The seven functions include: fs.rmdir(...), fs.appendFile(...),
fs.truncate(...), fs.write(...), fs.readFile(...), fs.writeFile(...), and fs.writeAll
(...) [15]. However, developers forgot to change an eighth function—fs.read(...)—
to also invoke the added function (see line 19 in Figure 1).

Figure 1: A program commit should add one function and change eight functions to invoke
the newly added one. However, developers forgot to change one of the eight functions—
fs.read(...) [15].

CoRec reveals the missing change with the following steps. CoRec ﬁrst trains
an ML model with the program co-changes extracted from Node.js software
version history. Then given the exemplar commit, based on the added function
maybeCallback(...) and each changed function (e.g., fs.write(...)), CoRec ex-
tracts any commonality between the changed function and any unchanged one.
For each function pair, CoRec applies its ML model to the extracted commonal-
ity features and predicts whether the function pair should be changed together.
Because fs.write(...) and fs.read(...)

• commonly access one variable binding,

• commonly invoke two functions: makeCallback(...) and wrapper(...),

• declare the same parameters in sequence,

• have token-level similarity as 41%, and

• have statement-level similarity as 42%,

4

1.+ functionmaybeCallback(cb) {2.+   returntypeofcb=== 'function' ? cb: rethrow();3.+ } 4.fs.write= function(fd, buffer, offset, length, position, callback) {5.-callback = makeCallback(arguments[arguments.length-1]);6.…7.req.oncomplete= wrapper;8.if(buffer instanceofBuffer) {9.…10.+     callback = maybeCallback(callback);11.returnbinding.writeBuffer(…);12.}13.…14.+   callback = maybeCallback(position);15.returnbinding.writeBuffer(fd, buffer, offset, …);16.}17.fs.read= function(fd, buffer, offset, length, position, callback) {18.-callback = makeCallback(arguments[arguments.length-1]);19.…// an edit that developers forgot to apply://+  callback = maybeCallback(callback);20.req.oncomplete= wrapper;21.binding.read(fd, buffer, offset, …);22.} Figure 2: A JS class can be deﬁned with an expression (see (a)) or a declaration (see (b)).

the pre-trained ML model inside CoRec considers the two functions to share suf-
ﬁcient commonality and thus recommends developers to also change fs.read(...)
to invoke maybeCallback(...). In this way, CoRec can suggest entities for change,
which edits developers may otherwise miss.

3. Terms and Deﬁnitions

This section ﬁrst introduces concepts relevant to JS programming, and then

describes the terminology used in our research.

ES6 and ES5. ECMA Script is the standardized name for JavaScript [16].
ES6 (or ECMAScript2015) is a major enhancement to ES5, and adds many
features intended to make large-scale software development easier. ES5 is fully
supported in all modern browsers, and major web browsers support some fea-
tures of ES6. Our research is applicable to both ES5 and ES6 programs.

Software Entity. We use software entity to refer to any deﬁned JS class,
function, variable, or any independent statement block that is not con-
tained by the deﬁnition of classes, functions, or variables. When developers
write JS code, they can deﬁne each type of entities in multiple alternative ways.
For instance, a class can be deﬁned with a class expression (see Figure 2 (a)) or
class declaration (see Figure 2 (b)). Similarly, a function can be deﬁned with
a function expression or function declaration. A variable can be deﬁned with a
variable declaration statement; the statement can either use keyword const to
declare a constant variable, or use let or var to declare a non-constant variable.
Edited Entity. When maintaining JS software, developers may add, delete,
or change one or more entities. Therefore, as with prior work [17], we deﬁned
a set of edited entities to describe the possible entity-level edits, including
Added Class (AC), Deleted Class (DC), Added Function (AF), Deleted Func-
tion (DF), Changed Function (CF), Added Variable (AV), Deleted Variable
(DV), Changed Variable (CV), Added Statement Block (AB), Deleted State-
ment Block (DB), and Changed Statement Block (CB). For example, if a new
class is declared to have a constructor and some other methods, we consider the
revision to have one AC, multiple AFs, and one or more AV (depending on how
many ﬁelds are deﬁned in the constructor).

5

constRectangle = class{constructor(height, width) {this.height= height;this.width= width;}area() {returnthis.height* this.width;}};console.log(newRectangle(5, 8).area());classRectangle{constructor(height, width) {this.area= height * width;}}console.log(newRectangle(5, 8).area);(a)(b)Figure 3: The procedure to extract changed entities given a commit.

Multi-Entity Edit and CDG. As with prior work [18], we use multi-
entity edit to refer to any commit that has two or more edited entities. We
use change dependency graph (CDG) to visualize the the relationship be-
tween co-changed entities in a commit. Speciﬁcally, each CDG has at least two
nodes and one edge. Each node represents an edited entity, and each edge rep-
resents the referencer-referencee relationship between entities (e.g., a function
calls another function). Namely, if an edited entity E1 refers to another edited
entity E2, we say E1 depends on E2. A related CDG is constructed to connect
the two entities with a directed edge pointing to E2—the entity being depended
upon (i.e. E1 → E2). For each program commit, we may create zero, one, or
multiple CDGs.

4. Characterization Study

This section introduces our study methodology (Section 4.1) and explains
the empirical ﬁndings (Section 4.2). The purpose of this characterization study
is to identify recurring change pattern (RCP) of JS programs. An RCP is a
CDG subgraph that is commonly shared by the CDGs from at least two distinct
commits. RCPs deﬁne diﬀerent types of edits, and serve as the templates of co-
change rules. Our approach in Section 5 mines concrete co-change rules for the
most common RCPs.

4.1. Study Methodology

We implemented a tool to automate the analysis. Given a set of program
commits in JS repositories, our tool ﬁrst characterizes each commit by extract-
ing the edited entities (Section 4.1.1) and constructing CDG(s) (Section 4.1.2).
Next, it compares CDGs across commits to identify RCPs (Section 4.1.3).

4.1.1. Extraction of Edited Entities

As shown in Figure 3, we took three steps to extract any edited entities for

each commit.

Step 1: AST Parsing. Given a program commit c, this step ﬁrst locates the
old and new versions of each edited JS ﬁle. For every edited ﬁle (fo, fn), this
step adopts Esprima [19] and typed-ast-util [20] to generate Abstract Syntax
Trees (ASTs): (asto, astn). Esprima is a high performance, standard-compliant
JavaScript parser that supports the syntax of both ES5 and ES6; however, it

6

3. Program Differencingfor each (!", !#)•AC, AV, AF, AB•DC, DV, DF, DB•CV, CF, CB 1. AST Parsing($%&', $%&()(f1_old, f1_new)(f2_old, f2_new)……(fn_old, fn_new)Commit c2. Entity Extraction($%&', $%&()(ESo, ESn)Figure 4: Extracting edited entities from a program commit of Meteor [21].

cannot infer the static type binding information of any referenced class, function,
or variable. Meanwhile, given JS ﬁles and the project’s package.json ﬁle, typed-
ast-util produces ASTs annotated with structured representations of TypeScript
types, which information can facilitate us to precisely identify the referencer-
referencee relationship between edited entities. We decided to use both tools for
two reasons. First, when a project has package.json ﬁle, we rely on Esprima to
identify the code range and token information for each parsed AST node, and
rely on typed-ast-util to attach relevant type information to those nodes. Sec-
ond, if a project has no package.json ﬁle, Esprima is still used to generate ASTs
but we deﬁned a heuristic approach (to be discussed later in Section 4.1.2) to
identify the referencer-referencee relationship between entities with best eﬀorts.
To facilitate our discussion, we introduce a working example from a pro-
gram revision [21] of Meteor [22]. As shown in Figure 4, the program revi-
In this step, CoRec creates a pair of ASTs for
sion changes seven JS ﬁles.
each edited ﬁle and stores the ASTs into JSON ﬁles for later processing (e.g.,
tools/buildmessages-ast.json (old) and tools/buildmessages-ast.json (new)).

Step 2: Entity Extraction. From each pair of ASTs (asto, astn) (i.e., JSON
ﬁles), this step extracts the entity sets (ESo, ESn). In the example shown in
Figure 4, ESo lists all entities from the old JS ﬁle, and ESn corresponds to the
new ﬁle. We deﬁned four kinds of entities to extract: variables (V), functions
(F), classes (C), and statement blocks (B). A major technical challenge here
is how to extract entities precisely and consistently. Because JS programming
supports diverse ways of deﬁning entities and the JS syntax is very ﬂexible,
we cannot simply check AST node types of statements to recognize entity def-
initions. For instance, a variable declaration statement can be interpreted as
a variable-typed entity or a statement block, depending on the program con-
text. To eliminate ambiguity and avoid any confusion between diﬀerently typed
entities, we classify and extract entities in the following way:

• A code block is treated as a function deﬁnition if it satisﬁes either of the fol-
lowing two requirements. First, the AST node type is “FunctionDeclaration”
(e.g., runBenchmarks() on line 7 in Figure 5) or “MethodDefinition”. Sec-
ond, (1) the block is either a “VariableDeclaration” statement (e.g., const

7

getRectArea = function(...){...};) or an“Assignment” expression (see line
11 and line 20 of Figure 5); and (2) the right-side operand is either
“FunctionExpression”, or “CallExpression” that outputs another function as
return value of the called function. In particular, if any deﬁned function
has its prototype property explicitly referenced (e.g., Benchmark.prototype
on lines 20 and 24) or is used as a constructor to create any object (e.g.,
line 12), we reclassify the function deﬁnition as a class deﬁnition, because
the function usage is more like the usage of a class.

• A code block is considered to be a class deﬁnition if it meets either of the
following two criteria. First, the block uses keyword class. Second, the
block deﬁnes a function, while the codebase either references the function’s
prototype (e.g., Benchmark.prototype on lines 20 and 24 in Figure 5) or uses
the function as a constructor to create any object (see line 12).

• A code block is treated as a variable declaration if (1) it is either a
“VariableDeclaration” statement (e.g., var silent = ... on line 2 in Fig-
ure 5) or an “Assignment” expression, (2) it does not deﬁne a function or
class, (3) it does not belong to the deﬁnition of any function but may
belong to a constructor (see lines 15-17), and (4) it does not declare a
required module (see line 1). Particularly, when a variable declaration is
an assignment inside a class constructor (e.g., lines 15-17), it is similar to
the ﬁeld declaration in Java.

• A code block is treated as a statement block if (1) it purely contains
statements, (2) it does not deﬁne any class, function, or variable, and (3)
it does not belong to the deﬁnition of any class or function. For example,
lines 3-6 in Figure 5 are classiﬁed as a statement block.

Step 3: Program Diﬀerencing. To identify any edited entity between ESo
and ESn, we ﬁrst matched the deﬁnitions of functions, variables, and classes
across entity sets based on their signatures.
If any of these entities (e.g., a
function deﬁnition) solely exists in ESo, an entity-level deletion (e.g., DF) is
inferred; if an entity (e.g., a variable deﬁnition) solely exists in ESn, an entity-
level insertion (e.g., AV) is inferred. Next, for each pair of matched entities,
we further exploited a ﬁne-grained AST diﬀerencing tool—GumTree [23]—to
identify expression-level and statement-level edits. If any edit is reported, we
inferred an entity-level change (e.g., CF shown in Figure 4). Additionally, we
matched statement blocks across entity sets based on their string similarities.
Namely, if a statement block b1 ∈ ESo has the longest common subsequence
with a block b2 ∈ ESn and the string similarity is above 50%, we considered
the two blocks to match. Furthermore, if the similarity between two matched
blocks is not 100%, we inferred a block-level change CB.

4.1.2. CDG Construction

For each program commit, we built CDGs by representing the edited entities
as nodes, and by connecting edited entities with directed edges if they have either

8

Figure 5: Code snippets from the ﬁle benchmark.common.js of Node.js in revision 00a1d36 [15],
whose related entity types are shown on the right.

of the following two kinds of relationship:

• Access. If an entity E1 accesses another entity E2 (i.e., by reading/writing
a variable, invoking a function, or using a class), we consider E1 to be de-
pendent on E2.

• Containment. If the code region of E1 is fully covered by that of E2, we

consider E1 to be dependent on E2.

The technical challenge here is how to identify the relationship between
edited entities. We relied on ESprima’s outputs to compare code regions between
edited entities in order to reveal the containment relations. Additionally, when
package.json ﬁle is available, we leveraged the type binding information inferred
by typed-ast-util to identify the access relationship. For instance, if there is a
function call bar() inside an entity E1 while bar() is deﬁned by a JS module f2,
then typed-ast-util can resolve the fully qualiﬁed name of the callee function as
f2.bar(). Such resolution enables us to eﬀectively link edited entities no matter
whether they are deﬁned in the same module (i.e., JS ﬁle) or not.

Since some JS projects have no package.json ﬁle, we could not adopt typed-
ast-util to resolve bindings in such scenarios. Therefore, we also built a simpler
but more applicable approach to automatically speculate the type binding in-
formation of accessed entities as much as possible. Speciﬁcally, suppose that ﬁle
f1 deﬁnes E1 to access E2. To resolve E2 and link E1 with E2’s deﬁnition, this
intuitive approach ﬁrst scans all entities deﬁned in f1 to see whether there is
any E2 deﬁnition locally. If not, this approach further examines all require and
import statements in f1, and checks whether any required or imported module

9

N/A1.varassert = require(‘assert’);2.varsilent = +process.env.NODE_BENCH_SILENT;3.if(module === require.main) {4.…5.runBenchmarks();6.}7.functionrunBenchmarks() {8.vartest = test.shift();9.…10.}11.exports.creatBenchmark = function(fn, options) {12.returnnewBenchmark(fn, options);13.}14.functionBenchmark(fn, options) {15.this.fn = fn;16.this.options = options;17.this.config = parseOpts(options);18.…19.}20.Benchmark.prototype.report = function(value) {21.varheading = this.getHeading();22.…23.};24.Benchmark.prototype.getHeading = function() {25.varconf = this.config;26.…27.}variablestatement blockfunctionfunctionclassfunctionfunctionvariablevariablevariableFigure 6: A simpliﬁed program commit that adds a function spaces(...), changes a function
capture(...), and changes a statement block [21]

Figure 7: The CDG corresponding to the program commit shown in Figure 6

deﬁnes a corresponding entity with E2’s name; if so, this approach links E1 with
the retrieved E2’s deﬁnition.

Compared with typed-ast-util, our approach is less precise because it cannot
infer the return type of any invoked function. For instance, if we have const foo
= bar() where bar() returns a function, our approach simply assumes foo to be a
variable instead of a function. Consequently, our approach is unable to link foo’s
deﬁnition with any of its invocations. Based on our experience of applying both
typed-ast-util and the heuristic method to the same codebase (i.e., nine open-
source projects), the diﬀerences between these two methods’ results account for
no more than 5% of all edited entities. It means that our heuristic method is
still very precise even though no package.json ﬁle is available.

Figures 6 and 7 separately present the code changes and CDG related to
tools/buildmessage.js, an edited ﬁle mentioned in Figure 4. According to Fig-
ure 6, the program commit modiﬁes ﬁle tools/buildmessage.js by deﬁning a
to
new function spaces(...) and updating an existing function capture(...)
invoke the new function.
It also changes ﬁle tools/commands-package.js by up-
dating the function invocation of capture(...) inside a statement block (i.e.,
main.registerCommand(...);). Given the old and new versions of both edited JS
ﬁles, our approach can construct the CDG shown in Figure 7. In this CDG,
each directed edge starts from a dependent entity E1, and points to the entity
on which E1 depends. Each involved function, variable, or class has its fully
qualiﬁed name included in the CDG for clarity. As statement blocks have no
fully qualiﬁed names, we created a unique identiﬁer for each block with (1) the
module name (e.g., tools.commands-packages) and (2) index of the block’s ﬁrst
character in that module (e.g., 69933).

10

1.+ varspaces = function(n) {2.+return_.times(n, function() { return' ' }).join(’’);3.+ };4.varcapture = function(option, f) {5.…6.-console.log("START CAPTURE", nestingLevel, options.title, "took " + (end -start));7.+   console.log(spaces(nestingLevel* 2), "START CAPTURE", nestingLevel, options.title, "took " + (end -start));8.… 9.}1.main.registerCommand({…}, function(options) {2.…3.-varmessages = buildmessage.capture(function() {4.+   varmessages = buildmessage.capture({ title: 'Combining constraints' }, function(){5.allPackages= project.getCurrentCombinedConstraints();6.});7.… tools/buildmessage.jstools/commands-packages.jsCB	tools.commands-packages.	statement_block_69933	Function	Invocation	CF	tools.buildmessage.	capture(options,	f)	AF	tools.buildmessage.	spaces(n)	Function	Invocation	Table 1: Subject projects

Project

Description

Node.js

Meteor

Ghost

Habitica

PDF.js

React

Serverless

Webpack

Node.js [14] is a cross-platform JS
runtime environment. It executes JS
code outside of a browser.

Meteor [22] is an ultra-simple
environment for building modern web
applications.

Ghost [25] is the most popular
open-source and headless Node.js content
management system (CMS) for
professional publishing.

Habitica [26] is a habit building program
that treats people’s life like a Role
Playing Game.

PDF.js [27] is a PDF viewer that is built
with HTML5.

React [28] is a JS library for building
user interfaces.

Serverless [29] is a framework used to
build applications comprised of
microservices that run in response to
events.

Webpack [30] is a module bundler, which
mainly bundles JS ﬁles for usage in a
browser. assets.

Storybook

Storybook [31] is a development
environment for UI components.

Electron

Electron [32] is a framework that
supports developers to write
cross-platform desktop applications using
JS, HTML, and CSS.

# of
KLOC

# of
Com-
mits

# of
Edited
Entities

1,755

2,701

11,287

255

3,011

10,274

115

1,263

5,142

129

1,858

6,116

104

286

1,754

1,050

4,255

4,415

63

1,110

3,846

37

43

35

1,099

3,699

528

2,277

673

1,898

4.1.3. Extraction of Recurring Change Patterns (RCP)

As with prior work [18], we extracted RCPs by comparing CDGs across
program commits. Intuitively, given a CDG g1 from commit c1 and the CDG
g2 from commit c2, we matched nodes based on their edit-entity labels (e.g.,
AF) while ignoring the concrete code details (e.g., tools.buildmessage.spaces(n)
in Figure 7). We then established edge matches based on those node matches.
Namely, two edges are matched only if they have matching starting nodes and
matching ending nodes. Next, based on all established matches, we identiﬁed
the largest common subgraph between g1 and g2 using the oﬀ-the-shelf subgraph
isomorphism algorithm VF2 [24]. Such largest common subgraphs are consid-
ered as RCPs because they commonly exist in CDGs of diﬀerent commits.

4.2. Empirical Findings

To characterize JS code changes, we applied our study approach to a subset
of available commits in 10 open-source projects, as shown in Table 1. We chose
these projects because (1) they are popularly used; (2) they are from diﬀerent

11

Figure 8: Commit distributions based on the number of edited entities each of them contains

application domains; and (3) they contain a large number of available commits.
For simplicity, to sample the commits that may fulﬁll independent maintenance
tasks, we searched each software repository for commits whose messages contain
any of the following keywords: “bug”, “ﬁx”, “error”, “adjust”, and “failure”.

Table 1 shows the statistics related to the sampled commits. In particular,
column # of KLOC presents the code size of each project (i.e., the number
of kilo lines of code (KLOC)). Column # of Commits reports the number
of commits identiﬁed via our keyword-based search. Column # of Edited
Entities reports the number of edited entities extracted from those sampled
commits. According to this table, the code size of projects varies signiﬁcantly
from 35 KLOC to 1755 KLOC. Among the 10 projects, 528–3,011 commits
were sampled, and 1,898–11,287 edited entities were included for each project.
Within these projects, only Node.js has no package.json ﬁle, so we adopted our
heuristic approach mentioned in Section 4.1.2 to link edited entities. For the
remaining nine projects, as they all have package.json ﬁles, we leveraged the type
binding information inferred by typed-util-ast to connect edited entities.

4.2.1. Commit Distributions Based on The Number of Edited Entities

We ﬁrst clustered commits based on the number of edited entities they con-
tain. Because the commit distributions of diﬀerent projects are very similar to
each other, we present the distributions for four projects in Figure 8. Among the
10 projects, 41%–52% of commits are multi-entity edits. Speciﬁcally, 15%–19%
of commits involve two-entity edits, and 7%–10% of commits are three-entity

12

Table 2: Multi-entity edits and created CDGs

# of Multi-Entity
Edits

# of Multi-Entity
Edits with CDG(s)
Extracted

% of Multi-Entity
Edits with CDG(s)
Extracted

1,401

1,445

604

962

711

538

480

483

243

277

785

670

356

345

372

320

171

253

119

123

56%

46%

59%

36%

52%

60%

36%

52%

49%

44%

Project

Node.js

Metoer

Ghost

Habitica

PDF.js

React

Serverless

Webpack

Storybook

Electron

edits. The number of commits decreases as the number of edited entities in-
creases. The maximum number of edited entities appears in Node.js, where
a single commit modiﬁes 335 entities. We manually checked the commit on
GitHub [33], and found that four JS ﬁles were added and three other JS ﬁles
were changed to implement HTTP/2.

We noticed that about half of sampled program revisions involve multi-entity
edits. This observation implies the importance of co-change recommendation
tools. When developers have to frequently edit multiple entities simultaneously
to achieve a single maintenance goal, it is crucially important to provide au-
tomatic tool support that can check for the completeness of code changes and
suggest any missing change when possible. In order to build such tools, we de-
cided to further explore relations between co-changed entities (see Section 4.2.2).

Finding 1: Among the 10 studied projects, 41–52% of studied commits
are multi-entity edits. It indicates the necessity of our research to char-
acterize multi-entity edits and to recommend changes accordingly.

4.2.2. Commit Distributions Based on The Number of CDGs

We further clustered multi-entity edits based on the number of CDGs con-
structed for each commit. As shown in Table 2, our approach created CDGs for
36–60% of the multi-entity edits in distinct projects. On average, 49% of multi-
entity edits contain at least one CDG. Due to the complexity and ﬂexibility of
the JS programming language, it is very challenging to statically infer all possi-
ble referencer-referencee relationship between JS entities. Therefore, the actual
percentage of edits that contain related co-changed entities can be even higher
than our measurement. Figure 9 presents the distributions of multi-entity edits
based on the number of CDGs extracted. Although this ﬁgure only presents the
commit distributions for four projects: Node.js, Meteor, Ghost, and Habitica,

13

Figure 9: The distributions of multi-entity edits based on the number of CDGs

we observed similar distributions in other projects as well. As shown in this
ﬁgure, the number of commits decreases signiﬁcantly as the number of CDGs
increases. Among all 10 projects, 73%–81% of commits contain single CDGs,
9%–18% of commits have two CDGs extracted, and 3%–7% of commits have
three CDGs each. The commit with the largest number of CDGs constructed
(i.e., 16) is the one with the maximum number of edited entities in Node.js as
mentioned above in Section 4.2.1.

The high percentage of multi-entity edits with CDGs extracted (i.e., 49%)
implies that JS programmers usually change syntactically related entities simul-
taneously in program revisions. Such syntactic relevance between co-changed
entities enlightened us to build tools that recommend co-changes by observing
the syntactic dependences between changed and unchanged program entities.
To concretize our approach design for co-change recommendations, we further
explored the recurring syntactic relevance patterns between co-changed entities,
i.e., RCPs (see Section 4.2.3).

Finding 2: For 36–60% of multi-entity edits in the studied projects, our
approach created at least one CDG for each commit. It means that many
simultaneously edited entities are syntactically relevant to each other.

4.2.3. Identiﬁed RCPs

By comparing CDGs of distinct commits within the same project repository,
we identiﬁed RCPs in all projects. As listed in Table 3, 35–221 RCPs are ex-
tracted from individual projects. In each project, there are 113–782 commits

14

010203040506070809002468101214160102030405060708090024681012141601020304050607080900246810121416%	of	commits	with	CDG(s)	Node.js	Ghost	#	of	CDGs	%	of	commits	with	CDG(s)	Meteor	#	of	CDGs	01020304050607080900246810121416%	of	commits	with	CDG(s)	#	of	CDGs	Habitica	%	of	commits	with	CDG(s)	#	of	CDGs	Table 3: Recurring change patterns and their matches

# of
RCPs

# of Commits with
RCP Matches

# of Subgraphs Matching
the RCPs

221

200

133

116

86

110

57

80

42

35

782

658

351

339

367

316

164

243

113

117

2,385

1,719

1,223

706

640

899

372

583

337

228

Projects

Node.js

Metoer

Ghost

Habitica

PDF.js

React

Serverless

Webpack

Storybook

Electron

that contain matches for RCPs. In particular, each project has 228–2,385 sub-
graphs matching RCPs. By comparing this table with Table 2, we found that
95%–100% of the commits with CDGs extracted have matches for RCPs.
It
means that if one or more CDGs can be built for a commit, the commit is very
likely to share common subgraphs with some other commits. In other words,
simultaneously edited entities are usually correlated with each other in a ﬁxed
number of ways. If we can characterize the frequently occurring relationship
between co-changed entities, we may be able to leverage such characterization
to predict co-changes or reveal missing changes.

By accumulating the subgraph matches for RCPs across projects, we iden-
tiﬁed ﬁve most popular RCPs, as shown in Figure 10. Here, P1 means that
when a callee function is changed, one or more of its caller functions are also
changed. P2 means that when a new function is added, one or more existing
functions are changed to invoke that new function. P3 shows that when a new
variable is added, one or more existing functions are changed to read/write the
new variable. P4 presents that when a new variable is added, one or more new
functions are added to read/write the new variable. P5 implies that when a
function is changed, one or more existing statement blocks invoking the function
are also changed. Interestingly, the top three patterns commonly exist in all 10
projects, while the other two patterns do not exist in some of the projects. The
top three patterns all involve simultaneously changed functions.

Finding 3: Among the commits with CDGs extracted, 95%–100% of
commits have matches for mined RCPs. In particular, the most popular
three RCPs all involve simultaneously changed functions.

4.2.4. Case Studies for The Three Most Popular RCPs

We conducted two sets of case studies to understand (1) the semantic mean-
ings of P1–P3 and (2) any co-change indicators within code for those patterns.

15

Figure 10: 5 most popular recurring change patterns among the 10 projects

In each set of case studies, we randomly sampled 20 commits matching each of
these RCPs and manually analyzed the code changes in all 60 commits.

Observations for P1 (*CF

The Semantic Meanings of P1–P3. In the 20 commits sampled for each
pattern, we summarized the semantic relevance of entity-level changes as below.
f
−→CF). We found the caller and callee func-
tions changed together in three typical scenarios. First, in about 45% of the in-
spected commits, both caller and callee functions experienced consistent changes
to invoke the same function(s), access the same variable(s), or execute the same
statement(s). Second, in about 40% of the commits, developers applied adap-
tive changes to callers when callees were modiﬁed. The adaptive changes involve
modifying caller implementations when the signatures of callee functions were
updated, or moving code from callees to callers. Third, in 15% of cases, callers
and callees experienced seemingly irrelevant changes.

Observations for P2 (*CF

f
−→AF). Such changes were applied for two
major reasons. First, in 65% of the studied commits, the added function im-
plemented some new logic, which was needed by the changed caller function.
Second, in the other 35% of cases, changes were applied for refactoring purposes.
Namely, the added function was extracted from one or more existing functions
and those functions were simpliﬁed to just invoke the added function.

Observations for P3 (*CF v−→AV). Developers applied such changes in two
typical scenarios. First, in 60% of cases, developers added a new variable for
feature addition, which variable was needed by each changed function (i.e., cross-
cutting concern [34]). Second, in 40% of the cases, developers added variables
for refactoring purposes. For instance, some developers added a variable to
replace a whole function, so all caller functions of the replaced function were
consistently updated to instead access the new variable. Additionally, some
other developers added a variable to replace some expression(s), constant(s), or
variable(s). Consequently, the functions related to the replaced entities were
consistently updated for the new variable.

The Code Indicators for Co-Changes in P1–P3. To identify potential
ways of recommending changes based on the mined RCPs, we randomly picked
20 commits matching each pattern among P1–P3; we ensured that each sampled
commit has two or more co-changed functions (e.g., *CF) referencing another

16

Pattern IndexPattern ShapeP1                 P2               P3                 P4               P5Function InvocationVariable Read/Write*CF: One or more changed functions       *CB: One or more changed statement blocksfvCBedited entity. We then inspected the co-changed functions in each commit, to
decide whether they share any commonality that may indicate their simultane-
ous changes. As shown in Table 4, the three case studies I–III correspond to the
three patterns P1–P3 in sequence. In our manual analysis, we mainly focused
on four types of commonality:

• FI: The co-changed functions commonly invoke one or more peer functions
of the depended entity E (i.e., CF in P1, AF in P2, and AV in P3). Here,
peer function is any function that is deﬁned in the same ﬁle as E.

• VA: The co-changed functions commonly access one or more peer variables
of the depended entity E. Here, peer variable is any variable that is
deﬁned in the same ﬁle as E.

• ST: The co-changed functions commonly share at least 50% of their to-
ken sequences. We calculated the percentage with the longest common
subsequence algorithm between two token strings.

• SS: The co-changed functions commonly share at least 50% of their state-
ments. We computed the percentage by recognizing identical statements
between two given functions f1(...) and f2(...). Assume that the two
functions separately contain n1 and n2 statements, and the number of
common statements is n3. Then the percentage is calculated as

n3 × 2
n1 + n2

× 100%

(1)

Table 4: Commonality observed between the co-changed functions

Case Study

I (for P1: *CF

f
−→CF)
f
II (for P2: *CF
−→AF)
III (for P3: *CF v−→AV)

Commonality

FI
8

12

6

VA
5

7

13

ST
7

8

6

SS
4

6

5

No
Commonality
4

2

3

According to Table 4, 80%–90% of co-changed functions share certain com-
monality with each other. There are only 2–4 commits in each study where the
co-changed functions share nothing in common. Particularly, in the ﬁrst case
study, the FI commonality exists in eight commits, VA exists in ﬁve commits,
ST occurs in seven commits, and SS occurs in four commits. The summation of
these commonality occurrences is larger than 20, because the co-changed func-
tions in some commits share more than one type of commonality. Additionally,
the occurrence rates of the four types of commonality are diﬀerent between case
studies. For instance, FI has 8 occurrences in the ﬁrst case study; it occurs in
12 commits of the second study and occurs in only 6 commits of the third study.
As another example, most commits (i.e., 13) in the third study share the VA
commonality, while there are only 5 commits in the ﬁrst study having such com-
monality. The observed diﬀerences between our case studies imply that when

17

Figure 11: CoRec consists of three phases: commit crawling, training, and testing

developers apply multi-entity edits matching diﬀerent RCPs, the commonality
shared between co-changed functions also varies.

Finding 4: When inspecting the relationship between co-changed func-
tions in three case studies, we found that these functions usually share
certain commonality. This indicates great opportunities for developing
co-change recommendation approaches.

5. Our Change Recommendation Approach: CoRec

f
−→CF, *CF

In our characterization study (see Section 4), we identiﬁed three most pop-
f
−→AF, and *CF v−→AV. In all these patterns, there
ular RCPs: *CF
is at least one or more changed functions (i.e., *CF) that references another
edited entity E (i.e., CF, AF, or AV). In the scenarios when two or more
co-changed functions commonly depend on E, we also observed certain com-
monality between those functions. This section introduces our recommendation
system—CoRec—which is developed based on the above-mentioned insights.
As shown in Figure 11, CoRec has three phases. In the following subsections
(Sections 5.1-5.3), we explain each phase in detail.

5.1. Phase I: Commit Crawling

Given the repository of a project P , Phase I crawls commits to locate any
data usable for machine learning. Speciﬁcally, for each commit in the repository,
this phase reuses part of our study approach (see Sections 4.1.1 and 4.1.2) to
extract edited entities and to create CDGs. If a commit c has any subgraph
matching P1, P2, or P3, this phase recognizes the entity Em matching E (i.e.,
an entity matching CF in P1, matching AF in P2, or matching AV in P3) and
any co-changed function matching *CF. We denote these co-changed function(s)

18

Software	Repository	of	Program	P	Phase	I:	Commit	Crawling	Extraction	of	Edited	Entities	CDG	Construction	RCP	Matching	Commits	matching	P1,	P2,	or	P3	Matches	for	*CF	Unchanged	functions	Matches	for	E	(CF,	AF,	or	AV)		Phase	II:	Training			Single-Function	Features	(2)		Commonality	Features	between	Functions	(7)		Co-Evolution	Feature	between	Functions	(1)	Feature	Extraction	Machine	Learning	Three	Alternative	Classifiers	A	New	Commit	of	P	Function	Pair	Preparation	Phase	III:	Testing	Any	Function	to	Co-Change	Table 5: A list of features extracted for function pair (f1, f2)

Id

Feature

Id

Feature

1

Number of Em-relevant parameter
types in f2

2 Whether f2 has the Em-related type

3
4
5

Number of common peer variables
Number of common peer functions
Number of common parameter types

6

7

8
9
10

Whether f1 and f2 have the same
return type
Whether f1 and f2 are deﬁned in the
same way
Token similarity
Statement similarity
Co-evolution frequency

with CF Set = {cf1, cf2, . . .}, and denote the unchanged function(s) in edited
JS ﬁles from the same commit with U F Set = {uf1, uf2, . . .}. If CF Set has
at least two co-changed functions, CoRec considers the commit to be usable for
model training and passes Em, CF Set, and U F Set to the next phase.

5.2. Phase II: Training

This phase has two major inputs: the software repository of program P,
and the extracted data from each relevant commit (i.e., Em, CF Set, and
U F Set).
In this phase, CoRec ﬁrst creates positive and negative training
samples, and then extracts features for each sample. Next, CoRec trains a ma-
chine learning model by applying Adaboost (with Random Forest as the “weak
learner”) [35] to the extracted features. Speciﬁcally, to create positive samples,
CoRec enumerates all possible function pairs in CF Set, because each pair of
these functions were co-changed with Em. We represent the positive samples
with P os = {(cf1, cf2), (cf2, cf1), (cf1, cf3), . . .}. To create negative samples,
CoRec pairs up each changed function cf ∈ CF Set with an unchanged function
uf ∈ U F Set, because each of such function pairs were not co-changed. Thus,
we denote the negative samples as N eg = {(cf1, uf1), (uf1, cf1), (cf1, uf2), . . .}.
By preparing positive and negative samples in this way, given certain pair of
functions, we expect the trained model to predict whether the functions should
be co-changed or not.

CoRec extracts 10 features for each sample. As illustrated in Figure 11, two
features reﬂect code characteristics of the second function in the pair, seven
features capture the code commonality between functions, and one feature fo-
cuses on the co-evolution relationship between functions. Table 5 presents more
details of each feature. Speciﬁcally, the 1st and 2nd features are about the
relationship between f2 and Em. Their values are calculated as below:

• When Em is CF or AF, the 1st feature records the number of types used in
f2 that match any declared parameter type of Em. Intuitively, the more
type matches, the more likely that f2 should be co-changed with Em. The
2nd feature checks whether the code in f2 uses the return type of Em.

• When Em is AV, the 1st feature is set to zero, because there is no param-
eter type involved in variable declaration. The 2nd feature checks whether
the code in f2 uses the data type of the newly added variable.

19

The 3rd and 4th features were calculated in similar ways. Speciﬁcally, de-
pending on which JS ﬁle deﬁnes Em, CoRec locates peer variables (i.e., variables
deﬁned within the same ﬁle as Em) and peer functions (i.e., functions deﬁned in
the same ﬁle). Next, CoRec identiﬁes the accessed peer variables (or peer func-
tions) by each function in the pair, and intersects the sets of both functions to
count the commonly accessed peer variables (or peer functions). Additionally,
the 7th feature checks whether f1 and f2 are deﬁned in the same manner. In
our research, we consider the following ﬁve ways to deﬁne functions:

(1) via FunctionDeclaration, e.g., function foo(...){...},

(2) via VariableDeclaration, e.g., var foo = function(...){...},

(3) via MethodDefinition, e.g., Class A {foo(...){...}},

(4) via PrototypeFunction to extend the prototype of an object or a function,

e.g., x.prototype.foo = function(...){...}, and

(5) via certain exports-related statements, e.g., exports.foo = function(...){...}

and module.exports = {foo: function(...){...}}.

If f1 and f2 are deﬁned in the same way, the 7th feature is set to true. Finally, the
10th feature assesses in the commit history, how many times the pair of functions
were changed together before the current commit. Inspired by prior work [5],
we believe that the more often two functions were co-changed in history, the
more likely that they are co-changed in the current or future commits.

Depending on the type of Em, CoRec takes in extracted features to actually
train three independent classiﬁers, with each classiﬁer corresponding to one
f
pattern among P1–P3. For instance, one classiﬁer corresponds to P1: *CF
−→CF.
Namely, when Em is CF and one of its caller functions cf is also changed,
this classiﬁer predicts whether there is any unchanged function uf that invokes
Em and should be also changed. The other two classiﬁers separately predict
functions for co-change based on P2 and P3. We consider these three binary-
class classiﬁers as an integrated machine learning model, because all of them can
take in features from one program commit and related software version history,
in order to recommend co-changed functions when possible.

5.3. Phase III: Testing

This phase takes in two inputs—a new program commit cn and the re-
lated software version history, and recommends any unchanged function that
should have been changed by that commit. Speciﬁcally, given cn, CoRec reuses
the steps of Phase I (see Section 5.1) to locate Em, CF Set, and U F Set.
CoRec then pairs up every changed function cf ∈ CF Set with every un-
changed one uf ∈ U F Set, obtaining a set of candidate function pairs Candi =
{(cf1, uf1), (uf1, cf1), (cf1, uf2), . . .}. Next, CoRec extracts features for each
candidate p and sends the features to a pre-trained classiﬁer depending on Em’s
type. If the classiﬁer predicts the function pair to have co-change relationship,
CoRec recommends developers to also modify the unchanged function in p.

20

Table 6: Numbers of commits that are potentially usable for model training and testing

# of Commits
Matching P1

# of Commits
Matching P2

# of Commits
Matching P3

92
67
21
11
14
18
26
22
2
7

77
59
24
8
12
12
12
24
1
3

65
39
28
5
14
5
8
8
4
6

280

232

182

Project

Node.js
Meteor
Ghost
Habitica
PDF.js
React
Serverless
Webpack
Storybook
Electron

Sum

6. Evaluation

In this section, we ﬁrst introduce our experiment setting (Section 6.1) and the
metrics used to evaluate CoRec’s eﬀectiveness (Section 6.2). Then we explain
our investigation with diﬀerent ML algorithms and present CoRec’s sensitivity
to the adopted ML algorithms (Section 6.3), through which we ﬁnalize the
default ML algorithm applied in CoRec. Next we expound on the eﬀectiveness
comparison between CoRec and two existing tools: ROSE [5] and Transitive
Associate Rules (TAR) [9] (Section 6.4). Finally, we present the comparison
between CoRec and a variant approach that trains one uniﬁed classiﬁer instead
of three distinct classiﬁers to recommend changes (Section 6.5).

6.1. Experiment Setting

We mined repositories of the 10 open-source projects introduced in Section 4,
and found three distinct sets of commits in each project that are potentially us-
able for model training and testing. As shown in Table 6, in total, we found 280
commits matching P1, 232 commits matching P2, and 182 commits matching
P3. Each of these pattern matches has at least two co-changed functions (*CF)
depending on Em. In our evaluation, for each data set of each project, we could
use part of the data to train a classiﬁer and use the remaining data to test
the trained classiﬁer. Because Storybook and Electron have few commits, we
excluded them from our evaluation and simply used the identiﬁed commits of
the other eight projects to train and test classiﬁers.

We decided to use k-fold cross validation to evaluate CoRec’s eﬀectiveness.
Namely, for every data set of each project, we split the mined commits into k
portions roughly evenly; each fold uses (k −1) data portions for training and the
remaining portion for testing. Among the eight projects, because each project
has at least ﬁve commits matching each pattern, we set k = 5 to diversify our
evaluation scenarios as much as possible. For instance, Habitica has ﬁve commits
matching P3. When evaluating CoRec’s capability of predicting co-changes for
Hibitica based on P3, in each of the ﬁve folds, we used four commits for training
and one commit for testing. Figure 12 illustrates our ﬁve-fold cross validation
procedure. In the procedure, we ensured that each of the ﬁve folds adopted a

21

Figure 12: Typical data processing for each fold of the ﬁve-fold cross validation

Table 7: Total numbers of prediction tasks involved in the ﬁve-fold cross validation

Project

Node.js
Meteor
Ghost
Habitica
PDF.js
React
Serverless
Webpack

Sum

# of Tasks
Matching P1

# of Tasks
Matching P2

# of Tasks
Matching P3

398
401
76
30
41
72
81
138

1,237

309
229
77
23
31
37
38
90

834

223
107
99
18
35
17
23
22

544

distinct data portion to construct prediction tasks for testing purposes. For any
testing commit that has n co-changed functions (*CF) depending on Em, i.e.,
CF Set = {cf1, cf2, . . . , cfn}, we created n prediction tasks in the following way.
In each prediction task, we included one known changed function cfi (i ∈ [1, n])
together with Em and kept all the other (n − 1) functions unchanged. We
regarded the (n − 1) functions as ground truth (GT ) to assess how accurately
CoRec can recommend co-changes given Em and cfi.

For instance, one prediction task we created in React includes the followings:
Em = src/isomorphic/classic/types.ReactPropTypes.createChainableTypeChecker(...),
cf = src/isomorphic/classic/types.ReactPropTypes.createObjectOfTypeChecker(...),
and GT ={src/isomorphic/ classic/types.ReactPropTypes.createShapeTypeChecker(...)}.
When CoRec blindly pairing cf with any unchanged function, it may extract
feature values as below: feature1 = 1, feature2 = true, feature3 = 0, feature4 =
2, feature5 = 0, feature6 = true, feature7 = true, feature8 = 76%, feature9 =
45%, feature10 = 1}. Table 7 shows the total numbers of prediction tasks we
created for all projects and all patterns among the ﬁve-fold cross validation.

6.2. Metrics

We deﬁned and used four metrics to measure a tool’s capability of recom-
mending co-changed functions: coverage, precision, recall, and F-score. We also

22

Portion 1Portion 2Portion 3Portion 4Portion 5TrainingTestingCommit c1: Em, cf1, cf2, …cfn…Task t11: Em, cf1, {cf2, …, cfn}Task t12: Em, cf2, {cf1, cf3, …, cfn}…Task t1n: Em, cfn,{cf1, …, cfn-1}Commit c2: …deﬁned the weighted average to measure a tool’s overall eﬀectiveness among all
subject projects for each of the metrics mentioned above.

Coverage (Cov) is the percentage of tasks for which a tool can provide
suggestion. Given a task, a tool may or may not recommend any change to
complement the already-applied edit, so this metric assesses the tool applica-
bility. Intuitively, the more tasks for which a tool can recommend one or more
changes, the more applicable this tool is.

Cov =

# of tasks with a tool’s suggestion
Total # of tasks
Coverage varies within [0%, 100%]. If a tool always recommends some change(s)
given a task, its coverage is 100%. All our later evaluations for precision, recall,
and F-score are limited to the tasks covered by a tool. For instance, suppose
that given 100 tasks, a tool can recommend changes for 10 tasks. Then the tool’s
coverage is 10/100 = 10%, and the evaluations for other metrics are based on
the 10 instead of 100 tasks.

× 100%

(2)

Precision (Pre) measures among all recommendations by a tool, how many

of them are correct:

P re =

# of correct recommendations
Total # of recommendations by a tool

× 100%

(3)

This metric evaluates how precisely a tool recommends changes. If all sugges-
tions by a tool are contained by the ground truth, the precision is 100%.

Recall (Rec) measures among all the expected recommendations, how

many of them are actually reported by a tool:

Rec =

# of correct recommendations by a tool
Total # of expected recommendations

× 100%

(4)

This metric assesses how eﬀectively a tool retrieves the expected co-changed
functions. Intuitively, if all expected recommendations are reported by a tool,
the recall is 100%.

F-score (F1) measures the accuracy of a tool’s recommendation:

F 1 =

2 × P re × Rec
P re + Rec

× 100%

(5)

F-score is the harmonic mean of precision and recall. Its value varies within [0%,
100%]. The higher F-scores are desirable, as they demonstrate better trade-oﬀs
between the precision and recall rates.

Weighted Average (WA) measures a tool’s overall eﬀectiveness among

all experimented data in terms of coverage, precision, recall, and F-score:

Γoverall =

(cid:80)8

i=1 Γi ∗ ni
(cid:80)8
i=1 ni

.

(6)

In the formula, i varies from 1 to 8, representing the 8 projects used in our
evaluation (Storybook and Electron were excluded). Here, i = 1 corresponds to
Node.js and i = 8 corresponds to Webpack; ni represents the number of tasks

23

built from the ith project. Γi represents any measurement value of the ith project
for coverage, precision, recall, or F-score. By combining such measurement
values of eight projects in a weighted way, we were able to assess a tool’s overall
eﬀectiveness Γoverall.

6.3. Sensitivity to The Adopted ML Algorithm

We designed CoRec to use Adaboost, with Random Forests as the weak
learners to train classiﬁers. To make this design decision, we tentatively inte-
grated CoRec with ﬁve alternative algorithms: J48 [36], Random Forest [37],
Na¨ıve Bayes [38], Adaboost (default), and Adaboost (Random Forest).

• J48 builds a decision tree as a predictive model to go from observations
about an item (represented in the branches) to conclusions about the
item’s target value (represented in the leaves).

• Na¨ıve Bayes calculates the probabilities of hypotheses by applying Bayes’
theorem with strong (na¨ıve) independence assumptions between features.

• Random Forest is an ensemble learning method that trains a model to
make predictions based on a number of diﬀerent models. Random Forest
trains a set of individual models in a parallel way. Each model is trained
with a random subset of the data. Given a candidate in the testing set,
individual models make their separate predictions and Random Forest uses
the one with the majority vote as its ﬁnal prediction.

• Adaboost is also an ensemble learning method. However, diﬀerent from
Random Forest, Adaboost trains a bunch of individual models (i.e., weak
learners) in a sequential way. Each individual model learns from mis-
takes made by the previous model. We tried two variants of Adaboost:
(1) Adaboost (default) with decision trees as the weak learners, and (2)
Adaboost (Random Forest) with Random Forests as the weak learners.

f
−→CF, *CF

Figure 13 illustrates the eﬀectiveness comparison when CoRec adopts diﬀer-
ent ML algorithms. The three subﬁgures (Figure 13 (a)–(c)) separately present
f
−→AF, and *CF v−→AV.
the comparison results on the data sets of *CF
We observed similar phenomena in all subﬁgures. By comparing the ﬁrst four
basic ML algorithms (J48, Na¨ıve Bayes, Random Forest, and Adaboost (de-
fault)), we noticed that Random Forest achieved the best results in all metrics.
Among all datasets, Na¨ıve Bayes obtained the lowest recall and accuracy rates.
Although Adaboost obtained the second highest F-score, its coverage is the low-
est probably because it uses decision trees as the default weak learners. Based
on our evaluation with the ﬁrst four basic algorithms, we were curious how well
Adaboost performs if it integrates Random Forests as weak learners. Thus, we
also experimented with a ﬁfth algorithm: Adaboost (Random Forest).

As shown in Figure 13, Adaboost (Random Forest) and Random Forest
achieved very similar eﬀectiveness, and both of them considerably outperformed
the other algorithms. But compared with Random Forest, Adaboost (Random

24

(a) The *CF

f
−→CF data

(b) The *CF

f
−→AF data

(c) The *CF v−→AV data

Figure 13: Comparison between diﬀerent ML algorithms on diﬀerent data sets

25

Forest) obtained better precision, better recall, better accuracy, and equal or
slightly lower coverage. Thus, we chose Adaboost (Random Forest) as the de-
fault ML algorithm used in CoRec. Our results imply that although ensemble
learning methods generally outperform other ML algorithms, their eﬀectiveness
also depends on (1) what weak learners are used and (2) how we organize weak
learners. Between Adaboost (Random Forest) and Adaboost (default), the only
diﬀerence exists in the used weak learner (Random Forest vs. Decision Tree).
Our evaluation shows that Random Forest helps improve Adaboost’s perfor-
mance when it is used as the weak learner. Additionally, between Random
Forest and Adaboost (default), the only diﬀerence is how they combine decision
trees as weak learners. Our evaluation shows that Random Forest outperforms
Adaboost by training weak learners in a parallel instead of sequential way.

Finding 5: CoRec is sensitive to the adopted ML algorithm. CoRec
obtained the lowest prediction accuracy when Na¨ıve Bayes was used, but
acquired the highest accuracy when Adaboost (Random Forest) was used.

6.4. Eﬀectiveness Comparison with ROSE and TAR

In our evaluation, we compared CoRec with a popularly used tool ROSE [5]
and a more recent tool Transitive Associate Rules (TAR) [9]. Both of these
tools recommend changes by mining co-change patterns from version history.

Speciﬁcally, ROSE mines the association rules between co-changed entities

from software version history. An exemplar mined rule is shown below:

(7)

{( Qdmodule.c, f unc, Graf Obj getattr())} ⇒
(cid:8) (qdsupport.py, f unc, outputGetattrHook()). (cid:9)
This rule means that whenever the function GrafObj getattr() in a ﬁle Qdmodule.c
is changed, the function outputGetattrHook() in another ﬁle qdsupport.py should
also be changed. Based on such rules, given a program commit, ROSE ten-
tatively matches all edited entities with the antecedents of all mined rules and
recommends co-changes if any tentative match succeeds. Similar to ROSE, TAR
also mines association rules from version history. However, in addition to the
mined rules (e.g., E1 ⇒ E2 and E2 ⇒ E3), TAR also leverages transitive
inference to derive more rules (e.g., E1 ⇒ E3); it computes the conﬁdence
value of each derived rule based on the conﬁdence values of original rules (e.g.,
conf (E1 ⇒ E3) = conf (E1 ⇒ E2) × conf (E2 ⇒ E3)).

In our comparative experiment, we applied ROSE and TAR to the con-
structed prediction tasks and version history of each subject project. We con-
ﬁgured ROSE with support = 1 and conf idence = 0.1, because the ROSE
paper [5] mentioned this setting multiple times and it achieved the best results
by balancing recall and precision. For consistency, we also conﬁgured TAR with
support = 1 and conf idence = 0.1.

As shown in Table 8, CoRec outperformed ROSE and TAR in terms of all
f
measurements. Take Webpack as an example. Among the 138 *CF
−→CF pre-
diction tasks in this project, CoRec provided change recommendations for 89%

26

Table 8: Evaluation results of CoRec, ROSE, and TAR for *CF

f
−→CF tasks (%)

Project

Node.js
Meteor
Ghost
Habitica
PDF.js
React
Serverless
Webpack

CoRec

ROSE

TAR

Cov Pre Rec F1 Cov Pre Rec F1 Cov Pre Rec F1

77
88
73
80
71
91
84
89

68
72
67
80
77
86
77
71

69
70
74
78
81
76
79
81

69
71
71
79
79
81
78
75

61
46
50
40
29
32
64
50

24
16
20
7
27
59
20
7

56
43
53
37
41
70
75
29

34
24
29
12
33
64
32
12

65
52
50
35
33
32
68
50

15
15
14
5
8
57
16
5

62
47
57
42
45
74
80
34

24
23
22
9
14
64
27
9

WA

83

72

73

73

53

21

52

29

57

15

59

24

Table 9: Result comparison among CoRec, ROSE, and TAR for *CF

f
−→AF tasks (%)

Project

Node.js
Meteor
Ghost
Habitica
PDF.js
React
Serverless
Webpack

CoRec

ROSE

TAR

Cov Pre Rec F1 Cov Pre Rec F1 Cov Pre Rec F1

79
86
85
87
65
71
84
75

69
77
86
77
87
84
71
79

74
82
85
85
88
82
85
85

72
80
85
81
87
83
77
82

59
40
46
56
22
16
73
53

20
22
18
4
9
66
19
16

52
44
46
23
28
7
59
46

29
29
26
7
14
13
29
24

61
46
50
58
23
17
74
56

14
21
14
2
11
67
15
13

61
50
49
39
58
8
60
49

23
29
22
4
19
14
24
21

WA

81

76

80

78

54

21

48

28

56

16

55

24

of tasks; with these recommendations, CoRec achieved 71% precision, 81% re-
call, and 75% accuracy. On the other hand, ROSE and TAR recommended
changes for only 50% of tasks; based on its recommendations, ROSE acquired
only 7% precision, 29% recall, and 12% accuracy, while TAR obtained 5% pre-
cision, 34% recall, and 9% accuracy. Among the eight subject projects, the
weighted average measurements of CoRec include 83% coverage, 72% precision,
73% recall, and 73% accuracy. Meanwhile, the weighted average measurements
of ROSE include 53% coverage, 21% precision, 52% recall, and 29% accuracy.
TAR achieved 59% average recall, but its average precision and accuracy are
the lowest among the three tools. Such measurement contrasts indicate that
CoRec usually recommended more changes than ROSE or TAR, and CoRec’s
recommendations were more accurate.

f
−→CF tasks, we also compared CoRec with ROSE and
In addition to *CF
f
−→AF and *CF v−→AV tasks, as shown in Tables 9 and 10. Similar
TAR for *CF
to what we observed in Table 8, CoRec outperformed ROSE and TAR in terms
f
of all metrics for both types of tasks. As shown in Table 9, given *CF
−→AF
tasks, on average, CoRec achieved 81% coverage, 76% precision, 80% recall,
and 78% accuracy. ROSE acquired 54% coverage, 21% precision, 48% recall,
and 28% accuracy. TAR obtained 56% coverage, 16% precision, 55% recall,
and 24% accuracy. In Table 10, for Serverless, CoRec achieved 70% coverage,

27

Table 10: Result comparison among CoRec, ROSE, and TAR for *CF

v
−→AV tasks (%)

Project

Node.js
Meteor
Ghost
Habitica
PDF.js
React
Serverless
Webpack

CoRec

ROSE

TAR

Cov Pre Rec F1 Cov Pre Rec F1 Cov Pre Rec F1

79
72
84
89
78
89
70
87

72
77
75
82
87
73
80
86

77
84
81
85
84
78
85
83

74
81
78
83
85
76
82
85

55
26
46
27
20
36
34
36

20
2
18
20
4
8
0
8

65
14
46
45
28
33
0
33

31
4
26
28
8
13
-
13

56
27
38
28
20
12
38
40

16
2
8
17
5
98
1
3

74
31
70
54
29
34
13
34

26
3
14
26
8
50
2
5

WA

79

76

81

78

45

17

54

25

47

12

62

19

80% precision, 85% recall, and 82% accuracy. Meanwhile, ROSE only provided
recommendations for 34% of the tasks, and none of these recommendations
is correct. TAR only provided recommendations for 38% of tasks; with the
recommendations, TAR achieved 1% precision, 13% recall, and 2% accuracy.

Comparing the results shown in Tables 8–10, we found the eﬀectiveness of
CoRec, ROSE, and TAR to be stable across diﬀerent types of prediction tasks.
Speciﬁcally among the three kinds of tasks, on average, CoRec achieved 79%–
83% coverage, 72%–76% precision, 73%–81% recall, and 73%–78% accuracy.
On the other hand, ROSE achieved 45%–54% coverage, 17%–21% precision,
48%–54% recall, and 25%–29% accuracy; TAR achieved 47%–56% coverage,
12%–16% precision, 55%–62% recall, and 19%–24% accuracy. The consistent
comparison results imply that CoRec usually recommended co-changed func-
tions for more tasks, and CoRec’s recommendations usually had higher quality.
Two major reasons can explain why CoRec worked best. First, ROSE
and TAR purely use the co-changed entities in version history to recommend
changes. When the history data is incomplete or some entities were never co-
changed before, both tools may lack evidence to predict co-changes and thus
obtain lower coverage and recall rates. Additionally, TAR derives more rules
than ROSE via transitive inference. Namely, if E1 ⇒ E2 and E2 ⇒ E3, then
E1 ⇒ E3. However, it is possible that E1 and E3 were never co-changed be-
fore, neither are they related to each other anyhow. Consequently, the derived
rules may contribute to TAR’s lower precision. Meanwhile, CoRec extracts nine
features from a given commit and one feature from the version history; even
though history data provides insuﬃcient indication on the potential co-change
relationship between entities, the other features can serve as supplements.

Second, ROSE and TAR observe no syntactic or semantic relationship be-
tween co-changed entities; thus, they can infer incorrect rules from co-changed
but unrelated entities and achieve lower precision. In comparison, CoRec ob-
serves the syntactic relationship between co-changed entities by tracing the
referencer-referencee relations; it also observes the semantic relationship by ex-
tracting features to reﬂect the commonality (1) between co-changed functions
(*CF), and (2) between any changed function cf and the changed entity E on

28

Table 11: The eﬀectiveness of CoRecu when it trains and tests a uniﬁed classiﬁer (%)

Project

Node.js
Meteor
Ghost
Habitica
PDF.js
React
Serverless
Webpack

WA

Cov

Pre

Rec

72
77
53
55
29
76
54
66

70

50
59
61
53
60
75
47
54

56

57
58
70
68
73
73
61
63

61

F1

53
59
65
60
66
74
53
58

59

which cf depends (E is CF in P1, AF in P2, and AV in P3).

Although CoRec outperformed ROSE and TAR in our experiments, we con-
sider CoRec as a complementary tool to existing tools. This is because CoRec
bases its change recommendations on the three most popular RCPs we found.
If some changes do not match any of the RCPs, CoRec does not recommend
any change but ROSE may suggest some edits.

Finding 6: CoRec outperformed ROSE and TAR when predicting co-
changed functions based on the three recurring change patterns (P1–P3).
CoRec serves as a good complementary tool to both tools.

6.5. Comparison with A Variant Approach

Readers may be tempted to train a uniﬁed classiﬁer instead of three separate
classiﬁers, because the three classiﬁers all take in the same format of inputs
and output the same types of predictions (i.e., whether to co-change or not).
However, as shown in Table 4, the commonality characteristics between co-
changed functions vary with RCPs. For instance, the co-changed functions in
P2 usually commonly invoke peer functions (i.e., FI), the co-changed functions
in P3 often commonly read/write peer variables (i.e., VA), and the co-changed
functions in P1 have weaker commonality signals for both FI and ST (i.e.,
If we mix the co-changed functions matching
common token subsequences).
diﬀerent patterns to train a single classiﬁer, it is quite likely that the extracted
features between co-changed functions become less informative, and the trained
classiﬁer has poorer prediction power.

To validate our approach design, we also built a variant approach of CoRec—
CoRecu—that trains a uniﬁed classiﬁer with the program commits matching ei-
ther of the three RCPs (P1–P3) and predicts co-change functions with the single
classiﬁer. To evaluate CoRecu, we clustered the data portions matching distinct
RCPs for each project, and conducted ﬁve-fold cross validation. As shown in
Table 11, on average, CoRecu recommended changes with 70% coverage, 56%
precision, 61% recall, and 59% accuracy. These measured values are much lower
than the weighted averages of CoRec reported in Tables 8–10. The empirical

29

comparison corroborates our hypothesis that when data matching distinct RCPs
are mixed to train a uniﬁed classiﬁer, the classiﬁer works less eﬀectively.

Finding 7: CoRecu worked less eﬀectively than CoRec by training a
uniﬁed classiﬁer with data matching distinct RCPs. This experiment
validates our approach design of training three separate classiﬁers.

7. Threats to Validity

Threats to External Validity: All our observations and experiment results
are limited to the software repositories we used. These observations and results
may not generalize well to other JS projects, especially to the projects that use
the Asynchronous Module Deﬁnition (AMD) APIs to deﬁne code modules and
their dependencies. In the future, we would like to include more diverse projects
into our data sets so that our ﬁndings are more representative.

Given a project P , CoRec adopts commits in P ’s software version history
to train classiﬁers that can recommend co-changes for new program commits.
When the version history has few commits to train classiﬁers, the applicability of
CoRec is limited. CoRec shares such limitation with existing tools that provide
project-speciﬁc change suggestions based on software version history [5, 39, 9].
To further lower CoRec’s requirement to available commits in software version
history, we plan to investigate more ways to extract features from commits and
better capture the characteristics of co-changed functions.

Threats to Internal Validity: In our experiments, we sampled a subset of
commits in each project based on the keywords “bug”, “ﬁx”, “error”, “adjust”,
and “failure” in commit messages. Our insight is that developers may apply
tangled changes (i.e., unrelated or loosely related code changes) in a single
commit [40]; such commits can introduce data noise and make our research in-
vestigation biased. Based on our experience, the commits with above-mentioned
keywords are likely to ﬁx bugs, and thus each of such commits may be applied
to achieve one maintenance goal and contain no tangled changes. However, the
keywords we used may not always accurately locate bug ﬁxes, neither do they
guarantee that developers apply no tangled changes in individual sampled com-
mits. In the future, we plan to sample commits in other ways and analyze how
our observations vary with the sampling techniques.

Threats to Construct Validity: When creating recommendation tasks for
classiﬁer evaluation, we always assumed that the experimented commits con-
tain accurate information of all co-changed functions. It is possible that devel-
opers made mistakes when applying multi-entity edits. Therefore, the imper-
fect evaluation data set based on developers’ edits may inﬂuence our empirical
comparison between CoRec and ROSE. We share this limitation with prior
In the future, we plan to mitigate the problem
work [5, 39, 9, 41, 42, 8, 7].
by conducting user studies with developers. By carefully examining the edits
made by developers and the co-changed functions recommended by tools, we
can better assess the eﬀectiveness of diﬀerent tools.

30

8. Related Work

The related work includes empirical studies on JS code and related program

changes, JS bug detectors, and co-change recommendation systems.

8.1. Empirical Studies on JS Code and Related Program Changes

Various studies were conducted to investigate JS code and related changes [43,
44, 45, 46, 47]. For instance, Ocariza et al. conducted an empirical study of 317
bug reports from 12 bug repositories, to understand the root cause and con-
sequence of each reported bug [43]. They observed that 65% of JS bugs were
caused by the faulty interactions between JS code and Document Object Mod-
els (DOMs). Gao et al. empirically investigated the beneﬁts of leveraging static
type systems (e.g., Facebook’s Flow [48] and Microsoft’s TypeScript [49]) to
check JS programs [45]. To do that, they manually added type annotations to
buggy code and tested whether Flow and TypeScript reported an error on the
buggy code. They observed that both Flow 0.30 and TypeScript 2.0 detected
15% of errors, showing great potential of ﬁnding bugs.

Silva et a. [47] extracted changed source ﬁles from software version history,
and revealed six co-change patterns by mapping frequently co-changed ﬁles to
their ﬁle directories. Our research is diﬀerent in three ways. First, we focused on
software entities with ﬁner granularities than ﬁles; we extracted the co-change
patterns among classes, functions, variables, and statement blocks. Second,
since unrelated entities are sometimes accidentally co-changed in program com-
mits, we exploited the syntactic dependencies between entities to remove such
data noise and to improve the quality of identiﬁed patterns. Third, CoRec
uses the identiﬁed patterns to further recommend co-changes with high quality.
Wang et al. [18] recently conducted a study on multi-entity edits applied to
Java programs, which study is closely relevant to our work. Wang et al. focused
on three kinds of software entities: classes, methods, and ﬁelds. They created
CDGs for individual multi-entity edits, and revealed RCPs by comparing CDGs.
The three most popular RCPs they found are: *CM m−→CM (a callee method is
co-changed with its caller(s)), *CM m−→AM (a method is added, and one or more
f
existing methods are changed to invoke the added method), and *CM
−→AF (a
ﬁeld is added, and at least one existing method is changed to access the ﬁeld).
Our research is inspired by Wang et al.’s work. We decided to conduct a
similar study on JS programs mainly because JS is very diﬀerent from Java. For
instance, JS is weakly typed and has more ﬂexible syntax rules; Java is strongly
typed and variables must be declared before being used. JS is a script language
and mainly used to make web pages more interactive; Java is used in more do-
mains. We were curious whether developers’ maintenance activities vary with
the programming languages they use, and whether there are unique co-change
patterns in JS programs. In our study, we adopted JS parsing tools, identiﬁed
four kinds of entities in various ways, and did reveal some co-change patterns
unique to JS programs because of the language’s unique features. Surprisingly,
the three most popular JS co-change patterns we observed match exactly with

31

the Java co-change patterns mentioned above. Our study corroborates obser-
vations made by prior work. More importantly, it indicates that even though
diﬀerent programming languages provide distinct features, developers are likely
to apply multi-entity edits in similar ways. This phenomenon sheds lights on
future research directions of cross-language co-change recommendations.

8.2. JS Bug Detectors

Researchers built tools to automatically detect bugs or malicious JS code [50,
51, 52, 53, 2, 54, 55, 56]. For example, EventRacer detects harmful data races
in even-driven programs [53]. JSNose combines static and dynamic analysis
to detect 13 JS smells in client-side code, where smells are code patterns that
can adversely inﬂuence program comprehension and software maintenance [2].
TypeDevil adopts dynamic analysis to warn developers about variables, prop-
erties, and functions that have inconsistent types [55]. DeepBugs is a learning-
based approach that formulates bug detection as a binary classiﬁcation problem;
it is able to detect accidentally swapped function arguments, incorrect binary
operators, and incorrect operands in binary operations [56]. EarlyBird conducts
dynamic analysis and adopts machine learning techniques for early identiﬁcation
of malicious behaviors of JavaScript code [52].

Some other researchers developed tools to suggest bug ﬁxes or code refac-
torings [57, 58, 59, 60, 61, 62, 63]. With more details, Vejovis suggests program
repairs for DOM-related JS bugs based on two common ﬁx patterns: parameter
replacements and DOM element validations [60]. Monperrus and Maia built a
JS debugger to help resolve “crowd bugs” (i.e., unexpected and incorrect out-
puts or behaviors resulting from the common and intuitive usage of APIs) [61].
Given a crowd bug, the debugger sends a code query to a server and retrieves all
StackOverﬂow answers potentially related to the bug ﬁx. An and Tilevich built
a JS refactoring tool to facilitate JS debugging and program repair [63]. Given a
distributed JS application, the tool ﬁrst converts the program to a semantically
equivalent centralized version by gluing together the client and server parts.
After developers ﬁxed bugs in the centralized version, the tool generates ﬁxes
for the original distributed version accordingly. In Model-Driven Engineering,
ReVision repairs incorrectly updated models by (1) extracting change patterns
from version history, and (2) matching incorrect updates against those patterns
to suggest repair operations [64].

Our methodology is most relevant to the approach design of ReVision. How-
ever, our research is diﬀerent in three aspects. First, our research focuses on
entity-level co-change patterns in JS programs, while ReVision checks for con-
sistencies diﬀerent UML artifacts (e.g., the signature of a message in a sequence
diagram must correspond to a method signature in the related class diagram).
Second, the co-changed recommendation by CoRec intends to complete an ap-
plied multi-entity edit, while the repair operations proposed by ReVision tries to
complete consistency-preserving edit operations. Third, we conducted a large-
scale empirical study to characterize multi-entity edits and experimented CoRec
with eight open-source projects, while ReVision was not empirically evaluated.

32

8.3. Co-Change Recommendation Systems

Approaches were introduced to mine software version history and to extract
co-change patterns [47, 65, 66, 67, 5, 68, 39, 9, 69, 70, 71, 72, 73, 6]. Speciﬁcally,
Some researchers developed tools (e.g., ROSE) to mine the association rules
between co-changed entities and to suggest possible changes accordingly [5, 68,
39, 9, 69, 73, 6]. Some other researchers built hybrid approaches by combining
information retrieval (IR) with association rule mining [70, 71, 72]. Given a
software entity E, these approaches use IR techniques to (1) extract terms from
E and any other entity and (2) rank those entities based on their term-usage
overlap with E. Meanwhile, these tools also apply association rule mining to
commit history in order to rank entities based on the co-change frequency. In
this way, if an entity G has signiﬁcant term-usage overlap with E and has been
co-changed a lot with E, then G is recommended to be co-changed with E.

Shirabad et al. developed a learning-based approach that predicts whether
two given ﬁles should be changed together or not [67]. In particular, the re-
searchers extracted features from software repository to represent the relation-
ship between each pair of ﬁles, adopted those features of ﬁle pairs to train an
ML model, and leveraged the model to predict whether any two ﬁles are rele-
vant (i.e., should be co-changed) or not. CoRec is closely related to Shirabad
et al.’s work. However, it is diﬀerent in two aspects. First, CoRec predicts
co-changed functions instead of co-changed ﬁles. With ﬁner-granularity rec-
ommendations, CoRec can help developers to better validate suggested changes
and to edit code more easily. Second, our feature engineering for CoRec is based
on the quantitative analysis of frequent change patterns and qualitative analysis
of the commonality between co-changed functions, while the feature engineer-
ing by Shirabad is mainly based on their intuitions. Consequently, most of our
features are about the code commonality or co-evolution relationship between
functions; while the features deﬁned by Shirabad et al. mainly focus on ﬁle
names/paths, routines referenced by each ﬁle, and the code comments together
with problem reports related to each ﬁle.

f
−→CF, *CF

Wang et al. built CMSuggester—an automatic approach to suggest method-
level co-changes [7, 8]. Diﬀerent from CMSuggester, CoRec is an ML-based
instead of rule-based approach; it requires for data to train an ML model be-
fore suggesting changes while CMSuggester requires tool builders to hardcode
the suggestion strategies. CoRec recommends changes based on three RCPs:
f
−→AF, and *CF v−→AV; CMSuggester recommends changes based
*CF
on the last two patterns. Our approach is more applicable. CMSuggester applies
partial program analysis (PPA) to identify the referencer-referencee relationship
between entities; it does not work when PPA is inapplicable. Meanwhile, CoRec
uses two alternative ways to infer the referencer-referencee relationship: typed-
ast-util can accurately resolve bindings and link entities, while our heuristic
approach links entities less accurately but is always applicable even if typed-
ast-util does not work. Additionally, our evaluation is more comprehensive. We
evaluated CoRec by integrating it with diﬀerent ML algorithms and using the
program data of eight open-source projects; CMSuggester was evaluated with

33

the data of only four projects. Overall, CoRec is more ﬂexible due to its usage
of ML and is applicable to more types of co-change scenarios.

9. Conclusion

f
−→CF, *CF

It is usually tedious and error-prone to develop and maintain JS code. To
facilitate program comprehension and software debugging, we conducted an
empirical study on multi-entity edits in JS projects and built an ML-based
co-change recommendation tool CoRec. Our empirical study explored the fre-
quency and composition of multi-entity edits in JS programs, and investigated
the syntactic and semantic relevance between frequently co-changed entities. In
particular, we observed that (i) JS software developers frequently apply multi-
entity edits while the co-changed entities are usually syntactically related; (ii)
there are three most popular RCPs that commonly exist in all studied JS code
f
−→AF, and *CF v−→AV; and (iii) among the entities
repositories: *CF
matching these three RCPs, co-changed functions usually share certain com-
monality (e.g., common function invocations and common token subsequences).
Based on our study, we developed CoRec, which tool extracts code features
from the multi-entity edits that match any of the three RCPs, and trains an ML
model with the extracted features to specially characterize relationship between
co-changed functions. Given a new program commit or a set of entity changes
that developers apply, the trained model extracts features from the program
revision and recommends changes to complement applied edits as necessary.
Our evaluation shows that CoRec recommended changes with high accuracy and
outperformed two existing techniques. In the future, we will investigate novel
approaches to provide ﬁner-grained code change suggestions and automate test
case generation for suggested changes.

References

[1] The 10 most popular programming languages,
GitHub,

according to the
https://www.businessinsider.com/

Microsoft-owned
most-popular-programming-languages-github-2019-11 (2019).

[2] A. M. Fard, A. Mesbah, Jsnose: Detecting javascript code smells, in: 2013
IEEE 13th International Working Conference on Source Code Analysis and
Manipulation (SCAM), 2013, pp. 116–125.

[3] A. Saboury, P. Musavi, F. Khomh, G. Antoniol, An empirical study of
code smells in javascript projects, 2017, pp. 294–305. doi:10.1109/SANER.
2017.7884630.

[4] R. Ferguson, Introduction to JavaScript, Apress, Berkeley, CA, 2019, pp.

1–10. doi:10.1007/978-1-4842-4395-4_1.
URL https://doi.org/10.1007/978-1-4842-4395-4_1

34

[5] T. Zimmermann, P. Weisgerber, S. Diehl, A. Zeller, Mining version histories

to guide software changes, in: Proc. ICSE, 2004, pp. 563–572.

[6] T. Rolfsnes, L. Moonen, S. D. Alesio, R. Behjati, D. Binkley, Aggregating
association rules to improve change recommendation, Empirical Software
Engineering 23 (2) (2018) 987–1035.

[7] Y. Wang, N. Meng, H. Zhong, Cmsuggester: Method change suggestion to

complement multi-entity edits, in: Proc. SATE, 2018, pp. 137–153.

[8] Z. Jiang, Y. Wang, H. Zhong, N. Meng, Automatic method change
suggestion to complement multi-entity edits, Journal of Systems and
Software 159.
URL http://login.ezproxy.lib.vt.edu/login?url=http://search.
ebscohost.com/login.aspx?direct=true&db=edselc&AN=edselc.
2-52.0-85073505497&site=eds-live&scope=site

[9] M. A. Islam, M. M. Islam, M. Mondal, B. Roy, C. K. Roy, K. A. Schneider,
[research paper] detecting evolutionary coupling using transitive association
rules, in: Proc. SCAM, 2018, pp. 113–122.

[10] Z. P. Fry, W. Weimer, A human study of fault localization accuracy, in:
2010 IEEE International Conference on Software Maintenance, 2010, pp.
1–10. doi:10.1109/ICSM.2010.5609691.

[11] T. T. Nguyen, H. A. Nguyen, N. H. Pham, J. Al-Kofahi, T. N. Nguyen,
Recurring bug ﬁxes in object-oriented programs, in: ACM/IEEE Interna-
tional Conference on Software Engineering, 2010, pp. 315–324. doi:http:
//doi.acm.org/10.1145/1806799.1806847.

[12] Z. Yin, D. Yuan, Y. Zhou, S. Pasupathy, L. Bairavasundaram, How do ﬁxes

become bugs?, in: Proc. ESEC/FSE, 2011, pp. 26–36.

[13] J. Park, M. Kim, B. Ray, D.-H. Bae, An empirical study of supplementary
bug ﬁxes, in: IEEE Working Conference on Mining Software Repositories,
2012, pp. 40–49.

[14] Nodejs node, https://github.com/nodejs/node (2020).

[15] Fs: make callback mandatory to all async functions, https://github.

com/nodejs/node/commit/21b0a27 (2016).

[16] What Is ES6 and What Javascript Programmers Need to Know, https:
//www.makeuseof.com/tag/es6-javascript-programmers-need-know/
(2017).

[17] X. Ren, F. Shah, F. Tip, B. G. Ryder, O. Chesley, Chianti: A tool for
change impact analysis of java programs, in: Proceedings of the 19th An-
nual ACM SIGPLAN Conference on Object-oriented Programming, Sys-
tems, Languages, and Applications, OOPSLA ’04, ACM, New York, NY,

35

USA, 2004, pp. 432–448. doi:10.1145/1028976.1029012.
URL http://doi.acm.org/10.1145/1028976.1029012

[18] Y. Wang, N. Meng, H. Zhong, An empirical study of multi-entity changes

in real bug ﬁxes, in: Proc. ICSME, 2018, pp. 287–298.

[19] Esprima (2020).

URL https://esprima.org/

[20] typed-ast-util,

https://github.com/returntocorp/typed-ast-util

(2020).

[21] Fix some ﬁbers vs SQLite issues, https://github.com/meteor/meteor/

commit/e9a88b00b9cdd35eb281c7113fcaa5155f006ea3 (2020).

[22] Meteor, https://github.com/meteor/meteor (2020).

[23] J. Falleri, F. Morandat, X. Blanc, M. Martinez, M. Monperrus, Fine-
grained and accurate source code diﬀerencing, in: ACM/IEEE Interna-
tional Conference on Automated Software Engineering, ASE ’14, Vasteras,
Sweden - September 15 - 19, 2014, 2014, pp. 313–324. doi:10.1145/
2642937.2642982.
URL http://doi.acm.org/10.1145/2642937.2642982

[24] L. P. Cordella, P. Foggia, C. Sansone, M. Vento, A (sub)graph isomor-
phism algorithm for matching large graphs, IEEE Transactions on Pat-
tern Analysis and Machine Intelligence 26 (10) (2004) 1367–1372. doi:
10.1109/TPAMI.2004.75.

[25] Ghost, https://github.com/TryGhost/Ghost (2020).

[26] Habitrpg habitica, https://github.com/HabitRPG/habitica (2020).

[27] Mozilla pdf, https://github.com/mozilla/pdf.js/ (2020).

[28] Facebook react, https://github.com/facebook/react (2020).

[29] Serverless, https://github.com/serverless/serverless (2020).

[30] Webpack, https://github.com/webpack/webpack (2020).

[31] Storybook, https://github.com/storybookjs/storybook (2020).

[32] Electron, https://github.com/electron/electron (2020).

[33] Http2:

introducing HTTP/2,

https://github.com/nodejs/node/

commit/e71e71b5138c3dfee080f4215dd957dc7a6cbdaf (2017).

[34] J. Ingeno, Software Architect’s Handbook: Become a Successful Software
Architect by Implementing Eﬀective Architecture Concepts, Packt Pub-
lishing, 2018.

36

[35] Y. Freund, R. E. Schapire, Experiments with a new boosting algorithm, in:
Proceedings of the Thirteenth International Conference on International
Conference on Machine Learning, ICML’96, Morgan Kaufmann Publishers
Inc., San Francisco, CA, USA, 1996, pp. 148–156.

[36] J. R. Quinlan, C4.5: Programs for Machine Learning, Morgan Kaufmann

Publishers Inc., San Francisco, CA, USA, 1993.

[37] A. Liaw, M. Wiener, Classiﬁcation and regression by randomforest, R News

2 (3) (2002) 18–22.
URL https://CRAN.R-project.org/doc/Rnews/

[38] D. D. Lewis, Naive (bayes) at forty: The independence assumption in infor-
mation retrieval, in: C. N´edellec, C. Rouveirol (Eds.), Machine Learning:
ECML-98, Springer Berlin Heidelberg, Berlin, Heidelberg, 1998, pp. 4–15.

[39] T. Rolfsnes, S. D. Alesio, R. Behjati, L. Moonen, D. W. Binkley, Gen-
eralizing the analysis of evolutionary coupling for software change impact
analysis, in: Proc. SANER, 2016, pp. 201–212.

[40] K. Herzig, A. Zeller, The impact of tangled code changes, in: 2013 10th
Working Conference on Mining Software Repositories (MSR), 2013, pp.
121–130. doi:10.1109/MSR.2013.6624018.

[41] N. Meng, M. Kim, K. McKinley, Lase: Locating and applying systematic

edits, in: Proc. ICSE, 2013, pp. 502–511.

[42] Tan, Ming, Online defect prediction for imbalanced data, Master’s thesis,

University of Waterloo (2015).

[43] F. Ocariza, K. Bajaj, K. Pattabiraman, A. Mesbah, An empirical study of
client-side javascript bugs., 2013 ACM / IEEE International Symposium
on Empirical Software Engineering and Measurement, Empirical Software
Engineering and Measurement, 2013 ACM / IEEE International Sympo-
sium on, Empirical Software Engineering and Measurement (ESEM), 2012
ACM-IEEE International Symposium on (2013) 55 – 64.
URL http://login.ezproxy.lib.vt.edu/login?url=http://search.
ebscohost.com/login.aspx?direct=true&db=edseee&AN=edseee.
6681338&site=eds-live&scope=site

[44] M. Selakovic, M. Pradel, Performance issues and optimizations in
javascript: An empirical study, in: 2016 IEEE/ACM 38th International
Conference on Software Engineering (ICSE), 2016, pp. 61–72. doi:10.
1145/2884781.2884829.

[45] Z. Gao, C. Bird, E. T. Barr, To type or not to type: Quantifying de-
tectable bugs in javascript, in: 2017 IEEE/ACM 39th International Con-
ference on Software Engineering (ICSE), 2017, pp. 758–769. doi:10.1109/
ICSE.2017.75.

37

[46] P. Gyimesi, B. Vancsics, A. Stocco, D. Mazinanian, ´A. Besz´edes, R. Ferenc,
A. Mesbah, Bugsjs: a benchmark of javascript bugs, in: 2019 12th IEEE
Conference on Software Testing, Validation and Veriﬁcation (ICST), 2019,
pp. 90–101. doi:10.1109/ICST.2019.00019.

[47] L. L. Silva, M. T. Valente, M. A. Maia, Co-change patterns: A large scale
empirical study, Journal of Systems and Software 152 (2019) 196 – 214.
doi:https://doi.org/10.1016/j.jss.2019.03.014.
URL
S0164121219300597

http://www.sciencedirect.com/science/article/pii/

[48] Flow: A Static Type Checker for JavaScript, https://flow.org (2020).

[49] TypeScript - JavaScript that scales., https://www.typescriptlang.org

(2020).

[50] M. Cova, C. Kruegel, G. Vigna, Detection and analysis of drive-by-
download attacks and malicious javascript code, in: Proceedings of the
19th International Conference on World Wide Web, WWW ’10, Associa-
tion for Computing Machinery, New York, NY, USA, 2010, pp. 281–290.
doi:10.1145/1772690.1772720.
URL https://doi.org/10.1145/1772690.1772720

[51] F. S. Ocariza Jr., K. Pattabiraman, A. Mesbah, Autoﬂox: An automatic
fault localizer for client-side javascript, in: 2012 IEEE Fifth International
Conference on Software Testing, Veriﬁcation and Validation, 2012, pp. 31–
40.

[52] K. Sch¨utt, M. Kloft, A. Bikadorov, K. Rieck, Early detection of malicious
behavior in javascript code, in: Proceedings of the 5th ACM Workshop on
Security and Artiﬁcial Intelligence, AISec ’12, Association for Computing
Machinery, New York, NY, USA, 2012, pp. 15–24. doi:10.1145/2381896.
2381901.
URL https://doi.org/10.1145/2381896.2381901

[53] V. Raychev, M. Vechev, M. Sridharan, Eﬀective race detection for event-
driven programs, ACM SIGPLAN NOTICES 48 (10) (2013) 151 – 166.
URL
//search.ebscohost.com/login.aspx?direct=true&db=edswsc&AN=
000327697300008&site=eds-live&scope=site

http://login.ezproxy.lib.vt.edu/login?url=http:

[54] J. Park, Javascript api misuse detection by using typescript, in: Proceed-
ings of the Companion Publication of the 13th International Conference on
Modularity, MODULARITY ’14, Association for Computing Machinery,
New York, NY, USA, 2014, pp. 11–12. doi:10.1145/2584469.2584472.
URL https://doi.org/10.1145/2584469.2584472

[55] M. Pradel, P. Schuh, K. Sen, Typedevil: Dynamic type inconsistency anal-
ysis for javascript, 2015 IEEE/ACM 37th IEEE International Conference

38

on Software Engineering (2015) 314.
URL http://login.ezproxy.lib.vt.edu/login?url=http://search.
ebscohost.com/login.aspx?direct=true&db=edb&AN=110064267&
site=eds-live&scope=site

[56] M. Pradel, K. Sen, Deepbugs: A learning approach to name-based bug de-
tection, Proc. ACM Program. Lang. 2 (OOPSLA). doi:10.1145/3276517.
URL https://doi.org/10.1145/3276517

[57] A. Feldthaus, T. Millstein, A. Møller, M. Sch¨afer, F. Tip, Tool-supported
refactoring for javascript, SIGPLAN Not. 46 (10) (2011) 119–138. doi:
10.1145/2076021.2048078.
URL https://doi.org/10.1145/2076021.2048078

[58] F. Meawad, G. Richards, F. Morandat, J. Vitek, Eval begone!

semi-
automated removal of eval from javascript programs, SIGPLAN Not.
47 (10) (2012) 607–620. doi:10.1145/2398857.2384660.
URL https://doi.org/10.1145/2398857.2384660

[59] S. H. Jensen, P. A. Jonsson, A. Møller, Remedying the eval that men do, in:
Proceedings of the 2012 International Symposium on Software Testing and
Analysis, ISSTA 2012, Association for Computing Machinery, New York,
NY, USA, 2012, pp. 34–44. doi:10.1145/2338965.2336758.
URL https://doi.org/10.1145/2338965.2336758

[60] F. Ocariza, K. Pattabiraman, A. Mesbah, Vejovis: Suggesting ﬁxes for
javascript faults, in: Proceedings - International Conference on Software
Engineering, no. 1, Electrical and Computer Engineering, University of
British Columbia, 2014, pp. 837–847.
URL http://login.ezproxy.lib.vt.edu/login?url=http://search.
ebscohost.com/login.aspx?direct=true&db=edselc&AN=edselc.
2-52.0-84993660437&site=eds-live&scope=site

[61] M. Monperrus, A. Maia, Debugging with the Crowd: a Debug Recom-
mendation System based on Stackoverﬂow, Research Report hal-00987395,
Universit´e Lille 1 - Sciences et Technologies (2014).
URL https://hal.archives-ouvertes.fr/hal-00987395

[62] M. Selakovic, M. Pradel, Poster: Automatically ﬁxing real-world javascript
performance bugs, 2015 ICSE International Conference on Software
Engineering. (2015) 811.
URL
edb&AN=111750044&site=eds-live&scope=site.

http://search.ebscohost.com/login.aspx?direct=true&db=

[63] K. An, E. Tilevich, Catch & release: An approach to debugging distributed
full-stack javascript applications, in: M. Bakaev, F. Frasincar, I.-Y. Ko
(Eds.), Web Engineering, Springer International Publishing, Cham, 2019,
pp. 459–473.

39

[64] M. Ohrndorf, C. Pietsch, T. Kehrer, ReVision: A tool for history-based
model repair recommendations, in: Proc. ICSE-Companion, 2018, p. 105.

[65] H. Gall, K. Hajek, M. Jazayeri, Detection of logical coupling based on

product release history, in: Proc. ICSM, 1998, pp. 190–198.

[66] H. Gall, M. Jazayeri, J. Krajewski, CVS release history data for detecting

logical couplings, in: Proc. IWPSE, 2003, pp. 13–23.

[67] J. S. Shirabad, T. C. Lethbridge, S. Matwin, Mining the maintenance his-
tory of a legacy software system, in: Proc. ICSM, 2003, pp. 95–104.

[68] A. T. T. Ying, G. C. Murphy, R. T. Ng, M. Chu-Carroll, Predicting source
code changes by mining change history., IEEE Trans. Software Eng. 30 (9)
(2004) 574–586.

[69] H. Kagdi, J. I. Maletic, B. Sharif, Mining software repositories for trace-

ability links, in: Proc. ICPC, 2007, pp. 145–154.

[70] H. H. Kagdi, M. Gethers, D. Poshyvanyk, Integrating conceptual and log-
ical couplings for change impact analysis in software, Empirical Software
Engineering 18 (2012) 933–969.

[71] M. Gethers, B. Dit, H. Kagdi, D. Poshyvanyk, Integrated impact analysis

for managing software changes, in: Proc. ICSE, 2012, pp. 430–440.

[72] M. B. Zanjani, G. Swartzendruber, H. Kagdi, Impact analysis of change
requests on source code based on interaction and commit histories, in:
Proc. MSR, 2014, pp. 162–171.

[73] L. L. Silva, M. T. Valente, M. de Almeida Maia, Co-change clusters: Ex-
traction and application on assessing software modularity, Trans. Aspect-
Oriented Software Development 12 (2015) 96–131.

40

