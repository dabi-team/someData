Towards Self-Regulating AI: Challenges and Opportunities of AI
Model Governance in Financial Services
Hongda Shen
University of Alabama
Huntsville, Alabama
hs0017@alumni.uah.edu

Jiahao Chen
J. P. Morgan AI Research
New York, New York
jiahao.chen@jpmorgan.com

Eren Kurshan
Columbia University
New York, New York
ek2529@columbia.edu

0
2
0
2

t
c
O
9

]

G
L
.
s
c
[

1
v
7
2
8
4
0
.
0
1
0
2
:
v
i
X
r
a

ABSTRACT
AI systems have found a wide range of application areas in finan-
cial services. Their involvement in broader and increasingly critical
decisions has escalated the need for compliance and effective model
governance. Current governance practices have evolved from more
traditional financial applications and modeling frameworks. They
often struggle with the fundamental differences in AI characteris-
tics such as uncertainty in the assumptions, and the lack of explicit
programming. AI model governance frequently involves complex
review flows and relies heavily on manual steps. As a result, it faces
serious challenges in effectiveness, cost, complexity, and speed.
Furthermore, the unprecedented rate of growth in the AI model
complexity raises questions on the sustainability of the current
practices. This paper focuses on the challenges of AI model gover-
nance in the financial services industry. As a part of the outlook, we
present a system-level framework towards increased self-regulation
for robustness and compliance. This approach aims to enable poten-
tial solution opportunities through increased automation and the
integration of monitoring, management, and mitigation capabilities.
The proposed framework also provides model governance and risk
management improved capabilities to manage model risk during
deployment.

KEYWORDS
Financial services, model governance, model risk management,
machine learning, artificial intelligence

ACM Reference Format:
Eren Kurshan, Hongda Shen, and Jiahao Chen. 2020. Towards Self-
Regulating AI: Challenges and Opportunities of AI Model Governance in
Financial Services. In ACM International Conference on AI in Finance (ICAIF
’20), October 15–16, 2020, New York, NY, USA. ACM, New York, NY, USA,
8 pages. https://doi.org/10.1145/3383455.3422564

1 INTRODUCTION
In recent years, AI adoption in the financial services industry has
grown significantly. AI applications span a wide range of business
functions such as new product development, business operations,
customer service and client acquisition [21, 27]. The range and

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ICAIF ’20, October 15–16, 2020, New York
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7584-9/20/10. . . $15.00
https://doi.org/10.1145/3383455.3422564

Figure 1: Financial institutions in the US are regulated by a number
of regulatory entities at local (yellow), state (yellow), federal (blue)
and international levels (green) [24].
criticality of the decisions made by AI have increased the need for
compliance and effective model governance.

In financial services, all models are required to go through an
internal risk management and regulatory compliance process [30].
As shown in Figure 1, this was historically built to address the
requirements of a complex system of regulatory entities. Since its
origins from the compliance with regulations from the Office of
the Comptroller of the Currency (OCC) and CRD IV, CRR in the
European Union, this risk management and compliance process has
evolved with modeling frameworks used in finance. The resulting
process struggles with the differences in the underlying charac-
teristics of the AI models such as intrinsic learning from the data
without explicit programming and the lack of a priori assumptions.
Traditional financial modeling frameworks produce more trans-
parent models, in which the outputs have clear conditional de-
pendencies on the model parameters and variables. Hence, the
model governance reviews on these variables, parameters, and
their relationships are more effective. In contrast, recent model-
ing frameworks more often produce a model as the output, with
intrinsically encoded data representations and no clear dependen-
cies between the model parameters and the outputs. As a result,
current governance practices struggle with AI models, regardless
of how rigorous the reviews are, and face serious issues in terms
of effectiveness, agility, cost, and complexity. It is unclear if the
fine-grained and manual reviews are sustainable with the current
rates of growth in the AI model complexity. Model governance
requires more system-level evaluation for the AI models, as well as
more data and behavior-centric analysis.

In recent months, the AI models in financial services have gone
through a natural stress test due to the global pandemic. During
this period, most models experienced serious accuracy and relia-
bility issues [25]. Financial firms struggled with recalibrating and
retraining their existing AI models, as well as trying to rapidly build
new ones. Many of these efforts faced serious challenges, due to

1

 
 
 
 
 
 
ICAIF ’20, October 15–16, 2020, New York

Eren Kurshan, Hongda Shen, and Jiahao Chen

limited automation and the complexities of the development and
governance processes [25]. However, they built up the interest and
urgency to develop more robust AI systems and upgrading model
governance solutions.

Overall, these factors motivate a strategic change in AI gov-
ernance, from the existing sequential manual processes towards
a system-level approach with increased self-regulation capabilities
through continuous monitoring and mitigation. This can be achieved
at the boundary of the AI and governance system designs. This
paper has 3 primary goals:
• Overview some of the common challenges in AI model gover-
nance and start an initial conversation on the pain points and
solution opportunities.

• Present an AI system framework that enables the integration
and automation of key governance functions towards increased
self-regulation.

• Discuss the potential solution opportunities as well as adoption

considerations.

The paper is structured as follows: Section 1.1 is a high-level
overview of model risk management; Section 2 discusses some
of the practical issues in AI governance; Section 3 presents an
AI system framework and building blocks; Section 4 overviews
the potential solution opportunities; and Section 5 discusses the
adoption considerations. This paper does not aim to propose the
next-generation risk management framework for the financial
services industry. Rather, it explores challenges and novel solution
approaches specifically for AI models, from a model and system
development perspective. Also, it does not aim to fully automate
governance processes or achieve complete self-regulation. Instead,
it highlights opportunities to improve the AI system design and
streamline the process through increased automation. In this paper,
we use a broader definition of model governance which covers
the end-to-end process, including but not limited to, model risk
management.

1.1 Overview of the Model Governance Process
Model risk management aims to identify and minimize the risks
associated with the models used in the financial services industry.
At a high level, it aims to identify key risks, assess, minimize, and
monitor them over the model’s lifecycle. The process has many
steps covering the development, implementation, testing, and de-
ployment stages. The exact processes vary significantly across the
industry. Some of the common steps include (but are not limited
to): (1) Model Risk Rating: Initial risk assessment guides the entire
governance process and typically considers the materiality and
the model characteristics. Higher risk models go through more
rigorous and extensive reviews. Model use review ensures that the
model is used for the intended use case and matches the require-
ments. (2) Initial Model Validation: (2.a) Model Assessment: Reviews
the model in terms of the reasonableness of the overall modeling
framework. It typically includes an assessment of the application
characteristics, assumptions, and data inputs. (2.b) Process Verifica-
tion: High-level variable selection, model code, and data sources
are verified. (2.c) Results Analysis: Evaluates the performance of
the model. Frequently involves back-testing, benchmarking and
other evaluations. (3) On-Going Monitoring: Supervises the model

Figure 2: Regulatory requirements by the Office of the Comptroller
of the Currency (OCC) over the model’s lifecycle.

behavior periodically during the deployment. Periodic reviews of its
outputs are planned based on the model risk classification (quarterly,
semi-annually, yearly), where the higher risk models are reviewed
more frequently.

2 AI MODEL GOVERNANCE CHALLENGES
2.1 Governance Process Design
Challenge 1. Model governance was designed for traditional finan-
cial models, not AI.
As discussed in Section 1, model governance has its historical roots
in traditional financial models such as capital analysis and asset
pricing. The resulting governance processes were designed around
statistical models with clear assumptions, well-established relation-
ships among variables, and explicit programs. More recent modeling
frameworks exhibit fundamentally different characteristics such
as learning without explicit programming, inherent opacity, and
conditional uncertainties. These dissimilarities limit the effective-
ness of the current practices. Studies have highlighted the need
to customize the governance process to focus more on AI-specific
criteria and risks [12, 22].

Challenge 2. Regulatory requirements span the full model lifecycle,
while governance feedback mostly focuses on post-development.

Figure 2 shows some of the regulatory checks mandated by the
OCC, covering the entire model development and deployment time-
lines. Nevertheless, model governance reviews typically focus on
later, post-development stages, whose timing is partially driven by
the model review backlogs and bandwidth limitations of the model
risk management teams. The lack of automated governance tools
and feedback during the earlier stages of development causes poten-
tial risks and concerns to be identified in later stages. This delays
the development timelines and causes restarts to the development
and governance processes.

2.2 Manual Processes and Reviews
Challenge 3. Each AI governance review is unique and has subjective
variations.
In recent years, financial institutions have taken substantial steps
towards standardizing their internal model risk management proce-
dures. Many firms offer high-level guidelines, process templates and
documents. However, governance procedures still rely on custom
steps decided on a case-by-case basis. Often, the individual gover-
nance steps, target metrics, and KPIs are customized for each model.
Governance reviews vary from model to model, governance team

2

Towards Self-Regulating AI: Challenges and Opportunities of AI Model Governance in Financial Services

ICAIF ’20, October 15–16, 2020, New York

Challenge 5. Defining accountability in the complex governance
process flows is difficult.

Complex review flows exacerbate the underlying accountability
issues in AI models. The interlacing governance flows inherently
distribute the decision making power pervasively among numerous
stakeholder organizations. At times, the strategic goals and risk
appetites of the teams involved may conflict. Business organization
strategies may align with higher risk and performance modeling
decisions, while risk and compliance organizations may require
the opposite. During the sequential review flows, one committee’s
recommendations may even conflict that of others. As a result,
the accountability of the resulting decisions gets locked into the
underlying complexities of the process.

2.3 Process Cost, Fines and Penalties
Challenge 6. Current governance practices translate to high compli-
ance costs.
Compliance processes are usually highly manual functions in the fi-
nancial services industry. Last year, US financial firms spent around
US$80 billion for compliance purposes, which is expected to grow
by 50% to US$120 billion in 2025 [11]. Despite the increase in spend-
ing, compliance remains a challenge. Human-based compliance
processes and the lack of automation have been considered among
the top reasons for the high compliance costs. As a result, regu-
latory technology offerings have gained traction in recent years.
However, integrating, scaling and calibrating third-party vendor
solutions have been demanding exercises.

Challenge 7. Despite being considered the standard and safest path,
manual governance processes still result in high regulatory penalties.
In addition to the high cost of compliance and governance, the
resulting process is challenged with high penalties in the form
of growing regulatory fines. US financial institutions paid close
to US$320 billion in regulatory fines in the years 2008–2016 [17].
This raises questions on the effectiveness of the current manual
governance practices.

2.4 AI Model Complexity
Challenge 8. AI model complexity is growing exponentially, making
fine-grained governance reviews infeasible.
AI model complexity is increasing at an unprecedented rate [5].

Figure 5: Model complexity continues to grow at an exponential
rate [29].
Metrics for the number of building blocks as well as the complexity

3

Figure 3: A simplified model development workflow in finan-
cial services, including standard development steps, production
steps (blue), regulatory steps (yellow), and numerous feedback loops
(green and red).

Figure 4: A sample partial view of AI model governance committee
reviews.

to team, and even over time, subject to the available capacity of risk
management teams. As a result, the outcomes of the review process
inherit some subjectivity, which makes it difficult to standardize,
and is more prone to errors and human oversights [12].

Challenge 4. Model governance workflows often have complex and
interlacing manual stages.
AI governance reviews frequently follow complex flow diagrams
with numerous review stages and interdependencies, representing
separation of coverage among aspects of business, risk manage-
ment, and compliance [10]. The resulting process can take a year
or even longer. Figures 3 and 4 illustrate parts of development and
model committee review flows respectively. Though the exact pro-
cess may vary from firm to firm, it frequently involves elaborate
committee reviews with many stakeholders represented, like model
development, deployment, model risk management, audit, line of
business, legal, and compliance. Each team serves a different role
such as model developer, owner, user, stakeholder, administrator,
monitoring unit, risk committee, as well as representing their or-
ganization’s unique business strategies, goals, and requirements.
Each group or committee review may be recursive, incurring fur-
ther nested reviews. For instance, within the legal and compliance
review, regulatory subcommittees such as the Fair Lending Act
Committee, country, and territory committees may each perform
nested reviews. While they can make significant changes to the
model, each review requires a final and locked version of the model
and its documentation. Any significant change recommended by
a committee is performed by the model development team and
restarts the process with an updated and locked model.

ICAIF ’20, October 15–16, 2020, New York

Eren Kurshan, Hongda Shen, and Jiahao Chen

within each of the building blocks point to consistent increases.
For example, the number of AI model parameters increases almost
tenfold annually [29], which is much more aggressive than the
traditional Moore’s Law scaling. As the number of parameters ap-
proach one trillion, such complex models motivate a reevaluation
of fine-grained or parameter-level governance reviews.

2.5 Agility and Time to Review

Challenge 9. AI governance reviews can often take a year or longer.
Most model governance guidelines provide optimistic timelines for
AI governance reviews (on the order of weeks or months depending
on the firm). However, in practice, the end-to-end AI governance
review process frequently takes close to a year or longer. The length
of the process is driven by factors like complex review flows and the
lack of automation. The number of models in large financial firms is
increasing by 10–25% according to recent reports [9]. The length of
the review has implications on the successful adoption of AI, cost,
operational efficiency, and model performance. Recent months have
further highlighted the importance of time-to-review and time-to-
deploy metrics for robust and effective modeling processes.

Challenge 10. Long governance reviews negatively impact the AI
model performance and agility.
One unintended consequence of long model governance reviews is
degraded model performance. AI models are typically locked during
the governance reviews and start aging, along with underlying data
and assumptions. Model aging becomes a major concern as review
times lengthen. The effects are even more prominent in dynamic
or adversarial environments, e.g., in fraud detection, where model
agility is crucial for effectiveness.

2.6 Regulatory Complexity and Uncertainty
Challenge 11. Regulatory complexity and uncertainty make gover-
nance increasingly difficult.
As discussed earlier, financial institutions are expected to comply
with numerous regulations. For US credit models, these may include
the Fair Housing Act, Consumer Credit Protection Act, Fair Credit
Reporting Act, Equal Credit Opportunity Act, Fair and Accurate
Credit Transactions Act, as well as regulatory directives such as
Regulation B. Furthermore, these regulations are enforced by dif-
ferent regulators at the international, federal, state, and local levels,
as summarized in Figure 1 [24]. In recent years, AI and data regu-
lations have become more rigorous and added to the underlying
financial regulatory complexity (such as GDPR and CCPA). While
regulatory systems and complexity serve numerous purposes, build-
ing compliant AI models in this complex regulatory landscape is a
formidable task. Up to 67% of the financial institutions in the US
and EU perceive regulatory complexity and uncertainty as concerns
[15]. More importantly, up to 94% and 97% of financial firms in AI
leadership positions indicate seeing regulatory complexity and un-
certainty respectively as problems, which is remarkably higher than
the perceptions by the laggards in terms of AI maturity. Increased
automation and system-level support are essential in addressing
these concerns.

Challenge 12. Regulatory complexity incentivizes AI models to com-
ply with the most restrictive regulatory guidelines.

4

Figure 6: The perception of financial regulatory complexity and
uncertainty by AI program maturity (source statistics from[15]).

Regulatory complexity has multiple dimensions, including the types
of applications, regulation types, regulatory entities, jurisdictions
such as local, state, country, and territory levels, each with their
own unique requirements In response to the resulting complex-
ity, it is easiest to build models that adhere to the largest possible
set of regulations, especially as new regulations in one jurisidic-
tion inspire adoption elsewhere [6]. For instance, if an AI model
is deployed in multiple territories, where one territory has more
conservative guidelines, model governance would prefer a model
that is designed for the most conservative guidelines across all
territories. Since the current processes do not have any separation
of regulatory functions from the AI models, model performance
typically gets impacted by these additional restrictions.

2.7 Modeling Restrictions
Challenge 13. Governance restricts the AI models to conservative
modeling approaches.
In an attempt to reduce the AI risks, governance practices com-
monly constrain the models to conservative modeling approaches.
Restrictions on the modeling framework include offline-only train-
ing, model types (such as restrictions on the use of neural networks),
etc. Despite showing promising results in many application areas,
many modeling techniques have largely not been allowed in fi-
nancial services under current governance guidelines. Such strict
restrictions do not always produce lower risk outcomes, e.g., when
the application characteristics mismatch with modeling assump-
tions. Even limited and conditional use of emerging approaches may
provide strategic, long-term benefits, e.g., reinforcement, online,
incremental, and active learning [4, 13, 26].

Challenge 14. AI governance usually assumes static or near-static
environments, which may be unrealistic.
AI model governance processes frequently enforce the use of static
or near-static environment assumptions for the approved model
use period [2]. Even though AI models do not rely on assumptions,
in some cases the static assumptions are established by restricting
the model training or use periods to approximate the static con-
ditions. The mismatch in the assumed and real conditions causes
performance problems. As an example, in payment systems, fraud
scoring models are expected to assume near-static transactions and
fraud patterns. In reality, fraud patterns often go through major
changes in short periods of time. In person-to-person (P2P) pay-
ments the fraud tactics can change in a matter of days and cause
significant losses as the underlying models are not equipped to
deal with dynamic environments. Furthermore, static assumptions
prevent models from dealing with unexpected changes and limit
robustness.

Towards Self-Regulating AI: Challenges and Opportunities of AI Model Governance in Financial Services

ICAIF ’20, October 15–16, 2020, New York

2.8 Regulatory Monitoring and Reporting
Challenge 15. Intermittent monitoring fails to identify critical
changes in the environment, data drifts, or data quality issues.
Financial institutions are required to perform on-going monitor-
ing reviews at least on an annual basis. For many AI models, the
on-going monitoring frequencies remain close to these baseline
requirements. The lack of continuous monitoring causes the critical
environment changes, data drifts, and data quality issues remain
undetected. This, in turn, has performance, robustness, and compli-
ance implications.

Challenge 16. Traditionally, model monitoring has been performed
on a limited number of metrics.
Model development and governance processes have mostly been
built with static environment assumptions. In recent months, due
to effects of the global pandemic, the requirement to demonstrate
mostly static operational environments for AI models has been
reevaluated by many financial services firms. There has been signif-
icant interest in monitoring a broader range of metrics, to capture
the changes in the underlying data, model behavior and to ensure
robustness, even for adaptive models.

Challenge 17. Regulatory metrics are growing faster than the ca-
pacity of the manual governance processes.
A growing number of metrics are being proposed and used for
regulatory purposes [3], such as quantifying bias in credit applica-
tions using demographic parity [7], equalized odds [8, 18, 20], and
other group fairnesses [23], Adding more and increasingly sophis-
ticated metrics to the monitoring list is not scalable, due to manual
data extraction, reporting, committee reviews. The metrics them-
selves may even be mutually contradictory [23, 28] or be difficult
to operationalize.

Challenge 18. Reporting performance and monitoring results to the
appropriate committees is a manual and laborious process.
During model deployment, the development team is responsible
for collecting the data or metrics of interest, producing a written
report using predetermined templates, and presenting it to numer-
ous committees. Stakeholders or internal committees do not have
access to the deployment system and its behavior other than this
indirect, committee review path. Depending on the model, this
manual process may take a few weeks to months. The inherent
complexity and length of the reporting process limit the frequency
and effectiveness of monitoring.

2.9 Mitigation and Regulatory Control
Challenge 19. Regulatory issues are not detected or rectified
promptly due to the lack of run-time monitoring or mitigation.
When monitoring is insufficient, regulatory breaches may be dis-
covered only during the model retraining exercises or through
regulatory inquiries, by which time an institution is at risk of incur-
ring fines, penalties and reputational damage [1]. This highlights
the need to perform continuous monitoring as well as run-time
remediation. Using a robotics analogy, the current practices would
translate to not including any real-time monitoring (camera) or
control (steering or other actuation feedback) in the broader sys-
tem.

Figure 7: High-level view of the AI system framework with self-
regulatory capabilities.

2.10 Robustness and Stress Testing
Challenge 20. Despite being commonly used for traditional financial
models, stress tests are not available for broader AI use cases.
Traditional models such as capital analysis rely heavily on the
industry-wide standard stress tests. At this point in time, no such
tests are available for most AI models in financial services. Model
development organizations often try to mimic aspects of the stress
tests by experimenting with limited and theoretical scenarios. How-
ever, as discussed in Section 1, the nature of AI models require
extensive scenario and stress testing for robustness and compliance
purposes.

Challenge 21. Model development organizations face serious prob-
lems in reassuring governance committees about compliance.
Model development teams face genuine difficulties in assuring risk
management organizations that the AI models are on target for
compliance, robustness, and risk criteria. This translates to an open-
ended burden of proof for the modeling teams. It involves justifying
all model parameters, modeling decision, data, features, assump-
tions, architecture, and techniques. Yet, even after many rounds of
reviews and additional scrutiny, AI model compliance still remains
unclear.

3 SYSTEM-LEVEL FRAMEWORK TOWARDS

INCREASED AI SELF-REGULATION

The long list of AI governance challenges motivates the need for
new solution approaches. In this section, we envision and present
a high-level AI system framework and modular building blocks
towards increased self-regulation and more efficient AI governance
in financial services. The proposed framework lies at the boundary
of the AI and governance systems. It is not intended as a specific
design blueprint, but a high-level design approach that can be
customized based on the AI model, application, and the firms’ needs.

Capability 1. Continuous regulatory monitoring and reporting dur-
ing deployment
Figure 7 shows the high-level architecture of the proposed AI sys-
tem framework and regulatory modules. The system aims to in-
corporate run-time monitoring, regulatory control, and mitigation
capabilities in the production environment. The figure shows a

5

ICAIF ’20, October 15–16, 2020, New York

Eren Kurshan, Hongda Shen, and Jiahao Chen

broad selection of modules for completeness, yet a smaller subset of
modules can be customized for each application, such as mortgage
applications or credit card payments. Monitoring capabilities are
intended for self-regulation purposes. The outputs of the monitor-
ing system ties to regulatory functions to ensure that the system
behaves within guidelines.

Monitoring functions may include: (i)model vital statistics: essen-
tial metrics and statistics of the model’s behavior and data (such
as volumes, data quality metrics, etc.); (ii)input data: monitoring of
the input streams (such as fairness metrics, data drift, population
stability metrics etc.); (iii) extended data monitoring: analysis of the
model’s input/output behavior using extended data sources, like
features that the AI model is not allowed to access, correlation of the
input features with protected classes such as age, race, gender, etc.;
(iv) output monitoring: monitoring for performance, fairness, and
other regulatory criteria; (v) emerging pattern and anomaly detec-
tion: continuous monitoring of the abnormal and emerging patterns
in the input/output streams, as well as extended data streams; (vi)
automated report generation: reporting for development teams, gov-
ernance committees, or regulators, such as regulatory metrics, alert
frequencies, alerting levels, compliance confidence scores, etc.

Capability 2. Integration of key self-regulatory building blocks
The framework enables the integration of modular, self-regulatory
blocks in the AI system itself to cover common requirements, with-
out increasing the complexity of the core AI models. These blocks
are expected to be: (i) customized for the application types, (ii) used
by multiple application and model types with customizations. As
discussed earlier, the specific list of modules is bespoke for each
application type based on regulatory guidance. Modular blocks
provide a natural way to separate the regulatory criteria from the
AI model, while keeping them in the same system. This enables the
system to integrate and update governance blocks independent of
the AI model. It ensures improved compliance for a wider range
of models without having to rebuild the regulatory functionalities
within each model instance. AI specific analysis modules can be
incorporated in the system: (i) explainability analysis: for regulatory
and customer inquiries as well as audit purposes; (ii) data quality
analysis: to continuously monitor data quality issues (such as miss-
ing, unexpected, out-of-range data, systemic issues and failures
etc.); (iii) fairness analysis: monitoring and mitigating bias (such
as fairness metrics, input correlations with protected classes, etc.
Recent policies and regulatory publications emphasize and clarify
the transparency and explainability requirements in adverse action
notices [14].

The lack of universal regulatory guidelines remains a notable
challenge, and will likely be overcome through regulatory lead-
ership. However, each financial institution has its own internal
guidelines based on the combination of the firm’s risk appetite
and regulatory guidelines. In this study, we propose using internal
guidelines for the framework in the near term. As an example, a
commonly used metric in bias is the prediction accuracy delta for
different classes, for which the specific thresholds can be customized
by each firm based on their risk appetite. Once implemented, the
proposed system also has the opportunity to enforce these internal
regulatory metrics pervasively across many AI models.

6

Capability 3. Reusable libraries of module templates, system and
regulatory guidelines
The development of the proposed governance modules involves: (i)
Libraries of module templates for different application and reg-
ulation types, such as bias/fairness, explainability, data quality
templates with baseline functionalities. In some cases the module
libraries may include full models as building blocks; (ii) Customiza-
tion and configuration of the module templates by the development
teams, such as customizing the metrics, specifying the input fea-
tures, outputs, etc.; (iii) Criteria selection based on the firms risk
appetite, application type and regulations, such as the conditions
and quantitative metrics for governance criteria, alerting and mit-
igation; (iv) Guidance on system architecture requirements, such
as the required list of modules, metrics and functionalities for an
application type; (v) Libraries for control scenarios and mitigation
actions are expected to be gradually built for application types by
the model governance and development organizations.

Capability 4. Run-time mitigation with human-in-the-loop alerting
The system-level framework aims to gradually build automated
mitigation capabilities. In early stages, it targets partially auto-
mated run-time mitigation and human-in-the-loop alerting. The
goal is to manage the model’s behavior more effectively during
run-time. The system does not aim to provide compliance guaran-
tees. The approach provides a number of orthogonal capabilities:
(i) Scenario-based mitigation for well-defined control paths, for
which the scenarios can be extracted through the use of continu-
ous monitoring data from pre-deployment testing, historical data,
and established scenarios such as abnormal payment transaction
patterns during holidays; (ii) System-level remediation through
the use of alternative models, such as using shadow AI models
outputs for population segments if the primary AI model shows
detectable bias during continuous monitoring; (iii) Post-processing
and re-calibration of the model outputs, such as calibration of model
scores to compensate for detected issues.

Capability 5. Governance controller and AI-based governance
For increased automation in some cases, a governance controller
may be used, along with human-in-the-loop validations of con-
trol actions. The controller interacts with the individual modules
through module output monitoring, alerting, and mitigation actions.
It may select the output from a collection of alternative AI models
to ensure the system-level compliance and robustness. Furthermore,
the controller enables the run-time management of the model ac-
cording to the firms’ risk policy guidelines. In the later stages of
adoption, AI itself will play a big role in the governance and regu-
latory functions. This involves 2 orthogonal paths: (i) intrinsically
self-regulating AI models; (ii) AI-based external governance and
regulatory control. The proposed approach is essential for both
paths, providing key capabilities in data and monitoring, risk and
regulatory guidance, required metrics and criteria etc.

Capability 6. Dynamic reconfiguration for change management,
model replacement and scale-out
The inherent modularity of the system-framework enables plugging
in alternative regulatory modules, reconfiguring or changing them
independent of the core AI model. As regulations are updated,
corresponding modules can be updated across the firm without
retraining the individual AI models. This is essential to ensure

Towards Self-Regulating AI: Challenges and Opportunities of AI Model Governance in Financial Services

ICAIF ’20, October 15–16, 2020, New York

compliance during the lifetime of the AI system when: (i) a new
AI model is developed to replace an older model; (ii) the AI model
is to be scaled out and used in multiple territories with different
regulations or regulatory variations; (iii) the regulatory guidelines
change over time as new regulations are introduced.

Capability 7. Governance throughout the model lifecycle
One of the unique capabilities that the system-level approach aims
to offer is to provide a regulatory framework throughout the en-
tire model lifecycle, where the capabilities may be customized for
individual application types. As they are template based, gover-
nance modules can be utilized in different platforms during pre-
deployment testing or development. The pre-deployment modules
may closely mimic deployment versions, where development func-
tionalities and modules may include: (i) Data quality modules and
guidelines; (ii) Regulatory criteria, such as bias and explainability;
(iii) Performance metrics, guidelines and benchmarking require-
ments for individual applications. Through the increased automa-
tion of development and pre-deployment governance functions,
initial model approval process can be improved significantly.

Capability 8. Custom robustness tests for the AI model
Continuous monitoring provides the opportunity to collect vast
amounts of run-time behavioral data. The proposed system uses
this resource to develop custom robustness and stress tests for
the AI model. This relies on identifying the weaknesses, failure
patterns, and risky scenarios/patterns during continuous monitor-
ing. These model-specific scenarios can be extended with general
scenarios, such as those identified in earlier or alternative mod-
els. The outcomes of these tests directly tie into the mitigation
functions in the proposed system-level framework. In addition, the
framework provides the ability to integrate externally developed
stress-tests from regulators or vendors, as they become available.
Robustness tests and stress tests are valuable tools in assuring the
model risk management teams about compliance. These capabilities
have the ability to refocus the governance review processes from
the parameter-level justifications to the system-level targets with
quantifiable outcomes. It also helps assure the AI governance teams
by showing a reasonable number and range scenarios are covered
through these analyses. In later stages, the generative adversarial
modeling capabilities may be used for advanced and continuous
stress tests.

4 SOLUTION OPPORTUNITIES
The system-level approach provides novel capabilities and solution
opportunities:

Opportunity 1. Develop customized and scalable solutions for robust
AI systems and governance challenges.
The system-level approach provides the opportunity to alleviate
majority of the pain points discussed in Section 2 by integrating
key capabilities in the AI system itself. This allows improvements
in performance, cost, complexity, time-to-develop, time-to-review,
and the ability to scale out. It enables the robust operation of the
models as well as providing a new run-time risk mitigation path
for model risk management and governance organizations. The
combination of increased model and regulatory complexity raises
questions on the sustainability of the current practices. The solution

7

approach provides a sustainable solution under the dual forces of
complexity.

Opportunity 2. Integrate key governance capabilities in a unified,
in-house framework with increased automation.
Governance and compliance solutions frequently suffer from frag-
mented environments, inconsistent tools, and techniques. The self-
regulating AI system approach provides a unified in-house gover-
nance framework as well as the ability to customize for different
application types. It enables incorporating the third-party solutions
in the same architecture with the in-house solutions, while univer-
sally managing all based on the firms’ risk management policies
and guidelines.

Opportunity 3. Enable the run-time management and mitigation
of the models.
Financial institutions have largely avoided the run-time manage-
ment of AI models. The system-level approach aims to provide real-
time monitoring and mitigation capabilities to effectively manage
AI models in production. The monitoring and mitigation capabili-
ties can be built up from the basic human-in-the-loop alerting to
partially or fully automated control and controller architectures.
AI models will likely play a big role in the regulatory and gover-
nance functions in near future. The proposed framework provides
enabling capabilities and data sources for AI-based governance
solutions.

Opportunity 4. Implement firms’ risk policies and regulatory guide-
lines pervasively and at scale.
The libraries of governance module templates, architectural require-
ments and regulatory criteria provide the opportunity to uniformly
implement the firms risk management policies and the regulations
at scale. The guidelines on the regulatory module selection for each
application, the contents of the module templates, thresholds and
criteria can be pervasively implemented for all AI models.

Opportunity 5. Enable the development of custom robustness tests.
The lack of stress tests and run-time monitoring are serious chal-
lenges in managing the risks of AI solutions. Continuous run-time
monitoring provides the opportunity to develop robustness tests
customized for the model. This shifts the AI model governance
focus from the existing parameter-level processes to an empirical
and system-level approach.

Opportunity 6. Enhance risk management throughout the model
lifecycle.
AI governance reviews mostly focus on the pre-deployment and
deployment through the initial model approval and on-going moni-
toring processes respectively. With the availability of the libraries
of governance modules and tools, the corresponding functional
capabilities can be integrated in earlier stages of the model devel-
opment cycle and in different platforms, which can significantly
improve the end-to-end process. Finally, the proposed approach
provides a new risk management stage during the deployment of
the model, through continuous monitoring and mitigation.

5 ADOPTION CONSIDERATIONS
This section discusses some of the key adoption considerations for
the proposed framework.

ICAIF ’20, October 15–16, 2020, New York

Eren Kurshan, Hongda Shen, and Jiahao Chen

Managing Expectations: Autonomy, trustworthiness and robust-
ness remain among the list of AI grand challenges. The proposed
system does not offer a panacea, but rather a step in improving the
overall governance process and enabling solution opportunities.

Organization and Process Enhancements: Without any internal
process adjustments, it is not possible to achieve the full potential
of the governance framework. Possible process improvement op-
portunities include: reducing the number and complexity of the
reviews, simplifying the roles and responsibilities of the numerous
committees, providing insights to the committees via direct access
to monitoring and compliance metrics.

Development of Libraries and Module Templates: The proposed
framework relies on libraries and guidelines to be provided by the
governance organizations. Similarly, the customization and inte-
gration stages are expected to be performed by development teams.
Addressing these needs requires significant initial investment in
the system, which should yield long-term payoff. Gradual integra-
tion of these capabilities is a reasonable approach to achieve the
end-state of the system.

Regulatory Standards and Guidelines: Recent studies agree on the
need for standard regulatory guidelines for AI [16, 19, 22]. In this
study, we acknowledge the challenges in achieving such universal
guidelines and rely on financial firms’ internal risk guidelines as an
alternative for the time being. While some of the internal standards
are already available, others may be added gradually over time.

Platform and Deployment Considerations: The proposed system-
level approach is mostly platform-agnostic. While the primary ap-
plication of the proposed system is the deployment platforms, the
modules and capabilities can be utilized both in pre-deployment
and deployment platforms, while the underlying templates can be
extended to development platforms.

6 CONCLUSIONS AND OUTLOOK
As AI systems have become integral parts of the financial services
industry, it is essential to have effective model governance practices
to ensure robustness and compliance. Legacy governance processes
in practice today suffer from performance, cost, complexity, agility,
and scaling issues in AI models. This paper tries to focus on some
of the common challenges in AI model governance in the financial
services industry. Given the unprecedented growth in AI complex-
ity, the feasibility of the current governance practices for the next
generation of AI models is questionable. The paper aims to start
an initial conversation on the key challenges and solution oppor-
tunities. A system-level framework with modular building blocks
is presented. This approach aims to incorporate increased automa-
tion, integration and configurability into the AI system towards
self-regulation. It enables some key capabilities to alleviate the
existing challenges and enable more effective and compliant AI
solutions.

Disclaimer. This paper was prepared for informational purposes in part
by the Artificial Intelligence Research group of JPMorgan Chase & Co and
its affiliates (“JP Morgan”), and is not a product of the Research Department
of JP Morgan. JP Morgan makes no representation and warranty whatsoever
and disclaims all liability, for the completeness, accuracy or reliability of the
information contained herein. This document is not intended as investment
research or investment advice, or a recommendation, offer or solicitation
for the purchase or sale of any security, financial instrument, financial
product or service, or to be used in any way for evaluating the merits of

8

participating in any transaction, and shall not constitute a solicitation under
any jurisdiction or to any person, if such solicitation under such jurisdiction
or to such person would be unlawful.

REFERENCES
[1] J. Armour, C. Mayer, and A. Polo. 2017. Regulatory Sanctions and Reputational
Damage in Financial Markets. J. Financial Quant. Anal. 52, 4 (2017), 1429–1448.
[2] Bank Policy Institute. 2020. Artificial Intelligence: recommendations for the

principled modernization of the regulatory framework. (2020).

[3] BLDS, Discover Financial Services, and H2O.ai. 2020. Machine Learning: consid-

erations for fairly and transparently expanding access to credit. (2020).

[4] R. Chalapathy, A. K. Menon, and S. Chawla. 2018. Anomaly Detection using

One-Class Neural Networks. arXiv:1802.06360

[5] Y. Cheng, D. Wang, P. Zhou, and T. Zhang. 2017. A Survey of Model Compression

and Acceleration for Deep Neural Networks. arXiv:1710.09282

[6] C. Chin. 2019. Highlights: The GDPR and CCPA as benchmarks for federal

privacy legislation. Brookings Institute (2019).

[7] A. Chouldechova. 2016. Fair Prediction with Disparate Impact: A Study of Bias

in Recidivism Prediction Instruments. J. Big Data 5, 2 (2016), 153–163.

[8] S. Corbett-Davies, E. Pierson, A. Feller, S. Goel, and A. Huq. 2017. Algorithmic

Decision Making and the Cost of Fairness. In ACM SIGKDD. 797–806.

[9] I. Crespo, P. Kumar, P. Noteboom, and M. Taymans. 2017. The Evolution of Model

Risk Management.

[10] O. Engdahl. 2014. Ensuring regulatory compliance in banking and finance through
effective controls: The principle of duality in the segregation of duties. Regulation
& Governance 8, 3 (2014), 332–349.

[11] S. English and S. Hammond. 2019. Cost of Compliance Report. Thomson Reuters

(2019).

[12] G. Agarwala et al. 2019. Building the Right Governance Model For AI/ML. E&Y

Report (2019).

[13] V. Mnih et.al. 2015. Human-level control through deep reinforcement learning.

Nature 518, 7540 (Feb. 2015), 529–533.

[14] P. A. Ficklin, T. Pahl, and P. Watkins. 2020. Providing Adverse Action Notices

When Using AI/ML Models. CFPB (2020).

[15] World Economic Forum. 2020. Transforming Paradigms: A Global AI in Financial

Services Survey.

[16] U. Gasser and V. Almeida. 2016. A Layered Model for AI Governance.

IEEE

Internet Comput. 21 (2016), 58–62.

[17] B. Gehra, J. Leiendecker, and G. Lienke. 2017. Compliance by Design: Banking’s

Unmissable Opportunity. Boston Consulting Group White Paper (2017).

[18] P. Goelz, A. Kahng, and A. D. Procaccia. 2019. Paradoxes in Fair Machine Learning.
In Advances in Neural Information Processing Systems, Vol. 32. 8342–8352.

[19] Google. 2019. Perspectives on Issues in AI Governance. (2019).
[20] M. Hardt, E. Price, and N. Srebro. 2016. Equality of Opportunity in Supervised
Learning. In Advances in Neural Information Processing Systems. Vol. 29. 3315–
3323. arXiv:1610.02413

[21] J. B. Heaton, Nicholas G. Polson, and J. H. Witte. 2016. Deep Learning in Finance.

(2016). arXiv:1602.06561

[22] Hong Kong Institute for Monetary and Financial Research. 2020. Artificial
Intelligence in Banking: The changing landscape in compliance and supervision.
[23] J. S. Kim, J. Chen, and A. Talwalkar. 2020. Model-Agnostic Characterization of
Fairness Trade-offs. In Proceedings of the International Conference on Machine
Learning. 9339–9349. arXiv:2004.03424

[24] M. Labonte. 2020. Who Regulates Whom? An Overview of the U.S. Financial

Regulatory Framework. Congressional Research Service R44918 (2020).

[25] M. P. Laurent, F. V. Weyenbergh, O. Plantefève, and M. Tejada. 2020. Banking

Models After COVID-19: Taking Model-risk Management to The Next Level.

[26] J. L. Lobo, I. Lana, J. Del Ser, M. N. Bilbao, and N. K. Kasabov. 2018. Evolving
Spiking Neural Networks for Online Learning Over Drifting Data Streams. Neural
Networks 108 (2018), 1–19.

[27] A. Oprea, A. Gal, I. Moulinier, J.Chen, M. Veloso, E.Kurshan, S. Kumar, and T.
Faruquie. 2019. NeurIPS 2019 Workshop on Robust AI in Financial Services: Data,
Fairness, Explainability, Trustworthiness, and Privacy.

[28] G. Pleiss, M. Raghavan, F. Wu, J. Kleinberg, and K. Q. Weinberger. 2017. On
Fairness and Calibration. In Advances in Neural Information Processing Systems.
Vol. 30. 5680–5689.

[29] N. Rao. 2019. Intel AI Summit 2019 Keynote.
[30] M. U. Scherer. 2016. Regulating Artificial Intelligence Systems. Harvard J. Law

Tech. 29, 2 (2016), 333–400.

