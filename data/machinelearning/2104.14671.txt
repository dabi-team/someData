2
2
0
2

t
c
O
1
1

]
E
S
.
s
c
[

2
v
1
7
6
4
1
.
4
0
1
2
:
v
i
X
r
a

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

1

SYNSHINE: improved ﬁxing of Syntax Errors

Touﬁque Ahmed, Noah Rose Ledesma, and Premkumar Devanbu

Abstract—Novice programmers struggle with the complex syntax of modern programming languages like JAVA, and make lot of syntax
errors. The diagnostic syntax error messages from compilers and IDEs are sometimes useful, but often the messages are cryptic and
puzzling. Novices could be helped, and instructors’ time saved, by automated repair suggestions when dealing with syntax errors.
Large samples of novice errors and ﬁxes are now available, offering the possibility of data-driven machine-learning approaches to help
novices ﬁx syntax errors. Current machine-learning approaches do a reasonable job ﬁxing syntax errors in shorter programs, but don’t
work as well even for moderately longer programs. We introduce SYNSHINE, a machine-learning based tool that substantially improves
on the state-of-the-art, by learning to use compiler diagnostics, employing a very large neural model that leverages unsupervised
pre-training, and relying on multi-label classiﬁcation rather than autoregressive synthesis to generate the (repaired) output. We
describe SYNSHINE’s architecture in detail, and provide a detailed evaluation. We have built SYNSHINE into a free, open-source
version of Visual Studio Code (VSCode); we make all our source code and models freely available.

Index Terms—Deep Learning, Program Repair, Naturalness

(cid:70)

1 INTRODUCTION

S YNTAX errors are easy to make, and will cause compiles

to fail. The challenges posed by syntax errors to novices
have been known for a long time [1]. More recent studies
have documented the challenges faced by novices in various
languages [2], [3], [4]. Novices make a wide range of syntax
mistakes [4], some of which are quite subtle; time that might
otherwise be spent on useful pedagogy on problem-solving
and logic is spent helping novices deal with such errors.
Unfortunately the error messages provided by compilers are
often not helpful; novices struggle to interpret the messages,
and sometimes even experts do! [5]. Consider for example,
the real program example in Fig. 1, where a novice student
just replaced a “*” with an “x” on line 8. None of the
big 4 IDEs (VSCode, IntelliJ, BlueJ, or Eclipse) provide a
direct diagnostic for this very understandable error. A lot of
time can be spent on such errors [6], and researchers have
called out for more attention to help novices [5] deal with
errors, speciﬁcally syntax errors. While semantic errors (bug-
patching) have received quite some attention, syntax errors
have attracted less interest.

The possibility of collecting novice error data, and the
emergence of high-capacity, highly conﬁgurable deep learn-
ing models, has raised the possibility of designing models
that can automatically ﬁx errors, and training them using
novice data. This approach is very attractive for several
reasons: a) Automated repair of syntax errors is helpful
to novices, and saves instructors’ time. b) Traditional ap-
proaches to automatically ﬁnding & ﬁxing syntax errors
require hand-coding fairly complex parser logic. c) Au-
tomatically learning models to ﬁx errors is an approach
that promises to be language-agnostic, as long as sufﬁcient
data is available. d) Learning ﬁxing strategies from samples
representative of novice errors promises to yield models

• All the authors are with the Department of Computer Science, University

of California, Davis, CA, 95616.
E-mail: {tfahmed, roseledesma, ptdevanbu}@ucdavis.edu

Manuscript in submission

that perform well on the most common mistakes that
novices make. Several recent approaches to this problem
have emerged, which are all arguably language-agnostic.

DeepFix [7] used sequence to sequence encoder/decoder
models (with roots in language translation) to ﬁx all sorts of
errors in C, while Santos et al. [8] used language models
to repair just syntax errors in JAVA (and thus is closer to
our work). All of the existing approaches take an erro-
neous program as input, and attempt to ﬁx them. DeepFix
(which uses an RNN-based Seq2Seq approach) works less
well on longer programs, since RNNs struggle with long-
range dependencies. Santos et al. faced similar challenges.
More recently, Ahmed et al. [9] trained a “lenient” parser
using synthetic data. Ahmed et al. use a 2 stage approach:
a ﬁrst (“BLOCKFIX”) repair the nesting structure, and the
second (“FRAGFIX”) to repair individual statements; we
refer to their tool in this paper as BF+FF , to indicate their
two stages. Their approach improves over both DeepFix
and Santos et al., especially for longer programs. Syntax
errors in longer programs (longer than 200-300 tokens) are
challenging for automated repair, because locating the error
is difﬁcult. All the above approaches ignore an important
source of information that could be of great value: the
error from the compiler! Compiler warnings include a lot of
useful information: including often the line number where
the error occurred, the tokens involved in the error, and
the nature of the error. This information could be used by
neural model to better locate and repair the error. Yasunaga
et al. [10] utilize C compiler warnings with a graph-based
self-supervised approach and outperform DeepFix in ﬁxing
compiler errors. However, compiler wanrnings have not
been applied to any approach that is speciﬁcally designed
for JAVA programs. Our model also uses compiler warnings,
but our performance remains robust as the programs’ length
increases.

In addition, existing approaches have not adequately
exploited the tremendous capacity of current DL models to
learn (without direct supervision) the statistics of very large
amounts of unlabeled sequential data. Modern pre-training

 
 
 
 
 
 
IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

2

approaches such as RoBERTa can ingest vast corpora of
sequential data (e.g., a billion tokens from GitHub-hosted
code) and learn the patterns of syntax, identiﬁer usage
patterns, arithmetic expressions, method call patterns etc.
These patterns are automatically learned and represented
as high-dimensional vector embeddings of tokens, without
requiring any human effort to label the data. These embed-
dings, however, have been shown to substantially improve
performance when used as pre-set embeddings in other
networks that can be “ﬁne-tuned” with smaller amounts of
human-labeled data.

In this paper, by using the diagnostics from a compiler,
and exploiting the ability to pre-train embeddings with high
capacity RoBERTa model, we build a tool, SYNSHINE, which
improves substantially on the state-of-the-art in automated
syntax repair in JAVA. We make the following contributions:

1) We utilize compiler diagnostics from javac, as well
as unsupervised pre-training to achieve substantial im-
provements, to implement a 3-stage syntax error repair
tool, which can ﬁx as much as 75% of programs with
single errors in the Blackbox dataset. This substantially
improves upon prior work in the area of “JAVA” syntax
error repair.

2) When generating ﬁxes, we rely on multi-label classiﬁ-
cation, rather than autoregressive synthesis, to simplify
the task of generating the repair.

3) We evaluate the contributions of the different stages of
our tool, and also the value of pre-training, and the use
of javac.

4) We evaluate the diversity of repairs that SYNSHINE can
perform; we also dig into the cases where it appears to
fail.

5) We have built SYNSHINE based repair into the widely
used, freely available, open-source Visual Studio Code
(VSCode) tool, and made all our software and data
available to the extent allowable under legal require-
ments1.

Note: Most of the novice code correction approaches are
designed for C including DeepFix [7], [10], [12], [13]. Some
recent works [10], [13] outperform DeepFix in ﬁxing C
compiler errors. They all take the complete program as input
and evaluate it on the DeepFix dataset with smaller se-
quences (up to 450 tokens). Ahmed et al. have already shown
that models taking complete program sequences tend to
fail more often for longer programs [9]. Unlike Blackbox,
DeepFix dataset does not have erroneous and ﬁxed program
pairs. That prevents us from comparing the model’s perfor-
mance with the human-produced ﬁxed versions. We train
DeepFix model on our JAVA dataset because DeepFix uses
the simplest inductive bias: sequence of program tokens and
does not depend on any language-speciﬁc compiler. Several
other approaches [10], [13] are both compiler- and language-
dependent, so they are not comparable with our approach.
Furthermore, we are able to accept complete programs, of
longer length than earlier approaches, and provide ﬁxes
leveraging both pre-training as well as compiler errors.

1. Blackbox data is distributed under U.K. Laws. Please contact

creators [11] for details.

2 BACKGROUND & MOTIVATION

Problem-solving, motivation & engagement, and difﬁcul-
ties in learning the syntax of programming language are
three fundamental challenges in introductory programming
courses [14]. The dropout and failure rates are still high
in introductory programming courses even after applying
advanced methods and tools [15], [16]. Helping novices with
programming syntax can prevent novices to get demoti-
vated [14] at the beginning of the learning process. In this
paper, we aim to help novice programmers by automatically
suggesting repairs for syntax errors. Consider the program
in Fig. 1, which is an actual example our dataset of novice
programs with errors [11]. Note the use of “x” instead of “*”
on line 8. Many school maths texts use “x” for multiply, so
this an understandable error.

In an introductory programming course, a novice may
make this error by force of habit, and then ﬁnd it quite chal-
lenging to ﬁx the problem. Most popular IDEs (Eclipse, In-
telliJ, Visual Studio Code) have trouble ﬁxing this; however,
our approach, which feeds a javac-based error diagnostic,
into a multi-stage repair engine that combines unsupervised
pre-training, with ﬁne-tuning, can resolve this.

1 import java.util.Scanner;
2 public class Multiplication
3 {
4

public static void main(String[] args){
Scanner sc = new Scanner(System.in);
int a = sc.nextInt();
int b = sc.nextInt();
int res = a x b;
System.out.println("The result is: " + res);

5

6

7

8

9

}

10
11 }

Fig. 1: Incorrect novice code sample

Researchers have been interested in compiler diagnostics
or syntax error messages for over half a century [17]. Barik et
al. reported [18] that the difﬁculties programmers face while
reading or understanding error messages are comparable to
the difﬁculty of reading source code. Understanding Java
error messages is quite challenging for two reasons; i) the
same error produces different diagnostics depending on
the context, and ii) the compiler may produce the same
diagnostic for different errors [18]. Though prior works [19],
[20] addressed ﬁxing errors in novice programs, DeepFix [7]
was the ﬁrst to apply deep learning to ﬁx errors. DeepFix
considers code repair as Neural Machine Translation (NMT)
and uses an encoder-decoder based deep learning model
to ﬁx errors in C programs. Though initially aimed at
semantic bugs, the approach also works for syntax errors.
This approach was limited by the use of RNN (recursive
neural network) seq2seq models—the RNN architecture is
challenged by longer inputs, and outputs; also since the
back-propagation through time (for the recursive elements)
is not easily parallelized, it’s challenging to exploit larger
datasets and additional processors. These became nagging
problems in NLP; initial efforts with basic attention mecha-
nisms [21] were supplanted by powerful multilayer models
with multiple attention heads to avoid recursive elements
altogether [22], yielding high-capacity, eminently paralleliz-
able transformer models. Certain errors, such as the ones
relating to block nesting, statement delimitation (with “;”)
etc. involve long-range syntax dependencies, and require
attending to very long contexts, which transformers can do

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

3

better; still, even these models fail when the dependencies
become much longer.

Ahmed et al. [9], developed BF+FF , using a multi-layer,
multi-head transformer approach, to address the limitations
of traditional seq2seq models. In addition, BF+FF used
a two-stage pipeline, with the ﬁrst stage addressing long-
range block nesting errors, even ones beyond the range of
transformers (BLOCKFIX) and the second stage addressing
shorter-range errors (FRAGFIX). Using the Blackbox [11]
dataset, they demonstrated that their approach substantially
improved over prior work on the same dataset [8] (which
used language models). BF+FF had important limitations,
noted in their paper;
it didn’t take advantage of error
localization and diagnosis provided by compilers; it also
didn’t effectively address errors in identiﬁers. Indeed, none
of the existing approaches dealt effectively with identiﬁers,
since they had to limit vocabulary. Deep learning models are
challenged by large vocabularies, which require very large
embedding and softmax layers. (See [23] details). We use
BPE [23] to address this issue.

By addressing these limitations, we were able to achieve
very substantial improvements on the state of the art for
ﬁxing JAVA programs. Ahmed et al.2 and Gupta et al.3 pro-
vided extensive source-available replication packages which
enabled us to provide a detailed comparison (See § 4).

3 METHODOLOGY
Previous work had various limitations: longer programs
were difﬁcult to repair; error messages from compilers were
not used; vocabulary limitations in DeepFix and design
choices in BF+FF limited the ability to address errors in
identiﬁer usage. SYNSHINE directly addresses these issues,
and achieves substantial improvements. We use a multi-
stage pipeline which incorporates the Java programming
language compiler (javac), along with three learned DL
neural networks (DNN). The ﬁrst DNN model is directly
based on the BLOCKFIX stage provided by BF+FF ; this
resolves (the potentially long-range dependent) nesting er-
rors in the program. In the second stage, SYNSHINE departs
from BF+FF . BF+FF uses the ﬁxed nesting structure from
BLOCKFIX to split the program into lines, and then just tries
to ﬁx every line; this leads to a lot of incorrect ﬁxes. Deepﬁx
and Santos et al. also try to ﬁx the entire program. The
second stage (LINEFIX) in SYNSHINE uses the line-location
of the error, as detected by the standard javac compiler,
together with the actual error message, and generates rel-
evant ﬁxes for delimiters, operators, and keywords; it also
ﬂags potential locations for errors in identiﬁer usage; these
locations are sent to the third & ﬁnal stage, UNKFIX. The
UNKFIX DNN model uses a Roberta-MLM to correct any
identiﬁers that ﬂagged as potentially wrong by LINEFIX.

3.1 Overall Architecture

Fig. 2 shows the architecture of our approach. When the IDE
ﬂags an error (step 1) we ﬁrst pass the program through a
block-nesting error checker (2), which is a simple pushdown
automaton, that checks the program’s nesting structure. If

2. https://zenodo.org/record/4420845
3. https://bitbucket.org/iiscseal/deepﬁx/src/master/

block-related issue is found, it’s sent from (2) to BLOCKFIX
(3) a transformer model (as provided in the open-source
BF+FF implementation [9]) for repair. In either case, the
code, hopefully now free of block-nesting errors, is sent
to step 4, where we try to locate the erroneous line using
javac. We identify the line that javac associates with the
syntax error, and pass it on to LINEFIX (step 5) with the
error message. In some cases, LINEFIX can ﬁx it directly; in
others, it passes a token position to UNKFIX (6), primarily
to ﬁx errors in identiﬁer usage. Finally, the ﬁxed code is
returned as a suggestion to the IDE (7).

We separate the line-level repairs into LINEFIX and UN-
KFIX to eke out more functions out of deep-learning model
capacity. LINEFIX outputs one of 154 possible editing com-
mands, to insert/delete/substitute delimiters, keywords,
operators, or identiﬁers. We limit its output vocabulary to
154. This limitation improves performance, but results in
more “unknown” ﬁxes, as described further below (§ 3.4).
These unknowns are resolved by the ﬁnal DNN model,
UNKFIX. UNKFIX uses a high-capacity masked-language
model to suggest a ﬁx (usually an identiﬁer being renamed
or inserted) given a location. In combination, these elements
allow us to substantially surpass the state-of-the-art.

3.2 javac errors: Promises and Perils

While novices often ﬁnd compiler error messages unhelp-
ful [5], our own experience suggests that they do help expe-
rienced developers! This suggests that with sufﬁcient train-
ing data, machine-learning models could learn something
about how to ﬁx syntax errors, from compiler syntax-error
diagnostics. Older machine-learning-based approaches had
not leveraged these diagnostics [7], [8], [9]. Recently, Dr-
Repair [10] uses these diagnostics for ﬁxing C programs;
SYNSHINE also uses them.

javac ﬂags syntactically incorrect programs with di-
agnostic errors; though the messages are not precise, they
are sometimes useful. Fig. 3 (a) presents an actual novice
program with two syntactic errors (missing “main” and
unwanted operator “+”). The javac compiler reports those
two errors for the given program 3 (b). Although these error
messages are unhelpful, javac does in this case ﬁnger the
actual lines with errors. Line-level syntax error localization
can be helpful, if the program is long. DeepFix, for example,
can not ﬁx longer programs; it relies on seq2seq translation
methods, and so has trouble with inputs longer than a few
100’s of tokens. BF+FF resolves this problem by trying to
ﬁx every line in the program using its FRAGFIX second
stage; this approach does induce a fair number of false
positives. javac promises more accurate location, which
could reduce this risk.

There is a potential issue with using javac, arising
mainly from the constraints of our novice error Blackbox
dataset. javac generates some error categories which can-
not be ﬁxed by editing the program directly. These errors
arise for example, from ﬁle-naming conventions and incom-
plete typing environments. For example, class name & ﬁle-
name mismatch errors, and missing class deﬁnition errors
are shown in Fig. 3 (c). The Blackbox dataset (also used by
Santos et al. [8] and Ahmed et al. [9]) only includes programs
with errors and their associated ﬁx; it does not include the

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

4

Fig. 2: Overall architecture of the SYNSHINE tool.

Fig. 3: Locating erroneous line using javac

complete programming environment. SYNSHINE only deals
with errors that can be ﬁxed by directly editing the JAVA
source; we ignore the others. This is a decision also made by
all the other papers that deal with syntax error correction [7],
[8], [9]; we do, however, make use of compiler diagnostics
for JAVA, and do manage to ﬁx a much larger portion of the
errors in the Blackbox dataset than prior work, as seen in
Table 2. Therefore, to remove the errors we don’t consider
from our training set, we simply wrote a wrapper around
javac, to retain just those errors that can be ﬁxed by editing
the source 4. However, it is important to note here that
these “unﬁxable” errors in our dataset are counted in the
denominator when we report our ﬁnal success rate; in other
words, these errors excluded from training are counted
against SYNSHINE and other tools as failures, and are not
ignored in our reported performance.

3.3 Recovering Block Structure: BLOCKFIX

Errors involving imbalanced curly braces are prevalent in
novice programs, and are hard to resolve because of the
long distance between the pair of braces. Ahmed et al. [9]
report that block nesting errors consist of around 20-25% of
all syntactic errors in novice programs [9]. They incorporate
a component, BLOCKFIX, for ﬁxing block-nesting errors.
BLOCKFIX uses a transformer-based machine-translation
model to locate & ﬁx block-nesting errors; the translation
model is trained on synthetic data with artiﬁcially gen-
erated nesting errors, and the corresponding ﬁx. It works

4. Most common ignored errors relate to “ﬁle and class name mis-

match” and “undeclared identiﬁers”.

with an abstracted version of the code without statements,
identiﬁers, and types to ﬁx errors in nesting structure.
In SYNSHINE, we simply adopt the BLOCKFIX component
from the implementation made available by Ahmed et al.’s
replication package.

Ahmed et al. abstracted out all the identiﬁers, constants,
expressions, and delimiters, retraining just the curly braces
and keywords (see Fig. 4). They then introduce structure-
related syntax corruptions, by adding or dropping the curly
braces at randomly chosen positions; and then teaching the
model to recover the original abstracted version from the
corrupted model. BLOCKFIX model learns to ﬁx such errors
by training on many such abstracted, corrupted pairs. After
ﬁxing the nesting error, the abstracted tokens are replaced
with the original ones, and the program is passed to the
following stages for further processing.

We found that javac works quite well in localizing the
error (at least the buggy line and ﬁnding the line is sufﬁcient
for our approach) if the program is free of nesting errors. This is
why we apply BLOCKFIX, before running javac to localize
and diagnose the error.

3.4 Fixing Line Error: LINEFIX

LINEFIX uses a RoBERTa based pre-training + ﬁne-tuning
approach. RoBERTa derives from BERT, which uses unla-
beled text data to pre-train deep bidirectional representa-
tions of text by jointly conditioning on both left and right
context in all layers of a deep transformer model [24] to
perform simple, self-supervised tasks like ﬁlling in masked
tokens. This model and training method effectively captures
the statistics of token co-occurrences in very large corpora

2.	Block	Error	Checker4.	Javac	Based	Error	Locator1.	IDE5.	LineFix7.	IDESuggestion	6.	UnkFix3.	BlockFixCode+Diagnostic	ErrorCode	W/OBlock	ErrorFixedCodeCode	w/Syntax	Error(a) Program with syntax error(b) javacerror message before fixing (c) javacerror message after fixing IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

5

Fig. 4: Abstracting source code for recovering block Struc-
ture.

Fig. 5: Pre-training and ﬁne-tuning using RoBERTa.

within the layers of the transformer model. This pre-trained
model learns excellent vector representations of code pat-
terns in the higher layers of the transformer; these learned
vector representations can be “ﬁne-tuned” with just one ad-
ditional output layer for speciﬁc tasks, and achieves state-of-
the-art performance. For pre-training, BERT uses two tasks:
ﬁll in masked out tokens using the context (also known as
Masked language modeling, or “MLM”) and predict the
next sentence given the previous one (the “NSP” task).
Liu et al.’s RoBERTa (Robustly Optimized BERT Pretraining
Approach) dominates BERT’s performance [25]. Liu et al.
drop the NSP objective but dynamically change the masking
pattern used in the MLM of BERT models.

Pre-training + ﬁne-tuning also works very well indeed
for code. One can gather millions of unlabeled code tokens
from open-source projects, conduct pre-training, and then
ﬁne-tune the model with a limited amount of labeled data
to achieve state-of-the-art performance in different software
engineering applications [26], [27], [28], [29] (albeit not yet
for code syntax repair). Since we are working on novice code
correction and our objective does not involve any relation
between two programs, such as Question Answering (QA)
and Natural Language Inference (NLI), training on NSP
is not beneﬁcial. Furthermore, using a dynamic masking
pattern to the training data helps the model achieve bet-
ter performance in downstream tasks. Therefore, We use
RoBERTa for pre-training and ﬁne-tuning of the model.

Why pre-training? As explained in the papers on BERT [24]
and RoBERTA [25], for natural
language, and the very
recent, but rapidly growing body of literature using pre-
training for code [26], [27], [28], [29], [30], [31], [32], [33], [34],
[35], [36], [37], pre-training is a way to exploit enormous
volumes of data in a self-supervised fashion to learn the
statistics of token sequences, and capture patterns in a
position-dependent vector notation. For our purposes, these
pre-trained models are automatically ingesting patterns of
syntax and identiﬁer usage from vast quantities of source
code (around a billion tokens) and bringing all this knowl-
edge implicitly to bear to the task of ﬁxing errors in syntax
and identiﬁer usage.

Pre-training To generate the dataset for pre-training, we
collected 5000 most starred Java projects from GitHub (since
our end-goal is to correct Java syntax errors). We tokenized

the ﬁles, yielding 1.2 billion tokens for the pre-training. For
the MLM pre-training over code, we randomly select 15% of
tokens, and replace with a unique token mask. The loss here
is the cross-entropy of the original masked token. Of the
15% selected tokens, 80% are replaced with a speciﬁc marker
mask, 10% are left unchanged, and a randomly selected
token replaces the remaining 10%. This training method
follows the standard RoBERTa protocol.

The architecture is as shown in Fig. 5. The main RoBERTa
model is in the central grey box, labeled “RoBERTa” in 5 (a)
and 5 (b). The left side is the architecture when RoBERTA is
being pre-trained; the last layer on top is the MLM, imple-
mented as a softmax layer taking the RoBERTa embeddings
as input, and produces an output token, The entire model is
trained using cross-entropy loss. Our RoBERTa architecture
consists of 12 attention layers, 768 hidden dimensions, and
12 self-attention heads in each layer. We applied Byte Level
BPE (Byte Pair Encoding) tokenizer [23] limiting the sub-
token vocabulary size to 25K.

We trained the MLM model using cross-entropy loss on
two NVIDIA Titan RTX GPUs for ﬁve epochs with a batch
size of 44 sequences and learning rate 5e − 5. When pre-
training completed, our MLM model achieved a ﬁnal loss
corresponding to a perplexity of 1.46, (cross-entropy 0.546
bits) which is rather low; RoBERTa for natural language
yields ﬁnal losses around 3.68-4.0 perplexity (1.88 to 2 bits).

Fine-tuning The ﬁne-tuning step here is to train LINEFIX,
a model that accepts an incorrect input line from a novice
program, (the line ﬂagged by javac as containing a syntax
error) together with the text of the error itself, and then
generates a set of locations and edit commands, using multi-
label classiﬁcation layers, as explained below.

For ﬁne-tuning and then for evaluation, we used realistic
novice programs with syntax errors and human-produced
ﬁxed versions. We used the exact dataset used by Santos et
al. [8] and Ahmed et al. [9] from the Blackbox [11] repository.
This dataset contains 1.7M pairs, of erroneous and ﬁxed
programs. Both Santos et al. and Ahmed et al. primarily
report their performance on programs with a single token
error because a single edit can ﬁx a large fraction of the
programs (around 57%). Therefore, for a fair comparison, we
also initially focused our evaluation on single token errors
and broke down our performance by token-length, as done
by Ahmed et al. We selected a test set of 100K samples, with

(a) Original function(b)AbstractedversionRoBERTa(12 Layer Transformer)Tok1<s><\s>Masked LMTokN------------Unlabeled correct program from GitHub(a) Pre-trainingRoBERTa12 Layer Transformer,PretrainedTok1<s><\s>Token PositionTokN----Incorrect Programs from BlackBoxFixPosition LossFix Loss+Total LossCross Entropy LossEmbeddingsEmbeddings(b) Fine-tuningIEEE TRANSACTIONS ON SOFTWARE ENGINEERING

6

samples stratiﬁed by length,
from the full dataset for the
evaluation. We divided the test dataset into ten token-length
ranges (lengths of 1-100, 101-200, ... , and 900-1000 tokens),
with each range having around 10K examples. We prepare
our ﬁne-tuning dataset from the remaining examples.

Since BLOCKFIX handles long-range block-nesting er-
rors, the LINEFIX stage is focused on those errors unrelated
to nesting. We discarded the programs with imbalanced
curly braces from the training set, and after tokenization,
we found around 540K examples to train the model. We
used javac (discussed in Section 3.2) to localize the error.
The input to the model then is the buggy line indicated
by javac, appended with a special separator token (de-
noted <SEP>) followed by the error message from javac.
Altogether, the maximum input is 150 sub-tokens, which
captures virtually all the input lines ﬂagged as erroneous in
our dataset. From this, the pre-trained RoBERTa model cal-
culates positional embeddings for each subtoken; however,
as with many RoBERTa-based classiﬁcation tasks, we use
just the embedding of the ﬁrst token.

The desired output is the matching edits required to

create the ﬁxed version, as explained next.

To make a complete ﬁx, the model should produce one or
more locations, and one or more “ﬁx”, viz edit commands.
The ﬁx has two parts: i) the type of ﬁx (insertion, deletion, or
substitute?) ii) the content of the ﬁx (is it a speciﬁc keyword,
delimiter, or any other token?). When the type is a deletion,
there is no content required: if the model identiﬁes the buggy
token at position x and recommends deletion, we just drop
that token. For substitute operation, if the location is x and
the edit command is substitute → y, we will replace the
token at position x with the token y. For insertion, if the
command for position x is insert → y, we will add the
suggested y token at the x + 1 position. For insertion at
the start of the line, we use a special token. For example,
consider the following buggy line from Fig. 3 (a).

public static void (String args[])

To ﬁx this missing “main”, LINEFIX should output the
location “3” and the ﬁx “insert → unk” (“main” is an
identiﬁer). This “unk” will be coverted to “main” with
another model. We will discuss it in Section 3.5.

Our model’s ﬁnal layer consists of two distinct multi-
label classiﬁcation output layers, one which outputs one or
more locations, another which outputs one or more ﬁxes.
The input to both these output layers, as explained above,
is the RoBERTa embedding of the ﬁrst token of the input.
From this input, the two separate multi-label classiﬁcation
output layers calculate the position(s), and ﬁx(es). Since
most (99%) of the erroneous lines are 100 tokens are less, we
output one or more positions (1-100) from the ﬁrst output
layer, and, from the second output layer we generate one
or more of 154 distinct possible ﬁxes. We remind the reader
that a multi-label classiﬁcation task involves generating an
output vector of class probabilities, where the classes are
non-exclusive. A single input might generate one or more
class labels. In our case, we take all class labels in the output
vector scoring above 0.5 as an assigned label. If none of the
classes are assigned a probability above 0.5, we just take the
highest probability class label. In almost all cases, we have
only one ﬁx per line, so one position and one edit command

are expected; however, in rare cases, more than one position
and more than one edit command could be generated. In
the former case, we just apply the edit command at that
position; in the latter case, which occurs very rarely, we
try all combinations and return the ﬁrst edit combination
that compiles. A somewhat more common case (for example
with multiple missing delimiters, like ”)”), we get one edit
command like insert →) and multiple locations, in which
case, we just apply the same edit at all locations.

There are reasons for our choice of multi-label classiﬁca-
tion, rather than simply synthesizing the ﬁxed output. Prior
approaches [7], [8], [9] used autoregressive5 code genera-
tion to synthesize repairs. Given the sizeable vocabularies
in code, many complex dependencies must be accounted
for when generating code tokens conditional on previous
tokens, the original input tokens, and the compiler error.
We simplify the problem into a multi-label classiﬁcation task
here; all that is required is to identify the token positions(s)
of the error, and the applicable edit commands. In the vast
majority of cases, there is usually only a single change
required per line). This allows the model to learn, and
rapidly reduce training loss and perform well under test.
In addition, the multi-labeling approach (rather than auto-
regressive generation also allows us to handle repairs that
require multiple ﬁxes on the same line (example below,
Fig 6). It’s important to note that a single line can con-
tain several token locations with errors, and distinct edit
commands at each position. Limiting the size of the set of
possible ﬁxes to 154 will limit the ability to ﬁx identiﬁer
names; this is handled by including ﬁx commands that
insert and substitute to unk in the output vocabulary of
LineFix; these ﬁxes are handled by a component is called
UNKFIX, which is described in § 3.5. Note that dealing with
multiple ﬁxes on different lines is easily manageable. If there
are multiple positions, all with the same ﬁx (like Fig. 6), one
can just perform that ﬁx at all the positions. However, for
multiple positions and multiple ﬁxes one needs to try all
combinations until the javac accepts with no errors. We
did not incorporate that to our code, because:

1) Trying all possible combinations will slow down the

entire process.

2) Two different errors in a line (even in a ﬁle) is very
rare. In the Blackbox data repository, for example, the
majority of ﬁles contain just a single syntactical error.

The standard way to train multi-label classiﬁcation lay-
ers is with binary cross-entropy loss (with logits), which
is what we use for our ﬁne-tuning. Since both the output
layers are closely related to each other, we ﬁne-tuned them
simultaneously for 5 epochs. We collected the loss from each
layer and added them to deﬁne the batch’s ﬁnal loss, and
updated the model accordingly. Note that the same pre-
trained model parameters (from Fig. 5 (a)) are used to ini-
tialize these; during ﬁne-tuning, all parameters in all layers
are modiﬁed (Fig. 5). We use the Huggingface open-source
implementation of RoBERTa [38] for both pre-training and
ﬁne-tuning.

5. Autoregressive generation conditions the generation of each token
on previously generated tokens, and is used in machine-translation
approaches.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

-System.out.println(*);
+System.out.println("*");

Fig. 6: Example requiring two edits to ﬁx

Utilizing Compiler Diagnostics during Fine-tuning Apart from
localizing the erroneous line, the compiler warning can
boost the performance of the ﬁne-tuning model. As an input
sequence to the model, we tried two versions, i.e., with
the warning, without warning. We observed a small but
signiﬁcant improvement in line-level code ﬁxing (detailed
in Section 4.2). Consider the following code snippet from
the Blackbox dataset. The variable “bmr” is declared twice,
and the second declaration is invalid. Though the javac
localizes the error correctly, it is really hard for the model
to resolve this without any hint. Our model fails to ﬁx this
one when trained without the compiler message. However,
with the compiler error message, our RoBERTa-based ﬁne-
tuned model can solve errors like this one by deleting the
token “double”. This particular example is ﬁxable with a
modern IDE; however, it serves as a good illustration of
how our model can use error messages. We remind the
reader that in general we can handle numerous examples
that IDEs cannot. Several typical examples are included in
the supplemental ﬁle https://bit.ly/3CMM0TP.

double bmr;
/* some additional irrelevant lines */
boolean isMale = male == ’M’;
if(isMale)
double bmr = ((9.5 * wgt) + (5.0 * hgt)
+ (6.7 * age) + 66.47);

Without Warning:
double bmr = ((9.5 * wgt) + (5.0 * hgt) + (6.7 * age) +
66.47);

With Warning:
double bmr = ((9.5 * wgt) + (5.0 * hgt) + (6.7 * age) +
66.47); <SEP> variable declaration not allowed here

LINEFIX works best with small sequences. Java is inher-
ently verbose, and so sequence lengths are often beyond
the model’s capacity. Compiler diagnostics help us in two
ways. Primarily, it helps us localize the error, and secondly,
the message (even if imprecise) helps deep learning models
ﬁx the error. This claim is supported by a study (Yasunaga
et al. [10]).

3.5 Recovering Unknown Tokens: UNKFIX
Recall that LineFix output is restricted to 154 distinct ﬁxes
in the ﬁne-tuning model. To deal with edits (inserts or sub-
stitutes of identiﬁers, constants etc.) outside of the limited
vocabulary of edits, have an “escape” mechanism. Out of
these 154, we included two unique outputs insert → unk
and substitute → unk to cover other changes. To precisely
identify these “unk” tokens, we use UNKFIX, which reuses
the masked-language model (MLM) we obtained during
pre-training. This masked language model can recover the
unk tokens if sufﬁcient context is given. After getting the
position information, we can collect sufﬁcient tokens from

7

the previous and following lines to ﬁll the input buffer, and
ask the pre-trained model to unmask the unk. Applying
this approach, we could ﬁx several unk-related program
errors like the following ones where the LineFix predicts
insert → unk and substitute → unk for “Item” and
“Integer”, and then the MLM is able to locate them correctly.

-public void takeItem (item ) {
+public void takeItem (Item item ) {

-float number = float.parseInt(text);
+float number = Integer.parseInt(text);

Note that though we designed UNKFIX primarily for iden-
tiﬁers, it can potentially handle other tokens, including
values.

3.6 Integrating SYNSHINE into VSCode

To make SYNSHINE more broadly accessible, we have made
it available within a popular IDE. We have initially chosen
VSCode since it’s widely available, free for students6, and
well-documented; in the future, we will incorporate SYN-
SHINE into other IDEs. The source code for the integration
is available in our replication package. A demo video is
viewable: https://youtu.be/AR1nd2PJczU.

In this VSCode integration, we desired fast response
times, and wanted to avoid the requirement for a GPU, since
many novices may not have a GPU. So for the SYNSHINE
deep learning model, we just used CPU ﬂoating point
operations; to avoid having to reload the (very large) model
for each repair request, we wrapped the SYNSHINE model
within a “correction” server, which services HTTP requests
from the IDE.

The IDE triggers a request to SYNSHINE when the user
requests a ﬁx suggestion. When SYNSHINE is triggered,
VSCode looks for the active text editor and extracts the
(erroneous) code content from there. After getting the con-
tent, VSCode sends an HTTP request to the code correction
server. Models are pre-loaded in the correction server, so
that it can immediately service requests. In this server, the
code goes through our proposed pipeline presented in Fig. 2,
and the code returns to the editor after ﬁnishing all the steps.
Now we have two versions of the code, i.e., the buggy code
and the corrected version. We highlight the difference and
present both versions to the user and allow them to accept
or reject the solution.

Note that the demo presented on the link mentioned
above was captured on a machine without any GPU. We
observe that SYNSHINE can operate on a CPU and is quite
fast at generating the solution even though the models
were trained on GPUs. Just to get a sense of the delay, we
randomly chose 200 erroneous programs of various lengths
from our dataset, and measured the response time (time
from the “SYNSHINE” button press to the time the ﬁxed code
is received back). The average response time is 0.88 seconds
(standard deviation 0.49s, maximum 2.2s). While this by no
means instantaneous, we can still provide a ﬁx for a syntax
error virtually always within a second or two, potentially

6. https://visualstudio.microsoft.com/students/

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

8

saving the novice and instructor’s time. Our approach to in-
tegrating SYNSHINE into VSCode thus arguably attenuates
the need for expensive GPUs, and facilitates the use of the
deep learning model in CPU-only machines. The CPU we
used for the experiment is “AMD Ryzen 7 2700X”. The code
correction server occupies 1.765 GB of the memory.

SynShine’s response time is signiﬁcantly lower than the
time needed by a programmer to ﬁx the program. Brown
and Altadmri divided the mistakes that occurred in the
Blackbox repository into 18 different classes, where 11 of
them are syntactical errors [39]. The programmers take 13-
1000 seconds (median) to ﬁx the mistakes [39]. Our model,
on the other hand, takes less than a second on average to
process the ﬁles and suggest a ﬁx.

4 EVALUATION & RESULTS

In our evaluation, we compare our work with several
baselines: Santos et al., DeepFix , BF+FF , and SequenceR.
The original DeepFix [7] used a GRU based RNN encoder-
decoder translation model, which takes an entire program
(with syntax error) as input, and produces a ﬁx. For baselin-
ing their BF+FF tool, Ahmed et al. used two versions of
DeepFix, one (“short”) trained on error-ﬁx pairs upto 400
tokens long and another (“long”) trained on error-ﬁx pairs
upto 800 tokens long. Another approach, SequenceR [40] has
reported success in ﬁxing semantic errors, when provided
with fault localization; it is also adaptable for syntax errors.
SequenceR differs from DeepFix in a few ways: it uses a
separate fault localizer, and also incorporates a copy mech-
anism. We describe the intricacies in full detail later. Ahmed
et al.’s BF+FF program used a 2-stage transformer-based
lenient parser, as described above. Our approach combines
several techniques: pre-training, compiler-based reporting,
and ﬁne-tuning with novice data.

Below, we present summary top-1 accuracy results, eval-
uated over a random sample of 100,000 examples of length
upto 1000 tokens, with single-token errors, taken from the
Blackbox dataset. The detailed result is presented in Table 2.
We follow the lead of the ﬁrst paper in the area [8] in
this table, reporting performance for single-token errors,
which constitute 57% of the data in Blackbox. We report the
numbers for more complex errors below. As can be seen,

Santos
et al. [8]
46.00%

DeepFix
(short)
63.25%

DeepFix
(long)
62.14%

SequenceR BF+FF

SYNSHINE

56.89%

56.91%

74.89%

TABLE 1: Summary Results: Santos et al. performance is as reported
by them; we measured the others

SYNSHINE achieves a substantial performance boost, over
all the prior approaches, elevating the performance further
and providing us with the motivation to build it into a
popular IDE to make it more widely available. Here below,
we evaluate the performance in more detail, comparing
SYNSHINE with the closer competitors (we exclude Santos et
al. from this comparison) and also examine the contributions
of our various stages to the signiﬁcant overall improvement.
We begin with an evaluation of the effect of program length
on performance, then we consider the effect of the various

components of SYNSHINE. Finally, we breakdown the per-
formance of SYNSHINE in repairing various categories of
syntax errors.

4.1 Fixing shorter & longer programs

Table 2 baselines the relative performance of SYNSHINE
against prior work, broken down by length, in categories.
The rows are different length ranges of programs. The
second column is the fraction of the Blackbox programs
falling in this length range. The next several columns are
are baselines from prior work: ﬁrst two are DeepFix (short)
trained on shorter error-ﬁx pairs (upto 400 tokens long),
DeepFix (long) trained on pairs up to 800 tokens long. The
next two are SequenceR, trained on all pairs in the training
set, and BF+FF , trained exactly provided in Ahmed et al.’s
scripts. Finally, on the last column we have our results from
SYNSHINE; the 3 columns to the right of the SYNSHINE
column represent the contributions of our 3 components.
As can be seen our overall performance exceeds the perfor-
mance of all the others in every length category, and on the
entire sample signiﬁcantly improves on all of them. Before
we examine the numbers in detail, we ﬁrst present some
relevant details on how we measured them.

All evaluations were done on a very large, randomly
chosen, representative sample of 100,000 error-ﬁx pairs from
Blackbox that were not seen during training by any of the
models. The percentages shown in the second column, and
the overall performance numbers (all numbers are top-1
accuracy) are thus robust estimates of actual performance
on programs up to 1000 tokens long, which constitute
around 95% of the Blackbox data. An additional evalua-
tion on a random sample of the entire dataset is reported
below. DeepFix (short), DeepFix (long), and BF+FF were
all trained and evaluated using the scripts made available
in the replication package of Ahmed et al. [9] and Gupta et
al. [7].

SequenceR [40] had to be retrained for syntax error
correction: Chen et al. originally developed SequenceR for
ﬁxing semantic bugs, viz., test failures. It uses the OpenNMT
translation framework [41] and thus had to be trained using
bug-ﬁx pairs. SequenceR assumes that the precise location
of the bug was known via fault-localization; the training
pairs consisted of a) the buggy region of code, bracketed
within <start_bug> . . . <end_bug> markers, augmented
with sufﬁcient context (preceding and succeeding tokens)
to make up 1000 tokens of input b) and the correspond-
ing ﬁx, which is the region including the changed code,
upto a maximum of 100 tokens; longer ﬁx regions will
fail (this almost never happens in our setting). They used
an RNN sequence-to-sequence encoder-decoder model that
uses LSTM for the recurrent nodes, and incorporates a copy
mechanism to enable the model to generate speciﬁc local
variables, etc. in ﬁxes. We used the code provided by Chen
et al, and trained the model using Blackbox data; we used
the javac compiler to ﬁnd the error location, and created
training/test pairs using the javac indicated location (with
context), together with the corresponding novice ﬁx. In our
case, since most novices’ programs are shorter than 1000
tokens, we provided the entire novice program as context.
Once SequenceR is trained, it can generate ﬁxes, given the

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

9

Token
Range

Percent of
Overall Data

DeepFix
(short)

DeepFix
(long)

SequenceR BF+FF

1-100
101-200
201-300
301-400
401-500
501-600
601-700
701-800
801-900
901-1000
Overall

31.01%
29.43%
15.25%
8.56%
5.51%
3.63%
2.17%
1.90%
1.34%
1.19%

76.71%
69.98%
63.27%
53.71%
42.17%
32.84%
23.76%
17.10%
11.43%
8.80%
63.25%

73.72%
67.15%
60.29%
54.02%
45.47%
39.78%
33.02%
26.57%
22.88%
17.94%
62.14%

59.21%
57.21%
55.40%
54.64%
54.54%
54.47%
54.35%
53.78%
55.56%
53.87%
56.89%

65.16%
60.24%
54.47%
50.01%
46.19%
42.81%
38.07%
35.35%
32.24%
29.62%
56.91%

By
BLOCKFIX
21.01%
17.53%
14.35%
10.18%
7.71%
5.95%
3.80%
3.04%
2.20%
1.27%
15.56%

SYNSHINE
By
LINEFIX
58.86%
58.98%
56.00%
54.45%
54.00%
53.83%
53.62%
51.98%
52.84%
51.63%
57.22%

By
UNKFIX
2.41%
1.96%
1.93%
1.89%
1.88%
2.19%
2.10%
2.65%
2.19%
2.10%
2.11%

Total

82.28%
78.47%
72.28%
66.52%
63.59%
61.97%
59.52%
57.67%
57.23%
55.00%
74.89%

TABLE 2: Baselining SYNSHINE against prior work on syntax error correction. SequenceR was provided with javac localization.

novice program with error, with location indicated as above.
However, SequenceR cannot insert or delete entire lines, so
it cannot ﬁx many nesting errors (for example, by inserting
or deleting a line with a single ”{” or ”}” delimiter).

Our overall accuracy ranges between 55% to 82%, and
always outperforms DeepFix long (18%-74%), and short
(9%-77%), SequenceR (54%-59%) and BF+FF (29%-65%).
Both SequenceR and SYNSHINE beneﬁt from the error lo-
cation provided by javac. By improving on prior work at
every range, on the entire representative 100,000 sample,
SYNSHINE achieves signiﬁcant gains in overall performance
(bottom line) over the state of the art. Two factors contribute
to this improvement: i) javac-based error localization and ii)
robustness of LINEFIX and UNKFIX. javac-based error local-
ization enables a more selective LineFix+UnkFix to the most
likely errorful code, thus reducing false positives; Ahmed et
al.’s BF+FF attempts corrections throughout the program,
resulting in more mistaken corrections. The robustness of
LINEFIX and UNKFIX is really boosted by the pre-training +
Fine-tuning strategy; we explore the relative beneﬁts of this
step further below.

Table 2, in columns under the SYNSHINE header, also
shows relative contributions of the components of SYN-
SHINE. First stage is BLOCKFIX borrowed from BF+FF .
About 20%-25% programs, regardless of length have nesting
errors. BLOCKFIX’s accuracy decreases with program length,
and we observe that the contribution of BLOCKFIX is low
after 700 tokens. However, for the other 75% to 80% pro-
grams without nesting errors, LINEFIX & UNKFIX perform
pretty consistently. Finally, we note that 1-1000 tokens cover
about 95% of the overall data. To observe the performance of
SYNSHINE on the overall distribution, including programs
over 1000 tokens long, we test it on 5000 random samples.
We found that our model can repair 75.36% of the programs,
and as before, comfortably exceeds performance of prior
tools. Note that if the BLOCKFIX model has already ﬁxed
the curly braces and there is no other error, javac will not
produce any error message, and LINEFIX will not process
that. Note that we always compare the end-to-end tokens
of the reference and the model’s proposed sequence; if
needless “over” ﬁxes are applied, that will be counted as
wrong. Moreover, none of the ﬁxes are credited twice. If the
model is ﬁxed by UNKFIX, it alone receives credit; we did
not count it in the LINEFIX column. Likewise, we credited
a sample in the LINEFIX column, if it is completely ﬁxed by
LINEFIX and does not receive any help from UNKFIX.

We also applied our model on ﬁles that required 2
and 3 edits to ﬁx the program and observed 29.4% and
14.4% accuracy, which is much higher than the reported
accuracy by Ahmed et al. (19% and 9%). Finally, we note
that Ahmed et al. report on a blended strategy where shorter
uncompilable programs could be sent to DeepFix and longer
ones to BF+FF , thus obtaining better performance than
either at all lengths. A similar strategy could be employed
here, blending SYNSHINE with other models, trying all
the proposed solutions, and picking the ones that compile.
However we didn’t implement this approach: we just inte-
grated SYNSHINE into VSCode since it performs quite well
at all lengths on its own, and avoids the need to load and
run many models, and try repeated compiles.

4.2 LINEFIX: The role of Compiler Errors

SYNSHINE differs from both versions of DeepFix, and Se-
quenceR, because it’s multistage; it differs from BF+FF
mainly because of the two new components, LINEFIX and
UNKFIX. We simply reused the BLOCKFIX component made
available by Ahmed et al. 7, and ﬁnd performance very
similar to that reported by them for this component. The
improvements reported in Table 2 clearly arise from our two
new components. We now focus in on LINEFIX and evaluate
how it contributes to overall performance. LINEFIX’s task
is to take an input line ﬂagged as a relevant syntax error
(by javac), together with the actual error, and then output
a position, and an editing hint (insert, substitute, delete).
LINEFIX improves upon the FRAGFIX stage of BF+FF in
two ways: ﬁrst, it uses pretraining+ﬁnetuning, and second,
it also takes the syntax error message from javac as an
additional input. The value of pre-training has been exten-
sively documented for code-related tasks [26], [27], [28], [29],
[30], [31], [32], [33], [34], [35], [36], [37], so we focus here
on the effect of providing compiler errors. Note again that
LINEFIX has two tasks: Localize the token to be replaced,
and and output an editing command with the correct Fix.
We evaluate the impact of compiler warnings using 10,000
randomly chosen erroneous lines, of various lengths, each
taken with and without the compiler syntax error messages.
Since we’re evaluating ﬁxing capability on single erroneous
lines, rather than entire programs, the numbers reported
below are higher than in Table 2.

7. https://zenodo.org/record/4420845

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

10

With compiler error?

No

Yes

Localization

Fix

Complete Correction (Location+Fix)

F-Score

90.75%

93.58%

F-Score

92.41%

93.18%

Accuracy

86.71%

89.39%

TABLE 3: Impact of using compiler error

Table 3 presents the impact of using the syntax error

message in our tool.

We gain around 2.7% improvement in overall accuracy
using the compiler error message. We also see improve-
ments on both Localization and Fix f-scores by providing
the compiler message along with the the erroneous line
(row 1 & 2). The improvement is more for the Localization
than for the Fix. We tested the statistical signiﬁcance of all
differences, using Binomial difference of proportions test on
a trial sample of 10,000; we then corrected the p-values us-
ing Benjamini-Hochberg. The improvements observed when
using compiler error message for overall accuracy and ﬁx
location f-score are highly signiﬁcant (p < 1e − 9); however,
the f-score for the ﬁx per se are only signiﬁcantly improved
(0.01 < p < 0.05). This suggests that the compiler error
message is of highly signiﬁcant help in providing our model
with information required to locate the precise token that
needs to be edited, and somewhat less so to identify the
precise edit that is required. It is very important to note how-
ever, that the javac compiler is of crucial help in locating
the line where the error is located. This above study also
shows that the actual error message per se helps our model
locate the token within that line that needs to be edited.

We present an illustrative example of how compiler error
messages help. Sometimes the compiler warnings are very
precise, e.g., when semicolons or other punctuations are to
be inserted. In such cases, it may appear that the task is quite
simple, and the model is simply “translating” the error into
a ﬁx. We sampled 50 programs and observed how many
of them can be ﬁxed just by reading the comments. We
observed that in roughly 60% cases, the javac warning is
not that helpful, and the model learns to respond in fairly
nuanced ways to address the error. Consider the following
repair that LINEFIX correctly achieves.

-return s == reverse ( String s ) ;
+return s == reverse ( s ) ;

javac per se not helpful: it produces an error message
suggesting to insert “)” after “String”. LINEFIX learns
to ignore such messages, and instead correctly omits the
token “String”. Therefore, the model is not just “translat-
ing” the message from javac into a ﬁx; The high capacity
of the model, enriched by pre-training and ﬁne-tuning, is
deployed to leverage the often incorrect, imprecise message
from javac into a good ﬁx. Depending on the error, it
can resolve a very imprecise message from javac. Indeed,
quite often the same error message from javac can lead the
model to provide very different (correct) ﬁxes.

4.3 When SYNSHINE fails, and when it works.

We now examine in further detail the cases where SYN-
SHINE works correctly, and where it does not. To be conser-
vative, we have deﬁned as a “failure” any ﬁx not exactly the

same as the one recorded in the Blackbox dataset; note that a)
the ﬁx recorded in Blackbox is created by an actual human
user, and also b) the recorded ﬁxes always compile without
error. We start with an examination of the cases where SYN-
SHINE fails to produce a correct ﬁx, as per our conservative
deﬁnition, and then examine in detail the diversity of ﬁxes
that it does provide.

Length

1-100
101-200
201-300
301-400
401-500
501-600
601-700
701-800
801-900
901-1000

Overall
Compilability
of ﬁxes
90.18%
86.13%
79.33%
73.35%
70.14%
67.83%
65.92%
64.00%
63.32%
60.76%

Fixes
Exactly Matching
Blackbox
82.28%
78.47%
72.28%
66.52%
63.59%
61.97%
59.52%
57.67%
57.23%
55.00%

Compilability
for
non-matching cases
44.58%
35.58%
25.43%
20.40%
17.99%
15.41%
15.81%
14.96%
14.24%
13.00%

TABLE 4: Compilability of SYNSHINE

Category

Keyword
Operator
Delimiter
Other

Prevalence of
Error Category
5.04%
5.87%
80.37%
8.72%

Fix Accuracy
(in %)
70.64%
77.73%
81.60%
60.94%

TABLE 5: Performance of SYNSHINE over diverse error
categories

Fix Failures Despite our over-conservative deﬁnition of “fail-
ure”, sometimes SYNSHINE can generate a solution that dif-
fers from the user-intended solution but is still compilable
with our javac-based compiler. In some cases, the solution
is even semantically correct. As an illustration, in Table 6,
examples 1, 2 & 3 are ﬁxes generated by SYNSHINE that not
only compile without error, but are also semantically correct.
By contrast, the last example in Table 6 is not semantically
correct but compilable. Ideally, we’d like to characterize how
often SYNSHINE ﬁnds ﬁxes that are not only compilable, but
also semantically correct. The compilability of a ﬁx that differs
from the user’s ﬁx recorded in Blackbox can be determined
automatically, and at scale (by just compiling!) and we
report it below; however, the semantic correctness of a ﬁx that
differs from a user’s ﬁx requires manual examination, and
is not practical to do at a large scale. We try to characterize
these to some extent by examining a small sample.

Table 4 presents the overall compilability of the solu-
tions. The second column is the overall compilability of the
generated ﬁx. This is calculated as the fraction of the num-
ber of attempted ﬁxes, that actually results in a successful
compilation. The third column is the proportion of ﬁxes
that we deem correct, based on exact match with the ﬁx
recorded in Blackbox (the numbers will match shown in the
rightmost column of Table 2). As can be seen, we record
many compilable cases as incorrect. The last column in
Table 4 shows the proportion of apparent failures that are
actually compilable: as an illustration, for programs up to
100 tokens long, about 45% of the cases that we record as an
incorrect ﬁx, in fact compile correctly. Depending on length,
between 13% and 45% of the ﬁxes we classify as failures

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

11

Seq No
1
2
3
4

Buggy Line

Model

Original Fixed

int i = ( ( int ) ( Math . random ( ) * 3 ) ;
int userInt 1 , int userInt 2 ;
System . out . print ( ( ” Hello, world. ” ) ;
System . out . println ( ” sum = ” + ( sum + ) ) ;

int i = ( ( int ) ( Math . random ( ) * 3 ) ) ;
int userInt 1 ; int userInt 2 ;
System . out . print ( ” Hello, world. ” ) ;
System . out . println ( ” sum = ” + ( sum ) ) ;

int i = ( int ) ( Math . random ( ) * 3 ) ;
int userInt 1 , userInt 2 ;
System . out . print ( ( ” Hello, world. ” ) ) ;
System . out . println ( ” sum = ” + ( sum + 5 ) ) ;

TABLE 6: Examples showing the compilability of the model

are actually compilable. Table 6, examples 1,2,3,4 are exactly
such ﬁxes.

Now what proportion of these “compilable failures” are
actually semantically correct? To get a (very) rough estimate
of this, we did a small manual study. We randomly collect
50 cases where the model generates a compilable ﬁx, that
fails to match the user ﬁx recorded in Blackbox. We found
that about 18% of programs are semantically correct.

To summarize: even in our very conservative evaluation,
SYNSHINE produces the same ﬁxes as recorded by a human
in a sizable fraction (roughly 75%) of errors in our novice
dataset; an examination of SYNSHINE’s failures suggests
that it could possibly be helpful in some additional cases.
Fix Diversity What kinds of errors does SYNSHINE ﬁx? In our
dataset, about 80% of the errors are related to delimiters,
and even solving only those would make a signiﬁcant
dent. However, the novices make syntax errors in using
keywords, operators, identiﬁers, and numbers; sometimes
they introduce illegal spaces, declarations, characters, etc.
We examined how SYNSHINE performs with respect to
different types of errors. For convenience, we divided the
error into four major categories- keywords (all Java key-
words), delimiters (e.g., semicolon, comma, parentheses,
braces, brackets), operators (all Java operators), and others
(identiﬁers, literals, and anything that falls outside the ﬁrst
three categories). To do categorization, we followed two
rules. Errors that required substitutes or inserts belonged to
the category of the substituted or inserted token; errors that
required deletion belonged to the category of the deleted
token. Thus if an error required a semicolon to be inserted,
it was in the “delimiter” category; if an error required an
extra “if” keyword to be deleted, it was in the “keyword”
category.

We randomly sampled a 5K test dataset, and determined
the error category prevalence in this dataset; see Table 5, ﬁrst
column, for the prevalence of errors in various categories.
Delimiter errors dominate, and thus our model learns to ﬁx
those best (81.6% accuracy); however, it performs well in
other categories (60%-78% accuracy). The take-away from
this analysis is that SYNSHINE performs reasonably well at
a wide range of syntax errors.

5 RELATED WORK
The most closely related works are DeepFix [7], BF+FF [9],
and Santos et al. [8] which we have discussed above. We also
discussed SequenceR [40]. We have compared SYNSHINE to
all of these.

Gupta et al. [12] applied reinforcement learning to a very
similar dataset like DeepFix [7]. It utilizes total count of
compiler errors as a part of the reward mechanism. How-
ever, RLAssist [12] shows only a very minor improvement
over DeepFix [7], and also it takes the whole program as
input. Therefore, we did not re-implement RLAssist [12].

Though RLAssist [12] looks into compiler errors but it
does not directly uses the error messages as we do. Deep-
Delta [42] is another approach that ﬁxes compiler errors
but mostly identiﬁer name-related errors, not syntax er-
rors. DeepDelta [42] was developed and tested on code
from professional developers at Google. The authors also
assume that precise knowledge of the location will be given
to the program. Yasunaga et al. [10], [13] introduce two
compiler-dependent approaches to ﬁx C program: DrRepair
that utilizes C compiler warnings with a graph-based self-
supervised approach, and BIFI that applies two models
“critic” and “ﬁxer” to ﬁx the programs. A tool for the C
programming language, Tracer, abstracts the code and uses
a seq2seq model on the source code abstractions that are
later concretized [43].

All the DNN based Automatic Program Repair (APR)
tasks have a fault localization step [40], [44], [45], [46],
[47], and these tools’ performance depends a lot on the
fault-localizer. Semantic code correction is an inherently
difﬁcult problem, and syntax correction can be considered
as a subset of semantic code correction problems. None
of the previous syntax correction tools has compared their
work with these tools because previous syntax correction
tools did not depend on any fault localizer. Some of the
APR tools [45], [48], [49], [50] expects syntactically correct
programs and those approaches are not applicable for syn-
tactical code correction. For our purposes the most directly
compatible recent APR tool was “SequenceR” [40] which
reported good performance, and also ﬁxes errors at the line
level; it was readily adapted to using the javac to locate
the line to be ﬁxed, so we chose it for comparison. Pradel
et al. also detect speciﬁc types of bugs (e.g., accidentally
swapped function arguments, incorrect binary operators,
and incorrect operands in binary operations) but in syntac-
tically correct code [51].

Brown et al. used BlueJ IDE to collect the data in Blackbox
repository [11] In this paper, we did a case study on the
performance of the popular IDEs (e.g., Eclipse, IntelliJ, VS-
Code, BlueJ) in ﬁxing novice programs. We compare repair
hints from Eclipse JDT Core Compiler for Java (ECJ) (used
in both Eclipse and VSCode) and javac (used by IntelliJ and
BlueJ). That is, both Eclipse and VSCode present the same
error messages, and IntelliJ and BlueJ present the same error
messages. Four IDEs, but ultimately, only two compilers.
SYNSHINE improves upon repair hints from both compilers.
Therefore, we primarily focus on Eclipse and IntelliJ for
the case study. We chose VSCode because it is popular,
well-documented, available free for students, and is easy to
extend. We were able to integrate SYNSHINE into VSCode
without any major difﬁculties.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

12

6 CONCLUSION

We have described SYNSHINE, a machine-learning based
tool to ﬁx syntax errors in programs. SYNSHINE leverages
RoBERTa pre-training, uses compiler errors (both location
and message), and generates ﬁxes using multi-label classi-
ﬁcation, rather than autoregressive generation, to achieve
substantial improvements in ﬁxing syntax errors. Our eval-
uation shows substantial improvements in ﬁxing rates over
the previous best results reported by BF+FF , and other
tools, at all program lengths. Our evaluations suggest that
the the use of compilers to locate the precise line pro-
vides a big advantage; our evaluations also suggest that
the compiler error message per se may be helpful in lo-
cating the precise token within the line that needs to be
repaired. We have built SYNSHINE into the VSCode IDE,
and have found that even without a GPU, the SYNSHINE-
enhanced VSCode can ﬁx syntax errors fairly quickly, often
in less than a second. We have made all the source-code
and data available, to the extent allowable under UK Law
applicable to the Blackbox dataset. SYNSHINE can ﬁx errors
that IDEs (Eclipse, IntelliJ, BlueJ, and VSCode) cannot. In
the supplementary materials (https://bit.ly/3CMM0TP) we
show several real-world examples of novice-made errors
that cannot be ﬁxed by any of these IDEs, but can be ﬁxed
by SYNSHINE. Finally, the entire source for our SYNSHINE,
including the VSCode extension, is made available anony-
mously at https://doi.org/10.5281/zenodo.4563241.

ACKNOWLEDGMENTS

This material is based upon work supported by the U.S.
National Science Foundation under Grant Nos. 1414172,
and 2107592. Any opinions, ﬁndings, and conclusions or
recommendations expressed in this material are those of
the author(s) and do not necessarily reﬂect the views of the
National Science Foundation. Ahmed was also supported
by UC Davis College of Engineering Dean’s Distinguished
Fellowship.

REFERENCES

[1]

J. C. Spohrer and E. Soloway, “Novice mistakes: Are the folk
wisdoms correct?” Communications of the ACM, vol. 29, no. 7, pp.
624–632, 1986.

[2] M. McCracken, V. Almstrum, D. Diaz, M. Guzdial, D. Hagan,
Y. B.-D. Kolikant, C. Laxer, L. Thomas, I. Utting, and T. Wilusz,
“A multi-national, multi-institutional study of assessment of pro-
gramming skills of ﬁrst-year cs students,” in Working group reports
from ITiCSE on Innovation and technology in computer science educa-
tion. ACM, 2001, pp. 125–180.

[4]

[3] E. Lahtinen, K. Ala-Mutka, and H.-M. J¨arvinen, “A study of the
difﬁculties of novice programmers,” Acm SIGCSE bulletin, vol. 37,
no. 3, pp. 14–18, 2005.
J. Jackson, M. Cobb, and C. Carver, “Identifying top java errors
for novice programmers,” in Proceedings Frontiers in Education 35th
annual conference.
IEEE, 2005, pp. T4C–T4C.
S. K. Kummerfeld and J. Kay, “The neglected battle ﬁelds of
syntax errors,” in Proceedings of the ﬁfth Australasian conference on
Computing education-Volume 20. Citeseer, 2003, pp. 105–111.
[6] P. Denny, A. Luxton-Reilly, and E. Tempero, “All syntax errors
are not equal,” in Proceedings of the 17th ACM annual conference
on Innovation and technology in computer science education, 2012, pp.
75–80.

[5]

[7] R. Gupta, S. Pal, A. Kanade, and S. Shevade, “Deepﬁx: Fixing
common c language errors by deep learning,” in Thirty-First AAAI
Conference on Artiﬁcial Intelligence, 2017.

[8] E. A. Santos, H. V. Campbell, D. Patel, A. Hindle, and J. N. Amaral,
“Syntax and sensibility: Using language models to detect and
correct syntax errors,” in 2018 IEEE 25th International Conference
on Software Analysis, Evolution and Reengineering (SANER).
IEEE,
2018, pp. 311–322.

[9] T. Ahmed, P. Devanbu, and V. Hellendoorn, “Learning lenient
parsing & typing via indirect supervision,” Empirical Software
Engineering Journal, (To Appear), 2021.

[10] M. Yasunaga and P. Liang, “Graph-based, self-supervised program
repair from diagnostic feedback,” in International Conference on
Machine Learning. PMLR, 2020, pp. 10 799–10 808.

[11] N. C. C. Brown, M. K ¨olling, D. McCall, and I. Utting, “Blackbox:
a large scale repository of novice programmers’ activity,” in Pro-
ceedings of the 45th ACM technical symposium on Computer science
education. ACM, 2014, pp. 223–228.

[12] R. Gupta, A. Kanade, and S. Shevade, “Deep reinforcement
learning for programming language correction,” arXiv preprint
arXiv:1801.10467, 2018.

[13] M. Yasunaga and P. Liang, “Break-it-ﬁx-it: Unsupervised learning

for program repair,” arXiv preprint arXiv:2106.06600, 2021.

[14] R. P. Medeiros, G. L. Ramalho, and T. P. Falc˜ao, “A systematic liter-
ature review on teaching and learning introductory programming
in higher education,” IEEE Transactions on Education, vol. 62, no. 2,
pp. 77–90, 2018.

[15] C. Watson and F. W. Li, “Failure rates in introductory program-
ming revisited,” in Proceedings of the 2014 conference on Innovation
& technology in computer science education, 2014, pp. 39–44.

[16] J. Bennedsen and M. E. Caspersen, “Failure rates in introductory
programming,” AcM SIGcSE Bulletin, vol. 39, no. 2, pp. 32–36,
2007.

[17] B. A. Becker, P. Denny, R. Pettit, D. Bouchard, D. J. Bouvier, B. Har-
rington, A. Kamil, A. Karkare, C. McDonald, P.-M. Osera et al.,
“Compiler error messages considered unhelpful: The landscape of
text-based programming error message research,” in Proceedings of
the working group reports on innovation and technology in computer
science education, 2019, pp. 177–210.

[18] T. Barik, J. Smith, K. Lubick, E. Holmes, J. Feng, E. Murphy-Hill,
and C. Parnin, “Do developers read compiler error messages?” in
2017 IEEE/ACM 39th International Conference on Software Engineer-
ing (ICSE).

IEEE, 2017, pp. 575–585.

[19] E. Kantorowitz and H. Laor, “Automatic generation of useful
syntax error messages,” Software: Practice and Experience, vol. 16,
no. 7, pp. 627–640, 1986.

[20] T. Schorsch, “Cap: an automated self-assessment tool to check
pascal programs for syntax, logic and style errors,” ACM SIGCSE
Bulletin, vol. 27, no. 1, pp. 168–172, 1995.

[21] M.-T. Luong, H. Pham, and C. D. Manning, “Effective approaches
to attention-based neural machine translation,” arXiv preprint
arXiv:1508.04025, 2015.

[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
in Advances in neural information processing systems, 2017, pp. 5998–
6008.

[23] R.-M. Karampatsis, H. Babii, R. Robbes, C. Sutton, and A. Janes,
“Big code != big vocabulary: Open-vocabulary models for source
code,” in International Conference on Software Engineering (ICSE),
2020.

[24] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-
training of deep bidirectional transformers for language under-
standing,” arXiv preprint arXiv:1810.04805, 2018.

[25] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,
M. Lewis, L. Zettlemoyer, and V. Stoyanov, “Roberta: A ro-
bustly optimized bert pretraining approach,” arXiv preprint
arXiv:1907.11692, 2019.

[26] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong,
L. Shou, B. Qin, T. Liu, D. Jiang et al., “Codebert: A pre-trained
model for programming and natural languages,” arXiv preprint
arXiv:2002.08155, 2020.

[27] A. Kanade, P. Maniatis, G. Balakrishnan, and K. Shi, “Learning and
evaluating contextual embedding of source code,” in International
Conference on Machine Learning. PMLR, 2020, pp. 5110–5121.
[28] E. Biswas, M. E. Karabulut, L. Pollock, and K. Vijay-Shanker,
“Achieving reliable sentiment analysis in the software engineer-
ing domain using bert,” in 2020 IEEE International Conference on
Software Maintenance and Evolution (ICSME).
IEEE, 2020, pp. 162–
173.

IEEE TRANSACTIONS ON SOFTWARE ENGINEERING

13

[29] T. Zhang, B. Xu, F. Thung, S. A. Haryono, D. Lo, and L. Jiang, “Sen-
timent analysis for software engineering: How far can pre-trained
transformer models go?” in 2020 IEEE International Conference on
Software Maintenance and Evolution (ICSME).
IEEE, 2020, pp. 70–
80.

[30] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco,
C. Clement, D. Drain, D. Jiang, D. Tang et al., “Codexglue: A
machine learning benchmark dataset for code understanding and
generation,” arXiv preprint arXiv:2102.04664, 2021.

[31] B. Roziere, M.-A. Lachaux, M. Szafraniec, and G. Lample, “Dobf: A
deobfuscation pre-training objective for programming languages,”
arXiv preprint arXiv:2102.07492, 2021.

[32] A. Mastropaolo, S. Scalabrino, N. Cooper, D. N. Palacio, D. Poshy-
vanyk, R. Oliveto, and G. Bavota, “Studying the usage of text-
to-text transfer transformer to support code-related tasks,” arXiv
preprint arXiv:2102.02017, 2021.

[33] K. Jesse, P. T. Devanbu, and T. Ahmed, “Learning type annotation:
is big data enough?” in Proceedings of the 29th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, 2021, pp. 1483–1486.

[34] T. Ahmed, P. Devanbu, and A. A. Sawant, “Learning to ﬁnd usage
of library functions in optimized binaries,” IEEE Transactions on
Software Engineering, 2021.

[35] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, L. Shujie, L. Zhou, N. Duan,
A. Svyatkovskiy, S. Fu et al., “Graphcodebert: Pre-training code
representations with data ﬂow,” in International Conference on
Learning Representations, 2020.

[36] W. Qi, Y. Gong, Y. Yan, C. Xu, B. Yao, B. Zhou, B. Cheng, D. Jiang,
J. Chen, R. Zhang et al., “Prophetnet-x: Large-scale pre-training
models for english, chinese, multi-lingual, dialog, and code gener-
ation,” arXiv preprint arXiv:2104.08006, 2021.

lation model for learning source code changes,” arXiv preprint
arXiv:1810.00314, 2018.

[50] F. Long and M. Rinard, “Automatic patch generation by learning
correct code,” in Proceedings of the 43rd Annual ACM SIGPLAN-
SIGACT Symposium on Principles of Programming Languages, 2016,
pp. 298–312.

[51] M. Pradel and K. Sen, “Deepbugs: A learning approach to name-
based bug detection,” Proceedings of the ACM on Programming
Languages, vol. 2, no. OOPSLA, pp. 1–25, 2018.

Touﬁque Ahmed is a Ph.D. student at UC Davis.
He received his B.Sc. and M.Sc. in Computer
Science and Engineering from Bangladesh Uni-
versity of Engineering and Technology (BUET)
in 2014 and 2016. He is currently working to-
ward the PhD degree with University of Cali-
fornia, Davis (UC Davis). His research interest
includes Software Engineering, the Naturalness
of Software, Machine Learning, and Sentiment
Analysis. He is the recipient of the ﬁve-year pres-
tigious Dean’s Distinguished Graduate Fellow-
ship (DDGF) offered by The Ofﬁce of graduate studies, The College of
Engineering, and The Graduate Group in Computer Science, UC Davis.

[37] W. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, “Uniﬁed
pre-training for program understanding and generation,” in
Proceedings
the North American
Chapter of the Association for Computational Linguistics: Human
Language Technologies. Online: Association for Computational
[Online]. Available:
Jun. 2021, pp. 2655–2668.
Linguistics,
https://www.aclweb.org/anthology/2021.naacl-main.211

the 2021 Conference

of

of

[38] Huggingface, “Huggingface transformers,” https://github.com/

huggingface/transformers.

[39] N. C. Brown and A. Altadmri, “Novice java programming mis-
takes: Large-scale data vs. educator beliefs,” ACM Transactions on
Computing Education (TOCE), vol. 17, no. 2, p. 7, 2017.

[40] Z. Chen, S. J. Kommrusch, M. Tufano, L.-N. Pouchet, D. Poshy-
vanyk, and M. Monperrus, “Sequencer: Sequence-to-sequence
learning for end-to-end program repair,” IEEE Transactions on
Software Engineering, 2019.

[41] G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. M. Rush, “Opennmt:
Open-source toolkit for neural machine translation,” arXiv preprint
arXiv:1701.02810, 2017.

[42] A. Mesbah, A. Rice, E. Johnston, N. Glorioso, and E. Aftandilian,

“Deepdelta: learning to repair compilation errors,” 2019.

[43] U. Z. Ahmed, P. Kumar, A. Karkare, P. Kar, and S. Gulwani, “Com-
pilation error repair: for the student programs, from the student
programs,” in Proceedings of the 40th International Conference on
Software Engineering: Software Engineering Education and Training,
2018, pp. 78–87.

[44] M. Tufano, J. Pantiuchina, C. Watson, G. Bavota, and D. Poshy-
vanyk, “On learning meaningful code changes via neural machine
translation,” in Proceedings of the 41st International Conference on
Software Engineering.

IEEE Press, 2019, pp. 25–36.

[45] Y. Li, S. Wang, and T. N. Nguyen, “Dlﬁx: Context-based code
transformation learning for automated program repair,” in 2020
42th International Conference on Software Engineering (ICSE), 2020.

[46] Y. Ding, B. Ray, P. Devanbu, and V. J. Hellendoorn, “Patching as
translation: the data and the metaphor,” 35th IEEE/ACM Interna-
tional Conference on Automated Software Engineering (ASE), 2020.
[47] T. Lutellier, H. V. Pham, L. Pang, Y. Li, M. Wei, and L. Tan,
“Coconut: combining context-aware neural translation models
using ensemble for program repair,” in Proceedings of the 29th ACM
SIGSOFT International Symposium on Software Testing and Analysis,
2020, pp. 101–114.

[48] C. Le Goues, T. Nguyen, S. Forrest, and W. Weimer, “Genprog: A
generic method for automatic software repair,” Ieee transactions on
software engineering, vol. 38, no. 1, pp. 54–72, 2011.

[49] S. Chakraborty, M. Allamanis, and B. Ray, “Tree2tree neural trans-

Noah Rose Ledesma is an undergraduate
Computer Science student at the UC Davis, and
a research assistant at the DECAL lab. He is
interested in machine learning, computer graph-
ics, autonomous vehicles, and other subjects.
Recently, Noah participated in an internship in
which he developed infrastructure for Google
Cloud’s artiﬁcial intelligence platform. He will be-
gin his career as a full-time software engineer
after he graduates in June 2022.

Premkumar Devanbu got his B. Tech from IIT
Madras and his Ph.D from Rutgers University.
He is currently Distinguished Professor of Com-
puter Science at UC Davis. His research inter-
ests include Empirical Software Engineering and
the applications of the Naturalness of Software.
He is an ACM Fellow.

