A PDD Decoder for Binary Linear Codes With
Neural Check Polytope Projection

Yi Wei, Ming-Min Zhao, Min-Jian Zhao and Ming Lei

1

0
2
0
2

n
u
J

1
1

]
P
S
.
s
s
e
e
[

1
v
0
4
2
6
0
.
6
0
0
2
:
v
i
X
r
a

Abstract— Linear Programming (LP) is an important decoding
technique for binary linear codes. However, the advantages of
LP decoding, such as low error ﬂoor and strong theoretical
guarantee, etc., come at the cost of high computational complexity
and poor performance at the low signal-to-noise ratio (SNR)
region. In this letter, we adopt the penalty dual decomposition
(PDD) framework and propose a PDD algorithm to address the
fundamental polytope based maximum likelihood (ML) decod-
ing problem. Furthermore, we propose to integrate machine
learning techniques into the most time-consuming part of the
PDD decoding algorithm, i.e., check polytope projection (CPP).
Inspired by the fact that a multi-layer perception (MLP) can
theoretically approximate any nonlinear mapping function, we
present a specially designed neural CPP (NCPP) algorithm to
decrease the decoding latency. Simulation results demonstrate
the effectiveness of the proposed algorithms.

Index Terms—Binary linear codes, check polytope projection,

LDPC, machine learning, MLP, neural network.

I. INTRODUCTION

Recently, the linear programming (LP) decoder, which is
based on LP relaxation of the maximum likelihood (ML)
decoding problem, has attracted increasing attention for de-
coding binary linear codes, especially for low density parity
check (LDPC) codes [1], [2]. Compared with the classical
belief propagation (BP) decoder, the LP decoder has stronger
theoretical guarantees on decoding performance and empiri-
cally is not observed to suffer from an error ﬂoor. However,
the above advantages of the LP decoder come at the cost
of two drawbacks, i.e., higher computational complexity and
poorer error-correcting performance when the signal-to-noise-
ratio (SNR) is low.

In order to overcome the above shortcomings, the work [3]
ﬁrst employed the alternating direction method of multipliers
(ADMM) to solve the ML decoding problem by exploiting
the fundamental polytope of the parity-check (PC) constraints.
In order to improve the error-rate performance in the low
SNR region, the work [4] added penalty terms to the linear
objective function to make pseudocodewords more costly and
the resulting decoder is known as the ADMM penalized
decoder. Afterwards, improvements over the ADMM penalized
decoder were achieved by modifying the penalty terms [5], [6].
Based on the cascaded decomposition method [7], the work
[8] adopted the penalty dual decomposition (PDD) framework
[9] and developed a PDD decoder, which was shown to
overperform the ADMM penalized decoder.

Y. Wei, M. M. Zhao, M. J. Zhao and M. Lei are with College of Infor-
mation Science and Electronic Engineering, Zhejiang University, Hangzhou
310027, China (email: {21731133, zmmblack, mjzhao, lm1029}@zju.edu.cn)
(Corresponding author: Ming-Min Zhao and Min-Jian Zhao.)

This work was supported in part by the National Key R&D Program of
China under Grant 2018YFB1802303, in part by the National Natural Science
Foundation of China under Grant 91938202, in part by the Zhejiang Provincial
Natural Science Foundation of China under Grant LQ20F010010, in part by
the Fundamental Research Funds for the Central Universities under Grant
2019QNA5011.

Focusing on the fundamental polytope based ML decoding
problem, intensive studies have been conducted to simplify
the check polytope projection (CPP) operations, which is the
most computationally intensive and time-consuming part in
the ADMM-based decoders [10]–[13]. Compared with the
original CPP algorithm which involves two sorting operations
[3], the work [10] employed a cut-search algorithm to remove
one sorting operation. In [11], the projection algorithm was
further simpliﬁed by transforming the CPP operation into
the projection onto a simplex. In [12], the authors presented
an iterative CPP (ICPP) algorithm which requires no sorting
operation and can substantially improve the decoding speed.
The work [14] revealed the recursive structure of the parity
polytope and presented an efﬁcient projection algorithm by
iteratively ﬁxing selected components of the projection.

In this letter, we propose a novel multi-layer perception
(MLP)-aided PDD decoder for binary linear codes. Different
from the PDD decoder in [8] which considers the minimum
polytope based LP formulation, we show that the PDD frame-
work can also be applied to the fundamental polytope based LP
formulation. The proposed decoder consists of two loops: in
the outer loop, we update the dual variables and certain penalty
parameter, while in the inner loop, we divide the primal
variables into several blocks and employ the block coordinate
descent (BCD) method to iteratively optimize each block
variable in closed-form. Furthermore, in order to simplify the
CPP operations in the proposed PDD decoder, we propose a
neural CPP (NCPP) algorithm, which is obtained by integrat-
ing a simple three-layer MLP (namely CPP-net) into the ICPP
algorithm in [12] to reduce the corresponding iteration number.
Simulation results demonstrate that the proposed PDD decoder
exhibits superior error-correcting performance and the NCPP
algorithm can reduce the latency signiﬁcantly.

II. PROBLEM FORMULATION

Consider a binary linear code C of length N speciﬁed
by an M × N PC matrix H. Let I , {1, · · · , N } and
J , {1, · · · , M } denote the sets of variables nodes and check
nodes of C, respectively. Suppose x = {xi = {0, 1}, i ∈ I}
is a codeword transmitted over a memoryless binary-input
symmetric-output channels, and y is the received signal. Then,
the received log-likelihood ratio (LLR) vector v ∈ RN ×1 can
be expressed as

vi = log

Pr(yi|xi = 0)
Pr(yi|xi = 1)
According to [4], the ML decoding problem can be formulated
as the following optimization problem:

, i ∈ I.

(1)

(cid:19)

(cid:18)

The work [1] proposed to relax problem (2) as follows:

min
x

vT x

s.t. x ∈ C.

min
x

vT x

s.t. x ∈ P :=

conv(Dj ),

j∈J
\

where conv(Dj ) is the convex hull of the codewords deﬁned
by the j-th row of the PC matrix H, and P is called the

(2)

(3)

 
 
 
 
 
 
fundamental polytope. Let dj denote the degree of check node
j, (3) can be expressed in a more compact form as [1]

min
x

vT x

s.t. Pj x ∈ PPdj , ∀j ∈ J ,

(4)

where Pj denotes a dj × N selection matrix which selects the
elements of x that participate in the j-th check equation. PPdj
is the PC polytope of dimension dj, which is deﬁned as the
convex hull of all even-parity binary vectors of length dj, i.e.,
PPdj = conv({e ∈ {0, 1}dj | kek1 is even}).

III. PDD DECODING ALGORITHM

In this section, we adopt the PDD framework to solve
problem (4) and develop a PDD decoding algorithm, where
the main idea is to introduce additional equality constraints to
handle the nontrivial constraints and the discrete variable x.
Firstly, we introduce auxiliary variables {zj ∈ Rdj ×1} to
equivalently transform constraint Pj x ∈ PPdj , ∀j ∈ J
into Pjx = zj, zj ∈ PPdj , ∀j ∈ J . Then, we relax
the binary variables {xi} to the interval [0, 1], and instead
to 0 or 1, we
of using penalty functions to enforce xi
propose to introduce auxiliary variables {ˆxi} which satisfy
ˆxi = xi and xi(ˆxi − 1) = 0. Let ˆx = [ˆx1, · · · , ˆxN ]T
M ]T , problem (4) can be equivalently
and z = [zT
formulated as

1 , · · · , zT

min
x, ˆx, z

vT x

s.t. Pj x = zj , zj ∈ PPdj , ∀j ∈ J ,

(5)

xi(ˆxi − 1) = 0, xi = ˆxi, 0 ≤ xi ≤ 1, ∀i ∈ I.

Next, we can see that the augmented Lagrangian problem of
(5) can be expressed as
min
x, ˆx, z

vT x + Pµm (x, ˆx, z)

(6)

s.t. zj ∈ PPdj , ∀j ∈ J , 0 ≤ xi ≤ 1, ∀i ∈ I

where

Pµm (x, ˆx, z) , µm
2

kPj x − zj +

yj
µm

k2

j∈J
X

(7)

2

,

(cid:17)

+

xi(ˆxi − 1) +

µm
2

i∈I (cid:16)
X

wi
µm

2 +

µm
2

xi − ˆxi +

ηi
µm

(cid:1)

(cid:0)

(cid:0)

(cid:1)
{yj}, {wi} and {ηi} denote the dual variables associated with
the constraints Pjx = zj , xi(ˆxi − 1) = 0 and xi = ˆxi,
respectively; µm represents the penalty parameter in the m-th
outer iteration. To this end, we propose to address problem
(6) by employing the BCD method in the inner iterations, and
update the dual variables and the penalty parameter {µm} in
the outer iterations.

In (6), it can be observed that the primal variables can be
divided into three blocks, i.e., x, ˆx and z. Therefore, the
BSUM iterations for problem (6) consists of the following
three steps (k denotes the inner iteration index):

1) Updating xk+1 given {ˆxk, zk}: The x subproblem is a
quadratic optimization problem with a simple constraint that
restricts its solution to lie in the interval [0,1], which can be
expressed as

min
x

vT x + Pµm (x, ˆx, z)

s.t. 0 ≤ xi ≤ 1, ∀i ∈ I.

(8)
We can observe that problem (8) can be naturally decomposed
into N subproblems, i.e.,
ak
i + bk
i,mx2
min
xi
i,m = µm
2 (cid:0)di + (ˆxk
i −1)+ηi −µm ˆxk

i,m = vi + 2αi +
i , and αi denotes the i-th element of the

i,mxi
i − 1)2 + 1(cid:1), bk

where ak
ωi(ˆxk

s.t. 0 ≤ xi ≤ 1,

(9)

2

2 Pj∈J PT

vector α = µm
j ). By resorting to the ﬁrst-
order optimality condition, the optimal solution of problem (9)
can be obtained by

− zk

j ( yj

µm

i = Π[0,1](−0.5bk
xk+1

(10)
where Π[0,1] denotes the Euclidean projection operation into
the interval [0, 1].

i,m/ak

i,m),

2) Updating zk+1 given {ˆxk, xk+1}: The optimization

problem of zj can be expressed as
k2

yj
µm
Similar to the ﬁrst step, the optimal solution of problem (11)
is given by

s.t. zj ∈ PPdj .

kPj xk − zj +

µm
2

min
zj

(11)

zk+1
j = ΠPPdj

(Pj xk +

yj
µm

),

(12)

where ΠPPdj

denotes the CPP operation.

3) Updating ˆxk+1 given {xk+1, zk+1}: The ˆx subprob-
lem can be written as the following unconstrained quadratic
optimization problem:

µm
2

i (ˆxi − 1) + wi
xk
µm

2 + µm

2 (xk

i − ˆxi + ηi
µm

(cid:0)

whose optimal solution can be easily obtained by

(cid:1)
(xk
(ωi − µmxk
i )xk
(cid:0)
Furthermore, the dual variables can be updated by

i )2 + 1
i − (ηi + µmxk
i )
(cid:1)

ˆxk+1
i = −

µm

4

.

(cid:1)

(cid:0)
= ym
= wm
= ηm

j ),

ym+1
j
wm+1
i
ηm+1
i

j + µm(Pj xm − zm
i + µm (xm
i + µm(xm
To summarize, the detailed steps of the PDD decoder are
listed in Algorithm 1, where c denotes a control parameter
that gradually increases the penalty parameter µm by a certain
amount during each outer iteration. According to [9],
the
proposed PDD decoder is guaranteed to converge.

i − 1)) ,
i ).

i (ˆxm
i − ˆxm

(15)

)2

,

(cid:17)
(13)

(14)

min
ˆx

i∈I
P

(cid:16)

IV. NCPP ALGORITHM

The projection ΠPPd (·) of a real-valued vector onto the
check polytope PPd in (12) is the most time-consuming part
in fundamental polytope based decoders, such as the proposed
PDD decoder and the ADMM-based decoders in [3] and [4],
etc. In this section, we propose a novel NCPP algorithm which
can further reduce the decoding latency of the ICPP algorithm
in [12]. The main idea of the proposed method is to reduce the
number of CPP iterations through a simple three-layer MLP
(namely CPP-net) with quantized parameters. In the following,
we ﬁrst give a brief review of the ICPP algorithm, and then the
structure of CPP-net is introduced followed by the proposed
NCPP algorithm, and ﬁnally we present the detailed process
of training sample generation and loss function design.

Algorithm 1 PDD Algorithm for Problem (4)

1: Initialize y0, {wi}0, {ηi}0 and z0 as all-zero vectors. Initialize all
elements in ˆx0 to 0.5. Set the initial penalty parameter µ0 and control
parameter c. Set m ← 0.

Update {xk+1, zk+1, ˆxk+1} by (10), (12) and (14). k ← k + 1.

Obtain z0 and ˆx0. Set k ← 0.
repeat

2: repeat
3:
4:
5:
6:
7:
8:
9:
10: until some convergence condition is met.

until some convergence condition is met.
xm ← xk+1, zm ← zk+1 and ˆxm ← ˆxk+1.
Update the dual variables by (15) and set µm+1 = cµm.
z0 ← zm, ˆx0 ← ˆxm, m ← m + 1.

1v
2v
3v
4v
5v
6v

input layer

Sin Act( )(cid:215)

Sin Act( )(cid:215)

Sin Act( )(cid:215)

Sin Act( )(cid:215)

s

hidden layer

output layer

Fig. 1: Structure of the proposed CPP-net when dj = 6.

A. Brief Review of the ICPP Algorithm

The ICPP algorithm proposed in [12] does not require
complex sorting operations, however the iterative nature of
the algorithm would increase the latency since it lies in each
iteration of the proposed PDD decoder and the ADMM-
based decoders. Generally, the ICPP algorithm to obtain
r = ΠPPd (v),v ∈ Rdj ×1 works as follows: 1) ﬁnd the
assistant hyperplane θ associated with v, which satisﬁes
θT v = p, p ∈ R (the value of p can be found by step 7 of
Algorithm 2, which will be introduced later) and determines
whether a point in the unit hypercube lies in the check polytope
or not, 2) iteratively derive the difference coefﬁcient s and
3) obtain the projection by r = Π[0,1]d (v − sθ). The vector
v′ = v − sθ can be interpreted as a shift of vector v in the
direction orthogonal to the assistant hyperplane θ, where the
amount of shift is determined by the value of s. In [12], an
estimate ˆs of s was iteratively obtained by ˆs = P ηk, where
ηk is the incremental projection coefﬁcient and ηkθ is how
much v is shifted at the k-th iteration. This iterative process
terminates when ηk falls below a certain threshold ǫ.

B. Structure of CPP-net

Since the assistant hyperplane θ is relatively easy to obtain,
the main difﬁculty of the CPP operation r = ΠPPd (v) lies in
the calculation of s, which can be viewed as the projection
of v to s, i.e., Πv→s(v). As a result, the CPP operation can
be alternatively expressed as r = Π[0,1]d (v − Πv→s(v)θ).
Motivated by the fact that a trained MLP with enough neurons
can approximate any nonlinear mappings, we introduce a
simple three-layer MLP to imitate the projection Πv→s(v) and
output an initial estimation of s for the purpose of reducing the
residual iteration number. Note that a classical MLP consists
of an input layer, an output layer and several hidden layers.
Each layer has multiple neurons, and each neuron can execute
an activation function on the weighted sum of the outputs
from the preceding layer. The activation function plays an
important role in neural networks and when it is non-linear,
a two-layer neural network can be proven to be a universal
function approximator.

The proposed CPP-net with dj inputs consists of three
layers, i.e., one input layer with dj neurons, one hidden layer
with ⌈dj/2⌉ neurons and one output layer with only one
neuron. In order to introduce non-linearity into the proposed
network, both hidden and output layers should contain acti-
vation functions. Note that the widely-used ReLU activation
function is not employed in the proposed CPP-net since it will
force almost half of the neurons to be silenced (veriﬁed by our
simulations) and limit the learning ability of CPP-net. Instead,
we propose a novel activation function constructed based on
the sin(·) function to improve the performance of CPP-net and
with low implementation cost. We refer to this function as
the SinAct(·) and its deﬁnition is given by

3

(a) E

/N
0

b

=5 dB

0.6

0.4

0.2

0

0.1

10

20

n
o
i
t
u
b
i
r
t
s
i
d
y
t
i
l
i
b
a
b
o
r
P

n
o
i
t
u
b
i
r
t
s
i
d
y
t
i
l
i
b
a
b
o
r
P

50

40

30
Iteration number
=2 dB
/N
(b) E
0

b

60

70

60

70

0.05

0

10

20

30
Iteration number

40

50

Fig. 2: Probability distribution of the number of iterations required
by the ICPP algorithm with different Eb/N0 (ǫ = 10−6).
sin( π

2 x) + 1

(cid:0)

(cid:1)

, −1 ≤ x ≤ 1
x< − 1
x>1

.

(16)

SinAct(x) =

1
2
0,
1,






For clarity, a simple example of the proposed CPP-net when
dj = 6 is depicted in Fig. 1.

Let yh ∈ R3×1 and ˜s ∈ R denote the outputs of the hidden
and output layers, respectively, then the data ﬂow of CPP-net
can be expressed as follows:

b yh + bb),

yh = SinAct(Wav + ba), ˜s = SinAct(wT

(17)
where Θ , {Wa, wb, ba, bb} denote the set of weights and
biases, which are the learnable parameters to be trained. There-
fore, the input-output mapping realized by the proposed CPP-
net is deﬁned by a chain of functions depending on Θ, i.e.,
˜s = FCPP-net(v; Θ) = SinAct(wb(SinAct(Wav + ba)) + bb).
In order to further reduce the computational complex-
ity of CPP-net, we propose to quantize the neural weights
a , wQ
{Wa, wb} obtained by training to {WQ
a ∈
[0, ±2k]⌈dj /2⌉×dj , wQ
b ∈ [0, ±2k]1×⌈dj /2⌉, k ∈ N and N
represents the set of natural numbers. Let wQ denote an
arbitrary element in {WQ
b }, we can see that the original
multiplication operations involved in the CPP-net can be
simpliﬁed as follows:

b }, where WQ

a , wQ

• If wQ = 0 or ±1, then no multiplication is required.
• If wQ = ±2k, k ∈ N \{0}, then the corresponding
multiplication operation can be replaced by the binary
shifting operation.

Considering that
the addition operations are simpler than
multiplications, we choose not to quantize the biases {ba, bb},
but instead ﬁnetune them with ﬁxed {WQ
b }. Note that
this can compensate the performance loss caused by the
quantization of {Wa, wb}, at least to certain extent.

a , wQ

C. NCPP algorithm

In this subsection, we present the proposed NCPP algorithm,
which is shown in Algorithm 2. We ﬁrst decide whether r
can be obtained within only one CPP iteration and if not,
we call the CPP-net to obtain an initial estimate ˜s of the
difference coefﬁcient s. Then, the output of the CPP-net is
fed to the subsequent CPP iterations to ensure that an accurate
CPP operation can be conducted even when ˜s is far from s.
Thus, the NCPP algorithm is expected to achieve the same
performance as the ICPP algorithm with lower complexity.
Note that the SinAct(·) function can be implemented as a
look-up-table and this will not degrade the error correcting
performance of the proposed decoder (ensured by steps 12-
14 in Algorithm 2). When the number of quantization bits is
large enough (e.g., larger than 3), the average iteration number
required by the NCPP algorithm is only slightly increased (less
than 0.5 in our simulations).

 
 
Algorithm 2 NCPP Algorithm

d (v).

i = −θ∗
i .

i∗ = arg mini |vi − 0.5|, θ∗

1: Intput: Vector v with dimension dj .
2: Output: r = ΠPP
3: θi = sgn(vi − 0.5), i = 1, · · · , d.
4: if |{i : θi = 1}| is even then
5:
6: end if
7: p = |{i : θi = 1}| − 1, u = Π[0,1]d (v), η = (θT u − p)/d.
8: if η < ǫ then
9:
10: else
11:
12:
13:
14:
15:
16: end if

˜s = FCPP-net(v), η0 = ˜s, k = 0.
repeat

until
r = u.

|ηk| < ǫ

r = u.

v = v − ηkθ, u = Π[0,1]d (v), k = k + 1, ηk = (θT u − p)/d.

D. Training Details

1) Training Sample Generation: Generally, an MLP is
trained to extract the underlying features from training samples
and learn the speciﬁc patterns to perform certain tasks, such
as classiﬁcation, clustering and forecasting, etc. Therefore, the
performance of the MLP depends critically on the quality of
the training data and in our case, not surprisingly, training with
training samples generated under different scenarios will lead
to performance differences over the same validation set.

Let (vp, ˆsp)P

p=1 denote the labeled training sample set with
size P , where vp and ˆsp represent the p-th feature and label,
respectively. More speciﬁcally, for the considered network,
vp is the input of the CPP operation, which is acquired
by collecting Pjx + yj
in (12) when running the PDD
µm
decoding algorithm, and the label ˆsp is the approximation of
sp, which is obtained by running the ICPP algorithm with
a predetermined iteration number Kp. Since the proposed
network aims to reduce the number of iterations required
by the ICPP algorithm, training samples obtained by using
different iteration numbers would have a critical impact on
the training results. In order to investigate the characteristic of
the iteration number, we illustrate its probability distribution
when Eb/N0 is set to 2 dB or 5 dB in Fig. 2, where the
threshold ǫ is ﬁxed to 10−6.1 we can observe that for both
cases, the proportion of Kp = 1 (i.e., the ICPP algorithm
converges within only one iteration) is larger than the others.
Since employing CPP-net
is unnecessary when Kp = 1,
the training samples obtained when Kp = 1 are useless for
network training and these instances should not be included
in the training sample set. In addition, considering that high
noise levels would prevent the proposed network from learning
the underlying mapping mechanism, the training samples with
Kp ≥ 2 are collected under a relatively high Eb/N0 (Eb/N0=5
dB is used in our simulations).

2) Loss Function: Loss function is used to measure the
differences between the network output and the true label,
and the performance of the network is heavily dependent on
it. In general, the loss function should be carefully deﬁned
according to the speciﬁc learning task. In the following, we
investigate the convergence property of the proposed
ﬁrst

4

k
ˆs

t
n
e
i
c
ﬃ
e
o
c

n
o
i
t
c
e
j
o
r
p

d
e
t
a
l

u
m
u
c
c
a

e
h
T

0.3

0.25

0.2

0.15

0.1

0.05

0

ICPP
˜sS initialization, NCPP
˜sL initialization, NCPP
The true diﬀerence coeﬃcient s

10

20

30

40

50

60

Iteration number k

Fig. 3: Convergence behavior of the NCPP algorithm with different
values of ˜s.

NCPP algorithm, based on which we present a novel loss
function that is able to accelerate the learning process.

In Fig. 3, we illustrate the typical convergence behaviors
of the proposed NCPP algorithm with different values of
˜s, where s denotes the true difference coefﬁcient, ˜sL and
˜sS denote two initial estimates of s with ˜sL>s, ˜sS<s and
˜sL − s = s − ˜sS>ǫ. Note that the CPP-net can be viewed as
a non-linear projector which is able to output an approximate
value of the difference coefﬁcient from the input v, therefore,
it is able to provide a good initial point for the ICPP algorithm.
The accumulated projection coefﬁcient up to iteration k, i.e.,
k
i=0 ηi, is regarded as the performance metric. Note
ˆsk = P
that the ICPP algorithm can be viewed as a special case of the
proposed NCPP algorithm with ˜s = 0. we can observe that
different values of ˜s lead to different numbers of iterations
with the same ǫ even when |˜sL − s| = |˜sS − s|, and taking
˜sL as the initial point results in a smaller iteration number.
Based on this observation, we design the loss function as
LCPP-net(Θ) = 1
2), where the
coefﬁcient κ is a weighting factor (hyperparameter) which
needs to be predeﬁned before training. We can see that the
proposed loss function consists of two terms, i.e., ˆsp − ˜sp
and ||ˆsp − ˜sp||2
2, ˆsp − ˜sp is designed such that a larger initial
estimate of s is preferred and ||ˆsp − ˜sp||2
2 is used to minimize
the difference between the network output and the label.

P
p=1(ˆsp − ˜sp + κ||ˆsp − ˜sp||2

P P

V. SIMULATION RESULT
In this section, computer simulations are carried out to
evaluate the error-correcting performance of the proposed PDD
decoder and the decoding latency of the NCPP algorithm. The
proposed network is implemented in Python using the Tensor-
Flow library with the Adam optimizer [15]. In the simulations,
we focus on additive white Gaussian noise channel with
binary phase shift keying (BPSK) modulation. The considered
binary linear codes are (96, 48) MacKay 96.33.964 LDPC
code C1, (575, 288) IEEE 802.16e LDPC code C2 and (2640,
1320) Margulis code C3 [16]. During the training process, we
collect 105 training samples and 104 validation samples with
Eb/N0 = 5 dB, Eb/N0 = 4.5 dB and Eb/N0 = 3 dB for C1,
C2 and C3 codes. The learning rate and the balance coefﬁcient
κ are set to 10−4 and 4.

We ﬁrst compare the BLER performance of the proposed
PDD decoder, the BP decoder (sum-product), the ADMM ℓ2
decoder in [4] and the PDD decoder in [8], as shown in Fig.
4.2 In all the curves, we collect at least 100 block errors for

1For the detailed simulation setup, please refer to Fig. 4 (a). Note that
Eb/N0=2 dB corresponds to the low SNR scenario, while Eb/N0=5 dB
denotes the high SNR scenario.

2 Note that for the considered codes, we have tested the ADMM penalized
decoders with many other penalty functions, and we ﬁnally chose the ADMM
ℓ2 decoder in terms of BLER performance.

100

10-1

10-2

R
E
L
B

10-3

10-4

10-5

10-6

(a) C1

PDD(ICPP)
PDD(NCPP)
BP
ADMM L2
PDD [8]

2

4

6

E

/N
0

b

100

10-1

10-2

10-3

R
E
L
B

10-4

10-5

10-6

10-7

10-8

1

(b) C2

PDD(ICPP)
PDD(NCPP)
BP
ADMM L2

100

10-1

10-2

R
E
L
B

10-3

10-4

10-5

10-6

3

4

10-7

1

2
/N
0

b

E

5

(c) C3

PDD(ICPP)
PDD(NCPP)
BP
ADMM L2

TABLE I: Iteration number comparison.

C1 (d = 6)
Ave
20.3675
11.0653

Wor
72
61

C2 (d = 6/7)
Wor
Ave
89
28.7205
73
15.7024

C4 (d = 8)
Ave
24.7334
13.5614

Wor
79
68

ICPP
NCPP

TABLE II: Computation complexity comparison.

C1

C2

C4

ICPP
NCPP

Muls
366.61
201.17

Adds
244.41
139.78

Muls
560.05
308.19

Adds
373.37
212.63

Muls
593.61
328.47

Adds
395.73
232.98

1.5
/N
0

b

E

2

2.5

VI. CONCLUSION

Fig. 4: BLER performance comparison of C1, C2 and C3 codes.

all data points. It can be observed that our proposed PDD
decoder shows better BLER performance at both low and high
SNR regions for C1 code. For longer LDPC codes, i.e., the C2
and C3 codes, the proposed PDD decoder achieves a similar
performance as the other counterparts when the SNR is low
and outperforms them when Eb/N0 ≥ 2 dB and Eb/N0 ≥ 1.4
dB for C2 and C3 codes, respectively. Speciﬁcally, 0.3 dB,
0.1 dB and 0.08 dB performance gains over the ADMM
ℓ2 decoder can be achieved at BLER=10−4 for C1, C2 and
C3 codes, respectively. Besides, although the proposed PDD
decoder achieves a similar BLER performance as that in [8],
it requires less auxiliary variables and thus potentially leads
to lower complexity.

Then, in TABLE I, we provide the iteration numbers re-
quired by the ICPP algorithm [12] and Algorithm 2 when
decoding C1 (d = 6), C2 (d = 6 or 7) codes and (128, 64)
CCSDS code C4 (d = 8) [16] with Eb/N0 = 3 dB and
ǫ = 10−6. Since the average (Ave) and worst case (Wor)
iteration numbers required by the CPP operation both affect
the decoding latency and throughput, we choose them as the
performance metrics. It can be seen from TABLE I that the
proposed CPP-net can reduce both the average and worst
case iteration numbers and in particular, the average iteration
number is reduced by nearly half.

Finally, we provide a computational complexity analysis of
the ICPP and NCPP algorithms, which is based on the numbers
of multiplications (Muls) and additions (Adds) required by the
CPP operation. For simplicity, we take C1 code as an example
and the analysis for C2 code can be similarly conducted. Note
that the complexity of one CPP iteration (step 13 in Algorithm
2) involves: 1) updating v, which requires d Muls and d Adds;
2) calculating η needs 2d Muls and d Adds. For C1 code, we
list the quantized parameters {WQ
b } of the CPP-net as
follows:

a , wQ

WQ

a =

0
0
1

"

0
0
1 −1

0 −2
0 −2
0

0
0
0

T

0
0
0#

, wQ

b = [−1, 1, 0]T .

(18)

Therefore, the complexity of one forward pass of the CPP-
net can be expressed as 2 Muls and 7 Adds. Based on the
average iteration number in TABLE I, the average numbers of
Adds and Muls required by the ICPP and NCPP algorithms are
listed as TABLE II. Given the fact that the CPP operations are
needed in each iteration of the proposed PDD decoder or the
ADMM ℓ2 decoder, employing Algorithm 2 is able to reduce
the computational complexity and decoding latency of these
decoders signiﬁcantly.

In this work, we presented a novel PDD decoder with for
binary linear codes. We showed that other than the minimum
polytope based LP problem, the PDD framework can also be
utilized to address the fundamental polytope based LP decod-
ing problem. Furthermore, a NCPP algorithm was proposed to
reduce the iteration number required by the ICPP algorithm,
and it is applicable to all ADMM or PDD based decoders that
involve the CPP operations. Simulation results demonstrated
the superior performance of the proposed PDD decoder and
the effectiveness of the NCPP algorithm for complexity and
latency reduction.

REFERENCES

[1] J. Feldman, M. J. Wainwright, and D. R. Karger, “Using linear program-
ming to decode binary linear codes,” IEEE Trans. Inf. Theory, vol. 51,
no. 3, pp. 954–972, Mar. 2005.

[2] M. Helmling, S. Ruzika, and A. Tanatmis, “Mathematical programming
decoding of binary linear codes: Theory and algorithms,” IEEE Trans.
Inf. Theory, vol. 58, no. 7, pp. 4753–4769, Jul. 2012.

[3] S. Barman, X. Liu, S. C. Draper, and B. Recht, “Decomposition methods
for large scale LP decoding,” IEEE Trans. Inf. Theory, vol. 59, no. 12,
pp. 7870–7886, Dec. 2013.

[4] X. Liu and S. C. Draper, “The ADMM penalized decoder for LDPC
codes,” IEEE Trans. Inf. Theory, vol. 62, no. 6, pp. 2966–2984, Jun.
2016.

[5] B. Wang, J. Mu, X. Jiao, and Z. Wang, “Improved penalty functions
of ADMM penalized decoder for LDPC codes,” IEEE Commun. Lett.,
vol. 21, no. 2, pp. 234–237, Feb. 2017.

[6] X. Jiao, H. Wei, J. Mu, and C. Chen, “Improved ADMM penalized
decoder for irregular low-density parity-check codes,” IEEE Commun.
Lett., vol. 19, no. 6, pp. 913–916, Jun. 2015.

[7] K. Yang, X. Wang, and J. Feldman, “A new linear programming
approach to decoding linear block codes,” IEEE Trans. Inf. Theory,
vol. 54, no. 3, pp. 1061–1072, Mar. 2008.

[8] M. M. Zhao, Q. Shi, Y. Cai, M. Zhao, and Q. Yu, “Decoding binary
linear codes using penalty dual decomposition method,” IEEE Commun.
Lett., vol. 23, no. 6, pp. 958–962, Jun. 2019.

[9] Q. Shi, M. Hong, X. Fu, and T.-H. Chang, “Penalty dual decomposition
method for nonsmooth nonconvex optimization,” arXiv preprint, 2017.
[Online]. Available: https://arxiv.org/abs/1712.04767v1

[10] X. Zhang and P. H. Siegel, “Efﬁcient iterative LP decoding of LDPC
codes with alternating direction method of multipliers,” in IEEE ISIT,
Jul. 2013, pp. 1501–1505.

[11] G. Zhang, R. Heusdens, and W. B. Kleijn, “Large scale LP decoding
with low complexity,” IEEE Commun. Lett., vol. 17, no. 11, Nov. 2013.
[12] H. Wei and A. H. Banihashemi, “An iterative check polytope projection
algorithm for ADMM-based LP decoding of LDPC codes,” IEEE
Commun. Lett., vol. 22, no. 1, pp. 29–32, Jan. 2018.

[13] H. Wei, X. Jiao, and J. Mu, “Reduced-complexity linear programming
decoding based on ADMM for LDPC codes,” IEEE Commun. Lett.,
vol. 19, no. 6, pp. 909–912, Jun. 2015.

[14] F. Gensheimer, T. Dietz, K. Kraft, S. Ruzika, and N. Wehn, “A low-
complexity projection algorithm for ADMM-based LP decoding,” arXiv:
1901.03240v1, Jan. 2019.

[15] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

in ICLR, 2015.

[16] M. Helmling, S. Scholl, F. Gensheimer, T. Dietz, K. Kraft, S. Ruzika,
and N. Wehn, “Database of channel codes and ML simulation results,”
www.uni-kl.de/channel-codes, 2017.

