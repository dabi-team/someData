Performance portability through machine learning guided kernel selection in SYCL
libraries

John Lawson

Codeplay Software Ltd.

0
2
0
2

g
u
A
0
3

]
F
P
.
s
c
[

1
v
5
4
1
3
1
.
8
0
0
2
:
v
i
X
r
a

Abstract

Automatically tuning parallel compute kernels allows libraries and frameworks to achieve performance on a wide
range of hardware, however these techniques are typically focused on ﬁnding optimal kernel parameters for particular
input sizes and parameters. General purpose compute libraries must be able to cater to all inputs and parameters
provided by a user, and so these techniques are of limited use. Additionally parallel programming frameworks such as
SYCL require that the kernels be deployed in a binary format embedded within the library. As such it is impractical to
deploy a large number of possible kernel conﬁgurations without inﬂating the library size.

Machine learning methods can be used to mitigate against both of these problems and provide performance for
general purpose routines with a limited number of kernel conﬁgurations. We show that unsupervised clustering methods
can be used to select a subset of the possible kernels that should be deployed and that simple classiﬁcation methods can
be trained to select from these kernels at runtime to give good performance. As these techniques are fully automated,
relying only on benchmark data, the tuning process for new hardware or problems does not require any developer eﬀort
or expertise.

Keywords: Auto-tuning, SYCL, GPGPU, Machine learning, Performance portability

1. Introduction

Auto-tuning has been widely studied as a technique
to allow libraries to obtain portable performance across
a range of devices by utilising parameterized kernels and
selecting the right parameters to match the compute ca-
pabilities of the diﬀerent devices.

For frameworks like OpenCL that provide their ker-
nels as source code this works especially well. The source
code can be conﬁgured using the preprocessor to handle
any number of possible parameter conﬁgurations. Other
parallel programming frameworks like CUDA and SYCL
provide the kernels in a compiled binary format, and so
each set of parameters requires a new binary blob contain-
ing the kernel compiled with those parameters. Supporting
many diﬀerent kernel instantiations in these libraries adds
complexity and a cost in terms of library size and build
times.

Standard auto-tuning techniques sample the kernel pa-
rameter space in order to determine the set of parameters
that give the best performance for a given problem. This
process aims to provide the absolute best performance for
that particular set of input sizes and problem parameters
and so is especially eﬀective when these inputs and prob-
lem parameters are constant. On the other hand the auto-
tuning must be done every time the inputs or parameters
change, which is typically a costly process.

Email address: john@codeplay.com (John Lawson)

As a result of this it is diﬃcult to use auto-tuning to
provide general purpose libraries that can cater to all pos-
sible inputs. We look at using unsupervised machine learn-
ing techniques to explore the space of kernel parameters
and select a subset of kernels that can be deployed in a
library to provide close to optimal performance on a wide
range of possible inputs. These clustering techniques allow
the library to achieve over 90% of the optimal performance
while limiting the library to include as few as four kernels.
We also consider how well machine learning classiﬁ-
cation methods can select from these kernels at runtime.
Decision trees are an eﬀective way to do this, preserving a
large proportion of the possible performance while being
easy to integrate into the library.

When combined, these automated approaches are an
eﬀective way to extract performance from parameterized
kernels suited for a wide range of possible inputs, and this
performance can be achieved with very little developer ef-
fort. These approaches allow a simple matrix multiplica-
tion kernel to provide performance similar to or even much
better than hand optimized BLAS implementations on a
range of hardware. We demonstrate this by comparing the
inference time of VGG16, a popular image classiﬁcation
network implemented using SYCL-DNN, an accelerated
neural network library, when using diﬀerent matrix mul-
tiplication routines. The tuned simple kernel is competi-
tive on desktop GPUs and performs better than optimized
BLAS libraries on integrated GPUs and mobile GPUs.

1

 
 
 
 
 
 
2. Background and related work

2.2. Auto tuning

2.1. SYCL and OpenCL

OpenCL [1] is a heterogeneous programming frame-
work developed originally by Apple and now maintained
by the Khronos Group. It is an open standard designed
to provide a cross platform way to program a wide range
of hardware from GPUs to FPGAs. OpenCL allows de-
velopers to write compute kernels in a subset of C, which
are embedded within applications and libraries as strings
of source code. This source code is then just-in-time (JIT)
compiled to match the target device at runtime.

By using JIT compilation, OpenCL allows developers
to use the preprocessor to inject constants and types into
generic kernels. Diﬀerent versions of the same kernels can
be compiled multiple times to match the diﬀerent inputs
and sizes at runtime while using the same source code.
As the same source code can be used for all the diﬀerent
parameter values, there is no cost to using this technique
beyond the additional compilation time to compile each
kernel.

SYCL [2] is a more recent open standard from the
Khronos Group, introduced in 2014 aiming to remove the
boilerplate and complexity of lower level heterogeneous
programming frameworks like OpenCL. Using SYCL a de-
veloper can write compute kernels using standard C++ as
well as make use of the strong C++ type system to track
data dependencies and manage data movement between
host and device.

OpenCL requires hardware vendors to package a C
compiler with their device drivers, but to support SYCL it
would be more challenging to include a full C++ compiler
and JIT compile heavily templated C++ kernels. Instead,
SYCL adopts a two stage compilation approach, where the
kernels are initially compiled to an intermediate represen-
tation (IR) that is bundled with the library or application
binary. This IR blob is then passed to the OpenCL JIT
compiler at runtime, signiﬁcantly reducing the amount of
work required to compile the kernels at runtime.

The downside of shipping kernels in a binary format
is that these now include the kernel parameters, and so a
diﬀerent binary blob is required for each instantiation of
the kernel.

There are many existing OpenCL and SYCL acceler-
ated compute libraries,
including the BLAS implemen-
tations clBLAS [3], CLBlast [4] and SYCL-BLAS [5, 6].
Each of these libraries is tuned for their target hardware
to some extent. These libraries either provide a set of hard-
coded kernel parameters for given inputs chosen by hand to
try to give good performance, or include more automated
approaches that include benchmark scripts that generate
these sets of parameters that can then be compiled into the
library. These automated approaches currently use heuris-
tics and limited numbers of kernel benchmarks to try to
establish which parameters to use.

There are many auto-tuning techniques that have been
widely studied. General purpose tuning frameworks such
as clTune [7] and Kernel Tuner [8] provide easy to use tun-
ing for compute kernels, tuning OpenCL, CUDA and other
kernels. The techniques used by these frameworks combine
kernel benchmarks to measure the performance of a given
of parameters, and a parameter search algorithm to selec-
tively sample from the parameter space while maximising
performance.

Despite the sophistication of these parameter search
algorithms, such auto-tuning systems can be expensive in
terms of power and time usage, and must be run for each
required set of inputs. This can be partially mitigated us-
ing machine learning to learn a model of the kernel perfor-
mance and using this model to predict reasonable parame-
ters to start the auto-tuning search. Techniques discussed
in [9] and in [10] replace the parameter search algorithms
with machine learning based approaches. A random sam-
ple of kernel conﬁgurations are benchmarked, and these
timings used to train a model that predicts the timings of
all other kernel conﬁgurations, allowing the optimal con-
ﬁguration to be directly chosen from the predicted times.
Other uses of machine learning in automated kernel op-
timization include predicting whether an operation would
be computed faster on CPU or GPU [11, 12], and whether
a kernel would perform better when manually caching data
in local memory [13].

Auto-tuning has been used to provide portable per-
formance on diﬀerent hardware for a variety of diﬀerent
computational tasks, including convolutions [14], matrix
multiplication [15, 4], FFTs [16] and stencils [17, 18].

A diﬀerent approach to auto-tuning is to explore the
diﬀerent kernel parameters during the end program run-
time. This dynamic approach of auto-tuning allows the
best available conﬁguration to be found if the same prob-
lem is computed multiple times. This is used in the Ten-
sorFlow [19] and MXNet [20] machine learning frameworks
with the cuDNN [21] launcher options. While this does not
provide as ﬁne grained control as the kernel based auto-
tuning techniques, it does allow coarse grained decisions
about the best algorithm or approach to take for given
problems on ﬁxed hardware.

3. A matrix multiply case study

Matrix multiplications are an integral part of modern
deep learning and many other domains, so having accel-
erated routines optimized for particular hardware gives a
signiﬁcant impact on the performance of these computa-
tions. The kernels that calculate a matrix multiplication
have been the target of many previous auto-tuning tech-
niques as the kernels can easily be written to make use
of many parameters. These kernels are less complicated
compared to other stencil or convolutional kernels, while
having enough scope for loop transformations, tiling and

2

caching memory accesses that they are good targets for
tuning.

In [22] we introduced a matrix multiply case study
using the parameterized kernels provided by the SYCL-
DNN [23] library. This paper continues the study of auto-
tuning these kernels, expanding the number of benchmarks
and the devices targeted by the tuning techniques.

Each work item in this matrix multiplication kernel
computes a small tile of the output. For integers R, A, C,
it loads an R×A tile from the left hand input and an A×C
tile from the right hand input, which are accumulated into
an R × C output tile. These tile sizes are compile time
constants that also correspond to the vector sizes used to
load the values from memory, so the possible values are 1,
2, 4 and 8. These three parameters give 64 diﬀerent kernel
conﬁgurations.

In addition to the compile time kernel constants we
considered the eﬀects of diﬀerent work group sizes on per-
formance, using a combination of 1, 8, 16, 32, 64 and 128.
As the total work group size for a kernel is limited by the
device drivers, we only used the following pairings: (1, 64),
(1, 128), (8, 8), (8, 16), (8, 32), (16, 8), (16, 16), (32, 8),
(64, 1) and (128, 1); giving a total of 640 possible conﬁg-
urations to select from.

To measure the eﬀects of the diﬀerent kernel param-
eters and work groups sizes we ran a number of bench-
marks on two platforms. With only 640 possible conﬁgu-
rations it is feasible to test the performance of every con-
ﬁguration. This allows us to evaluate whether the kernel
selection techniques manage to choose the best perform-
ing kernel and avoids any confounding factors that may
arise when combining these techniques with standard ker-
nel auto-tuning techniques. As auto-tuning will typically
try to selectively search the kernel parameter space it will
only end up sampling the performance of some kernel con-
ﬁgurations and so would immediately discount some ker-
nels from being chosen.

Fully connected and convolutional layers in machine
learning models can be computed using matrix multiplica-
tions. The SYCL-DNN library is designed to provide ac-
celerated routines for machine learning models so matrix
sizes derived in this way are representative of the typical
workloads for the library. The benchmarks use the ma-
trix sizes from three popular neural networks: VGG [24],
ResNet [25] and MobileNet [26]. Overall these gave 300
diﬀerent sets of sizes for the input matrices of the compu-
tations.

3.1. Data collection

The benchmarking framework used to collect the data
ran a small number of warmup iterations to ensure the de-
vices were running at optimal clock speeds and that the
kernels were compiled. The measurement collected was
the total time for a number of iterations of kernel execu-
tion, giving an overall mean time for each kernel execution.
The actual number of iterations varied depending on the

Figure 1: The performance of all diﬀerent kernel conﬁgurations for
three sets of input sizes on the AMD R9 Nano GPU, with varying
matrix sizes from square to rectangular. Multiplying small reason-
ably square matrices performs best overall and favors large tile sizes,
while tall skinny matrices perform poorly in all conﬁgurations.

time of execution, aiming for each benchmark to run for
around 1 second in total. Between each benchmark run
the framework paused for a short amount of time to help
reduce any thermal throttling, and device temperatures
were monitored during the benchmarking process to en-
sure there was no throttling.

The devices used to run the benchmarks were:

• An AMD R9 Nano GPU (driver v2482.3).

• An Intel i7-6700K CPU (driver v18.1.0.0920).

We used SYCL on top of OpenCL to target these devices,
providing the kernels as SPIR.

3.2. The dataset

The matrix sizes in the dataset vary, with some being
very large and others small, some fairly square with a large
batch size and others very tall and skinny. These diﬀerent
sizes provide diﬀerent performance characteristics for the
kernels on the hardware. For example the tall and skinny
matrices lead to very few threads being used in the mul-
tiplication and so for large compute devices like the AMD
GPU a lot of the compute capacity goes unused. Even us-
ing auto-tuning to select the best kernel will not solve this
problem, and really a separate kernel should be used that
is designed to utilize all the hardware for these sorts of
matrix inputs. This is beyond the scope of the paper, but
should such a kernel be available then the type of kernel
could be considered as another parameter that has to be
selected by a tuning system.

As an example of this, on the R9 Nano the best per-
forming conﬁguration (tiles (8, 4, 4), work-group (16, 16)
for m=512, k=784, n=512, batch=16) achieves 3160 gi-
gaﬂops per second, while the worst conﬁguration (tiles
(1, 8, 1), work-group (8, 8) for m=32, k=12321, n=27,
batch=1) only achieves 13 Gﬂops/sec. The best conﬁgu-
rations for the small cases are the ones that use the most

3

Parameter configuration0.00.51.01.52.02.53.0Teraflops/secm=512, k=4608, n=784m=128, k=12544, n=128m=512, k=784, n=512(a) AMD R9 Nano

(a) AMD R9 Nano

(b) Intel i7-6700K CPU

(b) Intel i7-6700K CPU

Figure 2: The number of times a conﬁguration of kernel parameters
achieves optimal performance in the dataset. For the AMD GPU,
one conﬁguration is best in 39 cases, but 80 distinct conﬁgurations
are best in at least one case. For the Intel CPU the top three conﬁg-
urations are best in 35, 28 and 25 cases respectively and 68 are best
in at least one case.

Figure 3: The percentage of the variance of the dataset accounted for
by each PCA component. For the AMD GPU over 80% of the vari-
ance is accounted for in the 4 main components, 90% is accounted
for in 7 components, and 95% in 14. For the Intel CPU 4 compo-
nents account for 80% of the variance, 6 components for 90% and 11
components for 95%.

threads and so achieve the highest utilisation of the GPU,
while the best conﬁgurations for large problems are the
ones that reuse the most data without spilling registers.
As the numbers of threads and numbers of registers are
device speciﬁc, these are the things that an automated
kernel deployment system would have to implicitly learn
from the dataset.

Figure 1 shows the performance for three diﬀerent sets
of input matrix sizes. The more square matrices (m=512,
k=784, n=512) allowed the kernels to perform best, but it
only achieved optimal performance in a very small number
of kernel conﬁgurations. In this case, of the 640 possible
conﬁgurations only 55 achieved over 2 teraﬂops/sec and
only 7 of those achieved over 3 teraﬂops/sec. This high-
lights the importance of tuning the kernel parameters and
ensuring that the best parameters are available in a library.
The second results in Figure 1 from a more rectangular
set of input matrices (m=512, k=4608, n=784) have three
kernel conﬁgurations that achieve over 2 teraﬂops/sec. All
three of these kernel conﬁgurations achieve over 3 ter-
aﬂops/sec with the square input sizes, but the best per-
forming conﬁguration for the square inputs achieves less
than 1.4 teraﬂops/sec for the rectangular input sizes. The
third set of results correspond to an input set with a very
large number of elements to accumulate, and as discussed
above the kernel used is not optimized for these cases and
so performs poorly overall.

The challenge faced by an automated kernel selection
program is that many diﬀerent conﬁgurations obtain the
best performance for diﬀerent matrix sizes. Figure 2 shows
that while there are a small number of conﬁgurations that
perform best in a large number of cases, there is a long
tail where many other conﬁgurations also perform best in

at least one of the benchmarks. This long tail illustrates
the problem with pruning the number of conﬁgurations
required to deploy within a library, and suggests that any
such pruning will result in some loss of performance. The
goal of this paper is to determine whether an automated
solution can minimize this loss in performance.

The dataset and the corresponding code is available
online [27]. The machine learning routines were provided
by scikit-learn [28].

3.3. Determining the target number of conﬁgurations

As discussed in Section 2.1 a SYCL library cannot de-
ploy an unlimited number of kernels, as they are embedded
within the library as binary blobs. As such the kernels that
should be deployed must be carefully selected to provide
as much performance as possible. The number of kernels
to deploy could be determined through trial and error by
investigating the achievable performance of diﬀerent num-
bers of kernels. A more tractable approach would be to
explore the variance within the dataset and use that to es-
timate how many kernels may encapsulate that variance.
Principal component analysis (PCA) [29, 30] ﬁnds a
new coordinate system for the dataset that concentrates
the variance into as few dimensions as possible. In this way
these principal dimensions contain the most distinguishing
information about the dataset. Figure 3 shows the amount
of total variance in the dataset that is accounted for by
each of the components identiﬁed by PCA. This highlights
that the data is fairly structured and that the majority
of the variance is encapsulated within a small number of
components.

As PCA shows that most of the dataset’s variance can
be encapsulated in less than 15 components we study how

4

Parameter configuration010203040CountParameter configuration010203040CountPCA component0.00.20.4Varianceas percentagePCA component0.00.20.4Varianceas percentagemuch performance can be encapsulated when providing
at most 15 kernels. We compare the performance that
is achievable when the number of kernels that would be
deployed in a library varies between 4 and 15.

3.4. Normalization method

For each set of matrix sizes, the benchmarks measured
the performance as gigaﬂops per second for each kernel.
This gives 640 ﬂoating point values describing the perfor-
mance, ranging from 0 to the maximum Gﬂops/sec of the
device.

When comparing the performance of kernels for ﬁxed
matrix sizes, it is helpful to consider the comparative per-
formance of the kernels instead of the raw ﬂops/s achieved.
By normalizing the data to only show the comparative per-
formance, the data is easier for an automated system to
understand. Such a normalization technique should map
the performance to a value between 0 and 1, with the best
performing kernels valued at or close to 1, while poor per-
forming kernels have a value closer to 0.

In the original work, the only normalization technique
considered was to scale the performance results relative to
the performance of the kernel that performed best. The
normalized value is obtained by dividing the achieved per-
formance by the maximal performance for a ﬁxed input.
This provides a uniform mapping that preserves the rela-
tive performance between all kernels.

As the kernel selection process should infer more from
the better performing kernels than the worst performing
kernels, and hopefully never tries to select kernels that give
mediocre performance, we can normalize the data to only
preserve the kernels that perform well. We study three
diﬀerent approaches of doing this.

The ﬁrst approach is to use a raw cutoﬀ point, so that
all results under a certain threshold are clamped to 0. In
the results below we consider a cutoﬀ value at 90% of the
peak performance, so all results that obtain less than 90%
of the optimal performance for each set of inputs is set to
0. This introduces sparsity in the data but does not change
any non-zero values, so they range between 0.9 and 1.

An extension of this is to rescale the normalized data
after clamping the poorly performing kernels. This ensures
that the values make full use of the 0 to 1 range but may
encourage the models to discard good performing kernels
that it thinks actually perform poorly. In the discussion
below we refer to this as the standard cutoﬀ normalization
technique (as opposed to the raw cutoﬀ).

A ﬁnal approach studied is to use a modiﬁed sigmoid
function to map the scaled values, with many of the less
well performing kernels mapped to 0. The sigmoid func-
tion f (x) = (1 + exp(50 ∗ (0.85 − x)))−1 was constructed
to map 85% performance to 0.5 with all values less than
80% mapped to less than 0.1.

Figure 4 shows the eﬀects these normalization tech-
niques have on the best performing set of inputs for the
AMD GPU, with the raw performance shown in Figure 1.

Figure 4: Comparison of diﬀerent data normalization techniques for
the best performing set of input sizes for the AMD GPU.

As the normalization techniques all clamp low perform-
ing kernel conﬁgurations to zero, only the conﬁgurations
achieving over 75% of the performance of the best conﬁg-
uration are shown.

4. Kernel selection

The techniques in this paper to deploy kernels in SYCL
libraries is made up of two steps. First the kernel conﬁg-
urations should be selected, and then a simple model is
constructed to choose which of these conﬁgurations to use
at runtime for a given problem. As SYCL kernels are
embedded into the library as binaries it is impractical to
include a large number of kernels. In order to balance per-
formance and binary size the number of kernels must be
pruned to those that give the best performance on a range
of diﬀerent problems.

The initial selection of kernels is done using unsuper-
vised clustering of the dataset. For a given set of ma-
trix sizes the dataset provides performance information for
each of the 640 kernel conﬁgurations. This performance in-
formation can be represented as a point in 640-dimensional
space, though as the raw times vary between matrix sizes
it is useful to normalize these coordinates.

Matrix sizes that have similar performance character-
istics will naturally end up with similar coordinates, and
so clustering techniques can be used to group these to-
gether. By considering these clusters of similarly perform-
ing matrix sizes we can extract which kernels give the best
performance.

4.1. Clustering techniques

There are many unsupervised machine learning clus-
tering techniques available which try to extract meaning
directly from the data. These each have diﬀerent behaviors
and consider diﬀerent aspects of the data, so may extract
widely varying sets of kernels.

4.1.1. K-means clustering

A relatively simple clustering method is k-means clus-
tering, which is an iterative method to ﬁnd k centroids that
minimize the distance from each points in the dataset to
their closest centroid. This method is eﬀective when the
clusters have shapes that are close to the unit ball in the
coordinate space, however if the cluster shapes are less

5

Kernel configuration0.00.51.0Normalizedperformancescaledrawcutoffsigmoidcutoffregular or intertwined the method will struggle to sepa-
rate the clusters.

4.1.2. PCA and k-means clustering

To help get around this, the coordinate space of the
dataset can be transformed to help separate the clusters.
One approach to do this is using Principal Component
Analysis to reduce the dimensionality of the dataset and
concentrate the variance of the dataset by making use of
the full range of values in each of the new dimensions, then
using k-means clustering on this transformed data.

4.1.3. Spectral clustering

Another similar approach is to use a spectral trans-
formation before using k-means clustering. A similarity
graph of the coordinates in the dataset can be represented
as an adjacency matrix. The eigenvectors of the Lapla-
cian of this matrix provide new coordinates that can be
clustered using k-means.

4.1.4. HDBScan

Density based methods can also be used to cluster data,
which use the density of the data to establish the bound-
aries between clusters. HDBScan [31, 32] is an example of
such a clustering method that uses a hierachical tree struc-
ture to construct the clusters and provide better estimates
of outlying data.

Unlike the other clustering methods, HDBScan does
not provide a parameter for the number of target clusters,
rather providing however many clusters it ﬁnds based on
its other hyperparameters. In order to limit the numbers
of clusters we compute the numbers of clusters for a sweep
of the hyperparameters and in the following use whichever
values gave the correct number of clusters.

4.1.5. Decision tree

While not a clustering method, decision trees can be
used to choose a subset of a dataset by artiﬁcially limit-
ing the number of leaf nodes in the tree. A decision tree
can be trained as a regression solver that maps the input
matrix sizes to the vector of performance data. Unlike
the clustering methods, this takes into account the ma-
trix sizes rather than just the performance data. Each leaf
node then ends up being a performance vector which is an
approximate representative of the performance vector for
all input sizes that end up at that node in the tree.

4.2. Selecting the kernels to deploy

To compare the eﬀectiveness of clustering methods for
selecting kernel conﬁgurations to deploy in a library we ex-
plored their outcomes given the benchmark dataset. We
used a selection method of choosing the kernels that gave
best performance by count. This Top-N method is a for-
mulation of the methods used when previously selecting
the kernels manually, and serves as a useful baseline to see
how diﬀerently more advanced methods perform.

The clustering methods provide either representatives
of the clusters, such as the centroids of the k-means clus-
ters, or just the cluster labels for each of the data entries.
When there are representatives of the clusters, these can
be used to select an optimal kernel by looking at which
kernel conﬁguration performs best for the representative.
When the full cluster is provided the optimal kernel is com-
puted by taking the geometric mean of all elements in the
cluster and choosing the best performing conﬁguration of
this mean set of values.

4.3. The results

The dataset was split into training and test subsets, al-
lowing a comparison of how well the techniques generalize
to previously unseen matrix sizes. Each proposed tech-
nique used the training dataset to select a ﬁxed number of
kernel conﬁgurations and the test dataset was used to eval-
uate what percentage of the optimal performance could be
achieved by only considering those selected kernels.

The optimal performance of the test data is given by
the benchmark data and normalized to between 0 and 1.
A geometric mean of each value for the best performing
kernel out of the selection was computed with all entries
of the test dataset to give this ﬁnal performance ﬁgure.

Figure 5 shows the percentage of the optimal perfor-
mance obtained by the diﬀerent clustering techniques on
the AMD GPU for the four diﬀerent normalization tech-
niques discussed in Section 3.4. The machine learning
methods all perform better than the Top-N method of se-
lecting the kernels based on those that perform best by
count, except when the number of kernels selected gets
very large. Some of the selection methods perform almost
as well when selecting as few as 6 kernels, and don’t im-
prove much as the number of kernels increases. This sug-
gests that there are a small number of kernels that perform
well for a wide range of input sizes, but that are not the
ones that actually perform best for a large number of in-
puts.

For example when the number of kernels is limited to

4, the 4 top kernels by count are:

• Tiles (4, 8, 4), work-group (16, 16)

• Tiles (4, 8, 4), work-group (8, 16)

• Tiles (4, 8, 4), work-group (8, 32)

• Tiles (8, 4, 4), work-group (8, 32)

The tile sizes are all similar, with slightly diﬀerent work-
group sizes. These conﬁgurations perform similarly, and
must perform well for some of the most common input
sizes. However they do not perform well on the large num-
ber of less optimal input sizes, and so overall this selection
gives poor performance. In comparison the decision tree
selection is:

• Tile (2, 8, 1), work-group (8, 32)

6

Figure 5: The performance of each pruning technique in Section 4
as a percentage of the optimal obtainable performance for the AMD
R9 Nano GPU, comparing the normalization techniques discussed in
Section 3.4.

Figure 6: The performance of each pruning technique in Section 4
as a percentage of the optimal obtainable performance for the Intel
i7-6700K CPU, comparing the normalization techniques discussed in
Section 3.4.

7

707580859095100Percent of optimalNormalization: scale707580859095100Percent of optimalNormalization: cutoff707580859095100Percent of optimalNormalization: rawcutoff456789101112131415Number of configurations chosen707580859095100Percent of optimalNormalization: sigmoidTopNDecisionTreeKMeansPCAKMeansSpectralHDBScan9092949698100Percent of optimalNormalization: scale9092949698100Percent of optimalNormalization: cutoff9092949698100Percent of optimalNormalization: rawcutoff456789101112131415Number of configurations chosen9092949698100Percent of optimalNormalization: sigmoidTopNDecisionTreeKMeansPCAKMeansSpectralHDBScan• Tile (2, 8, 4), work-group (16, 16)

• Tile (4, 4, 4), work-group (8, 32)

• Tile (4, 8, 4), work-group (8, 32)

It includes only one of the top performing conﬁgurations,
but this allows the overall kernel selection to be better
suited to the diﬀerent corner cases. These much more var-
ied conﬁgurations therefore give better performance across
a wider range of the input sizes.

All clustering methods performed well for the stan-
dard scaled normalization, though the Spectral clustering
method performed worst after TopN. For the more sparse
normalization techniques the performance of the clustering
methods start to become more varied. Both the decision
tree and k-means methods appear to perform well across
the diﬀerent normalization techniques, while the perfor-
mance of HDBScan can vary.

This is promising for extending this data to the much
more sparse data that would be generated by other auto-
tuning techniques that run benchmarks of many fewer con-
ﬁgurations. In these cases the data will naturally be much
more sparse than the brute force dataset, and these nor-
malization techniques mimic the data that might be ob-
tained from these approaches.

The clustering methods most aﬀected by normalization
method are HDBScan and spectral clustering. When the
data becomes more sparse these methods appear to select
less optimal kernels and therefore gain worse performance
overall. In addition HDBScan was the hardest to train, as
the numbers of clusters cannot be speciﬁed as a parameter,
so a parameter search is required to select the best options
to limit the numbers of kernels.

Figure 6 shows the same data but for the Intel i7-6700K
CPU. In the benchmarks this device was more consistent in
the performance that it achieved for diﬀerent input sizes.
As such all kernel selection techniques performed signif-
icantly better than for the AMD GPU, where there was
much more variation in the obtained performance.

In these benchmarks, the HDBScan density based clus-
tering technique performed surprisingly poorly and the re-
sults varied signiﬁcantly depending on the number of ker-
nels. For the standard normalization technique all tested
parameters gave only 4 or 5 kernels.

The decision tree clustering method performed well for
the AMD data, often achieving among the best perfor-
mance, however for the CPU this is not the case. It seemed
to lose the least performance on the raw cutoﬀ normaliza-
tion scheme, but for all other normalization schemes the
decision tree tends to be outperformed by the other clus-
tering methods.

4.4. Clustering conclusions

The baseline option of choosing the kernels by which
appear to be best most often is a weak approach. The
more intelligent clustering methods outperformed this in
the majority of cases, as they consider the distribution of

8

the data more generally and use that to select the kernels
that provide better performance across a wide range of
inputs.

The aim for the kernel clustering is to automatically
prune the number of kernels to provide in a library. As
such the chosen solution should provide good performance
regardless of the device or normalization scheme. The de-
cision tree, spectral clustering and HDBScan clustering
give varied performance across the devices and types of
normalization, whereas the K-means and PCA+K-means
clustering methods provide stable and good results. There
are deﬁnitely cases where these relatively simplistic clus-
tering techniques do not perform as well as some others,
but the diﬀerence is rarely large.

5. Deploying the kernels

Selecting which kernels to deploy in a library is only
half the story as our goal is to be able to support any inputs
required by our users. This requires a method to map the
user’s inputs to the best kernel conﬁguration provided by
the library. Such a process must be carried out before
launching each kernel to ensure that the optimal choice is
made at each point. This means that the selection process
must be both eﬀective and inexpensive to compute; there is
little point gaining a small performance boost in the kernel
if it is outweighed by time spent in a large classiﬁcation
system.

5.1. Classiﬁers

The previous sections investigated how to limit the
number of kernel conﬁgurations that should be provided
in a library. Selecting which of these kernels to run is
a classiﬁcation problem that maps the input matrix sizes
to the optimal kernel conﬁguration. For each entry in our
dataset we can see which of the chosen kernels provides the
best performance, and train a classiﬁer to do this selection
using standard supervised learning techniques.

There are many diﬀerent techniques for classiﬁcation
using machine learning. The classiﬁer will have to be run
each time a new matrix multiplication is launched by the
library and so the main challenge is to balance the eﬀec-
tiveness of the classiﬁer with the time taken to make a
classiﬁcation. More complicated state of the art classiﬁers
like neural networks may be very eﬀective, but they are
also computationally expensive and so would be a poor
choice to integrate in this way. Decision trees on the other
hand are easy to implement in a performant way and easy
to integrate in a library, as they can be implemented as a
series of nested if statements within the kernel launcher.
If a decision tree can eﬀectively infer the best kernel to
use for unseen matrix sizes then this would be an ideal
solution to use.

To establish whether this is the case, we compare the
eﬀectiveness of three decision trees to other classiﬁcation
techniques. The decision trees have increasing limits on

Table 1: The performance results for the classiﬁers as a percentage
of the absolute optimal performance, for the kernel conﬁgurations
selected by PCA+K-means for the AMD R9 Nano. Note that the
maximum achievable performance for the selection of conﬁgurations
is limited to 91.19%, 94.62%, 94.94% and 96.89% for the 5, 6, 8 and
15 conﬁgurations respectively.

Number of conﬁgurations

Classiﬁer

5

6

8

1NearestNeighbor
3NearestNeighbor
7NearestNeighbor

DecisionTreeA 88.16
DecisionTreeB 86.10
DecisionTreeC 84.56
77.37
78.15
75.38
LinearSVM 68.68
RadialSVM 70.93
86.91
MLP 63.61

RandomForest

86.82
90.62
85.39
78.93
78.64
74.85
74.46
70.93
89.31
56.35

85.53
83.21
82.30
77.79
76.85
75.08
67.31
70.93
87.60
64.39

15

85.64
83.01
83.66
75.48
76.82
77.39
77.62
70.93
83.96
62.99

Table 2: The performance results for the classiﬁers as a percentage
of the absolute optimal performance, for the kernel conﬁgurations
selected by PCA+K-means for the Intel i7-6700K CPU. Note that
the maximum achievable performance for the selection of conﬁgura-
tions is limited to 96.55%, 96.65%, 97.34% and 97.95% for the 5, 6,
8 and 15 conﬁgurations respectively.

Number of conﬁgurations

Classiﬁer

5

6

8

1NearestNeighbor
3NearestNeighbor
7NearestNeighbor

DecisionTreeA 91.65
DecisionTreeB 93.14
DecisionTreeC 92.26
91.36
91.18
88.00
LinearSVM 84.18
RadialSVM 80.49
93.65
MLP 74.30

RandomForest

92.59
91.86
91.11
91.36
90.26
90.15
76.20
83.80
93.90
79.23

93.50
93.87
91.51
91.40
91.61
89.22
88.32
78.55
93.26
79.23

15

92.29
90.15
91.28
89.73
86.42
87.96
85.64
83.80
93.85
76.88

the depth and numbers of samples allowed for leaf nodes.
Varying these parameters helps establish how much the
decision tree might be overﬁtting. Deeper trees can ﬁt
better to the training data, but will potentially overﬁt to
suit the training data and perform poorly on previously
unseen inputs.

The three decision trees are signiﬁed A, B and C. Deci-
sion tree A has no limit on the maximum depth and allows
splitting down to single sample leaf nodes if required. De-
cision tree B has a maximum depth of 6 and requires leaf
nodes to have at least 3 samples, while decision tree C has
a maximum depth of 3 and requires at least 4 samples at
the leaves. There are many other possible combinations of
parameters, however additional tuning of these risk over-
ﬁtting to the testing data set.

Nearest neighbor is another relatively simple classiﬁca-

9

tion technique that classiﬁes an input based on which of
the training inputs are closest to it. As such it requires
that the training dataset be stored alongside the classiﬁer
to compute which data points are the input’s neighbors.
As such it would be infeasible to deploy within the library
but provides a useful comparison for what similar classi-
ﬁers can achieve.

Other classiﬁers are more complex and require signif-
icantly more computation to infer a class from an input.
Classiﬁers like SVM, which computes the vectors that sep-
arate the classes, and random forest ensembles, made up
of multiple decision trees that are combined together, can
potentially provide better performance but would require
more work on the host when choosing the kernel to launch.
The comparisons made between these classiﬁers con-
sidered how well they could infer the optimal kernel given
the subset of kernels provided by the pruning techniques
discussed in Section 4. As the choice of kernels is limited
to this subset the maximum achievable performance is not
100%.

Tables 1 and 2 show the relative performance of the
diﬀerent classiﬁcation methods for a range of possible ker-
nel conﬁgurations. Overall the decision tree classiﬁcation
methods perform well, in many cases signiﬁcantly better
than the more computationally expensive methods.

One of the more surprising observations here is that
the performance does not improve as the number of classes
does, despite the theoretical maximum achievable perfor-
mance increasing. The absolute best performance for both
devices was obtained with just 6 kernel conﬁgurations, and
the decision tree obtaining best performance for either
6 or 8 kernel conﬁgurations. While the additional ker-
nel choices may allow higher theoretical performance, the
models seem to struggle to diﬀerentiate between similar in-
puts that would require diﬀerent kernels. As such having
the extra choice actually hinders the model’s performance
rather than allowing it to achieve better performance.

When comparing the three diﬀerent decision tree con-
ﬁgurations, the performance data does not support the
theory that the tree may overﬁt to the training data. The
more limited trees (B and C) tend to perform worse than
the unlimited decision tree (A), though the numbers are
not clear. When integrating the decision tree into the
SYCL library it is helpful to provide some limits, so as
to avoid heavily nested if statements and branching code.

6. Testing a full ML model

This work was carried out to help provide general pur-
pose compute libraries to accelerate machine learning ap-
plications. Comparing the inference time of a machine
learning model using these techniques to similar libraries
that use ore manual tuning techniques can show the eﬃ-
cacy of this work.

One of the popular image classiﬁcation models a few
years ago was VGG16 [24], developed at the Oxford Visual

Geometry Group in 2015. By modern standards it is a
simple neural network made up of 16 convolutional and
pooling layers. Despite the small number of layers it has
more parameters than most modern networks with 138
million, as the convolutional layers have many features.

While no longer state of the art, this model is still reg-
ularly used by machine learning practitioners and much
simpler than more recent image classiﬁcation networks,
making it a good candidate to use to evaluate the per-
formance of the kernel selection process. Comparing the
performance of individual kernels provides a good proxy to
determine how well a system will perform, but an evalua-
tion on the full system will help uncover any assumptions
and shortcomings that would not be visible at the micro-
benchmark scale.

A SYCL-DNN sample implements the VGG16 network
in SYCL using the pretrained weights provided by the
Keras Applications [33] Python module. It can perform
image classiﬁcation based on the ImageNet dataset, pro-
viding the class of an input image from the 1000 diﬀer-
ent ImageNet classes. This pretrained network achieves
71.3% performance classifying the top class of an image
in the ImageNet dataset.
It is not the best performing
model available through Keras but is one of the simplest
to implement.

In addition to testing the performance of this network
on the devices discussed earlier in this paper, we also
tested two additional OpenCL devices. The kernels used
were tuned for each device using the methods discussed
above and the resulting deployment and selection algo-
rithms were integrated into SYCL-DNN.

The devices used to test these techniques were:

• AMD R9 Nano GPU

• Intel i7-6700K CPU

• Intel HD 530 Gen9 GPU

• ARM Mali G71 GPU

6.1. Library comparisons

SYCL-DNN allows users to specify diﬀerent backends
that provide the matrix multiplication routines used in
neural networks. The library provides its own matrix mul-
tiplication, but if a platform has access to a BLAS or other
matrix library then it can be easily integrated to make use
of these optimized routines. This functionality was used
to provide comparisons to the tuned SYCL-DNN matrix
multiplication kernels, using both a SYCL-BLAS [5] back-
end and a CLBlast [4] backend.

SYCL-BLAS is another library developed by Codeplay
to provide basic linear algebra kernels. Designed with ex-
pression trees and templated kernels it allows users to eas-
ily fuse kernels together at compile time, reducing the need
to load and store data between kernel launches, and is op-
timized for a range of devices. SYCL-BLAS provides a

10

number of diﬀerent matrix multiplication routines, includ-
ing ones utilizing local (or shared) memory and ones de-
signed for tall skinny matrices that compute partial results
which are combined in a ﬁnal reduction. These kernels
are signiﬁcantly more sophisticated than the simple ker-
nel studied in this paper, however the parameters are all
tuned by hand requiring signiﬁcant developer eﬀort and
time.

CLBlast is an OpenCL based BLAS library designed
to be performant on a wide range of OpenCL devices. It
includes an automated tuning system to select the opti-
mal kernels for diﬀerent devices, though this system is
limited to selecting the single best kernel for each device.
Before running this benchmark, the CLBlast library was
tuned for each of the benchmark devices used. Similarly to
SYCL-BLAS, the CLBlast library contains multiple imple-
mentations of matrix multiplication kernels to help achieve
performance for diﬀerent matrix shapes.

6.2. Results

The model was executed a number of times to accu-
rately measure the time of completion. A single image
was used as an input, and the model classiﬁes the con-
tents of that image. The weights and initial image are
all transferred to the compute device before starting tim-
ing, so the benchmark time only includes the computation
and not data transfer. The SYCL-DNN matrix multipli-
cation routine was tuned to use 8 kernel conﬁgurations per
device selected using PCA+K-means and a decision tree
based runtime selection process. As discussed in Sections 4
and 5 these approaches give good performance for diﬀerent
matrix sizes and devices.

Figure 7 shows the execution time to compute one in-
ference using the VGG16 model. The diﬀerent devices
perform signiﬁcantly diﬀerently as would be expected as
they have vastly diﬀerent compute resources available.

The AMD R9 Nano performed an inference in less than
20ms using the optimized and tuned matrix multiplication
kernels from SYCL-BLAS and CLBlast. This GPU along
with this particular machine learning model was one of
the main targets of optimization during the development
of SYCL-BLAS so it is expected that it performs well,
outperforming both the kernel studied in this paper and
CLBlast. The SYCL-DNN kernel achieved times that were
not far oﬀ the others, despite the kernel being much sim-
pler than those in the heavily optimized libraries and not
making use of the GPU’s fast local memory.

By default CLBlast will use generic tuning parameters
based on similar devices, so for the R9 Nano the parame-
ters are based on similar AMD cards. Tuning CLBlast for
this speciﬁc GPU using the provided tuning tools didn’t
provide any beneﬁt, though the actual kernels used did
change. For the other devices the tuning often had a neg-
ative impact on the performance of CLBlast. This is likely
to be a result of the limited way that the tuning works
causing it to optimize for best results on matrix sizes that
diﬀer from those used in the VGG16 model. The GEMM

Figure 7: The inference time in miliseconds of a single image using the VGG16 model implemented using SYCL-DNN and diﬀerent matrix
backends when run on diﬀerent devices.

routine in particular is tuned for single matrices of size
1024x1042 and 256x256, whereas the inputs to GEMM
used in the model have a batch size of 16 and vary from
12544x64 to 512x512.

For the Intel CPU and integrated GPU the SYCL-
DNN kernel actually performed better than the optimized
libraries. The CPU has very diﬀerent performance charac-
teristics and compute resources to any of the GPUs, and
CLBlast particularly struggles to adapt to this.

Both SYCL-BLAS and CLBlast achieve similar per-
formance on the ARM Mali GPU, taking over 700ms per
inference. SYCL-DNN on the other hand achieves under
400ms per inference, as it makes use of 4 diﬀerent conﬁgu-
rations out of the chosen 8. This variety of possible kernel
conﬁgurations allows the library to hand the diﬀerent ma-
trix sizes where the other libraries only use a single kernel
conﬁguration.

One of the areas where the SYCL-DNN kernels are at
a disadvantage to the other libraries is in the ﬁnal fully
connected layers in the model. These fully connected lay-
ers are implemented as a matrix multiplication, but when
using a single image the activation tensor is actually a one
dimensional vector rather than a matrix. As such it is
much more eﬃcient to use a dedicated matrix-vector mul-
tiplication routine common in BLAS libraries. The SYCL-
DNN kernel is comparatively ineﬃcient in this case, as it
is designed to compute 2D tiles of the output, which would
only be one-dimensional. Despite this, the library manages
to provide suﬃcient performance on these operations that
the automatically tuned SYCL-DNN kernels outperform
the other libraries overall.

7. Conclusions

Auto-tuning allows libraries to achieve performance on
a wide range of devices without requiring vast amounts of
developer eﬀort to adapt kernels and routines to new hard-
ware. In this paper we used a matrix multiplication case
study to evaluate some methods to allow auto-tuning to
be deployed in compiled SYCL libraries, balancing binary
size, performance and adaptability to unseen inputs.

Unsupervised machine learning techniques like cluster-
ing provide eﬀective methods to reduce the large kernel pa-
rameter space for a wide range of diﬀerent input sizes with-
out sacriﬁcing much performance. Some of these methods
proved more reliable and resilient than others, with some
of the more advanced methods like density based cluster-
ing methods struggling to provide performant kernels in
some cases.

One of the concerns raised in the original paper [22]
introducing these ideas was that the techniques may rely
too heavily on the dense benchmark timing information.
Intelligent auto-tuning techniques only sample from the
very large kernel parameter space, while the data collected
for this study used a comparatively small parameter space
and so used a brute-force benchmarking technique. The
normalization techniques discussed in Section 3.4 intro-
duce sparsity into the data and Section 4 shows that while
this does have an impact on the performance of the ker-
nel selection routines, this diﬀerence is minimal. This is
promising for extending these results to more complicated
kernels that use more parameters that can take a larger
range of values.

After selecting the kernels to deploy in the SYCL li-
brary, there needs to be a runtime routine to choose which
of these kernels to execute for any given input. The tech-
niques discussed in Section 5 show that decision trees can
provide good performance, as well as being easy to imple-
ment and integrate into a library.

When integrated into SYCL-DNN, these techniques
met or vastly exceeded other optimized BLAS libraries
for a representative machine learning model. The perfor-
mance was competitive on a range of devices, from pow-
erful desktop GPUs through to embedded mobile GPUs,
even though the kernels themselves are relatively simple
and don’t use as many hardware features as those in the
other libraries.

Overall these tuning and deployment techniques pro-
vide an eﬃcient subset of all possible kernels where the ker-
nels have to be provided in binary format as with SYCL.
These completely automated approaches allow new devices
to be supported with very little developer eﬀort and rela-
tively small code changes.

11

SYCL-DNNSYCL-BLASCLBlasttunedCLBlastuntunedLibrary05101520Time (ms)AMD R9 NanoSYCL-DNNSYCL-BLASCLBlasttunedCLBlastuntunedLibrary050100150200Intel Gen9SYCL-DNNSYCL-BLASCLBlasttunedCLBlastuntunedLibrary02004006008001000Intel i7-6700KSYCL-DNNSYCL-BLASCLBlasttunedCLBlastuntunedLibrary0200400600ARM Mali-G71Acknowledgements

The author would like to thank Duncan McBain and
Daniel Soutar for thoughtful comments and interesting dis-
cussions about this work. This research did not receive any
speciﬁc grant from funding agencies in the public, commer-
cial, or not-for-proﬁt sectors.

References

References

[1] J. E. Stone, D. Gohara, G. Shi, OpenCL: A parallel program-
ming standard for heterogeneous computing systems, Comput-
ing in Science Engineering 12 (3) (2010) 66–73. doi:10.1109/
MCSE.2010.69.

[2] SYCL: C++ single-source heterogeneous programming for
OpenCL, https://www.khronos.org/sycl/, accessed: 2019-03-
11.

[3] clBLAS: A software library containing BLAS functions writ-
ten in opencl, https://github.com/clMathLibraries/clBLAS,
accessed: 2020-08-26.

[4] C. Nugteren, CLBlast: A tuned OpenCL BLAS library, in: Pro-
ceedings of the International Workshop on OpenCL, IWOCL
’18, ACM, New York, NY, USA, 2018, pp. 5:1–5:10.
doi:
10.1145/3204919.3204924.
URL http://doi.org/10.1145/3204919.3204924

[5] SYCL-BLAS: An implementation of BLAS using the SYCL
https://github.com/CodeplaySoftware/

open
SYCL-BLAS, accessed: 2019-04-09.

standard,

[6] J. I. Aliaga, R. Reyes, M. Goli, SYCL-BLAS: Leveraging expres-
sion trees for linear algebra, in: Proceedings of the 5th Interna-
tional Workshop on OpenCL, IWOCL 2017, ACM, New York,
NY, USA, 2017, pp. 32:1–32:5. doi:10.1145/3078155.3078189.
URL http://doi.org/10.1145/3078155.3078189

[7] C. Nugteren, V. Codreanu, Cltune: A generic auto-tuner for
OpenCL kernels, in: 2015 IEEE 9th International Symposium
on Embedded Multicore/Many-core Systems-on-Chip, 2015, pp.
195–202. doi:10.1109/MCSoC.2015.10.

[8] B. van Werkhoven, Kernel tuner: A search-optimizing GPU
code auto-tuner, Future Generation Computer Systems 90
(2019) 347 – 358. doi:10.1016/j.future.2018.08.004.
URL https://doi.org/10.1016/j.future.2018.08.004

[9] T. L. Falch, A. C. Elster, Machine learning based auto-tuning
for enhanced OpenCL performance portability, in: 2015 IEEE
International Parallel and Distributed Processing Symposium
Workshop, 2015, pp. 1231–1240. doi:10.1109/IPDPSW.2015.85.
[10] J. Bergstra, N. Pinto, D. Cox, Machine learning for predic-
in: 2012 In-
doi:

tive auto-tuning with boosted regression trees,
novative Parallel Computing (InPar), 2012, pp. 1–9.
10.1109/InPar.2012.6339587.

[11] D. Grewe, Z. Wang, M. P. O’Boyle, Portable mapping of data
parallel programs to opencl for heterogeneous systems, in: 2013
IEEE/ACM International Symposium on Code Generation and
Optimization (CGO), IEEE Computer Society, Los Alamitos,
CA, USA, 2013, pp. 1–10. doi:10.1109/CGO.2013.6494993.
URL https://doi.org/10.1109/CGO.2013.6494993

[12] W. F. Ogilvie, P. Petoumenos, Z. Wang, H. Leather, Active
learning accelerated automatic heuristic construction for paral-
lel program mapping, in: 2014 23rd International Conference
on Parallel Architecture and Compilation Techniques (PACT),
2014, pp. 481–482. doi:10.1145/2628071.2628128.

[13] T. D. Han, T. S. Abdelrahman, Automatic tuning of local mem-
ory use on gpgpus, in: ADAPT Workshop proceedings, 2015,
Vol. 1410.0759, 2014. arXiv:1410.0759.

[14] B. van Werkhoven, J. Maassen, H. E. Bal, F. J. Seinstra, Op-
timizing convolution operations on GPUs using adaptive tiling,
Future Generation Computer Systems 30 (2014) 14 – 26, spe-
cial Issue on Extreme Scale Parallel Architectures and Systems,

12

Cryptography in Cloud Computing and Recent Advances in
Parallel and Distributed Systems, ICPADS 2012 Selected Pa-
pers. doi:10.1016/j.future.2013.09.003.
URL https://doi.org/10.1016/j.future.2013.09.003
[15] Y. Li, J. Dongarra, S. Tomov, A note on auto-tuning GEMM
for GPUs, in: G. Allen, J. Nabrzyski, E. Seidel, G. D. van
Albada, J. Dongarra, P. M. A. Sloot (Eds.), Computational
Science – ICCS 2009, Springer Berlin Heidelberg, Berlin, Hei-
delberg, 2009, pp. 884–892.

[16] A. Nukada, S. Matsuoka, Auto-tuning 3-D FFT library for
CUDA GPUs, in: Proceedings of the Conference on High Per-
formance Computing Networking, Storage and Analysis, SC
’09, ACM, New York, NY, USA, 2009, pp. 30:1–30:10. doi:
10.1145/1654059.1654090.
URL http://doi.org/10.1145/1654059.1654090

[17] A. Mametjanov, D. Lowell, C. Ma, B. Norris, Autotuning
stencil-based computations on GPUs, in: 2012 IEEE Interna-
tional Conference on Cluster Computing, 2012, pp. 266–274.
doi:10.1109/CLUSTER.2012.46.

[18] Y. Zhang, F. Mueller, Auto-generation and auto-tuning of 3D
stencil codes on GPU clusters, in: Proceedings of the Tenth
International Symposium on Code Generation and Optimiza-
tion, CGO ’12, ACM, New York, NY, USA, 2012, pp. 155–164.
doi:10.1145/2259016.2259037.
URL http://doi.org/10.1145/2259016.2259037

[19] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin,
S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard,
Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg,
D. Man´e, R. Monga, S. Moore, D. Murray, C. Olah, M. Schus-
ter, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker,
V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. War-
den, M. Wattenberg, M. Wicke, Y. Yu, X. Zheng, TensorFlow:
Large-scale machine learning on heterogeneous systems, soft-
ware available from tensorﬂow.org (2015).
URL https://www.tensorflow.org/

[20] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao,
B. Xu, C. Zhang, Z. Zhang, Mxnet: A ﬂexible and eﬃcient
machine learning library for heterogeneous distributed systems,
ArXiv 1512.01274. arXiv:1512.01274.

[21] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran,
B. Catanzaro, E. Shelhamer, cudnn: Eﬃcient primitives for
deep learning, CoRR abs/1410.0759. arXiv:1410.0759.

[22] J. Lawson, Towards automated kernel selection in machine
learning systems: A SYCL case study,
in: 2020 IEEE In-
ternational Parallel and Distributed Processing Symposium
Workshops (IPDPSW), 2020, pp. 475–478.
doi:10.1109/
IPDPSW50202.2020.00086.

[23] R. Burns, J. Lawson, D. McBain, D. Soutar, Accelerated neural
networks on OpenCL devices using SYCL-DNN, in: Proceed-
ings of the International Workshop on OpenCL, IWOCL’19,
ACM, New York, NY, USA, 2019, pp. 10:1–10:4. doi:10.1145/
3318170.3318183.
URL http://doi.org/10.1145/3318170.3318183

[24] K. Simonyan, A. Zisserman, Very deep convolutional networks
for large-scale image recognition, CoRR abs/1409.1556. arXiv:
1409.1556.

[25] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for
in: 2016 IEEE Conference on Computer
image recognition,
Vision and Pattern Recognition (CVPR), 2016, pp. 770–778.
doi:10.1109/CVPR.2016.90.

[26] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, L. Chen, Mo-
bileNetV2: Inverted residuals and linear bottlenecks, in: 2018
IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition, 2018, pp. 4510–4520. doi:10.1109/CVPR.2018.00474.

[27] Towards automated kernel selection in macine learning sys-
tems: Supplementary code and dataset, https://github.com/
jwlawson/tuning_kernels, accessed: 2020-02-07.

[28] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,

M. Brucher, M. Perrot, E. Duchesnay, Scikit-learn: Machine
learning in Python, Journal of Machine Learning Research 12
(2011) 2825–2830.

[29] K. Pearson, LIII. On lines and planes of closest ﬁt to systems
of points in space, The London, Edinburgh, and Dublin Philo-
sophical Magazine and Journal of Science 2 (11) (1901) 559–572.
doi:10.1080/14786440109462720.
URL https://doi.org/10.1080/14786440109462720

[30] M. E. Tipping, C. M. Bishop, Probabilistic principal compo-
nent analysis, Journal of the Royal Statistical Society. Series B
(Statistical Methodology) 61 (3) (1999) 611–622.
URL http://www.jstor.org/stable/2680726

[31] R. J. G. B. Campello, D. Moulavi, J. Sander, Density-based
clustering based on hierarchical density estimates, in: J. Pei,
V. S. Tseng, L. Cao, H. Motoda, G. Xu (Eds.), Advances in
Knowledge Discovery and Data Mining, Springer Berlin Heidel-
berg, Berlin, Heidelberg, 2013, pp. 160–172.

[32] L. McInnes, J. Healy, Accelerated hierarchical density based
clustering, in: 2017 IEEE International Conference on Data
Mining Workshops (ICDMW), 2017, pp. 33–42. doi:10.1109/
ICDMW.2017.12.

[33] Keras applications: Reference implementations of popu-
lar deep learning models., https://github.com/keras-team/
keras-applications, accessed: 2020-08-27.

13

