0
2
0
2

n
u
J

1
1

]

G
L
.
s
c
[

1
v
0
3
7
6
0
.
6
0
0
2
:
v
i
X
r
a

AutoML vs. Deep Learning

Is deep learning necessary for simple classiﬁcation tasks?

joseph.romano@pennmedicine.upenn.edu

ttle@pennmedicine.upenn.edu

weixuanf@pennmedicine.upenn.edu

jhmoore@upenn.edu

Joseph D. Romano
Institute for Biomedical Informatics
University of Pennsylvania
Philadelphia, PA 19104, USA

Trang T. Le
Institute for Biomedical Informatics
University of Pennsylvania
Philadelphia, PA 19104, USA

Weixuan Fu
Institute for Biomedical Informatics
University of Pennsylvania
Philadelphia, PA 19104, USA

Jason H. Moore
Institute for Biomedical Informatics
University of Pennsylvania
Philadelphia, PA 19104, USA

Editor: N/A

Abstract

Automated machine learning (AutoML) and deep learning (DL) are two cutting-edge
paradigms used to solve a myriad of inductive learning tasks. In spite of their successes,
little guidance exists for when to choose one approach over the other in the context of
speciﬁc real-world problems. Furthermore, relatively few tools exist that allow the integra-
tion of both AutoML and DL in the same analysis to yield results combining both of their
strengths. Here, we seek to address both of these issues, by (1.) providing a head-to-head
comparison of AutoML and DL in the context of binary classiﬁcation on 6 well-characterized
public datasets, and (2.) evaluating a new tool for genetic programming-based AutoML
that incorporates deep estimators. Our observations suggest that AutoML outperforms
simple DL classiﬁers when trained on similar datasets for binary classiﬁcation but inte-
grating DL into AutoML improves classiﬁcation performance even further. However, the
substantial time needed to train AutoML+DL pipelines will likely outweigh performance
advantages in many applications.
Keywords: Automated Machine Learning, Genetic Programming, Evolutionary Algo-
rithms, Deep Learning, Pareto optimization

1. Introduction and Background

Deep learning (DL) and automated machine learning (AutoML) are two approaches for
constructing high-performance estimators that dramatically outperform traditional machine
learning in a variety of scenarios, especially on classiﬁcation and regression tasks. In spite
of their successes, there remains substantial debate—and little quantitative evidence—on

1

 
 
 
 
 
 
Romano et al

the practical advantages of the two approaches and how to determine which will perform
best on speciﬁc real-world problems.

To address this issue, we conduct a series of experiments to compare the performance
of DL and AutoML pipelines on 6 well-characterized binary classiﬁcation problems. We
also introduce and critically assess a new resource that leverages the strengths of both
approaches by integrating deep estimators into an existing AutoML tool for constructing
machine learning pipelines using evolutionary algorithms. Speciﬁcally, we sought to answer
two questions in this study:

1. How well does genetic programming-based AutoML perform on simple binary classi-

ﬁcation tasks in comparison to DL?

2. By augmenting AutoML with DL estimators, do we achieve better performance than

with either of the two alone?

Beyond these two questions, we also provide speciﬁc recommendations on choosing be-
tween DL and AutoML for simple classiﬁcation tasks, and a set of priorities for future
work on the development of methods that integrate DL and AutoML. Our new software
resource—which we have named TPOT-NN—is an extension to the previously described
Tree-based Pipeline Optimization Tool (TPOT), and is freely available online.

1.1 Deep learning and artiﬁcial neural networks

Deep learning is an umbrella term used to describe statistical methods—usually involving
artiﬁcial neural networks (ANNs)—that provide estimators consisting of multiple stacked
transformations (Schmidhuber, 2015; LeCun et al., 2015). Traditional estimators (i.e., “shal-
low” models, such as logistic regression or random forest classiﬁers) are limited in their
ability to approximate nonlinear or otherwise complex objective functions, resulting in re-
duced performance, particularly on datasets where entities with similar characteristics are
not cleanly separated by linear decision boundaries. Meanwhile, DL estimators overcome
It follows that, given a suﬃcient num-
this limitation through sequential nonlinearities.
ber of layers and an appropriate learning algorithm, even structurally simple feed-forward
neural networks with a ﬁnite number of neurons can approximate virtually any continuous
function on compact subsets of Euclidean space (Leshno et al., 1993).

Nonetheless, the successes of DL have been tempered by a number of important crit-
icisms. Compared to shallow models, it is substantially more complex to parameterize a
deep ANN due to the explosion of free parameters that results from increased depth of
the network or width of individual layers (Shi et al., 2016). Furthermore, DL models are
notoriously challenging to interpret since the features in a network’s intermediate layers are
a combination of features from all previous layers, which eﬀectively obscures the intuitive
meaning of individual feature weights in most nontrivial cases (Lipton, 2018; Lou et al.,
2012).

It is also worth noting that DL model architectures can reach immense sizes. For ex-
ample, the popular image classiﬁcation network ResNet performed best in an early study
when constructed with 110 convolutional layers, containing over 1.7 billion tunable param-
eters (He et al., 2016). However, for standard binary classiﬁcation on simple datasets,
smaller DL architectures can still substantially outperform shallow learners, both in terms

2

AutoML vs. Deep Learning

of error and training time (Auer et al., 2002; Collobert and Bengio, 2004). For the purpose
of establishing a baseline comparison between AutoML and DL, we restrict our analyses in
this study to this latter case of applications.

1.2 Automated Machine Learning

One of the most challenging aspects of designing an ML system is identifying the appropri-
ate feature transformation, model architecture and hyperparameterization for the task at
hand. For example, count data may beneﬁt from a square-root transformation. Similarly,
a support-vector machine (SVM) model might predict more accurately susceptibility to a
certain complex genetic disease than a gradient boosting model trained on the same dataset.
Further, diﬀerent choices of hyperparameters within that SVM model of kernel function k
and soft margin width C can lead to wildly diﬀerent performances. Traditionally, these
architecture considerations need to be made with the help of prior experience, brute-force
search, or experimenter intuition, all of which are undesirable for their own reasons.

AutoML, on the other hand, provides methods for automatically selecting these options
from a universe of possible architecture conﬁgurations. A number of diﬀerent AutoML tech-
niques can be used to ﬁnd the best architecture for a given task, but one that we will focus
on is based on genetic programming (GP). Broadly, GP constructs trees of mathematical
functions that are optimized with respect to a ﬁtness metric such as classiﬁcation accuracy
(Banzhaf et al., 1998). Each generation of trees is constructed via random mutations to the
tree’s structure or the operations performed at each node in the tree. Repeating this process
for a number of training generations produces an optimal tree. Like in natural evolution,
increasingly more ﬁt architectures are propagated forward while less ﬁt architectures “die
out”.

TPOT (Tree-based Pipeline Automation Tool) is a Python-based AutoML tool that
uses genetic programming to identify optimal ML pipelines for either regression or classiﬁ-
cation on a given (labeled) dataset. Brieﬂy, TPOT performs GP on trees where nodes are
comprised of operators, each of which falls into one of four operator types: preprocessors,
decomposition functions, feature selectors, or estimators (i.e., classiﬁers and regressors).
Input data enters the tree at leaf nodes, and predictions are output at the root node. Each
operator has a number of free parameters that are optimized during the training process.
TPOT maintains a balance between high performance and low model complexity using the
NSGA-II Pareto optimization algorithm (Deb et al., 2002). As a result, the pipelines learned
by TPOT consist of a relatively small number of operators (e.g., in the single-digits) that
can still meet or exceed the performance of competing state-of-the-art ML approaches.

In theory, TPOT can construct pipelines that are structurally equivalent to DL estima-
tors by stacking multiple shallow estimators in a serial conﬁguration. However, since the
individual objectives for each operator are decoupled, it is unclear whether these conﬁgu-
rations can attain the same performance as standalone DL estimators, such as multilayer
perceptrons (MLPs).

With the exception of our new deep learning estimators (which are implemented in
PyTorch), all operators are implemented in either Scikit-learn (Pedregosa et al., 2011) or
XGBoost (Chen and Guestrin, 2016), both of which are open-source, popular Python-
based machine learning libraries. TPOT natively performs cross validation and generates

3

Romano et al

Python scripts that implement the learned pipelines. For a more detailed description and
evaluations of TPOT, please see (Olson et al., 2016a,b; Le et al., 2020).

Figure 1: Example pipelines for model conﬁgurations used in this study. a.) Deep learning
strategy with no AutoML. b.) Example of a standard (no neural networks) TPOT pipeline
containing a logistic regression classiﬁer and a kernel SVM classiﬁer. c.) Example of a
TPOT-NN pipeline containing two multilayer perceptron estimators.

2. Results

We evaluated the pipelines’ performance based on two metrics: classiﬁcation accuracy of
the trained pipeline and the elapsed time to train the pipeline. Each experiment can be
grouped into 1 of 3 main conﬁgurations: NN (a single MLP classiﬁer with no GP); TPOT
(pipelines learned using GP, possibly containing multiple stacked estimators, preprocessors,
and feature transformers); and TPOT-NN (the same as TPOT, but with added MLP and
PyTorch logistic regression estimators). The contrived examples in Figure 1 illustrate the
diﬀerences between pipelines learned from the 3 conﬁgurations.

In §5.2, we describe MLP architectures in this study as well as rationale for our speciﬁc

design decisions.

2.1 Model performance comparison—NN, TPOT, and TPOT-NN

The prediction accuracy distributions of our experiments are shown in Figure 2. For each
of the 6 datasets, neural network estimators alone yielded the lowest average prediction
accuracy while the TPOT-NN pipelines performed best. In general, the TPOT-NN pipelines
performed only marginally better than the standard TPOT pipelines. Notably, the (non-
TPOT) neural network approach yielded substantially poorer performance on two of the
datasets (Hill Valley with noise and Hill Valley without noise). We discuss a likely
explanation in §3.2.

2.2 Validating TPOT-NN’s neural network estimators

The performance advantages of DL estimators are largely derived from their ability to ﬁt
complex nonlinear objectives, which is a consequence of stacking multiple neural layers in a
serial conﬁguration. To conﬁrm that the MLP estimator included in TPOT-NN leverages
this advantage, we used the TPOT-NN API to design a logistic regression (LR) classiﬁer,
which is functionally equivalent to the MLP classiﬁer with the intermediate (‘hidden’) layer

4

TPOT-NN classiﬁerTPOT classiﬁerANN classiﬁera.)b.)c.)DatasetTrainingdataTestingdataClasspredictionsDatasetTrainingdataTestingdataDatasetTrainingdataTestingdataClasspredictionsClasspredictionsMLPMLPMLPLRKernel SVMFeatureSelectorFeatureSelectorCombine FeaturesFeatureTransformerCombine FeaturesAutoML vs. Deep Learning

Figure 2: Prediction accuracy of neural network classiﬁers (NN), TPOT classiﬁers, and
TPOT-NN classiﬁers tested on 6 open-access datasets. Each point used to construct the
distributions corresponds to the accuracy of one trained pipeline.

removed—a shallow estimator implemented identically to the TPOT-NN MLP. We then
ran a series of experiments testing each of these classiﬁers alone (i.e., only using TPOT
to optimize model hyperparameters, and not to construct pipelines consisting of multiple
operators). The results of these experiments are summarized in Figure 3. As a means
for comparison, we visualized these results alongside experiments using the full TPOT-NN
implementation, to conﬁrm that the inclusion of all TPOT operators results in further
improvements to classiﬁcation accuracy.

The results of these experiments support this intuition: TPOT-NN’s implementation
of MLP classiﬁers performs better than the identically implemented LR classiﬁer in most
cases. Enabling all operators available to TPOT-NN results in an even more dramatic
performance increase in addition to a remarkable improvement in the consistency of results
on individual datasets. Together, these observations support the claims that (a.) TPOT-
NN’s new estimators leverage the increased estimation power characteristic of deep neural
network models, and (b.) TPOT-NN provides an eﬀective solution to improve on the results
yielded by “na¨ıve” DL models on simple classiﬁcation tasks.

5

                3 U H G L F W L R Q  D F F X U D F \ V S D P E D V H L R Q R V S K H U H E U H D V W  F D Q F H U  Z L V F R Q V L Q F D U  H Y D O X D W L R Q + L O O B 9 D O O H \ B Z L W K B Q R L V H + L O O B 9 D O O H \ B Z L W K R X W B Q R L V H $ F F X U D F \  G L V W U L E X W L R Q V   1 1   7 3 2 7   D Q G  7 3 2 7  1 1 1 1 7 3 2 7 7 3 2 7  1 1Romano et al

Figure 3: Prediction accuracy on each of the 6 evaluation databases stratiﬁed by estimators
enabled in TPOT. Each point used to construct the box plots is the classiﬁcation accuracy
of a single optimized TPOT pipeline.

2.3 Training duration of TPOT-NN models

The total training time for TPOT pipelines ranged from 4h 22m to 8d 19h 55m, with a
mean training time of 1d 0h 49m (± 1d 13h 28m). Table 1 and Figure 4 show how
training times vary across the diﬀerent experiments. To provide a basis of comparison
for standard TPOT, we include training time for pipelines consisting of a single shallow
estimator only, referred to as “Shallow (single estimator)”, where TPOT uses GP solely
for the purpose of optimizing hyperparameters on a pipeline containing a single estimator.
Both conﬁgurations involving NN estimators required a substantially larger amount of time
to train a single pipeline on average: an increase of 629% for NN pipelines and 336% for
TPOT-NN piplelines versus standard TPOT. Future studies on larger datasets should be
performed to establish how this relationship scales with respect to training set size.

6

                     3 U H G L F W L R Q  D F F X U D F \ / R J L V W L F U H J U H V V L R Q 0 X O W L O D \ H U 3 H U F H S W U R Q $ O O  H V W L P D W R U V $ F F X U D F \  G L V W U L E X W L R Q V  E \  H V W L P D W R U  W \ S H G D W D E D V H V S D P E D V H L R Q R V S K H U H E U H D V W  F D Q F H U  Z L V F R Q V L Q F D U  H Y D O X D W L R Q + L O O B 9 D O O H \ B Z L W K B Q R L V H + L O O B 9 D O O H \ B Z L W K R X W B Q R L V HAutoML vs. Deep Learning

Experiment type

Mean training time Minimum Maximum

Std. dev.

Shallow (single estimator)
TPOT
NN (single estimator)
TPOT-NN

5h 39m
0d 9h 26m
2d 20h 48m
1d 17h 8m

59m

4m 1d 23h 26m
11h 30m
10h 23m
4d 59m
8h 51m 8d 19h 55m 2d 7h 19m
3h 35m 6d 20h 6m 1d 7h 2m

Table 1: Training time statistics for single shallow estimators, ‘standard’ TPOT, single
neural network estimators, and TPOT-NN. In all cases, GP is enabled for the purpose of
tuning hyperparameters.

Figure 4: Comparison of shallow estimator, NN estimator (MLP), TPOT, and TPOT-NN
experiment training time distributions.

2.4 Structural topologies of pipelines learned by GP

TPOT assembles pipelines that consist of multiple operators—possibly including multiple
classiﬁers or regressors in addition to feature selectors and feature transformers—to achieve
better performance than individual machine learning estimators (Olson et al., 2016a). Since
the estimation capacity of simple feedforward neural networks is a monotonic function of
network depth, we sought to determine whether TPOT can automatically construct deep
architectures by stacking shallow estimators in the absence of a priori instruction to do so.

When TPOT-NN was forced to build pipelines comprised only of feature selectors, fea-
ture transformers, and logistic regression estimators, it did indeed construct pipelines con-
sisting of stacked arrangements of logistic layers that strongly resemble well-known DL
models. The following Python code is the output of one of these, selected at random from
the pool of LR-only TPOT-NN pipelines (hyperparameters have been removed for readabil-
ity):

7

          7 U D L Q L Q J  W L P H   G D \ V  6 K D O O R Z  V L Q J O H  H V W L P D W R U  1 1  V L Q J O H  H V W L P D W R U  7 3 2 7 7 3 2 7  1 1Romano et al

# Average CV score on the training set was: 0.9406477266781772
exported_pipeline = make_pipeline(

make_union(

StackingEstimator(estimator=make_pipeline(

StackingEstimator(estimator=PytorchLRClassifier(...)),
StackingEstimator(estimator=PytorchLRClassifier(...)),
PytorchLRClassifier(...) # LR3

# LR1
# LR2

)),
FunctionTransformer(copy) # Identity (skip)

),
PytorchLRClassifier(...) # LR4

)

The structure of this pipeline is virtually identical to a residual block—one of the major
innovations that has led to the success of the ResNet architecture. A graphical represen-
tation of this pipeline is shown in Figure 5. This suggests that AutoML could be used
as a tool for identifying new submodules for larger DL models. We discuss this possibility
further in §3.3.

Figure 5: Randomly selected pipeline learned when restricting TPOT’s pool of estimators
to logistic regression classiﬁers only. The structure of the pipeline resembles a residual
block—a major component of the ResNet DL architecture.

3. Discussion

3.1 Assessing the tradeoﬀ between model performance and training time

The amount of time needed to train a pipeline is an important pragmatic consideration in
real-world applications of ML. This certainly extends to the case of AutoML: The parame-
ters we use for TPOT include 100 training generations with a population size of 100 in each
generation, meaning that we eﬀectively evaluate 10,000 pipelines—each of which consists of
a variable number of independently optimizable operators—for every experiment (of which
there were 1,375 in the present study). As demonstrated in §2.3, we generally expect a
pipeline to train (in the case of ‘standard’ TPOT) in the range of several hours to slightly
over 1 day.

Our relatively simple MLP implementation (see §5.2) sits at the lower end—complexity
wise—of available deep learning estimators, and likewise is one of the fastest to train.
Regardless, including the MLP estimator in a TPOT experiment increases the average time
to learn a pipeline by at least 4-fold. Users will have to determine, on an individual basis

8

LR1LR2LR3LR4ClasspredictionsIdentity (skip)TrainingdataAutoML vs. Deep Learning

and dependent on the use case, whether the potential accuracy increase of at most several
percentage points is worth the additional time and computational investment.

Nonetheless, the data in Figure 2 demonstrate that it is unlikely for a TPOT-NN
pipeline to perform worse than a (non-NN) TPOT pipeline. In ‘mission critical’ settings
where training time is not a major concern, TPOT-NN can be expected to perform at least
as well as standard TPOT.

3.2 AutoML is eﬀective at recovering DL performance consistency

One of the most striking patterns in our results is that the NN-only models (both MLP
and LR) yielded highly inconsistent performance on several datasets, especially the two
“hill/valley” datasets (see Figure 2). However, this is unsurprising: these datasets contain
sequence data, where an estimator must be able to identify a ‘hill’ or a ‘valley’ that could
occur at any location in the sequence of 100 features. Therefore, the decision boundary of
an estimator over these data is being optimized over a highly non-convex objective function.
Standard DL optimization algorithms struggle with problems like these while heuristic and
ensemble methods tend to perform well more consistently, explaining the large diﬀerence in
classiﬁcation accuracy variance between NN-only and TPOT-NN experiments.

3.3 AutoML as a tool to discover novel DL architectures
Based on the results we describe in §2.4, AutoML (and TPOT-NN, in particular) may be
useful for discovering new neural network “motifs” to be composed into larger networks.
For example, by repeating the internal architecture shown in Figure 5 to a ﬁnal depth of
152 hidden layers and adjusting the number of nodes in those layers, the result is virtually
identical to the version of ResNet that won 1st place in 5 categories at two major image
recognition competitions in 2015 (He et al., 2016). In the near future, we plan to investigate
whether this phenomenon could be scaled into a larger, fully data-driven approach for
generating modular neural network motifs that can be composed into models eﬀective for
a myriad of learning tasks.

3.4 Future work on integrating AutoML and DL

Since one of our primary goals in this work was to provide a baseline for future devel-
opment of deep learning models in the context of AutoML, the two PyTorch models we
have currently built (logistic regression and MLP) are structurally simple. Future work on
TPOT-NN will allow expansion of its functionality to improve the capabilities of the exist-
ing models as well as incorporate other, more complex architectures, such as convolutional
neural networks, recurrent neural networks, and deep learning regressors.

4. Conclusions

AutoML and DL are immensely useful tools for approaching a wide variety of inductive
learning tasks, and it is clear that both hold strengths and weaknesses for speciﬁc use cases.
Rather than viewing them as competing methods, we instead propose that the two can
work synergistically: For at least the cases we explored in this study (classiﬁcation on 6
well-characterized datasets with relatively simple feature correlations), the addition of mul-

9

Romano et al

tilayer perceptron classiﬁers into the pool of available operators improves the performance of
AutoML. Since such learned pipelines often explicitly include feature selection and feature
transformation operators, they provide a feasible mechanism for improving interpretability
of models that make use of DL.

Currently, use of these DL estimators in TPOT signiﬁcantly increases training time
for pipelines, which likely will limit their applications in many situations. Nonetheless, this
suggests a multitude of novel directions for methodological research in machine learning and
artiﬁcial intelligence. TPOT-NN serves as both an early case study as well as a platform to
facilitate DL+AutoML research in a reproducible, transparent manner that is open to the
scientiﬁc community.

5. Methods

5.1 Datasets and experimental setup

We evaluated TPOT-NN on 6 well-studied publicly available datasets that were used pre-
viously to evaluate TPOT’s base implementation (i.e., no ANN estimators), shown in Ta-
ble 3. All datasets are contained in the PMLB Python package. Hill Valley with noise
and Hill Valley without noise consist of synthetic data; the rest are real data. The
number of data points, data types for each feature (i.e., binary, integer, or ﬂoating-point
decimal), number of features, and number of target classes are variable across the 6 datasets.

Parameter

Options Description

TPOT-NN enabled?

True

False

PyTorch LR and MLP estimators are enabled

Only non-NN estimators are enabled

Template pipeline?

True

Pipelines are restricted to the format:
Selector→Transformer→Classiﬁer

False

Pipeline structure is learned via GP

Included estimator(s) All

All estimators can be used in pipelines

TPOT enabled?

LR

MLP

True

False

Only logistic regression can be used

Only multilayer perceptron can be used

(Default)

GP only used for optimizing hyperparameters
(pipelines consist of a single estimator)

Table 2: Conﬁguration options used to construct the TPOT/TPOT-NN experiments in
this study. The “included estimator” and “TPOT enabled” parameters were used for testing
and validation of the TPOT-NN models.

We performed 720 TPOT experiments in total, corresponding to all combinations of the
conﬁguration parameters shown in Table 2. All conﬁgurations were run with 5 replicates on
each of the 6 datasets listed in Table 3, resulting in 30 experiments per conﬁguration. We

10

AutoML vs. Deep Learning

used an 80%/20% train/test split on the datasets and scored pipelines based on classiﬁcation
accuracy with 5-fold cross-validation.

Table 3: 6 datasets used to evaluate TPOT-NN. Names are identical to corresponding
labels used to denote the dataset in the PMLB Python library.

Dataset name

n Features Data type(s) Classes Real/synthetic

Hill Valley with noise
Hill Valley without noise
breast-cancer-wisconsin
car-evaluation
ionosphere
spambase

1212
1212
569
1728
351
4601

100 Float
100 Float
30 Float
21 One-hot
34 Mixed
57 Mixed

2
2
2
4
2
2

synthetic
synthetic
real
real
real
real

5.2 Baseline TPOT and NN estimators

For baseline (non-NN) TPOT experiments, we included all default operators from Scikit-
learn and XGBoost, and the default conﬁgurations for all trainable model parameters.

We constructed logistic regression (LR) and multilayer perceptron (MLP) models in
PyTorch to serve as neural network models. LR and MLP are largely considered the two
simplest neural network architectures, and are therefore suitable for initial evaluation of
new machine learning tools based on ANNs. Since a Scikit-learn LR model is included in
standard TPOT, we are able to directly compare the two LR implementations to validate
that the PyTorch models are compatible with the TPOT framework, and to quantify the
performance variation due to diﬀerences in the internal implementations of equivalent mod-
els. To allow for a similar comparison for MLP, we merged Scikit-learn’s MLP model into
TPOT, which has been omitted from the allowable operators in the past due to lengthy
training times and inconsistent performance compared to MLPs constructed using dedicated
deep learning libraries (such as PyTorch).

5.3 TPOT-NN

TPOT users can control the set of available operators—as well as the trainable parame-
ters and the values they can assume—by providing a ‘conﬁguration dictionary’ (a default
conﬁguration dictionary is used if the user does not provide one). We coded the new NN
estimators for TPOT within the main TPOT codebase, but provided a separate conﬁg-
uration dictionary that includes the NN estimators along with all default operators. We
wrote the new TPOT-NN models in PyTorch, but the TPOT-NN API could be adapted
to other neural computing frameworks. Since TPOT requires that all estimators imple-
ment an identical interface (compatible with Scikit-learn conventions for ﬁtting a model
and transforming data with the ﬁt model), we wrapped the PyTorch models in classes that
implement the necessary methods.

Users can also direct TPOT to utilize the NN models by providing a ‘template string’
instead of a conﬁguration dictionary. A generic template string for MLP might look like
“Selector-Transformer-PytorchMLPClassifier”, instructing TPOT to ﬁt a series of 3

11

Romano et al

operators (and their associated hyperparameters): any feature selector, followed by any
feature transformer, followed by an instance of the PyTorch MLP model. When no template
string is used, TPOT has the ability to learn pipelines with more complex structures.

5.4 Hardware and high-performance computing environment

All experiments were run on a high-performance computing (HPC) cluster at the University
of Pennsylvania. Each experiment was run on a compute node with 48 available CPU cores
and 256 GB of RAM. Job scheduling was managed using IBM’s Platform Load Sharing
Facility (LSF). All experiments involving PyTorch neural network estimators were run on
nodes equipped with NVIDIA® TITAN GPUs.

6. Code and Data Availability

TPOT-NN is a submodule included with the full TPOT Python distribution, which is
freely available on the Python Package Index (PyPI) and through GitHub [REF]. Due to
the substantially increased training time of the neural network models, users must explicitly
enable the use of the TPOT-NN estimators by passing the parameter config=’TPOT NN’
when instantiating a TPOT pipeline. The code we used to evaluate TPOT-NN is available
on GitHub in a separate repository [REF]. A frozen copy of all code, data, runtime output,
and trained models is also available on FigShare [REF].

7. Acknowledgements

This work was made possible through NIH grants T32-ES019851 (PI: Trevor Penning),
R01-LM010098 (PI: Moore), and R01-LM012601 (PI: Moore).

References

Peter Auer, Harald Burgsteiner, and Wolfgang Maass. Reducing communication for dis-
tributed learning in neural networks. In International Conference on Artiﬁcial Neural
Networks, pages 123–128. Springer, 2002.

Wolfgang Banzhaf, Peter Nordin, Robert E Keller, and Frank D Francone. Genetic pro-

gramming. Springer, 1998.

Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings
of the 22nd acm sigkdd international conference on knowledge discovery and data mining,
pages 785–794, 2016.

Ronan Collobert and Samy Bengio. Links between perceptrons, mlps and svms. In Pro-
ceedings of the twenty-ﬁrst international conference on Machine learning, page 23, 2004.

Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. A fast and elitist
multiobjective genetic algorithm: Nsga-ii. IEEE transactions on evolutionary computa-
tion, 6(2):182–197, 2002.

12

AutoML vs. Deep Learning

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for
image recognition. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016.

Trang T Le, Weixuan Fu, and Jason H Moore. Scaling tree-based automated machine
learning to biomedical big data with a feature set selector. Bioinformatics, 36(1):250–
256, 2020.

Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. nature, 521(7553):

436–444, 2015.

Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedfor-
ward networks with a nonpolynomial activation function can approximate any function.
Neural networks, 6(6):861–867, 1993.

Zachary C Lipton. The mythos of model interpretability. Queue, 16(3):31–57, 2018.

Yin Lou, Rich Caruana, and Johannes Gehrke.

Intelligible models for classiﬁcation and
regression. In Proceedings of the 18th ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 150–158, 2012.

Randal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, and Jason H. Moore. Evaluation
of a tree-based pipeline optimization tool for automating data science. In Proceedings of
the Genetic and Evolutionary Computation Conference 2016, GECCO ’16, pages 485–
492, New York, NY, USA, 2016a. ACM. ISBN 978-1-4503-4206-3. doi: 10.1145/2908812.
2908918. URL http://doi.acm.org/10.1145/2908812.2908918.

Randal S. Olson, Ryan J. Urbanowicz, Peter C. Andrews, Nicole A. Lavender, La Creis
Kidd, and Jason H. Moore. Applications of Evolutionary Computation: 19th European
Conference, EvoApplications 2016, Porto, Portugal, March 30 – April 1, 2016, Proceed-
ings, Part I, chapter Automating Biomedical Data Science Through Tree-Based Pipeline
Optimization, pages 123–137. Springer International Publishing, 2016b.
ISBN 978-3-
319-31204-0. doi: 10.1007/978-3-319-31204-0 9. URL http://dx.doi.org/10.1007/
978-3-319-31204-0_9.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blon-
del, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825–2830, 2011.

J¨urgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks, 61:

85–117, 2015.

Shaohuai Shi, Qiang Wang, Pengfei Xu, and Xiaowen Chu. Benchmarking state-of-the-art
deep learning software tools. In 2016 7th International Conference on Cloud Computing
and Big Data (CCBD), pages 99–104. IEEE, 2016.

13

Romano et al

Appendix A. Implementing TPOT-NN

TPOT-NN is implemented as a new feature within the existing TPOT software, and is
available for use in current releases of the software. TPOT operators consist of 3 main
types: feature selectors (e.g., ﬁltering out features with low variance), feature transformers
(e.g., min-max scaling), and estimators (e.g., any classiﬁcation or regression model). Prior
to TPOT-NN, all estimators were imported directly from the two third-party Python li-
braries Scikit-learn and XGBoost. We found that existing ‘plug-and-play’ neural network
estimators perform poorly in many case. Furthermore, one of the most powerful character-
istics of neural network models is their extreme ﬂexibility for deﬁning custom architectures
intended to solve speciﬁc analyses.

With this in mind, we implemented TPOT-NN as the following 2 components:

• An abstract base class (named PytorchEstimator) that adds a scikit-learn–like API
interface consistent to existing TPOT operators to user-deﬁned neural network models
implemented using PyTorch.

• Two simple PyTorch neural network models (a logistic regression classiﬁer and a
multilayer perceptron classiﬁer with 1 hidden layer) to serve as benchmarks for TPOT-
NN performance, both of which are used for evaluation of TPOT-NN in this study.

A.1 Using predeﬁned PyTorch estimators in TPOT-NN

PyTorch estimators are disabled by default, due to their lengthy training duration and
higher computational requirements. However, enabling them is done by simply initializ-
ing the TPOTClassifier object using a conﬁguration dictionary containing the PyTorch
estimators.

A.2 Deﬁning custom PyTorch estimators to use with TPOT-NN

New TPOT-NN estimators can be added by deﬁning a new Python class that inherits from
the PytorchClassifier mixin class. The new estimator should implement two components:

• An instance attribute named network that inherits from torch.nn.Module, deﬁning

the structure of the neural network.

• A method named init model() that assigns a number of attributes required for all

TPOT-NN estimators.

To enable the new estimator, the user should provide TPOT with a conﬁguration dic-
tionary containing the module path for the new estimator along with any trainable hyper-
parameters.

For the complete documentation, please refer to TPOT’s user guide (found at http:

//epistasislab.github.io/tpot/using/ at the time of writing).

14

