1
2
0
2

n
u
J

7

]

R
C
.
s
c
[

2
v
3
2
6
2
0
.
6
0
1
2
:
v
i
X
r
a

The Closer You Look, The More You Learn:
A Grey-box Approach to Protocol State Machine Learning

Chris McMahon Stone
University of Birmingham

Sam L. Thomas
University of Birmingham

Mathy Vanhoef
New York University Abu Dhabi

James Henderson
University of Birmingham

Nicolas Bailluet
ENS Rennes

Tom Chothia
University of Birmingham

Abstract
In this paper, we propose a new approach to infer state ma-
chine models from protocol implementations. Our method,
STATEINSPECTOR, learns protocol states by using novel pro-
gram analyses to combine observations of run-time mem-
ory and I/O. It requires no access to source code and only
lightweight execution monitoring of the implementation un-
der test. We demonstrate and evaluate STATEINSPECTOR’s ef-
fectiveness on numerous TLS and WPA/2 implementations. In
the process, we show STATEINSPECTOR enables deeper state
discovery, increased learning efﬁciency, and more insight-
ful post-mortem analyses than existing approaches. Further
to improved learning, our method led us to discover several
concerning deviations from the standards and a high impact
vulnerability in a prominent Wi-Fi implementation.

1 Introduction

Flaws in protocol state machines have led to major vulnerabil-
ities in many cryptographic protocols, ranging from TLS [18]
to WPA/2 [39]. Thus, the ability to extract these state ma-
chines from implementations and verify their correctness is
an attractive means to perform security analysis. Moreover,
since state machine models provide a succinct representation
of the inner workings of an implementation—summarising
tens of thousands of lines of code in a diagram that ﬁts on a
single page—they are also signiﬁcantly easier to reason about
and analyse than the implementation itself. Current state-of-
the-art approaches to extract state machines are built on active
model learning algorithms that descend from the work of An-
gluin on learning regular sets [4]. Angluin introduced an algo-
rithmic framework, the Minimally Adequate Teacher (MAT),
and the learning algorithm L*. These techniques were later
adopted for learning mealy-machine models [29, 34] and im-
plemented in the open-source library LearnLib [27, 32]. The
release of this library sparked a number of efforts to deploy
model learning for applications such as conformance testing,
legacy system inference, and most relevantly, security pro-
tocol analysis. In the security domain, it has been applied

to various protocols, including TLS [17, 18], SSH [22], Wi-
Fi [36], TCP [20], OpenVPN [16], and many others [2, 3, 37].

An inherent limitation of these works, which all perform a
type of black-box model inference, is that they rely entirely
on I/O observations. This means that any protocol behaviour
which does not manifest via I/O, or does so beyond the precon-
ﬁgured bound of the learning algorithm, cannot be extracted.
In this paper, we present a new paradigm for model learn-
ing which overcomes this limitation. Our technique examines
protocol behaviour “under-the-hood”, with respect to an im-
plementation’s concrete execution state and how it handles
I/O. As a result, we are able to learn models that capture
more behaviour and obtain insights beyond what is possible
by just considering I/O, allowing for easier model inspection
and understanding.

Our state machine learning method works in two stages. In
the ﬁrst stage, we capture snapshots of the implementation’s
execution context (i. e., memory and registers) whilst it runs
the protocol, which we then analyse to identify the locations
in memory that deﬁne the protocol state for each possible state.
In the second stage, we learn the complete state machine by
sending inputs to the protocol implementation and analysing
the identiﬁed locations to see which values the state deﬁning
memory takes, and so which protocol state has been reached.
This allows us to recognise each protocol state as soon as
we reach it, making learning more effective than previous
methods that require substantially more queries.

Similar to black-box learning, we are able to reason about
the correctness of our approach, in the sense that we can list
the assumptions that must hold for learning to terminate and
result in a correct state machine. We verify that our assump-
tions are reasonable and and realistic for analysing implemen-
tations of complex, widely deployed protocols, including TLS
and WPA/2. Further, we are able to demonstrate case studies
of protocols that contain difﬁcult to learn behaviour, that our
method is able to learn correctly, but state-of-the-art black-
box approaches cannot learn: either due to non-termination,
or because they output the incorrect state machine.

1

 
 
 
 
 
 
Table 1: Comparison of protocol model learning approaches. Style refers to active generation of inputs sequences, or passive
trace replays. Requirements denote: (P)rotocol Implementation, (A)bstraction functions for I/O, (T)races of protocol packets, and
(F)uzzer. Learned information categorised as input (I), input to protocol state (I2PS), and input to concrete state (I2CS).

Goals

Style

Requirements

P

A

T

F

Crypto-
protocols

What is learned?
I2PS
I

I2CS

State-classiﬁer

Black-box

G rey-box

MODEL
LEARNING [18, 36]
PULSAR [23]

Model extraction

Active

State-aware fuzzing

Passive

PROSPEX [14]
MACE [11]
AFLNET [30]

Protocol RE
Input discovery
State-aware fuzzing
STATEINSPECTOR Model extraction

Passive
Active
Active
Active

Contribution Summary:

• We present the design and implementation of a new
model inference approach, STATEINSPECTOR, which
leverages observations of run-time memory and program
analyses. Our approach is the ﬁrst which can map ab-
stract protocol states directly to concrete program states.

• We demonstrate that our approach is able to learn states
beyond the reach of black-box learning, while at the
same time improve learning efﬁciency. Further, the mod-
els we produce allow for novel, post-mortem memory
analyses not possible with other approaches.

• We evaluate our approach by learning the models of
widely used TLS and WPA/2 implementations. Our ﬁnd-
ings include a high impact vulnerability, several concern-
ing deviations from the standards, and unclear documen-
tation leading to protocol misconﬁgurations.

To facilitate future research, we make STATEINSPECTOR and
its test corpus publicly available as open source [1].

We organise the paper as follows: we ﬁrst provide the nec-
essary background on model learning and binary analysis,
then discuss the limitations of prior work and present a set
of guiding research questions. Next, we present a high-level
overview of our method, STATEINSPECTOR, and the assump-
tions we make, and follow it with a more in-depth discussion
of our algorithm and learning framework. We then provide a
thorough evaluation with respect to the current state of the art,
and our research questions, for a number of TLS and WPA/2
implementations. Finally, we discuss the validity of our as-
sumptions and the implications if they do not hold.

2 Background

2.1 Model Learning

The models we learn take the form of a Mealy machine. For
protocols, this type of model captures the states of the system
which can be arrived at by sequences of inputs from a set I,

I/O

I/O

Control-Flow-I/O
I/O
I/O
Memory

and trigger corresponding outputs from a set O. We consider
Mealy machines that are complete and deterministic. This
means that for each state, and input i ∈ I, there is exactly one
output and one possible succeeding state.

Mealy machine learning algorithms include L∗ [4, 29, 34]
and the more recent space-optimal TTT algorithm [26]. In
essence, all of these algorithms work by systematically posing
output queries from I+ (i. e., non-empty strings of messages),
each preceded by a special reset query. These queries are
mapped from abstract strings to concrete messages by an or-
acle, and then forwarded to the System Under Test (SUT).
Each response from the SUT is then mapped back to an ab-
stract output o ∈ O for each preﬁx of the input message string,
i. e., p ∈ I+. Once a hypothesis which achieves speciﬁc prop-
erties, namely completeness and closedness, has been built,
a secondary procedure begins where equivalence queries are
posed to the teacher. These queries attempt to ﬁnd counter-
examples to reﬁne the hypothesis. In black-box settings, these
can be generated in various ways, including random testing, or
more formally complete methods like Chow’s W-Method [12].
Unfortunately, using the W-Method is highly expensive, es-
sentially requiring an exhaustive search of all possible input
sequences up to a speciﬁed bound. Naturally, a higher bound
increases the number of queries required to learn a state ma-
chine and any states beyond this bound cannot be learned.
In order to strike a trade-off between ensuring learning is
tractable and obtaining representative model, selection of this
bound requires careful consideration. Learned models can be
analysed for interesting behaviour either manually, or auto-
matically, e. g., by using formal model checking [20, 22].

2.2 Binary Program Analysis

Our method does not require access to the source code of the
SUT. Instead, we perform our analyses at the level of assembly
language instructions, registers, and program memory. We
base our analyses on two techniques: taint propagation and
symbolic execution. Both methods compute properties of
memory locations and registers along an execution path.

2

Taint propagation tracks the inﬂuence of tainted variables
by instrumenting code and tracking data at run time. Any
new value that is derived from a tainted value will also carry
a taint, for example, if variable A is tainted and inﬂuences
variable B, and variable B in turn inﬂuences C, then variable C
is also tainted meaning it is (indirectly) inﬂuenced by A.

Symbolic execution computes constraints over symbolic
variables based on the operations performed on them. For
instance, suppose A is marked as symbolic, and a variable B
is derived from A using some expression B=f(A), then sym-
bolic execution will track this relation between both variables.
More importantly, if we encounter a branch condition (e. g.,
if-then-else) that depends on a symbolic variable, we can
query an SMT solver to obtain suitable assignments that ex-
plore all feasible branches.

3 Motivation and Related Work

In Table 1, in addition to STATEINSPECTOR, we summarise
ﬁve key approaches whose central aim is to learn a model of
the state machine implemented by a protocol. Each approach
learns this model for different reasons (i. e., fuzzing, reverse-
engineering, or conformance testing), and their assumptions
also differ (i. e., black or grey-box access, known or unknown
target protocols, and active or static trace-based learning). In
this section, we discuss the fundamental limitations of these
key approaches, and demonstrate the need for a new direction.
In doing so, we relate other works whose contributions also
bear noteworthy relevance. We ﬁnish by presenting a set of
research questions that serve as guiding principles for the
design and evaluation of our proposal.

Limited Coverage Learned model coverage in passive trace-
based learning approaches (e. g., [14, 23]) is entirely deter-
mined by the quality of pre-collected packet traces, and con-
sequently is most appropriate for learning unknown protocols.
On the other hand, since active methods can generate proto-
col message sequences on-the-ﬂy, their coverage capability
is self-determined. All state-of-the-art active methods (black-
box [17, 18, 21, 22, 36] and grey-box [11, 30]) use automata
learning algorithms (e. g., L* or TTT) for their query genera-
tion and model construction, and such algorithms are known
to have two key limitations. First, queries to a target system
are constructed with a ﬁxed, pre-deﬁned set of inputs (i. e.,
protocol messages). This means that behaviour triggered with
inputs outside of this set cannot be explored. To help allevi-
ate this issue, two approaches integrating automata learning
with grey-box analyses have been proposed—MACE [11],
which uses concolic execution, and AFLnet [30], which uses
fuzzing. Despite some success when applied to text-based
protocols, these and other existing model learning techniques
suffer from a second issue, namely, limited exploration with
respect to the depth of the state machine.

Consider the state machine in Figure 1. In this simple proto-

0

auth / error
data / error
close / error

init / ack

1

auth / ack

init / ack
x11

auth / error
data / error
close / error

data / error
close / error

auth / ack
data / ack

2

init / error

3

auth / error
data / error
init / error

close / close

-

close / close

all / -

Figure 1: State machine with a deep-state backdoor that can
only be activated by sending 12 consecutive init messages.

col, a backdoor exists that allows us to transition from state 1
(not authenticated) to state 2 (authenticated) by supplying an
input of 11 init messages, as opposed to an auth message1.
To learn this state machine, state-of-the-art model learning
with the TTT-Algorithm [26] and modiﬁed W-Method [18]
requires ~360k queries to discover the backdoor, but fails to
terminate after 1M queries—essentially operating by brute-
force. This is because the number of queries posed is polyno-
mial in the number of messages, states and exploration depth.
Moreover, to identify this backdoor, we must set a maximum
exploration depth of at least 11. In practice, however, since
query complexity explodes with a high bound, most works
use a much lower bound (e. g., 2 in [18, 22]), or use random
testing which lacks completeness. This query explosion is
also exacerbated by large input sets. Hence, for systems where
time-spent-per-query is noticeable (e. g., a few seconds, as
is the case with some protocols), it is apparent that learning
such systems requires a more efﬁcient approach.

Limited Optimisations Similar to the problem of limited
coverage, current approaches also lack sufﬁcient information
to optimise learning. That is, I/O observations do not enable
learning effort to be focused where it is likely to be most
fruitful. Consider de Ruiter et al.’s application of black-box
learning to RSA-BSAFE for C TLS [18]. Their learner cannot
detect that the server does not close the connection upon
handshake termination, and, thus, exhausts its exploration
bound, posing tens of thousands of superﬂuous queries.

Prospex’s grey-box approach [14] has the potential to avoid
such an issue through its alternative modelling of states based
on execution traces. However, it cannot capture states that
only manifest as changes to memory, e. g., counter increments
per message read, and its coverage is inherently limited due to
the use of trace-replays. Consequently, like MACE [11] and

1This backdoor is similar to a ﬂaw found in a WPA/2 implementation [36],

which permitted a cipher downgrade after 3 failed steps of the handshake.

3

AFLnet [30], it is not applicable to protocols with complex
state or replay-protection (i. e., security protocols).
Limited Insight A further downside of methods that infer
models based exclusively on I/O is that they may result in
an over-abstraction. This can impede an analyst in obtaining
a sufﬁciently detailed understanding of the implementation.
Additionally, as active learning methods produce models that
depend on how they map between concrete and abstract mes-
sages, their learned models will be inaccurate if the mapping
is incorrect. This mapping is done by a test harness, which is a
highly ﬂexible implementation of the target protocol (denoted
by P in Table 1). The harness must be able to send and inter-
pret protocol messages in an arbitrary order. However, this is
a complex task, and the developer of the harness often has to
make (undocumented) decisions on how to handle unexpected
or malformed messages. As a result, accurately interpreting
the resulting I/O model can be difﬁcult to almost impossible,
potentially making the resulting model effectively incorrect.
This downside is exempliﬁed in a black-box analysis of TLS
by de Ruiter et al. [18]. In particular, the author’s acknowl-
edge that their state machines for OpenSSL 1.0.1g and 1.0.1j
are incorrect due to differing assumptions between the imple-
mentation and their test harness. Unfortunately, determining
whether a resulting model is incorrect due to a ﬂawed test
harness, or whether there is in fact a bug in the tested im-
plementation, is impossible based on the I/O model alone.
Instead, an expert must manually analyse both the harness
and implementation to determine this. Thus, I/O models alone
provide limited insight, which extra grey-box information can
help overcome.

We note that some works extend black-box learning with
taint analysis to learn register automata for protocols [25, 33].
Such automata are more insightful than Mealy machines,
since they attempt to model conditions on state data for tran-
sitions between states. Unfortunately, these proposals assume
access to source code and do not scale to real-world protocols.
Research Questions Based on the above limitations of cur-
rent works, we use the following research questions to guide
the design of our new grey-box approach, STATEINSPECTOR:

RQ1: Can we expand the capability of model learning to lo-
cate deep states typically beyond reach of current approaches,
especially when learning with large input sets?

RQ2: Can we exploit grey-box information in order to apply
more targeted learning, resulting in efﬁciency gains?

RQ3: Can we employ observations of internal SUT behaviour,
e. g., API usage or memory classiﬁcations of states, for more
insightful analysis of the inferred I/O models?

4 Overview

the SUT (provided by the execution monitor and concolic
analyser) with observations of its I/O behaviour (provided
by the test harness) to learn models that provide comparable
guarantees to traditional black-box approaches. We organise
this section by ﬁrst stating the assumptions we make about
the state machines our approach learns, and then provide an
overview of its operation. So that the reader has a clear under-
standing of all assumptions we make of the implementations
we analyse, we also state assumptions about the conﬁgurable
parameters of our method. We defer discussion of the choice
of their concrete assignments, and how the assumptions can be
loosened (and the implications of doing so) to later sections,
so that they can be understood in context.

Our ﬁrst assumption is standard for state machine learning:

Assumption 1. The protocol state machine of the SUT is ﬁnite,
and can be represented by a mealy machine.

In all concrete implementations, each state of this mealy
machine will correspond to a particular assignment of values
to speciﬁc “state-deﬁning” memory locations, allocated by the
implementation. We use two terms to discuss these locations
and the values assigned to them, which we deﬁne as follows:

Deﬁnition 1 (Candidate State Memory Location) Any
memory location that takes the same value after the same
inputs, for any run of the protocol.

Deﬁnition 2 (A Set of State-Deﬁning Memory) A minimal
subset of candidate state memory locations whose values
during a protocol run uniquely determine the current state.

For simplicity, we also refer to a member of any state-
deﬁning memory set as state memory. Our next assumption
is that we know a priori the complete language needed to
interact with the SUT and hence generate the state machine:

Assumption 2. All protocol states can be reached via queries
built up from the inputs known to our testing harness I+.

Under these assumptions, we now describe the operation of
our approach. Our learner uses a test harness to interact with
the SUT. This test harness is speciﬁc to each protocol analysed
and tracks session state, and is responsible for communicating
with the SUT and translating abstract representations of input
and output messages (for the learner) to concrete representa-
tions (for the SUT). In the ﬁrst phase of learning, the learner
instructs the test harness to interact with the SUT to exer-
cise normal protocol runs, trigger errors, and induce timeouts
multiple times. We capture snapshots of the SUT’s execution
context (i. e., its memory) for each of these runs using an
execution monitor. We perform subsequent analysis of the
snapshots in order to determine a set of candidate memory
locations that can be used to reason about which state the SUT
is in. We make the following assumption about this memory:

We depict a high-level overview of our approach in Figure 2.
Our method combines insights into the runtime behaviour of

Assumption 3. A set of state-deﬁning memory for all states
is allocated along a normal protocol run, and this memory

4

Output queries

Test harness

I/O channel

Learner

SUT

Snapshots & watchpoint queries

Debug channel

API interceptor

Type & memory
classiﬁcations

Concolic analyser

Watchpoint handler

Figure 2: Grey-box State Learning Architecture.

Black-box component

Grey-box component

Execution monitor

Input/output

will take a non-default value in the normal run, during error
handling, or timeout routines.

Any memory location found to have the same value for each
sequence of inputs for each run is considered candidate state
memory. And any location that takes the same value in all
states is discarded. Assumption 3 ensures that this reduced set
of locations will be a set of state-deﬁning memory, however,
this set may also contain superﬂuous locations. Further, this
set of locations may not be state-deﬁning for every state, and
is rather a superset of locations for all states.

The next phase of learning uses the candidate set to con-
struct a model of the protocol state machine. During this
process, we identify the set of state-deﬁning memory for each
state. This allows us to determine if two states arrived at from
the same input sequence should be considered equal, even if
their memory differs, as we only need to consider the values
of state-deﬁning locations when performing this so-called
equivalence check. It also allows us to effectively ignore any
superﬂuous locations in our candidate set since they will not
be considered state-deﬁning for any state.

To learn the state machine, we queue all queries from I+ to
fulﬁll the completeness property for states in a mealy machine.
Namely, that all inputs are deﬁned for each state. We perform
these queries iteratively in increasing length. As in the ﬁrst
phase, we take snapshots of the execution state of the SUT on
each I/O action. Each time a distinct state, according to its I/O
behaviour and deﬁned state memory, is discovered, we queue
additional queries to ensure that the state is also fully deﬁned
(i. e., we attempt to determine all states reachable from it).
We continue building the automaton in this way until no new
states are found.

As well as state memory, it is possible that our candidate
set contains locations with assignments that appear to be state-
deﬁning for all states identiﬁed from the set of posed inputs,
but are actually not. For instance, if a protocol implementa-
tion maintains a counter of the messages received, it might
be mistaken for state memory, leading to a self loop in the
state machine being conﬂated with an inﬁnite progression of
seemingly different states. Recognising such memory will
prevent our framework from mistaking a looping state from
an inﬁnite series of states, and so ensures termination. To
determine if such memory exists and if it can be ignored, we

perform a merge check. This allows us to replace a series of
states repeated in a loop, with a single state and a self loop.
This check is only performed between states that have the
same observed input and output behaviour, and are connected
at some depth (i. e., one is reachable from the other). We make
the following assumption about this depth:

Assumption 4. The depth that we check for possible merge
states is larger than the length of any loop in the state machine
of the implementation being tested.

When performing a merge check, we consider two states
equal if the values assigned to their state-deﬁning locations
are equal. Therefore, for each location that differs, we must
determine if it behaves as state memory for each state (we
discuss the conditions for this in Section 5.5). To do so, we
use a novel analysis (implemented in the concolic analyser
in Figure 2). This analysis identiﬁes a memory location as
state-deﬁning when this location inﬂuences decisions about
which state we are in. Concretely, this is encoded by checking
if the location inﬂuences a branch that leads to a write to any
candidate state memory. That is, state-deﬁning memory will
control the state by directly controlling writes to (other) state
memory, and non-state-deﬁning memory will not. This leads
to our ﬁnal assumption:

Assumption 5. Any state-deﬁning memory will control a
branch, along an execution path triggered by messages from
the set I+, and it will lead to a write to candidate state mem-
ory within an a priori determined number of instructions.

We discuss the selection of the above bound in Sec-
tion 5.5.2, but note that it is a conﬁgurable parameter of our
algorithm. We complete learning when no further states can
be merged, and no new states (that differ in I/O behaviour or
state memory assignment) can be discovered by further input
queries. At this point, STATEINSPECTOR outputs a represen-
tative mealy machine for the SUT.

5 Methodology

The learner component of Figure 2 orchestrates our model
inference process. It learns how the SUT interacts with the

5

world by observing its I/O behaviour via the test harness, and
learns how it performs state-deﬁning actions at the level of
executed instructions and memory reads and writes, via the
execution monitor. Inference begins with a series of bootstrap
queries in order to identify candidate state memory, i. e., M.
All queries posed to the SUT in this stage serve the dual pur-
pose of identifying state memory and also reﬁning the model.
This ensures minimal redundancy and repetition of queries.
Given an estimation of M, we then proceed to construct the
state machine for the implementation by identifying each
state by its I/O behaviour and set of state-deﬁning memory.
Throughout this process we perform merge checks which de-
termine if two states are equivalent with respect to the values
of their state-deﬁning locations.

5.1 Monitoring Execution State

To monitor a SUT’s state while it performs protocol related
I/O, we take snapshots of its memory and registers at care-
fully chosen points. As many protocols are time-sensitive, we
ensure that this monitoring is lightweight to avoid impacting
the protocol’s behaviour. More speciﬁcally:

1. We avoid inducing overheads such that the cost of each

query becomes prohibitively high.

2. We do not interfere with the protocol execution, by, for

example, triggering timeouts.

3. We only snapshot at points that enable us to capture each

input’s effect on the SUT’s state.

To perform a snapshot, we momentarily pause the SUT’s
execution and copy the value of its registers and contents
of its mapped segments to a buffer controlled by the execu-
tion monitor. To minimise overheads, we buffer all snapshots
in memory, and only ﬂush them to disk after the SUT has
ﬁnished processing a full query. We ﬁnd that the overheads
induced by snapshotting in this way are negligible and do not
impact the behaviour of any of the protocols analysed.

To identify when to perform snapshots, we infer which
system calls (syscalls) and ﬁxed parameters a SUT uses to
communicate with its client/server. We do this by generating
a log of all system calls and passed parameters used during a
standard protocol run. We then identify syscall patterns in the
log and match them to provided inputs and observed outputs.
The number of syscalls for performing I/O is small, hence the
number of patterns we need to search for is also small, e. g.,
combinations of bind, recvmsg, recvfrom, send, etc. When
subsequently monitoring the SUT, we hook the identiﬁed
syscalls and perform state snapshots when they are called
with parameters matching the those in our log.

5.1.1 Mapping I/O Sequences To Snapshots

I/O events) and our execution monitor (which logs snapshot
events). We construct a mapping by aligning the logged events
from each component, as depicted in Figure 3. When doing
so, we face two key difﬁculties. First, implementations may
update state memory before or after responding to an input,
hence we must ensure we take snapshots to capture both
possibilities (i. e., case (cid:184) in Figure 3). Second, some parts of
a query may not trigger a response, hence we must account
for the absence of write-like syscalls triggering a snapshot for
some queries (i. e., cases (cid:183) and (cid:185)).

(cid:182)

(cid:183)

(cid:184)

(cid:185)

Figure 3: Mapping I/O sequences to snapshots. We depict
four cases (cid:182)-(cid:185), snapshot events are shown on the left and I/O
on the right. We indicate read/input events in red, write/output
events in blue, state changes in yellow, and connection close
events in green. (cid:182) shows the case where snapshots and state
changes are trivially aligned with I/O. (cid:183) shows the case where
a state change occurs without a corresponding output event. (cid:184)
shows the case where state changes occur after output events.
(cid:185) shows the case where a state change occurs after the last
input event, with no output before the connection is closed.

For all scenarios, we trigger input event snapshots after
read-like syscalls return, and output event snapshots just be-
fore write-like syscalls execute. We take additional snapshots
for output events before each read-like syscall. This enables
us to always capture state changes, even if no corresponding
write-like syscall occurs, or the state change happens after the
output is observed.

If there is no output to the ﬁnal input of a query q, then we
instruct the learner to pose an additional query with an extra
input, q||i for all i ∈ I. We expect there to be a read event
corresponding to the ﬁnal input of one of these queries—thus
providing us with a snapshot of the state after processing the
penultimate input. If the SUT does not perform such a read,
i. e., it stops processing new input, as in case (cid:185), we detect this
by intercepting syscalls related to socket closure, and inform
the learner that exploration beyond this point is unnecessary.

5.2

Identifying Candidate State Memory

In the ﬁrst stage of learning, our objective is to identify a set
of possible memory locations whose values represent state
(Deﬁnition 1), i. e., M. Our analyses aim to ﬁnd M such that
the only locations it contains are values that can be used to
discern if we are in a given state, for all possible protocol
states that the SUT can be in.

To map I/O sequences to snapshots, we maintain a monotonic
clock that is shared between our test harness (which logs

Snapshot Generation To form an initial approximation of
M, we perform bootstrap queries against the SUT, this pro-

6

(cid:182)

(cid:183)

(cid:184)

Figure 4: Allocation alignment across different executions.
Each large block represents the memory layout of a differ-
ent protocol run; coloured squares represent state-deﬁning
locations. Allocation alignment computes a mapping of allo-
cations of a given run (blue squares) to a base conﬁguration
(red squares). Using this mapping, we can diff snapshots that
have different conﬁgurations, e. g., (cid:182) and (cid:184), by ﬁrst mapping
them onto a common conﬁguration, i. e., (cid:183).

duces a set of memory snapshots where all of the memory
which tracks state is deﬁned and used (Assumption 3). Our
bootstrap queries take the form, BF = {b0, b1, ...bn}, where
bi ∈ I+. The ﬁrst of these queries b0 is set as the happy ﬂow of
the protocol. This is the expected normal ﬂow of the protocol
execution, which we assume prior knowledge of. For exam-
ple, in TLS 1.2, b0 = (ClientHelloRSA,ClientKeyExchange,
ChangeCipherSpec, Finished). The other queries in BF are
speciﬁc mutations of this happy ﬂow, automatically con-
structed with the intention of activating error states, timeout
states, and retransmission behaviour. Each of these queries
bx is derived by taking all preﬁxes px of the happy ﬂow b0,
where 0 < |px| ≤ |b0|, and for all inputs in I, and appending to
px each i ∈ I a ﬁxed number of times T such that bx = px||iT .
Every bootstrap query is executed at least twice, so that we
have at least two snapshots for each equivalent input sequence,
which we require for the next step of our analysis. When
compared to black-box approaches, one might assume that
our method requires many more queries in order to facilitate
learning. This is not the case. In fact, once we identify a
SUT’s state memory, we can reuse all of the bootstrap queries
for reﬁning our model in subsequent phases.

When possible, we also attempt to alternate functionally
equivalent input parameters to the SUT across bootstrap ﬂows.
We do this to maximise the potential number of locations we
eliminate due to not holding the same value at equivalent
states (Assumption 3). For example, for TLS implementa-
tions, we execute some bootstrap ﬂows with different server
certiﬁcates, which enables us to eliminate memory that would
otherwise have to be identiﬁed as non-state-deﬁning using our
concolic analyser—a comparatively heavyweight procedure.

Handling Dynamic Memory To facilitate handling multiple
simultaneous sessions, client/server implementations gener-
ally allocate state memory on-demand. This presents a chal-
lenge when identifying state-deﬁning locations, as logically
equivalent allocations may not reside at the same offsets
across different protocol executions. To address this non-
determinism, we compute a mapping of each execution’s
allocations to a single base conﬁguration. This enables us

to analyse all snapshots together with respect to a common
conﬁguration, rather than pair-wise. Figure 4 visualises our
approach. We ﬁrst construct an allocation log for each execu-
tion that records a timestamp, the stack pointer (i. e., callee
return address), and call parameters, of every call to a memory
allocation function (e. g., malloc and free). Then, to derive
a mapping between two logs, we align their allocations/frees,
by matching them on their log position, allocation size, and
calling context (recorded stack pointer). We choose our base
conﬁguration, or log, as the largest happy ﬂow log, under
the assumption that it will contain all allocations related to
state-deﬁning locations for any possible session.

Snapshot Difﬁng Following mapping each bootstrap snap-
shot’s allocations onto the base log, we diff them to obtain
our candidate set M. Each element m ∈ M corresponds to the
location of a contiguous range of bytes of dynamically allo-
cated memory, which we represent by: an allocation address,
an offset relative to the start of the allocation, and a size.

We perform difﬁng by ﬁrst grouping all snapshots by
their associated I/O sequences. Then, for each group, we
locate equivalent allocations across snapshots and identify
allocation-offset pairs which refer to byte-sized locations with
the same value. We then check that every identiﬁed location
also contains the same value in every other I/O equivalent
snapshot, and is a non-default value in at least one snapshot
group (Assumption 3). This gives us a set of candidate state
memory locations. We note that as this process is carried out
at the byte level, we additionally record all bytes that do not
abide by this assumption. This enables us to remove any mis-
classiﬁed bytes once we have established the real bounds of
individual locations (Section 5.3.1).

5.3 Minimising Candidate State Memory

Given our initial set of candidate state memory locations, we
reduce M further by applying the following operations:

1. Pointer removal: we eliminate any memory containing
pointers to other locations in memory. We do this by
excluding values which fall within the address-space of
the SUT and its linked libraries.

2. Static allocation elimination: we remove any full allo-
cations of memory which are assigned a single value in
the ﬁrst snapshot which does not change throughout the
course of our bootstrap ﬂows.

3. Static buffer elimination: we remove any contiguous
byte ranges larger than 32 bytes that remain static.

Static allocation and buffer elimination are used to ﬁlter
locations corresponding to large buffers of non-state-deﬁning
memory, for example, in OpenSSL, they eliminate locations
storing the TLS certiﬁcate.

7

1

appdata /
empty

2

cke /
empty

3

appdata /
empty

cke /
empty

cke /
closed

appdata /
empty

4

5

6

7

Figure 5: An example scenario where merge conditions for
states 1 and 2 are met, when D is set to 1. The state learner
would as such taint test any differing memory between the
two states to estimate if this memory is state deﬁning.

5.3.1 Candidate State Memory Type-Inference

The values stored at state-deﬁning locations are often only
meaningful when considered as a group, e. g., by treating four
consecutive bytes as a 32-bit integer. Since we perform snap-
shot difﬁng at a byte-level granularity, we may not identify
the most-signiﬁcant bytes of larger integer types if we do not
observe their values changing across snapshots. As learning
the range of values a location can take is a prerequisite to
determine some kinds of termination behaviour, we attempt
to monitor locations with respect to their intended types.

To learn each location’s bounds, we apply a simple type-
inference procedure loosely based upon that proposed by
Chen et al. [9]. We perform inference at the same time as we
analyse snapshots to handle uncertain state memory locations
(Section 5.5). During this analysis, we simulate the SUT’s
execution for a ﬁxed window of instructions, while doing
so, we analyse loads and stores from/to our candidate state
memory. To perform inference, we log the preﬁxes used in
each access, e. g., in mov byte ptr [loc_1], 0, we record
byte for loc_1. Then, following our main snapshot analysis,
we compute the maximal access size for each location and
assign each location a corresponding type. To disambiguate
overlapped accesses, we determine a location’s type based
on the minimal non-overlapped range. For example, if we
observe that loc_1 and loc_1+2 are both accessed as 4-byte
values, we compute loc_1’s type as two bytes and loc_1+2
as four bytes. Following type-inference, we update our model
and the state classiﬁcations maintained by our learner using
the new type-bounds discovered.

5.4 State Merging

We consider a unique assignment of candidate state memory a
unique state (Deﬁnition 2). Hence two states reachable by the
same inputs, with equivalent assignments for these locations

8

can be considered equal and merged. However, by applying a
trivial equality check, if M is not minimal, i. e., it is an over-
approximation, this check may not yield the correct outcome.
Which, in the worst case could lead to non-termination of
learning. We therefore must address this to ensure learning
is possible in all cases with respect to Assumption 1, namely
that the state machine being learned is ﬁnite. The method we
present in this section handles this possibility by allowing for
states to be merged under the assumption that our candidate
set contains superﬂuous locations. We attempt to merge states
in the model each time all queries in the queue of a given
length have been performed. We call this the merge check. It
operates by identifying pairs of base and merge states, which
can be merged into a single state. These pairs are selected
such that two properties hold: (cid:182) the merge state must have
I/O equivalence to the base, and (cid:183) the merge state should be
reachable from the base. I/O equivalence signiﬁes all input-
output pairs are equal for the two states, to a depth D. The
intuition is that if I/O differences have not manifested in D
steps, they may in fact be the same state, and we should there-
fore check if the differences in their memory are state relevant.
The base-to-merge state reachability must also be possible
in D steps (Assumption 4). We enforce this property based
on the observation that in security protocol implementations,
state duplication is more likely with states along the same
path. So-called loopback transitions often occur where inputs
are effectively ignored with respect to the state, however their
processing can still result in some changes to memory, result-
ing in duplication for our memory classiﬁed states. Loops in
models between multiple states are less common, but will be
checked by our learner provided the number of states involved
is less than or equal to D.

In Figure 5, we present an example of where the merge-
ability properties hold, with D = 1 for states 1 and 2—they
are I/O equivalent to depth 1, and state 2 is reachable from
state 1. Our merge check determines whether the memory that
distinguishes the states is state-deﬁning. Algorithm 1 (see Ap-
pendix) shows a sketch of our approach. In summary, for each
location of differing memory, and for each input message (to
ensure completeness), we monitor the execution of the SUT.
We supply it with input messages to force it into the state
we wish to analyse (i. e., state 1 or 2 in our example in Fig-
ure 5), we then begin to monitor memory reads to the tracked
memory location. We follow this by supplying the SUT with
each input in turn and perform context snapshots on the reads
to the memory location, which we call a watchpoint hit. We
then supply each of these snapshots as inputs to our concolic
analyser (detailed in the next section), which determines if the
SUT uses the value as state-deﬁning memory (Assumption 5).
If our analysis identiﬁes any location as state deﬁning, then
we do not perform a merge of the two states. Conversely, if
all tested locations are reported as not state-deﬁning, then we
can merge the merge state into the base state, and replace the
transition with a self loop.

5.5 Handling Uncertain State Memory

Forming our candidate set M by computing the differences
between snapshots gives us an over-approximation of all of
the locations used discern which state the SUT is in. However,
an implementation may not use all of these locations to decide
which state it is in for every state. As described in the previ-
ous section, when testing if two memory conﬁgurations for
the same I/O behaviour should be considered equivalent, we
must identify if differing values at candidate locations imply
that the conﬁgurations correspond to different states (i. e., the
differing locations are state-deﬁning), or if the values they
take are not meaningful for the particular state we are check-
ing. For a given state, by analysing how locations actually
inﬂuence execution we can easily tell state-deﬁning locations
from those that are not, as in all cases, non-state-deﬁning lo-
cations will have no inﬂuence on how state is updated. Since
this kind of analysis is expensive, the difﬁng phase is crucial
in minimising the number of candidate locations to analyse—
typically reducing the number to tens, rather than thousands.

5.5.1 Properties Of State-Deﬁning Memory

To conﬁrm a given location is state-deﬁning, we attempt to
capture execution traces of its location behaving as state-
deﬁning memory. We summarise these behaviours below,
which we base on our analysis of various implementations:

1. Control-dependence: writes to state memory are
control-dependent on reads of state memory. To illustrate
this, consider the typical case of a state enum ﬂag read
forming the basis of a decision for which code should
process an incoming message, and the resulting state
machine transition deﬁned by a write to state memory.

2. Data-dependence: non-state memory that is condition-
ally written to due to dependence on a read from state
memory may later inﬂuence a write to state memory.

3. State-deﬁning and state-inﬂuential locations: for a
particular state, its state-deﬁning memory locations (Def-
inition 2) will always be read from before they are writ-
ten to (to tell which state one is in), and subsequent
writes to state memory will be control- or data-dependent
upon the values of those reads (Assumption 5). For state-
inﬂuencing locations, e. g., input buffers, this property
will not hold—while the contents of a buffer may inﬂu-
ence state, it will not directly deﬁne it, and will be written
to before being read.

5.5.2 Discerning State-Deﬁning Locations

If a location holds more than one value when performing a
merge check, we apply an additional analysis to determine if
it is state-deﬁning. First, we identify execution paths that are
control-dependent on the value stored at the location. Then,

(cid:182) verify result
stored in eax

(cid:185) perform state
change

(cid:183) check

backdoor depth

(cid:184) skip check of
verify result

Close
Connection

Figure 6: OpenSSL client veriﬁcation bypass. At (cid:182) we check
the client certiﬁcate signature, at (cid:183) we check if the read_seq
counter is equal to 11, if so, we bypass the check of the result
of client certiﬁcation veriﬁcation via (cid:184). At (cid:185) we perform a
state update that is control-dependent on read_seq’s value.
STATEINSPECTOR identiﬁes the dependence and dynamically
adjusts the learning depth to discover the deep-state change.

we check if any of those paths induce a write to known state
memory. If so, we classify the location as state-deﬁning.

We base our analysis on a variation of byte-level taint prop-
agation combined with concolic execution. We apply it in
sequence to state snapshots taken on reads of the candidate
location addr. We start analysis from the instruction pc per-
forming the read that triggered the state snapshot. Our analy-
sis proceeds by tainting and symbolising addr, then tracing
forwards until we reach a branch whose condition is tainted
by addr. If we do not reach a tainted conditional within W
instructions, we do not continue analysing the path. At the
conditional, we compute two assignments for the value at
addr—one for each branch destination. We then symbolically
explore each of the two paths until we have processed W in-
structions or we reach a return instruction. If we observe a
write to known state memory on either path, we consider addr
to be state-deﬁning. Figure 6 depicts an example location that
requires such analysis. The counter read_seq is used to im-
plement a sneaky backdoor that bypasses client certiﬁcate
veriﬁcation in OpenSSL. Our analysis ﬁnds that read_seq
taints the branch jz update_state, and thus, leads to a state
change. We therefore classify it as state-deﬁning.

We select an analysis bound of W = 512 based on the ob-
servation that reads from memory involved in comparisons
tend to have strong locality to the branches they inﬂuence.
Since we analyse multiple state snapshots for each addr, gen-
erated for every possible input in I, we reduce the chance for
missing locations used in state-deﬁning ways outside of our
bound. In practice, we ﬁnd that our selection of W leads to no
false negatives, and note that it can be conﬁgured to a larger
value to handle problematic implementations.

9

6 Implementation

Our resulting implementation [1] consists of a Java-based
learner, a ptrace-based [7] execution monitor, and test har-
nesses in Python and Java. Our concolic analyser is built us-
ing Triton [31] and IDA Pro [24]. In total, STATEINSPECTOR
consists of over 10kloc across three different languages.

7 Results

In this section, we present the evaluation of our approach,
STATEINSPECTOR, with respect to the research questions out-
lined in Section 3. To this end, we test the implementations
of two security protocols, TLS and WPA/2. We additionally
carry out tests on a intentionally modiﬁed TLS implementa-
tion, as well as a number of purpose-built protocols.

We address RQ1 by demonstrating that our approach
makes it possible to learn models with both large input sets
and identify the presence of deep states in our purpose built
protocols and OpenSSL. In doing so, we also show how grey-
box observations make it possibly to target and optimise learn-
ing (RQ2), thereby avoiding cases of futile state exploration
which degrade black-box learning performance. Finally, in
order to illustrate the additional insights over I/O that our
method affords (RQ3), we discuss our ﬁndings regarding the
disparity between I/O and memory state classiﬁcations of
two OpenSSL implementations, as well as an investigation of
troublesome state memory in Hostap.

7.1 TLS

We evaluate the interesting examples of TLS 1.2 servers from
the work of de Ruiter et al. [18], the latest versions of these
servers, and both the server and client implementations of
Hostap’s internal TLS library. This spans 4 different code
bases, and 7 unique implementations. We learn these models
with two separate input sets, one containing the core TLS
messages (as in de Ruiter et al.), and where possible, another
with a much larger input set, including client authentication
and multiple key exchanges (as provided by the TLS-Attacker
test harness [35]). Table 2 lists the time taken to learn all mod-
els, as well as the identiﬁed candidate state memory details,
query statistics and number of states identiﬁed. Further, we
list the number of queries required to learn the same models
with state-of-the-art black-box learning, i. e., the TTT algo-
rithm [26] with the modiﬁed W-Method equivalence checking
of [18] with a conservative equivalence checking depth of 3.
For practical reasons, we cut off black-box experiments after
3 days if they show no sign of approaching termination.

7.1.1 OpenSSL

We tested a number of different versions of OpenSSL, rang-
ing from 2014 to 2020. Of particular interest is the OpenSSL

server running version 1.0.1g. A state machine for this im-
plementation was presented at Usenix 2015 [18]. It modelled
a previously discovered critical vulnerability: speciﬁcally,
if a ChangeCipherSpec (CCS) message was sent before the
ClientKeyExchange, the server would compute the session
keys using an empty master secret. Prior to ﬁnalising our
model, which we depict in Figure 7, we replicated the no-
tably different model presented in [18], by using an iden-
tical test harness. As described in Section 3, this harness
sent invalid Finished messages on paths with more than one
ClientHello. This results in a model where the dashed green
path (s0, s2, s6, s16, s13, s11) and dotted red path (s0, s2, s7,
s18, s15) in Figure 7 always lead to an Alert and connection
closure after receiving the ﬁnal input. As this was a test har-
ness conﬁguration issue, we sought to use STATEINSPECTOR
in order to conﬁrm this fact. In Table 3 (Appendix B), we list
the differences in memory between states on alternative paths
to handshake completion, and the known happy ﬂow state
s4. These differences were identiﬁed once learning had termi-
nated. As merge conditions were never met for these states,
we carried out a post-analysis stage, where we used STATEIN-
SPECTOR to determine whether the differing memory deﬁned
the respective states. We found that it did not. Despite these
states sharing different I/O deﬁnitions, when compared to
state s4, all were equivalent in terms of state-deﬁning mem-
ory, thus indicating that the test harness was responsible for
the differences. This example also illustrates where I/O in-
terpretation by the test harness is misleading. States s15 and
s17, which manifest due to the aforementioned early-CCS vul-
nerability, are also equivalent to the happy ﬂow state s4. The
I/O difference in this case is due to the fact that the test har-
ness cannot decrypt the response (hence DecryptError). We
note that this post-learning analysis does not come for free,
therefore, we only execute such checks when further model
investigation is needed.

When testing client authentication with the extended alpha-
bet, we also discovered inconsistencies in the documentation,
which, at worst, have the potential to cause vulnerable mis-
conﬁgurations. We discuss this further in Appendix D.

7.1.2 RSA-BSAFE-C

We selected this implementation as it serves to demonstrate
that our method is applicable not just to open-source im-
plementations, but also to closed-source. The implementa-
tion demonstrates a further advantage of our approach over
black-box learning. In particular, we observe that each time
the server sends an Alert message, it performs a partial
socket shutdown. Speciﬁcally, it executes a shutdown(n,
SHUT_RD) on the connection with the test harness. This means
that it no longer accepts input, however, this is not detectable
by the test harness. For our approach, this is not a problem; we
can observe the shutdown syscall and prevent further inputs
from this point. For black-box learning, the socket closure

10

Table 2: Model learning results on TLS 1.2 and WPA/2 servers. TLS experiments are repeated with two alphabets, core and
extended. Times speciﬁed in hours and minutes (hh:mm). Experiments which did not terminate within 3 days are denoted: (cid:4).

Protocol

Implementation

Classifying Mem.
Locations Allocations

Mem.
States

I/O
States

Total
Queries

I/O Mem.
Queries

Watchpoints
Queries Hits

Total Time

Black-Box
Queries Time

TLS 1.2 RSA-BSAFE-C 4.0.4
TLS 1.2
TLS 1.2
TLS 1.2
TLS 1.2
TLS 1.2
TLS 1.2

Hostap TLS
OpenSSL 1.0.1g
OpenSSL 1.0.1j
OpenSSL 1.1.1g
GnuTLS 3.3.12
GnuTLS 3.6.14

l

.
a
h
p
A
e
r
o
C

. TLS 1.2 RSA-BSAFE-C 4.0.4
a
h
p
A

l

OpenSSL 1.1.1g
OpenSSL 1.0.1g
GnuTLS 3.3.12
GnuTLS 3.6.14

TLS 1.2
TLS 1.2
TLS 1.2
TLS 1.2

.
t
x
E

88
159
126
125
126
172
138

165
467
554
185
157

14
32
16
16
6
7
7

7
66
110
5
6

11
11
19
13
6
16
18

14
11
40
23
13

9
6
12
9
6
7
8

7
11
20
19
11

128
194
488
291
88
265
973

918
535
2454
1427
730

109
160
280
165
86
129
452

688
524
1816
1274
673

19
34
208
126
2
36
522

230
11
594
153
57

4
17
116
23
0
42
121

635
0
609
128
37

00:06
00:24
00:18
00:10
00:02
00:10
00:36

01:08
01:05
04:21
02:05
01:32

204k
971
5819
1826
175
1353
3221

133k
1776
20898
66k
66k

(cid:4)
01:48
00:42
00:13
00:02
00:21
00:52

(cid:4)
01:25
21:05
(cid:4)
(cid:4)

WPA/2
WPA/2

6
5
† For Hostap, we stopped both learners at ~200 minutes, as we found that its state machine is inﬁnite and would prevent both approaches terminating.

2127
3000+

03:30
01:02

1629
264

261
597

825
138

804
126

138
135

24
5

03:30
08:00+

3
8

Hostap 2.8†
IWD 1.6

is not detected, and so learning continues exploring beyond
the receipt of an Alert message. As shown in Table 2, this
leads to many superﬂuous queries, and, as a result, the learner
fails to terminate within 3 days for either alphabet. In com-
parison, our algorithm is able to learn the same models with
128 queries in 6 minutes for the core TLS functionality, and
in ~1 hour for a more complex model capturing client authen-
tication and alternative key exchanges. Notably, this latter
test revealed that repeated ClientHello’s are only permitted
when the server is conﬁgured with forced client authentica-
tion.

7.1.3 GnuTLS

Our tests on the TLS server implementations in GnuTLS
3.3.12 and 3.6.14 showed substantial changes between the
two versions. In particular, each version required us to hook
different syscalls for snapshotting. State count also differed,
especially so when testing with the extended alphabet. Analy-
sis of the models revealed slightly different handling of Difﬁe-
Hellman key exchanges, which in the older version resulted
in a path of states separate from RSA key exchange paths.

The difference in learning performance between the two
approaches, in the case of the extended alphabet, was pro-
found. Black-box learning failed to terminate after 3 days
and over 60k queries. We found that this was due to multiple
states and inputs warranting empty responses. Consequently,
black-box learning exhausted its exploration bound, trying
all possible combinations of the troublesome inputs at the
affected states. In contrast, as shown in Table 2, STATEIN-
SPECTOR is able to handle such cases much more effectively.
This is in part because some groups of inputs are found to
result in equivalent snapshots, and when the state equivalence
is not immediately evident, our merging strategy quickly ﬁnds

memory differences are inconsequential.

7.1.4 Hostap-TLS

We tested Hostap’s internal TLS library as both a client and
server. Although this library is described as experimental, it is
used in resource-constrained Wi-Fi devices with limited ﬂash
space [13] and by Wi-Fi clients in embedded Linux images
created using Buildroot [8], e. g., motionEyeOS [15].

Surprisingly, we found that this TLS library always sends
TLS alerts in plaintext, which might leak information about
the connection. Further, some frames from the extended al-
phabet, such as Heartbeat requests or responses were not
supported, and sending them resulted in desynchronisation
of the TLS connection. We therefore do not include learning
results for this implementation with the extended alphabet.

More worrisome, the model showed that against a client,
the ServerKeyExchange message can be skipped, meaning
an adversary can instead send ServerHelloDone. The client
will process this message, and then hits an internal error when
sending a reply because no RSA key was received to encrypt
the premaster secret. When Ephemeral Difﬁe-Hellman is used
instead, the client calculates gcs as the premaster secret with
s equal to zero if no ServerKeyExchange message was re-
ceived. Because the exponent is zero, the default math library
of Hostap returns an internal error, causing the connection to
close, meaning an adversary cannot abuse this to attack clients.
Nevertheless, this does illustrate that the state machine of the
client does not properly validate the order of messages.

7.2 WPA/2’s 4-Way Handshake

We also tested WPA/2’s 4-way handshake which is used to
authenticate with Wi-Fi networks. There are two open source

11

Figure 7: Learned model for OpenSSL 1.0.1g

implementations of this handshake on Linux, namely the ones
in IWD (iNet Wireless Daemon) and Hostap, and we test both.
To learn the state machine, we start with an input alphabet
of size 4 that only contains messages which occur in normal
handshake executions. Our test harness automatically assigns
sensible values to all of the ﬁelds of these handshake mes-
sages. To produce larger input sets, we also tried non-standard
values for certain ﬁelds. For example, for the replay counter,
we tried the same value consecutively, set it equal to zero,
and other variations. To this end, we created two extended
alphabets: one of size 15 and another one of size 40.

To also detect key reinstallation bugs [39], we let the SUT
send an encrypted dataframe after completing the handshake.
In the inferred model, resulting dataframes encrypted using a
nonce equal to one are represented using AES_DATA_1, while
all other dataframes are represented using AES_DATA_n. A
key reinstallation bug, or a variant thereof, can now be auto-
matically detected by scanning for paths in the inferred model
that contain multiple occurrences of AES_DATA_1.

One obstacle that we encountered is handling retransmit-
ted handshake messages that were triggered due to timeouts.
Because this issue is orthogonal to evaluating our memory-
based state inference method, we disabled retransmissions,
and leave the handling of retransmissions as future work.

7.2.1 IWD

We found that the state machine of IWD does not enter a
new state after receiving message 4 of the handshake. This
means an adversary can replay message 4, after which it will
be processed by IWD, triggering a key reinstallation. Note
that this is not a default key reinstallation, i. e., we conﬁrmed
that existing key reinstallation tools cannot detect it [38]. The

discovered reinstallation can be abused to replay, decrypt, and
possibly forge data frames [39]. We reported this vulnerability
and it has been patched and assigned CVE-2020-NNNNN [5].
When running our tool on the patched version of IWD, the
state machine enters a new state after receiving message 4,
which conﬁrms that the vulnerability has been patched.

With black-box testing, the key reinstallation is only found
when using a small input alphabet and when the test har-
ness always sends handshake messages with a replay counter
equal to the last one used by the AP. If the harness instead
increments the replay counter after sending each handshake
message, the key reinstallation can only be discovered when
using an extended alphabet. However, in that case black-box
learning takes much longer: after 3 hours the learner creates
a hypothesis model that includes the vulnerability, but it is
unable to verify this hypothesis within 8 hours (at which point
we terminated the learner). This shows that our method han-
dles larger input alphabets more efﬁciently, especially when
queries are slow, resulting in more accurate models and in-
creasing the chance of ﬁnding bugs.

7.2.2 Hostapd

One obstacle with Hostapd is how the last used replay counter
in transmitted handshake messages is saved: our learner ini-
tially includes this counter in its candidate state memory set.
When trying to merge states with different replay counter
values, our concolic analysis determines this value to be state
deﬁning. Indeed, Hostapd checks whether incoming frames
include a particular value. However, with each loop of the
state machine, the expected value changes, and hence the
learner prevents the merge. This results in a violation of As-
sumption 1, meaning learning will not terminate. Like with

12

other violations of assumptions, we addressed this by includ-
ing a time bound on the learning. Because of this we also do
not explore a bigger alphabet. For a fair comparison, we used
the same time bound in our method and in black-box learn-
ing. The models which were produced with both approaches
within ~200 minutes were equivalent. Under these condi-
tions, we note that both resulting state machines followed the
standard and contained no surprising transitions.

7.3 Example Protocols

In order to highlight the capability of STATEINSPECTOR to
identify states deep in a protocol, we test a minimally modi-
ﬁed version of OpenSSL, in addition to a variety of example
protocols. We note that states in protocols beyond typical
learning bounds are not only theoretical—a state machine
ﬂaw in a WPA/2 router [36] found a cipher downgrade was
possible after 3 failed initiation attempts (which would have
been missed with conﬁgured bounds less than 3, as in [18]).
Our modiﬁcation of OpenSSL (version 1.0.1j), consists of
a 4 line addition (Appendix C, Listing 1), which hijacks ex-
isting state data to implement a simple client authentication
bypass, activated by n unexpected messages (see Figure 6
of Section 5.5.2). To learn this model, we used an extended
alphabet of 21 messages, including core TLS functionality,
client certiﬁcates, and various data frames. With n = 5, black-
box learning conﬁgured with a bound equal to n, fails to iden-
tify the state after 3 days of learning and ~100k queries (and
showed no signs of doing so soon). STATEINSPECTOR, on
the other hand, identiﬁes the main backdoor-activation state
in under 2 hours and ~1.2k queries and the second activation
variant (with two preceding ClientHellos, as opposed to
the usual one) in under 3 hours and ~2.5k queries. Doubling
the depth n results in 3.2k and 4k queries respectively—an
approximately linear increase. Black-box learning predictably
failed to locate the even deeper state due to its exponential
blow up in this particular scenario.

Our motivating example protocol of Section 3, Figure 1
also showed a dramatic advantage over black-box learning,
even with a simple model and small input alphabet. We im-
plemented this protocol similarly to a real protocol, with state
maintained between an enum and counting integer combined.
With black-box learning, this model took 360k queries and
20 hours to identify the auth-bypass but failed to terminate its
analysis after 4 days. STATEINSPECTOR identiﬁed the back-
door in 149 queries in just over minute, with the concolic
analyser taking up 20 seconds of that time.

8 Discussion

We have shown that relaxing the restriction of pure black-box
analysis improves practical model learning: we can learn new
types of states and gain more insights, all in less time. While
we have shown our assumptions make a justiﬁable trade-off

between practicality and completeness, we acknowledge that
they may not hold for all protocol implementations. Thus, in
this section, we discuss possible limitations.

If the machine is not ﬁnite (Assumption 1), then, like black-
box methods, our approach will fail to terminate. Likewise,
with respect to Assumption 4, if loops within a state machine
exist beyond the conﬁgured depth bound, and the memory
across these states is always changing (e. g., a counter), then
STATEINSPECTOR will interminably duplicate states. In both
of these cases, like black-box methods, we provide a means
to bound exploration. We choose to bound by a conﬁgurable
time, but this can also be done by exploration depth. In the
worst case, the resulting models will be no less representative
than those learned with black-box methods. False negatives
due to missed candidate state memory and concolic analysis
also have the potential to be problematic. As a consequence
of Assumption 3, we may miss candidate locations if they
do not change during any bootstrap ﬂow. An example of this
kind of memory is one allocated only after entering an error
state. Fortunately, our results and analysis of implementation
code indicate that this assumption holds for most protocols.

An inherent requirement of grey-box analysis is the abil-
ity to introspect a program’s execution state. Whilst we en-
sure this is performed as non-invasively as possible, it does
limit the types of SUT which can be tested. For example, our
method would not be appropriate for protocols running on
certain types of embedded device, e. g., EMV cards. How-
ever, for devices with sufﬁcient debug access, e. g., JTAG,
STATEINSPECTOR could be used to learn implementations
for which black-box learning proves to be too slow.

Finally, although STATEINSPECTOR is designed to work
with both closed and open-source implementations, it is often
the case—as with most of our tested protocols—that we do
have access to source code. If this is the case, our tool is able
to map uses of state memory to source code locations—which
substantially aids in the analysis of inferred models.

9 Future Work

One key contribution of STATEINSPECTOR is its ability to
identify the structures within a program which are used to
maintain its runtime state. Further, locations within the pro-
gram which manipulate state memory can also be easily identi-
ﬁed. Recent works in the domain of fuzzing (protocols [10,28]
and more general software [6, 19]) have sought to combine
program state coverage with traditional CFG block coverage
for fuzzing feedback. These efforts have hitherto required a
manual speciﬁcation of state changing locations—something
we believe can be aided by the approaches of STATEINSPEC-
TOR.

13

10 Conclusion

Black-box testing is fundamentally limited because it can
only reason about how a SUT interacts with the outside world.
On the one hand, this means that a large number of queries
are required to learn state machines even for simple protocols,
and on the other, because we have no insight into how the
SUT processes inputs, we cannot optimise queries to trigger
or avoid certain behaviours.

In contrast, our grey-box approach, STATEINSPECTOR,
overcomes many of these issues. It is able to efﬁciently handle
larger input spaces and protocols that trigger state changes at
non-trivial depths, resulting in more accurate and insightful
models, that can be learned in substantially fewer queries. As
testament to this, we were able to discover a new, high impact
vulnerability in IWD, and numerous concerning deviations
from the standards in other implementations.

References

[1] StateInspector (URL obscured for anonymous review).

https://github.com/XXXX/YYYYY.

[2] Fides Aarts, Joeri De Ruiter, and Erik Poll. Formal
In Software Testing,
models of bank cards for free.
Veriﬁcation and Validation Workshops (ICSTW), 2013
IEEE Sixth International Conference on, pages 461–468.
IEEE, 2013.

[3] Fides Aarts, Julien Schmaltz, and Frits Vaandrager. In-
ference and abstraction of the biometric passport. In
International Symposium On Leveraging Applications
of Formal Methods, Veriﬁcation and Validation, pages
673–686. Springer, 2010.

[4] Dana Angluin. Learning regular sets from queries
Information and computation,

and counterexamples.
75(2):87–106, 1987.

[5] Anonymous.

Iwd: CVE-2020-NNNNN (details ob-

scured for anonymous review).

[6] Cornelius Aschermann, Sergej Schumilo, Ali Abbasi,
and Thorsten Holz. Ijon: Exploring deep state spaces
via fuzzing. In 2020 IEEE Symposium on Security and
Privacy (SP), pages 1597–1612. IEEE, 2020.

[7] Erik Bosman. ptrace-burrito. Retrieved 3 Septem-
ber 2020 from https://github.com/brainsmoke/
ptrace-burrito, 2020.

[8] Buildroot Association. Buildroot. Retrieved 3 Septem-
ber 2020 from https://buildroot.org/, 2020.

[10] Yurong Chen, Tian Lan, and Guru Venkataramani. Ex-
ploring effective fuzzing strategies to analyze commu-
In Proceedings of the 3rd ACM
nication protocols.
Workshop on Forming an Ecosystem Around Software
Transformation, pages 17–23, 2019.

[11] Chia Yuan Cho, Domagoj Babic, Pongsin Poosankam,
Kevin Zhijie Chen, Edward XueJun Wu, and Dawn Song.
Mace: Model-inference-assisted concolic exploration
for protocol and vulnerability discovery. In USENIX
Security Symposium, volume 139, 2011.

[12] Tsun S. Chow. Testing software design modeled by
ﬁnite-state machines. IEEE transactions on software
engineering, (3):178–187, 1978.

[13] CodeApe123. Hostapd porting and use. Retrieved 9
January 2021 from https://blog.csdn.net/sean_
8180/article/details/86496922, 2020.

[14] Paolo Milani Comparetti, Gilbert Wondracek, Christo-
pher Kruegel, and Engin Kirda. Prospex: Protocol spec-
iﬁcation extraction. In Security and Privacy, 2009 30th
IEEE Symposium on, pages 110–125. IEEE, 2009.

[15] Calin Crisan. motioneyeos. Retrieved 3 September 2020
from https://github.com/ccrisan/motioneyeos,
2020.

[16] Lesly-Ann Daniel, Erik Poll, and Joeri de Ruiter. Infer-
ring openvpn state machines using protocol state fuzzing.
In 2018 IEEE European Symposium on Security and
Privacy Workshops (EuroS&PW), pages 11–19. IEEE,
2018.

[17] Joeri de Ruiter. A tale of the openssl state machine: A
large-scale black-box analysis. In Nordic Conference
on Secure IT Systems, pages 169–184. Springer, 2016.

[18] Joeri De Ruiter and Erik Poll. Protocol State Fuzzing of
TLS Implementations. In USENIX Security, volume 15,
pages 193–206, 2015.

[19] Andrea Fioraldi, Daniele Cono D?Elia, and Davide
Balzarotti. The use of likely invariants as feedback for
fuzzers. In Usenix, editor, USENIX 2021, 30th USENIX
Security Symposium, 11-13 August 2021, Virtual Con-
ference, 2021.

[20] Paul Fiter˘au-Bro¸stean, Ramon Janssen, and Frits Vaan-
drager. Combining model learning and model checking
to analyze tcp implementations. In International Con-
ference on Computer Aided Veriﬁcation, pages 454–471.
Springer, 2016.

[9] Peng Chen and Hao Chen. Angora: Efﬁcient fuzzing by
principled search. In 2018 IEEE Symposium on Security
and Privacy, 2018.

[21] Paul Fiterau-Brostean, Bengt Jonsson, Robert Merget,
Joeri de Ruiter, Konstantinos Sagonas, and Juraj So-
morovsky. Analysis of DTLS implementations using

14

protocol state fuzzing. In 29th USENIX Security Sympo-
sium (USENIX Security 20), pages 2523–2540. USENIX
Association, August 2020.

[33] Timo Schrijvers, FW Vaandrager, and NH Jansen. Learn-
ing register automata using taint analysis. Bachelors
Thesis, 2018.

[22] Paul Fiter˘au-Bro¸stean, Toon Lenaerts, Erik Poll, Joeri
de Ruiter, Frits Vaandrager, and Patrick Verleg. Model
learning and model checking of ssh implementations. In
Proceedings of the 24th ACM SIGSOFT International
SPIN Symposium on Model Checking of Software, SPIN
2017, pages 142–151. ACM, 2017.

[23] Hugo Gascon, Christian Wressnegger, Fabian Yam-
aguchi, Daniel Arp, and Konrad Rieck. Pulsar: State-
ful black-box fuzzing of proprietary network protocols.
In International Conference on Security and Privacy
in Communication Systems, pages 330–347. Springer,
2015.

[24] Hex-Rays.

Ida pro. Retrieved 3 September 2020
from https://www.hex-rays.com/products/ida/,
2020.

[25] Falk Howar, Bengt Jonsson, and Frits Vaandrager. Com-
bining black-box and white-box techniques for learning
register automata. In Computing and Software Science,
pages 563–588. Springer, 2019.

[34] Muzammil Shahbaz and Roland Groz. Inferring mealy

machines. FM, 9:207–222, 2009.

[35] Juraj Somorovsky. Systematic fuzzing and testing of
tls libraries. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security,
pages 1492–1504, 2016.

[36] Chris McMahon Stone, Tom Chothia, and Joeri
de Ruiter. Extending automated protocol state learning
for the 802.11 4-way handshake. In European Sympo-
sium on Research in Computer Security, pages 325–345.
Springer, 2018.

[37] Martin Tappler, Bernhard K Aichernig, and Roderick
Bloem. Model-based testing iot communication via
active automata learning. In 2017 IEEE International
conference on software testing, veriﬁcation and valida-
tion (ICST), pages 276–287. IEEE, 2017.

[38] Mathy Vanhoef. Krack attack scripts. Retrieved 30
January 2020 from https://github.com/vanhoefm/
krackattacks-scripts, 2021.

[26] Malte Isberner, Falk Howar, and Bernhard Steffen. The
ttt algorithm: a redundancy-free approach to active au-
tomata learning. In International Conference on Run-
time Veriﬁcation, pages 307–322. Springer, 2014.

[39] Mathy Vanhoef and Frank Piessens. Key Reinstallation
Attacks: Forcing Nonce Reuse in WPA2. In Proceed-
ings of the 24th ACM Conference on Computer and
Communication Security. ACM, 2017.

[27] Malte Isberner, Falk Howar, and Bernhard Steffen. The
open-source learnlib. In International Conference on
Computer Aided Veriﬁcation, pages 487–495. Springer,
2015.

[28] Roberto Natella and Van-Thuan Pham. Profuzzbench:
A benchmark for stateful protocol fuzzing, 2021.

[29] Oliver Niese. An integrated approach to testing complex
systems. PhD thesis, Universität Dortmund, 2003.

[30] Van-Thuan Pham, Marcel Böhme, and Abhik Roychoud-
hury. Aﬂnet: a greybox fuzzer for network protocols.
In 2020 IEEE 13th International Conference on Soft-
ware Testing, Validation and Veriﬁcation (ICST), pages
460–465. IEEE, 2020.

[31] QUARKSLAB. Triton. Retrieved 3 September 2020
from https://triton.quarkslab.com/, 2020.

[32] Harald Raffelt, Bernhard Steffen, and Therese Berg.
Learnlib: A library for automata learning and experi-
In Proceedings of the 10th international
mentation.
workshop on Formal methods for industrial critical sys-
tems, pages 62–71. ACM, 2005.

15

A Algorithm to Generate Watchpoint Hit

Snapshots

Algorithm 1: Generation of watchpoint hit snapshots.

Result: list of watchpointHitExecutionDumps
Input: (addr, size) of memory to test, preﬁx query p

to arrive at mergeState, the SUT

foreach (input, st) ∈ reachableFrm(mergeState, 1)
do

if out putFor(input, st) ∈ disabled then

continue;

end
initialiseWatchpointMonitor(SUT, addr, size);
executeQuery(SUT, p||input);

end

B OpenSSL State Memory Analysis

Table 3: The sets of differing memory for states on suspected
and conﬁrmed alternate paths to successful handshake com-
pletion, when compared to the legitimate happy ﬂow state 4
(s4) memory.

State IDs

Addresses of differing memory to s4

s14 (cid:26)

s15, s17, s13




{0x555555a162a0, 0x4b0, 0x13}
{0x555555a162a0, 0x4b0, 0x120}
{0x555555a162a0, 0x4b0, 0x0}
{0x555555a0e6b0, 0x160, 0x0}
{0x555555a0e6b0, 0x160, 0x10}
{0x555555a0e6b0, 0x160, 0x44}
{0x555555a0e6b0, 0x160, 0x68}
{0x555555a0e6b0, 0x160, 0x6c}
{0x555555a0e6b0, 0x160, 0xc0}
{0x555555a0e6b0, 0x160, 0xc8}
{0x555555a0e6b0, 0x160, 0xd2}

Using the learning output of STATEINSPECTOR, we carried
out an investigation into what the differing memory in Table 3
actually constituted. We were able to do this by referring to the
logs produced by the tool, which print the source code details
of any watchpoint hits made at reads of the tested memory.
All considered states shared the two differing addresses listed
at the top of the table. The ﬁrst of these represented a counter
referred to as the sequence number. This value increments
for each message received after the ChangeCipherSpec mes-
sage and is used for constructing MACs. The second address
in the table refers to the frame type of the just received mes-
sage. This value is determined as not state deﬁning because
from any given state it is always written to before being read,

i. e., it may be state inﬂuencing, but not deﬁning (ref. 5.5.1).
The third address in the table pertains to a ﬂag which deter-
mines if the client has attempted more than one ClientHello
message. Only prior to the receipt of a ClientKeyExchange
is this ﬂag ever read (and therefore used to deﬁne the state).
Consequently, it is not considered state deﬁning for any of
the states listed in the table. The remaining memory refers to
arbitrary certiﬁcate data which we also found was not state
deﬁning. In particular, we were unable to detect any reads of
the memory from the states considered.

C OpenSSL Client Authentication Bypass

Backdoor

Listing 1: A four line modiﬁcation to OpenSSL 1.0.1j,
which introduces a client authentication bypass at a conﬁg-
urable depth (BACKDOOR_DEPTH). In the ﬁrst part, we hijack
a counter, the read_sequence counter, and conditionally in-
crement it each time an unexpected data frame is read whilst
waiting on the ClientCertiﬁcateVerify signature. In the sec-
ond part, if this counter reaches the speciﬁed depth value,
invalid ClientCertiﬁcateVerify signatures are ignored, and the
handshake proceeds to the ChangeCipherSpec state. After
this point, the read_sequence is automatically reset by the
implementation for use in later MAC calculations.

@@ /ssl/s3_pkt.c:139 ssl3_read_n()

SSL3_BUFFER *rb;

+++ // Backdoor pt 1: Set-up
+++ if(s->state == SSL3_ST_SR_CERT_VRFY_A)
++(*s->s3->read_sequence);
+++

if (n <= 0) return n;
rb = &(s->s3->rbuf);

@@ /ssl/s3_srvr.c:683

ssl3_accept()

ret=ssl3_get_cert_verify(s);

--- if (ret <= 0) goto end;
+
+++ // Backdoor pt 2: Activation
+++ if(*s->s3->read_sequence != BACKDOOR_DEPTH
+++
+++
+++ } else {

&& ret <= 0) {
goto end;

...
s->state=SSL3_ST_SR_FINISHED_A;
s->init_num=0;

+++ }

16

D Misleading OpenSSL Documentation

When testing the built-in server of OpenSSL, we found that
conﬁguring it to enforce client authentication is rather error-
prone. In particular, after an initial reading of the documenta-
tion for how to do so, we came across the option below:

-verify int, -Verify int

The verify depth to use. This specifies the
maximum length of the client certificate
chain and makes the server request a
certificate from the client. With the
-verify option a certificate is requested
but the client does not have to send one,
with the -Verify option the client must
supply a certificate or an error occurs.

Following this, we used the -Verify option when starting
the built-in server. Surprisingly though, our learned models
indicated that the server accepted invalid client certiﬁcates. At
ﬁrst glance, this looked like a severe vulnerability, however,
upon closer inspection of the documentation we found an
extra option was required:

-verify_return_error

Verification errors normally just print a
message but allow the connection to
continue, for debugging purposes. If this
option is used, then verification errors
close the connection.

Our online investigation revealed we were not alone in our
confusion. A recent Github issue2 describes the same mis-
leading conﬁguration, and resulted in a patch to the OpenSSL
client implementation3. Unfortunately, a similar patch was
not written for the server implementation, resulting in a con-
fusing disparity between the command-line options for the
client and server. We have reported this issue to the OpenSSL
developers.

2https://github.com/openssl/openssl/issues/8079
3https://github.com/openssl/openssl/pull/8080

17

