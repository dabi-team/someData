2
2
0
2

b
e
F
5
2

]

G
L
.
s
c
[

1
v
1
7
5
2
1
.
2
0
2
2
:
v
i
X
r
a

NeuralKG: An Open Source Library for Diverse Representation
Learning of Knowledge Graphs
Wen Zhang1, Xiangnan Chen1, Zhen Yao1, Mingyang Chen2, Yushan Zhu2, Hongtao Yu2,
Yufeng Huang1, Zezhong Xu2, Yajing Xu1, Ningyu Zhang1,3, Zonggang Yuan5, Feiyu Xiong6,
Huajun Chen2,3,4
1School of Software Technology, 2College of Computer Science and Technology, Zhejiang University
3Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies

4Hangzhou Innovation Center, Zhejiang University 5Huawei Technologies Co., Lt

6Alibaba Group

ABSTRACT
NeuralKG is an open-source Python-based library for diverse rep-
resentation learning of knowledge graphs. It implements three
different series of Knowledge Graph Embedding (KGE) methods,
including conventional KGEs, GNN-based KGEs, and Rule-based
KGEs. With a unified framework, NeuralKG successfully reproduces
link prediction results of these methods on benchmarks, freeing
users from the laborious task of reimplementing them, especially
for some methods originally written in non-python programming
languages. Besides, NeuralKG is highly configurable and exten-
sible. It provides various decoupled modules that can be mixed
and adapted to each other. Thus with NeuralKG, developers and
researchers can quickly implement their own designed models and
obtain the optimal training methods to achieve the best perfor-
mance efficiently. We built an website1 to organize an open and
shared KG representation learning community. The library, experi-
mental methodologies, and model reimplement results of NeuralKG
are all publicly released at https://github.com/zjukg/NeuralKG.

KEYWORDS
Knowledge Graph, Knowledge Graph Embedding, Diverse Repre-
sentation Learning, Open Source, Link Prediction

ACM Reference Format:
Wen Zhang1, Xiangnan Chen1, Zhen Yao1, Mingyang Chen2, Yushan Zhu2,
Hongtao Yu2,, Yufeng Huang1, Zezhong Xu2, Yajing Xu1, Ningyu Zhang1,3,
Zonggang Yuan5, Feiyu Xiong6,, Huajun Chen2,3,4. 2018. NeuralKG: An
Open Source Library for Diverse Representation Learning of Knowledge
Graphs. In Proceedings of Make sure to enter the correct conference title from
your rights confirmation emai (Conference acronym ‚ÄôXX). ACM, New York,
NY, USA, 5 pages. https://doi.org/XXXXXXX.XXXXXXX

1 INTRODUCTION
Knowledge Graphs (KGs) represent real-world facts as symbolic
triples in the form of (head entity, relation, tail entity), abbreviated

1Project website: http://neuralkg.zjukg.cn

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY
¬© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00
https://doi.org/XXXXXXX.XXXXXXX

Table 1: Comparison of different KGE toolkits.

C-KGE GNN-based KGE

Input

Triple
Graph
Rule
OpenKE [12]
DGL-KE [34]
Pykg2vec [29]
AmpliGraph [8]
TorchKGE [5]
LIBKGE [6]
PyKEEN 1.0 [2]

NeuralKG

‚Ä¢

(cid:34)
(cid:34)
(cid:34)
(cid:34)
(cid:34)
(cid:34)
(cid:34)
(cid:34)

‚Ä¢
‚Ä¢

(cid:34)
(cid:34)

Rule-based KGE
‚Ä¢

‚Ä¢

(cid:34)

as (h,r,t), for example, (Earth, containedIn, SolarSystem). Currently,
many large-scale knowledge graphs have been proposed, such as
YAGO [20], Freebase [3], NELL [7] and Wikidata [25]. They are
widely used as background knowledge provider in many applica-
tions such as natural language understanding [17], recommender
system [30], and question-answering [13].

Traditional query and reasoning on KGs are based on manip-
ulation of symbolic representations, which is vulnerable to the
noisiness and incompleteness of KGs. Thus, with the development
of deep learning, representation learning of KGs has been widely
explored, aiming to embed a KG into a low-dimensional vector
space while preserving the structural and semantic information
contained in the KG. Thus these methods are also called Knowl-
edge Graph Embeddings (KGEs). Flourishing research has been
conducted in this area, and different series of KGEs are proposed.
Conventional KGEs (C-KGEs)[4, 28] apply a geometric assumption
in vector space for true triples and use single triple as input for
triple scoring. They ignore the rich graph structures of entities that
reflect their semantics. Thus Graph Neural Network (GNN) effective
in encoding graph structures are widely applied in KGEs. Based on
the input graph, GNN-based methods[19, 24] use representations
of entities aggregated from their neighbors in the graph instead of
embeddings of them for triple scoring, which help them capture the
graph patterns explicitly. Apart from triples and graphs with triples,
another essential part of KGs, logic rules, are also considered in
KGEs. Rules define higher-level semantics relationships between
different relations. Many Rule-based KGEs [11, 32] tried to inject
logic rules, either pre-defined or learned, into KGEs to improve the
expressiveness of models.

Although there are a lot of KGE methods and authors public their
source code, it is difficult for others to apply and compare them due

 
 
 
 
 
 
Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY

Trovato and Tobin, et al.

Figure 1: (a) The pipeline of NeuralKG. (b) Quick start example. (c) Hyperparameter tuning example.

to the difference of programming languages (e.g., C++, Python, Java)
and frameworks (e.g., TensorFlow, PyTorch, Theano). Thus several
open-source knowledge graph representation toolkits have been
developed, including OpenKE [12], Pykg2vec [29], TorchKGE [5],
LIBKGE [6], and DGL-KE [34]. They provide implementations and
evaluation results of widely-used and recently-proposed models,
making it more easier for people to apply those methods.

However, most of these toolkits focus on the implementation
C-KGEs, and a few of them provide the implementation for GNN-
based KGEs, and none of them supports Rule-based KGEs. Thus
for a more general purpose of diverse representation learning of
KGs, it is necessary to build a toolkit that supports the implementa-
tion and creation of all three series of methods mentioned before.
While building such a toolkit is non-trivial. These three series of
methods are different but not independent. For example, C-KGEs
are the backbone of most GNN-based and Rule-based KGEs. Thus,
decoupling components should be contained in the concise toolkit
to train all methods in a unified framework.

In this paper, we present such an open-source library for diverse
representation learning of KGs. We named it NeuralKG target-
ing at general neural representations in vector spaces for KGs. As
shown in Table 1, it supports the development and design of all
three series of KGEs, C-KGEs, GNN-based KGEs, and Rule-based
KGEs. It is a unified framework with various decoupled modules,
including KG Data Preprocessing, Sampler for negative sampling,
Monitor for hyperparameter tuning, Trainer covering the training,
and model validation. Thus users could utilize it for comprehensive
and diverse research and application of representation learning on
KGs. Furthermore, we provide detailed documentation for begin-
ners to make it easy to use NeuralKG library. We also created a
website of NeuralKG to organize an open and shared KG represen-
tation learning community. More importantly, we will also present
long-term technical support to meet new needs in the future. In
summary, the features of NeuralKG library are as follows:

‚Ä¢ Support diverse types of methods. NeuralKG, as a library
for diverse representation learning of KGs, provides imple-
mentations of three series of KGE methods, including C-
KGEs, GNN-based KGEs, and Rule-based KGEs.

‚Ä¢ Support easy customization. NeuralKG contains fine-grained
decoupled modules that are commonly used in different
KGEs. With NeuralKG, users can develop their KGEs quickly
and obtain optimal trained models and hyper-parameters.
‚Ä¢ With detailed documentation and support. The core
team of NeuralKG provides detailed documentation, an on-
line platform to organize a shared KG representation learning
community, and long-term technical maintenance.

2 NEURALKG LIBRARY
2.1 Library Overview
NeuralKG is built on PyTorch Lightning 2. It provides a general
workflow of diverse representation learning on KGs and is highly
modularized, supporting three series of KGEs. Figure 1 (a) shows
the pipeline of NeuralKG, given a knowledge graph. Firstly, the
KG data processing module processes the dataset into a format
that facilitates data loading and constructs several dictionaries of
data statistics. Secondly, the sampler module performs negative
sampling on the dataset based on statistical information from the
KG data processing module. Thirdly, the trainer module takes the
positive and negative samples from the sampler module as input
and conducts iterative training and validation of different models,
where concrete representation learning models, loss functions, and
evaluation tasks are integrated. At the same time, the monitor
module observes the training status of models. It enables early
stopping to prevent overfitting and avoid computation resources
wastes, and supports hyperparameter tuning to help users obtain
the optimally trained model.

2https://www.pytorchlightning.ai

NeuralKG: An Open Source Library for Diverse Representation Learning of Knowledge Graphs

Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY

2.2 Library Modules
This section introduces the details of various scalable modules
provided in NeuralKG. It includes KG Data Processing, Sampler,
Trainer and Monitor.

of nodes and edges. It uses uniform negative sampling to create neg-
ative samples. Similar to Negsampler, GraphSampler also supports
the customization of graph samplers such as KBATSampler and
CompGCNSampler for KBAT [16] and CompGCN [24] respectively.

2.2.1 KG Data Processing. KG data processing module prepro-
cesses the input KG dataset for data loading. It generates necessary
statistics for subsequent negative sampling and supports the devel-
opment of custom classes for personal data processing. It mainly
includes KGData, KGDataModule, and Grounding.

KGData. The KGData class loads raw KG data and assigns a
unique numerical id, usually starting from 0, for each entity and
relation in the dataset. It also provides functions to generate data
statistics required in negative sampling, including a function named
get_hr2t_rt2h_from_train which constructs a dictionary to store
tail entities corresponding to a head entity and relation pair (‚Ñé, ùëü )
in the training set, as well as head entities for each (ùëü , ùë°) pair, and
count_frequency function counting the frequency of each pair.

KGDataModule. The KGDataModule class is inherited from the
LightningDataModule class in PyTorch Lightning, which com-
bines the functionality of the Dataset and Dataloader classes in
PyTorch. It supplies several interfaces for stacking the correspond-
ing data into mini-batches for the training, validating , and testing
phases in Trainer module.

Grounding. The Grounding class reads rule dataset, provides the
function groundRule to generate groundings of each rule, that is
replacing all variables in rules with concrete entities in the dataset,
and saves all groundings to a file if necessary. It will be used in
Rule-based KGEs.

Sampler. Sampler module includes BaseSampler, RevSampler,

2.2.2
NegSampler, and GraphSampler. All these samplers are general and
independent of datasets and methods. Thus, users can flexibly use
different samplers based on their targets.

BaseSampler/RevSampler. BaseSampler and RevSampler are par-
ent classes of NegSampler and GraphSampler. During negative
sampling, they help filter generated negative samples that exist in
the training dataset, according to the dictionary of statistics from
KGData module. Furthermore, RevSampler also adds inverse rela-
tions into the training dataset, thus allowing models convert head
entity prediction (?, ùëü, ùë°) into tail entity prediction (ùë°, ùëü ‚àí1, ?), where
ùëü ‚àí1 is the inverse relation of ùëü .

NegSampler. The NegSampler class provides a variety of nega-
tive sampling methods given a triple (‚Ñé, ùëü, ùë°): UniSampler randomly
replacing ‚Ñé or ùë° following uniformly distribution [4], BernSampler
randomly replacing ‚Ñé or ùë° following a Bernoulli distribution cre-
ated for each relation based on statistics from KGData module [27],
AdvSampler applying self-adversarial negative sampling proposed
in [21], and AllSampler replacing ‚Ñé or ùë° with all entities in the
datasets. In addition, NegSampler supports customization of nega-
tive samplers such as CrossESampler for CrossE[33].

2.2.3 Trainer. NeuralKG uses the trainer module to guide models‚Äô
iterative training and validation. The detailed experimental guid-
ance and model reproduction results are provided in NeuralKG,
facilitating developers and researchers to quickly start and compre-
hensively compare different types of KGE models with NeuralKG.

Training. The trainer module specifies the training details of
models and controls models‚Äô training progress. In the trainer mod-
ule, loss function and optimizer could be specified and customized.
During training, it helps display the training progress bar of models
in real-time, and supports breakpoint retraining and saving training
logs for subsequent experimental analysis.

Evaluation. For evaluation of models, such as getting predicting
results on valid or test data, the trainer module accepts external
parameters such as check_per_epoch setting the number of rounds
of model evaluation, and limit_val_batches for validating the model
with a portion of the valid data to help users debug quickly during
the implementation of new models. By default, Mean Reciprocal
Rank (MRR) and Hit at top K (Hit@K) metrics are applied to evaluate
the link prediction results of models.

2.2.4 Monitor. Monitor module outputs the training status of the
models, early stop models‚Äô training if there are continuous drops
on user-specified evaluation metrics during evaluation. It saves the
model with the best evaluation results. Moreover, it supports hy-
perparameter tuning to help users obtain the best hyperparameter
setting regarding evaluation results.

Hyperparameter Tuning. The combination of different indepen-
dent components and hyperparameters in the training pipeline
has a significant impact on model performance [18]. Therefore,
NeuralKG utilizes Weights & Biases 3 to provide grid search, ran-
dom search, and Bayesian search for automatic hyperparameter
optimization. More importantly, NeuralKG treats everything as hy-
perparameters. Thus besides usual hyperparameters like learning
rate, batch size, and embedding dimension, users also can search for
standard components used in the representation learning model‚Äôs
pipeline such as negative sampling methods, loss functions, and
optimizers. Users only need to modify the configuration file in the
format "*.yaml" to perform hyperparameter optimization. The ex-
ample in Figure 1 (c) displays hyperparameter optimization of the
TransE [4] on the FB15K-237 [22] dataset using grid search.

2.3 Model Hub
This section introduces the model hubs of NeuralKG. NeuralKG
provides the implementation of three series of KGEs in three mod-
ules, KGEModel for C-KGEs, GNNModel for GNN-based KGEs, and
RuleModel for Rule-based KGEs. All models in NeuralKG are listed
in Table 2.

GraphSampler. The GraphSampler class uses the graph of statis-
tics built in DGL [26] to sample triples and record the normalization

3https://wandb.ai/site

Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY

Trovato and Tobin, et al.

Table 2: NeuralKG Model Hub.

Table 4: RGCN Evaluation Results on FB15K237.

Components

KGEModel

GNNModel

RuleModel

Models
TransE [4], TransH [27], TransR [15],
ComplEx [23], DistMult [28], RotatE [21],
ConvE [9], BoxE[1], CrossE [33], Sim-
plE [14]
RGCN [19], KBAT [16], CompGCN [24],
XTransE [31]
ComplEx-NNE+AER [10], RUGE [11]
IterE [32]

Table 3: TransE Evaluation Results on FB15K237. ‚àó repre-
sents the reproduced result of using toolkits, Œî indicates the
reproduced result reported by toolkits.

Toolkits
LIBKGE[6]Œî
OpenKE[12]
DGL-KE[34]‚àó
TorchKGE[5]‚àó
Pykg2vec[29]‚àó
AmpliGraph[8]Œî
PyKEEN 1.0[2]‚àó
NeuralKG

MRR HITS@1 HITS@3 HITS@10
0.31
0.29Œî
0.23
0.29
0.25
0.31
0.14
0.32

0.50
0.48Œî
0.43
0.46
0.42
0.50
0.30
0.51

0.22
0.19‚àó
0.13
0.20
0.16
0.22
0.05
0.23

0.35
0.32‚àó
0.27
0.31
0.28
0.35
0.11
0.36

2.3.1 KGEModel. This model hub covers a wide range of C-KGEs,
including models that use distance-based functions to calculate
sample scores, such as TransE [4] and Rotate [21], models that
use a similarity-based approach to obtain sample scores such as
Distmult[28], and models use convolutional neural network such
as ConvE [9]. Notes that some models proposed in the last year,
such as BoxE [1], are also included in NeuralKG, which are missing
in other toolkits. In Table 3, we show the link prediction results of
the most widely used and implemented model TransE on FB15k-
237. Compared to existing toolkits, NeuralKG achieves reasonable
results.

2.3.2 GNNModel. This model hub covers the common GNN-based
KGEs, including RGCN [19], KBAT [16], CompGCN [24] and XTransE
[31]. GNNs have been proved to be efficient and effective in encod-
ing graph structures, thus a series of works tried to implement GNN
as encoders for entity representations and apply conventional KGEs
as decoders for triple scoring. Table 4 presents the link prediction
results of the most widely used model RGCN [19] on FB15k-237,
which shows that NeuralKG successfully achieves comparable re-
sults compared to results reported in the original paper.

2.3.3 RuleModel. This model hub covers a series of methods that
injects logic rules during representation learning of KGs, including
ComplEx-NNE+AER [10], RUGE [11], IterE[32]. Rules are used to
regularize the relation embeddings or infer extra triples for training
via grounding. With rule injected, KGEs earns a higher expressive-
ness regarding semantics. In Table 5, we show the link prediction
results of RUGE [11] from NeuralKG and from the original paper.
They are comaprable.

RGCN-paper[19]
NeuralKG

MRR HITS@1 HITS@3 HITS@10
0.25
0.25

0.41
0.43

0.26
0.27

0.15
0.16

Table 5: RUGE Evaluation Results on FB15K.

RUGE-paper[11]
NeuralKG

MRR HITS@1 HITS@3 HITS@10
0.77
0.76

0.70
0.70

0.82
0.81

0.87
0.85

2.3.4 Quick Start. As shown in Table 2, 3, 4 and 5, NeuralKG pro-
vides implementations and evaluation results of widely used models
in three genres. With a unified model library, NeuralKG supports
users to quickly build their models. An example is shown in Figure 1
(b).

3 RELATED WORK
With the widespread use of knowledge graphs in recent years, many
open-source KGE toolkits for reproducible research have emerged
in the knowledge representation learning community. OpenKE is
an early open-source framework for KGEs, backed by both Ten-
sorFlow and PyTorch libraries and some features implemented in
C++. DGL-KE[34] is a scalable package for learning large-scale
knowledge graph embeddings and supports multi-GPU training.
AmpliGraph[8] and Pykg2vec[29] are released at the same time
in 2019, which focus on providing intuitive APIs to facilitate ef-
fective training of KGE models. PyKEEN[2] 1.0 is a re-designed
and re-implemented toolkit for the PyKEEN toolkit. Thanks to the
community‚Äôs efforts, it provides automatic memory optimization
modules to utilize the hardware resource as much as possible. Other
toolkits, LIBKGE[6] and TorchKGE[5], further help to build a mod-
ular framework to explore the impact of different components of
the KGE pipeline and get quick evaluation results. However, these
toolkits focus on the implementation of conventional KGEs which
ignore other types of methods, thus are not a general library for
representation learning of knowledge graphs.

4 CONCLUSION
With the development of neural knowledge graph representation
learning, different kinds of KGE models have been proposed, whose
performance in practical applications is affected by negative sam-
pling mechanisms, model design, and training strategies. In this
work, we introduce NeuralKG, an open-source and extensible knowl-
edge graph representation learning library. It uses a unified frame-
work to implement three types of KGE methods, including conven-
tional KGEs, GNN-based KGEs, and Rule-based KGEs, showing a
more comprehensive view of KGE methods than existing toolkits.
To meet users‚Äô customization needs, it contains decoupled modules,
helping users to develop their models and get the best training
methods for research and applications on knowledge graph repre-
sentation learning. In addition, we offer an online platform around
this project to organize an open and shared community. In the
future, the core team of NeuralKG will continuously provide imple-
mentations of new KGE methods of different types. At the same

NeuralKG: An Open Source Library for Diverse Representation Learning of Knowledge Graphs

Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY

[22] Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choud-
hury, and Michael Gamon. 2015. Representing text for joint embedding of text
and knowledge bases. In Proceedings of the 2015 conference on empirical methods
in natural language processing. 1499‚Äì1509.

[23] Th√©o Trouillon, Johannes Welbl, Sebastian Riedel, √âric Gaussier, and Guillaume
Bouchard. 2016. Complex embeddings for simple link prediction. In International
Conference on Machine Learning. 2071‚Äì2080.

[24] Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, and Partha P. Talukdar. 2020.
Composition-based Multi-Relational Graph Convolutional Networks. In 8th Inter-
national Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020.

[25] Denny Vrandeƒçiƒá and Markus Kr√∂tzsch. 2014. Wikidata: A Free Collaborative

Knowledge Base. Commun. ACM 57 (2014), 78‚Äì85.

[26] Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou,
Chao Ma, Lingfan Yu, Yu Gai, Tianjun Xiao, Tong He, George Karypis, Jinyang
Li, and Zheng Zhang. 2019. Deep Graph Library: A Graph-Centric, Highly-
Performant Package for Graph Neural Networks. arXiv preprint arXiv:1909.01315
(2019).

[27] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge
graph embedding by translating on hyperplanes. In Twenty-Eighth AAAI Confer-
ence on Artificial Intelligence.

[28] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015. Em-
bedding Entities and Relations for Learning and Inference in Knowledge Bases.
In International Conference on Learning Representations.

[29] Shih Yuan Yu, Sujit Rokka Chhetri, Arquimedes Canedo, Palash Goyal, and Mo-
hammad Abdullah Al Faruque. 2019. Pykg2vec: A Python Library for Knowledge
Graph Embedding. arXiv preprint arXiv:1906.04239 (2019).

[30] Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma.
2016. Collaborative knowledge base embedding for recommender systems. In
Proceedings of the 22nd ACM SIGKDD international conference on knowledge
discovery and data mining. 353‚Äì362.

[31] Wen Zhang, Shumin Deng, Han Wang, Qiang Chen, Wei Zhang, and Huajun Chen.
2019. XTransE: Explainable Knowledge Graph Embedding for Link Prediction
with Lifestyles in e-Commerce. In JIST (2) (Communications in Computer and
Information Science, Vol. 1157). Springer, 78‚Äì87.

[32] Wen Zhang, Bibek Paudel, Liang Wang, Jiaoyan Chen, Hai Zhu, Wei Zhang,
Abraham Bernstein, and Huajun Chen. 2019. Iteratively Learning Embeddings
and Rules for Knowledge Graph Reasoning. In The World Wide Web Conference.
ACM, 2366‚Äì2377.

[33] Wen Zhang, Bibek Paudel, Wei Zhang, Abraham Bernstein, and Huajun Chen.
2019.
Interaction Embeddings for Prediction and Explanation in Knowledge
Graphs. In Proceedings of the Twelfth ACM International Conference on Web Search
and Data Mining. ACM, 96‚Äì104.

[34] Da Zheng, Xiang Song, Chao Ma, Zeyuan Tan, Zihao Ye, Jin Dong, Hao Xiong,
Zheng Zhang, and George Karypis. 2020. DGL-KE: Training Knowledge Graph
Embeddings at Scale. In Proceedings of the 43rd International ACM SIGIR Confer-
ence on Research and Development in Information Retrieval (SIGIR ‚Äô20). Association
for Computing Machinery, New York, NY, USA, 739‚Äì748.

time, we‚Äôll try to enable the crowdsourcing of models from other
developers.

REFERENCES
[1] Ralph Abboud, ƒ∞smail ƒ∞lkan Ceylan, Thomas Lukasiewicz, and Tommaso Sal-
vatori. 2020. BoxE: A Box Embedding Model for Knowledge Base Completion.
In Proceedings of the Thirty-Fourth Annual Conference on Advances in Neural
Information Processing Systems (NeurIPS).

[2] Mehdi Ali, Max Berrendorf, Charles Tapley Hoyt, Laurent Vermue, Sahand Shar-
ifzadeh, Volker Tresp, and Jens Lehmann. 2021. PyKEEN 1.0: A Python Library
for Training and Evaluating Knowledge Graph Embeddings. Journal of Machine
Learning Research 22, 82 (2021), 1‚Äì6. http://jmlr.org/papers/v22/20-825.html
[3] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor.
2008. Freebase: A Collaboratively Created Graph Database for Structuring Human
Knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on
Management of Data. ACM, 1247‚Äì1250.

[4] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Ok-
sana Yakhnenko. 2013. Translating embeddings for modeling multi-relational
data. In Advances in neural information processing systems. 2787‚Äì2795.

[5] Armand Boschin. 2020. TorchKGE: Knowledge Graph Embedding in Python
and PyTorch. In International Workshop on Knowledge Graph: Mining Knowledge
Graph for Deep Insights.

[6] Samuel Broscheit, Daniel Ruffinelli, Adrian Kochsiek, Patrick Betz, and Rainer
Gemulla. 2020. LibKGE - A Knowledge Graph Embedding Library for Re-
producible Research. In Proceedings of the 2020 Conference on Empirical Meth-
ods in Natural Language Processing: System Demonstrations. 165‚Äì174. https:
//www.aclweb.org/anthology/2020.emnlp-demos.22

[7] Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R Hr-
uschka, and Tom M Mitchell. 2010. Toward an architecture for never-ending
language learning. In Twenty-Fourth AAAI Conference on Artificial Intelligence.
[8] Luca Costabello, Sumit Pai, Chan Le Van, Rory McGrath, Nicholas McCarthy,
and Pedro Tabacof. 2019. AmpliGraph: a Library for Representation Learning on
Knowledge Graphs. https://doi.org/10.5281/zenodo.2595043

[9] Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018.
Convolutional 2d knowledge graph embeddings. In Thirty-Second AAAI Confer-
ence on Artificial Intelligence.

[10] Boyang Ding, Quan Wang, Bin Wang, and Li Guo. 2018. Improving Knowledge
Graph Embedding Using Simple Constraints. In 56th Annual Meeting of the
Association for Computational Linguistics.

[11] Shu Guo, Quan Wang, Lihong Wang, Bin Wang, and Li Guo. 2017.
Knowledge Graph Embedding with Iterative Guidance from Soft Rules.
arXiv:1711.11231 [cs.AI]

[12] Xu Han, Shulin Cao, Lv Xin, Yankai Lin, Zhiyuan Liu, Maosong Sun, and Juanzi
Li. 2018. OpenKE: An Open Toolkit for Knowledge Embedding. In Proceedings of
EMNLP.

[13] Yanchao Hao, Yuanzhe Zhang, Kang Liu, Shizhu He, Zhanyi Liu, Hua Wu, and
Jun Zhao. 2017. An end-to-end model for question answering over knowledge
base with cross-attention combining global knowledge. In Proceedings of the 55th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers). 221‚Äì231.

[14] Seyed Mehran Kazemi and David Poole. 2018. SimplE Embedding for Link

Prediction in Knowledge Graphs.

[15] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning
entity and relation embeddings for knowledge graph completion. In Twenty-Ninth
AAAI Conference on Artificial Intelligence.

[16] Deepak Nathani, Jatin Chauhan, Charu Sharma, and Manohar Kaul. 2019. Learn-
ing Attention-based Embeddings for Relation Prediction in Knowledge Graphs.
In Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics. 4710‚Äì4723.

[17] Matthew E. Peters, Mark Neumann, Robert L. Logan IV, Roy Schwartz, Vidur
Joshi, Sameer Singh, and Noah A. Smith. 2019. Knowledge Enhanced Contextual
Word Representations. CoRR abs/1909.04164 (2019). arXiv:1909.04164 http:
//arxiv.org/abs/1909.04164

[18] Daniel Ruffinelli, Samuel Broscheit, and Rainer Gemulla. 2020. You CAN Teach
an Old Dog New Tricks! On Training Knowledge Graph Embeddings. In Interna-
tional Conference on Learning Representations. https://openreview.net/forum?id=
BkxSmlBFvr

[19] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan
Titov, and Max Welling. 2018. Modeling relational data with graph convolutional
networks. In European Semantic Web Conference. Springer, 593‚Äì607.

[20] Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of
semantic knowledge. In Proceedings of the 16th international conference on World
Wide Web. ACM, 697‚Äì706.

[21] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 2019. RotatE: Knowl-
edge Graph Embedding by Relational Rotation in Complex Space. In 7th Interna-
tional Conference on Learning Representations.

