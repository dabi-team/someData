Memory-Efﬁcient Convex Optimization for
Self-Dictionary Separable Nonnegative Matrix
Factorization: A Frank-Wolfe Approach

Tri Nguyen, Xiao Fu, and Ruiyuan Wu

1

2
2
0
2

y
a
M
9

]
P
S
.
s
s
e
e
[

2
v
5
3
1
1
1
.
9
0
1
2
:
v
i
X
r
a

Abstract—Nonnegative matrix factorization (NMF) often relies
on the separability condition for tractable algorithm design.
Separability-based NMF is mainly handled by two types of
approaches, namely, greedy pursuit and convex programming.
A notable convex NMF formulation is the so-called
self-
dictionary multiple measurement vectors (SD-MMV), which can
work without knowing the matrix rank a priori, and is arguably
more resilient to error propagation relative to greedy pursuit.
However, convex SD-MMV renders a large memory cost that
scales quadratically with the problem size. This memory challenge
has been around for a decade, and a major obstacle for applying
convex SD-MMV to big data analytics. This work proposes a
memory-efﬁcient algorithm for convex SD-MMV. Our algorithm
capitalizes on the special update rules of a classic algorithm
from the 1950s, namely, the Frank-Wolfe (FW) algorithm. It is
shown that, under reasonable conditions, the FW algorithm solves
the noisy SD-MMV problem with a memory cost that grows
linearly with the amount of data. To handle noisier scenarios,
a smoothed group sparsity regularizer is proposed to improve
robustness while maintaining the low memory footprint with
guarantees. The proposed approach presents the ﬁrst linear
memory complexity algorithmic framework for convex SD-MMV
based NMF. The method is tested over a couple of unsupervised
i.e., text mining and community detection, to
learning tasks,
showcase its effectiveness and memory efﬁciency.

Index Terms—Unsupervised multimodal analysis, sample com-

plexity, identiﬁability

I. INTRODUCTION

Nonnegative matrix factorization (NMF) aims at factoring
RM×N into a product of two latent

a data matrix X
nonnegative factor matrices, i.e.,

∈

X

W H, W

RM×K
+

, H

RK×N
+

,

(1)

∈

≈

≤

M, N

min
{

∈
where K
. The NMF technique has been a
workhorse for dimensionality reduction, representation learn-
ing, and blind source separation. It ﬁnds a plethora of appli-
cations in machine learning and signal processing; see, e.g.,
[1], [2]. In particular, NMF plays an essential role in many

}

This work is supported in part by the Army Research Ofﬁce (ARO) under

Project ARO W911NF19-1-0407.

T. Nguyen and X. Fu are with the School of Electrical Engineering and
Computer Science, Oregon State University, Corvallis, OR 97331, United
States. Email: (nguyetr9, xiao.fu)@oregonstate.edu

R. Wu is with the Department of Electronic Engineering, The Chi-
nese University of Hong Kong, Shatin, N.T., Hong Kong SAR. Emai:
rywu@cuhk.edu.hk

statistical model learning problems, e.g., topic modeling [3]–
[5], community detection [6]–[9], crowdsourced data labeling
[10], [11], and joint probability estimation [12].

One major challenge of NMF lies in computation. It was
shown in [13] that NMF is an NP-hard problem in the worst
case, even without noise. In the last two decades, many ap-
proximate algorithms were proposed to tackle the NMF prob-
lem; see [1], [2], [14]. Notably, a line of work exploiting phys-
ically reasonable assumptions to come up with polynomial-
time NMF algorithms has drawn considerable attention. To
be speciﬁc, the so-called separable NMF approaches leverage
the separability condition to design efﬁcient and tractable
algorithms. More importantly, separable NMF algorithms are
often provably robust to noise [3], [15]–[18].

The separability condition is a special sparsity-related con-
dition. Interestingly, it is nonetheless well-justiﬁed in many
applications. For example, in topic modeling, the separability
condition holds if every topic has an “anchor word” that
does not appear in other topics [19]. In community detection,
separability translates to the existence of a set of “pure nodes”
whose membership is only associated with a speciﬁc commu-
nity [7]. The “pure pixel assumption” in hyperspectral imaging
is also identical to separability, which means that there exist
pixels that only capture one material spectral signature [20],
[21]. In crowdsourced data labeling, separability is equivalent
to the existence of expert annotators who are specialized for
one class [10].

Two major categories of algorithms exist for separable
NMF. The ﬁrst category is greedy algorithms. The represen-
tative algorithm is the successive projection algorithm (SPA),
which was ﬁrst proposed in the hyperspectral imaging com-
munity [21], and was re-discovered a number of times from
different perspectives; see [3], [17], [22]–[26]. Many of these
greedy algorithms have a Gram-Schmidt structure, and thus are
very scalable. However, they also share the same challenge of
Gram-Schmidt, i.e., error propagation. The second category
formulates the separable NMF problem as all-at-once convex
optimization criteria (see, e.g., [15], [16], [19], [27]–[31]),
which are arguably more robust to noise and less prone to
error propagation.

Among the all-at-once convex formulations of separable
NMF, the self-dictionary multiple measurement vectors (SD-
MMV) based framework in [15], [16], [27]–[30] has a series
of appealing features, e.g., identiﬁability of the latent factors
without knowing the model rank, computational tractability,
and noise robustness. Nonetheless, this line of work has seri-

 
 
 
 
 
 
×

ous challenges in terms of memory. These algorithms induce
a variable that has a size of N
N , which is not possible to
instantiate if N reaches the level of N = 100, 000—which
leads to a 74.5GB matrix if the double precision is used.
Consequently, these algorithms are often used together with
a data selection pre-processing stage to reduce problem size
(see, e.g., [30]). This may again create an error propagation
issue—if the data selection stage missed some important data
samples (e.g., those related to anchor words or pure nodes
in topic modeling and community detection, respectively), the
algorithms are bound to fail.

Contributions. In this work, we revisit convex optimization-
based SD-MMV for separable NMF. Our goal is to offer an
NMF identiﬁability-guaranteed algorithm that is also provably
memory-efﬁcient. Our contributions are as follows:

A Memory-Efﬁcient Algorithmic Framework. We ﬁrst
•
show that applying with the standard FW algorithm onto
a special SD-MMV formulation for separable NMF admits
identiﬁability of the ground-truth latent factors, if the noise is
absent. More importantly, the memory cost of this algorithm
is O(KN ) other than O(N 2), where K
N often holds.
Based on this simple observation, we further show that similar
conclusions can be drawn even if noise is present—under more
conditions.

≪

Regularization-Based Performance Enhancement. To cir-
•
cumvent stringent conditions and to improve noise robustness,
we propose a smoothed group-sparsity regularization that is
reminiscent of the mixed norm regularization often used in
SD-MMV formulations (see [15], [27], [29], [32], [33]). We
show that the optimal solution of such regularized formulation
corresponds to the desired ground-truth NMF factors in the
noisy case—i.e., identiﬁability of the NMF holds. We further
show that this regularization can better safeguard the memory
consumption within the range of O(KN ) compared to the
unregularized version, if the FW algorithm is initialized rea-
sonably. To our best knowledge, the proposed FW algorithmic
framework is the ﬁrst convex SD-MMV method whose mem-
ory cost scales linearly in N , while existing methods such as
those in [15], [16], [27], [28], [30] all need O(N 2) memory.
Synthetic and Real data Evaluation. We test the proposed
•
approach on various synthetic and real datasets. In particular,
we evaluate our algorithm using a couple of text corpora,
i.e., NIST Topic Detection and Tracking (TDT2) [34] and the
Reuters-21578 data1 on topic mining tasks. We also use social
network data from [35], [36] to evaluate the performance on
mixed membership community detection tasks. The proposed
approach is benchmarked by competitive state-of-the-art base-
lines for solving separable NMF.

Notation. We follow the established conventions in signal
RN denotes a real-valued N -
processing. In particular, x
∈
dimensional vector; xn and [x]n both denote the nth element
RN ×M denotes a real-valued matrix; rank(
of x; X
)
·
denotes matrix rank; superscript ⊤ is used for transpose;
k · kp
where p
k · kF denotes the
1 denotes the vector ℓp norm;
Frobenius norm; X(n, :) and xℓ denote the nth row and ℓth

≥

∈

1http://www.daviddlewis.com/resources/testcollections/reuters21578

2

K

K

−

X(i, :)

k
X
k

M
i=1 k

n, :) and X(:,

and
, respectively;

column of X, respectively; xn,ℓ, [X]n,ℓ and X(n, ℓ) all de-
note the (n, ℓ)th element of X; X(
ℓ) denote
−
the submatrices constructing from X by removing the nth row
c denote the cardi-
and ℓth column, respectively;
|K|
nality and complement of the set
counts the number of nonzero rows of X;

X
krow−0
k∞,1 =
k∞ is the mixed ℓ∞/ℓ1 norm; supp(x) denotes
a set of all indices of non-zero elements of vector x, i.e.,
P
[x]i 6
supp(x) :=
x1, x2, . . . , xk}
denotes
; conv
i
{
}
|
; 1 and 0 denote an all-
x1, x2, . . . , xk}
convex hull of set
{
one matrix/vector and an all-zero matrix/vector, respectively,
with proper sizes; ei denotes the ith unit vector; IN denotes
an identity matrix with a size of N
N ; both notations
×
x
0 and X
0 mean that the nonnegativity is applied
; σmax(X) and
[N ] means i
element-wise; i
1, . . . , N
σmin(X) denote the largest and smallest singular value of X,
respectively; λmax(X) denotes the largest eigenvalue of X;
N denotes the set of natural numbers.

≥
∈

= 0

∈ {

≥

{

}

II. PROBLEM STATEMENT

Consider a noisy NMF model, i.e.,

X = W H + V , W

0, H

0,

(2)

≥

≥

RM×K and H

RK×N are nonnegative latent
where W
∈
factors as deﬁned before, and V is a noise term. We further
assume that

∈

1⊤H = 1⊤,

(3)

i.e., the columns of H reside in the probability simplex. This
assumption naturally holds in many applications, e.g., topic
modeling, community detection, and hyperspectral unmixing
[6], [19], [20], [37]. When H does not have sum-to-one
columns, this assumption can be “enforced” through column
normalization of X, under the condition that W is nonnega-
tive, and there is no noise; see [1], [18]. We should mention
that although our interest lies in NMF, the proposed method
can also be applied to the so-called simplex-structured matrix
factorization, where W is not required to be nonnegative; see,
e.g., [8], [9], [25], [37].

Estimating the ground-truth W and H from X is in
general an NP-hard problem [13]. However, if the so-called
separability condition holds, the separable NMF algorithms
are often tractable, even under noise.

A. Separable NMF

The separable NMF problem uses the following premise:

Assumption 1 (Separability) There exists a set

=

ℓ1, . . . , ℓK}
) = IK . Equivalently, we have X(:,

K

{

such that H(:,
under (3), if noise is absent.

K

) = W

K

The condition was ﬁrst seen in [38] in the NMF literature.
The remote sensing community discovered it even earlier [21],
where the same condition is called the “pure pixel condition”
[20]. The condition has many names in applications, e.g.,

X

=

X

W

C
0

0

0

H

Fig. 1: Visualization of the idea of the row-sparsity-based SD-
MMV in [17], [27].

the “anchor word assumption” in topic modeling [3]–[5], the
“pure node condition” in community detection [6]–[8], and the
“specialized annotator condition” in crowdsourcing [11]; also
see its usage in speech processing [25], image analysis [39]
and wireless communications [32].

Under separability, the NMF task boils down to ﬁnding

,
K
since W
) if the noise level is not high. Then, H can
be estimated by (constrained) least squares if rank(W ) = K.

X(:,

≈

K

B. Convex Separable NMF

A way to look at the

-ﬁnding problem is to cast it as
a sparse atom selection problem. To be speciﬁc, when noise
is absent, consider the following row-sparsity minimization
formulation:

K

minimize
C∈RN ×N k

C

krow−0

(4a)

subject to X = XC, C

0, 1⊤C = 1⊤,

(4b)

≥
C
krow−0 counts the nonzero rows of C. For example,
, then an optimal solution of (4) is
1, . . . , K

where
k
if
=
{

K

}

C ⋆ =

H
0

(5)

(cid:20)
under mild conditions (e.g., rank(W ) = K). More formally,
we have:

(cid:21)

C ⋆(

K

, :) = H, C ⋆(

K

c, :) = 0;

(6)

K

see a proof for the general case in [17] and an illustration
can be identiﬁed by inspecting the
in Fig. 1. Hence,
nonzero rows of C ⋆. The formulation in (4) is reminiscent
of the multiple measurement vectors (MMV) problem from
compressive sensing [40], [41], but using the data itself as
the dictionary—which is the reason why (4) is called self-
dictionary multiple measurement vectors (SD-MMV) [17]. We
should mention that SD-MMV can be regarded as a way of
x1, . . . , xN }
picking up the vertices of the convex hull of
,
which is a popular perspective that many separable NMF
algorithms take; see, e.g., [3], [23], [24], [42]. Unlike the
classic vertex picking methods that often need the knowledge
of K, SD-MMV can work without knowing the number of
vertices.

{

C

The formulation in (4) is not easy to tackle, due to the
combinatorial nature of
krow−0. One popular way is to
use greedy pursuit, which selects the basis that represents
X using its convex combinations from the self-dictionary X
one by one. This leads to the successive projection algorithm
(SPA) [17], [18], [21], [23]. The greedy pursuit methods

k

3

are often effective, efﬁcient, and robust to a certain extent.
However, they also share the same challenge—the error could
be accumulated and propagated through the greedy steps.

Another line of work employs the convex relaxation idea
and use a convex surrogate for
krow−0—see [15], [27]–
[30], [32], [33], [43] for different options. For example, the
work in [15], [27] uses

C

k

N

C

k∞,1 =

k

C(n, :)

k

k∞

(7)

n=1
X
as their convex surrogate. When the noise is present, the work
in [15], [27] also advocated the following formulation

1
minimize
2 k
C∈RN ×N
subject to C

≥

X

XC

2
F + λ
k

−
k
0, 1⊤C = 1⊤,

C

k∞,1

(8a)

(8b)

≥

where λ
0 is a regularization parameter. The formulation is
convex and thus is conceptually easy to handle. After using any
convex optimization algorithm to obtain an optimal solution
C, the
set can be identiﬁed via inspecting
k∞
for all n and picking the K indices that are associated with
K largest row ℓ∞-norms. This method was shown to have
b
under noise [15]—also see similar results
identiﬁability of
for close relatives of (8) in [16], [28], [30], [43].

C(n, :)

K

K

b

k

In terms of noise robustness, the convex optimization-based
SD-MMV methods are arguably more preferable over greedy
simultaneously,
methods, since they identify all elements of
instead of in a one by one manner that is prone to error
propagation.

K

C. The Memory Challenge

Using convex programming to handle separable NMF is
appealing, since many off-the-shelf algorithms are readily
available. The convergence properties of convex optimization
algorithms are also well understood. However, general-purpose
convex optimization algorithms, e.g., the interior-point meth-
ods and gradient descent, may encounter serious challenges
when handling Problem (8). The reason is that the C variable
induces O(N 2) memory if a general-purpose solver is used.
This often makes running such algorithms costly and slow,
even when N is only moderate.

Many attempts were made towards accelerating convex
SD-MMV. The early work in [28] uses a fast projection to
expedite the algorithm for a variant of (8). The recent work
in [30] employed the fast gradient paradigm for accelerated
convergence. However, the O(N 2) memory issue could not
be circumvented in both approaches. In this work, our ob-
jective is a convex optimization algorithm that share similar
identiﬁcation properties of those in [15] but has provable
memory-efﬁciency—i.e., only O(N K) memory is needed for
instantiating the optimization variable C.

III. A FRANK-WOLFE APPROACH

A. Preliminaries on Frank-Wolfe Algorithm

Our development is based on the idea of the Frank-Wolfe
(FW) algorithm that was developed in the 1950s [44]. The FW

algorithm deals with problems of the following form:

f (θ)

minimize
θ∈Rd
subject to θ

,

∈ C

where
differentiable and convex.

is compact and convex and f (
·

C

) : Rd

→

The FW algorithm uses the following updates:

u

←

arg min

f (θt)⊤u,

u∈C ∇
αt)θt + αtu.

θt+1

(1

←

−

Here, αt is a pre-deﬁned sequence, i.e.,

(9a)

(9b)
R is

(10a)

(10b)

αt =

2
t + 2

, t = 0, 1, 2, . . . .

(11)

The above two steps constitute a standard procedure of the
FW algorithm [45], [44]. The idea of FW is intuitive: In each
iteration, FW linearizes the cost function and solve it locally
over the compact constraint set. For convex problems, the
plain-vanilla FW algorithm converges to an ε-optimal solution
using O(1/ε) iterations (which is also known as a sublinear
convergence rate). Recent works showed that FW (and its
variants) converges even faster (with a linear rate) under some
more conditions; see, e.g., [45], [46].

When dealing with large-scale optimization, especially
memory-wise costly optimization problems, FW may help
circumvent memory explosion due to its special update rule—
which has already facilitated economical nuclear norm opti-
mization paradigms that are important in recommender sys-
tems [45]. FW also features simple updates if the constraint
set is

=

θ

{

∈

C

|

RN

1⊤θ = 1, θ

0

.

}

≥

In this case, u in (10) is always a unit vector. This is because
(10a) is a linear program over the probability simplex
, and
C
the minimum is always attained at a vertex of
[47]. The
N
n=1. Hence, the solution of (10a) is ej
vertices of
where

are

C

C

{

en}
[
j = arg min
n∈[N ]

f (θt)]n.

∇

(12)

In our algorithm design, we will take advantage of this fact to
reduce the memory cost of solving SD-MMV.

Note that one needs not always start the FW algorithm from
t = 0. In [46], a warm-start based Frank-Wolfe (WS-FW)
algorithm was proposed for accelerated convergence. There,
one can use θtinit = θinit with tinit ≥
1 and start the FW
algorithm from the tinitth iteration using the corresponding
αtinit as deﬁned in (11). By carefully choosing tinit according
to the quality of θinit and the problem structure (e.g., the
curvature of the cost function), WS-FW is shown to converge
to the global optimum more efﬁciently.

B. Warm-up: The Noiseless SD-MMV

Our algorithm design starts with the simple formulation as

follows:

4

Note that in general, (13) does not admit identiﬁability of
—
the optimal solution of (13) needs not to have the form in (6).
A simple counter example is C = IN —which gives zero
objective value, but is not the desired solution. That is, one
cannot infer

from the nonzero rows of C = IN .

K

K

C. Noiseless Case and Simple Self-Dictionary Fitting

K

The ﬁrst observation is that although the criterion in (13)
, the FW algorithm guaran-

does not admit identiﬁability of
tees ﬁnding a C as in (6), thereby pining down

K

.

To see this, note that the updates of cℓ’s do not affect each

other. Consider the following update rule for ℓ = 1, . . . , N :

j

←

ct+1
ℓ ←

arg min
n∈[N ]
αt)ct

(1

−

[X⊤(Xct

ℓ −

ℓ + αtej,

xℓ)]n,

(14a)

(14b)

Since the formulation is convex, the above updates are guar-
anteed to solve the problem in (13). Before we examine what
solution the updates will lead to, let us discuss its memory
complexity, i.e., the reason why such updates are potentially
memory-economical.

To see how much memory that the algorithm needs beyond
storing X (which is inevitable), let us analyze the steps in
(14). First, evaluating gradient in (14a) is not memory-costing.
One can always evaluate ct
xℓ, followed by multiplying
ℓ −
with X, followed by another left multiplying X⊤. These three
operations produce vectors as their results and hence require
O(N ) memory complexity in total. Note that one can evaluate
the gradient w.r.t. C column by column, and no previously
evaluated columns need to be stored. Hence, the total memory
cost is O(N ). Second, the remaining memory requirement lies
in storing the iterates ct+1
for ℓ = 1, . . . , N . Consider an ideal
case where the j found in (14b) satisﬁes j
for all ℓ and
all t. Then, if the initialization C 0 = 0, the updating rule in
(14b) always maintains supp(ct
for all ℓ = 1, . . . , N .
ℓ)
This leads to an O(KN ) memory for storing C t.

⊆ K

∈ K

ℓ

Based on our discussion, the key to attain the above de-
scribed memory efﬁciency is to ensure that j
for all ℓ
and all t in (14b). We show that this is indeed the case under
some conditions:

∈ K

Theorem 1 (Noiseless Case, Memory Efﬁciency) Suppose
that Assumption 1 holds,
that no
repeated unit vectors exist in H 2, and that the noise is absent
(i.e., V = 0). Furthermore, deﬁne qt
hℓ).
Assume that the following holds:

that rank(W ) = K,

ℓ = W⊤W (Hct

ℓ −

qt
i,ℓ −

min
j

qt
j,ℓ 6

= 0,

i
∀

= arg min

j

qt
j,ℓ,

(15)

before ct
ℓ reaches an optimal solution. Then, the FW algorithm
in (10) with initialization C 0 = 0 always outputs the desired
solution in (6) using O(KN ) memory beyond storing X.

The proof is relegated to Appendix A. Note that Theorem 1
requires that Eq. (15) holds—i.e., no repeated minimum-value

minimize
C

1
2 k
k
subject to 1⊤C = 1⊤, C

XC

X

−

2
F

0.

≥

(13a)

(13b)

2In applications where there are many identical/similar columns in H (e.g.,
hyperspectral unmixing), the assumption can be “enforced” by clustering-
based pre-processing [27], [30].

6
elements of qℓ exist for any ct
some mild conditions:

ℓ. This is not hard to meet under

Proof: Let ℓ

, we have

∈ K

Fact 1 Assume that
continuous distribution. Then, we have

the columns of W follow any joint

Pr(qi,ℓ = qj,ℓ) = 0,

i, j

∀

∈

[K], ℓ

∈

[N ].

i z = b, w⊤

i z and yj = w⊤

Proof: Under the assumption, suppose that there exists
any b such that we have Pr(w⊤
j z = b) > 0 for
any vector z. That is,
there is a positive probability that
qℓ could have two identical elements. However, this cannot
happen. Indeed, note that yi = w⊤
j z are also
continuous random variables. Hence, Pr(yi = b, yj = b) = 0
for any b. This means that Pr(w⊤
i z = w⊤
jz) = 0. Letting
z = W (Hct
hℓ) and applying the above result complete
the proof.
Although Theorem 1 is concerned with the ideal noiseless
case, which is hardly practical, the observation in Theorem 1
opens another door for self-dictionary convex optimization-
based NMF. That is, it shows that using FW to solve the self-
dictionary problem may successfully circumvent the memory
explosion issue.

ℓ −

D. The Noisy Case

A natural question lies in if the same procedure works in
the presence of noise. The answer is afﬁrmative. To understand
this, we ﬁrst notice that in the noiseless case, we have

Hcℓ = hℓ, supp(cℓ)

ℓ
⊆ K ∀

∈

[N ] =

⇒

cℓ = c⋆
ℓ ,

ℓ
∀

∈

[N ].

where c⋆
leads to

ℓ is the ℓth column in C ⋆ deﬁned in (6). This further

c,

,

k

∀

∀

n

n

∈ K

∈ K

C(n, :)

C(n, :)
k

k∞ = 1,

k∞ = 0,
from the
which is the reason why one can easily infer
solution C = C ⋆. When noise is present, FW may not be
able to ﬁnd the exact C ⋆. However, an approximate C
C ⋆
often sufﬁces to identify
. In the following, Lemma 1 shows
that instead of ﬁnding cℓ such that Hcℓ = hℓ, seeking a cℓ
such that Hcℓ ≈
,
under reasonable conditions:

hℓ using FW does not hurt recovering

≈

K

K

K

Lemma 1 Assume that the separability condition holds, and
that rank(W ) = K, and that no repeated unit vectors appear
in H. Suppose that a feasible solution C satisﬁes supp(cℓ)
[N ]. Then, we have
ω for all ℓ

and

⊆

K

Hcℓ −

k

hℓk2 ≤

∈

C(n, :)
C(n, :)

1
k∞ ≥
k∞ = 0, n /

−

ω√K/2, n
;

∈ K

k

k

,

∈ K

can still be identiﬁed by inspecting

i.e.,
K
2/√K.

C(n, :)

k∞, if ω <

k

5

for some k

∈ K

ω

Hcℓ −

≥ k

hℓk2 =

≥

=

=

=

ek

(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
ci,ℓei −

ek

1)ek +

ci,ℓei −

(cid:13)
Xi∈K
(cid:13)
(cid:13)
(cid:13)
1
(cid:13)
√K (cid:13)
Xi∈K
(cid:13)
(cid:13)
(cid:13)
1
(cid:13)
(ck,ℓ −
√K (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
√K 

(1

−

1


(1

2
√K

−

ck,ℓ)

(cid:13)
(cid:13)
1
(cid:13)
(cid:13)
(cid:13)

Xi6=k

ci,ℓei(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1

ck,ℓ) +

ci,ℓ


Xi6=k

(since 1⊤cℓ = 1).

=

C(k, :)

⇒ k

−
On the other hands, since supp(cℓ)

k∞ ≥

ck,ℓ ≥

1

at 0, we have

ω

√K
2

⊆ K

for k

.

∈ K
and C is initialized

C(n, :)
k

k∞ = 0

for n

c.

∈ K

This completes the proof.

Next, we show in the following theorem that even in the
noisy case, applying FW onto (13) produces a solution that is
a reasonable approximation of C ⋆. To proceed, let us deﬁne
the following quantities:

Deﬁnition 1 Deﬁne the following terms:

hn,ℓ,

n∈K,ℓ∈Kc

1≤i≤N k

1≤k≤K k

δ := max

γ := max

wkk2 , d(H) := max
vik2 .
c are
In particular, a small d(H) means that all hℓ’s for ℓ
sufﬁciently different from the unit vectors. This is a desirable
case, since small perturbation would not confuse such hℓ’s
and the unit vectors.

∈ K

Theorem 2 (Noisy Case, Memory Efﬁciency) Suppose that
Assumption 1 holds, that there is no repeated unit vector
qt =
in H, and that rank(W ) = K. Furthermore,
W⊤W (Hct
hℓk2. During FW’s updates, as-
Hcℓ −
sume a positive gap between the smallest and second smallest
elements of

ℓ −
qt exists, i.e.,

hℓ)/

let

e

k

min
ℓ∈[N ],t

qt
e
i,ℓ −
(cid:18)

min
j

qt
j,ℓ

≥

(cid:19)

ν,

i
∀

= arg min

j

qt
j,ℓ,

(16)

before the FW algorithm terminates. Also assume that

e

e

e

δ

γ2 +

≤ s

νη(1

d(H))

−

4σmax(W ) −

γ,

(17)

for some η > 0. Then, if one terminates FW when
Xct
η + 2δ, the algorithm produces a solution
satisﬁes

ℓk2 ≤

xℓ −
k
C that

H

cℓ −

hℓk2 ≤

k

(η + 4δ)/σmin(W ).

b

b

6
In addition, during the process, supp(ct
always holds
ℓ)
for all t and ℓ, and thus only O(KN ) memory is taken for
instantiating C t for all t.

⊆ K

K

K

-revealing

The proof can be found in Appendix B. Theorem 2 shows
that under certain conditions, the FW algorithm for solving
C using only O(N K)
(13) indeed gives a reasonable solution
memory. Combining with Lemma 1, one can see that
can
be correctly selected if the noise level
is not high. Both
b
Theorems 1 and 2 reveal an unconventional identiﬁability
result. Note that the formulation in (13) per se does not have
identiﬁability of C ⋆. That is, the optimal solutions of (13) do
—as mentioned, IN is also an optimal
not necessarily reveal
solution. However, when one uses a particular algorithm (i.e.,
FW) to solve (13), the produced solution sequence converges
C, even if noise is present.
to a
On the other hand, the identiﬁability and memory efﬁciency
come with caveats. First, the condition in (16) is hard to check
or guarantee. As seen in Fact 1, ν > 0 does exist under mild
conditions—but the quantity of this parameter may change
from instance to instance and is hard to acquire. Using the
formulation in (13) is also not the most “natural”, since we
know that the desired C ⋆ should be very row-sparse—why
not using this information for performance enhancement. Can
we get rid of the condition in (16) and effectively use the prior
knowledge on C ⋆? The answer is afﬁrmative, with a re-design
of the objective function. In the next section, we will discuss
these aspects.

K

b

IV. PERFORMANCE ENHANCEMENT VIA REGULARIZATION

Under the formulation in (13), the gap speciﬁed in (16)
was essential for the FW to pick j
in every step. In this
section, our idea is to use a regularization term to help the
FW algorithm to achieve the same goal while not relying on
the condition in (16). In addition, since the desired C ⋆ in (6)
has a row-sparse structure, it is natural to add a regularization
to exploit this prior information.

∈ K

C

k∞,1 or

Using row-sparsity-promoting regularization terms is a com-
mon practice for self-dictionary convex optimization-based
NMF; see, e.g., [15], [16], [27]–[30], [32], [33]. In particular,
[15], [27], [29], [32], [33] all used the popular convex mixed
C
norms such as
1) for row-
sparsity encouraging—which are reminiscent of compressive
sensing [40], [41]. However, such convex norms may not
be the best to combine with our FW framework—since FW
arguably works the best with differentiable objectives due to
the use of gradient. There are subgradient versions of FW for
nonsmooth cost functions (see, e.g., [48]), but the algorithmic
structure is less succinct.

kq,1 (where q

≥

k

k

We still hope to use a regularization term like

k∞,1,
which was shown to have nice identiﬁability properties in SD-
MMV [15], [27]. To make this mixed-norm based nonsmooth
row-sparsity regularization “FW-friendly”, we use the follow-
ing lemma:

k

C

Lemma 2
ϕµ(x) = µ log

[49] For µ > 0 and x

1/N

(cid:16)

P

N
i=1 exp(xi/µ)
(cid:17)

RN

+ , deﬁne
. Then function ϕµ(x)

∈

is a smooth approximation of

x
k

k∞, i.e.,

6

lim
µ→0
x

ϕµ(x) =

x

k∞

k
µ log(N )

ϕµ(x)

x

k∞ .

k∞ −

k

≤
The above smoothing technique is from [49]. A proof is
presented in Appendix K in the supplementary material for
completeness. Building upon Lemma 2, a smooth approxima-
tion of

≤ k

C

k∞,1 is readily obtained as

k

N

Φµ(C) =

ϕµ(C(n, :))

C

k∞,1 .

≈ k

n=1
X

Using Φµ(C), our working formulation is as follows:

minimize
C∈RN ×N

1
2 k
subject to C

X

XC

2
F + λΦµ(C)

−
k
0, 1⊤C = 1⊤.

≥

(18a)

(18b)

The formulation can be understood as a smoothed version of
those in [15], [27], [29], [32]. Note that the problem in (18) is
still convex, but easier to handle by gradient-based algorithms
relative to the nonsmooth version.

A. Identiﬁability of

K

Our ﬁrst step is to understand the optimal solution of (18)—
i.e., if one optimally solves Problem (18), is the solution still
-revealing as in the nonsmooth version? To this end, we will

K
use the following deﬁnition [15], [16]:

Deﬁnition 2 The quantity κ(W ) is deﬁned as follows:

κ(W ) := min
k∈[K],
1⊤θ=1,θ≥0

wk −
k

W (:,

k)θ

k2 .

−

(19)

The term κ(W ) in a way reﬂects the “conditioning” of W .
If κ(W ) is large, it means that every wk is far away from
the convex hull spanned by the remaining columns of W ,
w1, . . . , wK}
which implies that conv
is well-stretched over
all directions. This is a desired case, since such convex hulls
are more resilient to small perturbations when estimating the
vertices (i.e., wk for k = 1, . . . , K).

{

With this deﬁnition, we show that the optimal solution of

(18) can reveal

K

under some conditions:

Theorem 3 (Identiﬁability) Assume
1
holds, that there is no repeated unit vector in H, and that
2
rank(W ) = K. Also assume that
F for
≤
δ. Then, any optimal solution Copt of
vik2 ≤
some ρ and
k
Problem (18) satisﬁes:

that Assumption

(ρ/N )

vik

V

k

k

k

2

Copt(n, :)

Copt(n, :)

k

k

,

β,

k∞ > 1
−
N
2ρ
k∞ ≤

n
∀
K
−
λN k
+ µN log(N ) + βK,

∈ K
V

2
F

k

where β = √4ρ(1−K/N )kV k2

n

∈ K
F+2λK+2δ/κ(W )(1−d(H)).

∀

(20a)

(20b)

c

The proof is relegated to Appendix C.

7

K

Theorem 3 reveals the “interplay” between the noise level
and the hyperparameters λ, µ. In other words, it states that
given a certain noise level, there may exist a pair of λ, µ such
that
will be identiﬁed using the proposed FW algorithm. For
example, when the noise level is not high, a natural choice
is to concentrate more on the ﬁtting error rather than the
regularization term. This is reﬂected in conditions (20a) and
2
(20b). Since
F is small, δ would be also small. Then,
k
with a small λ to suppress the term 2λK in the expression
of β, a small β can be expected. Similarly, with a small µ
to suppress the term µN log N in (20b), the right hand side
(RHS) of (20b) would be close to 0. In addition, when the
noise level is relatively high, one would want to increase λ
2
to counter the effect of increasing
F in (20b), but not to
increase it to an overly large extent (in order to keep β close
to 0, due to the presence of 2λK in the expression of β).

V
k

V

k

k

Theorem 3 asserts that with the proposed regularization,
ﬁnding an optimal solution of (18) is useful for identifying
. Such optimal solutions can be attained by any convex op-
K
timization algorithm. Nonetheless, theorem 3 only safeguards
the ﬁnal solution of our formulation in (18), which is a similar
result as in [15] for the nonsmooth regularization version (8).
However, Theorem 3 does not speak for the memory efﬁciency.

B. Memory Efﬁciency of Frank-Wolfe for (18)

In this subsection, we show that running FW to optimize
the new objective in (18) costs only O(KN ) memory under
some conditions—which are arguably milder compared to the
conditions for the unregularized case as shown in Theorem 2.
In the regularized case, the FW updates become the following:

j = arg min

[gℓ]n,

n∈[N ]

ct+1
ℓ = (1

−

α)ct

ℓ + αej ,

(21)

X

XC

f (C t) in which

where gℓ is the ℓth column of
f (C) :=
1
2
F + λΦµ(C). Similar as before, the key to
2 k
establish memory efﬁciency is to show that j picked in
(21) always belongs to
. The next theorem shows that the
regularization helps achieve this goal:

∇

−

K

b

b

k

Theorem 4 (Regularized Case, Memory Efﬁciency)
Consider the regularized case and the FW algorithm in (21).
Assume that Assumption 1 holds and that rank(W ) = K.
Also assume that (i) in iteration t, C t satisﬁes supp(ct
ℓ)
⊆ K
[N ] and (ii) there exists at least an n⋆
such
for all ℓ
that C t(n⋆, :) is not a constant row vector. If the noise bound
satisﬁes

∈ K

∈

δ

≤

γ2 + υ

γ,

−

p

(22)

λexp(−ψ/µ)

(d′(H)2 + 2d′(H) + 5)λmax(W⊤W )/2)

where

υ =

1
4

(λ/N

−

−

RM×N , λ, tinit, C init(if Warm Start)

Algorithm 1: MERIT
Input: X

∈

C = C init;

1 if ‘Warm Start’= True then
2
3 else
4

C = 0;
tinit = 0;

5
6 end
7 for t
←
α
8
←
r
←
for n
←
rn ←

9

end
for ℓ

10

11

12

13

14

15

16

17

18

tinit, tinit + 1, tinit + 2, . . . do
2/(t + 2);
0;

1, . . . , N do

N
i=1 exp(C(n, i)/µ);

// ‘

xℓ);
/ r ;

P
1, . . . , N do
X⊤(Xcℓ −
exp(cℓ/µ)
·

←
pℓ ←
yℓ ←
an element-wise division
between 2 vectors
gℓ ←
uℓ ←
cℓ ←

pℓ + λyℓ;
ei such that i = arg minj [gℓ]j;
(1

α)cℓ + αuℓ;

−

·

/’ denotes

end

19
20 end

Output: C.

in which we have

ψ :=

d′(H) =

min
1≤i,j≤N,
n,i6=ct

n∈K,ct

n,j (cid:12)
(cid:12)

max(2(d(H)

ct
n,i −

ct
n,j

,

(cid:12)
(cid:12)

1/2)2 + 1/2, 2(1/2

−

1/K)2 + 1/2),

−

then, to attain ct+1
of ct

p
ℓ such that jℓ ∈ K

ℓ

for all ℓ

[N ].

∈

from ct

ℓ, FW will update the jℓth element

The proof can be found in Appendix D. The noise bound
in Theorem 4 is arguably more favorable relative to the
unregularized case in Theorem 2. The reason is that λ can be
tuned to compensate noise. This is also intuitive since a larger
λ means that one has lower conﬁdence in the data quality due
to higher noise.

The key condition in Theorem 4 is the existence of C t(n⋆, :)
that is not a constant—which is reﬂected in ψ. At ﬁrst glance,
this is hard to guarantee since it
is a characterization of
C t. Nonetheless, we show that if one can properly initialize
the FW algorithm with an initial solution C init that satisﬁes
a certain regularity condition, the non-constant condition is
automatically satisﬁed—due to the “predictable” update rule
of FW:

Proposition 1 (Initialization Condition) Let C init be a fea-
sible initial solution. Deﬁne

Dn

ij(C) =

tinit(tinit + 1)
2

cn,i −

|

cn,j|

,
for some tinit ∈
∈ K
there exists a pair of i⋆, j⋆ such that the following regularity
condition is satisﬁed:

1. Suppose that for some n⋆

N, tinit ≥

Dn⋆

N.

i⋆j⋆ (C init) /
∈
Running WS-FW with C t = C init starting with t = tinit
t for
for T iterations, the produced solution sequence
}
tinit + 1 satisﬁes C t(n⋆, :) is not a constant row vector
t
≥
and

(23)

C

{

min
n∈K,i,j
n,i6=ct
ct

n,j (cid:12)
(cid:12)

ct
n,i −

ct
n,j

≥

2ξ
T (T + 1)

(cid:12)
(cid:12)

Dn

i,j(C init)

z

.

−

(cid:12)
(cid:12)

(cid:12)
(cid:12)

where ξ := min n∈K,i,j,z∈N
i,j (Cinit) /∈N

Dn

The proof is in Appendix J. Theorem 4 and Proposition 1
together mean that for a given problem instance and under
a certain noise level, there exist proper parameters λ, µ that
guarantee the recovery of
using O(KN ) memory to instan-
tiate C t.

K

∈ K

Simply speaking, Proposition 1 asserts that, if there exists
a row in C init(n⋆, :) where n⋆
and this row has two
elements whose difference is not a natural number,
then
Theorem 4 holds with in ﬁnite iterations. Under Theorem 4
and Proposition 1, it is natural to run the WS-FW with a
t = tinit > 0 as in [46]. Many lightweight algorithms (e.g., the
greedy methods) can be employed to provide the initialization.
The condition in Proposition 1 is fairly mild, since it
boils down to the existence of two distinctive elements in
any row of the initial C. The Proposition also suggests that
using some existing algorithms to initialize the proposed FW
algorithm may be appropriate, since one needs at least one
n⋆
such that the speciﬁed conditions
are met. Any greedy algorithm, e.g., [17], [18], could help
offer this n⋆ using a couple of iterations. Although checking
the initialization condition is easy, we should also mention
that it may not be necessary to really check it in practice.
The reason is that the regularity condition in Proposition 1 is
only sufﬁcient—which means that in practice one often needs
not to enforce C init to satisfy it. In fact, the FW algorithm
works well and maintains a low memory footprint even using
C init = 0.

supp(C init)

∩ K

∈

To summarize, we present the memory-efﬁcient Frank-Wolfe
based nonnegative matrix factorization (MERIT) algorithm in
Algorithm 1. An implementation of MERIT can be down-
loaded from the authors’ website3.

V. NUMERICAL RESULTS

In this section, we use synthetic and real data experiments
to showcase the effectiveness of the proposed FW-based
approach.

A. Synthetic Data Simulations

We create synthetic data matrices with different M, N and
K, under the noisy signal model X = W H + V . The

3https://github.com/XiaoFuLab/Frank-Wolfe-based-method-for-SD-MMV

8

}

{

U

K

−

=

), the N

matrix W is drawn from the uniform distribution
(0, 1), the
ﬁrst K columns of H are assigned to be an identity matrix
K remaining
(which means that
1, . . . , K
columns of H are generated so that every column resides in
the probability simplex (see details later). After adding zero-
mean σ2-variance Gaussian noise V to W H, the columns of
the data matrix are then permuted randomly to obtain the ﬁnal
X—this means that
for each random trial is different. The
signal-to-noise ratio (SNR) used in this section is deﬁned as
2
2)/(M N σ2)dB.
SNR= 10 log10(
Baselines and Metric. We use the SPA algorithm [17],
[18], [21] that
-
K
identiﬁcation in separable NMF as our baseline. We also
employ the FastGradient algorithm [30] that is designed
to solve a convex self-dictionary formulation of SD-MMV. The
algorithm uses accelerated gradient for fast convergence, and
is considered state-of-the-art.

P
is the prominent greedy algorithm for

W hℓk

N
ℓ=1 k

K

k

K

K

K

We apply the algorithms to the problem instances and select
from their outputs as follows. For SPA, we use the ﬁrst
K
K indices output by the greedy steps to serve as
. For
FastGradient, we use the authors’ implementation and its
. Following Theorem 3, we select
default methods to pick up
C(n, :)
k∞ values as
indices of K rows that have K largest
.
an estimation of
To select the hyperparameter λ of MERIT, we use an idea
similar to that in [30], which is a heuristic that selects λ to
balance the data ﬁtting residue and the regularization term. In
F /Φµ(C 0)
XC 0
our case, the suggestion is to set λ =
X
(or simply λ =
F /K when K is known), where
(cid:13)
C 0 is an initial solution that can be constructed by some
(cid:13)
(cid:13)
fast separable NMF algorithms, e.g., SPA. The parameter µ
(cid:13)
is set to be 10−5 throughout this section unless otherwise
speciﬁed, as it is fairly inconsequential. The hyperparameter
of FastGradient is chosen by its default heuristic.

XC 0

X

(cid:13)
(cid:13)

(cid:13)
(cid:13)

−

−

We use a number of metrics to evaluate the performance.
identiﬁ-

In particular, we primarily use the success rate of
cation, which is deﬁned as

K

success rate = Pr(

K

=

).

K

b

W = X(:,

In our simulations, the success rate is estimated using 50
random trials. We also adopt two complementary metrics from
[30], namely, the mean-removed spectral angle (MRSA) and
the relative approximation error (RAE). In a nutshell, the
MRSA measures how well W is estimated via
)
K
W (together
and the RAE measures how well the estimated
H) can reconstruct the data X; see details
with an estimated
c
b
in [30]. Following [30], the MRSA values are normalized to an
interval of [0, 100]. In addition, the RAE values are in between
0 and 1. Lower MRSAs and higher RAEs correspond to better
performance of separable NMF.
Results. We ﬁrst evaluate the algorithms under a setting from
c)’s columns are middle points
[30, Sec. 4.1], where H(:,
between the extreme points of the unit simplex. This way,
noise could easily confuse the xℓ’s associated with the middle
points with the true extreme points of conv
—
thereby presenting a challenging case for separable NMF
algorithms.

x1, . . . , xN }

c

c

K

{

Fig. 2: Success rates of the algorithms under various SNRs;
(M, N, K) = (50, 55, 10).

SNR

9

t

e
a
r

s
s
e
c
c
u
s

)

B
G

(

S
S
R
n

i

t
s
o
c

y
r
o
m
e
M

SNR

(a) Success rate under various
SNRs; N = 200.

N
(b) Memory consumption un-
der various N s; SNR=10dB.

Fig. 4: Success rate performance and memory costs of the
algorithms; M = 50, K = 40.

TABLE I: Performance of the algorithms under various K’s;
N = 200, M = 80, SNR=10dB.

K

success rate
SPA FastGradient MERIT

40 0.98
50 0.84
60 0.42
70 0.00

0.98
1.00
1.00
1.00

1.00
1.00
1.00
1.00

SPA
54.7824
55.4776
56.4556
58.9853

e
t
a
r

s
s
e
c
c
u
s

FastGradient MERIT

SPA
54.7292 0.7686
55.1517 0.7827
55.2206 0.7941
55.6658 0.8022

RAE

FastGradient MERIT
0.7686
0.7830
0.7955
0.8069

0.7686
0.7830
0.7955
0.8069

MRSA

54.7724
55.1517
55.2206
55.6658

e
t
a
r

s
s
e
c
c
u
s

SNR

(a) MRSA

SNR
(b) RAE

Fig. 3: The MRSAs and RAEs of the algorithms under various
SNRs. (M, N, K) = (50, 55, 10).

SNR

SNR

(a) Varying λ; µ = 1e−5.

(b) Varying µ; λ = 0.1.

Fig. 2 shows the success rates of different methods under the
generative model and setting from [30], where (M, N, K) =
(50, 55, 10). One can see that FastGradient and MERIT
both exhibit more satisfactory performance relative to SPA,
which echos our comment that the all-at-once convex ap-
proaches often have better noise robustness relative to greedy
pursuit. In particular, FastGradient and MERIT reach
100% success rates at SNR= 16dB and SNR= 10dB, respec-
tively, while SPA does not reach this accuracy even when
SNR= 20dB. Fig. 3 shows the MRSAs and RAEs of the
algorithms under the same setting, where similar observations
are made.

K

Fig. 4 (a) shows the success rates of the algorithms un-
der different SNRs using (M, K, N ) = (50, 40, 200) and
c)’s that are less special than that in the previous case;
H(:,
c)’s columns are generated following the uniform
i.e., H(:,
RK. One
Dirichlet distribution with its parameter being 1
can see that the algorithms perform similarly as in the case of
Fig. 2, except that the gap between SPA and the convex ap-
proaches MERIT (with regularization) and FastGradient
is larger than that in the previous case.

K

∈

Fig. 5: Performance varies on different λ and µ; results are
averaged over 30 trials; (M, K, N ) = (50, 40, 200).

gram. The RSS is measured using a Linux built-in command
named time4. One can see that MERIT’s memory growth
along with N is very graceful, but FastGradient quickly
reaches the level that is close to 5GB when N = 10, 000—
while MERIT uses less than 0.1GB memory under the same
problem size.

Table I shows the performance of the algorithms under var-
ious K’s. As expected, SPA works better when K is relatively
small. The performance deteriorates when K increases, show-
ing the effect of error accumulation. The convex approaches
work similarly and exhibit consistently good performance
across all the K’s under test.

Fig. 5 shows the impact of the hyperparameters λ and µ
on the MERIT algorithm. One can see that for lower SNRs,
a larger λ often works better—which is consistent with our
analysis and intuition. The parameter µ is less consequential.
That is, the wide range of µ’s tested in our simulations give
almost the same success rate curves.

Fig. 4 (b) shows the memory costs of the two convex
optimization-based algorithms under different N ’s. Here, we
set (M, K) = (50, 40) and SNR= 10dB. The memory is
measured in terms of maximum resident set size (RSS), which
is the amount of allocated memory in RAM for a running pro-

B. Real Data Experiment: Topic Modeling
Data. We use two popular datasets, namely, the NIST Topic
Detection and Tracking (TDT2) and the Reuters-21578 cor-

4https://man7.org/linux/man-pages/man1/time.1.html

 
 
 
 
 
 
 
pora, for the evaluation. Following the settings in topic mod-
eling papers, e.g., [4], [50], we use single-topic documents so
that classic evaluation metrics (e.g., clustering accuracy [50],
[51]) can be easily used. TDT2 contains D = 8, 384 single-
topic documents with N = 36, 771 words as its vocabulary,
while Reuters-21578 contains D = 8, 293 single-topic docu-
ment with N = 18, 933 words in its vocabulary. All stop words
are removed before running each trial.

)

B
G

(

S
S
R
n

i

t
s
o
c

y
r
o
m
e
M

)

B
G

(

S
S
R
n

i

t
s
o
c

y
r
o
m
e
M

10

×

is,

Note that we follow the formulation in [3]–[5] that ap-
plies NMF in the correlation domain. That
the word-
word correlation (or co-occurrence) data matrix X has a size
N . The matrix X is estimated by the Gram ma-
of N
trix of the word-document term-frequency-inverse-document-
frequency (TF-IDF) representation of the data; see more
details in [4], [5]. Following the work in [17], a pre-processing
step for noise reduction is used in this subsection. Speciﬁcally,
before running the separable NMF algorithms, the principal
component analysis (PCA)-based dimensionality reduction
(DR) is used to reduce the row dimension of the co-occurrence
matrix to M ′ = 2K, which serves as an over estimate for K.
In practice, such DR method can be easily implemented with
any M ′ > K if K is roughly known. After the DR process,
one factor of the dimension-reduced factorization model may
not be nonnegative. Thus, technically, the model is not “NMF”.
Nonetheless, all the SD-MMV methods can still be applied
since the nonnegativity of the left factor (W ) is never used in
attaining
Baseline and Algorithm Settings. We use a number of sepa-
rable NMF based topic modeling algorithms as our baselines
in this experiment. In addition to SPA and FastGradient,
we also use XRAY [52] and FastAnchor [3] that are both
greedy algorithms and variants of SPA developed under the
context of topic modeling. The LDA [53] method using Gibbs
sampling [54] is also employed, as a standard baseline.

K

.

We use warm start for both MERIT and FastGradient.
, denoted

Particularly, we use SPA to extract an estimate of
as

SPA. Then, we compute C init by

K

K

C init(

b

K

SPA, :) = arg min

1⊤H=1⊤,H≥0 k

X

−

X(:,

)H

2
F,

k

K

b

K

SPA, :) = 0. For MERIT, we set λ =
and let C init(
c
10−6 and µ = 10−5 in all cases. We try multiple λ’s
for FastGradient and present
the best performance it
b
attains. For MERIT, since we use WS-FW (cf. Sec. III-A),
the tinit needs to be determined. In the WS-FW paper [46],
tinit in theory is computed using the curvature constant as-
sociated with optimization problem and initial duality gap.
However, as admitted in [46], estimating the curvature con-
is still an open challenge. Hence, we use a heuris-
stant
tic that tinit = round(1/RMSEinit), where RMSEinit =
2
F. This reﬂects the idea that a better

XC init

1/N

initialization should use a larger tinit.
p
Metrics. We use the three metrics from [4], [5], namely,
coherence (Coh), word similarity count (SimCount), and
clustering accuracy (ClustAcc). The Coh metric evaluates
if a single topic’s quality by measuring the coherence of the
high-frequency words contained in this topic. The SimCount
metric measures how diverse are the mined topics. The

X

−

k

k

N

(a) TDT2

N
(b) Reuters-21578

Fig. 6: Memory consumption of FastGradient and
MERIT, under different sample sizes.

c

ClustAcc compares the estimated
H with the ground-truth
labels after automatic permutation removal using the Kuhn-
Munkres algorithm; see more details of the evaluation process
in [1], [4] and the deﬁnition of ClustAcc in [51, Sec. 5.2]..
A good topic mining algorithm is expected to attain high
Coh values, low SimCount and high ClustAcc values.
Among the three, ClustAcc is arguably the most indicative
for the quality of the mined topics if the objective is to use
the topics for downstream tasks. For each trial, we randomly
draw documents associated with K topics from the datasets
and apply the algorithms. The results for each K are averaged
from 50 trials.
Results. Tables II shows the performance of the algorithms on
TDT2 and Reuters-21578, respectively. One can see that both
the regularized and unregularized versions of the proposed
method, i.e., MERIT and MERIT(0) (i.e., the version of
MERIT with λ = 0), exhibit competitive performance. The
proposed methods outperform all the baselines on TDT2 in
terms of Coh and ClustAcc. The SimCount performance
of the proposed algorithms is also reasonable. In particular,
when K = 10, the ClustAcc of MERIT exhibits a 5%
improvement compared to the best baseline, which is a re-
markable margin. On Reuters-21578, the MERIT method also
consistently offers the best and second best performance in
terms of ClustAcc in most cases.

From these results, one can see that the convex optimiza-
tion based separable NMF algorithms, i.e., FastGradient,
MERIT and MERIT(0),
in general work better than the
greedy methods, namely, SPA, XRAY, and FastAnchor.
This is consistent with our observation in the synthetic data
experiments. This advocates using such all-at-once algorithms
for real applications. Another observation is that MERIT
slightly outperforms MERIT(0), which shows that the de-
signed regularization is still effective on topic modeling prob-
lems.

Fig. 6 shows the memory consumption of the algorithms on
TDT2 and Reuters-21578, respectively. Since the data matrix’s
size changes in each trial due to the varying stop words, we
only plot the ﬁrst trial for each K. One can see that when
N reaches 20, 000, FastGradient uses more than 20GB
memory, while MERIT and MERIT(0) use less than 2GB.
We observe that MERIT(0) works well in the topic modeling
experiment, perhaps because the data model is reasonably well
aligned with that in (2) without much modeling error.

 
 
 
 
 
 
 
 
TABLE II: Performance of the algorithms on: (Left) TDT2; vocabulary size= 36, 771, and (Right) Reuters-21578; vocabulary
size = 18, 933

11

Coherence

Similarity
Count

Accuracy

Method / K
SPA

MERIT
MERIT(0)
SPA
FastAnchor
XRAY
LDA

TDT2

3

4

5

6

7

8

9

10

3

4

Reuters-21578
6

5

7

8

9

10

XRAY
LDA

-346.6 -388.4 -404.9 -432.0 -418.6 -438.2 -443.5 -456.7
FastAnchor -468.6 -483.4 -483.3 -495.9 -525.8 -536.2 -546.5 -543.2
-347.4 -389.2 -405.4 -432.0 -419.0 -439.4 -443.2 -459.2
-521.6 -526.2 -530.4 -546.0 -550.0 -538.8 -543.1 -553.1
FastGradient -553.8 -517.1 -537.2 -534.6 -561.9 -562.7 -571.9 -585.5
-351.5 -375.7 -385.8 -394.4 -399.3 -417.2 -417.5 -429.1
-345.0 -388.4 -404.8 -433.4 -420.1 -439.4 -444.3 -458.3

XRAY
LDA

-402.7 -416.4 -420.5 -442.1 -516.5 -520.3 -541.5 -548.3
FastAnchor -655.0 -681.0 -693.6 -711.1 - 757.5 -827.7 -832.8 -843.4
-404.4 -415.2 -422.7 -441.6 -516.3 -519.6 -542.2 -548.6
-674.1 -677.2 -686.3 -715.2 -705.9 -762.9 -776.8 -776.5
FastGradient -657.1 -768.3 -782.0 -821.8 -847.1 -967.7 -989.5 -959.8
-430.6 -452.8 -466.4 -494.0 -539.2 -541.1 -564.8 -570.8
-401.7 -413.3 -422.5 -440.8 -511.2 -518.2 -536.0 -544.0

1.06
1.06
1.00
1.08

43.62
11.22
43.4
27.5
FastGradient 14.80 26.34 47.16 62.28 71.24 100.58 109.84 127.32
36.08
43.06

10.24 14.24 23.18
4.80
7.98
6.18
10.24 14.16 23.18
12.24 17.28
7.84

7.92
13.30 21.16
10.56 14.38 22.62

27.56
9.92
28.00
21.84

5.76
3.90
5.66
5.62

3.64
2.02
3.88
2.96

28.52
27.50

4.98
3.64

5.76
5.78

7.46
5.40
6.76
3.20

15.16 23.82 51.98
8.46
13.06 20.06
14.18 23.82 52.06
12.48
9.32
6.46
FastGradient 12.96 20.62 30.42 47.56
16.04 21.88 36.08
15.18 23.24 45.12

7.34
7.38

54.9

42.28

59.38 158.50 235.62 219.16
25.56
57.84
59.64 160.96 235.10 221.50
24.60
21.22
39.68
33.56
82.86 106.66 144.38
60.46
48.36
93.32 131.62 141.42
54.60 145.52 223.66 214.60

Method / K
SPA

MERIT
MERIT(0)
SPA
FastAnchor
XRAY
LDA

1.56
1.06

MERIT
MERIT(0)
SPA
FastAnchor
XRAY
LDA

0.87
0.77
0.87
0.78
FastGradient 0.70
0.88
0.86

MERIT
MERIT(0)

0.83
0.72
0.82
0.77
0.71
0.88
0.83

0.81
0.67
0.80
0.74
0.65
0.85
0.80

0.81
0.63
0.81
0.75
0.64
0.86
0.81

0.78
0.66
0.78
0.73
0.61
0.84
0.78

0.76
0.63
0.75
0.72
0.56
0.82
0.76

0.75
0.65
0.75
0.68
0.58
0.80
0.75

0.72
0.65
0.71
0.70
0.57
0.77
0.72

MERIT
MERIT(0)
SPA
FastAnchor
XRAY
LDA

0.64
0.60
0.63
0.63
FastGradient 0.62
0.66
0.64

MERIT
MERIT(0)

0.57
0.57
0.57
0.57
0.57
0.62
0.58

0.54
0.52
0.54
0.53
0.56
0.53
0.54

0.51
0.52
0.51
0.51
0.51
0.53
0.52

0.49
0.46
0.49
0.46
0.50
0.51
0.49

0.44
0.42
0.45
0.44
0.48
0.48
0.44

0.42
0.38
0.42
0.41
0.44
0.43
0.42

0.40
0.37
0.40
0.42
0.46
0.45
0.41

C. Real Data Experiment: Community Detection

≈

×

The link between mixed membership stochastic model
(MMSB)-based overlapped community detection and simplex-
structured matrix factorization has often been used in the liter-
ature; see [6]–[9]. By applying eigendecomposition to a node-
node undirected adjacency matrix, the membership estimation
problem boils down to a noisy matrix factorization under
probability simplex constraints. This leads to X
W H,
RK×N is extracted by the eigendecomposition of
where X
∈
N adjacency matrix (the rows of X are the ﬁrst
the N
K eigenvectors), H’s columns are the mixed membership
vectors correspond to the N node. More precisely, hk,ℓ is the
probability that node ℓ is associated with community k. This
physical interpretation also means that 1⊤H = 1⊤ and H
0.
Hence, both SPA and convex optimization based separable
NMF algorithms can be applied to tackle this problem, if
the separability condition holds. In the context of commu-
nity detection, separability is equivalent to the existence of
“pure nodes” for each community, i.e., nodes who are only
associated with a single community.
Data. We use two co-authorship networks, namely, the Data
Base systems and Logic Programming (DBLP) data and the
Microsoft Academic Graph (MAG) data. A community in
DBLP is deﬁned as a group of conferences. The “ﬁeld of
study” is considered as a community in MAG. The ground-
truth memberships of the nodes in these two datasets are
known; see the Matlab format of this data from [7].

≥

In our experiments, we consider the nodes who contribute
to 99% of the energy (in terms of squared Euclidean norm).
The remaining nodes are not used in the algorithms due to
their rare collaboration with others. The detailed statistics of
the used DBLP and MAG data are given in Table III.
Baselines. We compare MERIT and MERIT(0) with three
two algorithms are GeoNMF [7] and
baselines. The ﬁrst
SPOC [8] as they are reportedly popular within the class
of greedy method in context of community detection. And
FastGradient as a candidate for the convex optimiza-

TABLE III: Statistics of the DBLP and MAG datasets Used
In The Experiments.

Dataset
DBLP1
DBLP2
DBLP3
DBLP4
DBLP5
MAG1
MAG2

# nodes
6437
3248
3129
1032
3509
1447
4974

# communities
6
3
3
3
4
3
3

TABLE IV: SRC Performance on DBLP and MAG.

Dataset GeoNMF SPOC FastGradient MERIT MERIT(0)
DBLP1
DBLP2
DBLP3
DBLP4
DBLP5
MAG1
MAG2

0.2996
0.2126
0.2972
0.3479
0.1720
0.1173
0.1531

0.2974
0.2948
0.2629
0.2661
0.1977
0.1349
0.1451

0.3145
0.3237
0.1933
0.1601
0.0912
0.0441
0.2426

0.2912
0.2931
0.2766
0.3559
0.1983
0.1074
0.1374

0.2937
0.3257
0.2763
0.3559
0.1983
0.1149
0.2414

tion based approach is included. MERIT uses the same
hyperparameters setting as in the topic modeling problem,
i.e., λ = 10−6, µ = 10−5. Again, we try multiple λ’s for
FastGradient and report its best performance.
Metric. Following [7]–[9], we evaluate the performance based
on the averaged Spearman’s rank correlation (SRC) coefﬁ-
H
cient between the learned community membership matrix
and the ground-truth H.

By the deﬁnition, the SRC measures the ranking similarity
c
between the estimated and ground-truth mixed membership
H is obtained by probability simplex-
vectors. The estimated
W =
constrained least squares using data X and the basis
X(:,
). By deﬁnition, SRC can take value in the interval from
1 to 1. A higher value indicates a better alignment between

−
H and H.
b
Results. From Table IV, one can see that the proposed MERIT
c
method offers competitive SRC results over all the datasets

c

c

K

)

B
G

(

S
S
R
n

i

t
s
o
c

y
r
o
m
e
M

N

7: The memory

Fig.
FastGradient after the eigendecomposition step.

consumption

of MERIT and

under test. Similar as before, the convex all-at-once algorithms,
especially, MERIT, and MERIT(0), often output competitive
SRC results. In particular, the MERIT class’ SRC values are
in top 2 over ﬁve out of seven datasets. The greedy methods
(i.e., GeoNMF, and SPOC) also work reasonably well, but
less competitive in some datasets. For example, in MAG2 and
DBLP2, the performance gap between the greedy algorithms
and the convex methods are particularly articulate.

7

Fig.

compares

consumption

the memory

of
FastGradient and MERIT. As N reaches 6500,
FastGradient consumes more than 2GB while MERIT
and MERIT(0) use signiﬁcant
i.e., under
0.3GB. This is consistent with our observations in the topic
modeling examples.

less memory,

A ﬁnal remark is that

the theorems in this work are
based on worst-case analyses, and thus the noise bounds and
identiﬁcation conditions are naturally pessimistic. However,
note that our conditions are only sufﬁcient (instead of sufﬁcient
and necessary). This may explain the reason why the proposed
method works under many settings of the experiments where
the noise level is quite high.

VI. CONCLUSION

In this work, we revisited convex optimization-based self-
dictionary SD-MMV for separable NMF. This line of work
emerged about a decade ago as a tractable and robust means
for solving the challenging NMF problem. The method is
recognized as an important category of NMF algorithms,
but has serious challenges when dealing with big data. In
particular, existing convex SD-MMV approaches’ memory
complexity scales quadratically with the number of samples,
which is hardly feasible for datasets that have more than 1000
samples. We proposed a new algorithm based on the Frank-
Wolfe method, or, conditional gradient. Unlike existing algo-
rithms, our method is shown to have linear memory complexity
under mild conditions—even in the presence of noise. For
performance enhancement, we also offered a smoothed row-
sparsity-promoting regularizer, and showed that it can provide
stronger guarantees for memory efﬁciency against noise under

12

the FW framework. We tested the algorithm using synthetic
data and real-world topic modeling and community detection
datasets. The results corroborate our theoretical analyses.

REFERENCES

[1] X. Fu, K. Huang, N. D. Sidiropoulos, and W.-K. Ma, “Nonnegative
matrix factorization for signal and data analytics: Identiﬁability, Algo-
rithms, and Applications,” IEEE Signal Process. Mag., vol. 36, no. 2,
pp. 59–80, March 2019.

[2] N. Gillis, Nonnegative Matrix Factorization. SIAM, 2020.
[3] S. Arora, R. Ge, Y. Halpern, D. Mimno, A. Moitra, D. Sontag, Y. Wu,
and M. Zhu, “A practical algorithm for topic modeling with provable
guarantees,” in International Conference on Machine Learning, 2013,
pp. 280–288.

[4] K. Huang, X. Fu, and N. D. Sidiropoulos, “Anchor-free correlated
topic modeling: Identiﬁability and algorithm,” in Advances in Neural
Information Processing Systems, 2016, pp. 1786–1794.

[5] X. Fu, K. Huang, N. D. Sidiropoulos, Q. Shi, and M. Hong, “Anchor-
free correlated topic modeling,” IEEE Trans. Pattern Anal. Mach. Intell.,
vol. 41, no. 5, pp. 1056–1071, May 2019.

[6] K. Huang and X. Fu, “Detecting overlapping and correlated communities
without pure nodes: Identiﬁability and Algorithm,” in International
Conference on Machine Learning, 09–15 Jun 2019, pp. 2859–2868.
[7] X. Mao, P. Sarkar, and D. Chakrabarti, “On mixed memberships and
symmetric nonnegative matrix factorizations,” in International Confer-
ence on Machine Learning, 2017, pp. 2324–2333.

[8] M. Panov, K. Slavnov, and R. Ushakov, “Consistent estimation of mixed
memberships with successive projections,” International Workshop on
Complex Networks and their Applications, pp. 53–64, 2017.

[9] S. Ibrahim and X. Fu, “Mixed membership graph clustering via system-

atic edge query,” IEEE Trans. Signal Process. accepted, 2021.

[10] ——, “Crowdsourcing via annotator co-occurrence imputation and
provable symmetric nonnegative matrix factorization,” in International
Conference on Machine Learning, 2021.

[11] S. Ibrahim, X. Fu, N. Kargas, and K. Huang, “Crowdsourcing via
pairwise co-occurrences: Identiﬁability and Algorithms,” in Advances
in Neural Information Processing Systems, 2019, pp. 7847–7857.
[12] S. Ibrahim and X. Fu, “Recovering joint probability of discrete random
variables from pairwise marginals,” IEEE Trans. Signal Process., vol. 69,
pp. 4116–4131, 2021.

[13] S. A. Vavasis, “On the complexity of nonnegative matrix factorization,”

SIAM Journal on Optimization, vol. 20, no. 3, pp. 1364–1377, 2009.

[14] X. Fu, N. Vervliet, L. De Lathauwer, K. Huang, and N. Gillis, “Comput-
ing large-scale matrix and tensor decomposition with structured factors:
A uniﬁed nonconvex optimization perspective,” IEEE Signal Process.
Mag., vol. 37, no. 5, pp. 78–94, 2020.

[15] X. Fu and W.-K. Ma, “Robustness analysis of structured matrix fac-
torization via self-dictionary mixed-norm optimization,” IEEE Signal
Processing Letters, vol. 23, no. 1, pp. 60–64, 2015.

[16] N. Gillis and R. Luce, “Robust near-separable nonnegative matrix fac-
torization using linear optimization,” The Journal of Machine Learning
Research, vol. 15, no. 1, pp. 1249–1280, 2014.

[17] X. Fu, W.-K. Ma, T.-H. Chan, and J. M. Bioucas-Dias, “Self-dictionary
sparse regression for hyperspectral unmixing: Greedy pursuit and pure
pixel search are related,” IEEE J. Sel. Topics Signal Process., vol. 9,
no. 6, pp. 1128–1141, 2015.

[18] N. Gillis and S. A. Vavasis, “Fast and robust recursive algorithms for
separable nonnegative matrix factorization,” IEEE Trans. Pattern Anal.
Mach. Intell., vol. 36, no. 4, pp. 698–714, 2014.

[19] S. Arora, R. Ge, and A. Moitra, “Learning topic models–going beyond
SVD,” in 2012 IEEE 53rd Annual Symposium on Foundations of
Computer Science, 2012, pp. 1–10.

[20] W.-K. Ma, J. M. Bioucas-Dias, T. H. Chan, N. Gillis, P. Gader, A. J.
Plaza, A. Ambikapathi, and C. Y. Chi, “A signal processing perspective
on hyperspectral unmixing: Insights from remote sensing,” IEEE Signal
Process. Mag., vol. 31, no. 1, pp. 67–81, 2014.

[21] U. Ara´ujo, B. Saldanha, R. Galv˜ao, T. Yoneyama, H. Chame, and
V. Visani, “The successive projections algorithm for variable selection in
spectroscopic multicomponent analysis,” Chemometrics and Intelligent
Laboratory Systems, vol. 57, no. 2, pp. 65–73, 2001.

[22] H. Ren and C.-I. Chang, “Automatic spectral

target recognition in
hyperspectral imagery,” IEEE Trans. Aerosp. Electron. Syst., vol. 39,
no. 4, pp. 1232–1249, 2003.

 
 
 
 
[23] T.-H. Chan, W.-K. Ma, A. Ambikapathi, and C.-Y. Chi, “A simplex vol-
ume maximization framework for hyperspectral endmember extraction,”
IEEE Trans. Geosci. Remote Sens., vol. 49, no. 11, pp. 4177–4193, 2011.
[24] J. M. Nascimento and J. M. Dias, “Vertex component analysis: A fast
algorithm to unmix hyperspectral data,” IEEE Trans. Geosci. Remote
Sens., vol. 43, no. 4, pp. 898–910, 2005.

[25] X. Fu, W.-K. Ma, K. Huang, and N. D. Sidiropoulos, “Blind separation
of quasi-stationary sources: Exploiting convex geometry in covariance
domain,” IEEE Trans. Signal Process., vol. 63, no. 9, pp. 2306–2320,
May 2015.

[26] X. Fu, W.-K. Ma, T.-H. Chan, J. M. Bioucas-Dias, and M.-D. Iordache,
“Greedy algorithms for pure pixels identiﬁcation in hyperspectral un-
mixing: A multiple-measurement vector viewpoint,” in 21st European
Signal Processing Conference, 2013, pp. 1–5.

[27] E. Esser, M. Moller, S. Osher, G. Sapiro, and J. Xin, “A convex model
for nonnegative matrix factorization and dimensionality reduction on
physical space,” IEEE Trans. Image Process., vol. 21, no. 7, pp. 3239–
3252, 2012.

[28] B. Recht, C. Re, J. Tropp, and V. Bittorf, “Factoring nonnegative
matrices with linear programs,” in Advances in Neural Information
Processing Systems, 2012, pp. 1214–1222.

[29] E. Elhamifar, G. Sapiro, and R. Vidal, “See all by looking at a few:
Sparse modeling for ﬁnding representative objects,” in IEEE Conference
on Computer Vision and Pattern Recognition, 2012, pp. 1600–1607.

[30] N. Gillis and R. Luce, “A fast gradient method for nonnegative sparse
regression with self-dictionary,” IEEE Trans. Image Process., vol. 27,
no. 1, pp. 24–37, 2018.

[31] T. Mizutani, “Ellipsoidal rounding for nonnegative matrix factorization
under noisy separability,” The Journal of Machine Learning Research,
vol. 15, no. 1, pp. 1011–1039, 2014.

[32] X. Fu, N. D. Sidiropoulos, and W.-K. Ma, “Power spectra separation via
structured matrix factorization,” IEEE Trans. Signal Process., vol. 64,
no. 17, pp. 4592–4605, 2016.

[33] R. Ammanouil, A. Ferrari, C. Richard, and D. Mary, “Blind and fully
images,” IEEE Trans. Image

constrained unmixing of hyperspectral
Process., vol. 23, no. 12, pp. 5510–5518, Dec. 2014.

[34] J. Fiscus, G. Doddington, J. Garofolo, and A. Martin, “NIST’s 1998
topic detection and tracking evaluation (TDT2),” in Proceedings of the
1999 DARPA Broadcast News Workshop, 1999, pp. 19–24.

[35] M. Ley, “The DBLP computer science bibliography: Evolution, research
issues, perspectives,” in International Symposium on String Processing
and Information Retrieval, 2002, pp. 1–10.

[36] A. Sinha, Z. Shen, Y. Song, H. Ma, D. Eide, B.-J. Hsu, and K. Wang,
“An overview of microsoft academic service (MAS) and applications,”
in International World Wide Web Conference, 2015, pp. 243–246.
[37] X. Fu, K. Huang, B. Yang, W.-K. Ma, and N. D. Sidiropoulos, “Robust
volume minimization-based matrix factorization for remote sensing and
document clustering,” IEEE Trans. Signal Process., vol. 64, no. 23, Dec
2016.

[38] D. Donoho and V. Stodden, “When does non-negative matrix factoriza-
tion give a correct decomposition into parts?” in Advances in Neural
Information Processing Systems, vol. 16, 2003, pp. 1141–1148.
[39] T.-H. Chan, W.-K. Ma, C.-Y. Chi, and Y. Wang, “A convex analysis
framework for blind separation of non-negative sources,” IEEE Trans.
Signal Process., vol. 56, no. 10, pp. 5120 –5134, oct. 2008.

[40] J. A. Tropp, “Algorithms for simultaneous sparse approximation. Part
II: Convex relaxation,” Signal Processing, vol. 86, no. 3, pp. 589–602,
2006.

[41] J. Chen and X. Huo, “Theoretical results on sparse representations of
multiple-measurement vectors,” IEEE Trans. Signal Process., vol. 54,
no. 12, pp. 4634 –4643, Dec. 2006.

[42] M. E. Winter, “N-ﬁndr: An algorithm for fast autonomous spectral end-
member determination in hyperspectral data,” in Imaging Spectrometry
V, vol. 3753.
International Society for Optics and Photonics, 1999, pp.
266–275.

[43] N. Gillis, “Robustness analysis of hottopixx, a linear programming
model for factoring nonnegative matrices,” SIAM Journal on Matrix
Analysis and Applications, vol. 34, no. 3, pp. 1189–1212, 2013.
[44] M. Frank, P. Wolfe et al., “An algorithm for quadratic programming,”
Naval Research Logistics Quarterly, vol. 3, no. 1-2, pp. 95–110, 1956.
[45] M. Jaggi, “Revisiting Frank-Wolfe: Projection-free sparse convex opti-
mization.” in International Conference on Machine Learning, 2013, pp.
427–435.

[46] R. M. Freund and P. Grigas, “New analysis and results for the
Frank–Wolfe method,” Mathematical Programming, vol. 155, no. 1-2,
pp. 199–230, 2016.

13

[47] S. Boyd, S. P. Boyd, and L. Vandenberghe, Convex optimization.

Cambridge university press, 2004.

[48] K. K. Thekumparampil, P. Jain, P. Netrapalli, and S. Oh, “Projec-
tion efﬁcient subgradient method and optimal nonsmooth Frank-Wolfe
method,” Advances in Neural Information Processing Systems, vol. 33,
pp. 12 211–12 224, 2020.

[49] Y. Nesterov, “Smooth minimization of non-smooth functions,” Mathe-

matical programming, vol. 103, no. 1, pp. 127–152, 2005.

[50] D. Cai, X. He, and J. Han, “Locally consistent concept factorization
for document clustering,” IEEE Transactions on Knowledge and Data
Engineering, vol. 23, no. 6, pp. 902–913, 2010.

[51] ——, “Document clustering using locality preserving indexing,” IEEE
Transactions on Knowledge and Data Engineering, vol. 17, no. 12, pp.
1624–1637, 2005.

[52] A. Kumar, V. Sindhwani, and P. Kambadur, “Fast conical hull algorithms
for near-separable non-negative matrix factorization,” in International
Conference on Machine Learning, 2013, pp. 231–239.

[53] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent Dirichlet allocation,”

Journal of Machine Learning Research, vol. 3, pp. 993–1022, 2003.

[54] D. M. Blei, “Probabilistic topic models,” Communications of the ACM,

vol. 55, no. 4, pp. 77–84, 2012.

k

−

X

XC

Since

2
F =
k

APPENDIX A
PROOF OF THEOREM 1
N
2
Xcℓ −
2, i.e., the cost
ℓ=1 k
function is decomposable over the columns of X, one can
P
xℓk
Xcℓ −
consider each f (cℓ) = 1/2
k
cℓf (cℓ)⊤u
is minimized over the probability simplex. This is a linear
program, and its solution is always attained at a vertex of the
probability simplex [47]. Hence, the solution is an unit vector,
i.e.,

xℓk
2
2 individually.

The FW algorithm ﬁrst ﬁnds u such that

∇

u = en⋆ , n⋆ = arg min
n∈[N ]

[

∇

f (cℓ)]n.

Next, we show that n⋆

have

always holds. To see this, we

∈ K

f (cℓ) = X⊤(Xcℓ −
∇

xℓ)

= H⊤W⊤W (Hcℓ −
1W⊤W (Hcℓ −
h⊤
...
N W⊤W (Hcℓ −
h⊤
hℓ)

= 




∈

hℓ)
hℓ)

hℓ)






Recall that qℓ = W⊤W (Hcℓ −
f (cℓ)]n = h⊤
nqℓ
K

∇

[

RK. Hence, we have

=

hi,nqi,ℓ

i=1
X

min
j∈[K]

≥

(cid:18)
= min
j∈[K]

qj,ℓ

K

qj,ℓ

hj,n

(cid:19)

j=1
X

(24)

= 0, then the lower bound can be attained if hn = ej⋆
If qℓ 6
where j⋆ = arg minj∈[K] qj,ℓ. Note that such hn always exists
because of the separability assumption. Let us denote it as hn⋆ ,
we have n⋆ = arg minn[
f (cℓ)]n. Since hn⋆ is a unit vector,
and because of the assumption that non-repeated unit vectors
appears in H, one can conclude n⋆

∇

.

If there are more than one smallest element in qℓ, say, there
are 2 smallest elements q1,ℓ = q2,ℓ = minj qj,ℓ, then, any hn⋆
in a form of hn⋆ =
1

⊤ for any 0

z, 0, . . . , 0

z, 1

z

∈ K

≤

≤

−

(cid:2)

(cid:3)

also makes the lower bound in (24) attained. Such hn⋆ might
be not a unit vector and hence n⋆ /
. Nonetheless, the
assumption that no duplicate minimal values appear in the
gradient [cf. Eq. (15)] assures that this case never happens.

∈ K

If qℓ = 0, which means that the objective function already

reaches optimal value of 0 since
W⊤W (Hcℓ −
Therefore, one should stop FW.

hℓ) = 0 =

Hcℓ = hℓ.

⇒

APPENDIX B
PROOF OF THEOREM 2

Under the noisy model, for any cℓ, we have
f (cℓ) = X⊤(Xcℓ −
∇

xℓ)
= (W H + V )⊤((W H + V )cℓ −

(W hℓ + vℓ)).

Hence,

nW⊤ + v⊤
n)(W Hcℓ + V cℓ −

n)((W H + V )cℓ −
W hℓ −

W hℓ −
vℓ)

vℓ)

[

∇
= (h⊤
= h⊤
+ h⊤

f (cℓ)]n = (h⊤
nW⊤ + v⊤
nW⊤W (Hcℓ −
nW⊤(V cℓ −

hℓ)
vℓ) + v⊤

= h⊤
|

nW⊤W (Hcℓ −

ǫn
hℓ) + ǫn.
{z

n(W Hcℓ + V cℓ −

W hℓ −

vℓ)

(25)
}

Similar to the proof of Theorem 1, the key is to show that for
1

, where

N , n⋆

ℓ

≤

≤

ℓ ∈ K
ℓ = arg min

n⋆

n

nW⊤W (Hcℓ −
h⊤

hℓ) + ǫn.

Since the proof in the sequel holds for all ℓ, we omit the
subscript and use n⋆ instead. From the proof of Theorem 1,
when ǫn = 0, we know that hn⋆ = ej where ej is a certain
unit vector. Our goal is to show that with such n⋆, for any
n

= n⋆,
nW⊤W (Hcℓ −
h⊤
Equivalently, we hope to show the following

hℓ) + ǫn > h⊤

n⋆ W⊤W (Hcℓ −

(hn −
(hn −

hn⋆ )⊤W⊤W (Hcℓ −
ej)⊤W⊤W (Hcℓ −

hℓ) > ǫn⋆

hℓ) > ǫn⋆

ǫn
−
ǫn,

−

⇐⇒

(26)

which will leads to that the FW algorithm selects n⋆ at the
current iteration with cℓ.

We will prove (26) using the following two Lemmas. Their
proofs are provided in Appendix E and Appendix F in the
supplementary material.

14

γ

Using upper bound of noise δ, we have

δ

γ2 +

≤ s

(1

γ2 +

(1

d(H))νη
−
4σmax(W ) −
d(H))νη

−
4σmax(W )

(1

d(H))νη

−
σmax(W )

.

(29)

δ2 + γ2 + 2δγ

⇔

4(δ2 + 2δγ)

⇔

≤

≤

Using (27), (28), and (29), one can see that

(hn −

ej)⊤W W (Hcℓ −

hℓ)

≥

≥
≥

(1

d(H))νη

−
σmax(W )
4(2γδ + δ2)
ǫn⋆

ǫn,

−

which is exactly (26). Until now, we have established that
the FW algorithm will not update any C(n, :) such that
c before it reaches the stopping criterion. Whenever
n
FW terminates at

C, we have

∈ K

η + 2δ

which leads to

cℓ −
X
xℓk2
b
≥ k
W (H
cℓ −
=
k
b
W (H
cℓ −
b
H
σmin(W )
k
b

≥ k
≥

hℓ) + V
hℓ)
cℓ −

cℓ −
V
k2 − k
b
hℓk2 −
b

vℓk2
cℓ −
2δ,

vℓk2

b
hℓk2 ≤

η + 4δ
σmin(W )

.

H
k

cℓ −

This completes the proof.

b

APPENDIX C
PROOF OF THEOREM 3

Without loss of generality, assume that

hℓ) + ǫn⋆.

and H(:, 1:K) = I. Therefore, we have

=

{

K

1, . . . , K

}

C ⋆ =

H
0

(cid:20)
that is the desired solution. Note that C ⋆ is an optimal solution
of Problem (4). Our goal is to show that Problem (18)’s
optimal solutions are close to C ⋆. We will show this step
by step.

(cid:21)

a) Step 1: We ﬁrst ﬁnd an upper bound of the objective
N , it can be seen

ℓ

function associated with C ⋆. For 1
that

≤

≤

Lemma 3 For any ℓ
n⋆ := arg maxn h⊤

[N ], n
∈
nW⊤W (Hcℓ −
hℓ)

[N ], n
∈
hℓ), we have

= n⋆, where

xℓ −

k

νη(1

d(H))

−
σmax(W )

≥

,

(27)

Xc⋆

ℓ k2 =
k
≤ k
=

(hn −

ej)⊤W⊤W (Hcℓ −

where ν is deﬁned in (16), and δ satisﬁes (17) in Theorem 2.

(W H + V )c⋆
vℓ −
ℓ k2 +
k

ℓ k2
V c⋆

ℓ k2

W Hc⋆

W hℓ + vℓ −
W hℓ −
V c⋆
vℓ −
ℓ k2
vℓk2 + max
ℓ k
vℓk2 ,
2 max

ℓ k

k

≤ k

vℓk2

Lemma 4 For any ℓ
n⋆ := arg maxn h⊤

[N ], n
∈
nW⊤W (Hcℓ −
ǫn ≤
ǫn⋆

−

4(2γδ + δ2).

[N ], n
∈
hℓ), we have

= n⋆, where

which leads to

≤

(28)

xℓ −
k

Xc⋆
ℓ k

2
2 ≤

4 max

ℓ k

2
2 ≤

4

ρ
N k

V

2
F .
k

vℓk

6
6
6
15

(35)

f (C ⋆)

≥

f (C), or

C(k, :)

k∞ ≥

k

ck,k ≥

Consequently, we have

Combining (33)-(34), we have

xℓ −

k

2
Xc⋆
2 =
ℓ k

xℓ −
k

Xc⋆
ℓ k

2
2

(30)

τ

1

−

≥

⇒

r

N

4ρ

K
−
N k

V

2
F + 2λK + 2δ
k
κ(W )

N

Xℓ=K
N

4

ρ
N k

V

k

2
F = 4ρ

K
−
N k

V

2
F

k

N

Xℓ=1
N

≤

Xℓ=K

In case τ = 1, the inequality above holds trivially.

c) Step 3: Since hk,k = 1 for k

= [K], we have

∈ K

N

τ = H(k, :)ck =

hk,ℓcℓ,k = hk,kck,k +

hk,ℓcℓ,k

= ck,k +

Xℓ=1
hk,ℓcℓ,k

Xℓ6=k
ck,k + (1
−

≤

ck,k) max
ℓ6=k

hk,ℓ ≤

1.

Xℓ6=k

(36)

(37)

By (37), for k

, we have

∈ K

τ

≤

ck,k + (1

ck,k) max
ℓ6=k

hk,ℓ ≤

−

ck,k + (1

−

ck,k)d(H),

(32)

Hence, it can be seen that for k

, the following holds:

∈ K

d(H)
d(H)

τ
1

−
−

N

4ρ

V

K
−
N k
κ(W )(1

1

−

≥

r

2
F + 2λK + 2δ
k

.

d(H))

−
β

Meanwhile, for 1

C(n, :)

k∞ −

k

This leads to

N

∈ K

≤
≤
µ log(N )

n

N , we have

|

{z

}

ϕµ(C(n, :))

( Lemma 2).

≤

C(n, :)

µN log(N )

k

n=1
X
1
λ

≤

f (C ⋆)

k∞ −
N

2ρ

≤

K
−
λN k

V

k

2
F + K,

N

ϕµ(C(n, :))

1
λ

λ

≤

n=1
X

where the last inequality was established in Step 1.

b
Finally, for n /

∈ K

C(i, :)
k

k∞

C(n, :)
k

k∞ ≤

C(i, :)

k∞

Xi∈Kc k
k∞ −

=

C(i, :)

k

Xi∈K

Xi∈[N ]
2ρ

V

≤

N

(cid:18)
= 2ρ

K
−
λN k
K
−
λN k
This completes the proof.

N

V

2
F + K + µN log(N )
k

K(1

β)

−

−

(cid:19)

2
F + µN log(N ) + βK.
k

APPENDIX D
PROOF OF THEOREM 4

where the ﬁrst equality holds because we have xℓ = Xc⋆
1

N .

ℓ

ℓ for

≤
By Lemma 2, we have

≤

Φ(C ⋆)

C

≤ k

k∞,1 = K.

(31)

Combining (30) and (31), we have

f (C ⋆) =

1
2

b

≤

2ρ

N

Xℓ=1
N

xℓ −
k
K
−
N k

Xc⋆
ℓ k

2

2 + λΦµ(C ⋆)

V

2
F + λK.
k

Consider any solution C such that
f (C ⋆)
2

f (C). Then, we have

2

≥

b

4ρ

N

K
b
−
N k

V

2
F + 2λK
k

X

≥ k

−

N

b
XC

b

2
F + 2λΦµ(C)
k

Xℓ=1
k

X

≥ k

XC

≥ k
=

2
F =
−
k
2
xk −
Xckk
2 ,
W hk + vk −
k
W hk −
(
k
wk −
(
k
Therefore, we have

≥

≥

W Hck)
W Hckk2 −

xℓ −

k

Xcℓk

2
2

∈ K
(W H + V )ckk
vk −
k2 − k
2δ)2,
k

2
2

,
k
V ckk2)2,
.
∈ K

∈ K
k

N

4ρ

r

K
−
N k

V

2
F + 2λK + 2δ
k

wk −

≥ k

W Hckk2 , (33)

for any k

.

∈ K

b) Step 2: Following the idea in [16, Lemma 17], deﬁne
1. Suppose

τ = H(k, :)ck. It is readily seen that 0
τ < 1, one can see that for 1

K

≤

≤

k

τ

≤

≤

=

W Hckk2

wk −
k
wk −
[wkH(k, :)ck + W (:,
k)H(
=
−
−
k
k)H(
τ )wk −
k, :)ckk2
W (:,
(1
−
−
k
k, :)ck
H(
wk −
τ )
= (1
−
τ
1
(cid:13)
(cid:13)
(cid:13)
k), H(
(cid:13)

W (:,

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k)

−

−

−

−

k, :)ck]

k2

where W (:,
k, :) refers to a sub-matrix constructed
by removing the kth column and kth row from W and H,
respectively.

−

−

H(

−

k, :)ck, it is easy to verify that

Denote θ =

1⊤θ = 1, θ

≥

1

1
τ
0. Then,

−

W Hckk2 ≥
wk −
by the deﬁnition of κ(W ) for k

k

−
.

∈ K

(1

τ )κ(W )

(34)

ℓ

⊆ K

By theorem’s assumption, supp(cinit
⊆ K
ization. Our goal is to prove that if supp(ct
ℓ)
supp(ct+1

always holds.

)

)

ℓ

holds at initial-
holds, then

⊆ K

To proceed, we will need the following lemmas:

16

that for any given ℓ, there must be at least an n
such that
cn,ℓ < cn,ℓ′ for a certain ℓ′. Therefore, by Lemma 5, we have
the following inequality:

∈ K

yn,ℓ <

exp((cn,ℓ −

cn,⋆)/µ) < exp(

−

ψ/µ).

(40)

In the meantime, for m

1

|Ln|

ym,ℓ =

c, we have

∈ K
exp(cm,ℓ/µ)
N
i=1 exp(cm,i/µ)

=

1
N

,

because C(m, :) = 0⊤. For an n
c, we have
an m

P

∈ K

that satisﬁes (40) and

∈ K

gm,ℓ −

gn,ℓ = pm,ℓ + λym,ℓ −
pn,ℓ + λ

= pm,ℓ −

(39)

pn,ℓ −
1
N −

(cid:18)

λyn,ℓ

yn,ℓ

.

(cid:19)

Using Lemma 7, we can establish an lower bound of pm,ℓ −
pn,ℓ, i.e.,
pm,ℓ −
≥

(hm −
(d′(H)2 + 2d′(H) + 5)λmax(W⊤W )/2

hn)⊤W⊤W (Hcℓ −
hℓ)

hn)⊤W⊤W (Hcℓ −

pn,ℓ = (hm −

hℓ) + (ǫm −

4(2γδ + δ2)

4(2γδ + δ2)

ǫn)

−

≥ −
where the ﬁrst equality is by (25),
inequalities are by (28) and Lemma 7, respectively.

−

the ﬁrst and second

Therefore, we have
gn,ℓ ≥
gm,ℓ −

λ(1/N

yn,ℓ)

−

(d′(H)2 + 2d′(H) + 5)λmax(W⊤W )/2
4(2γδ + δ2)
λexp(

−
(d′(H)2 + 2d′(H) + 5)λmax(W⊤W )/2
4(2γδ + δ2)

ψ/µ)

−

−

−
> λ/N

−

−

0.

≥

(41)
where the last inequality can be derived from noise bound
given in (22).

Hence, one can see that

arg min
n

pn,ℓ + λyn,ℓ ∈ K

As a result, the update rule of FW will make supp(cℓ)
for the next iteration. This completes the proof.

⊆ K

Lemma 5 Denote yn,ℓ = exp(cn,ℓ/µ)/PN
i=1 exp(cn,i/µ) where
µ > 0. If cn,ℓ is not the largest element in row C(n, :), i.e.,

cn,ℓ < max

i

cn,i,

(38)

then we have yn,ℓ < (1/|Ln|)exp((cn,ℓ−
.
maxi cn,i, and
Ln :=
cn,i = cn,⋆}

{

i

|

cn,⋆)/µ) where cn,⋆ :=

Lemma 6 Given vector x
followings: x
then

∈
0, 1⊤x = 1,

≥

RK. Suppose x satisﬁes the
x
1,
k

a for some a

k∞ ≤

≤

x
k

k2 ≤

max (2(a

−

1/2)2 + 1/2, 2(1/2

1/K)2 + 1/2)

−

p

Lemma 7 For n
(hm −

, m

c, the following holds:

∈ K
∈ K
hn)⊤W⊤W (Hcℓ −
hℓ)
(d′(H)2 + 2d′(H) + 5)λmax(W⊤W )/2.
≥ −

where

d′(H) =

max(2(d(H)

1/2)2 + 1/2, 2(1/2

1/K)2 + 1/2).

−

−

p

Proofs of Lemmas 5, 6, and 7 are relegated to the supplemen-
tary material in Appendices G, H, and I, respectively.
Let us assume C t = C. The gradient of (18) is

f (C) = X⊤(XC

X) + λ

Φµ(C).

∇
Denote P = X⊤(XC
that

b

−

−
X) and Y =

∇
Φµ(C). One can see

∇

Y (n, ℓ) = yn,ℓ =

exp(cn,ℓ/µ)
N
i=1 exp(cn,i/µ)

.

such that
Our goal is to show that there always exists n
P
cn,ℓ satisﬁes condition (38) in Lemma 5 for any ℓ. If this holds,
f (C)]n,ℓ is expected
then the corresponding gradient value [
to be small, since the corresponding yn,ℓ is small.
To this end, denote gℓ as the ℓth column of

f (C), i.e.,

∈ K

∇

b

∇

gℓ = pℓ + λyℓ,

b

where pℓ and yℓ are the ℓth columns of P and Y , respectively.
Our objective then amounts to showing that j

where

∈ K

j = arg min

pn,ℓ + λyn,ℓ.

n∈[N ]

∈ K

= [K] and hn = en
Again, w.o.l.g., we assume that
K
. We use a contradiction to show our conclusion.
for n
, cn,ℓ is the largest element in
Suppose that for every n
row C(n, :)—i.e., cn,ℓ ≥
.
∈ K
Since cn,ℓ is the largest element, and since C(n, :) is not a
constant by our assumption, one can always ﬁnd an ℓ′ such
that cn,ℓ > cn,ℓ′. Then, we have

∈ K
cn,ℓ′ for all ℓ′

= ℓ for every n

N

K

1 = 1⊤cℓ =

ci,ℓ =

ci,ℓ

= cn,ℓ +

K

i=1
X
ci,ℓ ≥

Xi6=n

i=1
X

K

K

cn,ℓ +

ci,ℓ′ >

ci,ℓ′ = 1.

Xi6=n

i=1
X

The third equality holds because of the assumption that
supp(cℓ)
. The above is a contradiction, which means

⊆ K

6
17

qj,ℓ

hℓ)

(cid:12)
(cid:12)

Supplementary Materials of “Memory-Efﬁcient Convex
Optimization for Self-Dictionary Separable Nonnegative
Matrix Factorization: A Frank-Wolfe Approach ”

Tri Nguyen, Xiao Fu, and Ruiyuan Wu

which is by (43), we get

(hn −

ej)⊤W⊤W (Hcℓ −

APPENDIX E
PROOF OF LEMMA 3

≥

By the stopping criterion, before FW terminates, η + 2δ
Xcℓ −
k
holds:

≤
xℓk2 holds. Hence, the following chain of inequalities

where the last step is by (44).

hℓ)
d(H)qj,ℓ + (1

≥
= (1

−
νη(1

−
d(H))(qs,ℓ −
d(H))
−
σmax(W )

,

d(H))qs,ℓ −
qj,ℓ)

APPENDIX F
PROOF OF LEMMA 4

For 1

ℓ

≤

=

ǫn|

|

|

≤

vℓ)

N , we have

≤
h⊤
nW⊤(V cℓ −
+ v⊤
nW⊤V cℓ
h⊤
+
v⊤
nV cℓ
+
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n(W Hcℓ + V cℓ −
nW⊤vℓ
h⊤
+
v⊤
nvℓ
+
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)
Note that we have

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

W hℓ −
vℓ)
|
v⊤
nW (Hcℓ −

nW⊤V cℓ
h⊤

≤ k

V cℓk2

W hnk2 k
max

k k

wkk2

max

i k

vik2

(cid:17)

(cid:19) (cid:16)

≤
(cid:18)
= γδ.

(cid:12)
(cid:12)

(cid:12)
(cid:12)

and

nW⊤vℓ
h⊤

≤ k

(cid:12)
(cid:12)

(cid:12)
(cid:12)

≤

In addition, it is seen that

vℓk2

W hnk2 k
max

k k

wkk2

(cid:18)

max

i k

vik2

= γδ.

(cid:17)

(cid:19) (cid:16)

η + 2δ

≤ k
=
k

≤ k
≤ k

≤ k
≤ k

(W H + V )cℓ −
W hℓ −
vℓk2
W (Hcℓ −
hℓ) + V cℓ −
vℓk2
W (Hcℓ −
hℓ)
V cℓ −
vℓk2
k2 +
k
W (Hcℓ −
hℓ)
V cℓk2 +
k2 +
k
k
W (Hcℓ −
hℓ)
V cℓk2 + δ
k2 +
k
W (Hcℓ −
hℓ)
k2 + 2δ
σmax(W )
Hcℓ −
k
hℓk2 ≥
Hcℓ −

hℓk2 + 2δ,
η
.
σmax(W )

≤
=

⇒ k

vℓk2

(42)

vjk2 = δ.

vik2 ≤
V cℓk2 ≤
The last inequality holds since
maxj k
hℓ). In addition, w.o.l.g., let
qj,ℓ, qs,ℓ be the smallest and the second smallest elements in
qℓ, respectively. By the deﬁnition of d(H), we have

Let qℓ = W⊤W (Hcℓ −

i=1 ci,ℓ k

P

k

N

K

h⊤

nqℓ =

hk,nqk,ℓ

Xk=1

= hj,nqjℓ + hs,nqs,ℓ +

hk,nqk,ℓ

Xk6=j,k6=s

hj,nqj,ℓ + hs,nqs,ℓ + qs,ℓ

hk,n

≥

≥

= hj,nqj,ℓ + hs,nqs,ℓ + qs,ℓ(1
= hj,nqj,ℓ + qs,ℓ(1
d(H)qj,ℓ + (1

hj,n)
−
d(H))qs,ℓ,

−

Xk6=j,k6=s
−

hj,n −

where the last inequality holds because

(d(H)

−
By the deﬁnition of ν in (16), we have

−

hj,n)qs,ℓ ≥

(d(H)

hj,n)qj,ℓ.

hs,n)

v⊤
nW (Hcℓ −

hℓ)

(43)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

≤ k
≤ k

W (Hcℓ −
vnk2 k
hℓ)
W Hcℓk2 +
vnk2 (
k
2γδ.

k2
W hℓk2)
k

We also have

≤

v⊤
nV cℓ

vnk2 k

V cℓk2 ≤

≤ k

δ2,

v⊤
nvℓ

vnk2 k

≤ k

vℓk2 ≤

δ2.

(cid:12)
Combining the upper bounds, we have
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

qj,ℓ

qs,ℓ −
=

k

Hcℓ −
νη
σmax(W )

hℓk2
.

≥

min
j6=i

(cid:18)

min
i

(wj −

wi)⊤W

Hcℓ −
Hcℓ −
k

hℓ
hℓk2 (cid:19)
(44)

and therefore,

ǫn| ≤

|

2(2γδ + δ2),

Consequently, we have

ǫn⋆

ǫn ≤ |

ǫn⋆

ǫn| ≤

2

ǫn| ≤

|

−

−

4(2γδ + δ2).

(45)

jW⊤W (Hcℓ −
e⊤
In addition, since for any n

= j, we have

hℓ) = e⊤

jqℓ = qj,ℓ.

APPENDIX G
PROOF OF LEMMA 5

nW⊤W (Hcℓ −
h⊤
d(H)qj,ℓ + (1
−
≥

hℓ) = h⊤

nqℓ
d(H))qs,ℓ,

Let
C(n, :):

Ln be a set of indices of the largest elements in row
.

Ln :=

arg maxi cn,i}
{

6
Let cn,⋆ be the maximum value in row C(n, :), i.e., cn,⋆ =

cn,i where i

yn,ℓ =

∈ Ln
exp(cn,ℓ/µ)
N
i=1 exp(cn,i/µ)

=

P

i∈Ln

exp(cn,⋆/µ) +

exp(cn,ℓ/µ)

i /∈Ln
1
P
cn,ℓ/µ) +

P

exp(cn,⋆/µ

1

−

exp(cn,⋆/µ
exp((cn,ℓ −

cn,ℓ/µ)

−
cn,⋆)/µ),

=

<

=

P
|Ln|

|Ln|
1

|Ln|

2

|

exp(cn,i/µ)

exp(cn,i/µ

i /∈Ln

cn,ℓ/µ)

−

That concludes

yn,ℓ <

1

|Ln|

exp((cn,ℓ −

cn,⋆)/µ).

(46)

APPENDIX H
PROOF OF LEMMA 6

Suppose that x1 = maxi xi, which is without

loss of

generality. It is easy to verify a

x1 ≥

≥

1/K. Therefore,

x
k

2
2 =
k

K

i=1
X

K

i = x2
x2

1 +

x2
i

i=2
X
K

2

x2
1 +

xi

≤
= x2

1 + (1
= 2(x1 −
max

i=2
X
−
1
)2 +
2

!
x1)2
1
2
)2 +

2(a

1
2

−

≤

(cid:18)

1
2

, 2(

1
K −

1
2

)2 +

1
2

(cid:19)

APPENDIX I
PROOF OF LEMMA 7

hn,

Let a = hm −
(hm −

b = Hcℓ −
hm)⊤W⊤W (Hcℓ −

hℓ. The LHS of (39) is

hℓ) = a⊤W⊤W b.

Note that

a⊤W⊤W b =

=

1
2
1
2

a⊤ b⊤

(cid:20)

(cid:3)

(cid:2)
z⊤Zz,

0 W⊤W

W⊤W

0

a
b

(cid:21)

(cid:21) (cid:20)

where we denote z =

W⊤W
(cid:20)
Since characteristic polynomial of Z is

(cid:21)

(cid:20)

, Z =

0

.
(cid:21)

0 W⊤W

a
b

det(Z

−

λI) = det(λ2I
= det(λI

W⊤W W⊤W )
−
W⊤W ) det(λI + W⊤W ),

−

hence λ is eigenvalue of W⊤W leads to
of Z.

−

λ, λ are eigenvalues

18

(hm −

hn)⊤W⊤W (Hcℓ −

hℓ)
|

= z⊤Zz

k

z

2
2

2
λmax(G)
2
k
≤
= λmax(W⊤W )
z
k
k
2
= λmax(W⊤W )(
Hcℓ −
hm −
hnk
2 +
k
k
hnk2)2
λmax(W⊤W )((
hmk2 +
k
k
hℓk2)2)
Hcℓk2 +
+ (
k
k
hmk2 + 1)2 + (1 + 1)2)
λmax(W⊤W )((
k
≤
= λmax(W⊤W )(
hmk2 + 5).
hmk
k
k

≤

2
hℓk
2)

2
2 + 2
Furthermore, using Lemma 6 on hm,
hmk2 ≤
k

max (2(d(H)

−

= d′(H)
p

1/2)2 + 1/2, 2(1/2

1/K)2 + 1/2)

−

Thus,

2

|

⇒

(hm −
≤
(hm −

hℓ)
|

hn)⊤W⊤W (Hcℓ −
(d′(H)2 + 2d′(H) + 5)λmax(W⊤W )
hn)⊤W⊤W (Hcℓ −
hℓ)
(d′(H)2 + 2d′(H) + 5)λmax(W⊤W )/2.
≥ −

APPENDIX J
PROOF OF PROPOSITION 1
Let C tinit(ℓ, :) = C t(ℓ, :). The FW algorithm’s element-wise

updating rule can be expressed as

ct+1
n,i =

ct+1
n,j =

1
(cid:18)
1

−

(cid:18)

−

2
t + 2
2
t + 2

(cid:19)

ct
n,i +

(cid:19)
ct
n,j +

2
t + 2
2
t + 2

I t
n,i

J t
n,j,

n,j can only be either 0 or 1.

where I t
Let St
n,j, and U t
ct
The updating rule in terms of St

n,i, J t
n,(i,j) = ct

n,i −

St+1
n,(i,j) =

t
t + 2

St

n,(i,j) +

a) Step 1: We ﬁrst show that St

the following relation with Stinit

n,(i,j):

n,i −

J t
n,j.

n,(i,j) = I t
i,j is
2
t + 2
n,(i,j) for t > tinit has

n,(i,j).

U t

t(t + 1)
2

St

n,(i,j) =

tinit(tinit + 1)
2

Stinit
n,(i,j)+

t−1

(k+1)U k

n,(i,j).

Xk=tinit

(47)
Indeed, (47) can be shown by induction. Since (47) involves
the sample tuple n, i, j on both sides, and for the sake of sim-
plicity, we omit these subscript temporarily in the following
induction proof.

To see this, let us consider the ﬁrst iteration ﬁrst. We have

Stinit+1 = tinit

tinit+2 Stinit + 2

tinit+2 U tinit. This means that

(tinit + 1)(tinit + 2)
2

Stinit+1 =

(tinit + 1)tinit
2
(tinit + 1)U tinit.

Stinit+

 
19

This further ensures that C t(n⋆, :) at the tth iteration is
not a constant row.

Combine two cases and take the minima, we have, for

tinit ≤

t

T ,

≤
min
n∈K,
i,j
n,i6=ct
ct

n,j

ct
n,i −

ct
n,j

(cid:12)
(cid:12)

= min
n∈K,
i,j
n,(i,j)6=0

St

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

St

n,(i,j)

(cid:12)
(cid:12)
(cid:12)

≥

≥

This completes the proof.

2
T (T + 1)

,

2ξ
T (T + 1)

(cid:19)

min

(cid:18)
2ξ
T (T + 1)

.

APPENDIX K
PROOF OF LEMMA 2

Let xmax := maxi xi.
N

ϕµ(x) = µ log

1
N

i=1
X

exp(xi/µ)

!

= µ log

1
N

exp(xmax/µ)

N

i=1
X

= µ

 −

log N +

xmax
µ

+ log

µ log N + xmax + µ log

=

−

x

≤ k

k∞.
Hence, it is seen that

exp((xi −

xmax)/µ)

!

N

i=1
X
N

exp((xi −

xmax)/µ)

!!

exp((xi −

xmax)/µ)

!

i=1
X

In addition,

ϕµ(x) = µ log

µ log

≥

N

1
N

1
N

(cid:18)

exp(xi/µ)

i=1
X
exp(xmax/µ)

!

(cid:19)

This completes the proof.

µ log N + xmax.

=

−

(cid:12)
(cid:12)
z

,

(49)

lim
µ→0

ϕµ(x) = xmax =

x

k∞.

k

To proceed, suppose that (47) holds for any t > tinit + 1,

i.e.,

t(t + 1)
2

St =

tinit(tinit + 1)
2

t−1

Stinit +

(k + 1)U k.

Xk=tinit

We consider the next iteration. One can see that
(t + 1)(t + 2)
2

t
t + 2

St+1 =

St +

2
t + 2

U t

(cid:19)

(t + 1)(t + 2)
2
t(t + 1)
2

tinit(tinit + 1)
2

=

=

(cid:18)
St + (t + 1)U t

=

tinit(tinit + 1)
2

t−1

Stinit +

(k + 1)U k

Xk=tinit
+ (t + 1)U t

t

Stinit +

(k + 1)U k.

Xk=tinit

b) Step 2: As a result of step 1, we have

St

n,(i,j)

=

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

2
t(t + 1)

(cid:12)
(cid:12)
(cid:12)

tinit(tinit + 1)
2

Stinit
n,(i,j)+

t−1

(k + 1)U k

n,(i,j)

.

(48)

Observe that the second term inside absolute operator of (48)
is an integer,

Xk=tinit

(cid:12)
(cid:12)
(cid:12)

St

n,(i,j)

(cid:12)
(cid:12)
(cid:12)

≥

=

(cid:12)
(cid:12)
(cid:12)

≥

2
t(t + 1)
2
t(t + 1)
2
T (T + 1)

min
z∈Z

min
z∈N

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
min
z∈N

tinit(tinit + 1)
2

Dn

i,j(C init)

−
i,j(C init)

Dn

−

Stinit
n,(i,j) + z

z

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
i,j and the equality holds
(cid:12)

where we use the deﬁnition of Dn
because both min operators results in
i,j(C init), Dn

Dn

⌈·⌉

⌉−

Dn

i,j (C init)
,

min(
⌈
where
we establish lower bound of
sibilities regarding to Dn
i,j(C init)

• If Dn

N, then

⌊·⌋

St
i,j(C init):
(cid:12)
(cid:12)
(cid:12)

n,(i,j)

(cid:12)
(cid:12)
(cid:12)

i,j(C init)

Dn

i,j(C init)
⌋

),

−⌊

denotes ceiling and ﬂoor operators, resp. Next,
by considering 2 pos-

∈




St
(cid:12)
St
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n,(i,j)

n,(i,j)

= 0 or
2
T (T + 1)

≥

,

because both terms inside the absolute operator in (49)
are integers



• If Dn

i,j(C init) /
∈

N, then by the deﬁnition of ξ,

St

n,(i,j)

2ξ
T (T + 1)

.

≥

(cid:12)
Such pair of i, j exists for some n⋆
(cid:12)
(cid:12)
j⋆, and hence

(cid:12)
(cid:12)
(cid:12)

, e.g., i = i⋆, j =

∈ K

St
n⋆,(i⋆,j⋆)| ≥

|

2ξ
T (T + 1)

> 0.

 
 
 
 
 
