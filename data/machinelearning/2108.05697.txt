1
2
0
2

g
u
A
1
1

]
S
D
.
s
c
[

1
v
7
9
6
5
0
.
8
0
1
2
:
v
i
X
r
a

Local Correlation Clustering with Asymmetric Classiﬁcation Errors∗

Jafar Jafarov†
University of Chicago

Sanchit Kalhan†
Northwestern University

Konstantin Makarychev†
Northwestern University

Yury Makarychev†
Toyota Technological Institute at Chicago

Abstract

In the Correlation Clustering problem, we are given a complete weighted graph G with its edges
labeled as “similar” and “dissimilar” by a noisy binary classiﬁer. For a clustering C of graph G, a similar
edge is in disagreement with C, if its endpoints belong to distinct clusters; and a dissimilar edge is in
disagreement with C if its endpoints belong to the same cluster. The disagreements vector, dis, is a vector
indexed by the vertices of G such that the v-th coordinate disv equals the weight of all disagreeing edges
incident on v. The goal is to produce a clustering that minimizes the ℓp norm of the disagreements vector
for p ≥ 1. We study the ℓp objective in Correlation Clustering under the following assumption: Every
similar edge has weight in the range of [αw, w] and every dissimilar edge has weight at least αw (where
α ≤ 1 and w > 0 is a scaling parameter). We give an O (cid:16)(1/α)1/2−1/2p · log 1/α(cid:17) approximation algorithm
for this problem. Furthermore, we show an almost matching convex programming integrality gap.

1 Introduction

Grouping objects based on the similarity between them is a ubiquitous and important task in machine learn-
ing. This similarity information between objects can be represented in many ways, some of them being pair-
wise distances between objects (objects which are closer are more similar) or the degree of similarity between
pairs of objects (objects which are more similar have a higher degree of similarity). Bansal, Blum, and Chawla
(2004) introduced the Correlation Clustering problem, a versatile model that elegantly captures this task of
grouping objects based on similarity information. Since its introduction, the correlation clustering problem
has found use in a variety of applications, such as co-reference resolution (see e.g., Cohen and Richman (2001,
2002)), spam detection (see e.g., Ramachandran et al. (2007), Bonchi et al. (2014)), image segmentation (see
e.g., Wirth (2010)) and multi-person tracking (see e.g., Tang et al. (2016, 2017)). In the Correlation Cluster-
ing problem, we are given a set of objects with pairwise similarity information. Our goal is to partition the
objects into clusters that agree with this information as much as possible. The pairwise similarity information
is given as a weighted graph G with edges labeled as either “positive/similar” or as “negative/dissimilar” by
a noisy binary classiﬁer. For a clustering
, if its endpoints belong
if its endpoints belong to the same cluster.
to distinct clusters; and a negative edge is in disagreement with
To ascertain the quality of the clustering produced, Bansal et al. (2004) studied the Correlation Clus-
tering problem under two complimentary objectives. Over the years, the objective that has received the
most attention is to ﬁnd a clustering that minimizes the total weight of edges in disagreement. For the
case of complete unweighted graphs, Bansal et al. (2004) gave a constant factor approximation algorithm
for this objective. Ailon, Charikar, and Newman (2008) improved the approximation ratio to 3 by present-
ing a simple-yet-elegant combinatorial algorithm. They also presented a 2.5-approximation algorithm based
on Linear Programming (LP) rounding which was later derandomized without any loss in approximation ratio
by van Zuylen, Hegde, Jain, and Williamson (2007). Finally, Chawla, Makarychev, Schramm, and Yaroslavtsev

, a positive edge is in disagreement with

C

C

C

∗The conference version of this paper appeared in the proceedings of ICML 2021.
†Equal contribution. Jafar Jafarov and Yury Makarychev were supported by NSF CCF-1718820, CCF-1955173, and NSF
TRIPODS CCF-1934843/CCF-1934813. Sanchit Kalhan and Konstantin Makarychev were supported by NSF CCF-1955351
and NSF TRIPODS CCF-1934931.

1

 
 
 
 
 
 
(2015) gave an LP rounding algorithm which improved the approximation ratio to 2.06. The standard LP was
shown to have an integrality gap of 2 by Charikar, Guruswami, and Wirth (2003) for the case of complete un-
weighted graphs. For the case of general weighted graphs, Charikar et al. (2003) and Demaine, Emanuel, Fiat, and Immorlica
(2006) gave an O(log n)-approximation algorithm.

, E+, E−)

dis(
V
|
ments at u with respect to

RV is a

P

∈

Deﬁne the disagreements vector to be a vector indexed by the vertices of G. Given a clustering

,
P
-dimensional vector where the u-th coordinate is equal to the weight of disagree-
.
wuv ·
. That is, disu(

(u, v) is in disagreement with

, E+, E−) =

1

|
P

E

{

P

(u,v)
∈
P

P}
Thus, minimizing the total weight of disagreements is equivalent to ﬁnding a clustering minimizing the ℓ1
norm of the disagreements vector. Another objective for Correlation Clustering that has received attention
recently is to minimize the weight of disagreements at the vertex that is worst oﬀ (also known as Min Max
Correlation Clustering). This is equivalent to ﬁnding a clustering that minimizes the ℓ
norm of the dis-
agreements vector. Observe that minimizing the ℓ1 norm is a global objective since the focus is on minimizing
), minimizing the ℓp
the total weight of disagreements. In contrast, for higher values of p (particularly p =
norm becomes a more local objective since the focus shifts towards minimizing the weight of disagreements
at a single vertex. Minimizing the ℓ2 norm of the disagreements vector can thus provide a balance between
these global and local perspectives – it considers the weight of disagreements at all vertices but penalizes
vertices that are worse oﬀ more heavily. The following scenario is a showcase that minimizing the ℓ2 norm
might be a more suitable objective than minimizing the ℓ1 norm. Consider a recommender system such
that input is a bipartite graph with left and right sides representing customers and services, respectively. A
positive edge implies that a customer is satisﬁed with the service; whereas a negative edge implies that they
are dissatisﬁed with or have not used the service. We may be interested in grouping customers and services
so that the total and the individual dissatisfaction of customers are minimized.

∞

∞

Deﬁnition 1.1. (Local Correlation Clustering) Given an instance of Correlation Clustering G = (V, E =
E+

1, the local objective is to ﬁnd a partitioning

that minimizes the ℓp norm.

E−) and p

∪

≥

P

k

p)

kp = (
x

We use the standard deﬁnition of the ℓp norm of a vector x:

1
p . Since its in-
troduction by Puleo and Milenkovic (2018), local objectives for Correlation Clustering have been mainly
studied under two models (see Charikar, Gupta, and Schwartz (2017), Ahmadi, Khuller, and Saha (2019),
Kalhan, Makarychev, and Zhou (2019)). We will refer to these models as (1) Correlation Clustering on
Complete Graphs, and (2) Correlation Clustering with Noisy Partial Information.
In the ﬁrst model,
the input graph G is complete and unweighted. For this model, the ﬁrst approximation algorithm was
by Puleo & Milenkovic (2018) with an approximation factor of 48 for minimizing the ℓp norm. This was later
improved to 7 by Charikar et al. (2017). Lastly, Kalhan et al. (2019) provided a 5 approximation algorithm.
In the second model, G is an arbitrary weighted graph with possibly missing edges. For minimizing the ℓ
∞
norm of the disagreements vector in this model, Charikar et al. (2017) provided a O(√n) approximation.
Kalhan et al. (2019) gave an O(n
2p n)-approximation algorithm for minimizing the ℓp norm of
the disagreements vector.

xu|

2 + 1

1
2 −

log

u |

P

1
2p

·

1

We study local objectives in a diﬀerent model – Correlation Clustering with Asymmetric Classiﬁcation
Errors – recently introduced by Jafarov, Kalhan, Makarychev, and Makarychev (2020). In this model, the
input graph G is complete and weighted. Furthermore, the ratio of the smallest edge weight to the largest
1. Thus, for some w > 0, each positive edge weight lies in the interval
positive edge weight is at least α
[αw, w] and each negative edge weight is at least αw. This model better captures the subtleties in real world
instances than the standard models. Since real world instances rarely have equal edge weights, assumptions
in the Correlation Clustering on Complete Graphs model are too strong.
In contrast, in the Correlation
Clustering with Noisy Partial Information model, we can have edge weights that are arbitrarily small or
large, an assumption which is too weak. In many real world instances, the edge weights lie in some range
[a, b] with a, b > 0. For this model, Jafarov et al. (2020) gave a (3 + 2 ln 1
α ) approximation for minimizing the
ℓ1 norm of the disagreements vector.

≤

Deﬁnition 1.2. Correlation Clustering with Asymmetric Classiﬁcation Errors is a variant of Correlation
Clustering on Complete Graphs. We assume that the weight of each positive edge lies in [αw, w] and the
weight of each negative edge lies in [αw,

(0, 1] and w > 0.

), where α

∞

∈

2

Our Contributions.
In this paper we study the task of minimizing local objectives (Deﬁnition 1.1) under
the Correlation Clustering with Asymmetric Classiﬁcation Errors model (Deﬁnition 1.2). Our main result

1
2 −

1
2p

1
is an O
α
which we now state.
(cid:16)(cid:0)

(cid:1)

·

log 1
α

approximation algorithm for minimizing the ℓp norm of the disagreements vector,

(cid:17)

Theorem 1.3. There exists a polynomial-time O
jective in the Correlation Clustering with Asymmetric Classiﬁcation Errors model.

·

log 1
α

1
α

(cid:17)

(cid:16)(cid:0)

(cid:1)

1
2 −

1
2p

-approximation algorithm for the ℓp ob-

For p = 1, our algorithm provides an O(log 1

α ) approximation, which matches the approximation guarantee
given by Jafarov et al. (2020) up to constant factors. Consider p = 2, that is, the ℓ2 norm. If we ignored
the edge weights and applied the state of the art algorithm in the Correlation Clustering on Complete
Graphs model, we would get an O( 1
α ) approximation. If we were to use the state of the art algorithm in
the Correlation Clustering with Noisy Partial Information model, we would get an ˜O
approximation.
However, by using our algorithm (Theorem 1.3), we obtain an ˜O
improvement when 1/α

approximation, which is a huge
(cid:1)

n1/4

1/α

n.

1/4

(cid:0)

≪
Corollary 1.4. There exists a polynomial-time O
in the Correlation Clustering with Asymmetric Classiﬁcation Errors model.

log 1
α

1/α

1/4

(cid:0)(cid:0)

(cid:1)

(cid:1)

·

(cid:0)(cid:0)

(cid:1)

(cid:1)

-approximation algorithm for the ℓ2 objective

Finally, we present the implication of our main result for the ℓ

norm, Kalhan et al.
(2019) presented an ˜O(√n) approximation under the Correlation Clustering with Noisy Partial Information
model. Using our algorithm for Correlation Clustering with Asymmetric Classiﬁcation Errors we obtain an
˜O(
1/α)-approximation factor, which is a signiﬁcant improvement to the approximation guarantee in this
setting.
p

norm. For the ℓ

∞

∞

Corollary 1.5. There exists a polynomial-time O(
in the Correlation Clustering with Asymmetric Classiﬁcation Errors model.

1/α

·

log 1/α)-approximation algorithm for the ℓ

p

objective

∞

We emphasize that our approximation ratio for the ℓp norm is independent of the graph size and only

depends on α.

Our algorithm relies on the natural convex programming relaxation for this problem (Section 2). We
compliment our positive result (Theorem 1.3) by showing that it is likely to be the best possible based on
the natural convex program, by providing an almost matching integrality gap.

Theorem 1.6. The natural convex programming relaxation for the ℓp objective in the Correlation Clustering
with Asymmetric Classiﬁcation Errors model has an integrality gap of Ω

1
2 −

1/α

1
2p

.

Organization of the paper.
In Section 2, we describe the convex relaxation that we will use in our
algorithm for Correlation Clustering. In Section 3, we introduce a novel technique for partitioning metric
spaces. This forms the main technical basis for our algorithm for Correlation Clustering. In Section 4, we
prove our main result, Theorem 1.3. In Section 5, we describe our metric space partitioning scheme and
give a proof overview of its correctness. In sections 6, 7, 7.3 and A, we formally prove the correctness of our
partitioning scheme, Theorem 3.1. In Appendix B, we prove our integrality gap result, Theorem 1.6.

(cid:0)(cid:0)

(cid:1)

(cid:1)

2 Convex Relaxation

Our algorithm for minimizing local objectives is based on rounding the optimal solution to a suitable convex
program (Figure 1). This convex program is similar to the relaxations used in Charikar et al. (2017) and
Kalhan et al. (2019). In this convex program, we have a variable xuv for every pair of vertices u, v
V .
The variable xuv captures the distance between u and v in the “multicut metric”. In the integral solution,
xuv = 0 if u and v are in the same partition and xuv = 1 if u and v are in diﬀerent partitions. In order
to enforce that the partitioning is consistent, we add triangle inequality constraints between all triplets of
vertices (P2). We also require that distance xuv is symmetric (P3).

∈

For every vertex u

(P1). The objective of the convex program is thus to minimize

∈

V , we use the variable yu to denote the total weight of violated edges incident on u
kp – the ℓp norm of the vector y. Notice
y

k

3

minimize
subject to

y
kp
k
yu =

wuvxuv +

Xv:(u,v)
∈

E−

xv1v3

E+

Xv:(u,v)
∈
xv1v2 + xv2v3 ≥
xuv = xvu
[0, 1]
xuv ∈

wuv(1

xuv)

−

for all u

V

∈
for all v1, v2, v3 ∈
for all u, v
for all u, v

V
V

∈
∈

(P)
(P1)

(P2)
(P3)
(P4)

V

Figure 1: Convex relaxation for Correlation Clustering with min ℓp objective for p

1 or p =

.
∞

≥

that each constraint in the convex program is linear, and the objective function
the Minkowski inequality).

k · kp : Rn

→

R is convex (by

P

What remains to be shown is that the relaxation presented in Figure 1 is valid. To this end, consider any
partition
= (P1, P2, . . . , Pk) of the set of vertices V . For every pair of vertices u, v, if u and v lie in the same
partition, we assign the corresponding variable xuv a value of 0, else we assign it a value of 1. Note that such
an assignment satisﬁes the triangle inequality (P2). Variable yu thus captures the total weight of violated
edges incident on u; every similar edge (u, v) incident on u that crosses a partition contributes wuv ·
xuv = wuv
xuv) = wuv to yu. Thus, yu is
to yu, and every dissimilar edge present within a cluster contributes wuv ·
, E+, E−). Hence, an integral convex program solution deﬁned in such a manner is feasible
equal to disu(
and has the same cost as the partitioning. It is possible, however, that the cost of the optimal fractional
solution is less than the cost of the optimal integral solution, and hence the convex program in Figure 1 is a
relaxation to our problem. We note that our relaxation is simpler than the relaxation used in Kalhan et al.
(2019). The additional variables in their convex program are not needed in our case because all edge weights
belong to the interval [αw, w].

(1

−

P

3 A New Technique for Partitioning Metric Spaces

0. We use Ball(v, l) =

We will use the following notation: Given expressions X and Y , we write X . Y if X
Y for some
constant C > 0 (that is, X = O(Y )). We deﬁne & similarly. Furthermore, let X + = 0 if X < 0 and X + = X
to denote the set of vertices at a distance of at most l from v.
if X
In this section, we describe our main technical tool – a novel probabilistic scheme for partitioning metric
spaces which may be of independent interest. This partitioning scheme forms the basis of our algorithm
(Algorithm 1) for Correlation Clustering. We begin by stating this technical result.

u : d(u, v)

≤

≤

≥

C

{

}

l

·

Theorem 3.1. For every q
ﬁnite metric space (X, d). Fix two positive numbers r and R such that β = r/R
Then, there exists a probabilistic partitioning

1 there exists a β∗q = Θ

satisfying properties (1), (2), and (3):

< 1 such that the following holds. Consider a
β∗q . Let Dβ = 2(q+1) ln 1/β.

≥

≤

(cid:0)

(cid:1)

1
q ln(q+1)

(1) diam(P )

≤

2R for every P

∈ P

P
(always);

(2) For every point u in X, the following bound holds:

Ball(u,R) (cid:18)

Xv
∈

Pr

P

(cid:8)

(u)

=

(v)

P

Dβ

−

(cid:9)

d(u, v)
R

+

. βq

(cid:19)

d(u, v)
R

,

Ball(u,2R)

Xv
∈

where

P

(u) denotes the partition of

that contains u.

P

(3) Moreover, for every u in X, we always have,

Ball(u,r)

Xv
∈

1

P

(cid:8)

(u)

=

P

(v)

. β

D2
β

·

(cid:9)

Ball(u,2R)

Xv
∈

d(u, v)
R

.

The partitioning we construct in Theorem 3.1 resembles a 2D–separating 2R-bounded stochastic de-
composition of a metric space Bartal (1996); C˘alinescu et al. (2000); Fakcharoenphol et al. (2004). Recall

4

6
6
that a 2D–separating 2R-bounded stochastic decomposition satisﬁes property (1) of Theorem 3.1 and the
2D-separating condition: for every u, v

X,

∈

Pr

(u)

=

(v)

P

D

−

P

d(u, v)

R ≤

0.

(3.1)

(cid:8)

(cid:9)

At a very high level, the goals of our partitioning and the 2D–separating 2R-bounded stochastic decom-
position are similar: decompose a metric space in clusters of diameter at most 2R so that nearby points lie in
the same cluster with high enough probability. However, the speciﬁc conditions are quite diﬀerent. Loosely
speaking, property (2) of Theorem 3.1 says that the decomposition satisﬁes (3.1) with D = Dβ on average
up to an additive error term of O(βq)
R . Crucially, property (3) provides an analogous
guarantee not only in expectation, but also in the worst case (which a 2D-separating decomposition does not
satisfy).

Ball(u,2R)

d(u,v)

P

∈

v

Property (3) plays a key role in proving our main result, Theorem 1.3. For the standard objective function
for Correlation Clustering (minimizing the ℓ1 norm of the disagreements vector), properties (1) and (2) are
suﬃcient since an upper bound on the expected weight of disagreements on a single vertex implies an upper
bound on the expected weight of the total disagreements. The situation gets trickier when we consider
minimizing arbitrary ℓp (p > 1) norms of the disagreements vector. For instance, having an upper bound on
the expected weight of disagreements on a single vertex does not necessarily translate to an upper bound on
the expected weight of disagreements on a worst vertex (ℓ
norm). We overcome this nonlinear nature of
the problem for higher values of p by using the deterministic (worst-case) guarantee given by property (3) of
Theorem 3.1.

∞

Also note that coeﬃcients Dβ and β do not depend on the size

of the metric space (in our algorithm,
they will only depend on α, which is deﬁned as the ratio of the smallest edge weight to the largest positive
edge weight). However, the optimal value of D in the 2D-separating condition is Θ(log

X

X

).

|

|

|

|

4 Correlation Clustering via Metric Partitioning

In this section, we will prove our main theorem, Theorem 1.3. Our algorithm (Algorithm 1) for minimizing
local objectives for Correlation Clustering with Asymmetric Classiﬁcation Errors begins by solving the convex
relaxation in Figure 1 to obtain a solution
) on V by setting distances
d(u, v) = xuv.

V . It then deﬁnes a metric d(
·

xuv}u,v

{

∈

·

,

We let q = 2. Let α∗ be the solution of equation 3√α∗/ ln 1/α∗ = β∗2 (note that α∗ is an absolute constant).
α∗. If α > α∗, we just redeﬁne α as α∗ (this will increase the approximation ratio only

We assume that α
by a constant factor). We set r = √α/ln 1/α and R = 1/3. Note that r/R

≤

At this point, the algorithm makes use of our key technical contribution – a new probabilistic scheme
for partitioning metric spaces (Algorithm 2) – and outputs the partitioning thus obtained. Please refer to
Algorithm 1 for a summary.

P

P
∈

has the desired approximation ratio in Theorem 1.3, we bound the weight of disagreements
To show that
at every vertex u
. To this end, we show that two useful quantities, the total weight
V with respect to
of disagreements at u and the expected weight of disagreements at u can be bounded in terms of yu, the
cost paid by the convex program for vertex u. In Theorem 4.1, we make use of the properties of
given
V . Then, in Section 4.1, we
by Theorem 3.1 to get a bound on these two quantities for each vertex u
use the bounds from Theorem 4.1 to complete the proof of Theorem 1.3: we show that if the total cost of
disagreements and the expected cost of disagreements with respect to
V , then
achieves the desired approximation ratio in Theorem 1.3. We remind the reader that given
the partitioning
, E+, E−) denotes the weight of edges incident
a partitioning
∈
on u that are in disagreement with respect to
. Moreover, yu denotes the convex programming (CP) cost
of the vertex u.

P
of the vertex set and a vertex u

are bounded for every u

V , disu(

P

P

P

P

P

∈

∈

= ln( 1

Deﬁne A1 = ln 1/α and A

∞
V . The ﬁrst quantity, disu(

to a vertex u
disagreement with
We then get a stronger bound for our second quantity of interest, E[disu(
a vertex u. In particular, we show that E[disu(

α )/√α = 1/r. Our analysis focuses on bounding two key quantities related
, E+, E−), is the total weight of edges incident on u that are in
yu.
, E+, E−)], the expected cost of

. We show that this quantity can be charged to the CP cost of u and is at most A

, E+, E−)]

yu.

∞ ·

P

P

P

∈

P

A1 ·

≤

5

β∗2 < 1.

≤

6
Algorithm 1 Correlation Clustering Algorithm

Input: G = (V, E+, E−, w, α),
Deﬁne a metric d on V such that d(u, v) = xuv for all u, v
Deﬁne r = (√α/ln 1/α), R = 1/3, q = 2.

xuv}u,v

V .

{

∈

V .

∈

= Metric Space Partitioning Scheme(V, d, r, R, q).

P
Output

.

P

Theorem 4.1. Given an instance of Correlation Clustering with Asymmetric Classiﬁcation Errors (Deﬁni-
of the vertex set such that the following holds for every vertex
tion 1.2), Algorithm 1 outputs a partitioning
u

V :

P

∈
(a) disu(

P
(b) E[disu(

, E+, E−) . A

yu;
∞ ·
, E+, E−)] . A1 ·

yu,

P
where A1 = ln(1/α) and A

= ln( 1

α )/√α.

∞

Proof. Without loss of generality we assume that the scaling parameter w is 1. Thus, for every positive
edge e+
α. Write the formula for
disu(

∈
, E+, E−) for a given vertex u

[α, 1], while for every negative edge e−

E+, we+

E−, we−

V ,

≥

∈

∈

P

∈

disu(

P

, E+, E−) =

X(u,v)
∈

E+

1

wuv ·

{P

(u)

=

(v)
}

P

+

Let E≥

r be the set of positive edges (v, w) in E+ with xvw ≥

1

wuv ·

{P

(u) =

.

(v)
}

P

E−

X(u,v)
∈
r. Observe that

disu(

P

, E+, E−) = disu(

P

, ∅, E−) + disu(

P

, E≥

r, ∅) + disu(

P

, E+

\

E≥

r, ∅).

(4.1)

Recall that β = r/R = 3√α/ln 1/α, q = 2, and Dβ = Θ (ln 1/β) = Θ (ln 1/α). From Theorem 3.1, part (a), we
is at most 2R. For any negative edge to be in disagreement,
know that the diameter of each partition P in
E− is at
both its endpoints must lie in the same partition. Thus, the length xuv for any such edge (u, v)
most 2R, and hence its CP contribution is at most (1

2R) = 1/3. Hence,

P

∈

disu(

P

, ∅, E−) =

X(u,v)
∈

E−

Then,

P
To complete the proof of Theorem 4.1, part (a) we write:

≤ |{

∈

disu(

, E≥

r, ∅)

v : (u, v)

E≥

r

}| ≤

yu
r

= A

∞

yu.

−
wuv1

(u) =

(v)

} ≤

P

3yu.

{P

disu(

P

, E+

\

E≥

r, ∅) =

Ball(u,r)

Xv
∈

1

wuv ·

P

(u)

=

(v)

P

(cid:8)
(u)

=

(cid:9)

(v)

.

P

1

P

≤

Xv
∈
The inequality above holds because the weight of each positive edge is at most 1. Next, using the bound for

Ball(u,r)

(cid:8)

(cid:9)

Ball(u,r) 1

v

∈

P

(u)

=

P

P

(cid:8)

(cid:9)

(v)

from Theorem 3.1 part (c), we get,

Ball(u,r)

Xv
∈

1

P

(cid:8)

(u)

=

P

(v)

. β

D2
β

·

d(u, v)
R

d(u, v)
R

Ball(u,2R)

Xv
∈
yu
= A
α

yu,

∞ ·

(cid:9)

.

.

Ball(u,2R)

Xv
∈
(ln2(1/α))

√α
ln(1/α) ·

√α
ln(1/α) ·

ln2(1/α)

·

6

6
6
6
6
6
where the last inequality follows from the fact that each positive edge weight is at least α. Thus, from (4.1)
it follows:

We now prove Theorem 4.1, part (b). We separately consider short and long positive edges. Let E≤

disu(

P

, E+, E−) . A

∞ ·

yu.

R be

the set of positive edges (v, w)

∈
yu ≥

Ball(u,R)

Xv
∈

E+ with xvw ≤

R. Note that

=

wuvd(u, v) =

Ball(u,R)

Xv
∈

Therefore, we have

1
3

Ball(u,R)

Xv
∈

wuv

d(u, v)
R

.

wuv min(d(u, v), 1

d(u, v))

−

(4.2)

E[disu(

P

, E≤

R, ∅)

3Dβ ·

−

yu]

≤

E

1

wuv ·

{P

(u)

=

P

(v)

} −

Ball(u,R)

h Xv
∈

Dβ

wuv

d(u, v)
R

i

=

wuv

Ball(u,R)

Xv
∈

(cid:16)

Pr

{P

(u)

=

≤

wuv

Pr

(u)

=

{P

(v)

} −

Dβ

(v)

} −

Dβ

P

P

Ball(u,R)

Xv
∈
d(u, v)
R

(cid:17)

d(u, v)
R

+

.

(cid:17)

Since all edges (u, v) in E≤

E[ disu(

P

, E≤

(cid:16)

Ball(u,R)

Xv
∈
R are positive, we have wuv ≤
R, ∅)

yu]

3Dβ ·

−

≤

Xv∈Ball(u,R)
s.t. (u,v)
∈

E+ (cid:16)

1. Consequently,

Pr

{P

(u)

=

P

(v)

} −

Dβ

d(u, v)
R

+

.

(cid:17)

We bound the right hand side using property (2) of Theorem 3.1:

Pr

{P

(u)

=

P

(v)

} −

Dβ

d(u, v)
R

Xv
∈

Ball(u,R) (cid:16)

+

. β2

d(u, v)
R

d(u, v)

(cid:17)

.

≤

≤

Ball(u,2R)

Xv
∈
α
ln2(1/α)

Ball(u,2R)

Xv
∈

Ball(u,2R)

Xv
∈
yu.

1
ln2(1/α)

2
ln2(1/α) ·

wuv ·

2 min(d(u, v), 1

d(u, v))

−

Here, we used that wuv ≥

α and d(u, v)

2(1

−

≤

d(u, v)) for v

Ball(u, 2R). Thus,

Furthermore, disu(

P

, E+

\

E≤

R, ∅)

≤

yu. Therefore, from (4.1) it follows that

∈
1
ln2(1/α)

yu.

yu . A1 ·
(cid:1)

E[disu(

P

, E≤

R, ∅)] .

ln(1/α) +

1
yu ≤
R ·
E[disu(

(cid:0)
A1 ·
, E+, E−)] . A1yu.

P

We now use Theorem 4.1 to prove Theorem 1.3.

7

6
6
6
6
6
4.1 Proof of Theorem 1.3

In this section, we show that the partitioning
ratio – thereby proving our main theorem, Theorem 1.3. To show this, we will use the fact that
the properties in Theorem 4.1.

output by Algorithm 1 achieves the desired approximation
satisﬁes

P

P

Proof of Theorem 1.3. If p =
item (a), as desired. So we assume that p <

∞

, then we get an O(A

) = O((1/α)1/2 ln 1/α) approximation by Theorem 4.1,
below. Given the guarantees from Theorem 4.1, we observe,

∞

∞

E

"
V
Xu
∈

disu(

P

, E+, E−)p

#

=

.

=

.

V
Xu
∈

V
Xu
∈

V
Xu
∈

V
Xu
∈

E[disu(

P

, E+, E−)p

−

1

disu(

P

·

, E+, E−)]

E

(A

∞ ·

yu)p
−

1

disu(

P

·

, E+, E−)

(cid:2)
(A

∞ ·

yu)p
−

1E

disu(

P

, E+, E−)

(cid:3)

yu)p
−

1

(A

∞ ·

(cid:2)
A1 ·

·

yu =

Ap

(cid:3)
yp
u,

·

V
Xu
∈

where A =
follows that

Ap

1
−
∞ ·

A1

(cid:0)

1

p . Note that the desired approximation factor is O(A). From Jensen’s inequality, it

(cid:1)

E

(cid:20)(cid:16) Xu

∈

V

disu(

P

, E+, E−)p

1
p

(cid:21)

(cid:17)

E

≤  

"
V
Xu
∈

1
p

disu(

P

, E+, E−)p

#!

1
p

.

V
Xu
∈

Ap

yp
u

·

!

= A

y

kp.

· k

This ﬁnishes the proof.

5 Overview of Metric Partitioning Scheme

In this section we describe our partitioning scheme and give a proof overview of Theorem 3.1. A pseudocode
for this partitioning scheme is given in Algorithm 2. More speciﬁcally, in Section 5.1 we reduce the problem
to choosing a random set of particular interest as stated in Theorem 5.1.
In Section 5.2 we describe an
algorithm for choosing such a random set and give a proof overview of its correctness. The pseudocode for
choosing a random set is given in Algorithm 3.

5.1

Iterative Clustering

Given a metric space (X, d), our partitioning scheme uses an iterative algorithm – Algorithm 2 to obtain
. Let Xt denote the set of not-yet clustered vertices at the start of iteration t of Algorithm 2. At step t,
P
the algorithm ﬁnds and outputs random set Pt ⊆
Xt. It then updates the set of not-yet clustered vertices
Pt), and repeats this step until all vertices are clustered. Algorithm 2 makes use of the following
(Xt+1 = Xt \
theorem in each iteration to ﬁnd the random set Pt.

We need the following notation to state the theorem. Let δP (u, v) be the cut metric induced by the set
P and
P or both u and v are in P . We denote

P ; δP (u, v) = 0 if u

P and v /
∈

P or u /
∈

P or u /
∈

P and v

P and v

P or v

∈

∈

∈

∈
∨P (u, v) be the indicator of the event u
.

∈

∈

P : δP (u, v) = 1 if u
P . Also, let
v /
∈
1, 2, . . . k
[k] =

{

}

Theorem 5.1. For every q
ﬁnite metric space (X, d). Fix two positive numbers r and R such that β = r/R
Then, there exists an algorithm for ﬁnding a random set P satisfying properties (a), (b), and (c):

< 1 such that the following holds. Consider a
β∗q . Let Dβ = 2(q+1) ln 1/β.

1 there exists a β∗q = Θ

≤

≥

(cid:1)

(cid:0)

1
q ln(q+1)

(a) diam(P )

≤

2R (always);

8

 
R1

t

z

R0 = R/Dβ

R

R > r > 0, q

≥

Figure 2: Balls with Diﬀerent Radii

1, β = r/R, Dβ = 2(q + 1) ln 1/β, R0 = R/Dβ, R1 = R

R0.

−

(b) For every point u in X, the following bound holds:

Pr

δP (u, v) = 1

Ball(u,R)(cid:16)

Xv
∈

(cid:8)

(cid:9)

Dβ

−

d(u, v)
R

Pr

{∨P (u, v) = 1

}

+

. βq

(cid:17)

E

·

"

Ball(u,2R)

Xv
∈

d(u, v)

R · ∨P (u, v)
#

.

(c) Moreover, for every u in X, we always have

δP (u, v) . β

D2
β ·

·

d(u, v)

R · ∨P (u, v).

Ball(u,r)

Xv
∈

Xv
∈
Informally, Theorem 5.1 is a “single-cluster” version of Theorem 3.1, and there is a one-to-one correspon-
if we assume that each

dence between their properties. In Section 6, we show that Theorem 3.1 holds for
partition P

satisﬁes Theorem 5.1. Thus, to obtain Theorem 3.1, it remains to prove Theorem 5.1.

Ball(u,2R)

P

∈ P

5.2 Selecting a Single Cluster

We will use the following deﬁnitions. Let r and R be positive numbers with r < R. Deﬁne β = r/R
Dβ = 2(q + 1) ln 1/β where q
1. Let R0 = R/Dβ and R1 = R
≥
We choose β∗q so that r < R0 < R (see Section 7 for details).

β∗q and
R0. We let ρq(β) = (1/β)q+1 (see Figure 2).

≤

−

Given a metric space (X, d) and parameters r and R, our procedure for ﬁnding a random set P

X
begins by ﬁnding a pivot point z with a densely populated neighborhood – namely, z is chosen such that a
ball of radius R0 around z contains the maximum number of points. More formally,

⊆

z = arg max
X |

u

∈

Ball(u, R0)
|

.

(5.1)

We refer to this ball of small radius around z as the “core” of the cluster. Our choice of the pivot z is
inspired by the papers by Charikar et al. (2003); Puleo & Milenkovic (2018); Charikar et al. (2017). We then
consider a ball of large radius R1 around the pivot z and examine the following two cases – “Heavy Ball” and
“Light Ball”. If this ball of large radius around z is suﬃciently populated, that is, if the number of points
in Ball(z, R1) is at least (1/β)q+1 times the number of points in the core, we call this case “Heavy Ball”. In
the case of Heavy Ball, we will show that P = Ball(z, R1) (a ball around z of radius slightly less than R)
satisﬁes the properties of Theorem 5.1. In the case of “Light Ball”, the ball of large radius around z is not
suﬃciently populated. In this case, the algorithm ﬁnds a radius t (t
R) such that P = Ball(z, t) satisﬁes
the properties of Theorem 5.1. In the following subsections we provide an overview of the proof for these two
cases. A formal proof of Theorem 5.1 can be found in Section 7.

≤

5.2.1 Heavy Ball

The Heavy Ball P is a ball of radius R1 around z which contains many points. As the diameter of P is
2R1 < 2R, it is easy to see that a Heavy Ball satisﬁes property (a) of Theorem 5.1. We now focus on showing

9

Algorithm 2 Metric Space Partitioning Scheme
Input: Metric Space (X, d) and r, R > 0, q
Deﬁne t = 0, X1 = X.
repeat

≥

1.

t = t + 1.
Pt = Cluster Select(Xt, d, r, R, q).
Xt+1 = Xt \
until Xt = ∅
Output (P1, P2, . . . , Pt).

Pt.

Algorithm 3 Cluster Select

Input: Metric space (X, d) and r, R > 0, q
Deﬁne: β = r/R, Dβ = 2(q + 1) ln 1/β .
Deﬁne: R0 = R/Dβ, R1 = R
Select z = arg maxu
if
Ball(z, R1)

−
.
Ball(u, R0)
|
Ball(z, R0)
|

X |
∈
ρq(β)
|
Set P = Ball(z, R1).

| ≥

then

· |

1

≥

R0, ρq(β) = (1/β)q+1.

else

S

as stated in Deﬁnition 5.3.

Consider S as stated in Deﬁnition 5.2.
Consider πinv
Let F be the cumulative distribution function stated in Deﬁnition 5.4.
Choose a random x
[0, R/2] according to F .
Set P = Ball(z, πinv

∈
S (x)).

end if
Output P .

z

}

∈

X

\ {

that properties (b) and (c) hold for Heavy Ball. Observe as z was chosen according to (5.1), for every point
u
, u has a less populated neighborhood of radius R0 than that of z. This combined with the
fact that Ball(z, R1) is heavy, implies that for every u, there are suﬃciently many points in P at a distance
of at least R0 from u. Thus, for any point u
X, we can expect the sum of distances between u and the
points in P to be large. In fact, we show that the left hand sides of properties (b) and (c) can be charged
R,
to
P
X, we can
∈
R ∨P (u, v). This allows

v
v
∈
∈
charge the left hand sides of properties (b) and (c) to the quantity
us to conclude that a Heavy Ball satisﬁes Theorem 5.1.

R , the sum of distances between u and the points in P . For points u such that d(z, u)

R . Thus, for every u
d(u,v)

v
∈
Ball(u, 2R) and hence,

R ∨P (u, v)

Ball(u,2R)

Ball(u,2R)

d(u,v)

d(u,v)

d(u,v)

P
⊆

P

P

≤

≥

∈

∈

P

P

v

P

5.2.2 Light Ball

In this subsection, we consider the case of
the case of Light Ball, we choose a random radius t
(a) of Theorem 5.1 holds trivially since the radius t < R.

∈

|

, which we call Light Ball. In
Ball(z, R1)
|
(0, R1] and set P = Ball(z, t). Observe that property

Ball(z, R0)
|

< ρq(β)

· |

Now consider property (c) of Theorem 5.1. Recall that for every point u

∈

X, property (c) gives a bound on
δP (u, v).

the total number of points separated from u (by P ) residing in a small ball Ball(u, r), i.e.,

Ball(u,r)
P
(0, R1]
Note that property (c) gives a deterministic guarantee on P . Therefore, we choose a random radius t
from the set of all radii for which property (c) of Theorem 5.1 holds. More speciﬁcally, we deﬁne the following
set.

∈

∈

v

Deﬁnition 5.2. Let S be the set of all radii s in (3R0, R1] such that for every u
satisﬁes:

∈

X set P = Ball(z, s)

δP (u, v)

25β

D2
β ·

·

≤

Ball(u,r)

Xv
∈

Ball(u,2R)

Xv
∈

d(u, v)

R · ∨P (u, v).

(5.2)

10

|

X

upper bounded by the size of the metric space,

|
Now we show why we can expect the set S to be large. Consider P = Ball(z, s) such that s

The set S can be computed in polynomial time since the number of distinct clusters P = Ball(z, t) is
. By the same token, S is a ﬁnite union of disjoint intervals.
S. As S is
computed according to Deﬁnition 5.2, it implies that the boundary of P is somewhat sparsely populated – as
X, it bounds the number of points within a small neighborhood of Ball(u, r) that are separated
for every u
from u (note that
Ball(u,r) δP (u, v) is trivially 0 for points u that are not close to the boundary of P ).
Since Ball(z, R1) does not contain many points, the number of points in Ball(z, s′) cannot grow too quickly
as we increase the radius s′ from 0 to R1. This suggests that for many of such radii s′, the ball P = Ball(z, s′)
has a sparsely populated boundary, and hence the set S should be large. In fact, we use the above argument
to show that the Lebesgue measure of the set S satisﬁes µ(S)
R/2. This will allow us to deﬁne a continuous
probability distribution on S.

P

≥

∈

∈

∈

v

What remains to be shown is that for a random radius t

S, the set P = Ball(z, t) satisﬁes property (b)
of Theorem 5.1. For this purpose we deﬁne a measure preserving transformation πS that maps an arbitrary
measurable set S to the interval [0, µ(S)].

∈

Deﬁnition 5.3. Consider a measurable set S
πS(x) = µ([0, x]

S). Also, for y

[0, µ(S)], let

⊂

∩

∈

[0, R]. Deﬁne function πS : [0, R]

[0, µ(S)] as follows

→

πinv
S (y) = min

x : πS(x) = y

.

}

{

Recall that the set S stated in Deﬁnition 5.2 is a ﬁnite union of disjoint intervals. In this case, what
πS does is simply pushing the intervals in S towards 0, and thus, allowing us to treat the set S as a single
interval [0, µ(S)]. For the rest of the proof overview, we assume that S = [0, µ(S)] and πS is the identity.
This simpliﬁes the further analysis of Theorem 5.1 immensely.

Next, we deﬁne a cumulative distribution function F on [0, R/2]

[0, µ(S)]:

⊆

Deﬁnition 5.4. Let F :

[0, R/2]

→

[0, 1] be a cumulative distribution function such that

F (x) =

.

(5.3)

x/R0

R/2R0

e−
e−

1
1

−
−

∈

We choose a random x

[0, R/2] according to F and set P = Ball(z, πinv

S (x)) (see Algorithm 3). Since we
assume in this proof overview that πS is the identity, P = Ball(z, x). Now, we show that the radius x chosen
in such a manner guarantees that the cluster P satisﬁes property (b). Loosely speaking, the motivation
behind our particular choice of cumulative distribution function F is the following: For two points u, v
X,
function F bounds the probability of u and v being separated by P , in terms of Dβ times the probability
that either u or v lies in P . Unfortunately, this bound does not hold for points u with d(z, u) close to R/2.
However, the choice of parameters for function F in Deﬁnition 5.4 gives us two desired properties. Without
d(z, v). Then, the probability that P separates the points u and
loss of generality assume that d(z, u)
v, Pr(δP (u, v)) = Pr(d(z, u)
d(z, v), the
F (d(z, u)). Thus, choosing F according to
probability that either u or v lies in P , Pr(
Deﬁnition 5.4 ensures:

F (d(z, u)). Moreover, as d(z, u)

−
∨P (u, v)) = 1

d(z, v)) = F (d(z, v))

≤

≤

≤

−

≤

∈

x

• (Property I) F (d(z, v))
−
for a formal argument).

F (d(z, u)) is bounded in terms of Dβ times 1

F (d(z, u)) (Please see Claim 7.10

−

• (Property II) The probability that the cluster P includes points u such that d(z, u) > R/2

small (please see Claim 7.9).

R0, is very

−

In fact, (Property II) of function F is the reason why we are able to guarantee that property (b) satisﬁes (3.1)
only on average, with the error term coming from our inability to guarantee (3.1) for points on the boundary.
We refer the reader to Section 7.5 for a formal proof. Thus, we conclude the case of Light Ball and show that
it satisﬁes Theorem 5.1.

6 Proof of Theorem 3.1

In this section, we present the proof of our main technical result – Theorem 3.1 – an algorithm for partitioning
a given metric space (X, d) into a number of clusters

= (P1, . . . , Pk) (where k is not ﬁxed).

P

11

Recall our iterative process for obtaining this partitioning – Algorithm 2 – which makes use of Theorem 5.1

in each iteration to select a cluster from the set of not-yet clustered vertices.

The proof of Theorem 5.1 is presented in Section 7. We now present the proof of Theorem 3.1 assuming

Theorem 5.1.

Proof of Theorem 3.1. Property (a) of Theorem 5.1 guarantees that diam(Pi)
thus property (1) of Theorem 3.1 holds.

≤

2R for every i

[k] and

∈

We now show that property (2) holds. Fix u

[k]. Note that set Pi satisﬁes
property (b) of Theorem 5.1 regardless of what set Xi we have in the beginning of iteration i. That is, for
every set Y

X. Consider iteration i

Y , we have

X and u

∈

∈

⊂

∈

Pr

δPi(u, v) = 1

Ball(u,R)

Xv
∈

∩

Y (cid:18)

(cid:8)

Xi = Y

|

Dβ

−

d(u, v)
R

Pr

{∨Pi(u, v) = 1

|

Xi = Y

+

}

(cid:19)

(cid:9)

. βq

E

·

"

d(u, v)

R · ∨Pi (u, v)

Xi = Y

|

.

#

(6.1)

Y

∩

Xv
∈
We observe that inequality (6.1) can be written as follows (for all u

Ball(u,2R)

X).

∈

Pr

δPi(u, v) = 1 and u, v

Ball(u,R) (cid:18)

Xv
∈

(cid:8)

Dβ

−

d(u, v)
R

Xi |

∈

Xi = Y

(cid:9)

Pr

{∨Pi (u, v) = 1 and u, v
d(u, v)

. βq

E

·

Xi |

∈

Xi = Y

+

}

(cid:19)

If u /
∈

(cid:20) Xv
Y , then all terms in (6.2) are equal to 0, and the inequality trivially holds.

Y , then
corresponding terms in (6.1) and (6.2) with v
Y
Evi happens if both points u and v are not
are equal to 0. Denote the event that u, v
clustered at the beginning of iteration i). We take the expectation of (6.2) over Xi = Y and add up the
inequalities over all i

Evi (that is,
[k]. Using the subaddivity of function x

Y are equal to each other; all terms in (6.2) with v /
∈

x+, we obtain

∈
Xi by

Ball(u,2R)

If u

∈

∈

∈

R · ∨Pi (u, v)

·

1

u, v

{

Xi} |

∈

Xi = Y

.

(6.2)

(cid:21)

∈

7→

Ball(u,R)  

Xv
∈

[k]
Xi
∈

Pr

δPi(u, v) = 1 and

(cid:8)

Evi

−

(cid:9)

Dβ

d(u, v)
R

Pr

{∨Pi (u, v) = 1 and

+

Evi}!

+

Pr

{∨Pi(u, v) = 1 and

Evi}!

(6.3)

≤

Pr

δPi (u, v) = 1 and

Ball(u,R)
Xv
∈
[k]
i
∈

(cid:8)

Dβ

d(u, v)
R

Evi

−

(cid:9)

. βq

E

·

"

Ball(u,2R)
Xv
∈
[k]
i
∈

d(u, v)

R · ∨Pi (u, v)

1

.
{Evi}#

·

Now consider any v
some iteration i. That is, for some i

\ {

X

∈

u

}

[k]:

∈

. If u and v are separated by the partitioning

, then they are separated at

P

•

Evi happens (in other words, u and v are not clustered at the beginning of iteration i)

• δPi (u, v) = 1 (exactly one of them gets clustered in iteration i)

Further, there is exactly one i such that both events above happen. On the other hand, if u and v are not
separated by

[k]. We conclude that

then δPi(u, v) = 0 for all i

P

∈

(u)

=

1

{P

P

(v)

=

1

δPi(u, v) = 1 and

(cid:9)

[k]
Xi
∈

(cid:8)

12

Evi

.

(cid:9)

(6.4)

 
6
In particular, the expectations of the expressions on both sides of (6.4) are equal:

Pr

{P

(u)

=

(v)
}

P

=

Pr

δPi (u, v) = 1 and

[k]
Xi
∈

(cid:8)

Evi

.

(cid:9)

(6.5)

Evi happens and (ii)

Now consider the ﬁrst iteration i at which at least one of the vertices u and v gets clustered. Note that (i)
∨Pi (u, v) = 1 (that is, (i) both points u and v are not clustered at the beginning
event
∨Pi (u, v) = 0
of iteration i; (ii) but at least one of them gets clustered in iteration i). Further, for j < i,
Evi” happens exactly for one
and for j > i,
value of i

Evj does not happen. We conclude that event “

∨Pi(u, v) = 1 and

[k]. Therefore,

∈

and

∨Pi (u, v)

·

1

{Evi}

= 1

k
Xi
∈

(6.6)

(6.7)

Pr

{∨Pi(u, v) = 1 and

Evi}

=

E[

∨Pi (u, v)1

{Evi}

] = 1.

[k]
Xi
∈

[k]
Xi
∈

Plugging (6.5) and (6.7) into (6.3), we obtain

Pr

(u) =

(v)

P

Dβ

−

P

Ball(u,R)  

Xv
∈

+

d(u, v)

R !

. βq

E

·

d(u, v)
R

.
(cid:21)

(cid:20) Xv

∈

Ball(u,2R)

(cid:9)
We conclude that property (2) holds. Next, we show that property (3) holds for every u
X. As in the
analysis of property (2), we consider some iteration i. Then property (c) of Theorem 5.1 guarantees that if
u

∈

(cid:8)

Xi then

∈

Ball(u,r)
[k] Xv
∈
We rewrite (6.8) as follows:

Xi
∈

Xi

∩

δPi(u, v) .

β

·

D2

β ·  

[k]
Xi
∈

Ball(u,2R)

Xv
∈

∩

Xi

d(u, v)

R · ∨Pi (u, v)
!

(6.8)

δPi (u, v)

1

·

{Evi}

Xi
∈

[k] Xv
∈

Ball(u,r)

D2

β ·  

·

Ball(u,2R)

Xv
∈

d(u, v)

R · ∨Pi (u, v)

1

.
{Evi}!

·

.

β

[k]
Xi
∈
X: if u

Note that this inequality holds for all u
Xi, it is equivalent to (6.8); if u /
∈
are equal to 0, and the inequality trivially holds. Using formulas (6.4) and (6.6), we get

∈

∈

Xi, then both sides

Ball(u,r)

Xv
∈

1

P

(cid:8)

(u)

=

P

(v)

. β

D2
β ·

·

(cid:9)

Ball(u,2R)

Xv
∈

d(u, v)
R

.

Therefore, property (3) holds.

7 Proof of Theorem 5.1

In Section 5.1, we describe an iterative approach to ﬁnding a probabilistic metric decomposition for Theo-
rem 3.1. In this section, we show how to ﬁnd one cluster P of the partitioning. Given a metric space (X, d)
X that satisﬁes the three properties listed
and positive numbers r and R, our algorithm selects a subset P
R0 and ρq(β) = (1/β)q+1
in Theorem 5.1. Recall that β = r/R, Dβ = 2(q + 1) ln 1/β, R0 = R/Dβ, R1 = R
β∗q for some small
(see Figure 3).
β∗q = Θ
and, consequently, R0 = R/Dβ is also small. Speciﬁcally, we assume that r < R0 < R1 < R
and R0 + r < R1/100.

In this proof, we assume that β = r/R is suﬃciently small (i.e, β

1
(q ln(q+1)

−

⊆

≤

(cid:0)

(cid:1)

13

6
6
R1

t

z

γ

light shell
of width r

−

R0 = R/Dβ

R

R > r > 0, q

≥

1, β = r/R, Dβ = 2(q + 1) ln 1/β, R0 = R/Dβ, R1 = R

R0.

−

Figure 3: Light Ball

Our algorithm for selecting the cluster P starts by picking a pivot point z that has the most points within

a ball of small radius R0. That is, z is the optimizer to the following expression:

z = arg max
X |

u

∈

Ball(u, R0)
|

.

(7.1)

The algorithm then checks if the ball of a larger radius, R1, around z has signiﬁcantly more points in it in
comparison to the ball of radius R0 around z. If the ratio of the number of points in these two balls exceeds
ρq(β), the algorithm selects the set of points Ball(z, R1) as our cluster P . We refer to this case as the “Heavy
Ball” case. In Section 7.2, we show that this set P satisﬁes the properties of Theorem 5.1.
Ball(z, R0)
|

, then the algorithm outputs cluster P = Ball(z, t) where
t
(3R0, R1] for which the
set P = Ball(z, s) satisﬁes Deﬁnition 5.2. Then, it chooses a random radius t in S (non-uniformly) so that
random set P = Ball(z, t) satisﬁes property (b) of Theorem 5.1. In Section 7.4, we discuss how to ﬁnd the
set S and show that µ(S)
R/2 (where µ(S) is the Lebesgue measure of set S). Finally, in Section 7.5, we
describe a procedure for choosing a random radius t in S.

If, however,
(0, R] is chosen as follows. First, the algorithm ﬁnds the set S of all radii s

Ball(z, R1)
|

< ρq(β)

· |

≥

∈

∈

|

7.1 Useful Observations

In this section, we prove several lemmas which we will use for analyzing both the “Heavy Ball” and “Light
Ball” cases. First, we show an inequality that will help us lower bound the right hand sides in inequalities
(b) and (c) of Theorem 5.1.

Lemma 7.1. Assume that z is chosen according to (7.1). Consider t in (3R0, R1] and u in X with d(z, u)
[2R0, R]. Let P = Ball(z, t). Denote:

∈

YP =

Ball(u,2R)

Xv
∈

d(u, v)

∨P (u, v)
R

.

Then,

P

|

| ≤

2DβYP .

Remark: Note that in Theorem 5.1, the right side of inequality (b) equals βqE[YP ], and the right side of
inequality (c) equals β

D2

Yp.

Proof. Observe that P

⊂

·

β ·
Ball(u, 2R). Hence,

For every v

P

∈

\

YP =

Xv
∈
Ball(u, R0), we have d(u,v)

Ball(u,2R)

d(u, v)

∨P (u, v)
R

≥

d(u, v)
R

.

P
Xv
∈

R0
R = D−

β . Thus,

1

R ≥
YP ≥

D−
β

1

P

|

Ball(u, R0)
|

.

\

14

We need to lower bound the size of P

Ball(u, R0). On the one hand, we have

\

P

|

\

Ball(u, R0)

| ≥ |

| − |

P

P

Ball(u, R0)
|
.
Ball(z, R0)
|

≥ |
Here, we used that Ball(z, R0) is the largest ball of radius R0 in X. On the other hand, Ball(z, R0)
P
P

P
\
\
P
\
\
We now provide a lemma that will help us verify property (b) of Theorem 5.1 for that point u.

Ball(u, R0), since d(z, u)
Ball(u, R0)
|

|
≥
, we get the desired inequality

Ball(u, R0)
Ball(u, R0)

Ball(z, R0)
|
P

⊂
. Combining two bounds on

2R0. Thus,

| ≥ |
| ≥ |

| − |

/2.

|

|

|

Lemma 7.2. Consider an arbitrary probability distribution of t in (3R0, R1]. Let P = Ball(z, t), where z is
chosen according to (7.1). If for each point u
Ball(z, R) at least one of the following two conditions holds,
then P satisﬁes property (b) of Theorem 5.1 for all points u in X.
Condition I:

∈

t

Pr
{

≥

d(z, u)

R0}

−

. βq+1

·

Condition II: For every v

Ball(u, R0), we have

∈

E
Ball(z, t)
|
|
Ball(z, R0)
|

|

.

Pr

δP (u, v) = 1

Dβ

−

d(u, v)
R

Pr

{∨P (u, v) = 1

}

. βq+1.

(7.2)

(7.3)

(cid:8)

(cid:9)

Remark: This lemma makes the argument about the distribution of t from the proof overview section
(Section 5) more precise. As we discuss in subsection Light Ball 5.2.2, we have chosen the distribution of t
(Cumulative distribution function F , Deﬁnition 5.4) to satisfy two properties: (Property I) the probability
that u and v are separated by P is upper bounded by the probability that u or v is in P times O(Dβ ); and
(Property II) The probability that t is close to πinv
S (R/2) is small. Thus, Condition I of Lemma 7.2 holds for
u with d(z, u) that are suﬃciently close to πinv
S (R/2), and Condition II holds for u with for smaller values of
d(z, u).

Proof. Consider one term from the left hand side of property (b) of Theorem 5.1 for some u in X:

δP (u, v) = 1

(cid:16)
}

{

Note that
{∨P (u, v) = 1
}
δP (u, v) = 1
Pr
}
{

d(u, v)

+

Pr

δP (u, v) = 1

{∨P (u, v) = 1
denotes the event that exactly one of the points u and v lies in P ; whereas

R ·

Dβ

Pr

−

(cid:17)

(cid:8)

(cid:9)

}

.

denotes the event that at least one of u and v lies in P . Thus, Pr
. Hence, this expression is positive only if Dβ ·
d(u, v) < R/Dβ = R0.

d(u, v)/R < 1, which is equivalent to

{∨P (u, v) = 1

} ≥

Thus, in the left hand side of property (b), we can consider only v in Ball(u, R0) (rather than Ball(u, R)).
R0 = R1 and, consequently,
Moreover, if d(z, u) > R, then for all v
δP (u, v) = 0. Therefore, for such u, the left hand side of property (b) equals 0, and the inequality (b) holds
Ball(z, R)). Similarly, since
trivially. We will thus assume that d(z, u)
t > 3R0, we will assume that d(z, u)
Ball(u, R0) is in P , and thus
δP (u, v) = 0).

R (which is equivalent to u
P and every v

Ball(u, R0), we have d(z, v) > R

2R0 (otherwise, u

∈
∈

−

≤

≥

∈

∈

We now show that if Condition I or II of Lemma 7.2 holds for u

for that u.

Ball(z, R) then property (b) is satisﬁed

∈

I. Suppose, the ﬁrst condition holds for u
P . In the former case, t
P, u
6∈
d(z, u)

v
t
−
hand side of (b) as follows

R0. Using that

Ball(u, R0)

∈
≥

≥

|

∈

Ball(z, R). If δP (u, v) = 1 then either u

P or
R0. In either case,
d(z, u)
by our choice of z (see (7.1)), we bound the left

d(z, v)

P, v

≥

≥

−

6∈

∈

d(z, u); in the latter case, t
Ball(z, R0)
|

| ≤ |

Ball(u,R0)

Xv
∈

δP (u, v) = 1

Pr
{

}

t

Pr
{
t

Pr

{

d(z, u)

d(z, u)

R0}
R0}

−

−

≥

≥

Ball(u, R0)
|
Ball(z, R0)
|

≤ |

≤ |

15

We now use the inequality from Condition I to get the bound

Ball(u,R0)

Xv
∈

δP (u, v) = 1

Pr
{

}

. βq+1E

Ball(z, t)
|

|

= βq+1E

P

|

.

|

Finally, by Lemma 7.1, we have the following bound on βq+1E

βq+1E

P

|

| ≤

βq+1

·

2DβE[YP ]

:

|
βqE[YP ].

P

|

≤

(7.4)

Here, we used that 2βDβ = 2r/R0 < 1 by our choice of β∗q . The right hand side of the inequality in property
(b) equals βqE[YP ]. Thus, property (b) holds.
II. Suppose now that the second condition holds for u
(b) is upper bounded by O(βq+1). Hence, the entire sum is upper bounded by O(βq+1
in turn is upper bounded by O(βq+1
Ball(z, R0)
|

Ball(z, R). Then, each term in the left hand side of
), which

Ball(u, R0)
|

). Then,

∈

|

|
Ball(z, R0)

βq+1

|

βq+1
|
βqE[YP ].

Ball(z, t)
|

= βq+1E

P

|

|

| ≤
≤

The last inequality follows from (7.4). We conclude that property (b) of Theorem 5.1 holds.

7.2 Heavy Ball Case

In this subsection, we analyze the case when
Ball(z, R1)
then the algorithm outputs P = Ball(z, R1). We will show that Theorem 5.1 holds for such a cluster P .

Ball(z, R0)
|

. If this condition is met,

ρq(β)

| ≥

· |

|

We ﬁrst prove properties (a) and (b). Since the radius of P is R1 ≤
1 and E

R, its diameter is at most 2R.
So property (a) of Theorem 5.1 holds. To show property (b), we apply Lemma 7.2 (item I) with t = R1.
Trivially, Pr
. Thus, (7.2) is
satisﬁed and property (b) also holds.

Ball(z, R0)
|

Ball(z, t)
|

Ball(z, R1)

R0} ≤

ρq(β)
|

d(z, u)

| ≥

−

≥

=

{

t

|

|

We now show property (c) of Theorem 5.1. Observe that if d(z, u) /
∈

r, R1 + r], then δP (u, v) = 0
Ball(u, r) (because P = Ball(z, R1)). Hence, for such u, property (c) holds. Thus, we assume that
r, R1 + r]

[R1 −

for all v
u

∈
[2R0, R].
[R1 −
We bound the left hand side of (c) as follows:

⊆

∈

Ball(u,r)

Xv
∈

δ(u, v)

≤ |

Ball(u, r)

| ≤ |

Ball(u, R0)
|

Ball(z, R0)
|

,

≤ |

R0 and then that z satisﬁes (7.1). Since we are in the Heavy Ball Case, we have

≤
. Therefore,

here we ﬁrst use that r
Ball(z, R0)
P
|

ρq(β)
|

| ≥

|

δ(u, v)

P

|

≤ |

/ρq(β).

Ball(u,r)

Xv
∈

By Lemma 7.1, the right hand side is upper bounded by

2DβYP /ρq(β) = 2Dβ βq+1 YP . βD2

β YP .

The right hand side of (c) equals βD2

β YP . Hence, property (c) is satisﬁed.
Thus we have shown that Theorem 5.1 holds for the case of Heavy Balls. To complete the proof, we show

that Theorem 5.1 also holds for the case of Light Balls – we give this proof in Section 7.3.

7.3 Light Ball Case

We now consider the case when
s

. Recall that S is the set of all radii
· |
(3R0, R1] for which property (c) of Theorem 5.1 holds (Deﬁnition 5.2). The set S can be found in

Ball(z, R0)
|

Ball(z, R1)

ρq(β)

| ≤

|

∈

16

polynomial time since the number of distinct balls Ball(z, s) is upper bounded by the number of points in
the metric space. We now recall map πS used in Algorithm 3.
Map πS. In Section 5.2, we deﬁne a measure preserving transformation πS that maps a given measurable
set S
[0, R] to the interval [0, µ(S)] (Deﬁnition 5.3). We need this transformation in Algorithm 3. If S is
the union of several disjoint intervals (as in our algorithm) then πS simply pushes all intervals to the left so
that every two consecutive intervals touch each other. We show the following lemma.

⊂

Lemma 7.3. For any measurable set S, πS is a continuous non-decreasing 1-Lipschitz function, and πinv
is a strictly increasing function deﬁned for all y in [0, µ(S)]. Moreover, there exists a set Z0 of measure zero
S.
such that for all y

Z0, we have πinv

[0, µ(S)]

S

S (y)

∈
S (y) is a right inverse for πS(x): πS(πinv

∈

\

S (y)) = y (but not necessarily a left inverse).

Proof. Note that πinv
Let

IS(x) =

(

if x
1,
0, otherwise

∈

S

IS(t)

be the indicator function of set S. Then πS(x) =
0
diﬀerentiation theorem, πS(x) is almost everywhere diﬀerentiable and dπS(x)
Let X0 = [0, R]

x
0 IS(t)dt (we use Lebesgue integration here). Since
1, function πS is non-decreasing, 1-Lipschitz, and absolutely continuous. By the Lebesgue
dx = IS(x) almost everywhere.
X0, we have

S and Z0 = πS(X0). Since πS is absolutely continuous and IS(x) = 0 for x

≤

≤

R

∈

\

Z0, then πS(πinv

Finally, we verify that πinv

Now if y /
∈
S (a)) and b = πS(πinv
S (b).

a = πS(πinv
S (a) < πinv
πinv

S (y)) = y /
∈

S

µ(Z0)

≤

ZX0

dπS(x)
dx

dx =

IS(x)dx = 0.

ZX0
X0 or, equivalently, πinv

Z0, thus πinv

S (y) /
∈
is strictly increasing. Consider a, b
S (b)). Thus, πS(πinv

[0, µ(S)] with a < b. Note that
∈
S (a)) < πS(πinv
S (b)). Since πS is non-decreasing,

S (y)

∈

S, as required.

Note that if S is a union of ﬁnitely many disjoint open intervals, then Z0 is the image of the endpoints of

those intervals under πS.

7.4 Clusters Satisfying Property (c) of Theorem 5.1

We ﬁrst show that if
with a γ-light shell of width r.

Ball(z, R1)
|

|

< ρq(β)

Ball(z, R0)
|

· |

, then µ(S)

≥

R/2. To this end, we deﬁne a ball

Deﬁnition 7.4. We say that the ball of radius t

r around z has a γ-light shell of width r if

≥

Ball(z, t + r)

| − |

Ball(z, t

r)

| ≤

γ

−

|

r

t
−

0
Z

Ball(z, x)
|

|

dx.

We let Sγ be the set of all radii t in the range (3R0, R1] such that Ball(z, t) has a γ-light shell of width

≥

R/2 for γ = 25r/R2

S and (b) µ(Sγ)

r. We now show that (a) Sγ ⊂
Lemma 7.5. We have Sγ ⊂
S.
Proof. Consider a number t from Sγ and the ball of radius t around z: P = Ball(z, t). Let us pick an arbitrary
point u. We are going to prove that inequality (5.2) holds and therefore t
Ball(u, r).
S. Consider v
∈
P . Thus, if
Observe that δP (u, v) = 1 only if both u and v belong to the r neighborhoods of P and X
δP (u, v) = 1, we must have d(z, u), d(z, v)
r, t + r], then the left side of (5.2)
equals 0, and we are done. Hence, we can assume that d(z, u)

0. and, therefore, µ(S)

[t
−
r, t + r].

R/2.

−

≥

∈

∈

[t

\

r, t + r]. If d(z, u) /
∈
−

∈

[t

Using the observation above, we bound the left hand side of (5.2) as

δP (u, v)

≤ |

Ball(z, t + r)

| − |

Ball(z, t

.

r)
|

−

Ball(u,r)

Xv
∈

17

(7.5)

dx.

We now need to lower bound the right hand side of (5.2). Note that Ball(u, 2R) contains Ball(z, t), since

d(z, u)

t + r

≤

≤

R1 + r = R

−

R0 + r < R,

and t < R. Thus,

For all v

∈

Ball(z, t)

≡

Ball(u,2R)

Xv
∈
P , we have

By the triangle inequality, we have

Ball(z,t)

Xv
∈

d(u, v)

R ∨p (u, v)

1
R

≥

d(u, v)

∨p (u, v).

Ball(z,t)

Xv
∈

∨P (u, v) = 1. Hence,
d(u, v)

∨p (u, v) =

d(u, v)

Ball(z,t)

Xv
∈

d(u, v)

≥

(d(z, u)

−

d(z, v))+

((t

r)

−

−

≥

d(z, v))+.

Observe that

Hence, (7.5) is lower bounded by

((t

r)

−

−

d(z, v))+ =

r

t
−

0
Z

1

d(z, v)

x

dx.

≤

(cid:8)

(cid:9)

r

t
−

1

d(z, v)

x

dx =

r

t
−

1

d(z, v)

x

dx =

r

t
−

0
Ball(z,t) Z

Xv
∈

Xv
∈
Since the ball of radius t has a γ-light shell of width r, the expression above is, in turn, lower bounded by

Ball(z,t)

(cid:8)

(cid:8)

(cid:9)

(cid:9)

≤

0

Z

≤

Ball(z, x)
|

|

0

Z

Thus, the right hand side of inequality (5.2) is lower bounded by

|

Ball(z, t + r)

| − |
γ

Ball(z, t

r)
|

.

−

25βD2
β
R ·

|

Ball(z, t + r)

| − |
γ

Ball(z, t

r)
|

.

−

This completes the proof of Lemma 7.5, since

25βD2
β
R ·

1
γ

=

Lemma 7.6. We have µ

Sγ

R/2.

≥

25βD2
25R

βR2
0
r

·

(r/R) D2

β (R/Dβ)2
Rr

=

= 1.

To prove this lemma, we use the following result from Appendix A.

(cid:0)

(cid:1)

Lemma 7.7. Consider a non-decreasing function Φ : [0, R]
[0, R
(0, 1/r]. Then, for the subset S of numbers t
and γ

≤

R with Φ(0) = 1 and R > 0. Let r
r] for which inequality

∈

(0, R]

→
−
t

∈

Φ(t + r)

≥

Φ(t) + γ

Φ(x)dx

0
Z

(7.6)

holds, we have Φ(R)

eηµ(S)

−

1, where η =

γ/(e

−

≥

1)r, and µ(S) is the measure of set S.

Proof of Lemma 7.6. We apply Lemma 7.7 to the function

p

Φ(t) = |

Ball(z, t + 3R0)
|
Ball(z, 3R0)
|

|

18

0. Note that to be able to apply Lemma 7.7 we
with parameters r′ = 2r, R′ = R1 −
need γ < 1/r′ which is equivalent to βDβ < 1/5√2. The latter holds due to β being suﬃciently small, i.e.,
β

. Observe that Φ(0) = 1 and

3R0 −

r, and γ = 25r/R2

Θ

1
(q ln(q+1)

≤

(cid:0)

(cid:1)

Φ(R′)

Ball(z, R1)
|
|
Ball(z, 3R0)
|
|

≤

≤

ρq(β)
|

Ball(z, R0)
|

Ball(z, R0)
|

|

= ρq(β).

Here, we used that the Ball(z, R1) is light. From Lemma 7.7, we get that Φ(R′)

γ/(e

−

p

1)r′, and S′ is the set of t for which Inequality (7.6) holds. Thus,

µ(S′)

≤

=

=

=

1 + ln Φ(R′)
η′

1 + ln ρq(β)
η′

≤

=

1 + Dβ/2
η′

(e

1)r′
−
γ

·

(1 + Dβ/2)

2(e

1)r

−
25r

R0 ·

·

(1 + Dβ/2)

1)

2(e

−
25

·

(R0 + R/2) < 0.4(R + R0).

s

r

r

eη′µ(S′)

1, where η′ =

−

≥

where we used R0 ·
is at least µ(S′′)
≥
R0 + r < R1/100.

Dβ = R and that
R0)
((R

−

2(e
r)

−
−

−

3R0 −
p
Sγ. Consider an arbitrary t

≥

We claim that S′′+3R0+r

Then,

⊂

1) < 2. Therefore for the measure of the set S′′ = [0, R′]
0.4(R + R0)

S′
R/2. Here, we relied on our assumption that

\

S′′. First, observe that t+3R0+r

(3R0, R1].

∈

∈

|

Ball(z, t + 3R0 + r′)
|
Ball(z, 3R0)
|

|

|

Ball(z, t + 3R0)
|
Ball(z, 3R0)
|

|

−

= Φ(t + r′)

Φ(t)

−

t

< γ

Φ(x)dx = γ

0
Z

t

0
Z

|

Ball(z, x + 3R0)
|
Ball(z, 3R0)
|

|

dx.

For t′ = t + 3R0 + r, we get

Ball(z, t′ + r)

| − |

Ball(z, t′

r)
|

−

< γ

|

t′

3R0−

r

−

Ball(z, x + 3R0)
|

|

dx

t′

r

−

Z0
r

−

t′

= γ

Z3R0

Ball(z, x)
|

|

dx < γ

Ball(z, x)
|

|

dx.

Z0

Thus, t′

∈

Sγ. This ﬁnishes the proof.

Lemma 7.6 together with Lemma 7.5 imply the following corollary.

Corollary 7.8. Let S be the set deﬁned in Deﬁnition 5.2. Then, µ(S)

R/2.

≥

7.5 Clusters Satisfying Property (b) of Theorem 5.1

We now show how to choose a random t
(b) of Theorem 5.1. We ﬁrst choose a random x
deﬁned in Deﬁnition 5.4, and then let t = πinv
section. Note that by Lemma 7.3, t = πinv

S, so that the random cluster P = Ball(z, t) satisﬁes property
[0, R/2] with the cumulative distribution function F (x)
(3R0, R1] is the set obtained in the previous
= 0 (see Lemma 7.3).
Z0}
To show that property (b) is satisﬁed, we verify that for every u in Ball(z, R), Condition I or Condition

⊂
S with probability 1, since Pr
{

S (x), where S

S (x)

∈

∈

∈

∈

x

II of Lemma 7.2 holds.

19

Pick a point u in Ball(z, R). We consider two cases: πS(d(z, u)) > R/2

R0 and πS(d(z, u))

R/2

R0.

We prove that u satisﬁes Condition I of Lemma 7.2 in the former case and Condition II in the latter case.
First case: πS(d(z, u)) > R/2

R0. Write,

−

≤

−

−
t

Pr
{

d(z, u)

R0}

= Pr
{

x

−

≥

≥

πS(d(z, u)

.

R0)
}

−

Since πS is a 1-Lipschitz function, we have

πS(d(z, u)

R0)

−

≥

πS(d(z, u))

R0 ≥

−

R/2

−

2R0.

Therefore,

We prove the following claim.

Claim 7.9. We have

Proof. Write:

t

Pr
{

≥

d(z, u)

R0} ≤

1

−

−

F (R/2

2R0).

−

F (R/2

1

−

−

2R0) . βq+1.

F (R/2

2R0) =

−

1

Note that e−Dβ/2 = βq+1. Then,

−R
2R0

e

e

−

−
1

2R0
R0

e
·
−R
2R0

=

e2e−Dβ/2
e−Dβ/2

1
−
1
−

.

F (R/2

1

−

−

2R0) =

(e2
1

−

1)
−
βq+1 ·

βq+1.

Since the denominator of the right hand side is greater than 1/2 (recall that we assume that β is suﬃciently
small), we have 1

2R0) . βq+1.

F (R/2

−

−

Claim 7.9 ﬁnishes the analysis of the ﬁrst case, since
R0.

t
Second case: πS(d(z, u))

≥

Ball(z, t)
|

/

|

|

Ball(z, R0)

| ≥

1 for every value of

R/2

≤

R0. In this case, for every v

Ball(u, R0), we have

−
πS(d(z, v))

∈
πS(d(z, u) + R0)

≤

R/2.

≤

Here, we used that πS is a 1-Lipschitz function. We claim that inequality (7.3) holds for every two points
R0. In particular, it holds for v1 = u and
R/2 and d(v1, v2)
v1, v2 ∈
v2 = v. Without loss of generality assume, that d(z, v1)

X with πS(d(z, v1)), πS(d(z, v2))

d(z, v2). Then,

≤

≤

δP (v1, v2) = 1

Pr
{

}

≤
= Pr
d(z, v1)
{
= Pr
πS(d(z, v1))
{
= F (πS(d(z, v2)))

≤

t < d(z, v2)
}
x < πS(d(z, v2))
}
F (πS(d(z, v1))).

≤

−

Here, we used that random variable x has distribution function F . We show the following claim.
Claim 7.10. For all x1 ≤

x2 in the range [0, R/2], we have

Proof. We have

F (x2)

F (x1)

Dβ ·

≤

−

x1)

(x2 −
R

1

−

·

(cid:0)

F (x1) + 2βq+1

.

(cid:1)

F (x2)

−

x2

F (x1) =

F ′(x)dx

x1

Z
(x2 −

≤

= (x2 −

= Dβ ·

[x1,x2]

x1) max
x
∈
e−
1
−
x1)

x1)

·
(x2 −
R

F ′(x)

x1/R0 /R0
R/2R0
e−

x1/R0

e−

.

·

1

−

R/2R0

e−

20

Here, we used that R0 = R/Dβ. We now need to upper bound the third term on the right hand side:

x1/R0

e−

R/2R0

e−

1

−

= 1

= 1

−

−

(1

−

e−
1

F (x1) +

x1/R0

e−

R/2R0 )
e−
e−

−

−
R/2R0

R/2R0

.

R/2R0

e−

1

−

As in Claim 7.9, let us use that e−

R/2R0 = βq+1 and 1

βq+1

−

≥

1/2 to get

Combining the bounds above, we get the following inequality:

x1/R0

e−

e−

R/2R0 ≤

1

−

F (x1) + 2βq+1.

1

−

F (x2)

F (x1)

Dβ ·

≤

−

x1

x2 −
R

1

−

·

(cid:16)

F (x1) + 2βq+1

.

(cid:17)

Using Claim 7.10 and the inequality

πS(d(z, v2)))

πS(d(z, v1)

−

d(z, v1)

d(z, v2)
−
d(v1, v2),

≤
≤

we derive the following upper bound

δP (v1, v2) = 1

Pr
{

} ≤

Then,

Therefore,

Pr

{∨P (v1, v2) = 1

}

Dβ

≤

d(v1, v2)
R

·

1

−

(cid:0)

F (πS(d(z, v1)))) + 2βq+1

.

(cid:1)

= Pr
{

= 1

−

t

d(z, v1)
≤
F (πS(d(z, v1))).

}

δP (v1, v2) = 1

Pr
{

Dβ

d(v1, v2)
R

} −

Pr

{∨P (v1, v2) = 1

·

} ≤

2Dβ

d(v1, v2)
R

βq+1.

Thus, the left hand side of (7.3) is upper bounded by

βq+1

2Dβ ·
R0 and R0 = R/Dβ.

·

d(v1, v2)
R

≤

2βq+1.

Here, we use that d(v1, v2)

≤

References

Ahmadi, S., Khuller, S., and Saha, B. Min-max correlation clustering via multicut. In Proceedings of the

Conference on Integer Programming and Combinatorial Optimization, pp. 13–26, 2019.

Ailon, N., Charikar, M., and Newman, A. Aggregating inconsistent information: ranking and clustering.

Journal of the ACM (JACM), 55(5):23, 2008.

Bansal, N., Blum, A., and Chawla, S. Correlation clustering. Machine learning, 56(1-3):89–113, 2004.

Bartal, Y. Probabilistic approximation of metric spaces and its algorithmic applications. In Proceedings of

37th Conference on Foundations of Computer Science, pp. 184–193. IEEE, 1996.

21

Bonchi, F., Garc´ıa-Soriano, D., and Liberty, E. Correlation clustering: from theory to practice. In Proceedings
of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1972, 2014.

C˘alinescu, G., Karloﬀ, H., and Rabani, Y. An improved approximation algorithm for multiway cut. Journal

of Computer and System Sciences, 60(3):564–574, 2000.

Charikar, M., Guruswami, V., and Wirth, A. Clustering with qualitative information. In Proceedings of the

Symposium on Foundations of Computer Science, 2003.

Charikar, M., Gupta, N., and Schwartz, R. Local guarantees in graph cuts and clustering. In Proceedings of

the Conference on Integer Programming and Combinatorial Optimization, pp. 136–147, 2017.

Chawla, S., Makarychev, K., Schramm, T., and Yaroslavtsev, G. Near optimal LP rounding algorithm for
correlation clustering on complete and complete k-partite graphs. In Proceedings of the Symposium on
Theory of Computing, pp. 219–228, 2015.

Cohen, W. and Richman, J. Learning to match and cluster entity names.

In Proceedings of the ACM

SIGIR-2001 Workshop on Mathematical/Formal Methods in Information Retrieval, 2001.

Cohen, W. W. and Richman, J. Learning to match and cluster large high-dimensional data sets for data
integration. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pp. 475–480, 2002.

Demaine, E. D., Emanuel, D., Fiat, A., and Immorlica, N. Correlation clustering in general weighted graphs.

Theoretical Computer Science, 361(2-3):172–187, 2006.

Fakcharoenphol, J., Rao, S., and Talwar, K. A tight bound on approximating arbitrary metrics by tree

metrics. Journal of Computer and System Sciences, 69(3):485–497, 2004.

Jafarov, J., Kalhan, S., Makarychev, K., and Makarychev, Y. Correlation clustering with asymmetric clas-
In Proceedings of the International Conference on Machine Learning, pp. 4641–4650,

siﬁcation errors.
2020.

Kalhan, S., Makarychev, K., and Zhou, T. Correlation clustering with local objectives. In Advances in Neural

Information Processing System, pp. 9341–9350, 2019.

Puleo, G. J. and Milenkovic, O. Correlation clustering and biclustering with locally bounded errors. IEEE

Transactions on Information Theory, 64(6):4105–4119, 2018.

Ramachandran, A., Feamster, N., and Vempala, S. Filtering spam with behavioral blacklisting. In Proceedings

of the Conference on Computer and Communications Security, pp. 342–351, 2007.

Tang, S., Andres, B., Andriluka, M., and Schiele, B. Multi-person tracking by multicut and deep matching.

In Proceedings of the European Conference on Computer Vision, pp. 100–111, 2016.

Tang, S., Andriluka, M., Andres, B., and Schiele, B. Multiple people tracking by lifted multicut and person
re-identiﬁcation. In Proceedings of the Conference on Computer Vision and Pattern Recognition, pp. 3539–
3548, 2017.

van Zuylen, A., Hegde, R., Jain, K., and Williamson, D. P. Deterministic pivoting algorithms for constrained
ranking and clustering problems. In Proceedings of the Symposium on Discrete Algorithms, pp. 405–414,
2007.

Wirth, A. Correlation clustering. In Encyclopedia of Machine Learning, pp. 227–231. Springer, 2010.

22

A Proof of Lemma 7.7

We ﬁrst prove Lemma 7.7 for the case when S is a measure zero set. Speciﬁcally, we show the following
lemma.

Lemma A.1. Suppose, a non-decreasing function Φ : [0, R]
inequality for all t

→
Y0, where set Y0 has measure zero:

[0, R

r]

∈

−

\

R with Φ(0) = 1 satisﬁes the following

Φ(t + r)

t

Φ(t) + γ

Φ(x)dx,

(A.1)

≥

0
Z
(0, 1/r], then Φ(t)

for some R > 0, r

(0, R/2] and γ

∈
1)r. Consequently, we have Φ(R)

∈

γ/(e

−

eηR

1.

−

≥

max
{

≥

eηt

−

1, 1

}

for all t

∈

[0, R], where η =

p
Proof. Since Φ(0) = 1 and Φ(t) is non-decreasing, we have Φ(t)
eηt
Φ(t)
≥
holds for t
[0, R] for integer i
hypothesis holds for i = 0. Suppose, it holds for i, we prove it for i + 1.

0. We now prove that
1. We establish this inequality by induction. The inductive hypothesis is that this inequality
1. Thus, the inductive
[0, 1/η + ir]

≥
1 > eηt

1/η, we have Φ(t)

1 for all t

0. For t

≥

≤

≥

≥

∈

∩

−

−

First, consider an arbitrary t∗

right by r. Let t = t∗
we have Φ(x)
−

eηx

−

1 for all x

[1/η, 1/η + (i + 1)r]
r. Note that t > 0, since r < 1/η. Also, t /
∈

[0, R]

∩

∈

\

[1/η, t]. Using Inequality (A.1), we obtain the following bound

(Y0 + r), where Y0 + r is the set Y0 shifted
Y0. Then, by the inductive hypothesis,

≥

∈
Φ(t∗) = Φ(t + r)

1/η

t

Φ(t) + γ

Φ(x)dx + γ

Φ(x)dx

0
Z
1 + γ

1/η

1 dx + γ

1/η

Z
t

eηx

1 dx

−

0
Z
1 + γ/η + γ/η
1(1 + γ/η).

·

(eηt

−

−

−

1/η

Z
1

1)

−

≥

eηt

−

≥
= eηt
= eηt

Since η =
get

γ/(e

−

1)r, we have γ/η = (e

−

1)ηr. Now, using the inequality ex

1 + (e

−

≤

1)x for x

∈

[0, 1], we

p

Φ(t∗)

≥

≥

eηt
eηt

−

−

·

1(1 + γ/η) = eηt
eηr = eη(t+r)
1

1(1 + (e
1 = eηt∗

−

−

−
1.
−

1)ηδ)

To ﬁnish the proof, we need to show that Φ(t∗∗)
[0, R]
Y0+r has measure zero, there exists an increasing sequence t∗k of numbers in [0, 1/η+(i+1)r]
that tends to t∗∗ as k
we have

. Using that Φ is a non-decreasing function and eηt

[1/η, 1/η +(i+1)r]

(Y0 +r). Since
∩
(Y0+r))
([0, R]
1 is a continuous function,

→ ∞

≥

∈

∩

∩

\

−

−

eηt∗∗

1 for t∗∗

Φ(t∗∗)

lim
k
→∞

≥

Φ(t∗k)

lim
k
→∞

≥

eηt∗
k−

1 = eηt∗∗

1.

−

We now show that Lemma A.1 implies Lemma 7.7. Loosely speaking, in the proof, we shift all intervals
from the set S to the left to obtain a single interval [0, µ(S)]. We then apply Lemma A.1 to the transformed
function.

Proof of Lemma 7.7. Let πS and πinv
and let Y0 be a measure zero set as in Lemma 7.7. We claim that Φ∗(t) satisﬁes (A.1) for all t
Fix t

S be the maps deﬁned in Section 7.3. Deﬁne Φ∗(t) as Φ∗(t) = Φ(πinv
[0, π(S)]

[0, π(S)]

Y0. Write

∈

S (t))
Y0.

\

∈

\

Φ∗(t + r) = Φ(πinv

S (t + r))

Φ(πinv

S (t) + r).

≥

23

Here, we used that (a) πinv
thus

S (t+r)

≥

πinv
S (t)+r and (b) Φ is a monotone function. By Lemma 7.7, πinv

S (t)

S,

∈

Φ∗(t + r)

Φ(πinv

S (t) + r)

Φ(πinv

S (t)) + γ

≥

≥

0
Z

πinv

S (t)

Φ(x)dx.

We now observe that Φ∗(t) = Φ(πinv

S (t)) and

πinv

S (t)

Φ(x)dx

0
Z

πinv

S (t)

0
Z

πinv

S (t)

0
Z

t

Φ(x)

1(x

·

∈

S)dx

Φ(x)dπS (x)

Φ∗(x)dx.

≥

=

=

Here, we used that dπS(x) = 1(x
we have

∈

S)dx and πS(πinv

0
Z
S (t)) = t. Thus, we showed that for all t

[0, π(S)]

Y0,

\

∈

Φ∗(t + r)

≥

t

Φ∗(t) +

Φ∗(x)dx.

0

Z

We now use Lemma A.1 with function Φ∗ and R′ = µ(S). We obtain the following inequality:

Φ∗(µ(S))

≥

eηµ(S)

−

1,

which concludes the proof of Lemma 7.7.

B Integrality Gap

In this section, we present an integrality gap example for the convex program (P ) in Figure 1.

. Consider a complete graph on n vertices. Let P be a path of length
1. Denote its endpoints by s and t and the set of its edges by EP . All edges in P are positive edges of

Construction. Let n = 1 +
n
weight 1. Edge (s, t) is a negative edge of weight 1. All other edges are positive edges of weight α.

1/α
⌉

p

−

⌈

The cost of the integral solution Clearly, every integral solution
P

(since all these edges cannot be satisﬁed simultaneously). Thus, disu(

P

should violate some edge (u, v)

, E+, E−)

≥

P

∈
1 and

(s, t)
}
, E+, E−)

∪ {
dis(
P

k

1.

kp ≥

The cost of the CP solution. We deﬁne the CP solution as follows. Denote the distance between u and
v along P by distP (u, v). Let xuv = distP (u, v)/(n
1). Note that xst = 1. The values of variables yu are
determined by constraints (P1) of the convex program.

−

Now we upper bound the contribution of every edge (u, v) (incident on u) to yu in formula (P1). The
xst) = 0
α. Since

contribution of (u, v)
1/(n
−
(whether or not it is incident on u), and the contribution of every other edge (u, v) is wuvxuv ≤
1) + αn . √α. Now,
every vertex u is incident on at most 2 edges from EP , we have yu ≤
. n1/pα1/2 . α1/2

1); the contribution of edge (s, t) is wst(1

EP is wuvxuv = 1

−
1/(2p).

n1/p

2/(n

−

∈

y

·

−

k

kp ≤

max
u |

yu|

·

Integrality gap We conclude that the integrality gap is at least Ω((1/α)1/2

−

1/(2p)).

24

