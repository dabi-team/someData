Fast Multi-label Learning*

Xiuwen Gong, Dong Yuan, Wei Bao

1

1
2
0
2

g
u
A
1
3

]

G
L
.
s
c
[

1
v
0
7
5
3
1
.
8
0
1
2
:
v
i
X
r
a

Abstract

Embedding approaches have become one of the most pervasive techniques for multi-label classiﬁcation. However,

the training process of embedding methods usually involves a complex quadratic or semideﬁnite programming

problem, or the model may even involve an NP-hard problem. Thus, such methods are prohibitive on large-scale

applications. More importantly, much of the literature has already shown that the binary relevance (BR) method is

usually good enough for some applications. Unfortunately, BR runs slowly due to its linear dependence on the size of

the input data. The goal of this paper is to provide a simple method, yet with provable guarantees, which can achieve

competitive performance without a complex training process. To achieve our goal, we provide a simple stochastic

sketch strategy for multi-label classiﬁcation and present theoretical results from both algorithmic and statistical learning

perspectives. Our comprehensive empirical studies corroborate our theoretical ﬁndings and demonstrate the superiority

of the proposed methods.

Online Classiﬁcation, Multi-label, Metric Learning, k-Nearest Neighbour (kNN).

Index Terms

I. INTRODUCTION

Multi-label classiﬁcation [1–4], in which each instance can belong to multiple labels simultaneously, has signiﬁ-

cantly attracted the attention of researchers as a result of its wide range of applications, which range from document

classiﬁcation and automatic image annotation to video annotation. For example, in automatic image annotation, one

needs to automatically predict relevant keywords, such as beach, sky and tree, to describe a natural scene image.

When classifying documents, one may need to classify them into different groups, such as Science, Finance and

Sports. In video annotation, labels such as Government, Policy and Election may be needed to describe the subject

of the video.

A popular strategy in multi-label learning is binary relevance (BR)[5], which independently trains a linear

regression model for each label independently. Recently, some sophisticated models are developed to improve

the performance of BR. For example, embedding approaches [6–10] have become popular techniques. Even though

embedding methods improve the prediction performance of BR to some extent, their training process usually

involves a complex quadratic or semideﬁnite programming problem, as in [11], or their model may involve an

NP-hard problem, as in [8] and [12]. Thus, these kinds of methods are prohibitive on large-scale applications.

*The original version of this paper has been published on IJCAI 2021.

X. Gong, D. Yuan, W. Bao are with the Faculty of Engineering, The University of Sydney, Australia (E-mails: xiuwen.gong@sydney.edu.au;

dong.yuan@sydney.edu.au; wei.bao@sydney.edu.au)

 
 
 
 
 
 
2

Much of the literature, such as [13], [14] and [15], has already shown that BR with appropriate base learner is

usually good enough for some applications, such as document classiﬁcation [15]. Unfortunately, BR runs slowly

due to its linear dependence on the size of the input data. The question is how to overcome these computational

obstacles yet obtain comparable results with BR.

To address the above problem, we provide a simple stochastic sketch strategy for multi-label classiﬁcation.

In particular, we carefully construct a small sketch of the full data set, and then use that sketch as a surrogate

to perform fast optimization. This paper ﬁrst introduces stochastic σ-subgaussian sketch, and then proposes the

construction of a sketch matrix based on Walsh-Hadamard matrix to reduce the expensive matrix multiplications of

σ-subgaussian sketch. From an algorithmic perspective, we provide provable guarantees that our proposed methods

are approximately as good as the exact solution of BR. From a statistical learning perspective, we provide the

generalization error bound of multi-label classiﬁcation using our proposed stochastic sketch model.

Experiments on various real-world data sets demonstrate the superiority of the proposed methods. The results

verify our theoretical ﬁndings. We organize this paper as follows. The second section introduces our proposed

stochastic sketch for multi-label classiﬁcation. The third section provides the provable guarantees for our algorithm

from both algorithmic and statistical learning perspectives, and experimental results are presented in the fourth

section. The last section provides our conclusions.

II. STOCHASTIC SKETCH FOR MULTI-LABEL CLASSIFICATION

Assume that x(i) ∈ Rp×1 is a real vector representing an input (instance), and y(i) ∈ {0, 1}q×1 is a real vector

representing the corresponding output (i ∈ {1 . . . n}). n denotes the number of training samples. The input matrix
is X ∈ Rn×p and the output matrix is Y ∈ {0, 1}n×q. (cid:104)·, ·(cid:105) and In×n represent the inner product and the n × n
identity matrix, respectively. We denote the transpose of the vector/matrix by the superscript (cid:48) and the logarithms
to base 2 by log. Let || · ||2 and || · ||F represent the l2 norm and Frobenius norm, respectively. Let V ∈ Rp×q be

the regressors and N (0, 1) denote the standard Gaussian distribution.

A simple linear regression model for BR [5] learns the matrix V through the following formulation:

min
V ∈Rp×q

1
2

||XV − Y ||2
F

(1)

Assuming that n > p and n > q, the computational complexity for this problem is O(npq + np2) [16]. The

computational cost of an exact solution for problem 1 will be prohibitive on large-scale settings. To solve this

problem, we construct a small sketch of the full data set by stochastic projection methods, and then use that sketch
as a surrogate to perform fast optimization for problem 1. Speciﬁcally, we deﬁne a sketch matrix S ∈ Rm×n and

S (cid:54)= 0, where m < n is the projection dimension and 0 is the zero matrix with all the zero entries. The input

matrix X and output matrix Y are approximated by their sketched matrix SX and SY , respectively. We aim to

solve the following sketched problem of problem 1.

min
V ∈Rp×q

1
2

||SXV − SY ||2
F

(2)

Motivated by [12, 17, 18], we use a k-nearest neighbor (kNN) classiﬁer in the embedding space for prediction,

instead of using an expensive decoding process [11]. Next, we introduce two kinds of stochastic sketch methods.

3

A. Stochastic σ-Subgaussian Sketch

The entries of a sketch matrix can be simply deﬁned as i.i.d random variables from certain distributions, such as

Gaussian distribution and Bernoulli distribution. [19] has already shown that each of these distributions is a special

case of Subgaussian distribution, which is deﬁned as follows:

Deﬁnition 1 (σ-Subgaussian). A row si ∈ Rn of the sketch matrix S is σ-Subgaussian, if it has zero mean and for
any vector ζ ∈ Rn and (cid:15) ≥ 0, we have

P (|(cid:104)si, ζ(cid:105)| ≥ (cid:15)||ζ||2) ≤ 2e− n(cid:15)2

2σ2

Clearly, a vector with i.i.d standard Gaussian entries or Bernoulli entries is 1-Subgaussian. We refer any matrix
S ∈ Rm×n to a Subgaussian sketch if its rows are zero mean, 1-Subgaussian, and with the covariance matrix

cov(si) = In×n. A Subgaussian sketch is straightforward to construct. However, given the Subgaussian sketch
S ∈ Rm×n, the cost of computing SX and SY is O(npm) and O(nqm), respectively. Next, we introduce the

following technique to reduce this time complexity.

B. Stochastic Walsh-Hadamard Sketch

Inspired by [20], we propose to construct the sketch matrix based on Walsh-Hadamard matrix to reduce the

expensive matrix multiplications of Subgaussian sketch. Formally, a stochastic Walsh-Hadamard sketch matrix
S ∈ Rm×n is obtained with i.i.d. rows of the form:

√

si =

neiHR,

i = 1, · · · , m

where {e1, · · · , em} is a random subset of m rows uniformly sampled from In×n, R ∈ Rn×n is a random diagonal
matrix whose entries are i.i.d. Rademacher variables and H ∈ Rn×n constitutes a Walsh-Hadamard matrix deﬁned

as:

Hij = (−1)(cid:104)B(i)−1,B(j)−1(cid:105),

i, j = 1, · · · , n

where B(i) and B(j) represent the binary expression with τ -bit of i and j (assume 2τ = n).

Then, we can employ fast Walsh-Hadamard transform [21] to perform SX and SY in O(np log m) and O(nq log m).

III. MAIN RESULTS

Since we address problem 2 rather than directly solving problem 1, which has great advantages for fast optimiza-
tion, it is interesting to ask the question: what is the relationship between problem 2 and problem 1? Let V ∗ and ˆV be
F and g( ˆV ) = ||SX ˆV − SY ||2
the optimal solutions of problem 1 and problem 2. We deﬁne f (V ∗) = ||XV ∗ − Y ||2
F .
We will prove that we can choose an appropriate m such that the two optimal objectives f (V ∗) and g( ˆV ) are

approximately the same. This means that we can speed up the computation of problem 1, without sacriﬁcing too

much accuracy. Furthermore, we provide the generalization error bound of the multi-label classiﬁcation problem

4

using our proposed stochastic sketch model. To measure the quality of approximation, we ﬁrst deﬁne the δ-optimality

approximation as follows:

Deﬁnition 2 (δ-Optimality Approximation). Given δ ∈ (0, 1), ˆV is a δ-optimality approximation solution, if

(1 − δ)f (V ∗) ≤ g( ˆV ) ≤ (1 + δ)f (V ∗)

According to the properties of Matrix norm, we have g( ˆV ) ≤ ||S||F f ( ˆV ), so g( ˆV ) is proportional to f ( ˆV ).

Therefore, the closeness of g( ˆV ) and f (V ∗) implies the closeness of f ( ˆV ) and f (V ∗).

A. σ-Subgaussian Sketch Guarantee

We ﬁrst introduce the tangent cone, which is used by [22]:

Deﬁnition 3 (Tangent Cone). Given a set C ⊆ Rp and x∗ ∈ C, the tangent cone of C at x∗ is deﬁned as
K = clconv{r ∈ Rp|r = t(x − x∗) for some t ≥ 0 and x ∈ C}, where clconv denotes the closed convex hull.

The tangent cone arises naturally in the convex optimality conditions: any r ∈ K deﬁnes a feasible direction at

the optimal x∗, and optimality means that it is impossible to decrease the objective function by moving in directions

belonging to the tangent cone. Then, we introduce the Gaussian width, which is an important complexity measure

used by [23]:

Deﬁnition 4 (Gaussian Width). Given a closed set Y ⊆ Rn, the Gaussian width of Y, denoted by ω(Y), is deﬁned

as:

where g ∼ N (0, In×n).

ω(Y) = Eg[sup
z∈Y

|(cid:104)g, z(cid:105)|]

This complexity measure plays an important role in learning theory and statistics [24]. Let Sn−1 = {z ∈
Rn|||z||2 = 1} be the Euclidean sphere. XK represents the linearly transformed cone: {Xr ∈ Rn|r ∈ K}, and we
use Gaussian width to measure the width of the intersection of XK and Sn−1. This paper deﬁnes Y = XK ∩ Sn−1.

We state the following theorem for guaranteeing the σ-Subgaussian sketch:

Theorem 1. Let S ∈ Rm×n be a stochastic σ-Subgaussian sketch matrix, c1 and c2 be universal constants.
, ˆV is a δ-optimality
Given any δ ∈ (0, 1) and m = O(( c1

δ )2ω2(Y)), then with probability at least 1 − 6qe− c2mδ2

σ4

approximation solution.

The proof sketch of this theorem can be found in the supplementary material.

Remark. Theorem 1 guarantees that the stochastic σ-Subgaussian sketch method is able to construct a small

sketch of the full data set for the fast optimization of problem 1, while preserving the δ-optimality of the solution.

5

TABLE I: The results of Hamming Loss on the various data sets.

SS+GAU

SS+WH

DATA SET BR+LIB BR+kNN FASTXML SLEEC m = 256 m = 512 m = 1024 m = 256 m = 512 m = 1024

COREL5K

0.0098

0.0095

0.0093

0.0094

0.0095

0.0095

0.0094

0.0103

0.0102

NUS(VLAD) 0.0211

0.0213

0.0209

0.0207

0.0221

0.0218

0.0216

0.0230

0.0225

NUS(BOW)

0.0215

0.0220

0.0216

0.0213

0.0227

0.0223

0.0222

0.0229

0.0226

0.0099

0.0218

0.0223

RCV1X

0.0017

0.0019

0.0019

0.0018 0.00189 0.00188

0.00187

0.00199 0.00195

0.00192

TABLE II: The results of Example-F1 on the various data sets.

SS+GAU

SS+WH

DATA SET BR+LIB BR+kNN FASTXML SLEEC m = 256 m = 512 m = 1024 m = 256 m = 512 m = 1024

COREL5K

0.1150

0.0930

0.0530

0.0824

0.0475

0.0446

0.0659

0.0539

0.0817

NUS(VLAD) 0.1247

0.1547

0.1118

0.1578

0.1099

0.1310

0.1460

0.1001

0.1289

NUS(BOW)

0.0984

0.1012

0.0892

0.1122

0.0896

0.0932

0.0952

0.0882

0.0903

RCV1X

0.2950

0.2894

0.2367

0.2801

0.2063

0.2767

0.2813

0.2173

0.2621

0.0902

0.1443

0.0920

0.2796

B. Walsh-Hadamard Sketch Guarantee

We generalize the concept of Gaussian width to two additional measures, S-Gaussian width and Rademacher

width:

Deﬁnition 5 (S-Gaussian Width). Given a closed set Y ⊆ Rn and a stochastic sketch matrix S ∈ Rm×n, the

S-Gaussian width of Y, denoted by ωS(Y), is deﬁned as:

where g ∼ N (0, Im×m).

ωS(Y) = Eg,S[sup
z∈Y

|(cid:104)g,

Sz
√
m

(cid:105)|]

Deﬁnition 6 (Rademacher Width). Given a closed set Y ⊆ Rn, the Rademacher width of Y, denoted by Υ(Y), is

deﬁned as:

where (cid:36) ∈ {±1}n is an i.i.d. vector of Rademacher variables.

Υ(Y) = E(cid:36)[sup
z∈Y

|(cid:104)(cid:36), z(cid:105)|]

Next, we still deﬁne Y = XK ∩ Sn−1 and state the following theorem for guaranteeing the Walsh-Hadamard

sketch:

Theorem 2. Let S ∈ Rm×n be a stochastic Walsh-Hadamard sketch matrix, c1, c2 and c3 be universal constants.
Given any δ ∈ (0, 1) and m = O(( c1
c2e−

δ )2(Υ(Y) + (cid:112)6log(n))2ω2
Υ(Y)2+log(nm) (cid:1), ˆV is a δ-optimality approximation solution.

S(Y)), then with probability at least 1 − 6q(cid:0) c2

(mn)2 +

c3mδ2

6

Remark. An additional term (Υ(Y) + (cid:112)6log(n))2 appears in the sketch size, so the required sketch size for the

Walsh-Hadamard sketch is larger than that required for the σ-Subgaussian sketch. However, the potentially larger

sketch size is offset by the much lower cost of matrix multiplications via the stochastic Walsh-Hadamard sketch

matrix. Theorem 2 guarantees that the stochastic Walsh-Hadamard sketch method is also able to construct a small

sketch of the full data set for the fast optimization of problem 1, while preserving the δ-optimality of the solution.

C. Generalization Error Bound

This subsection provides the generalization error bound of the multi-label classiﬁcation problem using our

proposed two stochastic sketch models. Because our results can be applied to two models, we simply call our

stochastic sketch models SS. Assume our model is characterized by a distribution D on the space of inputs and labels
X ×{0, 1}q, where X ⊆ Rp. Let a sample {(x(j), y(j))} be drawn i.i.d. from the distribution D, where y(j) ∈ {0, 1}q

(j ∈ {1, . . . , n}) are the ground truth label vectors. Assume n samples D = {(x(1), y(1)), · · · , (x(n), y(n))} are

drawn i.i.d. n times from the distribution D, which is denoted by D ∼ Dn. For two inputs x(z), x(j) in X , we deﬁne
d(x(z), x(j)) = ||x(z) − x(j)||2 as the Euclidean metric in the original input space and dpro(x(z), x(j)) = (cid:107) ˆV (cid:48)x(z) −
ˆV (cid:48)x(j)(cid:107)2 as the metric in the embedding input space. Let hD
input x using our model SS-kNN, which is trained on D. The performance of SS-kNN: (hD

(x) represent the prediction of the i-th label for

(·), · · · , hD

(·)) :

knni

knn1

knnq

X → {0, 1}q is then measured in terms of its generalization error, which is its expected loss on a new example

(x, y) drawn according to D:

ED∼Dn,(x,y)∼D

(cid:16)

q
(cid:88)

i=1

(cid:96)(yi, hD

knni

(x))

(cid:17)

(3)

where yi means the i-th label and (cid:96)(yi, hD

knni

(x)) represents the loss function for the i-th label. We deﬁne the loss

function as follows for the analysis.

(cid:96)(yi, hD

knni

(x)) = P (yi (cid:54)= hD

knni

(x))

For the i-th label, we deﬁne the function as follows:

The Bayes optimal classiﬁer b∗ for the i-th label is deﬁned as

νi
j(x) = P (yi = j|x), j ∈ {0, 1}.

b∗
i (x) = arg max
j∈{0,1}

νi
j(x)

(4)

(5)

(6)

Before deriving our results, we ﬁrst present several important deﬁnitions and theorems.

Deﬁnition 7 (Covering Numbers, [25]). Let (X , d) be a metric space, A be a subset of X and ε > 0. A set B ⊆ X

is an ε-cover for A, if for every a ∈ A, there exists b ∈ B such that d(a, b) < ε. The ε-covering number of A,

N (ε, A, d), is the minimal cardinality of an ε-cover for A (if there is no such ﬁnite cover then it is deﬁned as ∞).

7

TABLE III: The training time (in second) on the various data sets.

SS+GAU

SS+WH

DATA SET

BR+LIB BR+kNN FASTXML SLEEC m = 256 m = 512 m = 1024 m = 256 m = 512 m = 1024

COREL5K

7.198

0.678

4.941

736.670

0.196

NUS(VLAD)

222.21

179.04

715.86

9723.49

25.29

NUS(BOW)

511.83

351.64

1162.53

11391.54

52.05

0.218

51.68

72.65

0.366

93.97

120.37

RCV1X

22607.53

353.42

1116.05

78441.93

72.53

114.55

144.17

0.119

11.87

25.41

48.88

0.197

20.22

34.32

55.94

0.239

33.04

48.85

72.22

Deﬁnition 8 (Doubling Dimension, [26]). Let (X , d) be a metric space, and let ¯λ be the smallest value such
that every ball in X can be covered by ¯λ balls of half the radius. The doubling dimension of X is deﬁned as :
ddim(X ) = log2(¯λ).

Theorem 3 ([26]). Let (X , d) be a metric space. The diameter of X is deﬁned as diam(X ) = sup
x,x(cid:48)∈X

d(x, x(cid:48)). The

ε-covering number of X , N (ε, X , d), is bounded by:

N (ε, X , d) ≤

(cid:16) 2diam(X )
ε

(cid:17)ddim(X )

(7)

We provide the following generalization error bound for SS-1NN:

Theorem 4. Given a metric space (X , dpro), assume function νi : X → [0, 1] is Lipschitz with constant L with
respect to the sup-norm for each label. Suppose X has a ﬁnite doubling dimension: ddim(X ) = D < ∞ and

diam(X ) = 1. Let D = {(x(1), y(1)), · · · , (x(n), y(n))} and (x, y) be drawn i.i.d. from the distribution D. Then,

we have

ED∼Dn,(x,y)∼D

(cid:16)

q
(cid:88)

i=1

P (yi (cid:54)= hD

1nni

(cid:17)

(x))

≤

q
(cid:88)

i=1

2P (b∗

i (x) (cid:54)= yi) +

3qL|| ˆV ||F
n1/(D+1)

(8)

Inspired by Theorem 19.5 in [27], we derive the following lemma for SS-kNN:

Lemma 1. Given metric space (X , dpro), assume function νi : X → {0, 1} is Lipschitz with constant L with
respect to the sup-norm for each label. Suppose X has a ﬁnite doubling dimension: ddim(X ) = D < ∞ and

diam(X ) = 1. Let D = {(x(1), y(1)), · · · , (x(n), y(n))} and (x, y) be drawn i.i.d. from the distribution D. Then,

we have

ED∼Dn,(x,y)∼D

(cid:16)

q
(cid:88)

i=1

P (yi (cid:54)= hD

knni

(cid:17)

(x))

≤

q
(cid:88)

i=1

(1 + (cid:112)8/k)P (b∗

i (x) (cid:54)= yi)+

q(6L|| ˆV ||F + k)
n1/(D+1)

The following corollary reveals important statistical properties of SS-1NN and SS-kNN.

(9)

8

Fig. 1: Experiment results of SS+GAU and SS+WH on rcv1x data set.

Corollary 1. As n goes to inﬁnity, the error of the SS-1NN and SS-kNN converges to the sum of twice the Bayes
error and 1 + (cid:112)8/k times Bayes error over the labels, respectively.

A. Data Sets and Baselines

IV. EXPERIMENT

We abbreviate our proposed stochastic σ-Subgaussian sketch and stochastic Walsh-Hadamard sketch to SS+GAU

and SS+WH, respectively. In the experiment, we set the entries in the σ-Subgaussian sketch matrix as i.i.d standard

Gaussian entries. This section evaluates the performance of the proposed methods on four data sets: corel5k,

nus(vlad), nus(bow) and rcv1x. The statistics of these data sets are presented in website1. We compare SS+GAU

and SS+WH with several state-of-the-art methods, as follows.

• BR [5]: We implement two base classiﬁers for BR. The ﬁrst uses linear classiﬁcation/regression package

LIBLINEAR [28] with l2-regularized square hinge loss as the base classiﬁer. We simply call this baseline

BR+LIB. The second uses kNN as the base classiﬁer. We simply call this baseline BR+kNN and count the

kNN search time as the training time.

• FastXML [1]: An advanced tree-based multi-label classiﬁer.

• SLEEC [12]: A state-of-the-art embedding method, which is based on sparse local embeddings for large-scale

multi-label classiﬁcation. We use solvers of FastXML and SLEEC provided by the respective authors with

default parameters.

Following the similar settings in [29] and [12], we set k = 10 for the kNN search in all kNN based methods.

The sketch size m is chosen in a range of {64, 128, 256, 512, 1024}. Following [7], [11] and [30], we consider the

Hamming Loss and Example-F1 measures to evaluate the prediction performance of all the methods. The smaller

the value of the Hamming Loss, the better the performance, while the larger the value of Example-F1, the better

the performance.

1http://mulan.sourceforge.net

9

B. Results

Figure 1 shows that with the increasing sketch size, the training time of SS+GAU and SS+WH rise, while the

prediction performance of SS+GAU and SS+WH becomes better. The results verify our theoretical analysis. The

Hamming Loss, Example-F1 and training time comparisons of various methods on corel5k, nus(vlad), nus(bow)

and rcv1x data sets are shown in Table I, Table II and Table III, respectively. From Tables I,

II and III, we can

see that:

• BR and SLEEC usually achieve better results, which is consistent with the empirical results in [12] and [15].

However, SLEEC is the slowest method compared to other baselines.

• Because we perform the optimization only on a small sketch of the full data set, our proposed methods are

signiﬁcantly faster than BR and state-of-the-art embedding approaches. Moreover, we can maintain competitive

prediction performance by setting an appropriate sketch size. The empirical results illustrate our theoretical

studies.

V. CONCLUSION

This paper carefully constructs stochastic σ-Subgaussian sketch and Walsh-Hadamard sketch for multi-label

classiﬁcation. From an algorithmic perspective, we show that we can obtain answers that are approximately as good

as the exact answer for BR. From a statistical learning perspective, we also provide the generalization error bound

of multi-label classiﬁcation using our proposed stochastic sketch model. Lastly, our empirical studies corroborate

our theoretical ﬁndings, and demonstrate the superiority of the proposed methods.

SUPPLEMENTARY: THE PROOF OF IMPORTANT THEOREMS AND LEMMAS

A. Proof of Theorem 1

We ﬁrst present the following lemma, which is derived from [31] and [32].

Lemma 1. Let S ∈ Rm×n be a stochastic σ-Subgaussian sketch matrix. Then there are universal constants c1 and
c2 such that for any subset Y ⊆ Sn−1, any u ∈ Sn−1 and δ ∈ (0, 1), we have

|z(cid:48)S z| ≤

sup
z∈Y

c1√
m

ω(Y) + δ

with probability at least 1 − e− c2mδ2

σ4

, and we have

with probability at least 1 − 3e− c2mδ2

σ4

, where S = S(cid:48)S − In×n.

|z(cid:48)S u| ≤

sup
z∈Y

5c1√
m

ω(Y) + 3δ

(10)

(11)

Theorem 1. Let S ∈ Rm×n be a stochastic σ-Subgaussian sketch matrix, c1 and c2 be universal constants.
, ˆV is a δ-optimality
Given any δ ∈ (0, 1) and m = O(( c1

δ )2ω2(Y)), then with probability at least 1 − 6qe− c2mδ2

σ4

approximation solution.

10

1 , · · · , V ∗
F and ||SX ˆV − SY ||2
i=1 ||SX ˆVi − SYi||2

q , ˆV1, · · · , ˆVq and Y1, · · · , Yq be q columns of matrix V ∗, ˆV and Y , respectively. Then
2 and ||SX ˆV −
2. We

F = (cid:80)q
F can be decomposed to ||XV ∗ − Y ||2
2. Next, we study the relationship between ||XV ∗

2 and ||SX ˆVi − SYi||2

i=1 ||XV ∗

i − Yi||2

i − Yi||2

i . According to Deﬁnition 3 in the main paper, we know that M belongs to the tangent cone

Proof. Let V ∗

||XV ∗ − Y ||2
F = (cid:80)q
SY ||2
deﬁne M = ˆVi − V ∗
of C at V ∗
i .

Because V ∗

Yi, XM (cid:105) + ||XM ||2

i ∈ arg minr∈Rp ||Xr − Yi||2
2. Then, we get:

2, we have ||XV ∗

i − Yi||2

2 ≤ ||X ˆVi − Yi||2

2 = ||XV ∗

i − Yi||2

2 + 2(cid:104)XV ∗

i −

2(cid:104)XV ∗

i − Yi, XM (cid:105) + ||XM ||2

2 ≥ 0

(12)

As ˆVi ∈ arg minr∈Rp ||SXr−SYi||2
SYi, SXM (cid:105) + ||SXM ||2

2, we have ||SXV ∗

2 ≥ ||SX ˆVi−SYi||2

2 = ||SXV ∗

2. Then, we get ||SXM ||2

i − SYi, SXM (cid:105) ≤ 2||SXV ∗

i −SYi||2
2 ≤ −2(cid:104)SXV ∗

i −SYi||2

2+2(cid:104)SXV ∗

i −
i − SYi||2||SXM ||2

and

||SXM ||2 ≤ 2||SXV ∗

i − SYi||2

(13)

We derive the following:

||SX ˆVi − SYi||2
2

=||SXV ∗

i − SYi||2

2+||SXM ||2

2+2(cid:104)SXV ∗

i − SYi, SXM (cid:105)

=||SXV ∗

i − SYi||2

2 + ||XM ||2

2 + (cid:104)XM, S XM (cid:105)

+ 2(cid:104)XV ∗

i − Yi, S XM (cid:105) + 2(cid:104)XV ∗

i − Yi, XM (cid:105)

By using Lemma 1, with probability at least 1 − 4e− c2mδ2

σ4

, we have

||SX ˆVi − SYi||2
2

≤||SXV ∗

i −SYi||2

2 + ||XM ||2

+ 2||XV ∗

i − Yi||2||XM ||2(1 +

2(1 +

ω(Y) + δ)

c1√
m
5c1√
m
i − Yi||2||XM ||2 ≤ γ||XV ∗

ω(Y) + 3δ)

where Y = XK ∩ Sn−1. Given γ > 0, we have 2||XV ∗

sake of clarity, we deﬁne ψ = 1 + 5c1√
expression, with probability at least 1 − 4e− c2mδ2

σ4

m ω(Y) + 3δ and ϕ = 1 + c1√

, we have

2. For the
m ω(Y) + δ, and then substitute them to the above

2 + 1/γ||XM ||2

i − Yi||2

||SX ˆVi − SYi||2
2

≤||SXV ∗

i −SYi||2

2+ γψ||XV ∗

i − Yi||2

2+(

ψ
γ

+ ϕ)||XM ||2
2

(14)

Clearly, we have ω( XV ∗
||XV ∗

i −Yi
i −Yi||2

) ≤ ω(Y). By using Lemma 1, with probability at least 1 − e− c2mδ2

σ4

, we have

||SXV ∗

i − SYi||2
2

=||XV ∗

i − Yi||2

≤||XV ∗

i − Yi||2

2 + (cid:104)XV ∗
c1√
m

2(1 +

i − Yi, S (XV ∗
XV ∗
i − Yi
||XV ∗
i − Yi||2

ω(

i − Yi)(cid:105)

) + δ)

(15)

≤||XV ∗

i − Yi||2

2ϕ

By using Lemma 1, with probability at least 1 − e− c2mδ2
m ω(Y) − δ) = ||XM ||2
||XM ||2

, we have ||SXM ||2
2(2 − ϕ). By using Eq.(13), with probability at least 1 − e− c2mδ2

2 + (cid:104)XM, S XM (cid:105) ≥
, we have

2 = ||XM ||2

2(1 − c1√

σ4

σ4

11

||XM ||2

2 ≤

||SXM ||2
2
2 − ϕ

≤ 4

||SXV ∗

i − SYi||2
2
2 − ϕ

Eq.(14), Eq.(15) and Eq.(16) imply that, with probability at least 1 − 6e− c2mδ2

σ4

, we have

||SX ˆVi − SYi||2
2
ψ
γ+ϕ
2 − ϕ
ψ
γ+ϕ
2−ϕ
ψ
γ

≤(1 + 4

)||SXV ∗

i −SYi||2

2+ γψ||XV ∗

i −Yi||2
2

≤(1+4

)ϕ||XV ∗

i −Yi||2

2+ γψ||XV ∗

i −Yi||2
2

≤(ϕ − 4

− 4ϕ + γψ)||XV ∗

i − Yi||2
2

By setting γ = 4, with probability at least 1 − 6e− c2mδ2

σ4

, we have

||SX ˆVi − SYi||2
2

≤(3ψ − 3ϕ)||XV ∗

i − Yi||2
2

=(

12c1√
m

ω(Y) + 6δ)||XV ∗

i − Yi||2
2

Eq.(18) implies that, with probability at least 1 − 6qe− c2mδ2

σ4

, we have

||SX ˆV −SY ||2

F ≤ (

12c1√
m

ω(Y) + 6δ)||XV ∗−Y ||2
F

By using Eq.(12) and Lemma 1 again, with probability at least 1 − 4e− c2mδ2
||SX ˆVi − SYi||2
2

σ4

, we have

≥||SXV ∗

i −SYi||2

2+(cid:104)XM, S XM (cid:105)+2(cid:104)XV ∗

i −Yi, S XM (cid:105)

≥||SXV ∗

i −SYi||2

2 − ||XM ||2
2(

ω(Y) + δ)

− 2||XV ∗

i − Yi||2||XM ||2(

ω(Y) + 3δ)

c1√
m
5c1√
m

(16)

(17)

(18)

(19)

We deﬁne ˆψ = 5c1√
probability at least 1 − 4e− c2mδ2

σ4

m ω(Y) + 3δ and ˆϕ = c1√

, we have

m ω(Y) + δ, and then substitute them to the above expression, with

||SX ˆVi − SYi||2
2

≥||SXV ∗

i −SYi||2

2− γ ˆψ||XV ∗

i − Yi||2

2−(

ˆψ
γ

+ ˆϕ)||XM ||2
2

By using Lemma 1 again, with probability at least 1 − e− c2mδ2

σ4

, we have

||SXV ∗

i − SYi||2

2 ≥ ||XV ∗

i − Yi||2

2(1 − ˆϕ)

Similar to Eq.(16), by using Eq.(13) and Lemma 1, with probability at least 1 − e− c2mδ2

σ4

, we have

||XM ||2

2 ≤

||SXM ||2
2
1 − ˆϕ

≤ 4

||SXV ∗

i − SYi||2
2
1 − ˆϕ

(20)

(21)

(22)

Eq.(20), Eq.(21) and Eq.(22) imply that, with probability at least 1 − 6e− c2mδ2

σ4

, we have

||SX ˆVi − SYi||2
2

≥(1−4

ˆψ
γ+ˆϕ
1−ˆϕ

)(1−ˆϕ)||XV ∗

i −Yi||2

2− γ ˆψ||XV ∗

i −Yi||2
2

≥(1 − ˆϕ − 4

ˆψ
γ

− 4 ˆϕ − γ ˆψ)||XV ∗

i − Yi||2
2

By setting γ = 2, with probability at least 1 − 6e− c2mδ2

σ4

, we have

||SX ˆVi − SYi||2
2

≥(1 − 4 ˆψ − 5 ˆϕ)||XV ∗

i − Yi||2
2

=(1 −

25c1√
m

ω(Y) − 17δ)||XV ∗

i − Yi||2
2

Eq.(24) implies that, with probability at least 1 − 6qe− c2mδ2

σ4

, we have

||SX ˆV −SY ||2

F ≥ (1−

25c1√
m

ω(Y)−17δ)||XV ∗−Y ||2
F

12

(23)

(24)

(25)

By rescaling δ and redeﬁning the universal constants appropriately for Eq.(19) and Eq.(25), we prove Theorem

1.

B. Proof of Theorem 2

Let On = {z ∈ Rn|||z||2 ≤ 1} be the Euclidean ball of radius one. We present the following Lemma, which is

derived from [32].

Lemma 2. Let S ∈ Rm×n be a stochastic Walsh-Hadamard sketch matrix. Then there are universal constants c1
and c2 such that for any subset Y ⊆ On, any u ∈ Sn−1 and δ ∈ (0, 1), we have

with probability at least 1 − (cid:0) c1

(mn)2 + c1e−

c2mδ2

Υ(Y)2+log(nm) (cid:1), and we have

|z(cid:48)S z| ≤ Φ(Y) +

sup
z∈Y

δ
2

|z(cid:48)S u| ≤ 39Φ(Y) + 3δ

sup
z∈Y

(26)

(27)

with probability at least 1 − (cid:0)3 c1
(cid:112)6log(nm))ωS(Y)/

m.

√

(mn)2 + 3c1e−

c2mδ2

Υ(Y)2+log(nm) (cid:1), where S = S(cid:48)S − In×n. Φ(Y) = 8(Υ(Y) +

Theorem 2. Let S ∈ Rm×n be a stochastic Walsh-Hadamard sketch matrix, c1, c2 and c3 be universal constants.
Given any δ ∈ (0, 1) and m = O(( c1
c2e−

δ )2(Υ(Y) + (cid:112)6log(n))2ω2
Υ(Y)2+log(nm) (cid:1), ˆV is a δ-optimality approximation solution.

S(Y)), then with probability at least 1 − 6q(cid:0) c2

(mn)2 +

c3mδ2

Proof. The proof idea is similar to the proof of Theorem 1. We also deﬁne V ∗
as q columns of matrix V ∗, ˆV and Y , respectively. Then, we study the relationship between ||XV ∗

q , ˆV1, · · · , ˆVq and Y1, · · · , Yq
2 and

1 , · · · , V ∗

i − Yi||2

||SX ˆVi − SYi||2

2. Let M = ˆVi − V ∗

i . By using Lemma 2, with probability at least 1 − 4(cid:0) c1

(mn)2 + c1e−

we have

||SX ˆVi − SYi||2
2

≤||SXV ∗

i −SYi||2

2 + ||XM ||2

2(1 + Φ(Y) +

δ
2

)

+ 2||XV ∗

i − Yi||2||XM ||2(1 + 39Φ(Y) + 3δ)

13

c2mδ2
Υ(Y)2+log(nm) (cid:1),

where Y = XK ∩ Sn−1. Given γ > 0, we have 2||XV ∗

i − Yi||2||XM ||2 ≤ γ||XV ∗

sake of clarity, we redeﬁne ψ = 1 + 39Φ(Y) + 3δ and ϕ = 1 + Φ(Y) + δ
expression, with probability at least 1 − 4(cid:0) c1

Υ(Y)2+log(nm) (cid:1), we have

c2mδ2

(mn)2 + c1e−

i − Yi||2

2. For the
2 , and then substitute them to the above

2 + 1/γ||XM ||2

||SX ˆVi − SYi||2
2

≤||SXV ∗

i −SYi||2

2+ γψ||XV ∗

i −Yi||2

2+(

ψ
γ

+ϕ)||XM ||2
2

(28)

Proposition 1. Φ( XV ∗
||XV ∗

i −Yi
i −Yi||2

) ≤ 2Φ(Y)

Proof. Υ(XK ∩ Sn−1) = E(cid:36)[supz∈XK∩Sn−1 |(cid:104)(cid:36), z(cid:105)|] ≥ E(cid:36)[(cid:80)n
(cid:80)n

i=1 |zi|2 = 1. Υ({ XV ∗
i −Yi
i −Yi||2

get Υ({ XV ∗
||XV ∗

i −Yi
i −Yi||2

}) = E(cid:36)[sup

||XV ∗
}) ≤ 2Υ(XK ∩ Sn−1). Clearly, we have ωS({ XV ∗
||XV ∗

−Yi
−Yi||2

z∈{

}

XV ∗
i
||XV ∗
i

i −Yi
i −Yi||2

i=1 |zi||(cid:36)i|] = (cid:80)n

i=1 |zi|E(cid:36)i[|(cid:36)i|] = (cid:80)n

|(cid:104)(cid:36), z(cid:105)|] ≤ E(cid:36)[||(cid:36)||2|| XV ∗
||XV ∗

i −Yi
i −Yi||2

i=1 |zi| ≥
||2] = 1. Then, we

}) ≤ ωS(Y). Combining these yields

the result.

By using Lemma 2 and Proposition 1, with probability at least 1 − (cid:0) c1

(mn)2 + c1e−

c2mδ2

Υ(Y)2+log(nm) (cid:1), we have

||SXV ∗

i − SYi||2
2

=||XV ∗

i − Yi||2

2 + (cid:104)XV ∗

≤||XV ∗

i − Yi||2

2(1 + Φ(

i − Yi, S (XV ∗
i − Yi
i − Yi||2

XV ∗
||XV ∗

i − Yi)(cid:105)
δ
2

) +

)

≤||XV ∗

i − Yi||2

2(1 + 2Φ(Y) +

δ
2

)

By using Lemma 2 and Eq.(13), with probability at least 1 − (cid:0) c1

c2mδ2

Υ(Y)2+log(nm) (cid:1), we have

||XM ||2

2 ≤

||SXM ||2
2
2 − ϕ

≤ 4

(mn)2 + c1e−
||SXV ∗

i − SYi||2
2
2 − ϕ

Eq.(28), Eq.(29) and Eq.(30) imply that, with probability at least 1 − 6(cid:0) c1

(mn)2 + c1e−

c2mδ2

Υ(Y)2+log(nm) (cid:1), we have

||SX ˆVi − SYi||2
2
ψ
γ+ϕ
2 − ϕ

≤(1 + 4

)||SXV ∗

i −SYi||2

2+ γψ||XV ∗

i −Yi||2
2

≤(1+ 2Φ(Y)+

δ
2

−4

ψ
γ

−4ϕ+γψ)||XV ∗

i −Yi||2
2

By setting γ = 4, with probability at least 1 − 6(cid:0) c1

(mn)2 + c1e−

c2mδ2

Υ(Y)2+log(nm) (cid:1), we have

||SX ˆVi−SYi||2

2 ≤ (1+115Φ(Y)+

15
2

δ)||XV ∗

i −Yi||2
2

(29)

(30)

(31)

(32)

Eq.(32) implies that, with probability at least 1 − 6q(cid:0) c1

||SX ˆV −SY ||2

F ≤ (1+115Φ(Y)+

(mn)2 + c1e−
15
2

c2mδ2

Υ(Y)2+log(nm) (cid:1), we have

δ)||XV ∗−Y ||2
F

14

(33)

By using Eq.(12) and Lemma 2 again, with probability at least 1 − 4(cid:0) c1

(mn)2 + c1e−

c2mδ2

Υ(Y)2+log(nm) (cid:1), we have

||SX ˆVi − SYi||2
2

≥||SXV ∗

i −SYi||2

2+(cid:104)XM, S XM (cid:105)+2(cid:104)XV ∗

i −Yi, S XM (cid:105)

≥||SXV ∗

i −SYi||2

2 − ||XM ||2

2(Φ(Y) +

δ
2

)

− 2||XV ∗

i − Yi||2||XM ||2(39Φ(Y) + 3δ)

We redeﬁne ˆψ = 39Φ(Y)+3δ and ˆϕ = Φ(Y)+ δ
at least 1 − 4(cid:0) c1

Υ(Y)2+log(nm) (cid:1), we have

(mn)2 + c1e−

c2mδ2

2 , and then substitute them to the above expression, with probability

||SX ˆVi − SYi||2
2

≥||SXV ∗

i −SYi||2

2− γ ˆψ||XV ∗

i −Yi||2

2−(

ˆψ
γ

+ˆϕ)||XM ||2
2

(34)

By using Lemma 2 and Proposition 1 again, with probability at least 1 − (cid:0) c1

(mn)2 + c1e−

c2mδ2

Υ(Y)2+log(nm) (cid:1), we have

||SXV ∗

i − SYi||2

2 ≥ ||XV ∗

i −Yi||2

2(1−2Φ(Y)−

δ
2

)

Similar to Eq.(30), by using Lemma 2 and Eq.(13), with probability at least 1 − (cid:0) c1

(mn)2 + c1e−

have

||XM ||2

2 ≤ 4

||SXV ∗

i − SYi||2
2
1 − ˆϕ

(35)

c2mδ2

Υ(Y)2+log(nm) (cid:1), we

Eq.(34), Eq.(35) and Eq.(36) imply that, with probability at least 1 − 6(cid:0) c1

(mn)2 + c1e−

c2mδ2

Υ(Y)2+log(nm) (cid:1), we have

||SX ˆVi − SYi||2
2
ˆψ
γ+ˆϕ
1 − ˆϕ

≥(1 − 4

)||SXV ∗

i −SYi||2

2− γ ˆψ||XV ∗

i −Yi||2
2

≥(1− 2Φ(Y)−

δ
2

−4

ˆψ
γ

−4 ˆϕ−γ ˆψ)||XV ∗

i −Yi||2
2

By setting γ = 2, with probability at least 1 − 6(cid:0) c1

(mn)2 + c1e−

c2mδ2

Υ(Y)2+log(nm) (cid:1), we have

Eq.(38) implies that, with probability at least 1 − 6q(cid:0) c1

||SX ˆVi−SYi||2

2 ≥ (1−162Φ(Y)−

29
2
(mn)2 + c1e−
29
2

δ)||XV ∗

i −Yi||2
2

c2mδ2

Υ(Y)2+log(nm) (cid:1), we have

δ)||XV ∗−Y ||2
F

||SX ˆV −SY ||2

F ≥ (1−162Φ(Y)−

(36)

(37)

(38)

(39)

By rescaling δ and redeﬁning the universal constants appropriately for Eq.(33) and Eq.(39), we prove Theorem

2.

C. Proof of Theorem 4

Theorem 4. Given a metric space (X , dpro), assume function νi : X → [0, 1] is Lipschitz with constant L with
respect to the sup-norm for each label. Suppose X has a ﬁnite doubling dimension: ddim(X ) = D < ∞ and

diam(X ) = 1. Let D = {(x(1), y(1)), · · · , (x(n), y(n))} and (x, y) be drawn i.i.d. from the distribution D. Then,

15

we have

Proof.

ED∼Dn,(x,y)∼D

(cid:16)

q
(cid:88)

i=1

P (yi (cid:54)= hD

1nni

(cid:17)

(x))

≤

q
(cid:88)

i=1

2P (b∗

i (x) (cid:54)= yi) +

3qL|| ˆV ||F
n1/(D+1)

ED∼Dn,(x,y)∼D

(cid:16)

q
(cid:88)

i=1

P (yi (cid:54)= hD

1nni

(x))

=

q
(cid:88)

i=1

ED∼Dn,(x,y)∼D

(cid:16)

P (yi (cid:54)= hD

1nni

(x))

(cid:17)

(cid:17)

(40)

(41)

Now, we focus on P (yi (cid:54)= hD
with respect to the sup-norm, we have ||νi(x) − νi(x(cid:48))||∞ = sup

1nni

(x)) for the i-th label. Given x, x(cid:48) ∈ X , due to νi(·) is Lipschitz with constant L

|νi

j(x) − νi

j(x(cid:48))| ≤ Ldpro(x, x(cid:48)) and

j∈{0,1}

P (yi (cid:54)= y(cid:48)

i|x, x(cid:48))

=P (yi=1|x)P (y(cid:48)

i=0|x(cid:48))+P (yi=0|x)P (y(cid:48)

i=1|x(cid:48))

=

≤

=

(cid:88)

j∈{0,1}
(cid:88)

j∈{0,1}
(cid:88)

j∈{0,1}

j(x)(1 − νi
νi

j(x(cid:48)))

νi
j(x)(1 − νi

j(x) + Ldpro(x, x(cid:48)))

j(x)(1 − νi
νi

j(x)) + Ldpro(x, x(cid:48))

As dpro(x, x(cid:48)) = || ˆV (cid:48)x − ˆV (cid:48)x(cid:48)||2 ≤ || ˆV ||F d(x, x(cid:48)), we get

i|x, x(cid:48))

P (yi (cid:54)= y(cid:48)
(cid:88)

≤

j(x)(1 − νi
νi

j(x)) + L|| ˆV ||F d(x, x(cid:48))

(42)

(43)

j∈{0,1}

Assume (x(cid:48), y(cid:48)) is the nearest neighbor of (x, y) in D: (x(cid:48), y(cid:48)) = arg min(x(i),y(i))∈D dpro(x, x(i)). Then, we have
(cid:17)
(cid:16)
P (yi (cid:54)= y(cid:48)
i)
ED∼Dn,(x,y)∼D

. Following Eq.(42), we get:

= ED∼Dn,(x,y)∼D

P (yi (cid:54)= hD

(x))

(cid:17)

(cid:16)

1nni

ED∼Dn,(x,y)∼D

≤ED∼Dn,(x,y)∼D

(cid:16)

P (yi (cid:54)= hD
(cid:16) (cid:88)

1nni

(cid:17)

(x))

j(x)(1 − νi
νi

j(x))

+L|| ˆV ||F ED∼Dn,(x,y)∼D

j∈{0,1}
(cid:16)

(cid:17)

d(x, x(cid:48))

(cid:17)

(44)

Assume the solution of arg maxj∈{0,1} νi

j(x) is 1. The ﬁrst term of the right side of Eq.(44) does not depend on

D. Thus

ED∼Dn,(x,y)∼D

(cid:16) (cid:88)

j(x)(1 − νi
νi

j(x))

(cid:17)

j∈{0,1}

16

(cid:16)

=E(x,y)∼D

1(x)(1−νi
νi

1(x)) + νi

0(x)(1−νi

0(x))

(cid:17)

≤E(x,y)∼D(1 − νi

1(x)) + E(x,y)∼D(νi

0(x))

(45)

=2E(x,y)∼D(1 − νi

1(x)) = 2P (b∗

i (x) (cid:54)= yi)

Then, we start to bound the second term of the right side of Eq.(44). Let {C1, · · · , CN } be an ε-cover of X of
cardinality N = N (ε, X , d). Given a sampling D, for x ∈ Ci such that D ∩ Ci (cid:54)= ∅, we have d(x, x(cid:48)) ≤ ε, while
for x ∈ Ci such that D ∩ Ci = ∅, we have d(x, x(cid:48)) ≤ diam(X ) = 1. The expression [D ∩ Ci (cid:54)= ∅] evaluates to 1

if D ∩ Ci (cid:54)= ∅ is true and to 0 otherwise. Thus, we have

ED∼Dn,(x,y)∼D

(cid:16)

(cid:17)

d(x, x(cid:48))

(cid:16) N
(cid:88)

≤ED∼Dn

P (Cj)(ε[D ∩ Cj(cid:54)=∅]+[D ∩ Cj=∅])

(cid:17)

j=1

(cid:16)

P (Cj)

≤

N
(cid:88)

j=1

εED∼Dn ([D ∩ Cj (cid:54)= ∅])

+ED∼Dn ([D ∩ Cj = ∅])

(cid:17)

(46)

Since P (Cj)ED∼Dn([D ∩ Cj = ∅]) = P (Cj)(1 − P (Cj))n ≤ 1/en, where e is the exponent constant. This result,

Eq.(46) and Theorem 3 imply that

By setting ε = 2n− 1

D+1 , we get

ED∼Dn,(x,y)∼D

(cid:16)

d(x, x(cid:48))

(cid:17)

(cid:16)

(cid:16)

≤

≤

ε +

ε +

N
en
1
en

(cid:17)

)D(cid:17)

(

2
ε

ED∼Dn,(x,y)∼D

(cid:16)

(cid:17)

d(x, x(cid:48))

≤

3
n1/(D+1)

Eq.(44), Eq.(45) and Eq.(48) imply that:

ED∼Dn,(x,y)∼D

(cid:16)

P (yi (cid:54)= hD

1nni

(x))

(cid:17)

≤2P (b∗

i (x) (cid:54)= yi) +

3L|| ˆV ||F
n1/(D+1)

Eq. (41), Eq. (44), Eq. (45) and Eq. (48) imply the result.

D. Proof of Lemma 1

(47)

(48)

(49)

Proof. We ﬁrst focus on ED∼Dn,(x,y)∼D
knni
set D = {(x(1), y(1)), · · · , (x(n), y(n))}, let π1(x), · · · , πn(x) be a reordering of {1, · · · , n)} according to their

for the i-th label. For each x ∈ X and training

(x))

P (yi (cid:54)= hD

(cid:16)

(cid:17)

distance to x, dpro. That is, for all j < m, dpro(x, xπj (x)) ≤ dpro(x, xπj+1(x)). Let {C1, · · · , CN } be an ε-cover

of X of cardinality N = N (ε, X , d). Eq.(19.3) in [27] implies that

17

ED∼Dn,(x,y)∼D
(cid:16) (cid:88)

≤ED∼Dn

(cid:16)

(cid:17)

(x))

P (yi (cid:54)= hD
knni
(cid:17)

P (Cj)

j:|Cj ∩D|<k

(cid:16)

+ max
z

PD∼Dn,(x,y)∼D

yi (cid:54)= hD
(cid:17)
dpro(x, xπz(x)) ≤ || ˆV ||F ε

knni

(x)|∀z ∈ [k],

(50)

Following the proof of Theorem 19.5 in [27], the ﬁrst term of the right side of Eq.(50) is bounded by 2N k
en .
The second term of the right side of Eq.(50) is bounded by (1 + (cid:112)8/k)P (b∗
i (x) (cid:54)= yi) + 3L|| ˆV ||F ε. By setting
ε = 2n− 1

D+1 and combining Theorem 3 , we get

(cid:16)

P (yi (cid:54)= hD

knni

(cid:17)

(x))

i (x) (cid:54)= yi)

ED∼Dn,(x,y)∼D
≤(1 + (cid:112)8/k)P (b∗
6L|| ˆV ||F + k
n1/(D+1)

+

(51)

We apply Eq.(51) for each label and take the sum to derive the result.

REFERENCES

[1] Y. Prabhu and M. Varma, “FastXML: A fast, accurate and stable tree-classiﬁer for extreme multi-label learning,” in SIGKDD, August

2014, pp. 263–272.

[2] I. E. Yen, X. Huang, P. Ravikumar, K. Zhong, and I. S. Dhillon, “PD-Sparse : A primal and dual sparse approach to extreme multiclass

and multilabel classiﬁcation,” in ICML, 2016, pp. 3069–3077.

[3] W. Liu, D. Xu, I. W. Tsang, and W. Zhang, “Metric learning for multi-output tasks,” IEEE Transactions on Pattern Analysis and Machine

Intelligence, vol. 41, no. 2, pp. 408–422, 2019.

[4] X. Gong, D. Yuan, and W. Bao, “Online metric learning for multi-label classiﬁcation,” in AAAI, 2020, pp. 4012–4019.

[5] G. Tsoumakas, I. Katakis, and I. P. Vlahavas, “Mining multi-label data,” in Data Mining and Knowledge Discovery Handbook, 2010, pp.

667–685.

[6] D. Hsu, S. Kakade, J. Langford, and T. Zhang, “Multi-label prediction via compressed sensing,” in Advances in Neural Information

Processing Systems 22, 2009, pp. 772–780.

[7] Y.-N. Chen and H.-T. Lin, “Feature-aware label space dimension reduction for multi-label classiﬁcation,” in NIPS, 2012, pp. 1538–1546.

[8] H. Yu, P. Jain, P. Kar, and I. S. Dhillon, “Large-scale multi-label learning with missing labels,” in ICML, 2014, pp. 593–601.

[9] W. Liu and I. W. Tsang, “Making decision trees feasible in ultrahigh feature and label dimensions,” Journal of Machine Learning Research,

vol. 18, pp. 81:1–81:36, 2017.

[10] W. Liu, I. W. Tsang, and K. M¨uller, “An easy-to-hard learning paradigm for multiple classes and multiple labels,” Journal of Machine

Learning Research, vol. 18, pp. 94:1–94:38, 2017.

[11] Y. Zhang and J. G. Schneider, “Maximum margin output coding,” in Proceedings of the 29th International Conference on Machine

Learning, 2012, pp. 1575–1582.

[12] K. Bhatia, H. Jain, P. Kar, M. Varma, and P. Jain, “Sparse local embeddings for extreme multi-label classiﬁcation,” in NIPS, 2015, pp.

730–738.

[13] O. Luaces, J. D´ıez, J. Barranquero, J. J. del Coz, and A. Bahamonde, “Binary relevance efﬁcacy for multilabel classiﬁcation,” Progress in

AI, vol. 1, no. 4, pp. 303–313, 2012.

[14] G. Madjarov, D. Kocev, D. Gjorgjevikj, and S. Dzeroski, “An extensive experimental comparison of methods for multi-label learning,”

Pattern Recognition, vol. 45, no. 9, pp. 3084–3104, 2012.

18

[15] A. Y. Taha and S. Tiun, “Binary relevance (BR) method classiﬁer of multi-label classiﬁcation for Arabic text,” Journal of Theoretical and

Applied Information Technology, vol. 84, no. 3, pp. 414–422, 2016.

[16] G. H. Golub and C. F. V. Loan, Matrix Computations.

Johns Hopkins University Press, 1996.

[17] K. Q. Weinberger and L. K. Saul, “Distance metric learning for large margin nearest neighbor classiﬁcation,” Journal of Machine Learning

Research, vol. 10, pp. 207–244, 2009.

[18] B. Kulis, “Metric learning: A survey,” Foundations and Trends in Machine Learning, vol. 5, no. 4, pp. 287–364, 2013.

[19] J. Matousek, “On variants of the Johnson-Lindenstrauss lemma,” Random Struct. Algorithms, vol. 33, no. 2, pp. 142–156, 2008.

[20] N. Ailon and B. Chazelle, “The fast Johnson-Lindenstrauss transform and approximate nearest neighbors,” SIAM Journal on Computing,

vol. 39, no. 1, pp. 302–322, 2009.

[21] B. J. Fino and V. R. Algazi, “Uniﬁed matrix treatment of the fast Walsh-Hadamard transform,” IEEE Transactions on Computers, vol. 25,

no. 11, pp. 1142–1146, 1976.

[22] R. T. Rockafellar and R. J.-B. Wets, Variational Analysis. Springer-Verlag Berlin Heidelberg, 2004.

[23] Y. Gordon, “Some inequalities for Gaussian processes and applications,” Israel J. Math, vol. 50, pp. 109–110, 1985.

[24] V. Koltchinskii and D. Panchenko, Rademacher Processes and Bounding the Risk of Function Learning. Springer-Verlag, 2000.

[25] J. Shawe-Taylor, P. L. Bartlett, R. C. Williamson, and M. Anthony, “Structural risk minimization over data-dependent hierarchies,” IEEE

Transactions on Information Theory, vol. 44, no. 5, pp. 1926–1940, 1998.

[26] R. Krauthgamer and J. R. Lee, “Navigating nets: Simple algorithms for proximity search,” in Proceedings of the Fifteenth Annual ACM-

SIAM Symposium on Discrete Algorithms, 2004, pp. 798–807.

[27] S. Shalev-Shwartz and S. Ben-David, Understanding Machine Learning: From Theory to Algorithms. New York: Cambridge University

Press, 2014.

[28] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin, “LIBLINEAR: A library for large linear classiﬁcation,” Journal of

Machine Learning Research, vol. 9, pp. 1871–1874, 2008.

[29] M.-L. Zhang and Z.-H. Zhou, “ML-KNN: A lazy learning approach to multi-label learning,” Pattern Recognition, vol. 40, no. 7, pp.

2038–2048, 2007.

[30] Y. Guo and D. Schuurmans, “Multi-label classiﬁcation with output kernels,” in ECML/PKDD, 2013, pp. 417–432.

[31] S. Mendelson, A. Pajor, and N. Tomczak-Jaegermann, “Reconstruction and subgaussian operators in asymptotic geometric analysis,”

Geometric and Functional Analysis, vol. 17, no. 4, pp. 1248–1282, 2007.

[32] M. Pilanci and M. J. Wainwright, “Randomized sketches of convex programs with sharp guarantees,” IEEE Trans. Information Theory,

vol. 61, no. 9, pp. 5096–5115, 2015.

