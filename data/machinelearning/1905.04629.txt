1
2
0
2

n
u
J

3

]

G
L
.
s
c
[

2
v
9
2
6
4
0
.
5
0
9
1
:
v
i
X
r
a

1

Efﬁcient Low-Rank Semideﬁnite Programming
with Robust Loss Functions

Quanming Yao, Member IEEE, Hansi Yang, En-Liang Hu Member IEEE,
and James T. Kwok, Fellow IEEE

Abstract—In real-world applications, it is important for machine learning algorithms to be robust against data outliers or corruptions. In
this paper, we focus on improving the robustness of a large class of learning algorithms that are formulated as low-rank semi-deﬁnite
programming (SDP) problems. Traditional formulations use the square loss, which is notorious for being sensitive to outliers. We
propose to replace this with more robust noise models, including the (cid:96)1-loss and other nonconvex losses. However, the resultant
optimization problem becomes difﬁcult as the objective is no longer convex or smooth. To alleviate this problem, we design an efﬁcient
algorithm based on majorization-minimization. The crux is on constructing a good optimization surrogate, and we show that this
surrogate can be efﬁciently obtained by the alternating direction method of multipliers (ADMM). By properly monitoring ADMM’s
convergence, the proposed algorithm is empirically efﬁcient and also theoretically guaranteed to converge to a critical point. Extensive
experiments are performed on four machine learning applications using both synthetic and real-world data sets. Results show that the
proposed algorithm is not only fast but also has better performance than the state-of-the-arts.

Index Terms—Semi-deﬁnite programming, Robustness, Majorization-minimization, Alternating direction method of multipliers

(cid:70)

1 INTRODUCTION

M ANY machine learning problems involve the search

for matrix-valued solutions. The corresponding opti-
mization problems are often formulated as linear semideﬁ-
nite programs (SDP) [1], [2], [3], [4] of the form:

minZ∈S+ tr(ZA)

s.t. tr(ZQτ ) = tτ , ∀τ = 1, . . . , m,

(1)

in which the objective is linear and the target matrix
Z ∈ Rn×n is positive semi-deﬁnite (PSD). Here, S+ is the
cone of PSD matrices, Qτ and A are some matrices and
tτ is a scalar. Prominent examples include matrix comple-
tion [5], [6], [7], [8], [9], ensemble learning [10], cluster-
ing [11], [12], sparse PCA [13], [14], [15], maximum vari-
ance unfolding (MVU) [16], [17], and non-parametric kernel
learning (NPKL) [18], [19]. The instantiations of Qτ , tτ and
A are application-dependent (with details in Section 5).
For example, in matrix completion, Qτ is constructed from
the positions of observed entries, tτ is the corresponding
observed value, and A is the identity matrix. In NPKL, Qτ
is constructed from sample indices in the must-link/cannot-
link constraint, tτ indicates whether it is a must-link or
cannot-link, and A is a Laplacian matrix for the data mani-
fold.

A standard SDP solver is the interior-point method
(IPM) [1]. In each iteration, a sub-problem based on the

• Q. Yao is with 4Paradigm Inc and Department of Electronic Engineering,

Tsinghua University. E-mail: qyaoaa@connect.ust.hk

• H. Yang is with Department of Electronic Engineering, Tsinghua Univer-

sity. E-mail: yhs17@mails.tsinghua.edu.cn

• E.-L. Hu is with Department of Mathematics, Yunnan Normal University.

•

Email: ynel.hu@gmail.com
J.T. Kwok is with Department of Computer Science and Engi-
neering, Hong Kong University of Science and Technology. Email:
jamesk@cse.ust.hk

• The work is performed when H. Yang was an intern in 4Paradigm; and

correspondence is to Q. Yao.

Lagrangian and log-det barrier function has to be solved nu-
merically, and each such sub-problem iteration takes O(n3)
time. To reduce the computational cost, Yang et al. [20]
introduced an efﬁcient Newton-based algorithm SDPNAL+
to solve the augmented Lagrangian of (1). While O(n3) time
is still required in each iteration of the sub-problem, the
total number of iterations can be much smaller, and enables
SDPNAL+ to be faster than IPM.

An alternative approach is to completely avoid enforcing
the variate to be PSD. Instead, Z is replaced by a low-rank
decomposition XX (cid:62), where X ∈ Rn×r and rank(Z) ≤ r
[4], [11], [21], [22], [23]. Many learning problems, such as
matrix completion, MVU and NPKL, also prefer a low-
rank Z. Problem (1) is then transformed to the nonlinear
optimization problem:

minX tr(XX (cid:62)A) s.t. tr(XX (cid:62)Qτ ) = tτ , ∀τ = 1, . . . , m. (2)

It can be shown theoretically that the factorized problem
is equivalent to the original problem when the rank of the
solution is deﬁcient [22], [23], [24], [25], [26], [27]. Burer
and Monteiro [21] introduced SDPLR, which uses the aug-
mented Lagrangian method together with limited memory
BFGS to solve (2). Due to the loss of convexity, more inner
and outer loop iterations may be needed.

To

avoid handling

constraints
(tr(XX (cid:62)Qτ ) = tτ ) in (2), a common trick is to allow them
to be slightly violated [28] This leads to the optimization
problem:

the m difﬁcult

min
X

(cid:88)m

τ =1

1
2

(tr(X (cid:62)Qτ X) − tτ )2 +

γ
2

tr(X (cid:62)AX),

(3)

where the ﬁrst term measures constraint violations, and γ is
a hyper-parameter controlling the corresponding penalty. To

 
 
 
 
 
 
prevent over-ﬁtting, we further add the regularizer (cid:107)X(cid:107)2
(3), leading to:

F to

γ
2

λ
2

(cid:88)m

1
2

τ =1

(cid:107)X(cid:107)2

F , (4)

tr(X (cid:62)AX)+

(tr(X (cid:62)Qτ X)−tτ )2 +

min
X
where λ > 0 is a tradeoff parameter. This regularizer has
also been popularly used in matrix factorization applica-
tions [29], [30], [31], [32]. One then only needs to solve
the smooth unconstrained optimization problem (4) w.r.t.
X. Gradient descent [24], [25], [33] has been developed as
the state-of-the-art solver for this type of problems. It has
convergence guarantees with linear/sub-linear convergence
rates for certain low-rank formulations [24], [34].

Recall that the square loss is used in (3) and (4) to
measure constraint violations. It is well-known that the
square loss is sensitive to outliers [30], [32], [35], [36],
[37]. This can be problematic as, for example, in MVU,
the samples can be corrupted [38]; in kernel learning, the
similarity constraints may come from spammers [39]; in
matrix completion, there can be attacks in the observed en-
tries [40], [41]. These corruptions and noise can signiﬁcantly
deteriorate the performance [32], [39]. To make the models
more robust, a common approach is to replace the square
loss by more robust noise models. These include the (cid:96)1-
loss [35], [42] and, more recently, concave losses such as the
minimax concave penalty (MCP) [43] and log-sum penalty
(LSP) [44]. These concave loss functions are similar in shape
to Tukey’s biweight function in robust statistics [35], which
ﬂattens more for larger values. Recently, they have also been
successfully used in matrix factorization [30], [32], [36], [37],
[41], [45], [46], [47]. However, so far they have not been used
in SDPs.

Motivated by the needs for both optimization efﬁciency
and robustness in learning the matrix variate, we propose
in this paper the use of robust loss functions with the
matrix factorization in (4). However, the resulting optimiza-
tion problem is neither convex (due to factorization) nor
smooth (due to the robust loss). Hence, none of the above-
mentioned solvers can be used. To handle this difﬁcult prob-
lem, we propose a new optimization algorithm based on
Majorization-Minimization (MM) [48], [49]. The crux of MM
is on constructing a good surrogate that is easier to optimize.
We show that this surrogate can be efﬁciently optimized
by the alternating direction method of multipliers (ADMM)
[50], [51]. While MM only guarantees convergence to limit
points, we show that the proposed algorithm ensures con-
vergence to a critical point even when the ADMM is only
solved inexactly. Efﬁciency and robustness of the proposed
algorithm are demonstrated on ﬁve machine learning ap-
plications, namely, PSD matrix completion, nonparametric
kernel learning, maximum variance unfolding, sparse PCA,
and symmetric non-negative matrix factorization. Results
show that it is not only faster, but also has better perfor-
mance over the state-of-the-arts.

A preliminary version of this paper has been published
in the IJCAI-2019 conference [52]. Compared with the con-
ference version, the major changes here are:
• In [52], we assumed that the optimization of the convex
surrogate is solved exactly. Here, we allow the subprob-
lem to be solved only inexactly, making the whole algo-
rithm more efﬁcient in practice (Section 3.1.2). Besides, we

2

show that when the inexactness is properly controlled,
convergence to critical points is still theoretically guaran-
teed (Section 3.2).

• To further promote robustness, we consider using a non-
convex loss (Section 4) to replace the (cid:96)1-loss used in the
conference version. We show that the proposed algorithm
can still be applied with some modiﬁcations, and conver-
gence is also guaranteed.

• Two more applications namely, PSD matrix completion
(Section 5.1) and symmetric nonnegative matrix factoriza-
tion (Section 5.5), are presented.

• Extensive experiments with more applications, baseline
algorithms, convergence studies, and ablation study are
performed in Section 6.

Notations. We use uppercase letters for matrices, and low-
ercase letters for scalars. The transpose of a vector or matrix
is denoted by the superscript (·)(cid:62). The identity matrix is
denoted I. For a matrix A = [aij], (cid:107)A(cid:107)F = ((cid:80)
ij)1/2 is
its Frobenius norm; (cid:107)A(cid:107)∗ = (cid:80)
i σi(A) is its nuclear norm,
where σi(A) is the ith singular value of A; and tr(A) is its
trace (when the matrix is square). A matrix is positive semi-
deﬁnite (PSD) if its eigenvalues are non-negative. Besides,
(cid:12) denotes the element-wise product between two matrices:
[A (cid:12) B]ij = AijBij; and |S| is the size of a set S.

ij a2

2 RELATED WORKS
2.1 Majorization-Minimization (MM)

Majorization-minimization (MM) [48], [49] is a general tech-
nique to make difﬁcult optimization problems easier. Con-
sider a function f (X), which is hard to optimize. Let the
iterate at the kth MM iteration be Xk. The next iterate is
generated as

(5)

Xk+1 = Xk + arg min ˜X hk( ˜X; Xk),
where hk is a surrogate that is being optimized instead of f .
A good surrogate should have the following properties [48]:
(P1). f ( ˜X + Xk) ≤ hk( ˜X; Xk) for any ˜X;
(P2). 0 ∈ arg min ˜X (hk( ˜X; Xk) − f ( ˜X + Xk)) and f (Xk) =

hk(0; Xk); and
(P3). hk is convex on ˜X.
Condition (P3) allows the minimization of hk in (6) to be
easily solved. Moreover, from (P1) and (P2), we have
f (Xk+1) ≤ min ˜X h( ˜X; Xk) ≤ h(0; Xk) = f (Xk).
Thus, the objectives obtained in successive iterations are
non-increasing. However, MM does not guarantee conver-
gence of the sequence {Xk} [32], [48], [49], [53].

(6)

2.2 Alternating Direction Method of Multipliers (ADMM)

Recently, the alternating direction method of multipliers
(ADMM) [50], [51] has been popularly used in machine
learning and data mining. Consider optimization problems
of the form

minX,Y φ(X) + ψ(Y ) : AX + By = c,

(7)

where φ, ψ are convex functions, and A, B (resp. c) are
constant matrices (resp. vector). ADMM considers the aug-
mented Lagrangian L(X, Y, ν) = φ(X) + ψ(Y ) + ν(cid:62)(AX +

2 (cid:107)AX +By−c(cid:107)2

By−c)+ ρ
2, where ν is the dual variable, and
ρ > 0 is a penalty parameter. At the tth iteration, the values
of X and Y (denoted Xt and Yt) are updated by minimizing
L(X, Y, νt) w.r.t. X and Y in an alternating manner:

Xt+1 = arg minX L(X, Yt, νt),
Yt+1 = arg minY L(Xt+1, Y, νt).

(8)

(9)

Then, ν is updated as νt+1 = νt + ρ(AXt+1 + BYt+1 − c).

2.3 Robust Matrix Factorization (RMF)
In matrix factorization (MF), the data matrix O ∈ Rm×n
is approximated by U V (cid:62), where U ∈ Rm×r, V ∈ Rn×r
and r (cid:28) min(m, n) is the rank. In general, some entries
of O may be missing (as in applications such as structure
from motion [54] and recommender systems [29]). The MF
problem is thus formulated as:

min
U,V

1
2

(cid:107)Ω (cid:12) (O − U V (cid:62))(cid:107)2

F +

λ
2

((cid:107)U (cid:107)2

F + (cid:107)V (cid:107)2

F ),

(10)

where Ω ∈ {0, 1}m×n contain indices to the observed entries
in O (with Ωij = 1 if Oij is observed, and 0 otherwise), and
λ ≥ 0 is a regularization parameter. The square loss in (10)
is sensitive to outliers. In [36], it is replaced by the (cid:96)1-loss,
leading to robust matrix factorization (RMF):

(cid:107)Ω (cid:12) (O − U V (cid:62))(cid:107)1 +

min
U,V

λ
2

((cid:107)U (cid:107)2

F + (cid:107)V (cid:107)2

F ).

(11)

In recent years, many RMF solvers have been developed,
e.g., [30], [32], [45], [55]. However, as the objective in (11)
is neither convex nor smooth, these solvers lack scalability,
robustness and/or convergence guarantees. Recently, the
RMF-MM algorithm [32] solves (11) using MM. Let the kth
iterate be (Uk, Vk). RMF-MM tries to ﬁnd increments ( ˜U , ˜V )
that should be added to (Uk, Vk) in order to obtain the target
(U, V ), i.e., U = Uk + ˜U and V = Vk + ˜V . Substituting into
(11), the objective can be rewritten as

F k( ˜U , ˜V ) ≡(cid:107)Ω (cid:12) (O−(Uk + ˜U )(Vk + ˜V )(cid:62))(cid:107)1
(cid:107)Uk + ˜U (cid:107)2

(cid:107)Vk + ˜V (cid:107)2
F .

F +

+

λ
2

λ
2

The following Proposition constructs a surrogate H k satisfy-
ing properties (P1)-(P3) in Section 2.1 for being a good MM
surrogate. Unlike F k, H k is jointly convex in ( ˜U , ˜V ).

Proposition 1 ( [32]). Let nnz(Ω(i,:)) (resp. nnz(Ω(:,j))) be the
number of nonzero elements in the ith row (resp. jth column) of
Ω, Λr = Diag(
nnz(Ω(m,:))), and Λc =
√
nnz(Ω(:,n))). Then, F k( ˜U , ˜V ) ≤
Diag(
H k( ˜U , ˜V ), where

nnz(Ω(1,:)), . . . ,
√

nnz(Ω(:,1)), . . . ,

√

√

k − ˜U V (cid:62)
H k( ˜U , ˜V ) ≡ (cid:107)Ω(cid:12)(O−UkV (cid:62)
λ
2

(cid:107)Uk + ˜U (cid:107)2

F +

λ
2

+

(cid:107)Vk + ˜V (cid:107)2

k −Uk ˜V (cid:62))(cid:107)1 +
1
2

1
2
(cid:107)Λc ˜V (cid:107)2
F .

F +

(cid:107)Λr ˜U (cid:107)2
F

(12)

Equality holds iff ( ˜U , ˜V ) = (0, 0).

Because of the coupling of ˜U , Vk (resp. Uk, ˜V ) in ˜U V (cid:62)
k
(resp. Uk ˜V (cid:62)) in (12), H k is still difﬁcult to optimize. Thus,
RMF-MM uses ADMM to optimize (12). RMF-MM is guar-
anteed to generate critical points of (11).

3

3 SDP LEARNING WITH (cid:96)1-LOSS

Here, we replace the square loss in (4) by the more robust
(cid:96)1-loss. This leads to the following robust version of (4):

min
X

R(X) ≡

(cid:88)m

τ =1

|tr(X (cid:62)Qτ X) − tτ |

+

γ
2

tr(X (cid:62)AX) +

λ
2

(cid:107)X(cid:107)2
F .

(13)

With the (cid:96)1-loss, the objective in (13) is neither convex nor
smooth. Hence, existing algorithms for solving (4) (such
as L-BFGS [21], gradient descent [24], [25], and coordinate
descent [56]) can no longer be used.

3.1 Optimization Algorithm

Recall from Section 2.1 that MM is a general technique to
make difﬁcult optimization problems easier to optimize.
Recently, MM has also been used successfully in the RMF
solvers of RMF-MM [32] and RMFNL [41]. In this Section,
we design an efﬁcient solver for (13) based on MM. While
RMF-MM and RMFNL construct the surrogate by ﬁrst fac-
torizing the target matrix Z as XY (cid:62) and then bounding X
and Y separately, our construction of the surrogate for (13)
is signiﬁcantly different.

3.1.1 Constructing a Convex Surrogate
Let Xk be the iterate at the kth MM iteration. Recall from
(5) that the next iterate is constructed as Xk + ˜X for some
˜X ∈ Rn×r. The following Lemma bounds R for any ˜X,
where R is the objective deﬁned in (13).
Lemma 1. Let C = A + λ

γ I. For any ˜X ∈ Rn×r,

R(Xk + ˜X) ≤

(cid:88)m

τ =1

|tr(2 ˜X (cid:62)Qτ Xk + X (cid:62)

k Qτ Xk) − tτ |

+

(cid:88)m

τ =1

|tr( ˜X (cid:62)Qτ ˜X)|+

γ
2

(cid:62)

tr( ˜X

C ˜X + (Xk + 2 ˜X)(cid:62)CXk).

As |tr( ˜X (cid:62)Qτ ˜X)| is convex only when Qτ ∈ S+ [3],
the upper bound above is not convex in general. The fol-
lowing provides a looser bound on |tr( ˜X (cid:62)Qτ ˜X)| which
is convex w.r.t. ˜X. We ﬁrst
introduce some notations.
Given a symmetric square matrix M , let its eigenvalues
be γi’s and the corresponding eigenvectors be vi’s. Let
M+ = (cid:80)
(cid:62) be the matrix constructed by
using only the positive eigenvalues, and similarly M− =
(cid:80)
(cid:62) is constructed from only the negative
eigenvalues. Obviously, M = M+ + M−.

i max(γi, 0)vivi

i min(γi, 0)vivi

Lemma 2.
1
2 (Qτ + Q(cid:62)

τ )+ − 1

2 (Qτ + Q(cid:62)

τ )− is PSD.

|tr( ˜X (cid:62)Qτ ˜X)| ≤ tr( ˜X (cid:62) ¯Qτ ˜X), where ¯Qτ =

Combining Lemmas 1 and 2, a surrogate Hk is con-

structed as follows.
Proposition 2. R( ˜X + Xk) ≤ Hk( ˜X; Xk), where
Hk( ˜X; Xk) ≡ tr( ˜X (cid:62)(B ˜X + γCXk))
(cid:88)m

+ 2

τ =1

|tr( ˜X (cid:62)Qτ Xk) + (bk)τ | + ck, (14)

¯Qτ + 1
(cid:62)Qτ Xk) − tτ ), ck = γ

τ =1

with B = (cid:80)m
1
2 (tr(Xk
holds iff ˜X = 0.

2 (λI + γA+), C = A + λ

2 tr(Xk

(cid:62)(A + λ

γ I, (bk)τ =
γ I)Xk). Equality

It is easy to see that Hk( ˜X; X) is convex w.r.t. ˜X and
R(Xk) = Hk(0; Xk). Besides, from Proposition 2, we also
have R( ˜X + Xk) ≤ Hk( ˜X; Xk) for any ˜X, and 0 =
arg min ˜X (Hk( ˜X; Xk) − R( ˜X + Xk)). Thus, Hk satisﬁes the
three desired properties for a MM surrogate in Section 2.1.

3.1.2 Solving the Surrogate Inexactly by ADMM

From (5), Xk is updated at the kth MM iteration as Xk+1 =
Xk + ˜X ∗, where

˜X ∗ = arg min ˜X Hk( ˜X; Xk).

(15)

First, (15) can be easily rewritten as

min ˜X,{eτ } tr( ˜X (cid:62)(B ˜X + γCXk)) + 2

(cid:88)m

τ =1

|eτ | + ck (16)

s.t.

eτ = tr( ˜X (cid:62)Qτ Xk) + (bk)τ , τ = 1, . . . , m.

As in Section 2.2, let ˜ντ be the dual variable associated with
the τ th constraint in (16). The dual of (16) is given by the
following Proposition.

Proposition 3. The dual problem of (16) is

max{˜ντ }∈C Dk({˜ντ }),

(17)

where Dk({˜ντ }) = ck+ γ
2
γ (bk)τ ) − 1
− 2
τ1=1
(Qτ2 Xk)) − γ2
tr((CXk)(cid:62) B−1
∪m
τ =1{˜ντ | |˜ντ | ≤ 2}.

(cid:80)m
(cid:80)m

(cid:80)m

4

4

τ =1 ˜ντ (tr((CXk)(cid:62)B−1 Qτ Xk)
τ2=1 ˜ντ1 ˜ντ2 (tr((Qτ1Xk)(cid:62)B−1
and C =

(CXk))

By using the Slater condition [3], the following Lemma

shows that strong duality for (16) and (17) holds.

Lemma 3. Strong duality for (16) and (17) holds.

In the following, we again use ADMM to solve (16).
At the kth ADMM iteration, it can be easily shown that
the updates in (8) and (9) have the following closed-form
solutions:

˜Xt+1 = ˜Xt − ˜B−1
(eτ )t+1 = max(0, ˜e−

k (2B ˜Xt + ˜CkXk),
τ ) + min(0, ˜e+

τ ),

(18)

(19)

τ =1 (ρ(tr( ˜Xt

where ˜Bk = 2B + ρ (cid:80)m
(cid:80)m
tr( ˜X
variables {˜ντ }τ =1,...,m is then updated as

t+1Qτ Xk) + (bk)τ + ˜ντ ±2

τ , ˜Ck = γC +
τ =
2ρ . Each of the ADMM dual

(cid:62)Qτ Xk) − eτ + (bk)τ )−˜ντ )Qτ , and ˜e±

τ =1 Qτ XkX (cid:62)

k Q(cid:62)

(cid:62)

4

Algorithm 1 Solving subproblem (15) by ADMM.
Require: pre-deﬁned tolerance (cid:15)k;
1: Initialization: t = 1, ˜X1 = 0;
2: while δk( ˜Xt, {(˜ντ )t}) ≥ (cid:15)k do
obtain ˜Xt from (18);
3:
for τ = 1, . . . , m do
4:
5:
6:
7:
8:
9:
10: end while
11: return ˜Xt.

end for
compute duality gap δk( ˜Xt, {(˜ντ )t});
update t = t + 1;

obtain (eτ )t+1 from (19);
update (˜ντ )t+1 from (20);

3.1.3 Complete Algorithm

The whole procedure for solving (13), which will be called
SDP-RL (SDP with Robust Loss), is shown in Algorithm 2.
Note that SDPLR [21] and SDPNAL+ [20] can also solve
optimization problems of the form:

minZ∈S+ tr(ZA) s.t. |tr(ZQτ )−tτ | ≤ ∆, τ = 1, . . . , m,

(22)

which is equivalent to the following optimization problem

minZ∈S+ tr(ZA) + λ

(cid:88)m

τ =1

|tr(ZQτ ) − tτ |,

with (cid:96)1-loss, when the regularization parameter λ is a
properly set. Table 1 compares the proposed SDP-RL with
other algorithms using the (cid:96)1 and square losses on matrix
completion problems (Section 5.1). As can be seen, SDP-RL
is both robust and fast.

Algorithm 2 SDP-RL: SDP with robust loss.
1: Initialization: X1 = 0.
2: for k = 1, . . . , K do
3:
4:
5: end for
6: return XK+1.

obtain ˜Xt from Algorithm 1 with tolerance (cid:15)k;
update Xk+1 = ˜Xt + Xk;

3.2 Convergence Analysis

We make the following Assumption on the objective in (13).

Assumption 1.

lim
(cid:107)X(cid:107)F →∞

R(X) = ∞ and inf
X

R(X) > −∞.

(˜ντ )t+1 = (˜ντ )t +ρ(cid:0)(eτ )t+1 −tr( ˜Xt+1

(cid:62)Qτ Xk)+(bk)τ

(cid:1).

(20)

Because of strong duality (Lemma 3), the duality gap is
zero at optimality. Recall that (15) and (16) have the same
objective value, one can thus use the duality gap

δk( ˜Xt, {(˜ντ )t}) = Hk( ˜Xt; Xk) − Dk({(˜ντ )t}),

(21)

at the tth ADMM iteration to monitor convergence. In
other words, the ADMM iterations can be stopped and an
approximate solution to (15) is found when δk( ˜Xt, {(˜ντ )t})
is smaller than a pre-deﬁned threshold (cid:15)k. The whole pro-
cedure for approximately solving subproblem (15) is shown
in Algorithm 1.

Related algorithms such as RMF-MM [32] and RMFNL
[41] solve the sub-problem exactly when their ADMM iter-
ations terminate. Here, we relax this condition and allow
solving the sub-problem inexactly. Hence, the proofs in [32],
[41] cannot be directly applied.

We assume the following condition on the sequence of

thresholds {(cid:15)k} in Algorithm 2.
Assumption 2. (cid:15)k ≥ 0 for k = 1, . . . , ∞ and (cid:80)∞
ﬁnite positive constant.

k=1 (cid:15)k is a

Remark 1. A popular choice satisfying Assumption 2 is (cid:15)k =
c0/kb0 , where c0 > 0 and b0 > 1 are constants [58], [59].

Usually, MM only guarantees that the objective value
is non-increasing [32], [48], [49]. In contrast, the following

TABLE 1: Comparison of the proposed SDP-RL algorithm with existing algorithms on matrix completion problems (details
are in Section 5.1). The last row shows SDP-RL on sparse data, where “nnz” is the number of nonzero elements. For
algorithms with subproblems, T is the number of iterations to solve the subproblem.

5

FW [5]
L-BFGS [28]
nmAPG [57]
ADMM((cid:96)1) [50]
SDPLR [21]
SDPNAL+ [20]

SDP-RL

dense data
sparse data

factorized
×
√
√

×
×
×
√

model

complexity

loss
square loss
square loss
square loss
(cid:96)1 loss
(cid:96)1 loss
(cid:96)1 loss
(cid:96)1 loss or
nonconvex loss

space
O(n2)
O(nr)
O(nr)
O(n2)
O(nr)
O(n2)
O(n2)
O(nnz(Ω) + nr)

time (per-iteration)
O(n2)
O(nr2)
O(nr2)
O(n2rT )
O(n2rT )
O(n3T )
O(n2rT )
O((nnz(Ω)r + nr2)T )

Theorem shows that the sequence of iterates obtained is
bounded, and its limit points are also critical points.

Theorem 1. With Assumptions 1 and 2, for the sequence {Xk}
generated from Algorithm 2, we have (i) {Xk} is bounded; and
(ii) any limit point of {Xk} is a critical point of R.

4 SDP LEARNING USING NONCONVEX LOSS

The (cid:96)1-loss always linearly penalizes the difference between
the prediction and noisy observation. In very noisy circum-
stances, a loss function φ ﬂatter than the (cid:96)1 loss can be more
robust [41], [60], [61]. Some common examples include the
Geman penalty [62], Laplace penalty [63], log-sum penalty
(LSP) [44], and leaky-minimax concave penalty (MCP) [43]
(Figure 1). They have been used in applications such as
robust matrix factorization for afﬁne rigid structure-from-
motion [41], where outliers arise from feature mismatch; and
sparse coding to learn more discriminative dictionaries [42],
[46], in which large deviations come from damaged, deteri-
orating, or missing parts of an image.

Fig. 1: More robust loss function φ in Table 2 and the (cid:96)1 loss.

In this section, we make the following Assumption on φ.

Assumption 3. φ(α) is a concave and increasing function on
α ≥ 0 with φ(0) = 0.

TABLE 2: Example nonconvex φ functions.

Geman penalty [62]

Laplace penalty [63]
log-sum penalty (LSP) [44]

leaky-minimax concave
penalty (MCP) [43]

(cid:40)

φ(α)
|α|
θ+|α|
1 − exp(− |α|
θ )
log(1 + |x|)

2 α2 + θ|α|,

− 1
η|α| + 1

2 (θ − η)2, α > θ − η

0 ≤ α ≤ θ − η

Table 2 shows the corresponding φ functions for some
popular nonconvex penalties. With a nonconvex loss φ,
problem (13) becomes

˙R(X) ≡

min
X

(cid:88)m

φ(cid:0)|tr(X (cid:62)Qτ X) − tτ |(cid:1)
λ
2

(cid:107)X(cid:107)2
F .

tr(X (cid:62)AX) +

τ =1
γ
2

+

(23)

Because of the nonconvexity of φ, optimization of (23) is
even more difﬁcult. Again, we will alleviate this problem
with the use of MM.

4.1 Convex Surrogate and Its Optimization
For any ˜X ∈ Rn×r, the following Lemma bounds ˙R(Xk +
˜X), where ˙R is the objective in (23).
Lemma 4. For any ˜X ∈ Rn×r,

˙R(Xk + ˜X) ≤

γ
2
+

+

tr((2Xk + ˜X)(cid:62)C ˜X) + ˙ck
(cid:88)m

(qk)τ |tr( ˜X (cid:62)Qτ ˜X)|
(qk)τ |tr((2 ˜X +Xk)(cid:62)QτXk)−tτ |,

τ =1

(cid:88)m

τ =1

(24)

(qk)τ = φ(cid:48)(cid:0)|tr(X (cid:62)

k Qτ Xk) − tτ |(cid:1), and

k Qτ Xk) − tτ |) − (qk)τ tr(X (cid:62)

˙ck =
k Qτ Xk) − tτ |) +

where
(cid:80)m
γ
2 tr(X (cid:62)

τ =1(φ(|tr(X (cid:62)
k CXk).

Combining with Lemma 2, we construct a new surrogate

as follows:

Proposition 4.

˙R( ˜X + Xk) ≤ ˙Hk( ˜X; Xk), where

˙Hk( ˜X; Xk) = tr( ˜X (cid:62)(B ˜X + γCXk))
(cid:88)m

+ 2

τ =1

|tr( ˜X (cid:62) ˙Qτ Xk) + (˙bk)τ | + ˙ck,

(cid:0)

2 (qk)τ

Obviously,

k Qτ Xk)−tτ

˙Qτ = (qk)τ Qτ , B and C as deﬁned in Proposition 2, and
(cid:1). Equality holds iff ˜X = 0.
(˙bk)τ = 1
tr(X (cid:62)
˙Hk( ˜X; X) is convex w.r.t. ˜X. Moreover, it can
be easily seen that the three desirable properties for MM
surrogates (Section 2.1) are also satisﬁed by ˙Hk( ˜X; X). As
in Section 3.1.2, the optimization subproblem for ˙Hk can be
reformulated as:

min ˜X tr( ˜X (cid:62)(Q ˜X + γCXk)) + 2
s.t. eτ = tr( ˜X (cid:62) ˙Qτ Xk) + (˙bk)τ

(cid:88)m

|eτ | + ˙ck,

τ =1
τ = 1, . . . , m.

This is of the same form as (16), and so can be solved
analogously with ADMM. Let

6

˙Dk({ντ }) =

(cid:88)m

(cid:88)m

˙Q(cid:62)
τ ,

˙Qτ XkX (cid:62)
k
˙Qτ Xk)−eτ +(˙bk)τ )−ντ ) ˙Qτ ,

τ =1
(ρ(tr( ˜X (cid:62)
t
τ =1
˙Qτ Xk) + (˙bk)τ + ντ ±2/2ρ,

ντ (tr((CXk)(cid:62)Q−1 ˙Qτ Xk) −

2
γ

(˙bk)τ )

ντ1ντ2(tr(( ˙Qτ1Xk)(cid:62)Q−1( ˙Qτ2 Xk))

τ =1

(cid:88)m

τ1=1

τ2=1

˙Bk = 2Q + ρ
˙Ck = γC +
τ = tr( ˜X
˙e±
γ
2
(cid:88)m

(cid:62)
t+1
(cid:88)m

−

−

1
4
γ2
4

tr((CXk)(cid:62)Q−1(CXk)) + ˙ck.

The resultant procedure, which consists of Algorithms 3
and 4, are slight modiﬁcations of Algorithms 1 and 2,
respectively. The main difference is on how ˙Qτ (resp. Qτ )
and (˙bk)τ (resp. (bk)τ ) are computed. Thus, the complexity
results in Table 1 still apply.

Algorithm 3 Variant of Algorithm 1 for nonconvex loss.
Require: pre-deﬁned tolerance (cid:15)k.
1: Initialization: t = 1, ˜X1 = 0;
2: while δk( ˜Xt, {(˜ντ )t}) ≥ (cid:15)k do
update ˜Xt+1 = ˜Xt − ˙B−1
3:
for τ = 1, . . . , m do
4:
5:

k (2Q ˜Xt + ˙CkXk);

update ( ˙eτ )t+1 = max(0, ˙e−
τ ) + min(0, ˙e+
(cid:62)
(ντ )t+1 = (ντ )t + 1
ρ ( ˙eτ −tr(X
k

˙Qτ ˜Xt+1)−(˙bk)τ );

τ );

end for
compute δk( ˜Xt, {(˜ντ )t}), the upper-bound on inex-
actness;
t = t + 1;

6:
7:
8:

9:
10: end while
11: return ˜Xt.

Algorithm 4 SDP-RL for nonconvex loss.
1: Initialization: X1 = 0.
2: for k = 1, . . . , K do
3:
4:
5: end for
6: return XK+1.

obtain ˜Xt via Algorithm 3 with tolerance (cid:15)k;
update Xk+1 = ˜Xt + Xk;

4.2 Convergence Analysis
In Section 3.2, the convex (cid:96)1-loss is considered, and the
critical points can be characterized by the subgradient of (cid:96)1.
However, the subgradient cannot be used on a nonconvex
loss φ. The following ﬁrst introduces generalizations of the
subgradient and critical point.

Deﬁnition 4.1 ( [64]). The Frechet subdifferential of f at x
is ˆ∂f (x) = {u : limy(cid:54)=x inf y→x
≥ 0}. The
limiting subdifferential (or simply subdifferential) of f at x
is ∂f (x) = {u : ∃xk → x, f (xk) → f (x), uk ∈ ˆ∂f (xk) →
u, as k → ∞}.

f (y)−f (x)−u(cid:62)(y−x)
(cid:107)y−x(cid:107)2

When f is smooth, ∂f (x) reduces to the gradient. When
f is nonsmooth but convex, ∂f (x) is the set of all subgradi-
ents of f at x. An example is shown in Figure 2.

(a) subgradient.

(b) subdifferential.

Fig. 2: Plots of subgradient and Frechet subdifferential,
where p denotes the normal direction.

Deﬁnition 4.2 ( [64]). x is a critical point of f if 0 ∈ ∂f (x).
We make the following Assumption on ˙R, which is

analogous to Assumption 1 on R.

Assumption 4.

lim
(cid:107)X(cid:107)F →∞

˙R(X) = ∞ and inf
X

˙R(X) > −∞.

Convergence to critical points is then ensured by the
following Theorem. Its proof can be easily adapted from
that of Theorem 1.

Theorem 2. With Assumptions 2, 3 and 4, for the sequence
{Xk} generated from Algorithm 4, we have (i) {Xk} is bounded;
(ii) any limit point of {Xk} is a critical point of

˙R.

5 EXAMPLE ROBUST SDP APPLICATIONS

In this section, we illustrate a number of applications that
can be robustiﬁed with the proposed formulations. For
simplicity, we focus on the (cid:96)1 loss. These can be easily
extended to the nonconvex loss in Section 4.

5.1 Positive Semi-Deﬁnite Matrix Completion

The ﬁrst example is on completing a partially observed PSD
matrix [7], [8]. This has applications in, e.g., learning of the
user-item matrix recommender systems [5] and multi-armed
bandit problems [9]. Let the data matrix be O, and Ω ≡
{(i, j)} be the set of indices for the observed Oij’s. PSD
matrix completion can be formulated as ﬁnding a Z ∈ S+
via the following optimization problem:

(cid:88)

min
Z∈S+

(i,j)∈Ω

1
2

(Zij − Oij)2 +

γ
2

tr(Z),

(25)

where the ﬁrst term measures the loss, and the second term
encourages the matrix Z to be low-rank (note that (cid:107)Z(cid:107)∗ =
tr(Z) for Z ∈ S+). Let Q(i,j) be a matrix of zeros except
that Q(i,j)
ij = 1. Problem (25) is then of the form in (3), with
Qτ = Q(i,j), tτ = Oij, and A = 0.

The square loss in (25) may not be appropriate in some
applications. For example, the love-and-hate attack [40] in
recommender systems ﬂips high ratings to low values, and
vice versa [41]. The corrupted ratings then become large
outliers, and using the square loss can lead to signiﬁcant
performance degradation [32], [41]. To improve robustness,
we replace the square loss by the (cid:96)1-loss, leading to

(cid:88)

min
Z∈S+

(i,j)∈Ω

|Zij − Oij| +

γ
2

tr(Z).

(26)

Let Z = XX (cid:62). It is easy to see that Zij = x(cid:62)
is the ith row of X, and tr(Z) = (cid:107)X(cid:107)2
then be rewritten as:
(cid:88)

|tr(X (cid:62)Q(i,j)X) − Oij| +

min
X

(i,j)∈Ω

γ
2

i xj, where x(cid:62)
i
F . Problem (26) can

(cid:107)X(cid:107)2
F .

(27)

5.1.1 Utilizing Data Sparsity

Algorithms 1 and 2 can be directly used to solve (27).
However, each iteration of Algorithm 1 has to construct Qτ
(deﬁned in Lemma 2) and invert ˜Bk (in (18)). A straight-
forward implementation leads to O(n2r) time and O(n2)
space. Recall that the partially observed matrix O is sparse.
In the following, we show how data sparsity can be used to
speed up optimization of (27), as has been demonstrated in
other matrix learning problems [31], [41].
Proposition 5. Let ˜x(cid:62)
i
Xk). The objective in (16) can be rewritten as
1
2
|eτ |

γ
F +
2
+ λtr( ˜X (cid:62)Xk) + 2

i ) be the ith row of ˜X (resp.,

(resp., (xk)(cid:62)

F +
(cid:88)m

(cid:107)Λc ˜X(cid:107)2
F

(cid:107)Λr ˜X(cid:107)2

min ˜X

(cid:107) ˜X(cid:107)2

1
2

τ =1

s.t. eij = ˜x(cid:62)

i (xk)j + (bk)ij, ∀(i, j) ∈ Ω,

where Λr and Λc are as deﬁned in Proposition 1, and (bk)ij =
1
2 ((xk)(cid:62)

i (xk)j − Oij).

Using Proposition 5, the ADMM updates for ˜X and eτ

(in (18) and (19), respectively) become

˜Xt+1 = ˜Xt − ˆB−1
(eij)t+1 = max (cid:0)0, ˆe−

(cid:1) + min (cid:0)0, ˆe+
τ

(cid:1) ,

k ((Λr + Λc + γI) ˜X + ˆCkXk),

(28)

i (xk)j − eij + (bk)ij) − νij)Q(i,j), ˆe±

τ
k ), ˆCk =
where ˆBk = Λr + Λc + γI + Ω (cid:12) (ρXkX (cid:62)
γI + (cid:80)
(i,j)∈Ω(ρ(˜x(cid:62)
ij =
i (xk)j +(bk)ij +(νij ±2)/ρ. To update ˜Xt in (28) (and
(˜xt+1)(cid:62)
also Xk in Algorithm 2), we only need to store sparse matri-
ces ( ˆBk and ˆCk) and diagonal matrices (Λr and Λc). More-
over, the second term on the R.H.S. of (28), which involves
inversion of ˆBk, can be computed in O(nr2 + nnz(Ω)) time
using conjugate gradient descent [28]. Finally, each (ντ )t+1
is updated as (νij)t+1 = (νij)t + (eij− ˜x(cid:62)
i (xk)j + (bk)ij)/ρ in
O(r) time. Thus, each ADMM iteration in Algorithm 1 only
takes O(nr2 + nnz(Ω)r) time and O(nr + nnz(Ω)) space,
which is much faster than the other SDP methods for the (cid:96)1
loss (see Table 1).

5.2 Robust NPKL

In nonparametric kernel learning (NPKL) [65], one tries to
learn a kernel matrix from data. In this section, we adopt
the formulation in [18], [19]. Let T = M ∪ C, where M is
the set containing sample pairs that should belong to the
same class (must-link set), and C is the set containing sample
pairs that should not belong to the same class (cannot-link
set). This can be encoded by the matrix O, such that Oij = 1
for a must-link (i, j) pair, and Oij = 0 for a cannot-link pair.
The NPKL problem is formulated as the following SDP:

minZ∈S+

(cid:88)

(i,j)∈T

(cid:0)Zij − Oij

(cid:1)2

+

γ
2

tr(ZL),

(29)

where Z is the target kernel matrix and L is the Laplacian
of the k-nearest neighbor graph of data. The ﬁrst term in

7

(29) measures the difference between Zij and Oij, while the
second term encourages smoothness on the data manifold
by aligning Z with L.

The must-links and cannot-links are usually provided by
users or crowdsourcing. These can be noisy as there may be
human errors and spammers/attackers on crowdsourcing
platforms [39]. As in Section 5.1, we let Z = XX (cid:62) and
obtain the following robust NPKL formulation:

(cid:88)

|tr(X (cid:62)Q(i,j)X) − Oij| +

min
X

(i,j)∈T

γ
2

tr(X (cid:62)LX) +

λ
2

(cid:107)X(cid:107)2
F ,

where Q(i,j) is the same as in Section 5.1. Obviously, this is
again of the same form as (16), with Qτ = Q(i,j), tτ = Oij
and A = L. When |T | is small, data sparsity can also be
utilized as in Section 5.1.1.

5.3 Robust CMVU

[16]

Maximum variance unfolding (MVU)
is an effec-
tive dimensionality reduction method. It produces a low-
dimensional data representation by simultaneously maxi-
mizing the variance of the embedding and preserving the
local distances of the original data. Colored maximum vari-
ance unfolding (CMVU) is a “colored” variant of MVU [17],
with class label information. Let

¯K = HT H,

(30)

where T is a kernel matrix on labels, and H (with Hij =
1(i = j) − 1
n ) is a matrix that centers the data and labels
in the feature space. CMVU is formulated as the following
optimization problem:

(cid:88)

min
Z∈S+

(i,j)∈Ω

(cid:0)Zii +Zjj −2Zij −dij

(cid:1)2

−

γ
2

tr(Z ¯K),

(31)

where dij is the squared Euclidean distance between the
ith and jth samples in the original space, Ω is a set of
neighbor pairs whose distances are to be preserved in the
embedding, and γ controls the tradeoff between distance
preservation (the ﬁrst term) and dependence maximization
(second term).

Often, outliers and corrupted samples are introduced
during data collection. Again, by letting Z = XX (cid:62), we
have the following robust CMVU formulation which is of
the same form as (16):

γ
2

(cid:88)

(i,j)∈Ω

tr(X (cid:62) ¯KX)+

|tr(X (cid:62) ˆQ(i,j)X)−dij|−

min
X
where ˆQ(i,j) = Q(i,i) + Q(j,j) − Q(i,j) − Q(i,j) and Q(i,j) is
the same as that in Section 5.1. This again is the same form
as (13), with Qτ = ˆQ(i,j), tτ = dij and A = − ¯K. When |Ω|
is small, data sparsity can also be utilized as in Section 5.1.1.

(cid:107)X(cid:107)2
F ,

λ
2

5.4 Sparse PCA

Sparse PCA [13], [15] is a popular method to extract sparse
principal components from the data, i.e., sparse vectors x
that maximizes x(cid:62)Σx for a given covariance matrix Σ ∈
Rn×n. It can be relaxed to the following SDP [13]:

minZ∈S+ −tr(ZΣ)

s.t.

tr(Z) = 1, (cid:107)Z(cid:107)1 ≤ k,

(32)

where k is a hyper-parameter controlling the sparsity.

As in previous sections, we let Z = XX (cid:62). Note that
tr(Z) = (cid:107)X(cid:107)2
Q(i,j)X)|, where Q(i,j) is
in (27). By moving the constraints in (32) to the objective, it
can be reformulated as

F and |Zij| = |tr(X

(cid:62)

(cid:62)

i,j

(cid:88)

|tr(X

min
X

Q(i,j)X)| +

λ
2
which is the same as (13), with Qτ = Q(i,j), tτ = 0 and
A = −Σ.

tr(X (cid:62)ΣX), (33)

(cid:107)X(cid:107)2

F −

γ
2

5.5 Symmetric NMF

Symmetric nonnegative matrix factorization (NMF) [66],
[67] aims to factorize a non-negative and symmetric matrix
O by solving

minX

1
2

(cid:107)O − XX (cid:62)(cid:107)2
F

s.t. Xij ≥ 0.

(34)

SNMF is popular in clustering analysis [68] as it can effec-
tively identify low-dimensional data representations.

Again, the square loss in (34) may not be appropriate in
some scenarios (for example, noisy observed data in cluster-
ing, which affect the observed Oij’s), leading to degraded
performance. Similar to Section 5.1, we have the following
robust SNMF formulation:

min
X

(cid:88)

i,j

|tr(X (cid:62)Q(i,j)X) − Oij| +

λ
2

(cid:107)X(cid:107)2
F ,

(35)

which is of the form in (13) with Qτ = Q(i,j), tτ = Oij and
A = 0.

6 EXPERIMENTS

In this section, experiments are performed on ﬁve ma-
chine learning applications, namely, PSD matrix completion
(Section 6.1), non-parametric kernel learning (Section 6.2),
maximum variance unfolding (Section 6.3), sparse PCA
(Section 6.4), and symmetric NMF (Section 6.5). Depending
on the loss function and whether the matrix variate is
factored, the following SDP solvers will be compared:
1) Solver for SDP problem (3) (i.e., square loss and matrix

variate is not factored):
a) FW [5], which uses the Frank-Wolfe algorithm [6];
2) Solvers for problem (4) (i.e., square loss and factored

matrix variate):
a) nmAPG [57], which uses the state-of-the-art acceler-

ated gradient descent algorithm; and

b) L-BFGS [28], which uses the popular quasi-Newton

solver for smooth minimization problem.

3) Solvers for SDP problem with (cid:96)1-loss, i.e., (22):

a) ADMM((cid:96)1), which solves the nonsmooth but convex

problem with ADMM [50];

b) SDPLR [21], which considers (22) and solves with the

augmented Lagrangian method;

c) SDPNAL+ [20], which also solves (22) but with the

Newton-CG augmented Lagrangian method.
4) Solvers for problem (13) (i.e., (cid:96)1-loss and factored matrix

variate):
a) SDP-RL((cid:96)1): the proposed Algorithm 2, and data
sparsity is utilized as discussed in Section 5.1.1;

8

b) SDP-RL-dense, which is the same as SDP-RL((cid:96)1) ex-

cept that data sparsity is not utilized;

5) Solver for problem (23) (i.e., nonconvex loss and factored

matrix variate):
a) SDP-RL(MCP), the proposed Algorithm 4 which uses
the leaky-MCP loss in Table 2, with θ = 5 and η =
0.05. As reported in [41], [59], [69], the nonconvex
losses in Table 2 usually have similar performance.
For all the SDP-RL variants above, ADMM is used as
the solver for the convex surrogate. We set the maximum
number of ADMM iterations to 1000, and a tolerance (cid:15)k of
max(10−8, c0/kb0 ) as in Remark 1, where b0 = 1.5.

All these algorithms are implemented in Matlab. Each
of these is stopped when the relative change of objective
values in successive iterations is smaller than 10−5 or when
the number of iterations reaches 2000. To reduce statistical
variability, results are averaged over ﬁve repetitions. Results
for the best-performing method and those that are not
signiﬁcantly worse (according to the pairwise t-test with
95% signiﬁcance level) are highlighted. Experiments are run
on a PC with a 3.07GHz CPU and 32GB RAM.

6.1 PSD Matrix Completion

In this section, experiments are performed on PSD matrix
completion (Section 5.1) in the context of recommender
systems. Following [32], we mimic the love/hate attacks,
and some ratings in the synthetic data are randomly set to
the highest/lowest values.

The ground-truth matrix M is generated as a low-rank
matrix V V (cid:62), where V ∈ Rm×r with entries sampled i.i.d.
from N (0, 1). This is then corrupted as M (cid:48) = M + N + S,
where N is a noise matrix and S is a sparse matrix modeling
large outliers (with o being the fraction of nonzero entries).
The entries of N are sampled i.i.d. from N (0, 0.1), while the
nonzero entries of S are sampled uniformly from {−σ, σ}.
We randomly draw 1
m sr log(m)% of the elements from M (cid:48)
as (noisy) observations for training, where s controls the
sampling ratio. Half of the remaining uncorrupted entries in
M are used for validation (hyper-parameter tuning) and the
rest for testing. We experiment with matrix size m ∈ {500,
1000, 1500, 2000}, and set the ranks for all factorization-
based methods to the ground-truth (i.e., 5). The other pa-
rameters are set as o = 0.05, s = 2 and σ = 10.

Let XX (cid:62) be the matrix recovered and M be the
clean ground-truth matrix. For performance evaluation,
we use (i) the testing root mean squared error (RMSE):
(cid:113) 1
(Mij − (XX (cid:62))ij)2; and (ii) CPU time.

(cid:80)

(cid:107)Ωtest(cid:107)1

(i,j)∈Ωtest

6.1.1 Results
The testing RMSEs and CPU time are in Table 3. Conver-
gence of the testing RMSE versus CPU time is in Figure 3.
Though methods based on the square loss (FW, nmAPG,
and L-BFGS) are very fast, they have much higher testing
RMSE’s than methods based on the (cid:96)1 and nonconvex
losses. In particular, FW yields a much larger testing RMSE
than nmAPG and L-BFGS. This is because FW does not
explicitly utilize low-rank factorization but relies only on the
nuclear-norm regularizer. Moreover, it uses rank-one update
in each iteration, and is only as fast as nmAPG and L-BFGS.
Thus, FW will not be included in the sequel.

9
TABLE 3: Testing RMSEs and CPU time (sec) on synthetic data with different matrix sizes (m). The number in brackets is
the percentage of observed elements. ‘-’ indicates the algorithm fails to converge in 104 seconds.

loss

algorithm

square

(cid:96)1

FW
nmAPG
L-BFGS
ADMM((cid:96)1)
SDPLR
SDPNAL+
SDP-RL-dense
SDP-RL((cid:96)1)

leaky-MCP SDP-RL(MCP)

m = 500 (12.43%)
CPU
testing
time
RMSE
2±1
3.2±0.1
2±1
0.964±0.006
2±1
0.964±0.006
0.494±0.008
54±9
0.497±0.008 5064±135
397±45
0.488±0.006
46±6
0.246±0.004
3±1
0.246±0.004
6±1
0.126±0.002

m = 1000 (6.91%)
CPU
testing
time
RMSE
5±1
3.8±0.1
5±1
0.785±0.004
4±1
0.794±0.006
564±48
0.394±0.008
6784±246
0.396±0.004
1562±189
0.388±0.006
436±24
0.216±0.003
11±1
0.216±0.003
16±2
0.121±0.002

m = 1500 (4.88%)
CPU
testing
time
RMSE
8±1
4.2±0.1
6±1
0.637±0.008
0.638±0.006
6±1
0.356±0.006 1546±38

-
-

-
-

m = 2000 (3.80%)
CPU
testing
time
RMSE
12±1
4.4±0.1
19±2
0.615±0.008
0.615±0.007
17±2
0.332±0.006 2387±44

-
-

-
-

0.172±0.002 1588±46
23±2
0.172±0.002
27±3
0.117±0.002

0.164±0.002 2658±63
37±2
0.164±0.002
46±2
0.113±0.001

(a) m = 500.

(b) m = 1000.

(c) m = 1500.

(d) m = 2000.

Fig. 3: Convergence of the testing RMSE vs CPU time (sec) of various algorithms on synthetic data. SDPLR and SDPNAL+
are too slow on m = 1500 and 2000, thus are not shown.

TABLE 4: Testing RMSEs and CPU time (sec) on synthetic data with different observation sampling ratios (s). The number
in brackets is the percentage of observed elements.

loss

algorithm

square

(cid:96)1

leaky-MCP

nmAPG
L-BFGS
ADMM((cid:96)1)
SDP-RL((cid:96)1)
SDP-RL(MCP)

s = 1 (1.90%)

s = 2 (3.80%)

s = 4 (7.60%)

s = 8 (15.20%)

testing
RMSE
0.896±0.008
0.896±0.007
0.436±0.008
0.256±0.004
0.168±0.001

CPU
time
17±3
16±1
1638±56
36±3
50±3

testing
RMSE
0.615±0.008
0.615±0.007
0.332±0.006
0.164±0.002
0.113±0.001

CPU
time
19±2
17±2
2387±44
37±2
46±2

testing
RMSE
0.442±0.003
0.443±0.003
0.264±0.005
0.109±0.001
0.077±0.002

CPU
time
21±2
21±3
3765±38
55±5
67±6

testing
RMSE
0.258±0.006
0.256±0.007
0.189±0.003
0.084±0.003
0.053±0.002

CPU
time
25±3
27±4
5582±87
78±6
121±8

TABLE 5: Testing RMSEs and CPU time (sec) on synthetic data with different fractions of outlying entries (o).
o = 0.20

o = 0.025

o = 0.10

o = 0.05

o = 0

loss

square

algorithm

CPU
testing
time
RMSE
17±3
0.005±0.001
nmAPG
0.005±0.001
15±3
L-BFGS
0.009±0.002 2846±123 0.192±0.0032873±83 0.199±0.0032870±62 0.222±0.002 2893±146 0.269±0.0022869±41
ADMM((cid:96)1)
34±2
0.007±0.001
SDP-RL((cid:96)11)
41±3
leaky-MCP SDP-RL(MCP) 0.007±0.001

testing
RMSE
0.228±0.003
0.228±0.003

testing
RMSE
0.422±0.003
0.422±0.003

testing
RMSE
0.309±0.002
0.309±0.002

testing
RMSE
0.590±0.002
0.590±0.002

0.142±0.002
0.119±0.001

0.110±0.002
0.109±0.001

0.113±0.001
0.111±0.001

0.161±0.002
0.134±0.001

CPU
time
16±3
14±1

CPU
time
17±2
14±2

CPU
time
16±1
15±1

CPU
time
10±2
11±1

37±3
44±7

40±4
47±3

44±3
51±3

39±3
47±4

(cid:96)1

Among algorithms based on the (cid:96)1-loss, SDP-RL((cid:96)1) is
the fastest as it exploits data sparsity. SDP-RL(MCP) yields
slightly lower RMSE, but is slightly slower than SDP-RL((cid:96)1).
As SDPLR and SDPNAL+ have comparable accuracies with
ADMM((cid:96)1), but are much slower and even fail to converge
on large-scale problems. Thus, SDPLR and SDPNAL+ will
also be dropped in the subsequent comparisons.

6.1.2 Varying the Number of Observed Entries

We ﬁx the matrix dimension m = 2000, outlier ratio o =
0.05, outlier amplitude σ = 10, and vary the sampling ratio
s in {1, 2, 4, 8}. A larger s means that more elements are
observed. Table 4 shows the testing RMSEs and CPU time.
When s increases, the testing RMSE decreases and CPU time
increases in general, which agrees with intuition.

6.1.3 Varying the Number of Outlying Entries

We vary the fraction of entries o in the sparse noise matrix
S in {0, 0.025, 0.05, 0.1, 0.2}. The other parameters are set
as m = 2000, r = 2, s = 2 and σ = 5. Results are shown
in Table 5. When there is no outlier (o = 0), nmAPG and
L-BFGS perform the best, as they use the square loss which
matches with the Gaussian noise generated. As o increases,
the testing RMSEs of all algorithms increase as expected.
Moreover, using the nonconvex loss leads to more robust
results than both the square loss and (cid:96)1 loss, particularly
when the noise is large.

6.1.4 Varying the Magnitude of Outlying Entries

We vary the magnitude σ of outlying entries
in
{2.5, 5, 10, 20}. The other parameters are ﬁxed at m =

TABLE 6: Testing RMSEs and CPU time (sec) on synthetic data with different maximum outlier amplitudes (σ).

σ = 2.5

σ = 5.0

σ = 10.0

σ = 20.0

10

loss

square

algorithm

CPU
testing
time
RMSE
22±1
0.170±0.001
nmAPG
0.170±0.001
17±3
L-BFGS
0.191±0.002 2868±141 0.199±0.003 2870±62 0.332±0.006 2837±84 0.418±0.008 2906±45
ADMM((cid:96)1)
35±3
0.114±0.001
SDP-RL((cid:96)1)
43±2
leaky-MCP SDP-RL(MCP) 0.113±0.001

testing
RMSE
0.615±0.008
0.615±0.007

testing
RMSE
0.309±0.002
0.309±0.002

testing
RMSE
1.36±0.01
1.36±0.02

0.164±0.002
0.113±0.001

0.183±0.002
0.112±0.001

0.113±0.001
0.111±0.001

CPU
time
19±2
17±2

CPU
time
16±1
15±3

CPU
time
16±1
15±1

37±2
46±2

40±4
47±3

49±7
60±6

(cid:96)1

2000, r = 2, s = 2 and o = 0.05. Results are shown in Ta-
ble 6. As σ increases, the testing RMSEs of most algorithms
also increase (as in Section 6.1.3). The only exception is SDP-
RL(MCP), whose loss remains almost unchanged. This again
shows that SDP-RL(MCP) is more robust.

6.1.5 Varying Tolerance for Subproblem

We experiment with the termination criterion of the ADMM
solver (in Algorithms 1 and 3). We vary b0 in Remark 1 in
{1.25, 1.5, 2.0}, and c0 = R(X0) so that the inexactness
scales with the objective value. The other parameters are
ﬁxed at m = 2000, r = 5, o = 0.25, s = 2 and σ = 10.

Figure 4 shows convergence of the relative objective
R(Xk)/R(X0) vs the number of iterations in Algorithm 2
(resp. Algorithm 4) for SDP-RL((cid:96)1) (resp. SDP-RL(MCP)).
Recall that each iteration of Algorithm 2 (resp. Algorithm 4)
makes one call to Algorithm 1 (resp. Algorithm 3). As can be
seen, a larger b0 (smaller tolerance) leads to fewer iterations
of Algorithm 2 and Algorithm 4. However, solving the
ADMM to such a higher precision means more time to solve
the surrogate (Table 8). Figure 5 shows convergence w.r.t. the
total CPU time. As can be seen, b0 = 1.5 is a good empirical
choice, and we will use this in the sequel.

(a) SDP-RL((cid:96)1).

(b) SDP-RL(MCP).

Fig. 5: Convergence of relative objective vs CPU time (sec)
at different tolerance on inexactness.

to similar testing RMSEs. Some initializations lead to faster
convergence, but Gaussian initialization is not always better
than zero initialization.

(a) Algorithm 2.

(b) Algorithm 4.

Fig. 6: Convergence of testing RMSE versus different initial-
izations for the proposed algorithms.

(a) SDP-RL((cid:96)1).

(b) SDP-RL(MCP).

Fig. 4: Convergence of relative objective vs number of
iterations in Algorithms 2 and 4 at different tolerance on
inexactness.

6.1.6 Effect of Different Initializations

In this experiment, we study the following two initializa-
tions of X: (i) zero initialization (i.e., X1 = 0) as shown
in Algorithms 2 and 4; and (ii) Gaussian initialization, in
which elements of X1 are independently sampled from
the standard normal distribution. We randomly generate 5
Gaussian initializations. The other parameters are ﬁxed at
m = 2000, r = 5, o = 0.25, s = 2 and σ = 10.

Figure 6(a) (resp. Figure 6(b)) shows convergence of
testing RMSE versus the number of iterations in Algorithm 2
for SDP-RL((cid:96)1) (resp. number of iterations in Algorithm 4
for SDP-RL(MCP)). As can be seen, all initializations lead

6.2 Robust NPKL

In this section, experiment is performed on the adult data
set “a1a”, which has been commonly used in the NPKL
literature [19]. It contains ¯n = 1605 123-dimensional sam-
ples. Following [70], we randomly sample 6¯n pairs and
construct set T = {Tij}, where Tij = 1 if samples i and
j have the same label, and Tij = 0 otherwise. We then
randomly sample 60% of the pairs in T for training, 20%
for validation and hyper-parameter tuning, and the rest for
testing. The numbers of must-link and cannot-link pairs in
the training/validation/testing sets are shown in Table 9.
The Laplacian L in (29) is constructed from a graph G.
Each node in G corresponds to a training sample, and is
connected to its two nearest training samples based on the
distance in the feature space.

To test the robustness of NPKL algorithms, we ﬂip
some must-link constraints in the training set to cannot-link

TABLE 7: Testing RMSEs and CPU time (sec) in the robust NPKL experiment.
5% ﬂippd labels

10% ﬂipped labels

loss

algorithm

square

(cid:96)1

leaky-MCP

SimpleNPKL
nmAPG
L-BFGS
ADMM((cid:96)1)
SDP-RL((cid:96)1)
SDP-RL(MCP)

testing RMSE
0.54±0.01
0.31±0.01
0.31±0.01
0.23±0.01
0.21±0.01
0.19±0.02

CPU time
407±24
7±2
4±1
775±24
55±33
67±27

testing RMSE
0.60±0.01
0.35±0.01
0.35±0.01
0.29±0.01
0.28±0.01
0.26±0.01

CPU time
419±27
8±1
4±1
784±19
72±36
88±27

11

TABLE 8: Average per-iteration CPU time (sec) of SDP-RL.

SDP-RL((cid:96)1)
SDP-RL(MCP)

b0 = 1.25
0.15
0.16

b0 = 1.5
0.29
0.28

b0 = 2.0
0.47
0.47

TABLE 9: Numbers of must-link and cannot-link pairs in the
robust NPKL experiment.

training
validation
testing

must-link
3637
1210
1220

cannot-link
2141
716
706

constraints, and vice versa. This mimics the label ﬂipping
attacks in real-world applications [39]. The total number of
constraints ﬂipped is varied in {5%, 10%}.

Besides comparing with the previous methods based on
the square loss, (cid:96)1 loss and leaky-MCP loss, we also compare
with SimpleNPKL [19], which is based on the square loss but
does not use the low-rank factorization. As for the rank r of
the initial solution X, we follow [21] and set its value to be
2 ≤ |T |. For performance eval-
the largest r satisfying
uation, we follow [32], [41] and use the (i) testing root mean
square error, RMSE = ((cid:80)
(Zij − Tij)2/|Ttest|)1/2,
where ¯X is the output of the algorithm and Ttest is the testing
set, and (ii) CPU time.

(i,j)∈Ttest

r(r+1)

(a) 5% ﬂipped labels.

(b) 10% ﬂipped labels.

Fig. 7: Convergence of testing RMSE vs CPU time (sec) in
the robust NPKL experiment.

Table 7 and Figure 7 show performance of the compared
algorithms. As can be seen, algorithms based on the (cid:96)1 and
non-convex losses have lower testing RMSE’s than those
based on the square loss, with SDP-RL(MCP) being the best.
Moreover, SDP-RL((cid:96)1) and SDP-RL(MCP) are faster than
ADMM((cid:96)1).

6.3 Robust CMVU

In this section, we perform experiment on robust CMVU
using the commonly-used USPS data set, which contains
2007 256-dimensional samples. As in [17], the set Ω in (31)
is constructed by using the nearest 1% neighbors of each

sample, leading to |Ω| = 401, 400. The squared Euclidean
distance tij for each (i, j) ∈ Ω is computed from the clean
data set (before adding noise). We randomly sample 60% of
the pairs from Ω for training, 20% for validation and hyper-
parameter tuning, and the rest for testing. As for the rank
r of the initial solution X, we follow [21] and set its value
to the largest r satisfying r(r + 1)/2 ≤ |Ω|. The tradeoff
parameter γ in (31) is ﬁxed at 0.01.

(i,j)∈Ωtest

(cid:0)Zii + Zjj − 2Zij − tij

For performance evaluation, we use (i) the testing RMSE:
((cid:80)
2 , where Ωtest is
the testing portion of Ω, and (ii) CPU time. Since NPKL and
CMVU can be solved using the same algorithm, we use the
same baselines as in Section 6.2, i.e. SimpleCMVU [19].

/|Ωtest|) 1

(cid:1)2

6.3.1 Small Gaussian Noise
Here, we add Gaussian noise from N (0, 0.01·Var(x)) to each
feature in the training set, where Var(x) is a vector contain-
ing the variance of each feature. Table 10 and Figure 8 show
the results. The observations are almost the same as that
in Section 6.2. SDP-RL(MCP) has the lowest testing RMSE,
while ADMM((cid:96)1) and SDP-RL((cid:96)1) are better than nmAPG
and L-BFGS. SDP-RL((cid:96)1) is also much more efﬁcient than
ADMM((cid:96)1).

6.3.2 Large Outliers

In this experiment, we add large outliers which may appear
in the data [32], [41]. First, we randomly sample some
samples (5% and 10%) from the training set. For each
selected sample xi, we add random noise from N (0, 5˜x),
where ˜x is a vector containing the largest absolute feature
value for that dimension. Table 10 and Figure 8 show the
performance against outliers. Again, SDP-RL(MCP) has the
lowest testing RMSE among the algorithms. Moroever, SDP-
RL((cid:96)1) is much faster than ADMM((cid:96)1).

6.4 Sparse PCA

In this section, we perform sparse PCA experiment on the
colon cancer data set [5], which contains 2000 micro-array
readings from 62 subjects. We set λ = 0, γ = 10 in (33), and
try different embedding dimensions r = {50, 100, 200}. As
there are no missing data in sparse PCA, data sparsity is not
utilized for SDP-RL. We also compare with two state-of-the-
art sparse PCA methods:
1) nonlinear IPM [71], which obtains the sparse princi-
pal components from the following inverse eigenvalue
, where α is a hyper-
problem: minx∈Rn
parameter controlling the sparsity of x. When α = 0, it
reduces to original PCA.

(1−α)(cid:107)x(cid:107)2+α(cid:107)x(cid:107)1
x(cid:62)Σx

2) SpaSM [72], which solves the sparse PCA problem in (33)

with the SPCA algorithm in [15].

TABLE 10: Testing RMSEs and CPU time (sec) in the robust CMVU experiment.

loss

square

(cid:96)1

leaky-MCP

algorithm

SimpleCMVU
nmAPG
L-BFGS
ADMM((cid:96)1)
SDP-RL((cid:96)1)
SDP-RL(MCP)

small deviations

5% large outliers

10% large outliers

testing RMSE
0.48±0.02
0.34±0.01
0.34±0.01
0.30±0.03
0.29±0.02
0.25±0.02

CPU time
837±19
342±5
424±7
3090±27
165±23
206±38

testing RMSE
0.77±0.03
0.65±0.04
0.46±0.01
0.34±0.03
0.32±0.03
0.29±0.02

CPU time
1675±49
691±15
645±15
2944±23
113±50
156±53

testing RMSE
0.97±0.03
0.76±0.01
0.58±0.01
0.35±0.03
0.33±0.02
0.30±0.03

CPU time
1263±33
280±2
574±2
3124±29
113±33
162±40

12

(a) small deviations.

(b) 5% large outliers.

(c) 10% large outliers.

Fig. 8: Convergence of testing RMSE vs CPU time (sec) in the robust CMVU experiment.

(a) r = 50.

(b) r = 100.

(c) r = 200.

Fig. 9: Percentage of explained variance vs CPU time (sec) for the various algorithms on the sparse PCA problem.

For performance evaluation, as in [5], [13], we use the (i)
CPU time, (ii) sparsity of XX (cid:62) (i.e., ratio of zero elements),
and (iii) explained variance (i.e., tr(ZΣ) in (32)). Experi-
ments are repeated ﬁve times.

6.4.1 Results

Results are shown in Table 11. As can been seen, due to
the use of the non-convex loss, SDP-RL(MCP) produces the
best solution compared with the other approaches. Besides,
both SDP-RL((cid:96)1) and SDP-RL(MCP) are much faster than
ADMM((cid:96)1), SpaSM and nonlinear IPM. Figure 9 shows con-
vergence of the explained variance with CPU time. As can
be seen, SDP-RL((cid:96)1) and SDP-RL(MCP) also converge much
faster than the other approaches.

6.4.2 Effect of Different Initializations

In this experiment, we study the following two initializa-
tions of X: (i) zero initialization as in Algorithm 2 and 4;
and (ii) standard PCA. Results are shown in Table 12. As
can be seen, different initializations have little impact on the
ﬁnal performance, but initialization by PCA can have faster
convergence. This also agrees with the common practice of
using PCA as initialization for sparse PCA [15].

As can be seen from experiments in both Section 6.1.6
and here, the choice of initialization is application-speciﬁc.
Empirically, different initializations have little impact on the

TABLE 11: Performance of various sparse PCA algorithms
on the colon cancer data set.

r

50

100

200

algorithm

CPU time (sec)

sparsity

nonlinear IPM
SpaSM
ADMM((cid:96)1)
SDP-RL((cid:96)1)
SDP-RL(MCP)
nonlinear IPM
SpaSM
ADMM((cid:96)1)
SDP-RL((cid:96)1)
SDP-RL(MCP)
nonlinear IPM
SpaSM
ADMM((cid:96)1)
SDP-RL((cid:96)1)
SDP-RL(MCP)

1.06±0.12
0.64±0.03
0.55±0.06
0.21±0.02
0.23±0.02
6.18±0.36
3.49±0.23
3.12±0.25
0.75±0.07
0.86±0.12
244.36±20.68
120.94±8.26
118.28±12.25
7.42±0.23
7.68±0.35

0.73
0.63
0.76
0.76
0.76
0.75
0.67
0.79
0.79
0.79
0.79
0.75
0.82
0.82
0.82

explained
variance
8.98
8.92
9.23
9.23
9.58
21.83
21.87
21.86
22.67
23.22
60.18
62.74
64.24
66.44
67.92

ﬁnal performance of the proposed algorithm, but a better
initialization can lead to faster convergence.

6.5 Symmetric NMF

In this section, we perform experiments on symmetric non-
negative matrix factorization (SNMF). Data generation is
similar to that in Section 6.1, with the ground-truth matrix
M generated as V V (cid:62). In the ﬁrst experiment, V ∈ Rm×5
is synthetic, with m ∈ {1000, 2000}. Each element of V

13

TABLE 12: Effect of different ways to initialize SDP-RL in
the sparse PCA experiment.

REFERENCES

r

50

100

200

loss in
SDP-RL

initialization

(cid:96)1

MCP

(cid:96)1

MCP

(cid:96)1

MCP

zero
PCA
zero
PCA
zero
PCA
zero
PCA
zero
PCA
zero
PCA

CPU time
(sec)
0.21±0.02
0.11±0.01
0.23±0.02
0.11±0.02
0.75±0.07
0.29±0.04
0.86±0.12
0.35±0.07
7.42±0.23
4.14±0.19
7.68±0.35
4.35±0.23

sparsity

0.76
0.75
0.76
0.78
0.79
0.80
0.79
0.79
0.82
0.81
0.82
0.82

explained
variance
9.23
9.26
9.58
9.57
22.67
22.78
23.22
23.24
66.44
66.48
67.92
68.03

is sampled independently from the standard exponential
distribution. We then corrupt M by adding a sparse matrix
S, which models a fraction of o large outliers sampled
uniformly from {0, σ}, to obtain M (cid:48) = M + S. The train-
ing/validation/test set split follows that in Section 6.1. The
second experiment is similar, except that V is constructed
from real-world data set. Speciﬁcally, following [68], we
construct V ∈ R2007×10 as the one-hot label matrix for the
USPS dataset in Section 6.3.

The rank r of the initial X solution is set to the ground-
truth, i.e. 5 for the esynthetic data and 10 for USPS dataset.
The other parameters are set as o = 0.05, s = 2 and σ = 10.
We compare SDP-RL with three commonly-used SNMF
methods [67], [68]: Newton’s method (Newton) [66], regu-
larized eigenvalue decomposition (rEVD) [73], and block-
coordinate descent (BCD) [67]. All three solve (34) (with the
square loss) while the proposed SDP-RL solves problem (35)
(with the (cid:96)1-loss). For performance evaluation, we follow
Section 6.1 and use the testing RMSE and CPU time.

Results are shown in Table 13, and the convergence of
testing RMSE w.r.t. CPU time is shown in Figure 10. Again,
they demonstrate that SDP-RL (using either the (cid:96)1 or MCP
loss) is signiﬁcantly more robust (lower testing RMSE) on
noisy data as compared to methods based on the square loss.
Moreover, SDP-RL((cid:96)1) is more efﬁcient than ADMM((cid:96)1).

7 CONCLUSION
In this paper, we propose a robust and factorized formu-
lation of SDP by replacing the commonly used square loss
with more robust losses ((cid:96)1-loss and non-convex losses). As
the resulting optimization problem is neither convex nor
smooth, existing SDP solvers cannot be applied. We pro-
pose a new solver based on majorization-minimization. By
allowing inexactness in the underlying ADMM subproblem,
the algorithm is much more efﬁcient while still guaranteed
to converge to a critical point. Experiments are performed
on ﬁve applications: matrix completion, kernel learning,
matrix variance unfolding, sparse PCA, and symmetric non-
negative matrix factorization. Empirical results demonstrate
the efﬁciency and robustness over state-of-the-arts SDP
solvers.

8 ACKNOWLEDGMENT
This research was supported in part by the National Natural
Science Foundation of China (No.61663049).

[1] C. Helmberg, F. Rendl, R. Vanderbei, and H. Wolkowicz, “An
interior-point method for semideﬁnite programming,” SIAM Jour-
nal on Optimization, 1996.

[2] L. Vandenberghe and S. Boyd, “Semideﬁnite programming,”

[3]

SIAM Review, vol. 38, no. 1, pp. 49–95, 1996.
S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge
University Press, 2004.

[4] A. Lemon, A. So, and Y. Ye, “Low-rank semideﬁnite programming:
Theory and applications,” Foundations and Trends in Optimization,
vol. 2, no. 1-2, pp. 1–156, 2016.
S. Laue, “A hybrid algorithm for convex semideﬁnite optimiza-
tion,” in International Conference on Machine Learning, 2012, pp.
177–184.

[5]

[6] M. Jaggi, “Revisiting Frank-Wolfe: Projection-free sparse convex
optimization,” in International Conference on Machine Learning,
2013.

[7] M. Laurent and A. Varvitsiotis, “Positive semideﬁnite matrix com-
pletion, universal rigidity and the strong arnold property,” Linear
Algebra and its Applications, vol. 452, pp. 292–317, 2014.

[8] W. E. Bishop and M. Y. Byron, “Deterministic symmetric positive
semideﬁnite matrix completion,” in Neural Information Processing
Systems, 2014, pp. 2762–2770.

[9] A. Bhargava, R. Ganti, and R. Nowak, “Active positive semideﬁ-
nite matrix completion: Algorithms, theory and applications,” in
Artiﬁcial Intelligence and Statistics, 2017, pp. 1349–1357.

[10] V. Singh, L. Mukherjee, J. Peng, and J. Xu, “Ensemble cluster-
ing using semideﬁnite programming with applications,” Machine
Learning, vol. 79, no. 1-2, pp. 177–200, 2010.

[11] B. Kulis, A. C. Surendran, and J. C. Platt, “Fast low-rank semideﬁ-
nite programming for embedding and clustering,” in International
Conference on Artiﬁcial Intelligence and Statistics, 2007, pp. 235–242.
[12] A. Pirinen and B. Ames, “Exact clustering of weighted graphs via
semideﬁnite programming.” Journal of Machine Learning Research,
vol. 20, no. 30, pp. 1–34, 2019.

[13] A. D’aspremont, E. L. Ghaoui, M. I. Jordan, and G. R. G. Lanckriet,
“A direct formulation for sparse PCA using semideﬁnite program-
ming,” SIAM Review, vol. 49, no. 3, pp. 434–48, 2007.

[14] X.-T. Yuan and T. Zhang, “Truncated power method for sparse
eigenvalue problems,” Journal of Machine Learning Research, vol. 14,
no. Apr, pp. 899–925, 2013.

[15] H. Zou and L. Xue, “A selective overview of sparse principal

component analysis,” Proceedings of the IEEE, 2018.

[16] K. Q. Weinberger, F. Sha, and L. Saul, “Learning a kernel matrix
for nonlinear dimensionality reduction,” in International Conference
on Machine Learning, 2004, pp. 839–846.

[17] L. Song, A. Smola, K. Borgwardt, and A. Gretton, “Colored maxi-
mum variance unfolding,” in Neural Information Processing Systems,
2008.

[18] Z. Li, J. Liu, and X. Tang, “Pairwise constraint propagation by
semideﬁnite programming for semi-supervised classiﬁcation,” in
International Conference on Machine Learning, 2008, pp. 576–583.
[19] J. Zhuang, I. Tsang, and S. Hoi, “A family of simple non-
parametric kernel learning algorithms,” Journal of Machine Learning
Research, vol. 12, pp. 1313–1347, 2011.

[20] K.-C. Toh, L. Yang, and D. Sun, “SDPNAL+: a majorized semis-
mooth Newton-CG augmented Lagrangian method for semidef-
inite programming with nonnegative constraints,” Mathematical
Programming Computation, 2015.

[21] S. Burer and R. Monteiro, “A nonlinear programming algorithm
for solving semideﬁnite programs via low-rank factorization,”
Mathematical Programming, vol. 95, pp. 329–357, 2003.

[22] S. Burer and R. D. Monteiro, “Local minima and convergence in
low-rank semideﬁnite programming,” Mathematical Programming,
vol. 103, no. 3, pp. 427–444, 2005.

[23] M. Journe´e, F. Bach, P. Absil, and R. Sepulchre, “Low-rank op-
timization on the cone of positive semideﬁnite matrices,” SIAM
Journal on Optimization, pp. 2327–2351, 2010.

[24] B. Srinadh, K. Anastasios, and S. S., “Dropping convexity for faster
semideﬁnite optimization,” in Conference on Learning Theory, 2016.
[25] Q. Zheng and J. Lafferty, “A convergent gradient descent algo-
rithm for rank minimization and semideﬁnite programming from
random linear measurements,” in Neural Information Processing
Systems, 2015.

TABLE 13: Testing RMSEs and CPU time (sec) in the SNMF experiment.

loss

square

(cid:96)1

algorithm

Newton
rEVD
BCD
ADMM((cid:96)1)
SDP-RL((cid:96)1)

leaky-MCP SDP-RL(MCP)

synthetic (m = 1000)

synthetic (m = 2000)

USPS

testing RMSE
0.783±0.007
0.799±0.005
0.781±0.008
0.433±0.006
0.212±0.004
0.119±0.002

CPU time
9±1
0.5±0.1
1.6±0.5
515±27
12±1
15±3

testing RMSE
0.564±0.003
0.571±0.002
0.565±0.003
0.330±0.005
0.158±0.002
0.112±0.001

CPU time
22±6
1.2±0.2
2.9±0.8
2763±47
35±2
48±4

testing RMSE
0.821±0.006
0.832±0.007
0.823±0.009
0.486±0.007
0.267±0.009
0.194±0.006

CPU time
25±7
2.0±0.4
3.2±0.6
3018±65
39±4
57±8

14

(a) synthetic (m = 1000).

(b) synthetic (m = 2000).

(c) USPS.

Fig. 10: Convergence of testing RMSE vs CPU time (sec) in the SNMF experiment.

[26] N. Boumal, V. Voroninski, and A. S. Bandeira, “The non-convex
burer-monteiro approach works on smooth semideﬁnite pro-
grams,” Advances in Neural Information Processing Systems, pp.
2765–2773, 2016.

[27] ——, “Deterministic guarantees for burer-monteiro factorizations
of smooth semideﬁnite programs,” Communications on Pure and
Applied Mathematics, vol. 73, no. 3, pp. 581–608, 2020.

[28] J. Nocedal and S. Wright, Numerical optimization. Springer Science

& Business Media, 2006.

[29] A. Mnih and R. Salakhutdinov, “Probabilistic matrix factoriza-
tion,” in Neural Information Processing Systems, 2008, pp. 1257–1264.
[30] Y. Zheng, G. Liu, S. Sugimoto, S. Yan, and M. Okutomi, “Practical
low-rank matrix approximation under robust (cid:96)1-norm,” in Com-
puter Vision and Pattern Recognition, 2012, pp. 1410–1417.

[31] T. Hastie, R. Mazumder, J. Lee, and R. Zadeh, “Matrix completion
and low-rank SVD via fast alternating least squares,” Journal of
Machine Learning Research, vol. 16, pp. 3367–3402, 2015.

[32] Z. Lin, C. Xu, and H. Zha, “Robust matrix factorization by ma-
jorization minimization,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, no. 99, 2017.

[33] S. Bhojanapalli, A. Kyrillidis, and S. Sanghavi, “Dropping convex-
ity for faster semi-deﬁnite optimization,” in Conference on Learning
Theory. PMLR, 2016, pp. 530–582.

[34] T. Pumir, S. Jelassi, and N. Boumal, “Smoothed analysis of the
low-rank approach for smooth semideﬁnite programs,” in Neural
Information Processing Systems, 2018.

[35] P. J. Huber, “Robust estimation of a location parameter,” in Break-

throughs in Statistics. Springer, 1992, pp. 492–518.

[36] F. D. L. Torre and M. Black, “A framework for robust subspace
learning,” International Journal of Computer Vision, vol. 54, no. 1,
pp. 117–142, 2003.

[37] E. J. Cand`es, X. Li, Y. Ma, and J. Wright, “Robust principal
component analysis?” Journal of the ACM, vol. 58, no. 3, pp. 1–37,
2011.

[38] O. Dekel, O. Shamir, and L. Xiao, “Learning to classify with

missing and corrupted features,” Machine Learning, 2010.

[39] V. Raykar, S. Yu, L. Zhao, G. Valadez, C. Florin, L. Bogoni, and
L. Moy, “Learning from crowds,” Journal of Machine Learning
Research, 2010.

[40] R. Burke, M. P. O’Mahony, and N. J. Hurley, “Robust collaborative
Springer,

recommendation,” in Recommender systems handbook.
2015, pp. 961–995.

[41] Q. Yao and J. Kwok, “Scalable robust matrix factorization with
nonconvex loss,” in Neural Information Processing Systems, 2018,
pp. 5061–5070.

[42] C. Lu, J. Shi, and J. Jia, “Online robust dictionary learning,” in
IEEE Conference on Computer Vision and Pattern Recognition, 2013,
pp. 415–422.

[43] C. Zhang, “Nearly unbiased variable selection under minimax
concave penalty,” Annals of Statistics, vol. 38, no. 2, pp. 894–942,
2010.

[44] E. Cand`es, M. Wakin, and S. Boyd, “Enhancing sparsity by
reweighted (cid:96)1 minimization,” Journal of Fourier Analysis and Ap-
plications, vol. 14, no. 5-6, pp. 877–905, 2008.

[45] A. Eriksson and A. Van Den Hengel, “Efﬁcient computation of
robust low-rank matrix approximations in the presence of missing
data using the (cid:96)1-norm,” in Computer Vision and Pattern Recogni-
tion, 2010, pp. 771–778.

[46] W. Jiang, F. Nie, and H. Huang, “Robust dictionary learning
with capped (cid:96)1-norm,” in International Joint Conference on Artiﬁcial
Intelligence, 2015.

[47] Q. Yao and J. T. Kwok, “Efﬁcient learning with a family of
nonconvex regularizers by redistributing nonconvexity,” Journal
of Machine Learning Research, vol. 18, no. 1, pp. 6574–6625, 2017.
[48] K. Lange, R. Hunter, and I. Yang, “Optimization transfer using sur-
rogate objective functions,” Journal of Computational and Graphical
Statistics, vol. 9, no. 1, pp. 1–20, 2000.

[49] D. Hunter and K. Lange, “A tutorial on MM algorithms,” American

Statistician, vol. 58, no. 1, pp. 30–37, 2004.

[50] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed
optimization and statistical learning via the alternating direction
method of multipliers,” Foundations and Trends in Machine Learning,
vol. 3, no. 1, pp. 1–122, 2011.

[51] B. He and X. Yuan, “On the o(1/n) convergence rate of the
Douglas-Rachford alternating direction method,” SIAM Journal on
Numerical Analysis, vol. 50, no. 2, pp. 700–709, 2012.

[52] E.-L. Hu and Q. Yao, “Robust learning from noisy side-information
by semideﬁnite programming,” in International Joint Conference on
Artiﬁcial Intelligence, 2019.

[53] J. Mairal, “Optimization with ﬁrst-order surrogate functions,” in
International Conference on Machine Learning, 2013, pp. 783–791.
[54] R. Basri, D. Jacobs, and I. Kemelmacher, “Photometric stereo
with general, unknown lighting,” International Journal of Computer
Vision, vol. 72, no. 3, pp. 239–257, 2007.

[55] E. Kim, M. Lee, C. Choi, N. Kwak, and S. Oh, “Efﬁcient (cid:96)1-norm-
based low-rank matrix approximations for large-scale problems
using alternating rectiﬁed gradient method,” IEEE Transactions on
Neural Networks and Learning Systems, vol. 26, no. 2, pp. 237–251,
2015.

[56] E.-L. Hu, B. Wang, and S.-C. Chen, “BCDNPKL: Scalable non-
parametric kernel learning using block coordinate descent,” in
International Conference on Machine Learning, 2011, pp. 209–216.
[57] H. Li and Z. Lin, “Accelerated proximal gradient methods for non-
convex programming,” in Neural Information Processing Systems,
2015, pp. 379–387.

[58] M. Schmidt, N. L. R., and F. R. Bach, “Convergence rates of inexact

proximal-gradient methods for convex optimization,” in Neural
Information Processing Systems, 2011, pp. 1458–1466.

[59] Q. Yao, J. T. Kwok, F. Gao, W. Chen, and T.-Y. Liu, “Efﬁcient inexact
proximal gradient algorithm for nonconvex problems,” 2017.
[60] T. Zhang, “Analysis of multi-stage convex relaxation for sparse

regularization,” Journal of Machine Learning Research, 2010.

[61] P. Gong, C. Zhang, Z. Lu, J. Huang, and J. Ye, “A general iterative
shrinkage and thresholding algorithm for non-convex regularized
optimization problems,” in International Conference on Machine
Learning, 2013, pp. 37–45.

[62] D. Geman and C. Yang, “Nonlinear image recovery with half-
quadratic regularization,” IEEE Transactions on Image Processing,
vol. 4, no. 7, pp. 932–946, 1995.

[63] J. Trzasko and A. Manduca, “Highly undersampled magnetic res-
onance image reconstruction via homotopic-minimization,” IEEE
Transactions on Medical Imaging, vol. 28, no. 1, pp. 106–121, 2009.

[64] F. H. Clarke, Optimization and nonsmooth analysis.

SIAM, 1990,

vol. 5.

[65] S. Hoi, R. Jin, and M. Lyu, “Learning nonparametric kernel ma-
trices from pairwise constraints,” in International Conference on
Machine Learning, 2007, pp. 361–368.

[66] Z. He, S. Xie, R. Zdunek, G. Zhou, and A. Cichocki, “Symmetric
nonnegative matrix factorization: Algorithms and applications to
probabilistic clustering,” IEEE Transactions on Neural Networks,
vol. 22, no. 12, pp. 2117–2131, Dec 2011.

[67] Q. Shi, H. Sun, S. Lu, M. Hong, and M. Razaviyayn, “Inexact
block coordinate descent methods for symmetric nonnegative ma-
trix factorization,” IEEE Transactions on Signal Processing, vol. 65,
no. 22, pp. 5995–6008, Nov 2017.

[68] D. Kuang, S. Yun, and H. Park, “SymNMF: nonnegative low-rank
approximation of a similarity matrix for graph clustering,” Journal
of Global Optimization, vol. 62, no. 3, pp. 545–574, 2015. [Online].
Available: https://doi.org/10.1007/s10898-014-0247-2

[69] Q. Yao, J. Kwok, T. Wang, and T. Liu, “Large-scale low-rank
matrix learning with nonconvex regularizers,” IEEE Transactions
on Pattern Analysis and Machine Intelligence, 2018.

[70] E.-L. Hu and J. T. Kwok, “Low-rank matrix learning using bicon-
vex surrogate minimization,” IEEE Transactions on Neural Networks
and Learning Systems, vol. 30, no. 11, pp. 3517–3527, 2019.

[71] M. Hein and T. B ¨uhler, “An inverse power method for nonlin-
ear eigenproblems with applications in 1-spectral clustering and
sparse pca,” in Neural Information Processing Systems, 2010, pp. 847–
855.

[72] K. Sj ¨ostrand, L. H. Clemmensen, R. Larsen, G. Einarsson, and B. K.
Ersbøll, “Spasm: A matlab toolbox for sparse statistical modeling,”
Journal of Statistical Software, vol. 84, no. 10, 2018.

[73] K. Huang, N. D. Sidiropoulos, and A. Swami, “Non-negative ma-
trix factorization revisited: Uniqueness and algorithm for symmet-
ric decomposition,” IEEE Transactions on Signal Processing, vol. 62,
no. 1, pp. 211–224, Jan 2014.

IEEE) is currently
Quanming Yao (member,
a senior scientist
in 4Paradigm (Hong Kong)
and an incoming assistant professor (tenure-
track) of Department of Electrical Engineering
Tsinghua University. His research interests are
in machine learning, nonconvex optimization,
and automated machine learning. He obtained
his Ph.D. degree in the Department of Com-
puter Science and Engineering at Hong Kong
University of Science and Technology (HKUST)
in 2018. He has received Wunwen Jun Prize for
Excellence Youth of Artiﬁcial Intelligence (issued by CAAI, 2019), the 1st
runner up of Ph.D. Research Excellence Award (School of Engineering,
HKUST, 2018-2019) and Google Fellowship (machine learning, 2016).

15

Hansi Yang joins Tsinghua University in 2017,
and is an undergraduate student with Depart-
ment of Electronic Engineering. He is currently
an intern in machine learning research group
of 4Paradigm Inc supervised by Dr. Yao. His
research interests are in machine learning and
automated machine learning.

En-Liang Hu (member, IEEE) En-Liang Hu re-
ceived his Ph.D. degree in computer science
from the Nanjing University of Aeronautics and
Astronautics, Nanjing, China, in 2010. He is cur-
rently a Professor with the Department of Mathe-
matics, Yunnan Normal University, Cheng Gong
Campus, Kunming. His current research inter-
ests include machine learning, data mining, and
optimization.

James T. Kwok (Fellow,
IEEE) received the
Ph.D. degree in computer science from The
Hong Kong University of Science and Tech-
nology in 1996. He is a Professor with the
Department of Computer Science and Engi-
neering, Hong Kong University of Science and
Technology. His research interests include ma-
chine learning, deep learning, and artiﬁcial intel-
ligence. He received the IEEE Outstanding 2004
Paper Award and the Second Class Award in
Natural Sciences by the Ministry of Education,
China, in 2008. He is serving as an Associate Editor for the IEEE Trans-
actions on Neural Networks and Learning Systems, Neural Networks,
Neurocomputing, Artiﬁcial Intelligence Journal, International Journal of
Data Science and Analytics, Editorial Board Member of Machine Learn-
ing, Board Member, and Vice President for Publications of the Asia
Paciﬁc Neural Network Society. He also served/is serving as Senior
Area Chairs / Area Chairs of major machine learning / AI conferences
including NIPS, ICML, ICLR, IJCAI, AAAI and ECML.

APPENDIX A
PROOF
A.1 Lemma 1

Proof. First, we have

(cid:0)(Xk + ˜X)(cid:62)Qτ (Xk + ˜X)(cid:1) − tτ |
|tr
= |tr(2 ˜X (cid:62)Qτ Xk + X (cid:62)
≤ |tr(2 ˜X (cid:62)Qτ Xk + X (cid:62)

k Qτ Xk) − tτ + tr( ˜X (cid:62)Qτ ˜X)|,
k Qτ Xk) − tτ | + |tr( ˜X (cid:62)Qτ ˜X)|. (36)

Then, let C = A + λ

γ I, we have,

γ
2
=

(cid:0)(Xk + ˜X)(cid:62)A(Xk + ˜X)(cid:1) +
tr
γ
2
Recall that

C ˜X + (Xk + 2 ˜X)(cid:62)CXk

(cid:0) ˜X

tr

(cid:62)

λ
2

(cid:0)(Xk + ˜X)(cid:62)(Xk + ˜X)(cid:1)
tr
(cid:1).

(37)

(cid:88)m

R( ˜X + Xk) =
γ
2

τ =1
tr(( ˜X + Xk)(cid:62)A( ˜X + Xk)) +

|tr(( ˜X + Xk)(cid:62)Qτ ( ˜X + Xk)) − tτ |
λ
2

(cid:107) ˜X + Xk(cid:107)2
F .

+

Combining (36), (37) and the deﬁnition of R( ˜X +Xk) above,
thus for any ˜X ∈ Rn×r we have
(cid:88)m
R( ˜X + Xk) ≤

|tr(2 ˜X (cid:62)Qτ Xk + X (cid:62)
(cid:0) ˜X

k Qτ Xk) − tτ |
C ˜X + (Xk + 2 ˜X)(cid:62)CXk

(cid:88)m

τ =1

tr

(cid:62)

+

|tr( ˜X (cid:62)Qτ ˜X)| +

γ
2
Thus, we obtain the Lemma.

τ =1

2 (Qτ + Q(cid:62)

A.2 Lemma 2
τ ) ˜X)
Proof. Since we have tr( ˜X (cid:62)Qτ ˜X) = tr( ˜X (cid:62) 1
and 1
τ ) is always symmetric for any Qτ ∈ S+,
we only need to prove that: let ¯S = S+ − S− where
S ∈ Sn is any symmetric matrix, then |tr( ˜X (cid:62)S ˜X)| ≤
tr( ˜X (cid:62) ¯S ˜X) holds for every ˜X ∈ Rn×r. Let λmax =
(cid:80)n
i=1 min(λi, 0)viv(cid:62)
i .

and λmin = (cid:80)n

i=1 max(λi, 0)viv(cid:62)
i

2 (Qτ + Q(cid:62)

Thus, we have
|tr( ˜X (cid:62)S ˜X)| = |tr( ˜X (cid:62)(

(cid:88)n

i=1

λiviv(cid:62)

i ) ˜X)|,

≤ |tr( ˜X (cid:62)λmax ˜X)| + |tr( ˜X (cid:62)λmin ˜X)|,
= tr( ˜X (cid:62)λmax ˜X) − tr( ˜X (cid:62)λmin ˜X),
= tr( ˜X (cid:62)S+ ˜X) − tr( ˜X (cid:62)S− ˜X) = tr( ˜X (cid:62) ¯S ˜X).

Thus, we obtain the Lemma.

A.3 Proposition 2

Proof. Combining Lemma 1 and 2 we will have,
R( ˜X + Xk) ≤

(cid:88)m

(cid:62)

tr

τ =1

|tr( ˜X (cid:62)Qτ ˜X)| +

|tr(2 ˜X (cid:62)Qτ Xk + X (cid:62)
(cid:0) ˜X
1
(tr(X (cid:62)
2
(cid:0) ˜X

γ
2
|tr( ˜X (cid:62)Qτ Xk) +
γ
2

tr( ˜X (cid:62) ¯Qτ ˜X) +

tr

(cid:62)

(cid:88)m

+

τ =1
(cid:88)m

τ =1

≤ 2

+

(cid:88)m

τ =1

k Qτ Xk) − tτ )|

k Qτ Xk) − tτ |
C ˜X + (Xk + 2 ˜X)(cid:62)CXk

C ˜X + (Xk + 2 ˜X)(cid:62)CXk

(cid:1),

Then, let B = Q + 1
1
2 (tr(Xk

(cid:62)Qτ Xk) − tτ ), and ck = γ

2 (λI + γA+), Q = (cid:80)m

2 tr(Xk

τ =1
(cid:62)(A + λ

¯Qτ , (bk)τ =
γ I)Xk),

R( ˜X + Xk) ≤2

(cid:88)m

|tr( ˜X (cid:62)Qτ Xk) + (bk)τ |
+ tr( ˜X (cid:62)(B ˜X + γCXk)) + ck,

τ =1

then R( ˜X + Xk) ≤ Hk( ˜X; Xk) where
Hk( ˜X; Xk) =tr( ˜X (cid:62)(B ˜X + γCXk))

+ 2

(cid:88)m

τ =1

|tr( ˜X (cid:62)Qτ Xk) + (bk)τ | + ck,

16

and the equality holds iff ˜X = 0.

A.4 Proposition 3

Proof. Following its deﬁnition, denote the dual problem to
be max{˜ντ } Dk({˜ντ }), which is deﬁned as:

Dk({˜ντ }) = min
˜X,eτ

tr( ˜X (cid:62)(B ˜X + γCXk)) + 2

(cid:88)m

τ =1

|eτ | + ck,

+

(cid:88)m

τ =1

˜ντ (tr( ˜X (cid:62)Qτ Xk) + (bk)τ − eτ ).

The problem above can be solved by minimizing w.r.t. ˜X
and eτ separately. We ﬁrst consider minimizing w.r.t. eτ ,
which is given by mineτ 2 (cid:80)m
τ =1 ˜ντ eτ and it
follows easily that when |˜ντ | > 2, the problem above
achieves − inf. So it requires |˜ντ | ≤ 2 and the minimized
point is 0. We then turned on to consider minimizing w.r.t.
˜X, which is (we omit the ck term for simplicity, as it has no
inﬂuence to the optimization problem)

τ =1|eτ | − (cid:80)m

(cid:1).

min
˜X

tr( ˜X (cid:62)(B ˜X +γCXk))+

m
(cid:88)

τ =1

˜ντ (tr( ˜X (cid:62)Qτ Xk)+(bk)τ ),

= min
˜X

tr( ˜X (cid:62)(B ˜X +(γC +

m
(cid:88)

˜ντ Qτ )Xk))+

m
(cid:88)

˜ντ (bk)τ . (38)

τ =1

τ =1

Let D = γC + (cid:80)m

τ =1 ˜ντ Qτ , (38) is equivalent to
1
4

tr(D(cid:62)B−1D).

˜ντ (bk)τ −

τ =1

(cid:88)m

min
˜X

Thus, the optimal of (38) is

˜X ∗ = −

1
2
Then, the dual problem is max|˜ντ |≤2 Dk({˜ντ }) where Dk is
deﬁned in Proposition 3.

B−1D.

A.5 Lemma 3

Proof. Speciﬁcally, since our problem in (16) is convex (con-
vex objective and linear constraints), we only need to check
the Slater’s condition, i.e. there exists a strictly feasible point
for this problem, in order to prove its strong duality. And the
proof is trivial as we have ˜X ∈ Rn×r, {eτ ∈ R}, therefore
the constraints eτ = tr( ˜X (cid:62)Qτ Xk) + (bk)τ can always be
satisﬁed by choosing an appropriate eτ .

(cid:1),

A.6 Theorem 1

We ﬁrst introduce the following Lemma 5 and 6. When a
function f has multiple input parameters, ∂kf means taking
subdifferential to its kth parameter.

Lemma 5. There exists a positive constant α > 0, such that
1) Hk( ˜X1; Xk) ≥ Hk( ˜X2; Xk) + α

2 (cid:107) ˜X1 − ˜X2(cid:107)2

F holds for

any ˜X1, ˜X2; and

2) R(Xk) − R(Xk+1) ≥ α

2 (cid:107)Xk+1 − Xk(cid:107)2 − (cid:15)k.

Proof. Part 1). Recall from (14) in Proposition 2 that Hk is
deﬁned as

Hk( ˜X; Xk) ≡ tr( ˜X (cid:62)(B ˜X + γCXk))
(cid:88)m

+ 2

τ =1

|tr( ˜X (cid:62)Qτ Xk) + (bk)τ | + ck.

(39)

Thus, to show Part 1) holds, we only need to show that the
smallest eigenvalue of B in the quadratic term, i.e., the ﬁrst
term in Hk, is positive. This can be easily seen from the
deﬁnition of B, i.e., B = (cid:80)m
2 (λI + γA+), since
τ =1
A+, ¯Qτ are PSD from its deﬁnition and Lemma 2, I is the
identity matrix, and λ is required to be positive.

¯Qτ + 1

17

2 (tr(Xk

γ I and (bk)τ = 1

Since C = A + λ
(cid:62)Qτ Xk) − tτ )
(deﬁned in Proposition 2), we can deduce that the Lemma
holds by comparing (45) and (46).
Part 2). Recall that ˜Xt = Xk+1 − Xk is deﬁned at step 3 of
Algorithm 2. Using δk’s deﬁnition in (21), we have
δk( ˜Xt, {(˜ντ )t}) = Hk( ˜Xt; Xk) − Dk({ντ }t),

≥ Hk( ˜Xt; Xk) − Hk( ˜X ∗; Xk) ≥ 0.

Since Hk is a continuous function and

limt→∞ δk( ˜Xt, {(˜ντ )t}) = 0,

Part 2). From Proposition 2, we know

we have

R(Xk) = Hk(0, Xk),

R(Xk + ˜X ∗) ≤ Hk( ˜X ∗, Xk).

(40)

(41)

limt→∞ Hk( ˜Xt; Xk) − Hk( ˜X ∗; Xk) = 0,

which means 0 ∈ limt→∞ ∂2Hk( ˜Xt, Xk).

Recall that ˜X ∗ is approximated by ˜Xt in step 3 of Algo-
rithm 2. Using (40) and (41), we have

R(Xk) − R(Xk + ˜X ∗) ≥ Hk(0, Xk) − Hk( ˜X ∗, Xk),
= (cid:2)Hk(0, Xk) − Hk( ˜Xt, Xk)(cid:3)

+ (cid:2)Hk( ˜Xt, Xk) − Hk( ˜X ∗, Xk)(cid:3).

Using δk’s deﬁnition in (21), we have

(cid:15)k ≥ δk( ˜Xt, {(˜ντ )t}) = Hk( ˜Xt; Xk) − Dk({ντ }t),

≥ Hk( ˜Xt; Xk) − Hk( ˜X ∗; Xk).

Thus,

−(cid:15)k ≤ Hk( ˜Xt, Xk) − Hk( ˜X ∗, Xk) ≤ 0.

(43)

From part 1), we also have

Hk(0, Xk) − Hk( ˜X ∗, Xk)
≥

(cid:107)Xk+1 − Xk(cid:107)2 =

α
2

α
2

(cid:107) ˜X ∗(cid:107)2.

(44)

Finally, combining (42)-(44), we then obtain

R(Xk) − R(Xk+1) ≥

α
2

(cid:107)Xk+1 − Xk(cid:107)2 − (cid:15)k.

Thus, part 2) holds.

Lemma 6.
2) 0 ∈ limt→∞ ∂2Hk( ˜Xt, Xk).

1) ∂2Hk(0, Xk) = ∂R(Xk); and

Proof. Part 1). Recall the deﬁnition of R(X) in (13), i.e.,

(cid:88)m

R( ˜X + Xk) =
γ
2

τ =1
tr(( ˜X + Xk)(cid:62)A( ˜X + Xk)) +

|tr(( ˜X + Xk)(cid:62)Qτ ( ˜X + Xk)) − tτ |
λ
2

(cid:107) ˜X + Xk(cid:107)2
F .

+

Thus,

∂R(Xk) = γAXk + λXk+

+

(cid:88)m

τ =1

sign(tr(X (cid:62)

k Qτ Xk) − tτ )

1
2

(Qτ + Q(cid:62)

τ )Xk.

(45)

Then, recall the deﬁnition of Hk in (39), we have

∂2Hk(0, Xk) = γCXk

+2

(cid:88)m

τ =1

sign

(cid:0)(bk)τ

(cid:1) 1
2

(Qτ + Q(cid:62)

τ )Xk.

(46)

Proof. (of Theorem 1) Conclusion (i). From part 2) in
Lemma 5, we have
α
2

(cid:107)Xk+1 − Xk(cid:107)2 ≤ R(Xk) − R(Xk+1) + (cid:15)k.

Thus,

(42)

(cid:88)K

k=1

α
2

(cid:107)Xk+1 − Xk(cid:107)2
F
(cid:88)K

≤

R(Xk) − R(Xk+1) + (cid:15)k

k=1

≤ R(X1) − R(XK+1) +
(cid:88)∞

≤ R(X1) − inf R +

(cid:15)k

(47)

k=1
(cid:15)k.

k=1

(cid:88)K

From Assumption 2, we know that the last term in (47), i.e.,
(cid:80)∞
k=1 (cid:15)k, is ﬁnite. Together with Assumption 1, we have

limk→∞(cid:107)Xk+1 − Xk(cid:107)2

F = 0,
and 0 ≤ limk→∞ (cid:107)Xk(cid:107)2

F < ∞,

(48)

which means that the sequence {Xk} is bounded and has at
least one limit points.

Conclusion (ii). From Part 1)
in Lemma 6, we have
∂2Hk(0, Xk) = ∂R(Xk). Then, denote the limit point of
sequence {Xk} as X∗, and let {Xkj } be a sub-sequence of
{Xk} such that

X∗ = limkj→∞ Xkj .

(49)

Thus, to prove the limit point X∗ is also a critical point for
R(X), we only need to show

0 ∈ limkj→∞ ∂2Hkj (0, X∗).

(50)

Using Part 2) in Lemma 6, we should have

Thus, denote limt→∞ ˜X kj

∗ , we only need to prove

t

0 ∈ limt→∞ ∂2Hkj ( ˜X kj
= ∂2Hkj (limt→∞ ˜X kj
t = ˜X kj
limkj→∞ ˜X kj

∗ = 0.

t

, Xkj ),
, Xkj ).

(51)

(52)

Since (cid:80)∞
which implies that

k=1 (cid:15)k is ﬁnite, we must have limkj→∞ (cid:15)kj = 0,

limkj→∞(Xkj +1 − Xkj − ˜X kj

∗ ) = 0.

(53)

Then from (48), we have

limkj→∞ Xkj +1 − Xkj = 0,

(54)

Then, (52) follows easily from (53) and (54). Finally, (50)
can be obtained by combining (51) and (52). Thus, any limit
point of {Xk} is a critical point of R.

A.7 Lemma 4
Proof. Since φ(·) is concave on (0, ∞), we have φ(y) ≤
φ(x) + φ(cid:48)(x)(y − x) for any x, y ∈ (0, ∞). That means for
any α, β ∈ R, φ(|β|) ≤ φ(|α|) + φ(cid:48)(|α|)(|β| − |α|). Then, we
consider our objective:

R(Xk + (cid:101)X) =

(cid:88)m

τ =1

φ(|tr((Xk + (cid:101)X)(cid:62)Qτ (Xk + (cid:101)X)) − tτ |),

γ
2
(cid:88)m

+

=

+

tr((Xk + (cid:101)X)(cid:62)A(Xk + (cid:101)X)) +

λ
2

(cid:107)Xk + (cid:101)X(cid:107)2
F ,

φ(|tr((Xk + (cid:101)X)(cid:62)Qτ (Xk + (cid:101)X)) − tτ |),

γ
2

k CXk) +

τ =1
γ
tr(X (cid:62)
2
γ I. Denote (qk)τ = φ(cid:48)(cid:0)|tr(X (cid:62)

tr((2Xk + ˜X)(cid:62)C ˜X),

k Qτ Xk) − tτ |(cid:1)

,

where C = A + λ
we will have

φ(|tr((Xk + (cid:101)X)(cid:62)Qτ (Xk + (cid:101)X)) − tτ |)
≤ (qk)τ |tr((Xk + (cid:101)X)(cid:62)Qτ (Xk + (cid:101)X)) − tτ |
+ (cid:0)φ(|tr(X (cid:62)
k Qτ Xk) − tτ |) − (qk)τ |tr(X (cid:62)

k Qτ Xk) − tτ |(cid:1)

And it follows easily that:

|tr((Xk + (cid:101)X)(cid:62)Qτ (Xk + (cid:101)X)) − tτ |
≤ |tr((2 ˜X + Xk)(cid:62)Qτ Xk) − tτ | + |tr( ˜X (cid:62)Qτ ˜X)|

Denote

˙ck =

(cid:88)m

(cid:0)φ(|tr(X (cid:62)
− tτ |) − (qk)τ |tr(X (cid:62)

τ =1

k Qτ Xk)
k Qτ Xk) − tτ |(cid:1) +

λ
γ

I,
C = A +
k Qτ Xk) − tτ |(cid:1).
(qk)τ = φ(cid:48)(cid:0)|tr(X (cid:62)

γ
2

tr(X (cid:62)

k CXk),

Thus, we have
˙R(Xk + ˜X) ≤

+

+

γ
tr((2Xk + ˜X)(cid:62)C ˜X) + ˙ck
2
(cid:88)m

(qk)τ |tr( ˜X (cid:62)Qτ ˜X)|
(qk)τ |tr((2 ˜X + Xk)(cid:62)Qτ Xk) − tτ |.

τ =1

(cid:88)m

τ =1

for any ˜X ∈ Rn×r.

A.8 Proposition 4
Proof. Since φ(·) is an increasing function on (0, ∞), we
˙Qτ = (qk)τ Qτ and (˙bk)τ =
have (qk)τ > 0. Let
(cid:1)
1
tr(X (cid:62)
k Qτ Xk) − tτ
2 (qk)τ
, we will have:
˙R(Xk + ˜X) ≤

(cid:0)

γ
2

tr((2Xk + ˜X)(cid:62)C ˜X) + ˙ck
(cid:88)m

|tr( ˜X (cid:62) ˙Qτ ˜X)| + 2

|tr( ˜X (cid:62) ˙Qτ Xk) + (˙bk)τ |.

+

(cid:88)m

τ =1

τ =1

18

tr( ˜X (cid:62) ˜Qτ ˜X). Let Q = (cid:80)m
will have:

τ =1

˜Qτ + 1

2 (λI + γA+) and we

˙R(Xk + ˜X) ≤tr( ˜X (cid:62)(Q ˜X + γCXk))

+ 2

(cid:88)m

τ =1

|tr( ˜X (cid:62) ˙Qτ Xk) + (˙bk)τ | + ˙ck.

Thus ˙R( ˜X + Xk) ≤ ˙Hk( ˜X; Xk) where
˙Hk( ˜X; Xk) =tr( ˜X (cid:62)(Q ˜X + γCXk))

+ 2

(cid:88)m

τ =1

|tr( ˜X (cid:62) ˙Qτ Xk) + (˙bk)τ | + ˙ck.

and the equality holds iff ˜X = 0.

A.9 Theorem 2

The proof here is similar to that of Theorem 1. We ﬁrst
introduce the following Lemma 7 and 8.

Lemma 7. There exists a positive constant α > 0, such that
1)

2 (cid:107) ˜X1 − ˜X2(cid:107)2 holds for any

˙Hk( ˜X1, Xk) − ˙Hk( ˜X2, Xk) ≥ α
˜X1, ˜X2; and
˙R(Xk) − ˙R(Xk+1) ≥ α

2)

2 (cid:107)Xk+1 − Xk(cid:107)2 − (cid:15)k.

Proof. Part 1). Recall from (14) in Proposition 4 that
deﬁned as
˙Hk( ˜X; Xk) ≡ tr( ˜X (cid:62)(B ˜X + γCXk))
+2

|tr( ˜X (cid:62) ˙Qτ Xk) + (˙bk)τ | + ˙ck,

(cid:88)m

˙Hk is

τ =1

Thus, to show the Lemma holds, we only need to show that
the smallest eigenvalue of B in the quadratic term, i.e., the
ﬁrst term in ˙Hk, is positive. This can be easily seen from the
deﬁnition of B, i.e., B = (cid:80)m
2 (λI + γA+), since
τ =1
˙Qτ are PSD from its deﬁnition and Lemma 2, I is the
A+,
identity matrix, and λ is required to be positive.

˙Qτ + 1

Part 2). From Proposition 4, we have

˙R(Xk) = ˙Hk(0, Xk),

(55)

˙R(Xk + ˜X ∗) ≤ ˙Hk( ˜X ∗, Xk).
(56)
Recall that ˜X ∗ is approximated by ˜Xt in step 3 of Algo-
rithm 4. Using (40) and (41), we have

˙R(Xk) − ˙R(Xk+1) ≥ ˙Hk(0, Xk) − ˙Hk( ˜X ∗, Xk),
= (cid:2) ˙Hk(0, Xk) − ˙Hk( ˜Xt, Xk)(cid:3)

+ (cid:2) ˙Hk( ˜Xt, Xk) − ˙Hk( ˜X ∗, Xk)(cid:3).

(57)

Using δk’s deﬁnition in (21), we have

(cid:15)k ≥ δk( ˜Xt, {(˜ντ )t}) = Hk( ˜Xt; Xk) − Dk({ντ }t),

≥ Hk( ˜Xt; Xk) − Hk( ˜X ∗; Xk).

Thus,

−(cid:15)k ≤ ˙Hk( ˜Xt, Xk) − ˙Hk( ˜X ∗, Xk) ≤ 0.

(58)

From part 1), we also have:

˙Hk(0, Xk) − ˙Hk( ˜X ∗, Xk)
≥

α
2
Finally, combining (57)-(59), we then obtain

(cid:107)Xk+1 − Xk(cid:107)2 =

α
2

(cid:107) ˜X ∗(cid:107)2.

(59)

Then, similar with Proposition 2, denote ˜Qτ = (qk)τ ¯Qτ .
Following Lemma 2, we will have |tr( ˜X (cid:62) ˙Qτ ˜X)| ≤

˙R(Xk) − ˙R(Xk+1) ≥

α
2

(cid:107)Xk+1 − Xk(cid:107)2 − (cid:15)k.

∂2 ˙Hk(0, Xk) = γCXk

Then from (63), we have

Thus, part 2) holds.

1) ∂2 ˙Hk(0, Xk) = ∂ ˙R(Xk); and

Lemma 8.
2) 0 ∈ limk→∞ ∂2 ˙Hk( ˜Xk, Xk).
Proof. Part 1). First, recall from the deﬁnition of ˙R(X) in (23)
that
˙R( ˜X + Xk) =

φ(|tr((Xk + (cid:101)X)(cid:62)Qτ (Xk + (cid:101)X)) − tτ |)

(cid:88)m

τ =1
γ
tr(X (cid:62)
2

k CXk) +

γ
2

tr((2Xk + ˜X)(cid:62)C ˜X),

+

and

˙Hk( ˜X; Xk) =tr( ˜X (cid:62)(Q ˜X + γCXk))

+ 2

(cid:88)m

τ =1

|tr( ˜X (cid:62) ˙Qτ Xk) + (˙bk)τ | + ˙ck.

We have
∂ ˙R(Xk) = γAXk + λXk+

(qk)τ sign(tr(X (cid:62)

k Qτ Xk)−tτ )

1
2

( ˙Qτ + ˙Q(cid:62)

τ )Xk (60)

+

(cid:88)m

τ =1

and

+ 2

(cid:88)m

τ =1

where (qk)τ = φ(cid:48)(cid:0)|tr(X (cid:62)
λ
γ

C = A +

I and (˙bk)τ =

( ˙Qτ + ˙Q(cid:62)

τ )Xk

. Since

1
2

sign((˙bk)τ )
k Qτ Xk) − tτ |(cid:1)
1
2

(qk)τ

(cid:0)

tr(X (cid:62)

k Qτ Xk) − tτ

(61)

(cid:1),

we can deduce that the Lemma holds from (60) and (61).
Part 2). Recall that ˜Xk is deﬁned at step 3 of Algorithm 4
and δk is in (21), we have

δk( ˜Xt, {(˜ντ )t}) = ˙Hk( ˜Xt; Xk) − ˙Dk({ντ }t),

≥ ˙Hk( ˜Xt; Xk) − ˙Hk( ˜X ∗; Xk) ≥ 0.

˙Hk

Since
is
limt→∞ δk( ˜Xt, {(˜ντ )t}) = 0, we have

continuous

a

function

and

limt→∞ ˙Hk( ˜Xt; Xk) − ˙Hk( ˜X ∗; Xk) = 0,

which means 0 ∈ limt→∞ ∂2 ˙Hk( ˜Xt, Xk).

Proof. (of Theorem 2) Conclusion (i). From part 2) in
2 (cid:107)Xk+1 − Xk(cid:107)2 ≤ ˙R(Xk) − ˙R(Xk+1) +
Lemma 7, we have α
(cid:15)k. Thus,
α
(cid:88)K
2

˙R(Xk) − ˙R(Xk+1) + (cid:15)k,

(cid:107)Xk+1 − Xk(cid:107)2

F ≤

(cid:88)K

k=1

k=1

≤ ˙R(X1) − ˙R(XK+1) +
(cid:88)∞
≤ ˙R(X1)−inf ˙R+

(cid:88)K

k=1

(cid:15)k,

(cid:15)k.

(62)

k=1

From Assumption 2, we know that the last term in (62), i.e.,
(cid:80)∞
k=1 (cid:15)k, is ﬁnite. Together with Assumption 1, we have

limk→∞(cid:107)Xk+1 − Xk(cid:107)2

F = 0,
and 0 ≤ limk→∞ (cid:107)Xk(cid:107)2

F < ∞,

(63)

which means that the sequence {Xk} is bounded and has at
least one limit points.

19

sequence {Xk} as X∗, and let {Xkj } be a sub-sequence of
{Xk} such that

X∗ = limkj→∞ Xkj .

(64)

Thus, to prove a limit point X ∗ is also a critical point for
˙R(X), we only need to show

0 ∈ limkj→∞ ∂2 ˙Hkj (0, X ∗).

Using Part 2) in Lemma 8, we have

Thus, denote limt→∞ ˜X kj

∗ , we only need to prove

, Xkj ),
, Xkj ).

t

0 ∈ limt→∞ ∂2 ˙Hkj ( ˜X kj
= ∂2 ˙Hkj (limt→∞ ˜X kj
t = ˜X kj
limkj→∞ ˜X kj

∗ = 0

t

Since (cid:80)∞
which implies that

k=1 (cid:15)k is ﬁnite, we must have limkj→∞ (cid:15)kj = 0,

limkj→∞(Xkj +1 − Xkj − ˜X kj

∗ ) = 0.

(65)

(66)

(67)

(68)

(69)

limkj→∞ Xkj +1 − Xkj = 0,

And (67) follows easily from (68) and (69). Finally, (64) can
be obtained by combining (65) and (67). Thus, any limit
point of {Xk} is a critical point of ˙R.

A.10 Proposition 5

2 (˜x(cid:62)

We ﬁrst introduce the following Lemma.
Lemma 9. tr( ˜X (cid:62) ¯Qτ ˜X) = 1
i ˜xi + ˜x(cid:62)
j ˜xj).
τ )/2, thus ˜Qτ is a
Proof. We ﬁrst denote ˜Qτ = (Qτ + Q(cid:62)
zero matrix with only ( ˜Qτ )ij = ( ˜Qτ )ji = 1/2. It can be
easily seen that ˜Qτ has only three different eigenvalues: 0
and ±1/2. Thus, for ¯Qτ = (Qτ + Q(cid:62)
τ )−/2
we can see that it is also a zero matrix with only ( ¯Qτ )ii =
( ¯Qτ )jj = 1/2. Therefore it follows easily that tr( ˜X (cid:62) ¯Qτ ˜X) =
(˜x(cid:62)

τ )+/2 − (Qτ + Q(cid:62)

i ˜xi + ˜x(cid:62)

j ˜xj)/2.

Next, we start to prove Proposition 5.

(resp., (xk)(cid:62)

Proof. Let ˜x(cid:62)
i
Xk). Denote Q(i,j) as a zero matrix with only Q(i,j)
Obviously we should have tr( ˜X (cid:62)Q(i,j)Xk) = ˜x(cid:62)
tr( ˜X (cid:62)Q(i,j) ˜X) = ˜x(cid:62)
general SDP problem is

i ) be the ith row of ˜X (resp.,
ij = 1.
i (xk)j and
i ˜xj. Recall that the objective in (16) for

tr( ˜X (cid:62)(B ˜X + γCXk)) + 2

min
˜X
s.t. eτ = tr( ˜X (cid:62)Qτ Xk) + (bk)τ

(cid:88)m

τ =1

|eτ |

τ = 1, . . . , m.

For our matrix completion problem, we have Qτ = Q(i,j),
tτ = Oij and A = 0. This gives us B = Q + γ
2 I, γC = γI
and (bk)ij = 1

i (xk)j − Oij).
From Lemma 9, we need to sum the row ˜x(cid:62)

i ˜xi and
j ˜xj once when Ωij is not zero. Thus, for a speciﬁc

column ˜x(cid:62)
˜x(cid:62)
i ˜xi, we will sum it nnz(Ω(i,:)) + nnz(Ω(:,i)) times, i.e.,

2 ((xk)(cid:62)

Conclusion (ii). From Part 1)
in Lemma 8, we have
∂2 ˙Hk(0, Xk) = ∂ ˙R(Xk). Then, denote the limit point of

(cid:88)m

τ =1

tr( ˜X (cid:62) ¯Qτ ˜X) =

(cid:88)n

i=1

1
2

(nnz(Ω(i,:)) + nnz(Ω(:,i)))˜x(cid:62)

i ˜xi.

√

Let Λr = Diag(
Diag(

√

nnz(Ω(:,1)), . . . ,

nnz(Ω(1,:)), . . . ,

√

nnz(Ω(:,n))), we have

√

nnz(Ω(n,:))) and Λc =

20

(cid:107)Λr ˜X(cid:107)2

F =

(cid:107)Λc ˜X(cid:107)2

F =

(cid:88)n

i=1

(cid:88)n

j=1

nnz(Ω(i,:))˜x(cid:62)
nnz(Ω(:,j))˜x(cid:62)

i ˜xi,

j ˜xj.

Combining them together, we will have

(cid:107)Λr ˜X(cid:107)2

F + (cid:107)Λc ˜X(cid:107)2

F =

(cid:88)n

i=1

(nnz(Ω(i,:)) + nnz(Ω(:,i)))˜x(cid:62)

i ˜xi.

Thus,

and

tr( ˜X (cid:62)Q ˜X) =

(cid:88)m

τ =1
(cid:107)Λr ˜X(cid:107)2

tr( ˜X (cid:62) ¯Qτ ˜X),
1
2

F +

(cid:107)Λc ˜X(cid:107)2
F .

=

1
2

tr( ˜X (cid:62)B ˜X) = tr( ˜X (cid:62)Q ˜X) +
1
2

(cid:107)Λr ˜X(cid:107)2

F +

=

tr( ˜X (cid:62) ˜X),

γ
2
1
(cid:107)Λc ˜X(cid:107)2
2

F +

γ
2

(cid:107) ˜X(cid:107)2
F .

And it follows easily that

tr( ˜X (cid:62)γCXk) = tr( ˜XλIXk) = λtr( ˜X (cid:62)Xk).

Combining it all together, the objective then becomes

min
˜X

(cid:107)Λr ˜X(cid:107)2

1
2

F +

(cid:107) ˜X(cid:107)2

γ
2
+ λtr( ˜X (cid:62)Xk) + 2

F +
(cid:88)

1
2

(cid:107)Λc ˜X(cid:107)2
F

|eij|,

(i,j)∈Ω

s.t. eij = ˜x(cid:62)
2 ((xk)(cid:62)

where (bk)ij = 1

i (xk)j − Oij).

i (xk)j + (bk)ij, ∀(i, j) ∈ Ω,

