2
2
0
2

p
e
S
5

]

G
L
.
s
c
[

2
v
3
1
1
2
1
.
1
0
2
2
:
v
i
X
r
a

Published in Transactions on Machine Learning Research (09/2022)

HEAT: Hyperedge Attention Networks

Dobrik Georgiev∗
Department of Computer Science and Technology, University of Cambridge, UK

dgg30@cam.ac.uk

Marc Brockschmidt
Microsoft Research, Cambridge, UK†

Miltiadis Allamanis
Microsoft Research, Cambridge, UK†

mmjb@google.com

mallamanis@google.com

Reviewed on OpenReview: https: // openreview. net/ forum? id= gCmQK6McbR

Abstract

Learning from structured data is a core machine learning task. Commonly, such data is
represented as graphs, which normally only consider (typed) binary relationships between
pairs of nodes. This is a substantial limitation for many domains with highly-structured
data. One important such domain is source code, where hypergraph-based representations
can better capture the semantically rich and structured nature of code.

In this work, we present HEAT, a neural model capable of representing typed and qualiﬁed
hypergraphs, where each hyperedge explicitly qualiﬁes how participating nodes contribute. It
can be viewed as a generalization of both message passing neural networks and Transformers.
We evaluate HEAT on knowledge base completion and on bug detection and repair using
a novel hypergraph representation of programs. In both settings, it outperforms strong
baselines, indicating its power and generality.

1 Introduction

Large parts of human knowledge can be formally represented as sets of relations between entities, allowing
for mechanical reasoning over it. Common examples of this view are knowledge graphs representing our
environment, databases representing business details, and ﬁrst-order formulas describing mathematical insights.
Such structured data hence regularly appears as input to machine learning (ML) systems.

In practice, this very generic framework is not easy to handle in ML models. One issue is that the set of
relations is not necessarily known beforehand, and that the precise structure of a relation is not easily ﬁxed.
As an example, consider studied(person:P , institution:I, major:M ), encoding the fact that a person
P studied at institution I, majoring in M . However, if the institution is unknown, we may want to just
consider studied(person:P , major:M ), or we may need to handle case of people double-majoring, using
studied(person:P , major:M2, major:M1).

Existing ML approaches usually cast such data as hypergraphs, and extend the ﬁeld of graph learning
approaches to this setting, but struggle with its generality. Some solutions require to know the set of relations
beforehand (ﬁxing their arity, and assigning ﬁxed meanings to each parameter), while others abstract the set
of entities to a simple set and forego the use of the name of the relation and its parameters.

∗Work mainly performed while interning at Microsoft Research, Cambridge, UK.
†Now at Google Research.

1

 
 
 
 
 
 
Published in Transactions on Machine Learning Research (09/2022)

τ1

τ2

ρi e

τ3

n1

n2

n3

Figure 1: Representation of relation ρ(τ1:n1, τ2:n2, τ3:n3) as typed and qualiﬁed hyperedge e, i.e. a
relation of type ρ, and the qualiﬁed participation of the nodes n1 as τ1, n2 as τ2, and n3 as τ3 in e. HEAT
operates on hypergraphs with such hyperedges.

One form of data that can proﬁt from being modeled as a hypergraph is program source code. Existing
approaches model code either as simple sequence of tokens (Hindle et al., 2012) or as a graph (Allamanis
et al., 2018b; Hellendoorn et al., 2020). While the former can easily leverage successful techniques from the
NLP domain, it is not possible to include additional domain-speciﬁc knowledge (such as the ﬂow of data) in
the input. On the other hand, graph models are able to take some of this additional information, but struggle
to represent more complex relationships that require hyperedges.

In this work, we propose a new architecture, HEAT (HyperEdge ATtention), that is able to handle an open
set of relations that may appear in several variants. To this end, we combine the idea of message passing
schemes that follow the graph structure with the ﬂexibility and representational power of Transformer
architectures. Concretely, our model handles arbitrary relations by presenting each one as a separate sequence
in a Transformer, where an idea akin to standard positional encodings is used to represent how each entity
participates in the relation. The Transformer output is then used to update the representations of participating
entities.

We illustrate the success of this technique in two very diﬀerent settings: learning to infer additional relations
in knowledge graphs, and learning to ﬁnd and repair bugs in programs. In both cases, our model shows
improvements over strong state-of-the-art methods. Concretely, we (a) deﬁne HEAT as a novel hypergraph
neural network architecture (Sec. 2), (b) deﬁne a novel representation of programs as hypergraphs (Sec. 3),
(c) evaluate HEAT on the tasks of detecting and repairing bugs (Sec. 4.1) and link prediction in knowledge
graphs (Sec. 4.2).

Our implementation of the HEAT model is available on the heat branch of https://github.com/microsoft/
neurips21-self-supervised-bug-detection-and-repair/tree/heat. This includes code for the extrac-
tion of hypergraph representations of Python code as discussed in Sec. 3.

2 The HEAT Model

In this work, we are interested in representing typed and qualiﬁed relations of the form relName(parName1:n1,
parName2:n2, ...) where relName represents the name of a relation, n1, n2, . . . are entities participating
in the relation, with parName1, . . . describes (qualiﬁes) their role in the relation.

Formally, such relations can be represented as typed and qualiﬁed hypergraphs: we consider a set of nodes
(entities) N = {n1, . . .} and a set of hyperedges H = {e1, . . .}. Each hyperedge e = (ρ, {(τ1, n1) , . . . , (τk, nk)})
describes a named relationship of type ρ among nodes n1 . . . nk where the role of node nk within e is
qualiﬁed using τk. Fig. 1 illustrates the form of such a hyperedge. Note that this is in contrast to traditional
hypergraphs where nodes participate in a hyperedge without any qualiﬁers. Instead, typed and qualiﬁed
graphs can accurately represent a large range of domains maintaining valuable information.

2

Published in Transactions on Machine Learning Research (09/2022)

2.1 Background

Message passing neural networks (MPNN) (Gilmer et al., 2017) operate on sets of nodes N and sets of
(typed) edges E, where each edge (ni, τi, nj) of type τ connects a pair of nodes. In MPNNs, each node n ∈ N
is associated with representations h(t)
n stems from input
features, but subsequent representations are computed by exchanging information between nodes. Optionally,
each edge e may also be associated with a state/representation h(t)
that is also computed incrementally.
e
Concretely, each edge gives rise to a “message” using a learnable function fm:

n that are computed incrementally. The initial h(0)

m(t)

(ni,τ,nj ) = fm

(cid:16)

h(t)
ni

, τ, h(t)
nj

, h(t)
e

(cid:17)

.

To update the representation of a node n, all incoming messages are aggregated with a permutation-invariant
function Agg(·) into a single update, i.e.

u(t)
nj

= Agg

(cid:16)n

m(t)

(ni,τ,nj ) | (ni, τ, nj) ∈ E

o(cid:17)

.

(1)

Agg is commonly implemented as summation or max-pooling, though attention-based variants ex-
ist (Veličković et al., 2018). Finally, each node’s representation is updated using its original representation
and the aggregated messages. Many diﬀerent update mechanisms have been considered, ranging from just
using the aggregated messages (Kipf & Welling, 2017) to gated update functions (Li et al., 2016).

Transformers (Vaswani et al., 2017) learn representations of sets of elements N without explicit edges.
Instead, they consider all pairwise interactions and use a learned attention mechanism to identify particularly
important pairs. Transformer layers are split into two sublayers. First, multihead attention (MHA) uses the
representations of all N to compute an “update” un for each n ∈ N , i.e.

h
u(t)
n1

, . . . , u(t)
nk

i

= MHA

(cid:16)h

h(t)
n1

, . . . , h(t)
nk

i(cid:17)

.

(2)

Internally, MHA uses an attention mechanism to determine the relative importance of pairs of entities and
to compute the updated representations. Note that MHA treats its input as a (multi-)set, and hence is
permutation-invariant. To provide ordering information, a common approach is to use positional encodings,
i.e. to extend each representation with explicit information about the position. In practice, this means that
MHA operates on
, where pi provides information about the position of ni, and ⊕
is a combination operation (commonly element-wise addition).

h
n1 ⊕ p1, . . . , h(t)
h(t)

nk ⊕ pk

i

The second Transformer sublayer is used to combine the updated representations with residual information
and add computational depth to the model. This is implemented as

q(t)
ni

= LN

h(t+1)
ni

= LN

(cid:16)

(cid:16)

h(t)
ni

+ u(t)
ni

(cid:17)

q(t)
ni

+ FFN

(cid:16)

(cid:17)(cid:17)

,

q(t)
ni

(3)

(4)

where LN is layer normalisation (Ba et al., 2016) and FFN is a feedforward neural network (commonly with
one large intermediate hidden layer).

2.2 HEAT: An Attention-Based Hypergraph Neural Network

We now present HEAT, a Transformer-based message passing hypergraph neural network that learns to
represent typed and qualiﬁed hypergraphs.

Overall, we follow the core idea of the standard message passing paradigm: our aim is to update the representa-
tion of each entity using messages arising from its relations to other entities. Following Battaglia et al. (2018),
we also explicitly consider representations of each hyperedge, based on their type and the representation of
adjacent entities. Intuitively, we want each entity to “receive” one message per hyperedge it participates in,
reﬂecting both its qualiﬁer (i.e. role) in that hyperedge, as well as the relation to other participating entities.

3

Published in Transactions on Machine Learning Research (09/2022)

h(t)
e

n1 r(t)
h(t)
τ1

n2 r(t)
h(t)
τ2

n3 r(t)
h(t)
τ3

⊕

⊕

⊕

Multihead Attention

m(t)
e

m(t)

e→n1

m(t)

e→n2

m(t)

e→n3

(a) Message computation for hyperedge e from Fig. 1. The
previous node states h(t)
ni are combined with their respec-
tive qualiﬁer embeddings r(t)
τi . Then, a multihead attention
mechanism computes the messages m(t)

e→· and m(t)
e .

e1→n

m(t)
...
m(t)

ek→n

Agg

h(t)
n

Add & Norm

Feed Forward

Add & Norm

h(t+1)
n

(b) The state update for each node n in the hypergraph.
Apart from Agg this matches a standard Transformer
layer of Vaswani et al. (2017).

Figure 2: The HEAT Architecture.

To compute these messages, we borrow ideas from the Transformer architecture. We view each qualiﬁed
hyperedge as a set of entities, with each entity being associated with a position (its qualiﬁer in the hyperedge),
and then use multihead attention — as in Eq. 2 — to compute one message per involved entity:

h

m(t)

e , m(t)

e→n1

, . . . , m(t)

e→nk

i

= MHA

(cid:16)h

e , r(t)
h(t)
τ1

⊕ h(t)
n1

, . . . , r(t)
τk

⊕ h(t)
nk

i(cid:17)

.

(5)

is the current representation of the hyperedge, h(t)
ni

is the current representation of node ni, r(t)
Here, h(t)
τi
e
is the representation (embedding) of the qualiﬁer τi which denotes the role of ni in e, and ⊕ combines two
vectors. Fig. 2a illustrates this. In this work, we consider element-wise addition for ⊕ but other operators,
such as vector concatenation would be possible.

We can then compute a single update u(t)
the hyperedges it occurs in, i.e.

ni for node ni by aggregating the relevant messages computed for

u(t)
ni

= Agg

(cid:16)n

m(t)

e→ni

| e = (ρ, {. . . , (τi, ni), . . .}) ∈ H

o(cid:17)

.

This is equivalent to the aggregation of messages in MPNNs (cf. Eq. 1). We then compute an updated entity
representation h(t+1)
following the Transformer update as described in Eqs. 3, 4. This is illustrated in Fig. 2b.
The core diﬀerence to the Transformer case is the aggregation Agg(·) used to combine all messages (as in a
message passing network). Agg(·) can be any permutation invariant function, such as elementwise-sum and
max, or even an another transformer (see Sec. 4.1 for experimental evaluation of these variants).

ni

e are also updated as in Eqs. 3, 4 using the messages m(t)
e

Finally, the hyperedge states h(t)
computed as in
Eq. 5, and no aggregation is required because there is only a single message for each hyperedge e. Initial node
states h(0)
ni are initialised with node-level information (as in GNNs). Qualiﬁer embeddings rτi (resp. initial
hyperedge states h(0)
e ) are obtained by breaking the name of τi (resp. ρe) into subtokens (e.g., parName is
split into par and name or foo_bar2 into foo, bar, and 2), embedding these through a (learnable) vocabulary
embedding matrix, and then sum-pooling. Then for each HEAT layer r(t)
is computed through a linear
τi
learnable layer of the sum-pooled embedding, i.e. r(t)

τi = W (t)

τ rτi + b(t)
τ .

Generalising Transformers Note that if the hypergraph is made up of a single hyperedge of the form
Seq(pos1:n1, pos2:n2, ...) then HEAT degenerates to the standard transformer of Vaswani et al. (2017)
with positional encodings over the sequence n1, n2, . . . at each layer. In the case of such sequence positions,
we simply use the sinusoidal positional encodings of Vaswani et al. (2017) as position embedding, rather
than using a learnable embedding. Thus HEAT can be thought to generalise transformers from sets (and
sequences) to richer structures.

4

Published in Transactions on Machine Learning Research (09/2022)

Computational Considerations A naïve implementation of a HEAT message passing step requires one
sample per hyperedge to be fed into a multihead attention layer, as each hyperedge of k nodes gives rise to
a sequence of length k + 1, holding the edge and all node representations. To enable eﬃcient batching, this
would require to pad all such sequences to the longest sequence present in a batch. However, the length of
these sequences may vary wildly. On the other hand, processing each hyperedge separately would not make
use of parallel computation in modern GPUs.

To resolve this, we use two tricks. First, we consider “microbatches” with a pre-deﬁned set of sequence lengths
{16, 64, 256, 768, 1024}, minimising wastage from padding. Second, we “pack” several shorter sequences into
a single sequence, and then use appropriate attention masks in the MHA computation to avoid interactions.
For example, a hyperedge of size 9 and a hyperedge of size 7 can be joined together to yield a sequence
of length 16. Appx. A details the packing algorithm used. This process is performed in-CPU during the
minibatch preparation.

3 Representing Code as Hypergraphs

Program source code is a highly structured object that can be augmented with rich relations obtained from
program analyses. Traditionally, source code is represented in machine learning either as a sequence of tokens
(Hindle et al., 2012), a tree (Yin & Neubig, 2017), or as a graph (Allamanis et al., 2018b; Hellendoorn et al.,
2020). Graph-based representations (subsuming tree-based ones) commonly consider pairwise relationships
among entities in the code, whereas token-level ones simply consume a token sequence. In this section, we
present a novel hypergraph representation of code that retains the best of both representations.

Our program hypergraph construction is derived from the graph construction presented by Allamanis et al.
(2021), but uses hyperedges to produce a more informative and compact representation. The set of nodes
in the generated hypergraphs include tokens, expressions, abstract syntax tree (AST) nodes, and symbols.
However, in contrast to prior work, we do not only consider pairwise relationships, but instead use a typed
and qualiﬁed hypergraph (Fig. 1). We detail the used hyperedges and their advantages next. Fig. 3 in Appx. B
illustrates some of the considered relations on a small synthetic snippet.

Tokens For the token sequence t1, t2, ..., tL of a snippet of source code, we create the relation Tokens(p1:t1,
p2:t2, ...). This is the entire information used by token-level Transformer models, considering all-to-all
relations among tokens. Note that in standard graph-based code representations, the token sequence is
usually represented using a chain of NextToken binary edges, meaning that long-distance relationships are
hard to discover for models consuming such graphs. For very long token sequences, which may cause memory
issues, we “chunk” the sequence into overlapping segments of length L, similar to windowed, sparse attention
approaches. We use L = 512 in the experiments. Within HEAT and speciﬁcally for the Tokens(·) hyperedges,
to reduce the used parameters and to allow arbitrarily long sequences, the qualiﬁer embeddings rp1, rp2, rp3, ...
are computed using the ﬁxed sinusoidal embeddings of Vaswani et al. (2017) instead of learnable embeddings.

AST We represent the program’s abstract syntax tree using AstNode relations, with qualiﬁers corresponding
to the names of children of each node. For example, an AST node of a binary operation is represented as
AstNode(node:nBinOp, left:nleft, op:nop, right:nright). Similarly, a AstNode(node:nIfStmt, cond:nc,
then:nt, else:ne) represents an if statement node nIfStmt. In cases where the children have the same
qualiﬁer and are ordered, (e.g. the sequential statements within a block) we create numbered relations, i.e.
AstNode(node:nBlock, s1:n1, s2:n2, ...). For Python, we use as qualiﬁer names those used in libCST.
In contrast, most graph-based approaches use Child edges to connect a parent node to all of its children, and
thus lose information about the speciﬁc role of child nodes. As a consequence, this means that left and right
children of non-commutative operations can not easily be distinguished. Allamanis et al. (2021) attempts to
rectify this using a NextSibling edge type, but still is not able to make use of the known role of each child.

Control and Data Flow To indicate that program execution can ﬂow from any of the AST nodes
p1, p2, ... to one of the AST nodes s1, s2, ..., we use the relation CtrlF(prev:np1, prev:np2, ..., succ:ns1,
succ:ns2 ). For example, if c: p1 else: p2; s1 would yield CtrlF(prev:p1, prev:p2, succ:s1). Sim-
ilarly, we use relations MayRead and MayWrite to represent dataﬂow, where the previous location at which a

5

Published in Transactions on Machine Learning Research (09/2022)

symbol may have been read (written to) are connected to the succeeding locations. Note that these relations
compactly represent data and control ﬂow consolidating N -to-M relations (e.g. N states may lead to one of
M states) into a single relation, which would otherwise require N · M edges in a standard graph.

use

Symbols We
Symbol(sym:ns, occ:n1, occ:n2, ..., may_last_use:nu1,
may_last_use:nu2 , ... ) to connect all nodes n1, n2, ... referring to a symbol (e.g. variable) to a
fresh node ns, introduced for each occurring symbol. We annotate within this relation the nodes nu1, nu2 , ...
that are the potential last uses of the symbol within the code snippet.

relation

the

Functions A challenge in representing source code is how to handle calls to (potentially user-deﬁned)
functions. As it is usually not feasible to include the source code of all called functions in the model input
(for computational and memory reasons), appropriate abstractions need to be used. For a call (invocation) of
a function foo(arg1, ..., argN) deﬁned by def foo(par1, ..., parN): ..., we introduce the relation
foo(rval:n, par1:narg1, ..., par3:nargN) where n is the invocation expression node and narg1, . . . , nargN
are the nodes representing each of the arguments. Hence, we generate one relation symbol for each deﬁned
function, and match nodes representing arguments to the formal parameter names as qualiﬁers. This repre-
sentation allows to naturally handle the case of variable numbers of parameters and arbitrary functions.

Syntactic sugar and operators are converted in the same way, using the name of the corresponding built-in
function. For example, in Python, a in b is converted into the relation __contains__(self:nb, item:na)
and a -= b is converted into __isub__(self:na, other:nb) following the reference Python data model.

Finally, we use the relation Returns(fn:nf, from:n1, from:n2, ...) to connect all possible return points
for a function with the AST node nf for the function deﬁnition. Similarly, a Yields(·) is deﬁned for generator
functions.

4 Evaluation

We evaluate HEAT on two tasks
(Allamanis
et al., 2021) and knowledge base completion (Galkin et al., 2020). We implemented it as a Py-
Torch (Paszke et al., 2019) Module, available on the heat branch of https://github.com/microsoft/
neurips21-self-supervised-bug-detection-and-repair/tree/heat.

from the literature: bug detection and repair

4.1 HEAT for Bug Detection & Repair

We evaluate HEAT on the bug localisation and detection task of Allamanis et al. (2021) in the supervised
setting. This is a hard task that requires combining ambiguous information with reasoning capabilities able
to detect bugs in real-life source code.

For this, we built on the open-source release of PyBugLab of Allamanis et al. (2021), making two changes:
(1) we adapt the graph construction from programs to produce hypergraphs as discussed in Sec. 3, and (2)
use HEAT to compute entity representations from the generated graphs, rather than GNNs or GREAT.

Dataset We use the code of Allamanis et al. (2021) to generate a dataset of randomly inserted bugs to train
and evaluate a neural network in a supervised fashion. Consequently, we obtain a new variant of the “Random
Bugs” test dataset, consisting of ∼ 760k graphs. We additionally re-extract the PyPIBugs dataset with the
provided script, generating hypergraphs as consumed by HEAT, and graphs generated by the baseline models.
However, since the PyPIBugs dataset is provided in the form of GitHub URLs referring to the buggy commit
SHAs, some of them have been removed from GitHub and thus our PyPIBugs dataset contains 2354 samples,
20 less than the one used by Allamanis et al. (2021).

Model Architecture We modify the architecture of Allamanis et al. (2021) to use 6 HEAT layers with
hidden dimension of 256, 8 heads, feed-forward (FFN in Eq. 4) hidden layer of 2048, and dropout rate of
0.1. As discussed above, our datasets diﬀer slightly from the data used by Allamanis et al. (2021), and we
re-evaluated their released code on our new datasets. We found this rerun to perform notably better than what

6

Published in Transactions on Machine Learning Research (09/2022)

Table 1: Evaluation Results on Supervised Bug Detection and Repair on supervised PyBugLab.

Random Bugs

PyPIBugs

Joint

Loc. Repair

Joint

Loc. Repair

GNN (Allamanis et al., 2021)†
GREAT (Allamanis et al., 2021)†

GNN (Allamanis et al., 2021) (rerun)
GREAT (Allamanis et al., 2021) (rerun)
HEAT

62.4
51.0

69.8
65.6
76.5

73.6
61.9

79.6
74.4
83.1

81.2
76.3

83.4
81.8
88.5

20.0
16.8

22.0
16.8
24.6

28.4
25.8

28.3
21.9
29.6

HEAT – DeepSet-based messages
HEAT – without qualiﬁer embeddings
HEAT – Agg (cid:44) CrossAtt
HEAT – without FFN
HEAT – without hyperedge state

74.0
69.2
76.5
73.7
75.7
† Reported on a diﬀerent random bugs dataset and a slightly diﬀerent PyPIBugs dataset.

81.1
76.7
83.0
80.7
82.4

23.2
19.1
23.4
20.7
23.0

28.6
25.7
27.9
25.4
28.6

87.3
85.6
88.2
87.0
88.0

61.8
58.6

66.7
67.7
71.0

69.0
65.0
71.5
69.2
71.1

was originally reported in the paper. In private communication, the authors explained that their public code
included a small change to the model architecture compared to the paper: the subnetworks used for selecting
a program repair rule are now shallow MLPs (rather than inner products), which increases performance across
the board. Our HEAT extension follows the code release, and hence we use max-pooled subtoken embeddings
to initialise node embeddings, a pointer network-like submodel to select which part of the program to repair,
and a shallow MLPs to select the repair rules. We also re-use the PyBugLab supervised objective, which is
composed of two parts: a PointerNet-style objective requiring to identify the graph node corresponding to a
program bug, and a ranking objective requiring to select a ﬁxing program rewrite at the selected location.

Results We show the results of our experiments in Table 1, where “Loc.” refers to the accuracy in identifying
the buggy location in an input program, “Repair” to the accuracy in determining the correct ﬁx given the
buggy location, and “Joint” to solving both tasks together. The results indicate that HEAT improves
performance on both considered datasets, improving the joint localisation and repair accuracy by ∼ 10% over
the two well-tuned baselines.

In particular, we observe that HEAT substantially improves over GREAT (Hellendoorn et al., 2020), which
also adapts the Transformer architecture to include relational information. However, GREAT eschews explicit
modelling of edges, and instead uses (binary) relations between tokens to bias the attention weights in the
MHA computation. We observe a less pronounced gain over GNNs, which we believe is due to the simpler
information ﬂow across long distances and the clearer way of encoding structural information in HEAT. (see
Sec. 3) We note that Allamanis et al. (2021) showed that their models also outperform ﬁne-tuned variants
of the cuBERT (Kanade et al., 2020) model, which stems from self-supervised pre-training using masking
objectives. Consequently, we believe that HEAT outperforms such large models as well, though a comparison
to recent very large models, adapted to the task (Chen et al., 2021; Austin et al., 2021; Li et al., 2022) is left
to future work.

Variations and Ablations To understand the importance of diﬀerent components of HEAT, we experi-
ment with ﬁve ablations and variations, shown on the bottom of Table 1.

First, we study the importance of using multi-head attention to compute messages. In particular, we are
interested in determining whether considering interactions between diﬀerent nodes participating in a hyper-
edge is necessary. To this end, we consider an alternative scheme in which we ﬁrst compute a hyperedge
representation using aggregation of all adjacent nodes, using their qualiﬁer information, i.e.

e = Agg0 (cid:16)h
q(t)

h(t)
e , rτ1 ⊕ h(t)
n1

, rτ2 ⊕ h(t)
n2

i(cid:17)

, ...

.

7

Published in Transactions on Machine Learning Research (09/2022)

Table 2: Results on the Random Bugs dataset when applying HEAT on a graph dataset representing edges
as (binary) edges.

Joint Localisation & Repair Localisation Repair

GNN (Allamanis et al., 2021) (rerun)
GREAT (Allamanis et al., 2021) (rerun)
HEAT (on binarised hypergraphs)
HEAT (on hypergraphs)

69.8
65.6
71.6
76.5

79.6
74.4
79.3
83.1

83.4
81.8
85.5
88.5

In our experiments, we use a Deep Set (Zaheer et al., 2017) model to implement Agg0. We then compute
messages for each node ni using a single linear layer W , i.e.

m(t)

e→ni

= ReLU

(cid:16)

W · [rτi ⊕ h(t)
ni

(cid:17)

, q(t)
e ]

.

The results indicate that this model variant is still stronger than the GNN and GREAT architectures, but
that HEAT proﬁts from explicitly considering the relationships of nodes participating in a hyperedge.

Next, we analyse the importance of using qualiﬁer information in our model. To this end, we consider a
variant of HEAT in which we remove from Eq. 5 the qualiﬁer information rτi, i.e. to

h
m(t)

e , m(t)

e→n1

, m(t)

e→n2

i
, ...

= MHA

(cid:16)h

e , h(t)
h(t)
n1

, h(t)
n2

i(cid:17)

, ...

.

The results clearly indicate that the qualiﬁer-as-position scheme used in HEAT is crucial for good performance.
In particular, it indicates that the qualiﬁer information contained in the data is very valuable, and emphasises
the importance of considering qualiﬁed hypergraphs.

We also considered using a more expressive aggregation mechanism for messages, replacing the max pooling
we use to implement Agg. Speciﬁcally, we use a multi-head attention between the current node state (as
queries in the attention mechanism) and the messages computed for all adjacent hyperedges (appearing as
keys and values).

This is reminiscent of graph attention networks (Veličković et al., 2018), which use an attention mechanism
to determine the respective importance of binary edges when aggregating messages. In our experiments, this
performed slightly worse than the aggregation using a simple max, while being substantially more memory-
and compute-intensive.

Next, we analyse whether the representational capacity is improved by the feedforward network (Eq. 4) in
the node and edge update. Removing these substantially reduces the number of parameters of the model.
The results indicate that these intermediate, per-representation computation steps add valuable capacity to
the model. This is especially apparent on the PyPIBugs dataset.

Another ablation we consider is a model variant in which we do not use evolving edge states, but instead
re-use h(0)
e on all HEAT layers. We observe a similar or slightly reduced performance on the joint localisation
and repair. This suggests that updating the edge state provides valuable representational capacity to the
model. Edge states, can be seen as the [CLS] token in traditional transformers, providing “scratch space” for
storing intermediate information for each hyperedge.

Finally, we assess the importance of the underlying data representation, investigating whether our represen-
tation of programs as hypergraphs (diﬀering from the PyBugLab baseline) alone explains the performance
improvement. For this, we convert the generated hyperedges to (binary) edges and use HEAT to learn from
these (binary) graphs. This experiment intents to disentagle the eﬀects of the diﬀerent data representation
from the eﬀects of HEAT’s architecture. The results, shown in Table 2, indicate that even on binarised
hypergraphs, HEAT outperforms the baseline GNN and GREAT models (especially on accuracy of choosing
program repairs), even though large parts of its architecture are not properly utilised by the data. However,
HEAT also takes advantage of the more expressive and compact data representation.

8

Published in Transactions on Machine Learning Research (09/2022)

4.2 HEAT for Knowledge Graph Completion

Knowledge graphs (KGs) can be accurately represented as typed and qualiﬁed hypergraphs. We now focus on
link prediction over a hyper-relational KG, which can be viewed as completing a knowledge base by additional
likely facts.

Dataset Following the discovery of test leaks and design ﬂaws by Galkin et al. (2020) in common benchmark
datasets such as WikiPeople (Guan et al., 2019) and JF17K (Wen et al., 2016) we chose one of the variations
of the new WD50K dataset presented there – WD50K (100). It is derived from Wikidata, containing 31k
statements, all of which use some qualiﬁer. To model a qualiﬁed triple statement of the form (s, r, o, Q) with Q
a set of qualiﬁer/entity pairs (qr i, qvi), we create a single hyperedge with special qualiﬁers src (resp.) obj for
the source and object of the relation, i.e. (r, {(src, s) , (obj, o)} ∪ Q). Using the example of Fig. 1 of Galkin
et al. (2020), the fact that Einstein studied mathematics at ETH Zurich in his undergraduate is expressed as
the hyperedge: EducatedAt(src:nEinstein, obj:nETH Zurich, degree:nBachelor, major:nMathematics).

In the released WD50K dataset, raw Wikidata identiﬁers (e.g. P69, Q937, etc.) are used to refer to entities and
relation names. We enrich the dataset by additionally retrieving natural language information for these entities
from Wikidata (e.g. replacing P69 by “educated at”) and allow the models to consume this information, to
encourage similar treatment of similarly named relationships and entities.

Model Architecture We modiﬁed the open-source release of StarE by Galkin et al. (2020), replacing the
GNN-based StarE encoder by HEAT. We used a single layer of HEAT with embedding size of 100 and a
single layer of the Transformer used for calculating the ﬁnal predictions1. Apart from some training parameters
(see below), hyperparameters (dropout, etc.) remain as documented in Appendix C of Galkin et al. (2020).
Since our goal is to evaluate the eﬀectiveness of HEAT, the remainder of the model is identical to the original
implementation of Galkin et al. (2020): queries for a relation j are represented as the concatenation of the
entity embeddings, relation embedding, and qualiﬁer embeddings, as shown in Fig. 3 of Galkin et al. (2020).
These are then passed through a Transformer block and a fully-connected layer to obtain the probability over
each entity being a possible object of the relation.

To process the additional natural language information about relations and entities we extracted (see above),
we build a vocabulary using byte pair encoding (BPE), and then embed the individual tokens and use sum
pooling to obtain initial node and relation representations. We evaluated variations of both the original
StarE model and our HEAT-based variant using this information, see below.

Training and evaluation Training is performed as in Galkin et al. (2020) using binary cross entropy with
label smoothing. In this link prediction task, a matching score is calculated for all possible relation objects
given source, relation and qualiﬁer-entity pairs (Dettmers et al., 2018, p.3). We trained our model for 1k
epochs with a learning rate of 0.0004 and batch size of 512. Hyperparameters were ﬁne-tuned manually, using
the provided validation set in the StarE implementation. For direct comparison with Table 3 of Galkin et al.
(2020) we report mean reciprocal rank (MRR) and hits at 1 and 10 (H@1, H@10) matching the original
evaluation setting. We train and evaluate all models using 5 random seeds and report standard deviations.

Results Table 3 shows the results of our evaluation on WD50K (100). We reran the original StarE
implementation, both to validate our setup and to obtain standard deviations. First, we observe that using
HEAT instead of the original GNN-based StarE encoder improves results, without any further changes
to the architecture. We expect that recent orthogonal work improving StarE by Yu & Yang (2021) would
similarly improve with our model.

Next, we consider the inﬂuence of using natural language information extracted from Wikidata. We note
that the original StarE encoder does not proﬁt from this information. In contrast, the HEAT-based model
slightly improves results, even though most words in the extracted data are extremely sparse. Finally, we
evaluate the less expressive message aggregation scheme of max pooling (as discussed in Sec. 4.1). Here, we
see that its performance is only marginally worse.

1Figure 3 of Galkin et al. (2020), bottom rectangle.

9

Published in Transactions on Machine Learning Research (09/2022)

Table 3: Link prediction results on WD50K (100) of Galkin et al. (2020). Standard deviations are obtained
over 5 diﬀerent seeds.

Model
StarE (GNN)† (Galkin et al., 2020)
StarE (GNN) – with NL information
StarE (HEAT) – Agg (cid:44) max, no NL information
StarE (HEAT) – Agg (cid:44) CrossAtt, no NL information
StarE (HEAT) – Agg (cid:44) CrossAtt, with NL information
Hy-Transformer? (Yu & Yang, 2021)

MRR

H@1

H@10

0.654±0.002
0.653±0.003

0.586±0.002
0.586±0.003

0.777±0.002
0.774±0.005

0.666±0.003
0.666±0.003
0.667±0.003

0.605±0.004
0.599±0.004
0.601±0.003

0.779±0.002
0.787±0.003
0.789±0.001

0.699

0.637

0.812

† Rerun of Galkin et al. (2020) implementation with 5 seeds.
∗ Current state-of-the-art.

5 Related Work

We review closely related techniques for learning on hypergraphs, and then brieﬂy discuss some particularly
relevant work from the application areas of learning on code and knowledge bases.

Hypergraph learning We broadly classify hypergraph learning into three approaches: hyperedge expan-
sion, spectral methods, and spatial (message passing-based) methods. Compared to HEAT, none of the
existing methods can directly work on typed and qualiﬁed hypergraphs.

In the ﬁrst class, hypergraphs are transformed into (binary) graphs and then a standard GNN is applied
on the resulting graph. This approach is taken by Agarwal et al. (2006), who use clique and star expansion
(representing each hyperedge as a full or a star graph respectively), Yadati et al. (2019), who represent each
hyperedge by a simple weighted edge whose endpoints can be further connected with weighted edges to
mediator nodes and Yang et al. (2020), who create a node for each pair of incident nodes and hyperedges
and connect those stemming from the same node or hyperedge. These approaches can re-use existing GNN
architectures, but at the cost of a signiﬁcantly increased number of edges.

The next class of methods aims at generalising the concepts of graph Laplacians and spectral convolutions to
the domain of hypergraphs. Feng et al. (2019) use the observations that nodes in the same hyperedge should
not diﬀer much in embedding to deﬁne a normalised hypergraph Laplacian. A similar approach has been
employed by Fu et al. (2019), who utilise the hypergraph p-Laplacian. Bai et al. (2021) extend Feng et al.
(2019) where the node-edge incidence matrix is weighted via an attention mechanism. Such methods are
usually applied to transductive learning tasks and either do not fully support typed or qualiﬁed hyperedges
or are limited to a ﬁxed number of hyperedge types or nodes in a hyperedge. This makes them inapplicable
in our setting.

The ﬁnal class of methods generalises the concept of neural message passing to hypergraphs. HyperSAGNN
(Zhang et al., 2020) is a self-attention based neural network, capable of handling variable-sized hyperedges,
but has been developed for the purposes of hyperedge prediction and is not directly applicable for node
classiﬁcation. HyperSAGE (Arya et al., 2020) performs convolution in two steps: nodes to hyperedges and
hyperedges to nodes, where the aggregation during the node and hyperedge message passing step is a power-
mean function, but it suﬀers from poor parallelisation and other practical issues (Huang & Yang, 2021, p. 2).
Chien et al. (2021) propose AllSet, a message passing scheme based on representing hyperedges as sets and
aggregations using permutation-invariant functions. AllSet can provably subsume a substantial number of
previous hypergraph convolution methods. Chien et al. (2021) propose two implementations of AllSet, using
DeepSets (Zaheer et al., 2017) and Transformers (Vaswani et al., 2017). This work is most similar to ours, but
does not consider the setting of qualiﬁed hyperedges. A natural consequence is that the model then computes
a single “message” per hyperedge, whereas HEAT computes diﬀerent messages for each participating node
(which only is required when nodes play diﬀerent roles in the hyperedge). In Sec. 4.1, we consider two relevant
ablations. These show that the use of qualiﬁer information is crucial for good results in the PyBugLab setting,
and that our message computation based on multihead attention is stronger than a DeepSet-based alternative.

10

Published in Transactions on Machine Learning Research (09/2022)

To the best of our knowledge, we are the ﬁrst to focus on processing typed and qualiﬁed hyperedges: two
hyperedges with the same element set can be diﬀerent if their type does not match and elements in a
hyperedge can have diﬀerent qualiﬁers (roles). Furthermore, our architecture is not restricted to a ﬁxed
number of hyperedge types/qualiﬁer and can generalise to types/qualiﬁers unseen during train time.

Knowledge Graph Completion Knowledge graph (KG) completion has emerged due to the incomplete-
ness of KGs (Ji et al., 2021). Embedding-based models (Bordes et al., 2013; Schlichtkrull et al., 2018; Shi
& Weninger, 2017) ﬁrst learn a low-dimensional embedding and then use it to calculate scores based on
these embeddings and rank the top k candidates. HEAT is a variation of an embedding-based model, but we
focus on hyper-relational KGs. Other, non-embedding-based approaches also exist, e.g. reinforcement learning
(Xiong et al., 2017) or rule-based ones (Rocktäschel & Riedel, 2017). Since we do not use such techniques,
we omit discussing them here and refer the reader to Ji et al. (2021, §IV.A).

Similarly to hypergraph learning, other works model hyper-relational KGs by simplifying qualiﬁed relations
to simpler representations: Wen et al. (2016) use clique expansion, which can be costly, (Fatemi et al., 2020)
represent hyper-relational facts as n-ary relations, but do not have explicit source/object of the relation and
instead of considering the qualiﬁer of an entity as in HEAT, their model only considers the position of an
entity within the relation; Guan et al. (2019) break n-ary facts into n + 2 qualiﬁer/entity pairs2, making
qualiﬁer/entity pairs indistinguishable to “standard” (s, rel, o) triples. These expansion-based approaches
cannot leverage semantic information such as the interaction of diﬀerent qualiﬁer/entity pairs (Yu & Yang,
2021).

Closest to our work is StarE (Galkin et al., 2020). StarE uses a GNN-like convolution, consisting of several
steps (cf. equations (5)-(7) of Galkin et al., 2020) on hyper-relational graphs to calculate updated entity
embeddings, which are then fed through a Transformer module. In Sec. 4 we show that HEAT outperforms
their GNN-based encoder.

Learning on Code Over the last decade, machine learning has been applied to code on a variety of
tasks (Allamanis et al., 2018a). A central theme in this research area is the representation of source code.
Traditionally, token-level representations have been used (Hindle et al., 2012), but Allamanis et al. (2018b)
showed that graph-level approaches can leverage additional semantic information for substantial improvements.
Subsequently, Fernandes et al. (2019); Hellendoorn et al. (2020) showed that combining token-level and graph-
level approaches yields best results. By using a relational transformer model over the code tokens Hellendoorn
et al. (2020) overcomes the inability of GNN models to handle long-range interactions well, while allowing to
make use of additional semantic information expressed as graph edges over the tokens. However, token-based
representations do not provide unambiguous locations for annotating semantic information (i.e. edges) for
non-token units such as expressions (e.g. a+b or a+b+c). Additionally, all these approaches have been limited
to standard (binary) edges, making the resulting graphs large and/or imprecise (see Sec. 3). Our experiments
with HEAT show that a suitable representation as typed and qualiﬁed hypergraph further improves over the
combination of token-level and binary graph-level approaches.

6 Conclusions & Discussion

We introduced HEAT, a neural network architecture that operates on typed and qualiﬁed hypergraphs. To
model such hypergraphs, HEAT combines the idea of message passing with the representational power of
Transformers. Furthermore, we showed how to convert program source code into such hypergraphs. Our
experiments show that HEAT is able to learn well from these highly structured hypergraphs, outperforming
strong recent baselines on their own datasets. A core insight in our work is to apply the power of Transformers
to several, overlapping sets of relations at the same time. This allows to concurrently model sequences and
graph-structured data. We believe that this opens up exciting opportunities for future work in the joint
handling of natural language and knowledge bases.

2+2 for the source and object qualiﬁers

11

Published in Transactions on Machine Learning Research (09/2022)

References

Sameer Agarwal, Kristin Branson, and Serge J. Belongie. Higher order learning with graphs. In International

Conference on Machine Learning (ICML), 2006.

Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. A survey of machine learning

for big code and naturalness. ACM Computing Surveys (CSUR), 51(4):1–37, 2018a.

Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs with

graphs. In International Conference on Learning Representations (ICLR), 2018b.

Miltiadis Allamanis, Henry Jackson-Flux, and Marc Brockschmidt. Self-supervised bug detection and repair.

In Neural Information Processing Systems (NeurIPS), 2021.

Devanshu Arya, Deepak K. Gupta, Stevan Rudinac, and Marcel Worring. Hypersage: Generalizing inductive

representation learning on hypergraphs. arXiv preprint arXiv:2010.04558, 2020.

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language
models. arXiv preprint arXiv:2108.07732, 2021.

Lei Jimmy Ba, Ryan Kiros, and Geoﬀrey E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450,

2016.

Song Bai, Feihu Zhang, and Philip H. S. Torr. Hypergraph convolution and hypergraph attention. Pattern

Recognit., 110:107637, 2021. doi: 10.1016/j.patcog.2020.107637.

Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinícius Flores Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Çaglar Gülçehre,
Francis Song, Andrew J. Ballard, Justin Gilmer, George E. Dahl, Ashish Vaswani, Kelsey Allen, Charles
Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matthew Botvinick,
Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases, deep learning, and graph networks.
arXiv preprint arXiv:1806.01261, 2018.

Antoine Bordes, Nicolas Usunier, Alberto García-Durán, Jason Weston, and Oksana Yakhnenko. Translating
In Neural Information Processing Systems (NeurIPS),

embeddings for modeling multi-relational data.
2013.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models
trained on code. arXiv preprint arXiv:2107.03374, 2021.

Eli Chien, Chao Pan, Jianhao Peng, and Olgica Milenkovic. You are AllSet: A multiset function framework

for hypergraph neural networks. arXiv preprint arXiv:2106.13264, 2021.

Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d knowledge

graph embeddings. In AAAI Conference on Artiﬁcial Intelligence, 2018.

Bahare Fatemi, Perouz Taslakian, David Vázquez, and David Poole. Knowledge hypergraphs: Prediction
beyond binary relations. In International Joint Conferences on Artiﬁcial Intelligence (IJCAI), 2020.

Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. Hypergraph neural networks. In AAAI

Conference on Artiﬁcial Intelligence, 2019.

Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. Structured neural summarization.

In

International Conference on Learning Representations (ICLR), 2019.

Sichao Fu, Weifeng Liu, Yicong Zhou, and Liqiang Nie. HpLapGCN: Hypergraph p-laplacian graph convolu-

tional networks. Neurocomputing, 362:166–174, 2019. doi: 10.1016/j.neucom.2019.06.068.

12

Published in Transactions on Machine Learning Research (09/2022)

Mikhail Galkin, Priyansh Trivedi, Gaurav Maheshwari, Ricardo Usbeck, and Jens Lehmann. Message passing

for hyper-relational knowledge graphs. In Empirical Methods in Natural Language Processing, 2020.

Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message

passing for quantum chemistry. In International Conference on Machine Learning (ICML), 2017.

Saiping Guan, Xiaolong Jin, Yuanzhuo Wang, and Xueqi Cheng. Link prediction on n-ary relational data.

In World Wide Web Conference, 2019.

Vincent J Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber. Global relational

models of source code. In International Conference on Learning Representations, 2020.

Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. On the naturalness of

software. In 2012 34th International Conference on Software Engineering (ICSE), 2012.

Jing Huang and Jie Yang. UniGNN: a uniﬁed framework for graph and hypergraph neural networks. In

Internal Joint Conferences on Artiﬁcial Intelligence (IJCAI), 2021.

Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. A survey on knowledge graphs:
IEEE Transactions on Neural Networks and Learning

Representation, acquisition, and applications.
Systems, 2021.

Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Learning and evaluating contextual

embedding of source code. In International Conference on Machine Learning (ICML), 2020.

Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. In

International Conference on Learning Representations (ICLR), 2017.

Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. In

International Conference on Learning Representations (ICLR), 2016.

Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James
Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor
Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy,
Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu,
and Oriol Vinyals. Competition-level code generation with alphacode. arXiv preprint arXiv:2203.07814,
2022.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
In Neural
Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library.
Information Processing Systems (NeurIPS). 2019.

Tim Rocktäschel and Sebastian Riedel. End-to-end diﬀerentiable proving. In Neural Information Processing

Systems (NeurIPS), 2017.

Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling.
Modeling relational data with graph convolutional network. In Extended Semantic Web Conference (ESWC),
2018.

Baoxu Shi and Tim Weninger. Proje: Embedding projection for knowledge graph completion. In AAAI

Conference on Artiﬁcial Intelligence, 2017.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems (NeurIPS),
2017.

Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio.

Graph Attention Networks. In International Conference on Learning Representations (ICLR), 2018.

13

Published in Transactions on Machine Learning Research (09/2022)

Jianfeng Wen, Jianxin Li, Yongyi Mao, Shini Chen, and Richong Zhang. On the representation and embedding
of knowledge bases beyond binary relations. In International Joint Conferences on Artiﬁcial Intelligence
(IJCAI), 2016.

Wenhan Xiong, Thien Hoang, and William Yang Wang. DeepPath: A reinforcement learning method for

knowledge graph reasoning. arXiv preprint arXiv:1707.06690, 2017.

Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin, Anand Louis, and Partha P. Taluk-
dar. HyperGCN: A new method for training graph convolutional networks on hypergraphs. In Neural
Information Processing Systems (NeurIPS), 2019.

Chaoqi Yang, Ruijie Wang, Shuochao Yao, and Tarek F. Abdelzaher. Hypergraph learning with line expansion.

arXiv preprint arXiv:2005.04843, 2020.

Pengcheng Yin and Graham Neubig. A syntactic neural model for general-purpose code generation.

In
Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pp. 440–450, 2017.

Donghan Yu and Yiming Yang. Improving hyper-relational knowledge graph completion. arXiv preprint

arXiv:2104.08167, 2021.

Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabás Póczos, Ruslan Salakhutdinov, and Alexan-

der J. Smola. Deep sets. In Neural Information Processing Systems (NeurIPS), 2017.

Ruochi Zhang, Yuesong Zou, and Jian Ma. Hyper-SAGNN: a self-attention based graph neural network for

hypergraphs. In International Conference on Learning Representations (ICLR), 2020.

14

Published in Transactions on Machine Learning Research (09/2022)

A Packing Hyperedges for HEAT

Hyperedges in HEAT have a variable size, i.e. a variable number of nodes that participate in each hyperedge.
For each hyperedge HEAT uses a multihead attention to compute the outgoing messages mei→nj . However,
multihead attention has a quadratic computational and memory cost with respect to the size of the hyperedge.

A naïve solution would require us to pad all hyperedges up to a maximum size. However, this would be
wasteful. On the other hand, we need to batch the computation across multiple hyperedges for eﬃcient GPU
utilisation. To achieve a good trade-oﬀ between large batch sizes (a lot of elements) and minimal padding, we
consider a small set “microbatches”. To pack the hyperedges optimally, we would solve the following integer
linear program (ILP)

min

1..k
X

j

(cid:0)L2

j · yj

(cid:1) −

X

l2
i

i

s.t.
yj ∈ {0, 1}, xij ∈ {0, 1}, ∀i, j

1..k
X

j

xij = 1, ∀i

xijsi ≤ yjLj, ∀j,

1..n
X

i

where yj is a binary variable indicating whether “bucket” j of size Lj (i.e. belonging in the microbatch
of size Lj) is used (i.e. contains any elements). Then, lj is the width of the jth hyperedge. xij are binary
variables for i = 1..n and j = 1..k indicating if the element i is in bucket j and si the width of bucket i.
The objective creates as few buckets as possible to minimise the wasted (quadratic) space/time needed for
multihead attention over variable-sized sequences. The ﬁrst constraint requires that each element is assigned
to exactly one bucket and the second constraint that each used bucket is ﬁlled up to its capacity.

However, the complexity of this ILP is prohibitive. Instead, we resort in using a greedy algorithm to select
the “microbatches” used. This is detailed in Algorithm 1.

Algorithm 1 Greedy Hyperedge Packing into Microbatches

buckets ← [ ]
for h in sortedDescendingByWidth(hyperedges) do

wasAdded ← False
for bucket in buckets do

if bucket.remainingSize ≥ h.width then

bucket.add(h)
wasAdded ← True
break

if not wasAdded then

bucketSize ← smallestFittingMicrobatchWidth(h.width)
newBucket ← createBucket(bucketSize)
newBucket.add(e)
buckets.append(newBucket)

return GroupBucketsToMicrobatches(buckets)

15

Published in Transactions on Machine Learning Research (09/2022)

if 1 is_foo 2 ( 3 x 4 ) 5 6 : 7 19

[INDENT] 8 x 9 = 10 foo 11 ( 12 x 13 ) 14 15 16 17 19

[DEDENT] 18 19

y 20 . 21 bar 22 23 ( 24 x 25 ) 26 27

Sample Hyperedges (Relations)
•Tokens(p1:n1, p2:n2, p3:n3, p4:n4, p5:n5, p6:n7, p7:n8, p8:n9, p9:n10, p10:n11, p11:n12,
p12:n13, · · · )

•AstNode(node:n19, test:n6, body:n17)

•AstNode(node:n17, value:n16, target:n9)

•foo(rval:n12, fzz:n9)
Assuming that foo is deﬁned as def foo(fzz): ...

•__getattribute__(rval:n23, self:n20, name:n22)

•MayRead(prev:n4, prev:n13, succ:n25)

•CtrlF(prev:n6, prev:n16, succ:n23)

•Symbol(sym:nx, occ:n4, occ:n9, occ:n13, may_last_use:n25)

Figure 3: Sample relations for the synthetic snippet shown on the top. The AST nodes and tokens are
wrapped in boxes and numbered appropriately in a preorder fashion. Only a few samples of the relations
(mapped to hyperedges) are shown below.

B Code as Hypergraph Example

Fig. 3 shows a synthetic code snippet with all the token and AST nodes enclosed in boxes. Some sample
relations are also shown. Finally, Fig. 4 shows a full hypergraph for the following code snippet

def foo(a, b):
if a in b:
a += 1

return a * 2

16

Published in Transactions on Machine Learning Research (09/2022)

Figure 4: A full hypergraph sample for the snippet disc Hyperedges are denoted as shaded (blue) boxes. Best
viewed on screen. The Tokens(·) hyperedge is omitted for clarity but the sequence of tokens is placed in the
box (right).

17

FunctionDefFunctionDef$rvalSymboloccurrenceReturnsfnControlFlowNextprevIndentedBlockbodyIndentedBlock$rvalParametersparamsParameters$rvalParamparams1Param$rvalParamparamsParam$rvalIfbodyIf$rvalSimpleStatementLinebody1SimpleStatementLine$rvalIndentedBlockbodyIndentedBlock$rvalComparisontestComparison$rval___assign$rvalComparisonTargetcomparisonsComparisonTarget$rval__contains__$rvalsourcenextControlFlowNextprevSimpleStatementLinebodySimpleStatementLine$rvalReturnbodyReturn$rvalfromnextReturnbodyReturn$rvalfromControlFlowNextnextBinaryOperationvalueBinaryOperation$rval__add__$rvalnextprevfoosymbolaSymbolsymbolbSymbolsymboldeffoo<DEDENT>(name):<INDENT>aif,nameoccurrenceMayWriteprevious_usesbcommanameoccurrenceMayWriteprevious_uses<INDENT>return:ainleftselfoccurrencenext_usesMayUselast_may_writesbcomparatoritemoccurrencemay_final_usenext_usesoperatora<DEDENT>valueoccurrencemay_final_usenext_usesusesreturna+1operatorrightotherleftselfoccurrencemay_final_usenext_usesuses