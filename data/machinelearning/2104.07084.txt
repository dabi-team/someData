1
2
0
2

t
c
O
7
1

]
E
M

.
t
a
t
s
[

2
v
4
8
0
7
0
.
4
0
1
2
:
v
i
X
r
a

Grouped Variable Selection with Discrete Optimization:
Computational and Statistical Perspectives

Hussein Hazimeh∗ Rahul Mazumder† Peter Radchenko‡

October 19, 2021

Abstract

We present a new algorithmic framework for grouped variable selection that is based on
discrete mathematical optimization. While there exist several appealing approaches based
on convex relaxations and nonconvex heuristics, we focus on optimal solutions for the (cid:96)0-
regularized formulation, a problem that is relatively unexplored due to computational chal-
lenges. Our methodology covers both high-dimensional linear regression and nonparametric
sparse additive modeling with smooth components. Our algorithmic framework consists of ap-
proximate and exact algorithms. The approximate algorithms are based on coordinate descent
and local search, with runtimes comparable to popular sparse learning algorithms. Our exact
algorithm is based on a standalone branch-and-bound (BnB) framework, which can solve the
associated mixed integer programming (MIP) problem to certiﬁed optimality. By exploiting
the problem structure, our custom BnB algorithm can solve to optimality problem instances
with 5 × 106 features and 103 observations in minutes to hours – over 1000 times larger than
what is currently possible using state-of-the-art commercial MIP solvers. We also explore sta-
tistical properties of the (cid:96)0-based estimators. We demonstrate, theoretically and empirically,
that our proposed estimators have an edge over popular group-sparse estimators in terms of
statistical performance in various regimes. We provide an open source implementation of our
proposed framework.

1

Introduction

Sparsity plays a ubiquitous role in modern statistical regression, especially when the number
of predictors is large relative to the number of observations.
In this paper, we focus on the
case where predictors have a natural group structure. Typical examples where such a structure
appears are models with multilevel categorical predictors and models that represent nonlinear
eﬀects of continuous variables using basis functions [19, 78, 32]. Grouping may also arise from
scientiﬁcally meaningful prior knowledge about the collection of the predictor variables. More
speciﬁcally, we consider the usual linear regression framework with response yn×1 and model
matrix Xn×p = [x1, . . . , xp]. We suppose that the p predictors are divided into q pre-speciﬁed,
non-overlapping groups. For a given β ∈ Rp and each g ∈ {1, ..., q}, we denote by βg the sub-
vector of β whose coeﬃcients correspond to the predictors in group g. Following the traditional

∗Google Research; work done while at the Massachusetts Institute of Technology, hazimeh@google.com
†Massachusetts Institute of Technology, rahulmaz@mit.edu
‡University of Sydney, peter.radchenko@sydney.edu.au

1

 
 
 
 
 
 
approach in high-dimensional regression, we assume that few of the regression coeﬃcients are
nonzero, i.e., the model is sparse. This leads to a natural generalization of the classical best
subset selection problem in linear regression [54, 12] to the group setting:

min
β

(cid:107)y − Xβ(cid:107)2

2 + λ0

q
(cid:88)

g=1

1(βg (cid:54)= 0),

(1)

where 1(·) is the indicator function, and λ0 is a non-negative regularization parameter that
controls the number of nonzero groups selected. We will refer to Problem (1) as the Group
(cid:96)0 problem.

Problem (1) is NP-Hard [57] and poses computational challenges. A rich body of prior work
explores sparsity-inducing methods to obtain approximate solutions to (1). Popular methods
include: convex optimization based procedures, such as Group Lasso [78], which is a generalization
of the Lasso approach [69] to the grouped setting, and local solutions to nonconvex optimization
problems arising from group-nonconvex regularizers, such as SCAD, MCP and others [80, 39].
Despite the appeal of these approaches, the statistical and computational aspects of optimal
solutions to (1) remain to be understood at a deeper level. To this end, we aim to advance the
computational frontiers of Problem (1) using novel tools from discrete optimization. Our proposed
combinatorial optimization-based algorithms are scalable. In particular, they can deliver optimal
solutions to (1) for instances that are much larger than state-of-the-art approaches. We also
develop a better understanding of the statistical properties of Problem (1) both theoretically and
empirically.

Computation. We propose new algorithms based on combinatorial optimization for solving
Problem (1) and its variants. First we present approximate algorithms: they deliver high-quality
solutions using a combination of cyclic coordinate descent and local combinatorial optimiza-
tion [33]. These algorithms have runtimes comparable to popular approaches for grouped variable
selection (for example, Group Lasso or MCP), but deliver solutions with considerably improved
statistical performance (for example, in terms of prediction and variable selection), as we demon-
strate in our experiments. Our approximate algorithms deliver good-quality feasible solutions
to (1) but are unable to certify (global) optimality of solutions via matching lower bounds on the
optimal objective value of (1). Certifying optimality is not only important from a methodological
perspective but can also be beneﬁcial in practice for mission-critical applications. For example,
having certiﬁably optimal solutions can engender trust and provide transparency in consequen-
tial applications such as healthcare. Thus, we propose a new tailored branch-and-bound based
optimization framework for solving (1) to certiﬁable optimality.

In our exact (global optimization) framework, we formulate the Group (cid:96)0 problem as a Mixed
Integer Program (MIP). However, in a departure from earlier work [12, 11], we propose a custom
branch-and-bound (BnB) algorithm to solve the MIP. Indeed, MIP-based techniques have gained
considerable traction recently to solve to (near) optimality the best subset selection problem,
where all groups are of size one [12, 11, 50, 52, 33, 77, 35]. All these works, with the exception
of [35], leverage capabilities of powerful commercial MIP solvers such as Gurobi and CPLEX.
These solvers have gained wide adoption in the past two decades due to major advances in
algorithms and software development [14, 40]. However, these general-purpose solvers may take
several hours to certify optimality on small instances (for example, with p = 1000). In contrast,
our custom BnB algorithm exploits problem-speciﬁc structure to scale to much larger instances.

2

For example, it can solve to optimality instances with p = 5 × 106 – this is 1000 times larger than
what can be handled using Gurobi’s MIP-solver. Our BnB algorithm generalizes to the grouped
setting the approach of [35] developed for the best subset selection problem.

Statistical properties. Statistical properties of Group Lasso have been extensively studied,
and it has been shown, both empirically and theoretically, that it performs well in sparse high-
dimensional settings [22, 3, 56, 37, 76, 46, 58], under certain assumptions on the data. However,
Group Lasso also has its shortcomings, similar to those of Lasso in high dimensional linear
regression [12, 19, 33]. More speciﬁcally, depending on the penalty weight, the resulting model
may either be very dense or, alternatively, comes with overly shrunk nonzero coeﬃcients. This
problem is aggravated when the groups are correlated with each other, as Group Lasso tends
to bring in all of the correlated groups in lieu of searching for a more parsimonious model. For
further discussions of these issues in the special case of Lasso see, for example, [81, 51, 19, 12], and
the references therein. In this paper, we demonstrate, both empirically and theoretically, that the
Group (cid:96)0 methodology has advantages over its Group Lasso counterpart in a variety of regimes. In
particular, as a consequence of directly controlling the sparsity level in the optimization problem,
our framework leads to substantially sparser models under similar data ﬁdelity. Moreover, in
many scenarios where the predictors are highly correlated, our approach performs better in terms
of both estimation and prediction.

Additive models with (cid:96)0-sparsity.
In addition to linear models, we also study an impor-
tant example of regression with group structure that arises in high-dimensional sparse additive
modeling [32, 31]. Here, we estimate a nonparametric multivariate regression function in q co-
variates, (x1, . . . , xq), which we model as a sparse additive sum of the form (cid:80)
j∈S fj(xj), where
S ⊂ {1, . . . , q}. In this setting, each group generally corresponds to the basis representation of
a given additive component, one for each of the q predictors. Because the groups are allowed to
be large, additional regularization needs to be imposed, typically in the form of a roughness type
penalty on the regression functions. A number of successful Group Lasso-based approaches have
been proposed and analyzed in this setting – see, for example, [53, 65, 38, 42, 63, 79] and the ref-
erences therein. To our knowledge, this is the ﬁrst paper to explore statistical and computational
aspects of Group (cid:96)0-based formulations in the context of sparse additive modeling. We show
theoretically and empirically that Group (cid:96)0 based methods enjoy certain statistical advantages
when compared to the Group Lasso-based counterparts.

Contributions. The focus of this paper is on Problem (1) and the sparse additive modeling
problem (which can be formulated as a variant of Problem (1), as we discuss in Section 2). Our
main contributions for these two problems can be summarized as follows:

• We develop fast approximate algorithms, based on ﬁrst-order and local combinatorial op-
timization. We establish convergence guarantees for these algorithms and provide useful
characterizations of the corresponding local minima. Our experiments indicate that these
algorithms can have an edge in terms of statistical performance over popular alternatives
for grouped variable selection.

• We present mixed integer second order cone program (MISOCP) formulations for the Group
(cid:96)0-based estimators; and design a novel specialized, nonlinear branch-and-bound (BnB)
framework for solving the MISOCP to global optimality. Our custom BnB solver can
handle instances with 5 × 106 features and 103 observations – more than a 1000 times larger

3

than what can be handled by state-of-the-art commercial MISOCP solvers.

• We establish non-asymptotic prediction and estimation error bounds for our proposed esti-
mators, for both the high-dimensional linear regression and sparse additive modeling prob-
lems. We show that under the assumption of sparsity, these error bounds compare favorably
with the ones for Group Lasso.

• We demonstrate empirically that our approach appears to outperform the state of the art
(for example, Group Lasso and available algorithms for nonconvex penalized estimators) in
a variety of high-dimensional regimes and under diﬀerent statistical metrics (for example,
prediction, estimation, and variable selection). We provide open-source implementations of
both our approximate and exact optimization algorithms1.

Organization.
In Section 2, we present formulations for the Group (cid:96)0 and sparse additive
modeling problems. Section 3 presents approximate algorithms based on ﬁrst-order and local
combinatorial optimization algorithms. Then, in Section 4, we present our exact MIP algorithm.
Statistical properties of our approach are investigated in Section 5. Section 6 presents compu-
tational experiments. Technical proofs and additional computational details are provided in the
supplement.

Notation. For any non-negative integer k, we denote the set {1, ..., k} by [k]. The complement
of a set A is denoted by Ac. We denote the index sets corresponding to the q groups of predictors
by Gg, for g ∈ [q] so that ∪q
g=1Gg = [p] and Gg ∩ G(cid:96) = ∅ for all g (cid:54)= (cid:96). For a vector θ, we use the
notation Supp(θ) to denote the group support, i.e., Supp(θ) = {g | θg (cid:54)= 0, g ∈ [q]}. We also
deﬁne a measure of (cid:96)0-group sparsity (i.e., number of nonzero groups): G(θ) := (cid:80)q
g=1 1(θg (cid:54)= 0).
We denote the gradient of a scalar-valued function, say J(θ), by ∇J(θ). Moreover, we use the
notation ∇θg J(θ) to refer to the subvector of ∇J(θ) corresponding to the variables in θg. Vectors
and matrices are denoted in boldface.

2 Optimization problems considered

In this section, we present optimization formulations for the Group (cid:96)0 approach (and its variants),
as well as the (cid:96)0-sparse additive function estimation approach.

2.1 Group (cid:96)0 with ridge regularization

The algorithms discussed in this paper apply to the Group (cid:96)0 estimator (1) with an optional ridge
regularization term:

min
β

(cid:107)y − Xβ(cid:107)2

2 + λ0

q
(cid:88)

g=1

1(βg (cid:54)= 0) + λ2(cid:107)β(cid:107)2
2,

(2)

where λ0 > 0 controls the number of selected groups, and λ2 ≥ 0 controls the strength of the
ridge regularization. Our proposed algorithms apply to both settings: λ2 = 0 and λ2 > 0 in
Problem (2). The choice of the ridge term in (2) is motivated by earlier work in the context
of best-subset selection [52, 33], which suggest that when the signal-to-noise ratio (SNR) is low,

1Our open-source code is available on github at https://github.com/hazimehh/L0Group.

4

additional ridge regularization can improve the prediction performance of best-subset selection
(both theoretically and empirically). Additionally, as discussed in Section 4.2, the choice λ2 > 0,
allows for deriving stronger MIP formulations by appealing to perspective formulations [28, 30].

2.2 Nonparametric additive models with (cid:96)0-sparsity
In the multivariate setting, estimating the conditional mean function E(y|x) = f (x1, . . . , xq)
becomes notoriously diﬃcult, due to curse of dimensionality. To overcome this problem, additive
approximation schemes [31] are commonly used as an eﬀective methodology: f (x) = (cid:80)q
j=1 fj(xj).
A popular approach [see, for example, 74] is to choose fj from some smooth functional class Cj,
such as the class of twice continuously diﬀerentiable functions. Given the observations (yi, xi),
i ∈ [n], the additive model f (x) can be estimated by solving the following optimization problem:

min
f

n
(cid:88)

(yi −

i=1

q
(cid:88)

j=1

fj(xij))2 + λ

q
(cid:88)

j=1

Pen(fj),

(3)

where Pen(fj) is a roughness penalty that controls the amount of smoothness in function fj.

A key ingredient in the additive function ﬁtting framework is the estimation of a univariate
smooth regression function based on observations (yi, ui), i ∈ [n]. Suppose, for simplicity, that
the uis are distinct and ui ∈ [0, 1] for all i. For illustration, let us take Pen(g) = (cid:82) 1
0 (g(cid:48)(cid:48)(u))2du.
Then, the solution to the corresponding (inﬁnite dimensional) univariate problem is of the form:
g(u) = α0 + α1u + (cid:80)n
j=1 γjNj(u), where Nj(u) are some cubic spline basis functions, such as
truncated power series functions, natural cubic splines or the B-spline basis functions, with knots
chosen at the distinct data points ui, i ∈ [n]. Note that (cid:82) 1
0 (g(cid:48)(cid:48)(u))2du = γ(cid:48)Ωγ, where Ω is an
n × n positive deﬁnite matrix with the elements ωij = (cid:82) 1
0 N (cid:48)(cid:48)
If we refer to the
corresponding functional class as C, deﬁne the elements of g as gi := g(ui), for i = 1, . . . , n, and
let (cid:107)g(cid:107)2

C := γ(cid:48)Ωγ, then the univariate optimization problem is equivalent to

i (u)N (cid:48)(cid:48)

j (u)du.

(ˆg1, . . . , ˆgm) = ˆg ∈ arg min (cid:107)y − g(cid:107)2

2 + λ(cid:107)g(cid:107)2
C.

(4)

Problem (4) is a generalized least squares problem in (α0, α1, γ). A direct extension to the
additive model setting is given by the following formulation:

min (cid:107)y −

q
(cid:88)

j=1

fj(cid:107)2

2 + λ

q
(cid:88)

j=1

(cid:107)fj(cid:107)2

Cj ,

(5)

where we minimize over fj ∈ Cj for all j, and fj = (fj(xij), . . . , fj(xnj)).

We wish to impose sparsity on the additive components fj, j ∈ [q], which naturally leads to

the following optimization problem:

min (cid:107)y −

q
(cid:88)

j=1

fj(cid:107)2

2 + λ0

q
(cid:88)

j=1

1(fj (cid:54)= 0) + λ

q
(cid:88)

j=1

(cid:107)fj(cid:107)2

Cj .

We note that the choice Pen(fj) =

(cid:113)(cid:82) (f (cid:48)(cid:48)

j (u))2du leads to the optimization problem

min (cid:107)y −

q
(cid:88)

j=1

fj(cid:107)2

2 + λ0

1(fj (cid:54)= 0) + λ

q
(cid:88)

j=1

(cid:107)fj(cid:107)Cj .

q
(cid:88)

j=1

5

(6)

(7)

j (cid:107)fj(cid:107)2
Cj

Problems (6) and (7) are close cousins and result in similar estimators. The terms (cid:80)
(cid:80)

j (cid:107)fj(cid:107)Cj and
encourage smoothness in each of the additive components, while the sum of indicators
directly controls the number of included predictors. In Section 5, we establish theoretical error
bounds for the estimator that corresponds to Problem (7).

Connections with Group Lasso-type penalization schemes. For Grouped Lasso-type
penalization schemes, the choice of the penalty becomes rather subtle. Problem (3) with
Pen(fj) = (cid:107)fj(cid:107)2
does not induce sparsity in (cid:107)fj(cid:107)Cj ’s for ﬁnite λ. Alternatively, the choice
Cj
Pen(fj) = (cid:107)fj(cid:107)Cj does result in several components (cid:107)fj(cid:107)Cj being set to zero when λ is large. Note,
however, that (cid:107)fj(cid:107)Cj = 0 does not imply fj = 0. This is because (cid:107)fj(cid:107)Cj
is a seminorm that
is not aﬀected by the linear components of fj. To set fj = 0 one needs to include the linear
components into the penalty. To overcome these limitations, alternatives have been proposed –
here we mention some penalization schemes that are used to encourage selection and smoothness.
2 + λ(cid:48)(cid:107)fj(cid:107)2
, where (cid:107)fj(cid:107)2 denotes the usual (cid:96)2 norm of
One possible choice [53] is Pen(fj) =
Cj
the vector fj. The corresponding penalization term is λ (cid:80)
j Pen(fj), and, hence, the parameters
λ and λ(cid:48) jointly control smoothness and sparsity. The sum of (cid:107)fj(cid:107)2
leads to double
penalization, thereby potentially resulting in unwanted shrinkage that may interfere with variable
selection. Similar issues arise with the choices Pen(fj) = (cid:107)fj(cid:107)2 + λ(cid:48)(cid:107)fj(cid:107)Cj , considered in [19], and
Pen(fj) =

, which appears in [53].

2 and λ(cid:48)(cid:107)fj(cid:107)2
Cj

(cid:107)fj(cid:107)2

(cid:107)fj(cid:107)2

(cid:113)

(cid:113)

2 + λ(cid:48)(cid:107)fj(cid:107)2
Cj

+ ˜λ(cid:107)fj(cid:107)2
Cj

Thus, the choice of Pen(fj) plays an important role in obtaining sparsity for Lasso-type reg-
ularization methods. In contrast, the levels of smoothness and sparsity are controlled separately
in the (cid:96)0-formulations: Problems (6) and (7). Group Lasso-type penalization schemes may be
interpreted as convex relaxations of the (cid:96)0-penalty appearing in Problem (7), as discussed in the
Supplement 9.

Other choices of smooth function classes. We note that the above framework, where each
additive component is taken to be a cubic spline, can be generalized to more ﬂexible smooth
nonparametric models, depending upon the choice of Pen(·) and the functional classes Cjs. For
example, one may consider the class of functions that are τ times continuously diﬀerentiable,
together with the choice Pen(fj) = (cid:82) f (τ )
(u)du, where f (τ ) denotes the τ th derivative of fj –
solutions to these problems are given by splines of order τ [74].

j

Another popular paradigm pursued in several works [42, 45, 64] is the Reproducing Kernel
Hilbert Space (RKHS) framework, wherein every Cj is taken to be a Hilbert space encouraging
some form of smoothness on fj. Here, Pen(fj) = (cid:107)fj(cid:107)Kj is an appropriate Hilbert space norm.

2.3 General problem formulation considered in this paper

Our focus in this paper is on Problem (2) and the sparse additive modeling problems deﬁned in
(6) and (7). These three problems can all be formulated as follows:

min
β

β(cid:48)Pβ + (cid:104)a, β(cid:105) + λ0G(β) + λ1

q
(cid:88)

g=1

(cid:107)Pgβg(cid:107)2,

(8)

for suitable choices of a, P (cid:23) 0, Pg (cid:31) 0, g ∈ [q], where we recall that G(β) := (cid:80)q
0). The term (cid:80)q
Problems (1) and (6) can be obtained by setting λ1 = 0 and choosing P and a appropriately.

g=1 1(βg (cid:54)=
g=1 (cid:107)Pgβg(cid:107)2 is only used for the sparse additive modeling problem in (7).

6

To simplify the presentation, we apply a change of variable in Problem (8): θg = P

g ∈ [q]. This leads to the following equivalent problem:

min
θ

h(θ) := θ(cid:48)Wθ + (cid:104)b, θ(cid:105)
(cid:125)

(cid:124)

(cid:123)(cid:122)
:=(cid:96)(θ)

+ λ0G(θ) + λ1

q
(cid:88)

g=1

(cid:107)θg(cid:107)2

,

(cid:124)

(cid:123)(cid:122)
:=Ω(θ)

(cid:125)

1
2

g βg for

(9)

for appropriately deﬁned2 W and b. For notational convenience, we deﬁne:

(cid:96)(θ) := θ(cid:48)Wθ + (cid:104)b, θ(cid:105) and Ω(θ) := λ0G(θ) + λ1

q
(cid:88)

g=1

(cid:107)θg(cid:107)2.

Our algorithmic development will focus on Problem (9).

Overview of our algorithms: Problem (9) is nonconvex due to the discontinuity in G(θ).
In Section 3, we design fast algorithms that can obtain high-quality approximate solutions for
this problem.
In Section 4, we develop an exact algorithmic framework, based on a custom
MIP solver, which obtains certiﬁably optimal solutions to (9). Our algorithm constructs: (i) a
sequence of feasible solutions, whose objective values are valid upper bounds, and (ii) a sequence
of lower bounds (a.k.a. dual bounds). As our BnB algorithm progresses, these upper and lower
bounds converge towards the optimal objective of Problem (9). The solver terminates and certiﬁes
optimality when the upper and lower bounds match3. Our experiments indicate that high-quality
initial solutions, as available from the algorithms presented in Section 3, can signiﬁcantly speed
up convergence and reduce memory requirements in our BnB algorithm.

3 Approximate Algorithms

In this section, we develop fast approximate algorithms to obtain high quality local minimizers for
Problem (9). While these algorithms do not deliver certiﬁcates of optimality (via dual bounds),
they attain nearly-optimal (and at times optimal) solutions to many statistically challenging
instances, in running times comparable to group Lasso-based algorithms.

A main workhorse of our approximate algorithms is a nonstandard application of cyclic block
coordinate descent (BCD) to the discontinuous objective function (9). We draw inspiration from
the appealing scalability properties of coordinate descent in sparse learning problems [see, for
example, 29, 5, 33]. Our second algorithm is based on local combinatorial search and is used to
improve the quality of solutions obtained by BCD. We establish convergence guarantees for these
two algorithms.

Our algorithms arise from studying necessary optimality conditions for Problem (9). To
this end, we show that the quality of solutions obtained by BCD are of higher quality than
local solutions corresponding to the popular proximal gradient descent (PGD) [61] algorithm4.

2Let D1 = diag(P
3In practice, MIP solvers terminate when the diﬀerence between the upper and lower bounds are below a small,

q ) be a block diagonal matrix. Then W = D−1

1 , and b = D−1

1 PD−1

1 , . . . , P

1 a.

1
2

1
2

user-deﬁned threshold.

4Though PGD is popularly used in the context of convex optimization problems, it also leads to useful algorithms
for nonconvex sparse learning problems. In particular, PGD for our problem can be viewed as a generalization of
the iterative hard thresholding (IHT) algorithm [16] to the group setting.

7

The local minimizers corresponding to local combinatorial search form a smaller subset of those
available from BCD. In this section, we establish the following hierarchy among the classes of
local minima:

Global Minima ⊆ Local Search Minima ⊆ BCD Minima ⊆ PGD Minima.

(10)

Above, PGD minima correspond to the ﬁxed points of the PGD algorithm; they include all
the ﬁxed points of our proposed BCD algorithm. As we move from right to left in the above
hierarchy, the classes become smaller, i.e., they impose stricter necessary optimality conditions.
At the top of the hierarchy we have the global minimizers of the problem, which can be obtained
using our exact MIP-based framework (we discuss this in Section 4). Our approximate algorithms
are inspired by recent work [33] on the sparse regression problem, but the approach presented
here has notable diﬀerences.
In particular, the coordinate descent algorithm in [33] performs
exact minimization per coordinate, which can be computationally expensive when extended to
the group setting. Thus, our proposed BCD algorithm performs inexact minimization per group.
In addition, the presence of (cid:96)2 norms in our objective function makes the analysis for the rate of
convergence for our algorithm diﬀerent.

3.1 Block Coordinate Descent

We present a cyclic BCD algorithm to obtain good feasible solutions to Problem (9) and establish
convergence guarantees. We ﬁrst introduce a useful upper bound for (cid:96)(θ). For every g ∈ [q], we
deﬁne Sg = {(θ, ˜θ) | θi = ˜θi, ∀i ∈ [q] s.t. i (cid:54)= g}. By the Block Descent Lemma [9], the following
upper bound holds for every g ∈ [q]:

(cid:96)(θ) ≤ (cid:96)(˜θ) + (cid:104)∇θg (cid:96)(˜θ), θg − ˜θg(cid:105) +

Lg
2

(cid:107)θg − ˜θg(cid:107)2
2,

∀(θ, ˜θ) ∈ Sg,

(11)

where Lg is the “group-wise” Lipschitz constant of ∇(cid:96)(θ), i.e., Lg is a constant which satisﬁes:
(cid:107)∇θg (cid:96)(θ) − ∇θg (cid:96)(˜θ)(cid:107)2 ≤ Lg(cid:107)θg − ˜θg(cid:107)2, for all (θ, ˜θ) ∈ Sg. Since (cid:96)(θ) is a quadratic function,
Lg = 2σmax(Wg), where Wg is the submatrix of W with columns and rows restricted to group
g, and σmax(·) denotes the largest eigenvalue.

Cyclic BCD sequentially minimizes the objective of (9) with respect to one group of variables
while the other groups are held ﬁxed. Let θl be the iterate obtained by the algorithm after the
l-th iteration. Then, in iteration l + 1, the variables in a group g (say), are updated while the
other groups are held ﬁxed. Speciﬁcally, we have (θl, θl+1) ∈ Sg. Using (11) with ˜θ = θl, ˆLg > Lg
and adding Ω(θ) to both sides we get:

h(θ) ≤ ˜g(θ; θl) := (cid:96)(θl) + (cid:104)∇θg (cid:96)(θl), θg − θl

g(cid:105) +

ˆLg
2

(cid:107)θg − θl

g(cid:107)2

2 + Ω(θ).

(12)

Note that the left hand side of (12) is the objective function of Problem (9). We obtain θl+1
minimizing the upper bound on our objective, ˜g(θ; θl), with respect to θg:

g

by

θl+1
g ∈ arg min

θg

˜g(θ; θl) = arg min

θg

ˆLg
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

θg −

(cid:16)

θl
g −

1
ˆLg

(cid:17)
∇θg (cid:96)(θl)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

+ Ω(θg).

(13)

8

Although nonconvex, the minimization problem in (13) admits a closed-form solution, which can
be obtained via the operator H : Ru → Ru deﬁned as follows:

H(z; λ; ˆLg) =

(cid:104)
(cid:107)z(cid:107)2 − λ1
ˆLg

(cid:105)




z
(cid:107)z(cid:107)2
0


if (cid:107)z(cid:107)2 >

otherwise

(cid:113) 2λ0
ˆLg

+ λ1
ˆLg

(14)

where λ = (λ0, λ1). It can be readily seen that an optimal solution of (13) is given by H(z; λ; ˆLg),
where z = θl

∇θg (cid:96)(θl). Below we summarize our proposed cyclic BCD algorithm.

g − 1
ˆLg

Algorithm 1: Cyclic Block Coordinate Descent (BCD)

• Input: Initialization θ0 and ˆLg for every g ∈ [q].

• Repeat Steps 1, 2 for l = 0, 1, 2, . . . until convergence:

1. g ← 1 + (l mod q) and θl+1

j ← θl
g ← H(z; λ; ˆLg), where z = θl

2. θl+1

j for all j (cid:54)= g
g − (1/ ˆLg)∇θg (cid:96)(θl).

Convergence Analysis. To establish convergence of the sequence {θl} in Algorithm 1, we make
use of the following assumption.

Assumption 1. At least one of the following conditions holds:

(a) Strong Convexity: W (cid:31) 0.

(b) Restricted Strong Convexity: Let ˆθ be a (Group Lasso) solution deﬁned as ˆθ ∈
g=1 (cid:107)θg(cid:107)2. Let k = maxθ{(cid:107)θ(cid:107)0 | G(θ) ≤ G(ˆθ)}. Every collection
arg minθ (cid:96)(θ) + λ1
of k columns in W are linearly independent, and the initial solution θ0 (in Algorithm 1)
satisﬁes h(θ0) ≤ h(ˆθ).

(cid:80)q

Assumption 1(a) holds if a ridge regularization term is used, i.e., it holds for Problem (2)
with λ2 > 0. Assumption 1(b) is less restrictive because we can have W (cid:23) 0. Suppose that
for some non-negative integer u, every set of u columns in W are linearly independent. Then,
in the Group Lasso problem (deﬁned in Assumption 1(b)), λ1 can be chosen suﬃciently large so
that some Group Lasso solution ˆθ satisﬁes k ≤ u. If ˆθ is used to initialize Algorithm 1, then
Assumption 1(b) is satisﬁed.

The following theorem establishes a linear convergence guarantee for the sequence generated

by Algorithm 1.

Theorem 1. Let {θl} be the sequence generated by Algorithm 1 and suppose that Assumption 1
holds. Then,

1. The group support stabilizes after a ﬁnite number of iterations, i.e., there exists an integer

K and a support S ⊆ [q] such that Supp(θl) = S for all l ≥ K.

9

2. The sequence {θl} converges to a solution θ∗, with Supp(θ∗) = S (as deﬁned in Part 1),

satisfying:

(cid:96)(θS) + λ1

(cid:88)

g∈S

(cid:107)θg(cid:107)2

θ∗
S ∈ arg min

θS
(cid:115)

(cid:107)θ∗

g(cid:107)2 ≥

2λ0
ˆLg

,

∀g ∈ S

(cid:107)∇θg (cid:96)(θ∗)(cid:107)2 ≤

(cid:113)

2λ0 ˆLg + λ1,

∀g ∈ Sc.

(15)

(16)

(17)

3. The function θS (cid:55)→ (cid:96)(θS) is strongly convex with a strong convexity parameter σS > 0. Let
LS be the Lipschitz constant of ∇θS (cid:96)(θS). Deﬁne ˆLmax = maxg∈S ˆLg + 2λ1 and ˆLmin =
ming∈S ˆLg + 2λ1. Then, for l ≥ K, the following holds:

h(θ(l+1)q) − h(θ∗) ≤

1 −

(cid:32)

(cid:33)

(cid:16)

σS
η

h(θlq) − h(θ∗)

(cid:17)

,

(18)

where η = 2 ˆLmax(1 + |S|(LS + 2λ1|S|)2 ˆL−2

min).

The proof of Theorem 1 is in the supplement. We present here a high-level sketch of the
proof. We establish part 1 by proving a suﬃcient decrease condition. For part 2, we show that
the objective function restricted to the group support S is strongly convex, and thus convergence
follows from standard results on cyclic BCD, e.g., [9]. To establish the linear rate of convergence
in part 3 of the theorem, we extend the result of [6] who show that cyclic BCD can achieve a linear
rate of convergence on smooth and strongly convex functions: note that our objective function
after support stabilization is not smooth due to the presence of the term (cid:80)
Optimality conditions of BCD and PGD. The conditions in Theorem 1 (part 2) characterize
a ﬁxed point of Algorithm 1. These are necessary optimality conditions for Problem (9) since any
global minimizer must be a ﬁxed point for Algorithm 1. In what follows, we will show that the
necessary optimality conditions imposed by PGD (which is a generalization of [16] to the group
setting) are generally less restrictive compared to those imposed by Algorithm 1. Note that PGD
is an iterative algorithm whose updates for Problem (9) are given by:

g∈S (cid:107)θg(cid:107)2.

θl+1 ∈ arg min

θ

(cid:26) 1
2τ

(cid:107)θ − (θl − τ ∇(cid:96)(θl))(cid:107)2

2 + Ω(θ)

(cid:27)

,

(19)

where τ > 0 is a step size. Let L be the Lipschitz constant of ∇(cid:96)(θ). For a constant step size,
the update in (19) converges if τ = 1/ ˆL where ˆL is a constant chosen such that ˆL > L [see,
for example, 33, 47]. For the choice τ = 1/ ˆL, it can be readily checked that any ﬁxed point of
PGD satisﬁes the three optimality conditions in Theorem 1 (part 2), but with ˆLg replaced by ˆL.
The group-wise Lipschitz constant Lg satisﬁes Lg ≤ L (for any g). In many high-dimensional
problems, we can have Lg (cid:28) L [see 5, 33]. Thus, Algorithm 1 generally imposes more restrictive
necessary optimality conditions compared to PGD, which can lead to higher quality local minima
in practice. This establishes a part of the hierarchy in (10).

10

3.2 Local Combinatorial Search

In this section, we introduce a local combinatorial search algorithm to improve the quality of
solutions obtained by cyclic BCD (Algorithm 1). The algorithm performs the following two steps
in the t-th iteration:

1. Block Coordinate Descent: We run Algorithm 1 initialized at the current solution θt−1
to obtain a solution θt. We denote the indices of the nonzero groups in θt by Supp(θt) = S.
2. Group Combinatorial Search: We attempt to improve the solution θt by swapping
groups of variables from inside and outside the support S. In particular, we search for two
subsets S1 ⊆ S and S2 ⊆ Sc such that removing S1 from the support, adding S2 to the
support, and then optimizing over the groups in S2, improves the current objective. To
ensure that the local search problem is computationally feasible, we restrict our search to
subsets satisfying |S1| ≤ m and |S2| ≤ m, where m is a pre-speciﬁed integer that takes
relatively small values (for example, in the range 1 to 10).

We present a formal description of the optimization problem in step 2 (above). We denote
the standard basis of Rp by {e1, . . . , ep}. Given a set J ⊆ [q], we deﬁne the p × p matrix UJ
as follows: the i-th column of UJ is ei if i ∈ ∪g∈J Gg and 0 otherwise. In other words, for any
θ ∈ Rp, we have (UJ θ)i = θi if i ∈ ∪g∈J Gg and 0 otherwise. The optimization problem in Step 2
is given by:

min
S1,S2,θ

h(θt − US1θt + US2θ)

s.t.

S1 ⊆ S, S2 ⊆ Sc, |S1| ≤ m, |S2| ≤ m,

(20)

where we recall that S = Supp(θt). If there is a feasible solution ˆθ to (20) satisfying h(ˆθ) < h(θt),
then we move to the improved solution ˆθ; otherwise, we terminate the algorithm. We summarize
the algorithm below:

Algorithm 2: Local Combinatorial Search

• Input: Initial solution θ0 and swap subset size m.

• Repeat Steps 1–3 for t = 1, 2, . . . until convergence:

1. Run Algorithm 1 initialized from θt−1 to obtain a solution θt.
2. Search for a feasible solution ˆθ to (20) satisfying h(ˆθ) < h(θt).
3. If step 2 succeeds, θt ← ˆθ. Otherwise, terminate.

Theorem 2 establishes that Algorithm 2 converges in a ﬁnite number of iterations and char-

acterizes the corresponding solution.

Theorem 2. Let {θt} be the sequence of iterates generated by Algorithm 2 and suppose Assump-
tion 1 holds. Then, θt converges in a ﬁnite number of iterations to a solution that we denote
by θ†. Let S = Supp(θ†). Then, θ† satisﬁes the necessary optimality conditions in part 2 of
Theorem 1. In addition, θ† satisﬁes:

h(θ†) ≤ min
S1,S2,θ

h(θ† − US1θ† + US2θ)

s.t.

S1 ⊆ S, S2 ⊆ Sc, |S1| ≤ m, |S2| ≤ m.

(21)

11

Theorem 2 shows that the solutions obtained by Algorithm 2 impose more restrictive necessary
optimality conditions (in particular, condition (21)) compared to Algorithm 1, which justiﬁes
part of the hierarchy in (10). This is expected, as every iteration of Algorithm 2 improves over
a solution obtained by Algorithm 1. The quality of solutions returned by Algorithm 2 depends
on the swap subset size m. For a suﬃciently large choice of m, the algorithm will return a
global minimizer. Intuitively, the computational cost of the local search in step 2 of Algorithm
2 increases with m. In our experiments, we observe that small choices such as m = 1 can lead
to signiﬁcant improvements in solution quality compared to algorithms that do not incorporate
combinatorial optimization. These improvements are most pronounced in settings where n (cid:28) p or
the predictors across groups are highly correlated. In Section 4.1.2, we present a MIP formulation
for the local search problem in Algorithm 2 for m > 1. For the special case of m = 1, we use our
own custom implementation that is more eﬃcient than using a MIP-based approach.

3.3 Algorithms for the cardinality constrained formulation

Algorithms 1 and 2 provide solutions for the (penalized) formulation in (9). While this leads to a
family of high-quality estimators across a range of model sizes, it does not allow for explicit control
over the number of nonzero groups G(θ). To this end, we consider the cardinality constrained
variant of problem (9):

min
θ

E(θ) := (cid:96)(θ) + λ1

(cid:88)

g∈[q]

(cid:107)θg(cid:107)2

s.t. G(θ) ≤ k.

(22)

In order to obtain a solution to (22) with a desired support size, we propose the following
procedure. First, we run Algorithm 2 (say) over a grid of λ0-values to obtain a sequence of
solutions. Then, if a desired support size, say k, is missing, we obtain it by applying proximal
gradient descent (PGD) to Problem (22):

θl+1 ∈ arg min
θ: G(θ)≤k






1
2τ

(cid:107)θ − (θl − τ ∇(cid:96)(θl))(cid:107)2

2 + λ1

(cid:88)

g∈[q]

(cid:107)θg(cid:107)2






,

(23)

where τ > 0 is a step size and the initial solution θ0 can be obtained from Algorithm 2 (for
example, we take a solution with group support size closest to k).

The next proposition establishes the convergence of update (23) and describes its ﬁxed points.

Proposition 1. Let {θl} be the sequence of iterates generated the PGD updates (23). Let L be
the Lipschitz constant of ∇(cid:96)(θ) and a scalar ˆL such that ˆL > L. Then, {θl} converges for a step
size τ = 1/ ˆL. Moreover, a solution θ∗ with group support S is a ﬁxed point of (23) iﬀ G(θ∗) ≤ k,
and

θ∗
S ∈ arg min

θS

E(θS)

and

(cid:107)∇θg (cid:96)(θ∗)(cid:107)2 ≤ γ(k)

for g ∈ Sc,

where γg = (cid:107) ˆLθ∗

g − ∇θg (cid:96)(θ∗)(cid:107)2, and γ(k) denotes the kth largest value in the sequence {γg}q

g=1.

We omit the proof of Proposition 1 as it can be established by a simple extension to the

standard results on the convergence of IHT [for example, those in 16, 5].

12

4 Mixed Integer Programming

In this section, we propose MIP formulations and algorithms to solve (9) and the combinatorial
search problem in Algorithm 2. Section 4.1 introduces MIP formulations, and Section 4.2 presents
a new BnB algorithm for solving the corresponding problems to optimality.

4.1 MIP Formulations

4.1.1 Formulations for Problem (9)

Below we present two MIP-formulations for (9).
Big-M Formulation: We ﬁrst present a Big-M based MIP formulation for Problem (9):

min
θ,z

(cid:96)(θ) + λ0

q
(cid:88)

g=1

zg + λ1

q
(cid:88)

g=1

(cid:107)θg(cid:107)2

s.t. (cid:107)θg(cid:107)2 ≤ Muzg, g ∈ [q]
zg ∈ {0, 1} , g ∈ [q]

(24a)

(24b)

(24c)

where, the optimization variables are θ (continuous) and z (binary). Above, Mu is an a-priori
speciﬁed constant (leading to the name “Big-M”) such that some optimal solution, say θ∗, to (9)
satisﬁes maxg∈[q] (cid:107)θ∗
g(cid:107)2 ≤ Mu. In (24), the binary variable zg controls whether all the regression
coeﬃcients in group g are zero or not: zg = 0 implies that θg = 0, and zg = 1 implies that
(cid:107)θg(cid:107)2 ≤ Mu. Such Big-M formulations are commonly used in mixed integer programming to
model relations between discrete and continuous variables, and have been recently used in (cid:96)0-
regularized regression [12, 77] (for example). Various techniques have been proposed to estimate
the constant Mu in practice; see [12] for a discussion on estimating the Big-M in the context of
linear regression. The constraints in (24b) are second order cones [17]. Moreover, the objective
function in (24) can be written as a linear function, with additional second order cone constraints
to express the quadratic function (cid:96)(θ) and the terms (cid:107)θg(cid:107)2, g ∈ [q]. Thus, Problem (24) can be
reformulated as a Mixed Integer Second Order Cone Program (MISOCP), which can be modeled
and solved (for small/moderate problem instances) with commercial MIP solvers such as Gurobi,
CPLEX, and MOSEK. We present an eﬃcient, standalone BnB algorithm for (24) in Section 4.2.

Perspective reformulation: Recall that Problem (9) contains a ridge term in its objective.
The ridge term can be used to derive stronger MIP formulations for (9) based on the perspective
formulation [28, 30]. As we discuss below, the perspective-based formulation diﬀers from the Big-
M formulation (24)—when λ2 > 0, it usually leads to tighter convex relaxations and consequently,
reduced MIP runtimes. First, we rewrite (9) as

min
θ,z

˜(cid:96)(θ) + λ0

q
(cid:88)

g=1

zg + λ1

q
(cid:88)

g=1

(cid:107)θg(cid:107)2 + λ2

q
(cid:88)

g=1

(cid:107)θg(cid:107)2
2

s.t.

(24b), (24c)

(25)

where (cid:96)(θ) = ˜(cid:96)(θ) + λ2(cid:107)θ(cid:107)2

2. Using the perspective reformulation [28, 30, 27] for the ridge term

13

(cid:80)

g∈[q] (cid:107)θg(cid:107)2

2 in the objective, we can reformulate (25) as

min
θ,z,s

˜(cid:96)(θ) + λ0

q
(cid:88)

g=1

zg + λ1

q
(cid:88)

g=1

(cid:107)θg(cid:107)2 + λ2

q
(cid:88)

g=1

sg,

s.t. (cid:107)θg(cid:107)2 ≤ Muzg, g ∈ [q]
(cid:107)θg(cid:107)2
2 ≤ sgzg, g ∈ [q]
zg ∈ {0, 1} , sg ≥ 0, g ∈ [q].

(26a)

(26b)

(26c)

(26d)

Compared to (25), formulation (26) uses additional auxiliary variables sg ∈ R≥0, g ∈ [q] and
rotated second order cone constraints: (cid:107)θg(cid:107)2
2 ≤ sgzg for g ∈ [q]. Each sg takes the place of the
2 in the objective function in (24). Speciﬁcally, any optimal solution (θ∗, z∗, s∗) to (26)
term (cid:107)θg(cid:107)2
must satisfy s∗

g = (cid:107)θ∗

g(cid:107)2
2.

Although the MIP formulations (26) and (25) are equivalent, their continuous relaxations
are generally diﬀerent. The following proposition states that the relaxation of (26) is generally
tighter (i.e., has a higher objective) than the relaxation of (25).

Proposition 2. Let v1 and v2 be the objective values of (25) and (26) upon relaxing the binary
variable zg to [0, 1] for all g ∈ [q]. Let (θ∗, z∗, s∗) be an optimal solution to the relaxation
corresponding to v2. Then, the following holds:
(cid:88)

(cid:16)

(cid:17)

v2 − v1 ≥ λ2

(cid:107)θ∗

g(cid:107)2
2

(z∗

g )−1 − 1

.

g∈[q]|z∗

g >0

Proposition 2 implies that using formulation (26) (over formulation (24)) can lead to tighter
lower bounds for the root node relaxation; and hence tighter dual bounds for the node relax-
ations in the BnB tree. This can result in improved runtimes in the overall BnB solver (as we
demonstrate in our experiments). Thus, in our algorithmic framework in Section 4.2, we focus
on formulation (26). To be clear, our BnB procedure applies even without the presence of a ridge
term (i.e., λ2 = 0). Speciﬁcally, if λ2 = 0 in (26), the conic constraints (26c) can be removed and
formulation (26) reduces to the Big-M formulation in (24).

4.1.2 MIP formulation for local combinatorial search

We present a MIP formulation for the local search problem5 that arises in Algorithm 2. Prob-
lem (20) can be formulated using the following Big-M based MIP:

min
u,z,θ

(cid:96)(u) + λ0

s.t. u = θt −

q
(cid:88)

g=1
(cid:88)

zg + λ1

q
(cid:88)

g=1

(cid:107)ug(cid:107)2

Ugθt(1 − zg) +

g∈S
(cid:107)ug(cid:107)2 ≤ Muzg, g ∈ Sc
(cid:88)
(cid:88)

zg ≥ |S| − m,

g∈S

g∈Sc

zg ∈ {0, 1} , g ∈ [q].

zg ≤ m

Ugθ

(cid:88)

g∈Sc

(27a)

(27b)

(27c)

(27d)

5We recommend the use of the MIP formulations when m ≥ 2. When m = 1 a solution to the local search

procedure can be computed eﬃciently from ﬁrst principles.

14

In the formulation above, we assume that Mu is chosen suﬃciently large so that some optimal
solution to (20), say θ∗, satisﬁes (cid:107)θ∗
g(cid:107)2 ≤ Mu, g ∈ Sc. As we discuss below, the objective in (27)
represents h(u) with u = θt − US1θt + US2θ, where h(u), S1 and S2 are as deﬁned in (20). Note
that the variable u is an auxiliary variable introduced to simplify the presentation. The binary
variables zg, g ∈ [q] are used to select the subsets S1 ⊆ S and S2 ⊆ Sc. In particular, for g ∈ S,
zg = 0 iﬀ g ∈ S1, and this is encoded by constraint (27a). On the other hand, for g ∈ Sc, zg = 1
iﬀ g ∈ S2, and this is encoded by constraints (27a) and (27b). Therefore, (cid:80)q
g=1 zg is equal to
G(u). The constraints (27c) enforce |S1| ≤ m and |S2| ≤ m.

The local search MIP-formulation (27) has a smaller search space compared to the full prob-
lem (24). This is due to the additional constraints appearing in (27c). Furthermore, Problem (27)
eﬀectively uses |Sc|-many ‘free’ continuous group-variables—this is in contrast to |S| + |Sc| con-
tinuous group-variables appearing in the full problem. Thus, for small values of m, Problem (27)
can be typically solved faster than the MIP formulation of (8). While (27) is based on a Big-M
formulation, in the presence of an additional ridge regularizer, one can also derive a perspective
reformulation using ideas similar to (26).

4.2 Exact optimization via a custom nonlinear Branch-and-Bound algorithm

High-performance commercial MIP solvers, such as Gurobi and CPLEX, often deliver state-of-
the-art performance for a variety of MIP problems. These solvers are based on a BnB framework,
which can solve MIP problems to global optimality, typically without having to explicitly enu-
merate all (exponentially many) solutions in the search space. These solvers are general-purpose
and do not take into account the speciﬁc structure of the problems we consider here. Therefore,
their performance can suﬀer: we have empirically observed that they may require several hours
to solve (to certiﬁable optimality) instances of (26) with p ∼ 103, and larger problems can take
much longer.

To address this lack of scalability in general-purpose MIP solvers, we propose a specialized,
nonlinear BnB framework for solving (26) to certiﬁable optimality. Our framework takes into
account problem structure to achieve scalability. As we demonstrate in the experiments section,
our BnB can solve instances with p ∼ 5 × 106 to certiﬁable optimality in minutes to hours,
whereas Gurobi takes prohibitively long (at least a day) for p ∼ 103. An important feature of our
proposal is an open-source, standalone implementation of the BnB solver, which does not rely
on sophisticated and proprietary BnB-capabilities of commercial MIP solvers (e.g., Gurobi). We
ﬁrst give a high-level overview of our novel nonlinear BnB framework and then dive into speciﬁc
technical details.

Overview of nonlinear BnB: Nonlinear BnB is a general framework for solving mixed integer
nonlinear programs [8]. This framework constructs a search tree to partition the set of feasible
solutions of the given MIP (Problem (26) in our case). Instead of explicitly enumerating all the
(exponentially many) feasible solutions, BnB uses intelligent enumeration and methods to prune
parts of the tree by using lower bounds (dual bounds) on the optimal objective value. In what
follows, we brieﬂy describe how the tree is constructed and pruned. Starting at the root node,
the algorithm solves a nonlinear convex relaxation of Problem (26), where all binary variables
are relaxed to [0, 1] – this is usually referred to as the root relaxation. Then, the algorithm
chooses a branching variable, say zg, and creates two child nodes (optimization subproblems):
one with zg = 0 and another with zg = 1, where all other binary variables are relaxed to [0, 1].

15

The algorithm then proceeds recursively: for every unvisited node, it solves the corresponding
optimization problem and checks if there is any fractional (i.e., non-binary) variable zg. If there is
any fractional zg, the branching process must continue — to this end, the algorithm branches on
one fractional zg, generating two new child nodes. Thus, every node in the search tree corresponds
to an optimization subproblem and every edge represents a branching decision.

While growing the search tree, BnB maintains an upper bound on the objective function
(which can be obtained from any feasible solution to the problem). If the optimization subproblem
at the current node leads to an objective value that exceeds the upper bound, then the node is
pruned (i.e., no children are generated for this node), because none of its descendants can have a
better objective value than the upper bound. Another case where BnB can safely prune a node
is when the corresponding subproblem leads to an integral solution, i.e., a binary z (since there
will be no variables to branch on). For further discussion on nonlinear BnB, see [8].

Speciﬁc details: There are many delicate details in BnB that can critically aﬀect its scalability:
for example, the choice of the algorithm for solving the continuous node subproblems, obtaining
upper bounds, branching, and tree-search strategies. We discuss our choices below:

• Subproblem solver: The optimal solutions of the continuous optimization subproblems
encountered in the course of BnB are typically sparse (see Section 4.2.1 for further dis-
cussions). To solve these subproblems, we propose an active-set algorithm, which exploits
sparsity by considering a reduced problem restricted to a small subset of groups. Moreover,
we share information on the active sets across the BnB tree to speed up convergence (see
Section 4.2.2).

• Upper bounds: Better upper bounds can lead to aggressive pruning in the search tree,
which can reduce the overall runtime. We obtain the initial upper bound using the approx-
imate algorithms of Section 3. As we demonstrate in the experiments, our approximate
algorithms typically obtain optimal or near-optimal solutions, making them a good choice
to initialize BnB. Moreover, at every node of BnB, we attempt to improve the upper bound
by using the sparsity pattern of the solution to the current node’s subproblem. More con-
cretely, let S ⊆ q denote the group support of the latter subproblem’s solution. Then, we
obtain a new upper bound, by restricting optimization to S, i.e., we solve:

min
θ

˜(cid:96)(θ) + λ1

q
(cid:88)

g=1

(cid:107)θ(cid:107)2 + λ2(cid:107)θ(cid:107)2
2

s.t.

θSc = 0, (cid:107)θg(cid:107)2 ≤ Mu, g ∈ [q].

• Branching and search strategies: The branching strategy selects the next variable to
branch on, while the search strategy decides which unexplored node in the search tree to
visit next. Many elaborate strategies for branching and search have been proposed in the
literature – see [55] for a survey. When the initial upper bound is of high quality, more
aggressive pruning is possible, and simple strategies tend to work relatively well in practice
[for example, see the discussion in 23]. Since our approximate algorithms typically return
good upper bounds, we rely on simple strategies. For branching, we use maximum fractional
branching [8, 55], which branches on the factional variable zg whose value is closest to 0.5.
For search, we use breadth-ﬁrst search and switch to depth-ﬁrst search if memory issues are
encountered.

16

Our approach extends our recent work [35] for the best subset selection problem (with a group
size of one). We note that there are important diﬀerences as the Group (cid:96)0 problem involves a
diﬀerent and more challenging optimization formulation. Speciﬁcally, the Big-M constraints in
(26b) translate to second order cones, instead of box-constraints that appear when the group
sizes are one. Furthermore, in the group setup, we have a non-smooth term (cid:80)
g∈[q] (cid:107)θg(cid:107)2 in the
objective of (26). The conic constraints and (cid:96)2 norms in our problem require special care when
developing the subproblem solver (for example, when reformulating the subproblems in Section
4.2.1 and designing the active set algorithm in Section 4.2.2). It is also worth mentioning that in
the simplest case where λ1 = λ2 = 0, our solver solves a MISOCP, whereas [35] solves a mixed
integer quadratic program.

4.2.1 Relaxation reformulation

In this section, we study the convex relaxation arising at a node of the BnB search tree. We
present a particular reformulation of this problem that leads to (i) useful insights about the
sparsity in the solutions of the convex relaxation; and (ii) computational beneﬁts. To simplify
the presentation, we will ﬁrst focus on the root relaxation of (26), which is obtained by relaxing
all the binary variables in (26) to [0, 1].

Note that the root relaxation involves the variables (β, z, s). In Proposition 3, we show that
the root relaxation can be reformulated in the β space, leading to a regularized least squares
problem. The associated regularizer can be characterized in terms of the reverse Huber penalty
[60] (see also [27]), which is a function H : R → R deﬁned as follows:

H(t) =

(cid:40)

|t|
(t2 + 1)/2

if |t| ≤ 1

otherwise.

(28)

Proposition 3. The root relaxation obtained by relaxing the binary variables in (26) to [0, 1] is
equivalent to:

min
θ

F (θ) := ˜(cid:96)(θ) +

q
(cid:88)

g=1

where λ = (λ0, λ1, λ2) and

Ψ(θg; λ, Mu) s.t. (cid:107)θg(cid:107)2 ≤ Mu, g ∈ [q].

(29)

Ψ(θg; λ, Mu) :=

(cid:40)

2λ0H((cid:112)λ2/λ0(cid:107)θg(cid:107)2) + λ1(cid:107)θg(cid:107)2
(λ0/Mu + λ1 + λ2Mu)(cid:107)θg(cid:107)2

if (cid:112)λ0/λ2 ≤ Mu
if (cid:112)λ0/λ2 > Mu.

The reformulation in (29) eliminates the the conic and Big-M constraints from the root re-
laxation, at the expense of introducing the non-smooth penalty (cid:80)q
g=1 Ψ(θg; λ, Mu) which is
separable across the blocks {θg}q
1. Depending on the choices of λ and Mu, the penalty Ψ is
either the (cid:96)2 norm or a combination of the reverse Huber penalty and the (cid:96)2 norm. In either case,
the penalty is sparsity-inducing. In essence, Problem (29) is similar to the Group Lasso problem
[78], with two exceptions: (i) Problem (29) has the additional constraints: (cid:107)θg(cid:107)2 ≤ Mu, g ∈ [q],
and (ii) when (cid:112)λ0/λ2 ≤ Mu, the penalty involves the reverse Huber penalty.
Node relaxations within the BnB tree: The convex relaxation subproblem encountered at
a node of the BnB search tree is similar to the root relaxation, except that some of the zgs are

17

ﬁxed to 0 or 1. The ﬁxed zgs are determined by the branching decisions made starting from the
root until reaching the node. The convex relaxation at a particular node can be reformulated in
the β-space similar to the reformulation of the root relaxation in (29), except that: (i) if zg = 0
then the corresponding group should be removed from the objective function; and (ii) if zg = 1,
then the penalty Ψ(θg; λ, Mu) should be replaced with ˜Ψ(θg; λ) := λ1(cid:107)θg(cid:107)2 + λ2(cid:107)θg(cid:107)2
2. More
precisely, let Z and N be the sets of indices of the zgs that are ﬁxed to 0 and 1, respectively.
Then, the following subproblem is solved at the corresponding node:

˜(cid:96)(θ) +

min
θ

(cid:88)

g∈N c

Ψ(θg; λ, Mu) +

(cid:88)

g∈N

˜Ψ(θg; λ) s.t. θZ = 0, (cid:107)θg(cid:107)2 ≤ Mu, g ∈ [q].

(30)

In the next section, we develop a scalable algorithm for solving Problem (29). The BnB subprob-
lem (30) can be solved similarly after accounting for the ﬁxed zgs.

4.2.2 Active-Set subproblem solver

As discussed earlier, a solution to Problem (29) is expected to be sparse in θ (this will be also
true for the node sub-problems in the BnB tree). To exploit this sparsity, we use an active-set
algorithm: We start by solving Problem (29) restricted to a small subset of groups (i.e., the active
set). After convergence on the active set, we augment the active set with a collection of groups
that violate the optimality conditions for the full problem (if any) and then resolve the problem
restricted to the augmented active set. The algorithm keeps iterating between solving a reduced
optimization problem and augmenting the active set, until the optimality conditions for the full
problem are satisﬁed. Such active-set algorithms have proven to be eﬀective in scaling up the
solvers for group Lasso-type problems [for example, see 34]—our usage diﬀers in that we use this
active-set strategy within every node of the BnB tree.

Next, we describe our active-set algorithm more formally. Let A ⊆ [q] be the active set. The

algorithm starts by solving (29) restricted to the active set, i.e.,

ˆθ ∈ arg min

θ

F (θ) s.t. (cid:107)θg(cid:107)2 ≤ Mu, g ∈ [q], θAc = 0.

(31)

After solving (31), we check if ˆθ satisﬁes the optimality condition for the full problem. Equiva-
lently, for every group g ∈ Ac, we check if the following holds

0 ∈ arg min

θg

F (ˆθ1, . . . , θg, . . . , ˆθq) s.t. (cid:107)θg(cid:107)2 ≤ Mu.

(32)

Since θg = 0 is in the interior of the feasible set, condition (32) is equivalent to the zero-
subgradient condition: 0 ∈ ∂θg F (ˆθ1, . . . , ˆθg−1, 0, ˆθg+1, . . . , ˆθq), and can be checked in closed
form.

We repeat the procedure of solving the restricted subproblem in (31) and augmenting A with
groups that violate (32), until there are no more violations. The algorithm is summarized below.

Algorithm 3: An Active-set Algorithm for (29)

• Input: Initial solution ˆθ and initial active set A.

• Repeat Steps 1–3 till convergence:

18

1. Solve the restricted problem (31) to get a solution ˆθ.
2. V ← {g ∈ Ac | (32) is violated}.
3. If V is empty terminate, otherwise6, A ← A ∪ V.

Algorithm 3 is guaranteed to converge to an optimal solution for Problem (29) in a ﬁnite number
of steps, as there are ﬁnitely many groups.

Choice of the active set: The quality of the initial active set A can have a important eﬀect on
the number of iterations in Algorithm 3. Due to the choice of our branching rule, the parent and
its two child nodes solve similar subproblems; the only diﬀerence between these subproblems is
that a single zg is ﬁxed to 0 or 1 in the children. Thus, the solutions and supports of the parent
and its children are unlikely to diﬀer by much. We therefore initialize the active set of every node
in the BnB tree (except the root) with the support of its parent. For the root node, we initialize
the active set with the support of the warm start, obtained from the approximate algorithms that
are discussed in Section 3.

Solving the restricted subproblem: The convex sub-problem (31) in Step 1 has a small active
set and can be solved with a variety of optimization algorithms: for example, BCD, proximal
gradient methods [9] or an interior point solver (as available in Gurobi). In our experiments, we
use the latter due to its good performance in practice.

5 Statistical Theory

In this section we derive non-asymptotic prediction and estimation error bounds for the Group (cid:96)0
estimators, and compare them to the bounds that have been established for the corresponding
Group Lasso-based approaches. We focus on linear regression models in Section 5.1 and on
nonparametric additive models in Section 5.2.

In our analysis, we focus on constrained speciﬁcations of the proposed estimators, leaving the
penalized case for future research. To simplify the presentation, we consider the setting where
the model is correctly speciﬁed, so that the true regression function is a feasible solution to the
corresponding optimization problem. However, our results can be generalized to allow for model
misspeciﬁcation.

We say that a constant is universal if it does not depend on other parameters, such as n, q or k.
We use the notation (cid:38) and (cid:46) to indicate that inequalities ≥ and ≤, respectively, hold up to pos-
itive universal multiplicative factors, and write (cid:16) when the two inequalities hold simultaneously.
We use a ∨ b to denote max(a, b).

5.1 Linear Model

We assume that the observed data follows the model y = Xβ∗ + (cid:15), where X is deterministic
and the elements of (cid:15) are independent N (0, σ2) with σ > 0. We deﬁne k∗ = G(β∗) and refer to
2 as the prediction error for estimator (cid:98)β. Given β ∈ Rp and J ⊆ [q], we write βJ
n−1(cid:107)X(cid:98)β − Xβ∗(cid:107)2
6In some cases, |V| can be large, which can slow down the solver in Step 1. Thus, if V has more than K groups,
we augment A with the K groups in V that have the largest violation (instead of A ← A ∪ V). In our experiments
we set K = 10. We found this helpful to keep the size of the active set manageable during the course of the
algorithm.

19

for the sub-vector of β indexed by ∪g∈J Gg. Consider the following deﬁnition, in which we use the
notation (cid:107)β(cid:107)2,1 = (cid:80)q

g=1 (cid:107)βg(cid:107)2.

Deﬁnition 1. Given a positive integer k and a constant c ≥ 1, let

γk =

min
β(cid:54)=0, G(β)≤k

√
√

k(cid:107)Xβ(cid:107)2
n(cid:107)β(cid:107)2,1

(cid:40)

and κk,c = min

J⊆[q],|J|≤k

min
β(cid:54)=0, (cid:107)βJc (cid:107)2,1≤c(cid:107)βJ (cid:107)2,1

√
√

k(cid:107)Xβ(cid:107)2
n(cid:107)βJ (cid:107)2,1

(cid:41)

.

The above deﬁnition is most meaningful under the scaling of the features where (cid:107)xj(cid:107)2 (cid:16)

n
for all j. As we discuss below, constants κ−1
k∗,c, with c > 1, appear in the prediction and estimation
error bounds for the Group Lasso estimator, while γ−1
appears in the estimation error bound for
2k∗
the Group (cid:96)0 estimator. The following result establishes a useful relationship for these quantities.

√

Proposition 4. γ2k ≥ κk,c/

√

2, for all positive integers k and all c ≥ 1.

We study estimator (cid:98)β that solves the following optimization problem:

min
β

(cid:107)y − Xβ(cid:107)2
2

s.t.

q
(cid:88)

g=1

1(βg (cid:54)= 0) ≤ k,

(33)

where k is a ﬁxed parameter that controls the sparsity level. We note that (33) is a special case
of the cardinality constrained problem considered in Section 3.3. We write Tg for the number of
features in group g ∈ [q] and deﬁne ¯Tk = max|J|≤k
g∈J Tg/k, noting the following relationships
in the special case where every group has the same number of T features: ¯Tk = T and p = qT .
Our ﬁrst result provides the prediction error bound for (cid:98)β, which holds without any assumptions
on the design.

(cid:80)

Theorem 3. Let δ0 ∈ (0, 1) and suppose that (cid:98)β is a global solution to optimization problem (33)
for k ≥ k∗. Then,

1
n

(cid:107)X(cid:98)β − Xβ∗(cid:107)2
2

(cid:46) σ2k

(cid:104) ¯Tk + log(q/k)
n

(cid:105)

+ σ2(cid:104) log(1/δ0)

n

(cid:105)

with probability at least 1 − δ0.

Letting δ0 = (k/q)k and using Deﬁnition 1, we derive the following result.

Corollary 1. If (cid:98)β is a global solution to optimization problem (33) for k = k∗, then

1
n

(cid:107)X(cid:98)β − Xβ∗(cid:107)2
2

(cid:46) σ2k∗

(cid:104) ¯Tk∗ + log(q/k∗)
n

(cid:105)

(cid:104) ¯Tk∗ + log(q/k∗)
n

(cid:105)1/2(cid:2)γ2k∗

(cid:3)−1

(cid:107)(cid:98)β − β∗(cid:107)2,1 (cid:46) σk∗

with probability at least 1 − (k∗/q)k∗.

We make several observations regarding the established error bounds, comparing them to
the bounds for the Group Lasso estimator, denoted by (cid:98)βGL, which replaces the (cid:96)0 constraint in
Problem (33) with a penalty on (cid:107)β(cid:107)2,1. To simplify the comparison of the corresponding rates,
we focus on the setting where Tg = T for all g ∈ [q] and k = k∗.

20

Remark 1. The Group (cid:96)0 prediction error rate provided in Corollary 1 matches the corresponding
optimal prediction error rate established in [46]. The estimation error rate in Corollary 1 is also
optimal provided that γ−1
is bounded by a universal constant under the aforementioned feature
2k∗
scaling (cid:107)xj(cid:107)2 (cid:16)

n.

√

√

Remark 2. Let (cid:107)xj(cid:107)2 (cid:16)
k∗,c is bounded by a universal constant
for some c > 1. Then, the error bounds for the Group Lasso estimator [see, for example, Section
8.3 of 19] are

n for all j and assume that κ−1

n−1(cid:107)X(cid:98)βGL − Xβ∗(cid:107)2
2

(cid:46) σ2k∗

(cid:104) T + log(q)
n

(cid:105)

and

(cid:107)(cid:98)β − β∗(cid:107)2,1 (cid:46) σk∗

(cid:104) T + log(q)
n

(cid:105)1/2

.

(34)

The Group (cid:96)0 rates discussed in Remark 1 are better than those in display (34), because they
replace the log(q) term with log(q/k∗). Moreover, in view of Proposition 4, the assumption on γ2k∗
in Remark 1 is weaker than the Group Lasso assumption on κk∗,c. Finally, the Group (cid:96)0 prediction
error bound holds without any assumptions on the design.

The last observation represents an important non-trivial advantage of (cid:96)0-based approaches
over Lasso-type methods. [82] provide examples of design matrices in the usual linear regression
context for which the Lasso prediction error is lower-bounded by a constant multiple of 1/
n,
generally leading to a much larger prediction error than the one for the (cid:96)0-based method.7

√

Remark 3. One advantage of estimator (33) is that tuning parameter k directly controls the
sparsity of the proposed estimator. In particular, the (cid:98)β that achieves the bounds in Corollary 1
satisﬁes G((cid:98)β) ≤ k∗. On the other hand, the (cid:98)βGL that achieves bounds (34) is typically much
more dense. The following inequality, which holds with high probability, is provided in [46]:

G((cid:98)βGL) ≤

(cid:105)

(cid:104) 64φmax
κk∗,3

k∗.

Here, φmax is the maximum eigenvalue of X(cid:62)X/n. Thus, the right-hand side is at least 64k∗.

The error rates presented above can also apply to approximate solutions obtained after an
early termination of the MIO solver. Upon termination, the solver provides the upper and
lower bounds on the value of the objective function. We denote these bounds by U B and LB,
respectively, and write τ = (U B − LB)/U B for the corresponding optimality gap. The next
result considers an approximate solution (cid:101)β and demonstrates that the bounds in Corollary 1 hold
for (cid:101)β when τ is bounded away from one and τ (cid:46) k∗[ ¯Tk∗ + log(q/k∗)]/n.
Corollary 2. Let k = k∗ and suppose that τ ≤ 1 − c for some positive universal constant c.
Then,

1
n

(cid:107)X(cid:101)β − Xβ∗(cid:107)2
2

(cid:46) σ2k∗

(cid:104) ¯Tk∗ + log(q/k∗)
n

(cid:105)

+ σ2τ

with probability at least 1 − (k∗/q)k∗.

An attractive feature of Theorem 3 is that the uncertainty parameter δ0 is independent of the
tuning parameter k. This allows us to control the expected prediction error8, as we demonstrate
in the following result.

7The lower-bound applies to a wide class of coordinate-separable M-estimators, including local optima of non-

convex regularizers such as SCAD and MCP.

8An application of Deﬁnition 1 yields a corresponding bound on the expected estimation error.

21

Corollary 3. Under the conditions of Theorem 3,

E(cid:107)X(cid:98)β − Xβ∗(cid:107)2
2

(cid:46) σ2k(cid:2) ¯Tk + log(q/k)(cid:3).

5.1.1 Selecting the model size tuning parameter k

The results presented above rely on the fact that k ≥ k∗. We now analyze a BIC-type approach
for selecting k that does not require the knowledge of the true sparsity level. Approaches of this
type have been shown to be successful in the setting of high-dimensional linear regression for the
purposes of model selection [41, 75].

We denote the global solution to Problem (33) by (cid:98)βk and let ˇT = maxg∈[q] Tg. The next result

focuses on the estimator (cid:98)β

B

= (cid:98)βˆk, where

ˆk = arg min

k≤q

(cid:107)y − X(cid:98)βk(cid:107)2

2 + ak[ ˇT + log(q/k)]

and a is a non-negative tuning parameter.
Theorem 4. There exists a universal constant a0, such that if a0σ2 ≤ a (cid:46) σ2, then

B

E(cid:107)X(cid:98)β

− Xβ∗(cid:107)2
2

(cid:46) σ2(k∗ ∨ 1)(cid:2) ˇT + log(q/k∗)(cid:3) and EG((cid:98)β

B

) (cid:46) k∗ ∨ 1.

Theorem 4 demonstrates that, for an appropriate choice of a, estimator (cid:98)β

optimal prediction error rate in Remark 1 and has the same order of group sparsity as β∗.

B

achieves the

5.1.2 The case of large group sizes

The results presented so far illustrate that, at least on average, the group sizes need to be of a
smaller order than n to achieve prediction consistency. We now consider the challenging setting
where some groups may have more than n features.
In this setting, additional regularization
within each group is required for good predictive performance. One way to impose additional
regularization is to encourage sparsity within groups and, thus, perform bi-level variable selection
[39]. This can be achieved by including an additional (cid:96)0 penalty on the number of features
within each group or an additional (cid:96)1 penalty on the coeﬃcients within each group. Here, we
take the approach of regularizing using the (cid:96)2-penalty, without encouraging additional sparsity.
As discussed in Section 2.2, we take a similar approach in nonparametric additive modeling,
where groups sizes of order n arise naturally via the cubic spline representation of the functional
components.

We assume, for concreteness, that the features have been normalized to achieve (cid:107)xj(cid:107)2 =

for all j ∈ [p], and focus on the following optimization problem:

√

n

min
β

(cid:107)y − Xβ(cid:107)2

2 + λ

(cid:112)Tg(cid:107)βg(cid:107)2

s.t.

(cid:88)

g∈[q]

q
(cid:88)

g=1

1(βg (cid:54)= 0) ≤ k.

(35)

Because the (cid:96)2-penalty parameter is allowed to vary across groups, (35) is a slight generalization
of the cardinality constrained problem considered in Section 3.3, but the algorithms presented
here will apply to (35) with minor adjustments. We deﬁne ˜q = p/ ming∈[q] Tg and note that ˜q = q
when all the groups are of the same size. The next result establishes a prediction error bound for
the large group-size setting.

22

Theorem 5. Let δ0 ∈ (0, 1) and suppose that (cid:98)β is a global solution to problem (35) for k ≥ k∗.
There exist universal positive constants c0, such that if λ ≥ c0σ(cid:112)n log(2e˜q), then

1
n

(cid:107)X(cid:98)β − Xβ∗(cid:107)2
2

(cid:46) λ
n

(cid:88)

g∈[q]

(cid:112)Tg(cid:107)β∗

g(cid:107)2

with probability at least 1 − 1/[4e˜q].

Focusing on the case Tg = T for all g ∈ [q] and k = k∗, for concreteness, we derive the
corresponding prediction error rate9 of σ(cid:112)T log(q)/n(cid:107)β∗(cid:107)2,1. This error rate demonstrates that
prediction consistency can be achieved even when T (cid:29) n, provided that the coeﬃcients in
β∗ are suﬃciently small. For example, when each coeﬃcient equals 1/T , the resulting rate is
σk∗

(cid:112)log(q)/n.

5.2 Nonparametric Additive Model

We study the performance of the proposed approach in the deterministic design setting. We write
(cid:107) · (cid:107)L2 for the L2 norm of a real-valued function on [0, 1]. Using the notation in Section 2.2, we
let Cj = C for all j and focus on the case where C is an L2-Sobolev space:

C =

(cid:110)
(cid:111)
g : [0, 1] (cid:55)→ R, (cid:107)g(cid:107)L2 + (cid:107)g(m)(cid:107)L2 < ∞

and Pen(g) = (cid:107)g(m)(cid:107)L2.

We deﬁne Cgr = {f : [0, 1]q (cid:55)→ R, f (x) = (cid:80)q
of additive functions. We associate each f ∈ Cgr with the vector f = (cid:80)q
(cid:0)fj(x1j), ..., fj(xnj)(cid:1), and let

j=1 fj(xj), fj ∈ C} as the corresponding space
j=1 fj, where fj =

G(f ) =

q
(cid:88)

j=1

1(fj (cid:54)= 0),

Pengr(f ) =

q
(cid:88)

j=1

Pen(fj).

We focus on the estimator that globally solves the following optimization problem:

min
f ∈Cgr

(cid:107)y − f (cid:107)2

n + λnPengr(f )

s.t. G(f ) ≤ k,

(36)

√

n.10 To ensure identiﬁability of the represen-
where (cid:107)·(cid:107)n denotes the Euclidean norm divided by
tation f (x) = (cid:80)q
j=1 fj(xj), additional restrictions are typically imposed. For example, a popular
method is to separate out the constant term and require that (cid:80)n
i=1 fj(xij) = 0 for each j. Here
we follow the approach of [68] and avoid specifying a particular set of restrictions. We treat every
representation of f as equivalent, with the understanding that one particular representation is
used when evaluating properties of the components, such as (cid:107)fj(cid:107)n.

We are interested in comparing estimator (36), denoted by (cid:98)f , with the widely popular Group
Lasso-based approach, which replaces the (cid:96)0 constraint in Problem (36) with a penalty on
(cid:80)q
j=1 (cid:107)fj(cid:107)n. Theoretical properties of the latter approach have been investigated extensively
[see, for example, 53, 42, 64, 67, 79, 68, and the references therein]. To compare the error bounds
for the two estimators, we need the following deﬁnition.

9When T = 1, this rate matches the slow error rate for the Lasso estimator [for example, 19].
10We acknowledge the notational inconsistency when n ≤ 2.

23

Deﬁnition 2. Given a positive integer k, a constant ξ ∈ (1, ∞] and an index set J ⊆ [q], let

(cid:107)fj(cid:107)n (cid:54)= 0, G(f ) ≤ k, 2n−m/(2m+1)Pengr(f ) ≤ (ξ − 1)

(cid:88)q

j=1

(cid:107)fj(cid:107)n}

Ak,ξ ={f ∈ Cgr :

BJ,ξ ={f ∈ Cgr :

q
(cid:88)

j=1
q
(cid:88)

j=1

(cid:107)fj(cid:107)n (cid:54)= 0,

(cid:88)

j /∈J

(cid:107)fj(cid:107)n + n−m/(2m+1)Pengr(f ) ≤ ξ

(cid:88)

j∈J

(cid:107)fj(cid:107)n}

ψ(k, ξ) = min
f ∈Ak,ξ

√

k(cid:107)f (cid:107)n
j=1 (cid:107)fj(cid:107)n

(cid:80)q

and φ(k, ξ) = min

J⊆[q],|J|≤k

(cid:40)

min
f ∈BJ,ξ

√

k(cid:107)f (cid:107)n
j∈J (cid:107)fj(cid:107)n

(cid:80)

(cid:41)

.

As we discuss below, constants φ(2k, ξ)−1 appear in the error bounds for the Group Lasso-
based approach, while constants ψ(k, ξ)−1 appear in some of the bounds that we establish for (cid:98)f .
The following result establishes a useful relationship for these quantities.

Proposition 5. For all positive integers k and all ξ ∈ (1, ∞], ψ(2k, ξ) ≥ φ(k, ξ)/

√

2.

We assume that the observed data follows the model y = f ∗ + (cid:15), where f ∗ ∈ Cgr, and the
elements of (cid:15) are independent N (0, σ2) with σ > 0. We refer to (cid:107)(cid:98)f − f ∗(cid:107)2
n as the prediction error
for estimator (cid:98)f . We write rn = n−m/(2m+1), suppressing the dependence on m for notational
simplicity, noting that r2
n is the optimal prediction error rate in the univariate regression setting
where f ∗ ∈ C. For example, in the case where C is the second order Sobolev space, which
n = n−4/5. We deﬁne α = 1/(4m + 2) and note that
corresponds to m = 2, the above rate is r2
α = 1/10 when m = 2. The next result, in which we treat m ≥ 1 as a ﬁxed integer, establishes
prediction error bounds for the proposed approach.

Theorem 6. Let k∗ = G(f ∗) and consider optimization Problem (36) with k ≥ k∗. There exists
a universal constant c1, such that if λn ≥ c1σ(cid:2)k2αr2

(cid:112)log(eq/k)/n(cid:3), then

n + kαrn

(cid:107)(cid:98)f − f ∗(cid:107)2
n

(cid:46) σ2k

(cid:104)

k2αr2

n +

(cid:105)

log(eq/k)
n

+ λnPengr(f ∗)

(37)

with probability at least 1 − (k/q)k. Furthermore, for every ξ ∈ (1, ∞], there exists a ﬁnite
constant c2, which depends only on ξ, such that if λn ≥ c2σ(cid:2)r2

(cid:112)log(q)/n(cid:3), then

n + rn

(cid:107)(cid:98)f − f ∗(cid:107)2
n

(cid:46) σ2k

(cid:104)

r2
n +

log(q)
n

(cid:105)(cid:2)ψ(2k, ξ)(cid:3)−2 + λnPengr(f ∗)

(38)

with probability at least 1 − 1/q.

We make the following observations regarding the established error bounds. To simplify the
comparison of the error rates, we focus on the setting where k = k∗ and Pengr(f ∗) (cid:16) σk∗.
The last relationship holds, for example, when the scaled roughness of each nonzero component,
Pen(f ∗

j )/σ, is bounded above and below by positive universal constants.

Remark 4. The expression in error bound (37) is optimized for the setting where Pengr(f ∗) (cid:16) σk.
However, as we show in the proof, the bound can be improved when σk and Pengr(f ∗) have diﬀerent
orders of magnitude.

24

Remark 5. The prediction error rate provided in (38) is analogous to the rate established
in [68] for the Group Lasso-based approach11, however, the latter rate replaces ψ(2k∗, ξ)−2 with
φ(k∗, ξ)−2. By Proposition 5, the former rate is at least as good as the latter, with a potential
improvement due to the additional (cid:96)0 group sparsity requirement in the deﬁnition of ψ. If for
some ﬁxed ξ > 1 quantity ψ(2k∗, ξ)−1 is bounded by a universal constant, then inequality (38)
yields the following prediction error rate:

(cid:107)(cid:98)f − f ∗(cid:107)2
n

(cid:46) σ2k∗

(cid:104)
r2
n +

log(q)
n

(cid:105)
.

This rate matches the one established in [68] for the Group Lasso-based approach under an anal-
ogous (but somewhat stronger) assumption12 on φ(k∗, ξ)−1.

Remark 6. Bound (37) yields the following error rate without imposing assumptions on the
design:

(cid:107)(cid:98)f − f ∗(cid:107)2
n

(cid:46) σ2k∗

(cid:104)

∗ r2
k2α

n +

log(eq/k∗)
n

(cid:105)
.

If k∗ (cid:46) 1 or k2α

∗ r2
n

(cid:46) log(eq/k∗)/n, then the above expression can be upper-bounded by

σ2k∗

(cid:2)r2

n +

log(eq/k∗)
n

(cid:3).

Thus, (cid:98)f achieves the corresponding minimax lower bound on the prediction error [64, 67, 68].

Remark 7. When q = k∗, the prediction error rate given by bound (37) is k1+1/(2m+1)
improves over the corresponding k1+3/(2m+1)
the former rate is k6/5
consequence of the more reﬁned entropy bounds derived in our proofs.

r2
n, which
n rate13 derived in [45]. In particular, when m = 2,
r2
∗ n−4/5. The improvement in the rate is a

∗ n−4/5, while the latter is k8/5

∗

∗

Remark 8. In the special case of m = 2 and k∗ (cid:46) 1, bound (37) yields the prediction error rate
of n−4/5 + log(q)/n, which matches the optimal univariate rate of n−4/5 when log(q) (cid:46) n1/5.

Remark 9. If for some ﬁxed ξ > 1 quantity ψ(2k∗, ξ)−1 is bounded by a universal constant, then
a direct consequence of Theorem 6 is the following estimation error rate:

(cid:107)(cid:98)fj − f ∗

j (cid:107)n (cid:46) σk∗

(cid:114)

(cid:104)
rn +

log(q)
n

(cid:105)
.

q
(cid:88)

j=1

6 Experiments

We present experiments that shed light on the practical performance of our proposals compared
to the state of the art. Our algorithms are implemented in Python and are available at https:

11To the best of our knowledge, the bounds in [68] are overall the strongest in the literature for the Group
Lasso-based approach, due to the relative weakness of the imposed conditions: see the discussion in Remark 12 of
[68].

12For a comprehensive discussion of this assumption, we refer the reader to [53, 68], and the references therein.
13Theorem 1 in [45] treats the number of predictors (q = k∗) as ﬁxed and omits it from the expression for the
error rate. However, an examination of the proof of their Theorem 1 and the entropy bound in their Lemma A.1,
which explicitly accounts for the number of predictors, reveals the eﬀect of the dimension k∗.

25

//github.com/hazimehh/L0Group. In Section 6.1, we investigate the statistical properties of our
algorithms for the Group (cid:96)0 problem. In Section 6.2, we present computation times of our MIP
algorithm. Section 6.3 investigates nonparametric sparse additive models. Additional numerical
experiments can be found in the supplementary material.

6.1 Grouped variable selection

We consider both synthetic and real datasets in our experiments, as discussed below.
Synthetic data generation. The underlying model is y = Xβ∗ + (cid:15), where β∗ ∈ Rp has q
groups, all with the same size. Once we generate X (see below), every column is standardized
iid∼ N (0, σ2), i = 1, . . . , n, are independent of X, and σ2 is
to have unit (cid:96)2-norm. The errors (cid:15)i
chosen to achieve a desired signal-to-noise ratio (SNR)14. We note that the SNR values in our
experiments are suﬃciently high to make the true model support recovery possible.

Two diﬀerent types of X are considered: (a) example=1: We ﬁrst generate group represen-
tatives γ1, . . . , γq ∼ MVNq(0, Σ), where, Σq×q = ((σij)), with σij = ρ|i−j|. Given a γg, the
covariates xj, j ∈ Gg are generated by adding independent Gaussian noise to a scalar multiple
of γg, to achieve pairwise correlation of 0.9 within the group.
(b) example=2: Here we take
X ∼ MVNp(0, Σ), where σij = ρ, for all i (cid:54)= j, with σjj = 1 for all j.

To generate the true population regression coeﬃcients, the k∗ nonzero groups are taken to
be equally spaced in {1, . . . , q}. All the nonzero entries of β∗ are drawn independently from a
standard Gaussian distribution.
Competing algorithms and tuning. In the experiments of this section, we focus on the Group
(cid:96)0 problem deﬁned in (1), and study the performance of our algorithms. We compare against
the following state-of-the-art grouped variable selection methods: Group Lasso (based on (cid:96)2,1
regularization), Group MCP, and Group SCAD – these estimators are computed by using the
R package grpreg [18]. For synthetic data (Sections 6.1.1 and 6.1.2), we construct a separate
validation set with a ﬁxed design. We tune the parameters of the diﬀerent problems to minimize
the prediction error on the validation set. Speciﬁcally, for each of Group (cid:96)0 and Group Lasso, we
tune the regularization parameter over a (one-dimensional) grid with 100 values. For MCP and
SCAD, we tune the ﬁrst parameter λ over a grid with 100 values, and leave the second parameter
γ to its default value in grpreg. For MCP and SCAD, high-dimensional BIC (HBIC) is another
method for doing tuning parameter selection [75] – we present results for MCP and SCAD based
on HBIC tuning following [75] in the Supplementary material (Section B).
Performance measures. Given an estimator ˆβ, we consider the following performance mea-
sures:

• True Positives (TP): The number of nonzero groups that are in both ˆβ and β∗.

• False Positives (FP): The number of nonzero groups in ˆβ but not in β∗

• Recovery F1 Score: The harmonic mean of precision and recall,

i.e., F1 Score =
2P R/(P + R), where P = TP/(TP + FP) is precision and R = TP/k∗ is recall. We
note that an F1 Score of 1 implies perfect support recovery.

• Test MSE: This is deﬁned as 1

n (cid:107)Xˆβ − Xβ∗(cid:107)2
2.

14For a generative model of the form yi = µi + (cid:15)i, we deﬁne SNR = Var(µ)/Var((cid:15)).

26

6.1.1 Statistical performance for varying number of observations

In this experiment, we study the eﬀect of varying the number of observations n on the performance
of Group (cid:96)0 and other state-of-the-art group regularizers (Group Lasso, MCP, and SCAD). We
obtain approximate estimators to the Group (cid:96)0 problem using Algorithms 1 and 2 (with m = 1).
We generate 500 datasets having exponentially decaying correlation (i.e., under example=1) with
a correlation parameter ρ = 0.9, p = 5000, a group size of 4, number of nonzero groups k∗ = 25,
and SNR = 10. This setting is relatively diﬃcult for recovery as each group is highly correlated
with a few others. We report the mean and the standard error for each performance measure in
Figure 1.

Figure 1: Performance measures for varying number of observations on a synthetic dataset with
highly correlated features. The measures are averaged over 500 repetitions, and the standard error
is represented using error bars. Alg. 1 and Alg. 2 are our proposed algorithms. Here, “Lasso” is a
shorthand for Group Lasso, we use the same convention for SCAD, MCP. All methods are tuned
by minimizing the prediction error on a validation set. The average running time (in seconds)
across n is 11 for Algorithm 1, 42 for Algorithm 2, and 4 for Group Lasso, MCP, and SCAD. This
diﬀerence in running time may be in part due to the eﬃcient C implementation used in grpreg,
as opposed to our prototype in Python.

Figure 1 shows that Algorithm 2 notably outperforms the other methods in terms of variable
selection; it perfectly recovers the support for n ≈ 2000. Group MCP and SCAD require roughly
4500 observations to recover the true support, whereas Group Lasso does not recover the support
even when n = p. Moreover, Algorithms 1 and 2 attain the smallest support sizes for any n,
whereas the other methods require much larger supports, especially for small n. Algorithm 2 has
the lowest test MSE for all n. The test MSE of MCP matches that of Algorithm 2 in most of
the cases, while the other methods lag behind. We also note that there is a gap between the test
MSE of Algorithms 1 and 2. This diﬀerence is likely due to Algorithm 2 doing a better job in
optimization.

In Supplementary Material Section B, we report the results of the same experiment but with
lower feature correlation (ρ = 0.5 and ρ = 0). The performance of all methods improve as
correlation decreases, but the results are qualitatively similar to what we see in Figure 1.
In
addition, we also report results based on HBIC tuning for Group MCP and SCAD regularizers
– with HBIC tuning, the sparsity of Group MCP and SCAD estimators generally improves, but
their prediction performance suﬀers.

27

500100015002000250030003500400045005000Number of Observations0.00.20.40.60.81.0Recovery F1 Score500100015002000250030003500400045005000Number of Observations012345Test MSE500100015002000250030003500400045005000Number of Observations020406080100120140Support SizeAlg. 1Alg. 2MCPSCADLasso6.1.2 Statistical performance on high-dimensional instances

We compare the performance of the diﬀerent methods under two high-dimensional settings. In
both settings, we generate data with constant correlation (i.e., under example=2) and SNR = 10.
Below is a description of the settings:

• Setting 1: ρ = 0.9, n = 1000, p = 100, 000, k = 10, and a group size of 10.

• Setting 2: ρ = 0.3, n = 1000, p = 100, 000, k = 20, and a group size of 4.

For each setting, we generate 500 replications, on which we train and tune the algorithms. The
tuning here is based on validation MSE – results based on HBIC tuning for (group) SCAD and
MCP estimators are presented in the supplementary material (Section B). To ensure a fair com-
parison in terms of running time, we solve the Group (cid:96)0 problem approximately using Algorithm
2 (with m = 1), which typically has the same order of running time (seconds in this case) as the
other group selection methods considered here. We report the averaged results for Settings 1 and
2 in Table 1.

Table 1: Performance measures for Setting 1 (top panel) and Setting 2 (bottom panel). Means
are reported along with their standard errors—we consider 500 replications.

Algorithm

(cid:107)ˆβ(cid:107)0

TP

FP

MSE

(cid:107)ˆβ − β∗(cid:107)∞

1

g
n
i
t
t
e
S

100.3 (1.2)
Group (cid:96)0
2086.1 (27.8)
Group Lasso
Group MCP
294.8 (6.7)
Group SCAD 686.3 (18.6)

8.9 (0.09)
9.6 (0.05)
9.4 (0.06)
9.5 (0.05)

1.1 (0.1)
199.0 (2.8)
20.1 (0.7)
59.1 (1.9)

2 Group (cid:96)0

g
n
i
t
t
e
S

Group Lasso
Group MCP
Group SCAD

79.4 (0.1)
1126.5 (11.7)
139.6 (2.1)
310.4 (4.9)

19.7 (0.03)
19.8 (0.02)
19.8 (0.02)
19.9 (0.02)

0.2 (0.02)
261.8 (2.9)
15.1 (0.5)
57.8 (1.2)

19.2 (1.1)
26.9 (1.1)
19.8 (1.2)
24.5 (1.2)

1.12 (0.03)
5.06 (0.10)
1.20 (0.03)
1.56 (0.06)

1.17 (0.03)
1.44 (0.02)
1.10 (0.04)
1.21 (0.03)

0.356 (0.007)
0.703 (0.007)
0.352 (0.006)
0.415 (0.008)

Under both settings, Group (cid:96)0 selects signiﬁcantly smaller support sizes and false positives
than other methods, and is more consistent across the replications (as evidenced by the small
standard error). For example, in Table 1 (top), Group (cid:96)0 has a support size which is roughly
20 times smaller than the one for the Lasso and 3 times smaller than one for MCP. For few of
the instances, one true positive is missed in Group (cid:96)0, but the diﬀerence with the other methods
is marginal. In terms of MSE and the estimation error (i.e., (cid:107)β − β∗(cid:107)∞), Group (cid:96)0 appears to
outperform the other methods, with the diﬀerences being most pronounced in the high correlation
setting of Table 1 (top). This aligns with the results in Figure 1, where we saw that Group (cid:96)0 leads
to important improvements when features are highly correlated and n is small.

6.1.3 Real data

We study the performance of the diﬀerent methods on the Amazon Reviews dataset [33]. As the
focus of this paper is on group variable selection in the case where the non-overlapping groups
are pre-speciﬁed, we perform pre-processing of the features to obtain a grouping of the features

28

Figure 2: Test MSE on the Amazon Reviews dataset (n = 3500, p = 3368, and q = 100). For
Group (cid:96)0, we consider additional ridge regularization and vary the corresponding regularization
parameter λ2 ∈ {0.5, 1, 2}.

(based on simple exploratory analysis). All the diﬀerent group-sparse estimators make use of
the same group structure – this allows us to fairly compare their performance given the chosen
group-structure. We note that the downstream results depend upon the input group structure.
After preprocessing, the dataset consists of 3482 predictors divided into 100 groups. We use 3500
and 2368 observations for training and testing, respectively. Additional details on the dataset
and preprocessing are discussed in the Supplement C. On this dataset, we ﬁt regularization paths
for Group (cid:96)0, Lasso, and SCAD15. For Group (cid:96)0, we use an additional ridge regularization term16
and consider λ2 ∈ {0.5, 1, 2}. In Figure 2, we plot the test MSE at diﬀerent sparsity levels. The
results indicate that the lowest MSE is roughly the same for Group (cid:96)0 (λ2 = 1), Lasso, and SCAD;
with Group (cid:96)0 having a clear advantage in terms of the support size. Speciﬁcally, Group (cid:96)0 with
λ2 = 1 attains the lowest MSE at 5 groups whereas Group Lasso and SCAD require around 60
groups to achieve a similar MSE performance.

In the Supplement B.4, we report results on another real dataset; and our conclusions are

qualitatively similar to the example in Figure 2.

6.2 MIP-based global optimality certiﬁcates: Timing comparisons

Here, we compare the running time of our BnB solver with Gurobi for obtaining globally optimal
solutions (we note that Algorithms 1, 2 presented earlier are approximate algorithms.) We
generate synthetic data under example=2, and we study the eﬀect of the number of predictors
p on the running time. Speciﬁcally, we vary p ∈ {103, 104, 105, 106, 5 × 106} and ﬁx the other
data generation parameters as follows: group size of 10, n = 103, ρ = 0.1, k∗ = 5, SNR = 10,
and set all nonzero coeﬃcients in β∗ to 1. We limit our largest problem instance to n = 103 and
p = 5 × 106 due to memory limitations – as our data-matrix X is dense, generating and storing
a copy of it is memory intensive. We solve the MIP in (26) to optimality, for two cases: (i) with
ridge regularization (λ2 > 0) and (ii) without ridge regularization (λ2 = 0). In both cases, we
ﬁx λ1 = 0. For case (i), we choose (λ0, λ2) so that the solution obtained has k∗ nonzero groups
and minimizes the (cid:96)2 estimation error. More formally, for a ﬁxed choice of (λ0, λ2), let θ(λ0, λ2)

15We also tried group MCP, but the solver faced numerical problems – hence, their results are not reported.
16This is found to be useful here due to high feature correlations within a group.

29

denote a solution of (26). Then, we choose the parameters of case (ii) as follows:

(λ∗

0, λ∗

2) ∈ arg min
(λ0,λ2)

(cid:107)θ(λ0, λ2) − β∗(cid:107)2 s.t. G(θ(λ0, λ2)) = k∗.

0, λ∗

We estimate (λ∗
2) by running Algorithm 2 on a two-dimensional grid with λ0 ∈ {103, 2 ×
103, . . . , 104} and λ2 ∈ {10−5, 10−4, . . . , 105}. For case (ii), we choose λ0 so that the corresponding
solution has k∗ nonzero groups. Let S∗ be the support of the true solution β∗, and let ˆβ be the
solution obtained by solving minβ (cid:96)(β)
s.t. β(S∗)c = 0. Then, in both cases, we set Mu to
maxg∈[q] (cid:107)ˆβg(cid:107)2. For the two solvers, we set the optimality gap17 to 1% and use a warm start
obtained from Algorithm 2. The running times were measured on a cluster with CentOS 7. Each
job (i.e., a single run of a solver over one dataset) was allocated 4 cores of an Intel Xeon Gold
6130 CPU @ 2.10GHz processor and up to 120 GB of RAM. For each job, we set a time limit of
24 hours.

Table 2: Running time in seconds for solving Problem (26) to optimality. A dash (-) indicates
that Gurobi cannot solve the problem in 24 hours and has an optimality gap of 100% upon
termination.

p

103
104
105
106
5 × 106

Case (i): λ2 = λ∗
2
Gurobi
Ours

Case (ii): λ2 = 0
Gurobi
Ours

96

199

231

386

1922

24223

-

-

-

-

373

466

1136

1628

11627

8737

-

-

-

-

In Table 2, we report the running time (in seconds) for cases (i) and (ii). In both cases, the
results indicate that our BnB can solve instances with p = 5 × 106 in the order of minutes to
hours, whereas Gurobi cannot solve the problem beyond p = 103 within the 24-hour time limit.
Speciﬁcally, for p ≥ 104, Gurobi’s optimality gap is 100%. The reason behind this large gap is
that Gurobi cannot solve the root relaxation in the 24-hour time limit, so the best lower bound
upon termination is 0. The running times for our BnB solver in case (i) are lower than case (ii),
and this can be attributed the perspective reformulation which exploits the presence of the ridge
regularizer to speed up computation.
It is also worth mentioning that our implementation of
BnB is a prototype that does not exploit parallelism (commercial solvers like Gurobi can exploit
parallelism). Parallelizing our BnB implementation is expected to make it faster, especially on
diﬃcult instances where the search tree is large. In the Supplement Section B.5, we report the
running times of our BnB algorithm and Gurobi for diﬀerent choices of Mu.

In Supplement Section B.5, we present an example showing how the runtime of our BnB

algorithm changes as n is increased.

17Given an upper bound UB and a lower bound LB, the optimality gap is deﬁned as (UB-LB)/UB.

30

6.3 Nonparametric Additive Models

We study an expanded version of the popular Boston Housing dataset18 as an application of our
MIP framework to (cid:96)0-sparse additive modeling. The dataset consists of 13 covariates. To get
a better idea about the performance in the presence of irrelevant covariates, we augmented the
data with 50 irrelevant covariates. Speciﬁcally, we selected 5 covariates uniformly at random. For
each selected covariate, we randomly permuted the entries of the covariate vector and augmented
the data with the permuted vector—we repeated this step 10 times. This led to 63 covariates in
total. We randomly sampled 406 observations for training and 50 observations for validation, and
we standardized the response and the covariates. We predict house price using the 63 covariates.
We compare the performance of sparse additive models based on Group (cid:96)0 and Group Lasso.
In both approaches, we used B-splines of degree 3 for the basis functions, with 10 knots equi-
spaced in the covariates. For the Group (cid:96)0-based approach, we used formulation (6) and tuned λ
over a grid of 100 values between 10−5 and 10−2 (equi-spaced on a logarithmic scale). We obtained
the Group Lasso-based approach by relaxing all the binary variables in the MIP formulation of
(6) to the interval [0, 1], and we tuned λ over a grid of 100 values ranging from 10−4 to 1 (equi-
spaced on a logarithmic scale). In Figure 3, we plot the test MSE versus the number of nonzeros,
for each of the two models. The results indicate that the Group (cid:96)0-based approach achieves the
minimum test MSE at 7 nonzeros, whereas the Group Lasso-based method achieves its minimum
MSE at around 60 nonzeros (without matching the performance of Group L0).

Figure 3: Test MSE versus the number of nonzeros on the Boston Housing dataset (with additional
noisy covariates).

7 Concluding Remarks

We revisit a well-known family of problems in sparse learning where the variables are naturally
organized into a collection of pre-speciﬁed non-overlapping groups. We study both the associated
linear regression problem and the problem of nonparametric additive modelling with smooth com-
ponents. In contrast to the earlier work, we pursue MIP-based methods to solve the underlying
discrete optimization problem to optimality, and at scale. Our algorithmic contributions include
(i) fast algorithms based on coordinate descent and combinatorial local search to obtain good

18The dataset was downloaded from https://archive.ics.uci.edu/ml/datasets/Housing.

31

feasible solutions; and (ii) an exact algorithm based on a custom branch-and-bound procedure,
which exhibits signiﬁcant speedups compared to oﬀ-the-shelf commercial MIP solvers. We present
statistical theory for our estimators encompassing both the linear model and the nonparametric
additive model settings.

Our paper contributes to the growing body of work exploring MIP-based tools to address
a broad range of computational problems that arise in statistics. An incomplete list of recent
references includes classiﬁcation [25], factor driven regression [43], learning acyclic graphs [49],
sparse PCA [26], signal estimation [2, 44], among others – we also refer the reader to a recent
book [10] and survey [70].

There are several directions for future work. One important application of group-sparse
models arises in sparse representation of multiple measurement vectors, where the response is
multivariate and the regression coeﬃcients form a matrix [71, 48, 24].
In a series of works,
[21, 66, 73] study diﬀerent convex relaxations of group-(cid:96)0-type formulations and their theoretical
properties. Other examples of group-sparse problems can be found in [4, 83]. It will be interesting
to extend the approaches presented here to these more general settings.

8 Acknowledgements

We would like to thank the Associate Editor and the referees for their thoughtful and constructive
comments that helped us improve the paper. We thank Shibal Ibrahim for his help with the
Boston Housing dataset experiment. The research was partially supported by the Oﬃce of Naval
Research (ONR-N000141512342, ONR-N000141812298), National Science Foundation (NSF-IIS-
1718258).

9 Convex Relaxation of Problem (7)

Consider Problem (7) and suppose that the solution to this problem is bounded. Moreover, we
assume that the (cid:96)2-norms of every group fj satisﬁes: (cid:107)fj(cid:107)2 ≤ Mu. Then it follows that the
problem is equivalent to:

min (cid:107)y −

q
(cid:88)

j=1

fj(cid:107)2

2 + λ0

(cid:88)

j∈[q]

zj + λ

q
(cid:88)

j=1

(cid:107)fj(cid:107)Cj

s.t. (cid:107)fj(cid:107)2 ≤ Muzj, zj ∈ {0, 1}, j ∈ [q].

(39)

Relaxing the zj’s in the above to [0, 1], leads to the following formulation:

min (cid:107)y −

q
(cid:88)

j=1

fj(cid:107)2

2 + λ

q
(cid:88)

j=1

(cid:107)fj(cid:107)Cj + λ1

q
(cid:88)

j=1

(cid:107)fj(cid:107)2

s.t. (cid:107)fj(cid:107)2 ≤ Mu

(40)

where λ1 := λ0
problem as follows:

Mu . Next, we (i) drop the constraints in the above, and (ii) rewrite the resulting

Γ1 := min (cid:107)y −

q
(cid:88)

j=1

fj(cid:107)2

2 + λ(

q
(cid:88)

j=1

(cid:107)fj(cid:107)Cj +

λ1
λ

q
(cid:88)

j=1

(cid:107)fj(cid:107)2).

(41)

32

Note that (41) is a relaxation of (39) (and consequently of (7)). Now, using the fact that

((cid:107)fj(cid:107)Cj +

(cid:107)fj(cid:107)2)/

√

2 ≤

λ1
λ

(cid:115)

(cid:107)fj(cid:107)2
Cj

+

(cid:19)2

(cid:18) λ1
λ

(cid:107)fj(cid:107)2
2,

it follows that the following

Γ2 := min (cid:107)y −

q
(cid:88)

j=1

fj(cid:107)2

2 +

√

2λ

q
(cid:88)

j=1



(cid:115)



(cid:107)fj(cid:107)2
Cj

+

(cid:19)2

(cid:18) λ1
λ



(cid:107)fj(cid:107)2
2

 ,

(42)

is an upper bound to Problem (41) (with the tuning parameters kept ﬁxed). Note that Prob-
+ λ(cid:48)(cid:107)fj(cid:107)2
lem (42) is indeed the penalty considered in [53], with the choice of Pen(fj) =
2,
where λ(cid:48) is appropriately chosen to match (42).

(cid:107)fj(cid:107)2
Cj

(cid:113)

We note that the penalty chosen in formulation (41) is similar to the penalty considered

in [64], wherein the authors consider an RKHS framework with penalization:

q
(cid:88)

(cid:113)

λ

j=1

jKjβj + λ(cid:48)
β(cid:48)

q
(cid:88)

j=1

(cid:107)Kjβj(cid:107)2,

where Kj indicates the kernel basis matrix for the jth coordinate.

A Proofs

A.1 Proof of Theorem 1

The following lemma shows that there is a suﬃcient decrease in the objective after every group
update in Algorithm 1. The result of this lemma will be used in the proof of Theorem 1.

Lemma 1. (Suﬃcient Decrease) The sequence of iterates {θl} in Algorithm 1 satisﬁes the fol-
lowing for every l and g = 1 + (l mod q):

ˆLg − Lg
2
Proof of Lemma 1. Fix some l ≥ 0 and let g = 1 + (l mod q). Applying (11) to (θl+1, θl)

h(θl) − h(θl+1) ≥

g − θl+1

g (cid:107)2
2.

(A.1)

(cid:107)θl

and adding Ω(θl+1) to both sides, we get:

h(θl+1) ≤ (cid:96)(θl) + (cid:104)∇θg (cid:96)(θl), θl+1

g − θl

g(cid:105) +

Lg
2

(cid:107)θl+1

g − θl

g(cid:107)2

2 + Ω(θl+1).

(A.2)

By rewriting the term Lg
regrouping terms, we get:

2 (cid:107)θl+1

g − θl

g(cid:107)2

2 in the above as Lg− ˆLg

2

(cid:107)θl+1

g − θl

g(cid:107)2

2 +

ˆLg
2 (cid:107)θl+1

g − θl

g(cid:107)2

2 and

h(θl+1) ≤ ˜g(θl+1; θl) +

Lg − ˆLg
2

(cid:107)θl+1

g − θl

g(cid:107)2
2.

(A.3)

But ˜g(θl+1; θl) ≤ ˜g(θl; θl) (by the deﬁnition of θl+1 in (13)). Moreover, ˜g(θl; θl) = h(θl), which
implies ˜g(θl+1; θl) ≤ h(θl). Using the latter bound in (A.3), we arrive to the result of the lemma.
Proof of the theorem. In the rest of this proof, we utilize the following deﬁnition: E(θS) :=

(cid:96)(θS) + λ1

(cid:80)

g∈S (cid:107)θg(cid:107)2.

33

• Part 1. We will show that the event Supp(θl) (cid:54)= Supp(θl+1) cannot happen inﬁnitely often.
Suppose that Supp(θl) (cid:54)= Supp(θl+1) holds for some l. Then, either one of the following
cases must hold for g = 1 + (l mod q): (I) θl
or (II) θl
. Next, we
will consider Case (I). Since θl+1
(cid:54)= 0, then from the deﬁnition of the thresholding operator
g
(cid:113) 2λ0
in (14), we have (cid:107)θl+1(cid:107)2 >
ˆLg

. Plugging the latter inequality into Lemma 1, we get:

g = 0 (cid:54)= θl+1

g (cid:54)= 0 = θl+1

g

g

h(θl) − h(θl+1) ≥

ˆLg − Lg
ˆLg

λ0.

(A.4)

The same result in (A.4) applies for Case (II) as well. Thus, whenever the support changes,
the objective improves by a positive constant (deﬁned in the r.h.s of (A.4)), which combined
with the fact that h(θ) ≥ 0, implies that the support cannot change inﬁnitely often.

• Part 2. First, we will show that the function E(θS) is strongly convex. This trivially holds
under Assumption 1(a). Next, we will assume that only Assumption 1(b) is satisﬁed. In
this case, we have h(θ0) ≤ h(ˆθ) (where ˆθ is deﬁned in Assumption 1(b)). Since Algorithm
1 is a descent algorithm, we have h(θl) ≤ h(ˆθ) for all l ≥ 0. Thus, E(θl) + λ0G(θl) ≤
E(ˆθ)+λ0G(ˆθ), which combined with the fact that E(θl) ≥ E(ˆθ), implies that G(θl) ≤ G(ˆθ)
for all l. Thus, by the deﬁnition of k in the assumption, we have (cid:107)θl(cid:107)0 ≤ k for all l. But
since every k columns in W are linearly independent, we conclude that E(θS) is strongly
convex.

After the support stabilizes (by Part 1), Algorithm 1 becomes equivalent to minimizing the
strongly convex function E(θS) using cyclic CD. By standard results on CD (e.g., see [9]),
this is guaranteed to converge to a stationary solution θ∗ of E(θS). This establishes (15).
Finally, we will show that (16) and (17) hold. By the deﬁnition of the thresholding operator
in (14), we have

(cid:107)θl

g(cid:107)2 >

(cid:115)

2λ0
ˆLg

,

∀g ∈ S.

Taking the limit as l → ∞, we arrive to (16). Similarly, we have

(cid:107)∇θg (cid:96)(θl)(cid:107)2 ≤

(cid:113)

2λ0 ˆLg + λ1,

∀g ∈ Sc.

Taking the limit l → ∞ leads to (17).

(A.5)

(A.6)

• Part 3. After support stabilization, Algorithm 1 is equivalent to performing cyclic CD
to minimize the function E(θS). Moreover, every iterate of the algorithm after support
stabilization, i.e., θl
} (this
follows from (14)). Note that ∇θS E(θS) is group-wise Lipschitz continuous over D, i.e., the
following holds for every g ∈ [q]:

S for l ≥ K, belongs to the set D := {θS | (cid:107)θS(cid:107)2 ≥

(cid:113) 2λ0
ˆLg

(cid:107)∇θS E(θ1

S) − ∇θS E(θ2

S)(cid:107)2 ≤ ˜Lg(cid:107)θ1

S − θ2

S(cid:107)2, ∀θ1

S, θ2

S ∈ D s.t. θ1

i = θ2

i ∀i (cid:54)= g

where ˜Lg = ˆLg + 2λ1. Similarly, ∇θS E(θS) has a (global) Lipschitz constant of LS + 2|S|λ1,
over D.

34

Lemma 3.3 of [6] bounds the objective values of cyclic CD after one full cycle. Their result
holds for continuously diﬀerentiable functions whose gradient is Lipschitz over Rn. Our
function’s gradient is Lipschitz over D, but we note that [6]’s result can be easily extended
to D, leading to the following bound:

E(θlq

S ) − E(θ(l+1)q

S

) ≥

1
2η

(cid:107)∇θS E(θlq

S )(cid:107)2
2,

∀ l ≥ K,

(A.7)

where η is deﬁned in the statement of the theorem. In part 2, we have shown that E(θS)
is strongly convex. Thus, the following holds:

E(αS) ≥ E(θS) + (cid:104)∇E(θS), αS − θS(cid:105) +

σS
2

(cid:107)αS − θS(cid:107)2
2,

∀αS, θS.

(A.8)

Minimizing both sides in (A.8) w.r.t. αS and rearranging terms, we get

E(θS) − E(θ∗

S) ≤

1
2σS

(cid:107)∇θS E(θS)(cid:107)2
2,

∀θS.

Inequalities (A.7) and (A.9) lead to:

(E(θlq

S ) − E(θ∗

S)) − (E(θ(l+1)q

S

) − E(θ∗

S)) ≥

≥

1
2η
σS
η

(cid:107)∇θS E(θlq

S )(cid:107)2
2

(E(θlq

S ) − E(θ∗

S)).

Rearranging the terms in the above yields:
(cid:32)

E(θ(l+1)q) − E(θ∗) ≤

1 −

(cid:33)

σS
η

(cid:16)

(cid:17)
E(θlq) − E(θ∗)

.

(A.9)

(A.10)

(A.11)

(A.12)

Finally, we note that the function E in the above can be replaced by h (because of support
stabilization), which establishes part 3.

A.2 Proof of Theorem 2

By Theorem 1, the support of the iterates in Algorithm 1 stabilizes, say on a support S, and
converges to a solution of minθ,Supp(θ)=S h(θ). The latter observation along with the fact that Step
2 of Algorithm 2 ensures strict descent, imply that the sequence of solutions θt in Algorithm 2
must have distinct supports. Therefore, the algorithm terminates in a ﬁnite number of iterations.
Note that θ† is the output of Algorithm 1 so it must satisfy the characterization given in part 2
of Theorem 1. Moreover, the search in Step 2 must fail at θ†, and thus (21) holds.

A.3 Proof of Proposition 2

Let F1(θ, z) and F2(θ, z, s) be the objective functions in (25) and (26), respectively. Note that
by deﬁnition, v2 = F2(θ∗, z∗, s∗). Since (θ∗, z∗) is feasible for the problem corresponding to v1,
we have:

v2 − v1 ≥ F2(θ∗, z∗, s∗) − F1(θ∗, z∗)

(A.13)

Since (θ∗, z∗, s∗) is optimal for the problem of v2, it must satisfy s∗
g = 0 and s∗
otherwise (because this is the smallest value of sg, which satisﬁes (26c)). Plugging s∗
term F2(θ∗, z∗, s∗) in (A.13) and simplifying, leads to the result of the proposition.

g = 0 if z∗

(cid:107)θ∗
g(cid:107)2
2
z∗
g

g =
g into the

35

A.4 Proof of Proposition 3

The root relaxation of (26) can be written as:

min
θ

(cid:40)

˜(cid:96)(θ) + λ1

q
(cid:88)

g=1

(cid:107)θg(cid:107)2 +

q
(cid:88)

g=1

(cid:41)

min
zg,sg

(λ0zg + λ2sg)

s.t. (cid:107)θg(cid:107)2 ≤ Muzg, g ∈ [q]
2, g ∈ [q]

sgzg ≥ (cid:107)θg(cid:107)2
zg ∈ [0, 1] , sg ≥ 0, g ∈ [q]

(A.14)

(A.15)

(A.16)

(A.17)

Deﬁne

ω(θg; λ, Mu) = min
zg,sg

(λ0zg + λ2sg)

s.t.

(A.15), (A.16), (A.17).

(A.18)

Note that the above optimization problem appears inside the second summation of (A.14). Next,
we will derive a closed form expression for (A.18). Let (θg, zg, sg) be some feasible solution.
Then, the solution (θg, ˆzg, sg), where ˆzg = max{ (cid:107)θg(cid:107)2
Mu }, has an objective value which is less
sg
than or equal to that of (θg, zg, sg) (since ˆzg is the smallest possible choice of zg which satisﬁes
all the constraints)—if θg = 0 and sg = 0, we assume that (cid:107)θg(cid:107)2
= 0, which leads to ˆzg = 0.
sg
Thus, replacing constraints (A.15) and (A.16) with the constraint z = max{ (cid:107)θg(cid:107)2
, (cid:107)θg(cid:107)2
Mu } does not
sg
change the optimal objective of the problem. This replacement leads to the following equivalent
problem:

, (cid:107)θg(cid:107)2

2

2

2

ω(θg; λ, Mu) = min
zg,sg

(λ0zg + λ2sg)

s.t.

zg = max

(cid:110) (cid:107)θg(cid:107)2
2
sg

,

(cid:107)θg(cid:107)2
Mu

(cid:111)

, zg ∈ [0, 1], sg ≥ 0.

(A.19)

In the above, we can eliminate zg by plugging its expression into the the objective and the
constraint zg ∈ [0, 1], which leads to the following equivalent formulation:

ω(θg; λ, Mu) = min
sg

max

(cid:40)

λ0(cid:107)θg(cid:107)2
2
sg

(cid:124)

(cid:123)(cid:122)
Term 1

+ λ2sg

,

λ0(cid:107)θg(cid:107)2
Mu

(cid:125)

(cid:124)

(cid:123)(cid:122)
Term 2

+ λ2sg
(cid:125)

(cid:41)

s.t. sg ≥ (cid:107)θg(cid:107)2

2, (cid:107)θg(cid:107)2 ≤ Mu.

(A.20)

Suppose that Term 1 in (A.20) attains the maximum. This holds iﬀ Term 1 ≥ Term 2, which
simpliﬁes to: sg ≤ Mu(cid:107)θg(cid:107)2. Term 1 is convex in sg, so the solution of (A.20) (obtained via
g = (cid:112)λ0/λ2(cid:107)θg(cid:107)2
solving the ﬁrst order optimality condition, assuming sg ≤ Mu(cid:107)θg(cid:107)2) is given s∗
if (cid:107)θg(cid:107)2 ≤ (cid:112)λ0/λ2 ≤ M , and s∗
g = (cid:107)θg(cid:107)2
g into (A.20),
leads to ω(θg; λ, Mu) = 2λ0H((cid:112)λ2/λ0(cid:107)θg(cid:107)2), for (cid:112)λ0/λ2 ≤ (cid:107)θg(cid:107)2 ≤ Mu.

2 if (cid:112)λ0/λ2 ≤ (cid:107)θg(cid:107)2 ≤ M . Plugging s∗

Now suppose Term 2 attains the maximum in (A.20). There are two lower bounds on sg
in this case: sg ≥ Mu(cid:107)θg(cid:107)2 (from Term 1 ≤ Term 2) and sg ≥ (cid:107)θg(cid:107)2
2 (from the feasible set in
(A.20)). Since (cid:107)θg(cid:107)2 ≤ Mu, we have Mu(cid:107)θg(cid:107)2 ≥ (cid:107)θg(cid:107)2
2, which implies that sg ≥ Mu(cid:107)θg(cid:107)2 is
the only lower bound needed. Thus, we can simplify (A.20) to:

ω(θg; λ, Mu) = min
sg

λ0(cid:107)θg(cid:107)2
Mu

+ λ2sg s.t. sg ≥ Mu(cid:107)θg(cid:107)2, (cid:107)θg(cid:107)2 ≤ Mu.

36

g = Mu(cid:107)θg(cid:107)2, and this holds for (cid:112)λ0/λ2 ≥ Mu.
The optimal solution of the above is given by s∗
g into (A.20) leads to ω(θg; λ, Mu) = (λ0/Mu + λ2Mu)(cid:107)θg(cid:107)2, for (cid:112)λ0/λ2 ≥ Mu.
Plugging s∗
Finally, we replace the inner minimization in (A.14) by the closed form expression of ω(θg; λ, Mu),
which leads to the result of the proposition.

A.5 Proof of Proposition 4

Because κk,c ≥ κk,1 for c ≥ 1, it is suﬃcient to derive the stated inequality for c = 1.

We consider an arbitrary β satisfying β (cid:54)= 0 and G(β) ≤ 2k. We let J0 ⊆ [q] index the k
(cid:107)2,1 ≤ (cid:107)βJ0(cid:107)2,1. The stated

largest values in the set {(cid:107)βg(cid:107)2,1}g∈[q], noting that |J0| = k and (cid:107)βJ c
inequality follows from an observation that

0

√
2k(cid:107)Xβ(cid:107)2
√
n(cid:107)β(cid:107)2,1

≥

A.6 Proof of Theorem 3

√
√

2

2k(cid:107)Xβ(cid:107)2
n(cid:107)βJ0(cid:107)2,1

≥

κk,1√
2

.

Optimality of (cid:98)β and feasibility of β∗ imply (cid:107)y − X(cid:98)β(cid:107)2

2 ≤ (cid:107)y − Xβ∗(cid:107)2

2, which leads to

(cid:107)X((cid:98)β − β∗)(cid:107)2

2 ≤ 2(cid:15)(cid:62)X((cid:98)β − β∗).

(A.21)

We will derive a bound for the right hand side of inequality (A.21).

First, we consider a ﬁxed subset J ⊆ [q] such that |J| = 2k. We deﬁne IJ = ∪g∈J Gg and
s = ¯Tkk, noting that |IJ | ≤ 2s. We choose an orthonormal basis Φ = [φ1, ..., φ2s], such that the
2/σ2 has
corresponding linear space contains the one spanned by features {xj}j∈IJ . Then, (cid:107)Φ(cid:62)(cid:15)(cid:107)2
chi-square distribution with at most 2s degrees of freedom, and

(cid:15)(cid:62)Xθ ≤ (cid:107)Φ(cid:62)(cid:15)(cid:107)2(cid:107)Xθ(cid:107)2,

for all θ ∈ Rp with supp(θ) ⊆ IJ . Applying a chi-square tail bound [for example, the one in
Section 8.3.2 of 19], we derive that |Φ(cid:62)(cid:15)|2 (cid:46) σ2s(1 + a) with probability at least 1 − exp(−2sa).
Consequently, with probability at least 1 − exp(−2sa), inequality

(cid:15)T Xθ (cid:46)

(cid:104)
σ2s(1 + a)

(cid:105)1/2

(cid:107)Xθ(cid:107)2

(A.22)

holds uniformly for all θ ∈ Rp with supp(θ) ⊆ IJ .

We now extend this bound to all subsets J ⊆ [q] that have size 2k. Note that the number
of such subsets is bounded by (qe/2k)2k. Applying the union bound, we deduce that inequal-
ity (A.22) holds uniformly over both such J and θ with probability at least 1 − exp(−2sa +
2k log(qe/2k)). We note that G((cid:98)β − β∗) ≤ 2k and take a = ¯T −1
log(eq/2k) + [2s]−1 log(1/δ0). It
follows that

k

(cid:15)T X((cid:98)β − β∗) (cid:46)

(cid:104)
σ2k[ ¯Tk + log(eq/k)] + σ2 log(1/δ0)

(cid:105)1/2

(cid:107)X((cid:98)β − β∗)(cid:107)2,

(A.23)

with probability at least 1 − δ0. We complete the proof by combining the above bound with
inequality (A.21).

37

A.7 Proof of Corollary 2

To simpify the presentation, we deﬁne L(β) = (cid:107)y − Xβ(cid:107)2
and U B = LB/(1 − τ ), we derive

2. Because U B = L((cid:101)β), LB ≤ L(β∗),

L((cid:101)β2) ≤ L(β∗)/(1 − τ ).

As L(β∗) = (cid:107)(cid:15)(cid:107)2

2, we can rewrite the above inequality as follows:

(cid:107)y − X(cid:101)β2(cid:107)2

2 ≤ (cid:107)(cid:15)(cid:107)2/(1 − τ ).

Repeating the steps in the proof of Theorem 3, taking ito account the optimality gap, and letting
δ0 = (k/q)k/2 we arrive at inequality

(cid:107)X((cid:101)β − β∗)(cid:107)2
2

(cid:46) σ2k[ ¯Tk + log(eq/k)] + (cid:107)(cid:15)(cid:107)2τ /(1 − τ ),

which holds with probability at least 1 − (k/q)k/2. Standard chi-square tail bounds [for example,
those in Section 8.3.2 of 19] imply that, wich an appropriate multiplicative constant, inequality
(cid:107)(cid:15)(cid:107)2/n (cid:46) σ2[1 ∨ k log(eq/k)/n] holds with probability at least 1 − (k/q)k/2. Because τ ≤ 1 and
1/(1 − τ ) is upper-bounded by a universal constant, we then conclude that inequality

1
n

(cid:107)X((cid:101)β − β∗)(cid:107)2
2

(cid:46) σ2k

(cid:104) ¯Tk + log(eq/k)
n

(cid:105)

+ σ2τ

holds with probability at least 1 − (k/q)k.

A.8 Proof of Corollary 3

We let c0 be the universal constant from the error bound in Theorem 3 and deﬁne

W = (cid:107)X(cid:98)β − Xβ∗(cid:107)2

2 − c0σ2k(cid:2) ¯Tk + log(q/k)(cid:3).

By Theorem 3 we have W ≤ c0σ2 log(1/δ0) with probability at least 1 − δ0. Hence,

P(cid:0)W > w(cid:1) ≤ e−w/[c0σ2],

for every non-negative w. Consequently,

EW ≤

(cid:90) ∞

0

P(cid:0)W > w(cid:1)dw ≤

(cid:90) ∞

0

e−w/[c0σ2]dw ≤ c0σ2.

Thus, by the deﬁnition of W , we have

E(cid:107)X(cid:98)β − Xβ∗(cid:107)2

2 ≤ c0σ2k(cid:2) ¯Tk + log(q/k)(cid:3) + c0σ2,

which implies the bound in the statement of Corollary 3.

38

A.9 Proof of Theorem 4
We let B(β) = G(β)[ ˇT + log(q/G(β))] to simplify the presentation. Using the deﬁnitions of (cid:98)βk
and (cid:98)β

, we derive

B

B

(cid:107)2
2 + aB((cid:98)β

B

) ≤ (cid:107)y − X(cid:98)β

2 + aB(β∗),
(cid:107)2

∗

(cid:107)y − X(cid:98)β

which implies

B

(cid:107)X((cid:98)β

− β∗)(cid:107)2

2 + aB((cid:98)β

B

) ≤ 2(cid:15)(cid:62)X((cid:98)β

B

− β∗) + aB(β∗).

(A.24)

Revisiting the derivation of inequality (A.23) in the proof of Theorem 3, we note that (cid:98)β only
played a role through its sparsity bound G((cid:98)β) ≤ k. Thus, the corresponding probability lower-
bound applies to the event of inequality (A.23) holding for each β with G(β) ≤ k, rather than
just (cid:98)β. We let δ0 = (k/[eq])k(cid:15)0, where (cid:15)0 ∈ (0, 1) is an arbitrary value, and conclude that for each
k ∈ [q], event

Ak =

(cid:26) (cid:15)T X(β − β∗)
(cid:107)X(β − β∗)(cid:107)2

(cid:46) σ(cid:112)B(β − β∗) + log(1/(cid:15)0), ∀β ∈ Rp s.t. G(β) ≤ k

(cid:27)

holds with probability at least 1 − (cid:15)0. We note that neither (cid:15)0 nor the universal multiplicative
constant in the deﬁnition of Ak depends on k. We deﬁne A = ∩q

k=1Ak and note that

P(Ac) ≤

q
(cid:88)

k=1

P(Ac

k) ≤

q
(cid:88)

(k/[eq])k(cid:15)0 ≤

k=1

q
(cid:88)

k=1

e−k(cid:15)0 ≤ (cid:15)0.

On the event A, we have

B

(cid:15)(cid:62)X((cid:98)β

− β∗) (cid:46) σ

(cid:46) σ

(cid:113)

B((cid:98)β

(cid:113)

B((cid:98)β

B

B

− β∗) + log(1/(cid:15)0)(cid:107)X((cid:98)β

B

− β∗)(cid:107)

) + B(β∗) + log(1/(cid:15)0)(cid:107)X((cid:98)β

B

− β∗)(cid:107).

Consequently, making the universal constant a in the BIC penalty suﬃciently large and using
inequality (A.24), we deduce that

B

(cid:107)X(cid:98)β

∗

− X(cid:98)β

(cid:107)2 + σ2B((cid:98)β

B

) (cid:46) σ2B(β∗) + σ2 log(1/(cid:15)0)

(A.25)

with probability at least 1 − (cid:15)0. Repeating the argument in the proof of Corollary 3, we derive

B

E(cid:107)X(cid:98)β

∗

− X(cid:98)β

(cid:107)2 (cid:46) σ2B(β∗) + σ2,

(A.26)

which establishes the prediction error bound in the statement of Theorem 4.

It is only left to derive the group sparsity bound for (cid:98)β

. Treating expressions of the form
0 · ∞ as 0, we deﬁne function b(x) = x[ ˇT + log(q/x)] for x ∈ [0, q] and note that b(x) is monotone
increasing with b(x) ≥ x for all x ∈ [0, q]. We note that inequality (A.25) implies b(ˆk) (cid:46)
b(k∗ ∨ 1)[1 + log(1/(cid:15)0)]. Further exploiting the properties of deterministic function b(x), we
can then deduce that ˆk (cid:46) [1 + log(1/(cid:15)0)]2(k∗ ∨ 1) with probability at least 1 − (cid:15)0. Following the
argument in the proof of Corollary 3, we derive the bound Eˆk/(k∗ ∨1) (cid:46) 1+(cid:82) ∞

0 e−w1/2dw (cid:46) 1.

B

39

A.10 Proof of Theorem 5
To simplify the presentation, we write P (β) = (cid:80)
optimization problem (35). By the optimality of (cid:98)β, we have
(cid:107)X((cid:98)β − β∗)(cid:107)2
We deﬁne (cid:101)θg = ((cid:98)βg − β∗
g)/(cid:107)(cid:98)βg − β∗
predictors in group g, and observe the following inequalities:

g∈[q]

2 + λP ((cid:98)β) ≤ 2(cid:15)(cid:62)X((cid:98)β − β∗) + λP (β∗).

(A.27)

g(cid:107)2, write Xg for the submatrix of X corresponding to the

(cid:112)Tg(cid:107)βg(cid:107)2 for the penalty function in

(cid:15)(cid:62)X((cid:98)β − β∗) ≤

(cid:88)

g∈[q]

(cid:107)(cid:15)(cid:62)Xg(cid:98)θg(cid:107)2(cid:107)(cid:98)βg − β∗

g(cid:107)2 ≤ max
g∈[q]

(cid:32)

(cid:107)(cid:15)(cid:62)Xg(cid:98)θg
(cid:112)Tg

(cid:33)

(cid:13)
(cid:13)2

P ((cid:98)β − β∗).

(A.28)

Given a θ ∈ Rp, we write θ(cid:93)

1, ..., θ(cid:93)

p for a non-increasing rearrangement of |θ1|, ..., |θp|. By

Theorem 4.1 in [7], event

F = (cid:8)(cid:15)(cid:62)Xθ ≤ [4 +

√

2]σ max (cid:0) (cid:88)p

j=1

(cid:112)n log(2p/j) , (cid:112)log(1/δ0)(cid:107)Xθ(cid:107)2

(cid:1), ∀θ ∈ Rp(cid:9)

θ(cid:93)
j

holds with probability at least 1 − δ0/2. Using Stirling’s formula together with Cauchy-Schwartz
(cid:112)n log(2p/j) ≤ (cid:112)n(cid:107)θ(cid:107)0 log(2ep/(cid:107)θ(cid:107)0)(cid:107)θ(cid:107)2; we also note inequalities
inequality, we derive (cid:80)
√
(cid:107)Xθ(cid:107)2 ≤
n. Hence, as
(cid:107)(cid:98)θg(cid:107)2 = 1 for all g ∈ [q], we deduce that event F implies that

n(cid:107)θ(cid:107)1 ≤ (cid:112)n(cid:107)θ(cid:107)0(cid:107)θ(cid:107)2, which rely on the normalization (cid:107)xj(cid:107)2 =

j θ(cid:93)

√

j

max
g

(T −1/2
g

(cid:107)(cid:15)(cid:62)Xg(cid:98)θg(cid:107)2) ≤ [4 +

√

√

2]σ

n max (cid:0)(cid:112)log(2e˜q), (cid:112)log(1/δ0)(cid:1).

2]σ(cid:112)n log(2e˜q) with
Taking δ0 = 1/[2e˜q], we conclude that maxg(T −1/2
g
probability at least 1 − 1/[4e˜q]. Restricting our attention to the corresponding high-probability
event, and taking into account inequalities (A.27) and (A.28), we derive

(cid:107)(cid:15)(cid:62)Xg(cid:98)θg(cid:107)2) ≤ [4 +

√

(cid:107)X((cid:98)β − β∗)(cid:107)2

2 + λP ((cid:98)β) ≤ [8 + 2

√

2]σ(cid:112)n log(2e˜q)P ((cid:98)β − β∗) + λP (β∗).

We note that P ((cid:98)β − β∗) ≤ P ((cid:98)β) + P (β∗), chose the universal constant c0 in the statement of
Theorem 5 to satisfy c0 > 8 + 2

2, and conclude that (cid:107)X((cid:98)β − β∗)(cid:107)2

2 ≤ 2λP (β∗).

√

A.11 Proof of Proposition 5

Consider an arbitrary f ∈ A2k,ξ. Let J0 be the index set corresponding to the k components fj
with the largest (cid:107) · (cid:107)n norm. Write rn for n−m/(2m+1). Note that

rnPeng(f ) ≤ (ξ/2 − 1/2)

(cid:88)q

j=1

(cid:107)fj(cid:107)n ≤ (ξ − 1)

(cid:88)

j∈J0

(cid:107)fj(cid:107)n,

and hence

(cid:88)

(cid:107)fj(cid:107)n + rnPengr(f ) ≤ ξ

(cid:88)

(cid:107)fj(cid:107)n.

j /∈J0
Consequently, f ∈ B(J0, ξ). To complete the proof, we note that

j∈J0

√

2k(cid:107)f (cid:107)n
j=1 (cid:107)fj(cid:107)n

(cid:80)q

≥

√

2k(cid:107)f (cid:107)n

2 (cid:80)

(cid:107)fj(cid:107)n

j∈J0

√

≥ φ(k, ξ)/

2.

40

A.12 Proof of Theorem 6
By analogy with the (cid:107) · (cid:107)n notation, we deﬁne ((cid:15), v)n = (1/n) (cid:80)n
global optimality of (cid:98)f , together with the feasibility of f ∗, implies the following inequality:

i=1 (cid:15)ivi, for each v ∈ Rn. The

(cid:107)(cid:98)f − f ∗(cid:107)2

n + λnPengr( (cid:98)f ) ≤ 2((cid:15), (cid:98)f − f ∗)n + λnPengr(f ∗).

(A.29)

To control the term ((cid:15), (cid:98)f − f ∗)n we need the following result, which is proved in Section A.13.

Lemma 2. Let Fs = {f : f ∈ Cgr, G(f ) ≤ s} and let γ be a ﬁxed constant such that rn ≤ sγ.
Then, with probability at least 1 − (cid:15), inequality

((cid:15)/σ, f )n (cid:46)

(cid:104)

s1/2+γ/(2m)rn +

(cid:114)

s log(eq/s)
n

(cid:104)
+

s1/2−γ(2m−1)/(2m)r2

n + s−γrn

+

(cid:114)

(cid:114)

(cid:105)

log(1/(cid:15))
n

(cid:107)f (cid:107)n

s log(eq/s)
n

+ s−γrn

(cid:114)

(cid:105)

log(1/(cid:15))
n

Pengr(f )

holds uniformly over f ∈ Fs.

We now prove inequalities (37) and (38) in the statement of Theorem 6.
Proof of inequality (37). We note that G( (cid:98)f −f ∗) ≤ 2k. Applying Lemma 2 with f = (cid:98)f −f ∗,

s = 2k and (cid:15) = (k/q)k, we conclude that, with probability at least 1 − (k/q)k,

((cid:15)/σ, (cid:98)f − f ∗)n ≤ ˜c1k1/2(cid:104)

kγ/(2m)rn +

(cid:114)

log(eq/k)
n

(cid:105)
(cid:107)(cid:98)f − f ∗(cid:107)n
(cid:114)

(cid:104)
k1/2−γ(2m−1)/(2m)r2
+(c1/4)

n + k1/2−γrn

(cid:105)

log(eq/k)
n

Pengr( (cid:98)f − f ∗)(A.30)

for some universal constants ˜c1 and c1.

For the remainder of the proof we restrict our attention to the random event on which (A.30)
holds. We will establish a general prediction error bound, from which inequality (37) will follow
by setting19 γ = m/(2m + 1). We let

τn := 2˜c1σk1/2(cid:104)

kγ/(2m)rn +

(cid:104)

λn ≥ c1σ

k1/2−γ(2m−1)/(2m)r2

n + k1/2−γrn

and

(cid:114)

(cid:114)

log(eq/k)
n
log(eq/k)
n

(cid:105)

(cid:105)
,

noting that when γ = m/(2m + 1), the last inequality matches the corresponding lower-bound
on λn in the statement of Theorem 6. Multiplying inequality (A.29) by two and then apply-
ing (A.30) with f = (cid:98)f − f ∗, we derive

2(cid:107)(cid:98)f − f ∗(cid:107)2

n + λnPengr( (cid:98)f ) ≤ 2τn(cid:107)(cid:98)f − f ∗(cid:107)n + 3λnPengr(f ∗)
≤ (cid:107)(cid:98)f − f ∗(cid:107)2

n + 3λnPengr(f ∗).

n + τ 2

Consequently,

(cid:107)(cid:98)f − f ∗(cid:107)2
n

(cid:46) σ2k

(cid:104)

kγ/mrn +

(cid:105)

log(eq/k)
n

+ λnPengr(f ∗).

19Setting γ equal to m/(2m + 1), or any other positive value, does not violate the conditions imposed on γ in

the statement of Lemma 2.

41

Inequality (37) then follows from the above bound by letting γ = m/(2m + 1). We note that this
choice of γ optimizes the prediction error rate in the setting where Pengr(f ∗) (cid:16) σk, however, the
rate can be improved when Pengr(f ∗) and σk have diﬀerent orders of magnitude.

Proof of inequality (38). Applying Lemma 2 with s = 1 and (cid:15) = 1/q, we deduce that with

probability at least 1 − 1/q, inequality

((cid:15)/σ, fj)n (cid:46)

(cid:114)

(cid:104)
rn +

log(q)
n

(cid:105)(cid:104)

(cid:105)
(cid:107)fj(cid:107)n + rnPen(fj)

holds uniformly over f ∈ Cgr and j ∈ [q]. The above bound implies that there exists a universal
constant c0, such that

((cid:15)/σ, f )n =

q
(cid:88)

j=1

((cid:15)/σ, fj)n ≤ c0

(cid:114)

(cid:104)
rn +

log(q)
n

(cid:105)(cid:104)

q
(cid:88)

j=1

(cid:105)
(cid:107)fj(cid:107)n + rnPengr(f )

.

Letting f = (cid:98)f − f ∗, we conclude that

((cid:15)/σ, (cid:98)f − f ∗)n ≤ c0

(cid:114)

(cid:104)

rn +

log(q)
n

(cid:105)(cid:104)

q
(cid:88)

j=1

(cid:107)(cid:98)fj − f ∗

j (cid:107)n + rnPengr( (cid:98)f − f ∗)

(cid:105)

(A.31)

with probability at least 1 − 1/q.

For the remainder of the proof we restrict our attention to the random event on which (A.31)
holds. We deﬁne µn = 4c0σ(cid:2)rn + (cid:112)log(q)/n(cid:3) and let λn ≥ 4µnrnξ/(ξ − 1). Applying inequal-
ity (A.31), we rewrite inequality (A.29) as follows:

2(cid:107)(cid:98)f − f ∗(cid:107)2

n + λnPengr( (cid:98)f − f ∗) ≤ µn

q
(cid:88)

j=1

(cid:107)(cid:98)fj − f ∗

j (cid:107)n + 3λnPengr(f ∗).

(A.32)

We now consider two possible cases.
(cid:80)q

Case i): µn

j=1 (cid:107)(cid:98)fj − f ∗

j (cid:107)n ≥ 3λnPengr(f ∗). It follows that

2(cid:107)(cid:98)f − f ∗(cid:107)2

n + λnPengr( (cid:98)f − f ∗) ≤ 2µn

q
(cid:88)

j=1

(cid:107)(cid:98)fj − f ∗

j (cid:107)n,

(A.33)

and, consequently, 2rnPengr( (cid:98)f − f ∗) ≤ 4(µnrn/λn) (cid:80)q
Taking into account inequality G( (cid:98)f − f ∗) ≤ 2k and Deﬁnition 2, we then derive

j (cid:107)n ≤ (ξ − 1) (cid:80)q

j=1 (cid:107)(cid:98)fj − f ∗

j=1 (cid:107)(cid:98)fj − f ∗

j (cid:107)n.

(cid:107)(cid:98)fj − f ∗

j (cid:107)n ≤ [2k]1/2[ψ(2k, ξ)]−1(cid:107)(cid:98)f − f ∗(cid:107)n.

(A.34)

(cid:88)

j≤q

Combining this bound with inequality (A.33), we colclude

(cid:107)(cid:98)f − f ∗(cid:107)2

n ≤ µn[2k]1/2[ψ(2k, ξ)]−1(cid:107)(cid:98)f − f ∗(cid:107)n,

which implies the stated prediction error bound.
j=1 (cid:107)(cid:98)fj − f ∗

Case ii): µn

(cid:80)q

j (cid:107)n < 3λnPengr(f ∗). Going back to inequality (A.32), we derive

2(cid:107)(cid:98)f − f ∗(cid:107)2

n + λnPengr( (cid:98)f − f ∗) ≤ 6λnPengr(f ∗),

which implies the stated prediction error bound.

42

A.13 Proof of Lemma 2
Given J ⊆ [q], we deﬁne a functional class F(J) = {f : f (x) = (cid:80)
need the following result, which is proved in Section A.14.

j∈J fj(xj), fj ∈ C}. We will

Lemma 3. Let J ⊆ [q] and let γ be a ﬁxed constant such that rn ≤ |J|γ. Then, with probability
at least 1 − e−t, inequality

((cid:15)/σ, f )n (cid:46)

(cid:104)

|J|1/2+γ/(2m)rn + (cid:112)t/n

(cid:105)
(cid:107)f (cid:107)n +

(cid:104)
|J|1/2−γ(2m−1)/(2m)r2

n + |J|−γrn

(cid:105)

(cid:112)t/n

Pengr(f )

holds uniformly over f ∈ F(J).

Let Ms denote the number of distinct subsets of [q] that have size s. We note that log(Ms) ≤
s log(eq/s) and, thus, Mse−t ≤ es log(eq/s)−t. Applying Lemma 3 together with the union bound,
we derive that, with probability at least 1 − es log(eq/s)−t, inequality

((cid:15)/σ, f )n (cid:46)

(cid:104)
s1/2+γ/(2m)rn + (cid:112)t/n

(cid:105)

(cid:107)f (cid:107)n +

(cid:104)

s1/2−γ(2m−1)/(2m)r2

n + s−γrn

(cid:112)t/n

(cid:105)
Pengr(f )

holds uniformly over f ∈ Fs. We complete the proof by noting that for t = s log(eq/s) + log(1/(cid:15))
the above inequality becomes

((cid:15)/σ, f )n (cid:46)

(cid:104)

s1/2+γ/(2m)rn +

(cid:114)

s log(eq/s)
n

(cid:104)
s1/2−γ(2m−1)/(2m)r2
+

n + s−γrn

+

(cid:114)

(cid:114)

(cid:105)

log(1/(cid:15))
n

(cid:107)f (cid:107)n

s log(eq/s)
n

+ s−γrn

(cid:114)

(cid:105)

log(1/(cid:15))
n

Pengr(f ),

and the corresponding lower-bound on the probability simpliﬁes to 1 − (cid:15).

A.14 Proof of Lemma 3

Given a positive constant δ and a metric space H endowed with the norm (cid:107)·(cid:107), we use the standard
notation and write H(δ, H, (cid:107) · (cid:107)) for the δ-entropy of H with respect to (cid:107) · (cid:107). More speciﬁcally,
H(δ, H, (cid:107) · (cid:107)) is the natural logarithm of the smallest number of balls with radius δ needed to
cover H.

With a slight abuse of notation, we extend the domain of (cid:107) · (cid:107)n from vectors in Rn to real-
valued functions on [0, 1]q by letting (cid:107) · (cid:107)n be the empirical L2-norm. Thus, given a function h,
we let (cid:107)h(cid:107)n = [(cid:80)n
i=1 h(xi)2/n]1/2. This extension is consistent in the sense that (cid:107)f (cid:107)n = (cid:107)f (cid:107)n and
(cid:107)fj(cid:107)n = (cid:107)fj(cid:107)n for f ∈ Cgr, j ∈ [q].

We let H(J) = {h : h ∈ F(J), (cid:107)h(cid:107)n/(rn|J|−γ) + Pengr(h) ≤ 1}, noting that (cid:107)h(cid:107)n ≤ rn|J|−γ
and Pengr(h) ≤ 1 for every h ∈ H(J). By Corollary 8.3 in [72] (cf. Lemma 12 in the supplementary
material for [68]),

((cid:15)/σ, h)n (cid:46) n−1/2

sup
h∈H(J)

(cid:90) rn|J|−γ

0

(cid:112)H(u, H(J), (cid:107) · (cid:107)n)du + rn|J|−γ(cid:112)t/n

(A.35)

with probability at least 1 − e−t. To bound the entropy, we will use the following result, proved
in Section A.15.

43

Lemma 4. H(u, H(J), (cid:107) · (cid:107)n) (cid:46) |J|(1/u)1/m for u ∈ (0, 1).

Noting that rn = n−m/(2m+1) and, thus, n−1/2 = r(2m+1)/(2m)

n

, we derive

(cid:90) rn|J|−γ

n−1/2

0

(cid:112)H(u, H(J), (cid:107) · (cid:107)n)du (cid:46) n−1/2

(cid:90) rn|J|−γ

0

|J|1/2u−1/(2m)du

(cid:46) |J|1/2n−1/2(cid:104)

rn|J|−γ(cid:105)(2m−1)/(2m)

= r(2m+1)/(2m)+(2m−1)/(2m)

n

|J|1/2−γ(2m−1)/(2m)

= r2

n|J|1/2−γ(2m−1)/(2m).

Applying bound (A.35), we conclude that

((cid:15)/σ, h)n (cid:46) r2

n|J|1/2−γ(2m−1)/(2m) + rn|J|−γ(cid:112)t/n

sup
h∈H(J)

with probability at least 1 − e−t. The statement of the lemma is then a consequence of the fact
that for every f ∈ F(J), function f /(cid:2)(cid:107)f (cid:107)n/(rn|J|−γ) + Pengr(f )(cid:3) falls in the class H(J).

A.15 Proof of Lemma 4

We will establish the stated entropy bound for the functional space H(cid:48)
J = {h : h ∈ F(J), (cid:107)h(cid:107)n +
Pengr(h) ≤ 1}. The same bound will then automatically hold for H(J), because rn|J|−γ ≤ 1 and,
hence, H(cid:48)
J ⊆ H(J). We treat m as ﬁxed, so that universal constants in inequalities below are
allowed to depend on m.

Consider an arbitrary g ∈ C. By the Sobolev embedding theorem [for example, 59, Theorem
3.13], we can write g as a sum of a polynomial of degree m − 1 and a function ˜g that satisﬁes
(cid:46) Pen(g), where we note that Pen(g) = Pen(˜g). Applying Lemma 10.9 in [72], which
(cid:107)˜g(cid:107)L2
J ⊆ {p + ˜h : p ∈
builds on the interpolation inequality of [1], we derive (cid:107)˜g(cid:107)∞ (cid:46) Pen(˜g). Thus, H(cid:48)
PJ , ˜h ∈ ˜HJ }, where

PJ = {p : p(x) = α0 +

(cid:88)

m−1
(cid:88)

αjlxl

j, α0 ∈ R, αjl ∈ R ∀j, k, (cid:107)p(cid:107)n ≤ 2}

l=1
˜HJ = {˜h : ˜h ∈ F(J), Pengr(˜h) ≤ 1, (cid:107)˜hj(cid:107)∞ (cid:46) Pen(˜hj) ∀j ∈ J}.

j∈J

We are able to impose the bound (cid:107)p(cid:107)n ≤ 2 in the deﬁnition of PJ , because if h = p + ˜h for h ∈ H(cid:48)
J
and ˜h ∈ ˜HJ , then (cid:107)p + ˜h(cid:107)n ≤ 1 and (cid:107)˜h(cid:107)n ≤ Pengr(˜h) ≤ 1. Consequently,

H(u, H(cid:48)

J , (cid:107) · (cid:107)n) ≤ H(u/2, PJ , (cid:107) · (cid:107)n) + H(u/2, ˜HJ , (cid:107) · (cid:107)∞),

(A.36)

where we used the fact that the unit ball with respect to the (cid:107) · (cid:107)∞-norm is contained within
the corresponding ball with respect to the (cid:107) · (cid:107)n-norm. We note that PJ is a ball of radis 2,
with respect to the (cid:107) · (cid:107)n-norm, in a linear functional space of dimension |J|(m − 1) + 1. Hence,
H(u/2, PJ , (cid:107) · (cid:107)n) (cid:46) |J| + |J| log(1/u) by, for example, Corollary 2.6 in [72]. Thus, the result of
Lemma 4 follows from A.36 if we also establish that H(δ, ˜HJ , (cid:107) · (cid:107)∞) (cid:46) |J|(1/δ)1/m for δ ∈ (0, 1).

44

It is only left to derive the stated bound on H(δ, ˜HJ , (cid:107) · (cid:107)∞). Note that we can represent

functional class ˜HJ as follows:

˜HJ =




˜h : ˜h(x) =



(cid:88)

j∈J

λjgj(xj),

(cid:88)

j∈J

|λj| ≤ 1, gj ∈ C, Pen(gj) ≤ 1, (cid:107)gj(cid:107)∞ ≤ 1 ∀j ∈ J






.

Given functions ˜h(x) = (cid:80)

j∈J λjgj(xj) and ˜h(cid:48)(x) = (cid:80)

j∈J λ(cid:48)

(cid:107)˜h − ˜h(cid:48)(cid:107)∞ ≤ (cid:107)

(cid:88)

j∈J

λjgj −

(cid:88)

j∈J

λjg(cid:48)

j(cid:107)∞ + (cid:107)

j∈J

jg(cid:48)
(cid:88)

j(xj) in ˜HJ , we have
(cid:88)

λjg(cid:48)

j −

jg(cid:48)
λ(cid:48)

j(cid:107)∞

j∈J
(cid:88)

|λj − λ(cid:48)
j|

j∈J

≤ max
j∈J

(cid:107)gj − g(cid:48)

j(cid:107)∞

(cid:88)

j∈J

|λj| + max
j∈J

(cid:107)g(cid:48)

j(cid:107)∞

≤ max
j∈J

(cid:107)gj − g(cid:48)

j(cid:107)∞ +

|λj − λ(cid:48)

j|.

(cid:88)

j∈J

Consequently, if we let G = {g : g ∈ C, Pen(g) ≤ 1, (cid:107)g(cid:107)∞ ≤ 1}, let (cid:107) · (cid:107)1 denote the (cid:96)1-norm and
let Bd

1 denote a unit (cid:96)1-ball in Rd, then

H(δ, ˜HJ , (cid:107) · (cid:107)∞) ≤ |J|H(δ/2, G, (cid:107) · (cid:107)∞) + H(δ/2, B|J|

1 , (cid:107) · (cid:107)1).

By the results in [13], H(δ/2, Gj, (cid:107) · (cid:107)∞) (cid:46) (1/δ)1/m. By the standard bounds on the covering
numbers of a norm ball, H(δ/2, B|J|
1 , (cid:107) · (cid:107)1) (cid:46) |J| + |J| log(1/δ). Thus, H(δ, ˜HJ , (cid:107) · (cid:107)∞) (cid:46)
|J|(1/δ)1/m for δ ∈ (0, 1).

B Additional Experimental Results

B.1 Performance for Varying Number of Observations

In Figure B.1, we report the results of the experiment of Section 6.1.1 with HBIC tuning [75] for
Group MCP and SCAD. With HBIC tuning, solutions from Group MCP and SCAD are more
sparse with worse prediction accuracy, compared to validation MSE tuning. In Figures B.2 and
B.3, we report the results of the same experiment in Section 6.1.1 but with lower correlation
coeﬃcients ρ = 0.5 and ρ = 0.0, respectively. Each of the ﬁgures B.2 and B.3 presents results
based on MSE validation tuning and HBIC tuning (for Group SCAD and MCP).

B.2 Performance for Varying SNR

Here we study the performance of the diﬀerent algorithms for varying SNR. Similar to the ex-
periment of Section 6.1.1, we ﬁx a correlation parameter ρ = 0.9, p = 5000, a group size of 4,
number of nonzero groups k∗ = 25. We vary the SNR in {0.5, 1, 2, 4, 6, 8, 10} and the sample size
n ∈ {1000, 5000}. The results for n = 5000 and n = 1000 are shown in Figures B.4 and B.5,
respectively. Each ﬁgure presents results based on MSE validation tuning and HBIC tuning (for
Group SCAD and MCP).

B.3 Statistical Performance on High-dimensional Instances

In Table B.1, we report the results of the experiment of Section 6.1.2 using HBIC tuning for
Group MCP and SCAD estimators and validation MSE tuning for other estimators.

45

High Correlation Setting (ρ = 0.9). Tuning: Validation MSE for (cid:96)0 and Lasso; HBIC for SCAD
and MCP.

Figure B.1: Performance measures for varying number of observations on a synthetic dataset with
a correlation coeﬃcient ρ = 0.9. We use HBIC-based tuning for Group SCAD and MCP, and
validation MSE-based tuning for other estimators. The standard error of the mean is represented
using error bars. Alg. 1 and Alg. 2 are our proposed algorithms. Here, “Lasso” is a shorthand
for Group Lasso, we use the same convention for SCAD, MCP.

Table B.1: Performance measures for Setting 1 (top panel) and Setting 2 (bottom panel) where,
tuning parameters for Group MCP and SCAD are selected by HBIC tuning, and other estimators
are chosen by validation MSE tuning. Means are reported along with their standard errors (we
consider 500 replications).

Algorithm

(cid:107)ˆβ(cid:107)0

TP

FP

MSE

1

g
n
i
t
t
e
S

100.3 (1.2)
Group (cid:96)0
2086.1 (27.8)
Group Lasso
Group MCP
162.0 (5.5)
Group SCAD 368.0 (13.4)

2 Group (cid:96)0

g
n
i
t
t
e
S

Group Lasso
Group MCP
Group SCAD

79.4 (0.1)
1126.5 (11.7)
109.4 (1.5)
196.2 (4.4)

8.9 (0.09)
9.6 (0.05)
6.0 (0.13)
6.9 (0.12)

19.7 (0.03)
19.8 (0.02)
19.4 (0.07)
19.5 (0.05)

1.1 (0.1)
199.0 (2.8)
10.1 (0.5)
29.9 (1.3)

0.2 (0.02)
261.8 (2.9)
8.0 (0.3)
29.5 (1.1)

19.2 (1.1)
26.9 (1.1)
48.8 (1.4)
147.8 (20.2)

1.12 (0.03)
5.06 (0.10)
2.62 (0.26)
3.27 (0.22)

(cid:107)ˆβ − β∗(cid:107)∞

1.17 (0.03)
1.44 (0.02)
1.7 (0.03)
1.57 (0.03)

0.356 (0.007)
0.703 (0.007)
0.438 (0.010)
0.534 (0.009)

B.4 Performance on the Birthweight Dataset

We study the Birthweight dataset, taken from the R package grpreg. Here, we predict birth weight
using 7 grouped covariates. The dataset has 189 observations, which we randomly split into 75%
for training and 25% for testing. On this dataset, we ﬁt regularization paths for Group (cid:96)0, Lasso,
and SCAD. For Group (cid:96)0, we use an additional (cid:96)2 regularization and consider λ2 ∈ {1, 2, 4}. In
Figure B.6, we plot the test MSE versus the sparsity level for the diﬀerent methods. The results
show that the Group (cid:96)0-based methods outperform Group Lasso and SCAD when the group size
is 2 or more.

46

500100015002000250030003500400045005000Number of Observations0.00.20.40.60.81.0Recovery F1 Score500100015002000250030003500400045005000Number of Observations012345Test MSE500100015002000250030003500400045005000Number of Observations020406080100120140Support SizeAlg. 1Alg. 2MCPSCADLassoMild Correlation Setting (ρ = 0.5). Tuning: Validation MSE for all methods.

Mild Correlation Setting (ρ = 0.5). Tuning: Validation MSE for (cid:96)0 and Lasso; HBIC for SCAD
and MCP.

Figure B.2: Performance measures for varying number of observations on a synthetic dataset with
a correlation coeﬃcient ρ = 0.5. Top panel shows validation MSE-based tuning for all methods,
bottom panel shows HBIC-based tuning for SCAD and MCP. The standard error of the mean is
represented using error bars. Alg. 1 and Alg. 2 are our proposed algorithms. Here, “Lasso” is a
shorthand for Group Lasso, we use the same convention for SCAD, MCP.

B.5 Additional Timing Comparisons

Here we consider the same setup as in the experiment of Section 6.2, and we report the running
times for additional values of Mu to demonstrate the sensitivity of the runtime to Mu. Let M ∗
be the value of Mu used in Section 6.2 – note that this is the smallest value of Mu. We express
our choices of Mu in terms of M ∗. We report the results for cases (i) and (ii) in Tables B.2 and
B.3, respectively.

B.5.1 Timings with diﬀerent n

To understand the sensitivity of runtimes of our BnB procedure for diﬀerent values of n, we ran
the same experiment of Section 6.2 with diﬀerent values of n ∈ {1000, 5000, 7000, 10000} with
p = 104 held ﬁxed. This experiment was carried out on a machine with a 6-core Intel Core i7-
8750H processor and 16GB of RAM—due to memory limits, we did not consider larger values of
n. The running time is reported in Table B.4 below. The table shows that the runtime increases
with increasing n — we believe this is mainly due to the increased runtimes in solving the node

47

500100015002000250030003500400045005000Number of Observations0.00.20.40.60.81.0Recovery F1 Score500100015002000250030003500400045005000Number of Observations012345Test MSE500100015002000250030003500400045005000Number of Observations020406080100120140Support SizeAlg. 1Alg. 2MCPSCADLasso500100015002000250030003500400045005000Number of Observations0.00.20.40.60.81.0Recovery F1 Score500100015002000250030003500400045005000Number of Observations012345Test MSE500100015002000250030003500400045005000Number of Observations020406080100120140Support SizeAlg. 1Alg. 2MCPSCADLassoUncorrelated Setting (ρ = 0). Tuning: Validation MSE for all methods.

Uncorrelated Setting (ρ = 0). Tuning: Validation MSE for (cid:96)0 and Lasso; HBIC for SCAD and
MCP.

Figure B.3: Performance measures for varying number of observations on a synthetic dataset with
a correlation coeﬃcient ρ = 0. Top panel shows validation MSE-based tuning for all methods,
bottom panel shows HBIC-based tuning for SCAD and MCP. The standard error of the mean is
represented using error bars. Alg. 1 and Alg. 2 are our proposed algorithms. Here, “Lasso” is a
shorthand for Group Lasso, we use the same convention for SCAD, MCP.

48

500100015002000250030003500400045005000Number of Observations0.00.20.40.60.81.0Recovery F1 Score500100015002000250030003500400045005000Number of Observations012345Test MSE500100015002000250030003500400045005000Number of Observations020406080100120140Support SizeAlg. 1Alg. 2MCPSCADLasso500100015002000250030003500400045005000Number of Observations0.00.20.40.60.81.0Recovery F1 Score500100015002000250030003500400045005000Number of Observations012345Test MSE500100015002000250030003500400045005000Number of Observations020406080100120140Support SizeAlg. 1Alg. 2MCPSCADLassoVarying SNR: n = 5000, p = 5000. Tuning: Validation MSE for all methods.

Varying SNR: n = 5000, p = 5000. Tuning: Validation MSE for (cid:96)0 and Lasso; HBIC for SCAD
and MCP.

Figure B.4: Performance measures for varying SNR on a synthetic dataset with n = 5000,
p = 5000, ρ = 0.9. Top panel shows validation MSE-based tuning for all methods, bottom panel
shows HBIC-based tuning for SCAD and MCP. The standard error of the mean is represented
using error bars. Alg. 1 and Alg. 2 are our proposed algorithms.

49

0.52.04.06.08.010.0SNR0.00.20.40.60.81.0Recovery F1 Score0.52.04.06.08.010.0SNR024681012Test MSEAlg. 1Alg. 2MCPSCADLasso0.52.04.06.08.010.0SNR0255075100125150175200Support Size0.52.04.06.08.010.0SNR0.00.20.40.60.81.0Recovery F1 Score0.52.04.06.08.010.0SNR024681012Test MSEAlg. 1Alg. 2MCPSCADLasso0.52.04.06.08.010.0SNR0255075100125150175200Support SizeVarying SNR: n = 1000, p = 5000. Tuning: Validation MSE for all methods.

Varying SNR: n = 1000, p = 5000. Tuning: Validation MSE for (cid:96)0 and Lasso; HBIC for SCAD
and MCP.

Figure B.5: Performance measures for varying SNR on a synthetic dataset with n = 1000,
p = 5000, ρ = 0.9. Top panel shows results based on validation MSE-tuning for all methods; and
bottom panel shows results for HBIC-based tuning for Group MCP and SCAD. The standard
error of the mean is represented using error bars. Alg. 1 and Alg. 2 are our proposed algorithms.
Here Lasso is a shorthand for Group Lasso (similar convention applies to MCP, SCAD).

Figure B.6: Test MSE on the Birthweight dataset. For Group (cid:96)0, we consider additional ridge
regularization and vary the corresponding regularization parameter λ2 ∈ {1, 2, 4}. Group sizes 3
and 4 could not be attained using Group Lasso and SCAD.

50

0.52.04.06.08.010.0SNR0.00.20.40.60.81.0Recovery F1 Score0.52.04.06.08.010.0SNR051015202530354045Test MSEAlg. 1Alg. 2MCPSCADLasso0.52.04.06.08.010.0SNR0255075100125150175200Support Size0.52.04.06.08.010.0SNR0.00.20.40.60.81.0Recovery F1 Score0.52.04.06.08.010.0SNR051015202530354045Test MSEAlg. 1Alg. 2MCPSCADLasso0.52.04.06.08.010.0SNR0255075100125150175200Support Sizerelaxations of the BnB tree.

Table B.2: Running time in seconds for solving case (i), i.e., the MIP in (26) with λ2 = λ∗
2, to
optimality. A dash (-) indicates that Gurobi cannot solve the problem in 24 hours and has an
optimality gap of 100% upon termination.

p

103
104
105
106
5 × 106

Mu = M ∗

Mu = 1.5M ∗
Ours Gurobi Ours Gurobi Ours Gurobi

Mu = ∞

96
199
231
386
1922

24223
-
-
-
-

186
245
404
1014
3686

12320
-
-
-
-

192
333
421
1250
4036

2399
-
-
-
-

Table B.3: Running time in seconds for solving case (ii), i.e., the MIP in (26) with λ2 = 0, to
optimality. A star or dash (-) indicates that the solver cannot solve the problem in 24 hours. For
star, the optimality gap (in percent) is shown in parenthesis, whereas the gap is 100% for dash.

p

103
104
105
106

Mu = M ∗
Ours Gurobi
373
466
1136
1628

8737
-
-
-

Mu = 1.5M ∗
Ours
913
2813
*(4.7)
*(5.1)

Gurobi
10675
-
-
-

Mu = 2M ∗

Ours
1010
*(3.9)
*(20.7)
*(21.6)

Gurobi
13901
-
-
-

Table B.4: Running time (in seconds) of our BnB method for diﬀerent values of n and p = 104.
Additional details can be found in Section B.5.1.

n

p

Time (seconds)

1000
5000
7000
10000

104
104
104
104

199
340
556
5796

C Additional Details on the Datasets

C.1 Description of the Amazon Reviews Dataset

This dataset is a subset of the Amazon Grocery and Gourmet Food dataset [36]. To obtain
X and y, we follow the same steps described in [33], and we restrict X to the top 5500 words
in the corpus. Here X is a TF/IDF representation of the text reviews and y is a continuous

51

variable which measures review helpfulness. To obtain the groups, we employ an unsupervised
method that only makes use of the covariates. We draw inspiration from the work of [20], who
use a clustering on the features followed by a group Lasso procedure on the selected groups. We
run Latent Dirichlet Allocation (LDA) [15] on the corpus using scikit-learn [62], where we
set the number of groups to 100. We then use the LDA solution to construct a collection of
i=1, each corresponding to a topic. Here π(i)
probability vectors {π(i)}100
refers to the probability of
encountering word j in topic i. We assign word j to the group with index arg maxi{π(i)
i=1 (i.e.,
to the group that allocates j the highest probability). For example, the top 5 words in group 1
are “coﬀee roast cup keurig cups” so the topic is on coﬀee. Group 2 has “bpa worse cans dented
claim”, which refers to problems with the packaging of the product. To obtain the training set, we
sub-sample uniformly at random from the corpus and remove any covariates with zero variance
(after sub-sampling), which reduces the number of covariates from 5500 to 3482. Note that the
100 groups have diﬀerent sizes, ranging between 9 and 85.

j }100

j

We note that the above grouping procedure is one of many possible ways to obtain a grouping
of the features. Our goal here is to obtain a partition of the features, to be used as an input for
all the group sparse estimators, so that we are able to compare the performances of the diﬀerent
estimators. The downstream results depend upon the input groups. It may be interesting to see
if one can simultaneously learn the grouping structure and build a sparse prediction model so as
to optimize a suitable joint estimation criterion. This, of course, goes beyond the scope of the
group-selection problem that we are studying in this paper, and is left as future work.

References

[1] S Agmon. Lectures on Elliptic Boundary Value Problems. Van Nostrand, Princeton, NJ,

1965.

[2] Alper Atamturk, Andres Gomez, and Shaoning Han. Sparse and smooth signal estimation:
Convexiﬁcation of l0-formulations. Journal of Machine Learning Research, 22(52):1–43, 2021.

[3] F.R. Bach. Consistency of the group lasso and multiple learning kernel. Journal of Machine

Learning Research, 9:1179–1225, 2008.

[4] Francis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozinski. Structured

sparsity through convex optimization. Statistical Science, 27(4):450–468, 2012.

[5] Amir Beck and Yonina C. Eldar. Sparsity constrained nonlinear optimization: Optimality

conditions and algorithms. SIAM Journal on Optimization, 23(3):1480–1509, 2013.

[6] Amir Beck and Luba Tetruashvili. On the convergence of block coordinate descent type

methods. SIAM Journal on Optimization, 23(4):2037–2060, 2013.

[7] Pierre C Bellec, Guillaume Lecu´e, and Alexandre B Tsybakov. Slope meets lasso: improved

oracle bounds and optimality. The Annals of Statistics, 46(6B):3603–3642, 2018.

[8] Pietro Belotti, Christian Kirches, Sven Leyﬀer, Jeﬀ Linderoth, James Luedtke, and Ashutosh
Mahajan. Mixed-integer nonlinear optimization. Acta Numerica, 22, 05 2013. doi: 10.1017/
S0962492913000032.

52

[9] D.P. Bertsekas. Nonlinear Programming. Athena scientiﬁc optimization and computation
series. Athena Scientiﬁc, 2016. ISBN 9781886529052. URL https://books.google.com/
books?id=TwOujgEACAAJ.

[10] Dimitris Bertsimas and Jack Dunn. Machine learning under a modern optimization lens.

Dynamic Ideas LLC, 2019.

[11] Dimitris Bertsimas and Bart Van Parys. Sparse high-dimensional regression: Exact scalable

algorithms and phase transitions. The Annals of Statistics, 48(1):300–323, 2020.

[12] Dimitris Bertsimas, Angela King, and Rahul Mazumder. Best subset selection via a modern

optimization lens. Annals of Statistics, 44(2):813–852, 2016.

[13] M. S. Birman and M. Z. Solomjak. Piecewise-polynomial approximations of functions of the

classes wα

p . Math. USSR-Sbornik, 2(3):295–317, 1967.

[14] Robert E Bixby. A brief history of linear and mixed-integer programming computation.
Documenta Mathematica, Extra Volume: Optimization Stories, pages 107–121, 2012.

[15] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of

machine Learning research, 3(Jan):993–1022, 2003.

[16] Thomas Blumensath and Mike Davies.

Iterative thresholding for sparse approximations.

Journal of Fourier Analysis and Applications, 14(5-6):629–654, 2008.

[17] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press,

Cambridge, 2004.

[18] Patrick Breheny and Jian Huang. Group descent algorithms for nonconvex penalized linear
and logistic regression models with grouped predictors. Statistics and computing, 25(2):
173–187, 2015.

[19] P. B¨uhlmann and S. Van de Geer. Statistics for high-dimensional data: methods, theory and

applications. Springer, 2011.

[20] Peter B¨uhlmann, Philipp R¨utimann, Sara van de Geer, and Cun-Hui Zhang. Correlated
variables in regression: clustering and sparse estimation. Journal of Statistical Planning and
Inference, 143(11):1835–1858, 2013.

[21] Jie Chen and Xiaoming Huo. Theoretical results on sparse representations of multiple-
measurement vectors. IEEE Transactions on Signal processing, 54(12):4634–4643, 2006.

[22] C. Chesneau and M. Hebiri. Some theoretical results on the grouped variables lasso. Math-

ematical Methods of Statistics, 17:317–326, 2008.

[23] Jens Clausen and Michael Perregaard. On the best search strategy in parallel branch-and-
bound: Best-ﬁrst search versus lazy depth-ﬁrst search. Annals of Operations Research, 90:
1–17, 1999.

53

[24] Shane F Cotter, Bhaskar D Rao, Kjersti Engan, and Kenneth Kreutz-Delgado. Sparse
solutions to linear inverse problems with multiple measurement vectors. IEEE Transactions
on Signal Processing, 53(7):2477–2488, 2005.

[25] Antoine Dedieu, Hussein Hazimeh, and Rahul Mazumder. Learning sparse classiﬁers: Con-
tinuous and mixed integer optimization perspectives. Journal of Machine Learning Research,
22(135):1–47, 2021.

[26] Santanu S. Dey, Rahul Mazumder, and Guanyi Wang. Using l1-relaxation and integer pro-

gramming to obtain dual bounds for sparse pca, 2021.

[27] H. Dong, K. Chen, and J. Linderoth. Regularization vs. Relaxation: A conic optimization

perspective of statistical variable selection. ArXiv e-prints, October 2015.

[28] Antonio Frangioni and Claudio Gentile. Perspective cuts for a class of convex 0–1 mixed

integer programs. Mathematical Programming, 106(2):225–236, 2006.

[29] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Regularization paths for generalized
linear models via coordinate descent. Journal of Statistical Software, 33(1):1–22, 2010. URL
http://www.jstatsoft.org/v33/i01/.

[30] Oktay G¨unl¨uk and Jeﬀ Linderoth. Perspective reformulations of mixed integer nonlinear
programs with indicator variables. Mathematical programming, 124(1-2):183–205, 2010.

[31] T. Hastie and R. Tibshirani. Generalized Additive Models. Chapman and Hall, London,

1990.

[32] Trevor Hastie, Robert Tibshirani, and Martin Wainwright. Statistical Learning with Sparsity:

The Lasso and Generalizations. CRC Press, FL, 2015.

[33] Hussein Hazimeh and Rahul Mazumder. Fast best subset selection: Coordinate descent and

local combinatorial optimization algorithms. Operations Research, 68(5):1517–1537, 2020.

[34] Hussein Hazimeh and Rahul Mazumder. Learning hierarchical interactions at scale: A convex
optimization approach. In International Conference on Artiﬁcial Intelligence and Statistics,
pages 1833–1843, 2020.

[35] Hussein Hazimeh, Rahul Mazumder, and Ali Saab. Sparse regression at scale: Branch-and-

bound rooted in ﬁrst-order optimization. arXiv preprint arXiv:2004.06152, 2020.

[36] Ruining He and Julian McAuley. Ups and downs: Modeling the visual evolution of fashion
trends with one-class collaborative ﬁltering. In Proceedings of the 25th International Confer-
ence on World Wide Web, WWW ’16, page 507–517, Republic and Canton of Geneva, CHE,
2016. International World Wide Web Conferences Steering Committee. ISBN 9781450341431.
doi: 10.1145/2872427.2883037. URL https://doi.org/10.1145/2872427.2883037.

[37] J. Huang and T. Zhang. The beneﬁt of group sparsity. The Annals of Statistics, 38:1978–

2004, 2010.

[38] J. Huang, J.L. Horowitz, and F. Wei. Variable selection in nonparametric additive models.

The Annals of Statistics, 38:2282–2313, 2010.

54

[39] J. Huang, B. Breheny, and S. Ma. A selective review of group selection in high-dimensional

models. Statistical Science, 27:481–499, 2012.

[40] Michael J¨unger, Thomas M Liebling, Denis Naddef, George L Nemhauser, William R Pul-
leyblank, Gerhard Reinelt, Giovanni Rinaldi, and Laurence A Wolsey. 50 Years of Integer
Programming 1958-2008: From the Early Years to the State-of-the-art. Springer Science &
Business Media, 2009.

[41] Yongdai Kim, Sunghoon Kwon, and Hosik Choi. Consistent model selection criteria on high

dimensions. The Journal of Machine Learning Research, 13:1037–1057, 2012.

[42] Vladimir Koltchinskii and Ming Yuan. Sparsity in multiple kernel learning. The Annals of

Statistics, 38(6):3660–3695, 2010.

[43] Sokbae Lee, Yuan Liao, Myung Hwan Seo, and Youngki Shin. Factor-driven two-regime

regression. The Annals of Statistics, 49(3):1656–1678, 2021.

[44] Sokbae Lee, Yuan Liao, Myung Hwan Seo, and Youngki Shin. Sparse hp ﬁlter: Finding kinks

in the covid-19 contact rate. Journal of econometrics, 220(1):158–180, 2021.

[45] Y. Lin and H. H. Zhang. Component selection and smoothing in multivariate nonparametric

regression. The Annals of Statistics, 34:2272–2297, 2006.

[46] K. Lounici, M. Pontil, S. van de Geer, and A. Tsybakov. Oracle inequalities and optimal

inference under group sparsity. The Annals of Statistics, 39(4):2164–2204, 2011.

[47] Zhaosong Lu.

Iterative hard thresholding methods for l0 regularized convex cone pro-
gramming. Mathematical Programming, 147(1):125–154, Oct 2014. ISSN 1436-4646. doi:
10.1007/s10107-013-0714-4. URL https://doi.org/10.1007/s10107-013-0714-4.

[48] Dmitry Malioutov, M¨ujdat Cetin, and Alan S Willsky. A sparse signal reconstruction per-
spective for source localization with sensor arrays. IEEE transactions on signal processing,
53(8):3010–3022, 2005.

[49] Hasan Manzour, Simge Kucukyavuz, Hao-Hsiang Wu, and Ali Shojaie. Integer programming
for learning directed acyclic graphs from continuous data. Informs Journal on Optimization,
3(1):46–73, 2021.

[50] Rahul Mazumder and Peter Radchenko. The Discrete Dantzig Selector: Estimating sparse
IEEE Transactions on Information

linear models via mixed integer linear optimization.
Theory, 63 (5):3053 – 3075, 2017.

[51] Rahul Mazumder, Jerome Friedman, and Trevor Hastie. Sparsenet: Coordinate descent with
non-convex penalties. Journal of the American Statistical Association, 117(495):1125–1138,
2011.

[52] Rahul Mazumder, Peter Radchenko, and Antoine Dedieu. Subset selection with shrinkage:

Sparse linear modeling when the snr is low. arXiv preprint arXiv:1708.03288, 2017.

[53] L Meier, S. van de Geer, and P. B¨uhlmann. High-dimensional additive modeling. The Annals

of Statistics, 37:3779–3821, 2009.

55

[54] Alan Miller. Subset selection in regression. CRC Press Washington, 2002.

[55] David R Morrison, Sheldon H Jacobson, Jason J Sauppe, and Edward C Sewell. Branch-
and-bound algorithms: A survey of recent advances in searching, branching, and pruning.
Discrete Optimization, 19:79–102, 2016.

[56] Y. Nardi and A. Rinaldo. On the asymptotic properties of the group lasso estimator for

linear models. Electronic Journal of Statistics, 2:605–633, 2008.

[57] Balas Natarajan. Sparse approximate solutions to linear systems. SIAM journal on comput-

ing, 24(2):227–234, 1995.

[58] G. Obozinski, M. J. Wainwright, and M. I. Jordan. Support and union recovery in high-

dimensional multivariate regression. The Annals of Statistics, 39:1–47, 2011.

[59] John Tinsley Oden and Junuthula Narasimha Reddy. An introduction to the mathematical

theory of ﬁnite elements. Wiley, New York, 1976.

[60] Art B Owen. A robust hybrid of lasso and ridge regression. Contemporary Mathematics, 443

(7):59–72, 2007.

[61] Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in optimiza-

tion, 1(3):127–239, 2014.

[62] Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion,
Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, and Vincent Dubourg.
Scikit-learn: Machine learning in python.
the Journal of machine Learning research, 12:
2825–2830, 2011.

[63] P. Radchenko and G. M. James. Variable selection using adaptive nonlinear interaction
structures in high dimensions. Journal of the American Statistical Association, 105:1541–
1553, 2010.

[64] Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Minimax-optimal rates for sparse
additive models over kernel classes via convex programming. Journal of Machine Learning
Research, 13(Feb):389–427, 2012.

[65] P. Ravikumar, J. Laﬀerty, H. Liu, and L. Wasserman. Sparse additive models. Journal of

the Royal Statistical Society, B., 71:1009–1030, 2009.

[66] Mihailo Stojnic, Farzad Parvaresh, and Babak Hassibi. On the reconstruction of block-sparse
signals with an optimal number of measurements. IEEE Transactions on Signal Processing,
57(8):3075–3085, 2009.

[67] T. Suzuki and M. Sugiyama. Fast learning rate of multiple kernel learning: Trade-oﬀ between

sparsity and smoothness. Annals of Statistics, 41:1381–1405, 2013.

[68] Zhiqiang Tan and Cun-Hui Zhang. Doubly penalized estimation in additive regression with

high-dimensional data. The Annals of Statistics, 47(5):2567–2600, 2019.

56

[69] R Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society, Series B, 58:267–288, 1996.

[70] Andreas M Tillmann, Daniel Bienstock, Andrea Lodi, and Alexandra Schwartz. Cardinality
minimization, constraints, and regularization: A survey. arXiv preprint arXiv:2106.09606,
2021.

[71] Joel A Tropp. Algorithms for simultaneous sparse approximation. part ii: Convex relaxation.

Signal Processing, 86(3):589–602, 2006.

[72] Sara Van de Geer. Empirical Processes in M-Estimation. Cambridge University Press,

Cambridge, 2000.

[73] Ewout Van Den Berg and Michael P Friedlander. Theoretical and empirical results for
recovery from multiple measurements. IEEE Transactions on Information Theory, 56(5):
2516–2527, 2010.

[74] G. Wahba. Spline Models for Observational Data. SIAM, Philadelphia, 1990.

[75] Lan Wang, Yongdai Kim, and Runze Li. Calibrating non-convex penalized regression in

ultra-high dimension. Annals of statistics, 41(5):2505, 2013.

[76] F. Wei and J. Huang. Consistent group selection in high-dimensional linear regression.

Bernoulli, 16:1369–1384, 2010.

[77] Weijun Xie and Xinwei Deng. Scalable algorithms for the sparse ridge regression. SIAM

Journal on Optimization, 30(4):3359–3386, 2020.

[78] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables.

Journal of the Royal Statistical Society, Series B, 68:49–67, 2006.

[79] Ming Yuan and Ding-Xuan Zhou. Minimax optimal rates of estimation in high dimensional

additive models. The Annals of Statistics, 44(6):2564–2593, 2016.

[80] Cun-Hui Zhang. Nearly unbiased variable selection under minimax concave penalty. The

Annals of statistics, 38(2):894–942, 2010.

[81] Cun-Hui Zhang and Jian Huang. The sparsity and bias of the lasso selection in high-

dimensional linear regression. Annals of Statistics, 36(4):1567–1594, 2008.

[82] Yuchen Zhang, Martin J Wainwright, and Michael I Jordan. Optimal prediction for sparse
linear models? Lower bounds for coordinate-separable M-estimators. Electronic Journal of
Statistics, 11(1):752–799, 2017.

[83] Peng Zhao, Guilherme Rocha, and Bin Yu. The composite absolute penalties family for
grouped and hierarchical variable selection. The Annals of Statistics, 37(6A):3468–3497,
2009.

57

