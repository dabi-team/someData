1
2
0
2

g
u
A
6
2

]
E
S
.
s
c
[

2
v
9
3
1
1
1
.
8
0
1
2
:
v
i
X
r
a

Learning GraphQL Query Costs
Extended Version

Georgios Mavroudeas
Rensselaer Polytechnic Institute

Guillaume Baudart
Inria Paris, École normale supérieure
– PSL university

Alan Cha
IBM Research

Martin Hirzel
IBM Research

Jim A. Laredo
IBM Research

Malik Magdon-Ismail
Rensselaer Polytechnic Institute

Louis Mandel
IBM Research

Erik Wittern
IBM Research

ABSTRACT
GraphQL is a query language for APIs and a runtime for executing
those queries, fetching the requested data from existing microser-
vices, REST APIs, databases, or other sources. Its expressiveness and
its flexibility have made it an attractive candidate for API providers
in many industries, especially through the web. A major drawback
to blindly servicing a client’s query in GraphQL is that the cost
of a query can be unexpectedly large, creating computation and
resource overload for the provider, and API rate-limit overages and
infrastructure overload for the client. To mitigate these drawbacks,
it is necessary to efficiently estimate the cost of a query before exe-
cuting it. Estimating query cost is challenging, because GraphQL
queries have a nested structure, GraphQL APIs follow different
design conventions, and the underlying data sources are hidden. Es-
timates based on worst-case static query analysis have had limited
success because they tend to grossly overestimate cost. We propose
a machine-learning approach to efficiently and accurately estimate
the query cost. We also demonstrate the power of this approach by
testing it on query-response data from publicly available commer-
cial APIs. Our framework is efficient and predicts query costs with
high accuracy, consistently outperforming the static analysis by a
large margin.

1 INTRODUCTION
GraphQL is an open-source technology for building APIs to support
client-server communication [11, 20]. GraphQL has two intercon-
nected components: (i) a query language that clients use to specify
the data they want to retrieve or mutate, and (ii) a server-side run-
time to parse, validate, and execute these queries.

A central architectural design choice in GraphQL is to shift
control over what data a request can receive or mutate from API
providers to clients. In competing technologies, like the REpre-
sentational State Transfer (REST) architecture, providers define
accessible resources and their API endpoints. In GraphQL, clients
define queries that can retrieve or mutate multiple related resources
in a single request (thus avoiding unwanted round-trips), and select
only data they intend to use (thus avoiding over-fetching) [5, 6].
As a result, GraphQL is very suitable for creating diverse client
experiences and many organizations, such as Shopify, GitHub, Yelp,
Starbucks, NBC, among others, have elected to use GraphQL to
build mobile applications and engage with their ecosystem part-
ners [13].

1

Web API management is a challenging software engineering
problem, for which GraphQL provides advantages but also intro-
duces new challenges. A significant downside for providers when
shifting control to clients is the risk of overly complex queries,
which are expensive and lead to overloaded servers and/or databases.
Even small GraphQL queries can yield excessively large responses [8,
16]. Empirical work shows that on many public GraphQL APIs, a
linear increase in query size can cause an exponential increase in
result size due to the nested nature of queries [30].

Unlike in REST APIs, where providers can avoid excessive API
use by designing resources and endpoints carefully and limiting
the number of allowed requests per time interval, in GraphQL, lim-
iting the number of requests is not enough since a single query
can break the system. As such, some GraphQL server implementa-
tions track query costs dynamically during execution [26]. Once a
critical threshold is met, the server aborts execution and returns a
partial result or an error. Unfortunately, this approach can lock up
resources while producing unusable results. Hartig et al. propose
to analyze the cost of queries before executing them [16]. Their
analysis relies on probing the backend server for data-size infor-
mation, for example, determining how many users are stored in
the database if a query requests a list of users. However, this re-
quires the server to offer probing facilities, which could themselves
strain resources. In contrast, Cha et al. propose a static query cost
analysis that does not depend on dynamic information from the
server, but in consequence only provides upper bounds on cost [8].
This approach has been incorporated into IBM API Connect [19], a
commercially available API management product.

Unfortunately, these upper bounds are often loose and this gap
between estimated and actual cost makes the upper bound exces-
sively conservative as a query filter, resulting in low amortized
efficiency. More accurate cost estimates could allow providers to
loosen their cost thresholds and help them better provision server
resources. In addition, clients can better understand the costs of
their queries and how often they can execute them for given rate
limits.

Therefore, we propose a machine-learning (ML) solution that
predicts query costs based on experience generated over multiple
user-server communication sessions. Our solution extracts features
from query code by combining approaches from natural-language
processing, graph neural networks, as well as symbolic features in-
cluding ones from static compiler analysis (such as the cost estimate

 
 
 
 
 
 
schema { query: Query }
type Query {

viewer: User!
licenses: [License]!
repository(owner: String! name: String!): Repository }

type License { name: String! body: String! }
type Repository {

issues(first: Int): IssueConnection!
languages(first: Int): LanguageConnection }

type IssueConnection { nodes: [Issue] }
type LanguageConnection { nodes: [Language] }
type User { id: ID! name: String bio: String }
type Issue { id: ID! }
type Language { name: String! }

Figure 1: Simplified extract of the GitHub GraphQL schema.

query {

licenses { name }
repository(owner: "graphql", name: "graphiql") {

issues(first: 2) { nodes { id } }
languages(first: 100) { nodes {name} } } }

Figure 2: Query for the GitHub GraphQL API.

in [8]). It then builds separate regressors for each set of features
and combines the component models into a stacking ensemble.

Compared to the static approaches, our solution can underesti-
mate cost of a query but provides estimates that are closer to the
actual value.

This paper makes the following contributions:

• A set of complementary feature extractors for GraphQL query

code.

• A general ML workflow to estimate query cost that can be applied

to any given GraphQL API.

• A search space of ML model architectures for GraphQL query cost
prediction, comprising of choices for ensembling, preprocessing,
and regression operators.

• An empirical study of our approach on two commercial APIs,
comparing it to previous work and evaluating the practical ap-
plicability.

Our approach can help API providers better evaluate the risk of
client queries, and it can help clients better understand the cost of
their queries to make the best use of their budget.

This paper is an extended version of [24].

2 BACKGROUND
GraphQL queries are strongly typed by means of a schema, and are
executed via a set of data retrieval functions, called resolvers. The
schema defines both the structure of queries and the types of the
values returned. Figure 1 is a simplified extract of GitHub’s GraphQL
API written with the Schema Definition Language (SDL). The entry
point of the API is the field query which returns a value of type Query.
A value of type Query can contain the fields viewer, licenses, and
repository which return respectively a single User (the ! indicates
that the value cannot be null), a list of Licenses (the square brackets

Mavroudeas, Baudart, Cha, Hirzel, Laredo, Magdon-Ismail, Mandel, and Wittern

{ "licenses": [

{"name": "GNU Affero General Public License v3.0"},
{"name": "Apache License 2.0"},
{"name": "BSD 2-Clause \"Simplified\" License"},
{"name": "BSD 3-Clause \"New\" or \"Revised\" License"},
... ],

"repository": {
"issues": {

"nodes": [ {"id": "...NTQ="}, {"id": "...ODg="} ] },

"languages": {

"nodes": [ {"name": "HTML"}, {"name": "JavaScript"},
{"name": "Shell"}, {"name": "TypeScript"},
{"name": "CSS"} ] } } }

Figure 3: Response corresponding to the query of Figure 2.

[] are list markers), and a Repository. The parentheses after the
field repository define additional arguments, indicating that the
client must also provide the owner and the name of the requested
repository. GraphQL also includes a number of built-in scalar types
such as String, which is used in the field name in Language, and ID
used in the field id in Issue

Resolvers are functions that retrieve data for each field in an
object type. A resolver can obtain the data from any source, be it
from a database, another API, or even from a file.

On the client side, the GraphQL queries must follow the schema
defined by the service provider. Starting from the query field, a
query is a tree composed of nested fields such that the leaves of
the tree must be fields of basic types (Int, String, enumeration,
etc.). Fulfilling the query is a matter of calling the resolvers for
each field in the query and composing the returned values into a
response. Figure 2 is an example of a valid query with respect to
the schema of Figure 1. This query asks for the list of open-source
licenses available on GitHub and information about the "graphiql"
repository from the "graphql" organization. Notice that the query
is composed of only desired fields and not all fields need to be
requested (e.g. viewer is not). Figure 3 shows the response produced
by the GitHub GraphQL API after executing the query of Figure 2. A
JSON object is returned and it contains the same fields as the query.
For each field in the query with an object type (e.g., repository,
which returns a value of type Repository), the corresponding field
in the response contains an object following the structure of the
sub-query. For each field in the query with a list return type (e.g.,
licenses, which returns a list of Licenses), the corresponding field
in the response contains a list where each element is an object with
the fields requested by the sub-query.

In order to reflect the cost of executing a query and the size of
the response, Cha et al. define respectively the resolve complexity
and the type complexity [8]. These complexities are the sums of
the fields present in either the query or the response, weighted by
a configuration associated to the type and resolver of each field.
To simplify the presentation, in this paper we only focus on type
complexity.

2

Learning GraphQL Query Costs

Formally, we define the response type complexity, tcx(r, 𝑡, 𝑐), for

a response 𝑟 of type 𝑡 with a configuration 𝑐 as follows:

tcx({field1 : r1,...,fieldn : rn}, 𝑡, 𝑐) =

tcx(field1 : r1, t.field1, 𝑐)+
... + tcx(fieldn : rn, t.fieldn, 𝑐)

tcx(field : v, 𝑡, 𝑐)

tcx(field : r, 𝑡, 𝑐)

= c[t].typeWeight
when 𝑣 is a scalar

= c[t].typeWeight + tcx(r, 𝑡, 𝑐)

when 𝑟 is an object

tcx(field : [r1,...,rn], 𝑡, 𝑐) = 𝑤 + tcx(r1, 𝑡, 𝑐) +

... + 𝑤 + tcx(rn, 𝑡, 𝑐)
where 𝑤 = c[t].typeWeight

The case tcx({field1 : r1,...,fieldn : rn}, 𝑡, 𝑐) is the entry point
of the function: if the response is a JSON object composed of the
fields field1, ..., field𝑛 then the complexity is the sum of the complex-
ity of each field. The other three cases correspond to the definition
of the complexity of a field depending on the shape of the value
associated to it. tcx(field : v, 𝑡, 𝑐) corresponds to the case where
the value 𝑣 is a scalar (number, string, or boolean). The complexity
is the weight of the type 𝑡 (which is the type of the field field) in the
configuration 𝑐. tcx(field : r, 𝑡, 𝑐) corresponds to the case where 𝑟
is a JSON object. The complexity is the weight associated to the type
of the field in the configuration plus the complexity of the object 𝑟 .
Finally, tcx(field : [r1,...,rn], 𝑡, 𝑐) corresponds to the case where
the value of the field is an array of objects. The complexity is 𝑛
times the weight of the type 𝑡 plus the sum of the complexities of
every elements of the array.

If we use a configuration where the weight of a scalar type is 0,
and the weight of all other types is 1, then the type complexity
of the response in Figure 3, with 13 "licenses" and 5 "languages",
is 23 (= 13 Licenses + 1 Repository + 1 IssueConnection + 2 Issues
+ 1 LanguageConnection + 5 Languages).

Without list types, the cost of a query will always be, at worst,
linear with respect to the size of the query because each field in the
query should have a corresponding field in the response. Thus, the
issues of estimating query costs come from the lists which can be
of arbitrary length. Moreover, nested lists can yield exponentially
large responses [16]. In our example, the length of the lists issues
and languages are bounded by the argument first, as dictated by
the connection model [12]. Cha et al. [8] use this information to
statically compute an upper bound on the response size from the
query. While the upper bound they compute is as tight as possible,
the estimate (known as query type complexity) can grossly differ
from the actual cost (known as response type complexity). For exam-
ple, the static analysis assumes that the query of Figure 2 returns
at worst a list of 100 programming languages, but the GraphiQL
repository uses only 5 programming languages.

3 METHODOLOGY
The goal of this work is to automatically learn more accurate query
cost estimates from data. First, we propose a set of specialized fea-
tures that can be applied to any GraphQL API. These features turn
GraphQL queries into suitable input for classic machine learning
techniques. Second, we propose a hierarchical model to learn a

cost estimate given a GraphQL query. Separate regressors for each
features are combined into a stacking ensemble to obtain the final
estimate.

3.1 Setup
Let Q and R be the space of the possible queries and responses, SDL
the set of possible schemas1, and C the set of possible configurations.
Given a valid GraphQL schema 𝑠 ∈ SDL and a series of 𝑛 query-
response pairs (𝑞𝑖, 𝑟𝑖 ) ∈ Q × R with 𝑖 ∈ {1, . . . , 𝑛}, we would like
to learn a function estimate : SDL × C × Q → R which returns an
estimation of the type complexity of a query 𝑞 given a schema 𝑠,
and a configuration 𝑐:

estimate(𝑠, 𝑐, 𝑞) ≈ tcx(r, Query, 𝑐)

To simplify the problem, we decompose estimate(.) into two

functions ℎ : SDL × C × Q → R𝑘 and 𝑚 : R𝑘 → R:

estimate(𝑠, 𝑐, 𝑞) = 𝑚(ℎ(𝑠, 𝑐, 𝑞))

The function ℎ defines an embedding of GraphQL queries. Given a
schema a configuration and query which adhere to the schema, it
returns a numerical vector representing the query. Then 𝑚 takes a
query embedding and returns an estimate of the complexity.

3.2 Feature Extraction
We design three distinct feature extraction methods.

Field Features. A GraphQL schema defines a finite number of
types and a finite number of fields. We can thus represent all possi-
ble fields by a vector of fixed size where each index represents a
particular field. We create a feature vector for each GraphQL query,
enumerating the total number of times a specific field appears in-
side a query. This corresponds to a Bag-of-Words representation of
the query [15]. We call this feature extraction function ℎ𝑓 .

Graph Embeddings. The field features only capture information
about the cardinality of fields. To capture information about the
syntactic structure of the query, we use a second set of features
based on graph embeddings. The idea is that a graph neural network
can map the abstract syntax tree of a GraphQL query into a low-
dimensional embedding space, from which we can then extract the
numerical features. To do that, we employed the graph2vec [25]
technique, one of the most popular approaches in this area. The
idea of graph2vec is to define a function that takes as input a graph
and returns a vector of real numbers such that similar graphs are
mapped to similar vectors. To define this function, a neural network
is trained. The principle of the training is the same as the one of
doc2vec where documents are graphs and words are sub-graphs.
The objective of the training is to put next to each other in the
embedding space the sub-graphs that appear in the same context
and propagate this information at the graph level. The training does
not need labeled data, it just needs a set of graphs. Once trained, to
compute the embedding, a graph is decomposed into a set of sub-
graphs and fed to the neural network. The value of a low dimension
hidden layer of the network is used as the vector characterizing the
graph. We call this feature extraction function ℎ𝑔.

1SDL: Schema Definition Language

3

Mavroudeas, Baudart, Cha, Hirzel, Laredo, Magdon-Ismail, Mandel, and Wittern

Operator

Categorical Continuous

free

total

free

total

(polynomial features)

(standard scaler)

SS
No-Op (identity transform)
Poly
NYST (Nystroem)
LR
TR
Rid
RFR
KNB (𝑘 nearest neigbors)
GBR (gradient boosting)

(linear regression)
(decision tree)
(Ridge regression)
(random forest)

0
0
3
3
0
4
3
5
4
8

2
0
3
3
2
4
5
5
5
8

0
0
3
2
0
2
2
2
0
2

0
0
3
2
0
2
2
2
0
2

Figure 4: Lale pipeline: round nodes represent ML operators
and rectangular nodes represent a choice between different
operators.

Summary Features. The last set of features is a six-dimensional
encoding of the queries using symbolic code analysis techniques.
They include (i) the static analysis upper bound of Cha et al. [8].
They also include features that summarize the structure of the
abstract syntax tree of the queries. These are (ii) query size, the
number of nodes in the abstract syntax tree, (iii) width, the max-
imum number of children a tree node has, and (iv) nesting, the
maximum depth of the tree. Finally, we extract two features related
to lists: (v) lists, the number of fields in a query requesting a list, and
(vi) the sum of list limits (e.g, first). The features vector of Figure 2
is [118, 17, 2, 3, 3, 115] (the list licenses has default length 13). We
call the function that extracts all of these summary features ℎ𝑠 .

3.3 Learning
There are many well-known operators that implement regression
algorithms (e.g., linear regression, gradient boosting regressors) and
also feature preprocessing (e.g., polynomial features transformer). A
library like scikit-learn [7] implements many of these operators, but
picking the right operators and configuring their hyperparameters
is a tedious task and depends on the dataset. We thus use Lale [4], a
state-of-the-art automated machine learning (auto-ML) tool, to au-
tomatically select the best operators and tune the hyperparameters
given a query/response dataset.

Definition of three models. In Section 3.2, we defined three set
of features: field features, graph embeddings features, and summary
features, For each set of features, we train a model independently,
but all three of these models have the same architecture presented in
Figure 4. We define a pipeline where (i) the first step uses a standard
scaler (SS) to give all features a mean of 0 and a standard deviation
of 1; (ii) the second step does other feature transformation; and
(iii) the last step does the prediction.

For the feature transformation and prediction steps, we config-
ure the auto-ML tool such that it chooses the best solution among
multiple algorithms. There are three possible feature transforma-
tions: No-Op leave the features unchanged, Poly is a polynomial

Table 1: Number of hyperparameters to optimize and total
for each operator in the pipeline of Figure 4.

expansion of the features, and NYST is Nystroem transformer. For
the last part, we selected six different predictors: linear regres-
sion (LR), decision tree (TR), Ridge regression (Rid), random for-
est (RFR), 𝑘-nearest neighbors where the number of neighbors is
fixed to 3 (KNB), and gradient boosting regressors (GBR).

Model selection. This pipeline defines a space of 18 possible com-
binations for each of the three models. Furthermore, each of the
operators of the pipeline also has a set of hyperparameters to con-
figure. We have fixed some of the hyperparameters, such as 𝑘 = 3
for the 𝑘-nearest neighbors, but we left 43 hyperparameters free.
The auto-ML tool then chooses the best solution among the possible
combinations of algorithms and hyperparameters configurations.
To select the best model, we use 𝑛-fold cross validation and the
Bayesian optimizer from Hyperopt [22].

Combination of models. We train the three models independently
and define a new hierarchical model using the outputs of the three
models as input for a final model. This final model provides the
final estimation in the prediction phase. In general, using a stacked
ensemble in an ML framework [31], learning each predictor sepa-
rately and using the predictions as features for the final predictor,
can improve accuracy. After experimentation, we found that in our
case, this method performs better than concatenating the individual
features together into a wide vector and using a single regressor.
Figure 5 describes our final architecture. More formally, given
a training set of query-response pairs 𝑋 = {(𝑞1, 𝑟1), . . . , (𝑞𝑛, 𝑟𝑛)},
we train three distinct models 𝑚𝑓 , 𝑚𝑔, 𝑚𝑠 , each taking as inputs the
corresponding features sets we constructed from the queries earlier.
Each model returns a cost estimation ˆ𝐶𝑓 , ˆ𝐶𝑔, or ˆ𝐶𝑠 , which will be an
𝑚-dimensional vector (the cost estimation for each of the queries in
the training set). In turn we use these three features to train a new
model 𝑚final, which returns the final cost prediction ˆ𝐶final. After
the learning phase, the cost prediction on a new query 𝑞 can be
done in a similar way. First, we extract the features 𝑥 𝑓 , 𝑥𝑔, 𝑥𝑠 , from
the query, then we get the cost estimates ˆ𝑐 𝑓 , ˆ𝑐𝑔, ˆ𝑐𝑠 from 𝑚𝑓 , 𝑚𝑔, 𝑚𝑠
respectively, and finally we insert these estimates into our last
model 𝑚final to get the final cost estimate ˆ𝑐final.

4

ChoiceChoiceSSNo-OpLRPolyNYSTTRRidRFRKNBGBRLearning GraphQL Query Costs

ℎ𝑓 (·)

𝑞

ℎ𝑔 (·)

𝑥 𝑓

𝑥𝑔

𝑚𝑓 (·)

𝑚𝑔 (·)

𝑚𝑠 (·)

ˆ𝑐 𝑓

ˆ𝑐𝑔

ℎ𝑠 (·)

𝑥𝑠

ˆ𝑐𝑠

[ ˆ𝑐 𝑓 ˆ𝑐𝑔 ˆ𝑐𝑠 ]

𝑚final (·)

ˆ𝑐final

Figure 5: Cost estimation process for an input query 𝑞𝑖 . First,
we extract the numerical features 𝑥 𝑓 , 𝑥𝑔, 𝑥𝑠 , from the corre-
sponding functions ℎ𝑓 , ℎ𝑔, ℎ𝑠 . The features are inserted into
the independent cost estimation models 𝑚𝑓 , 𝑚𝑔, 𝑚𝑠 , which
in turn produce the first cost estimates for each of the fea-
tures. These costs are concatenated into a new feature vector
[ ˆ𝑐 𝑓 , ˆ𝑐𝑔, ˆ𝑐𝑠 ], the input for the final model 𝑚final which gener-
ates the final cost estimate ˆ𝑐final.

4 RESULTS
In our evaluation, we first study how our new ML query cost esti-
mation compares to both the static analysis in [8] and the actual
cost of the response. We then evaluate the importance of the dif-
ferent features that we have extracted to obtain a good estimate
of the cost. Finally, we study the practical usage of our approach.
For that, we look into how an API management layer could filter
queries using the ML estimates and simulate how our estimator
would react to malicious queries.

This can be summarized with the following research questions:

RQ1: Does our approach return accurate estimates?
RQ2: Are all the features useful for the estimation?
RQ3: What are the practical benefits of the new estimation?
RQ4: Are the ML cost estimates robust to malicious queries?

4.1 Experimental Setup

Data. Following the methodology in [8], we collected over 100,000
responses from the GitHub GraphQL API and 30,000 from the Yelp
GraphQL API. The queries are synthetically generated but the re-
sponses come from real industry APIs. The dataset is available
at https://github.com/Alan-Cha/graphql-complexity-paper-artifact.
Table 2 presents the dataset characteristics, where query size, width,
nesting, and lists are the corresponding summary features from
Section 3.2, and response is the actual cost of the query result.

5

Table 2: Data statistics for the GitHub and Yelp datasets.

GitHub

Yelp

mean

std min max mean

std min max

Query Size
Width
Nesting
Lists
Response

109
23
3
47
79

43
8
1.4
23
67

7
2
1
0
0

1,425
53
9
503
2,548

66
11
3
74
1,301

30
3
0.5
53
2,111

5
1
1
0
0

229
21
3
370
7,363

(a) GitHub static analysis.

(b) GitHub ML estimation.

(c) Yelp static analysis.

(d) Yelp ML estimation.

Figure 6: Comparison of complexity estimations between
static analysis (left) and ML approach (right) on the GitHub
(top) and Yelp (bottom) datasets. The red line represent the
actual complexity of the responses and the blue dots show
the estimates. Opacity indicates density.

Training. The final model is selected using 5-fold cross valida-
tion [21] and the Hyperopt-sklearn optimizer [22]. We let our op-
timizer run for sixty hours for each trained pipeline for both the
Yelp and GitHub datasets, exploring a total of 1, 500 combinations
of models and hyperparameters, whichever of the two finishes first.
Once operators and hyperparameters are chosen, training a given
model is relatively fast.

Results. It is important to emphasize that finding the optimal
ML pipeline was not our goal. The main goal was to provide a
framework that could be easily adapted for other APIs and datasets.
However, we need to underline the fact that, in general, the pre-
ferred estimator chosen by the Hyperopt optimizer in most of the
training pipelines was the gradient boosting regressor for both
GitHub and Yelp datasets. The specific pipelines for both datasets
are presented in Table 3.

4.2 RQ1: Accuracy
We compare our approach to the static analysis proposed in [8],
which was shown to outperform the three most popular libraries for

0200400Complexity0250500EstimatedComplexity1001011020200400Complexity0250500EstimatedComplexity100101102025005000Complexity50000100000EstimatedComplexity100101102103025005000Complexity200040006000EstimatedComplexity100101102103Mavroudeas, Baudart, Cha, Hirzel, Laredo, Magdon-Ismail, Mandel, and Wittern

𝑚𝑓

𝑚𝑔

𝑚𝑠

𝑚final

GitHub
Yelp

SS → No-Op → GBR SS → Poly → GBR SS → Poly → GBR SS → Poly → GBR
SS → No-Op → GBR SS → Poly → GBR
SS → No-Op → GBR SS → Poly → LR

Table 3: Pipeline configurations for the ML models.

Static

ML

MAE

std

MAE

std

GitHub
Yelp

31.5
14,180.5

263.8
30,827.9

8.2
60.7

35.5
180.4

Table 4: MAE Comparison between the ML approach and the
static analysis.

computing GraphQL query cost. Figure 6 compares the estimations
of the static analysis with those from our ML approach. We observe
that, while the static analysis guarantees an upper bound, the price
in terms of over-estimation can be significant, especially with larger
query sizes. On the other hand, for both datasets, the ML estimates
stay remarkably close to the actual response complexity even for
the largest queries.

Note that, in order to create the visualizations for the static
analysis estimates versus the ML ones, we only show the 99.8%
of the estimates for GitHub data and 99.5% of Yelp data. To get
these percentages we first sort the static analysis estimates from
the lowest to the highest and get the size corresponding to the
percentage mentioned above. The reason for this is that some huge
outliers dominated the graphs, impeding the reader from extracting
any meaningful conclusions from them.

These plots also illustrate the difference between the two APIs.
The random query generator is able to smoothly explore the com-
plexity space of the GitHub API. For the Yelp API, however, queries
form dense clusters that are distant from each other, resulting in
the static analysis’ estimates precision degrading significantly as
the query complexity increases.

To quantify the precision of the ML approach compared to the
static analysis, we computed the Mean Absolute Error (MAE) be-
tween the estimated cost and the actual cost of the responses on
the two datasets. Results are summarized in Table 4.

MAE = 1/𝑛

𝑛
∑︁

𝑖=1

|𝑐𝑖 − ˆ𝑐𝑖 |

For both datasets, the accuracy gain of the ML approach compared
to the static analysis is striking both in terms of average value, and
standard deviation. This further validates the observation that ML
approach is accurate for large queries, which are challenging for
the static analysis. To further underline our claim, we present in
Figure 7 the raw error distribution percentages of the ML and static
analysis. Given a query with response cost 𝑐 and a prediction ˆ𝑐, we
define the error percentage as 𝐸𝑟𝑟𝑜𝑟 % = ˆ𝑐−𝑐
𝑐 .

6

(a) GitHub error distributions.

(b) Yelp error distributions.

Figure 7: Error percentage distribution for the ML and static
predictions. For visual purposes we have removed outliers
with error% reaching up to 120, 000% in static analysis.

Table 5: Accuracy comparison for each feature

GitHub

Yelp

Summary features
Field features
Embedding features
Final combination

MAE

8.7
14.9
31.58
8.2

std

36.4
40.2
45.7
35.5

MAE

102.4
320.8
880.9
60.7

std

280.8
715.6
813.4
180.4

Static analysis

31.5

263.8

14,180.5

30,827.9

4.3 RQ2: Features Selection
As described in Sections 3.2 and 3.3, the ML estimates are based on
three groups of features, namely summary features (including the
result of the static analysis), field features, and graph embedding
features. But are all these features necessary? To answer this ques-
tion, we looked at estimates obtained using each group of features
separately. Table 5 summarizes the results. We observe that for
both datasets, none of the feature alone is competitive with the
stacked ensemble presented in Figure 5, that combines the results
of cost estimation models trained from all three groups of features
separately.

The performance of each group of features depends on the
dataset. For instance, while the summary features give reasonable
estimates for both datasets, the field features are much more useful
for GitHub than for Yelp. This could be related to the underlying
structure of the two datasets as well as to the data generation pro-
cess. Table 5 also shows that the automatic feature extraction of
the neural networks used to build the graph embedding features
fails to produce accurate estimates for either dataset, underscoring
the importance of the more descriptive features.

To delve a bit deeper into the feature analysis, we performed an
independent univariate test to capture the relationship between the
features and the target (here the query cost). We want to understand

02505007501000Error%0250050007500StaticML050010001500Error%010002000StaticMLLearning GraphQL Query Costs

Yelp

GitHub
F𝑠 : Lists
F𝑠 : ResolveComplexity
F𝑓 : MarketPlaceCategories F𝑠 : TypeComplexity
F𝑓 : Licenses
F𝑓 : Permissions
F𝑠 : Nesting
F𝑓 : Label
F𝑓 : Description
F𝑠 : TypeComplexity
F𝑓 : Sum of Variables
F𝑠 : Nodes

F𝑓 : Code
F𝑠 : Lists
F𝑠 : Nodes
F𝑓 : Parent Categories
F𝑓 : Country Whitelist
F𝑓 : Country Blacklist
F𝑠 : Nesting
F𝑠 : Sum of Variables

Table 6: Top-10 features with the largest dependency to the
query costs based on the mutual information criterion for
GitHub and Yelp. In bold text we see the common summary
features in top positions for both datasets.

how each feature, if used separately, impact the target values. For
this experiment to be meaningful, we used the summary features
(F𝑠 ) and the field features (F𝑓 ), which are interpretable, as they are
related to known quantities. Given a feature 𝑋𝑖 and its correspond-
ing data 𝑥𝑖1, . . . , 𝑥𝑖𝑁 and the target query costs 𝑌 = {𝑦1, . . . , 𝑦𝑁 },
we calculate the mutual information between 𝑋𝑖 ∈ F𝑠 ∪ F𝑓 , and 𝑌
using the scikit-learn’s implementation of the mutual information
metric.2 This metric tries to capture the dependence between two
random variables. We report the top-10 features with the biggest
dependencies (with respect to this metric) to the target in Table 6.
It is noticeable that a subset of the summary features, which are
common in both datasets, can be found in the top positions for both
datasets. This could provide some further insight on why the model
𝑚𝑠 constructed by these features performs better in comparison to
the models 𝑚𝑓 , and 𝑚𝑔 as we see on table 5.

4.4 RQ3: Practicality
Now that we have access to accurate complexity estimates, the main
question is: how useful are these estimates in practice? API managers
offer, through a client-selected plan, a rate limit, allowing a certain
number of points per time window. Points could be attributed to
individual REST calls or to the query cost in the case of GraphQL.
Several API management vendors implement these strategies [14,
19, 28]. To mimic this behavior, we built a simulator that acts as
an API manager whose goal is to filter queries based on the client
plan. We select a threshold of points to represent the rate limit on
a given time window.

First, the client sets a threshold, that is, the maximal aggregate
cost that the client is willing to pay for a query. Then the simulator
acts as a gateway between the API and the client, rejecting queries
for which the estimated cost is above the threshold. To evaluate
the benefit of our approach, we compare the acceptance rate of a
simulator relying on the static analysis against the acceptance rate
of a simulator relying on our ML approach.

2https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.
mutual_info_regression.html

7

(a) GitHub

(b) Yelp

Figure 8: Comparison of the acceptance rate against increas-
ing threshold for the static analysis (red) and the ML ap-
proach (blue). The acceptance rate is computed as the num-
ber of queries whose sum of estimated complexity is be-
low the threshold level divided by the number of sampled
queries.

Figure 8 shows the evolution of the acceptance rate for increasing
values of the simulation threshold. We used a different range of
thresholds for the experiments in Yelp and GitHub respectively due
to their specific characteristics (in general Yelp contains queries
with larger costs). The results are averaged over 1,000 simulations,
and for each simulation we randomly select 1,000 queries. Overall,
as expected, the ML cost estimation policy is able to accept a bigger
proportion of queries for both APIs. The staircase shape of the Yelp
results can be explained by the peculiar cluster-like distribution
of query complexity in the dataset (see Figure 6). For the same
reason, the static analysis plateaus at 80% until the threshold is
considerably larger due to the substantial overestimation within
this range. When the threshold reaches a high enough value, the
static analysis reaches 100% acceptance rate too.

Figure 9 shows the results of the simulation for a series of thresh-
old values. The low threshold is such that the complexity of 25% of
the queries in the dataset is under the threshold (25th percentile).
The high threshold is five times the 75th percentile. While it is still
possible that a few queries go over budget due to the approximate
nature of the ML estimates, the average cumulative cost (red) al-
ways stays below the maximum budget (black). This suggests that
our approach can be used to implement rate limiting policies.

For the smallest threshold value, the simulator can only accept
very few queries for which the difference with the maximum possi-
ble cost can be significant. The cumulative cost curve thus remains
clearly under the budget line and there is no noticeable difference
between the static analysis and the ML estimates. For the largest
threshold values in general, the ML estimates curve is much closer
to the budget limit than the static analysis curve which indicates
a better utilization of the available budget. For this experiment,
the benefit of the ML predictor compared to the static predictor is
limited for GitHub (Figures 9(a) and 9(b)) but more significant for
Yelp (Figures 9(c) and 9(d)).

Finally we conclude this experiment by exploring what is the
average budget violation per threshold and when there is a vio-
lation what is the average percentage the ML predictor exceeds
the budget by. Figure 10 showcases that in general there is a very
small percentage of violations which quickly reaches zero as the
threshold-budget increases. For GitHub, we observe that at most 6%

102103Threshold0.250.500.751.00AcceptanceRateMLStatic102103104105Threshold0.00.51.0AcceptanceRateMLStatic(a) GitHub, Threshold: 44

(b) GitHub, Threshold: 474

(c) Yelp, Threshold: 15

(d) Yelp, Threshold: 6710

Figure 9: Cumulative cost incurred by the the static analysis
and ML approach on accepted queries. The budget curve is
the cumulative sum of the budget allocated per query. The
budget of each query is considered constant and given by the
value of the threshold. The static curve and the ML curve
show the cumulative sum of the response complexity of the
queries that were accepted because their expected complex-
ity computed respectively by the static analysis and ML ap-
proach was below the threshold.

(a) GitHub, Threshold Violations

(b) GitHub, Violations Above Threshold

(c) Yelp, Threshold Violations

(d) Yelp, Violations Above Threshold

Figure 10: Average percentage of violations per threshold
and average amount of violation percentage per threshold
from the ML estimates. Figures (a) and (c) show how many
accepted queries violate a given budget on average. Figures
(c) and (d) show how much accepted queries violate the bud-
get on average.

Mavroudeas, Baudart, Cha, Hirzel, Laredo, Magdon-Ismail, Mandel, and Wittern

of the queries violate the budget and they are at most 80% above it,
see figures 10(a), and 10(b). For Yelp, we observe a 15% of violations
on small thresholds with a maximum violation of a little above 100%
above the threshold as figures 10(c), and 10(d) indicate.

4.5 RQ4: Robustness
The ML paradigm provides accurate estimates with high proba-
bility, but it opens the door to the undesirable possibility of cost
underestimation. Due to possible underestimation, using the ML
prediction to filter queries can result in accepting a query whose
actual cost is above the budget threshold. This is not possible with
the static analysis as it calculates and upper bound cost. The ML’s
under- and overestimation of costs are generally small and statisti-
cally fluctuate around the actual cost, averaging out in the long run
as shown in Section 4.4. However, it paves the way to adversarial
malicious queries. The goal in this section is to evaluate the extent
to which the ML query cost estimate is robust to malicious requests.
Unfortunately, there is no single definition of a malicious request
in the context of database management and web communication.
Intuitively, a malicious query is disguised, presenting as a query
of small or minimum size, yet producing a result that is huge or
of maximum size. Small queries can produce exponentially sized
output. For instance, Cha et al. [8] present an example of a real-
world pathological query in which they recursively request the
same first 100 issues from the Node repository using the GitHub
GraphQL API. To study the robustness of the ML paradigm against
malicious queries, we set up a simple experiment that relies on two
basic properties of malicious queries,

• The query is disguised, presenting as a small input but nev-
ertheless realizing the upper bound cost computed by the
static analysis.

• The actual cost and matching upper bound cost are huge.
It is the second property that makes the query malicious and capable
of breaking the system. This is the regime we would like to be robust
to.

To simulate such a malicious query, we pick a small generic
query-example from the dataset as the “disguised” query. To craft
a malicious query, we then assume that its upper bound cost is
huge, keeping all other features constant, and we also assume that
the actual cost matches the upper bound. Hence, for an original
query-example with a feature vector (𝑥1, . . . , 𝑥𝑁 , static bound), we
create a feature vector (𝑥1, . . . , 𝑥𝑁 , huge static bound) which cor-
responds to a simulated malicious query. Note that the malicious
query corresponding to the simulated feature vector may not be
realizable.

We then compute the ML predicted cost be for the simulated
malicious query. If the prediction is a small cost, then the query
could be accepted and break the system. For the ML to be robust,
its prediction on the simulated malicious query should be large,
ideally increasing rapidly as the static bound increases.

We have performed this robustness analysis on random queries
selected from the GitHub and Yelp datasets. The qualitative depen-
dence of cost on upper bound cost is consistent for all cases we
tried, and we present a representative result in Figure 11. Notice that
as the upper bound cost increases, the ML estimate first remains
about the same, because it focuses on the other features of the input.

8

02505007501000AcceptedQueries102104CumulativeCostMLStaticBudget02505007501000AcceptedQueries103105CumulativeCostMLStaticBudget02505007501000AcceptedQueries101103CumulativeCostMLStaticBudget02505007501000AcceptedQueries104106CumulativeCostMLStaticBudget102103104105Threshold024MeanViolation%102103104105Threshold020406080AboveThreshold%102104Threshold051015MeanViolation%102104Threshold050100AboveThreshold%Learning GraphQL Query Costs

(a) GitHub

(b) Yelp

Figure 11: ML estimates as static analysis estimates increase.

However, as the simulated query enters the malicious regime with
a huge upper bound cost, the ML predicted cost rapidly increases.
That is, for malicious queries which otherwise look normal but
have huge actual cost and upper bound cost, the ML prediction is
also huge and will thus prevent the system from executing such
queries. How the ML estimate increases with the upper bound cost
depends on the specific machine learning model used. For example
the plateaus at Figure 11(a) and Figure 11(b) come from the choice
of gradient boosted trees (GBR) which ignore further increases in
the upper bound cost. The estimate reaches a final plateau because
this algorithm is not good at extrapolating values outside of ranges
of the training set. The machine learning model can be tailored
to the provider’s risk preferences. For instance, we could create
a model where the output could be set to increase with the static
analysis upper bound.

The ML framework is flexible. A more conservative provider
has several options for further increasing the robustness of the ML
estimated cost. The first is to manually increase the contribution of
the upper bound cost. In the limit, the provider even has the option
to use only the upper bound cost if no underestimation in cost can
be tolerated. Alternatively, the ML model can be trained with both
typical and malicious queries. This means inclusion of a wider
collection of query examples in the data, including instances of ma-
licious queries. While several enhancements to robustness against
malicious queries are possible, our goal here was to demonstrate
that such robustness is already inherent to the ML paradigm.

5 THREATS TO VALIDITY AND DISCUSSION

Internal validity. The first threat comes from the realism of the
test datasets. As far as we know, at the time of writing, there are
no publicly available datasets of real-world production workloads
for GraphQL queries and responses that we could use to test our
framework directly. The generated dataset available at [9] was
small in size (10,000 query-response pairs) and not sufficient for
ML purposes, instead we used the tooling provided to generate a
much larger dataset (100,000 query-response pairs) for our experi-
ments. These queries were issued to GitHub and Yelp, which are
two publicly available commercial APIs so they correspond with
real-world data. While we were able to produce good estimates for
these datasets, it is important to keep in mind that randomly gener-
ated queries may not be representative of queries that human users
may realistically design and run. Moreover, the paper proposes a
framework that should be trained by the service provider specifi-
cally for its system. The service provider (or gateway provider) can

9

use automatically generated queries to bootstrap the system, but it
can use also actual traffic for the training.

The second threat is due to the number of datasets used in the
study. We used two services, GitHub and Yelp, for the evaluation and
we observe considerable differences in the data. These differences
have an impact on the quality of the estimate, but it still provides
an improvement compared to the state-of-the-art. This suggests
that each API provider will need to tune the learners to address the
specific characteristics of their system.

External validity. An external threat to validity is the viability
and persistence of the predictions over time. In general, the dy-
namics of actively managed data sources often change and evolve
through time. They develop different morphology and structure and
the data itself may also change. This fact suggests that someone
cannot expect persistently accurate predictions from a learning
algorithm, meaning that the training of the learner needs to be
updated with the new dynamics, if not continuously within a rea-
sonable time frame.

Lastly, we need to consider what happens when our learners
encounter malicious queries. In Section 4.5, we tried to recreate
this scenario by simulating pathological queries whose actual cost
matches their upper bound cost. We observe some robustness
against malicious queries and this can be attributed to the fact
that we use the static analysis as a feature in our algorithm. More-
over, the more malicious queries the system receives, the better it
becomes at recognizing them if they are used for retraining. We
also showed in Section 4.4 that the underestimation of some query
costs was compensated by the overestimation of some others.

6 RELATED WORK
Section 1 discusses previous work related to GraphQL cost estima-
tion such as a query analysis that probes the service backend [16]
and an analysis that computes an upper bound [8]. This section
discusses the broader context of ML techniques for cost estimation.
Our work is an instance of machine learning for code (ML for
code). ML for code has been extensively studied in the software
engineering community [2, 18, 27], including for optimizing com-
putational performance [29]. There are several works that use code
as input, usually in the form of a token sequence, and then train ML
models for a variety of tasks (for example, code completion). To the
best of our knowledge, our work is the first to apply ML to GraphQL.
Cummins et al. first train a language model on OpenCL source code,
then train a deep learning classifier to pick performance optimiza-
tions [10]. We focus on GraphQL instead of OpenCL and predict
performance instead of picking an optimization. Our work obtains
some of its features for code from a graph neural network (GNN).
Using GNNs for code was pioneered by Allamanis et al. [3], who
represented C# code as a multi-graph with edges for syntax, data
flow, and control flow. They used their model for variable name
prediction tasks. In contrast, we focus on GraphQL and predict
query cost.

The database community has also studied query performance pre-
diction (QPP). Akdere et al. use support vector machines and other
ML techniques to predict the performance of relational database
queries [1]. Besides hand-crafted features such as operator occur-
rence counts and query optimizer estimates, they propose stacking

102103104105StaticEstimate100200MLEstimate104106108StaticEstimate020004000MLEstimatemodels for individual relational operators to obtain a model for a
composite query. Marcus and Papaemmanouil use similar features
and a similar stacking idea, but with deep learning, thus enabling
end-to-end learning with back-propagation into earlier stacked
layers [23]. In contrast to both of these works, our work targets
GraphQL, which is more join-heavy, and does not require query
optimizer estimates. Hasan and Gandon use support vector ma-
chines and 𝑘-nearest neighbors for QPP for SPARQL [17]. They use
hand-crafted features including operator occurrence counts, tree
depth and size, and graph edit distance to similar queries. While
SPARQL has some similarities to GraphQL, they differ substantially.
Furthermore, our work also uses a sound conservative upper bound
on query cost as well as graph neural network features.

7 CONCLUSION
This paper proposes a methodology for using ML to estimate the
cost of GraphQL queries. We experimentally show that our ML
approach outperform the leading existing static analysis approach
using two commercial GraphQL APIs, namely GitHub and Yelp. We
believe that an ML approach to query complexity estimation can
be useful for both API providers and clients. API providers benefit
by allowing them to loosen cost thresholds and better provision
server resources, while clients benefit by allowing them to better
understand the costs of their queries and what is allowable within
their rate limits. In addition, our approach can be used in conjunc-
tion with other types of analyses to create an overall more robust
API management system.

REFERENCES
[1] Mert Akdere, Ugur Cetintemel, Matteo Riondato, Eli Upfal, and Stanley B. Zdonik.
2012. Learning-based Query Performance Modeling and Prediction. In Interna-
tional Conference on Data Engineering (ICDE). 390–401. https://doi.org/10.1109/
ICDE.2012.64

[2] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018.
A Survey of Machine Learning for Big Code and Naturalness. ACM Computing
Surveys (CSUR) 51, 4 (2018), 81:1–81:37. https://doi.org/10.1145/3212695
[3] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learning
to Represent Programs with Graphs. In International Conference on Learning
Representations (ICLR). https://openreview.net/forum?id=BJOFETxR-&noteId=
BJOFETxR-

[4] Guillaume Baudart, Martin Hirzel, Kiran Kate, Parikshit Ram, and Avraham
Shinnar. 2020. Lale: Consistent Automated Machine Learning. In KDD Workshop
on Automation in Machine Learning (AutoML@KDD). https://arxiv.org/abs/2007.
01977

[5] Gleison Brito, Thaís Mombach, and Marco Tulio Valente. 2019. Migrating to
GraphQL: A Practical Assessment. CoRR abs/1906.07535 (2019). arXiv:1906.07535
http://arxiv.org/abs/1906.07535

[6] Gleison Brito and Marco Tulio Valente. 2020. REST vs GraphQL: A Controlled
Experiment. In 2020 IEEE International Conference on Software Architecture (ICSA).
IEEE, 81–91.

[7] Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas
Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort,
Jaques Grobler, Robert Layton, Jake VanderPlas, Arnaud Joly, Brian Holt, and
Gaël Varoquaux. 2013. API design for machine learning software: experiences
from the scikit-learn project. CoRR abs/1309.0238 (2013). arXiv:1309.0238
http://arxiv.org/abs/1309.0238

Mavroudeas, Baudart, Cha, Hirzel, Laredo, Magdon-Ismail, Mandel, and Wittern

[8] Alan Cha, Erik Wittern, Guillaume Baudart, James C Davis, Louis Mandel, and
Jim A Laredo. 2020. A principled approach to GraphQL query cost analysis.
In Foundations of Software Engineering (FSE). 257–268. https://doi.org/10.1145/
3368089.3409670

[9] Alan Cha, Erik Wittern, Guillaume Baudart, James C Davis, Louis Mandel, and
Jim A Laredo. 2020. A Principled Approach to GraphQL Query Cost Analysis
Research Paper Artifact. https://doi.org/10.5281/zenodo.4023299

[10] Chris Cummins, Pavlos Petoumenos, Zheng Wang, and Hugh Leather. 2017.
End-to-End Deep Learning of Optimization Heuristics. In Conference on Parallel
Architectures and Compilation Techniques (PACT). 219–232. https://doi.org/10.
1109/PACT.2017.24

[11] Linux Foundation. 2019. GraphQL Foundation. https://foundation.graphql.org/
news/2019/03/12/the-graphql-foundation-announces-collaboration-with-the-
joint-development-foundation-to-drive-open-source-and-open-standards/.
Accessed: 2021-02-24.

[12] The GraphQL Foundation. 2021. Pagination – Complete Connection Model.

https://graphql.org/learn/pagination Accessed: 2021-02-24.

[13] The GraphQL Foundation. 2021. Who’s Using GraphQL? https://graphql.org/

users Accessed: 2021-02-24.

[14] Google. 2020. Apigee. https://cloud.google.com/apigee Accessed: 2021-02-24.
[15] Zellig Harris. 1954. Distributional structure. Word 10, 2-3 (1954), 146–162.

https://doi.org/10.1007/978-94-009-8467-7_1

[16] Olaf Hartig and Jorge Pérez. 2018. Semantics and complexity of GraphQL. In
International World Wide Web Conferences (WWW). 1155–1164. https://doi.org/
10.1145/3178876.3186014

[17] Rakebul Hasan and Fabien Gandon. 2014. A Machine Learning Approach to
SPARQL Query Performance Prediction. In Joint Conferences on Web Intelligence
and Intelligent Agent Technologies (WI-IAT). 266–273. https://doi.org/10.1109/WI-
IAT.2014.43

[18] Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar T. De-
vanbu. 2012. On the Naturalness of Software. In International Conference on
Software Engineering (ICSE). 837–847. https://doi.org/10.1109/ICSE.2012.6227135
[19] IBM. 2020. API Connect. https://www.ibm.com/cloud/api-connect Accessed:

2021-02-24.

[20] Facebook Inc. 2015. GraphQL. http://spec.graphql.org/July2015/. Accessed:

2021-02-24.

[21] Ron Kohavi. 1995. A Study of Cross-Validation and Bootstrap for Accuracy
Estimation and Model Selection. In IJCAI. Morgan Kaufmann, 1137–1145.
[22] Brent Komer, James Bergstra, and Chris Eliasmith. 2019. Hyperopt-Sklearn. In
Automated Machine Learning - Methods, Systems, Challenges, Frank Hutter, Lars
Kotthoff, and Joaquin Vanschoren (Eds.). Springer, 97–111. https://doi.org/10.
1007/978-3-030-05318-5_5

[23] Ryan Marcus and Olga Papaemmanouil. 2019. Plan-Structured Deep Neural
Network Models for Query Performance Prediction. In Conference on Very Large
Data Bases (VLDB). 1733–1746. http://www.vldb.org/pvldb/vol12/p1733-marcus.
pdf

[24] Georgios Mavroudeas, Guillaume Baudart, Alan Cha, Martin Hirzel, Jim A. Laredo,
Malik Magdon-Ismail, Louis Mandel, and Erik Wittern. 2021. Learning GraphQL
Query Cost. In International Conference on Automated Software Engineering.
[25] Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui
Chen, Yang Liu, and Shantanu Jaiswal. 2017. graph2vec: Learning distributed
representations of graphs. arXiv preprint arXiv:1707.05005 (2017).

[26] Prisma. 2019.

"Security and GraphQL".

https://www.howtographql.com/

advanced/4-security/. Accessed: 2021-02-24.

[27] Musfiqur Rahman, Dharani Palani, and Peter C. Rigby. 2019. Natural Software
Revisited. In International Conference on Software Engineering (ICSE). 37–48.
https://doi.org/10.1109/ICSE.2019.00022

[28] RedHat. 2020. 3Scale. https://www.3scale.net Accessed: 2021-02-24.
[29] Zheng Wang and Michael O’Boyle. 2018. Machine Learning in Compiler Opti-
mization. Proc. IEEE 106, 11 (2018), 1879–1901. https://doi.org/10.1109/JPROC.
2018.2817118

[30] Erik Wittern, Alan Cha, James C Davis, Guillaume Baudart, and Louis Mandel.
2019. An empirical study of GraphQL schemas. In International Conference on
Service Oriented Computing (ICSOC). 3–19. https://doi.org/10.1007/978-3-030-
33702-5_1

[31] David H Wolpert. 1992. Stacked generalization. Neural networks 5, 2 (1992),

241–259.

10

