Graph-based Heuristic Search
for Module Selection Procedure
in Neural Module Network

Yuxuan Wu and Hideki Nakayama

The University of Tokyo
{wuyuxuan,nakayama}@nlab.ci.i.u-tokyo.ac.jp

Abstract. Neural Module Network (NMN) is a machine learning model
for solving the visual question answering tasks. NMN uses programs to
encode modules’ structures, and its modularized architecture enables it
to solve logical problems more reasonably. However, because of the non-
diﬀerentiable procedure of module selection, NMN is hard to be trained
end-to-end. To overcome this problem, existing work either included
ground-truth program into training data or applied reinforcement learn-
ing to explore the program. However, both of these methods still have
weaknesses. In consideration of this, we proposed a new learning frame-
work for NMN. Graph-based Heuristic Search is the algorithm we pro-
posed to discover the optimal program through a heuristic search on the
data structure named Program Graph. Our experiments on FigureQA
and CLEVR dataset show that our methods can realize the training
of NMN without ground-truth programs and achieve superior eﬃciency
over existing reinforcement learning methods in program exploration.

0
2
0
2

p
e
S
0
3

]
I

A
.
s
c
[

1
v
9
5
7
4
1
.
9
0
0
2
:
v
i
X
r
a

1 Introduction

With the development of machine learning in recent years, more and more tasks
have been accomplished such as image classiﬁcation, object detection, and ma-
chine translation. However, there are still many tasks that human beings perform
much better than machine learning systems, especially those in need of logical
reasoning ability. Neural Module Network (NMN) is a model proposed recently
targeted to solve these reasoning tasks [1,2]. It ﬁrst predicts a program indicating
the required modules and their layout, and then constructs a complete network
with these modules to accomplish the reasoning. With the ability to break down
complicated tasks into basic logical units and to reuse previous knowledge, NMN
achieved super-human level performance on challenging visual reasoning tasks
like CLEVR [3]. However, because the module selection is a discrete and non-
diﬀerentiable process, it is not easy to train NMN end-to-end.

To deal with this problem, a general solution is to separate the training into
two parts: the program predictor and the modules. In this case, the program
becomes a necessary intermediate label. The two common solutions to provide
this program label are either to include the ground-truth programs into train-
ing data or to apply reinforcement learning to explore the optimal candidate

 
 
 
 
 
 
2

Y. Wu, H. Nakayama

Fig. 1. Our learning framework enables the NMN to solve the visual reasoning problem
without ground-truth program annotation.

program. However, these two solutions still have the following limitations. The
dependency on ground-truth program annotation makes NMN’s application hard
to be extended to datasets without this kind of annotation. This annotation is
also highly expensive while being hand-made by humans. Therefore, program
annotation cannot always be expected to be available for tasks in real-world
environments. In view of this, methods relying on ground-truth program an-
notation cannot be considered as complete solutions for training NMN. On the
other hand, the main problem in the approaches based on reinforcement learning
is that with the growth of the length of programs and number of modules, the
size of the search space of possible programs becomes so huge that a reasonable
program may not be found in an acceptable time.

In consideration of this, we still regard the training of NMN as an open
problem. With the motivation to take advantage of NMN on broader tasks and
overcome the diﬃculty in its training in the meanwhile, in this work, we pro-
posed a new learning framework to solve the non-diﬀerentiable module selection
problem in NMN.

In this learning framework, we put forward the Graph-based Heuristic Search
algorithm to enable the model to ﬁnd the most appropriate program by itself.
Basically, this algorithm is inspired by Monte Carlo Tree Search (MCTS). Sim-
ilar to MCTS, our algorithm conducts a heuristic search to discover the most
appropriate program in the space of possible programs. Besides, inspired by the
intrinsic connection between programs, we proposed the data structure named
Program Graph to represent the space of possible programs in a way more rea-
sonable than the tree structure used by MCTS. Further, to deal with the cases
that the search space is extremely huge, we proposed the Candidate Selection
Mechanism to narrow down the search space.

With these proposed methods, our learning framework implemented the
training of NMN regardless of the existence of the non-diﬀerentiable module
selection procedure. Compared to existing work, our proposed learning frame-
work has the following notable characteristics:

Graph-based Heuristic Search for Module Selection in NMN

3

– It can implement the training of NMN with only the triplets of {question,

image, answer} and without the ground-truth program annotation.
– It can explore larger search spaces more reasonably and eﬃciently.
– It can work on both trainable modules with neural architectures and non-

trainable modules with discrete processing.

2 Related Work

2.1 Visual Reasoning

Generally, Visual Reasoning can be considered as a kind of Visual Question
Answering (VQA) [4]. Besides the requirement of understanding information
from both images and questions in common VQA problems, Visual Reasoning
further asks for the capacity to recognize abstract concepts such as spatial,
mathematical, and logical relationships. CLEVR [5] is one of the most famous
and widely used datasets for Visual Reasoning. It provides not only the triplets
of {question, image, answer} but also the functional programs paired with each
question. FigureQA [6] is another Visual Reasoning dataset we focus on in this
work. It provides questions in ﬁfteen diﬀerent templates asked on ﬁve diﬀerent
types of ﬁgures.

To solve Visual Reasoning problems, a naive approach would be the combi-
nation of Convolutional Neural Network (CNN) and Recurrent Neural Network
(RNN). Here, CNN and RNN are responsible for extracting information from
images and questions, respectively. Then, the extracted information is combined
and fed to a decoder to obtain the ﬁnal answer. However, this methodology of
treating Visual Reasoning simply as a classiﬁcation problem sometimes cannot
achieve desirable performance due to the diﬃculty of learning abstract concepts
and relations between objects [4,6,3]. Instead, more recent work applied models
based on NMN to solve Visual Reasoning problems [3,7,8,9,10,11,12].

2.2 Neural Module Network

Neural Module Network (NMN) is a machine learning model proposed in 2016 [1,2].
Generally, the overall architecture of NMN can be considered as a controller and
a set of modules. Given the question and the image, ﬁrstly, the controller of
NMN takes the question as input and outputs a program indicating the required
modules and their layout. Then, the speciﬁed modules are concatenated with
each other to construct a complete network. Finally, the image is fed to the
assembled network and the answer is acquired from the root module. As far
as we are concerned, the advantage of NMN can be attributed to the ability
to break down complicated questions into basic logical units and the ability to
reuse previous knowledge eﬃciently.

By the architecture of modules, NMN can further be categorized into three

subclasses: the feature-based, attention-based, and object-based NMN.

4

Y. Wu, H. Nakayama

For feature-based NMNs, the modules apply CNNs and their calculations
are directly conducted on the feature maps. Feature-based NMNs are the most
concise implementation of NMN and were utilized most in early work [3].

For attention-based NMNs, the modules also apply neural networks but their
calculations are conducted on the attention maps. Compared to feature-based
NMNs, attention-based NMNs retain the original information within images bet-
ter so they achieved higher reasoning precision and accuracy [1,2,7,9].

For object-based NMNs, they regard the information in an image as a set of
discrete representations on objects instead of a continuous feature map. Corre-
spondingly, their modules conduct pre-deﬁned discrete calculations. Compared
to feature-based and attention-based NMNs, object-based NMNs achieved the
highest precision on reasoning [10,11]. However, their discrete design usually
requires more prior knowledge and pre-deﬁned attributes on objects.

2.3 Monte Carlo Methods

Monte Carlo Method is the general name of a group of algorithms that make use
of random sampling to get an approximate estimation for a numerical comput-
ing [13]. These methods are broadly applied to the tasks that are impossible or
too time-consuming to get exact results through deterministic algorithms. Monte
Carlo Tree Search (MCTS) is an algorithm that applied the Monte Carlo Method
to the decision making in game playing like computer Go [14,15]. Generally, this
algorithm arranges the possible state space of games into tree structures, and
then applies Monte Carlo estimation to determine the action to take at each
round of games. In recent years, there also appeared approaches to establish
collaborations between Deep Learning and MCTS. These work, represented by
AlphaGo, have beaten top-level human players on Go, which is considered to be
one of the most challenging games for computer programs [16,17].

3 Proposed Method

3.1 Overall Architecture

The general architecture of our learning framework is shown as Fig.2. As stated
above, the training of the whole model can be divided into two parts: a. Pro-
gram Predictor and b. modules. The main diﬃculty of training comes from the
side of Program Predictor because of the lack of expected programs as training
labels. To overcome this diﬃculty, we proposed the algorithm named Graph-
based Heuristic Search to enable the model to ﬁnd the optimal program by itself
through a heuristic search on the data structure Program Graph. After this
searching process, the most appropriate program that was found is utilized as
the program label so that the Program Predictor can be trained in a supervised
manner. In other words, this searching process can be considered as a procedure
targeted to provide training labels for the Program Predictor.

The abstract of the total training workﬂow is presented as Algorithm 1. Note
that here q denotes the question, p denotes the program, {module} denotes the

Graph-based Heuristic Search for Module Selection in NMN

5

Fig. 2. Our Graph-based Heuristic Search algorithm assists the learning of the Program
Predictor.

set of modules available in the current task, {img} denotes the set of images that
the question is asking on, {ans} denotes the set of answers paired with images.
Details about the Sample function are provided in Appendix A.

Program Predictor, {module} ← Intialize()
for loop in range(M ax loop) do

Algorithm 1 Total Training Workﬂow
1: function Train()
2:
3:
4:
5:
6:
7:
8: end function

end for

q, {img}, {ans} ← Sample(Dataset)
p ← Graph-based Heuristic Search(q, {img}, {ans}, {module})
Program Predictor.train(q, p)

3.2 Program Graph

To start with, we ﬁrst give a precise deﬁnition of the program we use. Note that
each of the available modules in the model has a unique name, ﬁxed numbers of
inputs, and one output. Therefore, a program can be deﬁned as a tree meeting
the following rules :

i) Each of the non-leaf nodes stands for a possible module, each of the leaf

nodes holds a (cid:104)END(cid:105) ﬂag.

ii) The number of children that a node has equal to the number of inputs of

the module that the node represents.

For the convenience of representation in prediction, a program can also be
transformed into a sequence of modules together with (cid:104)END(cid:105) ﬂags via pre-order
tree traversal. Considering that the number of inputs of each module is ﬁxed,
the tree form can be rebuilt from such sequence uniquely.

Then, as for the Program Graph, Program Graph is the data structure we use
to represent the relation between all programs that have been reached through-
out the searching process, and it is also the data structure that our algorithm
Graph-based Heuristic Search works on. A Program Graph can be built meeting
the following rules :

i) Each graph node represents a unique program that has been reached.

QuestionProgram PredictorFor training:Graph-based Heuristic Search√Program{Image}{Answer}{Module}Module Networka.b.6

Y. Wu, H. Nakayama

Fig. 3. Illustration of part of a Program Graph

ii) There is an edge between two nodes if and only if the edit distance of
their programs is one. Here, insertion, deletion, and substitution are the three
basic edit operations whose edit distance is deﬁned as one. Note that the edit
distance between programs is judged on their tree form.

iii) Each node in the graph maintains a score. This score is initialized as the
output probability of the program of a node according to the Program Predictor
when the node is created, and can be updated when the program of a node is
executed.

Fig.3 is an illustration of a Program Graph consisting of several program
nodes together with their program trees as examples. To distinguish the node in
the tree of a program and the node in the Program Graph, the former will be
referred to as m n for “module node” and the latter will be referred to as p n
for “program node” in the following discussion. Details about the initialization
of the Program Graph are provided in Appendix B.

3.3 Graph-based Heuristic Search

Graph-based Heuristic Search is the core algorithm in our proposed learning
framework. Its basic workﬂow is presented as the M ain function in line 1 of
Algorithm 2. After Program Graph g gets initialized, the basic workﬂow can be
described as a recurrent exploration on the Program Graph consisting of the
following four steps :

i) Collecting all the program nodes in Program Graph g that have not been

fully explored yet as the set of candidate nodes {p n}c.

ii) Calculating the Expectation for all the candidate nodes.
iii) Selecting the node with the highest Expectation value among all the

candidate nodes.

iv) Expanding on the selected node to generate new program nodes and

update the Program Graph.

The details about the calculation of Expectation and expanding strategy are

as follows.

Program Grapha b c … : modules[E] : <END> flagGraph-based Heuristic Search for Module Selection in NMN

7

g ← InitializeGraph(q)
for step in range(M ax step) do

Algorithm 2 Graph-based Heuristic Search
1: function Main(q, {img}, {ans}, {module})
2:
3:
4:
5:
6:
7:
8:
9:
10:
11: end function

{p n}c ← p n for p n in g and p n.fully explored == False
p ni.Exp ← FindExpectation(p ni, g) for p ni in {p n}c
p ne ← p ni s.t. p ni.Exp = max{p ni.Exp for p ni in {p n}c}
Expand(p ne, g, {img}, {ans}, {module})

end for
p nbest ← p ni s.t. p ni.score = max{p ni.score for p ni in {p ni}}
return p nbest.program

p ne.score ← accuracy(p ne.program, {img}, {ans}, {module})
p ne.visited ← True

end if
{m n}c ← m n for m n in p ne.program and m n.expanded == False
m nm ← Sample({m n}c)
{program}new ← Mutate(p ne.program, m nm, {module})
for programi in {program}new do

p ne.visit count ← p ne.visit count + 1
if p ne.visited == False then

12: function Expand(p ne, g, {img}, {ans}, {module})
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28: end function

if LegalityCheck(programi) == True then

g.update(programi)

end if

end for
m nm.expanded ← True
p ne.fully explored ← True if {m n}c.remove(m nm) == ∅

Expectation Expectation is a grade deﬁned on each program node to determine
which node should be selected for the following expansion step. This Expectation
is calculated through the following Equation 1.

D
(cid:88)

Exp =

wd ∗ max{p nj.score | p nj in g, distance(p ni, p nj) ≤ d}

d=0

+

α
p ni.visit count + 1

(1)

Intuitively, this equation measures how desirable a program is to guide the
modules to answer a given question reasonably. Here, D, wd, and α are hyper-
parameters indicating the max distance in consideration, a sequence of weight
coeﬃcients while summing best scores in the diﬀerent distance d, and the scale
coeﬃcient to encourage visiting unexplored nodes, respectively.

8

Y. Wu, H. Nakayama

In this equation, the ﬁrst term observes the nodes nearby and ﬁnd the highest
score in each diﬀerent distance d from 0 to D. Then, these scores are weighted
by wd and summed up. Note that the distance here is measured on the Program
Graph, which also equals to the edit distance between two programs. The second
term in this equation is a balance term negatively correlated to the number of
times that a node has been visited and expanded on. This term balances the
grades of unexplored or less explored nodes.

Expansion Strategy Expansion is another important procedure in our pro-
posed algorithm as shown in line 12 of Algorithm 2. The main objective of this
procedure is to generate new program nodes and update the Program Graph.
To realize this, the ﬁve main steps are as follows:

i) If the node p ne in Program Graph is visited for the ﬁrst time, try its
program by building the model with speciﬁed modules to answer the question,
then update the score of the node with the accuracy. If there are modules with
neural architecture, these modules should also be trained here, but the updated
parameters are retained only if the new accuracy exceeds the previous one.

ii) Collect the module nodes that have not been expanded on yet within the
program, then sample one from them as the module node m nm to expand on.
iii) Mutate the program at module m nm to generate a new set of programs
{program}new with three edit operations: insertion, deletion, and substitution.
iv) For the new programs judged to be legal, if there is not yet a node
representing the same program in the Program Graph g, then create a new
program node representing this program and add it to g. The related edge should
also be added to g if it does not exist yet.

v) If all of the module nodes have been expanded on, then mark this program

node p ne as fully explored.

For the Mutation in step iii), the three edit operations are illustrated by
Fig.4. Here, insertion adds a new module node between the node m nm and its
parent node. The new module can be any of the available modules in the model.
If the new module has more than one inputs, m nm should be set as one of its
children, and the rest of the children are set to leaf nodes with (cid:104)END(cid:105) ﬂag.

Deletion deletes the node m nm and set its child as the new child of m nm’s
parent. If m nm has more than one child, only one of them should be retained
and the others are abandoned.

Substitution replaces the module of m nm with another module. The new
module can be any of the modules that have the same number of inputs as
m nm.

For insertion and deletion, if they are multiple possible mutations because
the related node has more than one child as shown in Fig.4, all of them are
retained.

These rules ensure that newly generated programs consequentially have legal
structures, but there are still cases that these programs are not legal in the
sense of semantics, e.g., the output data type of a module does not match the
input data type of its parent. Legality check is conducted to determine whether

Graph-based Heuristic Search for Module Selection in NMN

9

a program is legal and should be added to the Program Graph, more details
about this function are provided in Appendix C.

Fig. 4. Example of the mutations generated by the three opeartions insertion, deletion,
and subsitution.

3.4 Candidate Selection Mechanism for Modules

The learning framework presented above is already a complete framework to
realize the training of the NMN. However, in practice we found that with the
growth of the length of programs and the number of modules, the size of search
space explodes exponentially. This brings trouble to the search. To overcome
this problem, we further proposed the Candidate Selection Mechanism (CSM),
which is an optional component within our learning framework. Generally speak-
ing, if CSM is activated, it selects only a subset of modules from the whole of
available modules. Then, only these selected modules are used in the following
Graph-based Heuristic Search. The abstract of the training workﬂow with CSM
is presented as Algorithm 3.

Here, we included another model named Necessity Predictor into the learn-
ing framework. This model takes the question as input, and predicts a Nm-
dimensions vector as shown in Fig.5. Here, Nm indicates the total number of

Program Predictor, Necessity Predictor, {module} ← Intialize()
for loop in range(M ax loop) do

Algorithm 3 Training Workﬂow with Candidate Selection Mechanism
1: function Train()
2:
3:
4:
5:
6:
7:
8:
9:
10: end function

q, {img}, {ans} ← Sample(Dataset)
{module}candidate ← Necessity Predictor(q, {module})
p ← Graph-based Heuristic Search(q, {img}, {ans}, {module}candidate)
Necessity Predictor.train(q, p)
Program Predictor.train(q, p)

end for

InsertionDeletionSubstitutionandand10

Y. Wu, H. Nakayama

modules. Each value in the output vector is a real number in the range of [0,
1] indicating the possibility that each module is necessary for the solution of
the given question. Np and Nr are the two hyperparameters for the candidate
modules selection procedure. Np indicates the number of modules that are se-
lected according to the predicted possibility value, i.e., to select Np modules
with the top Np prediction values. Nr indicates the number of modules that are
selected randomly besides the Np ones. Then, the union of these two selections
with Np + Nr modules becomes the candidate modules for the following search.
For the training of this Necessity Predictor, the best program found in the
search is transformed into a Nm-dimensions boolean vector indicating whether
each module appeared in the program. Then, this boolean vector is set as the
training label so that the Necessity Predictor can also be trained in a supervised
manner as Program Predictor does.

Fig. 5. The process to selecte the Np + Nr candidate modules

4 Experiments and Results

Our experiments are conducted on the FigureQA and the CLEVR dataset. Their
settings and results are presented in the following subsections respectively.

4.1 FigureQA Dataset

The main purpose of the experiment on FigureQA is to certify that our learning
framework can realize the training of NMN on a dataset without ground-truth
program annotations and outperform the existing methods with models other
than NMN.

An overview of how our methods work on this dataset is shown in Fig.6.
Considering that the size of the search space of the programs used in FigureQA
is relatively small, the CSM introduced in Section 3.4 is not activated.

Generally, the workﬂow consists of three main parts. Firstly, the technique of
object detection [18] together with optical character recognition [19] are applied

Question:How many cubes are there?Necessity PredictorFilter_shape[cube]Filter_shape[cylinder]Filter_shape[sphere]……CountExistSceneSceneRandomTopCandidatesGraph-based Heuristic Search for Module Selection in NMN

11

Table 1. Setting of hyperparameters in our experiment

M ax loop M ax step D
4

1000

100

wd
(0.5, 0.25, 0.15, 0.1)

α
0.05

to transform the raw image into discrete element representations as shown in
Fig.6.a. For this part, we applied Faster R-CNN [20,21] with ResNet 101 as the
backbone for object detection and Tesseract open source OCR engine [22,23] for
text recognition. All the images are resized to 256 by 256 pixels before following
calculations.

Secondly, for the part of program prediction as shown in Fig.6.b., we applied
our Graph-based Heuristic Search algorithm for the training. The setting of the
hyperparameters for this part are shown in Table 1. The type of ﬁgure is treated
as an additional token appended to the question.

Thirdly, for the part of modules as shown in Fig.6.c., we designed some
pre-deﬁned modules with discrete calculations on objects. Their functions are
corresponded to the reasoning abilities required by FigureQA. These pre-deﬁned
modules are used associatively with modules with neural architecture. Details of
all these modules are provided in Appendix D.

Table 2 shows the results of our methods compared with baseline and existing
methods. “Ours” is the primitive result from the experiment settings presented
above. Besides, we also provide the result named “Ours + GE” where “GE”
stands for ground-truth elements. In this case, element annotations are obtained
directly from ground-truth plotting annotations provided by FigureQA instead
of the object detection results. We applied this experiment setting to measure
the inﬂuence of the noise in object detection results.

Through the result, ﬁrstly it can be noticed that both our method and our
method with GE outperform all the existing methods. In our consideration, the
superiority of our method mainly comes from the successful application of NMN.
As stated in Section 2.2, NMN has shown outstanding capacity in solving logical

Fig. 6. An example of the inference process on FigureQA

Object DetectionQ.Is Aqua the maximum?Program PredictorFor training:Graph-based Heuristic SearchDiscriminator, LookUp,FindElement, <End>,<End>A:  Yesa.b.c.12

Y. Wu, H. Nakayama

Table 2. Comparison of accuracy with previous methods on the FigureQA dataset.

Method

Text only [6]
CNN+LSTM [6]
Relation Network [6,24]
Human [6]
FigureNet [25]
PTGRN [26]
PReFIL [27]
Ours
Ours + GE

Accuracy

Validation Sets
Set 2
Set 1

Test Sets

Set 1

Set 2

50.01%
56.16%
72.54%

50.01%
56.00%
72.40%
91.21%
84.29%
86.25%
86.23%
94.88% 93.16%
94.84% 93.26%
95.74% 95.55% 95.61% 95.28%
96.61% 96.52%

problems. However, limited by the non-diﬀerentiable module selection procedure,
the application of NMN can hardly be extended to those tasks without ground-
truth program annotations like FigureQA. In our work, the learning framework
we proposed can realize the training of NMN without ground-truth programs so
that we succeeded to apply NMN on this FigureQA. This observation can also
be certiﬁed through the comparison between our results and PReFIL.

Compared to PReFIL, considering that we applied the nearly same 40-layer
DenseNet to process the image, the main diﬀerence we made in our model is the
application of modules. The modules besides the ﬁnal Discriminator ensure that
the inputs fed to the Discriminator are related to what the question is asking on
more closely.

Here, another interesting fact shown by the result is the diﬀerence between
accuracies reached on set 1 and set 2 of both validation sets and test sets. Note
that in FigureQA, validation set 1 and test set 1 adopted the same color scheme
as the training set, while validation set 2 and test set 2 adopted an alternated
color scheme. This diﬀerence leads to the diﬃculty of the generalization from
the training set to the two set 2. As a result, for PReFIL the accuracy on each
set 2 drops more than 1.5% from the corresponding set 1. However, for our
method with NMN, this decrease is only less than 0.4%, which shows a better
generalization capacity brought by the successful application of NMN.

Also, Appendix E reports the accuracies achieved on test set 2 by diﬀerent
question types and ﬁgure types. It is worth mentioning that our work is the ﬁrst
one to exceed human performance on every question type and ﬁgure type.

4.2 CLEVR Dataset

The main purpose of the experiment on CLEVR is to certify that our learn-
ing framework can achieve superior searching eﬃciency compared to the classic
reinforcement learning method.

For this experiment, we created a subset of CLEVR containing only those
training data whose questions appear at least two times in all training questions.

Graph-based Heuristic Search for Module Selection in NMN

13

There are 31252 diﬀerent questions together with their corresponding programs
in this subset. The reason of applying such a subset is that the size of the whole
space of possible programs is approximately up to 1040, which is so huge that
no existing method can realize the search in it without any prior knowledge or
simpliﬁcation on programs. Considering that the training of modules is highly
time-consuming, we only activate the part of program prediction in our learning
framework, which is shown as Fig.6.b. With this setting, the modules speciﬁed by
the program would not be trained actually. Instead, a boolean value indicating
whether the program is correct or not is returned to the model as a substitute for
the question answering accuracy. Here, only the programs that are exactly the
same as the ground-truth programs paired with given questions are considered
as correct.

In this experiment, comparative experiments were made on the cases of both
activating and not activating the CSM. The structures of the models used as
the Program Predictor and the Necessity Predictor are as follows. For Program
Predictor, we applied a 2-layer Bidirectional LSTM with hidden state size of 256
as the encoder, and a 2-layer LSTM with hidden state size of 512 as the decoder.
Both the input embedding size of encoder and decoder are 300. The setting of
hyperparameters are the same as FigureQA as shown in Table.1 except that
M ax loop is not limited. For Necessity Predictor, we applied a 4-layer MLP.
The input of the MLP is a boolean vector indicating whether each word in the
dictionary appears in the question, the output of the MLP is a 39-dimensional
vector for there are 39 modules in CLEVR, the size of all hidden layers is 256. The
hyperparameters Np and Nr are set to 15 and 5 respectively. For the sentence
embedding model utilized in the initialization of the Program Graph, we applied
the GenSen model with pre-trained weights [28,29].

For the baseline, we applied REINFORCE [30] as most of the existing work [3,12]

did to train the same Program Predictor model.

The searching processes of our method, our method without CSM, and REIN-
FORCE are shown by Fig.7. Note that in this ﬁgure, the horizontal axis indicates
the times of search, the vertical axis indicates the number of correct programs
found. The experiments on our method and our method without CSM are re-
peated four times each, and the experiment on REINFORCE is repeated eight
times. Also, we show the average results as the thick solid lines in this ﬁgure
indicating the average times of search used to ﬁnd speciﬁc numbers of correct
programs. Although in this subset of CLEVR, the numbers of correct programs
that can be ﬁnally found are quite similar for the three methods, their searching
processes show great diﬀerences. From this result, three main conclusions can
be drawn.

Firstly, in terms of the average case, our method shows a signiﬁcantly higher

eﬃciency in searching appropriate programs.

Secondly, the searching process of our method is much more stable while the

best case and worst case of REINFORCE diﬀer greatly.

Thirdly, the comparison between the result of our method and our method

without CSM certiﬁed the eﬀectiveness of the CSM.

14

Y. Wu, H. Nakayama

Fig. 7. Relation between the times of search and the number of correct programs found
within the searching processes of three methods.

5 Conclusion

In this work, to overcome the diﬃculty of training NMN because of its non-
diﬀerentiable module selection procedure, we proposed a new learning framework
for the training of the NMN. Our main contribution in this framework can be
summarized as follows.

Firstly, we proposed the data structure named Program Graph to represent

the search space of programs more reasonably.

Secondly and most importantly, we proposed the Graph-based Heuristic
Search algorithm to enable the model to ﬁnd the most appropriate program
by itself to get rid of the dependency on the ground-truth programs in training.
Thirdly, we proposed the Candidate Selection Mechanism to improve the

performance of the learning framework when the search space is huge.

Through the experiment, the experiment on FigureQA certiﬁed that our
learning framework can realize the training of NMN on a dataset without ground-
truth program annotations and outperform the existing methods with models
other than NMN. The experiment on CLEVR certiﬁed that our learning frame-
work can achieve superior eﬃciency in searching programs compared to the clas-
sic reinforcement learning method. In view of this evidence, we conclude that
our proposed learning framework is a valid and advanced approach to realize the
training of NMN.

Nevertheless, our learning framework still cannot deal with the extremely
huge search spaces, e.g., the whole space of possible programs in CLEVR. We
leave further study on methods that can realize the search in such enormous
search spaces as the future work.

Acknowledgment

This work was supported by JSPS KAKENHI Grant Number JP19K22861.

Graph-based Heuristic Search for Module Selection in NMN

15

References

1. Andreas, J., Rohrbach, M., Darrell, T., Klein, D.: Neural module networks. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
(2016) 39–48

2. Andreas, J., Rohrbach, M., Darrell, T., Klein, D.: Learning to compose neural
networks for question answering. In: Proceedings of the 2016 Conference of the
North American Chapter of the Association for Computational Linguistics: Human
Language Technologies. (2016) 1545–1554

3. Johnson, J., Hariharan, B., van der Maaten, L., Hoﬀman, J., Fei-Fei, L.,
Lawrence Zitnick, C., Girshick, R.: Inferring and executing programs for visual
reasoning.
In: Proceedings of the IEEE International Conference on Computer
Vision. (2017) 2989–2998

4. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C., Parikh,
D.: Vqa: Visual question answering. In: Proceedings of the IEEE international
conference on computer vision. (2015) 2425–2433

5. Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C.,
Girshick, R.: Clevr: A diagnostic dataset for compositional language and elemen-
tary visual reasoning. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition. (2017) 2901–2910

6. Kahou, S.E., Michalski, V., Atkinson, A., K´ad´ar, ´A., Trischler, A., Bengio, Y.:
Figureqa: An annotated ﬁgure dataset for visual reasoning. In: International Con-
ference on Learning Representations. (2018)

7. Hu, R., Andreas, J., Rohrbach, M., Darrell, T., Saenko, K.: Learning to reason:
End-to-end module networks for visual question answering. In: Proceedings of the
IEEE International Conference on Computer Vision. (2017) 804–813

8. Hu, R., Andreas, J., Darrell, T., Saenko, K.: Explainable neural computation via
In: Proceedings of the European conference on

stack neural module networks.
computer vision (ECCV). (2018) 53–69

9. Mascharka, D., Tran, P., Soklaski, R., Majumdar, A.: Transparency by design:
Closing the gap between performance and interpretability in visual reasoning. In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
(2018) 4942–4950

10. Shi, J., Zhang, H., Li, J.: Explainable and explicit visual reasoning over scene
graphs. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. (2019) 8376–8384

11. Yi, K., Wu, J., Gan, C., Torralba, A., Kohli, P., Tenenbaum, J.: Neural-symbolic
In: Ad-

vqa: Disentangling reasoning from vision and language understanding.
vances in Neural Information Processing Systems. (2018) 1031–1042

12. Mao, J., Gan, C., Kohli, P., Tenenbaum, J.B., Wu, J.: The neuro-symbolic concept
learner: Interpreting scenes, words, and sentences from natural supervision.
In:
International Conference on Learning Representations. (2019)

13. Metropolis, N., Ulam, S.: The monte carlo method. Journal of the American

statistical association 44 (1949) 335–341

14. Kocsis, L., Szepesv´ari, C.: Bandit based monte-carlo planning.

In: European

conference on machine learning, Springer (2006) 282–293

15. Coulom, R.: Eﬃcient selectivity and backup operators in monte-carlo tree search.
In: International conference on computers and games, Springer (2006) 72–83

16

Y. Wu, H. Nakayama

16. Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den Driessche, G.,
Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al.: Master-
ing the game of go with deep neural networks and tree search. nature 529 (2016)
484

17. Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A.,
Hubert, T., Baker, L., Lai, M., Bolton, A., et al.: Mastering the game of go without
human knowledge. Nature 550 (2017) 354

18. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for ac-
curate object detection and semantic segmentation. In: Proceedings of the IEEE
conference on computer vision and pattern recognition. (2014) 580–587

19. Singh, S.: Optical character recognition techniques: a survey. Journal of emerging

Trends in Computing and information Sciences 4 (2013) 545–550

20. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object
detection with region proposal networks. In: Advances in Neural Information Pro-
cessing Systems (NIPS). (2015)

21. Yang, J., Lu, J., Batra, D., Parikh, D.: A faster pytorch implementation of faster

r-cnn. https://github.com/jwyang/faster-rcnn.pytorch (2017)

22. Smith, R.: An overview of the tesseract ocr engine. In: Ninth International Con-
ference on Document Analysis and Recognition (ICDAR 2007). Volume 2., IEEE
(2007) 629–633

23. Smith, R.: Tesseract open source ocr engine.

https://github.com/tesseract-

ocr/tesseract (2019)

24. Santoro, A., Raposo, D., Barrett, D.G., Malinowski, M., Pascanu, R., Battaglia,
P., Lillicrap, T.: A simple neural network module for relational reasoning.
In:
Advances in neural information processing systems. (2017) 4967–4976

25. Reddy, R., Ramesh, R., Deshpande, A., Khapra, M.M.: Figurenet: A deep learn-
ing model for question-answering on scientiﬁc plots. In: 2019 International Joint
Conference on Neural Networks (IJCNN), IEEE (2019) 1–8

26. Cao, Q., Liang, X., Li, B., Lin, L.: Interpretable visual question answering by rea-
soning on dependency trees. IEEE Transactions on Pattern Analysis and Machine
Intelligence (2019)

27. Kaﬂe, K., Shrestha, R., Price, B., Cohen, S., Kanan, C.: Answering questions about
data visualizations using eﬃcient bimodal fusion. arXiv preprint arXiv:1908.01801
(2019)

28. Subramanian, S., Trischler, A., Bengio, Y., Pal, C.J.: Learning general purpose
distributed sentence representations via large scale multi-task learning. In: Inter-
national Conference on Learning Representations. (2018)

29. Subramanian,

S., Trischler, A., Bengio, Y., Pal, C.J.:

Gensen.

https://github.com/Maluuba/gensen (2018)

30. Williams, R.J.: Simple statistical gradient-following algorithms for connectionist

reinforcement learning. Machine learning 8 (1992) 229–256

Graph-based Heuristic Search for Module Selection in NMN

17

Appendix A Training Data Sampling

The basic sampling unit of training data is triplet as (q, {img}, {ans}). Gener-
ally, as shown in Fig.8, we maintain three sets {U nmet}, {U nsolved}, {Solved}
to distinguish training data in diﬀerent status.

Intuitively, {U nmet} contains training data that have not been met and used.

At the beginning of learning, all the training data is stored in {U nmet}.

{U nsolved} contains training data that has been sampled from {U nmet}
but on which the ﬁnal accuracy achieved in the following search did not reach a
hyperparameter named Acceptable Boundary.

{Solved} contains training data that has been sampled from {U nmet} or

{U nsolved} and the ﬁnal accuracy reached the Acceptable Boundary.

We denote the numbers of training data triplets in these three sets as Num,

Nus and Ns, respectively.

Fig. 8. Data sampling strategy

For each Sample step in each training loop, the training data can be sampled
from either {U nmet} or {U nsolved} with probability Pum and Pus as shown in
Equation 2.

Pum =

(cid:40)

Ns+1 ,

e− Nus
0,

Pus = 1 − Pum

if Num > 0 ;
otherwise

(2a)

(2b)

Appendix B Program Graph Initialization

To initialize the Program Graph, at most three initial program nodes are created
as the starting points for the following search. The programs of them are:

i) The program predicted by the Program Predictor model.
ii) The program found for the question within {Solved} that is closest to the

current given question.

iii) The shortest legal program.
Speciﬁcally for ii), this term only works when {Solved} is not empty. If so,
a pre-trained sentence embedding model SE(·) is utilized to judge the semantic

SamplingProgram SearchYesNo18

Y. Wu, H. Nakayama

distance between questions and ﬁnd a question qc from {Solved} that is seman-
tically closest to the current given question q. This process can be expressed
as Equation 3. Here, SE(·) takes the question sentence as input and outputs a
ﬁxed-length vector. (cid:107) SE(q) − SE(qs) (cid:107) judges the L2 distance between SE(q)
and SE(qs). Then, the program found for qc in previous searches becomes the
initial program for the Program Graph.

qc = arg min

(cid:107) SE(q) − SE(qs) (cid:107)

(3)

qs∈{Solved}

Appendix C Legality Check for Programs

As stated in Section 3.3, our rules for generating mutations on programs can
ensure the legality of structure, but not necessarily the legality of semantics.
Here, the illegality of semantics mainly comes from the type system of modules.
Within NMN, the inputs and outputs passed between modules are restricted with
types such as feature map, number, object, or set of objects. The calculation of
NMN fails if the intermediate data fed to a module does not match the data
type that module requires.

Generally, there are two solutions to this problem. One is to add the ille-
gal programs to the Program Graph anyway yet mark these programs as non-
executable and skip the step of trying these programs to get the accuracies.
However, excessive illegal programs within the Program Graph waste plenty of
searching steps on them so that the eﬃciency of search drops obviously.

The other solution is to simply refuse to add these illegal programs to the
Program Graph. However, in this way the Program Graph is possible to become
disconnected. Therefore, some sub-graphs may never be reached from others.

In consideration of this, we applied a compromise between these two solu-
tions. We used a hyperparameter named T olerance to restrict the maximum
count of data type mismatches that can be tolerated. The programs of which
the count of data type mismatches is not greater than T olerance will still be
added to the Program Graph although they cannot be executed to obtain the
accuracy. This setting can balance the eﬃciency and coverage of the search.

Appendix D Modules Used in FigureQA Dataset

The modules used in the experiment on the FigureQA dataset are shown in
Table 3. Here, the column of “Shape” indicates the number and type of inputs
and output. The column of “Architecture” indicates whether the module is pre-
deﬁned with rule-based calculation, or is a trainable neural network.

Speciﬁcally for the behavior of each module, “Find Element” ﬁnds an element
that matches the given keyword from all the detected elements. Here, keywords
are the name of colors extracted from the questions. Because there are at most
two keywords within a question, two of this module are required and each of
them corresponds to one of the keywords.

Graph-based Heuristic Search for Module Selection in NMN

19

Table 3. Modules used in the experiment on FigureQA dataset

Name
Find Element
Look Up
Look Down
Look Left
Look Right
Find Same

Discriminator

Shape
(None) → Element
(Element) → Element
(Element) → Element
(Element) → Element
(Element) → Element
(Element) → Elements
(Element/Elements/None) * 2
→ Answer

Architecture
pre-deﬁned
pre-deﬁned
pre-deﬁned
pre-deﬁned
pre-deﬁned
pre-deﬁned

Number
2
1
1
1
1
1

neural network

N

“Look Up” ﬁnds the closest element that is in the area of from 45◦ top left

to 45◦ top right of the given element.

“Look Down”, “Look Left”, and “Look Right” behave similarly to “Look

Up”.

“Find Same” ﬁnds a set of elements with the same attributes as the given

element. In this experiment, we specify this attribute to color.

“Discriminator” has two inputs. For each input, it masks the original image
with the bounding boxes of the given element or sets of elements. Then, the
masked image is fed to a neural network to infer the answer. The input can
also be empty. In this case, it would directly feed the original image to the
neural network. To compare our method with existing work fairly, we use a 40-
layer DenseNet similar to the one applied in PReFIL as the backbone of the
Discriminator. The architecture of Discriminator is shown in Fig.9. The number
of ﬁlters in the ﬁrst convolutional layer of DenseNet is 64. Considering that the
two inputs of Discriminator are parallel and most of their features are similar, the
ﬁrst convolutional layer works on them independently with shared weight. All
three following dense blocks have 12 layers. Their growth rate is set to 12. The
number of ﬁnal classes is 2 representing the answer “Yes” or “No” in FigureQA.
For training, we used cross-entropy loss and SGD optimizer with learning rate
decay. The batch size is set to 64. The learning rate is initialized to be 0.1 and

Fig. 9. Architecture of our Discriminator with a 40 layer DenseNet as backbone

ConvolutionConvolutionConcatnateAnswerDenseBlockTransitionDenseBlockTransitionDenseBlockClassifier40 layer DenseNet 20

Y. Wu, H. Nakayama

drops to 0.01, 0.001, 0.0001, and 0.00001 on epoch 8, 12, 16, and 20, respectively.
The maximum number of the epochs of training is 24, yet considering that the
training of a 40-layer DenseNet on the entire 24 epochs is highly time-consuming,
during the search only the training on the ﬁrst 4 epochs are conducted and
the validation accuracy is returned then. After the search on each question is
completed, the Discriminator speciﬁed by the optimal program will be trained
again on the entire 24 epochs.

Appendix E Results by Question Type and Figure Type

in FigureQA Dataset

Table 4. Accuracy on Test Set 2 by diﬀerent question types.

RN Human PReFIL Ours
Question Template
98.44
76.78
Is X the minimum?
98.79
83.47
Is X the maximum?
94.07
66.69
Is X the low median?
94.29
66.50
Is X the high median?
99.43
80.49
Is X less than Y?
99.45
81.00
Is X greater than Y?
95.77
69.57
Does X have the minimum area under the curve?
97.65
Does X have the maximum area under the curve? 78.45
80.90
58.57
Is X the smoothest?
85.19
56.28
Is X the roughest?
95.42
69.65
Does X have the lowest value?
96.68
76.23
Does X have the highest value?
95.19
67.75
Is X less than Y?
95.29
67.12
Is X greater than Y?
95.22
68.75
Does X intersect Y?
95.28
72.18
Overall

97.06
97.18
86.39
86.91
96.15
96.15
94.22
95.36
78.02
79.52
90.33
93.11
90.12
89.88
89.62
91.21

97.20
98.07
93.07
93.00
98.20
98.07
94.00
96.91
71.87
74.67
92.17
94.83
92.38
92.00
91.25
92.79

Table 5. Accuracy on Test Set 2 by diﬀerent ﬁgure types.

Figure Type
Vertical Bar
Horizontal Bar
Pie
Line
Dot Line
Overall

RN
77.13
77.02
73.26
66.69
69.22
72.18

Human PReFIL Ours
98.55
98.25
95.90
99.32
97.98
96.03
94.31
92.84
88.26
92.66
87.79
90.55
93.11
89.57
87.20
95.28
92.79
91.21

