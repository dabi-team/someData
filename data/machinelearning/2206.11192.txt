Functional or imperative? On pleasant semantics for differentiable
programming languages

Michael Innes 1

2
2
0
2

n
u
J

2
2

]
L
P
.
s
c
[

1
v
2
9
1
1
1
.
6
0
2
2
:
v
i
X
r
a

Abstract

In machine learning (ML), researchers and engi-
neers seem to be at odds. System implementers
would prefer models to be declarative, with de-
tailed type information and semantic restrictions
that allow models to be optimised, rearranged
and parallelised. Yet practitioners show an over-
whelming preference for dynamic,
imperative
languages with mutable state, and much engi-
neering effort is spent bridging the resulting se-
mantic divide. Is there a fundamental conﬂict?
This article explores why imperative and func-
tional styles are used, and how future language
designs might get the best of both worlds.

1. Why functional?

General purpose, numerical programming languages are
the right tools for building models in machine learning,
probabilistic programming, and differentiable program-
ming (∂P ). In general, models may be no less complex
than general-purpose programs, and need similar tools to
manage that complexity (Innes et al., 2018a).

ML practitioners tend to use imperative languages – mainly
Python, and to a lesser extent R, Matlab, Julia and a smat-
tering of others – for their work. Yet ML, and particularly
deep learning, frameworks often look quite unlike normal
libraries, imposing an atypical functional style of program-
ming (Colah, 2015).

The term “functional” originated with the use of higher-
order functions, and is now associated with a number of
related ideas. Two that matter for our purposes are:

1. Value semantics, particularly for arrays. Mathematical
operations create new arrays, rather than operating in-
place.

2. Referential transparency, in which a function call can

1Independent.

Correspondence

to: Michael

Innes

<mike.j.innes@gmail.com>.

Copyright 2022 by the author(s).

be replaced by its return value. In other words, im-
plicit dependencies (such as model conﬁguration ob-
jects) can be assumed ﬁxed.

JAX, a Python library for numerics and code transforms
(Frostig et al., 2018), is a clean example of these issues at
It turns the Python interpreter into a compiler, us-
play.
ing objects that abstractly represent arrays to run a partial
evaluation of user code. In effect the result of dictionary
lookups and dynamic dispatch is cached on ﬁrst run, which
avoids overhead during model training. But strictly speak-
ing this is in violation of Python’s semantics: the result of
a dictionary lookup, or almost any other Python operation,
might well change at run time. So JAX simply asks users
to provide a pure function and assumes that they have done
so, with undeﬁned behaviour if the rule is violated. This
has some limitations and sharp edges that would be amelio-
rated with better language-level support.

Julia, a numerical programming language (Bezanson et al.,
2017), is revealing in a different way. Julia’s abstract inter-
pretation is analogous to JAX’s partial evaluation (a proof-
of-concept hybrid, Innes 2020a, showed that they are two
sides of the same coin). But Julia’s compiler must respect
the language’s semantics, without making additional as-
sumptions. Mutable objects, such as Dicts, stymie infer-
ence and must be given explicit type parameters to allow
inference of downstream code. Limited constant propaga-
tion is supported on immutable objects, like Tuples, but
not arrays or dictionaries. Julia’s source-transform based
autodiff library, Zygote, disables array mutation in user
functions due to the complexity of managing their deriva-
tives. Deep learning with Flux, a Julia-based ML stack
(Innes et al., 2018b), therefore looks functional: but in this
case the core language’s use of mutation impedes, rather
than enabling, high performance code.

The trend towards functional-style, relatively declarative
programming with compilers like XLA (Sabne, 2020) is
a signiﬁcant shift from the conventional style of scientiﬁc
and numerical computing,1 in which arrays are explicitly al-
located and modiﬁed in loops, with a close correspondence

1And indeed the style Julia was designed to cater for, albeit

with more modern language affordances.

 
 
 
 
 
 
On pleasant semantics for differentiable programming languages

between program and machine instructions. The change is
partly driven by increasingly diverse hardware, as well as
the relatively ﬁxed set of core numerical kernels used in
ML. Kernels individually beneﬁt from hand optimisation,
but compositions of them have less room for improvement,
even for hardware experts. Low-level control is therefore
less valuable for users, and general optimisations like sim-
ple loop fusion, memory placement and model-level distri-
bution and parallelism are best done by compilers, on rep-
resentations far more abstract than machine code (Li et al.,
2020).

But the last of these is most often an anti-feature, and the
second is the job of the compiler. It really is only the syntax
that we want.

Despite being superﬁcial in a sense, syntax is an important
part of the user experience, and a perfectly good reason for
users to prefer imperative over functional languages. But
now we are surely asking to have our cake and eat it too:
we don’t like mutable state, yet we want to write programs
with explicit change. In fact, there is no contradiction. To
explain why we’ll have to challenge some misconceptions
about “imperative” syntax.

2. Why imperative?

The discussion above seems to suggest that Haskell, OCaml
or Scala would be better frontends for numerical compilers.
Yet researchers have voted with their feet (or at least with
their keyboards): Python is by far the most popular lan-
guage for ML,2 despite being poorly suited to the task on
paper. Why are dynamic, imperative languages so popular?
Partly this is to do with conventional (though not inherent)
differences between static and dynamic languages – things
like interactivity, ease of use for beginners and the ﬂex-
ibility of dictionary-oriented programming (Dubois et al.,
1996). We don’t delve into those topics, except to point
out that there are perfectly serviceable dynamic, functional
languages like Clojure and Mathematica, so it can’t be a
complete explanation.3

Given that ML frameworks eschew shared mutable state,
the last difference of note between functional and imper-
ative is their conventional syntax choices. Clearly, nested,
structured control ﬂow (as in the loop, if/else statements
and break, return and continue) is a helpful way to
express algorithms, especially the iterative algorithms com-
mon in numerical programming. Just as importantly, the
imperative style allows for a notion of change that is easier
to think with, compared with having to decompose and re-
construct everything: “change column i of matrix A” is a
more natural description than “take all columns but the ith
from A, and concatenate them together with this new one
in the right position to create B”, and more so as updates
get more complex.

Syntax like A[i] = x typically does three things: it pro-
vides a syntactic notion of change as above; it efﬁciently
modiﬁes memory in-place; and it may alter the behaviour
of seventeen other threads of execution that reference A.

2Google Trends supports “python machine learning” being
about 10× more popular than “matlab machine learning”, which
in turn is about 5× more searched than “julia machine learning”.
3There are also some historical accidents that favour Python
speciﬁcally. For example, its unusual choice to use reference
counting happens to be ideal for managing large GPU arrays,
where Julia’s tracing GC struggles.

3. The procedural is pure

When discussing imperative programming, it’s common to
conﬂate mutable local variables with mutable data struc-
tures. In fact they are hardly related. Here’s an example
with the former, but not the latter:

fn pow(x, n) {

r = 1
while n > 0 {

r *= x
n -= 1

}
return r

}

LLVM, a compiler for those most imperative of languages
(Lattner & Adve, 2004), lowers such functions to an inter-
mediate representation (IR) similar to the following:

fn pow(x, n) {

fn block1() {

r = 1
block2(n, r)

}
fn block2(n, r) {

if n > 0 {

block3(n, r)

else {

block4(r)

}

}
fn block3(n, r) {

n_ = n - 1
r_ = r * x
block2(n_, r_)

}
fn block4(r) {

return r

}
block1()

}

LLVM and similar compilers call this form “static single
assignment”, or SSA, because variables are syntactically
immutable – in other words, they behave like bindings in
functional languages. We could equally call this a set of
mutually tail-recursive pure functions (Appel, 1998). This

On pleasant semantics for differentiable programming languages

form is excellent for a wide variety of analysis, optimisa-
tion and transformation (eg autodiff on SSA, Innes 2020b).
All local control ﬂow, however complex, can be lowered to
SSA (Cytron et al., 1989).

The main point is that “mutable” locals are a corollary of
control ﬂow structures, rather than of other kinds of muta-
tion. And control ﬂow is just syntax sugar for simple func-
tions. If syntax sugar does not fundamentally change our
programming paradigm, and the latter example is a func-
tional program, the former must be too. At the least, we
can agree that mutable locals need not violate referential
transparency, because there’s no external way to tell they
are being used.4

It is right to say that variables like r are changing. But
we use the word change in the sense of changing clothes,
without assuming any physical alteration.5 A change might
compile down to a real modiﬁcation (of registers, memory
or whatever) but this is only coincidence. Change and mu-
tation are distinct; C just happens to conﬂate them.

4. The more things change...

The exploration above suggests that clearer notions of
change may help us get the beneﬁts of imperative-style pro-
gramming, without losing those of the functional style. But
simply adding control ﬂow and locals to a functional lan-
guage is not enough. For one thing, it is clunky to write

xs = append(xs, x)

every time we want to change xs (in this case, by append-
ing a new element to the list). It has the beneﬁt of being
explicit about what is changing, but at the cost of some ver-
bosity, especially when multiple inputs are affected.

We propose a design that takes inspiration from the C-like
address operator &, writing this as

append(&xs, x)

which is still explicit, but less verbose. We refer to & as the
“swap operator”. All function calls have a hidden return
value, a list of swapped arguments, so that for example

result = foo(&a, b, &c)

is roughly equivalent to

[result, rs] = foo(a, b, c)
a = rs[1]
c = rs[3]

4Shared mutable state can also be emulated in a functional
language. But a more invasive, global transformation would be
needed to capture all of its “features”, like race conditions and
nondeterminism, so it represents a more fundamental difference.
5change derives from words meaning “barter” or “substitute”;

“cause to turn or pass from one state to another”.

We can write a function to swap two variables as follows:

fn switch(&x, &y) {
[x, y] = [y, x]
return

}

a = 1
b = 2
switch(&a, &b) # now a = 2, b = 1

where the &a in the function signature indicates that the
ﬁnal value of a should be returned to the caller.

The use of & is like the address operator in C, C++, Rust
and Swift (Ritchie et al., 1988). But whereas the change
implied is similar, those languages conﬂate that change
with mutation of the stack, with all the knock-on effects
that implies. In our implementation no references, pointers
or mutable state are created; the & is syntax sugar for a func-
tional program, which means that program transformations
and analyses are not impeded.

A last sprinkle help us recover full convenience. Syntax
like A[i] = x and foo.bar = baz can lower to func-
tion calls (eg set(&A, x, i)). We can update nested
structures like append(&foo.xs, x), which roughly
becomes:

xs = foo.xs
append(&xs, x)
foo.xs = xs

This looks imperative. But only variables are changing –
not values.

Note that in most languages with mutable locals, closures
capture variables by reference. In this more functional set-
ting they should of course capture by value instead. This
is more intuitive in many cases anyway (eg a closure that
captures a loop counter by reference can lead to confusing
bugs).

A side effect of this approach is to avoid a leaky abstraction
most imperative language have:
types that happen to be
implemented as heap-allocated references, like dictionaries
or objects, are mutable, whereas basic types like numbers
or strings are not. More generally, possible modiﬁcations
are dictated by the underlying representation in memory,
and the choice to make an interface in-place or not may be
dictated by implementation concerns. When change and
mutation are distinct, any value is just as changeable as any
other.

With these relatively minor tweaks – structured control
ﬂow, mutable locals and a bit of syntax sugar – we can
write numerical programs almost exactly as we would in
Python or Julia. But because our data structures are values,
we have the guarantees needed for aggressive compilation

On pleasant semantics for differentiable programming languages

and transformation by default. Such optimisation can be
done robustly by the main compiler, without special cases
or sharp edges, and while playing nicely with existing lan-
guage tooling: error messages, debuggers and proﬁlers.

5. Turning change into mutation

One reason that change, in the sense we have discussed,
and mutation are conﬂated is that doing things in-place is
a good optimisation. Where performance matters, it is cru-
cial that this optimisation be predictable to the programmer.
If it isn’t, algorithms may not only have unnecessary over-
heads but also higher computational complexity (because
mutations are usually constant time, whereas copies are lin-
ear). Having to make copies is therefore a signiﬁcant down-
side of the functional style.

is not

fatal, however.

It
Reference counting (and
in Rust or Lobster,
its compile-time analogues, as
Matsakis & Klock 2014; van Oortmerssen) provides a so-
lution: any time the count is 1, no-one else can see our
mutations, so it is safe to update the old data in place.

Koka (Leijen, 2016) has an implementation of this idea,
and it turns out to beneﬁt from the functional pattern of
deconstructing and rebuilding data. For example, say we
have a type Foo with named ﬁelds xs, ys and zs. We
want to append a new item, x, to the xs array. In a func-
tional style we might write the update something like this,
using pattern matching to destructure foo:

Foo(xs, ys, zs) = foo
xs2 = append(xs, x)
foo2 = Foo(xs2, ys, zs)

After the ﬁrst line has executed, foo is not used, and can
therefore be released. Assuming foo is unique, releasing
it will in turn release its hold on xs. xs is now only ref-
erenced from one place (the current stack frame) and can
safely be updated by mutation.

Consider in contrast the version we wrote above, using an
imperative-style update syntax to change foo:

xs = foo.xs
append(&xs, x)
foo.xs = xs

Unlike the last example, foo is live after the ﬁrst line, be-
cause we need it to run the last line. That in turns means
xs is referenced in two places (the current stack frame, and
foo) and can’t be modiﬁed in place.

Fortunately, the swap syntax we introduced for nested up-
dates, append(&foo.xs, x), can also account for this
case. Rather than expanding directly to the above, it ex-
pands to code that decomposes and rebuilds foo more like
the functional-style example. This can work recursively,

meaning that updates to deeply-nested structures will still
reliably happen in place where possible.

6. Related work

There are many interesting efforts to bridge the gap be-
tween functional and imperative, including within numer-
ical and differentiable programming.

Dex (Maclaurin et al., 2019) aims to do numerical program-
ming and code transformation in a Haskell-like setting, no-
tably using constructs for structured mutation inspired by
algebraic effects.

Koka (Leijen, 2016) is a recent functional language with
support for side effects based on algebraic effect handlers.
Notably, effects imply that order matters, so Koka uses a
C-like syntax for sequences of statements.

Clojure (Hickey, 2008) is related for somewhat tangential
reasons. A key beneﬁt of languages like Python is the ease
of “dictionary-oriented programming”, where the built-in
data types are ﬂexible enough that users typically don’t
have to build their own. Clojure showed that this was practi-
cal in a functional setting with its persistent data structures,
which will be an important component of any system re-
sembling a “functional Python”.

Rust (Matsakis & Klock, 2014) takes a different tack to
eliminating shared mutable state, not by avoiding mutation
but by restricting sharing through static analysis. This ap-
proach is probably inappropriate for our use case, but Rust
libraries have thought carefully about how to express more
advanced techniques, such as multiple threads working on
an output array, in a structured and safe way. Those APIs
could inspire functional approaches that can reliably com-
pile to efﬁcient code.

7. Conclusion

The functional and imperative styles are less at odds than
they may ﬁrst appear. At least in machine learning, the
functional approach is valuable for its semantic beneﬁts,
while imperative style brings syntactic and expressive con-
venience. And though the beneﬁts of FP are real, there is
no need to make users do SSA transformation by hand. Per-
haps surprisingly, it is possible to construct a very impera-
tive looking language that nonetheless retains the most im-
portant functional properties. Such designs are promising
for probabilistic and differentiable programming, offering
high ﬂexibility to users expressing models, without sacri-
ﬁcing the ability to do advanced analysis, optimisation and
transformation of the resulting code.

Thanks to Zenna Tavares for encouraging this paper, and
to Maximilian Schleich for reviewing a draft.

On pleasant semantics for differentiable programming languages

References

Appel, A. W. SSA is functional programming. ACM Sig-

plan Notices, 33(4):17–20, 1998.

Bezanson, J., Edelman, A., Karpinski, S., and Shah, V. B.
Julia: A fresh approach to numerical computing. SIAM
review, 59(1):65–98, 2017.

Li, M., Liu, Y., Liu, X., Sun, Q., You, X., Yang, H., Luan,
Z., Gan, L., Yang, G., and Qian, D. The deep learn-
IEEE Trans-
ing compiler: A comprehensive survey.
actions on Parallel and Distributed Systems, 32(3):708–
727, 2020.

Maclaurin, D., Radul, A., Johnson, M. J., and Vytiniotis, D.

Dex: array programming with typed indices. 2019.

Colah,

C.

Neural

and

types,
https://colah.github.io/posts/2015-09-NN-Types-FP/,
2015. Accessed: 2022-05-24.

functional

networks,
programming.

Matsakis, N. D. and Klock, F. S. The rust language. ACM

SIGAda Ada Letters, 34(3):103–104, 2014.

Cytron, R., Ferrante, J., Rosen, B. K., Wegman, M. N.,
and Zadeck, F. K. An efﬁcient method of computing
In Proceedings of the
static single assignment form.
16th ACM SIGPLAN-SIGACT symposium on Principles
of programming languages, pp. 25–35, 1989.

Ritchie, D. M., Kernighan, B. W., and Lesk, M. E. The C
programming language. Prentice Hall Englewood Cliffs,
1988.

Sabne, A. XLA: Compiling machine learning for peak per-

formance. 2020.

van Oortmerssen, W. Memory management in Lobster.

Dubois, P. F., Hinsen, K., and Hugunin, J. Numerical
python. Computers in Physics, 10(3):262–267, 1996.

https://aardappel.github.io/lobster/memory_management.html.
Accessed: 2022-05-24.

Frostig, R., Johnson, M. J., and Leary, C. Compiling ma-
chine learning programs via high-level tracing. Systems
for Machine Learning, pp. 23–24, 2018.

Hickey, R. The Clojure programming language.

In Pro-
ceedings of the 2008 symposium on Dynamic languages,
pp. 1–1, 2008.

Innes, M.

Partial evaluation & abstract interpretation.

https://mikeinnes.io/2020/07/29/mjolnir,
2020a. Accessed: 2022-05-24.

Innes, M. Sense & sensitivities: The path to general-
purpose algorithmic differentiation. Proceedings of Ma-
chine Learning and Systems, 2:58–69, 2020b.

Innes, M., Karpinski, S., Shah, V., Barber, D., Saito Stene-
torp, P., Besard, T., Bradbury, J., Churavy, V., Danisch,
S., Edelman, A., et al. On machine learning and program-
ming languages. Association for Computing Machinery
(ACM), 2018a.

Innes, M., Saba, E., Fischer, K., Gandhi, D., Rudilosso,
M. C., Joy, N. M., Karmali, T., Pal, A., and Shah,
V. Fashionable modelling with Flux. arXiv preprint
arXiv:1811.01457, 2018b.

Lattner, C. and Adve, V. Llvm: A compilation framework
for lifelong program analysis & transformation. In Inter-
national Symposium on Code Generation and Optimiza-
tion, 2004. CGO 2004., pp. 75–86. IEEE, 2004.

Leijen, D. Algebraic effects for functional programming.
Technical report, Technical Report. MSR-TR-2016-29.
Microsoft Research technical report, 2016.

