Learning Pseudo-Backdoors for Mixed Integer Programs

Aaron Ferber, 1 Jialin Song, 2 Bistra Dilkina,1 Yisong Yue 2
1 University of Southern California
2 California Institute of Technology
aferber@usc.edu, jssong@caltech.edu, bdilkina@usc.edu, yyue@caltech.edu

1
2
0
2

n
u
J

9

]

G
L
.
s
c
[

1
v
0
8
0
5
0
.
6
0
1
2
:
v
i
X
r
a

Abstract

We propose a machine learning approach for quickly solv-
ing Mixed Integer Programs (MIP) by learning to prioritize
a set of decision variables, which we call pseudo-backdoors,
for branching that results in faster solution times. Learning-
based approaches have seen success in the area of solving
combinatorial optimization problems by being able to ﬂex-
ibly leverage common structures in a given distribution of
problems. Our approach takes inspiration from the concept
of strong backdoors, which corresponds to a small set of
variables such that only branching on these variables yields
an optimal integral solution and a proof of optimality. Our
notion of pseudo-backdoors corresponds to a small set of
variables such that only branching on them leads to faster
solve time (which can be solver dependent). A key advan-
tage of pseudo-backdoors over strong backdoors is that they
are much amenable to data-driven identiﬁcation or predic-
tion. Our proposed method learns to estimate the solver per-
formance of a proposed pseudo-backdoor, using a labeled
dataset collected on a set of training MIP instances. This
model can then be used to identify high-quality pseudo-
backdoors on new MIP instances from the same distribu-
tion. We evaluate our method on the generalized independent
set problems and ﬁnd that our approach can efﬁciently iden-
tify high-quality pseudo-backdoors. In addition, we compare
our learned approach against Gurobi, a state-of-the-art MIP
solver, demonstrating that our method can be used to improve
solver performance.

Introduction
Mixed integer programs (MIPs) are widely used math-
ematical models for combinatorial optimization prob-
lems (Conforti et al. 2014) that are generally solved to
optimality with a tree search algorithm called branch-
and-bound (Land and Doig 2010). Backdoors, initially in-
troduced for SAT (Williams, Gomes, and Selman 2003;
Dilkina, Gomes, and Sabharwal 2009) and then generalized
to MIPs (Dilkina et al. 2009), are deﬁned as subsets of inte-
ger variables such that only branching on them yields an op-
timal integral solution and a certiﬁcate of optimality. While
their contribution to SAT has theoretical and practical lim-
itations (J¨arvisalo and Junttila 2007; J¨arvisalo and Niemel¨a
2008; Semenov et al. 2018), solve time speedup in MIPs

Copyright © 2021, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

has been observed by prioritizing “backdoors” in branching
(Fischetti and Monaci 2011). We consider a subset of integer
variables a pseudo-backdoor if prioritizing those variables
leads to faster MIP solve time.

We introduce a data-driven approach to predicting
pseudo-backdoors for distributions of MIPs with a scor-
ing model to identify high-quality pseudo-backdoors among
a sample of candidates, and a subsequent classiﬁcation
model
to decide whether to use a candidate pseudo-
backdoor by setting branching priority accordingly or
just use the default solver. We conduct empirical evalua-
tions on the Generalized Independent Set Problem (GISP)
(Colombi, Mansini, and Savelsbergh 2017) and show that
our models achieve faster solve times than Gurobi.

Learning Pseudo-Backdoors for MIP
Our goal in ﬁnding pseudo-backdoors is to quickly solve
MIPs. In a MIP we are asked to ﬁnd real-valued settings for
n decision variables x ∈ Rn, which maximize a linear ob-
jective function cT x, subject to m linear constraints Ax ≤ b,
and with a subset I ⊆ [n] of the decision variables required
to be integral xi ∈ Z ∀i ∈ I. The problem can be written as
maxx{cT x : Ax ≤ b, xi ∈ Z ∀i ∈ I}.

Given a MIP, speciﬁed by P = (c, A, b, I), we want to
ﬁnd a pseudo-backdoor subset B ⊆ I, of the integral deci-
sion variables such that prioritizing branching on these de-
cision variables yields fast solve times. We consider a distri-
butional setting of MIP solving where we train a model on
several MIP instances from a distribution and deploy it on
unseen instances from the same distribution.

Our approach uses two learned models: one that scores
subsets of variables according to performance as pseudo-
backdoors, and another that classiﬁes whether to use a given
pseudo-backdoor over a standard solver. The score model
S(P, B; θS) is parametrized by neural network parameters
θS which takes as input the MIP speciﬁcation P , and a can-
didate subset B, then predicts a score that characterizes if
B is a good pseudo-backdoor. The classiﬁer C(P, B; θC ) is
parametrized by neural network parameters θC and predicts
whether prioritizing B in branching would produce a smaller
runtime than a standard solver. Our models use Graph Atten-
tion Network (Veliˇckovi´c et al. 2018) to perform message
passing followed by global attention pooling (Li et al. 2016)
to represent the graph as a single feature vector, which is

 
 
 
 
 
 
dataset

gisp easy
gisp easy
gisp easy

gisp hard
gisp hard
gisp hard

solver mean

stdev

25 pct median

75 pct win / tie / loss vs grb

grb
scorer
scorer + cls

grb
scorer
scorer + cls

611
960
601

2533
2373
2326

182
755
247

939
855
855

488
515
481

1840
1721
1654

580
649
568

2521
2262
2215

681
915
663

2976
2926
2866

0 / 100 / 0
41 / 0 / 59
24 / 70 / 6

0 / 100 / 0
47 / 0 / 53
47 / 27 / 26

Table 1: Runtime comparison in seconds of standard gurobi (grb), the score model (scorer), and the score model with subsequent
classiﬁcation (scorer+cls) on the test set for 2 hardness settings of gisp.

then fed into a fully connected network to produce scalar
predictions. The MIP is represented as a bipartite graph as
in (Gasse et al. 2019) for permutation invariance and param-
eter sharing. Each MIP has two sets of nodes, one with with
nodes representing variables and another representing con-
straints. Variable nodes have features of the variable’s ob-
jective coefﬁcient, and root LP status. Constraint nodes have
features of the right hand side constant bj, root LP dual vari-
ables, and sense (≤, ≥ or =). There is an edge between a
variable i and a constraint j with attribute Aij if variable i
appears in constraint j. The candidate pseudo-backdoor set
B is represented as an additional binary feature for each vari-
able node with value 1 if the variable is in B and 0 otherwise.
Encoding the input (P, B) as a graph allows us to leverage
state-of-the-art techniques in making predictions on graphs.
We train the score model S by learning to score sub-
sets of integer variables based on their quality (i.e. run-
time) as pseudo-backdoors. The training data contains mul-
tiple pseudo-backdoor candidates for each train MIP in-
stance. For a MIP P and two candidate subsets of in-
teger variables B1, B2 of P , we compute the marginal
ranking loss (Tsochantaridis et al. 2005) loss(s1, s2, y) =
max(0, −y(s1−s2)+m) for a given margin value m, where
s1 = S(P, B1; θS), s2 = S(P, B; θS) are the scalar score
estimates and y is the ranking label (−1 if B1 leads to a
smaller runtime than B2, 1 othewise). Using the ranking loss
trains the model to focus on distinguishing between relative
performance per-instance rather than accurately modeling
the absolute performance. At test time, we sample pseudo-
backdoor candidate sets, score them using our model, and
use the one with the best score to set branching priorities.

We learn a subsequent classiﬁer module to determine
whether to use the candidate subset or the default MIP
solver. The classiﬁer has the same architecture as the scor-
ing model, taking as input the bipartite graph representation
of the MIP P and a candidate subset B, and outputting a
scalar logit for binary classiﬁcation. The training data con-
tains the most promising pseudo-backdoor for each MIP in-
stance, with a binary label indicating whether the pseudo-
backdoor solved the MIP faster than the standard solver.

Experiment Results

We evaluate our method’s two components on MIP instances
sampled from two hardness settings of the Generalized Inde-
pendent Set Problem (GISP) (Hochbaum and Pathria 1997).

Each hardness setting has 3 sets of 100 MIPs each, Ds for
ﬁtting the score model, Dc for the classiﬁcation model, and
Dt for testing results. For each of the 300 MIPs, we sample
50 candidate pseudo-backdoors containing 1% of the inte-
ger variables in a given MIP, with variable selection proba-
bility proportional to the variables’ root LP fractionality as
in (Dilkina et al. 2009).

Table 1 shows that the score model alone performs well
on many instances. On the hard instances, scorer performs
well, having 6% faster runtimes than gurobi on average, and
winning in 47 instances. However, scorer has 57% slower
runtimes on the easy instances, but still outperformed gurobi
on 41 instances, demonstrating that while it has poorer per-
formance on average, it has potential for yielding fast solve
times on many instances. Additionally, we can see that the
score model alone has much higher variability than Gurobi
on the easy instances. This undesired property further mo-
tivates the inclusion of the classiﬁer model to improve the
overall performance. The scorer + cls model outperforms
Gurobi across both MIP distributions in terms of the time
distribution for MIP solving. It performs well across distri-
butions of instances, having the lowest solve times on aver-
age and at different quantiles. The score + cls pipeline out-
performs gurobi by 2%, 8% on average for easy, and hard in-
stances respectively. In terms of win / tie / loss, the scorer’s
losses against gurobi are reduced by the classiﬁer while its
wins are retained. Furthermore, the variability on the gisp
easy instances is reduced to be on par with Gurobi.

Conclusion
In this extended abstract, we have presented a ﬂexible
method for learning and using pseudo-backdoors to improve
MIP solving. Inspired by previous work on backdoors in
MIPs, our method is comprised of two parts: an initial rank-
ing model which scores candidate pseudo-backdoors ac-
cording to how fast they will solve a given MIP, and a sub-
sequent classiﬁcation module to determine whether the se-
lected pseudo-backdoor will indeed result in faster runtime
compared with a base solver. Across varying difﬁculty levels
of GISP distributions, our method ﬁnds high-quality pseudo-
backdoors on unseen MIP instances. Furthermore, we ﬁnd
that the combination of the score model and the classiﬁ-
cation model yields consistently faster solution times com-
pared to a state-of-the-art solver.

Williams, R.; Gomes, C. P.; and Selman, B. 2003. Backdoors
to typical case complexity. In Proceedings of the 18th in-
ternational joint conference on Artiﬁcial intelligence, 1173–
1178.

References
Colombi, M.; Mansini, R.; and Savelsbergh, M. 2017. The
generalized independent set problem: Polyhedral analysis
and solution approaches. European Journal of Operational
Research 260(1): 41–55.

Conforti, M.; Cornu´ejols, G.; Zambelli, G.; et al. 2014. In-
teger programming, volume 271. Springer.

Dilkina, B.; Gomes, C. P.; Malitsky, Y.; Sabharwal, A.; and
Sellmann, M. 2009. Backdoors to combinatorial optimiza-
tion: Feasibility and optimality. In International Conference
on AI and OR Techniques in Constriant Programming for
Combinatorial Optimization Problems, 56–70. Springer.

Dilkina, B.; Gomes, C. P.; and Sabharwal, A. 2009. Back-
doors in the context of learning. In International Conference
on Theory and Applications of Satisﬁability Testing, 73–79.
Springer.

Fischetti, M.; and Monaci, M. 2011. Backdoor branching.
In International Conference on Integer Programming and
Combinatorial Optimization, 183–191. Springer.

Gasse, M.; Ch´etelat, D.; Ferroni, N.; Charlin, L.; and Lodi,
A. 2019. Exact Combinatorial Optimization with Graph
Convolutional Neural Networks. In Advances in Neural In-
formation Processing Systems 32.

Hochbaum, D. S.; and Pathria, A. 1997. Forest harvesting
and minimum cuts: a new approach to handling spatial con-
straints. Forest Science 43(4): 544–554.

J¨arvisalo, M.; and Junttila, T. 2007. Limitations of restricted
branching in clause learning. In International Conference on
Principles and Practice of Constraint Programming, 348–
363. Springer.

J¨arvisalo, M.; and Niemel¨a, I. 2008. The effect of structural
branching on the efﬁciency of clause learning SAT solving:
An experimental study. Journal of Algorithms 63(1-3): 90–
113.

Land, A. H.; and Doig, A. G. 2010. An automatic method
for solving discrete programming problems. In 50 Years of
Integer Programming 1958-2008, 105–132. Springer.

Li, Y.; Zemel, R.; Brockschmidt, M.; and Tarlow, D. 2016.
Gated Graph Sequence Neural Networks. In Proceedings of
ICLR’16.

Semenov, A.; Zaikin, O.; Otpuschennikov, I.; Kochemazov,
S.; and Ignatiev, A. 2018. On cryptographic attacks using
backdoors for SAT. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence.

Tsochantaridis, I.; Joachims, T.; Hofmann, T.; Altun, Y.; and
Singer, Y. 2005. Large margin methods for structured and in-
terdependent output variables. Journal of machine learning
research 6(9).

Veliˇckovi´c, P.; Cucurull, G.; Casanova, A.; Romero, A.; Li`o,
In-
P.; and Bengio, Y. 2018. Graph Attention Networks.
ternational Conference on Learning Representations URL
https://openreview.net/forum?id=rJXMpikCZ. Accepted as
poster.

