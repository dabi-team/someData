This is a preprint. The published version can be accessed at IEEE Transactions on Robotics.

1

Learning from Sparse Demonstrations

Wanxin Jin, Todd D. Murphey, Dana Kuli¬¥c, Neta Ezer, Shaoshuai Mou

2
2
0
2

g
u
A
8

]

O
R
.
s
c
[

3
v
9
5
1
2
0
.
8
0
0
2
:
v
i
X
r
a

Abstract‚ÄîThis paper develops the method of Continuous Pon-
tryagin Differentiable Programming (Continuous PDP), which
enables a robot to learn an objective function from a few
sparsely demonstrated keyframes. The keyframes, labeled with
some time stamps, are the desired task-space outputs, which a
robot is expected to follow sequentially. The time stamps of the
keyframes can be different from the time of the robot‚Äôs actual
execution. The method jointly Ô¨Ånds an objective function and a
time-warping function such that the robot‚Äôs resulting trajectory
sequentially follows the keyframes with minimal discrepancy
loss. The Continuous PDP minimizes the discrepancy loss using
projected gradient descent, by efÔ¨Åciently solving the gradient of
the robot trajectory with respect to the unknown parameters.
The method is Ô¨Årst evaluated on a simulated robot arm and
then applied to a 6-DoF quadrotor to learn an objective function
for motion planning in unmodeled environments. The results
show the efÔ¨Åciency of the method, its ability to handle time
misalignment between keyframes and robot execution, and the
generalization of objective learning into unseen motion condi-
tions.

Index Terms‚ÄîLearning from demonstrations, Pontryagin Dif-
ferentiable Programming (PDP), inverse reinforcement learning,
inverse optimal control, motion planning, optimal control.

I. INTRODUCTION

T HE appeal of learning from demonstrations (LfD) lies in

its capability to facilitate robot programming by simply
providing demonstrations. It circumvents the need for expertise
of modeling and control design, empowering non-experts to
program robots as needed [1]. LfD has been successfully ap-
plied manufacturing [2], assistive robots [3], and autonomous
vehicles [4].

LfD can be broadly categorized into two classes based on
what to learn from demonstrations. The Ô¨Årst branch of LfD
focuses on learning policies [5]‚Äì[9], which maps directly from
robot states, environment, or raw observation data to robot ac-
tions. While effective in many situations, policy learning typ-
ically requires a considerable amount of demonstration data,

Wanxin Jin is with the General Robotics, Automation, Sensing and Per-
ception (GRASP) Laboratory, University of Pennsylvania, PA 19104, USA.
Wanxin Jin is the corresponding author. Email: wanxinjin@gmail.com.

Todd D. Murphey is with the Department of Mechanical Engineering,
Northwestern University, Evanston, IL 60208, USA. This material is partially
based upon work supported by the National Science Foundation under award
1837515. Email: t-murphey@northwestern.edu.

Dana Kuli¬¥c is with the Monash University, Clayton, VIC 3800, Australia.

Email: dana.kulic@monash.edu.

Neta Ezer works for the Northrop Grumman Corporation, Linthicum
Heights, MD 21090, USA. Distribution Statement A: Approved for Pub-
lic Release; Distribution is Unlimited; #20-1350; Dated 08/03/20. Email:
neta.ezer@ngc.com.

Shaoshuai Mou is with the School of Aeronautics and Astronautics,
Purdue University, West Lafayette, IN 47906, USA. Dr. Mou‚Äôs research is
supported in part by grants from the Research in Applications for Learning
Machines (REALM) Consortium of Northrop Grumman Corporation, Rolls-
Royce Corporation, and the NASA University Leadership Initiative (ULI).
Email: mous@purdue.edu.

Fig. 1: Illustration of learning from sparsely demonstrated
keyframes. Each keyframe is a desired output with a time
stamp. We aim to learn an objective function from keyframes
such that the robot motion (blue line) follows these keyframes.
At Ô¨Årst glance, it may seem a problem of ‚Äòcurve Ô¨Åtting‚Äô (i.e.,
Ô¨Ånding a kinematic path). However, a key difference of our
problem is that learning an objective function enables a robot
to generalize new motion in unseen situations, such as given a
new initial condition (green dashed line). A key feature of the
proposed method is that in addition to learning an objective
function, we jointly learn a time-warping function to account
for the misalignment between the keyframe time ti and robot
actual execution time (due to dynamics constraint).

and the learned policy may generalize poorly to unseen tasks
[1]. To alleviate this, the second line of LfD focuses on learn-
ing an objective (cost or reward) function from demonstrations
[10], from which the policies or trajectories are derived. These
methods assume the optimality of demonstrations and use
inverse reinforcement learning (IRL) [11] or inverse optimal
control (IOC) [12] to estimate objective functions. Since an
objective function is a compact and high-level representation
of a task and control principle, learning objective functions has
shown an advantage over policy imitation in terms of better
generalization [13] and relatively lower data complexity [10].
Despite appealing, objective learning based LfD inherits some
limitations from existing IOC/IRL methods 1 [14]‚Äì[19].

First, existing IOC/IRL methods cannot handle the time
misalignment between demonstrations and actual execution of
a robot [20]. For instance, the speed of demonstrations maybe
not be achievable by a robot, as the robot is actuated by weak
motors and cannot move as fast as the demonstrations. Second,
existing methods usually require the demonstrations of com-
plete motion trajectories or at least a continuous segment of
states-inputs, making it challenging in data collection for high-
dimensional and long-horizon tasks. Third, existing IOC/IRL
may not be efÔ¨Åcient when handling high-dimensional contin-

1The literature review here mainly focuses on model-based IRL methods.

Keyframe#1attimeùíïùüèGoalObstacleObstacleStart1Keyframe#2attimeùíïùüêKeyframe#3attimeùíïùüëObstacleNewstart2GeneralizationWeallowthedemonstratedkeyframetimeùë°$tobedifferentfromrobotexecutiontime.Producedbythelearnedcostfunction 
 
 
 
 
 
This is a preprint. The published version can be accessed at IEEE Transactions on Robotics.

2

uous systems/tasks or learning complex objective functions,
such as deep neural network objective functions.

This paper develops the Continuous Pontryagin Differen-
tiable Programming method, abbreviated as the Continuous
PDP, to address the existing challenges. The method requires
only a few keyframes demonstrated at sparse time instances,
and it learns both an objective function and a time-warping
function, which accounts for the time misalignment between
demonstration and robot actual execution. The Continuous
PDP minimizes a discrepancy loss between the robot re-
produced motion and the given keyframes via the projected
gradient descent. This is done by efÔ¨Åciently computing the
analytical gradient of the robot trajectory with respect to tun-
able parameters in the objective and time-warping functions.
The highlights of the Continuous PDP are listed as follows.

(i) It requires as input the keyframe demonstrations, deÔ¨Åned
as a small number of sparse desired task-space outputs,
which the robot is expected to follow sequentially, as in
Fig. 1.

(ii) As the time stamp of each keyframe may not correctly
reÔ¨Çect the time of robot execution, in addition to learning
an objective function, the method jointly searches for
a time-warping function, which accounts for the time
misalignment between keyframes and robot execution.

(iii) The method can efÔ¨Åciently handle continuous-time high-
dimensional systems and accepts any differentiable pa-
rameterization of objective functions.

A. Related Work

Since the theme of this paper belongs to the category of
objective learning, in the following we mainly review IOC/IRL
methods. For other types of LfD, e.g., learning policies, please
refer to the recent surveys [1], [21].

1) Classic Strategies in IOC/IRL: Existing IOC/IRL meth-
ods can be categorized into two classes. The Ô¨Årst class adopts a
bi-level framework, where an objective function is updated on
an outer level while the corresponding reinforcement learning
(or optimal control) problem is solved on an inner level. Dif-
ferent methods in this class use different strategies to update
an objective function. Representative work includes feature-
matching IRL [10], where an objective function is updated to
match the feature values of the reproduced trajectory with the
ones of the demonstrations, max-margin IRL [14], [22], where
an objective function is updated by maximizing the margin
between the objective value of the reproduced trajectory and
that of demonstrations, and max-entropy IRL [15], which
Ô¨Ånds an objective function such that the trajectory distribution
has maximum entropy while subject to the empirical feature
values. The second class of IOC/IRL [17]‚Äì[19], [23], [24]
directly solves for objective function parameters by establish-
ing the optimality conditions, such as KKT conditions [25] or
Pontryagin‚Äôs Maximum Principle [26], [27]. The key idea is
that a demonstration is assumed to be optimal and thus must
satisfy the optimality condition. By directly minimizing the
violation of the optimality conditions by demonstration data,
one can compute the objective function parameters.

2) IOC with Trajectory Loss: One type of bi-level IOC/IRL
formulation also uses a trajectory loss as its learning criterion.
A trajectory loss is to evaluate the discrepancy between the
demonstrations and the robot motion reproduced by the ob-
jective function estimate. For example, [16] and [28] develop
a bi-level IOC approach which learns an objective function
from human locomotion data. In their work, the trajectory loss
is minimized via a derivative-free technique [29], where the
key is to approximate the loss using a quadratic function. The
approach requires solving optimal control problems multiple
times at each update, thus is computationally expensive. Fur-
ther, the derivative-free methods are known to be challenging
for the problem of large size [30]. In [31],
the authors
convert a bi-level IOC to a plain optimization by replacing
the lower-level optimal control problem with its optimality
conditions (the Pontryagin‚Äôs Maximum Principle). Although
the converted plain optimization can be solved by an off-
the-shelf nonlinear optimization solver, the decision variables
of the plain optimization include both objective parameters
and system trajectory (and dual variables); thus dramatically
increasing the size of the optimization. Besides, both lines of
methods have not considered the time misalignment between
demonstrations and robot execution.

Compared to the derivative-free methods in [16], [28], the
proposed Continuous PDP solves IOC/IRL by directly com-
puting the analytical gradient of a trajectory loss with respect
to tunable parameters in an objective function and a time-
warping function, thus is capable of solving high-dimensional
continuous tasks. Compared to [31],
the Continuous PDP
maintains the bi-level hierarchy of the problem and solves
IOC by differentiating through the inner-level optimal control
system. Maintaining a bi-level structure enables us to treat
the outer and inner level subproblems separately, avoiding the
mixed treatment that can lead to a dramatic increase in the size
of optimization. In Section V-E3, we provide the comparison
between the Continuous PDP and [31].

3) IOC/IRL via differentiable through inner-level optimiza-
tion: The recent work focuses on solving bi-level IOC/IRL
by differentiating through inner-level optimization. E.g., [32]
learns a cost function from visual demonstrations by differenti-
ating through the inner-level MPC. SpeciÔ¨Åcally, those methods
treat the inner-level optimization as an unrolling computational
graph of repetitively applying gradient descent, such that the
automatic differentiation [33] can be applied. However, as
shown in [34] and [35], auto-differentiating an ‚Äòunrolling‚Äô
graph has the following drawback: (i) it needs to store all
intermediate results along with the graph, thus is memory-
expensive; and (ii) the accuracy of the unrolling differentiation
depends on the length of the ‚Äòunrolled‚Äô graph, thus facing a
trade-off between complexity and accuracy. In contrast, the
Continuous PDP computes the gradient directly on the optimal
trajectory produced from the inner level, without memorizing
how this inner-level solution is obtained. Thus, there are no
above challenges for the proposed method.

4) Time-warping: Using time-warping functions to model
the time misalignment between two temporal sequences has
been extensively studied in signal processing [36] and pattern
recognition [37]. In [38], [39], time-warping is used in LfD

This is a preprint. The published version can be accessed at IEEE Transactions on Robotics.

3

for learning and producing robot trajectories. In [20], a time-
warping function between robot and demonstrator is learned
for optimal tracking. All the above methods focus on learning
policy or trajectory models instead of objective functions. For
time-misalignment in IOC/IRL, a main technical challenge is
how to incorporate the search of a time-warping function into
the objective learning process. The Continuous PDP addresses
this challenge by Ô¨Ånding an objective function and a time-
warping function simultaneously using gradient descent.

5) Incomplete Trajectory or Sparse Waypoints: Some meth-
ods focus on learning from incomplete trajectories. In [23],
[40], the authors develop a method to solve IOC with trajectory
segments. It requires the length of a segment to satisfy a re-
covery condition and cannot directly learn from sparse points.
[41], [42] consider learning from a set of sparse waypoints, but
they learn a kinematic model instead of an objective function.
Compared to those methods, the proposed method learns an
objective function and a time-warping function from a small
set of time-stamped sparse keyframes, i.e., a few desired task-
space outputs. In Section V-E1, we will provide a comparison
with [41].

6) Sensitivity Analysis and Continuous PDP: The idea of
the Continuous PDP is similar to the well-known sensitivity
analysis [43], [44] in nonlinear optimization, where the KKT
conditions are differentiated to obtain the gradient of a solution
with respect to the objective function parameters. In sensitivity
analysis, it requires to compute the inverse of the Hessian
matrix in order to apply the well-known implicit function
theorem [45]. If trying to apply the sensitivity analysis to a
continuous-time optimal control problem in our formulation,
we may face the following challenge. Since the optimality
condition of a continuous-time optimal control problem is
Pontryagin‚Äôs Maximum Principle [26], which is a set of ODE
equations. To apply the sensitivity analysis, one would need
to Ô¨Årst discretize the continuous-time system, and this will
lead to a Hessian matrix of the size at least T
‚àÜt (T is
the time horizon, and ‚àÜt is the discretization interval); this
will cause huge computation cost when taking its inverse (the
complexity is at least O(( T
‚àÜt )2)). The reason why we do not
formulate the problem in discrete-time in the Ô¨Årst place is that
otherwise, learning a discrete time-warping function will lead
the problem to a mixed-integer optimization, which becomes
more challenging to attack.

‚àÜt √ó T

Compared to sensitivity analysis, the Continuous PDP has
the following new technical aspects. First, it directly differen-
tiates the ODE equations in Pontryagin‚Äôs Maximum Principle
[26], producing Differential Pontryagin‚Äôs Maximum Principle;
and second, importantly, it develops Riccati-type equations
to solve the Differential Pontryagin‚Äôs Maximum Principle to
obtain the trajectory gradient (Lemma 1). The complexity of
this process is only O(T ). The Continuous PDP is an extension
of our previous work Pontryagin Differentiable Programming
(PDP) [34], [46] into the continuous-time systems. For a more
detailed comparison between PDP and the sensitivity analysis,
we refer the reader to [34], [46].

The following paper is organized as follows: Section II sets
up the problem. Section III reformulates the problem using
time-warping techniques. Section IV proposes the Continuous

PDP method. Experiments are given in Sections V and VI.
Section VII presents discussion, and Section VIII draws con-
clusions.

II. PROBLEM FORMULATION

Consider a robot with the following continuous dynamics:

Àôx(t) = f (x(t), u(t)) with x(0),

(1)

where x(t) ‚àà Rn is the robot state; u(t) ‚àà Rm is the control
input; vector function f : Rn √ó Rm (cid:55)‚Üí Rn is assumed to be
twice-differentiable, and t ‚àà [0, ‚àû) is time. Suppose the robot
motion over a time horizon tf > 0 is controlled by minimizing
the following parameterized cost function:

J(p) =

(cid:90) tf

0

c(x(t), u(t), p)dt + h(x(tf ), p),

(2)

where c(x, u, p) and h(x, p) are the running and Ô¨Ånal costs,
respectively, both of which are assumed twice-differentiable;
and p ‚àà Rr is a tunable parameter vector. For a Ô¨Åxed choice
of p, the robot produces a trajectory of states and inputs

Œæp = {Œæp(t) | 0 ‚â§ t ‚â§ tf } with Œæp(t) = {xp(t), up(t)}. (3)

which minimizes (2) subject to (1). The subscript in Œæp means
that the trajectory implicitly depends on p.

The goal of learning from demonstrations is to estimate
the cost function parameter p from the given demonstrations
by a user (usually a human). Suppose that a user provides
demonstrations in a task space (e.g., Cartesian space or vision
measurement), which is a known differentiable mapping of the
robot state-input pair:

y = g(x, u),

(4)

where g : Rn √ó Rm ‚Üí Ro deÔ¨Ånes a mapping from the robot
state-input to a task output y ‚àà Ro. The user‚Äôs demonstrations
include (i) an expected time horizon T , and (ii) a number of
N keyframes, each of which is a desired output labeled with
an expected time stamp œÑi, denoted as

D = {y‚àó(œÑi) | œÑi ‚àà [0, T ], i = 1, 2, ¬∑ ¬∑ ¬∑ , N }.

(5)

Here, y‚àó(œÑi) is the ith keyframe demonstrated by the user,
and œÑi is the expected time stamp at which the user wants the
robot to reach y‚àó(œÑi). The keyframe time {œÑ1, œÑ2, ..., œÑN } can
be sparsely located within range [0, T ]. As the user can freely
choose N and œÑi relative to the expected horizon T , we call D
as keyframes. As shown later in experiments, N can be small.
Note that both the expected horizon T and the expected
time stamps œÑi are in the time axis of the user‚Äôs demonstration.
This demonstration time axis may not be identical to the actual
time axis of robot execution; in other words, T and œÑi may
not be achievable by the robot. For example, when the robot is
actuated by a weak servo motor, its motion inherently cannot
meet œÑi. To accommodate the time misalignment between the
robot execution and keyframes, we introduce a time warping
function:

t = w(œÑ ),

(6)

This is a preprint. The published version can be accessed at IEEE Transactions on Robotics.

4

which maps from keyframe time œÑ to robot time t. We make
the following reasonable assumption: w is strictly increasing
in the range [0, T ], continuously differentiable, and w(0) = 0.
Given the keyframes D, the problem of interest is to Ô¨Ånd
cost function parameter p and a time-warping function w(¬∑)
such that the task discrepancy loss is minimized:
y‚àó(œÑi), g(cid:0)Œæp(w(œÑi))(cid:1)(cid:17)
(cid:16)

(cid:88)N

(7)

l

,

min
p,w

i=1

where l(a, b) is a given differentiable scalar function deÔ¨Åned
in the task space which quantiÔ¨Åes a distance metric between
vectors a and b, e.g., l(a, b) = (cid:107)a ‚àí b(cid:107)2. Minimizing (7)
means that we want the robot to Ô¨Ånd cost function parameter
p and a time-warping function w(¬∑), such that its reproduced
trajectory gets as close to the given keyframes as possible.

III. PROBLEM REFORMULATION BASED ON
TIME-WARPING TECHNIQUES

In this section, we re-formulate the problem of interest using

the time-warping techniques.

A. Parametric Time Warping Function

To facilitate learning of an unknown time-warping function,
we Ô¨Årst parameterize the time-warping function. Recall that a
differentiable time-warping function w(œÑ ) satisÔ¨Åes w(0) = 0
and is strictly increasing in the range [0, T ], i.e.,

v(œÑ ) =

dw(œÑ )
dœÑ

> 0

(8)

for all œÑ ‚àà [0, T ]. We use a polynomial time-warping function:

t = wŒ≤(œÑ ) =

(cid:88)s

i=1

Œ≤iœÑ i,

(9)

where Œ≤ = [Œ≤1, Œ≤2, ¬∑ ¬∑ ¬∑ , Œ≤s]T ‚àà Rs is the coefÔ¨Åcient vector.
Since wŒ≤(0) = 0, there is no constant (zero-order) term in (9)
(i.e., Œ≤0 = 0). Due to the requirement of dwŒ≤/dœÑ = vŒ≤(œÑ ) > 0
for all œÑ ‚àà [0, T ], one can obtain a feasible set, denoted as ‚Ñ¶Œ≤,
such that dwŒ≤(œÑ )
dœÑ > 0 for all œÑ ‚àà [0, T ] if Œ≤ ‚àà ‚Ñ¶Œ≤. The choice
of polynomial degree s will decide the representation power
of (8): larger s means that wŒ≤(œÑ ) can represent more complex
time warping curves. Note that although we use a polynomial
time-warping function, the method in this paper allows for
more general parameterization of a time-warping function, as
long as it is differentiable. This paper uses polynomial time-
warping functions due to the simplicity for implementation.

B. Equivalent Formulation by Time Warping

Substituting the parametric time-warping function wŒ≤ in (9)
into both the robot dynamics (1) and cost function (2), we
obtain the following time-warped dynamics

dx
dœÑ

=

(cid:16)

f

dwŒ≤
dœÑ

x(wŒ≤(œÑ )), u(wŒ≤(œÑ ))

(cid:17)

with x(0),

(10)

and the time-warped cost function

(cid:90) T

J(p, Œ≤) =

dwŒ≤
dœÑ

0
+ hp(x(wŒ≤(T ))).

cp(x(wŒ≤(œÑ )), u(wŒ≤(œÑ )))dœÑ

Here, the left side of (10) is due to chain rule: dx
dœÑ , and
the time horizon satisÔ¨Åes tf = wŒ≤(T ) (note that T is speciÔ¨Åed
by the demonstrator). For notation simplicity, we write dwŒ≤
dœÑ =
vŒ≤(œÑ ), x(w(œÑ )) = x(œÑ ), u(w(œÑ )) = u(œÑ ), and dx
dœÑ = Àôx(œÑ ).
Then, the above time-warped dynamics (10) and time-warped
cost function (11) are rewritten as:

dœÑ = Àôx dt

Àôx(œÑ ) = vŒ≤(œÑ )f (x(œÑ ), u(œÑ )) with x(0)

(12a)

and

J(p, Œ≤)=

(cid:90) T

0

vŒ≤(œÑ )c(x(œÑ ), u(œÑ ), p)dœÑ + h(x(T ), p), (12b)

respectively. We pack the tunable cost parameter p and time-
warping parameter Œ≤ together as

Œ∏ = [pT, Œ≤T]T ‚àà Rr+s.

(13)

For a Ô¨Åxed Œ∏, the optimal trajectory from solving the above
time-warped optimal control system (12) is rewritten as

ŒæŒ∏ = {ŒæŒ∏(œÑ ) | 0 ‚â§ œÑ ‚â§ T },

(14)

with ŒæŒ∏(œÑ ) = {xŒ∏(œÑ ), uŒ∏(œÑ )}. The discrepancy loss (7) can
now be deÔ¨Åned as

L(ŒæŒ∏, D) =

N
(cid:88)

i=1

(cid:16)

y‚àó(œÑi), g(cid:0)ŒæŒ∏(œÑi)(cid:1)(cid:17)

.

l

(15)

Minimizing (15) over Œ∏ is a process of simultaneously search-
ing for a cost function J(p) and time-warping function wŒ≤(œÑ ).
In sum, the problem of interest is now reformulated as the
following optimization

min
Œ∏‚ààŒò

L(ŒæŒ∏, D)

(16)

s.t. ŒæŒ∏ is from the optimal control system (12).
Here Œò deÔ¨Ånes a feasible set of Œ∏, Œò = Rr √ó ‚Ñ¶Œ≤. (16) is a
bi-level optimization, where the upper level is to minimize a
discrepancy loss between the keyframes D and the reproduced
time-warped trajectory ŒæŒ∏, and the inner level is to generate
such ŒæŒ∏ by solving the optimal control problem (12). In
the next section, we will develop the Continuous Pontryagin
Differentiable Programming to efÔ¨Åciently solve (16).

IV. CONTINUOUS PONTRYAGIN DIFFERENTIABLE
PROGRAMMING

A. Algorithm Overview

To solve the optimization (16), we start with an arbitrary

initial guess Œ∏0 ‚àà Œò, and apply the gradient descent

(cid:18)

Œ∏k+1 = ProjŒò

Œ∏k ‚àí Œ∑k

(cid:19)

,

dL
dŒ∏

(cid:12)
(cid:12)
(cid:12)Œ∏k

(17)

where k is the iteration index; Œ∑k is the step size (or learning
rate); ProjŒò is a projection operator to enforce the feasibility
(cid:12)
of Œ∏k in Œò, e.g., ProjŒò(Œ∏) = arg minz‚ààŒò(cid:107)Œ∏ ‚àíz(cid:107); and dL
(cid:12)Œ∏k
dŒ∏
denotes the gradient of the loss (15) directly with respect to
Œ∏ evaluated at Œ∏k. Applying the chain rule, we have

(11)

dL
dŒ∏

(cid:12)
(cid:12)
(cid:12)Œ∏k

=

N
(cid:88)

i=1

‚àÇl
‚àÇŒæŒ∏(œÑi)

(cid:12)
(cid:12)
(cid:12)ŒæŒ∏k

‚àÇŒæŒ∏(œÑi)
‚àÇŒ∏

(cid:12)
(cid:12)
(cid:12)Œ∏k

,

(œÑi)

(18)

This is a preprint. The published version can be accessed at IEEE Transactions on Robotics.

5

(cid:12)
(cid:12)Œ∏k

where

‚àÇl
‚àÇŒæŒ∏ (œÑi)
(cid:16)

(cid:12)
(cid:12)ŒæŒ∏k
y‚àó(œÑi), g(cid:0)ŒæŒ∏(œÑi)(cid:1)(cid:17)

loss l =
time-œÑi trajectory point ŒæŒ∏(œÑi), evaluated at value ŒæŒ∏k
‚àÇŒæŒ∏ (œÑi)
‚àÇŒ∏

(œÑi) is the gradient of the single keyframe
in (15) with respect to the
(œÑi), and
is the gradient of the time-œÑi trajectory point ŒæŒ∏(œÑi),
with respect to Œ∏, evaluated at value Œ∏k. From (17) and (18),
we can draw the computational diagram in Fig. 2. Fig. 2 shows
that at each iteration k, the update of Œ∏k includes the following
three steps:
Step 1: Obtain the optimal trajectory ŒæŒ∏k

by solving the
optimal control (trajectory optimization) problem (12)
with current Œ∏k;

Step 2: Compute the gradient

‚àÇl
‚àÇŒæŒ∏ (œÑi)

Step 3: Compute the gradient ‚àÇŒæŒ∏ (œÑi)

‚àÇŒ∏

(œÑi);

(cid:12)
(cid:12)ŒæŒ∏k
(cid:12)
;
(cid:12)Œ∏k

Step 4: Apply chain rule (18) to compute dL
dŒ∏

Œ∏k using (17) to Œ∏k+1.

(cid:12)
(cid:12)Œ∏k

, and update

Fig. 2: Computational diagram of the Continuous Pontryagin
Differentiable Programming.

The interpretation of the above procedure is straightforward.
At each update k, the Ô¨Årst step is to use the current parameter
Œ∏k to compute the current optimal trajectory ŒæŒ∏k by solving
the optimal control problem (12). In Step 2 and Step 3,
the gradient of the loss with respect to the trajectory point,
‚àÇl
(œÑi), and the gradient of the trajectory point with
‚àÇŒæŒ∏ (œÑi)
(cid:12)
respect to parameters, ‚àÇŒæŒ∏ (œÑi)
, are computed, respectively.
(cid:12)Œ∏k
In Step 4, the total gradient of the loss with respect to the
parameter, dL
, is assembled via chain rule (18), and then
dŒ∏
used to update Œ∏k by the projected gradient descent (17).

(cid:12)
(cid:12)ŒæŒ∏k

(cid:12)
(cid:12)Œ∏k

‚àÇŒ∏

‚àÇL

In Step 1, the optimal trajectory ŒæŒ∏k can be solved by
available optimal control (trajectory optimization) solvers such
as iLQR [47], DDP [48], Casadi [49], GPOPS [50], etc.
‚àÇŒæŒ∏ (œÑi) can be readily computed by
In Step 2, the gradient
directly differentiating the given loss (15). The main challenge,
however, lies in Step 3, i.e., computing ‚àÇŒæŒ∏
, the gradient
‚àÇŒ∏
of the optimal trajectory ŒæŒ∏ with respect to the parameter Œ∏
of the optimal control system (12). In what follows, we will
efÔ¨Åciently solve it by proposing the technique of Differential
Pontryagin‚Äôs Maximum Principle. For notation simplicity, we
suppress the iteration index k below.

(cid:12)
(cid:12)Œ∏k

if ŒæŒ∏ satisÔ¨Åes the second-order sufÔ¨Åcient condition, that is, Œ∏
is a locally unique optimal trajectory (see Lemma 1 in [46]).
Both our later experiments and previous empirical results [34],
[51] show that the differentiability condition is very mild. For
more detailed results about the differentiability for a general
optimal control system with respect to system parameters, we
refer the reader to [46].

Consider an optimal trajectory ŒæŒ∏ in (14) produced by an
optimal control system (12) with a Ô¨Åxed Œ∏. The Pontryagin‚Äôs
Maximum Principle [26] states a set of ODE conditions that ŒæŒ∏
must satisfy. To present the Pontryagin‚Äôs Maximum Principle,
deÔ¨Åne the Hamiltonian [52]:

H(œÑ ) = vŒ≤(œÑ )cp(x(œÑ ), u(œÑ ))

(19)
where Œª(œÑ ) ‚àà Rn is called the costate, 0 ‚â§ œÑ ‚â§ T . According
to the Pontryagin‚Äôs Maximum Principle [26], there exists

+ Œª(œÑ )TvŒ≤(œÑ )f (x(œÑ ), u(œÑ )),

{ŒªŒ∏(œÑ ) | 0 ‚â§ œÑ ‚â§ T },

(20)

ÀôxŒ∏(œÑ ) =

(xŒ∏(œÑ ), uŒ∏(œÑ ), ŒªŒ∏(œÑ )),

associated with the optimal trajectory ŒæŒ∏ in (14), such that the
following ODE equations hold [26]:
‚àÇH
‚àÇŒªŒ∏
‚àÇH
‚àÇx
‚àÇH
‚àÇu
‚àÇhp
‚àÇx

(xŒ∏(œÑ ), uŒ∏(œÑ ), ŒªŒ∏(œÑ )),

(xŒ∏(œÑ ), uŒ∏(œÑ ), ŒªŒ∏(œÑ )),

and xŒ∏(0) = x(0).

‚àí ÀôŒªŒ∏(œÑ ) =

ŒªŒ∏(T ) =

(xŒ∏(T ))

(21d)

(21b)

(21c)

(21a)

0 =

Here, (21a) is the dynamics; (21b) is the costate ODE; (21c)
is the input ODE, and (21d) is the boundary conditions. Given
ŒæŒ∏, one can always solve the corresponding {ŒªŒ∏(œÑ ) | 0 ‚â§ œÑ ‚â§
T } by integrating the costate ODE in (21b) backward in time
with the boundary condition in (21d).

Recall that our technical challenge is to obtain the gradient
‚àÇŒæŒ∏
‚àÇŒ∏ . Towards this goal, we differentiate the above Pontryagin‚Äôs
Maximum Principle in (21) on both sides with respect to
the system parameter Œ∏, yielding the following Differential
Pontryagin‚Äôs Maximum Principle:

d
dœÑ
d
dœÑ

(

(

‚àÇxŒ∏
‚àÇŒ∏
‚àÇŒªŒ∏
‚àÇŒ∏

‚àí

) = F (œÑ )

‚àÇxŒ∏
‚àÇŒ∏

+ G(œÑ )

‚àÇuŒ∏
‚àÇŒ∏

+ E(œÑ ),

(22a)

) = Hxx(œÑ )

0 = Hux(œÑ )

‚àÇxŒ∏
‚àÇŒ∏

‚àÇxŒ∏
‚àÇŒ∏

‚àÇuŒ∏
+ Hxu(œÑ )
‚àÇŒ∏
+ F (œÑ )T ‚àÇŒªŒ∏
‚àÇŒ∏

‚àÇuŒ∏
+ Huu(œÑ )
‚àÇŒ∏
+ G(œÑ )T ‚àÇŒªŒ∏
‚àÇŒ∏

+ Hxe(œÑ ),

+ Hue(œÑ ),

(22b)

(22c)

B. Differential Pontryagin‚Äôs Maximum Principle

In this section, we focus on efÔ¨Åciently solving the analytical
gradient of a trajectory of a continuous-time optimal control
system with respect to the system parameter. We assume that
the resulting optimal trajectory ŒæŒ∏ in (14) is differentiable with
respect to the system parameter Œ∏. This assumption is satisÔ¨Åed

‚àÇŒªŒ∏
‚àÇŒ∏

(T ) = Hxx(T )

‚àÇxŒ∏
‚àÇŒ∏

+ Hxe(T )

and

‚àÇxŒ∏
‚àÇŒ∏

(0) = 0.

(22d)

The coefÔ¨Åcient matrices in the above (22) are deÔ¨Åned as

F (œÑ )=

‚àÇ2H
‚àÇŒªŒ∏‚àÇxŒ∏

, G(œÑ )=

‚àÇ2H
‚àÇŒªŒ∏‚àÇuŒ∏

, E(œÑ )=

‚àÇ2H
‚àÇŒªŒ∏‚àÇŒ∏

,

(23a)

Optimal control system in (12) parameterized by ùúΩLossDifferential Pontryagin‚Äôs Maximum Principle (Lemma 1) Chain ruleUpdatetrajectory!!"#=Proj$!!‚àí(!)*)!+%!ùëëùêøùëëùúΩùëëùùÉùúΩ(ùúè)ùëëùúΩùëëùëôùëëùùÉùúΩ(ùúè)ùêø(ùúâ!,ùíü)ùùÉùúΩùúΩThis is a preprint. The published version can be accessed at IEEE Transactions on Robotics.

6

Hxx(œÑ )=

‚àÇ2H
(‚àÇxŒ∏)2 , Hxu(œÑ )=

‚àÇ2H
‚àÇxŒ∏‚àÇuŒ∏

, Hxe(œÑ )=

‚àÇ2H
‚àÇxŒ∏‚àÇŒ∏

,

(23b)

Hux(œÑ )=HT

xu(œÑ ), Huu(œÑ )=

Hxx(T )=

‚àÇ2hp
(‚àÇxŒ∏)2 , Hxe(T )=

‚àÇ2H
(‚àÇuŒ∏)2 , Hue(œÑ )=
‚àÇ2hp
‚àÇxŒ∏‚àÇŒ∏

.

‚àÇ2H
‚àÇuŒ∏‚àÇŒ∏

, (23c)

(23d)

Once we obtain the optimal trajectory ŒæŒ∏ and the associated
costate trajectory {ŒªŒ∏(œÑ ) | 0 ‚â§ œÑ ‚â§ T } in (20), all the above
coefÔ¨Åcient matrices in (23) are known and their computation is
straightforward. Given the Differential Pontryagin‚Äôs Maximum
Principle in (22), one can observe that these ODEs have a
similar form to the original Pontryagin‚Äôs Maximum Principle
‚àÇŒ∏ as a new state variable, ‚àÇuŒ∏
in (21). Thus, if one thinks of ‚àÇxŒ∏
‚àÇŒ∏
as a new control variable, and ‚àÇŒªŒ∏
‚àÇŒ∏ as a new costate variable,
then the Differential Pontryagin‚Äôs Maximum Principle in (22)
can be thought of as the Pontryagin‚Äôs Maximum Principle of
a new LQR system, as investigated in [34], [46]. By deriving
the equivalent Raccati-type equations, the lemma below gives
an efÔ¨Åcient way to compute the trajectory gradient ‚àÇŒæŒ∏ (œÑ )
,
0 ‚â§ œÑ ‚â§ T , from the above (22).

‚àÇŒ∏

Lemma 1. If Huu(œÑ ) in (23c) is invertible for all 0 ‚â§ œÑ ‚â§ T ,
deÔ¨Åne the following differential equations for matrix variables
P (œÑ ) ‚àà Rn√ón and W (œÑ ) ‚àà Rn√ó(r+s):

‚àí ÀôP = Q(œÑ ) + A(œÑ )TP + P A(œÑ ) ‚àí P R(œÑ )P,
ÀôW = P R(œÑ )W ‚àí A(œÑ )TW ‚àí P M (œÑ ) ‚àí N (œÑ ),

(24a)

(24b)

with P (T ) = Hxx(T ) and W (T ) = Hxe(T ). Here,

A(œÑ ) = F ‚àí G(Huu)‚àí1Hux,
R(œÑ ) = G(Huu)‚àí1GT,
M (œÑ ) = E ‚àí G(Huu)‚àí1Hue,
Q(œÑ ) = Hxx ‚àí Hxu(Huu)‚àí1Hux,
N (œÑ ) = Hxe ‚àí Hxu(Huu)‚àí1Hue,

(25a)

(25b)

(25c)

(25d)

(25e)

the Differential Pontryagin‚Äôs Maximum Principle, Lemma 1
gives an efÔ¨Åcient way to compute the gradient of an optimal
trajectory with respect to the parameter in an optimal control
system. By Lemma 1, one can obtain the derivative of the
trajectory point ŒæŒ∏(œÑ ), at any time 0 ‚â§ œÑ ‚â§ T , with respect
to the system parameter Œ∏, i.e., ‚àÇŒæŒ∏

‚àÇŒ∏ (œÑ ).

‚àÇuŒ∏ ‚àÇuŒ∏

Additionally, we have the following comments on Lemma 1.
First, (24) are Riccati-type equations, which are derived from
Differential Pontryagin‚Äôs Maximum Principle in (22). Second,
Lemma 1 requires the matrix Huu(œÑ )= ‚àÇ2H
in (23c) to be
invertible, this is in fact a necessary condition [46] for the
differentiability of ŒæŒ∏. As we have mentioned at the beginning
of this subsection, if ŒæŒ∏ satisÔ¨Åes the second-order sufÔ¨Åcient
condition (i.e.,
trajectory) for
is a locally unique optimal
the optimal control problem (12), then ŒæŒ∏ is differentiable
in Œ∏ and Huu(œÑ ) is automatically invertible (see [46] for
the details and proofs). A similar invertiblility requirement
is common in sensitivity analysis methods [43], [44], where
they analogously requires the Hessian matrix to be invertible
in order to apply the implicit function theorem [45]. Both our
later experiments and other related existing work [34], [35],
[46] have empirically shown that the invertibility of Huu(œÑ )
is a mild condition and could be easily satisÔ¨Åed. With Lemma
1, we summarize the overall algorithm of the Continuous PDP
in Algorithm 1.

Algorithm 1: Learning from sparse demonstrations.
Input: keyframes D in (5) and learning rate {Œ∑k}.
Initialization: initial parameter guess Œ∏0,
for k = 0, 1, 2, ¬∑ ¬∑ ¬∑ do

by solving the optimal

Obtain the optimal trajectory ŒæŒ∏k
control problem in (12) with current Œ∏k;
Obtain the costate trajectory {ŒªŒ∏k (œÑ )} by integrating
(21b) given (21d);
Compute ‚àÇŒæŒ∏ (œÑi)

using Lemma 1 for i = 1, 2, ¬∑ ¬∑ ¬∑ N ;

‚àÇŒ∏

(cid:12)
(cid:12)Œ∏k
(cid:12)
(cid:12)ŒæŒ∏k

Compute

‚àÇl
‚àÇŒæŒ∏ (œÑi)

(œÑi) from (15);

Obtain dL

dŒ∏ |Œ∏k using the chain rule (18);
(cid:16)

Update Œ∏k+1 ‚Üê ProjŒò

Œ∏k ‚àí Œ∑k

dL
dŒ∏

(cid:12)
(cid:12)Œ∏k

(cid:17)

;

are all known given (23). The gradient of the optimal trajec-
tory ŒæŒ∏, denoted as

‚àÇŒæŒ∏(œÑ )
‚àÇŒ∏

=

(cid:18) ‚àÇxŒ∏
‚àÇŒ∏

(œÑ ),

‚àÇuŒ∏
‚àÇŒ∏

(cid:19)

(œÑ )

,

0 ‚â§ œÑ ‚â§ T

(26)

end

is obtained by integrating the following ODEs up to œÑ :

‚àÇuŒ∏
‚àÇŒ∏

= ‚àí (Huu(œÑ ))‚àí1(cid:16)

‚àÇxŒ∏
‚àÇŒ∏
+ G(œÑ )TW (œÑ ) + G(œÑ )TP (œÑ )

Hux(œÑ )

(œÑ ) + Hue(œÑ )

(cid:17)

,

(œÑ )

(27a)

‚àÇxŒ∏
‚àÇŒ∏

(cid:19)

d
dœÑ

(cid:18) ‚àÇxŒ∏
‚àÇŒ∏

=F (œÑ )

‚àÇxŒ∏
‚àÇŒ∏

(œÑ ) + G(œÑ )

‚àÇuŒ∏
‚àÇŒ∏

(œÑ ) + E(œÑ ),

(27b)

with ‚àÇxŒ∏
are solutions to (24a) and (24b), respectively.

‚àÇŒ∏ (0) = 0 in (22d). Here, the matrices P (œÑ ) and W (œÑ )

The proof of Lemma 1 is given in Appendix. Lemma 1 states
that for the optimal control system (12), the gradient of its
optimal trajectory ŒæŒ∏ with respect to the system parameter Œ∏
can be obtained in two steps: Ô¨Årst, integrate (24) backward in
time to obtain P (œÑ ) and W (œÑ ) for 0 ‚â§ œÑ ‚â§ T ; and second,
obtain ‚àÇŒæŒ∏
‚àÇŒ∏ (œÑ ) by integrating (27) forward in time. Based on

V. NUMERICAL EXPERIMENTS

In this section, we evaluate different aspects of the proposed
method using a two-link robot arm performing reaching tasks.
The dynamics of a robot arm (moving horizontally) is [53]

M (q)¬®q + c(q, Àôq) = œÑ ,

(28)

where M (q) ‚àà R2√ó2 is the inertia matrix, c(q, Àôq) ‚àà R2 is the
Coriolis term; q = [q1, q2]T ‚àà R2 is the joint angle vector,
and œÑ = [œÑ1, œÑ2]T ‚àà R2 is the joint toque vector. The physical
parameters for the dynamics are: m1=2kg and m2=1kg for
the mass of each link; l1=1m and l2=1m for the length of each
link (assume mass is evenly distributed). The state and control
vectors are x = [q, Àôq]T ‚àà R4 and u = œÑ ‚àà R2, respectively.

This is a preprint. The published version can be accessed at IEEE Transactions on Robotics.

7

For the task of reaching to a goal state xg = [qg

1, qg

2, 0, 0]T ‚àà

R4, we set the cost function (2) as
c(x, u, p) =p1(q1 ‚àí qg

1)2 + p2(q2 ‚àí qg
2 + 0.5(cid:107)u(cid:107)2,

2)2+

p3 Àôq2

1 + p4 Àôq2

1+p4 Àôq2

2)2 + p3 Àôq2

h(x, p) =p1(q1‚àíqg

(29a)
1)2+p2(q2‚àíqg
2. (29b)
with the tunable parameter p = [p1, p2, p3, p4]T ‚àà R4. Note
that (29) is a weighted distance-to-goal function with a Ô¨Åxed
weight to (cid:107)u(cid:107)2, because otherwise, learning all weights will
lead to scaling ambiguity [23]. We set the goal state xg =
2 , 0, 0, 0]T, and the initial state x(0) = [‚àí œÄ
[ œÄ
4 , ‚àí5, 3]T.
For parametric time-warping function (9), we simply use

2 , 3œÄ

t = wŒ≤(œÑ ) = Œ≤œÑ,

(30)

with ‚Ñ¶Œ≤ = {Œ≤ | Œ≤ > 0} (more complex time-warping functions
will be used later). The overall parameter to be tuned is Œ∏ =
[pT, Œ≤]T ‚àà R5. The task-space mapping (4) is

q = g(x, u),

(31)

meaning that the keyframe only includes the position infor-
mation. For the discrepancy loss (15), we use the squared l2
norm:

L(ŒæŒ∏, D) =

(cid:88)N

i=1

(cid:107)q‚àó(œÑi) ‚àí g(cid:0)ŒæŒ∏(œÑi)(cid:1)(cid:107)2.

(32)

In the following experiments, we evaluate different aspects of
the method and provide analysis for each evaluation.

A. Different Number of Keyframes

First, we evaluate the performance of the proposed method
for learning from different numbers of keyframes. D is gener-
ated from known/true cost and time-warping functions. Given

Œ∏true = [3, 3, 3, 3, 5]T,

(33)

the robot optimal trajectory is computed by solving the optimal
control problem (12), shown in Fig. 3. Then, we select some
points (red dots) from Fig. 3 as our keyframes D, listed in
Table I. We evaluate the performance of the proposed method
to recover Œ∏true given different numbers of the keyframes. The
learning rate is Œ∑ = 0.1, and the initial Œ∏0 is randomly given.
For each evaluation case, we have run the experiment for 10
trials with different random seeds for Œ∏0.

Fig. 3: Generating keyframes (marked as red dots) from an
optimal trajectory with Œ∏true. The gray dashed lines label the
goal pose for each joint, i.e., [qg

2]T = [œÄ/2, 0]T.

1, qg

We choose different numbers of keyframes from Table I to
learn the time-warping and cost functions, and the results are

TABLE I: Keyframes D generated in Fig. 3

No.

Time stamp œÑi (T = 1)

.

#1
#2
#3
#4
#5
#6
#7
#8

œÑ1 = 0.067s
œÑ2 = 0.2s
œÑ3 = 0.267s
œÑ4 = 0.333s
œÑ5 = 0.467s
œÑ6 = 0.6s
œÑ7 = 0.8s
œÑ8 = 0.933s

Keyframe y‚àó(œÑi)
q‚àó(œÑ1) = [‚àí2.497, 2.301]
q‚àó(œÑ2) = [‚àí1.71, 1.353]
q‚àó(œÑ3) = [‚àí1.142, 0.924]
q‚àó(œÑ4) = [‚àí0.629, 0.606]
q‚àó(œÑ5) = [0.201, 0.25]
q‚àó(œÑ6) = [0.791, 0.108]
q‚àó(œÑ7) = [1.319, 0.049]
q‚àó(œÑ8) = [1.512, 0.043]

in Fig. 4. The left panel of Fig. 4 shows the loss (32) versus
iteration, and the right shows the parameter error (cid:107)Œ∏ ‚àí Œ∏true(cid:107)2
versus iteration.

Fig. 4: Learning from different numbers of keyframes. The
left panel shows the loss (32) versus iteration, the right shows
the parameter error (cid:107)Œ∏k ‚àí Œ∏true(cid:107)2 versus iteration. The solid
line and shaded area denote the mean and standard derivation
over all 10 trials.

Fig. 4 shows that when the number of keyframes N ‚â• 3
, D) and parameter
(blue, green, and red lines), the loss L(ŒæŒ∏k
error (cid:107)Œ∏k ‚àí Œ∏true(cid:107)2 converge to zeros, indicating that both
the cost and time-warping functions are successfully learned.
When N ‚â§ 2, while the loss converges to zero, Œ∏k does
not converge to Œ∏true (orange and purple lines in the right
panel). This indicates when N ‚â§ 2, there are multiple cost
and time-warping functions, besides Œ∏true, that lead to the given
keyframes. In other words, with fewer keyframes, we cannot
uniquely determine the cost and time-warping functions, as
they are over-parameterized relative to given keyframes. Intu-
itively, to uniquely determine Œ∏true, the number of constraints
imposed by the given keyframes, oN (recall o is the dimension
of g()), should be no less than the number of all unknown
parameters, r+s, that is, N ‚â• r+s
o . Please refer to Section
VII-A for more analysis.

From the right panel of Fig. 4, we also observe that different
numbers of keyframes (N ‚â• 3) also inÔ¨Çuence the converge
rate. For instance, the convergence rate with 8 keyframes (red
line) is faster than that of 4 keyframes (blue line). Since the
proposed method updates the cost and time-warping functions
by Ô¨Ånding the deepest descent direction of loss, thus, the more
keyframes are given, the better informed the gradient direction
will be, making the convergence to the true parameters faster.
Lastly, we test the generalization of the learned cost and
time-warping functions, by setting the robot arm to new initial
state x(0) = [‚àí œÄ
4 , 0, 0, 0]T and new horizon T =2 (both are
very different from the ones in learning). The generated motion

0.000.250.500.751.002101q1qg10.000.250.500.751.000.00.51.01.52.0q2qg2Generation of keyframes02505007501000Iteration102410181012106100L(,)02505007501000Iteration102101100101||true||21 keyframe2 keyframes3 keyframes4 keyframes8 keyframesThis is a preprint. The published version can be accessed at IEEE Transactions on Robotics.

8

using the learned Œ∏ (mean value over all trials) is shown in
Fig. 5, where we have also plotted the trajectory of Œ∏true
for reference. To compare the generalization performance,
we compute the distance between the Ô¨Ånal state x(T ) of
the generalized motion and the goal state xg = [ œÄ
2 , 0, 0, 0]T,
and list the results in Table II. Both Fig. 5 and Table II
show that the learned Œ∏ enables to generate new motion in
unseen conditions. Further, Table II shows that the increasing
keyframes could lead to better generalization. Notably, we
see that although the learned Œ∏s from 1 or 2 keyframes are
different from Œ∏true, they can still obtain fair generalization.
This could be due to the formulation of the distance-to-goal
features (29). Although the learned weight vector Œ∏ is different
from Œ∏true, the distance-to-goal features largely contributes to
a similar performance. We will show later in Section V-D
that when (29) is replaced with a neural cost function, fewer
keyframes will lead to poor generalization. Thus, for the same
number of keyframes, different cost function formulations
could lead to different generalization abilities. But as we will
see in Section V-D, a common observation is that the more
keyframes are given, the better the generalization will be for
the learned cost function.

Fig. 5: Generalization of the learned cost function given new
initial condition x(0) and new horizon T . The gray dashed
lines mark the goal for each joint [qg

2]T = [œÄ/2, 0]T.

1, qg

TABLE II: Distance between the Ô¨Ånal state x(T ) of the
generalized motion and the goal xg, i.e., (cid:107)x(T ) ‚àí xg(cid:107).

The learned Œ∏ (mean value) from (cid:107)x(T ) ‚àí xg(cid:107)

1 keyframe
2 keyframes
3 keyframes
4 keyframes
8 keyframes
True Œ∏true

0.00581
0.00580
0.00392
0.00382
0.00358
0.00346

Fig. 6: Learning from non-optimal keyframes. The Ô¨Årst column
shows the given keyframes (red dots), which deviate from the
optimal trajectory (black lines), and the reproduced trajectory
(orange lines) from the learned Œ∏ (mean value over all 10
trials). The second column shows the loss and parameter error
versus iteration; the solid line and shaded area denote the
mean and standard derivation over all 10 trials. The third
column shows the generalization of the learned Œ∏ to new initial
condition x(0) and new time horizon T . The gray dashed lines
in the Ô¨Årst and third columns mark the goal for each joint
[qg
2]T = [œÄ/2, 0]T. As calculated in Table II, (cid:107)x(T ) ‚àí xg(cid:107)
for the generalized motion in the third column is 0.107.

1, qg

In Fig. 6, the loss and parameter error versus iteration are
shown in the top and bottom panels of the second column,
respectively. The solid line and shaded area denote the mean
and standard derivation over all 10 trials. We use the learned
Œ∏ to reproduce the optimal trajectory of the robot, which is
shown in the Ô¨Årst column (orange lines). In the third column,
we test the generalization of the learned Œ∏ to the new initial
condition x(0) = [‚àí œÄ
4 , 0, 0, 0]T and new horizon T = 2. Here,
we also compare with the trajectory of Œ∏true (dashed black
lines). From Fig. 6, we have the following comments.

Since the keyframes in the Ô¨Årst column are non-optimal,
there does not exist a Œ∏ that exactly corresponds to those non-
optimal keyframes. Thus, the loss in the second column does
not converge to zero. Despite those, the method still Ô¨Ånds a Œ∏
such that its produced trajectory is closest to the keyframes, as
shown by orange lines in the Ô¨Årst column. The second column
shows that the learned Œ∏ is different from Œ∏true.

The generalization in the third column shows that given the
new initial condition and horizon, the generalized motion still
approaches the goal, and the Ô¨Ånal distance of the generalized
motion to the goal is (cid:107)x(T ) ‚àí xg(cid:107) = 0.107, which is larger
compared to the one in Table II.

B. Non-optimal Keyframes

C. Different Time-Warping Functions

Next, we evaluate the performance of the proposed method
given non-optimal keyframes. This emulates the situation
where a demonstration could be polluted by biased sensing
error, noise, hardware error, etc. We select keyframes D by
corrupting each keyframe in Fig. 4 with a biased error, as
shown in the Ô¨Årst column (red dots) of Fig. 6. We evaluate the
performance of the method given such biased keyframes. The
other experiment settings follow the previous experiment. We
have run each experiment for 10 trials with different random
seeds for the initial Œ∏0.

In this set of experiments, we test the learning performance
of using polynomial time-warping functions of different com-
plexity. The keyframes D are the red dots in the Ô¨Årst column
of Fig. 6. For each polynomial time-warping function, we have
run the experiment for 10 trials with different random seeds for
initial Œ∏0. Other experiment settings follow the previous one.
The results are summarized in Table III. Here, the Ô¨Årst column
shows the learned time-warping functions; the second column
is the Ô¨Ånal converged losses, and the statistics (mean+standard
deviation) are over 10 trials. We have the following comments.

0.00.51.01.52.00.50.00.51.01.5q1qg10.00.51.01.52.00.30.20.10.0q2qg21 keyframe2 keyframes3 keyframes4 keyframes8 keyframes0.00.51.050q1trueLearned 05010024L(,)01201q2truelearned 0.00.51.002q2050100Iteration789||true||20120.20.0q2This is a preprint. The published version can be accessed at IEEE Transactions on Robotics.

9

TABLE III: Learning with different time-warping functions

Learned time-warping function

t = 2.55œÑ
t = 2.90œÑ ‚àí 0.55œÑ 2
t = 2.94œÑ + 0.28œÑ 2 ‚àí 0.88œÑ 3
t = 2.89œÑ + 0.53œÑ 2 ‚àí 0.49œÑ 3 ‚àí 0.60œÑ 4

min L(ŒæŒ∏, D) (mean¬±std)
1.017 ¬± 0.014
0.876 ¬± 0.008
0.831 ¬± 0.006
0.822 ¬± 0.002

Table III shows that a higher order of polynomial time-
warping function leads to the lower Ô¨Ånal loss. This is because
introduces additional degrees
a higher degree polynomial
of freedom, which enable to represent more complex time
mapping and contribute to further decreasing the loss. Mean-
while, Table III shows that (i) the Ô¨Årst-order terms in all
learned time-warping polynomials are similar, (ii) the higher-
order terms are relatively small compared to the Ô¨Årst-order
term, and (iii) adding higher-order terms to the time-warping
polynomial only decreases a small amount of Ô¨Ånal loss. All
those observations indicate that the Ô¨Årst-order term dominates
the Ô¨Ånal performance. We may conclude that in practice, it is
preferable to start with a simpliÔ¨Åed time-warping function. The
subsequent experiments will use the Ô¨Årst-order time-warping
function for simplicity.

D. Learning Neural Cost Functions

In this session, we test the ability of the proposed method to
learn neural-network cost functions. This is useful if a weight-
feature cost function formulation cannot be speciÔ¨Åed due to
the lack of prior knowledge. We set the cost function (29) with
the following neural-network cost function,

c(x, u, p) = œÜT
h(x, p) = œÜT

p(x)œÜp(x) + 0.05(cid:107)u(cid:107)2,
p(x)œÜp(x),

(34)

where œÜp(x) is a 4-8 fully-connected neural network [54] (i.e.,
4-neuron input layer and 8-neuron output layer), and p ‚àà R40
is the parameter of the neural network, i.e., all weight matrices
and bias vectors. Note that (34) uses dot product in the output
layer of the neural network to guarantee the positiveness of the
cost function. The time-warping polynomial has the degree of
one. We use the keyframes in Fig. 3 (also in Table I). Other
experiment settings are the same as the previous ones. In each
evaluation case below, we have run the experiment for 10 trials
with different random seeds for the initial Œ∏0.

We plot the learning and generalization results in Fig. 7.
We test with three cases of the keyframes shown in red dots
in the second row, and the corresponding results are shown in
each column. In each case, the Ô¨Årst row shows the loss versus
iteration; and second and third rows show the reproduced
trajectories (orange lines) by the learned cost and time-warping
functions; and the fourth and Ô¨Åfth rows show the generalization
(blue lines) of the learned cost function to new initial state
x(0) = [‚àí œÄ
4 , 0, 0, 0]T and new horizon T = 2. The motion
(dashed black lines) of Œ∏true is also plotted for reference. In
Table IV, we compute the distance between x(T ) of the gen-
eralized motion and the goal xg = [ œÄ
2 , 0, 0, 0]T to measure the
generalization performance. We have the following comments.

(a) Case 1

(b) Case 2

(c) Case 3

Fig. 7: Learning neural cost functions from keyframes. Three
cases of keyframes are used, shown in the red dots in each
column. In each case, the Ô¨Årst row shows loss versus iteration
(the solid line and shaded area denote the mean and standard
derivation over all 10 trials). The second and third rows show
the reproduced trajectories (orange lines) of the learned Œ∏. The
fourth and Ô¨Åfth rows show the generalization (blue lines) of
the learned Œ∏ to new initial state and horizon, and the motion
(dashed black lines) of Œ∏true is also plotted for reference. In
second-Ô¨Åfth rows, the gray dashed lines mark the goal for each
joint [qg

2]T = [œÄ/2, 0]T.

1, qg

TABLE IV: Distance of x(T ) of the generalized motion (in
the fourth and Ô¨Åfth rows in Fig. 7) to goal xg = [ œÄ
2 , 0, 0, 0]T.

The learned Œ∏ (mean value over all trials)

(cid:107)x(T ) ‚àí xg(cid:107)

Case 1
Case 2
Case 3
True Œ∏true

1.638
0.717
0.388
0.00346

First, compared to the distance-to-goal cost (29), the neural
cost (34) is goal-blind, meaning that the goal qg = [ œÄ
2 , 0]T is
not encoded in the neural cost function before training. Thus, it
is crucial for the robot to learn a goal-encoded neural cost for
the success of the task. Case 1 and Case 2 use four keyframes
to learn a cost function. The results in fourth and Ô¨Åfth rows
of Fig. 7 and in Table IV indicate that Case 2 has a better
generalization than Case 1 does: Case 2 has a Ô¨Ånal distance
(cid:107)x(T ) ‚àí xg(cid:107)=0.717, while Case 1 has (cid:107)x(T ) ‚àí xg(cid:107)=1.638.
This is because the keyframes in Case 1 are mainly clustered
at the beginning of motion, and thus cannot provide sufÔ¨Åcient
information about the Ô¨Ånal goal. In contrast, Case 2 has a
keyframe at the goal, and thus the learned neural cost function
captures such goal information.

Second, we add more keyframes in Case 3. It shows that
more keyframes lead to better generalization of the learned
neural cost function: the Ô¨Ånal distance is (cid:107)x(T )‚àíxg(cid:107) = 0.388,
which is better than those in Case 2 and Case 1.

0100200Iteration05L(,)0.00.51.02.50.0q10.00.51.002q2KeyframesReproduced01202q1 (neural)true0120.20.0q20100200Iteration020L(,)0.00.51.02.50.0q10.00.51.002q2KeyframesReproduced01202q1 (neural)true0120.250.00q20100200Iteration050L(,)0.00.51.02.50.0q10.00.51.002q2KeyframesReproduced01202q1 (neural)true0120.20.0q2This is a preprint. The published version can be accessed at IEEE Transactions on Robotics.

10

Lastly, the learned neural cost function in Case 2 or Case
3, while controlling the robot to approach the goal, has a
trajectory that is different from the true one (black dashed
lines). This manifests the generalizability of learning cost func-
tions. We also note that the neural cost function (34) is over-
parameterized, relative to the fewer given keyframes. Despite
this, the learned neural cost still shows a fair generalization to
new motion conditions, given a proper selection of keyframes.

E. Comparison with Related Methods

In this session, we compare the proposed method with the
related work. For all comparisons below, the learning process
uses the keyframe data in Fig. 3 (Table I). The generalization
is tested by setting the robot to a new initial condition x(0) =
[‚àí œÄ
4 , 0, 0, 0]T and a new time horizon T = 2. Other settings
follow the previous experiments if not explicitly stated.

1) Comparison with Kinematic Learning [41]: Following
[41], we Ô¨Åt the keyframes in Table I with a Ô¨Åfth-order spline,
as shown in the brown lines in Fig. 8a. The Ô¨Åtted spline is then
used to generalize the robot motion in the new condition (i.e., a
new initial condition and a new horizon). To do this, following
the idea of [41], we compare which given keyframe is closest
to the new x(0), then from which we perform extrapolation
based on the Ô¨Åtted spline to generate the new trajectory over
the new horizon T = 2. The generated trajectories are plotted
in Fig. 8b. For comparison, we also plot the generalized motion
of the previously learned weighted cost (29) and neural cost
(34) in Fig. 8c. We have the following comments on the results.

Second, given the same number of keyframes, learning cost
functions shows evident advantage in generalization. As in Fig.
8c, both the learned weighted cost function and neural cost
function can successfully control the robot to reach the goal in
new conditions. The reason why cost functions have superior
performance is that a cost function is a compact representation
of robot motion, and it represents a space of motion trajectories
parameterized by different initial conditions and time horizons.
Previous work [1] had the same conclusion.

2) Comparison with Numerical Differentiation: Recall that
a key technique of the Continuous PDP is Differential Pon-
tryagin‚Äôs Maximum Principle, which efÔ¨Åciently computes the
analytic gradient of the trajectory of a continuous-time optimal
control system with respect to system parameters. An alterna-
tive is numerical differentiation, that is, one uses numerical
differentiation to obtain dL
dŒ∏ . The experiments below compare
those two options. Other experiment settings are the same as
the previous ones.

Consider the neural cost function in (34). We vary the size
of the neural network, i.e., the dimension of p, and the system
time horizon T . We compare the computation time needed to
compute dL
dŒ∏ by Continuous PDP and numerical differentiation.
The results are in Fig. 9, based on which we have the following
comments.

(a) Spline Ô¨Åtting to the
keyframes [41]

(b) Generalization of
the Ô¨Åtted spline [41]

(c) Generalization for
the proposed method

Fig. 8: Comparison between [41] and the proposed method. (a)
is the spline model Ô¨Åtted to keyframes; (b) is the generalization
of the Ô¨Åtted spline in new motion condition (new x(0) and
new T ); and (c) is the generalization of the learned weighted
cost function (29) and learned neural cost function (34) in the
previous experiments. The gray dashed lines mark the goal of
each joint [qg
2]T = [œÄ/2, 0]T. The Ô¨Ånal distance (cid:107)x(T )‚àíxg(cid:107)
of the generalized motion is 33.670 for the Ô¨Åtted spline in (b),
0.388 for the learned neural cost function in (c), and 0.00358
for the learned weighted cost function in (c).

1, qg

First, the spline function Ô¨Åts well to the keyframes (red dots
in Fig. 8a). However, the generalization of the obtained spline
model is poor: the generalized motion has a Ô¨Ånal distance
of 33.670 to the goal. The poor performance is because the
spline is only a local kinematic model, and it cannot generalize
motion that is far away from the keyframes.

(a) Varying parameter dimension

(b) Varying system horizon

Fig. 9: Comparison of computation time between numerical
differentiation and Continuous PDP.

Fig. 9a shows an exponential increase of computational time
of numerical differentiation when the number of system pa-
rameters (dimension of Œ∏) increases. This is because numerical
differentiation requires evaluating the loss by perturbing the
parameter vector in each dimension. Each perturbation and
evaluation require solving an optimal control problem once,
thus causing high-computational cost for high-dimensional Œ∏.
In contrast, the Continuous PDP solves analytical gradients by
performing the Riccati-type iteration (Lemma 1). Since there is
no need to repetitively solve optimal control problems during
the differentiation, the proposed method can handle the large-
scale optimization problem, such as Œ∏ ‚àà R100 in Fig. 9a.

Fig. 9b shows the comparison results given different system
horizons. One observation is that the complexity of Continuous
PDP is approximately linear to the system horizon T . This is
because the numerical integration of the Riccati-type equations
in Lemma 1 is linear to the horizon T .

3) Comparison with [31]:

In this part, we compare the
proposed method with [31]. As discussed in the related work,
[31] formulates a problem similar to (16) which also mini-

0.00.51.020q10.00.51.00123q201220100q1012100q2012101q1NeuralWeighted0120.20.10.0q210152025405075100Dimension of 0246Running time [sec]Numerical gradientAnalytic gradient0.20.40.60.81.01.21.41.6Horizon T01234Running time [sec]Numerical gradientAnalytic gradientThis is a preprint. The published version can be accessed at IEEE Transactions on Robotics.

11

mizes a trajectory discrepancy loss (32), but the authors solve
it by replacing the inner optimal control problem with the
Pontryagin‚Äôs Maximum Principle conditions, thus turning a
bi-level optimization into a plain constrained optimization. We
compare their method with the Continuous PDP in terms of
convergence, sensitivity to different initialization, and gener-
alization.

(a) The method in [31].

(b) The proposed method.

Fig. 10: Comparison between the method in [31] (a) and
the proposed method (b). Each method has three trials using
different initial guesses Œ∏0, and at each trial, both methods
start from the same Œ∏0. Different trials are shown in different
color. The Ô¨Årst and second rows show the loss and parameter
error versus iteration, respectively. The third and fourth rows
show the reproduced trajectory of the learned Œ∏. For [31], the
converted plain optimization is solved by IPOPT [55]. Gray
dashed lines mark the goal of each joint [qg
2]T = [œÄ/2, 0]T.

1, qg

Both methods use the same keyframes shown in the third
and fourth rows in Fig. 10, Ô¨Årst-order polynomial time-warping
function (30), and the cost function parameterization (29).
Other experiment settings are the same as the previous exper-
iments unless explicitly stated. Fig. 10 presents the results of
[31] (left) and the proposed method (right). Here, each method
has three trials from different initial guesses Œ∏0 (i.e., using
different random seeds). Different trials are shown in different
colors. The Ô¨Årst and second rows plot the loss and parameter
error versus iteration, respectively. The third and fourth rows
show the reproduced trajectories with the learned Œ∏. We have
the following comments.

First, following [31], the converted plain optimization has
504 constraint equations and 509 decision variables. This is
large-scale and non-convex optimization, and we used IPOPT
[55] to solve it. But IPOPT is very likely to get stuck to local
optima for this problem. This has been illustrated by Fig. 10a:
the loss has converged to a small value, but the learned Œ∏ is far

away from Œ∏true. Also, in the third and fourth rows, although
the produced trajectories are close to the keyframes (red dots),
they are very different from the ground truth in Fig. 3.

The proneness of [31] to get stuck to bad solutions could be
due to two main reasons. First, since Pontryagin‚Äôs Maximum
Principle is just a necessary condition,
the solutions that
satisfy this condition may include the saddle points, which
might not necessarily be the solution to the original optimal
control problem. Thus, such a problem reformulation is not
equivalent to the original bi-level problem in general. Second,
the converted plain optimization can be large-scale and highly
nonlinear. If not properly initialized, it would easily get stuck
into a local solution.

In contrast, the proposed method solves the problem by
maintaining the bi-level structure. This bi-level treatment leads
to more numerical tractability. The lower-level optimal control
problem can be solved by many available trajectory optimiza-
tion methods such as iLQR [47], DDP [48], and the upper level
uses gradient-descent. Also, the bi-level treatment can lead to
better performance in Ô¨Ånding good (if not global) solutions. As
empirically shown in Fig. 10b, with various random guesses
Œ∏0s, the proposed method all converges to the true Œ∏true.

Finally, we need to mention that in the Continuous PDP,
Pontryagin‚Äôs Maximum Principle is only used for differentiat-
ing the trajectory of the optimal control system, not replacing
the optimal control system. In other words, the trajectory has
to be computed on the lower level before its differentiation
can be done. Therefore, the proposed method in this paper is
fundamentally different from [31].

VI. LEARNING FROM KEYFRAMES FOR PLANNING IN
UNKNOWN ENVIRONMENTS

This section presents an application scenario of the proposed
method: a robot learns a motion planner from demonstrated
keyframes to navigate through an unknown environment. A
user provides a few keyframes in the vicinity of obstacles in
an environment, and a robot learns a cost function from those
keyframes such that its produced motion can avoid the obsta-
cles. Experiments in this section are based on a 6-DoF quadro-
tor. The code can be accessed at https://github.com/wanxinjin/
Learning-from-Sparse-Demonstrations. A real-world demon-
stration is given at https://youtu.be/BYAsqMxW5Z4.

A. 6-DoF Quadrotor Setup

The equation of motion of a quadrotor Ô¨Çying in SE(3) (full

position and attitude) space is given by

ÀôrI = vI ,

m ÀôvI = mgI + f I ,
1
2

ÀôqB/I =

‚Ñ¶(œâB)qB/I ,

JB ÀôœâB = œÑ B ‚àí œâB √ó JBœâB.

(35a)

(35b)

(35c)

(35d)

Here, the subscripts B and I denote a quantity expressed in
the body and world coordinate frames, respectively; m is the
quadrotor mass; rI = [rx, ry, rz]T ‚àà R3 and vI ‚àà R3 are the
quadrotor‚Äôs position and velocity, respectively; JB ‚àà R3√ó3

050100150Iteration050L(,)Trial 1Trial 2Trial 3050100150Iteration1051010||true||2Trial 1Trial 2Trial 30.000.250.500.751.00101q11e6Trial 1Trial 2Trial 30.000.250.500.751.0050q21e6Trial 1Trial 2Trial 3050100150200Iteration101102L(,)Trial 1Trial 2Trial 302505007501000Iteration101100101||true||2Trial 1Trial 2Trial 30.000.250.500.751.0020q1Trial 1Trial 2Trial 30.000.250.500.751.00012q2Trial 1Trial 2Trial 3This is a preprint. The published version can be accessed at IEEE Transactions on Robotics.

12

is the moment of inertia; œâB ‚àà R3 is the angular velocity;
qB/I ‚àà R4 is the unit quaternion [56] describing the attitude
of the quadrotor with respect to the world frame; (35c) is
the quaternion calculus, and ‚Ñ¶(œâB) is the matrix of œâB for
quaternion multiplication [56]; œÑ B ‚àà R3 is the torque vector
applied to the quadrotor; and f I ‚àà R3 is the total force vector.
The net force magnitude (cid:107)f I (cid:107) = f ‚àà R (along the z-axis of
the body frame) and torque œÑ B = [œÑx, œÑy, œÑz] are generated by
thrust [T1, T2, T3, T4] of four propellers via

Ô£π

Ô£∫
Ô£ª =

Ô£Æ

Ô£Ø
Ô£∞

f
œÑx
œÑy
œÑz

Ô£Æ

Ô£Ø
Ô£∞

1
0
‚àílw/2
Œ∫

1
‚àílw/2
0
‚àíŒ∫

1
0
lw/2
Œ∫

Ô£π

Ô£∫
Ô£ª

1
lw/2
0
‚àíŒ∫

Ô£Æ

Ô£Ø
Ô£∞

T1
T2
T3
T4

Ô£π

Ô£∫
Ô£ª ,

(36)

with lw the wing length of the quadrotor and Œ∫ a Ô¨Åxed
constant. In our experiment, the gravity constant is 10m/s2
and all other dynamics parameters are units. The state vector
B]T ‚àà R13 and the control vector is
is x = [rT
I , vT
u = [T1, T2, T3, T4]T ‚àà R4.

B/I , œâT

I , qT

Fig. 11: Quadrotor Ô¨Çying in an environment with obstacles.
The goal is to let the quadrotor to Ô¨Çy from the left, go through
the two gates (from left to right), and Ô¨Ånally land near the goal
position on the right with goal attitude. The plotted trajectory
is a planned motion with a random cost function, which fails
to achieve the goal.

To achieve SE(3) maneuvering, we need to carefully design
the attitude error. Following [57], we deÔ¨Åne the attitude error
between the current attitude q and goal qg as

e(q, qg) =

1
2

trace(I ‚àí RT(qg)R(q)),

(37)

where R(q) ‚àà R3√ó3 is the rotation matrix corresponding to
quaternion q (see [56] for more details). For the cost function
formulation (2), we use a generic polynomial function:

We arbitrarily choose Ô¨Åve keyframes near the two gates,
listed in Table V. Here, ‚Äòarbitrarily‚Äô means that we do not
know whether these keyframes are realizable by an exact cost
function. Without much deliberation, we assign a time stamp to
each keyframe, such that they are (almost) evenly spaced in the
time horizon [0, T ] (later we will also test the method given the
random assignment of the time stamps). We also do not know
whether œÑi and T are achievable for the quadrotor dynamics.
The keyframes here contain only position information, i.e.,

c(x, u, p)=p1r2

x + p2r2

y + p3r2

z + p4rx + p5ry + p6rz

+ p7rxry + p8rxrz + p9ryrz + 0.1(cid:107)u(cid:107)2, (38a)

h(x)=10(cid:107)rI ‚àírg

I (cid:107)2 + 5(cid:107)vI (cid:107)2+

100e(qB/I , qg

B/I ) + 5(cid:107)œâB(cid:107)2,

(38b)

where rI = [rx, ry, rz]T is the quadrotor‚Äôs position, and we
have Ô¨Åxed the Ô¨Ånal cost h(x) since the quadrotor is always
expected to land near a goal position rg
I with goal attitude
qg
B/I ; and the goal velocities here are zeros. The cost function
parameter p = [p1, p2, p3, p4, p5, p6, p7, p8, p9]T ‚àà R9 will
determine how the quadrotor reaches the goal (i.e., the speciÔ¨Åc
Ô¨Çying trajectory of the quadrotor).

As shown in Fig. 11, we aim the quadrotor to Ô¨Çy from the
left initial position rI (0) with vI (0), qB/I (0), and œâB(0),
sequentially pass through two gates (from left to right gates),
and Ô¨Ånally land near the goal position rg
I with goal attitude
qg
B/I on the right. With a random cost function, the quadrotor
trajectory (blue line) does not meet the task requirement.

B. Learning from Keyframes

TABLE V: Keyframes D for the quadrotor.

Keyframe #

Time stamp œÑi

Keyframe y‚àó(œÑi)

#1
#2
#3
#4
#5

œÑ1 = 0.1s
œÑ2 = 0.2s
œÑ3 = 0.4s
œÑ4 = 0.6s
œÑ5 = 0.8s
horizon T = 1s

rI (œÑ1) = [‚àí4, ‚àí6, 3]
rI (œÑ2) = [1, ‚àí6, 3]
rI (œÑ3) = [1, ‚àí1, 4]
rI (œÑ4) = [‚àí1, 1, 5]
rI (œÑ5) = [2, 3, 4]

rI = y = g(x, u)

(39)

The time-warping function is the Ô¨Årst-order polynomial func-
tion (30), and loss L(ŒæŒ∏, D) is in (32). The learning rate is
Œ∑=10‚àí2. The quadrotor‚Äôs initial state is rI (0) = [‚àí8, ‚àí8, 5]T,
qB/I (0) = [1, 0, 0, 0]T, vI (0) = [15, 5, ‚àí10]T, and œâB(0) =
0. The goal position is rg
I = [8, 8, 0]T and the goal attitude
qg
B/I = [1, 0, 0, 0]T (recall the goal velocities here are zeros).

(a) Loss for Fig. 13a (b) Loss for Fig. 13b (c) Loss for Fig. 13c

(d) Loss for Fig. 13d (e) Loss for Fig. 13e

(f) Loss for Fig. 13f

Fig. 12: Loss versus iteration, corresponding to different cases
in Fig. 13. In each case,
the solid line and shaded area
denote the mean and standard derivation over 10 trial of the
experiment, respectively. The Ô¨Ånal loss (mean+std) for each
case is: 0.203¬±0.035 in (a), 0.625¬±0.470 in (b), 3.819¬±0.805
in (c), 8.548¬±0.880 in (d), 8.647¬±2.022 in (e), 21.777¬±6.608
for p ‚àà R4 and 130.902 ¬± 0.006 for p ‚â• 0 in (f).

050100150200Iteration0246L(,)050100150200Iteration010203040L(,)050100150200Iteration0255075100L(,)0255075100Iteration0100200300L(,)050100150200Iteration050100150L(,)050100150200Iteration0100200300400L(,)(40) with pR4(40) with p0This is a preprint. The published version can be accessed at IEEE Transactions on Robotics.

13

(a) Learning from keyframe #1.

(b) Learning from keyframes #1 and #3.

(c) Learning from keyframes #2 and #5.

(d) Learning from keyframes #1-#5.

(e) Keyframes #1-#5 with random stamps.

(f) Learning (40) from keyframes #1-#5.

Fig. 13: Learning from keyframes given in Table V. (13a)-(13d) show the reproduced trajectories by the cost functions learned
from different number of keyframes. (13e) shows the case where we randomize the time stamp œÑi of each keyframe in Table
V. (13f) shows the reproduced motion of the learned distance-to-obstacle cost function (40); here the green line corresponds to
the unconstrained weights subcase, i.e., p ‚àà R4, and the orange to the constrained weights subcase, i.e., p ‚â• 0. Corresponding
to each of the above-mentioned Ô¨Ågures, the loss versus iteration is given in Fig. 12.

1) Varying Number of Keyframes: Fig. 12a-12d and Fig.
13a-13d show different cases where we learn a cost function
from different numbers of keyframes. In each case, we have
run each experiment case for 10 trials, with each trial using
different random seeds for the initial Œ∏0. Fig. 12a-12d plot
the loss L(ŒæŒ∏, D) versus iteration, and Fig. 13a-13d show the
reproduced trajectory using the learned cost and time-warping
functions. We have the following comments on the results.

Fig. 13a-13d show that given keyframes in different loca-
tions, the proposed method always Ô¨Ånds a cost function and
a time-warping function such that the quadrotor‚Äôs reproduced
motion can get close to the keyframes. Fig. 13a-13d also show
that by increasing the number of keyframes and putting the
keyframes around the gates, the quadrotor can successfully
learn a cost function to Ô¨Çy through the two gates. Since
the keyframes are arbitrarily placed and exact cost and time-
warping functions (in the parameterization) may not exist, the
Ô¨Ånal losses are not zeros as in Fig. 12a-12d. Recall that we
only make the path cost tunable, while the Ô¨Ånal cost given
and Ô¨Åxed. Different placement of keyframes leads to different
learned path costs and thus different motion trajectories. This
cost formulation can be useful for learning how to move
instead of where to move.

2) Random Time Stamps: In Fig. 13e and Fig. 12e, we
randomize the time stamp œÑi of each keyframe in Table V
(drawn from a uniform distribution), and the cost function
is learned from the randomly-timed keyframes. The other
settings follow the previous experiment. Fig. 12e plots the loss
versus iteration, and Fig. 13e show the reproduced trajectory
from the learned cost and time-warping functions.

Comparing Fig. 13d and Fig. 13e with Fig. 12d and Fig.

12e, respectively, we can see that the choice of time stamps of
keyframes does not affect too much the learning: the Ô¨Ånal loss
(mean+std) is 8.548¬±0.880 for Fig. 13d and 8.647¬±2.022 for
Fig. 13e. This result is understandable because whatever the
keyframe time is, the proposed method always learns a time-
warping function, which maps demonstration time to the robot
dynamics time; thus performance of robot execution will not
change signiÔ¨Åcantly. The results show the importance of using
a time-warping function in general LfD problems. The ability
to handle the time misalignment is one of the key features of
the proposed method.

3) Distance-to-Obstacle Cost Parameterization: In Fig. 13f
and Fig. 12f, we replace the polynomial cost function (38a)
with the following distance-to-obstacle cost function:

c(x, u, p)= ‚àí

4
(cid:88)

i=1

pi(cid:107)rI ‚àí oi(cid:107)2 + 0.1(cid:107)u(cid:107)2,

(40)

where the given oi is the obstacle i‚Äôs position, which is the
position of the left and right pillars of the two gates, shown
in Fig. 11; and p ‚àà R4 are the weights for each to-obstacle
distance (cid:107)rI ‚àí oi(cid:107)2. We learn (40) from the Ô¨Åve keyframes in
Table V. Other experiment settings follow the previous session.
We further divide the experiment into two subcases: In the Ô¨Årst
subcase (green line), we treat the weights p as unconstrained
variables (i.e., it could be p ‚â§ 0, the obstacles could have
an ‚Äòattracting‚Äô effect on quadrotor motion); and in the second
subcase (orange line), we force p ‚â• 0. The loss for those two
subcases are plotted in Fig. 12f, and the reproduced trajectories
of the learned cost functions are in Fig. 13f, where the green
line corresponds to the unconstrained weights subcase while
the orange to the constrained weights subcase.

XYTop viewXZFront viewX (m)84048Y (m)808Z (m)02468XYTop viewXZFront viewX (m)84048Y (m)808Z (m)02468XYTop viewXZFront viewX (m)84048Y (m)808Z (m)02468XYTop viewXZFront viewX (m)84048Y (m)808Z (m)02468XYTop viewXZFront viewX (m)84048Y (m)808Z (m)02468XYTop viewXZFront viewX (m)84048Y (m)808Z (m)02468unconstrained weightsconstrained weightsThis is a preprint. The published version can be accessed at IEEE Transactions on Robotics.

14

In the unconstrained weights subcase in Fig. 13f, one
observation is that the motion (green line) is similar to the
motion in Fig. 13d and Fig. 13e. In fact, the learned weights
are p=[‚àí0.806, ‚àí1.406, ‚àí2.578, ‚àí2.246]T‚â§0. This indicates
that each obstacle has an attracting effect on the quadrotor‚Äôs
motion. Considering the distance-to-obstacle cost in (40) is
a second-order polynomial function in rI , similar to (38a),
the results in Fig. 13f could explain those in Fig. 13d and
Fig. 13e. SpeciÔ¨Åcally, one can intuitively think of learning a
general polynomial cost function (38a) as a process of Ô¨Ånding
some ‚Äòvirtual attracting points‚Äô in the unknown environment,
and both their locations and attracting weights will be encoded
in the learned polynomial coefÔ¨Åcients.

We also note that the reproduced motion (green line) in
the unconstrained weights subcase in Fig. 13f has a larger
distance to the keyframes than the motion in 13d. This has
been quantitatively shown by their Ô¨Ånal loss values in Fig.
12: the former has a Ô¨Ånal loss 21.777 ¬± 6.608 while the latter
8.548 ¬± 0.880. This is because the formulation (38a) has p ‚àà
R9, while (40) only has p ‚àà R4. Learning (38a) allows us to
optimize both weights and locations of the ‚Äòvirtual attracting
points‚Äô, while learning (40) allows us to only optimize the
weights as the location of the obstacle oi are given.

In the non-negative weights subcase (the orange line) in Fig.
13f, since we always force p ‚â• 0, the obstacles only have a
‚Äòrepelling‚Äô effect on the quadrotor motion, and thus the Ô¨Ånal
quadrotor motion avoids all obstacles, as in Fig. 13f. Also, Fig.
12f shows that the Ô¨Ånal loss of this subcase is 130.902¬±0.006
is higher than 21.777 ¬± 6.608 of the subcase of unconstrained
weights. This is because the search space {p | p ‚â• 0} in the
former is only part of that {p | p ‚àà R4} of the latter subcase.

In summary of all experiments in this subsection, we
conclude: (i) the proposed method can learn a cost function
(and a time-warping function) from sparse keyframes for
motion planning in an unmodeled environment; (ii) since the
method jointly learns a time-warping function, the time stamps
of keyframes do not signiÔ¨Åcantly inÔ¨Çuence the performance;
and (iii) learning a generic (e.g., polynomial or neural) cost
function can be intuitively thought of Ô¨Ånding some virtual
attracting points in the unknown environment, whose locations
and weights will be encoded in the learned cost function.

C. Generalization of Learned Cost Functions

In this session, we will test the generalization of the cost
functions learned in the previous session. We will set the
quadrotor with a new initial condition, a new landing goal,
and new placement of obstacles. Given these new conditions,
we use the learned cost and time-warping functions to plan the
motion of the quadrotor, respectively. We check if the motion
plan can successfully achieve the task goal: Ô¨Çying through
the gates and landing near the goal position. Quantitatively,
we evaluate the generalization performance by calculating
the averaged distance between the generalized motion and
keyframes and the averaged distance between the generalized
motion and the centers of obstacles (gates).

1) New Initial Conditions: In Fig. 14a-Fig. 14c, we test the
generalization of the cost function learned in Fig. 13d to new

initial conditions (the landing position is the same as the one
in the learning stage in the previous session). Here, we use
the following new initial conditions, as also visualized in Fig.
14a-Fig. 14c, respectively,

New initial condition 1: position rI (0)=[‚àí8, ‚àí10, 1]T, atti-
tude quaternion qB/I (0)=[0.88, ‚àí0.42, 0.19, 0.14]T, velocity
vI (0)=[15, 0, 0]T, and angular velocity œâB(0)=[0, 0, 0]T.

New initial condition 2: position rI (0)=[6, ‚àí8, 2]T, attitude
quaternion qB/I (0)=[0.88, ‚àí0.45, ‚àí0.05, ‚àí0.15]T, velocity
vI (0)=[10, 0, 0]T, and angular velocity œâB(0)=[0, 0, 0]T.

New initial condition 3: position rI (0)=[‚àí8, 5, 1]T, atti-
tude quaternion qB/I (0) = [0.88, 0.14, 0.14, 0.43]T, velocity
vI (0)=[10, ‚àí20, 0]T, and angular velocity œâB(0)=[0, 0, 0]T.
Note that the above new initial conditions are very different
from the ones used in learning stage (Fig. 13). For each
new initial condition, the generalized motion is shown in Fig.
14a-Fig. 14c, respectively. Fig. 14c also plots the generalized
motion (cyan color) of the kinematic learning method [41] as a
comparison. In Fig. 14e, we plot the generalized motion from
the learned distance-to-obstacle cost function (40) in Fig. 13f.
Here, the green line corresponds to the unconstrained weights
subcase and the orange to the constrained weights subcase.
The quantitative measures for all generalized motion in Fig.
14 are in Table VI.

TABLE VI: Measure of the generalized motion.

Avg. distance to keyframes

Avg. distance to center of gate

1.460

1.014

0.863

1.187

1.715 (the proposed)
3.428 (kinematic learning [41])

1.020 (the proposed)
3.660 (kinematic learning [41])

1.841

1.492

1.641 (unconstrained weights)
7.580 (constrained weights)

1.423 (unconstrained weights)
7.950 (constrained weights)

1.902 (dist.-to-obstacle cost)
1.514 (polynomial cost)

1.617 (dist.-to-obstacle cost)
3.186 (polynomial cost)

Fig.

14a

14b

14c

14d

14e

14f

A keyframe‚Äôs distance to a trajectory is the distance between this keyframe
and its nearest point on the trajectory, and we average the distance over
all keyframes.

2) New Landing Goal: Fig. 14d tests the generalization of
the cost function learned in Fig. 13d to a new landing goal. The
initial condition is as follows: position rI (0)=[‚àí8, ‚àí8, 2]T,
attitude quaternion qB/I (0)=[0.88, ‚àí0.42, 0.19, 0.14]T, veloc-
ity vI (0)=[15, 5, ‚àí2]T, and angular velocity œâB(0)=[0, 0, 0]T.
We set the new landing goal to rg
B/I =
[0.97, 0, 0, 0.25]T. The quantitative measure for the generalized
motion is in Table VI.

I = [‚àí8, 8, 2]T and qg

3) New Placement of Obstacles: Fig. 14f tests the gener-
alization of the learned cost functions under new placement
of obstacles. We change the positions of two gates and then
use the learned cost functions to generate new motion in the
new environment. The initial condition is the same as the one
in Fig. 14d and the landing goal as that in Fig. 14a. Fig.
14f shows the generalized motion of the distance-to-obstacle
cost function (40) (unconstrained weights, learned in Fig. 13f)
and the polynomial cost function (38a) (learned in Fig. 13e).

This is a preprint. The published version can be accessed at IEEE Transactions on Robotics.

15

(a) New initial condition 1. Generalization of
the polynomial cost learned in Fig. 13e.

(b) New initial condition 2. Generalization of
the polynomial cost learned in Fig. 13e.

(c) New initial condition 3. Comparing with
generalization of kinematic learning [41]

(d) New landing goal. Generalization of the
polynomial cost learned in Fig. 13e.

(e) New initial condition 2. Generalization of
distance-to-obstacle cost learned in Fig. 13f.

(f) New placement of obstacles. Generaliza-
tion comparison between the polynomial cost
(38a) and distance-to-obstacle cost (40).

Fig. 14: Generalization test of the cost functions learned in Fig. 13. (a)-(c) are the generalized motion of the polynomial cost
function (learned in Fig. 13e) given different new initial conditions. In (c), we also compare with the generalized motion of the
kinematic learning method [41] (discussed in Section V-E1). (d) shows the generalized motion of the polynomial cost function
(learned in Fig. 13e) given a new landing goal. (e) is the generalized motion of the distance-to-obstacle cost function (learned in
Fig. 13f) given new initial condition 2; here, green and orange colors correspond to the unconstrained and constrained weights
subcases, respectively. (f) is the generalization of the polynomial cost function (learned in Fig. 13e) and the distance-to-obstacle
cost function (learned in Fig. 13f) given new placement of obstacles. All quantitative measures are in Table VI.

Note that in the generalization of the distance-to-obstacle cost
(40), oi is set as the obstacle‚Äôs new location. The quantitative
measure for the generalized motion is in Table VI.

4) Result Analysis: With the new initial conditions and new
landing goal, Fig. 14a- 14d show that the generalized motion
can still follow the keyframes, pass through the gates, and land
near the goal. Fig. 14c also shows that the generalization of the
kinematic learning [41] fails to Ô¨Çy through both gates. As dis-
cussed in Section V-E1, since [41] focuses on learning a low-
level kinematic representation, it has limited generalizability
particularly when the new conditions are very different from
ones in learning. In contrast, a learned cost function can be
shared across different motion conditions. Thus, learning cost
functions shows better generalizability. Fig. 14e also shows
that the generalization of the distance-to-obstacle cost function
(40) (unconstrained weights) is comparable to that in Fig. 14b.
The special attention should be paid to Fig. 14b and Fig.
14e (unconstrained weights), where the quadrotor seems to
have ignored the Ô¨Årst two keyframes (and hence the left gate).
This could be explained by Bellman‚Äôs principle of optimality
[58]. SpeciÔ¨Åcally, the motion in Fig. 14b can be interpreted as
the Ô¨Ånal segment of the ‚Äòcomplete‚Äô trajectory in Fig. 13e, i.e.,
it can be viewed as the solution to a sub-problem, for which
the initial condition starts from a middle point of a ‚Äòcomplete‚Äô
trajectory and minimizes the remaining cost-to-go. In other
words, if a complete trajectory from the initial start to a goal

is optimal with respect to a cost function, the sub-trajectory
of this complete trajectory from any middle point to the goal
is also optimal with respect to the same cost function. Thus,
the quadrotor motion in Fig. 14b and Fig. 14e is continuing
to Ô¨Ånish the rest optimal motion instead of Ô¨Çying back to pass
through the Ô¨Årst gate.

Fig. 14f shows the generalization of the learned distance-to-
obstacle cost function (40) versus that of the learned generic
polynomial cost function (38a) in a varying environment.
With Fig. 14f and Table VI, one can conclude that
the
polynomial cost function generalizes poorly to new place-
ment of obstacles, compared to the distance-to-obstacle cost
function. SpeciÔ¨Åcally, the generalized motion of the learned
polynomial cost function still
tries to follow the original
keyframes instead of going through the new gates: its distance
to the keyframes is 1.514 versus the distance-to-obstacle cost
function‚Äôs 1.902, while its distance to the centers of gates
is 3.186 versus the distance-to-obstacle cost‚Äôs 1.617. This is
understandable because the learned polynomial cost function
only ‚Äòremembers‚Äô the representation of the keyframes in the
original environment and is unaware of the obstacle changes in
the new environment. On the contrary, the distance-to-obstacle
cost function (40) is deÔ¨Åned on the locations of obstacles, and
can be updated with the new locations of obstacles. Hence,
in the new environment in Fig. 14f, the generalized motion of
the distance-to-obstacle cost tries to go through the new gates.
As indicated in Table VI, the generalization of the distance-

XYTop viewXZFront viewX (m)84048Y (m)808Z (m)02468XYTop viewXZFront viewX (m)84048Y (m)808Z (m)02468XYTop viewXZFront viewX (m)84048Y (m)808Z (m)02468the proposedkinematic learningXYTop viewXZFront viewX (m)84048Y (m)808Z (m)02468XYTop viewXZFront viewX (m)84048Y (m)808Z (m)02468unconstrained weightsconstrained weightsXYTop viewXZFront viewX (m)84048Y (m)808Z (m)02468Distance-to-obstacle costGeneral polynomial costThis is a preprint. The published version can be accessed at IEEE Transactions on Robotics.

16

to-obstacle cost function has a smaller distance (1.617) to
the new gates than the polynomial cost does (3.186). This
can also be visualized in Fig. 14f, where the motion of the
distance-to-obstacle cost is attempting to reach the left gate
(although it is not successfully passing through it). The above
results suggest that an environment-dependent formulation of
cost functions, such as a cost function that is deÔ¨Åned on both
robot state and environment features, could generalize better in
a varying environment. But one also needs to note that such a
formulation additionally requires the knowledge/model of the
environment features. More discussion of the cost formulations
is given in Section VII-B.

In summary of all the above experiments and analyses,
we conclude that (i) the proposed method can learn a cost
function from a small number of keyframes; (ii) the learned
cost function shows good generalization to unseen motion
conditions; and (iii) to generalize to varying environments, an
environment-informed formulation of cost functions would be
needed, such as the cost function formulation which depends
on both robot‚Äôs state and environment features.

VII. DISCUSSION

This section further provides discussion on some aspects of

the proposed method.

A. Why Do Keyframes SufÔ¨Åce?

We provide one explanation for why sparse keyframes can
sufÔ¨Åce to recover a cost function. Consider problem (16). For
trajectory ŒæŒ∏ produced by optimal control system (12), since
we are only interested in the trajectory points ŒæŒ∏(œÑi) at the
time stamps œÑi (1 ‚â§ i ‚â§ N ), we discretize the optimal control
system at these time steps, yielding [50]

dynamics: xi+1 = ¬Øf (xi, ¬Øui, Œ∏), x0 = x(0),
N ‚àí1
(cid:88)

objective: J(Œ∏) =

¬Øc(xi, ¬Øui, Œ∏) + ¬Øh(xN , ¬ØuN , Œ∏), (41b)

(41a)

i=0

where we denote xi = x(œÑi), and discrete-time ¬Øf satisÔ¨Åes

xi+1 = ¬Øf (xi, ¬Øui, Œ∏) = xi +

(cid:90) œÑi+1

œÑi

vŒ≤(œÑ )f (x(œÑ ), u(œÑ ))dœÑ,

and the discrete version of the cost function satisÔ¨Åes

¬Øc(xi, ¬Øui, Œ∏) =

(cid:90) œÑi+1

vŒ≤(œÑ )cp(x(œÑ ), u(œÑ ))dœÑ,

¬Øh(xN , ¬ØuN , Œ∏) =

œÑi
(cid:90) T

œÑN

vŒ≤(œÑ )cp(x(œÑ ), u(œÑ ))dœÑ + hp(x(T )).

Here, the new input ¬Øui ‚àà Rd in ¬Øf may not necessarily have
the same dimension as u(œÑ ) ‚àà Rn in the original f , e.g., ¬Øui
contains all possible controls over time range [œÑi, œÑi+1] [50].

The solution {x0:N , ¬Øu0:N } to the discrete-time optimal control
system (41) satisÔ¨Åes the KKT conditions:

+

Œªi =

xi+1 = ¬Øf (xi, ¬Øui, Œ∏),
‚àÇ ¬ØfT
‚àÇ¬Øc
‚àÇxi
‚àÇxi
‚àÇ ¬ØfT
‚àÇ¬Øc
‚àÇ ¬Øui
‚àÇ ¬Øui
‚àÇ¬Øh
‚àÇ¬Øh
‚àÇ ¬ØuN
‚àÇxN

ŒªN =

0 =

+

,

Œªi+1,

Œªi+1,

i = 0, ¬∑ ¬∑ ¬∑ N ‚àí 1,

i = 1, ¬∑ ¬∑ ¬∑ N ‚àí 1,

i = 0, ¬∑ ¬∑ ¬∑ N ‚àí 1,

(42)

= 0

i = N.

The output of the discrete-time system (41) can be overloaded
by y(œÑi) = g(xi, ¬Øui). To simplify analysis, we assume that
keyframes D are realizable by a Œ∏. Then,

y‚àó(œÑi) = g(xi, ¬Øui).

(43)

Given the keyframes D in (5), recovering a cost function can
be viewed as a problem of solving a set of non-linear equations
in (42) and (43), where unknowns are {x1:N , ¬Øu0:N , Œª1:N , Œ∏} ‚àà
R2N n+(N +1)d+(r+s), and the total number of constraints
(equations) are 2N n + (N + 1)d + N o. Here, (r + s) is the
dimension of Œ∏ and o is the dimension of y. A necessary con-
dition to uniquely determine {x1:N , ¬Øu0:N , Œª1:N , Œ∏} requires
the number of constraints to be no less than the number of
unknowns, yielding

N ‚â•

.

(44)

r + s
o

On the other hand, if (44) is not fulÔ¨Ålled or given D is less in-
formative, the unknowns then cannot be uniquely determined,
which means that there might exist multiple Œ∏s such that all
resulting trajectories pass the same sparse keyframes. This case
has been shown in Section V-A (Fig. 4).

Note that the above discussion uses a perspective different
from the development of this paper. It should be noted that
the above explanation fails to explain the case where the given
keyframes are not realizable: minŒ∏ L(ŒæŒ∏, D) > 0, e.g., sub-
optimal data as in Section V-B. We leave its further exploration
as one future direction of this work.

B. Cost Function Formulation

In general, there are two types of cost function formulations,

as discussed below.

1) Cost Depending Purely on Robot States: The Ô¨Årst type
of cost function formulation can be written as cp(x, u),
which only depends on robot state and input (x, u). The
polynomial cost function (38a) in Section VI belongs to this
type. This formulation type can generalize well to different
motion conditions, e.g., new initial condition and new goals,
as shown in Fig. 14a - Fig. 14e. However, it cannot generalize
to varying environments as in Fig. 14f, as the environment
information is not explicitly captured in this cost formulation.
2) Cost Depending on Robot States and Environment Fea-
tures: The second type of cost formulations can be written
as cp(x, u, o), which depends on both the robot state-input
(x, u) and the environment features o. Here, o should be given
for the environment where the robot is trained. Demonstrations
from different environments can also be used as the training
data. The cost functions in (29) and (40) belong to this type.

This is a preprint. The published version can be accessed at IEEE Transactions on Robotics.

17

One advantage of this formulation is that it has the ability to
generalize to a new environment given its environment features
o. Section VI-C3 has shown such an advantage by comparing
with the Ô¨Årst type of cost formulation. At the same time, one
should note that the second type of cost formulation requires
the knowledge of environment features o, which may need
additional modeling effort.

3) Running Cost and Final Cost: The cost function in (2)
includes two terms: a running cost term c(¬∑) and a Ô¨Ånal cost
term h(¬∑). If no knowledge about the task goal is available,
one can use a (deep) neural network to represent both costs, as
shown in Section V-D. Since a neural cost function is usually
goal-blind, the training data needs to include a keyframe at the
goal. If the task goal is known, such as in motion planning in
Section VI, the Ô¨Ånal cost h(¬∑) can be set to the distance-to-
goal cost, and the running cost c(¬∑) is to be tuned. Tuning a
running cost will determine how the robot moves to the goal.
This has been shown in Section VI.

4) Limitation: We should note that whatever a cost for-
mulation is, the proposed method requires all functions to
be differentiable. This can be a limitation of the proposed
method, compared to some existing feature-based IRL meth-
ods such as max-entropy IRL [15], which permits non-
differentiable features. How to extend the proposed method
to non-differentiable systems is a topic for our future work.

C. Convergence and Numerical Integration Error

1) Algorithm Convergence: The proposed Continuous PDP
solves a bi-level optimization problem (16) using gradient
descent. It treats the trajectory ŒæŒ∏ of the inner-level optimal
control system simply as an ‚Äòimplicit‚Äô differentiable function
of the system parameter Œ∏. Generally, bi-level optimization
is known to be strongly NP-hard [59], [60]. Under certain as-
sumptions, one can prove that the gradient-descent method can
converge to a stationary point [61]. With further assumptions
on the outer-level and inner-level problems, such as convexity
and smoothness, [62] shows that the gradient-descent method
could converge to the global solution. However, in our case,
the requirement of convexity is too restricted to optimal control
systems (12). As a future direction of this work, we will try
to explore the milder conditions for its convergence.

2) Numerical Integration Error: Another issue that might
arise is the numerical integration error in solving the gradient
of the inner-level trajectory using Lemma 1, as it requires in-
tegrating several ODEs in both backward and forward passes.
However, our previous experimental experience shows that
due to the side effect of a time-warping function, numerical
integration error/stability can be potentially mitigated. A sim-
ilar process has also been successfully used in some optimal
control software such as [50].

A side effect of using a time-warping function t = w(œÑ ) is
that one can scale a long-horizon integration into a smaller
horizon problem by time-warping transformation,
then re-
scale the solution back after integration (some reÔ¨Ånement can
be done afterwards). For example, (cid:82) tf
0 c(x(t), u(t))dt in (2)
over [0, tf ] can be transformed to (cid:82) T
dw(œÑ )
dœÑ c(x(œÑ ), u(œÑ ))dœÑ
in (12b) over the new horizon [0, T ], using the time-warping

0

function tf = w(T ). In our problem of interest, since T
is given, one can manually pick a relatively small horizon
T < tf and a small integration step size to mitigate the
error of numerical integration. In our previous experiments,
we set the keyframe horizon as T = 1 for good numerical
integration accuracy. This time-warping trick has been suc-
cessfully adopted by some optimal control software such as
[50] for numerical stability. One important caveat is that by
using the time-warping transformation t = w(œÑ ), as shown
from (2) to (12b), we have changed the original integrand
c(x(t), u(t)) to the new dw(œÑ )
dœÑ c(x(œÑ ), u(œÑ )). Hence, if one
wants to signiÔ¨Åcantly decrease the horizon, i.e., T (cid:28) tf ,
dw(œÑ )
dœÑ would be very large, which may increase the stiffness of
dw(œÑ )
dœÑ c(x(œÑ ), u(œÑ )), causing numerical instability. Although
issues in our
we have rarely encountered such numerical
previous experiments with T = 1s, one might be cautious
when handling stiff ODEs/systems.

D. Model-free versus Model-based

The formulation in this paper assumes robot dynamics to be
known. We would point out that the proposed Continuous PDP
is also able to solve model-free IOC/IRL, i.e., jointly learning
a dynamics model and a cost function from keyframes. To do
that, one needs to replace the known dynamics (1) with a pa-
rameterized dynamics model, which should be differentiable.
The Continuous PDP can update all parameters (including both
dynamics and objective parameters) using gradient descent.
We refer the reviewer to our previous work Pontryagin Dif-
ferentiable Programming (PDP) [34], [46] (discrete-time) for
the model-free IOC/IRL formulation and experiments.

In fact, after the problem reformulation in Section III.B, as
shown in (12a), the parameter of the time-warping function
has been absorbed into the dynamics model and becomes the
unknown parameter in the dynamics. Thus, the Continuous
PDP has already shown its ability to jointly update the
parameters in both the dynamics model and cost function.

VIII. CONCLUSIONS

This paper proposes the method of Continuous Pontryagin
Differentiable Programming (Continuous PDP) to enable a
robot to learn an objective function from a small set of demon-
strated keyframes. As the given time stamps of the keyframes
may not be achievable in the robot‚Äôs actual execution, the
Continuous PDP jointly Ô¨Ånds an objective function and a time-
warping function such that the robot‚Äôs Ô¨Ånal motion attains the
minimal discrepancy loss to the keyframes. The Continuous
PDP minimizes the discrepancy loss using projected gradient
descent, by efÔ¨Åciently computing the gradient of the optimal
trajectory with respect to the tunable function parameters in
the system. The efÔ¨Åcacy and capability of the Continuous PDP
are demonstrated in robot arm and 6-DoF quadrotor planning
tasks.

This is a preprint. The published version can be accessed at IEEE Transactions on Robotics.

18

APPENDIX
PROOF OF LEMMA 1

We consider the equation of Differential Pontryagin‚Äôs Max-
imum Principle in (22). Suppose that Huu(œÑ ) in (23c) is
invertible for all 0 ‚â§ œÑ ‚â§ T . We can solve ‚àÇuŒ∏
‚àÇŒ∏ from (22c):
+G(œÑ )T ‚àÇŒªŒ∏
‚àÇŒ∏

.
(45)
Substituting (45) into both (22a) and (22b) and combining the
deÔ¨Ånition of matrices in (25), we have

= ‚àíH ‚àí1

+Hue(œÑ )

‚àÇuŒ∏
‚àÇŒ∏

‚àÇxŒ∏
‚àÇŒ∏

Hux(œÑ )

uu (œÑ )

(cid:16)

(cid:17)

d
dœÑ
d
dœÑ

(

(

‚àÇxŒ∏
‚àÇŒ∏
‚àÇŒªŒ∏
‚àÇŒ∏

‚àí

) = A(œÑ )

) = Q(œÑ )

‚àÇxŒ∏
‚àÇŒ∏
‚àÇxŒ∏
‚àÇŒ∏

‚àÇŒªŒ∏
‚àí R(œÑ )
‚àÇŒ∏
+ A(œÑ )(cid:48) ‚àÇŒªŒ∏
‚àÇŒ∏

+ M (œÑ ),

(46a)

+ N (œÑ ).

(46b)

Motivated by (22d), we assume

‚àÇŒªŒ∏
‚àÇŒ∏

= P (œÑ )

‚àÇxŒ∏
‚àÇŒ∏

+ W (œÑ ),

(47)

with P (œÑ ) ‚àà Rn√ón and W (œÑ ) ‚àà Rn√ó(s+r), 0 ‚â§ œÑ ‚â§ T , are
two time-varying matrices. Of course, the above (47) holds for
œÑ = T because of (22d), if

P (œÑ ) = Hxx(T )

and W (œÑ ) = Hxe(T ).

(48)

Substituting (47) to (46a) and (46b), respectively, to elimi-

nate ‚àÇxŒ∏

(

‚àÇŒ∏ , we obtain the following
d
dœÑ
d
dœÑ

‚àÇxŒ∏
‚àÇŒ∏
)=(Q+ ÀôP +ATP )

‚àÇxŒ∏
‚àÇŒ∏
‚àÇxŒ∏
‚àÇŒ∏

)=(A ‚àí RP )

(

‚àÇxŒ∏
‚àÇŒ∏

‚àí ÀôP

+ (‚àíRW + M ),

(49a)

+(A(cid:48)W +N + ÀôW ), (49b)

,

dœÑ

ÀôW = dW (œÑ )

where ÀôP = dP (œÑ )
, and we here have suppressed
the dependence of œÑ for all time-varying matrices. By mul-
tiplying (‚àí ÀôP ) on both sides of (49a), and equaling the left
sides of (49a) and (49b), we have

dœÑ

(‚àíP A + P RP )

‚àÇxŒ∏
‚àÇŒ∏
‚àÇxŒ∏
‚àÇŒ∏
The above equation holds if

=(Q + ÀôP + ATP )

+ (P RW ‚àí P M )

+ (ATW + N + ÀôW ).

(50)

‚àíP A + P RP = Q + ÀôP + ATP,
P RW ‚àí P M = ATW + N + ÀôW,

(51a)

(51b)

which directly are (24). Substituting (47) into (45) yields (27a),
and (27b) directly results from (22a). This completes the proof.

REFERENCES

[1] H. Ravichandar, A. S. Polydoros, S. Chernova, and A. Billard, ‚ÄúRecent
learning from demonstration,‚Äù Annual Review of

advances in robot
Control, Robotics, and Autonomous Systems, vol. 3, 2020.

[2] M. DeniÀása, A. Gams, A. Ude, and T. PetriÀác, ‚ÄúLearning compliant move-
ment primitives through demonstration and statistical generalization,‚Äù
IEEE/ASME transactions on mechatronics, vol. 21, no. 5, pp. 2581‚Äì
2594, 2015.

[3] C. Moro, G. Nejat, and A. Mihailidis, ‚ÄúLearning and personalizing
socially assistive robot behaviors to aid with activities of daily living,‚Äù
ACM Transactions on Human-Robot Interaction, vol. 7, no. 2, pp. 1‚Äì25,
2018.

[4] M. Kuderer, S. Gulati, and W. Burgard, ‚ÄúLearning driving styles for
autonomous vehicles from demonstration,‚Äù in IEEE International Con-
ference on Robotics and Automation, 2015, pp. 2641‚Äì2646.

[5] D. A. Pomerleau, ‚ÄúEfÔ¨Åcient training of artiÔ¨Åcial neural networks for
autonomous navigation,‚Äù Neural computation, vol. 3, no. 1, pp. 88‚Äì97,
1991.

[6] P. Englert, A. Paraschos, M. P. Deisenroth, and J. Peters, ‚ÄúProbabilistic
model-based imitation learning,‚Äù Adaptive Behavior, vol. 21, no. 5, pp.
388‚Äì403, 2013.

[7] S. Calinon, F. Guenter, and A. Billard, ‚ÄúOn learning, representing, and
generalizing a task in a humanoid robot,‚Äù IEEE Transactions on Systems,
Man, and Cybernetics, vol. 37, no. 2, pp. 286‚Äì298, 2007.

[8] R. Rahmatizadeh, P. Abolghasemi, L. B¬®ol¬®oni, and S. Levine, ‚ÄúVision-
based multi-task manipulation for inexpensive robots using end-to-end
learning from demonstration,‚Äù in IEEE International Conference on
Robotics and Automation, 2018, pp. 3758‚Äì3765.

[9] F. Torabi, G. Warnell, and P. Stone, ‚ÄúBehavioral cloning from observa-
tion,‚Äù in International Joint Conference on ArtiÔ¨Åcial Intelligence, 2018,
pp. 4950‚Äì4957.

[10] P. Abbeel and A. Y. Ng, ‚ÄúApprenticeship learning via inverse reinforce-
ment learning,‚Äù in International Conference on Machine Learning, 2004,
pp. 1‚Äì8.

[11] A. Y. Ng, S. J. Russell et al., ‚ÄúAlgorithms for inverse reinforcement
learning.‚Äù in International Conference of Machine Learning, vol. 1,
2000, p. 2.

[12] P. Moylan and B. Anderson, ‚ÄúNonlinear regulator theory and an inverse
optimal control problem,‚Äù IEEE Transactions on Automatic Control,
vol. 18, no. 5, pp. 460‚Äì465, 1973.

[13] J. MacGlashan and M. L. Littman, ‚ÄúBetween imitation and intention
learning,‚Äù in International Joint Conference on ArtiÔ¨Åcial Intelligence,
2015.

[14] N. D. Ratliff, J. A. Bagnell, and M. A. Zinkevich, ‚ÄúMaximum margin
planning,‚Äù in International Conference on Machine Learning, 2006, pp.
729‚Äì736.

[15] B. D. Ziebart, A. L. Maas, J. A. Bagnell, and A. K. Dey, ‚ÄúMaximum
entropy inverse reinforcement learning.‚Äù in Association for the Advance-
ment of ArtiÔ¨Åcial Intelligence, vol. 8, 2008, pp. 1433‚Äì1438.

[16] K. Mombaur, A. Truong, and J.-P. Laumond, ‚ÄúFrom human to humanoid
locomotion‚Äîan inverse optimal control approach,‚Äù Autonomous Robots,
vol. 28, no. 3, pp. 369‚Äì383, 2010.

[17] A. Keshavarz, Y. Wang, and S. Boyd, ‚ÄúImputing a convex objective
function,‚Äù in IEEE International Symposium on Intelligent Control.
IEEE, 2011, pp. 613‚Äì619.

[18] A.-S. Puydupin-Jamin, M. Johnson, and T. Bretl, ‚ÄúA convex approach
to inverse optimal control and its application to modeling human
locomotion,‚Äù in International Conference on Robotics and Automation,
2012, pp. 531‚Äì536.

[19] P. Englert, N. A. Vien, and M. Toussaint, ‚ÄúInverse kkt: Learning cost
functions of manipulation tasks from demonstrations,‚Äù International
Journal of Robotics Research, vol. 36, no. 13-14, pp. 1474‚Äì1488, 2017.
[20] P. Kingston and M. Egerstedt, ‚ÄúTime and output warping of control
systems: Comparing and imitating motions,‚Äù Automatica, vol. 47, no. 8,
pp. 1580‚Äì1588.

[21] T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel, J. Peters
et al., ‚ÄúAn algorithmic perspective on imitation learning,‚Äù Foundations
and Trends in Robotics, vol. 7, no. 1-2, pp. 1‚Äì179, 2018.

[22] A. Doerr, N. D. Ratliff, J. Bohg, M. Toussaint, and S. Schaal, ‚ÄúDirect
loss minimization inverse optimal control.‚Äù in Robotics: Science and
Systems, 2015.

[23] W. Jin, D. Kuli¬¥c, S. Mou, and S. Hirche, ‚ÄúInverse optimal control from
incomplete trajectory observations,‚Äù International Journal of Robotics
Research, vol. 40, no. 6-7, pp. 848‚Äì865, 2021.

[24] W. Jin, D. Kuli¬¥c, J. F.-S. Lin, S. Mou, and S. Hirche, ‚ÄúInverse optimal
control for multiphase cost functions,‚Äù IEEE Transactions on Robotics,
vol. 35, no. 6, pp. 1387‚Äì1398, 2019.

[25] H. W. Kuhn and A. W. Tucker, ‚ÄúNonlinear programming,‚Äù in Traces and
Emergence of Nonlinear Programming. Springer, 2014, pp. 247‚Äì258.
[26] L. S. Pontryagin, V. G. Boltyanskiy, R. V. Gamkrelidze, and E. F.
Mishchenko, The Mathematical Theory of Optimal Processes.
John
Wiley & Sons, Inc., 1962.

[27] W. Jin and S. Mou, ‚ÄúDistributed inverse optimal control,‚Äù Automatica,

vol. 129, p. 109658, 2021.

[28] K. Mombaur, A.-H. Olivier, and A. Cr¬¥etual, ‚ÄúForward and inverse
optimal control of bipedal running,‚Äù in Modeling, simulation and op-
timization of bipedal walking. Springer, 2013, pp. 165‚Äì179.

This is a preprint. The published version can be accessed at IEEE Transactions on Robotics.

19

[29] M. J. Powell, ‚ÄúThe bobyqa algorithm for bound constrained optimization
without derivatives,‚Äù Cambridge NA Report, University of Cambridge,
Cambridge, pp. 26‚Äì46, 2009.

[30] L. M. Rios and N. V. Sahinidis, ‚ÄúDerivative-free optimization: a review
of algorithms and comparison of software implementations,‚Äù Journal of
Global Optimization, vol. 56, no. 3, pp. 1247‚Äì1293, 2013.

[31] K. Hatz, J. P. Schloder, and H. G. Bock, ‚ÄúEstimating parameters in
optimal control problems,‚Äù SIAM Journal on ScientiÔ¨Åc Computing,
vol. 34, no. 3, pp. A1707‚ÄìA1728, 2012.

[32] N. Das, S. Bechtle, T. Davchev, D. Jayaraman, A. Rai, and F. Meier,
‚ÄúModel-based inverse reinforcement learning from visual demonstra-
tions,‚Äù in Conference on Robotic Learning, pp. 1930‚Äì1942.

[33] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga,
S. Moore, D. G. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden,
M. Wicke, Y. Yu, and X. Zheng, ‚ÄúTensorFlow: A system for Large-Scale
machine learning,‚Äù in 12th USENIX Symposium on Operating Systems
Design and Implementation (OSDI 16). USENIX Association, 2016,
pp. 265‚Äì283.

[34] W. Jin, Z. Wang, Z. Yang, and S. Mou, ‚ÄúPontryagin differentiable pro-
gramming: An end-to-end learning and control framework,‚Äù in Advances
in Neural Information Processing Systems, 2020.

[35] B. Amos, I. Jimenez, J. Sacks, B. Boots, and J. Z. Kolter, ‚ÄúDifferentiable
mpc for end-to-end planning and control,‚Äù in Advances in Neural
Information Processing Systems, 2018, pp. 8299‚Äî-8310.

[36] H. Sakoe and S. Chiba, ‚ÄúDynamic programming algorithm optimization
for spoken word recognition,‚Äù IEEE Transactions on Acoustics, Speech,
and Signal Processing, vol. 26, no. 1, pp. 43‚Äì49, 1978.

[37] C.-Y. Chang, D.-A. Huang, Y. Sui, L. Fei-Fei, and J. C. Niebles,
‚ÄúD3tw: Discriminative differentiable dynamic time warping for weakly
supervised action alignment and segmentation,‚Äù in IEEE Conference on
Computer Vision and Pattern Recognition, 2019, pp. 3546‚Äì3555.
[38] A. Vakanski, I. Mantegh, A. Irish, and F. Janabi-ShariÔ¨Å, ‚ÄúTrajectory
learning for robot programming by demonstration using hidden markov
model and dynamic time warping,‚Äù IEEE Transactions on Systems, Man,
and Cybernetics, vol. 42, no. 4, pp. 1039‚Äì1052, 2012.

[39] N. Vukovi¬¥c, M. Miti¬¥c, and Z. Miljkovi¬¥c, ‚ÄúTrajectory learning and
reproduction for differential drive mobile robots based on gmm/hmm and
dynamic time warping using learning from demonstration framework,‚Äù
Engineering Applications of ArtiÔ¨Åcial Intelligence, vol. 45, pp. 388‚Äì404,
2015.

[40] Z. Liang, W. Jin, and S. Mou, ‚ÄúAn iterative method for inverse optimal

control,‚Äù in Asian Control Conference, 2022, pp. 959‚Äì964.

[41] B. Akgun, M. Cakmak, K. Jiang, and A. L. Thomaz, ‚ÄúKeyframe-based
learning from demonstration,‚Äù International Journal of Social Robotics,
vol. 4, no. 4, pp. 343‚Äì355, 2012.

[42] B. Akgun, M. Cakmak, J. W. Yoo, and A. L. Thomaz, ‚ÄúTrajectories
and keyframes for kinesthetic teaching: A human-robot
interaction
perspective,‚Äù in ACM/IEEE international conference on Human-Robot
Interaction, 2012, pp. 391‚Äì398.

[43] A. V. Fiacco, ‚ÄúSensitivity analysis for nonlinear programming using
penalty methods,‚Äù Mathematical programming, vol. 10, no. 1, pp. 287‚Äì
311, 1976.

[44] A. Levy and R. Rockafellar, ‚ÄúSensitivity of solutions in nonlinear
programs with nonunique multiplier,‚Äù Recent Advances in Nonsmooth
Optimzation, pp. 215‚Äì223.

[45] S. G. Krantz and H. R. Parks, The implicit function theorem: history,
theory, and applications. Springer Science & Business Media, 2012.
[46] W. Jin, S. Mou, and G. J. Pappas, ‚ÄúSafe pontryagin differentiable
programming,‚Äù in Advances in Neural Information Processing Systems,
2021.

[47] W. Li and E. Todorov, ‚ÄúIterative linear quadratic regulator design for
nonlinear biological movement systems,‚Äù in International Conference
on Informatics in Control, Automation and Robotics, vol. 2, 2004, pp.
222‚Äì229.

[48] D. H. Jacobson and D. Q. Mayne, Differential dynamic programming.

Elsevier Publishing Company, 1970, no. 24.

[49] J. A. E. Andersson, J. Gillis, G. Horn, J. B. Rawlings, and M. Diehl,
‚ÄúCasADi ‚Äì A software framework for nonlinear optimization and opti-
mal control,‚Äù Mathematical Programming Computation, vol. 11, no. 1,
pp. 1‚Äì36, 2019.

[50] M. A. Patterson and A. V. Rao, ‚ÄúGpops-ii: A matlab software for solving
multiple-phase optimal control problems using hp-adaptive gaussian
quadrature collocation methods and sparse nonlinear programming,‚Äù
ACM Transactions on Mathematical Software, vol. 41, no. 1, pp. 1‚Äì37,
2014.

[51] C. D. Kolstad and L. S. Lasdon, ‚ÄúDerivative evaluation and computa-
tional experience with large bilevel mathematical programs,‚Äù Journal of
optimization theory and applications, vol. 65, no. 3, pp. 485‚Äì499, 1990.
John Wiley

[52] F. L. Lewis, D. Vrabie, and V. L. Syrmos, Optimal control.

& Sons, 2012.

[53] M. W. Spong and M. Vidyasagar, Robot dynamics and control.

John

Wiley & Sons, 2008.

[54] C. C. Aggarwal et al., ‚ÄúNeural networks and deep learning,‚Äù Springer,

vol. 10, pp. 978‚Äì3, 2018.

[55] A. W¬®achter and L. T. Biegler, ‚ÄúOn the implementation of an interior-
point Ô¨Ålter line-search algorithm for large-scale nonlinear programming,‚Äù
Mathematical programming, vol. 106, no. 1, pp. 25‚Äì57, 2006.

[56] J. B. Kuipers, Quaternions and rotation sequences. Princeton University

Press, 1999, vol. 66.

[57] T. Lee, M. Leok, and N. H. McClamroch, ‚ÄúGeometric tracking control of
a quadrotor uav on se(3),‚Äù in IEEE Conference on Decision and Control,
2010, pp. 5420‚Äì5425.

[58] R. Bellman, ‚ÄúDynamic programming,‚Äù Science, vol. 153, no. 3731, pp.

34‚Äì37, 1966.

[59] P. Hansen, B. Jaumard, and G. Savard, ‚ÄúNew branch-and-bound rules for
linear bilevel programming,‚Äù SIAM Journal on scientiÔ¨Åc and Statistical
Computing, vol. 13, no. 5, pp. 1194‚Äì1217, 1992.

[60] A. Sinha, P. Malo, and K. Deb, ‚ÄúA review on bilevel optimization: from
classical to evolutionary approaches and applications,‚Äù IEEE Transac-
tions on Evolutionary Computation, vol. 22, no. 2, pp. 276‚Äì295, 2017.
[61] K. Ji, J. Yang, and Y. Liang, ‚ÄúBilevel optimization: Convergence
analysis and enhanced design,‚Äù in International Conference on Machine
Learning, 2021, pp. 4882‚Äì4892.

[62] S. Ghadimi and M. Wang, ‚ÄúApproximation methods for bilevel program-

ming,‚Äù arXiv preprint arXiv:1802.02246, 2018.

Wanxin Jin is a postdoctoral researcher in the
GRASP Laboratory at the University of Pennsylva-
nia. He received the Ph.D. degree in Autonomy and
Control at Purdue University in 2021. From 2016
to 2017, he was a Research Assistant at Technical
University Munich, Germany. Wanxin‚Äôs research in-
terests include robotics, control, machine learning,
and optimization, with emphasis on learning, plan-
ning, and control of robots as they interact with the
world and humans.

Todd D. Murphey received his B.S. degree in
mathematics from the University of Arizona and
the Ph.D. degree in Control and Dynamical Systems
from the California Institute of Technology. He is a
Professor of Mechanical Engineering at Northwest-
ern University. His laboratory is part of the Center
for Robotics and Biosystems, and his research inter-
ests include robotics, control, machine learning in
physical systems, and computational neuroscience.

This is a preprint. The published version can be accessed at IEEE Transactions on Robotics.

20

Dana Kuli¬¥c conducts research in robotics and
human-robot
interaction (HRI), and develops au-
tonomous systems that can operate in concert with
humans, using natural and intuitive interaction strate-
gies while learning from user feedback to improve
and individualize operation over long-term use. Dana
Kuli¬¥c received the combined B. A. Sc. and M. Eng.
degree in electro-mechanical engineering, and the
Ph. D. degree in mechanical engineering from the
University of British Columbia, Canada, in 1998 and
2005, respectively. From 2006 to 2009, Dr. Kuli¬¥c
was a JSPS Post-doctoral Fellow and a Project Assistant Professor at the
Nakamura-Yamane Laboratory at the University of Tokyo, Japan. In 2009,
Dr. Kuli¬¥c established the Adaptive System Laboratory at the University of
Waterloo, Canada, conducting research in human robot interaction, human
motion analysis for rehabilitation and humanoid robotics. Since 2019, Dr.
Kuli¬¥c is a professor and director of Monash Robotics at Monash University,
Australia. In 2020, Dr. Kuli¬¥c was awarded the ARC Future Fellowship.
Her research interests include robot learning, humanoid robots, human-robot
interaction and mechatronics.

Neta Ezer is the Northrop Grumman Corporate
Director of Strategic Planning. Dr. Ezer previously
served as Technical Fellow and Chief Technologist
for Human-Machine Teaming in Northrop Grumman
Mission Systems and as an ArtiÔ¨Åcial Intelligence
(AI) Architect for the Northrop Grumman AI Cam-
paign. Prior to Northrop Grumman, Dr. Ezer was a
Senior Human Engineering Researcher at the Futron
Corporation, working on NASA Orion, International
Space Station and human-robot interaction research.
She served as Assistant Professor of Industrial De-
sign at the Georgia Institute of Technology. Dr. Ezer has over 15 years of
experience in human factors, user experience and AI, with over 40 published
papers and proceedings in these areas. Dr. Ezer holds a B.S. in Industrial
Design and M.S. and PhD degrees in Engineering Psychology from the
Georgia Institute of Technology.

Shaoshuai Mou is an Associate Professor in the
School of Aeronautics and Astronautics at Purdue
University. Before joining Purdue, he received a
Ph.D. in Electrical Engineering at Yale University
in 2014 and worked as a postdoc researcher at
MIT for a year after that. His research interests
include multi-agent system, control and learning,
robotics control, human-robot teaming, resilient au-
tonomy, and also experimental research involving
autonomous air and ground vehicles. Dr. Mou co-
directs Purdue University‚Äôs Center for Innovation in
Control, Optimization and Networks (ICON), which aims to integrate classical
theories in control/optimization/networks with recent advances in machine
learning/AI/data science to address fundamental challenges in autonomous
and connected systems.

