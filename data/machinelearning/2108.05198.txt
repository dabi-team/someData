Natural Language-Guided Programming

Geert Heyman
geert.heyman@nokia-bell-labs.com
Nokia Bell Labs
Belgium

Pascal Justen
pascal.justen@nokia-bell-labs.com
Nokia Bell Labs
Belgium

Rafael Huysegems
rafael.huysegems@nokia-bell-labs.com
Nokia Bell Labs
Belgium

Tom Van Cutsem
tom.van_cutsem@nokia-bell-labs.com
Nokia Bell Labs
Belgium

1
2
0
2

t
c
O
7

]
E
S
.
s
c
[

2
v
8
9
1
5
0
.
8
0
1
2
:
v
i
X
r
a

Abstract
In today’s software world with its cornucopia of reusable
software libraries, when a programmer is faced with a pro-
gramming task that they suspect can be completed through
the use of a library, they often look for code examples using
a search engine and then manually adapt found examples to
their specific context of use. We put forward a vision based
on a new breed of developer tools that have the potential
to largely automate this process. The key idea is to adapt
code autocompletion tools such that they take into account
not only the developer’s already-written code but also the
intent of the task the developer is trying to achieve next,
formulated in plain natural language. We call this practice of
enriching the code with natural language intent to facilitate
its completion natural language-guided programming.

To show that this idea is feasible we design, implement and
benchmark a tool that solves this problem in the context of
a specific domain (data science) and a specific programming
language (Python). Central to the tool is the use of language
models trained on a large corpus of documented code. Our
initial experiments confirm the feasibility of the idea but also
make it clear that we have only scratched the surface of what
may become possible in the future. We end the paper with
a comprehensive research agenda to stimulate additional
research in the budding area of natural language-guided
programming.

CCS Concepts: • Software and its engineering → Inte-
grated and visual development environments; • Com-
puting methodologies → Natural language processing.

Keywords: code completion, code prediction, natural
language-guided programming, example-centric program-
ming

1 Introduction
In many areas of software development, developers find
themselves spoiled with thousands of readily available soft-
ware libraries (also commonly called modules or packages).
The growing practice of building software out of open source

components further adds to that trend1, making modern
software stacks highly diverse and fast-evolving.

To make this more concrete, consider a data scientist using
Python for data analysis. A trained data scientist familiar
with the ecosystem will typically combine a variety of li-
braries to complete any given job. She might use Python’s
built-in libraries to download or manipulate raw data files,
use the popular Pandas library to manipulate tabular data,
use scientific computing packages such as NumPy to manipu-
late numerical data, use large machine learning frameworks
such as Scikit-learn to train predictive models on the data
and finally use one of the many popular data visualization
libraries such as Matplotlib or Plotly to chart her insights.
If the data scientist is in need of training sophisticated
deep neural network models to fit the data, she is spoiled
with choice among multiple large and high-quality libraries
that will help her do that with just a few lines of code. Two of
the most widely used libraries are Tensorflow and Pytorch.
Unfortunately for our data scientist, these major machine
learning toolkits are constantly tweaking their APIs. In the
last three years alone, Tensorflow has received no less than
16 major releases while Pytorch saw 9 major releases2.

Keeping up with the large and ever-changing “API surface”
of all of these combined library dependencies poses a serious
learning challenge to any prospective data scientist. This
problem is neither specific to data science nor is it specific to
Python. Other domains of software development feature a
similarly rich and sprawling software library ecosystem [44].

1.1 Example Embedding to the Rescue?

Luckily the growing body of APIs is accompanied by grow-
ing sources of online documentation. The proliferation of
online code examples embedded in library documentation,
tutorials or Q&A websites such as Stack Overflow3 has led to
a programming phenomenon called Example Embedding [4],

1According to a 2019 survey ran by Open Source consultancy firm Tidelift
up to 93% of surveyed applications used open source components and up
to 70% of the codebases of surveyed applications consisted of open source
code. https://tidelift.com/subscription/managed-open-source-survey
2https://medium.com/analytics-vidhya/pytorch-is-growing-tensorflow-is-
not-6986c5e52d6f, retrieved April 2021.
3www.stackoverflow.com

 
 
 
 
 
 
Geert Heyman, Rafael Huysegems, Pascal Justen, and Tom Van Cutsem

Figure 1. In natural language-guided programming, a programmer formulates tasks using natural language in a specific code
context. An NLGP assistant suggests relevant code that matches the task and the context.

also known as example-centric programming [5]. A developer
engages in example embedding when they search for code
examples (in local or online repositories), copy-paste the
code into their code editor and then adapt the code to fit
their specific needs and their specific code context.

The activity of example embedding is largely done man-
ually, with little dedicated tool support, which can make it
time-consuming and error-prone. Indeed, prior empirical
studies in software engineering found that up to 35% of a de-
veloper’s worktime can be spent on code search [43]. There
is also evidence that suggests that code found in documenta-
tion or online knowledge bases is rarely immediately usable.
For example, one study of Stack Overflow found that a mere
25.61% of Python code snippets could be readily executed,
and the figures were even worse for other languages [47].
Even when the developer does find a high-quality code exam-
ple, they must still edit the code to fit their specific context,
e.g. by renaming variables or deleting superfluous statements.
These edits are an opportunity for bugs to creep into the
developer’s codebase.

Given these observations, we conjecture that automating
the activity of example embedding has the potential to pos-
itively affect both developer productivity as well as code
quality.

1.2 Automating Example Embedding

We envision that this common practice of example embed-
ding will become more and more automated through new
tools that leverage advances in machine learning and natural
language processing. We will refer to the practice of using
tools to automate the example embedding process as natu-
ral language-guided programming (abbreviated NLGP). In a
coding environment that supports natural language-guided
programming, when the programmer is faced with a task
that they believe can be solved using a library, they simply
state their intent using natural language in-line in the code
and an NLGP tool suggests code that 1) addresses the task

and 2) fits the context, choosing variable and function names
that match the already existing code. We will refer to such a
developer tool as an NLGP assistant. The diagram in Figure 1
describes the basic NLGP workflow.

Much like refactoring assistants in modern IDEs now help
software developers more quickly and reliably apply refac-
torings to their codebase, we envision that natural language-
guided programming tools will help developers more quickly
and reliably perform example embedding. The goal of an
NLGP assistant is to help programmers write idiomatic code
for tasks that can be solved using available libraries.

1.3 Paper Contributions

• We demonstrate our vision of natural language-guided
programming in the domain of data science and ma-
chine learning using Python libraries (Section 2).
• We contribute the design and implementation of an
NLGP assistant for this domain. The core of our ap-
proach is based on language models (Section 3) which
require training on a large corpus of documented code.
We develop three language model variants to study the
impact of natural language intent on prediction qual-
ity together with a benchmark and a user evaluation
(Section 4).4

• We articulate a Research Agenda for natural language-
guided programming: what are key open research
questions that need to be addressed to move this field
forward? (Section 6).

2 Case Study: NLGP for Data Science and

Machine Learning in Python

In the introduction we described how a data scientist would
typically use a multitude of Python libraries to do their job.
Let us now walk through a concrete experience of what it

4The models are shared on https://huggingface.co/Nokia, and the bench-
mark and user annotations can be downloaded from https://zenodo.org/
record/5384768#.YTDsN9MzZUJ.

Natural Language-Guided Programming

would be like to perform data analysis using natural language-
guided programming.

Before we get started, we give a brief background on the
typical programming environment used by data scientists.

2.1 Background: the Python Data Science Stack

In recent years Python has become the language of choice
for an increasing number of data scientists, data engineers,
and machine learning researchers.5 As mentioned in the in-
troduction, one reason for this is Python’s large ecosystem of
scientific computing libraries, sometimes called the “Python
data science stack” or simply the “data stack”.6 Table 1 lists
key projects in this stack.

Table 1. Key projects in the Python Data Stack

Project
NumPy

Purpose
N-dimensional arrays and extensive math
operations.
Advanced math (solvers, optimizers).
Rich data manipulation for tabular data.
2D data plotting.

SciPy
Pandas
Matplotlib
Scikit-learn Comprehensive machine learning toolkit.
Interactive notebooks with text, code,
Jupyter
math and graphics.

A Jupyter notebook is an interactive coding environment
composed of cells. The two most commonly used cells are
code cells and markdown cells. Markdown cells contain sim-
ple markup text and can be rendered to a variety of formats.
Code cells may contain code in one of the languages sup-
ported by the Jupyter protocol. Here we will focus only on
Python code.

A code cell can be executed after which the result of the
code is inserted as output in the notebook. Jupyter has built-
in support to render certain program values as rich media
(such as tables or graphics). This allows a data scientist to
easily inspect the intermediate output of their data transfor-
mations. This interactive style of programming aligns well
with NLGP because it allows for programmers to rapidly test
and explore auto-generated code by executing it.

2.2 Natural Language-Guided Programming in

Jupyter

Let us put ourselves in the position of Dana the data scientist.
Dana is tasked with analyzing stock market prices stored in
a comma-separated value (CSV) file.

Dana knows that Pandas is the go-to library to manipulate
tabular data like this, so she starts by importing the library:

import pandas as pd

5https://towardsdatascience.com/top-programming-languages-for-data-
science-in-2020-3425d756e2a7, retrieved April 2021
6https://hub.packtpub.com/python-data-stack/, retrieved April 2021.

Figure 2. Jupyter notebook with the result of executing the
code cell after inserting the NLGP assistant’s suggested code.

The Pandas library offers a data structure called a
“dataframe” to manipulate tabular data. Dana now needs
to read the CSV file into memory and convert it into such
a dataframe. She knows that Pandas has an API for this but
forgot the details. Rather than looking up the right API call
in the documentation or searching the Web for an example,
using an NLGP assistant Dana can simply write her intent
as a single-line comment in the code:

import pandas as pd
# read stock_data.csv

Dana then triggers her NLGP assistant using a hotkey,
which looks at her code and her intent and produces the
following suggestion:

df = pd.read_csv('stock_data.csv', delimiter=',')

That suggestion looks relevant so Dana hits ’enter’ to

insert the line into the code cell:

import pandas as pd
# read stock_data.csv
df = pd.read_csv('stock_data.csv', delimiter=',')

Once the suggested code is merged, Dana is free to modify
it. Perhaps the CSV file used a ’;’ separator rather than a ’,’
separator. Dana can easily update the suggested parameter.
At this point, Dana can run the code to inspect the contents
of df and verify that the data was parsed correctly. Figure 2
shows the output of the above code cell in a Jupyter notebook
environment. The call to df.head() was added to visualize
the first five rows of the dataframe.

Now Dana would like to select only a subset of the
columns. Again, rather than looking up how to do this in
the documentation, she can state her intent as a comment:

# select columns 'Date', 'Open' and 'Close'

The NLGP assistant suggests the following lines of code:

df = df[['Date', 'Open', 'High', 'Low', 'Close']]
df.head()

Even though the suggestion contains some spurious
columns, it looks mostly right to Dana, so she incorporates
the suggestion and makes a few edits:

# select columns 'Date', 'Open' and 'Close'
df = df[['Date', 'Open', 'Close']]
df.head()

Dana can continue to write code in this way by stating
her intent upfront and accepting relevant code suggestions
into her notebook:

# convert 'Date' column to datetime value
df['Date'] = pd.to_datetime(df['Date'])

# add a new column called 'Month'
df['Month'] = df['Date'].dt.month

# group by Month and compute the average
df.groupby('Month').mean()

# chart the data
df.plot()

In the above code, the comments in blue indicate the pro-
grammer’s “intent”. The NLGP assistant is invoked after
entering the intent. The code following each intent is gener-
ated by the NLGP assistant. The code suggestions presented
above are actual code predictions made by the tool intro-
duced in Section 4.

The above NLGP session focused on assisting Dana with
the Pandas library only, but we expect an NLGP assistant
to be able to offer suggestions for tasks requiring other li-
braries through the same unified interface. For example,
Dana might also use the popular Scikit-learn library for
machine learning tasks. Example intents that she might use
to describe such tasks include:

• “split the data in a training and a validation set”
• “cluster the data using K-means”
• “plot a confusion matrix of the classifier”

The user stating their intent is an integral part of natural
language-guided programming. However, we make no as-
sumptions on how that intent is communicated to the NLGP
assistant. In the NLGP session described above, the developer
states their intent as an in-line comment in the code. Other
NLGP assistants may offer a text field separate from the code
to guide the code autocompletion. One potential benefit of
inlining the intent in the code is that it self-documents the
interaction with the tool, forming a potential source of future
data to better learn the translation from intent to code.

In the next section, we introduce language models as a
general technique for predicting text or code. Subsequently,

Geert Heyman, Rafael Huysegems, Pascal Justen, and Tom Van Cutsem

we show how an NLGP assistant for Python can be built
using this technique.

3 Background: Language Models
Language models are statistical models that estimate the
probability of a given sequence of tokens, such as words
in an English-language text with respect to a reference cor-
pus. This is a very general problem with many applications,
including text autocompletion, speech recognition and ma-
chine translation [20]. In the past decade, advances in the
field of deep learning, such as the Transformer model archi-
tecture [38], have significantly improved the effectiveness of
language models on practical applications. One particularly
successful set of language models based on Transformers is
the GPT (Generative Pretrained Transformer) model family
including GPT-2 [29] and its successor GPT-3 [6]. GPT mod-
els have been trained on large volumes of text from public
Web pages. Their capability to generate seemingly human-
written text has received widespread attention both within
and outside the research community.7

Hindle et al. [17] were the first to formally observe that
code, like natural language, is repetitive and predictable and
that language models can also be used to create effective
statistical models of source code, paving the way towards
new kinds of code completion tools. Hindle et al.’s work was
based on (adapted versions of) n-gram models and in recent
years there has been an ongoing debate about what type
of language models (based on n-grams or deep learning) is
best for modeling source code [15, 21]. In recent years, in
line with what has been observed in NLP in general, lan-
guage models based on deep learning such as Transformers
have been shown to achieve production-quality levels of
code completion, with companies offering code completion
products based on this technology.8

We now review how language models can be used to gen-
erate sequences that fit a given context. A language model es-
timates the probability distribution 𝑃 (𝑋𝑛 |𝑋1, ..., 𝑋𝑛−2, 𝑋𝑛−1)
of the n𝑡ℎ token 𝑋𝑛 given the previous tokens in the sequence.
With this conditional probability distribution, we can pre-
dict the most likely token in a given context. By adding the
predicted token to the context, we can iteratively expand
the prediction to form sequences of arbitrary length. Gener-
ating the most likely sequence is intractable9 so in practice
approximate algorithms such as beam search [12] are used
to explore the search space.

7See e.g. this New York Times article dd. July 29, 2020: https://www.nytimes.
com/2020/07/29/opinion/gpt-3-ai-automation.html
8See Tabnine blog dd.
July 15, 2019 https://web.archive.org/web/
20201204055827if_/https://www.tabnine.com/blog/deep, archived Dec. 4,
2020 and GitHub CoPilot, https://copilot.github.com/, retrieved Aug. 3, 2021.
9The space and time complexity for generating the most likely sequence
scales exponentially: 𝑂 ( |𝑉 |𝐿), where |𝑉 | is the vocabulary size and 𝐿 is
the sequence length.

Natural Language-Guided Programming

To make the problem tractable, most language mod-
els make the simplifying assumption that 𝑋𝑛 only
depends on a window with the 𝐶 previous tokens:
𝑃 (𝑋𝑛 |𝑋1, ..., 𝑋𝑛−2, 𝑋𝑛−1) = 𝑃 (𝑋𝑛 |𝑋𝑛−𝐶, ..., 𝑋𝑛−2, 𝑋𝑛−1). For in-
stance, GPT-2 language models can process a maximum of
1024 tokens at a time, which means that 𝐶 can be at most
1023.

Language models can be applied to different types of token
sequences: tokens can correspond to words, subword units,
or individual characters. When applying language models to
source code, where the number of unique identifiers tends to
be large [17], subword units are desirable. When we discuss
training of language models on code in this work, we assume
the use of byte-pair encoding (BPE) [11, 32] as used in GPT-
2. BPE is a compression algorithm for splitting words into
subword tokens such that the most frequent (sub)words are
tokenized as a single token. For example, applying the GPT-
2 tokenizer to the code string ‘b = np.zeros(10)’ would
result in the following subword units: ‘b’, ‘␣=’, ‘␣np’, ‘.’ ,
‘zer’, ‘os’, ‘(’, ‘10’ and ‘)’.

In the next section, we describe how an effective NLGP

assistant can be built based on the GPT-2 model.

4 Building an NLGP Assistant using

Language Models

To study the feasibility of NLGP we build and evaluate a
prototype of an NLGP assistant for Python. Our NLGP as-
sistant uses a language model to autocomplete code cells
based on both existing code in the cell, as well as the de-
veloper’s intent, specified as a comment (as introduced in
Section 2). The language model is trained on a collection of
preprocessed Jupyter notebooks (details of our dataset are
covered in Section 4.2).

We first introduce three strategies for preprocessing the
data, leading to three distinct language models. Next, we
cover more details on how we prepare the data and train the
models. Finally, we report on an initial user study to evaluate
the quality of the models’ code predictions.

4.1 Language Models for NLGP

The starting point for all of the language models trained in
this paper is the GPT-2 Medium model checkpoint released
by OpenAI [29]. The model checkpoint was pretrained by
OpenAI on general-purpose English-language text crawled
from the Web. From preliminary experiments we concluded
that starting from a pretrained model gave significantly bet-
ter results than starting from an equivalent randomly initial-
ized transformer model that was not pretrained on text.

To train a language model to be able to autocomplete code
based on existing code and a natural language intent, we need
relevant training data. The challenge here lies in finding a suf-
ficiently large amount of code that is self-documented with
the developer’s intent. Given that there exists no sufficiently

large dataset of Python code that is explicitly annotated with
the developer’s intent using natural language10, we need
creative ways to teach the language model how to associate
natural language intent with code. One assumption is to rely
on textual comments in the code. We consider three distinct
ways to use comments in code to train language models:

No Comments The no comments model is trained on
a dataset where all original comments are stripped from
the training data. This model serves as a baseline and will
allow us to quantify how important it is to consider natural
language intent in addition to pure code.

Docstring Comments The docstring model is trained
on a dataset where we also first strip all comments from the
training data. However, here we annotate a selection of call
sites with synthetic comments. These comments contain a
summary of the called method’s or function’s docstring. The
intuition is that a docstring typically contains a short one-
sentence description of the intent of the function or method.
We describe this procedure in detail in Section 4.4.

By annotating the call site with the docstring, we hope
to teach the model to associate code context preceding the
call with keywords from the docstring and the subsequent
method or function call. This setup is meant to assess the
feasibility of NLGP models in domains where code is not
documented with relevant comments.

Natural Comments The natural model is trained on
comments interleaved with code as they naturally occur in
Jupyter notebooks. This includes text in markdown cells as
well as in-line comments in code cells. In this dataset no call
sites are annotated with docstrings.

4.2 Jupyter Notebook Dataset

Jupyter notebooks are a mix of executable code and descrip-
tive text. This makes them an interesting source for collecting
training and evaluation data for an NLGP assistant. To con-
struct a dataset, we searched GitHub for all projects that
contain at least one Jupyter notebook, have a permissive li-
cense and received at least one star. Next, we apply a heuristic
to filter out project forks: when multiple projects have the
same name, only the project with the most stars is retained.
We then download all notebooks in the project and convert
them to .py source files using the nbconvert tool.11,12 This
tool converts any non-code cells into inline comments. We
parse the .py files using a Python3 parser and reject any
files that contain parse errors. The resulting files are split
90/10 across a training and evaluation set. We ensure that
notebooks that belong to the same GitHub project end up in
the same split. In this way, we obtain 297,845 and 32,967 .py
files for training and evaluation purposes respectively.

10We survey relevant datasets in Section 5.
11https://nbconvert.readthedocs.io/en/latest/
12We skipped notebooks containing code written in languages other than
Python (e.g. Julia, R), as well as notebooks under .ipynb_checkpoint/ folders.

Each .py file in the training split was further preprocessed

and cleaned using following heuristics:

• Any markdown content before the first code cell de-

limiter is removed;

• Comments that were inserted by nbconvert to delimit
code cells (# In [], # In [1] , # In [2], etc. ) are replaced
by a special <|cell|> token;

• Comments are separated from the subsequent code
by a special <|endofcomment|> token (more details
below);

• Multi-line comments are truncated to a maximum of

two lines;

• Markdown header symbols, which are inserted by the
nbconvert tool, are stripped (e.g., # ## some title
is converted to # some title);

• Non-English comments are stripped. We used the cld3

tool13 to automatically detect the language;
• Empty cells and empty comments are removed.
• Spaces are replaced by special whitespace tokens (e.g.,
’ is replaced by a single ’<|4space|>’ token).

’

4.3 Language Model Setup for Intent-Guided Code

Prediction

To use a language model to generate predictions in an NLGP
context, two issues remain: 1) What is the stopping criterium
(when has the model predicted enough code to address the
intent)?; 2) How to force the model to predict source code
instead of autocompleting the inline comment with more
natural language? If the model were to autocomplete the
intent, it may inadvertently change its meaning, which is
undesirable.

To address these challenges, we introduce additional sym-
bols <|endofcomment|> and <|cell|> to encode structural in-
formation, as illustrated in the following example:

...
# Choose number of features automatically
# use RFECV to select features <|endofcomment|>
rfe = RFECV(random_forest, n_jobs=−1, step=1)
rfe.fit(X_train, y_train)

feature_scores['RFECV'] = X.shape[1] − \

rfe.ranking_.astype(float).reshape(−1, 1)

<|cell|>
# output number of features <|endofcomment|>
print("#features=", np.sum(rfe.support_))
<|cell|>

An <|endofcomment|> token is inserted after every in-
line comment that is followed by source code. That is, for
multiple successive inline comment lines, we only insert the
token after the last comment line. At prediction time, we
append this symbol to the end of the user intent to prompt

13https://github.com/google/cld3

Geert Heyman, Rafael Huysegems, Pascal Justen, and Tom Van Cutsem

the model to predict source code rather than to autocom-
plete the comment. A <|cell|> token is inserted at the end
of every Jupyter notebook code cell. At prediction time, no
more tokens are predicted after the model has predicted a
<|cell|> token. We found that this simple heuristic works
well in practice, but there is room to experiment with more
sophisticated stopping criteria in future work.

As a final step, we concatenate all preprocessed .py files
into a single training file using the <|endoftext|> symbol
to encode the original file boundaries.

We generate predictions using beam search with a
beamwidth of 3, where the prediction of the <|cell|> token
signals that a beam hypothesis is complete. We enforce that
the model predicts between 10 and 150 tokens by setting
the probability of the stopping token to zero for the first 10
tokens and by stopping the beam search procedure after the
beam hypotheses are 150 tokens long. The maximum context
length is set to 700 tokens.

We made a slight adjustment to the GPT-2 model and
the GPT-2 tokenizer to ensure that our special tokens
(<|4space|>, <|endoftext|> , <|endofcomment|>, etc.)
are tokenized as a single token and are encoded with their
own set of (embedding) parameters that are initialized at
random and trained from scratch. We use the transformers
library [42] to make these changes.

In Section 4.5, we describe how we used the evaluation

split to create a labeled test set.

4.4 Docstring Comment Injection
The docstring model is trained on a synthetic dataset where
all naturally occurring comments in the training data are
first removed, after which a random sample of call sites is
instrumented with new comments taken from docstrings.
More specifically, when a call is made to a documented li-
brary API, an additional inline comment is added to the code,
describing the purpose of the call. The goal is to augment
the source code with comments that capture the intent of
the calls using short natural language statements.

For example, given the following snippet of Python code:

from sklearn.cluster import KMeans
k = KMeans()
k.fit(Xtrain)
y = k.predict(Xtest)

The goal is to transform it into:

from sklearn.cluster import KMeans
# K−Means clustering
k = KMeans()
# Compute k−means clustering
k.fit(Xtrain)
# Predict closest cluster for each sample
y = k.predict(Xtest)

Natural Language-Guided Programming

Figure 3. High-level process flow to inject Python docstrings into code.

Figure 3 depicts the high-level process that we followed
to implement this transformation. The first objective is to
create a mapping from the names of callable program entities
(functions, methods, constructors) to their docstrings:

1. From the python source files in the training set, the
root module names are extracted and counted. The
250 most frequently used, non-standard Python library
root module names are kept.

2. A blank virtual environment is created in which pack-
ages, together with their package dependencies are in-
stalled using the ‘pip‘ command [1]. For most packages,
pip is able to install via root module name (e.g. numpy,
sklearn, etc). Only a few need an explicit module-
package name mapping.

3. Using a custom crawler program all installed packages
and standard python libraries/packages are recursively
scanned to find all callable program entities. For each
harvested entity, the fully qualified pathname (FQPN)
and the first sentence from the associated docstring
are automatically extracted and stored in the mapping
table.

4. We obtain a mapping from FQPN to docstring titles for
64.6% of the visited callable entities. Entities without
an associated docstring are ignored and not recorded
in the mapping.

Table 2. Example entity-docstring mappings

Fully qualified path name
sklearn.cluster.KMeans()
sklearn.cluster.KMeans().predict()

Docstring title
’K-Means clustering’
’Predict closest cluster each sample ...’

Table 2 lists a short fragment of the entity-docstring map-
ping for two entities from the sklearn library. In a second
phase, we parse the source files in the dataset and visit all

call sites using an AST walker. For each call site, we try to
resolve the call to a named entity, e.g. the call k.predict()
would resolve to sklearn.cluster.KMeans().predict().
Because of Python’s dynamic typing, we are only able to
resolve a subset of calls using a basic program flow analysis.
Still, this allows us to resolve 51.3% of call sites to one of the
entity names stored in the mapping.

In a final phase, the AST annotator chooses a random
sample of resolved call sites and then inserts the associated
docstring title in front of the call. The docstring is always
inserted on the previous line of the statement enclosing the
visited call site. In our experiments we chose a sampling rate
of 20%. A deeper study of the effect of the sampling rate on
the prediction quality is left as future work.

4.5 Creating an NLGP Benchmark

To assess the prediction quality of an NLGP assistant we
need a good benchmark. As no such benchmark exists, we
set out to create our own.

A benchmark for NLGP requires realistic scenarios (test
cases) where an NLGP assistant needs to complete the code
based on a natural language intent. Each test case is a triplet
𝑐/𝑖/𝑡 containing a code context 𝑐; a natural language intent
𝑖, provided in the form of an inline comment; and a target
code snippet 𝑡, a reference code snippet that addresses the
intent and is a natural completion of the code context 𝑐.

We created a benchmark containing such triplets in two
stages. In a first generation stage we automatically mine
candidate triplets from the Jupyter notebook dataset. In a
second curation stage we filter remaining candidates based
on human review.

Generation Stage To create realistic and unbiased 𝑐/𝑖/𝑡
triplets, we chose to mine triplets from our Jupyter notebook
dataset. More specifically, we sample candidate test cases

only from source files that were set aside for evaluation (i.e.
not occurring in the training set):

Table 3. Summary statistics for the NLGP benchmark. LoC
stands for ‘lines of code".

Geert Heyman, Rafael Huysegems, Pascal Justen, and Tom Van Cutsem

1. We scan the source files for lines that only contain an

inline comment and whitespace.

2. Next, we filter out non-English comments and com-
ments that are longer than 10 tokens. This cut-off was
informed by studying the query lengths of the user
study done by Xu et. al [46]: over 97% user queries
consisted of 10 tokens or less.

3. From the remaining comments, we then sample at
random and create a set of candidate test cases 𝑐𝑐 /𝑖𝑐 /𝑡𝑐 :
• The candidate context 𝑐𝑐 is extracted from the start
of the comment’s source file up to the line of the
comment.

• The candidate intent 𝑖𝑐 is set to the sampled com-

ment including any leading whitespace.

• The candidate target code 𝑡𝑐 is set to all the code
(excluding comments) that follows the comment.

4. We filter out candidates that overlap with code in the
training set. Specifically, we concatenate the last three
non-empty lines in the candidate context with the
candidate intent and check if the resulting piece of
code occurs in the training dataset. If an exact match
is found, the candidate is dropped.

Curation Stage Mined candidate test cases 𝑐𝑐 /𝑖𝑐 /𝑡𝑐 were
reviewed by human annotators and refined into representa-
tive test cases 𝑐/𝑖/𝑡:

1. We generate three non-overlapping batches of can-
didate test cases. Each batch contained 200 distinct
cases.

2. The 3 batches were assigned for review to 9 human
reviewers. Each batch was assigned for review to a
group of 3 reviewers. As such, a total of 600 candidate
test cases were reviewed, each case receiving 3 reviews.
3. Annotators were asked to decide i) whether the candi-
date test case is relevant, ii) were allowed to slightly
rephrase the candidate intent (e.g. rephrasing a com-
ment in the code like “and now let’s plot the data” to
a more succinct intent like “plot the data”), and iii)
were requested to mark in the candidate target code
𝑡𝑐 which specific lines of code 𝑡 best addressed the
intent. Appendix A.1 provides further details about
the annotation process, including the detailed guide-
lines that were given to the annotators, a screenshot
of the annotation interface, and statistics about the
inter-annotator agreement.

4. When 2 out of 3 reviewers judged that a candidate
test case is relevant and the difference between their
respective target code selections 𝑡𝑐 was not more than
2 lines, the test case was added to the benchmark. 201
out of 600 code snippets were selected in this way.
5. We postprocess the resulting test cases such that:

number of samples
average LoC context
average LoC target code
average # tokens in intent

201
268
2.45
5.39

• All the code before the first line of target code is

moved to the context

• Import statements in the context that are only re-
quired for the target code are moved to the target
code because it is unrealistic to assume a user will
have written such import statements before issuing
a query

• All comments in the target code (if any) are stripped
After going through the curation stage we end up with a
benchmark of 201 representative test cases 𝑐/𝑖/𝑡 that we can
now use to validate the quality of code predictions made by
the models. Table 3 displays some key statistics about the
benchmark.

4.6 Evaluation

We now assess the performance of our language models on
the NLGP benchmark. Recall that we trained models on three
distinct datasets:

No comments A model trained on only code, with com-
ments stripped out. This model serves as a baseline to
measure the importance of natural language intent;

Docstring A model trained on code augmented with in-
jected docstring comments on 20% of calls to APIs
from libraries documented with pydoc docstrings.

Natural A model trained on code including all the com-

ments that occur naturally in the code.

For each model, we create a code prediction 𝑝 for each 𝑐/𝑖/𝑡
triplet in our benchmark. We provide concrete examples of
𝑐/𝑖/𝑡/𝑝 cases in Appendix A.3. The average prediction latency
on an 11GB GeForce GTX 1080 Ti GPU was 1.87 seconds14.
To assess how well the generated code prediction 𝑝 com-
pares to the reference code 𝑡, we set up a human evaluation
study. We first introduce the study, then discuss how the
human evaluation results correlate with standard text com-
parison metrics such as BLEU [28].

4.6.1 Human Evaluation Study: Setup. Using the pre-
pared 𝑐/𝑖/𝑡/𝑝 entries, we ran a small human evaluation study
where the first three authors of the paper manually scored
the code predictions of each model across four dimensions:

14In work following the reported experiments, we used the ONNX runtime
framework (https://www.onnxruntime.ai/) to bring down the average pre-
diction latency of these models down to 0.3 seconds, which is sufficiently
fast to enable interactive code predictions within an IDE.

Natural Language-Guided Programming

usefulness, coverage, precision, and compatibility. Specifi-
cally, for the predictions of each model on 100 test cases in
our benchmark, each participant rates the following state-
ments on a 4-point scale (Strongly disagree, Disagree, Agree,
Strongly agree):

Usefulness The predicted code is helpful for implementing

the given intent;

Coverage The predicted code completely covers the intent;
Precision The predicted code mostly contains code that is

relevant to the intent;

Compatibility The predicted code is compatible with the
code context (e.g., it reuses variable names from the
context if appropriate)

To avoid that the annotators have a bias to a certain model,
the predictions were presented in random order and the
annotation interface did not display the model name.

4.6.2 Human evaluation study: results. Figure 4 re-
ports the answer distributions to the survey questions for
each model. The results indicate that the models trained on
comments significantly outperform the no comments model.
As expected, it is more difficult for models to guess the pro-
grammer’s intent from only undocumented code.

Both the docstring and natural models exhibit decent
performance with similar overall scores even though the
natural model results in a better intent coverage. This dif-
ference can be attributed to the fact that the original inline
comments are more diverse than docstring titles support-
ing the fact that the natural model can translate a more
diverse set of intents. The relatively small gap between the
two does indicate that even in domains where code is not
heavily documented the NLGP approach is feasible when a
procedure similar to our docstring-injection is feasible. The
models score particularly well w.r.t. compatibility, implying
that the models can generate code predictions customized
to the code context.

Finally, the usefulness scores of both the docstring and
natural models reflect that the majority of their predictions
are considered useful. These results support the feasibility of
natural language-guided programming and suggest that our
NLGP assistant prototype may already be helpful to support
data scientists in practice.

4.6.3 Metrics. Running human evaluation studies to val-
idate code prediction models is time-consuming. For this
reason, it is desirable to have good metrics that can automat-
ically score predicted code. One way to accomplish this is to
compare the predicted code with the reference target code
(also called the “ground truth” code).

Previous work to measure the quality of predicted code [8,
13, 19, 45, 50] mostly treats the code as plain text and uses the
Bilingual evaluation under study (BLEU) score [28]. BLEU is
a standard metric in natural language text translation. The

Table 4. Average BLEU and IoU score on the bench-
mark, along with their correlation coefficients with human-
assigned usefulness scores.

model
natural
docstring
no comments
all models

BLEU IoU 𝜌 BLEU,H 𝜌 IoU,H
0.62
0.25
0.57
0.18
0.73
0.06
0.63
0.16

0.70
0.65
0.74
0.69

0.45
0.40
0.27
0.37

BLEU score is based on the ratio of common n-grams found
in the prediction and the reference text (the target code).

Intersect-over-Union (IoU), also known as Jaccard dis-
tance, between sets of tokens derived from code fragments
has also been used to evaluate code prediction tools. For
example, Murali et al. compute Jaccard distance between
sets of API calls occurring in the predicted and the target
code [26].

We computed both BLEU and IoU metrics for the code
predictions in our benchmark and correlated them with the
usefulness scores that were assigned in the human evaluation
study. Our goal here is to measure how well these metrics
can act as a proxy for the average usefulness score assigned
by human experts.

Before applying the metrics, the predicted code and the
target code are tokenized using the tree-sitter library [37].
The tokenization is illustrated in the following example.
Original code:

from matplotlib import pyplot as plt
plt.hist(means_100)
plt.show()

Resulting tokens used to compute the metrics:

from matplotlib import pyplot as plt plt . hist
( means_100 ) plt . show ( )

Table 4 shows the metric results for each model. 𝜌 BLEU,H
and 𝜌 IoU,H denote the Pearson correlation between the met-
rics and human judgments for usefulness, computed using
the Pearson product-moment correlation method [41]. 15 We
observe that across all the models in our test, the IoU metric
correlates more strongly with human judgements than BLEU.
Furthermore, the correlation factors for IoU are also more
consistent across models.

Figure 5 visualizes the relation between the two metrics
(BLEU and IoU) and the user-perceived usefulness with the
predictions of the natural model. The best linear fit for the
data points is shown in blue, while the red dotted line visual-
izes the theoretical line on which the metric and usefulness
would have perfect correlations.

15To be able to correlate the 4-point reviewer scale with the other metrics,
we convert from the 4-point scale to the interval 0-1 as follows: Strongly
disagree = 0, Disagree = 1/3, Agree = 2/3, Strongly agree = 1.

Geert Heyman, Rafael Huysegems, Pascal Justen, and Tom Van Cutsem

Figure 4. Distributions of the answers in the prediction quality survey: three data science experts each assess model predictions
for 100 test cases w.r.t. claims about their usefulness, coverage of the intent, precision, and compatibility with the existing
source code.

In 6 of 30 failure cases, IoU was overly sensitive to a miss-
ing import statement in the predicted code, particularly when
the code to predict was short, while the annotators seem to
care less.

From these observations we conclude that exploring new
metrics for both NLGP and code prediction models in general
is a relevant area for further research.

4.7 Summary

We introduced three code prediction models for Python
trained on three different datasets: a model trained on only
code (no comments), code with injected docstring comments
(docstring) and code with unmodified comments (natural).
From an initial benchmark and user study we find that a
model trained on natural comments leads to better results
in an NLGP context (i.e. predictions based on both prior
code and an explicit natural language intent). In future work
we want to explore whether we can further boost the pre-
diction quality beyond the natural model by combining
natural with injected comments and adapting the docstring
comments to better match how developers express intent.

5 Related work
5.1 Example-Centric Programming

As mentioned in the introduction, example-centric program-
ming [5] tools are a precursor to natural language-guided
programming tools. These tools help users more quickly
identify code examples from local or online repositories.
BluePrint [5] allows Adobe Flex users to find relevant code
examples from online documentation from within the Adobe
Flex Builder IDE. BluePrint takes as input a natural language
search query and augments this query with the program-
ming language and framework version used by the developer.
Unlike NLGP assistants, Blueprint does not take into account

Figure 5. Scatter plots that map out the relation between
the metric scores (horizontal axis) and the user-perceived
usefulness (vertical axis) for the predictions of the natural
model.

We manually examined 30 cases where the difference be-
tween IoU score and usefulness score was larger than 0.33.
Under a perfect correlation, this difference would correspond
to one step higher or lower on the annotation scale (e.g. the
difference between ’Disagree’ and ’Agree’). We found that
IoU tends to underestimate the human-assigned usefulness
and identified three root causes for the mismatches:

In 12 of 30 failure cases, the prediction contained the target
code but also included other code, such as instructions to
display or print variables. While this additional code was
often relevant, it significantly decreased the IoU score.

In 7 of 30 failure cases, the model predicted code that sat-
isfied the expressed intent but was syntactically significantly
different from the target code. In some cases, the predicted
code was more compact and idiomatic than the actual tar-
get code. These cases are inherently difficult for any metric
that relies purely on the syntactic similarity between the
predicted and target code.

Natural Language-Guided Programming

the specific code context and does not adapt the code exam-
ples to the specific context of use.

tools

Code

assistant

like Prospector

and
PARSEWeb [36] focus on the problem of helping de-
velopers navigate complex object-oriented APIs. These
approaches share with NLGP the idea of mining common
coding idioms from existing code repositories, but do not
employ natural language intent to guide the search.

[23]

5.2 Statistical Code Prediction Models

We cover related work that specifically frames code autocom-
pletion as a statistical code prediction problem. We divide
related work into three categories, depending on what in-
put the prediction model uses: context-only models use only
the existing code context to predict subsequent code tokens;
intent-only models use only natural language intent as input
without regard for prior code context; finally context+intent
models use both.

Context-Only Models Tabnine [35] and Kite [22] are
recent examples of proprietary code autocompletion tools
that were trained on code context only. For both, the aim is
to complete the line of code that the developer is actively
editing. Tabnine uses a GPT-2 language model [29] trained
on open source code. A detailed study of GPT-2 autocom-
pletion was carried out by Svyatkovskiy et al. [34]. They
also discussed optimizations such as completion caching to
enable efficient deployments of these models. While such
statistical code completers can be very effective, they assume
that the developer already knows how to start implementing
a task. If a developer were to invoke such tools at the end of
an inline comment as one would write for an NLGP assistant,
these tools would try to autocomplete the comment rather
than the next lines of code.

Intent-Only Models Some approaches in this category
focus on predicting only API call(s) while others try to pre-
dict the entire target code. Raghothaman et al. [30] use the
clickthrough data of a web search engine to train a model
that can translate from user queries into APIs that are likely
relevant to the query. They then post-process the relevant
APIs into type-safe code examples. This approach does not
adapt the generated code to the context of use. Gu et al.
[13] use an RNN encoder-decoder to generate API usage se-
quences for a given natural language (NL) query. Srinivasan
et al. [19] predict Java methods given an NL query and a
summary of the rest of the class, with a custom LSTM-based
encoder-decoder model with attention. Clement et al. [8]
use a T5 encoder-decoder transformer trained on different
objectives to predict the Python implementation of a method
given its signature and (if available) its docstring. The au-
thors use the Python subset of the CodesearchNet dataset
[18] and scrape GitHub repositories for methods with and
without docstrings. Yin and Neubig [50] generate Python
code using an LSTM-based encoder-decoder that produces

syntactically correct code by construction. Xu et al. [46] per-
formed a user study with a code prediction plugin based on
an ensemble of TRANX [50] and a code search model. The
plugin suggests code snippets based on a natural language
query that is issued within an IDE. Their setup is therefore
closely related to natural language-guided programming,
except that the plugin does not leverage the surrounding
code. The user study did not provide conclusive evidence
that the plugin had a significant influence on programmer
productivity: neither on the speed with which programmers
solved tasks nor on the correctness of the implementations.
Context+Intent Models Murali et al. [25] predict a Java
method body given 1) NL keywords and 2) API calls or classes
that should be used. Based on these inputs a probabilistic
encoder-decoder named “Gaussian Encoder-Decoder” (GED)
was used to learn a distribution over simplified control-flow
graphs (“program sketches”). Agashe et al. [2] study both API
call prediction and full code prediction based on the preced-
ing code and a natural language query. They experimented
with LSTMs and small Transformer models without pretrain-
ing on a large text corpus. Orlanski and Gittens [27] studied
code generation from StackOverflow questions, which often
embed code snippets to provide additional context.

Chen et al. [7] introduce Codex, a series of large language
models (up to 12B parameters) based on the GPT-3 [6] archi-
tecture. By training the models on a large corpus (159GB)
of open source code they find that Codex performs signifi-
cantly better than GPT-3 on the task of predicting full code
solutions to programming problems from natural language
docstrings. Whereas our work on NLGP focuses on gener-
ating small snippets of code to help a programmer more
effectively explore and use known APIs, Codex is evaluated
on generating functionally correct code. Testing functional
correctness of code requires executable unit tests which may
not always be available in practical settings.

5.3 Code Prediction Benchmark Data

In our search for usable benchmarks, we find that existing
benchmarks typically consist of intent and target code while
benchmarks suitable to test NLGP assistants require context,
intent, and target code.

Intent/Target Benchmarks: Yin et al. [49] create a cu-
rated dataset called CoNaLa [9] from Stack Overflow posts.
Heyman et al. [16] created a benchmark for “annotated code
search”: the retrieval of code snippets (target code) annotated
with a short natural language description (intent). Yao et al.
[48] mined question-code (Intent/target) pairs in Python and
SQL. Hamel Husain et al. [18] collected query/code (intent/-
target) pairs from Bing search queries that have high click-
through rates to code written in Go, Java, JS, PHP Python or
Ruby. Barone et al. [3] extracted 100K target/intent samples
from Github projects. The intent is retrieved from docstrings
that describe function declarations and bodies. Chen et al. [7]

introduce HumanEval, a benchmark of 164 manually com-
posed programming problems and their Python solutions,
consisting of a function signature, docstring, function body
and several unit tests. While this benchmark is useful to mea-
sure functional correctness of generated code, the benchmark
problems are typical programming challenges focused on
mathematical concepts using built-in abstractions like num-
bers and lists, and are therefore not suitable to assess code
generation for programmer intents in API-rich settings such
as the Python data science domain. The problems are also
self-contained, not requiring prior code context.

Context/Intent/Target Benchmarks: Agashe et al. [2]
created a dataset of 3.7K curated examples called JuICe. The
samples are extracted from Jupyter notebooks containing
Python code. As these notebooks were originally created as
student assignments, the natural language intents tend to be
long, descriptive and often contain information that is only
loosely related to the target code. An average intent in the
dataset measures 58.33 tokens and is therefore less suited
for NLGP, where we expect the intent to be formulated as a
short query of between 3 and 10 tokens. This expectation is
based in part on observations from a user study conducted
by Xu et al. [46].

5.4 Program Synthesis

Program synthesis methods [14] study the broader problem
of generating programs from specifications. Specifications
can range from highly formal and unambiguous (e.g. a for-
mula in logic) to informal and ambiguous (e.g. input-output
examples, natural language or program sketches [33]). Most
closely related to NLGP is the idea of program synthesis from
natural language input [10]. These methods focus on trans-
lating a natural language intent (often just a single sentence)
into a short program that covers the intent. A key difference
with NLGP is that these methods typically focus on helping
end-users in specific domains: the natural language input is
restricted to a specific application domain and the programs
are written in a domain-specific language (DSL) that is of-
ten custom-built to solve a specific problem. This contrasts
with NLGP which is aimed at helping professional software
developers solve a variety of tasks using a general-purpose
programming language.

6 A Research Agenda for Natural
Language-Guided Programming

As with any proposal that aims to offer radically new ways
to program computers, the idea of writing code guided by
free-form natural language brings with it a whole new range
of problems and unexplored areas. What research questions
does the programming community need to address to turn
natural language-guided programming from a research idea
into a reliable “proven” method of programming? We list
significant open questions that remain unanswered by the

Geert Heyman, Rafael Huysegems, Pascal Justen, and Tom Van Cutsem

case study presented in this work. Each of these represents a
major avenue for future research in natural language-guided
programming:
More Diverse Training Data It is to be expected that train-
ing models on more source code will further increase the
quality of code predictions. In addition, rather than simply
training models on more code, it would be useful to consider
additional sources of NL intent/code pairs, such as tutorial
documentation or Q&A forum threads (such as those found
on Stack Overflow).
Better Benchmark Datasets Progress in machine learning
and NLP is often driven by high-quality benchmarks (e.g.,
the GLUE benchmark [40]). In the same vein, we believe
better benchmarks for code prediction are a key enabler for
better NLGP assistants. In this work we have taken the first
steps towards this goal, but our benchmark remains limited
in size (201 examples) and in scope (Python data science).
We hope that the community will advance these efforts.
Better Metrics Development of a benchmark not only en-
tails creating curated triplets of realistic code contexts, NL
intents and ground-truth code completions, but also entails
finding better code scoring metrics whose output correlates
even better with user-perceived usefulness.

Right now, the most effective way to measure the useful-
ness of a code-prediction tool is to have human experts rate
the predicted code in relation to the stated task and the given
code context. This method is not very scalable, especially
when considering comparing multiple (or multiple versions
of) code prediction models.

What is needed is an easy to calculate and objective metric
that can score the output of code prediction models with
reference to one or more ground-truth solutions.

In this work, we used standard metrics such as BLEU and
IoU to compute the similarity between predicted code and
the ground-truth target code. We have shown that these met-
rics correlate with user-perceived usefulness to some extent
(Section 4.6.3). There is ample opportunity to improve upon
these metrics with new metrics more specifically tailored to
code.
Effect on Productivity Does NLGP positively affect devel-
oper productivity as measured by e.g. the time to complete
set programming tasks? Even though the ultimate goal of
code-prediction models is to maximize the productivity of
developers and the quality of the code, there is a relative
paucity of research that quantifies these claims. Recent work
by Xu et al. [46] aims to address this through a controlled
user study where two groups of programmers were tasked to
complete a set of well-defined programming tasks, with and
without the help of a code-prediction tool. The results from
the study were inconclusive as to the positive effect of the
code-prediction tool under study. There is a clear need for
more of these studies with larger participation, more diverse
tasks and more code-prediction tools.

Natural Language-Guided Programming

Effect on Code Quality Does NLGP positively affect the
quality of code as measured by e.g. reported bugs attributed
to code (partially) suggested by NLGP assistants? Does NLGP
positively affect the maintainability of code?
Effect on Learning Curve What is the effect of NLGP on
the learning curve of a developer? For example, are NLGP as-
sistants better suited to junior developers or are they helpful
across many levels of prior coding experience? Are NLGP
assistants more useful for developers new to a project or do
they remain useful even for senior developers on the team?
Impact of Text-to-Code Ratio How does the effectiveness
of NLGP relate to the ratio of code versus natural language
text in coding environments? Our case study focused on
Jupyter notebooks where the ratio of natural language text
(in-line comments, markdown text cells) compared to code
is likely higher than in a typical Python script (a ‘.py‘ source
file). It is intuitively clear that a higher ratio of text-to-code
will help NLGP, but we have yet to establish objective rela-
tionships between text-to-code ratio and NLGP effectiveness
as measured through benchmarks.
Inference Latency For modern neural architectures such
as Transformers, deep learning researchers have observed
that larger models perform better. Our initial experience in
training GPT-2 models of various sizes (not further detailed
in this paper) for the NLGP task confirms this observation.
We have deliberately kept the model size constrained to keep
the latency of code predictions within an acceptable thresh-
old of 2 seconds. We expect increasing the inference speed of
language models by algorithmic or hardware improvements
will be an important driver to enable larger and therefore
better code predictions.
Effective Use of Code Context How to leverage the code
context more effectively? Because for transformer architec-
tures such as GPT-2, memory and time complexity scale
quadratically in the sequence length, it is infeasible to pro-
vide an entire code file as context to such models. Our case
study uncovered that a significant proportion of the mis-
takes occurred when relevant code (e.g. import statements)
fell out of the model’s context window. For example, we ob-
served that the language models we trained at times predict
functional calls that look relevant but do not exist. We con-
jecture that this phenomenon is caused by training with a
limited context window, because during training the model
will at times be forced to predict function calls without see-
ing its definition or imports. Therefore, new strategies to
select what parts of a code file will be included in the context
window, more efficient tokenization methods (i.e. encoding
the same code with fewer tokens), and exploring linear/-
subquadratic transformer variants could all lead to more
informed predictions.
API Versioning Building an NLGP assistant by training a
model on existing code runs the risk of biasing the model’s
predictions towards older or more frequently used versions

of common APIs. Ideally, an NLGP tool would also have
access to the precise versions of the libraries used by the de-
veloper so that it can tailor its code suggestions to those ver-
sions. This would help overcome a key limitation of example-
centric programming, as studies of Stack Overflow found that
code found in answers to coding questions was frequently
obsolete [31], with one study finding that for a sample of
known-obsolete answers only 20.5% were updated to reflect
the latest API usage [51].

Impact of Interactive Programming Can interactive pro-
gramming environments further improve the effectiveness of
NLGP by giving the NLGP assistant access to (a description
of) the runtime values manipulated by the code?

Interactive programming environments such as notebook
environments (Jupyter, Zeppelin, Observable, etc.) or IDEs
that prominently support read-eval-print loops (e.g. Dr-
Racket, BlueJ) offer the capability to execute small code frag-
ments and get immediate feedback on their runtime effects.
For example, in a Jupyter notebook, the output of a code cell
is often inserted as rich output in the notebook environment
itself (as a graphic, a structured table or as plain text).

Taking this one step further, “Live Programming” envi-
ronments [24] aim to merge code and runtime context even
further, giving near-continuous feedback on the runtime
values stored in program variables. We conjecture that an
NLGP assistant could make effective use of these additional
context inputs to improve its suggestions.

7 Conclusion
We define natural language-guided programming as the pro-
gramming practice of using intelligent code completion tools
to automate routine programming tasks by stating the de-
sired intent using natural language. An NLGP assistant is a
tool that autocompletes a piece of code guided by natural
language intent.

We demonstrate natural language-guided programming
for automating routine data science and machine learning
tasks using Python. We contribute the design, implementa-
tion and evaluation of a proof-of-concept NLGP assistant
based on language modeling.

We conduct experiments with pretrained models (GPT-2),
revealing that preparation of the data to contain a good mix
of natural language intent and code is critical to improve code
prediction quality. Our experiments suggest that comments
that occur naturally in the code are sufficient for language
models to learn the relationship between intent and code.
Our docstring injection method further indicates that NLGP
can be made feasible in domains where the source code lacks
good inline comments.

We construct a curated benchmark to measure the qual-
ity of code predictions. Our initial human evaluation study
provides evidence that our best models can generate code
predictions that expert data scientists find useful, and that

are compatible with the context of use. As such, our work
can be seen as a first step towards making automatic example
embedding a reality.

Much work remains to be done to turn NLGP from an
initial idea into a practical, reliable programming practice.
We end the paper with a Research Agenda for NLGP, inviting
the programming research community to work on better
benchmarks, to set up user studies to quantify the impact
on productivity, and to invent novel metrics to automate the
scoring of code predictions. Our initial experiments reveal
inconsistencies between widely used metrics and human
judgments, which we hope will inspire others to invent better
alternatives.

Acknowledgments
We would like to thank our colleagues Frederik Vandeputte,
Bart Theeten, Maayan Goldstein, Guillermo Rodriguez-
Navas and Cecilia Gonzalez-Alvarez for discussions and their
help collecting and labeling the data used in our experiments.

A Appendix
A.1 Annotation Process
A.1.1 Annotation Guidelines. The annotators received
the following guidelines:

• Skip candidates for which the candidate intent:

– only contains commented source code
– does not express the intent of (part of) the candidate
target code (e.g. skip comments from exercise notebooks
such as "Start your code here" )

– is domain-specific and cannot be translated into the
target code without expert knowledge about a particular
domain. The intent should be expressed in terms that
refer to the functionality of Python/ Python libraries, it
should not express the more high-level, domain-specific
goal for why the libraries are needed. Note that this
guideline does not imply that the intent has to include
library names or API calls.

• Skip candidates for which the target code:

– does not contain at least one non-trivial API call (e.g.

"Fg = Fn * g");

– exclusively consists of setup/initialization code;
– is a non-idiomatic implementation of the intent

• For the remaining test cases:

– select the target code 𝑡 from the candidate target code 𝑡𝑐
– reformulate the candidate intent 𝑖𝑐 to make it more real-
istic, if necessary. For example, "and now plot the data"
can be reformulated as "plot the data". However, to avoid
that the intent would be systematically biased towards
the annotator preferences, annotators are not allowed
to further reformulate the original intent. Similarly, an-
notators are instructed to not correct potential typos.

Geert Heyman, Rafael Huysegems, Pascal Justen, and Tom Van Cutsem

Figure 6. Screenshot of the user interface for annotating
candidate test cases to create the NLGP benchmark.

A.1.2 Annotation Interface. Figure 6 shows the user in-
terface for annotating candidate test cases to create the NLGP
benchmark. It is implemented as a web service using the La-
bel Studio framework.16

A.1.3 Inter-Annotator Agreement. We evaluated the
inter-annotator agreement on three aspects: the decision
to accept or skip a test case, the intent selection, and the
target code selection. The Fleiss kappa score with regard to
accepting/skipping test cases is 0.515. This reflects moderate
agreement [39]. Out of the candidate test cases that were
accepted by at least two annotators, annotators had the same
intent for 74% of the cases and had annotated the same tar-
get code for 71% of the cases. For the cases without exact
agreement, the intents had an average edit distance of 3.3
tokens and the target code snippets differed with an average
of 3.2 lines of code.

A.2 NLGP Benchmark Library Distribution

Figure 7 gives insight into what libraries are used in the
target code fragments in the NLGP benchmark. For each of
the 201 test cases, we analyzed the imports and calls in the
target code and attempt to resolve these to their root module.
We rely on a similar resolution method to what was used
for the docstring injection (see 4.4). Note that there will be
cases where the root module is not resolved correctly, but

16https://labelstud.io/

Natural Language-Guided Programming

Figure 7. Plot of the frequencies with which modules are used in the target code of the 201 test cases in the NLGP benchmark.

Figure 8. Three test cases from the NLGP benchmark and the predictions for the natural, docstring and no comments models.
We display each test case with the truncated context in black, the intent in blue and the target code in red.

overall the method should be accurate enough to capture the
module distribution.

A.3 Examples

In Figure 8, we list three test cases from the NLGP benchmark
and the predictions made by the three models under test. We
also provide the scores assigned by BLEU and IoU as well
as the average usefulness score assigned by the users. Note
that due to space constraints we truncated the context code.

References
[1] 2021. pip install documentation. Retrieved April 9, 2021 from https:

//pip.pypa.io/en/stable/reference/pip_install/

[2] Rajas Agashe, Srinivasan Iyer, and Luke Zettlemoyer. 2019. JuICe: A
Large Scale Distantly Supervised Dataset for Open Domain Context-
based Code Generation. In Proceedings of the 2019 Conference on Empir-
ical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing (EMNLP-IJCNLP). As-
sociation for Computational Linguistics, Hong Kong, China, 5436–5446.
https://doi.org/10.18653/v1/D19-1546

[3] Antonio Valerio Miceli Barone and Rico Sennrich. 2017. A parallel
corpus of Python functions and documentation strings for automated
code documentation and code generation. CoRR abs/1707.02275 (2017).
arXiv:1707.02275 http://arxiv.org/abs/1707.02275

[4] Ohad Barzilay. 2011. Example Embedding. In Proceedings of the 10th
SIGPLAN Symposium on New Ideas, New Paradigms, and Reflections on
Programming and Software (Onward! 2011). Association for Computing
Machinery, New York, NY, USA, 137–144. https://doi.org/10.1145/
2089131.2089135

[5] Joel Brandt, Mira Dontcheva, Marcos Weskamp, and Scott R. Klemmer.
2010. Example-Centric Programming: Integrating Web Search into the
Development Environment. In Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems (CHI ’10). Association for
Computing Machinery, New York, NY, USA, 513–522. https://doi.org/
10.1145/1753326.1753402

[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam,
Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. 2020. Language Models are Few-Shot Learners.
In Advances in Neural Information Processing Systems, H. Larochelle,
M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (Eds.), Vol. 33. Curran
Associates, Inc., 1877–1901. https://proceedings.neurips.cc/paper/
2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf

[7] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas
Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,
Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke
Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Fe-
lipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios
Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss,
Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin,
Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, An-
drew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,
Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie
Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,
Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Lan-
guage Models Trained on Code. arXiv:cs.LG/2107.03374

Geert Heyman, Rafael Huysegems, Pascal Justen, and Tom Van Cutsem

[8] Colin B. Clement, Dawn Drain, Jonathan Timcheck, Alexey Svy-
atkovskiy, and Neel Sundaresan. 2020. PyMT5: multi-mode trans-
lation of natural language and Python code with transformers.
arXiv:cs.LG/2010.03150

[9] CoNaLa. 2021. CoNaLa: The Code/Natrual Language Challenge. Re-

trieved April 2, 2021 from https://conala-corpus.github.io/

[10] Aditya Desai, Sumit Gulwani, Vineet Hingorani, Nidhi Jain, Amey
Karkare, Mark Marron, Sailesh R, and Subhajit Roy. 2016. Program
Synthesis Using Natural Language. In Proceedings of the 38th Inter-
national Conference on Software Engineering (ICSE ’16). Association
for Computing Machinery, New York, NY, USA, 345–356.
https:
//doi.org/10.1145/2884781.2884786

[11] Philip Gage. 1994. A new algorithm for data compression. C Users
Journal 12, 2 (1994), 23–38. https://dl.acm.org/doi/10.5555/177910.
177914

[12] Alex Graves. 2012. Sequence transduction with recurrent neural net-

works. arXiv preprint arXiv:1211.3711 (2012).

[13] Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim.
2016. Deep API Learning. In Proceedings of the 2016 24th ACM SIGSOFT
International Symposium on Foundations of Software Engineering (FSE
2016). Association for Computing Machinery, New York, NY, USA,
631–642. https://doi.org/10.1145/2950290.2950334

[14] Sumit Gulwani, Oleksandr Polozov, and Rishabh Singh. 2017. Program
Synthesis. Foundations and Trends in Programming Languages 4, 1-2
(2017), 1–119. https://doi.org/10.1561/2500000010

[15] Vincent J. Hellendoorn and Premkumar Devanbu. 2017. Are Deep
Neural Networks the Best Choice for Modeling Source Code?. In Pro-
ceedings of the 2017 11th Joint Meeting on Foundations of Software
Engineering (ESEC/FSE 2017). Association for Computing Machinery,
New York, NY, USA, 763–773. https://doi.org/10.1145/3106237.3106290
[16] Geert Heyman and Tom Van Cutsem. 2020. Neural Code Search Re-
visited: Enhancing Code Snippet Retrieval through Natural Language
Intent. arXiv:cs.IR/2008.12193

[17] Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar
Devanbu. 2012. On the Naturalness of Software. In Proceedings of the
34th International Conference on Software Engineering (ICSE ’12). IEEE
Press, 837–847. https://dl.acm.org/doi/10.5555/2337223.2337322
[18] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and
Marc Brockschmidt. 2020. CodeSearchNet Challenge: Evaluating the
State of Semantic Code Search. arXiv:cs.LG/1909.09436

[19] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer.
2018. Mapping Language to Code in Programmatic Context. In Proceed-
ings of the 2018 Conference on Empirical Methods in Natural Language
Processing. Association for Computational Linguistics, Brussels, Bel-
gium, 1643–1652. https://doi.org/10.18653/v1/D18-1192

[20] Daniel Jurafsky and James H Martin. 2008. Speech and Language Pro-
cessing: An introduction to speech recognition, computational linguistics
and natural language processing. https://dl.acm.org/doi/book/10.5555/
1214993

[21] Rafael-Michael Karampatsis and Charles Sutton. 2019. Maybe deep
neural networks are the best choice for modeling source code. arXiv
preprint arXiv:1903.05734 (2019).

[22] Kite. 2021. Code faster. Stay in flow. Retrieved April 2, 2021 from

https://www.kite.com/

[23] David Mandelin, Lin Xu, Rastislav Bodík, and Doug Kimelman. 2005.
Jungloid Mining: Helping to Navigate the API Jungle. In Proceedings of
the 2005 ACM SIGPLAN Conference on Programming Language Design
and Implementation (PLDI ’05). Association for Computing Machinery,
New York, NY, USA, 48–61. https://doi.org/10.1145/1065010.1065018
[24] Sean McDirmid. 2013. Usable Live Programming. In Proceedings of the
2013 ACM International Symposium on New Ideas, New Paradigms, and
Reflections on Programming and Software (Onward! 2013). Association
for Computing Machinery, New York, NY, USA, 53–62. https://doi.
org/10.1145/2509578.2509585

Natural Language-Guided Programming

[25] Vijayaraghavan Murali, Swarat Chaudhuri, and Chris Jermaine. 2017.
Bayesian Sketch Learning for Program Synthesis. CoRR abs/1703.05698
(2017). arXiv:1703.05698 http://arxiv.org/abs/1703.05698

[26] Vijayaraghavan Murali, Letao Qi, Swarat Chaudhuri, and Chris Jer-
maine. 2018. Neural Sketch Learning for Conditional Program Gener-
ation. In International Conference on Learning Representations.
[27] Gabriel Orlanski and Alex Gittens. 2021. Reading StackOverflow En-
courages Cheating: Adding Question Text Improves Extractive Code
Generation. In Proceedings of the 1st Workshop on Natural Language
Processing for Programming (NLP4Prog 2021). Association for Compu-
tational Linguistics, Online, 65–76. https://doi.org/10.18653/v1/2021.
nlp4prog-1.8

[28] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.
BLEU: A Method for Automatic Evaluation of Machine Translation. In
Proceedings of the 40th Annual Meeting on Association for Computational
Linguistics (ACL ’02). Association for Computational Linguistics, USA,
311–318. https://doi.org/10.3115/1073083.1073135

[29] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and
Ilya Sutskever. 2019. Language Models are Unsupervised Multitask
Learners. (2019).

[30] Mukund Raghothaman, Yi Wei, and Youssef Hamadi. 2016. SWIM:
Synthesizing What i Mean: Code Search and Idiomatic Snippet Syn-
thesis. In Proceedings of the 38th International Conference on Software
Engineering (ICSE ’16). Association for Computing Machinery, New
York, NY, USA, 357–367. https://doi.org/10.1145/2884781.2884808
[31] Chaiyong Ragkhitwetsagul, Jens Krinke, Matheus Paixao, Giuseppe
Bianco, and Rocco Oliveto. 2021. Toxic Code Snippets on Stack Over-
flow. IEEE Transactions on Software Engineering 47, 3 (2021), 560–581.
https://doi.org/10.1109/TSE.2019.2900307

[32] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Ma-
chine Translation of Rare Words with Subword Units. In Proceedings of
the 54th Annual Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers). Association for Computational Linguistics,
Berlin, Germany, 1715–1725. https://doi.org/10.18653/v1/P16-1162

[33] Armando Solar-Lezama. 2008. Program Synthesis by Sketching. Ph.D.

Dissertation. USA. Advisor(s) Bodik, Rastislav. AAI3353225.

[34] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundare-
san. 2020. Intellicode compose: Code generation using transformer.
In Proceedings of the 28th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software
Engineering. 1433–1443.

[35] Tabnine. 2021. Code faster with AI completions. Retrieved April 2,

2021 from https://www.tabnine.com/

[36] Suresh Thummalapenta and Tao Xie. 2007. Parseweb: A Programmer
Assistant for Reusing Open Source Code on the Web. In Proceedings of
the Twenty-Second IEEE/ACM International Conference on Automated
Software Engineering (ASE ’07). Association for Computing Machinery,
New York, NY, USA, 204–213. https://doi.org/10.1145/1321631.1321663
[37] tree sitter. 2021. tree-sitter: An incremental parsing system for pro-
gramming tools. Retrieved April 23, 2021 from https://github.com/
tree-sitter/tree-sitter

[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017.
Attention is All you Need. In Advances in Neural Information Pro-
cessing Systems, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Cur-
ran Associates, Inc. https://proceedings.neurips.cc/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf

[39] Anthony J Viera, Joanne M Garrett, et al. 2005. Understanding in-
terobserver agreement: the kappa statistic. Fam med 37, 5 (2005),
360–363.

[40] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy,
and Samuel Bowman. 2018. GLUE: A Multi-Task Benchmark and
Analysis Platform for Natural Language Understanding. In Proceedings
of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting
Neural Networks for NLP. Association for Computational Linguistics,
Brussels, Belgium, 353–355. https://doi.org/10.18653/v1/W18-5446

[41] Wikipedia. 2021.

Retrieved
April 23, 2021 from https://en.wikipedia.org/wiki/Pearson_correlation_
coefficient

Pearson correlation coefficient.

[42] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi
Louf, Morgan Funtowicz, and Jamie Brew. 2019. HuggingFace’s
Transformers: State-of-the-art Natural Language Processing. CoRR
abs/1910.03771 (2019). arXiv:1910.03771 http://arxiv.org/abs/1910.
03771

[43] Xin Xia, Lingfeng Bao, David Lo, Pavneet Singh Kochhar, Ahmed E.
Hassan, and Zhenchang Xing. 2017. What Do Developers Search
for on the Web? Empirical Softw. Engg. 22, 6 (Dec. 2017), 3149–3185.
https://doi.org/10.1007/s10664-017-9514-4

[44] Bowen Xu, Le An, Ferdian Thung, Foutse Khomh, and David Lo. 2020.
Why reinventing the wheels? An empirical study on library reuse and
re-implementation. Empir Software Eng 25 (2020), 755–789. https:
//doi.org/10.1007/s10664-019-09771-0

[45] Frank F. Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan Vasilescu, and
Graham Neubig. 2020. Incorporating External Knowledge through Pre-
training for Natural Language to Code Generation. In Proceedings of the
58th Annual Meeting of the Association for Computational Linguistics.
Association for Computational Linguistics, Online, 6045–6052. https:
//doi.org/10.18653/v1/2020.acl-main.538

[46] Frank F. Xu, Bogdan Vasilescu, and Graham Neubig. 2021.

In-IDE
Code Generation from Natural Language: Promise and Challenges.
arXiv:cs.SE/2101.11149

[47] Di Yang, Aftab Hussain, and Cristina Videira Lopes. 2016. From Query
to Usable Code: An Analysis of Stack Overflow Code Snippets. In
Proceedings of the 13th International Conference on Mining Software
Repositories (MSR ’16). Association for Computing Machinery, New
York, NY, USA, 391–402. https://doi.org/10.1145/2901739.2901767
[48] Ziyu Yao, Daniel S. Weld, Wei-Peng Chen, and Huan Sun. 2018.
StaQC: A Systematically Mined Question-Code Dataset from Stack
Overflow. In Proceedings of the 2018 World Wide Web Conference
(WWW ’18). International World Wide Web Conferences Steering
Committee, Republic and Canton of Geneva, CHE, 1693–1703. https:
//doi.org/10.1145/3178876.3186081

[49] Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and
Graham Neubig. 2018. Learning to Mine Aligned Code and Natu-
ral Language Pairs from Stack Overflow. In Proceedings of the 15th
International Conference on Mining Software Repositories (MSR ’18).
Association for Computing Machinery, New York, NY, USA, 476–486.
https://doi.org/10.1145/3196398.3196408

[50] Pengcheng Yin and Graham Neubig. 2018. TRANX: A Transition-
based Neural Abstract Syntax Parser for Semantic Parsing and Code
Generation. arXiv:cs.CL/1810.02720

[51] Haoxiang Zhang, Shaowei Wang, Tse-Hsun Chen, Ying Zou, and
Ahmed E. Hassan. 2021. An Empirical Study of Obsolete Answers on
Stack Overflow. IEEE Transactions on Software Engineering 47, 4 (2021),
850–862. https://doi.org/10.1109/TSE.2019.2906315

