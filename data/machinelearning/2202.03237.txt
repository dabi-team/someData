2
2
0
2

b
e
F
7

]

R

I
.
s
c
[

1
v
7
3
2
3
0
.
2
0
2
2
:
v
i
X
r
a

Introducing the Expohedron for Efficient Pareto-optimal
Fairness-Utility Amortizations in Repeated Rankings
Jean-Michel Renders
Patrick Loiseau
jean-michel.renders@naverlabs.com
patrick.loiseau@inria.fr
Naver Labs Europe
Univ. Grenoble Alpes, Inria, CNRS,
France
Grenoble INP, LIG
France

Till Kletti
till.kletti@naverlabs.com
Naver Labs Europe
France

(a) 𝑛 = 2. A line segment in R2

(b) 𝑛 = 3. A polygon in R3

(c) 𝑛 = 4. A polyhedron in R4

Figure 1: Examples of the DCG expohedron for 𝑛 ∈ {2, 3, 4} items. The vertices are the DCG exposures
, . . . ,
group S𝑛 . The expohedron is the convex hull of these vertices. Expohedra live in hyperplanes of dimension 𝑛 − 1.

(cid:16)

1
log2 (2)

(cid:17)

1
log2 (𝑛+1)

under application of the symmetric

ABSTRACT
We consider the problem of computing a sequence of rankings that
maximizes consumer-side utility while minimizing producer-side
individual unfairness of exposure. While prior work has addressed
this problem using linear or quadratic programs on bistochastic
matrices, such approaches, relying on Birkhoff-von Neumann (BvN)
decompositions, are too slow to be implemented at large scale.

In this paper we introduce a geometrical object, a polytope that
we call expohedron, whose points represent all achievable exposures
of items for a Position Based Model (PBM). We exhibit some of its
properties and lay out a Carathéodory decomposition algorithm
with complexity 𝑂 (𝑛2 log(𝑛)) able to express any point inside the
expohedron as a convex sum of at most 𝑛 vertices, where 𝑛 is the
number of items to rank. Such a decomposition makes it possible
to express any feasible target exposure as a distribution over at
most 𝑛 rankings. Furthermore we show that we can use this poly-
tope to recover the whole Pareto frontier of the multi-objective
fairness-utility optimization problem, using a simple geometrical
procedure with complexity 𝑂 (𝑛2 log(𝑛)). Our approach compares
favorably to linear or quadratic programming baselines in terms
of algorithmic complexity and empirical runtime and is applicable
to any merit that is a non-decreasing function of item relevance.
Furthermore our solution can be expressed as a distribution over

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
WSDM ’22, February 21–25, 2022, Tempe, AZ, USA
© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9132-0/22/02. . . $15.00
https://doi.org/10.1145/3488560.3498490

1

only 𝑛 permutations, instead of the (𝑛 − 1)2 + 1 achieved with
BvN decompositions. We perform experiments on synthetic and
real-world datasets, confirming our theoretical results.

CCS CONCEPTS
• Mathematics of computing → Permutations and combinations;
• Information systems → Probabilistic retrieval models; Con-
tent ranking.

KEYWORDS
ranking, fairness, amortization, pareto-optimal, muli-objective op-
timization, Carathéodory, expohedron, GLS, balanced words

ACM Reference Format:
Till Kletti, Jean-Michel Renders, and Patrick Loiseau. 2022. Introducing
the Expohedron for Efficient Pareto-optimal Fairness-Utility Amortizations
in Repeated Rankings. In Proceedings of the Fifteenth ACM International
Conference on Web Search and Data Mining (WSDM ’22), February 21–25,
2022, Tempe, AZ, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/
10.1145/3488560.3498490

1 INTRODUCTION
Ranking systems are nowadays widely deployed in the world wide
web and make it possible for users of search engines to quickly
retrieve items that are relevant with respect to their query. Those
items can be websites, movies, songs, research papers and many
other things. With web search and recommendation having an ever
increasing influence on people’s behavior and determining which
opportunities people are given, the questions whether ranking
systems are fair and how to make them fair, has gathered much
attention in recent years [3, 23, 30].

A typical ranking system estimates a relevance value for each
item and orders the items by decreasing relevance values; this is

 
 
 
 
 
 
WSDM ’22, February 21–25, 2022, Tempe, AZ, USA

Kletti et al.

called the Probability Ranking Principle (PRP) and provides un-
der some assumptions the highest expected utility for the users
[27]. However, it has been noted that PRP rankings can be very
unfair to item producers [3, 30] insofar as their exposure is not
proportional to their merit. For instance when all items are almost
equally relevant, the last item will receive much less exposure than
the first item, even though their merits are almost equal. To avoid
this unfairness, a common approach is to deliver different rankings
each time a user issues the query, in such a way that the average
exposure given to each item is proportional to its merit. This is
called amortization [3].

In recent work, Singh and Joachims give a method to compute a
distribution over rankings that maximizes utility while satisfying a
fairness constraint [30]. They express a distribution over rankings
as a bistochastic matrix whose elements represent the proportions
with which an item should be at a certain rank. They formulate
the utility objective as a linear function of a bistochastic matrix
and the fairness objective as a linear constraint on the bistochastic
matrix. The optimal bistochastic matrix is then decomposed as a
distribution over permutations using the Birkhoff-von Neumann
(BvN) algorithm [9]. A similar approach has been used in follow-up
work [32, 33, 39]. However it has been noted that such an approach
does not scale well [11] as it involves a linear program (LP) over 𝑛2
variables, where 𝑛 is the number of items to rank. Using state-of-
the-art LP solvers adapted to our problem, the total complexity of
such an approach is then 𝑂 (𝑛5).

In this paper we lay out a method that computes the whole Pareto-
frontier of the fairness-utility problem, in less time than it takes an
LP to compute just one endpoint of the frontier, both in terms of
algorithmic complexity and empirical runtime. Furthermore, our
method is applicable to any reasonable notion of merit, requiring
only that more relevant items have greater or equal merit than less
relevant items.

Our solution strongly relies on the introduction of a geometrical
object, a polytope that we call expohedron, as it generalizes the
well-known permutohedron. While the permutohedron is defined
as the convex hull of all permutations of the vector (1, . . . , 𝑛) ∈ R𝑛,
the 𝜸 -expohedron is defined as the convex hull of all permutations
of an arbitrary vector 𝜸 ∈ R𝑛. Figure 1 illustrates some expohedra
for 𝑛 ∈ {2, 3, 4}. We are not the first to study this kind of polytope,
also called orbit polytope [21], but to the best of our knowledge,
we are the first to apply it to a fair ranking problem. When we set
𝜸 to be the vector of exposures a.k.a. examination parameters of
a Position Based Model (PBM), each vertex of the 𝜸 -expohedron
represents the exposures allocated to the items by a certain ranking.
A vector obtained by a convex combination of 𝑁 vertices represents
the average exposure obtained by the items when delivering each of
the 𝑁 rankings with proportions given by the convex coefficients.

Our main technical contributions are:

(1) We propose an efficient algorithm that takes any target exposure
inside the expohedron as input and finds a distribution over 𝑛
rankings whose expected value is equal to the target exposure,
where 𝑛 is the number of items to rank. Such an algorithm is
called a Carathéodory decomposition [4, 20]. Our algorithm has
complexity 𝑂 (𝑛2 log(𝑛)) and is to the best of our knowledge the
first Carathéodory decomposition algorithm in the expohedron.

2

(2) We propose an efficient algorithm that finds all Pareto-optimal
exposure vectors in the expohedron, given a linear utility objec-
tive and a quadratic fairness objective as in [7], relying only on
simple geometrical constructions. The result of this algorithm
is a set of exposure vectors, each of which can be expressed as
the expectation of a distribution using contribution (1). This
algorithm has complexity 𝑂 (𝑛2 log(𝑛)).

(3) As an alternative to randomly sampling rankings from a distri-
bution, we propose to use a deterministic scheduling method
based on balanced words [36]. Such an approach, as we will
argue, better approximates the actual proportions in the distri-
bution, and thus decreases the variance around the expected
value when the rankings are actually delivered.

(4) We perform experiments on synthetic data and on the TREC2020
Fairness Track [2] and MSLR [25] datasets to compare our meth-
ods to several baselines in terms of Pareto-optimality and effi-
ciency.

2 RELATED WORK

Fairness in ranking. There is a large body of literature that stud-
ies problems of fairness in AI systems. Several applications are
studied, such as classification [10] or regression [19]. In the context
of ranking, there are pre-processing methods [28, 43], whose aim is
to "de-bias" the data before it is processed by a ranking algorithm;
in-processing methods [43], that aim to modify the ranking algo-
rithm itself; and post-processing methods, which re-order the items
only in the end [3, 30, 42]. Our work is a post-processing approach.
Fairness can be enforced on the side of the ranking recipients
[8] or on the side of the item producers [3, 30, 42]. We consider
fairness on the item producer side.

Fairness of exposure can be enforced at the level of groups [30]
with the idea that each group of items should receive a certain
amount of exposure, or at the level of the individual with the idea
that every item [7] or every item producer [3] should receive a
certain amount of exposure. This is called individual fairness [3]. In
this paper we use individual fairness at the level of the items, i.e.,
we ensure that each item gets a certain target exposure.

There are many published definitions of fairness. Some group
fairness definitions [42] look at the items in the top-𝑘 ranks and
enforce that each group is present with equal proportions for every
𝑘. Other approaches called fairness of exposure [3, 7, 18, 22, 30, 31,
33, 38–40], like our own approach, seek to adjust the exposure of
the items, to a certain range of admissible or optimal values. This
is the case with a Learning to Rank approach by Oosterhuis [22]
that applies gradient-descent to find the optimal scores of the items
when the ranking policy is a Plackett-Luce distribution.

Approaches using fairness of exposure depend on the exposure
model that is used. In a Position Based Model (PBM), the exposure
of an item depends only on its rank [6]. With other models such as
Dynamic Bayesian Network (DBN) [5, 6], the exposure of an item
also depends on higher ranked items. Our approach is applicable
to PBMs.

Optimizing bistochastic matrices. Singh and Joachims [30] for-
mulate a ranking policy as a bistochastic matrix whose elements
are the probabilities for an item to end up at a certain rank. When a
PBM is used, it is possible to formulate fairness as a linear constraint

Introducing the Expohedron for Efficient Pareto-optimal Fairness-Utility Amortizations in Repeated Rankings

WSDM ’22, February 21–25, 2022, Tempe, AZ, USA

and utility as a linear objective. The problem is solved using a linear
program (LP) and an optimal bistochastic matrix is found. It is then
possible to decompose the bistochastic matrix as a convex sum of
permutation matrices. This is called a Birkhoff-von Neumann (BvN)
decomposition and expresses the bistochastic matrix as an expected
value of a distribution over permutations. The ranking policy then
consists in sampling from this distribution. This approach has later
been adopted in subsequent papers [32, 33, 39]. Such an approach
finds only one point of the Pareto-frontier: the point with minimal
unfairness, whereas our approach takes less time to find the whole
frontier. This is achieved by working in the expohedron instead of
the Birkhoff polytope (i.e., the set of bistochastic matrices).

Controller. Another more heuristic approach is to use controllers.
A controller generally starts by delivering a PRP ranking and com-
putes at every time-step the exposure difference w.r.t. a certain
target value. This error term is added to the relevance scores and
for the next ranking the items are ordered by this new score. In
other words, a controller greedily corrects the empirical unfairness
at every time-step by giving adequate bonuses to disadvantaged
items. This can be done in a static setting [34] or within a Learning
to Rank framework [18].

3 MODEL AND PROBLEM STATEMENT

Setting. We suppose that we are given a query 𝑞 to which is
associated a set of 𝑛 items indexed by 𝑖. In our setting, all queries
are treated independently. Therefore in the remainder of this paper,
we omit indexing with 𝑞. We further suppose that we are given a
vector of relevance values 𝝆 ∈ [0, 1]𝑛 that expresses how much each
item is relevant to the query. We assume that the query is repeated
by the users many times, giving us sufficiently many opportunities
to execute our amortization.

Ranking. In this paper we denote S𝑛 the set of all permutation
matrices of size 𝑛. We define a ranking as a permutation matrix
𝜋 ∈ S𝑛 such that 𝜋𝑖𝑘 = 1 if and only if the item 𝑖 is at rank 𝑘.

PBM exposure models. We work with a Position Based Model
+ whose 𝑘th component
(PBM) [6], represented by a vector 𝜸 ∈ R𝑛
is the exposure associated to rank 𝑘. We assume w.l.o.g.1 that the
exposure decreases with increasing rank. The exposure is a measure
of how much attention the users dedicate to a certain rank.

Given a ranking 𝜋 ∈ S𝑛, the exposure associated to the items is
given by the vector ℰ (𝜋) = 𝜋𝜸 , where the 𝑖th element of ℰ (𝜋) is
the exposure allocated to item 𝑖.

Given a distribution D of 𝑁 rankings 𝜋1, . . . , 𝜋𝑁 delivered with
proportions 𝑝1, . . . , 𝑝𝑁 , the expected exposure associated to the
items and denoted ℰ (D) is given by

ℰ (D) =

𝑁
∑︁

𝑖=1

𝑝𝑖 ℰ (𝜋𝑖 ) =

𝑁
∑︁

𝑖=1

𝑝𝑖 𝜋𝑖𝜸 .

(1)

Utility. Given a ranking 𝜋 ∈ S𝑛, the utility is the scalar product
of the relevance vector with the exposures associated to the items:

𝑈 (𝜋) := 𝝆⊤ℰ (𝜋) = 𝝆⊤𝜋𝜸 .

(2)

1It is w.l.o.g., because we can just rename the ranks to make it true.

3

When one chooses the 𝑘th element of 𝜸 to be 𝛾𝑘 = 1/log2 (𝑘 + 1) for
all 𝑘 ∈ {1, . . . , 𝑛}, then 𝑈 (𝜋) is the Discounted Cumulative Gain
(DCG) metric [14] of ranking 𝜋. When one chooses 𝛾𝑘 = (1−𝑝)𝑝𝑘−1
for some 𝑝 ∈ (0, 1), then 𝑈 is the Rank Biased Precision (RBP) metric
[17] of ranking 𝜋. Given a distribution D of 𝑁 rankings 𝜋1, . . . , 𝜋𝑁
delivered with proportions 𝑝1, . . . , 𝑝𝑁 , the average utility is

𝑁
∑︁

𝑈 (D) =

𝑝𝑖𝑈 (𝜋) =

𝑁
∑︁

𝑝𝑖 𝝆⊤𝜋𝑖𝜸 = 𝝆⊤ℰ (D).

(3)

𝑖=1

𝑖=1
(Un)fairness. Since there exist multiple definitions of fairness,
in this paper we assume that a decision-maker has decided which
exposures are fair by determining a target exposure for each item,
for instance as in [7] where the target exposure is defined as an
affine function of the relevance vector 𝝆 (some kind of meritocratic
fairness). This target is to be understood as the amount of exposure
an item deserves each time the query is made. If this target exposure
is reached then the system is fair by definition. In order to define
how unfair a system is we need a function that measures how far
we are from the target exposure. In this paper we proceed as in [7]
in adopting the standard euclidean norm. Formally, if we denote
ℰ ∗ the target exposure and ℰ (D) the actual expected exposure
allocated to the items, then the unfairness is
(cid:13)ℰ (D) − ℰ ∗(cid:13)
(cid:13)2.

𝐹 (D, ℰ ∗) :=(cid:13)

(4)

While in [30], fairness is defined as a condition that is either satisfied
or not, we consider here a way to quantify it. This will allow us to
trade off utility with fairness.

Objective. Note that both 𝑈 anf 𝐹 depend on D only through its
expectation so that we can write 𝑈 (ℰ (D)) and 𝐹 (ℰ (D), ℰ ∗). Our
goal is to solve a Multi-Objective Optimisation (MOO) problem with
two objectives, the maximization of utility and the minimization of
unfairness after the delivery of a large number of rankings:

max
D

𝐹 (ℰ (D), ℰ ∗).

𝑈 (ℰ (D)), min
D
In principle one would expect the optimization variable to be a
sequence of rankings. However we decompose the problem into 3
distinct sub-steps.
(1) Find all Pareto-optimal expected exposure vectors ℰ. We solve

(5)

this problem in Section 5.

(2) Given an expected exposure vectors ℰ, find a probability distri-
bution D over 𝑛 rankings, whose expectation ℰ (D) is equal to
ℰ. We solve this problem in Section 4.2.

(3) Deliver the rankings of the support of D on a finite number of
samples, with proportions as close as possible to the probabili-
ties in D. We solve this problem in Section 6.
This procedure is analogous to the one used by Singh and Joachims
[30]. In their paper Step (1) is done using linear programming, but
only an endpoint of the Pareto-frontier is determined and it is em-
pirically found to be slower than our method. Step (2) is done using
a BvN decomposition algorithm [9], but the decomposition is done
over at most (𝑛 − 1)2 + 1 rankings, whereas we use at most 𝑛 and
are much quicker doing so. Step (3) is done using sampling, which
does not very well approximate the target proportions at low time
horizons.

WSDM ’22, February 21–25, 2022, Tempe, AZ, USA

Kletti et al.

4 THE EXPOHEDRON AND A

CARATHÉODORY DECOMPOSITION

The permutation simplex. A naive way of representing distribu-
tions over permutations is to consider the space R𝑛!, with each
basis vector representing one permutation in S𝑛. Every point in
the R𝑛!-simplex then corresponds to exactly one distribution over
permutations, given by its decomposition into basis vectors. Fur-
thermore the R𝑛!-simplex is the convex hull of all permutations
as represented in the space R𝑛!. In practice this approach is not
feasible, because of the high dimensionality of the space R𝑛!.

The Birkhoff polytope. In the case of PBM exposure models, we
can work in a smaller, more tractable space: the space of bistochas-
tic matrices a.k.a. Birkhoff polytope [30]. Indeed in order to know
the expected exposure an item gets from a distribution over rank-
ings, it suffices to know the marginal probabilities to end up at
each rank. This reduces the dimensionality of the space to 𝑛2. The
Birkhoff polytope is also the convex hull of all permutations, repre-
sented by permutation matrices. There exists an algorithm, called
Birkhoff-von Neumann (BvN) decomposition [9], that can express
any bistochastic matrix of size 𝑛 × 𝑛 as a convex sum of at most
(𝑛 − 1)2 + 1 permutation matrices.

The expohedron. It is possible to reduce even further the dimen-
sionality of the space we work in. Indeed what interests us in the
end is the exposure that the items get, not with which frequency
they are at certain ranks. For a PBM with exposure vector 𝜸 , the
𝜸 -expohedron is formally defined as

Π(𝜸 ) := Conv (cid:0)𝜋𝜸 | 𝜋 ∈ S𝑛 (cid:1) .
It is the convex hull of exposure vectors achievable with all possible
rankings.
Theorem 1 (Carathéodory [4, 20]). Let Π ∈ R𝑛 be the convex hull
of a finite number of points v1, . . . , v𝑁 . Then any point x ∈ Π can be
expressed as the convex sum of at most 𝑛 + 1 points v𝑖 .

(6)

Carathéodory’s theorem tells us that it is possible to express any
point in the expohedron as a convex sum of at most 𝑛 + 1 vertices.
Such a convex sum is called Carathéodory decomposition. In our
case the sum can be made over at most 𝑛 vertices, because the
expohedron lives within a hyperplane of R𝑛. Indeed by permuting
the elements of 𝜸 , the sum of the elements of 𝜋𝜸 remains unchanged.
Therefore the vector 1 is orthogonal to the expohedron.

A Carathéodory decomposition can express any point ℰ inside
the expohedron as the expected value ℰ (D) of a distribution D
over at most 𝑛 rankings.

4.1 Properties of the expohedron

Majorization. The concept of majorization [16] gives us an easy-
to-check criterion for verifying if a point is inside the expohedron.
We say that a vector a ∈ R𝑛 is majorized by a vector b ∈ R𝑛 and
we write a ≺ b if and only if

∀𝑘 ∈ {1, . . . , 𝑛},

𝑘
∑︁

𝑖=1

↑
𝑖 ≤
b

𝑘
∑︁

𝑖=1

↑
𝑖 ,
a

𝑛
∑︁

𝑖=1

↑
𝑖 =
b

𝑛
∑︁

𝑖=1

↑
𝑖 ,
a

(7)

where a↑ and b↑ are the vectors a and b with elements ordered
from the smallest to the greatest.

4

Then one can show that a point x is inside the 𝜸 -expohedron if

and only if it is majorized by 𝜸 [16, 26]. Formally

x ∈ Π(𝜸 ) ⇐⇒ x ≺ 𝜸 .

(8)

Order-preserving zone. We introduce the concept of order-preserving

zones or zones as 𝑛! subsets of R𝑛, each corresponding to a permu-
tation. We denote by 𝑍 (𝜋) the zone corresponding to the set of
vectors x such that the elements of 𝜋x are sorted in increasing order.
Figures 2 and 3 illustrate the subdivision of an expohedron into 6
zones for 𝑛 = 3 documents. If a point x has ties, then we consider
the point to be a member of all zones corresponding to the possible
orderings.

Face characterization. Recall that a face of a polytope is a poly-
tope on its boundary. For instance vertices, edges and facets of a
dice, are all faces of the dice. We give an easy-to-check criterion
for identifying the lowest-dimensional face in which a point of the
expohedron lies. A face of the expohedron is characterized by a
zone and a subset of {1, . . . , 𝑛}, that we call splits.

Specifically, the face characterized by zone 𝑍 (𝜋) and by the splits
𝑆, is composed of all the points x ∈ Π(𝜸 ) satisfying the condition

∀𝑖 ∈ 𝑆,

𝑖
∑︁

𝑘=1

(𝜋x)𝑘 =

𝑖
∑︁

𝑘=1

,

↑
𝑘

𝜸

(9)

where 𝜸 ↑ is the vector 𝜸 with elements ordered from the smallest
to the greatest. The dimension of a face is given by dim(𝐹 ) = 𝑛 − |𝑆 |.
For a vertex, we have 𝑆 = {1, . . . , 𝑛} and its dimension is 0. For the
whole polytope, we have 𝑆 = {𝑛} and its dimension is 𝑛 − 1. From
the majorization property it follows that all faces of the expohedron
have 𝑛 in their splits.

With this property it becomes easy to determine the faces in
which a point x of the expohedron lies. It suffices to set 𝜋 such that
𝜋x is sorted in increasing order and to check for which 𝑖 (9) holds.

4.2 Carathéodory decomposition
The expohedron is a convex hull of dimension 𝑛 − 1, so from
Carathéodory’s theorem [4, 20] we know that any of its points can
be expressed as a convex combination of at most 𝑛 of its vertices.
This is already a significant improvement over the approach using
BvN decomposition, as BvN decompositions use at most (𝑛 − 1)2 + 1
permutation matrices [15].

There exist already some published Carathéodory decomposition
algorithms for permutohedra [13, 41], but they cannot naturally
be extended to expohedra. Expohedra are not always zonotopes,
which is a key property used in [13], and they are generally not
defined with 𝜸 = (1, . . . , 𝑛), which is a key property used in [41].
In this section we propose to adapt a generic Carathéodory
algorithm, the GLS method named after Grötschel, Lovász and
Schrijver [12]. Its main idea is illustrated in Figure 2.

The GLS method for decomposing a point x consists in choosing
an arbitrary vertex v1 and drawing a half-line starting from v1 and
passing through x. This half-line intersects the polytope’s boundary
at a point p2 lying on a face of dimension 𝑛 − 2. It is easy to see
that x can be expressed as a convex sum of v1 (a vertex) and p2 (not
necessarily a vertex). The point p2 is then decomposed using the
same method: we choose an arbitrary vertex v2 from the face of p2
and draw a half-line from v2 through p2 intersecting the polytope’s

Introducing the Expohedron for Efficient Pareto-optimal Fairness-Utility Amortizations in Repeated Rankings

WSDM ’22, February 21–25, 2022, Tempe, AZ, USA

v2

p2

x = p1

p3 = v3

v2

v1

v0

v1

Figure 2: GLS procedure, inspired from an illustration in [13]. The barycenter
x of the expohedron is decomposed into a convex sum of vertices v1, v2, v3.
The dotted grey lines materialize the subdivision of the expohedron into
order-preserving zones.

boundary at a point p3 lying on a face of dimension 𝑛 − 3 and we
express p2 as a convex sum of v2 and p3. Repeating this procedure
recursively, there will come a point at which the intersection p𝑛 is
itself a vertex of the polytope and we are finished.

This procedure has two delicate operations that need to be

adapted to the particular polytope on which it is applied:
(1) We need a method to find the intersection of a half-line with

the polytope’s boundary;

(2) For a point on a face, we need to find a vertex on the same face.
To find a vertex on the same face as a point x, it suffices to
permute the elements of 𝜸 such that its result to argsort is the
same as for x. This operation has complexity 𝑂 (𝑛 log(𝑛)), because
it is as complex as a sorting operation, and can be expressed in
python2 as 𝜸 [argsort(argsort(−x))].

For Step (1), one can use the majorization criterion to make
a bisection search on the half-line. Such a bisection search has
complexity 𝑂 (𝑛 log(𝑛)), because the sort operation to check the
majorization needs to be done 𝑂 (1) times.

Algorithm 1 The GLS method in the expohedron.

1: procedure GLS(Input: x ∈ Π(𝜸 ))
2:

v1 ← 𝜸 [argsort(argsort(−x))]

vertex

⊲ Choose an initial

⊲ Set the initial vertex’s weight to 1

𝛼1 ← 1
p1 ← x
for 𝑖 ∈ {1, . . . , 𝑛} do

p𝑖+1 ← max𝜆 ≥1 v𝑖 +𝜆(p𝑖+1−v𝑖 ) s.t. v𝑖 +𝜆(p𝑖+1−v𝑖 ) ≺ 𝜸
𝛼𝑖+1 ← 𝛼𝑖 − ∥p𝑖 −p𝑖+1 ∥
𝛼𝑖 ⊲ Update convex coefficients
∥p𝑖 −v𝑖 ∥
𝛼𝑖 ← ∥p𝑖 −p𝑖+1 ∥
𝛼𝑖
∥p𝑖 −v𝑖 ∥
v𝑖+1 ← 𝜸 [argsort(argsort(−p𝑖+1))]

3:

4:

5:

6:

7:

8:

9:

end for

10:
11: end procedure Output: 𝛼1, . . . , 𝛼𝑛, v1, . . . , v𝑛

Theorem 2. Algorithm 1 find a Carathéodory decomposition of any
point in the expohedron in 𝑂 (𝑛2 log(𝑛)) time.

Indeed the GLS procedure consists in executing at most 𝑛 times
the steps (1) and (2), which have complexity 𝑂 (𝑛 log(𝑛)), so the
complexity of the algorithm we propose is 𝑂 (𝑛2 log(𝑛)).

2Assuming 𝜸 has decreasing components.

5

Figure 3: The Pareto curve in a DCG expohedron for 𝝆 = (0.55, 0.6, 0.65) rep-
resented as a blue arrow. The point v0 is the meritocratic target exposure,
∥𝜸 ∥1
∥𝝆 ∥1 𝝆. The red circles are equi-unfairness curves and the blue lines are
i.e.,
equi-utility lines. The green line segments form the Pareto-curve in the expo-
hedron. The dotted grey lines materialize the subdivision of the expohedron
into order-preserving zones.

5 FINDING THE PARETO CURVE
In this section we show how we can solve the MOO problem (5). We
assume that the target exposure ℰ ∗ is in the same zone as the PRP
ranking. This is a weak assumption, because it is another way of
saying that items with higher relevance must not get less exposure
than items with lower relevance. Furthermore we assume that the
target exposure is inside the expohedron, i.e., the target is feasible.
The decision variable ℰ also needs to be inside the expohedron,
in order to be the expected value of a distribution over rankings
(i.e., a convex sum of vertices). So our constraint can be written as
ℰ ≺ 𝜸 . Formally our optimization problem is:
𝝆⊤ℰ, min
ℰ

2
(cid:13)ℰ − ℰ ∗(cid:13)
(cid:13)
2,
(cid:13)
The maximization objective is linear and the minimization objec-
tive is quadratic and isotropic. The level sets of the utility objective
are hyperplanes with orthogonal vector 𝝆 and the level sets of the
fairness objective are hyperspheres centered at ℰ ∗.

s.t. ℰ ≺ 𝜸 .

max
ℰ

(10)

It is now easy to see that amongst all points on an equi-utility
hyperplane, the point that minimizes unfairness is the one at which
an equi-unfairness hypersphere is tangent, see Figure 3. So to find
the Pareto curve, we just need to start from v0 = ℰ ∗ and go in the
direction of 𝝆 until we intersect the border of the expohedron. This
intersection can be computed analytically using (12), because we
stay in the same zone by moving in the direction of 𝝆. Indeed the
intersection of a half-line v + 𝜆𝝆 with the expohedron must satisfy
(11)

v + 𝜇𝝆 s.t. v + 𝜇𝝆 ≺ 𝜸 .

𝜆 = arg max
𝜇 ≥0

The elements of v + 𝜆𝝆 stay in the same order for 𝜆 ≥ 0, because v
and 𝝆 are in the same zone. It follows after some algebraic manipu-
lations that

(cid:27)

|𝐷𝑘 < 0

,

(12)

𝑖=1 𝜸

↑
𝑖 , 𝐷𝑘 = (cid:205)𝑘

𝑖=1 (x − v) ↑

where 𝐺𝑘 = (cid:205)𝑘
𝑖 , and the
𝑖=1 v
arrow ↑ indicates a vector sorted from smallest to greatest element.
Once we are at the intersection, we can apply a similar reasoning
to get the next segment of the Pareto curve. The intersection of a
hypersphere with the affine subspace containing the current face is
another hypersphere of lower dimension. Similarly the intersection
of a utility hyperplane with the affine subspace containing the

(cid:26) 𝐺𝑘 − 𝑉𝑘
𝐷𝑘

𝜆 = min

𝑘
↑
𝑖 , 𝑉𝑘 = (cid:205)𝑘

WSDM ’22, February 21–25, 2022, Tempe, AZ, USA

Kletti et al.

current face is itself an affine subspace of lower dimension. Thus
we need to go in the direction of the projection of 𝝆 on the affine
subspace containing the current face, until we meet the border
of the expohedron again in a face of lower dimension. In the end
we are guaranteed to end up at a point maximizing utility, which
constitutes the endpoint of the Pareto curve.

Algorithm 2 Pareto set identification. We assume w.l.o.g. that the
zone 𝑍 (𝜋) of 𝝆, v(0) is the one where the values are ordered from
smallest to greatest. This corresponds to making a change of basis
such that 𝜋 becomes the identity matrix. We denote 𝜸 ↑ the vector
𝜸 ordered from smallest to greatest element.

1: procedure Pareto(Input: 𝜸 , target exposure v(0) , 𝝆)
2:

↑
𝑖=1 𝜸
𝑖
(0)
𝑖=1 v
𝑖

𝐺𝑘 ← (cid:205)𝑘
𝑉𝑘 ← (cid:205)𝑘
⊲ Initilize the set of splits
I0 ← {𝑛}
𝝆 (0) ← 𝝆 − (𝝆⊤1)1/𝑛 ⊲ Project 𝝆 on the hyperplane with

normal vector 1
𝑙 ← 0
while 𝝆⊤v(𝑙) < 𝝆⊤𝜸 ↑ do ⊲ While utility is not maximal

𝑖=1 𝝆

|𝐷𝑘 < 0(cid:111)

(𝑙)
𝑖
(cid:110) 𝐺𝑘 −𝑉𝑘
𝐷𝑘

𝐷𝑘 ← (cid:205)𝑘
𝜆𝑙 ← min𝑘
v(𝑙+1) ← v(𝑙) + 𝜆𝑙 𝝆 (𝑙)
(𝑙+1)
𝑉𝑘 ← (cid:205)𝑘
𝑖=1 v
𝑖
I𝑙+1 = {𝑖1, . . . , 𝑖𝑙+1} ← which(𝑉𝑘 == 𝐺𝑘 ) ⊲ I𝑙+1 is the

⊲ Compute the intersection

set of splits identifying the current face.

⊲ Identify the new split

{𝑖 𝑗 } ← I𝑙+1 \ I𝑙
𝑚/(cid:205)𝑖 𝑗 +1
𝜓 ← (cid:205)𝑖 𝑗
↑
𝑚=𝑖 𝑗 −1 𝜸
𝝂 ← (0, . . . , 0, 1, . . . , 1
(cid:32)(cid:32)
(cid:32)(cid:32)
(cid:124)
(cid:125)
(cid:123)(cid:122)
𝑖 𝑗 −1,...,𝑖 𝑗

↑
𝑚

𝑚=𝑖 𝑗 +1 𝜸
, 0, . . . , 0)⊤
, −𝜓, . . . , −𝜓
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:125)
(cid:124)
(cid:123)(cid:122)
𝑖 𝑗 +1,...,𝑖 𝑗 +1

⊲ The

new normal vector to the face that was just intersected.

𝝆 (𝑙+1) ← 𝝆 (𝑙) − [(𝝆 (𝑙) )⊤𝝂]𝝂/∥𝝂 ∥2

2 ⊲ Project 𝝆 on the

new face

𝑙 ← 𝑙 + 1

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

end while

18:
19: end procedure, Output: a sequence of at most 𝑛 points
(v(𝑙) )𝑙 ∈ {0,...,𝑛−1} that defines the Pareto curve as the union
of the line segments connecting these points.

Theorem 3. Algorithm 2 returns the Pareto front for the multi-
objective optimization problem (10) and has complexity 𝑂 (𝑛2 log(𝑛)).

Indeed the while loop is done at most 𝑛 − 1 times and in each
loop the most complex operation is the sorting operation with
complexity 𝑂 (𝑛 log(𝑛)).

6 BALANCED WORDS
A standard approach for delivering a sequence of rankings while
respecting certain desired proportions as much as possible, is to
sample from a categorical distribution. For instance Singh and
Joachims [30] do a BvN decomposition, then randomly sample
from the obtained distribution. In the same vein, Oosterhuis [22]

optimizes the parameters of a Plackett-Luce (PL) distribution [24]
and directly samples the sequence of rankings from it.

In this section we argue that there is a better way of deliver-
ing rankings from a distribution that does not involve stochastic
sampling. For example suppose we do coin flips and we have deter-
mined that heads and tails should be delivered each 50% of the time.
If we do ten coin flips (i.e., stochastic sampling), the probability that
we get 5 heads and 5 tails (i.e., that we are fair) is 252/1024 ≈ 1/4.
Therefore a better policy is to manually take the coin and lay it
alternatively once on the head once on the tail, and so on.

This intuition can be generalized to non-uniform distributions
over more than two possible values by using 𝑚-balanced words
[36]. When expressed in the terms of our problem, a generator
of 𝑚-balanced words produces a sequence of rankings such that,
in any pair of sub-strings with identical length, the frequency of
any ranking differs at most by 𝑚. Such a sequence is called a word
and a ranking corresponds to a letter in this word. In other words,
this generator guarantees that the generated sequence delivers the
rankings with proportions as close as possible to the target ones.
An important theoretical fact is that the best achievable 𝑚 is at most
the number of unique letters (i.e., rankings) minus 1 [29]. Another
advantage of this approach with respect to a BvN decomposition
then becomes apparent: The fact that our distribution is over at
most 𝑛 distinct rankings instead of (𝑛 − 1)2 + 1 means that we
are able to deliver our distribution in a much more balanced way
than with a BvN decomposition, i.e., with 𝑚 = 𝑛 − 1 instead of
𝑚 = (𝑛 − 1)2. An algorithm capable of efficiently generating 𝑚-
balanced words, given a certain distribution of letters is given in
Algorithm 1 of [29] and in Appendix B. This generator is equivalent
to the well-known Stride Scheduling algorithm, used to generate fair
sequences in resource (CPU) management for concurrent processes
[37].

7 EXPERIMENTS
In order to empirically verify our claims, we perform experiments
on both synthetic data and on publicly available datasets. Our source
code is available on github3. We executed the computations on a
laptop with an Intel®Core™i7-8650U CPU @ 1.90GHz processor.
We use the TREC2020 Fairness Track evaluation queries and
items [2] for which we computed relevance probabilities ourselves.
The computed values as well as details about the method we used
to compute them will be made available in our github repository as
they are not of primary importance for this paper. We also use the
MSLR dataset [25] for which we used the ground truth relevances
graded from 0 (worst grade) to 4 (best grade), in order to check the
influence of having discrete-valued relevance values. We normalize
them to the range [0, 1] by dividing them by 4.

For both datasets we restrain ourselves to queries having fewer
than 100 items, because our LP baseline would take too much time
on these queries and because of some numerical issues in our im-
plementation, discussed in more detail in Appendix A. Furthermore
we eliminate uninteresting queries having only one document and
queries for which all relevance values are equal. This leaves 795
amongst 867 queries for MSLR and 198 amongst 200 queries for
TREC.

3https://github.com/tillkletti/expohedron

6

Introducing the Expohedron for Efficient Pareto-optimal Fairness-Utility Amortizations in Repeated Rankings

WSDM ’22, February 21–25, 2022, Tempe, AZ, USA

7.1 Metrics and baselines
For the exposure model we use the exposure vector 𝜸 of DCG whose
𝑘th element is 1/log2 (𝑘 + 2). The metric measuring the utility is
nDCG, which divides the DCG by the "ideal" DCG obtained with
PRP rankings only. To aggregate results over several queries, we
compute the arithmetic mean of the nDCGs. This indicates for a
given ranking policy, what percentage of the maximal utility we
achieve on average.

We consider the relevance values to be the merit of the items. By

e
m
T

i

6

4

2

0

Expo
LP
GLS
expo end
BvN

Runtime in seconds

default, we define the target exposure as ∥𝜸 ∥1
𝝆, so that the sum of
∥𝝆 ∥1
exposures has the right scale. When the target exposure happens
not to be inside the expohedron, this definition leads to infeasible
target exposures. In that case, we add a constant to the merits of
all documents until the target becomes feasible, by choosing the
smallest value 𝑏 ∈ R+ such that ℰ ∗ := (1 − 𝑏)
𝑛 1 ≺ 𝜸 .
This can be efficiently computed using (12), since it corresponds to

𝝆 + 𝑏 ∥𝜸 ∥1

∥𝜸 ∥1
∥𝝆 ∥1

finding the intersection of the line segment

(cid:20) ∥𝜸 ∥1
∥𝝆 ∥1

(cid:21)

𝝆, ∥𝜸 ∥1
𝑛 1

with

the border of the expohedron. To aggregate results over several
queries, we average a normalized unfairness: ∥ℰ−ℰ∗ ∥2
to bring the
∥𝜸 ∥1
exposures to the same scale. We evaluate our own method and 4
baselines, in their ability to solve our MOO problem, both in terms
of effectiveness and efficiency:

(1) Our own Expohedron (Expo) method with our own Carathéodory
decomposition (GLS) delivered using an 𝑚-balanced word (BW)
generator as described in [29]. In a variant (expo end), we com-
pute only the fairness endpoint of the Pareto-front.

(2) A Plackett-Luce (PL) distribution [24] with parameters 𝝆 and
with a varying temperature parameter 𝜏. The temperature pa-
rameter controls the interpolation between PRP rankings and a
uniform distribution over all rankings.

(3) A Linear Program (LP) approach as proposed by [30]. There is
no trade-off between relevance and fairness with this approach;
only an endpoint of the Pareto front is found, corresponding
to zero unfairness, called fairness endpoint. We use the cvxopt
solver [1] to solve the LP and we use a BvN decomposition
implemented in [35].

(4) A Quadratic Program (QP) that finds bistochastic matrices max-

imizing, for varying 𝛼, the scalarized MOO

max
𝜋 ∈Conv( S𝑛)

𝛼𝝆⊤𝜋𝜸 − (1 − 𝛼)(cid:13)

2
(cid:13)𝜋𝜸 − ℰ ∗(cid:13)
2.
(cid:13)

(13)

(5) A Controller (Ctrl) as proposed by [34] with parameter 𝐾 = 1
and for varying gain, hoping to find approximately Pareto-
optimal solutions. This controller computes at each time-step
whether an item is disadvantaged and tries to compensate this
in future rankings by increasing its relevance with a small bonus.
Then a PRP ranking is delivered using the modified relevances.

7.2 Results

How does the runtime of our method and of the baselines vary
when the number of items increases ? In order to fully control the
number of items, we use synthetic data for this research question.
For each 𝑛 ∈ {2, . . . , 100}, we sample 100 random relevance vectors
whose elements are uniformly independently distributed in [0, 1].

7

0

20

40

60

80

100

Number of items

Figure 4: Runtime of different algorithmic modules as a function of the num-
ber of items 𝑛. For each evaluated 𝑛, 100 random relevance vectors are sam-
pled uniformly and independently. The shaded areas correspond to 95% quan-
tiles. The yellow curve corresponding to the BvN leaves the frame, reaching
70 seconds by 𝑛 = 100. For a global comparison, the sum of the expo end and
GLS curves should be compared with the sum of the LP and BvN curves.

For each of those 𝑛, we compute the full Pareto front (expo) in the
expohedron. We compute the bistochastic matrices corresponding
to the fairness endpoint of the point using LP and compute its
BvN decomposition. For a fair comparison with LP, we derive the
fairness endpoint for each 𝑛 using our Expo method (expo end),
and its Carathéodory decomposition with Algorithm 1 (GLS).

The average times the different algorithmic components take
are reported in Figure 4. It appears that the BvN decomposition is
particularly slow, while the computation of the fairness endpoint
using our Expo method is almost instantaneous. In particular it
appears that the advantage of our method (fairness endpoint com-
putation + GLS decomposition) w.r.t. an LP approach (including the
BvN decomposition) increases with increasing number of items.

How far are the baseline methods from complete Pareto optimality?
Among our baselines, QP is guaranteed to produce Pareto-optimal
points, provided that the associated QP solver has sufficient preci-
sion. For both the TREC and the MSLR datasets, we generate points
of all Pareto-fronts (one front per query) by varying the trade-off
parameter 𝛼 in (13) between 0 and 1. To plot utility-unfairness
points corresponding to the PL approach, we compute for each
query the performance of PL after 𝑇 = 1 000 rankings, with the
temperature parameter 𝜏 varying in [1 ·10−3, 1] for TREC and in
[1 · 10−5, 10] for MSLR. The performances corresponding to the
same temperature are aggregated over the different queries with
the aggregation method described in 7.1.

For Ctrl, we generate points in the same utility-unfairness space
by varying the gain within [0, 1 000] for TREC and within [0, 10 000]
for MSLR. The performances corresponding to the same gain are
aggregated over the different queries to produce the plotted points.
Our Expo method does not produce a parametrized set of utility-
unfairness solutions. Knowing that our Expo method should the-
oretically produce the same solutions as QP for some value of 𝛼,
we check that every solution of the QP, expressed as an exposure
vector after multiplying the optimal bistochastic matrix by the 𝜸
vector, corresponds to a point of the Expo-derived Pareto-front and
associate the corresponding 𝛼 to the latter. We are then able to ag-
gregate the performances over the different queries, by considering
the solutions associated to the same 𝛼.

WSDM ’22, February 21–25, 2022, Tempe, AZ, USA

Kletti et al.

Aggregated Pareto fronts
·10−2

Aggregated Pareto fronts
·10−2

Convergence speed

Convergence speed

QP
Expo
PL
Ctrl

s
s
e
n
r
i
a
f
n
u
d
e
z
i
l
a
m
r
o
N

8

6

4

2

0

QP
PL
Ctrl
Expo

s
s
e
n
r
i
a
f
n
u
d
e
z
i
l
a
m
r
o
N

8

6

4

2

0

0.94 0.96 0.98
Utility (nDCG)

1

0.94 0.96 0.98
Utility (nDCG)

1

(a) MSLR

(b) TREC

Figure 5: Aggregated Pareto fronts. The red and blue points are overlaid along
the Pareto front and therefore hard to distinguish.

s
s
e
n
r
i
a
f
n
u
d
e
z
i
l
a
m
r
o
N

10−1

10−2

10−3

10−4

Expo bw

Expo rand

PL

LP rand

Ctrl

0 200 400 600 8001,000
Time

(a) MSLR

Expo bw

Expo rand

PL

LP rand

Ctrl

s
s
e
n
r
i
a
f
n
u
d
e
z
i
l
a
m
r
o
N

10−1

10−2

10−3

10−4

0 200 400 600 8001,000
Time

(b) TREC

The results are reported in Figure 5. It appears without surprise
that both QP and Expo perform identically, while PL is far from
Pareto-optimality, except in the region where nDCG reaches 1. More
surprisingly Ctrl performs well for a heuristic approach, with its
points being indistinguishable from the actual Pareto front, except
for an outlier obtained with gain = 0 for the MSLR dataset. However,
as it does not explicitly address the problem as a MOO problem and
as it uses a parameter (the gain) which controls very indirectly the
utility-fairness trade-off, there is as of yet no guarantee that it will
always result in near-optimal Pareto-fronts in all settings.

How do the objectives evolve over time ? Does using balanced words
provide an advantage w.r.t. sampling ? For each method we select
the parameter that brings it closest to minimal unfairness. For Expo
this corresponds to simply choosing the fairness endpoint. For PL
we select 𝜏 = 1. For Ctrl we select a gain of 0.6 for TREC and a
gain of 10 for MSLR. For LP there is no parameter to set and we
do not apply a QP, since LP does the same job more efficiently.
We aggregate the results over all queries by taking the average
normalized unfairness at each time-step.

The results are reported in Figure 6. It appears that sampling
from a distribution found via BvN or via GLS has no notable effect
on transient performance. However using balanced words instead of
sampling does have a beneficial effect on the speed of convergence
to the target exposure. It also appears that Ctrl does converge more
quickly than all other methods, at least for the fairness endpoint.

Is Expo faster than existing baselines ? In Table 1 we report the
time it took to compute the solutions using our Expo method and the
baselines. It appears that Expo is clearly superior to the other exact
methods LP and QP in terms of runtime. For 𝑇 = 1 000 rankings Ctrl
is quicker than Expo. However for 𝑇 = 10 000 rankings, Ctrl takes
ten times as long, whereas for the Expo method, the computation
of the Pareto-front (or endpoint) and GLS need to be performed
but once. Then once a solution is found, it can be delivered very
quickly using balanced words. Thus for larger time horizons with
an order of magnitude of several thousand rankings, Expo becomes
quicker than Ctrl for the total runtime.

8

Figure 6: Average normalized unfairness as a function of the number of deliv-
ered rankings. The red and blue curves are almost equal and may be difficult
to distinguish.

Table 1: Average runtimes in seconds for each dataset for 𝑇 = 1 000 and 𝑇 =
10 000 rankings.

Expo endpoint
Expo
LP
QP
GLS
BvN
BW
rand

TREC
𝑇 = 1 000
0.0014
0.0806
0.0858
0.8424
0.0737
2.2623
0.0009
0.0388

Expo+GLS+BW 0.1552
2.3869
LP+BvN+rand
0.0245
Ctrl
0.0145
PL

MSLR
𝑇 = 1 000
0.0018
0.0199
2.1369
15.2095
0.5396
29.1706
0.0010
0.0469

0.5605
31.3544
0.0268
0.0174

TREC
𝑇 = 10 000
0.0014
0.0806
0.0858
0.8424
0.0737
2.2623
0.0094
0.4060

MSLR
𝑇 = 10 000
0.0018
0.0199
2.1369
15.2095
0.5396
29.1706
0.0093
0.4340

0.1637
2.7541
0.2201
0.1525

0.5688
31.7415
0.2543
0.1991

8 CONCLUSION
Our novel geometrical framework makes it possible to efficiently
compute all Pareto-optimal fairness-utility amortizations for a PBM.
Amongst the methods that are provably Pareto-optimal, our method
is the overall quickest. The controller from [34] empirically per-
forms equally well and is quicker for time horizons lower than
several thousands.

In future work we plan to extend our framework to group fair-
ness and to different exposure models that are not PBMs, such as
Dynamic Bayesian Network models [6].

ACKNOWLEDGMENTS
This work has been partially supported by MIAI@Grenoble Alpes,
(ANR-19-P3IA-0003).

Introducing the Expohedron for Efficient Pareto-optimal Fairness-Utility Amortizations in Repeated Rankings

WSDM ’22, February 21–25, 2022, Tempe, AZ, USA

REFERENCES
[1] 2021. cvxopt/cvxopt. https://github.com/cvxopt/cvxopt original-date: 2013-02-

22T22:14:31Z.

[2] Asia J. Biega, Fernando Diaz, Michael D. Ekstrand, and Sebastian Kohlmeier.
2020. Overview of the TREC 2020 Fair Ranking Track. In The Twenty-Eighth Text
REtrieval Conference (TREC 2020) Proceedings.

[3] Asia J. Biega, Krishna P. Gummadi, and Gerhard Weikum. 2018. Equity of
Attention: Amortizing Individual Fairness in Rankings. In The 41st International
ACM SIGIR Conference on Research & Development in Information Retrieval (Ann
Arbor, MI, USA) (SIGIR ’18). 405–414.

[4] C. Carathéodory. 1907. Über den Variabilitätsbereich der Koeffizienten von
Potenzreihen, die gegebene Werte nicht annehmen. Math. Ann. 64, 1 (March
1907), 95–115.

[5] Olivier Chapelle and Ya Zhang. 2009. A dynamic bayesian network click model
for web search ranking. In Proceedings of the 18th international conference on
World wide web (WWW ’09). 1–10.

[6] Aleksandr Chuklin, Ilya Markov, and Maarten de Rijke. 2015. Click Models for
Web Search. Synthesis Lectures on Information Concepts, Retrieval, and Services 7,
3 (July 2015), 1–115.

[7] Fernando Diaz, Bhaskar Mitra, Michael D. Ekstrand, Asia J. Biega, and Ben
Carterette. 2020. Evaluating Stochastic Rankings with Expected Exposure. In
Proceedings of the 29th ACM International Conference on Information & Knowledge
Management (CIKM ’20). 275–284.

[8] Virginie Do, Sam Corbett-Davies, Jamal Atif, and Nicolas Usunier. 2021. Online
certification of preference-based fairness for personalized recommender systems.
arXiv:2104.14527 [cs, stat] (April 2021). arXiv: 2104.14527.

[9] Fanny Dufossé and Bora Uçar. 2016. Notes on Birkhoff–von Neumann decom-
position of doubly stochastic matrices. Linear Algebra Appl. 497 (May 2016),
108–115.

[10] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard
Zemel. 2012. Fairness through Awareness. In Proceedings of the 3rd Innovations
in Theoretical Computer Science Conference (Cambridge, Massachusetts) (ITCS
’12). 214–226.

[11] Sahin Cem Geyik, Stuart Ambler, and Krishnaram Kenthapadi. 2019. Fairness-
Aware Ranking in Search & Recommendation Systems with Application to
LinkedIn Talent Search. In Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining (KDD ’19). 2221–2231.
[12] Martin Grötschel, László Lovász, and Alexander Schrijver. 1993. Geometric Algo-
rithms and Combinatorial Optimization (2 ed.). Springer-Verlag, Berlin Heidel-
berg.

[13] Ruben Hoeksma, Bodo Manthey, and Marc Uetz. 2016. Efficient implementation
of Carathéodory’s theorem for the single machine scheduling polytope. Discrete
Applied Mathematics 215 (Dec. 2016), 136–145.

[14] Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated Gain-based Evaluation

of IR Techniques. ACM Trans. Inf. Syst. 20, 4 (Oct. 2002), 422–446.

[15] M. Marcus and R. Ree. 1959. Diagonals of doubly stochastic matrices. The

Quarterly Journal of Mathematics 10, 1 (Jan. 1959), 296–302.

[16] Albert W. Marshall, Ingram Olkin, and Barry C. Arnold. 2011. Inequalities: Theory
of Majorization and Its Applications. Springer New York, New York, NY.
[17] Alistair Moffat and Justin Zobel. 2008. Rank-biased precision for measurement
of retrieval effectiveness. ACM Transactions on Information Systems 27, 1 (Dec.
2008), 2:1–2:27.

[18] Marco Morik, Ashudeep Singh, Jessica Hong, and Thorsten Joachims. 2020. Con-
trolling Fairness and Bias in Dynamic Learning-to-Rank. In Proceedings of the 43rd
International ACM SIGIR Conference on Research and Development in Information
Retrieval (SIGIR ’20). 429–438.

[19] Harikrishna Narasimhan, Andrew Cotter, Maya Gupta, and Serena Wang. 2020.
Pairwise Fairness for Ranking and Regression. In Proceedings of the AAAI Confer-
ence on Artificial Intelligence (AAAI ’21). 5248–5255.

[20] Márton Naszódi and Alexandr Polyanskii. 2019. Perron and Frobenius meet

Carathéodory. arXiv:1901.00540 [math] (Jan. 2019). arXiv: 1901.00540.

[21] Shmuel Onn. 1993. Geometry, complexity, and combinatorics of permutation

polytopes. Journal of Combinatorial Theory, Series A 64, 1 (Sept. 1993), 31–49.

[22] Harrie Oosterhuis. 2021. Computationally Efficient Optimization of Plackett-
Luce Ranking Models for Relevance and Fairness. In Proceedings of the 44th
International ACM SIGIR Conference on Research and Development in Information
Retrieval (SIGIR ’21). 1023–1032.

[23] Evaggelia Pitoura, Kostas Stefanidis, and Georgia Koutrika. 2021. Fairness in
rankings and recommendations: an overview. The VLDB Journal (Oct. 2021).
[24] R. L. Plackett. 1975. The Analysis of Permutations. Journal of the Royal Statistical

Society. Series C (Applied Statistics) 24, 2 (1975), 193–202.

[25] Tao Qin and Tie-Yan Liu. 2013.

Introducing LETOR 4.0 Datasets. CoRR

abs/1306.2597 (2013). http://arxiv.org/abs/1306.2597

[26] R. Rado. 1952. An Inequality. Journal of the London Mathematical Society s1-27,

1 (1952), 1–6.

[27] Stephen Robertson. 1977. The Probability Ranking Principle in IR. Journal of

Documentation 33 (Dec. 1977), 294–304.

[28] Babak Salimi, Luke Rodriguez, Bill Howe, and Dan Suciu. 2019. Interventional
Fairness: Causal Database Repair for Algorithmic Fairness. In Proceedings of the
2019 International Conference on Management of Data (Amsterdam, Netherlands)
(SIGMOD ’19). 793–810.

[29] Shinya Sano, Naoto Miyoshi, and Ryohei Kataoka. 2004. m-Balanced words: A
generalization of balanced words. Theoretical Computer Science 314, 1-2 (Feb.
2004), 97–120.

[30] Ashudeep Singh and Thorsten Joachims. 2018. Fairness of Exposure in Rankings.
In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining (London, United Kingdom) (KDD ’18). 2219–2228.
[31] Ashudeep Singh and Thorsten Joachims. 2019. Policy Learning for Fairness
in Ranking. In Advances in Neural Information Processing Systems, H. Wallach,
H. Larochelle, A. Beygelzimer, F. d’Alché Buc, E. Fox, and R. Garnett (Eds.), Vol. 32.
Curran Associates, Inc.

[32] Ashudeep Singh, David Kempe, and Thorsten Joachims. 2021. Fairness in Ranking
under Uncertainty. arXiv:2107.06720 [cs] (July 2021). arXiv: 2107.06720.
[33] Yi Su, Magd Bayoumi, and Thorsten Joachims. 2021. Optimizing Rankings for
Recommendation in Matching Markets. arXiv:2106.01941 [cs] (June 2021). arXiv:
2106.01941.

[34] Thibaut Thonet and Jean-Michel Renders. 2020. Multi-grouping Robust Fair Rank-
ing. In Proceedings of the 43rd International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR ’20). 2077–2080.

[35] Brandon Trabucco. 2021.

brandontrabucco/bvn.

https://github.com/

brandontrabucco/bvn original-date: 2020-09-15T00:11:02Z.

[36] Laurent Vuillon. 2003. Balanced words. Bulletin of the Belgian Mathematical

Society - Simon Stevin 10, 5 (Dec. 2003), 787–805.

[37] C. A. Waldspurger and E. Weihl. W. 1995. Stride Scheduling: Deterministic
Proportional- Share Resource Management. Technical Report. Massachusetts
Institute of Technology, USA.

[38] Lequn Wang, Yiwei Bai, Wen Sun, and Thorsten Joachims. 2021. Fairness of
arXiv:

Exposure in Stochastic Bandits. arXiv:2103.02735 [cs] (March 2021).
2103.02735.

[39] Lequn Wang and Thorsten Joachims. 2021. User Fairness, Item Fairness, and
Diversity for Rankings in Two-Sided Markets. In Proceedings of the 2021 ACM
SIGIR International Conference on Theory of Information Retrieval (ICTIR ’21).
23–41.

[40] Yao Wu, Jian Cao, Guandong Xu, and Yudong Tan. 2021. TFROM: A Two-sided
Fairness-Aware Recommendation Model for Both Customers and Providers. In
Proceedings of the 44th International ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR ’21). 1013–1022.

[41] Shota Yasutake, Kohei Hatano, Shuji Kijima, Eiji Takimoto, and Masayuki Takeda.
2011. Online Linear Optimization over Permutations. In Algorithms and Compu-
tation (Lecture Notes in Computer Science), Takao Asano, Shin-ichi Nakano, Yoshio
Okamoto, and Osamu Watanabe (Eds.). Springer, Berlin, Heidelberg, 534–543.

[42] Meike Zehlike, Francesco Bonchi, Carlos Castillo, Sara Hajian, Mohamed Mega-
hed, and Ricardo Baeza-Yates. 2017. FA*IR: A Fair Top-k Ranking Algorithm.
In Proceedings of the 2017 ACM on Conference on Information and Knowledge
Management (Singapore, Singapore) (CIKM ’17). 1569–1578.

[43] Meike Zehlike and Carlos Castillo. 2020. Reducing Disparate Exposure in Ranking:
A Learning To Rank Approach. In Proceedings of The Web Conference 2020 (WWW
’20). 2849–2855.

9

WSDM ’22, February 21–25, 2022, Tempe, AZ, USA

Kletti et al.

maximum utility. This issue can be avoided by putting a tolerance
in line 7 of Algorithm 2. Typically for our experiments, setting a
tolerance of 10−6 was sufficient.

B Balanced words
We lay out here Algorithm 3 to efficiently generate 𝑚-balanced
words [29]. In practice we let let the algorithm warm up for a
few hundred iterations, when 𝝓 is initialized to 0. Otherwise the
algorithm starts by delivering each ranking once, regardless of its
density.

Algorithm 3 A generator of 𝑚-balanced words
1: procedure BW(Input: a density 𝛼1, . . . , 𝛼𝑁 ∈ R+, (cid:205)𝑁
and an alphabet A with elements (𝑎𝑖 )𝑖 ∈ {1,...,𝑁 })

𝑖=1 𝛼𝑖 = 1

⊲ Generate the next element of the

𝝓 ← 0 ∈ R𝑁
𝑡 ← 0
while True do

sequence

2:

3:

4:

5:

6:

7:

8:

𝑖∗ ← arg min𝑖 ∈ {1,...,𝑁 } 𝝓𝑖
x𝑡 ← 𝑎𝑖∗
𝝓𝑖∗ ← 𝝓𝑖∗ + 1
𝛼𝑖∗
𝑡 ← 𝑡 + 1

end while

9:
10: end procedure Output: A sequence x ∈ AN. x is an 𝑁 − 1-

balanced word.

APPENDICES
A Implementation
Our algorithms 1 and 2 are exact algorithms on paper, but in practice,
when implemented in a machine that cannot handle real numbers,
some numerical difficulties are encountered. In this section we
explain some tricks we used to tackle these difficulties and their
limitations.

Consider Algorithm 1. When doing a bisection search with a
given precision on a half-line, in order to find its intersection with
the border of the expohedron, the point p that is found only approx-
imately lies on the intersected face. Therefore the next half-line
starting at a vertex v and passing through p is not exactly contained
in the subspace containing the face of v and p. The further away
a point on the half-line is from v, the bigger this error will be. In
the worst case the error will be big enough so that the next inter-
section with the border of the expohedron is not on a face of lower
dimension, thereby preventing convergence of the algorithm. Such
a problem can be avoided by projecting the approximate point p on
the hyperplane containing the face on which p lies. This substan-
tially reduces the error of p and makes it possible to avoid many
convergence problems.

Our implementation still encounters convergence problems when
the number of documents is high. In that case the expohedron is a
very high-dimensional polytope and some faces are smaller than
the implementation’s precision, which makes face-identification
error-prone by the end of Algorithm 2, when it is already close to

10

