1
2
0
2

r
a

M
4
1

]

G
L
.
s
c
[

1
v
7
5
0
8
0
.
3
0
1
2
:
v
i
X
r
a

RecSim NG: Toward Principled Uncertainty Modeling for Recommender
Ecosystems∗

M. MLADENOV†, Google Research
C. HSU†, Google Research
V. JAIN, Google Research
E. IE, Google Research
C. COLBY, Google Research
N. MAYORAZ, Google Research
H. PHAM, Google Research
D. TRAN, Google Research
I. VENDROV, Google Research
C. BOUTILIER, Google Research

The development of recommender systems that optimize multi-turn interaction with users, and model the interactions of different

agents (e.g., users, content providers, vendors) in the recommender ecosystem have drawn increasing attention in recent years.

Developing and training models and algorithms for such recommenders can be especially difficult using static datasets, which often

fail to offer the types of counterfactual predictions needed to evaluate policies over extended horizons. To address this, we develop

RecSim NG, a probabilistic platform for the simulation of multi-agent recommender systems. RecSim NG is a scalable, modular,

differentiable simulator implemented in Edward2 and TensorFlow. It offers: a powerful, general probabilistic programming language for

agent-behavior specification; tools for probabilistic inference and latent-variable model learning, backed by automatic differentiation

and tracing; and a TensorFlow-based runtime for running simulations on accelerated hardware. We describe RecSim NG and illustrate

how it can be used to create transparent, configurable, end-to-end models of a recommender ecosystem, complemented by a small

set of simple use cases that demonstrate how RecSim NG can help both researchers and practitioners easily develop and train novel

algorithms for recommender systems.

1 INTRODUCTION

Recent years have seen increased emphasis, both in research and in practice, on recommender systems that are capable

of sophisticated interaction with users. State-of-the-art recommender systems have moved beyond traditional models

that simply present items and passively observe immediate user reactions (e.g., clicks, consumption, ratings, purchase,

etc.), and have evolved to include systems capable of exploring user interests [16, 32], optimizing user engagement

∗A shorter abstract of this work appeared as “Demonstrating Principled Uncertainty Modeling for Recommender Ecosystems with RecSim NG,”
Demonstration paper at RecSys ’20: Fourteenth ACM Conference on Recommender Systems (2020).
†Contact author.

Authors’ addresses: M. Mladenov, Google Research, 1600 Amphitheatre Parkway, Mountain View, CA, 94043, mmladenov@google.com; C. Hsu, Google
Research, 1600 Amphitheatre Parkway, Mountain View, CA, 94043, cwhsu@google.com; V. Jain, Google Research, 1600 Amphitheatre Parkway, Mountain
View, CA, 94043, vihanjain@google.com; E. Ie, Google Research, 1600 Amphitheatre Parkway, Mountain View, CA, 94043, eugeneie@google.com; C.
Colby, Google Research, 1600 Amphitheatre Parkway, Mountain View, CA, 94043, ccolby@google.com; N. Mayoraz, Google Research, 1600 Amphitheatre
Parkway, Mountain View, CA, 94043, nmayoraz@google.com; H. Pham, Google Research, 1600 Amphitheatre Parkway, Mountain View, CA, 94043,
huberpham@google.com; D. Tran, Google Research, 1600 Amphitheatre Parkway, Mountain View, CA, 94043, trandustin@google.com; I. Vendrov, Google
Research, 1600 Amphitheatre Parkway, Mountain View, CA, 94043, ivendrov@google.com; C. Boutilier, Google Research, 1600 Amphitheatre Parkway,
Mountain View, CA, 94043, cboutilier@google.com.

1

 
 
 
 
 
 
2

M. Mladenov, C. Hsu, V. Jain, E. Ie, C. Colby, N. Mayoraz, H. Pham, D. Tran, I. Vendrov, and C. Boutilier

over multi-step horizons [14, 22, 27, 43, 63], or engaging in natural language dialogue [17, 46]. Loosely speaking, such

recommenders can often be viewed as engaging in multi-turn (or non-myopic), cooperative exploration with the user to

better serve the user’s needs. We use the term collaborative interactive recommenders (CIRs) as a catch-all term for such

systems.

The development of cutting-edge CIRs faces a number of challenges. In contrast to traditional myopic approaches to

recommender systems, CIRs cannot generally be trained using static data sets comprised of recommender actions and

observed user (immediate) responses, even when organized into trajectories—predicting the impact of counterfactual

actions on user behavior is crucial. Moreover, once we model non-myopic user behavior, the interaction between users

(e.g., content consumers, potential customers) and providers (e.g., content creators or providers, vendors or retailers)

takes on added importance in predicting recommender performance. Indeed, almost every practical recommender

system lies at the heart of a complex, multi-agent ecosystem. This amplifies the need for recommender methods to

capture the long-term behavior of participants, as well as the (potentially strategic) interactions that emerge between

them. Finally, because CIRs often adopt novel user interfaces or interaction modalities, there is often quite limited

training data from which to learn how users respond to recommender actions within these new modalities.

Simulation models provide an effective way to tackle the challenges above. Models of user behavior—how users

respond to novel recommender actions or interaction modes—and how it evolves over time can be used to assess the

performance of a proposed CIR over long horizons. Moreover, when coupled with models of the behavior of other

entities that engage with the recommender platform (e.g., content providers), simulation provides an effective way of

evaluating the dynamics of the recommender ecosystem as a whole.

The use of simulation comes with its own challenges, of course, as we discuss below. To facilitate the development

and study of recommendation models and algorithms in the complex environments characteristic of CIRs, we developed

RecSim NG, a configurable platform for both authoring and learning recommender system simulation environments.

Effective simulation can be used to evaluate existing recommender policies, or generate data to train new policies (in

either a tightly coupled online fashion, or in batch mode). Just as progress in reinforcement learning (RL) research

has been greatly accelerated by simulation [2, 12, 13], we believe that RecSim NG can support the development of

state-of-the-art CIRs and ecosystem-aware recommenders. Broadly, RecSim NG provides a probabilistic programming

framework that supports: (a) the natural, modular, “causal” specification of user and other agent behavior, and its

dynamics, within CIR ecosystems; (b) the ability to learn the parameters of such models from data; and (c) a number

of probabilistic inference techniques, beyond straightforward Monte Carlo simulation, for evaluating recommender

algorithms.

More concretely, the main goals of RecSim NG and our contributions embody the following principles:

• Models of agent behavior (e.g., users, content providers) across a sequence of interactions with a CIR
must reflect that agent’s state and how it evolves while engaging with the CIR. RecSim NG incorporates

the probabilistic programming environment Edward2 [51] to specify common design patterns for user state and

behaviors (e.g., user preferences, satisfaction, choice and consumption behavior). RecSim NG also allows the

natural expression of causal generative models of user (and other participant) behavior and utility, including user

state, choice, response and engagement models, and user-state transition dynamics. These models are specified

as a composable set of dynamic Bayesian networks (DBNs) [19, 31] which we organize in an object-oriented

fashion [39] using three main concepts: entities (e.g., users, recommenders), behaviors (e.g., state transitions, user

choices) and stories (e.g., user-system interaction details).

RecSim NG

3

• Models of user state must support latent or unobservable state variables. CIRs must often engage in
(implicit or explicit) latent state estimation from various observable behaviors to build internal models of user

preferences, user psychological state (e.g., satisfaction, frustration), and other exogenous environmental factors

to compute effective (long-term) recommendation strategies. RecSim NG allows the flexible specification of

both observable and latent aspects of a user’s or agent’s state so that practitioners can experiment with the

state-estimation capabilities of different model architectures.

• Behavior models require flexibility in their model structure and probabilistic inference methods. Rec-
Sim NG allows one to impose as much (or as little) structure on behavior models as needed. For example, in

psychometric models of user choice, it is quite natural to specify structural priors, biases and parametrizations,

while the precise parameters must usually be estimated from behavioral data. RecSim NG supports the flexibility

of imposing model structure and learning of model parameters from data, or learning the structure itself. Because

many realistic models have latent factors, model learning requires sophisticated probabilistic inference, beyond

the usual Monte Carlo rollouts embodied by most simulation environments. RecSim NG supports latent variable

inference, including several Markov-chain Monte Carlo (MCMC) and variational methods.

• Ecosystem modeling is critical in complex CIRs, including the modeling of: incentives of individual
agents that drive their behaviors; variable observability criteria for pairs/groups of agents; and the

interaction between agents as mediated by the CIR. The use of entities, stories and behaviors by RecSim

NG allows the natural specification of the interaction between agents in the system, while making it easy to

uncover any independence that exists. Moreover, scalability is critical, especially in CIRs with large populations.

RecSim NG provides scalable TensorFlow-based execution to support several forms of posterior inference beyond

standard MC rollouts.

• Modular modeling of agent behavior provides valuable flexibility in generating data to test the ca-
pabilities of novel CIR algorithms, models and modes of interaction. The agent-behavior abstractions

offered by RecSim NG—based on probabilistic graphical models and adopting an object-orientation—encourages

the decomposition of agent behaviors into natural, reusable components that can be used in a variety of different

environments. This ensures relatively stable behaviors that facilitate the comparison of different recommendation

strategies and interaction modes. For example, one can readily compare the performance of two CIRs in the same

domain that use slightly different interfaces (e.g., one GUI-based, the other voice-based) on the same “population”

of simulated users by using the same user latent-state, state-transition and choice models, while changing only

the user response model to accommodate the different interfaces.

In this work, we primarily view simulation as a tool to explore, evaluate, and compare different recommender models,

algorithms, and strategies. While the so-called “sim2real” perspective is valuable, we focus on simulations that reflect

particular phenomena of interest—specific aspects of user behavior or ecosystem interaction arising in live systems—to

allow the controlled evaluation of recommender methods at suitable levels of abstraction. We illustrate this below with

several specific use cases. RecSim NG should be equally valuable to researchers (e.g., as an aid to reproducibility and

model sharing) and practitioners (e.g., to support rapid model refinement and evaluation prior to training in a live

system). That said, RecSim NG may also serve as a valuable tool to generate training data for live recommender systems.

The remainder of the paper is organized as follows. We discuss related work in Sec. 2. We describe RecSim NG in

Sec. 3 in stages. We first explicate the modeling language in general terms (Sec. 3.1), then detail its use of entities,

behaviors and stories more concretely using a simple, stylized example (Sec. 3.2). We next illustrate the use of some

4

M. Mladenov, C. Hsu, V. Jain, E. Ie, C. Colby, N. Mayoraz, H. Pham, D. Tran, I. Vendrov, and C. Boutilier

reusable behavioral building blocks from the RecSim NG library in a slightly more elaborate example (Sec. 3.3). We also

describe RecSim NG’s scalable execution in TensorFlow (Sec. 3.4) and probabilistic inference and learning capabilities

(Sec. 3.5). In Sec. 4, we present three use cases that demonstrate RecSim NG’s capabilities and provide simple but

suggestive empirical results.

The RecSim NG library is available for download,1 and more detailed exposition of it capabilities and its usage can
be found in the tutorials offered there. Some of the details of the examples and use cases presented in the sequel are left

unstated, but full details (including models, algorithms and their specific parametrizations) can be found in the library.

2 RELATED WORK

RecSim NG—both the underlying framework and motivating use cases illustrating its capabilities—draw inspiration

from multiple research areas. These include work on classical recommender systems, reinforcement learning, sequential

models in recommender systems, conversational recommenders, multi-agent modeling, and probabilistic programming,

to name a few. A detailed review of each of these areas is beyond the scope of this expository overview, hence we briefly

discuss just a few key connections and influences.

Perhaps the dominant “classical” approach to recommender systems is collaborative filtering (CF), which includes

early matrix factorization models [11, 30, 42] and more recent neural CF approaches [18, 26, 58] to predict users’ item

preferences. Increasingly, recommenders predict more detailed user responses to recommendations, such as click-

through-rates (CTR) or engagement (e.g., dwell/watch/listen time, purchase behavior, etc.) [18, 54]. We use CF-based

embeddings and user response models in several of the RecSim NG use cases discussed in the sequel..

Modeling sequential user behavior in using RL to optimize recommender strategies has long been considered

important [43, 47]. With the advent of deep RL, such models have attracted considerable attention over the past several

years [14, 15, 22, 27, 63]. Indeed, sequential behavior modeling and RL-based optimization is a key motivation for

RecSim NG. The difficulty of evaluating RL methods using static datasets [25], due to the impact of counterfactual
actions on user trajectories, is a primary reason to use simulation [22, 29, 40].2 We illustrate a simple RL use case
below (drawing also on elements from bandit models in recommenders, e.g., [16, 32]). We also use RecSim NG to model

(and learn) aspects of latent user state. Explicit latent variable models have been explored on occasion in RSs (e.g.,

[9, 23, 34, 41]). However, the use of recurrent models has been method of choice is recent models of sequential user

behavior [48, 57].

Ecosystem modeling in recommenders is somewhat uncommon, though it has some connections to the study of

fairness in ML. Studies of group fairness in ranking, for example, often by “regularize” standard ranking metrics like

NDCG to encourage parity of the ranks across demographic groups [59, 61]. Of special relevance is recent work that

attempts to fairly assign users/exposure to content providers [5, 45]. Also related is a recent game-theoretic model of

recommender systems whose providers act strategically—by making available or withholding content—to maximize

their own user engagement on the platform [3, 4]. Finally, recent work has explored the impact of a recommender’s

policies on content provider behavior, how this impacts long-term user utility, and how to optimize policies under such

conditions to maximize social welfare over extended horizons [35]. We adopt a simplified variant of this latter model in

one of our illustrative use cases in Sec. 4.

As a simulation platform, RecSim NG shares much with recent systems used in RL and recommender systems

research. Simulation has played an critical role in the evaluation of RL methods in recent years. Many of these platforms,

1See https://github.com/google-research/recsim_ng.
2Indeed, simulation is a critical component or RL research as we elaborate below.

RecSim NG

5

including ALE [2], Dopamine [13], and OpenAI Gym [12] offer of a collection of environments against which RL

algorithms can be benchmarked and compared. These platforms often provide a set of well-known RL algorithms for

comparison. Our work shares some of the same motivations, but differs in that we focus on allowing the direct authoring

and learning of environments to push development of algorithms that handle new domain characteristics rather than

on benchmarking per se. ELF [49] is similar to RecSim NG in this respect, as it allows configuration of real-time strategy

games to support the development of new RL methods, overcoming some of the challenges of doing research with

commercial games (e.g., by allowing access to internal game state).

Simulation has recently found direct use in recommenders as well. Three platforms are especially related to RecSim

NG. Yao et al. [60] use extensive simulation studies to examine the impact of recommender policies on long-term user

behavior, and specifically to disentangle the influence of the recommender itself from that of a user’s own preferences

on this behavior. While not a configurable platform like RecSim NG, the models developed in that work share much

with some of the model structures RecSim NG is intended to support. Rohde et al. [40] propose RecoGym, a configurable

recommender systems simulation environment. It allows the study of sequential user interaction that combines organic

navigation with intermittent recommendation (or ads). RecoGym, however, does not allow configuration of user

state transitions, but instead allows bandit-style feedback. Finnaly, we note that RecSim NG is a major extension and

reconception of the earlier RecSim platform developed by Ie et al. [29]. Like RecoGym, RecSim is configurable and is

well-suited to the study of sequential user interaction. It also allows the relatively straightforward configuration of

user state dynamics. RecSim NG shares much with these platforms, but offers the ease, generality and flexibility of

probabilistic programming, general probabilistic inference (not just trajectory generation), and scalable execution in

TensorFlow.

Rule-based and learning-focused environments have been used for goal-oriented dialog agents [38, 55] and interactive

search [33]. Generative adversarial networks have been used to generate virtual users for high-fidelity recommender

environments to support learning policies that can be transferred to real systems [44, 62]. Multi-agent simulation has a

long history, and has recently found use for modeling agent interactions in economic [64], social media [1] and social

network settings [8, 53]. None of these models support the authoring of general interaction models.

As a probabilistic programming framework, RecSim NG builds on the area of deep probabilistic programming,

enabling probabilistic models with deep neural networks, hardware accelerators, and composable inference methods

[6, 50, 52]. RecSim NG proposes new abstractions for user modeling, multi-agent modeling and ecosystem interaction

that are well-suited to CIRs. There has been prior work on multi-agent probabilistic systems [21, 24], typically rooted

in cognitive science. These investigations usually involve only a handful of agents and small (if any) datasets. They also

do not examine user-item ecosystems or item providers nor do they offer support for flexible probabilistic inference

methods, neural network models, etc.

3 RECSIM NG MODELING ENVIRONMENT

In this section, we outline the RecSim NG modeling formalism. We start with a low-level formal description of the

RecSim NG model semantics. We then present the high-level abstractions of behaviors and entities, give an overview of

hierarchical agent modeling and the RecSim NG behavior modeling library, and conclude with a brief introduction to

probabilistic reasoning in RecSim NG.

6

M. Mladenov, C. Hsu, V. Jain, E. Ie, C. Colby, N. Mayoraz, H. Pham, D. Tran, I. Vendrov, and C. Boutilier

3.1 Modeling Formalism

Abstractly, a RecSim NG model (also called a RecSim NG "simulation") represents a Markovian stochastic process. We
assume some state space S over which a Markov process gives rise to trajectories, that is, sequences of states from S of
length 𝑛, (𝑠𝑖 ∈ S)𝑛

𝑖=1, such that

𝑝 (𝑠0, 𝑠1, . . . , 𝑠𝑛−1) = 𝑝0 (𝑠0)

𝑇 (𝑠𝑖, 𝑠𝑖+1),

(cid:214)

𝑖

where 𝑝0 is an initial state density, and 𝑇 is a transition kernel. We can think of S, for example, as the state of the
recommender ecosystem, which itself might be the collection of states of all the individual agents in it.

In most cases, 𝑠 ∈ S is not just some vector in Euclidean space. Instead, S is a highly structured object consisting of
sub-components (e.g., the states of the individual agents) which may evolve autonomously, or through interactions
with each other. In RecSim NG we assume that the state space factors as the Cartesian product S = ×𝑖𝑆𝑖 , where each
𝑆𝑖 is a component of the state space. For example, each component might reflect the state of some agent, or some
random variable in the recommender environment (say, the weather or traffic conditions, or an emerging news trend).

A factorization of the state space allows us to specify the initial state and transition kernel in a factored way as well. To

do so, we use the language of Dynamic Bayesian Networks (DBNs) [19].

Suppose that Γ, Γ−1 are two (possibly different) directed acyclic graphs (DAGs) whose nodes are the components of

the state space. We then assume that

𝑇 (𝑠𝑡 , 𝑠𝑡 −1) =

(cid:214)

𝑇 𝑖 (𝑠𝑖

𝑡 |𝑠PaΓ (𝑖)
𝑡

𝑖

, 𝑠PaΓ−1 (𝑖)
𝑡 −1

),

that is, the state of the component 𝑖 depends on the state of its parents in Γ (its intra-slice dependencies), as well as the
preceding state of its parents in Γ−1 (its inter-slice dependencies). The components 𝑇 𝑖 of the transition kernel are termed
conditional probability densities (CPDs) or sometimes factors. We assume that the initial state density can be factored

similarly, but the graphs may be different (we will not spell this out explicitly).

It is important to note that the notion of Markovianness employed above is defined exclusively from the point of

view of the simulation runtime, not from the perspective of any individual agent or action (e.g., user, content provider,

recommender platform) in the simulation. This implies that the simulator must be able to observe all random variables—

for example, the simulator must have access to the state of all agents. This does not impact the ability of RecSim NG to

simulate partially observable problems (e.g., partially observable MDPs where users have latent state that is not directly

observable by the recommender). In such scenarios, only part of the simulation state is communicated to a particular

decision-making agent, meaning that its own state CPD can only depend on a specific subset of the simulation variables.

A RecSim NG model is thus a specification of some number of component random variables, their initial distributions,

their transition kernels, and the structure of the inter- and intra-slice dependencies. We illustrate this with a simple
example of how these are implemented in RecSim NG using a deterministic simulation that counts to 𝑛.

1 # Declare a component variable , its name and state space .
2 count_var = Variable ( name =" count " , spec = ValueSpec (n= FieldSpec () ))
3 # Define the variable 's initial state , in this case deterministic .
4 def count_init () -> Value :
return Value (n =0)

5
6 # Define the variable 's kernel .
7 def count_next ( previous_value : Value ) -> Value :

8

9

n_prev = previous_value . get ("n")

return Value (n= n_prev + 1)

RecSim NG

7

10 # Finally bind the initial state distribution and kernel to the variable .
11 count_var . initial_value = variable . value ( count_init )
12 # We declare an inter - slice dependency on the variable 's previous value .
13 count_var . value = variable . value ( count_next , ( count_var . previous ,) )

0

(cid:17)

(cid:16)
𝑠0
0

(cid:12)
𝑠PaΓ0 (0)
(cid:12)
(cid:12)
0

(note that 𝑠PaΓ0 (0)

The snippet above declares a single state component 𝑠0 with name "count" represented by the Python variable
count_var. Its initial distribution 𝑝0 is established by binding count_var.initial_value to a function that samples from
𝑝0
happens to be the empty set in this case), and the transition kernel 𝑇 is established
by binding count_var.value to a function sampling from 𝑇 𝑖 (𝑠0
= {0}.
The set of parents of a component random variable is declared by the list of components in the second argument of
variable.value, where the .previous property indicates a Γ−1 dependency (for Γ dependencies, the variables are used
as is). All data-generating functions that interact with the RecSim NG runtime return a Value object, which is essentially
a structured dictionary.

= ∅ and 𝑠PaΓ−1 (0)
𝑡

), where 𝑠PaΓ (0)

, 𝑠PaΓ−1 (0)
𝑡 −1

𝑡 |𝑠PaΓ (0)
𝑡

𝑡

Once fully defined, count_var can be simulated by the RecSim NG runtime to generate trajectories or simply return
the variable’s state after 𝑛 steps of simulation. We discuss dependencies in somewhat more detail in Sec. 3.2, but refer to
the online tutorials for a more complete treatment.3

3.2 Scalable Behavioral Modeling in RecSim NG

Component random variables together with their distributions and dependencies are the “assembly language“ of RecSim
NG. The Variable API is intentionally kept simple in order to avoid unnecessarily restricting the range of applications
that can be implemented. There are, however, several reasons not to rely on variables alone for complex recommender

system simulations. First, even a simple recommender ecosystem model may involve millions of random variables

per time-step, such as user choices, evolving preferences, stochastic recommendation policy decisions, and so on.

Modeling these as individual random variables within a “flat” DBN would be extremely tedious. Second, expressing

agent dynamics exclusively through kernels is somewhat contrary to how we tend to think of agents. Intuitively, an

agent is an actor with specific properties (goals, preferences, etc.) that engages in behaviors to achieve some desired

outcome. For example, a content provider may produce content that aims to maximize user engagement subject to

budget/time constraints; an advertiser may bid on advertising opportunities to maximize conversion rates; or a user

may issues a sequence of queries and consume particular recommended content that helps meet some informational or

entertainment goal.

In an agent-based simulation, there are two common sources of structure that can be harnessed to make complex

models easier to implement:

• Parameter sharing/encapsulation: An agent may have multiple behaviors such as “choose,“ “observe,“ “update state,“
etc. All of these behaviors might be influenced by a common set of parameters, such as the agent’s preferences,

its risk sensitivity, its overall satisfaction, etc. This fits nicely within the object-oriented paradigm in which the

various behaviors are expressed as methods and the shared parameters are encapsulated properties of the object.

(In fact, agent-based modeling is often used to justify object-oriented programming.)

• Behavioral generalization: While every agent in a population is unique to some degree, not everything they do
is idiosyncratic. In fact, most behavioral models tend to be expressed in terms of a population “template” that

abstracts agent individuality by reducing it to some small set of agent-specific, “personalized” parameter values.

3See https://github.com/google-research/recsim_ng.

8

M. Mladenov, C. Hsu, V. Jain, E. Ie, C. Colby, N. Mayoraz, H. Pham, D. Tran, I. Vendrov, and C. Boutilier

For example, a Luce-Shephard or multinomial logit (MNL) model for user choice (i.e., a model which specifies the

probability with which a user selects a specific item from a slate of recommended items) posits that an agent
𝑎 will choose an item 𝑖 with probability proportional to 𝑒 𝑓aff (a,i) , where a is the agent’s feature vector, i is a
feature vector characterizing the item, and 𝑓aff is an affinity function. While the agent features are personalized
to each agent, the affinity model and the choice distribution family (softmax/MNL in this case) are common to

the entire population. Templatized models of this form offer two critical benefits: (a) they allow us to define large

populations of agents very concisely; and (b) they can make heavy use of the accelerated computation available

on modern hardware (through, for example, highly parallel hardware architectures).

Entities and Behaviors. In RecSim NG, the two types of structure above are harnessed through the “Entity” pattern.

A RecSim NG Entity is a class that models the parameters and behaviors of an entire population of agents, making use

of batched execution as much as possible.

The following example shows a partially implemented Entity prototype for a user model. This user model reflects a
configurable user population of flexible size (as reflected by the num_users parameters). The state of each user in this
population is a 𝑑-dimensional interest vector intended to represent a user’s affinity for specific content items. Each
user has three Behaviors, which both influence and are influenced by the user’s state. The initial_state behavior
stochastically generates user initial state values. The response behavior is intended to model how the user responds to
a slate of recommendations given its state (e.g., the probability of clicking, rating, commenting, etc.), while next_state
dictates how the user state evolves as a function of the previous state and the outcome of the current response.

In the following snippet, the initial_state state behavior is implemented to draw the user’s initial interest vector
from a 𝑑-dimensional unit normal distribution. Note that this behavior initializes the states of all users in the entire
population, as opposed to that of a single user. (We illustrate examples of the response and next_state behaviors in the
following subsection.)

class User ( entity . Entity ):

def __init__ ( self , num_users , parameters ) -> None :

super () . __init__ ( name =" MyUserModel ")

self . _parameters = parameters

self . _num_users = num_users

def initial_state ( self ) -> Value :

return Value ( interest = ed . Normal ( loc = tf . zeros ( self . _num_users ,

self . _parameters [" pref_dimension " ]) , scale = self . _parameters [" initial_pref_scale " ]) ))

def next_state ( self , old_state : Value , response : Value ) -> Value :

pass

def response ( self , state : Value , recommended_slate : Value ) -> Value :

pass

A RecSim NG entity owns several component random variables and declares their domains. Each behavior takes
the values of some random variables as input and then returns the values of other random variables. The Value is
a mapping of random variables. The domains of random variables are declared using OpenAI Gym spaces [12] in
specs(), specifying the variable’s shape (dimensions) and type (discrete, continuous). In the above example, item is
a 𝑑-dimensional random variable but the others are just 1-dimensional random variables. The resulting ValueSpec is

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

RecSim NG

9

a mapping of random variable domains. Structured data types such as tuples, dictionaries, and nested compositions

thereof are also supported.

Stories. A story is the top-level code responsible for creating fully-defined component random variables. The function

of a story is essentially to define a family of parametrized simulations by ingesting a set of user-defined parameters

(such as number of users, model parameters, entity constructors and so on), creating all entities, component random

variables, binding the component random variables to the entities’ behaviors, and outputting the fully defined random

variables, which can then be passed to the runtime for simulation. Consider the following example.

def simple_user_recommender_story ( num_users , user_model_ctor , recommender_agent_ctor ) :

# Create entities .

user_model = user_model_ctor ( num_users )

recommender_agent = recommender_agent_ctor ()

# Define random variables .

user_state = Variable ( name =" user state " , ...)

response = Variable ( name =" user response " , ...)

rec_slate = Variable ( name =" slates " , ...)

# Bind random variables to behaviors .

# CPDs at t =0.

user_state . initial_value = variable . value ( user_model . initial_state )

rec_slate . initial_value = variable . value ( recommender_agent . slate )

user_response . initial_value = variable . value ( user_model . response , ( user_state , rec_slate ) )

# Transition kernels .

rec_slate . value = variable . value ( recommender_agent . slate )

response . value = variable . value ( user_model . response , ( user_state . previous , rec_slate ))

user_state . value = variable . value (

user_model . next_state , ( user_state . previous , user_response ))

return [ rec_slate , user_response , user_state ]

The use of stories is not strictly required for creating RecSim NG simulations, however, it has two major benefits.

First, it condenses the information flow between behaviors into a single function, and second, it allows us to templetize

the creation of a parametrized suite of simulation scenarios. For example, we could create a suite of experiments testing

how a given recommender deals with populations of varying sizes by running the story against multiple values of the
num_users parameter. Alternatively, we could test different pairs of recommender agents and user models by swapping
out user_model_ctor and recommender_agent_ctor.

Finally, the RecSim NG learning APIs offer some additional convenience when using stories. For example,

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

1 component_rvs , trainable_variables = story_with_trainable_variables ( lambda :

simple_user_recommender_story (...) )

captures all trainable TensorFlow variables created during the execution of the story.

3.3 The RecSim NG Behavioral Modeling Library

While the entity/behavior/story pattern removes considerable complexity from the implementation of large-scale

stochastic simulations, writing population-level stochastic simulations in TensorFlow can still be challenging in certain

10

M. Mladenov, C. Hsu, V. Jain, E. Ie, C. Colby, N. Mayoraz, H. Pham, D. Tran, I. Vendrov, and C. Boutilier

situations. Inspired by the success of compositional deep learning frameworks such as Keras,4 RecSim NG provides
additional avenues for complexity reduction in the form of a curated library of behavioral building blocks, which provide

highly vectorized implementations of common elements in agent-based models. These are, in a sense, behavioral layers.

We provide a brief overview of the RecSim NG library and illustrate the compositional APIs through which these

building blocks are used.

Choice models: Agent choice is perhaps the most fundamental behavior in modeling recommender systems. Users

often choose an item for consumption from a slate of recommended items (or abstain from choice); content providers

might choose what type of content to offer at different points in time; and the policy of the recommender system

itself involves the choice of a slate of items to present to users at different times. The totality of these choices jointly

determines the satisfaction and behavior of all participants in the ecosystem.

The RecSim NG modeling library currently supports the following modeling paradigm: when an agent (e.g., a user)

is presented with a choice situation consisting of a slate of items, the agent’s affinity for each item is calculated using

an affinity model. An affinity model ingests a set of item features, as well as (optional) side information (e.g., user or

environment context, user state) and outputs a vector of scores, one for each item on the slate (and optionally a “no
choice” score). For instance, affinity for item 𝑖 might simply be the negative Euclidean distance between 𝑖 and the
agent’s target item in some embedding space (here the target item would be part of the agent’s state). The affinities are

then passed as parameters to some choice distribution, such as a greedy, MNL, cascade or Plackett-Luce model, which

samples zero or more chosen items.

State models: Understanding the impact of recommendations on user behavior and recommender engagement metrics,

especially over long horizons, requires understanding the user’s state. While observable features such as demographics

are certainly valuable, many interesting user properties are latent, i.e., not directly observable by the recommender

system. Many latent state features are reasonably static—for instance, general item preferences and interests, or stable

psychological or personality characteristics like curiosity, patience, etc. Others are transient and may exhibit interesting

dynamics—for example, the user’s current task, the current context (e.g., the user’s companions in the moment, the

activity the user is engaged in), or mental state (e.g., satisfaction, frustration).

RecSim NG offers three categories of state models which can be applied to specific state variables:

• Static: the state is sampled once at 𝑡 = 0 and then kept constant.
• Dynamic: the state evolves dynamically according to some (controllable) transition kernel.
• Estimators/belief states: these aim to summarize a sequence of observations, compiling them into sufficient
statistics that reflect an agent’s belief about some unobserved state variable. These are typically used to represent

the recommender’s beliefs about, say, user state, but could also be used to model a user’s state of knowledge

about available content.

Static state models include various ways of sampling 𝑁 -dimensional vectors from some structured distribution (e.g., a
Gaussian mixture model, or hierarchical sampling of points from a set of hard clusters). Dynamic state models include

finite-state controlled Markov chains, linear-Gaussian dynamical systems, as well as various recurrent neural network

4See https://keras.io.

RecSim NG

11

cells. Estimation models encompass methods like Kalman filters, direct posterior calculation for finite distributions, and

simple finite-observation history arrays.

Algorithmic primitives: RecSim NG offers implementations of a variety of common recommendation system algo-

rithms. This includes a fairly extensive collection of bandit algorithms, including algorithms for the standard multi-armed

bandit setting as well as various generalizations and extensions (e.g., linear and general contextual bandits). These

methods cover many well-established algorithmic paradigms (e.g., optimism/UCB, posterior sampling) as well as

black-box trainable RNN agents. These algorithms can be used in stand-alone fashion, or as part of more complex

policies. They can even be used to implement non-recommender agents; e.g. a user’s exploration behavior can be

modeled as a multi-armed bandit.

Prototypes: The RecSim NG library contains abstract classes for some commonly used types of agents (users, recom-

mender, content providers), to serve as guidance for implementing custom classes. Inheriting from these classes also

ensures that the custom implementations can be used within RecSim’s existing recommender stories.

Note that not all of the above model types will be available in the initial release version of RecSim NG. Moreover,

this is not an exhaustive list—the library will grow as new applications are introduced.

Illustration: We conclude this section with an example of how these building blocks are used in the construction of

Entities. We revisit the user-model Entity example from Sec. 3.2; but instead of manually implementing all behaviors,

we use RecSim NG building blocks. Specifically, we implement the following model:

• The user’s state consists of a 𝑑-dimensional interest vector 𝑆. Its initial value is distributed according to a

𝑑-dimensional unit normal distribution.

• At each time step, the user is presented with a slate of items (1, . . . , 𝑘). Each item 𝑖 is represented by a 𝑑-
dimensional feature vector 𝐹𝑖 , and a quality scalar 𝑞𝑖 ∈ [−1, . . . , 1]. The user must chose one of the 𝑘 items for
consumption. The probability of choosing any particular item is proportional to exp −||𝐹𝑖 − 𝑆 ||. We note that
quality 𝑞𝑖 plays no direct role in the user’s choice—we assume that 𝑞𝑖 is only observed by the user if/once the
item is chosen and consumed.

• Having consumed an item, the user’s interest evolves as 𝑆𝑡 = 𝑆𝑡 −1 + 𝜆𝑞𝑖 (𝐹𝑖 − 𝑆𝑡 −1) + 𝜉, where 𝜆 is a sensitivity
parameter and 𝜉 is Gaussian noise. Notice that if the consumed item has positive quality, the user’s interest
vector moves toward the item vector, and it moves away if the item has negative quality.

We map these elements to the RecSim NG model library as follows. For interest dynamics, we use a

ControlledLinearGaussianStateModel, with an identity transition matrix and 𝜆𝐼 as a control matrix. This model takes
𝑞𝑖 (𝐹𝑖 −𝑆𝑡 −1) as its control input. To implement user choice, we use a TargetPointSimilarityModel to compute the negative
Euclidean distance of the user’s interest to all items in the slate, then pass these affinities to a MultinomialLogitChoiceModel,
which samples the chosen item. An abridged implementation is given below. Note that all of these building blocks

support batch execution, so this code simultaneously simulates the entire population of users.

1

2

3

4

5

6

7

class User ( entity . Entity ):

def __init__ ( self , num_users , parameters ) -> None :

super () . __init__ ( name =" MyUserModel ")

self . _parameters = parameters

self . _num_users = num_users

self . _affinity_model = choice_lib . affinities . TargetPointSimilatrity (

... , similarity_type = ' negative_euclidean ')

12

M. Mladenov, C. Hsu, V. Jain, E. Ie, C. Colby, N. Mayoraz, H. Pham, D. Tran, I. Vendrov, and C. Boutilier

self . _selector = choice_lib . selectors . MultinomialLogitChoiceModel (...)

self . _interest_model = state_lib . dynamic . ControlledLinearScaledGaussianStateModel (...)

def initial_state ( self ) -> Value :

initial_interest = self . _interest_model . initial_state ()

return initial_interest . prefixed_with (" interest "). union ( Value ( satisfaction = ed . Deterministic ( loc =

tf . ones ( self . _num_users ))))

def next_state ( self , old_state : Value , response : Value ) -> Value :

chosen_docs = response . get ( ' choice ')

chosen_doc_features = selector_lib . get_chosen ( slate_docs , chosen_docs )

# Calculate utilities .

user_interests = previous_state . get ( ' interest . state ')

doc_features = chosen_doc_features . get ( ' doc_features ')

# User interests are increased / decreased towards the consumed document 's

# topic proportional to the document quality .

direction = tf . expand_dims (

chosen_doc_features . get ( ' doc_quality ') , axis = -1) * (

doc_features - user_interests )

next_interest = self . _interest_model . next_state (

previous_state . get ( ' interest ') ,

Value ( input = direction ))

return next_interest . prefixed_with ( ' interest ')

def response ( self , state : Value , recommended_slate : Value ) -> Value :

affinities = self . _affinity_model . affinities (

state . get ( ' interest . state ') ,

recommended_slate . get ( ' doc_features ')). get ( ' affinities ')

choice = self . _selector . choice ( affinities )

return choice

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

In RecSim NG , model building follows a hierarchical pattern—building blocks are also entities owned by the “parent”

entity. The building block entities output the result of the behaviors they implement, which can then be added to
the state of the parent entity by means of the Value.union method. Additionally, the “Value” object supports (tree-
based) hierarchical indexing. For example, if user_model._satisfaction_model.next_state outputs Value(state=...)
and user_model._interest_model.next_state outputs Value(state=...), then we can compose those with prefixes, as
in:

1 interest_next_state = self . _interest_model . next_state ( interest_args )
2 satisfaction_next_state = self . _satisfaction_model . next_state ( satisfaction_args )
3 result = interest_next_state . prefixed_with ( ' interest '). union ( satisfaction_next_state . prefixed_with ( '

satisfaction '))

to create a Value object with keys ’interest.state’ and ’satisfaction.state’.

3.4 Runtimes and Model Execution

Once a set of component random variables has been fully defined, it is passed to a RecSim NG runtime, which generates

trajectories from the stochastic process. For example:

1 model_dbn = network_lib . Network ( list_of_variables )

RecSim NG

13

2 runtime = runtime_ctor ( network = model_dbn )
3 trajectory = runtime . trajectory ( trajectory_length )

RecSim NG features a modular runtime architecture, in which different types of runtimes can be implemented. A

runtime is essentially an implementation of a sampler for a DBN. RecSim NG offers a pure Python runtime, which

implements direct ancestral sampling in a Python for-loop. A more sophisticated runtime is the TensorFlow runtime,
which wraps the sampling process in a TensorFlow scan operator. The TensorFlow runtime allows the entire simulation
to be compiled into a single TensorFlow graph, which confers some significant benefits, among them:

• The simulation will take advantage of all optimizations offered by TensorFlow’s AutoGraph compiler, including

XLA (accelerated linear algebra) if available.

• The simulation will automatically utilize all available CPU or GPU cores on the host machine without any

additional effort required by the RecSim NG developer.

• The simulation graph can execute on specialized hardware such as Tensor Processing Units (TPUs).

RecSim NG runtimes are also able to handle distributed execution, as well as alternative sampling methods, such as

Markov Chain Monte-Carlo samplers.

Finally, we note that while much of the RecSim NG functionality is currently offered through the TensorFlow back

end, the core RecSim NG architecture is back-end independent, enabling applications to be developed within other

computational frameworks (such as JAX or PyTorch).

3.5 Probabilistic Programming

We now elaborate on the probabilistic programming ethos that underlies RecSim NG, which supports sophisticated

probabilistic inference, and is critical to model learning and recommender policy optimization. In Sec. 3.1, we defined a

RecSim NG program as a stochastic process described by DBN. We now discuss in greater details precisely how DBNs

are defined in RecSim NG. Recall that a DBN is a Markovian stochastic process whose kernel factorizes as

𝑇 (𝑠𝑡 , 𝑠𝑡 −1) =

(cid:214)

𝑇 𝑖 (𝑠𝑖

𝑡 |𝑠PaΓ (𝑖)
𝑡

𝑖

, 𝑠PaΓ−1 (𝑖)
𝑡 −1

),

where PaΓ (𝑖) and PaΓ−1 (𝑖) denote sets of random variables. respectively, whose current time step (present) and prior
time step (previous) values jointly determine the distribution from which 𝑠𝑖

𝑡 is generated.

The equation above embodies a declarative style for defining the core stochastic process; that is, to specify a DBN, we
specify the conditional distributions 𝑇𝑖 using mathematical functions in some symbolic language. This is clearly not what
we have done in the examples so far, which have used more algorithmic (or imperative) definitions by implementing

behaviors as sequences of TensorFlow operators which transform given inputs into the desired outputs.

Generally, imperative specifications tend to be much more intuitive than declarative ones—especially in the context

of agent-based modeling, where we can literally imagine an agent engaging in a sequence of steps to accomplish some

objective, task or effect. That said, they tend to be less useful than declarative approaches since they only describe how

values are produced. As a result, they do not allow us to transform the induced distribution or derive new mathematical

facts to aid in specific inference tasks (for example, computing its gradients).

There is, however, a fairly wide set of cases where imperative definitions can be converted to declarative ones

in reasonable time and space, allowing us to have the best of both worlds. To achieve this, RecSim NG uses the
probabilistic programming environment Edward2 [51].5 Edward 2 is used as a layer on top of TensorFlow Probability

5See https://github.com/google/edward2.

14

M. Mladenov, C. Hsu, V. Jain, E. Ie, C. Colby, N. Mayoraz, H. Pham, D. Tran, I. Vendrov, and C. Boutilier

(TFP) which employs program transformation to allow us to treat our imperative sampling code as if it were a symbolic
declarative definition of the distribution.6 RecSim NG exposes a small (but easily extensible) set of Edward 2 program
transformations tailored to simulation-specific tasks (including those of special relevance for simulating agents involved

in recommender systems).

One significant difference between RecSim NG and a pure Monte Carlo simulator is that it can evaluate the probabilities
of trajectories according to the DBN induced by the simulation. This functionality is contained in the log_probability
module. This module enables the calculation of the log probabilities of data trajectories relative to the stochastic process

defined by a RecSim NG model. This, together with the differentiability provided by the TensorFlow runtime, enables

the implementation of powerful data assimilation methods. For example, the update step of a maximum-likelihood

simulation parameter-fitting method can be as simple as:

1 with tf . GradientTape () as tape :

2

3

4

5

log_probs = [

log_probability . log_probability_from_value_trajectory ( recsim_model , traj )

for traj in trajectories

]

negative_likelihood = -tf . reduce_sum ( log_probs )

6
7 grad = tape . gradient ( negative_likelihood , simulation_parameters )

Through the log_probability API, RecSim NG can interface with the MCMC machinery provided by TensorFlow
Probability. This powers various posterior inference tasks, which in turn enables model learning given partially observed

trajectories. For example, suppose we are given trajectories generated by the user model specified in Sec. 3.3, but where

the values of the user interest vectors are not provided—this would reflect the data available to the recommender system

itself in most natural settings, which observes only user choice behavior, not the latent state that drives it. One option

for dealing with such missing data (e.g., by the recommender attempting to predict future user choice behavior) would

be to use an expectation-maximization (EM) [7] algorithm estimate both user choice behavior and its dynamics. An

implementation of a stochastic version of EM could be as simple as :

1 for i in range ( num_iters ):

2

3

4

5

6

7

8

posterior_samples = run_chain ()

log_probs = []

with tf . GradientTape () as tape :

log_probs = [ unnormalized_log_prob_train ( posterior_sample ) for in posterior_samples ]

neg_elbo = -tf . reduce_mean ( log_probs )

grads = tape . gradient ( neg_elbo , [ model_parameters ])

optimizer . apply_gradients ( zip ( grads , [ model_parameters ])

Here the unnormalized_log_prob_train function injects the proposed interest imputation into the partial trajectory and
evaluates its log probability:

1 def unnormalized_log_prob_train ( proposed_interest ):
# Hold out the user intent in the trajectories .

2

3

4

5

6

7

user_state_dict = dict ( traj [ ' user state ']. as_dict )

user_state_dict [ ' interest '] = proposed_interest

traj [ ' user state '] = Value (** user_state_dict )

return log_probability . log_probability_from_value_trajectory (

variables = variables , value_trajectory = traj , num_steps = horizon - 1)

6See https://github.com/tensorflow/probability.

RecSim NG

15

The run_chain function invokes TFP’s Hamiltonian Monte Carlo sampler. This example is developed in full detail in the
RecSim NG tutorials.

The log_probability API is highly flexible and, together with differentiability, enables a wide array of training and
inference methods, such as: policy gradient reinforcement learning, adversarial training, variational Bayesian inference,

generative adversarial networks and many others.

4 USE CASES

In this section, we offer a small sampling of the ways in which RecSim NG can be used to develop or assess recommender

algorithms. We present three use cases—each using a somewhat simplified, stylized model of user or content-provider

behavior and relatively simple recommender policies—that showcase the execution and inference capabilities offered by

RecSim NG and the specific mechanisms by which it can be used to evaluate, optimize, or train models and algorithms.

We leave out many of the details of these models to focus attention on broader uses to which RecSim NG can be put—full

details for these use cases (including complete model specifications and parameter values) can be found in the online
tutorials.7 We also point to research articles that use more elaborate, realistic models and policies on which several of
these use cases are based. (We note that some of the results in this cited work were produced using early versions of

RecSim NG.) Finally, we note that RecSim NG’s TensorFlow-based implementation allows one to train recommender

policies through direct coupling with a RecSim NG environment.

4.1 Partially Observable RL

One of the greatest challenges facing the deployment of RL in recommender systems is optimizing policies in the presence

of user latent state (satisfaction, interests, etc.), unobservable contextual conditions (location, activity, companions), and

other latent environmental factors [20, 36]. The induced partially observable RL problems require defining policies over

some summary of the user and/or environment history—effectively involving some form of state estimation—which

can be exceptionally challenging.

Fortunately, generally applicable methods like policy-gradient optimization, coupled with variance-reduction heuris-

tics and methods for unbiasing training, often provide good results for non-Markovian problems. However, tuning policy

parameters in this way is by its nature heuristic, depending on many factors such as the policy architecture, parameter

initialization, choice of optimizer (and its settings), and so on, for which there is very little (general) theoretical guidance.

It is hence crucial to understand conditions under which recommender algorithms, and the specific representations they

adopt, work well or fail—this can be used to improve algorithm development in research settings, or increase confidence

in one’s models prior to deploying them in live experiments or production settings. Given a specific user/environment

model, RecSim NG makes it quite straightforward to train a policy using policy gradient using TensorFlow, TFP, and its

log probability API. We illustrate this using a simplified environment model and policy representation.

We adopt a simple probabilistic factorization model to explore and uncover user interests, and use RecSim NG to

learn a latent representation of users using policy gradient. Our simulation environment is similar to that used by Ie, et

al. [28], but with the addition of partial observability. The recommender policy presents a slate of content items to a

user at each interaction, from which the user selects at most one item for consumption. Users and items are modeled

in a fashion similar to that described in Sec. 3.3: each user has a dynamic, latent topic/interest vector, whose initial

value is sampled randomly, and which evolves based on the topic and the quality of any recommended item consumed.

7See https://github.com/google-research/recsim_ng/tree/master/recsim_ng/colab.

16

M. Mladenov, C. Hsu, V. Jain, E. Ie, C. Colby, N. Mayoraz, H. Pham, D. Tran, I. Vendrov, and C. Boutilier

Specifically, if a high-quality item is consumed, a user’s interests move (stochastically) in the direction of the item’s

topic, while lower-quality consumption has the opposite effect. The quality of an item also impacts the user’s immediate

engagement with an item: lower quality items have lower expected engagement. Finally, the probability of a user

selecting (and engaging) with a recommended item uses a simple MNL model: the selection probability is greater if that

item’s topic is closer to the user’s current interest vector.

The RecSim NG environment also correlates topics with quality: the quality distribution for some topics skews higher

than for others. Thus, the recommender system faces interesting short/long-term tradeoffs, as well as exploration/ex-

ploitation tradeoffs, of the type familiar to RL researchers and practitioners: (a) to promote greater user engagement over

the long-run, the recommender is inclined to recommend items from high-quality topics, which will gradually nudge

the user’s interests toward such topics; (b) however, such a strategy comes at a short-term cost if the recommended

items are not close to the user’s current interests; and (c) the optimal items(s) to place on a slate to make this tradeoff

depend on the user’s latent interest vector, the estimation of which the recommender can improve by taking active

exploration or probing steps by recommending specific items.

To reflect the partially observable nature of the problem, we design a recommender policy class that maintains a finite

history of its engagement with a user consisting of the past 15 consumed items and learns an embedding of this history.

Based on this embedding and user engagement, the recommender uses a deep network to learn a belief state (or estimate
of the user latent state) ℎ𝑢 ; it also learns a similar item representation. Finally, we train the stochastic recommender
policy (which selects a slate of items based on its belief state for the user in question) using the policy gradient method
REINFORCE [56], where the policy parameters are comprised of the embeddings and the parameters that encode ℎ𝑢 .
Specifically, using RecSim NG, given the current policy, we sample trajectories for 𝐵 users over horizon 𝑇 , and then
iteratively update the policy parameters using the policy gradients (of the score function surrogate of the cumulative
user engagement) computed by automatic differentiation in TensorFlow. In the snippet below, the slate docs log prob
variable, which computes the log probability of the slates composed by the recommender, is automatically generated by

RecSim NG’s log probability API.

1 with tf . GradientTape () as tape :

2

3

4

5

6

7

8

9

10

11

last_state = tf_runtime . execute ( num_steps = horizon - 1)

last_metric_value = last_state [ ' metrics state ' ]. get ( ' cumulative_reward ')

log_prob = last_state [ ' slate docs log prob ' ]. get ( ' doc_ranks ')

objective = -tf . tensordot ( tf . stop_gradient ( last_metric_value ) , log_prob , 1)

objective /= float ( num_trajectories )

grads = tape . gradient ( objective , trainable_variables )

if optimizer :

grads_and_vars = list ( zip ( grads , trainable_variables ))

optimizer . apply_gradients ( grads_and_vars )

Fig. 1 shows how, in a simple experiment, average-per-user cumulative engagement time induced by the REINFORCE-
trained policy improves with the number of policy-gradient iterations.8 This demonstrates how RecSim NG can be used
to power policy-gradient training to solve partially-observable RL problems. We note that the policy class over which

we optimize is intentionally very simple for illustrative purposes. But the same methodology can be applied to more

realistic models and policy classes, e.g., RNN-based policies—indeed, this technique has been applied to contextual

8The size of the slate is two and 𝑑 is twenty. The number of users 𝐵 in each batch is 1000 and the horizon 𝑇 is 100.

RecSim NG

17

Fig. 1. The cumulative consumption time as a function of REINFORCE iterations averaged over 20 runs. Results shown for different
amounts of history (memory) used as input to the policy.

bandit problems, where RecSim NG is used in the meta-learning of problem-distribution-specific bandit policies that

minimize Bayes regret [10].

RecSim NG makes it very easy to compare different policy classes or learning methods under a variety of different

model conditions. For instance, we can change the amount of user history used by the policy, use REINFORCE to

optimize the resultant policies and measure the impact on performance: Fig. 1 also includes training performance curves

for several different history lengths. It is also straightforward to, for instance: (a) change the degree to which user

interests evolve—from purely static to very rapid—and test how much history is required to get good policy performance

under different conditions; (b) vary the optimization horizon to compare the performance of myopic policies to RL-based

policies; or (c) vary the user choice model to study the impact on the resulting optimized policies.

4.2 Learning Latent Variable Models

The use case in Sec. 4.1 shows the importance of managing partial observability in the design and optimization of

recommender policies. There, partial observability—and hence the non-stationarity of the dynamics relative to the

features observable by the recommender—was driven by a user’s latent (and evolving) interests. Note however that the

recommender made no assumptions about the source of this non-stationarity. By contrast, a policy based on a model

which makes this latent-state structure explicit can often be easier to learn/optimize.

More generally, learning latent variable models can be extremely useful for recommender systems, as they not only

allow for the effective compression of interaction histories or activity streams, but also offer the prospect of greater

interpretability through the lens of meaningful attributes such as user interests, intent, satisfaction, so on. Of course,

learning latent variable models relies in fundamental ways on probabilistic inference—in particular, the computation of

the posterior of the latent variable(s) given observation histories. Such inference is readily enabled by RecSim NG’s APIs,

and can be tightly integrated with TFP’s inference machinery, which we illustrate using one of its MCMC routines.

We demonstrate the use of RecSim NG to learn a simple latent variable model by generating data streams capturing

the interaction of users with a recommender whose policy is “fixed” (i.e., non-adaptive). The user-recommender

interaction is like that in the previous use case—a user is presented with a slate of recommended items and chooses

one (or zero) for consumption using an MNL choice model, based on affinity with the user’s interest vector. The

recommender policy is a non-adaptive, randomized policy. More critically, the user model differs from the one above

in two ways. First, each user’s (latent) interest vector is static. Second, each user has a dynamic, scalar satisfaction

level that increases or decreases depending on immediate “trend” in recommendation quality. More precisely, if the

050100150200Policy Gradient Iteration505560657075Average Cumulative Rewardmemory size 3memory size 5memory size 1018

M. Mladenov, C. Hsu, V. Jain, E. Ie, C. Colby, N. Mayoraz, H. Pham, D. Tran, I. Vendrov, and C. Boutilier

Fig. 2. ELBO versus training step of the Monte-Carlo EM algorithm.

most attractive item on the recommended slate at time 𝑡 is worse that the most attractive item on the previous slate at
time 𝑡 − 1 (i.e., if the user has less affinity, given her latent interest vector, with the current item than the prior item),
then the user’s satisfaction (stochastically) decreases. Likewise, if the most attractive item on the slate improves from

one stage to the next, satisfaction (stochastically) increases. The amount of increase or decrease at each interaction
is governed by a latent sensitivity parameter 𝛼𝑢 which differs for each user 𝑢—specifically, 𝛼𝑢 influences the mean of
a normal distribution from which the updated satisfaction is drawn.9 The user’s choice process either selects a item
for consumption, using the sum of an item’s affinity (computed as the negative Euclidean distance between the item

and the intent) and the user’s satisfaction as logits, or abstains according to a constant “no choice” logit (set to 0 for

illustration). The user’s satisfaction effectively acts as a boost to all item logits compared to the constant "no choice"

logit; thus, at high levels of satisfaction, the user is more likely to select some item for consumption rather than opt out

with “no choice.” If the user’s satisfaction drops sufficiently and no good items are recommended, the user effectively

drops out of the recommender platform in the sense that the probability of selecting any item becomes negligible.

We test RecSim NG’s model learning capabilities by treating user satisfaction as observable (e.g., it may correspond

to a user’s observable degree of engagement with a consumed), but attempt to learn each user’s latent interest vector
and sensitivity parameter 𝛼𝑢 . We use the RecSim model above (the ground-truth model) to generate user trajectories,
i.e., training data. We use a different RecSim NG model (the learning model) to train using this trajectory data: it’s goal

is to fit a new user model to the generated “ground-truth” trajectory data. In particular, we use a simple Hamiltonian

Monte Carlo [37] routine implemented in TensorFlow Probability as a posterior sampler within a Monte-Carlo EM
learning algorithm (as outlined in Sec 3.5). In the learned simulation model, initially 𝛼𝑢 is sampled from 𝑈 (0, 1). In
this example, the learning algorithm recovers the ground-truth model parameters. The progress of the algorithm is

illustrated in Fig. 2 by means of the increasing evidence lower bound (ELBO) over training iterations.

4.3 Ecosystem Modeling

One aspect of recommender systems research that has received relatively little attention is the role of multiagent

interactions among participants (e.g, different users, content providers, etc.) in the recommender ecosystem, and the
impact these have on recommendation policy quality.10 Using simulation to reason about recommender ecosystems—
and optimize recommender policies—requires a flexible modeling framework that supports the effective expression

of the behaviors of all participants. Moreover, since the behavior trajectories of the ecosystem agents are no longer

independent—e.g., the behavior of a user might influence the response of a content provider, or the recommender policy

9We refer to the online tutorial for a complete specification of the model.
10There are exceptions, of course, for example, recent work on fairly allocating the user exposure to content providers [5, 45], game-theoretic optimization
of policies when providers act strategically [3, 4], or optimizing collective user utility (or social welfare) over extended horizons when recommender
policies induce dynamic behavior among providers [35]. We adopt a simplified variant of this latter model in this section.

RecSim NG

19

may correlate the behaviors of multiple users—simulation is no longer “embarrassingly parallel.” In such cases, it is

critical that simulation libraries offer superior performance, especially when the number of agents is large. RecSim

NG offers reusable modeling blocks—for example, the same types of state and choice models can be used for both

content provider and users/content consumers—and can enhance performance considerably using graph compilation,

Tensorflow XLA (accelerated linear algebra) and TPU accelerators.

To demonstrate these capabilities, we develop a simple ecosystem model in which a recommender system mediates

the interaction between users and content providers by making recommendations to users over time. This model

is a (very simplified) version of that developed by Mladenov et al. [35], who study the long-term equilibria of such

ecosystems. We adopt a simplified user model whereby each user is characterized by a static, observable user interest

vector. As above, the vector determines item affinities, which in turn are used as inputs to a multinomial logit choice

model that determines item selection from a recommended slate. User utility for any consumed item is simply her

affinity for the item perturbed with Gaussian noise. The aim of the recommender is to maximize cumulative user utility

(over all users) over a fixed horizon.

Ecosystem effects emerge because of content provider behavior. Every recommendable item is made available by
a unique provider 𝑐. Like users, each provider has an “interest vector” around which the items it makes available
are centered. User and provider interest vectors are sampled using a mixture model to generate subpopulations or

“communities” of different sizes, with members of any given community having interests that are more-or-less aligned.
Moreover, each provider 𝑐 is assumed to require a certain amount of cumulative past-discounted engagement 𝐸𝑐 over any
fixed period: here engagement is measured by the cumulative number of its content items consumed by any user over
the recent past, with more distant consumption discounted relative to more recent consumption. The greater 𝐸𝑐 is, the
greater the number of items (randomly drawn near its interest vector) that provider 𝑐 will generate at any given period.
We compare two different recommender policies in this setting. The first is a standard “myopic” policy that, for any
user 𝑢, always recommends the slate of 𝑘 items for which the user has greatest affinity (we assume that, somewhat
unrealistically, the recommender can observe the user’s interest vector, in order to simplify the model to focus on

ecosystem interactions). Under such a policy, the behavior of providers has the potential to give rise to “rich-get-richer”

phenomena: providers that initially (e.g., for random reasons) attract significant engagement make available a greater

number of items at subsequent periods, which increases the odds of attracting even further future engagement; by

contrast, providers with limited engagement tend to reduce the number of items they generate, which can decrease

future engagement. These dynamics can lead to the eventual decrease in diversity of the type of content made available,

with a concomitant decrease in overall user utility (specifically, disadvantaging users whose interests lie near those of

low-engaged providers).

The second recommender policy is “aware” of these provider dynamics, which it counteracts by generating recom-
mended slates using a content provider “boost” 𝐵𝑐 to score the top 𝑘 items to add to the recommended slate. Specifically,
the boost 𝐵𝑐 for provider 𝑐 increases (decreases) proportionally to the difference between 𝑐’s current cumulative
discounted engagement and the mean engagement across all providers. This boost (which is capped in a tunable fashion
via an auxiliary parameter 𝐿) is then used to randomly increase (or decrease if it is negative) each item’s affinity for a
given user 𝑢. This increases the odds of low-engaged providers having their content recommended on the slate of top 𝑘
items.11

11We note that the ecosystem-aware policy here uses a very simple heuristic, and refer to [35] for a formal analysis and more sophisticated policy
optimization algorithm for this type of problem.

20

M. Mladenov, C. Hsu, V. Jain, E. Ie, C. Colby, N. Mayoraz, H. Pham, D. Tran, I. Vendrov, and C. Boutilier

Fig. 3. Cumulative user utility averaged over all users, for various values of the boost cap parameter 𝐿 (40 runs). The shaded region
shows standard error.

We use RecSim NG to compare the two policies with respect to long-term average user utility (i.e., user social welfare)

by randomly sampling 2000 users, 80 providers, 400 items per period and measuring cumulative user utility over a

horizon of 300 periods. In fact, we evaluate several different instantiations of ecosystem-aware policy for different
values of the boost-cap parameter 𝐿: here 𝐿 = 0 corresponds to a myopic policy (it offers no boost at all), while very
large values of 𝐿 promote under-served providers in a very aggressive way. Fig 3 plots the cumulative user utility
(averaged over 40 random runs) and shows that an ecosystem-aware policy with 𝐿 = 1.2 achieves significantly better
cumulative user utility (161.34) than the myopic policy (130.77); but as this parameter is increased further, cumulative
utility decreases due to the recommendation of content that is increasingly far away from the user’s interests. RecSim

NG can be used to explore the impact of different policy formats, parametrizations or objective functions easily; but it

can also be used to assess under which environment conditions different policies work well. For example, in this model,

it is straightforward to change the underlying (random) “community structure” by adjusting the joint distribution
from which user and provider interests are sampled.12 Likewise, one can adjust the user choice model, the provider
content-generation model, etc.

Finally, we note that many simulation runs are typically required to reduce confidence intervals of around the estimates

of random variables of interest (e.g., average user utility) to acceptable levels. In such circumstances, performance

characteristics are critical. We compared the runtime of this ecosystem simulation on CPU vs. Google TPUv2, while

varying the number of simulation runs from 8 to 1024 (each run is a sampled ecosystem). We found that the inherent

parallelization capabilities of RecSim NG allow Google TPUv2 to achieve roughly a 6X speedup across a variety of user

population sizes.

5 CONCLUDING REMARKS

We have outlined RecSim NG, a scalable, modular, differentiable, probabilistic platform for the simulation of multi-

agent recommender systems. Implemented in Edward2 and TensorFlow, it offers: a powerful, general probabilistic

programming language for agent-behavior specification; tools for probabilistic inference and latent-variable model

learning, backed by automatic differentiation and tracing; and a TensorFlow-based runtime for executing simulations on

accelerated hardware. We illustrated how RecSim NG can be used to create transparent, configurable, end-to-end models

of a recommender ecosystem. We also outlined a set of simple use cases that demonstrated how RecSim NG can be used

12As an example, the model used here induces community structure hierarchically by first sampling provider interests, then sampling user interests by
treating provider interests as the core of a mixture model (see the online tutorials for details), the parameters of which can be used to generate “tighter”
or “looser” communities (users with similar interests) of varying size.

0.00.51.01.52.02.53.0Provider Boost Cap L110120130140150160170Cumulative User UtilityAverage Social Welfare with Different Provider BoostRecSim NG

21

to: develop and evaluate interactive, sequential RL-based recommenders algorithms; learn latent-state models from

data trajectories; and evaluate different recommender policies in complex multi-agent domains with substantial agent

interaction. We also pointed to other research efforts that used RecSim NG to evaluate more sophisticated algorithms

(beyond the simple methods we used for illustration purposes in this paper).

Our hope is that RecSim NG will help both researchers and practitioners easily develop, train and evaluate novel

algorithms for recommender systems. There are a number of profitable next steps we plan to take to further this

objective. Obviously, the development and distribution of models that cover additional use cases will help exemplify

new design patterns and algorithmic principles for those looking to avail themselves of RecSim NG. As we continue to

apply RecSim NG to novel problems, we plan to add new, reusable modeling components to the open-source library—we

also welcome and encourage contributions to the open-source library from other developers and users of RecSim NG.

We are also investigating the release of increasingly realistic user models that can serve as benchmarks for the research

community, as well as methods that can facilitate “sim-to-real” transfer using RecSim NG.

REFERENCES

[1] John Ahlgren, Maria Eugenia Berezin, Kinga Bojarczuk, Elena Dulskyte, Inna Dvortsova, Johann George, Natalija Gucevska, Mark Harman, Ralf
Lämmel, Erik Meijer, Silvia Sapora, and Justin Spahr-Summers. 2020. WES: Agent-based User Interaction Simulation on Real Infrastructure.
arXiv:2004.05363.

[2] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. 2013. The Arcade Learning Environment: An Evaluation Platform for General Agents.

Journal of Artificial Intelligence Research 47 (June 2013), 253–279.

[3] Omer Ben-Porat, Gregory Goren, Itay Rosenberg, and Moshe Tennenholtz. 2019. From Recommendation Systems to Facility Location Games. In

Proceedings of the Thirty-third AAAI Conference on Artificial Intelligence (AAAI-19). Honolulu, 1772–1779.

[4] Omer Ben-Porat and Moshe Tennenholtz. 2018. A Game-Theoretic Approach to Recommendation Systems with Strategic Content Providers. In

Advances in Neural Information Processing Systems 31 (NeurIPS-18). Montreal, 1118–1128.

[5] Asia J. Biega, Krishna P. Gummadi, and Gerhard Weikum. 2018. Equity of Attention: Amortizing Individual Fairness in Rankings. In The 41st

International ACM SIGIR Conference on Research & Development in Information Retrieval. New York, NY, USA, 405–414.

[6] Eli Bingham, Jonathan P Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis Karaletsos, Rohit Singh, Paul Szerlip, Paul Horsfall,
and Noah D Goodman. 2019. Pyro: Deep universal probabilistic programming. The Journal of Machine Learning Research 20, 1 (2019), 973–978.

[7] Christopher M Bishop. 2006. Pattern Recognition and Machine Learning. Springer, New York.
[8] Jim Blythe and Alexey Tregubov. 2018. Farm: Architecture for Distributed Agent-based Social Simulations. In International Workshop on Massively

Multiagent Systems (MMAS-18). Springer, Stockholm, 96–107.

[9] Stephen Bonner and David Rohde. 2019. Latent Variable Session-Based Recommendation. (2019). arXiv:1904.10784.
[10] Craig Boutilier, Chih-Wei Hsu, Branislav Kveton, Martin Mladenov, Csaba Szepesvári, and Manzil Zaheer. 2020. Differentiable Meta-Learning of

Bandit Policies. In Advances in Neural Information Processing Systems 33 (NeurIPS-20). Virtual.

[11] Jack S. Breese, David Heckerman, and Carl Kadie. 1998. Empirical Analysis of Predictive Algorithms for Collaborative Filtering. In Proceedings of the

14th Conference on Uncertainty in Artificial Intelligence. Madison, WI, 43–52.

[12] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. 2016. OpenAI Gym. (2016).

arXiv:arXiv:1606.01540 preprint arXiv:1606.01540.

[13] Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G. Bellemare. 2018. Dopamine: A Research Framework for Deep

Reinforcement Learning. (2018). arXiv:1812.06110.

[14] Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and Ed Chi. 2018. Top-K Off-Policy Correction for a REINFORCE

Recommender System. In 12th ACM International Conference on Web Search and Data Mining (WSDM-19). Melbourne, Australia, 456–464.

[15] Sungwoon Choi, Heonseok Ha, Uiwon Hwang, Chanju Kim, Jung-Woo Ha, and Sungroh Yoon. 2018. Reinforcement Learning-based Recommender

System using Biclustering Technique. (2018). arXiv:1801.05532.

[16] Konstantina Christakopoulou and Arindam Banerjee. 2018. Learning to Interact with Users: A Collaborative-Bandit Approach. In Proceedings of the

2018 SIAM International Conference on Data Mining. San Diego, 612–620.

[17] Konstantina Christakopoulou, Filip Radlinski, and Katja Hofmann. 2016. Towards Conversational Recommender Systems. In Proceedings of the 22Nd
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (San Francisco, California, USA) (KDD ’16). ACM, New York, NY,
USA, 815–824.

[18] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks for YouTube Recommendations. In Proceedings of the 10th ACM

Conference on Recommender Systems. Boston, 191–198.

[19] Thomas Dean and Keiji Kanazawa. 1989. A Model for Reasoning about Persistence and Causation. Computational Intelligence 5, 3 (1989), 142–150.

22

M. Mladenov, C. Hsu, V. Jain, E. Ie, C. Colby, N. Mayoraz, H. Pham, D. Tran, I. Vendrov, and C. Boutilier

[20] Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. 2019. Challenges of Real-World Reinforcement Learning. (2019). arXiv:1904.12901.
[21] Owain Evans, Andreas Stuhlmüller, John Salvatier, and Daniel Filan. 2017. Modeling Agents with Probabilistic Programs. http://agentmodels.org.

Accessed: 2020-5-27.

[22] Jason Gauci, Edoardo Conti, Yitao Liang, Kittipat Virochsiri, Yuchen He, Zachary Kaden, Vivek Narayanan, and Xiaohui Ye. 2018. Horizon: Facebook’s

Open Source Applied Reinforcement Learning Platform. (2018). arXiv:1811.00260 [cs.LG].

[23] Claudio Gentile, Shuai Li, and Giovanni Zappella. 2014. Online Clustering of Bandits. In Proceedings of the Thirty-first International Conference on

Machine Learning (ICML-14). Beijing, 757–765.

[24] Noah D Goodman and Andreas Stuhlmüller. 2014. The Design and Implementation of Probabilistic Programming Languages. http://dippl.org.

Accessed: 2020-5-27.

[25] F. Maxwell Harper and Joseph A. Konstan. 2016. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems

5, 4 (2016), 19:1–19:19.

[26] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural Collaborative Filtering. In Proceedings of the 26th

International Conference on World Wide Web (WWW-17). Perth, Australia, 173–182.

[27] Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui Wu, Heng-Tze Cheng, Tushar Chandra, and Craig Boutilier. 2019. SlateQ: A
Tractable Decomposition for Reinforcement Learning with Recommendation Sets. In International Joint Conference on Artifical Intelligence (IJCAI).
Macau, 2592–2599.

[28] Eugene Ie, Vihan Jain, Jing Wang, Sanmit Navrekar, Ritesh Agarwal, Rui Wu, Heng-Tze Cheng, Morgane Lustman, Vincent Gatto, Paul Covington, Jim
McFadden, Tushar Chandra, and Craig Boutilier. 2019. Reinforcement Learning for Slate-based Recommender Systems: A Tractable Decomposition
and Practical Methodology. (2019). preprint arXiv:1905.12767.

[29] Eugene Ie, Chih wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, Jing Wang, Rui Wu, and Craig Boutilier. 2019. RecSim: A Configurable

Simulation Platform for Recommender Systems. (2019). arXiv:1909.04847.

[30] Joseph A. Konstan, Bradley N. Miller, David Maltz, Jonathan L. Herlocker, Lee R. Gordon, and John Riedl. 1997. GroupLens: Applying Collaborative

Filtering to Usenet News. Commun. ACM 40, 3 (1997), 77–87.

[31] Uri Lerner, Brooks Moses, Maricia Scott, Sheila McIlraith, and Daphne Koller. 2002. Monitoring a Complex Physical System using a Hybrid Dynamic

Bayes Net. In Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence (UAI-02). Edmonton, 301–310.

[32] Shuai Li, Alexandros Karatzoglou, and Claudio Gentile. 2016. Collaborative Filtering Bandits. In Proceedings of the 39th International ACM
SIGIR Conference on Research and Development in Information Retrieval (Pisa, Italy) (SIGIR ’16). ACM, New York, NY, USA, 539–548. https:
//doi.org/10.1145/2911451.2911548

[33] Qianlong Liu, Baoliang Cui, Zhongyu Wei, Baolin Peng, Haikuan Huang, Hongbo Deng, Jianye Hao, Xuanjing Huang, and Kam-Fai Wong. 2019.
Building Personalized Simulator for Interactive Search. In Proceedings of the Twenty-eighth International Joint Conference on Artificial Intelligence
(IJCAI-19). Macau, 5109–5115.

[34] Odalric-Ambrym Maillard and Shie Mannor. 2014. Latent Bandits. In Proceedings of the Thirty-first International Conference on Machine Learning

(ICML-14). Beijing, 136–144.

[35] Martin Mladenov, Elliot Creager, Kevin Swerksy, Omer Ben-Porat, Richard S. Zemel, and Craig Boutilier. 2020. Optimizing Long-term Social Welfare
in Recommender Systems: A Constrained Matching Approach. In Proceedings of the Thirty-seventh International Conference on Machine Learning
(ICML-20). Vienna, 6987–6998.

[36] Martin Mladenov, Ofer Meshi, Jayden Ooi, Dale Schuurmans, and Craig Boutilier. 2019. Advantage Amplification in Slowly Evolving Latent-State

Environments. In International Joint Conference on Artifical Intelligence (IJCAI). To appear.

[37] Radford Neal. 2011. MCMC using Hamiltonian dynamics. In Handbook of Markov Chain Monte Carlo. Chapman and Hall.
[38] Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu, and Kam-Fai Wong. 2018. Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue
Policy Learning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for
Computational Linguistics, Melbourne, Australia, 2182–2192.

[39] Avi Pfeffer and Daphne Koller. 1997. Object-oriented Bayesian Networks. In Proceedings of the Thirteenth Conference on Uncertainty in Artificial

Intelligence (UAI-97). Providence, RI, 303–313.

[40] David Rohde, Stephen Bonner, Travis Dunlop, Flavian Vasile, and Alexandros Karatzoglou. 2018. RecoGym: A Reinforcement Learning Environment

for the problem of Product Recommendation in Online Advertising. (2018). arXiv:1808.00720 [cs.IR].

[41] Nachiketa Sahoo, Param Vir Singh, and Tridas Mukhopadhyay. 2012. A Hidden Markov Model for Collaborative Filtering. Management Information

Systems Quarterly 36, 4 (2012), 1329–1356.

[42] Ruslan Salakhutdinov and Andriy Mnih. 2007. Probabilistic Matrix Factorization. In Advances in Neural Information Processing Systems 20 (NIPS-07).

Vancouver, 1257–1264.

[43] Guy Shani, David Heckerman, and Ronen I. Brafman. 2005. An MDP-based Recommender System. Journal of Machine Learning Research 6 (2005),

1265–1295.

[44] Jing-Cheng Shi, Yang Yu, Qing Da, Shi-Yong Chen, and An-Xiang Zeng. 2019. Virtual-Taobao: Virtualizing Real-world Online Retail Environment

for Reinforcement Learning. In Proceedings of the Thirty-third AAAI Conference on Artificial Intelligence (AAAI-19). Honolulu, 4902–4909.

[45] Ashudeep Singh and Thorsten Joachims. 2018. Fairness of Exposure in Rankings. In Proceedings of the 24th ACM SIGKDD International Conference on

Knowledge Discovery & Data Mining (KDD ’18). Association for Computing Machinery, New York, NY, USA, 2219–2228.

RecSim NG

23

[46] Yueming Sun and Yi Zhang. 2018. Conversational Recommender System. (2018). arXiv:1806.03277 [cs.IR].
[47] Nima Taghipour, Ahmad Kardan, and Saeed Shiry Ghidary. 2007. Usage-based Web Recommendations: A Reinforcement Learning Approach. In

Proceedings of the First ACM Conference on Recommender Systems (RecSys07). ACM, Minneapolis, 113–120.

[48] Yong Kiam Tan, Xinxing Xu, and Yong Liu. 2016. Improved Recurrent Neural Networks for Session-based Recommendations. In Proceedings of the

1st Workshop on Deep Learning for Recommender Systems. Boston, 17–22.

[49] Yuandong Tian, Qucheng Gong, Wenling Shang, Yuxin Wu, and C. Lawrence Zitnick. 2017. ELF: An Extensive, Lightweight and Flexible Research

Platform for Real-time Strategy Games. In Advances in Neural Information Processing Systems 30 (NIPS-17). Long Beach, CA, 2659–2669.

[50] Dustin Tran, Mike Dusenberry, Mark van der Wilk, and Danijar Hafner. 2019. Bayesian layers: A module for neural network uncertainty. In Advances

in Neural Information Processing Systems. 14633–14645.

[51] Dustin Tran, Matthew D. Hoffman, Dave Moore, Christopher Suter, Srinivas Vasudevan, Alexey Radul, Matthew Johnson, and Rif A. Saurous. 2018.
Simple, Distributed, and Accelerated Probabilistic Programming. In Advances in Neural Information Processing Systems 31 (NeurIPS-18). Montreal,
7598–7609.

[52] Dustin Tran, Matthew D Hoffman, Rif A Saurous, Eugene Brevdo, Kevin Murphy, and David M Blei. 2017. Deep probabilistic programming. arXiv

preprint arXiv:1701.03757.

[53] Alexey Tregubov and Jim Blythe. 2020. Optimization of Large-Scale Agent-Based Simulations Through Automated Abstraction and Simplification.

In 21st International Workshop on Multi-Agent-Based Simulation (MABS-20). 81–93.

[54] Aaron van den Oord, Sander Dieleman, and Benjamin Schrauwen. 2013. Deep Content-based Music Recommendation. In Advances in Neural

Information Processing Systems 26 (NIPS-13). Lake Tahoe, NV, 2643–2651.

[55] Wei Wei, Quoc Le, Andrew Dai, and Jia Li. 2018. AirDialogue: An Environment for Goal-oriented Dialogue Research. In Proceedings of 2018

Conference on Empirical Methods in Natural Language Processing (EMNLP-18). Brussels, 3844–3854.

[56] Ronald J. Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. In Machine Learning. 229–256.
[57] Chao-Yuan Wu, Amr Ahmed, Alex Beutel, Alexander J. Smola, and How Jing. 2017. Recurrent Recommender Networks. In Proceedings of the Tenth

ACM International Conference on Web Search and Data Mining (WSDM-17). Cambridge, UK, 495–503.

[58] Ji Yang, Xinyang Yi, Derek Zhiyuan Cheng, Lichan Hong, Yang Li, Simon Xiaoming Wang, Taibai Xu, and Ed H. Chi. 2020. Mixed Negative Sampling

for Learning Two-tower Neural Networks in Recommendations. In Proceedings of the Web Conference (WWW-20). Taipei, 441–447.

[59] Ke Yang and Julia Stoyanovich. 2017. Measuring Fairness in Ranked Outputs. In SSDBM ’17. ACM, 22:1–22:6.
[60] Siriu Yao, Yoni Halpern, Nithum Thain, Xuezhi Wang, Kang Lee, Flavien Prost, Ed H. Chi, Jilin Chen, and Alex Beutel. 2020. Measuring Recommender
System Effects with Simulated Users. In 2nd Workshop on Fairness, Accountability, Transparency, Ethics and Society on the Web (FATES-20). Taipei.
[61] Meike Zehlike, Francesco Bonchi, Carlos Castillo, Sara Hajian, Mohamed Megahed, and Ricardo Baeza-Yates. 2017. FA*IR: A Fair Top-k Ranking
Algorithm. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management (CIKM ’17). Association for Computing
Machinery, New York, NY, USA, 1569–1578.

[62] Xiangyu Zhao, Long Xia, Zhuoye Ding, Dawei Yin, and Jiliang Tang. 2019. Toward Simulating Environments in Reinforcement Learning Based

Recommendations. (2019). arXiv:1906.11462.

[63] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang Tang. 2018. Deep Reinforcement Learning for Page-wise Recommenda-

tions. In Proceedings of the 12th ACM Conference on Recommender Systems (RecSys-18). Vancouver, 95–103.

[64] Stephan Zheng, Alexander Trott, Sunil Srinivasa, Nikhil Naik, Melvin Gruesbeck, David C. Parkes, and Richard Socher. 2020. The AI Economist:

Improving Equality and Productivity with AI-Driven Tax Policies. arXiv:2004.13332.

