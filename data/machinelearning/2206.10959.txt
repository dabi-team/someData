Defect Prediction Using Stylistic Metrics

Rafed Muhammad Yasir
Institute of Information Technology
University of Dhaka
Dhaka, Bangladesh
bsse0733@iit.du.ac.bd

Dr. Ahmedul Kabir
Institute of Information Technology
University of Dhaka
Dhaka, Bangladesh
kabir@iit.du.ac.bd

2
2
0
2

g
u
A
6
2

]
E
S
.
s
c
[

3
v
9
5
9
0
1
.
6
0
2
2
:
v
i
X
r
a

Abstract—Defect prediction is one of

the most popular
research topics due to its potentiality to minimize software
quality assurance effort. Existing approaches have examined
defect prediction from various perspective such as complexity and
developer metrics. However, none of these consider programming
style for defect prediction. This paper aims at analyzing the
impact of stylistic metrics on both within-project and cross-
project defect prediction. For prediction, 4 widely used ma-
chine learning algorithms namely Naive Bayes, Support Vector
Machine, Decision Tree and Logistic Regression are used. The
experiment is conducted on 14 releases of 5 popular, open source
projects. F1, Precision and Recall are inspected to evaluate the
results. Results reveal that stylistic metrics are good predictor of
defects.

Index Terms—defect prediction, stylistic metrics, cross-project,

within-project

To evaluate the models, 14 releases of 5 popular, open
source projects such as opencv, emscripten are used. 3 metrics
namely F1, Recall and Precision are used for evaluation.
Similar to [7], models yielding Recall greater than 70%
and Precision greater than 50% are considered as acceptable
results. Among the 4 algorithms which obtains highest mean
F1,
is regarded as the best. For within-project prediction,
Decision Tree provides the best result (mean F1 is 78.33).
For cross-project prediction, Decision Tree and Support Vector
Machine yield the best results (mean F1s are 72.07 and 72.57
respectively). Furthermore, 9 out of 14 releases in cross-project
defect prediction obtain acceptable result. In within-project
defect prediction, 6 out of 9 cases are acceptable result.

I. INTRODUCTION

A defect is a deviation between the expected and actual
behavior of a program [1]. Fixing defects can consume ap-
proximately 80% of the total cost of a software project [2].
To reduce this cost, defect prediction is required. It can help
test managers to locate bugs and optimise the allocation of
limited testing resources [3].

Existing studies in defect prediction can be divided into two
categories namely within-project (both training and testing is
performed on a single project) and cross-project (training and
testing data are from different projects) defect prediction [4].
Both of these categories have been examined from various
viewpoints such as size and complexity metrics, code smells
etc. However, the impact of programming style on defect
prediction has not been analyzed yet. Programming style
reveals individual’s choice at the time of writing source code,
such as preference of for loop over while loop [5]. Oman et al.
found that code complexity metrics are not enough to charac-
terize programming style [6]. Therefore, stylistic metrics may
improve defect prediction by capturing information which can
not be covered by complexity metrics.

This paper investigates the impact of stylistic metrics on
both within-project and cross-project defect prediction. It uses
60 stylistic metrics, e.g., percentage of if-else, and predicts
whether a ﬁle is buggy or clean. For prediction, 4 widely
used machine learning algorithms namely Naive Bayes (NB),
Support Vector Machine (SVM), Decision Tree (DT), and
Logistic Regression (LR) are used [7], [8], [9].

II. BACKGROUND AND RELATED WORK

Defect prediction can be divided into two categories- within-
project and cross-project defect prediction [4]. If both training
and testing is performed on a single project, it is called within-
project defect prediction. On the other hand, when model is
trained using data from one or several projects and evaluated
on another project, it is called cross-project defect prediction.
For new projects or projects with limited historical data, it
is difﬁcult to build within-project defect prediction model.
For these projects, cross-project defect prediction is useful.
However, the different distribution of defects in the training
and test project makes cross-project defect prediction difﬁcult.
For within-project and cross-project defect prediction, ex-
isting techniques have used mostly code and process metrics
[10]. Code metrics refer to source code properties such as size
and complexity. Process metrics are collected from historical
information archived in different software repositories (e.g.,
version control and issue tracking systems). For example,
number of developers. [11], [7], [8] focused on code metrics,
whereas [12], [9] considered process metrics.

Jureczko and Spinellis used ckjm metrics suite (e.g.,
weighted methods per class, lines of code) for within-project
defect prediction [11]. They collected these metrics for ﬁve
proprietary and eleven open source projects such as Lucene,
Log4j etc. Each of the project had at least two versions. For
each project, regression model is constructed using project
version i-1 and tested on version i. Their models were able
to ﬁnd 80% of the defects within 36% (mean value) of the
classes.

 
 
 
 
 
 
He et al. used the same set of metrics as [11] to ana-
lyze cross-project defect prediction [7]. They investigated 34
releases of 10 open source projects such as Ant, jEdit etc.
They used 5 machine learning algorithms namely Naive Bayes,
J48, Support Vector Machine, Decision Table, and Logistic
Regression to build cross-project and within-project prediction
models. Their study revealed that training data from carefully
selected cross-project may provide better prediction results
than one from the same project.

Wang et al. used semantic features, extracted from pro-
grams’ Abstract Syntax Trees (ASTs) to improve defect
prediction [8]. For example,
type of an AST node. They
generated these features using Deep Belief Network. Usng
these features, they built both within-project and cross-project
defect prediction models. Results reveal that these semantic
features signiﬁcantly improve both within-project and cross-
project defect prediction.

Matsumoto et al. investigated the effect of developers fea-
ture on defect prediction [12]. They considered two types of
developer metrics that characterize developer’s activity (e.g.,
number of commitments) and modules revised by developers
(e.g., number of developers revising module) respectively.
Their experiment on 3 versions of Eclipse (version 3.00, 3.10
and 3.20) revealed that developer metrics can improve within-
project defect prediction compared to models using static or
change metrics.

Soltanifar et al. examined the impact of code smells (e.g.,
excessive parameter list) on within-project defect prediction
[9]. For this purpose, two algorithms namely Naive Bayes and
Logistic Regression were used. The developed models were
compared with models using code churn metrics (e.g., lines
of code added/deleted). The comparison was conducted on
2 projects from telecommunication industry and web content
management system. Their study found that code smell is a
good indicator of defect proneness.

The above discussion indicates that defect prediction has
been analyzed from various viewpoints such as ckjm metrics,
semantic metrics, code smells. However, none of these inves-
tigated the impact of stylistic metrics on defect prediction.
Therefore, this paper initiates the work of analyzing within-
project and cross-project defect prediction from stylistic met-
rics perspective.

III. METHODOLOGY

A. Dataset

To conduct experiments, 14 releases of 5 popular, open
source (available on GitHub) c++ projects, such as, opencv,
bitcoin, are used. The description of the projects are shown in
Table I. For example, project opencv has 27709 commits and
40800 stars.

1https://github.com/bitcoin/bitcoin
2https://github.com/opencv/opencv
3https://github.com/rethinkdb/rethinkdb
4https://github.com/emscripten-core/emscripten
5https://github.com/cocos2d/cocos2d-x

TABLE I: Description of the Projects

Project

bitcoin1

opencv2

rethinkdb3

emscripten4

cocos2d-x5

Number of Commits

Number of Stars

22493

27709

33422

20271

37330

41600

40800

23000

18300

14000

To prepare the dataset, the git repositories of each project
are collected. The master branch is selected in all cases. If the
master branch is not available, the default branch is selected.
Next, each commit of the repositories are analyzed to identify
whether it is a bug ﬁxing commit. A commit message is
searched for keywords such as "Fixed", "Bug", "Patch" and
other expressions [13] to determine if it is bug ﬁxing. If a bug
ﬁxing commit is found, the bug introducing commit of the ﬁles
ﬁxed are found using the SZZ algorithm [14]. Next, the ﬁles
between the bug introducing commit and bug ﬁxing commit
are labeled as buggy. All the other ﬁles are by default labeled
as clean. While labeling, only the source ﬁles (cc, cxx, cpp,
cu, c) are considered. For the labeled ﬁles of all versions, 60
stylistic features, proposed in [5], are extracted. For example,
average number of functions and variable name length.

TABLE II: Summary of the Dataset

Release

bitcoin-0.7.0

bitcoin-0.8.0

cocos2d-x-3.0.0

cocos2d-x-3.5.0

cocos2d-x-3.8.0

emscripten-1.25

emscripten-1.30

emscripten-1.35

opencv-2.4.6.2

opencv-2.4.7

opencv-3.0.0

rethinkdb-1.8

rethinkdb-1.12

rethinkdb-1.16

Total Number

Number of

Percentage of

of Files

Buggy Files

Buggy Files

90

165

751

868

993

1647

1890

2765

1065

1083

1106

448

470

526

64

72

362

583

664

103

136

337

569

571

608

174

184

212

71.11

43.64

48.20

67.17

66.87

6.25

7.20

12.19

53.43

52.72

54.97

38.84

39.15

40.30

Since change patterns in the ﬁrst three years of a project
are unstable, versions released in those years are excluded
from the dataset [15], [16]. Furthermore, latest changes may
be labeled clean even if they are buggy. Therefore, versions
released in the recent three years of a project are excluded as
well [15], [16].

If there are more than three versions of a project after
ﬁltering, a maximum of three versions are selected, as followed

in [8]. The versions selected and the percentage of buggy ﬁles
in each of them are listed in Table II.

B. Approach

This paper analyzes the impact of stylistic metrics on
both within-project and cross-project defect prediction. For
prediction, 4 machine learning algorithms namely Naive Bayes
(NB), Support Vector Machine (SVM), Decision Tree (DT),
and Logistic Regression (LR) are selected since they are
widely used in defect prediction [7], [8], [9]. Jiarpakdee et
al. suggested to remove correlated metrics before constructing
a defect model since they may impact the interpretation of the
model [17]. Therefore, stylistic metrics exceeding Variance
Inﬂation Factor (VIF) score 5 are removed. This is a com-
monly used threshold in prior studies [17], [18], [19]. Table II
reveals that the defect data is imbalanced. Hence, re-sampling
technique SMOTE is used on the training data, as followed in
[8].

Fig. 1: Training and Test Data Selection for Within-project
and Cross-project Defect Prediction

For within-project defect prediction, data from version i and
i+1 are used for training and testing the model respectively,
as shown in Fig 1. For example, data of bitcoin-0.7.0 is used
to predict defects in bitcoin-0.8.0. For cross-project defect
prediction, combination of all releases except the test project
are used for training (shown in Fig 1). Similar to [7], com-
binations upto 3 releases are used due to limited computing
power. For example, to test bitcoin-0.7.0, the training data are
(cocos2d-x-3.0.0), (opencv-2.4.7), (cocos2d-x-3.0.0, cocos2d-
x-3.5.0), (cocos2d-x-3.0.0, emscripten-1.25), (cocos2d-x-3.8.0,
opencv-3.0.0, rethinkdb 1.16) and so on.

IV. EXPERIMENT

A. Performance Evaluation

In this study, Precision, Recall, and F1 are used to measure
defect prediction results. These metrics are widely used for
evaluation in defect prediction [8]. These metrics are calcu-
lated using (1), (2) and (3).

P recision =

T rue P ositive
T rue P ositive + F alse P ositive

(1)

Recall =

T rue P ositive
T rue P ositive + F alse N egative

F 1 =

2 ∗ P recision ∗ Recall
P recision + Recall

(2)

(3)

True positive is the number of buggy ﬁles that are classiﬁed
correctly. False positive and false negative are the numbers of
ﬁles wrongly classiﬁed as buggy and clean respectively. The
values of Recall and Precision are usually mutually exclusive
[7]. This paper considers prediction results with Recall greater
than 70% and Precision greater than 50% as acceptable results
as followed in [7]. F1 outputs a single value by considering
both precision and recall. The higher the F value, the better
the prediction result is.

B. Result Analysis

Table III shows the result obtained using the four al-
gorithms. To assess prediction results, mean values of F1,
Precision and Recall are calculated.

TABLE III: Prediction Result of the Four Algorithms

Category

Indicators

NB

DT

Mean(F1)

Within-project

Mean(Precision)

Mean(Recall)

Mean(F1)

Cross-project

Mean(Precision)

Mean(Recall)

59.33

74.22

52.67

69.93

63.14

82.57

78.33

83.33

77.11

72.07

63.64

86.64

SVM

63.44

70.33

61.78

72.57

65.14

86.43

LR

63.78

72.44

57.33

71.64

64.43

88.14

TABLE IV: Within-Project Defect Prediction Result Using
Decision Tree

Releases (Tr -> T)

Precision

Recall

F1

Project

Bitcoin

0.7.0 -> 0.8.0

Cocos2d-x

3.0.0 -> 3.5.0

*Cocos2d-x

3.5.0 -> 3.8.0

*Emscripten

1.25 -> 1.30

*Emscripten

1.30 -> 1.35

*Opencv

2.4.6.2 -> 2.4.7

Opencv

2.4.7 -> 3.0.0

*RethinkDB

1.8 -> 1.12

*RethinkDB

1.12 -> 1.14

90

50

79

98

94

95

72

87

85

28

58

81

98

96

96

67

90

80

43

53

80

98

95

96

69

89

82

Mean

83.33

77.11

78.33

For within-project defect prediction, Decision Tree per-
formed the best, based on mean F1 (78.33). Wilcoxon Signed-
Rank test is performed to check whether this result is statisti-
cally signiﬁcant [20]. The mean F1 is signiﬁcantly greater for
Decision Tree, at signiﬁcance level = 0.05. The mean values
of Precision and Recall for Decision Tree are 83.33 and 77.11

TABLE V: Cross-Project Defect Prediction Result Using Support Vector Machine

Release

Most suitable training set

Release1

Release2

Release3

Precision

Recall

F

bitcoin-0.7.0

emscripten-1.35

opencv-3.0

rethinkdb-1.8

*bitcoin-0.8.0

cocos2d-x-3.0

emscripten-1.35

opencv-2.4.7

*cocos2d-x-3.0

opencv-3.0

rethinkdb-1.8

rethinkdb-1.16

cocos2d-x-3.5.0

emscripten-1.30

rethinkdb-1.12

rethinkdb-1.16

cocos2d-x-3.8.0

bitcoin-0.7.0

bitcoin-0.8.0

rethinkdb-1.16

*emscripten-1.25

bitcoin-0.8.0

cocos2d-x-3.0

opencv-2.4.7

*emscripten-1.30

bitcoin-0.8.0

cocos2d-x-3.0

opencv-2.4.7

*emscripten-1.35

cocos2d-x-3.0

opencv-2.4.6.2

-

*opencv-2.4.6.2

bitcoin-0.8.0

emscripten-1.35

rethinkdb-1.12

opencv-2.4.7

emscripten-1.25

-

-

opencv-3.0

cocos2d-x-3.5.0

emscripten-1.25

emscripten-1.35

*rethinkdb-1.8

bitcoin-0.8.0

emscripten-1.35

opencv-3.0

*rethinkdb-1.12

bitcoin-0.8.0

cocos2d-x-3.0.0

cocos2d-x-3.5.0

*rethinkdb-1.16

bitcoin-0.8.0

cocos2d-x-3.5.0

emscripten-1.25

50

79

80

33

53

94

93

88

52

50

49

62

63

66

62

80

72

99

47

99

100

99

87

93

88

97

98

89

55

79

76

50

50

96

96

93

65

65

63

76

76

76

Mean

65.14

86.43

72.57

respectively. Detailed results for Decision Tree are presented
in Table IV. In the table, 6 out of 9 cases are acceptable result
(Recall greater than 70% and Precision greater than 50%),
which are shown using star(*) mark.

For cross-project defect prediction, Support Vector Machine
performed the best, based on mean F1 (72.57). Wilcoxon
Signed-Rank test reveal that the mean F1 of Support Vector
Machine is signiﬁcantly greater than Naive Bayes and Lo-
gistic Regression. However, there is no signiﬁcant difference
between Support Vector Machine and Decision Tree. Detailed
results for Support Vector Machine are presented in Table IV.
Here, 9 out of 14 releases obtain Recall greater than 70%
and Precision greater than 50%. The result may be further
improved by combining more than 3 project data for training.

V. THREATS TO VALIDITY

This section presents potential aspects which may threat the

validity of the study:

• External Validity: The experiment is conducted on 14
releases of 5 popular, open source, c++ projects. The ﬁnd-
ings may not be generally applicable for other languages
such as Java or other projects.

• Construct Validity: Following prior study [7], the thresh-
olds of Recall (greater than 70%) and Precision (greater
than 50%) for acceptable result are set. Next, 4 widely
used machine learning algorithms are chosen in this
study. Using other algorithms may impact the results. For
capturing programming style, 60 metrics proposed by [5]

are used. These metrics may not be enough to completely
characterize programming styles.

• Reliability: To ensure reliability of the study, the dataset
is made publicly available at https://doi.org/10.5281/
zenodo.3591973.

VI. CONCLUSION

This paper investigates the impact of stylistic features on
defect prediction. The study is conducted for both within-
project and cross-project defect prediction. 4 machine learning
models namely Naive Bayes, Logistic Regression, Support
Vector Machine and Decision Tree are trained. F1, Precision
and Recall are used to evaluate the results. Results show
that stylistic features are good indicators of defects at a ﬁle
level granularity. For within-project prediction, Decision Tree
provides the best result (mean F1 is 78.33). For cross-project
prediction, Decision Tree and Support Vector Machine yield
the best results (mean F1s are 72.07 and 72.57 respectively).
In future, more than three combinations of training dataset
for cross-project prediction will be analyzed to improve re-
sults. Furthermore, important stylistic metrics will be identi-
ﬁed and incorporated with traditional metrics used in defect
prediction such as OOP metrics and code churn metrics.

VII. ACKNOWLEDGEMENT

This research is supported by Bangladesh Research and

Education Network (BdREN).

REFERENCES

[1] M. Monperrus, “Automatic software repair: a bibliography,” ACM Com-

puting Surveys (CSUR), vol. 51, no. 1, p. 17, 2018.

[2] F. Zhang, Q. Zheng, Y. Zou, and A. E. Hassan, “Cross-project defect
prediction using a connectivity-based unsupervised classiﬁer,” in Pro-
ceedings of the 38th International Conference on Software Engineering,
pp. 309–320, ACM, 2016.

[3] Q. Song, Y. Guo, and M. Shepperd, “A comprehensive investigation of
the role of imbalanced learning for software defect prediction,” IEEE
Transactions on Software Engineering, 2018.

[4] M. Yan, Y. Fang, D. Lo, X. Xia, and X. Zhang, “File-level defect
prediction: Unsupervised vs. supervised models,” in 2017 ACM/IEEE
International Symposium on Empirical Software Engineering and Mea-
surement (ESEM), pp. 344–353, IEEE, 2017.

[5] Q. Mi, J. Keung, and Y. Yu, “Measuring the stylistic inconsistency
in software projects using hierarchical agglomerative clustering,” in
Proceedings of the The 12th International Conference on Predictive
Models and Data Analytics in Software Engineering, p. 5, ACM, 2016.
[6] P. W. Oman and C. R. Cook, “Programming style authorship analysis,” in
Proceedings of the 17th conference on ACM Annual Computer Science
Conference, pp. 320–326, ACM, 1989.

[7] Z. He, F. Shu, Y. Yang, M. Li, and Q. Wang, “An investigation on
the feasibility of cross-project defect prediction,” Automated Software
Engineering, vol. 19, no. 2, pp. 167–199, 2012.

[8] S. Wang, T. Liu, and L. Tan, “Automatically learning semantic features
for defect prediction,” in 2016 IEEE/ACM 38th International Conference
on Software Engineering (ICSE), pp. 297–308, IEEE, 2016.

[9] B. Soltanifar, S. Akbarinasaji, B. Caglayan, A. B. Bener, A. Filiz, and
B. M. Kramer, “Software analytics in practice: a defect prediction model
using code smells,” in Proceedings of the 20th International Database
Engineering & Applications Symposium, pp. 148–155, ACM, 2016.
[10] Z. Li, X.-Y. Jing, and X. Zhu, “Progress on approaches to software
defect prediction,” IET Software, vol. 12, no. 3, pp. 161–175, 2018.
[11] M. Jureczko and D. Spinellis, “Using object-oriented design metrics to
predict software defects,” Models and Methods of System Dependability.
Oﬁcyna Wydawnicza Politechniki Wrocławskiej, pp. 69–81, 2010.
[12] S. Matsumoto, Y. Kamei, A. Monden, K.-i. Matsumoto, and M. Naka-
mura, “An analysis of developer metrics for fault prediction,” in Pro-
ceedings of the 6th International Conference on Predictive Models in
Software Engineering, p. 18, ACM, 2010.

[13] J. ´Sliwerski, T. Zimmermann, and A. Zeller, “When do changes induce
ﬁxes?,” in ACM sigsoft software engineering notes, vol. 30, pp. 1–5,
ACM, 2005.

[14] D. Spadini, M. Aniche, and A. Bacchelli, “PyDriller: Python framework
for mining software repositories,” in Proceedings of the 2018 26th
ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering - ESEC/FSE
2018, (New York, New York, USA), pp. 908–911, ACM Press, 2018.

[15] T. Jiang, L. Tan, and S. Kim, “Personalized defect prediction,” in
2013 28th IEEE/ACM International Conference on Automated Software
Engineering (ASE), pp. 279–289, Ieee, 2013.

[16] M. Tan, L. Tan, S. Dara, and C. Mayeux, “Online defect prediction
for imbalanced data,” in 2015 IEEE/ACM 37th IEEE International
Conference on Software Engineering, vol. 2, pp. 99–108, IEEE, 2015.
[17] J. Jiarpakdee, C. Tantithamthavorn, and A. E. Hassan, “The impact
of correlated metrics on the interpretation of defect models,” IEEE
Transactions on Software Engineering, pp. 1–1, 2019.

[18] J. Fox, Applied regression analysis and generalized linear models. Sage

Publications, 2015.

[19] S. McIntosh, Y. Kamei, B. Adams, and A. E. Hassan, “The impact of
code review coverage and code review participation on software quality:
A case study of the qt, vtk, and itk projects,” in Proceedings of the
11th Working Conference on Mining Software Repositories, pp. 192–
201, ACM, 2014.

[20] R. E. Walpole, R. H. Myers, S. L. Myers, and K. Ye, Probability and
statistics for engineers and scientists, vol. 5. Macmillan New York,
1993.

