9
1
0
2

c
e
D
0
3

]
L
M

.
t
a
t
s
[

1
v
9
7
9
2
1
.
2
1
9
1
:
v
i
X
r
a

End-to-end Learning, with or without Labels

Corinne Jones, Vincent Roulet, Zaid Harchaoui

University of Washington

{

cjones6, vroulet, zaid

@uw.edu

}
January 1, 2020

Abstract

We present an approach for end-to-end learning that allows one to jointly learn a feature
representation from unlabeled data (with or without labeled data) and predict labels for unlabeled
data. The feature representation is assumed to be speciﬁed in a diﬀerentiable programming
framework, that is, as a parameterized mapping amenable to automatic diﬀerentiation. The
proposed approach can be used with any amount of labeled and unlabeled data, gracefully
adjusting to the amount of supervision. We provide experimental results illustrating the
eﬀectiveness of the approach.

1

Introduction

Deep networks trained end-to-end are now ubiquitous, being used for tasks ranging from lung cancer
screening to music transcription to pose estimation (Ardila et al., 2019; Thickstun et al., 2018; Li
et al., 2018). One frequent obstacle when considering new application domains is the need for these
networks to be trained in a supervised manner on vast quantities of labeled data. Unfortunately,
in many application domains only a small amount of labeled data can be collected or even exists.
However, there often exist vast quantities of unlabeled data that are left untouched with a supervised
training approach. Recent work as surveyed in (Oliver et al., 2018) exploits this unlabeled data
in a variety of ways to learn feature representations, either with unsupervised or semi-supervised
approaches.

Recently a number of papers have focused on ad hoc approaches using both labeled and unlabeled
data in order to achieve the best possible performance in a common domain-speciﬁc task. Examples
of such approaches include Deep Clustering (Caron et al., 2018) and TagLM (Peters et al., 2017),
which achieved remarkable results in computer vision and in natural language processing, respectively.
The impressive results obtained on common domain-speciﬁc tasks are often achieved thanks to a
combination of interesting algorithms and clever tricks informed by expert domain knowledge. These,
with the added recourse to industry-scale computing for parameter exploration, result in a lack of
clarity of the overall objective actually considered and minimized. Furthermore, the connections
with unsupervised learning objectives on the one hand and with supervised learning objectives on
the other hand are often partial and unclear.

In this paper we propose a uniﬁed framework for end-to-end learning, applicable when there
is only unlabeled data, or some unlabeled data and some labeled data, or only labeled data. The
proposed framework motivates a precise objective function to be minimized. Furthermore the
connections with classical unsupervised learning and supervised learning objectives are clear. Indeed,
the objective naturally reduces to that of a clustering problem when we have no training set labels

1

 
 
 
 
 
 
and that of a classiﬁcation problem when we have all of the training set labels. Moreover, due to
its simplicity, this setup can be extended. For example, indirect constraints on the labels, such as
requiring two unlabeled observations to have diﬀerent labels, can be readily incorporated.

After reviewing related work on unsupervised and semi-supervised learning in Section 2, we
present the framework in Section 3.1. We show that our proposed objective is smoother than a
straightforward alternative. We address how to optimize the objective function in Section 3.2.
Optimizing over the labels requires care, and for this we present a novel algorithm based on a convex
relaxation of the problem. Finally, we demonstrate the proposed approach in Section 4, showing
that our method, called XSDC, outperforms the supervised baseline.

2 Related Work

We survey recent approaches to end-to-end learning, that is, the joint learning of the parameterized
mapping corresponding to the feature representation and the parameterized task-speciﬁc prediction
mapping, in the presence of unlabeled data. These approaches relate to the areas of unsupervised
and semi-supervised learning.

Unsupervised learning. Most unsupervised deep feature learning methods can be broadly
classiﬁed into one of two categories: methods that optimize a surrogate loss, often based on
known structure in the data; and methods that directly optimize a loss function of interest. Early
examples of the former set of methods include auto-encoders, which attempt to reconstruct the input
observations through a deep network (LeCun, 1987; Goodfellow et al., 2016). Other more recent
examples attempt to approximate a kernel at each layer of a network (Bo et al., 2011; Mairal et al.,
2014; Daniely et al., 2017). Most recently, many papers have been taking advantage of structure in
the data. This includes training to distinguish between multiple views of images or patches and
other images or patches (Wang and Gupta, 2015; Dosovitskiy et al., 2016; Sermanet et al., 2018;
Bachman et al., 2019), learning to predict the relative location of patches in images (Doersch et al.,
2015; Noroozi and Favaro, 2016), and predicting color from grayscale images (Zhang et al., 2016). It
also includes learning to distinguish segments within time series or patches within images, or to
predict future observations in time series (Hyv¨arinen and Morioka, 2016; van den Oord et al., 2018;
L¨owe et al., 2019). A downside to these latter approaches is the focus on achieving state-of-the-art
results on domain-speciﬁc tasks in computer vision and signal processing at the expense of the
conciseness of the formulation.

The second category of unsupervised methods typically alternately optimizes the parameters of
the network and the labels or cluster assignments of the observations. In this thread, several papers
alternate between obtaining assignments or soft assignments and optimizing the parameters of a loss
function aimed at creating well-separated clusters (Xie et al., 2016; Yang et al., 2016; Ghasedi Dizaji
et al., 2017; H¨ausser et al., 2017). In contrast, Bojanowski and Joulin (2017) randomly generate
outputs and then alternately optimize over the parameters of the model and the assignment of
labels to outputs. The most direct approach may be that of Caron et al. (2018), who alternately
cluster the data to obtain pseudo-labels and take steps to optimize the multinomial logistic loss
on the observations with the given pseudo-labels. A drawback of these approaches is the design of
an ad-hoc objective not clearly related to objectives commonly used in unsupervised or supervised
learning, or the combined use of two diﬀerent objectives, one for optimizing the network and one for
clustering. In this paper we build our formulation on a uniﬁed objective that encompasses learning
with unlabeled data only, learning with labeled and unlabeled data, and learning with labeled data
only.

2

Semi-supervised learning. We brieﬂy summarize recent papers in semi-supervised learning.
See the surveys of Chapelle et al. (2010) and Oliver et al. (2018) for a broader overview. Several
recent methods create a semi-supervised algorithm by modifying a supervised algorithm. These
modiﬁcations come in diﬀerent ﬂavors, such as adding a penalty to a supervised learning objective
to encourage similar inputs to be close together in feature space (Belkin et al., 2006; Bachman et al.,
2014; Kamnitsas et al., 2018; Iscen et al., 2019), adding a penalty to encourage high-conﬁdence
outputs (Grandvalet and Bengio, 2004), or rounding outputs to obtain pseudo-labels (Lee, 2013;
Berthelot et al., 2019). Other approaches add a supervised loss to an unsupervised loss similar to
those in the second category of unsupervised methods mentioned above (Zhai et al., 2019) or learn
a feature representation in an unsupervised manner before ﬁne-tuning with labeled data (Wu et al.,
2018).

In contrast to these approaches, there is a set of principled approaches that do not learn a feature
representation through a deep network but optimize over both the parameters of an aﬃne functional
classiﬁer and the label assignments. Such approaches include constrained “forward prediction” using
ridge regression or penalized multinomial logistic regression where the labels are also learned (Bach
and Harchaoui, 2007; Joulin and Bach, 2012; Flammarion et al., 2017) and constrained “reverse
prediction” using a k-means formulation or generalizations thereof (Xu et al., 2009; White and
Schuurmans, 2012). The advantage of the “forward prediction” objective introduced by Bach
and Harchaoui (2007) is that it allows one to easily incorporate additional information about the
clustering problem. Namely, it paved the way to several popular weakly supervised techniques
developed by Bojanowski et al. (2014, 2015) and Alayrac et al. (2016) for computer vision problems.

Relation to existing methods. This work may be viewed as an extension of DIFFRAC (Bach
and Harchaoui, 2007) for learning feature representations. As argued by Jones et al. (2019), a
feature representation deﬁned by a deep network can be related to an approximation of a feature
map associated with a composition of kernels. From this viewpoint, the approach in this paper can
also be interpreted as learning a kernel, i.e., a similarity measure, acting on pairs of examples.

In addition to using deep networks, we improve upon the work of Bach and Harchaoui (2007) by
proposing a convex relaxation of the labeling subproblem. This relaxation allows us to handle several
types of constraints on the labels. The proposed labeling procedure recovers the Sinkhorn-Knopp
algorithm (Sinkhorn and Knopp, 1967; Peyr´e and Cuturi, 2019) when there is no labeled data and
the sizes of the clusters are assumed to be known.

3 Learning with any Level of Supervision

In this section we present the end-to-end learning framework, which takes advantage of any amount
of labeled and unlabeled data.

3.1 Problem formulation

Rd, each belonging to one of k classes. The class label corre-
Consider observations x1, . . . , xn
sponding to each xi, denoted by y(cid:63)
the set of
the set of indices corresponding to the unlabeled
indices corresponding to the labeled data and by
U
data. We aim to use both the labeled and unlabeled data to learn (1) the parameters V(cid:96) at each layer
; V ) : Rd
(cid:96) = 1, 2 . . . , m of a diﬀerentiable deep network φ(
; and (2)
}
·
Rk of a classiﬁer on the output features φ(xi; V ), i = 1, . . . , n.
the parameters W

k, may or may not be observed. We denote by

V1, . . . , Vm
{

RD, where V =

RD×k and b

∈
i ∈ {

0, 1
}

→

S

∈

∈

3

To this end, we consider solving the problem

min
Y ∈C,V,W,b

1
n

n
(cid:88)

i=1

(cid:96) (cid:0)yi, W T φ(xi; V ) + b(cid:1) + Ω(V, W ) ,

{

Y

0, 1

n×k : Y 1k = 1, yi = y(cid:63)

where
=
C
2 is the square loss, and Ω(V, W ) := α (cid:80)m
ˆy
y
(cid:107)
(cid:107)
terms. Here α
constrained so each point is assigned to a unique class.

is the constraint set on the labels, (cid:96)(y, ˆy) =
2
F contains the regularization
Vj
W
(cid:107)
0 are regularization parameters. Note that the label matrix Y is

2
F + λ
(cid:107)

∈ S}
j=1 (cid:107)

0 and λ

i for i

∈ {

≥

≥

−

}

(cid:107)

The advantage of this objective is that it captures three regimes: the unsupervised regime,
in which clustering is performed to learn the labels Y in addition to the network and classiﬁer
parameters V, W, and b; the supervised regime, in which supervised training is performed to learn
the parameters V, W, and b; and the semi-supervised regime, in which a combination of clustering
and supervised training are performed to learn the unknown labels yi with i
, in addition to the
network and classiﬁer parameters V, W, and b. Given an optimization algorithm for this objective,
we may therefore proceed with training regardless of the amount of labeled data.

∈ U

Avoiding trivial solutions. The above objective can lead to two diﬀerent types of trivial
=
solutions: one that maps all observations to the same embedding, i.e., φ(x1; V ) = φ(x2; V ) =
φ(xn; V ); and one that assigns all observations to the same cluster, i.e., y1 = y2 =
= yn. We
· · ·
avoid the ﬁrst problem by subtracting the penalty ρ (cid:80)n
¯φ
2
2 on the squared norms of
(cid:107)
the centered embeddings, where ¯φ = 1/n (cid:80)n
i=1 φ(xi; V ). The second problem exists even when the
network parameters V are ﬁxed, as noted by Bach and Harchaoui (2007, Section 2.3). To avoid this
behavior we add constraints enforcing that the clusters have a minimum and maximum size, i.e.,
nmin1k

nmax1k for some nmax

φ(xi; V )

Y T 1n

i=1 (cid:107)

nmin.

· · ·

−

≥
Formally, we consider then the problem

≤

≤

where

with ρ, α, λ

≥
(cid:48) =

min
Y ∈C(cid:48),V,W,b

1
n

n
(cid:88)

i=1

(cid:96) (cid:0)yi, W T φ(xi; V )+b(cid:1) +

R

(V, W )

(1)

(V, W )=α

R

m
(cid:88)

j=1

Vj

(cid:107)

2
F + λ
(cid:107)

W
(cid:107)

2
F −

(cid:107)

ρ

n
(cid:88)

i=1

φ(xi; V )
(cid:107)

−

2
2

¯φ
(cid:107)

0. The constraint set for Y is now

C

}
In the following we denote simply φi(V ) = φ(xi; V ) and Φ(V ) = (φ1(V ), . . . , φn(V ))T .

∈ S

∈ {

≤

≤

n×k : Y 1k = 1, yi = y(cid:63)
i

for i

, nmin1k

Y T 1n

nmax1k

.

Y
{

0, 1
}

We shall in Section 3.2 present an algorithm to optimize this objective. We term the overall
algorithm XSDC for “X-Supervised Discriminative Clustering”, where “X” can be “un”, “semi” or
“-”, hence covering all cases.

Comparison to reverse prediction objective. The main component of our objective is regu-
2
larized “forward prediction” least squares, which is given by
F
for features Φ(V ). We could alternatively consider “reverse prediction” least squares, which is given
2
by
F /n and is used in k-means. In both cases we would alternate between updating
(cid:107)
the parameters V and W and estimating the labels Y .

2
F /n + λ
(cid:107)

Φ(V )W

1nbT

Φ(V )

Y W

W

−

−

−

Y

(cid:107)

(cid:107)

(cid:107)

(cid:107)

4

One way in which we can compare the quality of the objectives generated by these two options
for learning a representation is via their smoothness properties, i.e., their Lipschitz-continuity and
the Lipschitz-continuity of their gradients. These control the step sizes of optimization methods;
see Bertsekas (2016) and Nesterov (2018) for a discussion of the interplay between smoothness
properties and rates of convergence. We now proceed to show that when ﬁxing the labels Y the
forward prediction objective is smoother than the reverse prediction objective for appropriate choices
of the regularization parameter λ.

For both objectives, we consider ﬁxed labels Y

n×k with Y 1k = 1n. Moreover, for
simplicity we will take α = ρ = 0. Consider the “forward prediction” objective from (1). Deﬁne the
centering matrix Πn = In
n /n. After minimizing over the bias b, the problem may be written
as

0, 1
}

1n1T

∈ {

−

min
V

Ff (Φ(V )) := min
V,W

= min
V

Πn[Y

1
Φ(V )W ]
n (cid:107)
(cid:107)
−
λ tr[Y Y T A(Φ(V ))] ,

2
F + λ

W

(cid:107)

2
F
(cid:107)

(2)

where A(Φ) = Πn

(cid:0)ΠnΦΦT Πn + nλ I(cid:1)−1

Πn.

The corresponding “reverse prediction” problem is given by

min
V

Fr(Φ(V )) := min
V,W

= min
V

1
n (cid:107)
1
n

Φ(V )

tr[(I

−

Y W

2
F
−
(cid:107)
PY )Φ(V )Φ(V )T ] ,

where PY = Y (Y T Y )−1Y T is an orthonormal projector.

To compare the smoothness with respect to any matrix Vj, j = 1, . . . , m it suﬃces to compute
the smoothness with respect to Φ. The next two propositions do that and suggest that our “forward
prediction” objective is generally smoother than the “backward prediction” objective. The proofs
may be found in Appendix A.

R such that for all Φ

Rn×D. Assume there exists
Proposition 1. Let
∈
B. Let nmax be a bound on the maximum number of points
B
in a cluster. Then the Lipschitz constants of Ff and Fr with respect to the spectral norm can be
estimated by

be the set of all possible feature matrices Φ

Φ
(cid:107)

∈ Z

≤

Z

∈

(cid:107)

,

2

Lf := 2

nmax
λn2 B and Lr :=

2
n

B ,

respectively. We therefore have Lf

Lr for λ

nmax/n.

≥

≤

Proposition 2. Under the same assumption as Proposition 1, the Lipschitz constants of

Fr with respect to the spectral norm can be estimated by

∇

Ff and

∇

(cid:96)f :=

8B2nmax

n3λ2 +

2nmax
n2λ

and

(cid:96)r :=

2
n

,

respectively. We therefore have (cid:96)f

(cid:96)r for λ

≥

≤

nmax/(2n) +

(cid:112)

max+16B2nmax/(2n).
n2

5

Figure 1: Example equivalence matrix M and objective function for varying levels of supervision.
For simplicity we set α = ρ = 0 in the objective functions.

3.2 Optimization

The algorithm XSDC that we propose to optimize the objective function (1) works on mini-batches.
Below we will see that, with the square loss, we only ever need to work with the equivalence matrix
M := Y Y T rather than the label matrix Y itself during training. Therefore, at each iteration we
ﬁrst optimize M for a mini-batch given ﬁxed V, W , and b. Then we update W, b, and V for ﬁxed
M . Optimizing over M is the diﬃcult part, and we propose a novel method for doing so. For the
optimization over V, W, and b we use the Ultimate Layer Reversal Stochastic Gradient Optimization
method (ULR-SGO) from Jones et al. (2019), which we review next.

Ultimate layer reversal. Rather than using stochastic gradient optimization to learn V, W , and
b, we use the ULR-SGO method from Jones et al. (2019). It proceeds as follows. Denote the objective
function (1) for ﬁxed Y by Fulr(V, W, b). At each iteration, compute ˆFulr(V ) := minW,b Fulr(V, W, b),
rewriting the objective exclusively in terms of V . Then, using ˆFulr, update V by taking one gradient
step.

−

As long as Fulr is twice diﬀerentiable and Fulr viewed as a function of W and b is strongly convex,
stationary
gradient descent on this objective converges to a stationary point and the resultant ε
stationary points of the original problem. If ˆFulr(V ) is not available in closed form we
points are ε
may estimate it using a quadratic approximation of the loss around the current estimate of V . In
addition, this method can also be applied on mini-batches and in the setting where V is constrained.
Optimizing using ULR-SGO has two main beneﬁts. First, it was empirically shown by Jones et al.
(2019) to converge faster than standard stochastic gradient optimization. Second, in the case of the
square loss it allows us to work with the equivalence matrix M = Y Y T rather than the assignment
matrix Y during the alternating optimization. To see this, observe that from Equation (2) we have

−

ˆFulr(V ) = λ tr[M A(Φ(V ))] + R(V ) ,

ΠnΦ(V )
(cid:107)

where A is deﬁned as in Equation (2) and the regularization term is R(V ) := α (cid:80)m
2
F −
(cid:107)
2
F . Since we only need to optimize over M we can avoid dealing with the problem of
ρ
(cid:107)
there being many solutions Y (cid:63) caused by the optimal objective value being the same if the columns
of Y are permuted. In practice this means that we avoid performing an additional rounding step to
obtain the assignment matrix. Figure 1 displays examples of the equivalence matrix M and the
objective function in the cases of no labeled data, some labeled data, and fully labeled data.

j=1 (cid:107)

Vj

6

NosupervisionLimitedsupervisionFullsupervisionM=11111minM,VTraceMA(Φ(V))s.t.M∈C0M=11011000111minM,VTraceMA(Φ(V))s.t.M∈C0,Mijﬁxedfor(i,j)∈KM=1101011010001011101000101minVTraceMA(Φ(V))MﬁxedAlgorithm 1 Matrix Balancing

n×n encoding known

1: Inputs: Matrix A
∈
Matrix ˆM
2:
∈ {
relations mi,j

Rn×n

0, 1, ?
}
0, 1
}

∈ {

∈ K

with (i, j)

3:
4: Hyperparameters:
5: Minimum and maximum cluster sizes nmin, nmax,
6: number of iterations T , entropic regularization µ
7: Initialize: ˜Q = µ−1A
log(1n1T
−
n∆ = (nmax
nmin)/2
8:
nΣ = (nmax + nmin)/2
u = v = 1n

n /k)

−

9:

10:
11: for t = 1, . . . , T do
12:

13:

14:

15:

16:

Nij
Nij
pv,i

ui

pu,i

←
←

←

←

←

mij/(uivj) ,
˜Qij) ,
exp(
−
(cid:16)
PB∞(nΣ,n∆)
pv,i/(N T
i,·v) ,
(cid:16)
PB∞(nΣ,n∆)
pu,i/(N T

·,iu) ,

(cid:17)

N T

i,·v

(cid:17)

N T

·,iu

,

,

vi

17:
18: end for
19: Output: M = diag(u)N diag(v)

←

(i, j)
(i, j) /

∈ K
∈ K
i = 1, . . . , n

i = 1, . . . , n

i = 1, . . . , n

i = 1, . . . , n

Matrix balancing. Next, consider the objective function (1) when ﬁxing V, W , and b and
optimizing over only the equivalence matrix M = Y Y T . As shown in Proposition 4 in Appendix B,
this problem is NP-complete in general. Therefore, we consider a convex relaxation of it. We
use an entropic regularizer h(M ) = (cid:80)n
i,j=1 Mij log(Mij), which makes the objective strongly
convex and enforces positivity of M . This regularizer appears in a Bregman divergence term
, which can be used to ensure the output does
M0
h(M0), M
Dh(M ; M0) = h(M )
(cid:105)
not stray too far from an initial guess M0. Speciﬁcally, we consider the problem

h(M0)

− (cid:104)∇

−

−

min
M

1
2

tr(M A) + µDh(M ; M0)

(3)

subject to Mij = mij

(i, j)

nmin1n
nmin1n

≤

≤

∀
M 1n
≤
M T 1n

∈ K
nmax1n
nmax1n ,

≤

:= (

)

∈ K

S × S

where the values mij for i, j

represent the known entries of M .
Optimizing the dual of this problem via alternating minimization (see Appendix B), we obtain
Algorithm 1. Note that in the case where the cluster sizes are predetermined and no labeled data
exists, this reduces to the Sinkhorn-Knopp algorithm. The matrix balancing algorithm can be
analyzed using the proof technique of Soules (1991). We detail in Appendix B the computation of
the Jacobian driving the iteration process towards a ﬁxed point. In practice we ﬁnd that 10 steps of
the alternating minimization suﬃce.

(1, 1), . . . , (n, n)
}

∪ {

Overall algorithm. The overall XSDC algorithm for the case where some labeled data is present
is summarized in Algorithm 2. The algorithm proceeds as follows. First, we initialize the pa-

7

Algorithm 2 XSDC (when some labeled data is present)

1: Input: Labeled data XS, YS
2:

Unlabeled data XU
Randomly initialized network parameters V (1)
Number of iterations T

3:

4:
5: Initialize:

V (1), W (1), b(1)
←
starting from V (1)
6: for t = 1, . . . , T do
X (t), Y (t)
7:
M (t)
V (t+1)

←

8:

←

9:
10: end for
11: ˆYU
←
12: ˆW , ˆb
13: Output:

←

Optimize (1) over V, W, b using XS, YS,

Draw minibatch of samples

←
MatrixBalancing(A(Φ(X (t); V (t))), Y (t)Y (t)T
ULR-SGO step(Φ(X (t); V (t)), M (t), V (t))

)

NearestNeighbor(Φ(X; V (T )), YS)
RegLeastSquares(X; [YS, ˆYU ])

ˆYU , V (T ), ˆW , ˆb

rameters V randomly and then optimize the objective on the labeled data to obtain initial es-
timates of V, W, and b. Next, we proceed to optimize using the labeled and unlabeled data
(t)
together. At each iteration, we draw a mini-batch of nb inputs X (t) = (x
nb ) with cor-
responding labels Y (t) (some known, some unknown). We compute the output of the network
(t)
Φ(X (t); V (t)) = (φ(x
nb ; V (t)))T and the corresponding matrix A(Φ(X (t); V (t))) =
Πnb(ΠnbΦ(X (t); V (t))Φ(X (t); V (t))T Πnb + nbλ I)−1Πnb. We then perform matrix balancing to obtain
M (t). Fixing M (t), we then take a gradient step based on the ULR-SGO objective. At the end we
obtain labels ˆYU for the unlabeled data using 1-nearest neighbor on the feature representations
Φ(X; V ). Finally, we estimate the parameters W and b by computing the solution to the least
squares problem with X and [YS, ˆYU ]. The algorithm for the case when no labeled data is present is
similar; see Appendix C.2.

(t)
1 ; V (t)), . . . , φ(x

(t)
1 , . . . , x

The XSDC algorithm has two signiﬁcant beneﬁts in addition to working with any amount of
labeled data. First, learning the features does not require knowledge of the number of clusters.
Instead, it requires only a bound on the fraction of points per cluster, for use in the matrix balancing.
Specifying such a bound is easier than providing the number of clusters. The only time we must use
knowledge of the number of clusters is when evaluating the performance of the learned features.

Second, the algorithm is trivially extendable to the case where we have additional must-link or
must-not-link information related to the labels. For example, if we know observations i and j must
not have the same label, we can encode that constraint in the above problem by adding (i, j) to
K
and setting mij = 0. The algorithm itself is otherwise identical. This is an important extension for
cases where labelers may not have been able to identify the correct label for an observation (e.g.,
“Welsh springer spaniel”) but could provide certain relevant label information (e.g., the label should
be a dog breed).

4 Experiments

The framework we proposed may be applied to any amount of labeled and unlabeled data. In the
experiments we illustrate how the proposed approach can be used for end-to-end learning when

8

few labels are known. Our goal is not to obtain state-of-the-art results in any speciﬁc application
domain. Rather, we aim to show that our method is able to successfully leverage unlabeled data
when labeled data is scarce. We show that when additional labeled data is unavailable but unlabeled
data is plentiful we can typically use the unlabeled data to improve the classiﬁcation accuracy.

We would expect our approach to improve if domain-speciﬁc tricks were used. However, exploring
specialized versions of our algorithm for speciﬁc applications is beyond the scope of this paper.
Instead, we focus on unifying learning with no labeled data, some labeled data, and fully labeled
data in a single training objective. We do this in a domain-agnostic manner.

4.1 Choice of φ

One beneﬁt of the XSDC algorithm is that it can learn a similarity measure for clustering. Typical
clustering methods either do not transform the features or use a kernel-based method. However,
clustering in the original feature space is often ineﬀective. Moreover, clustering using the Gram
matrix on the inputs is infeasible when there are a large number of observations and ineﬀective
when the kernel is improperly chosen (Perez-Cruz and Bousquet, 2004). In this work we therefore
consider kernel networks for φ that are trained to approximate a kernel at each layer.

Many methods for approximating kernels exist, including random Fourier features and the
Nystr¨om method (Rahimi and Recht, 2007; Williams and Seeger, 2000; Mohri et al., 2012). Random
Fourier features are data-independent and the parameters of the Nystr¨om method are typically
selected at random or via quantization (Oglic and G¨artner, 2017). We will instead learn the
parameters of the Nystr¨om method, similarly to Mairal (2016). The regularized Nystr¨om method
approximates a kernel k by computing the inner products of features φ(x) deﬁned by φ(x) =
(k(V T V ) + (cid:15)I)−1/2k(V T x) for some small (cid:15) > 0 where the matrix V contains the parameters.

We expect similar behavior for other kinds of networks, given observations made by Lee et al.

(2018); Matthews et al. (2018); Belkin et al. (2018), and Jones et al. (2019).

4.2 Experimental details

Experimental setup. The experiments focus on four datasets: the vectorial datasets Gisette
(Guyon et al., 2004) and MAGIC (Bock et al., 2004) and the image datasets MNIST (LeCun et al.,
2001) and CIFAR-10 (Krizhevsky and Hinton, 2009). The sizes and dimensions of each dataset may
be found in Table 1 in Appendix C. For more information regarding the dataset splits and how the
data was transformed, see Appendix C.

The architectures we use in the experiments are kernel networks. For the vectorial datasets
we use single-layer kernel networks (KNs) that approximate a Gaussian RBF kernel using the
Nystr¨om method. In contrast, for MNIST we use a convolutional kernel network (CKN) translation
of LeNet-5 (LeCun et al., 2001; Jones et al., 2019) and for CIFAR-10 we use a CKN applied to the
gradient map on the inputs (CKN-GM) (Mairal et al., 2014). For each of these networks we use
32 ﬁlters per layer for the hidden layers. These architectures and datasets were chosen because
they represent a broad spectrum in terms of performance. For details on the parameter values and
hold-out validation, see Appendix C. The hold-out validation is performed on the datasets for each
quantity of labeled data but with a single random seed. The best parameters found are used for all
other random seeds.

Training. The training is performed as follows. The network parameters are initialized by
randomly sampling from the feature representations at each layer of the network. Then the network

9

Figure 2: Average performance across 10 trials of XSDC when varying the quantity of labeled data.
The error bars show one standard deviation from the mean.

is trained for 100 iterations using the labeled data. Finally, the network is trained using the ULR-
SGO algorithm and matrix balancing on the labeled and unlabeled data for 400 iterations. Unless
otherwise speciﬁed, nmin = nmax in the matrix balancing, i.e., all classes are assumed to be equally
represented within each mini-batch. We evaluate the performance of the learned representations
every 10 iterations.

Code. The code for this project is written using Faiss, PyTorch, SciPy, and YesWeCKN (Johnson
et al., 2017; Paszke et al., 2017; Virtanen et al., 2019; Jones et al., 2019). It may be found online at
https://github.com/cjones6/xsdc.

4.3 Results

Improvement with unlabeled data.
In the experiments we ﬁrst compare the XSDC algorithm
to two simple baselines: an initial supervised training of the classiﬁer when the network has random
weights (“random initialization”) and an initial supervised training of both the network and the
classiﬁer (“supervised initialization”). In the latter case the network is trained on only the labeled
data. In both cases, when evaluating the performance the labels of the unlabeled data are ﬁrst
estimated using 1-nearest neighbor with the labeled data. The classiﬁer is then trained on the
labeled and unlabeled data. The reported accuracy of the supervised initialization is the test
accuracy after 100 iterations. In contrast, the reported accuracy of XSDC when labeled data exists
is the test accuracy observed at the iteration where the validation accuracy is highest. We report
this value because the algorithm can overﬁt before 500 iterations. In the case where no labeled
data exists we report the highest observed test accuracy. We performed 10 trials when varying the
random seed and report the mean and standard deviation of the corresponding results.

We would expect that XSDC would provide an improvement over the supervised initialization
when there are gains to be had from additional labeled data. Otherwise, we would expect training
on additional unlabeled data to provide little to no beneﬁt. This is what we see in Figure 2. Figure 2
compares the accuracy of the XSDC algorithm to the initializations as the quantity of labeled data
varies. From all of the plots we can see that the performance of XSDC relative to the supervised
baseline is much larger when the quantity of labeled data is smaller. With 50 labeled examples the
accuracy on Gisette increases by 5% on average when using XSDC instead of the supervised baseline.
On MAGIC the gain is more modest, at 1%. For MNIST the gain is 13%, while for CIFAR-10 it is
5%. In contrast, for 500 labeled observations XSDC outperforms the supervised baseline by 2% on
Gisette but is only 0.3% better than the baseline on MAGIC. The latter results make sense since
the increase in performance of the supervised initialization with the quantity of labeled data has
started leveling oﬀ by then. On MNIST the improvement when there are 500 labeled observations

10

Figure 3: Average performance across 10 trials of XSDC with matrix balancing and two alternative
labeling methods (pseudo-labeling and deep clustering) when varying the quantity of labeled data.
The error bars show one standard deviation from the mean.

drops to 0.3%, while on CIFAR-10 it drops to 1.1%.

There are two other noteworthy aspects of Figure 2. First, it shows that XSDC can improve
over the unsupervised initialization even in the case where there is no labeled data. The relative
improvement in accuracy over the unsupervised baseline ranges from 14% on MAGIC to 56% on
CIFAR-10 when no labeled data is present. Second, the standard error of the diﬀerence in the
performance between the supervised baseline and XSDC tends to be larger when the gap in the
performance between XSDC and the supervised baseline is larger, as expected. For example, on
Gisette the standard error of the diﬀerence in the performance of XSDC and the supervised baseline
is 1.6% in the case of 50 labeled observations, but only 0.9% in the case of 500 labeled observations.
We also visualize the results, examining the case where 50 images from MNIST are labeled.
Figure 6 in Appendix D depicts the feature representations of the unlabeled data at various points
of the training process. For each plot the feature representation was projected to 2-D using t-SNE
(Van Der Maaten and Hinton, 2008). The observations are color-coded according to their true labels.
For more details regarding the visualization, see Appendix D. Comparing Figures 6c and 6d, we
can see that XSDC tends to increase the separation between clusters relative to the supervised
initialization. The digits 4, 7, and 9 are a bit less separated. However, the digits 5 and 8 are each
generally all in one cluster after running XSDC.

Comparison to alternative labeling methods. Next, we compare to two alternative labeling
methods: pseudo-labeling (Lee, 2013) and deep clustering (Caron et al., 2018). Pseudo-labeling is
a method designed to learn feature representations from labeled data and unlabeled data. Label
assignment is performed by predicting labels from regression on the learned features. In contrast,
deep clustering is a method designed to learn feature representations from unlabeled data and assign
labels to unlabeled data. Label assignment is performed by k-means clustering with the learned
features. Designing a variant working with both labeled data and unlabeled data was beyond the
scope of Caron et al. (2018). See Appendix C.4 for how we adapted pseudo-labeling and deep
clustering to the unsupervised setting and the semi-supervised setting, respectively.

Figure 3 displays results comparing the labeling method in XSDC (matrix balancing) to the
labeling methods from pseudo-labeling and deep clustering in the case where some labeled data is
present. From the plots we can see that the accuracy with matrix balancing and pseudo-labeling
are only signiﬁcantly diﬀerent when training the LeNet-5 CKN on MNIST. However, both matrix
balancing and pseudo-labeling typically outperform deep clustering when training the kernel network
on Gisette and the LeNet-5 CKN on MNIST. On average, matrix balancing is 0.8-3% better than
deep clustering when training the kernel network on Gisette and 0.3-21% better than deep clustering
when training the LeNet-5 CKN on MNIST. These results suggest that for certain architectures

11

and datasets, using label information may be essential to achieving a performance close to the best
possible one. On the other hand, the choice of how that label information is incorporated, whether
it is by matrix balancing or pseudo-labeling, may matter less frequently in terms of the performance.

Improvement with additional constraints. As noted in Section 3.2, XSDC can seamlessly
incorporate additional must-link and must-not-link constraints. To assess the beneﬁt of adding such
constraints, we provide additional experiments with LeNet-5 on MNIST. We consider two forms of
additional constraints: (1) Must-not-link constraints derived from knowledge of whether or not each
unlabeled observation was from class 4 or 9; and (2) Random correct must-link and must-not-link
constraints among pairs of unlabeled observations and random correct must-not-link constraints
between pairs of unlabeled and labeled observations. The pairs of classes in (1) were selected because
they are frequently confused. This attempts to mimic a situation in which a labeler knows that an
observation belongs to one of two classes, but is not sure which one. Each random constraint in (2)
was added with probability 1/3, yielding approximately the same number of constraints as (1). See
Appendix C.5 for additional details.

Figure 4b visualizes the feature representations resulting from constraints of the form (1) for the
case of 50 labeled observations from MNIST. Examining this ﬁgure, we can see that the clusters are
generally well-separated, including the bright green, light blue, and purple clusters, which correspond
to the digits 4, 7, and 9, respectively. Visually, this is an improvement over the t-SNE projections
when the additional constraints are not used (cf. Figure 4a).

Finally, Figure 7 in Appendix D displays results comparing the test accuracy on MNIST
when including and not including the additional constraints on the labels. As expected, adding
the additional constraints generally improves the performance. The addition of random correct
constraints results in the best performance, likely because these provide more knowledge related to
the diﬃcult-to-distinguish classes.

Performance with unbalanced data. The XSDC algorithm can handle unbalanced datasets by
changing the bounds on the cluster sizes in the matrix balancing algorithm. To present an example
of how XSDC performs on unbalanced unlabeled data we again trained LeNet-5 on MNIST. We
used 50 labeled observations, equally distributed across classes. For the unlabeled data we varied
the fraction of labels 0-4 and the fraction of labels 5-9 between 5% and 95%. For training we use
the hold-out validation set to determine the bounds on the cluster sizes.

The results are presented in Figure 7b in Appendix D. Training with XSDC on both the labeled
and unlabeled data is nearly always better than training on the labeled data only (dashed curve).
As expected, the performance tends to be better for more balanced data. The best accuracy was
85%, obtained with 40% 0-4’s, while the worst accuracy was 64%, obtained with 95% 0-4’s. In
contrast, the accuracy when training on only the labeled data was 69%. These results suggest that
as long as one believes that the unlabeled data is not extremely unbalanced, it could be beneﬁcial
to use it during training.

Sensitivity to hyperparameters. The XSDC algorithm generally has four or ﬁve hyperpa-
rameters to tune in the semi-supervised case (depending on the network). In order to assess the
importance of these parameters, we perform a sensitivity analysis, again for LeNet-5 on MNIST with
50 labeled observations. Figure 8 in Appendix D displays the results when varying one parameter
at a time, ﬁxing the others to their values from cross-validation. From the plots we can see that the
parameter that requires the most careful tuning in this setting is the semi-supervised learning rate.

12

(a) XSDC

(b) XSDC with additional constraints

Figure 4: Visualizations of the unlabeled MNIST features obtained when training the LeNet-5
CKN with 50 labeled observations. The CKN features were projected to 2-D using t-SNE. The
constraints used in (b) were derived from knowledge of whether the label for each unlabeled point
lay in the set

.

4,9
}

{

The learning rate for the supervised initialization, along with the penalties on the centered features
and classiﬁer weights, just need to be suﬃciently small.

5 Conclusion

In this work we presented a principled learning algorithm called XSDC that can be used on any
amount of labeled and unlabeled data. In the special case of unsupervised learning the objective
is a clustering objective in which the feature representation is also learned. In contrast, in the
special case of supervised learning, the objective is a classiﬁcation objective. We demonstrated the
eﬀectiveness of XSDC on four datasets, showing that when adding additional labeled data would
help, substituting it with unlabeled data often yields large performance improvements.

Acknowledgements

The authors would like to gratefully acknowledge support from NSF CCF-1740551, NSF DMS-
1810975, the program “Learning in Machines and Brains” of CIFAR, and faculty research awards.
This work was ﬁrst presented at the Women in Machine Learning Workshop in December 2019, for
which the ﬁrst author received travel funding from NSF IIS-1833154.

13

References

J.-B. Alayrac, P. Bojanowski, N. Agrawal, J. Sivic, I. Laptev, and S. Lacoste-Julien. Unsupervised
In Conference on Computer Vision and Pattern

learning from narrated instruction videos.
Recognition, pages 4575–4583, 2016.

D. Ardila, A. P. Kiraly, S. Bharadwaj, B. Choi, J. J. Reicher, L. Peng, D. Tse, M. Etemadi, W. Ye,
G. Corrado, D. P. Naidich, and S. Shetty. End-to-end lung cancer screening with three-dimensional
deep learning on low-dose chest computed tomography. Nature Medicine, 2019.

F. R. Bach and Z. Harchaoui. DIFFRAC: a discriminative and ﬂexible framework for clustering. In

Neural Information Processing Systems, pages 49–56, 2007.

P. Bachman, O. Alsharif, and D. Precup. Learning with pseudo-ensembles. In Advances in Neural

Information Processing Systems, pages 3365–3373, 2014.

P. Bachman, R. D. Hjelm, and W. Buchwalter. Learning representations by maximizing mutual

information across views. CoRR, abs/1906.00910, 2019.

M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric framework for learning
from labeled and unlabeled examples. Journal of Machine Learning Research, 7:2399–2434, 2006.

M. Belkin, S. Ma, and S. Mandal. To understand deep learning we need to understand kernel

learning. In International Conference on Machine Learning, pages 540–548, 2018.

D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. Raﬀel. MixMatch: A holistic
approach to semi-supervised learning. In Advances in Neural Information Processing Systems,
pages 5050–5060, 2019.

D. P. Bertsekas. Nonlinear programming. Athena Scientiﬁc, 3rd edition, 2016.

L. Bo, K. Lai, X. Ren, and D. Fox. Object recognition with hierarchical kernel descriptors. In

Conference on Computer Vision and Pattern Recognition, pages 1729–1736, 2011.

R. Bock, A. Chilingarian, M. Gaug, F. Hakl, T. Hengstebeck, M. Jirina, J. Klaschka, E. Kotrc,
P. Savicky, S. Towers, A. Vaicilius, and W. Wittek. Methods for multidimensional event classiﬁca-
tion: A case study using images from a Cherenkov gamma-ray telescope. Nuclear Instruments and
Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated
Equipment, 516(2):511–528, 2004.

P. Bojanowski and A. Joulin. Unsupervised learning by predicting noise. In International Conference

on Machine Learning, pages 517–526, 2017.

P. Bojanowski, R. Lajugie, F. Bach, I. Laptev, J. Ponce, C. Schmid, and J. Sivic. Weakly supervised
action labeling in videos under ordering constraints. In European Conference on Computer Vision,
pages 628–643, 2014.

P. Bojanowski, R. Lajugie, E. Grave, F. Bach, I. Laptev, J. Ponce, and C. Schmid. Weakly-supervised
alignment of video with text. In International Conference on Computer Vision, pages 4462–4470,
2015.

M. Caron, P. Bojanowski, A. Joulin, and M. Douze. Deep clustering for unsupervised learning of

visual features. In European Conference on Computer Vision, pages 139–156, 2018.

14

C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions on

Intelligent Systems and Technology, 2:27:1–27:27, 2011.

O. Chapelle, B. Schlkopf, and A. Zien. Semi-Supervised Learning. The MIT Press, 1st edition, 2010.

A. Daniely, R. Frostig, V. Gupta, and Y. Singer. Random features for compositional kernels. CoRR,

abs/1703.07872, 2017.

C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual representation learning by context

prediction. In International Conference on Computer Vision, pages 1422–1430, 2015.

A. Dosovitskiy, P. Fischer, J. T. Springenberg, M. A. Riedmiller, and T. Brox. Discriminative
unsupervised feature learning with exemplar convolutional neural networks. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 38(9):1734–1747, 2016.

N. Flammarion, B. Palaniappan, and F. Bach. Robust discriminative clustering with sparse

regularizers. Journal of Machine Learning Research, 18:80:1–80:50, 2017.

K. Ghasedi Dizaji, A. Herandi, C. Deng, W. Cai, and H. Huang. Deep clustering via joint convolu-
tional autoencoder embedding and relative entropy minimization. In International Conference on
Computer Vision, pages 5747–5756, 2017.

I. J. Goodfellow, Y. Bengio, and A. C. Courville. Deep Learning. Adaptive computation and

machine learning. MIT Press, 2016.

Y. Grandvalet and Y. Bengio. Semi-supervised learning by entropy minimization. In Advances in

Neural Information Processing Systems, pages 529–536, 2004.

I. Guyon, S. R. Gunn, A. Ben-Hur, and G. Dror. Result analysis of the NIPS 2003 feature selection

challenge. In Advances in Neural Information Processing Systems, pages 545–552, 2004.

P. H¨ausser, A. Mordvintsev, and D. Cremers. Learning by association - A versatile semi-supervised
training method for neural networks. In Conference on Computer Vision and Pattern Recognition,
pages 626–635, 2017.

A. Hyv¨arinen and H. Morioka. Unsupervised feature extraction by time-contrastive learning and
nonlinear ICA. In Advances in Neural Information Processing Systems, pages 3765–3773, 2016.

A. Iscen, G. Tolias, Y. Avrithis, and O. Chum. Label propagation for deep semi-supervised learning.

In Conference on Computer Vision and Pattern Recognition, pages 5070–5079, 2019.

J. Johnson, M. Douze, and H. J´egou. Billion-scale similarity search with GPUs. CoRR,

abs/1702.08734, 2017.

C. Jones, V. Roulet, and Z. Harchaoui. Kernel-based translations of convolutional networks. arXiv

preprint arXiv:1903.08131, 2019.

A. Joulin and F. R. Bach. A convex relaxation for weakly supervised classiﬁers. In International

Conference on Machine Learning, 2012.

K. Kamnitsas, D. C. Castro, L. L. Folgoc, I. Walker, R. Tanno, D. Rueckert, B. Glocker, A. Criminisi,
and A. V. Nori. Semi-supervised learning via compact latent space clustering. In International
Conference on Machine Learning, pages 2464–2473, 2018.

15

R. M. Karp. Reducibility among combinatorial problems. Kiberneticheskiuı Sbornik. Novaya Seriya,

12:16–38, 1975.

A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical

report, University of Toronto, 2009.

Y. LeCun. Modeles connexionnistes de l’apprentissage. PhD thesis, Universit´e P. et M. Curie (Paris

6), June 1987.

Y. LeCun, L. Bottou, Y. Bengio, and P. Haﬀner. Gradient-based learning applied to document

recognition. In Intelligent Signal Processing, pages 306–351. IEEE Press, 2001.

D.-H. Lee. Pseudo-label: The simple and eﬃcient semi-supervised learning method for deep
neural networks. In International Conference on Machine Learning Workshop on Challenges in
Representation Learning, 2013.

J. Lee, Y. Bahri, R. Novak, S. S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. Deep neural
networks as Gaussian processes. In International Conference on Learning Representations, 2018.

Y. Li, G. Wang, X. Ji, Y. Xiang, and D. Fox. DeepIM: Deep iterative matching for 6D pose

estimation. In European Conference on Computer Vision, pages 695–711, 2018.

S. L¨owe, P. O’Connor, and B. Veeling. Putting an end to end-to-end: Gradient-isolated learning of
representations. In Advances in Neural Information Processing Systems, pages 3033–3045, 2019.

J. Mairal. End-to-end kernel learning with supervised convolutional kernel networks. In Advances

in Neural Information Processing Systems, pages 1399–1407, 2016.

J. Mairal, P. Koniusz, Z. Harchaoui, and C. Schmid. Convolutional kernel networks. In Advances in

Neural Information Processing Systems, pages 2627–2635, 2014.

A. Matthews, J. Hron, M. Rowland, R. E. Turner, and Z. Ghahramani. Gaussian process behaviour
in wide deep neural networks. In International Conference on Learning Representations, 2018.

M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of Machine Learning. Adaptive

computation and machine learning. MIT Press, 2012.

Y. Nesterov. Lectures on convex optimization. Springer, 2nd edition, 2018.

M. Noroozi and P. Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles.

In European Conference on Computer Vision, pages 69–84, 2016.

D. Oglic and T. G¨artner. Nystr¨om method with kernel k-means++ samples as landmarks. In

International Conference on Machine Learning, pages 2652–2660, 2017.

A. Oliver, A. Odena, C. A. Raﬀel, E. D. Cubuk, and I. J. Goodfellow. Realistic evaluation of deep
semi-supervised learning algorithms. In Advances in Neural Information Processing Systems,
pages 3239–3250, 2018.

A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga,
and A. Lerer. Automatic diﬀerentiation in PyTorch. In Advances in Neural Information Processing
Systems Workshop on Autodiﬀ, 2017.

16

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot,
and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning
Research, 12:2825–2830, 2011.

F. Perez-Cruz and O. Bousquet. Kernel methods and their potential use in signal processing. IEEE

Signal Processing Magazine, 21(3):57–65, May 2004.

M. E. Peters, W. Ammar, C. Bhagavatula, and R. Power. Semi-supervised sequence tagging with
bidirectional language models. In Association for Computational Linguistics, ACL 2017, pages
1756–1765, 2017.

G. Peyr´e and M. Cuturi. Computational optimal transport. Foundations and Trends in Machine

Learning, 11(5-6):355–607, 2019.

A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in Neural

Information Processing Systems, pages 1177–1184, 2007.

P. Sermanet, C. Lynch, Y. Chebotar, J. Hsu, E. Jang, S. Schaal, and S. Levine. Time-contrastive
networks: Self-supervised learning from video. In International Conference on Robotics and
Automation, pages 1134–1141, 2018.

R. Sinkhorn and P. Knopp. Concerning nonnegative matrices and doubly stochastic matrices. Paciﬁc

Journal of Mathematics, 21:343–348, 1967.

G. W. Soules. The rate of convergence of Sinkhorn balancing. Linear algebra and its applications,

150:3–40, 1991.

J. Thickstun, Z. Harchaoui, D. P. Foster, and S. M. Kakade. Invariances and data augmentation
for supervised music transcription. In International Conference on Acoustics, Speech and Signal
Processing, pages 2241–2245, 2018.

A. van den Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding.

CoRR, abs/1807.03748, 2018.

L. Van Der Maaten and G. Hinton. Visualizing data using t-SNE. Journal of Machine Learning

Research, 9:2579–2605, 2008.

P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski,
P. Peterson, W. Weckesser, J. Bright, S. van der Walt, M. Brett, J. Wilson, K. J. Millman,
N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, I. Polat, Y. Feng, E. W.
Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. A. Quintero, C. R.
Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and Contributors. SciPy
1.0-Fundamental algorithms for scientiﬁc computing in Python. CoRR, abs/1907.10121, 2019.

X. Wang and A. Gupta. Unsupervised learning of visual representations using videos. In International

Conference on Computer Vision, pages 2794–2802, 2015.

M. White and D. Schuurmans. Generalized optimal reverse prediction. In International Conference

on Artiﬁcial Intelligence and Statistics, pages 1305–1313, 2012.

C. K. I. Williams and M. W. Seeger. Using the Nystr¨om method to speed up kernel machines. In

Advances in Neural Information Processing Systems, pages 682–688, 2000.

17

Z. Wu, Y. Xiong, S. X. Yu, and D. Lin. Unsupervised feature learning via non-parametric instance
discrimination. In Conference on Computer Vision and Pattern Recognition, pages 3733–3742,
2018.

J. Xie, R. B. Girshick, and A. Farhadi. Unsupervised deep embedding for clustering analysis. In

International Conference on Machine Learning, pages 478–487, 2016.

L. Xu, M. White, and D. Schuurmans. Optimal reverse prediction: a uniﬁed perspective on
supervised, unsupervised and semi-supervised learning. In International Conference on Machine
Learning, pages 1137–1144, 2009.

J. Yang, D. Parikh, and D. Batra. Joint unsupervised learning of deep representations and image
clusters. In Conference on Computer Vision and Pattern Recognition, pages 5147–5156, 2016.

X. Zhai, A. Oliver, A. Kolesnikov, and L. Beyer. S4L: Self-supervised semi-supervised learning.

CoRR, abs/1905.03670, 2019.

R. Zhang, P. Isola, and A. A. Efros. Colorful image colorization. In European Conference on

Computer Vision, pages 649–666, 2016.

A Smoothness of the Objective Function

In this appendix we estimate the smoothness constants for “forward prediction” regularized least
squares and “backward prediction” least squares. Regularized forward prediction least squares
learns to predict the label matrix Y from the features Φ:

min
W

1
n (cid:107)

Y

ΦW

−

1nbT

2
F + λ
(cid:107)

W
(cid:107)

(cid:107)

2
F

−

In contrast, reverse prediction least squares learns to predict the features Φ from the labels Y :

min
W

1
n (cid:107)

Φ

−

Y W

2
F .

(cid:107)

As noted by Xu et al. (2009), the solution of the forward prediction problem can be recovered from
the solution of the reverse prediction problem as long as Φ is full rank.

Now we return to Proposition 1, which compared the Lipschitz constants of the two objectives,

and provide its proof.

R such that for all Φ

Rn×D. Assume there exists
Proposition 1. Let
∈
B. Let nmax be a bound on the maximum number of points
B
in a cluster. Then the Lipschitz constants of Ff and Fr with respect to the spectral norm can be
estimated by

be the set of all possible feature matrices Φ

Φ
(cid:107)

∈ Z

≤

Z

∈

(cid:107)

,

2

Lf := 2

nmax
λn2 B and Lr :=

2
n

B ,

respectively. We therefore have Lf

Lr for λ

nmax/n.

≥

≤

Proof. After minimizing in the classiﬁer variable W , the forward prediction objective reads

Ff (Φ) = λ tr[Y Y T Πn(ΠnΦΦT Πn + nλ I)−1Πn] .

18

Deﬁne G(Φ) = (cid:0)ΠnΦΦT Πn + nλ In

(cid:1)−1

. The gradient of Ff is then

Since

G(Φ)

(cid:107)

2
(cid:107)

≤

1/(nλ),

(cid:107)

2
(cid:107)

≤

nmax,

Πn

2

(cid:107)

(cid:107)

1 and

G(Φ)ΠnΦ

2

(cid:107)

ΠnΦ

≤ (cid:107)

2/(nλ), we obtain
(cid:107)

Ff (Φ) =

2λΠnG(Φ)ΠnY Y T ΠnG(Φ)ΠnΦ.

−

∇
Y Y T

Ff (Φ)
(cid:107)

2

(cid:107)∇

≤

2

≤
(cid:107)
nmax
λn2 B =: Lf .

Next, recall that the reverse prediction objective for ﬁxed cluster assignments Y may be written as
Fr(Φ) = 1
PY )ΦΦT ] where PY = Y (Y T Y )−1Y T is an orthonormal projector. Its gradient,
−
PY )Φ, can therefore be bounded as

Fr(Φ) = 2

n tr[(I
n (I

−

∇

Fr(Φ)

(cid:107)∇

2

(cid:107)

≤

2
n

B =: Lr.

Hence, taking λ

≥

nmax/n, we have (cid:96)f

(cid:96)r.

≤

Before moving on to the smoothness of the gradient we prove a lemma. The lemma estimates
the Lipschitz constant of the gradient of the “forward prediction” objective function Ff (Φ) from
Section 3.1.

Rn×D and deﬁne the function Ff : Rn×D
Lemma 3. Consider a feature matrix Φ
with Φ
by Ff (Φ) = λ tr[Y Y T Πn(ΠnΦΦT Πn + nλ I)−1Πn]. Assume there exists B such that for all Φ
Φ
(cid:107)

B. Then for all Φ1, Φ2

∈ Z

∈ Z

≤

∈

(cid:107)

,

2

R
,

→
∈ Z

Ff (Φ1)

(cid:107)∇

− ∇

Ff (Φ2)

(cid:107)2 ≤

(cid:18) 8B2nmax

n3λ2 +

2nmax
n2λ

(cid:19)

Φ1
(cid:107)

Φ2

2 .
(cid:107)

−

Hence, an upper bound on the Lipschitz constant of the gradient of Ff (Φ) is given by

(cid:96)f :=

8B2nmax

n3λ2 +

2nmax
n2λ

.

Proof. Note that the gradient of Ff is given by
G(Φ) = (ΠnΦΦT Πn + nλI)−1. Now let
=
using that

∇
(cid:107) · (cid:107)
1 since Πn is a projection matrix,

(cid:107) · (cid:107)

Πn
(cid:107)

Ff (Φ) =

2λΠnG(Φ)ΠnY Y T ΠnG(Φ)ΠnΦ, where
2 denote the spectral norm and observe that,

−

Ff (Φ1)

Ff (Φ2)
(cid:107)

(cid:107) ≤
1
2λ (cid:107)∇
− ∇
(cid:13)
(cid:13)ΠnG(Φ1)ΠnY Y T ΠnG(Φ1)ΠnΦ1
(cid:13)
(cid:13)G(Φ1)ΠnY Y T ΠnG(Φ1)ΠnΦ1
(cid:124)

−
(cid:123)(cid:122)
(a)
(cid:13)
(cid:13)G(Φ2)ΠnY Y T ΠnG(Φ2)ΠnΦ1
(cid:124)

+

=

≤

ΠnG(Φ2)ΠnY Y T ΠnG(Φ2)ΠnΦ2

−
G(Φ2)ΠnY Y T ΠnG(Φ2)ΠnΦ1

(cid:13)
(cid:13)
(cid:125)

(cid:13)
(cid:13)

G(Φ2)ΠnY Y T ΠnG(Φ2)ΠnΦ2

.

(cid:13)
(cid:13)
(cid:125)

−
(cid:123)(cid:122)
(b)

First consider term (a). We have that

(cid:13)
(cid:13)G(Φ1)ΠnY Y T ΠnG(Φ1)ΠnΦ1
(cid:13)
(cid:13)[G(Φ1)ΠnY Y T ΠnG(Φ1)

−

=

G(Φ2)ΠnY Y T ΠnG(Φ2)ΠnΦ1

(cid:13)
(cid:13)

G(Φ2)ΠnY Y T ΠnG(Φ2)]ΠnΦ1

(cid:13)
(cid:13)

−

19

(cid:13)
(cid:13)G(Φ1)ΠnY Y T ΠnG(Φ1)
(cid:124)

≤

−
(cid:123)(cid:122)
(c)

(cid:13)
G(Φ2)ΠnY Y T ΠnG(Φ2)
(cid:13)
(cid:125)

Φ1

.

(cid:107)

(cid:107)

We may bound term (c) by

(cid:13)
(cid:13)G(Φ1)ΠnY Y T ΠnG(Φ1)

G(Φ2)ΠnY Y T ΠnG(Φ2)

(cid:13)
(cid:13)

−
(cid:13)
(cid:13)G(Φ1)ΠnY Y T ΠnG(Φ1)
+
(cid:13)
(cid:13)G(Φ1)ΠnY Y T Πn

−
(cid:13)
(cid:13)G(Φ1)ΠnY Y T ΠnG(Φ2)
G(Φ1)
(cid:107)
(cid:124)

(cid:13)
(cid:13)

≤

≤

G(Φ1)ΠnY Y T ΠnG(Φ2)(cid:13)
(cid:13)
(cid:13)
G(Φ2)ΠnY Y T ΠnG(Φ2)
(cid:13)
(cid:13)
(cid:13)G(Φ2)ΠnY Y T Πn
G(Φ2)
(cid:107)
(cid:125)

+

−

−
(cid:123)(cid:122)
(d)

(cid:13)
(cid:13)

G(Φ1)
(cid:107)

−

G(Φ2)

.

(cid:107)

Furthermore, we can bound term (d) via
= (cid:13)

G(Φ1)

G(Φ2)
(cid:107)

−

(cid:107)

(cid:13)G(Φ1) (cid:2)G(Φ1)−1
G(Φ2)
G(Φ1)
(cid:107)

(cid:107) (cid:107)

≤ (cid:107)

G(Φ2)−1(cid:3) G(Φ2)(cid:13)
(cid:13)
G(Φ2)−1(cid:13)
(cid:13)
(cid:125)

−
(cid:13)
(cid:13)G(Φ1)−1
(cid:124)

−
(cid:123)(cid:122)
(e)

Finally, we can bound term (e) by

(cid:13)
(cid:13)G(Φ1)−1

−

G(Φ2)−1(cid:13)

(cid:13) =

≤

≤ (cid:107)
Using this above, a bound on term (a) is thus

(cid:13)
(cid:13)ΠnΦ1ΦT
(cid:13)
(cid:13)Φ1ΦT
Φ1

1 −
Φ1

1 Πn
−
Φ1ΦT
2
Φ2

(cid:107)(cid:107)

−

(cid:107)

(cid:13)
(cid:13)

ΠnΦ2ΦT
2 Πn
(cid:13) + (cid:13)
(cid:13)
(cid:13)Φ1ΦT
2 −
Φ1
Φ2
+
−

(cid:107)(cid:107)

(cid:107)

(cid:13)
(cid:13)

Φ2ΦT
2
.
Φ2

(cid:107)

(cid:13)
(cid:13)G(Φ1)ΠnY Y T ΠnG(Φ1)ΠnΦ1
(cid:0)(cid:13)
(cid:13)G(Φ1)ΠnY Y T Πn

−
(cid:13)
(cid:13)G(Φ2)ΠnY Y T Πn

G(Φ2)ΠnY Y T ΠnG(Φ2)ΠnΦ1
G(Φ2)

G(Φ1)

(cid:13)
(cid:13) +

(cid:13)
(cid:1)
(cid:13)

(cid:13)
(cid:13)

(cid:107)

(cid:107) (cid:107)

(

Φ1
(cid:107)

(cid:107)

+

Φ2

Φ1

)
(cid:107)

(cid:107)

(cid:107)

Φ1

(cid:107)(cid:107)

Φ2

.

(cid:107)

−

(cid:107)

≤

Next, consider term (b). We have that

(cid:13)
(cid:13)G(Φ2)ΠnY Y T ΠnG(Φ2)ΠnΦ1
(cid:13)
(cid:13)G(Φ2)ΠnY Y T ΠnG(Φ2)
Φ1

(cid:13)
(cid:13)

(cid:107)

−

−

≤

G(Φ2)ΠnY Y T ΠnG(Φ2)ΠnΦ2
Φ2

.

(cid:107)

(cid:13)
(cid:13)

Therefore, returning to the original quantity of interest, we have

1
2λ (cid:107)∇
(cid:8)(cid:0)(cid:13)
+ (cid:13)

≤

Ff (Φ1)

− ∇

Ff (Φ2)
(cid:13)
(cid:13) +

(cid:13)G(Φ1)ΠnY Y T Πn
(cid:13)G(Φ2)ΠnY Y T ΠnG(Φ2)(cid:13)
(cid:9)
(cid:13)

(cid:107)
(cid:13)
(cid:13)G(Φ2)ΠnY Y T Πn
Φ2
Φ1

× (cid:107)

−

(cid:107)

G(Φ1)
(cid:107)

G(Φ2)
(cid:107)

(cid:107) (cid:107)

Φ1

(
(cid:107)

(cid:107)

+

Φ2
(cid:107)

(cid:107)

)

Φ1
(cid:107)

(cid:107)

(cid:13)
(cid:1)
(cid:13)

.

We have
G(Φ1)

(cid:107)

(cid:107)
≤

2
(cid:107)

Y Y T

2
1/(nλ) and

≤

(cid:107)

G(Φ2)
(cid:107)

(cid:107)

2

≤

1/(nλ). Therefore, we have

nmax, where nmax is a bound on the maximum size of the clusters. Lastly,

1
2λ (cid:107)∇

Ff (Φ1)

Ff (Φ2)

(cid:107)2 ≤

− ∇

=

(cid:40)

(cid:18) 1
nλ

2nmax
nλ
(cid:18) 4B2nmax

n3λ3 +

(cid:19)2

(2B)B +

(cid:41)

nmax
n2λ2

Φ1
(cid:107)

−

Φ2

2
(cid:107)

(cid:19)

nmax
n2λ2

Φ1

(cid:107)

Φ2

2 ,

(cid:107)

−

20

and so an upper bound on the Lipschitz constant is given by

(cid:96)f :=

(cid:18) 8B2nmax

n3λ2 +

(cid:19)

.

2nmax
n2λ

Now we recall Proposition 2 from Section 3.1, which compared the Lipschitz constants of the

gradients of the forward and reverse prediction objectives, and provide its proof.

Proposition 2. Under the same assumption as Proposition 1, the Lipschitz constants of

Fr with respect to the spectral norm can be estimated by

∇

Ff and

∇

(cid:96)f :=

8B2nmax

n3λ2 +
(cid:96)r for λ

2nmax
n2λ

and

(cid:96)r :=

2
n

,

respectively. We therefore have (cid:96)f

≤

Proof. The gradient of Ff is given by
we have that

∇

(cid:112)

≥
Ff (Φ) =

nmax/(2n) +

max+16B2nmax/(2n).
n2
2λΠnG(Φ)ΠnY ˜Y T ΠnG(Φ)ΠnΦ. By Lemma 3

−

Ff (Φ1)

(cid:107)∇

− ∇

Ff (Φ2)

(cid:107)2 ≤

(cid:18) 8B2nmax

n3λ2 +

2nmax
n2λ

(cid:19)

Φ1
(cid:107)

Φ2

2 .
(cid:107)

−

Next, observe that the gradient of Fr is

∇

Fr(Φ) = 2

n (I

PY )Φ. Hence, we have

Fr(Φ1)

Fr(Φ2)

2
(cid:107)

≤

−

(cid:107)∇

Φ1

Φ2

2 .

(cid:107)

−

−
2
n (cid:107)

For λ

≥

nmax/(2n) +

(cid:112)

max + 16B2nmax/(2n), we therefore have (cid:96)f
n2

(cid:96)r.

≤

B Solving the Label Assignment Problem

Now we address the problem of optimizing the labels for the unlabeled data. The following
proposition shows that this discrete problem is in general NP-complete for k > 2.

Proposition 4. Let A

∈

Rn×n. The label assignment problem

min
Y

s.t.

tr(Y Y T A)

k
(cid:88)

j=1

Yij = 1,

i = 1, . . . , n

is NP-complete for k > 2.

Yij

0, 1

∈ {

} ∀

i = 1, . . . , n, j = 1, . . . , k

Proof. The proof will follow by showing that the k-coloring problem is a special case of the
matrix balancing problem. Let G be an undirected, unweighted graph with no self-loops. Deﬁne
n×n to be the adjacency matrix of G. Then G is k-colorable if and only if the following
A
problem has minimum value zero:

0, 1
}

∈ {

min
Y

k
(cid:88)

(cid:88)

j=1

i,i(cid:48)∈A

Yi,jYi(cid:48),j

21

Noting that

s.t.

k
(cid:88)

j=1

Yij = 1,

i = 1, . . . , n

Yij

0, 1

∈ {

} ∀

i = 1, . . . , n, j = 1, . . . , k .

k
(cid:88)

(cid:88)

j=1

i,i(cid:48)∈A

Yi,jYi(cid:48),j = tr(Y Y T A) ,

we may rewrite the above problem as

min
Y

s.t.

tr(Y Y T A)

k
(cid:88)

j=1

Yij = 1,

i = 1, . . . , n

∀

Yij

0, 1

∈ {

} ∀

i = 1, . . . , n, j = 1, . . . , k .

This is a special case of the matrix balancing problem, in which A is the adjacency matrix of a graph.
Therefore, as the k-coloring problem is NP-complete for k > 2 (Karp, 1975), the label assignment
problem with discrete assignments is also NP-complete for k > 2.

B.1 Matrix balancing algorithm

Due to the previous result we optimize a convex relaxation of the label assignment problem. Consider
Rn×n with some known entries mij whose indices
a similarity matrix A
2. Then, given minimum and maximum cluster sizes nmin and nmax,
lie in the set
}
the problem we solve is

Rn×n and a matrix M

∈
1, . . . , n

K ⊂ {

∈

min
M ∈Rn×n

Tr(M T A)

subject to nmin1

M 1

≤

Mij = mij,

nmax1, nmin1
,

≤
(i, j)

∈ K

M T 1

≤

≤

nmax1, M

0

≥

where 1 = 1n is the vector of ones in Rn. The constants nmax and nmin are upper and lower bounds
on the sizes of the clusters, respectively.

We consider an entropic regularization of the problem. Speciﬁcally, we use the entropic regularizer

h(M ) =

(cid:88)

ij

Mij log(Mij) with

∇

h(M ) = 11T + log(M ) ,

where log(M ) is understood to be applied element-wise. We make a proximal step from an initial guess
M0 by considering the Bregman divergence Dh(M ; M0) = h(M )
=
h(M )
a feasible M0 is given by
(M0)ij = 1/k, where k is the number of clusters. Consider then the problem

+ C, where C is a constant.

11T + log(M0), M

h(M0)
=

h(M0), M

−
K

− (cid:104)∇

− (cid:104)

M0

−

If

∅

(cid:105)

(cid:105)

Tr(M T A) + µDh(M ; M0)

min
M ∈Rn×n
subject to nmin1

M 1

≤

≤

nmax1, nmin1

M T 1

≤

≤

nmax1, M

0

≥

22

which is equivalent to

Mij = mij,

(i, j)

,

∈ K

min
M ∈Rn×n

Tr(M T Q) + µh(M )

subject to nmin1

M 1

≤

Mij = mij,

nmax1, nmin1
.

≤
(i, j)

∈ K

M T 1

≤

≤

nmax1, M

0

≥

(4)

where Q = A

µ11T

−

−

µ log(M0).

Dual problem. Considering the problem scaled by µ−1 and introducing Lagrange multipliers, we
obtain

max
α∈Rn,β∈Rn,γ∈Rn,δ∈Rn,λ∈R|K|

min
M ∈Rn×n

µ−1 Tr(M T Q) + h(M ) + αT (nmin1

βT (nmax1

M 1)

−

−

M T 1)

δT (nmax1

+ γT (nmin1
(cid:88)

−
λij(Mij

+

(i,j)∈K

−
mij)

−

M 1)

−
M T 1)

−

subject to M

0, α

0,

β

0,

γ

0,

δ

0 .

≥

≥

≥

≥

≥

The minimum in M for α, β, γ, δ, and λ ﬁxed can be computed analytically. It reads

where ˜Q = µ−1Q + 11T = µ−1A
Λij = 0 otherwise. The dual problem then reads

−

M (cid:63) = exp(

( ˜Q + (β

α)1T + 1(δ

γ)T + Λ)) ,

−

−
−
log(M0) and Λ = [Λij]n
i,j=1 with Λij = λij if (i, j)

and

∈ K

min
α∈Rn,β∈Rn,γ∈Rn,δ∈Rn,λ∈R|K|

1T exp(

( ˜Q + (β

α)1T + 1(δ

γ)T + Λ))1

−

−
nmin(α + γ)T 1 + nmax(β + δ)T 1 +

−

−

(cid:88)

(i,j)∈K

λijmij

subject to α

0,

β

0,

γ

≥

≥

Using the change of variables a = β

α, b = β + α, c = δ

−

0,

δ

0 .

≥

γ, d = δ + γ, the dual problem is then

≥

−

min
a∈Rn,b∈Rn,c∈Rn,d∈Rn,λ∈R|K|

exp(

a)T exp(

( ˜Q + Λ)) exp(

−

c)

−

−
nmax

+

nmin

−
2

(b + d)T 1 +

nmax + nmin
2

(a + c)T 1 +

(cid:88)

(i,j)∈K

λijmij

subject to b

,

a
|

d

c
|
≥ |

.

≥ |
Minimization in b and d can be performed analytically (using nmax
reads

nmin) such that the problem

≥

min
a∈Rn,c∈Rn,λ∈R|K|

exp(

a)T exp(

( ˜Q + Λ)) exp(

c) + n∆(
(cid:107)

−

1 +

a
(cid:107)

1) + nΣ(a + c)T 1
c
(cid:107)

(cid:107)

(5)

−
(cid:88)

−

+

λijmij ,

(i,j)∈K

where n∆ = (nmax

−

nmin)/2 and nΣ = (nmax + nmin)/2.

23

Alternating minimization. We consider an alternating minimization scheme to solve (5).

Minimization in λ. Diﬀerentiating with respect to λ leads to

exp(

ij) = exp( ˜Qij) exp(a(cid:63)
λ(cid:63)

i ) exp(c(cid:63)

j )mij ,

−

(i, j)

.

∈ K

Minimization in a. Minimization in a for c and λ ﬁxed reads, denoting x = exp(

Λ(cid:63))) exp(

c(cid:63)),

−

min
a

max
(cid:107)η(cid:107)∞≤1

exp(

a)T x + aT (nΣ1 + n∆η) .

−

( ˜Q +

−

Switching the max and min, the minimum in a for η ﬁxed (which is always deﬁned since nΣ1+n∆η
for

1) is given by

η

∞
(cid:107)

≤

(cid:107)

0

≥

exp(

a(cid:63)(η)i) = (nΣ + n∆ηi)/xi

−

i = 1, . . . , n .

Denoting θ = nΣ1 + n∆η, the problem then reads

max
(cid:107)θ−nΣ1(cid:107)∞≤n∆

θT (1 + log(x))

θT log(θ) .

−

Its minimum is reached at

θ(cid:63) = PB∞(nΣ1,n∆)(x) ,
where PB∞(nΣ1,n∆) is the projection on the inﬁnity ball of radius n∆ centered at nΣ1. We then get

exp(

−

a(cid:63)
i ) = PB∞(nΣ,n∆)

(cid:16)

exp[

( ˜Q + Λ(cid:63))]T

i,· exp(

−

(cid:110)

(cid:17)
c(cid:63))

−

/

exp[

( ˜Q + Λ(cid:63))]T

i,· exp(

−

(cid:111)

c(cid:63))

.

−

Note that in the case where the clusters are constrained to all have the same size, i.e., nmin = nmax,
the numerator reduces to nΣ.

Minimization in c. Minimization in c is analogous and we get

exp(

−

c(cid:63)) = PB∞(nΣ,n∆)

exp[

( ˜Q + Λ(cid:63))]T

·,i exp(

(cid:16)

(cid:17)
a(cid:63)))

/

(cid:110)

exp[

( ˜Q + Λ(cid:63))]T

·,i exp(

−

(cid:111)

a(cid:63))

.

−

Deﬁne u = exp(

a), v = exp(

c), and N = exp(

( ˜Q + Λ)). Given the above equations, the

steps of the alternating minimization are given by:

−

−

−

−

−

N

(t+1)
ij

N

(t+1)
ij

(t+1)
u
i

= mij/

(cid:17)

,

(cid:16)

(t)
(t)
i v
u
j
˜Qij) ,
(cid:18)

= exp(

−
= PB∞(nΣ,n∆)

N

(t+1)
i,·

T

T

(i, j)

∈ K

(i, j) /

∈ K

i = 1, . . . , n

T

v(t)

(cid:19)

,

(cid:19)

(cid:18)

v(t)

/

N

(t+1)
i,·

(t+1)
i

v

= PB∞(nΣ,n∆)

(cid:18)

N

(t+1)
·,i

u(t+1)

(cid:19)

(cid:18)

/

N

T

(t+1)
·,i

u(t+1)

(cid:19)

,

i = 1, . . . , n .

Assuming these converge to values N (cid:63), u(cid:63), and v(cid:63), the ﬁnal output is

M (cid:63) = diag(u(cid:63))N (cid:63) diag(v(cid:63)) .

The algorithm is summarized in Algorithm 1.

24

B.2 Jacobian of the Sinkhorn iterations

Consider for A

∈

Rn×n symmetric, the balancing problem

min
M ∈Rn×n

Tr(M (cid:62)A) + µh(M )

subject to M 1n = α, M (cid:62)1n = β ,

with α, β

Rn, h(M ) = (cid:80)
Its dual can be written

∈

ij Mij log(Mij) and 1n denoting the vector of all ones in Rn.

min
a,b∈Rn

exp(

−

a)(cid:62) exp(

µ−1A

−

1n1(cid:62)

n ) exp(

b) + a(cid:62)α + b(cid:62)β ,

−

−

and after a change of variables

u(cid:62)Qv

min
u,v

−

log(u)(cid:62)α

−

log(v)(cid:62)β ,

where Q = exp(

µ−1A

1n1(cid:62)

n ).

−

−

Alternating minimization on (6) reads, starting from v0 = 1n, for t

ut+1 = α./(Qvt),

vt+1 = β./(Q(cid:62)ut+1)

0,

≥

(6)

(7)

where ./ denotes the element-wise division of two vectors. The iterations (7) can be written in
matrix form as

Qt+1 = ˜Πβ(Πα(Qt)) ,

(8)

where for given vectors α, β

Rn,

∈

Πα(Q) = diag(α./(Q1n))Q ,

˜Πβ(Q) = Q diag(β./(Q(cid:62)1n)) = (Πβ(Q(cid:62)))(cid:62) ,

i , with ei the ith canonical
starting from Q0 = Q and we denoted for x
vector in Rn. We present the computation of the Jacobian of the iterative process around a ﬁxed
point in the following proposition. This Jacobian drives the application of the generalized Ostrowski
theorem presented by Soules (1991).

i=1 xieie(cid:62)

Rn, diag(x) = (cid:80)n

∈

Proposition 5. For a given Q∗ such that Πα(Q∗) = Q∗ and Πβ(Q∗) = Q∗, the Jacobian of the
vectorized composition π = Vect

Vect−1 at q∗ = Vect(Q∗) reads

Πα

˜Πβ

◦

◦

◦
n
(cid:88)

i,j=1

π(q∗) =

∇

(eje(cid:62)

j −

1nu(cid:62)

i eje(cid:62)
j )

(eie(cid:62)

i −

⊗

eie(cid:62)
i

1nv(cid:62)

j ) ,

where ui = Q(cid:62)ei
(Q1n)i

, vj =

Qej
(Q(cid:62)1n)j

.

Rn×n, denote by Vect(Q)

Proof. For a matrix Q
∈
columns of Q). Denote by Vect−1 : Rn2
Q. Denote by T the linear operator in Rn2
We are interested in the vectorized formulation of Πα, Πβ that read for q

the vectorized matrix (given by stacking the
Rn×n the inverse operation such that Vect−1(Vect(Q)) =
Rn×n, Vect(Q(cid:62)) = T Vect(Q).

such that for any Q

Rn2

→

∈

∈

,

Rn2

∈

πα(q) = Vect(Πα(Vect−1(q)))

˜πβ(q) = T πβ(T q)

25

Precisely, denote π = ˜πβ

◦

πα. The iterations (8) then read

qt+1 = π(qt).

Our assumption is that π has a ﬁxed point q∗. We need then to show that π is Frechet diﬀerentiable
at q∗ and that ρ(

π(q∗)) < 1. Since

∇

π(q) =

∇
πα(q) and

πα(q)

∇

˜πβ(πα(q)) ,

∇

∇

∇

it boils down to analyzing that
˜πβ(πα(q))) < 1.
and ρ(
πβ(q)T (cid:62). In the following denote by
We begin by computing
B the Kronecker product of two matrices A, B and In the identity matrix in Rn×n. Recall the
A) Vect(X). We have

A
Kronecker formula for matrices with appropriate sizes: Vect(AXB) = (B(cid:62)
, denoting Q = Vect−1(q), two formulations of πα:
for q

˜πβ(πα(q)) are deﬁned on q∗ and that ρ(

πα(q). Note that

˜πβ(q) = T (cid:62)

πα(q)) < 1

Rn2

∇

∇

∇

∇

∇

⊗

⊗

∈

πα(q) = (In
⊗
πα(q) = (Q(cid:62)

⊗
eie(cid:62)
i )

∈

where E = (cid:80)n
i=1(ei
using the chain rule

⊗

diag(α./(Q1n)))q

In) Vect(diag(α./(Q1n))) = (Q(cid:62)

⊗

In)E[α./(1(cid:62)

Rn2×n such that Vect(diag(x)) = Ex for x

In)q]

n ⊗
Rn. We therefore get

∈

πα(q) = (In

(cid:124)

∇

⊗

diag(α./(Q1n)))
(cid:125)

(cid:123)(cid:122)
A

+ (1n
(cid:124)

⊗

In) diag(

−

(cid:123)(cid:122)
B

α./(Q1n).2)E(cid:62)(Q

,

In)
(cid:125)

⊗

where we used that f : x
element-wise application of the square operation and the gradient of g : x
Then we get

α./x for x

f (x) = diag(

→

∇

∈

Rn has gradient

−
→

α./x.2) with .2 the
g(x) = A(cid:62).
Ax is

∇

A =

n
(cid:88)

i=1

(In

[αi/(Q1n)i]eie(cid:62)

i ) =

⊗

[αi/(Q1n)i](In

eie(cid:62)

i ) ,

⊗

n
(cid:88)

i=1

B =

(1n

−

⊗

In)





n
(cid:88)

[αj/(Q1n)2

j ]eje(cid:62)
j

j=1

(cid:32) n
(cid:88)





i=1

(cid:33)

(e(cid:62)

i ⊗

eie(cid:62)

i )(Q

In)

⊗





=

−

n
(cid:88)

[αj/(Q1n)2

j ](1n

j=1



eje(cid:62)
j )



⊗

(cid:32) n
(cid:88)

(e(cid:62)

i Q

i=1

(cid:33)

eie(cid:62)
i )

⊗

=

n
(cid:88)

−

i=1

[αi/(Q1n)2

i ](1n(Q(cid:62)ei)(cid:62)

eie(cid:62)

i ) .

⊗

Therefore, denoting ui = Q(cid:62)ei
(Q1n)i

, we have

πα(q) =

∇

n
(cid:88)

i=1

αi
(Q1n)i

(cid:16)(cid:104)

In

1nu(cid:62)
i

−

(cid:105)

⊗

(cid:17)

.

eie(cid:62)
i

We get similarly, denoting vj =

Qej
(Q(cid:62)1n)j

,

πβ(q) =

∇

n
(cid:88)

j=1

βj
(Q(cid:62)1n)j

T

(cid:16)(cid:104)

In

(cid:105)

1nv(cid:62)
j

−

⊗

eje(cid:62)
j

(cid:17)

T

26

Table 1: Details regarding the datasets used in the experiments

Dataset

Training size Validation size Test size Dimension # Classes

CIFAR-10
Gisette
MAGIC
MNIST

40,000
4,800
8,026
50,000

10,000
1,200
2,006
10,000

10,000
1,000
4,755
10,000

3,072
5,000
10
784

10
2
2
10

=

n
(cid:88)

j=1

(cid:16)

βj
(Q(cid:62)1n)j

eje(cid:62)

j ⊗

(cid:104)

In

1nv(cid:62)
j

−

(cid:105)(cid:17)

.

Therefore, denoting ˜q = πα(q), ˜Q = Vect−1(˜q) and ˜vj =

˜Qej
( ˜Q(cid:62)1n)j

, we get

π(q) =

∇

n
(cid:88)

i,j=1

αiβj
(Q1n)i( ˜Q(cid:62)1n)j

(eje(cid:62)

j −

1nu(cid:62)

i eje(cid:62)
j )

(eie(cid:62)

i −

⊗

eie(cid:62)
i

1n˜v(cid:62)

j ) .

In particular for q∗ such that πα(q∗) = q∗ and πβ(q∗) = q∗, we get

π(q∗) =

∇

n
(cid:88)

i,j=1

(eje(cid:62)

j −

1nu(cid:62)

i eje(cid:62)
j )

(eie(cid:62)

i −

⊗

eie(cid:62)
i

1nv(cid:62)

j ) .

C Additional Experimental Details

Here we provide additional details related to the datasets and the training.

C.1 Datasets

The details of the sizes and dimensions of each dataset we consider may be found in Table 1. For
the MAGIC dataset, which does not have a train/test split, we randomly split the data 75%/25%
into train/test sets. For Gisette, MAGIC, and CIFAR-10 we set aside 20% of the training set to use
as a validation set, while for MNIST we set aside the standard 17%. For the imbalanced experiment
we present on MNIST the size of the training dataset we use is always either 24,995 or 25,000,
depending on rounding, with 50 labeled observations. Each class with labels 0-4 has the same
number of unlabeled observations, and same for classes 5-9.

The datasets are transformed prior to usage as follows. Gisette is the scaled version found in the
LibSVM database (Chang and Lin, 2011). MAGIC and MNIST are standardized. For CIFAR-10 we
use the gradient map. As some of our experiments use the version of XSDC that assumes the classes
are balanced, we randomly remove from the MAGIC dataset 4,223 observations in the training set
with label 1.

C.2 XSDC with no labeled data

The XSDC algorithm for the purely unsupervised case is summarized in Algorithm 3. Aside from
removing the supervised initialization step, the only diﬀerence lies in the estimation of ˆYU and
the evaluation of the performance. Speciﬁcally, since we do not have any labeled data with which

27

Algorithm 3 XSDC (when no labeled data is present)

1: Input: Unlabeled data XU
2:

Randomly initialized network parameters V (1)
Number of iterations T

Draw minibatch of samples

←
MatrixBalancing(A(Φ(X (t); V (t))), Y (t)Y (t)T
ULR-SGO step(Φ(X (t); V (t)), M (t), V (t))

)

3:
4: for t = 1, . . . , T do
X (t), Y (t)
5:
M (t)
V (t+1)

←

6:

←

7:
8: end for
9: ˆYU
←
10: ˆW , ˆb
11: Output:

←

SpectralClustering(M (T +1))
RegLeastSquares(X; ˆYU )

ˆYU , V (T ), ˆW , ˆb

to perform nearest neighbor estimation, we instead use spectral clustering. Note that the cluster
numbers output by spectral clustering do not necessarily map to the correct labels (e.g., cluster 0
might correspond to the label 1 rather than 0). Therefore, to evaluate the accuracy of the method
we ﬁnd the optimal relabeling of the classes that aligns with the true labels. We do so by solving a
maximum weight matching problem with the Hungarian algorithm.

C.3 Parameters

The algorithm proposed in this paper and the models used require a large number of parameters to
be set. Next we discuss the choices for these parameters.

Fixed parameters. The parameters that are ﬁxed throughout the experiments and not validated
are as follows. The number of ﬁlters in the networks is set to 32 and the network’s parameters V
are initialized layer-wise with 32 feature maps drawn uniformly at random from the output of the
previous layer. The networks use the Nystr¨om method to approximate the kernel at each layer. The
regularization in the Nystr¨om approximation is set to 0.001, and 20 Newton iterations are used to
compute the inverse square root of the Gram matrix on the parameters V(cid:96) at each layer (cid:96), as in
Jones et al. (2019). The bandwidth is set to the median pairwise distance between the ﬁrst 1000
observations for the single-layer networks. It is set to 0.6 for the convolutional networks. The batch
size for both the labeled and unlabeled data is set to 4096 for Gisette and MAGIC and 1024 for
MNIST and CIFAR-10 (due to GPU memory constraints). The features output by the network φ
are centered and normalized so that on average they have unit (cid:96)2 norm, as in Mairal et al. (2014).
The initial training phase on just the labeled data is performed for 100 iterations, as the validation
loss has typically started leveling oﬀ by 100 iterations. The entropic regularization parameter in
the matrix balancing is set to the median absolute value of the entries in A. If this value results
in divergence of the algorithm, it is multiplied by a factor of two until the algorithm converges.
The value n∆ is set to zero unless otherwise speciﬁed. The number of iterations of alternating
minimization in the matrix balancing algorithm is set to 10. The number of nearest neighbors used
for estimating the labels on the unlabeled data is set to 1.

Hold-out validation. Due to the large number of hyperparameters, we tune them sequentially
as follows when labeled data, and hence a labeled validation set, exists. First, we tune the penalty λ
on the classiﬁer weights over the values 2i for i =
39, . . . , 0. To do so, we train the classiﬁer

40,

−

−

28

−

−

10,

on only the labeled data using the initial random network parameters. We then re-cross-validate
this value every 100 iterations. Next, we tune the learning rate for the labeled data. For the modest
values of α = ρ = 2−4 we validate the ﬁxed learning rate for the labeled data over the values 2i for
i =
9, . . . , 5. To evaluate the performance the labels for the unlabeled data are estimated
using 1-nearest neighbor. The labeled and unlabeled data are then used to train the classiﬁer used
to compute the performance. For the imbalanced experiments on MNIST only we then tune the
minimum and maximum size of the classes over the values 0.01b, 0.02b, . . . , 0.2b, where b is the
batch size (ﬁxing the semi-supervised learning rate to 2−5). For all other experiments we ﬁx these
values to b/k, where k is the number of classes in the dataset. We then tune the semi-supervised
learning rate, again over the values 2i for i =
9, . . . , 5. Following this, we validate ρ over the
10,
values 2i for i =
9, . . . , 10. For the single-layer networks we then tune α over the values 2i
9, . . . , 10. For the convolutional networks we do not penalize the ﬁlters since they are
for i =
constrained to lie on the sphere.

10,

10,

−

−

−

−

−

−

When no labeled data exists we consider the hyperparameters in the same manner as during the
9, . . . , 5 for the semi-supervised
39, . . . , 0 for λ. We then consider
−
9, . . . , 10 for ρ. Finally, if applicable, we consider the values 2i for
9, . . . , 10 for α. We report the best performance observed on the test set. Developing a

hold-out validation. First we consider the values 2i for i =
learning rate. Next we consider the values 2i for i =
the values 2i for i =
i =
method for tuning the hyperparameters on an unlabeled validation set is left for future work.

−
40,

10,

10,

10,

−

−

−

−

−

−

C.4 Training with competing labeling methods

In the comparisons we substitute our matrix balancing method with alternative labeling methods
and retain the remainder of the XSDC algorithm. The pseudo-labeling code is our own, but we used
code from Caron et al. (2018) to implement the k-means version of deep clustering.1 Two important
details regarding the implementations are as follows. First, for pseudo-labeling when some of the
data is labeled we estimate W and b based on the labeled data in the current mini-batch, as that is
what is done in XSDC. When labeled data is not present we estimate W and b based on the cluster
assignments for the entire dataset. Second, for deep clustering we modify the dimension of the
dimensionality reduction. In the original implementation the authors performed PCA, reducing the
dimensionality of the features output by the network to 256. As the features output by the networks
we consider have dimension less than 256, we instead keep the fewest number of components that
account for 95% of the variance.

We perform the parameter tuning as follows. First, we follow the tuning procedure as detailed
in Section C.3. For pseudo-labeling there are no additional parameters to tune. However, for deep
clustering there are two additional parameters to tune: the number of clusters in k-means and
the number of iterations between cluster updates. During the initial parameter tuning stage these
parameters are set to the true number of clusters k, and 50 iterations, respectively. Afterward we
tune these two remaining parameters sequentially. We ﬁrst tune the number of clusters over the
values k, 2k, 4k, 8k, 16k, 32k where k is the true number of clusters. We then tune the number of
iterations between cluster updates over the values 10, 25, 50, 100.

C.5 Additional constraints

In one set of experiments we examine the eﬀect of adding additional constraints. We consider two
4, 9
types of constraints: (1) Constraints based on knowledge of whether the label was in the set
}
{
or not; and (2) Random correct must-link and must-not-link constraints among pairs of unlabeled

1https://github.com/facebookresearch/deepcluster

29

Illustration of the kinds of additional constraints that were added. Green denotes the
Figure 5:
original constraints while purple denotes the constraints that were added. The numbers outside of
the grids denote the true labels.

observations and random correct must-not-link constraints between pairs of unlabeled and labeled
observations.

The two types of constraints are illustrated in Figure 5. Each grid point (i, j), if ﬁlled, denotes
whether observations i and j have the same label (1) or not (0). The true labels are the values
outside of the grids. Green backgrounds correspond to knowing the labels corresponding to (i, j).
Purple backgrounds denote the additional known constraints. The left-most panel gives an example
of an initial matrix M in which the labels corresponding to the ﬁrst two observations are known (0
and 9). The second panel shows the entries we can ﬁll in once we know whether each observation
belongs to the set
. Finally, the third panel shows random correct constraints. The constraint
at entry (2, 3) is a must-not-link constraint, whereas the constraint at entry (3, 4) is a must-link
constraint.

4, 9
}
{

D Additional Experimental Results

In this section we present additional experimental results that expand upon those presented in
Section 4.

Visualization of feature representations. Figure 6 visualizes the feature representations of
unlabeled MNIST digits at diﬀerent points in the training process. In each case the features were
projected to 2-D with the Scikit-Learn (Pedregosa et al., 2011) implementation of t-SNE (Van Der
Maaten and Hinton, 2008). Based on these t-SNE representations, the plots were then produced.
The code to produce the plots was adapted from Andrej Karpathy’s Matlab code.2 It ﬁrst scales
the t-SNE representations so they all lie in [0, 1]2 and creates an evenly-spaced 40
40 grid over
[0, 1]2. Then, for each square in the grid, it checks whether any image’s t-SNE representation lies in
that square. If any such images exist, it chooses one at random and displays the original image in
that square. The images are then color-coded according to the ground-truth labels.

×

The batch size used to produce the t-SNE representations and the corresponding plots was 4096.
The default parameters were used for generating the t-SNE representations. We created each plot
with 20 random initializations and present the ones that visually appeared to be the best.

Figure 6a visualizes the raw digits. By “raw” we mean that the digits were not input into the
network; they were only standardized before applying t-SNE. Comparing this ﬁgure with Figure 6b,
which visualizes the feature representations from the network with randomly initialized ﬁlters, we
can see that in both plots the 4’s and 9’s are mixed together. Furthermore, the 5’s are split into two

2https://cs.stanford.edu/people/karpathy/cnnembed/

30

Originalconstraints{4,9}constraintsRandomconstraintsM=09440100019141410M=094401000001090104010400010M=09440100010901104114010parts in Figure 6a. Figure 6c depicts the feature representations after the supervised initialization.
Figure 6c improves upon 6b because the 4’s and 9’s are better-separated. However, the 5’s have
once again been split into two parts. Finally, Figure 6d depicts the features representations after
running XSDC. Comparing Figures 6c and 6d, we can see that XSDC tends to separate the clusters
relative to the supervised initialization. While the 5’s and 8’s are generally all in one cluster after
running XSDC, the 4’s, 7’s, and 9’s are a bit less separated.

4, 9
}
{

Eﬀect of additional constraints. Next, Figure 7 compares the test accuracy of the LeNet-5
CKN on MNIST when adding additional constraints. As explained in Section 4, we consider two
types of additional constraints. The ﬁrst type of constraints are generated based on knowledge of
whether each unlabeled point has a label in the set
or not. The second type of constraints
is randomly generated constraints with approximately the same number of known entries in the
adjacency matrix M as for the ﬁrst type. We can see that both types of additional constraints
generally improve the performance of the method. The largest gap occurs when there are 50
labeled observations, as expected. There the accuracies on MNIST when using the additional
4, 9
}
{
constraints and the random constraints are 1% better and 5% better, respectively, than without
them. This gap shrinks to 0.03% and 0.8% at 500 observations. As noted in Section 4, it is likely
that the better performance with the random constraints than with the
constraints is because
the former provide more information to be able to distinguish between labels for often-confused
classes. Note that the performance with the
constraints at 150 labels is worse than without
4, 9
the constraints. This is probably because the hold-out validation overﬁt on the validation set.

4, 9
}

}

{

{

Eﬀect of unbalanced unlabeled data. Figure 7b displays the accuracy of the LeNet-5 CKN
on MNIST when 50 observations are labeled and the unlabeled data is unbalanced. To make the
unlabeled data unbalanced we vary the fraction of the labels
and the fraction of the
labels
. Within each category 0-4 all of the labels are equally represented, and similarly
}
for 5-9. The imbalance parameter in the plot denotes the fraction of the labels that are from the set

5, 6, 7, 8, 9
{

0, 1, 2, 3, 4

}

{

0, 1, 2, 3, 4
}

.

{

The performance is generally better when the data is closer to being balanced, as expected. The
only time training with the unbalanced data is signiﬁcantly worse than training on only the labeled
data is when 95% of the unlabeled data is comprised of images of the digits 0-4. At that point the
accuracy is 5% below the purely supervised performance.

Sensitivity analysis. Finally, we perform a sensitivity analysis of the hyperparameters that are
tuned via cross-validation. To be consistent with the other experiments, the setting we choose is
when training a LeNet-5 CKN on MNIST with 50 labels. For this architecture the parameters
we cross-validate over are the learning rates for the supervised initialization and semi-supervised
learning and the penalties on the centered features and classiﬁer weights. Here we vary one parameter
at a time, ﬁxing all others to their values obtained from cross-validation (which were 2−7 and 2−6 for
the learning rates of the supervised initialization and semi-supervised learning, respectively, and 2−5
for the penalty on the centered features. The penalty on the classiﬁer weights was cross-validated
every 100 iterations.). We see that the most important parameter to tune is the semi-supervised
learning rate. The other parameters only have a noticeable detrimental eﬀect if they are too large.

31

(a) Raw

(b) Unsupervised initialization

(c) Supervised initialization

(d) XSDC

Figure 6: Visualizations of the unlabeled MNIST features obtained when training the LeNet-5
CKN with 50 labeled observations (where applicable). The CKN features were projected to 2-D
using t-SNE. The features were obtained at diﬀerent stages, as indicated in the sub-captions.

32

(a) Eﬀect of additional constraints

(b) Eﬀect of unbalanced data

Figure 7: Average accuracy across 10 trials of XSDC after training a LeNet-5 CKN on MNIST
when adding additional constraints and varying the fraction of labeled data (left) and when varying
constraints” in (a) are derived
the balance of the unlabeled data (right). The “additional
. The imbalance
from knowledge of whether the label for each unlabeled point lies in the set
parameter in (b) denotes the fraction of the labels that are from the set
. All classes in
the set

are equally represented, and similarly for

4,9
{
}
0, 1, 2, 3, 4
}

4,9
}

{

.

{
5, 6, 7, 8, 9
}

{

0, 1, 2, 3, 4
}

{

33

Figure 8: Sensitivity analysis of the hyperparameters tuned during cross-validation when using
XSDC to train a LeNet-5 CKN on MNIST with 50 labeled observations.

34

