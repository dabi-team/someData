0
2
0
2

b
e
F
7
1

]

Y
S
.
s
s
e
e
[

5
v
9
3
2
7
0
.
9
0
9
1
:
v
i
X
r
a

Received: Added at production

Revised: Added at production

Accepted: Added at production

DOI: xxx/xxxx

RESEARCH ARTICLE

Adaptive Dynamic Programming for Model-free Tracking of
Trajectories with Time-varying Parameters†

Florian Köpf | Simon Ramsteiner | Michael Flad | Sören Hohmann

Institute of Control Systems, Karlsruhe
Institute of Technology (KIT), Karlsruhe,
Germany

Correspondence
Florian Köpf, Institute of Control Systems,
Karlsruhe Institute of Technology (KIT),
Kaiserstr. 12, 76131 Karlsruhe, Germany.
Email: ﬂorian.koepf@kit.edu

Summary

In order to autonomously learn to control unknown systems optimally w.r.t. an
objective function, Adaptive Dynamic Programming (ADP) is well-suited to adapt

controllers based on experience from interaction with the system. In recent years,
many researchers focused on the tracking case, where the aim is to follow a desired

trajectory. So far, ADP tracking controllers assume that the reference trajectory fol-
lows time-invariant exo-system dynamics—an assumption that does not hold for

many applications. In order to overcome this limitation, we propose a new Q-function
which explicitly incorporates a parametrized approximation of the reference tra-

jectory. This allows to learn to track a general class of trajectories by means of
ADP. Once our Q-function has been learned, the associated controller copes with

time-varying reference trajectories without need of further training and independent
of exo-system dynamics. After proposing our general model-free oﬀ-policy track-

ing method, we provide analysis of the important special case of linear quadratic

tracking. We conclude our paper with an example which demonstrates that our new
method successfully learns the optimal tracking controller and outperforms existing

approaches in terms of tracking error and cost.

KEYWORDS:
Adaptive Dynamic Programming, Optimal Tracking, Optimal Control, Reinforcement Learning

1

INTRODUCTION

Adaptive Dynamic Programming (ADP) which is based on Reinforcement Learning has gained extensive attention as a model-
free adaptive optimal control method. 1 In ADP, pursuing the objective to minimize a cost functional, the controller adapts its
behavior on the basis of interaction with an unknown system. The present work focuses on the ADP tracking case, where a
reference trajectory is intended to be followed while the system dynamics is unknown. As the long-term cost, i.e. value, of a
state changes depending on the reference trajectory, a controller that has learned to solve a regulation problem cannot be directly
transferred to the tracking case.

Therefore, in literature, there are several ADP tracking approaches in discrete time 2,3,4,5 and continuous time. 6,7 All of these
methods assume that the reference trajectory 𝒓𝑘 can be modeled by means of a time-invariant exo-system 𝒓𝑘+1 = 𝒇 ref(𝒓𝑘) (and
̇𝒓(𝑡) = 𝒇 ref(𝒓(𝑡)), respectively). Then, an approximated value function (or Q-function) is learned in order to rate diﬀerent states (or
state-action combinations) w.r.t their expected long-term cost. Based on this information, approximated optimal control laws are

†This is a preprint submitted to the Int J Adapt Control Signal Process. The substantially revised version will be published in the Int J Adapt Control Signal Process

(DOI: 10.1002/ACS.3106).

 
 
 
 
 
 
2

Köpf ET AL.

derived. Whenever this reference trajectory and thus the function 𝒇 ref changes, the learned value function and consequently the
controller is not valid anymore and needs to be re-trained. Therefore, the exo-system tracking case with time-invariant reference
dynamics 𝒇 ref is not suited for all applications. 8 For example in autonomous driving, process engineering and human-machine
collaboration, it is often required to track ﬂexible and time-varying trajectories. In order to account for various references, the
multiple-model approach presented by Kiumarsi et al. 9 uses a self-organizing map that switches between several learned models.
However, in their approach, new sub-models need to be trained for each exo-system 𝒇 ref.

Thus, our idea is to deﬁne a state-action-reference Q-function that explicitly incorporates the course of the reference trajectory
in contrast to the commonly used Q-function (see e.g. Sutton and Barto 10) which only depends on the current state 𝒙𝑘 and
control 𝒖𝑘. This general idea has ﬁrst been proposed in our previous work, 11 where the reference 𝒓𝑘 is given on a ﬁnite horizon
and assumed to be zero thereafter. Thus, the number of weights to be learned depends on the horizon on which the reference
trajectory is considered. As the reference trajectory is given for each time step, this allows high ﬂexibility, but the sampling time
and (unknown) system dynamics signiﬁcantly inﬂuence the reasonable horizon length and thus the number of weights to be
learned. Based on these challenges, our major idea and contribution in the present work is to approximate the reference trajectory
by means of a potentially time-varying parameter set 𝑷 𝑘 in order to compress the information about the reference compared to
our previous work 11 and incorporate this parameter into a new Q-function. In doing so, the Q-function explicitly represents the
dependency of the expected long-term cost on the desired reference trajectory. Hence, the associated optimal controller is able
to cope with time-varying parametrized references. We term this method Parametrized Reference ADP (PRADP).

Our main contributions include:
• The introduction of a new reference-dependent Q-function that explicitly depends on the reference-parameter 𝑷 𝑘.
• Function approximation of this Q-function in order to realize Temporal Diﬀerence (TD) learning (cf. Sutton 12).

• Rigorous analysis of the form of this Q-function and its associated optimal control law in the special case of linear-

quadratic (LQ) tracking.

• A comparison of our proposed method with algorithms assuming a time-invariant exo-system 𝒇 ref and the ground truth

optimal tracking controller.

In the next section, the general problem deﬁnition is given. Then, PRADP is proposed in Section 3. Simulation results and a
discussion are given in Section 4 before the paper is concluded.

2 GENERAL PROBLEM DEFINITION

Consider a discrete-time controllable system

(1)
)
0 is the discrete time step, 𝒙𝑘 ∈ ℝ𝑛 the system state and 𝒖𝑘 ∈ ℝ𝑚 the input. The system dynamics 𝒇 (⋅) is assumed

𝒙𝑘, 𝒖𝑘
(

𝒙𝑘+1 = 𝒇

,

where 𝑘 ∈ ℕ
to be unknown.

Furthermore, let the parametrized reference trajectory 𝒓(𝑷 𝑘, 𝑖) ∈ ℝ𝑛 which we intend to follow be described by

𝒑⊺
𝑘,1
𝒑⊺
𝑘,2
𝒓(𝑷 𝑘, 𝑖) = 𝑷 𝑘𝝆(𝑖) = ⎡
⋮
⎢
𝒑⊺
⎢
𝑘,𝑛
⎢
⎢
⎣

⎤
⎥
⎥
⎥
At any time step 𝑘, the reference trajectory is described by means of a parameter matrix 𝑷 𝑘 ∈ ℝ𝑛×𝑝 and given basis functions
⎥
⎦
𝝆(𝑖) ∈ ℝ𝑝. Here, 𝑖 ∈ ℕ
0 denotes the time step on the reference from the local perspective at time 𝑘, i.e. for 𝑖 = 0, the reference at
time step 𝑘 results and 𝑖 > 0 yields a prediction of the reference for future time steps. Thus, in contrast to methods which assume
that the reference follows time-invariant exo-system dynamics 𝒇 ref, the parameters 𝑷 𝑘 in (2) can be time-varying, allowing much
more diverse reference trajectories.

𝝆(𝑖).

(2)

Our aim is to learn a controller which does not know the system dynamics and minimizes the cost

𝐽𝑘 =

∞

∑𝑖=0

𝛾 𝑖𝑐(𝒙𝑘+𝑖, 𝒖𝑘+𝑖, 𝒓(𝑷 𝑘, 𝑖)),

(3)

Köpf ET AL.

3

where 𝛾 ∈ [0, 1) is a discount factor and 𝑐(⋅) denotes a non-negative one-step cost. We deﬁne our general problem as follows.

Problem 1. For a given parametrization of the reference by means of 𝑷 𝑘 according to (2), an optimal control sequence that
minimizes the cost (3) is denoted by 𝒖∗
𝑘 . The system dynamics is unknown. At each time
step 𝑘, ﬁnd 𝒖∗
𝑘.

𝑘+1, … and the associated cost by 𝐽 ∗

𝑘, 𝒖∗

3

PARAMETRIZED REFERENCE ADP (PRADP)

In order to solve Problem 1, we ﬁrst propose a new, modiﬁed Q-function whose minimizing control represents a solution 𝒖∗
𝑘 to
Problem 1. In the next step, we parametrize this Q-function by means of linear function approximation. Then, we apply Least-
Squares Policy Iteration (LSPI) (cf. Lagoudakis and Parr 13) in order to learn the unknown Q-function weights from data without
requiring a system model. Finally, we discuss the structure of this new Q-function for the linear-quadratic tracking problem,
where analytical insights are possible.

3.1

Proposed Q-Function

The relative position 𝑖 on the current reference trajectory that is parametrized by means of 𝑷 𝑘 according to (2) needs to be
considered when minimizing the cost 𝐽𝑘 as given in (3). In order to do so, one could explicitly incorporate the relative time
𝑖 into the Q-function that is used for ADP. This would yield a Q-function of the form 𝑄(𝒙𝑘, 𝒖𝑘, 𝑷 𝑘, 𝑖). However, this would
unnecessarily increase the complexity of the Q-function and hence the challenge to approximate and learn such a Q-function.
Thus, we decided to implicitly incorporate the relative time 𝑖 on the current reference trajectory parametrized by 𝑷 𝑘 into the
reference trajectory parametrization. This yields a shifted parameter matrix 𝑷 (𝑖)

𝑘 according to the following deﬁnition.

Deﬁnition 1. (Shifted Parameter Matrix 𝑷 (𝑖)

𝑘 be deﬁned such that

𝑘 ) Let the matrix 𝑷 (𝑖)
𝑷 (𝑖)
(
⇔ 𝑷 (𝑖)

𝒓

= 𝒓(𝑷 𝑘, 𝑖 + 𝑗)

𝑘 , 𝑗
𝑘 𝝆(𝑗) = 𝑷 𝑘𝝆(𝑖 + 𝑗).

)

Thus,

𝑷 (𝑖)

𝑘 = 𝑷 𝑘𝑻 (𝑖)

(4a)

(4b)

(5)

is a modiﬁed version of 𝑷 𝑘 = 𝑷 (0)
𝑘 such that the associated reference trajectory is shifted by 𝑖 time steps, where 𝑻 (𝑖) is a suitable
matrix. Note that 𝑻 (𝑖) is in general ambiguous as in the general case 𝑝 > 1 the system of equations (4b) in order to solve for 𝑷 (𝑖)
𝑘
is underdetermined. Thus, 𝑻 (𝑖) can be any matrix such that (4) holds.

Our proposed Q-function which explicitly incorporates the reference trajectory by means of 𝑷 𝑘 is given as follows.

Deﬁnition 2. (Parametrized Reference Q-Function) Let

𝑄𝝅

𝒙𝑘, 𝒖𝑘, 𝑷 𝑘

= 𝑐

(

)

= 𝑐

∞

+

𝛾 𝑖𝑐

)

∑𝑖=1
+ 𝛾𝑄𝝅

𝒙𝑘, 𝒖𝑘, 𝒓(𝑷 𝑘, 0)
(
𝒙𝑘, 𝒖𝑘, 𝒓(𝑷 𝑘, 0)
(

)

𝒙𝑘+𝑖, 𝝅
(

𝒙𝑘+1, 𝝅
(

𝑘

𝒙𝑘+𝑖, 𝑷 (𝑖)
)
(
𝒙𝑘+1, 𝑷 (1)
(

𝑘

)

, 𝑷 (1)
𝑘

.

)

, 𝑟(𝑷 𝑘, 𝑖)
)

(6)

Here, 𝝅 ∶ ℝ𝑛 × ℝ𝑛×𝑝 → ℝ𝑚 denotes the current control policy.
Therefore, 𝑄𝝅(𝒙𝑘, 𝒖𝑘, 𝑷 𝑘) represents the accumulated discounted cost if the system is in state 𝒙𝑘, the control 𝒖𝑘 is applied at
time 𝑘 and the policy 𝝅(⋅) is followed thereafter while the reference trajectory is parametrized by 𝑷 𝑘. Based on (6), the optimal
Q-function 𝑄∗(⋅) is given by

𝑄∗

= 𝑐

𝒙𝑘, 𝒖𝑘, 𝑷 𝑘

𝒙𝑘+1, 𝝅
(
𝒙𝑘+1, 𝝅∗
(
Here, the optimal control policy is denoted by 𝝅∗(⋅), hence 𝝅∗(𝒙𝑘+1, 𝑷 (1)
𝑘 ) = 𝒖∗
as can be seen from the following Lemma.

𝒙𝑘, 𝒖𝑘, 𝒓(𝑷 𝑘, 0)
(
𝒙𝑘, 𝒖𝑘, 𝒓(𝑷 𝑘, 0)
(

+ min
𝝅
+ 𝛾𝑄∗

= 𝑐

)

)

)

(

𝛾𝑄𝝅

𝒙𝑘+1, 𝑷 (1)
(
𝒙𝑘+1, 𝑷 (1)
(

, 𝑷 (1)
𝑘
𝑘
)
, 𝑷 (1)
𝑘
)

)

𝑘

.

𝑘+1. This Q-function is useful for solving Problem 1

)

(7)

4

Köpf ET AL.

Lemma 1. The control 𝒖𝑘 minimizing 𝑄∗

Proof. With (7)

𝒙𝑘, 𝒖𝑘, 𝑷 𝑘
(

)

is a solution for 𝒖∗

𝑘 minimizing 𝐽𝑘 in (3) according to Problem 1.

𝑄∗

min
𝒖𝑘

(

𝒙𝑘, 𝒖𝑘, 𝑷 𝑘

= 𝑐

𝒙𝑘, 𝒖∗

+ 𝛾𝑄∗

𝒙𝑘+1, 𝒖∗
(

𝑘+1, 𝑷 (1)

𝑘

𝑘, 𝒓(𝑷 𝑘, 0)
∞

𝛾 𝑖𝑐

∑𝑖=0

)
𝒙𝑖, 𝒖𝑖, 𝒓(𝑷 𝑘, 𝑖)
(

)

)

(
= min

𝒖𝑘,𝒖𝑘+1,…

= 𝐽 ∗
𝑘

follows, which completes the proof.

Thus, if the Q-function 𝑄∗(𝒙𝑘, 𝒖𝑘, 𝑷 𝑘) is known, the desired optimal control 𝒖𝑘 is given by

)

(8)

(9)

Lemma 1 and (9) reveal the usefulness of 𝑄∗

for solving Problem 1. Thus, we express this Q-function by means
of linear function approximation in the following. Based on the temporal-diﬀerence (TD) error, the unknown Q-function weights
can then be estimated.

(

)

𝒖∗
𝑘 = arg min

𝑄∗(𝒙𝑘, 𝒖𝑘, 𝑷 𝑘).

𝒖𝑘
𝒙𝑘, 𝒖𝑘, 𝑷 𝑘

3.2

Function Approximation of the Tracking Q-Function

As classical tabular Q-learning is unable to cope with large or even continuous state and control spaces, it is common to represent
the Q-function, which is assumed to be smooth, by means of function approximation 14. This leads to

𝑄∗

𝒙𝑘, 𝒖𝑘, 𝑷 𝑘

(10)
Here, 𝒘 ∈ ℝ𝑞 is the unknown optimal weight vector, 𝝓 ∈ ℝ𝑞 a vector of activation functions and 𝜖 the approximation error.
According to the Weierstrass higher-order approximation Theorem 15 a single hidden layer and appropriately smooth hidden
layer activation functions 𝝓(⋅) are capable of an arbitrarily accurate approximation of the Q-function. Furthermore, if 𝑞 → ∞,
𝜖 → 0.

𝒙𝑘, 𝒖𝑘, 𝑷 𝑘

𝒙𝑘, 𝒖𝑘, 𝑷 𝑘

+ 𝜖

)

)

(

)

(

(

.

= 𝒘⊺𝝓

As 𝒘 is a priori unknown, let the estimated optimal Q-function be given by

̂𝑄∗

𝒙𝑘, 𝒖𝑘, 𝑷 𝑘

= ̂𝒘⊺𝝓

𝒙𝑘, 𝒖𝑘, 𝑷 𝑘

.

In analogy to (9), the estimated optimal control law is deﬁned as
̂𝝅∗(𝒙𝑘, 𝑷 𝑘) = arg min

(

)

𝒖𝑘

(

)

̂𝑄∗

𝒙𝑘, 𝒖𝑘, 𝑷 𝑘

.

(

)

Based on this parametrization of our new Q-function, the associated TD error 12 is deﬁned as follows.

(11)

(12)

Deﬁnition 3. (TD Error of the Tracking Q-Function) The TD error which results from using the estimated Q-function ̂𝑄∗(⋅)
(11) in the Bellman-like equation (7) is deﬁned as

𝛿𝑘 = 𝑐

𝒙𝑘, 𝒖𝑘, 𝒓(𝑷 𝑘, 0)

(

= 𝑐

𝒙𝑘, 𝒖𝑘, 𝒓(𝑷 𝑘, 0)

)

(

+ 𝛾 ̂𝑄∗

𝒙𝑘+1, ̂𝝅∗
(
+ 𝛾 ̂𝒘⊺𝝓

𝒙𝑘+1, 𝑷 (1)
)
(
𝒙𝑘+1, 𝑷 (1)
𝒙𝑘+1, ̂𝝅∗
(

(

𝑘

𝑘

, 𝑷 (1)
𝑘
)
, 𝑷 (1)
𝑘
)

− ̂𝑄∗

𝒙𝑘, 𝒖𝑘, 𝑷 𝑘

(
− ̂𝒘⊺𝝓

)

)

𝒙𝑘, 𝒖𝑘, 𝑷 𝑘
(

)

)

.

(13)

Our goal is to estimate ̂𝒘 in order to minimize the squared TD error 𝛿2

𝑘 as the TD error quantiﬁes the quality of the Q-function

approximation. However, (13) is scalar while 𝑞 weights need to be estimated. Thus, we utilize 𝑁 ≥ 𝑞 tuples
𝑘, ̂𝑄∗+

, 𝑘 = 1, … , 𝑁,

𝑘 =

𝑘

𝑐𝑘, ̂𝑄∗
{

}

𝒙𝑘, 𝒖𝑘, 𝒓(𝑷 𝑘, 0)

𝑐𝑘 = 𝑐
𝑘 = ̂𝒘⊺𝝓𝑘 = ̂𝒘⊺𝝓
̂𝑄∗

(

,
𝒙𝑘, 𝒖𝑘, 𝑷 𝑘
(

)

)

̂𝑄∗+

𝑘 = ̂𝒘⊺𝝓+

𝑘 = ̂𝒘⊺𝝓

𝒙𝑘+1, ̂𝝅∗
(

𝒙𝑘+1, 𝑷 (1)
(

𝑘

, 𝑷 (1)
𝑘
)

)

(14)

where

and

Köpf ET AL.

5

from interaction with the system in order to estimate ̂𝒘 using Least-Squares Policy Iteration (LSPI) (cf. Lagoudakis and Parr 13).
Stacking (13) for the tuples 𝑘, 𝑘 = 1, … , 𝑁, yields

=

𝛿1
⋮
⎡
⎤
𝛿𝑁
⎢
⎥
⏟⏟⏟
⎢
⎥
⎣
⎦
𝜹

+

𝑐1
⋮
⎡
⎤
𝑐𝑁
⎢
⎥
⏟⏟⏟
⎢
⎥
⎣
⎦
𝒄

𝝓+⊺
1
⋮
⎡
𝝓+⊺
⎢
𝑁
⎢
⎣

𝛾

−

⎛
⎤
⎞
⎤
⎜
⎥
⎟
⎥
⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟
⎜
⎥
⎟
⎥
⎦
⎦
⎝
⎠
𝚽

𝝓⊺
1
⋮
⎡
𝝓⊺
⎢
𝑁
⎢
⎣

If the excitation condition

holds, ̂𝒘 minimizing 𝜹⊺𝜹 exists, is unique and given by

according to Åström and Wittenmark, Theorem 2.1. 16

(

)

̂𝒘 =

𝚽⊺𝚽

−1 𝚽⊺𝒄

rank 𝚽⊺𝚽 = 𝑞

̂𝒘.

(15)

(16)

(17)

Note 1. Using 𝑷 (1)
combination with (1)) that the Markov property holds, which is commonly required in ADP. 1

𝑘 = 𝑷 𝑘𝑻 (1) (5) in the training tuple 𝑘 (14) rather than an arbitrary subsequent 𝑷 𝑘+1 guarantees (in

Remark 1. The procedure described above is an extension of Lagoudakis and Parr, Section 5.1 13 to the tracking case where
minimizing the squared TD error is targeted. In addition, an alternative projection method described by Lagoudakis and Parr,
Section 5.2 13 which targets the approximate Q-function to be a ﬁxed point under the Bellman operator has been implemented.
Both procedures yielded indistinguishable results for our linear-quadratic simulation examples but might be diﬀerent in the
general case.

Note that ̂𝝅∗(⋅) in ̂𝑄∗+

𝑘 depends on ̂𝒘, i.e. the estimation of ̂𝑄∗+

𝑘 depends on another estimation (of the optimal control law).
This mechanism is known as bootstrapping (cf. Sutton and Barto 10) in Reinforcement Learning. As a consequence, rather than
estimating ̂𝒘 once by means of the least-squares estimate (17), a policy iteration is performed starting with ̂𝒘(0). This procedure
is given in Algorithm 1, where 𝑒 ̂𝒘 is a threshold for the terminal condition.
Note 2. Due to the use of a Q-function which explicitly depends on the control 𝒖𝑘, this method performs oﬀ-policy learning. 10
Thus, during training, the behavior policy (i.e. 𝒖𝑘 which is actually applied to the system) might include exploration noise in order
to satisfy the rank condition (16) but due to the greedy target policy ̂𝝅∗ (cf. the policy improvement step (12)), the Q-function
associated with the optimal control law is learned.

With ̂𝑄(𝑖)(⋅) = ̂𝒘(𝑖)⊺𝝓(⋅) and 𝑄 ̂𝝅(𝑖)

according to (6) with 𝝅 = ̂𝝅(𝑖), the following convergence properties also hold for our

tracking Q-function.

Theorem 1. (Convergence Properties of the Q-function, cf. Lagoudakis and Parr, Theorem 7.1 13) Let ̄𝜖 ≥ 0 bound the errors
between the approximate Q-function ̂𝑄(𝑖) and true Q-function 𝑄 ̂𝝅(𝑖)

associated with ̂𝝅(𝑖) over all iterations, i.e.

̂𝑄(𝑖) − 𝑄 ̂𝝅(𝑖)

∞

≤ ̄𝜖, ∀𝑖 = 1, 2, … .

‖
Then, Algorithm 1 yields control laws such that
‖
‖
lim sup
𝑖→∞

‖
‖
‖
̂𝑄(𝑖) − 𝑄∗

‖
‖
‖

‖
‖
‖

≤

∞

2𝛾 ̄𝜖
(1 − 𝛾)2

.

Algorithm 1 PRADP based on LSPI
1: initialize 𝑖 = 0, ̂𝒘(0)
2: do

(18)

(19)

policy evaluation: calculate ̂𝒘(𝑖+1) according to (17), where ̂𝒘 = ̂𝒘(𝑖+1)
policy improvement: obtain ̂𝝅(𝑖+1) from (12)
𝑖 = 𝑖 + 1

3:

4:

5:
6: while

̂𝒘(𝑖) − ̂𝒘(𝑖−1)

‖
‖
‖

> 𝑒 ̂𝒘

2
‖
‖
‖

6

Köpf ET AL.

Proof. The proof is adapted from Bertsekas and Tsitsiklis, Proposition 6.2 17.

Lagoudakis and Parr 13 point out that the appropriate choice of basis functions and the sample distribution (i.e. excitation)
determine ̄𝜖. According to Theorem 1, Algorithm 1 converges to a neighborhood of the optimal tracking Q-function under
appropriate choice of basis functions 𝝓(⋅) and excitation. However, for general nonlinear systems (1) and cost functions (3), an
appropriate choice of basis functions and the number of neurons is “more of an art than science” 18 and still an open problem.
As the focus of this paper lies on the new Q-function for tracking purposes rather than tuning of neural networks, we focus on
linear systems and quadratic cost functions in the following—a setting that plays an important role in control engineering. This
allows analytic insights into the structure of 𝑄∗
and thus proper choice of 𝝓(⋅) for function approximation in order
to demonstrate the eﬀectiveness of the proposed PRADP method.

𝒙𝑘, 𝒖𝑘, 𝑷 𝑘
(

)

3.3

The LQ-Tracking Case

In the following, assume

and

𝒙𝑘+1 = 𝑨𝒙𝑘 + 𝑩𝒖𝑘,

∞

𝐽𝑘 =

𝛾 𝑖

𝒙𝑘+𝑖 − 𝒓(𝑷 𝑘, 𝑖)

∑𝑖=0
∞

[(

⊺

𝑸

)

𝒙𝑘+𝑖 − 𝒓(𝑷 𝑘, 𝑖)
(

)

+𝒖⊺

𝑘+𝑖𝑹𝒖𝑘+𝑖

]

=∶

𝛾 𝑖

∑𝑖=0

𝑘,𝑖𝑸𝒆𝑘,𝑖 + 𝒖⊺
𝒆⊺
[

𝑘+𝑖𝑹𝒖𝑘+𝑖

.

]

(20)

(21)

Here, 𝑸 penalizes the deviation of the state 𝒙𝑘+𝑖 from the reference 𝒓(𝑷 𝑘, 𝑖) and 𝑹 penalizes the control eﬀort. Furthermore, let
the following assumptions hold.

Assumption 1. Let 𝑸 = 𝑸⊺ ⪰ 𝟎, 𝑹 = 𝑹⊺ ≻ 𝟎, (𝑨, 𝑩) be controllable and (𝑪, 𝑨) be detectable, where 𝑪 ⊺𝑪 = 𝑸.
Assumption 2. Let the matrix 𝑻 (𝑖) which deﬁnes the shifted parameter matrix 𝑷 (𝑖)
∀𝑗 = 1, … , 𝑝, holds, where 𝜆𝑗 are the eigenvalues of

𝑘 according to (5) be such that

𝛾𝑻 (1).

Note 3. Assumption 1 is rather standard in the LQ setting in order to ensure the existence and uniqueness of a stabilizing solution
to the discrete-time algebraic Riccati equation associated with the regulation problem given by (20) and (21) for 𝑷 𝑘 = 𝟎 (cf.
Kučera, Theorem 8 19). Furthermore, it is obvious that the reference trajectory 𝒓(𝑷 𝑘, 𝑖) must be deﬁned such that a controller
exists which yields ﬁnite cost 𝐽𝑘 in order to obtain a reasonable control problem. As will be seen in Theorem 2, Assumption 2
guarantees the existence of this solution.

√

𝜆𝑗

< 1,

|
|
|

|
|
|

The tracking error 𝒆𝑘,𝑖 can be expressed as

𝒆𝑘,𝑖 = 𝒙𝑘+𝑖 − 𝒓(𝑷 𝑘, 𝑖) = 𝒙𝑘+𝑖 − 𝑷 (𝑖)

𝑘 𝝆(0) =

⊺

−𝝆(0) ⋯ 𝟎
⋮ ⋱ ⋮
𝟎 ⋯ −𝝆(0)⎤
⎥
⎥
⎦

𝑰 𝑛
⎡
⎤
⎢
⎥
⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟
⎢
⎥
⎣
⎦
=∶𝑴

⎡
⎢
⎢
⎣

,

𝒙𝑘+𝑖
𝒑(𝑖)
𝑘,1
⎡
⎤
⋮
⎢
⎥
𝒑(𝑖)
⎢
⎥
𝑘,𝑛
⎢
⎥
⏟⏟⏟
⎢
⎥
⎣
⎦
=∶𝒚𝑘,𝑖

(22)

𝑖 = 0, 1, … , where 𝑰 𝑛 denotes the 𝑛 × 𝑛 identity matrix and 𝒚𝑘,𝑖 the extended state. The associated optimal controller is given in
the following Theorem.

Theorem 2. (Optimal Tracking Control Law) Let a reference (2) with shift matrix 𝑻 (𝑖) as in Deﬁnition 1 be given. Then,

(i) the optimal controller which minimizes (21) subject to the system dynamics (20) and the reference is linear w.r.t. 𝒚𝑘,𝑖 (cf.

(22)) and can be stated as

𝑖 = 0, 1, … . Here, the optimal gain 𝑳 is given by

𝑳 = (𝛾 ̃𝑩⊺ ̃𝑺 ̃𝑩 + 𝑹)−1𝛾 ̃𝑩⊺ ̃𝑺 ̃𝑨,

𝝅∗(𝒙𝑘+𝑖, 𝑷 (𝑖)

𝑘 ) = 𝒖∗

𝑘+𝑖 = −𝑳𝒚𝑘,𝑖,

(23)

(24)

(27)

(28)

Köpf ET AL.

where

7

(25)

,

𝑨 𝟎 ⋯ 𝟎
𝟎 𝑻 (1)⊺ ⋯ 𝟎
⋮ ⋱ ⋮
⋮
𝟎 ⋯ 𝑻 (1)⊺
𝟎

𝑩
𝟎
⋮
𝟎

⎤
⎥
⎥
⎥
⎥
⎦

̃𝑩 = ⎡
⎢
⎢
⎢
⎢
⎣

,
⎤
⎥
⎥
⎥
⎥
⎦

̃𝑨 = ⎡
⎢
⎢
⎢
⎢
⎣

̃𝑨 ∈ ℝ𝑛(𝑝+1)×𝑛(𝑝+1), ̃𝑩 ∈ ℝ𝑛(𝑝+1)×𝑚, ̃𝑸 = 𝑴 ⊺𝑸𝑴 and ̃𝑺 denotes the solution of the discrete-time algebraic Riccati equation
̃𝑺 = 𝛾 ̃𝑨⊺ ̃𝑺 ̃𝑨 − 𝛾 ̃𝑨⊺ ̃𝑺 ̃𝑩(𝑹 + ̃𝑩⊺ ̃𝑺 ̃𝑩)−1 ̃𝑩⊺ ̃𝑺 ̃𝑨 + ̃𝑸.

(26)

(ii) Furthermore, under Assumptions 1–2, the optimal controller 𝝅∗(𝒙𝑘+𝑖, 𝑷 (𝑖)

𝑘 ) exists and is unique.

Proof. (i) With (22), the discounted cost (21) can be reformulated as

Furthermore, note that with (20) and (5)

𝐽𝑘 =

∞

∑𝑖=0

𝛾 𝑖

𝒚𝑘,𝑖

⊺𝑴 ⊺𝑸𝑴 𝒚𝑘,𝑖 + 𝒖⊺

𝑘+𝑖𝑹𝒖𝑘+𝑖

[

.

]

𝑨𝒙𝑘+𝑖 + 𝑩𝒖𝑘+𝑖
𝑻 (1)⊺𝒑(𝑖)
𝑘,1
⋮
𝑻 (1)⊺𝒑(𝑖)
𝑘,𝑛

⎤
⎥
⎥
⎥
⎥
⎦

𝒚𝑘,𝑖+1 = ⎡
⎢
⎢
⎢
⎢
⎣

= ̃𝑨𝒚𝑘,𝑖 + ̃𝑩𝒖𝑘+𝑖

holds. With 𝛾, ̃𝑨, ̃𝑩, ̃𝑸 and 𝑹, a standard discounted LQ regulation problem results from (27) for the extended state 𝒚𝑘,𝑖.
𝛾 ̃𝑩, ̃𝑸 and 𝑹 (cf. Gaitsgory
Considering that the discounted problem is equivalent to the undiscounted problem with
et al. 20), the given problem can be reformulated to a standard undiscounted LQ problem. For the latter, it is well-known that the
optimal controller is linear w.r.t. the state (here 𝒚𝑘,𝑖) and the optimal gain is given by (24) (see e.g. Lewis et al., Section 2.4 21),
thus (23) holds and the ﬁrst theorem assertion follows.

𝛾 ̃𝑨,

√

√

(ii) For the second theorem assertion, we note that the stabilizability of (

𝛾 ̃𝑨,

In addition, 𝑸 ⪰ 𝟎 yields ̃𝑸 ⪰ 𝟎. As (𝑪, 𝑨) is detectable (Assumption 1), with ̃𝑪 ⊺ ̃𝑪 = ̃𝑸, ( ̃𝑪,
all additional states in ̃𝑨 compared to 𝑨 are stable due to Assumption 2. Finally, due to ̃𝑸 ⪰ 𝟎, 𝑹 ≻ 𝟎, (
and ( ̃𝑪,

𝛾 ̃𝑨) detectable, a unique stabilizing solution exists Kučera, Theorem 8. 19

√

√

√

𝛾 ̃𝑩) directly follows from Assumptions 1–2.
𝛾 ̃𝑨) is also detectable, because
𝛾 ̃𝑩) stabilizable

𝛾 ̃𝑨,

√

√

√

Note 4. The proof of Theorem 2 demonstrates that in case of known system dynamics by means of 𝑨 and 𝑩, the optimal tracking
𝛾 ̃𝑩, ̃𝑸
controller 𝑳 can be directly calculated by solving the discrete-time algebraic Riccati equation 22 associated with
and 𝑹.

𝛾 ̃𝑨,

√

√

Equation (28) demonstrates that the Markov property holds (cf. Note 1). As a consequence of Theorem 2, for unknown system

dynamics, this yields the following problem in the LQ PRADP case.

Problem 2. For 𝑖 = 0, 1, … , ﬁnd the linear extended state feedback control (23) minimizing (21) and apply 𝒖∗
unknown system (20).

𝑘 = −𝑳𝒚𝑘,0 to the

Before we derive the control law 𝑳, we analyze the structure of ̂𝑄∗

Lemma.

𝒙𝑘, 𝒖𝑘, 𝑷 𝑘

(

)

associated with Problem 2 in the following

Lemma 2. (Structure of the Tracking Q-Function) The Q-function associated with Problem 2 has the form

𝑄∗(𝒙𝑘, 𝒖𝑘, 𝑷 𝑘) = 𝒛⊺

𝑘𝑯𝒛𝑘 =

⊺

𝒙𝑘
𝒖𝑘
⎡
𝒑𝑘,1∶𝑛
⎢
⎢
⎣

⎤
⎥
⎥
⎦

⎡
⎢
⎢
⎣

𝒉𝑥𝑥 𝒉𝑥𝑢 𝒉𝑥𝑝
𝒉𝑢𝑥 𝒉𝑢𝑢 𝒉𝑢𝑝
𝒉𝑝𝑥 𝒉𝑝𝑢 𝒉𝑝𝑝

𝒙𝑘
𝒖𝑘
⎡
𝒑𝑘,1∶𝑛
⎢
⎢
⎣

⎤
⎥
⎥
⎦

and 𝑯 is chosen such that 𝑯 = 𝑯 ⊺.

,

⎤
⎥
⎥
⎦

(29)

where 𝒛𝑘 =

𝑘 𝒖⊺
𝒙⊺
[

𝑘 𝒑⊺

𝑘,1∶𝑛

⊺

=

]

𝑘 𝒖⊺
𝒙⊺
[

𝑘 𝒑⊺

𝑘,1 … 𝒑⊺

𝑘,𝑛

⊺

]

Köpf ET AL.

(30)

8

Proof. With (6) and (7)

𝑄∗(𝒙𝑘, 𝒖𝑘, 𝑷 𝑘) = 𝑐

𝒙𝑘, 𝒖𝑘, 𝒓(𝑷 𝑘, 0)
(

∞

+

𝛾 𝑖𝑐

𝒙𝑘+𝑖, 𝝅∗
(

𝒙𝑘+𝑖, 𝑷 (𝑖)
(

𝑘

, 𝒓(𝑷 𝑘, 𝑖)
)

)
follows. With (20), (23) and (5) it is obvious that the states 𝒙𝑘+𝑖 and controls 𝝅∗(𝒙𝑘+𝑖, 𝑷 (𝑖)
𝑘 ) are linear w.r.t. 𝒛𝑘, ∀𝑖 = 0, 1, … .
From this linear dependency and with (22), linearity of 𝒆𝑘,𝑖 w.r.t. 𝒛𝑘, ∀𝑖 = 0, 1, … results. Due to the linear dependencies of 𝒆𝑘,𝑖
and 𝝅∗(⋅) and the quadratic structure of 𝑐(⋅) in (21), the Q-function in (30) is quadratic w.r.t. 𝒛𝑘, thus (29) holds.

∑𝑖=1

)

As a consequence of Lemma 2, 𝑄∗ can be exactly parametrized by means of ̂𝑄∗ according to (11) if ̂𝒘 = 𝒘 corresponds to
the non-redundant elements of 𝑯 = 𝑯 ⊺ (doubling elements of ̂𝒘 associated with oﬀ-diagonal elements of 𝑯) and 𝝓 = 𝒛𝑘 ⊗ 𝒛𝑘,
where ⊗ denotes the Kronecker product. Based on Lemma 2, the optimal control law is given as follows.

Theorem 3. (Optimal Tracking Control Law in Terms of 𝑯) The unique optimal extended state feedback control minimizing
𝐽𝑘 (21) is given by

Proof. According to Lemma 1, the desired control 𝒖∗
the necessary condition

𝑘 = 𝝅∗(𝒙𝑘, 𝑷 𝑘) = −𝑳𝒚𝑷 𝑘
𝒖∗

𝑘 = −𝒉−1

𝑢𝑢

𝒉𝑢𝑥 𝒉𝑢𝑝

[

]

𝒙𝑘
𝒑𝑘,1∶𝑛]
[

.

(31)

𝑘 minimizing 𝑄∗(𝒙𝑘, 𝒖𝑘, 𝑷 𝑘) is also minimizing 𝐽𝑘. With (29) and 𝑯 = 𝑯 ⊺,

yields the control 𝒖∗

𝑘 given in (31). Furthermore,

𝜕𝑄∗(𝒙𝑘, 𝒖𝑘, 𝑷 𝑘)
𝜕𝒖𝑘

= 2

𝒉𝑢𝑥𝒙𝑘 + 𝒉𝑢𝑝𝒑𝑘,1∶𝑛 + 𝒉𝑢𝑢𝒖𝑘

(

!
= 𝟎

)

𝜕2𝑄∗(𝒙𝑘, 𝒖𝑘, 𝑷 𝑘)
𝜕𝒖2
𝑘

= 2𝒉𝑢𝑢

(32)

(33)

demonstrates that 𝒉𝑢𝑢 ≻ 𝟎 is required in order to ensure that the control 𝒖∗
following. Therefore, let 𝑄∗
Then, it is obvious that

𝑘 (31) minimizes 𝐽𝑘 (21). This will be shown in the
reg(𝒙𝑘, 𝒖𝑘) be the optimal Q-function related to the regulation case, i.e. where 𝒓(𝑷 𝑘, 𝑖) = 𝒓(𝟎, 𝑖) = 𝟎.

∀𝒙𝑘 ∈ ℝ𝑛, 𝒖𝑘 ∈ ℝ𝑚, must be true. Furthermore, for the regulation case, it is well-known that

𝑄∗(𝒙𝑘, 𝒖𝑘, 𝟎) = 𝑄∗

reg(𝒙𝑘, 𝒖𝑘),

𝒉reg,𝑥𝑥 𝒉reg,𝑥𝑢
𝛾𝑩⊺𝑺𝑨 𝛾𝑩⊺𝑺𝑩 + 𝑹][
𝒉reg,𝑢𝑥 𝒉reg,𝑢𝑢][
[
holds (see e.g. Bradtke et al. 23). Here, 𝑺 is the solution of the discrete-time algebraic Riccati equation

reg(𝒙𝑘, 𝒖𝑘) =

𝛾𝑨⊺𝑺𝑨 + 𝑸 𝛾𝑨⊺𝑺𝑩

𝒙𝑘
𝒖𝑘]
[

𝒙𝑘
𝒖𝑘]
[

𝒙𝑘
𝒖𝑘]

𝑄∗

=

[

𝒙𝑘
𝒖𝑘]

⊺

⊺

Under Assumption 1, 𝑺 = 𝑺 ⊺ ⪰ 𝟎 exists and is unique (Kučera, Theorem 8 19). Thus, from (34) and (35),

𝑺 = 𝛾𝑨⊺𝑺𝑨 − 𝛾𝑨⊺𝑺𝑩(𝑹 + 𝑩⊺𝑺𝑩)−1𝑩⊺𝑺𝑨 + 𝑸.

results. This completes the proof.

Thus, if 𝑯 (or equivalently 𝒘) is known, both 𝑄∗ and 𝝅∗ can be calculated.

𝒉𝑢𝑢 = 𝒉reg,𝑢𝑢 = 𝛾𝑩⊺𝑺𝑩 + 𝑹 ≻ 𝟎

(34)

(35)

(36)

(37)

4

RESULTS

In order to validate our proposed PRADP tracking method, we show simulation results in the following, where the reference
trajectory is parametrized by means of cubic splines1. Furthermore, we compare the results with an ADP tracking method from
literature which assumes that the reference can be described by a time-invariant exo-system 𝒇 ref(𝒓𝑘). Finally, we compare our
learned controller that does not know the system dynamics with the ground truth controller which is calculated based on full
system knowledge.

1Other approximations can be used by choosing diﬀerent basis functions 𝝆(𝑖) (e.g. linear interpolation with 𝝆(𝑖) =

⊺

𝑖𝑇 1
]

[

or zero-order hold with 𝝆(𝑖) = 1).

Köpf ET AL.

9

4.1

Cubic Polynomial Reference Parametrization

We choose 𝒓(𝑷 𝑘, 𝑖) to be a cubic polynomial w.r.t. 𝑖, i.e. 𝝆(𝑖) =
The associated transformation in order to obtain the shifted version 𝑷 (𝑖)
following:

[

⊺

(𝑖𝑇 )3 (𝑖𝑇 )2 𝑖𝑇 1

, where 𝑇 denotes the sampling time.
𝑘 of 𝑷 𝑘 according to Deﬁnition 1 thus results from the

]

((𝑖 + 𝑗)𝑇 )3
((𝑖 + 𝑗)𝑇 )2
(𝑖 + 𝑗)𝑇
1

𝒓(𝑷 𝑘, 𝑖 + 𝑗) = 𝑷 𝑘𝝆(𝑖 + 𝑗) = 𝑷 𝑘 ⎡
⎢
⎢
⎢
⎢
⎣

1 3𝑖𝑇 3(𝑖𝑇 )2 (𝑖𝑇 )3
2𝑖𝑇 (𝑖𝑇 )2
0 1
= 𝑷 𝑘 ⎡
⎤
1
0 0
⎢
⎥
⎢
⎥
0
0 0
⎢
⎥
⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟
⎢
⎥
⎣
⎦
𝑻 (𝑖)

𝑖𝑇
1

⎤
⎥
⎥
⎥
⎥
⎦

𝝆(𝑗) = 𝑷 (𝑖)

𝑘 𝝆(𝑗).

(38)

In order to fully describe 𝒓(𝑷 𝑘, 𝑖), the values of 𝑷 𝑘 remain to be determined. Therefore, given sampling points of the reference
trajectory every 𝑑 = 25 time steps, let 𝑷 𝑘, 𝑘 = 𝑑𝑗, 𝑗 = 0, 1, … , result from cubic spline interpolation. In between the sampling
points, let 𝑷 𝑘+𝑖 = 𝑷 (𝑖)
𝑘 , 𝑖 = 1, 2, … , 𝑑 − 1 (cf. Deﬁnition 1 and (38)). This way, the controller is provided with 𝑷 𝑘 at each time
step 𝑘 when facing Problem 2.

Note 5. The given procedure to generate parameters 𝑷 𝑘 decouples the sampling time of the controller from the availability of
sampling points given for the reference trajectory (in our example only every 𝑑 = 25 time steps).

4.2

Example System

Consider a mass-spring-damper system with 𝑚sys = 0.5 kg, 𝑐sys = 0.1 N m−1 and 𝑑sys = 0.1 kg s−1. Discretization of this system
using Tustin approximation with 𝑇 = 0.1 s yields

𝒙𝑘+1 =

0.9990 0.0990
−0.0198 0.9792]

[

𝒙𝑘 +

0.0099
0.1979]
[

𝑢𝑘.

Here, 𝑥1 corresponds to the position, 𝑥2 to the velocity of the mass 𝑚sys and the control 𝑢𝑘 corresponds to a force.

We desire to track the position (i.e. 𝑥1), thus we set

𝑸 =

100 0
0 0]

[

and 𝑹 = 1

(39)

(40)

in order to strongly penalize the deviation of the ﬁrst state from the parametrized reference (cf. (21)) and 𝛾 = 0.9. In this example
setting, Assumptions 1–2 hold.

4.3

Simulations

In order to investigate the beneﬁts of our proposed PRADP tracking controller, we compare our method with an ADP tracking
controller from literature, 2,3 which assumes that the reference trajectory is generated by a time-invariant exo-system 𝒇 ref(𝒓𝑘).
Both our method (with 𝑒 ̂𝒘 = 1 × 10−5 in Algorithm 1) and the comparison method from literature are trained on data of 500
time steps, where Gaussian noise with zero mean and standard deviation of 1 is applied to the system input 𝑢𝑘 for excitation.
Note that none of the methods requires the system dynamics (20). Let 𝒓0 =
. The reference trajectory during training is

⊺

𝑟𝑘+1,1
𝑟𝑘+1,2]
[

= 𝒓𝑘+1 = 𝒇 ref(𝒓𝑘) =

𝒓𝑘

(41)

0 1
]

[

0.9988 0.0500
−0.0500 0.9988]
[
⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟
𝑭 ref

for the comparison method and the associated spline for our method.

The learned controllers both of our method and the comparison algorithm are tested on a reference trajectory for 𝑥1 that equals
the sine described by 𝑟𝑘,1 according to (41) for the ﬁrst 250 time steps. Then, the reference trajectory deviates from this sine as is
depicted in Fig. 1 in gray. Here, the blue crosses mark the sampling points for spline interpolation, the black dashed line depicts
𝑥1 resulting from our proposed method and the red dash-dotted line shows 𝑥1 for the comparison method. Furthermore, to gain
insight into the tracking quality by means of the resulting cost, 𝑐
is depicted in Fig. 2 for both methods. Note the
logarithmic ordinate which is chosen in order to render the black line representing the cost associated with our method visible.
(

𝒙𝑘, 𝒖𝑘, 𝒓(𝑷 𝑘, 0)

)

10

The optimal controller 𝑳 calculated using full system information (see Theorem 2 and Note 4) results in

𝑳 =

6.30 2.26 −0.31 −0.97 −2.37 −6.40 0 0 0 0

.

Köpf ET AL.

(42)

[
= 1.08 × 10−13. Thus, the
Comparing the learned controller 𝑳PRADP with the ground truth solution 𝑳 yields
learned controller is almost identical to the ground truth solution which demonstrates that the optimal tracking controller has
‖
‖
successfully been learned using PRADP without knowledge of the system dynamics.

𝑳PRADP − 𝑳

‖
‖

]

4.4

Discussion

As can be seen from Fig. 1, our proposed method successfully tracks the parametrized reference trajectory. In contrast, the method
proposed by e.g. Luo et al. 2 and Kiumarsi et al. 3 causes major deviation from the desired trajectory as soon as the reference
does not follow the same exo-system which it was trained on (i.e. as soon as (41) does not hold anymore after 250 time steps).
In addition, the cost in Fig. 2 reveals that both methods yield small and similar costs as long as the reference trajectory follows
𝑭 ref. However, as soon as the reference trajectory deviates from the time-invariant exo-system description 𝑭 ref at 𝑘 > 250, the
cost of the comparison method drastically exceeds the cost associated with our proposed method. With

max
𝑘

𝑐exo-system

𝒙𝑘, 𝒖𝑘, 𝒓(𝑷 𝑘, 0)

≈ 270

and max

𝑘

𝑐PRADP

𝒙𝑘, 𝒖𝑘, 𝒓(𝑷 𝑘, 0)

≈ 2.8,

(

)

(

)

our method clearly outperforms the comparison method. PRADP does not require the assumption that the reference trajectory
follows time-invariant exo-system dynamics but is nevertheless able to follow this kind of reference (see 𝑘 ≤ 250 in the simula-
tions) as well as all other references that can be approximated by means of the time-varying parameter 𝑷 𝑘. Thus, PRADP can
be interpreted as a more generalized tracking approach compared to existing ADP tracking methods.

sampling points
𝑥1,PRADP

𝑟1(𝑷 𝑘, 0) (reference)
𝑥1, exo-system

6

4

2

0

−2

,

𝑘
1
𝑥

0

100

200

300

400
time step 𝑘

500

600

700

800

FIGURE 1 Tracking results of our proposed method compared with a state of the art ADP tracking controller.

Köpf ET AL.

11

𝑐PRADP

𝑐exo-system

103

102

101

100

)

10−1

10−2

10−3

(

)
0

,
𝑘
𝑷
(
𝒓
,
𝑘
𝒖

,
𝑘
𝒙

𝑐

10−4

10−5

10−6

10−7

0

100

200

300

400
time step 𝑘

500

600

700

800

FIGURE 2 One-step cost 𝑐
ordinate.

(

𝒙𝑘, 𝒖𝑘, 𝒓(𝑷 𝑘, 0)

)

both for our proposed method and the comparison method. Note the logarithmic

5

CONCLUSION

In this paper, we proposed a new ADP-based tracking controller termed Parametrized Reference Adaptive Dynamic Program-
ming (PRADP). This method implicitly incorporates the approximated reference trajectory information into the Q-function that
is learned. This allows the controller to track time-varying parametrized references once the controller has been trained and does
not require further adaptation or re-training compared to previous methods. Simulation results showed that our learned con-
troller is more ﬂexible compared to state-of-the-art ADP tracking controllers which assume that the reference to track follows a
time-invariant exo-system. Motivated by a straightforward choice of basis functions, we concentrated on the LQ tracking case in
our simulations where the optimal controller has successfully been learned. However, as the mechanism of PRADP allows more
general tracking problem formulations (see Section 3), general function approximators can be used in order to approximate 𝑄
and allow for nonlinear ADP tracking controllers in the future.

References

1. Lewis F, Vrabie D. Reinforcement learning and adaptive dynamic programming for feedback control. IEEE Circuits and

Systems Magazine 2009; 9(3): 32–50. doi: 10.1109/MCAS.2009.933854

2. Luo B, Liu D, Huang T, Wang D. Model-Free Optimal Tracking Control via Critic-Only Q-Learning. IEEE Transactions

on Neural Networks and Learning Systems 2016; 27(10): 2134–2144. doi: 10.1109/TNNLS.2016.2585520

3. Kiumarsi B, Lewis FL, Modares H, Karimpour A, Naghibi-Sistani MB. Reinforcement Q-learning for optimal
tracking control of linear discrete-time systems with unknown dynamics. Automatica 2014; 50(4): 1167–1175.
doi: 10.1016/j.automatica.2014.02.015

12

Köpf ET AL.

4. Köpf F, Ebbert S, Flad M, Hohmann S. Adaptive Dynamic Programming for Cooperative Control with Incomplete

Information. In: 2018 IEEE International Conference on Systems, Man and Cybernetics (SMC); 2018.

5. Dierks T, Jagannathan S. Optimal tracking control of aﬃne nonlinear discrete-time systems with unknown internal dynam-
ics. In: 2009 Joint 48th IEEE Conference on Decision and Control (CDC) and 28th Chinese Control Conference (CCC);
2009: 6750–6755

6. Modares H, Lewis FL. Linear Quadratic Tracking Control of Partially-Unknown Continuous-Time Systems Using Rein-
forcement Learning. IEEE Transactions on Automatic Control 2014; 59(11): 3051–3056. doi: 10.1109/TAC.2014.2317301

7. Zhang K, Zhang H, Xiao G, Su H. Tracking control optimization scheme of continuous-time nonlinear system via online
single network adaptive critic design method. Neurocomputing 2017; 251: 127–135. doi: 10.1016/j.neucom.2017.04.008

8. van Nieuwstadt MJ. Trajectory Generation for Nonlinear Control Systems. Dissertation. California Institute of Technology,

1997.

9. Kiumarsi B, Lewis FL, Levine DS. Optimal control of nonlinear discrete time-varying systems using a new neural network

approximation structure. Neurocomputing 2015; 156: 157–165. doi: 10.1016/j.neucom.2014.12.067

10. Sutton RS, Barto AG. Reinforcement Learning: An introduction. Cambridge Massachusetts: MIT Press. 2nd ed. 2018.

11. Köpf F, Westermann J, Flad M, Hohmann S. Adaptive Optimal Control for Reference Tracking Independent of Exo-System

Dynamics. arXiv e-prints 2019: arXiv:1906.05085.

12. Sutton RS. Learning to Predict by the Methods of Temporal Diﬀerences. Machine learning 1988; 3(1): 9–44.

doi: 10.1023/A:1022633531479

13. Lagoudakis MG, Parr R. Least-squares policy iteration. Journal of machine learning research 2003; 4: 1107–1149.

14. Buşoniu L, Babuška R, Schutter dB, Ernst D. Reinforcement learning and dynamic programming using function approxi-

mators. 39. CRC press . 2010.

15. Hornik K, Stinchcombe M, White H. Universal approximation of an unknown mapping and its derivatives using multilayer

feedforward networks. Neural Networks 1990; 3(5): 551–560. doi: 10.1016/0893-6080(90)90005-6

16. Åström KJ, Wittenmark B. Adaptive control. Reading, Mass.: Addison-Wesley. 2nd ed. 1995.

17. Bertsekas DP, Tsitsiklis JM. Neuro-Dynamic Programming. Belmont, Massachusetts: Athena Scientiﬁc . 1996.

18. Wang D, He H, Liu D. Adaptive Critic Nonlinear Robust Control: A Survey. IEEE transactions on cybernetics 2017; 47(10):

3429–3451. doi: 10.1109/TCYB.2017.2712188

19. Kučera V. The discrete Riccati equation of optimal control. Kybernetika 1972; 8(5): 430–447.

20. Gaitsgory V, Grüne L, Höger M, Kellett CM, Weller SR. Stabilization of strictly dissipative discrete time systems with

discounted optimal control. Automatica 2018; 93: 311–320. doi: 10.1016/j.automatica.2018.03.076

21. Lewis FL, Vrabie DL, Syrmos VL. Optimal control. Hoboken: Wiley. 3rd ed. ed. 2012.

22. Arnold WF, Laub AJ. Generalized eigenproblem algorithms and software for algebraic Riccati equations. Proceedings of

the IEEE 1984; 72(12): 1746–1754. doi: 10.1109/PROC.1984.13083

23. Bradtke SJ, Ydstie BE, Barto AG. Adaptive linear quadratic control using policy iteration. In: Proceedings of the 1994

American Control Conference; 1994: 3475–3479.

Köpf ET AL.

13

How to cite this article: Köpf, F., S. Ramsteiner, M. Flad, and S. Hohmann (2019), Adaptive Dynamic Programming for
Model-free Tracking of Trajectories with Time-varying Parameters, Int J Adapt Control Signal Process, 2020.

