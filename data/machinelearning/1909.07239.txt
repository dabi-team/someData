0
2
0
2

b
e
F
7
1

]

Y
S
.
s
s
e
e
[

5
v
9
3
2
7
0
.
9
0
9
1
:
v
i
X
r
a

Received: Added at production

Revised: Added at production

Accepted: Added at production

DOI: xxx/xxxx

RESEARCH ARTICLE

Adaptive Dynamic Programming for Model-free Tracking of
Trajectories with Time-varying Parametersâ€ 

Florian KÃ¶pf | Simon Ramsteiner | Michael Flad | SÃ¶ren Hohmann

Institute of Control Systems, Karlsruhe
Institute of Technology (KIT), Karlsruhe,
Germany

Correspondence
Florian KÃ¶pf, Institute of Control Systems,
Karlsruhe Institute of Technology (KIT),
Kaiserstr. 12, 76131 Karlsruhe, Germany.
Email: ï¬‚orian.koepf@kit.edu

Summary

In order to autonomously learn to control unknown systems optimally w.r.t. an
objective function, Adaptive Dynamic Programming (ADP) is well-suited to adapt

controllers based on experience from interaction with the system. In recent years,
many researchers focused on the tracking case, where the aim is to follow a desired

trajectory. So far, ADP tracking controllers assume that the reference trajectory fol-
lows time-invariant exo-system dynamicsâ€”an assumption that does not hold for

many applications. In order to overcome this limitation, we propose a new Q-function
which explicitly incorporates a parametrized approximation of the reference tra-

jectory. This allows to learn to track a general class of trajectories by means of
ADP. Once our Q-function has been learned, the associated controller copes with

time-varying reference trajectories without need of further training and independent
of exo-system dynamics. After proposing our general model-free oï¬€-policy track-

ing method, we provide analysis of the important special case of linear quadratic

tracking. We conclude our paper with an example which demonstrates that our new
method successfully learns the optimal tracking controller and outperforms existing

approaches in terms of tracking error and cost.

KEYWORDS:
Adaptive Dynamic Programming, Optimal Tracking, Optimal Control, Reinforcement Learning

1

INTRODUCTION

Adaptive Dynamic Programming (ADP) which is based on Reinforcement Learning has gained extensive attention as a model-
free adaptive optimal control method. 1 In ADP, pursuing the objective to minimize a cost functional, the controller adapts its
behavior on the basis of interaction with an unknown system. The present work focuses on the ADP tracking case, where a
reference trajectory is intended to be followed while the system dynamics is unknown. As the long-term cost, i.e. value, of a
state changes depending on the reference trajectory, a controller that has learned to solve a regulation problem cannot be directly
transferred to the tracking case.

Therefore, in literature, there are several ADP tracking approaches in discrete time 2,3,4,5 and continuous time. 6,7 All of these
methods assume that the reference trajectory ğ’“ğ‘˜ can be modeled by means of a time-invariant exo-system ğ’“ğ‘˜+1 = ğ’‡ ref(ğ’“ğ‘˜) (and
Ì‡ğ’“(ğ‘¡) = ğ’‡ ref(ğ’“(ğ‘¡)), respectively). Then, an approximated value function (or Q-function) is learned in order to rate diï¬€erent states (or
state-action combinations) w.r.t their expected long-term cost. Based on this information, approximated optimal control laws are

â€ This is a preprint submitted to the Int J Adapt Control Signal Process. The substantially revised version will be published in the Int J Adapt Control Signal Process

(DOI: 10.1002/ACS.3106).

 
 
 
 
 
 
2

KÃ¶pf ET AL.

derived. Whenever this reference trajectory and thus the function ğ’‡ ref changes, the learned value function and consequently the
controller is not valid anymore and needs to be re-trained. Therefore, the exo-system tracking case with time-invariant reference
dynamics ğ’‡ ref is not suited for all applications. 8 For example in autonomous driving, process engineering and human-machine
collaboration, it is often required to track ï¬‚exible and time-varying trajectories. In order to account for various references, the
multiple-model approach presented by Kiumarsi et al. 9 uses a self-organizing map that switches between several learned models.
However, in their approach, new sub-models need to be trained for each exo-system ğ’‡ ref.

Thus, our idea is to deï¬ne a state-action-reference Q-function that explicitly incorporates the course of the reference trajectory
in contrast to the commonly used Q-function (see e.g. Sutton and Barto 10) which only depends on the current state ğ’™ğ‘˜ and
control ğ’–ğ‘˜. This general idea has ï¬rst been proposed in our previous work, 11 where the reference ğ’“ğ‘˜ is given on a ï¬nite horizon
and assumed to be zero thereafter. Thus, the number of weights to be learned depends on the horizon on which the reference
trajectory is considered. As the reference trajectory is given for each time step, this allows high ï¬‚exibility, but the sampling time
and (unknown) system dynamics signiï¬cantly inï¬‚uence the reasonable horizon length and thus the number of weights to be
learned. Based on these challenges, our major idea and contribution in the present work is to approximate the reference trajectory
by means of a potentially time-varying parameter set ğ‘· ğ‘˜ in order to compress the information about the reference compared to
our previous work 11 and incorporate this parameter into a new Q-function. In doing so, the Q-function explicitly represents the
dependency of the expected long-term cost on the desired reference trajectory. Hence, the associated optimal controller is able
to cope with time-varying parametrized references. We term this method Parametrized Reference ADP (PRADP).

Our main contributions include:
â€¢ The introduction of a new reference-dependent Q-function that explicitly depends on the reference-parameter ğ‘· ğ‘˜.
â€¢ Function approximation of this Q-function in order to realize Temporal Diï¬€erence (TD) learning (cf. Sutton 12).

â€¢ Rigorous analysis of the form of this Q-function and its associated optimal control law in the special case of linear-

quadratic (LQ) tracking.

â€¢ A comparison of our proposed method with algorithms assuming a time-invariant exo-system ğ’‡ ref and the ground truth

optimal tracking controller.

In the next section, the general problem deï¬nition is given. Then, PRADP is proposed in Section 3. Simulation results and a
discussion are given in Section 4 before the paper is concluded.

2 GENERAL PROBLEM DEFINITION

Consider a discrete-time controllable system

(1)
)
0 is the discrete time step, ğ’™ğ‘˜ âˆˆ â„ğ‘› the system state and ğ’–ğ‘˜ âˆˆ â„ğ‘š the input. The system dynamics ğ’‡ (â‹…) is assumed

ğ’™ğ‘˜, ğ’–ğ‘˜
(

ğ’™ğ‘˜+1 = ğ’‡

,

where ğ‘˜ âˆˆ â„•
to be unknown.

Furthermore, let the parametrized reference trajectory ğ’“(ğ‘· ğ‘˜, ğ‘–) âˆˆ â„ğ‘› which we intend to follow be described by

ğ’‘âŠº
ğ‘˜,1
ğ’‘âŠº
ğ‘˜,2
ğ’“(ğ‘· ğ‘˜, ğ‘–) = ğ‘· ğ‘˜ğ†(ğ‘–) = â¡
â‹®
â¢
ğ’‘âŠº
â¢
ğ‘˜,ğ‘›
â¢
â¢
â£

â¤
â¥
â¥
â¥
At any time step ğ‘˜, the reference trajectory is described by means of a parameter matrix ğ‘· ğ‘˜ âˆˆ â„ğ‘›Ã—ğ‘ and given basis functions
â¥
â¦
ğ†(ğ‘–) âˆˆ â„ğ‘. Here, ğ‘– âˆˆ â„•
0 denotes the time step on the reference from the local perspective at time ğ‘˜, i.e. for ğ‘– = 0, the reference at
time step ğ‘˜ results and ğ‘– > 0 yields a prediction of the reference for future time steps. Thus, in contrast to methods which assume
that the reference follows time-invariant exo-system dynamics ğ’‡ ref, the parameters ğ‘· ğ‘˜ in (2) can be time-varying, allowing much
more diverse reference trajectories.

ğ†(ğ‘–).

(2)

Our aim is to learn a controller which does not know the system dynamics and minimizes the cost

ğ½ğ‘˜ =

âˆ

âˆ‘ğ‘–=0

ğ›¾ ğ‘–ğ‘(ğ’™ğ‘˜+ğ‘–, ğ’–ğ‘˜+ğ‘–, ğ’“(ğ‘· ğ‘˜, ğ‘–)),

(3)

KÃ¶pf ET AL.

3

where ğ›¾ âˆˆ [0, 1) is a discount factor and ğ‘(â‹…) denotes a non-negative one-step cost. We deï¬ne our general problem as follows.

Problem 1. For a given parametrization of the reference by means of ğ‘· ğ‘˜ according to (2), an optimal control sequence that
minimizes the cost (3) is denoted by ğ’–âˆ—
ğ‘˜ . The system dynamics is unknown. At each time
step ğ‘˜, ï¬nd ğ’–âˆ—
ğ‘˜.

ğ‘˜+1, â€¦ and the associated cost by ğ½ âˆ—

ğ‘˜, ğ’–âˆ—

3

PARAMETRIZED REFERENCE ADP (PRADP)

In order to solve Problem 1, we ï¬rst propose a new, modiï¬ed Q-function whose minimizing control represents a solution ğ’–âˆ—
ğ‘˜ to
Problem 1. In the next step, we parametrize this Q-function by means of linear function approximation. Then, we apply Least-
Squares Policy Iteration (LSPI) (cf. Lagoudakis and Parr 13) in order to learn the unknown Q-function weights from data without
requiring a system model. Finally, we discuss the structure of this new Q-function for the linear-quadratic tracking problem,
where analytical insights are possible.

3.1

Proposed Q-Function

The relative position ğ‘– on the current reference trajectory that is parametrized by means of ğ‘· ğ‘˜ according to (2) needs to be
considered when minimizing the cost ğ½ğ‘˜ as given in (3). In order to do so, one could explicitly incorporate the relative time
ğ‘– into the Q-function that is used for ADP. This would yield a Q-function of the form ğ‘„(ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜, ğ‘–). However, this would
unnecessarily increase the complexity of the Q-function and hence the challenge to approximate and learn such a Q-function.
Thus, we decided to implicitly incorporate the relative time ğ‘– on the current reference trajectory parametrized by ğ‘· ğ‘˜ into the
reference trajectory parametrization. This yields a shifted parameter matrix ğ‘· (ğ‘–)

ğ‘˜ according to the following deï¬nition.

Deï¬nition 1. (Shifted Parameter Matrix ğ‘· (ğ‘–)

ğ‘˜ be deï¬ned such that

ğ‘˜ ) Let the matrix ğ‘· (ğ‘–)
ğ‘· (ğ‘–)
(
â‡” ğ‘· (ğ‘–)

ğ’“

= ğ’“(ğ‘· ğ‘˜, ğ‘– + ğ‘—)

ğ‘˜ , ğ‘—
ğ‘˜ ğ†(ğ‘—) = ğ‘· ğ‘˜ğ†(ğ‘– + ğ‘—).

)

Thus,

ğ‘· (ğ‘–)

ğ‘˜ = ğ‘· ğ‘˜ğ‘» (ğ‘–)

(4a)

(4b)

(5)

is a modiï¬ed version of ğ‘· ğ‘˜ = ğ‘· (0)
ğ‘˜ such that the associated reference trajectory is shifted by ğ‘– time steps, where ğ‘» (ğ‘–) is a suitable
matrix. Note that ğ‘» (ğ‘–) is in general ambiguous as in the general case ğ‘ > 1 the system of equations (4b) in order to solve for ğ‘· (ğ‘–)
ğ‘˜
is underdetermined. Thus, ğ‘» (ğ‘–) can be any matrix such that (4) holds.

Our proposed Q-function which explicitly incorporates the reference trajectory by means of ğ‘· ğ‘˜ is given as follows.

Deï¬nition 2. (Parametrized Reference Q-Function) Let

ğ‘„ğ…

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜

= ğ‘

(

)

= ğ‘

âˆ

+

ğ›¾ ğ‘–ğ‘

)

âˆ‘ğ‘–=1
+ ğ›¾ğ‘„ğ…

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ’“(ğ‘· ğ‘˜, 0)
(
ğ’™ğ‘˜, ğ’–ğ‘˜, ğ’“(ğ‘· ğ‘˜, 0)
(

)

ğ’™ğ‘˜+ğ‘–, ğ…
(

ğ’™ğ‘˜+1, ğ…
(

ğ‘˜

ğ’™ğ‘˜+ğ‘–, ğ‘· (ğ‘–)
)
(
ğ’™ğ‘˜+1, ğ‘· (1)
(

ğ‘˜

)

, ğ‘· (1)
ğ‘˜

.

)

, ğ‘Ÿ(ğ‘· ğ‘˜, ğ‘–)
)

(6)

Here, ğ… âˆ¶ â„ğ‘› Ã— â„ğ‘›Ã—ğ‘ â†’ â„ğ‘š denotes the current control policy.
Therefore, ğ‘„ğ…(ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜) represents the accumulated discounted cost if the system is in state ğ’™ğ‘˜, the control ğ’–ğ‘˜ is applied at
time ğ‘˜ and the policy ğ…(â‹…) is followed thereafter while the reference trajectory is parametrized by ğ‘· ğ‘˜. Based on (6), the optimal
Q-function ğ‘„âˆ—(â‹…) is given by

ğ‘„âˆ—

= ğ‘

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜

ğ’™ğ‘˜+1, ğ…
(
ğ’™ğ‘˜+1, ğ…âˆ—
(
Here, the optimal control policy is denoted by ğ…âˆ—(â‹…), hence ğ…âˆ—(ğ’™ğ‘˜+1, ğ‘· (1)
ğ‘˜ ) = ğ’–âˆ—
as can be seen from the following Lemma.

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ’“(ğ‘· ğ‘˜, 0)
(
ğ’™ğ‘˜, ğ’–ğ‘˜, ğ’“(ğ‘· ğ‘˜, 0)
(

+ min
ğ…
+ ğ›¾ğ‘„âˆ—

= ğ‘

)

)

)

(

ğ›¾ğ‘„ğ…

ğ’™ğ‘˜+1, ğ‘· (1)
(
ğ’™ğ‘˜+1, ğ‘· (1)
(

, ğ‘· (1)
ğ‘˜
ğ‘˜
)
, ğ‘· (1)
ğ‘˜
)

)

ğ‘˜

.

ğ‘˜+1. This Q-function is useful for solving Problem 1

)

(7)

4

KÃ¶pf ET AL.

Lemma 1. The control ğ’–ğ‘˜ minimizing ğ‘„âˆ—

Proof. With (7)

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜
(

)

is a solution for ğ’–âˆ—

ğ‘˜ minimizing ğ½ğ‘˜ in (3) according to Problem 1.

ğ‘„âˆ—

min
ğ’–ğ‘˜

(

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜

= ğ‘

ğ’™ğ‘˜, ğ’–âˆ—

+ ğ›¾ğ‘„âˆ—

ğ’™ğ‘˜+1, ğ’–âˆ—
(

ğ‘˜+1, ğ‘· (1)

ğ‘˜

ğ‘˜, ğ’“(ğ‘· ğ‘˜, 0)
âˆ

ğ›¾ ğ‘–ğ‘

âˆ‘ğ‘–=0

)
ğ’™ğ‘–, ğ’–ğ‘–, ğ’“(ğ‘· ğ‘˜, ğ‘–)
(

)

)

(
= min

ğ’–ğ‘˜,ğ’–ğ‘˜+1,â€¦

= ğ½ âˆ—
ğ‘˜

follows, which completes the proof.

Thus, if the Q-function ğ‘„âˆ—(ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜) is known, the desired optimal control ğ’–ğ‘˜ is given by

)

(8)

(9)

Lemma 1 and (9) reveal the usefulness of ğ‘„âˆ—

for solving Problem 1. Thus, we express this Q-function by means
of linear function approximation in the following. Based on the temporal-diï¬€erence (TD) error, the unknown Q-function weights
can then be estimated.

(

)

ğ’–âˆ—
ğ‘˜ = arg min

ğ‘„âˆ—(ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜).

ğ’–ğ‘˜
ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜

3.2

Function Approximation of the Tracking Q-Function

As classical tabular Q-learning is unable to cope with large or even continuous state and control spaces, it is common to represent
the Q-function, which is assumed to be smooth, by means of function approximation 14. This leads to

ğ‘„âˆ—

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜

(10)
Here, ğ’˜ âˆˆ â„ğ‘ is the unknown optimal weight vector, ğ“ âˆˆ â„ğ‘ a vector of activation functions and ğœ– the approximation error.
According to the Weierstrass higher-order approximation Theorem 15 a single hidden layer and appropriately smooth hidden
layer activation functions ğ“(â‹…) are capable of an arbitrarily accurate approximation of the Q-function. Furthermore, if ğ‘ â†’ âˆ,
ğœ– â†’ 0.

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜

+ ğœ–

)

)

(

)

(

(

.

= ğ’˜âŠºğ“

As ğ’˜ is a priori unknown, let the estimated optimal Q-function be given by

Ì‚ğ‘„âˆ—

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜

= Ì‚ğ’˜âŠºğ“

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜

.

In analogy to (9), the estimated optimal control law is deï¬ned as
Ì‚ğ…âˆ—(ğ’™ğ‘˜, ğ‘· ğ‘˜) = arg min

(

)

ğ’–ğ‘˜

(

)

Ì‚ğ‘„âˆ—

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜

.

(

)

Based on this parametrization of our new Q-function, the associated TD error 12 is deï¬ned as follows.

(11)

(12)

Deï¬nition 3. (TD Error of the Tracking Q-Function) The TD error which results from using the estimated Q-function Ì‚ğ‘„âˆ—(â‹…)
(11) in the Bellman-like equation (7) is deï¬ned as

ğ›¿ğ‘˜ = ğ‘

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ’“(ğ‘· ğ‘˜, 0)

(

= ğ‘

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ’“(ğ‘· ğ‘˜, 0)

)

(

+ ğ›¾ Ì‚ğ‘„âˆ—

ğ’™ğ‘˜+1, Ì‚ğ…âˆ—
(
+ ğ›¾ Ì‚ğ’˜âŠºğ“

ğ’™ğ‘˜+1, ğ‘· (1)
)
(
ğ’™ğ‘˜+1, ğ‘· (1)
ğ’™ğ‘˜+1, Ì‚ğ…âˆ—
(

(

ğ‘˜

ğ‘˜

, ğ‘· (1)
ğ‘˜
)
, ğ‘· (1)
ğ‘˜
)

âˆ’ Ì‚ğ‘„âˆ—

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜

(
âˆ’ Ì‚ğ’˜âŠºğ“

)

)

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜
(

)

)

.

(13)

Our goal is to estimate Ì‚ğ’˜ in order to minimize the squared TD error ğ›¿2

ğ‘˜ as the TD error quantiï¬es the quality of the Q-function

approximation. However, (13) is scalar while ğ‘ weights need to be estimated. Thus, we utilize ğ‘ â‰¥ ğ‘ tuples
ğ‘˜, Ì‚ğ‘„âˆ—+

, ğ‘˜ = 1, â€¦ , ğ‘,

î‰€ğ‘˜ =

ğ‘˜

ğ‘ğ‘˜, Ì‚ğ‘„âˆ—
{

}

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ’“(ğ‘· ğ‘˜, 0)

ğ‘ğ‘˜ = ğ‘
ğ‘˜ = Ì‚ğ’˜âŠºğ“ğ‘˜ = Ì‚ğ’˜âŠºğ“
Ì‚ğ‘„âˆ—

(

,
ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜
(

)

)

Ì‚ğ‘„âˆ—+

ğ‘˜ = Ì‚ğ’˜âŠºğ“+

ğ‘˜ = Ì‚ğ’˜âŠºğ“

ğ’™ğ‘˜+1, Ì‚ğ…âˆ—
(

ğ’™ğ‘˜+1, ğ‘· (1)
(

ğ‘˜

, ğ‘· (1)
ğ‘˜
)

)

(14)

where

and

KÃ¶pf ET AL.

5

from interaction with the system in order to estimate Ì‚ğ’˜ using Least-Squares Policy Iteration (LSPI) (cf. Lagoudakis and Parr 13).
Stacking (13) for the tuples î‰€ğ‘˜, ğ‘˜ = 1, â€¦ , ğ‘, yields

=

ğ›¿1
â‹®
â¡
â¤
ğ›¿ğ‘
â¢
â¥
âŸâŸâŸ
â¢
â¥
â£
â¦
ğœ¹

+

ğ‘1
â‹®
â¡
â¤
ğ‘ğ‘
â¢
â¥
âŸâŸâŸ
â¢
â¥
â£
â¦
ğ’„

ğ“+âŠº
1
â‹®
â¡
ğ“+âŠº
â¢
ğ‘
â¢
â£

ğ›¾

âˆ’

â›
â¤
â
â¤
âœ
â¥
âŸ
â¥
âŸâââââââââââŸâââââââââââŸ
âœ
â¥
âŸ
â¥
â¦
â¦
â
â 
ğš½

ğ“âŠº
1
â‹®
â¡
ğ“âŠº
â¢
ğ‘
â¢
â£

If the excitation condition

holds, Ì‚ğ’˜ minimizing ğœ¹âŠºğœ¹ exists, is unique and given by

according to Ã…strÃ¶m and Wittenmark, Theorem 2.1. 16

(

)

Ì‚ğ’˜ =

ğš½âŠºğš½

âˆ’1 ğš½âŠºğ’„

rank ğš½âŠºğš½ = ğ‘

Ì‚ğ’˜.

(15)

(16)

(17)

Note 1. Using ğ‘· (1)
combination with (1)) that the Markov property holds, which is commonly required in ADP. 1

ğ‘˜ = ğ‘· ğ‘˜ğ‘» (1) (5) in the training tuple î‰€ğ‘˜ (14) rather than an arbitrary subsequent ğ‘· ğ‘˜+1 guarantees (in

Remark 1. The procedure described above is an extension of Lagoudakis and Parr, Section 5.1 13 to the tracking case where
minimizing the squared TD error is targeted. In addition, an alternative projection method described by Lagoudakis and Parr,
Section 5.2 13 which targets the approximate Q-function to be a ï¬xed point under the Bellman operator has been implemented.
Both procedures yielded indistinguishable results for our linear-quadratic simulation examples but might be diï¬€erent in the
general case.

Note that Ì‚ğ…âˆ—(â‹…) in Ì‚ğ‘„âˆ—+

ğ‘˜ depends on Ì‚ğ’˜, i.e. the estimation of Ì‚ğ‘„âˆ—+

ğ‘˜ depends on another estimation (of the optimal control law).
This mechanism is known as bootstrapping (cf. Sutton and Barto 10) in Reinforcement Learning. As a consequence, rather than
estimating Ì‚ğ’˜ once by means of the least-squares estimate (17), a policy iteration is performed starting with Ì‚ğ’˜(0). This procedure
is given in Algorithm 1, where ğ‘’ Ì‚ğ’˜ is a threshold for the terminal condition.
Note 2. Due to the use of a Q-function which explicitly depends on the control ğ’–ğ‘˜, this method performs oï¬€-policy learning. 10
Thus, during training, the behavior policy (i.e. ğ’–ğ‘˜ which is actually applied to the system) might include exploration noise in order
to satisfy the rank condition (16) but due to the greedy target policy Ì‚ğ…âˆ— (cf. the policy improvement step (12)), the Q-function
associated with the optimal control law is learned.

With Ì‚ğ‘„(ğ‘–)(â‹…) = Ì‚ğ’˜(ğ‘–)âŠºğ“(â‹…) and ğ‘„ Ì‚ğ…(ğ‘–)

according to (6) with ğ… = Ì‚ğ…(ğ‘–), the following convergence properties also hold for our

tracking Q-function.

Theorem 1. (Convergence Properties of the Q-function, cf. Lagoudakis and Parr, Theorem 7.1 13) Let Ì„ğœ– â‰¥ 0 bound the errors
between the approximate Q-function Ì‚ğ‘„(ğ‘–) and true Q-function ğ‘„ Ì‚ğ…(ğ‘–)

associated with Ì‚ğ…(ğ‘–) over all iterations, i.e.

Ì‚ğ‘„(ğ‘–) âˆ’ ğ‘„ Ì‚ğ…(ğ‘–)

âˆ

â‰¤ Ì„ğœ–, âˆ€ğ‘– = 1, 2, â€¦ .

â€–
Then, Algorithm 1 yields control laws such that
â€–
â€–
lim sup
ğ‘–â†’âˆ

â€–
â€–
â€–
Ì‚ğ‘„(ğ‘–) âˆ’ ğ‘„âˆ—

â€–
â€–
â€–

â€–
â€–
â€–

â‰¤

âˆ

2ğ›¾ Ì„ğœ–
(1 âˆ’ ğ›¾)2

.

Algorithm 1 PRADP based on LSPI
1: initialize ğ‘– = 0, Ì‚ğ’˜(0)
2: do

(18)

(19)

policy evaluation: calculate Ì‚ğ’˜(ğ‘–+1) according to (17), where Ì‚ğ’˜ = Ì‚ğ’˜(ğ‘–+1)
policy improvement: obtain Ì‚ğ…(ğ‘–+1) from (12)
ğ‘– = ğ‘– + 1

3:

4:

5:
6: while

Ì‚ğ’˜(ğ‘–) âˆ’ Ì‚ğ’˜(ğ‘–âˆ’1)

â€–
â€–
â€–

> ğ‘’ Ì‚ğ’˜

2
â€–
â€–
â€–

6

KÃ¶pf ET AL.

Proof. The proof is adapted from Bertsekas and Tsitsiklis, Proposition 6.2 17.

Lagoudakis and Parr 13 point out that the appropriate choice of basis functions and the sample distribution (i.e. excitation)
determine Ì„ğœ–. According to Theorem 1, Algorithm 1 converges to a neighborhood of the optimal tracking Q-function under
appropriate choice of basis functions ğ“(â‹…) and excitation. However, for general nonlinear systems (1) and cost functions (3), an
appropriate choice of basis functions and the number of neurons is â€œmore of an art than scienceâ€ 18 and still an open problem.
As the focus of this paper lies on the new Q-function for tracking purposes rather than tuning of neural networks, we focus on
linear systems and quadratic cost functions in the followingâ€”a setting that plays an important role in control engineering. This
allows analytic insights into the structure of ğ‘„âˆ—
and thus proper choice of ğ“(â‹…) for function approximation in order
to demonstrate the eï¬€ectiveness of the proposed PRADP method.

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜
(

)

3.3

The LQ-Tracking Case

In the following, assume

and

ğ’™ğ‘˜+1 = ğ‘¨ğ’™ğ‘˜ + ğ‘©ğ’–ğ‘˜,

âˆ

ğ½ğ‘˜ =

ğ›¾ ğ‘–

ğ’™ğ‘˜+ğ‘– âˆ’ ğ’“(ğ‘· ğ‘˜, ğ‘–)

âˆ‘ğ‘–=0
âˆ

[(

âŠº

ğ‘¸

)

ğ’™ğ‘˜+ğ‘– âˆ’ ğ’“(ğ‘· ğ‘˜, ğ‘–)
(

)

+ğ’–âŠº

ğ‘˜+ğ‘–ğ‘¹ğ’–ğ‘˜+ğ‘–

]

=âˆ¶

ğ›¾ ğ‘–

âˆ‘ğ‘–=0

ğ‘˜,ğ‘–ğ‘¸ğ’†ğ‘˜,ğ‘– + ğ’–âŠº
ğ’†âŠº
[

ğ‘˜+ğ‘–ğ‘¹ğ’–ğ‘˜+ğ‘–

.

]

(20)

(21)

Here, ğ‘¸ penalizes the deviation of the state ğ’™ğ‘˜+ğ‘– from the reference ğ’“(ğ‘· ğ‘˜, ğ‘–) and ğ‘¹ penalizes the control eï¬€ort. Furthermore, let
the following assumptions hold.

Assumption 1. Let ğ‘¸ = ğ‘¸âŠº âª° ğŸ, ğ‘¹ = ğ‘¹âŠº â‰» ğŸ, (ğ‘¨, ğ‘©) be controllable and (ğ‘ª, ğ‘¨) be detectable, where ğ‘ª âŠºğ‘ª = ğ‘¸.
Assumption 2. Let the matrix ğ‘» (ğ‘–) which deï¬nes the shifted parameter matrix ğ‘· (ğ‘–)
âˆ€ğ‘— = 1, â€¦ , ğ‘, holds, where ğœ†ğ‘— are the eigenvalues of

ğ‘˜ according to (5) be such that

ğ›¾ğ‘» (1).

Note 3. Assumption 1 is rather standard in the LQ setting in order to ensure the existence and uniqueness of a stabilizing solution
to the discrete-time algebraic Riccati equation associated with the regulation problem given by (20) and (21) for ğ‘· ğ‘˜ = ğŸ (cf.
KuÄera, Theorem 8 19). Furthermore, it is obvious that the reference trajectory ğ’“(ğ‘· ğ‘˜, ğ‘–) must be deï¬ned such that a controller
exists which yields ï¬nite cost ğ½ğ‘˜ in order to obtain a reasonable control problem. As will be seen in Theorem 2, Assumption 2
guarantees the existence of this solution.

âˆš

ğœ†ğ‘—

< 1,

|
|
|

|
|
|

The tracking error ğ’†ğ‘˜,ğ‘– can be expressed as

ğ’†ğ‘˜,ğ‘– = ğ’™ğ‘˜+ğ‘– âˆ’ ğ’“(ğ‘· ğ‘˜, ğ‘–) = ğ’™ğ‘˜+ğ‘– âˆ’ ğ‘· (ğ‘–)

ğ‘˜ ğ†(0) =

âŠº

âˆ’ğ†(0) â‹¯ ğŸ
â‹® â‹± â‹®
ğŸ â‹¯ âˆ’ğ†(0)â¤
â¥
â¥
â¦

ğ‘° ğ‘›
â¡
â¤
â¢
â¥
âŸâââââââââââââââââââŸâââââââââââââââââââŸ
â¢
â¥
â£
â¦
=âˆ¶ğ‘´

â¡
â¢
â¢
â£

,

ğ’™ğ‘˜+ğ‘–
ğ’‘(ğ‘–)
ğ‘˜,1
â¡
â¤
â‹®
â¢
â¥
ğ’‘(ğ‘–)
â¢
â¥
ğ‘˜,ğ‘›
â¢
â¥
âŸâŸâŸ
â¢
â¥
â£
â¦
=âˆ¶ğ’šğ‘˜,ğ‘–

(22)

ğ‘– = 0, 1, â€¦ , where ğ‘° ğ‘› denotes the ğ‘› Ã— ğ‘› identity matrix and ğ’šğ‘˜,ğ‘– the extended state. The associated optimal controller is given in
the following Theorem.

Theorem 2. (Optimal Tracking Control Law) Let a reference (2) with shift matrix ğ‘» (ğ‘–) as in Deï¬nition 1 be given. Then,

(i) the optimal controller which minimizes (21) subject to the system dynamics (20) and the reference is linear w.r.t. ğ’šğ‘˜,ğ‘– (cf.

(22)) and can be stated as

ğ‘– = 0, 1, â€¦ . Here, the optimal gain ğ‘³ is given by

ğ‘³ = (ğ›¾ Ìƒğ‘©âŠº Ìƒğ‘º Ìƒğ‘© + ğ‘¹)âˆ’1ğ›¾ Ìƒğ‘©âŠº Ìƒğ‘º Ìƒğ‘¨,

ğ…âˆ—(ğ’™ğ‘˜+ğ‘–, ğ‘· (ğ‘–)

ğ‘˜ ) = ğ’–âˆ—

ğ‘˜+ğ‘– = âˆ’ğ‘³ğ’šğ‘˜,ğ‘–,

(23)

(24)

(27)

(28)

KÃ¶pf ET AL.

where

7

(25)

,

ğ‘¨ ğŸ â‹¯ ğŸ
ğŸ ğ‘» (1)âŠº â‹¯ ğŸ
â‹® â‹± â‹®
â‹®
ğŸ â‹¯ ğ‘» (1)âŠº
ğŸ

ğ‘©
ğŸ
â‹®
ğŸ

â¤
â¥
â¥
â¥
â¥
â¦

Ìƒğ‘© = â¡
â¢
â¢
â¢
â¢
â£

,
â¤
â¥
â¥
â¥
â¥
â¦

Ìƒğ‘¨ = â¡
â¢
â¢
â¢
â¢
â£

Ìƒğ‘¨ âˆˆ â„ğ‘›(ğ‘+1)Ã—ğ‘›(ğ‘+1), Ìƒğ‘© âˆˆ â„ğ‘›(ğ‘+1)Ã—ğ‘š, Ìƒğ‘¸ = ğ‘´ âŠºğ‘¸ğ‘´ and Ìƒğ‘º denotes the solution of the discrete-time algebraic Riccati equation
Ìƒğ‘º = ğ›¾ Ìƒğ‘¨âŠº Ìƒğ‘º Ìƒğ‘¨ âˆ’ ğ›¾ Ìƒğ‘¨âŠº Ìƒğ‘º Ìƒğ‘©(ğ‘¹ + Ìƒğ‘©âŠº Ìƒğ‘º Ìƒğ‘©)âˆ’1 Ìƒğ‘©âŠº Ìƒğ‘º Ìƒğ‘¨ + Ìƒğ‘¸.

(26)

(ii) Furthermore, under Assumptions 1â€“2, the optimal controller ğ…âˆ—(ğ’™ğ‘˜+ğ‘–, ğ‘· (ğ‘–)

ğ‘˜ ) exists and is unique.

Proof. (i) With (22), the discounted cost (21) can be reformulated as

Furthermore, note that with (20) and (5)

ğ½ğ‘˜ =

âˆ

âˆ‘ğ‘–=0

ğ›¾ ğ‘–

ğ’šğ‘˜,ğ‘–

âŠºğ‘´ âŠºğ‘¸ğ‘´ ğ’šğ‘˜,ğ‘– + ğ’–âŠº

ğ‘˜+ğ‘–ğ‘¹ğ’–ğ‘˜+ğ‘–

[

.

]

ğ‘¨ğ’™ğ‘˜+ğ‘– + ğ‘©ğ’–ğ‘˜+ğ‘–
ğ‘» (1)âŠºğ’‘(ğ‘–)
ğ‘˜,1
â‹®
ğ‘» (1)âŠºğ’‘(ğ‘–)
ğ‘˜,ğ‘›

â¤
â¥
â¥
â¥
â¥
â¦

ğ’šğ‘˜,ğ‘–+1 = â¡
â¢
â¢
â¢
â¢
â£

= Ìƒğ‘¨ğ’šğ‘˜,ğ‘– + Ìƒğ‘©ğ’–ğ‘˜+ğ‘–

holds. With ğ›¾, Ìƒğ‘¨, Ìƒğ‘©, Ìƒğ‘¸ and ğ‘¹, a standard discounted LQ regulation problem results from (27) for the extended state ğ’šğ‘˜,ğ‘–.
ğ›¾ Ìƒğ‘©, Ìƒğ‘¸ and ğ‘¹ (cf. Gaitsgory
Considering that the discounted problem is equivalent to the undiscounted problem with
et al. 20), the given problem can be reformulated to a standard undiscounted LQ problem. For the latter, it is well-known that the
optimal controller is linear w.r.t. the state (here ğ’šğ‘˜,ğ‘–) and the optimal gain is given by (24) (see e.g. Lewis et al., Section 2.4 21),
thus (23) holds and the ï¬rst theorem assertion follows.

ğ›¾ Ìƒğ‘¨,

âˆš

âˆš

(ii) For the second theorem assertion, we note that the stabilizability of (

ğ›¾ Ìƒğ‘¨,

In addition, ğ‘¸ âª° ğŸ yields Ìƒğ‘¸ âª° ğŸ. As (ğ‘ª, ğ‘¨) is detectable (Assumption 1), with Ìƒğ‘ª âŠº Ìƒğ‘ª = Ìƒğ‘¸, ( Ìƒğ‘ª,
all additional states in Ìƒğ‘¨ compared to ğ‘¨ are stable due to Assumption 2. Finally, due to Ìƒğ‘¸ âª° ğŸ, ğ‘¹ â‰» ğŸ, (
and ( Ìƒğ‘ª,

ğ›¾ Ìƒğ‘¨) detectable, a unique stabilizing solution exists KuÄera, Theorem 8. 19

âˆš

âˆš

âˆš

ğ›¾ Ìƒğ‘©) directly follows from Assumptions 1â€“2.
ğ›¾ Ìƒğ‘¨) is also detectable, because
ğ›¾ Ìƒğ‘©) stabilizable

ğ›¾ Ìƒğ‘¨,

âˆš

âˆš

âˆš

Note 4. The proof of Theorem 2 demonstrates that in case of known system dynamics by means of ğ‘¨ and ğ‘©, the optimal tracking
ğ›¾ Ìƒğ‘©, Ìƒğ‘¸
controller ğ‘³ can be directly calculated by solving the discrete-time algebraic Riccati equation 22 associated with
and ğ‘¹.

ğ›¾ Ìƒğ‘¨,

âˆš

âˆš

Equation (28) demonstrates that the Markov property holds (cf. Note 1). As a consequence of Theorem 2, for unknown system

dynamics, this yields the following problem in the LQ PRADP case.

Problem 2. For ğ‘– = 0, 1, â€¦ , ï¬nd the linear extended state feedback control (23) minimizing (21) and apply ğ’–âˆ—
unknown system (20).

ğ‘˜ = âˆ’ğ‘³ğ’šğ‘˜,0 to the

Before we derive the control law ğ‘³, we analyze the structure of Ì‚ğ‘„âˆ—

Lemma.

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜

(

)

associated with Problem 2 in the following

Lemma 2. (Structure of the Tracking Q-Function) The Q-function associated with Problem 2 has the form

ğ‘„âˆ—(ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜) = ğ’›âŠº

ğ‘˜ğ‘¯ğ’›ğ‘˜ =

âŠº

ğ’™ğ‘˜
ğ’–ğ‘˜
â¡
ğ’‘ğ‘˜,1âˆ¶ğ‘›
â¢
â¢
â£

â¤
â¥
â¥
â¦

â¡
â¢
â¢
â£

ğ’‰ğ‘¥ğ‘¥ ğ’‰ğ‘¥ğ‘¢ ğ’‰ğ‘¥ğ‘
ğ’‰ğ‘¢ğ‘¥ ğ’‰ğ‘¢ğ‘¢ ğ’‰ğ‘¢ğ‘
ğ’‰ğ‘ğ‘¥ ğ’‰ğ‘ğ‘¢ ğ’‰ğ‘ğ‘

ğ’™ğ‘˜
ğ’–ğ‘˜
â¡
ğ’‘ğ‘˜,1âˆ¶ğ‘›
â¢
â¢
â£

â¤
â¥
â¥
â¦

and ğ‘¯ is chosen such that ğ‘¯ = ğ‘¯ âŠº.

,

â¤
â¥
â¥
â¦

(29)

where ğ’›ğ‘˜ =

ğ‘˜ ğ’–âŠº
ğ’™âŠº
[

ğ‘˜ ğ’‘âŠº

ğ‘˜,1âˆ¶ğ‘›

âŠº

=

]

ğ‘˜ ğ’–âŠº
ğ’™âŠº
[

ğ‘˜ ğ’‘âŠº

ğ‘˜,1 â€¦ ğ’‘âŠº

ğ‘˜,ğ‘›

âŠº

]

KÃ¶pf ET AL.

(30)

8

Proof. With (6) and (7)

ğ‘„âˆ—(ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜) = ğ‘

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ’“(ğ‘· ğ‘˜, 0)
(

âˆ

+

ğ›¾ ğ‘–ğ‘

ğ’™ğ‘˜+ğ‘–, ğ…âˆ—
(

ğ’™ğ‘˜+ğ‘–, ğ‘· (ğ‘–)
(

ğ‘˜

, ğ’“(ğ‘· ğ‘˜, ğ‘–)
)

)
follows. With (20), (23) and (5) it is obvious that the states ğ’™ğ‘˜+ğ‘– and controls ğ…âˆ—(ğ’™ğ‘˜+ğ‘–, ğ‘· (ğ‘–)
ğ‘˜ ) are linear w.r.t. ğ’›ğ‘˜, âˆ€ğ‘– = 0, 1, â€¦ .
From this linear dependency and with (22), linearity of ğ’†ğ‘˜,ğ‘– w.r.t. ğ’›ğ‘˜, âˆ€ğ‘– = 0, 1, â€¦ results. Due to the linear dependencies of ğ’†ğ‘˜,ğ‘–
and ğ…âˆ—(â‹…) and the quadratic structure of ğ‘(â‹…) in (21), the Q-function in (30) is quadratic w.r.t. ğ’›ğ‘˜, thus (29) holds.

âˆ‘ğ‘–=1

)

As a consequence of Lemma 2, ğ‘„âˆ— can be exactly parametrized by means of Ì‚ğ‘„âˆ— according to (11) if Ì‚ğ’˜ = ğ’˜ corresponds to
the non-redundant elements of ğ‘¯ = ğ‘¯ âŠº (doubling elements of Ì‚ğ’˜ associated with oï¬€-diagonal elements of ğ‘¯) and ğ“ = ğ’›ğ‘˜ âŠ— ğ’›ğ‘˜,
where âŠ— denotes the Kronecker product. Based on Lemma 2, the optimal control law is given as follows.

Theorem 3. (Optimal Tracking Control Law in Terms of ğ‘¯) The unique optimal extended state feedback control minimizing
ğ½ğ‘˜ (21) is given by

Proof. According to Lemma 1, the desired control ğ’–âˆ—
the necessary condition

ğ‘˜ = ğ…âˆ—(ğ’™ğ‘˜, ğ‘· ğ‘˜) = âˆ’ğ‘³ğ’šğ‘· ğ‘˜
ğ’–âˆ—

ğ‘˜ = âˆ’ğ’‰âˆ’1

ğ‘¢ğ‘¢

ğ’‰ğ‘¢ğ‘¥ ğ’‰ğ‘¢ğ‘

[

]

ğ’™ğ‘˜
ğ’‘ğ‘˜,1âˆ¶ğ‘›]
[

.

(31)

ğ‘˜ minimizing ğ‘„âˆ—(ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜) is also minimizing ğ½ğ‘˜. With (29) and ğ‘¯ = ğ‘¯ âŠº,

yields the control ğ’–âˆ—

ğ‘˜ given in (31). Furthermore,

ğœ•ğ‘„âˆ—(ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜)
ğœ•ğ’–ğ‘˜

= 2

ğ’‰ğ‘¢ğ‘¥ğ’™ğ‘˜ + ğ’‰ğ‘¢ğ‘ğ’‘ğ‘˜,1âˆ¶ğ‘› + ğ’‰ğ‘¢ğ‘¢ğ’–ğ‘˜

(

!
= ğŸ

)

ğœ•2ğ‘„âˆ—(ğ’™ğ‘˜, ğ’–ğ‘˜, ğ‘· ğ‘˜)
ğœ•ğ’–2
ğ‘˜

= 2ğ’‰ğ‘¢ğ‘¢

(32)

(33)

demonstrates that ğ’‰ğ‘¢ğ‘¢ â‰» ğŸ is required in order to ensure that the control ğ’–âˆ—
following. Therefore, let ğ‘„âˆ—
Then, it is obvious that

ğ‘˜ (31) minimizes ğ½ğ‘˜ (21). This will be shown in the
reg(ğ’™ğ‘˜, ğ’–ğ‘˜) be the optimal Q-function related to the regulation case, i.e. where ğ’“(ğ‘· ğ‘˜, ğ‘–) = ğ’“(ğŸ, ğ‘–) = ğŸ.

âˆ€ğ’™ğ‘˜ âˆˆ â„ğ‘›, ğ’–ğ‘˜ âˆˆ â„ğ‘š, must be true. Furthermore, for the regulation case, it is well-known that

ğ‘„âˆ—(ğ’™ğ‘˜, ğ’–ğ‘˜, ğŸ) = ğ‘„âˆ—

reg(ğ’™ğ‘˜, ğ’–ğ‘˜),

ğ’‰reg,ğ‘¥ğ‘¥ ğ’‰reg,ğ‘¥ğ‘¢
ğ›¾ğ‘©âŠºğ‘ºğ‘¨ ğ›¾ğ‘©âŠºğ‘ºğ‘© + ğ‘¹][
ğ’‰reg,ğ‘¢ğ‘¥ ğ’‰reg,ğ‘¢ğ‘¢][
[
holds (see e.g. Bradtke et al. 23). Here, ğ‘º is the solution of the discrete-time algebraic Riccati equation

reg(ğ’™ğ‘˜, ğ’–ğ‘˜) =

ğ›¾ğ‘¨âŠºğ‘ºğ‘¨ + ğ‘¸ ğ›¾ğ‘¨âŠºğ‘ºğ‘©

ğ’™ğ‘˜
ğ’–ğ‘˜]
[

ğ’™ğ‘˜
ğ’–ğ‘˜]
[

ğ’™ğ‘˜
ğ’–ğ‘˜]

ğ‘„âˆ—

=

[

ğ’™ğ‘˜
ğ’–ğ‘˜]

âŠº

âŠº

Under Assumption 1, ğ‘º = ğ‘º âŠº âª° ğŸ exists and is unique (KuÄera, Theorem 8 19). Thus, from (34) and (35),

ğ‘º = ğ›¾ğ‘¨âŠºğ‘ºğ‘¨ âˆ’ ğ›¾ğ‘¨âŠºğ‘ºğ‘©(ğ‘¹ + ğ‘©âŠºğ‘ºğ‘©)âˆ’1ğ‘©âŠºğ‘ºğ‘¨ + ğ‘¸.

results. This completes the proof.

Thus, if ğ‘¯ (or equivalently ğ’˜) is known, both ğ‘„âˆ— and ğ…âˆ— can be calculated.

ğ’‰ğ‘¢ğ‘¢ = ğ’‰reg,ğ‘¢ğ‘¢ = ğ›¾ğ‘©âŠºğ‘ºğ‘© + ğ‘¹ â‰» ğŸ

(34)

(35)

(36)

(37)

4

RESULTS

In order to validate our proposed PRADP tracking method, we show simulation results in the following, where the reference
trajectory is parametrized by means of cubic splines1. Furthermore, we compare the results with an ADP tracking method from
literature which assumes that the reference can be described by a time-invariant exo-system ğ’‡ ref(ğ’“ğ‘˜). Finally, we compare our
learned controller that does not know the system dynamics with the ground truth controller which is calculated based on full
system knowledge.

1Other approximations can be used by choosing diï¬€erent basis functions ğ†(ğ‘–) (e.g. linear interpolation with ğ†(ğ‘–) =

âŠº

ğ‘–ğ‘‡ 1
]

[

or zero-order hold with ğ†(ğ‘–) = 1).

KÃ¶pf ET AL.

9

4.1

Cubic Polynomial Reference Parametrization

We choose ğ’“(ğ‘· ğ‘˜, ğ‘–) to be a cubic polynomial w.r.t. ğ‘–, i.e. ğ†(ğ‘–) =
The associated transformation in order to obtain the shifted version ğ‘· (ğ‘–)
following:

[

âŠº

(ğ‘–ğ‘‡ )3 (ğ‘–ğ‘‡ )2 ğ‘–ğ‘‡ 1

, where ğ‘‡ denotes the sampling time.
ğ‘˜ of ğ‘· ğ‘˜ according to Deï¬nition 1 thus results from the

]

((ğ‘– + ğ‘—)ğ‘‡ )3
((ğ‘– + ğ‘—)ğ‘‡ )2
(ğ‘– + ğ‘—)ğ‘‡
1

ğ’“(ğ‘· ğ‘˜, ğ‘– + ğ‘—) = ğ‘· ğ‘˜ğ†(ğ‘– + ğ‘—) = ğ‘· ğ‘˜ â¡
â¢
â¢
â¢
â¢
â£

1 3ğ‘–ğ‘‡ 3(ğ‘–ğ‘‡ )2 (ğ‘–ğ‘‡ )3
2ğ‘–ğ‘‡ (ğ‘–ğ‘‡ )2
0 1
= ğ‘· ğ‘˜ â¡
â¤
1
0 0
â¢
â¥
â¢
â¥
0
0 0
â¢
â¥
âŸââââââââââââââŸââââââââââââââŸ
â¢
â¥
â£
â¦
ğ‘» (ğ‘–)

ğ‘–ğ‘‡
1

â¤
â¥
â¥
â¥
â¥
â¦

ğ†(ğ‘—) = ğ‘· (ğ‘–)

ğ‘˜ ğ†(ğ‘—).

(38)

In order to fully describe ğ’“(ğ‘· ğ‘˜, ğ‘–), the values of ğ‘· ğ‘˜ remain to be determined. Therefore, given sampling points of the reference
trajectory every ğ‘‘ = 25 time steps, let ğ‘· ğ‘˜, ğ‘˜ = ğ‘‘ğ‘—, ğ‘— = 0, 1, â€¦ , result from cubic spline interpolation. In between the sampling
points, let ğ‘· ğ‘˜+ğ‘– = ğ‘· (ğ‘–)
ğ‘˜ , ğ‘– = 1, 2, â€¦ , ğ‘‘ âˆ’ 1 (cf. Deï¬nition 1 and (38)). This way, the controller is provided with ğ‘· ğ‘˜ at each time
step ğ‘˜ when facing Problem 2.

Note 5. The given procedure to generate parameters ğ‘· ğ‘˜ decouples the sampling time of the controller from the availability of
sampling points given for the reference trajectory (in our example only every ğ‘‘ = 25 time steps).

4.2

Example System

Consider a mass-spring-damper system with ğ‘šsys = 0.5 kg, ğ‘sys = 0.1 N mâˆ’1 and ğ‘‘sys = 0.1 kg sâˆ’1. Discretization of this system
using Tustin approximation with ğ‘‡ = 0.1 s yields

ğ’™ğ‘˜+1 =

0.9990 0.0990
âˆ’0.0198 0.9792]

[

ğ’™ğ‘˜ +

0.0099
0.1979]
[

ğ‘¢ğ‘˜.

Here, ğ‘¥1 corresponds to the position, ğ‘¥2 to the velocity of the mass ğ‘šsys and the control ğ‘¢ğ‘˜ corresponds to a force.

We desire to track the position (i.e. ğ‘¥1), thus we set

ğ‘¸ =

100 0
0 0]

[

and ğ‘¹ = 1

(39)

(40)

in order to strongly penalize the deviation of the ï¬rst state from the parametrized reference (cf. (21)) and ğ›¾ = 0.9. In this example
setting, Assumptions 1â€“2 hold.

4.3

Simulations

In order to investigate the beneï¬ts of our proposed PRADP tracking controller, we compare our method with an ADP tracking
controller from literature, 2,3 which assumes that the reference trajectory is generated by a time-invariant exo-system ğ’‡ ref(ğ’“ğ‘˜).
Both our method (with ğ‘’ Ì‚ğ’˜ = 1 Ã— 10âˆ’5 in Algorithm 1) and the comparison method from literature are trained on data of 500
time steps, where Gaussian noise with zero mean and standard deviation of 1 is applied to the system input ğ‘¢ğ‘˜ for excitation.
Note that none of the methods requires the system dynamics (20). Let ğ’“0 =
. The reference trajectory during training is

âŠº

ğ‘Ÿğ‘˜+1,1
ğ‘Ÿğ‘˜+1,2]
[

= ğ’“ğ‘˜+1 = ğ’‡ ref(ğ’“ğ‘˜) =

ğ’“ğ‘˜

(41)

0 1
]

[

0.9988 0.0500
âˆ’0.0500 0.9988]
[
âŸâââââââââââŸâââââââââââŸ
ğ‘­ ref

for the comparison method and the associated spline for our method.

The learned controllers both of our method and the comparison algorithm are tested on a reference trajectory for ğ‘¥1 that equals
the sine described by ğ‘Ÿğ‘˜,1 according to (41) for the ï¬rst 250 time steps. Then, the reference trajectory deviates from this sine as is
depicted in Fig. 1 in gray. Here, the blue crosses mark the sampling points for spline interpolation, the black dashed line depicts
ğ‘¥1 resulting from our proposed method and the red dash-dotted line shows ğ‘¥1 for the comparison method. Furthermore, to gain
insight into the tracking quality by means of the resulting cost, ğ‘
is depicted in Fig. 2 for both methods. Note the
logarithmic ordinate which is chosen in order to render the black line representing the cost associated with our method visible.
(

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ’“(ğ‘· ğ‘˜, 0)

)

10

The optimal controller ğ‘³ calculated using full system information (see Theorem 2 and Note 4) results in

ğ‘³ =

6.30 2.26 âˆ’0.31 âˆ’0.97 âˆ’2.37 âˆ’6.40 0 0 0 0

.

KÃ¶pf ET AL.

(42)

[
= 1.08 Ã— 10âˆ’13. Thus, the
Comparing the learned controller ğ‘³PRADP with the ground truth solution ğ‘³ yields
learned controller is almost identical to the ground truth solution which demonstrates that the optimal tracking controller has
â€–
â€–
successfully been learned using PRADP without knowledge of the system dynamics.

ğ‘³PRADP âˆ’ ğ‘³

â€–
â€–

]

4.4

Discussion

As can be seen from Fig. 1, our proposed method successfully tracks the parametrized reference trajectory. In contrast, the method
proposed by e.g. Luo et al. 2 and Kiumarsi et al. 3 causes major deviation from the desired trajectory as soon as the reference
does not follow the same exo-system which it was trained on (i.e. as soon as (41) does not hold anymore after 250 time steps).
In addition, the cost in Fig. 2 reveals that both methods yield small and similar costs as long as the reference trajectory follows
ğ‘­ ref. However, as soon as the reference trajectory deviates from the time-invariant exo-system description ğ‘­ ref at ğ‘˜ > 250, the
cost of the comparison method drastically exceeds the cost associated with our proposed method. With

max
ğ‘˜

ğ‘exo-system

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ’“(ğ‘· ğ‘˜, 0)

â‰ˆ 270

and max

ğ‘˜

ğ‘PRADP

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ’“(ğ‘· ğ‘˜, 0)

â‰ˆ 2.8,

(

)

(

)

our method clearly outperforms the comparison method. PRADP does not require the assumption that the reference trajectory
follows time-invariant exo-system dynamics but is nevertheless able to follow this kind of reference (see ğ‘˜ â‰¤ 250 in the simula-
tions) as well as all other references that can be approximated by means of the time-varying parameter ğ‘· ğ‘˜. Thus, PRADP can
be interpreted as a more generalized tracking approach compared to existing ADP tracking methods.

sampling points
ğ‘¥1,PRADP

ğ‘Ÿ1(ğ‘· ğ‘˜, 0) (reference)
ğ‘¥1, exo-system

6

4

2

0

âˆ’2

,

ğ‘˜
1
ğ‘¥

0

100

200

300

400
time step ğ‘˜

500

600

700

800

FIGURE 1 Tracking results of our proposed method compared with a state of the art ADP tracking controller.

KÃ¶pf ET AL.

11

ğ‘PRADP

ğ‘exo-system

103

102

101

100

)

10âˆ’1

10âˆ’2

10âˆ’3

(

)
0

,
ğ‘˜
ğ‘·
(
ğ’“
,
ğ‘˜
ğ’–

,
ğ‘˜
ğ’™

ğ‘

10âˆ’4

10âˆ’5

10âˆ’6

10âˆ’7

0

100

200

300

400
time step ğ‘˜

500

600

700

800

FIGURE 2 One-step cost ğ‘
ordinate.

(

ğ’™ğ‘˜, ğ’–ğ‘˜, ğ’“(ğ‘· ğ‘˜, 0)

)

both for our proposed method and the comparison method. Note the logarithmic

5

CONCLUSION

In this paper, we proposed a new ADP-based tracking controller termed Parametrized Reference Adaptive Dynamic Program-
ming (PRADP). This method implicitly incorporates the approximated reference trajectory information into the Q-function that
is learned. This allows the controller to track time-varying parametrized references once the controller has been trained and does
not require further adaptation or re-training compared to previous methods. Simulation results showed that our learned con-
troller is more ï¬‚exible compared to state-of-the-art ADP tracking controllers which assume that the reference to track follows a
time-invariant exo-system. Motivated by a straightforward choice of basis functions, we concentrated on the LQ tracking case in
our simulations where the optimal controller has successfully been learned. However, as the mechanism of PRADP allows more
general tracking problem formulations (see Section 3), general function approximators can be used in order to approximate ğ‘„
and allow for nonlinear ADP tracking controllers in the future.

References

1. Lewis F, Vrabie D. Reinforcement learning and adaptive dynamic programming for feedback control. IEEE Circuits and

Systems Magazine 2009; 9(3): 32â€“50. doi: 10.1109/MCAS.2009.933854

2. Luo B, Liu D, Huang T, Wang D. Model-Free Optimal Tracking Control via Critic-Only Q-Learning. IEEE Transactions

on Neural Networks and Learning Systems 2016; 27(10): 2134â€“2144. doi: 10.1109/TNNLS.2016.2585520

3. Kiumarsi B, Lewis FL, Modares H, Karimpour A, Naghibi-Sistani MB. Reinforcement Q-learning for optimal
tracking control of linear discrete-time systems with unknown dynamics. Automatica 2014; 50(4): 1167â€“1175.
doi: 10.1016/j.automatica.2014.02.015

12

KÃ¶pf ET AL.

4. KÃ¶pf F, Ebbert S, Flad M, Hohmann S. Adaptive Dynamic Programming for Cooperative Control with Incomplete

Information. In: 2018 IEEE International Conference on Systems, Man and Cybernetics (SMC); 2018.

5. Dierks T, Jagannathan S. Optimal tracking control of aï¬ƒne nonlinear discrete-time systems with unknown internal dynam-
ics. In: 2009 Joint 48th IEEE Conference on Decision and Control (CDC) and 28th Chinese Control Conference (CCC);
2009: 6750â€“6755

6. Modares H, Lewis FL. Linear Quadratic Tracking Control of Partially-Unknown Continuous-Time Systems Using Rein-
forcement Learning. IEEE Transactions on Automatic Control 2014; 59(11): 3051â€“3056. doi: 10.1109/TAC.2014.2317301

7. Zhang K, Zhang H, Xiao G, Su H. Tracking control optimization scheme of continuous-time nonlinear system via online
single network adaptive critic design method. Neurocomputing 2017; 251: 127â€“135. doi: 10.1016/j.neucom.2017.04.008

8. van Nieuwstadt MJ. Trajectory Generation for Nonlinear Control Systems. Dissertation. California Institute of Technology,

1997.

9. Kiumarsi B, Lewis FL, Levine DS. Optimal control of nonlinear discrete time-varying systems using a new neural network

approximation structure. Neurocomputing 2015; 156: 157â€“165. doi: 10.1016/j.neucom.2014.12.067

10. Sutton RS, Barto AG. Reinforcement Learning: An introduction. Cambridge Massachusetts: MIT Press. 2nd ed. 2018.

11. KÃ¶pf F, Westermann J, Flad M, Hohmann S. Adaptive Optimal Control for Reference Tracking Independent of Exo-System

Dynamics. arXiv e-prints 2019: arXiv:1906.05085.

12. Sutton RS. Learning to Predict by the Methods of Temporal Diï¬€erences. Machine learning 1988; 3(1): 9â€“44.

doi: 10.1023/A:1022633531479

13. Lagoudakis MG, Parr R. Least-squares policy iteration. Journal of machine learning research 2003; 4: 1107â€“1149.

14. BuÅŸoniu L, BabuÅ¡ka R, Schutter dB, Ernst D. Reinforcement learning and dynamic programming using function approxi-

mators. 39. CRC press . 2010.

15. Hornik K, Stinchcombe M, White H. Universal approximation of an unknown mapping and its derivatives using multilayer

feedforward networks. Neural Networks 1990; 3(5): 551â€“560. doi: 10.1016/0893-6080(90)90005-6

16. Ã…strÃ¶m KJ, Wittenmark B. Adaptive control. Reading, Mass.: Addison-Wesley. 2nd ed. 1995.

17. Bertsekas DP, Tsitsiklis JM. Neuro-Dynamic Programming. Belmont, Massachusetts: Athena Scientiï¬c . 1996.

18. Wang D, He H, Liu D. Adaptive Critic Nonlinear Robust Control: A Survey. IEEE transactions on cybernetics 2017; 47(10):

3429â€“3451. doi: 10.1109/TCYB.2017.2712188

19. KuÄera V. The discrete Riccati equation of optimal control. Kybernetika 1972; 8(5): 430â€“447.

20. Gaitsgory V, GrÃ¼ne L, HÃ¶ger M, Kellett CM, Weller SR. Stabilization of strictly dissipative discrete time systems with

discounted optimal control. Automatica 2018; 93: 311â€“320. doi: 10.1016/j.automatica.2018.03.076

21. Lewis FL, Vrabie DL, Syrmos VL. Optimal control. Hoboken: Wiley. 3rd ed. ed. 2012.

22. Arnold WF, Laub AJ. Generalized eigenproblem algorithms and software for algebraic Riccati equations. Proceedings of

the IEEE 1984; 72(12): 1746â€“1754. doi: 10.1109/PROC.1984.13083

23. Bradtke SJ, Ydstie BE, Barto AG. Adaptive linear quadratic control using policy iteration. In: Proceedings of the 1994

American Control Conference; 1994: 3475â€“3479.

KÃ¶pf ET AL.

13

How to cite this article: KÃ¶pf, F., S. Ramsteiner, M. Flad, and S. Hohmann (2019), Adaptive Dynamic Programming for
Model-free Tracking of Trajectories with Time-varying Parameters, Int J Adapt Control Signal Process, 2020.

