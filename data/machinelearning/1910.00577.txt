Structural Language Models of Code

Uri Alon 1 Roy Sadaka 1 Omer Levy 2 3 Eran Yahav 1

0
2
0
2

l
u
J

9
2

]

G
L
.
s
c
[

4
v
7
7
5
0
0
.
0
1
9
1
:
v
i
X
r
a

Abstract
We address the problem of any-code comple-
tion – generating a missing piece of source
code in a given program without any restric-
tion on the vocabulary or structure. We intro-
duce a new approach to any-code completion
that leverages the strict syntax of programming
languages to model a code snippet as a tree –
structural language modeling (SLM). SLM es-
timates the probability of the program’s abstract
syntax tree (AST) by decomposing it into a prod-
uct of conditional probabilities over its nodes.
We present a neural model that computes these
conditional probabilities by considering all AST
paths leading to a target node. Unlike previ-
ous techniques that have severely restricted the
kinds of expressions that can be generated in this
task, our approach can generate arbitrary code in
any programming language. Our model signif-
icantly outperforms both seq2seq and a variety
of structured approaches in generating Java and
C# code. Our code, data, and trained models are
available at http://github.com/tech-srl/
slm-code-generation/. An online demo is
available at http://AnyCodeGen.org.

1. Introduction

Code completion is the problem of generating code given
its surrounding code as context. In its most general form,
this problem is extremely challenging as it requires reason-
ing over an unbounded number of syntactic structures and
user-deﬁned symbols. Previous approaches have avoided
this issue by limiting the generation problem: program
synthesis approaches are often tailored to domain-speciﬁc
languages (Gulwani, 2011; Polozov & Gulwani, 2015; De-
vlin et al., 2017; Ellis et al., 2019), while other recent ap-
proaches generate code in general languages like Java and

Israel

1Technion,
3Facebook AI Research.
Alon
<roysadaka@gmail.com>,
levy@gmail.com>, Eran Yahav <yahave@cs.technion.ac.il>.

2Tel
Correspondence
Roy
Levy

<urialon@cs.technion.ac.il>,
Omer

University
Uri
to:
Sadaka
<omer-

Aviv

C#, but severely restrict the syntax, vocabulary, domain,
or nature of the generated programs (Murali et al., 2018;
Brockschmidt et al., 2019; Young et al., 2019).

We introduce the task of any-code completion – generating
code in a general-purpose programming language without
any restriction on its vocabulary or structure. Speciﬁcally,
we focus on generating code in context: given a program
P and some part of the program p, the task is to predict
p from the rest of the program P −=P\p. Any-code com-
pletion thus generalizes the restricted completion task of
Brockschmidt et al. (2019), in which the target code con-
tained only primitive types (e.g., int and string) and ex-
cluded user-deﬁned functions. Figure 1 shows two any-
code completion examples.

In related tasks such as semantic parsing (Dong & Lapata,
2018; Yu et al., 2018; Iyer et al., 2019), natural-language-
to-code (Allamanis et al., 2015; Iyer et al., 2018), and edit-
to-code (Yin et al., 2019; Zhao et al., 2019), models must
use separate encoders and decoders because of the different
modalities of the input (e.g. natural language text) and the
output (code). In contrast, we leverage the fact that our in-
put and output are of the same modality (code), and pursue
better generalization by modeling them jointly.

We present a new approach that explicitly models the
source and the target code as the same tree – structural
language modeling (SLM). SLM estimates the probability
of the program’s abstract syntax tree (AST) by decompos-
ing it into a product of conditional probabilities over its
nodes. We present a neural model that computes these con-
ditional probabilities by considering all AST paths lead-
ing to a target node, generalizing over traditional language
models that consider sequences of words. While prior work
uses AST paths to read programs (Alon et al., 2019b), we
generate code by predicting the next node along the set of
paths, generating the target AST node-by-node.

We evaluate SLMs on Java any-code completion, achieving
a new state of the art: exact-match accuracy@1 of 18.04%
and accuracy@5 of 24.83% (previous SOTA: 16.93% and
23.17%). SLMs also outperform existing models in the re-
stricted completion task of Brockschmidt et al. (2019) in
C# by a wide margin, 37.61% accuracy@1 compared to
26.42%. Our ablation study reveals the importance of joint
modeling of the source and target code, rather than sep-

 
 
 
 
 
 
Structural Language Models of Code

public static Path[] stat2Paths(

public static string Camelize(

FileStatus[] stats) {

if (stats == null) return null;
Path[] ret = new Path[stats.length];
for (int i = 0; i < stats.length; ++i){

ret[i] =

}
return ret;

}

;

this string input)

var word = input.Pascalize();
return word.Length > 0 ?

+ word.Substring(1)

: word;

.ToLower()

{

}

True ref (Java):

stats[i].getPath()

True ref (C#):

word.Substring(0, 1)

SLM
top-5:

(25.2%)
(3.3%)
(2.5%)
(1.7%)
(0.8%)

stats[i].getPath()
Path(stats[i])
new Path(stats[i], charset)
stat(stats[i], ret)
new Path(stats[i])

SLM
top-5:

(14.1%)
(8.2%)
(5.8%)
(2.4%)
(1.9%)

word.Substring(0, 1)
word.trim()
word.Substring(1)
input.Substring(0, 1)
wordValue.Substring(0, 1)

(a)

(b)

Figure 1. Examples from the Java (left) and C# (right) test sets. The highlighted expression in each example is the target p, which our
models correctly generated from the rest of the snippet. Additional and larger examples can be found in Appendices F and G.

arating encoders from decoders. Finally, we discuss the
theoretical advantages of SLMs, and show how they gen-
eralize many previous structural approaches for code gen-
eration. An interactive demo of our model is presented at
http://AnyCodeGen.org.

2. Code Generation as Structural Language

Modeling

We model the task of any-code completion by comput-
ing the probability of a program P r (P), similar to how a
language model computes the probability of a natural lan-
guage sentence. While language models typically assume a
sequence as their input, our input is an abstract syntax tree
AP . We thus introduce a structural language modeling ap-
proach (SLM).

The intuition behind this idea is that a language model
could generalize better by modeling the tree rather than the
sequential form of the program. Further, learning from the
AST allows a model to save learning capacity, instead of
having to re-learn known syntactic patterns from the text.

We ﬁrst show a chain-rule decomposition of the tree’s prob-
ability P r (AP ) into a product of conditional node proba-
bilities, and then describe our path-based model for com-
puting the individual conditional probabilities. We explain
how to construct a tree from local node predictions, and ﬁ-
nally discuss how our approach differs from previous work
on production-based tree generation.

Representing Code as a Tree A program P is a sequence
of tokens that can be unambiguously mapped to an abstract
syntax tree (AST) AP , where every node represents an el-
ement in the language (e.g. conditions, loops, variable dec-

larations) from a set T . Each AST leaf (terminal) has an
associated user-deﬁned value v ∈ V. Nonterminal nodes
can have a varying number of children nodes.

Decomposing the Probability of a Tree Given a tree AP ,
we ﬁrst traverse the tree, depth-ﬁrst,1 to induce an ordering
over its nodes a0, . . . , a|AP | ∈ AP . We decompose the
probability of a tree P r (AP ) using the chain rule, akin to
the standard approach in language modeling:

P r (AP ) =

(cid:89)

t

P r (at|a<t)

(1)

where a<t are all the nodes that were traversed before at.

In any-code completion, part of the tree (AP − ) is already
observed. Therefore, we order the nodes of AP − to be be-
fore the nodes of the target p, and compute only the condi-
tional probabilities over the nodes in p, essentially condi-
tioning on the observed tree AP − .

Representing Partial Trees via Paths How can we rep-
resent the partial tree composed of a<t when computing
P r (at|a<t)? In standard language modeling, the structure
is linear, and a<t is a sequence. One way to represent a
partial tree is to linearize it according to the traversal order
(Xiao et al., 2016); however, this creates artiﬁcially long
distances between the current node at and ancestor nodes
(e.g., the root a0). Another option is to use only the path
from the root node to at (Rabinovich et al., 2017), but this
ignores a lot of contextual information (e.g., sibling nodes).

We follow Alon et al. (2018) and use the set of paths from
every leaf to at together with the path from the root to

1Depth-ﬁrst ordering is a common practice in tree generation
(Maddison & Tarlow, 2014; Raychev et al., 2016), but in principle
our framework also allows for other orderings.

Structural Language Models of Code

(a)

(d)

(b)

(e)

(c)

...
if( x > 1 ) {

...

}
...

(f)

Figure 2. The subtree representing x > 1 is generated given its surrounding tree. At each step, the model generates the next node
(denoted by ? ) of path1, path2 and path3 using the root path R. Dashed lines denote the AST structure; solid lines denote AST paths.
Most AST paths are omitted from the ﬁgure, for clarity.

at.
Intuitively, each path captures the effect of a differ-
ent, possibly distant, program element on at, along with
the syntactic relationship between them. For example, in
Figure 1 (left) the three paths originating from Path[]
ret inform the model about the existence of ret which
is an array of type Path. Thus, when completing ret[i]
= ... – the completion should be a Path object. Other
paths inform the model that the target is inside a For loop,
iterated stats.length times. Considering the informa-
tion ﬂowing from all paths, our model correctly generates
stats[i].getPath().

We denote the (candidate) node at time t as at, its (given)
parent, which is currently expanded, by π (at), and the set
of all paths as St:

St = {(cid:96)

π (at) |(cid:96) ∈ leaves (a<t)}

(cid:59)

(cid:59)

π (at) is the (only) path in the tree between a
where (cid:96)
leaf (cid:96) and the current node to expand π (at). We denote the
path from the root of the program as Rt = a0
π (at),
which represents the current, relative position of π (at) in
the program (marked as R in Figure 2). Whereas prior
work used whole paths (between two leaf nodes) to encode
ASTs (Alon et al., 2019a;b), our model observes partial
paths (between a leaf and any other node) and learns to
extend them by predicting their next node.

(cid:59)

Figure 2 illustrates the traversal order of a subtree that rep-
resents the expression x > 1 and some of the paths used
to compute the probability at each step. At each step, the

probability of the next node is computed given the paths St
from the root and every given leaf up to the current node
to expand. Figure 2(d) shows how after the terminal node
with the value x is given, path3 originating from this leaf
is also used to compute the probability of the next nodes.

Our path-based approach generalizes previous approaches
such as “parent feeding” and “previous action” encoding
(Yin & Neubig, 2017), context nodes (Bielik et al., 2016),
and some of the graph-edges of Brockschmidt et al. (2019).
See Section 8 for further discussion.

Generating Trees
In se-
quence generation, the length
of the target sequence is con-
trolled by generating an EOS
token to stop. When generat-
ing trees, we require a more
sophisticated
mechanism
to control arity and depth.
We augment AP in two
ways to allow node-by-node
generation.

Figure 3. Augmenting the
AST with EOSnode and
EOStok nodes.

First, we add a special EOSnode node to every nontermi-
nal to control for arity. Generating this node indicates that
the parent node has no more children nodes. Second, we
end each subtoken sequence with a special EOStok node to
control for depth during generation; we decompose each
terminal node nv into a sequence of terminal nodes Tv
by splitting up the node’s value v into subtokens based on

IfExprMethodRoot?GreaterIfExprMethodRoot?GreaterNameIfExprMethodRoot?GreaterNameIfExprxMethodRoot?GreaterNameIntExpIfExprxMethodRoot?GreaterNameIntExpxEOStok1EOStokEOSnodeStructural Language Models of Code

camel notation. For example, if v = toLowerCase, then
Tv = to → lower → case → EOStok. Figure 3 shows
an example of both EOSnode and EOStok in action.

Node Trees vs. Production Trees While we predict a sin-
gle node at each step, previous work (Iyer et al., 2018;
2019) predicts a grammar production rule. This represen-
tation decomposes the code in a way that often forces the
model to predict with partial information. For instance,
consider generating the expression str.Substring(3).
The model of Brockschmidt et al. (2019) would ﬁrst predict
the rule Expr→Expr.Substring(Expr), and only then
expand Expr→str and Expr→3. That is, the model needs
to predict the method name (Substring) before the invok-
ing object (str). Further, the Substring method can get
either one or two arguments, forcing the model to choose
whether to use the one- or two-argument rule in advance.
Node generation, however, allows us to predict the pres-
ence of a function call and only then to predict its object
and method name, rather than predicting these a priori.

3. Model Architecture

In the previous section, we described how we can gener-
ate code given the probabilities P r (at|a<t), where a<t is
represented by the set of partial AST paths St. Here, we
present a neural model that estimates P r (at|St). We ﬁrst
encode each path in St as a vector (Section 3.1); then, we
contextualize and aggregate the entire set. Finally, we pre-
dict the target node at by combining a subtoken vocabulary
with a syntactic copy mechanism (Section 3.3).

3.1. Encoding AST Paths

Given a partial AST path,
n1, . . . , nk, our goal is to create a vector representation.

i.e., a sequence of nodes

We ﬁrst represent each node ni using embeddings. A
subtoken node is represented by the index of its subtoken
w in the embedding matrix Esubtoken; AST nodes are rep-
resented as a pair ni = (τ, κ) where τ is the node type, e.g.
IfStatement, and κ is the node index among its sibling
nodes. We represent node types using a learned embedding
matrix Etype and the child indices using a learned matrix
Eindex. The node’s vector representation is the concatena-
tion of the type and index vectors.

Given a set of partial paths S (omitting the itera-
tor t for simplicity), we denote their encodings as
H = {(cid:59)f (n1, . . . , nk) | (n1, . . . , nk) ∈ S}.
Efﬁcient Computation When modeling a subtree, there
are large overlaps between paths from different time steps.
In particular, paths that originate from the same leaf share
the same preﬁx. We therefore apply the LSTM on the pre-
ﬁx once and cache the intermediate state across sufﬁxes,
speeding up both training and inference signiﬁcantly. An
example is shown in Figure 7 (supplementary material).

3.2. Aggregating Multiple Paths

Given the set of paths S leading up to the parent π(a) of
the target node a, our goal is to represent S in the context
of predicting a. To do so, we introduce the aggregation
function g (H, r, i). As its input, g takes the set of encoded
paths H, the encoded root path r, and the child index i of
the currently predicted child node a relative to its parent.

We ﬁrst contextualize the path encodings H using a trans-
former encoder (Vaswani et al., 2017).3 In parallel, we ap-
ply a non-linear transformation to the encoding of the root
path r =(cid:59)f (R), in order to inform it that we wish to predict
the i-th child of π(a):

Z = Transformer (H)

(cid:101)r = Wr · ReLU (Ci · r)

In this formulation, the parameter matrix Ci is used when
the child index is i, while the parameter matrix Wr is used
for every instance.

We then compute attention over the set of contextualized
path encodings Z using the index-informed root-path en-
coding (cid:101)r as the query; we pass the weighted average (cid:101)z and
the root-path encoding (cid:101)r through another fully-connected
layer; we denote the resulting vector representation as (cid:101)h:

α = softmax (Z · (cid:101)r)

(cid:101)z =

(cid:88)

j

αj · Zj

(cid:101)h = g (H, r, i) = ReLU (Wg [(cid:101)z; (cid:101)r])

where semicolons (;) denote vector concatenation.

3.3. Predicting with a Syntactic Copy Mechanism

(cid:40)

e (ni) =

w

Esubtoken
(cid:2)Etype

τ

; Eindex
κ

ni is the subtoken w
(cid:3) ni is the AST node (τ, κ)

We can now predict a from the representation (cid:101)h.
If the
target node’s parent π(a) is a nonterminal AST node, then
a must be an AST node; otherwise, a is a subtoken.

We encode the entire path using a uni-directional LSTM
stack, and take the ﬁnal states:2

Predicting AST Nodes If a is an AST node, we predict a
using a softmax over the node type embeddings Etype:

(cid:59)f (n1, . . . , nk) = LSTM (e (n1) , . . . , e (nk))

P r (a|S) = softmax

(cid:16)

(cid:17)
Etype · (cid:101)h

(π(a) is a nonterminal)

2Replacing the LSTMs with transformers yielded similar re-

sults in preliminary experiments.

3Since H is a set, we do not use positional embeddings.

Structural Language Models of Code

Predicting Subtokens Programs repeatedly refer to previ-
ously declared symbols, resulting in highly repetitive us-
age of identiﬁers. We therefore use a copy mechanism
(Gu et al., 2016) to allow our model to predict either en-
tire tokens or individual subtokens that exist in the context.
As we show in Section 6, copying greatly improves our
model’s performance. For brevity, we describe how entire
tokens are copied, and elaborate on the copy of subtokens
in Appendix D. We score each leaf (cid:96) using a bilinear func-
tion (Wc) between its path’s encoding H(cid:96) and (cid:101)h. At the
same time, we score the token w, which is the token as-
sociated with (cid:96), from a limited vocabulary using the inner
product between its representation in the subtoken embed-
ding matrix Esubtoken and (cid:101)h.

scopy ((cid:96)) = H(cid:96) · Wc · (cid:101)h

sgen (w) = Esubtoken

w

· (cid:101)h

The scores scopy and sgen are then summed over all oc-
currences that correspond to the same symbol and subse-
quently normalized via softmax. A key difference from
most previous work (Ling et al., 2016; Yin & Neubig,
2017) is that our copy mechanism uses the syntactic rela-
tion to the source (the path H(cid:96)), rather than the sequential
relation or the graph-node representation (Yin et al., 2019).

4. Experimental Setup

4.1. Benchmarks

Any-Code Completion: Java We take the Java-small
dataset of Alon et al. (2019a), which is a re-split of the
dataset of Allamanis et al. (2016). It contains 11 GitHub
projects, broken down into a single method per example,
and split to train/dev/test by project to reduce code overlap.
This dataset was found to contain the least code duplica-
tion by Allamanis (2019). We create any-code completion
examples by selecting every expression larger than a single
AST node as the target, using the remainder of the method
as the context. We remove methods containing the word
“test” in their body or ﬁle name, and omit 10% of the exam-
ples by ﬁltering out methods longer than 20 lines to avoid
conﬁgurations, initializations, and auto-generated code. To
make the task even harder, we remove examples where the
target appears as-is in the context. Ultimately, this dataset
contains 1.3M/10k/20k train/dev/test examples.

Restricted Completion: C# To provide a fair compari-
son to Brockschmidt et al. (2019), we create an additional
benchmark where the missing code is more limited. We
use the code of Brockschmidt et al. (2019) which ﬁlters
out examples where the targets contain non-primitive types
or user-deﬁned functions. We extract the exact same types
of limited expressions. Since the dataset of Brockschmidt
et al. (2019) is not publicly available, we consulted with
Brockschmidt et al. directly and extracted examples from
the raw dataset of Allamanis et al. (2018) using their “un-

seen projects test” set. This dataset contains 30 GitHub
projects broken down to one method per example. This
dataset contains 16k/8k/3k train/dev/test examples.

Our datasets are available at: http://github.com/
tech-srl/slm-code-generation/. Detailed statistics
are provided in Figure 6 in Appendix A.

Metrics Following Brockschmidt et al. (2019), we report
exact match accuracy at 1 and 5. We also introduce a new
tree@k metric which counts a prediction as correct if the
entire tree structures, ignoring leaf values, are identical.
For example, x > 1 and y > 2 would not count as identi-
cal in exact match, but would count as “tree-match identi-
cal” because both express that an identiﬁer is greater than
an integer (NAME > INT). The tree@k metric is interest-
ing because it allows us to tease apart the model’s syntactic
errors from incorrect subtoken predictions.

4.2. Baselines

We compare our model to a variety of original implemen-
tations and adaptations of existing models. We put signiﬁ-
cant effort to perform a fair comparison, including adding a
copy mechanism to the NMT baselines and subtokenization
as in our model. We adapt strong baselines from the liter-
ature to our task, even if they were designed to different
tasks such as NL→code and code→NL. We re-train all the
following baselines on the same datasets as our models.

NMT We use standard autoregressive sequence-to-
sequence NMT baselines, in which we subtokenize the
given code snippet, replace the target in the source with
a special PRED symbol, and train the network to predict the
target as a sequence of subtokens. Transformerbase+copy
(Vaswani et al., 2017) uses the implementation of Open-
NMT (Klein et al., 2017) with a copy mechanism (Gu et al.,
2016). Transformersmall+copy uses dmodel=256, dff=1024,
and 4 self attention heads per layer. BiLSTM→LSTM+copy
is a 2-layer bidirectional LSTM encoder-decoder with
d=512 and attention. seq2tree+copy follows Aharoni &
Goldberg (2017) and learns to generate the linearized,
subtokenized target AST.

Java-speciﬁc Baselines We use the original implementa-
tion of Iyer et al. (2018), and also their seq2prod base-
line which is a re-implementation of Yin & Neubig (2017);
these are designed for NL→code tasks, in which we feed
the code context as the NL input. The model of Iyer et al.
(2018) is designed to get additional input of the available
variables and their types, for which we do not feed types.
While these models could also be applied to other lan-
guages, their implementation only supports Java.

C#-speciﬁc Baselines We compare our model to the graph-
based GNN →NAG model using the implementation of
Brockschmidt et al. (2019). Bielik et al. (2016) kindly

Structural Language Models of Code

Model

acc@1

acc@5

tree@1

tree@5

code2seq (Alon et al., 2019a)
Iyer et al. (2018)
seq2prod (Yin & Neubig, 2017)
Transformersmall (Vaswani et al., 2017)+copy
Transformerbase (Vaswani et al., 2017)+copy
BiLSTM→LSTM (Luong et al., 2015)+copy
seq2tree (Aharoni & Goldberg, 2017)+copy

SLM (this work)

10.68
5.94
8.05
14.23
16.65
16.93
16.81

18.04

15.56
9.19
11.82
21.35
24.05
23.17
23.04

24.83

30.46
25.54
30.77
31.83
34.68
34.29
38.14

39.10

43.94
36.75
41.73
47.40
50.52
49.72
52.36

55.32

Table 1. Results on any-code completion in Java.

trained and tested their non-neural PHOG model on our
C# dataset. We note that PHOG does not have an explicit
copy mechanism, and considers only context to the left of
the target code, while we consider also context to the right.
Extending PHOG could potentially improve its results.

In both Java and C#, we compare to code2seq (Alon et al.,
2019a), which is a strong code→NL model. We train it to
generate the target code as a sequence of subtokens.

Model

acc@1

acc@5

tree@1

tree@5

GNN→NAG
code2seq
seq2seq+copy
seq2tree+copy
PHOG

SLM (this work)

15.19
6.20
26.42
22.29
7.40

37.61

27.05
10.05
37.94
35.86
12.00

45.51

26.48
21.97
34.10
31.85
–

51.10

40.09
30.89
49.23
48.53
–

59.82

Table 2. Results on restricted completion in C#.

4.3. Implementation and Hyperparameter Settings

Architecture We use embeddings of size 512, 2 layers of
LSTMs with 256 units, and 4 transformer layers with 8 at-
tention heads. We kept a small subtoken vocabulary of size
1000 to encourage the model to learn to copy; larger vo-
cabularies did not show an improvement. These resulted in
a very lightweight model of only 15M parameters, which is
close to Transformersmall (11.8M parameters). In compar-
ison, Transformerbase had more than 45M parameters (3×
more parameters than our model).

Training We train the model end-to-end on a single
V100 GPU, using cross entropy and the Adam optimizer
(Kingma & Ba, 2015), an initial learning rate of 10−4 mul-
tiplied by 0.95 every 20k steps. We bucket examples based
on the number of predictions in the target subtree (nodes
+ subtokens + EOS), and vary the batch size such that each
batch contains about 512 targets. We train the model to pre-
fer copying entire tokens rather than copying subtokens, if
possible, by optimizing for the entire token as the true la-
bel. We apply dropout of 0.25 in the Transformer layers,
and a recurrent dropout of 0.5 in the LSTMs.

Inference We perform beam search with width of 5 and
optimize for accuracy@1.

5. Results

Any-Code Completion: Java Table 1 shows that our SLM
achieves over 1.1% and 0.78% better acc@1 and acc@5
(respectively) over the two strongest baselines. The im-
provement over Transformersmall, which is closer to our
model in the number of parameters, is even higher: over

3.8% and 3.4% in acc@1 and acc@5.

The NMT baselines performed better than code-speciﬁc
baselines. We hypothesize that the reason is that the NMT
baselines are more generic, while the code-speciﬁc base-
lines are designed for different tasks: seq2prod is designed
for tasks which involve generating code given natural lan-
guage input; Iyer et al. (2018) additionally expects all
member methods, ﬁelds, and their types as input; code2seq
is designed to generate sequences rather than code, and
does not have a copy mechanism. An approximation of
code2seq with a copy mechanism is presented in Section 6.

Interestingly, the syntactically-informed seq2tree baseline
achieved the highest tree@k among the baselines, while our
model achieved higher acc@k and tree@k. This shows that
leveraging the syntax can beneﬁt NMT models as well.

Restricted Completion: C# Table 2 shows the results for
the restricted completion task in C#, where seq2seq+copy
is the BiLSTM→LSTM+copy model which performed the
best among the Java baselines. We ﬁrst observe that the
seq2seq+copy and the seq2tree+copy baselines outperform
the GNN →NAG of Brockschmidt et al. (2019), who intro-
duced this task. Although Brockschmidt et al. (2019) did
compare to a seq2seq baseline, their GNN →NAG model
could copy symbols from the context, but their baseline did
not. To conduct a fair comparison with our SLM model,
we equipped the seq2seq and seq2tree baselines with a
copy mechanism. Even though the seq2seq+copy and the
seq2tree+copy baselines perform substantially better than
the state of the art in this setting, our SLM model is able to
go beyond, achieving signiﬁcant gains over all models.

Structural Language Models of Code

Ablation

acc@1

acc@5

tree@1

tree@5

Paths→Seq
Seq→Path
Paths→Paths
No Root Att
No Copy

SLM (original)

12.95
12.12
17.63
14.43
10.72

18.04

18.52
17.12
24.62
18.48
15.70

24.83

33.44
28.68
37.78
28.20
30.61

39.10

43.43
43.99
53.98
35.65
44.35

55.32

Table 3. Ablations on any-code completion in Java.

The superiority of our model over GNN →NAG may also
be related to the GNN bottleneck (Alon & Yahav, 2020),
which hinders GNNs from propagating long-range mes-
sages. In contrast, propagating long-range messages using
paths is natural for our model.

6. Ablation Study

To understand the importance of the various components
and design decisions in our model, we conducted an exten-
sive ablation study.

Paths→Seq follows code2seq (Alon et al., 2019a) and sep-
arates the model to an encoder and a decoder, where the de-
coder generates the target code as a sequence of subtokens.
The main difference from code2seq is that Paths→Seq in-
cludes a copy mechanism, as in our model.

Seq→Path follows Rabinovich et al. (2017) and separates
our model to an encoder and a decoder (including a copy
mechanism), where the encoder encodes the context as a
sequence of subtokens using a BiLSTM, and the decoder
generates the missing subtree using the root path and the
index of the generated child.

Paths→Paths is similar to our SLM model except that it
uses separate encoder and decoder. These encoder and de-
coder have untied weights, unlike our SLM model which
models the source and the target jointly.

No Root Attention uses max pooling instead of attention
in aggregating multiple paths (see Section 3.2). The index-
informed path from the root to the target’s parent (R in
Figure 2) is concatenated with the result, instead of being
used as attention query.

No Copy replaces copy mechanism with a much larger vo-
cabulary (25k subtokens instead of 1k).

Results Table 3 shows the results of these alternatives. As
our SLM model performs better than Paths→Paths, this ab-
lation shows the importance of joint modeling of the con-
text and the target subtree by parameter tying.

Each of Paths→Paths and the seq2seq baselines (Ta-
ble 1) performs better than Paths→Seq and Seq→Path; this

shows the importance of using the same type of encoder
and decoder for any-code completion, rather than combin-
ing “an optimal encoder” with “an optimal decoder”. While
this distinction between encoder and decoder types might
be necessary for semantic parsing (Rabinovich et al., 2017;
Dong & Lapata, 2018), NL→code (Yin & Neubig, 2017)
and code→NL (Alon et al., 2019a; Fernandes et al., 2019)
tasks because of the different modalities of the input and
the output, this discrepancy may hurt generalization when
the output is essentially a missing part of the input’s AST.

Paths→Paths performs better than the seq2seq baselines
(Table 1), showing the advantage of using paths over tex-
tual sequences, even without parameter tying.

No Root Attention degrades acc@1 and acc@5 by 3.6% to
6.3%. This shows that dynamically attending to the context
paths given the current root path is crucial.

Not using a copying mechanism results in a degradation of
7.3% to 9.1%. Programs use symbols and identiﬁers repet-
itively, thus the ability to copy symbols from the context is
crucial for this task. For this reason, we included a copying
mechanism in all NMT baselines in Section 4.

7. Qualitative Analysis

Our main results (Table 1 and Table 2) reveal a gap be-
tween acc@k and tree@k: when ignoring identiﬁer values
and comparing only the tree structure, accuracy is signif-
icantly higher across all models. While our SLM model
performs better than all baselines in acc@k, our model also
shows greater potential for improvement in its tree@k re-
sults, which are much higher than the baselines’. We thus
focus on studying the cases where the tree was predicted
correctly, but the model failed to generate the code exactly
including names.

Figure 4(a) shows an example of this case:
the ground
truth has a structure of the form: NAME.NAME() > INT.
Our model predicts value.length() > 0 (a tree-match)
as its ﬁrst candidate and value.length() > 55 (the
ground truth) as its second. Null-checking a string is of-
ten followed by checking that it is also not empty, making
the ﬁrst candidate a reasonable prediction as well.

Figure 4(b) shows another example: in this case, the ground
truth thisValue == thatValue ? 0 :
1 was pre-
dicted correctly only as the second candidate. Neverthe-
less, the top-3 candidates are tree-matches since all of them
are of the form: NAME == NAME ? INT : INT. Interest-
ingly, the ﬁfth candidate (thisValue == thatValue)
?

1 is logically-equivalent to the ground truth.

0 :

In both examples, our model’s top candidate differs from
the ground truth by a single identiﬁer or literal:
in Fig-
ure 4(a) the model predicted 0 instead of 55; in Figure 4(b)

Structural Language Models of Code

private static void log(String value) {

public int compareTo(LongWritable o) {

if (value != null

&&

)

value = value.substring(0, 55)+"...";

LOG.info(value);

}

long thisValue = this.value;
long thatValue = o.value;
return (thisValue < thatValue ? -1 :

(

}

));

True ref:

value.length() > 55

thisValue == thatValue ?

0 : 1

SLM
top-5:

(9.6%) value.length() > 0
(7.3%) value.length() > 55
(1.8%) value.startsWith("...")
(1.5%) !value.startsWith("...")
(0.9%) value.charAt(0) == '.'

(cid:88)

(16.3%) thisValue == thisValue ? 0 :
(11.0%) thisValue == thatValue ? 0 :
1
0 : 1

(9.5%) thisValue == value ?
(6.6%) thisValue > thatValue ?
(6.1%) (thisValue == thatValue) ? 0 :

0 :

1
1 (cid:88)

1 ↔

(a)

(b)

Figure 4. Examples for cases where the top candidate is a “tree-match” (marked with
), but only the second candidate is an “exact
match” (marked with (cid:88) in bold). Predictions that are logically equivalent to the ground truth are marked with ↔. Additional (and larger)
examples along with the predictions of the baselines are shown in Appendices F and G.

the model predicted thisValue instead of thatValue.
Such single subtoken errors are responsible for 30% of the
cases where the model’s top prediction is a tree-match but
not an exact match. Single token (whole identiﬁer or literal)
mismatches are responsible for 74% of these cases. Thus,
improving our model’s ability to predict the right names
has the potential to enhance our gains furthermore. De-
tailed results of allowing such mistakes in our model and
in the baselines can be found in Appendix C.

In Figure 5,

Additional possible post-ﬁltering could ﬁlter out can-
the ﬁrst,
didates that do not compile.
third and fourth candidates do not compile, because the
this.currentAttempt object does not have getCount,
get, nor getTime methods.
If the model’s predictions
would have been considered in the context of the entire
project including its dependencies, these candidates could
have been ﬁltered out, and the (correct) ﬁfth candidate
would be ranked second. We leave compiler-guided code
generation to future work.

Additional examples can be found in Appendices F and G,
and in our interactive demo at http://AnyCodeGen.org.

8. Related Work

Generalizing Previous Approaches Our approach frames
code generation as predicting the next node in all partial
AST paths. This simple framing generalizes most previous
work, without hand-crafted edges and special actions:

• Models that use information about ancestor nodes
only (Rabinovich et al., 2017), as well as the “Parent
Feeding” of Yin & Neubig (2017), are generalized by
our model, since all paths that go into a node at pass
through its parent, and the path from the root is the

attention query.

• The “previous action encoding” of Yin & Neubig
(2017) is also a special case of our approach, because
St contains the paths starting from the previously ex-
panded leaves of Ap into the currently expanded node
π (at), such as path3 in Figure 2(e).

• The “context node” of PHOG (Bielik et al., 2016) is
just one of the previously-traversed leaf nodes in a<t.
Thus, not only that our model conditions on this con-
text node as well, our model also takes into account
the syntactic relation, i.e., the path, between the con-
text and π (at). Moreover, while PHOG conditions on
a single leaf, SLMs condition on every leaf in a<t.
• Finally, Brockschmidt et al. (2019) deﬁne special
graph edges (e.g., “NextSib” and “Child”) to capture
relations on the AST. Allamanis et al. (2018) fur-
ther deﬁnes data-ﬂow and control-ﬂow graph edges
such as “ComputedFrom” and “GuardedByNegation”.
Most of these relations can be expressed as partial
AST paths without manually designing them.

Program Generation Learning to generate programs is
one of the oldest problems in machine learning (Waldinger
& Lee, 1969) and has been considered by some as the “holy
grail of computer science” (Pnueli & Rosner, 1989; Gul-
wani et al., 2017). Typically, the task is to generate a pro-
gram given some form of input or context, such as com-
plete formal speciﬁcations (Green, 1981; Si et al., 2019) or
input-output examples (Gulwani, 2011; Devlin et al., 2017;
Parisotto et al., 2017; Balog et al., 2017; Gaunt et al., 2017).
While these approaches work well in some cases, they are
often bounded to DSLs that prevent them from being ap-
plied to realistic, general-purpose code.

Bielik et al. (2016) learn a dynamic DSL expression that
points to a single context that guides the generation of

Structural Language Models of Code

public float getProgress() {
this.readLock.lock();
try {

if (this.currentAttempt != null) {

return

}
return 0;

} finally {

this.readLock.unlock();

;

}

}

True ref:

SLM top-5:

this.currentAttempt.getProgress()

(31.3%)
(30.6%)
(1.5%)
(1.2%)
(0.9%)

this.currentAttempt.getCount()
-1
this.currentAttempt.get()
this.currentAttempt.getTime()
this.currentAttempt.getProgress() (cid:88)

Figure 5. An example from our test set in which a compiler-guided generation could ﬁlter out non-compiling candidates, and thus rank
the ground truth second instead of ﬁfth. Four out of the ﬁve candidates are “tree-match” (marked with
), the ﬁfth candidate is an
“exact match” (marked with (cid:88) in bold), and only the second and the ﬁfth candidate compile (marked with

).

a JavaScript program. Maddison & Tarlow (2014) and
Amodio et al. (2017) generate general-purpose uncondi-
tional code, and do not deal with the challenge of ﬁtting
the code to a given context.

Brockschmidt et al. (2019) addressed a similar code com-
pletion task as ours using a graph encoder and a neural at-
tribute grammar decoder. However, they limit their model
to generate only primitive types or arrays of these; use
a closed vocabulary; and omit user-deﬁned functions. In
this paper, we lift these constraints and allow any, general-
purpose, generation of code, of all types and containing any
names. As we show in Section 5, our model performs sig-
niﬁcantly better.

Murali et al. (2018) generate code given a set of APIs in
a ”Java-like” language; they state that their approach is
thus intrinsically limited to generate only API-heavy pro-
grams. Yin et al. (2019) generate general-purpose code by
applying a given edit to a given code snippet. Brody et al.
(2020) predict code edits directly given other edits that oc-
curred in the context. Yin & Neubig (2017) and Rabinovich
et al. (2017) used a top-down syntactic approach for gen-
erating general-purpose code given a natural language de-
scription. Models that address APIs→code, edit→code, or
NL→code tasks must model the input separately and differ-
ently from the output code. As we show in Section 6, mod-
eling the source and the target differently perform poorly
in our task, in which the input is code as well.

Chen et al. (2018) addressed JavaScript↔CoffeeScript
translation with a tree-to-tree approach, which required a
strong alignment between the source and target trees.

9. Conclusion

We presented a novel approach for any-code completion:
joint modeling of an AST and its missing subtree using a
structural language model. Our approach generalizes most
previous work in this area while reaching state-of-the-art
performance on challenging benchmarks. We demonstrate
our approach in generating general-purpose code, in re-
stricted and unrestricted settings, in two languages. Our
model outperforms a variety of strong baselines, including
programming language-oriented models and strong NMT
models applied in our settings.

We believe that structural language modeling enables a
wide range of future applications, similarly to how lan-
guage modeling research has contributed to NLP in recent
years. Our approach also has a variety of direct applications
such as code completion, detecting and ﬁxing unlikely ex-
isting code, and re-ranking solutions produced by another
synthesizer or solver. To these ends, we make all our code,
datasets, and trained models publicly available.

Acknowledgments

We would like to thank Guy Waldman for developing the
AnyCodeGen.org website, Pavol Bielik and Martin Vechev
for training their PHOG model on our dataset, Srinivasan
Iyer for his useful advice, the guidance in training his
model and adapting it to our task, Marc Brockschmidt for
his useful implementation tips and guidance in training his
model, and Miltiadis Allamanis for the guidance in repro-
ducing his C# dataset.

Supplementary Material

Structural Language Models of Code

Java

C#

#projects - training
#projects - validation
#projects - test
#examples - training
#examples - validation
#examples - test
Avg. number of paths
Avg. source length - lines
Avg. source length - tokens
Avg. source length - subtokens
Avg. target length - tokens
Avg. target length - subtokens
Avg. target length - tree nodes
Avg. target length - tree targets

9
1
1
1,309,842
10,000
20,000
27.8
10.4
77.7
100.6
5.4
7.8
3.8
10.8

25
2
3
16,295
8,183
3,305
131.1
57.5
264.3
343.6
3.9
5.0
3.9
10.8

Figure 6. Statistics of our datasets. When not mentioned otherwise,
the statistic was measured on the training set.

Figure 7. Efﬁcient computation: partial paths for dif-
ferent time steps share the same preﬁx, allowing a
shared computation. In this example, the preﬁx is the
shared path from the leaf (not shown) to Greater,
and is much longer than either of the sufﬁxes.

A. Data Statistics

Figure 6 shows some statistics of our used datasets. In Java: for the validation set, we randomly sampled 10, 000 examples
from the raw validation set; for the test set, we randomly sampled 20, 000 examples from the raw test set.

We will release all datasets, raw and preprocessed, with the ﬁnal version.

B. Additional Evaluation Details

For both Java and C# models, we experimented with the following hyper-parameter values. We performed beam search
on the validation set after every training iteration, and we selected the best conﬁguration and checkpoint according to
accuracy@1 on the validation set. After the best conﬁguration was chosen, we ran a single evaluation run on the test set.

• (cid:59)f ∈ {LST M, T ransf ormer} – how to encode each path.

• LSTM #layers ∈ {1, 2}

• dsubtoken ∈ {256, 512} – embedding size.

• Transformer layers ∈ {0, 1, 2, 3, 4}

• lr ∈ {10−3, 10−4, 10−5} – learning rate

• Learning rate decay every {10000, 20000, 40000} steps.

C. Qualitative Analysis cont. - Correct Tree, Incorrect Names

In Section 7 we discussed the gap between acc@k and tree@k. We found that 30% of the examples in the gap could have
been exact match if a single subtoken prediction was ﬁxed; 74% of the examples in the gap could have been exact match
if a single identiﬁer prediction was ﬁxed. Table 4 shows the accuracy of our model and the leading baselines if a single
subtoken or a single token mismatches were counted as correct: One SubToken Diff and One Token Diff are similar to
exact match, except that they allow a single subtoken or a single token mistake, respectively. As Table 4 shows, not only
that our model performs better than the baselines in exact match, it also shows a greater potential for improvement.

GreaterNameIntExpIfExprx1time=3time=4Structural Language Models of Code

Model

Exact-match (acc@k)
@5

@1

One SubToken Diff
@5

@1

One Token Diff
@5

@1

Transformerbase +copy
BiLSTM→LSTM +copy
seq2tree +copy

SLM (this work)

16.65
16.93
16.81

18.04

24.05
23.17
23.04

24.83

23.08
22.39
24.02

24.40

34.06
31.68
33.89

35.19

29.39
27.23
32.67

33.68

43.46
38.92
43.75

46.57

Tree@k
@1

@5

34.68
34.29
38.14

50.52
49.72
52.36

39.10

55.32

Table 4. Examining the gap between acc@k and tree@k: the acc@k and tree@k results here are the same as in Table 1; One SubToken
Diff allows a single subtoken mismatch; One Token Diff allows a single token mismatch.

D. Copying Single Subtokens

In addition to scoring the entire token to be copied, we also score each of the subtokens composing it according to their
position. For each position i, we add a scoring function scopyi, such that scopyi ((cid:96)) produces the copying score of the i’th
subtoken of (cid:96), which we denote as (cid:96)i:

sw = sgen (w) +

(cid:88)

val((cid:96))=w

scopy token ((cid:96)) +

(cid:88)

(cid:88)

i

val((cid:96)i)=w

scopyi ((cid:96))

P r (a|S) = softmax (s)

Where scopy token is the scoring function of copying the entire token, described in Section 3.3.

For example, a token of getX is scored entirely using scopy token; each of its subtokens, get and X, are scored using scopy1
and scopy2 respectively. That is, the model can either copy the entire token, or copy only some of its subtokens. This
ability is especially useful in generating a name like setX, where getX appears in the context, and X is any unknown,
user-deﬁned, subtoken; the model learns to generate set from the vocabulary, and copy only the subtoken X.

protected void checkRpcAdminAccess() throws IOException, AccessControlException {

UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
UserGroupInformation zkfcUgi = UserGroupInformation.getLoginUser();
if (adminAcl.isUserAllowed(ugi)

|| ugi.getShortUserName().equals( zkfcUgi.getShortUserName() )) {

LOG.info("Allowed RPC access from " + ugi
+ " at " + Server.getRemoteAddress());

return;

}

String msg = "Disallowed RPC access from " + ugi

+ " at " + Server.getRemoteAddress() + ". Not listed in " + DFSConfigKeys.DFS_ADMIN;

LOG.warn(msg);
throw new AccessControlException(msg);

}

True ref:

zkfcUgi.getShortUserName()

SLM top-5 candidates:

zkfcUgi.getShortUserName()
DFSConfigKeys.DFS
zkfcUgi.getUserName()
zkfcUgi.getUser()
zkfcUgi.getUserId()

(11.7%)
(4.5%)
(2.6%)
(1.7%)
(0.6%)

(exact match)

(tree-match)
(tree-match)
(tree-match)

Entirely copied tokens are marked in brown; unknown copied subtokens are marked in blue; in-vocabulary subtokens are marked in
black; subtokens that are both in-vocabulary and copied from context are marked in purple.

Figure 8. A Java Any-Code Completion example from our test set along with the predictions of our model. The predictions of the
baselines are shown in Figure 12 below.

E. Example: Usefulness of Copy Mechanism

Structural Language Models of Code

As shown in Section 6, the ability to copy is crucial for the any-code completion task, because of the repetitive use of
identiﬁers and symbols in programs. Figure 8 shows a representative example for the necessity of the copy mechanism:
generating the ground truth zkfcUgi.getShortUserName() is feasible only thanks to the copy mechanism, since zkfc
is obviously an UNK subtoken which was not observed in the training data.

In this case, since both zkfcUgi and getShortUserName appear in context, both were copied as entire tokens, rather
than generated using subtokens. This example also shows how the ability to copy entire tokens ease the generation process
by reducing the number of target symbols (our SLM model is able to copy and combine single subtokens as well).

F. Java Examples

Figures 9 to 18 contain examples from our test set for the any-code completion task in Java, along with the prediction of our
model and some of the baselines. The highlighted expressions are the true references that should be generated. Indentation
and line breaks may have been altered for typesetting reasons.

G. C# Examples

Figures 19 to 26 contain examples from our test set for the restricted completion task in C# along with the prediction of
our model some of the baselines. The highlighted expressions are the true references that should be generated. Indentation
and line breaks may have been altered for typesetting reasons.

Structural Language Models of Code

private C findCounter(T key) {
int i = key.ordinal();
if (counters[i] == null) {

counters[i] = newCounter(key);

}
return (C) counters[i] ;

}

Model
True ref:

SLM (this work)

Transformerbase +copy

BiLSTM→LSTM +copy

Seq2tree +copy

Prediction
(C) counters[i]
(C) counters[i]
(C) this
counters[i]
(C) this
(C) counters[i]
(C) counters
(C) this
(C) counters[i]
counters[i]
(C) counters[i]
(C) counters[i].ordinal()
(C) counters.get(i)

(71.6%)
(6.3%)
(4.8%)

private void handleTaskFinishedEvent(TaskFinishedEvent event) {
TaskInfo taskInfo = info.tasksMap.get( event.getTaskId() );
taskInfo.counters = event.getCounters();
taskInfo.finishTime = event.getFinishTime();
taskInfo.status = TaskStatus.State.SUCCEEDED.toString();
taskInfo.successfulAttemptId = event.getSuccessfulTaskAttemptId();

}

Model
True ref:

SLM (this work)

Transformerbase +copy

BiLSTM→LSTM +copy

Seq2tree +copy

Prediction
event.getTaskId()
event.getTaskName()
event.getId()
event.getTask()
event.getName()
event.getTaskId()
event.getTaskInfo()
event.getTaskId()
event.getId()
event.getTask()
taskInfo.getTaskId()()
event.name
event.type
event.getId()
event.id
event.getKey()
event.getId()
event.getPath()
event.getDescription()
event.getTaskName()
event.getTaskName(

(8.8%)
(8.2%)
(3.4%)
(3.3%)
(3.3%)

(Syntax error)

Figure 9. Java examples from our test set along with the predictions of our model and the baselines.

Structural Language Models of Code

private static void log(String value) {

if (value!= null && value.length() > 55 )

value = value.substring(0, 55) + "...";

LOG.info(value);

}

Model
True ref:

SLM (this work)

Transformerbase +copy

BiLSTM→LSTM +copy

Seq2tree +copy

Prediction
value.length() > 55
value.length() > 0
value.length() > 55
value.startsWith("...")
value.length() > 55
value.length() > 0)
value.length() > 1
value.length() > 55
value.startsWith("")
value.startsWith("...")
value.length() 55
value.endsWith("info")
value.length() 55

(9.6%)
(7.3%)
(1.8%)

(Syntax error)

(Syntax error)

private List<INode> initChildren() {

if (children == null) {

final ChildrenDiff combined = new ChildrenDiff();
for (DirectoryDiff d = DirectoryDiff.this; d != null; d = d.getPosterior() ) {

combined.combinePosterior(d.diff, null);

}
children = combined.apply2Current(ReadOnlyList.Util.asList(

currentDir.getChildrenList(Snapshot.CURRENT_STATE_ID)));

}
return children;

}

Model
True ref:

SLM (this work)

Transformerbase +copy

BiLSTM→LSTM +copy

Seq2tree +copy

Prediction
d = d.getPosterior()
d = d.getParent()
d = d.getChildrenList()
d = d
d = combined
d = d.getPosterior()
d = d
d = d.diff
d = d.getChildren()
d = d.currentDir
d = d.currentStateId
--d
d = d
d = d.getParent()
d = d.next
d = d.get()
d d.next
d d.parent
d d.getParent()
d d.getChildren()
d d.getRoot()

(18.8%)
(14.9%)
(4.5%)
(2.5%)
(1.8%)

(Syntax error)
(Syntax error)
(Syntax error)
(Syntax error)
(Syntax error)

Figure 10. Java examples from our test set along with the predictions of our model and the baselines.

Structural Language Models of Code

public float getProgress() {
this.readLock.lock();
try {

if (this.currentAttempt != null) {

return this.currentAttempt.getProgress() ;

}
return 0;

} finally {

this.readLock.unlock();

}

}

(31.3%)
(30.6%)
(1.5%)
(1.2%)
(0.9%)

Model
True ref:

SLM (this work)

Transformerbase +copy

BiLSTM→LSTM +copy

Seq2tree +copy

Prediction
this.currentAttempt.getProgress()
this.currentAttempt.getCount()
-1
this.currentAttempt.get()
this.currentAttempt.getTime()
this.currentAttempt.getProgress()
this.currentAttempt.getProgress()
this.currentAttempt.floatValue()
this.currentAttempt.getFloat()
this.currentAttempt.get()
this.currentAttempt.getTime()
this.currentAttempt.getProgress()
this.currentAttempt.float()
this.currentAttempt.get()
this.currentAttempt.size()
this.currentAttempt.compute()
this.currentAttempt.getProgress()
this.currentAttempt.floatValue()
this.currentAttempt.get()
this.currentAttempt.getValue()
(float)this.currentAttempt.size()

public int compareTo(LongWritable o) {

long thisValue = this.value;
long thatValue = o.value;
return (thisValue < thatValue ? -1 : ( thisValue == thatValue ? 0 : 1 ));

}

Model
True ref:

SLM (this work)

Transformerbase +copy

BiLSTM→LSTM +copy

Seq2tree +copy

0 : 1
0 : 1
0 : 1

0

0 : 1

Prediction
thisValue == thatValue ?
thisValue == thisValue ?
thisValue == thatValue ?
thisValue == value ?
thatValue >> thatValue
thatValue > thatValue ? 1 :
thatValue > thatValue
thisValue - thatValue
thatValue & thatValue
thatValue ?
0
thisValue thatValue
thisValue thatValue 0 1
thisValue thatValue 1 0

1 :

(16.3%)
(11.0%)
(9.5%)

(Syntax error)
(Syntax error)
(Syntax error)

Figure 11. Java examples from our test set along with the predictions of our model and the baselines.

Structural Language Models of Code

private static String getNameServiceId(

Configuration conf, String addressKey) {

String nameserviceId = conf.get(DFS_NAMESERVICE_ID);
if (nameserviceId != null) {

return nameserviceId;

}
Collection<String> nsIds = getNameServiceIds(conf);
if (1 == nsIds.size() ) {

return nsIds.toArray(new String[1])[0];

}
String nnId = conf.get(DFS_HA_NAMENODE_ID_KEY);
return

getSuffixIDs(conf, addressKey, null, nnId, LOCAL_ADDRESS_MATCHER)[0];

}

Model
True ref:

SLM (this work)

Transformerbase +copy

BiLSTM→LSTM +copy

Seq2tree +copy

Predictions
nsIds.size()
nsIds.size()
conf.size()
getSuffixIDs(conf).length
-1
ns.size()
conf.size()
-1
Integer.MAX VALUE
conf.size()
1
nsIds.size()
stringPool.blank

(83.7%)
(3.0%)
(2.5%)

protected void checkRpcAdminAccess() throws

IOException, AccessControlException {

UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
UserGroupInformation zkfcUgi = UserGroupInformation.getLoginUser();
if (adminAcl.isUserAllowed(ugi) ||

ugi.getShortUserName().equals( zkfcUgi.getShortUserName() )) {

LOG.info("Allowed RPC access from " + ugi
+ " at " + Server.getRemoteAddress());

return;

}

String msg = "Disallowed RPC access from " + ugi

+ " at " + Server.getRemoteAddress()
+ ". Not listed in " + DFSConfigKeys.DFS_ADMIN;

LOG.warn(msg);
throw new AccessControlException(msg);

}

Model
True ref:

SLM (this work)

Transformerbase +copy

BiLSTM→LSTM +copy

Seq2tree +copy

Predictions
zkfcUgi.getShortUserName()
zkfcUgi.getShortUserName()
DFSConfigKeys.DFS
zkfcUgi.getUserName()
server.getRemoteAddress()
server.getRemoteUserName()
server.getShortUserName()
server.getUserName()
zkfcUgi.getUserName()
ugiUgi.getUserName()
dfsConfigKeys.dfsAdmin
zkfc.getUserName()
zkfcUgi.getRemoteAddress()

(11.7%)
(4.5%)
(2.6%)

Figure 12. Java examples from our test set along with the predictions of our model and the baselines.

Structural Language Models of Code

static String replaceSubstitution(

String base, Pattern from, String to, boolean repeat) {

Matcher match = from.matcher(base);
if (repeat) {

return match.replaceAll(to) ;

} else {

return match.replaceFirst(to);

}

}

Model
True ref:

SLM (this work)

Transformerbase +copy

BiLSTM→LSTM +copy

Seq2tree +copy

Prediction
match.replaceAll(to)
match.toString()
match.replaceAll(to)
match.replaceAll(to, from)
match.replaceFirst(to)
replace.replaceFirst(to)
matcher.replaceFirst(to)
match.getFirst()
match.replaceFirst(to)
match.replaceFirst(to, to)
match.replaceFirst(base)
match.replaceFirst(to)
match.replaceFirst(repeat)

(9.0%)
(8.2%)
(6.5%)

public void responseReceived(ResponseReceivedEvent event) {

RequestResult result = event.getRequestResult();
Date startDate = result.getStartDate();
Date stopDate = result.getStopDate();
long elapsed = stopDate.getTime() - startDate.getTime();
synchronized (this) {

this.lastE2Elatency = elapsed;

}
if ( LOG.isDebugEnabled() ) {

int statusCode = result.getStatusCode();
String etag = result.getEtag();
HttpURLConnection urlConnection =

(HttpURLConnection) event.getConnectionObject();

int contentLength = urlConnection.getContentLength();
String requestMethod = urlConnection.getRequestMethod();
long threadId = Thread.currentThread().getId();
LOG.debug(String.format(

"SelfThrottlingIntercept:: ResponseReceived:
... threadId=%d, Status=%d, Elapsed(ms)=%d,
... ETAG=%s, contentLength=%d, requestMethod=%s",
threadId, statusCode, elapsed, etag, contentLength, requestMethod));

}

}

Model
True ref:

SLM (this work)

Transformerbase +copy

BiLSTM→LSTM +copy

Seq2tree +copy

Prediction
LOG.isDebugEnabled()
elapsed != null
LOG.isDebugEnabled()
!LOG.isDebugEnabled()
stopDate != null
result.hasStatusCode()
result.hasStatusCode() != elapsed
result != null
elapsed > 0
result.getStatusCode() == workflowConstants.STATUS
event.getConnectionObject() instanceof HttpUrlConnection
startDate != null
LOG.isDebugEnabled()

(32.1%)
(29.0%)
(2.4%)

Figure 13. Java examples from our test set along with the predictions of our model and the baselines.

Structural Language Models of Code

private static boolean isNameResolved(InetAddress address) {

String hostname = address.getHostName() ;
String ip = address.getHostAddress();
return !hostname.equals(ip) || NetUtils.isLocalAddress(address);

}

Model
True ref:

SLM (this work)

Transformerbase +copy

BiLSTM→LSTM +copy

Seq2tree +copy

Prediction
address.getHostName()
address.getHostname()
address.getHostName()
inetAddress.getByName(address.getAddress())
address.getHostAddress()
address.getLastElement().getValue()
address.getAddress()
address.getHostAddress()
address.getPort()
address.getAddress()
address.getHostAddress()
address.getPort()
address.getAddress()

(3.5%)
(2.0%)
(0.7%)

private synchronized void initJournals(List<URI> dirs) {

int minimumRedundantJournals = conf.getInt(

DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_MINIMUM_KEY,
DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_MINIMUM_DEFAULT);

journalSet = new JournalSet(minimumRedundantJournals);
for (URI u : dirs) {
boolean required =

FSNamesystem.getRequiredNamespaceEditsDirs(conf).contains(u);

if ( u.getScheme() .equals(NNStorage.LOCAL_URI_SCHEME)) {
StorageDirectory sd = storage.getStorageDirectory(u);
if (sd != null) {
journalSet.add(

new FileJournalManager(conf, sd, storage),
required, sharedEditsDirs.contains(u));

}

} else {

journalSet.add(createJournal(u),

required, sharedEditsDirs.contains(u));

}

}
if (journalSet.isEmpty()) {

LOG.error("No edits directories configured!");

}

}

Model
True ref:

SLM (this work)

Transformerbase +copy

BiLSTM→LSTM +copy

Seq2tree +copy

Prediction
u.getScheme()
u.getName()
u.getScheme()
u.getVersion()
journalSet.LOCAL URI SCHEME
u.getName()
Boolean.true
u.toString()
Boolean.true
u.getURI()
u.getScheme()
u.getName()
storage.getLocalUriScheme()

(27.4%)
(13.1%)
(8.2%)

Figure 14. Java examples from our test set along with the predictions of our model and the baselines.

Structural Language Models of Code

static EnumSet<FileAttribute> parse(String s) {

if (s == null || s.length() == 0) {

return EnumSet.allOf(FileAttribute.class);

}
EnumSet<FileAttribute> set = EnumSet.noneOf(FileAttribute.class);
FileAttribute[] attributes = values();
for (char c : s.toCharArray() ) {

int i = 0;
for (; i < attributes.length && c != attributes[i].symbol; i++) ;
if (i < attributes.length) {

if (!set.contains(attributes[i])) {

set.add(attributes[i]);

} else {

throw new IllegalArgumentException("There are more than one '"

+ attributes[i].symbol + "' in " + s);

}

} else {

throw new IllegalArgumentException("'" + c + "' in "

+ s + " is undefined.");

}

}
return set;

}

Model
True ref:

SLM (this work)

Transformerbase +copy

BiLSTM→LSTM +copy

Seq2tree +copy

Prediction
s.toCharArray()
s.toCharArray()
attributes[0].value
attributes[undefined].length
s.split(" "
set.split(" ")
attributes.keySet()
attributes.length
attributes[0]
attributes[0].next
set.toArray()
s.toCharArray()
set.toCharArray()

(22.4%)
(18.5%)
(4.6%)

public static Path[] stat2Paths(FileStatus[] stats) {

if (stats == null)

return null;

Path[] ret = new Path[stats.length];
for (int i = 0; i < stats.length; ++i) {

ret[i] = stats[i].getPath() ;

}
return ret;

}

Model
True ref:

SLM (this work)

Transformerbase +copy

BiLSTM→LSTM +copy

Seq2tree +copy

Prediction
stats[i].getPath()
stats[i].getPath()
Path(stats[i])
new Path(stats[i], charset)
stats[i]
stats[i].getPath()
new Path(stats[i])
stats[i]
new Path(stats[i])
stats[i].toString()
stats[i]
new Path(stats[i])
stat(stats[i])

(25.2%)
(3.3%)
(2.5%)

Figure 15. Java examples from our test set along with the predictions of our model and the baselines.

Structural Language Models of Code

void ensureCurrentDirExists() throws IOException {

for (

Iterator<StorageDirectory> it = storage.dirIterator();
it.hasNext(); ) {

StorageDirectory sd = it.next();
File curDir = sd.getCurrentDir();
if ( !curDir.exists() && !curDir.mkdirs()) {

throw new IOException("Could not create directory " + curDir);

}

}

}

(29.0%)
(25.8%)
(24.4%)

Model
True ref:

SLM (this work)

Transformerbase +copy

BiLSTM→LSTM +copy

Seq2tree +copy

Prediction
!curDir.exists()
!curDir.exists()
curDir != null
curDir.exists()
curDir != null
!curDir.exists()
curDir.exists()
curDir != null
curDir.exists()
sd != null
curDir != null
curDir.exists()
!curDir.exists()

public static byte[] getXAttr(final Map<?, ?> json, final String name)

throws IOException {

if (json == null) {

return null;

}
Map<String, byte[]> xAttrs = toXAttrs(json);
if (xAttrs != null) {

return xAttrs.get(name) ;

}
return null;

}

Model
True ref:

SLM (this work)

Transformerbase +copy

BiLSTM→LSTM +copy

Seq2tree +copy

Prediction
xAttrs.get(name)
xAttrs.get(name)
xAttrs.get(xAttrs)
xAttrs.toByteArray()
xAttrs.get(name)
xAttrs.toByteArray()
new byte[0]
xAttrs.getBytes()
new byte[0]
xAttrs.toByteArray()
xAttrs.get(name)
xAttrs.get()
xAttrs.get(0)

(28.2%)
(5.8%)
(4.4%)

Figure 16. Java examples from our test set along with the predictions of our model and the baselines.

Structural Language Models of Code

private void setFlag(long flag) {

long prev;
do {

prev = unsafe.getLongVolatile(null, this.slotAddress);
if ( (prev & flag) != 0) {

return;

}

} while (!unsafe.compareAndSwapLong(

null, this.slotAddress, prev, prev | flag));

}

Model
True ref:

SLM (this work)

Transformerbase +copy

BiLSTM→LSTM +copy

Seq2tree +copy

Prediction
(prev & flag)
(prev & flag)
(prev & flagSlot)
unsafe.get(prev)
(prev & flag)
(prev | flag)
unsafe.compareTo(prev)
prev
prev + 1
prev - 1
unsafe prev flag
(volatile prev unsafe.get())
(volatile prev unsafe.getLongVolatile(null, prev))

(8.9%)
(5.4%)
(5.0%)

(Syntax error)
(Syntax error)
(Syntax error)

public synchronized void setInput(byte[] b, int off, int len) {

if (b == null) {

throw new NullPointerException();

}
if (off < 0 || len < 0 || off > b.length - len) {

throw new ArrayIndexOutOfBoundsException();

}
finished = false;
if (len > uncompressedDirectBuf.remaining()) {

this.userBuf = b;
this.userBufOff = off;
this.userBufLen = len;

} else {

((ByteBuffer) uncompressedDirectBuf).put(b, off, len);
uncompressedDirectBufLen = uncompressedDirectBuf.position();

}
bytesRead += len;

}

(41.3%)
(23.4%)
(14.1%)

Model
True ref:

SLM (this work)

Transformerbase +copy

BiLSTM→LSTM +copy

Seq2tree +copy

Predictions
len < 0
len < 0
off > b.length
len > b.length
off < 0
len < 0
b == null
off < 0
len < 0
b == null
off < 0
len < 0
0 < off

Figure 17. Java examples from our test set along with the predictions of our model and the baselines.

Structural Language Models of Code

private int readData(byte[] buf, int off, int len) throws IOException {

int bytesRead = 0;
while (bytesRead < len) {

int n = IOUtils.wrappedReadForCompressedData(

in, buf, off + bytesRead , len - bytesRead);

if (n < 0) {

return bytesRead;

}
bytesRead += n;

}
return len;

}

Model
True ref:

SLM (this work)

Transformerbase +copy

BiLSTM→LSTM +copy

Seq2tree +copy

Prediction
off + bytesRead
bytesRead - bytesRead
off + bytesRead
off - bytesRead
off - bytesRead
off + len
len - bytesRead
-bytesRead
bytesRead++
bytesRead - bytesRead
compressed bytesRead
off + bytesRead
len - bytesRead

(35.0%)
(14.1%)
(9.4%)

(Syntax error)

private Path getPath(int curId, int limitPerDir, Type type) {

if (curId <= 0) {

return basePath;

}
String name = "";
switch(type) {
case FILE:

name = FILE_PREFIX + new Integer(curId % limitPerDir).toString();
break;

case DIRECTORY:

name = DIR_PREFIX + new Integer(curId % limitPerDir).toString();
break;

}
Path base = getPath((curId / limitPerDir), limitPerDir, Type.DIRECTORY);
return

new Path(base, name) ;

}

Model
True ref:

SLM (this work)

Transformerbase +copy

BiLSTM→LSTM +copy

Seq2tree +copy

Prediction
new Path(base, name)
new Path(base, name)
new Path(base, name, limitPerDir)
new Path(base, name, type)
new Path(base)
new Path(name)
getPath(base)
new Path(base)
new File(base)
new Path(base.getPath())
new Path(base)
new File(base, name)
new Path(base, name)

(6.0%)
(2.9%)
(2.8%)

Figure 18. Java examples from our test set along with the predictions of our model and the baselines.

Structural Language Models of Code

private static IEnumerable<Token> OfSequence(

this IEnumerable<Token> tokens, Token nameToken, TypeDescriptor info)

{

}

var nameIndex = tokens.IndexOf(t => t.Equals(nameToken));
if ( nameIndex >= 0 )
{

return info.NextValue.MapValueOrDefault(

_ => info.MaxItems.MapValueOrDefault(

n => tokens.Skip(nameIndex + 1).Take(n),

tokens.Skip(nameIndex + 1).TakeWhile(v => v.IsValue())),

tokens.Skip(nameIndex + 1).TakeWhile(v => v.IsValue()));

}
return new Token[] { };

Model
True ref:

SLM (this work)

BiLSTM→LSTM +copy

GNN →NAG (Brockschmidt et al., 2019)

(22.6%)
(19.1%)
(13.9%)

Prediction
nameIndex >= 0
nameIndex >= 0
nameIndex == -1
nameIndex > -1
!nameIndex
nameIndex == -1
nameIndex < 0
nameIndex == 0
nameIndex > 0
nameIndex < 0

public static IEnumerable<T[]> Group<T>(

this IEnumerable<T> source, int groupSize)

{

if (groupSize < 1)
{

throw new ArgumentOutOfRangeException(nameof(groupSize));

}
T[] group = new T[groupSize];
int groupIndex = 0;
foreach (var item in source)
{

group[groupIndex++] = item;
if ( groupIndex == groupSize )
{

yield return group;
group = new T[groupSize];
groupIndex = 0;

}

}

}

Model
True ref:

SLM (this work)

BiLSTM→LSTM +copy

GNN →NAG (Brockschmidt et al., 2019)

Prediction
groupIndex == groupSize
groupIndex < 0
groupIndex == -1
groupIndex < groupIndex
group.IsNullOrEmpty()
groupGroup[groupIndex++]
group.EndsWith(group)
groupIndex == 0
groupIndex == 1
groupIndex == groupSize

(21.4%)
(10.3%)
(5.3%)

Figure 19. C# examples from our test set of the restricted completion task along with the predictions of our model and the baselines.

Structural Language Models of Code

internal static void AddLine(StringBuilder builder,

string value, int maximumLength)

{

if (builder == null)
{

throw new ArgumentNullException(nameof(builder));

}
if (value == null)
{

throw new ArgumentNullException(nameof(value));

}
if (maximumLength < 1)
{

throw new ArgumentOutOfRangeException(nameof(value));

}

value =

value.Trim() ;

builder.AppendWhen(builder.Length > 0, Environment.NewLine);
do
{

var wordBuffer = 0;
var words = value.Split(' ');
for (var i = 0; i < words.Length; i++)
{

if (words[i].Length < (maximumLength - wordBuffer))
{

builder.Append(words[i]);
wordBuffer += words[i].Length;
if ((maximumLength - wordBuffer) > 1 && i != words.Length - 1)
{

builder.Append(" ");
wordBuffer++;

}

}
else if (words[i].Length >= maximumLength && wordBuffer == 0)
{

builder.Append(words[i].Substring(0, maximumLength));
wordBuffer = maximumLength;
break;

}
else break;

}
value = value.Substring(Math.Min(wordBuffer, value.Length));
builder.AppendWhen(value.Length > 0, Environment.NewLine);

}
while (value.Length > maximumLength);
builder.Append(value);

}

Model
True ref:

SLM (this work)

BiLSTM→LSTM +copy

GNN →NAG

Prediction
value.Trim()
value.Trim()
value.Substring(0, maximumLength)
value.Replace(maximumLength, maximumLength
maximumLength - 1
value.Trim()
valueLength++
value + <UNK>
value + maximumLength
value.Substring(0, maximumLength)

(16.0%)
(10.9%)
(10.7%)

Figure 20. C# examples from our test set of the restricted completion task along with the predictions of our model and the baselines.

Structural Language Models of Code

public static string[] TrimStringArray(this IEnumerable<string> array)
{

return array.Select(item => item.Trim() ).ToArray();

}

Model
True ref:

SLM (this work)

BiLSTM→LSTM +copy

GNN →NAG (Brockschmidt et al., 2019)

Prediction
item.Trim()
item.Trim()
item.ToUpperInvariant()
item.ToUpper()
item.Trim()
item.ToTrim()
item.]
item + <UNK>
item + item
item + 1

(20.1%)
(3.5%)
(1.6%)

(Syntax error)

public static string Camelize(this string input)
{

var word = Pascalize(input);
return word.Substring(0, 1) .ToLower() + word.Substring(1) ;

}

Model
True ref:

SLM (this work)

BiLSTM→LSTM +copy

GNN →NAG

Prediction

word.Substring(0, 1)
word.Substring(0, 1)
word.Trim()
word.Substring(1)
input.Replace("&", " )
input.Replace(1, ’’)
input.Replace("&", "")
word.CombineWith(<UNK>)
word.Trim()
word.CombineWith(input)

word.Substring(1)
word.Substring(1)
wordData.Substring(1)
word.Substring(0, 1)
input.Replace("&", " <UNK> )
input + "." + input
input.Substring(0, 1)
word.CombineWith(<UNK>)
word + <UNK>
word.Replace(<UNK>, <UNK>)

Figure 21. C# examples from our test set of the restricted completion task along with the predictions of our model and the baselines.

Structural Language Models of Code

public string Truncate(string value, int length, string truncationString,

TruncateFrom truncateFrom = TruncateFrom.Right)

{

if (value == null)

return null;

if (value.Length == 0)

return value;

if (truncationString == null)

truncationString = string.Empty;

if (truncationString.Length > length)

return truncateFrom == TruncateFrom.Right ?

value.Substring(0, length) : value.Substring(value.Length - length);

var alphaNumericalCharactersProcessed = 0;

if (value.ToCharArray().Count(char.IsLetterOrDigit) <= length)

return value;

if (truncateFrom == TruncateFrom.Left)
{

for (var i = value.Length - 1; i > 0; i--)
{

if (char.IsLetterOrDigit(value[i]))

alphaNumericalCharactersProcessed++;

if (alphaNumericalCharactersProcessed + truncationString.Length

== length)

return truncationString + value.Substring(i);

}

}

for (var i = 0; i < value.Length - truncationString.Length; i++)
{

if (char.IsLetterOrDigit(value[i]))

alphaNumericalCharactersProcessed++ ;

if (alphaNumericalCharactersProcessed + truncationString.Length

== length)

return value.Substring(0, i + 1) + truncationString;

}

return value;

}

Model
True ref:

SLM (this work)

BiLSTM→LSTM +copy

GNN →NAG

Prediction
alphaNumericalCharactersProcessed++
alphaNumericalCharactersProcessed++
iCount++
iIndex++
i++
truncation++
alpha--
alphaNumericalCharactersProcessed++
alphaNumericalCharactersProcessed--
--alphaNumericalCharactersProcessed

(48.1%)
(5.8%)
(1.6%)

Figure 22. C# examples from our test set of the restricted completion task along with the predictions of our model and the baselines.

Structural Language Models of Code

public static int BinarySearch<TItem, TSearch>(
this IList<TItem> list, TSearch value,
Func<TSearch, TItem, int> comparer)

{

if (list == null)
{

throw new ArgumentNullException("list");

}
if (comparer == null)
{

throw new ArgumentNullException("comparer");

}

var lower = 0;
var upper = list.Count - 1;

while (lower <= upper)
{

var middle = lower + (upper - lower) / 2;
var comparisonResult = comparer(value, list[middle]);
if ( comparisonResult < 0 )
{

upper = middle - 1;

}
else if ( comparisonResult > 0 )
{

lower = middle + 1;

}
else
{

return middle;

}

}

return lower;

}

Model
True ref:

SLM (this work)

BiLSTM→LSTM +copy

GNN →NAG

Prediction
comparisonResult < 0
comparisonResult < 0
comparisonResult > 0
middle == comparisonResult
lowerResult == middle
lowerResult == 0
lower != middle
comparisonResult == 0
comparisonResult > 0
comparisonResult < 0

comparisonResult > 0
comparisonResult > 0
comparisonResult < 0
comparisonResult == 0
lower < 0
lower + "."
lower != middle
comparisonResult == 0
comparisonResult > 0
comparisonResult == middle

Figure 23. C# examples from our test set of the restricted completion task along with the predictions of our model and the baselines.

Structural Language Models of Code

public override string ToString()
{

// use reflection to display all the properties that
// ... have non default values
StringBuilder result = new StringBuilder();
var props = this.GetType().GetTypeInfo().DeclaredProperties;
result.AppendLine("{");
foreach (var prop in props)
{

if (prop.Name != "Content" && prop.Name != "Subtitle"

&& prop.Name != "Title" && prop.Name != "UniqueId")

{

}

object value = prop.GetValue(this);
bool valueIsNull = value == null;
object defaultValue = Common.GetDefault(prop.PropertyType);
bool defaultValueIsNull = defaultValue == null;
if ((valueIsNull != defaultValueIsNull)
// one is null when the other isn't

|| ( !valueIsNull

&& (value.ToString() != defaultValue.ToString())))

// both aren't null, so compare as strings

{

}

result.AppendLine(prop.Name + " : " + prop.GetValue(this));

}
result.AppendLine("}");
return result.ToString();

}

Model
True ref:

SLM (this work)

BiLSTM→LSTM +copy

GNN →NAG

Prediction
!valueIsNull
!valueIsNull
!defaultValueIsNull
!valueIsNull.IsNullOrEmpty()
!defaultValueIsNull
(defaultValueIsNull || value)
(defaultValueIsNull || defaultValue)
!valueIsNull
!defaultValueIsNull
!!valueIsNull

(52.4%)
(9.0%)
(3.2%)

Figure 24. C# examples from our test set of the restricted completion task along with the predictions of our model and the baselines.

Structural Language Models of Code

public TradierOrderResponse PlaceOrder(string accountId,

TradierOrderClass classification,
TradierOrderDirection direction,
string symbol,
decimal quantity,
decimal price = 0,
decimal stop = 0,
string optionSymbol = "",
TradierOrderType type = TradierOrderType.Market,
TradierOrderDuration duration = TradierOrderDuration.GTC)

{

}

//Compose the request:
var request = new RestRequest("accounts/{accountId}/orders");
request.AddUrlSegment("accountId", accountId.ToString());

//Add data:
request.AddParameter("class", GetEnumDescription(classification));
request.AddParameter("symbol", symbol);
request.AddParameter("duration", GetEnumDescription(duration));
request.AddParameter("type", GetEnumDescription(type));
request.AddParameter("quantity", quantity);
request.AddParameter("side", GetEnumDescription(direction));

//Add optionals:
if (price > 0) request.AddParameter("price", Math.Round(price, 2));
if (stop > 0) request.AddParameter("stop", Math.Round(stop, 2));
if ( optionSymbol != "" )

request.AddParameter("option_symbol", optionSymbol);

//Set Method:
request.Method = Method.POST;

return Execute<TradierOrderResponse>(request,

TradierApiRequestType.Orders);

Model
True ref:

SLM (this work)

BiLSTM→LSTM +copy

GNN →NAG

Prediction
optionSymbol != ""
optionSymbol != ""
optionSymbol == ""
optionSymbol.IsNullOrEmpty()
!stopSymbol
stopSymbol != optionSymbol
(stopSymbol " && optionSymbol)
optionSymbol == <UNK>
optionSymbol == symbol
optionSymbol != symbol

(5.5%)
(4.4%)
(1.1%)

(Syntax error)

Figure 25. C# examples from our test set of the restricted completion task along with the predictions of our model and the baselines.

Structural Language Models of Code

[Test, TestCaseSource("GetLeanDataLineTestParameters")]
public void GetSourceMatchesGenerateZipFilePath(

LeanDataLineTestParameters parameters)

{

var source = parameters.Data.GetSource(

parameters.Config, parameters.Data.Time.Date, false);

var normalizedSourcePath = new FileInfo(source.Source).FullName;
var zipFilePath = LeanData.GenerateZipFilePath(
Globals.DataFolder, parameters.Data.Symbol,
parameters.Data.Time.Date,
parameters.Resolution, parameters.TickType);

var normalizeZipFilePath = new FileInfo(zipFilePath).FullName;
var indexOfHash = normalizedSourcePath.LastIndexOf(

"#", StringComparison.Ordinal);

if (indexOfHash > 0)
{

normalizedSourcePath =

normalizedSourcePath.Substring(0, indexOfHash) ;

}
Assert.AreEqual(normalizeZipFilePath, normalizedSourcePath);

}

Model
True ref:

SLM (this work)

BiLSTM→LSTM +copy

GNN →NAG

Prediction
normalizedSourcePath.Substring(0, indexOfHash)
normalizedSourcePath.Substring(0, indexOfHash)
normalizedSourcePath.Substring(1)
normalizedSourcePath.Remove(indexOfHash)
indexOfHash + "<UNK>"
indexOfHash > normalizedOfHash
indexOfHash > 0
normalizedSourcePath + normalizeZipFilePath
normalizedSourcePath + normalizedSourcePath
normalizedSourcePath + normalizeZipFilePath + <UNK>

(28.3%)
(8.8%)
(8.2%)

Figure 26. C# examples from our test set of the restricted completion task along with the predictions of our model and the baselines.

References

Structural Language Models of Code

Aharoni, R. and Goldberg, Y. Towards string-to-tree neural machine translation. In Proceedings of the 55th Annual Meeting

of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 132–140, 2017.

Allamanis, M. The adverse effects of code duplication in machine learning models of code. In Proceedings of the 2019
ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reﬂections on Programming and Software,
pp. 143–153. ACM, 2019.

Allamanis, M., Tarlow, D., Gordon, A., and Wei, Y. Bimodal modelling of source code and natural language. In Interna-

tional conference on machine learning, pp. 2123–2132, 2015.

Allamanis, M., Peng, H., and Sutton, C. A convolutional attention network for extreme summarization of source code. In

International conference on machine learning, pp. 2091–2100, 2016.

Allamanis, M., Brockschmidt, M., and Khademi, M. Learning to represent programs with graphs.

In International

Conference on Learning Representations, 2018.

Alon, U. and Yahav, E. On the bottleneck of graph neural networks and its practical implications. arXiv preprint

arXiv:2006.05205, 2020.

Alon, U., Zilberstein, M., Levy, O., and Yahav, E. A general path-based representation for predicting program properties.
In Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation, pp.
404–419, 2018.

Alon, U., Brody, S., Levy, O., and Yahav, E. code2seq: Generating sequences from structured representations of code. In

International Conference on Learning Representations, 2019a.

Alon, U., Zilberstein, M., Levy, O., and Yahav, E. code2vec: Learning distributed representations of code. Proceedings of

the ACM on Programming Languages, 3(POPL):1–29, 2019b.

Amodio, M., Chaudhuri, S., and Reps, T. Neural attribute machines for program generation.

arXiv preprint

arXiv:1705.09231, 2017.

Balog, M., Gaunt, A. L., Brockschmidt, M., Nowozin, S., and Tarlow, D. Deepcoder: Learning to write programs. In

International Conference on Learning Representations, 2017.

Bielik, P., Raychev, V., and Vechev, M. Phog: probabilistic model for code. In International Conference on Machine

Learning, pp. 2933–2942, 2016.

Brockschmidt, M., Allamanis, M., Gaunt, A. L., and Polozov, O. Generative code modeling with graphs. In International

Conference on Learning Representations, 2019.

Brody, S., Alon, U., and Yahav, E. Neural edit completion. arXiv preprint arXiv:2005.13209, 2020.

Chen, X., Liu, C., and Song, D. Tree-to-tree neural networks for program translation. In Advances in Neural Information

Processing Systems, pp. 2547–2557, 2018.

Devlin, J., Uesato, J., Bhupatiraju, S., Singh, R., Mohamed, A.-r., and Kohli, P. Robustﬁll: Neural program learning under

noisy i/o. In International Conference on Machine Learning, pp. 990–998, 2017.

Dong, L. and Lapata, M. Coarse-to-ﬁne decoding for neural semantic parsing. In Proceedings of the 56th Annual Meeting

of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 731–742, 2018.

Ellis, K., Nye, M., Pu, Y., Sosa, F., Tenenbaum, J., and Solar-Lezama, A. Write, execute, assess: Program synthesis with a

repl. In Advances in Neural Information Processing Systems, pp. 9169–9178, 2019.

Fernandes, P., Allamanis, M., and Brockschmidt, M. Structured neural summarization. In International Conference on

Learning Representations, 2019.

Gaunt, A. L., Brockschmidt, M., Kushman, N., and Tarlow, D. Differentiable programs with neural libraries. In Interna-

tional Conference on Machine Learning, pp. 1213–1222, 2017.

Structural Language Models of Code

Green, C. Application of theorem proving to problem solving. In Readings in Artiﬁcial Intelligence, pp. 202–222. Elsevier,

1981.

Gu, J., Lu, Z., Li, H., and Li, V. O. Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings
of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1631–1640,
2016.

Gulwani, S. Automating string processing in spreadsheets using input-output examples. In Proceedings of the 38th annual

ACM SIGPLAN-SIGACT symposium on Principles of programming languages, pp. 317–330, 2011.

Gulwani, S., Polozov, O., Singh, R., et al. Program synthesis. Foundations and Trends® in Programming Languages, 4

(1-2):1–119, 2017.

Iyer, S., Konstas, I., Cheung, A., and Zettlemoyer, L. Mapping language to code in programmatic context. In Proceedings

of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 1643–1652, 2018.

Iyer, S., Cheung, A., and Zettlemoyer, L. Learning programmatic idioms for scalable semantic parsing. In Proceedings of
the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP), pp. 5429–5438, 2019.

Kingma, D. and Ba, J. Adam: A method for stochastic optimization. In International Conference on Learning Represen-

tations, 2015.

Klein, G., Kim, Y., Deng, Y., Senellart, J., and Rush, A. M. Opennmt: Open-source toolkit for neural machine translation.

In Proceedings of ACL 2017, System Demonstrations, pp. 67–72, 2017.

Ling, W., Blunsom, P., Grefenstette, E., Hermann, K. M., Koˇcisk`y, T., Wang, F., and Senior, A. Latent predictor networks
In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics

for code generation.
(Volume 1: Long Papers), pp. 599–609, 2016.

Luong, M.-T., Pham, H., and Manning, C. D. Effective approaches to attention-based neural machine translation.
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1412–1421, 2015.

In

Maddison, C. and Tarlow, D. Structured generative models of natural source code. In International Conference on Machine

Learning, pp. 649–657, 2014.

Murali, V., Qi, L., Chaudhuri, S., and Jermaine, C. Neural sketch learning for conditional program generation. In Interna-

tional Conference on Learning Representations, 2018.

Parisotto, E., Mohamed, A.-r., Singh, R., Li, L., Zhou, D., and Kohli, P. Neuro-symbolic program synthesis. In Interna-

tional Conference on Learning Representations, 2017.

Pnueli, A. and Rosner, R. On the synthesis of a reactive module. In Proceedings of the 16th ACM SIGPLAN-SIGACT

symposium on Principles of programming languages, pp. 179–190. ACM, 1989.

Polozov, O. and Gulwani, S. Flashmeta: a framework for inductive program synthesis. In Proceedings of the 2015 ACM
SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications, pp. 107–
126, 2015.

Rabinovich, M., Stern, M., and Klein, D. Abstract syntax networks for code generation and semantic parsing. In Pro-
ceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.
1139–1149, 2017.

Raychev, V., Bielik, P., Vechev, M., and Krause, A. Learning programs from noisy data. In Proceedings of the 43rd Annual

ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pp. 761–774, 2016.

Si, X., Yang, Y., Dai, H., Naik, M., and Song, L. Learning a meta-solver for syntax-guided program synthesis.

In

International Conference on Learning Representations, 2019.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all

you need. In Advances in Neural Information Processing Systems, pp. 6000–6010, 2017.

Structural Language Models of Code

Waldinger, R. J. and Lee, R. C. Prow: A step toward automatic program writing. In Proceedings of the 1st international

joint conference on Artiﬁcial intelligence, pp. 241–252, 1969.

Xiao, C., Dymetman, M., and Gardent, C. Sequence-based structured prediction for semantic parsing. In Proceedings of
the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1341–1350,
2016.

Yin, P. and Neubig, G. A syntactic neural model for general-purpose code generation. In Proceedings of the 55th Annual

Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 440–450, 2017.

Yin, P., Neubig, G., Allamanis, M., Brockschmidt, M., and Gaunt, A. L. Learning to represent edits. In International

Conference on Learning Representations, 2019.

Young, H., Bastani, O., and Naik, M. Learning neurosymbolic generative models via program synthesis. In International

Conference on Machine Learning, pp. 7144–7153, 2019.

Yu, T., Li, Z., Zhang, Z., Zhang, R., and Radev, D. Typesql: Knowledge-based type-aware neural text-to-sql generation. In
Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 2 (Short Papers), pp. 588–594, 2018.

Zhao, R., Bieber, D., Swersky, K., and Tarlow, D. Neural networks for modeling source code edits. arXiv preprint

arXiv:1904.02818, 2019.

