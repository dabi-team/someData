2
2
0
2

r
p
A
7

]
S
D
.
s
c
[

1
v
9
6
5
3
0
.
4
0
2
2
:
v
i
X
r
a

Faster algorithms for learning to link, align sequences, and price
two-part tariﬀs

Maria-Florina Balcan
Carnegie Mellon University
ninamf@cs.cmu.edu

Christopher Seiler
Carnegie Mellon University
cseiler@cs.cmu.edu

Dravyansh Sharma
Carnegie Mellon University
dravyans@cs.cmu.edu

April 8, 2022

Abstract

Data-driven algorithm conﬁguration [16, 1] is a promising, learning-based approach for be-
yond worst-case analysis of algorithms with tunable parameters. An important open problem
is the design of eﬃcient data-driven algorithms for algorithm families with more than one pa-
rameter. In this work we provide algorithms for eﬃcient (output-polynomial) multidimensional
parameter tuning, i.e. for families with a small constant number of parameters, for three very dif-
ferent combinatorial problems — linkage-based clustering, dynamic programming for sequence
alignment, and auction design for two-part tariﬀ schemes. We extend the single-parameter
clustering algorithm of [4] to multiple parameters and to the sequence alignment problem by
proposing an execution graph which compactly represents all the states the algorithm could at-
tain for all possible parameter values. A key problem-speciﬁc challenge is to eﬃciently compute
how the partition of the parameter space (into regions with unique algorithmic states) changes
with a single algorithmic step. We give algorithms which improve on the runtime of previously
best known results for linkage-based clustering, sequence alignment and two-part tariﬀ pricing.

1

Introduction

The classic approach to the design and analysis of combinatorial algorithms (e.g. [21]) considers
worst-case problem instances on which the guarantees of the algorithm, say running time or memory
usage or the quality of solution, must hold. However, in several applications of algorithms in
practice, such as aligning genomic sequences or designing online ad auctions, one often ends up
solving multiple related problem instances. The data-driven algorithm design paradigm captures
this common setting and allows the design and analysis of algorithms that use machine learning
to learn how to solve the instances which come from the same domain [16, 1]. Typically there are
large (possibly inﬁnite) parameterized algorithm families to choose from, and data-driven algorithm

1

 
 
 
 
 
 
Problem

Linkage-based clustering

Global pairwise
sequence alignment
Two-part tariﬀ auctions

Parameter
dimensionality
d = 2
any d
d = 2
any d
d = 2

O(Rp3)

Our results

Baseline
(Prior Work)
O(p18 log p)
O(p8d+2 log p) O(R2p · LP(d, p2) + Rp3d)
O(R2 + Rmn)
—
O(N 3K3)

O(Rmn)
O((dL4)d ˜R2L+1Tdp)
O(RN K),
R = O(N 2K min{N, K})
O(R2(2L(cid:48))2L(cid:48)+1N K)

d = 2L(cid:48)

O((N K)O(L(cid:48)))

Table 1: Summary of running times of the proposed algorithms. R is the number of pieces in the
dual class function for the given problem instance (worst case polynomial in the input instance
for clustering and pricing problems). p is the size of the clustering instance. m, n are lengths of
sequences to be aligned, L is the maximum number of subproblems in the sequence alignment DP
update, ˜R is the maximum over the number of pieces in the dual class function for all subproblems,
TDP is the time to solve the DP on a single instance (typically poly(m, n)). K is the number of
units of the item sold, N is the number of auction samples and L(cid:48) is the menu length.

design approaches provide techniques to select algorithm parameters that provably perform well
for instances from the same domain. Data-driven algorithms have been proposed and analyzed for
a wide variety of combinatorial problems, including clustering, computational biology, mechanism
design, graph-based semi-supervised learning, and integer programming [10, 4, 3, 9, 11].

The parameterized family may occur naturally in well-known algorithms used in practice, or one
could potentially design new families interpolating known heuristics. For the problem of aligning
pairs of genomic sequences, one typically obtains the best alignment using a dynamic program with
some costs assigned to edits of diﬀerent kinds, such as insertions, substitutions, or reduplications
[25]. These costs are the natural parameters for the alignment algorithm. The quality of the
alignment can strongly depend on the parameters, and the best parameters can vary depending on
the application (e.g. aligning DNAs or RNAs), the pair of species being compared, or the purpose
of the alignment. Similarly, item prices are natural parameters in auction design [10, 8]. On
the other hand, for linkage-based clustering, one usually chooses from a set of diﬀerent available
heuristics, such as single or complete linkage. Using an interpolation of these heuristics to design
a parameterized family and tune the parameter, one can often obtain signiﬁcant improvements in
the quality of clustering [7, 4].

In our work, we approach a major open question in this area, speciﬁcally designing eﬃcient algo-
rithms for the case where the parameter space is multidimensional. A key challenge is that the
loss function for the parameterized combinatorial algorithms is discontinuous, and to successfully
optimize we need to ﬁnd a partition of the parameter space into regions or pieces where the loss
function behaves well. We provide multidimensional algorithms for computing these pieces for three
distinct problems — linkage-based clustering, global pairwise sequence alignment, and designing
two-part tariﬀ (TPT) auctions — that are eﬃcient for small constant dimension. These algorithms
signiﬁcantly increase the scope of application of the data-driven methods for these problems. For

2

Figure 1: The ﬁrst three levels of an example execution tree of a clustering instance on four points,
with a two-parameter algorithm (P = (cid:78)2). Successive partitions P0, P1, P2 are shown at merge
levels 0, 1, and 2, respectively, and the nested shapes show cluster merges.

sequence alignment, this allows us to learn the dynamic program over a much richer family of
algorithms. For linkage-based clustering, this allows us to interpolate several linkage heuristics to
learn the best for a given dataset, to interpolate several distance metrics, and even to learn the
best linkage procedure and the distance metric at the same time. Prior work [4] only show eﬃcient
implementations for interpolating single parameter families, i.e. a pair of heuristics or a pair of
distance metrics. We learn a convex combination of distance functions, optimizing a loss function
that is discontinuous, in contrast to the classic objective of learning a Mahalanobis distance func-
tion which involves convex optimization [26, 14]. For TPT auctions, our techniques yield eﬃcient
output-polynomial algorithms that improve on the eﬃciency of algorithms from prior work.

Summary of contributions. We design algorithmic techniques that lead to output polynomial
algorithms for three distinct problems. Our algorithms scale polynomially (linearly for d = 2) with
the size of output partition in the worst case (See Table 1) and are eﬃcient for small constant d.

1. We show how to learn multidimensional parameterized algorithm families in hierarchical clus-
tering algorithms. We provide the ﬁrst multi-parameter algorithms, which enable the interpo-
lation of several linkage heuristics, and simultaneously learning the linkage procedure and the
distance metric. Our algorithm extends the execution tree based approach introduced for a
single-parameter family in [4] to multiple dimensions (Figure 1). The key step in computing the
execution tree is to compute the convex polytopic subdivisions of the parameter space corre-
sponding to a single merge step in the linkage procedure. We do this in output-polynomial time
by removing redundant constraints in any polytope using Clarkson’s algorithm [12] and per-
forming an implicit search over the graph of neighboring polytopes. Our algorithm signiﬁcantly

3

reduces the running time for baselines obtained using [7] (See Table 1).

2. For dynamic programming based sequence alignment, we provide an algorithm which extends
the execution tree approach to an execution directed acyclic graph (DAG) based approach. To
implement the approach, we need novel subroutines for combining parameter space partitions
induced by the subproblems. For a small ﬁxed d, our algorithm is eﬃcient for small constant
L, the maximum number of subproblems needed to compute any single DP update. Prior work
by [17] only provides an algorithm for computing the full partition for d = 2, and the naive
approach comparing all pairs of alignments to compute the partition requires exponential time
in the input. In the two-parameter case, we improve on the running time on prior best results
from quadratic in the output (i.e. number of regions in parameter space partition) to linear.
3. For designing auctions with two-part tariﬀs [8], we provide an eﬃcient algorithm which gives
a ﬁrst output-sensitive algorithm for this setting, and improves the worst-case running time
complexity of prior best results (even for worst-case output sizes). Our algorithm adapts tech-
niques developed for the linkage-based clustering problem. We also extend our algorithm to a
more general setting with 2L(cid:48) tariﬀ parameters and again obtain improved, output-polynomial
runtime guarantees. In practice, L(cid:48) is a small constant [8].

Key insights and challenges. For linkage-based clustering, we extend the execution tree based
approach of [4] for single-parameter families to high dimensions. While in the single parameter
family the pieces are intervals, and computing their reﬁnements is achieved by simply determining
critical parameter values bounding the pieces for any node of the execution tree in order, we
approach the signiﬁcantly harder challenge of eﬃciently computing convex polytopic subdivisions.
Our approach involves enumerating the cells of a convex subdivision of the parameter space using
an implicit breadth-ﬁrst search on the graph of neighboring cells in the partition. For computing a
single cell, we use an eﬃcient output-sensitive algorithm (Clarkson’s algorithm [12]) for determining
the non-redundant constraints in a system of linear equations. For sequence alignment, we deﬁne
a compact representation of the execution DAG and use primitives from computational geometry,
in particular for overlaying two convex subdivisions in output-polynomial time. A key challenge
in this problem is that the execution graph is no longer a tree but a DAG, and we need to design
an eﬃcient representation for this DAG. Unlike the clustering problem, we cannot directly apply
Clarkson’s algorithm since the number of possible alignments of two strings (and therefore the
number of candidate hyperplanes) is exponential in the size of the strings. Instead, we carefully
combine the polytopes of subproblems in accordance with the dynamic program update for solving
the sequence alignment (for a ﬁxed value of parameters), and the number of candidate hyperplanes
is polynomial inside regions where all the relevant subproblems have ﬁxed optimal alignments.
Finally, for two-part tariﬀ auction design, we extend our algorithm for clustering to yield a ﬁrst
output-sensitive algorithm for this setting, and further show worst-case upper bounds on output
complexity using combinatorial arguments for counting cells in line arrangements induced by the
problem.

4

2 Notation and setup

We follow the notation of [1]. For a given algorithmic problem (say clustering, or sequence align-
ment), let Π denote the set of problem instances of interest. We also ﬁx a (potentially inﬁnite)
family of algorithms A, parameterized by a set P ⊆ Rd. Let Aρ denote the algorithm in the
family A parameterized by ρ ∈ P. The performance of any algorithm on any problem instance is
given by a utility function u : Π × P → [0, H], i.e. u(x, ρ) measures the performance on problem
instance x ∈ Π of algorithm Aρ ∈ A. The utility of a ﬁxed algorithm Aρ from the family is given
by uρ : Π → [0, H], with uρ(x) = u(x, ρ). We are interested in the structure of the dual class of
functions ux : P → [0, H], with ux(ρ) = uρ(x), which measure the performance of all algorithms of
the family for a ﬁxed problem instance x ∈ Π. For many parameterized algorithms, the dual class
functions are piecewise structured in the following sense [3].

Deﬁnition 1 (Piecewise structured with linear boundaries). A function class H ⊆ RY that maps
a domain Y to R is (F, G, k)-piecewise decomposable for a class G ⊆ {0, 1}Y of linear threshold
functions and a class F ⊆ RY of piece functions if the following holds: for every h ∈ H, there are k
linear threshold functions g1, . . . , gk ∈ G and a piece function fb ∈ F for each bit vector b ∈ {0, 1}k
such that for all y ∈ Y, h(y) = fby (y) where by = (g1(y), . . . , gk(y)) ∈ {0, 1}k.

We will refer to connected subsets of the parameter space where the dual class function is a ﬁxed
piece function fc as the (dual) pieces or regions, when the dual class function is clear from the
context. We develop techniques that yield eﬃcient algorithms for computing these pieces in the
multiparameter setting, i.e. constant d > 1, for a variety of problems which have this property.
The running times of our algorithms are polynomial in the output size (i.e, number of pieces). For
i ∈ Z+, we will use [i] to denote the set of positive integers {1, . . . , i}. We will use I{·} to denote
the indicator function which takes values in {0, 1}.

3 Linkage-based clustering

Clustering data into groups of similar points is a fundamental tool in data analysis and unsupervised
machine learning. A variety of clustering algorithms have been introduced and studied but it is
not clear which algorithms will work best on speciﬁc tasks. Also the quality of clustering is heavily
dependent on the distance metric used to compare data points. Interpolating multiple metrics and
distance functions can result in signiﬁcantly better clustering [4].

Problem setup: Let X be the data domain. A clustering instance from the domain consists of a point
set S = {x1, . . . , xn} ⊆ X and an (unknown) target clustering C = (C1, . . . , Ck), where the sets
C1, . . . , Ck partition S into k clusters. Linkage-based clustering algorithms output a hierarchical
clustering of the input data, represented by a cluster tree. We measure the agreement of a cluster
tree T with the target clustering C in terms of the Hamming distance between C and the closest
pruning of T that partitions it into k clusters (i.e., k disjoint subtrees that contain all the leaves
of T ). More formally, the loss (cid:96)(T, C) = minP1,...,Pk minσ∈Sn
i=1 |Ci \ Pσi|, where the ﬁrst

(cid:80)k

1
|S|

5

minimum is over all prunings P1, . . . , Pk of the cluster tree T into k subtrees, and the second
minimum is over all permutations of the k cluster indices.

A merge function D deﬁnes the distance between a pair of clusters Ci, Cj ⊆ X in terms of the
pairwise point distances given by a metric d. Cluster pairs with smallest values of the merge
function are merged ﬁrst. For example, single linkage uses the merge function Dsgl(Ci, Cj; d) =
mina∈Ci,b∈Cj d(a, b) and complete linkage uses Dcmpl(Ci, Cj; d) = maxa∈Ci,b∈Cj d(a, b). Instead of
using extreme points to measure the distance between pairs of clusters, one may also use more
central points, e.g. we deﬁne median linkage as Dmed(Ci, Cj; d) = median({d(a, b) | a ∈ Ci, b ∈ Cj}),
where median(·) is the usual statistical median of an ordered set S ⊂ R1. Appendix B provides
synthetic clustering datasets where one of single, complete or median linkage leads to signiﬁcantly
better clustering than the other two. Single, median and complete linkage are 2-point-based ([4];
also Deﬁnition 4 in Appendix D), i.e. the merge function D(A, B; d) only depends on the distance
d(a, b) for two points (a, b) ∈ A × B.

α (A, B; δ) = (cid:80)

Let ∆ = {D1, . . . , Dl} denote a ﬁnite family of merge functions (measure distances between clusters)
and δ = {d1, . . . , dm} be a ﬁnite collection of distance metrics (measure distances between points).
We deﬁne a parameterized family of linkage-based clustering algorithms that allows us to learn
both the merge function and the distance metric. It is given by the interpolated merge function
D∆
Di∈∆,dj ∈δ αi,jDi(A, B; dj), where α = {αi,j | i ∈ [l], j ∈ [m], αi,j ≥ 0}. In order
to ensure linear boundary functions for the dual class function, our interpolated merge function
D∆
α (A, B; δ) takes all pairs of distance metrics and linkage procedures. Due to invariance under
constant multiplicative factors, we can set (cid:80)
i,j αi,j = 1 and obtain a set of parameters which
allows α to be parameterized by d = lm − 1 values2. Deﬁne the parameter space P = (cid:78)d =
(cid:110)
ρ ∈ (cid:0)R≥0(cid:1)d | (cid:80)
; for any ρ ∈ P we get α(ρ) ∈ Rd+1 as α(ρ)
d+1 =
1 − (cid:80)
i∈[d] ρi.

i = ρi for i ∈ [d], α(ρ)

i ρi ≤ 1

(cid:111)

ρ (A, B; δ) to denote the interpolated merge function D∆

We focus on learning the optimal ρ ∈ P for a single instance (S, Y). With slight abuse of notation
we will sometimes use D∆
α(ρ)(A, B; δ). As
a special case we have the family D∆
ρ (A, B; d0) that interpolates merge functions (from set ∆)
for diﬀerent linkage procedures but the same distance metric d0. Another interesting family only
interpolates the distance metrics, i.e. use a distance metric dρ(a, b) = (cid:80)
j dj(a, b) and use a
ﬁxed linkage procedure. We denote this by D1

dj ∈δ α(ρ)

ρ(A, B; δ).

3.1 Overview of techniques

First, we show that for a ﬁxed clustering instance S ∈ Π, the dual class function uS(·) is piecewise
constant with a bounded number of pieces and linear boundaries. We only state the result for the
interpolation of merge functions D∆
ρ (A, B; d0) below for ∆ a set of 2-point-based merge functions,

1median(S) is the smallest element of S such that S has at most half its elements less than median(S) and at most
half its elements more than median(S). For comparison, the more well-known average linkage is Davg(Ci, Cj; d) =
mean({d(a, b) | a ∈ Ci, b ∈ Cj}). We may also use geometric medians of clusters. For example, we can deﬁne mediod
linkage as Dgeomed(Ci, Cj; d) = d(arg mina∈Ci

d(a, a(cid:48)), arg minb∈Cj

d(b, b(cid:48))).

(cid:80)

(cid:80)

a(cid:48)∈Ci

b(cid:48)∈Cj

2In contrast, the parametric family in [4] has l + m − 2 parameters but it does not satisfy Deﬁnition 1.

6

and defer the proof and analogous results for the general family D∆
interpolation D1

ρ(A, B; δ) to Appendix D.

ρ (A, B; δ) and distance metric

Lemma 3.1. Let ∆ be a set of 2-point-based merge functions. Consider the family of clustering
algorithms with the parameterized merge function D∆
ρ (A, B; d0). Let U be the set of functions {uρ :
S (cid:55)→ u(S, ρ) | ρ ∈ Rd} that map a clustering instance S to R. The dual class U ∗ is (F, G, |S|4d+4)-
piecewise decomposable, where F = {fc : U → R | c ∈ R} consists of constant functions fc : uρ (cid:55)→ c
and G = {gw : U → {0, 1} | w ∈ Rd} consists of halfspace indicator functions gw : uρ (cid:55)→ I{w ·ρ < 0}.

We will extend the execution tree approach introduced by [4] for computing the pieces (intervals) of
single-parameter linkage-based clustering. Informally, the execution tree is deﬁned as the partition
tree where each node represents an interval where the ﬁrst t merges are identical, and edges corre-
spond to the subset relationship between intervals obtained by reﬁnement from a single merge. The
execution, i.e. the sequence of merges, is unique along any path of this tree. The same properties,
i.e. reﬁnement of the partition with each merge and correspondence to the algorithm’s execution,
continue to hold in the multidimensional case, but with convex polytopes instead of intervals. A
formal treatment of the execution tree is deferred to Appendix D.1. Computing the children of
any node of the execution tree corresponds to computing the subdivision of a convex polytope into
polytopic cells where the next merge step is ﬁxed. We compute the cells by following the neighbors,
keeping track of the cluster pairs merged for the computed cells to avoid recomputation. For any
single cell, we ﬁnd the bounding polytope along with cluster pairs corresponding to neighboring
cells by employing an eﬃcient algorithm for determining the tight constraints in a linear system.
Theorem 3.3 gives the runtime complexity of the proposed algorithm for computing the children of
any node of the execution tree. Theorem 3.5 provides the total time to compute all the leaves of
the execution tree.

3.2 Eﬃcient algorithm to enumerate cells of a subdivision

Suppose we have executed a ﬁxed sequence of t merges within a cell P ∈ Pt; let nt = n − t be
the number of unmerged clusters, i.e. C1, . . . , Cnt are collections of points from the instance S.
For readability, we let Dρ(i, j) denote Dρ(Ci, Cj; δ). Our approach to enumerate the cells of a
subdivision is divided into two parts: ﬁrst, given an arbitrary cluster pair i, j ∈ [nt], we determine
the boundary of the cell Q ∈ Pt+1 for which cluster pair i, j is merged (Algorithm 1); then, we
iteratively search for cluster pairs merged in adjacent cells to Q in Pt+1 (Algorithm 2). For each
i, j ∈ [nt] deﬁne Qi,j ⊆ P to be the cell such that if ρ ∈ Qi,j, then cluster i is merged with cluster
j. Now, Qi,j is exactly the set of points

Qi,j = {ρ ∈ P : Dρ(i, j) ≤ Dρ(i(cid:48), j(cid:48)) for all i(cid:48), j(cid:48) ∈ [nt]}.

To provide an output-sensitive guarantee for this problem, we compute only the non-redundant
half-spaces which provide the boundary of each cell Q. First, we provide the exact notion of a
redundant constraint.

7

Algorithm 1: ComputeVertexPolytope
Input: Region P , cluster pair i, j
L ← P
for all cluster pairs i(cid:48), j(cid:48) ∈ [nt] with {i(cid:48), j(cid:48)} (cid:54)= {i, j} do

/* All cells must be fully contained in P */

Add the half-space inequality bT ρ ≤ c corresponding to Dρ(i, j) ≤ Dρ(i(cid:48), j(cid:48)) to L

I ← Clarkson(L)
A = {(i(cid:48), j(cid:48)) : i(cid:48), j(cid:48) ∈ [nt] and the constraint corresponding to i(cid:48), j(cid:48) is in I}
Q ← {L[(cid:96)] : (cid:96) ∈ I}
return (Q, A)

/* Label the constraint (b, c) with cluster pair (i(cid:48), j(cid:48)) */

Deﬁnition 2 (Redundant constraint). Consider a list L of k half-space constraints, where each
i x ≤ bi for some ai ∈ Rd, bi ∈ R. We say that constraint
half-space constraint is of the form aT
i ∈ [k] is redundant if constraint Li is implied by all other constraints; in other words, removing
constraint i does not change the set of solutions to L.

An important ingredient for ensuring good output-sensitive running time in our algorithm is Clark-
son’s algorithm [12]. The key idea for Clarkson’s algorithm is to maintain a set I of non-redundant
contraints detected so far, and solve LPs that detect redundancy of a remaining constraint (not in
I) when added to I. If the constraint is redundant relative to I, it must also be redundant in the
full linear sytem, otherwise we can add a (potentially diﬀerent) non-redundant constraint to I (see
Appendix C for details). The following runtime guarantee is known for the algorithm.

Theorem 3.2 (Clarkson’s algorithm [12]). Given a list L of k half-space constraints in d dimen-
sions, Clarkson’s algorithm outputs a set I ⊆ [k] yielding an equivalent solution set to L without
redundant constraints. Furthermore, Clarkson’s algorithm runs in time O(k · LP(d, s + 1)), where
s = |I| and LP(v, c) is the time required to solve an LP with v variables and c constraints.

Using Clarkson’s algorithm with the name Clarkson, we present our subroutine (Algorithm 1),
which is a simple wrapper of Clarkson along with the boundary constraints.

In order to provide our key algorithm, we use Algorithm 1 along with a search for neighboring
regions as follows.

Eﬀectively, our algorithm can be seen as a breadth-ﬁrst search algorithm, where neighbors are
computed dynamically by ComputeVertexPolytope. However, the underlying graph which we
are traversing is unknown; neighbors must be computed on-the-ﬂy by ComputeVertexPolytope.
At the end of our algorithm, we output all the cluster pairs (i, j) we found, as well as their
associated polytopes. To analyze the performance of ComputeSubdivision, we formally deﬁne
the underlying graph being searched.

Deﬁnition 3. Deﬁne the region adjacency graph of the polytope P , written GP = (VP , EP ), as
follows:

• VP are the (i, j) for 1 ≤ i < j ≤ nt;

8

Algorithm 2: ComputeSubdivision
Input: Region P
ρ0 ← an arbitrary point in P
(i0, j0) ← arg mini,j Dρ0(i, j; δ)
mark ← ∅, polys ← new hashtable, q ← new queue
q.enqueue((i0, j0))
while q.non empty() do
(i, j) ← q.dequeue()
Continue to next iteration if (i, j) ∈ mark
mark ← mark ∪ {(i, j)}
(poly, neighbors) ← ComputeVertexPolytope(P, i, j)
polys[(i, j)] ← poly
q.enqueue((i(cid:48), j(cid:48))) for each (i(cid:48), j(cid:48)) ∈ neighbors

return polys

• For (i, j), (i(cid:48), j(cid:48)) ∈ VP , add the edge {(i, j), (i(cid:48), j(cid:48))} to EP if Qi,j ∩ Qi(cid:48),j(cid:48)

(cid:54)= ∅. Equivalently,

there exists some ρ ∈ Qi,j such that Dρ(i, j) = Dρ(i(cid:48), j(cid:48)).

We write V ∗

P to denote the set of vertices with non-empty corresponding regions Qi,j; that is,

V ∗
P = {(i, j) | Qi,j (cid:54)= ∅}.

This allows us to state the following guarantee about the runtime of Algorithm 2. A key useful
observation needed for the proof is that the vertices V ∗
P form a connected component of GP (Lemma
D.8).

Theorem 3.3. Suppose that |EP | = E and |V ∗
O(n2
and LP(d, E) is the cost of solving a linear program with d variables and E constraints.

P in time
t K), where K is the time it takes to compute the merge function Dρ(i, j)

P | = V ; then, we can compute the set V ∗

t · LP(d, E) + V · n2

t · LP(d, deg(i, j)) + n2

P , the algorithm ComputeVertexPolytope lists
Proof. It suﬃces to argue that, given (i, j) ∈ V ∗
t K) time. Then, since all the elements of V ∗
(i, j)’s neighbors in GP in O(n2
P
form a connected component (Lemma D.8), enumerating V ∗
P reduces to a breadth-ﬁrst search (BFS)
on GP . If the cost of computing the neighborhood N ((i, j)) is n2
t · f (d, deg(i, j), K) + g(d, nt, K)
for some functions f , g (where f is superadditive in the second argument), then when we sum over
all the vertices in V ∗
P , by the Handshake Lemma the overall cost of listing neighbors is at most
O(n2
t · f (d, E, K) + V · g(d, nt, K)). Here, we show that
f (d, E, K) ≤ LP(d, E) and g(d, nt, K) ≤ n2
Now, to show that ComputeVertexPolytope has the desired time bound, note that we have
t K) computation to list all of the half-space constraints. Then, Clarkson runs in time
an O(n2
O(n2
t constraints, d dimensions, and deg(i, j) nonredundant
constraints. Thus f (d, E, K) = LP(d, E), which is superadditive in E, and g(d, nt, K) ≤ n2
t K, as
desired.

t · LP(d, deg(i, j))), since there are n2

P | · g(d, nt, K)) = O(n2

t · f (d, |EP |, K) + |V ∗

t K.

9

Remark 3.4. In the case of single, median, and complete linkage, we may assume K = O(1) by
carefully maintaining a hashtable containing distances between every pair of clusters. Each merge
requires overhead at most O(n2
t ), which is absorbed by the cost of solving the LP corresponding to
the cell of the merge.

Now that we have proven runtime bounds for a single iteration of ComputeSubdivision, we
can provide a bound for the overall runtime of the algorithm. In particular this implies that our
algorithm is output-linear for d = 2. (Proof in Appendix D.2)

Theorem 3.5. Let S be a clustering instance with |S| = n, and let Ri = |Pi| and R = Rn.
Furthermore, let Ht = (cid:12)
(cid:12) denote the number of adjacencies between
any two pieces of Pi and H = Hn. Then, the leaves of the execution tree on S can be computed in
time O (cid:0)(cid:80)n

i=1 (d! · Hi + RiK) (n − i + 1)2(cid:1).

t | Q1 ∩ Q2 (cid:54)= ∅}(cid:12)

(cid:12){(Q1, Q2) ∈ P 2

We have the following corollary which states that our algorithm is output-linear for d = 2.

Corollary 3.6. Suppose d = 2; then, computing the leaves of the execution tree of any clustering
instance S can be done in time O(RKn3).

Proof. The key observation is that on any iteration i, the number of adjacencies Hi = O(Ri). This
is because for any region P ∈ Pi, P is a polygon divided into convex subpolygons, and the graph
GP has vertices which are faces and edges which cross between faces. Since the subdivision of P
can be embedded in the plane, so can the graph GP . Thus GP is planar, meaning Hi = O(Ri).
Plugging into Theorem 3.5, noting that (n − i + 1)2 ≤ n2, Hi ≤ H, and Ri ≤ R, we obtain the
desired runtime bound of

(cid:33)

(R + RK)n2

= O(RKn3).

(cid:32) n
(cid:88)

O

i=1

4 Learning weights for global sequence alignment

Sequence alignment is a fundamental combinatorial problem with applications to computational
biology. For example, to compare two DNA, RNA or amino acid sequences the standard approach
is to align two sequences to detect similar regions and compute the minimum edit distance [13,
25, 15]. However, the optimal alignment depends on the relative costs or weights used for speciﬁc
substitutions, insertions/deletions, or gaps in the sequences (consecutive deletions, which may have
a diﬀerent cost than a single deletion). Given a set of weights, the minimum edit distance alignment
computation is a simple dynamic program. Our goal is to learn the weights, such that the alignment
produced by the dynamic program has application-speciﬁc desirable properties.

Formally, we are given a pair of sequences s1, s2 over some alphabet Σ of lengths m = |s1| and
n = |s2|. Let − /∈ Σ denote the space character. A space-extension t of a sequence s over Σ is a
sequence over Σ ∪ {−} such that removing all occurrences of − in t gives s. A global alignment (or

10

simply alignment) of s1, s2 is a pair of sequences t1, t2 such that |t1| = |t2|, t1, t2 are space-extensions
of s1, s2 respectively, and for no 1 ≤ i ≤ |t1| we have t1[i] = t2[i] = −. Let s[i] denote the i-th
character of a sequence s and s[: i] denote the ﬁrst i characters of sequence s. For 1 ≤ i ≤ |t1|, if
t1[i] = t2[i] we call it a match. If t1[i] (cid:54)= t2[i], and one of t1[i] or t2[i] is the character − we call it
a space, else it is a mismatch. A sequence of − characters (in t1 or t2) is called a gap. Matches,
mismatches, gaps and spaces are commonly used features of an alignment, i.e. functions that map
sequences and their alignments (s1, s2, t1, t2) to Z≥0 (for example, the number of spaces). A common
measure of cost of an alignment is some linear combination of features. For example if there are d
features given by lk(·), k ∈ [d], the cost may be given by c(s1, s2, t1, t2, ρ) = (cid:80)d
k=1 ρklk(s1, s2, t1, t2)
where ρ = (ρ1, . . . , ρd) are the parameters [19]. Let τ (s, s(cid:48), ρ) = arg mint1,t2 c(s, s(cid:48), t1, t2, ρ) and
C(s, s(cid:48), ρ) = mint1,t2 c(s, s(cid:48), t1, t2, ρ) denote the optimal alignment and its cost respectively.

For a ﬁxed ρ, suppose the sequence alignment problem can be solved, i.e. we can ﬁnd the alignment
with the smallest cost, using a dynamic program Aρ with linear parameter dependence (described
below), which holds for typical features (see Appendix E for well-known examples). For any problem
(s1, s2) ∈ Π, the dynamic program Aρ (ρ ∈ P) solves a set π(s1, s2) = {Pi | i ∈ [k], Pk = (s1, s2)}
of subproblems (typically, π(s1, s2) ⊆ Πs1,s2 = {(s1[: i(cid:48)], s2[: j(cid:48)]) | i(cid:48) ∈ {0, . . . , m}, j(cid:48) ∈ {0, . . . , n}} ⊆
Π) in some ﬁxed order P1, . . . , Pk = (s1, s2). Crucially, the subproblems sequence P1, . . . , Pk do
not depend on ρ3. In particular, a problem Pj can be eﬃciently solved given optimal alignments
and their costs (τi(ρ), Ci(ρ)) for problems Pi for each i ∈ [j − 1]. To solve a (non base case)
subproblem Pj, we consider V alternative cases q : Π → [V ], i.e. Pj belongs to exactly one of the
if Pj(s1[: i(cid:48)], s2[: j(cid:48)]), we could have two cases corresponding to s1[i(cid:48)] = s2[j(cid:48)] and
V cases (e.g.
s1[i(cid:48)] (cid:54)= s2[j(cid:48)]). Typically, V will be a small constant. For any case v = q(Pj) ∈ [V ] that Pj may
belong to, the cost of the optimal alignment of Pj is given by a minimum over Lv terms of the form
cv,l(ρ, Pj) = ρ·wv,l+σv,l(ρ, Pj), where l ∈ [Lv], wv,l ∈ Rd, σv,l(ρ, Pj) = Ct(ρ) ∈ {C1(ρ), . . . , Cj−1(ρ)}
is the cost of some previously solved subproblem Pt = (s1[: i(cid:48)
v,l,j]) (i.e.
t depends on v, l, j but not on ρ), and cv,l(ρ, Pj) is the cost of alignment τv,l(ρ, Pj) = Tv,l(τt(ρ))
which extends the optimal alignment for subproblem Pt by a ρ-independent transformation Tv,l(·).
That is, the DP update for computing the cost of optimal alignment takes the form

t]) = (s1[: i(cid:48)

v,l,j], s2[: j(cid:48)

t], s2[: j(cid:48)

DP (ρ, Pj) = min

{ρ · wq(Pj ),l + σq(Pj ),l(ρ, Pj)},

l

(1)

and the optimal alignment is given by DP (cid:48)(ρ, Pj) = τq(Pj ),l∗(ρ, Pj), where l∗ = arg minl{ρ·wq(Pj ),l +
σq(Pj ),l(ρ, Pj)}. The DP speciﬁcation is completed by including base cases {C(s, s(cid:48), ρ) = ρ · ws,s(cid:48) |
| (s, s(cid:48)) ∈ B(s1, s2)} for the optimal alignment DP)
(s, s(cid:48)) ∈ B(s1, s2)} (or {τ (s, s(cid:48), ρ) = τs,s(cid:48)
corresponding to a set of base case subproblems B(s1, s2) ⊆ Πs1,s2. Let L = maxv∈[V ] Lv denote
the maximum number of subproblems needed to compute a single DP update in any of the cases.
Our main result is to provide an algorithm for computing the polytopic pieces of the dual class
functions in output-polynomial time for small constants d and L.

3For the sequence alignment DP in Appendix E.1, we have π(s1, s2) = Πs1,s2 and we ﬁrst solve the base case
subproblems (which have a ﬁxed optimal alignment for all ρ) followed by problems (s1[: i(cid:48)], s2[: j(cid:48)]) in a non-decreasing
order of i(cid:48) + j(cid:48) for any value of ρ.

11

P [i −1][j −1]
(ATGG,TGC)

P [i − 1][j]
(ATGG,TGCA)

P [i][j]

(ATGGC,TGCA)

Overlay

Subdivide

Merge

P [i][j − 1]
(ATGGC,TGC)

Figure 2: Incoming nodes used for computing the pieces at the node P [i][j] in the execution DAG.

1, t(cid:48)

2) (cid:54)= (t1, t2), c(s1, s2, t1, t2, ρ) ≤ c(s1, s2, t(cid:48)

Related work and overview of new techniques: [17] provide a solution for computing the pieces for
sequence alignment with only two free parameters, i.e. d = 2 using a ray-search based approach
([23]) in O(R2 + RTdp) time, where R is the number of pieces in u(s1,s2)(·). [19] solve the simpler
problem of ﬁnding a point in the polytope (if one exists) in the parameter space where a given
alignment (t1, t2) is optimal by designing an eﬃcient separation oracle for a linear program with
one constraint for every alignment (t(cid:48)
2, ρ), where
ρ ∈ Rd for general d. We provide a ﬁrst (to our knowledge) algorithm for the global pairwise
sequence alignment problem with general d for computing the full polytopic decomposition of the
parameter space with ﬁxed optimal alignments in each polytope. We adapt the idea of execution
tree to the sequence alignment problem by deﬁning an execution DAG of decompositions for preﬁx
sequences (s[: i], s[: j]). We overlap the decompositions corresponding to the subproblems in the
DP update and sub-divide each piece in the overlap using ﬁxed costs of all the subproblems within
each piece. The number of pieces in the overlap are output polynomial (for constant d and L) and
the sub-division of each piece involves at most L2 hyperplanes. By memoizing decompositions and
optimal alignments for the subproblems, we obtain the above output-polynomial runtime guarantee
for constant d, L. For d = 2, we get a running time of O(RTdp).

1, t(cid:48)

4.1 Eﬃcient algorithm for global sequence alignment

We consider the family of dynamic programs Aρ which compute the optimal alignment τ (s1, s2, ρ)
given any pair of sequences (s1, s2) ∈ Σm × Σn = Π for any ρ ∈ Rd. For any alignment (t1, t2),
the algorithm has a ﬁxed real-valued utility (diﬀerent from the cost function above) which captures
the quality of the alignment, i.e. the utility function u((s1, s2), ρ) only depends on the alignment
τ (s1, s2, ρ). The dual class function is piecewise constant with convex polytopic pieces (Lemma in
Appendix G). Thus, for any ﬁxed problem (s1, s2), the space of parameters ρ can be partitioned into
R convex polytopic regions where the optimal alignment (and therefore the dual class function) is
ﬁxed. The optimal parameter can then be found by simply comparing the costs of the alignments in
each of these pieces. For the rest of this section we consider the algorithmic problem of computing

12

these R pieces eﬃciently.

For the clustering algorithm family, as we have seen in Section 3, we get a reﬁnement of the
parameter space with each new step (merge) performed by the algorithm. This does not hold for the
sequence alignment problem. Instead we obtain the following DAG, from which the desired pieces
can be obtained by looking at nodes with no out-edges (call these terminal nodes) . For subproblems
falling under the base cases, we have DAGs with no edges. Using the recurrence relation (1), we
note that the optimal alignment for the pair of sequences (s1[: i], s2[: j]) can be obtained from the
optimal alignments for subproblems {(s1[: iv(cid:48),l], s2[: jv(cid:48),l])}l∈[Lv(cid:48) ] where v(cid:48) = q(s1[: i], s2[: j]). The
DAG for (s1[: i], s2[: j]) is therefore simply obtained by using the DAGs Gv(cid:48),l for the subproblems
and adding directed edges from the terminal nodes of Gv(cid:48),l to new nodes vp,i,j corresponding to each
piece p of the partition P [i][j] of P given by the set of pieces of u(s1[:i],s2[:j])(ρ). A more compact
representation of the execution graph would have only a single node vi,j for each subproblem
(s1[: i], s2[: j]) (storing the corresponding partition P [i][j]) and edges directed towards vi,j from
nodes of subproblems used to solve (s1[: i], s2[: j]). A naive representation of the execution graph
to encode this relationship would yield an exponentially large tree corresponding to the recursion
tree of the recurrence relation (1).

Formally we deﬁne a compact execution graph Ge = (Ve, Ee) as follows. For the base cases, we have
nodes labeled by (s, s(cid:48)) ∈ B(s1, s2) storing the base case solutions (ws,s(cid:48), τs,s(cid:48)) over the unpartitioned
parameter space P = Rd. For i, j > 0, we have a node vi,j labeled by (s1[: i], s2[: j]) and the
corresponding partition P [i][j] of the parameter space, with incoming edges from nodes of the
relevant subproblems. This graph is a DAG since every directed edge is from some node vi,j to a
node vi(cid:48),j(cid:48) with i(cid:48) + j(cid:48) > i + j; an example showing a few nodes of the DAG is depicted in Figure
2. Algorithm 3 gives a procedure to compute the partition of the parameter space for any given
problem instance (s1, s2) using the compact execution DAG.

Algorithm 3: ComputeCompactExecutionDAG
Input: Execution DAG Ge = (Ve, Ee), problem instance (s1, s2)
P0 ← P
v1, . . . , vn ← topological ordering of vertices Ve
for i = 1 to n do

Let Si be the set of nodes with incoming edges to vi
For vs ∈ Si, let Ps denote the partition corresponding to vs
Pi ← ComputeOverlayDP({Ps | s ∈ Si})
for each p ∈ Pi do

p(cid:48) ← ComputeSubdivisionDP(p, (s1, s2))
Pi ← Pi \ {p} ∪ p(cid:48)

Pi ← ResolveDegeneraciesDP(Pi)

return Partition Pn

/* From previous iterations */

To implement Algorithm 3, we will need to specify implementations of the ComputeOverlayDP,
ComputeSubdivisionDP and ResolveDegeneraciesDP subroutines. We defer the formal de-
tails to Appendix F in favor of brief sketches. ComputeOverlayDP computes an overlay of

13

polytopic subdivisions and uses Clarkson for intersecting polytopes with output-sensitive eﬃ-
ciency. ComputeSubdivisionDP adapts the ComputeSubdivision Algorithm 2, in each piece
of the overlay we need to ﬁnd the polytopic subdivision induced by O(L2) hyperplanes (the set
of hyperplanes depends on the piece). Finally ResolveDegeneraciesDP merges pieces where
the optimal alignment is identical (Merge in Figure 2) using a simple search over the resulting
subdivision.

For our implementation of the subroutines, we have the following guarantee for Algorithm 3 (Proof
and implementation details in Appendix F).

Theorem 4.1. Let Ri,j denote the number of pieces in P [i][j], and ˜R = maxi≤m,j≤n Ri,j. If the time
complexity for computing the optimal alignment is O(Tdp), then Algorithm 3 can be used to compute
the pieces for the dual class function for any problem instance (s1, s2), in time O(d!L4d ˜R2L+1Tdp).

Proof Sketch. The overlay Pi has at most ˜RL cells, each cell can be computed by redundant hyper-
plane removal (say using Clarkson’s algorithm) in ˜O(d! ˜R2L+1) time. For any cell in the overlay, all
the required subproblems have a ﬁxed optimal alignment, and there are at most L2 hyperplanes
across which the DP update can change. As a consequence we can ﬁnd the subdivision of each cell
in Pi by adapting Algorithm 2 in ˜O( ˜R2L) total time, and the number of cells in the subdivided
is still ˜RL. Resolving degeneracies is easily done by a simple BFS ﬁnding maximal
overlay P (cid:48)
i
i , again in ˜O( ˜R2L) time. Finally the subroutines
components over the cell adjacency graph of P (cid:48)
(except ComputeSubdivisionDP, for which we added time across all p ∈ Pi) run |Ve| times, the
number of nodes in the execution DAG, which is O(Tdp).

For the special case of d = 2, we have an algorithm which runs in time O(RTdp), where R is the
number of pieces in P [m][n] which improves on the prior result O(R2 + RTdp) for two-parameter
sequence alignment problems. The algorithm employs the ray search technique of [23] (also em-
ployed by [17] but for more general sequence alignment problems) and enjoys the following runtime
guarantee (proof in Appendix G).

Theorem 4.2. For the global sequence alignment problem with d = 2, for any problem instance
(s1, s2), there is an algorithm to compute the pieces for the dual class function in O(RTdp) time,
where Tdp is the time complexity of computing the optimal alignment for a ﬁxed parameter ρ ∈ R2,
and R is the number of pieces of u(s1,s2)(·).

5 Eﬃcient revenue-maximizing two-part tariﬀ auction design

A two-part tariﬀ consists of a ﬁxed price p1 and a price p2 per unit of item sold. For N samples
of the auction and K units of the auctioned item, [8] give an algorithm for computing the revenue
maximizing tariﬀ in O(N 3K3) time for a two-part tariﬀ scheme. In this work we provide a more
eﬃcient algorithm with output-sensitive time complexity of O(RN K), where R is the number of
pieces in the total dual class function Ux(·) = (cid:80)
x∈x ux(·), where the sum is over N auction samples
denoted by x. This corresponds to the number of distinct regions in the (p1, p2) space for which

14

the buyer makes some ﬁxed choice of the number of items purchased in each sample of the auction.
We will also show that R = O(N 2K min{N, K}), therefore our algorithm is no less eﬃcient in the
worst case and potentially much faster when the number of pieces R is small.

Setup: The seller has K identical units of an item. Suppose the buyers have valuation functions
vi : {1, . . . , K} → R≥0 where i ∈ {1, . . . , N } denotes the sample (diﬀerent samples could correspond
to the same buyer or diﬀerent buyers), and the value is assumed to be zero if no items are bought.
A buyer will purchase q quantities of the item in sample i that maximizes their utility ui(q) =
vi(q) − (p1 + p2q), buying zero units if the utility is negative for each q > 0. The revenue, which we
want to maximize as the seller, is zero if no item is bought, and p1 + p2q if q > 0 items are bought.
The algorithmic parameter we want to select is the price ρ = (cid:104)p1, p2(cid:105), and the problem instances are
speciﬁed by the valuations vi. [8] also consider a generalization of the above scheme: instead of just
a single two-part tariﬀ (TPT), suppose the seller provides a menu of TPTs (p1
2 ) of
1, pj
length L. The buyer selects a tariﬀ (pj
2) from the menu as well as the item quantity q to maximize
1 + pj
their utility ui(q) = vi(q) − (pj
2q). This problem has 2L parameters, ρ = (p1
2 ),
and the single tariﬀ setting corresponds to L = 1.

2), . . . , (pL

2, . . . , pL

1 , pL

1 , pL

1, p1

1, p1

The dual class functions are known to be piecewise linear with linear boundaries ([10], restated as
Lemma H.1 in Appendix H).

We show how to adapt the algorithms from Section 3 for this problem (Algorithm 8 in Appendix
H). The key diﬀerences are in how the polytope for a single vertex in the region adjacency graph
(Deﬁnition 3) is computed and what information needs to be stored for the vertices and edges of
the graph. We can compute the polytope for any vertex in the region adjacency graph in O(N Kr)
time, where r is the number of hyperplanes bounding the polytope, using Clarkson’s redundancy
removal algorithm. This is because for any price ρ, say the buyer buys quantities qi ∈ {0, . . . , K}
for diﬀerent samples to optimize their utility. Then for each sample we have K potential alternative
quantities given by hyperplanes ui(qi) ≥ ui(q(cid:48)), q(cid:48) (cid:54)= qi. The overall computation time is therefore
O(RN K) for computing all the pieces, using similar arguments as in Section 3. We can also
compute the revenue maximizing tariﬀ eﬃciently once we have the pieces (Theorem 5.1, proof in
Appendix H).

Theorem 5.1. There is an algorithm that, given valuation functions vi(·) for i ∈ [N ], learns the
revenue-maximizing tariﬀ for K units of the good from the N samples in O(RN K) time, where R
is the number of pieces in the total dual class function U(cid:104)v1,...,vN (cid:105)(·).

Proof Sketch. We ﬁrst use Algorithm 8 to compute the dual pieces, in O(RN K) time. As shown in
Theorem 3.3, the breadth-ﬁrst search over the region adjacency graph takes O(Er ·LP(2, N K)+Vr ·
N K) time, where Er, Vr are number of edges and vertices respectively in the graph. Using Seidel’s
algorithm for solving the LP and planarity of the graph, we get a running time bound of O(RN K).
To compute the revenue-maximizing tariﬀ we just need to compare the maximum revenue in each
piece. To compute the maximum revenue in any piece, we can compare the total revenues at the
vertices for the allocation according to the piece. This takes O(N v) time, where v is the number
of vertices in the piece. Summing over all pieces, and again using planarity, we get O(RN ) time
for the maximization.

15

We further show that our bound is never worse than the O(N 3K3) complexity of the algorithm
proposed in [8]. This is a consequence of the following theorem (proof in Appendix H).

Theorem 5.2. The number of pieces R in the total dual class function U(cid:104)v1,...,vN (cid:105)(·) is at most
O(N 2K min{N, K}).

Proof Sketch. The key idea is to upper bound the number of boundaries for the pieces corresponding
to individual samples, and then use a combinatorial argument to upper bound the total number of
pieces for all samples. We ﬁrst show that for any single sample, the number of bounding lines for
the pieces is O(K). The key observation is that of the O(K2) hyperplanes ui(q) ≥ ui(q(cid:48)) for q <
q(cid:48) ∈ {0, . . . , K}, many are redundant; across all samples we have O(N K) bounding lines. Finally,
the combinatorial fact that L lines divide the plane into at most 1+L(L+1)/2 regions implies there
are O(N 2K2) pieces. We further show R is O(N 3K) by applying the same combinatorial argument
to each of O(N K) horizontal ‘slabs’ corresponding to some ﬁxed non-zero allocation in all samples.
We have additional (non-horizontal) lines ui(0) = ui(qi) for each sample i, and therefore at most
O(N 2) pieces per slab.

We also give an output-quadratic algorithm for small constant length tariﬀ menu. The details of
the algorithm (Algorithm 9) and its analysis are straightforward extensions of the corresponding
results for L = 1 and are deferred to Appendix H.

Theorem 5.3. Algorithm 9, given valuation functions vi(·) for i ∈ [N ], computes all the R pieces
of the total dual class function U(cid:104)v1,...,vN (cid:105)(·) in time O(R2(2L)2L+1N K), where the menu length is
L, there are K units of the good and N is the number of auction samples.

6 Conclusion and discussion

We have developed approaches for tuning multiple parameters by computing the piecewise decom-
position of dual class functions with linear boundaries for three diﬀerent combinatorial problems.
Our approaches are eﬃcient when the number of parameters is a small constant and are also output-
sensitive. Our algorithms for the three problems share common ideas including execution graphs
and using Clarkson’s algorithm, an output-sensitive algorithm for removing redundant hyperplanes
from a linear system. Our techniques may be useful for tuning other combinatorial algorithms
beyond the ones considered here. At this point we have three diﬀerent examples of problems, but
it is an interesting open question to identify a general class of problems for which our techniques
apply. We only consider a small constant number of parameters — developing algorithms that scale
better with the number of parameters is another direction with potential for future research.

Acknowledgements

We thank Dan DeBlasio for useful discussions and Mikhail Khodak for helpful feedback. This
material is based on work supported by the National Science Foundation under grants CCF-1910321,

16

IIS-1901403, and SES-1919453; the Defense Advanced Research Projects Agency under cooperative
agreement HR00112020003; a Simons Investigator Award; an AWS Machine Learning Research
Award; an Amazon Research Award; a Bloomberg Research Grant; a Microsoft Research Faculty
Fellowship.

References

[1] Maria-Florina Balcan. “Data-Driven Algorithm Design.” In: Beyond Worst Case Analysis of

Algorithms. Ed. by Tim Roughgarden. Cambridge University Press, 2020.

[2] Maria-Florina Balcan, Avrim Blum, Dravyansh Sharma, and Hongyang Zhang. “On the power
of abstention and data-driven decision making for adversarial robustness”. In: arXiv preprint
arXiv:2010.06154 (2020).

[3] Maria-Florina Balcan, Dan DeBlasio, Travis Dick, Carl Kingsford, Tuomas Sandholm, and
Ellen Vitercik. “How much data is suﬃcient to learn high-performing algorithms? Generaliza-
tion guarantees for data-driven algorithm design”. In: Proceedings of the 53rd Annual ACM
SIGACT Symposium on Theory of Computing. 2021, pp. 919–932.

[4] Maria-Florina Balcan, Travis Dick, and Manuel Lang. “Learning to Link”. In: International

Conference on Learning Representations. 2020.

[5] Maria-Florina Balcan, Travis Dick, and Dravyansh Sharma. “Learning piecewise Lipschitz
functions in changing environments”. In: International Conference on Artiﬁcial Intelligence
and Statistics. PMLR. 2020, pp. 3567–3577.

[6] Maria-Florina Balcan, Travis Dick, and Ellen Vitercik. “Dispersion for data-driven algorithm
design, online learning, and private optimization”. In: 2018 IEEE 59th Annual Symposium
on Foundations of Computer Science (FOCS). IEEE. 2018, pp. 603–614.

[7] Maria-Florina Balcan, Vaishnavh Nagarajan, Ellen Vitercik, and Colin White. “Learning-
theoretic foundations of algorithm conﬁguration for combinatorial partitioning problems”.
In: Conference on Learning Theory. PMLR. 2017, pp. 213–274.

[8] Maria-Florina Balcan, Siddharth Prasad, and Tuomas Sandholm. “Eﬃcient Algorithms for

Learning Revenue-Maximizing Two-Part Tariﬀs.” In: IJCAI. 2020, pp. 332–338.

[9] Maria-Florina Balcan, Siddharth Prasad, Tuomas Sandholm, and Ellen Vitercik. “Sample
Complexity of Tree Search Conﬁguration: Cutting Planes and Beyond”. In: Advances in
Neural Information Processing Systems (2021).

[10] Maria-Florina Balcan, Tuomas Sandholm, and Ellen Vitercik. “A general theory of sample
complexity for multi-item proﬁt maximization”. In: Proceedings of the 2018 ACM Conference
on Economics and Computation. 2018, pp. 173–174.

[11] Maria-Florina Balcan and Dravyansh Sharma. “Data driven semi-supervised learning”. In:

Advances in Neural Information Processing Systems 34 (2021).

17

[12] Kenneth L Clarkson. “More output-sensitive geometric algorithms”. In: Proceedings 35th

Annual Symposium on Foundations of Computer Science. IEEE. 1994, pp. 695–702.

[13] Peter Clote and Rolf Backofen. Computational Molecular Biology: An Introduction. John

Wiley Chichester; New York, 2000. Chap. Sequence alignment.

[14] Jason V Davis, Brian Kulis, Prateek Jain, Suvrit Sra, and Inderjit S Dhillon. “Information-
theoretic metric learning”. In: Proceedings of the 24th international conference on Machine
learning. 2007, pp. 209–216.

[15] Dan DeBlasio and John Kececioglu. Parameter advising for multiple sequence alignment.

Vol. 26. Springer, 2018.

[16] Rishi Gupta and Tim Roughgarden. “A PAC approach to application-speciﬁc algorithm se-
lection”. In: Proceedings of the 7th Innovations in Theoretical Computer Science Conference.
2016.

[17] Dan Gusﬁeld, Krishnan Balasubramanian, and Dalit Naor. “Parametric optimization of se-

quence alignment”. In: Algorithmica 12.4 (1994), pp. 312–326.

[18] Dan Gusﬁeld and Paul Stelling. “Parametric and inverse-parametric sequence alignment with

XPARAL”. In: Methods in Enzymology 266 (1996), pp. 481–494.

[19] John Kececioglu and Eagu Kim. “Simple and fast inverse alignment”. In: Annual International

Conference on Research in Computational Molecular Biology. Springer. 2006, pp. 441–455.

[20] John Kececioglu, Eagu Kim, and Travis Wheeler. “Aligning protein sequences with predicted
secondary structure”. In: Journal of Computational Biology 17.3 (2010), pp. 561–580.

[21] Jon Kleinberg and Eva Tardos. Algorithm design. Pearson Education India, 2006.

[22] Krishnan Kumaran, Dimitri J Papageorgiou, Martin Takac, Laurens Lueg, and Nicolas V
Sahinidis. “Active metric learning for supervised classiﬁcation”. In: Computers & Chemical
Engineering 144 (2021), p. 107132. issn: 0098-1354.

[23] Nimrod Megiddo. “Combinatorial optimization with rational objective functions”. In: Pro-
ceedings of the tenth annual ACM symposium on Theory of computing. 1978, pp. 1–12.

[24] Shiliang Sun and Qiaona Chen. “Hierarchical Distance Metric Learning for Large Margin

Nearest Neighbor Classiﬁcation.” In: IJPRAI 25 (Nov. 2011), pp. 1073–1087.

[25] Michael S Waterman. Mathematical methods for DNA sequences. Boca Raton, FL (USA);

CRC Press Inc., 1989. Chap. Sequence alignments.

[26] Kilian Q Weinberger and Lawrence K Saul. “Distance metric learning for large margin nearest

neighbor classiﬁcation.” In: Journal of Machine Learning Research 10.2 (2009).

18

A Additional related work

Data-driven algorithm design. The framework for data-driven design was introduced by [16] and
is surveyed in [1]. Data-driven algorithms have been proposed and analyzed for a wide variety
of combinatorial problems, including clustering [4], computational biology [3], mechanism design
[10], semi-supervised learning [11] and integer programming [9]. Also the approach have proven
useful in providing guarantees like diﬀerential privacy [6], adversarial robustness [2] and adaptivity
to changing environments [5]. Typically these are NP-hard problems with a variety of known
heuristics. Data-driven design provides techniques to select the heuristic from a collection of them
which is best suited for data coming from some domain.

Linkage-based clustering. [7] introduce several single parameter families to interpolate well-known
linkage procedures for linkage-based clustering, and study the sample complexity of learning the
parameter.
[4] consider families with multiple parameters, and also consider the interpolation of
diﬀerent distance metrics. However the proposed execution-tree based algorithm for computing
the constant-performance regions in the parameter space only applies to single-parameter families,
and therefore the results can be used to interpolate linkage procedures or distance metrics, but
not both. Our work removes this limitation and provides algorithms for general multi-parameter
families.

Other approaches to learning a distance metric to improve the quality of clustering algorithms have
been explored. For instance, [26] and [22] consider versions of global distance metric learning, a
technique to learn a linear transformation of the input space before applying a known distance
metric. [24] also consider a hierarchical distance metric learning, which allows the learned distance
function to depend on already-merged clusters. For such distance learning paradigms, the underly-
ing objective function is often continuous, admitting gradient descent techniques. However, in our
setting, the objective function is neither convex nor continuous in ρ; instead, it is piecewise constant
for convex pieces. As a result, instead of relying on numerical techniques for ﬁnding the optimum
ρ, our multidimensional approach enumerates all the pieces for which the objective function is con-
stant. Furthermore, this technique determines the exact optimum ρ rather than an approximation;
one consequence of doing so is that our analysis can be extended to apply generalization guarantees
from [4] when learning the optimal ρ over a family of instances.

Sequence alignment. Prior work has considered learning these weights by examining the cost of
alignment for the possible weight settings. [3] considers the problem for how many training examples
(say pairs of DNA sequences with known ancestral or evolutionary relationship) are needed to learn
the best weights for good generalization on unseen examples. More closely related to our work is
the line of work which proposes algorithms which compute the partition of the space of weights
which result in diﬀerent optimal alignments [17, 18]. In this work, we propose a new algorithm
which computes this partition more eﬃciently.

Two-part tariﬀs. Data-driven algorithms have been proposed and studied for auction design [6, 10].
Prior work has focused mainly on the generalization guarantees, although some eﬃcient algorithms
have been proposed [8]. Our algorithms have output-sensitive complexity and are more eﬃcient

19

Table 2: Hamming accuracy of clustering using single, complete and median linkage on diﬀerent
datasets.

Dataset

Single Linkage Complete Linkage Median Linkage

Rings
Disks
Outliers
BalancedOutliers

95.08 ± 2.58%
64.03 ± 3.90%
65.79 ± 0.00%
50.00 ± 0.00%

54.88 ± 0.79%
96.02 ± 0.81%
65.74 ± 0.25%
98.58 ± 0.35%

55.79 ± 0.08%
96.17 ± 0.76%
96.24 ± 2.13%
50.00 ± 0.00%

than [8] even for worst-case outputs.

B Synthetic examples comparing the quality of single, complete

and median linkage procedures

In the following we will exhibit synthetic clustering datasets to demonstrate that each of two-point
based linkage procedures, i.e. single, complete and median linkage, may be sub-optimal on some
clustering instances. This will motivate the need to consider and learn an interpolation of the three
linkage procedures on a given clustering dataset coming from an unknown data distribution. All
our clustering datasets will consist of points in R2 and we will use the Euclidean metric as the
distance function throughout.

The Rings and Disks datasets described below are synthetic datasets proposed by [4] where single
linkage signiﬁcantly outperforms complete linkage, and complete linkage outperforms single linkage
respectively. We compare the performance of median linkage with single and complete linkage on
these datasets (Table 2). Single linkage leads to signiﬁcantly better clusters than both complete
and median linkage for the Rings dataset. We further propose two new synthetic datasets where
median and complete linkage respectively are the only dominant heuristics.

Datasets. In the Rings dataset we have two rings have radiuses 0.4 and 0.8, respectively, and are
both centered at the origin. N = 50 points are drawn uniformly from each ring. The Disks dataset
has two circular disks each with radius 0.4 and centered at (1.5, 0.4) and (1.5, −0.4), respectively.
Again, N points are drawn uniformly from each disk. The Outliers dataset has two unit squares
centered at (0.5, 0.5) and (1.7, 0.5) with uniformly drawn N = 50 points each. Another N points
are drawn along the straight line joining (0.6, 3.0) and (1.6, 3.0). Finally two outliers are added,
at points (1.4, 2.0) and (3.5, 0.6). The BalancedOutliers dataset has two unit squares centered at
(1.1, 1.8) and (1.7, 0.5) with uniformly drawn N = 50 points each. Two outliers are added, at points
(0.0, 0.0) and (3.2, 0.5). The datasets are depicted in Figure 3.

Experiments. For each dataset, we consider M = 100 instances drawn according to the respective
distributions and report the average Hamming accuracy of the clusters formed by single, complete
and median linkage procedures in Table 2. While single linkage dominates on the Rings dataset,
both complete and median linkage perform well on the Disks instances. Outliers can cause single

20

and complete linkage procedures to fail as the merge function may be strongly inﬂuenced by single
points in the clusters (but further away than closest and median distances). Complete linkage
tends to be robust to BalancedOutliers, i.e. when we have two large clusters and the outliers
not too far relative to furthest points in the clusters. Thus we have datasets where each one
of the linkage procedures (single, complete and median) signiﬁcantly dominates the other two.
It is also empirically veriﬁed that the dominating procedure on our datasets also dominates any
linear interpolation of the other two procedures. This implies that single-parameter interpolations
between pairs of clusters are not suﬃcient (on at least one of the above datasets) and motivates
the need for two-parameter interpolations (even for two-point based functions).

(a) Rings

(b) Outliers

(c) Disks

(d) BalancedOutliers

Figure 3: Synthetic datasets for demonstrating variance in performance of linkage procedures.

C Clarkson’s Algorithm

For completeness, we include the details of the Clarkson’s algorithm [12] for the interested reader.

Problem Setup. Given a linear system of inequalities Ax ≤ b, an inequality Aix ≤ bi is said to be
redundant in the system if the set of solutions is unchanged when the inequality is removed from
the system. Given a rational linear system (A ∈ Qm×d, b ∈ Qm), ﬁnd an equivalent system with no
redundant inequalities.

Note that to test if a single inequality Aix ≤ bi is redundant, it is suﬃcient to solve the following
LP in d variables and m constraints.

maximize Aix
subject to Ajx ≤ bj,

∀j ∈ [m] \ {i}

(2)

Aix ≤ bi + 1

Using this directly to solve the redunancy removal problem gives an algorithm with running time
m · LP(d, m). This can be improved using Clarkson’s algorithm if the number of non-redundant
constraints s is much less than the total number of constraints m (Theorem 3.2).

We assume that an interior point z ∈ Qd satisfying Ax < b is given. At a high level, one maintains
the set of non-redundant constraints I discovered so far i.e. Aix ≤ bi is not redundant for each
i ∈ I. When testing a new index k, the algorithm solves an LP of the form 2 and either detects that

21

Akx ≤ bk is redundant, or ﬁnds index j ∈ [m] \ I such that Ajx ≤ bj is non-redundant. The latter
involves the use of a procedure RayShoot(A, b, z, x) which ﬁnds the non-redundant hyperplane hit
by a ray originating from z in the direction x − z (x ∈ Qd) in the linear system A, b. The size of
the LP needed for this test is LP(d, |I| + 1) from which the complexity follows.

Algorithm 4: Clarkson
Input: A ∈ Qm×d, b ∈ Qm, z ∈ Qd.
I ← ∅, J ← [m]
while J (cid:54)= ∅ do
Select k ∈ J
Detect if Akx ≤ bk is redundant in AI∪{k}x ≤ bI∪{k} by solving LP (2).
x∗ ← optimal solution of the above LP
if redundant then
J ← J \ {k}

j ← RayShoot(A, b, z, x∗) /* Ajx = bj is a facet-inducing hyperplane hit by ray from z along x∗ − z */
J ← J \ {j}
I ← I ∪ {j}

return polys

To implement the RayShoot procedure, we can simply ﬁnd the intersections of the ray x∗ − z with
the hyperplanes Ajx ≤ bj and output the one closest to z.

Algorithm 5: RayShoot
Input: A ∈ Qm×d, b ∈ Qm, z ∈ Qd, x ∈ Qd.
if Ai · (x − z) = 0 for some i then

z ← z + ((cid:15), (cid:15)2, . . . , (cid:15)d) for suﬃciently small (cid:15)

ti ← bi−Ai·z
Ai·(x−z)
return arg mini{ti | ti > 0}

/* intersection of z + t(x − z) with Aix = bi */

We restate the running time guarantee of Algorithm 4 from Section 3.

Theorem 3.2 (Clarkson’s algorithm [12]). Given a list L of k half-space constraints in d dimen-
sions, Clarkson’s algorithm outputs a set I ⊆ [k] yielding an equivalent solution set to L without
redundant constraints. Furthermore, Clarkson’s algorithm runs in time O(k · LP(d, s + 1)), where
s = |I| and LP(v, c) is the time required to solve an LP with v variables and c constraints.

D Additional details and proofs from Section 3

Deﬁnition 4 (2-point-based merge function [4]). A merge function D is 2-point-based if for any
pair of clusters A, B ⊆ X and any metric d, there exists a set of points (a, b) ∈ A × B such
that D(A, B; d) = d(a, b). Furthermore, the selection of a and b only depend on the relative
ordering of the distances between points in A and B. More formally, for any metrics d and d(cid:48)

22

such that d(a, b) ≤ d(a(cid:48), b(cid:48)) if and only if d(cid:48)(a, b) ≤ d(cid:48)(a(cid:48), b(cid:48)), then D(A, B; d) = d(a, b) implies
D(A, B; d(cid:48)) = d(cid:48)(a, b).

For instance, single, median and complete linkage are 2-point-based, since the merge function
D(A, B; d) only depends on the distance d(a, b) for some a ∈ A, b ∈ B. We have the following
observation about our parameterized algorithm families D∆
ρ (A, B; δ) when ∆ consists of 2-point-
based merge functions which essentially establishes piecewise structure with linear boundaries (in
the sense of Deﬁnition 1).

Lemma D.1. Suppose S ∈ Π is a clustering instance, ∆ is a set of 2-point-based merge functions
with |∆| = l, and δ is a set of distance metrics with |δ| = m. Consider the family of clustering algo-
rithms with the parameterized merge function D∆
ρ (A, B; δ). The corresponding dual class function
uS(·) is piecewise constant with O(|S|4lm) linear boundaries.

Proof. Let (aij, bij, a(cid:48)
ga : P → R denote the function

ij, b(cid:48)

ij)1≤i≤l,1≤j≤m ⊆ S be sequences of lm points each; for each such a, let

ga(ρ) =

(cid:88)

i∈[l],dj ∈δ

αi,j(ρ)(dj(aij, bij) − dj(a(cid:48)

ij, b(cid:48)

ij))

ij, b(cid:48)

and let G = {ga | (aij, bij, a(cid:48)
ij)1≤i≤l,1≤j≤m ⊆ S} be the collection of all such linear functions;
notice that |G| = O(|S|4lm). Fix ρ, ρ(cid:48) ∈ P with g(ρ) and g(ρ(cid:48)) having the same sign patterns
for all such g. For each A, B, A(cid:48), B(cid:48) ⊆ S, Di ∈ ∆, and dj ∈ δ, we have Di(A, B; dj) = dj(a, b)
and Di(A(cid:48), B(cid:48); dj) = dj(a(cid:48), b(cid:48)) for some a, b, a(cid:48), b(cid:48) ∈ S (since Di is 2-point-based). Thus we can
write Dρ(A, B; δ) = (cid:80)
i∈[m],dj ∈δ αi,j(ρ)dj(aij, bij) for some aij, bij ∈ S; similarly, Dρ(A(cid:48), B(cid:48); δ) =
(cid:80)
ij ∈ S. As a result, Dρ(A, B; δ) ≤ Dρ(A(cid:48), B(cid:48); δ) if and
ij, b(cid:48)
i∈[m],dj ∈δ αi,j(ρ)dj(a(cid:48)

ij) for some a(cid:48)

ij, b(cid:48)

only if

(cid:88)

i∈[l],dj ∈δ

αi,j(ρ) (cid:0)dj(aij, bij) − dj(a(cid:48)

ij, b(cid:48)

ij)(cid:1) ≤ 0

which is exactly when ga(ρ) ≤ 0 for some sequence a. Since ga(ρ) and ga(ρ(cid:48)) have the same sign
pattern, we have Dρ(A, B; δ) ≤ Dρ(A(cid:48), B(cid:48); δ) if and only if Dρ(cid:48)(A, B; δ) ≤ Dρ(cid:48)(A(cid:48), B(cid:48); δ). So ρ and
ρ(cid:48) induce the same sequence of merges, meaning the algorithm’s output is constant on each piece
induced by g, as desired.

From Lemma D.1, we obtain a bound on the number of hyperplanes needed to divide P into
output-constant pieces. Let H be a set of hyperplanes which splits P into output-constant pieces;
then, a naive approach to ﬁnding a dual-minimizing ρ ∈ P is to enumerate all pieces generated by
H, requiring O(|H|d) runtime. However, by constructing regions merge-by-merge and successively
reﬁning the parameter space, we can obtain a better runtime bound which is output-sensitive in
the total number of pieces.

Proof of Lemma 3.1. This is a simple corollary of Lemma D.1 for m = 1. In this case l = d + 1.

Lemma D.2. Consider the family of clustering algorithms with the parameterized merge function
ρ(A, B; δ). Let U be the set of functions {uρ : S (cid:55)→ u(S, ρ) | ρ ∈ Rd} that map a clustering instance
D1

23

S to R. The dual class U ∗ is (F, G, |S|4)-piecewise decomposable, where F = {fc : U → R | c ∈ R}
consists of constant functions fc : uρ (cid:55)→ c and G = {gw : U → {0, 1} | w ∈ Rd} consists of halfspace
indicator functions gw : uρ (cid:55)→ I{w · ρ < 0}.

The key observation for the proof comes from [4] where it is observed that two parameterized
distance metrics dρ1, dρ2 behave identically (yield the same cluster tree) on a given dataset S if the
relative distance for all pairs of two points (a, b), (a(cid:48), b(cid:48)) ∈ S2 × S2, dρ(a, b) − dρ(a(cid:48), b(cid:48)), has the same
sign for ρ1, ρ2. This corresponds to a partition of the parameter space with |S|4 hyperplanes, with
all distance metrics behaving identically in each piece of the partition. More formally, we have

Proof of Lemma D.2. Let S be any clustering instance. Fix points a, b, a(cid:48), b(cid:48) ∈ S. Deﬁne the
linear function ga,b,a(cid:48),b(cid:48)(ρ) = (cid:80)
i ρi(di(a, b) − di(a(cid:48), b(cid:48))). If dρ(·, ·) denotes the interpolated distance
metric, we have that dρ(a, b) ≤ dρ(a(cid:48), b(cid:48)) if and only if ga,b,a(cid:48),b(cid:48)(ρ) ≤ 0. Therefore we have a set
H = {ga,b,a(cid:48),b(cid:48)(ρ) ≤ 0 | a, b, a(cid:48), b(cid:48) ∈ S} of |S|4 hyperplanes such that in any piece of the sign-
pattern partition of the parameter space by the hyperplanes, the interpolated distance metric
behaves identically, i.e. for any ρ, ρ(cid:48) in the same piece dρ(a, b) ≤ dρ(a(cid:48), b(cid:48)) iﬀ dρ(cid:48)(a, b) ≤ dρ(cid:48)(a(cid:48), b(cid:48)).
The resulting clustering is therefore identical in these pieces. This means that for any connected
component R of Rd \ H, there exists a real value cR such that uρ(s1, s2) = cR for all ρ ∈ Rd. By
deﬁnition of the dual, u∗
s1,s2(uρ) = uρ(s1, s2) = cR. For each hyperplane h ∈ H, let g(h) ∈ G denote
the corresponding halfspace. Order these k = |S|4 functions arbitrarily as g1, . . . , gk. For a given
connected component R of Rd \ H, let bR ∈ {0, 1}k be the corresponding sign pattern. Deﬁne the
function f (bR) = fcR and for b not corresponding to any R, f (b) = f0. Thus, for each ρ ∈ Rd,

u∗
s1,s2(uρ) =

(cid:88)

b∈{0,1}k

I{gi(uρ) = bi∀i ∈ [k]}f (b)(uρ).

Corollary D.3. For any clustering instance S ∈ Π, the dual class function uS(·) for the family in
Lemma D.2 is piecewise constant with O (cid:0)|S|4d(cid:1) pieces.

Lemma D.4. Let S ∈ Π be a clustering instance, ∆ be a set of merge functions, and δ be a set
of distance metrics. Then, the corresponding dual class function uS(·) is piecewise constant with
O(16|S|) linear boundaries of pieces.

Proof. For each subset of points A, B, A(cid:48), B(cid:48) ⊆ S, let gA,B,A(cid:48),B(cid:48) : P → R denote the function

gA,B,A(cid:48),B(cid:48)(ρ) = Dρ(A, B; δ) − Dρ(A(cid:48), B(cid:48); δ)

and let G be the collection of all such functions for distinct subsets A, B, A(cid:48), B(cid:48). Observe that G
is a class of linear functions with |G| ≤ (cid:0)2|S|(cid:1)4
= 16|S|. Suppose that for ρ, ρ(cid:48) ∈ P, g(ρ) and g(ρ(cid:48))
have the same sign for all g ∈ G; then, the ordering over all cluster pairs A, B of Dρ(A, B; δ) is the
same as that of Dρ(cid:48)(A, B; δ). At each stage of the algorithm, the cluster pair A, B ⊆ S minimizing
Dρ(A, B; δ) is the same as that which minimizes Dρ(cid:48)(A, B; δ), so the sequences of merges produced

24

by ρ and ρ(cid:48) are the same. Thus the algorithm’s output is constant on the region induced by
gA,B,A(cid:48),B(cid:48), meaning uS(·) is piecewise constant on the regions induced by G, which have linear
boundaries.

D.1 Execution Tree

Deﬁnition 5 (Execution tree). Let S be a clustering instance with |S| = n, and ∅ (cid:54)= P ⊆ [0, 1]d.
The execution tree on S with respect to P is a depth-n rooted tree T , whose nodes are deﬁned
recursively as follows: r = ([], P) is the root, where [] denotes the empty sequence; then, for any
node v = ([u1, u2, . . . , ut], Q) ∈ T with t < n − 1, the children of v are deﬁned as

(cid:40)(cid:18)

(cid:19)

children(v) =

[u1, u2, . . . , ut, (A, B)], QA,B

:

A, B ⊆ S is the (t + 1)st merge by Aρ for
exactly the ρ ∈ QA,B, with ∅ (cid:54)= QA,B ⊆ Q

(cid:41)

For an execution tree T with v ∈ T and each i with 0 ≤ i ≤ t, we let Pi denote the set of
Q such that there exists a depth-i node v ∈ T and a sequence of merges M with v = (M, Q).
Intuitively, the execution tree represents all possible execution paths (i.e. sequences for the merges)
for the algorithm family when run on the instance S as we vary the algorithm parameter ρ ∈ P.
Furthermore, each Pi is a subdivision of the parameter space into pieces where each piece has the
ﬁrst i merges constant. The following lemmas establish the structure of Pi.

Lemma D.5. Let S be a clustering instance and T be its execution tree with respect to P. Then,
if a sequence of merges M = [u1, u2, . . . , ut] is attained by Aρ for some ρ ∈ P, then there exists
some v ∈ T at depth t with v = (M, Q) and with Q ⊆ P being the exact set of values of ρ for which
Aρ may attain M. Conversely, for every node v = (M, Q) ∈ T , M is a valid sequence of merges
attainable by Aρ for some ρ ∈ P.

Proof. We proceed by induction on t. For t = 0, the only possible sequence of merges is the empty
sequence, which is obtained for all ρ ∈ P. Furthermore, the only node in T at depth 0 is the root
([], P), and the set P is exactly where an empty sequence of merges occurs.

Now, suppose the claim holds for some t ≥ 0. We show both directions in the induction step.

For the forward direction, let Mt+1 = [u1, u2, . . . , ut, ut+1], and suppose Mt+1 is attained by Aρ
for some ρ ∈ P. This means that Mt = [u1, u2, . . . , ut] is attained by Aρ as well; by the induction
hypothesis, there exists some node vt = (Mt, Qt) ∈ T at depth t, where ρ ∈ Qt and Qt is exactly
the set of values for which A may attain Mt. Now, ut+1 is a possible next merge by Aρ for some
ρ ∈ Qt; by deﬁnition of the execution tree, this means vt has some child vt+1 = (Mt+1, Qt+1) in T
such that Qt+1 is the set of values where ut+1 is the next merge in Qt. Moreover, Qt+1 is exactly
the set of values ρ ∈ P for which Aρ can attain the merge sequence Mt+1. In other words for any
ρ(cid:48) ∈ P \ Qt+1, Aρ(cid:48) cannot attain the merge sequence Mt+1. Otherwise, either some ρ(cid:48) ∈ P \ Qt
attains Mt+1, meaning Aρ(cid:48) attains Mt (contradicting the induction hypothesis), or Aρ(cid:48) attains
Mt+1 for some ρ(cid:48) ∈ Qt+1 \ Qt, contradicting the deﬁnition of Qt+1.

25

For the backward direction, let vt+1 = (Mt+1, Qt+1) ∈ T at depth t + 1. Since vt+1 is not the
root, vt+1 must be the child of some node vt, which has depth t. By the induction hypothesis,
vt = (Mt, Qt), where Mt = [u1, u2, . . . , ut] is attained by Aρ for some ρ ∈ P. Thus by deﬁnition
of the execution tree, Mt+1 has the form [u1, u2, . . . , ut, (A, B)], for some merging of cluster pairs
(A, B) which is realizable for ρ ∈ Qt. Thus Mt+1 is a valid sequence of merges attainable by Aρ
for some ρ ∈ P.

Lemma D.6. Let S be a clustering instance and T be its execution tree with respect to P. Suppose
P is a convex polytope; then, for each v = (M, Q) ∈ T , Q is a convex polytope.

Proof. We proceed by induction on the tree depth t. For t = 0, the only node is ([], P), and P
is a convex polytope. Now, consider a node v ∈ T at depth t + 1; by deﬁnition of the execution
tree, v = (Mv, Qv) is the child of some node u ∈ T , where the depth of u is t.
Inductively,
we know that w = (Mw, Qw), for some convex polytope Qw. We also know Mw has the form
Mw = [u1, u2, . . . , ut], and thus Qv is deﬁned to be the set of points ρ ∈ Qw where the merge
sequence Mv = [u1, u2, . . . , ut, (A, B)] is attainable for some ﬁxed A, B ⊆ S. Notice that the
deﬁnition of being attainable by the algorithm Aρ is that Dρ(A, B; δ) is minimized over all choices
of next cluster pairs A(cid:48), B(cid:48) to merge. That is, Qv is the set of points

Qv = {ρ ∈ Qw | Dρ(A, B; δ) ≤ Dρ(A(cid:48), B(cid:48); δ) for all available cluster pairs A(cid:48), B(cid:48) after Mw}

Since Dρ(A, B; δ) is an aﬃne function of ρ, the constraint Dρ(A, B; δ) ≤ Dρ(A(cid:48), B(cid:48); δ) is a half-
space. In other words, Qv is the intersection of a convex polytope Qw with ﬁnitely many half space
constraints, meaning Qv is itself a convex polytope.

It follows from Lemma D.6 that Pi forms a convex subdivision of P, where each Pi+1 is a reﬁnement
of Pi; Figure 1 (in the appendix) shows an example execution tree corresponding to a partition
of a 2-dimensional parameter space. From Lemma D.5, the sequence of the ﬁrst i merges stays
constant on each region P ∈ Pi. Our algorithm computes a representation of the execution tree of
an instance S with respect to P; to do so, it suﬃces to provide a procedure to list the children of
a node in the execution tree. Then, a simple breadth-ﬁrst search from the root will enumerate all
the leaves in the execution tree.

Now, our goal is to subdivide P into regions in which the (j + 1)st merge is constant. Each region
corresponds to a cluster pair being merged at step j + 1. Since we know these regions are always
convex polytopes (Lemma D.6), we can provide an eﬃcient algorithm for enumerating these regions.

Our algorithm provides an output-sensitive guarantee by ignoring the cluster pairs which are never
merged. Supposing there are nt unmerged clusters, we start with some point x ∈ P and determine
which piece W it is in. Then, we search for more non-empty pieces contained in P by listing the
“neighbors” of W . The neighbors of W are pieces inside P that are adjacent to W ; to this end,
we will more formally deﬁne a graph GP associated with P where each vertex is a piece and two
vertices have an edge when the pieces are adjacent in space. Then we show that we can enumerate
neighbors of a vertex eﬃciently and establish that GP is connected. It follows that listing the pieces

26

is simply a matter of running a graph search algorithm from one vertex of GP , thus only incurring
a cost for each non-empty piece rather than enumerating through all n4

t pairs of pieces.

D.2 Results needed for our multidimensional algorithm

Lemma D.7. Fix an aﬃne function f : R → Rd via f (x) = xa + b, for a, b ∈ Rd and a (cid:54)= 0d. For
a subset S ⊆ R, if f (S) is convex and closed, then S is also convex and closed.

Proof. First note that f is injective, since a (cid:54)= 0d. To show convexity of S, take arbitrary x, y ∈ S
and λ ∈ [0, 1]; we show that λx + (1 − λ)y ∈ S. Consider f (λx + (1 − λ)y):

f (λx + (1 − λ)y) = (λx + (1 − λ)y)a + b

= λ(xa + b) + (1 − λ)(ya + b)

By deﬁnition, ya+b, xa+b ∈ f (S), so it follows that f (λx+(1−λ)y) ∈ f (S) by convexity of f (S). So
there exists some z ∈ S with f (z) = f (λx + (1 − λ)y), but since f is injective, λx + (1 − λ)y = z ∈ S.
Thus S is convex.

To show closedness of S, we show R \ S is open. Let N (x, r) denote the open ball of radius r around
x, in either one-dimensional or d-dimensional space. Let x ∈ R \ S; we know f (x) /∈ f (S) since f
is injective. Since Rd \ f (S) is open, there exists some r > 0 with N (f (x), r) ⊆ Rd \ f (S). Then,
take ε = r
(cid:107)a(cid:107)2

> 0; for every y ∈ N (x, ε), we have

(cid:107)f (x) − f (y)(cid:107)2 = (cid:107)xa + b − ya − b(cid:107)2 < |x − y|(cid:107)a(cid:107)2 ≤ r

and so f (y) ∈ N (f (x), r) ⊆ Rd \ f (S), meaning y /∈ S since f is injective. Thus N (x, ε) ⊆ R \ S,
meaning S is closed as desired.

This allows us to prove the following key lemma. We describe a proof sketch ﬁrst. For arbitrary
P , we show that there exists a path from (i, j) to (i(cid:48), j(cid:48)) in GP . We pick arbitrary
(i, j), (i(cid:48), j(cid:48)) ∈ V ∗
points w ∈ Qi,j, x ∈ Qi(cid:48),j(cid:48); we can do this because by deﬁnition, V ∗
P only has elements corresponding
to non-empty cluster pairs. Then, we draw a straight line segment in P from w to x. When we do
so, we may pass through other sets on the way; each time we pass into a new region, we traverse
an edge in GP , so the sequence of regions we pass through on this line determines a GP -path from
w to x.

Lemma D.8. The vertices V ∗
other connected components of GP are isolated vertices.

P of the region adjacency graph GP form a connected component; all

Proof. It suﬃces to show that for arbitrary vertices (i1, j1), (i2, j2) ∈ VP , there exists a path from
(i1, j1) to (i2, j2) in GP . For ease of notation, deﬁne Q1 = Qi1,j1 and Q2 = Qi2,j2.

Fix arbitrary points u ∈ Q1 and w ∈ Q2. If u = w then we’re done, since the edge from (i1, j1) to
(i2, j2) exists, so suppose u (cid:54)= w. Consider the line segment L deﬁned as

L = {λu + (1 − λ)w : λ ∈ [0, 1]}

27

Since Q1, Q2 ⊆ P , we have u, w ∈ P . Furthermore, by convexity of P , it follows that L ⊆ P .

Deﬁne the sets Ri,j as

Ri,j = Qi,j ∩ L

Since each Qi,j and L are convex and closed, so is each Ri,j. Furthermore, since (cid:83)
must have (cid:83)

i,j Ri,j = L. Finally, deﬁne the sets Si,j as

i,j Qi,j = P , we

Si,j = {t ∈ [0, 1] : tu + (1 − t)w ∈ Ri,j} ⊆ [0, 1]

Note that Si,j is convex and closed; the aﬃne map f : Si,j → Ri,j given by f (x) = xu + (1 − x)w =
x(u − w) + w has Ri,j as an image. Furthermore, u − w (cid:54)= 0d; by Lemma D.7, the preimage Si,j
must be convex and closed. Furthermore, (cid:83)

i,j Si,j = [0, 1].

The only convex, closed subsets of [0, 1] are closed intervals. We sort the intervals in increasing
order based on their lower endpoint, giving us intervals I1, I2, . . . , I(cid:96). We also assume all intervals
are non-empty (we throw out empty intervals). Let σ(p) denote the corresponding cluster pair
associated with interval Ip; that is, if the interval Ip is formed from the set Si,j, then σ(p) = (i, j).

Deﬁne ai, bi to be the lower and upper endpoints, respectively, of Ii. We want to show that for all
1 ≤ i ≤ (cid:96) − 1, the edge {σ(i), σ(i + 1)} ∈ EP ; this would show that σ(1) is connected to σ((cid:96)) in the
VP . But σ(1) = (i1, j1) and σ((cid:96)) = (i2, j2), so this suﬃces for our claim.

Now consider intervals Ii = [ai, bi] and Ii+1 = [ai+1, bi+1].
It must be the case that bi = ai+1;
otherwise, some smaller interval would ﬁt in the range [bi, ai+1], and it would be placed before Ii+1
in the interval ordering.

Since bi ∈ Ii ∩ Ii+1, by deﬁnition, ubi + (1 − bi)w ∈ Rσ(i) ∩ Rσ(i+1). In particular, ubi + (1 − bi)w ∈
Qσ(i) ∩ Qσ(i+1); by deﬁnition of EP , this means {σ(i), σ(i + 1)} ∈ EP , as desired.

Proof of Theorem 3.5. Let T be the execution tree with respect to S, and let Tt denote the vertices
of T at depth t. From Theorem 3.3, for each node v = (M, Q) ∈ T with depth t, we can compute
the children of v in time O(n2
t K), where Vv is the number of children of v, and




t · LP(d, Ev) + Vv · n2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(Q1, Q2) ∈ P 2



t+1 | Q1 ∩ Q2 (cid:54)= ∅ and

u1 = (M1, Q1), u2 = (M2, Q2)
for some children u1, u2 of v

(cid:12)

(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)

.

Ev =

Now, observe

(cid:88)

v∈Tt+1

Ev ≤ Ht+1

since Ht+1 counts all adjacent pieces Q1, Q2 in Pt+1; each pair is counted at most once by some
Ev. Similarly, we have (cid:80)
Vv ≤ Rt+1, since Rt+1 counts the total size of Pt+1. Note that
nt+1 = (n − t), since t merges have been executed by time t + 1, so ni = n − i + 1. Using Seidel’s

v∈Tt+1

28

algorithm for solving linear programs, we can set LP(d, Ev) = d! · Ev, so that the total cost of
computing Pi is



O



n
(cid:88)

(cid:88)

i=1

v∈Ti

as desired.

d! · Ev(n − i + 1)2 + VvK(n − i + 1)2

 = O



(d! · Hi + RiK) (n − i + 1)2

(cid:33)

(cid:32) n
(cid:88)

i=1

E Example dynamic programs for sequence alignment

We exhibit how two well-known sequence alignment formulations can be solved using dynamic
programs which ﬁt our model in Section 4. In E.1 we show a DP with two free parameters (d = 2),
and in E.2 we show another DP which has three free parameters (d = 3).

E.1 Mismatches and spaces

Suppose we only have two features, mismatches and spaces. The alignment that minimizes the cost
c may be obtained using a dynamic program in O(mn) time. The dynamic program is given by
the following recurrence relation for the cost function which holds for any i, j > 0, and for any
ρ = (ρ1, ρ2),

C(s1[: i], s2[: j], ρ) =






C(s1[: i − 1], s2[: j − 1], ρ)

min (cid:8)ρ1 + C(s1[: i − 1], s2[: j − 1], ρ),
ρ2 + C(s1[: i − 1], s2[: j], ρ),
ρ2 + C(s1[: i], s2[: j − 1], ρ)

(cid:9)

if s1[i] = s2[j],

if s1[i] (cid:54)= s2[j].

The base cases are C(φ, φ, ρ) = 0, C(φ, s2[: j], ρ) = jρ2, = C(s1[: i], φ, ρ) = iρ2 for i, j ∈ [m] × [n].
Here φ denotes the empty sequence. One can write down a similar recurrence for computing the
optimal alignment τ (s1, s2, ρ).

We can solve the non base-case subproblems (s1[: i], s2[: j]) in any non-decreasing order of i + j.
Note that the number of cases V = 2, and the maximum number of subproblems needed to compute
a single DP update L = 3 (L1 = 1, L2 = 3). For a non base-case problem (i.e. i, j > 0) the cases are
given by q(s1[: i], s2[: j]) = 1 if s1[i] = s2[j], and q(s1[: i], s2[: j]) = 2 otherwise. The DP update in
each case is a minimum of terms of the form cv,l(ρ, (s1[: i], s2[: j])) = ρ · wv,l + σv,l(ρ, (s1[: i], s2[: j])).
For example if q(s1[: i], s2[: j]) = 2, we have w2,1 = (cid:104)1, 0(cid:105) and σ2,1(ρ, (s1[: i], s2[: j])) equals
C(s1[: i − 1], s2[: j − 1], ρ), i.e. the solution of previously solved subproblem (s1[: i − 1], s2[: j − 1]),
the index of this subproblem depends on l, v and index of (s1[: i], s2[: j]) but not on ρ itself.

29

E.2 Mismatches, spaces and gaps

Suppose we have three features, mismatches, spaces and gaps. Typically gaps (consecutive spaces)
are penalized in addition to spaces in this model, i.e. the cost of a sequence of three consecutive
gaps in an alignment (. . . a − − − b . . . , . . . a(cid:48) p q r b(cid:48) . . . ) would be 3ρ2 + ρ3 where ρ2, ρ3 are costs for
spaces and gaps respectively [20]. The alignment that minimizes the cost c may again be obtained
using a dynamic program in O(mn) time. We will need a slight extension of our DP model from
Section 4 to capture this. We have three subproblems corresponding to any problem in Πs1,s2 (as
opposed to exactly one subproblem, which was suﬃcient for the example in E.1). We have a set
of subproblems π(s1, s2) with |π(s1, s2)| ≤ 3|Πs1,s2| for which our model is applicable. For each
(s1[: i], s2[: j]) we can compute the three costs (for any ﬁxed ρ)

• Cs(s1[: i], s2[: j], ρ) is the cost of optimal alignment that ends with substitution of s1[i] with

s2[j].

• Ci(s1[: i], s2[: j], ρ) is the cost of optimal alignment that ends with insertion of s2[j].

• Cd(s1[: i], s2[: j], ρ) is the cost of optimal alignment that ends with deletion of s1[i].

The cost of the overall optimal alignment is simply C(s1[: i], s2[: j], ρ) = min{Cs(s1[: i], s2[:
j], ρ), Ci(s1[: i], s2[: j], ρ), Cd(s1[: i], s2[: j], ρ)}.

The dynamic program is given by the following recurrence relation for the cost function which holds
for any i, j > 0, and for any ρ = (ρ1, ρ2, ρ3),

Cs(s1[: i], s2[: j], ρ) = min





ρ1 + Cs(s1[: i − 1], s2[: j − 1], ρ),
ρ1 + Ci(s1[: i − 1], s2[: j − 1], ρ),
ρ1 + Cd(s1[: i − 1], s2[: j − 1], ρ)

Ci(s1[: i], s2[: j], ρ) = min

Cd(s1[: i], s2[: j], ρ) = min










ρ2 + ρ3 + Cs(s1[: i], s2[: j − 1], ρ),
ρ2 + Ci(s1[: i], s2[: j − 1], ρ),
ρ2 + ρ3 + Cd(s1[: i], s2[: j − 1], ρ)

ρ2 + ρ3 + Cs(s1[: i − 1], s2[: j], ρ),
ρ2 + ρ3 + Ci(s1[: i − 1], s2[: j], ρ),
ρ2 + Cd(s1[: i − 1], s2[: j], ρ)

By having three subproblems for each (s1[: i], s2[: j]) and ordering the non base-case problems again
in non-decreasing order of i + j, the DP updates again ﬁt our model (1).

30

F Subroutines of Algorithm 3

We start with some well-known terminology from computational geometry.

Deﬁnition 6. A (convex) subdivision S of P ⊆ Rd is a ﬁnite set of disjoint d-dimensional (convex)
sets (called cells) whose union is P . The overlay S of subdivisions S1, . . . , Sn is deﬁned as all
nonempty sets of the form (cid:84)

i∈[n] si with si ∈ Si.

The ComputeOverlay procedure takes a set of partitions, which are convex polytopic subdivi-
sions of Rd, and computes their overlay. We will represent a convex polytopic subdivision as a
list of cells, each represented as a list of bounding hyperplanes. Now to compute the overlay of
subdivisions P1, . . . , PL, with lists of cells C1, . . . , CL respectively, we deﬁne |C1| × · · · × |CL| sets
of hyperplanes Hj1,...,jL = {(cid:83)
is the jl-th cell of Pl and H(c) denotes the
hyperplanes bounding cell c. We compute the cells of the overlay by applying Clarkson’s algorithm
[12] to each Hj1,...,jL. We have the following guarantee about the running time of Algorithm 6.

)}, where c(l)
jl

l∈[L] H(c(l)
jl

Algorithm 6: ComputeOverlayDP
Input: Convex polytopic subdivisions P1, . . . , PL of Rd, represented as lists Cj of hyperplanes
for each cell in the subdivision
H(c(l)
jl
for each j1, . . . , jL ∈ |C1|, . . . , |CL| do

) ← hyperplanes bounding jl-th cell of Pl for l ∈ [L], jl ∈ Cl

Hj1,...,jL ← {(cid:83)
H (cid:48)

l∈[L] H(c(l)
jl
← Clarkson(Hj1,...,jL)

)}

j1,...,jL

C ← non-empty lists of hyperplanes in H (cid:48)
return Partition represented by C

j1,...,jL

for jl ∈ Cl

Lemma F.1. Let Ri,j denote the number of pieces in P [i][j], and ˜R = maxi≤m,j≤n P [i][j]. There
is an implementation of the ComputeOverlayDP routine in Algorithm 3 which computes the
overlay of L convex polytopic subdivisions in time O(L ˜RL+1 · LP(d, ˜RL + 1)), which is O(d!L ˜R2L+1)
for Seidel’s algorithm.

Proof. Consider Algorithm 6. We apply the Clarkson’s algorithm at most ˜RL times, once corre-
sponding to each L-tuple of cells from the L subdivisions. Each iteration corresponding to cell c in
the output overlay O (corresponding to C) has a set of at most L ˜R hyperplanes and yields at most
Rc non-redundant hyperplanes. By Theorem 3.2, each iteration takes time O(L ˜R · LP(d, Rc + 1)),
where LP(d, Rc + 1) is bounded by O(d!Rc) for Seidel’s algorithm. Note that (cid:80)
c Rc corresponds to
the total number of edges in the cell adjacency graph of O, which is bounded by ˜R2L. Further note
that Rc ≤ ˜RL for each c ∈ C and |C| ≤ ˜RL to get a runtime bound of O(L ˜RL+1 · LP(d, ˜RL + 1)).

We now consider an implementation for the ComputeSubdivision subroutine. The algorithm
computes the hyperplanes across which the subproblem used for computing the optimal alignment
changes in the recurrence relation (1) by adapting Algorithm 2.

31

2) ← optimal alignment of (s1, s2) for parameter ρ0, using subproblem

Algorithm 7: ComputeSubdivisionDP
Input: Convex Polytope P , problem instance (s1, s2)
v ← the DP case q((s1, s2)) for the problem instance
ρ0 ← an arbitrary point in P
(t0
1, t0
(s1[: iv,l0], s2[: jv,l0]) for some l0 ∈ [Lv]
mark ← ∅, polys ← new hashtable, poly queue ← new queue
poly queue.enqueue(l0)
while poly queue.non empty() do
l ← poly queue.dequeue()
Continue to next iteration if l ∈ mark
mark ← mark ∪ {l}
L ← P
for all subproblems (s1[: iv,l1], s2[: jv,l1]) for l1 ∈ [Lv], l1 (cid:54)= l do

Add the half-space inequality bT ρ ≤ c corresponding to cv,l(ρ, (s1, s2)) ≤ cv,l1(ρ, (s1, s2))
to L
/* Label the constraint (b, c) with l1 */

I ← Clarkson(L)
poly queue.enqueue(l(cid:48))
for each l(cid:48) such that the constraint labeled by it is in I
polys[l] ← {L[(cid:96)] : (cid:96) ∈ I}

return polys

Lemma F.2. Let Ri,j denote the number of pieces in P [i][j], and ˜R = maxi≤m,j≤n Ri,j. There
is an implementation of ComputeSubdivisionDP routine in Algorithm 3 with running time at
most O((L2d+2 + L2d ˜RL) · LP(d, L2 + ˜RL)) for each outer loop of Algorithm 3. If Seidel’s algorithm
is used to solve the LP, this is at most O(d!L2d+2 ˜R2L).

Proof. Consider Algorithm 7. For any piece p in the overlay, all the required subproblems have a
ﬁxed optimal alignment, and we can ﬁnd the subdivision of the piece by adapting Algorithm 2 (using
O(L2+ ˜RL) hyperplanes corresponding to subproblems and piece boundaries). The number of pieces
in the subdivision is at most L2d since we have at most L2 hyperplanes intersecting the piece, so we
need O(L2d+2 + L2d ˜RL) time to list all the pieces Cp. The time needed to run Clarkson’s algorithm
(L2 + ˜RL) · LP(d, Rc + 1)) = O((cid:80)
is upper bounded by O((cid:80)
(L2 + ˜RL) · LP(d, L2 + ˜RL)) =
O((L2d+2 + L2d ˜RL) · LP(d, L2 + ˜RL)). For Seidel’s algorithm this is at most O(d! ˜R2LL2d+4).

c∈Cp

c∈Cp

Lemma F.3. Let Ri,j denote the number of pieces in P [i][j], and ˜R = maxi≤m,j≤n Ri,j. There
is an implementation of ResolveDegeneraciesDP routine in Algorithm 3 with running time at
most O( ˜R2LL4d) for each outer loop of Algorithm 3.

Proof. The ResolveDegeneraciesDP is computed by a simple BFS over the cell adjacency graph
Gc = (Vc, Ec) (i.e. the graph with polytopic cells as nodes and edges between polytopes sharing
facets). We need to ﬁnd ﬁnding maximal components of the cell adjacency graph where each node
in the same component has the same optimal alignment. This is achieved by a simple BFS or
DFS in O(|Vc| + |Ec|) time. As noted in the proof of Lemma F.2, we have |Vc| ≤ L2d ˜RL since

32

the number of cells within each piece p is at most L2d and there are at most ˜RL pieces in the
overlay. Since |Ec| ≤ |Vc|2, we have an implementation of ResolveDegeneraciesDP in time
O((L2d ˜RL)2) = O( ˜R2LL4d).

Finally we can put all the above together to give a proof of Theorem 4.1.

Proof of Theorem 4.1. The proof follows by combining Lemma F.1, Lemma F.2 and Lemma F.3.
Note that in the execution DAG, we have |Ve| ≤ |Ee| = O(Tdp). Further, we invoke Com-
puteOverlayDP and ResolveDegeneraciesDP |Ve| times across all iterations and Compute-
SubdivisionDP across the |Ve| outer loops.

G Additional Proofs from Section 4

The following results closely follow and extend the corresponding results from [3]. Speciﬁcally,
we generalize to the case of two sequences of unequal length, and provide sharper bounds on the
number of distinct alignments and boundary functions in the piecewise decomposition (even in the
case of equal lengths). We ﬁrst have a bound on the total number of distinct alignments.

Lemma G.1. For a ﬁxed pair of sequences s1, s2 ∈ Σm × Σn, with m ≤ n, there are at most
m(m + n)m distinct alignments.

Proof. For any alignment (t1, t2), by deﬁnition, we have |t1| = |t2| and for all i ∈ [|t1|], if t1[i] = −,
then t2[i] (cid:54)= − and vice versa. This implies that t1 has exactly n − m more gaps than t2. To prove
the upper bound, we count the number of alignments (t1, t2) where t2 has exactly i gaps for i ∈ [m].
(cid:1)
There are (cid:0)n+i
i
choices for placing the gap in t1. Thus, there are at most (cid:0)n+i
possibilities since i ≤ m. Summing over all i, we have at most m(m + n)m alignments of s1, s2.

(cid:1) choices for placing the gap in t2. Given a ﬁxed t2 with i gaps, there are (cid:0)

n
n−m+i
i!(m−i)!(n−m+i)! ≤ (m+n)m

n
n−m+i

(cid:1) =

(n+i)!

(cid:1)(cid:0)

i

This implies that the dual class functions are piecewise-structured in the following sense.

Lemma G.2. Let U be the set of functions {uρ : (s1, s2) (cid:55)→ u(s1, s2, ρ) | ρ ∈ Rd} that map sequence
pairs s1, s2 ∈ Σm × Σn to R. The dual class U ∗ is (F, G, m2(m + n)2m)-piecewise decomposable,
where F = {fc : U → R | c ∈ R} consists of constant functions fc : uρ (cid:55)→ c and G = {gw : U →
{0, 1} | w ∈ Rd} consists of halfspace indicator functions gw : uρ (cid:55)→ I{w · ρ < 0}.

Proof. Fix a pair of sequences s1 and s2. Let τ be the set of optimal alignments as we range over
all parameter vectors ρ ∈ Rd. By Lemma G.1, we have |τ | ≤ m(m + n)m. For any alignment
(t1, t2) ∈ τ , the algorithm Aρ will return (t1, t2) if and only if

d
(cid:88)

i=1

ρili(s1, s2, t1, t2) >

d
(cid:88)

i=1

ρili(s1, s2, t(cid:48)

1, t(cid:48)
2)

(cid:1) ≤ m2(m+n)2m hyperplanes
for all (t(cid:48)
such that across all parameter vectors ρ in a single connected component of Rd \ H, the output of

2) ∈ τ \{(t1, t2)}. Therefore, there is a set H of at most (cid:0)|τ |

1, t(cid:48)

2

33

the algorithm Aρ on (s1, s2) is ﬁxed. This means that for any connected component R of Rd \ H,
there exists a real value cR such that uρ(s1, s2) = cR for all ρ ∈ Rd. By deﬁnition of the dual,
s1,s2(uρ) = uρ(s1, s2) = cR. For each hyperplane h ∈ H, let g(h) ∈ G denote the corresponding
u∗
(cid:1) functions arbitrarily as g1, . . . , gk. For a given connected component
halfspace. Order these k = (cid:0)|τ |
2
R of Rd \ H, let bR ∈ {0, 1}k be the corresponding sign pattern. Deﬁne the function f (bR) = fcR
and for b not corresponding to any R, f (b) = f0. Thus, for each ρ ∈ Rd,

u∗
s1,s2(uρ) =

(cid:88)

b∈{0,1}k

I{gi(uρ) = bi∀i ∈ [k]}f (b)(uρ).

Proof of Theorem 4.2. We note that for any alignment (t1, t2), the boundary functions for the piece
where (t1, t2) is an optimal alignment are straight lines through the origin of the form

ρ1l1(s1, s2, t1,t2) + ρ2l2(s1, s2, t1, t2) > ρ1l1(s1, s2, t(cid:48)

1, t(cid:48)

2) + ρ2l2(s1, s2, t(cid:48)

1, t(cid:48)
2)

for some alignment (t(cid:48)
2) diﬀerent from (t1, t2). The intersection of these halfplanes is either the
empty set or the region between two straight lines through the origin. The output subdivision
therefore only consists of the axes and straight lines through the origin in the positive orthant.

1, t(cid:48)

We will present an algorithm using the ray search technique of [23]. The algorithm computes
the optimal alignment (t1, t2), (t(cid:48)
2) at points ρ = (0, 1) and ρ = (1, 0). If the alignments are
identical, we conclude that (t1, t2) is the optimal alignment everywhere. Otherwise, we ﬁnd the
optimal alignment (t(cid:48)(cid:48)
2) for the intersection of line L joining ρ = (0, 1) and ρ = (1, 0), with the
line L(cid:48) given by

1, t(cid:48)(cid:48)

1, t(cid:48)

ρ1l1(s1, s2, t1,t2) + ρ2l2(s1, s2, t1, t2) > ρ1l1(s1, s2, t(cid:48)

1, t(cid:48)

2) + ρ2l2(s1, s2, t(cid:48)

1, t(cid:48)
2)

1, t(cid:48)(cid:48)

1, t(cid:48)(cid:48)

2) = (t1, t2) or (t(cid:48)(cid:48)

If (t(cid:48)(cid:48)
2), we have exactly 2 optimal alignments and the piece
boundaries are given by L(cid:48) and the axes. Otherwise we repeat the above process for alignment pairs
(t(cid:48)(cid:48)
2), (t1, t2). Notice we need to compute at most R + 1 dynamic programs to
compute all the pieces, giving the desired time bound.

2) and (t(cid:48)(cid:48)

2) = (t(cid:48)

2), (t(cid:48)

1, t(cid:48)(cid:48)

1, t(cid:48)(cid:48)

1, t(cid:48)

1, t(cid:48)

H Additional details and proofs from Section 5

H.1 Piecewise structure with linear boundaries

The following lemma restates the result from [10] in terms of Deﬁnition 1. Note that uρ in the
following denotes the revenue function (or seller’s utility) and should not be confused with the
buyer utility function ui.
Lemma H.1. Let U be the set of functions {uρ : v(·) (cid:55)→ pj∗
(cid:104)1, q(cid:105), ρj = (cid:104)pj

2 q∗ | q∗, j∗ = arg maxq,j v(q) − ρj ·
2(cid:105)} that map valuations v(·) to R. The dual class U ∗ is (F, G, (KL)2)-piecewise

1 + pj∗

1, pj

34

Algorithm 8: ComputePriceRegions
Input: Valuations vi(q) for i ∈ [N ] q(i)
(0, 0) */
mark ← ∅, polys ← new hashtable, poly queue ← new queue
poly queue.enqueue((q(i)
0
while poly queue.non empty() do

for i ∈ [N ]))

0 ← maxq{vi(q)} for i ∈ [N ]

/* Buyer preferences for price

(q1, . . . , qN ) ← poly queue.dequeue()
Continue to next iteration if (q1, . . . , qN ) ∈ mark
mark ← mark ∪ {(q1, . . . , qN )}
L ← {ρ1 ≥ 0, ρ2 ≥ 0}
for all q(i) (cid:54)= qi ∈ [K] for each i ∈ [N ] do

Add the half-space inequality bT ρ ≤ c corresponding to ui(q(i), ρ) to L

/* Label the constraint (b, c) with (i, q(i)) */

I ← Clarkson(L)
poly queue.enqueue((q1, . . . , qi−1, q, qi+1, . . . , qN ))
for each (i, q) such that the corresponding constraint is in I
polys[(i, q)] ← {L[(cid:96)] : (cid:96) ∈ I}

return polys

decomposable, where F = {fc : U → R | c ∈ R2L} consists of linear functions fc : uρ (cid:55)→ ρ · c and
G = {gw : U → {0, 1} | w ∈ R2L} consists of halfspace indicator functions gw : uρ (cid:55)→ I{w · ρ < 0}.

H.2 Adapted Algorithms and Runtime Bounds

We adapt our algorithm for linkage-based clustering to this setting to yield Algorithm 8. We
start with computing the buyer’s preferred quantities for each sample (or allocation) for the origin,
ρ = (cid:104)0, 0(cid:105). We compute the corresponding polytope by determining the non-redundant constraints,
and insert all the neighboring allocations into a queue. We then repeat this for previously unvis-
ited allocations in the queue until the queue is empty, performing an implicit BFS on the region
adjacency graph.

Proof of Theorem 5.1. We will ﬁrst use Algorithm 8 to compute the dual pieces. We show this
takes O(RN K) time. As shown in Theorem 3.3, the breadth-ﬁrst search over the region adjacency
graph takes O(Er · LP(2, N K) + Vr · N K) time, where Er, Vr are number of edges and vertices
respectively in the graph. Using Seidel’s algorithm for solving the LP gives LP(2, N K) = O(N K).
Since the graph is planar, we have Er = O(Vr). Finally the vertices of the region adjacency graph
correspond to the dual pieces, i.e. Vr = R, which gives a running time bound of O(RN K).

To compute the revenue-maximizing tariﬀ we just need to compare the maximum revenue in each
piece. To compute the maximum revenue in any piece, we can compare the total revenues at the
vertices for the allocation according to the piece. This takes O(N v) time, where v is the number
of vertices in the piece. Summing over all pieces, we get O(N V ) time to compute the piecewise

35

Algorithm 9: ComputePriceRegionsGeneral
Input: Valuations vi(q) for i ∈ [N ]
q(i)
0 ← maxq{vi(q)} for i ∈ [N ], j(i)
mark ← ∅, polys ← new hashtable, poly queue ← new queue
poly queue.enqueue((q(i)
0
while poly queue.non empty() do

for i ∈ [N ], j(i)
0

0 ← 1 for i ∈ [N ]

for i ∈ [N ]))

/* Buyer preferences for price ρ = 02L */

(q1, . . . , qN , j1, . . . , jN ) ← poly queue.dequeue()
Continue to next iteration if (q1, . . . , qN , j1, . . . , jN ) ∈ mark
mark ← mark ∪ {(q1, . . . , qN )}
L ← {ρj
2 ≥ 0 | j ∈ [L]}
for all q(i) (cid:54)= qi ∈ [K] for each i ∈ [N ], for each j(i) (cid:54)= ji ∈ [L] for each i ∈ [N ] do

1 ≥ 0, ρj

Add the half-space inequality bT ρ ≤ c corresponding to ui(q(i), (ρj(i)

1

, ρj(i)
2

)) to L

/* Label the constraint (b, c) with (i, q(i), j(i)) */

I ← Clarkson(L)
poly queue.enqueue((q1, . . . , qi−1, q, qi+1, . . . , qN , j1, . . . , ji−1, j, ji+1, . . . , jN ))
for each (i, q, j) such that the corresponding constraint is in I
polys[(i, q, j)] ← {L[(cid:96)] : (cid:96) ∈ I}

return polys

revenues, where V is the total number of vertices in the output subdivision. Since the output is a
planar subdivsion, V = O(R). Further, in O(R) time we can ﬁnd the maximum revenue over all
pieces. Thus computing the maximization from the pieces takes O(RN ) time, and the overall time
is O(RN K).

Proof of Theorem 5.2. The key idea is to upper bound the number of boundaries for the pieces
corresponding to individual samples, and then use a combinatorial argument to upper bound the
total number of pieces for all samples.

We ﬁrst show that for any single sample, the number of bounding lines for the pieces is O(K).
This seems counterintuitive since for any sample i, we have O(K2) hyperplanes ui(q) ≥ ui(q(cid:48)) for
If q > 0, ui(q) = ui(q(cid:48)) are axis-parallel lines with intercepts vi(q(cid:48))−vi(q)
q < q(cid:48) ∈ {0, . . . , K}.
.
Since for any pair q, q(cid:48) the buyer has a ﬁxed (but opposite) preference on either side of the axis-
parallel line, we have at most K distinct horizontal ‘slabs’ corresponding to buyer’s preference of
quantities, i.e. regions between lines p2 = a and p2 = b for some a, b > 0. Thus we have at most
K non-redundant lines. Together with another K lines ui(0) = ui(q(cid:48)) and the axes, we have O(K)
bounding lines in all as claimed. Across all samples we have O(N K) bounding lines. Finally, use
the simple combinatorial fact that L lines divide the plane in at most 1 + L(L + 1)/2 regions to
conclude that there are O(N 2K2) pieces.

q(cid:48)−q

We further show R is O(N 3K). Apply the same argument as above but this time apply the com-
binatorial argument to each horizontal ‘slab’. We have O(N K) axis parallel slabs, with additional
(non-horizontal) lines ui(0) = ui(qi) for each sample i, and therefore at most O(N 2) pieces per
slab.

36

Proof of Theorem 5.3. We will use Algorithm 9 to compute the dual pieces. As shown in Theorem
3.3, the breadth-ﬁrst search over the region adjacency graph takes O(Er ·LP(2L, LN K)+Vr ·LN K)
time, where Er, Vr are number of edges and vertices respectively in the graph. Using Seidel’s
algorithm for solving the LP gives LP(2L, LN K) = O((2L)!LN K) = O((2L)2L+1N K). We have
Er = O(V 2
r ). Finally the vertices of the region adjacency graph correspond to the dual pieces, i.e.
Vr = R, which gives a running time bound of O(R2(2L)2L+1N K).

37

