Graph Conditioned Sparse-Attention for Improved Source Code Understanding

Junyan Cheng, Iordanis Fostiropoulos, Barry Boehm

University of Southern California
Los Angeles, California 90007
junyanch@usc.edu, fostirop@usc.edu, boehm@usc.edu

1
2
0
2

c
e
D
3

]

G
L
.
s
c
[

2
v
3
6
6
0
0
.
2
1
1
2
:
v
i
X
r
a

Abstract

Transformer (Vaswani et al. 2017) architectures have been
successfully used in learning source code representations. The
fusion between a graph representation like Abstract Syntax
Tree (AST) and a source code sequence makes the use of cur-
rent approaches computationally intractable for large input
sequence lengths. Source code can have long-range dependen-
cies that require larger sequence lengths to model effectively.
Current approaches have a quadratic growth in computational
and memory costs with respect to the sequence length. Using
such models in practical scenarios is difﬁcult. In this work,
we propose the conditioning of a source code snippet with
its graph modality by using the graph adjacency matrix as an
attention mask for a sparse self-attention mechanism and the
use of a graph diffusion mechanism to model longer-range
token dependencies. Our model reaches state-of-the-art results
in BLEU, METEOR, and ROUGE-L metrics for the code
summarization task and near state-of-the-art accuracy in the
variable misuse task. The memory use and inference time
of our model have linear growth with respect to the input se-
quence length as compared to the quadratic growth of previous
works.

Traditional program static analysis techniques generate rich
graph representations for code. Such representations can con-
tain valuable information that is being successfully used for
automatic code analysis techniques. For example, Arzt et al.
(2014) applied inter-procedural Control Flow Graphs (CFG)
in the taint analysis for Android apps. Calcagno and Diste-
fano (2011) used call graphs in the analysis of memory safety
for C programs. How to effectively incorporate those repre-
sentations has been one of the core challenges for learning dis-
tributed representations of code (Allamanis, Brockschmidt,
and Khademi 2017). Transformers have succeeded in mul-
tiple natural language understanding tasks, and have also
been evaluated on the understanding of programming lan-
guages (Ahmad et al. 2020). A lot of efforts have been made
on incorporating code graph representations into a Trans-
former’s architecture. Ahmad et al. (2020) use Structure-
Based Traversal (SBT) to serialize Abstract Syntax Trees
(AST) as input to a Transformer. However, they observed an
accuracy decrease for the code summarization task. GREAT
(Hellendoorn et al. 2020) uses a relation-aware self-attention
mechanism to model edge information as the relationship
between tokens. Combined with a fused graph representation,

Hellendoorn et al. (2020) achieved state of art in the variable
misuse task. Z¨ugner et al. (2021) extract relative positional
information from AST structure to improve accuracy in code
summarization.

There are two shortcomings of Transformers architecture,
the ﬁrst is that it makes no assumptions on the inductive
biases of the data, thus it requires a large amount of training
data to outperform a model with good inductive biases as
shown by recent works (Dosovitskiy et al. 2020). A way
of introducing inductive bias is through a graph structure, a
graph structure brings relational inductive biases among input
tokens (Battaglia et al. 2018). Inductive bias may improve
the sample efﬁciency (Battaglia et al. 2018) and thus improve
performance with a smaller size dataset.

The second shortcoming is that the computational com-
plexity of the attention matrix is quadratic with respect to
the sequence length. The above methods have achieved good
accuracy by introducing inductive bias from a graph into a
Transformer architecture. However, they are computation-
ally inefﬁcient as the sequence length increases. They are
evaluated on snippets of limited length (e.g. 150, 400 tokens
for Java and Python dataset, and 512 for VarMisuse dataset).
In the context of a source code ﬁle or software system, the
sequence length is minuscule in capturing longer-range de-
pendencies that are naturally present. We found that, for pop-
ular repositories such as Tensorﬂow, the average number of
lines per ﬁle is 208.4, additional metrics on popular GitHub
projects could be found in Appendix B. The quadratic growth
with respect to the sequence length in a Transformer archi-
tecture (Child et al. 2019), in combination with the need for
longer sequence lengths for improved model accuracy, makes
the current works difﬁcult to be applied in a practical setting.
In recent years, there have been many methods proposed
to improve the efﬁciency of Transformer architecture. One
major category is the sparse attention mechanism which re-
duces the number of attending pairs of tokens for the attention
computation.

In this paper, we propose a novel method that introduces
inductive bias from a graphical representation of a sequence.
Our method results in both high accuracy and computational
efﬁciency. We propose the masking of a sequence with its
graphical representation using a sparse attention mechanism.
Because our masked attention is applied using the edges from

 
 
 
 
 
 
Figure 1: Examples of our model evaluated on the code summarization task on Java (red block) and Python (blue block) test set.
GREAT refers to the work by (Hellendoorn et al. 2020)

an AST that grow linearly as the sequence length increase, we
decrease the computation complexity of the attention matrix
from quadratic to linear.

We propose the use of a graph diffusion mechanism to
address the difﬁculty of modeling dependencies between
isolated nodes in an input sequence. Combined with an ap-
proximation algorithm, our approach is equivalent to a multi-
hop self-attention layer. We achieve higher accuracy while
keeping a low computational cost.

We evaluated our model on two tasks, two code sum-
marization datasets, and one variable misuse dataset. Two
qualitative examples for code summarization are presented
in Figure 1, more examples could be found in Appendix
H. The results show that our model outperforms baselines
in code summarization. On the variable misuse task, our
model performs slightly below the state-of-the-art base-
lines in terms of accuracy but largely outperforms all other
baselines. We analyze the computational efﬁciency of our
model and show a signiﬁcant advantage in both memory
use and CPU inference time compared to all previous work.
Our code, data, and models are publicly available in https:
//github.com/chengjunyan1/Graph-Sparse-Transformer.

1 Related works
One focus of previous deep learning methods for learning dis-
tributed representation of source code is fusing information
from graph representations like AST, CFG, PDG (Program
Dependency Graph). Alon et al. (2019) proposed the use of
random walks to learn an embedding for an AST, the AST
embeddings are then concatenated with token embeddings
and a context vector learned by an attention mechanism as
input to a decoder. Hu et al. (2018a) propose SBT to ﬂatten an
AST into a sequence which can then be used as input to a se-
quence model like LSTM. Huo, Li, and Zhou (2020) learned
a CFG representation by DeepWalk (Perozzi, Al-Rfou, and
Skiena 2014) with CNN and a sequence representation by
LSTM, the representations then concatenated as the context
vector.

There are methods of exploiting graph information in se-
quence models. Tai, Socher, and Manning (2015) fused tree

information into a sequence using a tree-structured LSTM.
Yao, Mao, and Luo (2019) constructed a heterogeneous graph
combining information from document and text which im-
proves accuracy in the text classiﬁcation task. Nguyen et al.
(2020) incorporated the constituency tree into the computa-
tion of the value part of the self-attention layer which effec-
tively conditions inductive bias on the attention computation.
Transformer-based architectures recently became popular
in multiple source code tasks. Ahmad et al. (2020) used a
Transformer in the code summarization task, using the source
code sequence as input to the model. They propose to use rel-
ative positional encoding and copy mechanism and improve
the model accuracy when compared to a vanilla Transformer.
Hellendoorn et al. (2020) leveraged edge attribute informa-
tion as the relation between tokens which are used as bias
terms in the attention calculation. Z¨ugner et al. (2021) incor-
porated AST structure as relative positional information and
explored multilingual learning in the programming language
discipline.

Despite the success, a drawback of a Transformer architec-
ture is the high computational cost of the self-attention mech-
anism. Several improvements in the Transformer architecture
have been proposed. There are two main directions towards
more efﬁcient self-attention computation. The ﬁrst direction
is the sparsiﬁcation of the attention computation which at-
tends fewer pairs of tokens and reduces the computational
cost (Child et al. 2019; Roy et al. 2020; Zaheer et al. 2020).
Another direction focuses on the low-rank approximation
of the attention matrix (Kitaev, Kaiser, and Levskaya 2020;
Choromanski et al. 2020). In this work, we concentrate on
the ﬁrst direction which is the sparsiﬁcation of attention com-
putation. Tay et al. (2020) reviewed previous works around
improving the efﬁciency of Transformers in detail.

2 Method
To solve the challenges of exploiting graph information and
efﬁcient computation, we combine a graph with the sparse
attention computation in the masking mechanism. In this sec-
tion, we ﬁrstly describe our graph sparse attention method
in detail, then introduce the attention diffusion mechanism

to address the problem of modeling dependencies between
isolated nodes in a sequence. Finally, discuss the implemen-
tation problem in the sparse attention computation.

2.1 Sparse Attention with Graph Information

Sparsiﬁcation of the attention computation is one major di-
rection to improve the efﬁciency of Transformers. Sparse-
attention achieved success in multiple disciplines, by im-
proving the computational efﬁciency, and even improve the
accuracy of the model (Child et al. 2019; Zaheer et al. 2020).
This method can be viewed as applying a mask on the atten-
tion matrix. The self-attention for one attention head can be
written as

is the embedding for the attribute of node Vi. Following
Hellendoorn et al. (2020), we use multiple edge attributes for
each pair of nodes as a bias term added on the query vector
W Qhi, deﬁned as W E (cid:80)
k ek
ij where k means the k-th edge
between node i and j, and ek
ij is the learned embedding for
the attribute of edge Ek
ij.

Following the methodology of Hellendoorn et al. (2020),
for the variable misuse task, we evaluate on a multigraph of
AST, CFG, and artiﬁcial graph structures introduced by the
authors.

The resulting attention formula for a Graph Conditioned

Sparse Mask is

i = W O (cid:88)
ht+1

mijAijW V ht
j

j

Aij = σ(W Qht

i, W Kht
j)

i = W O (cid:88)
ht+1

γijAijW V ht
j

j

(1)

Aij = σ(W Qht

i + W E (cid:88)

ij, W Kht
ek
j)

k

(2)

where ht is the set of hidden states in layer t, A is the
attention weight matrix, m is the mask matrix, σ is the
SoftMax function, W Q ∈ Rdmodel×dk , W K ∈ Rdmodel×dk ,
W V ∈ Rdmodel×dv , W O ∈ RHdk×dmodel are parameter ma-
trices, and H is the number of attention heads. When mij = 0
the attention weight between nodes i and j will not be com-
puted providing an efﬁciency advantage. The main challenge
for applying sparse attention successfully is choosing a mean-
ingful sparse mask. Correctly identifying which tokens we
should attend to can create a lower sparsity mask while main-
taining or improving accuracy. Different approaches have
been proposed on deciding a good rule to generate a mask,
like Child et al. (2019), Roy et al. (2020).

The intuition behind sparse attention is that the learned atten-
tion matrix is usually sparse (Kitaev, Kaiser, and Levskaya
2020). We can avoid wasting computational resources by
masking low-interaction token pairs in the attention computa-
tion instead of letting the model learn to assign a low attention
weight. For programming languages, the graph representa-
tion of source code already provides clues as to which token
pairs should be attended to during the attention computation.
Representations like AST, CFG can depict a “ground truth”
of the interactions between source code sequence tokens.
This is because AST and CFG are exact and non-ambiguous
graphs generated at a compiler-level. The distance between
two nodes in a graph conveys information about the level
of interaction between them during the program execution.
The nodes that have direct edges between them represent
strong interactions as opposed to nodes that are separated by
multiple hops.

Graph Conditioned Sparse Mask. Given a graph G =
{γ, V, E} where γ is the adjacency matrix, V is the node-set,
E is the edge set. By replacing the ﬁx-pattern mask ma-
trix m in eq.1 with the adjacency matrix γ, we embed the
graph structure information into the self-attention computa-
tion mechanism.

From the recursive attention relationship in eq.1 we use
the node attributes as the input hidden states h0, where h0
i

With more information-dense graph representations, the
model could achieve higher accuracy with lower sparsity
than a mask with a weaker statistical bias between tokens.
In our experiments, we evaluate such a claim by randomly
generated masks. Using a mask that considers the interactions
between all pairs of tokens can represent a complete graph.
A sparse graph representation of the sequence can introduce
isolation between nodes that don’t interact which can help
eliminate redundancy and noise. This improves the learned
representation and the performance on downstream tasks, we
do experiments for both a random mask and a complete mask
and discuss our results in Section 4.5.

2.2 Sparse Attention with Graph Diffusion

(a)

(b)

Figure 2: (a) Attention weights visualized for an AST with
a Graph Conditioned Sparse Mask, the magnitude of trans-
parency of an edge is proportional to the strength of attention
weight between two nodes. (b) The same AST after Self-
attention Diffusion Mechanism where K = 2.

We propose Self-attention Diffusion Mechanism to effec-
tively model the dependency of isolated tokens in a code
sequence. To explain the need for a diffusion mechanism we
need to introduce a graphical view of the attention mecha-
nism.

The Transformer could be regarded as executing a message-
passing function on a complete graph. The Sparse-Attention

mechanism is equivalent to a standard Transformer model for
mij = 1 in eq.1. An attention layer performs a 1-hop mes-
sage passing between each node and its neighboring nodes. In
the case of a Transformer, each token can get global informa-
tion from each other token due to the fully connected graph
mask, thus a 1-hop message passing is enough to model the
relationship between tokens.

In the case of Sparse Attention, the attention graph is not
fully connected, a 1-hop message passing mechanism has im-
plications when two nodes have a strong relationship but are
not directly connected. A code-based generated graph intro-
duces isolation among nodes. There are naturally long-range
dependencies in code that we can’t effectively model in this
way. Consider the AST in Figure 2(a) it would require 3-hops
for message passing to occur from the leaf node ‘a’ to the root
node ‘Assign’, which requires at least three attention layers.
Thus for larger graphs, it would require a signiﬁcantly larger
model to learn effective relationships which is impractical.

Graph diffusion (Klicpera, Weiß enberger, and G¨unnemann
2019), can help learn multi-hop information. We use the
attention matrix as the transition matrix, the diffused sparse
attention matrix D is deﬁned as:

D = (cid:80)∞

i=0 θiAi where (cid:80)∞

i=0 θi = 1, θi > 0

The diffusion mechanism introduces no additional param-
eters to the model, however, calculating the exact diffusion
matrix is intractable and can only be approximated. The dif-
fusion mechanism can introduce new edges to the original
graph and increase the computational cost.

We adopt the Approximate Computation for Attention
Diffusion by Wang et al. (2020) to solve this problem. We
deﬁne the computation of Sparse Diffusion Mechanism as:

Z (K) ≈ DW V ht

ht+1 = W OZ (K),

lim
K→∞
Z (k+1) = (1 − α)AZ (k) + αZ (0)
where, Z (0) = W V ht

(3)

We use α as a hyper-parameter that deﬁnes the strength
of the diffusion mechanism. We approximate Z (K) using a
recurrence relationship with hyper-parameter K that controls
how many hops of messages a node could receive through
diffusion.

We found that a small K = 2 is enough for getting a good
approximation of D which makes the computational cost
of the diffusion mechanism small. We tested the computa-
tional cost of the diffusion mechanism for different K in our
experiments, the results are presented in Section 4.4.

Implementation

2.3
Sparse computations are efﬁcient for both inference and train-
ing time. The parallel computing techniques, such as a GPU,
currently used for training deep learning models cannot fully
take advantage of sparse computations. It is an active research
topic on how to improve the efﬁciency of sparse matrix oper-
ations on a GPU (Gray, Radford, and Kingma 2017; Yao et al.

2019). There are existing optimization techniques like block-
wise (Zaheer et al. 2020), that improve the computational
efﬁciency of the sparse mask computation for a GPU. For
our method, it is not applicable because such optimization
techniques make assumptions on the structure of the attention
mask. In contrast, a Graph Conditioned Sparse Mask that can
display randomness created by the graph structures generated
from AST. The insight we make agrees with our experimental
results of similar training performance on a GPU between
our approach and previous works.

In this work, we use an open-source library like DGL (Wang
et al. 2019) that provides optimized sparse computations on
GPU and CPU. The computational advantages of GPU are
limited since they are optimized for dense matrix compu-
tations. Any additional performance advantages are limited
only to the optimization at the hardware or software level of
sparse matrix computations.

3 Experiments

We evaluated our model in two tasks, Code summarization,
and Variable misuse. Unless mentioned, all hyper-parameters
used in our experiments are presented in appendix A.

For code summarization task, the goal is to generate a
meaningful, human-readable summary of a given code snip-
pet. On the variable misuse task, the goal is to predict whether
a bug exists in a snippet and if it exists ﬁnd the location of
the two tokens in the code snippet that should be swapped.
Speciﬁcally, there are two locations predicted, the ﬁrst loca-
tion corresponds to the misused variable (or a special value
for “no-bug” classiﬁcation), and the second location is vari-
able that should be used to replace the misused one.

3.1 Code summarization

The experiments are conducted on a Java dataset (Hu et al.
2018b) and a Python dataset (Barone and Sennrich 2017).
We used JavaParser1 to extract the AST and javalang2 for
parsing Java source code, python ast3 to parse and get the
AST for Python.

For this task, we use a graph representation that connects
each token in the source code only with its closest parent in
AST. AST nodes refer to non-leaf nodes in AST. A token may
have multiple parents in an AST, but we only connect the
token to the AST node with the deepest depth which is also its
closest AST parent. All edges in the graph are bi-directional,
and all nodes are self-looped. The above procedure results
in the fusion of AST and a code sequence in one graph. The
graph structure generated this way is closer to a tree, thus
having sparse connections between nodes can improve the
computation of the attention mask. We only provide the token
representations as input for the decoder and discard the AST
node representations. An illustration of the model can be
found in Appendix D.

1https://javaparser.org/
2https://github.com/c2nes/javalang
3https://docs.python.org/3/library/ast.html

Table 1: Results for Code Summarization task.

Java

Python

DeepCom (Hu et al. 2018a)
Dual Model (Wei et al. 2019)
Transformer (Vaswani et al. 2017)
GREAT (Hellendoorn et al. 2020)
C2NL (Ahmad et al. 2020)
Ours
Ours (w/o D)
Ours (800 SL)

BLEU METEOR ROUGE-L BLEU METEOR ROUGE-L
39.75
42.39
43.41
44.05
44.58
45.21
44.36
45.55

52.67
53.61
52.71
53.01
54.76
55.00
54.49
55.40

37.35
39.45
44.31
43.75
46.73
46.56
46.43
46.80

23.06
25.77
25.91
26.42
26.43
26.55
26.06
27.13

20.78
21.80
31.08
31.19
32.52
33.31
33.02
33.41

9.98
11.14
18.57
18.65
19.77
19.82
19.65
19.71

Table 2: Results for Variable Misuse task.

Joint

Bug-free Localization

RNNs (Hellendoorn et al. 2020)
GGNNs (Hellendoorn et al. 2020)
Sandwiches (Hellendoorn et al. 2020)
Transformers (Vaswani et al. 2017)
Transformers-10L (Vaswani et al. 2017)
GREAT (Hellendoorn et al. 2020)
Ours
Ours (w/o D)
Ours-E
Ours-E (w/o D)

52.18% 82.57%
65.38% 90.28%
77.98% 88.76%
66.05% 91.70%
71.22% 90.16%
78.21% 88.98%
75.36% 79.75%
70.53% 73.24%
74.43% 93.15%
68.87% 92.93%

63.56%
79.64%
86.09%
73.39%
79.00%
86.14%
85.56%
84.50%
86.14%
84.39%

Repair
Params
63.22% 9.68M
75.76% 41.19M
85.16% 43.95M
76.79% 26.22M
80.46% 38.82M
85.85% 26.22M
82.79% 26.22M
78.93% 26.22M
81.56% 26.22M
77.37% 26.22M

3.2 Variable misuse

The experiments are conducted on the VarMisuse dataset
from Allamanis, Brockschmidt, and Khademi (2017). We use
positional encoding only for the bug-free classiﬁcation task.
The sequence of tokens is necessary for deciding whether an
expression is valid or not. A positional encoding provides
additional information that improves the model performance
in this task. We found a shallow version of our model with
positional encoding is adequate for learning to classify bug-
free snippets. We used an ensemble model composed of a
5-layered version of our model without positional encoding
and a 1-layered model with positional encoding in this task.

3.3 Details about Baselines

We choose GREAT and a vanilla Transformer as our main
baselines for both tasks, we also compared our model with
other baselines in each task respectively. The code for pre-
processing the raw source code ﬁles used by GREAT is not
publicly available. In the code summarization task, we pre-
processed both Java and Python dataset with the methodology
outlined in section 3.1 and used it to train a GREAT model.
For the variable misuse task, we used the public preprocessed
data provided by the authors of GREAT to train our model.
In the code summarization task, we use the code publicly
available by GREAT to train a model using identical data,
model, and training conﬁguration as our model. The results
of other baselines are directly reported from Ahmad et al.
(2020) for the code summarization task and Hellendoorn

et al. (2020) for the variable misuse task. Additional details
on the baselines for each task are further described in Section
4.

3.4 Performance Analysis

We analyzed the performance of our model in memory use
and CPU inference speed. Memory use corresponds to both
train and test time memory use and can be a limiting factor on
the training speed based on the effective batch size that can
be used as well as the throughput during inference time. We
compare our model with GREAT and Transformer, additional
comparison with the remainder baselines can be found in
Appendix C. We perform all our experiments in this section
on an artiﬁcially generated dataset.

We generate random sequences in varying lengths, from
100 to 10000 tokens and by a step size of 100. For both
the Graph Conditioned Masking and the Sparse Diffusion
Masking we use randomly generated masks with triple the
number of edges as the sequence length.

The mask we use would be identical to a real dataset that
uses an AST graph for the masking mechanism. The number
of edges in an AST is the number of nodes minus one. Other
artiﬁcial graph structures like the one used by Hellendoorn
et al. (2020) have the same linear relationship of edges to
nodes in real source code datasets. Thus the edge number
in both cases grows linearly with the sequence length. Addi-
tional analysis on the growth pattern of edges can be found
in Appendix E.

For consistency in the performance evaluation, all experi-
ments are performed only with the encoder of the model. The

representation extracted by an encoder can be used for any
downstream task. Moreover, different models use different
decoder architectures not allowing for a direct comparison of
the attention mechanism.

3.5 Performance test on real code repositories

Table 3: Ablation studies on 10 real Java repositories. “F”
represents directly using full sequence, remaining models
are using truncated sequence with max length of 2000. “D”
denotes our model with diffusion. Results shows the aver-
age CPU and GPU time (ms) and memory use (mb). “Total”
denotes total ﬁle number used for test test.

Avg.
Ours (F)
Ours-D (F)
Ours
Ours-D
GREAT
Trans.
Avg.
tokens
nodes
Total

CPU
1153.21
1460.63
795.65
872.95
1967.45
910.51
Full
2133.32
2809.96

Mem
821.81
1003.81
486.97
617.93
3148.06
1578.85
Trunc.
1358.45
1760.45
28083

GPU
60.30
74.55
55.97
67.45
102.74
69.58

We performed performance test on 10 most popular Java
repositories on GitHub including Flink, Eclipse Che, Elastic-
search, Hadoop, JDK1.8, libGDX, Presto, Spring Framework,
IntelliJ Community, and Groovy. We cleared the non-java ﬁle
and the non-class ﬁles like the ﬁles used as headers. Statistics
and results could be found in Table 3.

4 Results and Analysis

4.1 Code summarization results

Our method outperforms all baselines in Java and Python
and in all metrics, See Table 1. The “Ours” refers to our full
model with Graph Conditioned Sparse Masked attention and
a Diffusion Mechanism. The “Ours (w/o D)” refers to our
model without the Diffusion Mechanism. The “Ours (800
SL)” represents our full model trained with a larger maximum
sequence length sequence of 800 instead of 150 and 400 for
the Java and Python datasets. Our experiments demonstrate
that longer sequence lengths greatly improve the accuracy
of the model. All of the previous works only evaluate on a
limited sequence length and are unable to capture long-range
dependencies found in source code.

As additional baselines, we report “C2NL” (Ahmad et al.
2020) which is a vanilla Transformer equipped with the rela-
tive positional encoding and copy mechanism. “DeepCom”
(Hu et al. 2018a) uses a structure-based traversal to ﬂatten
ASTs into sequences as input for a seq2seq model that con-
sists of LSTMs. “Dual Model” (Wei et al. 2019) considers
Code Summarization (CS) and Code Generation (CG) as a
dual-task, it jointly trains a CS model and a CG model that
consists of a BiLSTM encoder and an LSTM decoder.

Table 4: Additional experiments on Java dataset. “L” repre-
sents the number of layers. “B.”, “M.”, “R.” refers to BELU,
METEOR and ROUGE-L respectively.

L K
-
3
2
3
3
3
4
3
2
6
2
6
2
6
2
6
3
6
4
6
-
10
2
10
2-hop attn.

α
-
0.25
0.25
0.25
0.10
0.15
0.25
0.35
0.25
0.25
-
0.25

B.
38.86
39.40
39.81
40.17
44.87
44.99
45.21
44.88
45.33
45.38
45.17
46.22
43.90

M.
20.54
20.86
21.25
21.41
26.37
26.37
26.55
26.31
55.12
55.22
26.76
27.97
25.24

R.
50.11
50.51
50.71
51.11
54.68
54.92
55.00
54.64
26.64
26.70
55.02
55.81
53.49

Param
22.1M
22.1M
22.1M
22.1M
44.1M
44.1M
44.1M
44.1M
44.1M
44.1M
73.6M
73.6M
50.4M

4.2 Variable misuse results

The results are presented in Table 2. ”Ours-E” refers to the en-
semble model as described in Section 3.2. ”Ours-E (w/o D)”
is the ensemble model without a diffusion mechanism. ”Joint”
refers to the joint bug localization and bug repair accuracy
which is the main metric used in this task. ”Localization” and
”Repair” refer to bug localization and bug repair accuracy
respectively. ”Bug-free” refers to the accuracy of classifying
the presence of a bug or not in a snippet. We refer readers
to Allamanis, Brockschmidt, and Khademi (2017) for more
details about this task and the metrics. ”RNNs” refers to a 2-
layer RNN, ”GGNN” is an 8-layer GGNN, ”Sandwiches” is
a hybrid model which structure is ”1R 4G 1R 4G 1R” where
”1R” represents 1 layer of RNN, ”4G” represents 4 layers of
GGNN, sandwiches are the stack of these models.

Since the data preprocessing for GREAT is not publicly
available, we cannot make a dataset with a larger maximum
sequence length as our methodology in the Code Summa-
rization task. The results show that with the same number
of parameters, the accuracy of our method is close to the
state-of-the-art method as GREAT (about 4.83% worse in
the main metric). We still largely outperformed the previous
state-of-the-art method Transformer by 12.69% with the same
number of parameters in the same task and metric. Given our
observations from Section 4.1, we anticipate that a model
trained on a longer sequence length could increase the gap
between our model performance and the baselines.

4.3 Additional experiments for diffusion

mechanism

We additional performed ablation study on K and α, and
the gaining from diffusion on higher layer models, and the
effect of K on lower layer models. Results are listed on
Table 4. The results shows that in shallow model (3 layers)
where the hops are low, increasing K signiﬁcantly improve
performance. In a deeper model, increasing K gives less
improvement with a cost of efﬁciency, thus a K = 2 gives
good balance. A too high or too low α are both not ideal.

In deep model (10 layers), diffusion could still have gain.
For model further deeper, there will be other problems like
losing rank (Dong, Cordonnier, and Loukas 2021) which may
inﬂuence the results and outside the scope of this paper. Such
problems also reﬂected in our experiment on a 2-hop attention
layer that stacking two attention layers in each model layer,
which massage passing ability equivalent to a 6 layers model
with K = 2, result shows that 2-hop attention layer decreased
the accuracy in contrary.

4.4 Performance analysis

(a)

(b)

Figure 3: Tests on (a) memory use (b) CPU inference time.

Results on our artiﬁcial dataset are presented in Figure
3. Figure 3(a) shows our experiments on inference memory
use. During training, memory use has the same growth trend
as inference. We also implemented the calculation of sparse
attention using a traditional dense matrix and observed iden-
tical growth rates as our baselines. Additional ﬁgures are
in Appendix C. As the sequence length increased, dense at-
tention computation methods reached a prohibitively high
memory use. Such memory requirements ﬁrstly make training
on larger sequence lengths intractable and secondly during
inference doesn’t allow for the deployment of these models
to most user devices. For example, GREAT consumed more
than 70GB of memory on the sequences with a length of

10000. With our sparse diffusion method we have a memory
cost of less than 4GB for the same sequence length. Figure
3(b) shows the experimental results on CPU inference time
and follows the same pattern as memory use. When the se-
quence length is 10000, GREAT used about 150s to encode
the input, while our model with a diffusion mechanism only
needed a fraction of the time.

The performance advantage relies on sparsity of our atten-
tion matrix which is dependent on number of edges in the
conditioning graph structure. For both AST and the GREAT
dataset, edge number grows linearly with the sequence length.
Additional performance results for real code snippets and
Java repositories listed in Appendix G.

4.5 Analysis on attention mask

We perform an ablation study on the effect of attention mask
on the code summarization task to verify our claim that the
graph-based mask adds more information than a random
mask or no mask at all. We used a “Complete” mask, which
is a constant mask matrix of 1 as used by a Transformer,
a “Random” mask with an edge density of 3% which is
identical to the density of the graph structure in our method.
The results show that our method performs better than both
a random mask with similar density and the complete graph
in both summarization datasets. Moreover, the loss diverged
in the Python dataset when a random mask was used. For
the complete mask, it adds redundant connections which
decrease the accuracy of the model due to the added noise.
The full results can be found in Appendix F.

5 Conclusion

In this work we propose a Graph Conditioned Sparse Mask
Attention mechanism to introduce inductive bias and a Graph
Diffusion Mechanism to address the isolation between nodes
introduced by the sparse attention. We are able to model long-
range dependencies of n-hop degree between graph nodes
with no additional parameters or computational performance
degradation. We outperform all baselines in terms of stan-
dardized metrics in the code summarization task and achieve
near state-of-the-art accuracy in the variable misuse task. Our
approach yields signiﬁcant improvements in memory use and
CPU inference time, reducing the growth of computational
cost from quadratic to linear with respect to the sequence
length. We conclude that using our approach we can train
and perform inference on longer sequences that can model
long-range dependencies that are naturally present in source
code. Additional advances in sparse computation could help
our method achieve better efﬁciency. Other future directions
include better graph representations that have richer informa-
tion than AST while matching the edge sparsity of AST.

References
Ahmad, W. U.; Chakraborty, S.; Ray, B.; and Chang, K.-
W. 2020. A Transformer-based Approach for Source Code

Summarization. In Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics (ACL).
Allamanis, M.; Brockschmidt, M.; and Khademi, M. 2017.
Learning to represent programs with graphs. arXiv preprint
arXiv:1711.00740.
Alon, U.; Zilberstein, M.; Levy, O.; and Yahav, E. 2019.
Code2vec: Learning Distributed Representations of Code.
Proc. ACM Program. Lang., 3(POPL).
Arzt, S.; Rasthofer, S.; Fritz, C.; Bodden, E.; Bartel, A.;
Klein, J.; Le Traon, Y.; Octeau, D.; and McDaniel, P. 2014.
Flowdroid: Precise context, ﬂow, ﬁeld, object-sensitive and
lifecycle-aware taint analysis for android apps. Acm Sigplan
Notices, 49(6): 259–269.
Barone, A.; and Sennrich, R. 2017. A parallel corpus of
Python functions and documentation strings for automated
code documentation and code generation. In The 8th Inter-
national Joint Conference on Natural Language Processing
(IJCNLP 2017), volume 2, 314–319.
Battaglia, P. W.; Hamrick, J. B.; Bapst, V.; Sanchez-Gonzalez,
A.; Zambaldi, V.; Malinowski, M.; Tacchetti, A.; Raposo, D.;
Santoro, A.; Faulkner, R.; et al. 2018. Relational inductive
biases, deep learning, and graph networks. arXiv preprint
arXiv:1806.01261.
Calcagno, C.; and Distefano, D. 2011. Infer: An automatic
program veriﬁer for memory safety of C programs. In NASA
Formal Methods Symposium, 459–465. Springer.
Child, R.; Gray, S.; Radford, A.; and Sutskever, I. 2019.
Generating Long Sequences with Sparse Transformers.
arXiv:1904.10509.
Choromanski, K.; Likhosherstov, V.; Dohan, D.; Song, X.;
Gane, A.; Sarlos, T.; Hawkins, P.; Davis, J.; Mohiuddin, A.;
Kaiser, L.; Belanger, D.; Colwell, L.; and Weller, A. 2020.
Rethinking Attention with Performers. arXiv:2009.14794.
Dong, Y.; Cordonnier, J.-B.; and Loukas, A. 2021. Attention
is Not All You Need: Pure Attention Loses Rank Doubly
Exponentially with Depth. arXiv:2103.03404.
Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,
D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;
Heigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16
words: Transformers for image recognition at scale. arXiv
preprint arXiv:2010.11929.
Gray, S.; Radford, A.; and Kingma, D. P. 2017. Gpu kernels
for block-sparse weights. arXiv preprint arXiv:1711.09224,
3.
Hellendoorn, V. J.; Sutton, C.; Singh, R.; Maniatis, P.; and
Bieber, D. 2020. Global Relational Models of Source Code.
In International Conference on Learning Representations.
Hu, X.; Li, G.; Xia, X.; Lo, D.; and Jin, Z. 2018a. Deep Code
Comment Generation. In 2018 IEEE/ACM 26th International
Conference on Program Comprehension (ICPC), 200–20010.
Hu, X.; Li, G.; Xia, X.; Lo, D.; Lu, S.; and Jin, Z. 2018b.
Summarizing Source Code with Transferred API Knowledge.
In Proceedings of the Twenty-Seventh International Joint
Conference on Artiﬁcial Intelligence, IJCAI-18, 2269–2275.
International Joint Conferences on Artiﬁcial Intelligence Or-
ganization.

Huo, X.; Li, M.; and Zhou, Z.-H. 2020. Control Flow Graph
Embedding Based on Multi-Instance Decomposition for Bug
Localization. In AAAI.
Kitaev, N.; Kaiser, L.; and Levskaya, A. 2020. Reformer:
The Efﬁcient Transformer. In International Conference on
Learning Representations.
Klicpera, J.; Weiß enberger, S.; and G¨unnemann, S. 2019. Dif-
fusion Improves Graph Learning. In Wallach, H.; Larochelle,
H.; Beygelzimer, A.; d'Alch´e-Buc, F.; Fox, E.; and Garnett,
R., eds., Advances in Neural Information Processing Systems,
volume 32, 13354–13366. Curran Associates, Inc.
Nguyen, X.-P.; Joty, S.; Hoi, S.; and Socher, R. 2020. Tree-
Structured Attention with Hierarchical Accumulation.
In
International Conference on Learning Representations.
Perozzi, B.; Al-Rfou, R.; and Skiena, S. 2014. DeepWalk:
Online Learning of Social Representations. In Proceedings of
the 20th ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’14, 701–710. New
York, NY, USA: ACM. ISBN 978-1-4503-2956-9.
Roy, A.; Saffar, M.; Vaswani, A.; and Grangier, D. 2020. Ef-
ﬁcient Content-Based Sparse Attention with Routing Trans-
formers. arXiv:2003.05997.
Tai, K. S.; Socher, R.; and Manning, C. D. 2015. Improved
Semantic Representations From Tree-Structured Long Short-
Term Memory Networks. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguistics and
the 7th International Joint Conference on Natural Language
Processing (Volume 1: Long Papers), 1556–1566. Beijing,
China: Association for Computational Linguistics.
Tay, Y.; Dehghani, M.; Bahri, D.; and Metzler, D. 2020. Efﬁ-
cient Transformers: A Survey. arXiv:2009.06732.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, u.; and Polosukhin, I. 2017. At-
tention is All You Need. In Proceedings of the 31st Interna-
tional Conference on Neural Information Processing Systems,
NIPS’17, 6000–6010. Red Hook, NY, USA: Curran Asso-
ciates Inc. ISBN 9781510860964.
Wang, G.; Ying, R.; Huang, J.; and Leskovec, J. 2020.
Direct Multi-hop Attention based Graph Neural Network.
arXiv:2009.14332.
Wang, M.; Zheng, D.; Ye, Z.; Gan, Q.; Li, M.; Song, X.;
Zhou, J.; Ma, C.; Yu, L.; Gai, Y.; Xiao, T.; He, T.; Karypis,
G.; Li, J.; and Zhang, Z. 2019. Deep Graph Library: A
Graph-Centric, Highly-Performant Package for Graph Neural
Networks. arXiv preprint arXiv:1909.01315.
Wei, B.; Li, G.; Xia, X.; Fu, Z.; and Jin, Z. 2019. Code
Generation as Dual Task of Code Summarization. NeurIPS,
6559–6569.
Yao, L.; Mao, C.; and Luo, Y. 2019. Graph convolutional
networks for text classiﬁcation. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, volume 33, 7370–7377.
Yao, Z.; Cao, S.; Xiao, W.; Zhang, C.; and Nie, L. 2019.
Balanced Sparsity for Efﬁcient DNN Inference on GPU. Pro-
ceedings of the AAAI Conference on Artiﬁcial Intelligence,
33: 5676–5683.

Zaheer, M.; Guruganesh, G.; Dubey, K. A.; Ainslie, J.; Al-
berti, C.; Ontanon, S.; Pham, P.; Ravula, A.; Wang, Q.; Yang,
L.; et al. 2020. Big bird: Transformers for longer sequences.
Advances in Neural Information Processing Systems, 33.
Z¨ugner, D.; Kirschstein, T.; Catasta, M.; Leskovec, J.; and
G¨unnemann, S. 2021. Language-Agnostic Representation
Learning of Source Code from Structure and Context. In
International Conference on Learning Representations.

A Summary of Hyper-parameters

B Investigation on GitHub projects

Base Model Conﬁg Value

Num of layers
Attention heads
dk
dv
dmodel
df f
K
α

6
8
64
64
512
2048
2
0.25

Project

Total lines Avg lines

Flutter
Go lang.
Kubernetes
d3
Deno
React
Tensorﬂow
VScode
Vue
Linux

911261
1696067
4236562
2672
309239
327844
2894245
1068910
142670
21163798

269.6
220.7
265.8
296.9
228.2
181.2
208.4
245.3
266.7
349.4

Table 6: The projects are selected from the most starred
projects in GitHub. Blank lines and comments are omit-
ted when counting the number of lines, only code ﬁles are
counted. All project have about 200 lines of code per ﬁle,
and one line of code could contain multiple tokens, and any
single ﬁle may contains more than 200 lines of code. Many
software projects could easily exceed the sequence length
limit of a Transformers architecture

Train (Code Sum.) Value

C Additional Performance test with dense

Dropout rate
Optimizer
Learning rate
Decay rate
Max epoch num
Early stop epochs
Training Batch set
Testing Beam size
Max src. vocab size
Max tgt. vocab size
Max code len. (J)
Max code len. (P)
Max sum. len. (J)
Max sum. len. (P)

0.2
Adam
0.0001
0.99
200
20
30
4
50000
30000
150
400
50
30

Train (Var. Misuse) Value

Dropout rate
Optimizer
Batch size
Learning rate
Max code len.:

0.1
Adam
7500
0.0001
512

implementation

(a)

(b)

Table 5: The hyperparameters are consistent with Ahmad et al.
(2020) for the code summarization task and Hellendoorn et al.
(2020) for the variable misuse task.

Figure 4: We perform an additional experiment and compare
the sparse implementation and dense implementation of our
method in the growth of (a) memory use and (b) CPU time
with respect to the sequence length.

D Model structure for Code summarization

F Experiments on Attention mask

Ours
(w/o D)

FC

RD

BLEU METEOR ROUGE-L
44.36
33.02
44.23
31.06
43.24

J
P
J
P
J
P Diverged Diverged

54.49
46.43
53.46
43.37
52.33
Diverged

26.06
19.65
26.66
18.34
25.14

Table 7: “RD” represents random mask with 3% sparsity,
“FC” is a complete graph mask, “J” and “P” refers to Java
and Python dataset respectively. Results show that our graph
based method outperform both random and complete graphs
in the two dataset and in all metrics.

G Performance test on largest snippet in

Java dataset

Model

Mem
(GB)

CPU
(ms)

Original

GPU
(ms)

Seq
Len.

Sparse (w/o D)
Sparse (K=2)
Sparse (K=6)
Other

103
2417
2417
121
3761
3253
4905
148
6695
OOM OOM OOM

5956
(8802)

Sparse (w/o D)
Sparse (K=2)
Sparse (K=6)
Dense (w/o D)
Dense (K=2)
Dense (K=6)
GREAT
Transformer

Truncated
847
1316
2314
6177
6890
7952
4532
1768

819
1106
1674
6015
6298
6873
7732
6028

73
96
134
120
194
256
535
119

2000
(3014)

Table 8: Performance test on a real code snippet. “Sparse”
and “Dense” represents sparse and dense implementation
of our method respectively, “Trans.” is vanilla Transformer.
Unlike other methods in the table, we do not use graph in-
formation from the sequence for the vanilla Transformer. In
the Sequence Length column, the number in the parenthesis
represents the total number of nodes (token number plus AST
node number) for the sample.

H Additional code summarization examples
H.1 Examples from test set

Figure 5: The seq2seq architecture we used for the code sum-
marization task. The AST node representations are discarded
in the decoding stage.

E Growth pattern of node and edge number

(a)

(b)

Figure 6: We analyzed the growth pattern of sequence length
with the respect to the edge number (a) The number of edges
grows linearly with the sequence length in an AST. (b) The
edge number also grows linearly with respect to the node
number in GREAT’s graph structure.

public ﬁnal boolean

isExceptionHandlerEquivalent (BasicBlock other ){
if ( exceptionHandlers != other . exceptionHandlers ) {

Enumeration<BasicBlock> e1=getExceptionHandlers();
Enumeration<BasicBlock>

e2=other.getExceptionHandlers();

okToWrite = false ;
throw e;

}

}

Original ﬁle: org.apache.hadoop.hdfs.tools. ofﬂineIm-

ageViewer.TextWriterImageVisitor

Summary: Write the image data to a ﬁle.

public class StringTupleDeserializerMap implements
MapFunction<byte[], Tuple1<String>> {

@Override
public Tuple1<String> map(byte[] value) throws

Exception {

return new Tuple1<>(new String(value, 5,

value . length − 5,
ConﬁgConstants .DEFAULT CHARSET));

}

}

Original

ﬁle:

org.apache.ﬂink.python.api.

func-

tions.util.StringTupleDeserializerMap

Summary: Parses the given string representation of a tu-

ple.

while (e1.hasMoreElements()) {

if
(! e2.hasMoreElements()) return false ;
if (e1.nextElement() != e2.nextElement() )

return false ;

}
if (e2.hasMoreElements()) return false ;

}
return true ;

}

Reference: compare the in scope exception handlers of

two blocks.

Ours: return true if the block is equivalent to the exception

handlers, or false otherwise.

GREAT: convert this block to another objects .

def

issues closed since ( period= timedelta (days=365),
project =‘ipython / ipython ’ , pulls =False) :
which = (‘ pulls ’ if pulls else ‘ issues ’ )
if

isinstance ( period ,
timedelta ) :
since = round hour (( datetime .utcnow() − period ) )

else :

since = period

url = (‘ https :// api . github . com/repos/%s/%s?
state =closed&sort=updated&since=%s&per page=%i’ %

(project, which, since . strftime (ISO8601),
PER PAGE))

allclosed = get paged request ( url ,

headers=make auth header())

ﬁltered = [ i for i

in allclosed if

( parse datetime ( i [‘ closed at ’ ]) > since) ]

if pulls :

ﬁltered = [ i for i

in ﬁltered if
( parse datetime ( i [‘ merged at’ ]) > since) ]
in ﬁltered if

ﬁltered = [ i for i

( i [‘ base’ ][‘ ref ’ ] == ‘master’ ) ]

else :

ﬁltered = [ i for i

in ﬁltered if (not

is pull request ( i ) ) ]

return ﬁltered

Reference: get all issues closed since a particular point in

time .

Ours: return a list of closed issues .
GREAT: pull requests iso unreferenced for single-line

issues .

H.2 Examples from real snippets
Here we show the summarizations our model generated for
two real snippets chosen from large Java projects.

protected void write ( String toWrite ) throws IOException

{

if (! okToWrite)

throw new IOException(” ﬁle not open for writing . ”) ;

if ( printToScreen )

System.out . print ( toWrite ) ;

try {

fw. write ( toWrite ) ;

} catch (IOException e) {

