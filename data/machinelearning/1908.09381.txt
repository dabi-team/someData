Tutorial and Survey on Probabilistic Graphical
Model and Variational Inference in Deep
Reinforcement Learning

Xudong Sun
Department of Statistics
Ludwig Maximillian University of Munich
Munich, Germany
Email: xudong.sun@stat.uni-muenchen.de

Bernd Bischl
Department of Statistics
Ludwig Maximillian University of Munich
Munich, Germany

9
1
0
2

c
e
D
8

]

G
L
.
s
c
[

5
v
1
8
3
9
0
.
8
0
9
1
:
v
i
X
r
a

Abstract—Aiming at a comprehensive and concise tutorial
survey, recap of variational inference and reinforcement learning
with Probabilistic Graphical Models are given with detailed
derivations. Reviews and comparisons on recent advances in deep
reinforcement learning are made from various aspects. We offer
detailed derivations to a taxonomy of Probabilistic Graphical
Model and Variational Inference methods in deep reinforcement
learning, which serves as a complementary material on top of
the original contributions.

Keywords—Probabilistic Graphical Models; Variational Infer-

ence; Deep Reinforcement Learning

I. INTRODUCTION

Despite the recent successes of Reinforcement Learning,
powered by Deep Neural Networks, in complicated tasks like
games [1] and robot locomotion [2], as well as optimization
tasks like Automatic Machine Learning [3]. The ﬁeld still
faces many challenges including expressing high dimensional
state and policy, exploration in sparse reward, etc. Probabilistic
Graphical Model and Variational Inference offers a great tool
to express a wide spectrum of trajectory distributions as well as
conducting inference which can serve as a control method. Due
to the emerging popularity, we present a comprehensive and
concise tutorial survey paper with the following contributions:

•

•

We provide Probabilistic Graphical Models for many
basic concepts of Reinforcement Learning, which is
rarely covered in literature. We also provide Probabilistic
Graphical Models to some recent works on Deep Rein-
forcement Learning [4], [5] which does not exist in the
original contributions.
We cover a taxonomy of Probabilistic Graphical Model
and Variational Inference [6] methods used in Deep
Reinforcement Learning and give detailed derivations to
many of the critical equations, which is not given in
the original contributions. Together with the recap of
variational inference and deep reinforcement learning, the
paper serves as a self-inclusive tutorial to both beginner
and advanced readers.

A. Organization of the paper

In section I-B, we ﬁrst

introduce the fundamentals of
Probabilistic Graphical Models and Variational Inference, then
we review the basics about reinforcement learning by con-
necting probabilistic graphical models (PGM) in section II-A,
II-B, II-C, as well as an overview about deep reinforcement
learning, accompanied with a comparison of different methods
in section II-D. In section III-A, we discuss how undirected
graph could be used in modeling both the value function and
the policy, which works well on high dimensional discrete state
and action spaces. In section III-B, we introduce the directed
acyclic graph framework on how to treat the policy as posterior
on actions, while adding many proofs that does not exist
in the original contributions. In section III-C, we introduce
works on how to use variational inference to approximate the
environment model, while adding graphical models and proofs
which does not exist in the original contributions.

B. Prerequisite on Probabilistic Graphical Models and Vari-
ational Inference, Terminologies and Conventions

|

⊥⊥

C)

We use capital letter to denote a Random Variable (RV),
while using the lower case letter to represent the realization.
To avoid symbol collision of using A to represent advantage in
many RL literature, we use Aact explicitly to represent action.
A to represent B is conditionally indepen-
We use (B
dent from C, given A, or equivalently p(B
A)
A). Directed Acyclic Graphs
or p(BC
|
(DAG) [7] as a PGM offers an instinctive way of deﬁning fac-
torized joint distributions of RV by assuming the conditional
independence [7] through d-separation [7]. Undirected Graph
including Markov Random Fields also speciﬁes the conditional
independence with local Markov property and global Markov
property [8].

A, C) = p(B

A) = P (B

A)P (C

|

|

|

|

x) =

Variational Inference (VI) approximates intractable posterior
z) with
distribution p(z
latent variable z speciﬁed in a probabilistic graphical model,
by a variational proposal posterior distribution qφ(z
x),
characterized by variational parameter φ. By optimizing the

p(z)p(x

z(cid:48) )dz(cid:48)
|

p(z(cid:48) )p(x

z(cid:48)

1

|

|

|

(cid:82)

 
 
 
 
 
 
(cid:20)

log

+ Eq

qφ(z

(cid:21)

x)

|
p(z)
Eq [log p(z

x)] +

|

(cid:21)

qφ(z
p(z

x)
x)

|
|

trajectory τ [14]. Figure 2 illustrates part of a trajectory in one
rollout. The state space
, which can be
either discrete or continuous and multi-dimensional, are each
represented with one continuous dimension in Figure 2 and
plotted in an orthogonal way with different colors, while we
.
use the thickness of the plate to represent the reward space

and action space

A

S

R

log p(x)

(cid:20)

Eq

log

=Eq [log p(x

z)]

=

−

p(x, z))

−
Eq log qφ(z

|
DKL(qφ(z
x)
|
x)
|
=
F (φ, θ) + Hq(p)
H(q)
=ELBO(φ, θ) + DKL(qφ(z

−

−

||

−

x)

p(z

||

|

x))

(1)

|

Evidence Lower Bound (ELBO) [6], VI assigns the values to
observed and latent variables at the same time. VI is widely
used in Deep Learning Community like variational resampling
[9]. VI is also used in approximating the posterior on the
weights distribution of neural networks for Thompson Sam-
pling to tackle the exploration-exploitation trade off in bandit
problems [10], as well as approximating on the activations
distribution like Variational AutoEncoder [11].

As a contribution of this paper, we summarize the relation-
ship of evidence log p(x), KL divergence DKL, cross entropy
Hq(p), entropy H(q), free energy F (φ, θ) and ELBO(φ, θ)
in Equation (1).

II. REINFORCEMENT LEARNING AND DEEP
REINFORCEMENT LEARNING
A. Basics about Reinforcement Learning with graphical model

Fig. 1. Concept of Reinforcement Learning

1) RL Concepts, Terminology and Convention: As shown
in Figure 1, Reinforcement Learning (RL) involves optimizing
the behavior of an agent via interaction with the environment.
At time t, the agent lives on state St, By executing an action at
St), the agent jumps to another
according to a policy [12] π(a
|
state St+1, while receiving a reward Rt. Let discount factor γ
decides how much the immediate reward is favored compared
to longer term return, with which one could also allow
tractability in inﬁnite horizon reinforcement learning [12], as
well as reducing variance in Monte Carlo setting [13]. The
goal is to maximize the accumulated rewards, G = (cid:80)T
t=0 γtRt
which is usually termed return in RL literature.

→ ∞

For simplicity, we interchangeably use two conventions
whenever convenient: Suppose an episode last from t = 0 : T ,
with T
correspond to continuous non-episodic reinforce-
ment learning. We use another convention of t
∞}
by assuming when episode ends, the agent stays at a self
absorbing state with a null action, while receiving null reward.
By unrolling Figure 1, we get a sequence of state, action and
in an episode, which is coined

reward tuples

(St, Aact

∈ {

· · ·

0,

,

{

t

, Rt)
}

Fig. 2.

Illustration of State, Action and Reward Trajectory

2) DAGs for (Partially Observed ) Markov Decision Pro-
cess: Reinforcement Learning is a stochastic decision process,
which usually comes with three folds of uncertainty. That is,
under a particular stochastic policy characterized by π(a
s) =
|
s), within a particular environment characterized by state
p(a
|
st, a) and reward distribution
transition probability p(st+1|
st, at), a learning agent could observe different
function p(rt|
trajectories with different unrolling realizations. This is usually
modeled as a Markov Decision Process [12], with its graphical
model shown in Figure 3, where we could deﬁne a joint
probability distribution over the trajectory of state , action and
reward RVs. In Figure 3, we use dashed arrows connecting
state and action to represent the policy, upon ﬁxed policy π,
we have the trajectory likelihood in Equation (2)

p(τ ) = p(s0)

T
(cid:89)

t=0

p(st+1|

st, at)p(rt|

st, at)π(at|

st)

(2)

Upon observation of a state st in Figure 3, the action at the
time step in question is conditionally independent with the
S0, Aact
state and action history
, which
Et =
, St
0 ,
{
could be denoted as (Aact
St. A more realistic model,
t ⊥⊥ Et)
|

1}
−

· · ·

Fig. 3. Directed Acyclic Graph For Markov Decision Process

however, is the Partially Observable Markov Decision process
[15], with its DAG representation shown in Figure 4, where
the agent could only observe the state partially by observing
Ot through a non invertible function of the next state St+1 and
st+1, at), while
the action at, as indicated the Figure by p(ot|

AgentatStEnvironmentActionat∼π(a|St)StateSt+1RewardRttStat∼π(a|st)Srt=0Rat+Nst+Nrt+N6=0rt+2N=0AAacttRtRt+1Rt+2Aactt+1StSt+1St+2Aactt+2p(at|st)p(rt|st,at)p(st+1|st,at)the distributions on other edges are omitted since they are the
same as in Figure 3. Under the graph speciﬁcation of Figure
4, the observable Ot is no longer Markov, but depends on the
whole history, however, the latent state St is still Markov. For
POMDP, belief state bt is deﬁned at time t, which is associated
with a probability distribution bt(st) over the hidden state
St, with (cid:80)
b(St) = 1, where state S takes value in latent
[15]. The latent state distribution associated with
state space

S
S

environment (including transition probability and reward prob-
ability) respectively. State action value function [12] is deﬁned
in Equation (9), where in Equation (10), its relationship to the
state value function is stated.

St = s, Aact
Qπ(s, a) (
∀
=Eπ,ε[Rt(St = s, Aact

t = a)

t = a) +

∞(cid:88)

γiRt+i(St+i, Aact

t+i)] (9)

=Eπ,ε[Rt(St = s, Aact

i=1
t = a) + γV π(St+1)]

Combining Equation (8) and Equation (9), we have

V (s) =

(cid:88)

a

π(a
|

s)Q(s, a)

Deﬁne optimal policy [12] to be

(10)

(11)

(12)

Fig. 4. Probabilistic Graphical Model for POMDP

belief state can be updated in a Bayesian way in Equation (6).

π∗ = arg max

π

= arg max

π

V π(s),

s
∀

S

∈
Eπ[Rt + γV π(St+1)]

B. Value Function, Bellman Equation, Policy Iteration

Deﬁne state value function of state s

in Equation
(7), where the corresponding Bellman Equation is derived in
Equation (8).

∈ S

Taking the optimal policy π∗ into the Bellman Equation in
Equation (8), we have

V π∗ (s) = Eπ∗,ε

(cid:104)
Rt(s, Aact

t

) + γV π∗ (St+1)

(cid:105)

(13)

Taking the optimal policy π∗ into Equation (9), we have

V π(s) = Eπ,ε[

∞(cid:88)

i=0

γiRt+i(St+i, Aact
t+i)

St = s]

| ∀

(7)

Qπ∗ (s, a) = Eπ∗,ε[Rt(s, a) +

∞(cid:88)

i=1

γiRt+i(St+i, Aact

t+i)] (14)

=Eπ,ε[Rt(St, Aact

t

) + γ

=Eπ,ε[Rt(St, Aact

t

) + γ

∞(cid:88)

i=1

∞(cid:88)

γ(i

−

1)Rt+i(St+i, Aact

t+i)]

Based on Equation (14) and Equation (13), we get

V π∗ (s) = max

a

Qπ∗ (s, a)

(15)

γi(cid:48) Rt+1+i(cid:48) (St+1+i(cid:48) , Aact

t+1+i(cid:48) )]

and

t

i(cid:48) =0
) + γV π(St+1)]
p(st+i+1|

=Eπ,ε[Rt(St, Aact
where St+i ∼
,
S
Aact
, and we have used
St+i+1) taking value from
π(a
t+i ∼
|
the π and ε in the subscript of the expectation E operation
to represent the probability distribution of the policy and the

st+i, at+i) takes value from

(8)

A

bt+1(st+1)
=p(st+1 |
=

ot, at, bt)
p(st+1, ot, at, bt)
p(ot, at, bt)

=p(ot |

at, st+1, bt)
(cid:80)

(cid:80)

=p(ot |

st+1, at)

=p(ot |

st+1, at)

=p(ot |

st+1, at)

p(st+1, at, bt)
p(st+1, at, bt)
p(st+1 |
at, bt)
p(ot |
at, bt)
p(st, st+1 |
p(ot |
at, bt)
p(st+1 |

st

st

at, bt)

(cid:80)

st

p(st+1 |

at, bt)

p(ot |

st, at, bt)p(st |
at, bt)
st, at)p(st |
at, bt)

p(ot |

at, bt)

(3)

(4)

(5)

(6)

Qπ∗ (s, a) = Eε,π∗

(cid:104)

Rt(s, a) + γmax

¯a

(cid:105)
Qπ∗ (St+1, ¯a)

(16)

For learning the optimal policy and value function, General
Policy Iteration [12] can be conducted, as shown in Figure
5, where a contracting process [12] is drawn. Starting from
initial policy π0, the corresponding value function V π0 could
be estimated, which could result in improved policy π1 by
greedy maximization over actions. The contracting process is
supposed to converge to the optimal policy π∗.

As theoretically fundamentals of learning algorithms, Dy-
namic programming and Monte Carlo learning serve as two ex-
tremities of complete knowledge of environment and complete
model free [12], while time difference learning [12] is more
ubiquitously used, like a bridge connecting the two extremities.
Time difference learning is based on the Bellman update error
δt = Q(st, at)

Rt(s, a) + γmax

Q(st+1, a)

(cid:17)

(cid:16)

.

−

a

C. Policy Gradient and Actor Critic

Reinforcement Learning could be viewed as a functional op-
timization process. We could deﬁne an objective function over
a policy πθ(a
s), as a functional, characterized by parameter
|
θ, which could correspond to the neural network weights, for
example.

Aactt−1Ot−1OtOt+1AacttSt−1StSt+1Aactt+1Rt−1RtRt+1p(ot|st+1,at)Fig. 5. General Policy Iteration

η(s) =

(cid:88)

k=0

γkP π(s0 →
(cid:88)

s, k + 1)

= h(s) +

γη(¯s)πθ(a
|

¯s)P (s
|

¯s, a)

¯s,a

=

=

=

=

(17)

(18)

Suppose all episodes start from an auxiliary initial state s0,

which with probability h(s), jumps to different state s
∈ S
without reward. h(s) characterizes the initial state distribution
which only depends on the environment. Let η(s) represent
the expected number of steps spent on state s, which can
be calculated by summing up the γ discounted probability
P π(s0 →
s, k + 1) of entering state s with k + 1 steps
from auxiliary state s0, as stated in Equation (17), which
can be thought of as the expectation of γk conditional on
state s. In Equation (18), the quantity is calculated by either
directly starting from state s, which correspond to k = 0 in
Equation (17), or entering state s from state ¯s with one step,
corresponding to k + 1

2 in Equation (17).

≥
For an arbitrary state s

, using s(cid:48) and s(cid:48)(cid:48)

to repre-
∈ S
sent subsequent states as dummy index, the terms in square
brackets in Equation (22) are simply Equation (21) with a and
∇θV π(θ)(s∞) = 0, Equation
s(cid:48) replaced by a(cid:48) and s(cid:48)(cid:48) . Since
(22) could be written as Equation (23), where sk represent the
state of k steps after s and P π(s
sk, k) already includes
integration of intermediate state sk
1, . . . s1 before reaching
state sk.

→
−

∇θV π(θ)(s) =
(cid:88)
(cid:88)

(cid:88)

(cid:88)

a
γkP π(s

k=1

sk

ak

∇θπθ(a

s)Qπ(θ)(s, a)+
|

sk, k)

∇θπθ(ak|

sk)Qπ(θ)(sk, ak)

→

(23)

Let objective function with respect to policy be deﬁned to
be the value function starting from auxiliary state s0 as in
Equation (24).

J(πθ) = V π(s0) = Eπ,ε

γtRt(S0 = s)

(24)

∞(cid:88)

t=0

The optimal policy could be obtained by gradient accent
optimization, leading to the policy gradient algorithm [12],

a

(cid:88)

∇θ
(cid:88)

a

+

a
(cid:88)

s(cid:48)

(cid:88)

a
(cid:88)

a
(cid:88)

a


(cid:34)

(cid:88)

∇θV π(θ)(s) =
(cid:88)

∇θ
∇θQπ(θ)(s, a)πθ(a
|

(cid:104)

a

Qπ(θ)(s, a)πθ(a

(cid:35)

s)
|

(19)

s) +

(cid:105)
s)Qπ(θ)(s, a)
∇θπθ(a
|





(cid:88)

s(cid:48) ,R

P (s(cid:48), R

s, a)
|

(cid:16)

R + γV π(θ)(s(cid:48))

(cid:17)



 πθ(a
|

s)

s)Qπ(θ)(s, a)
∇θπθ(a
|

γP (s(cid:48)

s, a)
|

∇θV π(θ)(s(cid:48))πθ(a
|

s)+

s)Qπ(θ)(s, a)
∇θπθ(a
|

s)Qπ(θ)(s, a) +
∇θπθ(a
|

(cid:88)

(cid:88)

a

s(cid:48)

γP (s(cid:48)

s)
s, a)πθ(a
|

|

(cid:88)



(cid:88)

a(cid:48)

s(cid:48)(cid:48)

γP (s(cid:48)(cid:48)

s(cid:48), a(cid:48))

|

∇θV π(θ)(s(cid:48)(cid:48) )πθ(a(cid:48)


s(cid:48))+
|

(cid:88)

a(cid:48)

∇θπθ(a(cid:48)

|

s(cid:48))Qπ(θ)(s(cid:48), a(cid:48))



(22)

as in Equation (29).

∇θJ(πθ) =
(cid:88)
(cid:88)
(cid:88)
=

k=0
(cid:88)

sk
(cid:88)

ak

η(s)

=

=

=

=

s
(cid:80)
(cid:80)

a
s η(s)
s η(s)

(cid:88)

¯s
(cid:88)

η(¯s)

η(¯s)

∇θV π(s0)
γkP π(s0 →

sk, k)

∇θπθ(ak|

sk)Qπ(θ)(sk, ak)

s)Qπ(θ)(s, a)
∇θπθ(a
|
(cid:88)

(cid:88)

η(s)

s)Qπ(θ)(s, a)
∇θπθ(a
|

s
(cid:88)

a
(cid:88)

s
(cid:88)

a
(cid:88)

µ(s)

µ(s)

s)Qπ(θ)(s, a)
∇θπθ(a
|
s)
πθ(a
s)Qπ(θ)(s, a)
s) ∇θπθ(a
|
πθ(a
|
|

¯s

(cid:20)

Eπ

∝

s

a
S)
∇θπθ(A
|
S)
πθ(A
|

(cid:21)

ˆQπ(θ)(S, A)

The policy gradient could be augmented to include zero
gradient baseline b(s), with respect to objective function J(πθ)
in Equation (28), as a function of state s, which does not
include parameters for policy θ, since (cid:80)
s) = 0. To
reduce variance of the gradient, the baseline is usually chosen
to be the state value function estimator ˆVw(s) to smooth out
the variation of Q(s, a) at each state, while ˆVw(s) is updated
in a Monte Carlo way by comparing with ˆQπθ (S, A) = Gt.
Vw(st) to
Vw(st), so bootstrap is used instead of

The actor-critic algorithm [12] decomposes Gt −

a ∇θπθ(a
|

be Rt + γVw(st+1)
Monte Carlo.

−

(20)

(21)

(25)

(26)

(27)

(28)

(29)

π0Vπ0······πiπi+1Vπiπ∗→Vπ∗i=0i=MpolicyevaluationpolicyimproveD. Basics of Deep Reinforcement Learning

Deep Q learning [1] makes a breakthrough in using neural
network as the functional approximator for value function on
complicated tasks. It solves the transition correlation problem
by random sampling from a replay memory. Speciﬁcally, the
reinforcement learning is transformed in a supervised learning
Q(st+1, a) from the
task by ﬁtting on the target Rt + γmax
replay memory with state st as input. However, the target
can get drifted easily which leads to unstable learning. In
[1], a target network is used to provide a stable target for
the updating network to be learned before getting updated
occasionally. Double Deep Q learning [16], however, solves
the problem by having two Q network and update the param-
eters in a alternating way. We review some state of art deep
reinforcement learning algorithms from different aspects:

a

1) Off Policy methods: Except for Deep Q Learning [1]
mentioned above, DDPG [17] extends Deterministic Policy
Gradient (DPG) [18] with deep neural network functional
approximator, which is an actor-critic algorithm and works
well in continuous action spaces.

2) On Policy methods: A3C [19] stands out in the asyn-
chronous methods in deep learning [19] which can be run
in parallel on a single multi-core CPU. Trust Region Pol-
icy Optimization [2] and Proximal Policy Optimization [20]
assimilates the natural policy gradient, which use a local
approximation to the expected return. The local approxima-
tion could serve as a lower bound for the expected return,
which can be optimized safely subject to the KL divergence
constraint between two subsequent policies, while in practice,
the constraint is relaxed to be a regularization.

3) Goal based Reinforcement Learning: In robot manipu-
lation tasks, the goal could be represented with state in some
cases [14]. Universal Value Function Approximator (UVFA)
[21] incorporate the goal into the deep neural network, which
let the neural network functional approximator also generalize
to goal changes in tasks, similar to Recommendation System
[22]. Work of this direction include [23], [14], for example.

4) Replay Memory Manipulation based Method: Replay
memory is a critical component in Deep Reinforcement Learn-
ing, which solves the problem of correlated transition in one
episode. Beyond the uniform sampling of replay memory in
Deep Q Network [1], Prioritized Experience Replay [24] im-
proves the performance by giving priority to those transitions
with bigger TD error, while Hindsight Experience Replay
(HER) [23] manipulate the replay memory with changing
goals to transition so as to change reward to promote explo-
ration. Maximum entropy regularized multi goal reinforcement
learning [14] gives priority to those rarely occurred trajectory
in sampling, which has been shown to improve over HER [14].
5) Surrogate policy optimization: Like surrogate model
used in Bayesian Optimization [25], lower bound surrogate
is also used in Reinforcement Learning. Trust Region Policy
Optimization (TRPO) [2] is built on the identity from [26]
in Equation (30), where ηπnew (s) means the state visitation
frequency under policy πnew and advantage Aπold
(at, st) =

J(πnew) = J(πold) +

(cid:88)

s

ηπnew (s)

(cid:88)

a

s)Aπold
πnew(a
|

(a, s)

(30)

Lπold(πnew)

=J(πold) +

(cid:88)

s

ηπold(s)

(cid:88)

a

s)Aπold
πnew(a
|

(at, st)

(31)

V πold

(at, st)

Qπold
(st). Based on Policy Advantage [26]
−
Aπold,ηold(πnew) = (cid:80)
s ηπold(s) (cid:80)
s)Aπold
a πnew(a
(a, s),
|
a local approximation Lπold (πnew) to Equation (30) can
be deﬁned in Equation (31), based on which, a surrogate
function M (πnew, πold) is deﬁned in Equation (32) that
minorizes J(πnew) at πold, where Dmax
KL (πold, πnew) =
s), πnew(a
s)) is the maximum KL diver-
max
|
s
gence, so MM [2] algorithm could be used to improve the
policy, leading to the trust region method [2].

DKL(πold(a
|

TABLE I
COMPARISON OF DEEP REINFORCEMENT LEARNING METHODS: ”S”
MEANS STATE AND ”A” MEANS ACTION, WHERE ”C” MEANS
CONTINUOUS, ”D” MEANS DISCRETE. ”STANDALONE” MEANS WHETHER
THE ALGORITHM WORK INDEPENDENTLY OR NEEDS TO BE COMBINED
WITH ANOTHER LEARNING ALGORITHM. ”VAR” MEANS WHICH
PROBABILITY THE VARIATIONAL INFERENCE IS APPROXIMATING, ”P”
MEANS WHETHER THE METHOD IS ON POLICY OR OFF POLICY. ”NA”
MEANS NOT APPLICABLE

S
Algorithm
c
Deep Q
c
A3C
TRPO/PPO c
c
DDPG
d
Boltzmann
c
VIME
c
VAST
c
SoftQ

A
d
c/d
c/d
c
d
c
d
c/d

standalone
y
y
y
y
y
n
n
y

var
na
na
na
na
na
pθ(st+1
p(st

|
p(at

st, at)
|
ot−k)
st)
|

p
off
on
on
off
on
na
na
on

III. TAXONOMY OF PGM AND VI IN DEEP
REINFORCEMENT LEARNING

Despite the success of deep reinforcement learning in many
talks, the ﬁeld still faces some critical challenges. One prob-
lem is exploration with sparse reward. In complicated real
environment, an agent has to explore for a long trajectory
before it can get any reward as feedback. Due to lack of
enough rewards, traditional Reinforcement Learning methods

M (πnew, πold)

=Lπold (πnew)

−

Aπold

γ
(s, a)
|

4max
a,s |
1

γ2

−

Dmax

KL (πold, πnew)

(32)

performs poorly, which lead to a lot of recent contributions in
the exploration methods. Another challenge is how to represent
policy in extremely large state and action spaces. Furthermore,
sometimes it is beneﬁcial to have multimodal behavior for
a agent when some trajectory might be equivalent to other
trajectories and we want to learn all of them.

In this section, we give detailed explanation on how graph-
ical model and variational inference could be used to model
and optimize the reinforcement learning process under these
challenges and form a taxonomy of these methods.

Together with the deep reinforcement

learning methods
mentioned in section II-D, we make a comparison of them
in Table I.

A. Policy and value function with undirected graphs

We ﬁrst discuss the application of undirected graphs in
deep reinforcement learning, which models joint distribution
of variables with cliques [7]. In [27], the authors use Restricted
Boltzmann Machine (RBM) [8], which has nice property
of tractable factorized posterior distribution over the latent
variables conditioned on observed variables. To deal with
MDPs of large state and action spaces, they model the state-
action value function with the negative free energy of a
Restricted Boltzmann Machine. Speciﬁcally, the visible states
of the Restricted Boltzmann Machine [27] consists of both
state s and action a binary variables, as shown in Figure 6,
where the hidden nodes consist of L binary variables, while
it can be
state variables si are dark colored to represent
observed and actions aj are light colored to represent it need
to be sampled. Together with the auxiliary hidden variables,
the undirected graph deﬁnes a joint probability distribution
over state and action pairs, which deﬁnes a stochastic policy
network that could sample actions out for on policy learning.
Since it
is pretty easy to calculate the derivative of the
free energy F (s, a) with respect to the coefﬁcient wk,j of
the network, one could use temporal difference learning to
update the coefﬁcients in the network. Thanks to properties
of Boltzmann Machine, the conditional distribution of action
over state p(a
s), which could be used as a policy, is still
|
Boltzmann distributed as in Equation (33), governed by the
free energy F (a, s), where Z(s) is the partition function [7]
and the negative free energy to approximate the state action
value function Q(s, a). By adjusting the temperature T , one
could also change between different exploration strength.

Fig. 6. Restricted Boltzmann Machine Value and Policy

s) = 1/Z(s)e−
p(a
|

F (s,a)/T = 1/Z(s)eQ(s,a)/T

(33)

A few steps of MCMC sampling [7] could be used to sample
actions, as an approximation of the policy, which can be fed
into a time difference learning method like SARSA [12], to

Fig. 7. Optimal Policy as posterior on actions: p(at|st, Ot:T = 1)

update the state value function Q(s, a)’s estimation. Such an
on-policy process has been shown to be empirically effective
in the large state actions spaces [27].

B. Variational Inference on ”optimal” Policies

1) policy as ”optimal” posterior: The Boltzmann Machine
deﬁned Product of Expert Model in [27] works well for large
state and action spaces, but are limited to discrete speciﬁcally
binary state and action variables. For continuous state and
action spaces, in [28], the author proposed deep energy based
models with Directed Acyclic Graphs (DAG) [7], which we
re-organize in a different form in Figure 7 with annotations
added. The difference with respect to Figure 3 is that, in Figure
7, the reward is not explicit expressed in the directed graphical
model. Instead, an auxilliary binary Observable O is used to
deﬁne whether the corresponding action at the current step
is optimal or not. The conditional probability of the action
being optimal is p(Ot = 1
st, at) = exp(r(st, at)), which
|
connects conditional optimality with the amount of award
received by encouraging the agent to take highly rewarded
actions in an exponential manner. Note that the reward here
must be negative to ensure the validity of probability, which
does not hurt generality since reward range can be translated
[13].

The Graphical Model

in Figure 7 in total deﬁnes the

trajectory likelihood or the evidence in Equation (34):

(cid:34)

p(τ ) =

p(s1)

(cid:89)

t

(cid:35)

p(st+1|

st, at)

exp

(cid:33)

r(st, at)

(34)

(cid:32)

(cid:88)

t

.

By doing so, the author is forcing a form of functional
expression on top of the conditional independence structure of
the graph by assigning a likelihood. In this way, calculating the
optimal policy of actions distributions becomes an inference
st, Ot:T = 1), which
problem of calculating the posterior p(at|
reads as, conditional on optimality from current time step
until end of episode, and the current current state to be st,
the distribution of action at, and this posterior corresponds to
the optimal policy. Observing the d-separation from Figure 7,
1 is conditionally independent of at given st, (O1:t
O1:t
1 ⊥⊥
−
Aact
St, so p(at|
)
t
2) Message passing for exact inference on the posterior:
In this section, we give detailed derivation on conducting
exact inference on the policy posterior which is not given

1, Ot:T ) = p(at|

st, Ot:T )

st, O1:t

−

−

|

sisi+1ajaj+1s,a∈{0,1}Dhk+1hk+2hk+3h∈{0,1}LF(s,a)wi,1wj+1,3Aactt−1Ot−1OtOt+1AacttSt−1StSt+1Aactt+1exp(r(st−1,at−1))p(Ot=1|st,at)p(st+1|st,at)st, Ot:T = 1)
p(at|
p(at, st, Ot:T = 1)
p(st, Ot:T = 1)

p(Ot:T = 1
|

at, st)p(at, st)

p(st, Ot:T = 1)

(cid:82)

a(cid:48)t

at(cid:48)

p(Ot:T = 1
at, st)p(at|
st)p(st)
|
(cid:82)
p(st, a(cid:48)
a(cid:48)
t, Ot:T = 1)d
t}
{
st)p(st)
p(Ot:T = 1
at, st)p(at|
|
st)p(st)d
at(cid:48), st)p(at(cid:48)|
p(Ot:T = 1
{
|
st)
at, st)p(at|
p(Ot:T = 1
|
p(Ot:T = 1
st)d
at(cid:48), st)p(at(cid:48)|
{
|
β(at, st)
a(cid:48)
t, st)d
β(a(cid:48)
t}
{

a(cid:48)
t}

a(cid:48)t

(cid:82)

(cid:82)

a(cid:48)t
β(at, st)
β(st)

=

=

=

=

=

=

=

=

(cid:90)

(cid:90)

=

=

=

=

=

a(cid:48)
t}

(35)

β(a(cid:48)

t, st)d

in [13]. Similar to the forward-backward message passing
algorithm [7] in Hidden Markov Models [7], the posterior
st, Ot:T = 1) could also be calculated by passing
p(at|
messages. We offer a detailed derivation of the decomposi-
st, Ot:T = 1) in Equation (35),
tion of the posterior p(at|
which is not available in [13]. In Equation (35), we deﬁne
st) and message
message β(at, st) = p(Ot:T = 1
at, st)p(at|
|
β(st) = (cid:82)
st) as a prior
. If we consider p(at|
a(cid:48)
t}
a(cid:48)t
{
with a trivial form of uniform distribution [13], the only policy
at, st).
related term becomes p(Ot:T = 1
|

In contrast to HMM, here, only the backward messages are
relevant. Additionally, the backward message β(at, st) here
is not a probability distribution as in HMM, instead, is just a
probability. In Figure 7, the backward message β(at, st) could
be decomposed recursively. Since in [13] the author only give
the conclusion without derivation, we give a detailed derivaion
of this recursion in Equation (36). The recursion in Equation
(36) start from the last time point T of an episode.

3) Connection between Message Passing and Bellman
equation: If we deﬁne Q function in Equation (37) and V
function in Equation (38)

Q(st, at) = log(β(at, st))

(37)

then the corresponding policy could be written as Equation

(39).

π(at|

st) = p(at|

st, Ot:T = 1) = exp(Q(st, at)

V (st))

−

(39)
Taking the logrithm of Equation (36), we get Equation (40)
which reduces to the risk seeking backup in Equation (41) as
mentioned in [13]:

Q(st, at) = r(st, at) + log Est+1∼

p(st+1|

st,at)[exp(V (st+1))]
(41)

β(st, at)

st, at)
=p(Ot = 1, Ot+1:T = 1
|
(cid:82) p(Ot = 1, Ot+1:T = 1, st, at, st+1, at+1)d

st+1, at+1}
{

p(st, at)

st+1, at+1}
st, at)d
p(Ot+1:T = 1, st+1, at+1, Ot = 1
{
|
st, at)p(Ot = 1
st, at)
|
((Ot+1:T , St+1, At+1 ⊥⊥
p(st+1, st, at)
p(st, at)

p(Ot+1:T = 1, st+1, at+1|
st+1, at+1}
d
{
(cid:90) p(Ot+1:T = 1, st+1, at+1)
p(st+1, at+1)
p(Ot = 1
st+1, at+1}
st, at)d
|
{
(cid:90)
st+1, at+1)p(st+1|
p(Ot+1:T = 1
|

st, at)p(Ot = 1
|

Ot)

|

st, at)

St, At)

d
{
(cid:90)

st+1, at+1}
β(st+1)p(st+1|

st, at)p(Ot = 1
|

st, at)dst+1

(36)

V (st) = log β(st) = log
(cid:90)

(cid:90)

β(st, at)dat

= log

exp(Q(st, at))dat ≈

max
at

Q(st, at)

(38)

The mathematical

insight here is that

if we deﬁne the
messages passed on the Directed Acyclic Graph in Figure 7,
then message passing correspond to a peculiar version Bellman
Equation like backup, which lead to an unwanted risk seeking
behavior [13]: when compared to Equation (10), the Q function
here is taking a softmax instead of expectation over the next
state.

4) Variational approximation to ”optimal” policy: Since
the exact inference lead to unexpected behavior, approximate
inference could be used. The optimization of the policy could
be considered as a variational
inference problem, and we
use the variational policy of the action posterior distribution
st), which could be represented by a neural network, to
q(at|
compose the proposal variational likelihood of the trajectory
as in Equation (42): where the initial state distribution p(s1)
and the environmental dynamics of state transmission is kept

log(β(st, at))
(cid:90)

= log

(cid:90)

= log

β(st+1)p(st+1|
st, at)dst+1
st, at)p(Ot = 1
|
exp[r(st, at) + V (st+1)]p(st+1|
exp(V (st+1))p(st+1|

st, at)dst+1

(cid:90)

st, at)dst+1 (40)

= r(st, at) + log

q(τ ) = p(s1)

(cid:89)

t

[p(st+1|

st, at)q(at|

st)]

(42)

log(p(O1:T ))

(cid:90)

= log

p(O1:T = 1, s1:T , a1:T )

q(s1:T , a1:T )
q(s1:T , a1:T )

ds1:T da1:T

= log Eq(s1:T ,a1:T )

p(O1:T = 1, s1:T , a1:T )
q(s1:T , a1:T )

Eq(s1:T ,a1:T )[log p(O1:T = 1, s1:T , a1:T )

−

log q(s1:T , a1:T )]
(43)

≥

=

−

(cid:88)

=

t=1:T

DKL(q(τ )

=Eq(s1:T ,a1:T )[

p(τ ))
|
(cid:88)

[r(st, at)

t=1:T

log q(at|

−

st)]]

Est,at[r(st, at) + H(π(at|

st))]

(44)

(45)

intact. Using the proposal trajectory as a pivot, we could derive
the Evidence Lower Bound (ELBO) of the optimal trajectory
as in Equation (43), which correspond to an interesting ob-
jective function of reward plus entropy return, as in Equation
(45).

5) Examples:

In [28], the state action value function is
deﬁned in Equation (46). and a soft version of Bellman update
similar to Q Learning [12] is carried out, which lead to policy
improvement with respect
to the corresponding functional
objective in Equation (47). Setting policy as Equation (39)
lead to policy improvement. We offer a detailed proof for a
key formula in Equation (48), which is stated in Equation
(19) of [28] without proof. In Equation (48), we use π(
s) to
s) to avoid symbol aliasing whenever
implicitly represent π(a
|
necessary. For the rest of the proof, we invite the reader to read
the appendix of [28]. Algorithms of the this kind of maximum
entropy family also include Soft Actor Critic [29].

·|

Qπ

sof t(s, a) = r0 + Er

∼

π,s0=s,a0=a[

∞(cid:88)

γt(rt + αH(π(.

t=1

st)))]
|

(46)

J(π)

E(st,at)

∞(cid:88)

ρπ

γl

−

tE(sl,al)[r(sl, al)+

(cid:88)

=

t

∼

l=t
st, at]]
|
ρπ [Qπ

∼

sl))

αH(π(.
|
(cid:88)
E(st,at)

=

t

sof t(st, at) + αH(π(.

st))]
|

(47)

s)) + Ea

π[Qπ

sof t(s, a)]

∼

s)[log π(a
|

s)

−

Qπ

sof t(s, a)]da

·|
π(a
|

π(a
|

s)[log π(a
|

s)

−

log[exp(Qπ

sof t(s, a))]]da

π(a
|

s)[log π(a
|

s)

−

log[

exp(Qπ
(cid:82) exp(Qπ

sof t(s, a))
sof t(s, a(cid:48)))da(cid:48)

H(π(
(cid:90)

=

=

=

−

−

−
(cid:90)

a
(cid:90)

a
(cid:90)

a

sof t(s, a(cid:48)))da(cid:48)]]da

exp(Qπ
(cid:90)

=

−

log

a
(cid:90)

π(a

s)[log π(a
|
|

s)

−

log[˜π(a

s)]
|

−

exp(Qπ

sof t(s, a(cid:48)))]da(cid:48)

=

−

DKL(π(

s)

˜π(

·|

||

(cid:90)

s)) + log

·|

exp(Qπ

sof t(s, a(cid:48)))da(cid:48)

(48)

C. Variational Inference on the Environment

Another direction of using Variational Inference in Rein-
forcement Learning is to learn an environmental model, either
on the dynamics or the latent state space posterior.

1}
−

s1:t, a1:t
{

ξt,at)DKL(p(θ

1) Variational inference on transition model: In Variational
Information Maximizing Exploration (VIME) [4], where dy-
st, at) for the agent’s interaction with
namic model pθ(st+1|
the environment is modeled using Bayesian Neural Network
[10]. The R.V. for θ is denoted by Θ, and is treated in a
Bayesian way by modeling the weight θ uncertainty of a
neural network. We represent this model with the graphical
model in Figure 8, which is not given in [4]. The belief
uncertainty about the environment is modeled as entropy of the
posterior distribution of the neural network weights H(Θ
ξt)
|
based on trajectory observations ξt =
. The
method encourages taking exploratory actions by alleviating
the average information gain of the agent’s belief about
the environment after observing a new state st+1, which
ξt)), and this is equiva-
is Ep(st+1|
ξt+1)
|
lent to the entropy minus conditional entropy H(Θ
ξt, at)
−
|
ξt+1). With the
H(Θ
ξt, at)
ξt, at, st+1) = H(Θ
|
|
|
help of Equation (49), as derived following the deﬁnition
of conditional mutual
information, we derive in Equation
(50) that the conditional entropy difference is actually the
average information gain, which is equal to the conditional
ξt, at) between environmental
mutual information I(Θ, St+1|
parameter Θ and the new state St+1. Such a derivation is not
given in [4]. Based on Equation (50), an intrinsic reward
can be augmented from the environmental reward function,
thus the method could be incorporated with any existing
reinforcement learning algorithms for exploration, TRPO [2],
for example. Upon additional observation of action at and
state st+1 pair on top of trajectory history ξt, the posterior
on the distribution of the environmental parameter θ, p(θ
ξt),
|
ξt+1) in a Bayesian way as derived
could be updated to be p(θ
|

H(Θ

p(θ

−

||

|

p(θ

ξt+1) =
|

=

=

=

=

p(θ, ξt, at, st+1)
p(ξt, at, st+1)
p(st+1|

θ, ξt, at)p(θ, ξt, at)
p(ξt, at, st+1)
θ, ξt, at)p(θ, ξt, at)

at, ξt)

p(st+1|

p(st+1|

p(at, ξt)p(st+1|
θ, ξt, at)p(θ
|
at, ξt)
p(st+1|
θ, ξt, at)p(θ
at, ξt)

p(st+1|

|

p(st+1|

ξt, at)

ξt)

(51)

at, ξt)

p(st+1|
(cid:90)
=

p(st+1, θ

at, ξt)dθ
|
p(st+1, θ, at, ξt)
p(at, ξt)

dθ

Θ

(cid:90)

Θ

(cid:90)

Θ

(cid:90)

Θ

=

=

=

p(st+1|

θ, at, ξt)p(θ, at, ξt)

p(at, ξt)

dθ

p(st+1|

θ, at, ξt)p(θ

ξt)dθ
|

(52)

Fig. 8. Probabilistic Graphical Model For VIME

I(X; Y

Z) =

|

(cid:90)

x,y,z
(cid:90)

p(z)p(x, y

z) log
|

p(x, y

z)
|
z)p(y

p(x

|

z)
|

dxdydz

p(z)p(x, y

z) log p(x
|

z)dxdzdy+
|

x,y,z

=

+

−
(cid:90)

p(x, y, z) log p(x

x,y,z
= H(X

Z)

|

−

H(X

|

y, z)dxdzdy
|
Y, Z)

(49)

in Equation (51), which is ﬁrst proposed in [30]. In Equation
(51), the denominator can be written as Equation (52), so that
the dynamics of the environment modeled by neural network
θ, at, ξt), could be used. The last step of
weights θ, p(st+1|
Equation (52) makes use of p(θ
ξt, at) = p(θ

|
Since the integral in Equation (52) is not tractable, vari-
ational treatment over the neural network weights posterior
distribution p(θ
ξt) is used, characterized by variational pa-
|
rameter φ, as shown in the dotted line in Figure 8. The
variational posterior about the model parameter θ, updated at
each step, could than be used to calculate the intrinsic reward
in Equation (50).

ξt).
|

2) Variational Inference on hidden state posterior: In Vari-
ational State Tabulation (VaST) [5], the author assume the high
dimensional observed state to be represented by Observable
O, while the transition happens at
the latent state space
represented by S, which is ﬁnite and discrete. The author

ξt, at)
H(Θ
|
(cid:90)
=Eξt,at

ξt, at, st+1) = I(Θ, St+1|
H(Θ
|
−
ξt, at) log[
p(st+1, θ

ξt, at)

ξt, at)
|

p(st+1, θ
ξt)p(st+1|

|

ξt, at)

p(θ

]dθ

|

Θ,

S

(cid:90)

dst+1

=Eξt,at

Θ,

S

p(st+1|

ξt, at)p(θ

|

ξt+1) log[

=Eξt,atEp(st+1|

ξt,at)DKL(p(θ

ξt+1)
|

||

p(θ

|

p(θ

ξt+1)
|
p(θ
ξt)
|
ξt))

]dθdst+1

(50)

assume a factorized form of observation and latent space joint
probability, which we explicitly state in Equation (53).

p(O, S) = πθ0 (s0)

T
(cid:89)

t=0

pθR (ot|

st)

T
(cid:89)

t=1

st
pθT (st|

−

1, at

1)

−

(53)

Additionally, we characterize Equation (53) with the prob-
abilistic graphical model in Figure 9 which does not exist
in [5]. Compared to Figure 7, here the latent state S is in
discrete space instead of high dimension, and the observation
is a high dimensional image instead of binary variable to
indicate optimal action. By assuming a factorized form of the
variational posterior in Equation (54),

T
(cid:89)

q(S0:T |

O0:T ) =

qφ(St|

Ot

−

k:t)

(54)

t=0
The author assume the episode length to be T , and default
frame prior observation to be blank frames. The Evidence
Lower Bound (ELBO) of the observed trajectory of Equation
(53) could be easily represented by a Varitional AutoEncoder
[31] like architecture, where the encoder qφ, together with
the reparametrization trick [31], maps the observed state O
into parameters for the Con-crete distribution [32], so back-
probagation could be used on deterministic variables to update
the weight of the network based on the ELBO, which is
decomposed into different parts of the reconstruction losses
of the variational autoencoder like architecture. Like VIME
[4], VaSt could be combined with other reinforcement learning
algorithms. Here prioritized sweeping [12] is carried out on the

Aactt−1AacttSt−1StΘΦSt+1Aactt+1p(st|st−1,at−1;θ)[14] R. Zhao, X. Sun, and V. Tresp, “Maximum entropy-regularized multi-

goal reinforcement learning,” arXiv preprint arXiv:1905.08786, 2019.

[15] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra, “Planning and
acting in partially observable stochastic domains,” Artiﬁcial i ntelligence,
vol. 101, no. 1-2, pp. 99–134, 1998.

[16] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning
with double q-learning,” in Thirtieth AAAI conference on artiﬁcial
intelligence, 2016.

[17] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep reinforcement
learning,” arXiv preprint arXiv:1509.02971, 2015.

[18] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,

“Deterministic policy gradient algorithms,” 2014.

[19] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep rein-
forcement learning,” in International conference on machine learning,
2016, pp. 1928–1937.

[20] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-
imal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,
2017.

[21] T. Schaul, D. Horgan, K. Gregor, and D. Silver, “Universal value func-
tion approximators,” in International Conference on Machine Learning,
2015, pp. 1312–1320.

[22] N. Kushwaha, X. Sun, B. Singh, and O. Vyas, “A lesson learned from
pmf based approach for semantic recommender system,” Journal of
Intelligent Information Systems, vol. 50, no. 3, pp. 441–453, 2018.
[23] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder,
B. McGrew, J. Tobin, O. P. Abbeel, and W. Zaremba, “Hindsight expe-
rience replay,” in Advances in Neural Information Processing Systems,
2017, pp. 5048–5058.

[24] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience

replay,” arXiv preprint arXiv:1511.05952, 2015.

[25] X. Sun, A. Bommert, F. Pﬁsterer, J. Rahnenf¨uhrer, M. Lang, and
B. Bischl, “High dimensional restrictive federated model selection with
multi-objective bayesian optimization over shifted distributions,” arXiv
preprint arXiv:1902.08999, 2019.

[26] S. Kakade and J. Langford, “Approximately optimal approximate rein-

forcement learning,” in ICML, vol. 2, 2002, pp. 267–274.

[27] B. Sallans and G. E. Hinton, “Reinforcement learning with factored
states and actions,” Journal of Machine Learning Research, vol. 5, no.
Aug, pp. 1063–1088, 2004.

[28] T. Haarnoja, H. Tang, P. Abbeel, and S. Levine, “Reinforcement learning
with deep energy-based policies,” in Proceedings of the 34th Interna-
tional Conference on Machine Learning-Volume 70.
JMLR. org, 2017,
pp. 1352–1361.

[29] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic
actor,” arXiv preprint arXiv:1801.01290, 2018.

[30] Y. Sun, F. Gomez, and J. Schmidhuber, “Planning to be surprised: Op-
timal bayesian exploration in dynamic environments,” in International
Conference on Artiﬁcial General Intelligence. Springer, 2011, pp. 41–
51.

[31] C. Doersch, “Tutorial on variational autoencoders,” arXiv preprint

arXiv:1606.05908, 2016.

[32] C. J. Maddison, A. Mnih, and Y. W. Teh, “The concrete distribution:
A continuous relaxation of discrete random variables,” arXiv preprint
arXiv:1611.00712, 2016.

Fig. 9. Graphical Model for Variation State Tabulation

Heviside activation of the encoder output directly, by counting
the transition frequency, instead of waiting for the slowly
1) in
learned environmental transition model pθT (st|
st
Equation (53). A potential problem of doing so is aliasing
between latent state s and observed state o. To alleviate
this problem, in [5], the author actively relabel the transition
history in the replay memory once found the observable has
been assigned a different latent discrete state.

1, at

−

−

IV. CONCLUSION

As a tutorial survey, we recap Reinforcement Learning with
Probabilistic Graphical Models, summarizes recent advances
of Deep Reinforcement Learning and offer a taxonomy of
Probabilistic Graphical Model and Variational Inference in
DRL with detailed derivations which are not included in the
original contributions.

REFERENCES

[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski
et al., “Human-level control
learning,”
Nature, vol. 518, no. 7540, p. 529, 2015.

through deep reinforcement

[2] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust
region policy optimization,” in International conference on machine
learning, 2015, pp. 1889–1897.

[3] X. Sun, J. Lin, and B. Bischl, “Reinbo: Machine learning pipeline search
and conﬁguration with bayesian optimization embedded reinforcement
learning,” 2019.

[4] R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. De Turck, and
P. Abbeel, “Vime: Variational information maximizing exploration,” in
Advances in Neural Information Processing Systems, 2016, pp. 1109–
1117.

[5] D. Corneil, W. Gerstner, and J. Brea, “Efﬁcient model-based deep
reinforcement learning with variational state tabulation,” arXiv preprint
arXiv:1802.04325, 2018.

[6] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe, “Variational inference:
A review for statisticians,” Journal of the American Statistical Associa-
tion, vol. 112, no. 518, pp. 859–877, 2017.

[7] C. M. Bishop, Pattern recognition and machine learning.

springer,

2006.

[8] A. Fischer and C. Igel, “Training restricted boltzmann machines: An

introduction,” Pattern Recognition, vol. 47, pp. 25–39, 01 2014.

[9] X. Sun, A. Gossmann, Y. Wang, and B. Bischl, “Variational resampling
based assessment of deep neural networks under distribution shift,” 2019.
[10] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, “Weight
uncertainty in neural networks,” arXiv preprint arXiv:1505.05424, 2015.
[11] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv

preprint arXiv:1312.6114, 2013.

[12] R. S. Sutton, A. G. Barto et al., Introduction to reinforcement learning.

MIT press Cambridge, 1998, vol. 2, no. 4.

[13] S. Levine, “Reinforcement learning and control as probabilistic infer-
ence: Tutorial and review,” arXiv preprint arXiv:1805.00909, 2018.

Aactt−1Ot−1OtOt+1AacttSt−1StSt+1Aactt+1p(st+1|st,at)