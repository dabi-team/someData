2
2
0
2

p
e
S
2

]

G
L
.
s
c
[

1
v
3
8
0
1
0
.
9
0
2
2
:
v
i
X
r
a

When Bioprocess Engineering Meets Machine Learning:
A Survey from the Perspective of Automated
Bioprocess Development

Nghia Duong-Trunga, Stefan Borna, Jong Woo Kima, Marie-Therese
Schermeyera, Katharina Paulicka, Maxim Borisyaka, Ernesto Martineza,
Mariano Nicolas Cruz-Bournazoua, Thorben Wernerb, Randolf Scholzb, Lars
Schmidt-Thiemeb, Peter Neubauera,∗

aTechnische Universit¨at Berlin, Faculty III Process Sciences, Institute of Biotechnology,
Chair of Applied and Molecular Microbiology.
Straße des 17. Juni 135, 10623 Berlin, Germany.
bUniversity of Hildesheim, Information Systems and Machine Learning Lab (ISMLL).
Universit¨atspl. 1, 31141 Hildesheim, Germany.

Abstract

Machine learning (ML) has signiﬁcantly contributed to the development of bio-

process engineering, but its application is still limited, hampering the enormous

potential for bioprocess automation. ML for model building automation can

be seen as a way of introducing another level of abstraction to focus expert

humans in the most cognitive tasks of bioprocess development. First, proba-

bilistic programming is used for the autonomous building of predictive models.

Second, machine learning automatically assesses alternative decisions by plan-

ning experiments to test hypotheses and conducting investigations to gather

informative data that focus on model selection based on the uncertainty of

model predictions. This review provides a comprehensive overview of ML-based

automation in bioprocess development. On the one hand, the biotech and bio-

engineering community should be aware of the potential and, most importantly,

the limitation of existing ML solutions for their application in biotechnology

and biopharma. On the other hand, it is essential to identify the missing links

to enable the easy implementation of ML and Artiﬁcial Intelligence (AI) solu-

∗Corresponding author: Peter Neubauer

Preprint submitted to Biochemical Engineering Journal

September 5, 2022

 
 
 
 
 
 
tions in valuable solutions for the bio-community. We summarize recent ML

implementation across several important subﬁelds of bioprocess systems and

raise two crucial challenges remaining the bottleneck of bioprocess automation

and reducing uncertainty in biotechnology development. There is no one-ﬁts-all

procedure; however, this review should help identify the potential automation

combining biotechnology and ML domains.

Keywords: Active Learning, Automation, Bioprocess Development,

Reinforcement Learning, Transfer Learning.

1. Introduction

Modern biolabs have automatized and parallelized many tasks, such that up

to thousands of experiments can run daily. These robotic experimental facili-

ties equipped with Liquid Handling Stations (LHS) [1, 2], parallel cultivation

systems, and High Throughput (HT) [3, 4] analytical devices are capable of

generating extensive data in a short time. The past decade’s focus was on hard-

ware development and integration with fairly simple data management tools

and rarely having advanced control and experimental design tools.

In other

words, despite the fast data generation, the current bottleneck - automated

data treatment, curation, and model building - remains unsolved. We have not

been able to trigger the fruitful symbiosis expected between i. robots that can

perform thousands of complex tasks but are waiting for humans to design the

experiments, ii. Active Learning (AL) algorithms remain for humans to achieve

the planned experiments, and iii. Machine Learning (ML) tools are waiting for

humans to treat and deliver the data in a machine-readable form. Decisions

must be taken under considerable high uncertainty, enabling a transition from

laboratory to industrial production at scale to advance fast in bioprocess devel-

opment. Transferring results from small to large scale represents a central chal-

lenge and is very time-consuming. Miniaturized and versatile multi-bioreactor

systems combined with LHS have the potential to signiﬁcantly contribute to

the rapid and successful upscaling of biotechnological processes from laboratory

2

Figure 1: Reducing uncertainty in the development.

scale to industrial production facilities with reproducible levels of productivity.

To drastically speed up the bioprocess development of innovative products, the

ubiquitous use of automation in active learning from data and model building

must be introduced in all stages from product conceptualization to reproducible

end-use properties. At any of these development stages, problem-solving and

decision-making require building a model with enough predictive capability. In

today’s practice, model building and data collection depend heavily on manual

tweaking and human intervention, which slows down the development eﬀort and

constitute a signiﬁcant obstacle to lower costs and shorter times to market.

1.1. Decisions and Models

As shown in Fig. 1, model building is an important activity to assess alter-

natives and advance fast in the bioprocess lifecycle by making rational decisions

that systematically reduce uncertainty. Model-based decision-making is widely

used in the development lifecycle of diﬀerent types of processes and products

(e.g., electrical, chemical, aeronautics) for cost-eﬀective design and improved

operation in the face of uncertainty. Mainly due to the so-called ”small data

problem,” bioprocess development has been an exception, though, with a signiﬁ-

cantly higher degree of empiric procedures, expert-based decisions, and strongly

3

segmented design strategies and strain screening methods. The increased com-

plexity of living organisms with thousands of intracellular biochemical reactions

and uncomprehended responses in its metabolic activity, combined with the dif-

ﬁculties in obtaining trustworthy observations, makes it very diﬃcult to build

sound mathematical models since data collected from many biological systems

are inherently scarce and low-dimensional. However, similar dynamic behaviors

within families of genetically modiﬁed microorganisms make enough room for

transfer learning, using available data to build predictive models for a new, un-

seen mutant. Based on this prior knowledge, experiments can be designed to

gather informative data to increase the predictive power.

1.2. Automated Model Building

Automation of the model building cycle aims to assist experts and scientists

in facilitating and transforming the decision-making in the context of biopro-

cess and biotechnology, not replacing them. Some aspects of model building are

more diﬃcult to automate because of technological challenges and because they

involve open-ended questions and context-dependent tasks that require human

intervention. Perhaps the most diﬃcult challenge to model building automation

is that data sources in the development pipeline are diverse, distributed, and

multi-structured. Moreover, not only the available data is highly heterogeneous,

but it also may be not informative regarding the purpose of the model for a given

stage. It contrasts sharply with the common assumption that suﬃcient amounts

of high-quality data are available for model building. Data collection methods

for bioprocess development are descriptive of an inherently dynamic behavior

since many process parameters are gathered online as continuous measurements.

They are available and highly dependent on the quality of sensor devices, stan-

dard operating procedures, material and methods used, frequency of measure-

ments (e.g., temperature, optical density, pH, oxygen, glucose, oxygen uptake

rate, stirring speed), analytical sample processing, calibration parameters, etc.

Then, the optimal design of experiments for gathering information must be an

integral part of the model building cycle. As a result, the model building at

4

the diﬀerent stages of bioprocess development will comprise automated proce-

dures for actively seeking or generating highly informative data regarding the

objective and type of decisions that must be taken in each stage in Fig. 1.

Faced with the choice of a large set of machine learning algorithms and an

even larger space of hyperparameter settings, even experts often must resort to

costly experimentation in time and money to determine what combination works

best for a given problem. Hence, automated model-building approaches must be

necessarily end-to-end. This not only spares non-experts the time and eﬀort of

extensive, often onerous trial-and-error experimentation but also enables biopro-

cess engineers to obtain substantially better performance with fewer data and

faster than possible without automation. In some applications of reinforcement

learning to control and optimize, close-loop experimentation must be part and

parcel of the model building cycle, making automation even more important.

Hyperparameters, in this case, drive learning and deﬁne which data is gathered

in the learning curve. Without a meta-learning level in model building, the ini-

tial setting of hyperparameters can easily prevent learning a predictive model

that can make a real diﬀerence without using a model at all for making decisions.

The use of machine learning for model building automation can be seen as a

way of introducing another level of abstraction to focus expert humans in the

most cognitive tasks of bioprocess development. First, probabilistic program-

ming is used for the autonomous building of predictive models. Second, machine

learning automatically assesses alternative decisions by planning experiments to

test hypotheses and then conducting investigations to gather informative data

that focus on model selection based on the uncertainty of model predictions.

Therefore, machine learning methods can be seen as meta-algorithms for model-

building tasks and automate data generation and hypothesis testing. Finally,

the automated model building uses algorithms that select and conﬁgure machine

learning algorithms. That is, meta-meta-algorithms that can be understood as

Bayesian machine experimenters that generate new data to transform a priori

knowledge into rational decisions that further bioprocess development.

5

1.3. Present State of Data and Models in Bioprocess Development

At the initial stages of development, fundamental problems are addressed,

such as strain screening which involves testing their robustness to alternative

operating conditions, cultivation media, and bioreactor designs. The availabil-

ity of Process Analytical Tools (PAT) [5, 6, 7] allows a deeper understanding

of the processes and the technological advances of HT and LHS in robotic plat-

forms [8, 9, 10] that can generate large amounts of experimental data to feed

the model building cycle. Yet, the bottleneck step of human-in-the-loop pre-

vents a rapid transition toward design and operating decisions at larger scales.

Thus, there exists an essential missing link toward model-based bioprocess sys-

tems engineering [11]: the conversion of automated experimental tasks and data

(e.g., cultivation, sampling, analytics) into knowledge expressed in mathematical

expressions. The large amounts of heterogeneous low quality data make man-

ual treatment and model development almost impossible. Hence, automating

model building using machine learning methods is envisioned as the alternative

of choice to speed up bioprocess development while providing a setting for prove-

nance and reproducibility to transform HT experimental data into information,

information into knowledge, and to use this knowledge to understand, control,

and optimize the bioprocess in its entire lifecycle.

As the developmental stages are more concerned with design and operations

decisions, the model building should focus on guaranteeing physiological con-

ditions that maximize productivity and product quality. For example, in the

fed-batch cultivation phase, both overfeeding and underfeeding yield inferior re-

sults in cell growth and product formation [12]. Several studies have resorted

to mechanistic models for (re)designing HT experiments of several fed-batch

mini-bioreactors. The main challenges which are pending to be addressed are

i) the use of impulsive control systems due to bolus-feeding for a miniaturized

system, ii) ill-conditioned parameter estimation, and iii) low predictive power

of mechanical models. In the work of [13, 9, 14], optimal experimental design

problems were studied to maximize the information content for eﬀective iden-

tiﬁcation of a mechanistic model. Model predictive control using a mechanical

6

model to maximize cell growth was implemented to an in silico system [15] and

validated using an HT experiment [16]. However, due to its imperfect structure,

a mechanical model cannot solely signiﬁcantly reduce the uncertainty related to

operating and design decisions at more advanced stages. The latest trend clearly

shows that machine learning techniques may give more room for more eﬃcient

utilization of available data and automate the generation of highly informative

new data. Machine learning is a viable and eﬀective solution to the preced-

ing problems. Over the past few decades, bioprocessing has seen a signiﬁcant

shift from manual modeling to data-driven modeling, e.g., applying machine

learning, partly due to a large amount of existing data for biological systems

[17, 18, 19]. It is an essential premise for integrating machine learning models

into well-informed bio-data, creating great potential for discovering advances in

the ﬁeld of bioprocessing [20, 21, 22]. Machine learning is also enhanced through

a wide range of research in mathematical foundations, modeling, and computa-

tional hardware. Besides, machine learning makes it easier for experts in other

ﬁelds to use. They do not need to spend time understanding the principles for

each model but can conveniently call the model on a speciﬁc data type. This

fascinating feature has attracted many bioengineering experts and encouraged

the application of machine learning to biological processes that are considered

among the most complex in the world. In fact, machine learning has proven

its eﬀectiveness in many areas of biology: 3D structure of proteins [23, 24], up-

downstream processes [25, 26, 27, 28], bioprocessing for chemical and biologic

product manufacturing [19, 29], enzymes and cell growth [30, 31, 32], cell culture

expression systems [33, 34, 35], and many others.

According to [36], machine learning was not as extensively used for biopro-

cess development as it had been anticipated compared to the big-data domains

such as vision, natural language processing, and speech recognition. A plau-

sible explanation would be that this drastic increase can only be achieved by

automating the model building cycle at all developmental stages due to the com-

plexity of machine learning methods and the number of hyperparameters that

must be chosen. The selection of the most eﬃcient algorithm and its parameters

7

is based on many factors, including the transformation from a bioprocessing en-

gineering problem into machine learning tasks, the quantity and quality of the

data collected, the availability of the data labels, the type of problem being

solved (regression, classiﬁcation, forecasting, control, etc.), the required over-

all accuracy and performance, availability of prior bioprocessing knowledge to

control the hyperparameters tuning [37, 38]. As a result, a key challenge for

model building automation is integrating a meta-learning layer for setting all

hyper-parameters using techniques such as Bayesian optimization, which can

take full advantage of cumulative data in the bioprocess lifecycle systemically

reduce uncertainty.

.

2. Elucidation of Machine Learning Strategies

2.1. Key Concepts

2.1.1. Brief Deﬁnition of Machine Learning in the Context of Bioprocess Engi-

neering

Machine learning is a subﬁeld of computer science that focuses on build-

ing applicable models and depends on training upon a collection of observa-

tions. These observations can come from simulation, nature, and other machine

learning models. The concentration of machine learning can be deﬁned as the

process of addressing and solving any particular problems by collecting enough

datasets and algorithm-based training a statistical approach to the collected

dataset. Machine learning is further subdivided into supervised, unsupervised,

semi-supervised, and reinforcement learning; see an illustration in Figure 2. In

this review, we do not intend to cover all theories of these mentioned subdi-

visions but rather concentrate on the most applicably principled explanations,

mainly supervised learning and reinforcement, to the problem in biochemical

engineering. Interested readers can refer to many complimentary references on

machine learning domains [39, 40, 41, 42].

8

Figure 2: Sub-ﬁelds of machine learning.

Let’s start by denoting a one-dimensional array x ∈ RD and a scalar y ∈ R

as one observation/sample/feature vector and its label/target respectively. We

denote a mapping function f : RD → R. In supervised learning, people work

with a collections of these pairs, e.g. dataset, {(x1, y1), (x2, y2), . . . , (xN , yN )}

where N is the total of data points. The collection of datasets is further divided

into a training set, a validation set, and a test set. Each xi is (x1

i , . . . , xD
i )
an ordered and indexed sequence of values where D is the dimensionality and

i , x2

each j ∈ {1, . . . , D} is called a feature. All observations in the datasets contain

the same feature at each index j. The label scalar y can be either a real number

or belong to a ﬁnite set of classes {1, 2, . . . , C}. In a complex task, y can be a

vector, a matrix, or any complex structure. For example, x is the feature vector

of oxygen, pH, glucose, and stirring speed of a bioreactor, y is the amount

of produced biomass at a particular time, e.g., the regression task. Or y is

9

either increasing or decreasing trends at a given time, e.g., the classiﬁcation

problem. The goal of any supervised learning model is automatically to use the
collection of x and y to learn corresponding parameters ˆθ where they can achieve

a prediction ˆy as close to y as possible. We measure the diﬀerence between ˆy

and y by an asymmetric loss function (cid:96)(y, ˆy). Then we can deﬁne the average

loss of all predictions, or empirical risk, on the training set as follows.

L def=

1
N

N
(cid:88)

i=1

(cid:96)(yi, f (xi, ˆθ))

(1)

where

ˆθ = argmin

L(θ) = argmin

θ

θ

1
N

N
(cid:88)

i=1

(cid:96)(yi, f (xi, θ))

(2)

where θ can be randomly initialized and eventually updated by the model during
training. Hence, ﬁtting a model and ﬁnding ˆθ in Equation 2 to minimize the

empirical risk in Equation 1. However, the true goal is to minimize the empirical

loss on unseen and future data that the model has not been trained. And it is

called generalization.

Another critical input of any machine learning model is tunable hyperpa-

rameters φ [43, 44] that have a signiﬁcant impact on the performance of the

model. These values are not derived from the training data and can, there-

fore, not be learned by the model. For example, early stopping rounds is a

hyperparameter in XGBoost [45, 46] model that indicates the number of rounds

without improvements after which we should stop training. Hyperparameters

can also derive from machine learning pipelines, such as the choice of imputation

technique on the missing data.

In machine learning, a baseline is any simple algorithm, with or without

learnable parameters, for solving a task, usually based on a heuristic experience,

randomization, or elementary summary statistics [47, 48]. It is an important

reminder to tackle new domains with machine learning techniques, such as bio-

engineering and bioprocessing. Before attempting to develop more sophisticated

models, obtaining existing simple baselines is equally important. For example,

10

if your problem is ﬁnal biomass prediction, you can pick a regression baseline

and measure its performance. This baseline performance will then become what

we compare to any future, more sophisticated approach.

A bioengineering expert should consider one essential point when applying

models: data partitioning. Given a dataset X ∈ Rn×m where n and m are the

number of rows and columns, respectively, and the equivalent target Y ∈ Rn, it

does not have a correct training procedure when someone trains a model, op-

timizes hyperparameters, and evaluates the model’s performance on X and Y .

We typically divide three disjoint sets of X and Y into training, validation, and

test sets. We utilize the training set to train the machine learning algorithm.

The validation set is needed to search for the best values of hyperparameters.

The engineer tries various combinations of hyperparameters in an experimental

space one by one, trains the model upon each variety, and notes the best perfor-

mance on the validation set. Those values that maximize the model performance

are then used to make predictions on the test set. The test’s performance is

the reported results where we judge the ﬁnal algorithm’s achievement. Some

references refer to validation and test partitions as holdout sets that should con-

tain observations that do not appear in the training set. To obtain an excellent

practical data partitioning into three nonunion parts, we should satisfy several

conditions as (i) do the partitioning on the raw data before everything else, (ii),

if applicable, randomly shuﬄe data before partitioning, (iii) validation and test

sets should be equivalent in the number of samples and distribution, and (iv)

avoid data leakage that no sample appears in more than one set [49, 50]. There

is no ideal ratio of partitioning. A good recommended practice is 70%-15%-15%

or 80%-10%-10% for training, validation, and test sets.

2.1.2. When to Use Machine Learning?

Machine learning is ubiquitous across all the ﬁelds that drive discovery in

science. However, it is not a powerful tool for solving all practical problems.

Many of the approaches are so complicated, or heavy-computing assumptions

that it is impossible or expensive if they are applied in the wrong contexts [51, 52]

11

which might lead to enormous wasted scientiﬁc eﬀorts, invested time and money

but receive in high risk of misinterpretations, over-optimistic results, error-prone

analyses and the illusion of successful conclusions. Some best practices and

important points to notice when biologists are embarking on bio-experiments

demanding machine learning can be found in [53]. We should consider deploying

and investing in machine learning in one of the following situations.

When it is cost-eﬀective. The cost factors of bioprocess engineering derive from

development, facilities, and operations [54, 55] which drive fundamental con-

sideration for investment choices. From the machine learning perspective, the

costs sum up from data collection, e.g., labor work, simulation data generation,

preparing and cleaning data, training the models, e.g., the expensive hardware

or cloud computing needed to train deep models, and building and running the

infrastructure to deploy and monitor the complete machine learning project life

cycle which includes further continuous monitoring the model, collecting new

data and keeping the model up to date.

When the problem has a simple objective. As we mentioned in 2.1.1, target y

can be a real number, a class, or a vector at a particular time, depending on

a practical problem. We can convey and transform bioprocess engineering and

formulate it as a machine learning problem with a concrete objective. Note

that a machine learning model can only work if y is consistent to a speciﬁc

during training and inference afterward. For example, y is a real number while

deploying the model and training on a training set.

It should also be a real

number when we generalize it to a test set or other data source. Obviously, we

build as many models for bioprocess modeling and control problems.

When the data is perceptive. It is laborious to imagine someone trying to classify

an image, video, speech, or text data without using machine learning. Consider

detection of Acute lymphoblastic leukemia, a type of blood cell cancer charac-

terized by the development of immature lymphocytes, from microscope images.

In the past, bio-engineers and bio-experts tried to solve the image detection by

12

applying handcrafted region notation to speciﬁed patches of pixels [56], equip-

ping them with intensive prior knowledge of the domain. It was laborious work

and error-prone detection where the reproducibility is impracticable to do con-

veniently.

2.1.3. Machine Learning Project Life Cycle

Machine learning is not simply a decision tree or a neural network model

but is often implemented as a process containing chained stages. We usually

begin with the data, applying data cleaning techniques, transformation, train-

ing/validation/test division, then using machine learning models. Next, we

evaluate their performance, deployment, monitoring, and maintenance. The hy-

perparameters of the entire process are often optimized based on existing data

and model knowledge. The whole process can be implemented and used for

predictions [57, 58, 59, 60]. A bioprocess-based machine learning project begins

with understanding a bioengineering problem that may or may not equip with

a machine learning part. It might also contain one or more diﬀerent machine

learning models if we can break down that bioengineering project into various

sub-tasks [61]. Once an engineering project is deﬁned, we transform it into a

speciﬁc machine learning problem, e.g., unsupervised, supervised, and reinforce-

ment learning. The result of applying machine learning should be materialized

into a deterministic predictive model, i.e., what should be the input and output

of the whole process? Bioengineering experts can investigate a machine learning

model as a black box. They do not care about algorithm design but the input

data, output results, and hyperparameters, if any. Note that the goal of machine

learning is not necessarily the same as the goal of biological engineering. For

example, a machine learning arm predicts ﬁnal biomass before induction in a

one-liter bioreactor setting and experimental conﬁguration. We optimize a loss,

e.g., root mean squared error, between the ground truth and predicted output

where any proposed models outperform baselines indicated by a low standard

deviation and high certainty. However, in this example, the goal of bioengineer-

ing might be how much labor work, monetary investment, and data collection

13

help the machine learning approach achieve its forecasting goal. In general, the

life cycle of a machine learning project, illustrated in Figure 3, consists of the

following stages: 1) bioengineering problem and its equivalent machine learning

tasks, 2) data engineering, e.g., data collection and preparation, feature engi-

neering, 3) machine learning engineering, e.g., model training, model evaluation

and tuning, model deployment, 4) machine learning in production, e.g., model

serving, model monitoring, and maintenance [62, 63, 64].

Figure 3: Machine learning bioengineering project life cycle.

2.2. Active Learning

2.2.1. What is Active Learning?

ML models are usually trained on large corpora of data created by a poten-

tially unknown process. As stated in Section 2.1.1, supervised machine learning

requires annotations for every data point in these corpora of data. Many real-

world problems do not have a fully annotated dataset that can be utilized for

ML since the annotation process is time-consuming and often requires human

domain experts. This forces ML practitioners to select which data points are

to be annotated, resulting in a reduced subset of the available data for model

training. This process is called Active Learning (AL). The same procedure can

be applied to newly acquired data that was not part of the original training set

and require complex simulation or expensive laboratory experiments.

14

The active learning task to query new data beneﬁcially can be seen as a

generalization of the classical problem of optimal experimental design (OED).

Experimental designs can be chosen optimally for diﬀerent purposes, e.g., to

discriminate model hypotheses, estimate model parameters, or predict at speciﬁc

points.

Since active learning methods are incremental (selecting the next data point

based on the current labelled data and model), they often require a so-called

seed set. This is a small set of labelled data (xi, yi)i∈Il used to train the initial

model and calibrate the AL method. For a graphical overview of the AL cycle

see Figure 4. The labelled set is initially comprised of the seed set. Each cycle

adds one or more datapoints to the labelled pool.

Figure 4: The active learning cycle. The seed set contains a small number of labelled samples.

Each cycle adds one or more datapoints to the labelled pool. Repeated until some stopping

criterion is met (usually a budget constraint or performance threshold).

15

2.2.2. Diﬀerent Sampling Scenarii

Active learning is generally data and model agnostic. It has been applied to

many diﬀerent combinations of data modalities and model archetypes. Diﬀerent

methods are categorized by the sampling algorithm that selects the data points

from the unlabeled pool.

Active learning distinguishes three scenarios:

Stream-based Selective Sampling Data xi is presented in a stream, e.g.,

images arriving from a camera, and a cost is incurred for acquiring target

values yi, e.g., labels by a human expert. The active learning algorithm

has to decide on a case-to-case basis if a sample is to be labelled or not.

Pool-based Sampling A large pool (or a subset thereof) of unlabelled in-

stances (xi)i∈Iu is given. The AL algorithm has to pick one or more data

points from the pool, which are to be labelled yi.

Query Synthesis The AL algorithm uses the current labelled data (xi, yi)I to

synthesize new cases xi for which the target value should be queried. This

does not rely on presented data points as in the previous two scenarios

but creates a new query point. This new point might correspond, i.e., to

an actual world experiment described by xi, whose outcome becomes the

annotation yi.

For all three sampling scenarios, there are potential applications in chemical

engineering and bio-engineering:

Whenever Machine Learning is applied to a new problem, the existing unlabeled

data needs to be annotated. Pool-based AL can select a small subset of data

points to be annotated, resulting in a robust model with signiﬁcantly reduced

annotation costs.

For tasks like anomaly detection in processes, one can utilize a pool of legacy

data with annotations of anomality and an incoming stream of new data without

annotation. Stream-based AL would choose those new data points for which the

anomaly detection model is unsure and therefore would beneﬁt from supervision

16

by an expert.

The query synthesis case is arguably the most important in this context. The

constructed queries could correspond to new experiments designed to build a

robust predictive model.

One should be aware that the distribution of the queries can be expected

to diﬀer from the distribution of the ordinary data-generating process. If, for

example, AL/OED is used to optimize a bioreactor model for later use in the

reactor’s control, it is entirely possible that the regular operating regime is dif-

ferent from the data distribution that creates a strong predictive model. The

assumption is that the created dataset, might it be skewed or not, contains all

relevant use cases of the problem resulting in a model that generalizes nonethe-

less.

2.2.3. Querying the Most Informative Data

A good intuition is that AL and OED will query the most informative data

for the purpose at hand, although not all algorithms deﬁne ”most informative”

in the same way.

The expected information gain (EIG), which is the expected reduction of

entropy by the queries, is an ideal Bayesian utility function for AL to optimize,

however estimating the EIG is computationally very challenging. Most of the

AL methods below use a diﬀerent objective, but a uniﬁed view ([65]) is possible,

which explains their relation to the EIG. All the following methods use diﬀerent

proxies of EIG to select ’informative’ queries.

Uncertainty Sampling Beginning with [66] this is a widely used class of

methods with diﬀerent underlying estimators of uncertainty. While work-

ing well in some instances, such algorithms can over-sample regions of the

space X where noise dominates.

The most prevalent measurement of uncertainty is the Shannon entropy

applied to the output of a classiﬁcation model (see Fig. 5 (c)).

If the

model assigns a high probability to one class and low probabilities to all

others, the entropy is between those probabilities is low.

If the model

17

assigns an equal probability to all classes (the model is uncertain) the

entropy is high. A sample is considered informative if it produces high

Figure 5: Diﬀerent uncertainty measures applied to a 3-way classiﬁcation problem (Ref. [67]

Fig. 5). Each corner represents one class. Each point within the triangle represents the pre-

diction of a model given an arbitrary datapoint. Each point indicates the assigned probability

to each class by its position. The colour indicates the amount of estimated uncertainty, where

red and blue indicates high and low uncertainty respectively.

entropy in the model’s output.

Reducing the version space Several approaches can be described as reduc-

ing the space of hypotheses compatible with the data, the so-called version

space. These approaches maintain an ensemble of many models rather

than just one. Each model represents one hypothesis about the available

data (see Fig. 6). An informative sample is considered one that produces

high disagreement between the hypotheses/models, forcing wrong models

to be dropped or updated. Repeating this process will push all models of

the ensemble to converge to the ”true” hypothesis. Algorithms that fall

in this class are Query by committee and Query by disagreement. This

method was implemented by [68] and applied to image classiﬁcation tasks.

Expected error reduction Another proxy of EIG is an estimate of the ex-

pected error after seeing a new query. Since all ML approaches rely on

some error function to optimize their models, one can estimate the ex-

pected improvement in these functions given a selected data point from

the unlabeled set. Points associated with a more considerable error reduc-

18

Figure 6: Diﬀerent hypotheses of classiﬁcation models for two types of classiﬁers (Ref.

[67]

Fig. 6). Each line or box respectively represents one correct hypothesis about the given

data. An informative sample would be any new point that contradicts one or more of these

hypotheses.

tion are considered more informative. This method has been successfully

applied in classiﬁcation ([69]). Since estimating the error reduction is com-

putationally very expensive, [70] trained a regression model to predict the

error reduction and applied it to 3D Electron Microscopy (Striatum) and

MRI brain scans (BRATS).

Variance reduction That is where the classical ’alphabetical’ frequentist OED

criteria can be placed.

In maximum likelihood estimation, the covari-

ance of the parameter estimates is bounded below by the inverse of the

Fisher information matrix (Cram´er-Rao). Diﬀerent criteria that control

the eigenvalues of the Fisher information are used to ﬁnd an optimal design

(D-optimal determinant, A-optimal trace, etc.). Controlling the variance

of the parameter estimates has an impact on the predictive variance of the

model but is still a diﬀerent problem. A-optimal design, however, implies

minimizing a lower bound on the predictive variance. Calculating and in-

verting the Fisher information matrics for all parameters of a large ANN

is prohibitive, but recently this approach has been applied to just the last

layer of an ANN [71]. It should be noted the Cr´amer-Rao lower bound

may underestimate the true variance, and that the variance can be a poor

descriptor for non-Gaussian distributions.

19

Minimizing the EIG A direct estimation of the EIG has usually been consid-

ered an intractable problem, but recently useful (sharp) upper and vari-

ational lower bounds have been discovered and exploited for Bayesian

Optimal Experimental Design (BOED) ([72],[73],[74], [75], [76], [77]).

2.2.4. Learning How to Active(ly) Learn

There are three issues to raise with the previously mentioned methods.

(i) Most design criteria, even the theoretically sound ones, do not directly im-

prove the utility of the predictions for the ﬁnal purpose.

Increasing a model’s information content or generalization capabilities is excel-

lent, but the exact relation to a speciﬁc prediction task or decision problem

is not apparent. Therefore it would be advisable to learn an active learning

strategy from data for the ﬁnal task, end-to-end. It directly connects a model’s

performance on a given task to the selected queries.

(ii) If a new set of queries or experimental designs are selected each time, a

complex nonlinear optimization problem has to be solved. This might require

more time. In a real-time setting where, i.e., experiments have to be redesigned

based on new incoming data, the sampling process also needs to be fast.

(iii) All methods discussed so far rely on heuristics to select their samples. These

heuristics only use a limited subset of the available information and do so in a

static fashion that does not adapt to the presented data.

All three issues can be addressed when recasting the problem as reinforcement

learning (RL) by parameterizing a policy that selects the samples. This pol-

icy is learned on previously acquired records of selected samples and improves

model performance. Crucially, the application of the policy to new data is swift

(solving (ii)). In this setting, the policy is trained end-to-end concerning the

ﬁnal use of the prediction model, thus avoiding a possible mismatch between the

optimization objective and the ﬁnal use case (solving (i)). Finally, the policy is

usually represented by an ANN, so it can incorporate large quantities of infor-

mation and dynamically learn how to utilize them (solving (iii)). This includes

summary statistics about the unlabeled pool ([78]) or additional information

20

about the model prediction and conﬁdence ([79]).

This makes ’learning to actively learn’ one the most promising approaches

for AL [79, 80, 78], [81]. Some of the recent BOED methods mentioned above

([72], [77]) are policy based, too.

If no suﬃcient legacy records of selected samples and improvement in model

performance are available, one needs to employ more complex reinforcement

learning approaches. The authors of [78] use model-based RL to solve the prob-

lem. The agent is primarily trained within a simulation of the AL process and

further improved on the limited real-world data.

The interested reader can refer to section 2.4 or directly to [82] for an intro-

duction into reinforcement learning.

2.2.5. A Special Case

All previously described AL methods are done by analytical and probabilistic

models. However, there are also discrete problems amenable to logical analysis

in the application domain of this article. The most prominent example is Adam

the Robot Scientist [83], an automatic system that designs experiments to de-

termine the gene function of yeast using deletion mutations and auxotrophic

growth experiments. The active learning strategy of Adam can be formally un-

derstood along the previously sketched lines as reducing the hypothesis space

by minimizing a probabilistic objective function (expected cost [84]). However,

the gene network to be uncovered is treated as a logical problem, and a central

ingredient of the algorithm is automatic logical reasoning. This is an interest-

ing case that recalls the ambiguous meaning of ’artiﬁcial intelligence, which can

refer to logical reasoning systems and to statistical learning models alike.

In real-world scenarios, logical reasoning on complex coded information and

statistical learning on collected data can both play a role, though the great

successes of machine learning of the latter kind have recently eclipsed the former.

21

2.2.6. Spotlight: Uncertainty Quantiﬁcation

This section aims to deepen our understanding of uncertainty sampling, as

it is the most straightforward and most used implementation of active learning.

As stated above, uncertainty sampling aims to measure the model’s conﬁdence

for a given prediction and uses this as a proxy for the EIG. The more uncertain

a model is, the more informative this sample is considered, and following that,

the more useful this sample will be when annotated. Figure 7 compares diﬀerent

setups for uncertainty sampling with entropy as uncertainty measure. We will

consider a 3-way classiﬁcation problem so that the model will assign one proba-

bility for each class (subﬁgure (a)). The classic uncertainty sampling will com-

pute the entropy across the three classes (subﬁgure (b)). Query-by-Committee

(a) Model output

(b) Classic uncertainty sampling

(c) Uncertainty sampling with MC Dropout

(d) Classic uncertainty with ensembles

Figure 7: Comparison of diﬀerent setups for uncertainty sampling with entropy for a 3-way

classiﬁcation problem

was previously introduced as an alternative to uncertainty sampling since it was

derived from a diﬀerent theoretical motivation. However, the measurement of

uncertainty in both frameworks is very similar. Since Query-by-Committee al-

22

gorithms maintain an ensemble of many models, uncertainty can be measured

on a per-class basis (across models) rather than per model (subﬁgure (d)). To

assign a scalar value to each sample, the per-class uncertainties are usually

summed ([68]). Since maintaining and updating many ANNs is computation-

ally very expensive, some methods try to simulate an ensemble by using an

approach called Monte-Carlo Dropout (MC Dropout) ([85]). For MC Dropout,

only a single ANN with Dropout-Layers is trained. During prediction, where

a single forward pass with a dropout rate of 0 is usually done, MC Dropout

performs multiple forward passes with non-zero dropout, resulting in slightly

diﬀerent versions of the prediction. Treating each forward pass as a separate

model in an ensemble, the same Query-by-Committee algorithm can be applied

([86], [68]) (subﬁgure (c)).

2.3. Transfer Learning

In terms of ordinary perception, humans do not learn to perform tasks in-

dividually. We build our knowledge on the similar challenges we have learned

before and adapt it to new ones. Machine learning has also been developed to

mimic this human process, namely how to reuse knowledge we already learn,

particularly by parameter values, and start processing new data based on ex-

isting parameter values. This machine learning methodology is called transfer

learning [87, 88, 89, 90]. This way, machine learning models do not start learning

from scratch for previously handled problems. This has signiﬁcant implications

for bioprocessing techniques in that the data is incomplete for a single model

training and requires the machine learning model to continue training itself

when new data is available. It gives a considerable advantage over traditional

approaches that require training from scratch, are computationally expensive,

and demand large amounts of data per training to achieve high performance.

Transfer learning, however, attempts to change it by developing methods to

transfer knowledge learned in one or more source tasks and use it to improve

understanding in a related target task [91, 87, 92]. It allows the model to be

applied to datasets drawn from some distribution diﬀerent from the one upon

23

which it was trained [93, 94].

Given a training dataset Xtrain ∈ Rn×m and the equivalent label Ytrain ∈ Rn

where n, and m are the number of observations, and the number of features re-

spectively. To construct a prediction model f , traditional machine learning

methods are trained by pairs of (Xtrain, Ytrain) = {(xi, yi), . . . , (xn, yn)}. Tra-

ditionally, a primary assumption in many machine learning algorithms is that

the training and future data must be in a similar feature space. More speciﬁ-

cally Xtrain and Xtest must be under the same distribution D. It means that the

training and the test dataset must share a similar marginal probability distribu-

tion P (X) over D. However, this assumption may be diﬃcult to follow in many

real-world applications. Modern object identiﬁcation and classiﬁcation models

with millions of parameters can take weeks to train fully. Transfer learning is a

technique that shortcuts a lot of this work by taking a fully-trained model for

a set of predeﬁned categories like ImageNet [95, 96, 97], and retrains from the

existing weights for new implemented classes [98]. Transfer learning aims to im-

prove understanding of the target task by leveraging knowledge transferability

from the source task. Thus, the transfer learning procedure can be deﬁned as

follows. Two identiﬁcation datasets Dsource and Dtest are constructed by two
diﬀerent research groups. Our task is to assign labels Y target
to test data X target
drawn from distribution Dtarget given the training data (X target

train ) drawn
from distribution Dtarget. The eﬀects of dissimilar observations will be reduced

train , Y target

test

test

by re-weighting the weights of instances in the source domain.

In contrast,

similar observations will contribute more to the target domain and may thus

lead to a more accurate model [99]. The diﬀerence between traditional machine

learning and transfer learning is presented in Figure (8).

Recall, a pre-trained model can be used in two ways: it can be as an initializer

[100], see Figure 9, or a feature extractor [101, 102], illustrated in Figure 10.

Updating parameters θ and the eﬀectiveness of the machine learning model

can be heavily dependent on the choice of parameter initialization strategy,

especially with millions of parameters as in a convolutional neural network [103,

104]. One advantage of transfer learning is that we might not need to update

24

Figure 8: Diﬀerence between traditional machine learning and transfer learning.

all those millions of parameters by freezing parts of the model or the whole

model except the last layer.

In this case, we might only need to ﬁne-tune a

couple of layers upon our new datasets. Another likable consideration can be

found in the optimization of dynamic systems [105]. Suppose your current task

is similar to the one the pre-trained model solved.

In that case, we might

have a high chance that the optimal values of parameters in both experimental

scenarios are identical. What the pre-trained model does in your task is to

update existing parameters upon new data points, usually leaving the network

architecture unchanged and re-using some of or all of the model weights [106].

2.4. Reinforcement Learning

Reinforcement learning (RL) is one of the signiﬁcant branches of machine

learning. While the supervised and unsupervised learning methods learn the

model from a given data set, RL methods learn to act.

In other words, the

outcome of the RL is the optimal decision rule for a given state, which is also

referred to as ‘policy’ given an objective [82]. In general, RL performs the follow-

ing three-step procedure iteratively: data generation, performance evaluation,

and policy improvement. By interacting with the dynamic systems according

to the policy, the RL agent receives the data consisting of states, actions, and

rewards. The data is used as a reinforcement signal that evaluates the perfor-

mance of the policy. The policy is improved based on the performance evaluation

with various types of optimization methods. The procedure is usually designed

to be stochastic, not only to act against the uncertain systems but also to add

25

Figure 9: Frozen (no update during training) and ﬁne-tuned (update during training) layers.

exploratory actions to the system to prevent trainable machine learning models

from being overﬁtted [107]. This addresses the trade-oﬀ between exploration

and exploration explicitly.

RL is deeply connected with process control in the sense that RL solves se-

quential decision-making problems [108]. RL has several potential advantages

over the standard approaches of bioprocess control that use mechanistic models

and mathematical programming. First, RL is ﬂexible to work with varying levels

of mechanical knowledge and structure of the systems [109, 110, 111]. Model-

free is a special characteristic that distinguishes RL from other process control

methods, and the reinforcement signal is solely used for policy improvement.

Therefore, model-free RL can handle (1) hybrid systems consisting of mixed

continuous and discrete states, actions, and events, (2) problems with vari-

ous objective functions encompassing tracking control, economic optimization,

and experimental design, and (3) model uncertainties that are not restricted

26

Figure 10: Initial layers as feature extractors.

to Gaussian distribution. This ﬂexibility is an appealing characteristic for the

bioprocess control and optimization [112], because biological models are often

challenging to build, and biological systems have a considerable level of uncer-

tainties. Recent advances in statistical machine learning enable feature analysis

of the raw sensory-level data by using deep neural networks and the implementa-

tion of various information-theoretic techniques. Synthesis with a deep learning

framework, deep RL (DRL) has successfully achieved a scale-up of RL meth-

ods to high-dimensional problems, showing remarkable performances in various

applications such as process scheduling, reaction mechanism, ﬂuid dynamics,

robotics, autonomous driving, etc. [113, 114, 115, 116, 117, 118, 119].

The RL’s second advantage is that most of the computation is done oﬀ-

line. In contrast, the conventional mathematical programming approaches need

consistent re-planning, which can lead to exorbitant on-line computational de-

mand. Because a single model cannot perfectly characterize the complexity

of the metabolism, bioprocesses have to be operated in a closed-loop manner,

27

adapting the model to the most recent experimental data [120] However, the

mathematical programming-based approaches such as model predictive control

(MPC) cannot match the online computation limit when the complexity is high

due to the combination of the model, operating constraints, and uncertainties.

Several researchers have focused on the RL framework as a complementary

method [121, 122, 123].

It is the nature of RL that the policy is obtained,

essentially the closed-loop feedback rule concerning states of the system. An

end-to-end closed-loop operation can be achieved if the RL is applied to indus-

trial bioprocesses using massive historical raw data in an oﬄine environment.

Motivated by these advantages, several pioneering pieces of work for the

bioprocess control were ﬁrst appeared in [124, 125, 126]. In these studies, the

RL methods use the lookup table that measures the optimality (e.g., ‘cost-to-

go’ function or Q-function) with respect to the discretized the state and action

space.

[124] used a fuzzy lookup-table guided by the expert knowledge in the

frame of a Q-learning algorithm.

It showed that the RL could achieve near-

optimal performance for the batch process control.

[126] combined the fuzzy

rule with the Q-learning method to determine the gains of a PID controller for a

tracking problem of the fed-batch bioreactor. [125] solved a free-end problem for

a fed-batch bioreactor using approximate dynamic programming, an analogous

algorithm to the RL. The RL algorithm was tested under diﬀerent initial condi-

tions and showed optimal performance without additional recomputation. This

is the ﬁrst work that recognizes the merit of RL for the closed-loop operation

in the presence of disturbances.

Recent works incorporate DRL methods, which allow for an extension to

the optimization under the continuous state and action space [127, 128, 29].

In [127], partially supervised RL was used to solve a tracking problem of a

yeast fermentation problem. Neural networks that map the state and setpoint

with the control input were trained and reﬁned using RL. A DRL algorithm,

asynchronous advantage actor-critic (A3C), was incorporated into the biomass

maximization problem of a fed-batch bioreactor [128].

[29] utilized the policy

gradient method for the optimization and recurrent neural networks (RNN) to

28

approximate the policy function. The RL method was performed preliminary

using oﬄine data, and the policy was further trained in the online implementa-

tion.

The main drawback of model-fee RL is that it is notoriously diﬃcult to use

due to the sensitivity to hyperparameters, intractable data requirement, and op-

timistic estimation of the Q-function values [129, 130, 110]. Even for the optimal

control of the most straightforward linear system with the quadratic objective

function, model-free RL fails to achieve a reliable solution compared to the

standard linear quadratic regulator algorithm [131]. This limits the actual ap-

plications to the control and optimization of the real bioprocesses. Model-based

RL can help solving the issue by using the mechanistic model as a simulator

for the oﬄine training, or utilizing the model equations’ gradients to accelerate

the training [132]. [133] suggested a two-stage optimal control for a closed-loop

dynamic optimization of a fed-batch bioreactor. In the high-level optimizer, dif-

ferential dynamic programming, a model-based RL that uses model gradient, is

used for the long-term planning with the economic objective of maximizing pro-

ductivity. Whereas in the low-level controller, MPC is used for the short-term

planning that tracks the high-level plan and, at the same time, rejects distur-

bances and model-plant mismatch.

[134] proposed the integrated formulation

of the MPC and RL, where the terminal cost function of the MPC is replaced

by the value function obtained by the model-free method. The method was

validated for the optimization of an industrial-scale penicillin bioreactor.

Another issue about RL is the consideration of critical process constraints

for safety and keeping the operating condition within the valid domain [107]. A

typical way to consider process constraints is to augment the amount of con-

straint violation as the penalization term to the objective. Using augmentation

solely cannot always guarantee the feasibility of the exploration. In [135, 136],

the probability of constraint violation was formulated as chance constraints,

and an adaptive back-oﬀ approach was implemented to reduce the violation.

Nevertheless, the trade-oﬀ between the original objective and constraint penal-

ization is not uniquely determined, therefore adding another hyperparameter

29

to the overall algorithm. This is not the case in conventional mathematical

programming-based approaches such as MPC. In this regard, model-based RL

can be useful. [137] suggested Gaussian processes regression for the data-driven

state-space model and model-based RL for fed-batch fermentation processes.

Mechanistic model-based RL approaches [133, 134] can naturally address con-

straints of fed-batch bioprocesses.

3. Current Integration of Machine Learning in Bioprocess Subﬁelds

Machine learning has signiﬁcantly contributed to the development of biopro-

cess engineering, but its application is still limited, hampering the enormous po-

tential for bioprocess automation. In this section, we summarize recent research

across several important subﬁelds of bioprocess systems, see Table 1, including

bioreactor engineering [138], biodevices and biosensors [139, 140, 141, 142], bio-

materials engineering [143, 144, 145], and metabolic engineering [18, 146, 147,

148]. Bioreactor engineering studies the correlation and eﬀects between com-

plex intrinsic factors that operate a bioreactor (e.g., contaminant concentra-

tions, temperature, pH level, substrates, stirring and mixing duration, rate of

nutrient inﬂow) and primary cellular metabolism (e.g., product synthesis and

nutrient uptake). In this subﬁeld of bioprocess engineering, machine learning

has contributed to necessary research such as (1) estimating and predicting state

variables at some points in the future (e.g., biomass concentration), (2) monitor-

ing the factors that aﬀect the bioreactor’s performance, and (3) automating the

bioprocess regarding safe operation and control purposes. The next subﬁeld of

bioprocess engineering is Biodevices and biosensors which machine learning im-

plementation can be found in three primary areas: (1) optimization and control

of microbial fuel cells, (2) development of soft and microﬂuidic sensors, and (3)

chemical analysis of data collected from real-time measurements. Next, we also

highlight the implementation of machine learning models to assist in the design

and engineering of biomaterials in which biological engineers are interested in

three primary research goals: (1) the eﬃcient design and production of exist-

30

ing biological materials, (2) acceleration in developing new biological materials

or improving the existing functions; and (3) quantiﬁcation and automation of

structural-functional relationships. The next subﬁeld that the authors want to

summarize in this review is metabolic engineering, in which the application of

machine learning focuses on (1) completing the missing information to recon-

struct the metabolic network, (2) identifying essential and inﬂuential enzymes

and genes expression to product synthesis, and (3) exploiting the complex in-

teractions between omics from ﬂuxomics to genomics and growth kinetics of

extracellular microorganisms.

As mentioned earlier, we highlight the bioprocess tasks, experimental datasets,

and machine learning approaches within the subﬁelds. Note that there are two

fundamental goals when bio-experts manipulate any bioprocess systems. The

ﬁrst goal is to make an accurate translation from bioprocess problems to ap-

propriate machine learning tasks that can produce a correct prediction on the

experimental datasets. The second goal is to ensure that anyone in the same

laboratory or further researchers can reproduce the experiments. Therefore, we

will also provide an in-depth investigation on the reproducibility capability of

these mentioned research that we either believe in the results or build up the

conﬁdence of reproducing the whole experiments and improving further from

there.

Many machine learning models have been utilized and integrated into biopro-

cess systems are support vector regression (SVR), partial least square regression

(PLSR), multi-gene genetic programming (MGGP), artiﬁcial neural network

(ANN), Gaussian process (GP), Convolutional neural network (CNN), nonlin-

ear model predictive control (NMPC), hierarchical recurrent sensing network

(HRSN), recurrent neural network (RNN), multilayer perceptron (MLP), rele-

vant vector machine (RVM), accelerating genetic algorithm (AGA), K-nearest

neighbors (KNN), support vector machine (SVM), convolutional neural network

(CNN), and principal components analysis (PCA). The authors do not aim to

introduce and explain all the above models again, which could be referred to

many references [148, 149].

31

]
0
5
1
[

s
e
Y

X

.

M
V
S
B
I
L

n
i

R
V
S

,

L
0
0
0
2

,

L
0
0
4

,

L
0
8
(

s
e
r
u
t
l
u
c

y
d
o
b

i
t
n
a

l
a
n
ﬁ

e
h
t

t
c
i
d
e
r
P

S
L
P
M
I
S

n
i

R
S
L
P

.
0
1
-
0
9

o
i
t
a
r

t
s
e
t
-
n

i
a
r
T

.
)
L
0
0
0
2
1

.

n
o
i
t
a
r
t
n
e
c
n
o
c

e
t
a
t
c
a
l

d
n
a

l
a
r
o
p
m
e
t

4
3
1

f
o

a
t
a
d

s
e
i
r
e
s

e
m
T

i

d
e
e
s

r
u
o
f

n

i

s
r
e
t
e
m
a
r
a
p

s
s
e
c
o
r
p

.
f
e
R

a
t
a
d

h
g
i
h

.
d
e
m

w
o
l

l
e
d
o
m

a
t
e

M

y
t
i
l
i
b
i
c
u
d
o
r
p
e
R

g
n
i
n
r
a
e
l

e
n
i
h
c
a
M

t
e
s
a
t
a
D

k
s
a
T

d
l
e
ﬁ
b
u
S

]
2
5
1
[

s
e
Y

X

N
N
A

.
x
o
b
l
o
o
t

M
V
S
-
S
L

g
n

i
s
u

b
0
1
0
2
R
B
A
L
T
A
M
n
i

P
G
G
M

b
0
1
0
2
R
B
A
L
T
A
M
n
i

R
V
S

.
e
r
a
w
t
f
o
s

S
P
I
T
P
G
g
n

i
s
u

P
M
J

e
r
a
w
t
f
o
s

l
a
c
i
t
s
i
t
a
t
s

n

i

,
r
e
y
a
l

n
e
d
d
i
h

1
(

9

n
o
i
s
r
e
v

.
)
r
e
y
a
l

n
e
d
d
i
h

n
i

s
n
o
r
u
e
n

9
-
2

.

n
o
i
t
a
d

i
l
a
v
-
s
s
o
r
c

d

l
o
f
-
0
1

.
e
l
b
a
l
i
a
v
a

t
o
n

e
r
a

a
t
a
D

.
e
l
b
a
l
i
a
v
a

t
o
n

e
r
a

a
t
a
D

m
o
r
f

n
e
k
a
t

e
r
e
w
a
t
a
D

.
0
2
-
0
8

o
i
t
a
r

t
s
e
t
-
n

i
a
r
T

.
)
C
F
M

(

l
l
e
c

l
e
u
f

l
a
i
b
o
r
c
i
m

]
1
5
1
[

f
o

e
c
n
a
m
r
o
f
r
e
p

e
h
t

t
c
i
d
e
r
P

r
o
t
c
a
e
r
o
i
B

g
n
i
r
e
e
n
i
g
n
e

32

.
f
e
R

a
t
a
d

h
g
i
h

.
d
e
m

w
o
l

l
e
d
o
m

a
t
e

M

y
t
i
l
i
b
i
c
u
d
o
r
p
e
R

g
n
i
n
r
a
e
l

e
n
i
h
c
a
M

t
e
s
a
t
a
D

k
s
a
T

d
l
e
ﬁ
b
u
S

e
g
a
p

s
u
o
i
v
e
r
p
m
o
r
f

d
e
u
n
i
t
n
o
c

1

e
l
b
a
T

]
3
5
1
[

s
e
Y

]
4
5
1
[

o
N

]
5
5
1
[

o
N

X

X

X

s
n
o
r
u
e
n

0
2

,
s
r
e
y
a
l

n
e
d
d
i
h

2
(

.

d
e
c
u
d
o
r
p

e
r
e
w
s
t
e
s
a
t
a
d

l
a
i
c
ﬁ

i
t
r
a

s
s
e
c
o
r
p

n
o
i
t
c
u
d
o
r
p
o
i
b

y
r
a
r
b
i
l

n
i
a
r
b
y
p

n
i

N
N
A

f
o

s
n
o
i
t
a
c
i
l

p
e
r

0
5

.
s
t
n

i
o
p
a
t
a
d

n
i
e
t
u
l

e
t
a
l
u
m
i
S

.
)
r
e
y
a
l

n
e
d
d
i

h

r
e
p

e
r
a

a
t
a
D

.
4
/
1
-
4
/
3

o
i
t
a
r

t
s
e
t
-
n

i
a
r
T

.

n
o
i
t
a
z
i
m

i
t
p
o

d
n
a

l
o
r
t
n
o
c

2
1

g
n
n

i

i
a
t
n
o
c

h
c
a
e

,
a
t
a
d

f
o

s
t
e
s

4

.
e
l
b
a
l
i
a
v
a

t
o
n

2

d
n
a

1

h
t
i
w
e
r
a
p
m
o
C

.

P
G

.

N
N
A
s
r
e
y
a
l

n
e
d
d
h

i

n
i

r
e
y
a
l

r
e
p

s
n
o
r
u
e
N

.
}
5
2
,
0
2
,
5
1
,
0
1
,
5
,
3
{

n
o
i
t
a
r
u
g
ﬁ
n
o
c

o
n

,

N
N
A

.
n
e
v
i
g

e
t
a
r
t
i

n

,

n
o
i
t
a
r
t
n
e
c
n
o
c

s
s
a
m
o
i
B

n

i

n
a
y
c
o
c
y
h
p

d
n
a

,

n
o
i
t
a
r
t
n
e
c
n
o
c

y
r
e
v
e

d
e
r
u
s
a
e
m
e
r
e
w
n
o
i
t
c
u
d
o
r
p

t
e
s
a
t
a
d

l
a
n

i
g
i
r
o

e
h
T

.
s
r
u
o
h

8

s
u
p

l

,
s
t
n

i
o
p

a
t
a
d

5
3
1

f
o

s
t
s
i
s
n
o
c

a
t
a
d

d
e
t
a
r
e
n
e
g

y
l
l
a
i
c
ﬁ

i
t
r
a

0
0
1

.
e
l
b
a
l
i
a
v
a

t
o
n

e
r
a

a
t
a
D

.
s
t
n

i
o
p

h
c
t
a
b
-
d
e
f

e
h
t

e
t
a
l
u
m
i
S

r
o
f

s
s
e
c
o
r
p

n
o
i
t
c
u
d
o
r
p

l
a
i
r
e
t
c
a
b
o
n
a
y
c

.

n
i
n
a
y
c
o
c
y
h
p
-
C

e
r
a

a
t
a
D

.
a
t
a
d

d
e
t
n
e
m

i
r
e
p
x
e

r
o
f

s
e
t
a
t
s

e
t
a
i
r
a
v
i
t
l
u
m

t
u
o
b
a

n
o
i
t
a
m
r
o
f
n

i

r
a
e
l
c

o
N

f
o

n
o
i
t
u

l
o
v
e

e
h
t

t
c
i
d
e
r
P

.
e
l
b
a
l
i
a
v
a

t
o
n

.
s
s
e
c
o
r
p

n
o
i
t
c
u
d
o
r
p

n
i
e
t
u
l

33

.
f
e
R

a
t
a
d

h
g
i
h

.
d
e
m

w
o
l

l
e
d
o
m

a
t
e

M

y
t
i
l
i
b
i
c
u
d
o
r
p
e
R

g
n
i
n
r
a
e
l

e
n
i
h
c
a
M

]
6
5
1
[

s
e
Y

X

l
a
n
o
i
t
u
l
o
v
n
o
c

g
n
i
n
i
a
t
n
o
c

7

h
t
i
w
r
e
y
a
l

t
u
p
n
i

1

,

N
N
C

s
r
e
y
a
l

n
e
d
d
i
h

2

,
s
n
o
r
u
e
n

r
e
y
a
l

t
u
p
t
u
o

1

,
s
k
c
o
l
b

.
s
n
o
r
u
e
n

3

h
t
i
w

]
7
5
1
[

o
N

X

.
n
e
v
i
g

n
o
i
t
a
r
u
g
ﬁ
n
o
c

o
n

,

P
G

t
n
e
r
e
ﬀ
d

i

0
4
m
o
r
f

n
e
k
a
t

e
r
e
w
a
t
a
D

t
n
e
r
e
ﬀ
d

i

n
o

s
o
i
r
a
n
e
c
s

l
a
t
n
e
m

i
r
e
p
x
e

h
c
a
E

.
s
r
o
t
c
a
e
r
o
i
b
o
t
o
h
p

L
0
2
1

.
s
t
n

i
o
p

a
t
a
d

0
0
0
9

s
n

i
a
t
n
o
c

o
i
r
a
n
e
c
s

.
0
3
-
0
7

o
i
t
a
r

t
s
e
t
-
n

i
a
r
T

.
e
l
b
a
l
i
a
v
a

t
o
n

e
r
a

a
t
a
D

t
o
n

e
r
a

a
t
a
D

.
t
e
s
a
t
a
d

d
e
t
a
l
u
m
S

i

.
e
l
b
a
l
i
a
v
a

e
g
a
p

s
u
o
i
v
e
r
p
m
o
r
f

d
e
u
n
i
t
n
o
c

1

e
l
b
a
T

t
e
s
a
t
a
D

k
s
a
T

d
l
e
ﬁ
b
u
S

s
s
a
m
o
i

b

l
a
g
l
a

e
h
t

e
t
a
l
u
m
i
S

e
n
e
l
o
b
a
s
i
b

d
n
a

h
t
w
o
r
g

.
n
o
i
t
c
u
d
o
r
p

s
l
e
d
o
m
C
P
M
N
d
e
s
a
b
-
P
G

.
l
o
r
t
n
o
c

n
o
z
i
r
o
h

e
t
i
n
ﬁ

r
o
f

i

c
i
d
u
ﬂ
o
r
c
i
m
e
z
i
r
e
t
c
a
r
a
h
C

.
r
o
s
n
e
s

t
f
o
s

t
n
e
r
e
ﬀ
i
d

6

e
r
a
p
m
o
C

34

]
8
5
1
[

s
e
Y

]
9
5
1
[

s
e
Y

X

X

.

N
N
R
n
o

d
e
s
a
b
N
S
R
H

t
f
o
s

o
w
t

m
o
r
f

d
e
t
c
e
l
l
o
c

e
r
e
w
a
t
a
D

d
e
d
i
v
o
r
p

s
i

l
e
d
o
m
e
h
T

d
e
s
s
e
c
o
r
p

e
h
T

.
s
r
o
s
n
e
s

e
r
u
s
s
e
r
p

.
b
u
h
t
i

G
n
o

.

b
u
h
t
i

G
n
o

e
l

b
a
l
i
a
v
a

e
r
a

a
t
a
d

r
e
f
s
n
a
r
t

d
e
i
l
p
p
A

.
t
e
N
U
s
e
R

e
h
T

.
e
u
q
i
n
h
c
e
t

g
n
i

n
r
a
e
l

n
o

d
e
d
i
v
o
r
p

s
i

l
e
d
o
m

.
b
u
h
t
i

G

n
a
m
a
R

l
a
r
t
c
e
p
s
r
e
p
y
h

n
o
i
l
l
i

m
5
.
1

t
u
p
h
g
u
o
r
h
t
-
r
e
h
g
i
h

s
s
e
c
o
r
P

.

b
u
h
t
i

G
n
o

e
l
b
a
l
i
a
v
a

.
s
e
g
a
m

i

r
a
l
u
c
e
l
o
m

e
r
a

a
t
a
d

d
e
s
s
e
c
o
r
p

e
h
T

.
s
e
g
a
m

i

d
n
a

y
p
o
c
s
o
r
t
c
e
p
s

n
a
m
a
R

s
r
o
s
n
e
s
o
i
b

s
e
c
i
v
e
d
o
i
B

d
n
a

]
1
6
1
[

s
e
Y

]
2
6
1
[

s
e
Y

]
0
6
1
[

s
e
Y

X

X

X

.
)
T
E
N
K
S
(

i

k
r
o
w
t
e
n

x
e
d
n

i

n
o

d
e
d
i
v
o
r
p

s
i

l
e
d
o
m
e
h
T

n
e
n
o
h
o
K
g
n
i
s
i

m

i
t
p
o

f
l
e
S

.
b
u
h
t
i

G

s
i

l
e
d
o
m
e
h
T

.
s
n
o
r
u
e
n

}
5
,
4
,
3
,
2
{

h
t
i
w
P
L
M

.
e
l
b
a
l
i
a
v
a

t
o
n

e
r
e
w
s
e
l
p
m
a
s

e
u
s
s
i
t

a
n
i
t
e
r

0
0
4
4
1

.

b
u
h
t
i

G
n
o

t
o
n

e
r
a

a
t
a
D

.
0
3
-
0
7

o
i
t
a
r

t
s
e
t
-
n

i
a
r
T

.
e
l
b
a
l
i
a
v
a

.
e
l
b
a
l
i
a
v
a

t
o
n

e
r
a

a
t
a
D

l
a
i

b
o
r
c
i
m
n
i

n
o
i
t
c
u
d
o
r
p

y
t
i
c
i
r
t
c
e
l
e
o
i
b

t
c
i
d
e
r
P

.
s
l
l
e
c

l
e
u
f

.
e
c
i
m
e
l
a
m

t
l
u
d
a
m
o
r
f

d
e
t
c
e
l
l
o
c

n

i
a
r
b

c
i
t
a
m
u
a
r
t

y
f
i
s
s
a
l
C

.

n
o
i
t
a
d

i
l
a
v
-
s
s
o
r
c

d

l
o
f
-
0
1

.
a
n
i
t
e
r

e
h
t

f
o

y
p
o
c
s
o
r
t
c
e
p
s

.
0
2
-
0
8

o
i
t
a
r

t
s
e
t
-
n

i
a
r
T

n
a
m
a
R
a
i
v

y
t
i
r
e
v
e
s

y
r
u
j
n
i

.
)
T
E
N
K
S
(

i

k
r
o
w
t
e
n

x
e
d
n

i

s
n
a
c
s

a
r
t
c
e
p
s

8
8

f
o

g
n
i
t
s
i
s
n
o
c

e
u
s
s
i
t

e
y
e

o
v
i
v
-
x
e

n
o

d
e
d
i
v
o
r
p

s
i

l
e
d
o
m
e
h
T

e
h
T

.
t
n
e
m
g
e
s

e
u
s
s
i
t

r
e
p

f
o

e
g
a
s
u

e
h
t

n
i

s
t
n
e
m
g
e
s

n
e
n
o
h
o
K
g
n
i
s
i

m

i
t
p
o

f
l
e
S

,
s
e
y
e

d
e
t
a
e
l
c
u
n
e

e
t
a
r
a
p
e
s

l
a
c
i
m
o
t
a
n
a

e
s
o
n
g
a
i
D

.
b
u
h
t
i

G

e
l
b
a
l
i
a
v
a

e
r
a

a
t
a
d

d
e
s
s
e
c
o
r
p

.
y
p
o
c
s
o
r
t
c
e
p
s

n
a
m
a
R

1
1
m
o
r
f

d
e
t
c
e
l
l
o
c

e
r
e
w
a
t
a
D

.
f
e
R

a
t
a
d

h
g
i
h

.
d
e
m

w
o
l

l
e
d
o
m

a
t
e

M

y
t
i
l
i
b
i
c
u
d
o
r
p
e
R

g
n
i
n
r
a
e
l

e
n
i
h
c
a
M

t
e
s
a
t
a
D

k
s
a
T

d
l
e
ﬁ
b
u
S

e
g
a
p

s
u
o
i
v
e
r
p
m
o
r
f

d
e
u
n
i
t
n
o
c

1

e
l
b
a
T

35

]
3
6
1
[

o
N

]
5
6
1
[

o
N

]
6
6
1
[

o
N

X

X

X

,
s
n
o
r
u
e
n

5

h
t
i
w
r
e
y
a
l

t
u
p
n
i

1
(

r
e
y
a
l

t
u
p
t
u
o

1

,
h
c
a
e

s
n
o
r
u
e
n

5

h
t
i
w
s
r
e
y
a
l

n
e
d
d
h

i

3

,
}
0
1
,
.
.
.
,
2
,
1
{
=
k

h
t
i
w
N
N
K

N
N
A

.

B
A
L
T
A
M
n
i

M
V
S

.
)
s
n
o
r
u
e
n

3

h
t
i
w

m
r
o
f
i
n
u

f
o

n
o
i
t
a
n
i
b
m
o
C

.

A
G
A
d
n
a

,

M
V
R

,
n
g
i
s
e
d

.
n
e
v
i
g

n
o
i
t
a
r
u
g
ﬁ
n
o
c

o
N

.
f
e
R

a
t
a
d

h
g
i
h

.
d
e
m

w
o
l

l
e
d
o
m

a
t
e

M

y
t
i
l
i
b
i
c
u
d
o
r
p
e
R

g
n
i
n
r
a
e
l

e
n
i
h
c
a
M

e
g
a
p

s
u
o
i
v
e
r
p
m
o
r
f

d
e
u
n
i
t
n
o
c

1

e
l
b
a
T

.
e
l
b
a
l
i
a
v
a

t
o
n

e
r
a

a
t
a
D

l
a
i
b
o
r
c
i
m
e
l
b
a
i
r
a
v

i
t
l
u
m

f
o

n
o
i
t
a
r
e
p
o

e
h
t

e
z
i
m

i
t
p
O

.
s
l
l
e
c

l
e
u
f

t
e
s
a
t
a
D

k
s
a
T

d
l
e
ﬁ
b
u
S

m
o
r
f

n
e
k
a
t

e
r
e
w
a
t
a
D

.
]
4
6
1
[

.
e
l
b
a
l
i
a
v
a

t
o
n

e
r
a

a
t
a
D

.
s
y
o
l
l
a

y
p
o
r
t
n
e
-
h
g
i
h

f
o

e
s
a
h
p

t
c
i
d
e
r
P

36

s
l
a
i
r
e
t
a
m
o
i
B

g
n
i
r
e
e
n
i
g
n
e

m
o
d
n
a
r

,
g
n
i
t
s
o
o
b

t
n
e
i
d
a
r
G

d
e
t
a
l

u
c
l
a
c

6
2

f
o

g
n
i
t
s
i
s
n
o
c

9
1
0
2
R

e
r
a

s
l
e
d
o
m
e
h
T

.
t
s
e
r
o
f

n

i
e
t
o
r
p

7
3

f
o

s
e
r
u
t
a
e
f

l
a
r
u
t
c
u
r
t
s

.
b
u
h
t
i

G
n
o

d
e
d
i
v
o
r
p

.

n
o
i
t
a
d

i
l
a
v
-
s
s
o
r
c

d

l
o
f
-
5

.
s
k
r
o
w
t
e
n

B
A
L
T
A
M
n

i

d
e
t
a
r
e
n
e
g

e
r
e
w
a
t
a
D

.
e
l
b
a
l
i
a
v
a

t
o
n

e
r
a

a
t
a
D

n

i
e
t
o
r
p

f
o

y
t
i
l
a
n
o
i
t
c
n
u
f

l
a
c
o
f
n
o
c
m
o
r
f

s
k
r
o
w
t
e
n

.
g
n

i
g
a
m

i

y
p
o
c
s
o
r
c
i
m

l
a
c
i
n
a
h
c
e
m

t
c
i
d
e
r
P

]
8
6
1
[

s
e
Y

X

0
3

h
t
i
w
r
e
y
a
l

n
e
d
d
i
h

1
(
N
N
A

l
a
t
n
e
m

i
r
e
p
x
E

.
)
s
n
o
r
u
e
n

a

n
o

n
e
v
i
g

s
a
h

y
t
i
l
i
b
i
c
u
d
o
r
p
e
r

.
]
7
6
1
[

e
t
i
s
b
e
w
d
e
t
a
c
i
d
e
d

t
c
n

i
t
s
i
d

1
4

g
n
n

i

i
a
t
n
o
c

s
F
O
M
5
8
3
3

e
c
n
a
m
r
o
f
r
e
p

e
h
t

t
c
i
d
e
r
P

a
t
a
d

d
e
s
s
e
c
o
r
P

.
s
e
i
g
o
l
o
p
o
t

k
r
o
w
t
e
n

c
i
n
a
g
r
o
-
l
a
t
e
m

f
o

.
e
l

b
a
l
i
a
v
a

e
r
a

.
)
s
F
O
M

(

s
k
r
o
w
e
m
a
r
f

.
f
e
R

a
t
a
d

h
g
i
h

.
d
e
m

w
o
l

l
e
d
o
m

a
t
e

M

y
t
i
l
i
b
i
c
u
d
o
r
p
e
R

g
n
i
n
r
a
e
l

e
n
i
h
c
a
M

t
e
s
a
t
a
D

k
s
a
T

d
l
e
ﬁ
b
u
S

e
g
a
p

s
u
o
i
v
e
r
p
m
o
r
f

d
e
u
n
i
t
n
o
c

1

e
l
b
a
T

]
9
6
1
[

s
e
Y

]
0
7
1
[

o
N

]
1
7
1
[

s
e
Y

X

X

X

c
i
t
s
i
g
o
l

,
t
s
e
r
o
f

m
o
d
n
a
r

,

N
N
K

e
b

n
a
c

s
e
d
o
c

d
n
a

a
t
a
d
w
a
R

h
t
i
w
s
d

i
c
a

o
n
i
m
a

t
c
e
t
e
D

n
o
i
t
a
r
u
g
ﬁ
n
o
c

o
N

.
n
o
i
s
s
e
r
g
e
r

e
l
b
a
n
o
s
a
e
r

n
o
p
u

d
e
d
i
v
o
r
p

r
e
y
a
l
-
e
l
g
n
i
s

s
u
o
r
o
p
o
n
a
n

o
N

.

B
A
L
T
A
M
n
i

M
V
S

.
n
e
v
i
g

n
o
i
t
a
r
u
g
ﬁ
n
o
c

.
e
l
b
a
l
i
a
v
a

t
o
n

e
r
a

a
t
a
D

e
p
a
h
s

l
l
e
c

y
f
i
s
s
a
l
C

.
s
e
p
y
t
o
n
e
h
p

.
n
e
v
i
g

.
t
s
e
u
q
e
r

l

.
e
d
ﬁ
u
s
i
d
m
u
n
e
d
b
y
l
o
m

)
s
r
e
y
a
l

n
e
d
d
i
h

0
1
(
N
N
A

.
0
2
-
0
2
-
0
6

o
i
t
a
r

t
s
e
t
-
n
o
i
t
a
d

i
l
a
v
-
n

i
a
r
T

o
N

.
x
o
b
l
o
o
T
g
n
i
n
r
a
e
L

g
n

i
s
n
e
c
i
L

.
t
s
e
u
q
e
r

e
l
b
a
n
o
s
a
e
r

n
o
p
u

p
e
e
D
B
A
L
T
A
M
n

i

d
e
d

i
v
o
r
p

e
b

n
a
c

s
e
d
o
c

d
n
a

a
t
a
d
w
a
R

.
n
e
v
i
g

n
o
i
t
a
r
u
g
ﬁ
n
o
c

.

d
e
i
l

p
p
a

e
b

t
h
g
i
m

s
e
e
f

h
g
u
o
r
h
t

s
e
l
c
i
t
r
a
p
o
r
c
i
m

s
e
l
d
e
e
n

c
i
m
r
e
d
o
p
y
h

f
o

n
o
i
t
c
e
j
n
i

t
c
i
d
e
r
P

37

.
f
e
R

a
t
a
d

h
g
i
h

.
d
e
m

w
o
l

l
e
d
o
m

a
t
e

M

y
t
i
l
i
b
i
c
u
d
o
r
p
e
R

g
n
i
n
r
a
e
l

e
n
i
h
c
a
M

t
e
s
a
t
a
D

k
s
a
T

d
l
e
ﬁ
b
u
S

e
g
a
p

s
u
o
i
v
e
r
p
m
o
r
f

d
e
u
n
i
t
n
o
c

1

e
l
b
a
T

]
4
7
1
[

s
e
Y

X

]
2
7
1
[

s
e
Y

X

r
e
y
a
l
-
4
1

h
t
i
w
k
r
o
w
t
e
n

l
a
r
u
e
N

d
e
n
i
b
m
o
c

e
r
u
t
c
e
t
i
h
c
r
a
N
N
C

.
s
n
o
i
t
c
e
n
n
o
c

p
i
k
s

-

t
e
N
U
h
t
i
w

.
e
l
b
a
l
i
a
v
a

t
o
n

s
i

l
e
d
o
m
e
h
T

e
c
r
u
o
s

n
e
p
o

l
l
i

F
p
a
G
t
s
o
o
B

.
l
o
o
t

0
0
6

s
u

l

p

s
r
i
a
p

e
r
u
t
c
u
r
t
s
-
k
s
a
m
0
0
3

t
o
n

e
r
a

a
t
a
D

.

d
e
t
n
e
m
g
u
a

s
r
i
a
p

.
e
l
b
a
l
i
a
v
a

.
g
n

i
t
n

i
r
p
D
3

d
e
s
a
b
-
t
h
g
i
l

n
i

t
c
e
ﬀ
e

g
n
i
r
e
t
t
a
c
s

t
c
e
t
e
D

.
]
3
7
1
[

e
s
a
b
a
t
a
d
G
G
B

i

n
o

e
l
b
a
l
i
a
v
a

e
r
a

a
t
a
D

.

b
u
h
t
i

G
s
’
l
l
i

F
p
a
G
t
s
o
o
B

c
i
l
o
b
a
t
e
m
a

n
i

s
p
a
g

l
l
i

F

.
k
r
o
w
t
e
n

s
e
m
y
z
n
e

c
ﬁ
i
c
e
p
s

y
f
i
t
n
e
d
I

]
5
7
1
[

s
e
Y

X

.

B
A
L
T
A
M
n
i

A
C
P

.
e
l
b
a
l
i
a
v
a

t
o
n

e
r
a

a
t
a
D

a

n

i

n
o
i
t
c
u
d
o
r
p

g
n
i
t
i

m

i
l

]
6
7
1
[

s
e
Y

X

.

m
r
o
f
t
a
l
p

d
e
s
a
b
-
b
e
w
x
u
l
F
M

.
e
l
b
a
l
i
a
v
a

e
r
a

s
e
d
o
c

e
c
r
u
o
S

,

N
N
K

,

M
V
S

s
e
i
l
p
p
a

x
u
l
F
M

.
e
e
r
t

n
o
i
s
i
c
e
d

0
0
1
m
o
r
f

d
e
t
c
e
l
l
o
c

a
t
a
D

s
i
s
y
l
a
n
a

x
u
ﬂ

c
i
l
o
b
a
t
e
m
C

-

.
e
l
b
a
l
i
a
v
a

e
r
a

a
t
a
D

.
s
r
e
p
a
p

l
a
i
r
e
t
c
a
b

e
h
t

t
c
i
d
e
r
P

.

m

s
i
l
o
b
a
t
e
m

l
a
r
t
n
e
c

.
y
a
w
h
t
a
p

g
n
i
r
e
e
n
i
g
n
e

c
i
l
o
b
a
t
e

M

38

]
7
7
1
[

s
e
Y

X

.
s
r
i
a
p

e
n
e
g
-
n
o
i
t
c
a
e
r

c
i
l
o
b
a
t
e
m
4
9
0
4

n
o
i
t
a
r
u
g
ﬁ
n
o
c

o
N

.

M
V
S

.
n
e
v
i
g

e
n
e
G

i
l
o
c

.

E
d
n
a

s
r
e
d
i
v
o
r
p

e
t
a
v
i
r
p

i
l
o
c

a
i
h
c
i
r
e
h
c
s
E

m
o
r
f

s
t
e
s
a
t
a
d

l
a
n
o
i
t
i
d
d
a

l
a
r
e
v
e
S

n

i

s
e
n
e
g

l
a
i
t
n
e
s
s
e

t
c
i
d
e
r
P

e
r
a

a
t
a
D

.
e
s
a
b
a
t
a
D
n
o
i
s
s
e
r
p
x
E

.

m

s
i
l
o
b
a
t
e
m

.
e
l
b
a
l
i
a
v
a

t
o
n

.
f
e
R

a
t
a
d

h
g
i
h

.
d
e
m

w
o
l

l
e
d
o
m

a
t
e

M

y
t
i
l
i
b
i
c
u
d
o
r
p
e
R

g
n
i
n
r
a
e
l

e
n
i
h
c
a
M

t
e
s
a
t
a
D

k
s
a
T

d
l
e
ﬁ
b
u
S

e
g
a
p

s
u
o
i
v
e
r
p
m
o
r
f

d
e
u
n
i
t
n
o
c

1

e
l
b
a
T

]
9
7
1
[

s
e
Y

X

g
n
i
n
r
a
e
l

e
v
i
t
c
a

y
l
p
p
A

.

M
V
S

e
s
a
b
a
t
a
d

e
m
y
z
n
e

e
n

i
l

n
o
A
D
N
E
R
B

.
h
c
a
o
r
p
p
a

.
]
8
7
1
[

]
0
8
1
[

o
N

]
1
8
1
[

s
e
Y

X

X

e
h
T

.
g
n
i
n
r
a
e
l

t
n
e
m
e
c
r
o
f
n
i
e
r

,

n
o
i
t
a
r
u
d

h
4
2

,
r
o
t
c
a
e
r
o
i
b

,
g
n
i
n
r
a
e
l
-

Q
d
e
t
t
ﬁ

l
a
r
u
e
N

s
u
o
u
n

i
t
n
o
c

f
o

a
t
a
d

d
e
t
a
l
u
m
S

i

n
o

d
e
d
i
v
o
r
p

e
r
a

s
l
e
d
o
m

.

i

n
m
5

y
r
e
v
e

t
n
e
m
e
r
u
s
a
e
m

.
b
u
h
t
i

G

.
e
l
b
a
l
i
a
v
a

t
o
n

s
i

a
t
a
D

n
i

y
g
o
l
o
p
o
t

2
-
0
1
-
5

f
o
N
N
A

h

6

y
r
e
v
e

d
e
t
c
e
l
l
o
c

e
r
e
w
s
t
e
s
a
t
a
D

t
a
h
t

s
e
t
a
r
t
s
b
u
s

g
n
i
t
c
e
l
e
S

s
’
e
m
y
z
n
e

n
a

d
n
a
p
x
e

t
s
e
b

e
m

i
t
-
l
a
e
r

a

e
s
o
p
o
r
P

e
h
t

r
o
f

n
o
i
t
a
z
i
m

i
t
p
o

s
e
r
u
t
l
u
c
-
o
c

f
o

l
o
r
t
n
o
c

s
u
o
u
n

i
t
n
o
c

e
h
t

n
i
h
t
i
w

.
y
t
i
u
c
s
i

m
o
r
p

e
s
o
l
y
x

t
c
i
d
e
r
P

.
s
r
o
t
c
a
e
r
o
i
b

39

g
n
i
n
r
a
e
L

p
e
e
D
B
A
L
T
A
M

s
t
n

i
o
p

a
t
a
d

0
4
3

n

i

g
n

i
t
l
u
s
e
r

l
a
v
r
e
t
n

i

s
s
a
m
o
i
b

,
n
o
i
t
p
m
u
s
n
o
c

.
x
o
b
l
o
o
T

.
e
l
b
a
l
i
a
v
a

t
o
n

e
r
a

a
t
a
D

.
)
s
n
u
r

7
2
(

.

n
o
i
t
c
u
d
o
r
p

l
o
t
i
l
y
x

d
n
a

]
2
8
1
[

s
e
Y

X

e
c
r
u
o
s

n
e
p
o

e
h
T

.
g
n
i

n
r
a
e
l

,
s
y
a
w
h
t
a
p

l
a
t
n
e
m

i
r
e
p
x
e

d
e
t
a
r
u
c

s
i
s
e
h
t
n
y
s
o
r
t
e
r
o
i
b

n
o

d
e
d
i
v
o
r
p

s
i

n
o
i
t
u

l
o
s

g
n

i
r
e
e
n

i
g
n
e

c
i
l
o
b
a
t
e
m
2
5
1

c
i
t
e
h
t
n
y
s

n
i

e
c
a
p
s

t
n
e
m
e
c
r
o
f
n
i
e
r

d
e
i
l

p
p
A

y
l
l
a
u
n
a
m
0
2

f
o

t
e
s
a
t
a
d

n
e
d
l
o
G

e
h
t

e
r
o
l
p
x
E

.
b
u
h
t
i

G

.
e
l
b
a
l
i
a
v
a

e
r
a

a
t
a
D

.
s
t
c
e
j
o
r
p

.

n
g
i
s
e
d

y
a
w
h
t
a
p

.
f
e
R

a
t
a
d

h
g
i
h

.
d
e
m

w
o
l

l
e
d
o
m

a
t
e

M

y
t
i
l
i
b
i
c
u
d
o
r
p
e
R

g
n
i
n
r
a
e
l

e
n
i
h
c
a
M

t
e
s
a
t
a
D

k
s
a
T

d
l
e
ﬁ
b
u
S

e
g
a
p

s
u
o
i
v
e
r
p
m
o
r
f

d
e
u
n
i
t
n
o
c

1

e
l
b
a
T

]
3
8
1
[

s
e
Y

X

s
i

l
e
d
o
m
e
h
T

.
n
e
v
i
g

.
s
e
c
n
e
u
q
e
s

r
e
t
o
m
o
r
p

e
l
b

i
c
u
d
n

i

n
o
i
t
a
r
u
g
ﬁ
n
o
c

o
n

,

N
N
C

0
0
0
,
7
2
3

d
n
a

e
v
i
t
u
t
i
t
s
n
o
c

0
0
0
,
5
7
6

.
b
u
h
t
i

G
n
o

e
l
b
a
l
i
a
v
a

.
e
l
b
a
l
i
a
v
a

e
r
a

a
t
a
D

n
o
i
s
s
e
r
p
x
e

n
i
e
t
o
r
p

t
c
i
d
e
r
P

.
s
e
c
n
e
u
q
e
s

r
e
t
o
m
o
r
p
m
o
r
f

40

.
g
n
i
r
e
e
n
i
g
n
e

s
s
e
c
o
r
p
o
i
b

f
o

s
d
l
e
ﬁ
b
u
s

s
u
o
i
r
a
v

n

i

i

g
n
n
r
a
e
l

i

e
n
h
c
a
m

f
o

n
o
i
t
a
c
i
l

p
p
A

:
1

e
l
b
a
T

4. Challenges and Future Research Directions

4.1. Challenge 1: Reproducibility Crisis

Machine learning is, to a considerable extent, an experimental science. As a

result, reproducibility of claims is a signiﬁcant concern [184, 185, 186]. Machine

learning experts have mentioned sustained reproducibility and explicitly encour-

age replicating the experimental results of the published study [187, 188, 189,

190, 191]. In the nine major machine learning conferences, including NeurIPS,

ICML, ICLR, ACL-IJCNLP, EMNLP, CVPR, ICCV, AAAI, and IJCAI, the

criterion of reproducibility has been highly required in every peer-reviewed pro-

cess and published research paper [192, 193]. To establish which algorithm is

better for a task, it is an essential rule that any experiment or design of algo-

rithms should be trained and tested on the same datasets representing the task.

This dataset can be publicly available or published together with the ﬁrst paper

addressing this task. The evaluation metrics will be calculated using the same

formulas as the ﬁrst published paper. In case of using a new set of formulas, it

is necessary to re-test the model in the ﬁrst publication, applying the methods

of optimal search for the participants on this new set of formulas. Take an

example as follows, we have two algorithms to compare. Algorithm A is our

development, and algorithm B is proposed by previous research. The compari-

son results depend on how much documentation is publicly made available. For

example, if we only have access to the written documents as published articles,

we have to self-implement algorithm B and test it on the data we collect our-

selves. In fact, there is practically no way to verify that we have implemented

and conﬁgured the algorithm in precisely the same way as the original authors,

especially the essential optimal hyperparameter values. Thus, the more litera-

ture (articles, algorithmic code, and data) provided by the original authors, the

easier it is for independent researchers to reproduce and demonstrate the pub-

lished results that the claims are valid. We proceed with the problem further

regarding the above algorithm A and B. Suppose we want to test algorithm A

on the same published data set. In that case, the question is whether we have

41

to test algorithm B again to verify the correctness or accept the results reported

as comparative results? This is a relevant question because newly proposed al-

gorithms are often compared with published models developed by third parties

without re-testing. However, one scenario exists when algorithm B compares

itself with many previous algorithms, but the code is not publicly available.

And instead, later researchers often take reported results to compare and ac-

cept as fact. In addition, independent research experts have found it diﬃcult

to obtain similar results when re-implementing complete experiments reported

in the scientiﬁc literature if all-important parameters and hyperparameters are

missing. Computer science, speciﬁcally machine learning, is in a favorable situ-

ation where identical empirical procedures can be followed using the same data

sets. Although in this case, the biggest challenge is the lab, diﬀerent hard-

ware, and software where the experiments are conducted. Reproducibility is

also demonstrated by applying algorithms A and B on the same data but in

diﬀerent laboratory conﬁgurations to produce the same results.

Interest has

grown not only in the machine learning community but also in bioengineering

[194, 195, 196], biomedical engineering [197], biology [198], and genome editing

[199] regarding the reproducibility of published scientiﬁc results.

However, the reproducibility requirement for biological systems is much more

diﬃcult because data are extracted from living organisms, chemicals, and or-

ganic interactions, e.g., proteins and strains of cells. Even the biomass collected

during experiments in the same laboratory, on the same bioreactor, diﬀered by

the time of year or collected by diﬀerent technicians. This complicates eﬀorts

to apply approaches from the ﬁeld of machine learning, where data is more

stable and redundant. Furthermore, increasingly sophisticated bioengineering

tools are making cell biology experiments more complex. The time to conduct

biological experiments is also longer, leading to more complex reproducibility.

Thorough validation can take months or even years to complete. That makes it

diﬃcult for laboratories that are not equipped with modern equipment to repeat

experiments that more qualiﬁed laboratories have done. Instead, the biological

sciences depend on other less reliable techniques for reproduction, resulting in

42

publications that are less conditional on comparison with previous studies.

According to a Nature survey of 1576 researchers, M. Baker points out that

the scientiﬁc community has a general view that there is an ongoing regenerative

capacity crisis [200]. Surveys have shown that more than 70% of researchers

have tried and failed to reproduce other scientists’ experiments, more than 50%

have been unable to replicate their experiments, and more than 30% believe in

published results even though they acknowledge that published results may be

wrong. Many bioengineering professionals reuse machine learning as a complete

implementation on a particular computing platform. However, another study

even concluded that a machine learning platform does not guarantee immediate

reproducibility and that the test results generated from a machine learning

platform cannot be trusted entirely [201].

4.2. Proposed Research 1: Promote a Culture of Reproducibility in Bioengineer-

ing

In addition to the techniques and methodologies proposed in machine learn-

ing [202, 203, 204], we need to change the culture regarding research repro-

ducibility. We must encourage the practice of reproducibility and help subse-

quent researchers as a cornerstone of science. This means to make our research

better, more open, and thus, attracting follow-up studies to develop from the

current results. We need to fund and encourage individuals and research groups

to conﬁrm (or sometimes disprove) the ﬁndings of others with reproducible re-

sults. We should not criticize studies whose ﬁndings cannot be conﬁrmed. In

contrast, our work attempts to replicate highly reproducible studies, even if the

results are not precisely the same. Journals can even create a new criteria cate-

gory for assessing which research supports or integrates research reproducibility.

The study replication levels can be found in Figure 11.

4.3. Challenge 2: Benchmark Datasets and Evaluations of Bioengineering ap-

proaches

Within computer science, benchmarking is the development of guidelines

and best practices.

It contains three sub-ﬁelds: scientiﬁc machine learning

43

Figure 11: Reproducibility levels.

benchmark, application benchmark, and system benchmark [205]. Application

benchmark concerns the complete deployment of machine learning applications

using various hardware and software settings. The benchmark evaluates the use

of resources, e.g., ﬁlesystems, software libraries and versions, hardware conﬁgu-

ration, and scaling factor of computing capacity, that aﬀect the time-to-solution

of the application. System benchmark concentrates on the availability of a ma-

chine learning application in a broader environment. The system benchmark

evaluates network throughput and the number of ﬂoating-point operations per

second. These two benchmarking frameworks are not technically suitable for

bioprocess engineering. This article focuses on the machine learning benchmark

and how to promote it within bioprocess engineering research. The machine

learning benchmark is much simpler and easy to implement.

It requires two

subjects: datasets and reference models as presented in Figure 12. Firstly,

benchmark datasets which are the fundamental cornerstone of machine learning

should be made available to the research community. The exact training, valida-

tion, and test sets are on which all the reference implementation will be based.

Secondly, proposed approaches and state-of-the-art will be developed along and

44

considered blueprints for use on diﬀerent benchmark datasets. Thirdly, an excel-

lent overall design of the machine learning benchmark has fostered great boosters

for research and discussion of corresponding areas: out-of-the-box downloading

and usage, interoperability, ease of customization [206, 207]. Bioprocess engi-

neers understand the most suitable machine learning models by looking at the

benchmarking performance on the equivalent datasets and types of investigated

problems. More speciﬁcally, Bioprocess experts might refer a blossoming of

benchmarks on neural networks and applications [208, 209, 210, 211, 212, 213],

time series [214, 215, 216, 217, 218], image data [219, 96, 220], text-based source

[221, 222, 223], via community competition 1, 2, and many others [224, 225].

Figure 12: Components of machine learning benchmark.

4.4. Proposed Research 2: Comprehensive Construction of Bioprocess Engineer-

ing Benchmark

The development of standards for bioprocessing engineering is essential to

accelerate its growth while also attracting the participation of experts from

many other ﬁelds. As we discussed above and the lessons learned from the

machine learning community for the necessary of an ideal benchmark. More

speciﬁcally, the benchmark (1) should provide publicly available datasets, while

also providing standard procedures on those public datasets like typical machine

learning tasks, such as classiﬁcation, regression, and prediction; and (2) must be

1https://www.kaggle.com/competitions
2https://paperswithcode.com/

45

generic enough and easily integrated to accommodate diﬀerent bio-research en-

gineering pipelines. However, an important point that makes the bioprocessing

speciﬁcation more prominent and speciﬁc to its ﬁeld is the bioprocessing meta-

data [226, 227, 228, 229]. Take a look at the following example of experimental

veriﬁcation.

A laboratory, named A, performed a veriﬁcation experiment of four opti-

mally designed experiments (two were performed by a kinetic model and the

others by an artiﬁcial neural network) [230]. These experiments were performed

in a glass tubular photoreactor with a capacity of 1 L (length of 15.5 cm and

diameter of 9.5 cm). A technician attaches an artiﬁcial light source to opposite

sides of the reactor using 14 W TL 5 tungsten incandescent lamps, manufactured

by Philip Co., China). The experiments started with two hyperparameters of

biomass concentration and nitrate concentration set to 0.27 g/L and 9 mM, re-

spectively. The experiments also set two other hyperparameters, the inﬂuential

nitrate concentration, and the ﬁxed culture temperature of 0.1 M and 35 °C for

all experiments. Cultures were continuously aerated with 2.5% CO2 in air at

0.2 vvm and pH = 7.5 at a stirring rate of 300 rpm. The technician varied the

nitrate feeding rate and light intensity daily throughout the experiment.

Let’s assume that laboratory A releases the experimental datasets and ref-

erence model, e.g., an artiﬁcial neural network in this case. If the laboratory,

named B, is interested in the experiments and wants to improve its current

project with a similar veriﬁcation experiment. Then laboratory B must know

the exact experimental settings and conﬁguration such as the equipment, chem-

ical origin, spacial location of equipment installation, nitrate feeding rate log,

and other necessary metadata. Hense, the bioprocess engineering benchmark

should have the third component: bioprocess metadata as presented in Figure

13. Unfortunately, the bioprocess engineering literature witnesses not many

the variety of benchmark-ready published articles and dedicated benchmarking

[231, 232, 233, 234, 235].

46

Figure 13: Components of bioprocess engineering benchmark.

5. Conclusion

Bioprocessing engineering is involved in solving many of the world’s great-

est challenges, where ML integration has proven immensely useful in bioprocess

automation and reducing uncertainty in the decision-making process. However,

many barriers and challenges need to be recognized to enhance the impact of the

bio-industry. The authors conﬁrm that the increasing use of ML in bioengineer-

ing and bioprocess automation will continue to be accelerated soon. This com-

bination has been enabled by important technological advances, hardware, and

software, which are continuing to evolve. Automation in experimental biotech-

nology, both at individual laboratories and the level of interlaboratory biosyn-

thesis, coupled with robust decision-making, will be driven by better machine

learning models. It could spark the emergence of fully automated, continuous

bioprocessing techniques that do not depend on or limit human intervention.

Most of the success has come from applying ML approaches developed in other

domains directly to biological data sources. In this work, we review the existing

applications of machine learning and artiﬁcial intelligence in bioprocess develop-

ment from the perspective of making a faster and less costly development spiral.

We mostly focus on the existing tools that, in our opinion, have not been con-

sidered yet despite having a great potential to automate model building in the

diﬀerent stages to make the computational methods reproducible, which in turn

provides full provenance to decisions taken. We have summarized recent ML

implementation across several important subﬁelds of bioprocess systems and

47

raise two crucial challenges remaining the bottleneck of bioprocess automation

and reducing uncertainty in biotechnology development. Although it is an am-

bitious goal, the combination of bioprocess engineering and machine learning

is likely to yield many of the biggest developments in bioprocess in the coming

years. Broader Impact: over the next decade: making bioprocess systems FAIR

[236, 237] - Findable, Accessible, Interoperable, and Reusable.

6. Acknowledgments

The authors kindly appreciate support of the German Federal Ministry

of Education and Research through the Program ”International Future Labs

for Artiﬁcial Intelligence (Grant number 01DD20002A)”. We acknowledge the

Open Access Publication Fund of Technische Universit¨at Berlin.

References

[1] A. Waldbaur, J. Kittelmann, C. P. Radtke, J. Hubbuch, B. E. Rapp,

Microﬂuidics on liquid handling stations (µf-on-lhs): an industry compat-

ible chip interface between microﬂuidics and automated liquid handling

stations, Lab on a Chip 13 (12) (2013) 2337–2343.

[2] C. P. Radtke, M. Delb´e, M. W¨orner, J. Hubbuch, Photoinitiated

miniemulsion polymerization in microﬂuidic chips on automated liquid

handling stations: Proof of concept, Engineering in Life Sciences 16 (6)

(2016) 505–514.

[3] K. Treier, S. Hansen, C. Richter, P. Diederich, J. Hubbuch, P. Lester,

High-throughput methods for miniaturization and automation of mon-

oclonal antibody puriﬁcation processes, Biotechnology progress 28 (3)

(2012) 723–732.

[4] J. A. Reuter, D. V. Spacek, M. P. Snyder, High-throughput sequencing

technologies, Molecular cell 58 (4) (2015) 586–597.

48

[5] M. K¨ans¨akoski, M. Kurkinen, N. von Weymarn, P. Niemel¨a, P. Neubauer,

E. Juuso, T. Eerik¨ainen, S. Turunen, S. Aho, P. Suhonen, Process ana-

lytical technology (pat) needs and applications in the bioprocess industry,

VTT Technical Research Centre of Finland 60 (2006) 99.

[6] J. Glassey, K. Gernaey, C. Clemens, T. W. Schulz, R. Oliveira, G. Stried-

ner, C.-F. Mandenius, Process analytical technology (pat) for biopharma-

ceuticals, Biotechnology Journal 6 (4) (2011) 369–377.

[7] L. L. Simon, H. Pataki, G. Marosi, F. Meemken, K. Hungerbuhler,

A. Baiker, S. Tummala, B. Glennon, M. Kuentz, G. Steele, et al., Assess-

ment of recent process analytical technology (pat) trends: a multiauthor

review, Organic Process Research & Development 19 (1) (2015) 3–62.

[8] P. Diederich, J. Hubbuch, High-throughput column chromatography per-

formed on liquid handling stations, Preparative chromatography for sep-

aration of proteins 100 (2017) 293–332.

[9] T. Barz, A. Sommer, T. Wilms, P. Neubauer, M. N. C. Bournazou, Adap-

tive optimal operation of a parallel robotic liquid handling station, IFAC-

PapersOnLine 51 (2) (2018) 765–770.

[10] S. Hans, M. Gimpel, F. Glauche, P. Neubauer, M. N. Cruz-Bournazou, Au-

tomated cell treatment for competence and transformation of escherichia

coli in a high-throughput quasi-turbidostat using microtiter plates, Mi-

croorganisms 6 (3) (2018) 60.

[11] M. Koutinas, A. Kiparissides, E. N. Pistikopoulos, A. Mantalaris, Biopro-

cess systems engineering: transferring traditional process engineering prin-

ciples to industrial biotechnology, Computational and structural biotech-

nology journal 3 (4) (2012) e201210022.

[12] J. Lee, S. Y. Lee, S. Park, A. P. Middelberg, Control of fed-batch fermen-

tations, Biotechnology advances 17 (1) (1999) 29–48.

49

[13] M. N. Cruz Bournazou, T. Barz, D. Nickel, D. Lopez C´ardenas,

F. Glauche, A. Knepper, P. Neubauer, Online optimal experimental re-

design in robotic parallel fed-batch cultivation facilities, Biotechnology

and bioengineering 114 (3) (2017) 610–619.

[14] J. W. Kim, N. Krausch, J. Aizpuru, T. Barz, S. Lucia, E. C. Mart´ınez,

P. Neubauer, M. N. C. Bournazou, Model predictive control guided with

optimal experimental design for pulse-based parallel cultivation, arXiv

preprint arXiv:2112.10548.

[15] J. W. Kim, N. Krausch, J. Aizpuru, T. Barz, S. Lucia, P. Neubauer,

M. N. C. Bournazou, Model predictive control and moving horizon esti-

mation for adaptive optimal bolus feeding in high-throughput cultivation

of E. coli, arXiv preprint arXiv:2203.07211.

[16] N. Krausch, J. W. Kim, T. Barz, S. Lucia, S. Groß, M. Huber, S. Schiller,

P. Neubauer, M. C. Bournazou, High-throughput screening of optimal

process conditions using model predictive control, Authorea Preprints.

[17] M. Mowbray, T. Savage, C. Wu, Z. Song, B. A. Cho, E. A. Del Rio-

Chanona, D. Zhang, Machine learning for biochemical engineering: A

review, Biochemical Engineering Journal 172 (2021) 108054.

[18] C. E. Lawson, J. M. Mart´ı, T. Radivojevic, S. V. R. Jonnalagadda,

R. Gentz, N. J. Hillson, S. Peisert, J. Kim, B. A. Simmons, C. J. Petzold,

et al., Machine learning for metabolic engineering: A review, Metabolic

Engineering 63 (2021) 34–60.

[19] T. Scheper, S. Beutel, N. McGuinness, S. Heiden, M. Oldiges, F. Lammers,

K. F. Reardon, Digitalization and bioprocessing: Promises and challenges,

Digital Twins (2020) 57–69.

[20] H. Narayanan, M. F. Luna, M. von Stosch, M. N. Cruz Bournazou,

G. Polotti, M. Morbidelli, A. Butt´e, M. Sokolov, Bioprocessing in the dig-

50

ital age: the role of process models, Biotechnology journal 15 (1) (2020)

1900172.

[21] P. Neubauer, F. Glauche, M. N. Cruz-Bournazou, Bioprocess development

in the era of digitalization, Engineering in Life Sciences 17 (11) (2017)

1140.

[22] P. Neubauer, E. Anane, S. Junne, M. N. Cruz Bournazou, Potential of

integrating model-based design of experiments approaches and process

analytical technologies for bioprocess scale-down, Digital Twins (2020)

1–28.

[23] G.-W. Wei, Protein structure prediction beyond alphafold, Nature Ma-

chine Intelligence 1 (8) (2019) 336–337.

[24] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger,
K. Tunyasuvunakool, R. Bates, A. ˇZ´ıdek, A. Potapenko, et al., Highly

accurate protein structure prediction with alphafold, Nature 596 (7873)

(2021) 583–589.

[25] L. Kaspersetz, S. Waldburger, M.-T. Schermeyer, S. L. Riedel, S. Gross,

P. Neubauer, M.-N. Cruz-Bournazou, Automated bioprocess feedback op-

eration in a high throughput facility via the integration of a mobile robotic

lab assistant, bioRxiv.

[26] D. Sch¨onberger, Deep copyright: up-and downstream questions related
to artiﬁcial intelligence (ai) and machine learning (ml), SCH ¨ONBERGER

Daniel, Deep Copyright: Up-and Downstream-Questions Related to Ar-

tiﬁcial Intelligence (AI) and Machine Learning (ML) in Droit d’auteur 4

(2018) 145–173.

[27] S. Haque, S. Khan, M. Wahid, S. A. Dar, N. Soni, R. K. Mandal, V. Singh,

D. Tiwari, M. Lohani, M. Y. Areeshi, et al., Artiﬁcial intelligence vs.

statistical modeling and optimization of continuous bead milling process

for bacterial cell lysis, Frontiers in microbiology 7 (2016) 1852.

51

[28] C. Walther, M. Voigtmann, E. Bruna, A. Abusnina, A.-L. Tscheließnig,

M. Allmer, H. Schuchnigg, C. Brocard, A. F¨ottinger-Vacha, G. Klima,

Smart process development: Application of machine-learning and inte-

grated process modeling for inclusion body puriﬁcation processes, Biotech-

nology Progress (2022) e3249.

[29] P. Petsagkourakis, I. O. Sandoval, E. Bradford, D. Zhang, E. A. del Rio-

Chanona, Reinforcement learning for batch bioprocess optimization, Com-

puters & Chemical Engineering 133 (2020) 106649.

[30] S. Mazurenko, Z. Prokop, J. Damborsky, Machine learning in enzyme

engineering, ACS Catalysis 10 (2) (2019) 1210–1223.

[31] D. Heckmann, C. J. Lloyd, N. Mih, Y. Ha, D. C. Zielinski, Z. B. Haiman,

A. A. Desouki, M. J. Lercher, B. O. Palsson, Machine learning applied

to enzyme turnover numbers reveals protein structural correlates and im-

proves metabolic models, Nature communications 9 (1) (2018) 1–10.

[32] J.-X. Tan, H. Lv, F. Wang, F.-Y. Dao, W. Chen, H. Ding, A survey for

predicting enzyme family classes using machine learning methods, Current

drug targets 20 (5) (2019) 540–550.

[33] T. Barz, J. Kager, C. Herwig, P. Neubauer, M. N. C. Bournazou, F. Gal-

vanin, Characterization of reactions and growth in automated continu-

ous ﬂow and bioreactor platforms—from linear doe to model-based ap-

proaches, in: Simulation and Optimization in Process Engineering, Else-

vier, 2022, pp. 273–319.

[34] N. Borisov, V. Tkachev, I. Muchnik, A. Buzdin, Individual drug treatment

prediction in oncology based on machine learning using cell culture gene

expression data, in: Proceedings of the 2017 International Conference on

Computational Biology and Bioinformatics, 2017, pp. 1–6.

[35] M. Ashraf, M. Khalilitousi, Z. Laksman, Applying machine learning to

stem cell culture and diﬀerentiation, Current Protocols 1 (9) (2021) e261.

52

[36] V. Venkatasubramanian, The promise of artiﬁcial intelligence in chemical

engineering: Is it here, ﬁnally, AIChE J 65 (2) (2019) 466–478.

[37] S. K. Niazi, J. L. Brown, Fundamentals of modern bioprocessing, CRC

Press, 2017.

[38] P. Neubauer, M. N. Cruz-Bournazou, Continuous bioprocess development:

methods for control and characterization of the biological system, Con-

tinuous Biomanufacturing: Innovative Technologies and Methods; John

Wiley & Sons: Hoboken, NJ, USA.

[39] F. Hutter, L. Kotthoﬀ, J. Vanschoren, Automated machine learning:

methods, systems, challenges, Springer Nature, 2019.

[40] M. Mohammed, M. B. Khan, E. B. M. Bashier, Machine learning: algo-

rithms and applications, Crc Press, 2016.

[41] K. P. Murphy, Probabilistic machine learning: an introduction, MIT press,

2022.

[42] S. Sra, S. Nowozin, S. J. Wright, Optimization for machine learning, Mit

Press, 2012.

[43] M. Feurer, F. Hutter, Hyperparameter optimization, in: Automated ma-

chine learning, Springer, Cham, 2019, pp. 3–33.

[44] L. Yang, A. Shami, On hyperparameter optimization of machine learning

algorithms: Theory and practice, Neurocomputing 415 (2020) 295–316.

[45] T. Chen, C. Guestrin, Xgboost: A scalable tree boosting system, in: Pro-

ceedings of the 22nd acm sigkdd international conference on knowledge

discovery and data mining, 2016, pp. 785–794.

[46] R. Mitchell, E. Frank, Accelerating the xgboost algorithm using gpu com-

puting, PeerJ Computer Science 3 (2017) e127.

53

[47] T. Chen, W. Zhang, Z. Jingyang, S. Chang, S. Liu, L. Amini, Z. Wang,

Training stronger baselines for learning to optimize, Advances in Neural

Information Processing Systems 33 (2020) 7332–7343.

[48] W. Chung, V. Thomas, M. C. Machado, N. Le Roux, Beyond variance

reduction: Understanding the true impact of baselines on policy optimiza-

tion, in: International Conference on Machine Learning, PMLR, 2021, pp.

1999–2009.

[49] M. S. Mahmud, J. Z. Huang, S. Salloum, T. Z. Emara, K. Sadatdiynov,

A survey of data partitioning and sampling methods to support big data

analysis, Big Data Mining and Analytics 3 (2) (2020) 85–101.

[50] N. Bussola, A. Marcolini, V. Maggio, G. Jurman, C. Furlanello, Ai slipping

on tiles: Data leakage in digital pathology, in: International Conference

on Pattern Recognition, Springer, 2021, pp. 167–182.

[51] P. Riley, Three pitfalls to avoid in machine learning (2019).

[52] D. Chicco, Ten quick tips for machine learning in computational biology,

BioData mining 10 (1) (2017) 1–17.

[53] J. G. Greener, S. M. Kandathil, L. Moﬀat, D. T. Jones, A guide to ma-

chine learning for biologists, Nature Reviews Molecular Cell Biology 23 (1)

(2022) 40–55.

[54] S. M. Wheelwright, Economic and cost factors of bioprocess engineering,

in: Biotechnology and Biopharmaceutical Manufacturing, Processing, and

Preservation, CRC Press, 2020, pp. 333–354.

[55] K. S. Ng, J. A. Smith, M. P. McAteer, B. E. Mead, J. Ware, F. O. Jackson,

A. Carter, L. Ferreira, K. Bure, J. A. Rowley, et al., Bioprocess decision

support tool for scalable manufacture of extracellular vesicles, Biotechnol-

ogy and bioengineering 116 (2) (2019) 307–319.

54

[56] L. Nanni, S. Brahnam, S. Ghidoni, A. Lumini, Bioimage classiﬁcation

with handcrafted and learned features, IEEE/ACM transactions on com-

putational biology and bioinformatics 16 (3) (2018) 874–885.

[57] R. Ashmore, R. Calinescu, C. Paterson, Assuring the machine learning

lifecycle: Desiderata, methods, and challenges, ACM Computing Surveys

(CSUR) 54 (5) (2021) 1–39.

[58] F. Kumeno, Sofware engneering challenges for machine learning applica-

tions: A literature review, Intelligent Decision Technologies 13 (4) (2019)

463–476.

[59] H. Luu, Managing the machine learning life cycle, in: Beginning Apache

Spark 3, Springer, 2021, pp. 395–429.

[60] M. Zaharia, A. Chen, A. Davidson, A. Ghodsi, S. A. Hong, A. Konwinski,

S. Murching, T. Nykodym, P. Ogilvie, M. Parkhe, et al., Accelerating

the machine learning lifecycle with mlﬂow., IEEE Data Eng. Bull. 41 (4)

(2018) 39–45.

[61] A. S. Rathore, S. Mishra, S. Nikita, P. Priyanka, Bioprocess control: cur-

rent progress and future perspectives, Life 11 (6) (2021) 557.

[62] N. Habibi, S. Z. M. Hashim, A. Norouzi, M. R. Samian, A review of

machine learning methods to predict the solubility of overexpressed re-

combinant proteins in escherichia coli, BMC bioinformatics 15 (1) (2014)

1–16.

[63] F. Mey, J. Clauwaert, K. Van Huﬀel, W. Waegeman, M. De Mey, Improv-

ing the performance of machine learning models for biotechnology: The

quest for deus ex machina, Biotechnology advances 53 (2021) 107858.

[64] S. Panjwani, I. Cui, K. Spetsieris, M. Mleczko, W. Wang, J. X. Zou,

M. Anwaruzzaman, S. Liu, R. Canales, O. Hesse, Application of machine

learning methods to pathogen safety evaluation in biological manufactur-

ing processes, Biotechnology Progress 37 (3) (2021) e3135.

55

[65] B. Settles, Active Learning, Synthesis Lectures on Artiﬁcial Intelligence

and Machine Learning Series, Morgan & Claypool, 2012.

URL https://books.google.de/books?id=z7toC3z_QjQC

[66] D. D. Lewis, W. A. Gale, A Sequential Algorithm for Training Text Clas-

siﬁers, in: B. W. Croft, C. J. van Rijsbergen (Eds.), SIGIR ’94, Springer

London, London, 1994, pp. 3–12.

[67] B. Settles, Active Learning Literature Survey 67.

[68] W. H. Beluch, T. Genewein, A. Nurnberger, J. M. Kohler, The Power

of Ensembles for Active Learning in Image Classiﬁcation,

in:

2018

IEEE/CVF Conference on Computer Vision and Pattern Recognition,

IEEE, Salt Lake City, UT, 2018, pp. 9368–9377. doi:10.1109/CVPR.

2018.00976.

URL https://ieeexplore.ieee.org/document/8579074/

[69] N. Roy, A. McCallum, Toward optimal active learning through sampling

estimation of error reduction, in: ICML, 2001.

[70] K. Konyushkova, R. Sznitman, P. Fua, Learning Active Learning from

Data 11.

[71] J. T. Ash, S. Goel, Gone Fishing: Neural Active Learning with Fisher

Embeddings 13.

[72] A. Foster, D. R. Ivanova, I. Malik, T. Rainforth, Deep Adaptive Design:

Amortizing Sequential Bayesian Experimental Design 12.

[73] A. Foster, M. Jankowiak, M. O’Meara, Y. W. Teh, T. Rainforth, A Uni-

ﬁed Stochastic Gradient Approach to Designing Bayesian-Optimal Exper-

iments 10.

[74] A. Foster, M. Jankowiak, E. Bingham, P. Horsfall, Y. W. Teh, T. Rain-

forth, N. Goodman, Variational Bayesian Optimal Experimental Design

12.

56

[75] S. Kleinegesse, M. U. Gutmann, Bayesian Experimental Design for Im-

plicit Models by Mutual Information Neural Estimation, arXiv:2002.08129

[cs, stat]ArXiv: 2002.08129.

URL http://arxiv.org/abs/2002.08129

[76] S. Kleinegesse, M. Gutmann, Eﬃcient Bayesian Experimental Design for

Implicit Models, arXiv:1810.09912 [cs, stat]ArXiv: 1810.09912.

URL http://arxiv.org/abs/1810.09912

[77] D. R. Ivanova, A. Foster, S. Kleinegesse, M. U. Gutmann, T. Rainforth,

Implicit Deep Adaptive Design: Policy-Based Experimental Design with-

out Likelihoods, arXiv:2111.02329 [cs, stat]ArXiv: 2111.02329.

URL http://arxiv.org/abs/2111.02329

[78] T.-T. Vu, M. Liu, D. Phung, G. Haﬀari, Learning How to Active Learn

by Dreaming 11.

[79] M. Fang, Y. Li, T. Cohn, Learning how to Active Learn: A Deep Re-

inforcement Learning Approach, in: Proceedings of the 2017 Conference

on Empirical Methods in Natural Language Processing, Association for

Computational Linguistics, Copenhagen, Denmark, 2017, pp. 595–605.

doi:10.18653/v1/D17-1063.

URL http://aclweb.org/anthology/D17-1063

[80] M. Liu, W. Buntine, G. Haﬀari, Learning How to Actively Learn: A Deep

Imitation Learning Approach, in: Proceedings of the 56th Annual Meet-

ing of the Association for Computational Linguistics (Volume 1: Long Pa-

pers), Association for Computational Linguistics, Melbourne, Australia,

2018, pp. 1874–1883. doi:10.18653/v1/P18-1174.

URL http://aclweb.org/anthology/P18-1174

[81] P. Bachman, A. Sordoni, A. Trischler, Learning Algorithms for Active

Learning 10.

57

[82] R. S. Sutton, A. G. Barto, Reinforcement learning: An introduction, MIT

press, 2018.

[83] R. D. King, K. E. Whelan, F. M. Jones, P. G. K. Reiser, C. H. Bryant,

S. H. Muggleton, D. B. Kell, S. G. Oliver, Functional genomic hypothesis

generation and experimentation by a robot scientist, Nature 427 (6971)

(2004) 247–252. doi:10.1038/nature02236.

URL http://www.nature.com/articles/nature02236

[84] C. H. Bryant, S. H. Muggleton, S. G. Oliver, D. B. Kell, P. Reiser,

R. D. King, Combining Inductive Logic Programming, Active Learning

and Robotics to Discover the Function of Genes 45.

[85] Y. Gal, Uncertainty in deep learning, University of Cambridge.

[86] E. Tsymbalov, M. Panov, A. Shapeev, Dropout-based Active Learning

for Regression, arXiv:1806.09856 [cs, stat] 11179 (2018) 247–258, arXiv:

1806.09856. doi:10.1007/978-3-030-11027-7_24.

URL http://arxiv.org/abs/1806.09856

[87] K. Weiss, T. M. Khoshgoftaar, D. Wang, A survey of transfer learning,

Journal of Big Data 3 (1) (2016) 9.

[88] M. Rostami, Transfer Learning Through Embedding Spaces, CRC Press,

2021.

[89] M. Long, H. Zhu, J. Wang, M. I. Jordan, Deep transfer learning with joint

adaptation networks, in: International conference on machine learning,

PMLR, 2017, pp. 2208–2217.

[90] N. Duong-Trung, L.-D. Quach, C.-N. Nguyen, Learning deep transferabil-

ity for several agricultural classiﬁcation problems, International Journal

of Advanced Computer Science and Applications 10 (1).

[91] L. Torrey, J. Shavlik, Transfer learning, in: Handbook of Research on

Machine Learning Applications and Trends: Algorithms, Methods, and

Techniques, IGI Global, 2010, pp. 242–264.

58

[92] M. Christopher, A. Belghith, C. Bowd, J. A. Proudfoot, M. H. Gold-

baum, R. N. Weinreb, C. A. Girkin, J. M. Liebmann, L. M. Zangwill,

Performance of deep learning architectures and transfer learning for de-

tecting glaucomatous optic neuropathy in fundus photographs, Scientiﬁc

reports 8 (1) (2018) 16685.

[93] N. Duong-Trung, L.-D. Quach, M.-H. Nguyen, C.-N. Nguyen, Classiﬁca-

tion of grain discoloration via transfer learning and convolutional neural

networks, in: Proceedings of the 3rd International Conference on Machine

Learning and Soft Computing, 2019, pp. 27–32.

[94] N. Duong-Trung, L.-D. Quach, M.-H. Nguyen, C.-N. Nguyen, A combi-

nation of transfer learning and deep learning for medicinal plant clas-

siﬁcation, in: Proceedings of the 2019 4th International Conference on

Intelligent Information Technology, 2019, pp. 83–90.

[95] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet: A

large-scale hierarchical image database.

[96] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,

A. Karpathy, A. Khosla, M. Bernstein, et al., Imagenet large scale visual

recognition challenge, International journal of computer vision 115 (3)

(2015) 211–252.

[97] N. Duong-Trung, D. N. Le Ha, H. X. Huynh, Classiﬁcation-segmentation

pipeline for mri via transfer learning and residual networks., in: Interna-

tional Conference on Research in Intelligent Computing in Engineering,

Annals of Computer Science and Information Systems, 2021, pp. 39–43.

[98] A. C. Tran, N. C. Tran, N. Duong-Trung, Recognition and quantity esti-

mation of pastry images using pre-training deep convolutional networks,

in: International Conference on Future Data and Security Engineering,

Springer, 2020, pp. 200–214.

59

[99] N. Duong-Trung, L.-D. Quach, C.-N. Nguyen, Towards classiﬁcation of

shrimp diseases using transferred convolutional neural networks, Advances

in Science, Technology and Engineering Systems Journal 5 (4) (2020) 724–

732.

[100] B. Neyshabur, H. Sedghi, C. Zhang, What is being transferred in transfer

learning?, Advances in neural information processing systems 33 (2020)

512–523.

[101] S. Mahajan, A. Raina, X.-Z. Gao, A. Kant Pandit, Plant recognition

using morphological feature extraction and transfer learning over svm and

adaboost, Symmetry 13 (2) (2021) 356.

[102] M. Izadpanahkakhk, S. M. Razavi, M. Taghipour-Gorjikolaie, S. H. Za-

hiri, A. Uncini, Deep region of interest and feature extraction models for

palmprint veriﬁcation using convolutional neural networks transfer learn-

ing, Applied Sciences 8 (7) (2018) 1210.

[103] M. V. Narkhede, P. P. Bartakke, M. S. Sutaone, A review on weight

initialization strategies for neural networks, Artiﬁcial intelligence review

55 (1) (2022) 291–322.

[104] D. Arpit, V. Campos, Y. Bengio, How to initialize your network? robust

initialization for weightnorm & resnets, Advances in Neural Information

Processing Systems 32.

[105] S. M. Safdarnejad, J. D. Hedengren, N. R. Lewis, E. L. Haseltine, Ini-

tialization strategies for optimization of dynamic systems, Computers &

Chemical Engineering 78 (2015) 39–50.

[106] M. Huh, P. Agrawal, A. A. Efros, What makes imagenet good for transfer

learning?, arXiv preprint arXiv:1608.08614.

[107] H. Yoo, H. E. Byun, D. Han, J. H. Lee, Reinforcement learning for batch

process control: Review and perspectives, Annual Reviews in Control 52

(2021) 108–119.

60

[108] D. P. Bertsekas, Dynamic programming and optimal control, Vol. 1,

Athena Scientiﬁc Belmont, MA, 2005.

[109] L. Busoniu, R. Babuska, B. De Schutter, D. Ernst, Reinforcement learn-

ing and dynamic programming using function approximators, CRC press,

2017.

[110] J. W. Kim, B. J. Park, H. Yoo, T. H. Oh, J. H. Lee, J. M. Lee, A model-

based deep reinforcement learning method applied to ﬁnite-horizon opti-

mal control of nonlinear control-aﬃne system, Journal of Process Control

87 (2020) 166–178.

[111] J. W. Kim, T. H. Oh, S. H. Son, D. H. Jeong, J. M. Lee, Convergence

analysis of the deep neural networks based globalized dual heuristic pro-

gramming, Automatica 122 (2020) 109222.

[112] J. M. Lee, J. H. Lee, Approximate dynamic programming-based ap-

proaches for input–output data-driven control of nonlinear processes, Au-

tomatica 41 (7) (2005) 1281–1288.

[113] J. W. Kim, G. B. Choi, J. M. Lee, A POMDP framework for integrated

scheduling of infrastructure maintenance and inspection, Computers &

Chemical Engineering 112 (2018) 239–252.

[114] T. H. Oh, J. W. Kim, S. H. Son, H. Kim, K. Lee, J. M. Lee, Automatic

control of simulated moving bed process with deep Q-network, Journal of

Chromatography A 1647 (2021) 462073.

[115] J. Horwood, E. Noutahi, Molecular design in synthetically accessible

chemical space via deep reinforcement learning, ACS omega 5 (51) (2020)

32984–32994.

[116] G. Novati, H. L. de Laroussilhe, P. Koumoutsakos, Automating turbu-

lence modelling by multi-agent reinforcement learning, Nature Machine

Intelligence 3 (1) (2021) 87–96.

61

[117] S. Levine, C. Finn, T. Darrell, P. Abbeel, End-to-end training of deep

visuomotor policies, The Journal of Machine Learning Research 17 (1)

(2016) 1334–1373.

[118] G. Williams, P. Drews, B. Goldfain, J. M. Rehg, E. A. Theodorou, Aggres-

sive driving with model predictive path integral control, in: 2016 IEEE

International Conference on Robotics and Automation (ICRA), IEEE,

2016, pp. 1433–1440.

[119] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,

A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al., Mastering the

game of Go without human knowledge, Nature 550 (7676) (2017) 354.

[120] S. Lucia, A. T˘atulea-Codrean, C. Schoppmeyer, S. Engell, Rapid devel-

opment of modular and sustainable nonlinear model predictive control

solutions, Control Engineering Practice 60 (2017) 51–62.

[121] L. Bu¸soniu, T. de Bruin, D. Toli´c, J. Kober, I. Palunko, Reinforcement

learning for control: Performance, stability, and deep approximators, An-

nual Reviews in Control.

[122] J. M. Lee, N. S. Kaisare, J. H. Lee, Choice of approximator and design of

penalty function for an approximate dynamic programming based control

approach, Journal of Process Control 16 (2) (2006) 135–156.

[123] J. M. Lee, J. H. Lee, An approximate dynamic programming based ap-

proach to dual adaptive control, Journal of process control 19 (5) (2009)

859–864.

[124] J. Wilson, E. Martinez, Neuro-fuzzy modeling and control of a batch pro-

cess involving simultaneous reaction and distillation, Computers & chem-

ical engineering 21 (1997) S1233–S1238.

[125] C. V. Peroni, N. S. Kaisare, J. H. Lee, Optimal control of a fed-batch biore-

actor using simulation-based approximate dynamic programming, IEEE

Transactions on Control Systems Technology 13 (5) (2005) 786–790.

62

[126] D. Li, L. Qian, Q. Jin, T. Tan, Reinforcement learning control with adap-

tive gain for a Saccharomyces cerevisiae fermentation process, Applied

Soft Computing 11 (8) (2011) 4488–4495.

[127] B. J. Pandian, M. M. Noel, Control of a bioreactor using a new partially

supervised reinforcement learning algorithm, Journal of Process Control

69 (2018) 16–29.

[128] Y. Ma, D. A. Nore˜na-Caro, A. J. Adams, T. B. Brentzel, J. A. Romagnoli,

M. G. Benton, Machine-learning-based simulation and fed-batch control of

cyanobacterial-phycocyanin production in Plectonema by artiﬁcial neural

network and deep reinforcement learning, Computers & Chemical Engi-

neering 142 (2020) 107016.

[129] P. Henderson, R.

Islam, P. Bachman,

J. Pineau, D. Precup,

D. Meger, Deep reinforcement learning that matters, arXiv preprint

arXiv:1709.06560.

[130] S. Fujimoto, H. Van Hoof, D. Meger, Addressing function approximation

error in actor-critic methods, arXiv preprint arXiv:1802.09477.

[131] B. Recht, A tour of reinforcement learning: The view from continuous

control, Annual Review of Control, Robotics, and Autonomous Systems

2 (2019) 253–279.

[132] E. Langlois, S. Zhang, G. Zhang, P. Abbeel, J. Ba, Benchmarking model-

based reinforcement learning, arXiv preprint arXiv:1907.02057.

[133] J. W. Kim, B. J. Park, T. H. Oh, J. M. Lee, Model-based reinforcement

learning and predictive control for two-stage optimal control of fed-batch

bioreactor, Computers & Chemical Engineering 154 (2021) 107465.

[134] T. H. Oh, H. M. Park, J. W. Kim, J. M. Lee, Integration of reinforcement

learning and model predictive control to optimize semi-batch bioreactor,

AIChE Journal (2022) e17658.

63

[135] E. Pan, P. Petsagkourakis, M. Mowbray, D. Zhang, E. A. del Rio-Chanona,

Constrained model-free reinforcement learning for process optimization,

Computers & Chemical Engineering 154 (2021) 107462.

[136] P. Petsagkourakis, I. O. Sandoval, E. Bradford, F. Galvanin, D. Zhang,

E. A. del Rio-Chanona, Chance constrained policy optimization for pro-

cess control and optimization, Journal of Process Control 111 (2022) 35–

45.

[137] M. Mowbray, P. Petsagkourakis, E. A. del Rio-Chanona, D. Zhang, Safe

chance constrained reinforcement learning for batch process control, Com-

puters & Chemical Engineering 157 (2022) 107630.

[138] K. Xiao, S. Liang, X. Wang, C. Chen, X. Huang, Current state and chal-

lenges of full-scale membrane bioreactor applications: A critical review,

Bioresource technology 271 (2019) 473–481.

[139] K. Sode, T. Yamazaki, I. Lee, T. Hanashi, W. Tsugawa, Biocapacitor:

A novel principle for biosensors, Biosensors and Bioelectronics 76 (2016)

20–28.

[140] B. Dai, L. Wang, Y. Wang, G. Yu, X. Huang, Single-cell nanometric coat-

ing towards whole-cell-based biodevices and biosensors, ChemistrySelect

3 (25) (2018) 7208–7221.

[141] S. Pradhan, A. Brooks, V. Yadavalli, Nature-derived materials for the

fabrication of functional biodevices, Materials Today Bio 7 (2020) 100065.

[142] P. Mehrotra, Biosensors and their applications–a review, Journal of oral

biology and craniofacial research 6 (2) (2016) 153–159.

[143] J. Ong, M. R. Appleford, G. Mani, Introduction to biomaterials: basic

theory with engineering applications, Cambridge University Press, 2014.

[144] M.-C. Tanzi, S. Far`e, G. Candiani, Foundations of biomaterials engineer-

ing, Academic Press, 2019.

64

[145] V. Dos Santos, R. N. Brandalise, M. Savaris, Engineering of biomaterials,

Springer, 2017.

[146] B. M. Woolston, S. Edgar, G. Stephanopoulos, Metabolic engineering:

past and future, Annual review of chemical and biomolecular engineering

4 (2013) 259–288.

[147] T. U. Chae, S. Y. Choi, J. W. Kim, Y.-S. Ko, S. Y. Lee, Recent advances

in systems metabolic engineering tools and strategies, Current opinion in

biotechnology 47 (2017) 67–82.

[148] K. V. Presnell, H. S. Alper, Systems metabolic engineering meets machine

learning: A new era for data-driven metabolic engineering, Biotechnology

journal 14 (9) (2019) 1800416.

[149] M. Banner, H. Alosert, C. Spencer, M. Cheeks, S. S. Farid, M. Thomas,

S. Goldrick, A decade in review: use of data analytics within the biophar-

maceutical sector, Current Opinion in Chemical Engineering 34 (2021)

100758.

[150] H. Le, S. Kabbur, L. Pollastrini, Z. Sun, K. Mills, K. Johnson, G. Karypis,

W.-S. Hu, Multivariate analysis of cell culture bioprocess data—lactate

consumption as process indicator, Journal of biotechnology 162 (2-3)

(2012) 210–223.

[151] L. Wei, Z. Yuan, M. Cui, H. Han, J. Shen, Study on electricity-generation

characteristic of two-chambered microbial fuel cell in continuous ﬂow

mode, International journal of hydrogen energy 37 (1) (2012) 1067–1073.

[152] A. Garg, V. Vijayaraghavan, S. Mahapatra, K. Tai, C. Wong, Performance

evaluation of microbial fuel cell by artiﬁcial intelligence methods, Expert

systems with applications 41 (4) (2014) 1389–1399.

[153] E. A. del Rio-Chanona, F. Fiorelli, D. Zhang, N. R. Ahmed, K. Jing,

N. Shah, An eﬃcient model construction strategy to simulate microalgal

65

lutein photo-production dynamic process, Biotechnology and Bioengineer-

ing 114 (11) (2017) 2518–2527.

[154] E. Bradford, A. M. Schweidtmann, D. Zhang, K. Jing, E. A. del Rio-

Chanona, Dynamic modeling and optimization of sustainable algal pro-

duction with uncertainty using multivariate gaussian processes, Comput-

ers & Chemical Engineering 118 (2018) 143–158.

[155] E. A. del Rio-Chanona, E. Manirafasha, D. Zhang, Q. Yue, K. Jing, Dy-

namic modeling and optimization of cyanobacterial c-phycocyanin pro-

duction process by artiﬁcial neural network, Algal Research 13 (2016)

7–15.

[156] E. A. del Rio-Chanona, J. L. Wagner, H. Ali, F. Fiorelli, D. Zhang, K. Hell-

gardt, Deep learning-based surrogate modeling and optimization for mi-

croalgal biofuel production and photobioreactor design, AIChE Journal

65 (3) (2019) 915–923.

[157] E. Bradford, L. Imsland, D. Zhang, E. A. del Rio Chanona, Stochastic

data-driven model predictive control using gaussian processes, Computers

& Chemical Engineering 139 (2020) 106844.

[158] S. Han, T. Kim, D. Kim, Y.-L. Park, S. Jo, Use of deep learning for char-

acterization of microﬂuidic soft sensors, IEEE Robotics and Automation

Letters 3 (2) (2018) 873–880.

[159] C. C. Horgan, M. Jensen, A. Nagelkerke, J.-P. St-Pierre, T. Vercauteren,

M. M. Stevens, M. S. Bergholt, High-throughput molecular imaging via

deep-learning-enabled raman spectroscopy, Analytical chemistry 93 (48)

(2021) 15850–15860.

[160] C. Banbury, R. Mason, I. Styles, N. Eisenstein, M. Clancy, A. Belli, A. Lo-

gan, P. Goldberg Oppenheimer, Development of the self optimising koho-

nen index network (skinet) for raman spectroscopy based detection of

anatomical eye tissue, Scientiﬁc reports 9 (1) (2019) 1–9.

66

[161] C. Banbury, I. Styles, N. Eisenstein, E. R. Zanier, G. Vegliante, A. Belli,

A. Logan, P. G. Oppenheimer, Spectroscopic detection of traumatic brain

injury severity and biochemistry from the retina, Biomedical optics ex-

press 11 (11) (2020) 6249–6261.

[162] A. Tardast, M. Rahimnejad, G. Najafpour, A. Ghoreyshi, G. C. Premier,

G. Bakeri, S.-E. Oh, Use of artiﬁcial neural network for the prediction of

bioelectricity production in a membrane less microbial fuel cell, Fuel 117

(2014) 697–703.

[163] F. Fang, G.-L. Zang, M. Sun, H.-Q. Yu, Optimizing multi-variables of

microbial fuel cell for electricity generation with an integrated modeling

and experimental approach, Applied energy 110 (2013) 98–103.

[164] D. B. Miracle, O. N. Senkov, A critical review of high entropy alloys and

related concepts, Acta Materialia 122 (2017) 448–511.

[165] W. Huang, P. Martin, H. L. Zhuang, Machine-learning phase prediction

of high-entropy alloys, Acta Materialia 169 (2019) 225–236.

[166] P. Asgharzadeh, A. I. Birkhold, Z. Trivedi, B. ¨Ozdemir, R. Reski,

O. R¨ohrle, A nanofe simulation-based surrogate machine learning model

to predict mechanical functionality of protein networks from live confocal

imaging, Computational and structural biotechnology journal 18 (2020)

2774–2788.

[167] Mof mechanical properties explorer: Adsorption advanced materials

group, university of cambridge (2019).

URL

http://aam.ceb.cam.ac.uk/mof-explorer/

mechanicalproperties/

[168] P. Z. Moghadam, S. M. Rogge, A. Li, C.-M. Chow, J. Wieme, N. Mo-

harrami, M. Aragones-Anglada, G. Conduit, D. A. Gomez-Gualdron,

V. Van Speybroeck, et al., Structure-mechanical stability relations of

67

metal-organic frameworks via machine learning, Matter 1 (1) (2019) 219–

234.

[169] M. Sarmadi, A. M. Behrens, K. J. McHugh, H. T. Contreras, Z. L. Tochka,

X. Lu, R. Langer, A. Jaklenec, Modeling, design, and machine learning-

based framework for optimal injectability of microparticle-based drug for-

mulations, Science advances 6 (28) (2020) eabb6594.

[170] A. B. Farimani, M. Heiranian, N. R. Aluru, Identiﬁcation of amino acids

with sensitive nanoporous mos2: towards machine learning-based predic-

tion, Nat. 2D Mater 2.

[171] F. Tourlomousis, C. Jia, T. Karydis, A. Mershin, H. Wang, D. M. Kalyon,

R. C. Chang, Machine learning metrology of cell conﬁnement in melt

electrowritten three-dimensional biomaterial substrates, Microsystems &

nanoengineering 5 (1) (2019) 1–19.

[172] S. You, J. Guan, J. Alido, H. H. Hwang, R. Yu, L. Kwe, H. Su, S. Chen,

Mitigating scattering eﬀects in light-based three-dimensional printing us-

ing machine learning, Journal of Manufacturing Science and Engineering

142 (8) (2020) 081002.

[173] Z. A. King, J. Lu, A. Dr¨ager, P. Miller, S. Federowicz, J. A. Lerman,

A. Ebrahim, B. O. Palsson, N. E. Lewis, Bigg models: A platform for

integrating, standardizing and sharing genome-scale models, Nucleic acids

research 44 (D1) (2016) D515–D522.

[174] T. Oyetunde, M. Zhang, Y. Chen, Y. Tang, C. Lo, Boostgapﬁll: improv-

ing the ﬁdelity of metabolic network reconstructions through integrated

constraint and pattern-based methods, Bioinformatics 33 (4) (2017) 608–

611.

[175] J. Alonso-Gutierrez, E.-M. Kim, T. S. Batth, N. Cho, Q. Hu, L. J. G.

Chan, C. J. Petzold, N. J. Hillson, P. D. Adams, J. D. Keasling, et al.,

68

Principal component analysis of proteomics (pcap) as a tool to direct

metabolic engineering, Metabolic engineering 28 (2015) 123–133.

[176] S. G. Wu, Y. Wang, W. Jiang, T. Oyetunde, R. Yao, X. Zhang,

K. Shimizu, Y. J. Tang, F. S. Bao, Rapid prediction of bacterial het-

erotrophic ﬂuxomics using machine learning and constraint programming,

PLoS computational biology 12 (4) (2016) e1004838.

[177] S. Nandi, A. Subramanian, R. R. Sarkar, An integrative machine learn-

ing strategy for improved prediction of essential genes in escherichia coli

metabolism using ﬂux-coupled features, Molecular BioSystems 13 (8)

(2017) 1584–1596.

[178] I. Schomburg, A. Chang, C. Ebeling, M. Gremse, C. Heldt, G. Huhn,

D. Schomburg, Brenda, the enzyme database: updates and major new

developments, Nucleic acids research 32 (suppl 1) (2004) D431–D433.

[179] R. Liu, M. C. Bassalo, R. I. Zeitoun, R. T. Gill, Genome scale engineer-

ing techniques for metabolic engineering, Metabolic engineering 32 (2015)

143–154.

[180] N. J. Treloar, A. J. Fedorec, B. Ingalls, C. P. Barnes, Deep reinforcement

learning for the control of microbial co-cultures in bioreactors, PLoS com-

putational biology 16 (4) (2020) e1007783.

[181] S. M. J. Pappu, S. N. Gummadi, Artiﬁcial neural network and regression

coupled genetic algorithm to optimize parameters for enhanced xylitol

production by debaryomyces nepalensis in bioreactor, Biochemical engi-

neering journal 120 (2017) 136–145.

[182] M. Koch, T. Duigou, J.-L. Faulon, Reinforcement learning for bioretrosyn-

thesis, ACS Synthetic Biology 9 (1) (2019) 157–168.

[183] B. J. Kotopka, C. D. Smolke, Model-driven generation of artiﬁcial yeast

promoters, Nature communications 11 (1) (2020) 1–13.

69

[184] M. Hutson, Artiﬁcial intelligence faces reproducibility crisis (2018).

[185] O. E. Gundersen, S. Kjensmo, State of the art: Reproducibility in arti-

ﬁcial intelligence, in: Proceedings of the AAAI Conference on Artiﬁcial

Intelligence, Vol. 32, 2018.

[186] B. Haibe-Kains, G. A. Adam, A. Hosny, F. Khodakarami, L. Waldron,

B. Wang, C. McIntosh, A. Goldenberg, A. Kundaje, C. S. Greene,

et al., Transparency and reproducibility in artiﬁcial intelligence, Nature

586 (7829) (2020) E14–E16.

[187] X. Bouthillier, C. Laurent, P. Vincent, Unreproducible research is repro-

ducible, in: International Conference on Machine Learning, PMLR, 2019,

pp. 725–734.

[188] J. Pineau, P. Vincent-Lamarre, K. Sinha, V. Larivi`ere, A. Beygelzimer,

F. d’Alch´e Buc, E. Fox, H. Larochelle, Improving reproducibility in ma-

chine learning research: a report from the neurips 2019 reproducibility

program, Journal of Machine Learning Research 22.

[189] X. Bouthillier, G. Varoquaux, Survey of machine-learning experimental

methods at neurips2019 and iclr2020, Ph.D. thesis, Inria Saclay Ile de

France (2020).

[190] J. Leipzig, D. N¨ust, C. T. Hoyt, K. Ram, J. Greenberg, The role of meta-

data in reproducible computational research, Patterns 2 (9) (2021) 100322.

[191] S. S. Alahmari, D. B. Goldgof, P. R. Mouton, L. O. Hall, Challenges for

the repeatability of deep learning models, IEEE Access 8 (2020) 211860–

211868.

[192] E. Raﬀ, A step toward quantifying independently reproducible machine

learning research, Advances in Neural Information Processing Systems 32.

[193] A. Sethi, A. Sankaran, N. Panwar, S. Khare, S. Mani, Dlpaper2code:

Auto-generation of code from deep learning research papers, in: Proceed-

ings of the AAAI Conference on Artiﬁcial Intelligence, Vol. 32, 2018.

70

[194] M. M. Jessop-Fabre, N. Sonnenschein, Improving reproducibility in syn-

thetic biology, Frontiers in Bioengineering and Biotechnology 7 (2019) 18.

[195] A. Amanullah, J. M. Otero, M. Mikola, A. Hsu, J. Zhang, J. Aunins, H. B.

Schreyer, J. A. Hope, A. P. Russo, Novel micro-bioreactor high through-

put technology for cell culture process development: Reproducibility and

scalability assessment of fed-batch cho cultures, Biotechnology and bio-

engineering 106 (1) (2010) 57–67.

[196] T. Fuchs, N. D. Arnold, D. Garbe, S. Deimel, J. Lorenzen, M. Masri,

N. Mehlmer, D. Weuster-Botz, T. B. Br¨uck, A newly designed automat-

ically controlled, sterilizable ﬂat panel photobioreactor for axenic algae

culture, Frontiers in bioengineering and biotechnology 9 (2021) 566.

[197] M. P. Raphael, P. E. Sheehan, G. J. Vora, A controlled trial for repro-

ducibility (2020).

[198] K. Roper, A. Abdel-Rehim, S. Hubbard, M. Carpenter, A. Rzhetsky,

L. Soldatova, R. D. King, Testing the reproducibility and robustness of the

cancer biology literature by robot, Journal of the Royal Society Interface

19 (189) (2022) 20210821.

[199] L. Teboul, Y. Herault, S. Wells, W. Qasim, G. Pavlovic, Variability in

genome editing outcomes: challenges for research reproducibility and clin-

ical safety, Molecular Therapy 28 (6) (2020) 1422–1431.

[200] M. Baker, 1,500 scientists lift the lid on reproducibility, Nature 533 (7604).

[201] O. E. Gundersen, S. Shamsaliei, R. J. Isdahl, Do machine learning plat-

forms provide out-of-the-box reproducibility?, Future Generation Com-

puter Systems 126 (2022) 34–47.

[202] S. N. Goodman, D. Fanelli, J. P. Ioannidis, What does research repro-

ducibility mean?, Science translational medicine 8 (341) (2016) 341ps12–

341ps12.

71

[203] U. Dirnagl, Rethinking research reproducibility, The EMBO Journal

38 (2) (2019) e101117.

[204] R. Tatman, J. VanderPlas, S. Dane, A practical taxonomy of reproducibil-

ity for machine learning research.

[205] J. Thiyagalingam, M. Shankar, G. Fox, T. Hey, Scientiﬁc machine learning

benchmarks, Nature Reviews Physics (2022) 1–8.

[206] M.-A. Z¨oller, M. F. Huber, Benchmark and survey of automated machine

learning frameworks, Journal of artiﬁcial intelligence research 70 (2021)

409–472.

[207] E. Denton, A. Hanna, R. Amironesei, A. Smart, H. Nicole, M. K. Scheuer-

man, Bringing the people back in: Contesting benchmark machine learn-

ing datasets, arXiv preprint arXiv:2007.07399.

[208] S. Dong, D. Kaeli, Dnnmark: A deep neural network benchmark suite for

gpus, in: Proceedings of the General Purpose GPUs, 2017, pp. 63–72.

[209] S. Alzahrani, B. Al-Bander, W. Al-Nuaimy, A comprehensive evaluation

and benchmarking of convolutional neural networks for melanoma diag-

nosis, Cancers 13 (17) (2021) 4494.

[210] V. P. Dwivedi, C. K. Joshi, T. Laurent, Y. Bengio, X. Bresson, Bench-

marking graph neural networks, arXiv preprint arXiv:2003.00982.

[211] Y. Hirose, N. Yoshinari, S. Shirakawa, Nas-hpo-bench-ii: A benchmark

dataset on joint optimization of convolutional neural network architecture

and training hyperparameters, in: Asian Conference on Machine Learning,

PMLR, 2021, pp. 1349–1364.

[212] H. Zhu, M. Akrout, B. Zheng, A. Pelegris, A. Jayarajan, A. Phanishayee,

B. Schroeder, G. Pekhimenko, Benchmarking and analyzing deep neural

network training, in: 2018 IEEE International Symposium on Workload

Characterization (IISWC), IEEE, 2018, pp. 88–100.

72

[213] R. V. Sharan, H. Xiong, S. Berkovsky, Benchmarking audio signal repre-

sentation techniques for classiﬁcation with convolutional neural networks,

Sensors 21 (10) (2021) 3434.

[214] J. Xie, Q. Wang, Benchmark machine learning approaches with classical

time series approaches on the blood glucose level prediction challenge, in:

KHD@ IJCAI, 2018.

[215] A. Javed, B. S. Lee, D. M. Rizzo, A benchmark study on time series

clustering, Machine Learning with Applications 1 (2020) 100001.

[216] K. Fauvel, V. Masson, E. Fromont, A performance-explainability frame-

work to benchmark machine learning methods: application to multivariate

time series classiﬁers, arXiv preprint arXiv:2005.14501.

[217] Y. Hao, X. Qin, Y. Chen, Y. Li, X. Sun, Y. Tao, X. Zhang, X. Du, Ts-

benchmark: A benchmark for time series databases, in: 2021 IEEE 37th

International Conference on Data Engineering (ICDE), IEEE, 2021, pp.

588–599.

[218] A. Bauer, M. Z¨uﬂe, S. Eismann, J. Grohmann, N. Herbst, S. Kounev,

Libra: A benchmark for time series forecasting methods, in: Proceedings

of the ACM/SPEC International Conference on Performance Engineering,

2021, pp. 189–200.

[219] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani,

J. Kirby, Y. Burren, N. Porz, J. Slotboom, R. Wiest, et al., The mul-

timodal brain tumor image segmentation benchmark (brats), IEEE trans-

actions on medical imaging 34 (10) (2014) 1993–2024.

[220] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,

P. Doll´ar, C. L. Zitnick, Microsoft coco: Common objects in context,

in: European conference on computer vision, Springer, 2014, pp. 740–755.

[221] I. Partalas, A. Kosmopoulos, N. Baskiotis, T. Artieres, G. Paliouras,

E. Gaussier,

I. Androutsopoulos, M.-R. Amini, P. Galinari, Lshtc:

73

A benchmark for

large-scale

text

classiﬁcation,

arXiv preprint

arXiv:1503.08581.

[222] O. Bojar, C. Buck, C. Federmann, B. Haddow, P. Koehn, J. Leveling,

C. Monz, P. Pecina, M. Post, H. Saint-Amand, et al., Findings of the

2014 workshop on statistical machine translation, in: Proceedings of the

ninth workshop on statistical machine translation, 2014, pp. 12–58.

[223] P. Rajpurkar, J. Zhang, K. Lopyrev, P. Liang, Squad: 100, 000+ questions

for machine comprehension of text, in: EMNLP, 2016.

[224] R. S. Olson, W. La Cava, P. Orzechowski, R. J. Urbanowicz, J. H. Moore,

Pmlb: a large benchmark suite for machine learning evaluation and com-

parison, BioData mining 10 (1) (2017) 1–13.

[225] J. D. Romano, T. T. Le, W. La Cava, J. T. Gregg, D. J. Goldberg,

P. Chakraborty, N. L. Ray, D. Himmelstein, W. Fu, J. H. Moore, Pmlb v1.

0: an open-source dataset collection for benchmarking machine learning

methods, Bioinformatics 38 (3) (2022) 878–880.

[226] S. Charaniya, W.-S. Hu, G. Karypis, Mining bioprocess data: opportuni-

ties and challenges, Trends in biotechnology 26 (12) (2008) 690–699.

[227] P. Grover, A. Shah, S. Sen, Mining and analysis of bioprocess data, in:

Machine Learning and IoT, CRC Press, 2018, pp. 29–42.

[228] S. Rommel, A. Schuppert, Data mining for bioprocess optimization, En-

gineering in life sciences 4 (3) (2004) 266–270.

[229] J. S. Alford, Bioprocess control: Advances and challenges, Computers &

Chemical Engineering 30 (10-12) (2006) 1464–1475.

[230] E. A. Del Rio-Chanona, N. R. Ahmed, J. Wagner, Y. Lu, D. Zhang,

K. Jing, Comparison of physics-based and data-driven modelling tech-

niques for dynamic optimisation of fed-batch bioprocesses, Biotechnology

and Bioengineering 116 (11) (2019) 2971–2982.

74

[231] Z. Wu, B. Ramsundar, E. N. Feinberg, J. Gomes, C. Geniesse, A. S.

Pappu, K. Leswing, V. Pande, Moleculenet: a benchmark for molecular

machine learning, Chemical science 9 (2) (2018) 513–530.

[232] A. F. Villaverde, D. Henriques, K. Smallbone, S. Bongard, J. Schmid,

D. Cicin-Sain, A. Crombach, J. Saez-Rodriguez, K. Mauch, E. Balsa-

Canto, et al., Biopredyn-bench: a suite of benchmark problems for dy-

namic modelling in systems biology, BMC systems biology 9 (1) (2015)

1–15.

[233] A. F. Villaverde, F. Fr¨ohlich, D. Weindl, J. Hasenauer, J. R. Banga,

Benchmarking optimization methods for parameter estimation in large

kinetic models, Bioinformatics 35 (5) (2019) 830–838.

[234] B. Ballnus, S. Hug, K. Hatz, L. G¨orlitz, J. Hasenauer, F. J. Theis, Compre-

hensive benchmarking of markov chain monte carlo methods for dynamical

systems, BMC systems biology 11 (1) (2017) 1–18.

[235] J. Riordon, D. Sovilj, S. Sanner, D. Sinton, E. W. Young, Deep learning

with microﬂuidics for biotechnology, Trends in biotechnology 37 (3) (2019)

310–324.

[236] P. Grandcolas, The rise of digital biology: We need not only open, fair

but also sustainable data!, in: Biodiversity Next: Building a global infras-

tructure for biodiversity data. Together., Vol. 3, pensoft, 2019, p. e37508.

[237] A. Jacobsen, R. de Miranda Azevedo, N. Juty, D. Batista, S. Coles, R. Cor-

net, M. Courtot, M. Crosas, M. Dumontier, C. T. Evelo, et al., Fair prin-

ciples: interpretations and implementation considerations (2020).

75

