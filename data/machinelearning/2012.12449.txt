Partial Identiﬁability in Discrete Data With Measurement Error

Noam Finkelstein∗1, Roy Adams∗1, Suchi Saria1,2,4, and Ilya Shpitser1,3

1Department of Computer Science, Johns Hopkins University
2Department of Applied Mathematics and Statistics, Johns Hopkins University
3Department of Biostatistics, Johns Hopkins University
4Bayesian Health

0
2
0
2

c
e
D
3
2

]
L
M

.
t
a
t
s
[

1
v
9
4
4
2
1
.
2
1
0
2
:
v
i
X
r
a

Abstract

When data contains measurement errors, it
is necessary to make assumptions relating
the observed, erroneous data to the unob-
served true phenomena of interest. These as-
sumptions should be justiﬁable on substantive
grounds, but are often motivated by mathe-
matical convenience, for the sake of exactly
identifying the target of inference. We adopt
the view that it is preferable to present bounds
under justiﬁable assumptions than to pursue
exact identiﬁcation under dubious ones. To
that end, we demonstrate how a broad class of
modeling assumptions involving discrete vari-
ables, including common measurement error
and conditional independence assumptions,
can be expressed as linear constraints on the
parameters of the model. We then use lin-
ear programming techniques to produce sharp
bounds for factual and counterfactual distribu-
tions under measurement error in such models.
We additionally propose a procedure for ob-
taining outer bounds on non-linear models.
Our method yields sharp bounds in a number
of important settings – such as the instrumen-
tal variable scenario with measurement error
– for which no bounds were previously known.

1

Introduction

Measurement error is a ubiquitous problem in ﬁelds
ranging from epidemiology and public health [21, 1]
to economics [15, 16, 17] to ecology [23, 22]. If unac-
counted for during analysis, measurement error can
lead biased results. Accounting for measurement error
bias requires making assumptions about how errors
occur so as to link the available data to the underlying
true values. As such, it is important to the validity of

∗Equal Contribution

1

the analysis and the resulting conclusions that these
assumptions be substantively justiﬁable.

In practice, however, analysts often make implausibly
strong assumptions for the sake of exactly identifying
their target of inference. In such cases, we advocate
instead for the use of weaker, credible assumptions to
identify bounds on the target of inference, referred to
as partial identiﬁcation. Previous work on partial iden-
tiﬁability under measurement error has largely focused
on deriving analytic bounds for speciﬁc combinations
of settings and assumptions. As a result, there remain
many important and common settings for which no
known bounds exist.

In this work, we adopt a latent variable formulation of
the measurement error problem. That is, we are inter-
ested in estimating a target parameter that involves
the distribution of a discrete unobserved variable X
using observations of a discrete observed proxy variable
Y . We show how sharp partial identiﬁcation bounds
for the target parameter can be computed numerically
and without extensive derivations for a class of mod-
els, including several models for which no analytical
bounds are currently known. Our approach, which is
similar in spirit to that of [3], is to encode the target
parameter and constraints imposed by the model as
a linear program which can be maximized and mini-
mized to produce sharp bounds. We show that this
approach can be used to compute bounds for factual
and counterfactual parameters of a discrete mismea-
sured variable, including its marginal distribution and
its moments, and the average treatment eﬀect (ATE)
of an intervention on the mismeasured variable.

The primary contribution of this paper is a collection
of modeling assumptions that can be encoded as lin-
ear constraints on a target parameter and, thus, are
amenable to the linear programming approach. These
include several common measurement error con-
straints (Section 2), graphical constraints encoded
by hidden variable Bayesian networks (Section 3), and
causal constraints relating potential outcomes under
diﬀerent interventions (Section 4). Our main result,

 
 
 
 
 
 
presented in Section 3, draws on results from [11] and
[9] to show how an extended class of instrumental
variable-type models produce linear constraints on a
target parameter and present a simple procedure for
obtaining outer bounds for graphs outside this class.

Our approach allows a practitioner to mix and match
any combination of the modeling assumptions described
in this paper with little eﬀort. This ﬂexibility means
that it is trivial to compute bounds under new models
and to test the sensitivity of the resulting bounds to
diﬀerent combinations of assumptions. Throughout, we
demonstrate the utility of this approach on synthetic
examples representing important use cases. Our goal
in these examples is to demonstrate how to apply the
approach to commonly occurring models so that practi-
tioners may adapt them to their speciﬁc data scenario.
These cases include:

1. Bounding the mean of a variable using a single

noisy proxy

2. Bounding the mean of a variable using two noisy

proxies with an exclusion restriction

3. Bounding the ATE in a randomized trial with

measurement error on the outcome

4. Bounding the ATE in an instrumental variable
model with measurement error on the outcome

These cases represent common scenarios occurring in
economics, public health, political science, and other
scientiﬁc settings. Among these, cases 2, 3, and 4 all
represent settings for which no sharp symbolic bounds
are currently known and previous work suggests that
deriving such symbolic bounds automatically for these
settings is infeasible [5]. We begin by introducing our
approach in the context of the simplest possible mea-
surement error model (Section 2). Next, we describe
a class a class of graphical models that yield linear
constraints

2 The basic measurement error

problem and the LP formulation

Suppose that we are interested in the distribution of
a discrete random variable X ∈ X , but instead of ob-
serving X directly, we can only observe a discrete noisy
proxy, denoted Y ∈ Y. Clearly, without any assump-
tions about the proxy distribution, P (Y |X), we cannot
say anything about the distribution of X. On the oppo-
site extreme, if P (Y |X) is known and invertible, then
P (X) is fully identiﬁable from observations of Y [21].

Our interest is in between these two extremes, where
we assume certain properties of P (Y |X), but not the

whole distribution. Our goal is to bound functions of
P , referred to as parameters of interest, under these
assumptions. Our approach is to translate the modeling
assumptions into a set of constraints on P (X, Y ) and
bound the parameter of interest by ﬁnding its maximum
and minimum subject to these constraints. Formally,
let ∆d be the d-dimensional simplex, let η : ∆|X |×|Y| →
R be the parameter of interest, and let M ⊆ ∆|X |×|Y|
be the set of distributions allowed under the modeling
assumptions. Then, assuming that M contains the
true distribution P0, we can bound η(P0) as

min
P ∈M

η(P ) ≤ η(P0) ≤ max
P ∈M

η(P )

(1)

We will show how to construct M such that this opti-
mization problem is tractable and the bounds produced
are sharp.

For notational simplicity, let φxy := P (X = x, Y = y)
be the joint distribution of X and Y . By construc-
tion, φ must satisfy the the probability constraints
(cid:80)
xy φxy = 1, and for all x, y, φxy ≥ 0. In addition, φ
must match the marginal for the observed proxy Y , so
that for all y, (cid:80)
x φxy = P (Y = y). These are called
the observed data constraints.

As mentioned above, we must make assumptions about
the relationship of the unobserved variable X and the
observed proxy Y . We seek to avoid strong, paramet-
ric assumptions, in favor of weaker assumptions that
may be justiﬁed by expert knowledge or domain re-
search. Below, we provide examples of several such
commonly made assumptions. We call constraints on
φ relating the proxies to the unobserved variables the
measurement error constraints 1.

(A0) Bounded error proportion: (cid:80)

x(cid:54)=y φxy ≤ (cid:15)

(A1) Unidirectional errors: (cid:80)

y<x φxy = 0

(A2) Symmetric error probabilities:

φxy = φxy ∀ |x − y| = |x − y(cid:48)|

(A3) Error probabilities decrease by distance:
φxy ≥ φxy(cid:48) ∀ |x − y| > |x − y(cid:48)|

Assumption (A0) may be reasonable when there is suﬃ-
cient previous literature to specify a range of plausible
error rates, but not the exact proxy distribution (e.g.,
see discussion of sensitivity analysis in [21]). Assump-
tion (A1) may be used to represent positive label only
data, which is common in areas such as ecology [23]
and public health [1]. Assumption (A2) represents
a generalization of the zero-mean measurement error

1Each of these constraints can easily be soften by adding
a slack parameter which can, in turn, be varied in a sensi-
tivity analysis.

mistaken by more than 2 points (errors of 2 points
or less are unconstrained by this assumption). The
resulting LP is shown below.

(2)

objective:

constraints:

(cid:88)

xφxy

x,y

(cid:88)

x

φxy = P (Y = y)

φxy ≥ 0

φxy ≤ (cid:15)

(cid:88)

|x−y|>2

φxy ≥ φxy(cid:48)
φxy = φxy(cid:48)

∀|x − y| < |x − y(cid:48)|
∀|x − y| = |x − y(cid:48)|

For two example proxy distributions, P (Y ), Figure 1
shows the resulting bounds on E[X] under diﬀerent
combinations of (A0), (A2), and (A3). Bounds under
each combination of assumptions were computed by
simply solving a slightly diﬀerent version of the LP in
Equation 2, highlighting the ease with which we can
perform sensitivity analysis without rederiving bounds
under each new model.

Were multiple proxies Y ≡ {Y1, ..., YK} observed with
no assumptions made about the relationship between
them, each proxy would be subjected to its own ob-
served data constraints and potentially its own measure-
ment error constraints, depending on what knowledge
is available about the error process. The objective,
and each of the other constraints, would then simply
be expressed on the margins of the full distribution
P (X, Y), which maintains linearity.

2.1 Extending the LP approach to other

parameters and models

In the remainder of this paper, we show how the LP
approach described in this section can be extended
to bound other parameters of interest and incorpo-
rate other modeling assumptions.
In the Section 3,
we show how to incorporate conditional independence
assumptions, encoded in a graphical model, relating
X to other observed and latent variables beyond Y .
We deﬁne a class of latent variable Bayesian networks
which produce linear constraints and show how to relax
the constraints imposed by models not in this class to
produce valid outer bounds. In Section 4, we consider
bounds on parameters of the distribution of X under
an intervention A. In this setting, our interest is in the
potential outcome variable X(a), deﬁned as the value
X would have taken had we intervened to set A = a.
Our goal, then, is to bound parameters of the distribu-
tion P (X(a)), including the average treatment eﬀect of
A on X. We show how to bound such parameters and
incorporate additional assumptions relating potential
outcomes under diﬀerent interventions.

Figure 1: Bounds on E[X] under diﬀerent measurement
error assumptions and observed proxy distributions,
P (Y ).

assumption – commonly made in settings where the
errors are due to imprecision of the instrument – to
discrete variables. Finally, Assumption (A3) may be
reasonable in settings where the values of X have a
natural ordering and small errors are more likely than
large ones.
Note that the target distribution P (X = x) = (cid:80)
y φxy
and all constraints described thus far can be written as
linear functions of φ. If our parameter of interest is also
a linear function of φ, then the assumed constraints,
in combination with the linear objective corresponding
to our parameter of interest, form a linear program
(LP) and bounds can be computed by both maximizing
and minimizing the parameter of interest using one of
a number of LP solvers. Parameters that are linear
in φ include P (X) and all of its moments. By the
intermediate value theorem, the objective can take any
value between its constrained maximum and minimum
values. It follows that for sets of assumptions that can
be linearly expressed, the bounds obtained through
this linear program are sharp, i.e. they exhaust the
information in the observed data under the stated
assumptions.

Application: Bounding E[X] using a single
proxy

To demonstrate this approach, suppose we are inter-
ested in bounding the expected value of a variable
X ∈ {0, .., 5} from observations of Y ∈ {0, ..., 5}. We
will consider bounds under (A2), (A3), and a weaker
version of (A0) wherein there is a known upper bound,
(cid:15) = 0.01, on the proportion of observations that are

012345P(Y)(A0)(A0)+(A2)(A0)+(A2)+(A3)E[X]012345P(Y)(A0)(A0)+(A2)(A0)+(A2)+(A3)E[X]3 Graphical constraints

3.1 Constructing linear constraints from the

In the previous section we relied on domain knowledge
about the joint distribution of X and its proxy Y . In
this section, we describe how assumptions encoded in
a graphical model can be used to further constrain our
target parameter. In particular, we describe a class of
graphs that result in linear constraints on the target
parameter and, thus, is amenable to the linear program-
ming approach introduced in the previous section. This
class includes the common instrumental variable (IV)
model, shown in Figure 2 (a), as well as the various
extensions of this model shown in Figure 3.

Suppose that we assume a latent variable Bayesian
network G = (V, E), where V and E represent the
vertices and edges of the network, respectively. For a
variable V ∈ V, let P aG(V ) be the parents of V in G
and ChG(V ) be the children of V in G. Additionally,
we refer to the set of observed and unobserved variables
with known cardinality as endogenous variables, de-
noted by O, and the set of unobserved variables with
unknown cardinality (typically latent confounders) as
exogenous variables, denoted by U. For example,
consider the Bayesian network in Figure 2 (a) in which
A, X, and Y are endogenous and U is exogenous. In
this model A is commonly referred to as an instrumen-
tal variable (IV), and the model is referred to as the
IV model. The independencies encoded in this graph,
namely A ⊥ U and A ⊥ Y | A, U , place constraints
on the joint distribution P (A, U, X, Y ) which, in turn,
places constraints on the target parameter. We refer
to the target parameter given by a graphical model
as graphical constraints. As before, our goal is to
maximize and minimize the target parameter ψ subject
to these constraints.

In general, the independence constraints imposed by a
Bayesian network on the joint distribution of variables
in the graph are non-linear. For example, the sim-
ple Markov chain model shown in Figure 2 (b) yields
quadratic constraints on P (A, X, Y ) (see Proposition
3 for more details). This makes optimizing over the
constraint set diﬃcult as, in general, quadratic pro-
gramming is NP-hard [18]. Further, the complete set
of latent variable graphical models that impose linear
constraints on P is unknown; however, in the remainder
of this section, we describe a class of graphs, includ-
ing commonly used graphs such as the IV model, that
do yield linear constraints. We will proceed by ﬁrst
illustrating how the basic IV model produces linear
constraints on P and then generalizing this result to a
class of graphs using results from [11] and [9].

IV model

To construct this class, we start by considering the IV
model shown in Figure 2 (a) which is known to place
linear constraints on P [3, 5]. In order to arrive at
linear constraints, we will not optimize directly over
the joint distribution φ = P (A, U, X, Y ) as we did in
Section 2. Instead, we will optimize over an equivalent
potential outcome distribution. Recall that a potential
outcome variable X(a) represents the value X would
have taken had we intervened to set A = a. Then, let
˜X = (X(a))a∈A and ˜Y = (Y (x))x∈X be the vectors of
potential outcome variables for X and Y given their
endogenous parents. Finally, let ψ be the joint distribu-
tion over ˜X, and ˜Y such that ψ˜x,˜y = P ( ˜X = ˜x, ˜Y = ˜y).

Under the consistency assumption, ψ is connected to
the distribution over endogenous variables by the linear
map P (x, y | a) = P (X(a) = x, Y (x) = y), where
the last term is obtained by marginalizing all other
variables in ˜X and ˜Y out of the distribution ψ. For
an explicit example of this marginalization, see Section
D in the Supplement. As observed in [5] and [3], all
independencies in IV graph are now given by A ⊥ ˜X, ˜Y
which can be written as

ψa,˜x,˜y = P (A = a)

(cid:88)

a(cid:48)

ψa(cid:48),˜x,˜y ∀ a, ˜x, ˜y,

(3)

which is linear in ψ since P (A) is identiﬁed from the
data. As G imposes no other constraints on ψ, and
marginalization is a linear operation, all constraints
on the conditional distribution P (x, y | a) are similarly
linear.

3.2 Graphs involving multiple instruments

This linearity result can be generalized to more com-
plex graphs involving multiple instruments using the
following proposition adapted from [11]:

Proposition 1 (Fine’s Theorem). Let G = (V, E) be a
latent variable Bayesian network. Suppose there exists
an exogenous latent variable Λ ∈ V such that (1) all
descendants of Λ are children of Λ and (2) all non-
descendants of Λ are observed, have exactly one child,
and that child is in ChG(Λ). Let the children of Λ
be denoted by X, and the non-descendants of Λ by A.
Then all constraints imposed by G on P (X | A) are
linear.

In such a graph, all variables in X are mutually con-
founded by Λ and we refer to the variables in A as
instruments. Note that this class of graphs trivially
includes the basic IV model (Figure 2 (a)) but also ex-
tends the basic IV model in two important ways. First,
we can now include multiple instruments as shown in

Figure 3 (a) (e.g., see [2, 20]). Second, we can include
instruments for both X and its proxy Y , which may
occur when some aspect of the measurement process is
randomized, such as the order of responses in a survey
or the gender of an in-person surveyor as shown in
Figure 3 (b) (e.g., see [4, 7]).

To derive the set of linear constraints imposed by such
a graph, we can generalize the procedure described
for the IV model as follows. Let K be the number
of variables in X = Ch(Λ). Then, for each variable
i=1, let ˜X be the potential outcomes of
X ∈ X = (Xi)K
X under each joint setting of its endogenous parents
and let ˜X = ( ˜Xi)K
i=1 be the set of all such potential
outcomes. As before, let ψa,˜x = P (A = a, ˜X = ˜x)
denote the joint distribution over the instruments and
potential outcomes. Because P (A) is assumed to be
known, the only relevant independency imposed by the
graph is given by A ⊥ ˜X, which can be written as

ψa,˜x = P (A = a)

(cid:88)

a(cid:48)

ψa(cid:48),˜x ∀ a, ˜x,

(4)

which is linear in ψ. Finally, ψ can be linearly
mapped to the distribution over endogenous variables
O = {A, X} as P (O = o) = P (A = a, ˜X1(oP a(X1)) =
x1, ..., ˜XK(oP a(XK )) = xK) where oP a(Xi) are the val-
ues of Xi’s parents in o. The right hand side is obtained
through the (linear) marginalization of all other poten-
tial outcomes out of ψ and thus the constraints imposed
by the model on the distribution of endogenous vari-
ables are linear. This class of graphs already contains
several useful models, but we will next expand this
class further to include graphs with non-randomized
instruments.

3.3 Graphs with non-randomized

instruments

In the basic IV model (Figure 2 (a)), the instrument
A is assumed to be unconfounded with X; however,
unconfoundedness is a strong assumption that does not
hold in a variety settings. Instead, we may be willing
to make a relaxed assumption that the confounders
for A and X are independent of the confounders for
X and Y , as shown in Figure 3 (c). We can extend
the class of graphs deﬁned in Proposition 1 to include
confounded instruments using the following special case
of Proposition 5 in [9]. An alternative proof of this
proposition is presented in the supplement.
Proposition 2. Suppose a vertex A in a Bayesian
network G has no observed or unobserved parents and
has a single child B. Then the model for the variables
in G is unchanged if an unobserved common parent of
A and B is added to the graph, or if the unobserved
common parent is added and the edge from A to B is
removed.

U

Y

A

X

(a)

A

Y

X

(b)

U

X

Y

Z

z

Az

a

U

X a

Y a

Z

A

(c)

(d)

Figure 2: (a) - (c) show example Bayesian networks.
Black variables represent observed variables, grey vari-
ables represent unobserved variables we wish to make
inferences on, and red variables represent unobserved
variables with unknown cardinality. (d) shows a sin-
gle world intervention graph (SWIG) corresponding to
graph (c) under the intervention Z = z and A = a.

This proposition has two important consequences: ﬁrst,
instruments in A may be confounded with their chil-
dren and, second, if an instrument is confounded with
its child, it need not have a directed edge to that child.
This result broadens the set of graphical models for
which the constraints on the observed data distribution
can be expressed linearly in ψ to include the graphs
such as those shown in Figures 3 (c) and (d). In par-
ticular, Figure 3 (d) can be used to represent a model
where A is a proxy for the true unobserved instrument.

Once we have expressed the modeling constraints as
linear constraints relating the parameters ψ to the
observed data conditional distributions P (x | a), we
can now proceed exactly as in Section 2, optimizing
with respect to ψ rather than φ. Since φ is linearly
related to ψ, the observed data constraint and all mea-
surement error constraints from the previous section
are still in linear in ψ and can be composed with the
graphical constraints in this section. Alternatively, the
measurement error constraints can be expressed for
each potential outcome, representing a belief that these
constraints hold in the observed data as well as under
various interventions.

Application: Bounding E[X] in the IV model

To demonstrate the use of graphical constraints, we
will extend our example from Section 2 to include an
additional binary instrument A ∈ {0, 1} and we will
assume the graphical model in Figure 2 (a). For a
complete description of the resulting LP, see Appendix
D. Assume also that the observed conditional distribu-
tions P (Y | A = a) for a = 0, 1 are the two marginal
distributions shown in Figure 1 and that P (A = 0) = 1
2 .
Then, using only the constraints encoded in the graph,

Λ

A

X

Y

B

(a)

U

Λ

A

Y

X

(c)

B

A

A

Λ

X

(b)

U

Λ

X

(d)

Y

Y

Figure 3: (a) and (b) are classical instrumental graphs
as described in Proposition 1, (c) and (d) are graphs
whose constraints on the observed data distribution
can also be represented linearly by Proposition 2.

we get the numerical bounds E[X] ∈ [0.31, 4.69]. These
bounds can be made substantially tighter by includ-
ing additional measurement error constraints, but it is
worth noting that we can achieve non-trivial bounds
by relying only on graphical constraints.

3.4 Computing bounds for non-linear models

Unfortunately, many relevant models do not fall into
the model class described above. In this section, we
describe how non-sharp outer bounds can be derived
for such cases. The only complete procedure for identi-
fying all constraints implied by Bayesian networks on
the distribution of a subset of their vertices is an appli-
cation of quantiﬁer elimination [12], which is infeasibly
slow for many problems. When constraints are known
to exist, for example by Evans’ e-separation criterion
[10], their exact form may not be known and may not
be linear. When constraints are known, but are not
linear, it may be possible to derive sharp bounds analyt-
ically. For example, the following proposition, proven
in the supplementary material, gives sharp bounds for a
three variable Markov chain (Figure 2 (b)) over binary
variables.

Proposition 3. Let X and Y be binary variables such
that X (cid:54)⊥⊥ Y , let A be a discrete variable such that
A ⊥⊥ Y |X and P (A = a) > 0 for all a, and let py =
P (Y = y) and py|a = P (Y = y | A = a). Then we
have the following sharp bounds on P (X = 1):

P (X = 1) ∈

(cid:91)

y∈{0,1}

(cid:20) py − mina py|a
1 − mina py|a

,

py
maxa py|a

(cid:21)

(5)

Such analytical bounds, however, are not typically avail-
able. In these cases, non-sharp bounds can be derived
for any graph by ﬁrst repeatedly appealing to Proposi-
tion 2 and then adding adding a latent confounder that
meets the criteria of Proposition 1. Speciﬁcally, any la-
tent variable Bayesian network G can be converted to a

Figure 4: A non-linear graphical model G and an ap-
plication of the linear relaxation procedure, resulting
in the linear relaxed model G(cid:48).

new graph G(cid:48) that meets the conditions of Proposition
1 through the following steps:

1. For any latent confounder U with exactly two
children A and B such that P a(A) = {U } and
Ch(A) = {B} or Ch(A) = ∅, add an edge from A
to B if it does not exist and remove U from the
graph.

2. Add a latent confounder Λ, and an edge from Λ to
each variable V in the graph for which P a(V ) (cid:54)= ∅.

An example application of this procedure is shown
in Figure 4. Because modifying the graph according
to Proposition 2 does not change the constraints on
P (X|A), Step 1 of this procedure does not change the
constraint set. Further, adding an additional latent con-
found can only remove independencies from the graph,
thus Step 2 represents relaxations of the constraints
on ψ. As a result, applying the LP approach to the
resulting model will result in outer bounds on the true
partial identiﬁcation set whose tightness will depend
on how many edges were added in steps 2 and 3. In
the following section, we extend our earlier discussion
of potential outcomes to consider partial identiﬁcation
bounds for causal parameters and constraints relating
multiple potential outcomes.

4 Causal parameters and constraints

In the previous section, we used potential outcomes to
reason about the distribution of a mismeasured vari-
able X. Suppose instead that we observe a treatment
variable T and are directly interested in the poten-
tial outcome X(t) under some intervention T = t. As
before, we do not observe X, but instead observe a
proxy Y . This scenario is common in ﬁelds like eco-
nomics and epidemiology, in which the treatment is
exactly known, but the outcome is measured through
inexact tools such as surveys. In this section, we will
show how the constraints presented in the previous
sections can be applied to target parameters involving
the distribution of X(t) and will introduce additional
constraints that apply speciﬁcally to causal inference
settings. We demonstrate this approach in two impor-
tant settings: a clinical trial with measurement error
on the outcome, and an IV model with measurement
error on the outcome.

4.1 Causal target parameters

In order to use the constraints from the previous sec-
tions to bound causal parameters, we ﬁrst need to
show how ψ, deﬁned in the previous section, can be lin-
early mapped to P (X(t)), and then how P (X(t)) can
be linearly mapped to various causal parameters. As-
sume a latent variable Bayesian network G that meets
the conditions of Proposition 1 and the corresponding
joint distribution ψ over instruments A and potential
outcomes ˜X is deﬁned as in Section 3. Then, for an
arbitrary treatment variable T ∈ A ∪ X and value t, we
can calculate the distribution over X(t) as in a struc-
tural equation model, by intervening on T in the graph
and repeatedly appealing to the consistency assump-
tion to marginalize out all variables other than X(t)
[19]. For example, assuming the IV model in Figure 2
(a), the distribution P (X(a)) is just a marginal of ψ
and the distribution P (Y (a)) can be derived as

P (Y (a) = y) =

(cid:88)

x

P (Y (x) = y, X(a) = x)

(6)

In general, for endogenous variable X and intervention
T = T , this expression can be constructed as follows.
Let ˜V represent all potential outcomes for variables
other than X. The general expression is then

P (X(t) = x) =

(cid:88)

v

P (X(g(v, T = t)) = x, V = v),

where g(v, T = t) is computed recursively as

g(v, T = t) =

(cid:40)

vP a(X)\{T }, T = t
{g(v, C(t) = vc(t)) for C ∈ Ch(T )} otherwise.

if X ∈ Ch(T )

This marginalization is linear in ψ, and thus any causal
parameter that can be written as a linear function of
P (X(t)) can also be written as a linear function of
ψ. This includes the average treatment eﬀect (ATE)
which is deﬁned as E[X(t) − X(t(cid:48))] as well as the
probability of non-zero treatment eﬀect, which can
be written as P (X(t) (cid:54)= X(t(cid:48))). As in the previous
sections, we can express observed data, measurement
error, and graphical constraints covered by Proposition
1 as linear constraints on ψ, allowing us to compute
bounds on target parameters involving P (X(t)) under
these constraints.

Importantly, the above mapping applies to the full
class of graphs deﬁned by Proposition 1, but applying
Proposition 2 to this setting requires a bit more care.
If T is in the set of instruments A, then augmenting
the graph as described in Proposition 2 does, in fact,
change the constraints on P (X(t)). If, however, T is in
X, then augmenting the graph according to Proposition

2 leaves the constraints on P (X(t)) unchanged, as all
instruments still satisfy the conditions of Proposition
2.

4.2 Causal Assumptions

Finally, we may want to make additional causal as-
sumptions, which relate potential outcomes under
diﬀerent interventions. For example, assuming we are
interested in X(t) and X is proxied by Y , below are
two commonly made monotonicity assumptions which
may be encoded as linear constraints.

(A4) Positive Eﬀect of Treatment on Truth: P (X(t) =

x, X(t(cid:48)) = x(cid:48)) = 0 ∀t(cid:48) > t, x(cid:48) < x

(A5) Positive Eﬀect of Truth on Proxy: P (Y (x) =

y, Y (x(cid:48)) = y(cid:48)) = 0 ∀x(cid:48) > x, y(cid:48) < y

Assumption (A4) is appropriate if there are strong
reasons to believe the outcome under treatment t will
be strictly higher than t(cid:48). Assumption (A5) is employed
whenever it is assumed that, even under measurement
error, intervening to increase X will lead to an increase
in Y . Additional causal constraints, such as limits
on the eﬀect size or the proportion aﬀected, may be
similarly imposed. As with the measurement error
assumptions, these equality constraints can be relaxed
by specifying that the sums are bounded from above,
rather than identically equal to zero.

Application: Bounding the ATE in a
randomized trial

To demonstrate computation of bounds for a causal
parameter, we will now extend our previous examples
to compute bounds on the ATE in a randomized trial
where there is measurement error on the outcome vari-
able. Assume that we are interested in the ATE of a
binary treatment A ∈ {0, 1} on a variable X using data
from a single noisy proxy Y , with X and Y deﬁned as
before. We will assume the graphical model shown in
Figure 2 (a) where U represents an unobserved con-
founder. This model trivially satisﬁes the conditions
of Proposition 1 and thus all graphical constraints can
be expressed linearly. Figure 5 (a) shows the resulting
bounds on the ATE as additional constraints are added.
With only the graphical constraints, the bounds com-
puted are the trivial bounds, though this will not be
the case for all target parameters. Adding the causal
assumptions (A4)-(A5), however, we are able to mean-
ingfully bound the ATE away from zero. This bound
becomes much tighter when the measurement error
constraints are added.

causal assumptions (A4)-(A5).

We re-emphasize that no known symbolic bounds ex-
ist for either of these settings and that the various
numerical bounds in each setting were computed by
simply making small changes to the implied LP. For a
full description of the LPs in both of these cases, see
Appendix E.

5 Related work

Measurement error occurs in many scientiﬁc settings
and there is substantial literature on identiﬁcation
spread across a number of diﬀerent methodological
sub-disciplines. Much of this work concerns point iden-
tiﬁcation in parametric models and we refer the inter-
ested reader to [6] and [13] for full treatments of these
topics. In this section, we review several results on
non-parametric partial identiﬁcation in measurement
error and related settings.

Several works, particularly in econometrics, have pre-
sented partial identiﬁability results under various mea-
surement error models. [15] consider the setting pre-
sented in Section 2, deriving sharp bounds on the dis-
tribution of the ground truth under a particular error
model where data is "contaminated" by data from an-
other, unknown, distribution. [17] consider the same
setting, presenting a procedure for verifying whether a
particular distribution is in the identiﬁed set under a
wide range of assumptions about the error distribution,
including some non-linear assumptions. [14] consider
partial identiﬁability in a class of ﬁnite mixture models
which includes, as a special case, the Markov chain
model considered in Proposition 3, similarly proposing
a method for verifying if a distribution is in the identi-
ﬁed set. Our work diﬀers from [17] and [14] in that our
methods do not require guess-and-check to calculate
the complete identiﬁed set.

The optimization-based approach we use to derive sharp
bounds is inspired by the approach used by [3] to derive
sharp bounds on causal eﬀects in trials with partial
compliance. This approach was similarly applied by
[16] to partially identify the ATE under measurement
error on the treatment variable. This work is also
related to eﬀorts to enumerate constraints on margins
of latent variable Bayesian Networks implied by the
model [24, 10, 8]. In such works, unobserved variables
are not of primary interest and do not have known
cardinality, so no attempt is made to bound functionals
of their distribution. However, as indicated by our use
of results from [5], constraints on the observed data
law can be used to derive restrictions on unobserved
variables of known cardinality.

(a)

(b)

Figure 5: Bounds on the ATE E[X(1) − X(0)] under
diﬀerent measurement error assumptions. (a) observed
data distributions and bounds under in a randomized
trial setting and (b) shows observed data distributions
and bounds in an instrumental variable setting. Note
that in the binary instrumental variable setting, Z can
be thought of as the assigned treatment and A can
be thought of as the observed treatment under partial
compliance. A = Z then corresponds to observed
compliance and, in these cases (A = Z = 0 and A =
Z = 1), the proxy distributions match the distributions
in the randomized trial.

Application: Bounding the ATE in an
instrumental variable model

Assume now that we are interested in the ATE of
A on X, but that A is no longer randomized.
In-
stead assume that we observe a binary instrumental
variable Z ∈ {0, 1} and that all variables follow the
graphical model shown in Figure 2 (c) where again
U represents an unobserved confounder. This model,
which represents the IV model with measurement error
on the outcome variable, similarly satisﬁes the con-
ditions of Proposition 1 and we can again express
all graphical constraints linearly. We assume that
P (A = 0 | Z = 0) = P (A = 1 | Z = 1) = 0.8 and
P (Y | A, Z) is distributed as shown in Figure 5 (b).
These distributions were chosen to roughly simulate a
trial with partial compliance where Z represents treat-
ment assignment and A represents actual treatment.
The resulting bounds on the ATE under diﬀerent as-
sumptions are also shown in Figure 5 (b). As expected,
these bounds are wider than those produced in the
randomize trial case, but the ATE can nevertheless be
meaningfully bounded away from zero with only the

012345P(Y(0))P(Y(1))(A4)(A4)-(A5)(A4)-(A6)(A0)-(A6)-505E[X(1)X(0)]P(Y|A,Z=0)012345A=0A=1P(Y|A,Z=1)A=0A=1(A4)(A4)-(A5)(A4)-(A6)(A0)-(A6)-505E[X(1)X(0)]6 Discussion

In this work, we presented an approach for computing
bounds on distributional and causal parameters involv-
ing a discrete variable which is subject to measurement
error. At the heart of this approach is the encoding of
the target parameter and modeling constraints as linear
functions of the joint distribution of all variables in the
model. The target parameter can then be maximized
and minimized, with respect to this distribution and
subject to the the modeling constraints, to produce
sharp bounds for any observed data distribution. In
particular, we provided a characterization of a class of
graphical models that can be linearly expressed, and
a procedure for ﬁnding a linear relaxation of models
outside this class. We applied our approach to produce
bounds under measurement error in settings with one
or more proxies, including multiple important settings
for which no known bounds currently exist.

References

[1] Roy Adams, Yuelong Ji, Xiaobin Wang, and Suchi
Saria. Learning models from data with measure-
ment error: Tackling underreporting. In Inter-
national Conference on Machine Learning, pages
61–70, 2019.

[2] Joshua D Angrist and Alan B Keueger. Does
compulsory school attendance aﬀect schooling and
earnings? The Quarterly Journal of Economics,
106(4):979–1014, 1991.

[3] Alexander Balke and Judea Pearl. Nonparametric
bounds on causal eﬀects from partial compliance
data. Journal of the American Statistical Associa-
tion, 1993.

[4] Stan Becker, Kale Feyisetan, and Paulina
Makinwa-Adebusoye. The eﬀect of the sex of in-
terviewers on the quality of data in a nigerian
family planning questionnaire. Studies in Family
Planning, pages 233–240, 1995.

[5] Blai Bonet. Instrumentality tests revisited. In
Jack S. Breese and Daphne Koller, editors, Pro-
ceedings of the 17th Conference on Uncertainty in
Artiﬁcial Intelligence, pages 48–55. Morgan Kauf-
mann, 2001.

[6] Raymond J Carroll, David Ruppert, Leonard A
Stefanski, and Ciprian M Crainiceanu. Measure-
ment error in nonlinear models: a modern perspec-
tive. CRC press, 2006.

[7] Joseph A Catania, Diane Binson, Jesse Canchola,
Lance M Pollack, Walter Hauck, and Thomas J
Coates. Eﬀects of interviewer gender, interviewer

choice, and item wording on responses to ques-
tions concerning sexual behavior. Public Opinion
Quarterly, 60(3):345–375, 1996.

[8] R. J. Evans. Margins of discrete Bayesian networks.
Annals of Statistics, 46(6A):2623–2656, 2018.

[9] Robin Evans. Graphs for margins of bayesian net-
works. Scandinavian Journal of Statistics, 2016.

[10] Robin J. Evans. Graphical methods for inequality

constraints in marginalized dags, 2012.

[11] Arthur Fine. Hidden variables, joint probability,
and the bell inequalities. Physics Review Letters,
48, 1982.

[12] D Gieger and Christopher Meek. Quantiﬁer elimi-
nation for statistical problems. In Proceedings of
the 15th Conference on Uncertainty in Artiﬁcial
Intelligence, 1999.

[13] Paul Gustafson. Measurement error and misclas-
siﬁcation in statistics and epidemiology: impacts
and Bayesian adjustments. CRC Press, 2003.

[14] Marc Henry, Yuichi Kitamura, and Bernard
Salanié. Partial identiﬁcation of ﬁnite mixtures
in econometric models. Quantitative Economics,
5(1):123–144, 2014.

[15] Joel L. Horowitz and Charles F. Manski. Identiﬁ-
cation and robustness with contaminated and cor-
rupted data. Econometrica, 63(2):281–302, 1995.

[16] Kosuke Imai and Teppei Yamamoto. Causal in-
ference with diﬀerential measurement error: Non-
parametric identiﬁcation and sensitivity analysis.
American Journal of Political Science, 54(2):543–
560, 2010.

[17] Francesca Molinari. Partial identiﬁcation of proba-
bility distributions with misclassiﬁed data. Journal
of Econometrics, 144(1):81 – 117, 2008.

[18] Panos M Pardalos and Stephen A Vavasis.
Quadratic programming with one negative eigen-
value is np-hard. Journal of Global optimization,
1(1):15–22, 1991.

[19] Judea Pearl. Causality. Cambridge University

Press, 2009.

[20] Davide Poderini, Rafael Chaves, Iris Agresti, Gon-
zalo Carvacho, and Fabio Sciarrino. Exclusivity
graph approach to instrumental inequalities. In
Uncertainty in Artiﬁcial Intelligence, pages 1274–
1283. PMLR, 2020.

[21] Kenneth J Rothman, Sander Greenland, and Tim-
othy L Lash. Modern epidemiology. Lippincott
Williams & Wilkins, 2008.

[22] Shiv Shankar, Daniel Sheldon, Tao Sun, John Pick-
ering, and Thomas G Dietterich. Three-quarter
sibling regression for denoising observational data.
In IJCAI, pages 5960–5966, 2019.

[23] Péter Sólymos, Subhash Lele, and Erin Bayne.
Conditional likelihood approach for analyzing sin-
gle visit abundance survey data in the presence of
zero inﬂation and detection error. Environmetrics,
23(2):197–205, 2012.

[24] Elie Wolfe, Robert AW. Spekkens, and Tobias
Fritz. The inﬂation technique for causal inference
with latent variables. https://arxiv.org/abs/
1609.00672, 2016.

A Proof of Proposition 3

Proof. By assumption, P (Y = 1|X = 0) (cid:54)= P (Y = 0|X = 0) and therefore

P (X = 1) =

P (Y = 1) − P (Y = 1|X = 0)
P (Y = 1|X = 1) − P (Y = 1|X = 0)

(7)

We will derive sharp bounds on P (X = 1) by taking the union of the sharp bounds when P (Y = 1|X = 0) >
P (Y = 0|X = 0) and when P (Y = 1|X = 0) < P (Y = 0|X = 0). The RHS of Equation 7 is continuous on each
of these sub-regions, thus, as in the the linear programming case, we can ﬁnd sharp bounds on each sub-region
by ﬁnding the maximum and minimum of P (X = 1) on each sub-region subject to the modeling constraints.
Consider ﬁrst the case when P (Y = 1|X = 0) > P (Y = 0|X = 0). For each value a ∈ A we have the following
constraint which combines the observed data constraint and the conditional independence assumption A ⊥⊥ Y |X:

P (Y = 1|A = a) =P (Y = 1|X = 0)(1 − P (X = 1|A = a))

+ P (Y = 1|X = 0)P (X = 1|A = a)

Let qy|x := P (Y = y|X = x) and πx|a := P (X = x|A = a). Then, using Equation 7, we can ﬁnd the sharp upper
bound for P (X = 1) on the P (Y = 1|X = 0) > P (Y = 0|X = 0) sub-region by solving the following (non-linear)
optimization problem:

max
q,π

p1 − q1|0
q1|1 − q1|0

s.t. p1|a = q1|0(1 − π1|a) + q1|1π1|a ∀a

q0|x + q1|x = 1 ∀x
π0|a + π1|a = 1 ∀a
0 ≤ qy|x, πx|a ≤ 1 ∀y, x, a

To solve this optimization problem, we will ﬁx q1|1 and optimize with respect to q1|0 and then optimize the
resulting function with respect to q1|1. That is, let

g(q1|1) = max
q1|0

p1 − q1|0
q1|1 − q1|0

s.t. p1|a = q1|0(1 − π1|a) + q1|1π1|a ∀a

0 ≤ π1|a ≤ 1 ∀a
0 ≤ q1|0 < q1|1

In this case, all constraints are satisﬁed if and only if 0 ≤ q1|0 ≤ mina p1|a and the maximum is achieved when
q1|0 = 0. Thus, g(q1|1) = p1
q1|1

. Next, we solve

max
q1|1

g(q1|1) =

p1
q1|1

s.t. p1|a = q1|1π1|a ∀a

0 ≤ π1|a ≤ 1 ∀a

In this case, all constraints are satisﬁed if and only if maxa p1|a ≤ q1|1 ≤ 1 and the maximum value that satisﬁes
. Applying similar reasoning to the minimization problem, we get a minimum value of
this constraint is
p1−mina p1|a
1−mina p1|a

. Thus, when q1|1 > q1|0, we have the following sharp bounds on P (X = 1)

p1
maxa p1|a

p1 − mina p1|a
1 − mina p1|a

≤ P (X = 1) ≤

p1
maxa p1|a

(8)

Finally, we repeat this derivation for q1|1 < q1|0 and take the union of these two sets of bounds to get the bounds
in Proposition 3. The bounds for the q1|1 < q1|0 are simply one minus the bounds for q1|1 < q1|0 and thus the
bounds for P (X = 1) are the same as the bounds for P (X = 0)2.

B Combining Proposition 3 with Measurement Error Assumptions

In the presence of additional measurement error assumptions, the bounds in Proposition 3 can be further reﬁned.
In this section we present a few such reﬁnements for measurement error assumptions (A1) and (A3) presented in
Section 2 of the main paper as well as an additional non-linear assumption. (A2) does not apply to the binary
case. All of these bounds can be derived with small modiﬁcations to the proof of Proposition 3.

Corollary 1. If, in addition to the assumptions made in Proposition 3, we assume P (Y = 1|X = 0) = 0 (i.e.
(A1)), we have the following sharp bounds

P (X = 1) ∈

(cid:20)
p1,

(cid:21)

p1
maxa p1|a

Corollary 2. If, in addition to the assumptions made in Proposition 3, we assume P (Y = 0|X = 0) > P (Y =
1|X = 0) (i.e. (A3)), we have the following sharp bounds

P (X = 1) ∈

(cid:20) p1 − mina p1|a
1 − mina p1|a

,

p1
maxa p1|a

(cid:21)

Corollary 3. If, in addition to the assumptions made in Proposition 3, we assume P (Y = 1|X = 0) = P (Y =
0|X = 1) (i.e. label independent noise), we have the following sharp bounds

P (X = 1) ∈

(cid:20)
p1,

p1 − p∗
1 − 2p∗

(cid:21)

(cid:20)

∪

1 −

p1 − p∗
1 − 2p∗ , 1 − p1

(cid:21)

where p∗ = min (cid:8)mina p1|a, 1 − maxa p1|a

(cid:9).

C Proof of Proposition 2

Proof. We ﬁrst address the case in which an unobserved common parent is added. The factorization of the
distribution over observed variables implied by G and by the DAG that results after the addition of the
common parent, denoted G(cid:48) diﬀer only in that the expression P (A)P (B | paG(B)) in the former is replaced with
(cid:82)
u P (U )P (A | U )P (B | paG(B), U ). We now show that these expressions are equivalent.

2This is unsurprising as it reﬂects simple label-switching. In fact, the two sub-regions in this proof correspond to the

two possible bipartite matchings between X and Y labels.

(cid:90)

u

P (U )P (A | U )P (B | paG(B), U )

=

=

(cid:90)

u
(cid:90)

u

P (U | paG(B))P (A | U, paG(B) \ {A})P (B | paG(B), U )

P (U, A, B | paG(B) \ {A})

= P (A, B | paG(B) \ {A})
= P (A | paG(B) \ {A})P (B | paG(B))
= P (A)P (B | paG(B))

The ﬁrst step is due {A, U } ⊥d paG(B) \ {A} in G(cid:48), because by construction B is a collider on all paths between
{A, U } and pa(B) \ {A}. The second step is due to the chain rule of probabilities. The next two steps are a
simple marginalization of U and an expansion according to the chain rule, respectively. The ﬁnal step is due
again to A ⊥d paG(B) \ {A} in G(cid:48). This shows that after marginalization of the added common parent of A and
B, we recover exactly the model Markov to G over the variables in G.

We now consider the scenario in which we add a common parent of A and B, and remove the directed edge from A
to B. We denote the resulting DAG by G(cid:48)(cid:48). The proof procedes very similarly; we show that P (A)P (B | paG(B))
is equivalent to (cid:82)
u P (U )P (A | U )P (B | paG(B) \ {A}, U ). To do so, we note that B ⊥d A | U, paG(B) in G(cid:48)(cid:48), as A
has no parents other than U , yielding

(cid:90)

u

P (U )P (A | U )P (B | paG(B) \ {A}, U )

(cid:90)

=

u

P (U )P (A | U )P (B | paG(B), U ).

We can now procede exactly as before, concluding the proof.

D Linear program for bounding E[X] with two proxies

Below is the full linear program used for the application example in Section 3 of the main paper. The distribution
ψ is over the variables ˜X = {X(a) : a ∈ A} and ˜Y = {Y (x) : x ∈ X }. This LP includes only the probability
constraints and the observed data constraint and, as described in Section 3, the graphical constraints are implicitly
enforced by ψ.

objective:

constraints:

(cid:88)

(P (A = 0)˜x0ψ˜x˜y + P (A = 1)˜x1ψ˜x˜y)

(9)

˜x,˜y
(cid:88)

[˜y˜x0 = y]ψ˜x˜y = P (Y = y | A = 0)

˜x,˜y
(cid:88)

[˜y˜x1 = y]ψ˜x˜y = P (Y = y | A = 1)

˜x,˜y

ψ˜x˜y ≥ 0

E Linear Program for Causal Bounds

First, we present the linear program used to bound the ATE in a randomized trial. As in the previous example,
the distribution ψ is over the variables ˜X = {X(a) : a ∈ A} and ˜Y = {Y (x) : x ∈ X }. We include assumptions

objective:

constraints:

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

(˜x1 − ˜x0)ψ˜x˜y

˜x,˜y

(cid:88)

˜x,˜y

ψ˜x˜y ≥ 0

[˜y˜xa = y]ψ˜x˜y = P (Y = y | A = a)

(cid:88)

˜x,˜y

[|˜y˜xa − ˜xa| > 2]ψ˜x˜y ≤ (cid:15)

(10)

∀ ˜x, ˜y

(P ROB)

∀ a, y

(OBS)

∀ a

(A0)

≤ λ

∀ a, |x − y| = |x − y(cid:48)|

(A2)

(cid:12)
(cid:12)
(cid:12)
ψ˜x˜y[˜xa = x]([˜y˜xa = y] − [˜y˜xa = y(cid:48)])
(cid:12)
(cid:12)
(cid:12)

(cid:88)

˜x,˜y
(cid:88)

ψ˜x˜y[˜xa = x]([˜y˜xa = y] − [˜y˜xa = y(cid:48)]) ≥ 0

∀ a, |x − y| > |x − y(cid:48)|

(A3)

˜x,˜y

(cid:88)

ψ˜x˜y[˜x1 < ˜x0] = 0

˜x,˜y
(cid:88)

˜x,˜y

ψ˜x˜y[˜yx < ˜yx(cid:48)] = 0

(A5)

∀x > x(cid:48)

(A6)

Figure 6: LP for bounding the ATE in a randomized trial.

(A0), and (A2) through (A6), as described in the example given in Section 4 of the main paper. To obtain an LP
corresponding to a subset of these assumptions, this LP can be modiﬁed by dropping constraints corresponding
to assumptions not in that subset. Note that each of the constraints on measurement error are repeated twice -
once to place restrictions on the relationship between X(a) and Y (a), and once for X(a(cid:48)) and Y (a(cid:48)). Then the
ATE can be bounded by solving the LP in Figure 6.

Next, we consider the LP used for bounding the ATE in the IV setting. We describe this LP in relation to the
LP in Figure 6. In this case, ψ is now also a distribution over ˜A = {A(z) : z ∈ Z}. Then objective then becomes
(cid:80)
˜a,˜x,˜y(˜x1 − ˜x0)ψ˜a˜x˜y. The potential outcomes A(z) and A(z(cid:48)) are similarly marginalized out of assumptions (A0),
and (A2) through (A6). The only substantive change is that the observed data constraints must be modiﬁed as
described in Section 4. This modiﬁcation yields the following constraints

(cid:88)

˜a,˜x,˜y

ψ˜a˜x˜y[˜az = a, ˜y˜xa = y] = P (Y = y, A = a | Z = z)

∀ z, a, y.

