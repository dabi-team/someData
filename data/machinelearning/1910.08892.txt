Bayesian Symbolic Regression

Ying Jin1, Weilin Fu1, Jian Kang2, Jiadong Guo1, Jian Guo1
1Peng Cheng Laboratory
2University of Michigan, Department of Biostatistics

0
2
0
2

n
a
J

6
1

]
E
M

.
t
a
t
s
[

3
v
2
9
8
8
0
.
0
1
9
1
:
v
i
X
r
a

Abstract

Interpretability is crucial for machine learning in many sce-
narios such as quantitative ﬁnance, banking, healthcare, etc.
Symbolic regression (SR) is a classic interpretable machine
learning method by bridging X and Y using mathematical ex-
pressions composed of some basic functions. However, the
search space of all possible expressions grows exponentially
with the length of the expression, making it infeasible for enu-
meration. Genetic programming (GP) has been traditionally
and commonly used in SR to search for the optimal solution,
but it suffers from several limitations, e.g. the difﬁculty in in-
corporating prior knowledge in GP; overly-complicated out-
put expression and reduced interpretability etc.
To address these issues, we propose a new method to ﬁt SR
under a Bayesian framework. Firstly, Bayesian model can
naturally incorporate prior knowledge (e.g., preference of ba-
sis functions, operators and raw features) to improve the ef-
ﬁciency of ﬁtting SR. Secondly, to improve interpretability
of expressions in SR, we aim to capture concise but infor-
mative signals. To this end, we assume the expected signal
has an additive structure, i.e., a linear combination of several
concise expressions, of which complexity is controlled by a
well-designed prior distribution. In our setup, each expres-
sion is characterized by a symbolic tree, and therefore the
proposed SR model could be solved by sampling symbolic
trees from the posterior distribution using an efﬁcient Markov
chain Monte Carlo (MCMC) algorithm. Finally, compared
with GP, the proposed BSR(Bayesian Symbolic Regression)
method doesn’t need to keep an updated “genome pool” and
so it saves computer memory dramatically.
Numerical experiments show that, compared with GP, the so-
lutions of BSR are closer to the ground truth and the expres-
sions are more concise. Meanwhile we ﬁnd the solution of
BSR is robust to hyper-parameter speciﬁcations such as the
number of trees in the model.

Introduction
Symbolic regression is a special regression model which as-
sembles different mathematical expressions to discover the
association between the response variable and the predic-
tors, with applications studied in (Willis et al. 1997), (David-

Copyright c(cid:13) 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

son, Savic, and Walters 1999), (Davidson, Savic, and Wal-
ters 2003), etc. Without a pre-speciﬁed model structure, it
is challenging to ﬁt symbolic regression, which requires to
search for the optimal solution in a large space of mathemat-
ical expressions and estimate the corresponding parameters
simultaneously.

Traditionally, symbolic regression is solved by combina-
torial optimization methods like Genetic Programming (GP)
that evolves over generations, see (Vladislavleva, Smits, and
den Hertog 2009), (Dabhi and Vij 2011), (Chen, Zhang,
and Xue 2017), (Vladislavleva 2008), etc. However, GP suf-
fers from high computational complexity and overly compli-
cated output expressions, and the solution is sensitive to the
initial value, see (Korns 2011). Some modiﬁcations of the
original GP algorithm have been proposed to address those
problems including (Amir Haeri, Ebadzadeh, and Folino
2017) which incorporates statistical information of gener-
ations, (McConaghy 2011) which deterministically builds
higher-level expressions from ’elite’ building blocks, (Icke
and Bongard 2013) which employs a hybrid of GP and de-
terministic methods, (Luo, Chen, and Jiang 2017) uses a di-
vide and conquer strategy to decompose the search space
and reduce the model complexity, and (Kommenda 2018)
which proposes a local optimization method to control the
complexity of symbolic regression.

Although some efforts have been made to improve GP,
its intrinsic disadvantages still remain unsolved. Some re-
search work explores SR estimation methods other than GP.
For example, (de Frana 2018) which introduces a new data
structure called Interaction-Transformation to constrain the
search space and simplify the output symbolic expression,
(McConaghy 2011) which uses pathwise regularized learn-
ing to rapidly prune a huge set of candidate basis functions
down to compact models, (Chen, Luo, and Jiang 2017) as-
sumes regression models are spanned by a number of elite
bases selected and updated by their proposed algorithm,
(Anjum et al. 2019) introduces a neuro-encoded expres-
sion programming with recurrent neural networks to im-
prove smoothness and stability of the search space, (Li et
al. 2019)which introduces an expression generating neural
network and proposes an Monte Carlo tree search algorithm
to produce expressions that match given leading powers.

 
 
 
 
 
 
In this work, we consider to ﬁt symbolic regression un-
der a Bayesian framework, which can naturally incorporate
prior knowledge, can improve model interpretability and can
potentially simplify the structure and ﬁnd prominent com-
ponents of complicated signals. The key idea is to repre-
sent each mathematical expression as a symbolic tree, where
each child node denotes one input value and the parent node
denotes the output value of applying the mathematical op-
erator to all the input values from its child nodes. To con-
trol model complexity, the response variable y is assumed
to be a linear combination of multiple parent nodes whose
descendant nodes (or leaf nodes) are the predictor x. We
develop a prior model for the tree structures and assign in-
formative priors to the associated parameters. Markov chain
Monte Carlo (MCMC) methods are employed to simulate
the posterior distributions of the underlying tree structures
which correspond to a combination of multiple mathemati-
cal expressions.

The paper is organized as follows. First, we present our
Bayesian symbolic regression model by introducing the tree
representation of mathematical expressions. Then we de-
velop an MCMC-based posterior computation algorithm for
the proposed model. Finally, we demonstrate the superiority
of the proposed method compared to existing alternatives via
numerical experiments.

In the following parts, we will refer to our symbolic re-
gression method based on Bayesian framework as Bayesian
Symbolic Regression or BSR in exchange.

Bayesian Symbolic Regression with
Linearly-Mixed Tree Representations
Denote by x = (x1, . . . , xd) ∈ Rd the predictor variables
and by y ∈ R the response variable. We consider a symbolic
regression model:

y = g(x) + (cid:15),

where g(·) is a function represented by a combination of
mathematical expressions taking predictors x as the input
variable. Speciﬁcally, the mathematical operators such as +,
×, . . ., and arithmetic functions like exp(·), cos(·), . . ., can
be in the search space of mathematical expressions. For ex-
ample, g(x) = x1 + 2 cos(x2) + exp(x3) + 0.1.

Choice of Basic Operators
All possible mathematical expressions are combinations of
elements in a set of basic functions. The choice of basic op-
erators is a building block of our tree representation, see
(Nicolau and Agapitos 2018). In this paper, we adopt the
commonly-used operators +, ×, exp(), inv(x) = 1/x,
neg(x) = −x and linear transformation lt(x) = ax + b
with parameters (a, b) ∈ R2. They are able to express − and
÷ with symmetric binary operators. In practice, the basic
operators can be speciﬁed by users.

From Expressions to Trees
The mathematical expression can be equivalently repre-
sented by a tree denoted by T , with non-terminal nodes indi-
cating operations and terminal nodes indicating the selected

Figure 1: Tree representation of cos(x1 + x2)

features. T is a binary tree but not necessarily a complete
tree.

Speciﬁcally, a non-terminal node has one child node if it
is assigned a unary operator, and two if assigned a binary
operator. For example, a non-terminal node with operator +
represents the operation that the values of its two child nodes
are added up. For a non-terminal unary operator, for example
exp(), it means taking exponential of the value of its child
node. Note that some operators may also be associated with
parameters, like linear transformation lt(x) = ax + b with
parameters (a, b) ∈ R2. We collect these parameters in a
vector Θ.

On the other hand, each terminal node η speciﬁed by
ik ∈ M represents a particular feature xik of the data vec-
tor. Here M is the vector including features of all terminal
nodes. For a tree of depth d, we start from the terminal nodes
by performing the operations indicated by their parents, then
go to their parents and perform upper-level operations ac-
cordingly. We obtain the output at the root node. For exam-
ple, the tree in Figure 1 represents g(x) = cos(x1 + x2),
which consists of two terminal nodes 1, 2 and two non-
terminal nodes cos, +.

In short, the tree structure T is the set of nodes T =
(η1, . . . , ηt), corresponding to operators with zero to two
child nodes. Some operators involve parameters aggregated
in Θ. From predictor x, terminal nodes select features spec-
iﬁed by M = (i1, . . . , ip), where ik indicates adopting xik
of vector x as the input of the corresponding node ηk. The
speciﬁcation of T , Θ and M represents an equivalent tree
for a mathematical expression g(·; T, M, Θ).

Priors on Tree Representations
Under a Bayesian modeling framework, it is critical to spec-
ify appropriate priors for parameters, as it has the ﬂexibility
to incorporate prior knowledge to facilitate more accurate
posterior inferences. In our model, we are interested in mak-
ing inferences on the tree structure T , the parameter Θ and
the selected feature indices M .

To ensure the model interpretability, we aim to control the
size of tree representations, or equivalently, the complexity
of mathematical expressions. The default prior of operators
and features are uniform distributions, indicating no pref-
erence for any particular operator or feature. They can be
user-speciﬁed weight vectors to pose preferences.

For a single tree, we adopt prior distributions on T , M
and Θ in a similar fashion as those for Bayesian regression
tree models in (Chipman, George, and McCulloch 1998) as
follows. Of note, although the prior models are similar, our

cos1+2model and tree interpretations are completely different from
the Bayesian regression tree model.

of σΘ = (σa, σb) is conjugate prior of normal distribution,
which is

Prior of Tree Structure T We specify the prior p(T ) by
assigning the probabilities to each event in the process of
constructing a speciﬁc tree. The prior construction starts
from the root node.

A node is randomly assigned a particular operator accord-
ing to the prior. The operator indicates whether it extends to
one child node, or split into two child nodes, or function as a
terminal node. Starting from the root, such growth performs
recursively on newly-generated nodes until all nodes are as-
signed operators or terminated.

Speciﬁcally, for a node with depth dη, i.e. the num-
ber of nodes passed from it to the node, with probability
p1(η, T ) = α(1 + dη)−β. It is a non-terminal node, which
means it has descendants. Here α, β are preﬁxed parame-
ters that guides the general sizes of trees in practice. The
prior also includes a user-speciﬁed basic operator set and a
corresponding weight vector indicating the probabilities of
adopting each operator for a newly-grown node. For exam-
ple, we specify the operator set (operator) as Ops= (exp(),
lt(), inv(), neg(), +, ×) where lt(x) = ax + b,
inv(x) = 1/x, neg(x) = −x, and the uniform weight
vector wop = (1/6, 1/6, 1/6, 1/6, 1/6, 1/6). Such default
choice shows no preference for any particular operator.

With probability p1(η, T ), the node η is assigned an oper-
ator according to wop if it is non-terminal and grows its one
or two child nodes. Then its child nodes grow recursively.
Otherwise it is a terminal node and assigned some feature in
a way speciﬁed later. The construction of a tree is completed
if all nodes are assigned or terminated.

Prior of Terminal Nodes M When a node is terminated,
it is assigned a feature of x according to the prior of features
as input of the expression. The number and locations of ter-
minal nodes are decided by structure of T . Conditioned on
T , the speciﬁc feature that one terminal node takes is ran-
domly generated with probabilities indicated by weight vec-
tor wf t. The default choice is uniform among all features,
i.e., wf t = (1/d, . . . , 1/d). It can also be user-speciﬁed to
highlight some preferred features.

Prior of lt() Parameters An important operator we
adopt here is linear transformation lt(x) = ax + b asso-
ciated with linear parameters (a, b) ∈ R2. lt() includes
scalings and well enriches the set of potential expressions.
Such operation is discussed in (Keijzer 2003) and proved to
improve the ﬁtting. Pairs of linear parameters (a, b) are as-
sembled in Θ and are considered independent.

Let L(T ) be the set of lt() nodes in T , and each node
η is associated with parameters (aη, bη), then the prior of Θ
is

p(Θ | T ) =

p(aη, bη),

(cid:89)

where aη’s, bη’s are independent and

η∈L(T )

aη ∼ N (1, σ2

a),

bη ∼ N (0, σ2

b ).

This indicates that the prior of the linear transformation is
a Gaussian and centered around identity function. The prior

a ∼ IG(νa/2, νaλa/2), σ2
σ2

b ∼ IG(νb/2, νbλb/2),

where νa, λa, νb, λb are pre-speciﬁed hyper-parameters.

Find the Signal: Linear Mixture of Simpler Trees
Many popular machine learning techniques, such as neural
networks, can approximate functions very well, but they are
difﬁcult to interpret. A widely celebrated advantage of sym-
bolic regression is its interpretability and good performance
of approximating functions. The model ﬁtting of symbolic
regression usually results in relatively simple mathematical
expressions, it is straightforward to understand the relation-
ship between the predictors x and the response variable y.

However, if symbolic regression produces too compli-
cated expressions, the interpretation of the model ﬁtting be-
comes challenging: there exists a tradeoff between simplic-
ity and accuracy. To highlight the superiority of symbolic
regression in interpretability over other methods, we aim at
ﬁnding the most prominent and concise signals. If the fea-
tures are strong and expressive, we assume that the expres-
sion should not involve too many features, and the transfor-
mation should not be too complicated.

Moreover, the real-world signal may be a combination of
simple signals, where only a small amount of simpler ones
play a signiﬁcant role. A simpler idea has its roots in (Keijzer
2004), where the output is appropriately scaled. SR has also
been addressed with methods related to generalized linear
models, summarized in (Zegklitz and Pos´ık 2017).

In this sense, we model the ﬁnal output y to be centered
at some linear combination of relatively simple expressions

y = β0 +

k
(cid:88)

i=1

βi · g(x; Ti, Mi, Θi) + (cid:15),

(cid:15) ∼ N (0, σ2)

where k is a pre-speciﬁed number of simple components,
g(x; Ti, Mi, Θi) is a relatively simple expression repre-
sented by a symbolic tree, and βi is the linear coefﬁcient
for the i-th expression. The coefﬁcients βi, i = 0, . . . , k
is obtained by OLS linear regression using intercept and
g(·; Ti, Mi, Θi), i = 1, . . . , k. Let {(Ti, Mi, Θi)}k
i=1 denote
the series of tuples (Ti, Mi, Θi), i = 1, . . . , k. Let OLS()
denote the OLS ﬁtting result, then a simpler form is

y = OLS(cid:0)x, {(Ti, Mi, Θi)}k

i=1

(cid:1) + (cid:15),

(cid:15) ∼ N (0, σ2)

where the prior of the noise scale is the conjugate inverse
gamma distribution

σ2 ∼ IG(ν/2, νλ/2)

where ν and λ are pre-speciﬁed parameters. Additionally
let (T, M, Θ) = {(Ti, Mi, Θi)}k

i=1, the joint likelihood is

p(y, (T, M, Θ), σ, σΘ | x)

=p(y | OLS(cid:0)x, T, M, Θ(cid:1), σ2)p(M, T )p(Θ | T, σ2

Θ)p(σ2

Θ)p(σ2)

=p(y | OLS(cid:0)x, T, M, Θ(cid:1), σ2)p(σ2) ×

k
(cid:89)

i=1

p(Mi | Ti)p(Ti)p(Θi | Ti, σ2

Θ).

Posterior Inference
We employ the Metropolis-Hastings (MH) algorithm pro-
posed in (Metropolis et al. 1953) and (Hastings 1970) to
make posterior inferences on the proposed model. Note that
(T, M, Θ) represents the set of k trees {Ti, Mi, Θi}k
i=1, and
(T s, M s, Θs) denotes the set of k trees that the MH algo-
rithm accepts at the s-th iteration.

With a pre-speciﬁed number of trees k, our method mod-
iﬁes the structure of the i-th tree by sampling from the pro-
posal q(· | ·), and accepts the new structure with probability
α, which can be calculated according to MH algorithm. Oth-
erwise the i-th tree stays at its original form. The k trees are
updated sequentially, so to illustrate, we ﬁrst show how a
single tree is modiﬁed at each time.

The sampling of a new tree consists of three parts. The
ﬁrst is the structure speciﬁed by T and M , which is dis-
crete. Here T and M stand for a single tree. The second
part is Θ aggregating parameters of all lt() nodes. The di-
mensionality of Θ may change with (T, M ) since the num-
ber of lt() nodes vary among different trees. To address
the trans-dimensional problem, we use the reversible jump
MCMC algorithm proposed by (Green 1995). For simplic-
ity, denote by S = (T, M ) the structure parameters. The
third part is sampling σ2 from an inverse gamma prior.

Structure Transition Kernel
We ﬁrst specify how the sampling algorithm jumps from a
tree structure to a new one. Inspired by (Chipman, George,
and McCulloch 1998) and considering the nature of calcu-
lation trees, we design the following seven reversible ac-
tions. The probabilities from S = (T, M ) to new structure
S∗ = (T ∗, M ∗) is denoted as the proposal q(S∗ | S).

• Stay: If the expression involves nl ≥ 0 lt() operators,
with probability p0 = nl/4(nl + 3), the structure S =
(T, M ) stays unchanged, and ordinary MH step follows
to sample new linear parameters.

• Grow: Uniformly pick a terminal node and activate it. A
sub-tree is then generated iteratively, where each time a
node is randomly terminated or assigned an operator from
the prior until all nodes are terminated or assigned.
To regularize the complexity of the expression, the pro-
posal grows with lower probability when the tree depth
and amount of nodes are large. The probability of Grow
(cid:9) ,where Nnt is the number
· min (cid:8)1,
is pg = 1−p0
3
of non-terminal nodes.

8
Nnt+2

• Prune: Uniformly pick a non-terminal node and turn it
into a terminal node by discarding its descendants. Then
randomly choose a feature of x to the newly pruned node.
We set the probability of Prune as pp = 1−p0
3 − pg such
that Grow and Prune share one-third of the probability
that the structure does not Stay.

• Delete: Uniformly pick a candidate node and delete it.

Speciﬁcally, the candidate should be non-terminal. Also,
if it is a root node, it needs to have at least one non-
terminal child node to avoid leaving a terminal node as
the root node. If the picked candidate is unary, then we

just let its child replace it. If it is binary but not root, we
uniformly select one of its children to replace it. If the
picked candidate is binary and the root, we uniformly se-
lect one of its non-terminal children to replace it.
We set the probability of Delete as pd = 1−p0
· Nc
where Nc is the number of aforementioned candidates.
• Insert: Uniformly pick a node and insert a node between
it and its parent. The weight of nodes assigned is wop. If
the inserted node is binary, the picked node is set as left
child of the new node, and the new right child is generated
according to the prior.
The probability of Insert is set as pi = 1−p0
3 − pd such
that Delete and Insert share one-third of the probability
that the structure does not Stay.

Nc+3 ,

3

• ReassignOperator: Uniformly pick a non-terminal node,

and assign a new operator according to wop.
If the node changes from unary to binary, its original child
is taken as the left child, and we grow a new sub-tree as
right child. If the node changes from binary to unary, we
preserve the left sub-tree (this is to make the transition
reversible).

• ReassignFeature: Uniformly pick a terminal node and

assign another feature with weight wf t.
The probability of ReassignOperator and ReassignFea-
ture is set as pro = prf = 1−p0
6
Note that the generation of the ’tree’ is top-down, creating
sub-trees from nodes. However, the calculation is bottom-
up, corresponding to transforming the original features and
combine different sources of information.

The above discrepancy can be alleviated by our design of
proposal. Grow and Prune creates and deletes sub-trees in
a top-down way, which corresponds to changing a ”block”,
or a higher level feature represented by the sub-tree in the
expression. On the other hand, Delete and Insert modify
the higher-level structure by changing the way such ”blocks”
combine and interact in a bottom-up way.

The choice of trainsition probabities q(S∗ | S) penalizes
tree structures with high complexity, e.g., too many lt()
nodes, which helps control complexity of the output. Con-
stants in q(S∗ | S) guarantee well-deﬁnedness of the prob-
abilities, which can be changed to favor certain transitions
over others.

Jump between Spaces of Parameters
Another issue of proposing new structure S∗ is that the num-
ber of linear transformation nodes may change. Therefore
the dimensionality of Θ may be different and RJMCMC
(reversible jump Markov Chain Monte Carlo) proposed in
(Green 1995) settles the problem well.

After we generate S∗ from S, there are three situations.
• No Change. When the new structure does not change the
number of lt() nodes, the dimensionality of parameters
does not change. In this case, it is sufﬁcient to use ordi-
nary MH step. Here the set of lt() nodes may change,
but the sampling of new parameters is i.i.d., so we are sat-
isﬁed with the MH step.

• Expansion. When the number of lt() nodes increases,
the dimensionality of Θ, denoted by pΘ, increases. We
may simultaneously lose some original lt() nodes and
have more new ones. But due to the i.i.d. nature of param-
eters we only consider the number of all lt() nodes.
Denote the new parameter as Θ∗. According to RJM-
CMC, we sample auxiliary variables U = (uΘ, un) where
dim(uΘ) = dim(Θ), dim(un) + dim(Θ) = dim(Θ∗).
The hyper-parameters Uσ = (σ2
b ) are independently
sampled from the inverse gamma prior, then each element
of uΘ and un is independently sampled from N (1, σ2
a) or
b ) accordingly. The new parameter Θ∗ along with
N (0, σ2
new auxiliary variable U ∗ is obtained by

a, σ2

(U ∗, Θ∗, σ∗

Θ) = je(Θ, U, Uσ) = je(Θ, uΘ, un, Uσ)
Θ + uΘ
2

(cid:16) Θ − uΘ
2

, un, Uσ

=

(cid:17)

,

,

, Θ∗ = ( Θ+uΘ
where U ∗ = Θ−uΘ
2
2
Then we discard U ∗ and get Θ∗, σ∗
Θ.

, un),

σ∗
Θ = Uσ.

• Shrinkage. Θ shrinks when the number of lt() nodes
decreases. Similar to the Expansion case, we may lose
some lt() nodes and also have new ones (especially in
the ReassignOperator transition), but only the dimension-
ality is of interest. Assume that the original parameter is
Θ = (Θ0, Θd) where Θd corresponds to the parameters
of nodes to be dropped. Denote the new parameter as Θ∗.
Firstly, Uσ = (σ2
b ) are sampled independently from
the inverse gamma prior. The new parameter candidate is
then obtained by ﬁrst sampling U , whose elements are
independently sampled from N (0, σ2
b ), re-
spectively, with dim(U ) = dim(Θ0). Then the new can-
didate Θ∗ as well as the corresponding auxiliary variable
U ∗ is obtained by

a) and N (0, σ2

a, σ2

(σ∗

Θ, Θ∗, U ∗) = js(Uσ, U, Θ) = js(Uσ, U, Θ0, Θd)

= (Uσ, Θ0 + U, Θ0 − U, Θd),
Θ = Uσ, Θ∗ = Θ0+U, U ∗ = (Θ0−U, Θd).

where σ∗
Then we discard U ∗ and obtain U ∗, σ∗
Θ.
For simplicity, we denote the two transformations je and
js as jS,S∗ , indicating a parameter transformation from S
to S∗, and the associated auxiliary variables are denoted
as U and U ∗ respectively. Note that dim(Θ) + dim(U ) =
dim(Θ∗) + dim(U ∗) in both cases.

Accepting New Candidates
Return to the K-tree case. We sequentially update the K
trees in a way similar to (Chipman, George, and McCulloch
2010) and (Hastie and Tibshirani 2000). Suppose we start
from tree (T (t)
j ), that is, the j-th tree of the t-
th accepted model, and that the newly proposed structure is
(T ∗

, M (t)
j

, Θ(t)

j , M ∗

j , Θ∗

j ). Denote

j

(T (t), M (t), Θ(t)) = {(T (t)
i
i , M ∗
(T ∗, M ∗, Θ∗) = {(T ∗

, M (t)
i
i )}k
i , Θ∗

i=1,

, Θ(t)

i )}k

i=1,

where (T ∗
i ) for i (cid:54)= j. Also
let S∗ = (T ∗, M ∗), S(t) = (T (t), M (t)). And (σ∗)2 is the

i , M ∗

i , Θ∗

i ) = (T (t)

, M (t)
i

, Θ(t)

i

newly-sampled version of (σ(t))2. For simplicity, let Σ(t) =
(cid:0)(σ(t))2, σ(t)
If dim(Θ(t)
acceptance rate

(cid:1) and Σ∗ = (cid:0)(σ∗)2, σ∗
(cid:1).
Θ
Θ
i ) = dim(Θ∗), the ordinary MH step gives the

R =

f (y | OLS(x, S∗, Θ∗), Σ∗)f (S∗)q(S(t) | S∗)
f (y | OLS(x, S(t), Θ(t)), Σ(t))f (S(t))q(S∗ | S(t))

.

(1)

If dim(Θ(t)
acceptance rate

i ) (cid:54)= dim(Θ∗), the RJMCMC method gives the

R =

f (y | OLS(x, S∗, Θ∗), Σ∗)f (Θ∗ | S∗)q(S(t) | S∗)
f (y | OLS(x, S(t), Θ(t)), Σ(t))f (Θ(t) | S(t))q(S∗ | S(t))

·

f (S∗)p(Σ∗)h(U ∗ | Θ∗, S∗, S(t))
f (S(t))p(Σ(t))h(U (t) | Θ(t), S(t), S∗)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

·

∂jS(t),S∗ (Θ(t), U (t))
∂(Θ(t), U (t))
(2)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

In each case, we accept the new candidate with probabil-
ity α = min{1, R} with R in Equation (1) or (2). If the new
candidate is accepted, we next update the (j+1)-th tree start-
ing from (T (t+1), M (t+1), Θ(t+1)) = (T ∗, M ∗, Θ∗) and
Σ(t+1) = Σ∗. Otherwise we update the (j + 1)-th tree start-
ing at (T (t), M (t), Θ(t)) with Σ(t).

Experiments
We carry out BSR on both simulated data and real-world
data. Firstly, we compare ﬁtness and generalization abil-
ity by comparing RMSEs on training and testing data. Sec-
ondly, we compare the complexity of the expressions gener-
ated by BSR and GP. Meanshile, we examine the robustness
of the proposed BSR method by testing whether the esti-
mated model is sensitive to the parameter K, which is the
number of trees used in the linear regression. We also apply
BSR on ﬁnancial data to ﬁnd effective ’signals’.

Simualtion Designs
Benchmark Problems We set up a benchmark mathemat-
ical expression sets with six tasks presented in Equations
(3) to (8). We ﬁt a BSR model on each of the tasks. These
formulas have been widely used to test other symbolic re-
gression methods, including those based on GP, see (Chen,
Xue, and Zhang 2015),(Topchy and Punch 2001) and (Chen
et al. 2016).

1 − 15

0 + 8x3

1 − 1.7x1

0 − 1.3x3

0 + 0.5x2

f1(x0, x1) = 2.5x4
f2(x0, x1) = 8x2
f3(x0, x1) = 0.2x3
(5)
f4(x0, x1) = 1.5 exp(x0) + 5 cos(x1)
(6)
f5(x0, x1) = 6.0 sin(x0) cos(x1)
(7)
f6(x0, x1) = 1.35x0x1 + 5.5 sin{(x0 − 1)(x1 − 1)} (8)

1 − 1.2x1 − 0.5x0

0 + 0.5x3

(3)

(4)

Datasets Simulation studies in (Chen, Xue, and Zhang
2015) are adopted here. For each target formula, we have
one training dataset and three testing datasets. The training
set consists of 100 samples with predictors generated inde-
pendently from U [−3, 3], and form the response variable

with the corresponding formula above. We consider three
different testing sets, all with size of 30. Predictors of the
three testing sets are generated from U [−3, 3], U [−6, 6] and
U [3, 6], respectively.

Parameter Settings Note that the GP algorithm consists
of two nested iterations, the inner loop for population and
the outer loop for generation. Therefore, the number of trees
generated by GP is Ng × Np, where Ng is the number of
generations and Np is the population size. We set Ng = 200
and Np = 100 here, generating a total of 200,000 trees. For
BSR, 100,000 trees are generated in total (in experiments,
it typically consumes less than 10,000 candidates to sta-
ble results). In addition, we specify K = 2 additive com-
ponents for BSR for all tasks. The basis function pool is
{+, −, ×, ÷, sin, cos, exp, x2, x3} for both methods. In or-
der to see the stability of their performances, we run the two
methods in each task for 50 times independently.

Simualtion Results

Accuracy and Generalization Abilities We use root
mean square error (RMSE) to measure ﬁtness on training
data, and use RMSE on testing set to see generalization.
The performances including mean and standard deviation of
RMSEs are summarized in Table 1. It turns out that BSR
outperforms GP in most tasks, except Equation ((8)). A plau-
sible reason is that the structure is far from linear structure,
which is one of the key assumptions of BSR.

formula to a concise and readable form. Speciﬁcally, we in-
troduce an additive symbolic tree structure for BSR model.
To check if BSR achieves this aim, we summarize the
complexity of the output from BSR and GP in Table 2,
namely the means and standard deviations of number of
nodes in each tree in the 50 replications.

Table 2: Complexity of Expressions
Number of Nodes (mean ± std)

Task
f1
f2
f3
f4
f5
f6

BSR
22.16 ± 7.44
12.25 ± 11.41
27.23 ± 10.61
13.64 ± 12.50
31.28 ± 9.13
20.08 ± 4.78

GP
40.85 ± 21.34
54.51 ± 38.89
22.88 ± 8.62
22.80 ± 8.82
19.80 ± 10.28
21.18 ± 25.73

According to Table 2, the number of nodes on trees gen-
erated by BSR is signiﬁcantly less that those generated by
GP, leading to more concise and readable expressions. Ta-
ble 5 lists some typical expressions output from BSR and
GP, where only two cases are exhibited due to limitations of
paper length, leaving others to appendix. It turns out that
expressions estimated by BSR are generally closer to the
ground truth and they are shorter and more comprehensible.
The simulation study here veriﬁes that, in favourable sce-
narios, BSR reaches its aim and shows its advantage in both
prediction accuracy and interpretability.

Table 1: RMSEs of Both Methods

Task

f1

f2

f3

f4

f5

f6

Dataset
train[-3,3]
test[-3,3]
test[-6,6]
test[3,6]
train[-3,3]
test[-3,3]
test[-6,6]
test[3,6]
train[-3,3]
test[-3,3]
test[-6,6]
test[3,6]
train[-3,3]
test[-3,3]
test[-6,6]
test[3,6]
train[-3,3]
test[-3,3]
test[-6,6]
test[3,6]
train[-3,3]
test[-3,3]
test[-6,6]
test[3,6]

RMSEs (mean ± std)
GP
2.71 ± 2.43
4.25 ± 4.59
116.29 ± 97.59
203.31 ± 168.34
3.56 ± 5.79
2.92 ± 4.41
121.41 ± 126.19
174.01 ± 173.71
0.63 ± 0.33
0.60 ± 0.35
28.97 ± 20.68
34.08 ± 25.41
0.72 ± 1.01
0.84 ± 1.12
24.62 ± 29.66
31.74 ± 36.77
0.78 ± 0.96
0.72 ± 0.83
1.58 ± 1.55
4.49 ± 5.07
3.17 ± 0.79
3.70 ± 0.93
5.13 ± 1.91
11.09 ± 12.58

BSR
2.00 ± 3.87
2.04 ± 3.27
92.09 ± 258.54
118.53 ± 311.57
7.30 ± 10.19
6.84 ± 10.10
95.33 ± 145.31
128.27 ± 221.73
0.19 ± 0.16
0.21 ± 0.20
9.38 ± 9.08
15.19 ± 32.24
0.14 ± 0.56
0.16 ± 0.62
6.96 ± 19.44
12.06 ± 38.27
0.68 ± 1.14
0.66 ± 1.13
1.09 ± 2.39
1.41 ± 3.57
3.99 ± 0.71
4.63 ± 0.62
12.22 ± 8.46
14.44 ± 10.39

Complexity of Expressions One of the most important
aim for BSR is to improve interpretability by restricting the

(a) RMSEs in training f1

(b) Complexity in training f1

(c) RMSEs in training f2

(d) Complexity in training f2

Figure 2: RMSEs and Complexities during training

To further illustrate the performance of BSR versus GP
in typical training processes, we plot the RMSEs of train-
ing data and testing dataset on [−3, 3] for BSR at every
acceptence during the training (See Figure 2). We also in-
clude the training RMSE of best individual at generations of
GP, which are evenly-paced to match the number of records.

Also we compare the complexity of models, evaluated by
number of nodes in the tree. Due to limitation of paper
length, we only exhibit results for f1 and f2, leaving others
to appendix. Figure 2 shows that BSR reduces both train-
ing and testing RMSE during the training process, with less
complex outputs compared to GP.

Table 3: Typical Expressions

Task

f1

f2

Truth

GP

BSR

Truth

GP

BSR

Task
f1
f2
f3
f4
f5
f6

−0.80 ) + cos(x1)) + ((sin((0.71x0)2)

Expressions
0 + 0.5x2

1 − 1.7x1

0 − 1.3x3

−0.80 ) + (((( x0

0.80 + 0.81)) − (((sin((0.80x0)2)

f1 = 2.5x4
y = ((exp(( −x0
− cos(x1)6) + sin((0.80x0)2)) + cos(x1)))
−((( x0
−((sin(((0.71x0))2) − 0.77))2) + 1.0)) + x1))
x2
0.78 ))2 +
0

+(0.76 + x1))) + (((
y = (−0.02) + (−1.30)[x3
+(0.49)[5.05x4
f2 = 8x2
0 + 8x3
y = (exp(1.82)x3
∗(exp(0.187) + cos((x2
+(x1 − 0.77)3 + exp(x1 − 0.38)(x1 − 0.38)
y = (−0.02) + (−1.38)[−7.56x2
+(8.00)[−0.30x2

x2
0
0.80 )
0 + 1.30x1 + 0.09]

0 + x2
1 − 15

0 cos(0.75))))))

1) + 5.26(x2

0 − (cos((0.90x0))

1 + 0.31]

0 + 2.85]

1 − 1.38]

0 + x3

Table 4: RMSEs for different K

K=2
2.04 ± 3.27
6.84 ± 10.10
0.21 ± 0.20
0.16 ± 0.62
0.66 ± 1.13
4.63 ± 0.62

RMSE (mean ± std)
K=4
2.86 ± 5.04
0.02 ± 0.03
0.06 ± 0.03
0.03 ± 0.06
0.29 ± 0.80
4.00 ± 0.34

K=8
0.64 ± 2.46
0.03 ± 0.1
0.03 ± 0.02
0.01 ± 0.01
0.42 ± 0.94
5.28 ± 4.38

Sensitivity to the Number of Components K The num-
ber of additive components K is an important hyper-
parameter in BSR model and it is interesting to study if the
optimal expression selected by BSR is sensitive to the choice
of K. To check this, we summarize the average RMSEs on
testing set [−3, 3] out of 50 replications in Table 4.

It turns out that RMSEs of these tasks are smaller as K
grows, but the improvement of performance is not signiﬁ-
cant when K is large enough. It is interesting to see that even
if K is set to be smaller than ground truth, BSR can auto-
matically ﬁnd an approximately equivalent additive compo-
nent structure in some single trees. On the other hand, when
K is signiﬁcantly larger than what it should be, BSR auto-
matically ”discards” the redundant trees by producing small
coefﬁcients in linear combination, making them similar to
white noise.

Experiments on Real World Data
In the quantitative ﬁnance industry, the most important task
is to ﬁnd ’alpha signals’ effective in predicting returns of
ﬁnancial securities such as stocks, futures and other deriva-
tives. These signals can be expressed as mathematical for-
mulas such as classic factors (Fama and French 1996). How-

ever, mining signals manually is extremely inefﬁcient, and
search directions are usually biased by human knowledge.
BSR provides an automatic way to select effective signals.
We apply BSR on ﬁnancial data to this end.

Datasets and Experimental Setting We collected the CSI
300 INDEX data, which includes time series about daily
prices from 2004 to 2019. Each record consists of ﬁve at-
tributes: open price, high price, low price, and close price,
which corresponds to the trading date. A total of 3586
records are collected in this study.

In our experiments, we deﬁne the label as the sign of the
return derived from the close price in Equation (9). The other
four attributes are predictors. In Equation (9), Close Price(t)
is the close price on the tth trading day, and Close Price(t +
1) means the close price on the next trading day. We set the
sequence from 2004 to 2016 as the training set, and those
from 2017 to 2019 as the testing set.

Return(t) =

Close Price(t + 1) − Close Price(t)
Close Price(t)

(9)

We set the number of trees generated by BSR as 10,000
and the number of additive components as 2. The basis func-
tion pool is set as {+, −, ×, ÷, exp, x2, x3}

Experiment Result To check if BSR can generate effec-
tive factors, we run the task for 200 times independently.
A single factor with an accuracy larger than 0.5 in both the
train set and the test set is considered useful. Finally, 15 ex-
pressions in the posterior modes meet that requirement. We
only exhibit one expression in Expression (10) due to limi-
tations of paper length, leaving others to appendix. Expres-
sion (10) achieves an accuracy of 0.539 in the train set and
an accuracy of 0.518 in the test set. An intuitive explanation
for Expression (10) is that the relative sizes of open price
and low price can predict the return, and if the open price
is much higher than the low price, the return will be more
likely to be positive than negative.

2.9 ∗ 10−4 − 1.2 ∗ 10−3 ∗

1

open2 + 1.9 ∗ 10−3 1

low2

(10)

Conclusions and Future Research
This paper proposes a new symbolic regression method
based on Bayesian statistics framework. Compared with tra-
ditional GP, the proposed method exhibits its advantage
in better model interpretability, simpler way to incorporate
prior knowledge and more cost-effective memory usage etc.
In the future, we are to continue to improve BSR in sev-
eral ways. For example, we will study new MCMC algo-
rithms to improve the search and sampling efﬁciency; we
will study a dynamic empirical bayes method to optimize
hyper-parameters in BSR; we will also study how to extend
the proposed algorithm for distributed computing to improve
computational efﬁciency.

References
[Amir Haeri, Ebadzadeh, and Folino 2017] Amir Haeri, M.;
Ebadzadeh, M. M.; and Folino, G. 2017. Statistical genetic
programming for symbolic regression. Appl. Soft Comput.
60(C):447–469.
[Anjum et al. 2019] Anjum, A.; Sun, F.; Wang, L.; and Or-
chard, J. 2019. A novel continuous representation of genetic
programmings using recurrent neural networks for symbolic
regression. CoRR abs/1904.03368.
[Chen et al. 2016] Chen, Q.; Xue, B.; Shang, L.; and Zhang,
M. 2016. Improving generalisation of genetic programming
for symbolic regression with structural risk minimisation. In
Proceedings of the Genetic and Evolutionary Computation
Conference 2016, 709–716. ACM.
[Chen, Luo, and Jiang 2017] Chen, C.; Luo, C.; and Jiang, Z.
2017. Elite bases regression: A real-time algorithm for sym-
bolic regression. CoRR abs/1704.07313.
[Chen, Xue, and Zhang 2015] Chen, Q.; Xue, B.; and Zhang,
M. 2015. Generalisation and domain adaptation in gp with
In 2015 IEEE
gradient descent for symbolic regression.
congress on evolutionary computation (CEC), 1137–1144.
IEEE.
[Chen, Zhang, and Xue 2017] Chen, Q.; Zhang, M.; and
Xue, B. 2017. Feature selection to improve generalization
of genetic programming for high-dimensional symbolic re-
gression. IEEE Transactions on Evolutionary Computation
21(5):792–806.
[Chipman, George, and McCulloch 1998] Chipman, H. A.;
George, E. I.; and McCulloch, R. E. 1998. Bayesian cart
model search. Journal of the American Statistical Associa-
tion 93(443):935–948.
[Chipman, George, and McCulloch 2010] Chipman, H. A.;
George, E. I.; and McCulloch, R. E. 2010. Bart: Bayesian
additive regression trees. Ann. Appl. Stat. 4(1):266–298.
[Dabhi and Vij 2011] Dabhi, V. K., and Vij, S. K. 2011. Em-
pirical modeling using symbolic regression via postﬁx ge-
In 2011 International Conference on
netic programming.
Image Information Processing, 1–6. IEEE.
[Davidson, Savic, and Walters 1999] Davidson, J. W.; Savic,
D.; and Walters, G. A. 1999. Method for the identiﬁcation
of explicit polynomial formulae for the friction in turbulent
pipe ﬂow. Journal of Hydroinformatics 1(2):115–126.
[Davidson, Savic, and Walters 2003] Davidson, J.; Savic, D.;
and Walters, G. 2003. Symbolic and numerical regres-
Information Sciences
sion: experiments and applications.
150(1):95 – 117. Recent Advances in Soft Computing.
[de Frana 2018] de Frana, F. O. 2018. A greedy search tree
heuristic for symbolic regression. Information Sciences 442-
443:18 – 32.
[Fama and French 1996] Fama, E. F., and French, K. R.
1996. Multifactor explanations of asset pricing anomalies.
The journal of ﬁnance 51(1):55–84.
[Green 1995] Green, P. J. 1995. Reversible jump markov
chain monte carlo computation and bayesian model deter-
mination. Biometrika 82(4):711–732.

2004.

[Hastie and Tibshirani 2000] Hastie, T., and Tibshirani, R.
2000. Bayesian backﬁtting (with comments and a rejoinder
by the authors. Statist. Sci. 15(3):196–223.
[Hastings 1970] Hastings, W. K. 1970. Monte carlo sam-
pling methods using markov chains and their applications.
Biometrika 57(1):97–109.
[Icke and Bongard 2013] Icke, I., and Bongard, J. C. 2013.
Improving genetic programming based symbolic regres-
In 2013 IEEE
sion using deterministic machine learning.
Congress on Evolutionary Computation, 1763–1770.
Improving symbolic re-
[Keijzer 2003] Keijzer, M. 2003.
gression with interval arithmetic and linear scaling. In Ryan,
C.; Soule, T.; Keijzer, M.; Tsang, E.; Poli, R.; and Costa,
E., eds., Genetic Programming, 70–82. Berlin, Heidelberg:
Springer Berlin Heidelberg.
Scaled symbolic re-
[Keijzer 2004] Keijzer, M.
gression. Genetic Programming and Evolvable Machines
5(3):259–269.
[Kommenda 2018] Kommenda, M. V. 2018. Local optimiza-
tion and complexity control for symbolic regression.
[Korns 2011] Korns, M. F. 2011. Accuracy in Symbolic Re-
gression. New York, NY: Springer New York. 129–151.
[Li et al. 2019] Li, L.; Fan, M.; Singh, R.; and Riley, P.
2019. Neural-guided symbolic regression with semantic
prior. CoRR abs/1901.07714.
[Luo, Chen, and Jiang 2017] Luo, C.; Chen, C.; and Jiang, Z.
2017. A divide and conquer method for symbolic regression.
IEEE Transactions on Evolutionary Computation.
[McConaghy 2011] McConaghy, T. 2011. FFX: Fast, Scal-
able, Deterministic Symbolic Regression Technology. New
York, NY: Springer New York. 235–260.
[Metropolis et al. 1953] Metropolis, N.; Rosenbluth, A. W.;
Rosenbluth, M. N.; Teller, A. H.; and Teller, E. 1953. Equa-
tion of state calculations by fast computing machines. The
journal of chemical physics 21(6):1087–1092.
[Nicolau and Agapitos 2018] Nicolau, M., and Agapitos, A.
2018. On the effect of function set to the generalisation of
symbolic regression models. In GECCO.
[Topchy and Punch 2001] Topchy, A., and Punch, W. F.
2001. Faster genetic programming based on local gradient
search of numeric leaf values. In Proceedings of the 3rd An-
nual Conference on Genetic and Evolutionary Computation,
155–162. Morgan Kaufmann Publishers Inc.
[Vladislavleva, Smits, and den Hertog 2009] Vladislavleva,
E. J.; Smits, G. F.; and den Hertog, D. 2009. Order of non-
linearity as a complexity measure for models generated by
symbolic regression via pareto genetic programming. IEEE
Transactions on Evolutionary Computation 13(2):333–349.
[Vladislavleva 2008] Vladislavleva, E. 2008. Model-based
problem solving through symbolic regression via pareto ge-
netic programming. Technical report.
[Willis et al. 1997] Willis, M. .; Hiden, H. G.; Marenbach, P.;
McKay, B.; and Montague, G. A. 1997. Genetic program-
ming: an introduction and survey of applications. In Second
International Conference On Genetic Algorithms In Engi-
neering Systems: Innovations And Applications, 314–319.

[Zegklitz and Pos´ık 2017] Zegklitz, J., and Pos´ık, P. 2017.
Symbolic regression algorithms with built-in linear regres-
sion. CoRR abs/1701.03641.

Appendix

Pseudo-codes of BSR

We sum up the BSR algorithm as follows.

Algorithm 1 pseudo-codes of MCMC-based Symbolic
Regression for linearly-mixed signals

Input: Datapoints x1, . . . , xn, labels y = (y1, . . . , yn); number
of components K, number of acceptance N ; transition kernel
(proposal) q(· | ·), prior distributions p(T, M, Θ), likelihood
function f (y | OLS(S, Θ, x));

Output: A chain of accepted models (T (t), M (t), Θ(t));
1: From prior p(T, M, Θ), generate independently K tree models
), i = 1, . . . , K;
i
2: Calculate linear regression coefﬁcients β(1) from datapoints
), i = 1, . . . , n us-

(structures and parameters) (T (1)

, M (1)
i

, Θ(1)
i

, M (1)
i

, Θ(1)
i

xi, labels yi and models (T (1)
ing OLS;

i

3: Number of accepted models m = 1;
4: while m < N do
5:
6:

for i = 1 → K do
Propose S∗

i = (T ∗

i , M ∗

i ) by sampling S∗
i

| S(m)
i

∼

q(·; S(m)
i

);

7:
8:
9:

10:

11:
12:
13:
14:

if dim(Θ∗

i ) (cid:54)= dim(Θ(m)

i

) then

i ∼ h(U (m)
i
i , Θ∗
i ) = j

Sample U (m)
Obtain (U ∗
Calculate linear regression coefﬁcients β∗ from
datapoints xi, labels yi and models (T ∗, M ∗, Θ∗) using OLS;
Calculate the ratio R in Equation (2);

, S(m)
| Θ(m)
, S∗
i );
i
i
, U (m)
(Θ(m)
);
i
i

(m)
i

,S∗
i

S

else

Directly sample Θ∗
i ∼ p(· | S∗
Calculate coefﬁcients β∗ from xi, yi, i = 1, . . . , n

i );

and models (T (m), M (m), Θ(m)) using OLS;

Calculate the ratio R in Equation (1);

(a) RMSEs in training f3

(b) Complexity in training f3

(c) RMSEs in training f3

(d) Complexity in training f3

(e) RMSEs in training f3

(f) Complexity in training f3

end if
α ← min(1, R);
Sample u ∼ U (0, 1);
if u < α then

for j = 1 → K do
if j = i then
S(m+1)

j

← S∗

j , Θ(m+1)

j

(g) RMSEs in training f4

(h) Complexity in training f4

← Θ∗
j ;

else

j
end if

S(m+1)

end for
β(m+1) ← β∗;
m ← m + 1;

← S(m)
j

, Θ(m+1)
j

← Θ(m)
j

;

(i) RMSEs in training f5

(j) Complexity in training f5

15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
end for
30:
31: end while

end if

Simulation results

Performance visualizations Figures on accuracy and
complexity of BSR and GP on simulated data which are not
included in the paper are summarized below.

(k) RMSEs in training f6

(l) Complexity in training f6

Table 6: Accuracy and Expressions in Real Data Analysis

#

1

2

3

4

5

6

Accuracy

Train

Test

0.539

0.518

0.530

0.525

0.531

0.501

0.539

0.518

0.532

0.511

0.532

0.518

Expression

2.9 ∗ 10−4 − 1.2 ∗ 10−3 ∗
+1.9 ∗ 10−3
−4.0 ∗ 10−2 − 1.1 ∗ 10−2 ∗

1
low2

1
open2

1
−2.9∗elow +0.84+open

−4.0 ∗ 10−2 ∗ high
close
1.3 ∗ 10−3 − 6.4 ∗ 10−5 ∗ eclose
+4.2 ∗ 10−8 ∗ (ehigh)2

4.2 ∗ 10−4 + 4.1 ∗ 10−3 ∗

1
ehigh2

−6.1 ∗ 10−5 ∗ e

1
open2∗high

1.9 ∗ 10−3 − 5.5 ∗ 10−4 ∗ close
−7.2 ∗ 10−7 ∗ (elow)2

5.7 ∗ 10−4 + 1.6 ∗ 10−4 ∗

1
1.1∗(open+2∗low)+0.43

−1.9 ∗ 10−7 ∗ (high3 ∗ elow − high)

Typical expressions Typical expressions produced by
BSR and GP in the simulation studies are summarized be-
low.

Task

f1

f2

f3

f4

Truth

GP

BSR

Truth

GP

BSR

Truth

GP

BSR

Truth

GP

BSR

Truth

f5

GP

f6

BSR

Truth

GP

BSR

Table 5: Typical Expressions

Expressions

0 − (cos((0.90x0))

0 + x3

0 + 2.85]

0 + 4.27]

1 + 0.31]

1 − 1.38]

1 − 1.7x1

0 + 0.5x2

0 − 1.3x3

1) + 5.26(x2

0 cos(0.75))))))

0 + x2
1 − 15

1 − 1.2x1 − 0.5x0

1 + 2.45x0 + x1 − 0.93]

0.80 + 0.81)) − (((sin((0.80x0)2)

f1 = 2.5x4
y = ((exp(( −x0
− cos(x1)6) + sin((0.80x0)2)) + cos(x1)))
−((( x0
−0.80 ) + (((( x0
−0.80 )
+ cos(x1)) + ((sin((0.71x0)2)
−((sin(((0.71x0))2) − 0.77))2) + 1.0)) + x1))
x2
0.78 ))2 +
0

x2
0
0.80 )
0 + 1.30x1 + 0.09]

+(0.76 + x1))) + (((
y = (−0.02) + (−1.30)[x3
+(0.49)[5.05x4
f2 = 8x2
0 + 8x3
y = (exp(1.82)x3
∗(exp(0.187) + cos((x2
+(x1 − 0.77)3 + exp(x1 − 0.38)(x1 − 0.38)
y = (−0.02) + (−1.38)[−7.56x2
+(8.00)[−0.30x2
f3 = 0.2x3
0 + 0.5x3
y = (4x1 − sin(1.32x1) − 0.69
−(sin(sin(1.32x1)/0.50)/0.76))
− sin(x0) − sin(sin(sin((cos(x1) + x1))))
y = (0.04) + (−0.30)[−0.67x3
+(−0.21)[−2.45x3
f4 = 1.5 exp(x0) + 5 cos(x1)
y = (((((exp(cos(x0)) + 0.59 + x0)
+exp(x0)) − cos(exp(cos(x1))))
− cos(exp(cos(sin(x1)x0))))
−x2
y = (−0.01) + (0.28)[17.74 cos(x1) + 0.45]
+(0.24)[6.26exp(x0) − 0.47]
f5 = 6.0 sin(x0) cos(x1)
y = 0.77exp(exp(sin(sin(cos(0.73x0)))))
∗x0 cos(x1)
y = (−7.06 ∗ 10−9) + (6.00)[sin(x0) cos(x1)]
+(2.66 ∗ 10−9)[sin(
0.34
sin2(x1)
−0.93exp(x0 + x1) − 0.95)]
f6 = 1.35x0x1 + 5.5 sin((x0 − 1)(x1 − 1))
y = ((((((x1 sin(x0) + x1x0
− sin( −x0
− sin((x0x1)2)) + sin( x1
− sin((x1 sin(x0))2)) − sin( −x0
− sin(x1 sin(x0) + x1x0))
− sin(x1 sin(x0) + x1x0)
y = (−0.19) + (−0.85)[1.69x0x1 + 1.19]
+(7.00 ∗ 10−3)[exp(sin(exp(exp(exp(x1)
+(1.37x1 − 1.01)3)3)))]

0.36 ) − sin((x0 + x1)))
0.36 ))

1 + x2
0)

0.36 ))

Expressions for Financial data

Here we present results of BSR on ﬁnancial data omitted in
the paper. Results include training accuracy, testing accuracy
and the corresponding expression BSR ﬁnds.

