2
2
0
2

t
c
O
0
2

]
L
M

.
t
a
t
s
[

4
v
4
2
1
0
1
.
5
0
9
1
:
v
i
X
r
a

To the reader’s attention

This paper contains an error in the proof of Theorem 3.1. Indeed, as shown in
[1], this result is false and counterexamples exist, even if, in practice, numerical
simulations suggest that Theorem 3.1 is “often” true. More discussions can be found
in [2]. The other theoretical and numerical results of this paper remain valid. We
have decided to leave the paper as it is because we think it can contribute to the
scientiﬁc discussion, which is one of trial and error.

The authors
20/10/2022

References

[1] Robert Beinert, Cosmas Heiss, and Gabriele Steidl. On Assignment Problems Related to
Gromov-Wasserstein Distances on the Real Line. arXiv preprint arXiv:2205.09006, 2022.
[2] Theo Dumont, Th´eo Lacombe, and Franc¸ois-Xavier Vialard. On the existence of Monge maps

for the Gromov-Wasserstein distance. preprint, hal-03818500, 2022.

 
 
 
 
 
 
Sliced Gromov-Wasserstein

Titouan Vayer
Univ. Bretagne-Sud, CNRS, IRISA
F-56000 Vannes
titouan.vayer@irisa.fr

R´emi Flamary
Univ. Cˆote d’Azur, CNRS, OCA Lagrange
F-06000 Nice
remi.flamary@unice.fr

Romain Tavenard
Univ. Rennes, CNRS, LETG
F-35000 Rennes
romain.tavenard@univ-rennes2.fr

Laetitia Chapel
Univ. Bretagne-Sud, CNRS, IRISA
F-56000 Vannes
laetitia.chapel@irisa.fr

Nicolas Courty
Univ. Bretagne-Sud, CNRS, IRISA
F-56000 Vannes
nicolas.courty@irisa.fr

Abstract

Recently used in various machine learning contexts, the Gromov-Wasserstein dis-
tance (GW ) allows for comparing distributions whose supports do not necessarily
lie in the same metric space. However, this Optimal Transport (OT) distance re-
quires solving a complex non convex quadratic program which is most of the time
very costly both in time and memory. Contrary to GW , the Wasserstein distance
(W ) enjoys several properties (e.g. duality) that permit large scale optimization.
Among those, the solution of W on the real line, that only requires sorting discrete
samples in 1D, allows deﬁning the Sliced Wasserstein (SW ) distance. This paper
proposes a new divergence based on GW akin to SW . We ﬁrst derive a closed
form for GW when dealing with 1D distributions, based on a new result for the
related quadratic assignment problem. We then deﬁne a novel OT discrepancy that
can deal with large scale distributions via a slicing approach and we show how
it relates to the GW distance while being O(n log(n)) to compute. We illustrate
the behavior of this so called Sliced Gromov-Wasserstein (SGW ) discrepancy in
experiments where we demonstrate its ability to tackle similar problems as GW
while being several order of magnitudes faster to compute.

1

Introduction

Optimal Transport (OT) aims at deﬁning ways to compare probability distributions. One typical
example is the Wasserstein distance (W ) that has been used for varied tasks ranging from computer
graphics [1] to signal processing [2]. It has proved to be very useful for a wide range of machine
learning tasks including generative modelling (Wasserstein GANs [3]), domain adaptation [4] or
supervised embeddings for classiﬁcation purposes [5]. However one limitation of this approach is that
it implicitly assumes aligned distributions, i.e. that lie in the same metric space or at least between
spaces where a meaningful distance across domains can be computed. From another perspective,
the Gromov-Wasserstein (GW ) distance beneﬁts from more ﬂexibility when it comes to the more
challenging scenario where heterogeneous distributions are involved, i.e. distributions whose supports
do not necessarily lie on the same metric space. It only requires modelling the topological or
relational aspects of the distributions within each domain in order to compare them. As such, it has

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

recently received a high interest in the machine learning community, solving learning tasks such as
heterogenous domain adaptation [6], deep metric alignment [7], graph classiﬁcation [8] or generative
modelling [9].

OT is known to be a computationally difﬁcult problem: the Wasserstein distance involves a linear
program that most of the time prevents its use to settings with more than a few tens of thousands of
points. For medium to large scale problems, some methods relying e.g. on entropic regularization
[10] or dual formulation [11] have been investigated in the past years. Among them, one builds
upon the mono-dimensional case where computing the Wasserstein distance can be trivially solved
in O(n log n) by sorting points in order and pairing them from left to right. While this 1D case
has a limited interest per se, it is one of the main ingredients of the sliced Wasserstein distance
(SW ) [12]: high-dimensional data are linearly projected into sets of mono-dimensional distributions,
the sliced Wasserstein distance being the average of the Wasserstein distances between all projected
measures. This framework provides an efﬁcient algorithm that can handle millions of points and
has similar properties to the Wasserstein distance [13]. As such, it has attracted attention and has
been successfully used in various tasks such as barycenter computation [14], classiﬁcation [15] or
generative modeling [16, 17, 18, 19].

Regarding GW , the optimization problem is a non-convex quadratic program, with a prohibitive
computational cost for problems with more than a few thousands of points: the number of terms grows
quadratically with the number of samples and one cannot rely on a dual formulation as for Wasserstein.
However several approaches have been proposed to tackle its computation. Initially approximated by
a linear lower bound [20], GW was thereafter estimated through an entropy regularized version that
can be efﬁciently computed by iterating Sinkhorn projections [21, 22]. More recently a conditional
gradient scheme relying on linear program OT solvers was proposed in [8]. However, as discussed
more in detail in Sec. 2, all these methods are still too costly for large scale scenarii. In this paper,
we propose a new formulation related to GW that lowers its computational cost. To that extent, we
derive a novel OT discrepancy called Sliced Gromov-Wasserstein (SGW ). It is similar in spirit to the
Sliced Wasserstein distance as it relies on the exact computation of 1D GW distances of distributions
projected onto random directions. We notably provide the ﬁrst 1D closed form solution of the GW
problem by proving a new result about the Quadratic Assignment Problem (QAP) for matrices that
are squared euclidean distances of real numbers. Computation of SGW for discrete distributions of
n points is O(L n log(n)), where L is the number of sampled directions. This complexity is the same
as the Sliced-Wasserstein distance and is even lower than computing the value of GW which is O(n3)
for a known coupling (once the optimization problem solved) in the general case [22]. Experimental
validation shows that SGW retains various properties of GW while being much cheaper to compute,
allowing its use in difﬁcult large scale settings such as large mesh matching or generative adversarial
networks.

+

(cid:107) (cid:80)

i πi,j = bj; (cid:80)

Notations The simplex histogram with n bins will be denoted as Σn = {a ∈ (R+)n, (cid:80)
i ai = 1}.
For two histograms a ∈ Σn and b ∈ Σm we note Π(a, b) the set of all couplings of a and b, i.e.
the set Π(a, b) = {π ∈ Rn×m
j πi,j = ai}. Sn is the set of all permutations of
{1, ..., n}.
We note (cid:107).(cid:107)k,p the (cid:96)k norm on Rp. For any norm (cid:107).(cid:107) we note d(cid:107).(cid:107) the distance induced by this norm.
δx is the dirac measure in x s.t. a discrete measure µ ∈ P(Rp) can be written µ = (cid:80)n
i=1 aiδxi with
xi ∈ Rp. For a continuous map f : Rp → Rq we note f # its push-forward operator. f # moves
the positions of all the points in the support of the measure to deﬁne a new measure f #µ ∈ P(Rq)
s.t. f #µ def.= (cid:80)
i aiδf (xi). We note O(p) the subset of Rp×p of all orthogonal matrices. Finally
Vp(Rq) is the Stiefel manifold, i.e.
the set of all orthonormal p-frames in Rq or equivalently
Vp(Rq) = {∆ ∈ Rq×p|∆T ∆ = Ip}.

2 Gromov-Wasserstein distance

OT provides a way of inferring correspondences between two distributions by leveraging their intrinsic
geometries. If one has measures µ and ν on two spaces X and Y , OT aims at ﬁnding a correspondence
(or transport) map π ∈ P(X × Y ) such that the marginals of π are respectively µ and ν. When a
meaningful distance or cost c : X × Y (cid:55)→ R+ across the two domains can be computed, classical OT

2

relies on minimizing the total transportation cost between the two distributions
w.r.t. π. The minimum total cost is often called the Wasserstein distance between µ and ν [23].

X×Y c(x, y)dπ(x, y)

´

However, this approach fails when a meaningful cost across the distributions cannot be deﬁned,
which is the case when µ and ν live for instance in Euclidean spaces of different dimensions or
more generally when X and Y are unaligned, i.e. when their features are not in correspondence.
This is particularly the case for features learned with deep learning as they can usually be arbitrarily
rotated or permuted. In this context, the W distance with the naive cost c(x, y) = (cid:107)x − y(cid:107) fails at
capturing the similarity between the distributions. Some works address this issue by realigning spaces
X and Y using a global transformation before using the classical W distance [24]. From another
perspective, the so-called GW distance [25] has been investigated in the past few years and rather
relies on comparing intra-domain distances cX and cY .

Deﬁnition and basic properties Let µ ∈ P(Rp) and ν ∈ P(Rq) with p ≤ q be discrete measures
on Euclidean spaces with µ = (cid:80)n
i=1 bjδyj of supports X and Y , where a ∈ Σn
and b ∈ Σm are histograms.
Let cX : Rp × Rp → R+ (resp. cY : Rq × Rq → R+) measures the similarity between the samples
in µ (resp. ν). The Gromov-Wasserstein (GW ) distance is deﬁned as:

i=1 aiδxi and ν = (cid:80)m

GW 2

2 (cX , cY , µ, ν) = min

J(cX , cY , π)

π∈Π(a,b)

(1)

where

J(cX , cY , π) =

(cid:88)

i,j,k,l

(cid:12)cX (xi, xk) − cY (yj, yl)(cid:12)
(cid:12)
(cid:12)

2

πi,jπk,l.

2 (cX , µ, ν) instead of GW 2

When p = q and cX = cY we will write GW 2
2 (cX , cY , µ, ν). The re-
sulting coupling π is a fuzzy correspondance map between the points of the distributions which
tends to associate pairs of points with similar distances within each pair: the more similar cX (xi, xk)
is to cY (yj, yl), the stronger the transport coefﬁcients πi,j and πk,l are. The GW distance en-
joys many desirable properties when cX and cY are distances so that (X, cX , µ) and (Y, cY , ν)
are called measurable metric spaces (mm-spaces) [25]. In this case, GW is a metric w.r.t.
the
measure preserving isometries. More precisely, it is symmetric, satisﬁes the triangle inequality
when considering three mm-spaces, and vanishes iff the mm-spaces are isomorphic, i.e. when
there exists a surjective function f : X → Y such that f #µ = ν (f preserves the measures) and
∀x, x(cid:48) ∈ X 2, cY (f (x), f (x(cid:48))) = cX (x, x(cid:48)) (f is an isometry). With a slight abuse of notations we
will say that µ and ν are isomorphic when this occurs. The GW distance has several interesting
properties, especially in terms of invariances. It is clear from its formulation in eq. (24) that it is
invariant to translations, permutations or rotations on both distributions when Euclidean distances are
used. This last property allows ﬁnding correspondences between complex word embeddings between
different languages [26]. Interestingly enough, when spaces have the same dimension, it has been
proven that computing GW is equivalent to realigning both spaces using some linear transformation
and then computing the W distance on the realigned measures (Lemma 4.3 in [24]).

2,p, cY (y, y(cid:48)) = (cid:107)y − y(cid:48)(cid:107)2

GW can also be used with other similarity functions for cX and cY (e.g. kernels [22] or squared
integrable functions [27]). In this work, we focus on squared euclidean distances, i.e. cX (x, x(cid:48)) =
(cid:107)x − x(cid:48)(cid:107)2
2,q. This particular case is tackled by the theory of gauged measure
spaces [27, 20] where authors generalize mm-spaces with weaker assumptions on cX , cY than the
distance assumptions. More importantly in our context, invariants are the same as for distances since
GW still vanishes iff there exists a measure preserving isometry (cf. supplementary material) and the
symmetry and triangle inequality are also preserved (see [20]).

Computational aspects The optimization problem (24) is a non-convex Quadratic Program (QP).
Those problems are notoriously hard to solve since one cannot rely on convexity and only descent
methods converging to local minima are available. The problem can be tackled by solving iterative
linearizations of the quadratic function with a conditional gradient as done in [8]. In this case, each
iteration requires the optimization of a classical OT problem, that is O(n3). Another approach
consists in solving an approximation of problem (24) by adding an entropic regularization as pro-
posed in [22]. This leads to an efﬁcient projected gradient algorithm where each iteration requires
solving a regularized OT with the Sinkhorn algorithm that has be shown to be nearly O(n2) and
implemented efﬁciently on GPU. Still note that even though iterations for regularized GW are faster,
the computation of the ﬁnal cost is O(n3) [22, Remark 1].

3

3 From 1D GW to Sliced Gromov-Wasserstein

In this section, we ﬁrst provide and prove a solution for an 1D Quadratic Assignement Problem
(QAP) with a quasilinear time complexity of O(n log(n)). This new special case of the QAP is
shown to be equivalent to the hard assignment version of GW , called the Gromov-Monge (GM )
problem, with squared Euclidean cost for distributions lying on the real line. We also show that, in
this context, solving GM is equivalent to solving GW . We derive a new discrepancy named Sliced
Gromov-Wasserstein (SGW ) that relies on these ﬁndings for efﬁcient computation.

Solving a Quadratic Assignement Problem in 1D In Koopmans-Beckmann form [28] a QAP
takes as input two n × n matrices A = (aij), B = (bij). The goal is to ﬁnd a permutation σ ∈ Sn,
the set of all permutations of {1, · · · , n}, which minimizes the objective function (cid:80)n
i,j=1 ai,jbσ(i),σ(j).
In full generality this problem is NP-hard. However when matrices A and B have simple known
structures, solutions can still be found (e.g. diagonal structure such as Toeplitz matrix or separability
properties such as ai,j = αiαj [29, 30, 31]). We refer the reader to [32, 33] for comprehensive
surveys on the QAP. The following theorem is a new result about QAP and states that it can be solved
when A and B are squared Euclidean distance matrices of sorted real numbers:
Theorem 3.1 (A new special case for the Quadratic Assignment Problem). For real numbers x1 <
· · · < xn and y1 < · · · < yn,

min
σ∈Sn

(cid:88)

i,j

−(xi − xj)2(yσ(i) − yσ(j))2

(2)

is achieved either by the identity permutation σ(i) = i (Id) or the anti-identity permutation σ(i) =
n + 1 − i (anti − Id). In other words:

∃σ ∈ {Id, anti − Id}, σ ∈ arg min

σ∈Sn

(cid:88)

i,j

−(xi − xj)2(yσ(i) − yσ(j))2

(3)

To the best of our knowledge, this result is new. It states that if one wants to ﬁnd the best one-to-one
correspondence of real numbers such that their pairwise distances are best conserved, it sufﬁces to
sort the points and check whether the identity has a better cost than the anti-identity. Proof of this
theorem can be found in the supplementary material. We postulate that this result also holds for
aij = |xi − xj|k and bij = −|yi − yj|k with any k ≥ 1 but leave this study for future works.

Gromov-Wasserstein distance on the real line When n = m and ai = bj = 1
n , one can look
for the hard assignment version of the GW distance resulting in the Gromov-Monge problem [34]
associated with the following GM distance:

GM2(cX , cY , µ, ν) = min
σ∈Sn

1
n2

(cid:88)

i,j

(cid:12)cX (xi, xj) − cY (yσ(i), yσ(j))(cid:12)
(cid:12)
(cid:12)

2

(4)

where σ ∈ Sn is a one-to-one mapping {1, · · · , n} → {1, · · · , n}. Interestingly when the permuta-
tion σ is known, the computation of the cost is O(n2) which is far better than O(n3) for the general
GW case. It is easy to see that this problem is equivalent to minimizing (cid:80)n
i,j=1 ai,jbσ(i),σ(j) with
aij = cX (xi, xj) and bij = −cY (yσ(i), yσ(j)). Thus, when a squared Euclidean cost is used for dis-
tributions lying on the real line, we exactly recover the solution of the GM problem deﬁned in eq. (2).
As matter of consequence, Theorem 3.1 provides an efﬁcient way of solving the Gromov-Monge
problem.

Moreover, this theorem also allows ﬁnding a closed form for the GW distance. Indeed, some recent
advances in graph matching state that, under some conditions on A and B, the assignment problem is
equivalent to its soft-assignment counterpart [35]. This way, using both Theorem 3.1 and [35], one
can ﬁnd a solvable case for the GW distance when p, q = 1 as stated in the following theorem:
Theorem 3.2 (Closed form for GW and GM in 1D for n = m and uniform weights). Let µ =
i=1 δyj ∈ P(R) with R equipped with the Euclidean distance
1
n
d(x, x(cid:48)) = |x − x(cid:48)|. Then GW2(d2, µ, ν) = GM2(d2, µ, ν).
Moreover, if x1 < · · · < xn and y1 < · · · < yn this result is achieved either by the identity or the
anti-identity permutation.

i=1 δxi ∈ P(R) and ν = 1

(cid:80)n

(cid:80)n

n

4

Sketch of the proof. Since d2 is conditionally negative deﬁnite of order 1 (see e.g. Prop 3 and 4
in [36]), one can use the theory developed in [35] to prove that the assignment problem of GM
is equivalent to GW . Note that this result is true also for cX (x, x(cid:48)) = (cid:107)x − x(cid:48)(cid:107)2
2,p , cY (y, y(cid:48)) =
(cid:107)y − y(cid:48)(cid:107)2

2,q for any p and q. Using Theorem 3.1 for the GM distance concludes the proof.

A more detailed proof is provided as supplementary material. In the following, we only consider
the case where µ and ν are discrete measures with the same number of atoms n = m, uniform
weights and p ≤ q. Note also that, while both possible solutions for problem (4) can be computed in
O(n log(n)), ﬁnding the best one requires the computation of the cost which seems, at ﬁrst sight, to
have a O(n2) complexity. However, under the hypotheses of Theorem 3.2, the cost can be computed
in O(n). Indeed, in this case, one can develop the sum in eq (4) to compute it in O(n) operations
using binomial expansion (see details in the supplementary materials) so that the overall complexity
of ﬁnding the best assignment and computing the cost is O(n log(n)) which is the same complexity
as the Sliced Wasserstein distance.

Sliced Gromov-Wasserstein discrepancy Theorem 3.2 can be put in perspective with the Wasser-
stein distance for 1D distributions which is achieved by the identity permutation when points are
sorted [37]. As explained in the introduction, this result was used to approximate the Wasserstein
distance between measures of Rp using the so called Sliced Wasserstein (SW) distance [14]. The
main idea is to project the points of the measures on lines of Rp where computing a Wasserstein
distance is easy since it only involves a simple sort and to average these distances. It has been proven
that SW and W are equivalent in terms of metric on compact domains [13]. In the same philosophy
we build upon Theorem 3.2 to deﬁne a ”sliced” version of the GW distance. In the following, we
consider µ ∈ P(Rp), ν ∈ P(Rq) be probability distributions (not necessarily discrete).
Let Sq−1 = {θ ∈ Rq : (cid:107)θ(cid:107)2,q = 1} be the q-dimensional hypersphere and λq−1 the uniform measure
on Sq−1 . For θ we note Pθ the projection on θ, i.e. Pθ(x) = (cid:104)x, θ(cid:105). For a linear map ∆ ∈ Rq×p
(identiﬁed with slight abuses of notation by its corresponding matrix), we deﬁne the Sliced Gromov-
Wasserstein (SGW) as follows:

SGW∆(µ, ν) = E

θ∼λq−1

[GW 2

2 (d2, Pθ#µ∆, Pθ#ν)] =

GW 2

2 (d2, Pθ#µ∆, Pθ#ν)dθ

(5)

Sq−1

ﬄ
Sq−1 =

´

where µ∆ = ∆#µ ∈ P(Rq) and
1
Sq−1 is the normalized integral and can be
vol(Sq−1)
seen as the expectation for θ following a uniform distribution of support Sq−1. The function ∆
acts as a mapping for a point in Rp of the measure µ onto Rq. When p = q and when we consider
∆ as the identity map we simply write SGW (µ, ν) instead of SGWIp (µ, ν). When p < q, one
straightforward choice is ∆ = ∆pad the ”uplifting” operator which pads each point of the measure
with zeros: ∆pad(x) = (x1, . . . , xp, 0, . . . , 0
(cid:124) (cid:123)(cid:122) (cid:125)
q−p

). The procedure is illustrated in Fig 1.

In general ﬁxing ∆ implies that some properties of GW , such as the rotational invariance, are lost.
Consequently, we also propose a variant of SGW that does not depends on the choice of ∆ called
Rotation Invariant SGW (RISGW ) and expressed for p ≥ q as the following:

RISGW (µ, ν) = min

SGW∆(µ, ν).

∆∈Vp(Rq)

(6)

We propose to minimize SGW∆ with respect to ∆ in the Stiefel manifold [38] which can be seen
as ﬁnding an optimal projector of the measure µ [39, 40]. This formulation comes at the cost
of an additional optimization step but allows recovering one key property of GW. When p = q
this encompasses for e.g. all rotations of the space, making RISGW invariant by rotation (see
Theorem 3.3).

Interestingly enough, SGW holds various properties of the GW distance as summarized in the
following theorem:
Theorem 3.3. Properties of SGW

• For all ∆, SGW∆ and RISGW are translation invariant. RISGW is also rota-
tional invariant when p = q, more precisely if Q ∈ O(p) is an orthogonal matrix,
RISGW (Q#µ, ν) = RISGW (µ, ν) (same for any Q(cid:48) ∈ O(p) applied on ν).

5

 
x1x4

x2

x3

y4

y1

y3

y2

Pθ#(∆#µ)

xθ
1

2 xθ
xθ
3

xθ
4

for θ ∈ Sq−1

Pθ#ν

yθ
1

yθ
2

yθ
3

yθ
4

Figure 1: Example in dimension p = 2 and q = 3 (left) that are projected on the line (right). The
solution for this projection is the anti-diagonal coupling.

• SGW and RISGW are pseudo-distances on P(Rp), i.e. they are symmetric, satisfy the

triangle inequality and SGW (µ, µ) = RISGW (µ, µ) = 0 .

• Let µ, ν ∈ P(Rp) × P(Rp) be probability distributions with compact supports.

If
SGW (µ, ν) = 0 then µ and ν are isomorphic for the distance induced by the (cid:96)1 norm on
Rp, i.e. d(x, x(cid:48)) = (cid:80)p

i| for (x, x(cid:48)) ∈ Rp × Rp. In particular this implies:

i=1 |xi − x(cid:48)

SGW (µ, ν) = 0 =⇒ GW2(d, µ, ν) = 0

(7)

(with a slight abuse of notation we identify the matrix Q by its linear application). A proof of this
theorem can be found in the supplementary material. This theorem states that if SGW vanishes then
measures must be isometric, as it is the case for GW . It states also that RISGW holds most of the
properties of GW in term of invariances.

Remark The ∆ map can also be used in the context of the Sliced Wasserstein distance so as to
deﬁne SW∆(µ, ν), RISW (µ, ν) for µ, ν ∈ P(Rp) × P(Rq) with p (cid:54)= q. Please note that from a
purely computational point of view, complexities of these discrepancies are the same as SGW and
RISGW . Also, unlike SGW and RISGW , these discrepancies are not translation invariant. More
details are given in the supplementary material.

(cid:80)n

i=1 δxi , ν = 1
n

In the following µ, ν are discrete measures with the same number of atoms
Computational aspects
(cid:80)n
i=1 δyi with xi ∈ Rp, yi ∈ Rq so
n = m, and uniform weights, i.e. µ = 1
n
that we can apply Theorem 3.2. Similarly to Sliced Wasserstein, SGW can be approximated by
replacing the integral by a ﬁnite sum over randomly drawn directions. In practice we compute SGW
as the average of GW 2
2 projected on L directions θ. While the sum in (47) can be implemented
with libraries such as Pykeops [41], Theorem 9.2 shows that computing (47) is achieved by an
O(n log(n)) sorting of the projected samples and by ﬁnding the optimal permutation which is
either the identity or the anti identity. Moreover computing the cost is O(n) for each projection
as explained previously. Thus the overall complexity of computing SGW with L projections is
O(Ln(p + q) + Ln log(n) + Ln) = O(Ln(p + q + log(n))) when taking into account the cost of
projections. Note that these computations can be efﬁciently implemented in parallel on GPUs with
modern toolkits such as Pytorch [42].

The complexity of solving RISGW is higher but one can rely on efﬁcient algorithms for optimizing
on the Stiefel manifold [38] that have been implemented in several toolboxes [43, 44]. Note that
each iteration in a manifold gradient decent requires the solution of SGW , that can be computed and
differentiated efﬁciently with the frameworks described above. Moreover, the optimization over the
Stiefel manifold does not depend on the number of points but only on the dimension d of the problem
so that overall complexity is niter(Ln(d + log(n)) + d3), which is affordable for small d. In practice,
we observed in the numerical experiments that RISGW converges in few iterations (the order of 10).

6

Figure 2: Illustration of SGW , RISGW and GW on spiral dataset for varying rotations on discrete
2D spiral dataset. (Left) Examples of spiral distributions for source and target with different rotations.
(Right) Average value of SGW , GW and RISGW with L = 20 as a function of rotation angle of
the target. Colored areas correspond to the 20% and 80% percentiles.

4 Experimental results

The goal of this section is to validate SGW and its rotational invariant on both quantitative (execution
time) and qualitative sides. All the experiments were conducted on a standard computer equipped
with a NVIDIA Titan X GPU.

SGW and RISGW on spiral dataset As a ﬁrst example, we use the spiral dataset from sklearn
toolbox and compute GW , SGW and RISGW on n = 100 samples with L = 20 sampled lines
for different rotations of the target distribution. The optimization of ∆ on the Stiefel manifold is
performed using Pymanopt [43] with automatic differentiation with autograd [45]. Some examples of
empirical distributions are available in Figure 2 (left). The mean value of GW , SGW and RISGW
are reported on Figure 2 (right) where we can see that RISGW is invariant to rotation as GW
whereas SGW with ∆ = Id is clearly not.

Runtimes comparison We perform a comparison between runtimes of SGW , GW and its
entropic counterpart [21]. We calculate these distances between two 2D random measures of
n ∈ {1e2, ..., 1e6} points. For SGW , the number of projections L is taken from {50, 200}. We
use the Python Optimal Transport (POT) toolbox [46] to compute GW distance on CPU. For
entropic-GW we use the Pytorch GPU implementation from [9] that uses the log-stabilized Sinkhorn
algorithm [47] with a regularization parameter ε = 100. For SGW , we implemented both a Numpy
implementation and a Pytorch implementation running on GPU. Fig. 3 illustrates the results.

SGW is the only method which
scales w.r.t.
the number of samples
and allows computation for n > 104.
While entropic-GW uses GPU, it is
still slow because the gradient step
size in the algorithm is inversely pro-
portional to the regularization parame-
ter [22] which highly curtails the con-
vergence of the method. On CPU,
SGW is two orders of magnitude
faster than GW . On GPU, SGW is
ﬁve orders of magnitude better than
GW and four orders of magnitude bet-
ter than entropic GW . Still the slope
of both GW implementations are sur-
prisingly good, probably due to their
maximum iteration stopping criteria.
In this experiment we were able to
compute SGW between 106 points in
1s. Finally note that we recover exactly a quasi-linear slope, corresponding to the O(n log(n))
complexity for SGW .

Figure 3: Runtimes comparison between SGW , GW ,
entropic-GW between two 2D random distributions with
varying number of points from 0 to 106 in log-log scale. The
time includes the calculation of the pair-to-pair distances.

7

Figure 4: Each sample in this Figure corresponds to a mesh and is colored by the corresponding time
iteration. One can see that the cyclical nature of the motion is recovered.

Meshes comparison In the context of computer graphics, GW can be used to quantify the corre-
spondances between two meshes. A direct interest is found in shape retrieval, search, exploration or
organization of databases. In order to recover experimentally some of the desired properties of the
GW distance, we reproduce an experiment originally conducted in [48] and presented in [21] with
the use of entropic-GW .

From a given time series of 45 meshes representing a galloping horse, the goal is to conduct a multi-
dimensional scaling (MDS) of the pairwise distances, computed with SGW between the meshes,
that allows ploting each mesh as a 2D point. As one can observe in Fig. 4, the cyclical nature of this
motion is recovered in this 2D plot, as already illustrated in [21] with the GW distance. Each horse
mesh is composed of approximately 9, 000 vertices. The average time for computing one distance is
around 30 minutes using the POT implementation, which makes the computation of the full pairwise
distance matrix impractical (as already mentioned in [21]). In contrast, our method only requires 25
minutes to compute the full distance matrix, with an average of 1.5s per mesh pair, using our CPU
implementation. This clearly highlights the beneﬁts of our method in this case.

SGW as a generative adversarial network (GAN) loss
In a recent paper [9], Bunne and col-
leagues propose a new variant of GAN between incomparable spaces, i.e. of different dimensions. In
contrast with classical divergences such as Wasserstein, they suggest to capture the intrinsic relations
between the samples of the target probability distribution by using GW as a loss for learning. More
formally, this translates into the following optimization problem over a desired generator G:

G∗ = arg min GW 2

2 (cX , cG(Z), µ, νG),

(8)

where Z is a random noise following a prescribed low-dimensional distribution (typically Gaussian),
G(Z) performs the uplifting of Z in the desired dimensional space, and cG(Z) is the corresponding
metric. µ and νG correspond respectively to the target and generated distributions, that we might
want to align in the sense of GW . Following the same idea, and the fact that sliced variants of the
Wasserstein distance have been successfully used in the context of GAN [17], we propose to use
SGW instead of GW as a loss for learning G. As a proof of concept, we reproduce the simple toy
example of [9]. Those examples consist in generating 2D or 3D distributions from target distributions
either in 2D or 3D spaces (Fig. 7 and supplementary material). These distributions are formed by
3, 000 samples. We do not use their adversarial metric learning as it might confuse the objectives
of this experiment and as it is not required for these low dimensional problems [9]. The generator
G is designed as a simple multilayer perceptron with 2 hidden layers of respectively 256 and 128
units with ReLu activation functions, and one ﬁnal layer with 2 or 3 output neurons (with linear
activation) as output, depending on the experiment. The Adam optimizer is used, with a learning

8

Figure 5: Using SGW in a GAN loss. First image shows the loss value along epochs. The next 4
images are produced by sampling the generated distribution (3, 000 samples, plotted as a continuous
density map). Last image shows the target 3D distribution.

rate of 2.10−4 and β1 = 0.5, β2 = 0.99. The convergence to a visually acceptable solution takes a
few hundred epochs. Contrary to [9], we directly back-propagate through our loss, without having
to explicit a coupling matrix and resorting to the envelope Theorem. Compared to [9] and the use
of entropic-GW , the time per epoch is more than one order of magnitude faster, as expected from
previous experiment.

5 Discussion and conclusion

In this work, we establish a new result about Quadratic Assignment Problem when matrices are
squared euclidean distances on the real line, and use it to state a closed form expression for GW
between monodimensional measures. Building upon this result we deﬁne a new similarity measure,
called the Sliced Gromov-Wasserstein and a variant Rotation-invariant SGW and prove that both
conserve various properties of the GW distance while being cheaper to compute and applicable in
a large-scale setting. Notably SGW can be computed in 1 second for distributions with 1 million
samples each. This paves the way for novel promising machine learning applications of optimal
transport between metric spaces.

Yet, several questions are raised in this work. Notably, our method perfectly ﬁts the case when the
two distributions are given empirically through samples embedded in an Hilbertian space, that allows
for projection on the real line. This is the case in most of the machine learning applications that use
the Gromov-Wasserstein distance. However, when only distances between samples are available,
the projection operation can not be carried anymore, while the computation of GW is still possible.
One can argue that it is possible to embed either isometrically those distances into a Hilbertian space,
or at least with a low distortion, and then apply the presented technique. Our future line of work
considers this option, as well as a possible direct reasoning on the distance matrix. For example,
one should be able to consider geodesic paths (in a graph for instance) as the equivalent appropriate
geometric object related to the line. This constitutes the direct follow-up of this work, as well as
a better understanding of the accuracy of the estimated discrepancy with respect to the ambiant
dimension and the projections number.

Acknowledgements

We would like to thank Nicolas Klutchnikoff for the hepful discussions. This work beneﬁted from
the support from OATMIL ANR-17-CE23-0012 project of the French National Research Agency
(ANR). We gratefully acknowledge the support of NVIDIA Corporation with the donation of the
Titan X GPU used for this research.

References

[1] Nicolas Bonneel, Gabriel Peyr´e, and Marco Cuturi. Wasserstein barycentric coordinates:
histogram regression using optimal transport. ACM Transactions on Graphics (TOG), 35(4):71–
1, 2016.

[2] Matthew Thorpe, Serim Park, Soheil Kolouri, Gustavo K. Rohde, and Dejan Slepˇcev. A
transportation lp distance for signal analysis. Journal of Mathematical Imaging and Vision,
59(2):187–210, 2017.

[3] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In

International Conference on Machine Learning, volume 70, pages 214–223, 2017.

9

[4] Nicolas Courty, R´emi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport
IEEE Transactions on Pattern Analysis and Machine Intelligence,

for domain adaptation.
39(9):1853–1865, 2017.

[5] G. Huang, C. Guo, M. Kusner, Y. Sun, F. Sha, and K. Weinberger. Supervised word mover’s
distance. In Advances in Neural Information Processing Systems, pages 4862–4870, 2016.
[6] Yuguang Yan, Wen Li, Hanrui Wu, Huaqing Min, Mingkui Tan, and Qingyao Wu. Semi-
In International Joint

supervised optimal transport for heterogeneous domain adaptation.
Conference on Artiﬁcial Intelligence, pages 2969–2975, 2018.

[7] Danielle Ezuz, Justin Solomon, Vladimir G. Kim, and Mirela Ben-Chen. GWCNN: A Metric
Alignment Layer for Deep Shape Analysis. Computer Graphics Forum, 36(5):49–57, 2017.
[8] Titouan Vayer, Laetitia Chapel, R´emi Flamary, Romain Tavenard, and Nicolas Courty. Optimal
In International Conference on

Transport for structured data with application on graphs.
Machine Learning, volume 97, 2019.

[9] Charlotte Bunne, David Alvarez-Melis, Andreas Krause, and Stefanie Jegelka. Learning
Generative Models across Incomparable Spaces. In International Conference on Machine
Learning, volume 97, 2019.

[10] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances

in Neural Information Processing Systems, pages 2292–2300, 2013.

[11] Aude Genevay, Marco Cuturi, Gabriel Peyr´e, and Francis Bach. Stochastic optimization for
large-scale optimal transport. In Advances in Neural Information Processing Systems, pages
3440–3448, 2016.

[12] Julien Rabin, Gabriel Peyr´e, Julie Delon, and Marc Bernot. Wasserstein barycenter and its
application to texture mixing. In International Conference on Scale Space and Variational
Methods in Computer Vision, pages 435–446. Springer, 2011.

[13] Nicolas Bonnotte. Unidimensional and Evolution Methods for Optimal Transportation. PhD

thesis, 2013.

[14] Nicolas Bonneel, Julien Rabin, Gabriel Peyr´e, and Hanspeter Pﬁster. Sliced and Radon Wasser-
stein Barycenters of Measures. Journal of Mathematical Imaging and Vision, 1(51):22–45,
2015.

[15] Soheil Kolouri, Yang Zou, and Gustavo K. Rohde. Sliced wasserstein kernels for probability
distributions. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.

[16] Soheil Kolouri, Phillip E. Pope, Charles E. Martin, and Gustavo K. Rohde. Sliced wasserstein

auto-encoders. In International Conference on Learning Representations, 2019.

[17] Ishan Deshpande, Ziyu Zhang, and Alexander G. Schwing. Generative modeling using the
sliced wasserstein distance. In IEEE Conference on Computer Vision and Pattern Recognition,
pages 3483–3491, 2018.

[18] Antoine Liutkus, Umut Simsekli, Szymon Majewski, Alain Durmus, and Fabian-Robert St¨oter.
Sliced-Wasserstein ﬂows: Nonparametric generative modeling via optimal transport and dif-
fusions. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research, pages 4104–4113, Long Beach, California, USA, 09–15 Jun 2019. PMLR.

[19] Jiqing Wu, Zhiwu Huang, Dinesh Acharya, Wen Li, Janine Thoma, Danda Pani Paudel, and
Luc Van Gool. Sliced wasserstein generative models. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.

[20] Samir Chowdhury and Facundo M´emoli. The gromov-wasserstein distance between networks

and stable network invariants. arXiv preprint arXiv:1808.04337, 2018.

[21] Justin Solomon, Gabriel Peyr´e, Vladimir G. Kim, and Suvrit Sra. Entropic metric alignment for

correspondence problems. ACM Transactions on Graphics (TOG), 35(4):72:1–72:13, 2016.

[22] Gabriel Peyr´e, Marco Cuturi, and Justin Solomon. Gromov-Wasserstein Averaging of Kernel
and Distance Matrices. In International Conference on Machine Learning, pages 2664–2672,
2016.

[23] C´edric Villani. Optimal Transport: Old and New. Springer, 2008.

10

[24] David Alvarez-Melis, Stefanie Jegelka, and Tommi S. Jaakkola. Towards optimal transport
with global invariances. In International Conference on Artiﬁcial Intelligence and Statistics,
volume 89, pages 1870–1879, 2019.

[25] Facundo Memoli. Gromov wasserstein distances and the metric approach to object matching.

Foundations of Computational Mathematics, pages 1–71, 2011.

[26] David Alvarez-Melis and Tommi S Jaakkola. Gromov-wasserstein alignment of word embedding

spaces. In Conference on Empirical Methods in Natural Language Processing, 2018.

[27] Karl-Theodor Sturm. The space of spaces: curvature bounds and gradient ﬂows on the space of

metric measure spaces. arXiv e-prints, page arXiv:1208.0434, 2012.

[28] Tjalling Koopmans and Martin J. Beckmann. Assignment problems and the location of economic

activities. Econometrica: journal of the Econometric Society, (53–76), 1957.

[29] Eranda C¸ ela, Vladimir Deineko, and Gerhard J. Woeginger. New special cases of the Quadratic
Assignment Problem with diagonally structured coefﬁcient matrices. European journal of
operational research, 267(3):818–834, 2018.

[30] Eranda C¸ ela, Nina S. Schmuck, Shmuel Wimer, and Gerhard J. Woeginger. The Wiener

maximum quadratic assignment problem. Discrete Optimization, 8:411–416, 2011.

[31] Eranda C¸ ela, Vladimir G. Deineko, and Gerhard J. Woeginger. Well-solvable cases of the QAP

with block-structured matrices. Discrete applied mathematics, 186:56–65, 2015.

[32] Eranda C¸ ela. The Quadratic Assignment Problem: Theory and Algorithms, volume 1. Springer

Science & Business Media, 2013.

[33] Eliane Loiola, Nair Abreu, Paulo Boaventura-Netto, Peter Hahn, and Tania Querido. A survey
of the quadratic assignment problem. European Journal of Operational Research, 176:657–690,
2007.

[34] Facundo M´emoli and Tom Needham. Gromov-Monge quasi-metrics and distance distributions.

arXiv:1810.09646, 2018.

[35] Haggai Maron and Yaron Lipman. (probably) concave graph matching. In Advances in Neural

Information Processing Systems, pages 408–418, 2018.
[36] Bernhard Sch¨olkopf. The kernel trick for distances.

Processing Systems, pages 301–307, 2001.

In Advances in Neural Information

[37] Gabriel Peyr´e and Marco Cuturi. Computational optimal transport. Foundations and Trends in

Machine Learning, 11 (5-6):355–602, 2019.

[38] P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix

manifolds. Princeton University Press, 2009.

[39] Franc¸ois-Pierre Paty and Marco Cuturi. Subspace robust Wasserstein distances. In Kamalika
Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference
on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 5072–
5081, Long Beach, California, USA, 09–15 Jun 2019. PMLR.

[40] Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo,
Zhizhen Zhao, David Forsyth, and Alexander G. Schwing. Max-sliced wasserstein distance and
its use for gans. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2019.

[41] Benjamin Charlier, Jean Feydy, and Joan Glaunes. Kernel operations on the gpu, with autodiff,

without memory overﬂows. https://github.com/getkeops/keops, 2018.

[42] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.

[43] James Townsend, Niklas Koep, and Sebastian Weichwald. Pymanopt: A python toolbox for
optimization on manifolds using automatic differentiation. The Journal of Machine Learning
Research, 17(1):4755–4759, 2016.

[44] Mayank Meghwanshi, Pratik Jawanpuria, Anoop Kunchukuttan, Hiroyuki Kasai, and Bamdev
arXiv preprint

Mishra. Mctorch, a manifold optimization library for deep learning.
arXiv:1810.01811, 2018.

11

[45] Dougal Maclaurin, David Duvenaud, and Ryan P Adams. Autograd: Effortless gradients in

numpy. In ICML 2015 AutoML Workshop, 2015.

[46] R’emi Flamary and Nicolas Courty. Pot python optimal transport library, 2017.

[47] Bernhard Schmitzer. Stabilized Sparse Scaling Algorithms for Entropy Regularized Transport

Problems. SIAM Journal on Scientiﬁc Computing, 41(3):A1443–A1481, 2016.

[48] Raif M Rustamov, Maks Ovsjanikov, Omri Azencot, Mirela Ben-Chen, Fr´ed´eric Chazal, and
Leonidas Guibas. Map-based exploration of intrinsic shape differences and variability. ACM
Transactions on Graphics (TOG), 32(4):72, 2013.

[49] R. Tyrrell Rockafellar. Convex analysis. Princeton Mathematical Series. Princeton University

Press, Princeton, N. J., 1970.

[50] K.G. Murty. Linear Complementarity, Linear and Nonlinear Programming. Sigma series in

applied mathematics. Heldermann, 1988.

[51] H. Cram´er et H. Wold. Some Theorems on Distribution Functions. Journal of the London

Mathematical Society, vol. s1-11, no. 4, pages 290–294, 1936.

6 Suplementary materials

Notations
In the following we will make the difference between a vector and a scalar by noting
in bold vectors so that x ∈ Rn and x ∈ R. We note by Fµ the Fourrier transform of µ for a
probability measure µ ∈ P(Rp). It is deﬁned for s ∈ Rp by Fµ(s) =
e−2iπ(cid:104)s,x(cid:105)dµ(x). We recall
the rearrangement inequality:

´

Theorem 6.1 (Rearrangement Inequality). Let x1 ≤ · · · ≤ xn, y1 ≤ · · · ≤ yn then we have:

∀σ ∈ Sn,

(cid:88)

i

xiyn+1−i ≤

(cid:88)

i

xiyσ(i) ≤

(cid:88)

xiyi

i

(9)

If the numbers are different then the lower bound (resp upper bound) is attained only for the
permutation which reverses the order (resp for the identiy permutation)

7 Proof for the QAP

In this section we aim at proving the new special case of the QAP, which is recalled in the next
theorem:

Theorem 7.1 (A new special case of the QAP.). For real numbers x1 < · · · < xn and y1 < · · · < yn,

min
σ∈Sn

(cid:88)

i,j

−(xi − xj)2(yσ(i) − yσ(j))2

(10)

is achieved either by the identity permutation σ(i) = i (Id) or the anti-identity permutation σ(i) =
n + 1 − i (anti − Id). In other words:

∃σ ∈ {Id, anti − Id}, σ ∈ arg min

σ∈Sn

(cid:88)

i,j

−(xi − xj)2(yσ(i) − yσ(j))2

(11)

Let us note I = {x, y ∈ Rn × Rn|x1 < x2 < · · · < xn , y1 < y2 < · · · < yn}. We consider for
x, y ∈ I:

max
σ∈Sn

Z(x, y, σ) = max
σ∈Sn

(cid:88)

i,j

(xi − xj)2(yσ(i) − yσ(j))2

(12)

12

The original problem is equivalent to maximizing Z(x, y, σ) over Sn. Given x, y ∈ I, we deﬁne
X def= (cid:80)

i xi and Y def= (cid:80)
Z(x, y, σ) = max
σ∈Sn

max
σ∈Sn

i yi. Then:
(cid:88)

(xi − xj)2(yσ(i) − yσ(j))2

(x2

i + x2

j )(y2

i,j
σ(i) + y2

σ(j)) − 2

(cid:88)

i,j

xixj(y2

σ(i) + y2

σ(j)) − 2

yσ(i)yσ(j)(x2

i + x2
j )

(cid:88)

i,j

(cid:88)

i,j

= max
σ∈Sn

+ 4

(cid:88)

i,j

= max
σ∈Sn

2n

(cid:88)

i

xixjyσ(i)yσ(j)

+ 4

(cid:88)

i,j

xixjyσ(i)yσ(j) + 2(

= max
σ∈Sn

2n

(cid:88)

i

i y2
x2

σ(i) − 4X

i y2
x2

σ(i) − 2

(cid:88)

xixj(y2

σ(i) + y2

σ(j)) − 2

yσ(i)yσ(j)(x2

i + x2
j )

(cid:88)

i,j

i,j
(cid:88)

(cid:88)

x2
i )(

y2
i )

i
xiy2

σ(i) − 4Y

i
(cid:88)

i

(cid:88)

i

x2
i yσ(i) + 4

(cid:88)

i,j

xixjyσ(i)yσ(j) + 2(

(cid:88)

(cid:88)

x2
i )(

y2
i )

i

i

(∗)

= Cte + 2(cid:0) max
σ∈Sn

(cid:88)

i

nx2

i y2

σ(i) − 2

(cid:88)

(Xxiy2

σ(i) + Y x2

i yσ(i)) + 2(

i

(cid:88)

xiyσ(i))2(cid:1)

i

where in (*) we deﬁned Cte def= 2((cid:80)
have:
∀x, y ∈ I, argmax

Z(x, y, σ) = argmax

i x2

σ∈Sn

σ∈Sn

i

i )((cid:80)

i y2

i ) the term that does not depend on σ. Overall we

(cid:88)

nx2

i y2

σ(i)−2

(cid:88)

i

(Xxiy2

σ(i)+Y x2

i yσ(i))+2(

(cid:88)

xiyσ(i))2

i

(13)

Since Z is invariant by translation of x, y we can suppose without loss of generality that X = Y = 0.
We consider the set D = {x, y ∈ Rn × Rn|x1 < x2 < · · · < xn , y1 < y2 < · · · < yn, (cid:80)
i xi =
(cid:80)

j yj = 0}. We want to ﬁnd for x, y ∈ D:

max
σ∈Sn

n

(cid:88)

i

i y2
x2

σ(i) + 2

(cid:33)2

xiyσ(i)

(cid:32)

(cid:88)

i

def
= max
σ∈Sn

g(x, y, σ)

We have the following result:
Lemma 7.2. Let x, y ∈ D and consider the problem:

max
π∈DS

(cid:88)

(x2

i y2

j + 2xiyjxkyl)πijπkl

ijkl

(QAP)

(QP)

where DS is the set of doubly stochastic matrices. Then (QP) and (QAP) are equivalent. More
precisely if σ∗ is an optimal solution of (QAP) then πσ∗ deﬁned by πσ∗ (i, j) = 1 if j = σ∗(i) else 0
for all (i, j) ∈ {1, . . . , n}2 is an optimal solution of (QP) and if π∗ is an optimal solution of (QP)
then it is supported on a permutation σ∗ which is an optimal solution of (QAP).

(cid:88)

n

ij

max
Pij ∈{0,1}
∀j (cid:80)
i Pij =1
∀i (cid:80)
j Pij =1

i y2
x2

j Pij + 2

(cid:88)

ijkl

xixkyjylPijPkl

Proof. The problem (QAP) can be rewritten as:
(cid:33)2

(cid:32)

(cid:88)

i y2
x2

j Pij + 2

xiyjPij

=

(cid:88)

i,j

n

max
Pij ∈{0,1}
∀j (cid:80)
i Pij =1
∀i (cid:80)
j Pij =1

∗=

max
Pij ∈{0,1}
∀j (cid:80)
i Pij =1
∀i (cid:80)
j Pij =1

ij

(cid:88)

ijkl

i y2
x2

j PijPkl + 2

(cid:88)

ijkl

xiyjxkylPijPkl =

max
Pij ∈{0,1}
∀j (cid:80)
i Pij =1
∀i (cid:80)
j Pij =1

(cid:88)

(x2

i y2

j + 2xiyjxkyl)PijPkl

ijkl

In (*) we used (cid:80)

k,l Pk,l = n. We consider the following relaxation of (14) as:

max
π∈DS

(cid:88)

ijkl

(x2

i y2

j + 2xiyjxkyl)πijπkl

13

(14)

(15)

which is a maximization of a convex function. More precislely it is quadratic programming problem
which Hessian is positive semi-deﬁnite xxT ⊗ yyT . Since the problem is a maximization of a convex
function an optimal solution π∗ of (QP) lies necassarily in the extremal points of DS [49] such that
both (QP) and (QAP) are equivalent: if π∗ is an optimal solution it is necessarily supported on a
σ∗ ∈ Sn such that σ∗ is an optimal solution of (QAP) and if σ∗ ∈ Sn is an optimal solution of (QAP)
ij = 1 if j = σ∗(i) else 0 for all (i, j) ∈ {1, . . . , n}2 is an optimal solution of
then π∗ deﬁned by π∗
(QP).

Lemma 7.3. Let x, y ∈ D. For σ ∈ Sn we note C(x, y, σ) = (cid:80)
solution of (QP) with σ∗ the permutation associated to π∗.
If C(x, y, σ∗) > 0 then π∗ = In is the identiy and if C(x, y, σ∗) < 0 then π∗ = Jn is the
anti-identity.

i xiyσ(i). Let π∗ an optimal

To prove this result we will rely on the following theorem which gives necessary conditions for being
an optimal solution of (QP):
Theorem 7.4 (Theorem 1.12 in [50]). Consider the following (QP):

Then if x∗ is an optimal solution of (16) it is an optimal solution of the following (LP):

minx f (x) = cx + xT Qx
Ax = b, x ≥ 0

s.t.

minx f (x) = (c + xT

∗ Q)x
Ax = b, x ≥ 0

s.t.

(16)

(17)

Proof. Of Lemma 7.3. Applying Theorem 7.4 in our case gives that if π∗ is a solution of (QP) it
necessarily a solution of the following (LP):

max
π∈DS

(cid:88)

ijkl

(x2

i y2

j + 2xiyjxkyl)π∗

ijπkl = n

(cid:88)

ij

i y2
x2

j π∗

ij + max
π∈DS

(cid:88)

2(

xiyjπ∗

ij)(

ij

(cid:88)

kl

xkylπkl) (18)

Since π∗ is supported on a permutation σ∗ this gives:

(cid:88)

n

i

x2
i y2

σ∗(i) + max
π∈DS

C(x, y, σ∗)

(cid:88)

kl

xkylπkl

(LP)

where C(x, y, σ∗) = 2 (cid:0)(cid:80)

i xiyσ∗(i)

(cid:1).

• If C(x, y, σ∗) > 0 then this (LP) has a unique solution which is the identity π∗ = In. This
is a consequence of the Rearrangement Inequality (see Theorem 6.1) which states that for
all permutations (cid:80)
i xiyi (since xi and yj are distinct). Using the fact that an
optimal solution of (LP) is supported on a permutation concludes.

i xiyσ(i) < (cid:80)

• If C(x, y, σ∗) < 0 then the anti-identity is the unique optimum with the same reasoning
i xiyσ(i) for all permutation because of Rearrangement Inequality.

i xiyn+1−i < (cid:80)

since (cid:80)

Using both results we can prove the following proposition which is the main ingredient to prove
Theorem 7.1:
Proposition 7.1. Let x, y ∈ D and σ∗ a solution of (QAP) i.e. σ∗ ∈ arg maxσ∈Sn g(x, y, σ). For
σ ∈ Sn we note C(x, y, σ) = (cid:80)
If C(x, y, σ∗) > 0 then σ∗ is the identiy permutation σ∗(i) = i and if C(x, y, σ∗) < 0 then σ∗ is
the anti-identity permutation σ∗(i) = n + 1 − i for all i ∈ {1, . . . , n}.

i xiyσ(i).

Proof. Let σ∗ be an optimal solution of (QAP) and π∗ deﬁned by π∗
ij = 1 if j = σ∗(i) else 0. By
Lemma 7.2 we know that π∗ is an optimal solution of (QP). Consider the case C(x, y, σ∗) > 0.
Suppose that σ∗ is not the identity, then π∗ (cid:54)= In which is not possible by Lemma 7.3 since π∗ is an
optimal solution of (QP). Same applies for C(x, y, σ∗) < 0 and the anti-identity.

14

To state that (QAP) admits the identity or the anti-identity as optimal permutations we will rely on
the previous proposition and on the continuity of the loss g:
Lemma 7.5 (Continuity of g). Let x, y ∈ D ﬁxed. There exists (cid:15)x,y > 0 such that for all (cid:107)h(cid:107) < (cid:15)x,y
we have:

arg max
σ∈Sn

g(x + h, y, σ) ⊂ arg max

g(x, y, σ)

(19)

σ∈Sn

Proof. Let x, y ∈ D, σ∗ ∈ arg maxσ∈Sn g(x, y, σ) and τ any permutation in Sn such that τ /∈
arg maxσ∈Sn g(x, y, σ). Then we have g(x, y, σ∗) > g(x, y, τ ). Let η = g(x, y, σ∗)−g(x, y, τ ) >
0. For all permutation β we have that g(., y, β) is continuous. In this way:

∀β ∈ Sn, ∃(cid:15)x,y(β, σ∗, τ ) > 0, ∀(cid:107)h(cid:107) < (cid:15)x,y(β, σ∗, τ ), |g(x + h, y, β) − g(x, y, β)| <

η
4

(20)

Let h ∈ Rn such that (cid:107)h(cid:107) <

min
(β,σ,τ (cid:48))∈(Sn)3
g(x + h, y, σ∗) − g(x + h, y, τ ) = g(x + h, y, σ∗) − g(x, y, σ∗)

(cid:15)x,y(β, σ, τ (cid:48)). By (20) applied to σ∗ and τ :

+ g(x, y, σ∗) − g(x, y, τ ) + g(x, y, τ ) − g(x + h, y, τ )

η
4

+ η −

η
4

> 0

> −

=

η
2

(21)

So that g(x + h, y, σ∗) > g(x + h, y, τ ) and in this way τ /∈ arg maxσ∈Sn g(x + h, y, σ) be-
cause σ∗ leads to a striclty better cost. Overall we have proven that for any permutation τ , if
(cid:15)x,y(β, σ, τ (cid:48)) then τ /∈ arg maxσ∈Sn g(x +
τ /∈ arg maxσ∈Sn g(x, y, σ) and (cid:107)h(cid:107) <
min
(β,σ,τ (cid:48))∈(Sn)3
h, y, σ) which proves that arg maxσ∈Sn g(x + h, y, σ) ⊂ arg maxσ∈Sn g(x, y, σ).

Using the previous lemma we can now prove the following result:
Lemma 7.6. Let x, y ∈ D ﬁxed. There exists (cid:15)0 ∈ Rn such that:

arg max
σ∈Sn
arg max
σ∈Sn

g(x + (cid:15)0, y, σ) ⊂ arg max

σ∈Sn

g(x, y, σ)

g(x + (cid:15)0, y, σ) ⊂ {Id, anti − Id}

(22)

Proof. Let x, y ∈ D. We consider (cid:15)0 = (ζ, −ζ, 0, . . . , 0) with ζ > 0 small enough to ensure
ζ < x2−x1
i(xi + (cid:15)0(i)) =
(cid:80)

and (cid:107)(cid:15)0(cid:107) < (cid:15)x,y deﬁned in Lemma 7.5. We have x + (cid:15)0, y ∈ D since (cid:80)

i xi + ζ − ζ = 0 and x1 + (cid:15)0(1) < · · · < xN + (cid:15)0(N ) since x1 + ζ < x2 − ζ.

2

Let σ∗
(cid:15)0
Moreover we have C(x + (cid:15)0, y, σ∗
(cid:15)0

∈ arg maxσ∈Sn g(x + (cid:15)0, y, σ). By Lemma 7.5 we have σ∗
(cid:15)0
) = (cid:80)
(0) − yσ∗
(cid:15)0

(i) + ζ(yσ∗
(cid:15)0

i xiyσ∗
(cid:15)0

(1)).

∈ arg maxσ∈Sn g(x, y, σ).

• If (cid:80)

i xiyσ∗
(cid:15)0

(i) = 0 then C(x + (cid:15)0, y, σ∗
(cid:15)0

) = ζ(yσ∗
(cid:15)0

(0) − yσ∗
(cid:15)0

distinct. We can apply Proposition 7.1 with x + (cid:15)0, y ∈ D to conclude that σ∗
the identity or the anti-identity.

(1)) (cid:54)= 0 since all yi are
(cid:15)0 is wether

• If (cid:80)

i xiyσ∗
(cid:15)0

(i) (cid:54)= 0 then σ∗
(cid:15)0
Proposition 7.1 with x, y ∈ D we can conclude that σ∗
anti-identity.

∈ arg maxσ∈Sn g(x, y, σ) and C(x, y, σ∗
(cid:15)0

) (cid:54)= 0 so by
(cid:15)0 is wether the identity or the

Corollary 7.1 (Theorem 7.1 is valid). Let x, y ∈ D. The identity or the anti-identity is an optimal
solution of (QAP)

Proof. Let x, y ∈ D. We consider (cid:15)0 deﬁned in Lemma 7.6 and σ∗
∈ arg maxσ∈Sn g(x + (cid:15)0, y, σ).
(cid:15)0
Then by Lemma 7.6 σ∗
∈
arg maxσ∈Sn g(x, y, σ) so it is an optimal solution of (QAP). This concludes that the identity or the
anti-identity is an optimal solution of (QAP) which proves Theorem 7.1.

(cid:15)0 is wether the identity or the anti-identity. Moreover by Lemma 7.6 σ∗
(cid:15)0

15

8 Computing GW in the 1d case

As seen in the previous theorem ﬁnding the optimal permutation σ∗ can be found in O(n log(n)).
Moreover the ﬁnal cost can be written using binomial expansion:

(cid:88)

i,j

(cid:0)(xi − xj)2 − (yσ∗(i) − yσ∗(j))2(cid:1)2

= 2n

(cid:88)

x4
i − 8

(cid:88)

x3
i

(cid:88)

xi + 6(

(cid:88)

i )2
x2

i
(cid:88)

i
(cid:88)

y3
i

y4
i − 8

+ 2n

i
(cid:88)

(cid:88)

xi)2(

i
yi)2

− 4(

i
(cid:88)

i

yi + 6(

i
(cid:88)

i

i )2
y2

i
(cid:88)

− 4n

i

i y2
x2

σ∗(i) + 8

− 8(

i
(cid:88)

i

xiyσ∗(i))2

(cid:88)

(cid:88)

((

i

i

xi)xiy2

σ∗(i) + (

yi)x2

i yσ∗(i))

(cid:88)

i

(23)

which can be computed in O(n) operations.

9 Claims about GW

This section aims at proving some claims in the paper about GW . Let us recall the notations of the
paper.
We consider discrete measures µ ∈ P(Rp) and ν ∈ P(Rq) with p ≤ q on euclidean spaces such that
µ = (cid:80)n
Let cX : Rp × Rp (cid:55)→ R+ (resp. cY : Rq × Rq (cid:55)→ R+) measuring the similarity between the points
in µ (resp. ν). The Gromov-Wasserstein (GW ) distance is deﬁned as:

i=1 bjδyj , where a ∈ Σn and b ∈ Σm are histograms.

i=1 aiδxi and ν = (cid:80)m

GW 2

2 (cX , cY , µ, ν) = min

J(cX , cY , π)

π∈Π(a,b)

(24)

where

J(cX , cY , π) =

(cid:88)

i,j,k,l

(cid:12)cX (xi, xk) − cY (yj, yl)(cid:12)
(cid:12)
(cid:12)

2

πi,jπk,l

9.1 GW when squared euclidean distances are used

When cX , cY are distances it has been shown in [25] that GW deﬁnes a distance on the space of metric
measure spaces quotiented by the measure-preserving isometries. More precisely, GW is symmetric,
satisﬁes the triangle inequality and GW 2
2 (cX , cY , µ, ν) = 0 iff there exists f : supp(µ) → supp(ν)
such that

f #µ = ν

∀x, x(cid:48) ∈ supp(µ)2, cX (x, x(cid:48)) = cY (f (x), f (x(cid:48)))

(25)

(26)

In the paper we claim the following lemma:
Lemma 9.1. Using previous notations with cX (x, x(cid:48)) = (cid:107)x − x(cid:48)(cid:107)2
Then GW 2
which satisﬁes (25) and (26)

2,q.
2 (cX , cY , µ, ν) = 0 iff there exists a measure preserving isometry from supp(µ) to supp(ν)

2,p , cY (y, y(cid:48)) = (cid:107)y − y(cid:48)(cid:107)2

Proof. If such an function exists by considering the coupling π = (Id × f )#µ it is clear that π is
optimal and has a zero cost so that GW 2
2 (cX , cY , µ, ν) = 0

2 (cX , cY , µ, ν) = 0. Conversely, GW 2

16

implies that cX (x, x(cid:48)) = cY (y, y(cid:48)) for all (x, y), (x(cid:48), y(cid:48)) in the support of an optimal plan π∗. This
sufﬁces to prove the existence of a measure preserving isometry (see (a) in Proof of Theorem 5.1 in
[25])

9.2 Equivalence between GM and GW in the discrete case

This paragraph aims at proving the equivalence between GM and GW . We will prove the following
theorem (that is more general than the one used in the paper which only considers one-dimensional
measures):
Theorem 9.2. Equivalence between GW and GM for discrete measures
With µ, ν deﬁned previously and cX (x, x(cid:48)) = (cid:107)x − x(cid:48)(cid:107)2
also that m = n and ∀i ∈ {1, ..., n}, ai = bi = 1
n
Then GW2(cX , cY , µ, ν) = GM2(cX , cY , µ, ν).

2,p , cY (y, y(cid:48)) = (cid:107)y − y(cid:48)(cid:107)2

2,q. Let us suppose

Proof. The proof is essentially based on theoretical results from [35] and on Theorem 7.1. In
− tr(BXT AX) where Πn the set of
[35] authors consider the minimizing energy problem min
X∈Πn
−

permutation matrices. In fact, the GM problem deﬁned in this chapter is equivalent to min
X∈Πn

tr(BXT AX) by considering A = ((cid:107)xi − xj(cid:107)2

2,p)i,j and B = ((cid:107)yi − yj(cid:107)2

2,q)i,j.

To tackle this problem authors propose to minimize −tr(BXT AX) over the set of doubly stochastic
matrices (which is the convex-hull of Πn):

DS = {X ∈ Rn×n s.t. X1n = XT 1n = 1n , X ≥ 0}
Minimizing −tr(BXT AX) over DS is equivalent to solving the GW distance when ai = bj = 1
n .
The paper proves that when both A and B are conditionally positive (or negative) deﬁnite of order
1 then the relaxation leads to the same optimum so that the minimum over DS is the same as the
minimum over Πn [35, Theorem 1]. Yet A and B deﬁned previously satisfy this property (see
examples under Deﬁnition 2 in [35]) and so GW and GM coincide.

Moreover when p = q = 1 and when the sample are sorted we can apply Theorem 7.1 to prove
that an optimal permutation of the GM problem is found whether at the identity or the anti-identity
permutation which concludes the proof.

10 Properties of SGW

(cid:107).(cid:107) is a norm on Rp. To state the properties of SGW , we will need the Arzela-Ascoli Theorem. Let
(X, d) be a compact metric space and C(X, Rp) the space of all continuous functions from X to Rp.
We recall:

• A family F ⊂ C(X, Rp) is bounded means that there exists a positive constant M < ∞

such that (cid:107)f (x)(cid:107) ≤ M for all x ∈ X and f ∈ F

• A family F ⊂ C(X, Rp) is equicontinuous means that for every (cid:15) > 0 there exists δ > 0

(which depends only on (cid:15)) such that for x, y ∈ X:

d(x, y) < δ ⇒ (cid:107)f (x) − f (y)(cid:107) < (cid:15) ∀f ∈ F.

The Arzela-Ascoli Theorem states that if (fn)n∈N is a sequence in C(X, Rp) that is bounded and
equicontinuous then it has a uniformly convergent subsequence.

We recall the theorem (measures µ and ν are deﬁned discrete measures with the same number of
atoms):
Theorem 10.1. Properties of SGW

• For all ∆, SGW∆ and RISGW are translation invariant. RISGW is also rota-
tional invariant when p = q, more precisely if Q ∈ O(p) is an orthogonal matrix,
RISGW (Q#µ, ν) = RISGW (µ, ν) (same for any Q(cid:48) ∈ O(p) applied on ν).

17

• SGW and RISGW are pseudo-distances on P(Rp), i.e. they are symmetric, satisfy the

triangle inequality and SGW (µ, µ) = RISGW (µ, µ) = 0 .

• Let µ, ν ∈ P(Rp) × P(Rp) be probability distributions with compact supports.

If
SGW (µ, ν) = 0 then µ and ν are isomorphic for the distance induced by the (cid:96)1 norm on
Rp, i.e. d(x, x(cid:48)) = (cid:80)p

i| for (x, x(cid:48)) ∈ Rp × Rp. In particular this implies:

i=1 |xi − x(cid:48)

SGW (µ, ν) = 0 =⇒ GW2(d, µ, ν) = 0

(27)

The invariance by translation is clear since the costs are invariant by translation of the support of the
measures. The pseudo-distances properties are straightforward thanks to the properties of GW . For
the invariance by rotation if p = q then Vp(Rp) is bijective with O(p) so for Q ∈ O(p):

RISGW (Q#µ, ν) = min

SGW∆(Q#µ, ν)

∆∈Vp(Rp)

= min

∆∈O(p)

SGW∆(Q#µ, ν)

∆∈O(p)

= min

E
θ∼λq−1
E
θ∼λq−1
= RISGW (µ, ν)

= min

∆(cid:48)∈O(p)

[GW (d2, Pθ#(∆Q#µ), Pθ#ν)]

(28)

[GW (d2, Pθ#∆(cid:48)#µ, Pθ#ν)]

On the other side for ν a change of formula on theta gives the result.

For the case SGW = 0 =⇒ GW = 0 it will be a consequence of the following theorem:
Theorem 10.2. Let µ, ν ∈ P(Rp) × P(Rp) be probability distributions such that µ, ν have compact
supports. If for almost all θ ∈ Sp−1, Pθ#µ and Pθ#ν are isomoprhic then µ and ν are isomorphic.
In other words if for almost all θ ∈ Sp−1 we have:

∃Tθ : supp(Pθ#µ) ⊂ R (cid:55)→ supp(Pθ#ν) ⊂ R, surjective s.t. Tθ#(Pθ#µ) = Pθ#ν
∀x, x(cid:48) ∈ supp(Pθ#µ), |Tθ(x) − Tθ(x(cid:48))| = |x − x(cid:48)|

(29)

Then there exists a measure preserving isometry f between supp(µ) and supp(ν). More precisely we
have f #µ = ν and:

∀x, x(cid:48) ∈ supp(µ), (cid:107)f (x) − f (x(cid:48))(cid:107)1 = (cid:107)x − x(cid:48)(cid:107)1

(30)

To prove this theorem we will exhibit the isometry. This result can be put in light of Cramer–Wold
theorem [51] which states that a probability measure is uniquely determined by the totality of its
one-dimensional projections. Equivalently, if we consider two probability measures so that the
one-dimensional measures resulting from the projections over all the hypersphere are equal then the
measures are equal. The equality relation is replaced in our theorem by the isomoprhism relation.

The proof is divided into four parts. In the ﬁrst one, we construct an ”almost orthogonal” basis on
which measures are isomorphic. Building upon this result we deﬁne a sequence of functions from
supp(µ) to supp(ν) and show that it has a convergent subsequence. We conclude by proving that the
limit of the subsequence is actually a good candidate for being the isometry we are looking for. In the
following (cid:107).(cid:107)1 denotes the (cid:96)1 norm, (cid:107).(cid:107)2 denotes the (cid:96)2 norm and p ≥ 2. We recall that Fµ is the
Fourier transform of µ.

We consider the following Qθ property for θ ∈ Sp−1:

∃Tθ : supp(Pθ#µ) ⊂ R (cid:55)→ supp(Pθ#ν) ⊂ R, surjective s.t. Tθ#(Pθ#µ) = Pθ#ν
∀x, x(cid:48) ∈ supp(Pθ#µ), |Tθ(x) − Tθ(x(cid:48))| = |x − x(cid:48)|

(Qθ)

Informally if we have the Qθ property for θ ∈ Sp−1 it implies that µ and ν are isomorphic on the 1D
line given by the projection w.r.t. θ. We have the following lemma:
Lemma 10.3. Let µ, ν ∈ P(Rp) × P(Rp) and suppose that Qθ holds for almost all θ ∈ Sp−1. Let
n > p − 1. There exists a basis (e1(n), ..., ep(n)) of Rp part of the following spaces:

Bn
p

def
= {(θ1, ..., θp) ∈ (Sp−1)p s.t. |(cid:104)θi, θj(cid:105)| <

1
n

}

(31)

18

and

Q

def
= {(θ1, ..., θp) ∈ (Sp−1)p s.t. ∀i ∈ {1, ..., p}, Qθi}

(32)

p−1 the product measure λp−1⊗...⊗λp−1 where λp−1 is the uniform measure on the sphere.
p ) > 0.
p ∩ Q) > 0.

Proof. We want to construct a basis (e1, ..., ep) as orthogonal as possible such that for all i we have
Qei.
We note λ⊗p
p is an open set as inverse image by a continuous function of an open set. Then λ⊗p
Bn
Moreover, since for almost all θ ∈ Sp−1 we have Qθ then λ⊗p
p−1(Bn
In this way we can consider (e1(n), ..., ep(n)) ∈ Bn
p ∩ Q. If n > p − 1 the Gram matrix of
(e1(n), ..., ep(n)) is strictly diagonal dominant, thus invertible, such that (e1(n), ..., ep(n)) is a basis.
Note that we can not consider directly an orthogonal basis since the set of all orthogonal basis has
measure zero.

p−1(Q) > 0 and so λ⊗p

p−1(Bn

We now express all the vectors and inner products in this new almost orthogonal basis as expressed in
the following lemma:
Lemma 10.4. Let n > p − 1 and a basis (e1(n), ..., ep(n)) as deﬁned in Lemma 10.3. Then all
x ∈ Rp can be written as:

x =

p
(cid:88)

i=1

[(cid:104)x, ei(n)(cid:105) + R(x, ei(n))]ei(n)

where |R(x, ei(n))| = o( 1

n ). Moreover for all (x, y) ∈ Rp × Rp:

(cid:104)x, y(cid:105) =

p
(cid:88)

(cid:104)x, ei(n)(cid:105)(cid:104)y, ei(n)(cid:105) + ˜R(x, y)

i=1

where | ˜R(x, y)| = o( 1

n ).

(33)

(34)

Proof. In the following xi denotes the i-th coordinate of a vector x in the standard basis, i.e. a
vector writes x = (x1, . . . , xp). For x ∈ Rp, we can write in the new basis x = (cid:80)p
i=1[(cid:104)x, ei(n)(cid:105) +
n ). Indeed,
R(x, ei(n))]ei with R(x, ei(n))

def
= xi − (cid:104)x, ei(n)(cid:105). We have also |R(x, ei(n))| = o( 1

x =

p
(cid:88)

i=1

xiei =⇒ ∀j, (cid:104)x, ej(cid:105) =

p
(cid:88)

i=1

xi(cid:104)ei, ej(cid:105) =⇒ xj − (cid:104)x, ej(cid:105) =

(cid:88)

i(cid:54)=j

xi(cid:104)ei, ej(cid:105)

=⇒ |R(x, ej(n))| = |

(cid:88)

i(cid:54)=j

xi(cid:104)ei, ej(cid:105)| =⇒ |R(x, ej(n))| ≤

1
n

(cid:88)

i(cid:54)=j

|xi|

Also in the same way for x, y ∈ Rp × Rp we can rewrite their inner product:

(cid:104)x, y(cid:105) =

p
(cid:88)

i=1

(cid:104)x, ei(n)(cid:105)(cid:104)y, ei(n)(cid:105) + ˜R(x, y)

(35)

with:

˜R(x, y)

def
= (cid:104)x, y(cid:105) −

p
(cid:88)

i=1

(cid:104)x, ei(n)(cid:105)(cid:104)y, ei(n)(cid:105)

(cid:88)

(cid:104)x, ei(n)(cid:105)(cid:104)y, ei(n)(cid:105)(cid:104)ej(n), ei(n)(cid:105) +

(cid:88)

(cid:104)x, ei(n)(cid:105)R(y, ej(n))(cid:104)ej(n), ei(n)(cid:105)

(cid:104)y, ej(n)(cid:105)R(x, ei(n))(cid:104)ej(n), ei(n)(cid:105) +

R(x, ej(n))R(y, ei(n))(cid:104)ej(n), ei(n)(cid:105)

i,j
(cid:88)

=

+

i(cid:54)=j
(cid:88)

i,j

i,j

and with the same calculus than for R we have | ˜R(x, y)| = o( 1

n ).

19

Proposition 10.1. Let µ, ν ∈ P(Rp) × P(Rp) and suppose that Qθ holds for almost all θ ∈ Sp−1
and that ν has compact support. There exists a sequence (fn)n∈N from supp(µ) to supp(ν) uniformly
bounded which satisﬁes:

∀n ∈ N, ∀x, x(cid:48) ∈ supp(µ)2, (cid:12)

(cid:12)(cid:107)fn(x) − fn(x(cid:48))(cid:107)1 − (cid:107)x − x(cid:48)(cid:107)1

(cid:12)
(cid:12) = o(

1
n

)

∀n ∈ N, ∀s ∈ Rp, |Ffn#µ(s) − Fν(s)| = o(

1
n

)

(36)

(37)

Proof. In the following xi denotes the i-th coordinate of a vector x in the standard basis, i.e. a vector
writes x = (x1, . . . , xp). We deﬁne:

∀n > p − 1, ∀x ∈ supp(µ), fn(x) = (Te1(n)((cid:104)x, e1(n)(cid:105)), ..., Tep(n)((cid:104)x, ep(n)(cid:105)))

(38)

where (ek(n))k∈{1,...,p} is the almost orthogonal basis deﬁne in Lemma 10.3, and Tek(n) is deﬁned
from (Qθ) since we have Qek(n) for all k. It is clear from the deﬁnition that fn(x) ∈ supp(ν).
Moreover for x, x(cid:48) ∈ supp(µ):

(cid:107)fn(x) − fn(x(cid:48))(cid:107)1 =

=

p
(cid:88)

k=1
p
(cid:88)

k=1

|Tek(n)((cid:104)x, ek(n)(cid:105)) − Tek(n)((cid:104)x(cid:48), ek(n)(cid:105))|

(∗)
=

p
(cid:88)

k=1

|(cid:104)x, ek(n)(cid:105) − (cid:104)x(cid:48), ek(n)(cid:105)|

|(cid:104)x − x(cid:48), ek(n)(cid:105)|

where in (*) we used that Tek(n) is an isometry since we have Qek(n) and (cid:104)x, ek(n)(cid:105) ∈
supp(Pek(n)#µ) (idem for x(cid:48)). In this way:

(cid:12)
(cid:12)(cid:107)fn(x) − fn(x(cid:48))(cid:107)1 − (cid:107)x − x(cid:48)(cid:107)1

(cid:12) = (cid:12)
(cid:12)
(cid:12)

p
(cid:88)

|(cid:104)x − x(cid:48), ek(n)(cid:105)| − |xk − x(cid:48)

k|(cid:12)
(cid:12) ≤

p
(cid:88)

(cid:12)
(cid:12)|(cid:104)x − x(cid:48), ek(n)(cid:105)| − |xk − x(cid:48)

k|(cid:12)
(cid:12)

k=1
p
(cid:88)

|(cid:104)x − x(cid:48), ek(n)(cid:105) − (xk − x(cid:48)

k)| =

k=1
p
(cid:88)

|R(x − x(cid:48), ek(n))| = o(

∗
≤

where in (*) the second triangular inequality ||x| − |y|| ≤ |x − y|. Hence:

k=1

k=1

1
n

)

1
n
Moreover we have by deﬁnition of the Fourier transform, for s ∈ RP ,

(cid:12)
(cid:12)(cid:107)fn(x) − fn(x(cid:48))(cid:107)1 − (cid:107)x − x(cid:48)(cid:107)1

(cid:12)
(cid:12) = o(

ˆ

ˆ

)

(39)

Ffn#µ(s) =

e−2iπ(cid:104)s,fn(x)(cid:105)dµ(x) =

e−2iπ (cid:80)p

k=1 skTek (n)((cid:104)x,ek(n)(cid:105))dµ(x)

Moreover using (Qθ) we have FTek (n)#(Pek (n)#µ)(t) = FPek (n)#ν(t) for all k ∈ {1, ..., p}, and any
real t ∈ R. This implies
e−2iπt(cid:104)ek(n),y(cid:105)dν(y). So by applying
this results for t = sk we have:

e−2iπt.Tek (n)((cid:104)ek(n),x(cid:105))dµ(x) =

´

´

ˆ

ˆ

e−2iπskTek (n)((cid:104)x,ek(n)(cid:105))dµ(x) =

e−2iπsk(cid:104)ek(n),y(cid:105)dν(y)

Combining both results:

ˆ

Ffn#µ(s) =

e−2iπ (cid:80)p

k=1 sk(cid:104)ek(n),y(cid:105)dν(y)

We can now bound |Ffn#µ(s) − Fν(s)| as:

ˆ

|Ffn#µ(s) − Fν(s)| = |Ffn#µ(s) −

e−2iπ(cid:104)s,y(cid:105)dν(y)|

ˆ

(40)

(41)

∗= |Ffn#µ(s) −

ˆ

e−2iπ[(cid:80)p

k=1(cid:104)s,ek(n)(cid:105)(cid:104)ek(n),y(cid:105)+ ˜R(s,y)]dν(y)|

ˆ

∗∗= |

e−2iπ (cid:80)p

k=1 sk(cid:104)ek(n),y(cid:105)dν(y) −

e−2iπ ˜R(s,y)e−2iπ (cid:80)p

k=1(cid:104)s,ek(n)(cid:105)(cid:104)ek(n),y(cid:105)dν(y)|

20

where in (*) we used the expression in the new base of the inner product (cid:104)s, y(cid:105) seen in Lemma 10.4,
in (**) we used (41). By injecting the expression of sk w.r.t. the new base we have:

ˆ

|Ffn#µ(s) − Fν(s)| ≤ |
ˆ

e−2iπ (cid:80)p

k=1((cid:104)s,ek(n)(cid:105)+R(s,ek(n)))(cid:104)ek(n),y(cid:105)dν(y)

−

e−2iπ ˜R(s,y)e−2iπ (cid:80)p
ˆ

k=1(cid:104)s,ek(n)(cid:105)(cid:104)ek(n),y(cid:105)dν(y)|

= (cid:12)
(cid:12)
ˆ

≤

=

≤

=

ˆ

ˆ

ˆ

ˆ

e−2iπ (cid:80)p

k=1(cid:104)s,ek(n)(cid:105)(cid:104)ek(n),y(cid:105)(e−2iπ (cid:80)p

k=1 R(s,ek(n))(cid:104)ek(n),y(cid:105) − e−2iπ ˜R(s,y))dν(y)(cid:12)
(cid:12)

|e−2iπ (cid:80)p

k=1 R(s,ek(n))(cid:104)ek(n),y(cid:105) − e−2iπ ˜R(s,y)|dν(y)

|e−2iπ ˜R(s,y)(e−2iπ((cid:80)p

k=1 R(s,ek(n))(cid:104)ek(n),y(cid:105)− ˜R(s,y)) − 1)|dν(y)

|e−2iπ((cid:80)p

k=1 R(s,ek(n))(cid:104)ek(n),y(cid:105)− ˜R(s,y)) − 1|dν(y)

|2ie−iπ((cid:80)p

k=1 R(s,ek(n))(cid:104)ek(n),y(cid:105)− ˜R(s,y)) sin(π(

p
(cid:88)

k=1

R(s, ek(n))(cid:104)ek(n), y(cid:105) − ˜R(s, y))|dν(y)

p
(cid:88)

k=1

R(s, ek(n))(cid:104)ek(n), y(cid:105) − ˜R(s, y))|dν(y)

|R(s, ek(n))(cid:104)ek(n), y(cid:105)| + | ˜R(s, y)|)dν(y)

≤

| sin(π(

ˆ

≤ π

(

p
(cid:88)

k=1

∗= o(

1
n

)

(42)

(43)

in (*) the fact that each term is o( 1

n ). In this way:

|Ffn#µ(s) − Fν(s)| = o(

1
n

)

Moreover (fn)n>p−1 is also uniformly bounded. To see that we consider x ∈ supp(µ). We have
that for all k ∈ {1, . . . , p} Tek(n)((cid:104)x, ek(n)(cid:105)) ∈ supp(Pek(n)#ν) by deﬁnition of Tek(n). So there
exists a y0(x, n, k) ∈ supp(ν) such that Tek(n)((cid:104)x, ek(n)(cid:105)) = (cid:104)y0(x, n, k), ek(n)(cid:105). In this way
|Tek(n)((cid:104)x, ek(n)(cid:105))| = |(cid:104)y0(x, n, k), ek(n)(cid:105)| ≤ (cid:107)y0(x, n, k)(cid:107)2(cid:107)ek(n)(cid:107)2 by Cauchy-Swartz.

(cid:113) 1

(cid:113) 1

Moreover (cid:107)ek(n)(cid:107)2 <
Mν we have (cid:107)y0(x, n, k)(cid:107)2 ≤ Mν
So we have for n ∈ N, x ∈ supp(µ),

n ≤

p−1 ≤ 1 and since ν has compact support then there is a constant

(cid:107)fn(x)(cid:107)2

2 =

p
(cid:88)

k=1

|Tek(n)((cid:104)x, ek(n)(cid:105))|2 ≤ pMν

Since on Rp all norms are equivalent this sufﬁces to state the existence of a constant C such that
∀x ∈ Rp, n ∈ N, (cid:107)fn(x)(cid:107)1 ≤ C so that (fn)n∈N is uniformly bounded. Reindexing (fn)n>p−1
gives the desired result.

We can now prove Theorem 10.2.

Proof of Theorem 10.2. We consider the sequence (fn)n∈N deﬁned in Proposition 10.1. We will
show that (fn)n∈N is equicontinuous. Let (cid:15) > 0, using (36) there exists a N ∈ N such that we have
for all x, x(cid:48) ∈ supp(µ):

(cid:107)fn(x) − fn(x(cid:48))(cid:107)1 ≤ (cid:15) + (cid:107)x − x(cid:48)(cid:107)1 for all n ≥ N

(44)

21

Now let δ < (cid:15). Suppose that (cid:107)x − x(cid:48)(cid:107)1 < δ then

(cid:107)fn(x) − fn(x(cid:48))(cid:107)1 < (cid:15) + δ < 2(cid:15) for all n ≥ N

(45)

Without loss of generality we can reindex (fn)n∈N for n large enough (n ≥ N ) so that (fn)n∈N is
equicontinuous with the previous argument.

Since (fn)n∈N is a uniformly bounded and equicontinuous sequence from the support of µ which
is compact to Rp we can apply Arzela-Ascoli theorem which states that (fn)n∈N has a uniformly
f this sequence.
convergent subsequence. We denote by (fφ(n))n this sequence. We have fφ(n) u
→
n→∞

Moreover equation (37) states that for all s ∈ Rp, Ffn#µ(s) →
In this way
n→∞
(Ffn#µ(s))n∈N is a convergent real valued sequence, so every adherence value goes to the same
limit, hence Ffφ(n)#µ(s) →
n→∞

Fν(s).

Fν(s).

Moreover the function f is a measure preserving isometry from supp(µ) to supp(ν). Indeed let (cid:15)1 >
0, s ∈ Rp, there exists from previous statements N0, N1 ∈ N such that for n ≥ N0, |Ffφ(n)#µ(s) −
Fν(s)| < (cid:15)1 and n ≥ N1, |Ffφ(n)#µ(s) − Ff #µ(s)| < (cid:15)1. Let n ≥ max(N0, N1)

|Ff #µ(s) − Fν(s)| ≤ |Ffφ(n)#µ(s) − Fν(s)| + |Ffφ(n)#µ(s) − Ff #µ(s)|

< 2(cid:15)1

As this result holds for any (cid:15)1 > 0 we have Ff #µ(s) = Fν(s) and by injectivity of the Fourrier
transform f #µ = ν such that f is measure preserving.
In the same way for any x, x(cid:48) ∈ supp(µ), (cid:15)2 > 0 and n large enough:

(cid:12)
(cid:12)(cid:107)f (x) − f (x(cid:48))(cid:107)1 − (cid:107)x − x(cid:48)(cid:107)1

(cid:12) ≤ (cid:12)
(cid:12)
+ (cid:12)

(cid:12)
(cid:12)(cid:107)fφ(n)(x) − fφ(n)(x(cid:48))(cid:107)1 − (cid:107)f (x) − f (x(cid:48))(cid:107)1
(cid:12)
(cid:12)
(cid:12)(cid:107)fφ(n)(x) − fφ(n)(x(cid:48))(cid:107)1 − (cid:107)x − x(cid:48)(cid:107)1
(cid:12) < 2(cid:15)2

f and (36). As this result holds true for any (cid:15)2 > 0 we have (cid:107)f (x) − f (x(cid:48))(cid:107) =

using fφ(n) u
→
n→∞

(cid:107)x − x(cid:48)(cid:107) for any x, x(cid:48) ∈ supp(µ) which concludes.

Corollary 10.1. Let µ, ν ∈ P(Rp) × P(Rp) with compact support. If SGW (µ, ν) = 0 then µ and
ν are isomorphic for the distance induced by the (cid:96)1 norm on Rp, i.e. d(x, x(cid:48)) = (cid:80)p
i| for
(x, x(cid:48)) ∈ Rp × Rp. In particular this implies:

i=1 |xi − x(cid:48)

SGW (µ, ν) = 0 =⇒ GW2(d, µ, ν) = 0

(46)

Proof. If SGW (µ, ν) = 0 then using the Gromov-Wasserstein properties it implies that for almost
all θ ∈ Sp−1 the projected measures are isomorphic. Moreover since µ, ν have compact support, it
is bounded and we can directly apply Theorem 10.2 to state the existence of a measure preserving
application f as deﬁned in Theorem 10.2. We consider the coupling π = (Id × f )#µ ∈ Π(µ, ν)
since f #µ = ν. Then we have:
ˆ ˆ

ˆ ˆ

|d(x, x(cid:48)) − d(y, y(cid:48))|2dπ(x, y)dπ(x(cid:48), y(cid:48)) =

|d(x, x(cid:48)) − d(f (x), f (x(cid:48)))|2dµ(x)dµ(x(cid:48))

ˆ ˆ

=

|(cid:107)x − x(cid:48)(cid:107)1 − (cid:107)f (x) − f (x(cid:48))(cid:107)1|2dµ(x)dµ(x(cid:48)) = 0

Since f is an isometry. This directly implies that GW2(d, µ, ν) = 0.

11 Algorithm for SGW

In practice, the computation trick presented in Equation (23) can be used to make the complexity of
the computation in line 7 linear with n.

22

(cid:80)n

(cid:80)n

Sliced Gromov-Wasserstein for discrete measures
i=1 δxi ∈ P(Rp) and ν = 1
1: p < q, µ = 1
n
2: ∀i, xi ← ∆(xi), sample uniformly (θl)l=1,...,L ∈ Sq−1
3: for l = 1, . . . , L do
4:
5:
6: end for
7: return 1
n2L

(cid:0)(cid:104)xi−xk, θl(cid:105)2−(cid:104)yσθl

(i)−yσθl

(k), θl(cid:105)2(cid:1)2

n

L
(cid:80)
l=1

n
(cid:80)
i,k=1

i=1 δyj ∈ P(Rq)

Sort ((cid:104)xi, θl(cid:105))i and ((cid:104)yj, θl(cid:105))j in increasing order
Solve (10) for reals ((cid:104)xi, θl(cid:105))i and ((cid:104)yj, θl(cid:105))j, σθl is the solution (σθl ∈ Anti-Id or Id )

Figure 6: Illustration of SW , RISW on spiral datasets for varying rotations on discrete 2D spiral
datasets. (left) Examples of spiral distributions for source and target with different rotations. (right)
Average value of SW and RISW with L = 20 as a function of rotation angle of the target. Colored
areas correspond to the 20% and 80% percentiles.

12 SW∆ and RISW

Analogously to SGW we can deﬁne for the Sliced-Wasserstein distance SW∆(µ, ν) for µ, ν ∈
P(Rp) × P(Rq) with p (cid:54)= q and its rotational invariant counterpart as:

SW∆(µ, ν) =

SW (Pθ#µ∆, Pθ#ν)dθ

Sq−1

RISW (µ, ν) = min

SW∆(µ, ν)

∆∈Vq(Rp)

(47)

where SW is the Sliced-Wasserstein distance. The complexity for computing SW∆ is O(Ln(p + q +
log(n))) which is exactly the same complexity as SGW∆. With these formulations, we can perform
the same experiment as for RISGW on the spiral dataset. The optimisation on the Stiefel manifold is
performed using Pymanopt as for SGW . Results are reported in Figure 6. As one can see, RISW
is rotational invariant on average whereas SW is not. One can also note that, due to the sampling
process of the spiral dataset, the variance is quite large. This can be explained by the fact that, unlike
SGW , the Sliced-Wasserstein may realign the distributions without taking the rotation into account.

13 Supplementary results for the SGW GAN Section

We give here supplementary results for the SGW GAN experiment in Fig. 7, where we consider
ﬁrst a generator that outputs 2D samples, with a two dimensional target, and then a generator that

23

0π/8π/43π/8π/2Rotation angle (radian)85868788899091ValueValues for increasing rotationRISWSW 
Figure 7: Using SGW in a GAN loss. The three rows depicts three different examples. First row is
2D (Generator) to 2D (Target) , Second 3D to 2D. First column is initialization, second one is at 100
Epochs, third one at 1000. Last column depicts the target distribution.

generates 3D samples form a 2D target distribution. Here again, the results are reported for 1000
epochs.

24

