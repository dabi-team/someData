CFR-RL: TrafÔ¨Åc Engineering with Reinforcement
Learning in SDN

Junjie Zhang, Member, IEEE, Minghao Ye, Zehua Guo, Senior Member, IEEE,
Chen-Yu Yen, and H. Jonathan Chao, Fellow, IEEE

1

0
2
0
2

r
p
A
4
2

]
I

N
.
s
c
[

1
v
6
8
9
1
1
.
4
0
0
2
:
v
i
X
r
a

Abstract‚ÄîTraditional TrafÔ¨Åc Engineering (TE) solutions can
achieve the optimal or near-optimal performance by rerouting
as many Ô¨Çows as possible. However, they do not usually consider
the negative impact, such as packet out of order, when frequently
rerouting Ô¨Çows in the network. To mitigate the impact of
network disturbance, one promising TE solution is forwarding
the majority of trafÔ¨Åc Ô¨Çows using Equal-Cost Multi-Path (ECMP)
and selectively rerouting a few critical Ô¨Çows using Software-
DeÔ¨Åned Networking (SDN) to balance link utilization of the
network. However, critical Ô¨Çow rerouting is not trivial because the
solution space for critical Ô¨Çow selection is enormous. Moreover,
it is impossible to design a heuristic algorithm for this problem
based on Ô¨Åxed and simple rules, since rule-based heuristics are
unable to adapt to the changes of the trafÔ¨Åc matrix and network
dynamics. In this paper, we propose CFR-RL (Critical Flow
Rerouting-Reinforcement Learning), a Reinforcement Learning-
based scheme that learns a policy to select critical Ô¨Çows for each
given trafÔ¨Åc matrix automatically. CFR-RL then reroutes these
selected critical Ô¨Çows to balance link utilization of the network
by formulating and solving a simple Linear Programming (LP)
problem. Extensive evaluations show that CFR-RL achieves near-
optimal performance by rerouting only 10%-21.3% of total
trafÔ¨Åc.

Index Terms‚ÄîReinforcement Learning, Software-DeÔ¨Åned Net-
working, TrafÔ¨Åc Engineering, Load Balancing, Network Distur-
bance Mitigation.

I. INTRODUCTION

The emerging Software-DeÔ¨Åned Networking (SDN) pro-
vides new opportunities to improve network performance [1].
In SDN, the control plane can generate routing policies based
on its global view of the network and deploy these policies
in the network by installing and updating Ô¨Çow entries at the
SDN switches.

TrafÔ¨Åc Engineering (TE) is one of important network fea-
tures for SDN [2]‚Äì[4], and is usually implemented in the con-
trol plane of SDN. The goal of TE is to help Internet Service
Providers (ISPs) optimize network performance and resource
utilization by conÔ¨Åguring the routing across their backbone
networks to control trafÔ¨Åc distribution [5], [6]. Due to dynamic
traditional TE [7]‚Äì[12]
load Ô¨Çuctuation among the nodes,

The work of Z. Guo was supported in part by National Key Research
and Development Program of China under Grant 2018YFB1003700 and
Beijing Institute of Technology Research Fund Program for Young Scholars.
(Corresponding author: Zehua Guo)

J. Zhang is with Fortinet, Inc., Sunnyvale, CA 94086 USA (e-mail:

junjie.zhang@nyu.edu).

M. Ye, C.-Y. Yen and H. J. Chao are with the Department of Electrical
and Computer Engineering, New York University, New York City, NY 11201
USA (e-mail: my1706@nyu.edu; cyy310@nyu.edu; chao@nyu.edu).

Z. Guo is with Beijing Institute of Technology, Beijing 100081, China (e-

mail: guo@bit.edu.cn).

reroutes many Ô¨Çows periodically to balance the load on each
link to minimize network congestion probability, where a Ô¨Çow
is deÔ¨Åned as a source-destination pair. One usually formulates
the Ô¨Çow routing problem with a particular performance metric
as a speciÔ¨Åc objective function for optimization. For a given
trafÔ¨Åc matrix, one often wants to route all the Ô¨Çows in such
a way that the maximum link utilization in the network is
minimized.

Although traditional TE solutions can achieve the optimal
or near-optimal performance by rerouting as many Ô¨Çows as
possible, they do not consider the negative impact, such as
packet out of order, when rerouting the Ô¨Çows in the network.
To reach the optimal performance, TE solutions might reroute
many trafÔ¨Åc Ô¨Çows to just slightly reduce the link utilization
on the most congested link, leading to signiÔ¨Åcant network
disturbance and service disruption. For example, a Ô¨Çow be-
tween two nodes in a backbone network is aggregated of many
micro-Ô¨Çows (e.g., Ô¨Åve tuples-based TCP Ô¨Çows) of different
applications. Changing the path of a Ô¨Çow could temporarily
affect many TCP Ô¨Çows‚Äô normal operation. Packets loss or out-
of-order may cause duplicated ACK transmissions, triggering
the sender to react and reduce its congestion window size
and hence decrease its sending rate, eventually increasing the
Ô¨Çow‚Äôs completion time and degrading the Ô¨Çow‚Äôs Quality of
Service (QoS). In addtion, rerouting all Ô¨Çows in the network
could incur a high burden on the SDN controller to calculate
and deploy new Ô¨Çow paths [4]. Because rerouting Ô¨Çows
to reduce congestion in backbone networks could adversely
affect the quality of users‚Äô experience, network operators have
no desire to deploy these traditional TE solutions in their
networks unless reducing network disturbance is taken into
the consideration in designing the TE solutions.

To mitigate the impact of network disturbance, one promis-
ing TE solution is forwarding majority of trafÔ¨Åc Ô¨Çows using
Equal-Cost Multi-Path (ECMP) and selectively rerouting a
few critical Ô¨Çows using SDN to balance link utilization of
the network, where a critical Ô¨Çow is deÔ¨Åned as a Ô¨Çow with
a dominant impact to network performance (e.g., a Ô¨Çow on
the most congested link) [4], [13]. Existing works show that
critical Ô¨Çows exist in a given trafÔ¨Åc matrix [4]. ECMP reduces
the congestion probability by equally splitting trafÔ¨Åc on equal-
cost paths while critical Ô¨Çow rerouting aims to achieve further
performance improvement with low network disturbance.

The critical Ô¨Çow rerouting problem can be decoupled into
two sub-problems: (1) identifying critical Ô¨Çows and (2) rerout-
ing them to achieve good performance. Although sub-problem
(2) is relatively easy to solve by formulating it as a Linear

 
 
 
 
 
 
Programming (LP) optimization problem, solving sub-problem
(1) is not
trivial because the solution space is huge. For
example, if we want to Ô¨Ånd 10 critical Ô¨Çows among 100
Ô¨Çows, the solution space has C10
‚âà 17 trillion combinations.
100
Considering the fact that trafÔ¨Åc matrix varies in the level of
minutes, an efÔ¨Åcient solution should be able to quickly and
effectively identify the critical Ô¨Çows for each trafÔ¨Åc matrix.
Unfortunately, it is impossible to design a heuristic algorithm
for the above algorithmically-hard problem based on Ô¨Åxed and
simple rules. This is because rule-based heuristics are unable
to adapt to the changes of the trafÔ¨Åc matrix and network
dynamics and thus unable to guarantee their performance when
their design assumptions are violated, as later shown in Section
VI-B.

In this paper, we propose CFR-RL (Critical Flow Rerouting-
Reinforcement Learning), a Reinforcement Learning-based
scheme that performs critical Ô¨Çow selection followed by
rerouting with linear programming. CFR-RL learns a policy
to select critical Ô¨Çows purely through observations, without
any domain-speciÔ¨Åc rule-based heuristic. It starts from scratch
without any prior knowledge, and gradually learns to make
better selections through reinforcement, in the form of reward
signals that reÔ¨Çects network performance for past selections.
By continuing to observe the actual performance of past
selections, CFR-RL would optimize its selection policy for
various trafÔ¨Åc matrices as time goes. Once training is done,
CFR-RL will efÔ¨Åciently and effectively select a small set of
critical Ô¨Çows for each given trafÔ¨Åc matrix, and reroute them
to balance link utilization of the network by formulating and
solving a simple linear programming optimization problem.

The main contributions of this paper are summarized as

follows:

1) We consider the impact of Ô¨Çow rerouting to network
disturbance in our TE design and propose an effective
scheme that not only minimizes the maximum link
utilization but also reroutes only a small number of Ô¨Çows
to reduce network disturbance.

2) We customize a RL approach to learn the critical Ô¨Çow
selection policy, and utilize LP as a reward function
to generate reward signals. This RL+LP combined ap-
proach turns out to be surprisingly powerful.

3) We evaluate and compare CFR-RL with other rule-based
heuristic schemes by conducting extensive experiments
on different topologies with both real and synthesized
trafÔ¨Åc. CFR-RL not only outperforms rule-based heuris-
tic schemes by up to 12.2%, but also reroutes 11.4%-
14.7% less trafÔ¨Åc on average. Overall, CFR-RL is able
to achieve near-optimal performance by rerouting only
10%-21.3% of total trafÔ¨Åc. In addition, the evalution
results show that CFR-RL is able to generalize to unseen
trafÔ¨Åc matrices.

The remainder of this paper is organized as follows. Section
II describes the related works. Section III presents the system
design. Section IV discusses how to train the critical Ô¨Çow
selection policy using a RL-based approach. Section V de-
scribes how to reroute the critical Ô¨Çows. Section VI evaluates
the effectiveness of our scheme. Section VII concludes the

2

paper and discusses future work.

II. RELATED WORKS

A. Traditional TE Solutions

In Multiprotocol Label Switching (MPLS) networks, a rout-
ing problem has been formulated as an optimization problem
where explicit routes are obtained for each source-destination
pair to distribute trafÔ¨Åc Ô¨Çows [7], [8]. Using Open Shortest
Path First (OSPF) and ECMP protocols, [9]‚Äì[11] attempt
to balance link utilization as even as possible by carefully
tuning the link costs to adjust path selection in ECMP. OSPF-
OMP (OMP, Optimized Multipath) [14], a variation of OSPF,
attempts to dynamically determine the optimal allocation of
trafÔ¨Åc among multiple equal-cost paths based on the exchange
of special trafÔ¨Åc-load control messages. Weighted ECMP [12]
extends ECMP to allow weighted trafÔ¨Åc splitting at each
node and achieves signiÔ¨Åcant performance improvement over
ECMP. Two-phase routing optimizes routing performance by
selecting a set of intermediate nodes and tuning the trafÔ¨Åc split
ratios to the nodes [15], [16]. In the Ô¨Årst phase, each source
sends trafÔ¨Åc to the intermediate nodes based on predetermined
split ratios, and in the second phase, the intermediate nodes
then deliver the trafÔ¨Åc to the Ô¨Ånal destinations. This approach
requires IP tunnels, optical-layer circuits, or label switched
paths in each phase.

B. SDN-Based TE Solutions

Thanks to the Ô¨Çexible routing policy from the emerging
SDN, dynamic hybrid routing [4] achieves load balancing for
a wide range of trafÔ¨Åc scenarios by dynamically rebalancing
trafÔ¨Åc to react to trafÔ¨Åc Ô¨Çuctuations with a preconÔ¨Ågured rout-
ing policy. Agarwal et al. [2] consider a network with partially
deployed SDN switches. They improve network utilization
and reduce packet loss by strategically placing the controller
and SDN switches. Guo et al. [3] propose a novel algorithm
named SOTE to minimize the maximum link utilization in an
SDN/OSPF hybrid network.

C. Machine Learning-Based TE Solutions

Machine learning has been used to improve the performance
of backbone networks and data center networks. For backbone
networks, Geyer et al. [17] design an automatic network
protocol using semi-supervised deep learning. Sun et al. [18]
selectively control a set of nodes and use a RL-based policy
to dynamically change the routing decision of Ô¨Çows traversing
the selected nodes. To minimize signaling delay in large SDNs,
Lin et al. [19] employ a distributed three-level control plane
architecture coupled with a RL-based solution named QoS-
aware Adaptive Routing. Xu et al. [20] use RL to optimize
the throughput and delay in TE. AuTO [21] is developed to
optimize routing trafÔ¨Åc in data center networks with a two-
layer RL. One is called the Peripheral System for deploying
hosts and routing small Ô¨Çows, and the other one is called the
Central System for collecting global trafÔ¨Åc information and
routing large Ô¨Çows.

However, all of the above works do not consider mitigating
the impact of network disturbance and service disruption
caused by rerouting.

3

split ratio (i.e., trafÔ¨Åc demand percentage) for each Ô¨Çow on
each link. Given a network with E links, there will be total
E ‚àóK split ratios in the routing solution, where K is the number
of critical Ô¨Çows. Since split ratios are continuous numbers, we
have to adopt the RL methods for continuous action domain
[24], [25]. However, due to the high-dimensional, continuous
action spaces, it has been shown that this type of RL methods
would lead to slow and ineffective learning when the number
of output parameters (i.e., E ‚àó K) is large [20], [26].

IV. LEARNING A CRITICAL FLOW SELECTION POLICY

In this section, we describe how to learn a critical Ô¨Çow

selection policy using a customized RL approach.

A. Reinforcement Learning Formulation

Input / State Space: An agent takes a state st = T Mt as
is a trafÔ¨Åc matrix at time step t that
an input, where T Mt
contains information of trafÔ¨Åc demand of each Ô¨Çow. Typically,
the network topology remains unchanged. Thus, we do not
include the topology information as a part of the input. The
results in Section VI-B show that CFR-RL is able to learn
a good policy œÄ without prior knowledge of the network.
It is worth noting that including additional information like
link states as a part of input might be beneÔ¨Åcial for training
the critical Ô¨Çow selection policy. We will investigate it in our
future work.

Action Space: For each state st , CFR-RL would select K
critical Ô¨Çows. Given that there are total N ‚àó (N ‚àí 1) Ô¨Çows in a
network with N nodes, this RL problem would require a large
action space of size CK
N ‚àó(N ‚àí1). Inspired by [27], we deÔ¨Åne the
action space as {0, 1, ..., (N ‚àó (N ‚àí 1)) ‚àí 1} and allow the
agent to sample K different actions in each time step t (i.e.,
t , a2
a1

t , ..., aK

t ).

Reward: After sampling K different critical Ô¨Çows (i.e.,
fK )
for a given state st , CFR-RL reroutes these critical Ô¨Çows and
obtains the maximum link utilization U by solving the rerout-
ing optimization problem (4a) (described in the following
section). Reward r is deÔ¨Åned as 1/U, which is set to reÔ¨Çect the
network performance after rerouting critical Ô¨Çows to balance
link utilization. The smaller U (i.e., the greater reward r), the
better performance. In other words, CFR-RL adopts LP as a
reward function to produce reward signals r for RL.

B. Training Algorithm

Fig. 2: Policy network architecture.

Fig. 1: An illustrative example of CFR-RL rerouting proce-
dure. Each link capability equal to 1. Best viewed in color.

III. SYSTEM DESIGN

In this section, we describe the design of CFR-RL, a
RL-based scheme that learns a critical Ô¨Çow selection policy
and reroutes the corresponding critical Ô¨Çows to balance link
utilization of the network.

We train CFR-RL to learn a selection policy over a rich va-
riety of historical trafÔ¨Åc matrices, where trafÔ¨Åc matrices can be
measured by SDN switches and collected by an SDN central
controller periodically [22]. CFR-RL represents the selection
policy as a neural network that maps a "raw" observation (e.g.,
a trafÔ¨Åc matrix) to a combination of critical Ô¨Çows. The neural
network provides a scalable and expressive way to incorporate
various trafÔ¨Åc matrices into the selection policy. CFR-RL
trains this neural network based on REINFORCE algorithm
[23] with some customizations, as detailed in Section IV.

Once training is done, CFR-RL applies the critical Ô¨Çow
selection policy to each real time trafÔ¨Åc matrix provided by the
SDN controller periodically, where a small number of critical
Ô¨Çows (e.g., K) are selected. The evaluation results in Section
VI-B1 show that selecting 10% of total Ô¨Çows as critical Ô¨Çows
(roughly 11%-21% of total trafÔ¨Åc) is sufÔ¨Åcient for CFR-RL to
achieve near-optimal performance, while network disturbance
(i.e., the percentage of total rerouted trafÔ¨Åc) is reduced by
at least 78.7% compared to rerouting all Ô¨Çows by traditional
TE. Then the SDN controller reroutes the selected critical
Ô¨Çows by installing and updating corresponding Ô¨Çow entries
at the switches using a Ô¨Çow rerouting optimization method
described in Section V. The remaining Ô¨Çows would continue
to be routed by the default ECMP routing. Note that the Ô¨Çow
entries at the switches for the critical Ô¨Çows selected in the
previous period will time out, and the Ô¨Çows would be routed
by either default ECMP routing or newly installed Ô¨Çow entries
in the current period. Figure 1 shows an illustrative example.
CFR-RL reroutes the Ô¨Çow from S0 to S4 to balance link load
by installing forwarding entries at the corresponding switches
along the SDN path.

There are two reasons we do not want to adopt RL for the
Ô¨Çow rerouting problem. Firstly, since the set of critical Ô¨Çows
is small, LP is an efÔ¨Åcient and optimal method to solve the
rerouting problem. Secondly, a routing solution consists of a

S0						S4:	1S2						S4:	1S0S4S1S2S31111Controller	(Running	CFR-RL)Flow	Entry	(FE)FEFETraffic	DemandTraffic	DemandSDN	pathECMP	path0Collect	traffic	demandsUpdate	flow	entriesECMP	pathCollect	traffic	demands......PolicyPolicy NetworkConvolutionalLayerFully ConnectedLayer02.23.63.206.88.95.30Traffic MatrixThe critical Ô¨Çow selection policy is represented by a neural
network. This policy network takes a state st = T Mt as an
input as described above and outputs a probability distribution
œÄ(at |st ) over all available actions. Figure 2 shows the architec-
ture of the policy network (details in Section VI-A1). Since K
different actions are sampled for each state st and their order
t , ..., aK
t )
does not matter, we deÔ¨Åne a solution atK
as a combination of K sampled actions. For selecting a
solution atK with a given state st , a stochastic policy œÄ(atK |st )
parameterized by Œ∏ can be approximated as follows1:

= (a1

t , a2

œÄ(atK |st ; Œ∏) ‚âà

K
(cid:214)

i=1

œÄ(ai

t |st ; Œ∏).

(1)

The goal of training is to maximize the network performance
over various trafÔ¨Åc matrices,
i.e., maximize the expected
reward E[rt ]. Thus, we optimize E[rt ] by gradient ascend,
using REINFORCE algorithm with a baseline b(st ). The policy
parameter Œ∏ is updated according to the following equation:

Œ∏ ‚Üê Œ∏ + Œ±

(cid:213)

t

‚àáŒ∏ logœÄ(atK |st ; Œ∏)(rt ‚àí b(st )),

(2)

where Œ± is the learning rate for the policy network. A good
baseline b(st ) reduces gradient variance and thus increases
speed of learning. In this paper, we use an average reward
for each state st as the baseline. (rt ‚àí b(st )) indicates how
much better a speciÔ¨Åc solution is compared to the "average
solution" for a given state st according to the policy. Intu-
itively, Eq.(2) can be explained as follows. If (rt ‚àí b(st )) is
positive, œÄ(atK |st ; Œ∏) (i.e., the probability of the solution atK ) is
increased by updating the policy parameters Œ∏ in the direction
‚àáŒ∏ logœÄ(atK |st ; Œ∏) with a step size of Œ±(rt ‚àí b(st )). Otherwise,
the solution probability is decreased. The net effect of Eq. (2)
is to reinforce actions that empirically lead to better rewards.
To ensure that the RL agent explores the action space ade-
quately during training to discover good policies, the entropy
of the policy œÄ is added to Eq. (2). This technique improves
the exploration by discouraging premature convergence to
suboptimal deterministic policies [28]. Then, Eq(2) is modiÔ¨Åed
to the following equation:

Œ∏ ‚Üê Œ∏ + Œ±

(cid:213)

(‚àáŒ∏ logœÄ(atK |st ; Œ∏)(rt ‚àí b(st ))

t

(3)

+Œ≤‚àáŒ∏ H(œÄ(¬∑|st ; Œ∏))),

where H is the entropy of the policy (the probability distribu-
tion over actions). The hyperparameter Œ≤ controls the strength
of the entropy regularization term. Algorithm 1 shows the
pseudo-code for the training algorithm.

V. REROUTING CRITICAL FLOWS

In this section, we describe how to reroute the selected

critical Ô¨Çows to balance link utilization of the network.

4

Algorithm 1 Training Algorithm

Initialize Œ∏, v = {} (keep track the sum of rewards for each
state), n = {} (keep track the visited count of each state)
for each iteration do

‚àÜŒ∏ ‚Üê 0
{st } ‚Üê Sample a batch of states with size B
for t = 1, ..., B do

Sample a solution atK according to policy œÄ(atK |st )
Receive reward rt
if st ‚àà v and st ‚àà n then

b(st ) = v[st ]

n[st ] (average reward for state st )

else

b(st ) = 0, v[st ] = 0, n[st ] = 0

end if
end for
for t = 1, ..., B do

Œ≤‚àáŒ∏ H(œÄ(¬∑|st ; Œ∏)))

v[st ] = v[st ] + rt
n[st ] = n[st ] + 1

‚àÜŒ∏ ‚Üê ‚àÜŒ∏ + Œ±(‚àáŒ∏ logœÄ(atK |st ; Œ∏)(rt ‚àí b(st )) +

end for
Œ∏ ‚Üê Œ∏ + ‚àÜŒ∏

end for

A. Notations
G(V, E)

ci, j
li, j
Ds,d

œÉs,d
i, j

network with nodes V and directed edges E
(|V | = N, |E | = M).
the capacity of link (cid:104)i, j(cid:105) ((cid:104)i, j(cid:105) ‚àà E).
the trafÔ¨Åc load on link (cid:104)i, j(cid:105) ((cid:104)i, j(cid:105) ‚àà E).
the trafÔ¨Åc demand from source s to destination
d (s, d ‚àà V, s (cid:44) d).
the percentage of trafÔ¨Åc demand from source s to
destination d routed on link (cid:104)i, j(cid:105) (s, d ‚àà V, s (cid:44)
d, (cid:104)i, j(cid:105) ‚àà E, (cid:104)s, d(cid:105) ‚àà fK ).

B. Explicit Routing For Critical Flows

By default, trafÔ¨Åc is distributed according to ECMP routing.
fK ) by con-
We reroute the small set of critical Ô¨Çows (i.e.,
ducting explicit routing optimization for these critical Ô¨Çows
(cid:104)s, d(cid:105) ‚àà fK .

The critical Ô¨Çow rerouting problem can be described as the
following. Given a network G(V, E) with the set of trafÔ¨Åc
demands Ds,d for the selected critical Ô¨Çows (‚àÄ(cid:104)s, d(cid:105) ‚àà fK ) and
the background link load { ¬Øli, j } contributed by the remaining
Ô¨Çows using the default ECMP routing, our objective is to ob-
tain the optimal explicit routing ratios {œÉs,d
i, j } for each critical
Ô¨Çow, so that the maximum link utilization U is minimized.

To search all possible under-utilized paths for the selected
critical Ô¨Çows, we formulate the rerouting problem as an
optimization as follows.

minimize U + (cid:15) ¬∑

(cid:213)

(cid:213)

(cid:104)i, j (cid:105) ‚ààE

(cid:104)s,d(cid:105) ‚àà fK

œÉs,d
i, j

(4a)

1To select K distinct actions, we do the action sampling without replace-
ment. The right side of Eq. (1) is the solution probability when sampling with
replacement, we use Eq. (1) to approximate the probability of the solution
atK given a state st for simplicity.

subject to

li, j = (cid:213)

(cid:104)s,d(cid:105) ‚àà fK

œÉs,d
i, j

¬∑ Ds,d + ¬Øli, j

i, j : (cid:104)i, j(cid:105) ‚àà E

(4b)

li, j ‚â§ ci, j ¬∑ U

i, j : (cid:104)i, j(cid:105) ‚àà E

(4c)

(cid:213)

œÉs,d
k,i

‚àí

(cid:213)

k:(cid:104)k,i(cid:105) ‚ààE

k:(cid:104)i,k (cid:105) ‚ààE

=

œÉs,d
i,k

‚àí1 if i = s
if i = d
1
otherwise
0
i ‚àà V, s, d : (cid:104)s, d(cid:105) ‚àà fK

Ô£±Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥
Ô£≥

(4d)

0 ‚â§ œÉs,d

i, j ‚â§ 1

s, d : (cid:104)s, d(cid:105) ‚àà fK, i, j : (cid:104)i, j(cid:105) ‚àà E

(4e)

(cid:15) ¬∑ (cid:205)

(cid:104)i, j (cid:105) ‚ààE

(cid:205)
(cid:104)s,d(cid:105) ‚àà fK

œÉs,d
i, j

in (4a) is needed because otherwise

the optimal solution may include unnecessarily long paths as
long as they avoid the most congested link, where (cid:15) ((cid:15) > 0)
is a sufÔ¨Åciently small constant to ensure that the minimization
of U takes higher priority [29]. (4b) indicates the trafÔ¨Åc load
on link (cid:104)i, j(cid:105) contributed by the trafÔ¨Åc demands routed by the
explicit routing and the trafÔ¨Åc demands routed by the default
ECMP routing. (4c) is the link capacity utilization constraint.
(4d) is the Ô¨Çow conservation constraint for the selected critical
Ô¨Çows.

By solving the above LP problem using LP solvers (such
as Gurobi [30]), we can obtain the optimal explicit routing
solution for selected critical Ô¨Çows {œÉs,d
i, j } (‚àÄ(cid:104)s, d(cid:105) ‚àà fK ). Then,
the SDN controller installs and updates Ô¨Çow entries at the
switches accordingly.

VI. EVALUATION

In this section, a series of simulation experiments are
conducted using real-world network topologies to evaluate
the performance of CFR-RL and show its effectiveness by
comparing it with other rule-based heuristic schemes.

A. Evaluation Setup

layer is a convolutional

1) Implementation: The policy neural network consists of
layer with
three layers. The Ô¨Årst
128 Ô¨Ålters. The corresponding kernel size is 3 √ó 3 and the
stride is set
to 1. The second layer is a fully connected
layer with 128 neurons. The activation function used for the
Ô¨Årst two layers is Leaky ReLU [31]. The Ô¨Ånal layer is a
fully connected linear layer (without activation function) with
N ‚àó (N ‚àí 1) neurons corresponding to all possible critical
Ô¨Çows. The softmax function is applied upon the output of Ô¨Ånal
layer to generate the probabilities for all available actions. The
learning rate Œ± is initially conÔ¨Ågured to 0.001 and decays every
500 iterations with a base of 0.96 until it reaches the minimum
value 0.0001. Additionally, the entropy factor Œ≤ is conÔ¨Ågured
to be 0.1. We found that the set of above hyperparameters
is a good trade-off between performance and computational
complexity of the model (details in Section VI-B5). Thus,
we Ô¨Åxed them throughout our experiments. The results in the
following experiments show CFR-RL works well on different
network topologies with a single set of Ô¨Åxed hyperparameters.
This architecture is implemented using TensorFlow [32].

5

TABLE I: ISP networks used in evaluation

Topology
Abilene
EBONE (Europe)
Sprintlink (US)
Tiscali (Europe)

Nodes
12
23
44
49

Directed Links
30
74
166
172

Pairs
132
506
1892
2352

2) Dataset:

In our evaluation, we use four real-world
network topologies including Abilene network and 3 ISP
networks collected by ROCKETFUEL [33]. The number of
nodes and directed links of the networks are listed in Table
I. For the Abilene network,
the measured trafÔ¨Åc matrices
and network topology information (such as link connectivity,
weights, and capacities) are available in [34]. Since Abilene
trafÔ¨Åc matrices are measured every 5 minutes, there are a total
of 288 trafÔ¨Åc matrices each day. To evaluate the performance
of CFR-RL, we choose a total 2016 trafÔ¨Åc matrices in the
Ô¨Årst week (starting from Mar. 1st 2004) as our dataset. For
ROCKETFUEL topologies, the link costs are given while the
link capacities are not provided. Therefore, we infer the link
capacities as the inverse of link costs, which is based on the
default link cost setting in Cisco routers. In other words, the
link costs are inversely proportional to the link capacities.
This approach is commonly adopted in literature [4], [13],
[15]. Besides, since trafÔ¨Åc matrices are also unavailable for
the ISP networks from ROCKETFUEL, we use a trafÔ¨Åc matrix
generation tool [35] to generate 50 synthetic exponential trafÔ¨Åc
matrices and 50 synthetic uniform trafÔ¨Åc matrices for each
network. Unless otherwise noted, we use a random sample of
70% of our dataset as a training set for CFR-RL, and use the
remaining 30% as a test set for testing all schemes.

3) Parallel Training: To speed up training, we spawn
multiple actor agents in parallel, as suggested by [28]. CFR-RL
uses 20 actor agents by default. Each actor agent is conÔ¨Ågured
to experience a different subset of the training set. Then, these
agents continually forward their (state, action, advantage (i.e,
rt ‚àí b(st ))) tuples to a central learner agent, which aggregates
them to train the policy neural network. The central learner
agent performs a gradient update using Eq(3) according to
the received tuples, then sends back the updated parameters
of the policy network to the actor agents. The whole process
can happen asynchronously among all agents. We use 21 CPU
cores to train CFR-RL (i.e., one core (2.6GHz) for each agent).
(1) Load Balancing Performance Ratio: To
demonstrate the load balancing performance of the proposed
CFR-RL scheme, a load balancing performance ratio is applied
and deÔ¨Åned as follows:

4) Metrics:

PRU = Uoptimal
UCFR-RL

,

(5)

where Uoptimal is the maximum link utilization achieved by
an optimal explicit routing for all Ô¨Çows2. PRU = 1 means
that the proposed CFR-RL achieves load balancing as good
as the optimal routing. A lower ratio indicates that the load

2The corresponding LP formulation is similar to (4a), except that the
i, j } for all Ô¨Çows.

objective becomes obtaining the optimal explicit ratios {œÉ s, d
Note that the background link load { ¬Øli, j } would be 0 for this problem.

balancing performance of CFR-RL is farther away from that
of the optimal routing.
(2) End-to-end Delay Performance Ratio: To model and mea-
sure end-to-end delay in the network, we deÔ¨Åne the overall
end-to-end delay in the network as ‚Ñ¶ = (cid:205)
) as

(

li . j
ci, j ‚àíli, j

(cid:104)i, j (cid:105) ‚ààE

described in [12]. Then, an end-to-end delay performance ratio
is deÔ¨Åned as follows:

PR‚Ñ¶ =

‚Ñ¶optimal
‚Ñ¶CFR-RL

,

(6)

where ‚Ñ¶optimal is the minimum end-to-end delay achieved by
an optimal explicit routing for all Ô¨Çows with an objective3
to minimize the end-to-end delay ‚Ñ¶. Note that the rerouting
solution for selected critical Ô¨Çows is still obtained by solving
(4a). The higher PR‚Ñ¶, the better end-to-end delay performance
achieved by CFR-RL. PR‚Ñ¶ = 1 means that the proposed CFR-
RL achieves the minimum end-to-end delay as the optimal
routing.
(3) Rerouting Disturbance: To measure the disturbance caused
by rerouting, we deÔ¨Åne rerouting disturbance as the percentage
of total rerouted trafÔ¨Åc4 for a given trafÔ¨Åc matrix, i.e.,

RD =

Ds,d

(cid:205)
(cid:104)s,d(cid:105) ‚àà fK
(cid:205)
s,d ‚ààV,s(cid:44)d

,

Ds,d

(7)

Ds,d is the total trafÔ¨Åc of selected critical Ô¨Çows

where (cid:205)

(cid:104)s,d(cid:105) ‚àà fK

that need to be rerouted and

Ds,d is the total trafÔ¨Åc

(cid:205)
s,d ‚ààV,s(cid:44)d

of all Ô¨Çows. The smaller RD, the less disturbance caused by
rerouting.

5) Rule-based Heuristics: For comparison, we also evaluate

two rule-based heuristics as the following:

1) Top-K: selects the K largest Ô¨Çows from a given trafÔ¨Åc
matrix in terms of demand volume. This approach is
based on the assumption that Ô¨Çows with larger trafÔ¨Åc
volumes would have a dominant
to network
performance.

impact

2) Top-K Critical: similar to Top-K approach, but selects
the K largest Ô¨Çows from the most congested links. This
approach is based on the assumption that Ô¨Çows travers-
ing the most congested links would have a dominant
impact to network performance.

B. Evaluation

1) Critical Flows Number: We conduct a series of experi-
ments with different number of critical Ô¨Çows selected, and Ô¨Åx
other parameters throughout the experiments.

Figure 3 shows the average load balancing performance
ratio achieved by CFR-RL with increasing number of critical
Ô¨Çows K. The initial value with K = 0 represents the default
ECMP routing. The results indicate that there is a considerable

3The objective of this LP problem is to obtain the optimal explicit routing

ratios {œÉ s, d

i, j } for all Ô¨Çows, such that ‚Ñ¶ is minimized.

4Although partial of trafÔ¨Åc Ô¨Çows might still be routed along the original
ECMP paths, updating routing at the switches might cause packets drop or
out-of-order. Thus, we still consider this amount of trafÔ¨Åc as rerouting trafÔ¨Åc.

6

Fig. 3: Average load balancing performance ratio of CFR-
RL with increasing number of critical Ô¨Çows K on the four
networks.

Fig. 4: Comparison of average load balancing performance
ratio where error bars span ¬± one standard deviation from the
average on the entire test set of the four networks.

room for further improvement when Ô¨Çows are routed by
ECMP. The sharp increases in the average load balancing
performance ratio for all four networks shown in Fig. 3
indicates that CFR-RL is able to achieve near-optimal load
balancing performance by rerouting only 10% Ô¨Çows. As a
result, network disturbance would be much reduced compared
to rerouting all Ô¨Çows as traditional TE. For the subsequent
experiments, we set K = 10% ‚àó N ‚àó (N ‚àí 1) for each network.

Fig. 5: Comparison of load balancing performance in the four
networks on each test trafÔ¨Åc matrix.

    N  N       N  N    1 X P E H U  R I  & U L W L F D O  ) O R Z V K                $ Y H U D J H  3 H U I R U P D Q F H  5 D W L R $ E L O H Q H  1 H W Z R U N    N  N       N  N    1 X P E H U  R I  & U L W L F D O  ) O R Z V K                   $ Y H U D J H  3 H U I R U P D Q F H  5 D W L R ( % 2 1 (  1 H W Z R U N    N  N       N  N    1 X P E H U  R I  & U L W L F D O  ) O R Z V K                   $ Y H U D J H  3 H U I R U P D Q F H  5 D W L R 6 S U L Q W O L Q N  1 H W Z R U N    N  N       N  N    1 X P E H U  R I  & U L W L F D O  ) O R Z V K                $ Y H U D J H  3 H U I R U P D Q F H  5 D W L R 7 L V F D O L  1 H W Z R U N $ E L O H Q H  1 H W Z R U N ( % 2 1 (  1 H W Z R U N 6 S U L Q W O L Q N  1 H W Z R U N 7 L V F D O L  1 H W Z R U N                   $ Y H U D J H PRU & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                    7 U D I I L F  0 D W U L [  , Q G H [                           PRU $ E L O H Q H  1 H W Z R U N & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3             7 U D I I L F  0 D W U L [  , Q G H [                              PRU ( % 2 1 (  1 H W Z R U N ( [ S R Q H Q W L D O  7 0              8 Q L I R U P  7 0   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3             7 U D I I L F  0 D W U L [  , Q G H [                           PRU 6 S U L Q W O L Q N  1 H W Z R U N ( [ S R Q H Q W L D O  7 0              8 Q L I R U P  7 0   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3             7 U D I I L F  0 D W U L [  , Q G H [                        PRU 7 L V F D O L  1 H W Z R U N ( [ S R Q H Q W L D O  7 0              8 Q L I R U P  7 0   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3TABLE II: Comparison of average rerouting disturbance

7

Topology
Abilene
EBONE (Exponential / Uniform)
Sprintlink (Exponential / Uniform)
Tiscali (Exponential / Uniform)

CFR-RL
21.3%

Top-K Critical
32.7%
11.2% / 10.0% 25.9% / 11.5% 32.9% / 11.7%
11.3% / 10.1% 23.6% / 13.8% 33.2% / 14.6%
11.2% / 10.0% 24.5% / 12.0% 32.7% / 12.2%

Top-K
42.9%

Fig. 6: Comparison of average end-to-end delay performance
ratio where error bars span ¬± one standard deviation from the
average on the entire test set of the four networks.

Fig. 8: Comparison of load balancing performance ratio in
CDF with the trafÔ¨Åc matrices from Tuesday, Wednesday,
Friday and Saturday in week 2.

Fig. 7: Comparison of end-to-end delay performance in the
four networks on each test trafÔ¨Åc matrix.

2) Performance Comparison: For comparison, we also
calculate the performance ratios and rerouting disturbances
for Top-K, Top-K critical, and ECMP according to Eqs.
(5), (6) and (7). Figure 4 shows the average load balancing
performance ratio that each scheme achieves on the entire test
set of the four networks. Figure 5 shows the load balancing
performance ratio on each individual trafÔ¨Åc matrix for the
four networks. Note that the Ô¨Årst 15 trafÔ¨Åc matrices in Figs.
5(b)-5(d) are generated by an exponential model and the
remaining 15 trafÔ¨Åc matrices are generated by an uniform
model. CFR-RL performs signiÔ¨Åcantly well in all networks.
For example, for the Abilene network, CFR-RL improves load
balancing performance by about 32.8% compared to ECMP,
and by roughly 7.4% compared to Top-K critical. For the
EBONE network, CFR-RL outperforms Top-K critical with
an average 12.2% load balancing performance improvement.
For Sprintlink and Tiscali networks, CFR-RL performs slightly
better than Top-K critical by 1.3% and 3.5% on average,
respectively. Moreover, Figure 6 shows the average end-to-
end delay performance ratio that each scheme achieves on
the entire test set of the four networks. Figure 7 shows the

Fig. 9: Comparison of end-to-end delay performance ratio
in CDF with the trafÔ¨Åc matrices from Tuesday, Wednesday,
Friday and Saturday in week 2.

end-to-end delay performance ratio on each test trafÔ¨Åc matrix
for the four networks. It is worth noting that the rerouting
solution for selected critical Ô¨Çows is still obtained by solving
(4a) (i.e., minimize maximum link utilization), though the end-
to-end delay performance is evaluated5 for each scheme. By
effectively selecting and rerouting critical Ô¨Çows to balance
link utilization of the network, CFR-RL outperforms heuristic
schemes and ECMP in terms of end-to-end delay in all
networks except the EBONE network. In the EBONE network,
heuristic schemes performs better with the exponential trafÔ¨Åc

5For the Abilene network, the real trafÔ¨Åc demands in the measured trafÔ¨Åc
matrices collected in [34] are relatively small, and thus the corresponding
end-to-end delay would be very small. To effectively compare end-to-end
delay performance of each scheme, we multiply each demand D s, d in a real
0.9
, where U ECMP
trafÔ¨Åc matrix T Mt by
is the maximum link utilization
U ECMP
t
achieved by ECMP routing on the trafÔ¨Åc matrix T Mt .

t

 $ E L O H Q H  1 H W Z R U N ( % 2 1 (  1 H W Z R U N 6 S U L Q W O L Q N  1 H W Z R U N 7 L V F D O L  1 H W Z R U N                   $ Y H U D J H PR‚Ñ¶ & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                    7 U D I I L F  0 D W U L [  , Q G H [                                 PR‚Ñ¶ $ E L O H Q H  1 H W Z R U N & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3             7 U D I I L F  0 D W U L [  , Q G H [                                 PR‚Ñ¶ ( % 2 1 (  1 H W Z R U N ( [ S R Q H Q W L D O  7 0              8 Q L I R U P  7 0   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3             7 U D I I L F  0 D W U L [  , Q G H [                           PR‚Ñ¶ 6 S U L Q W O L Q N  1 H W Z R U N ( [ S R Q H Q W L D O  7 0              8 Q L I R U P  7 0   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3             7 U D I I L F  0 D W U L [  , Q G H [                        PR‚Ñ¶ 7 L V F D O L  1 H W Z R U N ( [ S R Q H Q W L D O  7 0              8 Q L I R U P  7 0   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                  PRU                   & ' ) ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                  PRU                   & ' ) ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                  PRU                   & ' ) ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                  PRU                   & ' ) ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                           PR‚Ñ¶                   & ' ) ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                           PR‚Ñ¶                   & ' ) ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                           PR‚Ñ¶                   & ' ) ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                           PR‚Ñ¶                   & ' ) ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 38

of trafÔ¨Åc for uniform trafÔ¨Åc matrices. However, CFR-RL is
still able to perform slightly better than the two rule-based
heuristics. Overall, the above results indicate that CFR-RL is
able to achieve near-optimal load balancing performance and
greatly reduce end-to-end delay and network disturbance by
smartly selecting a small number of critical Ô¨Çows for each
given trafÔ¨Åc matrix and effectively rerouting the corresponding
small amount of trafÔ¨Åc.

As shown in Figs. 5(b)-5(d), Top-K critical performs well
with the exponential trafÔ¨Åc model. However, its performance is
degraded with the uniform trafÔ¨Åc model. One possible reason
for the performance degradation of Top-K critical is that all
links in the network are relatively saturated under the uniform
trafÔ¨Åc model. Alternative underutilized paths are not available
for the critical Ô¨Çows selected by Top-K critical. In other words,
there is no much room for rerouting performance improvement
by only considering the elephant Ô¨Çows traversing the most con-
gested links. Thus, Ô¨Åxed-rule heuristics are unable to guarantee
their performance, showing that their design assumptions are
invalid. In contrast, CFR-RL performs consistently well under
various trafÔ¨Åc models.

3) Generalization: In this series of experiments, we trained
CFR-RL on the trafÔ¨Åc matrices from the Ô¨Årst week (starting
from Mar. 1st 2004) and evaluate it for each day of the
following week (starting from Mar. 8th 2004) for the Abilene
network. We only present the results for day 2, day 3, day 5
and day 6, since the results for other days are similar. Figures
8 and 9 show the full CDFs of two types of performance ratio
for these 4 days. Figures 10 and 11 show the load balancing
and end-to-end delay performance ratios on each trafÔ¨Åc matrix
of these 4 days, respectively. The results show that CFR-RL
still achieves above 95% optimal load balancing performance
and average 88.13% end-to-end delay performance, and thus
outperforms other schemes on almost all trafÔ¨Åc matrices. The
load balancing performance of CFR-RL degrades on several
outlier trafÔ¨Åc matrices in day 2. There are two possible reasons
for the degradation: (1) The trafÔ¨Åc patterns of these trafÔ¨Åc
matrices are different from what CFR-RL learned from the
previous week. (2) Selecting K = 10% ‚àó N ‚àó (N ‚àí 1) is not
enough for CFR-RL to achieve near-optimal performance on
these outlier trafÔ¨Åc matrices. However, CFR-RL still performs
better than other schemes. Overall, the results indicate that real
trafÔ¨Åc patterns are relatively stable and CFR-RL generalizes
well to unseen trafÔ¨Åc matrices for which it was not explicitly
trained.

it

4) Training and Inference Time: Training a policy for the
Abilene network took approximately 10,000 iterations, and the
time consumed for each iteration is approximately 1 second.
As a result, the total training time for Abilene network is
approximately 3 hours. Since the EBONE network is relatively
larger,
took approximately 60,000 iterations to train a
policy. Then, the total training time for EBONE network is
approximately 16 hours. For larger networks like Sprintlink
and Tiscali, the solution space is even larger. Thus, more
iterations (e.g., approximately 90,000 and 100,000 iterations)
should be taken to train a good policy, and each iteration takes
approximately 2 seconds. Note that this cost is incurred ofÔ¨Çine
and can be performed infrequently depending on environment

Fig. 10: Comparison of load balancing performance ratio with
the trafÔ¨Åc matrices from Tuesday, Wednesday, Friday and
Saturday in week 2.

Fig. 11: Comparison of end-to-end delay performance ratio
with the trafÔ¨Åc matrices from Tuesday, Wednesday, Friday and
Saturday in week 2.

model. It is possible that rerouting the elephant Ô¨Çows selected
by heuristic schemes further balances load on non-congested
links and results in achieving smaller end-to-end delay. In
addition, Tab. II shows the average rerouting disturbance,
i.e., the average percentage of total trafÔ¨Åc rerouted by each
scheme (except ECMP) for the four networks. CFR-RL greatly
reduces network disturbance by rerouting at most 21.3%,
11.2%, 11.3%, and 11.2% of total trafÔ¨Åc on average for the
four networks, respectively. In contrast, Top-K critical reroutes
11.4% more trafÔ¨Åc for the Abilene network and 14.7%, 12.3%,
and 13.3% more trafÔ¨Åc for the EBONE, Sprintlink, and Tiscali
networks (for exponential trafÔ¨Åc matrices). Top-K performs
even worse by rerouting more than 42% of total trafÔ¨Åc on
average for the Abilene network and 32%, 33%, and 32%
of total trafÔ¨Åc on average for the other three networks (for
exponential
there
are no elephant Ô¨Çows in uniform trafÔ¨Åc matrices shown in
Fig. 5(b)-5(d). Thus, all three schemes reroute similar amount

is worth noting that

trafÔ¨Åc matrices). It

                7 U D I I L F  0 D W U L [  L Q G H [                        PRU ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                7 U D I I L F  0 D W U L [  L Q G H [                        PRU ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                7 U D I I L F  0 D W U L [  L Q G H [                        PRU ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                7 U D I I L F  0 D W U L [  L Q G H [                        PRU ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                7 U D I I L F  0 D W U L [  L Q G H [                                 PR‚Ñ¶ ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                7 U D I I L F  0 D W U L [  L Q G H [                                 PR‚Ñ¶ ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                7 U D I I L F  0 D W U L [  L Q G H [                                 PR‚Ñ¶ ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                7 U D I I L F  0 D W U L [  L Q G H [                                 PR‚Ñ¶ ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3TABLE III: Comparison of average load balancing perfor-
mance ratio with different sets of hyperparameters

Œ± = 0.01 (with decay)
Œ± = 0.001 (with decay)
Œ± = 0.0001*

Ô¨Ålters / neurons = 128, Œ≤ = 0.1
0.761
0.970
0.963

* without decay, since the initial learning rate is equal to the

minimum learning rate.

(a)

Ô¨Ålters / neurons = 64
Ô¨Ålters / neurons = 128
Ô¨Ålters / neurons = 256

Œ± = 0.001 (with decay), Œ≤ = 0.1
0.928
0.970
0.837

(b)

Œ≤ = 0.1
Œ≤ = 0.01

Ô¨Ålters / neurons = 128, Œ± = 0.001 (with decay)
0.970
0.958

(c)

stability. The policy neural network as described in Section
VI-A1 is relatively small. Thus, the inference time for the
Abilene and EBONE networks are less than 1 second, and
they are less than 2 seconds for the Sprintlink and Tiscali
networks.

5) Hyperparameters: Table III shows that how hyperpa-
rameters affect the load balancing performance of CFR-RL
in the Abilene network. For each set of hyperparameters, we
trained a policy for the Abilene network by 10,000 iterations,
and then evaluated the average load balancing performance
ratio over the whole test set. We only presented the results
for the Abilene network, since the results for other network
topologies are similar. In Tab. III(a), the number of Ô¨Ålters in
the convolutional layer and neurons in the fully connected
layer is Ô¨Åxed to 128 and entropy factor Œ≤ is Ô¨Åxed to 0.1.
We compare the performance with different learning rate Œ±.
The results show that training might become unstable if the
initial learning rate is too large (e.g., 0.01), and thus it cannot
converge to a good policy. In contrast, training with a smaller
learning rate is more stable but might require longer training
time to further improve the performance. As a result, we chose
Œ± = 0.001 to encourage exploration in the early stage of
training. We compared the performance with different sizes
of Ô¨Ålters and neurons in Tab. III(b). The results show that
too few Ô¨Ålters/neurons might restrict the representation that
the neural network can learn and thus causes under-Ô¨Åtting.
Meanwhile, too many neurons might cause over-Ô¨Åtting, and
thus the corresponding policy cannot generalize well to the
test set. In addition, more training time is required for a
larger neural network. In Tab. III(c), the results show that a
larger entropy factor encourages exploration and leads to a
better performance. Overall, the set of hyperparameters we
have chosen is a good trade-off between performance and
computational complexity of the model.

VII. CONCLUSION AND FUTURE WORK

With an objective of minimizing the maximum link utiliza-
tion in a network and reducing disturbance to the network
causing service disruption, we proposed CFR-RL, a scheme

9

that learns a critical Ô¨Çow selection policy automatically us-
ing reinforcement learning, without any domain-speciÔ¨Åc rule-
based heuristic. CFR-RL selects critical Ô¨Çows for each given
trafÔ¨Åc matrix and reroutes them to balance link utilization
of the network by solving a simple rerouting optimization
problem. Extensive evaluations show that CFR-RL achieves
near-optimal performance by rerouting only a limited portion
of total trafÔ¨Åc. In addition, CFR-RL generalizes well to trafÔ¨Åc
matrices for which it was not explicitly trained.

Yet,

there are several aspects that may help improving
the solution that we proposed in this contribution. Among
them, we are determining how CFR-RL can be updated and
improved.

Objectives: CFR-RL could be formulated to achieve other
objectives. For example, to minimize overall end-to-end delay
in the network (i.e., ‚Ñ¶ = (cid:205)
)) described in Section
(

li . j
ci, j ‚àíli, j

(cid:104)i, j (cid:105) ‚ààE

VI-A4(2), we can deÔ¨Åne reward r as 1/‚Ñ¶ and reformulate the
rerouting optimization problem (4a) to minimize ‚Ñ¶.

Table II shows an interesting Ô¨Ånding. Although CFR-RL
does not explicitly minimize rerouting trafÔ¨Åc,
it ends up
rerouting much less trafÔ¨Åc (i.e., 10.0%-21.3%) and performs
better than rule-based heuristic schemes by 1.3%-12.2%. This
reveals that CFR-RL is effectively searching the whole set of
candidate Ô¨Çows to Ô¨Ånd the best critical Ô¨Çows for various trafÔ¨Åc
matrices, rather than simply considering the elephant Ô¨Çows on
the most congested links or in the whole network as rule-based
heuristic schemes do. We will consider minimizing rerouting
trafÔ¨Åc as one of our objectives and investigate the trade-off
between maximizing performance and minimizing rerouting
trafÔ¨Åc.

Scalability: Scaling CFR-RL to larger networks is an im-
portant direction of our future work. CFR-RL relies on LP
to produce reward signals r. The LP problem would become
complex as the number of critical Ô¨Çows K and the size of a
network increase. This would slow down the policy training for
larger networks (e.g., the Tiscali network in Section VI-B4),
since the time consumed for each iteration would increase.
Moreover, the solution space would become enormous for
larger networks, and RL has to take more iterations to converge
to a good policy. To further speed up training, we can either
spawn even more actor agents (e.g., 30) in parallel to allow
the system to consume more data at each time step and thus
improve exploration [28], or apply GA3C [36] to ofÔ¨Çoad
the training to a GPU, which is an alternative architecture
of A3C and emphasizes on an efÔ¨Åcient GPU utilization to
increase the number of training data generated and processed
per second. Another possible design to mitigate the scalability
issue is adopting SDN multi-controller architectures. Each
controller takes care of a subset of routers in a large network,
and one CFR-RL agent is running on each SDN controller.
The corresponding problem naturally falls into the realm of
Multi-Agent Reinforcement Learning. We will evaluate if a
multi-SDN controller architecture can help provide additional
improvement in our approach.

Retraining: In this paper, we mainly described the RL-
based critical Ô¨Çow selection policy training process as an

ofÔ¨Çine task. In other words, once training is done, CFR-
RL remains unmodiÔ¨Åed after being deployed in the network.
However, CFR-RL can naturally accommodate future unseen
trafÔ¨Åc matrices by periodically updating the selection policy.
This self-learning technique will enable CFR-RL to further
adapt itself to the dynamic conditions in the network after
being deployed in real networks. CFR-RL can be retrained
by including new trafÔ¨Åc matrices. For example, the outlier
trafÔ¨Åc matrices (e.g., the 235th-240th trafÔ¨Åc matrices in Day
2) presented in Fig. 10 should be included for retraining, while
the generalization results shown in Section VI-B3 suggest
that retraining frequently might not be necessary. Techniques
to determine when to retrain and which new/old trafÔ¨Åc ma-
trix should be included/excluded in/from the training dataset
should be further investigated.

The above examples are some key issues that are left for

future work.

ACKNOWLEDGMENTS

The authors would like to thank the editors and reviewers

for providing many valuable comments and suggestions.

REFERENCES

[1] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, L. Peterson,
J. Rexford, S. Shenker, and J. Turner, ‚ÄúOpenÔ¨Çow: enabling innovation in
campus networks,‚Äù ACM SIGCOMM Computer Communication Review,
vol. 38, no. 2, pp. 69‚Äì74, 2008.

[2] S. Agarwal, M. Kodialam, and T. Lakshman, ‚ÄúTrafÔ¨Åc engineering
in software deÔ¨Åned networks,‚Äù in IEEE International Conference on
Computer Communications‚Äô13.

IEEE, 2013, pp. 2211‚Äì2219.

[3] Y. Guo, Z. Wang, X. Yin, X. Shi, and J. Wu, ‚ÄúTrafÔ¨Åc engineering in
sdn/ospf hybrid network,‚Äù in IEEE International Conference on Network
Protocols‚Äô14.

IEEE, 2014, pp. 563‚Äì568.

[4] J. Zhang, K. Xi, M. Luo, and H. J. Chao, ‚ÄúDynamic hybrid routing:
Achieve load balancing for changing trafÔ¨Åc demands,‚Äù in IEEE Interna-
tional Symposium on Quality of Service‚Äô14.
IEEE, 2014, pp. 105‚Äì110.
[5] J. Zhang, K. Xi, and H. J. Chao, ‚ÄúLoad balancing in ip networks using
generalized destination-based multipath routing,‚Äù IEEE/ACM Transac-
tions on Networking, vol. 23, no. 6, pp. 1959‚Äì1969, 2015.

[6] Z. Guo, W. Chen, Y.-F. Liu, Y. Xu, and Z.-L. Zhang, ‚ÄúJoint switch up-
grade and controller deployment in hybrid software-deÔ¨Åned networks,‚Äù
IEEE Journal on Selected Areas in Communications, vol. 37, no. 5, pp.
1012‚Äì1028, 2019.

[7] Y. Wang and Z. Wang, ‚ÄúExplicit routing algorithms for internet trafÔ¨Åc
engineering,‚Äù in IEEE International Conference on Computer Commu-
nications and Networks‚Äô99.

IEEE, 1999, pp. 582‚Äì588.

[8] E. D. Osborne and A. Simha, TrafÔ¨Åc engineering with MPLS. Cisco

Press, 2002.

[9] B. Fortz and M. Thorup, ‚ÄúOptimizing ospf/is-is weights in a changing
world,‚Äù IEEE Journal on Selected Areas in Communications, vol. 20,
no. 4, pp. 756‚Äì767, 2002.

[10] K. Holmberg and D. Yuan, ‚ÄúOptimization of internet protocol network
design and routing,‚Äù Networks: An International Journal, vol. 43, no. 1,
pp. 39‚Äì53, 2004.

[11] J. Chu and C.-T. Lea, ‚ÄúOptimal link weights for ip-based networks
supporting hose-model vpns,‚Äù IEEE/ACM Transactions on Networking,
vol. 17, no. 3, pp. 778‚Äì788, 2009.

[12] J. Zhang, K. Xi, L. Zhang, and H. J. Chao, ‚ÄúOptimizing network
performance using weighted multipath routing,‚Äù in IEEE International
Conference on Computer Communications and Networks‚Äô12.
IEEE,
2012, pp. 1‚Äì7.

[13] J. Zhang, K. Xi, M. Luo, and H. J. Chao, ‚ÄúLoad balancing for
multiple trafÔ¨Åc matrices using sdn hybrid routing,‚Äù in IEEE International
Conference on High-Performance Switching and Routing‚Äô14, 2014, pp.
44‚Äì49.

[14] C. VILLAMIZAR, ‚ÄúOspf optimized multipath (ospf-omp),‚Äù IETF
[Online]. Available:

Internet-Draft, draft-ietf-ospf-omp-03.txt, 1999.
https://ci.nii.ac.jp/naid/10026755527/en/

10

[15] M. Kodialam, T. Lakshman, J. B. Orlin, and S. Sengupta, ‚ÄúOblivious
routing of highly variable trafÔ¨Åc in service overlays and ip backbones,‚Äù
IEEE/ACM Transactions on Networking, vol. 17, no. 2, pp. 459‚Äì472,
2008.

[16] M. Antic, N. Maksic, P. Knezevic, and A. Smiljanic, ‚ÄúTwo phase
load balanced routing using ospf,‚Äù IEEE Journal on Selected Areas in
Communications, vol. 28, no. 1, pp. 51‚Äì59, 2009.

[17] F. Geyer and G. Carle, ‚ÄúLearning and generating distributed routing
protocols using graph-based deep learning,‚Äù in ACM Workshop on
Big Data Analytics and Machine Learning for Data Communication
Networks‚Äô18. ACM, 2018, pp. 40‚Äì45.

[18] P. Sun, J. Li, Z. Guo, Y. Xu, J. Lan, and Y. Hu, ‚ÄúSinet: Enabling scalable
network routing with deep reinforcement learning on partial nodes,‚Äù in
ACM SIGCOMM‚Äô19 Posters and Demos, 2019, pp. 88‚Äì89.

[19] S.-C. Lin, I. F. Akyildiz, P. Wang, and M. Luo, ‚ÄúQos-aware adaptive
routing in multi-layer hierarchical software deÔ¨Åned networks: A re-
inforcement learning approach,‚Äù in IEEE International Conference on
Services Computing‚Äô16.

IEEE, 2016, pp. 25‚Äì33.

[20] Z. Xu, J. Tang, J. Meng, W. Zhang, Y. Wang, C. H. Liu, and D. Yang,
‚ÄúExperience-driven networking: A deep reinforcement learning based
approach,‚Äù in IEEE International Conference on Computer Communi-
cations‚Äô18.

IEEE, 2018, pp. 1871‚Äì1879.

[21] L. Chen, J. Lingys, K. Chen, and F. Liu, ‚ÄúAuto: scaling deep rein-
forcement learning for datacenter-scale automatic trafÔ¨Åc optimization,‚Äù
in ACM SIGCOMM‚Äô18. ACM, 2018, pp. 191‚Äì205.

[22] H. Xu, Z. Yu, C. Qian, X. Li, and Z. Liu, ‚ÄúMinimizing Ô¨Çow statistics
collection cost of sdn using wildcard requests,‚Äù in IEEE International
Conference on Computer Communications‚Äô17, 2017, pp. 1‚Äì9.

[23] R. J. Williams, ‚ÄúSimple statistical gradient-following algorithms for
connectionist reinforcement learning,‚Äù Machine Learning, vol. 8, no. 3,
pp. 229‚Äì256, May 1992.

[24] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, ‚ÄúContinuous control with deep reinforcement
learning,‚Äù CoRR, vol. abs/1509.02971, 2015.

[25] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel, ‚ÄúTrust
region policy optimization,‚Äù CoRR, vol. http://arxiv.org/abs/1502.05477,
2015. [Online]. Available: http://arxiv.org/abs/1502.05477

[26] A. Valadarsky, M. Schapira, D. Shahaf, and A. Tamar, ‚ÄúLearning
to route,‚Äù in ACM Workshop on Hot Topics in Networks‚Äô17, ser.
HotNets-XVI. New York, NY, USA: ACM, 2017, pp. 185‚Äì191.
[Online]. Available: http://doi.acm.org/10.1145/3152434.3152441
[27] H. Mao, M. Alizadeh, I. Menache, and S. Kandula, ‚ÄúResource man-
agement with deep reinforcement learning,‚Äù in ACM Workshop on Hot
Topics in Networks‚Äô16, New York, NY, USA, pp. 50‚Äì56.

[28] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley,
D. Silver, and K. Kavukcuoglu, ‚ÄúAsynchronous methods for deep
learning,‚Äù CoRR, vol. http://arxiv.org/abs/1602.01783,
reinforcement
2016. [Online]. Available: http://arxiv.org/abs/1602.01783

[29] Yufei Wang, Zheng Wang, and Leah Zhang, ‚ÄúInternet trafÔ¨Åc engineering
without full mesh overlaying,‚Äù in IEEE International Conference on
Computer Communications‚Äô01, vol. 1, April 2001, pp. 565‚Äì571.
[30] L. Gurobi Optimization, ‚ÄúGurobi optimizer reference manual,‚Äù 2019.

[Online]. Available: http://www.gurobi.com

[32] M. Abadi

[31] A. L. Maas, A. Y. Hannun, and A. Y. Ng, ‚ÄúRectiÔ¨Åer nonlinearities
improve neural network acoustic models,‚Äù in ICML workshop on Deep
Learning for Audio, Speech and Language Processing‚Äô13, 2013, pp. 1‚Äì6.
large-scale
machine learning,‚Äù in USENIX Conference on Operating Systems
Design and Implementation‚Äô16, ser. OSDI‚Äô16. Berkeley, CA, USA:
USENIX Association, 2016, pp. 265‚Äì283.
[Online]. Available:
http://dl.acm.org/citation.cfm?id=3026877.3026899

‚ÄúTensorÔ¨Çow: A system for

and et

al.,

[33] N. Spring, R. Mahajan, and D. Wetherall, ‚ÄúMeasuring isp topologies
with rocketfuel,‚Äù in ACM SIGCOMM Computer Communication Review,
vol. 32, no. 4. ACM, 2002, pp. 133‚Äì145.

[34] Yin Zhang‚Äôs Abilene TM. [Online]. Available: http://www.cs.utexas.

edu/~yzhang/research/AbileneTM/

[35] TMgen: TrafÔ¨Åc Matrix Generation Tool. [Online]. Available: https:

//tmgen.readthedocs.io/en/latest/

[36] M. Babaeizadeh,

I. Frosio, S. Tyree, J. Clemons, and J. Kautz,
learning,‚Äù CoRR,
‚ÄúGA3C: gpu-based A3C for deep reinforcement
vol. http://arxiv.org/abs/1611.06256, 2016. [Online]. Available: http:
//arxiv.org/abs/1611.06256

11

H. Jonathan Chao (M‚Äô83-F‚Äô01) received the B.S.
and M.S. degrees in electrical engineering from
National Chiao Tung University, Taiwan, in 1977
and 1980, respectively, and the Ph.D. degree in
electrical engineering from The Ohio State Univer-
sity, Columbus, OH, USA, in 1985. He was the
Head of the Electrical and Computer Engineering
(ECE) Department at New York University (NYU)
from 2004 to 2014. He has been doing research
in the areas of software-deÔ¨Åned networking, net-
work function virtualization, datacenter networks,
high-speed packet processing/switching/routing, network security, quality-of-
service control, network on chip, and machine learning for networking. During
2000-2001, he was the Co-Founder and a CTO of Coree Networks, Tinton
Falls, NJ, USA. From 1985 to 1992, he was a Member of Technical Staff
at Bellcore, Piscataway, NJ, USA, where he was involved in transport and
switching system architecture designs and application-speciÔ¨Åed integrated
circuit implementations, such as the world‚Äôs Ô¨Årst SONET-like framer chip,
ATM layer chip, sequencer chip (the Ô¨Årst chip handling packet scheduling),
and ATM switch chip. He is currently a Professor of ECE at NYU, New
York City, NY, USA. He is also the Director of the High-Speed Networking
Lab. He has co-authored three networking books, Broadband Packet Switching
Technologies-A Practical Guide to ATM Switches and IP Routers (New
York: Wiley, 2001), Quality of Service Control
in High-Speed Networks
(New York: Wiley, 2001), and High-Performance Switches and Routers (New
York: Wiley, 2007). He holds 63 patents and has published more than
260 journal and conference papers. He is a fellow of the IEEE and the
National Academy of Inventors. He was a recipient of the Bellcore Excellence
Award in 1987. He was a co-recipient of the 2001 Best Paper Award from
the IEEE TRANSACTION ON CIRCUITS AND SYSTEMS FOR VIDEO
TECHNOLOGY.

Junjie Zhang received the B.S. degree in com-
puter science from Nanjing University of Posts &
Telecommunications, China, in 2006, the M.S. de-
gree in computer science and the Ph.D. degree in
electrical engineering from New York University,
New York, NY, USA, in 2010 and 2015, respec-
tively.

He has been with Fortinet, Inc., Sunnyvale, CA,
USA, since 2015. He holds two US patents in the
area of computer networking. His research interests
trafÔ¨Åc engineering,
include network optimization,

machine learning, and network security.

Minghao Ye received the Ô¨Årst B.E. degree in micro-
electronic science and engineering from Sun Yat-
sen University, Guangzhou, China, and the second
B.E. degree (Hons.) in electronic engineering from
Hong Kong Polytechnic University, Hong Kong, in
2017, the M.S. degree in electrical engineering from
New York University, New York, NY, USA, in 2019,
where he is currently pursuing the Ph.D. degree
with the Department of Electrical and Computer
Engineering. His research interests include trafÔ¨Åc
engineering, software-deÔ¨Åned networks, mobile edge

computing, and reinforcement learning.

Zehua Guo (M‚Äô19-SM‚Äô20) received a B.S. de-
gree from Northwestern Polytechnical University, an
M.S. degree from Xidian University, and a Ph.D.
degree from Northwestern Polytechnical University.
He is an Associate Professor at Beijing Institute of
Technology. He was a Research Fellow at Depart-
ment of Electrical and Computer Engineering, New
York University Tandon School of Engineering, a
Research Manager at ChinaCache, a Senior Software
Engineer at DidiChuxing, a Post-Doctoral Research
Associate at Department of Computer Science and
Engineering, University of Minnesota Twin Cities, and a Visiting Associate
Professor at Singapore University of Technology and Design. His research
interests include software-deÔ¨Åned networking, network function virtualization,
data center network, cloud computing, content delivery network, network
security, machine learning, and Internet exchange. Dr. Guo is an Associate
Editor for IEEE ACCESS and the EURASIP Journal on Wireless Communi-
cations and Networking (Springer), and an Editor for the KSII Transactions
on Internet and Information Systems. He was the Session Chair for the IEEE
International Conference on Communications 2018 and the Technical Program
Committee Member of Computer Communications (Elsevier). He is a Senior
Member of IEEE.

Chen-Yu Yen received the B.S. degree in electri-
cal engineering from National Taiwan University,
Taipei, Taiwan, in 2014, and the M.S. degree in
electrical engineering from Columbia University in
2018. He is currently pursuing the Ph.D. degree
with the Department of Electrical and Computer
Engineering, New York University, New York, NY,
USA. His research interests include reinforcement
learning, congestion control, and practical machine
learning for networking.

