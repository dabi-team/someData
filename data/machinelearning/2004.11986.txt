CFR-RL: Trafﬁc Engineering with Reinforcement
Learning in SDN

Junjie Zhang, Member, IEEE, Minghao Ye, Zehua Guo, Senior Member, IEEE,
Chen-Yu Yen, and H. Jonathan Chao, Fellow, IEEE

1

0
2
0
2

r
p
A
4
2

]
I

N
.
s
c
[

1
v
6
8
9
1
1
.
4
0
0
2
:
v
i
X
r
a

Abstract—Traditional Trafﬁc Engineering (TE) solutions can
achieve the optimal or near-optimal performance by rerouting
as many ﬂows as possible. However, they do not usually consider
the negative impact, such as packet out of order, when frequently
rerouting ﬂows in the network. To mitigate the impact of
network disturbance, one promising TE solution is forwarding
the majority of trafﬁc ﬂows using Equal-Cost Multi-Path (ECMP)
and selectively rerouting a few critical ﬂows using Software-
Deﬁned Networking (SDN) to balance link utilization of the
network. However, critical ﬂow rerouting is not trivial because the
solution space for critical ﬂow selection is enormous. Moreover,
it is impossible to design a heuristic algorithm for this problem
based on ﬁxed and simple rules, since rule-based heuristics are
unable to adapt to the changes of the trafﬁc matrix and network
dynamics. In this paper, we propose CFR-RL (Critical Flow
Rerouting-Reinforcement Learning), a Reinforcement Learning-
based scheme that learns a policy to select critical ﬂows for each
given trafﬁc matrix automatically. CFR-RL then reroutes these
selected critical ﬂows to balance link utilization of the network
by formulating and solving a simple Linear Programming (LP)
problem. Extensive evaluations show that CFR-RL achieves near-
optimal performance by rerouting only 10%-21.3% of total
trafﬁc.

Index Terms—Reinforcement Learning, Software-Deﬁned Net-
working, Trafﬁc Engineering, Load Balancing, Network Distur-
bance Mitigation.

I. INTRODUCTION

The emerging Software-Deﬁned Networking (SDN) pro-
vides new opportunities to improve network performance [1].
In SDN, the control plane can generate routing policies based
on its global view of the network and deploy these policies
in the network by installing and updating ﬂow entries at the
SDN switches.

Trafﬁc Engineering (TE) is one of important network fea-
tures for SDN [2]–[4], and is usually implemented in the con-
trol plane of SDN. The goal of TE is to help Internet Service
Providers (ISPs) optimize network performance and resource
utilization by conﬁguring the routing across their backbone
networks to control trafﬁc distribution [5], [6]. Due to dynamic
traditional TE [7]–[12]
load ﬂuctuation among the nodes,

The work of Z. Guo was supported in part by National Key Research
and Development Program of China under Grant 2018YFB1003700 and
Beijing Institute of Technology Research Fund Program for Young Scholars.
(Corresponding author: Zehua Guo)

J. Zhang is with Fortinet, Inc., Sunnyvale, CA 94086 USA (e-mail:

junjie.zhang@nyu.edu).

M. Ye, C.-Y. Yen and H. J. Chao are with the Department of Electrical
and Computer Engineering, New York University, New York City, NY 11201
USA (e-mail: my1706@nyu.edu; cyy310@nyu.edu; chao@nyu.edu).

Z. Guo is with Beijing Institute of Technology, Beijing 100081, China (e-

mail: guo@bit.edu.cn).

reroutes many ﬂows periodically to balance the load on each
link to minimize network congestion probability, where a ﬂow
is deﬁned as a source-destination pair. One usually formulates
the ﬂow routing problem with a particular performance metric
as a speciﬁc objective function for optimization. For a given
trafﬁc matrix, one often wants to route all the ﬂows in such
a way that the maximum link utilization in the network is
minimized.

Although traditional TE solutions can achieve the optimal
or near-optimal performance by rerouting as many ﬂows as
possible, they do not consider the negative impact, such as
packet out of order, when rerouting the ﬂows in the network.
To reach the optimal performance, TE solutions might reroute
many trafﬁc ﬂows to just slightly reduce the link utilization
on the most congested link, leading to signiﬁcant network
disturbance and service disruption. For example, a ﬂow be-
tween two nodes in a backbone network is aggregated of many
micro-ﬂows (e.g., ﬁve tuples-based TCP ﬂows) of different
applications. Changing the path of a ﬂow could temporarily
affect many TCP ﬂows’ normal operation. Packets loss or out-
of-order may cause duplicated ACK transmissions, triggering
the sender to react and reduce its congestion window size
and hence decrease its sending rate, eventually increasing the
ﬂow’s completion time and degrading the ﬂow’s Quality of
Service (QoS). In addtion, rerouting all ﬂows in the network
could incur a high burden on the SDN controller to calculate
and deploy new ﬂow paths [4]. Because rerouting ﬂows
to reduce congestion in backbone networks could adversely
affect the quality of users’ experience, network operators have
no desire to deploy these traditional TE solutions in their
networks unless reducing network disturbance is taken into
the consideration in designing the TE solutions.

To mitigate the impact of network disturbance, one promis-
ing TE solution is forwarding majority of trafﬁc ﬂows using
Equal-Cost Multi-Path (ECMP) and selectively rerouting a
few critical ﬂows using SDN to balance link utilization of
the network, where a critical ﬂow is deﬁned as a ﬂow with
a dominant impact to network performance (e.g., a ﬂow on
the most congested link) [4], [13]. Existing works show that
critical ﬂows exist in a given trafﬁc matrix [4]. ECMP reduces
the congestion probability by equally splitting trafﬁc on equal-
cost paths while critical ﬂow rerouting aims to achieve further
performance improvement with low network disturbance.

The critical ﬂow rerouting problem can be decoupled into
two sub-problems: (1) identifying critical ﬂows and (2) rerout-
ing them to achieve good performance. Although sub-problem
(2) is relatively easy to solve by formulating it as a Linear

 
 
 
 
 
 
Programming (LP) optimization problem, solving sub-problem
(1) is not
trivial because the solution space is huge. For
example, if we want to ﬁnd 10 critical ﬂows among 100
ﬂows, the solution space has C10
≈ 17 trillion combinations.
100
Considering the fact that trafﬁc matrix varies in the level of
minutes, an efﬁcient solution should be able to quickly and
effectively identify the critical ﬂows for each trafﬁc matrix.
Unfortunately, it is impossible to design a heuristic algorithm
for the above algorithmically-hard problem based on ﬁxed and
simple rules. This is because rule-based heuristics are unable
to adapt to the changes of the trafﬁc matrix and network
dynamics and thus unable to guarantee their performance when
their design assumptions are violated, as later shown in Section
VI-B.

In this paper, we propose CFR-RL (Critical Flow Rerouting-
Reinforcement Learning), a Reinforcement Learning-based
scheme that performs critical ﬂow selection followed by
rerouting with linear programming. CFR-RL learns a policy
to select critical ﬂows purely through observations, without
any domain-speciﬁc rule-based heuristic. It starts from scratch
without any prior knowledge, and gradually learns to make
better selections through reinforcement, in the form of reward
signals that reﬂects network performance for past selections.
By continuing to observe the actual performance of past
selections, CFR-RL would optimize its selection policy for
various trafﬁc matrices as time goes. Once training is done,
CFR-RL will efﬁciently and effectively select a small set of
critical ﬂows for each given trafﬁc matrix, and reroute them
to balance link utilization of the network by formulating and
solving a simple linear programming optimization problem.

The main contributions of this paper are summarized as

follows:

1) We consider the impact of ﬂow rerouting to network
disturbance in our TE design and propose an effective
scheme that not only minimizes the maximum link
utilization but also reroutes only a small number of ﬂows
to reduce network disturbance.

2) We customize a RL approach to learn the critical ﬂow
selection policy, and utilize LP as a reward function
to generate reward signals. This RL+LP combined ap-
proach turns out to be surprisingly powerful.

3) We evaluate and compare CFR-RL with other rule-based
heuristic schemes by conducting extensive experiments
on different topologies with both real and synthesized
trafﬁc. CFR-RL not only outperforms rule-based heuris-
tic schemes by up to 12.2%, but also reroutes 11.4%-
14.7% less trafﬁc on average. Overall, CFR-RL is able
to achieve near-optimal performance by rerouting only
10%-21.3% of total trafﬁc. In addition, the evalution
results show that CFR-RL is able to generalize to unseen
trafﬁc matrices.

The remainder of this paper is organized as follows. Section
II describes the related works. Section III presents the system
design. Section IV discusses how to train the critical ﬂow
selection policy using a RL-based approach. Section V de-
scribes how to reroute the critical ﬂows. Section VI evaluates
the effectiveness of our scheme. Section VII concludes the

2

paper and discusses future work.

II. RELATED WORKS

A. Traditional TE Solutions

In Multiprotocol Label Switching (MPLS) networks, a rout-
ing problem has been formulated as an optimization problem
where explicit routes are obtained for each source-destination
pair to distribute trafﬁc ﬂows [7], [8]. Using Open Shortest
Path First (OSPF) and ECMP protocols, [9]–[11] attempt
to balance link utilization as even as possible by carefully
tuning the link costs to adjust path selection in ECMP. OSPF-
OMP (OMP, Optimized Multipath) [14], a variation of OSPF,
attempts to dynamically determine the optimal allocation of
trafﬁc among multiple equal-cost paths based on the exchange
of special trafﬁc-load control messages. Weighted ECMP [12]
extends ECMP to allow weighted trafﬁc splitting at each
node and achieves signiﬁcant performance improvement over
ECMP. Two-phase routing optimizes routing performance by
selecting a set of intermediate nodes and tuning the trafﬁc split
ratios to the nodes [15], [16]. In the ﬁrst phase, each source
sends trafﬁc to the intermediate nodes based on predetermined
split ratios, and in the second phase, the intermediate nodes
then deliver the trafﬁc to the ﬁnal destinations. This approach
requires IP tunnels, optical-layer circuits, or label switched
paths in each phase.

B. SDN-Based TE Solutions

Thanks to the ﬂexible routing policy from the emerging
SDN, dynamic hybrid routing [4] achieves load balancing for
a wide range of trafﬁc scenarios by dynamically rebalancing
trafﬁc to react to trafﬁc ﬂuctuations with a preconﬁgured rout-
ing policy. Agarwal et al. [2] consider a network with partially
deployed SDN switches. They improve network utilization
and reduce packet loss by strategically placing the controller
and SDN switches. Guo et al. [3] propose a novel algorithm
named SOTE to minimize the maximum link utilization in an
SDN/OSPF hybrid network.

C. Machine Learning-Based TE Solutions

Machine learning has been used to improve the performance
of backbone networks and data center networks. For backbone
networks, Geyer et al. [17] design an automatic network
protocol using semi-supervised deep learning. Sun et al. [18]
selectively control a set of nodes and use a RL-based policy
to dynamically change the routing decision of ﬂows traversing
the selected nodes. To minimize signaling delay in large SDNs,
Lin et al. [19] employ a distributed three-level control plane
architecture coupled with a RL-based solution named QoS-
aware Adaptive Routing. Xu et al. [20] use RL to optimize
the throughput and delay in TE. AuTO [21] is developed to
optimize routing trafﬁc in data center networks with a two-
layer RL. One is called the Peripheral System for deploying
hosts and routing small ﬂows, and the other one is called the
Central System for collecting global trafﬁc information and
routing large ﬂows.

However, all of the above works do not consider mitigating
the impact of network disturbance and service disruption
caused by rerouting.

3

split ratio (i.e., trafﬁc demand percentage) for each ﬂow on
each link. Given a network with E links, there will be total
E ∗K split ratios in the routing solution, where K is the number
of critical ﬂows. Since split ratios are continuous numbers, we
have to adopt the RL methods for continuous action domain
[24], [25]. However, due to the high-dimensional, continuous
action spaces, it has been shown that this type of RL methods
would lead to slow and ineffective learning when the number
of output parameters (i.e., E ∗ K) is large [20], [26].

IV. LEARNING A CRITICAL FLOW SELECTION POLICY

In this section, we describe how to learn a critical ﬂow

selection policy using a customized RL approach.

A. Reinforcement Learning Formulation

Input / State Space: An agent takes a state st = T Mt as
is a trafﬁc matrix at time step t that
an input, where T Mt
contains information of trafﬁc demand of each ﬂow. Typically,
the network topology remains unchanged. Thus, we do not
include the topology information as a part of the input. The
results in Section VI-B show that CFR-RL is able to learn
a good policy π without prior knowledge of the network.
It is worth noting that including additional information like
link states as a part of input might be beneﬁcial for training
the critical ﬂow selection policy. We will investigate it in our
future work.

Action Space: For each state st , CFR-RL would select K
critical ﬂows. Given that there are total N ∗ (N − 1) ﬂows in a
network with N nodes, this RL problem would require a large
action space of size CK
N ∗(N −1). Inspired by [27], we deﬁne the
action space as {0, 1, ..., (N ∗ (N − 1)) − 1} and allow the
agent to sample K different actions in each time step t (i.e.,
t , a2
a1

t , ..., aK

t ).

Reward: After sampling K different critical ﬂows (i.e.,
fK )
for a given state st , CFR-RL reroutes these critical ﬂows and
obtains the maximum link utilization U by solving the rerout-
ing optimization problem (4a) (described in the following
section). Reward r is deﬁned as 1/U, which is set to reﬂect the
network performance after rerouting critical ﬂows to balance
link utilization. The smaller U (i.e., the greater reward r), the
better performance. In other words, CFR-RL adopts LP as a
reward function to produce reward signals r for RL.

B. Training Algorithm

Fig. 2: Policy network architecture.

Fig. 1: An illustrative example of CFR-RL rerouting proce-
dure. Each link capability equal to 1. Best viewed in color.

III. SYSTEM DESIGN

In this section, we describe the design of CFR-RL, a
RL-based scheme that learns a critical ﬂow selection policy
and reroutes the corresponding critical ﬂows to balance link
utilization of the network.

We train CFR-RL to learn a selection policy over a rich va-
riety of historical trafﬁc matrices, where trafﬁc matrices can be
measured by SDN switches and collected by an SDN central
controller periodically [22]. CFR-RL represents the selection
policy as a neural network that maps a "raw" observation (e.g.,
a trafﬁc matrix) to a combination of critical ﬂows. The neural
network provides a scalable and expressive way to incorporate
various trafﬁc matrices into the selection policy. CFR-RL
trains this neural network based on REINFORCE algorithm
[23] with some customizations, as detailed in Section IV.

Once training is done, CFR-RL applies the critical ﬂow
selection policy to each real time trafﬁc matrix provided by the
SDN controller periodically, where a small number of critical
ﬂows (e.g., K) are selected. The evaluation results in Section
VI-B1 show that selecting 10% of total ﬂows as critical ﬂows
(roughly 11%-21% of total trafﬁc) is sufﬁcient for CFR-RL to
achieve near-optimal performance, while network disturbance
(i.e., the percentage of total rerouted trafﬁc) is reduced by
at least 78.7% compared to rerouting all ﬂows by traditional
TE. Then the SDN controller reroutes the selected critical
ﬂows by installing and updating corresponding ﬂow entries
at the switches using a ﬂow rerouting optimization method
described in Section V. The remaining ﬂows would continue
to be routed by the default ECMP routing. Note that the ﬂow
entries at the switches for the critical ﬂows selected in the
previous period will time out, and the ﬂows would be routed
by either default ECMP routing or newly installed ﬂow entries
in the current period. Figure 1 shows an illustrative example.
CFR-RL reroutes the ﬂow from S0 to S4 to balance link load
by installing forwarding entries at the corresponding switches
along the SDN path.

There are two reasons we do not want to adopt RL for the
ﬂow rerouting problem. Firstly, since the set of critical ﬂows
is small, LP is an efﬁcient and optimal method to solve the
rerouting problem. Secondly, a routing solution consists of a

S0						S4:	1S2						S4:	1S0S4S1S2S31111Controller	(Running	CFR-RL)Flow	Entry	(FE)FEFETraffic	DemandTraffic	DemandSDN	pathECMP	path0Collect	traffic	demandsUpdate	flow	entriesECMP	pathCollect	traffic	demands......PolicyPolicy NetworkConvolutionalLayerFully ConnectedLayer02.23.63.206.88.95.30Traffic MatrixThe critical ﬂow selection policy is represented by a neural
network. This policy network takes a state st = T Mt as an
input as described above and outputs a probability distribution
π(at |st ) over all available actions. Figure 2 shows the architec-
ture of the policy network (details in Section VI-A1). Since K
different actions are sampled for each state st and their order
t , ..., aK
t )
does not matter, we deﬁne a solution atK
as a combination of K sampled actions. For selecting a
solution atK with a given state st , a stochastic policy π(atK |st )
parameterized by θ can be approximated as follows1:

= (a1

t , a2

π(atK |st ; θ) ≈

K
(cid:214)

i=1

π(ai

t |st ; θ).

(1)

The goal of training is to maximize the network performance
over various trafﬁc matrices,
i.e., maximize the expected
reward E[rt ]. Thus, we optimize E[rt ] by gradient ascend,
using REINFORCE algorithm with a baseline b(st ). The policy
parameter θ is updated according to the following equation:

θ ← θ + α

(cid:213)

t

∇θ logπ(atK |st ; θ)(rt − b(st )),

(2)

where α is the learning rate for the policy network. A good
baseline b(st ) reduces gradient variance and thus increases
speed of learning. In this paper, we use an average reward
for each state st as the baseline. (rt − b(st )) indicates how
much better a speciﬁc solution is compared to the "average
solution" for a given state st according to the policy. Intu-
itively, Eq.(2) can be explained as follows. If (rt − b(st )) is
positive, π(atK |st ; θ) (i.e., the probability of the solution atK ) is
increased by updating the policy parameters θ in the direction
∇θ logπ(atK |st ; θ) with a step size of α(rt − b(st )). Otherwise,
the solution probability is decreased. The net effect of Eq. (2)
is to reinforce actions that empirically lead to better rewards.
To ensure that the RL agent explores the action space ade-
quately during training to discover good policies, the entropy
of the policy π is added to Eq. (2). This technique improves
the exploration by discouraging premature convergence to
suboptimal deterministic policies [28]. Then, Eq(2) is modiﬁed
to the following equation:

θ ← θ + α

(cid:213)

(∇θ logπ(atK |st ; θ)(rt − b(st ))

t

(3)

+β∇θ H(π(·|st ; θ))),

where H is the entropy of the policy (the probability distribu-
tion over actions). The hyperparameter β controls the strength
of the entropy regularization term. Algorithm 1 shows the
pseudo-code for the training algorithm.

V. REROUTING CRITICAL FLOWS

In this section, we describe how to reroute the selected

critical ﬂows to balance link utilization of the network.

4

Algorithm 1 Training Algorithm

Initialize θ, v = {} (keep track the sum of rewards for each
state), n = {} (keep track the visited count of each state)
for each iteration do

∆θ ← 0
{st } ← Sample a batch of states with size B
for t = 1, ..., B do

Sample a solution atK according to policy π(atK |st )
Receive reward rt
if st ∈ v and st ∈ n then

b(st ) = v[st ]

n[st ] (average reward for state st )

else

b(st ) = 0, v[st ] = 0, n[st ] = 0

end if
end for
for t = 1, ..., B do

β∇θ H(π(·|st ; θ)))

v[st ] = v[st ] + rt
n[st ] = n[st ] + 1

∆θ ← ∆θ + α(∇θ logπ(atK |st ; θ)(rt − b(st )) +

end for
θ ← θ + ∆θ

end for

A. Notations
G(V, E)

ci, j
li, j
Ds,d

σs,d
i, j

network with nodes V and directed edges E
(|V | = N, |E | = M).
the capacity of link (cid:104)i, j(cid:105) ((cid:104)i, j(cid:105) ∈ E).
the trafﬁc load on link (cid:104)i, j(cid:105) ((cid:104)i, j(cid:105) ∈ E).
the trafﬁc demand from source s to destination
d (s, d ∈ V, s (cid:44) d).
the percentage of trafﬁc demand from source s to
destination d routed on link (cid:104)i, j(cid:105) (s, d ∈ V, s (cid:44)
d, (cid:104)i, j(cid:105) ∈ E, (cid:104)s, d(cid:105) ∈ fK ).

B. Explicit Routing For Critical Flows

By default, trafﬁc is distributed according to ECMP routing.
fK ) by con-
We reroute the small set of critical ﬂows (i.e.,
ducting explicit routing optimization for these critical ﬂows
(cid:104)s, d(cid:105) ∈ fK .

The critical ﬂow rerouting problem can be described as the
following. Given a network G(V, E) with the set of trafﬁc
demands Ds,d for the selected critical ﬂows (∀(cid:104)s, d(cid:105) ∈ fK ) and
the background link load { ¯li, j } contributed by the remaining
ﬂows using the default ECMP routing, our objective is to ob-
tain the optimal explicit routing ratios {σs,d
i, j } for each critical
ﬂow, so that the maximum link utilization U is minimized.

To search all possible under-utilized paths for the selected
critical ﬂows, we formulate the rerouting problem as an
optimization as follows.

minimize U + (cid:15) ·

(cid:213)

(cid:213)

(cid:104)i, j (cid:105) ∈E

(cid:104)s,d(cid:105) ∈ fK

σs,d
i, j

(4a)

1To select K distinct actions, we do the action sampling without replace-
ment. The right side of Eq. (1) is the solution probability when sampling with
replacement, we use Eq. (1) to approximate the probability of the solution
atK given a state st for simplicity.

subject to

li, j = (cid:213)

(cid:104)s,d(cid:105) ∈ fK

σs,d
i, j

· Ds,d + ¯li, j

i, j : (cid:104)i, j(cid:105) ∈ E

(4b)

li, j ≤ ci, j · U

i, j : (cid:104)i, j(cid:105) ∈ E

(4c)

(cid:213)

σs,d
k,i

−

(cid:213)

k:(cid:104)k,i(cid:105) ∈E

k:(cid:104)i,k (cid:105) ∈E

=

σs,d
i,k

−1 if i = s
if i = d
1
otherwise
0
i ∈ V, s, d : (cid:104)s, d(cid:105) ∈ fK





(4d)

0 ≤ σs,d

i, j ≤ 1

s, d : (cid:104)s, d(cid:105) ∈ fK, i, j : (cid:104)i, j(cid:105) ∈ E

(4e)

(cid:15) · (cid:205)

(cid:104)i, j (cid:105) ∈E

(cid:205)
(cid:104)s,d(cid:105) ∈ fK

σs,d
i, j

in (4a) is needed because otherwise

the optimal solution may include unnecessarily long paths as
long as they avoid the most congested link, where (cid:15) ((cid:15) > 0)
is a sufﬁciently small constant to ensure that the minimization
of U takes higher priority [29]. (4b) indicates the trafﬁc load
on link (cid:104)i, j(cid:105) contributed by the trafﬁc demands routed by the
explicit routing and the trafﬁc demands routed by the default
ECMP routing. (4c) is the link capacity utilization constraint.
(4d) is the ﬂow conservation constraint for the selected critical
ﬂows.

By solving the above LP problem using LP solvers (such
as Gurobi [30]), we can obtain the optimal explicit routing
solution for selected critical ﬂows {σs,d
i, j } (∀(cid:104)s, d(cid:105) ∈ fK ). Then,
the SDN controller installs and updates ﬂow entries at the
switches accordingly.

VI. EVALUATION

In this section, a series of simulation experiments are
conducted using real-world network topologies to evaluate
the performance of CFR-RL and show its effectiveness by
comparing it with other rule-based heuristic schemes.

A. Evaluation Setup

layer is a convolutional

1) Implementation: The policy neural network consists of
layer with
three layers. The ﬁrst
128 ﬁlters. The corresponding kernel size is 3 × 3 and the
stride is set
to 1. The second layer is a fully connected
layer with 128 neurons. The activation function used for the
ﬁrst two layers is Leaky ReLU [31]. The ﬁnal layer is a
fully connected linear layer (without activation function) with
N ∗ (N − 1) neurons corresponding to all possible critical
ﬂows. The softmax function is applied upon the output of ﬁnal
layer to generate the probabilities for all available actions. The
learning rate α is initially conﬁgured to 0.001 and decays every
500 iterations with a base of 0.96 until it reaches the minimum
value 0.0001. Additionally, the entropy factor β is conﬁgured
to be 0.1. We found that the set of above hyperparameters
is a good trade-off between performance and computational
complexity of the model (details in Section VI-B5). Thus,
we ﬁxed them throughout our experiments. The results in the
following experiments show CFR-RL works well on different
network topologies with a single set of ﬁxed hyperparameters.
This architecture is implemented using TensorFlow [32].

5

TABLE I: ISP networks used in evaluation

Topology
Abilene
EBONE (Europe)
Sprintlink (US)
Tiscali (Europe)

Nodes
12
23
44
49

Directed Links
30
74
166
172

Pairs
132
506
1892
2352

2) Dataset:

In our evaluation, we use four real-world
network topologies including Abilene network and 3 ISP
networks collected by ROCKETFUEL [33]. The number of
nodes and directed links of the networks are listed in Table
I. For the Abilene network,
the measured trafﬁc matrices
and network topology information (such as link connectivity,
weights, and capacities) are available in [34]. Since Abilene
trafﬁc matrices are measured every 5 minutes, there are a total
of 288 trafﬁc matrices each day. To evaluate the performance
of CFR-RL, we choose a total 2016 trafﬁc matrices in the
ﬁrst week (starting from Mar. 1st 2004) as our dataset. For
ROCKETFUEL topologies, the link costs are given while the
link capacities are not provided. Therefore, we infer the link
capacities as the inverse of link costs, which is based on the
default link cost setting in Cisco routers. In other words, the
link costs are inversely proportional to the link capacities.
This approach is commonly adopted in literature [4], [13],
[15]. Besides, since trafﬁc matrices are also unavailable for
the ISP networks from ROCKETFUEL, we use a trafﬁc matrix
generation tool [35] to generate 50 synthetic exponential trafﬁc
matrices and 50 synthetic uniform trafﬁc matrices for each
network. Unless otherwise noted, we use a random sample of
70% of our dataset as a training set for CFR-RL, and use the
remaining 30% as a test set for testing all schemes.

3) Parallel Training: To speed up training, we spawn
multiple actor agents in parallel, as suggested by [28]. CFR-RL
uses 20 actor agents by default. Each actor agent is conﬁgured
to experience a different subset of the training set. Then, these
agents continually forward their (state, action, advantage (i.e,
rt − b(st ))) tuples to a central learner agent, which aggregates
them to train the policy neural network. The central learner
agent performs a gradient update using Eq(3) according to
the received tuples, then sends back the updated parameters
of the policy network to the actor agents. The whole process
can happen asynchronously among all agents. We use 21 CPU
cores to train CFR-RL (i.e., one core (2.6GHz) for each agent).
(1) Load Balancing Performance Ratio: To
demonstrate the load balancing performance of the proposed
CFR-RL scheme, a load balancing performance ratio is applied
and deﬁned as follows:

4) Metrics:

PRU = Uoptimal
UCFR-RL

,

(5)

where Uoptimal is the maximum link utilization achieved by
an optimal explicit routing for all ﬂows2. PRU = 1 means
that the proposed CFR-RL achieves load balancing as good
as the optimal routing. A lower ratio indicates that the load

2The corresponding LP formulation is similar to (4a), except that the
i, j } for all ﬂows.

objective becomes obtaining the optimal explicit ratios {σ s, d
Note that the background link load { ¯li, j } would be 0 for this problem.

balancing performance of CFR-RL is farther away from that
of the optimal routing.
(2) End-to-end Delay Performance Ratio: To model and mea-
sure end-to-end delay in the network, we deﬁne the overall
end-to-end delay in the network as Ω = (cid:205)
) as

(

li . j
ci, j −li, j

(cid:104)i, j (cid:105) ∈E

described in [12]. Then, an end-to-end delay performance ratio
is deﬁned as follows:

PRΩ =

Ωoptimal
ΩCFR-RL

,

(6)

where Ωoptimal is the minimum end-to-end delay achieved by
an optimal explicit routing for all ﬂows with an objective3
to minimize the end-to-end delay Ω. Note that the rerouting
solution for selected critical ﬂows is still obtained by solving
(4a). The higher PRΩ, the better end-to-end delay performance
achieved by CFR-RL. PRΩ = 1 means that the proposed CFR-
RL achieves the minimum end-to-end delay as the optimal
routing.
(3) Rerouting Disturbance: To measure the disturbance caused
by rerouting, we deﬁne rerouting disturbance as the percentage
of total rerouted trafﬁc4 for a given trafﬁc matrix, i.e.,

RD =

Ds,d

(cid:205)
(cid:104)s,d(cid:105) ∈ fK
(cid:205)
s,d ∈V,s(cid:44)d

,

Ds,d

(7)

Ds,d is the total trafﬁc of selected critical ﬂows

where (cid:205)

(cid:104)s,d(cid:105) ∈ fK

that need to be rerouted and

Ds,d is the total trafﬁc

(cid:205)
s,d ∈V,s(cid:44)d

of all ﬂows. The smaller RD, the less disturbance caused by
rerouting.

5) Rule-based Heuristics: For comparison, we also evaluate

two rule-based heuristics as the following:

1) Top-K: selects the K largest ﬂows from a given trafﬁc
matrix in terms of demand volume. This approach is
based on the assumption that ﬂows with larger trafﬁc
volumes would have a dominant
to network
performance.

impact

2) Top-K Critical: similar to Top-K approach, but selects
the K largest ﬂows from the most congested links. This
approach is based on the assumption that ﬂows travers-
ing the most congested links would have a dominant
impact to network performance.

B. Evaluation

1) Critical Flows Number: We conduct a series of experi-
ments with different number of critical ﬂows selected, and ﬁx
other parameters throughout the experiments.

Figure 3 shows the average load balancing performance
ratio achieved by CFR-RL with increasing number of critical
ﬂows K. The initial value with K = 0 represents the default
ECMP routing. The results indicate that there is a considerable

3The objective of this LP problem is to obtain the optimal explicit routing

ratios {σ s, d

i, j } for all ﬂows, such that Ω is minimized.

4Although partial of trafﬁc ﬂows might still be routed along the original
ECMP paths, updating routing at the switches might cause packets drop or
out-of-order. Thus, we still consider this amount of trafﬁc as rerouting trafﬁc.

6

Fig. 3: Average load balancing performance ratio of CFR-
RL with increasing number of critical ﬂows K on the four
networks.

Fig. 4: Comparison of average load balancing performance
ratio where error bars span ± one standard deviation from the
average on the entire test set of the four networks.

room for further improvement when ﬂows are routed by
ECMP. The sharp increases in the average load balancing
performance ratio for all four networks shown in Fig. 3
indicates that CFR-RL is able to achieve near-optimal load
balancing performance by rerouting only 10% ﬂows. As a
result, network disturbance would be much reduced compared
to rerouting all ﬂows as traditional TE. For the subsequent
experiments, we set K = 10% ∗ N ∗ (N − 1) for each network.

Fig. 5: Comparison of load balancing performance in the four
networks on each test trafﬁc matrix.

    N  N       N  N    1 X P E H U  R I  & U L W L F D O  ) O R Z V K                $ Y H U D J H  3 H U I R U P D Q F H  5 D W L R $ E L O H Q H  1 H W Z R U N    N  N       N  N    1 X P E H U  R I  & U L W L F D O  ) O R Z V K                   $ Y H U D J H  3 H U I R U P D Q F H  5 D W L R ( % 2 1 (  1 H W Z R U N    N  N       N  N    1 X P E H U  R I  & U L W L F D O  ) O R Z V K                   $ Y H U D J H  3 H U I R U P D Q F H  5 D W L R 6 S U L Q W O L Q N  1 H W Z R U N    N  N       N  N    1 X P E H U  R I  & U L W L F D O  ) O R Z V K                $ Y H U D J H  3 H U I R U P D Q F H  5 D W L R 7 L V F D O L  1 H W Z R U N $ E L O H Q H  1 H W Z R U N ( % 2 1 (  1 H W Z R U N 6 S U L Q W O L Q N  1 H W Z R U N 7 L V F D O L  1 H W Z R U N                   $ Y H U D J H PRU & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                    7 U D I I L F  0 D W U L [  , Q G H [                           PRU $ E L O H Q H  1 H W Z R U N & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3             7 U D I I L F  0 D W U L [  , Q G H [                              PRU ( % 2 1 (  1 H W Z R U N ( [ S R Q H Q W L D O  7 0              8 Q L I R U P  7 0   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3             7 U D I I L F  0 D W U L [  , Q G H [                           PRU 6 S U L Q W O L Q N  1 H W Z R U N ( [ S R Q H Q W L D O  7 0              8 Q L I R U P  7 0   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3             7 U D I I L F  0 D W U L [  , Q G H [                        PRU 7 L V F D O L  1 H W Z R U N ( [ S R Q H Q W L D O  7 0              8 Q L I R U P  7 0   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3TABLE II: Comparison of average rerouting disturbance

7

Topology
Abilene
EBONE (Exponential / Uniform)
Sprintlink (Exponential / Uniform)
Tiscali (Exponential / Uniform)

CFR-RL
21.3%

Top-K Critical
32.7%
11.2% / 10.0% 25.9% / 11.5% 32.9% / 11.7%
11.3% / 10.1% 23.6% / 13.8% 33.2% / 14.6%
11.2% / 10.0% 24.5% / 12.0% 32.7% / 12.2%

Top-K
42.9%

Fig. 6: Comparison of average end-to-end delay performance
ratio where error bars span ± one standard deviation from the
average on the entire test set of the four networks.

Fig. 8: Comparison of load balancing performance ratio in
CDF with the trafﬁc matrices from Tuesday, Wednesday,
Friday and Saturday in week 2.

Fig. 7: Comparison of end-to-end delay performance in the
four networks on each test trafﬁc matrix.

2) Performance Comparison: For comparison, we also
calculate the performance ratios and rerouting disturbances
for Top-K, Top-K critical, and ECMP according to Eqs.
(5), (6) and (7). Figure 4 shows the average load balancing
performance ratio that each scheme achieves on the entire test
set of the four networks. Figure 5 shows the load balancing
performance ratio on each individual trafﬁc matrix for the
four networks. Note that the ﬁrst 15 trafﬁc matrices in Figs.
5(b)-5(d) are generated by an exponential model and the
remaining 15 trafﬁc matrices are generated by an uniform
model. CFR-RL performs signiﬁcantly well in all networks.
For example, for the Abilene network, CFR-RL improves load
balancing performance by about 32.8% compared to ECMP,
and by roughly 7.4% compared to Top-K critical. For the
EBONE network, CFR-RL outperforms Top-K critical with
an average 12.2% load balancing performance improvement.
For Sprintlink and Tiscali networks, CFR-RL performs slightly
better than Top-K critical by 1.3% and 3.5% on average,
respectively. Moreover, Figure 6 shows the average end-to-
end delay performance ratio that each scheme achieves on
the entire test set of the four networks. Figure 7 shows the

Fig. 9: Comparison of end-to-end delay performance ratio
in CDF with the trafﬁc matrices from Tuesday, Wednesday,
Friday and Saturday in week 2.

end-to-end delay performance ratio on each test trafﬁc matrix
for the four networks. It is worth noting that the rerouting
solution for selected critical ﬂows is still obtained by solving
(4a) (i.e., minimize maximum link utilization), though the end-
to-end delay performance is evaluated5 for each scheme. By
effectively selecting and rerouting critical ﬂows to balance
link utilization of the network, CFR-RL outperforms heuristic
schemes and ECMP in terms of end-to-end delay in all
networks except the EBONE network. In the EBONE network,
heuristic schemes performs better with the exponential trafﬁc

5For the Abilene network, the real trafﬁc demands in the measured trafﬁc
matrices collected in [34] are relatively small, and thus the corresponding
end-to-end delay would be very small. To effectively compare end-to-end
delay performance of each scheme, we multiply each demand D s, d in a real
0.9
, where U ECMP
trafﬁc matrix T Mt by
is the maximum link utilization
U ECMP
t
achieved by ECMP routing on the trafﬁc matrix T Mt .

t

 $ E L O H Q H  1 H W Z R U N ( % 2 1 (  1 H W Z R U N 6 S U L Q W O L Q N  1 H W Z R U N 7 L V F D O L  1 H W Z R U N                   $ Y H U D J H PRΩ & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                    7 U D I I L F  0 D W U L [  , Q G H [                                 PRΩ $ E L O H Q H  1 H W Z R U N & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3             7 U D I I L F  0 D W U L [  , Q G H [                                 PRΩ ( % 2 1 (  1 H W Z R U N ( [ S R Q H Q W L D O  7 0              8 Q L I R U P  7 0   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3             7 U D I I L F  0 D W U L [  , Q G H [                           PRΩ 6 S U L Q W O L Q N  1 H W Z R U N ( [ S R Q H Q W L D O  7 0              8 Q L I R U P  7 0   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3             7 U D I I L F  0 D W U L [  , Q G H [                        PRΩ 7 L V F D O L  1 H W Z R U N ( [ S R Q H Q W L D O  7 0              8 Q L I R U P  7 0   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                  PRU                   & ' ) ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                  PRU                   & ' ) ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                  PRU                   & ' ) ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                  PRU                   & ' ) ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                           PRΩ                   & ' ) ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                           PRΩ                   & ' ) ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                           PRΩ                   & ' ) ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                           PRΩ                   & ' ) ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 38

of trafﬁc for uniform trafﬁc matrices. However, CFR-RL is
still able to perform slightly better than the two rule-based
heuristics. Overall, the above results indicate that CFR-RL is
able to achieve near-optimal load balancing performance and
greatly reduce end-to-end delay and network disturbance by
smartly selecting a small number of critical ﬂows for each
given trafﬁc matrix and effectively rerouting the corresponding
small amount of trafﬁc.

As shown in Figs. 5(b)-5(d), Top-K critical performs well
with the exponential trafﬁc model. However, its performance is
degraded with the uniform trafﬁc model. One possible reason
for the performance degradation of Top-K critical is that all
links in the network are relatively saturated under the uniform
trafﬁc model. Alternative underutilized paths are not available
for the critical ﬂows selected by Top-K critical. In other words,
there is no much room for rerouting performance improvement
by only considering the elephant ﬂows traversing the most con-
gested links. Thus, ﬁxed-rule heuristics are unable to guarantee
their performance, showing that their design assumptions are
invalid. In contrast, CFR-RL performs consistently well under
various trafﬁc models.

3) Generalization: In this series of experiments, we trained
CFR-RL on the trafﬁc matrices from the ﬁrst week (starting
from Mar. 1st 2004) and evaluate it for each day of the
following week (starting from Mar. 8th 2004) for the Abilene
network. We only present the results for day 2, day 3, day 5
and day 6, since the results for other days are similar. Figures
8 and 9 show the full CDFs of two types of performance ratio
for these 4 days. Figures 10 and 11 show the load balancing
and end-to-end delay performance ratios on each trafﬁc matrix
of these 4 days, respectively. The results show that CFR-RL
still achieves above 95% optimal load balancing performance
and average 88.13% end-to-end delay performance, and thus
outperforms other schemes on almost all trafﬁc matrices. The
load balancing performance of CFR-RL degrades on several
outlier trafﬁc matrices in day 2. There are two possible reasons
for the degradation: (1) The trafﬁc patterns of these trafﬁc
matrices are different from what CFR-RL learned from the
previous week. (2) Selecting K = 10% ∗ N ∗ (N − 1) is not
enough for CFR-RL to achieve near-optimal performance on
these outlier trafﬁc matrices. However, CFR-RL still performs
better than other schemes. Overall, the results indicate that real
trafﬁc patterns are relatively stable and CFR-RL generalizes
well to unseen trafﬁc matrices for which it was not explicitly
trained.

it

4) Training and Inference Time: Training a policy for the
Abilene network took approximately 10,000 iterations, and the
time consumed for each iteration is approximately 1 second.
As a result, the total training time for Abilene network is
approximately 3 hours. Since the EBONE network is relatively
larger,
took approximately 60,000 iterations to train a
policy. Then, the total training time for EBONE network is
approximately 16 hours. For larger networks like Sprintlink
and Tiscali, the solution space is even larger. Thus, more
iterations (e.g., approximately 90,000 and 100,000 iterations)
should be taken to train a good policy, and each iteration takes
approximately 2 seconds. Note that this cost is incurred ofﬂine
and can be performed infrequently depending on environment

Fig. 10: Comparison of load balancing performance ratio with
the trafﬁc matrices from Tuesday, Wednesday, Friday and
Saturday in week 2.

Fig. 11: Comparison of end-to-end delay performance ratio
with the trafﬁc matrices from Tuesday, Wednesday, Friday and
Saturday in week 2.

model. It is possible that rerouting the elephant ﬂows selected
by heuristic schemes further balances load on non-congested
links and results in achieving smaller end-to-end delay. In
addition, Tab. II shows the average rerouting disturbance,
i.e., the average percentage of total trafﬁc rerouted by each
scheme (except ECMP) for the four networks. CFR-RL greatly
reduces network disturbance by rerouting at most 21.3%,
11.2%, 11.3%, and 11.2% of total trafﬁc on average for the
four networks, respectively. In contrast, Top-K critical reroutes
11.4% more trafﬁc for the Abilene network and 14.7%, 12.3%,
and 13.3% more trafﬁc for the EBONE, Sprintlink, and Tiscali
networks (for exponential trafﬁc matrices). Top-K performs
even worse by rerouting more than 42% of total trafﬁc on
average for the Abilene network and 32%, 33%, and 32%
of total trafﬁc on average for the other three networks (for
exponential
there
are no elephant ﬂows in uniform trafﬁc matrices shown in
Fig. 5(b)-5(d). Thus, all three schemes reroute similar amount

is worth noting that

trafﬁc matrices). It

                7 U D I I L F  0 D W U L [  L Q G H [                        PRU ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                7 U D I I L F  0 D W U L [  L Q G H [                        PRU ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                7 U D I I L F  0 D W U L [  L Q G H [                        PRU ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                7 U D I I L F  0 D W U L [  L Q G H [                        PRU ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                7 U D I I L F  0 D W U L [  L Q G H [                                 PRΩ ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                7 U D I I L F  0 D W U L [  L Q G H [                                 PRΩ ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                7 U D I I L F  0 D W U L [  L Q G H [                                 PRΩ ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3                7 U D I I L F  0 D W U L [  L Q G H [                                 PRΩ ' D \   & ) 5  5 / 7 R S  .  & U L W L F D O 7 R S  . ( & 0 3TABLE III: Comparison of average load balancing perfor-
mance ratio with different sets of hyperparameters

α = 0.01 (with decay)
α = 0.001 (with decay)
α = 0.0001*

ﬁlters / neurons = 128, β = 0.1
0.761
0.970
0.963

* without decay, since the initial learning rate is equal to the

minimum learning rate.

(a)

ﬁlters / neurons = 64
ﬁlters / neurons = 128
ﬁlters / neurons = 256

α = 0.001 (with decay), β = 0.1
0.928
0.970
0.837

(b)

β = 0.1
β = 0.01

ﬁlters / neurons = 128, α = 0.001 (with decay)
0.970
0.958

(c)

stability. The policy neural network as described in Section
VI-A1 is relatively small. Thus, the inference time for the
Abilene and EBONE networks are less than 1 second, and
they are less than 2 seconds for the Sprintlink and Tiscali
networks.

5) Hyperparameters: Table III shows that how hyperpa-
rameters affect the load balancing performance of CFR-RL
in the Abilene network. For each set of hyperparameters, we
trained a policy for the Abilene network by 10,000 iterations,
and then evaluated the average load balancing performance
ratio over the whole test set. We only presented the results
for the Abilene network, since the results for other network
topologies are similar. In Tab. III(a), the number of ﬁlters in
the convolutional layer and neurons in the fully connected
layer is ﬁxed to 128 and entropy factor β is ﬁxed to 0.1.
We compare the performance with different learning rate α.
The results show that training might become unstable if the
initial learning rate is too large (e.g., 0.01), and thus it cannot
converge to a good policy. In contrast, training with a smaller
learning rate is more stable but might require longer training
time to further improve the performance. As a result, we chose
α = 0.001 to encourage exploration in the early stage of
training. We compared the performance with different sizes
of ﬁlters and neurons in Tab. III(b). The results show that
too few ﬁlters/neurons might restrict the representation that
the neural network can learn and thus causes under-ﬁtting.
Meanwhile, too many neurons might cause over-ﬁtting, and
thus the corresponding policy cannot generalize well to the
test set. In addition, more training time is required for a
larger neural network. In Tab. III(c), the results show that a
larger entropy factor encourages exploration and leads to a
better performance. Overall, the set of hyperparameters we
have chosen is a good trade-off between performance and
computational complexity of the model.

VII. CONCLUSION AND FUTURE WORK

With an objective of minimizing the maximum link utiliza-
tion in a network and reducing disturbance to the network
causing service disruption, we proposed CFR-RL, a scheme

9

that learns a critical ﬂow selection policy automatically us-
ing reinforcement learning, without any domain-speciﬁc rule-
based heuristic. CFR-RL selects critical ﬂows for each given
trafﬁc matrix and reroutes them to balance link utilization
of the network by solving a simple rerouting optimization
problem. Extensive evaluations show that CFR-RL achieves
near-optimal performance by rerouting only a limited portion
of total trafﬁc. In addition, CFR-RL generalizes well to trafﬁc
matrices for which it was not explicitly trained.

Yet,

there are several aspects that may help improving
the solution that we proposed in this contribution. Among
them, we are determining how CFR-RL can be updated and
improved.

Objectives: CFR-RL could be formulated to achieve other
objectives. For example, to minimize overall end-to-end delay
in the network (i.e., Ω = (cid:205)
)) described in Section
(

li . j
ci, j −li, j

(cid:104)i, j (cid:105) ∈E

VI-A4(2), we can deﬁne reward r as 1/Ω and reformulate the
rerouting optimization problem (4a) to minimize Ω.

Table II shows an interesting ﬁnding. Although CFR-RL
does not explicitly minimize rerouting trafﬁc,
it ends up
rerouting much less trafﬁc (i.e., 10.0%-21.3%) and performs
better than rule-based heuristic schemes by 1.3%-12.2%. This
reveals that CFR-RL is effectively searching the whole set of
candidate ﬂows to ﬁnd the best critical ﬂows for various trafﬁc
matrices, rather than simply considering the elephant ﬂows on
the most congested links or in the whole network as rule-based
heuristic schemes do. We will consider minimizing rerouting
trafﬁc as one of our objectives and investigate the trade-off
between maximizing performance and minimizing rerouting
trafﬁc.

Scalability: Scaling CFR-RL to larger networks is an im-
portant direction of our future work. CFR-RL relies on LP
to produce reward signals r. The LP problem would become
complex as the number of critical ﬂows K and the size of a
network increase. This would slow down the policy training for
larger networks (e.g., the Tiscali network in Section VI-B4),
since the time consumed for each iteration would increase.
Moreover, the solution space would become enormous for
larger networks, and RL has to take more iterations to converge
to a good policy. To further speed up training, we can either
spawn even more actor agents (e.g., 30) in parallel to allow
the system to consume more data at each time step and thus
improve exploration [28], or apply GA3C [36] to ofﬂoad
the training to a GPU, which is an alternative architecture
of A3C and emphasizes on an efﬁcient GPU utilization to
increase the number of training data generated and processed
per second. Another possible design to mitigate the scalability
issue is adopting SDN multi-controller architectures. Each
controller takes care of a subset of routers in a large network,
and one CFR-RL agent is running on each SDN controller.
The corresponding problem naturally falls into the realm of
Multi-Agent Reinforcement Learning. We will evaluate if a
multi-SDN controller architecture can help provide additional
improvement in our approach.

Retraining: In this paper, we mainly described the RL-
based critical ﬂow selection policy training process as an

ofﬂine task. In other words, once training is done, CFR-
RL remains unmodiﬁed after being deployed in the network.
However, CFR-RL can naturally accommodate future unseen
trafﬁc matrices by periodically updating the selection policy.
This self-learning technique will enable CFR-RL to further
adapt itself to the dynamic conditions in the network after
being deployed in real networks. CFR-RL can be retrained
by including new trafﬁc matrices. For example, the outlier
trafﬁc matrices (e.g., the 235th-240th trafﬁc matrices in Day
2) presented in Fig. 10 should be included for retraining, while
the generalization results shown in Section VI-B3 suggest
that retraining frequently might not be necessary. Techniques
to determine when to retrain and which new/old trafﬁc ma-
trix should be included/excluded in/from the training dataset
should be further investigated.

The above examples are some key issues that are left for

future work.

ACKNOWLEDGMENTS

The authors would like to thank the editors and reviewers

for providing many valuable comments and suggestions.

REFERENCES

[1] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, L. Peterson,
J. Rexford, S. Shenker, and J. Turner, “Openﬂow: enabling innovation in
campus networks,” ACM SIGCOMM Computer Communication Review,
vol. 38, no. 2, pp. 69–74, 2008.

[2] S. Agarwal, M. Kodialam, and T. Lakshman, “Trafﬁc engineering
in software deﬁned networks,” in IEEE International Conference on
Computer Communications’13.

IEEE, 2013, pp. 2211–2219.

[3] Y. Guo, Z. Wang, X. Yin, X. Shi, and J. Wu, “Trafﬁc engineering in
sdn/ospf hybrid network,” in IEEE International Conference on Network
Protocols’14.

IEEE, 2014, pp. 563–568.

[4] J. Zhang, K. Xi, M. Luo, and H. J. Chao, “Dynamic hybrid routing:
Achieve load balancing for changing trafﬁc demands,” in IEEE Interna-
tional Symposium on Quality of Service’14.
IEEE, 2014, pp. 105–110.
[5] J. Zhang, K. Xi, and H. J. Chao, “Load balancing in ip networks using
generalized destination-based multipath routing,” IEEE/ACM Transac-
tions on Networking, vol. 23, no. 6, pp. 1959–1969, 2015.

[6] Z. Guo, W. Chen, Y.-F. Liu, Y. Xu, and Z.-L. Zhang, “Joint switch up-
grade and controller deployment in hybrid software-deﬁned networks,”
IEEE Journal on Selected Areas in Communications, vol. 37, no. 5, pp.
1012–1028, 2019.

[7] Y. Wang and Z. Wang, “Explicit routing algorithms for internet trafﬁc
engineering,” in IEEE International Conference on Computer Commu-
nications and Networks’99.

IEEE, 1999, pp. 582–588.

[8] E. D. Osborne and A. Simha, Trafﬁc engineering with MPLS. Cisco

Press, 2002.

[9] B. Fortz and M. Thorup, “Optimizing ospf/is-is weights in a changing
world,” IEEE Journal on Selected Areas in Communications, vol. 20,
no. 4, pp. 756–767, 2002.

[10] K. Holmberg and D. Yuan, “Optimization of internet protocol network
design and routing,” Networks: An International Journal, vol. 43, no. 1,
pp. 39–53, 2004.

[11] J. Chu and C.-T. Lea, “Optimal link weights for ip-based networks
supporting hose-model vpns,” IEEE/ACM Transactions on Networking,
vol. 17, no. 3, pp. 778–788, 2009.

[12] J. Zhang, K. Xi, L. Zhang, and H. J. Chao, “Optimizing network
performance using weighted multipath routing,” in IEEE International
Conference on Computer Communications and Networks’12.
IEEE,
2012, pp. 1–7.

[13] J. Zhang, K. Xi, M. Luo, and H. J. Chao, “Load balancing for
multiple trafﬁc matrices using sdn hybrid routing,” in IEEE International
Conference on High-Performance Switching and Routing’14, 2014, pp.
44–49.

[14] C. VILLAMIZAR, “Ospf optimized multipath (ospf-omp),” IETF
[Online]. Available:

Internet-Draft, draft-ietf-ospf-omp-03.txt, 1999.
https://ci.nii.ac.jp/naid/10026755527/en/

10

[15] M. Kodialam, T. Lakshman, J. B. Orlin, and S. Sengupta, “Oblivious
routing of highly variable trafﬁc in service overlays and ip backbones,”
IEEE/ACM Transactions on Networking, vol. 17, no. 2, pp. 459–472,
2008.

[16] M. Antic, N. Maksic, P. Knezevic, and A. Smiljanic, “Two phase
load balanced routing using ospf,” IEEE Journal on Selected Areas in
Communications, vol. 28, no. 1, pp. 51–59, 2009.

[17] F. Geyer and G. Carle, “Learning and generating distributed routing
protocols using graph-based deep learning,” in ACM Workshop on
Big Data Analytics and Machine Learning for Data Communication
Networks’18. ACM, 2018, pp. 40–45.

[18] P. Sun, J. Li, Z. Guo, Y. Xu, J. Lan, and Y. Hu, “Sinet: Enabling scalable
network routing with deep reinforcement learning on partial nodes,” in
ACM SIGCOMM’19 Posters and Demos, 2019, pp. 88–89.

[19] S.-C. Lin, I. F. Akyildiz, P. Wang, and M. Luo, “Qos-aware adaptive
routing in multi-layer hierarchical software deﬁned networks: A re-
inforcement learning approach,” in IEEE International Conference on
Services Computing’16.

IEEE, 2016, pp. 25–33.

[20] Z. Xu, J. Tang, J. Meng, W. Zhang, Y. Wang, C. H. Liu, and D. Yang,
“Experience-driven networking: A deep reinforcement learning based
approach,” in IEEE International Conference on Computer Communi-
cations’18.

IEEE, 2018, pp. 1871–1879.

[21] L. Chen, J. Lingys, K. Chen, and F. Liu, “Auto: scaling deep rein-
forcement learning for datacenter-scale automatic trafﬁc optimization,”
in ACM SIGCOMM’18. ACM, 2018, pp. 191–205.

[22] H. Xu, Z. Yu, C. Qian, X. Li, and Z. Liu, “Minimizing ﬂow statistics
collection cost of sdn using wildcard requests,” in IEEE International
Conference on Computer Communications’17, 2017, pp. 1–9.

[23] R. J. Williams, “Simple statistical gradient-following algorithms for
connectionist reinforcement learning,” Machine Learning, vol. 8, no. 3,
pp. 229–256, May 1992.

[24] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep reinforcement
learning,” CoRR, vol. abs/1509.02971, 2015.

[25] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel, “Trust
region policy optimization,” CoRR, vol. http://arxiv.org/abs/1502.05477,
2015. [Online]. Available: http://arxiv.org/abs/1502.05477

[26] A. Valadarsky, M. Schapira, D. Shahaf, and A. Tamar, “Learning
to route,” in ACM Workshop on Hot Topics in Networks’17, ser.
HotNets-XVI. New York, NY, USA: ACM, 2017, pp. 185–191.
[Online]. Available: http://doi.acm.org/10.1145/3152434.3152441
[27] H. Mao, M. Alizadeh, I. Menache, and S. Kandula, “Resource man-
agement with deep reinforcement learning,” in ACM Workshop on Hot
Topics in Networks’16, New York, NY, USA, pp. 50–56.

[28] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley,
D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep
learning,” CoRR, vol. http://arxiv.org/abs/1602.01783,
reinforcement
2016. [Online]. Available: http://arxiv.org/abs/1602.01783

[29] Yufei Wang, Zheng Wang, and Leah Zhang, “Internet trafﬁc engineering
without full mesh overlaying,” in IEEE International Conference on
Computer Communications’01, vol. 1, April 2001, pp. 565–571.
[30] L. Gurobi Optimization, “Gurobi optimizer reference manual,” 2019.

[Online]. Available: http://www.gurobi.com

[32] M. Abadi

[31] A. L. Maas, A. Y. Hannun, and A. Y. Ng, “Rectiﬁer nonlinearities
improve neural network acoustic models,” in ICML workshop on Deep
Learning for Audio, Speech and Language Processing’13, 2013, pp. 1–6.
large-scale
machine learning,” in USENIX Conference on Operating Systems
Design and Implementation’16, ser. OSDI’16. Berkeley, CA, USA:
USENIX Association, 2016, pp. 265–283.
[Online]. Available:
http://dl.acm.org/citation.cfm?id=3026877.3026899

“Tensorﬂow: A system for

and et

al.,

[33] N. Spring, R. Mahajan, and D. Wetherall, “Measuring isp topologies
with rocketfuel,” in ACM SIGCOMM Computer Communication Review,
vol. 32, no. 4. ACM, 2002, pp. 133–145.

[34] Yin Zhang’s Abilene TM. [Online]. Available: http://www.cs.utexas.

edu/~yzhang/research/AbileneTM/

[35] TMgen: Trafﬁc Matrix Generation Tool. [Online]. Available: https:

//tmgen.readthedocs.io/en/latest/

[36] M. Babaeizadeh,

I. Frosio, S. Tyree, J. Clemons, and J. Kautz,
learning,” CoRR,
“GA3C: gpu-based A3C for deep reinforcement
vol. http://arxiv.org/abs/1611.06256, 2016. [Online]. Available: http:
//arxiv.org/abs/1611.06256

11

H. Jonathan Chao (M’83-F’01) received the B.S.
and M.S. degrees in electrical engineering from
National Chiao Tung University, Taiwan, in 1977
and 1980, respectively, and the Ph.D. degree in
electrical engineering from The Ohio State Univer-
sity, Columbus, OH, USA, in 1985. He was the
Head of the Electrical and Computer Engineering
(ECE) Department at New York University (NYU)
from 2004 to 2014. He has been doing research
in the areas of software-deﬁned networking, net-
work function virtualization, datacenter networks,
high-speed packet processing/switching/routing, network security, quality-of-
service control, network on chip, and machine learning for networking. During
2000-2001, he was the Co-Founder and a CTO of Coree Networks, Tinton
Falls, NJ, USA. From 1985 to 1992, he was a Member of Technical Staff
at Bellcore, Piscataway, NJ, USA, where he was involved in transport and
switching system architecture designs and application-speciﬁed integrated
circuit implementations, such as the world’s ﬁrst SONET-like framer chip,
ATM layer chip, sequencer chip (the ﬁrst chip handling packet scheduling),
and ATM switch chip. He is currently a Professor of ECE at NYU, New
York City, NY, USA. He is also the Director of the High-Speed Networking
Lab. He has co-authored three networking books, Broadband Packet Switching
Technologies-A Practical Guide to ATM Switches and IP Routers (New
York: Wiley, 2001), Quality of Service Control
in High-Speed Networks
(New York: Wiley, 2001), and High-Performance Switches and Routers (New
York: Wiley, 2007). He holds 63 patents and has published more than
260 journal and conference papers. He is a fellow of the IEEE and the
National Academy of Inventors. He was a recipient of the Bellcore Excellence
Award in 1987. He was a co-recipient of the 2001 Best Paper Award from
the IEEE TRANSACTION ON CIRCUITS AND SYSTEMS FOR VIDEO
TECHNOLOGY.

Junjie Zhang received the B.S. degree in com-
puter science from Nanjing University of Posts &
Telecommunications, China, in 2006, the M.S. de-
gree in computer science and the Ph.D. degree in
electrical engineering from New York University,
New York, NY, USA, in 2010 and 2015, respec-
tively.

He has been with Fortinet, Inc., Sunnyvale, CA,
USA, since 2015. He holds two US patents in the
area of computer networking. His research interests
trafﬁc engineering,
include network optimization,

machine learning, and network security.

Minghao Ye received the ﬁrst B.E. degree in micro-
electronic science and engineering from Sun Yat-
sen University, Guangzhou, China, and the second
B.E. degree (Hons.) in electronic engineering from
Hong Kong Polytechnic University, Hong Kong, in
2017, the M.S. degree in electrical engineering from
New York University, New York, NY, USA, in 2019,
where he is currently pursuing the Ph.D. degree
with the Department of Electrical and Computer
Engineering. His research interests include trafﬁc
engineering, software-deﬁned networks, mobile edge

computing, and reinforcement learning.

Zehua Guo (M’19-SM’20) received a B.S. de-
gree from Northwestern Polytechnical University, an
M.S. degree from Xidian University, and a Ph.D.
degree from Northwestern Polytechnical University.
He is an Associate Professor at Beijing Institute of
Technology. He was a Research Fellow at Depart-
ment of Electrical and Computer Engineering, New
York University Tandon School of Engineering, a
Research Manager at ChinaCache, a Senior Software
Engineer at DidiChuxing, a Post-Doctoral Research
Associate at Department of Computer Science and
Engineering, University of Minnesota Twin Cities, and a Visiting Associate
Professor at Singapore University of Technology and Design. His research
interests include software-deﬁned networking, network function virtualization,
data center network, cloud computing, content delivery network, network
security, machine learning, and Internet exchange. Dr. Guo is an Associate
Editor for IEEE ACCESS and the EURASIP Journal on Wireless Communi-
cations and Networking (Springer), and an Editor for the KSII Transactions
on Internet and Information Systems. He was the Session Chair for the IEEE
International Conference on Communications 2018 and the Technical Program
Committee Member of Computer Communications (Elsevier). He is a Senior
Member of IEEE.

Chen-Yu Yen received the B.S. degree in electri-
cal engineering from National Taiwan University,
Taipei, Taiwan, in 2014, and the M.S. degree in
electrical engineering from Columbia University in
2018. He is currently pursuing the Ph.D. degree
with the Department of Electrical and Computer
Engineering, New York University, New York, NY,
USA. His research interests include reinforcement
learning, congestion control, and practical machine
learning for networking.

