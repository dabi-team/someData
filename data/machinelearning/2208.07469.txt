2
2
0
2

g
u
A
5
1

]

C
O
.
h
t
a
m

[

1
v
9
6
4
7
0
.
8
0
2
2
:
v
i
X
r
a

Semideﬁnite Programming versus Burer-Monteiro Factorization for Matrix
Sensing

Baturalp Yalcin,*1 Ziye Ma,*2 Javad Lavaei, 1 Somayeh Sojoudi2
1UC Berkeley, Department of Industrial Engineering and Operations Research,
2UC Berkeley, Department of Electrical Engineering and Computer Science
{baturalp_yalcin, ziyema, lavaei, sojoudi}@berkeley.edu

Abstract

ity problem

Many fundamental low-rank optimization problems, such as
matrix completion, phase synchronization/retrieval, power
system state estimation, and robust PCA, can be formulated
as the matrix sensing problem. Two main approaches for solv-
ing matrix sensing are based on semideﬁnite programming
(SDP) and Burer-Monteiro (B-M) factorization. The SDP
method suffers from high computational and space complexi-
ties, whereas the B-M method may return a spurious solution
due to the non-convexity of the problem. The existing theo-
retical guarantees for the success of these methods have led
to similar conservative conditions, which may wrongly imply
that these methods have comparable performances. In this pa-
per, we shed light on some major differences between these
two methods. First, we present a class of structured matrix
completion problems for which the B-M methods fail with
an overwhelming probability, while the SDP method works
correctly. Second, we identify a class of highly sparse matrix
completion problems for which the B-M method works and
the SDP method fails. Third, we prove that although the B-
M method exhibits the same performance independent of the
rank of the unknown solution, the success of the SDP method
is correlated to the rank of the solution and improves as the
rank increases. Unlike the existing literature that has mainly
focused on those instances of matrix sensing for which both
SDP and B-M work, this paper offers the ﬁrst result on the
unique merit of each method over the alternative approach.

1 Introduction
Low-rank matrix recovery problems have ubiquitous ap-
plications in machine learning and data analytics,
in-
cluding collaborative ﬁltering (Koren, Bell, and Volinsky
2009), phase retrieval (Candes et al. 2015; Singer 2011;
Boumal 2016; Shechtman et al. 2015), motion detection
(Fattahi and Sojoudi 2020), and power system state esti-
mation (Jin et al. 2020; Zhang, Madani, and Lavaei 2017;
Jin et al. 2019). This problem is formally deﬁned as follows:
Rd re-
Given a measurement operator
(
·
A
7→
(M∗) from
turning a d-dimensional measurement vector
Rm×n with rank r,
a low-rank ground truth matrix M∗
the goal is to obtain a matrix with rank less than equal to r
that conforms with the measurements, preferably the ground
truth matrix M∗. This problem can be stated as the feasibil-

) : Rm×n

A

∈

*These authors contributed equally.

Rm×n

(1)

ﬁnd M
s.t.

∈

(M) =
A
rank(M)

(M∗)
r.

A
≤

A

While the measurement operator
can be nonlinear as
in the case of one-bit matrix sensing (Davenport et al.
2014) and phase retrieval (Shechtman et al. 2015), matrix
sensing and matrix completion that are widely studied
have linear measurement operators (Candès and Recht 2009;
Recht, Fazel, and Parrilo 2010). We focus on the matrix
sensing and matrix completion problems throughout this pa-
per. Despite the linearity of
, there are two types of prob-
A
lems depending on the structure of the ground truth ma-
trix M∗. The ﬁrst type, symmetric problem, consists of a
low-rank positive semideﬁnite ground truth matrix M∗
∈
Rn×n, whereas the second type, asymmetric problem, con-
Rm×n that is possibly
sists of a ground truth matrix M∗
sign indeﬁnite and non-square. Since each asymmetric prob-
lem can be converted to an equivalent symmetric problem
(Zhang, Bi, and Lavaei 2021a), we study only the symmet-
ric problem in this paper.

∈

The matrix sensing and completion problems have lin-
ear measurements; hence, the ﬁrst constraint in problem (1)
is linear. Therefore, the only nonconvexity of the problem
arises from the nonconvex rank constraint. Earlier works
on these problems focused on their convex relaxations
by penalizing high-rank solutions (Candès and Recht 2009;
Recht, Fazel, and Parrilo 2010; Candès and Tao 2010). They
utilized the nuclear norm of a matrix as the convex sur-
rogate of the rank function. This led to semideﬁnite pro-
gramming (SDP) relaxations, which solve the original non-
convex problems exactly with high probability based on
some assumptions on the linear measurement operator and
the ground truth matrix, such as the Restricted Isometry
Property (RIP) and incoherence conditions. High computa-
tional time and storage requirements of the SDP algorithms
incentivized the implementation of the B-M factorization ap-
proach (Burer and Monteiro 2003). This approach factorizes
Rn×n as M = XXT
the symmetric matrix variable M
Rn×r, which obviates imposing
for some matrix X
the positive semideﬁniteness and rank constraints. Although
the dimension of the decision variable reduces dramatically

∈

∈

 
 
 
 
 
 
when r is small, the problem is still nonconvex since its ob-
jective function is nonconvex in terms of the factorized X.

Problem Formulation
Formally, the SDP formulation of the matrix sensing prob-
lem uses the nuclear norm of the variable,
k∗, to serve
as a surrogate of the rank, and replaces the rank constraint
in (1) with an objective to minimize
k∗. Due to the sym-
metricity and positive semideﬁniteness of the variable, the
nuclear norm is equivalent to the trace of the matrix variable
M. Hence, the SDP formulation can be written as

M

M

k

k

min
M∈Rn×n

tr(M)

s.t.

(M) = b, M

A

0,

(2)

(cid:23)

{

, . . . ,
i

(M∗) = [

A1, M∗
h

]T is given
Ad, M∗
where b =
i
h
A
Rn×n are called sensing matrices. More-
d
Ai}
and
i=1 ∈
over, the matrix completion problem is a special case of the
matrix sensing problem with each sensing matrix measuring
only one entry of M∗. We can represent the measurement
Rn×n for this special case,
AΩ : Rn×n
operator
as
which is deﬁned as follows:

7→

A

AΩ(M)ij :=

Mij
0

(cid:26)

if (i, j)
∈
otherwise,

Ω

where Ω is the set of indices of observed entries. We denote
the measurement operator as MΩ :=
AΩ(M) for simplic-
ity. Besides the SDP formulation, the B-M factorization for-
mulation of the matrix sensing (MS) and matrix completion
(MC) problems can be stated as

(MS)

min
X∈Rn×r

g

(XXT )

h

A
(XXT
h

b

,

−
i
M∗)Ω

,

(3a)

(3b)

(MC)

min
X∈Rn×r

g

i

7→

) : Rd

−
R is some twice continuously differen-
where g(
·
tiable function such that 0n×n is its unique minimizer and
) is positive deﬁnite at 0n×n. These as-
the Hessian of g(
·
sumptions are satisﬁed by the common loss functions con-
sidered in the literature. The main objective of this paper is
to compare the SDP and B-M methods for the MC and MS
problems.

is widely known that

Background and Related Work
the SDP formulation (2) can
It
be used to solve the matrix sensing problem if
the
sensing matrices are sampled independently from a
sub-Gaussian distribution and the number of measure-
ments d is large enough (Recht, Fazel, and Parrilo 2010;
Recht, Xu, and Hassibi 2008). This is also a sufﬁcient con-
dition for the sensing matrices to satisfy the RIP condition
with high probability, which is deﬁned below:
Deﬁnition 1 (RIP). (Candès and Recht 2009) The linear
Rm is said to satisfy δp-RIP if there
: Rn×n
map
is a constant δp ∈
M
(1

7→
[0, 1) such that

M

A

2

δp)
k

−

2
F ≤ kA

k

2
(M)
F
k
Rn×n satisfying rank(M)

(1 + δp)
k

≤

k

holds for all matrices M

∈

A

The RIP constant δp represents how similar the linear
operator
is to an isometry, and various upper bounds
on δp have been proposed to serve as sufﬁcient conditions
for the exact recovery (meaning that one can recover the
ground truth M∗ by solving the SDP problem). A few no-
table ones include δ4r < √2
1 in (Candes and Plan 2010),
−
δ5r < 0.607, δ3r < 0.472 in (Mohan and Fazel 2010), and
δ2r < 1/2, δ3r < 1/3 in (Cai and Zhang 2013). On the
other hand, when the sensing matrices are not sampled in-
dependently from a sub-Gaussian distribution or when the
RIP condition is not met, the SDP formulation may still re-
cover the ground truth matrix with a high probability. This
is the case for MC problems for which RIP fails to hold
while SDP works as long as entries of observation follow
an independent Bernoulli model (Candès and Recht 2009;
Candès and Tao 2010).

However, recent works have shown that if we use the B-M
method instead of the SDP approach, we can still recover the
ground truth matrix via ﬁrst-order methods under similar
RIP or coherence assumptions in both the matrix sens-
(Ge, Jin, and Zheng
ing and matrix completion cases
2017;
2016;
Bhojanapalli, Neyshabur, and Srebro
Park et al. 2017; Zhang et al. 2018; Zhu et al. 2018;
Zhang, Sojoudi, and Lavaei 2019; Zhang and Zhang 2020;
Bi and Lavaei 2021; Ha, Liu, and Barber 2020; Zhu et al.
2021; Zhang 2021; Zhang, Bi, and Lavaei 2021b; Ma et al.
2022; Ma and Sojoudi 2022). Namely, the state-of-the-art
result states that as long as δ˜r+r < 1/2 for the matrix
sensing problem, there exists no spurious1 local minima
for an over-parametrized B-M formulation and the gradient
descent algorithm can recover M∗ exactly (Zhang 2021).
Here, ˜r
r is the search rank that we choose manually in
the B-M formulation. If we know the value of r, we can
set ˜r to r, making the B-M approach enjoy the same RIP
guarantee as the SDP approach. Since the B-M approach
enjoys far better scalability, it has become an increasingly
popular tool for solving the matrix sensing problem.

≥

Nevertheless, the B-M approach cannot be routinely used
without careful consideration since it could fail on easy
(from an information-theoretic perspective) instances of the
problem as demonstrated in (Yalçın et al. 2022), especially
in cases when the RIP condition is not satisﬁed.

Thus, it is important to compare and contrast both the SDP
and B-M approaches to discover which method is superior
to the other one. This comparison is timely since special-
ized sparse SDP algorithms have become more efﬁcient in
recent years, making the SDP method more practical than
before (Zhang and Lavaei 2021; Yurtsever et al. 2017, 2021).
In this paper, we show that the SDP approach is more pow-
erful than the B-M method as far as the RIP measure is
concerned. We also discover that the B-M method is able
to solve certain instances for which the SDP approach fails.
This means that none of these techniques is universally bet-
ter than the other one and the best technique should be cho-
sen based on the nature of the problem. This work provides
the ﬁrst step towards understanding the trade-off between a

1A local minimum is called spurious if it is not a global mini-

p.

≤

mum.

well-known convex relaxation and ﬁrst-order descent algo-
rithms applied to the B-M factorization formulation.

Our Contributions
We provide a comparative analysis between the SDP ap-
proach and the B-M method. We ﬁrst present the advantages
of the SDP approach over the B-M method:

1. First, we focus on an important class of MC problems
recently studied in (Yalçın et al. 2022). That paper has
shown that even though this class has low information-
theoretic complexity, the B-M method would utterly fail
and the probability of success via ﬁrst-order methods
is almost zero. We prove that the SDP method success-
fully solves this class and, therefore, SDP may not suffer
from the unusual behavior of B-M with regard to easy
instances of MC. This also implies that the information-
theoretic and optimization complexities are expected to
be more aligned for SDP than B-M.

−

((1

2. We then investigate a class of MS problems found in
the recent paper (Zhang et al. 2022). Each MS instance
belonging to this class satisﬁes δ2-RIP with r = 1 for
some δ > 1/2 such that the B-M formulation leads to
δ)−1) spurious solutions and this number goes to
O
inﬁnity as δ approaches 1. We show that although each in-
stance is extremely non-convex based on the number of
spurious solutions, the SDP method successfully solves
all of the problems in this class. This implies that, unlike
the B-M method, the success of the SDP approach is not
directly correlated to the presence of many spurious solu-
tions.

3. The recent paper (Zhang, Bi, and Lavaei 2021a) has
shown that the sharpest RIP bound for the success of
the B-M method on the MS problem is 1/2 and this
is independent of the rank r. This is an undesirable re-
sult since high-rank problems have lower information-
theoretic complexity than low-rank problems. We derive
a sufﬁcient RIP bound for the SDP method and show that
it can increase from 1/2 to 1 as the rank r becomes larger.
This implies that the SDP approach does not suffer from
a major shortcoming of the B-M method.

Despite the above advantages, we show that the SDP ap-
proach is not universally better than the B-M method. To
prove this, we identify a class of MC problems with
(n)
observations in the rank-1 case for which B-M works while
SDP fails. It is clear from these comparisons that although
the B-M approach is known to be more powerful due to its
scalability property, the SDP approach enjoys some unique
merits and deserves to be revisited, especially in light of the
advancements of fast SDP solvers (Zhang and Lavaei 2021;
Yurtsever et al. 2017, 2021)

O

2 Notations
The symbol [n] represents the set of integers from 1 to n.
We use lower-case bold letters, namely x, to represent vec-
tors and capital bold letters, namely X, to represent matri-
ces. In refers to the identity matrix of size n
n and 0n×n
n dimensional matrix with zero entries.
refers to the n

×

×

i

k

⊗

X
k

x
denotes the Euclidean norm of the vector x,
and
k
k
X
kF are the 2-norm and the Frobenius norm of the ma-
k
trix X, respectively. For every vector x, [x]i denotes the i-
th entry and [x]i:j denotes the subvector of entries from in-
dex i to index j for i < j. Similarly, for every matrix X,
[X]i:j,k:l denotes the submatrix with rows between i and j
and columns between k and l with i < j and k < l. Let
= tr(AT B) be the inner product between matri-
A, B
h
ces. The Kronecker product between A and B is denoted
as A
B. For a matrix X, vec(X) is the usual vectoriza-
tion operation by stacking the columns of the matrix X into
Rn2
, mat(x) converts x to a
a vector. For a vector x
square matrix and matS(x) converts x to a symmetric ma-
trix, i.e., mat(x) = X and matS(x) = (X + XT)/2, where
Rn×n is the unique matrix satisfying x = vec(X).
X
0 mean that the matrix X
The notations X
(cid:23)
is positive semideﬁnite (PSD) and positive deﬁnite, respec-
tively. The set of n
+. For
×
R, we denote the gradient and the
a function f : Rm×n
2f (
), respectively. The Hessian is a
) and
Hessian as
f (
·
·
2f (X)]i,j,k,l = ∂2f (X)
four-dimensional tensor with [
∂Xi,j ∂Xk,l
for all i, j
to denote
and
the ceiling and ﬂoor operators, respectively. The cardinality
of a set

n PSD matrices is denoted as Sn

∈
is shown as

[m] and k, l

[n]. We use

0 and X

7→
∇

∈
.

⌊·⌋

⌈·⌉

∇

∇

≻

∈

∈

S

|S|

3 Advantages of the SDP Approach

B-M Fails While SDP Succeeds
In this section, we focus on a class of MC instances that was
ﬁrst proposed in (Yalçın et al. 2022) for which the B-M fac-
torization fails. We focus on the matrix completion problem
since it is the most common special case of the matrix sens-
ing problem that does not satisfy the RIP condition. We will
prove that while the B-M approach fails to recover M∗, the
SDP approach can provably ﬁnd M∗.

∈

≥

×

≥

1 and n

i,j for i, j

We will ﬁrst give an introduction to this class of MC in-
Sn
stances. Consider a rank-r ground truth matrix M∗
+
with r
2r. Let m := n/r and assume without
the loss of generality that n is divisible by r. We decompose
the ground truth matrix into blocks of dimension r
r; thus,
×
M∗ is an m
m block matrix whose block element at the
position (i, j) is denoted as M∗
[m]. We require
some graph-theoretic notions before introducing the under-
lying class of MC instances.
Deﬁnition 2 (Induced Measurement Set). Let
(
G2) = (
G1,
V
the node set
V
[m]
×
is deﬁned as follows: if (i, j)
M∗
i,j is observed; if (i, j)
tries of the block M∗
entries of the block is observed. The graph
as the block sparsity graph.

=
E2) be a pair of undirected graphs with
E2 ⊂
)
G
∈ E1, then the entire block
∈ E2, then all nondiagonal en-
i,j are observed; otherwise, none of the
is referred to

E1,
= [m] and the disjoint edge sets
[m], respectively. The induced measurement set Ω(

G
E1,

∈

G

,

We represent the general problem (1) with the linear mea-

and rank-r ground truth matrix M∗

surement operator
∈
Rn×n as
M∗,A,n,r. If this is a matrix completion problem
with the measurement set Ω, then this special case of the
M∗,Ω,n,r. Based on Deﬁnition
same problem is denoted as

A

P

P

2 and this notation, a low-complexity class of MC instances
will be introduced below. These instances have a low com-
plexity because graph-theoretical algorithms can solve them
in polynomial time in terms of n and r.
Deﬁnition 3 (Low-complexity class of MC instances). De-
, n, r) to be the class of low-complexity MC in-
ﬁne
(
G
L
M∗,Ω,n,r with the following properties:
stances
P
Sn
i) The ground truth M∗
+ is rank-r.
Rr×r is rank-r for all i, j
ii) The matrix M∗
iii) The measurement set Ω = Ω(

=
G1 is connected, non-bipartite, and its
G2), where
G1,
(
vertices have self-loops.
The next theorem borrowed from (Yalçın et al. 2022) il-

) is induced by

i,j ∈

[m].

∈

∈

G

G

lustrates the failure of the B-M factorization method.
Theorem 1. Consider a maximal independent set
G1)
(
S
,
G1 such that the induced subgraph by vertices in
of
S
], is connected. There exists an instance in
, n, r)
(
G2[
L
G
S
for which the problem (3b) has at least 2r|S(G1)|
2r spu-
−
rious local minima. In addition, the randomly initialized
gradient descent algorithm converges to a global minimum
(2−r|S(G1)|), while there is a
with probability at most
graph-theoretical algorithm that can solve the problem in

O

O

(n2/r2 + nr2) time.
The proof of Theorem 1 utilizes the Implicit Function
Theorem (IFT). Speciﬁcally, the work (Yalçın et al. 2022)
has generated ground truth matrices M∗ for which the B-M
method has 2r|S(G1)| global solutions and only 2r of them
correspond to the correct completion of the M∗. A generic
small perturbation of the problem results in a new instance of
an MC problem that belongs to the low-complexity class of
MC instances. The conditions on
G1 guarantee that the per-
turbed problem belongs to the low-complexity class, while
G2 guarantee that the Hessian of the ob-
the conditions on
jective function of the unperturbed problem is positive def-
inite at the global solutions. Since the instances in the low-
complexity class are well deﬁned, the new perturbed prob-
lem has a unique completion with 2r possible global solu-
tions for the B-M method. On the other hand, the other sta-
tionary points that correspond to global solutions of the un-
perturbed problem must be spurious local minima of the new
instance. This is concluded by using the IFT. The perturba-
tion that yields a new instance in the low-complexity class of
the MC problem is achieved by perturbing the ground truth
matrix M∗ = X∗(X∗)T by a small and generic perturba-
Rn×r. The new ground truth matrix is M∗(ǫ) =
tion ǫ
X∗(ǫ)(X∗(ǫ))T , where X∗(ǫ)i = X∗
G1) and
(
X∗(ǫ)i = ǫi otherwise and rank(X∗
i + ǫi) =
[m]. A generic perturbation ǫ does not belong to a
r,
measure zero set in Rn×r.

i
∀
It is desirable to study how the SDP method performs on
this low-complexity class of MC instances. We will present
the result for a larger class of problems that contains all in-
stances discussed in Theorem 1.
Theorem 2. Given
E2), con-
G1,
E1,
G2) = (
V
G1). Consider also
sider any maximal independent set
(
S
Rn×r,
M∗(ǫ) = X∗(ǫ)(X∗(ǫ))T for any arbitrary ǫ
∈
G1) and X∗(ǫ)i = ǫi
where X∗(ǫ)i = X∗
i + ǫi if i
(

i + ǫi if i
∈ S
i ) = rank(X∗

= (

∈ S

∈

∈

G

,

O

i
∀

i + ǫi) = r,

i ) = rank(X∗
otherwise and rank(X∗
[m].
∈
The SDP formulation (2) with the observation set Ω induced
G1 uniquely recovers the ground truth matrix M∗(ǫ).
by
Note that we do not require ǫ to be small or have access
to partial observations of blocks induced by edges in
G2.
Hence, Theorem 2 shows that SDP solves all MC instances
introduced in Theorem 1 and beyond. As a result of Theo-
rem 2, the SDP approach is a viable choice for those MC
instances for which the preferable and faster B-M factoriza-
tion method fails to recover the ground truth matrix. Similar
to perturbing the ground truth matrix, one can perturb the lin-
ear measurement operator of the matrix completion problem
AΩ as

AΩ(ǫ)(M)ij :=

(cid:26)

if (i, j)

Mij,
∈
ǫMij, otherwise

Ω

,

(4)

−

where ǫ > 0 is a sufﬁciently small real number (Zhang et al.
2022). Note that
AΩ(ǫ) satisﬁes the RIP condition with δ =
ǫ)/(1 + ǫ).
(1
Theorem 3. Suppose that g is the squared loss function, i.e
2. Consider the measurement set Ω deﬁned in
g(x) =
Theorem 1. For every sufﬁciently small ǫ > 0, there exists
a low-complexity instance of the MS problem
PM∗,AΩ(ǫ),n,r
with

(2r|S(G1)|) spurious local minima.

x
k

k

−

≥

((1

δ)−1) for δ

The proof of the above theorem is similar to the proof of
Theorem 1 because the conditions for unperturbed problems
are the same and a different small perturbation to the prob-
lem yields a similar number of spurious solutions. Hence,
the proof is omitted. The above theorem states that there are
not only MC instances but also MS instances that suffer from
this undesirable behavior of the B-M factorization approach.
The ground truth matrix M∗ is generated as in Theorem 1
to have 2r|S(G1)| global solutions for the unperturbed prob-
lem. Furthermore, the number of spurious solutions for this
scheme can be quantiﬁed as
1/2 in
O
the rank-1 case (Zhang et al. 2022). Nevertheless, the SDP
formulation approach trivially solves all these undesirable
MS instances. This is due to the fact the perturbed measure-
ment operator
AΩ(ǫ) corresponds to observing all the entries.
Hence, the feasible set only contains the ground truth matrix
M∗.
Proposition 1. Given a measurement set Ω, the SDP formu-
lation (2) uniquely recovers the rank-r ground truth matrix
M∗ for the MS instance
AΩ(ǫ) is de-
ﬁned in (4) and ǫ is an arbitrary nonzero number.
Hence, the SDP approach successfully solves all the in-
stances in Theorem 3 for which the RIP constant exists
(while greater than 1/2), unlike the B-M method. Conse-
quently, SDP could be the preferred method when the sufﬁ-
cient conditions on RIP for exact recovery by the B-M factor-
ization are not met. In the next part, we will provide sharper
sufﬁciency bounds for the SDP approach, which further cor-
roborates its strength.

PM∗,AΩ(ǫ),n,r, where

Sharper RIP bound for SDP
Since the SDP method is more powerful than the B-M
factorization for certain classes of MC and MS problems

as shown in the previous section and since specialized
SDP algorithms can solve large-scale MC and MS prob-
lems, it is useful to further study the SDP method through
the lens of the well-known RIP notion. We will derive a
strong lower bound δlb on the RIP constant δ to guaran-
tee convergence to the ground truth solution by using a
proof technique called the inexistence of incorrect solution
(Zhang, Sojoudi, and Lavaei 2019). We aim to ﬁnd a lin-
ear measurement operator
with the smallest RIP constant
such that the SDP formulation converges to a wrong solution.
To do so, we need to solve the optimization problem

A

δ

min
δ,A
s.t.

(M) =

A
tr(M)

(M∗)
A
tr(M∗)

≤
satisﬁes the δ

A

−

2r-RIP property,

≤

= M∗. The condition tr(M)

tr(M∗) guar-
where M
antees that SDP cannot uniquely recover M∗. Checking the
RIP constant for a linear measurement operator is proven to
be NP-hard (Tillmann and Pfetsch 2013). Therefore, it is dif-
ﬁcult to solve the problem (5) analytically. To simplify the
problem, we will introduce some notations. We use a matrix
representation of the measurement operator
A
A = [vec(A1), vec(A2), . . . , vec(Ad)]T

.
Rn×n.
Then, A vec(M) =
We deﬁne H = AT A, which is the matrix representation of
the kernel operator
to simplify the last constraint
of the problem (5).

(M) for every matrix M

as follows:
Rd×n2

A
=

T
A

H

A

∈

∈

To derive a RIP bound, we consider the following opti-
mization problem given M and M∗, where M is the global
solution of (2) and M∗ is the ground truth solution:

δ

min
δ,H
s.t. eT He = 0

(6)

H is symmetric and satisﬁes the δ2r-RIP,

where

e = vec(M∗

M).

−

= M∗ and
For this ﬁxed M and M∗, we assume that M
that rank(M∗
M) > 2r, since if rank(M∗
M)
2r,
the relation M = M∗ holds automatically by deﬁnition of
δ2r-RIP for any δ since it implies strong convexity. Denote
the optimal value to (6) as δ(e), which is a function of e. It
is desirable to ﬁnd

−

−

≤

δ∗ :=

δ(e).

min
e:tr(M)≤tr(M∗)
By the logic of in-existence of counterexample, we know
that if a problem H = AT A has δ2r-RIP with δ < δ∗, then
the solution to (2) will be M∗, which is the ground truth
solution. However, since the last constraint of (6) is non-
convex, it is useful to replace it with a surrogate condition
that allows solving the problem analytically. The following
problem helps to achieve this goal:

δ

min
δ,H
s.t. eT He
(1

2
≤
δ)In2

eck
k
H
(cid:22)

−

(cid:22)

2 + 2(l

3)δ

k
−
(1 + δ)In2 .

eck

2

(7)

⌈

⌉

n/r

ei}

and we deﬁne

l
i=1 and ec in the fol-
Here, l =
{
lowing fashion. First, consider the eigendecomposition of
M∗
M and assume that the eigenvalues are ordered in
−
terms of their absolute values, namely,
λ2| ≥ · · · ≥
. Let uk’s denote the corresponding orthonormal eigen-
λn|
|
vectors:

λ1| ≥ |

|

matS(e) = M∗

M =

−

Then, we deﬁne:

min{i∗r,n}

(5)

ei = vec



λkukuT
k .

n

Xk=1

λkukuT

k 

,

Xk=(i−1)∗r+1




l
e2r = e1 + e2, and ec =
i=3 ei. The next proposition
allows us to replace (6) with (7) because the optimal value
P
of the (7), δlb(e), gives a lower bound on δ(e).
Proposition 2. The optimal objective value of the problem
(7), δlb(e), is always less than or equal to the optimal objec-
tive value of the problem (6), i.e., δlb(e)

δ(e).

The proof of this proposition is central to the construction
of the sufﬁciency bound, which is based on using a convex
program to serve as an estimate of the non-convex problem.
After we extend the RIP2r constraint in (7) to be RIPn(thus
making it convex), it is necessary to somehow preserve the
information that the near isometric property of H should
only apply to low-rank matrices. This is achieved by chang-
ing the ﬁrst constraint so that e does not need to be com-
pletely in the null space of H. (7) approximately requires
that H only maps a certain low-rank sub-manifold to 0. The
full proof can be found in the Appendix. As a result of Propo-
sition 2, it immediately follows that

≤

δlb =

min
e:tr(M)≤tr(M∗)

δlb(e)

δ∗.

≤

In fact, we can obtain a lower bound on the value δlb by
solving the problem (7) analytically. The following lemma
quantiﬁes a lower bound on δlb.
Lemma 1. It holds that

δlb

≥

n + (n

−

2r
2r)(2l

.

5)

−
bound

The

presented

best-known

in
sufﬁciency
(Cai and Zhang 2013) is independent of n and r. This
sufﬁciency lower bound on the RIP constant presented in
Lemma 1 can be tighter than 1/2 depending on the size
of the problem n and the rank of the ground truth matrix
r. For instance, the SDP formulation converges to ground
truth solution whenever RIP constant δ is close to 1 as
n/2. On the other hand, whenever r/n is ratio is small,
r
e.g. rank-1 matrix sensing problem with large n, δ < 1/2 is
a stronger guarantee for recovery of the ground truth matrix.
Combined with the 1/2 sufﬁciency bound that works for
both the symmetric and asymmetric cases (Cai and Zhang
2013), we obtain the following result:

−→

6
6
Theorem 4. The global solution of the SDP formulation (2)
will be the ground truth matrix M∗ if the sensing matrix
A
satisﬁes the RIP condition with the RIP constant δ2r satisfy-
ing the inequality:

δ2r < max

1/2,

where l =

n/r

⌈

⌉

(cid:26)

.

2r
2r)(2l

,

5)

(cid:27)

−

n + (n

−

Compared with the existing sufﬁciency RIP bounds, this
new result has a striking advantage. The bound δ2r < 1/2
has already been proven to be the sharpest for the B-M for-
mulation, which is independent of the search rank. In con-
trast, Theorem 4 shows that the RIP bound for SDP exceeds
this bound and approaches 1 as the rank r increases.

In this section, we have shown that as opposed to the pop-
ular belief that B-M enjoys very similar RIP guarantees as
the SDP approach, there are real beneﬁts to switching to the
SDP formulation, making it a more competitive option since
specialized SDP solvers are becoming more efﬁcient in re-
cent years. However, we will next provide some problem
instances for which the SDP method fails to solve the prob-
lem while the B-M method contains no spurious solutions,
which balances the desirable properties of the SDP method.

∈

4 Advantages of the B-M Method
In this section, we give two classes of rank-1 matrix com-
pletion problems for which the B-M factorization does not
contain any spurious solution while SDP fails to recover its
ground truth matrix. Throughout this section, the rank-1 pos-
itive semideﬁnite ground truth matrix M∗ = x∗(x∗)T is as-
sumed not to contain any zero entries, meaning that x∗
= 0
i 6
for all i
[n]. Before proceeding with the results, we pro-
vide two small examples to highlight the underlying ideas
behind the main results.
Example 1. Consider a block sparsity graph
=
= 3 nodes and the edge set
=
,
(
V
. Namely, it is a chain graph with 3
(1, 1), (1, 2), (2, 3)
}
{
nodes and a self-loop at the ﬁrst node. First, we aim to show
that only second-order critical points are the global solu-
tions of the B-M factorization method. The B-M factoriza-
tion formulation (3b) with the squared loss function can be
explicitly written as minx∈R3 f (x), where

) with

G
E

|V|

E

f (x) =

1
4

X(i,j)∈E
i=j

(x2

i −

(x∗

i )2)2 +

1
2

X(i,j)∈E
i6=j

(xixj −

i x∗
x∗

j )2.

The corresponding gradient and Hessian are:

∂f (x)
∂xi

=

Xi,j∈E
i=j

(x2

i −

(x∗

i )2)xi +

(xixj −

i x∗
x∗

j )xj ,

Xi,j∈E
i6=j

∂2f (x)
∂x2
i

∂2f (x)
∂xi∂xj

= 1[(i, i)

](3x2

i −

∈ E

(x∗

i )2) +

x2
j ,

i x∗
x∗
j ,

2xixj −
0,

=

(cid:26)

Xi,j∈E

= j and (i, j)

if i
otherwise

∈ E

.

(cid:23)

∇

∇

f (ˆx) = 0 and

Each second-order critical point ˆx must satisfy the con-
2f (ˆx)
ditions
0. The third entry of
the gradient implies either ˆx2 = 0 or ˆx2 ˆx3 = x∗
2x∗
3. When-
ever ˆx2 = 0, the Hessian is not positive semideﬁnite since
2f (ˆx)]2:3,2:3 6(cid:23)
0. Thus, ˆx2 ˆx3 = x∗
3 must hold. Follow-
[
∇
1x∗
ing this, ∂f (ˆx)/∂x2 implies either ˆx1 = 0 or ˆx1 ˆx2 = x∗
2.
However, if ˆx1 = 0, then ∂f (ˆx)/∂x1 gives
2 ˆx2 = 0,
which implies ˆx2 = 0. This contradicts the earlier result.
Thus, each second-order critical point must have the follow-
ing properties:
ˆx2
1 = (x∗

ˆx2 ˆx3 = x∗

ˆx1 ˆx2 = x∗

1x∗
x∗

1x∗
2,

2x∗
3.

1)2,

2x∗

−

The solution to this system of equations proves the exact
recovery of the ground truth matrix M∗. Hence, the only
second-order critical points are the valid factors of the
x∗.
ground truth solution, i.e

The next step is to demonstrate the failure of the SDP for-
mulation (2) for some instances of the MC problem with this
given block sparsity matrix
. The problem (2) is equivalent
to the optimization

G

±

min
M∈R3×3

M2,2 + M3,3





(cid:23)

0.

s.t

2 M1,3
2x∗
3
3 M3,3

1)2 x∗
(x∗
x∗
1x∗
M3,1


and ˆM1,3 = ˆM3,1 =
x∗
3| ≥ |

1x∗
2 M2,2 x∗
x∗
2x∗
Consider a feasible solution ˆM with ˆM2,2 = ˆM3,3 =
. Note that ˆM is fea-
1x∗
x∗
2x∗
x∗
2|
3|
|
|
. While the objective value of the
sible whenever
|
3)2, the objective value
2)2 +(x∗
ground truth matrix M∗ is (x∗
of the feasible solution ˆM is 2
2x∗
x∗
. Under the assumption
3|
, the feasible solution ˆM is strictly better than the
x∗
x∗
2|
3|
|
ground truth solution. Thus, SDP fails to recover the ground
truth matrix.

x∗
2|

>



|

|

O

This example clearly demonstrates the existence of MC in-
stances for which the B-M method successfully converges to
the ground truth solution while the SDP fails to ﬁnd the solu-
tion. One reason is that the number of measurements is
(n)
in this example, which is the minimum threshold for exact
completion. However, the statistical guarantees on SDP of-
ten need more observations. We can generalize Example 1 to
any chain graph with n nodes and a single self-loop at one
of the ends.
Theorem 5. Consider the MC problem with a rank-1 pos-
Rn×n that can be
itive deﬁnite ground truth matrix M∗
∈
factorized as M∗ = x∗(x∗)T with x∗
[n]. Let
= 0,
i
∈
= n and
) be a block sparsity graph with
. Then, the B-M

G
E
method (3b) does not contain any spurious solutions.

V
(1, 1), (1, 2), (2, 3),

1, n)
}

= (
=

i
∀
|V|

, (n

· · ·

−

E

{

,

The proof of Theorem 6 needs a careful treatment of the
second-order optimality conditions and is deferred to the ap-
pendix. In addition to the success of the B-M factorization,
the next result establishes the failure of the SDP for the in-
stances described in the above theorem.
Rn×n
Theorem 6. Consider the ground truth matrix M∗
satisfying the conditions in Theorem 6. Suppose that there
exist two indices j, k such that x∗
j and j, k > 2. Then,
the SDP problem (2) fails to recover the ground truth matrix.

k > x∗

∈

6
6
E

|V|

) with

As mentioned before, SDP fails due to a lack of obser-
vations on the diagonal entries of the ground truth matrix.
Note that the RIP condition is not satisﬁed since these are
MC problems. As a result, whenever we do not have suf-
ﬁcient guarantees on linear measurement operator, none of
the methods are superior to the other one in terms of exact re-
covery. The next example identiﬁes another class of problem
instances that corroborates these ﬁndings.
Example 2. Consider a block sparsity graph
=
= 3 nodes and the edge set
=
,
(
V
. Namely, it is a simple cycle with 3
(1, 2), (2, 3), (3, 1)
}
{
nodes. The B-M factorization formulation (3b) with the
squared loss function can be written the same as in Exam-
ple 1. Firstly, we can show that each second-order criti-
cal point ˆx only has nonzero entries, i.e., ˆxi
= 0 for all
[n]. Without loss of generality, suppose by contradiction
i
that ˆx1 = 0. In order for the stationarity condition to hold,
either ˆx2 ˆx3 = x∗
3 or ˆx2 = ˆx3 = 0 should be satisﬁed.
The latter implies ˆx = 0 and
0 in that case. Thus,
6(cid:23)
ˆx2 ˆx3 = x∗
2x∗
3 must hold. Following this, ∂f (ˆx)/∂x1 yields
1x∗
x∗
1x∗
x∗
3 ˆx3 = 0. Combining these two equations
2 ˆx2 −
−
2)2, which does not have any real so-
yields (ˆx3)2 =
−
lution. As a result, each second-order critical point ˆx must
have only nonzero entries.

2f (ˆx)

2x∗

(x∗

G
E

∇

∈

some (i, j)

By the condition

f (ˆx) = 0, whenever ˆxi ˆxj = x∗

i x∗
j holds for every (i, j)

, then ˆxi ˆxj = x∗

i x∗

∇

j for

∈ E

∈
. This system of equations yields the ground truth solution.
E
Accordingly, a spurious solution ˆx must have the following
i x∗
characteristics: ˆxi ˆxj 6
(i, j)
j ,
∀
i x∗
x∗
. Deﬁne ai,j = xixj −
1, 2, 3
{
condition becomes

and ˆxi 6
∈
j . Then, the stationarity

= x∗

= 0,

i
∀

∈ E

}

f (ˆx) =

∇

ˆa1,2 ˆx2 + ˆa1,3 ˆx3
ˆa1,2 ˆx1 + ˆa2,3 ˆx3
ˆa1,3 ˆx1 + ˆa2,3 ˆx2#

"

= 0.

Multiplying the ﬁrst entry of the gradient by ˆx1/ˆx2 and sub-
stituting with the second entry gives ˆa1,2 ˆx1 =
ˆa2,3 ˆx3. Suc-
cessively, substituting this to the third entry of the gradient
results in

−

ˆx3(ˆa1,3 ˆx1 −

ˆa2,3 ˆx2) = 0.
= 0, we must have
Because we search for a solution with ˆxi 6
ˆa2,3 ˆx2 = 0. This condition combined with the last
ˆa1,3 ˆx1 −
entry of the gradient results in the condition ˆa1,3 ˆx1 = 0,
which is a contradiction. As a result, all the second-order
critical points are global solutions that yield the ground
truth matrix completion.

Our next goal is to show that the SDP formulation (2)
fails for this class of instances of MC instances. Note that
the SDP formulation of the matrix completion problem con-
sidered in this example is equivalent to the formulation:

min
M∈R3×3

M1,1 + M2,2 + M3,3

s.t

"
Without loss of generality, assume that x∗
x∗
3 by the
symmetry of the problem. Consider a feasible rank-2 solu-
tion ˆM given as ˆM1,1 = x∗
x∗
1(x∗
1)

1x∗
x∗
1x∗
M1,1 x∗
3
2
2 M2,2 x∗
x∗
2x∗
1x∗
3
3 M3,3# (cid:23)
2x∗
x∗
1x∗
x∗
3
x∗
2 ≤
1 ≤
2), ˆM2,2 = x∗
x∗

2(x∗

0.

3 −

3 −

−

2x∗

1 + x∗

2). Note that ˆM is feasible whenever
and M3,3 = x∗
3(x∗
1 + x∗
x∗
x∗
2. The objective value of the feasible solution
3 ≥
ˆM is
2 + 2x∗
1x∗
2x∗
3, whereas the objective of
3)2. The
2)2 + (x∗
the ground truth solution M∗ is (x∗
feasible solution ˆM is strictly better than the ground truth
solution if x∗
2. Hence, the SDP cannot recover
the ground truth solution for all the instances with a simple
cycle block sparsity graph.

1)2 + (x∗

3 + 2x∗

3 > x∗

1 + x∗

1x∗

Similar to Example 1, SDP fails in this example due to a
lack of diagonal observations. Next, we can generalize this
instance to any simple cycle block sparsity graph with an
odd number of vertices.

,

E

V

∈

[n]. Let

= (
= n = 2k + 1 and

Theorem 7. Consider the matrix completion problem with
Rn×n
a rank-1 positive deﬁnite ground truth matrix M∗
that can be factorized as M∗ = x∗(x∗)T with x∗
=
i
) be a block sparsity graph with
0,

G
−
{
. Then, the B-M factorization problem (3b)

i
∈
∀
=
|V|
|E|
1, 2k), (2k, 0)
}
does not contain any spurious solutions.
Rn×n
Theorem 8. Consider the ground truth matrix M∗
satisfying the conditions in Theorem 5. Suppose that the con-
k
2t)2 holds for some cho-
t=0(x∗
dition
sen node 0. Then, the SDP problem (2) fails to recover the
ground truth matrix.

(0, 1), (1, 2), . . . (2k

2t−1)2 >

k
t=1(x∗

P

P

=

∈

E

Note that we can choose any node as node 0 due to the
symmetry of the problem. Therefore, the condition stated in
Theorem 8 is not restrictive because this condition sufﬁces
to hold for a chosen node 0 among 2k + 1 ones. As a re-
sult of the above theorems, the B-M factorization can outper-
form the convex relaxation approach. One important exten-
sion of the work presented in this section would be ﬁnding
subgraphs for which the B-M method is successful while the
SDP is unsuccessful, and then attaching these subgraphs to
generate larger block sparsity graphs. It is known that SDP
will fail for these instances and it is intriguing to investigate
the behavior of the B-M method for those instances.

5 Conclusions
In this paper, we conducted a comparison between two main
approaches to the matrix completion and matrix sensing
problems: a convex relaxation that gives an SDP formulation
and the B-M factorization method. It is well-known that both
of these methods enjoy mathematical guarantees for the re-
covery of the ground truth matrix whenever the RIP assump-
tion is satisﬁed with a sufﬁciently small δ. We offered the
ﬁrst result in the literature that compares these two methods
whenever the RIP condition is not satisﬁed or only satisﬁed
with a large constant. We discovered classes of problems
for which B-M factorization fails while the SDP recovers
the ground truth matrix. The fact that specialized SDP algo-
rithms are improved in recent years and can compete with
simple ﬁrst-order descent algorithms inspired us to inves-
tigate sharper bounds on sufﬁcient conditions for the SDP
formulation. We provided RIP bounds for the SDP formu-
lation that depend on the rank of the solution and are auto-
matically satisﬁed for high-rank problems, unlike the B-M

6
6
method. On the other hand, when the number of measure-
ments from the ground truth matrix is not high, we showed
that SDP fails drastically while the B-M method does not
contain any spurious solutions on its optimization landscape.
As a result, we conclude that none of the methods outper-
forms the other one whenever the sufﬁciency guarantees are
not met. The parameters of the problem, such as dimension,
rank, and linear measurement operator, determine which so-
lution method performs better. Consequently, it is prudent to
apply both solution methods in case the RIP and incoherence
are not satisﬁed.

References
Bhojanapalli, S.; Neyshabur, B.; and Srebro, N. 2016. Glob-
al Optimality of Local Search for Low Rank Matrix Recov-
ery. In Advances in Neural Information Processing Systems,
volume 29.
Bi, Y.; and Lavaei, J. 2021. On the absence of spurious
local minima in nonlinear low-rank matrix recovery prob-
lems. In International Conference on Artiﬁcial Intelligence
and Statistics, 379–387. PMLR.
Boumal, N. 2016. Nonconvex phase synchronization. SIAM
Journal on Optimization, 26(4): 2355–2377.
Burer, S.; and Monteiro, R. D. 2003. A nonlinear program-
ming algorithm for solving semideﬁnite programs via low-
rank factorization. Mathematical Programming, 95(2): 329–
357.
Cai, T. T.; and Zhang, A. 2013. Sharp RIP bound for sparse
signal and low-rank matrix recovery. Applied and Computa-
tional Harmonic Analysis, 35(1): 74–93.
Candes, E. J.; Eldar, Y. C.; Strohmer, T.; and Voroninski, V.
2015. Phase retrieval via matrix completion. SIAM review,
57(2): 225–251.
Candes, E. J.; and Plan, Y. 2010. Tight oracle bounds for
low-rank matrix recovery from a minimal number of random
measurements. arXiv preprint arXiv:1001.0339.
Candès, E. J.; and Recht, B. 2009. Exact matrix comple-
tion via convex optimization. Foundations of Computational
mathematics, 9(6): 717–772.
Candès, E. J.; and Tao, T. 2010. The power of convex relax-
ation: Near-optimal matrix completion. IEEE Transactions
on Information Theory, 56(5): 2053–2080.
Davenport, M. A.; Plan, Y.; Van Den Berg, E.; and Wootters,
M. 2014. 1-bit matrix completion. Information and Infer-
ence: A Journal of the IMA, 3(3): 189–223.
Fattahi, S.; and Sojoudi, S. 2020. Exact guarantees on the
absence of spurious local minima for non-negative rank-1
robust principal component analysis. Journal of machine
learning research.
Ge, R.; Jin, C.; and Zheng, Y. 2017. No spurious local min-
ima in nonconvex low rank problems: A uniﬁed geometric
analysis. In International Conference on Machine Learning,
1233–1242. PMLR.
Ha, W.; Liu, H.; and Barber, R. F. 2020. An Equivalence
Between Critical Points for Rank Constraints Versus Low-
Rank Factorizations. SIAM Journal on Optimization, 30(4):
2927–2955.

Jin, M.; Lavaei, J.; Sojoudi, S.; and Baldick, R. 2020. Bound-
ary defense against cyber threat for power system state esti-
IEEE Transactions on Information Forensics and
mation.
Security, 16: 1752–1767.
Jin, M.; Molybog, I.; Mohammadi-Ghazi, R.; and Lavaei, J.
2019. Towards robust and scalable power system state es-
timation. In 2019 IEEE 58th Conference on Decision and
Control (CDC), 3245–3252. IEEE.
Koren, Y.; Bell, R.; and Volinsky, C. 2009. Matrix factoriza-
tion techniques for recommender systems. Computer, 42(8):
30–37.
Ma, Z.; Bi, Y.; Lavaei, J.; and Sojoudi, S. 2022. Sharp Re-
stricted Isometry Property Bounds for Low-rank Matrix Re-
covery Problems with Corrupted Measurements. AAAI-22.
Ma, Z.; and Sojoudi, S. 2022. Noisy Low-rank Matrix Opti-
mization: Geometry of Local Minima and Convergence Rate.
arXiv preprint arXiv:2203.03899.
Mohan, K.; and Fazel, M. 2010. New restricted isome-
In 2010 IEEE In-
try results for noisy low-rank recovery.
ternational Symposium on Information Theory, 1573–1577.
IEEE.
Park, D.; Kyrillidis, A.; Carmanis, C.; and Sanghavi, S. 2017.
Non-square Matrix Sensing Without Spurious Local Minima
In Proceedings of the
via the Burer–Monteiro Approach.
20th International Conference on Artiﬁcial Intelligence and
Statistics, volume 54 of Proceedings of Machine Learning
Research, 65–74.
Recht, B.; Fazel, M.; and Parrilo, P. A. 2010. Guaranteed
minimum-rank solutions of linear matrix equations via nu-
clear norm minimization. SIAM review, 52(3): 471–501.
Recht, B.; Xu, W.; and Hassibi, B. 2008. Necessary and suf-
ﬁcient conditions for success of the nuclear norm heuristic
for rank minimization. In 2008 47th IEEE Conference on
Decision and Control, 3065–3070. IEEE.
Shechtman, Y.; Eldar, Y. C.; Cohen, O.; Chapman, H. N.;
Miao, J.; and Segev, M. 2015. Phase retrieval with appli-
cation to optical imaging: a contemporary overview. IEEE
signal processing magazine, 32(3): 87–109.
Singer, A. 2011. Angular synchronization by eigenvectors
and semideﬁnite programming. Applied and Computational
Harmonic Analysis, 30(1): 20–36.
Tillmann, A. M.; and Pfetsch, M. E. 2013. The compu-
tational complexity of the restricted isometry property, the
nullspace property, and related concepts in compressed sens-
ing. IEEE Transactions on Information Theory, 60(2): 1248–
1259.
Yalçın, B.; Zhang, H.; Lavaei, J.; and Sojoudi, S. 2022. Fac-
torization approach for low-complexity matrix completion
problems: Exponential number of spurious solutions and
failure of gradient methods. In International Conference on
Artiﬁcial Intelligence and Statistics, 319–341. PMLR.
Yurtsever, A.; Tropp, J. A.; Fercoq, O.; Udell, M.; and
Cevher, V. 2021. Scalable Semideﬁnite Programming. SIAM
Journal on Mathematics of Data Science, 3(1): 171–200.
Yurtsever, A.; Udell, M.; Tropp, J.; and Cevher, V. 2017.
Sketchy Decisions: Convex Low-Rank Matrix Optimization

with Optimal Storage. In Singh, A.; and Zhu, J., eds., Pro-
ceedings of the 20th International Conference on Artiﬁcial
Intelligence and Statistics, volume 54 of Proceedings of Ma-
chine Learning Research, 1188–1196. PMLR.
Zhang, G.; and Zhang, R. Y. 2020. How Many Samples Is
a Good Initial Point Worth in Low-Rank Matrix Recovery?
In Advances in Neural Information Processing Systems, vol-
ume 33, 12583–12592.
Zhang, H.; Bi, Y.; and Lavaei, J. 2021a. General low-rank
matrix optimization: Geometric analysis and sharper bounds.
arXiv preprint arXiv:2104.10356.
Zhang, H.; Bi, Y.; and Lavaei, J. 2021b. General Low-
Rank Matrix Optimization: Geometric Analysis and Sharper
Bounds. In Advances in Neural Information Processing Sys-
tems.
Zhang, H.; Yalcin, B.; Lavaei, J.; and Sojoudi, S. 2022.
A Uniﬁed Complexity Metric for Nonconvex Matrix Com-
pletion and Matrix Sensing in the Rank-one Case. arXiv
preprint arXiv:2204.02364.
Zhang, R. Y. 2021. Sharp Global Guarantees for Noncon-
vex Low-Rank Matrix Recovery in the Overparameterized
Regime. ArXiv:2104.10790.
Zhang, R. Y.; and Lavaei, J. 2021. Sparse semideﬁnite pro-
grams with guaranteed near-linear time complexity via du-
alized clique tree conversion. Mathematical programming,
188(1): 351–393.
Zhang, R. Y.; Sojoudi, S.; and Lavaei, J. 2019. Sharp Re-
stricted Isometry Bounds for the Inexistence of Spurious Lo-
cal Minima in Nonconvex Matrix Recovery. Journal of Ma-
chine Learning Research, 20(114): 1–34.
Zhang, X.; Wang, L.; Yu, Y.; and Gu, Q. 2018. A Primal-
Dual Analysis of Global Optimality in Nonconvex Low-
Rank Matrix Recovery. In Proceedings of the 35th Interna-
tional Conference on Machine Learning, volume 80 of Pro-
ceedings of Machine Learning Research, 5862–5871.
Zhang, Y.; Madani, R.; and Lavaei, J. 2017. Conic relax-
ations for power system state estimation with line measure-
ments. IEEE Transactions on Control of Network Systems,
5(3): 1193–1205.
Zhu, Z.; Li, Q.; Tang, G.; and Wakin, M. B. 2018. Global
Optimality in Low-Rank Matrix Optimization. IEEE Trans-
actions on Signal Processing, 66(13): 3614–3628.
Zhu, Z.; Li, Q.; Tang, G.; and Wakin, M. B. 2021. The
global optimization geometry of low-rank matrix optimiza-
IEEE Transactions on Information Theory, 67(2):
tion.
1308–1331.

A Appendices

∈

Rn×n be the rank-r unperturbed ground truth m

Proof of Theorem 2
Proof. Let M∗
×
Hence, the ground truth matrix can be factorized as M∗ = X∗(X∗)T , X∗
∈ S1 and is 0r×r otherwise. We perturb the ground truth matrix by ǫ
i
X∗(ǫ)i = X∗
If all diagonal blocks are observed, then it will reduce to a feasibility problem and we can skip the following procedure. Since
∈ E1. Consider the
∈ S1 and j

r.
Rr×r is rank-r if
Rn×r, where M∗(ǫ) = (X∗ + ǫ)(X∗ + ǫ)T such that
i
∀
such that (i, j)

∈ S1 and X∗(ǫ)i = ǫi otherwise. Here, we assume that rank(X∗
6∈ S1 with (j, j)

m block matrix with each block having dimension r

S1 is maximal independent set, there exists two indices i
2
×

2 block sub-matrix with i-th and j-th block columns and rows:

Rn×r. Each square factor X∗

i + ǫi) = r,

i ∈
[m].

i + ǫi if i

6∈ E

×

∈

∈

∈

(cid:20)
The equality holds because the blocks (i, i) and (i, j) are in
full-rank, the above constraint is equivalent to

(cid:21)

(cid:20)

(cid:23)
(cid:21)
E1. By the Schur complement argument and since X∗

i + ǫi is

Mi,i Mi,j
Mj,i Mj,j

=

(X∗

i + ǫi)(X∗
ǫj(X∗

i + ǫi)T

i + ǫi)T

(X∗

i + ǫi)ǫT
j
Mj,j

0.

Mj,j (cid:23)
The unique trace minimizer for the diagonal blocks is Mj,j = ǫjǫT
in the independent set without a self-loop. Thus, the objective value cannot be less than
the minimum value is achieved whenever M∗

ǫjǫT
j .

j . By the same argument, this must hold for every j that is not
n
i=1 tr(X∗(ǫ)i(X∗(ǫ)i)T ). Therefore,

We now prove the uniqueness of the solution. Since the graph is connected, there exists a node k adjacent to the node j such

i,i = X∗(ǫ)i(X∗(ǫ))T . This makes M(ǫ)∗ an optimal solution.

P

that the edges (i, j) and (j, k) exist in the graph. Consider the 3

3 block submatrix

Mi,i Mi,j Mi,k
Mj,i Mj,j Mj,k
Mk,i Mk,j Mk,k#

"

=



X∗(ǫ)iX∗(ǫ)T
X∗(ǫ)jX∗(ǫ)T
Mk,i

×
i X∗(ǫ)iX∗(ǫ)T
j
j X∗(ǫ)jX∗(ǫ)T
i X∗(ǫ)jX∗(ǫ)T
k
j X∗(ǫ)kX∗(ǫ)T
X∗(ǫ)kX∗(ǫ)T
k

Mi,k

This is equivalent to following constraints by Schur complement



0.



(cid:23)



X∗(ǫ)iX∗(ǫ)T
X∗(ǫ)j X∗(ǫ)T

(cid:20)

i X∗(ǫ)iX∗(ǫ)T
j
i X∗(ǫ)jX∗(ǫ)T
j (cid:21)
Mi,kX∗(ǫ)−T

−

Mi,kX∗(ǫ)−T

X∗(ǫ)jX∗(ǫ)−1
k Mk,i

k X∗(ǫ)−1
k Mk,i
(X∗(ǫ)i −

(cid:20)
k X∗(ǫ)−1
k )T

Mi,kX∗(ǫ)−T

i −
X∗(ǫ)j(X∗(ǫ)i −

X∗(ǫ)iX∗(ǫ)T

k Mk,i Mi,kX∗(ǫ)−T

k X∗(ǫ)T
j
X∗(ǫ)jX∗(ǫ)T
j
k )X∗(ǫ)T
j

Mi,kX∗(ǫ)−T
0

(cid:20)
Another Schur complement argument gives

0,

0.

(cid:23)

(cid:23)

(cid:21)

(cid:21)

(X∗(ǫ)i −

Mi,kX∗(ǫ)−T

k )X∗(ǫ)T

j = 0.

j is full rank, we have X∗(ǫ)i −

Since X∗(ǫ)T
k . Note that ﬁlling
out the unobserved non-diagonal blocks is equivalent to adding the edge (i, k) to the graph
G1. Hence, we can always ﬁnd
such triple (i, j, k) deﬁned as above until ﬁlling out all missing entries. As a result, we obtain the unique solution M∗(ǫ) by
continuing iteratively.

k = 0. Thus, we obtain Mi,k = X∗(ǫ)iX∗(ǫ)T

Mi,kX∗(ǫ)−T

Proof of Proposition 2
Proof. To prove Proposition 2, we study intermediary problem.

min
δ, ˆH

s.t.

where

and P

∈

Rn×2r is deﬁned to be

δ

ˆeT ˆHˆe

(1

−

(1 + δ)
k
ˆH

≤
δ)I4r2

(cid:22)

2 + 2(l

eck
−
(1 + δ)I4r2 ,
(cid:22)

3)δ

2

eck

k

(8)

ˆe = PT e,

Rn2×4r2

P

∈

= P

P,

⊗

P = [u1 u2
M so that P T P = I. Denote the optimal solution to (8) as δP (e). Then, the

. . . u2r,]

where ui’s are orthonormal eigenvectors of M∗
following two lemmas will sufﬁce to prove Proposition 2.

−

Lemma 2. Given a ﬁxed vector e

Rn2, we have

∈

Lemma 3. Given a ﬁxed vector e

Rn2, we have

∈

δP (e)

δ(e).

≤

(9)

(10)
≤
Proof of Lemma 2. It sufﬁces to show that for any feasible pair (δ, ¯H) of (6), we can construct a feasible solution (δ, ˆH) to (8)
characterized as below

δP (e).

δlb(e)

δ = δ,

ˆH = PT ¯HP,

which directly proves the lemma. We can verify the feasibility of (δ, ˆH) as follows. The feasibility of the ﬁrst constraint is
certiﬁed by the following argument:

ˆeT ˆHˆe = eT PPT ¯HPPT e,

By the deﬁnition of P, one can write

PPT e = (P P T

Since eT ¯He = 0 and ¯H is symmetric, ¯H admits a factorization ¯H = ¯AT ¯A, making ¯Ae = 0. Also, we know that e = e2r + ec,
meaning that

P P T )e = vec(P P T (M∗

M)P P T ) = e1 + e2 = e2r,

⊗

−

Therefore,

¯Ae2r =

¯Aec.

−

ˆeT ˆHˆe = eT
2r

¯He2r = eT
c

¯Hec = (

ei)T ¯H(

ei).

l

l

Since ¯H satisﬁes δ2r-RIP, for every (i, j) such that i

= j, we have:

i=3
X

i=3
X

≤
where the last equality follows from the facts that eT
i ej = 0 and

(ei + ej)T ¯H(ei + ej)

(1 + δ)
k

ei + ejk

2 = (1 + δ)(
k

eik

2 +

ejk

k

2),

(ei + ej)T ¯H(ei + ej) = eT
i

¯Hei + 2eT
i

¯Hej + eT
j

Combining (11) and (12) yields that

Therefore,

eT
i

¯Hej ≤

δ(
k

eik

2 +

k

¯Hej ≥
2)

ejk

2eT
i

¯Hej + (1

δ)(
k

eik

−

2 +

2).

ejk

k

= j.

i
∀

l

l

l

ˆeT ˆHˆe = (

ei)T ¯H(

ei)

i=3
X

i=3
X

≤

l

(1 + δ)(

i=3
X
eck
= (1 + δ)
k

k

eik
2 + 2δ(l

2) + 2δ(l

3)(

−
eck

i=3
X

2.

3)
k

−

2)

eik

k

(11)

(12)

The above inequality directly veriﬁes the satisfaction of the ﬁrst constraint. For the second constraint, consider an arbitrary
vector ˜e

. Then,

R4r2

∈

˜eT ˆH˜e = ˜eT PT ¯HP˜e = ˜eT (P T

P T ) ¯H(P

P )˜e

⊗

⊗

= vec(P mat(˜e)P T )T ¯H vec(P mat(˜e)P T ).

By orthogonal projection, we know that P mat(˜e)P T
property of ¯H:

∈

Rn×n has rank 2r. Therefore, the following holds by the δ2r-RIP

(1

δ)
k

−

P mat(˜e)P T

2
F ≤

k

vec(P mat(˜e)P T )T ¯H vec(P mat(˜e)P T )

(1 + δ)
k

≤

P mat(˜e)P T

2
F

k

(13)

and since

P mat(˜e)P T

k

k

2
F = tr(P mat(˜e)T P T P mat(˜e)P T )
= tr(P mat(˜e)T mat(˜e)P T )
= tr(P T P mat(˜e)T mat(˜e))
= tr(mat(˜e)T mat(˜e))
=

2
2,

˜e
k

k

(13) automatically implies the satisfaction of the second constraint.

6
6
Proof of Lemma 3. It sufﬁces to show that for any feasible pair (δ, ˆH) of (8), we can construct a feasible solution (δ, H) to (7)
characterized as

To prove the lemma, it is enough to verify that the above pair (δ, H) is feasible to (7). We ﬁrst verify the second constraint.
Given an arbitrary vector e

, we have that

Rn2

δ = δ,

H = P ˆHPT + (1

δ)(In2

PPT ).

−

−

∈

eT He = eT P ˆHPT e + (1

δ)

eT e

−

−

eT PPT e

(cid:2)

(cid:3)

and deﬁning ˜e := PT e

R4r2

, we obtain:

∈

eT P ˆHPT e + (1

Also, since

˜e
k

2
2 ≤ k

k

e

k

which further implies that

eT P ˆHPT e + (1

δ)

eT e

δ)
k
2
2 and P is a projection matrix, one can write:

eT PPT e

(1

−

−

−

≥

˜e

(cid:3)

(cid:2)

2
2 + (1

δ)[

e

2
2 − k

k

˜e

k

k

−

2
2] = (1

e

δ)
k

k

2
2.

−

k

(1 + δ)[

e

2
2 − k

˜e
k

2
2]

k

k

≥

(1

δ)[

e

2
2 − k

˜e
k

k

k

2
2],

−

δ)

eT e

eT PPT e

−

−

(1 + δ)
k

˜e

k

≤

2
2 + (1 + δ)[

e

2
2 − k

k

˜e

2
2] = (1 + δ)
k

k

k

e

k

2
2.

Combining the above equations, we recover the second constraint of (7):
(cid:2)

To study the ﬁrst constraint, we have that

(1

e

δ)
k

−

(cid:3)
2
2 ≤

k

eT He

(1 + δ)
k

e

k

2
2.

≤

Note that

e

2
2 − k

ˆe
k

k

k

2
2 =

eck

k

2
2 due to

δ)

eT He = ˆeT ˆHˆe + (1
−
k
2
eck
(1 + δ)
2 + 2(l
(cid:2)
k
2
eck
3)δ
2 + 2(l

≤
= 2

−

k

e

k

2
2 − k
3)δ
k
−
2
eck
2.

k

2
2
2
2 + (1
(cid:3)

ˆe
k
eck

δ)
k

eck

2
2

−

2
2 =

e

k

k

λ2
i ,

n

i=1
X

2
2 =

ˆe
k

k

λ2
i .

2r

i=1
X

The proof of Proposition 2 follows directly from combining Lemma 2 and 3.

Proof of Lemma 1
Proof. We aim to solve (7) analytically to obtain a sufﬁcient RIP bound for problem (2). This amounts to deriving a closed-form
expression for δlb(e). We consider a simpler problem to solve (7):

max
η, ˜H

η

s. t. eT ˜He

1

η

c2 +

1 + η
2

d2

−
2

ηIn2

≤
˜H

(cid:22)

(cid:22)

In2

1
δ
−
1 + δ

,

1
1 + δ

H

(cid:19)

(cid:18)

with c2 = 2(l

3)
k

eck

−

2 and d2 = 2
2

eck

k

2
2. Given any feasible solution (δ, H) to (7), the tuple

is a feasible solution to problem (14). Therefore, if we denote the optimal value of (14) as η(e), then it holds that

η(e)

δlb(e)
1
1 + δlb(e)

−

≥

=

⇒

δlb(e)

η(e)
1
1 + η(e)

−

.

≥

We use the dual problem to solve for η(e):

min
U1,U2,γ

s. t.

tr(U2) +

(c2 + d2)

γ
2
γ
2

(c2
tr(U1) +
γeeT = U1 −

d2) = 1

−
U2, U1, U2 (cid:23)

0, γ

0.

≥

(14)

(15)

(16)

Since Slater’s condition holds for the convex program (14), the optimal solution to (16) is equivalent to that of (14), which is
η(e). Using a Lagrangian argument, η(e) can be solved as follows:

(c2

−

d2)) + γ

c2 + d2
2

+

min
U1(cid:23)0
U1−γeeT (cid:23)0 (cid:2)

tr(U1 −

γeeT )

−

β tr(U1)






(cid:3)



(cid:3)


β) tr(U1)

γ

e

2
2

k

k

−

−

(1

min
U1(cid:23)0
U1−γeeT (cid:23)0 (cid:2)
2
2 −

β)
k

−

e

k

+ γ(1

γ

e

2
2

k

k

(cid:27)

η(e) = max
β∈R

= max
β≤1

min
γ≥0 


β(1

−


min
γ≥0 


β(1

−

γ
2

γ
2

(c2

−

d2)) + γ

c2 + d2
2

+

d2)) + γ

c2 + d2
2
d2

+

c2

β(

−
2

(c2

−
c2 + d2
2

c2

β(

−
d2

−
2

+

2
2)

e

k

k

≥

2
2))

(cid:21)(cid:27)

e

k

k

0

(cid:27)

= max
β≤1

min
γ≥0

γ
2

−

β(1


(cid:26)

= max
β≤1

= max
β≤1

= min

(cid:26)

(cid:26)

γ(

β :

β + min
γ≥0
(cid:20)
c2 + d2
2
−
c2 + d2
2
2 + c2

e

2

1,

k

k

(cid:26)

,

d2

(cid:27)

−

where the ﬁrst equality uses the Lagrangian argument by introducing the Lagrange multiplier β, and the second equality con-
β) tr(U1) will be unbounded otherwise. The third equality results from the obvious choice of
straints β
1 since (1
−
≤
U1 = γeeT given that (1
β) is nonnegative. The ﬁfth equality results from the choice of γ = 0 constrained to the require-
−
ment that its coefﬁcient must be nonnegative.

Substituting η(e) = 1 into (15) results in the trivial lower bound δlb(e) of 0, which means that the lower bound indeed will
c2+d2
2+c2−d2 from now on. We know from (15) that in order to obtain a lower bound

not be negative. Hence, we will focus on
on δlb, we need to derive an upper bound on η(e). Note that

2kek2

c2 + d2
2
2 + c2

−

2

e

k

k

d2 =

The last equality follows from the relations

2
2

eck
2)
2(l
k
−
2
4)
2 + 2(l
k
−

k

=

2
eck
(l
2)
2
k
−
2
e2rk
3)
2 + (l
k
−

k

.

2
2

eck

2
2

eck

(17)

e

2

k

If we ﬁx

e2rk

k

2
2, then we can maximize (17) with respect to

2
2 =

e

k

k

k

ec + e2rk
k

k

2
2.

2
2 +

eck

e2rk

2
2 =
k
2
eck
2 ﬁrst. In this case, taking the derivative of (17) yields that
= 2

∂
eck2 (cid:18)
Therefore, (17) is maximized when
k
2
2 and
terms of

2
eck
(l
2)
2
k
−
2
e2rk
3)
2 + (l
k
k
−
2
eck
2 is set to be as large as possible. Before we derive the maximum value of
2
2, we introduce one key lemma.

e2rk
eck2k
eck
3)
k
−

2)
(l
−
k
2
e2rk
2 + (l

2
2
2
2)2 ≥

eck

2
2 (cid:19)

(
k

0.

∂

k

e1k

k

e2k

k

Lemma 4. Consider two PSD matrices M and M∗ such that tr(M)

tr(M∗) and rank(M∗) = r. Then,

σ(1)(M∗

M) +

+ σ(r)(M∗

M)

≥
−
where σi denote the i-th largest singular value of the matrix M∗
Proof. For each matrix A, we denote the ith eigenvalue as λ(i)(
·
λ(2)(A)

λ(1)(A)

· · ·

−

≤
σ(r+1)(M∗
M.

−
), meaning that

≥

λ(n)(A).

≥ · · · ≥

M) +

−

· · ·

σ(n)(M∗

M),

−

eck

k

2
2 in

(18)

By Weyl’s inequality, we know that

Hence,

since M∗ is of rank-r and M
M)
0, it holds that

(cid:23)

≥

λ(i+j−1)(M∗

M)

−

≤

λ(i)(M∗) + λ(j)(

−

M).

λ(r+1)(M∗

M)

−

≤

λ(r+1)(M∗) + λ(1)(

M)

−

0

≤

0. Therefore, we know that M∗

M has at most r positive eigenvalues. Also, since tr(M∗

−

−

λ(1)(M∗

M) +

−

· · ·

+ λ(r)(M∗

M)

−

≥ −

λ(r+1)(M∗

M)

−

− · · · −

λ(n)(M∗

M),

−

which implies that

since λ(k)(M∗

|

−

λ(1)(M∗

M)
|

+

+

λ(r)(M∗

M)

λ(r+1)(M∗

−

−
0 for all k > r. According to the deﬁnition, we have

| ≥ |

· · ·

−

|

M)
|

+

+

|

· · ·

λ(n)(M∗

M)
|

−

(19)

M)

≤
λ1(M∗
−
λ(r+1)(M∗

|

|

−
M), . . . , λn(M∗

|

+

· · ·
+

λr(M∗
+

M)
+
|
M)
|
M) are ordered with respect to their absolute values. As per the main text, we abbreviate

M)
−
|
λr+1(M∗

M)
−
λ(n)(M∗

M)
,
|
M)
|

−
λn(M∗

· · ·
M)
|

| ≥ |
M)

|
+ . . .

λ(1)(M∗

λ(r)(M∗

| ≥ |

(20)

· · ·

+

+

−

−

−

|

|

[n]. Combining (19) with (20) proves the original lemma.

since λ1(M∗
λi(M∗

−

−

−
Denote S1 :=

M) as λi for the sake of brevity for any i
λi|

n
i=r+1 |

and S2 :=

r
i=1 |

λr|
maximized when every absolute value is chosen to be as large as possible, namely
P
λr+⌊S2/|λr |⌋|

λr+⌈S2/|λr |⌉|

= S2 − ⌊

λi| ≤ |

λr+1|

λr|

, . . . ,

λi|

P

∈
. Given S2, since

=

,

|

|

|

|

|

as long as i > r, we know that

S2/

λr|⌋|

λr|

|

:= ˜λ

.

λr|

≤ |

Therefore,

As a result,

n

i=r+1
X

λ2
i ≤ ⌊

S2/

λr|⌋

|

r + ˜λ2
λ2

S2/

λr|⌋

|

≤ ⌊

λ2
r +

˜λ
λr|

|

λ2
r =

λ2
r.

S2
λr|

|

S1|
where the last inequality follows from Cauchy-Schwartz. Combining the above 2 inequalities, we obtain

λr| ≤

λr| ≤

S1

=

k

k

e1k

λ2
r = S2|

2
2,

|

S1
r ≤

S2
1
r ≤

r

2
2

e1k
r

S2
λr|

n

i=r+1
X

λ2
i ≤ k

e1k

2
2.

Furthermore, since

λr+1| ≥ · · · ≥ |

λn|

|

, one can write:

n

2
2 =

eck

k

λ2
i ≤

i=2r+1
X

2r
r

n
n

−
−

n

λ2
i

i=r+1
X

with equality holding if and only if

λr+1|

|

=

· · ·

=

|

. Combined with (21), we obtain

λn|
eck

k

2
2 ≤

2r
r k

2
2.

e1k

n
n

−
−

Consequently,

k
It results from (22) and (23) that

eck

2
2 ≤

n
n

2r
r

(
k

e2k

2
2 +

eck

k

2
2) =

⇒ k

e2k

2
2 ≥

n

r

−

2
2.

eck

2r k

−
−

max
e:tr(M)≤tr(M ∗)

η(e) =

max
e:tr(M)≤tr(M ∗)

2
eck
(l
2)
2
k
−
2
e2rk
2 + (l
3)
k
k
−
2
eck
2)
2
k
−
2
eck
2 + (l
n−2r k
2) n−2r
n−r

3)
k

−

2
2

eck

2
2

eck

≤

e1k

k

(l
2 + r
2
(l
−
r
n−2r + l
2)(n

Thus,

≤

=

3) n−2r
n−r

1 + (

(l
−
n + (n

−
2r)

−

−
2r)(l

.

3)

−

δlb

max
e:tr(M)≤tr(M ∗)

≥

δ(e)
1
1 + δ(e)

−

=

2r
2r)(2l

.

5)

−

n + (n

−

n
i=r+1 λ2

i is

P

(21)

(22)

(23)

Proof of Theorem 5
Proof. The matrix completion problem with the least-squares objective function can be written as

n−1

min
x∈Rn

f (x) = (x2

1 −

(x∗

1)2)2 + 2

(xixi+1 −

i x∗
x∗

i+1)2.

(24)

i=1
X
= 0,




∈

i
∀

We assume that every element of the vector x∗ is nonzero, i.e. x∗
[n]. In this case, the problem (24) does not have
i 6
any spurious solutions. In order to prove the inexistence of a non-global second-order critical solution, we need to investigate
the second-order necessary optimality conditions. The gradient and Hessian of the above objective can be written as
(x2
1)2)x1 + (x1x2 −
(x∗
1 −
i−1x∗
x∗
(xi−1xi −
n−1x∗
x∗
(xn−1xn −
1)2 + x2
3x2
(x∗
2,
1 −
i−1 + x2
x2
i+1,
x2
n−1,
2xixj −
0,

x∗
1x∗
2)x2,
i )xi−1 + (xixi+1 −
n)xn−1,

if i, j = 1
if i = j, i, j
if i, j = n
if
|
−
otherwise

if i = 1
if i
6∈ {
if i = n

2f (x)]i,j =

i+1)xi+1,

f (x)]i =

i x∗
x∗
j ,

i x∗
x∗







= 1

1, n

1, n

6∈ {

∇

∇

}

}

j

i

[

|

,

[

Note that for the point ˆx to be a second-order stationary point, we require
either ˆxn−1 ˆxn = x∗
semideﬁnite:

n or ˆxn−1 = 0. The latter results in

n−1x∗

2f (ˆx)

∇

6(cid:23)

f (ˆx) = 0 and

f (ˆx)]n = 0 implies
∇
(cid:23)
0 since the following principal minor is not positive

0. [

∇

∇

2f (ˆx)

2f (ˆx)]n−1:n,n−1:n =

[

∇

(ˆxn−2)2 + (ˆxn)2
n−1x∗
x∗
2ˆxn−1 ˆxn −
n
(ˆxn−2)2 + (ˆxn)2
n−1x∗
x∗
n

−

(cid:20)

=

(cid:20)

2ˆxn−1 ˆxn −

n−1x∗
x∗
n
(ˆxn−1)2

(cid:21)

n−1x∗
x∗
n
0

−

0.

6(cid:23)

(cid:21)

n = x∗

n is the only feasible option for ˆx to be a second-order critical point. Next, we show that no second-

= 0 for all i
The ﬁrst goal is to identify the structure of the stationary points with at least one zero entry. If ˆx1 = 0, [

n−1x∗
Hence, ˆxn−1 ˆx∗
order critical point ˆx can have a zero entry, i.e. ˆxi 6
ˆx2 = 0. Then, [
which cannot be a second-order critical point. Similarly, if ˆxn = 0, then [
ˆxn−2 = 0. Hence, the only stationary point with ˆxn = 0 is ˆx = 0. Consequently, ˆx1 6
stationary point.

f (ˆx)]1 implies
f (ˆx)]2 gives ˆx3 = 0. Continuing iteratively gives that the only possible stationary point with ˆx1 = 0 is ˆx = 0,
f (ˆx)]n−1 gives
= 0 for a second-order

f (ˆx)]n implies ˆxn−1 = 0. Then, [

= 0 and ˆxn 6

[n].

∇

∇

∇

∇

∈

k−1. Continuing iteratively backwards on [

Let k be deﬁned as an index with the property that ˆx1, ˆx2, . . . , ˆxk−1 6
x∗
k−2x∗
i
≤
≤
critical points, which are correct values for the corresponding edges:
ˆxk−2 ˆxk−1 = x∗
ˆxk−3 ˆxk−2 = x∗

f (ˆx))]i for 1

k−2x∗
k−3x∗

∇

k−1,
k−2,

k

−

= 0 and ˆxk = 0. Then, [

f (ˆx)]k−2 yields ˆxk−2 ˆxk−1 =
2 gives the following conditions on second-order

∇

...
1x∗
ˆx1 ˆx2 = x∗
2,
1)2.
(ˆx1)2 = (x∗

We can focus on entries of the stationary point ˆx corresponding to ˆxi for i > k. We show that ˆxk+1 6
every second-order critical point. If ˆxk+1 = 0, then [
∇
= 0, we have k
ground truth matrix. Since ˆxn−1, ˆxn 6
≤
submatrix of the Hessian will have the form

= 0 for
k = 0, which contradicts the assumption on the
2

2 for each second-order critical point. If ˆxk+2 = 0, then the 2

f (ˆx)]k = 0 gives x∗
n

= 0 and ˆxk+2 6

k−1x∗

×

−

2f (ˆx)]k:k+1,k:k+1 =

[

∇

(ˆxk−1)2 + (ˆxk+1)2
2ˆxk ˆxk+1 −
k+1
(ˆxk−1)2 + (ˆxk+1)2
kx∗
x∗

kx∗
x∗

k+1

−

kx∗
x∗
2ˆxk ˆxk+1 −
k+1
(ˆxk)2 + (ˆxk+2)2
x∗
kx∗
0

k+1

0.

−

6(cid:23)

(cid:21)

(cid:21)

=

(cid:20)

(cid:20)

Let m be deﬁned as an index with the property that ˆxk+1, ˆxk+2, . . . , ˆxm 6
previous arguments and the deﬁnition of m, we have the condition k + 3

= 0 and either ˆxm+1 = 0 or m = n. By the
n. We are not interested in the entries

m

≤

≤

after the m-th entry because we can show that a stationary point with this structure cannot be a second-order critical point
2f (ˆx)]1:m,1:m 6(cid:23)
because [
0. By using the ﬁrst-order partial derivatives and algebra, we obtain the following equations for
∇
the stationary point ˆx:

[

[

...

[

f (ˆx)]k = 0

∇

ˆxk+1 =

−→

−

(cid:16)

f (ˆx)]k+1 = 0

∇

ˆxk+2 =

−

−→

x∗
k−1
x∗

k+1
(cid:17)
x∗
k−1
x∗

k+1

(cid:16)

2

x∗
k+1 =

αx∗

k+1,

−

−2

x∗
k+2 =

α−1x∗

k+2,

−

(cid:17)

f (ˆx)]m−1 = 0

∇

ˆxm =

−→

x∗
k−1
x∗

k+1

−

(cid:16)

2(−1)(m−k+1)

(cid:17)

x∗
m =

−

α((−1)m−k+1)x∗
m,

. As a result, the possible candidates for second-order critical points with at least one zero entry have the

2

∗
k−1

x
x∗

k+1

(cid:16)

(cid:17)

ˆx = [x∗

1, x∗

2, . . . , x∗

k−1, 0,

αx∗

k+1,

−

−

α−1x∗

k−2, . . . ,

−

α((−1)m−k+1)x∗

m, . . . ]

where α =
form:

or the form

ˆx = [

x∗
1,

x∗
2, . . . ,

−

−

−

k−1, 0, αx∗
x∗

k+1, α−1x∗

k−2, . . . , α((−1)m−k+1)x∗

m, . . . ].

These points correspond to the same matrix completion and they lead to the same Hessian matrix. As mentioned before, we
focus on the m

m Hessian submatrix [

2f (ˆx)]1:m,1:m:

×

∇

[

2f (ˆx)]1:m,1:m =

∇



D

E
DT A B

,

BT C

R(k−2)×(m−k+2) and E

R3×(m−k−1), C

∈

∈

R(m−k−1)×(m−k−1), D



∈

R(k−2)×(k−2). The submatri-

∈

(x∗

k−1x∗
x∗
k
−
(x∗
k−1)4
k−1)2 +
(x∗
k+1)2
kx∗
x∗

k+1

0
kx∗
x∗
−
k+1)4(x∗
(x∗
(x∗
k−1)4

k+1
k+2)2



,





R3×3, B
where A
∈
ces can be written as
k−2)2
(x∗
k−1x∗
x∗
k

A = 

−

,

0
0
0

k+3)2

B =

C =

0

0
0
k+1x∗
x∗
α2(x∗

−
0
. . .
0
. . .
0
. . .
k+2
k+1)2 + α2(x∗
k+2x∗
x∗
0
...
0
0

k+3




















α−2(x∗

k+4)2

α2(x∗

k+3

k+2x∗
x∗
k+2)2 + α−2(x∗
k+3x∗
x∗
...
0
0

k+4

k+4

0
k+3x∗
x∗
k+3)2 + α2(x∗
...
0
0

k+5)2

. . .
. . .
. . .
. . .
. . .
. . .

0
0
0
...
m−1x∗
x∗
m
(α2)((−1)m−k)(x∗

m−1)2

,












,



k−1

0
...
0

0
...
0

D = 

0
. . .
...
. . .
x∗
k−2x∗
. . .


2f (x∗)]1:k−2,1:k−2.
E = [
∇
We investigate three different cases: A
2f (ˆx)]1:m,1:m 6(cid:23)
then [
∇
If A
0 with an eigenvalue equal to 0, we consider the following 4
(cid:23)
(x∗

0 0

0, A




(cid:23)

6(cid:23)

k−1)2 x∗

k−2x∗

k−1

0,
0 because A is a principal minor of the Hessian. Therefore, ˆx cannot be a second-order critical point.

0 with at least one of the eigenvalues being equal to 0, and A

0. If A

6(cid:23)

≻

4 principal minors of the Hessian:

k−3)2 + (x∗
k−2x∗
x∗
k−1
0
0






×

A

A

,











0 0 x∗

k+1x∗

k+2 α2(x∗

0
0
k+1x∗
x∗
k+1)2 + α2(x∗

k+2

k+3)2

.






The submatrices are not positive semideﬁnite by Schur complement if

A

−

and

1
k−3)2 + (x∗

(x∗

k−1)2 


k−1)2

(x∗

k−2)2(x∗
0
0

0
0
0

0
0
0


0

6(cid:23)

A

1
k+1)2 + α2(x∗

0 0
0 0
0 0

0
0
k+1)2(x∗

0,

α2(x∗

−

(x∗

k+3)2 


k+2)2

k−1)2 +
respectively. If Av = 0, then v1 and v3 cannot be equal to 0 at the same time because (x∗

(x∗
k−1)4
k+1)2 > 0. If v1 6
(x∗
k−1)2
k−1)2 < 0.
= 0 and we can use the second submatrix
Hence, the quadratic form of the Hessian has a descent direction. If v1 = 0, then v3 6
to show that the submatrix is not positive semideﬁnite. Thus, ˆx cannot be a second-order critical point. As a result, if ˆx is a
second-order critical point, then A
0. In that case, we can show that the submatrix

v2
k−2)2(x∗
1(x∗
k−3)2 + (x∗
(x∗

1
k−3)2 + (x∗

k−2)2(x∗
0
0

0 0
0 0
0 0


k−1)2 


= 0, then

vT Av

k−1)2

v = 0

(x∗

(x∗

vT

−

6(cid:23)

−

≻

cannot be positive semideﬁnite. This requires proving that C
3,3(x∗

A−1

BT A−1B = 




3,3 by using the cofactors of A as

We can calculate A−1

0. Note that

A B
BT C

(cid:20)

(cid:21)
BT A−1B
k+2)2

−

k+1)2(x∗
0
...
0

6(cid:23)
0 . . .
0 . . .
...
. . .
0 . . .

.

0
0

...

0




(x∗

k−1)2(x∗

k)2

,

(cid:17)

where

A−1

3,3 =

1
det(A)

(cid:16)

(x∗

k−2)2(x∗

k−1)2 + (x∗

k−2)2 (x∗
(x∗

k−1)4
k+1)2 −

det(A) = (x∗

k−2)2

(cid:16)

k+1)4
(x∗
k−1)2 (x∗
(x∗

k+2)2 + (x∗

k+1)2(x∗

k+2)2

(x∗

k)2(x∗

k+1)2

−

(x∗

k+1)4(x∗
k)2(x∗
(x∗
k−1)2

k+2)2

.

−

(cid:17)

An algebraic manipulation shows that
A−1
3,3(x∗
x∗
k. We can write the matrix C

k+2)2 is equal to α2(x∗

k+1)2(x∗

dA−1
3,3
d(x∗
k+1)2. As a result, A−1
3,3(x∗
BT A−1B as the summation of m
k+1)2

k)2 > 0. Thus, the value of A−1
k+1)2(x∗
k
−
−
k+2)2
k+1)2(x∗

α2(x∗

−

3,3 is minimized when (x∗

k)2

k)2 = 0,
k+1)2 by the non-zero assumption on

0. Whenever (x∗

−→

k+2)2 > α2(x∗
1 matrices as

BT A−1B =

C

−


























k+3)2

α2(x∗
k+2x∗
x∗
...
0
0
0 0
0 0
...
...
0 0
0 0

. . .
. . .
. . .
. . .
. . .

−

A−1
3,3(x∗
0
...
0
0
k+2x∗
x∗
k+3 α−2(x∗
...
0
0

k+3
k+2)2

0
0
...
0
0

. . .
. . .
. . .
. . .
. . .

+

0 0
0 0
...
...
0 0
0 0









. . .
. . .
. . .
. . .
. . .

0 0
0 0
...
...
0 0
0 0









+

+

· · ·

0
0
...

(α2)(−1)m−k+1
m−1x∗
x∗
m

(x∗

m)2

0
0
...
m−1x∗
x∗
m
(α2)(−1)m−k
(x∗

m−1)2

.










Consider the vector v deﬁned as v2t+1 =

x∗
k+2t+2
x∗

k+2

and v2t = α2 x∗
k+2t+1
x∗

k+2

for t = 0, . . . ,

(m

⌊

−

k

−

1)/2

⌋

. Then,

vT (C

−

BT A−1B)v = α2(x∗

k+1)2

−

A−1

3,3(x∗

k+1)2(x∗

k+2)2 + 0 +

+ 0 < 0.

· · ·

As a result, C
cannot have a second-order critical point ˆx with ˆxi = 0 for some i
1)2 and ˆxi ˆxi+1 = x∗
must satisfy (ˆx1)2 = (x∗
completion of the matrix M∗.

BT A−1B is not positive deﬁnite. Hence, the Hessian is not positive deﬁnite either. Consequently, the problem
[n]. Then, the only possible second-order-critical points
∈
1, which correspond to the valid factors of the ground truth

i+1, i = 1, . . . , n

i x∗

−

−

Proof of Theorem 6
Proof. To show that the SDP formulated as (2) fails to solve the problem, consider two indices j, k such that x∗
j and
j, k > 2. Then, we construct a feasible solution ˆM that is strictly better than the ground truth solution, which shows the failure
of SDP. Let ˆM = yyT + zzT , sum of two rank-1 matrices, where

k > x∗

and

yi = x∗
i ,

i
∀

[n]

k

,

}

\{

∈

yk = x∗
j

zi = 0,

i
∀

∈

[n]

j, k

,

}

\{

zj = zk = (
|

x∗
j x∗

k| −

(x∗

j )2)1/2.

Since the sum of PSD matrices is PSD and ˆM satisﬁes the observed entries, ˆM is a feasible solution. Moreover, the objective
. Hence, by the assumption, the feasible solution ˆM is strictly better
value corresponding to ˆM is
than the ground truth solution M∗. Thus, SDP fails to recover the true solution.

i6=j,i6=k(x∗

j x∗
x∗
k|

i )2 + 2

|

P

Proof of Theorem 7

Proof. Note that the nodes are numbered from 0 to 2k as opposed to earlier examples. Hence, the matrix completion problem
with the least-squares objective function can be written as

min
x∈Rn

f (x) = 2

2k

i=0
X

(xixi+1 −

i x∗
x∗

i+1)2 + 2(x0x2k −

0x∗
x∗

2k)2.

(25)

The gradient and Hessian of the above objective can be written as

1)x1,
i x∗
x∗
2k)x2k−1 + (x0x2k −

0x∗
x∗
0x∗
x∗
(x0x2k −
2k)x2k + (x0x1 −
i−1x∗
x∗
(xi−1xi −
i )xi−1 + (xixi+1 −
2k−1x∗
x∗
(x2k−1x2k −
x2
1 + x2
2k,
x2
i−1 + x2
0 + x2
x2
2xixj −
0,

if i, j = 0
if i = j, i, j
if i, j = 2k
if
−
|
otherwise

i+1,
2k−1,
i x∗
x∗
j ,

0, 2k

}

.

= 1

6∈ {

j

i

|

i+1)xi+1,
0x∗
x∗

2k)x0,

if i = 0
if i
6∈ {
if i = 2k

0, 2k

,

}

[

f (x)]i =

∇




[

2f (x)]i,j =

∇







The optimization problem (25) does not have any spurious solution ˆx such that ˆxi = 0 for some i = 0, . . . 2k. To prove
this, assume without loss of generality that ˆx0 = 0 for a stationary point ˆx. By the proof of Theorem 5, we know that
= 0 and either ˆxm+1 = 0 or m = n. Thus,
ˆx2k−1, ˆx2k, ˆx1, ˆx2 6
2 < m < 2k

= 0. Let m be deﬁned as an index such that ˆx1, ˆx2, . . . , ˆxm 6

2. One can characterize the stationary points as.

−

[

[
[
[
...
[

f (ˆx)]2k = 0
f (ˆx)]0 = 0
f (ˆx)]1 = 0
f (ˆx)]2 = 0

ˆx2k−1 ˆx2k = x∗
2k ˆx2k = x∗
0x∗
x∗
1x∗
ˆx1 ˆx2 = x∗
2,
2x∗
ˆx2 ˆx3 = x∗
3,

−→

−→ −
−→
−→

∇

∇
∇
∇

2k−1x∗
2k,
0x∗
1 ˆx1,

f (ˆx)]m = 0

∇

−→

ˆxm−1 ˆxm = x∗

m−1x∗
m.

Setting ˆx2k as a free variable gives the following characterization of the stationary points:

ˆx2k−2 =

ˆx2k−1 =

ˆx2k,

x∗
2k−2
x∗
2k
x∗
2k−1x∗
2k
ˆx2k

,

ˆx2k = ˆx2k,
ˆx0 = 0,

ˆx1 =

ˆx2 =

−

−

−

ˆx3 =
...
ˆxm =

1 =

x∗
2k ˆx2k
1)2 x∗
(x∗
1)2
(x∗
x∗
2 =
x∗
2k ˆx2k
αx∗
3,

−

−

αx∗
1,

α−1x∗
2,

α(−1)m−1

x∗
m.

−

Similar to proof of Theorem 5, we focus on the following (m + 2)
[
∇
respective order:

2f (ˆx)](2k−1):m,(2k−1):m where (2k

−

1) : m denotes the rows/columns corresponding to (2k

×

(m + 2) submatrix of the Hessian that is
1), 2k, 0, 1, . . . , m in that

−

where A

∈

R3×3, B
(x∗

R3×(m−1), C

R(m−1)×(m−1), D


R1×(m+2) and E

R. The submatrices can be written as

[

2f (ˆx)](2k−1):m,(2k−1):m =

∇



D

E
DT A B

,

BT C


∈

2k)2

∈
2k−1)2(x∗
(ˆx2k)2
2kx∗
x∗
0
0

−

∈

x∗
2kx∗
0
−
(ˆx2k)2 + (x∗
0x∗
x∗
1

−

2k)2(ˆx2k)2
1)2

(x∗

∈

0
0x∗
x∗
1
−
(x∗
1 )4(x∗
2)2
(x∗
2k)2(ˆx2k)2



,





α−2(x∗

4)2

2x∗
x∗
3
2)2 + α−2(x∗
3x∗
x∗
4
...
0
0

α2(x∗

5)2

0
3x∗
x∗
4
3)2 + α2(x∗
...
0
0

. . .
. . .
. . .
. . .
. . .
. . .

0
0
0
...
m−1x∗
x∗
m
(α2)((−1)m−2)(x∗

m−1)2

,











A = 

B =

"

0
0
1x∗
x∗
2
α2(x∗







,

0
0
0#
3)2

0
0
0

. . .
. . .
. . .
1)2 + α2(x∗
x∗
2x∗
3
0
...
0
0

C =

D =








2k−1x∗
x∗
0 . . .
2k
2k−2)2
(x∗
2k)2 (ˆx2k)2.
(x∗

0

(cid:3)

,

(cid:2)
E = (ˆx2k)2 +

6(cid:23)

0, A

0 with at least one zero eigenvalue, and A

Similar to proof of Theorem 5, we can investigate three different cases to demonstrate that ˆx cannot be a second-order critical
2f (ˆx) cannot be positive
point, which are A
semideﬁnite because A is a principal minor of the Hessian. The second case is when A is positive semideﬁnite but not positive
deﬁnite. In that case, for a vector v that satisﬁes Av = 0, we know that v1 and v3 cannot be zero simultaneously. Consider
2f (ˆx)](2k−1):1,(2k−1):1, which includes the rows/columns corresponding to
following principal minors that correspond to [
2f (ˆx)](2k):2,(2k):2, which includes the rows/columns corresponding to 2k, 0, 1, 2 in that
(2k
order, respectively:

1), 2k, 0, 1 in that order, and [

0. Firstly, if A

∇

∇

∇

0,

6(cid:23)

≻

(cid:23)

−

(x∗
2k−2)2
2k)2 (ˆx2k)2 x∗
(ˆx2k)2 +
(x∗
2k−1x∗
x∗
2k
0
0







2k−1x∗
2k

0 0

A



,










A

0 0 x∗

1x∗

2 α2(x∗

0
0
1x∗
x∗
2
1)2 + α2(x∗

3)2

.






The submatrices are not positive semideﬁnite by Schur complement if

and

If v1 6

= 0, then

A

−

A

−

(ˆx2k)2 +

1
(x∗
2k−2)2
2k)2 (ˆx2k)2 
(x∗


2k)2

(x∗

2k−1)2(x∗
0
0

0 0
0 0
0 0


0

6(cid:23)

0 0
0 0
0 0

0
0
1)2(x∗

(x∗

2)2


0.

6(cid:23)

1
1)2 + α2(x∗

α2(x∗

3)2 

2k)2
2k−1)2(x∗
0
0

(x∗

−

vT

vT Av

2k−1)2(x∗
2k)2
1
(x∗
(x∗
2k−2)2
2k−2)2
2k)2 (ˆx2k)2 
2k)2 (ˆx2k)2
(x∗
(x∗

= 0 and we can use the second submatrix
Hence, the quadratic form of the Hessian has a descent direction. If v1 = 0, then v3 6
to show that the submatrix is not positive semideﬁnite. Thus, ˆx is not a second-order critical point. As a result, if ˆx is a
second-order critical point, then A must be positive deﬁnite. In that case, however, we can show that the submatrix

v2
1(x∗
(ˆx2k)2 +

(ˆx2k)2 +

v = 0

< 0.

0 0
0 0
0 0


−

cannot be positive semideﬁnite, which implies that ˆx cannot be a second-order critical point. We prove the last proposition by
showing that C

0 by using Schur complement idea. Note that

BT A−1B

−

6(cid:23)

A B
BT C

(cid:20)

(cid:21)

BT A−1B = 





2)2

A−1

1)2(x∗
3,3(x∗
0
...
0

0 . . .
0 . . .
...
. . .
0 . . .

0
0
...
0



.






We calculate A−1

3,3 by using the cofactors of A as follows:

A−1

3,3 =

1
det(A)

(x∗

2k−1)2(x∗

2k)2 +

(x∗

2k−1)2(x∗
(x∗
1)2

2k)4

(x∗

2k)2(x∗

0)2

−

,

(cid:17)

where

det(A) =

(x∗

2k−1)2(x∗
(ˆx2k)2

(cid:16)
2k)2

dA−1
3,3
d(x∗

An algebraic manipulation shows that
A−1
1)2(x∗
3,3(x∗
the matrix C

2)2 is equal to α2(x∗

−

BT A−1B as the summation of m
α2(x∗

1)2. As a result, A−1

C

−

BT A−1B = 






(x∗

0)2(x∗

1)2

(x∗

0)2(x∗

1)4(x∗

2)2

.

−

(ˆx2k)2

(cid:17)
3,3 is minimized when (x∗

0)2
0 = 0,
1)2 by the non-zero assumption on u0. We can write

0. Whenever x∗

−→

(cid:16)

−

(x∗

2)2

2)2

−
1)2

3,3(x∗

1)2(x∗

1)2(x∗

1)4(x∗
2)2
2k)2 + (x∗
(x∗
−
0)2 > 0. Thus, the value of A−1
2)2 > α2(x∗
1)2(x∗
1 matrices as
A−1
3,3(x∗
0
...
0
0
x∗
2x∗
3
α−2(x∗
2)2
...
0
0

3)2
α2(x∗
2x∗
x∗
3
...
0
0

. . .
. . .
. . .
. . .
. . .



+

0 0
0 0
...
...
0 0
0 0









+

· · ·

0 . . .
0 . . .
...
. . .
0 . . .
0 . . .

0 0
0 0
...
...
0 0
0 0



+






















0 0
0 0
...
...
0 0
0 0

. . .
. . .
. . .
. . .
. . .

0
0
...

(α2)(−1)m−1
m−1x∗
x∗
m

(x∗

m)2

0
0
...
m−1x∗
x∗
m
(α2)(−1)m−2
m−1)2
(x∗

.










Consider the vector v deﬁned as v2t+1 =
results in zero for all the matrices above except for the ﬁrst one. Then,

and v2t = α2 x∗
2t+1
x∗
2

x∗
2t+2
x∗
2

for t = 0, . . . ,

(m

⌊

−

1)/2

⌋

so that vT (C

BT A−1B)v

−

vT (C

−

BT A−1B)v = α2(x∗

1)2

−

A−1

3,3(x∗

1)2(x∗

2)2 + 0 +

+ 0 < 0.

· · ·

As a result, C
cannot have a second-order critical point ˆx with ˆx∗

BT A−1B is not positive deﬁnite. Hence, the Hessian is not positive deﬁnite either. Consequently, the problem
[n].

−

i = 0 for some i

Any solution that meets the requirements of second-order necessary optimality conditions cannot contain any zero entries.
due to the
j for some (i, j)
and ˆxi 6
= 0

In addition, if a stationary point ˆx satisﬁes ˆxi ˆxj = x∗
stationarity condition. Therefore, a spurious solution has the following properties: ˆxi ˆxj 6
for all i = 0, . . . 2k.

i x∗
, then ˆxi ˆxj = x∗
j for all (i, j)
i x∗
= x∗
j for all (i, j)

i x∗

∈ E

∈ E

∈ E

∈

Deﬁne ai,j = xixj −

i x∗
x∗

j . Then, the stationarity condition for the B-M factorized problem (25) can be written as

[

f (ˆx)]i =

∇

ˆa0,2k ˆx2k + ˆa0,1 ˆx1 = 0,
ˆai−1,i ˆxi−1 + ˆai,i+1 ˆxi+1 = 0,
ˆa2k−1,2k ˆx2k−1 + ˆa0,2k ˆx0 = 0,




if i = 0
if i
6∈ {
if i = 2k

0, 2k

.

}

The following calculation yields a contradiction because none of the terms can be zero:



2k

As a result, all the second-order critical points are global solutions and we obtain the valid factors of the ground truth matrix
completion.

1)i[

(
−

f (ˆx)]i = 2ˆa0,2k ˆx0 ˆx2k = 0.

∇

i=0
X

Proof of Theorem 8
Proof. We aim to ﬁnd an instance of the matrix completion problem with an odd-numbered cycle graph for which the SDP
problem (2) fails. Consider the following rank-1 feasible solution ˆM = yyT with

y0 = ( ˆM0,0)1/2

x∗
0,

≥

y2t−1 =

(x∗

0)2

2t−1)2(x∗
ˆM0,0

1/2

!

,

y2t =

(x∗
(x∗

2t)2
0)2

(cid:18)

ˆM0,0

1/2

,

(cid:19)

where t = 1, . . . , k. Note that ˆM0,0 is free to choose and we can minimize the trace to ﬁnd the best rank-1 solution. This is
equivalent to the following optimization problem:

min
ˆM0,0≥(x∗

0 )2

ˆM0,0
(x∗
0)2

k

(x∗

2t)2 +

0)2
(x∗
ˆM0,0

k

(x∗

2t−1)2.

t=1
X
0)2, the solution ˆM is the same as ground truth solution. A basic ﬁrst-order optimality condition
Note that whenever ˆM0,0 = (x∗
k
0)2. However, the objective
2t−1)2 >
t=1(x∗
implies that whenever
value is strictly better than the trace of the ground truth matrix. Due to the symmetry of the problem, any node can be chosen
k
2t)2 holds for some chosen node 0, the SDP fails to solve the
t=0(x∗
as node 0. Therefore, if the condition
matrix completion problem.

2t)2, the optimal ˆM0,0 is strictly larger than (x∗

2t−1)2 >

P
k
t=1(x∗

k
t=0(x∗

t=0
X

P

P

P

 
