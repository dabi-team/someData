2
2
0
2

r
p
A
2
2

]

G
L
.
s
c
[

1
v
2
3
5
0
1
.
4
0
2
2
:
v
i
X
r
a

End-to-end symbolic regression with
transformers

Pierre-Alexandre Kamienny∗†1,2, Stéphane d’Ascoli††1,3, Guillaume Lample1 and François
Charton1

1Meta AI, Paris
2ISIR MLIA, Sorbonne Université, Paris
3Department of Physics, Ecole Normale Supérieure, Paris

Abstract

Symbolic regression, the task of predicting the mathematical expression of a function from the
observation of its values, is a difficult task which usually involves a two-step procedure: predicting
the "skeleton" of the expression up to the choice of numerical constants, then fitting the constants
by optimizing a non-convex loss function. The dominant approach is genetic programming, which
evolves candidates by iterating this subroutine a large number of times. Neural networks have
recently been tasked to predict the correct skeleton in a single try, but remain much less powerful.
In this paper, we challenge this two-step procedure, and task a Transformer to directly predict
the full mathematical expression, constants included. One can subsequently refine the predicted
constants by feeding them to the non-convex optimizer as an informed initialization. We present
ablations to show that this end-to-end approach yields better results, sometimes even without the
refinement step. We evaluate our model on problems from the SRBench benchmark and show that
our model approaches the performance of state-of-the-art genetic programming with several orders
of magnitude faster inference.

Introduction

Inferring mathematical laws from experimental data is a central problem in natural science; having
observed a variable y at n points {xi}i∈Nn
, it implies finding a function f such that yi ≈ f (xi) for all
i ∈ Nn. Two types of approaches exist to solve this problem. In parametric statistics (PS), the function f
is defined by a small number of parameters that can directly be estimated from the data. On the other
hand, machine learning (ML) techniques such as decision trees and neural networks select f from large
families of non-linear functions by minimizing a loss over the data. The latter relax the assumptions
about the underlying law, but their solutions are more difficult to interpret, and tend to overfit small
experimental data sets, yielding poor extrapolation performance.

Symbolic regression (SR) stands as a middle ground between PS and ML approaches: f is selected
from a large family of functions, but is required to be defined by an interpretable analytical expression.
It has already proved extremely useful in a variety of tasks such as inferring physical laws [1, 2].

SR is usually performed in two steps. First, predicting a “skeleton”, a parametric function using
a pre-defined list of operators – typically, the basic operations (+, ×, ÷) and functions (sqrt, exp, sin,
etc.). It determines the general shape of the law up to a choice of constants, e.g. f (x) = cos(ax + b).
Then, the constants in the skeleton (a, b) are estimated using optimization techniques, typically the
Broyden–Fletcher–Goldfarb–Shanno algorithm (BFGS).

∗pakamienny@fb.com
†stephane.dascoli@gmail.com
†Equal contribution.

1

 
 
 
 
 
 
Figure 1: Our model outperforms previous DL-based methods and offers at least an order of
magnitude inference speedup compared to SOTA GP-based methods. Pareto plot comparing the
average test performance and inference time of our models with baselines provided by the SRbench
benchmark [7], both on Feynman SR problems [1] and black-box regression problems. We use colors
to distinguish three families of models: deep-learning based SR, genetic programming-based SR
and classic machine learning methods (which do not provide an interpretable solutions). A similar
Pareto plot against formula complexity is provided in Fig. 11.

The leading algorithms for SR rely on genetic programming (GP). At each generation, a population
of candidates is predicted, and the fittest ones are selected based on the data, and mutated to build the
next generation. The algorithm iterates this procedure until a satisfactory level of accuracy is achieved.
While GP algorithms achieve good prediction accuracy, they are notably slow (see the Pareto plot of
Fig. 1). Indeed, the manually predefined function space to search is generally vast, and each generation
involves a costly call to the BFGS routine. Also, GP does not leverage past experience: every new problem
is learned from scratch. This makes GP techniques inapplicable to situations where fast computation is
needed, for instance in reinforcement learning and physics environments [3, 4].

Pre-training neural networks built for language modelling on large datasets of synthetic examples
has recently been proposed for SR [5, 6]. These references follow the two-step procedure (predicting the
skeleton then fitting the constants) inherited from GP. Once the model is pre-trained, the skeleton is
predicted via a simple forward pass, and a single call to BFGS is needed, thus resulting in a significant
speed-up compared to GP. However, these methods are not as accurate as state-of-the-art GP, and have
so far been limited to low-dimensional functions (D ≤ 3). We argue that two reasons underlie their
shortcomings.

First, skeleton prediction is an ill-posed problem that does not provide sufficient supervision: different
instances of the same skeleton can have very different shapes, and instances of very different skeletons
can be very close. Second, the loss function minimized by BFGS can be highly non-nonconvex: even
when the skeleton is perfectly predicted, the correct constants are not guaranteed to be found. For these
reasons, we believe, and will show, that doing away with skeleton estimation as a intermediary step can
greatly facilitate the task of SR for language models.

In this paper, we train Transformers over synthetic datasets to perform end-to-end
Contributions
(E2E) symbolic regression: solutions are predicted directly, without resorting to skeletons. To this
effect, we leverage a hybrid symbolic-numeric vocabulary, that uses both symbolic tokens for the
operators and variables and numeric tokens for the constants. One can then perform a refinement of
the predicted constants by feeding them as informed guess to BFGS, mitigating non-linear optimization
issues. Finally, we introduce generation and inference techniques that allow our models to scale to
larger problems: up to 10 input features against 3 in concurrent works.

Evaluated over the SRBench benchmark [7], our model significantly narrows the accuracy gap with
state-of-the-art GP techniques, while providing several orders of magnitude of inference time speedup

2

0.00.20.40.60.81.0Black-box median R2100102104Inference time (seconds)AFPAFP_FEAIFeynmanAdaBoostBSRDSREPLEXFEATFFXGP-GOMEAITEAKernelRidgeLGBMLinearMLPMRGPOperonOursOurs (no ref)Ours (skel)RandomForestSBP-GPXGBgplearn0.20.40.60.81.0Feynman mean accuracy (R2>0.99)102103104AFPAFP_FEAIFeynmanBSRDSREPLEXFEATFFXGP-GOMEAITEAMRGPOperonOursOurs (no ref)Ours (skel)SBP-GPgplearnDL-based SRGP-based SRML methods(see Fig. 1). We also demonstrate strong robustness to noise and extrapolation capabilities.

Finally, we will provide an online demonstration of our model at https://bit.ly/3niE5FS and
will open-source our implementation as a Scikit-learn compatible regressor at the following address:
https://github.com/facebookresearch/symbolicregression.

Related work SR is a challenging task that traces back from a few decades ago, with a large number of
open-source and commercial softwares, and has already been used to accelerate scientific discoveries [8,
9, 10]. Most popular frameworks for symbolic regression use GP [11, 12, 13, 14, 15, 16, 17, 18, 19] (see [7]
for a recent review), but SR has also seen growing interest from the Deep Learning (DL) community,
motivated by the fact that neural networks are good at identifying qualitative patterns.

Neural networks have been combined with GP algorithms, e.g. to simplify the original dataset [1], or
to propose a good starting distribution over mathematical expressions[20]. [21, 22] propose modifications
to feed-forward networks to include interpretable components, i.e. replacing usual activation functions
by operators such as cos, sin, however these are hard to optimize and prone to numerical issues.

Language models, and especially Transformers [23], have been trained over synthetic datasets to
solve various mathematical problems: integration [24], dynamical systems [25], linear algebra [26],
formal logic [27] and theorem proving [28]. A few papers apply these techniques to symbolic regression:
the aforementioned references [6, 5] train Transformers to predict function skeletons, while [29] infers
one-dimensional recurrence relations in sequences of numbers.

The recently introduced SRBench [7] provides a benchmark for rigorous evaluation of SR methods,

in addition to 14 SR methods and 7 ML baselines which we will compare to in this work.

1 Data generation

Our approach consists in pre-training language models on vast synthetic datasets. Each training example
is a pair: a set of N points (x, y) ∈ RD × R as the input, and a function f such that y = f (x) as the
target1 Examples are generated by first sampling a random function f , then a set of N input values
(xi)i∈NN

in RD, and computing yi = f (xi).

1.1 Generating functions

To sample functions f , we follow the seminal approach of Lample and Charton [24], and generate
random trees with mathematical operators as internal nodes and variables or constants as leaves. The
procedure is detailed below (see Table 3 in the Appendix for the values of parameters):

1. Sample the desired input dimension D of the function f from U{1, Dmax}.
2. Sample the number of binary operators b from U{D − 1, D + bmax} then sample b operators

from U{+, −, ×}2.

3. Build a binary tree with those b nodes, using the sampling procedure of [24].
4. For each leaf in the tree, sample one of the variables xd, d ∈ ND.
5. Sample the number of unary operators u from U{0, umax} then sample u operators from the

list Ou in Table 3, and insert them at random positions in the tree.

6. For each variable xd and unary operator u, apply a random affine transformation, i.e. replace

xd by axd + b, and u by au + b, with (a, b) sampled from Daff.

Note that since we require independent control on the number of unary operators (which is inde-
pendent of D) and binary operators (which depends on D), we cannot directly sample a unary-binary
tree as in [24]. Note also that the first D variables are sampled in ascending order to obtain the desired
input dimension, which means functions with missing variables such as x1 + x3 are never encountered;

1We only consider functions from RD into R; the general case f : RD → RP can be handled as P independent subproblems.
2Note that although the division operation is technically a binary operator, it appears much less frequently than additions

and multiplications in typical expressions [30], hence we replace it by the unary operator inv: x → 1/x.

3

Figure 2: Sketch of our model. During training, the inputs are all whitened. At inference, we whiten
them as a pre-processing step; the predicted function must then be unscaled to account for the whitening.

this is not an issue as our model can always set the prefactor of x2 to zero. As discussed quantitatively
in App. C, the number of possible skeletons as well as the random sampling of numerical constants
guarantees that our model almost never sees the same function twice, and cannot simply perform
memorization.

1.2 Generating inputs
For each function f : RD → R, we sample N ∈ U{10D, Nmax} input values xi ∈ RD from the
distribution Dx described below, and compute the corresponding output values yi = f (xi). If any xi
is outside the domain of definition of f or if any yi is larger 10100, the process is aborted, and we start
again by generating a new function. Note that rejecting and resampling out-of-domain values of xi,
the obvious and cheaper alternative, would provide the model with additional information about f , by
allowing it to learn its domain of definition.

To maximize the diversity of input distributions seen at training time, we sample our inputs from a
mixture of distributions (uniform or gaussian), centered around k random centroids3, see App. A for
some illustrations at D = 2. Input samples are generated as follows:

1. Sample a number of clusters k ∼ U{1, kmax} and k weights wi ∼ U(0, 1), which are then

normalized so that (cid:80)

i wi = 1.

2. For each cluster i ∈ Nk, sample a centroid µi ∼ N (0, 1)D, a vector of variances σi ∼ U(0, 1)D

and a distribution shape (gaussian or uniform) Di ∈ {N , U}.

3. For each cluster i ∈ Nk, sample (cid:98)wiN (cid:99) input points from Di(µi, σi) then apply a random

rotation sampled from the Haar distribution.

4. Finally, concatenate all the points obtained and whiten them by substracting the mean and

dividing by the standard deviation along each dimension.

1.3 Tokenization

Following [26], we represent numbers in base 10 floating-point notation, round them to four significant
digits, and encode them as sequences of 3 tokens: their sign, mantissa (between 0 and 9999), and
exponent (from E-100 to E100).

To represent mathematical functions as sequences, we enumerate the trees in prefix order, i.e. direct
Polish notation, as in [24]: operators and variables and integers are represented as single autonomous
tokens, and constants are encoded as explained above.

For example, the expression f (x) = cos(2.4242x) is encoded as [cos,mul,+,2424,E-3,x]. Note
that the vocabulary of the decoder contains a mix of symbolic tokens (operators and variables) and
numeric tokens, whereas that of the encoder contains only numeric tokens4.

3For k → ∞, such a mixture could in principe approximate any input distribution.
4The embeddings of numeric tokens are not shared between the encoder and decoder.

4

=⎡⎣⎢⎢𝑥1𝑥2𝑦⎤⎦⎥⎥⎡⎣⎢⎢237⎤⎦⎥⎥ScaleEmbedderTransformerReﬁneInputOutput=⎡⎣⎢⎢𝑥̃ 1𝑥̃ 2𝑦⎤⎦⎥⎥⎡⎣⎢⎢−10.57⎤⎦⎥⎥ScaledinputScaledoutputEncodeDecodeTokenizeFFN(𝑁,3(𝐷+1))𝑑𝑒𝑚𝑏⎡⎣⎢⎢−,1000,𝐸−3+,5000,𝐸−4+,2500,𝐸−3⎤⎦⎥⎥EmbedderTransformer(𝑁,)𝑑𝑒𝑚𝑏𝑁OutputTarget𝑦=+𝑥21𝑥2𝑦=+𝑥21𝑥2Cross-entropy=⎡⎣⎢⎢𝑥1𝑥2𝑦⎤⎦⎥⎥⎡⎣⎢⎢−10.52.5⎤⎦⎥⎥(𝑁,𝐷+1)InputTrainingInferenceUnscale𝑦=(+3.1+0.9(+2.6)𝑥̃ 1)2𝑥̃ 2𝑦=+𝑥21𝑥2𝑦=(+3+(+2.5)𝑥̃ 1)2𝑥̃ 2Figure 3: Attention heads reveal intricate mathematical analysis. We considered the expression
f (x) = sin(x)/x, with N = 100 input points sampled between −20 and 20 (red dots; the y-axis is
arbitrary). We plotted the attention maps of a few heads of the encoder, which are N × N matrices
where the element (i, j) represents the attention between point i and point j. Notice that heads 2, 3 and
4 of the second layer analyze the periodicity of the function in a Fourier-like manner.

2 Methods

Below we describe our approach for end-to-end symbolic regression; please refer to Fig. 2 for an
illustration.

2.1 Model

Embedder Our model is provided N input points (x, y) ∈ RD+1, each of which is represented as
3(D + 1) tokens of dimension demb. As D and N become large, this results in long input sequences (e.g.
6600 tokens for D = 10 and N = 200), which challenge the quadratic complexity of Transformers. To
mitigate this, we introduce an embedder to map each input point to a single embedding.

The embedder pads the empty input dimensions to Dmax, then feeds the 3(Dmax+1)demb-dimensional
vector into a 2-layer fully-connected feedforward network (FFN) with ReLU activations, which projects
5 The resulting N embeddings of dimension demb are then fed to the Transformer.
down to dimension demb

Transformer We use a sequence to sequence Transformer architecture [23] with 16 attention heads
and an embedding dimension of 512, containing a total of 86M parameters. Like [26], we observe that
the best architecture for this problem is asymmetric, with a deeper decoder: we use 4 layers in the
encoder and 16 in the decoder. A notable property of this task is the permutation invariance of the N
input points. To account for this invariance, we remove positional embeddings from the encoder.

As shown in Fig. 3 and detailed in App. B, the encoder captures the most distinctive features of the
functions considered, such as critical points and periodicity, and blends a mix of short-ranged heads
focusing on local details with long-ranged heads which capture the global shape of the function.

Training We optimize a cross-entropy loss with the Adam optimizer, warming up the learning rate
from 10−7 to 2.10−4 over the first 10,000 steps, then decaying it as the inverse square root of the number
of steps, following [23]. We hold out a validation set of 104 examples from the same generator, and train
our models until the accuracy on the validation set saturates (around 50 epochs of 3M examples).

Input sequence lengths vary significantly with the number of points N ; to avoid wasteful padding,
we batch together examples of similar lengths, ensuring that a full batch contains a minimum of 10,000
tokens. On 32 GPU with 32GB memory each, one epoch is processed in about half an hour.

5We explored various architectures for the embedder, but did not obtain any improvement; this does not appear to be a

critical part of the model.

5

Layer 1Head 1Head 2Head 3Head 4Head 5Head 6Head 7Head 8Layer 22.2 Inference tricks

In this section, we describe three tricks to improve the performance of our model at inference.

Model

Function f (x, y)

Target
Skeleton + BFGS
E2E no BFGS

sin(10x) exp(0.1y)
− sin(1.7x)(0.059y + 0.19)
sin(9.9x) exp(0.1y)

E2E + BFGS random init − sin(0.095x) exp(0.27y)
E2E + BFGS model init

sin(10x) exp(0.1y)

Table 1: The importance of an end-to-end model with refinement. The skeleton approach recovers
an incorrect skeleton. The E2E approach predicts the right skeleton. Refinement worsens original
prediction when randomly initialized, and yields the correct result when initialized with predicted
constants.

Refinement Previous language models for SR, such as [6], follow a skeleton approach: they first
predict equation skeletons, then fit the constants with a non-linear optimisation solver such as BFGS.
In this paper, we follow an end-to-end (E2E) approach: predicting simultaneously the function and the
values of the constants. However, we improve our results by adding a refinement step: fine-tuning the
constants a posteriori with BFGS, initialized with our model predictions6.

This results in a large improvement over the skeleton approach, as we show by training a Transformer
to predict skeletons in the same experimental setting. The improvement comes from two reasons: first,
prediction of the full formula provides better supervision, and helps the model predict the skeleton;
second, the BFGS routine strongly benefits from the informed initial guess, which helps the model
predict the constants. This is illustrated qualitatively in Table 1, and quantitatively in Table 2.

Scaling As described in Section 1.2, all input points presented to the model during training are
whitened: their distribution is centered around the origin and has unit variance. To allow accurate
prediction for input points with a different mean and variance, we introduce a scaling procedure at
inference time. Let f the function to be inferred, x be the input points, and µ = mean(x), σ = std(x).
As illustrated in Fig. 2 we pre-process the input data by replacing x by ˜x = x−µ
. The model then
σ
predicts ˆf (˜x) = ˆf (σx + µ), and we can recover an approximation of f by unscaling the variables in ˆf .
This gives our model the desirable property to be insensitive to the scale of the input points: DL-
based approaches to SR are known to fail when the inputs are outside the range of values seen during
training [29, 26]. Note that here, the scale of the inputs translates to the scale of the constants in the
function f ; although these coefficients are sampled in Daff during training, coefficients outside Daff can
be expressed by multiplication of constants in Daff.

Bagging and decoding Since our model was trained on N ≤ 200 input points, it does not perform
satisfactorily at inference when presented with more than 200 input points. To take advantage of large
datasets while accommodating memory constraints, we perform bagging: whenever N is larger than
200 at inference, we randomly split the dataset into B bags of 200 input points7.

For each bag, we apply a forward pass and generate C function candidates via random sampling or
beam search using the next token distribution. As shown in App. E (Fig. 16), the more commonly used
beam search [33] strategy leads to much less good results than sampling due to the lack of diversity

6To avoid BFGS having to approximate gradients via finite differences, we provide the analytical expression of the gradient

using sympytorch [31] and functorch [32].

7Smarter splits, e.g. diversity-preserving, could be envisioned, but were not considered here.

6

induced by constant prediction (typical beams will look like sin(x), sin(1.1x), sin(0.9x), . . .). This
provides us with a set of BC candidate solutions.

Inference time Our model inference speed has two sources: the forward passes described above on
one hand (which can be parallelized up to memory limits of the GPU), and the refinements of candidate
functions on the other (which are CPU-based and could also be parallelized, although we did not consider
this option here).

Since BC can become large, we rank candidate functions (according to their error on all input
points), get rid of redundant skeleton functions and keep the best K candidates for the refinement
step8. To speed up the refinement, we use a subset of at most 1024 input points for the optimization.
The parameters B, C and K can be used as cursors in the speed-accuracy tradeoff: in the experiments
presented in Fig. 1, we selected B = 100, C = 10, K = 10.

3 Results

In this section, we present the results of our model. We begin by studying in-domain accuracy, then
present results on out-of-domain datasets.

3.1 In-domain performance

We report the in-domain performance of our models by evaluating them on a fixed validation set
of 100,000 examples, generated as per Section 1. Validation functions are uniformly spread out over
three difficulty factors: number of unary operators, binary operators, and input dimension. For each
function, we evaluate the performance of the model when presented N = [50, 100, 150, 200] input
points (x, y), and prediction accuracy is evaluated on Ntest = 200 points sampled from a fresh instance
of the multimodal distribution described in Section 1.2.

We assess the performance of our model using two popular metrics: R2-score [7] and accuracy to

tolerance τ [6, 29]:

R2 = 1 −

(cid:80)Ntest
i
(cid:80)Ntest
i

(yi − ˆyi)2
(yi − ¯y)2 ,

(cid:18)

Accτ = 1

max
1≤i≤Ntest

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ˆyi − yi
yi

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

≤ τ

,

(1)

where 1 is the indicator function.

R2 is classically used in statistics, but it is unbounded, hence a single bad prediction can cause
the average R2 over a set of examples to be extremely bad. To circumvent this, we set R2 = 0 upon
pathological examples as in [7](such examples occur in less that 1% of cases)9. The accuracy metric
provides a better idea of the precision of the predicted expression as it depends on a desired tolerance
threshold. However, due to the presence of the max operator, it is sensitive to outliers, and hence to the
number of points considered at test time (more points entails a higher risk of outlier). To circumvent
this, we discard the 5% worst predictions, following [6].

End-to-end outperforms skeleton In Table 2, we report the average in-domain results of our models.
Without refinement, our E2E model outperforms the skeleton model trained under the same protocol
in terms of low precision prediction (R2 and Acc0.1 metrics), but small errors in the prediction of the
constants lead to lower performance at high precision (Acc0.001 metric). The refinement procedure
alleviates this issue significantly, inducing a three-fold increase in Acc0.001 while also boosting other
metrics.

Initializing BFGS with the constants estimated in the E2E phase plays a crucial role: with random
initialization, the BFGS step actually degrades E2E performance. However, refinement with random

8Though these candidates are the best functions without refinement, there are no guarantees that these would be the best

after refinement, especially as optimization is particularly prone to spurious local optimas.
9Note that predicting the constant function f = ¯y naturally yields an R2 score of 0.

7

Model

Skeleton + BFGS
E2E no BFGS
E2E + BFGS random init
E2E + BFGS model init

R2
0.43
0.62
0.44
0.68

Acc0.1 Acc0.01 Acc0.001
0.27
0.40
0.27
0.51
0.30
0.44
0.44
0.61

0.17
0.09
0.19
0.29

Table 2: Our approach outperforms the skeleton approach. Metrics are computed over the 10, 000
examples of the evaluation set.

initialization still achieves better results than the skeleton model: this suggests that the E2E model
predicts skeletons better that the skeleton model.

Ablation Fig. 4A,B,C presents an ablation over three indicators of formula difficulty (from left to right):
number of unary operators, number of binary operators and input dimension. In all cases, increasing
the factor of difficulty degrades performance, as one could expect. This may give the impression that
our model does not scale well with the input dimension, but we show that our model scales in fact very
well on out-of-domain datasets compared to concurrent methods (see Fig. 15 of the Appendix).

Fig. 4D shows how performance depends on the number of input points fed to the model, N . In
all cases, performance increases, but much more signicantly for the E2E models than for the skeleton
model, demonstrating the importance of having a lot of data to accurately predict the constants in the
expression.

Extrapolation and robustness
by varying the scale of the test points: instead of normalizing the test points to unit variance, we normal-
ize them to a scale σ. As expected, performance degrades as we increase σ, however the extrapolation
performance remains decent even very far away from the inputs (σ = 32).

In Fig. 4E, we examine the ability of our models to interpolate/extrapolate

Finally, in Fig. 4F, we examine the effect of corrupting the targets y with a multiplicative noise of
variance σ: y → y(1 + ξ), ξ ∼ N (0, ε). The results reveal something interesting: without refinement,
the E2E model is not robust to noise, and actually performs worse than the skeleton model at high
noise. This shows how sensitive the Transformer is to the inputs when predicting constants. Refinement
improves robustness significantly, but the initialization of constants to estimated values has less impact,
since the prediction of constants is corrupted by the noise.

3.2 Out-of-domain generalization

We evaluate our method on the recently released benchmark SRBench[7]. Its repository contains a
set of 252 regression datasets from the Penn Machine Learning Benchmark (PMLB)[34] in addition
to 14 open-source SR and ML baselines. The datasets consist in "ground-truth" problems where the
true underlying function is known, as well as "black-box" problems which are more general regression
datasets without an underlying ground truth.

We filter out problems from SRBench to only keep regression problems with D ≤ 10 with continuous
features; this results in 190 regression datasets, splitted into 57 black-box problems (combination of
real-world and noisy, synthetic datasets), 119 SR datasets from the Feynman [1] and 14 SR datasets from
the ODE-Strogatz [35] databases. Each dataset is split into 75% training data and 25% test data, on
which performance is evaluated.

The overall performance of our models is illustrated in the Pareto plot of Fig. 1, where we see that
on both types of problems, our model achieves performance close to state-of-the-art GP models such
as Operon with a fraction of the inference time10. Impressively, our model outperforms all classic ML

10Inference uses a single GPU for the forward pass of the Transformer.

8

Figure 4: Ablation over the function difficulty (top row) and input difficulty (bottom row). We
plot the accuracy at τ = 0.1 (Eq. 1), see App. D for the R2 score. We distinguish four models: skeleton,
E2E without refinement, E2E with refinement from random guess and E2E with refinement.
A: number of unary operators. B: number of binary operators. C: input dimension. D: Low-resource
performance, evaluated by varying the number of input points. E: Extrapolation performance, evaluated
by varying the variance of the inputs. F: Robustness to noise, evaluated by varying the multiplicative
noise added to the labels.

9

02468Number of unary ops u0.00.20.40.60.81.0AccuracyA2040Num of binary ops b0.00.20.40.60.81.0B246810Input dimension D0.00.20.40.60.81.0C50100150200Number of input pairs N0.00.20.40.60.81.0AccuracyDSkeletonE2E no refE2E random refE2E ref100101Test input variance 0.00.20.40.60.81.0EInterpolationExtrapolation0.0000.0250.0500.0750.100Noise variance 0.00.20.40.60.81.0Fmethods (e.g. XGBoost and Random Forests) on real-world problems with a lower inference time, and
while outputting an interpretable formula.

We provide more detailed results on Feynman problems in Fig. 5, where we additionally plot the
formula complexity, i.e. the number of nodes in the mathematical tree (see App. E for similar results on
black-box and Strogatz problems). Varying the noise applied to the targets noise, we see that our model
displays similar robustness to state-of-the-art GP models.

While the average accuracy or our model is only ranked fourth, it outputs formulas with lower
complexity than the top 2 models (Operon and SBP-GP), which is an important criteria for SR problems:
see App. 11 for complexity-accuracy Pareto plots. To the best of our knowledge, our model is the first
non-GP approach to achieve such competitive results for SR.

Figure 5: Our model presents strong accuracy-speed-complexity tradeoffs, even in presence of
noise. Results are averaged over all 119 Feynman problems, for 10 random seeds and three target noises
each as shown in the legend. The accuracy is computed as the fraction of problems for which the R2
score on test examples is above 0.99. Models are ranked according to the accuracy averaged over all
target noise.

Conclusion

In this work, we introduced a competitive deep learning model for SR by using a novel numeric-symbolic
approach. Through rigorous ablations, we showed that predicting the constants in an expression not
only improves performance compared to predicting a skeleton, but can also serve as an informed initial
condition for a solver to refine the value of the constants.

Our model outperforms previous deep learning approaches by a margin on SR benchmarks, and
scales to larger dimensions. Yet, the dimensions considered here remain moderate (D < 10): adapting
to the truly high-dimensional setup is an interesting future direction, and will likely require qualitative
changes in the data generation protocol. While our model narrows the gap between GP and DL based
SR, closing the gap also remains a challenge for future work.

This work opens up a whole new range of applications for SR in fields which require real-time
inference. We hope that the methods presented here may also serve as a toolbox for many future
applications of Transformers for symbolic tasks.

10

0.00.20.40.60.81.0OperonSBP-GPGP-GOMEAOursEPLEXMRGPAFP_FEOurs (skel)AIFeynmanFEATOurs (no ref)AFPgplearnFFXDSRITEABSRMean accuracy (R2>0.99)102104Formula complexityTarget Noise0.00.0010.010.1102103104Inference time (seconds)References

[1]

Silviu-Marian Udrescu and Max Tegmark. AI Feynman: a Physics-Inspired Method for Symbolic
Regression. 2020. arXiv: 1905.11481 [physics.comp-ph].

[2] M. Cranmer et al. ‘Discovering Symbolic Models from Deep Learning with Inductive Biases’. In:

ArXiv abs/2006.11287 (2020).

[3] Marta Garnelo, Kai Arulkumaran and Murray Shanahan. ‘Towards deep symbolic reinforcement

learning’. In: arXiv preprint arXiv:1609.05518 (2016).

[4] Mikel Landajuela et al. ‘Discovering symbolic policies with deep reinforcement learning’. In:

International Conference on Machine Learning. PMLR. 2021, pp. 5979–5989.

[5] Mojtaba Valipour et al. ‘SymbolicGPT: A Generative Transformer Model for Symbolic Regression’.

In: arXiv preprint arXiv:2106.14131 (2021).

[6] Luca Biggio et al. Neural Symbolic Regression that Scales. 2021. arXiv: 2106.06427 [cs.LG].
[7] William La Cava et al. ‘Contemporary symbolic regression methods and their relative perform-

ance’. In: arXiv preprint arXiv:2107.14351 (2021).

[8] Nikos Aréchiga et al. ‘Accelerating Understanding of Scientific Experiments with End to End

[9]

Symbolic Regression’. In: ArXiv abs/2112.04023 (2021).
Silviu-Marian Udrescu and Max Tegmark. ‘Symbolic Pregression: Discovering Physical Laws from
Raw Distorted Video’. In: Physical review. E 103 4-1 (2021), p. 043307.

[10] Anja Butter et al. ‘Back to the Formula – LHC Edition’. In: 2021.

[11] Michael Schmidt and Hod Lipson. ‘Age-fitness pareto optimization’. In: Genetic programming

theory and practice VIII. Springer, 2011, pp. 129–146.

[12] Michael Schmidt and Hod Lipson. ‘Distilling free-form natural laws from experimental data’. In:

science 324.5923 (2009), pp. 81–85.

[13] William La Cava et al. ‘Learning concise representations for regression by evolving networks of

trees’. In: arXiv preprint arXiv:1807.00981 (2018).

[14] Trent McConaghy. ‘FFX: Fast, scalable, deterministic symbolic regression technology’. In: Genetic

Programming Theory and Practice IX. Springer, 2011, pp. 235–260.

[15] Marco Virgolin et al. ‘Improving model-based genetic programming for symbolic regression of

[16]

[17]

small expressions’. In: Evolutionary computation 29.2 (2021), pp. 211–237.
Fabricio Olivetti de França and Guilherme Seidyo Imai Aldeia. ‘Interaction–Transformation Evol-
utionary Algorithm for Symbolic Regression’. In: Evolutionary computation 29.3 (2021), pp. 367–
390.

Ignacio Arnaldo, Krzysztof Krawiec and Una-May O’Reilly. ‘Multiple regression genetic program-
ming’. In: Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation.
2014, pp. 879–886.

[18] Marco Virgolin, Tanja Alderliesten and Peter A. N. Bosman. ‘Linear Scaling with and within
Semantic Backpropagation-Based Genetic Programming for Symbolic Regression’. In: Proceedings
of the Genetic and Evolutionary Computation Conference. GECCO ’19. Prague, Czech Republic:
Association for Computing Machinery, 2019, pp. 1084–1092. isbn: 9781450361118. doi: 10.1145/
3321707.3321758. url: https://doi.org/10.1145/3321707.3321758.

[19] Michael Kommenda et al. ‘Parameter identification for symbolic regression using nonlinear
least squares’. In: Genetic Programming and Evolvable Machines 21.3 (2020), pp. 471–501. doi:
10.1007/s10710-019-09371-3. url: https://doi.org/10.1007/s10710-019-09371-3.

[20] Brenden K Petersen et al. ‘Deep symbolic regression: Recovering mathematical expressions from

data via risk-seeking policy gradients’. In: arXiv preprint arXiv:1912.04871 (2019).

11

[21] Georg Martius and Christoph H Lampert. ‘Extrapolation and learning equations’. In: arXiv preprint

[22]

arXiv:1610.02995 (2016).
Subham Sahoo, Christoph Lampert and Georg Martius. ‘Learning equations for extrapolation and
control’. In: International Conference on Machine Learning. PMLR. 2018, pp. 4442–4450.
[23] Ashish Vaswani et al. ‘Attention is all you need’. In: Advances in neural information processing

systems. 2017, pp. 5998–6008.

[24] Guillaume Lample and François Charton. ‘Deep learning for symbolic mathematics’. In: arXiv

[25]

preprint arXiv:1912.01412 (2019).
François Charton, Amaury Hayat and Guillaume Lample. ‘Learning advanced mathematical
computations from examples’. In: arXiv preprint arXiv:2006.06462 (2020).
François Charton. ‘Linear algebra with transformers’. In: arXiv preprint arXiv:2112.01898 (2021).
[27] Christopher Hahn et al. ‘Teaching temporal logics to neural networks’. In: arXiv preprint arXiv:2003.04218

[26]

(2020).

[28]

[29]

Stanislas Polu and Ilya Sutskever. ‘Generative language modeling for automated theorem proving’.
In: arXiv preprint arXiv:2009.03393 (2020).
Stéphane d’Ascoli et al. ‘Deep Symbolic Regression for Recurrent Sequences’. In: arXiv preprint
arXiv:2201.04600 (2022).

[30] Roger Guimerà et al. ‘A Bayesian machine scientist to aid in the solution of challenging scientific

problems’. In: Science advances 6.5 (2020), eaav6971.

[31] Patrick Kidger. SympyTorch. https://github.com/patrick-kidger/sympytorch. 2021.
[32] Richard Zou Horace He. functorch: JAX-like composable function transforms for PyTorch. https:

[33]

[34]

[35]

//github.com/pytorch/functorch. 2021.
Sam Wiseman and Alexander M. Rush. Sequence-to-Sequence Learning as Beam-Search Optimiza-
tion. 2016. doi: 10.48550/ARXIV.1606.02960. url: https://arxiv.org/abs/1606.02960.
Jerome H Friedman. ‘Greedy function approximation: a gradient boosting machine’. In: Annals of
statistics (2001), pp. 1189–1232.
Steven H. Strogatz. Nonlinear Dynamics and Chaos: With Applications to Physics, Biology, Chemistry
and Engineering. Westview Press, 2000.

[36] Ying Jin et al. Bayesian Symbolic Regression. 2020. arXiv: 1910.08892 [stat.ME].
[37] T. Nathan Mundhenk et al. Symbolic Regression via Neural-Guided Genetic Programming Population

Seeding. 2021. arXiv: 2111.00053 [cs.NE].

12

A Details on the training data

In Tab. 3 we provide the detailed set of parameters used in our data generator. The probabilities of the
unary operators were selected to match the natural frequencies appearing in the Feynman dataset.

In Fig. 6, we show the statistics of the data generation.The number of expressions diminishes with
the input dimension and number of unary operators because of the higher likelihood of generating
out-of-domain inputs. One could easily make the distribution uniform by enforcing to retry as long as a
valid example is not found, however we find empirically that having more easy examples than hard
ones eases learning and provides better out-of-domain generalization, which is our ultimate goal.

In Fig. 7, we show some examples of the input distributions generated by our multimodal approach.

Notice the diversity of shapes obtained by this procedure.

Parameter

Description

Value

Dmax

Daff

bmax
Ob
umax

Ou

Nmin
Nmax
kmax

Max input dim

Distrib of (a,b)

Max binary ops
Binary operators
Max unary ops

Unary operators

Min number of points
Max number of points
Max num clusters

10
sign ∼ U{−1, 1},
mantissa ∼ U(0, 1),
exponent ∼ U(−2, 2)
5 + D
add:1, sub:1, mul:1
5
inv:5, abs:1, sqr:3, sqrt:3,
sin:1, cos:1, tan:0.2, atan:0.2,
log:0.2, exp:1
10D
200
10

Table 3: Parameters of our generator.

Figure 6: Statistics of the synthetic data. We calculated the latter on 10, 000 generated examples.

B Attention maps

A natural question is whether self-attention based architectures are optimally suited for symbolic
regression tasks. In Fig. 8, we show the attention maps produced by the encoder of our Transformer
model, which contains 4 layers avec 16 attention heads (we only keep the first 8 for the sake of space). In
order to make the maps readable, we consider one-dimensional inputs and sort them in ascending order.
The attention plots demonstrate the complementarity of the attention heads. Some focus on specific
regions of the input, whereas others are more spread out. Some are concentrated along the diagonal
(focusing on neighboring points), whereas others are concentrated along the anti-diagonal (focusing on
far-away points.

Most strikingly, the particular features of the functions studied clearly stand out in the attention plots.
Focus, for example, on the 7th head of layer 2. For the exponential function, it focuses on the extreme

13

510Input dimension0.000.050.100.150100200Number of input pairs0.0000.0050.0100.0150102030Number of binary ops0.000.020.040.06addmulsubdivBinary operators0100020003000024Number of unary ops0.00.20.4invcossinexppow2abspow3sqrtarctanlogtanUnary operators050100Figure 7: Diversity of the input distributions generated by the multimodal approach. Here we
show distributions obtained for D = 2.

points (near -1 and 1); for the inverse function, it focuses on the singularity around the origin; for the
sine function, it reflects the periodicity, with evenly spaces vertical lines. The same phenomenology can
be acrossed is several other heads.

C Does memorization occur?

It is natural to ask the following question: due to the large amount of data seen during training, is our
model simply memorizing the training set ? Answering this question involves computing the number of
possible functions which can be generated. To estimate this number, calculating the number of possible
skeleton Ns is insufficient, since a given skeleton can give rise to very different functions according to
the sampling of the constants, and even for a given choice of the constants, the input points {x} can be
sampled in many different ways.

Nonetheless, we provide the lower bound Ns as a function of the number of nodes in Fig. 9, using
the equations provided in [24]. For small expressions (up to four operators), the number of possible
expressions is lower or similar to than the number of expressions encountered during training, hence
one cannot exclude the possibility that some expressions were seen several times during training, but
with different realizations due to the initial conditions. However, for larger expressions, the number of
possibilities is much larger, and one can safely assume that the expressions encountered at test time
have not been seen during training.

D Additional in-domain results

Fig. 10, we present a similar ablation as Fig. 4 of the main text but using the R2 score as metric rather
than accuracy.

E Additional out-of-domain results

Complexity-accuracy
plexity on SRBench datasets.

In Fig. 11, we display a Pareto plot comparing accuracy and formula com-

Jin benchmark In Fig. 12, we show the predictions of our model on the functions provided in [36].
Our model gets all of them correct except for one.

Black-box datasets
SRBench.

In Fig. 13, we display the results of our model on the black-box problems of

14

Strogatz datasets Each of the 14 datasets from the ODE-Strogatz benchmark is the trajectory of a
2-state system following a first-order ordinary differential equation (ODE). Therefore, the input data
has a very particular, time-ordered distribution, which differs significantly from that seen at train
time. Unsurprisingly, Fig. 14 shows that our model performs somewhat less well to this kind of data in
comparison with GP-based methods.

Ablation on input dimension In Fig. 15, we show how the performance of our model depends on
the dimensionality of the inputs on Feynamn and black-box datasets.

Ablation on decoding strategy In Fig. 16, we display the difference in performance using two
decoding strategies.

15

(a) f (x) = x2

(b) f (x) = 1/x

(c) f (x) = sin(10x)

Figure 8: Attention maps reveal distinctive features of the functions considered. We presented
the model 1-dimensional functions with 100 input points sorted in ascending order, in order to better
visualize the attention. We plotted the self-attention maps of the first 8 (out of 16) heads of the
Transformer encoder, across all four layers. We see very distinctive patterns appears: exploding areas for
16
the exponential, the singularity at zero for the inverse function, and the periodicity of the sine function.

Layer 1Head 1Head 2Head 3Head 4Head 5Head 6Head 7Head 8Layer 2Layer 3Layer 4Layer 1Head 1Head 2Head 3Head 4Head 5Head 6Head 7Head 8Layer 2Layer 3Layer 4Layer 1Head 1Head 2Head 3Head 4Head 5Head 6Head 7Head 8Layer 2Layer 3Layer 4Figure 9: Our models only see a small fraction of the possible expressions during training. We
report the number of possible skeletons for each number of operators. Even after a hundred epochs, our
models have only seen a fraction of the possible expressions with more than 4 operators.

Figure 10: Ablation over the function difficulty (top row) and input difficulty (bottom row).
We plot the R2 score (Eq. 1). A: number of unary operators. B: number of binary operators. C:
input dimension. D: Low-resource performance, evaluated by varying the number of input points. E:
Extrapolation performance, evaluated by varying the variance of the inputs. F: Robustness to noise,
evaluated by varying the multiplicative noise added to the labels.

17

246810Input dimension1081013101810231028Number of expressionsNumber of skeletonsSeen in one epoch02468Number of unary ops u0.00.20.40.60.81.0R2 scoreA2040Num of binary ops b0.00.20.40.60.81.0B246810Input dimension D0.00.20.40.60.81.0C50100150200Number of input pairs N0.00.20.40.60.81.0R2 scoreDSkeletonE2E no refE2E random refE2E ref100101Test input variance 0.00.20.40.60.81.0EInterpolationExtrapolation0.0000.0250.0500.0750.100Noise variance 0.00.20.40.60.81.0FFigure 11: Complexity-accuracy pareto plot. Pareto plot comparing the average test performance
and formula complexity of our models with baselines provided by the SRbench benchmark [7], both
on Feynman SR problems [1] and black-box regression problems. We use colors to distinguish three
families of models: deep-learning based SR, genetic programming-based SR and classic machine learning
methods (which do not provide an interpretable solution).

(a) Jin-1

(b) Jin-2

(c) Jin-3

(d) Jin-4

(e) Jin-5

(f) Jin-6

Figure 12: Illustration of our model on a few benchmark datasets from the litterature. We show
the prediction of our model on six 2-dimensional datasets presented in [36] and used as a comparison
point in a few recent works [37]. The input points are marked as black crosses. Our model retrieves the
correct expression in all but one of the cases: in Jin5, the prediction matches the input points correctly,
but extrapolates badly.

18

0.00.20.40.60.81.0Black-box median R2101102103104105Formula complexityAFPAFP_FEAIFeynmanAdaBoostBSRDSREPLEXFEATFFXGP-GOMEAITEAKernelRidgeLGBMLinearMLPMRGPOperonOursOurs (no ref)Ours (skel)RandomForestSBP-GPXGBgplearn0.00.20.40.60.81.0Feynman mean accuracy (R2>0.99)102103104AFPAFP_FEAIFeynmanBSRDSREPLEXFEATFFXGP-GOMEAITEAMRGPOperonOursOurs (no ref)Ours (skel)SBP-GPgplearnDL-based SRGP-based SRML methods42024420242.5*x401.3*x30+0.5*x211.7*x14202442024Predicted020040060080010001200140016000200400600800100012001400160042024420248.0*x20+8.0*x3115.04202442024Predicted10007505002500250500750100010007505002500250500750100042024420240.2*x300.5*x0+0.5*x311.2*x14202442024Predicted806040200204060808060402002040608042024420241.5*exp(x0)+5.0*cos(x1)4202442024Predicted02550751001251501752002250408012016020024028032036042024420246.0*sin(x0)*cos(x1)4202442024Predicted5.003.752.501.250.001.252.503.755.0086420246842024420241.35*x0*x1+5.5*sin((x01.0)*(x11.0))4202442024Predicted30.022.515.07.50.07.515.022.530.030.022.515.07.50.07.515.022.530.0Figure 13: Performance metrics on black-box datasets.

Figure 14: Performance metrics on Strogatz datasets.

19

0.00.20.40.60.81.0OperonFEATSBP-GPOursEPLEXXGBGP-GOMEALGBMRandomForestITEAAdaBoostKernelRidgeAFPAFP_FEOurs (skel)Ours (no ref)FFXgplearnMLPDSRMRGPLinearBSRAIFeynmanMean R2102104Formula complexity100102104Inference time (seconds)0.00.20.40.60.81.0OperonGP-GOMEASBP-GPAFP_FEAFPFEATBSRFFXEPLEXOursgplearnDSRMRGPITEAOurs (skel)Ours (no ref)AIFeynmanMean R2101102103104Formula complexityTarget Noise0.00.0010.010.1101102103104Inference time (seconds)(a) Black-box

(b) Feynman

Figure 15: Performance metrics on SRBench, separated by input dimension.

Figure 16: Median R2 of our method without refinement on black-box datasets when B = 1,
varying the number of decoded function samples. The beam search [33] used in [6] leads to
low-diversity candidates in our setup due to expressions differing only by small modifications of the
coefficients.

20

2345678910Input dimension0.00.20.40.60.81.0R2algorithmOursOperonDSRgplearn123456789Input dimension0.50.60.70.80.91.0R2algorithmOursOperonDSRgplearn020406080100Number of samples C0.00.10.20.30.40.50.60.7R2Decoding strategySamplingBeam search