0
2
0
2

p
e
S
1

]
L
P
.
s
c
[

3
v
8
2
2
6
0
.
9
0
9
1
:
v
i
X
r
a

IR2Vec: LLVM IR based Scalable Program Embeddings

S. VENKATAKEERTHY, ROHIT AGGARWAL, SHALINI JAIN, MAUNENDRA SANKAR
DESARKAR, and RAMAKRISHNA UPADRASTA, Indian Institute of Technology Hyderabad
Y. N. SRIKANT, Indian Institute of Science

We propose IR2Vec, a Concise and Scalable encoding infrastructure to represent programs as a distributed
embedding in continuous space. This distributed embedding is obtained by combining representation learning
methods with flow information to capture the syntax as well as the semantics of the input programs. As
our infrastructure is based on the Intermediate Representation (IR) of the source code, obtained embeddings
are both language and machine independent. The entities of the IR are modeled as relationships, and their
representations are learned to form a seed embedding vocabulary. Using this infrastructure, we propose two
incremental encodings: Symbolic and Flow-Aware. Symbolic encodings are obtained from the seed embedding
vocabulary, and Flow-Aware encodings are obtained by augmenting the Symbolic encodings with the flow
information.

We show the effectiveness of our methodology on two optimization tasks (Heterogeneous device mapping
and Thread coarsening). Our way of representing the programs enables us to use non-sequential models
resulting in orders of magnitude of faster training time. Both the encodings generated by IR2Vec outperform
the existing methods in both the tasks, even while using simple machine learning models. In particular, our
results improve or match the state-of-the-art speedup in 11/14 benchmark-suites in the device mapping task
across two platforms and 53/68 benchmarks in the Thread coarsening task across four different platforms.
When compared to the other methods, our embeddings are more scalable, is non-data-hungry, and has better
Out-Of-Vocabulary (OOV) characteristics.

CCS Concepts: • Software and its engineering → Compilers; General programming languages; •
Computing methodologies → Machine learning; Knowledge representation and reasoning.

Additional Key Words and Phrases: LLVM, Intermediate Representations, Representation Learning, Compiler
Optimizations, Heterogeneous Systems

1 INTRODUCTION

With the growth of computing, comes the growth in computations. These computations are
necessarily the implementation of well-defined algorithms [CLRS09] as programs. Running these
programs on the rapidly evolving diverse architectures poses a challenge for compilers (and
optimizations) for exploiting the best performance. Because most of the compiler optimizations are
NP-Complete or undecidable [Muc97, Ric53], most of the modern compilers use carefully hand-
written heuristics for extracting superior performance of these programs on various architectures.
Several attempts have been made to improve the optimization decisions by using machine
learning algorithms instead of relying on sub-optimal heuristics. Some of such works include
prediction of unroll factors [SA05], inlining decisions [SCWK13], determining thread coarsening
factor [MDO14], Device mapping [GWO13], vectorization [MYP+19, HAAW+20] etc. For such
optimization applications, it is crucial to extract information from programs so that it can be used
to feed the machine learning models to drive the optimization decisions. The extracted information
should be in a form that is amenable to learning so as to improve the optimizations on the input
programs.

Authors’ addresses: S. VenkataKeerthy, cs17m20p100001@iith.ac.in; Rohit Aggarwal, cs18mtech11030@iith.ac.in; Shalini Jain,
cs15resch11010@iith.ac.in; Maunendra Sankar Desarkar, maunendra@cse.iith.ac.in; Ramakrishna Upadrasta, ramakrishna@
cse.iith.ac.in, Indian Institute of Technology Hyderabad, Hyderabad, 502 285; Y. N. Srikant, srikant@iisc.ac.in, Indian Institute
of Science, Bangalore, 560 012.

 
 
 
 
 
 
2

VenkataKeerthy, et al.

Primarily, there are two ways of representing programs as inputs to such machine learning
algorithms – Feature-based representations and Distributed representations. Feature-based rep-
resentation involves representing programs using hand-picked features—designed by domain
experts [FKM+11, GWO13, MDO14]—specific for the particular downstream applications. Exam-
ples for the features could be the number of basic blocks, number of branches, number of loops,
and even the derived/advanced features like arithmetic intensity. On the other hand, representation
learning involves using a machine learning model to automatically learn to represent the input as a
distributed vector [BCV13]. This learned representation—encoding of the program—is often called
as program embedding. Such a distributed representation is a real-valued vector whose dimensions
cannot be distinctly labelled, as opposed to that of feature-based representations.

However, most of the existing works on using distributed learning methods to represent programs
use some form of Natural Language Processing for modeling; all of them exploit the statistical
properties of the code and adhere to the Naturalness hypothesis [ABDS18]. These works primarily
use Word2Vec methods like skip-gram and CBOW [MCCD13], or encoder-decoder models like
Seq2Seq [SVL14] to encode programs as distributed vectors.

It can be noted that most of the existing representations of programs have been designed for
software engineering based applications. This includes algorithm classification [MLZ+16, RZM11],
code search and recommendation [KBL+17, BOL14, CLK+19, LYB+19], code synthesis [RSK17],
Bug detection [WCMAT16], Code summarization [IKCZ16], and Software maintenance [ABBS14,
ABBS15, APS16, GPKS17]. We, however, believe that a carefully designed embedding that can encode
the semantic characteristics of the program can be highly useful in making optimization decisions,
in addition to being applied for software engineering.

In this paper, we propose IR2Vec, an agglomerative approach for constructing a continuous,
distributed vector to represent source code at different (and increasing) levels of IR hierarchy - In-
struction, Function and Program. The vectors that are formed lower down the (program abstraction)
hierarchy is used to build the vectors at higher levels.

We make use of the LLVM compiler infrastructure [LA04] to process and analyze the code. The
input program is converted to LLVM-Intermediate Representation (LLVM-IR), a language and
machine-independent format. The initial vector representations of the (entities of the) IR, called
seed embeddings, is learned by considering its statistical properties in a Representation Learning
framework. Using these learned seed embeddings, hierarchical vectors for the new programs are
formed. To represent Instruction vectors, we propose two flavors of encodings: Symbolic and
Flow-Aware. The Symbolic encodings are generated directly from the learned representations. When
augmented with the flow analyses information, the Symbolic encodings become Flow-Aware.

We show that the generic embeddings of IR2Vec provide superior results when compared to
the previous works like DeepTune [CPWL17], Magni et al. [MDO14], and Grewe et al. [GWO13]
that were designed to solve specific tasks. We also compare IR2Vec with NCC by Ben-Nun et
al. [BNJH18]; both have a similar motivation in generating generic embeddings using LLVM IR,
though using different methodologies/techniques.

We demonstrate the effectiveness of the obtained encodings by answering the following Research

Questions (RQ’s) in the later sections:

RQ1: How well do the seed embeddings capture the semantics of the entities in LLVM IR?
As the seed embeddings play a significant role in forming embeddings at higher levels of
Program abstraction, it is of paramount importance that they capture the semantic meaning of the
entities to differentiate between different programs. We show the effectiveness of the obtained seed
embeddings in Sec. 5.1.

RQ2: How good are the obtained embeddings for solving diverse compiler optimization
applications? We show the richness of our embeddings by applying it for different tasks: (a)

IR2Vec: LLVM IR based Scalable Program Embeddings

3

Heterogeneous device mapping, and (b) Prediction of thread coarsening factor in Sec. 5.2 and 5.3
respectively.

RQ3: How scalable is our proposed methodology when compared to other methods? We
discuss various aspects by which our encoding is more scalable than the others in Sec. 6. We show
that IR2Vec has improved training time, and is non-data-hungry. Also, IR2Vec does not encounter
Out Of Vocabulary (OOV) words. These are the words that have not been exposed during the
training phase, and hence are not part of the seed embedding vocabulary, but are encountered
during test/inference phase.

Contributions: The following are our contributions:
• We propose a unique way to map LLVM-IR entities to real-valued distributed embeddings,

called a seed embedding vocabulary.

• Using the above seed embedding vocabulary, we propose a Concise and Scalable encoding

infrastructure to represent programs as vectors.

• We propose two embeddings: Symbolic and Flow-Aware that are strongly based on clas-
sic program flow analysis theory and evaluate them on two compiler optimizations tasks:
Heterogeneous device mapping and Thread coarsening.

• Our novel methodology of encodings is highly scalable and performs better than the state-
of-the-art techniques. We achieve an improved training time (upto 8000× reduction), our
method is non-data-hungry, and it does not encounter Out-Of-Vocabulary (OOV) words.
The paper is organized as follows: In Sec. 2, we discuss various related works and categorize them.
In Sec. 3, we give the necessary background information. In Sec. 4, we explain the methodology
for constructing the Symbolic and Flow-Aware encodings at various levels. In Sec. 5, we show
our experimental setup followed by the discussion of results: first, we discuss the effectiveness
of our seed embeddings, followed by our analysis and discussion on device mapping, and thread
coarsening. In Sec. 6, we discuss our perspectives of IR2Vec, focusing on training time, time to
generate encodings and OOV issues. Finally, in Sec. 7, we conclude the paper.

2 RELATED WORKS

Modeling code as a distributed vector involves representing the program as a vector, whose individual
dimensions cannot be distinctly labeled. Such a vector is an approximation of the original program,
whose semantic meaning is “distributed“ across multiple components. In this section, we categorize
some of the existing works that model codes, based on their representations, the applications that
they handle, and the embedding techniques that they use. Then, we discuss the details of some
specific recent works in this theme.

2.1 Representations, Applications and Embeddings

Representations. Programs are represented using standard syntactic formats like lexical to-
kens [ABBS15, APS16, CPWL17], Abstract Syntax Trees (ASTs) [BGEC20, AZLY18, RVK15], and
standard semantic formats like Program Dependence Graphs [ABK18], and abstracted traces [HLLR18].
Then, a neural network model like RNN or its variants is trained on the representation to form
distributed vectors.

We use LLVM IR [LLV18] as the base representation for learning the embeddings in high di-
mensional space. To the best of our knowledge, we are the first ones to model the entities of the
IR—Opcodes, Operands and Types—in the form of relationships and to use a translation based
model [BUGD+13] to capture such multi-relational data in higher dimensions.

Applications. In the earlier works, the training to generate embeddings was either application-
specific or programming language-specific: Allamanis et al. [ABBS15] propose a token-based neural

4

VenkataKeerthy, et al.

probabilistic model for suggesting meaningful method names in Java; Cummins et al. [CPWL17]
propose the DeepTune framework to create a distributed vector from the tokens obtained from code
to solve the optimization problems like thread coarsening and device mapping in OpenCL; Alon et
al. [AZLY19] propose code2vec, a methodology to represent codes using information from the AST
paths coupled with attention networks to determine the importance of a particular path to form
the code vector for predicting the method names in Java; Mou et al. [MLZ+16] propose a tree-based
CNN model to classify C++ programs; Gupta et al. [GPKS17] propose a token-based multi-layer
sequence to sequence model to fix common C program errors by students; Other applications
like learning syntactic program fixes from examples [RSD+17], bug detection [PS18, WSS18] and
program repair [XWY+17] model the code as an embedding in a high dimensional space followed
by using RNN like models to synthesize fixes. The survey by Allamanis et al. [ABDS18] covers
more such application-specific approaches.

On the other hand, our approach is more generic, and both application and programming
language independent. We show the effectiveness of our embeddings on two optimization tasks
(device mapping and thread coarsening) in Sec. 5. We believe that the scope of our work can be
extended beyond these applications, including program classification, code search, prediction of
vectorization/unroll factors, etc.

Embedding techniques. In encoding, entities are transformed into any numerical form amenable
to learning, while in embedding, it is transformed to real-valued high dimensional vectors. Sev-
eral attempts [ATGW15, BNJH18, VY16] have been made to represent programs as distributed
vectors in continuous space using word embedding techniques for diverse applications. Henkel
et al. [HLLR18] use the Word2Vec embedding model to generate the representation of a program
from symbolic traces. They generate and expose embeddings for the program [BNJH18, HLLR18],
or the embeddings themselves become an implicit part of the training for the specific downstream
task [AZLY19, PS18].

Our framework exposes a hierarchy of representations at the various levels of the program
- Instruction, Function and Program level. Our approach is the first one to propose using seed
embeddings. More significantly, we are the first ones to use program analysis (flow-analysis)
driven approaches—not machine learning based approaches—to form the agglomerative vectors of
programs beginning from the base seed encodings.

2.2 Similar works

The closest to our work is Ben-Nun et al.’s Neural Code Comprehensions (NCC) [BNJH18], who
represent programs using LLVM IR. They use skip-gram model [MSC+13] on conteXtual Flow
Graph (XFG), which models the data/control flow of the program to represent IR. The skip-gram
model is trained to generate embeddings for every IR instruction (inst2vec). So as to avoid Out Of
Vocabulary (OOV ) statements, they maintain a large vocabulary, one which uses large (> 640M)
number of XFG statement pairs. A more thorough comparison of our work with NCC [BNJH18]
(along with DeepTune [CPWL17]) is given in Sec. 6.

The recent work by Brauckmann et al. [BGEC20] represents programs as Abstract Syntax Trees
and annotates it with the control and data flow edges. A Graph Neural Network is used to learn the
representation of the program in a supervised manner to solve the specific tasks of device mapping
and thread coarsening. We achieve better performance on both the tasks, whereas they fail to show
improvements in the prediction of the thread coarsening factor.

Another recent work is ProGraML [CFBN+20], which constructs a flow graph with data and
control flow information from the Intermediate Representation of the program. They use inst2vec

IR2Vec: LLVM IR based Scalable Program Embeddings

5

embeddings [BNJH18] to represent the nodes of the graph, and use a Gated Graph Neural Network
to represent the program.

3 BACKGROUND
3.1 LLVM and Program semantics

LLVM is a compiler infrastructure that translates source-code to machine code by performing
various optimizations on its Intermediate Representation (LLVM IR) [LA04]. LLVM IR is a typed,
well-formed, low-level, Universal IR to represent any high-level language and translate it to a wide
spectrum of targets [LLV18]. Being a successful compiler, LLVM provides easy access to existing
control and data flow analysis and lets new passes (whether analyses or transformations) to be
added seamlessly.

The building blocks of LLVM IR include Instruction, Basic block, Function and Module. Every
instruction contains opcode, type and operands, and each instruction is statically typed. A basic
block is a maximal sequence of LLVM instructions without any jumps. A collection of basic
blocks form a function, and a module is a collection of functions. This hierarchical nature of
LLVM IR representation helps in obtaining embeddings at the corresponding levels of the program.
Characterizing the flow of information which flows into (and out of) each basic block constitutes the
data flow analysis. We study the impact of using such flow information as a part of the encodings.

3.2 Representation Learning

The effectiveness of a machine learning algorithm depends on the choice of data representation
and on the specific features used. As discussed earlier, Representation Learning is a branch of
machine learning that learns the representations of data by automatically extracting the useful
features [BCV13]. Unsupervised models for learning distributed representations broadly fall under
two major categories:

(1) Context-window based embedding: Methods such as Word2Vec [MCCD13], GloVe [PSM14]

fall under this category.

(2) Knowledge graph embedding: Methods such as TransE [BUGD+13], TransR [LLS+15], TransD

[JHX+15], TransH [WZFC14] fall under this category.

The context-window-based models operate on the basis of the surrounding context. The Knowl-
edge graph embedding methods guide the learning of the entity representations based on the
relationships that they participate in.

For program representations, we feel that it is important to consider semantic relationships rather
than considering surrounding contexts; the latter could just mean the neighboring instructions.
So, we prefer knowledge graph embedding models over context window embedding approaches.
These knowledge graph embedding models use relationships to group similar datapoints together.

Knowledge Graph Representations. A Knowledge Graph (KG) is a collection of (a) entities, and (b)
relationships between pairs of entities. Let ⟨h, r , t⟩ be a triplet from the knowledge graph, where the
entities h and t are connected by relationship r , and their representations are learned as translations
from head entity h to the tail entity t using the relation r in a high-dimensional embedding space.
For the same reason, these models are termed translational.

The representations are nothing but the vectors of pre-defined dimensions that are determined
automatically by a learning method. The learning method is based on the principle that: given
(representations of) any two items from the triplet (like ⟨h, r ⟩, ⟨r , t⟩, ⟨t, h⟩), it should be possible
to compute/predict the third item. Based on the above principle, different representation learning
algorithms for knowledge graphs model the relationships between h, r , and t in the triplets in
different ways.

6

VenkataKeerthy, et al.

Using TransE. Of the many varieties available for KG embeddings, we use TransE [BUGD+13], a
translational representation learning model, which embeds h, r and t on to the same high dimensional
space. It tries to learn the representations using the relationships of the form h + r ≈ t, for a triplet
⟨h, r , t⟩.

This is achieved by following a margin based ranking loss L, where the distance between (h +r , t)
and (h′ + r , t ′) (where, ⟨h′, r , t ′⟩ are invalid triplets) is at least separated by a margin m. It is given
as:

L = (cid:213)
⟨h,r,t ⟩

(cid:213)

⟨h′,r,t ′ ⟩

[m + distance(h + r , t) − distance(h′ + r , t ′)]+

Here, [x]+ denotes the hinge loss.
Among the other Knowledge Graph models, TransE has relatively much smaller number of
parameters to be learned, and has been used successfully in various algorithms for learning
representations. Adopting this for our setting ensures that the representations can be learned
in a faster and effective manner that scales well on huge datasets [JHX+15, HCL+18].

3.3 Necessity for a LLVM based embedding for optimizations

Traditionally, machine independent compiler optimizations were always considered to be part of the
“middle-end”[Muc97] of compilers. The most successful design of this language and architecture-
independent representations has been the LLVM IR [LA04, LLV18]. Using the LLVM-infrastructure,
carefully well-crafted heuristics have been implemented in various optimization passes to help in
achieving better performance. As the power of using ML in compilers has been recognized [CST02],
many ML-driven approaches have also been proposed [JJSV07] as alternatives to these heuristics.
However, they have primarily been feature-based, which means that integrating them into the
middle-end is prone to challenges [FKM+11], both at modeling as well as engineering level.

Hence, there is a necessity for a language and architecture agnostic embedding based compiler
framework that bridges the gap between the need for ML-based compiler optimizations, and their
adoptability in compiler infrastructures. Our work tries to bridge this particular gap between
embeddings from representation learning and the LLVM compiler. We believe that our work has a
unique potential to be called as a generic compiler-based embedding.

4 CODE EMBEDDINGS

In this section, we explain our methodology for obtaining code embeddings at various hierarchy
levels of the IR. We first give an overview of the methodology and then describe the process of
embedding instructions and basic blocks (BB) by considering the program flow information to
form a cumulative BB vector. We then explain the process to represent the functions and modules
by combining the individual BB vectors to form the final Code Vector. We propose two different
incremental embeddings at the instruction level.

4.1 Overview
The overview of the proposed methodology is shown in Fig. 1. Instructions in IR can be represented
as an Entity-Relationship Graph, with the instruction entities as nodes, and the relation between
the entities as edges. A translational learning model that we discussed in Sec. 3.2 is used to learn
these relations (Sec. 4.2). The output of this learning is a dictionary containing the embeddings of
the entities and is called Seed embedding vocabulary.

The above dictionary is looked up to form the embeddings at various levels of the input program.
At the coarsest level, instruction embeddings are obtained just by using the Seed embedding

IR2Vec: LLVM IR based Scalable Program Embeddings

7

Fig. 1. Overview of IR2Vec infrastructure

vocabulary. We call such encodings as Symbolic encodings. We use the Use-Def and Reaching
definition [Hec77, Muc97] information to form the instruction vector for Flow-Aware encodings.

The instructions which are live are used to form the Basic block Vector. This process of formation
of a basic block vector using the flow analysis information is explained in Sec. 4.3. The vector
to represent a function is obtained by using the basic block vectors of the function. The Code
vector is obtained by propagating the vectors obtained at the function level with the call graph
information, as explained in Sec. 4.3.2.

4.2 Learning Seed Embeddings of LLVM IR
4.2.1 Generic tuples. The opcode, type of operation (int, float, etc.) and arguments are extracted
from the LLVM IR. This extracted IR is preprocessed in the following way: first, the identifier
information is abstracted out with more generic information, as shown in Tab. 1. Next, the Type
information is abstracted to represent a base type ignoring its width. For example, the type i32 of
LLVM IR is represented as int.

Table 1. Mapping identifiers to generic representation

Identifier
Variables
Pointers
Constants
Function names
Address of a basic block

Generic representation
VAR
PTR
CONST
FUNCTION
LABEL

4.2.2 Code triplets. From this preprocessed data, three major relations are formed: (1) TypeOf:
Relation between the opcode and the type of the instruction, (2) NextInst: Relation between the
opcode of the current instruction and opcode of the next instruction; (3) Argi : Relation between
opcode and its ith operand. This transformation from actual IR to relation (⟨h, r , t⟩ triplets) is shown
in Fig. 2(a). These triplets form the input to the representation learning model.

Example. The LLVM IR shown in Fig. 2 corresponds to a function that sums up the two integer
arguments and returns the result. The first store instruction of LLVM IR in Fig. 2(a) is of integer
type with a variable as the first operand and a pointer as the second operand. It is transformed into
the corresponding triplets involving the relations TypeOf, NextInst, Arg1, Arg2, as shown. □

TrainingTripletsSeed EmbeddingVocabularyRepresentationLearningInferenceProgramCC++FortranLLVM-IRInstructions    Instruction Vector   Basic Block Vector   Function Vector   Program VectorLiveInstructionsUpdate& KillCallInstructionLLVM-IRInstructionsFor Flow-AwareencodingsUse-Def andReaching DeﬁnitionsEncodingsEncodingsEncodingsEncodings     EncodingsPrograms forTrainingDownstream Tasks8

VenkataKeerthy, et al.

Fig. 2. Overall schematic of IR2Vec: (a) [Data Collection] Mapping LLVM IR to ⟨h, r, t⟩ triplets. (b) [Vocabulary
training] Learning representations using TransE. (c) [After Training] Obtained Seed Embedding Vocabulary.
(d) [Inference] Formation of instruction vectors for new programs.

4.2.3 Learning Seed Embedding Vocabulary. As shown in Fig. 2(b), the generated Code triplets
⟨h, r , t⟩ are used as input to the TransE learning model (Sec. 3.2). On training, the model learns the
representations of the entities forming the Seed embedding vocabulary, as shown in Fig. 2(c).

4.3 Instruction Vector
(l ) - corresponding to Opcode, Type
Let the entities of instruction l, be represented as O (l ), T (l ), Ai
and ith Argument of the instruction and their corresponding vector representations from the
learned seed embedding vocabulary be ⟦O(l )⟧, ⟦T(l )⟧, ⟦Ai

(l )⟧. Then, an instruction of format

is represented as a vector which is computed as:

(cid:68)
O (l ) T (l )A1

(l )A2

(l ) · · · An

(l )(cid:69)

Wo .⟦O(l )⟧ + Wt .⟦T(l )⟧ + Wa .

(cid:16)⟦A1
Here, Wo, Wt and Wa are scalars (∈ [0, 1]), the plus (+) denotes the element-wise vector addition
operator, and the dot (.) denotes the scalar multiplication operator. Further, the Wo, Wt and Wa are
chosen with a heuristic that gives more weightage to opcode than type, and more weightage to
type than arguments:

(l )⟧ + · · · + ⟦An

(l )⟧ + ⟦A2

(l )⟧(cid:17)

(1)

(2)
This resultant vector that represents an instruction is the Instruction vector in Symbolic encod-

Wo > Wt > Wa

ings.

Example (contd.) For the store instruction shown in Fig. 2(d), the representations of opcode
store, type IntegerTy, and arguments VAR, PTR are fetched from the seed embedding vocabulary,
and the instruction is represented as Wo .(⟦store⟧) + Wt .(⟦IntegerTy⟧) + Wa .(⟦VAR⟧ + ⟦PTR⟧). In
the same figure, we also show a similar example of the return instruction. □

(d) Formation of Instruction Vector (Symbolic)+rthTransEtrh%a.addr = alloca i32, align 4%b.addr = alloca i32, align 4store i32 %a, i32* %a.addr, align 4store i32 %b, i32* %b.addr, align 4%0 = load i32, i32* %a.addr, align 4%1 = load i32, i32* %b.addr, align 4%add = add nsw i32 %0, %1ret i32 %add(c) Seed Embedding VocabularyIntegerTyPTRVARstoreOn Trainingret(a) Mapping of LLVM IR toh,r,ttriplets(b) Learning Seed Embeddings with TransEIR2Vec: LLVM IR based Scalable Program Embeddings

9

4.3.1 Embedding Data flow information. An instruction in LLVM IR may define a variable or a
pointer that could be used in another section of the program. The set of uses of a variable (or
pointer) gives rise to the use-def (UD) information of that particular variable (or pointer) in LLVM
IR [Hec77, Muc97]. In imperative languages, a variable can be redefined; meaning, in the flow of
the program, it has a different set of lifetimes. Such a redefinition is said to kill its earlier definition.
During the flow of program execution, only a subset of live definitions would reach the use of the
variable. Those definitions that reach the use of a variable are called its Reaching Definitions.

We model the instruction vector using such flow analyses information to form Flow-Aware
encodings. Each Aj , which has been already defined, is represented using the embedding of its
reaching definitions. The Instruction Vector for a reaching definition, if not calculated, is computed
in a demand-driven manner.

Instruction Vector for Flow-Aware encodings. If RD1, RD2, ... , RDn are the reaching definitions
(l )⟧ is calculated

4.3.2
of Aj
by aggregating over all the vectors of the reaching definitions as follows:

(l ), and ⟦RDi⟧ be their corresponding representations, then, the encoding ⟦Aj

⟦Aj

(l )⟧ =

n
(cid:213)

i = 1

⟦RDi⟧

(3)

The Σ stands for the vector sum of the operands.
For the cases where the definition is not available (for example, function parameters), the generic

entity representation of "VAR" or "PTR" from the learned seed embedding vocabulary is used.

An illustration is shown in Fig. 4a, where the instructions ISour ce11 and ISour ce2 reach IT ar дet as
arguments. Here, the definition of ISour ce11 could reach the argument AT ar дet of the instruction
IT ar дet either directly or via ISour ce12. And, definition ISour ce2 reaches the argument AT ar дet of the
instruction IT ar дet directly. Hence, ⟦ATarget⟧ is computed as ⟦ISource11

⟧ + ⟦ISource2⟧.

An instruction is said to be killed when the return value of that instruction is redefined. As
LLVM IR is in SSA form [CFR+91, LA04], each variable has a single definition, and the memory
gets (re-)defined. Based on this, we categorize the instructions into two classes: ones which define
memory, and the ones that do not. The first class of instructions is Write instructions, and the
second class of instructions is Read instructions.

For each instruction, the embeddings are formed as explained above. If these embeddings cor-
respond to a write instruction, future uses of the redefined value will take the embedding of the
current instruction, instead of the embedding corresponding to its earlier (killed) definition, until
the current definition itself gets redefined. This process of Kill and Update, along with the use
of reaching definitions for forming the instruction vectors within (and across) the basic block is
illustrated in Fig. 3 (and Fig. 4) for the corresponding Control Flow Graph (CFG) respectively.

Example (contd.) The CFG in Fig. 3 corresponds to a function that takes arr and size as its two
arguments. We expand the first basic block of this CFG and show its corresponding LLVM IR. The
values of the two arguments are allocated memory in I 1 and I 2; this is followed by storing them to
the local variables, ptr and range by the store instructions in I 4 and I 5. Similarly, memory for
the loop induction variable i is allotted by I 3 and is initialized to zero by I 6. □

Here, we show the process of propagating instruction vectors within the basic block.

Example (contd.) The definition of ptr in I 1 reaches the use of ptr in I 4. So, the representation
of I 1 is used in its place as shown, instead of the generic representation of PTR. Also, the store
instruction kills the definition of ptr in I 1, as it updates the value of ptr with that of arr. Hence,
in the further uses of ptr, the representation of I 4 is used instead of I 1. The same is the case for
other store instructions in I 5 and I 6. □

10

VenkataKeerthy, et al.

Fig. 3. Illustration of generating intra basic block Instruction Vectors

Similarly, in Fig. 4, we show how the propagation of instruction vectors happens across basic
blocks. In this case, more than one definition can reach a use via multiple control paths. Hence,
as explained in Eqn. 3, all possible definitions that can potentially reach a use are considered to
represent the argument of that instruction.

Example (contd.) In Fig. 4(b), the definitions of j from I 2, I 3 and I 9 can reach argument j of
instruction I 4 without being killed. Hence we conservatively consider all three definitions for
representing the argument j in I 4 as shown in Fig. 4(c). Fig 4(b) also shows another such example
for the instruction I 5. □

This resulting instruction vector which is formed by adding the exact flow information on the

Symbolic encodings’ instruction vector is used in obtaining Flow-Aware encodings.

(a) Overview

⟦I2⟧ = Wo (⟦alloca⟧) + Wt (⟦IntegerTy⟧)

⟦I4⟧ = Wo (⟦load⟧) + Wt (⟦PointerTy⟧)+
Wa (⟦I2⟧ + ⟦I3⟧ + ⟦I9⟧)

⟦I5⟧ = Wo (⟦load⟧) + Wt (⟦PointerTy⟧)+
Wa (⟦I1⟧ + ⟦I7⟧)

(c) Instruction vectors corresponding to Fig. 4b

(b) Example CFG

Fig. 4. Illustration of generating inter basic block Instruction Vectors.

4.3.3 Resolving Circular Dependencies. While forming the instruction vectors, circular dependen-
cies between two write instructions may arise if both of them write to the same location and the
(re-)definitions are reachable from each other.

I1:	%i	=	alloca	i32I2:	%j=	alloca	i32I4:	%t0	=	load	i32,	i32*	%j	I5:	%t1	=	load	i32,	i32*	%iI6:	%x	=	add	nsw	i32	%t1,	2	I7:	store	i32	%x,	i32*	%i	I8:	%y	=	add	nsw	i32,	%t0,	3I9:	store	i32	%y,	i32*	%jI3:	store	i32	5,	i32*	%jDEF[I1]	=	{i};	KILL[I1]	=	{};	RD[i]	=	{}DEF[I2]	=	{j};	KILL[I2]	=	{};	RD[j]	=	{}DEF[I3]	=	{j};	KILL[I3]	=	{j};	RD[j]	=	{I2}DEF[I4]	=	{t0};	KILL[I4]	=	{};	RD[j]	=	{I2,	I3,	I9}DEF[I5]	=	{t1};	KILL[I5]	=	{};	RD[i]	=	{I1,	I7}DEF[I6]	=	{x};	KILL[I6]	=	{};	RD[t1]	=	{I5}DEF[I7]	=	{i};	KILL[I7]	=	{i};	RD[x]	=	{I6},	RD[i]	=	{I1,	I7}DEF[I8]	=	{y};	KILL[I8]	=	{};	RD[t0]	=	{I4}DEF[I9]	=	{j};	KILL[I9]	=	{j};	RD[y]	=	{I8},	RD[j]	=	{I2,	I3,	I9}IR2Vec: LLVM IR based Scalable Program Embeddings

11

Example. To calculate ⟦I4⟧ (the encoding of I4), in the CFG shown in Fig. 51, it can be seen that
the definitions of i from I 3 and I 7 can reach the argument i in I 4. However, ⟦I5⟧ is needed for
computing ⟦I7⟧ and is yet to be computed. But for computing ⟦I5⟧, ⟦I7⟧ is needed. Hence this
scenario results in a circular dependency. □

This problem can be solved by posing the corresponding embedding equations as a set of
simultaneous equations to a solver. For example, the embedding equations of I5 and I7 shown in
Fig. 5 would be:

⟦I4⟧ =Wo(⟦load⟧) + Wt (⟦IntegerTy⟧) + Wa(⟦I3⟧ + ⟦I7⟧)
⟦I5⟧ =Wo(⟦store⟧) + Wt (⟦IntegerTy⟧) + Wa(⟦VAR⟧) + Wa(⟦I3⟧ + ⟦I7⟧)
⟦I7⟧ =Wo(⟦store⟧) + Wt (⟦IntegerTy⟧) + Wa(⟦VAR⟧) + Wa(⟦I3⟧ + ⟦I5⟧)

(4)

RD[I7] = {I3, I5 }; But ⟦I5⟧ is not calcul at ed, yet =⇒ Calcul at e ⟦I5⟧

RD[I5] = {I3, I7 }; But ⟦I7⟧ is not calcul at ed, yet =⇒ Calcul at e ⟦I7⟧
=⇒ ⟦I5⟧ depends on ⟦I7⟧ and vice ver sa .

Fig. 5. Control Flow Graph showing circular dependency

It can be seen that Eqn. 4 is a system of linear equations, where ⟦I4⟧, ⟦I5⟧ and ⟦I7⟧ are the
unknowns that are to be calculated, while the rest of the values are known. Such embedding
equations form a system of linear equations that can be posed to a solver to find the solution.

Just like any system of linear equations, there are three cases of solutions.
(1) Unique solution: In this case, the solution is obtained in a straight forward manner.
(2) Infinitely many solutions: In this case, any one of the obtained solutions can be considered

as the result.

(3) No solution: In this case, to obtain a solution, we perturb the value of Wa as Wa = Wa − δ
so that the modified system converges to a solution, with δ chosen randomly. If the system
does not converge with the chosen value of δ , another δ could be picked, and the process
can be iterated until the system converges.

In our entire experimentation setup described in Sec. 5, we however did not encounter cases (2)

and (3).

4.4 Construction of Code vectors from Instruction vector

After computing the instruction vector for every instruction of the basic block, we compute the
cumulative Basic Block vector by using the embeddings of those instructions that are not killed.
For a basic block BBi , the representation is computed as the sum of the representations of live
m
(cid:213)

instructions LI1, LI2, ..., LIm in BBi .

⟦BBi⟧ =

⟦LIk⟧

(5)

1Though this CFG is the classic irreducibility pattern [Hec77, Muc97], it is easy to construct reducible CFGs which have
circular dependencies.

k =1

I1:	%i	=	alloca	i32I2:	%0	=	load	i32,	i32*	%iI3:	store	i32	val1,	i32*	%0I4:	%1	=	load	i32,	i32*	%iI5:	store	i32	val2,	i32*	%iI6:	%2	=	load	i32,	i32*	%iI7:	store	i32	val3,	i32*	%i12

VenkataKeerthy, et al.

Procedure getInstrVec(Instruction I, Dictionary seedEmbeddings)
if ⟦I⟧ is already computed then

return ⟦I⟧

// Fetch value of Opcode of I from seed embeddings
// Fetch value of Type of I from seed embeddings
// Initializing n-d Argument vector to zeroes

⟦O⟧ ← seedEmbeddings[Opcode(I)]
⟦T⟧ ← seedEmbeddings[Type(I)]
⟦A⟧ ← ∅
for each argument Ai ∈ I do
is a Function then

if Ai

⟦Ai⟧ ← seedEmbeddings[“FUNCTION”]
if Encoding is Flow-Aware and definition of Ai is available then

⟦Ai⟧ ← getFuncVec(Ai )

else if Ai ∈ {VAR, PTR} then

if Encoding is Symbolic or Ai is not a use of a definition then

⟦Ai⟧ ← seedEmbeddings[“VAR” or “PTR”]

else

for each reaching definition RD of Ai do

⟦RD⟧ ← getInstrVec(RD)
if ⟦RD⟧ leads to cyclic dependency then

Resolve and obtain ⟦RD⟧ as shown in 4.3.3

⟦Ai⟧ ← ⟦Ai⟧ + ⟦RD⟧

else if Ai is a CONST then

⟦Ai⟧ ← seedEmbeddings[“CONST”]
else if Ai is a address of Basic block then
⟦Ai⟧ ← seedEmbeddings[“LABEL”]

⟦A⟧ ← ⟦A⟧ + ⟦Ai⟧

return Wo ∗ ⟦O⟧ + Wt ∗ ⟦T⟧ + Wa ∗ ⟦A⟧

Procedure getFuncVec(Function F, Dictionary seedEmbeddings)
if ⟦F⟧ is already computed then

return ⟦F⟧

⟦F⟧ ← ∅
for each basic block BBi ∈ F do

⟦BBi⟧ ← ∅
for each live instruction I ∈ BBi do

⟦I⟧ ← getInstrVec(I, seedEmbeddings)
⟦BBi⟧ ← ⟦BBi⟧ + ⟦I⟧

// Initializing n-d Function vector to zeroes

// Initializing n-d Basic block vector to zeroes

⟦F⟧ ← ⟦F⟧ + ⟦BBi⟧

return ⟦F⟧
The vector to represent a function F with basic blocks BB1, BB2, . . . , BBb is calculated as the sum

of vectors of all its basic blocks as:

⟦F⟧ =

b
(cid:213)

⟦BBi⟧

(6)

i=1
Our encoding and propagation also take care of programs with function calls; the embeddings
are obtained by using the call graph information. For every function call, the function vector for
the callee function is calculated, and this value is used to represent the call instruction. For the
functions that can be resolved only during link time, we just use the embeddings obtained for the
call instruction. The final vector that is obtained encodes the function. The above procedure is
applicable for recursive function calls as well. This process of obtaining the instruction vector and
function vector is outlined in Algorithms 1 and 2.

The procedure getInstrVec (Algorithm 1) computes the vector representation for a particular
instruction I . The representations of the opcode (⟦O⟧) and type (⟦T⟧) of the instruction are fetched
from the seed embedding vocabulary. For computing the representation of ⟦A⟧, we iterate over the

IR2Vec: LLVM IR based Scalable Program Embeddings

13

list of arguments of the instruction. If the argument corresponds to a function and the definition of
that function is available, we find the representation of that function and use it as the representation
of the argument. If the definition of the function is not available, then the generic representation of
function in the seed embedding vocabulary is used to represent the argument.

If the argument is a variable or pointer, we compute the representation of all its reaching
definitions and sum them up to represent the argument, in case of flow aware encodings. For
symbolic encodings, we use a simpler procedure by using the generic representation of variable or
pointer from the seed embedding vocabulary as the representation of the argument.

For the other cases, when the argument is a constant or an address of a basic block (its label), the
corresponding generic representations are used. Finally, representations of all the arguments (of
the instruction) are summed up to compute ⟦A⟧. And, the instruction vector is computed as the
weighted sum of ⟦O⟧, ⟦T⟧ and ⟦A⟧.

The procedure getFuncVec (Algorithm 2) computes the function vector. First, the representation
of every basic block in the function is calculated. The instruction vectors corresponding to the
live instructions are calculated by making a call to the procedure getInstrVec and the basic block
vector is obtained as the element-wise sum of these instruction vectors. Which, when summed up,
forms the function vector.

If ⟦F1⟧, ⟦F2⟧, . . . , ⟦Ff ⟧ are the embeddings of the functions F1, F2, . . . , Ff in a program, then the
code vector representing the program P is calculated as the sum of the embeddings of all such
functions as:

⟦P⟧ =

f
(cid:213)

i=1

⟦Fi⟧

(7)

5 EXPERIMENTAL RESULTS

We used the SPEC CPU 17 [BLvK18] benchmarks and Boost library [Boo18] as the datasets for
learning the representations. The programs in these benchmarks are converted to LLVM IR by
varying the compiler optimization levels (-O[0--3], -Os, -Oz) at random. The ⟨h, r , t⟩ triplets
are created from the resultant IRs using the relations that were explained in Sec. 4.2. Embeddings
of such triplets are learned using an open-source implementation of TransE [HCL+18], which was
explained in Sec. 3.2.

We wrote an LLVM analysis pass to extract the generic tuples from the IR so that they can be
mapped to form triplets. There were ≈ 134M triplets in the dataset, out of which ≈ 8K relations
were unique; from these, we obtain 64 different entities whose embeddings were learnt. The training
was done with SGD optimizer for 1500 epochs to obtain embedding vectors of 300 dimensions.

These learned embeddings of 64 different entities form the seed embeddings. Additional related
information is listed in the Tab. 5. We heuristically set Wo, Wt and Wa to 1, 0.5, 0.2, respectively.
The vectors at various levels were computed using another LLVM pass.

In this section, we attempt to answer RQ1 by showing the clusters formed by the entities of
seed embedding vocabulary and RQ2 by showing the effectiveness of IR2Vec embeddings on two
different optimization tasks.

5.1 RQ1: Evaluation of Seed Embeddings

The seed embeddings are analyzed to demonstrate whether the semantic relationship between the
entities are effectively captured by forming clusters. These clusters show that IR2Vec is able to
capture the characteristics of the instructions and the arguments.

The entities corresponding to the obtained seed embeddings are categorized as groups based
on the nature of the operations they perform — Arithmetic operators containing the integer and

14

VenkataKeerthy, et al.

(a) Types

(b) Arithmetic Operations Vs. Argu-
ments

(c) Arguments Vs. Logical Operations

(d) Arithmetic Vs. Casting Operations

(e) Pointer Vs. Arithmetic Operations

(f) Terminator Vs. Logical Vs. Arith-
metic Operations

Fig. 6. Comparison of embeddings of various seed entities

floating-point type arithmetic operations, Pointer operators which access memory, Logical operators
which perform logical operations, Terminator operators which form the last instruction of the basic
block, Casting operators that perform typecasting, Type information and Arguments.

Clusters showing these groups are plotted using PCA (Principal Component Analysis), a di-
mensionality reduction mechanism to project the points from the higher to lower dimensional
space [WEG87]. Here, to visualize the 300-dimensional data in 2 dimensions, we use 2-PCA, and
the resulting clusters are shown in Fig. 6.

In Fig. 6(a), we show the relation between various types. It can be observed that vectorTy being
an aggregate type can accept any of the other first-class types and lies approximately equidistant
from integerTy, pointerTy, structTy and floatTy. And, vectorTy lies farthest from voidTy
justifying that it is unlikely that voidTy elements will be aggregated together as vectors.

In Fig. 6(b), we show that all integer based arithmetic operators are grouped together and are
distinctly separated from floating-point based operators. It can be said that the analogies between
the operators are captured. For example, the distance between (add, fadd) is similar to that of the
distance between (sub, fsub). From Fig. 6(b), 6(e) and 6(f), it can also be seen that the arithmetic
operators are distinctly separated from the arguments, pointer operators and terminator operators.
Similarly, from Fig. 6(c), we can see that the logical operators are also distinctly separated from the
arguments.

In Fig. 6(d), we show the relationship between arithmetic and casting operators. It can be
clearly seen that the integer based casting operators like trunc, zext, sext , etc. are grouped
together with integer operators, and the floating-point based casting operators like fptrunc,
fpext, fptoui, etc. are grouped together with floating-point operators. On observing Fig. 6(d)
and Fig. 6(e), it can be seen that ptrtoint and inttotpr are closer to both integer operators and
pointer operators. Fig. 6(e) also demonstrates that the arithmetic operators are clearly distinct from
pointer operators. Logical operators operate on integers, and hence they are grouped together with
integer operators, as observed in Fig. 6(f).

In summary, these clusters show that the obtained seed embeddings are indeed meaningful as

they capture intrinsic syntactic and semantic relationships of LLVM entities.

IR2Vec: LLVM IR based Scalable Program Embeddings

15

5.2 RQ2 (a): Heterogeneous Device Mapping

Grewe et al. [GWO13] proposed the heterogeneous device mapping task to map OpenCL kernels
to the optimal target device - CPU or GPU in a heterogeneous system. In this experiment, we use
the embeddings obtained by IR2Vec to map OpenCL kernels to its optimal target.

Dataset. We use the dataset provided by Ben-Nun et al. [BNJH18] for this experiment. It consists of
256 unique OpenCL kernels from seven different benchmark suites comprising of AMD SDK, NPB,
NVIDIA SDK, Parboil, Polybench, Rodinia and SHOC. Taking the kernels and varying their two
auxiliary inputs (data size and workgroup size) for each kernel, a dataset is obtained. This leads to
about 670 CPU or GPU labelled data points for each of the two GPU devices – AMD Tahiti 7970
and NVIDIA 970.

Experimental Setup. The embeddings for each kernel are computed using IR2Vec infrastructure.
Gradient boosting classifier with a learning rate of 0.5, which allows a maximum depth of up to 10
levels and 70 estimators with ten-fold cross-validation is used to train the model.

We use simpler models like gradient boosting classifier, as our embeddings are generated at
the program/function level directly without forming sequential data that need sequential neural
networks like RNNs or LSTMs. More advantages of our modelling are discussed in Sec. 6.1. Similar
to the earlier methods, we use the runtimes corresponding to the predicted device, to calculate the
speedup against the static mapping heuristics proposed by Grewe et al. [GWO13].

We compare the prediction accuracy and speedup obtained by IR2Vec across two platforms
(AMD Tahiti 7970 and NVIDIA GTX 970) with the manual feature-extraction approach of Grewe et
al. [GWO13] and the state-of-the-art methodologies of DeepTune [CPWL17], and inst2vec [BNJH18].

Accuracy. Accuracy is computed as the percentage of correct device mapping predictions for a
kernel by the model over the total number of predictions during test time. In Fig. 7, we show the
accuracy of mapping using the encodings generated by IR2Vec and other methods. Flow-Aware
and Symbolic encodings achieve an average accuracy of 91.26% and 88.72% accuracy, respectively.
In Tab. 2, we show the percentage improvement of accuracy obtained by Flow-Aware encodings
over the other methods. It can be observed that the Flow-Aware encodings achieve higher perfor-
mance over the other methods [GWO13, CPWL17, BNJH18]. On AMD Tahiti 7970, our Flow-Aware
encodings achieve the highest accuracy in all 7 benchmark suites. In addition, our Symbolic encod-
ings achieve second-highest in 4/7 benchmark-suites. Similarly, in NVIDIA GTX 970, Flow-Aware
encodings achieve the highest accuracy in 3/7 cases, and Symbolic encodings achieve the highest
or second-highest accuracy in 4/7 cases.

Along with inst2vec encodings, the NCC framework [BNJH18] also proposes the inst2vec-imm
encodings to handle immediate values. For this, they formulate four immediate value handling
methods. As there is no single consistent winner among these value handling methods, NCC
considers the best result out of them. As the benchmark-wise results are not published, we are
unable to compare our results with inst2vec-imm in detail.

Table 2. % improvement in accuracy obtained by Flow-Aware encodings when compared to the other methods

Architecture

Grewe et al.
[GWO13]

DeepTune
[CPWL17]

inst2vec
[BNJH18]

inst2vec-
imm2[BNJH18]

IR2Vec
Symbolic

AMD Tahiti 7970
NVIDIA GTX 970

26.50%
22.96%

10.93%
11.70%

12.12%
9.69%

5.38%
3.54%

2.81%
2.92%

2The numbers are quoted from the paper.

16

VenkataKeerthy, et al.

Fig. 7. Accuracy for heterogeneous device mapping task on various benchmark suites.

Speedups. In Fig. 8, we show the speedups achieved in comparison with static mapping as the
baseline on both the platforms under consideration. With Flow-Aware encodings, we achieve about
88.38% and 77.65% of the maximum speedup given by the oracle on AMD Tahiti and NVIDIA
GTX resepectively, when compared to the 70.76% on AMD Tahiti, and 70.66% on NVIDIA GTX by
inst2vec and 72.90% on AMD Tahiti and 66.79% on NVIDIA GTX by Deeptune. (With Symbolic
encodings we achieve about 80.11% and 72.03% of the maximum speedup on AMD Tahiti and
NVIDIA GTX.)

Flow-Aware encodings achieve a geometric mean of 1.58× speedup on AMD Tahiti 7970 and a
1.26× speedup on NVIDIA GTX 970; in comparison, on the AMD platform, the speedups achieved
by the state-of-the-art models of DeepTune [CPWL17] and inst2vec [BNJH18] is 1.46× and 1.4×
respectively; while, on the NVIDIA platform, both DeepTune and inst2vec achieve a comparable
1.21× speedups. Symbolic encoding also performs better than the state-of-the-art by achieving a
speedup of 1.56× on AMD Tahiti and 1.24× on NVIDIA GTX.

Table 3. % improvement in speedup obtained by Flow-Aware encodings when compared to the other methods

Architecture

Grewe et al.
[GWO13]

DeepTune
[CPWL17]

inst2vec
[BNJH18]3

IR2Vec
Symbolic

AMD Tahiti 7970
NVIDIA GTX 970

29.65%
13.77%

7.96%
4.31%

12.95%
3.93%

1.43%
1.92%

IR2Vec: LLVM IR based Scalable Program Embeddings

17

Fig. 8. Plot showing the speedups corresponding to the device mapping predictions by various methods

In Tab. 3, we show the percentage improvement of speedups obtained by Flow-Aware encodings
in comparison with the earlier methods. It can be seen that the Flow-Aware encodings perform
better on both the platforms when compared to the other works.

On benchmarks like ep (embarrassingly parallel), where mapping them to GPUs gets the optimal
performance [GWO13], we map to GPU—in all the turns of 10 fold cross-validation—and hence
achieve the highest possible speedup. Similarly, for LU, which gives an optimal performance when
mapped to CPUs [GWO13], we map the kernels that ought to be mapped to CPU correctly, with

3The inst2vec-imm results given in their paper [BNJH18] are based on arithmetic mean, whereas our results are based
on geometric mean. inst2vec-imm is reported to achieve a (arithmetic mean) speedup of 3.47× and 1.44×; whereas our
Flow-Aware encodings achieve a speedup of 3.51× and 1.47× on AMD Tahiti and on NVIDIA GTX respectively.

SpeedUp(a) AMD SDK2.2x2.5xAMD Tahiti 7970Grewe et al.DeepTuneinst2vecIR2Vec ­ SymbolicIR2Vec ­ Flow­AwareOracle ­ Max speedup1.0x1.1x1.2x1.4x0.1x0.6x2.0x2.3xNVIDIA GTX 9701.0x1.2xBinomialOptionBitonicSortBlackScholesFastWalshTransformFloydWarshallMatrixMultiplicationMatrixTransposePrefixSumReductionScanLargeArraysSimpleConvolutionSobelFilterGeo Mean0.1x0.3x0.5x0.7xSpeedUp(b) Rodinia66.0x73.0xAMD Tahiti 797031.0x41.0x1.0x2.0x3.0x4.0x5.0x3.5x4.4xNVIDIA GTX 9701.6x1.9x2.3x2.6xbackpropbfscfdgaussianhotspotkmeanslavaMDleukocyteludnnnwparticlefilterpathfinderstreamclusterGeo Mean0.1x0.5x1.0xSpeedUp(c) Polybench143.0x146.0xAMD Tahiti 79709.0x51.0x1.0x2.0x3.0x4.0x5.0x1.9x2.2xNVIDIA GTX 9700.7x0.8x1.0x1.2x1.4x1.5x2DConvolution2mm3DConvolution3mmataxbicgcorrelationcovariancegemmgesummvgramschmidtmvtsyr2ksyrkGeo Mean0.1x0.4xSpeedUp(d) SHOC14.0x17.0xAMD Tahiti 79702.5x3.5x4.5x0.2x0.5x0.8x1.0x1.3x3.8x4.0xNVIDIA GTX 9701.0x1.5x2.0x2.5x3.0xBFSFFTGEMMMDMD5HashReductionS3DScanSortSpmvStencil2DTriadGeo Mean0.1x0.6x18

VenkataKeerthy, et al.

Fig. 8. (Contd.) Plot showing the speedups corresponding to the device mapping predictions by various
methods

very high confidence (of about 95%) in various turns of 10 fold cross-validation. In comparison,
DeepTune and inst2vec map LU to CPUs with a confidence of 77% and 85%, respectively.

Slowdown. Our predictions using Flow-Aware encodings result in the least number of slowdowns
both at the level of benchmarks and benchmark-suites. Our predictions result in a slowdown in
18/142 of benchmarks across two platforms; in comparison, inst2vec and DeepTune result in a
slowdown in 33 and 32 benchmarks, respectively. Also, at an aggregate benchmark-suite level,
the prediction using Flow-Aware encodings leads to a slowdown in 3/14 cases across platforms, in
comparison inst2vec and DeepTune result in slowdowns in three and six cases, respectively.

5.3 RQ2 (b): Prediction of optimal Thread Coarsening factor

Thread coarsening [VD08] is the process of increasing the work done by a single thread by fusing
two or more concurrent threads. Thread coarsening factor corresponds to the number of threads
that can be fused together. Selection of an optimal thread coarsening factor can lead to significant
improvements [MDO13] in the speedups on GPU devices and a naive coarsening would lead to a
substantial slowdown.

A thread coarsening factor of a kernel that gives the best speedup on a GPU could give the
worst performance with the same coarsening factor on another device (either within or across
vendors) because of the architectural characteristics of the device [MDO14, SF18]. For example,
nbody kernel, which has a higher degree of Instruction Level Parallelism, can be better exploited
by VLIW based AMD Radeon than SIMD based AMD Tahiti [MDO14].

Dataset. In this experiment, we follow the experimental setup proposed by Magni et al. [MDO14] to
predict the optimal thread coarsening factor—among {1, 2, 4, 8, 16, 32}—for a given kernel specific
to a GPU device. Even for this experiment, we use the dataset provided by Ben-Nun et al. [BNJH18].
It consists of about 68 datapoints from 17 OpenCL kernels on 4 different GPUs – AMD Radeon
5900, AMD Tahiti 7970, NVIDIA GTX 480 and NVIDIA Tesla K20c. These kernels are collectively
taken from AMD SDK, NVIDIA SDK and Parboil benchmarks. A datapoint consists of the kernel
and its runtime corresponding to each thread coarsening factor on a particular GPU device.

SpeedUp(e) NPB9.7x10.5xAMD Tahiti 79704.2x5.0x5.8x6.5x1.0x1.5x2.0x2.3x3.4x3.6xNVIDIA GTX 9702.5x3.0xBTCGEPFTLUMGSPGeo Mean0.7x0.8x1.0x1.1x1.3xSpeedUp(f) Parboil56.0x58.5xAMD Tahiti 797021.0x30.0x1.0x2.5x4.0x5.5x6.5x2.1x2.2xNVIDIA GTX 9700.7x1.0x1.3x1.5x1.7xbfscutcplbmsadspmvstencilGeo Mean0.0x0.1xSpeedUp(g) NVIDIA SDK6.0x8.5xAMD Tahiti 79701.0x1.5x2.0x2.5x3.0x0.3x0.5x1.2x1.3x1.4x1.5xNVIDIA GTX 9701.0xDotProductFDTD3dMatVecMulMatrixMulMersenneTwisterVectorAddGeo Mean0.2x0.4x0.6x(h) Geo Mean ­ AMD Tahiti 79701.00x1.15x1.30x1.45x1.60xSpeedup(i) Geo Mean ­ NVIDIA GTX 9701.00x1.06x1.12x1.18x1.24x1.30xIR2Vec: LLVM IR based Scalable Program Embeddings

19

Experimental Setup. Even for this task, we use gradient boosting classifier instead of LSTMs and
RNNs to predict the coarsening factor for the four GPU targets. For this experiment, we set the
learning rate as 0.05 with 140 decision stumps with 1 level, as the number of data points in the
dataset is very low. We use ten-fold cross-validation for measuring the performance.

Table 4. % improvement in speedup obtained by Flow-Aware encodings when compared to the other methods

Architecture

Magni et al.
[GWO13]

DeepTune
[CPWL17]

DeepTune-
TL
[CPWL17]

inst2vec
[BNJH18]4

IR2Vec
Symbolic

Radeon

AMD
5900
AMD Tahiti 7970
NVIDIA GTX 480
NVIDIA
Tesla
K20c

27.66%

25.41%
45.31%
52.84%

5.26%

5.26%

4.35%

29.37%
25.21%
15.41%

36.56%
18.89%
11.98%

18.17%
23.89%
11.98%

–

2.08%
3.98%
0.18%

Speedups. In Fig. 9, we show the speedups achieved by our encodings and earlier works on four
different platforms – AMD Radeon HD 5900, AMD Tahiti 7970, NVIDIA GTX 480 and NVIDIA
Tesla K20c.

On AMD Radeon, both of our encodings achieve a speedup of 1.2× when compared to the
state-of-the-art speedup of 1.15× and 1.14× achieved by inst2vec [BNJH18] and DeepTune model
with transfer learning (DeepTune-TL) [CPWL17]. In AMD Tahiti, Flow-Aware encodings achieve a
speedup of 1.23×; Symbolic encoding achieves a speedup of 1.2×, whereas the earlier works by
DeepTune-TL and inst2vec achieve a speedup of 0.9× and 1.04× respectively. In Tab. 4, we show the
percentage improvement of speedup obtained by Flow-Aware encodings over other methodologies.
From the table, we can infer that Flow-Aware gives better results for every architecture when
compared to the other methods.

We are the first ones to achieve a positive speedup on NVIDIA GTX 480; our Flow-Aware and
Symbolic encodings obtain a speedup of 1.18× and 1.13× respectively when compared to 0.95×
and 0.99× speedup achieved by inst2vec and DeepTune-TL. We get a speedup of 1.13× with both
of our proposed encodings on NVIDIA Tesla K20c. In contrast, DeepTune-TL and inst2vec obtain a
speedup of 1.01× on this platform.

On average, it can be seen that both encodings outperform the earlier methods for prediction of

the thread coarsening factor on all the four platforms under consideration.

Slowdown. Magni et al. [MDO14] observe that spmv and mvCoal kernels have irregular depen-
dences that causes a poor response to coarsening, and hence no performance improvement for
them is possible. For these kernels, IR2Vec obtains the baseline speedup without resulting in a slow-
down. In contrast, the earlier models result in negative speedups (Deeptune results in a slowdown
upto 0.36 × in AMD Radeon and inst2vec results in a slowdown of upto 0.63 × in AMD Radeon
and NVIDIA GTX). The same argument applies for stencil kernel (an iterative Jacobi stencil on
3D-grid), where the coarsening leads to slowdown (except in NVIDIA GTX), while IR2Vec still
obtain the baseline speedup.

When compared to the other methods, we obtain the best speedup on about 70% of the kernels
on all platforms. It can be observed that the Flow-Aware encodings rarely lead to slowdowns; this
happens in only 8/68 cases (17 benchmark-suits, across 4 platforms), even on these eight cases, the

4As per the results given in the NCC paper [BNJH18], inst2vec-imm achieves a (arithmetic) mean of 1.28×, 1.18×, 1.11×
and 1× speedup on AMD Radeon, AMD Tahiti, NVIDIA GTX and NVIDIA Tesla; whereas our Flow-Aware encodings achieve
a (arithmetic) mean of 1.25×, 1.3×, 1.26× and 1.16× respectively.

20

VenkataKeerthy, et al.

Fig. 9. Plot showing the speedups achieved by predicted coarsening factors by various methods

speedup is still close—within 10%—of the baseline. Whereas, predictions by inst2vec and DeepTune-
TL result in a slowdown in 18 and 21 cases. We believe that this is because of the flow information
associated with the obtained vectors.

6 RQ3: IR2VEC- PERSPECTIVES
We discuss some perspectives on IR2Vec. We answer RQ3 by doing a scalability study.

6.1 Training characteristics

By design, training with IR2Vec embeddings takes lesser training time. This is because our frame-
work has the flexibility to model the embeddings as non-sequential data at program or function
level. Whereas, other methods are limited to modelling the input programs only as sequential data,
and hence are bound to using sequential models like LSTMs and RNNs. Using such models will
involve training more number of parameters than non-sequential models like Gradient Boosting.

Device Mapping. Training for the device mapping task by IR2Vec takes ≈ 5 seconds on a P100
GPU, when compared to about 10 hours and 12 hours of training time taken by DeepTune [CPWL17]
and NCC [BNJH18] respectively. This results in a reduction of about ≈ 7200×–8640× in training
time without a reduction in performance.

The earlier works take much time for training because they involve training a large number
of parameters: DeepTune [CPWL17] uses ≈ 77K parameters, while NCC [BNJH18] uses ≈ 69K
parameters. In contrast, the IR2Vec predictions use Gradient Boosting, which is a collection of a
small number of shallow decision trees. This reduction in time is primarily possible because the

(a) AMD Radeon HD 5900binarySearchblackscholesconvolutiondwtHaar1DfastWalshfloydWarshallmriQmtmtLocalmvCoalmvUncoalnbodyreducesgemmsobelspmvstencilGeo Mean00.511.522.53SpeedupMagni et al.DeepTuneDeepTune-TLinst2vecIR2Vec - SymbolicIR2Vec - Flow-AwareOracle - Max speedup(b) AMD Tahiti 7970binarySearchblackscholesconvolutiondwtHaar1DfastWalshfloydWarshallmriQmtmtLocalmvCoalmvUncoalnbodyreducesgemmsobelspmvstencilGeo Mean00.511.522.53SpeedupbinarySearchblackscholesconvolutiondwtHaar1DfastWalshfloydWarshallmriQmtmtLocalmvCoalmvUncoalnbodyreducesgemmsobelspmvstencilGeo MeanSpeedup00.511.52(c) NVIDIA GTX 4803(d) NVIDIA Tesla K20cbinarySearchblackscholesconvolutiondwtHaar1DfastWalshfloydWarshallmriQmtmtLocalmvCoalmvUncoalnbodyreducesgemmsobelspmvstencilGeo Mean00.511.5SpeedupIR2Vec: LLVM IR based Scalable Program Embeddings

21

Fig. 10. Comparison of time taken to generate the Symbolic and Flow-Aware encodings from Seed Embedding
Vocabulary

embeddings obtained by IR2Vec enable us effectively use the Gradient boosting algorithm instead
of the compute-intensive and data-hungry neural networks (LSTM in this case) that do not fit well
in the cache hierarchy.

Thread Coarsening. Even for the thread coarsening task, our model takes lesser time of ≈10
seconds for training when compared to ≈11 hours of training time needed by DeepTune-TL and ≈1
hour of training time needed (and 77K and 69K parameters used) by DeepTune and NCC approaches.
This results in ≈ 360×–3960× reduction of training time, and again, achieving good speedups.

6.2 Symbolic vs. Flow-Aware

The seed embedding vocabulary captures intrinsic syntactic and semantic relations at the entity
level of the LLVM IR (Sec. 5.1). Hence, we believe that the primary strength of our encodings comes
from the seed embedding vocabulary. This directly leads to the Symbolic encodings that achieve
better performance than the other earlier methods on average. When the Symbolic encodings
are augmented with flow information of the program, it results in Flow-Aware encodings. These
encodings result in much more informative representation and hence lead to better accuracy and
speedup than all other methods. This is evident from the results shown in Sec. 5.

However, this improvement in accuracy comes with a minimal overhead. Generating the Flow-
Aware encodings take more time than Symbolic encoding, as it accounts for the time taken to
generate and propagate the program flow information and to resolve circular dependencies in
this process. We did an analysis of time taken by both of these encodings on a sample set of
straightforward programs involving the family of sorting, searching, dynamic and greedy programs
obtained from an online collection of programs [Gee03].

The comparison of time taken to generate the Symbolic and Flow-Aware encodings from the
Seed Embedding Vocabulary on the sample set is shown in Fig. 10. It can be observed that, on
an average, Flow-Aware encodings take 1.86 times more than that of Symbolic encodings. Their
memory representations are of the same size, as both of them result in a floating-point vector in
the same n-dimensions (300 dimensions for our setting).

12001400Flow-AwareSymbolic350400Time Taken in msbirthday-paradoxtower-of-hanoilargest-sum-contiguous-subarrayrotate-bits-of-an-integermin-cost-pathnth-fibonacci-numberlongest-increasing-subsequencesort-n-numbers-linear-timedfa-based-divisionlexicographic-rank-of-a-stringinsertion-sort-linked-listpermutations-stringlargest-independent-setchannel-assignmentanagram-substringrod-cuttingn-queenbinary-insertion-sortkth-smallest-largest-elementm-coloringmergeSort_LinkedListedit-distancequicksort-for-linked-listbinomial-coefficientmaximum-sum-increasing-subsequencek-closest-elementsmerge-sort-for-doubly-linked-listknights-tourhamiltonian-cycle-backtrackingkmpquicksort-linked-listlongest-palindrome-substringrabin-karpIterative_QuickSortsudokupalindrome-partitioningboyer-mooresubset-sumtransitive-closure-of-a-graphlongest-bitonic-subsequenceshortest-common-supersequencefloyd-warshalldfspartition-problembellman-fordvertex-cover-problemcycle-in-a-graphcycle-undirected-graphbox-stackingoptimal-binary-search-treekaratsubabfstrie-suffixeslongest-palindromic-subsequencematrix-chain-multiplicationbiconnectivitycut-vertices.cppeulerian-path-and-circuitgraph-coloringeuler-circuit-directed-graphboruvkasweighted-job-schedulingtopological-sortingtarjanaho-corasickstrongly-connected-componentslongest-path-directed-acyclic-graphNearly-sorted-algominimum-cut-in-a-directed-graphkasaiboolean-parenthesization-problemPrograms05010015020025022

VenkataKeerthy, et al.

Table 5. Comparison Matrix: DeepTune vs. NCC vs. IR2Vec

Comparison metric DeepTune [CPWL17]
Primary embedding Token and Character level
Files examined
Vocabulary size
Entities examined
Vocabulary training
time

Handpicked vocabulary
128 symbols
Application specific

Task dependent

NCC [BNJH18]
Instruction level
24,030
8,565 statement embeddings
≈640M XFG Statement Pairs ≈134M Triplets

IR2Vec
Entity level
13,029
64 entity embeddings

200 hrs on a P100

20 mins on a P100

6.3 Exposure to OOV words

For learning the representations of programs, the training phase of any method often involves
learning a vocabulary that contains the embeddings corresponding to the input. When an unseen
combination of underlying input constructs is encountered during inference, it would not be a
part of the vocabulary and leads to Out Of Vocabulary (OOV ) data points. In such cases, most of
the models treat all the OOV words in a similar manner by assigning a common representation
ambiguously, which may result in performance degradation.

Hence, to avoid OOV points, it is important to expose various and large (all possible) combinations
of the underlying entities during the training phase. For example, for generating the embeddings at
a statement-level of IR, all combinations of opcodes, types and arguments that can potentially form a
statement should be exposed during training. Similarly, for generating token-based embeddings, all
possible tokens that can possibly be encountered must have been exposed at the training time. But,
both of these approaches would lead to a huge training space of O(|opcodes | × |types | × |arдuments |)
and O(|tokens |) (where |tokens | can potentially be unbounded, with tokens being used more in the
sense of a lexeme [ALSU06]) respectively. As it can be seen, covering such a huge intractable space
is infeasible, and hence undesirable.

Consequently, the methods that learn statement level representations like NCC [BNJH18] face
OOV issues. However, DeepTune [CPWL17] uses a Token and Character based hybrid approach to
overcome this issue. DeepTune's method involves usage of the embeddings corresponding to the
token if it is present in vocabulary. Else, they break the token as a series of characters and use the
corresponding embeddings of the character.

On the other hand, IR2Vec forms embeddings at the entity level of the IR, and hence it is sufficient
to expose a training space of only O(|opcodes | + |types | + |arдuments |) to avoid OOV points. With
this insight, it can be seen that IR2Vec can avoid the OOV issue even on exposure to a smaller
number of programs at training time, when compared to the other approaches. Consequently, this
results in a smaller vocabulary, and hence achieving better performance than the other methods. A
comparison of IR2Vec with DeepTune and NCC with respect to training and vocabulary is shown
in Tab. 5.

A comparison of the number of OOV entities encountered by NCC and IR2Vec on the same set
of programs used in Sec. 6.2 is shown in Fig. 11. It can be seen that our method does not encounter
any OOVs even when exposed to lesser training data, thereby achieving good scalability.

7 CONCLUSIONS AND FUTURE WORK
We proposed IR2Vec, a novel LLVM-IR based framework that can capture the implicit characteristics
of the input programs in a task-independent manner. The seed embeddings were formed by
modelling IR as relations, and the encoding was obtained by using a translational model. This
encoding was combined with liveness, use-def, and reaching definition information, to form vectors
at various levels of the program abstraction like instruction, function and module. Overall, this
results in two encodings, which we term as Symbolic and Flow-Aware.

IR2Vec: LLVM IR based Scalable Program Embeddings

23

Fig. 11. Comparison of the number of OOV entities encountered by NCC and IR2Vec

When compared to earlier approaches, our approach of representing programs is non data-hungry,
takes less training time of up to 8640×, while maintaining a small vocabulary of only 64 entities. As
we use entity level seed embeddings, we do not encounter any OOV issues. We demonstrate the
effectiveness of the obtained encodings on two different tasks and obtain superior performance
results while achieving high scalability when compared with various similar approaches.

We envision that our framework can be applied to other applications beyond the scope of this
work. IR2Vec can be extended to classify whether a program is malicious or not by looking for
suspicious and obfuscated patterns. It can also be applied for detecting codes with vulnerabilities,
and to identify the patterns of code and replace them with its optimized equivalent library calls. It
can even be extended to aide in key optimizations like the prediction of vectorization, interleaving,
and unrolling factors. We also plan to extend the device mapping and thread coarsening experiments
with more datasets and on newer platforms.

The source code and other relevant material are available in http://www.compilers.cse.iith.ac.in/

research/ir2vec.

ACKNOWLEDGMENTS

We are grateful to Suresh Purini, Dibyendu Das, Govindarajan Ramaswamy and Albert Cohen, for
their valuable feedback on our work at various stages. We also thank Swapnil Dewalkar, Akash
Banerjee and Rahul Utkoor for the thoughtful discussions in the early stages of this work. We would
like to thank the anonymous reviewers of ACM TACO for their insightful and detailed comments
which helped in improving the paper.

This research is funded by the Department of Electronics & Information Technology and the
Ministry of Communications & Information Technology, Government of India. This work is partially
supported by a Visvesvaraya PhD Scheme under the MEITY, GoI (PhD-MLA/04(02)/2015-16),
an NSM research grant (MeitY/R&D/HPC/2(1)/2014), a Visvesvaraya Young Faculty Research
Fellowship from MeitY (MEITY-PHD-1149), and a faculty research grant from AMD.

REFERENCES

[ABBS14] Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. Learning natural coding conventions. In
Proc. of the 22nd ACM SIGSOFT International Symp. on Foundations of Software Engineering, FSE 2014, pages
281–293, USA, 2014. ACM.

[ABBS15] Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. Suggesting accurate method and class
names. In Proc. of the 2015 10th Joint Meeting on Foundations of Software Engineering, ESEC/FSE 2015, pages

150015501600#OOV in IR2Vec#OOV in NCC11001200#OOVnth-fibonacci-numberbirthday-paradoxlongest-bitonic-subsequencelongest-increasing-subsequencepalindrome-partitioninglongest-palindromic-subsequencematrix-chain-multiplicationrod-cuttingbinary-insertion-sortlexicographic-rank-stringpartition-problembinomial-coefficientmaximum-sum-increasing-subsequencek-closest-elementsboolean-parenthesizationkmpsubset-sumlargest-sum-contiguous-subarrayrotate-bits-of-integertower-of-hanoirabin-karpoptimal-binary-search-treeiterative-QuickSortmin-cost-pathshortest-common-supersequenceWord-wrapsort-n-numbers-linear-timetransitive-closure-of-graphlongest-palindrome-substringpermutations-of-stringanagram-substringfloyd-warshallboyer-moorechannel-assignmentm-coloringn-queenknights-touredit-distancehamiltonian-cycle-backtrackingZ-algorithmUnion-findsudokubox-stackinginsertion-sort-linked-listlargest-independent-setkth-smallest-largest-elementmergeSort_linked-listquicksort-double-linked-listkaratsubabellman-fordboruvkasquicksort-linked-listmerge-sort-for-doubly-linked-listvertex-coverdfscycle-in-a-graphcycle-undirected-graphcut-verticesbiconnectivitytrie-suffixesgraph-coloringbfseulerian-path-and-circuiteuler-circuit-directed-graphmin-cut-in-a-directed-graphaho-corasickNearly-sorted-algotarjantopological-sortingstrongly-connected-componentslongest-path-directed-acyclic-graphkasaiPrograms010020030040024

VenkataKeerthy, et al.

38–49, USA, 2015. ACM.

[ABDS18] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. A survey of machine learning for

big code and naturalness. ACM Computing Surveys (CSUR), 51(4):81, 2018.

[ABK18] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs with

graphs. In International Conference on Learning Representations, 2018.

[ALSU06] Alfred V. Aho, Monica S. Lam, Ravi Sethi, and Jeffrey D. Ullman. Compilers: Principles, Techniques, and Tools

(2nd Edition). Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 2006.

[APS16] Miltiadis Allamanis, Hao Peng, and Charles A. Sutton. A convolutional attention network for extreme
summarization of source code. In Proceedings of the 33nd International Conference on Machine Learning, ICML
2016, New York City, NY, USA, June 19-24, 2016, pages 2091–2100, 2016.

[ATGW15] Miltiadis Allamanis, Daniel Tarlow, Andrew Gordon, and Yi Wei. Bimodal modelling of source code and
natural language. In Proceedings of the 32nd International Conference on Machine Learning, volume 37 of
Proceedings of Machine Learning Research, pages 2123–2132, Lille, France, 07–09 Jul 2015. PMLR.
[AZLY18] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. A general path-based representation for predicting
program properties. In Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design
and Implementation, PLDI 2018, pages 404–419, New York, NY, USA, 2018. ACM.

[AZLY19] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. Code2vec: Learning distributed representations of

code. Proc. ACM Program. Lang., 3(POPL):40:1–40:29, January 2019.

[BCV13] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives.

IEEE Trans. Pattern Anal. Mach. Intell., 35(8):1798âĂŞ1828, August 2013.

[BGEC20] Alexander Brauckmann, Andrés Goens, Sebastian Ertel, and Jeronimo Castrillon. Compiler-based graph
representations for deep learning models of code. Proc. of the 29th International Conference on Compiler
Construction, pages 201–211, Feb 2020.

[BLvK18] James Bucek, Klaus-Dieter Lange, and Jóakim v. Kistowski. Spec cpu2017: Next-generation compute bench-
mark. In Companion of the 2018 ACM/SPEC International Conference on Performance Engineering, ICPE ’18,
pages 41–42, NY, USA, 2018. ACM.

[BNJH18] Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten Hoefler. Neural code comprehension: A learnable
representation of code semantics. In Proceedings of the 32Nd International Conference on Neural Information
Processing Systems, NIPS’18, pages 3589–3601, USA, 2018. Curran Associates Inc.

[BOL14] Sushil Bajracharya, Joel Ossher, and Cristina Lopes. Sourcerer: An infrastructure for large-scale collection

and analysis of open-source code. Sci. Comput. Program., 79:241–259, January 2014.

[Boo18] Boost. Boost C++ Libraries. https://www.boost.org/, 2018. Accessed 2019-05-16.

[BUGD+13] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Durán, Jason Weston, and Oksana Yakhnenko. Translating
embeddings for modeling multi-relational data. In Proceedings of the 26th International Conference on Neural
Information Processing Systems - Volume 2, NIPS’13, pages 2787–2795, USA, 2013. Curran Associates Inc.

[CFBN+20] Chris Cummins, Zacharias V Fisches, Tal Ben-Nun, Torsten Hoefler, and Hugh Leather. Programl: Graph-based

deep learning for program optimization and analysis. arXiv preprint arXiv:2003.10536, 2020.

[CFR+91] Ron Cytron, Jeanne Ferrante, Barry K Rosen, Mark N Wegman, and F Kenneth Zadeck. Efficiently computing
static single assignment form and the control dependence graph. ACM Transactions on Programming Languages
and Systems (TOPLAS), 13(4):451–490, 1991.

[CLK+19] Jose Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen, and Satish Chandra. When deep learning met code
search. In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, ESEC/FSE 2019, page 964âĂŞ974, New York, NY, USA,
2019. Association for Computing Machinery.

[CLRS09] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms,

Third Edition. The MIT Press, 3rd edition, 2009.

[CPWL17] Chris Cummins, Pavlos Petoumenos, Zheng Wang, and Hugh Leather. End-to-end deep learning of optimiza-
tion heuristics. In 2017 26th International Conference on Parallel Architectures and Compilation Techniques
(PACT), pages 219–232. IEEE, 2017.

[CST02] Keith D. Cooper, Devika Subramanian, and Linda Torczon. Adaptive optimizing compilers for the 21st century.

J. Supercomput., 23(1):7âĂŞ22, August 2002.

[FKM+11] Grigori Fursin, Yuriy Kashnikov, Abdul Wahid Memon, Zbigniew Chamski, Olivier Temam, Mircea Namolaru,
Elad Yom-Tov, Bilha Mendelson, Ayal Zaks, Eric Courtois, Francois Bodin, Phil Barnard, Elton Ashton, Edwin
Bonilla, John Thomson, Christopher K. I. Williams, and Michael O’Boyle. Milepost gcc: Machine learning
enabled self-tuning compiler. International Journal of Parallel Programming, 39(3):296–327, Jun 2011.
[Gee03] GeeksforGeeks. C/C++/Java programs. https://www.geeksforgeeks.org, 2003. Accessed 2019-08-15.

IR2Vec: LLVM IR based Scalable Program Embeddings

25

[GPKS17] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. Deepfix: Fixing common c language errors by
deep learning. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI’17, pages
1345–1351. AAAI Press, 2017.

[GWO13] Dominik Grewe, Zheng Wang, and Michael F. P. O’Boyle. Portable mapping of data parallel programs to opencl
for heterogeneous systems. In Proceedings of the 2013 IEEE/ACM International Symposium on Code Generation
and Optimization, CGO 2013, Shenzhen, China, February 23-27, 2013, pages 22:1–22:10. IEEE Computer Society,
2013.

[HAAW+20] Ameer Haj-Ali, Nesreen K. Ahmed, Ted Willke, Yakun Sophia Shao, Krste Asanovic, and Ion Stoica. Neu-
rovectorizer: End-to-end vectorization with deep reinforcement learning. In Proceedings of the 18th ACM/IEEE
International Symposium on Code Generation and Optimization, CGO 2020, page 242âĂŞ255, New York, NY,
USA, 2020. Association for Computing Machinery.

[HCL+18] Xu Han, Shulin Cao, Xin Lv, Yankai Lin, Zhiyuan Liu, Maosong Sun, and Juanzi Li. OpenKE: An open
toolkit for knowledge embedding. In Proceedings of the 2018 Conference on Empirical Methods in Natural
Language Processing: System Demonstrations, pages 139–144, Brussels, Belgium, November 2018. Association
for Computational Linguistics.

[Hec77] Matthew S. Hecht. Flow Analysis of Computer Programs. Elsevier Science Inc., New York, NY, USA, 1977.
[HLLR18] Jordan Henkel, Shuvendu K. Lahiri, Ben Liblit, and Thomas Reps. Code vectors: Understanding programs
through embedded abstracted symbolic traces. In Proceedings of the 2018 26th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2018,
page 163âĂŞ174, NY, USA, 2018. ACM.

[IKCZ16] S Iyer, I Konstas, A Cheung, and L Zettlemoyer. Summarizing source code using a neural attention model.
In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 2073–2083, 2016.

[JHX+15] G Ji, S He, L Xu, K Liu, and J Zhao. Knowledge graph embedding via dynamic mapping matrix. In Proceedings
of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Volume 1: Long Papers), pages 687–696, Beijing, China, July 2015.
Association for Computational Linguistics.

[JJSV07] P. J. Joseph, Matthew T. Jacob, Y. N. Srikant, and Kapil Vaswani. Statistical and machine learning techniques
in compiler design. In Y. N. Srikant and Priti Shankar, editors, The Compiler Design Handbook: Optimizations
and Machine Code Generation, Second Edition. CRC Press, 2007.

[KBL+17] V Kashyap, D B Brown, B Liblit, D Melski, and T Reps. Source forager: a search engine for similar source

code. arXiv preprint arXiv:1706.02769, 2017.

[LA04] Chris Lattner and Vikram Adve. Llvm: A compilation framework for lifelong program analysis & transforma-
tion. In Proceedings of the international symposium on Code generation and optimization: feedback-directed and
runtime optimization, page 75. IEEE Computer Society, 2004.

[LLS+15] Y Lin, Z Liu, M Sun, Y Liu, and X Zhu. Learning entity and relation embeddings for knowledge graph
completion. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAIâĂŹ15, page
2181âĂŞ2187. AAAI Press, 2015.

[LLV18] LLVM. LLVM Language Reference. https://llvm.org/docs/LangRef.html, 2018. Accessed 2019-08-20.
[LYB+19] Sifei Luan, Di Yang, Celeste Barnaby, Koushik Sen, and Satish Chandra. Aroma: Code recommendation via

structural code search. Proc. ACM Program. Lang., 3(OOPSLA), October 2019.

[MCCD13] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in

vector space. arXiv preprint arXiv:1301.3781, 2013.

[MDO13] Alberto Magni, Christophe Dubach, and Michael F. P. O’Boyle. A large-scale cross-architecture evaluation
of thread-coarsening. In William Gropp and Satoshi Matsuoka, editors, International Conference for High
Performance Computing, Networking, Storage and Analysis, SC’13, Denver, CO, USA - November 17 - 21, 2013,
pages 11:1–11:11. ACM, 2013.

[MDO14] Alberto Magni, Christophe Dubach, and Michael O’Boyle. Automatic optimization of thread-coarsening for
graphics processors. In Proc. of the 23rd international conference on Parallel Architectures and Compilation,
pages 455–466. ACM, 2014.

[MLZ+16] L Mou, G Li, L Zhang, T Wang, and Z Jin. Convolutional neural networks over tree structures for programming
language processing. In Proc. of the 13th AAAI Conf. on Artificial Intelligence, AAAI’16, pages 1287–1293.
AAAI Press, 2016.

[MSC+13] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representations of
words and phrases and their compositionality. In Proceedings of the 26th International Conference on Neural
Information Processing Systems - Volume 2, NIPS’13, pages 3111–3119, USA, 2013. Curran Associates Inc.

26

VenkataKeerthy, et al.

[Muc97] Steven S. Muchnick. Advanced Compiler Design and Implementation. Morgan Kaufmann Publishers Inc., San

Francisco, CA, USA, 1997.

[MYP+19] C Mendis, C Yang, Y Pu, S Amarasinghe, and M Carbin. Compiler auto-vectorization with imitation learning.
In Advances in Neural Information Processing Systems 32 (NeurIPS), pages 14598–14609. Curran Associates,
Inc., Dec 2019.

[PS18] Michael Pradel and Koushik Sen. Deepbugs: A learning approach to name-based bug detection. Proc. ACM

Program. Lang., 2(OOPSLA):147:1–147:25, October 2018.

[PSM14] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation.
In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages
1532–1543, 2014.

[Ric53] H. G. Rice. Classes of recursively enumerable sets and their decision problems. Transactions of the American

Mathematical Society, 74(2):358–366, 1953.

[RSD+17] R Rolim, G Soares, L D’Antoni, O Polozov, S Gulwani, R Gheyi, R Suzuki, and B Hartmann. Learning syntactic
program transformations from examples. In Proceedings of the 39th International Conference on Software
Engineering, ICSE ’17, pages 404–415, Piscataway, NJ, USA, 2017. IEEE Press.

[RSK17] Maxim Rabinovich, Mitchell Stern, and Dan Klein. Abstract syntax networks for code generation and semantic

parsing. arXiv preprint arXiv:1704.07535, 2017.

[RVK15] V Raychev, M Vechev, and A Krause. Predicting program properties from "big code". In Proc. of the 42nd
Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL ’15, pages 111–124,
USA, 2015. ACM.

[RZM11] N Rosenblum, X Zhu, and B P. Miller. Who wrote this code? identifying the authors of program binaries. In
Proceedings of the 16th European Conference on Research in Computer Security, ESORICS’11, pages 172–189,
Berlin, 2011. Springer-Verlag.

[SA05] M. Stephenson and S. Amarasinghe. Predicting unroll factors using supervised classification. In International

Symposium on Code Generation and Optimization, pages 123–134, March 2005.

[SCWK13] Douglas Simon, John Cavazos, Christian Wimmer, and Sameer Kulkarni. Automatic construction of inlining
heuristics using machine learning. In Proceedings of the 2013 IEEE/ACM International Symposium on Code
Generation and Optimization (CGO), CGO âĂŹ13, page 1âĂŞ12, USA, 2013. IEEE Computer Society.
[SF18] Nicolai Stawinoga and Tony Field. Predictable thread coarsening. ACM Trans. Archit. Code Optim., 15(2), June

2018.

[SVL14] I Sutskever, O Vinyals, and Q V. Le. Sequence to sequence learning with neural networks. In Proceedings of the
27th International Conference on Neural Information Processing Systems - Volume 2, NIPS’14, pages 3104–3112,
Cambridge, USA, 2014. MIT Press.

[VD08] Vasily Volkov and James W. Demmel. Benchmarking gpus to tune dense linear algebra. In Proceedings of the

2008 ACM/IEEE Conference on Supercomputing, SC âĂŹ08. IEEE Press, 2008.

[VY16] Martin Vechev and Eran Yahav. Programming with "big code". Found. Trends Program. Lang., 3(4):231–284,

December 2016.

[WCMAT16] S Wang, D Chollak, D Movshovitz-Attias, and L Tan. Bugram: Bug detection with n-gram language models.
In Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering, ASE 2016,
pages 708–719, NY, USA, 2016.

[WEG87] Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis. Chemometrics and intelligent

laboratory systems, 2(1-3):37–52, 1987.

[WSS18] Ke Wang, Rishabh Singh, and Zhendong Su. Dynamic neural program embeddings for program repair. In
6th International Conference on Learning Representations, ICLR 2018, Vancouver, Canada, Apr 30 - May 3, 2018,
Conference Track Proceedings, 2018.

[WZFC14] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by translating
on hyperplanes. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, AAAIâĂŹ14,
page 1112âĂŞ1119. AAAI Press, 2014.

[XWY+17] Y. Xiong, J. Wang, R. Yan, J. Zhang, S. Han, G. Huang, and L. Zhang. Precise condition synthesis for program
repair. In 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE), pages 416–426, May
2017.

