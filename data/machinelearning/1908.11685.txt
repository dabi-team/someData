Using LSTMs to Model the Java Programming
Language

Brendon Boldt

Marist College,
3399 North Rd. Poughkeepsie, NY, USA
brendon.boldt1@marist.edu

Abstract. Recurrent neural networks (RNNs), speciﬁcally long-short
term memory networks (LSTMs), can model natural language eﬀectively.
This research investigates the ability for these same LSTMs to perform
next “word” prediction on the Java programming language. Java source
code from four diﬀerent repositories undergoes a transformation that pre-
serves the logical structure of the source code and removes the code’s var-
ious speciﬁcities such as variable names and literal values. Such datasets
and an additional English language corpus are used to train and test stan-
dard LSTMs’ ability to predict the next element in a sequence. Results
suggest that LSTMs can eﬀectively model Java code achieving perplex-
ities under 22 and accuracies above 0.47, which is an improvement over
LSTM’s performance on the English language which demonstrated a per-
plexity of 85 and an accuracy of 0.27. This research can have applicability
in other areas such as syntactic template suggestion and automated bug
patching.

1

Introduction

Machine learning techniques of language modeling are often applied to natu-
ral languages, but techniques used to model natural languages such as n-gram,
graphed-based, and context sensitive models can be applicable to programming
languages as well [1] [2] [3]. One such application of a language model is next-
word prediction which can prove very useful for tasks such as syntactic template
suggestion and bug patching [2] [4]. There has been research into programming
language models which use Bayesian statistical inference (n-gram models) to per-
form next-word prediction [1]. Yet some of the most successful natural language
models have been built using recurrent neural networks (RNNs); their ability to
remember information over a sequence of tokens makes them particularly apt
for next-word prediction [5].

Speciﬁcally, long-short term memory (LSTM) RNNs have further improved
the basic RNN model by increasing the ability of an RNN to remember data

This is a pre-print of an article published in Artiﬁcial Neural Networks and Machine
Learning – ICANN 2017. The ﬁnal authenticated version is available online at:
https://doi.org/10.1007/978-3-319-68612-7_31

9
1
0
2

g
u
A
6
2

]
E
S
.
s
c
[

1
v
5
8
6
1
1
.
8
0
9
1
:
v
i
X
r
a

 
 
 
 
 
 
2

Brendon Boldt

over a long sequence of input without the signal decaying quickly [5]. LSTMs
are a sequence-to-word language model which means given a sequence of words
(e.g., words in the beginning of a sentence), the model will produce a probability
distribution describing what the next word in the sequence is.

In terms of the Java programming language, we are speciﬁcally investigating
next-statement prediction in method bodies. While other parts of Java source
code (e.g., class ﬁelds, import statements) do have semantic signiﬁcance, method
bodies make up the functional aspect of source code1 and most resemble nat-
ural language sentences. Just as individual semantic tokens (words) comprise
natural language sentences, statements, which can be thought of as semantic to-
kens, comprise method bodies. Furthermore, the semantics of individual natural
language words coalesce to form the semantics of sentence just as the seman-
tics of the statement in a method body form the semantics of the method as
a whole. By this analogy, language modeling techniques which operate on sen-
tences comprised of words could apply similarly to method bodies comprised of
statements.

2 Tokenizing Java Source Code

We are speciﬁcally looking at predicting the syntactic structure of the next state-
ment in within Java source code method bodies. The syntactic structure of a
complete piece of source code can be represented as an abstract syntax tree
(AST) where each node of the tree represents a distinct syntactic element (e.g.,
statement, boolean operator, literal integer). Method bodies are, in particular,
comprised of statements which, more or less, represent a self-contained action.
Each of these statements is the root of its own sub-AST which represents the
syntactic structure of only that statement. In this way, statements are indepen-
dent, semantically meaningful units of a method body which are suitable to be
tokenized for input into the RNN.

Nguyen et al. [2] studied a model for syntactic statement prediction called
ASTLan which uses Bayesian statistical inference to interpret and predict state-
ments in the form of sequential statement ASTs. While Bayesian statistical in-
ference can be applied to statements directly in their AST form, RNNs operate
on independent tokens such as English words. Thus, it is necessary that state-
ment ASTs be ﬂattened into a tokenized form in order to produce an RNN-based
model.

2.1 Statement-Level AST Tokenization

The RNN model described in Zaremba et al. [5] speciﬁcally uses space-delimited
text strings; hence, when the statement ASTs are tokenized, they must be rep-
resented as space-delimited text strings.

1 Functional insofar as method bodies describe the active (non-declarative) behavior

of the program.

Using LSTMs to Model the Java Programming Language

3

To show the tokenization of Java source, take the following Java statement:
int x = obj.getInt();. The corresponding AST, as given by the Eclipse AST
parser, appears in Figure 1 [6]. This statement, in turn, would be transformed
as follows2:

Fig. 1. The abstract syntax tree (AST) representation of of the Java statement int x
= obj.getInt();

_PrimitiveType_VariableDeclarationFragment(_SimpleName
_MethodInvocation(_SimpleName_SimpleName)))

_60(_39_59(_42_32(_42_42)))

The ﬁrst token uses the AST node names while second token represents the
same AST by instead using integer IDs corresponding to the AST node names
as assigned by the Eclipse parser (e.g., 60 corresponds to “PrimitiveType” nodes
and 42 corresponds to “SimpleName” nodes). Using integer IDs saves space and
is the format used in the actual LSTM.

Individual AST nodes are separated by underscores (“ ”) and parentheses
are used to denote a parent-child relationship so that the tree structure of the
statement is preserved. In fact, it is possible to recreate the syntax of the original
source code from the tokens; thus, this tokenization is lossless in terms of syn-
tactical information yet lossy in other areas. For example, variable and function
names are discarded during the translation to make the model independent of
variable and function names.

2.2 Method-Level Tokenization

Consider the following Java method:

int foo() {

int x = obj.getInt();
if (x > 0) {

x = x + 5;

2 VariableDeclarationStatement is not included in the tokenized version of the AST
since the syntax is adequately represented by starting with the root node’s children.

4

Brendon Boldt

}
return x;

}

Each statement in the method body is tokenized just as the single statement
was above, and the resulting tokens are delimited using spaces. Braces, while
not statements, are included (denoted by “{” and “}”) to retain the semantic
structure of the method body. The method above becomes the following sequence
of tokens:

(_39_42 { _60(_39_59(_42_32(_42_42)))
_25(_27(_42_34) { _21(_7(_42_27(_42_
34))) } _41(_42) }

The sequence of these tokens forms a “sentence” which the represents body of
a Java method. Sentences in the dataset are separated by the <eos> metatoken
to mark the end of a sentence. These sentences of tokens will then comprise the
corpus that the LSTM network uses to train and make predictions.

2.3 English and Java Source Corpora Used

Similarly to Zaremba et al. [5], we are using the Penn Treebank (PTB) for the
English language corpus as it provides an eﬀective, general sample of the English
language. For the Java programming languages, four diﬀerent corpora were each
built by processing (as described above) a large repository of Java source code.
The repositories used were the Java Development Kit (JDK), Google Guava,
ElasticSearch, and Spring Framework. The JDK is a good reference for Java
since it is a widely-used implementation of the Java language; the other three
projects were selected based on their high popularity on GitHub in addition to
the fact they are Java-based projects.

It is important to note that the PTB does not contain any punctuation
while the tokenized Java source contains “punctuation” only in the form of
statement body-delimiting curly braces (“{” and “}”) since these are integral
to the semantic structure of source code. All English and Java corpora use a
metatoken to mark the end of a sentence.

2.4 Vocabulary Comparison

In addition to preserving the logical structure of the source code, another goal of
the speciﬁc method of tokenization was to produce a vocabulary with a frequency
distribution similar to that of the English corpus. If the same Java statement
tokens appear too frequently, the tokenization might be generalizing the Java
source too much such that it loses the underlying semantics. If the statement
tokens, instead, all have a very low frequency it would be diﬃcult to eﬀectively
perform inference on the sequence of tokens within the allotted vocabulary size.
In all of the Java corpora, the left and right curly braces comprise approxi-
mately 35% of the total tokens present. This a disproportionately high number

Using LSTMs to Model the Java Programming Language

5

Table 1. Proportion and rank of the metatoken <unk>. Proportions and ranks are from
the adjusted Java corpora with the left and right curly braces removed.

Corpus
PTB
JDK
Guava
ElasticSearch
Spring Framework

Proportion Rank

0.0484
0.0724
0.0476
0.1618
0.0873

2
2
5
2
2

in comparison to the rest of the tokens, but removing them from the frequency
distribution, since they classify as punctuation, gives a more accurate represen-
tation of the vocabularies. The adjusted frequency distribution shown in Figure
2 compares the PTB to the JDK source code. The rate of occurrence for the
highest ranked words is signiﬁcantly higher in the JDK than in the PTB, but
the frequency distributions track closely together beyond the ﬁfth-ranked words.
Generally, all four Java corpora showed similar frequency distributions.

The statistical similarities between the English and the translated Java cor-
pora suggest that the Java statement tokens have an adequate amount of detail
in terms of mimicking English words. If the Java statement tokens were too
detailed, their frequencies would be far lower than those of English words; if
the Java statement tokens were not detailed enough, their frequencies would be
much higher than those of English words.

0.2

0.1

y
c
n
e
u
q
e
r
F

0

0

PTB
JDK

5

10

15
Word Rank

20

25

30

Fig. 2. Comparison of English and Java word frequency distributions. The y-axis rep-
resents the total proportion of the word with a given rank (speciﬁed by the x-axis).

Another consideration when comparing the English and Java corpora is the
prevalence of the metatoken <unk> which denotes a token not contained in the
language model’s vocabulary. Due to the nature of LSTMs, the vocabulary of
the language model is ﬁnite; hence, any word not contained in the vocabulary is
considered unknown. We speciﬁcally used a vocabulary size of 10, 000. A vocab-
ulary size which is too small will fail to represent enough words in the corpus;
the result is the LSTM seeing a high proportion of the <unk> metatoken. A vo-

6

Brendon Boldt

cabulary which is too large increases the computation required during training
and inference. The proportion of <unk> tokens in both the English and the Java
source data sets (save for ElasticSearch3) are < 10% which indicates that the
10, 000 word vocabulary accounts for approximately 90% of the corpus’ words
by volume. It is important that the Java corpora’s <unk> proportion is not sig-
niﬁcantly higher than that of the PTB since that would suggest that 10, 000 is
too small a vocabulary size to describe the tokenized Java source code.

3 Language Modeling

3.1 Neural Network Structure and Conﬁguration

In order to make a good comparison between language modeling in English and
Java, a model with demonstrated success at modeling English was chosen. The
model selected was an LSTM neural network, a type of RNN, as described in
Zaremba et al. [5]. This particular LSTM uses regularization via dropout to act
as a good language model for natural languages such as English [5].

The LSTM’s speciﬁc conﬁguration was the same as the “medium” conﬁgura-
tion described in Zaremba et al. [5] with the exception that the data was trained
for 15 epochs instead of 39 epochs. Beyond 15 epochs (on both the English and
Java datasets), the training cost metric (perplexity) continued to decrease while
the validation cost metric remained steady. This suggests that the model was
beginning to overﬁt the training data and that further training would not im-
prove performance on the test data. Speciﬁcally, this model contains two RNN
layers with a vocabulary size of 10, 000 words.

Each corpus was split into partitions such that 80% was training data and
the remaining 20% was split evenly between test and validation data. Perplexity,
the performance metric of the LSTM, is determined by the ability of the LSTM
to perform sequence-to-word prediction on the test set of that corpus. Perplex-
ity represents how well the prediction (in the form of a probability distribution)
given by the LSTM matches the actual word which comes next in the sentence.
A low perplexity means that the language model’s predicted probability distri-
bution matched closely the actual probability distribution, that is, it was better
able to predict the next word. Perplexity is the same metric that is used in
Zaremba et al. [5] to compare language models.

3.2 Language Model Metrics

We chose word-level perplexity was chosen as the metric for comparing the lan-
guage models’ performance on the given corpora since it provides a good mea-
surement of the model’s overall ability to predict words in the given corpus
[7]. Perplexity for a given model is calculated by exponentiating (base e) the
mean cross-entropy across all words in the test set. This is formally expressed
as follows:
3 ElasticSearch had a proportion of 16%

Using LSTMs to Model the Java Programming Language

7

Table 2. Perplexities (P ) given by Equation 1. Proportion of predictions which had
the correct word in their top-k predictions. “ElasticSearch” is written as “ES” and
“Spring Framework” is written as “SF”.

Corpus
PTB
JDK
Guava
ES
SF

P
85.288
21.808
18.678
11.397
11.318

Top 1 Top 5 Top 10 Language
0.269
0.474
0.519
0.576
0.560

English
Java
Java
Java
Java

0.552
0.716
0.751
0.784
0.783

0.470
0.652
0.696
0.739
0.722

N

1
N

P (L) = exp

H(L, wi)

,

(1)

!

i=1
X
where N is the test data set size, L is the language model, wi is the ith word
in the test set, and H(L, wi) is the natural log cross-entropy from wi to the
prediction given by L(wi). A lower perplexity represents a language model with
better predictive performance [8].

The cross-entropy is the opposite of summing the product of the probability
of that word appearing, i.e., 1 for the correct word and 0 for all other incorrect
words, and the natural logarithm of the output value of LSTM’s softmax layer.
The cross-entropy is deﬁned as follows:

V

H(L, w) = −

p(wi) ln L(wi) ,

(2)

i=1
X

where V is the vocabulary size and p(wi) is the probability of wi being the
correct word. Since the probability for incorrect words is 0 and the correct word
is 1, the sum can be reduced to −1 times the natural log of the probability of
the correct word as given by the LSTM. Thus, the cross-entropy is simply

H(L, w) = − ln Lw(w) .

(3)

Lw(w) represents the LSTM’s softmax output speciﬁcally for the word w.
Additionally, mean word-level accuracy was calculated for each language model
considering the top 1, 5, and 10 predictions made by the model.

4 Results

The perplexities achieved on the corpora by the LSTM are displayed in Table 2.
The smallest perplexity for non-English data sets was measured for the Spring
Framework, while the largest was for the JDK data. The table also indicates
that all four Java data sets showed a drastic reduction in perplexity compared

 
8

Brendon Boldt

to the English data set. Nonetheless, the perplexity achieved on the English
dataset is similar to that reported by Zaremba et al. [5]. These results indicate
the superiority of LSTMs on both programming languages and a language as
complex as the English language.

Table 2 shows the top-k accuracies for each corpus. Clearly, the results sug-
gest that the proposed LSTM model is able to more accurately model pre-
processed Java source code than it can English. The table also indicates that,
for the English data set, the use of a large number of predictors can dramatically
increase the overall rate of predictors with the correct next word; e.g., increasing
from one to ten predictors at least doubled the proportion of predictors. There
is a similar eﬀect over Java-based data sets; however, in these data sets the
predictors start at a higher proportion than with English.

5 Conclusion

In this paper, we have presented a way of modeling a predictive strategy over
the Java programming language using an LSTM. Using datasets such as PTB,
JDK, Guava, ElasticSearch, and Spring Framework we have shown that LSTMs
are suitable in predicting the next syntactic statements of source code based
on preceding statements. Results indicate that indicate that LSTMs can achieve
lower perplexities and, hence, produce more accurate models on the Java datasets
than the English dataset.

The pre-processed Java code represents a very general and cursory represen-
tation of the original code as it does not include anything such as variable names
or variable types. Future research along these lines could account for informa-
tion such as variable types, variable names, etc. It would also be beneﬁcial to
compare the modeling of Java with other programming languages or to train the
model across multiple repositories in one language.

References

1. Miltiadis Allamanis and Charles Sutton. Mining source code repositories at massive
scale using language modeling. In Proceedings of the 10th Working Conference on
Mining Software Repositories, MSR ’13, pages 207–216, Piscataway, NJ, USA, 2013.
IEEE Press.

2. Anh Tuan Nguyen and Tien N. Nguyen. Graph-based statistical language model for
code. In Proceedings of the 37th International Conference on Software Engineering
- Volume 1, ICSE ’15, pages 858–868, Piscataway, NJ, USA, 2015. IEEE Press.
3. Muhammad Asaduzzaman, Chanchal K. Roy, Kevin A. Schneider, and Daqing Hou.
A simple, eﬃcient, context-sensitive approach for code completion. Journal of Soft-
ware: Evolution and Process, 28(7):512–541, 2016. JSME-15-0030.R3.

4. Dongsun Kim, Jaechang Nam, Jaewoo Song, and Sunghun Kim. Automatic patch
generation learned from human-written patches. In Proceedings of the 2013 Inter-
national Conference on Software Engineering, ICSE ’13, pages 802–811, Piscataway,
NJ, USA, 2013. IEEE Press.

Using LSTMs to Model the Java Programming Language

9

5. Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network

regularization. CoRR, abs/1409.2329, 2014.

6. Eclipse

Foundation.

Eclipse

documentation

on

the AST class.

http://help.eclipse.org/luna/index.jsp?topic=%2Forg.eclipse.jdt.doc.isv%2Freference%2Fapi%2Forg%2Feclipse%2Fjdt%2Fcore%2Fdom%2FAST.html,
2016. Accessed: 2016-8-18.

7. Martin Sundermeyer, Hermann Ney, and Ralf Schl¨uter. From feedforward to re-
current lstm neural networks for language modeling. IEEE/ACM Transactions on
Audio, Speech and Language Processing (TASLP), 23(3):517–529, 2015.

8. Minsi Wang, Li Song, Xiaokang Yang, and Chuanfei Luo. A parallel-fusion rnn-lstm
architecture for image caption generation. In Image Processing (ICIP), 2016 IEEE
International Conference on, pages 4448–4452. IEEE, 2016.

