1
2
0
2

t
c
O
5
2

]

G
L
.
s
c
[

1
v
3
0
6
2
1
.
0
1
1
2
:
v
i
X
r
a

Common Information based Approximate State
Representations in Multi-Agent Reinforcement
Learning

Hsu Kao
University of Michigan
hsukao@umich.edu

Vijay Subramanian
University of Michigan
vgsubram@umich.edu

Abstract

Due to information asymmetry, ﬁnding optimal policies for Decentralized Par-
tially Observable Markov Decision Processes (Dec-POMDPs) is hard with the
complexity growing doubly exponentially in the horizon length. The challenge
increases greatly in the multi-agent reinforcement learning (MARL) setting where
the transition probabilities, observation kernel, and reward function are unknown.
Here, we develop a general compression framework with approximate common
and private state representations, based on which decentralized policies can be
constructed. We derive the optimality gap of executing dynamic programming
(DP) with the approximate states in terms of the approximation error parame-
ters and the remaining time steps. When the compression is exact (no error), the
resulting DP is equivalent to the one in existing work. Our general framework
generalizes a number of methods proposed in the literature. The results shed light
on designing practically useful deep-MARL network structures under the “cen-
tralized learning distributed execution” scheme.

1 INTRODUCTION

Finding optimal policies for Decentralized Partially Observable Markov Decision Processes (Dec-
POMDPs) is hard due to information asymmetry, which refers to the mismatch in the set of informa-
tion each agent has in a multi-agent environment. In fact, a ﬁnite-horizon Dec-POMDP with more
than one agent is NEXP-complete [Bernstein et al., 2002], implying a doubly exponential complex-
ity growth in the horizon length. In decentralized control theory, theoretical solutions have been
proposed to ﬁnd the optimal control laws for Dec-POMDPs. Notably among them is the common
information (CI) approach [Nayyar et al., 2013], a framework that decomposes the decision of a full
policy into the decision of a “prescription policy” from the CI known by all the agents, and the “pre-
scription” itself which is a full characterization of how the agents should act based on any realization
of their own private information (PI). This approach effectively transforms the decentralized model
back to a centralized one from the view of a ﬁctitious “coordinator” who only observes the CI, and
permits a coordinator level sequential decomposition using a belief state a’la POMDPs [Kumar and
Varaiya, 2015].

The challenge increases greatly in the multi-agent reinforcement learning (MARL) setting where the
model – transition probabilities, observation kernel, and reward function – is unknown. When the
agents learn concurrently, information asymmetry causes another issue called the “non-stationarity
issue,” since the effective environment observed by each agent is time-varying as the other agents
learn and update their policies. The issue can be alleviated in principle by the “centralized learning
and distributed execution” scheme [Dibangoye and Buffet, 2018] as the learning is from the coor-
dinator’s viewpoint; indeed, if agents only update their policies using CI, they can perfectly track

Preprint. Under review.

 
 
 
 
 
 
others’ policies. However, there is still a big gap in applying the CI approach to the MARL set-
ting. First, the Bayesian updates of the belief state in the CI approach require the knowledge of the
model, which are not available in the MARL setting. Moreover, the linear growth of length of private
histories leads to the doubly exponential growth of the space of prescriptions in time, which is ex-
plosively large even for toy-size environments and forbids any practical explorations in such space.
One natural question is whether we can restrict attention to some policies (and prescriptions) that
take some state variable as inputs without losing much performance, where the state variables en-
capsulate the crucial information relevant to future decisions in a time-invariant domain, and where
the representations (ways of encapsulation) can be learned without the knowledge of the model.

In this paper, we formulate good approximate common and private state representations for learn-
ing close-to-optimal policies in unknown ﬁnite-horizon Dec-POMDPs, where each agent receives
its own private information plus a common observation. The agents also share the same commonly
observed rewards; however, they may not know each others’ actions. We propose conditions in
Deﬁnition 3 for an approximate sufﬁcient private state (ASPS), which compresses an agent’s pri-
vate information, i.e., its action observation history (AOH), and conditions in Deﬁnition 5 for an
approximate sufﬁcient common state (ASCS), which compresses the ﬁctitious coordinator’s AOH,
with the actions being ASPS-based prescriptions and the observations being common observations.
Critically, using Theorem 4 and Theorem 6, in Theorem 7 we derive the optimality gap in terms of
the error parameters of our compression and the remaining time steps, between the values of two
dynamic programmings (DPs): one in Algorithm 1 for the optimal policy using the CI approach
without compression, with states being the complete coordinator’s AOHs and actions being the pre-
scriptions from Nayyar et al. [2013]; and the other in Algorithm 3 using our framework with states
being any valid1 ASCSs and the actions being ASPS-based prescriptions for valid ASPS. Our frame-
work generalizes a number of results in the literature: ﬁrst, it extends the approximate information
state (AIS) framework [Subramanian and Mahajan, 2019, Subramanian et al., 2020] to the multi-
agent setting; second, it extends the CI approach [Nayyar et al., 2013] and the follow-up sufﬁcient
private information (SPI) framework that compresses the private states [Tavafoghi et al., 2021], to
their general approximate state representation counterpart; third, it generalizes the work by Mao
et al. [2020] to include non-injective compressions and a general approximate common state repre-
sentation. Our results can provide guidance on designing Deep Learning (DL) structures to learn the
(compressed) state representations and the optimal policies (using learned representations) under
the centralized learning distributed execution scheme, which applies to practical ofﬂine or online
MARL settings.

Related Work. The problem of state representation is well studied in the single-agent POMDP
case. Stochastic control theory details the conditions an information state (IS) needs to satisfy so that
it acts as the Markov state in an equivalent MDP so one may only consider IS-based policies without
loss of generality [Mahajan and Mannan, 2016]; the belief state is an example of such IS [Kumar
and Varaiya, 2015]. Subramanian et al. [2020] extends the idea to an approximate information state
(AIS), where the IS conditions hold approximately; importantly, the optimality gap of running DP
with any valid AIS is quantiﬁed. Based on their AIS scheme, they propose a DL framework that
learns the AIS representation without knowing the model. Recent work on Deep Bisimulation for
Control (DBC) [Zhang et al., 2021b] in the DL literature uses similar ideas: they train an encoder
to predict well the instantaneous rewards and transitions, and use the encoder output to train the
policies. The encoder is an encapsulation or a compression. The optimality gap established is
similar to the result of the inﬁnite horizon case in Subramanian and Mahajan [2019]. There are
more representation learning schemes not requiring model knowledge in the DL or RL literature,
e.g. Ha and Schmidhuber [2019], with the bulk without theoretical guidance or guarantees.

In the multi-agent context, Nayyar et al. [2013] propose a belief IS for the coordinator using the CI
approach, without compressing agents’ private information. Tavafoghi et al. [2018] further compress
private histories to sufﬁcient private information (SPI) so that the corresponding spaces of the belief
IS and prescriptions are time-invariant. They identify conditions such that restricting attention to
SPI-based policies is without loss of optimality. However, not only do they consider a control setting
where the model is required, but also only present compression of the common history to a belief
state, which is a narrow class of compression schemes. Nevertheless, this work will be a starting
point of our work. Mao et al. [2020] consider an information state embedding that injectively maps
agents’ histories to representations in a ﬁxed domain, and quantify the effect of the embedding

1Satisfying the approximation criteria.

2

on the value function like Subramanian and Mahajan [2019]. However, their requirement that the
mapping is injective is impractical for two reasons: one, an injective mapping does not reduce
the policy complexity; and two, real world applications often demand non-injective encapsulations -
e.g., tiger [Kaelbling et al., 1998] where one IS is the number of right observations minus the number
of left observations, which is non-injective. Moreover, they also compress the common state to a
belief state, but it is unclear how this can be done in practice without model information.

Another line of work in deep-MARL literature also applies the notion of CI (also known as the com-
mon knowledge) to solving MARL problems [Schroeder de Witt et al., 2019, Foerster et al., 2019,
Lerer et al., 2019, Sokota et al., 2021]. They search for optimal policies for a Dec-POMDP when the
model is known, while we consider designing sample efﬁcient and lower regret learning algorithms
in an ofﬂine or online MARL setting for an unknown model. Moreover, many of them involve
heuristic or approximation methods without knowing the potential loss from the approximations or
apply a variety of machine learning schemes without a theoretical basis or understanding.

2 PRELIMINARIES

Notation. Let ∆(X ) denote the set of distributions on the space X , and Ω(X) denote the space
where the variable X takes values. Superscripts are used as the agent index and subscripts as the
time index. The notation X c:d
b ). In some cases
superscripts or subscripts are omitted, and if so the meaning will be clariﬁed. Capital letters are
used for random variables while lower case letters are for their realizations. For random variables X
with a realization x, we use the short hand notation P(·|x) , P(·|X = x) and E[·|x] , E[·|X = x].
If a random variable appears without realization in a place other than the operand of E, then it means
the related equation should hold for any Borel measurable subset in its domain.

a:b denotes the tuple (X c

b , . . . , X d

a, . . . , X d

a , . . . , X c

2.1 Dec-POMDP Model

Suppose there are N agents in the system. We consider the Dec-POMDP model, i.e., a tuple
(S, A, PT , R, O, PO, T, PI) where the quantities are: S is the state space; A = A1 × · · · × AN
is the joint action space whose elements are joint actions A = (A1, . . . , AN ); PT : S × A → ∆(S)
is the transition kernel mapping a current state and a joint action to a distribution of new states
PT : S × A → ∆(S); R : S × A → ∆(R) is the reward function mapping a current state and a
joint action to a probability distribution on the reals; O = O0 × O1 × · · · × ON is the joint observa-
tion space whose elements are joint observations O = (O0, O1, . . . , ON ), where O0 is commonly
observed but On is only observed by agent n; PT : S → ∆(O) is the observation kernel mapping a
current state to a distribution of joint observations; T is the time horizon; PI ∈ ∆(S) is the initial
state distribution. In comparison to the standard Dec-POMDP model [Oliehoek and Amato, 2016],
we have an additional common observation (including the reward), and our observations depend
only on the current state.
We assume S, A, O, and T are ﬁnite and known in advance, while PT , R, PO, and PI are unknown
t , On
in the MARL setting. Further, agents have perfect recall. At time t, agent n observes (O0
t )
generated from PO(St), then uses the policy An
1:t, g1:t−1, H n
t ) to select its action, where
gs = g1:N
1:t) is agent n’s private history and known as its AOH. The agents
receive a reward Rt , R(St, At) sampled from R(St, At), and the next state St+1 is generated
from PT (St, At). The goal is to ﬁnd a policy g = g1:T to maximize the common cumulative reward

1:t−1, On

t = (An

t = gn

and H n

t (O0

s

T

where the expectation is taken over the measure generated by policy g applied to model
(PT , R, PO, PI).

E

"

t=1
X

R(St, At)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

g

,

#

(1)

2.2 AIS Framework

In the single-agent POMDP setting, the spaces A and O are not product spaces, and at time t the
agent’s policy is of the form gt : Ω(Ht) → Ω(At), where Ht = (A1:t−1, O1:t) is the agent’s AOH.
Note the policy space grows exponentially in t as the length of Ht grows linearly in t. Subramanian

3

and Mahajan [2019] give conditions of a representation encapsulating the information in Ht that is
approximately sufﬁcient for decision purposes into a time-invariant space.

Deﬁnition 1: An (ǫ, δ)-approximate information state
ϑt(Ht) that satisﬁes the following properties:
(AIS1) It evolves recursively
φt(
(AIS2) It suﬃces for approximate performance evaluation |E[Rt|ht, at] − E[Rt|
b
b

Zt, At, Ot+1).

∀ ht, at.

Zt+1 =

Zt is the output of a function

b

b

b

(AIS3) It suﬃces for approximately predicting the observation,

i.e., ∀ ht, at, we have
zt, at)) ≤ δ , where K(·, ·) is a distance between two dis-

b

Zt =

b

zt, at]| ≤ ǫ

K(P(Ot+1|ht, at), P(Ot+1|
tributions2.

The value function at t obtained from Bellman equations with
Z’s as states falls behind the optimal
value function at the most by an expression linear in T − t, ǫ, and δ [Subramanian and Mahajan,
2019]. When ǫ = δ = 0, the expression is 0, and the AIS

Z degenerates to an IS Z.

b

b

In Subramanian and Mahajan [2019], a DL framework is provided to ﬁnd an “approximate mapping”
ϑt(·) for any given POMDP model. The idea is to interpret the quantities in the LHS of (AIS2) and
(AIS3) as driving the learning loss in DL, and let existing DL optimization algorithms ﬁnd good
mappings. The resulting AIS can then be used as the state in common policy approximation methods
b
to ﬁnd a near-optimal policy.

b

2.3 Common Information based DPs

2.3.1 DP with No Compression

1:t, g1:t−1, H n
In a DecPOMDP, the action decision for agent n at time t, An
t ), can be
split into two steps. In the ﬁrst step, based on past common observations and policies (O0
1:t, g1:t−1)
(using perfect recall), the agent decides gn
1:t, g1:t−1, ·); then in the second
step, it simply applies Γn
t ). The function Γn
t is called the
prescription (function), since it prescribes what the agent should do based on any possible realization
of its private information.

t and hence Γn
t to obtain the action An

t (·) , gn
t = Γn

t (O0
t (H n

t = gn

t to H n

t (O0

t ) → Ω(Γt), where H 0
t

1:t, g1:t−1) and Γt = Γ1:N

; then it sends Γt to every agent, and agent n selects An

This decomposition technique is called the CI approach [Nayyar et al., 2013]. Note that the actual
decision is carried out in the ﬁrst step and solely upon CI (perfect recall makes policy common
knowledge). One may then imagine there is a ﬁctitious coordinator, labelled agent 0. At time t, the
coordinator’s policy is of the form dt : Ω(H 0
1:t, Γ1:t−1) is equivalent
to (O0
t ).
It is shown that this decomposition is without loss of generality (so without loss of optimality too).
The coordinator observes common observation O0
t can be seen as
the coordinator’s AOH and will be called the full common state (FCS), while H n
t will be referred
to as the full private state (FPS) of agent n. From the perspective of the coordinator, the problem is
now a centralized POMDP, and the goal is to ﬁnd a policy d = d1:T that maximizes the expected
cumulative reward. This permits a sequential decomposition with FCS as the state and an FPS-based
prescription (meaning the prescription takes FPS as its input) as the action, which is presented in
Algorithm 1.

t and chooses action Γt; hence, H 0

t = Γn

, (O0

t (H n

t

T +1) , 0

Algorithm 1 Dynamic Programming with FCSs and FPS-based Prescriptions
VT +1(h0
for t = T, . . . , 1 do
t , γt) = E
R(St, Γt(H 1:N
t+1))|H 0
t , Γt, O0
Vt+1((H 0
(cid:2)
t ) = maxγt∈Ω(Γt) Qt(h0

))+
t = h0
t , γt)

t , Γt = γt

Qt(h0

Vt(h0

t

(cid:3)

In practice, the coordinator is virtual and the computation of the coordinator is carried out in all
agents – this is viable since the coordinator’s computation only requires CI, which every agent has
access to. Note the update of the state is done by direct concatenation of the incoming Γt and O0
t+1.

2For example, Wasserstein and total variation distances.

4

2.3.2 DP with BCS

t

|H 0

Nayyar et al. [2013] further compresses the FCS to the belief common state (BCS) Πt =
P(St, H 1:N
t ), which is the conditional distribution on the state and the FPSs given the FCS.
.
It is shown that restricting attention to coordinator’s policy of the form
dt : Ω(Πt) → Ω(Γt) is
without loss of optimality. The DP presented thus uses this BCS as the state and an FPS-based
prescription as the action – see Appendix A.2.

There are two problems with this approach when applied to the MARL setting. First, the BCS is
updated via a Bayesian update using PT and PO, which requires model knowledge. Second, the
growing length of H 1:N
t makes the spaces of Πt and Γt explosively large and impossible to explore.
However, at a conceptual level we can apply the AIS framework to the centralized POMDP of the
coordinator3; the underlying decentralized information structure coupled with increasing domain of
private information makes practical implementations of this scheme challenging.

2.3.3 DP with BCS and SPI

To alleviate the aforementioned dimensionality issue, Tavafoghi et al. [2018] further compresses
the FPS to a representation called the sufﬁcient private information (SPI) lying in a time-invariant
domain. They identify a set of conditions for the compression so that the SPI is sufﬁcient for decision
making purposes.
Deﬁnition 2: A suﬃcient private information (SPI) Z 1:N
functions Z n
t , H n
t (Z n
(SPI1) It evolves recursively, i.e., ∀ n ∈ [N ], Z n
(SPI2) It suﬃces for performance evaluation E[R(St, At)|h0

t , An
t , at] = E[R(St, At)|h0

t ) ∀ n ∈ [N ] satisfying the properties:

t−1, Γt−1, O0
t , hn

is a tuple of outputs of a set of

t , g1:t−1).
t , zn

t+1 = φn

t−1, On

t = ϑn

t (H 0

t , at]

t

∀ h0

t , hn

t , at.
suﬃces

(SPI3) It

and
t , z1:N
t+1|h0
t
(SPI4) It suﬃces for predicting other agents’ SPI P(Z −n
|h0
t

predicting
, γt, at) = P(Z 1:N

itself
t+1 , O0

for
t , h1:N
t

t+1 , O0

P(Z 1:N

t+1|h0

the
common
, γt, at) ∀ h0:N
t ) = P(Z −n

t , hn

t

t

, γt, at.
t , zn

|h0

observation

t ) ∀ h0

t , hn
t .

∼

t = Λn
t (Z n
The coordinator now considers SPI-based prescriptions Λt = Λ1:N
where An
t ), and
t
∼ 0
t = P(St, Z 1:N
1, Λ1, . . . , O0
t = (O0
the BCS is changed to Π
t ). It is shown that
t ) where H
restricting attention to coordinator’s policy of the form d
t) → Ω(Λt) is without loss of
t : Ω(Π
optimality. The resulting DP uses the BCS as the state and SPI-based prescription as the action – see
Appendix A.3. Note that the compression actually leads to an action compression for the coordinator
– from FPS-based prescriptions to SPI-based prescriptions – which has no loss in performance.

∼ 0
|H

∼

∼

t

∼

t, Z 1:N
t

With Π
, and Λt all lying in time-invariant spaces, the complexity no longer grows with
time. However, it is unclear how to ﬁnd mappings satisfying Deﬁnition 2 and update the BCSs in
an MARL setting. Further, the solution focuses on a decentralized setting wherein the (lossless)
compression functions are consistent (common knowledge), and the performance assessments and
predictions are based only on the information of any particular agent. Ensuring these properties in
the RL context would require signiﬁcant communication, particularly during training.

3 APPROXIMATE STATE REPRESENTATIONS

We seek to extend the idea of identifying representations sufﬁcient for approximately optimal deci-
sion making from Section 2.2 to the multi-agent setting, and develop a general compression frame-
work for common states and private states (hence also prescriptions) whose mappings can be learned
from samples obtained by interacting with the environment alone.

In this section, we propose our general states representation framework for approximate planning
and control in partially observable MARL problems. We start by compressing private histories
to ASPS; for the coordinator, this induces an action compression from FPS-based prescriptions to
ASPS-based prescriptions. Then based on this compression, the common history is further com-
pressed to ASCS.

3Strictly speaking, this requires a straightforward extension to time-varying action spaces for different time

steps – see Subramanian et al. [2020] Section 5 for details.

5

The framework we develop will be consistent with the philosophy of recent empirical MARL work
wherein there is a centralized agent called the supervisor. The supervisor observes all the quantities
and develops good compression of private information and common information that the coordina-
tor can use to produce close-to-optimal prescriptions (using the compressed common information),
which can be implemented by the agents using just their own compressed private information. We
detail the supervisor in Section 4.1 but point out here that it has the knowledge of H 0:N
for all
t ∈ [T ]. Note that this viewpoint is consistent with the “centralized training with distributed execu-
tion” setting of the empirical MARL work.

t

3.1 Compressing Private States

t =

Deﬁnition 3: An (ǫp, δp)-approximate suﬃcient private state (ASPS)
ϑn
Z n
t (H 0
outputs of a set of functions
(ASPS1) It evolves in a recursive manner, that is, ∀ n ∈ [N ],
b
(ASPS2) It suﬃces for approximate performance evaluation
z1:N
b
, at]
t

b
(ASPS3) It suﬃces for approximately predicting observations K
(cid:12)
(cid:12)

t ) ∀ n ∈ [N ] satisfying:
t =

b
≤ ǫp/4 ∀ h0:N

E[R(St, At)|h0
t ,

t , H n

≤ δp/8 ∀ h0:N

P(O0:N

, at.

, at.

Z n

(cid:12)
(cid:12)

t

t+1 |h0
t ,

z1:N
t

, at)
b

t

(cid:0)

Z 1:N
t

is a tuple of

φn
t (

b
Z n
t−1, Γt−1, O0
E[R(St, At)|h0
t , h1:N
t
b
P(O0:N

t+1 |h0

t , h1:N
t

t , On
t ).
, at] −

, at),

(cid:1)

This deﬁnition induces the ASPS-based prescription, which is a mapping
) → Ω(At)
Z N
λ1
that prescribes the action tuple for all ASPSs At =
t )) in a
t (
component-wise manner. One can run a DP with FCSs as states and ASPS-based prescriptions as
actions – see Algorithm 2.

t ), . . . ,
b

λt : Ω(
Z 1
b
b

) = (

Z 1:N
t

Z 1:N
t
λN
t (

λt(

b

b

b

b

b

b

T +1) , 0

Algorithm 2 Dynamic Programming with FCSs and ASPS-based Prescriptions
VT +1(h0
for t = T, . . . , 1 do
λt) = E
Z 1:N
Qt(h0
b
))
Rt(St,
λt(
t ,
t
t = h0
t+1))|H 0
Λt, O0
Vt+1((H 0
t ,
t ,
+
(cid:2)
b
b
b
b
Qt(h0
Vt(h0
λt)
t ,
t ) = maxbλt∈Ω(
b
b

Λt =

b
Λt)

λt

b

b

(cid:3)

b

b

b

. These functions also relate Ω(Γt) and Ω(

ϑ1:N
Λt) as
The compression is characterized by functions
t
ASPS-based prescriptions are a strict subset of FPS-based prescritions; this detail will be explained
in Section 4.2. For now we note that here the conditions we set for the action compression from
Λt) are on the private states instead of deﬁning an encapsulation directly on the actions
Ω(Γt) to Ω(
(i.e., prescriptions); moreover, the compression may depend on the common state h0
t as well. Hence,
this falls outside of the action compression scheme studied in Subramanian et al. [2020]. We bound
the error between the value functions obtained from Algorithm 2 and the optimal value functions
obtained from Algorithm 1 in the following theorem proved in Section 4.2.
Theorem 4: Assume the reward function R is uniformly bounded by ¯R. For any h0
and γ∗ ∈ argmaxγ Qt(h0

t , γ), there exists a

Λt) such that

t ∈ Ω(H 0
t )

λ ∈ Ω(

b

b

b

Qt(h0

t , γ∗) −

Qt(h0
t ,

λ) ≤

(ǫp + T ¯Rδp) + (¯t + 1)ǫp,
b

¯t(¯t + 1)
b
2
¯t(¯t + 1)
2

(ǫp + T ¯Rδp) + (¯t + 1)ǫp,

Vt(h0

t ) −

b
Vt(h0

b
t ) ≤

where ¯t = T − t.

b

3.2 Compressing Common States

(2)

(3)

While restricting attention to ASPS-based prescriptions, we further compress the common history
to an approximate representation by applying the state compression result of Subramanian and Ma-
hajan [2019].

6

Deﬁnition 5: An (ǫc, δc)-approximate suﬃcient common state (ASCS)
a function
t ) satisfying the properties:
(ASCS1) It evolves in a recursive manner, that is,
t−1,
(ASCS2) It suﬃces for approximate performance evaluation,

t (H 0
ϑ0

φ0
t (

t =

t =

Z 0

Z 0

Z 0

b
b
E[Rt(St, At)|h0
t ,

λt] − E[Rt(St, At)|

z0
t ,

λt]

b

b
≤ ǫc.

b

Z 0

t is the output of

b
Λt−1, O0
t ).
i.e., ∀ h0
t ,
b

λt, we have

(ASCS3) It suﬃces for approximately predicting common observation, i.e., ∀ h0
t ,
z0
t ,

(cid:12)
b
(cid:12)
≤ δc/2.

(cid:12)
(cid:12)
have K

λt), P(O0

b
t+1|h0
t ,

P(O0

t+1|

b
λt)

(cid:0)

b
t and ASPS
, which can be updated recursively using the incoming CI and PI. Agents use the same policy
Λt to
t . Approximately optimal policies then result from the

In our proposed representation framework, agents compress the CI and PI to ASCS
Z 1:N
t
V∗
Z 0
t : Ω(
d
b
their own ASPS
DP with ASCSs as states and ASPS-based prescriptions as actions – see Algorithm 3.
b

Λt) to decide the ASPS-based prescription

t ) → Ω(
Z n
t
b

to obtain the action An

t , then they apply

Λt from

Z 0

Z 0

b

b

b

b

b

b

b

(cid:1)

b

λt, we

Algorithm 3 Dynamic Programming with ASCSs and ASPS-based Prescriptions

b

V

V

Q

T +1(

T +1) , 0
z0

V
for t = T, . . . , 1 do
λt) = E
z0
R(St,
t ,
t(
b
Z 0
φ0
+V
t ,
t (
t+1(
(cid:2)
b
b
z0
t ) = maxbλt∈Ω(
b

t(

V

b

V

V

Z 1:N
))
Λt(
t
Z 0
Λt, O0
t+1))|
b
b
z0
Λt) Q
λt)
t ,
t(
b
b
b

V

t =

z0
t ,

Λt =

λt

b

b

(cid:3)

b

b

b

b

From Algorithm 2 to Algorithm 3, only the states are further compressed, so a gap result bounding
the difference between the two DPs holds, similar to the result in Subramanian and Mahajan [2019].
See Appendix C for details.
Theorem 6: Assume the reward function R is uniformly bounded by ¯R. For any h0
and

t ∈ Ω(H 0
t )

λ ∈ Ω(

b

Λt), with ¯t = T − t, we have
Qt(h0
t (h0
ϑ0
t ,
t (h0
ϑ0
Vt(h0
t ) − V
b
b

λ) − Q

t(

t(

b

b

V

V

λ) ≤ ¯t(ǫc + T ¯Rδc) + ǫc,

t ),
t )) ≤ ¯t(ǫc + T ¯Rδc) + ǫc.

b

3.3 Main Result

b

b

Our main result bounds the optimality gap of value functions obtained from performing DP with
the general common and private representations satisfying the conditions in Deﬁnition 3 and Deﬁni-
tion 5 as in Algorithm 3, in comparison to the optimal value functions computed from Algorithm 1.
Theorem 7: Assume the reward function R is uniformly bounded by ¯R. For any h0
t ∈ Ω(H 0
t )
and γ∗ ∈ argmaxγ Qt(h0

t , γ), there exists a

Λt) such that

λ ∈ Ω(

Qt(h0

t , γ∗) − Q

t(

t (h0
ϑ0

t ),

V

λ) ≤

V

Vt(h0
where ¯t = T − t.

t ) − V

t(

b
t (h0
ϑ0

b
t )) ≤

b

Proof: Combine Theorem 4 and Theorem 6.

¯t(¯t + 1)
(ǫp + T ¯Rδp) + (¯t + 1)(ǫc + ǫp) + ¯tT ¯Rδc,
b
2
¯t(¯t + 1)
2

(ǫp + T ¯Rδp) + (¯t + 1)(ǫc + ǫp) + ¯tT ¯Rδc,

b

(4)

(5)

(6)

(7)

(cid:4)

We observe that the action compression induced by private state compression leads to a gap quadratic
in remaining time ¯t = T − t (Theorem 4), and common state compression causes a gap linear in
remaining time. Also, note that the gap decreases to 0 as (ǫc, δc, ǫp, δp) go to 0. Having developed
this result, the remaining questions for learning using the sample data from the environment are: how
ϑ0:N
to learn the compression mappings
1:T with small error; and how to learn good policies with the
compressed representations. See Appendix E for a proposed scheme to answer both these questions
using DL methods.

b

7

3.4 Comparisons to Existing Schemes

Nayyar et al. [2013] and Tavafoghi et al. [2018] provide lossless (performance-wise) compression.
We refer to ASPS and its corresponding conditions with ǫp = δp = 0 as SPS; similarly, we refer to
ASCS and its corresponding conditions with ǫc = δc = 0 as SCS. Missing proofs in this subsection
are in Appendix D.

Relation to Nayyar et al. [2013]. The private history is not compressed in Nayyar et al. [2013],
so it is clearly a special case of SPS. The BCS proposed in Nayyar et al. [2013] is a special case of
SCS as well.
Proposition 8: The BCS Πt = P(St, H 1:N
tion 5 with ǫc = δc = 0.

t ) satisﬁes the conditions of an SCS in Deﬁni-

|H 0

t

Relation to Tavafoghi et al. [2018]. Our conditions of SPS and Tavafoghi et al. [2018]’s con-
ditions of SPI both lead to performance sufﬁciency of the space of SPI-based (or SPS-based) pre-
scriptions. The two sets of conditions are similar but not exactly the same. Condition (SPI1) cor-
responds to (SPS1); however, (SPS1) is stricter since we require policy-independent compression,
while Tavafoghi et al. [2018] allow policy-dependent compression. Condition (SPI3) ensures future
sufﬁciency as does (SPS3). Conditions (SPI2) and (SPI4) together ensure present sufﬁciency as does
(SPS3).
Proposition 9: (SPS1) and (SPS3) imply (SPI3).
Proposition 10: (SPS2) and (SPI4) imply (SPI2).

Restricting to SPS, their BCS Π
identical to Proposition 8 holds with FPS changed to SPS.

t = P(St, Z 1:N

|H 0

t

∼

t ) is a special case of SCS as well, so a result

Relation to Mao et al. [2020]. Their private state embedding does not require a recursive update
. With this additional assumption they show linearity
(ASPS1), but demands injective functions
of the optimality gap in remaining time. For the common state, the BCS they consider is a special
case of our SCS, just as the BCS of Tavafoghi et al. [2018].

ϑ1:N
t

b

4 OPTIMALITY GAP ANALYSIS

In this section, we outline the optimality gaps introduced in Section 3; details are in Appendix B.

4.1 Supervisor’s Functions

For better exposition, we introduce another set of Q/V functions from an omniscient supervisor’s
perspective, for the original decision problem. The supervisor can access the union of the infor-
mation of all agents: at time t the supervisor knows H 0:N
. In contrast, coordinator’s information
t
is the intersection of the information of all agents: H 0
t . The supervisor, however, only observes
what is happening, lets the coordinator decide all the policies and prescriptions, and implements
the coordinator’s policies. Let d∗
1:T be a coordinator’s optimal policy solved using Algorithm 1,
i.e. d∗
t , γt). Then the Q/V functions deﬁned in Algorithm 1 can be
rewritten as

t ) = argmaxγt∈Ω(Γt) Qt(h0

t (h0

T

Rτ

t , γt, d∗
h0

t+1:T

Qt(h0

t , γt) = E

"

τ =t
X
T

Vt(h0

t ) = E

"

(cid:12)
(cid:12)
(cid:12)
(cid:12)
Rτ

.

#

t , d∗
h0
t:T
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

#

(8)

(9)

τ =t
X
The supervisor’s Q/V functions use similar concepts, but with supervisor’s states and coordinator’s
policies.
Deﬁnition 11: For any h0:N

), γt ∈ Ω(Γt), deﬁne the supervisor’s Q function as

∈ Ω(H 0:N

t

t

QS

t (h0

t , h1:N
t

, γt) , E

T

"

τ =t
X

8

Rτ

t , h1:N
h0
t
(cid:12)
(cid:12)
(cid:12)
(cid:12)

, γt, d∗

t+1:T

,

#

(10)

and the supervisor’s V function as

T

V S
t (h0

t , h1:N
t

) , E

Rτ

where γ∗

t ∈ argmaxγt∈Ω(Γt) Qt(h0

"
τ =t
X
t , γt)4.

= QS

t (h0

t , h1:N
t

, γ∗

t ),

(11)

t , h1:N
h0
t

, d∗
t:T

#

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Then the coordinator’s Q/V functions can be expressed as the expectation of supervisor’s Q/V
functions taken over the conditional distribution on FPSs given the FCS:

Qt(h0

t , γt) =

P(h1:N
t

Xh1:N

t

Vt(h0

t ) =

P(h1:N
t

Xh1:N

t

4.2 Proof of Theorem 4

|h0

t )QS

t (h0

t , h1:N
t

|h0

t )V S

t (h0

t , h1:N
t

, γt),

).

(12)

(13)

t

t

b

b

t =
. A

Λt). Consider a ﬁxed h0

) are functions, there could be multiple h1:N

We ﬁrst determine the relationship between the space of FPS-based prescriptions Ω(Γt) and the
t . Since the compression mappings
space of ASPS-based prescriptions Ω(
Z 1:N
ϑ1:N
t , H 1:N
(H 0
’s that are mapped to the same
t
t
z1:N
Λt) can thus be thought of as a special element of Ω(Γt) that prescribes the same
λt ∈ Ω(
t
z1:N
action for all the FPSs h1:N
. Hence, we can construct an injective
’s mapped to the same ASPS
b
b
t
extension mapping from Ω(
Λt) to Ω(Γt) in this sense.
b
b
t ∈ Ω(H 0
Deﬁnition 12: For any h0
b
Ω(Γt) as follows: for any h1:N
and
ϑ1:N
t
λt(
b
b
Given the compression
of notation, γt = ψt(

b
Λt) × Ω(H 0
t ) →
z1:N
λt, h0
t =
to
ϑt), then choose the action according to
b
b
) ,

, ψt is well-deﬁned. Under this circumstance and with an abuse
b
when the considered compression is

λt, γt = ψt(
) (hence ψt implicitly depends on

t ), deﬁne the extension mapping ψt : Ω(

b
t ) will be written as γbλt,h0

t ) will ﬁrst compress h1:N

t , h1:N
t
). That is,

(h0
z1:N
t

t )(h1:N
t

t , h1:N
t

) = ψt(

λt, h0

ϑ1:N
t

(h0

λt(

)).

b

b

b

b

b

t

t

t

clear from the context and will be referred to as the γt extended from

λt under h0
t .

γt(h1:N
t
ϑ1:N
t
λt, h0
b

b

The following proposition says that for any FCS one can ﬁnd an ASPS-based prescription whose
extension nearly achieves the same Q-value as an optimal prescription, up to a gap linear in the
remaining time ¯t. This implies that it nearly sufﬁces to consider the class of prescriptions extended
from ASPS-based prescriptions for DP purposes.
Proposition 13: Assume the reward function R is uniformly bounded by ¯R. For any h0
Ω(H 0

t ) and γ∗ ∈ argmaxγ Qt(h0

t , γ), there exists a

Λt) with

λ ∈ Ω(

t ∈

b

which leads to

Qt(h0

t , γ∗) − Qt(h0

t , γbλ,h0

t

)

≤ ¯t(ǫp + T ¯Rδp) + ǫp,
b

b

(cid:12)
(cid:12)
(cid:12)
Vt(h0

t ) − max
b
Λt)

bλ∈Ω(

(cid:12)
(cid:12)
(cid:12)
t , γbλ,h0

t

Qt(h0

≤ ¯t(ǫp + T ¯Rδp) + ǫp.

(14)

(15)

Before proving this critical proposition we need a few intermediate results. The ﬁrst key lemma says
that with the same supervisor’s state, the supervisor’s Q-values for two different prescriptions will
be the same as long as they prescribe the same action for the given private information.
t ) and h ∈ Ω(H 1:N
Lemma 14: For any h0
that choose the same action on h, i.e. γ1(h) = γ2(h) = a; then
QS

(16)
is admissible under h0
t ,
t ) > 0. Throughout the rest of the paper, we assume that only admissible FPSs are considered.

4The supervisor’s Q function and V function are only deﬁned when the FPS h1:N

), let γ1, γ2 ∈ Ω(Γt) be two prescriptions

t , h, γ1) = QS

t ∈ Ω(H 0

i.e. P(h1:N

t , h, γ2).

t (h0

t (h0

|h0

t

t

t

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

9

(18)

(19)

(cid:12)
(cid:12)

z

b
o
b

Next we show that the supervisor’s Q-values will be nearly the same for two different FPSs that map
to the same ASPS and a prescription that prescribes the same action on these two FPSs.
Lemma 15: Assume R is uniformly bounded by ¯R. For any h0
Ω(H 1:N
t
ϑ1:N
(h0
t
action on these two FPSs γ(h1) = γ(h2) = a. Then
b

t ∈ Ω(H 0
t ), let h1, h2 ∈
Z 1:N
z =
), i.e.
z ∈ Ω(
t
t , h2), and let γ ∈ Ω(Γt) be a prescription that chooses the same

) be two FPSs under h0
t , h1) =

t that map to the same ASPS

ϑ1:N
t

(h0

b

b

b

b

QS

t (h0

t , h1, γ) − QS

t (h0

t , h2, γ)

≤ ¯t(ǫp + T ¯Rδp)/2 + ǫp/2.

(17)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Using the above two lemmas, we show that the supervisor’s V function will differ little for two
supervisor’s states with the same compression of private states.
Corollary 16: Assume R is uniformly bounded by ¯R. For any h0
Ω(H 1:N
t
ϑt(h0
t , h1) =

t ∈ Ω(H 0
Z 1:N
z ∈ Ω(
t
t , γ) be an optimal prescription. Then
b
≤ ¯t(ǫp + T ¯Rδp)/2 + ǫp/2.

t , h2), and let γ∗ ∈ argmaxγ Qt(h0

t that map to the same ASPS

t ), let h1, h2 ∈
z =

) be two FPSs under h0

t , h1, γ∗) − QS

t , h2, γ∗)

V S
t (h0

t (h0

ϑt(h0

t , h2)

t (h0

t (h0

), i.e.

QS

,

b

b

t , h1) − V S
b

b

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Proof of Proposition 13: Given an optimal prescription γ∗ ∈ argmaxγ Qt(h0
cally construct a

Λt) that serves for the claim. For each

λ ∈ Ω(

z ∈ Ω(

Z 1:N
t

), deﬁne

t , γ), we will speciﬁ-

b

b

Hbz =

h ∈ Ω(H 1:N

t

) :

ϑ1:N
t

(h0

t , h) =
b

n

b

to be the class of h’s in Ω(H 1:N
By the Axiom of Choice, for each
z ∈ Ω(
from arbitrary choice function, which we denote as ¯hbz. We then construct the
the corresponding extension in Ω(Γt) will be

ϑ1:N
.
t
) there exists a representative of the class Hbz coming
z) = γ∗(¯hbz);

) that are compressed to
Z 1:N
t

z under the considered compression

λ by

λ(

b

b

b

b

t

γbλ,h0

t

(h) = γ∗

¯h bϑ1:N

t

(h0

t ,h)

∀ h ∈ Ω(H 1:N

t

),

b

b

b

(20)

that is, the prescription ﬁrst compresses the input FPS and ﬁnds the representative of the corre-
sponding compression class, then it mimics what the optimal prescription would have done with the
representative. For any h ∈ Ω(H 1:N

), we have

t

(cid:16)

(cid:17)

QS

t (h0

t , h, γ∗) − QS

t (h0

t , h, γbλ,h0

t

)

≤

QS

t (h0

t , h, γ∗) − QS

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
+
(cid:12)

t

t (h0

t , ¯h bϑ1:N
(h0
t ,h), γ∗) − QS

t ,h), γ∗)
(cid:12)
t , ¯h bϑ1:N
t (h0
(cid:12)
(cid:12)

t

) − QS

t (h0

t , h, γbλ,h0

t

)

t ,h), γbλ,h0

t

(h0

)

(cid:12)
(cid:12)
(cid:12)

t

(h0

QS

t (h0

t , ¯h bϑ1:N
t , ¯h bϑ1:N
≤ (T − t)(ǫp + T ¯Rδp) + ǫp,

t (h0

QS

t ,h), γbλ,h0

(h0

+

t

t

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

as the ﬁrst term is bounded by (T −t)(ǫp +T ¯Rδp)/2+ǫp/2 due to Corollary 16, the second term is 0
due to Lemma 14, and the third term is bounded by (T − t)(ǫp + T ¯Rδp)/2 + ǫp/2 due to Lemma 15.
If it happens to be the case that h = ¯h bϑ1:N
t ,h), i.e. h is the representative, then the original term
|QS
t (h0
t (h0
t gives the
(cid:4)
claim.

)| is 0. Taking the conditional expectation on h given h0

t , h, γ∗) − QS

t , h, γbλ,h0

(h0

t

t

Proof of Theorem 4: There are three main quantities: Vt(h0
t ) is the value obtained from executing
) is from executing the opti-
optimal FPS-based prescriptions to the end, maxbλ∈Ω(
mal ASPS-based prescription for step t and then optimal FPS-based prescriptions afterwards to the
t ) is from executing optimal ASPS-based prescriptions to the end. Proposition 13 es-
end, and
tablishes that restricting to ASPS-based prescriptions in one step incurs a gap (between Vt(h0
t ) and
)) linear in T − t. Using an induction argument to accumulate this gap in
maxbλ∈Ω(
every step from T back to t yields the gap (between Vt(h0
t )) to be quadratic in T − t.
(cid:4)
See Appendix B for detailed derivations.

Λt) Qt(h0
b
b

Λt) Qt(h0

t , γbλ,h0

t , γbλ,h0

t ) and

Vt(h0

Vt(h0

b

t

t

b

10

5 CONCLUSION

In this paper, we developed a general approximate state representation framework for MARL prob-
lems in a Dec-POMDP setting. We bounded the optimality gap in terms of the approximation error
parameters and the number of remaining time steps. The theory provides guidance on designing
deep-MARL algorithms, which has great potential in practical uses. Future directions include: ex-
ploring DL methods for applications using our framework, designing a representation for prescrip-
tions, designing fully decentralized MARL schemes by adding communication, and extensions to
general-sum games.

References

Daniel S. Bernstein, Shlomo Zilberstein, and Neil Immerman. The complexity of decentralized
control of Markov decision processes. Mathematics of operations research, 27(4):819–840, 2002.

Jilles Dibangoye and Olivier Buffet. Learning to act in decentralized partially observable MDPs. In

International Conference on Machine Learning, 2018.

Norm Ferns, Prakash Panangaden, and Doina Precup. Bisimulation metrics for continuous Markov

decision processes. SIAM Journal on Computing, 40(6):1662–1714, 2011.

Jakob N. Foerster, Francis Song, Edward Hughes, Neil Burch, Iain Dunning, Shimon Whiteson,
Matthew Botvinick, and Michael Bowling. Bayesian action decoder for deep multi-agent rein-
forcement learning. In Proc. 36th International Conference on Machine Learning (ICML 2019),
2019.

David Ha and J¨urgen Schmidhuber. Recurrent world models facilitate policy evolution. arXiv

preprint arXiv:1809.01999, 2019.

Mehdi Jafarnia-Jahromi, Rahul Jain, and Ashutosh Nayyar. Online learning for unknown partially

observable MDPs. arXiv preprint arXiv:2102.12661, 2021.

Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in

partially observable stochastic domains. Artiﬁcial Intelligence, 101(1):99–134, 1998.

Ali Devran Kara and Serdar Yuksel. Near optimality of ﬁnite memory feedback policies in partially

observed Markov decision processes. arXiv preprint arXiv:2010.07452, 2020.

P. R. Kumar and Pravin Varaiya. Stochastic Systems: Estimation, Identiﬁcation, and Adaptive Con-

trol. SIAM, 2015.

Adam Lerer, Hengyuan Hu, Jakob Foerster, and Noam Brown. Improving policies via search in

cooperative partially observable games. arXiv preprint arXiv:1912.02318, 2019.

Timoth´ee Lesort, Natalia D´ıaz-Rodr´ıguez, Jean-Franois Goudou, and David Filliat. State represen-

tation learning for control: An overview. Neural Networks, 108:379–392, 2018.

Michael L. Littman, Richard S. Sutton, and Satinder P. Singh. Predictive representations of state. In

Advances in Neural Information Processing Systems (NIPS), 2001.

Aditya Mahajan and Mehnaz Mannan. Decentralized stochastic control. Annals of Operations

Research, 241(1):109–126, 2016.

Weichao Mao, Kaiqing Zhang, Erik Miehling, and Tamer Bas¸ar. Information state embedding in
partially observable cooperative multi-agent reinforcement learning. In 2020 IEEE 59th Annual
Conference on Decision and Control (CDC), 2020.

Ashutosh Nayyar, Aditya Mahajan, and Demosthenis Teneketzis. Decentralized stochastic control
with partial history sharing: A common information approach. IEEE Transactions on Automatic
Control, 58(7):1644–1658, July 2013.

Frans A. Oliehoek and Christopher Amato. A Concise Introduction to Decentralized POMDPs.

Springer, 2016.

11

Christian A. Schroeder de Witt, Jakob Foerster, Gregory Farquhar, Philip Torr, Wendelin B¨oehmer,
and Shimon Whiteson. Multi-agent common knowledge reinforcement learning. In Advances in
Neural Information Processing Systems (NIPS), 2019.

Samuel Sokota, Edward Lockhart, Finbarr Timbers, Elnaz Davoodi, Ryan D’Orazio, Neil Burch,
Martin Schmid, Michael Bowling, and Marc Lanctot. Solving common-payoff games with ap-
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
proximate policy iteration.
2021.

Jayakumar Subramanian and Aditya Mahajan. Approximate information state for partially observed

systems. In 2019 IEEE 58th Annual Conference on Decision and Control (CDC), 2019.

Jayakumar Subramanian, Amit Sinha, Raihan Seraj, and Aditya Mahajan. Approximate information
state for approximate planning and reinforcement learning in partially observed systems. arXiv
preprint arXiv:2010.08843, 2020.

Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press,

Cambridge, 2018.

Hamidreza Tavafoghi, Yi Ouyang, and Demosthenis Teneketzis. A sufﬁcient information approach
to decentralized decision making. In 2018 IEEE 57th Annual Conference on Decision and Control
(CDC), 2018.

Hamidreza Tavafoghi, Yi Ouyang, and Demosthenis Teneketzis. A uniﬁed approach to dynamic
IEEE Transactions on

decision problems with asymmetric information: Non-strategic agents.
Automatic Control, 2021.

Amy Zhang, Zachary C Lipton, Luis Pineda, Kamyar Azizzadenesheli, Anima Anandkumar, Lau-
rent Itti, Joelle Pineau, and Tommaso Furlanello. Learning causal state representations of partially
observable environments. arXiv preprint arXiv:1906.10437, 2021a.

Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning
arXiv preprint

invariant representations for reinforcement learning without reconstruction.
arXiv:2006.10742, 2021b.

Kaiqing Zhang, Zhuoran Yang, and Tamer Bas¸ar. Decentralized multi-agent reinforcement learning

with networked agents: Recent advances. arXiv preprint arXiv:1912.03821, 2019.

12

A Supplementary Details

A.1 More Related Work

In Kara and Yuksel [2020] consider a special type of AIS – the N -memory, which contains the infor-
mation from the last N steps. Here, the compression function is ﬁxed but in contrast to Subramanian
et al. [2020], the approximation error given each history need not be uniform. When the model is
known, they provide conditions that bound the regret of N -memory policies (policies that depend
on N -memory), and an algorithm that ﬁnds optimal policies within this class. The ﬁrst algorithm
to learn the optimal policies of POMDPs with sub-linear regret in an online setting is proposed in
Jafarnia-Jahromi et al. [2021]. Using a posterior sampling-based scheme, the algorithm maintains
the posterior distribution on the unknown parameters of the considered POMDP, and adopts the
optimal policy with respect to a set of parameters sampled from the distribution in each episode.
The posterior update in the algorithm, however, heavily relies on the knowledge of the observation
kernel, which is usually unknown in RL settings.

State representation for control is studied extensively in the literature [Lesort et al., 2018]. Early
work on predictive state representation (PSR) of POMDPs [Littman et al., 2001] only focuses on the
encapsulation of the histories and does not explore its system prediction ability. The bisimulation
relation clusters MDP states with similar rewards and transitions, and a bisimulation metric convexly
combines the errors of the rewards and the transitions between two states [Ferns et al., 2011]. The
difference of the value functions of two states can be upper-bounded by the metric. The causal state
representation [Zhang et al., 2021a] for POMDPs clusters the histories in the space of AOHs that
will produce the same future dynamics. Using the observation history as the state, the considered
POMDP can be transformed into an MDP, so that the results from the bisimulation literature can be
applied.

A.2 DP with BCS

Algorithm 4 Dynamic Programming with BCSs and FPS-based Prescriptions
.
VT +1(πT +1) , 0
for t = T, . . . , 1 do
.
Qt(πt, γt) = E
.
Vt(πt) = maxγt∈Ω(Γt)

R(St, Γt(H 1:N
.
h
Qt(πt, γt)

.
Vt+1(ηt(Π0

t , Γt, O0

)) +

t

t+1))|Πt = πt, Γt = γt

i

A.3 DP with BCS and SPI

Algorithm 5 Dynamic Programming with BCSs and SPI-based Prescriptions

T +1(π∼

∼
T +1) , 0
V
for t = T, . . . , 1 do
t, λt) = E
R(St, Λt(Z 1:N
h
t) = maxλt∈Ω(Λt) Q

t(π∼
t(π∼

t
t(π∼

∼
Q
∼
V

∼

t, λt)

∼
)) + V

t+1(η∼

∼

t(Π

t, Λt, O0

t+1))|Π

∼

t = π∼

t, Λt = λt

i

B Omitted Analysis in Section 4.2

The following lemma shows that given the FPS, the actions the chosen prescription chooses for other
FPSs does not affect the next step statistics.
t ), h ∈ Ω(H 1:N
Lemma 17: Let h0

), γ ∈ Ω(Γt), and a = γ(h) ∈ Ω(At). Then

t ∈ Ω(H 0

t

P(St+1, H 1:N

t+1 |H 0

t = h0

t , H 1:N

t = h, Γt = γ) = P(St+1, H 1:N

t+1 |H 0

t = h0

t , H 1:N

t = h, At = a).

13

=

=

=

=

=

=

st
X

st
X

st
X

st
X

st
X

Proof: We will omit specifying the original random variables when their realizations are given in
the proof.

P(St+1, H 1:N
P(st|h0

t+1 |h0
t , h, γ, a) · P(St+1, H 1:N

t , h, γ) = P(St+1, H 1:N
t+1 |h0

t+1 |h0
t , h, γ, a, st)

t , h, γ, a)

P(st|h0

t , h, a) · P(St+1|h0

t , h, γ, a, st) · P(H 1:N

t+1 |h0

t , h, γ, a, st, St+1)

(γ is after st)

P(st|h0

t , h, a) · P(St+1|a, st) · P(H 1:N

t+1 |h0

t , h, γ, a, st, St+1)

P(st|h0

t , h, a) · P(St+1|a, st) · P(O1:N

t+1|h0

t , h, γ, a, st, St+1) (H 1:N

(PT speciﬁes St+1 given St and At)
t+1 = (H 1:N

, At, O1:N

t+1))

t

P(st|h0

t , h, a) · P(St+1|a, st) · P(O1:N

t+1|St+1)

P(st|h0

t , h, a) · P(St+1, H 1:N

t+1 |h0

t , h, a, st)

st
X

= P(St+1, H 1:N

t+1 |h0

t , h, a).

(PO speciﬁes O1:N

t+1 given St+1)

(cid:4)

Proof of Lemma 14: The proof for the instantaneous part is straightforward as St is irrelevant to
the choice of Γt

E

Rt(St, Γt(H 1:N

t

))|h0

t , h, γ1

=

P(st|h0

t , h, γ1)Rt(st, γ1(h))

(cid:2)

P(st|h0

(cid:3)
t , h, γ1(h))Rt(st, a)

st
X

st
X

P(st|h0

t , h)Rt(st, a)

=

=

(γ1 and γ2 are exogenously given)

st
X
= E

Rt(St, Γt(H 1:N

.
To show equality for the continuation part, we ﬁrst deﬁne the following policy for all τ = t +
(cid:3)
1, . . . , T :

(by symmetry)

t , h, γ2

))|h0

(cid:2)

t

d′
τ (h0

τ ) =

(cid:26)

d∗
t , γ1, h0
τ (h0
d∗
τ (h0
τ )

t+1:τ )

τ = (h0
if h0
otherwise,

t , γ2, h0

t+1:τ ) ∀ h0

t+1:τ ,

t+1, γt+1, . . . , o0

τ is an optimal policy at time step τ . Also, we have h0

where d∗
t+1 when τ = t + 1, and
τ ) when τ > t + 1, so that the entire (h0
t+1:τ = (o0
h0
τ ). This
policy performs the optimal policy at all times, except when γ2 is chosen at time t, it will mimic
what the optimal policy would have done if γ1 was chosen instead; owing to perfect recall, future
prescriptions can depend on past ones. Then

t+1:τ ) ∈ Ω(H 0

t+1:τ = o0

t , γ1, h0

E

V S
t+1(H 0

t+1, H 1:N

t+1 )|h0

t , h, γ1

(cid:2)

T

= E

"

(cid:3)

T

Rτ (Sτ , Aτ )
(cid:12)
(cid:12)
(cid:12)
(cid:12)

T

τ =t+1
X
(∗)
= E

t , h, γ1, d∗
h0

t+1:T

#

= E

t , h, γ1, d′
h0

#

"

t+1:T

≤ E

τ =t+1
X
T

Rτ (Sτ , Aτ )
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Rτ (Sτ , Aτ )
(cid:12)
τ =t+1
X
(cid:12)
(cid:12)
where the inequality holds as d′
t+1:T is not an optimal choice from the current history. By symmetry,
(cid:12)
the inequality implies that
V S
t+1(H 0

Rτ (Sτ , Aτ )
(cid:12)
(cid:12)
(cid:12)
(cid:12)
t+1 )|h0

τ =t+1
X
V S
t+1(H 0

t , h, γ2, d∗
h0

t+1, H 1:N

t+1, H 1:N

t+1, H 1:N

V S
t+1(H 0

t+1 )|h0

t+1 )|h0

t , h, γ1

t , h, γ2

t , h, γ2

= E

= E

t+1:T

t+1:T

E

"

#

"

#

(cid:3)

(cid:2)

.

;

t , h, γ2, d′
h0

(cid:2)

(cid:3)

(cid:2)

14

(cid:3)

P(St+1:τ , O0:N

The equality labeled by (∗) follows from the fact that under the policy d′
t+1:T , choosing γ1 and
γ2 will generate the exact future statistics. We will show this in the following. We ﬁrst prove the
following claim using mathematical induction.
Claim: for all τ = t + 1, . . . , T , we have
t , h, γ1, a, d′
t+1:τ , At+1:τ |h0
Base case: the claim holds for τ = t + 1.
t , h, γ1, a, d′
t+1:T ) · P(St+1, O0:N

P(St+1, O0:N
P(st|h0

t+1 , At+1|h0
t , h, γ1, a, d′

t+1:T ) = P(St+1:τ , O0:N

t+1:τ , At+1:τ |h0

t , h, γ1, a, st, d′

t+1 , At+1|h0

t , h, γ2, a, d′

t+1:T ).

t+1:T )

t+1:T )

=

st
X

st
X

st
X

st
X

P(st|h0

t , h) · P(St+1, O0:N

t+1, At+1|h0

t , h, γ1, a, st, d′

t+1)

(st is independent of a given Γt)

P(st|h0

t , h) · P(St+1|h0

t , h, γ1, a, st, d′

t+1) · P(O0:N

t+1, At+1|h0

t , h, γ1, a, st, d′

t+1, St+1)

P(st|h0

t , h) · P(St+1|st, a) · P(O0:N

t+1 , At+1|h0

t , h, γ1, a, st, d′

t+1, St+1)

P(st|h0

t , h) · P(St+1|st, a) · P(O0:N

t+1 |h0

t , h, γ1, a, st, d′

t+1, St+1)

(PT speciﬁes St+1 given St and At)

st
X
· P(At+1|h0
P(st|h0

t , h, γ1, a, st, d′

t+1, St+1, O0:N
t+1)

t , h) · P(St+1|st, a) · P(O0:N

t+1 |St+1) · P(At+1|h0

t , h, γ1, a, st, d′

t+1, St+1, O0:N
t+1 )

st
X

st
X

st
X

P(st|h0

t , h) · P(St+1|st, a) · P(O0:N

t+1 |St+1) · I

P(st|h0

t , h) · P(St+1|st, a) · P(O0:N

t+1 |St+1) · I

(cid:8)

(cid:8)

= P(St+1, O0:N

t+1, At+1|h0

t , h, γ2, a, d′

t+1:T ).

(PO speciﬁes O0:N
t+1(h0, γ1, O0

t+1)(h, a, O1:N
t+1)

t+1 given St+1)

At+1 = d′

At+1 = d′

t+1(h0, γ2, O0

(cid:9)
t+1)(h, a, O1:N
t+1)

(deﬁnition of d′

(cid:9)
t+1)
(symmetric argument)

=

=

=

=

=

=

=

Induction step: assuming the claim holds for τ , we show it holds for τ + 1 as well.

P(St+1:τ +1, O0:N

t+1:τ +1, At+1:τ +1|h0

= P(St+1:τ , O0:N

· P(Sτ +1, O0:N

= P(St+1:τ , O0:N

· P(Sτ +1, O0:N

= P(St+1:τ , O0:N

t+1:τ , At+1:τ |h0
τ +1, Aτ +1|h0
t+1:τ , At+1:τ |h0
τ +1, Aτ +1|h0
t+1:τ , At+1:τ |h0

t+1:T )

t , h, γ1, a, d′
t+1:T )
t+1:T , St+1:τ , O0:N
t+1:T )
t+1:T , St+1:τ , O0:N
t+1:T ) · P(Sτ +1|Sτ , Aτ ) · P(O0:N

t+1:τ , At+1:τ )

t+1:τ , At+1:τ )

(induction hypothesis)

τ +1|Sτ +1)

· I

Aτ +1 = d′

τ +1

t , γ1, O0
h0

t+1(h0

t , γ1, O0

t+1), O0

t+2, . . . , O0

τ +1

(h, a, O1:N

t+1 , At+1, . . . , O1:N
τ +1)

(†)
= P(St+1:τ , O0:N
(cid:8)
Aτ +1 = d′

t+1:τ , At+1:τ |h0
(cid:0)
t , γ2, O0
h0

· I

τ +1

= P(St+1:τ , O0:N
(cid:8)

· P(Sτ +1, O0:N
= P(St+1:τ +1, O0:N

t+1:τ , At+1:τ |h0
(cid:0)
τ +1, Aτ +1|h0
t+1:τ +1, At+1:τ +1|h0

t+1:T ) · P(Sτ +1|Sτ , Aτ ) · P(O0:N
(cid:1)
τ +1|Sτ +1)

t+1(h0

t , γ2, O0

t+1), O0

t+2, . . . , O0

τ +1

(cid:9)

(h, a, O1:N

t+1 , At+1, . . . , O1:N
τ +1)

(cid:1)

(cid:9)

t+1:T )
t+1:T , St+1:τ , O0:N

t+1:τ , At+1:τ )

t , h, γ2, a, d′

t+1:T ),

where the equality in (†) holds due to the deﬁnition of policy d′. Note that with the CI-based
approach, a generic policy dt ﬁrst maps an FCS H 0
to a prescription Γt, which in term maps
t
an FPS H 1:N
t )(H 1:N
to an action At; therefore, dt(H 0
) = At refers to the ﬁ-
t
nal action At under the policy dt and the supervisor’s state (H 0
). The claim implies that

) = Γt(H 1:N
t
t , H 1:N
t

t

15

t , h, γ1, a, d′
t , h, γ1, a, d′
t , h, γ2, a, d′
t , h, γ1, a, d′
t , h, γ2, a, d′
t+1, d′
t , h, γ2, a, d′
t+1, d′
t , h, γ2, a, d′
t , h, γ2, a, d′

t , h, γ1, d′

P(Sτ , Aτ |h0
ing on h0
where τ = t + 1, . . . , T is what the expectations on both sides of (∗) are taken on.

t+1:T ) for all τ = t + 1, . . . , T , i.e. condition-
t+1:T , the distribution of (Sτ , Aτ ) is exactly the same given γ1 or γ2; and (Sτ , Aτ )
(cid:4)

t+1:T ) = P(Sτ , Aτ |h0

t , h, γ2, d′

t , h, d′

Proof of Lemma 15: We preceed the proof by mathematical induction. The instantaneous part and
the base case t = T follow trivially from (ASPS2)

E

Rt(St, At)|h0
E
Rt(St, At)|h0
≤
(cid:12)
(cid:2)
(cid:12)
≤ ǫp/4 + ǫp/4
(cid:12)
(cid:12)
= ǫp/2.

(cid:2)

t , h1, γ
t , h1, γ

− E
− E

Rt(St, At)|h0
t , h2, γ
Rt(St, At)|h0
z, γ
t ,
(cid:2)

(cid:3)

+
(cid:3)(cid:12)
(cid:12)

E

Rt(St, At)|h0
t ,

z, γ

− E

(cid:3)

(cid:2)

(cid:3)(cid:12)
(cid:12)

(cid:2)

(cid:12)
(cid:12)

b

(cid:3)

b

Rt(St, At)|h0
((ASPS2))
(cid:2)

t , h2, γ

(cid:3)(cid:12)
(cid:12)

For the continuation part, we have

V S
t+1((H 0
P(o0:N
t+1|h0

t , Γt, O0
t , h1, γ)V S

t+1), H 1:N
t+1((h0

t+1 )|h0

t , h1, γ

t , γ, o0

t+1), (h1, a, o1:N

t+1))

(cid:3)

E

(cid:2)
Xo0:N

t+1

P(o0:N

t+1, st+1|h0

t , h1, γ)V S

t+1((h0

t , γ, o0

t+1), (h1, a, o1:N

t+1))

P(o0:N

t+1|h0

t , h1, γ, st+1) · P(st+1|h0

t , h1, γ) · V S

t+1((h0

t , γ, o0

t+1), (h1, a, o1:N

t+1))

P(o0:N

t+1|st+1) · P(st+1|h0

t , h1, γ) · V S

t+1((h0

t , γ, o0

t+1), (h1, a, o1:N

t+1))

P(o0:N

t+1|st+1) · P(st+1|h0

t , h1, a) · V S

t+1((h0

t , γ, o0

t+1), (h1, a, o1:N

t+1))

(PO speciﬁes O0:N

t+1 given St+1)
(Lemma 17)

Xo0:N
st+1
t+1 X

Xo0:N
st+1
t+1 X

Xo0:N
st+1
t+1 X

Xo0:N
st+1
t+1 X

P(o0:N

t+1|h0

t , h1, a)V S

t+1((h0

t , γ, o0

t+1), (h1, a, o1:N

t+1)),

=

=

=

=

=

=

Xo0:N

t+1

and the same equality holds for h2. Therefore,

hV

S
t+1((H 0

t , Γt, O0

t+1), H 1:N

t+1 )|h0

t , h1, γi − E

hV

S
t+1((H 0

t , Γt, O0

t+1), H 1:N

t+1 )|h0

P(o0:N

t+1|h0

t , h1, a)V

P(o0:N

t+1|h0

t , h1, a)V

S
t+1((h0

t , γ, o0

t+1), (h1, a, o1:N

t+1)) −

S
t+1((h0

t , γ, o0

t+1), (h1, a, o1:N

t+1)) −

P(o0:N

t+1|h0
t ,

z, a)V
b

S
t+1((h0

t , γ, o0

t+1), (h1, a, o1:N

t+1)) −

t , h2, γi(cid:12)
(cid:12)
(cid:12)
S
t , γ, o0
t+1((h0

P(o0:N

t+1|h0

t , h2, a)V

t+1), (h2, a, o1:N

t+1))

P(o0:N

t+1|h0
t ,

z, a)V
b

S
t+1((h0

t , γ, o0

t+1), (h1, a, o1:N

t+1))

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

P(o0:N

t+1|h0

t , h2, a)V

S
t+1((h0

t , γ, o0

t+1), (h1, a, o1:N

t+1))

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

X
o0:N
t+1

X
o0:N
t+1

X
o0:N
t+1

P(o0:N

t+1|h0

t , h2, a)V

S
t+1((h0

t , γ, o0

t+1), (h1, a, o1:N

t+1)) −

P(o0:N

t+1|h0

t , h2, a)V

X
o0:N
t+1

S
t+1((h0

t , γ, o0

t+1), (h2, a, o1:N

t+1))

=

≤

E
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

X
o0:N
t+1

X
o0:N
t+1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

X
o0:N
t+1

X
o0:N
t+1

+

+

:= 1(cid:13) + 2(cid:13) + 3(cid:13).

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

For the ﬁrst two terms, we have

1(cid:13), 2(cid:13) ≤ 2kVt+1k∞ · δp/8 ≤ T ¯Rδp/4

16

(cid:14)

minx,y∈Ω(O0:N

If
by (ASPS3). Note that the above equation follows if K(·, ·) is the total variation distance.
it is instead the Wasserstein metric, then the total variation distance will still be bounded by
t+1),x6=y kx − yk; we can redeﬁne this value as δp so that the total variation dis-
δp
tance is still bounded by δp.
Now consider a ﬁxed realization of o0:N
t+1. We have
ϑ1:N
t+1), (h1, a, o1:N
t+1((h0
t , γ, o0
φ1:N
ϑ1:N
t , γ, o0:N
t+1(
t+1)
t
b
φ1:N
ϑ1:N
t , γ, o0:N
t+1(
t+1)
t
b
b
ϑ1:N
t+1), (h2, a, o1:N
t+1((h0
((ASPS1))
=
b
b
t+1), the two FPSs (h1, a, o1:N
so that under the public FCS (h0
t , γ, o0
t+1) will be
b
mapped to the same ASPS as well. Hence, by the induction hypothesis of Lemma 15 which leads to
Corollary 16 at the t + 1 step, we obtain

t+1) and (h2, a, o1:N

(h1), h0
(h2), h0

(assumption)

((ASPS1))

t , γ, o0

t+1)),

t+1))

=

=

V S
t+1((h0

t , γ, o0

t+1), (h1, a, o1:N

t+1)) − V S

t+1((h0

t , γ, o0

t+1), (h2, a, o1:N

t+1))

(cid:12)
(cid:12)

≤ (T − t − 1)(ǫp + T ¯Rδp)/2 + ǫp/2.

(cid:12)
(cid:12)

The last term can thus be bounded by
V S
t+1|h0
t+1((h0

t , h2, a)

P(o0:N

3(cid:13) ≤

t , γ, o0

t+1), (h1, a, o1:N

t+1)) − V S

t+1((h0

t , γ, o0

t+1), (h2, a, o1:N

t+1))

Xo0:N

t+1

≤

P(o0:N

t+1|h0

t , h2, a)[(T − t − 1)(ǫp + T ¯Rδp)/2 + ǫp/2]

(cid:12)
(cid:12)

Xo0:N

t+1

= (T − t − 1)(ǫp + T ¯Rδp)/2 + ǫp/2.

Combining the three terms plus the instantaneous part, it follows that

QS

t (h0

t , h1, γ) − QS

t (h0

t , h2, γ)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

≤ ǫp/2 + 2 · T ¯Rδp/4 + (T − t − 1)(ǫp + T ¯Rδp)/2 + ǫp/2
= (T − t)(ǫp + T ¯Rδp)/2 + ǫp/2.

(cid:12)
(cid:12)

(cid:4)

Proof of Corollary 16: Assume the optimal prescription γ∗ prescribes different actions on the two
FPSs h1, h2 ∈ Ω(H 1:N
), so that γ∗(h1) = a1 and γ∗(h2) = a2 where a1 6= a2; otherwise, the
claim directly follows by Lemma 15. Also, deﬁne γ′, γ′′ ∈ Ω(Γt) by

t

γ′(h) =

(cid:26)

if h = h2,
a1
γ∗(h) otherwise,

γ′′(h) =

if h = h1,
a2
γ∗(h) otherwise.

(cid:26)

Let υ1 = QS
t , h1, γ∗) and υ2 = QS
for simplicity. From (12), Qt(ht, γ∗) can be expanded as

t (h0

t (h0

t , h2, γ∗). Denote Bt , (T − t)(ǫp + T ¯Rδp)/2 + ǫp/2

Qt(h0

t , γ∗) =

P(h|h0

t )QS

t (h0

t , h, γ∗) + P(h1|h0

t )υ1 + P(h2|h0

t )υ2.

Likewise, we can also expand Qt(ht, γ′) to

h6=h1,h2
X

Qt(h0

t , γ′)

P(h|h0

t )QS

t (h0

t , h, γ′) + P(h1|h0

t )QS

t (h0

t , h1, γ′) + P(h2|h0

t )QS

t (h0

t , h2, γ′)

P(h|h0

t )QS

t (h0

t , h, γ′) + P(h1|h0

t )QS

t (h0

t , h1, γ′) + P(h2|h0
t )

QS

t (h0

t , h1, γ′) − Bt

(cid:2)

(Lemma 15)

(cid:3)

P(h|h0

t )QS

t (h0

t , h, γ∗) + P(h1|h0

t )QS

t (h0

t , h1, γ∗) + P(h2|h0
t )

QS

t (h0

t , h1, γ∗) − Bt

(cid:2)

(cid:3)
(Lemma 14)

P(h|h0

t )QS

t (h0

t , h, γ∗) + P(h1|h0

t )υ1 + P(h2|h0

t )(υ1 − Bt);

=

≥

=

=

h6=h1,h2
X

h6=h1,h2
X

h6=h1,h2
X

h6=h1,h2
X

17

by symmetry

Qt(h0

t , γ′′) ≥

h6=h1,h2
X

We have

P(h|h0

t )QS

t (h0

t , h, γ∗) + P(h1|h0

t )(υ2 − Bt) + P(h2|h0

t )υ2.

P(h|h0

t )QS

t (h0

t , h, γ∗) + P(h1|h0

t )υ1 + P(h2|h0

t )(υ1 − Bt) ≤ Qt(h0

t , γ′)

h6=h1,h2
X
≤ Qt(h0

t , γ∗) =

h6=h1,h2
X

and

P(h|h0

t )QS

t (h0

t , h, γ∗) + P(h1|h0

t )υ1 + P(h2|h0

t )υ2

P(h|h0

t )QS

t (h0

t , h, γ∗) + P(h1|h0

t )(υ2 − Bt) + P(h2|h0

t )υ2 ≤ Qt(h0

t , γ′′)

h6=h1,h2
X
≤ Qt(h0

t , γ∗) =

P(h|h0

t )QS

t (h0

t , h, γ∗) + P(h1|h0

t )υ1 + P(h2|h0

t )υ2.

Canceling and rearranging the terms yield

h6=h1,h2
X

−Bt ≤ υ1 − υ2 ≤ Bt.

(cid:4)

Proof of Theorem 4: We prove the result by induction. The base case trivially follows from Propo-
T +1) , 0 and
sition 13. Note that the continuation values at T + 1 are deﬁned to be 0, i.e. VT +1(h0
T ) and γ∗ ∈ Ω(Γt), we
VT +1(h0
have
b
QT (h0

T +1). Hence, for any h0

T +1) , 0 for any h0

T +1 ∈ Ω(H 0

T , γ∗) − ǫp = VT (h0

T ∈ Ω(H 0

QT (h0
T ,

QT (h0

VT (h0

λ) =

T ).

T , γbλ,h0

T ) − ǫp ≤ max
b
ΛT )

bλ∈Ω(

) = max
b
bλ∈Ω(
ΛT )

T

In the equation, QT (h0
λ) because there is no continuation value for T . Now
for the induction step, we assume the induction hypothesis, i.e. the claim holds for some t + 1 ≤ T
so that we have for any h0

QT (h0
T ,

T , γbλ,h0

) =

t+1 ∈ Ω(H 0
t+1),
b

b

T

b

b

b

Vt+1(h0

t+1) −

Vt+1(h0

t+1) ≤

(T − t − 1)(T − t)
2

(ǫp + T ¯Rδp) + (T − t)ǫp.

t ∈ Ω(H 0

t ) and optimal prescription γ∗ ∈ argmaxγ Qt(h0

t , γ),

Proposition 13 states that for any h0
there exists a
λ ∈ Ω(

b
Λt) such that
Qt(h0
b

b

t , γ∗) − Qt(h0

t , γbλ,h0

t

) ≤ (T − t)(ǫp + T ¯Rδp) + ǫp.

Write Ct , (T − t)(ǫp + T ¯Rδp) + ǫp for shorthand of notation. Then for this

λ, we have

λ) = Qt(h0

Qt(h0

t , γ∗) −
Qt(h0
t ,
≤ Ct + E[Rt + Vt+1(H 0
t+1)|h0
b
t , γbλ,h0

b
P(h0
t+1|h0

= Ct +

)

t

t , γbλ,h0

t

Vt+1(h0
h

t+1) −

Vt+1(h0
b

t+1)
i

t , γ∗) − Qt(h0
] − E[Rt +

t , γbλ,h0
Vt+1(H 0

t

) + Qt(h0

t+1)|h0

) −

t , γbλ,h0
b
t
]

t , γbλ,h0

t

Qt(h0
t ,

λ)

b

b

P(h0

t+1|h0

t , γbλ,h0

t

)

(cid:20)

b

(T − t − 1)(T − t)
2

(ǫp + T ¯Rδp) + (T − t)ǫp

(cid:21)

≤ Ct +

Xh0

t+1

Xh0

t+1

= (T − t)(ǫp + T ¯Rδp) + ǫp +

(T − t − 1)(T − t)
2

(ǫp + T ¯Rδp) + (T − t)ǫp

=

(T − t)(T − t + 1)
2

(ǫp + T ¯Rδp) + (T − t + 1)ǫp.

(cid:4)

18

C Omitted Analysis in Section 3.2

Proposition 18: Assume the reward function R is uniformly bounded by ¯R. Let h0
t (h0
Ω(H 0
ϑ0

t ) be two FCSs. If

2), then for any

t (h0
ϑ0

λ ∈ Ω(

z0 =

1) =

Λt),

1, h0

2 ∈

λ) −
b

Qt(h0
2,

λ)
b

≤ 2(T − t)(ǫc + T ¯Rδc) + 2ǫc.

b

b

(21)

Qt(h0
1,
b
(cid:12)
(cid:12)
(cid:12) b

b

b

(cid:12)
(cid:12)
(cid:12)

b

Proof: We proceed the proof again by mathematical induction. The instantaneous part as well as
the base case t = T trivially follow from (ASCS2)

Rt(St, At)|h0
1,
h
Rt(St, At)|h0
1,

λ

− E

− E

i

λ
b
i

≤

E

(cid:12)
E
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

h

≤ ǫc + ǫc = 2ǫc.

(cid:12)
(cid:12)
(cid:12)
For the continuation part in the induction step, we have

b

b

b

h

Rt(St, At)|h0
2,
h
Rt(St, At)|

z0,

λ

i(cid:12)
(cid:12)
λ
+
b
(cid:12)
i(cid:12)
(cid:12)
(cid:12)

E

Rt(St, At)|

z0,

λ

− E

Rt(St, At)|h0
2,

λ

h

i

b

b

h

i(cid:12)
((ASCS2))
(cid:12)
b
(cid:12)

=

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xo0

t+1

Xo0

t+1

+

E

Vt+1((H 0
t ,

Λt, O0

t+1))|h0
1,

h

b
P(o0

b
t+1|h0
λ)
1,

Vt+1((h0
1,

λ

− E

Vt+1((H 0
t ,
i
h
b
b
λ, o0
t+1)) −

P(o0

Λt, O0

t+1))|h0
2,

λ

b
t+1|h0
2,

i(cid:12)
(cid:12)
b
(cid:12)
Vt+1((h0
2,

λ)

λ, o0

P(o0

t+1|h0
1,

λ)

b

b
Vt+1((h0
1,

P(o0

t+1|

z0,

λ)

b

b
Vt+1((h0
1,

b
λ, o0

Xo0

t+1

Xo0

t+1

b
λ, o0

t+1)) −

b

b
b
z0,

P(o0

t+1|

b
t+1|h0
2,

t+1

Xo0

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Xo0
(cid:12)
(cid:12)
:= 1(cid:13) + 2(cid:13) + 3(cid:13).
(cid:12)
(cid:12)

P(o0

+

t+1

λ)

Vt+1((h0
1,

λ, o0

t+1)) −

b

λ)

b
Vt+1((h0
1,

b
λ, o0

t+1)) −

b

b

b

b
b
b
P(o0
t+1|h0
2,

P(o0

t+1|h0
2,

Xo0

t+1

Xo0

t+1

b

λ)

Vt+1((h0
1,

b

λ)

b
Vt+1((h0
2,

b

b

t+1))
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
t+1))
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
λ, o0
t+1))
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
t+1))
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

b
λ, o0

b

For the ﬁrst two terms, we have

1(cid:13), 2(cid:13) ≤ 2k

Vt+1k∞ · δc/2 ≤ T ¯Rδc

by (ASCS3).
Now consider a ﬁxed realization of o0

b
t+1. We have

λ, o0

t+1))
λ, o0
λ, o0
b
λ, o0
t+1)),
b
t+1) and (h0
so that the two FCSs (with ASPS-based prescription) (h0
2,
1,
to the same ASCS as well. Hence, by the induction hypothesis, we obtain

t+1((h0
ϑ0
1,
ϑ0
φ0
t (h1),
t+1(
b
b
ϑ0
φ0
t (h2),
t+1(
b
b
t+1((h0
ϑ0
2,
b
b

t+1)
t+1)

λ, o0

=

=

=

b

b

((ASCS1))

(assumption)

((ASCS1))

λ, o0

t+1) will be mapped

Vt+1((h0
1,

λ, o0

t+1)) −

Vt+1((h0
2,

λ, o0

≤ 2(T − t − 1)(ǫc + T ¯Rδc) + 2ǫc.

b

b

The last term can thus be bounded by

b

b

b

(cid:12)
(cid:12)
(cid:12) b

t+1))
(cid:12)
(cid:12)
(cid:12)

3(cid:13) ≤

P(o0

t+1|h0
2,

λ)

Vt+1((h0
1,

λ, o0

t+1)) −

Vt+1((h0
2,

λ, o0

Xo0

t+1

(cid:12)
(cid:12)
(cid:12) b

b

b

b

b

19

t+1))
(cid:12)
(cid:12)
(cid:12)

≤

P(o0

t+1|h0
2,

λ)[2(T − t − 1)(ǫc + T ¯Rδc) + 2ǫc] = 2(T − t − 1)(ǫc + T ¯Rδc) + 2ǫc.

Xo0

t+1

b

Combining the three terms plus the instantaneous part, it follows that

Qt(h0
1,

λ) −

Qt(h0
2,

λ)

(cid:12)
(cid:12)
(cid:12) b

b

b

(cid:12)
(cid:12)
(cid:12)

b

≤ 2ǫc + 2 · T ¯Rδc + 2(T − t − 1)(ǫc + T ¯Rδc) + 2ǫc
= 2(T − t)(ǫc + T ¯Rδc) + 2ǫc.

(cid:4)

Proof of Theorem 6: We proceed the proof again by mathematical induction. The base case t = T
trivially follows from (ASCS2). For the induction step, we have

V

= E
b

Qt(h0
t ,

λ)

t (h0
ϑ0

λ) − Q

t(
Rt(St, At)|h0
t ,
b
b
i
t+1)|h0
Vt+1(H 0
t ,
b

t ),
− E
b
h

λ

λ

h
+ E

Rt(St, At)|

t (h0
ϑ0

t ),

λ

− E

V

V

i
t+1(H 0
ϑ0
t+1))|
t+1(
b
b

t (h0
ϑ0

t ),

λ

h

≤ ǫc + E

= ǫc + E

Vt+1(H 0
b
h

Vt+1(H 0
b

i
λ
b

i
λ
b

t+1)|h0
t ,

− E

h

V

V

t+1(H 0
ϑ0
t+1(
b

t (h0
ϑ0
t+1))|
b

t ),

t+1)|h0
t ,

− E

t (h0
ϑ0

λ
t ),
b

((ASCS2))

i
λ
b

i

b

h
Vt+1(H 0
t+1)|
b
h
− E
b

t+1(

V

V

h

b
Xh0
t+1 h

+ E

h
Vt+1(H 0
b

t+1)|

i
ϑ0
t (h0
t ),
b

λ

ϑ0
t+1(H 0
b

i
t+1))|
b

ϑ0
t (h0

t ),

λ

= ǫc +

P(h0

i
λ) − P(h0
t+1|h0
t ,
b

b

t+1|

h
t (h0
ϑ0

t ),

λ)
b

Vt+1(h0

t+1)
b

i

b

+

P(h0

t+1|

Xh0
t+1
:= ǫc + 1(cid:13) + 2(cid:13).

b
t (h0
ϑ0
t ),

λ)

b

b

V

b
b
Vt+1(h0
t+1) − V
h

b

i

b
t+1(

t+1(h0
ϑ0

t+1))
i

b

The ﬁrst term is bounded by (ASCS3)

1(cid:13) =

P(o0

t+1|h0
t ,

λ) − P(o0

t+1|

t (h0
ϑ0

t ),

λ)

Vt+1((h0
t ,

λ, o0

t+1))

Xo0
t+1 h
Vt+1k∞ · δc/2 ≤ T ¯Rδc,
≤ 2k

b

b

i

b

b

b

while the second term can be bounded by the induction hypothesis

b

2(cid:13) ≤

P(h0

t+1|

ϑ0
t (h0

t ),

λ)

(T − t − 1)(ǫc + T ¯Rδc) + ǫc

Xh0

t+1

b
= (T − t − 1)(ǫc + T ¯Rδc) + ǫc.

b

(cid:2)

(cid:3)

Combining the terms, it follows that

V

Qt(h0
t ,

λ) − Q

t(

t (h0
ϑ0

t ),

λ) ≤ ǫc + T ¯Rδc + (T − t − 1)(ǫc + T ¯Rδc) + ǫc

b

b

b

b

= (T − t)(ǫc + T ¯Rδc) + ǫc.

The V part of the claim can be obtained by considering an optimal prescription
argmaxbλ∈Ω(

λ) in the Q part.

Qt(h0
t ,

b
Λt)

λ∗ ∈
(cid:4)

b

D Omitted Analysis in Section 3.4

b

b

As mentioned in Section 3.4, when considering ǫc = δc = ǫp = δp = 0, we use SCS and SPS to refer
to the common and private representations. Moreover, we use Z and ϑ to denote the compressed
state and the compression mapping when the error parameters are 0, instead of

Z and

ϑ.

20

b

b

Proof of Proposition 8: For (SCS1), BCSs can be updated recursively through Bayesian updates
[Nayyar et al., 2013]. For (SCS2), notice that

E[Rt(St, At)|h0

t , λt] =

P(st, h1:N

t

Xst,h1:N

t

|h0

t )R(st, γt(h1:N

t

)),

and the ensemble of P(st, h1:N
ilarly, it satisﬁes (SCS3) as well, since

|h0

t

t ) through their spaces is exactly Πt(h0

t ) = P(St, H 1:N

t

|h0

t ). Sim-

P(O0

t+1|h0

t , γt) =

P(st, h1:N

t

|h0

t ) ·

P(st+1|st, γt(h1:N

t

)) · P(O0

t+1|st+1).

t

Xst,h1:N
t ) = P(St, H 1:N

t

The quantity Πt(h0
P(O0

t+1|h0

t , γt).

st+1
X

|h0

t ) again exactly encapsulates what is needed to compute
(cid:4)

Proof of Proposition 9:

P(z1:N

t+1 , o0

t+1|h0

t , h1:N
t

, γt, at) =

P(z1:N

t+1 , o0:N

t+1, st+1|h0

t , h1:N
t

, γt, at)

P(st+1|h0

t , h1:N
t

P(st+1|h0

t , h1:N
t

P(st+1|h0

t , h1:N
t

Xo0:N
st+1
t+1 X
, γt, at) · P(o0:N

t+1|h0

t , h1:N
t

, γt, at, st+1) · P(z1:N

t+1 |h0

t , h1:N
t

, γt, at, st+1, o0:N
t+1)

, γt) · P(o0:N

t+1|st+1) · P(z1:N

t+1 |h0

t , h1:N
t

, γt, st+1, o0:N
t+1)

(redundancy of at and PO speciﬁes Ot+1 given St+1)
, γt, st+1, o0:N
t+1)

(Lemma 17)

t , h1:N
t

t+1 |h0

t+1|st+1) · P(z1:N

, at) · P(o0:N

P(st+1, o0:N

t+1|h0

t , h1:N
t

, at) · I{z1:N

t+1 = φ1:N

t+1(ϑt(h0

t , h1:N
t

), γt, o0:N

t+1)}

((SPS1))

Xo0:N
st+1
t+1 X

Xo0:N
st+1
t+1 X

Xo0:N
st+1
t+1 X

Xo0:N
st+1
t+1 X

P(o0:N

t+1|h0

t , h1:N
t

Xo0:N

t+1

P(o0:N

t+1|h0

t , z1:N
t

, at) · I{z1:N

t+1 = φ1:N

t+1(z1:N
t

, at) · I{z1:N

t+1 = φ1:N

t+1(z1:N
t

, γt, o0:N

t+1)}

, γt, o0:N

t+1)}

((SPS3))

=

=

=

=

=

=

t+1

Xo0:N
= P(z1:N

t+1 , o0

t+1|h0

t , z1:N
t

, γt, at).

Note the last equality follows as in (SPS3) it is implicitly assumed that z1:N

t = ϑt(h0

t , h1:N
t

).

(cid:4)

Proof of Proposition 10:

E[R(St, At)|h0

t , hn

t , at] =

R(st, at)P(st|h0

t , hn

t ) =

R(st, at)

P(st, z−n

t

|h0

t , hn
t )

=

=

=

=

st
X

st
X

st
X

st
X

R(st, at)

P(z−n
t

|h0

t ) · P(st|h0

t , hn

t , z−n
t

st
X
t , hn

st
X

−n
t

Xz

)

R(st, at)

R(st, at)

R(st, at)

−n
t

Xz

−n
t

Xz

−n
t

Xz

−n
t

Xz

P(z−n
t

|h0

t , hn
t )

P(st, h−n

t

|h0

t , hn

t , z−n
t

)

P(z−n
t

|h0

t , hn
t )

P(z−n
t

|h0

t , hn
t )

−n
t

Xh

−n
t

Xh

−n
t

Xh

P(h−n
t

P(h−n
t

|h0

t , hn

t , z−n
t

|h0

t , hn

t , z−n
t

) · P(st|h0

t , hn

t , z−n
t

, h−n
t

)

) · P(st|h0

t , hn

t , h−n
t

)

(z−n

t = ϑ−n

t

(h0

t , h−n
t

))

21

=

=

=

=

=

=

=

−n
t

Xz

−n
t

Xz

−n
t

Xz

−n
t

Xz

st
X

st
X

st
X

P(z−n
t

|h0

t , hn
t )

P(h−n
t

P(z−n
t

|h0

t , hn
t )

P(z−n
t

|h0

t , hn
t )

−n
t

Xh

−n
t

Xh

−n
t

Xh

P(h−n
t

P(h−n
t

P(z−n
t

|h0

t , hn
t )

P(h−n
t

|h0

t , hn

t , z−n
t

|h0

t , hn

t , z−n
t

|h0

t , hn

t , z−n
t

|h0

t , hn

t , z−n
t

)

R(st, at)P(st|h0

t , hn

t , h−n
t

)

st
X
)E[R(St, At)|h0

t , h1:N
t

, at]

)E[R(St, At)|h0

t , z1:N
t

, at]

((SPS2)))

)

R(st, at)P(st|h0

t , zn

t , z−n
t

)

−n
t

Xh
P(z−n
t

P(z−n
t

|h0

t , hn

t ) · P(st|h0

t , zn

st
X
t , z−n
t

)

P(h−n
t

|h0

t , hn

t , z−n
t

|h0

t , zn

t ) · P(st|h0

t , zn

t , z−n
t

−n
t

Xh
) · 1

)

((SPI4))

P(st, z−n

t

|h0

t , zn

t ) =

st
X

R(st, at)P(st|h0

t , zn

t ) = E[R(St, At)|h0

t , zn

t , at].

R(st, at)

R(st, at)

R(st, at)

−n
t

Xz

−n
t

Xz

−n
t

Xz

Note that the superscript −n only contains [N ] \ {n} and does not contain 0.

(cid:4)

E Algorithmic Framework

In this section we propose an MARL algorithmic framework using the theory developed in Section 3;
the designing detail is left as future work. The framework adopts the “centralized learning distributed
execution” scheme, i.e., the agents assume the omniscient supervisor’s view when they learn the
compressions and policies.

Algorithm 6 Deep-MARL Framework

1 Common part: coordinator computes (done in each agent in execution phase)

Z 0

t = ρ0(O0
t ,

Λt−1),

Λt = ϕ0(

Z 0

t ).

2 Private part: agent n computes
b
3 if in learning phase then

b

Z n

t = ρn(On

t , An

t−1), An

t = ϕn(

Z n
t ,

Λn

t ).

b

b

4

5

6

7

8

t+1) = ψC (

Coordinator computes ( ˆRt, ˆO0
b
Λt).
( ˆRt, ˆO0
t+1) is compared with ground truth (Rt, O0
b
Supervisor computes ( ˆRt, ˆO0:N
, A1:N
t
( ˆRt, ˆO0:N
t+1 ) and loss is back-propagated to (ρ0:N , ψS).
t+1 ) is compared with ground truth (Rt, O0:N
Coordinator computes

t+1) and loss is back-propagated to (ρ0, ψC ).

t
τ =t−W Rτ and performs policy gradient on ϕ0:N .

t+1) = ψS(

b
Z 0:N
t

Z 0
t ,

).

b

b

b

P

φ0:N
t

Λt as suggested by Section 3.2; the private policy network ϕn takes

the state networks ρ0:N modeled by recurrent neural
There are three types of functions within:
networks (RNNs), the policy networks ϕ0:N modeled by deep neural networks (DNNs), and the
prediction networks ψC and ψS also modeled by DNNs. The state networks ρ0:N serve the purpose
in Deﬁnition 3 and Deﬁnition 5, and their recursive evolution
of the compression mappings
structures suggest an RNN modeling. The common policy network ϕ0 takes
t as input and gives
Λn
Z n
the prescription
t
and outputs An
t . Here, we have to use a variable to represent the prescription function; hence, it
cannot be directly applied to
t . Effective design of representing prescription function is left as
future work, even though Lemma 14 provides a nice decomposition. Finally, the policy networks
ψC and ψS are used to produce the predicted reward and new observations. In the learning phase, the
predictions are compared with the ground truth and errors are back-propagated through the state and
prediction networks. This requires full knowledge of
; consequently, the learning has
to be centralized. A windowed (with length W ) cumulative reward is summed for the computation
of the loss in policy gradient methods [Sutton and Barto, 2018], which is back-propagated through

and O1:N

t and

Z 1:N
t

Z n

Z 0

b

b

b

b

b

b

t

b

22

the policy networks; actor-critic methods can also be employed here. In the execution phase, only
state and policy networks are required, and everything can be performed in a decentralized fashion.
Note that in our proof of Theorem 4 only the fact that

can be updated from

Z 1:N

t−1 is needed.

Z 1:N
t

t , hn

To design a fully decentralized learning scheme, one needs conditions similar to (ASPS2) and
(ASPS3) but only involving on
so that individ-
ual prediction networks that does not require supervisor’s view can be designed. This might demand
a “consistency condition” similarly to (SPI4), and it is likely that this is only possible through agents
communicating through some signal space or directly sending their parameters [Zhang et al., 2019].
Further, the required expectations over the realizations of the private information conditioned on the
common information could be hard to estimate. This is also left as future work.

b
t instead of the whole o1:N
zn

, h1:N
t

t and

b
z1:N
t

and

b

b

t

23

