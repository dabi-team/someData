Ripple Attention for Visual Perception with Sub-quadratic Complexity

Lin Zheng 1 Huijie Pan 1 Lingpeng Kong 1 2

2
2
0
2

n
u
J

5
1

]

V
C
.
s
c
[

2
v
3
5
4
2
0
.
0
1
1
2
:
v
i
X
r
a

Abstract

Transformer architectures are now central to se-
quence modeling tasks. At its heart is the attention
mechanism, which enables effective modeling of
long-term dependencies in a sequence. Recently,
transformers have been successfully applied in the
computer vision domain, where 2D images are
ﬁrst segmented into patches and then treated as 1D
sequences. Such linearization, however, impairs
the notion of spatial locality in images, which
bears important visual clues. To bridge the gap,
we propose ripple attention, a sub-quadratic at-
tention mechanism for vision transformers. Built
upon the recent kernel-based efﬁcient attention
mechanisms, we design a novel dynamic program-
ming algorithm that weights contributions of dif-
ferent tokens to a query with respect to their rela-
tive spatial distances in the 2D space in linear ob-
served time. Extensive experiments and analyses
demonstrate the effectiveness of ripple attention
on various visual tasks.

1. Introduction

The transformer architecture (Vaswani et al., 2017) has been
dominant in various important natural language process-
ing (NLP) tasks, including machine translation (Vaswani
et al., 2017; Dehghani et al., 2019), language understanding
(Devlin et al., 2018), language modeling (Dai et al., 2019;
Baevski & Auli, 2019) and many others. The cornerstone of
a transformer is the attention mechanism (Bahdanau et al.,
2014) which computes pair-wise interactions between any
token pairs of the input sequence. As a result, it is capable
of modeling long-term dependencies in a sequence, which
is an important factor to the success of transformers.

Recently, the transformer architecture has also found its
applications in the domain of computer vision (CV). It is

1Department of Computer Science, The University of Hong
Kong 2Shanghai Artiﬁcial Intelligence Laboratory. Correspon-
dence to: Lin Zheng <linzheng@connect.hku.hk>.

Proceedings of the 39 th International Conference on Machine
Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-
right 2022 by the author(s).

adopted for image classiﬁcation (Dosovitskiy et al., 2020;
Touvron et al., 2020; Liu et al., 2021a; Yang et al., 2021;
Wang et al., 2021b), segmentation (Wang et al., 2020b;
Strudel et al., 2021), low-level image processing (Chen
et al., 2020), image generation (Parmar et al., 2018), object
detection (Carion et al., 2020; Meng et al., 2021) and many
other tasks. In these vision applications, a 2D image is rep-
resented as a set of patches ﬂattened into a 1D sequence.
These patches are analogous to the tokens in sequence mod-
eling tasks that are commonly seen in NLP. Nevertheless,
such linearization undermines the inherent local structure of
a 2D image, which bears important visual clues (Simoncelli
& Olshausen, 2001). There often exist strong correlations
within local neighborhoods in an image. Therefore, paying
more attention to patches in a closer region could facilitate
gathering information that is particularly useful in visual
pattern recognition. This is similar to the concept of context
in NLP, just that the structural context of a visual token is
scattered in the 1D sequence, making it difﬁcult for the trans-
former to capture such prior knowledge. In contrast, the
convolutional neural network (CNN) (Fukushima & Miyake,
1982; LeCun et al., 1989; Krizhevsky et al., 2012), which
has been the de-facto architecture in computer vision tasks
for decades, utilizes local receptive ﬁelds and achieves good
performance. The drawback of that is, as convolution oper-
ations are limited to small receptive ﬁelds, they have great
difﬁculty in extracting global image features.

Therefore, it is appealing to incorporate the notion of spatial
vicinity into the transformer, while still preserving its capac-
ity of modeling long-term dependencies. To bridge the gap,
we propose ripple attention (Figure 1; §3), an efﬁcient atten-
tion mechanism for vision transformers based on recently
proposed linearized attention variants (§2.2). In ripple at-
tention, contributions from different tokens to a query are
weighted with respect to their relative spatial distances in
the 2D space. These spatial weights are derived through a
stick-breaking transformation (§3.2), which promotes local
correlations by leaning to assign larger weights to spatially
closer tokens. We then design a dynamic programming al-
gorithm (§3.3) that is capable of executing ripple attention
in linear observed time, taking advantage of the recently
proposed linearized attention (§2.2) and the summed-area
table technique (§3.3).

We validate our method by conducting extensive experi-

 
 
 
 
 
 
Ripple Attention for Visual Perception with Sub-quadratic Complexity

ments on image classiﬁcation and object detection tasks
(§4). Ripple attention signiﬁcantly improves the accuracy of
the original vision transformer in image classiﬁcation and
performs competitively with detection transformers for ob-
ject detection (§4.3), in asymptotically faster runtime (§5.3).
Further analysis on the rippling distance and ablation stud-
ies (§5.1) indicate that ripple attention favors contributions
from tokens in the vicinity yet preserves global information
from long-term dependencies.

2. Preliminary

2.1. Attention Mechanism

Let Q ∈ RN ×D denote a set of N query vectors, which
attend to M key and value vectors, denoted by matrices
K ∈ RM ×D and V ∈ RM ×C respectively. For a query
vector at position n, the softmax attention function computes
the following quantity1:

Attn (qn, K, V ) =

(cid:1)

M
(cid:88)

exp (cid:0)q(cid:62)
n km
m(cid:48)=1 exp (q(cid:62)
:= softmax(Kqn)(cid:62)V ,

(cid:80)M

m=1

n km(cid:48))

v(cid:62)
m

(1)

which is an average of the set of value vectors V weighted
by normalized similarity between different queries and keys.
However, such quantity requires computing the similarity
between all pairs of queries and keys, incurring quadratic
complexity in both time and memory. It makes the compu-
tational overhead for long sequences prohibitive, especially
in the case of vision tasks.

2.2. Linearized Attention

To reduce the computational complexity in attention mecha-
nism, prior works propose to linearize the softmax kernel
(Choromanski et al., 2020; Katharopoulos et al., 2020; Peng
et al., 2021). In particular, they replace the exponential
kernel used in softmax functions κ(q, k) := exp (cid:0)q(cid:62)k(cid:1)
with a dot product of two feature maps φ(q)(cid:62)φ(k), where
φ(·) : RD → RD(cid:48)
. Further details about the choice of fea-
ture maps can be found in Appendix C.1. With the feature
map, linearized attention can be written as:

LA (qn, K, V ) :=

M
(cid:88)

m=1

(cid:80)M

φ(qn)(cid:62)φ(km)
m(cid:48)=1 φ(qn)(cid:62)φ(km(cid:48))
m=1 φ(km)v(cid:62)
m
m(cid:48)=1 φ(km(cid:48))

.

v(cid:62)
m

(2)

=

φ(qn)(cid:62) (cid:80)M
φ(qn)(cid:62) (cid:80)M

In other words, by grouping together the computations of
keys and values, their statistics can be shared for all queries.
It therefore achieves linear complexity in both time and

1We omit the scaling factor for simplicity.

memory with respect to the length of the sequence, as we
only need to compute (cid:80)M
m=1 φ(km)v(cid:62)
m=1 φ(km)
once and then reuse them for each query.

m and (cid:80)M

3. Model

In this section, we introduce ripple attention, a novel at-
tention mechanism that features the relative spatial vicinity.
We start from a reformulation of the linearized attention
(§2.2) under the notation of vicinal groups (§3.1). This re-
formulation makes it straightforward to introduce a spatial
weight associated with each vicinal group, which is the cor-
nerstone of ripple attention. We then describe the derivation
of these spatial weights through a stick-breaking transfor-
mation (§3.2) and a dynamic programming algorithm (§3.3)
based on the summed-area table technique to perform the
computation of ripple attention efﬁciently.

3.1. Ripple Attention

We assume an input image consists of H × W patch tokens.
Given a query token at position (i, j), we partition the whole
set of patch tokens into R + 1 vicinal groups {Nr(i, j)}R
r=0,
according to their Chebyshev (or chessboard) distances r
from the position to the query, which means for every token
at position (m, n) ∈ Nr(i, j) we have max(|m − i|, |n −
j|) = r. Illustrations of such vicinal group partitioning can
be found in Figure 1a or Figure 1b, where each group is
marked by a different color.

Under the notation of vicinal groups, we can reformulate
the linearized attention LA (qij, K, V ) as:

(cid:80)

φ(qij)(cid:62) (cid:80)R
φ(qij)(cid:62) (cid:80)R

r=0

r=0

(m,n)∈Nr(i,j) φ(kmn)v(cid:62)
mn
(cid:80)
(m(cid:48),n(cid:48))∈Nr(i,j) φ(km(cid:48)n(cid:48))

.

(3)

This formulation is computationally equivalent since the
summations over φ(k)v(cid:62) and φ(k) also cover all positions
within the image. The essence of ripple attention is to let
tokens respond differently to a query, according to their
relative distances. Typically, tokens close to the query in the
2D space should weigh more than the tokens far away in
general, since there exist strong local correlations in images.
This control is translated into a spatial weight αr(i, j) as-
sociated with each vicinal group Nr(i, j) and can be easily
introduced into linearized attention2:

RippleAttn (qij, K, V ) :=
r=0 αr(i, j) (cid:80)
φ(qij)(cid:62) (cid:80)R
r=0 αr(i, j) (cid:80)
φ(qij)(cid:62) (cid:80)R

(m,n)∈Nr(i,j) φ(kmn)v(cid:62)
mn
(m(cid:48),n(cid:48))∈Nr(i,j) φ(km(cid:48)n(cid:48))

.

(4)

2The partitioning is hard to implement in softmax attention
mechanism. More discussions about ripple-softmax and its com-
plexity can be found in Appendix A.

Ripple Attention for Visual Perception with Sub-quadratic Complexity

(a)

(b)

(c)

Figure 1: A demonstration of vicinal groups in ripple attention on a 9 × 9 image I. Each square denotes a token, the circle
denotes the query position and we use deeper color to indicate a larger spatial weight. Left (a): an example of vicinal group
partitioning in the case where the query lies in the center of an image, resulting in a symmetric rippling effect over the 2D
space; Middle (b): another group partitioning on the same image but the query token is not centered. In this case, distal
vicinal groups (the top-left corner) receives almost no spatial weights. Right (c): the same case as in (b) with the threshold
τ set such that ˆr = 4. Groups beyond ˆr − 1 (indicated by the dashed line) contribute according to an equal spatial weight.

We deﬁne αr(i, j) ∈ (0, 1), ∀r = 0, 1, . . . , R and
(cid:80)R
r=0 αr(i, j) = 1, to reweigh contributions of different
vicinal groups with respect to a query in the attention compu-
tation. By respecting the spatial structure of images, ripple
attention constructs a structured context for queries, which
facilitates the model to reconcile both global and local infor-
mation. The name ripple attention comes from its similarity
to the ripples on the surface of water (Figure 1).

In other words, we only assume the supremum of each spa-
tial weight αr is decreasing. A stronger constraint could
be monotonicity, for example αr > αr(cid:48) if r < r(cid:48). We ar-
gue that the former is more favorable, because it offers the
ﬂexibility to let distant tokens outweigh when necessary;
the effective modeling of such long-term dependencies is
deemed as the key to the success of the transformer archi-
tecture as well.

3.2. Spatial Weights

To derive the spatial weights αr ∈ (0, 1) ∀r = 0, 1, . . . , R,3
we ﬁrst deﬁne a sequence of scalars {sr}, where sr ∈
(0, 1) ∀r = 1, . . . , R. The spatial weights are parameter-
ized as follows (with sR+1 = 1):

αr =

(cid:40)

s1,
sr+1

(cid:81)

r(cid:48)≤r (1 − sr(cid:48)) ,

if r = 0
otherwise

(5)

The sequence {sr} is generated through a small neural net-
work followed by a sigmoid function. See Appendix C.3
for more details. Our construction is analogous to the stick-
breaking process in Bayesian nonparametrics (Wasserman,
2006), except that sr here is a deterministic scalar rather
than a random variable. One of its appealing properties is:

sup






sr+1

(cid:89)

i≤r

(1 − si) : sr+1 ∈ (0, 1)






≥

In theory, the stick-breaking transformation produces R + 1
different spatial weights. However, as the weights be-
come trivially small near the end of this transformation,
the computational overhead incurred by them becomes
worthless. Therefore, we deﬁne a threshold τ to adap-
tively terminate the transformation at vicinal group Nˆr(i, j)
when the length of remaining stick is less than τ (i.e.,
1 − (cid:80)ˆr
r=0 αr(i, j) < τ ). We then merge all vicinal groups
Nr(i, j) with r ≥ ˆr and share the same weights among
them assuming they contribute equally:

αr(i, j) =

1 − (cid:80)ˆr−1

r(cid:48)=0 αr(cid:48)(i, j)

R − ˆr + 2

, if r >= ˆr.

(6)

This truncating-and-merging operation is demonstrated in
Figure 1c, where the threshold τ is set such that ˆr = 4. In
this case, all the remaining groups (outside the dashed line)
share the same weight according to equation 6. Compared
to Figure 1b, adaptive ripple allows a stop of the transforma-
tion before hitting the boundary, which prevents potentially
worthless computations for distal groups. At the same time,
it does weigh in the contributions from those groups, pre-
serving the ability to capture long-term dependencies.

sup






sr(cid:48)+1

(cid:89)

i≤r(cid:48)

(1 − si) : sr(cid:48)+1 ∈ (0, 1)






if r < r(cid:48).

3.3. Dynamic Programming

3In this section, we sometimes drop the dependence on position

(i, j) for spatial weights αr(i, j) when there is no ambiguity.

The only problem left now is how to compute ripple atten-
tion effectively. A naïve implementation of equation 4 has

Ripple Attention for Visual Perception with Sub-quadratic Complexity

a time complexity of O(HW R2). Since R is bounded by
max(H, W ) − 1, the computation is quadratic with respect
to the length of the sequence. We give detailed derivations of
runtime complexity of each attention variant in Appendix A.

In this section, we present a dynamic programming algo-
rithm built on the summed-area table (SAT) technique, a
classic algorithm in computer graphics and computer vision
(Crow, 1984; Viola et al., 2001), which reduces the time
complexity of ripple attention to O(HW R). SAT is an ef-
ﬁcient data structure that stores preﬁx sums for each pixel
position of an image such that summations over any window
in the image can be retrieved in constant time. For an image
I with height H and width W , it ﬁrst initializes the table
by computing the cumulative sum S of all the tokens above
and to the left of (i, j) inclusively in the 2D plane4:

S(i, j) =

i
(cid:88)

j
(cid:88)

i(cid:48)=1

j(cid:48)=1

I(i(cid:48), j(cid:48)).

(7)

For a square window with center (i, j) and radius r (i.e.,
a region centered at (i, j) with both its height and width
equal to 2r + 1), the summation over its elements is denoted
by W(i, j, r) and can be computed in constant time (Crow,
1984):

W(i, j, r) := S(i + r, j + r) − S(i − r − 1, j + r)−
S(i + r, j − r − 1) + S(i − r − 1, j − r − 1).

(8)

In this work, we consider {φ(kij)v(cid:62)
ij} and {φ(kij)} as
generalized pixels within the input image and construct
two SATs S1 and S2 to compute their preﬁx summations
respectively. According to equation 8, the window sums can
be obtained efﬁciently and are denoted as W1 and W2.
We show that the sum of φ(k)v(cid:62) and φ(k) over vicinal
group Nr(i, j) can also be computed within constant time
from SATs:
(cid:88)

mn = W1(i, j, r) − W1(i, j, r − 1);

φ(kmn)v(cid:62)

(m,n)∈Nr(i,j)
(cid:88)

(m,n)∈Nr(i,j)

φ(kmn) = W2(i, j, r) − W2(i, j, r − 1).

Intuitively, this can be viewed as taking the difference be-
tween the largest square window wrapped by the group and
the smallest square window containing the group. Equipped
with SATs, the formulation of ripple attention becomes:

RippleAttn (qij, K, V ) =
φ(qij)(cid:62) (cid:80)R
φ(qij)(cid:62) (cid:80)R

r=0 αr(i, j) (W1(i, j, r) − W1(i, j, r − 1))
r=0 αr(i, j) (W2(i, j, r) − W2(i, j, r − 1))

.

(9)

4In practice, we adopt a linear-complexity implementation
which ﬁrst performs the cumulative summation over the row axis
and then the column axis, yielding the same result.

In §3.2, we merge all groups Nr(i, j) with r >= ˆr assum-
ing equal contributions (equation 6), which can be jointly
computed in constant time using S(H, W ) − W(i, j, ˆr − 1).
Therefore, given a reasonable hyper-parameter choice of
τ , the algorithm can achieve linear observed time in the
sequence length. This is due to the fact that after the pre-
computation of SATs (in linear complexity), for each query
the required summations of vicinal groups can be computed
in constant time.

Efﬁcient Gradient Computation. The algorithm dis-
cussed above addresses the runtime complexity of the for-
ward pass of ripple attention.
In Appendix B, we also
present a dynamic programming algorithm to compute gra-
dients for the backward pass, again in O(HW R) time and
space complexity. The main idea is to utilize the symmetry
of vicinal groups and reformulate the gradient calculations
as summations over different groups, where computations
could be further reduced using SATs; in contrast, a nai¨ve
implementation would come with O(HW R2) complexity.

Complexity Analysis. As mentioned above, ripple atten-
tion runs in O(HW R) time complexity with the help of
dynamic programming on the introduced vicinal groups;
and it could achieve linear observed runtime in practice
with appropriate hyper-parameter conﬁguration. Algorithm
1 sketches the dynamic programming for the ripple attention
mechanism given a single query and a threshold τ .5 Due
to the ﬂexibility of ripple attention, its time complexity can
be further improved if we adapt the step size of the rippling
process. For example, we could achieve O(HW log R) time
complexity if we allow ripples to be exponentially thicker;
see §5.2 for more detailed discussion. As for the memory
consumption, we observe that the tensor W does not even
need to be explicitly materialized, since previously com-
puted results for closer vicinal groups could be reused for
more distant vicinal groups. Therefore, the space complex-
ity of ripple attention remains O(HW ) irrespective of the
rippling distance R.

4. Experiments

We conduct extensive experiments on image classiﬁcation
and detection tasks to demonstrate the effectiveness of ripple
attention.

4.1. Experimental Setup

Datasets For image classiﬁcation, we evaluate our model
on standard benchmark datasets: (1) ImageNet1k dataset
(Deng et al., 2009), consisting of approximately 1,280K/50K
images of 1000 classes for training/validation splits respec-

5Note that the algorithm can be easily executed in parallel for

all queries.

Ripple Attention for Visual Perception with Sub-quadratic Complexity

Algorithm 1 Dynamic Programming for Ripple Attention
Input: the key-value statistics I ∈ RH×W , query posi-
tion (i, j), spatial weights {αr(i, j)} and threshold τ ;
Output: The weighted sum over vicinal groups res :=
(cid:80)R

r=0 αr(i, j) (W(i, j, r) − W(i, j, r − 1));

tion (§2.2) instead of softmax attention. We also include
several variants that improve DEIT-LA, such as PERMUTE-
FORMER (Chen, 2021), SPE (Liutkus et al., 2021) and
Rotary positional embeddings (ROPE, Su et al., 2021) that
incorporates relative positional encodings.

Initialize res ← 0, Wcur ← 0, Wprev ← 0, S ∈
RH×W ← 0;
Compute summed-area table S by calling cumsum()
function twice over horizontal and vertical directions re-
spectively;
Compute ˆr by summing over αr(i, j) until 1 −
(cid:80)ˆr

r=0 αr(i, j) < τ ;

for r = 0, 1, . . . , ˆr − 1 do

(cid:46) Cumulative sum at the bottom-right corner
S1 ← S(i + r, j + r);
(cid:46) Cumulative sum at the bottom-left corner
S2 ← S(i − r − 1, j + r);
(cid:46) Cumulative sum at the top-right corner
S3 ← S(i + r, j − r − 1);
(cid:46) Cumulative sum at the top-left corner
S4 ← S(i − r − 1, j − r − 1);

(cid:46) Compute sum over elements in the window
Wcur ← S1 − S2 − S3 + S4;
res ← res + αr(i, j)(Wcur − Wprev);
Wprev ← Wcur;

end for
(cid:46) Compute the sum over all remaining groups
res ← res + αˆr(i, j)(S(H, W ) − W(i, j, ˆr − 1));
return res

tively; (2) CIFAR-100 (Krizhevsky et al., 2009), which
contains 50K images of 100 classes for training and 10K for
evaluation. For detection tasks, we conduct our experiment
on the COCO benchmark (Lin et al., 2014) consisting of
118k training and 5k validation images respectively.

Baselines Our model for image classiﬁcation is based on
the vision transformer architecture (ViT) (Dosovitskiy et al.,
2020; Touvron et al., 2020), where the attention block is
replaced with ripple attention. We compare ripple atten-
tion (referred to as RIPPLE hereafter) with various attention
mechanisms in ViT:

• DEIT, which adopts the same architecture as ViT and

vanilla softmax attention.6

• CONVIT, which imposes a soft convolutional inductive

bias on the vanilla attention mechanism.

• DEIT-LA, a DEIT model equipped with linearized atten-

6To facilitate comparisons and simplify experimental settings,

we do not use the distillation technique.

For object detection, we evaluate our model in the general
framework of detection transformer (DETR; Carion et al.,
2020) to test the generalization ability of RIPPLE. However,
due to the slow convergence of DETR, we are unable to run
DETR model for a full training schedule given limited com-
putational resources. Instead, we adopt SMCA (Gao et al.,
2021) as our baseline, a variant of DETR that greatly speeds
up the convergence by constraining the attention map in
the decoder side. Our model, referred to as SMCA-RIPPLE,
replaces all attention blocks with ripple attention in the trans-
former encoder. For completeness, we also compare with
SMCA-LA, an SMCA variant that adopts linearized attention
in encoder attention block.

4.2. Main Implementation Details

Here we discuss key ingredients for implementing RIPPLE;
see Appendix C for more comprehensive implementation
details and discussions.

Feature Map Parameterization. Note that RIPPLE is
based on the linearized attention mechanism (§2.2).
In
this work, the feature map φ(·) is deﬁned to be deterministic
with learnable parameters, which consists of a two-layer
MLP with trigonometric and ReLU activations in turn. We
ﬁnd it works well in our experiments. Detailed discussions
about our choice and a corresponding ablation study can be
found in Appendix C.1.

Rippling Attention Speciﬁcations.
In §3.2 we deﬁne a
threshold τ to control the termination of rippling process. In
practice we ﬁnd it beneﬁcial to introduce a hard constraint
Rmax such that the model explicitly limits the maximum
distance of rippling propagation to Rmax and then merges all
the remaining groups. In this way, we could not only further
reduce the computation overhead, but also encourage the at-
tention mechanism to allocate more weights to distal groups.
This can be seen as a stronger version of halting threshold
τ , which is easier to tune due to a more intuitive effect on
the rippling process. Given Rmax, our model is robust to
the change of τ ; therefore, we set τ to 0.001 throughout
our experiments and only conduct ablation studies on Rmax
(§5.1). We ﬁnd an intermediate value gives a reasonable
trade-off between local and long-term dependencies.

Furthermore, when applied to vision transformers for image
classiﬁcation tasks, we only replace the ﬁrst several attention
layers with ripple attention, while the remaining ones adopt
linearized attention.

Ripple Attention for Visual Perception with Sub-quadratic Complexity

Table 1: Image classiﬁcation results for different vision
transformers on ImageNet1k dataset. All the variants of
DEIT-LA, including PERMUTEFORMER, are trained by us.

Table 2: Classiﬁcation results for different vision transform-
ers on CIFAR-100 dataset. All of these models are trained
by us. APE denotes absolute positional embeddings.

Model

# Params

Top-1 Acc.

Top-5 Acc.

Models with quadratic complexity

DEIT
CONVIT (d’Ascoli et al., 2021)

5.72M
5.72M

72.20
73.11

Models with sub-quadratic complexity

DEIT-LA
DEIT-LA + SINCSPE (Liutkus et al., 2021)
DEIT-LA + CONVSPE (Liutkus et al., 2021)
DEIT-LA + ROPE (Su et al., 2021)
PERMUTEFORMER (Chen, 2021)

RIPPLE

5.76M
5.84M
6.69M
5.76M
5.76M

5.78M

70.67
67.32
67.64
71.19
71.42

73.02

91.10
91.71

90.16
88.14
88.40
90.48
90.51

91.56

4.3. Main Results

Results on ImageNet-1K Dataset. The results of com-
parisons among RIPPLE and other models on ImageNet1k
dataset are presented in Table 1. We observe RIPPLE out-
performs both DEIT-LA, upon which RIPPLE is built, and its
variants by a large margin. Although DEIT-LA gives a clear
performance drop compared to the standard vision trans-
former DEIT, RIPPLE still performs better than DEIT and
achieves results comparable to the improved variant CON-
VIT while in asymptotically faster runtime, which clearly
demonstrates the effectiveness of our approach.

Results on CIFAR-100 Dataset. We further conduct ex-
periments on CIFAR-100 dataset and report results in Ta-
ble 2. RIPPLE outperforms both DEIT-LA and DEIT by
a substantial margin on CIFAR-100 dataset, and also
achieves competitive performance compared to CONVIT.
This suggests that RIPPLE also generalizes well on a rela-
tively smaller dataset. Following the setting in (Wu et al.,
2021; Yan et al., 2021), we also make a comparison of these
models in the absence of absolute positional embeddings.
We observe a signiﬁcantly larger performance gap between
vanilla vision transformers and models designed to incorpo-
rate the notion of locality (Table 2). This implies RIPPLE
could structure the scattered spatial context, which is bene-
ﬁcial to information aggregation among patches. Still, the
performance decrease in RIPPLE in the absence of positional
embeddings suggests that absolute global positions contain
complementary information to the prior knowledge of local-
ity, which is also consistent with a recent study (Islam et al.,
2020).

Results on COCO Benchmark.
In Table 3 we report
the results for object detection. Again, we see the same
trend that the performance drops by over 2 AP when using
linearized attention in the encoder (SMCA-LA). However,
SMCA-RIPPLE improves SMCA-LA on all object scales with
a marginal increase of GFLOPs and almost catches up with
SMCA. The mAP gap between SMCA-RIPPLE and SMCA
is further narrowed down from 0.5 to 0.3 with 108 training

Model

DEIT-LA
DEIT
CONVIT

RIPPLE

w/ APE

w/o APE

# Params Top-1 Acc. Top-5 Acc.

# Params Top-1 Acc. Top-5 Acc.

5.42M
5.42M
5.42M

5.47M

67.00
67.87
74.34

73.94

88.57
89.71
92.87

92.37

5.36M
5.36M
5.36M

5.42M

54.04
53.64
73.88

72.94

79.66
80.30
92.20

91.86

Table 3: Object detection results for different detection
transformers on COCO benchmark under both 50 training
epoch schedule and 108 epoch schedule.

Model

# Params GFLOPs

Inference
time(s)

50 epochs

108 epochs

AP

APS APM APL

AP

APS APM APL

SMCA
SMCA-LA
SMCA-RIPPLE

41.5M
41.7M
41.8M

88
79
80

0.059
0.062
0.065

41.0
39.1
40.5

21.9
19.8
22.1

44.3
42.8
44.1

59.1
56.5
57.7

42.7
41.1
42.3

22.8
22.0
23.2

46.1
44.5
45.6

60.0
59.0
60.0

epochs. In addition, SMCA-RIPPLE achieves better results
than SMCA on small scale objects, which is attributed to the
promoted locality of ripple attention.

5. Analysis

5.1. Inspecting the Rippling Process

On the Effect of Maximum Rippling Distances. The
maximum rippling distance Rmax deﬁned in §4.1 controls
the boundary of RIPPLE with informative spatial weights.
To evaluate its effect on the modeling performance, we
vary the maximum rippling distance Rmax and report the
results on CIFAR-100 dataset in Table 4. Overall, RIPPLE
performs well with a moderate or larger Rmax. If Rmax is
too small, the performance drops signiﬁcantly, although still
outperforming DEIT-LA. It can be attributed to the fact that if
the stick-breaking transformation terminates early, the query
would attend mostly to its immediate spatial neighbors while
not sufﬁciently respecting global dependencies.

of

spatial weights

of Stick Breaking Transforms.
On the Effect
RIPPLE with ﬁxed
variant
construct
We
a
and
(i.e.,
exponentially-decayed
1/2, (1/2)2, (1/2)3, · · · , (1/2)Rmax), which is denoted by
FIXED-RIPPLE. This variant appears to be a trivial solution
for incorporating the locality into the model, although it
does not respect potentially strong long-term dependencies
and only assign diminishing weights to distal groups.
We ﬁnd RIPPLE with hard-coded weights also performs
better than DEIT-LA, which indicates the effectiveness
of recovering spatial structures in transformers. Our
stick-breaking transformation gives a further boost over
the ﬁxed-weight baseline, thanks to its ﬂexibility in the
spatial weights. We also consider a baseline RIPPLE w/o
SBT, which replaces the stick-breaking transformation
(§3.2) with a simple softmax function to generate spatial

Ripple Attention for Visual Perception with Sub-quadratic Complexity

Figure 2: Empirical running time (left) and memory consumption (right) under different numbers of tokens. All models are
tested with a batch size of 4 on a single NVIDIA V100 GPU machine, averaged by 10 runs.

Table 4: Classiﬁcation results on CIFAR-100 dataset un-
der different setups of the rippling process. The speed is
measured by the number of images processed per second
with a batch size of 64. RIPPLE w/o SBT represents ripple
attention whose spatial weights are generated through a soft-
max function instead of stick breaking transforms (SBT). “–”
indicates not applicable.

Model

locality

global dep. Rmax

Speed

Top-1 Acc.

Top-5 Acc.

RIPPLE

FIXED-RIPPLE

LOGARITHMIC-RIPPLE

TRUNCATED-RIPPLE

RIPPLE w/o SBT

DEIT-LA

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:55)

(cid:55)

(cid:51)

(cid:51)

(cid:51)

(cid:55)

(cid:51)

(cid:51)

2
4
8
16

4

–

4

4

–

832
792
578
382

795

810

820

784

2664

72.65
73.94
73.37
73.48

71.34

73.05

72.18

71.94

67.00

91.83
92.37
92.21
92.25

90.77

91.78

91.66

91.83

88.57

weights. This variant does not promote any locality
but has the potential to learn such pattern implicitly.
It
performs slightly better than hard-coded weights and much
worse compared to RIPPLE, verifying the effectiveness of
stick-breaking transformation.

To further investigate how spatial weights generated from
our proposed stick-breaking transform (SBT;§3.2) deviate
from FIXED-RIPPLE, we plot the training dynamic of the
average Jensen-Shannon divergence (JSD) between distribu-
tions induced by SBT and FIXED-RIPPLE, which is shown in
Figure 3. The JSD scores are averaged over all training sam-
ples, for each of which we further average over all attention
blocks and heads. Intuitively, a higher JSD value reﬂects a
large discrepancy between their induced distributions. Since
the logits in SBT are usually initialized around 0, the spatial
weights are close to the exponential weights during the early
stage of training; however, as soon as the training starts, the
JSD value rises sharply, which is possibly due to balancing
between global and local information; after that, the curve
decreases slightly, indicating that the mechanism might tend
to favor local correlations; ﬁnally it plateaus at a high JSD
value, which indicates that the induced distribution does not

Figure 3: Training dynamic of average JSD between induced
distributions of the proposed stick-breaking transform (SBT)
and ﬁxed exponentially decayed weights. The solid blue
line denotes the training dynamic of JSD between SBT and
FIXED-RIPPLE, while the dashed red line denotes the JSD
between the uniform distribution and FIXED-RIPPLE.

simply degenerate to a ﬁxed distribution nor to a vanilla
linearized attention (with uniform weights).

On the Effect of Global and Local Information. To
demonstrate the relation between global and local infor-
mation, we design another baseline TRUNCATED-RIPPLE,
which puts a hard termination of rippling process such that
all distant groups beyond Rmax = 4 are discarded (i.e.,
αr(i, j) = 0, if r ≥ 4) instead of merged. This results in a
limited receptive ﬁeld without global dependency modeling.
As shown in Table 4, the comparison among TRUNCATED-
RIPPLE, RIPPLE and DEIT-LA reveals that both global and
local information play an important role in modeling, while
the notion of locality is possibly more important than global
connectivity in vision tasks, which concurs with previous
ﬁndings (Dosovitskiy et al., 2020; d’Ascoli et al., 2021).

5.2. Alternative Partitioning schemes for Vicinal

Groups

Ripple attention is a ﬂexible framework in balancing be-
tween the running time complexity and predictive accuracy.
To explore this, we compare the full ripple attention against

1216232248264280296211221282Number of Tokens0500100015002000Running Time (ms)Ripple (DP)Ripple (Naive)DEIT-LADEIT1216232248264280296211221282Number of Tokens0.02.55.07.510.012.515.017.5Memory (GB)Ripple (DP)Ripple (Naive)DEIT-LADEIT050100150200250300Epochs0.040.060.080.100.120.140.160.180.20Average JSDRipple Attention for Visual Perception with Sub-quadratic Complexity

a rippling process where ripples get exponentially thicker so
that the process could reach the image boundary in logarith-
mic time. Formally, recall that vicinal groups with respect to
a query token at position (i, j) is deﬁned as {Nr(i, j)}R
r=0,
where every token at position (m, n) belongs to Nr(i, j)
if and only if max(|m − i|, |n − j|) = r. We general-
ize this notion by relaxing the equality condition, that is,
(m, n) ∈ Nr(i, j) if 2r ≤ max(|m − i|, |n − j|) < 2r+1,
which allows the size of vicinal groups to be exponentially
larger instead of keeping constant. In this way, the number
of vicinal groups R(cid:48) is O(log R). This method, which we
refer to as LOGARITHMIC-RIPPLE, enjoys O(HW log R)
time complexity and becomes more efﬁcient than base
ripple attention. As reported in Table 4, we see RIPPLE-
LOGARITHMIC also outperforms DEIT-LA by a large margin,
although leading to a clear performance drop compared to
the full ripple attention. This may be due to that RIPPLE-
LOGARITHMIC processes visual tokens at a coarser-grained
level. Nevertheless, RIPPLE-LOGARITHMIC justiﬁes the
ﬂexibility of our framework that one could trade off the task
accuracy for higher efﬁciency and vice versa. More details
and run-time comparison can be found in §D.2.

5.3. Empirical Running Time and Memory

Consumption

To verify the advantage of asymptotically faster running
complexity in RIPPLE, we conduct a simulation experi-
ment on vision transformers to compare the empirical run-
ning time and memory consumption of RIPPLE against its
baselines under different numbers of tokens. The detailed
setup can be found in Appendix E. Figures 2 demonstrate
the comparison results. As mentioned in §3.3, both DEIT
and RIPPLE (Naïve) come with quadratic complexity in the
number of tokens. We observe that RIPPLE with dynamic
programming (DP) performs signiﬁcantly better than RIP-
PLE (Naïve), which demonstrates the effectiveness of our
dynamic programming algorithm. Furthermore, RIPPLE
behaves similarly to DEIT-LA as the number of tokens in-
creases, verifying that it could be executed in linear observed
time. When processing a large number of tokens, RIPPLE
often achieves a 5× or even 10× reduction in running time
and memory compared to its quadratic counterparts.

level image processing (Chen et al., 2020) and image clas-
siﬁcation (Dosovitskiy et al., 2020; Touvron et al., 2020;
Liu et al., 2021a). A large body of research has been de-
voted into improving efﬁciency and effectiveness of vision
transformers (Dosovitskiy et al., 2020). Recent advances im-
prove original vision transformers from various perspectives,
such as data-efﬁcient training (Touvron et al., 2020), adopt-
ing pyramid architectures (Wang et al., 2021a; Liu et al.,
2021a; Heo et al., 2021) and incorporating the notion of
locality, which can be done by applying convolutional mod-
ules into the architecture (Li et al., 2021; Wu et al., 2021;
Yan et al., 2021; Xu et al., 2021; Yuan et al., 2021; d’Ascoli
et al., 2021), restricting the scope of the self-attention (Liu
et al., 2021a; Dong et al., 2021; Chen et al., 2021) or initial-
izing self-attention maps as a convolution kernel (d’Ascoli
et al., 2021). In contrast to these prior works, we directly
model the locality inside the attention mechanism yet permit
long-term dependencies, without relying on any convolu-
tional operations or limiting the receptive ﬁeld; at the same
time, ripple attention runs in linear observed time so that
the quadratic bottleneck in standard vision transformers can
be greatly alleviated. Our work is orthogonal to previous
works that modify the transformer architecture and it is
worth exploring their combination to improve the overall
vision transformer model design.

Our model is built on the linearized attention mechanism,
which approximates the softmax kernel with the dot product
of feature maps. The feature maps can be stochastic, such
as in RFA (Peng et al., 2021) and Performer (Choromanski
et al., 2020), or deterministic (Katharopoulos et al., 2020;
Schlag et al., 2021). Recently, many works are proposed
to improve linearized attention by incorporating relative po-
sitional encodings (Liutkus et al., 2021; Luo et al., 2021;
Chen, 2021; Su et al., 2021). Other efﬁcient attention mech-
anisms include methods that limit the attention pattern to
be sparse (Child et al., 2019; Ho et al., 2019; Kitaev et al.,
2020) or utilizes a low rank approximation by projecting in-
put sequences to fewer key-value pairs (Wang et al., 2020a).
A comprehensive review of recent advances in efﬁcient at-
tention mechanisms can be found in (Tay et al., 2020a;b).

7. Conclusion

6. Related Work

Transformer architectures (Vaswani et al., 2017) are ﬁrst
introduced for neural machine translation. Recently, re-
searchers begin to apply the transformer model in the com-
puter vision domain, showing promising results in various
tasks, such as image generation (Parmar et al., 2018), video
action recognition (Bertasius et al., 2021; Liu et al., 2021b),
segmentation (Wang et al., 2020b; Strudel et al., 2021),
object detection (Carion et al., 2020; Gao et al., 2021), low-

In this work, we present ripple attention, a novel atten-
tion mechanism for visual perception with sub-quadratic
complexity. In ripple attention, contributions of different
tokens to a query are weighted with respect to their spatial
distances in the 2D space. We design a dynamic program-
ming algorithm that computes weighted contributions for
all queries in linear observed time and derive the spatial
weights through an adaptive stick-breaking transformation.
We conduct extensive experiments and analyses to demon-
strate the effectiveness of ripple attention.

Ripple Attention for Visual Perception with Sub-quadratic Complexity

Acknowledgements

We thank the anonymous reviewers for their valuable sugges-
tions that greatly helped improve this work. This research
was supported in part by the joint research scheme of the
National Natural Science Foundation of China (NSFC) and
the Research Grants Council (RGC) under grant number
N_HKU714/21.

References

Baevski, A. and Auli, M. Adaptive input representations for
neural language modeling. In International Conference
on Learning Representations, 2019. URL https://
openreview.net/forum?id=ByxZX20qFQ.

Bahdanau, D., Cho, K., and Bengio, Y. Neural machine
translation by jointly learning to align and translate. arXiv
preprint arXiv:1409.0473, 2014.

Berman, M., Jégou, H., Vedaldi, A., Kokkinos, I., and
Douze, M. Multigrain: a uniﬁed image embedding for
classes and instances. arXiv preprint arXiv:1902.05509,
2019.

Bertasius, G., Wang, H., and Torresani, L. Is space-time
attention all you need for video understanding? In Pro-
ceedings of the International Conference on Machine
Learning (ICML), July 2021.

Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,
A., and Zagoruyko, S. End-to-end object detection with
transformers. In European Conference on Computer Vi-
sion, pp. 213–229. Springer, 2020.

Chen, C.-F., Panda, R., and Fan, Q. Regionvit: Regional-
to-local attention for vision transformers. arXiv preprint
arXiv:2106.02689, 2021.

Chen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z.,
Ma, S., Xu, C., Xu, C., and Gao, W. Pre-trained image
processing transformer. arXiv preprint arXiv:2012.00364,
2020.

Chen, P. Permuteformer: Efﬁcient relative position encoding
for long sequences. arXiv preprint arXiv:2109.02377,
2021.

Child, R., Gray, S., Radford, A., and Sutskever, I. Gen-
erating long sequences with sparse transformers. arXiv
preprint arXiv:1904.10509, 2019.

Choromanski, K., Likhosherstov, V., Dohan, D., Song, X.,
Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin,
A., Kaiser, L., et al. Rethinking attention with performers.
arXiv preprint arXiv:2009.14794, 2020.

Crow, F. C. Summed-area tables for texture mapping. In
Proceedings of the 11th annual conference on Computer
graphics and interactive techniques, pp. 207–212, 1984.

Cubuk, E. D., Zoph, B., Shlens, J., and Le, Q. V. Ran-
daugment: Practical automated data augmentation with a
reduced search space. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
Workshops, pp. 702–703, 2020.

Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q., and
Salakhutdinov, R. Transformer-XL: Attentive language
models beyond a ﬁxed-length context. In Proceedings
of the 57th Annual Meeting of the Association for Com-
putational Linguistics, pp. 2978–2988, Florence, Italy,
2019. Association for Computational Linguistics. URL
https://www.aclweb.org/anthology/P19-1285.

d’Ascoli, S., Touvron, H., Leavitt, M., Morcos, A., Biroli,
G., and Sagun, L. Convit: Improving vision transformers
with soft convolutional inductive biases. arXiv preprint
arXiv:2103.10697, 2021.

Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and
Kaiser, L. Universal transformers. In International Con-
ference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=HyzdRiR9Y7.

Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,
L. Imagenet: A large-scale hierarchical image database.
In 2009 IEEE conference on computer vision and pattern
recognition, pp. 248–255. Ieee, 2009.

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:
Pre-training of deep bidirectional transformers for lan-
guage understanding. arXiv preprint arXiv:1810.04805,
2018.

Dong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L.,
Chen, D., and Guo, B. Cswin transformer: A general
vision transformer backbone with cross-shaped windows.
arXiv preprint arXiv:2107.00652, 2021.

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,
Heigold, G., Gelly, S., et al. An image is worth 16x16
words: Transformers for image recognition at scale. arXiv
preprint arXiv:2010.11929, 2020.

Fukushima, K. and Miyake, S. Neocognitron: A self-
organizing neural network model for a mechanism of
visual pattern recognition. In Competition and coopera-
tion in neural nets, pp. 267–285. Springer, 1982.

Gao, P., Zheng, M., Wang, X., Dai, J., and Li, H. Fast
convergence of detr with spatially modulated co-attention.
arXiv preprint arXiv:2101.07448, 2021.

Ripple Attention for Visual Perception with Sub-quadratic Complexity

Glorot, X. and Bengio, Y. Understanding the difﬁculty
of training deep feedforward neural networks. In Pro-
ceedings of the thirteenth international conference on
artiﬁcial intelligence and statistics, pp. 249–256. JMLR
Workshop and Conference Proceedings, 2010.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition,
pp. 770–778, 2016.

Heo, B., Yun, S., Han, D., Chun, S., Choe, J., and Oh,
S. J. Rethinking spatial dimensions of vision transformers.
arXiv preprint arXiv:2103.16302, 2021.

Ho, J., Kalchbrenner, N., Weissenborn, D., and Salimans, T.
Axial attention in multidimensional transformers. arXiv
preprint arXiv:1912.12180, 2019.

Hoffer, E., Ben-Nun, T., Hubara, I., Giladi, N., Hoeﬂer,
T., and Soudry, D. Augment your batch: Improving
generalization through instance repetition. In Proceedings
of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 8129–8138, 2020.

Huang, G., Sun, Y., Liu, Z., Sedra, D., and Weinberger,
K. Q. Deep networks with stochastic depth. In European
conference on computer vision, pp. 646–661. Springer,
2016.

Islam, M. A., Jia, S., and Bruce, N. D. B. How much
position information do convolutional neural networks
encode? In International Conference on Learning Rep-
resentations, 2020. URL https://openreview.net/
forum?id=rJeB36NKvB.

Kasai, J., Peng, H., Zhang, Y., Yogatama, D., Ilharco, G.,
Pappas, N., Mao, Y., Chen, W., and Smith, N. A. Fine-
tuning pretrained transformers into rnns. arXiv preprint
arXiv:2103.13076, 2021.

Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
Transformers are rnns: Fast autoregressive transformers
In International Conference on
with linear attention.
Machine Learning, pp. 5156–5165. PMLR, 2020.

Kitaev, N., Kaiser, L., and Levskaya, A. Reformer:
The efﬁcient transformer. In International Conference
on Learning Representations, 2020. URL https://
openreview.net/forum?id=rkgNKkHtvB.

Krizhevsky, A., Hinton, G., et al. Learning multiple layers

of features from tiny images. 2009.

Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet
classiﬁcation with deep convolutional neural networks.
Advances in neural information processing systems, 25:
1097–1105, 2012.

LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard,
R. E., Hubbard, W., and Jackel, L. D. Backpropaga-
tion applied to handwritten zip code recognition. Neural
computation, 1(4):541–551, 1989.

Li, Y., Zhang, K., Cao, J., Timofte, R., and Van Gool, L.
Localvit: Bringing locality to vision transformers. arXiv
preprint arXiv:2104.05707, 2021.

Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-
manan, D., Dollár, P., and Zitnick, C. L. Microsoft coco:
Common objects in context. In European conference on
computer vision, pp. 740–755. Springer, 2014.

Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z.,
Lin, S., and Guo, B. Swin transformer: Hierarchical
vision transformer using shifted windows. arXiv preprint
arXiv:2103.14030, 2021a.

Liu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S.,
and Hu, H. Video swin transformer. arXiv preprint
arXiv:2106.13230, 2021b.

Liutkus, A., Cífka, O., Wu, S.-L., Simsekli, U., Yang, Y.-H.,
and Richard, G. Relative positional encoding for trans-
formers with linear complexity. In International Con-
ference on Machine Learning, pp. 7067–7079. PMLR,
2021.

Loshchilov, I. and Hutter, F.

dient descent with warm restarts.
arXiv:1608.03983, 2016.

Sgdr: Stochastic gra-
arXiv preprint

Loshchilov, I. and Hutter, F. Decoupled weight decay reg-
In International Conference on Learning

ularization.
Representations, 2019.

Luo, S., Li, S., Cai, T., He, D., Peng, D., Zheng, S., Ke,
G., Wang, L., and Liu, T.-Y. Stable, fast and accurate:
Kernelized attention with relative positional encoding.
arXiv preprint arXiv:2106.12566, 2021.

Meng, D., Chen, X., Fan, Z., Zeng, G., Li, H., Yuan, Y.,
Sun, L., and Wang, J. Conditional detr for fast training
convergence. arXiv preprint arXiv:2108.06152, 2021.

Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer,
N., Ku, A., and Tran, D. Image transformer. In Interna-
tional Conference on Machine Learning, pp. 4055–4064.
PMLR, 2018.

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., et al. Pytorch: An imperative style, high-performance
deep learning library. Advances in Neural Information
Processing Systems, 32:8026–8037, 2019.

Ripple Attention for Visual Perception with Sub-quadratic Complexity

Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith,
N. A., and Kong, L. Random feature attention. arXiv
preprint arXiv:2103.02143, 2021.

Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H.,
and Xia, H. End-to-end video instance segmentation with
transformers. arXiv preprint arXiv:2011.14503, 2020b.

Wasserman, L. All of nonparametric statistics. Springer

Science & Business Media, 2006.

Wightman, R. Pytorch image models. https://github.
com/rwightman/pytorch-image-models, 2019.

Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L.,
and Zhang, L. Cvt: Introducing convolutions to vision
transformers. arXiv preprint arXiv:2103.15808, 2021.

Xiao, T., Singh, M., Mintun, E., Darrell, T., Dollár, P., and
Girshick, R. Early convolutions help transformers see
better. arXiv preprint arXiv:2106.14881, 2021.

Xu, Y., Zhang, Q., Zhang, J., and Tao, D. Vitae: Vision
transformer advanced by exploring intrinsic inductive
bias. arXiv preprint arXiv:2106.03348, 2021.

Yan, H., Li, Z., Li, W., Wang, C., Wu, M., and Zhang, C.
Contnet: Why not use convolution and transformer at the
same time? arXiv preprint arXiv:2104.13497, 2021.

Yang, J., Li, C., Zhang, P., Dai, X., Xiao, B., Yuan, L.,
and Gao, J. Focal attention for long-range interactions
in vision transformers. Advances in Neural Information
Processing Systems, 34, 2021.

Yuan, K., Guo, S., Liu, Z., Zhou, A., Yu, F., and Wu, W. In-
corporating convolution designs into visual transformers.
arXiv preprint arXiv:2103.11816, 2021.

Yun, S., Han, D., Oh, S. J., Chun, S., Choe, J., and Yoo,
Y. Cutmix: Regularization strategy to train strong clas-
siﬁers with localizable features. In Proceedings of the
IEEE/CVF International Conference on Computer Vision,
pp. 6023–6032, 2019.

Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz,
D. mixup: Beyond empirical risk minimization. arXiv
preprint arXiv:1710.09412, 2017.

Zhong, Z., Zheng, L., Kang, G., Li, S., and Yang, Y. Ran-
dom erasing data augmentation. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence, volume 34,
pp. 13001–13008, 2020.

Schlag, I., Irie, K., and Schmidhuber, J. Linear transform-
In Interna-
ers are secretly fast weight programmers.
tional Conference on Machine Learning, pp. 9355–9366.
PMLR, 2021.

Shaw, P., Uszkoreit, J., and Vaswani, A. Self-attention with
In NAACL-HLT (2),

relative position representations.
2018.

Simoncelli, E. P. and Olshausen, B. A. Natural image statis-
tics and neural representation. Annual review of neuro-
science, 24(1):1193–1216, 2001.

Strudel, R., Garcia, R., Laptev, I., and Schmid, C. Seg-
menter: Transformer for semantic segmentation. arXiv
preprint arXiv:2105.05633, 2021.

Su, J., Lu, Y., Pan, S., Wen, B., and Liu, Y. Roformer:
Enhanced transformer with rotary position embedding.
arXiv preprint arXiv:2104.09864, 2021.

Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham,
P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long
range arena: A benchmark for efﬁcient transformers.
arXiv preprint arXiv:2011.04006, 2020a.

Tay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efﬁcient
transformers: A survey. arXiv preprint arXiv:2009.06732,
2020b.

Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles,
A., and Jégou, H. Training data-efﬁcient image trans-
formers & distillation through attention. arXiv preprint
arXiv:2012.12877, 2020.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention
is all you need. arXiv preprint arXiv:1706.03762, 2017.

Viola, P., Jones, M., et al. Robust real-time object detec-
tion. International journal of computer vision, 4(34-47):
4, 2001.

Wang, S., Li, B., Khabsa, M., Fang, H., and Ma, H. Lin-
former: Self-attention with linear complexity. arXiv
preprint arXiv:2006.04768, 2020a.

Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D.,
Lu, T., Luo, P., and Shao, L. Pyramid vision transformer:
A versatile backbone for dense prediction without convo-
lutions. arXiv preprint arXiv:2102.12122, 2021a.

Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D.,
Lu, T., Luo, P., and Shao, L. Pvtv2: Improved base-
lines with pyramid vision transformer. arXiv preprint
arXiv:2106.13797, 2021b.

Ripple Attention for Visual Perception with Sub-quadratic Complexity

Appendices

A. Analysis of Runtime Complexity

In this section, we give more details about the runtime complexity (§3.3) for different attention mechanisms in Table 5. In
particular, we focus on variants (1) Ripple-softmax and (2) Ripple (naïve), since the rest have been clariﬁed in the main text.

Complexity of Ripple-softmax. Ripple-softmax aims to inject radial bias through rippling into the vanilla softmax
attention. Here we present two possible ways to achieve this goal, either explicitly or implicitly. Given a query, we could
perform vanilla attention over one of its vicinal groups once so that the rippling effect could be explicitly encoded. Since
the number of tokens within vicinal group of distance r is directly proportional to r (note that the number of tokens within
vicinal group of distance r is simply the difference between (2r + 1)2 − (2r − 1)2 = 8r) and there are R groups in total,
the overall complexity would be O(R2); then considering all queries results in O(HW R2) complexity (along with a large
constant). On the other hand, we could also add speciﬁc spatial weights directly to the attention matrix, implicitly enforcing
the radial bias. This is similar to relative positional encodings (Shaw et al., 2018), but in this case the overall complexity is
at least O(H 2W 2) due to the computation of attention matrices, which is as inefﬁcient as vanilla softmax attention in terms
of complexity. In this work, we simply refer to the explicit method as Ripple-softmax.

Complexity of Ripple (naïve). Naïvely implementing ripple attention comes with O(HW R2) complexity, as mentioned
in the beginning of § 3.3. This is due to the fact that for each query, we have to ﬁrst sum over all tokens in one vicinal
group, which is O(r) (see the analysis in the paragraph above), and then aggregate over all groups. This results in O(R2)
complexity, and O(HW R2) for all queries (also with a large constant; see the empirical running comparison in §5.3).

B. Efﬁcient Gradient Computation in Ripple Attention

To perform gradient back-propagation for ripple attention, a naive implementation would be directly adopting the automatic
differentiation, since all operations in our forward pass (Algorithm 1) are differentiable; however, since many computations
in Algorithm 1 overlap with each other, it would lead to substantially repetitive calculations. In addition, we ﬁnd it even
takes O(T 2) time and memory, which is highly inefﬁcient compared to the forward pass.

In this section, we present an algorithm based on dynamic programming to perform efﬁcient back-propagation for ripple
attention, which again comes with sub-quadratic complexity. Recall that given a single query qij, all the keys K and values
V , ripple attention executes the following computation during the forward pass7:

RippleAttn (qij, K, V ) :=

q(cid:62)
ij
q(cid:62)
ij

(cid:80)R

r=0 αr(i, j) (cid:80)
(cid:80)R
r=0 αr(i, j) (cid:80)

(m,n)∈Nr(i,j) kmnv(cid:62)
mn
(m(cid:48),n(cid:48))∈Nr(i,j) km(cid:48)n(cid:48)

.

(10)

The main difference between linearized attention and ripple attention lies in the computation procedure of summation over
kv(cid:62) and k. Therefore, we put main focus on calculating gradients for the following quantity

yij :=

R
(cid:88)

r=0

(cid:88)

αr(i, j)

(m,n)∈Nr(i,j)

xmn,

(11)

where xmn ∈ RD denotes a D-dimensional vector located at position (m, n), which could be (unrolled) kmnv(cid:62)
mn or kmn.8
The remaining computation in ripple attention (e.g., dot product with qij) can be easily handled by standard automatic
differentiation. We are mainly interested in computing gradients with respect to αr(i, j) and xmn

In ripple attention, we maintain a summed area table (SAT) to efﬁciently retrieve the partial reduction over vicinal groups
(see Algorithm 1 for more details). Although all operations in Algorithm 1 is differentiable and thus admits the use of
automatic differentiation to calculate gradients, it is very inefﬁcient since computations of most intermediate nodes in the

7Without loss of generality, we merge the feature map φ(·) into the vector representation of queries and keys to simplify notations.
8We focus on the general form here since the derived algorithm applies to both the nominator and the denominator.

Ripple Attention for Visual Perception with Sub-quadratic Complexity

computation graph (for example, S1, S2, S3 and S4 in Algorithm 1) overlap with each other, resulting in a large amount of
repeated computation.

Here we inspect the form (equation 11) and show that the properties of vicinal groups could be made use of to derive
efﬁcient gradient computation.

L, that is, the gradient of our loss objective w.r.t.
Gradients with respect to spatial weights. We assume gradients ∇yij
output at all positions (i, j) are available during back-propagation. According to the chain rule, the partial derivative of the
objective L w.r.t. αr(i, j) has the following form:

∂L
∂αr(i, j)

=

D
(cid:88)

d=1

∂L
∂yijd

∂yijd
∂αr(i, j)

=

D
(cid:88)

d=1

∂L
∂yijd

(cid:88)

(m,n)∈Nr(i,j)

xmnd,

where xmnd and ymnd denote the d-th dimension of xmn and the output ymn respectively. The ﬁrst quality holds since
spatial weights at every position only depends on the output at that position; but since the same spatial weight applies to
all dimensions of yij, we have to reduce over the embedding dimension to compute the partial derivative. Similar to the
forward pass computation (equation 9), we recognize that the inner summation over the vicinal group Nr(i, j) can be again
computed efﬁciently by utilizing SATs with Algorithm 1.

Gradients with respect to xmn. The partial derivative w.r.t. element xmnd can be written as

∂L
∂xmnd

=

=

H
(cid:88)

W
(cid:88)

i=1

j=1

H
(cid:88)

W
(cid:88)

i=1

j=1

∂L
∂yijd

∂yijd
∂xmnd

∂L
∂yijd

R
(cid:88)

r=0

αr(i, j)I [(m, n) ∈ Nr(i, j)] .

(12)

where we deﬁne I [(m, n) ∈ Nr(i, j)] as the indicator function such that it is set to 1 if (m, n) ∈ Nr(i, j) and 0 otherwise.
A naive way to compute the partial derivatives above has O(H 2W 2R) complexity, since for every key vector at position
(m, n) we need to sum its inﬂuences over all positions. However, we show that we could again solve them via dynamic
programming.
Our key observation is that the vicinal group is symmetrical w.r.t. its arguments, that is, (m, n) ∈ Nr(i, j) if and only if
(i, j) ∈ Nr(m, n). Then the partial derivative (equation 12) is equivalent to

∂L
∂xmnd

=

=

=

=

H
(cid:88)

W
(cid:88)

i=1

j=1

H
(cid:88)

W
(cid:88)

i=1

j=1

∂L
∂yijd

∂L
∂yijd

R
(cid:88)

r=0

R
(cid:88)

r=0

αr(i, j)I [(m, n) ∈ Nr(i, j)]

αr(i, j)I [(i, j) ∈ Nr(m, n)]

R
(cid:88)

H
(cid:88)

W
(cid:88)

r=0

i=1

j=1

I [(i, j) ∈ Nr(m, n)]

∂L
∂yijd

αr(i, j)

R
(cid:88)

(cid:88)

r=0

(i,j)∈Nr(m,n)

∂L
∂yijd

αr(i, j).

Thanks to the symmetry, the computation of partial derivatives is converted into reduction over vicinal groups, it can be
effectively solved by dynamic programming (§3.3) in again O(HW R) time, which involves instantiating an SAT for the
quantity ∂L
mn or kmn into xmn yields the
∂ (cid:101)yijcd
term in the nominator and denominator, respectively.

αr(i, j) over all positions (i, j). Equipped with this result, substituting kmnv(cid:62)

C. Additional Implementation Details

We implement our model using PyTorch (Paszke et al., 2019) and PyTorch image models (timm) toolkit (Wightman, 2019).
We also implement a CUDA kernel for the ripple attention mechanism.

Ripple Attention for Visual Perception with Sub-quadratic Complexity

Table 5: Runtime complexity comparisons between different attention variants, with respect to an image with H × W
patches (in the ﬁrst row) and with respect to the number of patch tokens T := H × W (in the second row). †Ripple-softmax
indicates the complexity if we would like to implement ripple-like mechanisms in vanilla softmax attention.

Softmax attention Ripple-softmax†

O (cid:0)H 2W 2(cid:1)
O (cid:0)T 2(cid:1)

O (cid:0)HW R2(cid:1)
O (cid:0)T 2(cid:1)

Linearized attention Ripple (naïve) Ripple (DP)
O (HW R)
O (cid:0)T 3/2(cid:1)

O (cid:0)HW R2(cid:1)
O (cid:0)T 2(cid:1)

O (HW )
O (T )

C.1. Deterministic Adaptive Feature Maps for Linearized Attention

Background. Generally, a random feature map φω(·) is deﬁned by a function h(·) : RD → R, m uni-variate functions
f1, f2, . . . , fm : R → R as well as d identically distributed random vectors ω1, ω2, . . . , ωd following some distribution
(Choromanski et al., 2020):

φω(x) :=

h(x)
√
d

(cid:2)f1(ω(cid:62)

1 x), . . . , f1(ω(cid:62)

d x), . . . , fm(ω(cid:62)

1 x), . . . , fm(ω(cid:62)

d x)(cid:3)

yielding a map from RD to RD(cid:48)
construct various unbiased estimators for the quantity exp(x(cid:62)y), that is,

, where D(cid:48) = md. Then by setting different conﬁgurations of f ’s, ω’s and h, we could

exp(x(cid:62)y) = Eω1,...,ωd

(cid:2)φω(x)(cid:62)φω(y)(cid:3)

For instance, we could let m = 2, where f1 = sin, f2 = cos are trigonometric functions and h(x) = exp (cid:0)||x||2/2(cid:1) (Peng
et al., 2021; Choromanski et al., 2020). Although unbiased, researchers note that the use of trigonometric functions does not
ensure non-negative scores, which may lead to large estimate variance and unstable training (Choromanski et al., 2020).
Alternatively, we could construct an estimator by setting m = 1 with f1 = exp and h(x) = exp (cid:0)−||x||2/2(cid:1), which is
again unbiased but enjoys positiveness (FAVOR+, Choromanski et al., 2020).

Our proposed deterministic adaptive feature map. Recently, researchers also proposed various heuristic designs of
feature maps (Choromanski et al., 2020; Schlag et al., 2021; Kasai et al., 2021) that do not guarantee unbiasedness but
might either exhibit lower variance, simplify computation or bring other useful beneﬁts. Unfortunately, through extensive
preliminary experiments we found most of these linearized attention variants (either random or deterministic) did not work
well in the setting of vision transformers. We hypothesize there are two reasons for the performance drop: the ﬁrst one is the
usage of random samples, which suffers from the slow Monte Carlo convergence rate and instability during training; the
second one is due to ﬁxed weights, preventing the map from being adaptive and learning useful patterns. To this end, we
propose the following deterministic feature map:

φ(x) = ReLU(W2[sin(W1x); cos(W1x)] + b2).

(13)

Intuitively, we still follow the trigonometric feature map, except that we set W1 to be initialized as independent standard
Gaussian samples but then learnable during training; the generated feature is then passed through a fully connected layer
followed by a ReLU activation. It is deterministic and involves learnable parameters, which we found greatly improves
performance.

Comparison with other feature maps and ablation study. We conduct a simple ablation study to demonstrate the
effectiveness of our proposed feature map and report comparisons with other feature maps9, as shown in Table 6. In general,
we ﬁnd it works pretty well in practice and outperforms other feature maps that are either deterministic or random. For
our ablation study, we consider two variants of our proposed approach: (1) the method that recovers the original random
trigonometric feature map, that is, recasting W1 as random samples and re-drawing it at every iteration; (2) the method that
removes the fully connected layer (characterized by parameters W2 and b2). From Table 6, we see a great performance drop
if we use random weights, which indicates that random feature maps lead to more difﬁcult training in vision transformers.
In addition, a feed-forward layer will give a further performance boost due to the increased ﬂexibility. Therefore, we adopt
our proposed deterministic feature map throughout our work.

9For methods that adopt random features, we sample a set of random weights at every training step and use the same set of weights
during evaluation. We also attempted various ways to schedule the redrawing random weights during training, but did not observe any
performance gain.

Ripple Attention for Visual Perception with Sub-quadratic Complexity

Table 6: Classiﬁcation results on ImageNet1k dataset under different choices of feature maps. † indicates that our feature
map design without fully connected network is identical to T2R (Kasai et al., 2021). * denotes the model does not fully
converge.

Feature map

RFA (Peng et al., 2021)
Performer (Choromanski et al., 2020)
DPFP (Schlag et al., 2021)
T2R (Kasai et al., 2021)

Deterministic
(cid:55)
(cid:55)
(cid:51)
(cid:51)

Ours

Ours w/ randomly sampled W1
Ours w/o fully connected network

(cid:51)

(cid:55)
(cid:51)

Top-1 Acc.

67.10
65.92
63.95*
70.02

70.67

66.82
70.02†

C.2. Explicitly Controlling the Maximum Rippling Distance

In §3.2 we deﬁne the threshold τ to control the termination of rippling process. In practice we ﬁnd it beneﬁcial to introduce
a hard constraint Rmax such that the model limits the maximum distance of rippling propagation to Rmax and then merges
all the remaining groups. In this way, we could not only further reduce the computation overhead, but also encourage the
attention mechanism to allocate more weights to distal groups. This can be seen as a stronger version of halting threshold τ ,
which is easier to tune due to a more intuitive effect on the rippling process. We ﬁnd an intermediate value gives a reasonable
trade-off between local and long-term dependencies. Given Rmax, our model is robust to the change of τ ; therefore, we only
set τ to 0.001 by default and mainly conduct ablation studies on Rmax.

C.3. Parameterization of Spatial Weights

In terms of parameterizing spatial weights, we allocate an embedding vector for every of Rmax stick units, so that
they could adapt themselves to learn useful patterns from data. To compute spatial weights, we ﬁrst linearly project
each value vector vij and then perform dot-product with each of Rmax stick unit embeddings10 to produce Rmax logits
{or(i, j)}Rmax
r=1 . Every logit is then passed through a modiﬁed sigmoid function to yield the length of each stick unit
sr(i, j) = 1/ [1 + (Rmax − r) exp (−or(i, j))]. This modiﬁcation, which is inspired by the default stick-breaking transform
implementation in PyTorch distribution package (Paszke et al., 2019), ensures the model does not put most of mass on
the ﬁrst several sticks. We ﬁnd this trick slightly improves performance. Consequently, spatial weights {αr(i, j)}Rmax
r=0 are
derived by applying stick-breaking transformations to sr(i, j)’s according to equation 5.

C.4. Architecture Details

For image classiﬁcation, all model architectures follow the tiny variant of DEIT (Touvron et al., 2020), consists of 12
transformer layers, with the embedding dimension set to 192, except that we set the number of heads per attention block to 6
for all models. For object detection, our model is based on the architecture of SMCA with single scale features (Gao et al.,
2021), which could facilitate comparisons and demonstrate the effectiveness of ripple attention more clearly. In particular,
the number of transformer layers is 6 for both the encoder and decoder, with the number attention heads and the embedding
dimension set to 8 and 256, respectively; the backbone is the pre-trained ResNet-50 (He et al., 2016) on ImageNet1k with
ﬁxed batch-norm layers.

C.5. Speciﬁcs for Applying Ripple Attention in Vision Transformers

Average pooling instead of using class tokens for classiﬁcation. Since ripple attention directly operates on 2D images,
it is hard to directly employ the widely used class token for classiﬁcation tasks (Dosovitskiy et al., 2020; Touvron et al.,
2020). Instead, we adopt mean average pooling over all tokens instead of class tokens to extract feature vectors that are fed
into the classiﬁcation head.

10Since stick-breaking transformations ensure the produced weights to be inside a simplex, Rmax logits would sufﬁce to produce a

sequence of spatial weights with size Rmax + 1.

Ripple Attention for Visual Perception with Sub-quadratic Complexity

Multi-head ripple attention. Similar to multi-head attention (Vaswani et al., 2017), which is used in most Vision
transformer architectures, we also adopt a multi-head variant of ripple attention, where different heads maintain different sets
of spatial weights. The multi-head ripple attention allows different heads to focus on locality to various degrees, increasing
the overall expressiveness.

On the number of ripple layers. A straightforward implementation choice is to replace regular attention at all layers
of ViT with ripple attention. However, we ﬁnd empirically only replacing several initial transformer layers works equally
well. Since the input tokens of transformers consist of local patches, promoting local correlations at lower layers and
maintaining structural spatial contexts could facilitate information aggregation; but as tokens go higher, every token is
contextualized by global information and in this case adding the notion of locality might mislead the modeling. Therefore,
we propose to use a hybrid architecture, where the lower layers use ripple attention while upper ones still adopt linear
attention mechanisms. This choice is further supported by our ablation study experiments Appendix D.1, where our model
achieves the best performance over various settings if only the ﬁrst 9 transformer layers use ripple attention. Therefore,
throughout experiments we use this conﬁguration unless otherwise stated.

C.6. Training Setup

In this section, we describe our full training setup for both image classiﬁcation and object detection.

Training details for image classiﬁcation We following the same procedure to train the models as in DEIT (Touvron et al.,
2020), including the data-augmentation, the regularization and the hyper-parameter setting for a head-to-head comparison.
We use AdamW optimizer (Loshchilov & Hutter, 2019) to train our model on 8 NVIDIA V100 GPUs for 300 epochs on
both CIFAR-100 and ImageNet1k datasets. We adopt commonly used data augmentation methods, including random
clipping, cropping, Rand-Augment (Cubuk et al., 2020) and random erasing (Zhong et al., 2020). However, we remove
repeated augmentation (Hoffer et al., 2020) as we ﬁnd it slows down convergence for both linearized attention and ripple
attention, as also observed in previous studies (Berman et al., 2019; Xiao et al., 2021). For regularization, we employ
stochastic depth (Huang et al., 2016), Mixup (Zhang et al., 2017), Cutmix (Yun et al., 2019), all of which are set to default
settings in DeiT (Touvron et al., 2020). Training protocols that are speciﬁc to different datasets are listed as follows:

• For ImageNet1k dataset we set the batch size to 1024 and the learning rate to 0.001 with cosine learning rate decay

(Loshchilov & Hutter, 2016). The image size is set to 224 × 224 with patch size 16, resulting in 14 × 14 tokens.

• for CIFAR-100 dataset, the batch size and the learning rate is set to 512 and 0.0005 respectively, with the same cosine
learning rate decay. In terms of the image size, we use the original scale 32 × 32, where a patch size 2 is used to produce
16 × 16 non-overlapping patches.

During evaluation, we report top-1 and top-5 accuracy on the evaluation set of both ImageNet1k and CIFAR-100
datasets.

Training details for object detection We follow the same training protocol as SMCA (Gao et al., 2021). In particular,
we initialize the transformer parameters with Xavier initialization (Glorot & Bengio, 2010), and use the pretrained weights
on ImageNet1k for the backbone. We adopt the AdamW optimizer (Loshchilov & Hutter, 2019), set the weight decay to
10−4 and the learning rate to 10−5 and 10−4 for the backbone and transformer, respectively. We also decrease the learning
rate to 1/10 of its original value after 40 epochs for 50 epoch schedule and after 80 epochs for 108 epoch training schedule.
The dropout rate is set to 0.1. The data augmentation scheme and the loss objective is also the same as SMCA (Gao et al.,
2021). All detection models are trained on 8 NVIDIA V100 GPUs with a total batch size of 16.

D. Additional Experiment Results

D.1. On the Effect of Various Ripple Layers

As mentioned in §4.1, directly replacing all attention layers in DEIT-LA with RIPPLE could be a sub-optimal choice. To
validate this, we conduct an ablation study on ImageNet1k dataset to investigate the effect of different numbers of ripple
layers, where the ﬁrst several layers use ripple attention while upper ones still adopt linearized attention mechanism. The
results are shown in Table 7. In particular, we ﬁnd the model performance consistently improves as the number of ripple

Ripple Attention for Visual Perception with Sub-quadratic Complexity

Table 7: Classiﬁcation results on ImageNet1k dataset under different numbers of rippling layers for RIPPLE. The speed is
measured by the number of images processed per second with a batch size of 64 on a single NVIDIA V100 GPU machine,
averaged by 5 runs.

# ripple layers

Speed

Top-1 Acc.

Top-5 Acc.

0
3
6
9
12

2664
1355
916
792
563

70.67
71.63
72.41
73.02
72.69

90.16
90.42
90.32
91.56
91.30

Figure 4: Empirical running time (left) and memory consumption (right) under different numbers of tokens, averaged by 5
runs.

layers increases, but drops a little when the depth of ripple layers reaches a certain level (e.g., 9). Our observation aligns
with our intuition, which suggests using hybrid attention layers could achieve a good trade-off between locality promotion
and global dependency modeling. Therefore, RIPPLE uses 9 ripple layers by default throughout our experiments unless
otherwise stated.

D.2. On the Effect of Different Parameterization Schemes for Vicinal Groups

Ripple attention is a ﬂexible framework in that it allows the trade-off between the running time complexity and task accuracy.
To explore this, we compare the full ripple attention against a rippling process where ripples get exponentially thicker so that
the process could reach the image boundary in logarithmic time. Formally, recall that vicinal groups with respect to a query
token at position (i, j) is deﬁned as {Nr(i, j)}R
r=0, where every token at position (m, n) belongs to Nr(i, j) if and only if
max(|m − i|, |n − j|) = r. In the setting of RIPPLE-LOGARITHMIC, we generalize this notion by relaxing the equality
condition, that is, now (m, n) ∈ Nr(i, j) if and only if 2r ≤ max(|m − i|, |n − j|) < 2r+1, which allows the size of vicinal
groups to be exponentially larger instead of keeping constant. In this way, the number of all vicinal groups R(cid:48) is O(log R).
This model, which we refer to as RIPPLE-LOGARITHMIC, enjoys O(HW log R) time complexity and is more efﬁcient than
base ripple attention.

To empirically evaluate the efﬁciency of this variant, we plot the empirical running statistics of RIPPLE-LOGARITHMIC
under different numbers of tokens. For completeness, we also include a variant where Rmax scales linearly with the image
height (or width), denoted by RIPPLE-DENSE. As shown in Figure 4, we observe RIPPLE-LOGARITHMIC runs as fast as base
ripple (whose Rmax is ﬁxed) and becomes more efﬁcient than the dense version as the number of tokens increases. On the
other hand, all of these models run with the same amount of memory consumption, as their space complexity is constant in
the rippling distance. In terms of task performance, as reported in Table 4, we see a clear performance drop if we adopt
RIPPLE-LOGARITHMIC, which could be due to that RIPPLE-LOGARITHMIC processes visual tokens at a coarser-grained
level. This again justiﬁes the ﬂexibility of our framework: one could trade off the task accuracy for more efﬁciency and vice
versa.

1216232248264280296211221282Number of Tokens02505007501000125015001750Running Time (ms)Ripple (DP)Ripple (Naive)Ripple (Dense)Ripple (Logarithmic)DEIT-LADEIT1216232248264280296211221282Number of Tokens0.02.55.07.510.012.515.017.5Memory (GB)Ripple (DP)Ripple (Naive)Ripple (Dense)Ripple (Logarithmic)DEIT-LADEITRipple Attention for Visual Perception with Sub-quadratic Complexity

Table 8: Classiﬁcation results on CIFAR-100 dataset under the same speed constraint.

Models

Speed

# Params

Top-1 Acc.

4-layer DEIT-LA

DEIT-LA
4-layer RIPPLE

RIPPLE

6953

2686
2369

893

1.88M

5.50M
1.89M

5.52M

63.35

67.10
70.03

74.11

D.3. Performance Comparison under the Same Speed Constraint

We conduct an ablative experiment to compare the performance of RIPPLE against DEIT-LA under the same speed constraint.
The hyper-parameter conﬁguration remains the same as the main experiment. As shown in Table 8, we ﬁnd that a 4-layer
transformer with RIPPLE outperforms a 12-layer model with DEIT-LA by a reasonable margin while running at a similar
speed. This indicates the enlarged modeling capacity of RIPPLE compared to traditional linearized attention, demonstrating
the effectiveness of our approach.

E. Setup for Empirical Running Time and Memory Consumption

For the simulation experiment conducted in §5.3, we use the same vision transformer architecture for all models, whose
hyper-parameter setting is speciﬁed in Appendix C.4, except that we set embedding dimension to 96 and batch size to 4;
otherwise, most conﬁgurations tested here will make it infeasible for DEIT and RIPPLE (Naïve) to ﬁt into the 32GB memory
of a single NVIDIA V100 GPU machine.

