Non-Euclidean Contractivity of Recurrent Neural Networks

Alexander Davydov, Anton V. Proskurnikov, and Francesco Bullo

2
2
0
2

r
a

M
4
2

]

C
O
.
h
t
a
m

[

3
v
8
9
2
8
0
.
0
1
1
2
:
v
i
X
r
a

Abstract— Critical questions in dynamical neuroscience and
machine learning are related to the study of recurrent neural
networks and their stability, robustness, and computational
efﬁciency. These properties can be simultaneously established
via a contraction analysis.

This paper develops a comprehensive contraction theory
for recurrent neural networks. First, for non-Euclidean (cid:96)1/(cid:96)∞
logarithmic norms, we establish quasiconvexity with respect
to positive diagonal weights and closed-form worst-case ex-
pressions over certain matrix polytopes. Second, for locally
Lipschitz maps (e.g., arising as activation functions), we show
that their one-sided Lipschitz constant equals the essential
supremum of the logarithmic norm of their Jacobian. Third
and ﬁnal, we apply these general results to classes of recurrent
neural circuits, including Hopﬁeld, ﬁring rate, Persidskii, Lur’e
and other models. For each model, we compute the optimal
contraction rate and corresponding weighted non-Euclidean
norm via a linear program or, in some special cases, via a
Hurwitz condition on the Metzler majorant of the synaptic
matrix. Our non-Euclidean analysis establishes also absolute,
connective, and total contraction properties.

I. INTRODUCTION

Motivation from dynamical neuroscience and machine
learning. Tremendous progress made in neuroscience re-
search has produced new understanding of biological neural
processes. Similarly, machine learning has become a key
technology in modern society, with remarkable progress in
numerous computational tasks. Much ongoing research fo-
cuses on artiﬁcial learning systems inspired by neuroscience
that (i) generalize better, (ii) learn from fewer examples, and
(iii) are increasingly energy-efﬁcient. We argue that further
progress in these disciplines hinges upon modeling, analysis
and computational challenges, some of which we highlight
via the indicator (C) in what follows.

In dynamical neuroscience, several recurrent neural net-
work (RNN) models are widely studied, including membrane
potential models such as the Hopﬁeld neural network [18]
and ﬁring-rate models [29]. Clearly, such models are simpli-
ﬁcations of complex neural dynamics. For example, if f (x)
is an RNN model of a neural circuit, the true dynamics is
better estimated by

˙x(t) = f (x(t)) + g(x(t), x(t

τ )),

−

(1)

This material is based upon work supported by the National Science
Foundation Graduate Research Fellowship under Grant No. 2139319 and
AFOSR grant FA9550-22-1-0059.

Alexander Davydov and Francesco Bullo are with the Department of
Mechanical Engineering and the Center for Control, Dynamical Systems,
and Computation, University of California, Santa Barbara, 93106-5070,
USA. {davydov, bullo}@ucsb.edu.

Anton V. Proskurnikov is with the Department of Electronics
Italy

Telecommunications,

Politecnico

Torino,

Turin,

di

and
anton.p.1982@ieee.org.

where g captures model uncertainty and time-delays. In other
words, (C1): to account for uncertainty in the system, the
nominal dynamics f (x) must exhibit robust stability with
respect to unmodeled dynamics and delay.

Central pattern generators (CPGs) are biological neural
circuits that generate periodic signals and are the source of
rhythmic motor behaviors such as walking, swimming, and
breathing. To properly model CPGs in RNNs, a computa-
tional neuroscientist would need to ensure that, (C2): if an
RNN is interconnected with a CPG, then entrainment takes
place and the trajectories of the RNN converge to a unique
stable limit cycle.

Machine learning scientists have widely adopted discrete-
time RNNs for pattern recognition and analysis of sequential
data and much recent interest [4], [22], [35], [21] has focused
on the closely-related class of implicit neural networks. In
particular, training implicit networks corresponds to solving
ﬁxed-point problems of the form

x = Φ(Ax + Bu + b),

(2)

where x is the neural state variable, Φ is an activation
function, A and B are synaptic weights, u is the input
stimulus, and b is a bias term. Note that (i) the ﬁxed point
in equation (2) is the equilibrium point of a corresponding
RNN differential equation, (ii) the training problem requires
the efﬁcient computation of gradients of a given loss function
with respect to model parameters; in turn, this computation
can be cast again as a ﬁxed-point problem. In other words,
in the design of RNNs and implicit neural networks, it is
essential to pick model weights in such a way that (C3):
ﬁxed-point equations have unique solutions for all possible
inputs and activation functions, and (C4): ﬁxed-points and
corresponding gradients can be computed efﬁciently.

Finally, an additional challenge facing machine learning
scientists is robustness to adversarial perturbations. Indeed,
it is well-known [42] that artiﬁcial deep neural networks are
sensitive to adversarial perturbations: small input changes
may lead to large output changes and loss in pattern recog-
nition accuracy. One proposed remedy is to characterize
the Lipschitz constants of these networks and use them
as regularizers in the training process. This remedy leads
to certiﬁable robustness bounds with respect to adversarial
perturbations [36], [14]. In short, (C5): the input/output
Lipschitz constants of RNNs need to be tightly estimated,
e.g., in the context of the ﬁxed-point equation (2).

A contraction theory for neural networks. Motivated by
the challenges arising in neuroscience and machine learning,
this paper aims to perform a robust stability analysis of

 
 
 
 
 
 
continuous-time RNNs and develop optimization methods
for discrete-time RNN models. Serendipitously, both these
objectives can be simultaneously achieved through a con-
traction analysis for the RNN dynamics.

For concreteness’ sake, we brieﬂy review how the afore-
mentioned challenges (C1-C5) are addressed by a con-
traction analysis. Inﬁnitesimally contracting dynamics enjoy
highly ordered transient and asymptotic behaviors: (C1)
initial conditions are forgotten and the distance between
trajectories is monotonically vanishing [27], (C3) time-
invariant systems admit a unique globally exponentially
stable equilibrium with two natural Lyapunov functions (dis-
tance from the equilibrium and norm of the vector ﬁeld) [27],
(C2) periodic systems admit a unique globally exponentially
stable periodic solution or, for systems with periodic inputs,
each solution entrains to the periodic input [37], (C1) and
(C5) contracting vector ﬁelds enjoy highly robust behavior,
e.g., see [43], [10], including (a) input-to-state stability, (b)
ﬁnite input-state gain, (c) contraction margin with respect to
unmodeled dynamics, and (d) input-to-state stability under
delayed dynamics. Hence, the contraction rate is a natural
measure/indicator of robust stability. Paraphrasing [33], con-
tracting systems are in many ways similar to stable linear
systems (but without superposition principle).

With regards to (C4), our recent work [7], [21] shows
how to design efﬁcient ﬁxed-point computation schemes
for contracting systems (with respect to arbitrary and non-
Euclidean (cid:96)1/(cid:96)∞ norms) in the style of monotone operator
theory [38]. Speciﬁcally, for contracting dynamics with re-
spect to a diagonally-weighted (cid:96)1/(cid:96)∞ norm, optimal step-
sizes and convergence factors are given in [21, Theorem 2].
These results are directly applicable to the computation of
ﬁxed-points in implicit neural networks, as in equation (2).
These step-sizes, however, depend on the contraction rate.
Therefore, optimizing the contraction rate of the dynamics
directly improves the convergence factor of the correspond-
ing discrete algorithm.

Literature review. The dynamical properties of RNN
models have been studied for a few decades. Shortly after
Hopﬁeld’s original work [18], control-theoretic ideas were
proposed by is [28]. Later, [23], [15], [16] obtained various
version of the following result: Lyapunov diagonal stability
of the synaptic matrix is sufﬁcient, and in some cases neces-
sary, for the existence, uniqueness, and global asymptotic
stability of the equilibrium. Notably, [13] is the earliest
reference on the application of logarithmic norms to Hopﬁeld
neural networks and provides results on (cid:96)p logarithmic
norms of the Jacobian for networks with smooth activation
functions. [3] proposes a quasi-dominance condition on the
synaptic matrix (in lieu of Lyapunov diagonal stability). [34]
proposes the notion of the nonlinear measure of a map
to study global asymptotic stability; this notion is closely
related to the (cid:96)1 one-sided Lipschitz constant of the map. A
comprehensive survey on continuous-time RNNs is [45].

Recently,

the non-Euclidean contraction of monotone
Hopﬁeld neural networks is studied in [20]; see also

[9] for the interplay between Metzler matrices and non-
Euclidean logarithmic norms. Also recently, [31] studies
linear-threshold rate neural dynamics, where activation func-
tions are piecewise-afﬁne; it is shown that the dynamics have
a unique equilibrium if and only if the synaptic matrix is a
-matrix. Since checking this condition is NP-hard, more
P
conservative convex conditions are provided as well. The
importance of non-Euclidean log norms in contraction theory
is highlighted for example in [37], [2].

Contributions. This paper

Finally, contractivity of RNNs with respect to the (cid:96)2 norm
has been studied, e.g., see the early reference [13], the related
discussion in [35], and the recent work [25].
contributes

fundamental
control-theoretic understanding to the study of artiﬁcial
neural networks in machine learning and neuronal circuits
in neuroscience, thereby building a hopefully useful bridge
among these three disciplines.

Speciﬁcally, the paper develops a comprehensive contrac-
tion theory for RNN models through the following con-
tributions. First, we obtain novel logarithmic norm results
including (i) the quasiconvexity of the (cid:96)1 and (cid:96)∞ logarith-
mic norms with respect to diagonal weights and provide
novel optimization techniques to compute optimal weights
which yield larger contraction rates, (ii) logarithmic norm
properties of principal submatrices of a matrix with respect
to monotonic norms, and (iii) explicit formulas for the (cid:96)1
and (cid:96)∞ logarithmic norms under multiplicatively-weighted
uncertainty, resulting in a maximization of the logarithmic
norm over a matrix polytope. The formulas in (iii) generalize
previous results [13, Theorem 3.8], [17, Lemma 3] and [21,
Lemma 8].

Motivated by our non-Euclidean logarithmic norm results,
we deﬁne M -Hurwitz matrices, i.e., matrices whose Metzler
majorant is Hurwitz. We compare M -Hurwitz matrices with
other classes of matrices including quasidominant, totally
Hurwitz, and Lyapunov diagonally stable matrices.

Second, we provide a nonsmooth extension to contraction
theory. We show that, for locally Lipschitz vector ﬁelds,
the one-sided Lipschitz constant is equal to the essential
supremum of the logarithmic norm of the Jacobian. This
equality allows us to use our novel logarithmic norm results
and apply them to RNNs that have nonsmooth activation
functions such as ReLU.

Third and ﬁnally, we consider multiple models of recurrent
neural circuits and nonlinear dynamical models, including
Hopﬁeld, ﬁring rate, Persidskii, Lur’e, and others. We con-
sider activation functions that are weakly increasing and
Lipschitz (thus more general than the class of piecewise-
afﬁne activation functions). For each model, we propose a
linear program to characterize the optimal contraction rate
and corresponding weighted non-Euclidean (cid:96)1 or (cid:96)∞ norm.
In some special cases, we show that the linear program
reduces to checking an M -Hurwitz condition on the synaptic
matrix. Our results simplify the computation of a common
Lyapunov function over a polytope with 2n vertices to a
simple condition involving just 2 of its vertices or, in some
cases, all the way to a closed form expression.

For each model, we demonstrate that the dynamics enjoy
strong, absolute and total contractivity properties. In the spirit
of absolute and connective stability, absolute contractivity
means that the dynamics are contracting independently of
the choice of activation function and connective stability
means that the dynamics remain contracting whenever edges
between neurons are removed. Total contractivity means
that if the synaptic matrix is M -Hurwitz and is replaced
by any principal submatrix, the principle submatrix is also
M -Hurwitz. The process of replacing the nominal RNN
with a subsystem RNN is referred to as “pruning” both in
neuroscience and in machine learning.

Paper organization. Section II reviews known prelimi-
nary concepts. Section III provides novel logarithmic norms
results. Section IV studies nonsmooth contraction theory.
Section V establishes conditions for the contractivity of
classes of neural dynamics.

Acknowledgments The authors wish to thank Dr. Saber
Jafarpour for stimulating conversations and valuable insights.

II. REVIEW OF RELEVANT MATRIX ANALYSIS

◦

For two matrices (or vectors) A, B, we let A

B be

∈

∈

Rn, we deﬁne [η]

entrywise multiplication. Vector inequalities of the form x
≤
Rn×n
y are entrywise. For a vector η
to be the diagonal matrix with diagonal entries equal to η.
Rn be the all-ones and all-zeros vectors,
We let 1n, 0n
on Rn is monotonic if for
respectively. We say a norm
all x, y
y
, where the absolute
(cid:107)
(cid:107) ≤ (cid:107)
Rn×n is Metzler
value is applied entrywise. A matrix M
Rn×n, its
if Mij
0 for all i
∈
spectral abscissa is α(A) = max
(λ)
spec(A)
}
∈
Rn×n is deﬁned by
and its Metzler majorant

= j. For a matrix A
λ

y
| ≤ |

(cid:107) · (cid:107)
x

⇒ (cid:107)

Rn,

x
|

≥

=

∈

∈

∈

|

|

(cid:40)

aii,
aij

|

,

|

A

(
(cid:100)

(cid:101)

Mzr)ij =

A. Log norms

{(cid:60)
∈

(cid:101)

A
Mzr
(cid:100)
if i = j
= j
if i

.

Let

be a norm on Rn and its corresponding induced
norm on Rn×n. The logarithmic norm (also called log norm
or matrix measure) of a matrix A

Rn×n is

(cid:107) · (cid:107)

µ(A) := lim
h→0+

∈
In + hA
(cid:107)

h

1

.

(3)

(cid:107) −

(cid:107)

∈

≤

∞

p,R =
(cid:107)

∈
Rx
(cid:107)
(cid:107)

] and for invertible R
x

We refer to [11] for a list of properties of log norms, which
µ(A). For an (cid:96)p
include subadditivity, convexity, and α(A)
Rn×n, we deﬁne the
norm, p
[1,
R-weighted (cid:96)p norm by
p. It is known that
the corresponding log norm is then µp,R(A) = µp(RAR−1).
For diagonally weighted (cid:96)1, (cid:96)∞, and (cid:96)2 norms,
ηj
ηi |
,
}
ηj
ηi |
,
}

µ∞,[η]−1(A) = max

µ1,[η](A) = max

(cid:62)
Mzrη
A
(cid:101)
(cid:88)n

= min
{

| (cid:100)
aii +

i∈{1,...,n}

i∈{1,...,n}

aii +

j=1,j(cid:54)=i

j=1,j(cid:54)=i

(cid:88)n

aji

aij

bη

≤

≤

R

R

∈

∈

(cid:101)

b

b

|

|

A
bη
Mzrη
| (cid:100)
[η]A + A(cid:62)[η]

= min
{
b
µ2,[η]1/2(A) = min
{

.
2b[η]
}

(cid:22)

R

∈

|

The following result is due to [41] and [32, Lemma 3].

∈

∈

(cid:32)

∞

[1,

Rn

ηM,p,δ =

>0 by
(cid:33)
,

Lemma 1 (Optimal diagonally-weighted log norms for
Rn×n,
Metzler matrices). Given a Metzler matrix M
p

], and δ > 0, deﬁne ηM,p,δ

∈
w1/p
n
v1/q
n
] is deﬁned by 1/p + 1/q = 1 (with the
where q
[1,
convention 1/
= 0) and where v and w
>0 are
the right and left dominant eigenvectors of the irreducible
Metzler matrix M + δ1n1(cid:62)
n (whose existence is guaranteed
by the Perron-Frobenius Theorem). Then for each (cid:15) > 0 there
exists δ > 0 such that

w1/p
1
v1/q
1

∞
∞

, . . . ,

Rn

(4)

∈

∈

(i) α(M )
(ii) if M is irreducible, then α(M ) = µp,[ηM,p,0](M ).

µp,[ηM,p,δ](M )

α(M ) + (cid:15),

≤

≤

Lemma 1 also ensures that for Metzler matrices M
].

µp,[η](M ) = α(M ) for every p

[1,

∈

∈

∞

>0

Rn×n, inf η∈Rn
B. Classes of matrices
We say a matrix A
(i) Hurwitz stable, denoted by A
(ii) totally Hurwitz, denoted by A

Rn×n is

∈

, if α(A) < 0,

, if all principal

∈ H

∈ T H

submatrices of A are Hurwitz stable,

(iii) Lyapunov diagonally stable (LDS), denoted by A

, if there exists a η

Rn

∈
>0 such that µ2,[η]1/2(A) <

∈

stable, denoted by A

,

if

∈ MH

Rn×n is quasidominant [30] if there exists

LDS
0, and
(iv) M-Hurwitz

α(

A
(cid:100)
A matrix A

Mzr) < 0.
(cid:101)

a vector η

∈
Rn
>0 such that

∈
ηiaii >

(cid:88)n

ηj

aij
|
A

,

for all i

1, . . . , n

.
}

j=1,j(cid:54)=i

|
∈ {
Mzrη < 0n, which, in turn, is
This is equivalent to
equivalent (see, for example, [6, Theorem 15.17]) to the
inequality α(

Mzr) < 0, i.e.,

(cid:100)−

A

A

(cid:101)

.

The following results are essentially known in the litera-

−

∈ MH

(cid:100)−

(cid:101)

ture, but not collected in a uniﬁed manner.

Lemma 2 (Inclusions for classes of matrices). (A
implies (A
(A

∈ LDS
) implies (A

) implies (A

∈ LDS

), (A

).

∈ T H

)
∈ MH
), and

∈ T H

∈ H

We show that the counter-implications in Lemma 2 do not

hold.

Example 3.
(cid:20)

(i) (A
(cid:21)
1
1
A

1
−
2

A =

−
−
However, α(
(cid:100)
=
⇒
∈ T H (cid:54)
=
∈ H (cid:54)

⇒

A

=
∈ LDS (cid:54)
satisﬁes µ2(A) =

⇒

A

∈ MH

0.5, so A

) The matrix

.

Mzr) = √2
(cid:101)
A

∈ LDS

∈ T H

−
1 > 0, so A /

∈ LDS
.

−
) is proved in [5, Remark 4].
(cid:21)

) The matrix A =

∈ MH
(cid:20) 1
4

1
3

−
∈ T H

. However, A /

−

satisﬁes α(A) =
−
since it has a positive diagonal entry.

1, so A

∈ H

(ii) (A

(iii) (A

<latexit sha1_base64="FKQGvClqlehb2zZTOEPixQqiu1g=">AAAB9HicbVDLSsNAFL2pr1pfVZduBlvBVUmKqMuimy4r2Ae0oUymk3boZBJnJoUS+h1uXCji1o9x5984SbPQ1gMDh3Pu5Z45XsSZ0rb9bRU2Nre2d4q7pb39g8Oj8vFJR4WxJLRNQh7KnocV5UzQtmaa014kKQ48Trve9D71uzMqFQvFo55H1A3wWDCfEayN5FYHAdYTgnnSXFSH5YpdszOgdeLkpAI5WsPy12AUkjigQhOOleo7dqTdBEvNCKeL0iBWNMJkise0b6jAAVVukoVeoAujjJAfSvOERpn6eyPBgVLzwDOTaUa16qXif14/1v6tmzARxZoKsjzkxxzpEKUNoBGTlGg+NwQTyUxWRCZYYqJNTyVTgrP65XXSqdec69rVQ73SuMvrKMIZnMMlOHADDWhCC9pA4Ame4RXerJn1Yr1bH8vRgpXvnMIfWJ8/OoaRwQ==</latexit>H<latexit sha1_base64="ub/aFPcAUotUFWy0UUyd7H3i0aE=">AAAB+HicbVC7TsMwFL0pr1IeDTCyWLRITFVSIWCsgIGBoQj6kNqocly3teo4ke0glahfwsIAQqx8Cht/g9NmgJYjWTo6517d4+NHnCntON9WbmV1bX0jv1nY2t7ZLdp7+00VxpLQBgl5KNs+VpQzQRuaaU7bkaQ48Dlt+eOr1G89UqlYKB70JKJegIeCDRjB2kg9u1juBliPCObJ7fX9tNyzS07FmQEtEzcjJchQ79lf3X5I4oAKTThWquM6kfYSLDUjnE4L3VjRCJMxHtKOoQIHVHnJLPgUHRuljwahNE9oNFN/byQ4UGoS+GYyTakWvVT8z+vEenDhJUxEsaaCzA8NYo50iNIWUJ9JSjSfGIKJZCYrIiMsMdGmq4IpwV388jJpVivuWeX0rlqqXWZ15OEQjuAEXDiHGtxAHRpAIIZneIU368l6sd6tj/lozsp2DuAPrM8f8OySoQ==</latexit>LDS<latexit sha1_base64="3iFUp7ka1ZlIYabshyGdtVptgbA=">AAAB9XicbVDLSgMxFL1TX7W+qi7dBFvBVZkpRV0W3XQjVLAPaMeSSTNtaCYzJBmlDP0PNy4Uceu/uPNvzLSz0NYDgcM593JPjhdxprRtf1u5tfWNza38dmFnd2//oHh41FZhLAltkZCHsuthRTkTtKWZ5rQbSYoDj9OON7lJ/c4jlYqF4l5PI+oGeCSYzwjWRnoo9wOsxwTz5LYxKw+KJbtiz4FWiZOREmRoDopf/WFI4oAKTThWqufYkXYTLDUjnM4K/VjRCJMJHtGeoQIHVLnJPPUMnRlliPxQmic0mqu/NxIcKDUNPDOZhlTLXir+5/Vi7V+5CRNRrKkgi0N+zJEOUVoBGjJJieZTQzCRzGRFZIwlJtoUVTAlOMtfXiXtasW5qNTuqqX6dVZHHk7gFM7BgUuoQwOa0AICEp7hFd6sJ+vFerc+FqM5K9s5hj+wPn8A2PKSGA==</latexit>MH<latexit sha1_base64="aGcIe3XJ9zfbrQqpJTnBuOZUAYY=">AAAB9XicbVDLTgIxFL2DL8QX6tJNI5i4IjOEqEuiG5aY8EpgJJ3SgYZOZ9J2NGTCf7hxoTFu/Rd3/o0dmIWCJ2lycs69uafHizhT2ra/rdzG5tb2Tn63sLd/cHhUPD7pqDCWhLZJyEPZ87CinAna1kxz2oskxYHHadeb3qV+95FKxULR0rOIugEeC+YzgrWRHsqDAOsJwTxpNeblYbFkV+wF0DpxMlKCDM1h8WswCkkcUKEJx0r1HTvSboKlZoTTeWEQKxphMsVj2jdU4IAqN1mknqMLo4yQH0rzhEYL9fdGggOlZoFnJtOQatVLxf+8fqz9GzdhIoo1FWR5yI850iFKK0AjJinRfGYIJpKZrIhMsMREm6IKpgRn9cvrpFOtOFeV2n21VL/N6sjDGZzDJThwDXVoQBPaQEDCM7zCm/VkvVjv1sdyNGdlO6fwB9bnD+Ojkh8=</latexit>TH(cid:54)
(cid:54)
This insert corresponds to Lemma 6. For A

Rn×n, c

R, and η
∈
µ∞,[η]([c] + [d]A) = max (cid:8)µ∞,[η]([c] + dminA), µ∞,[η]([c] + dmaxA)(cid:9),
µ1,[η]([c] + A[d]) = max (cid:8)µ1,[η]([c] + dminA), µ1,[η]([c] + dmaxA)(cid:9),

Rn, 0

dmax

dmin

≤

≤

∈

∈

Rn

>0,

∈

µ∞,[η]([c] + dmaxA), µ∞,[η]([c] + dmaxA
µ∞,[η]([c] + A[d]) = max
{
µ1,[η]([c] + dmaxA), µ1,[η]([c] + dmaxA
µ1,[η]([c] + [d]A) = max
{

−

(dmax

dmin)(In

A))

−
(dmax

−
dmin)(In

−

◦
A))

.

}

◦

max
d∈[dmin,dmax]n
max
d∈[dmin,dmax]n
max
d∈[dmin,dmax]n
max
d∈[dmin,dmax]n

(5)

(6)

(7)

(8)

,
}

III. NOVEL LOG NORM RESULTS

A. Optimizing non-Euclidean log norms

First, we provide novel results on optimizing diagonal
weights for (cid:96)1 and (cid:96)∞ log norms and provide computational
methods to compute these weights.

Theorem 4 (Quasiconvexity of µ with respect to diagonal
Rn×n, consider the maps from Rn
weights). For ﬁxed A
>0
to R deﬁned by

∈

η

η

(cid:55)→

(cid:55)→

µ1,[η](A),
µ∞,[η]−1(A).

(9)

Then

(i) The maps in (9) are quasiconvex and their sublevel sets

are polytopes.

(ii) Minimizing the maps in (9) may be executed via the

optimization problems

inf
b∈R,η∈Rn

>0

b

s.t.

for µ1,[η](A) and

(cid:62)
Mzrη

A

(cid:101)

(cid:100)

≤

bη,

inf
b∈R,η∈Rn

>0

b

s.t.

for µ∞,[η]−1(A).

Mzrη

A

(cid:101)

(cid:100)

≤

bη,

(10)

(11)

Proof. Regarding statement (i), we provide the proof for p =
1 since p =
is essentially identical. In other words, we will
µ1,[η](A) are convex.
show that sublevel sets of the map η
Rn
R, the set
b
µ1,[η](A)
For ﬁxed b
η
b :=
>0 |
{
is characterized by η satisfying

(cid:55)→
∈

M

∞

≤

∈

}

(cid:88)n

ηiaii +

ηj

aji

ηib,

for all i

1, . . . , n

|

j=1,j(cid:54)=i

| ≤
Since each of these inequalities is linear in η, for ﬁxed b,
b
M
is a polytope, proving quasiconvexity. Statement (ii) follows
from the deﬁnitions of µ1,[η](A) and µ∞,[η]−1(A).

∈ {

.
}

Remark 5. The optimization problems in (10) and (11) may
[n for ε > 0 sufﬁciently small
be modiﬁed such that η
∞
so that the inf becomes a min. Then the problems may be
solved by a bisection on b
], where each step
[
of the algorithm is a linear program (LP) in η.

A
(cid:107)

,
(cid:107)

−(cid:107)

[ε,

A

∈

∈

(cid:107)

Next, we provide closed-form expressions for (cid:96)1 and (cid:96)∞
log norms over a certain polytopes of matrices. Polytopes
of interest are deﬁned by a nominal matrix multiplied by a

diagonally-weighted uncertainty and shifted by an additive
diagonal matrix. Such matrix polytopes arise in tests verify-
ing the contractivity of several classes of RNNs.

Lemma 6 (Worst-case (cid:96)1/(cid:96)∞ log norms under multiplicative
R,
scalings). Any A
and η

Rn×n, c
Rn, 0
>0 satisfy formulas (5)-(8).

dmax

dmin

Rn

≤

≤

∈

∈

∈

∈

Proof. First we consider formula (5). For 0
dmax
= i. With
aij
ij =
([c]+[d]A)
we have
|
|
the short-hand ri = aii + (cid:80)n
ηi/ηj, we compute
|

0+diaij
|

= di
aij

j=1,j(cid:54)=i |

≤
for j

dmin

≤

|

|

|

µ∞,[η]([c] + [d]A)

= max

i∈{1,...,n}

(cid:110)

([c] + [d]A)ii +

n
(cid:88)

j=1,j(cid:54)=i

([c] + [d]A)
|

|

ijηi/ηj

(cid:111)

ci + diri

.

}

= max

i∈{1,...,n}{
dmin

Since 0
ri > 0 and ri

≤

≤
0:

≤
max
i : ri>0{
ci + dmaxri

max
i : ri≤0{
ci + dminri

} ≤
ci + diri

} ≤

max
d∈[dmin,dmax]n
= max

i : ri>0{
max
d∈[dmin,dmax]n
= max

i : ri>0{
In summary,

dmax, we consider separately the cases of

ci + diri

}

= max
i : ri>0
µ∞,[η]([c] + dmaxA),

max
di∈[dmin,dmax]{

ci + diri

}

= max
i : ri≤0
µ∞,[η]([c] + dminA).

max
di∈[dmin,dmax]{

ci + diri

}

}

max
d∈[dmin,dmax]n
max
=
d∈[dmin,dmax]n

µ∞,[η]([c] + [d]A)

max
i∈{1,...,n}{
(cid:110)

ci + diri

}

=

max

max
max
i : ri≤0{
d∈[dmin,dmax]n
max (cid:8)µ∞,[η]([c] + dminA), µ∞,[η]([c] + dmaxA)(cid:9).

, max
i : ri>0{

ci + diri

}

ci + diri

≤
On the other hand, we note that

(cid:111)
}

µ∞,[η]([c] + [d]A)

max
d∈[dmin,dmax]n
max (cid:8)µ∞,[η]([c] + dmin[1n]A), µ∞,[η]([c] + dmax[1n]A)(cid:9)

≥
= max (cid:8)µ∞,[η]([c] + dminA), µ∞,[η]([c] + dmaxA)(cid:9),
thereby proving the equality (5). Next, regarding formula (6),
recall that µ1,[η](B) = µ∞,[η]−1 (B(cid:62)) for all B and compute

µ1,[η]([c] + A[d]) = max

max
d∈[dmin,dmax]n
= max (cid:8)µ∞,[η]−1 ([c] + dminA(cid:62)), µ∞,[η]−1([c] + dmaxA(cid:62))(cid:9)
= max (cid:8)µ1,[η]([c] + dminA), µ1,[η]([c] + dmaxA)(cid:9).

d∈[dmin,dmax]n

µ∞,[η]−1 ([c] + [d]A(cid:62))

(cid:54)
This concludes the proof of equality (6). Regarding for-
mula (7), we compute

max
d∈[dmin,dmax]n

µ∞,[η]([c] + A[d])

=

max
d∈[dmin,dmax]n

max
i∈{1,...,n}

ci + aiidi +

= max

i∈{1,...,n}

max
d∈[dmin,dmax]n

ci + aiidi +

max
i∈{1,...,n}

max
d∈[dmin,dmax]n

≤

ci + aiidi +

n
(cid:88)

j=1,j(cid:54)=i
n
(cid:88)

j=1,j(cid:54)=i
n
(cid:88)

j=1,j(cid:54)=i

ηi
ηj |

aijdj

ηi
ηj |

aijdj

|

|

ηi
ηj |

dmaxaij

|

max
i∈{1,...,n}

≤






ci + dmaxaii +

ci + dminaii +

n
(cid:88)

j=1,j(cid:54)=i
n
(cid:88)

j=1,j(cid:54)=i

ηi
ηj |

dmaxaij

, if aii
|

≥

0

ηi
ηj |

dmaxaij

, if aii < 0
|

max

≤

µ∞,[η]([c] + dmaxA),
{
µ∞,[η]([c] + dmaxA

−

(dmax

−

dmin)(In

.

A))
}

◦

On the other hand, letting i(cid:63)
(cid:80)
dmaxaij
entry, and d(cid:63) = dmax1n

ηi
ηj |

j(cid:54)=i

|

(dmax

arg maxi∈{1,...,n} dminaii +
, ei(cid:63) be the unit vector with 1 in its i(cid:63)-th

∈

dmin)ei(cid:63) , we have

−

−

µ∞,[η]([c] + A[d])

max
d∈[dmin,dmax]n
max
≥
{
= max
{

µ∞,[η]([c] + dmaxA[1n]), µ∞,[η]([c] + dmaxA[d(cid:63)])
}
µ∞,[η]([c] + dmaxA),
µ∞,[η]([c] + dmaxA

dmin)(In

(dmax

A))

−
thus concluding the proof of formula (7). Regarding for-
that µ1,[η](B) =
mula (8), we again leverage the fact
µ∞,[η]−1(B(cid:62)) to see that

−

◦

,
}

max
d∈[dmin,dmax]n
max
=
d∈[dmin,dmax]n

µ1,[η]([c] + [d]A)

µ∞,[η]−1 ([c] + A(cid:62)[d])

µ∞,[η]−1([c] + dmaxA(cid:62)),
= max
{
µ∞,[η]−1 ([c] + dmaxA(cid:62)
µ1,[η]([c] + dmaxA),
= max
{
µ1,[η]([c] + dmaxA

(dmax

−

dmin)(In

A))
}

◦

−

(dmax

dmin)(In

A))

.
}

◦

−

−

Thus, formulas (5)-(8) have been proved.

Recall that the log norm is a convex function and that
the maximum value of a convex function over a polytope is
achieved at one of the vertices of the polytope. In the special
case in Lemma 6, formulas (5)-(8) ensure that one needs to
check only 2 vertices of the polytope, rather than 2n.

Finally, we show how the optimal diagonal weights that
minimize the worst-case log norm of a matrix polytope as
in Lemma 6 can be easily computed.

Corollary 7. Let A, c, dmin, and dmax be as in Lemma 6.
) the
Then for µ[η](
·

) or µ∞,[η]−1(
·
·

) denoting either µ1,[η](

minimax problems

min
η∈[ε,∞[n
min
η∈[ε,∞[n

max
d∈[dmin,dmax]n
max
d∈[dmin,dmax]n

µ[η]([c] + [d]A),

µ[η]([c] + A[d]),

may each be solved by a bisection algorithm, each step of
which is an LP.

Proof. The proof is an immediate consequence of the for-
mulas (5)-(8) as well as the fact that a max of quasiconvex
functions is quasiconvex. Therefore, a bisection algorithm
similar to the one in Theorem 4(ii) may be used to compute
the optimal η.

Rn×n

∈

B. Monotonicity of diagonally-weighted log norms

Theorem 8 (Monotonicity of α and µ). For any A

α(

] and η

Mzr),
A
(cid:101)
(cid:100)
[1,
∞

(i) α(A)
≤
(ii) for all p
∈
∈
Mzr), with equality holding for p
µp,[η](
A
(cid:101)
(cid:100)
(iii) For p
1,
∈ {
inf
η∈Rn

∞}
µp,[η](A) = α(
(cid:100)

Mzr)

Rn

≥

A

(cid:101)

>0

,

α(A).

>0, we have µp,[η](A)

1,

∈ {

∞}

≤
.

∈

Proof. First, recall from [19, Theorem 8.1.18] that, for all
A

Rn×n,

(12)

ρ(A)

ρ(

A
|

),
|

≤
where ρ denotes the spectral radius of a matrix. Regarding
and deﬁne ¯A = A + γIn
statement (i), pick γ > maxi
ρ( ¯A)
=
so that
(which is true for any matrix) and, from inequality (12), we
know

|
Mzr + γIn. We note that α( ¯A)

¯A
|
|

Aii

≤

A

(cid:100)

(cid:101)

|

α(A) + γ = α( ¯A)
A

ρ( ¯A)

¯A
ρ(
≤
|
|
≤
Mzr + γIn) = α(

¯A
) = α(
)
|
|
Mzr) + γ.

= α(
(cid:100)
¯A
) = α(
|
|

A
(cid:101)
(cid:100)
Here ρ(
) follows from the Perron-Frobenius
Theorem for non-negative matrices. This proves state-
ment (i).

¯A
|
|

(cid:101)

(13)

B

Regarding statement (ii), note that the norm

(cid:107) · (cid:107)p,[η] is
monotonic, it is easy to see that, for all matrices B, we have
=
In +hA
|(cid:107)p,[η]. For small h > 0, we note
B
(cid:107)p,[η] ≤ (cid:107)|
(cid:107)
|
|
A
In + h
Mzr so that
(cid:101)
(cid:100)
In + hA
(cid:107)p,[η] ≤ (cid:107)|

A
In + h
(cid:100)

|(cid:107)p,[η] =

(cid:107)p,[η].

In + hA

(cid:107)
Therefore, for small enough h > 0,

Mzr

(cid:107)

(cid:101)

(cid:107)

In + hA
(cid:107)p,[η] −
h

1

≤

In + h
(cid:100)

(cid:107)

A

(cid:107)p,[η] −

1

.

Mzr
(cid:101)
h

Thus, statement (ii) follows from the formula (3) in the limit
0+. For p
as h
, statement (ii) holds by the
∞}
formulas for µ1,[η] and µ∞,[η].

∈ {

→

1,

Finally, regarding statement (iii), for p

statement (ii) we have

1,

∈ {

∞}

, by

inf
η∈Rn

>0

µp,[η](A) = inf
η∈Rn

>0

µp,[η](
(cid:100)

A

(cid:101)

Mzr).

Moreover,
inf η∈Rn

>0

since
A

µp,[η](
(cid:100)

(cid:101)

A
Mzr
(cid:100)
(cid:101)
Mzr) = α(

Mzr).

A
(cid:100)

(cid:101)

is Metzler, by Lemma 1,

(cid:101)

A

Theorem 8(iii) demonstrates

that using diagonally-
weighted (cid:96)1 and (cid:96)∞ log norms, the best bound one can
Mzr), which may be conservative.
achieve on α(A) is α(
(cid:100)
In the following example, we show that the (cid:96)2 norm does
not have the same conservatism. Despite the conservatism,
Theorem 4 demonstrates that optimizing diagonal weights
is computationally efﬁcient, being an LP at every step of
the bisection, while optimizing weights for the (cid:96)2 norm
is an LMI at every step, which is more computationally
challenging than an LP of similar dimension.

(cid:20) 1

(cid:21)
1
1 1

Example 9. The matrix A∗ =

has eigenvalues

−
1 + i, 1
{
Therefore, α(A∗) = 1 < 2 = α(
(cid:100)
(A∗ + A(cid:62)

∗ )/2 = I2 =

whereas

i
}

A∗

−

(cid:100)

Mzr has eigenvalues
(cid:101)

.
2, 0
}
Mzr). Additionally,
Mzr) = 2.

A∗

{

µ2(A∗) = 1 and µ2(
(cid:100)

A∗

⇒

(cid:101)

(cid:101)

C. Log norms of principal submatrices

}

I

∈

→

1, . . . , n

∈
, let AI

Given a matrix A
1, . . . , n

Rn and a non-empty index set
R|I|×|I| denote the principal
I ⊂ {
submatrix obtained by removing the rows and columns of A
which are not in
,
. Next, given a non-empty
I ⊂ {
}
Rn as follows:
deﬁne the zero-padding map padI : R|I|
padI(y) is obtained by inserting zeros among the entries of
y corresponding to the indices in
. For example,
}\I
, we deﬁne pad{1,3}(y1, y2) =
with n = 3 and
1, 3
}
{
on Rn
(y1, 0, y2). Then it is easy to see that given a norm
(cid:107)·(cid:107)
R≥0
I : R|I|
1, . . . , n
, the map
and non-empty
}
(cid:107)·(cid:107)
is a norm on R|I|.
padI(y)
y
deﬁned by
(cid:107)
(cid:107)
Lemma 10 (Norm and log norm of principal submatrices).
is monotonic, let µ and µI denote the log
Assume
(cid:107) · (cid:107)
norms associated to
I respectively. Any matrix
Rn×n satisﬁes
A

I ⊂ {
I =
(cid:107)
(cid:107)

1, . . . , n
{

(cid:107) · (cid:107)

(cid:107) · (cid:107)

and

→

=

I

∈
(i)
AI
I
(cid:107)
(ii) µI(AI)
(iii) if µ(A) < 0, then A

,
A
(cid:107)
µ(A),

≤ (cid:107)
≤

(cid:107)

.

∈ T H
Proof. Regarding statement (i), let DI denote the diagonal
matrix with entries (DI)ii = 1 if i
and (DI)ii = 0 if
i

∈ I

.

(cid:54)∈ I
With this notation, we are ready to compute

AI
(cid:107)

I =
(cid:107)

=

=

≤
=

max
y∈R|I|,(cid:107)y(cid:107)I =1 (cid:107)
max
y∈R|I|,(cid:107)y(cid:107)I =1 (cid:107)

AI y

I
(cid:107)
padI(AIy)

(cid:107)

max
y∈R|I|,(cid:107) padI (y)(cid:107)=1 (cid:107)

(DIADI) padI(y)
(cid:107)

max
x∈Rn,(cid:107)x(cid:107)=1 (cid:107)
DIADI

(cid:107)

(DIADI) x

(cid:107)
DI

(cid:107) ≤ (cid:107)

DI

A

(cid:107)(cid:107)

(cid:107)(cid:107)

=

A

.

(cid:107)

(cid:107)

(cid:107)

The last equality holds because the monotonicity of
implies

= 1. This concludes the proof of (i).

DI
(cid:107)

(cid:107)

Statement (ii) follows from the deﬁnition of log norm and
applying statement (i) to the matrix I|I| +hAI as a principal

(14)

(15)

(16)

(17)

(18)

(cid:107) · (cid:107)

submatrix of In + hA:

µI(AI) := lim
h→0+

lim
h→0+

≤

I|I| + hAI
(cid:107)

1

I

(cid:107)

−

h
In + hA
(cid:107)

(cid:107) −

h

1

= µ(A).

Finally, statement (iii) is an immediate consequence of (ii).

Corollary 11. If A
every non-empty

∈ MH ⊂
1, . . . , n

I ⊂ {

Rn×n, then AI
.
}
Mzr) < 0. By Lemma 1 and

∈ MH

for

(cid:101)

∈ MH

>0 such that µ1,[η](A) = µ1,[η](
(cid:100)

Proof. Since A
A
, α(
(cid:100)
Theorem 8(ii), for sufﬁciently small (cid:15) > 0, there exists η
Rn
≤
0. Then by Lemma 10(ii), for non-empty
µI,1,[η](AI)
α(
Mzr)
AI
(cid:100)
(cid:101)
conclude that AI

∈
Mzr)+(cid:15) <
(cid:101)
(cid:100)
1, . . . , n
,
}
I ⊂ {
µ1,[η](A) < 0. Moreover, by Theorem 8(ii),
Mzr) = µI,1,[η](AI) < 0. We
µI,1,[η](
(cid:100)
∈ MH

AI
.

≤
≤

Mzr)

α(

A

A

(cid:101)

(cid:101)

IV. ONE-SIDED LIPSCHITZ MAPS AND NONSMOOTH
CONTRACTION THEORY

A. Review of one-sided Lipschitz functions

We review weak pairings and one-sided Lipschitz maps as

introduced in [10]; see also the earlier works [40], [1].
Deﬁnition 12 (Weak pairing). A weak pairing on Rn is a
R satisfying:
map
(i) (Subadditivity and continuity in its ﬁrst argument)
Rn

: Rn

Rn

→

·(cid:75)

(cid:74)·

×

, for all x1, x2, y

x1, y

x2, y

+

,

x1 + x2, y
(cid:74)
and
,
(cid:74)·
x,

·(cid:75)
(ii) (Weak homogeneity)
(cid:74)
y
(cid:75)
(iii) (Positive deﬁniteness)
(iv) (Cauchy-Schwarz)

is continuous in its ﬁrst argument,
= α
(cid:74)
0,
= 0n,

x, αy
(cid:75)
Rn, α
≥
> 0 for all x

(cid:75)
(cid:75)
(cid:74)
αx, y
=
for all x, y
x, x
(cid:75)
(cid:75) | ≤ (cid:74)

y, y

(cid:74)

(cid:75)

x, x
(cid:75)

(cid:74)
x, y

(cid:74)
∈

x, y

(cid:74)−

| (cid:74)

1/2

=

−

(cid:75)

(cid:74)

(cid:75)

1/2 for all

and

(cid:75)

(cid:75) ≤ (cid:74)

x, y

∈

x, y

Rn.

Additionally, we say a weak pairing satisﬁes Deimling’s
) for
inequality if
lim
(cid:107)
h→0+
,
=
(cid:74)·

(cid:107) · (cid:107)
Deimling’s inequality is well-deﬁned since

y + hx
(cid:107)

h−1(
1/2.

1/2 deﬁnes

Rn, where

x, y
(cid:74)

(cid:75) ≤ (cid:107)

all x, y

(cid:107) − (cid:107)

·(cid:75)

∈

(cid:107)

y

y

a norm on Rn. Conversely, if Rn is equipped with a norm
(cid:107)·(cid:107)
,
then there exists a (possibly non-unique) weak pairing
·(cid:75)
(cid:74)·
1/2; see [10, Theorem 16]. Henceforth,
such that
we assume that weak pairings satisfy Deimling’s inequality.
The relationship between weak pairings and log norms is

(cid:107) · (cid:107)

·(cid:75)

(cid:74)·

=

,
(cid:74)·

·(cid:75)

,

∈

given by the little-known Lumer’s equality.

Lemma 13 (Lumer’s equality [10, Theorem 18]). Let
be a norm on Rn with compatible weak pairing

(cid:107) · (cid:107)
. Then

µ(A) =

sup
x∈Rn,x(cid:54)=0n

(cid:74)

Ax, x
2 ,
(cid:75)
x
(cid:107)

(cid:107)

for all A

∈

Deﬁnition 14 (One-sided Lipschitz functions [10, Deﬁni-
Rn is open and
tion 26]). Consider f : U
connected. We say f is one-sided Lipschitz with respect to
if there exists b
a weak pairing

Rn where U

R such that

→

⊆

,
(cid:74)·
f (y), x

·(cid:75)

−

f (x)

(cid:74)

−

∈
2,

y

(cid:75) ≤

b

x
(cid:107)

y

(cid:107)

−

for all x, y

U.

∈

,
·(cid:75)
(cid:74)·
Rn×n.

(19)

(cid:54)
We say b is a one-sided Lipschitz constant of f . Moreover,
the minimal one-sided Lipschitz constant of f is

osL(f ) := sup

x,y∈U,x(cid:54)=y

f (x)

(cid:74)

−
x
(cid:107)

f (y), x
2
y

−

(cid:107)

y

.

(cid:75)

−

(20)

If f is continuously differentiable and U is convex, it can
be shown that osL(f ) = supx∈U µ(Df (x)), where Df :=
∂f
∂x is the Jacobian matrix of f .
A vector ﬁeld f : Rn
≤
→
c < 0 is said to be strongly inﬁnitesimally contracting with
) satisfying ˙x = f (x)
·
e−ct
for
(cid:107) ≤
0. Moreover, if f is continuous, then all solutions

−
), y(
rate c. Any two trajectories x(
·
y(t)
additionally satisfy
all t
converge to a unique equilibrium.

Rn satisfying osL(f )

x(0)
(cid:107)

y(0)
(cid:107)

x(t)
(cid:107)

≥

−

−

B. Nonsmooth contraction theory

In this section we consider locally Lipschitz f and show
that in this case, the deﬁnition of osL does not depend on the
weak pairing and instead depends only on the norm through
the log norm.

Theorem 15 (osL simpliﬁcation for locally Lipschitz f ).
Rn locally Lipschitz on an open convex set,
For f : U
R the following statements are
U
equivalent:

Rn. Then for every c

→

⊆

∈

(i) osL(f )
≤
(ii) µ(Df (x))

c,

≤

c for almost every x

U .

∈

Recall that Df (x) exists for almost every x

Rademacher’s theorem.

U by

∈

To prove Theorem 15, we ﬁrst recall Clarke’s generalized
Jacobian from nonsmooth analysis and prove a generalization
of the mean-value theorem for vector-valued functions.

Deﬁnition 16 (Clarke’s generalized Jacobian [8, Deﬁni-
Rm be locally Lipschitz on an
tion 2.6.1]). Let f : U
U be the set of points where
open set U
⊂
f is not differentiable. Then Clarke’s generalized Jacobian
at x is

→
Rn and let Ωf

⊆

∂f (x) = conv

lim
{
i→∞

Df (xi)

xi

x and xi /
∈

Ωf

, (21)
}

→

|

where conv denotes the convex hull of a set.

In particular, it can be shown [12] if S

measure zero set that contains Ωf , then

Ωf is any

⊇

∂f (x) = conv

lim
i→∞

{

Df (xi)

xi

x and xi /
∈

S

.
}

→

|

(22)

The mean-value theorem has the following generalization
Rn,
denote the line

for locally Lipschitz functions. For any two points x, y
let [x, y] :=
t)y
tx + (1
{
segment connecting x and y.

[0, 1]

−

∈

∈

}

t

|

Lemma 17. For f : U
convex set U
∂f (u)
conv
{

⊆
u

|

[x, y]

→
Rn, let [x, y]

⊂

such that

}
f (y) = A(x

∈
f (x)

−

Rm locally Lipschitz on an open

U . Then there exists A

∈

Proof. Since [x, y] is a compact subset of Rn, it can be easily
seen that there exists a convex open set U0
U0
now follows from [8, Proposition 2.6.5].

⊂
U (where ¯U0 is the closure of U0). The statement

1 such that [x, y]

¯U0

⊂

⊂

Proof of Theorem 15. Regarding (ii) =
Since U is convex, [x, y]
exists A
u
conv
A(x
A

|
y). However, (ii) implies that µ(A)

U .
U . Then by Lemma 17, there
f (y) =
such that f (x)
[x, y]
U ,

−
∈
∂f (u) by continuity and convexity of µ. Therefore,

∂f (u)
{

(i), Let x, y

c for all u

⊂
∈

⇒

≤

−

∈

∈

}

∈

f (x)

(cid:74)

−

f (y), x

y

(cid:75)

−

=

≤

A(x

(cid:74)
µ(A)

−
x
(cid:107)

y), x

y

−

y

−
2

(cid:107)

≤

(cid:75)
x
c
(cid:107)

y

2,
(cid:107)

−

where the second line is due to Lumer’s equality, Lemma 13.
U such that Df (x) exists
Regarding (i) =
and let v

Rn and h > 0. Then by assumption,

(ii), let x

⇒

∈

∈

f (x + hv)

−
f (x + hv)

(cid:74)
h

f (x), hv

f (x), v

=

2

hv

c
(cid:107)
ch2

(cid:107)
v
(cid:107)

,
(cid:107)

(cid:75) ≤
(cid:75) ≤

⇒

−
which holds by the weak homogeneity of the weak pairing.
Dividing by h2 > 0 and taking the limit as h

0 implies

(cid:74)

lim
h→0+

(cid:115) f (x + hv)
h

−

f (x)

, v(cid:123)

≤

=

Df (x)v, v

⇒ (cid:74)
=
⇒

µ(Df (x))

(cid:75) ≤
≤

→

2

2

v

v

c
(cid:107)
c
(cid:107)
c,

(cid:107)

(cid:107)

where the ﬁnal implication holds by taking the supremum
over all v
= 1 together with Lumer’s equality,
v
(cid:107)
Lemma 13. Therefore, statement (ii) holds.

Rn with

∈

(cid:107)

Theorem 15 demonstrates that locally Lipschitz f enjoy a
similar simpliﬁcation in the osL deﬁnition as do continuously
differentiable functions.

In neural network models, nonsmooth activation functions
such as ReLU, LeakyReLU, and nonsmooth saturation func-
tions are prevalent; Theorem 15 allows us to use standard
log norm results to analyze these models.

V. CONTRACTING NEURAL DYNAMICS

We consider several models of neural circuits and charac-
terize their one-sided Lipschitz constants and therefore their
strong inﬁnitesimal contractivity.

A. Hopﬁeld neural network

We start with the continuous-time Hopﬁeld neural network

model, ﬁrst introduced in [18]:

˙x =

−

Cx + AΦ(x) + u =: fH(x),

(23)

Rn×n is a positive semi-deﬁnite diagonal matrix,
where C
Rn is a (possibly time-varying)
A
input, and Φ is a diagonal activation function. In other words,

Rn×n is arbitrary, u

∈

∈

∈

y).

−

1For instance, one can deﬁne U0 := {z ∈ Rn : dist(z, [x, y]) < (cid:15)},

where (cid:15) > 0 is small enough.

Φ(x) = [φ1(x1), . . . , φn(xn)], where each φi : R
Lipschitz and satisﬁes the slope-restricted constraints

→

φi(x)
x
φi(x)
x

0,

≥

dmin :=

inf
x,y∈R,x(cid:54)=y

dmax := sup

−
−
−
−
In other words, this ensures that φ(cid:48)
[dmin, dmax] for
i(x)
R. Many common activation functions
almost every x
satisfy these constraints including ReLU, tanh and sigmoids.

x,y∈R,x(cid:54)=y

(24)

∞

<

∈

∈

.

φi(y)
y
φi(y)
y

Theorem 18 (One-sided Lipschitzness of Hopﬁeld neural
network). Consider the Hopﬁeld neural network model (23)
with irreducible
Mzr and constant u
A
(cid:100)
(i) osL1,[η](fH) = max (cid:8)µ1,[η](
dmaxA)(cid:9), for arbitrary η

C + dminA), µ1,[η](
>0.

∈
(ii) The vector η minimizing osL1,[η](fH) is the solution to

Rn. Then

C +

−
Rn

−

∈

(cid:101)

inf
b∈R,η∈Rn

>0

b

R is

Rn

>0,

osL1,[η](fH) = sup

x∈Rn\ΩfH

µ1,[η](DfH(x))

= sup

x∈Rn\ΩfH
=
max
d∈[dmin,dmax]n
= max (cid:8)µ1,[η](

µ1,[η](

C + A DΦ(x))

−

µ1,[η](

C + A[d])

−

C + dminA), µ1,[η](

−

C + dmaxA)(cid:9),

−

where the second-to-last equality holds since the closure of
DΦ(x) is [dmin, dmax]n by the
the image of the map x
slope-restricted assumption on each φi and the last equality
holds by Lemma 6.

(cid:55)→

Statement (ii) holds by Corollary 7. Regarding state-

ment (iii), if C = cIn, then

osL1,[η](fH) =
=

=

−

−
(cid:40)

c + max (cid:8)µ1,[η](dminA), µ1,[η](dmaxA)(cid:9)
c + max (cid:8)dminµ1,[η](A), dmaxµ1,[η](A)(cid:9)
0
if µ1,[η](A)
if µ1,[η](A) < 0

c + dmaxµ1,[η](A),
c + dminµ1,[η](A),

≥

−
−

s.t.

(

(

C + dmin
A
(cid:100)
C + dmax

(cid:101)
A

(cid:62)
Mzr)η
(cid:62)
Mzr)η

(cid:100)

(cid:101)

−

−

bη,

bη.

≤

≤

Additionally, recall that η = wA is the optimal weight from
Lemma 1 for the irreducible Metzler matrix
Mzr with
respect to p = 1. Therefore,

A

(cid:101)

(cid:100)

inf
η∈Rn

>0
=

(iii) if C = cIn,

then, with wA

dominant eigenvector of

A

(cid:100)

∈
Mzr,
(cid:101)

Rn

>0 being the left

inf
η∈Rn
>0
(cid:40)

osL1,[η](fH) = osL1,[wA](fH)

=

c + max

dminα(

A
(cid:100)

(cid:101)

{

−

Mzr), dmaxα(

A
(cid:100)

(cid:101)

.
Mzr)
}

(25)

(iv) if dmin = 0 and C

0, then, with w∗

left dominant eigenvector of

(cid:31)

∈
C + dmax

−

Rn
>0 being the
A

Mzr,

(cid:100)

(cid:101)

compute

osL1,[η](fH) = osL1,[wA](fH)

c + dmaxµ1,[wA](A),
c + dminµ1,[wA](A),

0
if µ1,[wA](A)
if µ1,[wA](A) < 0

≥

−
−

(cid:40)

=

c + dmaxα(
A
(cid:100)
A
c + dminα(
(cid:100)

Mzr),
(cid:101)
Mzr),
(cid:101)
Regarding statement (iv), if dmin = 0 and C

if α(
(cid:100)
if α(
(cid:100)

−
−

A
A

(cid:101)
(cid:101)

Mzr)
0
≥
Mzr) < 0

.

0, we

(cid:31)

inf
η∈Rn

>0

osL1,[η](fH) = osL1,[w∗](fH)

= max (cid:8)α(

−

C), α(

C + dmax

−

Mzr)(cid:9).

A

(cid:101)

(cid:100)

(26)

In particular, Theorem 18 provides exact values for the
minimal one-sided Lipschitz constant of the Hopﬁeld neural
network with respect to diagonally-weighted (cid:96)1 norms.

As a consequence of this theorem, let b(cid:63), η(cid:63) be the optimal
solution for the LP in statement (ii). If b(cid:63) < 0,
then
the Hopﬁeld neural network (23) is strongly inﬁnitesimally
contracting with rate

with respect to

b(cid:63)

|

|

(cid:101)

Mzr

A
(cid:100)

the Hopﬁeld model. Consider,

Remark 19. In the event
the
that
results from Theorem 18 still provide tests for contraction
of
for example, case (iii)
is strongly inﬁnitesimally contracting
above. The model
Mzr is reducible, by
provided that osL(fH) < 0, and if
(cid:101)
Lemma 1 for every (cid:15) > 0, there exists η
>0 such that
∈
Mzr) + (cid:15). Thus, if (25) is negative,
µ1,[η](
then
Mzr) + (cid:15))
A
(cid:101)
may be made negative as well by taking (cid:15) small enough.

Mzr) + (cid:15)), dmin(α(
(cid:100)

Mzr)
A
(cid:100)
(cid:101)
c + max
−

A
α(
(cid:101)
(cid:100)
dmax(α(
(cid:100)

≤
{

Rn

A

A

}

(cid:100)

(cid:101)

(cid:107) · (cid:107)1,[η(cid:63)].
is reducible,

Proof of Theorem 18. Regarding statement (i), for any η

∈

−

−

osL1,[η](fH) = max (cid:8)µ1,[η](

C), µ1,[η](

C + dmaxA)(cid:9)

C), µ1,[η](

= max (cid:8)α(

C + dmaxA)(cid:9),
−
C) = maxi∈{1,...,n} −
cii =
which holds because µ1,[η](
α(
>0. Additionally, we have that η =
w∗ is the optimal weight for the irreducible Metzler matrix

C) for every η

Rn

−

−

−

∈

C + dmax

A

(cid:101)

(cid:100)

−

Mzr by Lemma 1. Thus,

inf
η∈Rn

>0

osL1,[η](fH ) = osL1,[w∗](fH )

α(
= max
{

−

C), α(

−

C + dmax

A
(cid:100)

(cid:101)

,
Mzr)
}

which proves the result.

B. Firing-rate neural network model

A related model, which is frequently used in the machine
learning literature and is closely-related to the Hopﬁeld
neural network model is the model

˙x =

−

Cx + Φ(Ax + u) =: fFR(x),

(27)

which we refer to as the ﬁring-rate model. The interpretation
Rn
for this name is that if Φ(x) is nonnegative for all x
(as is ReLU), then the positive orthant is forward-invariant

∈

and x is interpreted as a vector of ﬁring-rates, while in the
Hopﬁeld model, x can be negative and is thus interpreted as
a vector of membrane potentials.

In [29], it is shown that the models (23) and (27) are equiv-
alent and display the same set of behaviors. In alignment
with these results, we show that while the Hopﬁeld model
is naturally one-sided Lipschitz with respect to a diagonally-
weighted (cid:96)1 norm, the ﬁring-rate model is naturally one-sided
Lipschitz with respect to a diagonally-weighted (cid:96)∞ norm.

Theorem 20 (One-sided Lipschitzness of ﬁring-rate model).
Consider the ﬁring-rate model (27) with invertible A and
irreducible
A
(cid:100)
(i) osL∞,[η]−1(fFR)
dminA), µ∞,[η]−1(

Rn. Then
∈
µ∞,[η]−1(
max
−
{
, for arbitrary η

C + dmaxA)
>0
}
(ii) The choice of η minimizing osL∞,[η]−1 (fFR) is the

Mzr and constant u

C +
Rn

=

−

∈

(cid:101)

solution to

inf
b∈R,η∈Rn

>0

b

s.t.

(

(

C + dmin
(cid:100)
C + dmax

A

Mzr)η
(cid:101)
Mzr)η
A

−

−

bη,

bη.

≤

≤

(cid:100)

(cid:101)
Rn

∈
Mzr,
(cid:101)

(cid:100)

(iii) if C = cIn, then, with vA
dominant eigenvector of
A

>0 being the right

osL∞,[η](fFR) = osL∞,[vA]−1(fFR)

c + max

dminα(

−
(iv) if dmin = 0 and C

{

(cid:101)

Mzr), dmaxα(

A
(cid:100)
0, then, with v∗

right dominant eigenvector of

(cid:31)

A
(cid:100)

(28)

.
Mzr)
(cid:101)
}
Rn
>0 being the
Mzr,
A

(cid:100)

(cid:101)

∈

C + dmax

−

inf
η∈Rn

>0
=

Mzr)(cid:9).

(29)

(cid:100)
Rn

(cid:101)
>0 we compute

inf
η∈Rn

>0

osL∞,[η](fFR) = osL∞,[v∗]−1 (fFR)

= max (cid:8)α(

−

C), α(

−

C + dmax

A

Proof. Regarding statement (i), for any η

∈

osL∞,[η]−1 (fFR) = sup

x∈Rn\ΩfFR

µ∞,[η]−1 (DfFR(x))

= sup

µ∞,[η]−1(

C + DΦ(Ax + u)A)

x∈Rn\ΩfFR
=
max
d∈[dmin,dmax]n
= max (cid:8)µ∞,[η]−1 (

−

−

µ∞,[η]−1 (

C + [d]A)

C + dminA), µ∞,[η]−1 (

−

−

Additionally, recall that η = vA is the optimal weight from
Lemma 1 for the irreducible Metzler matrix
Mzr with
(cid:101)
respect to p =
. Therefore,

A

(cid:100)

∞

osL∞,[η]−1(fFR) = osL∞,[vA]−1(fFR)

inf
η∈Rn
>0
(cid:40)

=

=

−
−

(cid:40)

−
−

c + dmaxµ∞,[vA]−1(A),
c + dminµ∞,[vA]−1(A),

0
if µ∞,[vA]−1 (A)
if µ∞,[vA]−1 (A) < 0

≥

c + dmaxα(
c + dminα(

A
(cid:100)
A
(cid:100)

Mzr),
(cid:101)
Mzr),
(cid:101)

if α(
if α(

A
A

0
Mzr)
(cid:101)
≥
Mzr) < 0
(cid:101)

(cid:100)
(cid:100)

.

Regarding statement (iv), if dmin = 0 and C

0, we

(cid:31)

compute
osL∞,[η]−1 (fFR) = max (cid:8)µ∞,[η]−1(

C), µ∞,[η]−1(

C + dmaxA)(cid:9)

= max (cid:8)α(

−
C), µ∞,[η]−1 (

−
C + dmaxA)(cid:9),
−
−
cii =
C) = maxi∈{1,...,n} −
which holds because µ∞,[η]−1 (
α(
>0. Additionally, we have that η =
v∗ is the optimal weight for the irreducible Metzler matrix

C) for every η

Rn

−

−

∈

C + dmax

−

A

(cid:101)

(cid:100)

Mzr by Lemma 1. Thus,

inf
η∈Rn

>0

osL∞,[η]−1(fFR) = osL∞,[v∗]−1(fFR)

α(
= max
{

which proves the result.

−

C), α(

C + dmax

A

,

Mzr)
}

(cid:101)

(cid:100)

−

Rn×n, Theo-
Remark 21. In particular, for invertible A
rem 20(i) provides an exact value for the minimal one-sided
Lipschitz constant of the ﬁring rate model. In the event that A
is not invertible, then DΦ(Ax + u) may not fully capture the
set [dmin, dmax]n since A fails to be surjective. In particular,
>0, we instead get
for non-invertible A and arbitrary η

Rn

∈

∈

osL∞,[η]−1(fFR)
max

µ∞,[η]−1(
{

−

≤
C + dminA), µ∞,[η]−1(

C + dmaxA)

−

.
}

C. Other related models

We apply Theorem 18 and the log norm results in
Lemma 6 to the following related neural circuit models, all
of which are studied in the classic book [24]. In the following
Theorems, we assume all Metzler matrices are irreducible.

C + dmaxA)(cid:9),

Theorem 22 (Contractivity of special Hopﬁeld models).

(i)

If A

∈ MH

, and dmin > 0, the Persidskii-type2 model

where the second-to-last equality holds since the closure of
DΦ(Ax + u) is [dmin, dmax]n by
the image of the map x
the slope-restricted assumption on each φi and because A is
invertible. The last equality holds by Lemma 6.

(cid:55)→

Statement (ii) is a consequence of Corollary 7. Regarding

statement (iii), if C = cIn, then

osL∞,[η]−1 (fFR)
=

−

−
(cid:40)

c + max (cid:8)µ∞,[η]−1(dminA), µ∞,[η]−1(dmaxA)(cid:9)
c + max (cid:8)dminµ∞,[η]−1(A), dmaxµ∞,[η]−1 (A)(cid:9)
if µ∞,[η]−1(A)
0
if µ∞,[η]−1(A) < 0

c + dmaxµ∞,[η]−1(A),
c + dminµ∞,[η]−1(A),

≥

=

=

−
−

˙x = AΦ(x)

(ii) If

is strongly inﬁnitesimally contracting with respect to
norm

(cid:107) · (cid:107)1,[wA] with rate dmin
C + dmaxA

Mzr)
, the Hopﬁeld neural network
fH with dmin = 0 and positive diagonal C is strongly
inﬁnitesimally contracting with respect to
(cid:107)·(cid:107)1,[w∗] with
Mzr)(cid:9) > 0.
rate
C), α(

max (cid:8)α(

C + dmax

∈ MH

α(
(cid:100)

.
|

−

A

(cid:101)

|

−

−
Proof. Regarding statement (i), let fP(x) := AΦ(x). By
Theorem 18(iii) with c = 0,

−

(cid:101)

A
(cid:100)

dminα(
osL1,[wA](fP) = max
(cid:100)
{

A

Mzr), dmaxα(
(cid:100)

(cid:101)

A

(cid:101)

Mzr)

.
}

2See[24, Deﬁnition 3.2.1]

(cid:101)

A

since A

, α(
(cid:100)

Mzr) < 0,

so
However,
∈ MH
osL1,[wA](fP) = dminα(
Mzr). In particular, the Persidskii-
A
(cid:100)
type model is strongly inﬁnitesimally contracting with re-
spect to norm

(cid:101)
(cid:107) · (cid:107)1,[wA] with rate dmin
Regarding statement (ii), by Theorem 18(iv),
osL1,[w∗](fH) = max (cid:8)α(

C + dmax

C), α(

α(
(cid:100)

Mzr)

.
|

A

A

(cid:101)

|

−

−

(cid:100)

A

−

C + dmaxA

C +
In particular, since
dmax
Mzr) < 0 and since C is positive diagonal, we have
osL1,[w∗](fH) < 0 so that the Hopﬁeld neural network is
strongly inﬁnitesimally contracting with respect to
(cid:107) · (cid:107)1,[w∗]
C), α(
with rate

max (cid:8)α(

Mzr)(cid:9) > 0.

C + dmax

∈ MH

−

A

(cid:101)

−

−

−

(cid:100)

(cid:101)

Mzr)(cid:9).
(cid:101)
(cid:100)
, α(

Theorem 23. From [24, Theorem 3.2.4], consider

˙x = Ax

CΦ(x),

−

0. If A

with diagonal C
with corre-
−
sponding dominant left eigenvector w∗∗, then this model is
strongly inﬁnitesimally contracting with respect to
(cid:107) · (cid:107)1,[w∗∗]
with rate

∈ MH

dminC) > 0.

dminC

α(

(cid:23)

A

−

(cid:100)

Mzr
(cid:101)

−

Proof. We compute the one-sided Lipschitz constant of
CΦ(x) with respect to norm
f (x) := Ax

(cid:107) · (cid:107)1,[w∗∗].

−

osL1,[w∗∗](f ) = sup

x∈Rn\Ωf

µ1,[w∗∗](Df (x))

= sup

µ1,[w∗∗](A

CDΦ(x))

=

x∈Rn\Ωf
max
d∈[dmin,dmax]n
= µ1,[w∗∗](A
A
= µ1,[w∗∗](
(cid:100)
= α(
Mzr
(cid:100)

A

−
µ1,[w∗∗](A

C[d])

−

−

dminC)
dminC
−
dminC).

Mzr)

(cid:101)

(30)

(31)

(32)

(33)

(34)

(35)

(cid:101)

−
where the equality in (32) is because the closure of the image
DΦ(x) is [dmin, dmax]n, equality (33) is
of the map x
0, equality (34) is by Theorem 8(ii), and (35) is
because C
by Lemma 1. Moreover, since A
dminC
−
dminC) < 0 so f is strongly inﬁnitesimally contracting with
respect to

, α(
(cid:100)

dminC) > 0.

∈ MH

(cid:55)→

Mzr

Mzr

(cid:23)

−

A

A

(cid:101)

(cid:107)·(cid:107)1,[w∗∗] with rate

α(
(cid:100)

−

−

(cid:101)

Theorem 24. From [24, Theorem 3.2.10], consider

˙xi =

(cid:88)n

j=1

aijφij(xj)

for each i
}
slope-restricted in [dmin, dmax]. If dmin > 0 and

1, . . . , n

and with each φij Lipschitz and

∈ {

B := dmaxA

(dmax

dmin)(In

A)

◦

−

−
with corresponding dominant
left and right eigenvectors
wB, vB, respectively, then this model is strongly inﬁnitesi-
mally contracting with rate
Mzr) > 0 with respect to
α(
B
−
(cid:100)
(cid:107) · (cid:107)∞,[vB ]−1.
(cid:107) · (cid:107)1,[wB ] and
both

∈ MH

(cid:101)

,

Proof. First note that the assumption B
aii < 0 for every i
of B are dminaii and a necessary condition for B

implies that
since the diagonal elements
is

1, . . . , n

∈ MH

∈ {

}

∈ MH

Bii < 0 since
given by fi(x) = (cid:80)n

MH ⊂ T H

j=1 aijφij(xj). We compute

. Let f denote the vector ﬁeld

(Df (x))ij =

∂
∂xj

n
(cid:88)

j=1

aijφij(xj) = aijφ(cid:48)

ij(xj)

Rn. In other words, Df (x) = A

for almost every x
◦
Rn, where (DΦ(x))ij =
DΦ(x), for almost every x
∈
φ(cid:48)
ij(xj). We now proceed to elementwise upper bound
Df (x)
(cid:101)

Mzr. Observe that for every i

1, . . . , n

= j

∈ {

∈

(cid:100)

,

(
(cid:100)
(
(cid:100)

Df (x)

(cid:101)
Df (x)

aijφ(cid:48)
Mzr)ij =
Mzr)ii = aiiφ(cid:48)
(cid:101)

|

ij(xj)
ii(xi)

≤

| ≤

dmax

aij
|
|
dminaii = (
(cid:100)

= (
B
(cid:100)
(cid:101)
Mzr)ii,

B

(cid:101)

}
Mzr)ij,

1, . . . , n
A

where the second inequality holds because aii < 0 for every
i
. Now observe that for any matrix A
∈ {
}
Rn×n, if
A(cid:48) elementwise, then both µ1,[η](A)
Mzr
(cid:101)
µ1,[η](A(cid:48)) and µ∞,[η]−1 (A)
η

∈
≤
µ∞,[η]−1(A(cid:48)) hold for any

>0. Then we can observe that

Rn

≤

≤

(cid:100)

∈

osL1,[wB ](f ) = sup

x∈Rn\Ωf

µ1,[wB ](Df (x))

= sup

x∈Rn\Ωf
sup
≤
x∈Rn\Ωf
= µ1,[wB ](
(cid:100)

µ1,[wB ](
(cid:100)
µ1,[wB ](
(cid:100)

Df (x)
(cid:101)

Mzr)

B

Mzr)
(cid:101)

B

Mzr) = α(
(cid:100)

(cid:101)

B

Mzr),
(cid:101)

B

where the ﬁnal equality holds by Lemma 1. An analogous
computation shows that osL∞,[vB ]−1(f )
Mzr). As
α(
(cid:100)
a consequence, since B
, this model is strongly
inﬁnitesimally contracting with respect to both
(cid:107) · (cid:107)1,[wB ]
and

(cid:107) · (cid:107)∞,[vB ]−1 with rate
The next two theorems serve as non-Euclidean versions of
early results on contractivity of Lur’e systems (in application
to the entrainment problem) established ﬁrst in [44].

∈ MH

α(
(cid:100)

Mzr).

≤

−

B

(cid:101)

(cid:101)

Theorem 25 (Contractivity of Lur’e system). From [24,
Theorem 3.2.7], consider the Lur’e system

˙x = Ax + vφ(y),
y = w(cid:62)x,

Rn×n, v, w

R is Lipschitz
where A
and slope-restricted in [dmin, dmax]. Consider the following
two optimization problems:

Rn and φ : R

→

∈

∈

min
b∈R,η∈[ε,∞[n

b

s.t.

A + dminvw(cid:62)
(cid:101)
A + dmaxvw(cid:62)

(cid:62)
Mzr η
(cid:62)
Mzr η
(cid:101)

(cid:100)

(cid:100)

bη,

bη,

≤

≤

(36)

and

min
c∈R,ξ∈[ε,∞[n

c

s.t.

A + dminvw(cid:62)
Mzr ξ
(cid:101)
A + dmaxvw(cid:62)
Mzr ξ
Let b(cid:63), η(cid:63) be optimal parameters for (36) and c(cid:63), ξ(cid:63) be
optimal parameters for (37). Then

cξ,

cξ.

≤

≤

(cid:100)

(cid:101)

(cid:100)

(37)

(cid:54)
(i) if b(cid:63) < 0, then the closed-loop dynamics are strongly
with respect to

b(cid:63)

(ii) if c(cid:63) < 0, then the closed-loop dynamics are strongly
with respect to

inﬁnitesimally contracting with rate
(cid:107) · (cid:107)1,[η(cid:63)].
inﬁnitesimally contracting with rate
(cid:107) · (cid:107)∞,[ξ(cid:63)]−1 .

|

|

c(cid:63)
|

|

Proof. Let fL(x) := Ax+vφ(w(cid:62)x). Regarding statement (i),
computing the one-sided Lipschitz constant of fL with re-
spect to

Rn

>0 yields

(cid:107) · (cid:107)1,[η] for arbitrary η

∈

osL1,[η](fL) = sup

x∈Rn\ΩfL

µ1,[η](DfL(x))

= sup

x∈Rn\ΩfL

µ1,[η](A + vφ(cid:48)(w(cid:62)x)w(cid:62))

= max

d∈[dmin,dmax]

µ1,[η](A + d vw(cid:62))

µ1,[η](A + dminvw(cid:62)), µ1,[η](A + dmaxvw(cid:62))
= max
{

(38)

(39)

(40)

,
}
(41)

where the equality in (40) holds since the closure of the
φ(cid:48)(w(cid:62)x) is [dmin, dmax] by the
image of the map x
slope-restricted assumption on φ and the equality (41) holds
because the maximum of a convex function (µ in this case)
over a compact interval occurs at one of the endpoints of the
interval. As a consequence, osL1,[η](fL) < 0 if and only if

(cid:55)→

inf
η∈Rn

>0

max
{

µ1,[η](A+dminvw(cid:62)), µ1,[η](A+dmaxvw(cid:62))

< 0.

}

Therefore, if b(cid:63), η(cid:63) are optimal parameters for problem (36),
then µ1,[η(cid:63)](A + dminvw(cid:62))
≤
b(cid:63). Therefore, if b(cid:63) < 0, we conclude that the Lur’e system is
strongly inﬁnitesimally contracting with respect to
(cid:107) · (cid:107)1,[η(cid:63)]
. The proof of statement (ii) is essentially
with rate
|
identical, replacing

b(cid:63) and µ1,[η](A + dmaxvw(cid:62))

b(cid:63)
|

≤

(cid:107) · (cid:107)1,[η(cid:63)] with

(cid:107) · (cid:107)∞,[ξ(cid:63)]−1 .

to both

Mzr) > 0 with respect

(cid:107) · (cid:107)1,[wF ] and

α(
F
−
(cid:100)
(cid:101)
(cid:107) · (cid:107)∞,[vF ]−1 .
Proof. Let fML(x) = Ax + BΦ(Cx) and note DfML(x) =
[dmin, dmax]n. Also note
A + B[d]C for some d
(B[d]C)ij = (cid:80)m
k=1 BikdkCkj. The proof follows from
noting that the matrix F is an entry-wise upper bound on
the Metzler majorant of DfML(x), for all x, in analogy with
the proof of Theorem 24.

∈

D. Remarks on absolute, connective, and total contractivity

In this section we clarify that our results in Theorems 18–
26 indeed establish absolute, connective, and total contrac-
tion, in the following senses.

First, in the spirit of the classic work on absolute stabil-
ity [44], [15], by absolutely contracting we mean dynamical
systems that are contracting for all choices of activation
functions in a given class. (The class of activation function
in this paper is all weakly increasing Lipschitz functions.)

Second, in the spirit of the classic work on connective sta-
bility [39], by connectively contracting we mean dynamical
networks that remain contracting under the removal of any
possible subset of edges (other than self-loops). It is easy
to see that the action of removing any edge from a synaptic
matrix leads to an equal or larger contraction rate.

Third and ﬁnal, if each component of the state x cor-
responds to a single neuron, the removal of some neurons
corresponds to pruning the neural network. By Corollary 11,
, then any principal submatrix of A is also in
if A
∈ MH
. In other words, if any neurons are removed from the
MH
neural network, the resulting neural network is guaranteed to
remain contracting with an equal or larger contraction rate.
We refer to this property as total contraction, because of the
analogy with the property of totally Hurwitz matrices.

Theorem 26 (Multivariable Lur’e system). Consider the
multivariable Lur’e system

VI. DISCUSSION

˙x = Ax + BΦ(y),

y = Cx,

∈

Rn×n, B

Rn×m, C

where A
diagonal and is slope-restricted in [dmin, dmax] with dmin
)+ and (
Deﬁne (
·
·
. Deﬁne F
x, 0
min
}
{

)− by (x)+ = max
x, 0
}
{

Rn×n componentwise by

Rm×n, and Φ is
0.
and (x)− =

≥

∈

∈

∈

m
(cid:88)

m
(cid:88)

Fii = Aii + dmax

(BijCji)+ + dmin

(BijCji)−,

j=1

j=1

Fij =

Aij
|

|

(cid:40)

+ max

dmax

m
(cid:88)

(BikCkj)+ + dmin

m
(cid:88)

(BikCkj)−,

k=1

k=1

dmin

−

m
(cid:88)

(BikCkj)+

k=1

dmax

−

m
(cid:88)

k=1

(cid:41)

(BikCkj)−

,

= j. Then, if F

for i
with corresponding dom-
∈ MH
inant left and right eigenvectors wF , vF , the closed-loop
dynamics are strongly inﬁnitesimally contracting with rate

In this paper, we present novel non-Euclidean log norm
results and a non-smooth contraction theory simpliﬁcation
and we apply these results to study the contractivity of RNN
models, primarily focusing on the Hopﬁeld and ﬁring-rate
models. We provide efﬁcient algorithms for computing the
optimal non-Euclidean contraction rate and corresponding
norm. Our approach is robust with respect
to activation
function and additional unmodeled dynamics and, more
generally, establishes the strong contractivity property which,
in turn, implies strong robustness properties.

As a ﬁrst direction of future research, we plan to in-
vestigate contractivity under conditions such as Lyapunov
diagonal stability (LDS) of the synaptic matrix. LDS is
known to imply asymptotic stability of Hopﬁeld neural
networks, however, it is not known to imply contractivity
(with respect to a constant norm). More broadly, we believe
that our non-Euclidean contraction framework for RNNs
serves as a ﬁrst step to analyzing robustness and convergence
properties of other classes of neural circuits including central
pattern generators and other machine learning architectures
including modern Hopﬁeld networks [26].

(cid:54)
REFERENCES

[1] Z. Aminzare and E. D. Sontag. Contraction methods for nonlinear
systems: A brief introduction and some open problems. In IEEE Conf.
on Decision and Control, pages 3835–3847, December 2014. doi:
10.1109/CDC.2014.7039986.

[2] Z. Aminzare and E. D. Sontag.

Synchronization of diffusively-
connected nonlinear systems: Results based on contractions with
IEEE Transactions on Network Science
respect to general norms.
and Engineering, 1(2):91–106, 2014. doi:10.1109/TNSE.2015.
2395075.

[3] S. Arik. A note on the global stability of dynamical neural net-
IEEE Transactions on Circuits and Systems I: Fundamental
works.
Theory and Applications, 49(4):502–504, 2002. doi:10.1109/81.
995665.

[4] S. Bai, J. Z. Kolter, and V. Koltun. Deep equilibrium models.

In
Advances in Neural Information Processing Systems, 2019. URL:
https://arxiv.org/abs/1909.01377.

[5] G. P. Barker, A. Berman, and R. J. Plemmons. Positive diagonal
solutions to the Lyapunov equations. Linear and Multilinear Algebra,
5(4):249–256, 1978. doi:10.1080/03081087808817203.
[6] F. Bullo. Lectures on Network Systems. Kindle Direct Publishing,
1.6 edition, January 2022, ISBN 978-1986425643. URL: http://
motion.me.ucsb.edu/book-lns.

[7] F. Bullo, P. Cisneros-Velarde, A. Davydov, and S. Jafarpour. From
contraction theory to ﬁxed point algorithms on Riemannian and non-
Euclidean spaces. In IEEE Conf. on Decision and Control, December
doi:10.1109/CDC45484.
2021.
2021.9682883.

(Invited Tutorial Session).

[8] F. H. Clarke. Optimization and Nonsmooth Analysis. Canadian
Mathematical Society Series of Monographs and Advanced Texts. John
Wiley & Sons, 1983, ISBN 047187504X.

[9] S. Coogan. A contractive approach to separable Lyapunov functions
for monotone systems. Automatica, 106:349–357, 2019. doi:10.
1016/j.automatica.2019.05.001.

[10] A. Davydov, S. Jafarpour, and F. Bullo. Non-Euclidean contraction
theory for robust nonlinear stability. IEEE Transactions on Automatic
Control, July 2021. Submitted. URL: https://arxiv.org/abs/
2103.12263.

[11] C. A. Desoer and H. Haneda. The measure of a matrix as a tool to
analyze computer algorithms for circuit analysis. IEEE Transactions
on Circuit Theory, 19(5):480–486, 1972. doi:10.1109/TCT.
1972.1083507.

[12] M.J. Fabi´an and D. Preiss. On the Clarke’s generalized Jacobian. In
Z. Frol´ık, Souc¸ek, and M.J. Fabi´an, editors, 14th Winter School on
Abstract Analysis, pages 305–307, 1987. URL: https://eudml.
org/doc/221686.

[13] Y. Fang and T. G. Kincaid. Stability analysis of dynamical neural
IEEE Transactions on Neural Networks, 7(4):996–1006,

networks.
1996. doi:10.1109/72.508941.

[14] M. Fazlyab, M. Morari, and G. J. Pappas. Safety veriﬁcation and
robustness analysis of neural networks via quadratic constraints and
semideﬁnite programming. IEEE Transactions on Automatic Control,
2020. doi:10.1109/TAC.2020.3046193.

[15] M. Forti, S. Manetti, and M. Marini. Necessary and sufﬁcient
condition for absolute stability of neural networks. IEEE Transactions
on Circuits and Systems I: Fundamental Theory and Applications,
41(7):491–494, 1994. doi:10.1109/81.298364.

[16] M. Forti and A. Tesi. New conditions for global stability of neural
networks with application to linear and quadratic programming prob-
IEEE Transactions on Circuits and Systems I: Fundamental
lems.
Theory and Applications, 42(7):354–366, 1995. doi:10.1109/81.
401145.

[17] W. He and J. Cao. Exponential synchronization of chaotic neural
networks: a matrix measure approach. Nonlinear Dynamics, 55:55–
65, 2009. doi:10.1007/s11071-008-9344-4.

[18] J. J. Hopﬁeld. Neurons with graded response have collective com-
putational properties like those of two-state neurons. Proceedings of
the National Academy of Sciences, 81(10):3088–3092, 1984. doi:
10.1073/pnas.81.10.3088.

[19] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University

Press, 2nd edition, 2012, ISBN 0521548233.

[20] S. Jafarpour, A. Davydov, and F. Bullo. Non-Euclidean contraction
IEEE Transactions on
theory for monotone and positive systems.
Automatic Control, September 2021. Submitted. URL: https://
arxiv.org/abs/2104.01321.

[21] S. Jafarpour, A. Davydov, A. V. Proskurnikov, and F. Bullo. Robust
In Advances in
implicit networks via non-Euclidean contractions.
Neural Information Processing Systems, December 2021. URL:
http://arxiv.org/abs/2106.03194.

[22] A. Kag, Z. Zhang, and V. Saligrama. RNNs incrementally evolving on
an equilibrium manifold: A panacea for vanishing and exploding gradi-
ents? In International Conference on Learning Representations, 2020.
URL: https://openreview.net/forum?id=HylpqA4FwS.
[23] E. Kaszkurewicz and A. Bhaya. On a class of globally stable neural
circuits. IEEE Transactions on Circuits and Systems I: Fundamental
Theory and Applications, 41(2):171–174, 1994. doi:10.1109/81.
269055.

[24] E. Kaszkurewicz and A. Bhaya. Matrix Diagonal Stability in Systems

and Computation. Springer, 2000, ISBN 978-0-8176-4088-0.

[25] L. Kozachkov, M. Ennis, and J.-J. E Slotine. Recursive construction of
stable assemblies of recurrent neural networks, 2021. URL: https:
//arxiv.org/abs/2106.08928.

[26] D. Krotov and J. J. Hopﬁeld. Large associative memory problem in
neurobiology and machine learning. In International Conference on
Learning Representations, 2021. URL: https://openreview.
net/forum?id=X4y_10OX-hX.

[27] W. Lohmiller and J.-J. E. Slotine. On contraction analysis for non-
linear systems. Automatica, 34(6):683–696, 1998. doi:10.1016/
S0005-1098(98)00019-3.

[28] A. N. Michel, J. A. Farrell, and W. Porod. Qualitative analysis
IEEE Transactions on Circuits and Systems,

of neural networks.
36(2):229–243, 1989. doi:10.1109/31.20200.

[29] K. D. Miller and F. Fumarola. Mathematical equivalence of two
common forms of ﬁring rate models of neural networks. Neural
Computation, 24(1):25–31, 2012. doi:10.1162/NECO_a_00221.
Linear
Algebra and its Applications, 17(1):53–58, 1977. doi:10.1016/
0024-3795(77)90040-4.

[30] P. J. Moylan. Matrices with positive principal minors.

[31] E. Nozari and J. Cort´es. Hierarchical selective recruitment in linear-
threshold brain networks—part I: Single-layer dynamics and selective
inhibition. IEEE Transactions on Automatic Control, 66(3):949–964,
2021. doi:10.1109/TAC.2020.3004801.

[32] O. Pastravanu and M. Voicu. Generalized matrix diagonal stability
and linear dynamical systems. Linear Algebra and its Applications,
419(2):299–310, 2006. doi:10.1016/j.laa.2006.04.021.

[33] A. Pavlov and N. Van de Wouw. Convergent systems: nonlinear
simplicity. In N. van de Wouw, E. Lefeber, and A. I. Lopez, editors,
Nonlinear Systems, pages 51–77. Springer, 2017. doi:10.1007/
978-3-319-30357-4_3.

[34] H. Qiao, J. Peng, and Z.-B. Xu. Nonlinear measures: A new approach
to exponential stability analysis for Hopﬁeld-type neural networks.
IEEE Transactions on Neural Networks, 12(2):360–370, 2001. doi:
10.1109/72.914530.

[35] M. Revay, R. Wang, and I. R. Manchester.

Lipschitz bounded
equilibrium networks. 2020. URL: https://arxiv.org/abs/
2010.01732.

[36] M. Revay, R. Wang, and I. R. Manchester. A convex parameterization
IEEE Control Systems Letters,
of robust recurrent neural networks.
5(4):1363–1368, 2021. doi:10.1109/LCSYS.2020.3038221.
[37] G. Russo, M. Di Bernardo, and E. D. Sontag. Global entrainment
of transcriptional systems to periodic inputs. PLoS Computational
Biology, 6(4):e1000739, 2010. doi:10.1371/journal.pcbi.
1000739.

[38] E. K. Ryu and W. Yin. Large-Scale Convex Optimization via Monotone

Operators. Cambridge, 2022.

[39] D. D. ˇSiljak. Large-Scale Dynamic Systems Stability & Structure.

North-Holland, 1978, ISBN 0486462854.

[40] G. S¨oderlind. The logarithmic norm. History and modern theory.
BIT Numerical Mathematics, 46(3):631–652, 2006. doi:10.1007/
s10543-006-0069-9.

[41] J. Stoer and C. Witzgall. Transformations by diagonal matrices in
a normed space. Numerische Mathematik, 4:158–171, 1962. doi:
10.1007/BF01386309.

[42] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Good-
fellow, and R. Fergus.
In
International Conference on Learning Representations, 2014. URL:
https://arxiv.org/abs/1312.6199.

Intriguing properties of neural networks.

[43] H. Tsukamoto, S.-J. Chung, and J.-J. E Slotine. Contraction theory
for nonlinear stability analysis and learning-based control: A tutorial

overview. Annual Reviews in Control, 52:135–169, 2021. doi:10.
1016/j.arcontrol.2021.10.001.

[44] V. A. Yakubovich. Method of matrix inequalities in theory of nonlinear
control systems stability. I. Forced oscillations absolute stability.
Avtomatika i Telemekhanika, 25(7):1017–1029, 1964.
(In Russian).
URL: http://mi.mathnet.ru/eng/at11685.

[45] H. Zhang, Z. Wang, and D. Liu. A comprehensive review of
stability analysis of continuous-time recurrent neural networks. IEEE
Transactions on Neural Networks and Learning Systems, 25(7):1229–
1262, 2014. doi:10.1109/TNNLS.2014.2317880.

