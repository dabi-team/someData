1
2
0
2
c
e
D
0
1

]
I

A
.
s
c
[

1
v
1
4
8
5
0
.
2
1
1
2
:
v
i
X
r
a

Logical Boltzmann Machines

Son N. Tran1 and Artur d’Avila Garcez2

1University of Tasmania (sn.tran@utas.edu.au)
2City, University of London (a.garcez@city.ac.uk)

Abstract

The idea of representing symbolic knowledge in connectionist systems has
been a long-standing endeavour which has attracted much attention recently with
the objective of combining machine learning and scalable sound reasoning. Early
work has shown a correspondence between propositional logic and symmetrical
neural networks which nevertheless did not scale well with the number of vari-
ables and whose training regime was inefﬁcient. In this paper, we introduce Log-
ical Boltzmann Machines (LBM), a neurosymbolic system that can represent any
propositional logic formula in strict disjunctive normal form. We prove equiva-
lence between energy minimization in LBM and logical satisﬁability thus show-
ing that LBM is capable of sound reasoning. We evaluate reasoning empirically
to show that LBM is capable of ﬁnding all satisfying assignments of a class of
logical formulae by searching fewer than 0.75% of the possible (approximately
1 billion) assignments. We compare learning in LBM with a symbolic inductive
logic programming system, a state-of-the-art neurosymbolic system and a purely
neural network-based system, achieving better learning performance in ﬁve out of
seven data sets.

1 Introduction

The research community has been witnessing an increasing attention devoted to the
integration of learning and reasoning, especially through the combination of neural
networks and symbolic AI [10, 3] into neurosymbolic systems. At the core of a neu-
rosymbolic system there is an algorithm to represent symbolic knowledge in a neural
network. One of the goals is to leverage the parallel and distributed properties of the
network to perform reasoning. In many neurosymbolic approaches, the most used form
of knowledge representation is if-then rules whereby logical reasoning is built upon
Modus-Ponens as the only rule of inference [18, 5, 19, 4, 20, 9]. Given a formula
B ← A (read “B if A” following a logic programming notation) a neural network
would either infer approximately B = 1 (True) if A = 1 by forward chaining, or
search for the value of A to conﬁrm or refute the hypothesis B = 1 (backward chain-
ing). This has two shortcomings. First, Modus-Ponens alone may not capture entirely
the power of logical reasoning as required by an application. For example, it may be

1

 
 
 
 
 
 
the case in an application that if B = 0 (False), the neural network is expected to infer
approximately A = 0 (Modus-Tollens). Second, one may wish to allow other forms of
rules to be represented by the neural network such as disjunctive normal form (DNF)
with any number of negative literals.

In this paper, we introduce Logical Boltzmann Machines, a neurosymbolic system
that can represent any propositional logic formula in a neural network and achieve ef-
ﬁcient reasoning using restricted Boltzmann machines. We introduce an algorithm to
translate any logical formula described in DNF into a Boltzmann machine and we show
equivalence between the logical formula and the energy-based connectionist model. In
other words, we show soundness of the translation algorithm. Speciﬁcally, the con-
nectionist model will assign minimum energy to the assignments of truth-values that
satisfy the formula. This produces a new way of performing reasoning in neural net-
works by employing the neural network to search for the models of the logical formula,
that is, to search for assignments of truth-values that map the logical formula to true.
We show that Gibbs sampling can be applied efﬁciently for this search with a large
number of variables in the logical formula. If the number of variable is small, inference
can be carried out analytically by sorting the free-energy of all possible truth-value
assignments. Returning to our example with formula B ← A, Logical Boltzmann Ma-
chines can infer approximately B = 1 given A = 1, and it can infer approximately
A = 0 given B = 0 since both truth-value assignments - (A = T rue, B = T rue) and
(A = F alse, B = F alse) - would minimise the energy of the network.

In what concerns the representational issue of rules other than if-then rules, we
propose a new way of converting any logical formula into strict DNF (SDNF) which
is shown to map conveniently onto Restricted Boltzmann Machines (RBMs). In the
experiments reported in this paper, this new mapping into SDNF and RBMs is shown
to enable approximate reasoning with a large number of variables. The proposed ap-
proach is evaluated in a logic programming benchmark task whereby machine learn-
ing models are trained from data and background knowledge. Logical Boltzmann Ma-
chines achieved a better training performance (higher test set accuracy) in ﬁve out of
seven data sets when evaluated empirically on this benchmark in comparison with a
purely-symbolic learning system (Inductive Logic Programming system Aleph [16]),
a neurosymbolic system for Inductive Logic Programming (CILP++ [5]) and a purely-
connectionist system (standard RBMs [15]).
The contribution of this work is twofold:

• A theoretical proof to equivalently map logical formulas and probabilistic neural
networks, namely Restricted Boltzmann Machines, which can facilitate neuro-
symbolic learning and reasoning.

• A foundation for the employment of statistical inference methods to perform

logical reasoning.

The remainder of the paper is organised as follows. In the next section, we review
the related work. Section 3 describes and proves correctness of the mapping from any
logical formula into SDNF and then RBMs. Section 4 deﬁnes reasoning by sampling
and energy minimization in RBMs. Section 5 introduces the LBM system and evaluates
scalability of reasoning with an increasing number of variables. Section 6 contains the

2

experimental results on learning and the comparison with a symbolic, neurosymbolic
and a purely-neural learning system. We then conclude the paper and discuss directions
for future work.

2 Related Work

One of the earliest work on the integration of neural networks and symbolic knowl-
edge is Knowledge-based Artiﬁcial Neural Network [18] which encodes if-then rules
into a hierarchy of perceptrons. In another early approach [6], a single-hidden layer
neural network with recurrent connections is proposed to support logic programming
rules. An extension of that approach to work with ﬁrst-order logic programs, called
CILP++ [5], uses the concept of propositionalisation from Inductive Logic Program-
ming (ILP) whereby ﬁrst-order variables can be treated as propositional atoms in the
neural network. Also based on ﬁrst-order logic programs, [4] propose a differentiable
ILP approach that can be implemented by neural networks, while [2] maps stochastic
logic programs into a differentiable function also trainable by neural networks. These
are all supervised learning approaches.

Among unsupervised learning approaches, Penalty Logic [12] was the ﬁrst work
to integrate propositional and non-monotonic logic formulae into symmetric networks.
However, it required the use of higher-order Hopﬁeld networks which can become
complicated to construct and inefﬁcient to train with the learning algorithm of Boltz-
mann machines (BM). Such higher-order networks require transforming the energy
function into a quadratic form by adding hidden variables not present in the original
logic formulae for the purpose of building the network. More recently, several attempts
have been made to extract and encode symbolic knowledge into RBMs [11, 19]. These
are based on the structural similarity between symmetric networks and bi-conditional
logical statements and do not contain soundness results. By contrast, and similarly to
Penalty Logic, the approach introduced in this paper is based on a proof of equiva-
lence between the logic formulae and the symmetric networks, but without requiring
higher-order networks.

Alongside the above approaches which translate from a symbolic to a neural rep-
resentation, normally from if-then rules to a feedforward or recurrent neural network,
there are also hybrid approaches, which combine neural networks with symbolic AI
systems and logic operators, such as DeepProbLog [9] and Logic Tensor Networks
(LTN) [14]. While DeepProbLog adds a neural network module to probabilistic logic
programming, LTN represents the level of truth of ﬁrst-order logic statements in the
neural network. LTN employs a discriminative approach to infer the level of truth rather
than the generative approach adopted in this paper by LBM. Although a discriminative
approach can be very useful for evaluating assignments of variables to truth-values,
we argue that it is not adequate for the implementation of a search for the satisfying
assignments of logical formulae. For this purpose, the use of a generative approach is
needed as proposed in this paper with LBM.

3

3 Knowledge Representation in RBMs

An RBM [15] can be seen as a two-layer neural network with bidirectional (symmetric)
connections, which is characterised by a function called the energy of the RBM:

E (x, h) = −

(cid:88)

i,j

wijxihj −

(cid:88)

aixi −

(cid:88)

bjhj

i

j

(1)

where ai and bj are the biases of input unit xi and hidden unit hj, respectively, and wij
is the connection weight between xi and hj. This RBM represents a joint distribution
xh e− 1
p(x, h) = 1
τ E (x,h) is the partition function and pa-
rameter T is called the temperature of the RBM, x = {xi} is the set of visible units
and h = {hj} is the set of hidden units in the RBM.

τ E (x,h) where Z = (cid:80)

Z e− 1

In propositional logic, any well-formed formula (WFF) ϕ can be mapped into Dis-

junctive Normal Form (DNF), i.e. disjunctions (∨) of conjunctions (∧), as follows:

ϕ ≡

(cid:95)

j

(cid:94)

(
t∈STj

(cid:94)

xt ∧

¬xk)

k∈SKj

t∈STj

k∈SKj

xt ∧ (cid:86)

where ((cid:86)
¬xk) is called a conjunctive clause, e.g. x1 ∧ x2 ∧ ¬x3.
Here, we denote the propositional variables (literals) as xt for positive literals (e.g.
x1), xk for negative literals (e.g. ¬x3); STj and SKj denote, respectively, the sets of
Tj indices of the positive literals and Kj indices of the negative literals in the formula.
This notation may seem over-complicated but it will be useful in the proof of soundness
of our translation from SDNF to RBMs.

Deﬁnition 1 Let sϕ(x) ∈ {0, 1} denote the truth-value of a WFF ϕ given an assign-
ment of truth-values x to the literals of ϕ with truth-value T rue mapped to 1 and
truth-value F alse mapped to 0. Let E (x, h) denote the energy function of an energy-
based neural network N with visible units x and hidden units h. ϕ is said to be equiv-
alent to N if and only if for any assignment x there exists a function ψ such that
sϕ(x) = ψ(E (x, h)).

This deﬁnition of equivalence is similar to that of Penalty Logic [12], whereby all
assignments of truth-values satisfying a WFF ϕ are mapped to global minima of the
energy function of network N . In our case, by construction, assignments that do not
satisfy the WFF are mapped to maxima of the energy function.

Deﬁnition 2 A strict DNF (SDNF) is a DNF with at most one conjunctive clause that
maps to T rue for any assignment of truth-values x. A full DNF is a DNF where each
propositional variable must appear at least once in every conjunctive clause.

Lemma 1 Any SDNF ϕ ≡ (cid:87)
energy function:

j((cid:86)

xt ∧ (cid:86)

k∈SKj

t∈STj

¬xk) can be mapped onto an

E (x) = −

(cid:88)

j

(cid:89)

(
t∈STj

xt

(cid:89)

k∈SKj

(1 − xk))

4

(cid:81)

xt

t∈STj

xt ∧ (cid:86)

where STj (resp. SKj ) is the set of Tj (resp. Kj) indices of the positive (resp. negative)
literals in ϕ.
Proof. Each conjunctive clause (cid:86)
(cid:81)

¬xk in ϕ can be represented by
(1−xk) which maps to 1 if and only if xt = 1 (i.e. T rue) and xk =
0 (i.e. F alse) for all t ∈ STj and k ∈ SKj . Since ϕ is a SDNF, it is T rue if and only if
one conjunctive clause is T rue. Then, the sum (cid:80)
(1 − xk)) = 1
if and only if the assignment of truth-values to xt, xk is a model of ϕ. Hence, the neural
j((cid:81)
network with energy function E = − (cid:80)
(1 − xk)) is such that
sϕ(x) = −E (x).

j((cid:81)

k∈SKj

k∈SKj

k∈SKj

k∈SKj

t∈STj

t∈STj

t∈STj

xt

xt

(cid:81)

(cid:81)

z

x

y
F alse F alse F alse
T rue
F alse F alse
F alse
T rue
F alse
T rue
F alse
T rue
F alse F alse
T rue
T rue
F alse
T rue
F alse
T rue
T rue
T rue
T rue
T rue

sϕ(x, y, z) E (x, y, z)

1
0
0
1
0
1
1
0

−1
0
0
−1
0
−1
−1
0

Table 1: Energy function and truth-table for the formula ((x ∧ ¬y) ∨ (¬x ∧ y)) ↔ z;
we use the symbol ⊕ to denote exclusive-or, that is x ⊕ y ≡ ((x ∧ ¬y) ∨ (¬x ∧ y)).

Example 1 The formula ϕ ≡ (x ⊕ y) ↔ z, c.f. Table 1, can be converted into a SDNF
as follows:

ϕ ≡ (¬x ∧ ¬y ∧ ¬z) ∨ (¬x ∧ y ∧ z) ∨ (x ∧ ¬y ∧ z) ∨ (x ∧ y ∧ ¬z)

For each conjunctive clause in ϕ, a corresponding expression is added to the energy
function, e.g. xy(1 − z) corresponding to clause x ∧ y ∧ ¬z. Hence, the energy function
for N equivalent to ϕ becomes:

E = − (1 − x)(1 − y)(1 − z) − xy(1 − z)− x(1 − y)z − (1 − x)yz

We now show that any SDNF can be mapped onto an RBM.
j((cid:86)
Theorem 1 Any SDNF ϕ ≡ (cid:87)
equivalent RBM with energy function

xt ∧ (cid:86)

k∈SKj

t∈STj

¬xk) can be mapped onto an

E (x, h) = −

(cid:88)

hj(

(cid:88)

xt −

(cid:88)

xk − |STj | + (cid:15))

(2)

j

t∈STj

k∈SKj

where 0 < (cid:15) < 1, STj and SKj are, respectively, the sets of indices of the positive and
negative literals in each conjunctive clause j of the SDNF, and |STj | is the number of
positive literals in conjunctive clause j.

5

(cid:81)

(cid:81)

(cid:81)

xt

t∈STj

t∈STj

k∈SKj

Proof. We have seen in Lemma 1 that any SDNF ϕ can be mapped onto energy
function E = − (cid:80)
(1 − xk). For each expression ˜ej(x) =
j
− (cid:81)
(1 − xk), we deﬁne an energy expression associated with hid-
xt
k∈SKj
den unit hj as ej(x, hj) = −hj((cid:80)
xk − |STj | + (cid:15)). ej(x, hj) is
minimized with value −(cid:15) when hj = 1, written minhj (ej(x, hj)) = −(cid:15). This is be-
cause −((cid:80)
xt − (cid:80)
xk − |STj | + (cid:15)) = −(cid:15) if and only if xt = 1 and xk = 0
for all t ∈ STj and k ∈ SKj . Otherwise, −((cid:80)
xk − |STj | + (cid:15)) > 0
and minhj (ej(x, hj)) = 0 with hj = 0. By repeating this process for each ˜ej(x) we
obtain that any SDNF ϕ is equivalent to an RBM with the energy function: E (x, h) =
− (cid:80)
(cid:15) minhE (x, h).

xk − |STj | + (cid:15)) such that sϕ(x) = − 1

j hj((cid:80)

xt − (cid:80)

xt − (cid:80)

t xt∈STj

− (cid:80)

k∈SKj

k∈SKj

k∈SKj

t∈STj

t∈STj

t∈STj

k∈SKj

Figure 1: An RBM equivalent to the XOR formula (x ⊕ y) ↔ z.

Applying Theorem 1, an RBM for the XOR formula ϕ ≡ (x ⊕ y) ↔ z can be built

as shown in Figure 1. We choose (cid:15) = 0.5. The energy function of this RBM is:

E = −h1(−x − y − z + 0.5) − h2(x + y − z − 1.5)−

h3(x + y − z − 1.5) − h4(−x + y + z − 1.5)

For comparison, one may construct an RBM for XOR using Penalty Logic, as fol-
low. First, we compute the higher-order energy function: E p = 4xyz − 2xy − 2xz −
2yz + x + y + z, then we transform it to quadratic form by adding a hidden variable h1
to obtain: E p = 2xy − 2xz − 2yz − 8xh1 − 8yh1 + 8zh1 + x + y + z + 12h1, which is
not an energy function of an RBM, so we keep adding hidden variables until the energy
function of an RBM might be obtained, in this case:E p = −8xh1 − 8yh1 + 8zh1 +
12h1 − 4xh2 + 4yh2 + 2h2 − 4yh3 − 4zh3 + 6h3 − 4xh4 − 4zh4 + 6h4 + 3x + y + z.
The above example illustrates in a simple case the value of using SDNF, in that it
produces a direct translation into efﬁcient RBMs by contrast with existing approaches.
Next, we discuss the challenges of the conversion of WFFs into SDNF.

Representation Capacity: It is well-known that any formula ϕ can be converted
into DNF. If ϕ is not SDNF then by deﬁnition there is a group of conjunctive clauses
in ϕ which map to T rue when ϕ is satisﬁed. This group of conjunctive clauses can
always be converted into a full DNF which is also a SDNF. Therefore, any WFF can
be converted into SDNF. From Theorem 1, it follows that any WFF can be represented
by the energy of an RBM. For example, a ∨ b becomes (a ∧ ¬b) ∨ (¬a ∧ b) ∨ (a ∧ b).

6

We now describe a method for converting logical formulae into SDNF, which we use
in the empirical evaluations that follow.

Let us consider a clause:

(cid:95)

γ ≡

¬xt ∨

(cid:95)

xk

t∈ST

k∈SK

(3)

which can be rearranged as γ ≡ γ(cid:48) ∨ x(cid:48), where γ(cid:48) is a disjunctive clause obtained by
removing x(cid:48) from γ. x(cid:48) can be either ¬xt or xk for any t ∈ ST and k ∈ SK. We have:

γ ≡ (¬γ(cid:48) ∧ x(cid:48)) ∨ γ(cid:48)

(4)

because (¬γ(cid:48) ∧ x(cid:48)) ∨ γ(cid:48) ≡ (γ(cid:48) ∨ ¬γ(cid:48)) ∧ (γ(cid:48) ∨ x(cid:48)) ≡ T rue ∧ (γ(cid:48) ∨ x(cid:48)). By De Morgan’s
law (¬(a ∨ b) ≡ ¬a ∧ ¬b), we can always convert ¬γ(cid:48) (and therefore ¬γ(cid:48) ∧ x(cid:48)) into a
conjunctive clause.

By applying (4) repeatedly, each time we can eliminate a variable out of a disjunc-
tive clause by moving it into a new conjunctive clause. The disjunctive clause γ holds
true if and only if either the disjunctive clause γ(cid:48) holds true or the conjunctive clause
(¬γ(cid:48) ∧ x(cid:48)) holds true.

As an example, consider the application of the transformation above to an if-then

rule (logical implication):

(cid:94)

y ← (

xt ∧

(cid:94)

¬xk)

t∈ST

k∈SK

The logical implication is converted to DNF:

(cid:94)

(y ∧

xt ∧

(cid:94)

¬xk) ∨ (

(cid:95)

¬xt ∨

(cid:95)

xk)

t∈ST

k∈SK

t∈ST

k∈SK

(5)

(6)

Applying the variable elimination method in (4) to all variables in the clause
(cid:87)

xk, we obtain the SDNF of the logical implication as:

¬xt ∨ (cid:87)

t∈ST

k∈SK

(y ∧

(cid:94)

xt

(cid:94)

¬xk) ∨

(cid:95)

(cid:94)

(

xt ∧

(cid:94)

¬xk ∧ x(cid:48)
p)

t∈ST

k∈SK

p∈ST ∪SK

t∈ST \p

k∈SK \p

(7)

where S.\p denotes a set S from which p has been removed. x(cid:48)
p ≡ ¬xp if p ∈ ST .
Otherwise, x(cid:48)
p ≡ xp. This SDNF only has |ST | + |SK| + 1 clauses, making translation
to an RBM very efﬁcient. For example, using this method, the SDNF of y ← (x1 ∧
x2 ∧ ¬x3) is (y ∧ x1 ∧ x2 ∧ ¬x3) ∨ (x1 ∧ x2 ∧ x3) ∨ (x1 ∧ ¬x2) ∨ ¬x1. We need an
RBM with only 4 hidden units to represent this SDNF.1

1Of course, the number of hidden units will grow exponentially with the number of disjuncts in y (typi-
cally not allowed in logic programming), e.g. if y ≡ (a∨b∨c) then the full DNF will have seven conjunctive
clauses.

7

4 Reasoning in RBMs

Reasoning as Sampling

There is a direct relationship between inference in RBMs and logical satisﬁability, as
follows.

Proposition 1 Let N be an RBM constructed from a formula ϕ. Let A be a set of
indices of variables that have been assigned to either True or False (we use xA to
denote the set {xα|α ∈ A}). Let B be a set of indices of variables that have not been
assigned a truth-value (we use xB to denote {xβ|β ∈ B}). Performing Gibbs sampling
on N given xA is equivalent to searching for an assignment of truth-values for xB that
satisﬁes ϕ.

Proof. Theorem 1 has shown that the truth-value of ϕ is inversely proportional to an
RBM’s rank function, that is:

sϕ(xB, xA) ∝ −minhE (xB, xA, h)

(8)

Therefore, a value of xB that minimises the energy function also maximises the truth
value, because:

x∗

B = arg min

(min
h

E (xB, xA, h))

xB
= arg min
xB
= arg max
xB

(−sϕ(xB, xA))

(sϕ(xB, xA))

(9)

Now, we can consider an iterative process to search for truth-values x∗

B by minimis-
ing an RBM’s energy function. This can be done by using gradient descent to update
the values of h and then xB one at a time (similarly to the contrastive divergence al-
gorithm) to minimise E (xB, xA, h) while keeping the other variables (xA) ﬁxed. The
alternating updates are repeated until convergence. Notice that the gradients amount to:

∂ − E (xB, xA, h)
∂hj

∂ − E (xB, xA, h)
∂xβ

=

=

(cid:88)

xiwij + bj

i∈A∪B
(cid:88)

hjwβj + aβ

j

(10)

In the case of Gibbs sampling, given the assigned variables xA, the process starts
with a random initialisation of xB, and proceeds to infer values for the hidden units
hj and then the unassigned variables xβ in the visible layer of the RBM, using the
conditional distributions hj ∼ p(hj|x) and xβ ∼ p(xβ|h), respectively, where x =
{xA, xB} and

p(hj|x) =

1 + e− 1

τ

p(xβ|h) =

1 + e− 1

τ

1
(cid:80)

i xiwij +bj

=

− 1
τ

1 + e

1
∂−E (xB ,xA ,h)
∂hj

1
(cid:80)
j hj wβj +aβ

=

1
∂−E (xB ,xA ,h)
∂xβ

− 1
τ

1 + e

(11)

8

It can be seen from (11) that the distributions are monotonic functions of the nega-
tive energy’s gradient over h and xB. Therefore, performing Gibbs sampling on those
functions can be seen as moving randomly towards a local point of minimum energy,
or equivalently to an assignment of truth-values that satisﬁes the formula. Since the
energy function of the RBM and the satisﬁability of the formula are inversely propor-
tional, each step of Gibbs sampling to reduce the energy should intuitively generate a
sample that is closer to satisfying the formula.

Reasoning as Lowering Free Energy

When the number of unassigned variables is not large such that the partition function
can be calculated directly, one can infer the assignments of xB using the conditional
distribution:

P (xB|xA) =

e−FB(xA,xB)
eFB (xA, x(cid:48)

x(cid:48)
B

B)

(cid:80)

(12)

j(− log(1+exp(c (cid:80)

where FB = − (cid:80)
i∈A∪B wijxi+bj))) is known as the free energy.
The free energy term − log(1 + exp(c (cid:80)
i∈A∪B wijxi + bj)) is a negative softplus
function scaled by a non-negative value c called conﬁdence value. It returns a negative
output for a positive input and a close-to-zero output for a negative input. One can
change the value of c to make the function smooth as shown in Figure 2.

Figure 2: Plots of free energy − log(1 + exp(cx)) for various conﬁdence values c.

Each free energy term is associated with a conjunctive clause in the SDNF through
the weighted sum (cid:80)
i∈A∪B wijxi + bj. Therefore, if a truth-value assignment of xB
does not satisfy the formula, all energy terms will be close to zero. Otherwise, one free
energy term will be − log(1 + exp(c(cid:15))), for a choice of 0 < (cid:15) < 1 obtained from
Theorem 1. Thus, the more likely a truth assignment is to satisfy the formula, the lower
the free energy.

5 Logical Boltzmann Machines

Based on the previous theoretical results, we are now in position to introduce Logical
Boltzmann Machines (LBM). LBM is a neurosymbolic system that uses Restricted

9

Boltzmann Machines for distributed reasoning and learning from data and knowledge.
The LBM system converts any set of formulae Φ = {ϕ1, ..., ϕn} into an RBM by
applying Theorem 1 to each formula ϕi ∈ Φ. In the case of Penalty Logic, formulae
are weighted. Given a set of weighted formulae Φ = {w1 : ϕ1, ..., wn : ϕn}, one
can also construct an equivalent RBM where each energy term generated from formula
ϕi is multiplied by wi. In both cases, the assignments that minimise the energy of the
RBM are the assignments that maximise the satiﬁability of Φ, i.e. the (weighted) sum
of the truth-values of the formula.

Proposition 2 Given a weighted knowledge-base Φ = {w1 : ϕ1, ..., wn : ϕn}, there
exists an equivalent RBM N such that sΦ(x) = − 1
(cid:15) minhE (x, h), where sΦ(x) is the
sum of the weights of the formulae in Φ that are satisﬁed by assignment x.

A formula ϕi can be decomposed into a set of (weighted) conjunctive clauses from
its SDNF. If there exist two conjunctive clauses such that one is subsumed by the other
then the subsumed clause is removed and the weight of the remaining clause is re-
placed by the sum of their weights. Identical conjunctive clauses are treated in the
same way: one of them is removed and the weights are added. From Theorem 1, we
know that a conjunctive clause (cid:86)
xt∧(cid:86)
¬xk is equivalent to an energy term
ej(x, hj) = −hj((cid:80)
xt − (cid:80)
xk − |STj | + (cid:15)) where 0 < (cid:15) < 1. A weighted
t∈STj
conjunctive clause w(cid:48) : (cid:86)
¬xk, therefore, is equivalent to an energy
term w(cid:48)ej(x, hj). For each weighted conjunctive clause, we can add a hidden unit j to
an RBM with connection weights wtj = w(cid:48) for all t ∈ STj and and wkj = −w(cid:48) for
all k ∈ SKj . The bias for this hidden unit will be w(cid:48)(−|STj | + (cid:15)). The weighted
knowledge-base and the RBM are equivalent because sΦ(x) ∝ − 1
(cid:15) minhE (x, h),
where sΦ(x) is the sum of the weights of the clauses that are satisﬁed by x.

k∈SKj
xt ∧ (cid:86)

k∈SKj

k∈SKj

t∈STj

t∈STj

Example 2 (Nixon diamond problem) Consider the following weighted knowledge-
base:

1000 : n → r Nixon is a Republican.
1000 : n → q Nixon is also a Quaker.
10

: r → ¬p Republicans tend not to be Paciﬁsts.
: q → p Quakers tend to be Paciﬁsts.

10

Converting all formulae to SDNFs, e.g. 1000 : n → r ≡ 1000 : (n ∧ r) ∨ (¬n), pro-
duces 8 conjunctive clauses. After combining the weights of clause (¬n) which appears
twice, an RBM is created (Figure 3) from the following unique conjunctive clauses and
conﬁdence values: 1000 : n ∧ r,
10 :
¬r,

1000 : n ∧ q,

10 : r ∧ ¬p,

2000 : ¬n,

10 : q ∧ p,

10 : ¬q.

10

Figure 3: The RBM for the Nixon diamond problem. With (cid:15) = 0.5, this RBM has
energy function: E = −h1(1000n+1000r−1500)−h2(−2000n+1000)−h3(1000n+
1000q−1500)−h4(10r−10p−5)−h5(−10r+5)−h6(10q+10p−15)−h7(−10q+5).

6 Experimental Results

Reasoning

In this experiment we apply LBM to effectively search for satisfying truth assignments
of variables in large formulae. Let us deﬁne a class of formulae:

ϕ ≡

M
(cid:94)

i=1

M +N
(cid:95)

xj)

xi ∧ (

j=M +1

(13)

A formula in this class consists of 2M +N possible truth assignments of the vari-
ables, with 2N − 1 of them mapping the formula to T rue (call this the satisfying set).
Converting to SDNF as done before but now for the class of formulae, we obtain:

ϕ ≡

M +N
(cid:95)

j=M +1

M
(cid:94)

(
i=1

xi ∧

M +N
(cid:94)

j(cid:48)=j+1

¬xj(cid:48) ∧ xj)

(14)

Applying Theorem 1 to construct an RBM from ϕ, we use Gibbs sampling to infer the
truth values of all variables. A sample is accepted as a satisfying assignment if its free
energy is lower than or equal to − log(1+exp(c(cid:15)) with c = 5, (cid:15) = 0.5. We evaluate the
coverage and accuracy of accepted samples. Coverage is measured as the proportion
of the satisfying set that is accepted. In this experiment, this is the number of satisfying
assignments in the set of accepted samples divided by 2N − 1). Accuracy is measured
as the percentage of accepted samples that satisfy the formula.

We test different values of M ∈ {20, 25, 30} and N ∈ {3, 4, 5, 6, 7, 8, 9, 10}.
LBM achieves 100% accuracy in all cases, meaning that all accepted samples do sat-
isfy the formula. Figure 4 shows the coverage as Gibbs sampling progresses (after each
time that a number of samples is collected). Four cases are considered: M=20 and N=5,

11

Figure 4: Percentage coverage as sampling progresses in LBM. 100% coverage is
achieved for the class of formulae with different values for M and N averaged over
100 runs. The number of samples needed to achieve 100% coverage is much lower
than the number of possible assignments (2M +N ). For example, when M=20, N=10,
all satisfying assignments are found after ∼ 7.5 million samples are collected, whereas
the number of possible assignments is ∼ 1 billion, a ratio of sample size to the search
space of 0.75%. The ratio for M=30, N=10 is even lower at 0.37%.

M=20 and N=10, M=25 and N=10, M=30 and N=10. In each case, we run the sampling
process 100 times and report the average results with standard deviations. The number
of samples needed to achieve 100% coverage is much lower than the number of possi-
ble assignments (2M +N ). For example, when M=20, N=10, all satisfying assignments
are found after ∼ 7.5 million samples are collected, whereas the number of possible
assignments is ∼ 1 billion, producing a ratio of sample size to the search space size of
just 0.75%. The ratio for M=30, N=10 is even lower at 0.37% w.r.t. ∼ 1012 possible
assignments. As far as we know, this is the ﬁrst study of reasoning in neurosymbolic
AI to produce these results with such low ratios.

Figure 5 shows the time needed to collect all satisfying assignments for different
N in {3, 4, 5, 6, 7, 8, 9, 10} with M = 20. LBM only needs around 10 seconds for
N <= 8, ∼ 25 seconds for N = 9, and ∼ 68 seconds for N = 10. The curve grows
exponentially, similarly to the search space size, but at a much lower scale.

Learning from Data and Knowledge

In this experiment, we evaluate LBM at learning the same Inductive Logic Program-
ming (ILP) benchmark tasks used by neurosymbolic system CILP++ [5] in comparison
with ILP state-of-the-art system Aleph [16]. An initial LBM is constructed from a sub-
set of the available clauses2 used as background knowledge, more hidden units with
random weights are added to the RBM and the system is trained further from exam-
ples, following the methodology used in the evaluation of CILP++. Both conﬁdence
values and network weights are free parameters for learning.

2The rest of the clauses is used for training and validation in the usual way, whereby satisfying as-
signments are selected from each clause as training examples, for instance from clause y ← x1 ∧ ¬x2,
assignment y = 1, x1 = 1, x2 = 0 is converted into vector [1, 1, 0].

12

Figure 5: Time taken by LBM to collect all satisfying assignments compared with
the size of the search space (number of possible assignments up to ∼ 1 billion) as
N increases from 3 to 10 with ﬁxed M=20. LBM only needs around 10 seconds for
N <= 8, ∼ 25 seconds for N = 9, and ∼ 68 seconds for N = 10. The curve grows
exponentially, similarly to the search space size, but at a much lower scale.

We carry out experiments on 7 data sets: Mutagenesis [17], KRK [1], UW-CSE
[13], and the Alzheimer’s benchmark: Amine, Acetyl, Memory and Toxic [7]. For Mu-
tagenesis and KRK, we use 2.5% of the clauses as background knowledge to build the
initial LBM. For the larger data sets UW-CSE and Alzheimer’s benchmark, we use 10%
of clauses as background knowledge. The number of hidden units added to the LBM
was chosen arbitrarily to be 50. For a fair comparison, we also evaluate LBM against a
fully-connected RBM with 100 hidden units so as to offer the RBM more parameters
than the LBM since the RBM does not use background knowledge. Both RBM and
LBM are trained in discriminative fashion [8] using the conditional distribution p(y|x)
for inference. The code with these experiments will be made available.

The results using 10-fold cross validation are shown in Table 2, except for UW-CSE
which use 5 folds. The results for Aleph and CILP++ were collected from [5]. It can be
seen that LBM has the best performance in 5 out of 7 data sets. In the alz-acetyl data
set, Aleph is better than all other models in this evaluation, and the RBM is best in the
alz-amine data set, despite not using background knowledge (although such knowledge
is provided in the form of training examples).

7 Conclusion and Future Work

We introduced an approach and neurosymbolic system to reason about symbolic
knowledge at scale in an energy-based neural network. We showed equivalence be-
tween minimising the energy of RBMs and satisﬁability of Boolean formulae. We
evaluated the system at learning and showed its effectiveness in comparison with state-
of-the-art approaches. As future work we shall analyse further the empirical results and
seek to pinpoint the beneﬁts of LBM at combining reasoning and learning at scale.

13

Mutagenesis
KRK
UW-CSE
alz-amine
alz-acetyl
alz-memory
alz-toxic

Aleph CILP++ RBM LBM
96.28
80.85
99.80
99.60
89.43
84.91
78.25
78.71
69.46
66.82
71.84
68.57
84.95
80.50

91.70
98.42
70.01
78.99
65.47
60.44
81.73

95.55
99.70
89.14
79.13
62.93
68.54
82.71

Table 2: Cross-validation performance of LBM against Aleph, CILP++ and standard
RBM on 7 data sets.

References

[1] M. Bain and S. Muggleton. Machine intelligence 13. chapter Learning Optimal
Chess Strategies, pages 291–309. Oxford University Press, Inc., New York, NY,
USA, 1995.

[2] William W. Cohen, Fan Yang, and Kathryn Mazaitis. Tensorlog: Deep learning

meets probabilistic dbs. CoRR, abs/1707.05390, 2017.

[3] Artur d’Avila Garcez and Luis C. Lamb. Neurosymbolic ai: The 3rd wave, 2020.

[4] R. Evans and E. Grefenstette. Learning explanatory rules from noisy data. JAIR,

61:1–64, 2018.

[5] M. Franc¸a, G. Zaverucha, and A. Garcez. Fast relational learning using bot-
tom clause propositionalization with artiﬁcial neural networks. Mach. Learning,
94(1):81–104, 2014.

[6] A.S. Garcez, K. Broda, and D.M Gabbay. Symbolic knowledge extraction from
trained neural networks: A sound approach. Artif. Intel., 125(1–2):155–207, 2001.

[7] Ross D. King, Michael J. E. Sternberg, and Ashwin Srinivasan. Relating chemical
activity to structure: An examination of ilp successes. New Generation Comput-
ing, 13(3), Dec 1995.

[8] Hugo Larochelle, Michael Mandel, Razvan Pascanu, and Yoshua Bengio. Learn-
J. Mach.

ing algorithms for the classiﬁcation restricted boltzmann machine.
Learn. Res., 13(1):643–669, March 2012.

[9] Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester,
and Luc De Raedt. Deepproblog: Neural probabilistic logic programming.
In
S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
nett, editors, Advances in Neural Information Processing Systems 31, pages
3749–3759. Curran Associates, Inc., 2018.

[10] Gary Marcus. Deep learning: A critical appraisal. CoRR, abs/1801.00631, 2018.

14

[11] L. de Penning, A. d’Avila Garcez, L.C. Lamb, and J-J. Meyer. A neural-symbolic
cognitive agent for online learning and reasoning. In IJCAI, pages 1653–1658,
2011.

[12] G. Pinkas. Reasoning, nonmonotonicity and learning in connectionist networks
that capture propositional knowledge. Artif. Intell., 77(2):203–247, 1995.

[13] Matthew Richardson and Pedro Domingos. Markov logic networks. Mach.

Learn., 62(1-2):107–136, February 2006.

[14] Luciano Seraﬁni and Artur S. d’Avila Garcez. Learning and reasoning with logic

tensor networks. In AI*IA, pages 334–348, 2016.

[15] P. Smolensky. Constituent structure and explanation in an integrated connection-
ist/symbolic cognitive architecture. In Connectionism: Debates on Psychological
Explanation. 1995.

[16] A. Srinivasan. The aleph system, version 5. http://www.cs.ox.ac.uk/activities/

machlearn/Aleph/aleph.html, 2007.

[17] A. Srinivasan, S. H. Muggleton, R.D. King, and M.J.E. Sternberg. Mutagenesis:
Ilp experiments in a non-determinate biological domain. In Proceedings of the 4th
International Workshop on Inductive Logic Programming, volume 237 of GMD-
Studien, pages 217–232, 1994.

[18] G. Towell and J. Shavlik. Knowledge-based artiﬁcial neural networks. Artif.

Intel., 70:119–165, 1994.

[19] S. Tran and A. Garcez. Deep logic networks: Inserting and extracting knowledge
IEEE T. Neur. Net. Learning Syst., (29):246–258,

from deep belief networks.
2018.

[20] Fan Yang, Zhilin Yang, and William W Cohen. Differentiable learning of logical
rules for knowledge base reasoning.
In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in
Neural Information Processing Systems 30, pages 2319–2328. Curran Associates,
Inc., 2017.

15

