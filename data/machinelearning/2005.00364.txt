0
2
0
2

r
p
A
0
3

]

V
C
.
s
c
[

1
v
4
6
3
0
0
.
5
0
0
2
:
v
i
X
r
a

Generative Adversarial Data Programming

Arghya Pal1,∗, Vineeth N Balasubramanian1

Abstract

The paucity of large curated hand-labeled training data forms a major bottle-

neck in the deployment of machine learning models in computer vision and other

ﬁelds. Recent work (Data Programming) has shown how distant supervision sig-

nals in the form of labeling functions can be used to obtain labels for given data

in near-constant time. In this work, we present Adversarial Data Programming

(ADP), which presents an adversarial methodology to generate data as well as

a curated aggregated label, given a set of weak labeling functions. More inter-

estingly, such labeling functions are often easily generalizable, thus allowing our

framework to be extended to diﬀerent setups, including self-supervised labeled

image generation, zero-shot text to labeled image generation, transfer learning,

and multi-task learning.

Keywords: Generative Adversarial Network (GAN), Distant Supervision,

Self-supervised Labeled Image Generation, Zero-shot Text to Labeled Image

Synthesis, Medical Labeled Image Synthesis.

1. Introduction

Curated labeled data is a key building block of modern machine learning

algorithms and a driving force for deep neural network models that work in

practice. However, the creation of large-scale hand-annotated datasets in every

domain is a challenging task due to the requirement for extensive domain exper-

tise, long hours of human labor and time - which collectively make the overall

∗Corresponding author: Arghya Pal, email address: arghyapal5@gmail.com
1Postal Address: Department of Computer Science & Engineering, Indian Institute of

Technology Hyderabad, Hyderabad, India

Preprint submitted to ArXiv

May 4, 2020

 
 
 
 
 
 
process expensive and time-consuming. Even when data annotation is carried

out using crowdsourcing (e.g. Amazon Mechanical Turk), additional eﬀort is re-

quired to measure the correctness (or goodness) of the obtained labels. We seek

to address this problem in this work. In particular, we focus on automatically

learning the parameters of a given joint image-label probability distribution (as

provided in training image-label pairs) with a view to automatically create the

labeled dataset.

To this end, we exploit the use of distant supervision signals to generate

labeled data. These distant supervision signals are provided to our framework as

a set of weak labeling functions which represent domain knowledge or heuristics

obtained from experts or crowd annotators. This approach has a few advantages:

(i) labeling functions (which can even be just loosely deﬁned) are cheaper to

obtain than collecting labels for a large dataset; (ii) labeling functions act as an

implicit regularizer in the label space, thus allowing good generalization; (iii)

with a small ﬁne-tuning, labeling functions can be easily re-purposed for new

domains (transfer learning) and multi-task learning (discussed in Section 5.1);

and (v) the labeling functions can be generalized to using semantic attributes,

which aids adapting the approach of generalized zero-shot text to labeled image

generation 5.4. We note that, writing a set of labeling functions (as we found

in our experiments) is fairly easy and quick - we demonstrate three python-like

functions as labeling functions to weakly label the SVHN [1] digit “0” in Figure

2.2.a (def l1 - def l3). Figure 1 shows a few examples of our results to illustrate

the overall idea.

In practice, labeling functions can be associated with two kinds of dependen-

cies: (i) relative accuracies (shown as solid arrows in Figure 2.2.b), are weights

to labeling functions measuring the correctness of the labeling functions w.r.t.

the true class label ; and (ii) inter-function dependencies (shown as dotted line

in Figure 2.2.b) that capture the relationships between the labeling functions

concerning the predicted class label. In this work, we propose a novel adversarial

framework, i.e. Adversarial Data Programming (ADP) presented in Figure 2.1,

using Generative Adversarial Network (GAN) that learns these dependencies

2

along with the data distribution using a minmax game.

Our broad idea of learning relative accuracies and inter-function dependen-

cies of labeling functions is inspired by the recently proposed Data Programming

(DP) framework [2] (and hence, the name ADP), but our method is diﬀerent

in many ways: (i) DP learns P (˜y|X)), while ADP learns joint distribution, i.e.

P (X, y); (ii) DP uses Maximum Likelihood Estimation (MLE) to estimate rel-

ative accuracies of labeling functions. We instead use adversarial framework to

estimate relative accuracies and inter-function dependencies of labeling func-

tions. We note that, [3] and [4] provide insights on the advantage of using a

GAN-based estimator over MLE. (iii) We use adversarial approach to learn inter

functional dependencies of labeling functions and replaces the computationally

expensive factor graph modeling proposed in [2].

Figure 1: (A) Labeled image generations of ADP on diﬀerent datasets: (a)-(d)

Sample results of image-label pairs generated using the proposed ADP framework on SVHN [1],

CIFAR 10 [5], Chest-Xray-14 [6], and LSUN [7] datasets (respectively); (B) Cross-domain

labeled image generation using ADP:; (C) Transfer learning using ADP: ADP

transfers its knowledge from a source domain to a target domain if distant supervision signals

are common. We demonstrate transfer learning from MNIST [8] to SVHN; (D) Generalized

zero-shot text to labeled image generation: ZS-ADP is a ﬁrst-of-its-kind model that

performs zero-shot text to labeled image generation on Flower102 [9], UCSD Bird [10] and

Chest-Xray-14 [6] datasets.

3

Furthermore, we show applicability of ADP in diﬀerent tasks, such as: (i) Self

supervised labeled image generation (SS-ADP), that generates labeled image

using an unlabeled dataset. The SS-ADP dependencies of labeling functions

using image rotation based self-supervised loss (similar to the image rotation

loss proposed in [11]). (ii) Generalized zero-shot text to labeled image synthesis

(ZS-ADP) that generates labeled images from textual descriptions (see Section

5.4). We show that ZS-ADP infer zero-shot classes as well as seen classes of

generated images using labeling functions those are semantic attributes (similar

to semantic attributes proposed by Lampert et al.

[12]). To the best of our

knowledge, the ZS-ADP is the ﬁrst generalized zero-shot text to labeled image

generator.

As outcomes of this work, we show a framework to integrate labeling func-

tions within a generative adversarial framework to model joint image label dis-

tribution. To summarize:

• We propose a novel adversarial framework, ADP, to generate robust data-

label pairs that can be used as datasets in domains that have little data

and thus save human labor and time.

• The proposed framework can also be extended to incorporate general-

ized zero-shot text to labeled image generation, i.e. ZS-ADP; and self-

supervised labeled image generation, i.e. SS-ADP in Section 5.

• We demonstrate that the proposed framework can also be used in a trans-

fer learning setting, and multi-task joint distribution learning where im-

ages from two domains are generated simultaneously by the model along

with the labels, in Section 5.

2. Related Work

Distant Supervision: In this work, we explored the use of distant supervi-

sion signals (in the form of labeling functions) to generate labeled data points.

Distant supervision signals such as labeling functions are cheaper than manual

4

annotation of each data point, and have been successfully used in recent meth-

ods such as [2, 13]. MeTaL [14] extends [15] by identifying multiple sub-tasks

that follow a hierarchy and then provides labeling functions for sub-tasks. These

methods require unlabeled test data to generate a labeled dataset and computa-

tionally expensive due to the use of Gibbs sampling methods in the estimation

step (also shown in our results).

Learning Joint Distribution using Adversarial Methods: In this work,

we use an adversarial approach to learn the joint distribution by weighting a

set of domain-speciﬁc label functions using a Generative Adversarial Network

(GAN). We note eﬀorts [16, 17] which attempt to train GANs to sample from a

joint distribution. In this work, we propose a novel idea to instead use distant

supervision signals to accomplish learning the joint distribution of labeled im-

ages, and compare against these methods.

Generalized Zero-shot Learning: A typical generalized zero-shot model

(such as [18, 19, 20]) learns to transfer learned knowledge from seen to un-

seen classes by learning correlations between the classes at training time, and

recognizing both seen and unseen classes at test time. While, eﬀorts such as

[21, 22, 23, 24, 25] proposed text-to-image generation methodology and then

demonstrated results on zero-shot text to image generation. However, no work

has studied the problem of generalized zero-shot text-to-labeled-image genera-

tion so far. We integrate a set of semantic attribute-based distant supervision,

similar to proposed by Lampert et al.

[12], signals such as color, shape, part

etc. to identify seen and zero-shot visual class categories.

Self Supervised Labeled Image Generation: While self supervised learn-

ing is an active area of research, we found only one work [17] that performs self

supervised labeled image generation. In particular, [17] uses a GAN framework

that performs k-means cluster within discriminator and does an unsupervised

image generation. In this work, we instead use a set of labeling functions and

perform self supervision. We defer the discussion to Section 5.

5

Figure 2: (1) Overall architecture of the Adversarial Data Programming (ADP) framework.

The generator generates image X and parameters (Θ, Φ) which are used by the labeling func-

tions block (LFB) to generate labeled images (X, y); (2) Labeling Functions: Crowd experts

give distant supervision signals in the form of weak labeling functions: e.g. presence of blob;

(3) Dependencies of Labeling Functions: Labeling functions show “relative accuracies” (solid

arrow) and “inter-functional dependencies” (dotted line). The ADP encapsulates all labeling

functions in a uniﬁed abstract container called LFB. LFB helps learn parameters correspond-

ing to both kinds of dependencies: “relative accuracies” and “inter-functional dependency”

to generates labeled images.

3. Adversarial Data Programming: Methodology

Our central aim in this work is to learn parameters of a probabilistic model:

P (X, y|z)

(1)

that captures the joint distribution over the image X and the corresponding

labels y conditioned on a latent variable z.

To this end, we encode distant supervision signals as a set of (weak) deﬁni-

tions by annotators using which unlabeled image can be labeled. Such distant

supervision signals allow us to weakly supervise images where collection of di-

rect labeled image is expensive, time consuming or static. We encapsulate all

available distant supervision signals, henceforth called labeling functions, in a

6

DCommonDLabelDImage𝚯𝚽X :͠GCommonG ParameterG Imagez(1)  Adversarial Data Programming (ADP)GeneratorLabeling Functions Block (LFB)DiscriminatorDLFB𝚽𝚽Real𝚯TΛDLFB(𝚽)ε [0, 1]8y :͠(2) Labeling Functions are Given by Domain ExpertsExpertsdef l1(Image): If blob;return [1,0,...,0]def l3(Image): If Bag_of_Feature; return [1,0,...,0]def l2(Image): If vertical; return [0,1,...,0] (3) Relative accuracies and inter-functional dependencies are measured by considering real label as a hidden variableyl1l2l3l4Relative AccuraciesInter Dependencyuniﬁed abstract container called Labeling Functions Block (LFB, see Figure

2.1). Let LFB comprised of n labeling functions l1, l2, · · · , ln, where each label-

ing function is a mapping, i.e.: li : Xj → aij, that maps an unlabelled image
X to a class label vector, aij ∈ Rm, where m is the number of classes labels
with (cid:80)m

ij ≤ 1. For example, as shown in Figure 2.2, Xj
could be thought of as an image from the CIFAR 10 single digit dataset, and
aij ∈ R10 would be the corresponding label vector when the labeling function

ij = 1 and, 0 ≤ ak

k=1 ak

li is applied on Xj. The aij, for instance, could be the one-hot 10-dimensional

class vector, see Figure 2.2.

We characterize the set of labeling functions, L = {l1, l2, · · · , ln} as having

two kinds of dependencies: (i) relative accuracies are weights given to labeling

functions based on whether their outputs agree with true class label y of an

image X; and (ii) inter-function dependencies capture the relationships between

the labeling functions with respect to the predicted class label. To obtain a

ﬁnal label y for a given data point X using the LFB, we use two diﬀerent sets of

parameters, Θ and Φ to capture each of these dependencies between the labeling

functions. We, hence, denote the Labeling Function Block (LFB) as:

LF BL,Θ,Φ : Xj → aj

(2)

i.e. given a set of labeling functions L, a set of parameters capturing the rel-

ative accuracy-based dependencies between the labeling functions, Θ, and a

second set of parameters capturing inter-label dependencies, Φ, LF B provides

a probabilistic label vector, aj, for a given data input Xj.

Our Equation 1 that we seek to model in this work (Equation 1) hence

becomes:

P (X, LF BL,Θ,Φ(X)|z)

(3)

In the rest of this section, we show how we can learn the parameters of the

above distribution modeling image-label pairs using an adversarial framework

with a high degree of label ﬁdelity. We use Generative Adversarial Networks

(GANs) to model the joint distribution in Equation 3. In particular, we provide

a mechanism to integrate the LFB (Equation 2) into the GAN framework, and

7

show how Θ and Φ can be learned through the framework itself. Our adversarial

loss function is given by:

min max L(G, D) =

E(X,y)∼D log(D(X, y)) + E

( ˜X,Θ,Φ)∼Pf ake(z) log(1 − D( ˜X, LF BL,Θ,Φ( ˜X))

(4)

where G is the generator module and D is the discriminator module. The overall

architecture of the proposed ADP framework is shown in Figure 2.1.

3.1. The Proposed Framework

A. ADP Generator G( (cid:101)X, Θ, Φ|z): Given a noise input z ∼ N (0, I) and a set of

n-labeling functions L, the generator G(.) outputs an image X and the parame-

ters Θ and Φ, the dependencies between the labeling functions described earlier.

In particular, G(.) consists of three blocks: GCommon, GImage and GP arameter,

as shown in Figure 2.1. GCommon captures the common high-level semantic

relationships between the data and the label space, and is comprised only of

fully connected (FC) layers. The output of GCommon forks into two branches:
GImage and GP arameter, where GImage generates the image ˜X, and GP arameter

generates the parameters {Θ, Φ}. While GP arameter uses FC layers, GImage

uses Fully Convolutional (FCONV) layers to generate the image (more details

in Section 6). Figure 2.1 also includes a block diagram for better understanding.

B. ADP Discriminator D(X, y): The discriminator D(.) estimates the like-

lihood of an image-label input pair being drawn from the real distribution ob-

tained from training data. The D(.) takes a batch of either real or generated

image and inferred label (from LF B) pairs as input and maps that to a proba-

bility score to estimate the aforementioned likelihood of the image-label pair. To

accomplish this, D(.) has two branches: DImage and DLabel (shown in the Dis-

criminator part in Figure 2.1). These two branches are not coupled in the initial

layers, but the branches share weights in later layers and become DCommon to

extract joint semantic features that help D(.) classify correctly if an image-label

pair is fake or real.
C. Labeling Functions Block LF B(˜y| ˜X, Θ, Φ): This is a critical module of

the proposed ADP framework. Our initial work revealed that a simple weighted

8

Algorithm 1: Procedure to compute ΦReal

Input: Labeling functions {l1, · · · , ln}, Relative accuracies θ1, · · · , θn,

Output probability vectors of labeling functions a1, · · · , an of

n-generated images by G

Output: ΦReal

Set ΦReal = I(n, n);

/* I = Identity Matrix

for i = 1 to n do

/* For each labeling function

for j = i + 1 to n do

/* For each other labeling function

/* If one-hot encoding of the outputs of two functions match,

increment (i, j)th entry in ΦReal by 1

ΦReal(i, j) = ΦReal(i, j) + OneHot(θiai) · OneHot(θjaj);

end

end

for p = 1 to n do

ΦReal(p, .) =

(cid:80)n

ΦReal(p,.)
u=1 ΦReal(p,u) ;

end

*/

*/

*/

*/

Set ΦReal = ΦReal + ΦT

Real − diag(ΦReal) /* Complete using symmetry

*/

(linear or non-linear) sum of the labeling functions does not perform well in

generating out-of-sample image-label pairs. We hence used a separate adversar-

ial methodology within this block to learn the dependencies. We describe the

components of the LFB below.

C.1. Relative Accuracies, Θ, of Labeling Functions: In this, we assume that

all the labeling functions infer label of an image independently (i.e.

indepen-

dent decision assumption) and the parameter Θ gives relative weight to each

of the labeling functions based on their correctness of inferred label for true

class y. The output, Θ, of the GP arameter block in the ADP Generator G(.),

9

provides the relative accuracies of the labeling functions. Given the jth image
output generated by GImage: ˜Xj, the n-labeling functions {l1, l2, · · · , ln}, and

the probabilistic label vectors {aij, i = 1, · · · , n} obtained using the labeling

functions, we deﬁne the aggregated ﬁnal label as:

˜yj =

n
(cid:88)

i=1

˜θiaij = ˜Θ · aj

(5)

where ˜θi is the normalized version of θi, i.e. ˜θi =

(cid:80)n

θi
k=1 θk

. The aggregated label,

Algorithm 2: Training procedure of ADP

Input: Iterations: N , Number of steps to train D: k, m

Output: Trained ADP model

for N do

for k steps do

Draw m samples from G: {( ˜X1, Θ1, Φ1), · · · , ( ˜Xm, Θm, Φm)} and

subsequently infer corresponding labels {(cid:101)y1, · · · , (cid:101)ym} using
LFB(.)(Equation 7);

Update weights of D and DLF B (ψd and ψl respectively):

∇ψd

1
m

m
(cid:88)

(cid:104)

i=1

log D(Xi, yi) + log(1 − D( ˜Xi, ˜yi))

(cid:105)

∇ψl

1
m

m
(cid:88)

(cid:104)

i=1

log DLF B(Φreali) + log(1 − DLF B(Φi))

(cid:105)

end

Update weights of generator G (i.e. ψg):;

∇ψg

∇ψg

1
m

1
m

m
(cid:88)

(cid:104)

i=1

m
(cid:88)

(cid:104)

i=1

log(1 − DLF B(Φi))

(cid:105)

log(1 − D( ˜Xi, ˜yi))

(cid:105)

end

˜y, is provided as an output of the LFB.

10

C.2. Inter-function Dependencies, Φ, of labeling functions: In practice, a depen-

dency among labeling function is a common observation. Studies [2, 13] show

that such dependencies among labeling function proportionally increase with

the number of labeling functions. Modeling such inter-functional dependencies

act as an implicit regularizer in the label space leading to an improvement in the

labeled image generation quality and generated image-to-label correspondence.

While recent studies utilized factor graph [2, 13] to learn such dependencies

among labeling functions, we instead use an adversarial mechanism inside the
LFB to capture inter-function dependency ˜Φ that in turns inﬂuence the ﬁnal
relative accuracies, ˜θ. DLF B, a discriminator inside LFB, receives two inputs:

Φ, which is output by GP arameter, and ΦReal, which is obtained from Θ using

the procedure described in Algorithm 1. Algorithm 1 computes a matrix of

interdependencies between the labeling functions, ΦReal, by looking at the one-

hot encodings of their predicted label vectors. If the one-hot encodings match

for given data input, we increase the count of their correlation. The task of the

discriminator is to recognize the computed interdependencies as real, and the Φ

generated through the network in GP arameter as fake. The objective function

of our second adversarial module is hence:

min max L(G, DLF B) = E log(DLF B(Φreal(z))) + E log(1 − DLF B(Φ(z))) (6)

where ΦReal and Φ are obtained from GP arameter(z) as described above. More

details of the LFB are provided in implementation details in Section 6.

C.3. Final label prediction, ˜y, using LFB : We deﬁne the aggregated ﬁnal label

as:

˜yj = ˜Θj · ΦT

j · aj

(7)

The samples ( ˜X, ˜y) generated using the G and LF B modules thus provide sam-

ples from the desired joint distribution (Eqn 1) modeled using the framework.

11

3.2. Final Objective Function

We hence expand our objective function from Equation 4 to the following:

min max L(G, Dimage, Dlabel) =

E(X,y)∼D log(Dimage(X)) + Ez∼N (0,I) log(1 − Dimage(Gimage(z)))

(8)

+ E(X,y)∼D log(Dlabel(y)) + Ez∼N (0,I) log(1 − (Dlabel(LF B(G(z))))

3.3. Training

Algorithm 2 presents the overall stepwise routine of the proposed ADP

method. During the training phase, the algorithm updates weights of the model

by estimating gradients for a batch of labeled data points.

4. Theoretical Analysis

Theorem: For any ﬁxed generator G, the optimal discriminator D of the game

deﬁned by the objective function L(G, D) is

D∗(X, y) =

preal(X, y)
preal(X, y) + pf ake(X, y)

(9)

Proof: The training criterion for the discriminator D, given any generator

G, is to maximize the quantity L(G, D). Following [26], maximizing objective

function L(G, D) depends on the result of Radon-Nikodym Theorem [27], i.e.

Ez∼pf ake(z) log(1 − D(G(z))) = Ex∼preal(x) log(1 − D(x))

(10)

The objective function can be reformulated for ADP as:

L(G, D) ≡

(cid:90)

y

Ly(G, D)dy

(11)

Following Radon-Nikodym Theorem we can say:

Vy(G, D) =

(cid:90)

X

pdata(X, y) log(D(X, y))dX +

(cid:90)

z

pf ake(z) log(1 − D(G(z))dz

(cid:90)

+

=

pf ake(X, y) log(1 − D(X, y))dX

X
(cid:90)

X

(cid:20)
pdata(X, y) log(D(X, y)) + pf ake(X, y) log(1 − D(X, y))

(cid:21)

dX

(12)

12

Now, from Equation 11:

L(G, D) ≡

(cid:90)

Ly(G, D)dy

y
(cid:90)

(cid:90)

X

y

=

(cid:20)
pdata(X, y) log(D(X, y))+

(13)

pf ake(X, y) log(1 − D(X, y))

dydX

(cid:21)

Following [26], for any (a, b) ∈ R2\{0, 0}, the function y → a log y + b log(1 −

y) achieves its maximum in [0, 1] at

a
a+b , which proves the claim.

Theorem: The equilibrium of V(G, D) is achieved if and only if pdata(X, y) =

pg(X, y), and maxD L(G, D) attains the value − log(4).

Proof: Considering the optimal discriminator D∗(X, y) and the ﬁxed genera-

tor G described in Eqn 6 in Theorem 1, the min-max game in Eqn 4 can be

reformulated as:

C(G) = max

D

L(G, D) =

(cid:20)
pdata(X, y) log(D(X, y))

(cid:90)

(cid:90)

x

y

(cid:21)

+ pf ake(X, y) log(1 − D(X, y))

dydX

(cid:2) log D∗
(cid:34)

= E(X,y)∼preal

G(X, y)] + E(X,y)∼pf ake

= E(X,y)∼preal

log

preal(X, y)
preal(X, y) + pf ake(X, y)

(cid:2) log D∗
(cid:35)

G(X, y)]

(14)

(cid:34)

+ E(X,y)∼pf ake

log

pf ake(X, y)
preal(X, y) + pf ake(X, y)

(cid:35)

The training criterion reaches its global minimum preal(X, y) = pf ake(X, y)

and at this point, D∗

G(X, y) will have the value 1

2 . We hence have:

C(G) = E(X,y)∼preal

log

(cid:34)

preal(X, y)
preal(X, y) + pf ake(X, y)

(cid:35)

(cid:34)

+ E(X,y)∼pf ake

log

= E(X,y)∼preal

(cid:20)

log

(cid:35)

pf ake(X, y)
preal(X, y) + pf ake(X, y)
(cid:21)
(cid:21)

(cid:20)

+ E(X,y)∼pf ake

log

1
2

1
2

(15)

= − log 4

So, with a ﬁxed generator G, the training criterion C(G) attain its best possible

value when preal(X, y) = pf ake(X, y). At training phase, the criterion C(G1)

13

Figure 3: Multi-task joint distribution learning: (a) A modiﬁed ADP can generates

images of two domains X1 and X2 having a label correspondence, (b) labeled images from

MNIST-SVHN and LookBook-FMNIST, (c) Training progress of MNIST-SVHN joint labeled

image generation.

uses generator G1 and optimizes the objective function L(G, D)

C(G1) = − log 4 + KL

preal

(cid:32)

(cid:33)

preal + pf ake
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:32)

+ KL

pf ake

(cid:33)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

preal + pf ake
2
(cid:18)

≥ − log 4

(cid:19)

= − log 4 + 2 · JSD

preal||pf ake

≥ − log 4

in this equation, KL denotes the Kullback-Leibler divergence and JSD denotes

the Jensen-Shannon divergence. From the property JSD, it results non-negative

if preal(X, y) (cid:54)= pf ake(X, y) and zero when preal(X, y) = pf ake(X, y). So, the

global minimum is attained by the training criterion C(G) at the preal(X, y) =

pf ake(X, y). At that point, the criterion C(G) attains the value C(G) = − log 4

is the global minimum of C(G) and at that point the generator perfectly mimics

the real joint data-label distribution.

5. Extensibility of ADP in diﬀerent tasks

5.1. Transfer Learning using ADP

Distant supervision signals such as labeling functions (which can often be

generic) allows us to extend the proposed ADP to a transfer learning setting.

14

GcommonGImage 1GImage 2GParameterCommonLFBX1X2y222002X1:X2:X1:X2:LookBook - Fashion MNIST GenerationMNIST-SVHN Generationy :y :Training Progress of MNIST-SVHN Generation6    7    9     7    3    4    5    6(a)(b)(c)Figure 4: Block Diagram of SS-ADP:

In this setup, we trained ADP initially on a source dataset and then ﬁnetuned

the model to a target dataset, with very limited training. In particular, we ﬁrst

trained ADP on the MNIST dataset, and subsequently ﬁnetuned the Gimage

branch alone on the SVHN dataset. We note that the weights of Gcommon,

Gparameter and DLF B are unaltered. The ﬁnal ﬁnetuned model is then used to

generate image-label pairs (which we hypothesize will look similar to SVHN).

Figure 1C (named “Transfer Learning using ADP”) shows encouraging results

of our experiments in this regard.

5.2. Multi-task Joint Distribution Learning

Learning a cross-domain joint distribution from heterogeneous domains is

a challenging task. We show that the proposed ADP method can be used to

achieve this, by modifying its architecture as shown in Figure 3(a) (top), to

simultaneously generate data from two diﬀerent domains, i.e. X1 and X2. We

study this architecture on the: (1) MNIST and SVHN datasets, as well as (2)

LookBook and Fashion MNIST datasets; and show promising results of our

experiments in Figure 3(b). The LFB acts as a regularizer and maintains the

correlations between the domains in this case. We show joint multi-task joint

distribution training progress on MNIST and SVHN datasets in Figure 3(c).

5.3. Self Supervised Labeled Image Generation

Thus far, we show labeled image generation process using LF B(.), which

is integrated in the generator G(.). In this section, we show the labeled image

generation process from unlabeled data Du, and show a way to integrate LF B(.)

15

z𝒢(.)xfakeyfakeD(.)LFB𝚯𝚽yrealXReal / Fake Self Supervised Labeled Image GenerationrotationXrXron real unlabeled image to get “distant supervision” image labels. In particular,

LF B(.) gets the real unlabeled image X ∼ Du as input and infers the label y.

X r

Similar to [11], we provide LF B(.) a rotated version of image X, i.e. LF BL,Θ,Φ :
j → aj and y = Θ · ΦT · a, where r ∈ {0°, 90°, 180°, 270°}, see Figure 4. The
relative accuracy parameter Θ and inter-function dependency parameter Φ of

LF B(.) are learned based on how close the inferred label y of unlabeled image

X to the inferred label yr of the rotation X r of image X, i.e:

min
Θ,Φ

Lself = E y∼LF B(X)
yr∼LF B(X r)

||y − yr||1

(16)

Hence, the objective function of SS-ADP is as follows:

min max L(G, Dimage, Dlabel) =

EX∼Du log(Dimage(X)) + Ez∼N (0,I) log(1 − Dimage(Gimage(z)))

(17)

+ EX∼Du log(Dlabel(LF B(X))) + Ez∼N (0,I) log(1 − (Dlabel(G(z)))

and the ﬁnal objective is:

min max Lss(G, Dimage, Dlabel) = L(G, Dimage, Dlabel) + λLself

(18)

5.4. Generalized Zero-shot Text to Labeled Image Synthesis using ADP

We go further to introduce a ﬁrst-of-its-kind approach to generalized zero-

shot text to labeled image generation, i.e. p(X, LF B(X)|t), using a modiﬁed

version of ADP, henceforth called ZS-ADP. Generalized zero-shot classiﬁcation

[20, 19, 18] and text-to-(zero-shot)image synthesis p(X|t) in [21, 22, 25] were

studied separately in literature, we go beyond text-to-image synthesis and pro-

pose a novel framework ZS-ADP that performs text-to-labeled-image synthesis in

a generalized zero-shot setup. To accomplish such objective, we assume to have

a dataset DG with K classes. Of K classes, ﬁrst m seen classes have samples in

the form of tuples {X seen, yseen, tseen}, where an image X seen is associated with

a seen class yseen ∈ {y1, · · · , ym}, and tseen denotes textual description (such
as caption) of image X seen. Complementarily, we have no images for the rest

of the classes, i.e. yzero ∈ {ym+1, · · · , yK}, and only textual descriptions tzero

16

are available for zero-shot classes. Our primary aim is to learn the parameters

of the following probabilistic model at training time:

p(X seen, LF B(X seen)|tseen)

(19)

such that, at inference time, it can generate labeled images of both seen and

zero-shot classes:

p(X, LF B(X)|t)

(20)

where t ∈ {tseen ∪ tzero} and X ∈ {X seen ∪ X zero}.

To this end, ZS-ADP learns class-independent style-ness information (back-

ground, illumination, object orientation). While, ZS-ADP learns content-ness

information of visual appearances and attributes such as shape, color, and size

etc. using LF B module. We follow the semantic attribute based object class

identiﬁcation work of Lampert et al. [12], and modify the labeling functions in

LF B in terms of a set of p semantic attributes, i.e. s = {s1, s2, · · · , sp}. To

make this exposition self-contained, we are paraphrasing the idea of “identifying

an class based on semantic attributes” of [12] using an example: a object “ze-

bra” can be classiﬁed by recognizing semantic attributes, such as: “four legs”,

“has tail” and “white-black stripes on body”. Such semantic attributes can be

integrated within LF B as labeling functions without any architectural change

in ADP (and hence ZS-ADP). Formally, each semantic attribute acts as a la-

beling function (similar to LF B of ADP), i.e. si : Xj → {0, 1}. Similar to [12],

the LF B produces the ﬁnal class label y of the generated image Xj by ranking

the similarity scores between ground truth semantic attribute vectors of seen

and zero-shot classes sgt, and the semantic attribute vector sj for Xj:

y = arg

min
y∈{y1,··· ,yK }

(cid:16)

(Φ · Θ) ◦ sj) · (sgt
y )

(cid:17)

(21)

where (Θ, Φ) ∼ G is sampled from the generator G of ADP, and ◦ denotes

the Hadamard product. Following [12], we assume access to a deterministic

semantic attribute vector, sgt (ground truth), for each seen and zero-shot class.

The adversarial framework of ZS-ADP learns a non-linear mapping between

17

text and labeled image space:

min max V (G, Dimage, Dlabel) = E(X seen,tseen)∼DG log Dimage(X seen|ψ(tseen))

+ Ez∼N (0,I) log(1 − Dimage(G(z, ψ(tseen))))

+ E(yseen,tseen)∼DG log(Dlabel(yseen|ψ(tseen))

+ Ez∼N (0,I) log(1 − Dlabel(LF B(G(z, ψ(tseen))))

(22)

Since raw text can be vague and contain redundant information, we encode the

raw text using a text encoder and obtain a text encoding ψ(t). Our encoder

ψ(t) is inﬂuenced by the Joint Embedding of Text and Image (JETI) encoder

proposed by Reed et al. in [21].

At inference time, the raw text from either seen or zero-shot class is ﬁrst

encoded by the text encoder ψ(t), where, t ∈ {zero, seen}. The ZS-ADP gets

the noise vector z ∼ N (0, I) and encoded text ψ(t) as input and provides
image ˜X and dependency parameters {Θ, Φ} as outputs. Following Equation

21, the LF B(.) provides class label ˜y by aggregating the decisions of semantic

attributes.
5.4.1.1. Adding Cycle Consistency Loss to Semantic Attributes:. Since zero-

shot classes are not present at training time and ZS-ADP is optimized only

using seen classes, we often observed a strong bias to seen classes at inference

time. Such biases also been reported in earlier eﬀorts such as [20] and we

imposing an additional cycle-consistency loss to the ZS-ADP, i.e.:

Vzero(G, DImage, DLabel) = V (G, DImage, DLabel)

+ λEs∼LF B(G(z,sgt))||s − sgt||1

(23)

in addition to ZS-ADP, the generator G(.) gets the sgt of seen and zero-shot

classes, the noise vector z ∼ N (0, I) as input to learn dependency parameters

{Θ, Φ} (as opposed to the usual text embedding ψ(t) with z ∼ N (0, I) described

in Equation 22). Here, λ is a hyperparameter, which we vary and report results

in Figure 6(d).

18

Labeling Function 3: Labeling function based on Deep Feature

Input: Image, Number of Clusters = k (equal to number of classes y)

Output: Probabilistic label vector

/* Unsupervised deep learning based labeling function

*/

m = Num of kernels from ﬁfth layer of pre-trained AlexNet trained using

DeepCluster method [28];

for i=1· · · n do

for j = 1 · · · m do

vavgij = average value of Frobenius norm of activation map of jth
kernel on subset of training samples from kth cluster;

end

end

for j = 1 · · · m do

vImagej = value of Frobenius norm of activation map of jth kernel on

Image;

end

return OneHot

(cid:16)

arg mini

(cid:104)
|vavgij − vImagej |

(cid:105)(cid:17)

19

SVHN CIFAR LSUN CHEST

Long edges [29, 30],

PatchMatch [31],

Local Dense

Features [32],

Heuristic

Holistic spatial

envelope [33],

Domain

Knowledge

from Experts

Rib

border [34, 35],

Parametric

model [36, 37],

Statistical

shape

model [38],

Domain

Knowledge

from Experts

Image

SIFT [39], k-means clustering [40],

Processing

Bags of Keypoints [41]

Deep

Learning

DeepCluster [28], Deep features

from [42, 43], Deep Representation

from [44, 45, 46],

Table 1: Description of Labeling Functions used for SVHN [1], CIFAR 10 [5], LSUN [7] and

CHEST-Xray-14 [6] dataset.

6. Experiments and Results

6.1. Dataset

6.1.1. ADP and SS-ADP : We validated ADP and SS-ADP on the following

datasets: (i) SVHN [1]; (ii) CIFAR-10 [5]; (iii) LSUN [7]; and (iv) CHEST, i.e.

Chest-Xray-14 [6] datasets. For cross-domain multi-task learning and transfer

learning using ADP, we validated on: (i) digit dataset MNIST [8] and SVHN

[1]; (ii) cloth dataset: Fashion MNIST (FMNIST) [47] and LookBook [48]. We

grouped LookBook dataset 17 classes into 4 classes: coat, pullover, t-shirt, dress,

to match number of classes of FMNIST dataset.

6.1.2. ZS-ADP : We evaluated ZS-ADP on: (i) CUB 200 [10]; (ii) Flower-102

20

[9]; and (iii) Chest-Xray-14 [6]. We consider Nodule and Eﬀusion (randomly

selected) as zero-shot classes of Chest-Xray-14 dataset in our experiments.

6.2. Labeling Functions

6.2.1. ADP and SS-ADP shown in Table 1: We encode distant supervision

signals as a set of (weak) deﬁnitions using which unlabeled data points can

be labeled. We categorized labeling functions in Table 1 into three categories:

(i) Heuristic: We collect the labeling functions based on domain heuristics

such as knowledge bases, domain heuristics, ontologies. Additionally, these def-

initions can be harvested from educated guesses, rule-of-thumb from experts

obtained using crowdsourcing. The experts were given a batch ((cid:39) 4000 im-

ages of a dataset) and asked to provide a set of labeling functions. (ii) Image

Processing: Domain heuristics from Image processing and Computer Vision.

(iii) Deep Learning: We collect activation maps from pre-trained deep mod-

els (deep models trained in an unsupervised manner). We show an example of

labeling functions used for the SVHN dataset in Labeling Functions 3.

6.2.2. ZS-ADP : The CUB 200 dataset [10] provides a set of semantic attributes

to identify a class. While, for Flower-102 and Chest-XRay-14 dataset we fol-

low [49] to identify semantic attributes, followed by [50] and get a color-based

semantic attributes.

6.3. Implementation Details

ADP, SS-ADP and ZS-ADP : We adopt BigGAN 128 × 128 architecture [51] to

implement generators and discriminators of ADP, SS-ADP and ZS-ADP. We

slightly change the last layers of BigGAN model to produce images of intended

size of a dataset. In particular, GCommon + GImage is the BigGAN generator,

and GP arameter branch (3 Fully Connected FC layers) is forked after the “Non-

Local Block” of the BigGAN generator. Similarly, DImage + DCommon follows

BigGAN discriminator, while DLabel branch is added after “Non-Local Block” of

the BigGAN discriminator. We follow the oﬃcial hyperparametres of BigGAN,

i.e. z = 120d, train generator for 250k iterations and 5 discriminator iterations

21

before every generator iteration, optimizer={Adam}, learning rate for generator

is 5 · 10−5 and 2 · 10−4 for discriminator.

Figure 5: (Best viewed in color and while zoomed) Qualitative results of diﬀerent genera-

tive (both GAN and Non-GAN) methods, such as: WINN [52], Self Supervised SGAN [17],

semi-supervised S2GAN [17], Plug and Play PnP [53], f-VAEGAN-D2 FD2 [54], JointGAN

Joint [16], BigGAN Big [51] and our proposed SS-ADP and ADP, are given in columns. (A)

SVHN-(B) CIFAR 10: First columns represent class labels. We use abbreviation: Auto-

mobile “Auto”, Aerospace “Aero”; (C) LSUN: We use abbreviations: Tower “Twr”, Church

“Chrh”, Bridge “BR”, Conference Room “Cnf”, Restaurant “Rst”. (D) CHEST: We use

abbreviation, such as: Eﬀusion “Eﬀ”, Nodule “N”, Cardiomegaly “Card” etc. We show wrong

labels in color red and correct labels as black. While, labels are provided as input in some

methods (WINN, PnP, BigGAN), our proposed methods ADP and SS-ADP generates labeled

images. On CHEST generated images, we additionally get disease location mark (shown as

red boxes) from experts on generated images.

6.4. Qualitative Results Comparison with Prior Methods

6.4.1. ADP and SS-ADP : We compared our proposed SS-ADP and ADP meth-

ods against other generative (both GAN and Non-GAN) methods, such as:

WINN [52], Self Supervised SGAN [17], semi-supervised S2GAN [17], Plug and

Play PnP [53], f-VAEGAN-D2 FD2 [54], JointGAN Joint [16], BigGAN Big

22

[51], and show results in Figure 5. We discuss our improved results in terms

of: (i) Image Quality: The results of ADP and SS-ADP show a signiﬁcant

improvement with respect to the baseline BigGAN method. We observe, the

image “style” (i.e. background, illumination etc.) and “content” (i.e. object

shape, orientation) are capture thoroughly in ADP and SS-ADP. The improve-

ments in ADP and SS-ADP are likely due in part to the fact that LF B(.) acts

as a regularizer in the training objective of generator in Algorithm 2 encourages

ADP and SS-ADP to capture modes, thus resulting an improved “style”ness

and “content”ness. For example: in CIFAR 10, we can observe clear automo-

bile structure and color variation for CIFAR 10 automobile “Auto” generated

images (see Figure 5).

Figure 6: (Best Viwed in color, and, please zoom to see details) Generalized zero-shot labeled

image generation on Flower-102, CUB 200 and Chest-Xray-14 datasets. (a) Generated labeled

images by GAN-E2E [21], StackGAN++ [23], AttnGAN [24], Hierarchical [25], FD2 (that is f-

VAEGAN-D2) [54] and our method ZS-ADP. Existing state-of-the-art methods fail to capture

image-to-label correspondence of zero-shot classes, and converge only to seen class labels (red

in color). Only ZS-ADP gives quality image and good image-label correspondence.

(b)

Changing text and noise vector: Generated images not only have diﬀerent background but

the object color also changed, and hence a change in label. (c) Change in text but ﬁxed noise

vector: Style of one class (i.e. Sunﬂower) is transferred to another class (i.e. Windﬂower) by

changing content information.

23

(a)(b)(c)Methods

SVHN

CIFAR10

LSUN

CHEST

MIS

FID

CRT CRG MIS

FID

CRT CRG MIS

FID

CRT CRG MIS

FID

CRT CRG

(↑)

(↓)

(↑)

(↑)

(↑)

(↓)

(↑)

(↑)

(↑)

(↓)

(↑)

(↑)

(↑)

(↓)

(↑)

(↑)

WINN

1.21

21.72

SGAN

1.33

18.03

SGAN

2.03

10.07

PnP

FD2

Joint

Big

1.12

17.94

1.83

16.73

1.84

13.71

3.04

8.01

SSADP 3.51

8.32

ADP

3.74

7.29

56

51

62

53

72

74

75

79

83

43

48

58

48

70

71

67

74

81

0.73

28.92

1.94

15.91

1.37

17.93

0.92

27.60

1.63

17.02

1.17

18.81

2.44

13.47

1.61

12.91

2.82

9.21

54

38

54

57

58

66

67

69

72

30

30

43

49

50

61

49

67

70

1.01

19.05

1.13

18.72

3.18

7.71

2.32

10.41

2.81

10.90

1.91

14.19

3.97

7.72

4.02

6.41

4.81

5.37

62

68

75

53

73

61

73

74

88

42

60

64

52

62

51

63

71

87

1.08

18.92

2.01

11.84

3.49

6.29

3.06

7.64

2.89

13.63

2.31

9.90

3.31

6.17

3.62

6.01

4.01

5.25

58

71

72

73

69

74

74

72

82

46

64

71

60

58

73

63

71

80

Table 2: Qualitative results of generative (both GAN and Non-GAN) methods: WINN [52],

Self Supervised SGAN [17], Semi-supervised S2GAN [17], Plug and Play PnP [53], f-VAEGAN-

D2 FD2 [54], JointGAN Joint [16], BigGAN Big [51], SS-ADP and ADP, are given in columns:

(i) MIS (↑) Modiﬁed Inception Score [55]: (Higher value is better) Though our basic setup is

based on Big framework, we observe almost 1 unit performance boost for ADP. Similarly, the

SS-ADP outperforms Big method all cases showing the eﬃcacy of our proposed method on

labeled image generation; (ii) FID (↓) Frechet Inception Distance [56]: (Lower value is better)

Lower values of SS-ADP and ADP w.r.t other methods on FID imply that the generated

labeled images are both good in quality and produce versatile labeled image samples; (iii)

CRT (↑): (Higher value is better) Top-1 classiﬁcation accuracy percentage (values are in %)

of a ResNet-50 classiﬁer trained on real labeled images and tested on generated images; (iV)

CRG (↑): (Higher value is better) Top-1 classiﬁcation accuracy percentage (values are in %)

of a ResNet-50 classiﬁer trained on generated labeled images and tested on real images.

We note a good variation in background, orientation, and low-level object

structure on generated images by ADP and SS-ADP. Similarly, on CHEST

dataset, we observe low-level details, such as: exact lungs location, disease mark

(shown in red box) etc. on generated images, shown in Figure 3. However, all

other methods fail to capture such disease marks and generate the global image

of chest X-ray. (ii) Image to Label Correspondence: We observe a good

image-to-label correspondence (see Figure 5), thus proving our claim of using

labeling functions within adversarial framework of GAN architecture.

6.4.2 ZS-ADP : We compared our proposed ZS-ADP against ﬁve state-of-the-

art methods: Reed et al. (GAN-E2E) [21], StackGAN++ [23], AttnGAN (that

is AttentionGAN) [24], Hierarchical (that is Hierarchical text-to-image synthe-

24

A. Image to Label Correspondence (HTT/CRT )

Methods

Flower 102

CUB 200

Chest Xray

GAN E2E [21]

(5.38/53)

(5.91/58)

(4.21/59)

StackGAN++ [23]

(6.21/61)

(5.84/64)

(5.18/62)

AttnGAN [24]

(7.91/65)

(7.42/62)

(7.27/67)

Hierarchical [25]

(8.41/68)

(8.46/69)

(8.01/68)

FD2 [54]

ZS-ADP

(8.72/70)

(8.79/72)

(8.67/72)

(9.28/78)

(9.11/76)

(9.16/79)

B. Image Quality (FID Score)

Methods

Flower 102

CUB 200

Chest Xray

GAN E2E [21]

12.88

12.71

14.82

StackGAN++ [23]

7.67

AttnGAN [24]

Hierarchical [25]

FD2 [54]

ZS-ADP

5.91

4.41

4.32

3.28

7.23

5.42

4.46

4.02

3.11

6.18

5.27

4.91

3.67

3.16

Table 3: (A) Image to label correspondence of zero-shot classes by ZS-ADP. We report

results in the form HTT/CRT to show Human Turing Test (HTT) of labeled image, and, CRT

is the Top-1 classiﬁcation score of ResNet50 classiﬁer trained on real dataset and tested on

the generated dataset; (B) Image Quality: The quality of images are evaluated using the

FID score [56].

25

Figure 7: (a) Test-time classiﬁcation cross-entropy loss of a pre-trained ResNet model on

image-label pairs generated by ADP, ADP (only its Image-GAN component) with majority

voting, and ADP (only its Image-GAN component) with DP for labels; (b) Average running

time (in Mins.) of ADP against other methods to estimate relative accuracies and inter-

function dependencies.

sis) [25], and FD2 (that is f-VAEGAN-D2) [54]. Due to the unavailability of

generalized zero-shot text to labeled image generation methods, we modiﬁed the

generators of [21, 23, 24, 25, 54] in a way that the last layers of those generators

now generate both the image X and class label y.

In our experiment, we ﬁxed the λ = 0.3 of Equation 23 and get the generated

labeled images from text from ZS-ADP. The ZS-ADP generates good quality

images as well as good image-label correspondence, see Figure 6(a). We show

latent space interpolation in Figure 6(b)-(c). Figure 6(b) show labeled images

where we changed the noise distribution z ∼ N (0, I) and the textual description.

While, Figure 6(c), we ﬁxed the noise distribution z ∼ N (0, I) but change the

text, and we see minimal change in style, i.e. background or orientation, but a

change in color and shape.

6.5. Quantitative Result Comparison with prior methods

6.5.1. ADP and SS-ADP : For the sake of quantitative comparison among ADP,

SS-ADP and other generative methods, we adopted four evaluation metrics

26

# LFs.

SVHN

CIFAR10 Chest

LSUN

3

5

10

15

20

25

10.23% 21.02%

27.39% 23.82%

8.32%

8.53%

21.31% 18.30%

1.40% 4.81%

17.93% 11.62%

1.33%

4.92%

18.93% 13.05%

1.34%

4.80%

18.45% 12.83%

1.31%

4.73%

18.43% 12.82%

Table 4: Performance of ADP when number of labeling functions is varied.

(studied in [17, 16]): (i) MIS (↑) Modiﬁed Inception Score proposed in [55]
computes exp{Ex[KL(p(y|X)||p(y))]}, where X: image, p(y|X): softmax out-

put of a ResNet-50 classiﬁer (trained on real labeled images), and p(y):

la-

bel distribution of generated samples; (ii) FID (↓) Frechet Inception Distance

(FID) proposed in [56] computes F ID(X, Xg) = ||µX − µXg ||2
2(ΣX ΣXg ) 1
generated image; (iii) CRT (↑), i.e. Top-1 classiﬁcation accuracy (in %) of a

2 ), where, µ, Σ are mean and co-variance, and X: real image Xg:

2 + Tr(ΣX + ΣXg −

ResNet-50 classiﬁer trained on real labeled images and tested on generated im-

ages; and (iv) CRG(↑), i.e. Top-1 classiﬁcation accuracy (in %) of a ResNet-50

classiﬁer trained on generated labeled images and tested on real images. As a

baseline the BigGAN labeled image generator obtains MIS sores: 3.04 on SVHN,

2.44 on CIFAR 10, 3.97 on LSUN, 3.31 on CHEST datasets. While, FID scores:

8.01 on SVHN, 13.47 on CIFAR 10, 7.72 on LSUN and 6.17 on CHEST datasets.

In ADP, we observe that the generated images able to achieve an average

of ∼ 0.7 units of MIS and ∼ 2 units of FID boosts w.r.t the baseline BigGAN

method. On the other hand, we observe a smaller gap between CRT and CRG,

suggesting that the ResNet-50 classiﬁer of CRT can classify well the generated

images, and the ResNet-50 classiﬁer of CRG trained on generated labeled image

of ADP can classify real images (the generated images are good in quality and

have high image-to-label correspondence). We note that, though the BigGAN

27

Figure 8: Sample results of image-label pairs generated by combining a vanilla GAN (for

image generation) and DP [2] (for label generation) using the same labeling functions used in

this work. Row labels represent the original class label (am = automobile) and column labels

are provided by DP. Note the poor image-label correspondence, supporting the need for our

work.

secured a good CRT score but fails to perform well at CRG score across all

datasets. Such observation directly implying the advantage of using LF B(.) to

infer labels of generated images, within the adversarial framework of baseline

BigGAN.

While in SS-ADP, we note that the alternative approach of getting “distant

supervised” labels of unlabeled image X (as discussed in Section 5), by using la-

beling functions of LF B1(X), is complementary to the baseline BigGAN model

which is trained on real labeled images. The “SSADP” row on Table 2 shows

the experimental results. In particular, an improvement of MIS, FID, CRT and

CRG w.r.t imply show the eﬃcacy of the SS-ADP method.

6.5.2.ZS-ADP : ZS-ADP is evaluated using three evaluation metrics, such as:

(i) Image-Label Correspondence of Zero-Shot Classes; and (ii) Image quality,

and the results are shown in Table 3 (A) and (B).

6.5.2.1.Image to label Correspondence of Zero Shot Classes: We performed: (i)

HTT, i.e. Human Turing Test: 40 experts were asked to rate the image to

label correspondence of 400 zero shot labeled images. Experts were given a

score on a scale of 1-10, and the aggregated result is shown in Table 2. (ii) CRT ,

i.e. Classiﬁcation Score: Top-1 classiﬁcation performance of ResNet-50 classiﬁer

trained on real labeled images and tested on generated zero-shot labeled images.

2. Image Quality: Image quality is evaluated using the FID score [56]. The

results are shown in Table 3 (B). We observed our method performs fairly well

28

and secures a good FID for image quality.

7. Discussion and Analysis

7.1. Optimal Number of Labeling Functions

We trained ADP using diﬀerent number of labeling functions. Table 4 sug-

gests that 10-15 labeling functions provides the best performance. We report

the test time cross-entropy error of a pretrained ResNet model with image-label

pairs generated by ADP.

7.2. Comparison against Vote Aggregation Methods

We compared ADP, both with majority voting and Data Programming (DP,

[2]). We studied the test-time classiﬁcation cross-entropy loss of a pre-trained

ResNet model on image-label pairs generated by ADP, ADP (only its Image-

GAN component) with majority voting and DP. The results are presented in

Figure 7a.

7.3. Adversarial Data Programming vs MLE-based Data Programming:

To further quantify the beneﬁts of our ADP, we also show how our method

compares against Data Programming (DP) [2] using diﬀerent variants of MLE:

MLE, Maximum Pseudo likelihood, and Hamiltonian Monte Carlo. Figure 7b

presents the results and shows that ADP is almost 100X faster than MLE-based

estimation. Figure 8 also shows sample images generated by the vanilla GAN,

along with the corresponding label assigned by MLE-based DP using the same

labeling functions as used in our work.

8. Conclusions

Paucity of large curated hand-labeled training data forms a major bottle-

neck in deploying machine learning methods in practice on varied application

domains. Standard data augmentation techniques are often limited in their

scope. Our proposed Adversarial Data Programming (ADP) framework learns

the joint data-label distribution eﬀectively using a set of weakly deﬁned labeling

29

functions. The method shows promise on standard datasets, as well as in trans-

fer learning and multi-task learning. We also extended the methodology to a

generalized zero-shot labeled image generation task, and show its promise. Our

future work will involve understanding the theoretical implications of this new

framework from a game-theoretic perspective, as well as explore the performance

of the method on more complex datasets.

References

[1] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, A. Y. Ng, Reading

digits in natural images with unsupervised feature learning, in: NIPS work-

shop on deep learning and unsupervised feature learning, Vol. 2011, 2011,

p. 5.

[2] A. J. Ratner, C. M. De Sa, S. Wu, D. Selsam, C. R´e, Data programming:

Creating large training sets, quickly, in: Advances in Neural Information

Processing Systems, 2016, pp. 3567–3575.

[3] I. Danihelka, B. Lakshminarayanan, B. Uria, D. Wierstra, P. Dayan, Com-

parison of maximum likelihood and gan-based training of real nvps, arXiv

preprint arXiv:1705.05263.

[4] L. Theis, A. v. d. Oord, M. Bethge, A note on the evaluation of generative

models, arXiv preprint arXiv:1511.01844.

[5] A. Krizhevsky, V. Nair, G. Hinton, The cifar-10 dataset, online:

http://www. cs. toronto. edu/kriz/cifar. html 55.

[6] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, R. M. Summers, Chestx-ray8:

Hospital-scale chest x-ray database and benchmarks on weakly-supervised

classiﬁcation and localization of common thorax diseases, in: Proceedings

of the IEEE Conference on Computer Vision and Pattern Recognition,

2017, pp. 2097–2106.

30

[7] F. Yu, A. Seﬀ, Y. Zhang, S. Song, T. Funkhouser, J. Xiao, Lsun: Construc-

tion of a large-scale image dataset using deep learning with humans in the

loop, arXiv preprint arXiv:1506.03365.

[8] Y. LeCun, L. Bottou, Y. Bengio, P. Haﬀner, Gradient-based learning ap-

plied to document recognition, Proceedings of the IEEE 86 (11) (1998)

2278–2324.

[9] M.-E. Nilsback, A. Zisserman, Automated ﬂower classiﬁcation over a large

number of classes, in: 2008 Sixth Indian Conference on Computer Vision,

Graphics & Image Processing, IEEE, 2008, pp. 722–729.

[10] C. Wah, S. Branson, P. Welinder, P. Perona, S. Belongie, The Caltech-

UCSD Birds-200-2011 Dataset, Tech. Rep. CNS-TR-2011-001, California

Institute of Technology (2011).

[11] T. Chen, X. Zhai, M. Ritter, M. Lucic, N. Houlsby, Self-supervised gans

via auxiliary rotation loss, in: Proceedings of the IEEE Conference on

Computer Vision and Pattern Recognition, 2019, pp. 12154–12163.

[12] C. H. Lampert, H. Nickisch, S. Harmeling, Learning to detect unseen ob-

ject classes by between-class attribute transfer, in: Computer Vision and

Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, IEEE, 2009,

pp. 951–958.

[13] J. A. Fries, P. Varma, V. S. Chen, K. Xiao, H. Tejeda, P. Saha, J. Dunnmon,

H. Chubb, S. Maskatia, M. Fiterau, et al., Weakly supervised classiﬁcation

of aortic valve malformations using unlabeled cardiac mri sequences, Nature

communications 10 (1) (2019) 1–10.

[14] A. Ratner, B. Hancock, J. Dunnmon, R. Goldman, C. R´e, Snorkel metal:

Weak supervision for multi-task learning, in: Proceedings of the Second

Workshop on Data Management for End-To-End Machine Learning, ACM,

2018, p. 3.

31

[15] A. J. Ratner, H. R. Ehrenberg, Z. Hussain, J. Dunnmon, C. R´e, Learning

to compose domain-speciﬁc transformations for data augmentation, arXiv

preprint arXiv:1709.01643.

[16] Y. Pu, S. Dai, Z. Gan, W. Wang, G. Wang, Y. Zhang, R. Henao, L. Carin,

Jointgan: Multi-domain joint distribution learning with generative adver-

sarial nets, arXiv preprint arXiv:1806.02978.

[17] M. Lucic, M. Tschannen, M. Ritter, X. Zhai, O. Bachem, S. Gelly,

High-ﬁdelity image generation with fewer

labels,

arXiv preprint

arXiv:1903.02271.

[18] H. Zhang, P. Koniusz, Model selection for generalized zero-shot learning,

in: Proceedings of the European Conference on Computer Vision (ECCV),

2018, pp. 0–0.

[19] S. Liu, M. Long, J. Wang, M. I. Jordan, Generalized zero-shot learning with

deep calibration network, in: Advances in Neural Information Processing

Systems, 2018, pp. 2005–2015.

[20] F. Zhang, G. Shi, Co-representation network for generalized zero-shot learn-

ing, in: International Conference on Machine Learning, 2019, pp. 7434–

7443.

[21] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, H. Lee, Generative

adversarial text to image synthesis, arXiv preprint arXiv:1605.05396.

[22] H. Zhang, T. Xu, H. Li, S. Zhang, X. Huang, X. Wang, D. Metaxas, Stack-

gan: Text to photo-realistic image synthesis with stacked generative adver-

sarial networks, arXiv preprint arXiv:1612.03242.

[23] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, D. Metaxas, Stack-

gan++: Realistic image synthesis with stacked generative adversarial net-

works, arXiv preprint arXiv:1710.10916.

32

[24] T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, X. He, Attngan:

Fine-grained text to image generation with attentional generative adversar-

ial networks, in: Proceedings of the IEEE Conference on Computer Vision

and Pattern Recognition, 2018, pp. 1316–1324.

[25] Z. Zhang, Y. Xie, L. Yang, Photographic text-to-image synthesis with a

hierarchically-nested adversarial network, in: Proceedings of the IEEE Con-

ference on Computer Vision and Pattern Recognition, 2018, pp. 6199–6208.

[26] I. Goodfellow, Nips 2016 tutorial: Generative adversarial networks, arXiv

preprint arXiv:1701.00160.

[27] J. Donahue, P. Kr¨ahenb¨uhl, T. Darrell, Adversarial feature learning, arXiv

preprint arXiv:1605.09782.

[28] M. Caron, P. Bojanowski, A. Joulin, M. Douze, Deep clustering for un-

supervised learning of visual features, in: Proceedings of the European

Conference on Computer Vision (ECCV), 2018, pp. 132–149.

[29] S. Mandal, S. Sur, A. Dan, P. Bhowmick, Handwritten bangla character

recognition in machine-printed forms using gradient information and haar

wavelet, in: Image Information Processing (ICIIP), 2011 International Con-

ference on, IEEE, 2011, pp. 1–6.

[30] E. Alfonseca, K. Filippova, J.-Y. Delort, G. Garrido, Pattern learning for

relation extraction with a hierarchical topic model, in: Proceedings of the

50th Annual Meeting of the Association for Computational Linguistics:

Short Papers-Volume 2, Association for Computational Linguistics, 2012,

pp. 54–59.

[31] C. Barnes, E. Shechtman, A. Finkelstein, D. B. Goldman, Patchmatch:

A randomized correspondence algorithm for structural image editing, in:

ACM Transactions on Graphics (ToG), Vol. 28, ACM, 2009, p. 24.

33

[32] B. Sahu, P. K. Sa, S. Bakshi, A. K. Sangaiah, Reducing dense local feature

key-points for faster iris recognition, Computers & Electrical Engineering

70 (2018) 939–949.

[33] A. Oliva, A. Torralba, Modeling the shape of the scene: A holistic repre-

sentation of the spatial envelope, International journal of computer vision

42 (3) (2001) 145–175.

[34] F. Plourde, F. Cheriet, J. Dansereau, Semi-automatic detection of scoli-

otic rib borders using chest radiographs., Studies in health technology and

informatics 123 (2006) 533–537.

[35] F. Vogelsang, F. Weiler, J. Dahmen, M. W. Kilbinger, B. B. Wein, R. W.

Guenther, Detection and compensation of rib structures in chest radio-

graphs for diagnostic assistance, in: Medical Imaging 1998: Image Process-

ing, Vol. 3338, International Society for Optics and Photonics, 1998, pp.

774–786.

[36] J.-I. Toriwaki, Y. Suenaga, T. Negoro, T. Fukumura, Pattern recognition

of chest x-ray images, Computer Graphics and Image Processing 2 (3-4)

(1973) 252–271.

[37] B. Van Ginneken, B. T. H. Romeny, M. A. Viergever, Computer-aided

diagnosis in chest radiography: a survey, IEEE Transactions on medical

imaging 20 (12) (2001) 1228–1241.

[38] B. B. Ogul, E. S¨umer, H. Ogul, Unsupervised rib delineation in chest radio-

graphs by an integrative approach, in: International Conference on Com-

puter Vision Theory and Applications, Vol. 2, SCITEPRESS, 2015, pp.

260–265.

[39] D. G. Lowe, Distinctive image features from scale-invariant keypoints, In-

ternational journal of computer vision 60 (2) (2004) 91–110.

34

[40] F. Lin, W. W. Cohen, Power iteration clustering, in: ICML, 2010, pp. 655–

662.

URL https://icml.cc/Conferences/2010/papers/387.pdf

[41] G. Csurka, C. Dance, L. Fan, J. Willamowski, C. Bray, Visual catego-

rization with bags of keypoints, in: Workshop on statistical learning in

computer vision, ECCV, Vol. 1, Prague, 2004, pp. 1–2.

[42] J. Xie, R. Girshick, A. Farhadi, Unsupervised deep embedding for clustering

analysis, in: International conference on machine learning, 2016, pp. 478–

487.

[43] J. Yang, D. Parikh, D. Batra, Joint unsupervised learning of deep repre-

sentations and image clusters, in: Proceedings of the IEEE Conference on

Computer Vision and Pattern Recognition, 2016, pp. 5147–5156.

[44] M. A. Bautista, A. Sanakoyeu, E. Tikhoncheva, B. Ommer, Cliquecnn:

Deep unsupervised exemplar learning, in: Advances in Neural Information

Processing Systems, 2016, pp. 3846–3854.

[45] A. Dosovitskiy, J. T. Springenberg, M. Riedmiller, T. Brox, Discrimina-

tive unsupervised feature learning with convolutional neural networks, in:

Advances in neural information processing systems, 2014, pp. 766–774.

[46] R. Liao, A. Schwing, R. Zemel, R. Urtasun, Learning deep parsimonious

representations, in: Advances in Neural Information Processing Systems,

2016, pp. 5076–5084.

[47] H. Xiao, K. Rasul, R. Vollgraf, Fashion-mnist:

a novel

image

dataset for benchmarking machine learning algorithms, arXiv preprint

arXiv:1708.07747.

[48] D. Yoo, N. Kim, S. Park, A. S. Paek, I. S. Kweon, Pixel-level domain

transfer, in: European Conference on Computer Vision, Springer, 2016,

pp. 517–532.

35

[49] Z. Al-Halah, R. Stiefelhagen, Automatic discovery, association estimation

and learning of semantic attributes for a thousand categories, in: Proceed-

ings of the IEEE Conference on Computer Vision and Pattern Recognition,

2017, pp. 614–623.

[50] S. Reed, Z. Akata, H. Lee, B. Schiele, Learning deep representations of

ﬁne-grained visual descriptions, in: Proceedings of the IEEE Conference

on Computer Vision and Pattern Recognition, 2016, pp. 49–58.

[51] A. Brock, J. Donahue, K. Simonyan, Large scale gan training for high

ﬁdelity natural image synthesis, arXiv preprint arXiv:1809.11096.

[52] K. Lee, W. Xu, F. Fan, Z. Tu, Wasserstein introspective neural networks,

in: Proceedings of the IEEE Conference on Computer Vision and Pattern

Recognition, 2018, pp. 3702–3711.

[53] A. Nguyen, J. Clune, Y. Bengio, A. Dosovitskiy, J. Yosinski, Plug & play

generative networks: Conditional iterative generation of images in latent

space, in: Proceedings of the IEEE Conference on Computer Vision and

Pattern Recognition, 2017, pp. 4467–4477.

[54] Y. Xian, S. Sharma, B. Schiele, Z. Akata, f-vaegan-d2: A feature generating

framework for any-shot learning, in: Proceedings of the IEEE Conference

on Computer Vision and Pattern Recognition, 2019, pp. 10275–10284.

[55] S. Santurkar, L. Schmidt, A. Madry, A classiﬁcation-based study of co-

variate shift in gan distributions, in: International Conference on Machine

Learning, 2018, pp. 4487–4496.

[56] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, S. Hochreiter, Gans

trained by a two time-scale update rule converge to a local nash equilibrium,

in: Advances in neural information processing systems, 2017, pp. 6626–

6637.

36

