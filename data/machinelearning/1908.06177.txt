CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text

Koustuv Sinha 1,3,4, Shagun Sodhani 2,3, Jin Dong 1,3,
Joelle Pineau 1,3,4 and William L. Hamilton 1,3,4
1 School of Computer Science, McGill University, Canada
2 Universit´e de Montr´eal, Canada
3 Montreal Institute of Learning Algorithms (Mila), Canada
4 Facebook AI Research (FAIR), Montreal, Canada
{koustuv.sinha, sshagunsodhani, jin.dong, jpineau, wlh}
@{mail.mcgill.ca, gmail.com, mail.mcgill.ca, cs.mcgill.ca, cs.mcgill.ca}

9
1
0
2

p
e
S
4

]

G
L
.
s
c
[

2
v
7
7
1
6
0
.
8
0
9
1
:
v
i
X
r
a

Abstract

The recent success of natural language under-
standing (NLU) systems has been troubled by
results highlighting the failure of these mod-
els to generalize in a systematic and robust
way. In this work, we introduce a diagnostic
benchmark suite, named CLUTRR, to clarify
some key issues related to the robustness and
systematicity of NLU systems. Motivated by
classic work on inductive logic programming,
CLUTRR requires that an NLU system infer
kinship relations between characters in short
stories. Successful performance on this task
requires both extracting relationships between
entities, as well as inferring the logical rules
governing these relationships. CLUTRR al-
lows us to precisely measure a model’s abil-
ity for systematic generalization by evaluat-
ing on held-out combinations of logical rules,
and it allows us to evaluate a model’s robust-
ness by adding curated noise facts. Our em-
pirical results highlight a substantial perfor-
mance gap between state-of-the-art NLU mod-
els (e.g., BERT and MAC) and a graph neu-
ral network model that works directly with
symbolic inputs—with the graph-based model
exhibiting both stronger generalization and
greater robustness.

1

Introduction

Natural language understanding (NLU) systems
have been extremely successful at reading compre-
hension tasks, such as question answering (QA)
and natural language inference (NLI). An array of
existing datasets are available for these tasks. This
includes datasets that test a system’s ability to ex-
tract factual answers from text (Rajpurkar et al.,
2016; Nguyen et al., 2016; Trischler et al., 2016;
Mostafazadeh et al., 2016; Su et al., 2016), as well
as datasets that emphasize commonsense inference,
such as entailment between sentences (Bowman
et al., 2015; Williams et al., 2018).

Figure 1: CLUTRR inductive reasoning task.

However, there are growing concerns regarding
the ability of NLU systems—and neural networks
more generally—to generalize in a systematic and
robust way (Bahdanau et al., 2019; Lake and Ba-
roni, 2018; Johnson et al., 2017). For instance,
recent work has highlighted the brittleness of NLU
systems to adversarial examples (Jia and Liang,
2017), as well as the fact that NLU models tend to
exploit statistical artifacts in datasets, rather than
exhibiting true reasoning and generalization capa-
bilities (Gururangan et al., 2018; Kaushik and Lip-
ton, 2018). These ﬁndings have also dovetailed
with the recent dominance of large pre-trained lan-
guage models, such as BERT, on NLU benchmarks
(Devlin et al., 2018; Peters et al., 2018), which sug-
gest that the primary difﬁculty in these datasets is
incorporating the statistics of the natural language,
rather than reasoning.

An important challenge is thus to develop NLU
benchmarks that can precisely test a model’s capa-
bility for robust and systematic generalization. Ide-
ally, we want language understanding systems that
can not only answer questions and draw inferences
from text, but that can also do so in a systematic,
logical, and robust way. While such reasoning ca-
pabilities are certainly required for many existing
NLU tasks, most datasets combine several chal-
lenges of language understanding into one, such as
co-reference/entity resolution, incorporating world
knowledge, and semantic parsing—making it difﬁ-
cult to isolate and diagnose a model’s capabilities
for systematic generalization and robustness.

 
 
 
 
 
 
Our work. Inspired by the classic AI challenge
of inductive logic programming (Quinlan, 1990)—
as well as the recently developed CLEVR dataset
for visual reasoning (Johnson et al., 2017)—we
propose a semi-synthetic benchmark designed to
explicitly test an NLU model’s ability for system-
atic and robust logical generalization.

Our

benchmark

suite—termed CLUTRR
(Compositional Language Understanding and
Text-based Relational Reasoning)—contains
a large set of semi-synthetic stories involving
hypothetical families. Given a story, the goal
is to infer the relationship between two family
members, whose relationship is not explicitly
mentioned (Figure 1). To solve this task, a learning
agent must extract the relationships mentioned
in the text, induce the logical rules governing
the kinship relationships (e.g., the transitivity of
the sibling relation), and use these rules to infer
the relationship between a given pair of entities.
Crucially,
the CLUTRR benchmark allows us
to test a learning agent’s ability for systematic
generalization by testing on stories that contain
unseen combinations of logical rules. CLUTRR
also allows us to precisely test for the various
forms of model robustness by adding different
kinds of superﬂuous noise facts to the stories.

We compare the performance of several state-
of-the-art NLU systems on this task—including
Relation Networks (Santoro et al., 2017), Compo-
sitional Attention Networks (Hudson and Manning,
2018) and BERT (Devlin et al., 2018). We ﬁnd that
the generalization ability of these NLU systems
is substantially below that of a Graph Attention
Network (Veliˇckovi´c et al., 2018), which is given
direct access to symbolic representations of the sto-
ries. Moreover, we ﬁnd that the robustness of the
NLU systems generally does not improve by train-
ing on noisy data, whereas the GAT model is able
to effectively learn robust reasoning strategies by
training on noisy examples. Both of these results
highlight important open challenges for closing the
gap between machine reasoning models that work
with unstructured text and models that are given
access to more structured input.

2 Related Work

We draw inspiration from the classic work on induc-
tive logic programming (ILP), a long line of read-
ing comprehension benchmarks in NLP, as well as
work combining language and knowledge graphs.

Reading comprehension benchmarks. Many
datasets have been proposed to test the reading com-
prehension ability of NLP systems. This includes
the SQuAD (Rajpurkar et al., 2016), NewsQA
(Trischler et al., 2016), and MCTest (Richardson
et al., 2013) benchmarks that focus on factual
questions; the SNLI (Bowman et al., 2015) and
MultiNLI (Williams et al., 2018) benchmarks for
sentence understanding; and the bABI tasks (We-
ston et al., 2015), to name a few. Our primary
contribution to this line of work is the development
of a carefully designed diagnostic benchmark to
evaluate model robustness and systematic general-
ization in the context of NLU.
Question-answering with knowledge graphs.
Our work is also related to the domain of ques-
tion answering and reasoning in knowledge graphs
(Das et al., 2018; Xiong et al., 2018; Hamilton
et al., 2018; Wang et al., 2018; Xiong et al., 2017;
Welbl et al., 2018; Kartsaklis et al., 2018), where
either the model is provided with a knowledge
graph to perform inference over or where the model
must infer a knowledge graph from the text it-
self. However, unlike previous benchmarks in this
domain—which are generally transductive and fo-
cus on leveraging and extracting knowledge graphs
as a source of background knowledge about a ﬁxed
set of entities—CLUTRR requires inductive logical
reasoning, where every example requires reasoning
over a new set of previously unseen entities.

3 Benchmark Design

In order to design an NLU benchmark that explic-
itly tests inductive reasoning and systematic gen-
eralization, we build upon the classic ILP task of
inferring family (i.e., kinship) relations (Hinton
et al., 1986; Muggleton, 1991; Lavrac and Dze-
roski, 1994; Kok and Domingos, 2007; Rockt¨aschel
and Riedel, 2017). For example, given the facts
that “Alice is Bob’s mother” and “Jim is Alice’s
father”, one can infer with reasonable certainty
that “Jim is Bob’s grandfather.” While this exam-
ple may appear trivial, it is a challenging task to
design models that can learn from data to induce
the logical rules necessary to make such inferences,
and it is even more challenging to design models
that can systematically generalize by composing
these induced rules.

Inspired by this classic task of logical induction
and reasoning, the CLUTRR benchmark requires
an NLU system to infer and reason about kinship

Step 3. Apply backward chaining to sample a set
of facts that can prove the target relation
(and optionally sample a set of “distracting”
or “irrelevant” noise facts).

Step 4. Convert the sampled facts into a natural
language story through pre-speciﬁed text
templates and crowd-sourced paraphrasing.

Figure 2 provides a high-level overview of this
idea, and the following subsections describe the
data generation process in detail, as well as the
diagnostic ﬂexibility afforded by CLUTRR.

3.2 Story generation

The short stories in CLUTRR are essentially narra-
tivized renderings of a set of logical facts. In this
section, we describe how we sample the logical
facts that make up a story by generating random
kinship graphs and using backward chaining to
produce logical reasoning chains. The conversion
from logical facts to natural language narratives is
then described in Section 3.3.
Terminology and background.
Following
standard practice in formal semantics, we use the
term atom to refer to a predicate symbol and a
list of terms, such as [grandfatherOf, X, Y ],
where the predicate grandfatherOf denotes
the relation between the two variables, X and Y .
We restrict the predicates to have an arity of 2, i.e.,
binary predicates. A logical rule in this setting
is of the form H (cid:96) B, where B is the body of the
rule, i.e., a conjunction of two atoms ([α1, α2])
and H is the head, i.e., a single atom (α) that
can be viewed as the goal or query. For instance,
given a knowledge base (KB) R that contains
the single rule [grandfatherOf, X, Y ] (cid:96)
[[fatherOf, X, Z], [fatherOf, Z, Y ]],
[grandfatherOf, X, Y ]
the
eval-
the body
and only if
uates
B = [[fatherOf, X, Z], [fatherOf, Z, Y ]]
is also true in a given world. A rule is called a
grounded rule if all atoms in the rule are themselves
grounded, i.e., all variables are replaced with con-
stants or entities in a world. A fact is a grounded
binary predicate. A clause is a conjunction of two
or more atoms (C = (HC (cid:96) BC = ([α1, ..., αn])))
which can be built using a set of rules.

to true

query

if

In the context of our data generation process,
we distinguish between the knowledge base, R,
which contains a ﬁnite number of predicates and
rules specifying how kinship relations in a family
resolve, and a particular kinship graph G, which

Figure 2: Data generation pipeline. Step 1: generate
a kinship graph. Step 2: sample a target fact. Step 3:
Use backward chaining to sample a set of facts. Step 4:
Convert sampled facts to a natural language story.

relations by reading short stories. Requiring that
the models learn directly from natural language
makes this task much more challenging than the
purely symbolic ILP setting. However, we lever-
age insights from traditional ILP to generate these
stories in a semi-synthetic manner, providing pre-
cise control over the complexity of the reasoning
required to solve the task.

In its entirety, the CLUTRR benchmark suite al-
lows researchers to generate diverse semi-synthetic
short stories to test different aspects of inductive
reasoning capabilities. We publicly release the en-
tire benchmark suite, including code to generate
the semi-synthetic examples, the speciﬁc datasets
that we introduce here, and the different baselines
that we compare with.1

3.1 Overview of data generation process

The core idea behind the CLUTRR benchmark
suite is the following: Given a natural language
story describing a set of kinship relations, the goal
is to infer the relationship between two entities,
whose relationship is not explicitly stated in the
story. To generate these stories, we ﬁrst design a
knowledge base (KB) with rules specifying how
kinship relations resolve, and we use the following
steps to create semi-synthetic stories based on this
knowledge base:

Step 1. Generate a random kinship graph that sat-

isﬁes the rules in our KB.

Step 2. Sample a target fact (i.e., relation) to pre-

dict from the kinship graph.

1Benchmark suite

code

https://github.com/facebookresearch/clutrr.
datasets are available to view in this link.

can be obtained from
Generated

contains a grounded set of atoms specifying the
particular kinship relations that underlie a single
story. In other words, R contains the logical rules
that govern all the generated stories in CLUTRR,
while G contains the grounded facts that underlie a
speciﬁc story.
Graph generation. To generate the kinship graph
G underlying a particular story, we ﬁrst sample a
set of gendered2 entities and kinship relations using
a stochastic generation process. This generation
process contains a number of tunable parameters—
such as the maximum number of children at each
node, the probability of an entity being married to
another entity, etc.—and is designed to produce a
valid, but possibly incomplete “backbone graph”.
For instance, this backbone graph generation pro-
cess will specify “parent”/“child” relations between
entities but does not add “grandparent” relations.
After this initial generation process, we recursively
apply the logical rules in R to the backbone graph
to produce a ﬁnal graph G that contains the full set
of kinship relations between all the entities.
Backward chaining. The resulting graph G pro-
vides the background knowledge for a speciﬁc story,
as each edge in this graph can be treated as a
grounded predicate (i.e., fact) between two entities.
From this graph G, we sample the facts that make
up the story, as well as the target fact that we seek
to predict: First, we (uniformly) sample a target re-
lation HC, which is the fact that we want to predict
from the story. Then, from this target relation HC,
we run a simple variation of the backward chaining
(Gallaire and Minker, 1978) algorithm for k itera-
tions starting from HC, where at each iteration we
uniformly sample a subgoal to resolve and then uni-
formly sample a KB rule that resolves this subgoal.
Crucially, unlike traditional backward chaining, we
do not stop the algorithm when a proof is obtained;
instead, we run for a ﬁxed number of iterations k
in order to sample a set of k facts BC that imply the
target relation HC.

3.3 Adding natural language

So far, we have described the process of generat-
ing a conjunctive logical clause C = (HC (cid:96) BC),
where HC = [α∗] is the target fact (i.e., relation) we
seek to predict and BC = [α1, ..., αk] is the set of
supporting facts that imply the target relation. We
now describe how we convert this logical represen-

Figure 3: Illustration of how a set of facts can split and
combined in various ways across sentences.

tation to natural language through crowd-sourcing.
Paraphrasing using Amazon Mechanical Turk.
The basic idea behind our approach is that we show
Amazon Mechanical Turk (AMT) crowd-workers
the set of facts BC corresponding to a story and ask
the workers to paraphrase these facts into a narra-
tive. Since workers are given a set of facts BC to
work from, they are able to combine and split mul-
tiple facts across separate sentences and construct
diverse narratives (Figure 3). Appendix 1.6 con-
tains further details on our AMT interface (based
on the ParlAI framework (Miller et al., 2017)), data
collection, and the quality controls we employed.
Reusability and composition. One challenge for
data collection via AMT is that the number of possi-
ble stories generated by CLUTRR grows combina-
torially as the number of supporting facts increases,
i.e., as k = |BC| grows. This combinatorial ex-
plosion for large k—combined with the difﬁculty
of maintaining the quality of the crowd-sourced
paraphrasing for long stories—makes it infeasible
to obtain a large number of paraphrased examples
for k > 3. To circumvent this issue and increase
the ﬂexibility of our benchmark, we reuse and com-
pose AMT paraphrases to generate longer stories.
In particular, we collected paraphrases for stories
containing k = 1, 2, 3 supporting facts and then
replaced the entities from these collected stories
with placeholders in order to re-use them to gener-
ate longer semi-synthetic stories. An example of
a story generated by stitching together two shorter
paraphrases is provided below:

[Frank] went to the park with his father, [Brett].

[Frank] called his brother [Boyd] on the phone.

He wanted to go out for some beers. [Boyd] went

to the baseball game with his son [Jim].

Q: What is [Brett] and [Jim]’s relationship?

2Kinship and gender roles are oversimpliﬁed in our data

(compared to the real world) to maintain tractability.

Thus, instead of simply collecting paraphrases for a
ﬁxed number of stories, we instead obtain a diverse

Table 1: Statistics of the AMT paraphrases. Jaccard
word overlap is calculated within the templates of each
individual clause of length k.

Number of Paraphrases

# clauses

20
58
236

k = 1
k = 2
k = 3

1,868
1,890
2,258

Total

6,016

3,797

Unique Word Count

these two entities. This representation avoids the
pitfall of revealing information about the answer in
the question (Kaushik and Lipton, 2018).
Representing entities. When generating stories,
entity names are randomly drawn from a set of 300
common gendered English names. Thus, depend-
ing on each run, the entities are never the same.
This ensures that the entity names are simply place-
holders and uncorrelated from the task.

Jaccard Word Overlap Unigrams
Bigrams

0.201
0.0385

3.5 Variants of CLUTRR

collection of natural language templates that can be
programmatically recombined to generate stories
with various properties.
Dataset statistics. At the time of submission, we
have collected 6,016 unique paraphrases with an
average of 19 paraphrases for every possible logical
clause of length k = 1, 2, 3. Table 1 contains sum-
mary statistics of the collected paraphrases. Over-
all, we found high linguistic diversity in the col-
lected paraphrases. For instance, the average Jac-
card overlap in unigrams between pairs paraphrases
corresponding to the same logical clause was only
0.201 and only 0.0385 for bigrams. The Appendix
contains further examples of the paraphrases.
Human performance. To get a sense of the data
quality and difﬁculty involved in CLUTRR, we
asked human annotators to solve the task for ran-
dom examples of length k = 2, 3, ..., 6. We found
that time-constrained AMT annotators performed
well (i.e., > 70%) accuracy for k ≤ 3 but struggled
with examples involving longer stories, achieving
40-50% accuracy for k > 3. However, trained an-
notators with unlimited time were able to solve
100% of the examples (Appendix 1.7), highlight-
ing the fact that this task requires attention and
involved reasoning, even for humans.

3.4 Query representation and inference

Representing the question. The AMT paraphras-
ing approach described above allows us to convert
the set of supporting facts BC to a natural language
story, which can be used to predict the target rela-
tion/query HC. However, instead of converting the
target query, HC = [α∗], to a natural language ques-
tion, we instead opt to represent the target query as
a K-way classiﬁcation task, where the two entities
in the target relation are provided as input and the
goal is to classify the relation that holds between

The modular nature of CLUTRR provides rich di-
agnostic capabilities for evaluating the robustness
and generalization abilities of neural language un-
derstanding systems. We highlight some key diag-
nostic capabilities available via different variations
of CLUTRR below. These diagnostic variations
correspond to the concrete datasets that we gener-
ated in this work, and we describe the results on
these Datasets in Section 4.
Systematic generalization. Most prominently,
CLUTRR allows us to explicitly evaluate a model’s
ability for systematic generalization. In particular,
we rely on the following hold-out procedures to
test systematic generalization:

• During training, we hold out a subset of the col-
lected paraphrases, and we only use this held-out
subset of paraphrases when generating the test
set. Thus, to succeed on CLUTRR, an NLU sys-
tem must exhibit linguistic generalization and be
robust to linguistic variation at test time.

• We also hold out a subset of the logical clauses
during training (for clauses of length k > 2).3 In
other words, during training, the model sees all
logical rules but does not see all combinations of
these logical rules. Thus, in addition to linguistic
generalization, success on this task also requires
logical generalization.

• Lastly, as a more extreme form of both logical
and linguistic generalization, we consider the
setting where the models are trained on stories
generated from clauses of length ≤ k and evalu-
ated on stories generated from larger clauses of
length > k. Thus, we explicitly test the ability
for models to generalize on examples that re-
quire more steps of reasoning that any example
they encountered during training.

3One should not holdout clauses from length k = 2 in
order to allow models to learn the compositionality of all
possible binary predicates.

(Q1) How do state-of-the-art NLU models com-
pare in terms of systematic generalization?
Can these models generalize to stories with
unseen combinations of logical rules?
(Q2) How does the performance of neural lan-
guage understanding models compare to a
graph neural network that has full access to
graph structure underlying the stories?
(Q3) How robust are these models to the addition

of noise facts to a given story?

4.1 Baselines

Our primary baselines are neural language under-
standing models that take unstructured text as in-
put. We consider bidirectional LSTMs (Hochreiter
and Schmidhuber, 1997; Cho et al., 2014) (with
and without attention), as well as recently pro-
posed models that aim to incorporate inductive
biases towards relational reasoning: Relation Net-
works (RN) (Santoro et al., 2017) and Composi-
tional Memory Attention Network (MAC) (Hud-
son and Manning, 2018). We also use the large
pretrained language model, BERT (Devlin et al.,
2018), as well as a modiﬁed version of BERT hav-
ing a trainable LSTM encoder on top of the pre-
trained BERT embeddings. All of these models
(except BERT) were re-implemented in PyTorch
1.0 (Paszke et al., 2017) and adapted to work with
the CLUTRR benchmark.

Since the underlying relations in the stories gen-
erated by CLUTRR inherently form a graph, we
also experiment with a Graph Attention Network
(GAT) (Veliˇckovi´c et al., 2018). Rather than taking
the textual stories as input, the GAT baseline re-
ceives a structured graph representation of the facts
that underlie the story.
Entity and query representations. We use the
various baseline models to encode the natural lan-
guage story (or graph) into a ﬁxed-dimensional em-
bedding. With the exception of the BERT models,
we do not use pre-trained word embeddings and
learn the word embeddings from scratch using end-
to-end backpropagation. An important note, how-
ever, is that we perform Cloze-style anonymization
(Hermann et al., 2015) of the entities (i.e., names)
in the stories, where each entity name is replaced
by a @entity-k placeholder, which is randomly sam-
pled from a small, ﬁxed pool of placeholder tokens.
The embeddings for these placeholders are ran-
domly initialized and ﬁxed during training.5

5See Appendix 1.5 for a comparison of placeholder em-

Figure 4: Noise generation procedures of CLUTRR.

Robust Reasoning.
In addition to evaluating
systematic generalization, the modular setup of
CLUTRR also allows us to diagnose model robust-
ness by adding noise facts to the generated narra-
tives. Due to the controlled semi-synthetic nature
of CLUTRR, we are able to provide a precise tax-
onomy of the kinds of noise facts that can be added
(Figure 4). In order to structure this taxonomy, it is
important to recall that any set of supporting facts
BC generated by CLUTRR can be interpreted as
a path, pC, in the corresponding kinship graph G
(Figure 2). Based on this interpretation, we view
adding noise facts from the perspective of sampling
three different types of noise paths, pn, from the
kinship graph G:
• Irrelevant facts: We add a path pn, which has
exactly one shared end-point with pc. In this way,
this is a distractor path, which contains facts that
are connected to one of the entities in the target
relation, HC, but do not provide any information
that could be used to help answer the query.
• Supporting facts: We add a path pn, whose two
end-points are on the path pC. The facts on this
path pn are noise because they are not needed to
answer the query, but they are supporting facts
because they can, in principle, be used to con-
struct alternative (longer) reasoning paths that
connect the two target entities.

• Disconnected facts: We add paths which neither
originate nor end in any entity on pc. These
disconnected facts involve entities and relations
that are completely unrelated to the target query.

4 Experiments

We evaluate several neural language understanding
systems on the proposed CLUTRR benchmark to
surface the relative strengths and shortcomings of
these models in the context of inductive reason-
ing and combinatorial generalization.4 We aim to
answer the following key questions:

4Code to reproduce all the results in this section will be

released at https://github.com/facebookresearch/clutrr/.

Figure 5: Systematic generalization performance of different models when trained on clauses of length k = 2, 3
(Left) and k = 2, 3, 4 (Right).

To make a prediction about a target query given
a story, we concatenate the embedding of the story
(generated by the baseline model) with the embed-
dings of the two target entities and we feed this
concatenated embedding to a 2-layer feed-forward
neural network with a softmax prediction layer.

4.2 Experimental Setup

Hyperparameters. We selected hyperparameters
for all models using an initial grid search on the sys-
tematic generalization task (described below). All
models were trained for 100 epochs with Adam op-
timizer and a learning rate of 0.001. The Appendix
provides details on the selected hyperparameters.
Generated datasets. For all experiments, we gen-
erated datasets with 10-15k training examples. In
many experiments, we report training and testing re-
sults on stories with different clause lengths k. (For
brevity, we use the phrase “clause length” through-
out this section to refer to the value k = |BC|, i.e.,
the number of steps of reasoning that are required
to predict the target query.) In all cases, the training
set contains 5000 train stories per k value, and, dur-
ing testing, all experiments use 100 test stories per
k value. All experiments were run 10 times with
different randomly generated stories, and means
and standard errors over these 10 runs are reported.
As discussed in Section 3.5, during training we
holdout 20% of the paraphrases, as well as 10% of
the possible logical clauses.

4.3 Results and Discussion

With our experimental setup in place, we now ad-
dress the three key questions (Q1-Q3) outlined at
the beginning of Section 4.

bedding approaches.

Q1: Systematic Generalization

We begin by using CLUTRR to evaluate the abil-
ity of the baseline models to perform systematic
generalization (Q1). In this setting, we consider
two training regimes: in the ﬁrst regime, we train
all models with clauses of length k = 2, 3, and in
the second regime, we train with clauses of length
k = 2, 3, 4. We then test the generalization of these
models on test clauses of length k = 2, ..., 10.

Figure 5 illustrates the performance of different
models on this generalization task. We observe that
the GAT model is able to perform near-perfectly on
the held-out logical clauses of length k = 3, with
the BERT-LSTM being the top-performer among
the text-based models but still signiﬁcantly be-
low the GAT. Not surprisingly, the performance
of all models degrades monotonically as we in-
crease the length of the test clauses, which high-
lights the challenge of “zero-shot” systematic gen-
eralization (Lake and Baroni, 2018; Sodhani et al.,
2018). However, as expected, all models improve
on their generalization performance when trained
on k = 2, 3, 4 rather than just k = 2, 3 (Figure 5,
right). The GAT, in particular, achieves the biggest
gain by this expanded training.

Q2: The Beneﬁt of Structure

The empirical results on systematic generalization
also provide insight into how the text-based NLU
systems compare against the graph-based GAT
model that has full access to the logical graph struc-
ture underlying the stories (Q2). Indeed, the rela-
tively strong performance of the GAT model (Fig-
ure 5) suggests that the language-based models fail
to learn a robust mapping from the natural language
narratives to the underlying logical facts.

Table 2: Testing the robustness of the various models when training and testing on stories containing various types
of noise facts. The types of noise facts (supporting, irrelevant, and disconnected) are deﬁned in Section 3.5.

Training

Clean

Models

Testing

Clean
Supporting
Irrelevant
Disconnected

Supporting

Supporting

Irrelevant

Irrelevant

Disconnected Disconnected

Average

Unstructured models (no graph)

Structured model (with graph)

BiLSTM - Attention BiLSTM - Mean

RN

MAC

BERT

BERT-LSTM

0.58 ±0.05
0.76 ±0.02
0.7 ±0.15
0.49 ±0.05

0.67 ±0.06

0.51 ±0.06

0.57 ±0.07

0.61 ±0.08

0.53 ±0.05
0.64 ±0.22
0.76 ±0.02
0.45 ±0.05

0.49 ±0.06
0.58 ±0.06
0.59 ±0.06
0.5 ±0.06

0.63 ±0.08
0.71 ±0.07
0.69 ±0.05
0.59 ±0.05

0.37 ±0.06
0.28 ±0.1
0.24 ±0.08
0.24 ±0.08

0.67 ±0.03
0.66 ±0.06
0.55 ±0.03
0.5 ±0.06

0.66 ±0.07

0.68 ±0.05

0.65 ±0.04

0.32 ±0.09

0.57 ±0.04

0.52 ±0.06

0.5 ±0.04

0.56 ±0.04

0.25 ±0.06

0.53 ±0.06

0.57 ±0.06

0.45 ±0.11

0.4 ±0.1

0.17 ±0.05

0.47 ±0.06

0.59 ±0.08

0.54 ±0.07

0.61 ±0.06

0.30 ±0.07

0.56 ±0.05

GAT

1.0 ±0.0
0.24 ±0.2
0.51 ±0.15
0.8 ±0.17

0.98 ±0.01

0.93 ±0.01

0.96 ±0.01

0.77 ±0.09

To further conﬁrm this trend, we ran experiments
with modiﬁed train and test splits for the text-based
models, where the same set of natural language
paraphrases were used to construct the narratives in
both the train and test splits (see Appendix 1.3 for
details). In this simpliﬁed setting, the text-based
models must still learn to reason about held-out log-
ical patterns, but the difﬁculty of parsing the natural
language is essentially removed, as the same nat-
ural language paraphrases are used during testing
and training. We found that the text-based models
were competitive with the GAT model in this sim-
pliﬁed setting (Appendix Figure 1), conﬁrming that
the poor performance of the text-based models on
the main task is driven by the difﬁculty of parsing
the unseen natural language narratives.

Q3: Robust Reasoning
Finally, we use CLUTRR to systematically eval-
uate how various baseline neural language under-
standing systems cope with noise (Q3). In all the
experiments we provide a combination of k = 2
and k = 3 length clauses in training and testing,
with noise facts being added to the train and/or test
set depending on the setting (Table 2). We use the
different types of noise facts deﬁned in Section 3.5.
Overall, we ﬁnd that the GAT baseline outper-
forms the unstructured text-based models across
most testing scenarios (Table 2), which showcases
the beneﬁt of a structured feature space for robust
reasoning. When training on clean data and testing
on noisy data, we observe two interesting trends
that highlight the beneﬁts and shortcomings of the
various model classes:
1. All the text-based models excluding BERT ac-
tually perform better when testing on examples
that have supporting or irrelevant facts added.
This suggests that these models actually beneﬁt
from having more content related to the enti-
ties in the story. Even though this content is

not strictly useful or needed for the reasoning
task, it may provide some linguistic cues (e.g.,
about entity genders) that the models exploit. In
contrast, the BERT-based models do not beneﬁt
from the inclusion of this extra content, which
is perhaps due to the fact that they are already
built upon a strong language model (e.g., that
already adequately captures entity genders.)

2. The GAT model performs poorly when support-
ing facts are added but has no performance drop
when disconnected facts are added. This sug-
gests that the GAT model is sensitive to changes
that introduce cycles in the underlying graph
structure but is robust to the addition of noise
that is disconnected from the target entities.

Moreover, when we trained on noisy examples, we
found that only the GAT model was able to consis-
tently improve its performance (Table 2). Again,
this highlights the performance gap between the
unstructured text-based models and the GAT.

5 Conclusion

In this paper we introduced the CLUTRR bench-
mark suite to test the systematic generalization
and inductive reasoning capababilities of NLU sys-
tems. We demonstrated the diagnostic capabilities
of CLUTRR and found that existing NLU systems
exhibit relatively poor robustness and systematic
generalization capabilities—especially when com-
pared to a graph neural network that works directly
with symbolic input. These results highlight the
gap that remains between machine reasoning mod-
els that work with unstructured text and models that
are given access to more structured input. We hope
that by using this benchmark suite, progress can
be made in building more compositional, modular,
and robust NLU systems.

6 Acknowledgements

The authors would like to thank Jack Urbanek,
Stephen Roller, Adina Williams, Dzmitry Bah-
danau, Prasanna Parthasarathy, Harsh Satija for
useful discussions and technical help. The authors
would also like to thank Abhishek Das, Carlos Ed-
uardo Lassance, Gunshi Gupta, Milan Aggarwal,
Rim Assouel, Weiping Song, and Yue Dong for
feedback on the draft. The authors also like to thank
the many anonymous Mechanical Turk participants
for providing paraphrases, and thank Sumana Basu,
Etienne Denis, Jonathan Lebensold, and Komal
Teru for providing human performance measures.
The authors would also like to thank Sanghyun Yoo,
Jehun Jeon and Dr Young Sang Choi of Samsung
Advanced Institute of Technology (SAIT) for sup-
porting the previous workshop version of this work.
The authors are grateful to Facebook AI Research
(FAIR) for providing extensive compute and GPU
resources and support. This research was supported
by the Canada CIFAR Chairs in AI program.

References

Dzmitry Bahdanau,

Shikhar Murty, Michael
Noukhovitch, Thien Huu Nguyen, Harm de Vries,
and Aaron Courville. 2019. Systematic generaliza-
tion: What is required and can it be learned?
In
International Conference on Learning Representa-
tions.

Samuel R Bowman, Gabor Angeli, Christopher Potts,
and Christopher D Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
632–642.

Danqi Chen, Jason Bolton, and Christopher D Man-
ning. 2016. A thorough examination of the cnn/daily
mail reading comprehension task. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 2358–2367.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014.
Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734.

over paths in knowledge bases using reinforcement
learning. In International Conference on Learning
Representations.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Herve Gallaire and Jack Minker. 1978. Logic and Data

Bases. Perseus Publishing.

Suchin Gururangan, Swabha Swayamdipta, Omer
Levy, Roy Schwartz, Samuel Bowman, and Noah A
Smith. 2018. Annotation artifacts in natural lan-
In Proceedings of the 2018
guage inference data.
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 2 (Short Papers),
pages 107–112.

Will Hamilton, Payal Bajaj, Marinka Zitnik, Dan Juraf-
sky, and Jure Leskovec. 2018. Embedding logical
queries on knowledge graphs. In S. Bengio, H. Wal-
lach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett, editors, Advances in Neural Informa-
tion Processing Systems 31, pages 2026–2037. Cur-
ran Associates, Inc.

Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-
stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,
and Phil Blunsom. 2015. Teaching machines to read
In Advances in Neural Informa-
and comprehend.
tion Processing Systems, pages 1693–1701.

Geoffrey E Hinton et al. 1986. Learning distributed
representations of concepts. In Proceedings of the
eighth annual conference of the cognitive science so-
ciety, volume 1, page 12. Amherst, MA.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Drew Arad Hudson and Christopher D. Manning. 2018.
Compositional attention networks for machine rea-
In International Conference on Learning
soning.
Representations.

Robin Jia and Percy Liang. 2017. Adversarial exam-
ples for evaluating reading comprehension systems.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2021–2031.

Justin Johnson, Bharath Hariharan, Laurens van der
Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross
Girshick. 2017. Clevr: A diagnostic dataset for com-
positional language and elementary visual reason-
In Computer Vision and Pattern Recognition
ing.
(CVPR), 2017 IEEE Conference on, pages 1988–
1997. IEEE.

Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer,
Luke Vilnis,
Ishan Durugkar, Akshay Krishna-
murthy, Alex Smola, and Andrew McCallum. 2018.
Go for a walk and arrive at the answer: Reasoning

Dimitri Kartsaklis, Mohammad Taher Pilehvar, and
Nigel Collier. 2018. Mapping text
to knowl-
edge graph entities using multi-sense lstms. arXiv
preprint arXiv:1808.07724.

Divyansh Kaushik and Zachary C Lipton. 2018. How
much reading does reading comprehension require?
a critical investigation of popular benchmarks.
In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
5010–5015.

Stanley Kok and Pedro Domingos. 2007. Statistical
predicate invention. In Proceedings of the 24th Inter-
national Conference on Machine Learning, ICML
’07, pages 433–440, New York, NY, USA. ACM.

Brenden Lake and Marco Baroni. 2018. Generalization
without systematicity: On the compositional skills
of sequence-to-sequence recurrent networks. In In-
ternational Conference on Machine Learning, pages
2879–2888.

Nada Lavrac and Saso Dzeroski. 1994. Inductive logic
programming. In WLP, pages 146–160. Springer.

Alexander Miller, Will Feng, Dhruv Batra, Antoine
Bordes, Adam Fisch, Jiasen Lu, Devi Parikh, and
Jason Weston. 2017. Parlai: A dialog research soft-
In Proceedings of the 2017 Con-
ware platform.
ference on Empirical Methods in Natural Language
Processing: System Demonstrations, pages 79–84.

Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Pushmeet Kohli, and James Allen. 2016. A cor-
pus and cloze evaluation for deeper understanding
of commonsense stories. In Proceedings of NAACL-
HLT, pages 839–849.

Stephen Muggleton. 1991.

Inductive logic program-

ming. New Generation Computing, 8(4):295–318.

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. Ms marco: A human generated machine
arXiv preprint
reading comprehension dataset.
arXiv:1611.09268.

Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming
Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in pytorch.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In NAACL.

J R Quinlan. 1990. Learning logical deﬁnitions from

relations. Mach. Learn., 5(3):239–266.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 2383–2392.

Matthew Richardson, Christopher JC Burges, and Erin
Renshaw. 2013. Mctest: A challenge dataset for
the open-domain machine comprehension of text.

In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
193–203.

Tim Rockt¨aschel and Sebastian Riedel. 2017. End-to-
In Advances in Neural
end differentiable proving.
Information Processing Systems, pages 3788–3800.

Adam Santoro, Ryan Faulkner, David Raposo, Jack
Rae, Mike Chrzanowski, Theophane Weber, Daan
Wierstra, Oriol Vinyals, Razvan Pascanu, and Timo-
thy Lillicrap. 2018. Relational recurrent neural net-
works. In Advances in Neural Information Process-
ing Systems, pages 7310–7321.

Adam Santoro, David Raposo, David G Barrett, Ma-
teusz Malinowski, Razvan Pascanu, Peter Battaglia,
and Timothy Lillicrap. 2017. A simple neural net-
work module for relational reasoning. In Advances
in neural information processing systems, pages
4967–4976.

Shagun Sodhani, Sarath Chandar, and Yoshua. Bengio.
2018. On Training Recurrent Neural Networks for
Lifelong Learning. arXiv e-prints.

Yu Su, Huan Sun, Brian Sadler, Mudhakar Srivatsa,
Izzeddin Gur, Zenghui Yan, and Xifeng Yan. 2016.
On generating characteristic-rich question sets for qa
evaluation. In Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Process-
ing, pages 562–572.

Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Ka-
heer Suleman. 2016. NewsQA: A machine compre-
hension dataset. arXiv preprint, pages 1–12.

Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova,
Adriana Romero, Pietro Li`o, and Yoshua Bengio.
In International
2018. Graph attention networks.
Conference on Learning Representations.

Z. Wang, L. Li, D. D. Zeng, and Y. Chen. 2018.
Attention-based multi-hop reasoning for knowledge
In 2018 IEEE International Conference on
graph.
Intelligence and Security Informatics (ISI), pages
211–213.

Johannes Welbl, Pontus Stenetorp, and Sebastian
Riedel. 2018. Constructing datasets for multi-hop
reading comprehension across documents. Transac-
tions of the Association of Computational Linguis-
tics, 6:287–302.

Jason Weston, Antoine Bordes, Sumit Chopra, Alexan-
der M Rush, Bart van Merri¨enboer, Armand Joulin,
and Tomas Mikolov. 2015. Towards AI-Complete
question answering: A set of prerequisite toy tasks.

Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long Papers), volume 1, pages 1112–1122.

Wenhan Xiong, Thien Hoang, and William Yang Wang.
2017. Deeppath: A reinforcement learning method
for knowledge graph reasoning. In Proceedings of
the 2017 Conference on Empirical Methods in Natu-
ral Language Processing, pages 564–573.

Wenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo,
and William Yang Wang. 2018. One-shot relational
In Proceedings of
learning for knowledge graphs.
the 2018 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1980–1990.

1 Appendix

1.1

Implementation details of the baseline
models

Setup. We implemented all the models using the
encoder-decoder architecture. The encoders are
different baseline models (listed below).The en-
coder takes as input the given story (paragraph )
p = (p1, p2, ...) and produces the representation
of the story. In all the models, the decoder is im-
plemented as a 2-layer MLP which takes as input
the concatenated representation of the story and
the embedding of the entities (for which the rela-
tionship is to be predicted) and returns a softmax
distribution over the relation types. We now de-
scribe the different baseline models (encoders) in
detail:
LSTM (Hochreiter and Schmidhuber, 1997): The
input paragraph is processed by a two-layer Bidi-
rectional LSTM and the hidden state corresponding
to the last time-step is used as the representation of
the story.
LSTM+attention (Cho et al., 2014): Similar to
LSTM, but instead of using just the hidden state at
the last timestep, the model computes the attention-
weighted mean of the hidden state at all time steps
to use as the representation of the story.
Relation Networks - RN (Santoro et al., 2017):
An relation module (implemented as an MLP) is
used alongside the LSTM to learn pairwise rela-
tions among all the pairs of sentences. These rela-
tion representations are the output of the relational
module. Our input data is prepared as a batch of
sentences × words. Each sentence is fed to the
LSTM, followed by a pooling (e.g. mean, max)
over all hidden states of each sentence to generate
the sentence embeddings. The query embeddings
are no longer needed in the decoder since they have
been incorporated by the relational module when
learning relations between sentences.
MAC(Compositional Attention Network) (Hudson
and Manning, 2018): A MAC cell is similar to RN,
but it also which contains a control state c and mem-
ory state m and can iterate over the input several
times. The number of iterations is a hyperparame-
ter. Just like RN, MAC is added behind the LSTM.
In each iteration, the model attends to the embed-
dings of the query entities to generate the current
control ci. Another attention head over ci and all
hidden outputs of LSTM is used to distill the new
information ri. In the end, a linear layer is used to
generate the new memory mi by combining ri and

mi−1. The ﬁnal memory state gives the representa-
tion of the story. This model is the state-of-the-art
model for the CLEVER task.
BERT (Devlin et al., 2018): We adapt BERT pre-
trained language model to our task. Speciﬁcally, we
use two variants of BERT - the vanilla 12-layered
frozen BERT with pre-trained embeddings, and
BERT-LSTM, where a one-layer LSTM encoder
is added on top of pretrained BERT embeddings.
BERT encodes the sentences into 768-dimensional
vectors. To ensure that BERT does not treat the en-
tities as unknown tokens (and hence producing the
same representation for all of them), we represent
the entities with numbers in the vanilla BERT setup.
In BERT-LSTM, we replace the entity embeddings
by our entity embedding lookup policy (Refer Ap-
pendix 1.5). In both the cases, we use a simple
two-layer MLP decoder which takes as inputs the
pooled document representation and query repre-
sentations and produces the softmax distribution
over the relations.
Graph Attention Network(GAT) (Veliˇckovi´c
et al., 2018): Entity(modelled as nodes in the graph
) representations are learned by using the GAT
Graph Neural Network with attention-based ag-
gregation over the neighbor nodes. We modify the
GAT architecture by attending over each node vj in
the neighborhood of vi by concatenating the edge
representation ei,j to the representation of vi.
Relational Recurrent Network (RMC) (Santoro
et al., 2018): We also implemented RMC, a re-
cently proposed model for relational reasoning.
It works like an RNN, processing words step-by-
step, except that a memory matrix (num slots ×
mem size) is added as the hidden state. The rela-
tional bias is extracted in each step by using self-
attention over the concatenation of memory ma-
trix and the word input in the current step. The
ﬁnal memory matrix is the representation of the
story. Our implementation is based on another
open source implementation. 6. We noticed that
the performance of the model is signiﬁcantly less
across all the tasks by a large margin. Till the time
of the submission, we could not verify whether this
subpar performance is due to buggy implementa-
tion of the code or due to some unexplored hyper-
parameter combination. Hence we decided not to
include the results corresponding to this model in
the empirical evaluation. We will continue working
on verifying the implementation of the model.

6https://github.com/L0SG/relational-rnn-pytorch

1.2 Relations and KB used in CLUTRR

Benchmark

[grand, X, Y ] (cid:96) [[child, X, Z], [child, Z, Y ]],
[grand, X, Y ] (cid:96) [[SO, X, Z], [grand, Z, Y ]],
[grand, X, Y ] (cid:96) [[grand, X, Z],
[sibling, Z, Y ]],

[inv-grand, X, Y ] (cid:96) [[inv-child, X, Z],

[inv-child, Z, Y ]],
[inv-grand, X, Y ] (cid:96) [[sibling, X, Z],
[inv-grand, Z, Y ]],

[child, X, Y ] (cid:96) [[child, X, Z],
[sibling, Z, Y ]],

[child, X, Y ] (cid:96) [[SO, X, Z],
[child, Z, Y ]],

[inv-child, X, Y ] (cid:96) [[sibling, X, Z],
[inv-child, Z, Y ]],

[inv-child, X, Y ] (cid:96) [[child, X, Z],

[inv-grand, Z, Y ]],

[sibling, X, Y ] (cid:96) [[child, X, Z],

[inv-un, Z, Y ]],

[sibling, X, Y ] (cid:96) [[inv-child, X, Z],

[child, Z, Y ]],

[sibling, X, Y ] (cid:96) [[sibling, X, Z],

[sibling, Z, Y ]],
[in-law, X, Y ] (cid:96) [[child, X, Z],

[inv-in-law, X, Y ] (cid:96) [[SO, X, Z],

[SO, Z, Y ]],

[inv-child, Z, Y ]],
[un, X, Y ] (cid:96) [[sibling, X, Z],

[child, Z, Y ]],

[inv-un, X, Y ] (cid:96) [[inv-child, X, Z],

[sibling, Z, Y ]],

son,

In the CLUTRR Benchmark, the following kin-
father, husband,
ship relations are used:
brother, grandson, grandfather, son-in-law, father-
in-law, brother-in-law, uncle, nephew, daughter,
mother, wife, sister, granddaughter, grandmother,
daughter-in-law, mother-in-law, sister-in-law, aunt,
niece.

We used a small, tractable, and logically sound
KB of rules as mentioned above. We carefully
select this set of deterministic rules to avoid am-
biguity in the resolution. We use gender-neutral
predicates and resolve the gender of the predi-
cate in the head H of a clause C by deducing
the gender of the second constant. We have two
types of predicates, vertical predicates (parent-
child relations) and horizontal predicates (sibling
or signiﬁcant other). We denote all the vertical
predicates by its child-to-parent relation and ap-
pend the preﬁx inv- to the predicates for the
corresponding parent-to-child relation. For exam-

ple, grandfatherOf is denoted by the gender-
neutral predicate [inv-grand, X, Y ], where the
gender is determined by the gender of Y .

1.3 Effect of placeholder size and split

To analyze whether the language models fail to
learn a robust mapping from natural language nar-
ratives to underlying logical facts, we re-run the
generalization experiments with a reduced place-
holder size (20% of the full collected placeholders)
and we keep the same placeholders for both train-
ing and testing. We observe all language-based
models are now competitive with respect to GAT
on both training regimes k = 2, 3 and k = 2, 3, 4.
This shows the need for separating the placeholder
split to effectively test systematic generalization
because otherwise, current NLU systems tend to
exploit the underlying language layer to arrive at
the correct answer.

1.4 More evaluations on Robust Reasoning

We performed several additional experiments to
analyze the effect of different training regimes in
the Robust Reasoning setup (Table 3) of CLUTRR.
Speciﬁcally, we want to analyze the effect on zero-
shot generalization and robustness when training
with different noisy data settings. We notice that
the GAT model, having access to the true under-
lying graph of the puzzles, perform better across
different testing scenarios when trained with the
noisy data. As the Supporting facts contains cycles,
it is difﬁcult for GAT to generalize for a dataset
with cycles when it is trained on a dataset without
cycles. However, when trained with cycles, GAT
learns to attend to all the paths leading to the cor-
rect answer. This effect is disastrous when GAT is
tested on Irrelevant facts which contains dangling
paths as GAT still tries to attend to all the paths.
Training on Irrelevant facts proved to be most ben-
eﬁcial to GAT, as the model now perfectly attends
to only relevant paths.

Since Disconnected facts contains disconnected
paths, the message passing function of the graph
is unable to forward any information from the dis-
joint cliques, thereby having superior testing scores
throughout several scenarios.
Experiments on synthetic placeholders.
In or-
der to further understand the effect of language
placeholders on robustness, we performed another
set of experiments where we use bABI (Weston
et al., 2015) style simple placeholders (Table 4).
We observe a marked increase in performance of all

Figure 6: Systematic Generalizability of different models on CLUTRR-Gen task (having 20% less placeholders
and without training and testing placeholder split), when Left: trained with k = 2 and k = 3 and Right: trained
with k = 2, 3 and 4

Models

Testing

Clean
Supporting
Irrelevant
Disconnected

Clean
Supporting
Irrelevant
Disconnected

Clean
Supporting
Irrelevant
Disconnected

Training

Supporting

Irrelevant

Disconnected

Average

Unstructured models (no graph)

Structured model (with graph)

BiLSTM - Attention BiLSTM - Mean

RN

MAC

BERT

BERT-LSTM

0.38 ±0.04
0.67 ±0.06
0.44 ±0.03
0.31 ±0.21

0.57 ±0.05
0.38 ±0.22
0.51 ±0.06
0.44 ±0.26

0.45 ±0.02
0.47 ±0.03
0.47 ±0.05
0.57 ±0.07

0.47 ±0.08

0.32 ±0.04
0.66 ±0.07
0.39 ±0.03
0.25 ±0.16

0.56 ±0.05
0.31 ±0.16
0.52 ±0.06
0.54 ±0.27

0.47 ±0.03
0.46 ±0.05
0.48 ±0.03
0.57 ±0.06

0.4 ±0.09
0.68 ±0.05
0.51 ±0.08
0.47 ±0.08

0.46 ±0.13
0.61 ±0.07
0.5 ±0.04
0.55 ±0.05

0.53 ±0.09
0.54 ±0.03
0.52 ±0.04
0.45 ±0.11

0.45 ±0.03
0.65 ±0.04
0.46 ±0.09
0.41 ±0.06

0.67 ±0.05
0.61 ±0.04
0.56 ±0.04
0.61 ±0.06

0.5 ±0.06
0.58 ±0.06
0.51 ±0.05
0.4 ±0.1

0.19 ±0.06
0.32 ±0.09
0.2 ±0.06
0.2 ±0.08

0.24 ±0.06
0.27 ±0.06
0.25 ±0.06
0.26 ±0.03

0.22 ±0.09
0.22 ±0.06
0.17 ±0.04
0.17 ±0.05

0.39 ±0.06
0.57 ±0.04
0.36 ±0.05
0.32 ±0.04

0.46 ±0.08
0.46 ±0.04
0.53 ±0.06
0.45 ±0.08

0.44 ±0.05
0.38 ±0.08
0.38 ±0.05
0.47 ±0.06

0.46 ±0.08

0.52 ±0.07

0.53 ±0.06

0.23 ±0.07

0.43 ±0.05

GAT

0.92 ±0.17
0.98 ±0.01
0.5 ±0.23
0.92 ±0.05

0.92 ±0.0
0.77 ±0.12
0.93 ±0.01
0.85 ±0.25

0.75 ±0.07
0.78 ±0.12
0.56 ±0.26
0.96 ±0.01

0.82 ±0.09

Table 3: Testing the robustness of the various models when trained various types of noisy facts and evaluated on
other noisy / clean facts. The types of noise facts (supporting, irrelevant and disconnected) are deﬁned in Section
3.5 of the main paper.

Models

Testing

Clean
Supporting
Irrelevant
Disconnected

Clean
Supporting
Irrelevant
Disconnected

Clean
Supporting
Irrelevant
Disconnected

Training

Supporting

Irrelevant

Disconnected

Average

Unstructured models (no graph)

Structured model (with graph)

BiLSTM - Attention BiLSTM - Mean

RN

MAC

BERT

BERT-LSTM

0.96 ±0.01
0.96 ±0.03
0.92 ±0.02
0.8 ±0.04

0.63 ±0.02
0.66 ±0.03
0.89 ±0.04
0.64 ±0.02

0.9 ±0.05
0.87 ±0.04
0.87 ±0.03
0.91 ±0.04

0.83 ±0.08

0.97 ±0.01
0.96 ±0.03
0.93 ±0.01
0.83 ±0.04

0.61 ±0.07
0.64 ±0.04
0.86 ±0.1
0.62 ±0.05

0.82 ±0.12
0.82 ±0.05
0.85 ±0.03
0.91 ±0.03

0.88 ±0.05
0.97 ±0.01
0.9 ±0.03
0.76 ±0.08

0.85 ±0.09
0.69 ±0.06
0.74 ±0.11
0.72 ±0.05

0.94 ±0.02
0.85 ±0.03
0.83 ±0.03
0.8 ±0.17

0.94 ±0.02
0.97 ±0.01
0.91 ±0.01
0.86 ±0.04

0.8 ±0.07
0.76 ±0.06
0.78 ±0.06
0.73 ±0.04

0.93 ±0.04
0.88 ±0.04
0.87 ±0.02
0.71 ±0.11

0.48 ±0.08
0.75 ±0.07
0.56 ±0.04
0.27 ±0.06

0.53 ±0.09
0.42 ±0.08
0.61 ±0.1
0.41 ±0.04

0.68 ±0.07
0.54 ±0.08
0.59 ±0.09
0.49 ±0.1

0.57 ±0.08
0.88 ±0.05
0.54 ±0.06
0.42 ±0.08

0.44 ±0.06
0.43 ±0.08
0.83 ±0.06
0.61 ±0.05

0.64 ±0.02
0.5 ±0.05
0.58 ±0.09
0.79 ±0.1

0.82 ±0.08

0.83 ±0.07

0.84 ±0.06

0.58 ±0.07

0.60 ±0.05

GAT

0.92 ±0.17
0.98 ±0.01
0.5 ±0.23
0.92 ±0.05

0.92 ±0.0
0.77 ±0.12
0.93 ±0.01
0.85 ±0.25

0.75 ±0.07
0.78 ±0.12
0.56 ±0.26
0.96 ±0.01

0.82 ±0.09

Table 4: Testing the robustness on toy placeholders of the various models when trained various types of noisy facts
and evaluated on other noisy / clean facts. The types of noise facts (supporting, irrelevant and disconnected) are
deﬁned in Section 3.5 of the main paper.

NLU models, where they signiﬁcantly decrease the
gap between their performance with that of GAT,
even outperforming GAT on various settings. This
shows the signiﬁcance of using paraphrased place-
holders in devising the complexity of the dataset.

1.5 Comparison among different entity

embedding policies

In Cloze style reading comprehension tasks, it is
sometimes customary to choose UNK embeddings

for entity placeholders. (Chen et al., 2016) In our
task, however, choosing UNK embeddings for enti-
ties is not feasible as the query involves two entities
themselves. During preprocessing of our dataset,
we convert the entity names into a Cloze-style setup
where each entity is replaced by @entity-n token.
However, one has to be careful not to assign tokens
in the same order for all the stories, which will
lead to obvious overﬁtting since the models will
learn to work around positional markers as shown
in Chen et al. (2016). Therefore, we randomize
the Cloze-style entities themselves for each story.
We experimented with three different policies of
choosing the entity embeddings:

1. Fixed Random Embeddings: One simple and
intuitive choice is to assign a random embed-
ding to each entity and keep it ﬁxed through-
out the training. During our data-processing
pipeline, we ensure that all the entity tokens
are randomized using a pool of entity tokens,
hence the chances of a model learning to ex-
ploit the positional markers are slim.

2. Randomized Random Embeddings: We can
go one step further and randomize the random
embeddings at each epoch. This aggressive
strategy does not let the model learn any posi-
tional markings at all, however it might ham-
per the learning ability of models as the entity
representations are changing arbitrarily.

3. Learned Random Embeddings: Since our data
pre-processing pipeline randomly assigns the
entities on each story, we can as well learn
a pool of n entities, from which a subset is
always used to replace the entities.

We chose to report all experiments with respect
to ﬁxed random embeddings. We compared differ-
ent embedding policies with respect to the System-
atic Generalization task. We show a comparison be-
tween the Bidirectional LSTM and GAT in Figure
7. We see that the ﬁxed embedding policy has bet-
ter Systematic Generalization score, although the
advantage is minor compared to the other schemes.
For GAT, the advantage is practically nil for the
different schemes which shows that a Graph Neural
Network performs inductive reasoning in the same
manner irrespective of the initial node embedding
representation.

1.6 AMT Data collection process

We use ParlAI (Miller et al., 2017) Mturk interface
to collect paraphrases from the users. Speciﬁcally,
given a set of facts, we ask the users to paraphrase
the facts into a story. The users (turkers) are free
to construct any story they like as long as they
mention all the entities and all the relations among
them. We also provide the head H of the clause
as an inferred relation and speciﬁcally instruct the
users to not mention it in the paraphrased story.
In order to evaluate the paraphrased stories, we
ask the turkers to peer review a story paraphrased
by a different turker. Since there are two tasks -
paraphrasing a story and rating a story - we choose
to pay 0.5$ for each annotation. A sample task
description in our MTurk interface is as follows:

In this task, you will need to write a short, simple
story based on a few facts. It is crucial that the
story mentions each of the given facts at least
once. The story does not need to be complicated!
It just needs to be grammatical and mention the
required facts.

After writing the story, you will be asked to eval-
uate the quality of a generated story (based on
a different set of facts). It is crucial that you
check whether the generated story mentions
each of the required facts.

Example of good and bad stories: Good Example

Facts to Mention

• John is the father of Sylvia.
• Sylvia has a brother Patrick.

Implied Fact: John is the father of Patrick.

Written story

John is the proud father of the lovely Sylvia.
Sylvia has a love-hate relationship with her
brother Patrick.

Bad Example

Facts to Mention

• Vincent is the son of Tim.
• Martha is the wife of Tim.

Implied Fact : Martha is Vincent’s mother.

Written story

Vincent is married at Tim and his mother is
Martha.

The reason the above story is bad:

• This story is bad because it is nonsense /

ungrammatical.

• This story is bad because it does not men-

tion the proper facts.

• This story is bad because it reveals the im-

plied fact.

Figure 7: Systematic Generalization comparison with different Embedding policies

Relation Length

Human Performance
Time Limited Unlimited Time

Reported Difﬁculty

2
3
4
5
6

0.848
0.773
0.477
0.424
0.406

1
1
1
1
1

1.488 +- 1.25
2.41 +- 1.33
3.81 +- 1.46
3.78 +- 0.96
4.46 +- 0.87

Human performance

accuracies on
Table 5:
Humans are provided the
CLUTRR dataset.
Clean-Generalization version of
the dataset, and
we test on two scenarios: when a human is given
limited time to solve the task, and when a human is
given unlimited time to solve the task. Regardless of
time, our evaluators provide a score of difﬁculty of
individual puzzles.

To ensure that the turkers are providing high-
quality annotations without revealing the inferred
fact, we also launch another task to ask the turk-
ers to rate three annotations to be either good or
bad which are provided by a set of different turk-
ers. We pay 0.2$ for each HIT consisting of three
reviews. This helped to remove logical and gram-
matical inconsistencies to a large extent. Based
on the reviews, 79% of the collected paraphrases
passed the peer-review sanity check where all the
reviewers agree on the quality. This subset of the
placeholders is used in the benchmark. A sample
of programmatically generated dataset for clause
length of k = 2 to k = 6 is provided in the tables 7
to 11.

1.7 Human Evaluation

We performed a human evaluation study to ana-
lyze the difﬁculty of our proposed benchmark suite,
which is provided in Table 5. We perform the evalu-
ation in two scenarios: ﬁrst a time-limited scenario
where we ask AMT Turkers to solve the puzzle in
a ﬁxed time. Turkers were provided a maximum

time of 30 mins, but they solved the puzzles in an
average of 1 minute 23 seconds. Secondly, we use
another set of expert evaluators who are given am-
ple time to solve the tasks. Not surprisingly, if a
human being is given ample time (experts took an
average of 6 minutes per puzzle) and a pen and a
paper to aid in the reasoning, they get all the rela-
tions correct. However, if an evaluator is short of
time, they might miss important details on the rela-
tions and perform poorly. Thus, our tasks require
active attention.

In both cases, we asked Turkers and our expert
human evaluators to rate the difﬁculty of a given
task in a Likert scale of 1-5, where 1 corresponds to
very easy and 5 corresponds to very hard perceived
difﬁculty. This score increases as we increase the
complexity of the task by increasing the relations,
thereby suggesting that a human being perceives
similar difﬁculty while solving for larger relation
tasks. However, since a human being is a system-
atic learner, given enough time they can solve all
puzzles with perfect accuracy. We set aside the task
of testing noisy scenarios of CLUTRR to human
evaluators as future work.

2 Supplemental Material

To promote reproducibility, we follow the guide-
lines proposed by the Machine Learning Repro-
ducibility Checklist 7 and release the following
information regarding the experiments conducted
by our benchmark suite.

2.1 Details of datasets used

A downloadable link to the datasets used can be
found here 8. Details of the individual datasets can

7Machine Learning Reproducibility Checklist
8Dataset link in Google Drive

be found in Table 6. For all experiments, we use
10,000 training examples and a 100 testing example
for each testing scenario. We split the training data
80-20 into a dev set randomly on each run.

2.2 Details of Hyperparameters used

For all models, the common hyperparameters used
are: Embedding dimension: 100 (except BERT
based models), Optimizer: Adam, Learning rate:
0.001, Number of epochs: 100, Number of runs: 10.
Speciﬁc model-based hyperparameters are given as
follows:

• Bidirectional LSTM: LSTM hidden dimen-
sion: 100, # layers: 2, Classiﬁer MLP hidden
dimension: 200

• Relation Networks: fθ1 : 256, fθ2: 64, gθ :

64

• MAC: # Iterations: 6, shareQuestion:
True, Dropout - Memory, Read and Write:
0.2

• Relational Recurrent Networks: Memory
slots: 2, Head size: 192, Number of heads:
4, Number of blocks : 1, forget bias : 1, in-
put bias: 0, gate style: unit, key size: 64, #
Attention layers: 3, Dropout: 0

• BERT: Layers : 12, Fixed pretrained em-
beddings from bert-base-uncased us-
ing Pytorch HuggingFace BERT repository 9,
Word dimension: 768, appended with a two-
layer MLP for ﬁnal prediction.

• BERT-LSTM: Same parameters as above,
with a two-layer unidirectional LSTM encoder
on top of BERT word embeddings.

• GAT: Node dimension: 100, Message dimen-
sion: 100, Edge dimension: 20, number of
rounds: 3

9https://github.com/huggingface/pytorch-pretrained-

BERT

Dataset

Variant - Training

Variant - Testing

Training Clause length Testing Clause length

data 089907f8
data db9b8f04

Clean - Generalization Clean - Generalization

(k = 2, 3)
(k = 2, 3, 4)

(k = 2, 3, . . . , 10)

data 7c5b0e70 Clean

Clean, Supporting, Irrelevant, Disconnected

(k = 2, 3)

data 06b8f2a1 Supporting

Clean, Supporting, Irrelevant, Disconnected

(k = 2, 3)

data 523348e6 Irrelevant

Clean, Supporting, Irrelevant, Disconnected

(k = 2, 3)

data d83ecc3e Disconnected

Clean, Supporting, Irrelevant, Disconnected

(k = 2, 3)

(k = 2, 3)

(k = 2, 3)

(k = 2, 3)

(k = 2, 3)

Table 6: Details of publicly released data

Figure 8: Amazon Mechanical Turker Interface built using ParlAI which was used to collect data as well as peer
reviews.

Table 7: Snapshot of puzzles in the dataset for k=2

Puzzle

Question

Gender

Answer

Charles’s son Christopher entered rehab for
the ninth time at the age of thirty. Randolph
had a nephew called Christopher who had n’t
seen for a number of years.

Randolph is the

of Charles

Randolph and his sister Sharon went to the
park. Arthur went to the baseball game with
his son Randolph

. Sharon is the

of Arthur

Frank went to the park with his father, Brett.
Frank called his brother Boyd on the phone.
He wanted to go out for some beers.

Brett is the

of Boyd

Charles:male,
Christopher:male,
Randolph:male

brother

Arthur:male,
Randolph:male,
Sharon:female

Boyd:male,
Frank:male,
Brett:male

daughter

father

Table 8: Snapshot of puzzles in the dataset for k=3

Puzzle

Question

Gender

Answer

Roger was playing baseball with his sons Sam
and Leon. Sam had to take a break though
because he needed to call his sister Robin.

Leon is the

of Robin

Elvira is the

of Allison

Robin:female,
Sam:male,
Roger:male,
Leon:male

Allison:female,
Pedro:male,
Nancy:female,
Elvira:female

brother

grandmother

Elvira and her daughter Nancy went shopping
together last Monday and they bought new
shoes for Elvira’s kids. Pedro and his sister
Allison went to the fair. Pedro’s mother, Nancy,
was out with friends for the day.

Roger met up with his sister Nancy and her
daughter Cynthia at the mall to go shopping
together. Cynthia’s brother Pedro was going
to be the star in the new show.

Pedro is the

of Roger

Roger:male,
Nancy:female,
Cynthia:female,
Pedro:male

nephew

Table 9: Snapshot of puzzles in the dataset for k=4

Puzzle

Question

Gender

Answer

Celina has been visiting her sister, Fran all
week. Fran is also the daughter of Bethany.
Ronald loves visiting his aunt Bethany over
the weekends. Samuel’s son Ronald entered
rehab for the ninth time at the age of thirty.

Celina adores her daughter Bethany. Bethany
loves her very much, too. Jackie called her
mother Bethany to let her know she will be
back home soon. Thomas was helping his
daughter Fran with her homework at home.
Afterwards, Fran and her sister Jackie played
Xbox together.

Raquel is Samuel ’daughter and they go shop-
ping at least twice a week together. Ken-
neth and her mom, Theresa, had a big ﬁght.
Theresa’s son, Ronald, refused to get involved.
Ronald was having an argument with her sister,
Raquel.

Celina is the

of Samuel

Celina is the

of Thomas

Samuel is the

of Kenneth

Samuel:male,
Ronald:male,
Bethany:female,
Fran:female,
Celina:female

niece

Thomas:male,
Fran:female,
Jackie:female,
Bethany:female,
Celina:female

daughter

Kenneth:male,
Theresa:female,
Ronald:male,
Raquel:female,
Samuel:male

father

Table 10: Snapshot of puzzles in the dataset for k=5

Puzzle

Question

Gender

Answer

Steven’s son is Bradford. Bradford and his
father always go ﬁshing together on Sundays
and have a great time together. Diane is taking
her brother Brad out for a late dinner. Kristin,
Brad’s mother, is home with a cold. Diane’s fa-
ther Elmer, and his brother Steven, all got into
the rental car to start the long cross-country
roadtrip they had been planning.

Elmer went on a roadtrip with his youngest
child, Brad. Lena and her sister Diane are go-
ing to a restaurant for lunch. Lena’s brother
Brad is going to meet them there with his fa-
ther Elmer Brad ca n’t stand his unfriendly
aunt Lizzie.

Ira took his niece April ﬁshing Saturday. They
caught a couple small ﬁsh. Ronald was enjoy-
ing spending time with his parents, Damion
and Claudine. Damion’s other son, Dennis,
wanted to come visit too. Dennis often goes
out for lunch with his sister, April.

Bradford is the

of Kristin

Lizzie is the

of Diane

Ira is the

of Claudine

Kristin:female,
Brad:male,
Diane:female,
Elmer:male,
Steven:male,
Bradford:male

nephew

Diane:female,
Lena:female,
Brad:male,
Elmer:male,
Lizzie:female

aunt

Claudine:female,
Ronald:male,
Damion:male,
Dennis:male,
April:female,
Ira:male

brother

Table 11: Snapshot of puzzles in the dataset for k=6

Puzzle

Question

Gender

Answer

Mario wanted to get a good gift for his sis-
ter, Marianne. Jean and her sister Darlene
were going to a party held by Jean’s mom,
Marianne. Darlene invited her brother Roy
to come, too, but he was too busy. Teri and
her father, Mario, had an argument over the
weekend. However, they made up by Monday.
Agnes wants to make a special meal for her
daughter Teri’s birthday.

Robert’s aunt, Marianne, asked Robert to mow
the lawn for her. Robert said he could n’t
because he had a bad back. William’s par-
ents, Brian and Marianne, threw him a sur-
prise party for his birthday. Brian’s daughter
Jean made a mental note to be out of town for
her birthday! Agnes’s biggest accomplishment
is raising her son Robert. Jean is looking for a
good gift for her sister Darlene.

Sharon and her brother Mario went shopping.
Teri, Mario’s daughter, came too. Agnes, An-
nie’s mother, is unhappy with Robert. She
feels her son is cruel to Annie’s sister Teri, and
she wants Robert to be nicer. Robert’s sister,
Nicole, participated in the dance contest.

Roy is the

of Agnes

Darlene is the

of Agnes

Nicole is the

of Sharon

Agnes:female,
Teri:female,
Mario:male,
Marianne:female,
Jean:female,
Darlene:female,
Roy:male

nephew

Agnes:female,
Robert:male,
Marianne:female,
William:male,
Brian:male,
Jean:female,
Darlene:female

niece

Sharon:female,
Mario:male,
Teri:female,
Annie:female,
Agnes:female,
Robert:male,
Nicole:female

niece

