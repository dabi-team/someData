Mean-Field Controls with Q-learning for Cooperative MARL: Convergence and
Complexity Analysis∗

Haotian Gu† , Xin Guo‡ , Xiaoli Wei‡ , and Renyuan Xu§

Abstract. Multi-agent reinforcement learning (MARL), despite its popularity and empirical success, suﬀers
from the curse of dimensionality. This paper builds the mathematical framework to approximate
cooperative MARL by a mean-ﬁeld control (MFC) approach, and shows that the approximation error
is of O( 1√
). By establishing an appropriate form of the dynamic programming principle for both
N
the value function and the Q function, it proposes a model-free kernel-based Q-learning algorithm
(MFC-K-Q), which is shown to have a linear convergence rate for the MFC problem, the ﬁrst of
its kind in the MARL literature. It further establishes that the convergence rate and the sample
complexity of MFC-K-Q are independent of the number of agents N , which provides an O( 1√
)
N
approximation to the MARL problem with N agents in the learning environment. Empirical studies
for the network traﬃc congestion problem demonstrate that MFC-K-Q outperforms existing MARL
algorithms when N is large, for instance when N > 50.

Key words. Mean-Field Control, Multi-Agent Reinforcement Learning, Q-Learning, Cooperative Games, Dy-

namic Programming Principle.

AMS subject classiﬁcations. 49N80, 68Q32, 68T05, 90C40

1. Introduction. Multi-agent reinforcement learning (MARL) has enjoyed substantial
successes for analyzing the otherwise challenging games, including two-agent or two-team
computer games [53, 59], self-driving vehicles [52], real-time bidding games [26], ride-sharing
[30], and traﬃc routing [11]. Despite its empirical success, MARL suﬀers from the curse of
dimensionality known also as the combinatorial nature of MARL: its sample complexity by
existing algorithms for stochastic dynamics grows exponentially with respect to the number of
agents N . (See [20] and also Proposition 2.1 in Section 2). In practice, this N can be on the
scale of thousands or more, for instance, in rider match-up for Uber-pool and network routing
for Zoom.

One classical approach to tackle this curse of dimensionality is to focus on local policies,
namely by exploiting special structures of MARL problems and by designing problem-dependent
algorithms to reduce the complexity. For instance, [29] developed value-based distributed Q-
learning algorithm for deterministic and ﬁnite Markov decision problems (MDPs), and [44]
exploited special dependence structures among agents. (See the reviews by [68] and [70] and
the references therein).

Another approach is to consider MARL in the regime with a large number of homogeneous
agents. In this paradigm, by functional strong law of large numbers (a.k.a. propagation of
chaos) [27, 33, 56, 14], non-cooperative MARLs can be approximated under Nash equilibrium
by mean-ﬁeld games with learning, and cooperative MARLs can be studied under Pareto

∗Accepted August 16th, SIAM Journal on Mathematics of Data Science.
†Department of Mathematics, University of California, Berkeley, USA (haotian_gu@berkeley.edu).
‡IEOR Department, University of California, Berkeley, USA (xinguo@berkeley.edu, xiaoliwei@berkeley.edu).
§Industrial & Systems Engineering, University of Southern California, Los Angeles, USA (renyuanx@usc.edu).

1

1
2
0
2

t
c
O
1

]

G
L
.
s
c
[

6
v
1
3
1
4
0
.
2
0
0
2
:
v
i
X
r
a

 
 
 
 
 
 
2

GU, GUO, WEI AND XU

optimality by analyzing mean-ﬁeld controls (MFC) with learning. This approach is appealing
not only because the dimension of MFC or MFG is independent of the number of agents N ,
but also because solutions of MFC/MFG (without learning) have been shown to provide good
approximations to the corresponding N -agent game in terms of both game values and optimal
strategies [22, 28, 38, 46, 48].

MFG with learning has gained popularity in the reinforcement learning (RL) community
[13, 18, 24, 67, 69], with its sample complexity shown to be similar to that of single-agent RL
([13, 18]). Yet MFC with learning is by and large an uncharted ﬁeld despite its potentially wide
range of applications [30, 31, 62, 64]. The main challenge for MFC with learning is to deal with
probability measure space over the state-action space, which is shown ([17]) to be the minimal
space for which the Dynamic Programming Principle will hold. One of the open problems for
MFC with learning is therefore, as pointed out in [38], to design eﬃcient RL algorithms on
probability measure space.

To circumvent designing algorithms on probability measure space, [6] proposed to add
common noises to the underlying dynamics. This approach enables them to apply the standard
RL theory for stochastic dynamics. Their model-free algorithm, however, suﬀers from high
sample complexity as illustrated in Table 1 below, and with weak performance as demonstrated
in Section 7. For special classes of linear-quadratic MFCs with stochastic dynamics, [5] explored
the policy gradient method and [32] developed an actor-critic type algorithm.

Our work. This paper builds the mathematical framework to approximate cooperative
MARL by MFCs with learning. The approximation error is shown to be of O( 1√
). It then
N
identiﬁes the minimum space on which the Dynamic Programming Principle holds, and proposes
an eﬃcient approximation algorithm (MFC-K-Q) for MFC with learning. This model-free
Q-learning-based algorithm combines the technique of kernel regression with approximated
Bellman operator. The convergence rate and the sample complexity of this algorithm are shown
to be independent of the number of agents N , and rely only on the size of the state-action
space of the underlying single-agent dynamics (Table 1). As far as we are aware of, there is no
prior algorithm with linear convergence rate for cooperative MARL.

Mathematically, the DPP is established through lifting the state-action space and by
aggregating the reward and the underlying dynamics. This lifting idea has been used in
previous MFC framework ([43, 65] without learning and [17] with learning). Our work ﬁnds
that this lifting idea is critical for eﬃcient algorithm design for MFC with learning: the
resulting deterministic dynamics from this lifting trivialize the choice of the learning rate for
the convergence analysis and signiﬁcantly reduce the sample complexity.

Our experiment in Section 7 demonstrates that MFC-K-Q avoids the curse of dimensionality
and outperforms both existing MARL algorithms (when N > 50) and the MFC algorithm in
[6]. Table 1 summarizes the complexity of our MFC-K-Q algorithm along with these relevant
algorithms.

Organizations. Section 2 introduces the set-up of cooperative MARL and MFC with learning.
Section 3 establishes the Dynamical Programming Principle for MFC with learning. Section
4 proposes the algorithm (MFC-K-Q) for MFC with learning, with convergence and sample
complexity analysis. Section 5 is dedicated to the proof of the main theorem. Section 6
connects cooperative MARL and MFC with learning. Section 7 tests performance of MFC-K-Q

MFC WITH Q-LEARNING FOR MARL GAMES

3

Work

MFC/N-agent Method

Sample Complexity Guarantee

Our work
[6]
Vanilla N-agent N-agent
N-agent
[44]

MFC
MFC

Q-learning
Q-learning
Q-learning
Actor-critic Ω(poly((|X ||U|)f (log(1/(cid:15))) · log(1/δ) · N/(cid:15)))

Ω(Tcov · log(1/δ))
Ω((Tcov · log(1/δ))l · poly(log(1/(δ(cid:15)))/(cid:15)))
Ω(poly((|X ||U|)N · log(1/(δ(cid:15))) · N/(cid:15)))

Table 1
Comparison of algorithms

Tcov in Table 1 is the covering time of the exploration policy and l = max{3+1/κ, 1/(1−κ)} > 4
for some κ ∈ (0.5, 1). Other parameters are as in Proposition 2.1 and also in Theorem 5.6.
Note that [44] assumed that agents interact locally through a given graph so that local policies
can approximate the global one, yet f (log(1/(cid:15))) can scale as N for a dense graph.

in a network congestion control problem. Finally, some future directions and discussions are
provided in Section 8. For ease of exposition, proofs for all lemmas are in the Appendix.

Notation. For a measurable space (S, B), where B is σ-algebra on S, denote RS for the
set of all real-valued measurable functions on S, RS := {f : S → R|f is measurable}. For
each bounded f ∈ RS, deﬁne the sup norm of f as ||f ||∞ = sups∈S |f (s)|. In addition, when
S is ﬁnite, we denote |S| for the size of S, and P(S) for the set of all probability measures
on S: {p : p(s) ≥ 0, (cid:80)
s∈S p(s) = 1}, which is equivalent to the probability simplex in R|S|.
Moreover, in P(S), let dP(S) be the metric induced by the l1 norm:
for any u, v ∈ P(S),
dP(S)(u, v) = (cid:80)
s∈S |u(s) − v(s)|. P(S) is endowed with Borel σ-algebra induced by l1 norm.
1(x ∈ A) denotes the indicator function, i.e., 1(x ∈ A) = 1 if x ∈ A, and 1(x /∈ A) = 0 if x /∈ A.

2. MARL and MFC with Learning.

2.1. MARL and its Complexity. We ﬁrst recall cooperative MARL in an inﬁnite time
horizon, where there are N agents whose game strategies are coordinated by a central controller.
Let us assume the state space X and the action space U are all ﬁnite.

t

At each step t = 0, 1, · · · , the state of agent j (= 1, 2, · · · , N ) is xj,N

∈ U. Given the current state proﬁle xxxt = (x1,N

t ∈ X and she takes
an action uj,N
) ∈ X N and the
current action proﬁle uuut = (u1,N
) ∈ U N of N -agents, agent j will receive a reward
˜rj(xxxt, uuut) and her state will change to xj,N
t+1 according to a transition probability function
P j(xxxt, uuut). A Markovian game further restricts the admissible policy for agent j to be of the
form uj,N
t : X N → P(U) maps each state proﬁle xxx ∈ X N to a randomized
action, with P(U) the probability measure space on space U.

t (xxxt). That is, πj

, · · · , uN,N

, · · · , xN,N

t ∼ πj

t

t

t

t

In this cooperative MARL, the central controller is to maximize the expected discounted
aggregated accumulated rewards over all policies and averaged over all agents. That is to ﬁnd

1
N

sup
πππ

N
(cid:88)

j=1

vj(xxx, πππ), where vj(xxx, πππ) = E

(cid:20) ∞
(cid:88)

γt˜rj(xxxt, uuut)(cid:12)

(cid:12)xxx0 = xxx

(cid:21)

t=0

is the accumulated reward for agent j, given the initial state proﬁle xxx0 = xxx and policy

4

GU, GUO, WEI AND XU

t=0 with πππt = (π1

t , . . . , πN

t ). Here γ ∈ (0, 1) is a discount factor, uj,N

t ∼ πj

t (xxxt), and

πππ = {πππt}∞
xj,N
t+1 ∼ P j(xxxt, uuut).

The sample complexity of the Q learning algorithm of this cooperative MARL is exponential
with respect to N . Indeed, take Theorem 4 in [12] and note that the corresponding covering
time for the policy of the central controller will be at least (|X ||U|)N , then we see

Proposition 2.1. Let |X | and |U| be respectively the size of the state space X and the action
space U. Let Q∗ and QT be respectively the optimal value and the value of the asynchronous
(|X ||U|)N ·

Q-learning algorithm in [12] using polynomial learning rate with time T = Ω

poly

(cid:18)

(cid:18)

(cid:19)(cid:19)

N

(cid:15) · ln( 1
δ(cid:15) )

. Then with probability at least 1 − δ, (cid:107)QT − Q∗(cid:107)∞ ≤ (cid:15).

This exponential growth in sample complexity makes the algorithm diﬃcult to scale up.
The classical approach for this curse of dimensionality is to explore special network structures
(e.g., sparsity or local interactions among agents) for MARL problems. Here we shall propose
an alternative approach in the regime when there is a large number of homogeneous agents.

2.2. MFC with Learning: Set-up, Assumptions and Some Preliminary Results. To
overcome the curse of dimensionality in N , we now propose a mean-ﬁeld control (MFC)
framework to approximate this cooperative MARL when agents are homogeneous.

In this MFC framework, all agents are assumed to be identical, indistinguishable, and
interchangeable, and each agent j(= 1, · · · , N ) is assumed to depend on all other agents only
through the empirical distribution of their states and actions. That is, denote P(X ) and P(U)
as the probability measure spaces over the state space X and the action space U, respectively.
The empirical distribution of the states is µN

∈ P(X ), and the empirical

t =x)

(cid:80)N

j=1 1(xj,N
N

j=1 1(uj,N
distribution of the actions is νN
∈ P(U). Then, by law of large numbers,
N
this coperative MARL becomes an MFC with learning when N → ∞. Moreover, as all agents
are indistinguishable, one can focus on a single representative agent.

t (u) =

(cid:80)N

t (x) =
t =u)

Mathematically, this MFC with learning is as follows. At each time t = 0, 1, · · · , the
representative agent in state xt takes an action ut ∈ U according to the admissible policy
πt(xt, µt) : X × P(X ) → P(U) assigned by the central controller, who can observe the
population state distribution µt ∈ P(X ). Further denote Π := {π = {πt}∞
t=0|πt : X × P(X ) →
P(U) is measurable} as the set of admissible policies. The agent will then receive a reward
˜r(xt, µt, ut, νt) and move to the next state xt+1 ∈ X according to a probability transition
function P (xt, µt, ut, νt). Here P and ˜r rely on the state distribution µt and the action
distribution νt(·) := (cid:80)
x∈X πt(x, µt)(·)µt(x), and are possibly unknown.

The objective for this MFC with learning is to ﬁnd v the maximal expected discounted

accumulated reward over all admissible policies π = {πt}∞

t=0, namely

(MFC)

v(µ) = sup
π∈Π

vπ(µ) := sup
π∈Π

(cid:20) ∞
(cid:88)

E

t=0

γt˜r(xt, µt, ut, νt)

(cid:12)
(cid:12)
x0 ∼ µ
(cid:12)
(cid:12)

(cid:21)

,

subject to xt+1 ∼ P (xt, µt, ut, νt), ut ∼ πt(xt, µt).

with initial condition µ0 = µ.

MFC WITH Q-LEARNING FOR MARL GAMES

5

Note that after observing µt, the policy from the central controller πt(·, µt) can be viewed

as a mapping from X to P(U). In this case, we set

(2.1)

ht(·) := πt(·, µt)

for notation simplicity and denote H := {h : X → P(U)} as the space for ht(·). Note that H is
isomorphic to the product of |X | copies of P(U). Therefore, the set of admissible policies Π
can be rewritten as

(2.2)

Π :=

(cid:110)

π = {πt}∞

t=0 | πt : P(X ) → H is measurable

(cid:111)
.

This reformulation of the admissible policy set is key for deriving the Dynamic Programming
Principle (DPP) of (MFC): it enables us to show that the objective in (MFC) is law-invariant
and the probability distribution of the dynamics in (MFC) satisﬁes ﬂow property. This ﬂow
property is also crucial for establishing the convergence of the associated cooperative MARL
by (MFC).

Lemma 2.2. Under any admissible policy π = {πt}∞

t=0 ∈ Π, and the initial state distribution

x0 ∼ µ0 = µ, the evolution of the state distribution {µt}t≥0, is given by

(2.3)

µt+1 = Φ(µt, ht),

where ht(·) is deﬁned in (2.1) and the dynamics Φ is deﬁned as

(2.4)

Φ(µ, h) :=

(cid:88)

(cid:88)

x∈X

u∈U

P (x, µ, u, ν(µ, h))µ(x)h(x)(u) ∈ P(X ),

for any (µ, h) ∈ P(X ) × H and ν(µ, h)(·) := (cid:80)
function vπ deﬁned in (MFC) can be rewritten as

x∈X h(x)(·)µ(x) ∈ P(U). Moreover, the value

(2.5)

vπ(µ) =

∞
(cid:88)

t=0

γtr(µt, ht),

where for any (µ, h) ∈ P(X ) × H, the reward r is deﬁned as

(2.6)

r(µ, h) :=

(cid:88)

(cid:88)

x∈X

u∈U

˜r(x, µ, u, ν(µ, h))µ(x)h(x)(u).

Remark 2.3. Because of the aggregated forms of Φ and r from (2.4) and (2.6), they are also

called the aggregated dynamics and the aggregated reward, respectively.

We start with some standard regularity assumptions for MFC problems [4]. These as-
sumptions are necessary for the mean-ﬁeld approximation to cooperative MARL and for the
subsequent convergence and sample complexity analysis of the learning algorithm.

Let us use the l1 distance for the metrics dP(X ) and dP(U ) of P(X ) and P(U), and deﬁne
dH(h1, h2) = maxx∈X ||h1(x) − h2(x)||1 and dC((µ1, h1), (µ2, h2)) = ||µ1 − µ2||1 + dH(h1, h2)
for the space H and C := P(X ) × H, respectively. Moreover, we endow C with Borel σ algebra
generated by open sets in dC.

6

GU, GUO, WEI AND XU

Assumption 2.4 (Continuity and boundedness of ˜r). There exist ˜R > 0, L˜r > 0, such that

for all x ∈ X , u ∈ U, µ1, µ2 ∈ P(X ), ν1, ν2 ∈ P(U),

|˜r(x, µ1, u, ν1)| ≤ ˜R, |˜r(x, µ1, u, ν1) − ˜r(x, µ2, u, ν2)| ≤ L˜r · (||µ1 − µ2||1 + ||ν1 − ν2||1).

Assumption 2.5 (Continuity of P ). There exists LP > 0 such that for all x ∈ X , u ∈

U, µ1, µ2 ∈ P(X ), ν1, ν2 ∈ P(U),

||P (x, µ1, u, ν1) − P (x, µ2, u, ν2)||1 ≤ LP · (||µ1 − µ2||1 + ||ν1 − ν2||1).

Note that l1 distance between transition kernels P (x, µ, u, ν) in Assumption 2.5 is equivalent
to 1-Wasserstein distance when X and U are equipped with discrete metrics 1(x1 (cid:54)= x2) for
x1, x2 ∈ X and 1(u1 (cid:54)= u2) for u1, u2 ∈ U, respectively, see e.g., [15], [21]. Under Assumptions
2.4 and 2.5, it is clear that the probability measure ν over the action space, the aggregated
reward r in (2.6), and the aggregated dynamics Φ in (2.4) are all Lipschitz continuous, which
will be useful for subsequent analysis.

Lemma 2.6 (Continuity of ν).

(2.7)

(cid:107)ν(µ, h) − ν(µ(cid:48), h(cid:48))(cid:107)1 ≤ dC((µ, h), (µ(cid:48), h(cid:48))).

Lemma 2.7 (Continuity of r). Under Assumption 2.4,

(2.8)

|r(µ, h) − r(µ(cid:48), h(cid:48))| ≤ ( ˜R + 2L˜r)dC((µ, h), (µ(cid:48), h(cid:48))).

Lemma 2.8 (Continuity of Φ). Under Assumption 2.5,

(2.9)

(cid:107)Φ(µ, h) − Φ(µ(cid:48), h(cid:48))(cid:107)1 ≤ (2LP + 1)dC((µ, h), (µ(cid:48), h(cid:48))).

3. DPP for Q Function in MFC with learning. In this section, we establish the DPP of
the Q function for (MFC). Diﬀerent from the well-understood DPP for single-agent control
problem (see for example [36, chapter 9] and [35]), DPP for mean-ﬁeld control problem has
been established only recently on the lifted probability measure space [17, 43, 65]. We extend
the approach of [17] to allow P and ˜r to depend on the population’s action distribution νt.

First, by Lemma 2.2, (MFC) can be recast as a general Markov decision problem (MDP)
with probability measure space as the new state-action space. More speciﬁcally, recall the set
of admissible policies Π in (2.2), if one views the policy πt to be a mapping from P(X ) to H,
then (MFC) can be restated as the following MDP with unknown r and Φ:

(MDP)

v(µ) := sup
π∈Π

∞
(cid:88)

t=0

γtr(µt, ht)

subject to

µt+1 = Φ(µt, ht), µ0 = µ, and ht(·) in (2.1).

With this reformulation, we can deﬁne the associated optimal Q function for (MDP) starting
from arbitrary (µ, h) ∈ C = P(X ) × H,

(3.1)

Q(µ, h) := sup
π∈Π

(cid:20) ∞
(cid:88)

t=0

γtr(µt, ht)

(cid:12)
(cid:12)
µ0 = µ, π0(µ0) = h
(cid:12)
(cid:12)

(cid:21)
,

MFC WITH Q-LEARNING FOR MARL GAMES

7

with ht(·) deﬁned in (2.1). Similarly, deﬁne Qπ as the Q function associated with a policy π:

(3.2)

Qπ(µ, h) :=

(cid:20) ∞
(cid:88)

t=0

(cid:12)
(cid:12)
γtr(µt, ht)
(cid:12)
(cid:12)

(cid:21)
µ0 = µ, π0(µ0) = h

,

with ht(·) deﬁned in (2.1).

Remark 3.1. With this reformulation, (MFC) is now lifted from the ﬁnite state-action space
X and U to a compact continuous state-action space C embedded in an Euclidean space. In
addition, the dynamics become deterministic by the aggregation over the original state-action
space. Due to this aggregation for r, Φ, and the Q function, we will subsequently refer this Q in
(3.1) as an Integrated Q (IQ) function, to underline the diﬀerence between the Q function for
RL of single agent and that for MFC with learning.

The following theorem shows Bellman equation for the IQ function in (3.1).

Theorem 3.2. For any µ ∈ P(X ),

(3.3)

v(µ) = sup
h∈H

Q(µ, h) = sup
h∈H

sup
π∈Π

Qπ(µ, h).

Moreover, the Bellman equation for Q : C → R is

(3.4)

Q(µ, h) = r(µ, h) + γ sup
˜h∈H

Q(Φ(µ, h), ˜h).

Proof of Theorem 3.2. Recall the deﬁnition of v in (MDP) and Q in (3.1). For v(µ), the
supremum is taken over all the admissible policies Π, while for Q(µ, h), the supremum is
taken over all the admissible policies Π with a further restriction that π0(µ) = h. Now in
suph∈H Q(µ, h), since we are free to choose h, it is equivalent to v. Moreover,

v(µ) = sup
π∈Π

(cid:20) ∞
(cid:88)

t=0

(cid:12)
(cid:12)
γtr(µt, πt(µt))
µ0 = µ
(cid:12)
(cid:12)

(cid:21)

(cid:20) ∞
(cid:88)

(cid:12)
(cid:12)
γtr(µt, πt(µt))
µ0 = µ, π0(µ0) = h
(cid:12)
(cid:12)

(cid:21)

=

sup
π∈Π,π0(µ)=h,h∈H
(cid:12)
(cid:12)
γtr(µt, πt(µt))
µ0 = µ, π0(µ0) = h
(cid:12)
(cid:12)

t=0

(cid:21)

= sup
h∈H

Q(µ, h).

= sup
h∈H

sup
π∈Π,π0(µ)=h

(cid:20) ∞
(cid:88)

t=0

Q(µ, h) = sup
π∈Π

(cid:20) ∞
(cid:88)

t=0

(cid:12)
(cid:21)
(cid:12)
γtr(µt, πt(µt))
µ0 = µ, π0(µ0) = h
(cid:12)
(cid:12)

= r(µ, h) + sup
{πt}∞
t=1

= r(µ, h) + sup
{πt}∞
t=0

γ

t=1
(cid:20) ∞
(cid:88)

t=0

(cid:20) ∞
(cid:88)

(cid:12)
(cid:21)
(cid:12)
γtr(µt, πt(µt))
µ1 = Φ(µ, h)
(cid:12)
(cid:12)

(cid:12)
(cid:21)
(cid:12)
γtr(µt, πt(µt))
µ0 = Φ(µ, h)
(cid:12)
(cid:12)

= r(µ, h) + γv(Φ(µ, h)) = r(µ, h) + γ sup
h∈H

Q(Φ(µ, h), h),

where the third equality is from shifting the time index by one.

8

GU, GUO, WEI AND XU

Next, we have the following veriﬁcation theorem for this IQ function.
Proposition 3.3 (Veriﬁcation). Assume Assumption 2.4 and deﬁne Vmax := R

1−γ . Then,

• Q deﬁned in (3.1) is the unique function in {f ∈ RC : (cid:107)f (cid:107)∞ ≤ Vmax} satisfying the

Bellman equation (3.4).

• Suppose that for every µ ∈ P(X ), one can ﬁnd an hµ ∈ H such that hµ ∈ arg maxh∈H Q(µ, h),

t (µ) = hµ for any µ ∈ P(X ) and t ≥ 0, is an optimal stationary

t }∞
then π∗ = {π∗
policy of (MDP).

t=0, where π∗

In order to prove the proposition, let us ﬁrst deﬁne the following two operators.

• Deﬁne the operator B : RC → RC for (MDP)

(3.5)

(B q)(c) = r(c) + γ max
˜h∈H

q(Φ(c), ˜h).

• Deﬁne the operator Bπ : RC → RC for (MDP) under a given stationary policy {πt = π :

P(X ) → H}∞
t=0

(3.6)

(Bπ q)(c) = r(c) + γq(Φ(c), π(Φ(c))).

Proof. Since ||˜r||∞ ≤ R, for any µ ∈ P(X ) and h ∈ H, the aggregated reward function
(2.6) satisﬁes |r(µ, h)| ≤ R · (cid:80)
u∈U µ(x)h(x)(u) = R. In this case, for any µ ∈ P(X ),
h ∈ H and policy π, |Qπ(µ, h)| ≤ R · (cid:80)∞
t=0 γt = Vmax. Hence, Q of (3.1) and Qπ of (3.2) both
belong to {f ∈ RC : (cid:107)f (cid:107)∞ ≤ Vmax}. Meanwhile, by deﬁnition, it is easy to show that B and
Bπ map {f ∈ RC : (cid:107)f (cid:107)∞ ≤ Vmax} to itself.

x∈X

(cid:80)

Next, we notice that B is a contraction operator with modulus γ < 1 under the sup norm

on {f ∈ RC : (cid:107)f (cid:107)∞ ≤ Vmax}: for any (µ, h) ∈ C,

|Bq1(µ, h) − Bq2(µ, h)| ≤ γ max
˜h∈H

|q1(Φ(µ, h), ˜h) − q2(Φ(µ, h), ˜h)| ≤ γ(cid:107)q1 − q2(cid:107)∞.

Thus, (cid:107)Bq1 − Bq2(cid:107)∞ ≤ γ(cid:107)q1 − q2(cid:107)∞. By Banach Fixed Point Theorem, B has a unique ﬁxed
point in {f ∈ RC : (cid:107)f (cid:107)∞ ≤ Vmax}. By (3.4) in Theorem 3.2, the unique ﬁxed point is Q.

Similarly, we can show that for any stationary policy π, Bπ is also a contraction operator
with modulus γ < 1. Meanwhile, by the standard DPP argument as in Theorem 3.2, we have
Qπ = BπQπ. This implies Qπ is the unique ﬁxed point for Bπ in {f ∈ RC : (cid:107)f (cid:107)∞ ≤ Vmax}.

Now let π∗ be the stationary policy deﬁned in the statement of Proposition 3.3. By deﬁnition,
for any c ∈ C, Q(c) = r(c) + γ max˜h∈H Q(Φ(c), ˜h) = r(c) + γQ(Φ(c), π∗(Φ(c))) = Bπ∗Q(c).
Since Bπ∗ has a unique ﬁxed point Qπ∗ in {f ∈ RC : (cid:107)f (cid:107)∞ ≤ Vmax}, which is the IQ function
for the stationary policy π∗, clearly Qπ∗ = Q, and the optimal IQ function is attained by the
optimal policy π∗.

Lemma 3.4 (Characterization of Q). Assume Assumptions 2.4 and 2.5, and γ ·(2LP +1) < 1.

Q of (3.1) is continuous.

The continuity property of Q from Lemma 3.4, along with the compactness of H and

Proposition 3.3, leads to the following existence of stationary optimal policy.

Lemma 3.5. Assume Assumptions 2.4, 2.5 and γ · (2LP + 1) < 1. There exists an optimal

stationary policy π∗ : P(X ) → H such that Qπ∗ = Q.

MFC WITH Q-LEARNING FOR MARL GAMES

9

This existence of a stationary optimal policy is essential for the convergence analysis of
our algorithm MFC-K-Q in Algorithm 4.1. In particular, it allows for comparing the optimal
values of two MDPs with diﬀerent action spaces: (MDP) and its variant deﬁned in (5.9)-(5.10).
Note that the existence of a stationary optimal policy is well known when the state and
action spaces are ﬁnite (see for example [55]) or countably inﬁnite (see for example [36, chapter
9]). Yet, we are unable to ﬁnd any prior corresponding result for the case with continuous
state-action space.

4. MFC-K-Q Algorithm via Kernel Regression and Approximated Bellman Operator.
In this section, we will develop a kernel-based Q-learning algorithm (MFC-K-Q) for the MFC
problem with learning based on (3.4).

Note from (3.4), the MFC problem with learning is diﬀerent from the classical MDP [55]
in two aspects. First, the lifted state space P(X ) and lifted action space H are continuous,
rather than discrete or ﬁnite. Second, the maximum in the Bellman operator is taken over a
continuous space H.

To handle the lifted continuous state-action space, we use a kernel regression method
on the discretized state-action space. Kernel regression is a local averaging approach for
approximating unknown state-action pair from observed data on a discretized space called (cid:15)-net.
Mathematically, a set C(cid:15) = {ci = (µi, hi)}N(cid:15)
i=1 is an (cid:15)-net for C if min1≤i≤N(cid:15) dC(c, ci) < (cid:15) for all
c ∈ C. Here N(cid:15) is the size of C(cid:15). Note that compactness of C implies the existence of such an
(cid:15)-net C(cid:15). The choice of (cid:15) is critical for the convergence and the sample complexity analysis.
Correspondingly, we deﬁne the so-called kernel regression operator ΓK : RC(cid:15) → RC:

(4.1)

ΓKf (c) =

N(cid:15)(cid:88)

i=1

K(ci, c)f (ci),

where K(ci, c) ≥ 0 is a weighted kernel function such that for all c ∈ C and ci ∈ C(cid:15),

(4.2)

N(cid:15)(cid:88)

i=1

K(ci, c) = 1, and K(ci, c) = 0 if dC(ci, c) > (cid:15).

In fact, K can be of any form K(ci, c) = φ(ci,c)
φ(x, y) = 0 when dC(x, y) ≥ (cid:15). (See Section 7 for some choices of φ).

i=1 φ(ci,c)

(cid:80)N(cid:15)

, with some function φ satisfying φ ≥ 0 and

Meanwhile, to avoid maximizing over a continuous space H as in the Bellman equation
(3.4), we take the maximum over the (cid:15)-net H(cid:15) on H. Here H(cid:15) is an (cid:15)-net on H induced from
C(cid:15), i.e., H(cid:15) contains all the possible action choices in C(cid:15), whose size is denoted by NH(cid:15).

The corresponding approximated Bellman operator B(cid:15) acting on functions is then deﬁned

on the (cid:15)-net C(cid:15): RC(cid:15) → RC(cid:15) such that

(4.3)

(B(cid:15) q)(ci) = r(ci) + γ max
˜h∈H(cid:15)

ΓKq(Φ(ci), ˜h).

Since (Φ(ci), ˜h) may not be on the (cid:15)-net, one needs to approximate the value at that point via
the kernel regression ΓKq(Φ(ci), ˜h).

10

GU, GUO, WEI AND XU

In practice, one may only have access to noisy estimations {(cid:98)r(ci), (cid:98)Φ(ci)}N(cid:15)

i=1 instead of the
accurate data {r(ci), Φ(ci)}N(cid:15)
i=1 on C(cid:15). Taking this into consideration, Algorithm 4.1 consists
of two steps. First, it collects samples on C given an exploration policy. For each component
ci on the (cid:15)-net C(cid:15), the estimated data ((cid:98)r(ci), (cid:98)Φ(ci)) is computed by averaging samples in the
(cid:15)-neighborhood of ci. Second, the ﬁxed point iteration is applied to the approximated Bellman
operator B(cid:15) with {(cid:98)r(ci), (cid:98)Φ(ci)}N(cid:15)
i=1. Under appropriate conditions, Algorithm 4.1 provides an
accurate estimation of the true Q function with eﬃcient sample complexity (See Theorem 5.5).

Algorithm 4.1 Kernel-based Q-learning Algorithm for MFC (MFC-K-Q)
1: Input: Initial state distribution µ0, (cid:15) > 0, (cid:15)-net on C : C(cid:15) = {ci = (µi, hi)}N(cid:15)
policy π taking actions from H(cid:15) induced from C(cid:15), regression kernel K on C(cid:15).

i=1, exploration

2: Initialize: (cid:98)r(ci) = 0, (cid:98)Φ(ci) = 0, N (ci) = 0, ∀i.
3: repeat
4:

At the current state distribution µt, act ht according to π, observe µt+1 = Φ(µt, ht) and
rt = r(µt, ht).
for 1 ≤ i ≤ N(cid:15) do

5:

6:

7:

8:

9:

10:

16:

17:

if dC(ci, (µt, ht)) < (cid:15) then

N (ci)←N (ci) + 1.
(cid:98)r(ci)← N (ci)−1
N (ci)
(cid:98)Φ(ci)← N (ci)−1
N (ci)

· (cid:98)r(ci) + 1
· (cid:98)Φ(ci) + 1

N (ci) · rt
N (ci) · µt

end if
end for

11:
12: until N (ci) > 0, ∀i.
13: Initialize: (cid:98)q0(ci) = 0, ∀ci ∈ C(cid:15), l = 0.
14: repeat
15:

for ci ∈ C(cid:15) do
(cid:16)
(cid:98)ql+1(ci)←

end for
l = l + 1.

18:
19: until converge

(cid:98)r(ci)+ γ max˜h∈H(cid:15)

(cid:17)
ΓK (cid:98)ql((cid:98)Φ(ci), ˜h)

.

5. Convergence and Sample Complexity Analysis of MFC-K-Q. In this section, we will
establish the convergence of MFC-K-Q algorithm and analyze its sample complexity. The
convergence analysis in Section 5.1 relies on studying the ﬁxed point iteration of B(cid:15); and the
complexity analysis in Section 5.2 is based on an upper bound of the necessary sample size to
visit each (cid:15)-neighborhood of the (cid:15)-net at least once.

In addition to Assumptions 2.4 and 2.5, the following conditions are needed for the

convergence and the sample complexity analysis.

Assumption 5.1 (Controllability of the dynamics). For all (cid:15), there exists M(cid:15) ∈ N such that
for any (cid:15)-net H(cid:15) on H and µ, µ(cid:48) ∈ P(X ), there exists an action sequence (h1, . . . , hm) with
hi ∈ H(cid:15) and m < M(cid:15), with which the state µ will be driven to an (cid:15)-neighborhood of µ(cid:48).

MFC WITH Q-LEARNING FOR MARL GAMES

11

Assumption 5.2 (Regularity of kernels). For any point c ∈ C, there exist at most NK
points ci’s in C(cid:15) such that K(ci, c) > 0. Moreover, there exists an LK > 0 such that for all
c ∈ C(cid:15), c(cid:48), c(cid:48)(cid:48) ∈ C, |K(c, c(cid:48)) − K(c, c(cid:48)(cid:48))| ≤ LK · dC(c(cid:48), c(cid:48)(cid:48)).

Assumption 5.1 ensures the dynamics to be controllable. Assumption 5.2 is easy to be
satisﬁed: take a uniform grid as the (cid:15)-net, then NK is roughly bounded from above by 2dim(C);
meanwhile, a number of commonly used kernels, including the triangular kernel in Section 7,
satisfy the Lipschitz condition in Assumption 5.2.

5.1. Convergence Analysis. To start, recall the Lipschitz continuity of the aggregated
rewards r and dynamics Φ from Lemma 2.7 and Lemma 2.8. To simplify the notation, denote
Lr := ˜R + 2L˜r as the Lipschitz constant of r and LΦ := 2LP + 1 as the Lipschitz constant of Φ.
Next, recall that there are three sources of the approximation error in Algorithm 4.1: the
kernel regression ΓK on C with the (cid:15)-net C(cid:15), the discretized action space H(cid:15) on H, and the
sampled data (cid:98)r and (cid:98)Φ for both the dynamics and the rewards.

The key idea for the convergence analysis is to decompose the error based on these sources
and to analyze each decomposed error accordingly. That is to consider the following diﬀerent
types of Bellman operators:

• the operator B in (3.5) for (MDP);
• the operator BH(cid:15) : RC → RC which involves the discretized action space H(cid:15)

(5.1)

BH(cid:15)q(c) = r(c) + γ max
˜h∈H(cid:15)

q(Φ(c), ˜h);

• the operator B(cid:15) in (4.3) deﬁned on the (cid:15)-net C(cid:15), which involves the discretized action

space H(cid:15), and the kernel approximation;
• the operator (cid:98)B(cid:15) : RC(cid:15) → RC(cid:15) deﬁned by

(5.2)

( (cid:98)B(cid:15) q)(ci) = (cid:98)r(ci) + γ max
˜h∈H(cid:15)

ΓKq((cid:98)Φ(ci), ˜h),

which involves the discretized action space H(cid:15), the kernel approximation, and the
estimated data.

• the operator T that maps {f ∈ RP(X ) : (cid:107)f (cid:107)∞ ≤ Vmax} to itself, such that

(5.3)

T v(µ) = max
h∈H(cid:15)

(r(µ, h) + γv(Φ(µ, h))).

We show that under mild assumptions, each of the above operators admits a unique ﬁxed point.

Lemma 5.3. Assume Assumption 2.4. Let Vmax := R

1−γ . Then,

• B in (3.5) has a unique ﬁxed point in {f ∈ RC : (cid:107)f (cid:107)∞ ≤ Vmax}. That is, there exists a

unique Q such that

(5.4)

(B Q)(c) = r(c) + γ max
˜h∈H

Q(Φ(c), ˜h).

• BH(cid:15) in (5.1) has a unique ﬁxed point in {f ∈ RC : (cid:107)f (cid:107)∞ ≤ Vmax}. That is, there exists

a unique QH(cid:15) such that

(5.5)

BH(cid:15)QH(cid:15)(c) = r(c) + γ max
˜h∈H(cid:15)

QH(cid:15)(Φ(c), ˜h).

12

GU, GUO, WEI AND XU

• B(cid:15) in (4.3) has a unique ﬁxed point in {f ∈ RC(cid:15) : (cid:107)f (cid:107)∞ ≤ Vmax}. That is, there exists

a unique Q(cid:15) such that for any ci ∈ C(cid:15),

(5.6)

(B(cid:15) Q(cid:15))(ci) = r(ci) + γ max
˜h∈H(cid:15)

ΓKQ(cid:15)(Φ(ci), ˜h).

• (cid:98)B(cid:15) in (5.2) has a unique ﬁxed point in {f ∈ RC(cid:15) : (cid:107)f (cid:107)∞ ≤ Vmax}. That is, there exists

a unique (cid:98)Q(cid:15) such that for any ci ∈ C(cid:15), and (cid:98)r, (cid:98)Φ sampled from ci’s (cid:15)-neighborhood,
( (cid:98)B(cid:15) (cid:98)Q(cid:15))(ci) = (cid:98)r(ci) + γ max
(5.7)
˜h∈H(cid:15)

ΓK (cid:98)Q(cid:15)((cid:98)Φ(ci), ˜h).

• T has a unique ﬁxed point VH(cid:15) in {f ∈ RP(X ) : (cid:107)f (cid:107)∞ ≤ Vmax}. That is

(5.8)

T VH(cid:15)(µ) = max
h∈H(cid:15)

(r(µ, h) + γVH(cid:15)(Φ(µ, h))).

Lemma 5.4 (Characterization of QH(cid:15)). Assume Assumption 2.4. VH(cid:15) in (5.8) is the optimal
value function for the following MFC problem with continuous state space P(X ) and discretized
action space H(cid:15).

(5.9)

VH(cid:15)(µ) = sup
π∈Π(cid:15)

∞
(cid:88)

t=0

γtr(µt, πt(µt))

with Π(cid:15) := {π = {πt}∞
(5.10)

t=0|πt : P(X ) → H(cid:15)}, subject to

µt+1 = Φ(µt, πt(µt)), µ0 = µ.

Moreover, QH(cid:15) in (5.5) and VH(cid:15) in (5.8) satisfy the following relation:

(5.11)

QH(cid:15)(µ, h) = r(µ, h) + γVH(cid:15)(Φ(µ, h)),

and QH(cid:15) is Lipschitz continuous.
This connection between QH(cid:15) and the optimal value function VH(cid:15) of the MFC problem with
continuous state space P(X ) and discretized action space H(cid:15), is critical for estimating the error
bounds in the convergence analysis.

Theorem 5.5 (Convergence). Given (cid:15) > 0. Assume Assumptions 2.4, 2.5, 5.1, and 5.2,

and γ · LΦ < 1. Let (cid:98)B(cid:15) : RC(cid:15) → RC(cid:15) be the operator deﬁned in (5.2)
ΓKq((cid:98)Φ(ci), ˜h),

( (cid:98)B(cid:15) q)(ci) = (cid:98)r(ci) + γ max
˜h∈H(cid:15)

where (cid:98)r(c) and (cid:98)Φ(c) are sampled from an (cid:15)-neighborhood of c, then it has a unique ﬁxed point
(cid:98)Q(cid:15) in {f ∈ RC(cid:15) : ||f ||∞ ≤ Vmax}. Moreover, the sup distance between ΓK (cid:98)Q(cid:15) in (4.1) and Q in
(3.1) is

(5.12)

||Q − ΓK (cid:98)Q(cid:15)||∞ ≤

Lr + 2γNKLKVmaxLΦ
1 − γ

· (cid:15) +

2Lr
(1 − γLΦ)(1 − γ)

· (cid:15).

In particular, for a ﬁxed (cid:15), Algorithm 4.1 converges linearly to (cid:98)Q(cid:15).

MFC WITH Q-LEARNING FOR MARL GAMES

13

Proof of Theorem 5.5. The proof of the the convergence is to quantify ||Q − ΓK (cid:98)Q(cid:15)||∞ from

the following estimate

(5.13)

||Q − ΓK (cid:98)Q(cid:15)||∞ ≤ ||Q − QH(cid:15)||∞
(cid:125)

(cid:124)

(cid:123)(cid:122)
(I)

+ ||QH(cid:15) − ΓKQ(cid:15)||∞
(cid:123)(cid:122)
(cid:125)
(II)

(cid:124)

+ ||ΓKQ(cid:15) − ΓK (cid:98)Q(cid:15)||∞
(cid:123)(cid:122)
(cid:125)
(III)

(cid:124)

.

(I) can be regarded as the approximation error from discretizing the lifted action space H
by H(cid:15); (II) is the error from the kernel regression on C with the (cid:15)-net C(cid:15); and (III) is estimating
the error introduced by the sampled data (cid:98)r and (cid:98)Φ.

Step 1. We shall use 3.5 and Lemmas 5.4 to show that ||Q − QH(cid:15)||∞ ≤

(1−γLΦ)(1−γ) · (cid:15). By
Lemma 5.4, Q(c) − QH(cid:15)(c) = γ(cid:0)V (Φ(c)) − VH(cid:15)(Φ(c))(cid:1), where V is the optimal value function of
the problem on P(X ) and H in (MDP), and VH(cid:15) is the optimal value function of the problem
on P(X ) and H(cid:15) (5.9)-(5.10). Hence it suﬃces to prove that ||V − VH(cid:15)||∞ ≤
(1−γLΦ)(1−γ) · (cid:15).
We adopt the similar strategy as in the proof of Lemma 3.5.

Lr

Lr

Let π∗ be the optimal policy of (MDP), whose existence is shown in Lemma 3.5. For
any µ ∈ P(X ), let (µ, h) = (µ0, h0), (µ1, h1), (µ2, h2), . . . , (µt, ht), . . . be the trajectory of the
system under the optimal policy π∗, starting from µ. We have V (µ) = (cid:80)∞

t=0 γtr(µt, ht).

Now let hit be the nearest neighbor of ht in H(cid:15). dH(hit, ht) ≤ (cid:15). Consider the trajectory of
the system starting from µ and then taking hi0, . . . , hit, . . . , denote the corresponding state by
t. We have VH(cid:15) ≥ (cid:80)∞
µ(cid:48)
dP(X )(µ(cid:48)

t, hit), since VH(cid:15) is the optimal value function.
t−1, hit−1), Φ(µt−1, ht)(cid:1) ≤ LΦ · (cid:0)dP(X )(µ(cid:48)

t−1, µt−1) + (cid:15)(cid:1)

t, µt) = dP(X )

t=0 γtr(µ(cid:48)

(cid:0)Φ(µ(cid:48)

By the iteration, we have dP(X )(µ(cid:48)
t, µt) + (cid:15)(cid:1) ≤ Lr · Lt+1
(cid:0)dP(X )(µ(cid:48)

Φ −1
LΦ−1 · (cid:15), which implies

t, µt) ≤ LΦ−Lt+1

Φ
1−LΦ

· (cid:15), and |r(µ(cid:48)

t, hit) − r(µt, ht)| ≤ Lr ·

0 ≤ V (µ) − VH(cid:15)(µ) ≤

∞
(cid:88)

t=0

γt(r(µt, ht) − r(µ(cid:48)

t, hit)) ≤

∞
(cid:88)

t=0

γt · Lr ·

Lt+1
Φ − 1
LΦ − 1

· (cid:15) =

Lr
(1 − γLΦ)(1 − γ)

· (cid:15).

Here 0 ≤ V (µ) − VH(cid:15)(µ) is by the optimality of VC.

Step 2. We shall use Lemmas 5.3 and 5.4 to show that ||QH(cid:15) − ΓKQ(cid:15)||∞ ≤

Note that

Lr

(1−γLΦ)(1−γ) · (cid:15).

||ΓKQ(cid:15) − QH(cid:15)||∞ = ||ΓKB(cid:15)Q(cid:15) − QH(cid:15)||∞ = ||ΓKBH(cid:15)ΓKQ(cid:15) − QH(cid:15)||∞

≤ ||ΓKBH(cid:15)ΓKQ(cid:15) − ΓKBH(cid:15)QH(cid:15)||∞ + ||ΓKBH(cid:15)QH(cid:15) − QH(cid:15)||∞
= ||ΓKBH(cid:15)ΓKQ(cid:15) − ΓKBH(cid:15)QH(cid:15)||∞ + ||ΓKQH(cid:15) − QH(cid:15)||∞ ≤ γ||ΓKQ(cid:15) − QH(cid:15)||∞ + ||ΓKQH(cid:15) − QH(cid:15)||∞.

Here the ﬁrst and the third equalities hold since Q(cid:15) is the ﬁxed point of B(cid:15) and QH(cid:15) is the ﬁxed
point of BH(cid:15). The second inequality is by the fact that ΓK is a non-expansion mapping, i.e.,
(cid:107)ΓKf (cid:107)∞ ≤ (cid:107)f (cid:107)∞, and that BH(cid:15) is a contraction with modulus γ with the supremum norm.
Meanwhile, for any Lipschitz function f ∈ RC with Lipschitz constant L, we have for all c ∈ C,

|ΓKf (c) − f (c)| =

N(cid:15)(cid:88)

i=1

K(c, ci)|f (ci) − f (c)| ≤

N(cid:15)(cid:88)

i=1

K(c, ci)(cid:15)L = (cid:15)L.

14

GU, GUO, WEI AND XU

Note here the inequality follows from K(c, ci) = 0 for all dC(c, ci) ≥ (cid:15). Therefore, ||ΓKQ(cid:15) −
QH(cid:15)||∞ ≤

is the Lipschitz constant for QH(cid:15).

LQH(cid:15)
1−γ (cid:15), where LQH(cid:15)

= Lr

Final step. Let q0 denote the zero function on C(cid:15). By Lemma 5.3, Q(cid:15) = limn→∞ Bn

1−γLΦ

(cid:15) q0, and
(cid:15) q0, and en := ||qn − (cid:98)qn||∞. For any c ∈ C(cid:15),

(cid:98)Q(cid:15) = limn→∞ (cid:98)Bn

(cid:15) q0. Denote qn := Bn

(cid:15) q0, (cid:98)qn := (cid:98)Bn

en+1(c) = (cid:12)

(cid:12)(cid:98)r(c) + γ max
˜h∈H(cid:15)

ΓK (cid:98)qn((cid:98)Φ(c), ˜h) − r(c) − γ max
˜h∈H(cid:15)

ΓKqn(Φ(c), ˜h)(cid:12)
(cid:12)

≤ |(cid:98)r(c) − r(c)| + γ max
˜h∈H(cid:15)

(cid:12)ΓK (cid:98)qn((cid:98)Φ(c), ˜h) − ΓKqn(Φ(c), ˜h)(cid:12)
(cid:12)
(cid:12)

≤ (cid:15)Lr + γ max
˜h∈H(cid:15)

(cid:2)|ΓK (cid:98)qn((cid:98)Φ(c), ˜h) − ΓK (cid:98)qn(Φ(c), ˜h)| + |ΓK (cid:98)qn(Φ(c), ˜h) − ΓKqn(Φ(c), ˜h)|(cid:3).

Here |(cid:98)r(c) − r(c)| ≤ (cid:15)Lr because (cid:98)r(c) is sampled from an (cid:15)-neighborhood of c and by Assump-
tion 2.4. Moreover, for any ﬁxed ˜h,

|ΓK (cid:98)qn((cid:98)Φ(c), ˜h) − ΓK (cid:98)qn(Φ(c), ˜h)| = |

N(cid:15)(cid:88)

i=1

(K(ci, ((cid:98)Φ(c), ˜h)) − K(ci, (Φ(c), ˜h)))(cid:98)qn(ci)|

≤ 2NKLKVmax · dP(X )((cid:98)Φ(c), Φ(c)) ≤ 2NKLKVmaxLΦ(cid:15).

The ﬁrst inequality comes from Assumption 5.2, because K(ci, ((cid:98)Φ(c), ˜h)) − K(ci, (Φ(c), ˜h)) is
nonzero for at most 2NK index i ∈ {1, 2, . . . , N(cid:15)}, K is Lipschitz continuous, and ||(cid:98)qn||∞ ≤ Vmax.
The second inequality comes from the fact that (cid:98)Φ(c) is sampled from an (cid:15)-neighborhood of c
and by Assumption 2.5. Meanwhile,

|ΓK (cid:98)qn(Φ(c), ˜h) − ΓKqn(Φ(c), ˜h)| ≤ ||qn − (cid:98)qn||∞ = en,

since Γ is non-expansion. Putting these pieces together, we have

en+1 = max
c∈C(cid:15)

en+1(c) ≤ (cid:15)Lr + (cid:15)γ2NKLKVmaxLΦ + γen.

In this case, elementary algebra shows that en ≤ (cid:15) · Lr+γ2NK LK VmaxLΦ
non-expansion, ||ΓKQC(cid:15) − ΓK (cid:98)Q(cid:15)||∞ ≤ (cid:15) · Lr+γ2NK LK VmaxLΦ

1−γ
, hence the error bound (5.12).

, ∀n. Then since ΓK is

1−γ

The claim regarding the convergence rate follows from the γ−contraction of operator (cid:98)B(cid:15).

5.2. Sample Complexity Analysis. In classical Q-learning for MDPs with stochastic envi-
ronment, every component in the (cid:15)-net is required to be visited a number of times in order
to get desirable estimate for the Q function. The usual terminology covering time refers to
the expected number of steps to visit every component in the (cid:15)-net at least once, for a given
exploration policy. The complexity analysis thus focuses on the necessary rounds of the covering
time.

In contrast, visiting each component in the (cid:15)-net once is suﬃcient with deterministic
dynamics. We will demonstrate that using deterministic mean-ﬁeld dynamics to approximate
N-agent stochastic environment will indeed signiﬁcantly reduce the complexity analysis.

MFC WITH Q-LEARNING FOR MARL GAMES

15

To start, denote TC,π as the covering time of the (cid:15)-net under (random) policy π, such that

TC,π := sup

inf

(cid:110)

t > 0 : µ0 = µ, ∀ci ∈ C(cid:15), ∃ti ≤ t,

µ∈P(X )
(µti, hti) in the (cid:15)-neighborhood of ci, under the policy π

(cid:111)
.

Recall that an (cid:15)(cid:48)-greedy policy on H(cid:15) is a policy which with probability at least (cid:15)(cid:48) will uniformly
explore the actions on H(cid:15). Note that this type of policy always exists. And we have the
following sample complexity result.

Theorem 5.6 (Sample complexity). Given (cid:15), δ > 0 and Assumption 5.1, for any (cid:15)(cid:48) > 0, let

π(cid:15)(cid:48) be an (cid:15)(cid:48)-greedy policy on H(cid:15). Then

(5.14)

E[TC,π(cid:15)(cid:48) ] ≤

(M(cid:15) + 1) · (NH(cid:15))M(cid:15)+1
((cid:15)(cid:48))M(cid:15)+1

· log(N(cid:15)).

Here M(cid:15) is deﬁned in Assumption 5.1. Moreover, with probability 1 − δ, for any initial state µ,
under the (cid:15)(cid:48)-greedy policy, the dynamics will visit each (cid:15)-neighborhood of elements in C(cid:15) at least
once, after

(5.15)

(M(cid:15) + 1) · (NH(cid:15))M(cid:15)+1
((cid:15)(cid:48))M(cid:15)+1

· log(N(cid:15)) · e · log(1/δ).

time steps, where log(N(cid:15)) = Θ(|X ||U| log(1/(cid:15))), and NH(cid:15) = Θ(( 1

(cid:15) )(|U|−1|)|X |).

Theorem 5.6 provides an upper bound Ω(poly((1/(cid:15)) · log(1/δ))) for the covering time under
the (cid:15)(cid:48)-greedy policy, in terms of the size of the (cid:15)-net and the accuracy 1/δ. The proof of
Theorem 5.6 relies on the following lemma.

Lemma 5.7. Assume for some policy π, E[TC,π] ≤ T < ∞. Then with probability 1 − δ, for
any initial state µ, under the policy π, the dynamics will visit each (cid:15)-neighborhood of elements
in C(cid:15) at least once, after T · e · log(1/δ) time steps, i.e. P(TC,π ≤ T · e · log(1/δ)) ≥ 1 − δ.

Proof of Theorem 5.6. Recall there are N(cid:15) diﬀerent pairs in the (cid:15)-net. Denote the (cid:15)-
neighborhoods of those pairs by B(cid:15) = {Bi}N(cid:15)
i=1. Without loss of generality, we may assume that
Bi are disjoint, since the covering time will only become smaller if they overlap with each other.
Let Tk := min{t > 1 : k of B(cid:15) is visited}. Tk − Tk−1 is the time to visit a new neighborhood
after k − 1 neighborhoods are visited. By Assumption 5.1, for any Bi ∈ B(cid:15) with center (µi, hi),
µ ∈ P(X ), there exists a sequence of actions in H(cid:15), whose length is at most M(cid:15), such that
starting from µ and taking that sequence of actions will lead the visit of the (cid:15)-neighborhood of
µi. Then, at that point, taking hi will yield the visit of Bi. Hence ∀Bi ∈ B(cid:15), µ ∈ P(X ),

P(Bi is visited in M(cid:15) + 1 steps | µTk−1 = µ) ≥

(cid:18) (cid:15)(cid:48)
NH(cid:15)

(cid:19)M(cid:15)+1

.

P(a new neighborhood is visited in M(cid:15) + 1 steps |µTk−1 = µ) ≥ (N(cid:15) − k + 1) ·

(cid:18) (cid:15)(cid:48)
NH(cid:15)

(cid:19)M(cid:15)+1

.

16

GU, GUO, WEI AND XU

This implies E[Tk − Tk−1] ≤ M(cid:15)+1
(cid:15)(cid:48) )M(cid:15)+1. Summing E[Tk − Tk−1] from k = 1 to k = N(cid:15)
yields the desired result. The second part follows directly from Lemma 5.7. Meanwhile, NH(cid:15),
the size of the (cid:15)-net in H is Θ(( 1
(cid:15) )(|U|−1)|X |), because H is a compact (|U| − 1)|X | dimensional
manifold. Similarly, N(cid:15) = Θ(( 1
(cid:15) )|U||X |−1) as C is a compact |U||X | − 1 dimensional manifold.

N(cid:15)−k+1 · ( NH(cid:15)

6. Mean-ﬁeld Approximation to Cooperative MARL. In this section, we provide a com-
plete description of the connections between cooperative MARL and MFC, in terms of the
value function approximation and algorithmic approximation under the context of learning.

6.1. Value Function Approximation. First we will show that under the Pareto optimality
criterion, (MFC) is an approximation to its corresponding cooperative MARL, with an error of
O( 1√
N
Recall the admissible policy π = {πt}∞

t=0 ∈ Π. Note that the cooperative MARL in Section

).

2.1 with N identical, indistinguishable, and interchangeable agents becomes

(MARL)

sup
π

N (µN ) := sup
uπ
π

1
N

N
(cid:88)

j=1

vj,π(xj,N , µN ) = sup
π

1
N

subject to xj,N

t+1 ∼ P (xj,N

t

, µN

t , uj,N

t

, νN

t ), uj,N

t ∼ πt(xj,N

t

N
(cid:88)

(cid:104) ∞
(cid:88)

E

γt˜r(xj,N

t

, µN

t , uj,N

t

, νN
t )

(cid:105)
,

j=1

t=0
, µN

t ),

1 ≤ j ≤ N,

with initial conditions xj,N
x ∈ X . By symmetry, one can denote uπ

0 = xj,N (j = 1, 2, · · · , N ) and µN

0 (x) = µN (x) :=

N (µN ) := 1
N

(cid:80)N

j=1 vj,π(xj,N , µN ).

(cid:80)N

j=1 1(xj,N =x)
N

for

Deﬁnition 6.1. π(cid:15) is (cid:15)-Pareto optimal for (MARL) if

uπ(cid:15)
N ≥ sup
π

uπ
N − (cid:15).

Assumption 6.2 (Continuity of π). There exists LΠ > 0 such that for all x ∈ X , µ1, µ2 ∈

P(X ), and π ∈ Π,

(cid:107)πt(µ1, x) − πt(µ2, x)(cid:107)1 ≤ LΠ(cid:107)µ1 − µ2(cid:107)1,

for any t ≥ 0.

This Lipschitz assumption for admissible policies is commonly used to bridge games in the
N-player setting and the mean-ﬁeld setting [23, 18].

We are now ready to show that the optimal policy for (MFC) is approximately Pareto

optimal for (MARL) when N → ∞.

Theorem 6.3 (Approximation). Assume γ · (2LP + 1)(1 + LΠ) < 1 and Assumptions 2.4,
2.5 and 6.2, then there exists constant C = C(LP , L˜r, LΠ, |X |, |U|, ˜R, γ), depending on the
dimensions of the state and action spaces in a sublinear order ((cid:112)|X | + (cid:112)|U|), and independent
of the number of agents N , such that

(6.1)

(cid:12)
(cid:12)uπ
(cid:12)

N (µN ) − vπ(µN )

(cid:12)
(cid:12)
(cid:12) ≤ C

1
√
N

,

sup
π

for any initial condition xj,N
Here vπ and uπ

(x ∈ X ).
N are given in (MFC) and (MARL) respectively. Consequently, for any (cid:15)1 > 0,

0 = xj (j = 1, 2, · · · , N ) and µN (x) =

(cid:80)N

j=1 1(xj,N =x)
N

MFC WITH Q-LEARNING FOR MARL GAMES

17

there exists an integer D(cid:15)1 ∈ N such that when N ≥ D(cid:15)1, any (cid:15)2-optimal policy for (MFC) with
learning is ((cid:15)1 + (cid:15)2)-Pareto optimal for (MARL) with N players.

Corollary 6.4 (Optimal value approximation). Assume the same conditions as in Theorem
6.3. Further assume that there exists an optimal policy satisfying Assumption 6.2 for (MFC)
and (MARL). Denote π∗ ∈ arg supπ∈Π vπ and (cid:101)π ∈ arg supπ∈Π uπ
N , there exists a constant
C = C(LP , L˜r, LΠ, |X |, |U|, ˜R, γ), depending on the dimensions of the state and action spaces
in a sublinear order ((cid:112)|X | + (cid:112)|U|), such that

(6.2)

(cid:12)
(cid:12)vπ∗
(cid:12)

(µN ) − u(cid:101)π(µN )

(cid:12)
(cid:12)
(cid:12) ≤

C
√
N

,

with initial conditions xj,N

0 = xj,N and µN :=

(cid:80)N

j=1 1(xj,N =x)
N

.

Corollary 6.4 follows directly from Theorem 6.3 and the proof is deferred to Appendix B.

Proof of Theorem 6.3. First, by (2.6)

N (µN ) =
uπ

1
N

N
(cid:88)

(cid:88)

γtE(cid:2)˜r(xj,N

t

, µN

t , uj,N

t

t=0

j=1
∞
(cid:88)

γtE(cid:2)r(µN

t , πt(µN

t ))(cid:3),

+

, νN

t )(cid:3) −

1
N

N
(cid:88)

(cid:88)

j=1

t=0

γtE(cid:2)˜r(xj,N

t

, µN

t , uj,N

t

, ˜νN

t )(cid:3)

vπ(µN ) =

∞
(cid:88)

t=0

t=0

γtE[˜r(xt, µt, ut, νt)] =

∞
(cid:88)

t=0

γtr(µt, πt(µt)),

where ˜νN

t (u) := (cid:80)

x∈X πt(µ, x)(u)µN

t (x) = 1
N

(cid:80)N

j=1 πt(µN

t , xj,N

t

)(u).

By the continuity of r from Lemma 2.7 and Assumption 2.4,

(cid:12)
(cid:12)uπ
(cid:12)

N (µN ) − vπ(µN )

(cid:12)
(cid:12)
(cid:12)

sup
π

≤ ( ˜R + 2L˜r)

∞
(cid:88)

t=0

(cid:16)

γt sup
π

(cid:104)
(cid:107)µN,π
E

t − µπ

t (cid:107)1

(cid:105)

(cid:104)
+ E

(cid:107)πt(µt) − πt(µN

t )(cid:107)1

(cid:105)(cid:17)

+L˜r

∞
(cid:88)

t=0

(cid:104)
E

γt sup
π

(cid:107)νN,π

t − ˜νN,π

t

(cid:105)

(cid:107)1

≤ ( ˜R + 2L˜r)(1 + LΠ)

∞
(cid:88)

t=0

γt sup
π

(cid:104)
(cid:107)µN,π
E

t − µπ

t (cid:107)1

(cid:105)

+ L˜r

∞
(cid:88)

t=0

(cid:104)
E

γt sup
π

(cid:107)νN,π

t − ˜νN,π

t

(cid:105)
.

(cid:107)1

To prove (6.1), it is suﬃcient to estimate δ1,N
˜νN,π
t = O( 1√
t
N
ν(f ) := (cid:80)
u∈U f (u)ν(u). Then for any t ≥ 0

(cid:107)1. First, we show that δ2,N

t

E[(cid:107)µN,π

t (cid:107)1] and δ2,N
:= supπ
t −
). Denote for any ν ∈ P(U) and f : U → R,

t −µπ

E[(cid:107)νN,π

:= supπ

t

18

GU, GUO, WEI AND XU

(cid:13)
(cid:13)1

(cid:3) = E

(cid:20)

(cid:20)

E

(cid:13)
(cid:13)˜νN

t − νN
t

(cid:13)
(cid:13)1

(cid:12)
(cid:12)
x1,N
(cid:12)
t
(cid:12)

(cid:21)(cid:21)

, · · · , xN,N

t

(cid:0)˜νN

t (f ) − νN

t (f )(cid:1)

(cid:35)(cid:35)

(cid:12)
(cid:12)
x1,N
(cid:12)
t
(cid:12)

, · · · , xN,N

t

(6.3)

E (cid:2)(cid:13)

t − νN
(cid:13)˜νN
t
(cid:34)
(cid:34)

= E

E


E

= E

sup
f :U→{−1,1}





sup
f :U→{−1,1}

1
N

N
(cid:88)

(cid:88)

j=1

u∈U

πt(µN

t , xj,N

t

)(u)f (u) −

1
N

N
(cid:88)

j=1

f (uj,N
t

(cid:12)
(cid:12)
x1,N
)
(cid:12)
t
(cid:12)





, · · · , xN,N

t



 ,

t . Now consider a ﬁxed f : U → {−1, 1}. Conditioned on x1,N

where the ﬁrst equality is by law of total expectation and the last equality is by the deﬁnitions of
˜νN
t and νN
j=1 is
t
a sequence of independent random variables with uj,N
)(·). Therefore, conditioned
on x1,N
t

t , xj,N
t ∼ πt(µN
(cid:111)N

is a sequence of independent

)(u)f (u) − f (uj,N

, · · · , xN,N

, · · · , xN,N

u∈U πt(µN

t , xj,N

, {uj,N

t }N

(cid:110)(cid:80)

)

,

t

t

t

t

t

j=1

u∈U πt(µN

mean-zero random variables bounded in [−2, 2]. The boundedness further implies that each
(cid:80)
) is a sub-Gaussian random variable with variance bounded
by 4. (See Chapter 2 of [60] for the general introduction to sub-Gaussian random variables.)
Meanwhile, the independence implies that conditioned on x1,N

)(u)f (u) − f (uj,N

, · · · , xN,N

t , xj,N

,

t

t

t

t

1
N

N
(cid:88)

(cid:88)

j=1

u∈U

πt(µN

t , xj,N

t

)(u)f (u) −

1
N

N
(cid:88)

j=1

f (uj,N
t

)

is a mean-zero sub-Gaussian random variable with variance 4
mean-zero sub-Gaussian random variables {Xi}M
we have

N . In general, for a sequence of
i=1 with parameter σ2, by Eqn.(2.66) in [60],

E

sup
i=1,··· ,M

Xi

≤

Therefore, conditioned on x1,N

t

, · · · , xN,N

t

,

(cid:34)

(cid:35)

(cid:112)

2σ2 ln(M ).



E



sup
f :U→{−1,1}

1
N

N
(cid:88)

(cid:88)

j=1

u∈U

πt(µN

t , xj,N

t

)(u)f (u) −

1
N

N
(cid:88)

j=1

f (uj,N
t

(cid:12)
(cid:12)
x1,N
)
(cid:12)
t
(cid:12)

, · · · , xN,N

t


 ≤ (cid:112)8 ln(2)|U|/N

holds since we have in total 2|U| diﬀerent choices for f : U → {−1, 1} when taking the supremum.
Thus, following (6.3), we have

(6.4)

δ2,N
t = sup
π

E (cid:2)(cid:13)

(cid:13)˜νN

t − νN
t

(cid:3) ≤ (cid:112)8 ln(2)|U|/N .

(cid:13)
(cid:13)1

MFC WITH Q-LEARNING FOR MARL GAMES

19

t

and claim that δ1,N

Second, we estimate δ1,N
claim holds for t = 0 because δ1,N
t = 1
N
t ,ν denote a P(X )-valued random variable, with

Given x1,N
t PµN

, · · · , xN,N

t = O( 1√
N

j=1 δxj,N

let µN

, µN

(cid:80)N

t

t

t

0 = 0. Suppose the claim holds for t and consider t + 1.

and policy πt(µN

t ) at time t, for any ν ∈ P(U),

). This is done by induction. The

µN
t PµN

t ,ν(x) :=

1
N

N
(cid:88)

j=1

P (xj,N
t

, µN

t , uj,N

t

, ν)(x),

uj,N
t ∼ πt(µN

t , xj,N

t

).

We consider the following decomposition,

E (cid:2)(cid:107)µN

t+1 − µt+1(cid:107)1

(cid:3) ≤ E
(cid:124)

(6.5)

(cid:104)(cid:13)
(cid:13)µN
(cid:13)

t+1 − µN
(cid:123)(cid:122)
(I)
(cid:104)(cid:13)
(cid:13)µN
(cid:13)

+ E
(cid:124)

t PµN

t ,νN
t

(cid:13)
(cid:13)
(cid:13)1

(cid:105)

(cid:125)

+ E
(cid:124)

(cid:104)(cid:13)
(cid:13)µN
(cid:13)

t PµN

t PµN

t ,˜νN
t

(cid:13)
(cid:13)
(cid:13)1

(cid:105)

(cid:125)

t ,νN
t

− µN
(cid:123)(cid:122)
(II)
(cid:13)
(cid:105)
(cid:13)
t ))
(cid:13)1

t PµN

t ,˜νN
t

t , πt(µN

− Φ(µN
(cid:123)(cid:122)
(III)

+ E (cid:2)(cid:13)
(cid:124)

(cid:125)

(cid:13)Φ(µN

t , πt(µN
(cid:123)(cid:122)
(IV )

t )) − µt+1

Bounding (I) in RHS of (6.5): We proceed the similar argument as (6.3),

t PµN

t ,νN
t

E

= E

(cid:104)(cid:13)
t+1 − µN
(cid:13)µN
(cid:13)


E



sup
f :X →{−1,1}

1
N

(cid:13)
(cid:13)
(cid:13)1
N
(cid:88)

j=1

(cid:105)

(cid:104)
E

= E

(cid:104)(cid:13)
(cid:13)µN
(cid:13)

t+1 − µN

t PµN

t ,νN
t

(cid:13)
(cid:13)
(cid:13)1

(cid:12)
(cid:12)x1,N
(cid:12)

t

, · · · , xN,N

t

, u1,N
t

, · · · , uN,N

t

(cid:105)(cid:105)

f (xj,N

t+1) −

1
N

N
(cid:88)

(cid:88)

j=1

x∈X

P (xj,N
t

, µN

t , uj,N

t

, νN

t )(x)f (x)

(cid:12)
(cid:12)
x1,N
(cid:12)
t
(cid:12)

, · · · , xN,N

t

, u1,N
t

, · · · , uN,N

t





≤ (cid:112)8 ln(2)|X |/N .

Bounding (II) in RHS of (6.5):

E

(cid:104)(cid:13)
(cid:13)µN
(cid:13)

t PµN

t ,νN
t

− µN

t PµN

t ,˜νN
t

(cid:105)

(cid:13)
(cid:13)
(cid:13)1

= E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
N

N
(cid:88)

j=1

P (xj,N
t

, µN

t , uj,N

t

, νN

t ) −

1
N

N
(cid:88)

j=1

P (xj,N
t

, µN

t , uj,N

t

, ˜νN
t )





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)1

≤

1
N

N
(cid:88)

j=1

E

(cid:104)(cid:13)
(cid:13)P (xj,N
(cid:13)

t

≤ LP · E (cid:2)(cid:13)

(cid:13)νN

t − ˜νN
t

, µN

t , uj,N

t

, νN

t ) − P (xj,N

t

, µN

t , uj,N

t

(cid:105)

(cid:13)
, ˜νN
(cid:13)
t )
(cid:13)1

(cid:13)
(cid:13)1

(cid:3) ≤ LP

(cid:112)8 ln(2)|U|/N ,

in which the second last inequality holds by the Lipschitz property from Assumption 2.5 and
the last inequality holds by (6.4).

.

(cid:13)
(cid:13)1

(cid:3)

(cid:125)





20

GU, GUO, WEI AND XU

Bounding (III) in RHS of (6.5):

(cid:105)

(cid:13)
(cid:13)
t ))
(cid:13)1

− Φ(µN

t , πt(µN
N
(cid:88)

(cid:88)

1
N

j=1

x∈X

t PµN

t ,˜νN
t

E

(cid:104)(cid:13)
(cid:13)µN
(cid:13)
(cid:20)

(cid:20)
= E

E

sup
g:X →{−1,1}

N
(cid:88)

(cid:88)

(cid:88)

j=1

u∈U

x∈X

−

1
N

P (xj,N
t

, µN

t , uj,N

t

, ˜νN

t )(x)g(x)

P (xj,N
t

, µN

t , u, ˜νN

t )(x)πt(µN

t , xj,N

t

(cid:12)
(cid:12)
x1,N
)(u)g(x)
(cid:12)
t
(cid:12)

(cid:21)(cid:21)

, . . . , xN,N

t

For a ﬁxed g : X → {−1, 1}, conditioned on x1,N

t

, · · · , xN,N

t

,

(cid:40)

(cid:88)

x∈X

P (xj,N
t

, µN

t , uj,N

t

, ˜νN

t )(x)g(x) −

(cid:88)

(cid:88)

u∈U

x∈X

P (xj,N
t

, µN

t , u, ˜νN

t )(x)πt(µN

t , xj,N

t

(cid:41)N

)(u)g(x)

j=1

are independent mean-zero sub-Gaussian random variables. Meanwhile, since by deﬁnition,
we have for each j = 1, · · · , N , (cid:80)
t )(x) = 1, it is easy to show that
(cid:80)
t , uj,N
t )(x)g(x) is bounded by [−1, 1]. Therefore, using the same argument

x∈X P (xj,N

x∈X P (xj,N

t , uj,N

, µN

, µN

, ˜νN

, ˜νN

t

t

t

t

applied in the proof of (6.4), we can show that

E

(cid:104)(cid:13)
(cid:13)µN
(cid:13)

t PµN

t ,˜νN
t

− Φ(µN

t , πt(µN

(cid:13)
(cid:13)
t ))
(cid:13)1

(cid:105)

≤ (cid:112)8 ln(2)|X |/N .

Bounding (IV) in RHS of (6.5):

E(cid:2)(cid:107)Φ(µN

t , πt(µN

t )) − µt+1(cid:107)1

(cid:3) = E(cid:2)(cid:107)Φ(µN

t , πt(µN

t )) − Φ(µt, πt(µt))(cid:107)1

(cid:3) ≤ (2LP + 1)(1 + LΠ)E(cid:2)(cid:107)µN

t − µt(cid:107)1

(cid:3),

where the ﬁrst equality is from the ﬂow of probability measure µt+1 = Φ(µt, πt(µt)) by Lemma
2.2, and the ﬁrst inequality is by the continuity of Φ from Lemma 2.8.

By taking supremum over π on both sides of (6.5), we have δ1,N
t+1 ≤ (2LP + 1)(1 +
(cid:16)
(2LP +1)t(1+LΠ)t−

(cid:112)|U| + 2(cid:112)|X |)(cid:112)8 ln(2)/N , hence δ1,N

√

√

(LP
|X |)
|U |+2
(2LP +1)(1+LΠ)−1

t ≤

t +(LP

LΠ)δ1,N
(cid:17)(cid:112)8 ln(2)/N . Therefore
1

(cid:12)
(cid:12)uπ
(cid:12)

(cid:12)
(cid:12) ≤ ( ˜R + 2L˜r)
N (µN ) − vπ(µN )
(cid:12)

∞
(cid:88)

t=0

γtδ1,N

t + L˜r

∞
(cid:88)

t=0

γtδ2,N
t

(cid:112)|U| + 2(cid:112)|X |)

(cid:16)

1
1 − (2LP + 1)(1 + LΠ)γ

−

(cid:17)

1
1 − γ

+

(cid:112)|U|L˜r
1 − γ

(cid:111)(cid:112)8 ln(2)/N .

sup
π
(cid:110) ( ˜R + 2L˜r)(LP

≤

(2LP + 1)(1 + LΠ) − 1

This proves (6.1).

MFC WITH Q-LEARNING FOR MARL GAMES

21

6.2. Q-function Approximation under Learning . In this section we show that, with
O(log(1/(cid:15))) samples and with (cid:15) the size of (cid:15)-set, the kernel-based Q function from Algorithm 4.1
provides an approximation to the Q function of cooperative MARL, with an error of O((cid:15) + 1√
),
N
For the (MARL) problem speciﬁed in Section 6.1 and given the initial states xj,N and

actions uj,N from all agents (j = 1, 2, . . . , N ), let us deﬁne the corresponding Q function,

(6.6)

Qπ

N (µN , hN ) =

1
N

N
(cid:88)

j=1

subject to

˜r(xj,N , µN , uj,N , νN ) +

1
N

N
(cid:88)

(cid:104) ∞
(cid:88)

E

j=1

t=1

γt˜r(xj,N

t

, µN

t , uj,N

t

(cid:105)

, νN
t )

t+1 ∼ P (xj,N
xj,N

t

, µN

t , uj,N

t

xj,N
1 ∼ P (xj,N , µN , uj,N , νN ),
t ∼ π(µN
, νN

t ), uj,N

t , xj,N

t

), 1 ≤ j ≤ N, and t ≥ 1.

where µN (x) =

with the convention 0

(cid:80)N

, νN (u) =

j=1 1(xj,N =x)
N
0 = 0, and deﬁne

(cid:80)N

j=1 1(uj,N =u)
N

and hN (x)(u) =

(cid:80)N

j=1 1(xj,N =x; uj,N =u)
j=1 1(xj,N =x)

(cid:80)N

(6.7)

QN (µN , hN ) = sup
π

Qπ

N (µN , hN ).

Theorem 6.5. Fix (cid:15) > 0. Assume the same conditions as in Theorem 5.5, Theorem 6.3 and
Corollary 6.4. Then there exists some (cid:101)C = (cid:101)C(LP , LΠ, |X |, |U|, ˜R, L˜r, γ) > 0, depending on the
dimensions of the state and action spaces in a sublinear order ((cid:112)|X | + (cid:112)|U|), such that

(6.8)

||QN − ΓK (cid:98)Q(cid:15)||∞ ≤

Lr + 2γNKLKVmaxLΦ
1 − γ

· (cid:15) +

2Lr
(1 − γLΦ)(1 − γ)

· (cid:15) +

(cid:101)C
√
N

.

Combining Theorem 5.6 and Theorem 6.5 implies the following: ﬁx any (cid:15) > 0, there exists
an integer D(cid:15) ∈ N such that Algorithm 4.1 outputs a kernel-based Q function with C log(1/(cid:15))
samples. With high probability, this kernel-based Q function is (cid:15) close to the Q function of
MARL when the agent number N > D(cid:15). Here C = C(LP , LΠ, |X |, |U|, ˜R, L˜r, γ) is sublinear
with respect to |X | and |U| and independent of the number of agents N .

Proof of Theorem 6.5. First we have

(6.9)

QN (µN , hN ) =

1
N

N
(cid:88)

j=1

˜r(xj,N , µN , uj,N , νN ) + γ sup
π

On the other hand, by the deﬁnitions of Q in (3.1), µN and hN ,

E[uπ

N (µN

1 )]

Q(µN , hN ) =

(6.10)

=

1
N

1
N

N
(cid:88)

j=1

N
(cid:88)

j=1

˜r(xj,N , µN , uj,N , νN ) + sup
π∈Π

(cid:20) ∞
(cid:88)

t=1

γtr(µt, ht)

(cid:12)
(cid:12)
µ1 = Φ(µN , hN )
(cid:12)
(cid:12)

(cid:21)

˜r(xj,N , µN , uj,N , νN ) + γ sup
π∈Π

vπ(Φ(µN , hN ))

22

GU, GUO, WEI AND XU

with ht = πt(µt). Therefore,

|Q(µN , hN ) − QN (µN , hN )| = γ

(6.11)

≤ γ

(cid:12)
(cid:12)vπ∗
(cid:12)

(Φ(µN , hN )) − E[vπ∗

(µN

(cid:12)
(cid:12)
sup
(cid:12)
(cid:12)
π∈Π
(cid:12)
(cid:12)
(cid:12) + γ
1 )]

vπ(Φ(µN , hN )) − sup
π
N (µN
1 )

1 ) − u(cid:101)π

(µN

vπ∗

E

(cid:104)

(cid:12)
(cid:12)
(cid:12)

(cid:105)(cid:12)
(cid:12)
(cid:12)

E[uπ

N (µN

(cid:12)
(cid:12)
1 )]
(cid:12)
(cid:12)

where π∗ ∈ arg supπ∈Π vπ, (cid:101)π ∈ arg supπ∈Π uπ
respect to µN
1 .

N , and the expectation in (6.11) is taking with

For the second term in (6.11),

(6.12)

(cid:104)
vπ∗

E

(cid:12)
(cid:12)
(cid:12)

(µN

1 ) − u(cid:101)π

N (µN
1 )

(cid:105)(cid:12)
(cid:12) ≤ E
(cid:12)

(cid:104)
vπ∗

(cid:12)
(cid:12)
(cid:12)

(µN

1 ) − u(cid:101)π

N (µN
1 )

(cid:105)(cid:12)
(cid:12)
(cid:12) ≤

C
√
N

,

in which the ﬁrst inequality holds by convexity and the second inequality holds due to Corollary
6.4.

For the ﬁrst term in (6.11),

(cid:12)
(cid:12)vπ∗
(cid:12)

(Φ(µN , hN )) − E

µN
1

[vπ∗

(µN

(cid:12)
(cid:12)
1 )]
(cid:12)

(6.13)

≤ ( ˜R + 2L˜r)

∞
(cid:88)

t=0

γtE

(6.14)

≤ ( ˜R + 3L˜r + L˜rLΠ)

(cid:104)(cid:13)
(cid:13)µπ∗
(cid:13)

t − µπ∗

t

(cid:105)

(cid:13)
(cid:13)
(cid:13)1

+ L˜r

γtE

(cid:104) (cid:13)
(cid:13)νπ∗
(cid:13)

t − νπ∗

t

(cid:105)

(cid:13)
(cid:13)
(cid:13)1

∞
(cid:88)

t=0
(cid:105)

,

(cid:13)
(cid:13)
(cid:13)1

∞
(cid:88)

t=0

γtE

(cid:104)(cid:13)
(cid:13)µπ∗
(cid:13)

t − µπ∗

t

, π∗(µπ∗

t+1 = Φ(µπ∗

in which µπ∗
with initial condition µπ∗
t (u) = (cid:80)
νπ∗
and Assumption 2.4.

t )) with initial condition µπ∗

, π∗(µπ∗
t ))
0 = µN
t (x) and
1 .
, x)(u)µπ∗
t (x). (6.13) holds by the continuity of r from Lemma 2.7
(6.14) holds since by Lemma 2.6 and Assumption 6.2,

0 = Φ(µN , hN ), µπ∗
x∈X π∗(µπ∗

t+1 = Φ(µπ∗
, x)(u)µπ∗

In addition, νπ∗

t (u) = (cid:80)

x∈X π∗(µπ∗

t

t

t

t

(cid:13)
(cid:13)νπ∗
(cid:13)

t − νπ∗

t

(cid:13)
(cid:13)
(cid:13)1

≤

(cid:13)
(cid:13)µπ∗
(cid:13)

t − µπ∗

t

(cid:13)
(cid:13)
(cid:13)1

+ max
x∈X

(cid:13)
(cid:13)π∗(µπ∗
(cid:13)

t

, x) − π∗(µπ∗
t

, x)

(cid:13)
(cid:13)
(cid:13)1

≤ (1 + LΠ)

(cid:13)
(cid:13)µπ∗
(cid:13)

t − µπ∗

t

For t = 0,

0 − µπ∗

0

= E (cid:2)(cid:13)

(cid:13)µN

1 − Φ(µN , hN )(cid:13)
(cid:13)1

(cid:3)

(6.15)

E

= E

(cid:104)(cid:13)
(cid:13)µπ∗
(cid:13)
(cid:13)

(cid:13)
(cid:13)
µN
(cid:13)
(cid:13)
(cid:13)



1 −

1
N

(cid:105)

(cid:13)
(cid:13)
(cid:13)1
N
(cid:88)

j=1

u∈U

(cid:88)

P (xj,N , µN , u, νN )(x)hN (xj,N )(u)





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)1





= E

sup
g:X →{−1,1}

1
N
≤ (cid:112)8|X | ln(2)/N ,

N
(cid:88)

j=1

g(xj,N
1

) −

1
N

N
(cid:88)

(cid:88)

(cid:88)

j=1

x∈X

u∈U

P (xj,N , µN , u, νN )(x)hN (xj,N )(u)g(x)



(cid:13)
(cid:13)
(cid:13)1

.



where the second equality is by νN (u) = (cid:80)
and in the last inequality, {g(xj,N

) − (cid:80)

1

x∈X

x∈X µN (x)hN (x)(u) and by the deﬁnition of Φ,
(cid:80)
u∈U P (xj,N , µN , u, νN )(x)hN (xj,N )(u)g(x)}N
j=1

MFC WITH Q-LEARNING FOR MARL GAMES

23

are independent mean-zero sub-Gaussian random variables bounded by [−2, 2] and thus we
proceed the similar arguments as (6.3).

We now prove by induction it holds for all t ≥ 0 that

(6.16)

(cid:104)

E

(cid:107)µπ∗

t − µπ∗

t (cid:107)1

(cid:105)

≤ ((2LP + 1)(LΠ + 1))t(cid:112)8|X | ln(2)/N .

(6.16) holds when t = 0 given (6.15). Now assume (6.16) holds for t ≤ s. When t = s + 1, we
have

(cid:104)

E

(6.17)

(cid:107)µπ∗

s+1 − µπ∗

s+1(cid:107)1

(cid:105)

= E

(cid:104)(cid:13)
(cid:13)Φ(µπ∗
(cid:13)

s , π∗(µπ∗
(cid:32)

s )) − Φ(µπ∗

s , π∗(µπ∗

(cid:105)

(cid:13)
(cid:13)
s ))
(cid:13)1

≤ (2LP + 1)dC

(cid:16)

(cid:17)
s , π∗(µπ∗
µπ∗
s )

(cid:16)

(cid:17)
s , π∗(µπ∗
µπ∗
s )

,

(cid:33)

(cid:16)

(cid:107)µπ∗

s − µπ∗
= (2LP + 1)
≤ (2LP + 1)(1 + LΠ)(cid:107)µπ∗
≤ ((2LP + 1)(1 + LΠ))s+1(cid:112)8|X | ln(2)/N ,

s )(cid:107)1 + (cid:107)π∗(µπ∗
s − µπ∗
s (cid:107)1

s ) − π∗(µπ∗

s )(cid:107)1

(cid:17)

where the ﬁrst inequality holds by Lemma 2.8 and the second inequality holds by Assumption
6.2, and the third inequality holds by induction. Finally when (2LP + 1)(1 + LΠ)γ < 1,

(6.18)

(6.14) ≤ ( ˜R + 3L˜r + L˜rLΠ)

∞
(cid:88)

(cid:112)8|X | ln(2)|/N ((2LP + 1)(1 + LΠ)γ)t

= (cid:112)8|X | ln(2)|/N

t=0
˜R + 3L˜r + L˜rLΠ
1 − (2LP + 1)(1 + LΠ)γ

.

Therefore, combining (6.11), (6.12) and (6.18), we have proven that there exists some (cid:101)C =
(cid:101)C(LP , LΠ, |X |, |U|, ˜R, L˜r, γ) > 0 such that (cid:107)Q − QN (cid:107)∞ ≤ (cid:101)C√
. Here (cid:101)C depends on the
dimensions of the state and action spaces in a sublinear order ((cid:112)|X |+(cid:112)|U|) and is independent
of the number of agents N . Theorem 6.5 follows from combining the result above with Theorem
5.5.

N

7. Experiments. We will test the MFC-K-Q algorithm on a network traﬃc congestion
control problem. In the network there are senders and receivers. Multiple senders share a single
communication link which has an unknown and limited bandwidth. When the total sending
rates from these senders exceed the shared bandwidth, packages may be lost. Sender streams
data packets to the receiver and receives feedback from the receiver on success or failure in the
form of packet acknowledgements (ACKs). (See Figure 1 for illustration and [25] for a similar
set-up). The control problem for each sender is to send the packets as fast as possible and with
the risk of packet loss as little as possible. Given a large interactive population of senders, the
exact dynamics of the system and the rewards are unknown, thus it is natural to formulate
this control problem in the framework of learning MFC.

24

GU, GUO, WEI AND XU

Figure 1. Multiple network traﬃc ﬂows sharing the same link.

7.1. Set-up.
States. For a representative agent in MFC problem with learning, at the beginning of
each round t, the state xt is her inventory (current unsent packet units) taking values from
X = {0, . . . , |X | − 1}. Denote µt := {µt(x)}x∈X as the population state distribution over X .

Actions. The action is the sending rate. At the beginning of each round t, the agent
can adjust her sending rate ut, which remains ﬁxed in [t, t + 1). Here we assume ut ∈ U =
{0, . . . , |U| − 1}. Denote ht = {ht(x)(u)}x∈X ,u∈U as the policy from the central controller.

Limited bandwidth and packet loss. A system with N agents has a shared link of unknown
x∈X ,u∈U uht(x)(u)µt(x)
, each agent’s packet

bandwidth cN (c > 0). In the mean-ﬁeld limit with N → ∞, Ft = (cid:80)
is the average sending rate at time t. If Ft > c, with probability (Ft−c)
will be lost.

Ft

MFC dynamics. At time t + 1, the state of the representative agent moves from xt to xt − ut.
Overshooting is not allowed: ut ≤ xt. Meanwhile, at the end of each round, there are some
packets added to each agent’s packet sending queue. The packet fulﬁllment consists of two
scenarios. First a lost package will be added to the original queue. Then once the inventory hits
zero, a random fulﬁllment with uniform distribution Unif(X ) will be added to her queue. That
is, xt+1 = xt −ut +ut1t(L)+(1−1t(L)1(ut = xt)·Ut, where 1t(L) = 1(packet is lost in round t),
with 1 an indicator function and Ut ∼ Unif(X ).

Evolution of population state distribution µt. Deﬁne, for x ∈ X ,

˜µt(x) =

(cid:88)

x(cid:48)≥x

µt(x(cid:48))ht(x(cid:48))(x(cid:48) − x)

(cid:18)

1 − 1(Ft > c)

(cid:19)

Ft − c
Ft

+ µt(x)1(Ft > c)

Ft − c
Ft

.

Then ˜µt represents the state of the population distribution after the ﬁrst step of task ful-
ﬁllment and before the second step of task fulﬁllment. Finally, for x ∈ X , µt+1(x) =
(cid:16)
˜µt(x) + ˜µt(0)
|X | 1(x = 0), describes the transition of the ﬂows µt+1 = Φ(µt, ht).
|X |
Rewards. Consistent with [10] and [25], the reward function depending on throughput,
latency, with loss penalty is deﬁned as ˜r = a∗throughput−b∗latency2 −d∗loss, with a, b, d ≥ 0.

1(x (cid:54)= 0) + ˜µt(0)

(cid:17)

7.2. Performance of MFC-K-Q Algorithm. We ﬁrst test the convergence property and
performance of MFC-K-Q (Algorithm 4.1) for this traﬃc control problem with diﬀerent kernel
choices and with varying N . We then compare MFC-K-Q with MFQ Algorithm [6] on MFC,
Deep PPQ [25], and PCC-VIVACE [10] on MARL.

MFC WITH Q-LEARNING FOR MARL GAMES

25

We assume the access to an MFC simulator G(µ, h) = (µ(cid:48), r). That is, for any pair (µ, h) ∈
C, we can sample the aggregated population reward r and the next population state distribution
µ(cid:48) under policy h. We sample G(µ, h) = (µ(cid:48), r) once for all (µ, h) ∈ C(cid:15). In each outer iteration,
each update on (µ, h) ∈ C(cid:15) is one inner-iteration. Therefore, the total number of inner iterations
within each outer iteration equals |C(cid:15)|.

Applying MFC policy to N -agent game. To measure the performance of the MFC policy π

for an N -agent set-up, we apply π to the empirical state distribution of N agents.

Performance criteria. We assume the access to an N-agent simulator GN (xxx, uuu) = (xxx(cid:48), rrr).
That is, if agents take joint action uuu from state xxx, we can observe the joint reward rrr and the
next joint state xxx(cid:48). We evaluate diﬀerent policies in the N -agent environment.

We randomly sample K initial states {xxxk

xxxk
0 and collect the continuum rewards in each path for T0 rounds {¯rπ
is the average reward from N agents in round t under policy π. Then Rπ
is used to approximate the value function V π

Two performance criteria are used: the ﬁrst one C(1)
average reward from policy π; and the second criterion C(2)
measures the relative improvements of using policy π1 instead of policy π2.

0 ∈ X N }K

(cid:80)N

k,t}T0

k=1 and apply policy π to each initial state
i=1 rπ,i
N
t=1 γt¯rπ
k,t

k,t =
0) := (cid:80)T0

t=1. Here ¯rπ
N (xxxk
C with policy π, when T0 is large.
N (π) = 1
k=1 Rπ
N (xxxk
K
(cid:80)K
N (π1, π2) = 1
K

0) measures the
0 )−Rπ2
Rπ1
N (xxxk
0 )
Rπ1
N (xxxk
0 )

N (xxxk

(cid:80)K

k=1

k

Experiment set-up. We set γ = 0.5, a = 30, b = 10, d = 50, c = 0.4, M = 2, K = 500 and
T0 = 30, and compare policies with N = 5n agents (n = 1, 2, · · · , 20). For the (cid:15)-net, we take
uniform grids with (cid:15) distance between adjacent points on the net. The conﬁdence intervals are
calculated with 20 repeated experiments.

(a) Convergence of Q function.

(b) C (1)

N : Average reward.

Figure 2. Performance comparison among diﬀerent kernels.

Results with diﬀerent kernels. We use the following kernels with hyper-parameter (cid:15): triangular,
(cid:12)
(cid:12)(cid:15) −
(cid:15) (x, y) = 1{(cid:107)x−y(cid:107)2≤(cid:15)}
(cid:15) (x, y) = 1{(cid:107)x−y(cid:107)2≤(cid:15)}. We

(truncated) Gaussian, and (truncated) constant kernels. That is, φ(1)
exp(−|(cid:15) − (cid:107)x − y(cid:107)2|2), and φ(3)
1√
(cid:107)x − y(cid:107)2
(cid:15) (ci, c) = φ(j)
run the experiments for K(j)

. with j = 1, 2, 3 and (cid:15) = 0.1.

(cid:15) (x, y) = 1{(cid:107)x−y(cid:107)2≤(cid:15)}

(cid:12)
(cid:12), φ(2)

(cid:15) (ci,c)

2π

(cid:80)N(cid:15)

i=1 φ(j)

(cid:15) (ci,c)

All kernels lead to the convergence of Q functions within 15 outer iterations (Figure 2a).

26

GU, GUO, WEI AND XU

When N ≤ 10, the performances of all kernels are similar since (cid:15)-net is accurate for games with
N = 1
0.1 does the worst (Figure 2b):
treating all nearby (cid:15)-net points with equal weights yields relatively poor performance.

(cid:15) agents. When N ≥ 15, K(1)

0.1 performs the best and K(3)

Further comparison of K(j)

0.1’s suggests that appropriate choices of kernels for speciﬁc

problems with particular structures of Q functions help reducing errors from a ﬁxed (cid:15)-net.

(a) Convergence of Q function

(b) C (1)

N : Average reward.

(c) C (2)

N : Improvement of K (1)

0.1 from 1-NN.

(d) C (2)

N : Improvement of K (1)

0.1 from 3-NN.

Figure 3. Comparison between K 1

0.1(x, y) and k-NN (k = 1, 3).

Results with diﬀerent k-nearest neighbors. We compare kernel K(1)

0.1 (x, y) with the k-nearest-
neighbor (k-NN) method (k = 1, 3), with 1-NN the projection approach by which each point is
projected onto the closest point in C(cid:15), a simple method for continuous state and action spaces
[39, 58].

All K(1)

0.1 (x, y) and k-NN converge within 15 outer iterations. The performances of K1

0.1(x, y)
and k-NN are similar when N ≤ 10. However, K(1)
0.1 (x, y) outperforms both 1-NN and 3-
NN for large N under both criteria C(1)
0.1 (x, y), 1-NN, and 3-NN
have respectively average rewards of 1.4, 1.07, and 1.2 when N ≥ 65; under C(2)
0.1 (x, y)
outperforms 1-NN and 3-NN by 15% and 13% respectively when N = 10, by 29% and 21%
respectively when N = 15, and by 25% and 16% respectively when N ≥ 60.

N : under C(1)

N and C(2)

N , K(1)

N , K(1)

MFC WITH Q-LEARNING FOR MARL GAMES

27

Comparison with other algorithms. We compare MFC-K-Q with K(1)

0.1 with three representative
algorithms, MFQ from [6], Deep PPQ from [25], and PCC-VIVACE from [10] on MARL. Our
experiment demonstrates superior performances of MFC-K-Q.

• When N>40, MFC-K-Q dominates all these three algorithms (Figure 4a) and it learns
the bandwidth parameter c most accurately (Figure 4b). Despite being the best
performer when N<35, Deep PPQ suﬀers from the “curse of dimensionality” and the
performance gets increasingly worse when N increases;

• MFC-K-Q with K(1)

0.1 dominates MFQ, which is similar to our worst performer MFC-
K-Q with 1-NN. In general, kernel regression performs better than simple projection
(adopted in MFQ) where only one point is used to estimate Q;

• the decentralized PCC-VIVACE has the worst performance. Moreover, it is insensitive

to the bandwidth parameter c. See Figure 4b.

(a) C (1)

N : Average reward.

(b) Average sending ﬂow.

Figure 4. Performance comparison among diﬀerent algorithms.

8. Discussions and Future Works.
Related works on kernel-based reinforcement learning. Kernel method is a popular dimension
reduction technique to map high-dimensional features into a low dimension space that best
represents the original features. This technique was ﬁrst introduced for RL by [42, 41], in which
a kernel-based reinforcement learning algorithm (KBRL) was proposed to handle the continuity
of the state space. Subsequent works demonstrated the applicability of KBRL to large-scale
problems and for various types of RL algorithms ([2], [57] and [66]). However, there is no prior
work on convergence rate or sample complexity analysis.

Our kernel regression idea is closely related to [51], which combined Q-learning with kernel-
based nearest neighbor regression to study continuous-state stochastic MDPs with sample
complexity guarantee. However, our problem setting and technique for error bound analysis
are diﬀerent from theirs. In particular, Theorem 5.5 has both action space approximation
and state space approximation; whereas [51] has only state space approximation and their
action space is ﬁnite. The error control in [51] was obtained via martingale concentration
inequalities whereas ours is by the regularity property of the underlying dynamics. Other than
the kernel regression method, one could also consider the empirical (or approximate) dynamic

28

GU, GUO, WEI AND XU

programming approach to handle the inﬁnite dimensional problem [7, 19].

Stochastic vs deterministic dynamics. We reiterate that unlike learning algorithms for sto-
chastic dynamics where the choice of learning rate ηt is to guarantee the convergence of the Q
function (see e.g. [63]), MFC-K-Q directly conducts the ﬁxed point iteration for the approxi-
mated Bellman operator B(cid:15) on the sampled data set, and sets the learning rate as 1 to fully
utilize the deterministic nature of the dynamics. Consequently, complexity analysis of this
algorithm is reduced signiﬁcantly. By comparison, for stochastic systems each component in
the (cid:15)-net has to be visited suﬃciently many times for a decent estimate in Q-learning.

Sample complexity comparison. Theorem 5.6 shows that sample complexity for MFC with
learning is Ω(poly((1/(cid:15)) · log(1/δ))), instead of the exponential rate in N by existing algorithms
for cooperative MARL in Proposition 2.1. Careful readings reveal that this complexity analysis
holds for other exploration schemes, including the Gaussian exploration and the Boltzmann
exploration, as long as Lemma 5.7 holds.

Convergence under diﬀerent norms. Our main assumptions and results adopt the inﬁnity
norm ((cid:107) · (cid:107)∞) for ease of exposition. Under appropriate assumptions on the mixing behavior of
the mean-ﬁeld dynamic, and applying techniques in [40], the convergence results can also be
established under the Lp ((cid:107) · (cid:107)p) norm to allow for the function approximation of Q-learning.
In addition, by properly controlling the Lipschtiz constant, the empirical performance of the
neural network approximation may be further improved ([1]).

Extensions to other settings. For future research, we are interested in extending our framework
and learning algorithm to other variations of mean-ﬁeld controls including risk-sensitive mean-
ﬁeld controls ([3], [8], and [9]), robust mean-ﬁeld controls ([61]), mean-ﬁeld controls on polish
space ([45]), and partially observed mean-ﬁeld controls ([8, 47]).

If the state space of each individual player is a Polish space [45], one can adopt, instead of
the Q learning framework in this paper, Proximal Policy Optimization (PPO) type of algorithms
[50, 49]. In this framework, the mean-ﬁeld information on the lifted probability measure may
be incorporated via a mean embedding technique, which embeds the mean-ﬁeld states into a
reproducing kernel Hilbert space (RKHS) [54, 16].

Given the connection between the Q function and the Hamiltonian of nonlinear control
problem with single-agent [34], one may also extend the kernel-based Q learning algorithm to
more general nonlinear mean-ﬁeld control problems.

Acknowledgement. We are grateful to two anonymous referees from SIAM Journal on
Mathematics of Data Science for their detailed suggestions, which help improve the exposition
of the paper and in particular Section 6. We thank the authors of [37] who spotted an error in
the earlier proof of Theorem 6.3 in our original arxiv version; our correction of the error yields
a reﬁned upper bound of (6.4) and leads a sublinear dependence on the dimensions of the state
and action spaces with order ((cid:112)|X | + (cid:112)|U|) in the constant term C of Theorem 6.3 .

REFERENCES

[1] K. Asadi, D. Misra, and M. L. Littman, Lipschitz continuity in model-based reinforcement learning,

arXiv preprint arXiv:1804.07193, (2018).

[2] A. M. Barreto, D. Precup, and J. Pineau, Practical kernel-based reinforcement learning, Journal of

MFC WITH Q-LEARNING FOR MARL GAMES

29

Machine Learning Research, 17 (2016), pp. 2372–2441.

[3] A. Bensoussan, B. Djehiche, H. Tembine, and P. Yam, Risk-sensitive mean-ﬁeld-type control, in

2017 IEEE 56th Annual Conference on Decision and Control, IEEE, 2017, pp. 33–38.

[4] R. Carmona and F. Delarue, Probabilistic Theory of Mean Field Games with Applications I-II, Springer,

2018.

[5] R. Carmona, M. Laurière, and Z. Tan, Linear-quadratic mean-ﬁeld reinforcement learning: Conver-

gence of policy gradient methods, arXiv preprint arXiv:1910.04295, (2019).

[6] R. Carmona, M. Laurière, and Z. Tan, Model-free mean-ﬁeld reinforcement learning: Mean-ﬁeld

MDP and mean-ﬁeld Q-learning, arXiv preprint arXiv:1910.12802, (2019).

[7] W. Chen, D. Huang, A. A. Kulkarni, J. Unnikrishnan, Q. Zhu, P. Mehta, S. Meyn, and
A. Wierman, Approximate dynamic programming using ﬂuid and diﬀusion approximations with
applications to power management, in Proceedings of the 48h IEEE Conference on Decision and
Control (CDC) held jointly with 2009 28th Chinese Control Conference, IEEE, 2009, pp. 3575–3580.
[8] B. Djehiche and H. Tembine, Risk-sensitive mean-ﬁeld type control under partial observation, in
Stochastics of Environmental and Financial Economics, Springer, Cham, 2016, pp. 243–263.
[9] B. Djehiche, H. Tembine, and R. Tempone, A stochastic maximum principle for risk-sensitive
mean-ﬁeld type control, IEEE Transactions on Automatic Control, 60 (2015), pp. 2640–2649.
[10] M. Dong, T. Meng, D. Zarchy, E. Arslan, Y. Gilad, B. Godfrey, and M. Schapira, PCC
vivace: Online-learning congestion control, in 15th USENIX Symposium on Networked Systems Design
and Implementation (NSDI 18), Renton, WA, Apr. 2018, USENIX Association, pp. 343–356.
[11] S. El-Tantawy, B. Abdulhai, and H. Abdelgawad, Multiagent reinforcement learning for integrated
network of adaptive traﬃc signal controllers (MARLIN-ATSC): Methodology and large-scale application
on downtown Toronto, IEEE Transactions on Intelligent Transportation Systems, 14 (2013), pp. 1140–
1150.

[12] E. Even-Dar and Y. Mansour, Learning rates for Q-learning, Journal of Machine Learning Research,

5 (2003), pp. 1–25.

[13] Z. Fu, Z. Yang, Y. Chen, and Z. Wang, Actor-critic provably ﬁnds Nash equilibria of linear-quadratic

mean-ﬁeld games, arXiv preprint arXiv:1910.07498, (2019).

[14] J. Gärtner, On the McKean-Vlasov limit for interacting diﬀusions, Mathematische Nachrichten, 137

(1988), pp. 197–248.

[15] A. L. Gibbs and F. E. Su, On choosing and bounding probability metrics, International statistical review,

70 (2002), pp. 419–435.

[16] A. Gretton, K. Borgwardt, M. J. Rasch, B. Scholkopf, and A. J. Smola, A kernel method for

the two-sample problem, arXiv preprint arXiv:0805.2368, (2008).

[17] H. Gu, X. Guo, X. Wei, and R. Xu, Dynamic programming principles for learning MFCs, arXiv

preprint arXiv:1911.07314, (2019).

[18] X. Guo, A. Hu, R. Xu, and J. Zhang, Learning mean-ﬁeld games, in Advances in Neural Information

Processing Systems, 2019, pp. 4966–4976.

[19] W. B. Haskell, R. Jain, and D. Kalathil, Empirical dynamic programming, Mathematics of

Operations Research, 41 (2016), pp. 402–429.

[20] P. Hernandez-Leal, B. Kartal, and M. E. Taylor, A survey and critique of multiagent deep

reinforcement learning, Autonomous Agents and Multi-Agent Systems, 33 (2019), pp. 750–797.

[21] K. Hinderer, Lipschitz continuity of value functions in Markovian decision processes, Mathematical

Methods of Operations Research, 62 (2005), pp. 3–22.

[22] M. Huang, P. E. Caines, and R. P. Malhamé, Large-population cost-coupled LQG problems with
nonuniform agents: individual-mass behavior and decentralized ε-nash equilibria, IEEE Transactions
on Automatic Control, 52 (2007), pp. 1560–1571.

[23] M. Huang, R. P. Malhamé, and P. E. Caines, Large population stochastic dynamic games: closed-loop
McKean-Vlasov systems and the Nash certainty equivalence principle, Communications in Information
& Systems, 6 (2006), pp. 221–252.

[24] K. Iyer, R. Johari, and M. Sundararajan, Mean ﬁeld equilibria of dynamic auctions with learning,

Management Science, 60 (2014), pp. 2949–2970.

[25] N. Jay, N. Rotman, B. Godfrey, M. Schapira, and A. Tamar, A deep reinforcement learning
perspective on internet congestion control, in Proceedings of the 36th International Conference on

30

GU, GUO, WEI AND XU

Machine Learning, 2019, pp. 3050–3059.

[26] J. Jin, C. Song, H. Li, K. Gai, J. Wang, and W. Zhang, Real-time bidding with multi-agent
reinforcement learning in display advertising, in Proceedings of the 27th ACM International Conference
on Information and Knowledge Management, 2018, pp. 2193–2201.

[27] M. Kac, Foundations of kinetic theory, in Proceedings of the Third Berkeley Symposium on Mathematical
Statistics and Probability, vol. 3, University of California Press Berkeley and Los Angeles, California,
1956, pp. 171–197.

[28] J.-M. Lasry and P.-L. Lions, Mean ﬁeld games, Japanese journal of mathematics, 2 (2007), pp. 229–260.
[29] M. Lauer and M. Riedmiller, An algorithm for distributed reinforcement learning in cooperative multi-
agent systems, in Proceedings of the 17th International Conference on Machine Learning, Citeseer,
2000.

[30] M. Li, Z. Qin, Y. Jiao, Y. Yang, J. Wang, C. Wang, G. Wu, and J. Ye, Eﬃcient ridesharing order
dispatching with mean ﬁeld multi-agent reinforcement learning, in The World Wide Web Conference,
2019, pp. 983–994.

[31] K. Lin, R. Zhao, Z. Xu, and J. Zhou, Eﬃcient large-scale ﬂeet management via multi-agent deep
reinforcement learning, in Proceedings of the 24th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, 2018, pp. 1774–1783.

[32] Y. Luo, Z. Yang, Z. Wang, and M. Kolar, Natural actor-critic converges globally for hierarchical

linear quadratic regulator, arXiv preprint arXiv:1912.06875, (2019).

[33] H. P. McKean, Propagation of chaos for a class of non-linear parabolic equations, Stochastic Diﬀerential
Equations (Lecture Series in Diﬀerential Equations, Session 7, Catholic Univ., 1967), (1967), pp. 41–57.
[34] P. Mehta and S. Meyn, Q-learning and Pontryagin’s minimum principle, in Proceedings of the 48h IEEE
Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control Conference,
IEEE, 2009, pp. 3598–3605.

[35] S. Meyn, Algorithms for optimization and stabilization of controlled markov chains, Sadhana, 24 (1999),

pp. 339–367.

[36] S. Meyn, Control techniques for complex networks, Cambridge University Press, 2008.
[37] W. U. Mondal, M. Agarwal, V. Vaneet Aggarwal, and S. V. Ukkusuri, On the approximation
of cooperative heterogeneous multi-agent reinforcement learning (marl) using mean ﬁeld control (mfc),
arXiv preprint arXiv:2109.04024, (2021).

[38] M. Motte and H. Pham, Mean-ﬁeld Markov decision processes with common noise and open-loop

controls, arXiv preprint arXiv:1912.07883, (2019).

[39] R. Munos and A. Moore, Variable resolution discretization in optimal control, Machine Learning, 49

(2002), pp. 291–323.

[40] R. Munos and C. Szepesvári, Finite-time bounds for ﬁtted value iteration, Journal of Machine Learning

Research, 9 (2008), pp. 815–857.

[41] D. Ormoneit and P. Glynn, Kernel-based reinforcement learning in average-cost problems, IEEE

Transactions on Automatic Control, 47 (2002), pp. 1624–1636.

[42] D. Ormoneit and Ś. Sen, Kernel-based reinforcement learning, Machine Learning, 49 (2002), pp. 161–178.
[43] H. Pham and X. Wei, Discrete time McKean–Vlasov control problem: a dynamic programming approach,

Applied Mathematics & Optimization, 74 (2016), pp. 487–506.

[44] G. Qu, A. Wierman, and N. Li, Scalable reinforcement learning of localized policies for multi-agent

networked systems, arXiv preprint arXiv:1912.02906, (2019).

[45] N. Saldi, Discrete-time average-cost mean-ﬁeld games on polish spaces, Turkish Journal of Mathematics,

44 (2020), pp. 463–480.

[46] N. Saldi, T. Basar, and M. Raginsky, Markov–Nash equilibria in mean-ﬁeld games with discounted

cost, SIAM Journal on Control and Optimization, 56 (2018), pp. 4256–4287.

[47] N. Saldi, T. Başar, and M. Raginsky, Approximate nash equilibria in partially observed stochastic
games with mean-ﬁeld interactions, Mathematics of Operations Research, 44 (2019), pp. 1006–1033.
[48] N. Saldi, T. Başar, and M. Raginsky, Approximate markov-nash equilibria for discrete-time risk-

sensitive mean-ﬁeld games, Mathematics of Operations Research, (2020).

[49] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, Trust region policy optimization,

in International conference on machine learning, PMLR, 2015, pp. 1889–1897.

[50] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, Proximal policy optimization

MFC WITH Q-LEARNING FOR MARL GAMES

31

algorithms, arXiv preprint arXiv:1707.06347, (2017).

[51] D. Shah and Q. Xie, Q-learning with nearest neighbors, in Advances in Neural Information Processing

Systems, 2018, pp. 3111–3121.

[52] S. Shalev-Shwartz, S. Shammah, and A. Shashua, Safe, multi-agent, reinforcement learning for

autonomous driving, arXiv preprint arXiv:1610.03295, (2016).

[53] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrit-
twieser, I. Antonoglou, V. Panneershelvam, and M. Lanctot, Mastering the game of go with
deep neural networks and tree search, Nature, 529 (2016), p. 484.

[54] A. Smola, A. Gretton, L. Song, and B. Schölkopf, A Hilbert space embedding for distributions, in

International Conference on Algorithmic Learning Theory, Springer, 2007, pp. 13–31.
[55] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, MIT press, 2018.
[56] A.-S. Sznitman, Topics in propagation of chaos, in Ecole d’été de Probabilités de Saint-Flour XIX-1989,

Springer, 1991, pp. 165–251.

[57] G. Taylor and R. Parr, Kernelized value function approximation for reinforcement learning, in
Proceedings of the 26th International Conference on Machine Learning, 2009, pp. 1017–1024.
[58] H. Van Hasselt, Reinforcement learning in continuous state and action spaces, in Reinforcement Learning,

Springer, 2012, pp. 207–251.

[59] O. Vinyals, I. Babuschkin, J. Chung, M. Mathieu, M. Jaderberg, W. M. Czarnecki, A. Dudzik,
A. Huang, P. Georgiev, and R. Powell, Alphastar: Mastering the real-time strategy game starcraft
II, DeepMind Blog, (2019), p. 2.

[60] M. J. Wainwright, High-dimensional statistics: A non-asymptotic viewpoint, vol. 48, Cambridge

University Press, 2019.

[61] B.-C. Wang and Y. Liang, Robust mean ﬁeld social control problems with applications in analysis of

opinion dynamics, arXiv preprint arXiv:2002.12040, (2020).

[62] H. Wang, X. Wang, X. Hu, X. Zhang, and M. Gu, A multi-agent reinforcement learning approach to

dynamic service composition, Information Sciences, 363 (2016), pp. 96–119.
[63] C. J. Watkins and P. Dayan, Q-learning, Machine learning, 8 (1992), pp. 279–292.
[64] M. A. Wiering, Multi-agent reinforcement learning for traﬃc light control, in Proceedings of the 17th

International Conference Machine Learning, 2000, pp. 1151–1158.

[65] C. Wu, J. Zhang, et al., Viscosity solutions to parabolic master equations and McKean–Vlasov SDEs

with closed-loop controls, Annals of Applied Probability, 30 (2020), pp. 936–986.

[66] X. Xu, D. Hu, and X. Lu, Kernel-based least squares policy iteration for reinforcement learning, IEEE

Transactions on Neural Networks, 18 (2007), pp. 973–992.

[67] Y. Yang, R. Luo, M. Li, M. Zhou, W. Zhang, and J. Wang, Mean ﬁeld multi-agent reinforcement

learning, arXiv preprint arXiv:1802.05438, (2018).

[68] Y. Yang and J. Wang, An overview of multi-agent reinforcement learning from game theoretical

perspective, arXiv preprint arXiv:2011.00583, (2020).

[69] H. Yin, P. G. Mehta, S. P. Meyn, and U. V. Shanbhag, Learning in mean-ﬁeld games, IEEE

Transactions on Automatic Control, 59 (2013), pp. 629–644.

[70] K. Zhang, Z. Yang, and T. Başar, Multi-agent reinforcement learning: A selective overview of theories

and algorithms, arXiv preprint arXiv:1911.10635, (2019).

32

GU, GUO, WEI AND XU

Appendix A. Table of Parameters.

Notation Deﬁnition

RS
set of all real-valued measurable functions on measurable space S
set of all probability measures on S
P(S)
dP(S) metric induced by l1 norm: dP(S)(u, v) = (cid:80)

s∈S |u(s) − v(s)| for any u, v ∈ P(S)

γ
1(x ∈ A)

discount factor
indicator function of event {x ∈ A}

N number of agents
X state space of single agent
S action space of single agent

µN
t ∈ P(X )
νN
t ∈ P(U)
µt ∈ P(X )
νt ∈ P(U)

empirical state distribution of N agents at time t
empirical action distribution of N agents at time t
state distribution of the MFC problem at time t
action distribution of of the MFC problem at time t

C := P(X ) × H, the product space of P(X ) and H

H H := {h : X → P(U)} is the set of local policies
C
Π Π := {π = {πt}∞
individual reward

t=0 | πt : P(X ) → H} is the set of admissible policies

˜r(x, µ, u, ν(µ, h))

R bound of the reward, i.e., |˜r| < R

aggregated population reward r(µ, h) := (cid:80)
Lipschitz constant for transition matrix P

r(µ, h)
LP
L˜r Lipschitz constant for reward ˜r

Lr := ˜R + 2L˜r Lipschitz constant for r
LΦ := 2LP + 1 Lipschitz constant for Φ

(cid:80)

u∈U ˜r(x, µ, u, ν(µ, h))µ(x)h(x)(u)

x∈X

C(cid:15)
N(cid:15)
NH(cid:15)

(cid:15)-net on C
size of the (cid:15)-net C(cid:15) on C
size of the (cid:15)-net H(cid:15) on H

K(ci, c) weighted kernel function with ci ∈ C(cid:15) and c ∈ C

LK Lipschitz constant for kernel K
NK at most NK number of ci ∈ C(cid:15) satisﬁes K(c, ci) > 0
ΓK kernel regression operator from RC(cid:15) → RC
covering time of the (cid:15)-net under policy π
TC,π
constant appearing in Assumption 5.1, the controllability of the dynamics
M(cid:15)

Table 2
Summary of parameters for LMFC and MARL.

Appendix B. Proofs of Lemmas.

Proof of Lemma 2.2. At time step t, assume xt ∼ µt. Under the policy πt, it is easy to
check via direct computation that the corresponding action distribution νt is ν(µt, πt(·, µt)).

MFC WITH Q-LEARNING FOR MARL GAMES

33

Meanwhile, for any bounded function ϕ on X , by the law of iterated conditional expectation:
(cid:3)(cid:105)

(cid:105)
ϕ(x(cid:48))P (xt, µt, ut, νt)(x(cid:48))

Eπ[ϕ(xt+1)] = Eπ(cid:104)

Eπ(cid:2)ϕ(xt+1)|x0 . . . , xt

= Eπ(cid:104) (cid:88)

=

=

(cid:88)

ϕ(x(cid:48))Eπ(cid:104)

(cid:105)
P (xt, µt, ut, νt)(x(cid:48))

x(cid:48)∈X

x(cid:48)∈X
(cid:88)

x(cid:48)∈X

ϕ(x(cid:48))

(cid:88)

x∈X

µt(x)

(cid:88)

u∈U

πt(x, µt)(u)P (x, µt, u, νt)(x(cid:48)),

which concludes that xt+1 ∼ Φ(µt, πt(·, µt)). Here Eπ denotes the expectation under policy π.
Therefore, under π = {πt}∞
t=0 in
P(X ), and xt ∼ µt. Moreover, by Fubini’s theorem
(cid:21)

t=0, µt+1 = Φ(µt, πt(·, µt)) deﬁnes a deterministic ﬂow {µt}∞

(cid:20)

(cid:21)

vπ(µ) = Eπ

γt˜r(xt, µt, ut, νt)

(cid:12)
(cid:12)
x0 ∼ µ
(cid:12)
(cid:12)

=

γtEπ

∞
(cid:88)

t=0

(cid:12)
(cid:12)
˜r(xt, µt, ut, νt)
(cid:12)
(cid:12)

x0 ∼ µ

(cid:20) ∞
(cid:88)

t=0

(cid:20)
γtE

=

=

=

∞
(cid:88)

t=0
∞
(cid:88)

t=0
∞
(cid:88)

t=0

˜r(xt, µt, ut, νt)

(cid:12)
(cid:21)
(cid:12)
xt ∼ µt, ut ∼ πt(xt, µt)
(cid:12)
(cid:12)

γt (cid:88)

(cid:88)

x∈X

u∈U

˜r(x, µt, u, ν(µt, πt(·, µt)))µt(x)πt(x, µt)(u)

γtr(µt, πt(·, µt)).

This proves (2.5).

Proof of Lemma 2.6 .

(cid:107)ν(µ, h) − ν(µ(cid:48), h(cid:48))(cid:107)1 ≤ (cid:107)ν(µ, h) − ν(µ, h(cid:48))(cid:107)1 + (cid:107)ν(µ, h(cid:48)) − ν(µ(cid:48), h(cid:48))(cid:107)1

≤

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

x∈X

(cid:88)

x∈X

(cid:12)
(cid:12)
(h(x) − h(cid:48)(x))µ(x)
(cid:12)
(cid:12)
(cid:12)1
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(µ(x) − µ(cid:48)(x))h(cid:48)(x)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)1

µ(x)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)h(x) − h(cid:48)(x)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)1
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(µ(x) − µ(cid:48)(x))h(cid:48)(x)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)1

(cid:88)

x∈X

(cid:88)

x∈X

≤ max
x∈X

(cid:12)
(cid:12)
(cid:12)
(cid:12)h(x) − h(cid:48)(x)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)1

+

(cid:88)

(cid:88)

u∈U

x∈X

|µ(x) − µ(cid:48)(x)|h(cid:48)(x)(u)

= dH(h, h(cid:48)) + (cid:107)µ − µ(cid:48)(cid:107)1 = dC((µ, h), (µ(cid:48), h(cid:48))).

Proof of Lemma 2.7.

|r(µ, h) − r(µ(cid:48), h(cid:48))|
(cid:12)
(cid:88)
(cid:12)
(cid:12)

(cid:88)

˜r(x, µ, u, ν(µ, h))µ(x)h(x)(u) −

(cid:88)

(cid:88)

˜r(x, µ(cid:48), u, ν(µ(cid:48), h(cid:48)))µ(cid:48)(x)h(cid:48)(x)(u)

(cid:12)
(cid:12)
(cid:12)

u∈U

x∈X
u∈U
(For simplicity, denote ˜rx,u = ˜r(x, µ, u, ν(µ, h)), ˜r(cid:48)
(cid:12)
(cid:88)
(cid:12)
(cid:12)

x,u)µ(x)h(x)(u)

(˜rx,u − ˜r(cid:48)

(cid:12)
(cid:12)
(cid:12) +

(cid:88)

(cid:88)

(cid:88)

x∈X

(cid:12)
(cid:12)
(cid:12)

x,u = ˜r(x, µ(cid:48), u, ν(µ(cid:48), h(cid:48))).)
(cid:12)
x,u(µ(x)h(x)(u) − µ(cid:48)(x)h(cid:48)(x)(u))
˜r(cid:48)
(cid:12)
(cid:12).

x∈X

u∈U

x∈X

u∈U

=

≤

34

GU, GUO, WEI AND XU

By Assumption 2.4 and Lemma 2.6, for any x ∈ X , u ∈ U,

|˜rx,u − ˜r(cid:48)

x,u| ≤ L˜r((cid:107)µ − µ(cid:48)(cid:107)1 + (cid:107)ν(µ, h), ν(µ(cid:48), h(cid:48))(cid:107)1)

≤ L˜r · ((cid:107)µ − µ(cid:48)(cid:107)1 + dC((µ, h), (µ(cid:48), h(cid:48)))) ≤ 2L˜rdC((µ, h), (µ(cid:48), h(cid:48))).

Meanwhile,

≤

=

(cid:88)

(cid:88)

x∈X
(cid:88)

u∈U
(cid:88)

|µ(x)h(x)(u) − µ(cid:48)(x)h(cid:48)(x)(u)|

|µ(x) − µ(cid:48)(x)|h(x)(u) +

(cid:88)

(cid:88)

µ(cid:48)(x)|h(x)(u) − h(cid:48)(x)(u)|

x∈X
(cid:88)

u∈U
|µ(x) − µ(cid:48)(x)| +

x∈X

≤ (cid:107)µ − µ(cid:48)(cid:107)1 + max
x∈X

u∈U

x∈X
µ(cid:48)(x)(cid:107)h(x) − h(cid:48)(x)(cid:107)1

(cid:88)

x∈X

(cid:107)h1(x) − h2(x)(cid:107)1 = dC((µ, h), (µ(cid:48), h(cid:48))).

Combining all these results, we have

|r(µ, h) − r(µ(cid:48), h(cid:48))| ≤

(cid:88)

(cid:88)

x∈X

u∈U

|˜rx,u − ˜r(cid:48)

x,u|µ(x)h(x)(u) + ˜R

(cid:88)

(cid:88)

x∈X

u∈U

|µ(x)h(x)(u) − µ(cid:48)(x)h(cid:48)(x)(u)|

≤ ( ˜R + 2L˜r)dC((µ, h), (µ(cid:48), h(cid:48))).

Proof of Lemma 2.8.

(cid:107)Φ(µ, h) − Φ(µ(cid:48), h(cid:48))(cid:107)1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

(cid:88)

x∈X

u∈U

P (x, µ, u, ν(µ, h))µ(x)h(x)(u) −

(cid:88)

(cid:88)

x∈X

u∈U

P (x, µ(cid:48), u, ν(µ(cid:48), h(cid:48)))µ(cid:48)(x)h(cid:48)(x)(u)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)1
(cid:12)

x,u = P (x, µ(cid:48), u, ν(µ(cid:48), h(cid:48))).)
(For simplicity, denote Px,u = P (x, µ, u, ν(µ, h)), P (cid:48)
(cid:12)
(cid:12)
(cid:12)
x,u(µ(x)h(x)(u) − µ(cid:48)(x)h(cid:48)(x)(u))
P (cid:48)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
x,u)µ(x)h(x)(u)
(cid:12)

(Px,u − P (cid:48)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

+

(cid:12)
(cid:12)
(cid:12)1

(cid:12)
(cid:12)
(cid:12)1

.

=

≤

x∈X

u∈U

x∈X

u∈U

By Assumption 2.5 and Lemma 2.6, for any x and u,

||Px,u − P (cid:48)

x,u||1 ≤ LP · ((cid:107)µ − µ(cid:48)(cid:107)1 + (cid:107)ν(µ, h) − ν(µ(cid:48), h(cid:48))(cid:107)1)

≤ LP · ((cid:107)µ − µ(cid:48)(cid:107)1 + dC((µ, h), (µ(cid:48), h(cid:48)))) ≤ 2LP · dC((µ, h), (µ(cid:48), h(cid:48))).

Meanwhile, from the proof of Lemma 2.7, we know

(cid:88)

(cid:88)

x∈X

u∈U

|µ(x)h(x)(u) − µ(cid:48)(x)h(cid:48)(x)(u)| ≤ dC((µ, h), (µ(cid:48), h(cid:48))).

Combining all these results, we have

(cid:107)Φ(µ, h) − Φ(µ(cid:48), h(cid:48))(cid:107)1 ≤

(cid:88)

(cid:88)

x∈X

u∈U

||Px,u − P (cid:48)

x,u||1µ(x)h(x, u) +

(cid:88)

(cid:88)

x∈X

u∈U

||P (cid:48)

x,u||1|µ(x)h(x)(u) − µ(cid:48)(x)h(cid:48)(x)(u))|

≤ (2LP + 1)dC((µ, h), (µ(cid:48), h(cid:48))).

MFC WITH Q-LEARNING FOR MARL GAMES

35

Proof of Lemma 3.4. To prove the continuity of Q, ﬁrst ﬁx c and c(cid:48) ∈ C. Then there exists
some policy π such that Q(c) − Qπ(c) < (cid:15)
2 . Let c = (µ0, h0), (µ1, h1), (µ2, h2), . . . , (µt, ht), . . .
be the trajectory of the system starting from c and then taking the policy π. Then Qπ(c) =
(cid:80)∞

t=0 γtr(µt, ht).
Now consider the trajectory of the system starting from c(cid:48) and then taking h1, . . . , ht, . . . ,
t, ht), . . . . Note that this trajectory starting
t, ht). By Lemma 2.7

denoted by c(cid:48) = (µ(cid:48)
1, h1), (µ(cid:48)
from c(cid:48) may not be the optimal trajectory, therefore, Q(c(cid:48)) ≥ (cid:80)∞
and Lemma 2.8,

2, h2), . . . , (µ(cid:48)

t=0 γtr(µ(cid:48)

0), (µ(cid:48)

0, h(cid:48)

|r(µ(cid:48)

t, ht) − r(µt, ht)| ≤ Lr · dP(X )(µ(cid:48)

t, µt) = Lr · dP(X )(Φ(µ(cid:48)

≤ Lr · LΦ · dP(X )(µ(cid:48)

t−1, µt−1) ≤ · · · ≤ Lr · Lt

t−1, ht−1), Φ(µt−1, ht−1))
Φ · dC(c, c(cid:48)),

implying that

Q(c) − Q(c(cid:48)) ≤

≤

(cid:15)
2

(cid:15)
2

+ Qπ(c) − Q(c(cid:48)) ≤

(cid:15)
2

+ (r(c) − r(c(cid:48))) +

∞
(cid:88)

t=1

γt(r(µt, ht) − r(µ(cid:48)

t, ht))

+

∞
(cid:88)

t=0

γt · Lt

Φ · Lr · dC(c, c(cid:48)) =

(cid:15)
2

+

Lr
1 − γ · LΦ

· dC(c, c(cid:48)).

2 + Lr

, |Q(c(cid:48)) − Q(c)| ≤ (cid:15). This proves that Q is continuous.

Similarly, one can show Q(c(cid:48)) − Q(c) ≤ (cid:15)
(cid:15)·(1−γ·LΦ)
2Lr
Proof of Lemma 5.3. By deﬁnition, it is easy to show that B and BH(cid:15) map {f ∈ RC :
(cid:107)f (cid:107)∞ ≤ Vmax} to itself, B(cid:15) and (cid:98)B(cid:15) map {f ∈ RC(cid:15) : (cid:107)f (cid:107)∞ ≤ Vmax} to itself, and T maps
{f ∈ RP(X ) : (cid:107)f (cid:107)∞ ≤ Vmax} to itself.

· dC(c, c(cid:48)). Therefore, as long as dC(c, c(cid:48)) ≤

1−γ·LΦ

For B(cid:15), we have

(cid:107)B(cid:15)q1 − B(cid:15)q2(cid:107)∞ ≤ γ max
c∈C(cid:15)

max
˜h∈H(cid:15)

|ΓKq1(Φ(c), ˜h) − ΓKq2(Φ(c), ˜h)|

≤ γ max
c∈C(cid:15)

max
˜h∈H(cid:15)

N(cid:15)(cid:88)

i=1

K(ci, (Φ(c), ˜h))|q1(ci) − q2(ci)| ≤ γ(cid:107)q1 − q2(cid:107)∞,

where we use (4.2) for the property of kernel function K(ci, c).

Therefore, B(cid:15) is a contraction mapping with modulus γ < 1 under the sup norm on
{f ∈ RC(cid:15) : (cid:107)f (cid:107)∞ ≤ Vmax}. By Banach Fixed Point Theorem, the statement for B(cid:15) holds.
Similar arguments prove the statements for the other four operators.

Proof of Lemma 5.4. Using the same DPP argument as in Theorem 3.2, we can show the
value function for (5.9)-(5.10) is a ﬁxed point for T (5.3) in {f ∈ RP(X ) : (cid:107)f (cid:107)∞ ≤ Vmax}. By
Lemma 5.3, it coincides with VH(cid:15).

To prove (5.11), recall from Lemma 5.3 that T is a contraction mapping with modulus γ
with the supremum norm on {f ∈ RP(X ) : (cid:107)f (cid:107)∞ ≤ Vmax}, with a ﬁxed point VH(cid:15) which is the
value function of the MFC (5.9)-(5.10), i.e., (MDP) with the action space restricted to H(cid:15).

36

GU, GUO, WEI AND XU

Moreover, deﬁne ˜Q(µ, h) := r(µ, h) + γVH(cid:15)(Φ(µ, h)). Then

˜Q(µ, h) = r(µ, h) + γVH(cid:15)(Φ(µ, h))
= r(µ, h) + γ max
˜h∈H(cid:15)
= r(µ, h) + γ max
˜h∈H(cid:15)

˜Q(Φ(µ, h), ˜h).

(r(Φ(µ, h), ˜h) + γVH(cid:15)(Φ(Φ(µ, h), ˜h)))

So ˜Q ∈ {f ∈ RC : (cid:107)f (cid:107)∞ ≤ Vmax} is a ﬁxed point of BH(cid:15). By Lemma 5.3, ˜Q = QH(cid:15).

Now, since QH(cid:15) is the value function of the MFC problem (5.9), replacing Q with QH(cid:15) in

the argument of Lemma 3.5 and then taking (cid:15) → 0 yield the Lipschitz continuity of QH(cid:15).

Proof of Lemma 5.7. By Markov’s inequality,

P(TC,π > eT ) ≤

E[TC,π]
eT

≤

1
e

.

Since TC,π is independent of the initial state and the dynamics are Markovian, the probability
that C(cid:15) has not been covered during any time period with length eT is less or equal to 1
e .
Therefore, for any positive integer k, P(TC,π > ekT ) ≤ 1
ek . Take k = log(1/δ) and we get the
desired result.

Proof of Corollary 6.4. From (6.1), we have for any µN =

(cid:80)N

j=1 1(xj,N =x)
N

,

vπ∗

(µN ) −

C
√
N

≤ uπ∗

N (µN ) ≤ vπ∗

(µN ) +

C
√
N

, v(cid:101)π(µN ) −

C
√
N

≤ u(cid:101)π

N (µN ) ≤ v(cid:101)π(µN ) +

C
√
N

.

By the optimality condition, we have v(cid:101)π(µN ) ≤ vπ∗(µN ). Hence

(B.1)

N (µN ) ≤ v(cid:101)π(µN ) +
u(cid:101)π

C
√
N

≤ vπ∗

(µN ) +

C
√
N

.

Similarly since u(cid:101)π(µN ) ≥ uπ∗(µN ), we have

(B.2)

N (µN ) ≤ uπ∗
vπ∗

(µN ) +

C
√
N

≤ u(cid:101)π(µN ) +

C
√
N

.

Combining (B.1) and (B.2) leads to the desired result.

