Learning Intrusion Prevention Policies
through Optimal Stopping

Kim Hammar †‡ and Rolf Stadler†‡
† Division of Network and Systems Engineering, KTH Royal Institute of Technology, Sweden
‡ KTH Center for Cyber Defense and Information Security, Sweden
@kth.se
kimham, stadler
{
}
February 11, 2022

Email:

2
2
0
2

b
e
F
0
1

]
I

A
.
s
c
[

8
v
0
6
1
7
0
.
6
0
1
2
:
v
i
X
r
a

Abstract—We study automated intrusion prevention using
reinforcement learning. In a novel approach, we formulate the
problem of intrusion prevention as an optimal stopping problem.
This formulation allows us insight into the structure of the
optimal policies, which turn out to be threshold based. Since
the computation of the optimal defender policy using dynamic
programming is not feasible for practical cases, we approximate
the optimal policy through reinforcement learning in a simulation
environment. To deﬁne the dynamics of the simulation, we
emulate the target infrastructure and collect measurements. Our
evaluations show that the learned policies are close to optimal
and that they indeed can be expressed using thresholds.

Index Terms—Network Security, automation, optimal stopping,

reinforcement learning, Markov Decision Processes

I. INTRODUCTION

An organization’s security strategy has traditionally been
deﬁned, implemented, and updated by domain experts [1].
Although this approach can provide basic security for an
organization’s communication and computing infrastructure,
a growing concern is that infrastructure update cycles become
shorter and attacks increase in sophistication. Consequently,
the security requirements become increasingly difﬁcult
to
meet. As a response, signiﬁcant efforts are made to automate
security processes and functions. Over the last years, research
directions emerged to automatically ﬁnd and update security
policies. One such direction aims at automating the creation
of threat models for a given infrastructure [2]. A second
direction focuses on evolutionary processes that produce novel
exploits and corresponding defenses [3]. In a third direction,
the interaction between an attacker and a defender is modeled
as a game, which allows attack and defense policies to be
analyzed and sometimes constructed using game theory [4],
[5]. In a fourth direction, statistical tests are used to detect
attacks [6]. Further, the evolution of an infrastructure and
the actions of a defender is studied using the framework of
dynamical systems. This framework allows optimal policies
to be obtained using methods from control
theory [7] or
dynamic programming [8], [9]. In all of the above directions,
machine learning techniques are often applied to estimate
model parameters and policies [10], [11].

Many activities center around modeling the infrastructure
as a discrete-time dynamical system in the form of a Markov
Decision Process (MDP). Here, the possible actions of the
defender are deﬁned by the action space of the MDP, the
defender policy is determined by the actions that the defender
takes in different states, and the security objective is encoded
in the reward function, which the defender tries to optimize.
To ﬁnd the optimal policy in an MDP, two main methods
are used: dynamic programming and reinforcement learning.
The advantage of dynamic programming is that it has a strong
theoretical grounding and oftentimes allows to derive proper-
ties of the optimal policy [12], [13]. The disadvantage is that it
requires complete knowledge of the MDP, including the tran-
sition probabilities. In addition, the computational overhead is
high, which makes it infeasible to compute the optimal policy
for all but simple conﬁgurations [14], [13], [8]. Alternatively,
reinforcement learning enables learning the dynamics of the
model through exploration. With the reinforcement learning
approach, it is often possible to compute close approximations
of the optimal policy for non-trivial conﬁgurations [10], [15],
[14], [16]. As a drawback, however, theoretical insights into
the structure of the optimal policy generally remain elusive.

In this paper, we study an intrusion prevention use case
that involves the IT infrastructure of an organization. The
operator of this infrastructure, which we call the defender,
takes measures to protect it against a possible attacker while,
at the same time, providing a service to a client population.
The infrastructure includes a public gateway through which
the clients access the service and which also is open to
a plausible attacker. The attacker decides when to start an
intrusion and then executes a sequence of actions that includes
reconnaissance and exploits. Conversely, the defender aims
at preventing intrusions and maintaining the service to its
clients. It monitors the infrastructure and can block outside
access to the gateway, an action that disrupts the service
but stops any ongoing intrusion. What makes the task of the
defender difﬁcult is the fact that it lacks direct knowledge of
the attacker’s actions and must infer that an intrusion occurs
from monitoring data.

We study the use case within the framework of discrete-time
dynamical systems. Speciﬁcally, we formulate the problem of
ﬁnding an optimal defender policy as an optimal stopping
problem, where stopping refers to blocking access to the gate-

1

 
 
 
 
 
 
way. Optimal stopping is frequently used to model problems
in the ﬁelds of ﬁnance and communication systems [17], [18],
[6], [19]. To the best of our knowledge, ﬁnding an intrusion
prevention policy through solving an optimal stopping problem
is a novel approach.

By formulating intrusion prevention as an optimal stopping
problem, we know from the theory of dynamic programming
that the optimal policy can be expressed through a threshold
that is obtained from observations, i.e. from infrastructure
measurements [13], [12]. This contrasts with prior works that
formulate the problem using a general MDP, which does not
allow insight into the structure of optimal policies [10], [11],
[20], [21].

To account for the fact that the defender only has access to a
limited number of measurements and cannot directly observe
the attacker, we model the optimal stopping problem with a
Partially Observed Markov Decision Process (POMDP). We
obtain the defender policies by simulating a series of POMDP
episodes in which an intrusion takes place and where the
defender continuously updates its policy based on outcomes of
previous episodes. To update the policy, we use a state-of-the-
art reinforcement learning algorithm. This approach enables
us to ﬁnd effective defender policies despite the uncertainty
about the attacker’s behavior and despite the large state space
of the model.

We validate our approach to intrusion prevention for a non-
trivial infrastructure conﬁguration and two attacker proﬁles.
Through extensive simulation, we demonstrate that the learned
defender policies indeed are threshold based, that they con-
verge quickly, and that they are close to optimal.

We make two contributions with this paper. First, we
formulate the problem of intrusion prevention as a problem of
optimal stopping. This novel approach allows us a) to derive
properties of the optimal defender policy using results from
dynamic programming and b) to use reinforcement learning
techniques to approximate the optimal policy for a non-
trivial conﬁguration. Second, we instantiate the simulation
model with measurements collected from an emulation of the
target infrastructure, which reduces the assumptions needed to
construct the simulation model and narrows the gap between a
simulation episode and a scenario playing out in a real system.
This addresses a limitation of related work that rely on abstract
assumptions to construct the simulation model [10], [11], [20],
[21].

II. THE INTRUSION PREVENTION USE CASE

We consider an intrusion prevention use case that involves
the IT infrastructure of an organization. The operator of this
infrastructure, which we call the defender, takes measures to
protect it against an attacker while, at the same time, providing
a service to a client population (Fig. 1). The infrastructure
includes a set of servers that run the service and an intrusion
detection system (IDS) that logs events in real-time. Clients
access the service through a public gateway, which also is
open to the attacker.

Attacker

Clients
. . .

alerts

Gateway

1

1

IDS

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

25

26

19

20

21

22

23

24

27

28

29

30

31

Fig. 1: The IT infrastructure and the actors in the use case.

Defender

We assume that the attacker intrudes into the infrastructure
through the gateway, performs reconnaissance, and exploits
found vulnerabilities, while the defender continuously mon-
itors the infrastructure through accessing and analyzing IDS
statistics and login attempts at the servers. The defender has a
single action to stop the attacker, which involves blocking all
outside access to the gateway. As a consequence of this action,
the service as well as any ongoing intrusion are disrupted.

When deciding whether to block the gateway, the defender
must balance two objectives: to maintain the service to its
clients and to keep a possible attacker out of the infrastructure.
The optimal policy for the defender is to maintain service until
the moment when the attacker enters through the gateway, at
which time the gateway must be blocked. The challenge for
the defender is to identify the precise time when this moment
occurs.

In this work, we model the attacker as an agent that starts the
intrusion at a random point in time and then takes a predeﬁned
sequence of actions, which includes reconnaissance to explore
the infrastructure and exploits to compromise the servers.

We study the use case from the defender’s perspective.
The evolution of the system state and the actions by the
defender are modeled with a discrete-time Partially Observed
Markov Decision Process (POMDP). The reward function of
this process encodes the beneﬁt of maintaining service and the
loss of being intruded. Finding an optimal defender policy thus
means maximizing the expected reward. To ﬁnd an optimal
policy, we solve an optimal stopping problem, where the
stopping action refers to blocking the gateway.

III. THEORETICAL BACKGROUND

This section contains background information on Markov
decision processes, reinforcement learning, and optimal stop-
ping.

2

A. Markov Decision Processes

,

(cid:105)

P

=

M

,
A

[22], [23].

A Markov Decision Process (MDP) models the control of
a discrete-time dynamical system and is deﬁned by a seven-
at
at
tuple
st,st+1, γ, ρ1, T
st,st+1,
R
(cid:104)S
denotes the set of states and

S
denotes the set of actions.
A
at
st,st+1 refers to the probability of transitioning from state
P
st to state st+1 when taking action at (Eq. 1), which has the
s1, . . . , st]. Similarly,
Markov property P [st+1|
at
R is the expected reward when taking action at
st,st+1 ∈
R
at
and transitioning from state st to state st+1 (Eq. 2). If
st,st+1
at
st,st+1 are independent of the time-step t, the MDP is
and
said to be stationary. Finally, γ
(0, 1] is the discount factor,
[0, 1] is the initial state distribution, and T is the
ρ1 :
S →
time horizon.

st] = P [st+1|

R

P

∈

at

st,st+1 = P [st+1|
P
st,st+1 = E [rt+1|
R

at

st, at]
at, st, st+1]

(1)

(2)

The system evolves in discrete time-steps from t = 1 to t = T ,
which constitute one episode of the system.

S

,

,

Z

O

R

Z(cid:105)

MP

,
A

[13].

In contrast

P
(ot+1, st+1, at) = P[ot+1|

is an extension of an MDP [24],
an MDP,
servable. A POMDP is deﬁned by a nine-tuple
at
st,st+1 , γ, ρ1, T,

A Partially Observed Markov Decision Process (POMDP)
to
in a POMDP the states are not directly ob-
=
. The ﬁrst seven ele-
denotes the set of observations
st+1, at] is the observation

at
st,st+1,
(cid:104)S
ments deﬁne an MDP.
O
and
.
, and at ∈ A
, st+1 ∈ S
function, where ot+1 ∈ O
is deﬁned as bt(s) = P[st = s
The belief state bt ∈ B
ht]
|
for all s
1)-
. The belief space
) is the unit (
B
|S|−
simplex [25], [26], where ∆(
) denotes the set of probability
. bt is a sufﬁcient statistic of the state st
distributions over
based on the history ht of the initial state distribution, the
actions, and the observations: ht = (ρ1, a1, o1, . . . , at

∈
. By deﬁning the state at time t to be the belief state bt, a
=

H
POMDP can be formulated as a continuous-state MDP:

= ∆(

1, ot)

∈ S

S

S

S

−

,
A

at
bt,bt+1,

.
,
(cid:105)
The belief state can be computed recursively as follows [13]:

at
bt,bt+1, γ, ρ1, T

R

P

(cid:104)B

M

bt+1(st+1) = C

(ot+1, st+1, at)

Z

at
stst+1bt(st)

P

(3)

where C = 1/ (cid:80)
at
st,st+1bt(st)
is a normalizing factor independent of st+1 to make bt+1
sum to 1.

(ot+1, st+1, at) (cid:80)

S Z
∈

S P

st∈

st+1

(cid:88)

st∈S

B. The Reinforcement Learning Problem

Reinforcement learning deals with the problem of choosing
a sequence of actions for a sequentially observed state variable
to maximize a reward function [14], [16]. This problem can
be modeled with an MDP if the state space is observable, or
with a POMDP if the state space is not fully observable.

1, . . . , T
{

In the context of an MDP, a policy is deﬁned as a function
) denotes the set of
. In the case of a POMDP,
), or,
). In

π :
probability distributions over
a policy is deﬁned as a function π :
alternatively, as a function π :
1, . . . , T
{

H →
} × B →

∆(
A
∆(
A

), where ∆(

} × S →

∆(

A

A

A

3

both cases, a policy is called stationary if it is independent of
the time-step t.

An optimal policy π∗ is a policy that maximizes the
expected discounted cumulative reward over the time horizon
T :

π∗

∈

arg max

π

Π

∈

Eπ

(cid:35)

γt

1rt
−

(cid:34) T

(cid:88)

t=1

(4)

where Π is the policy space, γ is the discount factor, rt is the
reward at time t, and Eπ denotes the expectation under π.

It is well known that optimal deterministic policies exist
for MDPs and POMDPs with ﬁnite state and action spaces
[23], [13]. Further, for stationary MDPs and POMDPs with
inﬁnite or random time-horizons, optimal stationary policies
exist [23], [13].

The Bellman equations relate any optimal policy π∗ to the
R, where

two value functions V ∗ :

R and Q∗ :

and

A

S →

S × A →

are state and action spaces of an MDP [27]:
(cid:3)

(cid:2)rt+1 + γV ∗(st+1)
E

V ∗(st) = max
at∈A
(cid:2)rt+1 + γV ∗(st+1)

st, at
|
(cid:3)
st, at
|

Q∗(st, at) = E
π∗(st)

∈

arg max
at∈A

Q∗(st, at)

(5)

(6)

(7)

Here, V ∗(st) and Q∗(st, at) denote the expected cumulative
discounted reward under π∗ for each state and state-action pair,
respectively. In the case of a POMDP, the Bellman equations
contain bt instead of st. Solving the Bellman equations (Eqs.
5-6) means computing the value functions from which an
optimal policy can be obtained (Eq. 7).

Two principal methods are used for ﬁnding an optimal
policy in a MDP or POMDP: dynamic programming and
reinforcement learning.

First, the dynamic programming method (e.g. value itera-
tion [12], [23]) assumes complete knowledge of the seven-
tuple MDP or the nine-tuple POMDP and obtains an optimal
policy by solving the Bellman equations iteratively (Eq. 7),
with polynomial time-complexity per iteration for MDPs and
PSPACE-complete time-complexity for POMDPs [28].

Second, the reinforcement learning method computes or
approximates an optimal policy without requiring complete
knowledge of the transition probabilities or observation proba-
bilities of the MDP or POMDP. Three classes of reinforcement
learning algorithms exist: value-based algorithms, which ap-
proximate solutions to the Bellman equations (e.g. Q-learning
[29]); policy-based algorithms, which directly search through
policy space using gradient-based methods (e.g. Proximal
Policy Optimization (PPO) [30]); and model-based algorithms,
which learn the transition or observation probabilities of
the MDP or POMDP (e.g. Dyna-Q [16]). The three algo-
rithm types can also be combined, e.g. through actor-critic
algorithms, which are mixtures of value-based and policy-
based algorithms [16]. In contrast to dynamic programming
algorithms, reinforcement learning algorithms generally have
no guarantees to converge to an optimal policy except for the
tabular case [31], [32].

C. Markovian Optimal Stopping Problems

Episode

Optimal stopping is a classical problem in statistics with
a developed theory [33], [34], [35], [12], [23]. Example
applications of this problem are: selling an asset [12], detecting
distribution changes [6], machine replacement [13], valuing a
ﬁnancial option [17], and choosing a candidate for a job (the
secretary problem) [23].

Different versions of the problem can be found in the
literature. Including, discrete-time and continuous time, ﬁnite
horizon and inﬁnite horizon, single-stop and multiple stops,
fully observed and partially observed, independent and depen-
dent, and Markovian and non-Markovian. Consequently, there
are also different solution methods, most prominent being the
martingale approach [35] and the Markovian approach [34],
[12], [23]. In this paper, we consider a partially observed
Markovian optimal stopping problem in discrete-time with a
ﬁnite horizon and a single stop action.

A Markovian optimal stopping problem can be seen as
a speciﬁc kind of MDP or POMDP where the state of
the environment evolves as a discrete-time Markov process
(st)T
t=1 which is either fully or partially observed [23], [13].
At each time-step t of this decision process,
two actions
are available: “stop” and “continue”. The stop action causes
the interaction with the environment to stop and yields a
stopping-reward. Conversely, the continue action causes the
environment
time-step and yields a
continuation-reward. The stopping time τ is a random variable
dependent on s1, . . . , st and independent of st+1, . . . sT [35].

to evolve to the next

The objective is to ﬁnd a stopping policy π(st)

}
that maximizes the expected reward, where π(st) = S indi-
cates a stopping action. This induces the following maximiza-
tion at each time-step before stopping (the Bellman equation
[27]):

(cid:55)→ {

S, C

(cid:34)

max

(cid:3)

(cid:2)
S
E
ss(cid:48)
R
(cid:124) (cid:123)(cid:122) (cid:125)
stopping reward

, E
(cid:124)

(cid:35)

(cid:2)

C
ss(cid:48)

+ γV ∗(s(cid:48))(cid:3)
R
(cid:125)
(cid:123)(cid:122)
continuation reward

(8)

To solve the maximization above, standard solution methods
for MDPs and POMDPs can be applied, such as dynamic
programming and reinforcement learning [12], [18]. Further,
the solution can be characterized using dynamic programming
theory as the least excessive (or superharmonic) majorant of
the reward function, or using martingale theory as the Snell
envelope of the reward function [36], [35].

IV. FORMALIZING THE INTRUSION PREVENTION USE
CASE AND OUR REINFORCEMENT LEARNING APPROACH

In this section, we ﬁrst formalize the intrusion prevention
use case described in Section II and then we introduce our
solution method. Speciﬁcally, we ﬁrst deﬁne a POMDP model
of the intrusion prevention use case. Then, we describe our
reinforcement learning approach to approximate the optimal
defender policy. Lastly, we use the theory of dynamic program-
ming to derive the threshold property of the optimal policy.

4

time-step t = 1

Intrusion event

Intrusion ongoing

Early stopping times

t

t = T

Stopping times that
interrupt the intrusion

Fig. 2: Optimal stopping formulation of intrusion prevention;
the horizontal axis represents time; T is the time horizon; the
episode length is T
1; the dashed line shows the intrusion
start time; the optimal policy is to stop at the time of intrusion.

−

A. A POMDP Model of the Intrusion Prevention Use Case

We model the intrusion prevention use case as a partially
observed optimal stopping problem where an intrusion starts
at a geometrically distributed time and the stopping action
refers to blocking the gateway (Fig. 2). This type of optimal
stopping problem is often referred to as a quickest change
detection problem [35], [34], [6].

To formalize this model, we use a POMDP. This model
includes the state space and the observation space of the
defender. It further includes the initial state distribution, the
defender actions, the transition probabilities, the observation
function, the reward function, and the optimization objective.
1) States
, Initial State Distribution ρ1, and Observations
: The system state st is deﬁned by the intrusion state it ∈
O
, where it = 1 if an intrusion is ongoing. Further, we
0, 1
{
}
introduce a terminal state
, which is reached either when the
∅
defender stops or when the attacker completes an intrusion.
Thus,

S

.

=

0, 1
{

} ∪ ∅

S

At time t = 1 no intrusion is in progress. Hence, the initial
state distribution is the degenerate distribution ρ1(s1 = 0) = 1.
The defender has a partial view of the system state and does
not know whether an intrusion is in progress. Speciﬁcally,
if the defender has not stopped, it observes three counters
ot = (∆xt, ∆yt, ∆zt). The counters are upper bounded,
,
, ∆yt ∈ {
where ∆xt ∈ {
0, . . . , Ymax}
denote the number of severe IDS alerts,
∆zt ∈ {
warning IDS alerts, and login attempts generated during time-
step t, respectively. Otherwise, if the defender has stopped, it
observes ot = st =
. Consequently,
0, . . . , Xmax} ×
∅
{
.
0, . . . , Zmax} ∪ ∅
0, . . . , Ymax} × {
{
2) Actions

: The defender has two actions: “stop” (S)

0, , . . . , Xmax}

0, . . . , Zmax}

O

=

P

A

.
}

a
ss(cid:48)

S, C
{

3) Transition Probabilities

and “continue” (C). The action space is thus

=
: We model the start of
an intrusion by a Bernoulli process (Qt)T
t=1, where Qt ∼
Ber(p = 0.2) is a Bernoulli random variable. The time t of
the ﬁrst occurrence of Qt = 1 is the change point representing
the start time of the intrusion It, which thus is geometrically
Ge(p = 0.2) (Fig. 3). As the geometric
distributed, i.e. It ∼
distribution has the memoryless property, the intrusion start
time is Markovian.

A

Eq. 15 states that the reward in the terminal state is zero.
Eq. 16 indicates that stopping an intrusion incurs a reward but
stopping before an intrusion starts yields a loss, where S is the
stop action and 1 is the indicator function. Lastly, as can be
seen from Eq. 17, the defender receives a positive reward for
maintaining service and a loss for taking the continue action
C while under intrusion. This means that the maximal reward
is received if the defender stops when an intrusion starts.

∅

] <

6) Time Horizon T
∅

: The time horizon is deﬁned by the
is reached, which is a
time-step when the terminal state
. Since we know that the expectation of the
random variable T
∅
intrusion time It is ﬁnite, we conclude that the horizon is ﬁnite
for any policy πθ that is guaranteed to stop: Eπθ [T
∅

7) Policy Space Πθ, and Objective J: Since the POMDP
is stationary and the time horizon T
is not pre-determined, it
∅
is sufﬁcient to consider stationary policies. Further, although
an optimal deterministic policy exists [23], [13], we consider
stochastic policies to allow smooth optimization. Speciﬁcally,
we consider the space of stationary stochastic policies Πθ
), which is
where πθ ∈
parameterized by a vector θ

Πθ is a policy π :
Rd.
Πθ maximizes the expected
:

∈
The optimal policy π∗θ ∈

cumulative reward over the random horizon T
∅

H →

.
∞

∆(

A

J(θ) = Eπθ

(cid:34) T

∅(cid:88)

(cid:35)
1r(st, at)

γt

−

t=1

, π∗ = arg max

J(θ) (18)

πθ∈

Πθ

where we set the discount factor γ = 1.

Eq. 18 deﬁnes the objective of the optimal stopping prob-
lem. In the following section, we describe our approach for
solving this problem using reinforcement learning.

B. Our Reinforcement Learning Approach

Since the POMDP model is unknown to the defender, we use
a model-free reinforcement learning approach to approximate
the optimal policy. Speciﬁcally, we use the state-of-the-art
reinforcement learning algorithm PPO [30] to learn a policy
πθ :

that maximizes the objective in Eq. 18.
Due to computational limitations (i.e. ﬁnite memory), we
summarize the history ht = (a1, o1, a2, o2, . . . , at
1, ot) by a
vector ˆht = (xt, yt, zt, t), where xt, yt, zt are the accumulated
counters of the observations oi = (∆xi, ∆yi, ∆zi) for i =
1, . . . , t: xt = (cid:80)t

i=1 ∆xi, yt = (cid:80)t

i=1 ∆yi, zt = (cid:80)t

i=1 ∆zi.

H (cid:55)→ A

PPO implements the policy gradient method and uses

−

stochastic gradient ascent with the following gradient [30]:

(cid:34)

(cid:35)

(19)

ˆh)
(cid:125)

Aπθ (ˆh, a)
(cid:124)
(cid:123)(cid:122)
(cid:125)
critic

∇θJ(θ) = Eπθ

∇θ log πθ(a
|
(cid:123)(cid:122)
(cid:124)
actor
V πθ (ˆht) is the so-called
where Aπθ (ˆht, at) = Qπθ (ˆht, at)
advantage function [37]. We implement πθ with a deep neural
network that takes as input the summarized history ˆht and
produces as output a discrete conditional probability distribu-
ˆht) that is computed with the softmax function.
tion πθ(at|
The neural network structure of πθ follows an actor-critic
architecture and computes a second output (the critic) that
θ (ˆht), which in turn allows to
estimates the value function V πθ

−

Fig. 3: Left: the reward function for the stop and continue
actions; the intrusion starts at t = 29; right: the cumulative
distribution function (CDF) of the intrusion start time.

We
P [st+1|

the

deﬁne
st, at] as follows:

transition

probabilities

at
stst+1

P

=

] = 1

·

,
∅|∅
p

−

P [
∅|·
P [0
P [1
P [1

, S] = P [
0, C] = 1
|
0, C] = p
|
1, C] = 1
|

(9)

(10)

(11)

(12)

All other transitions have probability 0.

Eq. 9 deﬁnes the transition probabilities to the terminal state
. The terminal state is reached when taking the stop action S.
∅
Eq. 10-12 deﬁne the transition probabilities when taking the
continue action C. Eq. 10 captures the case where no intrusion
occurs and where it+1 = it = 0; Eq. 11 captures the start of
an intrusion where it = 0, it+1 = 1; and Eq. 12 describes the
case where an intrusion is in progress and it+1 = it = 1.

Z

4) Observation Function

(o(cid:48), s(cid:48), a): We assume that the
number of IDS alerts and login attempts generated during a
single time-step are random variables X
∼
fZ, dependent on the intrusion state and deﬁned on the sample
spaces ΩX =
, and
. Consequently, the probability that
ΩZ =
∆x severe alerts, ∆y warning alerts, and ∆z login attempts
are generated during time-step t is fXY Z(∆x, ∆y, ∆z
|
(o(cid:48), s(cid:48), a) = P[o(cid:48)

∼
0, 1, . . . , Ymax}
{

We deﬁne the observation function

0, 1, . . . , Xmax}
{

0, 1, . . . , Zmax}
{

, ΩY =

fY , Z

fX , Y

∼

it).
s(cid:48), a]
|

Z

as follows:

(cid:0)(∆x, ∆y, ∆z), it, C(cid:1) = fXY Z(∆x, ∆y, ∆z
(cid:0)

(cid:1) = 1

,
∅

,
∅

·

Z

Z

it)
|

(13)

(14)

5) Reward Function

a
s : The reward function is parameter-
R
ized by the reward that the defender receives for stopping an
intrusion (Rst = 100), the loss of stopping before an intrusion
100), the reward for maintaining service
has started (Res =
100),
(Rsla = 10), and the loss of being intruded (Rint =
respectively.

−

−

We deﬁne the deterministic reward function

to be (Fig. 3):

at
st = r(st, at)

R

·

) = 0

,
r (
∅
r (it, S) = 1it=0Res + 1it=1Rst
r (it, C) = Rsla + 1it=1Rint

(15)

(16)

(17)

5

120406080100time-stept−1000100RewardfunctionRatstStoprewardContinuerewardIntrusionstarts1020intrusionstarttimeit0.000.250.500.751.00CDFIt(it)It∼Ge(p=0.2)estimate Aπθ (ˆht, at) in Eq. 19 using the generalized advantage
estimator ˆAπθ

GAE [37].

The hyperparameters of our implementation are given in
Appendix B and were decided based on smaller search in
parameter space.

The defender policy is learned through simulation of the
POMDP. First, we simulate a given number of episodes. We
then use the episode outcomes and trajectories to estimate
the expectation of the gradient in Eq. 19. Then, we use the
estimated gradient and the PPO algorithm [30] with the Adam
optimizer [38] to update the policy. This process of simulating
episodes and updating the policy continues until the policy has
sufﬁciently converged.

C. Threshold Property of the Optimal Policy

The policy that solves the optimal stopping problem is
deﬁned by the optimization objective in Eq. 18. From the
theory of dynamic programming, we know that this policy
satisﬁes the Bellman equation [22], [12], [13], [25]:

(cid:0)b(1)(cid:1) = arg max

π∗

(cid:34)

r(cid:0)b(1), a(cid:1) +

a

∈A

(cid:88)

o

∈O

P[o

b(1), a]V ∗
|

(cid:35)

(cid:0)ba

o(1)(cid:1)

(20)

−

is the belief that

the sys-
where b(1) = P[st = 1
ht]
|
tem is in state 1 based on the observed history ht =
b(1) (see
1, ot). Consequently, b(0) = 1
(a1, o1, . . . , at
Section III-A for an overview of belief states). Moreover, ba
o(1)
is the belief state updated with the Bayes ﬁlter in Eq. 3 after
taking action a and observing o. Further, r(cid:0)b(1), a(cid:1) is the
expected reward of taking action a in belief state b(1), and
V ∗ is the value function.

−

We use Eq. 20 to derive properties of the optimal policy.

Speciﬁcally, we establish the following structural result.

Theorem 1. There exists an optimal policy π∗ which is a
threshold policy of the form:

(cid:0)b(1)(cid:1) =

π∗

(cid:40)

S (stop)
C (continue)

α∗

if b(1)
≥
otherwise

(21)

where α∗ is a threshold.

Proof. See Appendix A.

Theorem 1 states that there exists an optimal policy which
stops whenever the posterior probability that an intrusion
has started based on the history of IDS alerts and login
attempts exceeds a threshold level α∗. This implies that the
optimal policy is completely determined by α∗ given that
b(1) is known. Since b(1) is computed from the history of
observations, it also implies that the optimal defender policy
can expressed as a threshold function based on the observed
infrastructure metrics.

V. EMULATING THE TARGET INFRASTRUCTURE TO
INSTANTIATE THE SIMULATION

Client

Functions

Application servers

1
2
3

HTTP, SSH, SNMP, ICMP N2, N3, N10, N12
IRC, PostgreSQL, SNMP
FTP, DNS, Telnet

N31, N13, N14, N15, N16
N10, N22, N4

TABLE 1: Emulated client population; each client interacts
with application servers using a set of functions.

Time-steps t

Actions

Ge(0.2)

1–It
∼
It + 1–It + 7

It + 8–It + 11

It + 12–X + 16

It + 17–It + 22

(Intrusion has not started)
RECON, brute-force attacks (SSH,Telnet,FTP)
on N2, N4, N10, login(N2, N4, N10),
backdoor(N2, N4, N10), RECON
CVE-2014-6271 on N17, SSH brute-force attack on N12,
login (N17, N12), backdoor(N17, N12)
CVE-2010-0426 exploit on N12, RECON
SQL-Injection on N18, login(N18), backdoor(N18)
RECON, CVE-2015-1427 on N25, login(N25)
RECON, CVE-2017-7494 exploit on N27, login(N27)

TABLE 2: Attacker actions to emulate an intrusion.

distributions using measurements from an emulation system.
This procedure is detailed in this section.

A. Emulating the Target Infrastructure

The emulation system executes on a cluster of machines that
runs a virtualization layer provided by Docker [39] containers
and virtual connections. The emulation is conﬁgured following
the topology in Fig. 1 and the conﬁguration in Appendix C.
It emulates the clients, the attacker, and the defender, as well
as 31 physical components of the target infrastructure (e.g
application servers and the gateway). Each physical entity is
emulated using a Docker container. The containers replicate
important functions of the target infrastructure, including web
servers, databases, SSH servers, etc.

The emulation evolves in discrete time-steps of 30 seconds.
the attacker and the defender can

During each time-step,
perform one action each.

1) Emulating the Client Population: The client population
is emulated by three client processes that interact with the ap-
plication servers through different functions at short intervals,
see Table 1.

2) Emulating the Attacker: The start time of an intrusion
is controlled by a Bernoulli process as explained in Section
IV. We have implemented two types of attackers, NOISYAT-
TACKER and STEALTHYATTACKER, both of which execute
the sequence of actions listed in Table 2. The actions consist
of reconnaissance commands and exploits. During each time-
step, one action is executed.

The two types of attackers differ in the reconnaissance
command. NOISYATTACKER uses a TCP/UDP scan for recon-
naissance while STEALTHYATTACKER uses a ping-scan. Since
the ping-scan generates fewer IDS alerts than the TCP/UDP
scan, it makes the actions of STEALTHYATTACKER harder to
detect.

To simulate episodes of the POMDP we must know the
distributions of alerts and login attempts. We estimate these

3) Emulating Actions of the Defender: The defender takes
an action every time-step. The continue action has no effect

6

Metric

Command in the Emulation

Login attempts
IDS Alerts

cat /var/log/auth.log
cat /var/snort/alert.csv

TABLE 3: Commands used to measure the emulation.

Fig. 4: Empirical distributions of IDS alerts (top row) and
login attempts on two servers (bottom row); the graphs include
several distributions that are superimposed.

on the emulation. The stop action changes the ﬁrewall conﬁg-
uration of the gateway and drops all incoming trafﬁc.

B. Estimating the Distributions of Alerts and Login Attempts

In this section, we describe how we collect data from the
emulation and how we use the data to estimate the distributions
of alerts and login attempts.

1) Measuring the Number of IDS alerts and Login Attempts
in the Emulation: At the end of every time-step, the emulation
system collects the metrics ∆x, ∆y, ∆z, which contain
the alerts and login attempts that occurred during the time-
step. The metrics are collected by parsing the output of the
commands in Table 3. For the evaluation reported in this paper,
we collected measurements from 11000 time-steps.

the Target

2) Estimating the Distributions of Alerts and Login At-
tempts of
Infrastructure: Using the collected
measurements, we compute the empirical distribution ˆfXY Z,
which is our estimate of the corresponding distribution fXY Z
in the target infrastructure. For each (It, t) pair, we obtain one
empirical distribution.

Fig. 4 shows some of these distributions, which are super-
imposed. Although the distribution patterns generated during
an intrusion and during normal operation overlap, there is a
clear difference.

C. Simulating Episodes of the POMDP

During a simulation of the POMDP, the system state evolves
according the dynamics described in Section IV and the obser-
vations evolve according to the estimated distribution ˆfXY Z.
In the initial state, no intrusion occurs. In every episode,

either the defender stops before the intrusion starts or exactly
one intrusion occurs, the start of which is determined by a
Bernoulli process (see Section IV).

A simulated episode evolves as follows. During each time-
step, if an intrusion is ongoing, the attacker executes an action
in the predeﬁned sequence listed in Table 2. Subsequently, the
defender samples an action from the defender policy πθ. If
the action is stop, the episode ends. Otherwise, the simulation
samples the number of alerts and login attempts occurring
during this time-step from the empirical distribution ˆfXY Z. It
then computes the reward of the defender using the reward
function deﬁned in Section IV. The activities of the clients
are not explicitly simulated but are implicitly represented in
ˆfXY Z. The sequence of time-steps continues until the defender
stops or an intrusion completes, after which the episode ends.

VI. LEARNING INTRUSION PREVENTION POLICIES USING
SIMULATION

To evaluate our reinforcement learning approach for ﬁnding
defender policies, we simulate episodes of the POMDP where
the defender policy is updated and evaluated. We evaluate
the approach with respect
to the convergence of policies
and compare the learned policies to two baselines and to an
ideal policy which presumes knowledge of the exact time of
intrusion.

The evaluation is conducted using a Tesla P100 GPU and
the hyperparameters for the learning algorithm are listed in
Appendix B. Our implementation as well as the measurements
for the results reported in this paper are publicly available [40].

A. Evaluation Setup

We train two defender policies against NOISYATTACKER
and STEALTHYATTACKER until convergence, which occurs
after some 400 iterations. In each iteration, we simulate 4000
time-steps and perform 10 updates to the policy. After each
iteration, we evaluate the defender policy by simulating 200
evaluation episodes and compute various performance metrics.
We compare the learned policies with two baselines. The
ﬁrst baseline is a policy that always stops at t = 6, which
corresponds to the immediate time-step after the expected time
of intrusion E[It] = 5 (see Section IV). The policy of the
second baseline always stops after the ﬁrst IDS alert occurs,
i.e. (x + y)

1.

To evaluate the stability of the learning curves’ convergence,
we run each training process three times with different random
seeds. One training run requires approximately six hours of
processing time on a P100 GPU.

≥

B. Analyzing the Results

The red curves in Fig. 5 show the performance of the learned
policy against NOISYATTACKER, and the blue curves show the
policy performance against STEALTHYATTACKER. The purple
and the orange curves give the performance of the two baseline
policies. The dashed black curves give an upper bound on
the performance of the optimal policy π∗, which is computed
assuming that the defender knows the exact time of intrusion.

7

0501001502000.00.51.0ˆfX(∆x|it,It,t)#SevereIDSAlerts∆x050100150200ˆfY(∆y|it,It,t)#WarningIDSAlerts∆y05100.00.51.0ˆfZ(∆z|it,It,t)#FailedLogins∆z172.18.9.20.00.51.01.52.0ˆfZ(∆z|it,It,t)#FailedLogins∆z172.18.9.3normaloperationintrusioninprogressFig. 5: Learning curves; the graphs show from left to right: episodic reward, length of an episode, empirical detection probability,
empirical early stopping probability, and the number of steps between the start of an intrusion and the stop action; the curves
show the averages and the standard deviations of three training runs with different random seeds.

The ﬁve graphs in Fig. 5 show that the learned policies con-
verge, and that they are close to optimal in terms of achieving
maximum reward, detecting intrusion, avoid stopping when
there is no intrusion, and stopping right after an intrusion
starts. Further, the learned policies outperform both baselines
by a large margin (leftmost graph in Fig. 5).

The performance of the learned policy against NOISYAT-
TACKER is better than that against STEALTHYATTACKER (left-
most graph in Fig. 5). This indicates that NOISYATTACKER
is easier to detect for the defender. For instance, the learned
policy against STEALTHYATTACKER has a higher probability
of stopping early (second rightmost graph of Fig. 5). This
can also be seen in the second leftmost graph of Fig. 5,
the learned policy against
which shows that, on average,
STEALTHYATTACKER stops after 5 time-steps and the learned
policy against NOISYATTACKER stops after 6 time-steps.

Looking at the baseline policies, we can see that the baseline
t = 6 stops too early in 50 percent of the episodes, and the
1 baseline stops too early in 80 percent of the
(x + y)
episodes (second rightmost graph in Fig. 5).

≥

VII. THRESHOLD PROPERTIES OF THE LEARNED POLICIES
AND COMPARISON WITH THE OPTIMAL POLICY

When analyzing the learned policies, we ﬁnd that they can
be expressed through thresholds, just like the optimal policy
(Section IV-C). However, in contrast to the optimal policy,
the learned thresholds are based on the observed counters of
alerts and login attempts rather than the belief state (which is
unknown to the defender). Speciﬁcally, the top left graph in
Fig. 6 shows that the learned policies against both attackers
implement a soft threshold by stopping with high probability
if the number of alerts (xt + yt) exceeds 130. This indicates
that if the total number of alerts is above 130, an intrusion has
started, i.e. xt + yt is used to approximate the posterior bt(1).
the graphs in Fig. 6 also show the relative
importance of severe alerts xt, warning alerts yt, and login
attempts zt for policy decisions. Speciﬁcally, it can be seen
that xt has the highest importance, yt has a lower importance,
and zt has the least importance for policy decisions.

Moreover,

We also see that

the learned policy against NOISYAT-
TACKER is associated with a higher alert threshold than that

soft
thresholds

never
stop

warning alerts y

alerts

s e v ere

warning alerts y

alerts

s e v ere

x

x

Fig. 6: Probability of the stop action by the learned policies
in function of the number of alerts x, y and login attempts z.

of STEALTHYATTACKER (top left graph in Fig. 6). This is
consistent with our comment in Section V-A2 that STEALTHY-
ATTACKER is harder to detect.

Lastly, Fig. 7 suggest that the thresholds of the learned
policies are indeed close to the threshold of the optimal policy.
For instance,
the policy learned against NOISYATTACKER
stops immediately after the optimal stopping time.

VIII. RELATED WORK

The problem of automatically ﬁnding security policies has
been studied using concepts and methods from different ﬁelds,
most notably reinforcement learning, game theory, dynamic
programming, control theory, attack graphs, statistical tests,
and evolutionary computation. For a literature review of deep
reinforcement learning in network security see [15], and for an
overview of game-theoretic approaches see [4]. For examples
of research using dynamic programming, control theory, attack

8

01000200030004000#policyupdates−1000100200Rewardperepisode01000200030004000#policyupdates2468Episodelength(steps)01000200030004000#policyupdates0.20.40.60.81.0P[intrusioninterrupted]01000200030004000#policyupdates0.00.20.40.60.81.0P[earlystopping]01000200030004000#policyupdates1234UninterruptedintrusiontLearnedπθvsNoisyAttackerLearnedπθvsStealthyAttackert=6baseline(x+y)≥1baselineUpperboundπ∗0100200300400#totalalertsx+y0.00.51.0πθ(stop|x+y)πθvsStealthyAttackerπθvsNoisyAttacker0255075100#loginattemptsz0.000.050.100.15πθ(stop|z)πθvsStealthyAttackerπθvsNoisyAttacker0501001502000501001502000.00.20.40.60.81.0πθ(stop|x,y)vsStealthyAttacker0501001502000501001502000.00.20.40.60.8πθ(stop|x,y)vsNoisyAttackeroptimal stopping

close to optimal
stopping

early
stopping

Fig. 7: Comparison of the learned policies πθ and an upper
bound on the optimal policy π∗ for 4 sample episodes where
the intrusions start at it = 2, 4, 6, 8.

graphs, statistical tests, and evolutionary methods, see [8], [7],
[9], [6], and [3].

Most research on reinforcement learning applied to network
security is recent. Prior work that most resembles the approach
taken in this paper includes our previous research [10] and
the work in [11], [21], [20], [19], [41], and [42]. All of
these papers focus on network intrusions using reinforcement
learning.

This paper differs from prior work in the following ways:
(1) we formulate intrusion prevention as an optimal stopping
problem ([19] uses a similar approach); (2) we use an emulated
infrastructure to estimate the parameters of our simulation
model, rather than relying on abstract assumptions like [10],
[11], [20], [19], [41], [42]; (3) we derive a structural property
of the optimal policy; (4) we analyze the learned policies and
relate them to the optimal policy, an analysis which prior work
lacks [10], [11], [20], [41], [42]; and (5) we apply state-of-
the-art reinforcement learning algorithms, i.e. PPO [30], rather
than traditional ones as used in [11], [20], [19], [21], [41], [42].
Optimal stopping has previously been applied to model
problems in in different domains,
including ﬁnance [17],
queuing systems [18], and attack detection [6]. We believe
we are ﬁrst in using optimal stopping to model intrusion
prevention.

In this paper, we proposed a novel formulation of the
intrusion prevention problem as one of optimal stopping. This
allowed us to state that the optimal defender policy can be
expressed using a threshold obtained from infrastructure mea-
surements. Further, we used reinforcement learning to estimate
the optimal defender policies in a simulation environment.
In addition to validating the predictions from the theory, we

learned from the simulations a) the relative importance of
measurement metrics with respect to the threshold level and b)
that different attacker proﬁles can lead to different thresholds
of the defender policies.

We plan to extend this work in three directions. First,
the model of the defender in this paper is simplistic as it
allows only for a single stop action. We plan to increase
the set of actions that the defender can take to better reﬂect
today’s defense capabilities, while still keeping the structure
of the stopping formulation. Second, we plan to extend the
observation capabilities of the defender to obtain more realistic
policies. Third, in the current paper, the attacker policy is
static. We plan to extend the model to include a dynamic
attacker that can learn just like the defender. This requires
a game-theoretic formulation of the problem.

X. ACKNOWLEDGMENTS

This research has been supported in part by the Swedish
armed forces and was conducted at KTH Center for Cyber
Defense and Information Security (CDIS). The authors would
like to thank Pontus Johnson for useful input to this research,
and Forough Shahab Samani and Xiaoxuan Wang for their
constructive comments to an earlier draft of this paper.

APPENDIX

A. Proof of Theorem 1

We will work our way to the proof of Theorem 1 by

establishing some initial results.

Lemma 1. It is optimal to stop in belief state b(1) iff:

b(1)

≥

110 +

(cid:88)

o (1)(cid:1)(cid:16)
(cid:0)bC
p

V ∗

(o, 1, C) + (1

p)

Z

−

(o, 0, C)

Z

(22)

(cid:17)

o

∈O
o (1)(cid:1)(cid:16)
(cid:0)bC

300 +

V ∗

(cid:88)

o

∈O

(o, 1, C) + (1

p

Z

p)

Z

−

(o, 0, C)

− Z

(cid:17)

(o, 1, C)

Proof. Considering both actions of
S, C

(
A
), we derive from the Bellman equation (Eq. 20):
}
(cid:0)b(1)(cid:1)

the defender

{

π∗

=

(23)

= arg max
a

∈A

= arg max

(cid:34)

(cid:34)

(cid:34)

r(cid:0)b(1), a(cid:1) +

(cid:88)

o

∈O

P[o

|

b(1), a]V ∗

(cid:0)ba

o (1)(cid:1)

(cid:35)

r(cid:0)b(1), S(cid:1)
(cid:124)
(cid:123)(cid:122)
(cid:125)
a=S

, r(cid:0)b(1), C(cid:1) +

(cid:124)

(cid:88)

o

∈O

(cid:123)(cid:122)
a=C

b(1)200
(cid:123)(cid:122)
(cid:124)
ω

, 10
100
(cid:125)

−

−

b(1)100 +

(cid:124)

(cid:88)

o

∈O

P[o

|

(cid:123)(cid:122)
(cid:15)

b(1), C]V ∗

(cid:0)bC

o (1)(cid:1)

P[o

|

(cid:35)

(cid:125)

b(1), C]V ∗

(cid:0)bC

o (1)(cid:1)

(cid:35)

(cid:125)

In the above equation, ω is the expected reward for stopping
and (cid:15) is the expected cumulative reward for continuing. If
(cid:15) = ω, both actions of the defender, continuing and stopping,
maximize the expected cumulative reward. If ω
(cid:15), it is
optimal for the defender to stop.

≥

9

IX. CONCLUSION AND FUTURE WORK

= arg max

0.000.250.500.751.00P[stop|ot]it=2it=415913time-stept0.000.250.500.751.00P[stop|ot]it=615913time-steptit=8LearnedπθvsNoisyAttackerLearnedπθvsStealthyAttackerUpperboundπ∗Next,

(cid:80)
s
∈S
(cid:0)b(1)(cid:1)

π∗

(cid:80)
s

(cid:48)∈S

(cid:34)

= arg max

b(1)200

(cid:34)

= arg max

b(1)200

(cid:18)

b(1)

Z

(o, 1, C) + (1

(cid:34)

= arg max

b(1)200

−

−

−

−

we
b(s)

P

use
(o, s(cid:48), a) and

b(1), a]
P[o
|
0, 1
=
{
S

}

a
ss(cid:48)Z

=

to obtain:

(24)

(cid:35)
o (1)(cid:1)

(cid:0)bC

b(1), C]V ∗

P[o

|

V ∗

(cid:0)bC

o (1)(cid:1)

b(1)100 +

b(1)100 +

(cid:88)

o

∈O
(cid:88)

o

∈O

100, 10

100, 10

−

−

b(1)(cid:1)(cid:16)

p

Z

(o, 1, C) + (1

p)

Z

−

(o, 0, C)

(cid:17)(cid:19)(cid:35)

(cid:18)

100, 10 + b(1)

100 +

−

V ∗

o (1)(cid:1)(cid:16)
(cid:0)bC
Z

(cid:88)

o

∈O

(o, 1, C)

(o, 1, C) + (1

p)

Z

−

(o, 0, C)(cid:1)(cid:17)(cid:19)

(cid:18)
o (1)(cid:1)

(cid:0)bC

p

V ∗

(o, 1, C) + (1

p)

Z

−

(o, 0, C)

Z

(cid:19)(cid:35)

−

+

(cid:0)p

Z

(cid:88)

o

∈O

This implies that it is optimal to stop in belief state b(1) iff:

b(1)200

100

−

≥

10 + b(1)

100 +

−

(cid:18)

(cid:88)

o (1)(cid:1)(cid:16)
(cid:0)bC

V ∗

o

∈O

(o, 0, C)(cid:1)(cid:17)(cid:19)

(o, 1, C) + (1

p)

Z

−

(cid:18)
o (1)(cid:1)

(cid:0)bC

p

V ∗

(o, 1, C) + (1

p)

Z

−

(o, 0, C)

Z

(cid:19)

−

+

(cid:0)p

Z
(cid:88)

o

∈O

By rearranging terms, we get:

(o, 1, C)

Z

(25)

(26)

b(1)

≥

110 +

(cid:88)

o (1)(cid:1)(cid:16)
(cid:0)bC
p

V ∗

o

V ∗

∈O
o (1)(cid:1)(cid:16)
(cid:0)bC

p

(cid:88)

o

∈O

300 +

(cid:124)

(o, 1, C) + (1

p)

Z

−

Z

(cid:17)

(o, 0, C)

(o, 1, C) + (1

p)

Z

−

(o, 0, C)

− Z

Z

(cid:123)(cid:122)
αb(1)

(cid:17)

(o, 1, C)

(cid:125)

Lemma 1 shows that the optimal policy is determined by
the scalar thresholds αb(1). Speciﬁcally, it is optimal to stop in
αb(1). We conclude that the stopping
belief state b(1) if b(1)
set S —the set of belief states b(1)
[0, 1] where it is optimal
to stop—is:

≥

∈

S = (cid:8)b(1)

[0, 1] : b(1)

(cid:9)

αb(1)

(27)

∈

≥

Similarly, the continuation set C —the set of belief states
where it is optimal to continue—is C = [0, 1]

S .

Building on the above analysis, the main idea behind the
the stopping set S
proof of Theorem 1 is to show that
has the form S = [α∗, 1], where 0
1 is the
stopping threshold. Towards this goal, we state the following
two lemmas.

α∗

≤

≤

\

Lemma 2. The following lemma is due to Sondik [43].

The optimal value function:

V ∗(bt) = max
at∈A
is piecewise linear and convex with respect to b

bt, at]
E [rt+1 + V ∗(bt+1)
|

.

∈ B

Proof. See [43] or [13, pp. 155, Theorem 7.4.1].

Lemma 3. The stopping set S is a convex subset of the belief
space

.

B

Proof. A general proof is given in [13]. We restate it here to
show that it holds in our case.

To show that the stopping set S is convex, we need to
S , any linear

show that for any two belief states b1, b2 ∈
combination of b1, b2 is also in S . That is, b1, b2 ∈
S for any λ
λb1 + (1

S =

[0, 1].

⇒

Since V ∗(b) is convex (Lemma 2), we have by deﬁnition

∈

λ)b2 ∈
of convex sets that:

−

λV ∗(b1) + (1

(29)
λ)b2)
V ∗(λb1 + (1
−
S by assumption, the optimal action in
Further, as b1, b2 ∈
b1 and b2 is the stop action S. Thus, we have that V ∗(b1) =
100b1(0) and V ∗(b2) = Q∗(b2, S) =
Q∗(b1, S) = 100b1(1)
100b2(1)

100b2(0). Hence:

λ)V (b2)

−

−

≤

V ∗

−

−
(cid:0)λb1(1) + (1
λV ∗

(cid:0)b1(1)) + (1
≤
= λQ∗(b1, S) + (1
= λ(cid:0)100b1(1)
−
(cid:0)λb1 + (1
= Q∗
−
(cid:0)λb1(1) + (1

V ∗

≤

λ)b2(1)(cid:1)

λ)V ∗(b2(1)(cid:1)
λ)Q∗(b2, S)

−

−

b1(0)100(cid:1) + (1
λ)b2, S(cid:1)

λ)b2(1)(cid:1)

−

λ)(cid:0)100b2(1)

b2(0)100(cid:1)

−

−

where the last inequality is because V ∗ is optimal. Thus we
λ)b2(1)(cid:1).
λ)b2, S(cid:1) = V ∗
have that Q∗
S for
This means that if b1, b2 ∈
any λ

−
[0, 1]. Hence, S is convex.

S , then λb1 + (1

(cid:0)λb1(1)+(1

−
λ)b2 ∈

(cid:0)λb1+(1

−

∈

Now we use Lemma 3 to prove Theorem 1.

The belief space

Proof of Theorem 1. The proof of Theorem 1 follows the
same argument as the proof in [13, Corollary 12.2.2, pp. 258].
[0, 1]. In
consequence, using Lemma 3, we have that the stopping set S
is a convex subset of [0, 1]. That is, S has the form [α∗, β∗]
where 0
1. Thus, to show that the optimal
β∗
≤
policy is of the form:

= [0, 1] is deﬁned by b(1)

α∗

≤

≤

∈

B

(cid:0)b(1)(cid:1) =

π∗

(cid:40)

S

C

α∗

if b(1)
≥
otherwise

(36)

it sufﬁces to show that β∗ = 1, i.e. S = [α∗, β∗] = [α∗, 1].

If b(1) = 1, then the Bellman equation states that:

(cid:34)

π∗(1) = arg max

r(1, a) +

(cid:88)

o

∈O

1, a]V ∗

P[o

|

(cid:35)
o (1)(cid:1)

(cid:0)ba

a

∈A

(cid:34)

= arg max

,

100
(cid:124)(cid:123)(cid:122)(cid:125)
a=S

90 +

−

(cid:124)

(cid:88)

o

∈O

Z

(o, 1, C)V ∗

(cid:0)bC

o (1)(cid:1)

(cid:123)(cid:122)
a=C

(cid:125)

(37)

(38)

(cid:35)

Since s = 1 is an absorbing state until the terminal state is
reached, we have that bC
. This follows
from the deﬁnition of bC

o (1) = 1 for all o
o (Eq. 3). Consequently, we get:
(cid:35)
o (1)(cid:1)

Z(o, 1, C)V ∗

∈ O

(cid:0)bC

(cid:88)

100, −90 +

(30)

(31)

(32)

(33)

(34)

(35)

(39)

(40)

(28)

π∗(1) = arg max

(cid:34)

(cid:34)

o

∈O

(cid:35)

= arg max

100, −90 + V ∗(1)

10

α∗

C

S

Fig. 8: Left: optimal stopping thresholds b(1)
αb(1) ≥
convex optimal value function V ∗(b(1)).

−
0 it is optimal to stop; right: the piecewise linear and

αb(1), if b(1)

−

Parameters

Values

γ, lr α, batch, # layers, # neurons, clip (cid:15)
Xmax, Ymax, Zmax, GAE λ, ent-coef, activation

10−

4, 4
1, 5
103, 103, 103, 0.95, 5

·

·

103, 3, 64, 0.2

ID (s)

OS:Services:Exploitable Vulnerabilities

1
2
4
10
12
17
18
22
23
24
25
27
3,11,5-9
13-16,19-21,26,28-31

Ubuntu20:Snort(community ruleset v2.9.17.1),SSH:-
Ubuntu20:SSH,HTTP Erl-Pengine,DNS:SSH-pw
Ubuntu20:HTTP Flask,Telnet,SSH:Telnet-pw
Ubuntu20:FTP,MongoDB,SMTP,Tomcat,Teamspeak3,SSH:FTP-pw
Jessie:Teamspeak3,Tomcat,SSH:CVE-2010-0426,SSH-pw
Wheezy:Apache2,SNMP,SSH:CVE-2014-6271
Deb9.2:IRC,Apache2,SSH:SQL Injection
Jessie:PROFTPD,SSH,Apache2,SNMP:CVE-2015-3306
Jessie:Apache2,SMTP,SSH:CVE-2016-10033
Jessie:SSH:CVE-2015-5602,SSH-pw
Jessie: Elasticsearch,Apache2,SSH,SNMP:CVE-2015-1427
Jessie:Samba,NTP,SSH:CVE-2017-7494
Ubuntu20:SSH,SNMP,PostgreSQL,NTP:-
Ubuntu20:NTP, IRC, SNMP, SSH, PostgreSQL:-

TABLE 5: Conﬁguration of the target infrastructure (Fig. 1).

B. Hyperparameters: Table 4

C. Conﬁguration of the Infrastructure in Fig. 1: Table 5

10−

4, ReLU

·

REFERENCES

TABLE 4: Hyperparameters of the learning algorithm.

Finally, since V ∗(1)

≤

100, we conclude that:

(cid:34)

(cid:35)

π∗(1) = arg max

100,

−

90 + V ∗(1)

= S

(41)

This means that π∗(1) = S, hence b(1) = 1 is in the stopping
S , and since S is a convex subset
S . As 1
set, i.e. 1
∈
∈
[0, 1], we have that β∗ = 1. Then it follows that
[α∗, β∗]
S = [α∗, β∗] = [α∗, 1].

⊆

An Example to Illustrate Theorem 1: To illustrate the

implications of Theorem 1, consider the following example.
The observation ot is the number of IDS alerts that were
generated during time-step t, which is an integer scalar in the
. Further, assume that the
observation space
0, . . . , 5
}
{
O
observation function
(o(cid:48), s(cid:48), a) is deﬁned using the discrete
Z
uniform distribution
(
U

) as follows.

a, b
}
{

=

Z

Z

U

(o(cid:48), 0, C) =
(o(cid:48), 1, C) =
,
∅

U
) = 1
·

,
∅

Z

(

(

(

0, 4
{
0, 5
{

)
}
)
}

no intrusion

intrusion

(42)

(43)
(44)

The rest of the POMDP follows the deﬁnitions in Section IV.
Due to the small observation space, the optimal policy can
be computed using dynamic programming and value iteration.
In particular, we apply Sondik’s value iteration algorithm [43]
(cid:0)b(1)(cid:1) as well as
to compute the optimal value function V ∗
the optimal thresholds αb(1) (Fig. 8).
As can be seen in Fig. 8, b(1)

αb(1) is increasing in b(1)
and there exists a unique minimum belief point b(1)
0.357
such that b(1)
αb(1), which we denote by α∗. Hence
the stopping set is the convex set S = [0.357, 1], and the
continuation set C is the set C = [0, 0.357).

≈

≥

−

[1] A. Fuchsberger, “Intrusion detection systems and intrusion prevention
systems,” Inf. Secur. Tech. Rep., vol. 10, no. 3, p. 134–139, Jan. 2005.
[2] P. Johnson, R. Lagerstr¨om, and M. Ekstedt, “A meta language for
threat modeling and attack simulations,” in Proceedings of the 13th
International Conference on Availability, Reliability and Security, ser.
ARES 2018, New York, NY, USA, 2018.

[3] R. Bronfman-Nadas, N. Zincir-Heywood, and J. T. Jacobs, “An artiﬁcial
improve mobile malware detectors?” in 2018

arms race: Could it
Network Trafﬁc Measurement and Analysis Conference (TMA), 2018.

[4] T. Alpcan and T. Basar, Network Security: A Decision and Game-
Theoretic Approach, 1st ed. USA: Cambridge University Press, 2010.
[5] S. Sarıtas¸, E. Shereen, H. Sandberg, and G. D´an, “Adversarial attacks
on continuous authentication security: A dynamic game approach,” in
Decision and Game Theory for Security, Cham, 2019, pp. 439–458.
[6] A. G. Tartakovsky, B. L. Rozovskii, R. B. Blaˇzek, and H. Kim,
“Detection of intrusions in information systems by sequential change-
point methods,” Statistical Methodology, vol. 3, no. 3, 2006.

[7] W. Liu and S. Zhong, “Web malware spread modelling and optimal
control strategies,” Scientiﬁc Reports, vol. 7, p. 42308, 02 2017.
[8] M. Rasouli, E. Miehling, and D. Teneketzis, “A supervisory control
approach to dynamic cyber-security,” in Decision and Game Theory for
Security. Cham: Springer International Publishing, 2014, pp. 99–117.
[9] E. Miehling, M. Rasouli, and D. Teneketzis, “A pomdp approach to the
dynamic defense of large-scale cyber networks,” IEEE Transactions on
Information Forensics and Security, vol. 13, no. 10, 2018.

[10] K. Hammar and R. Stadler, “Finding effective security strategies through
reinforcement learning and Self-Play,” in International Conference on
Network and Service Management (CNSM 2020), Izmir, Turkey, 2020.
[11] R. Elderman, L. J. J. Pater, A. S. Thie, M. M. Drugan, and M. Wiering,
“Adversarial reinforcement learning in a cyber security simulation,” in
ICAART, 2017.

[12] D. P. Bertsekas, Dynamic Programming and Optimal Control, 3rd ed.

Belmont, MA, USA: Athena Scientiﬁc, 2005, vol. I.

[13] V. Krishnamurthy, Partially Observed Markov Decision Processes: From

Filtering to Controlled Sensing. Cambridge University Press, 2016.

[14] D. P. Bertsekas and J. N. Tsitsiklis, Neuro-dynamic programming.

Belmont, MA: Athena Scientiﬁc, 1996.

[15] T. T. Nguyen and V. J. Reddi, “Deep reinforcement learning for cyber

security,” CoRR, vol. abs/1906.05799, 2019.

[16] R. S. Sutton and A. G. Barto, Introduction to Reinforcement Learning,

1st ed. Cambridge, MA, USA: MIT Press, 1998.

[17] J. du Toit and G. Peskir, “Selling a stock at the ultimate maximum,”

The Annals of Applied Probability, vol. 19, no. 3, Jun 2009.

[18] A. Roy, V. S. Borkar, A. Karandikar, and P. Chaporkar, “Online
reinforcement learning of optimal threshold policies for markov decision
processes,” CoRR, vol. abs/1912.10325, 2019.

[19] M. N. Kurt, O. Ogundijo, C. Li, and X. Wang, “Online cyber-attack
learning approach,” IEEE

detection in smart grid: A reinforcement
Transactions on Smart Grid, vol. 10, no. 5, pp. 5174–5185, 2019.
[20] F. M. Zennaro and L. Erdodi, “Modeling penetration testing with
reinforcement learning using capture-the-ﬂag challenges and tabular q-
learning,” CoRR, vol. abs/2005.12632, 2020.

11

0.00.250.50.751.0b(1)−0.4−0.20.00.2b(1)−αb(1)0.000.250.500.751.00b(1)050100V∗(b(1))[21] J. Schwartz, H. Kurniawati, and E. El-Mahassni, “Pomdp + information-
decay: Incorporating defender’s behaviour in autonomous penetration
testing,” Proceedings of the International Conference on Automated
Planning and Scheduling, vol. 30, no. 1, pp. 235–243, Jun. 2020.
[22] R. Bellman, “A markovian decision process,” Journal of Mathematics

and Mechanics, vol. 6, no. 5, pp. 679–684, 1957.

[23] M. L. Puterman, Markov Decision Processes: Discrete Stochastic Dy-
namic Programming, 1st ed. USA: John Wiley and Sons, Inc., 1994.
[24] R. A. Howard, Dynamic Programming and Markov Processes. Cam-

bridge, MA: MIT Press, 1960.

[25] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra, “Planning and

[26] K.

acting in partially observable stochastic domains,” USA, 1996.

˚Astr¨om, “Optimal control of markov processes with incomplete
state information,” Journal of Mathematical Analysis and Applications,
vol. 10, no. 1, pp. 174–205, 1965.

[27] R. Bellman, Dynamic Programming. Dover Publications, 1957.
[28] C. H. Papadimitriou and J. N. Tsitsiklis, “The complexity of markov
decision processes,” Math. Oper. Res., vol. 12, p. 441–450, Aug. 1987.
[29] C. Watkins, “Learning from delayed rewards,” Ph.D. dissertation, 1989.
[30] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,

“Proximal policy optimization algorithms,” CoRR, 2017.

[31] T. Jaakkola, M. Jordan, and S. Singh, “Convergence of stochastic
iterative dynamic programming algorithms,” in Advances in Neural
Information Processing Systems, vol. 6, 1994.

[32] H. Robbins and S. Monro, “A Stochastic Approximation Method,” The
Annals of Mathematical Statistics, vol. 22, no. 3, pp. 400 – 407, 1951.

[33] A. Wald, Sequential Analysis. Wiley and Sons, New York, 1947.
[34] A. N. Shirayev, Optimal Stopping Rules. Springer-Verlag Berlin, 2007,

reprint of russian edition from 1969.

[35] G. Peskir and A. Shiryaev, Optimal stopping and free-boundary prob-
lems, ser. Lectures in mathematics (ETH Z¨urich). Springer, 2006.
[36] J. L. Snell, “Applications of martingale system theorems,” Transactions

of the American Mathematical Society, vol. 73, no. 2, 1952.

[37] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-
dimensional continuous control using generalized advantage estimation,”
in Proceedings of the International Conference on Learning Represen-
tations (ICLR), 2016.

[38] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
2014, international Conference for Learning Representations, San Diego.
[39] D. Merkel, “Docker: lightweight linux containers for consistent devel-
opment and deployment,” Linux journal, vol. 2014, no. 239, p. 2, 2014.
[40] K. Hammar and R. Stadler, “gym-optimal-intrusion-response,” 2021,

https://github.com/Limmen/gym-optimal-intrusion-response.

[41] W. Blum, “Gamifying machine learning for stronger security and ai

models,” 2019.

[42] A. Ridley, “Machine learning for autonomous cyber defense,” 2018, the

Next Wave, Vol 22, No.1 2018.

[43] E. J. Sondik, “The optimal control of partially observable markov pro-
cesses over the inﬁnite horizon: Discounted costs,” Operations Research,
vol. 26, no. 2, pp. 282–304, 1978.

12

